Under review as a conference paper at ICLR 2022
Robust Imitation via Mirror Descent
Inverse Reinforcement Learning
Anonymous authors
Paper under double-blind review
Ab stract
Adversarial imitation learning techniques are based on modeling statistical diver-
gences using agent and expert demonstration data. However, unbiased minimiza-
tion of these divergences is not usually guaranteed due to the geometry of the
underlying space. Furthermore, when the size of demonstrations is not sufficient,
estimated reward functions from the discriminative signals become uncertain and
fail to give informative feedback. Instead of formulating a global cost at once,
we consider reward functions as an iterative sequence in a proximal method. In
this paper, we show that rewards dervied by mirror descent ensures minimization
of a Bregman divergence in terms of a rigorous regret bound of O(1/T) for a
particular condition of step sizes {ηt}tT=1. The resulting mirror descent adver-
sarial inverse reinforcement learning (MD-AIRL) algorithm gradually advances a
parameterized reward function in an associated reward space, and the sequence
of such functions provides optimization targets for the policy space. We empir-
ically validate our method in discrete and continuous benchmarks and show that
MD-AIRL outperforms previous methods in various settings.
1 Introduction
Inverse reinforcement learning (IRL) is an algorithm of learning ground-truth rewards from demon-
strations of an expert that acts optimally with respect to an unknown reward function. Traditional
IRL approaches (Ng & Russell, 2000; Abbeel & Ng, 2004; Ziebart et al., 2008) solve the imita-
tion problem based on iterative algorithms, alternating between the reward estimation process and
a reinforcement learning (RL) algorithm. Recent adversarial imitation learning (AIL) studies (Ho
& Ermon, 2016; Fu et al., 2017) focus on formulating a statistical divergence minimization with a
fine-tuned representation of the target expert probability distribution (Ghasemipour et al., 2020).
One method to approximate the distance between probability distributions is using Bregman diver-
gences, a family of metric-like functions induced by a strongly convex function. Jeon et al. (2020)
generalized the AIL framework to an optimization problem with respect to a Bregman divergence
between policy distributions where the method shares on the essence of regularized Markov deci-
sion processes (Geist et al., 2019). Through the lens of differential geometries, this implies that a
policy distribution and a reward function can be associated with geometric constraints specified by
a convex regularizer (Shima, 2007). Apparently, the limitation of regularized IRL studies naturally
comes from a geometric property in which minimizing a Bregman divergence does not guarantee
unbiased progression due to the constraints of the underlying space (Butnariu & Resmerita, 2006).
The success of machine learning has been the key to learning such divergences with high preci-
sion. When the demonstration size is sufficiently large, the discriminator approximation of AIL can
model the entire expert policy distribution internally using nonlinear approximators such as neu-
ral networks (Finn et al., 2016; Fu et al., 2017). However, there are challenging tasks that require
long action sequences to be solved. When states can only be rarely visited by imitation learning
agents, the uncertainty of estimated expert policy often substantially affects the discrimination qual-
ity. Therefore, the development of a robust IRL method that is tolerant to unreliable discriminative
signals induced by imperfect demonstrations is necessary.
The reasonable question is how to learn with cost estimations that are not precisely determined
throughout the learning process. There have been considerable achievements in dealing with tempo-
ral costs in the online learning domain (Fiat & Woeginger, 1998; Hazan, 2019). An online learning
1
Under review as a conference paper at ICLR 2022
algorithm predicts a sequence of parameters {wt}T=ι ⊂ W that is incurred by a cost function Ft (∙) at
each iteration. The most ordinary approach in online learning is stochastic gradient descent (SGD):
wt+ι = Wt - ηtVFt(wt). SGD is a desirable algorithm when the set W is the Euclidean geometry
since it can provide unbiased progression for minimizing Ft (Raskutti & Mukherjee, 2015). How-
ever, policies appear in manifolds of probability distributions; a gradient may not be the direction of
the steepest descent in this case due to geometric constraints (Amari, 1998; 2016).
In this paper, we propose an iterative imitation learning framework that interprets the RL-IRL
scheme similar to proximal optimization methods (Boyd et al., 2004; Amari, 2016). We identify
two issues in the AIL studies that have characteristics of unconstrained updates: (1) a divergence
does not guarantee informative global directions to match the expert policy due to the geometry, and
(2) a representation of divergence often cannot be accurately obtained due to insufficient data.
Our method is motivated by a template of optimization algorithms specified by a convex function,
called mirror descent (MD; Nemirovsky & Yudin 1983). For parameters sequences, cost functions,
and step sizes, an MD update for a strongly convex function Ω is derived as
VΩ(wt+ι) = VΩ(wt) - ηtVFt(wt),	(1)
where VΩ is a bijective transformation that links the primal space and the dual space of gradients.
Different from these standard MD formulations, our methodology draws a sequence of functions
on a space formulated by a regularized reward operator which is an alternative to the dual space.
Consequently, the reward functions are projected optimization targets for the space of policies.
Our Contributions. We propose a novel IRL algorithm which facilitates the agent in robustly
imitating the expert. Our work is complementary to previous regularized IRL studies; we introduce
a geometric perspective for optimizing rewards and derive solutions with theoretical guarantees.
•	Instead of a monolithic estimation process of a global solution, we derive a sequence of reward
functions that provides local optimization targets for the space of policies (Section 3).
•	We prove that rewards derived by an MD algorithm guarantee convergent divergence minimization
performance along with a rigorous regret bound (Section 4).
•	We propose mirror descent adversarial inverse reinforcement learning (MD-AIRL), a novel IRL
algorithm that can be easily implemented on top of the standard AIL framework (Section 5).
•	We validate the outperforming performance of MD-AIRL on benchmarks with large discrete ac-
tion spaces and continuous action spaces (Section 6).
NEW R1, R2
2 Preliminaries
Notation. For finite sets X and Y , we define Y X as a set of functions from X to Y . ∆X (∆YX) is a
set of (conditional) probabilities over X (conditioned on Y). For a function f ∈ RS×A and a policy
π∈∆SA, fs and πs denote shorthand notation of f (s, ∙) and ∏(∙∣s), respectively. A Markov decision
process (MDP) is defined as a tuple (S, A, P, r, γ) with the state space S, the action space A, the
transition kernel P ∈ ∆SS×A, the reward function r ∈ RS×A and the discount factor γ ∈ [0, 1). A
Bregman divergence with respect to a convex function Ω for a state S is defined as DΩ(∏sk∏S)=
Ω(πs) - Ω(πs) - hVΩ(πs), πs - ∏si∕. We write Ω(π) = [Ω(πs)]s∈s and similarly for vΩ.
Regularized RL & IRL. We consider the RL-IRL framework in regularized MDPs (Geist et al.,
2019), where the policy is optimized along with a causal convex regularizer. The objective is to find
∏ which maximizes the expected discounted sum of rewards with a strongly convex regularizer Ω:
maximize Jω(∏, r):
π∈∆SA
En [Xi=0 γi{r(Si ,ai) - ω(π(ISi))}],
(2)
where the subscript π on the expectation denotes samples generated from the MDP and policy π.
Consider the convex conjugate of qE = VΩ(∏E): Ω*(qE) = max∏s∈∆A h∏s,qE〉a - Ω(πs). Differ-
entiating both sides with respect to qE, the gradient of conjugate VΩ* maps qE to the unique optimal
policy πE . Let Π be a bounded, open set for ∆SA ⊂ Π and R : Π → RS×A be a reward operator
which maps a policy to a reward function. The output r = R(∏E) is a representation of the expert's
behavior; but finding such operator is an ill-posed problem because every function r that makes the
state-action value function qE is a valid solution of regularized IRL (Geist et al., 2019). Recently,
Jeon et al. (2020) proposed a tractable solution, which we refer to the function as the regularized
reward function. We rewrite the previous notation by defining the regularized reward operator Ψω.
2
Under review as a conference paper at ICLR 2022
Definition 1 (Regularized reward operators). Define the regularized reward operator Ψω : ∏ → ψ∏
ψπ(s, a) := Ω0(s, a; π) - <πs, VΩ(πs)^4 + Ω(πs)	Vs ∈ S,a ∈ A,	(3)
for Ω0(s, ∙; π) = VΩ(πs) = [VpΩ(p)]p=∏(.∣s>
By using the operator in discriminative networks, Jeon et al. (2020) proposed regularized adversarial
IRL (RAIRL) as a generalization of AIL in terms of minimizing a specific Bregman divergence.
Mirror Descent. Let w be a parameter on a set W and Ft : W → R be a convex cost function
from a class of functions F . Replacing the L2 proximity term of proximal gradient descent with the
Bregman divergence (Gutman & Pena, 2018), the proximal form of the MD update is given as
WMDI= argmin〈VFt(WMD),w - wMD〉W + αtDc(w∣∣wMD),	(4)
w∈W
where at := '/% denotes an inverse of the step size ηt. MD was developed as a generalization of
SGD, where the local geometry is specified by a Bregman divergence (Gunasekar et al., 2020).
3 Iterative RL-IRL as a Proximal Optimization Method
We interpret the RL-IRL framework as a variant of proximal optimization methods and consider two
sequences {πt}t∞=1 and {ψt}t∞=1 that denote the learning policies and reward functions, respectively.
Associated reward function. An updated point in MD can be uniquely projected to the de-
sired space (∆A in our case) using a Bregman projection operator PΩ that locates iterative points
to the feasible region, i.e. ∏t+ι = PΩ(∏t+ι) ：= argmin∏∈∆S[DΩ(∏k∏t+ι)]s∈s for ∏t+ι ∈ ∏.
To avoid these computations, we preemptively constrain a reward operator to satisfy the projection
invariance with respect to regularized RL process of the associated state-action value function q:
PΩ(VΩ*(q)) = vΩ*(q). According to Lemma 1 of Jeon et al. (2020), a regularized reward func-
tion ψ∏ can replace q, since the induced Bregman divergence allows the learning in a greedy manner. FIX R3
The projection invariance of the operator Ψω can be shown by a bijective relation with VΩ* in the
space of ∆SA by the following lemma.
Lemma 1 (Natural isomorphism). Let Ψ ∈ Ψω(∆A) for Ψω(∏) = { ψ | ψ(s,a) = ψ∏(s,a), Vs ∈
S, a∈A,π ∈Π }. Then, VΩ*(ψ) is unique with respect to ψ, andfor every π = VΩ*(ψ), π ∈ ∆A.
FIX R3
Figure 1: A schematic illustration. Suppose an
update is constrained by a divergence with re-
spect to current πt (gray). MD is performed in
the space defined by Ψω. ∏t+ι is recovered in
the desired space ∆A (solid curve) using VΩ*.
The proof is in Appendix A. Figure 1 illustrates
that there is unique ψt for πt, hence the two func-
tions are isomorphic objects. Note that the trans-
formed Ψ0(∏t) is different from the form VΩ(∏t)
of the dual space that is used in classical MD algo-
rithms. The transformation is shifted by a vector
1c with a constant C = Ω(∏S) - h∏S, VΩ(∏S))/
for each s. However, if the underlying space is
△A, the operator VΩ* reconstructs the original
point for both Ψω and VΩ, since the distributivity
of Ω (GeiStetaL,2019): Ω*(y + 1c) = Ω*(y) + c,
holds (so VΩ*(y + 1c) = VΩ*(y)). Conse-
quently, we may omit the projection phase (the
dotted angle in Figure 1) when the reward func-
tion satisfies the constraint of Ψω (∆A).
NEW R3
As a result, we consider an updated reward function ψt+ι ∈ Ψω (∆A) as a projected target of MD, FIX R3
which can be associated by parameterization of △SA. For instance, a bijective mapping of ψt+1 can NEW R3
represent a softmax policy for a discrete space, or a Gaussian policy for a continuous space, using
its parameters. The subsequent RL process at t-th step becomes finding the next iteration πt+1 by NEW R3
maximize Jω(∏, ψt+ι)=En hX∞=0 γiDΩ(∏si∣∣∏S+ i)i，	Πt+1 = VΩ*(ψt+ι).	(5) FIX R3
The equation shows that an arbitrary regularized RL algorithm with the regularizer Ω forms a cu-
mulative discounted sum of Bregman divergences, thus the mapping to πt+1 can be achieved.
3
Under review as a conference paper at ICLR 2022
2
O•工 e∙os
t=10	t=100 (4X)
Shannon entropy	TSaHiS entropy
Io3Io0
.≥6 UBlU6①后
IO0
10-1
50	100	0	50
Step	Step
Figure 2: The policy πt minimizes an online cost Dω(∙k∏E,t) associated with a convex regularizer
such as negative Shannon and Tsallis entropies. The left example shows that the number of updates
of πt+1 vary by the step sizes η of MD formulation. The plots on the right show that DQ (πt k∏E) vary
by η (averaged over 10 different trials). The solid red lines represent the baselines of DQ(πE,t k∏E).
Online imitation learning. The necessity of online learning setup comes from observing that
our RL-IRL processes do not retain ψE = Ψω(πE), a representation of πE, during training. Instead,
consider a random process {πE,t}∞=1 where the estimation ΠE,t resides in a closed, convex neigh-
borhood of πE, estimated by a separate estimation algorithm such as maximum likelihood methods.
Substituting ψE to ψπE,t in Jω (π, ψE) turns the RL objective into an online learning problem:
minimize Ft(π) = -Jω(∏,Ψ∏e t) = En
π∈∆SA	E,t
[X∞=0 Yi DΩ (∏Si 怩it)].
(6)
FIX R3
For a deeper understanding of our setup, we consider an example in Figure 2. Assume that policies
of the learning agent and the expert follow multivariate Gaussian distributions initially at π1 =
N([0, 0]t, I) and ∏e = N([5, 3]t, ∑e) for ∑| < 1. The policy πt is trained by a cost function
Dω(∙k∏E,t) with the proximity constraint 1 Dω(∙∣∣πt) where the Gaussian policy Πe,t is also fitted
using stochastic samples from πE at each iteration, starting from Πe,1 = π1. We first observe that the
step size constant η affects the training speed in the early phase. Notably, the performance of certain
cases exceeds the baselines of Dω(∏e,t k∏E) by choosing the step size effectively low as η < 1. This
suggests that there is a clear advantage of the online imitation learning setup for unreliable ΠE,t.
Figure 3: Illustrations of mirror descent imitation learning in the (a) t-th iteration and (b) (t + 1)-
th iteration. Consider that {πE,t}∞=1 is a random process and Πe,t and πE,t+1 are sampled from a
neighborhood of πE with respect to a norm. The MD step is taken in the interval of πt and ΠE,t. Note
that by decreasing the step size of updates, the region of πt+1 (blue) shrinks.
MD update rules. Plugging each divergence of the cumulative cost Ft to the template of Equa-
tion (4), the optimization process for the subsequent ψt+1 of the MD formulation is derived as1
ψt+1 =ψΩ (πt+1), πs+i =argmin"DΩ (πs kπE,t),π S - πs SIA + αtDΩ (π SIIns )
∏S ∈Δa 、------------}
VΩ(∏S )-VΩ(∏E,t)
=argmin Dω (π sID- Dω (π SlIns) + αtDω (π SlIns)
πs∈∆A
=argminη Dω(ns k∏E,t)+(1 - ηt) Dq(nSlIns),	ηt = 1∕αt, ∀s ∈ S,⑺
ns ∈Δa	、----{z-----}	、--「L----}
estimated expert	learning agent
where the gradient of Dω is taken with respect to its first argument ns. The objective of reward
learning is analogous to finding an interpolation at each iteration where the point is controlled by
the step size ηt . Figure 3 shows that the region of nt (defined by a norm) gradually decreases
1 Bregman divergences are generally intractable to be computed, unless the policy is a specific parametric
model (e.g. exponential families). See the works of Nielsen & Nock (2011); Jeon et al. (2020) and Appendix B.
NEW R3
4
Under review as a conference paper at ICLR 2022
when ηt > ηt+1. Note that solving the optimization of Ft requires interaction between πt and the
dynamics of the given environment; thus, the RL process in Equation (5) plays the essential role of
sequential learning by the value measures of the reward function.
4 Analyses on Step Sizes, Convergence, and Regrets
Online IRL. We define an online cost ft(πt, τt) which involves a policy and a trajectory as inputs.
The trajectory τt = {si}i∞=0 is available at the t-th step by executing the policy πt in the environment.
An online cost function for reward ψ⅛,t = Ψn(∏E,t) can be expressed as
ft(πt,τt) = X∞=0γi{(Xaπt⑷sJψE,t(Si,a)) -a(nt(1si))}.	⑻
We refer the objective of the learning is to find a unique fixed point ∏ ∈ Π that minimizes
E[ft(∏, T)], where the expectation is taken over the entire steps (i.e. limt→∞ Eτι,∙∙∙ ,τt [f (∏t, τt)]).
Taking (stepwise) gradient for each π(∙∣s), π* is found by E[VΩ(π*(∙∣s)) — vΩ(πβ,t(∙∣s))] = 0
when t → ∞, hence VΩ(∏*) = limt→∞ E[VΩ(∏β,t(∙∣s))]. The assumption of ∏ = ∏* allows the
particular situation when the estimation algorithm of ∏β,t is actually convergent with t → ∞. This
also allows general situations where the estimated expert policy is not stationary; the algorithm finds
the unique point according to the expectation of gradient by scheduling the step size ηt .
FIX R3
NEW R3
sequence example.
Theoretical Analyses. We state two conditions of {ηt}t∞=1 to guarantee
convergence properties which is explained in the following analyses.
•	Convergent sequence & divergent series:
lim ηt = 0 and	ηt = ∞.	(9)
t→∞	t=1
•	Divergent series & convergent series of squared terms:
X∞ ηt = ∞ and X∞ ηt2 < ∞.	(10)
Under the satisfaction of both cases, a sequence that is divergent in its series (e.g. harmonic series
P∞=ι 1) is suitable as in Figure 4. In our arguments, a policy conditioned by a state is in a Banach
space called the LP space (RA, k∙kp), where ∣∣∙kp denotes ap-norm, where we assume 1 < p ≤ 2
for the convergence in the dual Lq space (1/p + 1/q = 1). The proofs are in Appendix A.
Theorem 1 (Stepsize considerations). Let Ω be strongly convex, VΩ be Lipschitz continuous,
and the associated Bregman divergences are bounded. Assume inf π∈∆S E[ft (π, τ)] > 0. Then
limτ →∞ Eτι,∙∙∙,ττ [P∞=o Do(∏Si k∏Ti)] = 0 if and only ifEq.(9)is satisfied.
(a)	If Jim ηt = 0, then T ∈ N, n <T, and c > 0 exist s.t. Eτι,∙∙∙,ττ [fτ (∏τ ,τr)] ≥ T--n.
(b)	Ifthe Step size takes theform ηt = t+pι, then Eτι,∙∙∙,ττ [P∞=o γiDΩ (∏Si∣∣∏Ti)] = O(1/T).
Theorem 2 addresses the convergence of the algorithm in a specific case when πE can be achieved.
Theorem 2 (Convergence in optimal cases). Let Ω be strongly convex, VΩ be Lipschitz continuous,
and the associated Bregman divergences are bounded. Assume π1 6= πE and inf π∈∆S E[ft (π, τ)] =
0. Then, limt→∞ Eq,…,τt [ft(∏t,τj] = 0 if and only if P∞=1 ηt = ∞. If ηt ≡ ηι, then there
exist c1,c2 ∈ (0,1) such that ci ∙ Ai ≤ AT ≤ c? ∙ Ai, where {At}∞=ι denote a SeqUenCe of At =
Eτι,…,τt[DΩ(π*kπt)].
Proposition 1 provides a sufficient condition for the almost certain convergence ofan MD algorithm
by imposing the stronger condition of step size in Equation (10).
Proposition 1 (Convergence in general cases). Assume inf π∈∆S E[ft (π, τ)] > 0. If the step size
sequence satisfies Eq. (10), then we have limt→∞ E∞=o γlDΩ(∏Si k∏Si) = 0 almost surely.
Regrets. Define the regret at t-th iteration for a sequence of cost functions {ft}t∈N as
Rt = t Pj=Ifj(∏j,Tj) - inf∏∈∆A{t Pj=Ifj(∏,Tj)}.	(11)
In the optimal case of infπ∈∆S E[ft (π, τt)] = 0, Rt is bounded by O(1/T) since ft inherits the
property of Bregman divergence so that the infimum is achieved by 0 at πE. By Proposition 1,
the updates regarding the policy converge when the step size sequence abides by Equation (10).
Therefore, even for the general case of infπ∈∆S E[ft(π, T)] > 0, RT is bounded to O(1/T).
FIX R3
5
Under review as a conference paper at ICLR 2022
Algorithm 1 Mirror Descent Adversarial Inverse Reinforcement Learning.
1:	Input: an expert trajectory dataset (Tj)T=1, a regularized reward function ψφ ∈ Ψω(∆A), an
agent policy πθ, an estimate policy πν, a neural network b : S → R, λ, α1, αT ∈ R+.
2:	for t — 1 to T do
3:	at - αι + αT-α1 (t - 1)
4:	Collect rollout trajectories τt by using the agent policy πθ .
5:	Optimize b via binary logistic regression for Db to classify Tt=	and τt.
6:	Optimize ∏ν via binary logistic regression for Dθ,ν to classify Tt and τh
7:	Optimize ψφ with the MD objective in Equation (7) using αt,	Ttt , Tt, πθ,	and πν.
8:	Train ∏θ via a regularized actor-critic method to maximize ψφ(s,a) with regularizer λΩ(∙).
9:	Output: πθ, ψφλ .
5 Mirror Descent Adversarial Inverse Reinforcement Learning
In this section, parameters θ, φ, and ν are presented representing agent policy, reward, and expert
policy functions respectively parameterized with neural networks. On top of the standard AIL algo-
rithm, we propose the MD-AIRL method, learning with a dual discriminator architecture motivated
by previous studies regarding multiple discriminators (Nguyen et al., 2017; Chongxuan et al., 2017).
AIL methods have dealt divergences between joint densities of states and actions (Ghasemipour
et al., 2020). To clearly distinguish matching overall state densities and imitating specific behavior,
we aim to disentangle these concepts and propose two structured discriminators:
Dθ,ν(s, a) = σ(log{∏ν(a∣s)∕∏θ(a∣s)} + b(s)) and	Db(S) = σ(b(s)),
where b : S → R is a neural network regarding states and σ(∙) denotes the sigmoid function. The
discriminators Db and Dθ,ν are trained with binary cross-entropy losses regarding trajectories:
maximize EπE [log Dθ,ν (s, a)] + Eπt [log(1 - Dθ,ν (s, a))],	(12)
maximizeEπE[logDb(s)] + Eπt [log(1 - Db(s))],
b
(13)
Note that b and θ are not trained for learning Dθ,ν . Let ρπ ∈ ∆S denote normalized state visitation
distribution of π, which is defined as ρ∏(s) := (1 - Y)E∏ [P∞=o YiI{si = s}] where I{∙} is an
α
a十β
indicator function. Since x 7→ α log σ(x) + β log(1 - σ(x)) attains its maximum at σ(x)
(Goodfellow et al., 2014), optimality is achieved when πν = πE and b(s) = log ρπE (s)∕ρπ(s).
Let ψφ denote the regularized reward function where the parameter φ is trained using a step size ηt,
the agent policy πθ, and the estimation of expert policy πν:
minimize Es 〜Tt
φt
限 DΩ(πφ(1s)kπν (1S)) + (1
-ηt)DΩ(πφ(∙ls)kπθ (•〔SH,
(14)
where ∏φ denotes the transformed policy VΩ(ψφ) and Tt denotes mini-batches of states using the
both agent and expert trajectories. The algorithm adjusts the term ηt = 1∕αt with a harmonic
progression by linearly increasing αt by the range of [α1, αT], which is derived from our analyses.
For a hyperparameter λ > 0, the MD-AIRL reward function is defined by a linear combination:
ψφλ (S, a) = λψφ(S, a) + b(S).
By using arbitrary regularized RL which uses λΩ(∙) as the regularization function, the reward learn-
ing of ψφλ regarding the agent πθ is decomposed into the following two terms:
E∏θ hψλ(s, a) - λα(πθ (∙ls))i = λE∏θ [ψφ(s, a) - ω(πΘ (1s))] - DKL(PnθkP∏E )
=-λE∏θ [DΩ(πθ (Is)knO(Is))] - DKL(PnθkP∏E ),
Minimizing the first term of E∏θ [Dω(∏S∣∣∏Φ)] represents the online learning of MD formulation.
However, ψφ that is trained on Tt cannot cover the entire reachable states since the state visitation is
heavily misaligned in challenging sequential decision problems. Therefore, we propose the second
auxiliary term of generalized KL divergence DKL(Pnθ kPnE), playing an additional role of facilitat-
ing the supports of state visitation densities to be matched properly. Algorithm 1 summarizes the
entire procedure of MD-AIRL. See Appendix C for detailed implementation.
6
Under review as a conference paper at ICLR 2022
Table 1: Bregman divergences with ground-truth distribution with five different types of regulariza-
tion. The numbers are multiplied by the dimension of action space (We report |A| ∙ Dω(∏t IInE)).
		|A|	= 102		|A|	= 103		|A| =	104
Algorithm	RAIRL	MD-AIRL	RAIRL	MD-AIRL	RAIRL	MD-AIRL
Shannon	2.55 ± 1.59	2.28 ± 1.20	140.3 ± 87.5	125.3 ± 61.8	5752.9 ± 2986	5943.1 ± 2967
Tsallis	0.21 ± 0.13	0.11 ± 0.04	0.55 ± 0.13	0.24 ± 0.03	4.95 ± 2.3	4.21 ± 0.16
exp	0.27 ± 0.17	0.13 ± 0.06	0.55 ± 0.12	0.23 ± 0.03	5.06 ± 2.44	4.97 ± 0.69
coS	0.05 ± 0.04	0.02 ± 0.01	0.03 ± 0.02	0.01 ± 0.01	0.21 ± 0.62	0.05 ± 0.05
Sin	0.34 ± 0.25	0.12 ± 0.04	3.82 ± 3.46	1.07 ± 0.75	8.12 ± 3.82	7.59 ± 1.02
SHANNON	TSALLIS
÷- MD-AIRL
—RAIRL
EXP
COS
SIN
IE-2
IE-3
ɪɪ 1E-4l
150K	300K O
Time Step
MD-AlRL
RAIRL
O 150K	300⅛
Time Step
MD-AlRL
RAIRL
O 150K	300K
Time Step
1E-2
IE-4
IE-3
IE-3
IE-5
+ MD-AIRLI IE-2
一^μ RAIRL ；
-ɪ MD-AIRL
+ RAIRL
^150K	300∣1e-40	150K	300
Time Step	Time Step
Figure 5: The average Bregman divergence measured on the log scale in multi-armed bandits at the
action size of |A| = 103. The shaded area represents 95% confidence interval for five runs.
6	Experimental Results
For the RL algorithm, We implemented a RAC (Yang et al., 2019) method that is a generalization
of SAC (Haarnoja et al., 2018) in terms of the regularization choice. We considered the class of
separable regularizers Ω(p) = -Ea〜p[夕(p(a))]: (1) Shannon entropy (夕(x) = log(χ)), (2) Tsallis
entropy Tq (g(x; q) = q-ι(xq-1 - 1), q = 2 by default), (3) exp regularizer (夕(x) = e - ex), (4)
cos regularizer (夕(x) = cos( (∏x)), and (5) Sin regularizer (W(X) = 1 一 Sin 2x). We evaluated our
approach on three topics (bandits, multiple goals, and MuJoCo environments). The main compar-
ative method Was RAIRL With a density-based model (RAIRL-DBM) since this model shares the
identical level of expressiveness as our method When the parameterization of ∆SA is specified.
6.1	Large-Scale Multi-armed Bandits
We first considered multi-armed bandit problems, Where the cardinality of action spaces is varied.
Learning the optimal distribution ofπE becomes challenging as |A| increases, because the frequency
of each sample becomes sparse due to the curse of dimensionality (Bellman et al., 1957). For
each experiment, a stateless expert distribution πE Was generated by the parameters of softmax
distribution πE(i) = exp(zi)/Pj exp(zj) Where the logits zi Were randomly initialized to a uniform
distribution. We set the action size to |A| = 102, 103, 104 and restricted the sample size to 16.
Figure 5 shoWs that the Bregman divergence Was large for MD-AIRL at the early training phase,
because We chose the initial step size η1 to be greater than 1 (α1 = 0.5). MD-AIRL exceeded the
discriminative performance of RAIRL after certain steps, While the progression of RAIRL mostly
stopped at local minima. Table 1 shoWs that MD-AIRL achieved overall loWer Bregman divergence
on average When three different cardinalities and five regularizers Were considered. MD-AIRL out-
performed RAIRL in four cases by choosing effectively loW step size at the ηT to be less than 1
(αT = 2). These results match properties of MD algorithms and our convergence analyses. There-
fore, We argue that a constrained update rule With appropriate step sizes is necessary for robust
reWard acquisition and imitation for the situations When the total number of data samples is limited.
6.2	Continuous Multi-Goal Environment
We then considered an environment With a tWo-dimensional continuous state space. In this env-
ioronment, an agent is a point mass initialized at the origin, and the four goals are located in the four
cardinal directions. To draW meaningful reWard surface, We considered multivariate Gaussian dis-
7
Under review as a conference paper at ICLR 2022
(a) Multi-Goal Environment
Shannon entropy
UO_SE_S①
1 1
AdO匕 UW
(b) Goal Entropy
Tsallis entropy
Step 5K	Step 300K
Step 5K	Step 300K
(c) Reward surfaces and trained policiesfr (state = (5, -1))
Figure 6: (a) The multi-goal environment, MD-AIRL trajectories, and the ground-truth rewards
are shown. (b) The information entropies for the probabilities of achieving four goals. The x-
axis indicates the q value of the Tsallis entropy regularizers. The Shannon entropy regularizer is
considered by the case of q = 1. (c) The top of each column shows regularized reward surfaces
obtained by πν. The middle and bottom show regularized rewards from πφ and the policy πθ .
tribution policies with full covariance matrices. We parameterized a covariance matrix of Gaussian
policy using lower triangular matrix which is an outcome of LDL decomposition (see Appendix C).
Figure 6 (a) shows trajectories generated by the trained agent. Figure 6 (b) shows that MD-AIRL
achieved higher entropy for reaching the goals. Figure 6 (c) shows reward surfaces with regularizers,
which was calculated by ψφ(s, a) + 夕(∏θ(a∣s)) for each point of a ∈ A and S = (5, -1). During
the training, the MD reward was similar to the estimated ground truth using adversarial training.
However, the surface of MD-AIRL became flatter than the ground-truth estimation when πt was
sufficiently close to the expert behavior. As a result, we claim that drastic changes in the target
distribution, which are one of the typical characteristics of adversarial frameworks, are prevented.
We argue that these characteristics mitigate overfitting caused by unreliable discriminatve signals.
Hopper
Walker2d	HaIfCheetah	Ant
Figure 7: Average scores in MuJoCo benchmarks. The x-axis indicates the q value of the Tsallis
entropy regularizers. Shaded regions indicate 95% confidence intervals for four different runs. Top:
4 demonstrations. Bottom: 100 demonstrations.
6.3	Continuous Control: MuJoCo
We validated MD-AIRL on MuJoCo continuous control tasks (Brockman et al., 2016). We as-
sumed diagonal Gaussian policies for both learner’s policy π and expert policy πE. Instead of the
tanh-squashed policy (Haarnoja et al., 2018), we used the hyperbolized environment assumption of
RAIRL, which means that tanh is regarded as a part of the environment.
For each tasks, we considered two different numbers of episodes collected by an expert policy. In
Figure 7, the performance of MD-AIRL, RAIRL, and behavior cloning (bc; Pomerleau, 1991) al-
8
Under review as a conference paper at ICLR 2022
Hopper
Time Step
Walker2d	HaIfCheetah	Ant
Time Step	Time Step
Figure 8: Average scores during training with 4 demonstrations (Tsanis regularize] T with q = 2).
gorithms are shown with the expert and random agent performance. In terms of sample efficiency,
MD-AIRL outperformed RAIRL on Hopper-v3, Walker-v3, and Ant-v3. The performance
gaps between MD-AIRL and RAIRL were more prominent in 4 episodes of expert demonstrations.
The training curves in Figure 8 indicate that MD-AIRL showed lower variance than RAIRL espe-
cially after early phase of training. It can be concluded that MD-AIRL inherits the sample efficiency
of AIL algorithms in challenging RL tasks benchmarks. Additionally, the algorithm is highly stable
with respect to limited sample sizes, which is in alignment with our theoretical analyses.
7	Related Works
Statistical manifolds. The Hessian of strongly convex function forms a metric tenser of a man-
ifold called Hessian geometries (Shima, 2007). Bregman divergences are similar to these metrics,
providing useful metric-like properties (Butnariu & Resmerita, 2006). Using the Bregman diver-
gence allows to solve many optimization problems, generalizing traditional approaches such as least
SqureS (Boyd et al., 2004; Hiriart-Urruty & Lemarechal, 20θ4). Probability distributions can be
considered as points in a geometric space. A representative statistical manifold is the information
geometry induced by the Fisher-Rao metric (Amari, 2016; Bauer et al., 2016; Nielsen, 2020).
Regularized IRL. Energy-based policies (i.e. Boltzmann distributions) have been appeared in
early IRL researches (Ramachandran & Amir, 2007; Neu & Szepesvari, 2007; BabeS-Vroman et al.,
2011). Notably, MaxEnt IRL (Ziebart et al., 2008; 2010) is a representative IRL algorithm based
on information theory. Other statistical entropies have also been applied to the imitation learning
problem, such as the Tsallis entropy derived from Tsallis statistics (Lee et al., 2018). Compared to
RAIRL which also can use various convex regularizers (Jeon et al., 2020), our work allows more
realistic situations where the expert policy cannot be precisely attained due to insufficient data,
thanks to theoretical foundations originated from optimization studies.
Mirror descent. MD is closely related to algorithms regarding non-Euclidean geometries with
discretization of steps such as natural gradients (Amari, 1998; Raskutti & Mukherjee, 2015; Gu-
nasekar et al., 2020). On the primal space, the infinitesimal limit of MD step corresponds to a
Riemannian gradient flow (Do Carmo, 2016; Gunasekar et al., 2020). The online MD algorithms
possess rigorous regret bounds (Srebro et al., 2011; Lei & Zhou, 2020); thus they can be highly
efficient in terms of the number of evaluations until convergence.
8	Discussion and Conclusions
In this paper, we presented a novel IRL framework. We provided an mirror descent solution on
reward functions and corresponding theoretical arguments. We proposed MD-AIRL, a practical
adversarial IRL framework that can solve challenging imitation learning tasks. We verified that the
proposed method has clear advantages over previous AIL methods in terms of robustness. As the
reward hypothesis is grounded in obtainining a robust representation of the expert policy using IRL,
we argued that current IRL studies lack robustness and theoretical guarantees for practical situations.
Considering RL and its inverse problem with geometric perspectives is vital for achieving desired
goals in realistic situations. Although our work covers various online imitation learning methods
with MD, it does not include some other cases when the proximity term is of other statistical diver-
gence families such as f-divergence (Amari, 2016). Additionally, the boundedness and continuity
assumptions on Bregman divergences in our analyses are usually justified, but outliers exist such as
KL divergences. More sophisticated analyses on these general cases remain as future works.
NEW R1, R2
FIX R3
NEW R3
9
Under review as a conference paper at ICLR 2022
References
Pieter Abbeel and Andrew Y Ng. Apprenticeship learning via inverse reinforcement learning. In
Proceedings of the twenty-first international conference on Machine learning, pp. 1. ACM, 2004.
Sreangsu Acharyya, Arindam Banerjee, and Daniel Boley. Bregman divergences and triangle in-
equality. In Proceedings of the 2013 SIAM International Conference on Data Mining, pp. 476-
484. SIAM, 2013.
Shun-ichi Amari. Natural gradient works efficiently in learning. Neural computation, 10(2):251-
276, 1998.
Shun-ichi Amari. Information geometry and its applications, volume 194. Springer, 2016.
Monica Babey-Vroman, Vukosi Marivate, KaUshik Subramanian, and Michael Littman. Appren-
ticeship learning about multiple intentions. In Proceedings of the 28th International Conference
on International Conference on Machine Learning, ICML’11, pp. 897-904, Madison, WI, USA,
2011. Omnipress. ISBN 9781450306195.
Martin Bauer, Martins Bruveris, and Peter W Michor. Uniqueness of the fisher-rao metric on the
space of smooth densities. Bulletin of the London Mathematical Society, 48(3):499-506, 2016.
Amir Beck. First-order methods in optimization. SIAM, 2017.
Amir Beck and Marc Teboulle. Mirror descent and nonlinear projected subgradient methods for
convex optimization. Operations Research Letters, 31(3):167-175, 2003.
R. Bellman, Rand Corporation, and Karreman Mathematics Research Collection. Dynamic
Programming. Rand Corporation research study. Princeton University Press, 1957. ISBN
9780691079516. URL https://books.google.co.kr/books?id=wdtoPwAACAAJ.
Stephen Boyd, Stephen P Boyd, and Lieven Vandenberghe. Convex optimization. Cambridge uni-
versity press, 2004.
Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman, Jie Tang, and
Wojciech Zaremba. Openai gym, 2016.
Dan Butnariu and Elena Resmerita. Bregman distances, totally convex functions, and a method for
solving operator equations in banach spaces. In Abstract and Applied Analysis, volume 2006.
Hindawi, 2006.
LI Chongxuan, Taufik Xu, Jun Zhu, and Bo Zhang. Triple generative adversarial nets. In Advances
in Neural Information Processing Systems, pp. 4088-4098, 2017.
Manfredo P Do Carmo. Differential geometry of curves and surfaces: revised and updated second
edition. Courier Dover Publications, 2016.
Amos Fiat and Gerhard J Woeginger. Online algorithms: The state of the art, volume 1442. Springer,
1998.
Chelsea Finn, Sergey Levine, and Pieter Abbeel. Guided cost learning: Deep inverse optimal control
via policy optimization. In International Conference on Machine Learning, pp. 49-58, 2016.
Justin Fu, Katie Luo, and Sergey Levine. Learning robust rewards with adversarial inverse rein-
forcement learning. arXiv preprint arXiv:1710.11248, 2017.
Matthieu Geist, Bruno Scherrer, and Olivier Pietquin. A theory of regularized Markov decision
processes. In Kamalika Chaudhuri and Ruslan Salakhutdinov (eds.), Proceedings of the 36th
International Conference on Machine Learning, volume 97 of Proceedings of Machine Learning
Research, pp. 2160-2169, Long Beach, California, USA, 09-15 Jun 2019. PMLR. URL http:
//proceedings.mlr.press/v97/geist19a.html.
Seyed Kamyar Seyed Ghasemipour, Richard Zemel, and Shixiang Gu. A divergence minimization
perspective on imitation learning methods. In Conference on Robot Learning, pp. 1259-1277.
PMLR, 2020.
10
Under review as a conference paper at ICLR 2022
Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair,
Aaron Courville, and Yoshua Bengio. Generative adversarial nets. In Advances in Neural Infor-
mation Processing Systems,pp. 2672-2680, 2014.
Suriya Gunasekar, Blake Woodworth, and Nathan Srebro. Mirrorless mirror descent: A more natural
discretization of riemannian gradient flow. arXiv preprint arXiv:2004.01025, 2020.
David H Gutman and Javier F Pena. A unified framework for bregman proximal methods: SUbgra-
dient, gradient, and accelerated gradient schemes. arXiv, pp. arXiv-1812, 2018.
Tuomas Haarnoja, Aurick Zhou, Pieter Abbeel, and Sergey Levine. Soft actor-critic: Off-policy
maximum entropy deep reinforcement learning with a stochastic actor. In International Confer-
ence on Machine Learning, pp. 1856-1865, 2018.
Elad Hazan. Introduction to online convex optimization. arXiv preprint arXiv:1909.05207, 2019.
Jean-Baptiste Hiriart-Urruty and Claude Lemarechal. Fundamentals of convex analysis. Springer
Science & Business Media, 2004.
Jonathan Ho and Stefano Ermon. Generative adversarial imitation learning. In Advances in Neural
Information Processing Systems, pp. 4565-4573, 2016.
Wonseok Jeon, Chen-Yang Su, Paul Barde, Thang Doan, Derek Nowrouzezahrai, and Joelle Pineau.
Regularized inverse reinforcement learning, 2020.
Ilya Kostrikov, Kumar Krishna Agrawal, Debidatta Dwibedi, Sergey Levine, and Jonathan Tomp-
son. Discriminator-actor-critic: Addressing sample inefficiency and reward bias in adversarial
imitation learning. arXiv preprint arXiv:1809.02925, 2018.
Kyungjae Lee, Sungjoon Choi, and Songhwai Oh. Maximum causal tsallis entropy imitation learn-
ing. In Advances in Neural Information Processing Systems, pp. 4403-4413, 2018.
Yunwen Lei and Ding-Xuan Zhou. Convergence of online mirror descent. Applied and Com-
putational Harmonic Analysis, 48(1):343 - 373, 2020. ISSN 1063-5203. doi: https://doi.
org/10.1016/j.acha.2018.05.005. URL http://www.sciencedirect.com/science/
article/pii/S1063520318300836.
Arkadil Semenovich Nemirovsky and David Borisovich Yudin. Problem Complexity and Method Ef-
ficiency in Optimization. A Wiley-Interscience publication. Wiley, 1983. ISBN 9780471103455.
URL https://books.google.co.kr/books?id=6ULvAAAAMAAJ.
Gergely Neu and Csaba Szepesvari. Apprenticeship learning using inverse reinforcement learning
and gradient methods. In Proceedings of the Twenty-Third Conference on Uncertainty in Artificial
Intelligence, pp. 295-302, 2007.
Andrew YNg and Stuart J Russell. Algorithms for inverse reinforcement learning. In Proceedings of
the Seventeenth International Conference on Machine Learning, pp. 663-670. Morgan Kaufmann
Publishers Inc., 2000.
Tu Nguyen, Trung Le, Hung Vu, and Dinh Phung. Dual discriminator generative adversarial
nets. In I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and
R. Garnett (eds.), Advances in Neural Information Processing Systems, volume 30. Curran As-
sociates, Inc., 2017. URL https://proceedings.neurips.cc/paper/2017/file/
e60e81c4cbe5171cd654662d9887aec2- Paper.pdf.
Frank Nielsen. An elementary introduction to information geometry. Entropy, 22(10):1100, 2020.
Frank Nielsen and Richard Nock. On r\’enyi and tsallis entropies and divergences for exponential
families. arXiv preprint arXiv:1105.3259, 2011.
Dean A Pomerleau. Efficient training of artificial neural networks for autonomous navigation. Neu-
ral computation, 3(1):88-97, 1991.
Deepak Ramachandran and Eyal Amir. Bayesian inverse reinforcement learning. In Proceedings of
International Joint Conference on Artificial Intelligence, pp. 2586-2591, 2007.
11
Under review as a conference paper at ICLR 2022
G. Raskutti and S. Mukherjee. The information geometry of mirror descent. IEEE Transactions on
Information Theory, 61(3):1451-1457, 2015. doi: 10.1109/TIT.2015.2388583.
Walter Rudin. Real and Complex Analysis, 3rd Ed. McGraw-Hill, Inc., USA, 1987. ISBN
0070542341.
Hirohiko Shima. The geometry of Hessian structures. World Scientific, 2007.
Nati Srebro, Karthik Sridharan, and Ambuj Tewari. On the universality of online mirror descent. In
Advances in neural information processing systems, pp. 2645-2653, 2011.
David Williams. Probability with martingales. Cambridge university press, 1991.
Wenhao Yang, Xiang Li, and Zhihua Zhang. A regularized approach to sparse optimal policy in
reinforcement learning. In Advances in Neural Information Processing Systems, pp. 5938-5948,
2019.
Brian D Ziebart, Andrew Maas, J Andrew Bagnell, and Anind K Dey. Maximum entropy inverse
reinforcement learning. In Proceedings of AAAI’08 Proceedings of the 23rd national conference
on Artifical intelligence, volume 3, pp. 1433-1438, 2008.
Brian D Ziebart, J Andrew Bagnell, and Anind K Dey. Modeling interaction via the principle of
maximum causal entropy. In Proceedings of the 27th International Conference on International
Conference on Machine Learning, pp. 1255-1262. Omnipress, 2010.
12
Under review as a conference paper at ICLR 2022
A	Proofs
We define the space of policy as an object ∆SA, which is a vector space formed by a collection of
|S| elements of unit (|A| - 1)-simplexes: △/ = { xιeι + + x∣A∣e∣A∣ | PiAI θi = 1 and θi ≥
0 for i ∈ A . We assume that each space of simplex is a subset of a specific Banach space called
Lp space (RA, |卜||), where ∣∣∙k is ap-norm on A. The dual space of LP space for 1 < p < ∞ is Lq
space (RA, ∣∙∣∣*), where ∣∣∙∣∣* is defined as a q-norm (1/p + 1/q = 1). We assume 1 < p ≤ 2 for the
convergence property in the dual Lq space. We start with following definitions.
Definition 2 (Lipschitz constants). Given two metric spaces (X, dX) and (Y, dY ) where dX denotes
the metric on set X and dY is the metric on set Y , a function f : X → Y is called Lipschitz
continuous if there exists a real constant k ≥ 0 such that, for all x1 and x2 in X,
dγ(f(x1),f(x2)) ≤ k ∙ dχ(x1,x2).	(15)
In particular, a function f is called Lipschitz continuous if there exists a constant k ≥ 0 such that,
kf(xι) - f (x2)lk ≤ k ∙ ∣∣xι - x2∣∣, ∀x1,x2	(16)
where norms ∣∣∙∣ and ∣∣∙∣∣* are endowed with each space X and Y respectively. For the smallest
L that substitutes k, L is called the Lipschitz constant and f is called the L-Lipschitz continuous
function.
Definition 3 (Discrete-time martingale). Ifa stochastic process {Zt}t≥1 satisfies E[|Zn|] ≤ ∞ and
1) E[Zn+1 |X1, .	. . , Xn]	≤	Zn,	2) E[Zn+1 |X1,	. . . , Xn]	= Zn,	3) E[Zn+1 |X1, . . . , Xn]	≥	Zn,
then, the stochastic process {Zt}t≥1 is called a 1) submartingale, 2) martingale, and 3) super-
martingale, respectively, with respect to a filtration {Xt}t≥1.
The following analyses and proofs follow the results appeared in previous literatures for general
aspects (Nemirovsky & Yudin, 1983; Gunasekar et al., 2020; Srebro et al., 2011; Lei & Zhou, 2020;
Beck & Teboulle, 2003; Raskutti & Mukherjee, 2015). Our analyses extend existing results to
imitation learning, and they are also highly general to cover various online methods for sequential
decision problems.
A.1 Proof of Lemma 1
Proof of Lemma 1. The conjugate operator of ψπs satisfies the following identity (Lemma 1 of Jeon
et al. (2020))
Ω*(ψ∏ )= max h∏s,ψ∏ iA - Ω(∏s)
πs∈RA
= max h∏s, VΩ(πs)iA - <πs, VΩ(πs))zl +Ω(πs) - Ω(∏s)
πs∈RA	A
= min Ω(∏s) - Ω(πs) - hVΩ(πs),Πs - πsiA
Πs∈RA
=min Dω(∏s∣∣∏s)
πs ∈RA
for every state S ∈ S .By the property of Bregman divergence, and the convexity of Dω(∏s∣∏s)
with respect to ∏s (Geist et al., 2019; Acharyya et al., 2013), the optimal condition is obtained by
the unique maximizing argument π(∙∣s) = π(∙∣s). By taking gradient to both sides with respect to
ψ∏ We yield∏ = VΩ*(ψ∏).
If there is another ∏ = ∏ that makes ψ∏ = ψ∏, this contradicts the property of unique maximizing
arguments for conjugates. Therefore, ψ∏ is uniquely defined for each ∏ and VΩ*(ψ) ∈ ∆A.	□
We model the reward approximator regarding actions as ψ ∈ Ψω(ΔA) = {ψ | ψ(s, a)=
ψ∏(s,a), ∀s ∈ S, a ∈ Α,∏ ∈ ∆A}, where ψ∏ is defined ψ∏(s,a) := Ω0(s,a; π)一
Ea0〜∏(∙∣s) [Ω0(s, a0; π)] + Ω(π(∙∣s)). Note that Ψω(∏) is different from VΩ(π), but it is shifted
by the amount of -E°o〜冗(,怛)[Ω0(s, a0; ∏)] + Ω(∏) for every element of ψ∏(s, ∙). Therefore, the
operators Ψω and VΩ are link functions that form natural isomorphisms of functions.
13
Under review as a conference paper at ICLR 2022
A.2 Proof of Theorem 1
We consider the unique fixed point of ∏ as the solution of inf∏∈δs limt→∞ E[ft(∏, τt)] where the
expectation indicates that we take consideration for all τt and ft . By equating derivatives to zero, we
write VΩ(∏^) = limt→∞ E[VΩ(∏E,t)]). We provide more general results than previous imitation
learning, as some of following statements include the case of infπ∈∆S E[ft (π, τ)] > 0, which
means the estimates {∏E,t}∞=ι do not converge to the fixed point of ∏*, so limt→∞k∏* — ∏E,tk > 0.
Thus, MD-AIRL and other online MD algorithms allow realistic settings such as scarcity of data or
imperfect demonstrations.
We first introduce a key relationship regarding cumulative gradients in our online MD setting.
Lemma 2. Let {∏t}∞==ι, {∏E,t}∞=ι, and {ηt}∞=ι be policy, estimate, and step size sequences, re-
spectively. The subsequent policy πt+1 in Equation (5) is obtained by an RL algorithm using the
derivation of ψt+1 in Equation (7), resulting to the following equation:
∏t+ι(∙∣s) = argminηtDΩ(∏sk∏st) + (1 — ηt)DΩ(∏sk∏s),	∀s ∈ S.	(17)
πs ∈∆A
We have for t ∈ N,
ηt(VΩ(∏t) — VΩ(∏E,t)) = VΩ(∏t) — VΩ(∏t+ι)	(18)
where we write VΩ(π) = [VΩ(π(∙∣s))]§三5
Proof of Lemma 2. Since the optimization is convex with respect to each πs, we equate the deriva-
tives to 0 at πt+1 as
ηt(VΩ(∏S+ι) — VΩ(∏E,t)) + (1 — ηt)(VΩ(∏S+ι) — VΩ(∏S)) = 0, ∀s ∈ S.
Finally, we derive Equation (18) as
ηt(VΩ(∏S+ι) — VΩ(∏E,t)) + (1 — ηt)(VΩ(∏S+ι) — VΩ(∏S)) = 0
^⇒ vω(π+I)- nNC/E,J -(I- nt)Va(nS) = 0
0 VΩ(∏S) — VΩ(∏S+1) = ηt(VΩ(∏S) — VΩ(∏E,t)).
Therefore, the proof is complete.	□
Lemma 2 indicates that the distances between dual maps are equivalent to ηt ∣∣VΩ(∏E,J —VC(nS) k*
Therefore, if the step size decreases as limt→∞ η = 0, limt→∞∣∣VΩ(∏t) — VΩ(∏t+ι)k* = 0; thus,
our argument in Section 3 is reasonable when Ω is strongly smooth.
Next, we reintroduce the three-point identity as follows.
Lemma 3 (Three-point identity). Let πa,πb, and πc be any policies with a given state. We have the
following identity:
<VΩ(∏a) — VΩ(∏b),∏c — ∏b>A = Dω (∏ck∏b) — DΩ(∏ck∏a ) + Dω (∏bk∏a ).
Proof of Lemma 3. This can be derived using the definition of divergence as follows.
DΩ(∏ck∏b) — DΩ(∏ck∏a) + DΩ(∏bk∏a) = Ω(∏c) — Ω(∏b) — <VΩ(∏b),∏c — ∏b)/
— Ω(∏c) + Ω(∏a) + <VΩ(∏a), ∏ 一 ∏0%
+ Ω(∏b) — Ω(∏a) — <VΩ(∏a), ∏b — ∏a %
=<VΩ(∏a) — VΩ(∏b), ∏c — nb%.
Therefore, the proof is complete.	□
We now introduce basic identities regarding a Bregman divergence Lemmas 4 and 5 that are used to
address progress of the algorithm.
Lemma 4. Let πa,πb, and πc be any policies with a given state. The following identity holds.
DΩ(∏ck∏b) — DΩ(∏ck∏a) = DΩ(∏ak∏b) + <VΩ(∏°) — VΩ(∏b),∏c — ∏a)χ.	(19)
14
Under review as a conference paper at ICLR 2022
Proof of Lemma 4. By Lemma 3, we have
DΩ(∏ck∏b) — DΩ(∏ck∏a) = -DΩ(∏bk∏a) + gΩ(∏) - VΩ(∏b),∏c - ∏b)∕.
Utilizing the identity of two Bregman divergences
Dω(∏,∏) + Dω(∏,∏) = (VΩ(π) - VΩ(Π),π - n)A，	(20)
we separate πc - πb into πc - πa and πa - πb and write the rest of derivation as followings.
Dθ(∏ck∏b) - DΩ(∏ck∏a)
=-DΩ(∏bk∏a ) + hVΩ(∏a) - VΩ(∏b), ∏ 一 ∏b>∕ +<VΩ(∏a) - VΩ(∏b), ∏c - ∏aiA
X--------------------{--------------------}
Eq. (20)
=DΩ(∏ak∏b) + <VΩ(∏a) - VΩ(∏b),∏c - ∏a%
Therefore, We obtain the desired identity.	□
Lemma 5. Let πa,πb, and πc be any policies with a given state. The following identity holds.
DΩ(∏bk∏a) - Dω(∏c∣∣∏o) = -<VΩ(∏c) - VΩ(∏a),∏c - ∏b% + Do(∏b∣∣∏c).	(21)
Proof of Lemma 5. By Lemma 3, We have
DΩ(∏bk∏a) - DΩ(∏ck∏a) = -D。(∏ck∏b) + <VΩ(∏a) - VΩ(∏b),∏c - ∏b)A∙
We separate VΩ(∏a) - VΩ(∏b) into VΩ(∏a) - VΩ(∏c) and VΩ(∏c) - VΩ(∏b) and write the rest
of derivation as folloWings.
DΩ(∏bk∏a) - DΩ(∏ck∏a)
=-Do(∏ck∏b) + hVΩ(∏c) - VΩ(∏b),∏c - ∏bi∕ +hVΩ(∏a) - VΩ(∏b),∏c - ∏)/
V--------------------{--------------------}
Eq. (20)
=Do(∏bk∏c) + <VΩ(∏a) - VΩ(∏c),∏c - ∏b>A
Therefore, we obtain the desired identity.	□
We show the key arguments to prove Theorem 1 in the following lemma.
Lemma 6. Assume inf∏∈δs E[ft (π, T)] > 0. Assume also that Ω is ω-strongly convex and VΩ is
L-Lipschitz continuous for ω ≥ 0 and L ≥ 0. If limt→∞ Eτι,∙∙∙,τt [P∞=o YiD。(∏Si∣∣∏Ei) ] = 0 for
∏e, then {ηt}∞=ι satisfies Equation (9). Furthermore, if Ω is strongly smooth, then Theorem 1 (a)
holds with some constants n ∈ N and c > 0.
Proof of Lemma 6. First, we show the condition of limt→∞ ηt = 0. Assuming all states are reach-
able (FU et al. 2017, Definition B.1), the condition limt→∞ Eτι,∙∙∙,τt [P∞=o YiDω (∏Si ∣∣∏Ei)] = 0, NEW R3
implies limt→∞ Eτι,∙∙∙,τt [k∏t - ∏e ∣∣] = 0, where ∣∣∙k is a matrix norm induced by the p-norm on A.
Then, our aim is to show that the gradient of strong convex function for ∏ converges to VΩ(∏β) as
t→∞ Eτι,∙∙∙,τt [∣∣VΩ(∏t) - VΩ(∏e儿]=0.	(22)
To prove this point, we use the continuity of VΩ at ∏; for any ε > 0, there exists some 0 < δ ≤ 1
such that ∣VΩ(∏) - VΩ(∏e)∣* < ε whenever ∣∣∏ - ∏ ∣∣ < δ.
When ∣π - πE ∣ ≥ δ, we apply the L-Lipschitz continuity assumption to find
∣∣VΩ(π) -VΩ(πb)∣∣* ≤ Lkn - ∏ek,	(23)
where ∣∣∙∣∣* is a matrix norm induced by the q-norm. Combining Equation (22) and Equation (23),
we know that
Eτι,…,τt [∣VΩ(∏t) - VΩ(∏e)∣∣*] ≤ ε + L ∙ E^…⑪[∣∏t - ∏e∣] ∙	(24)
However, limt→∞ E九…⑪[∣∣∏e - ∏t∣∣] = 0 ensures the existence of some n ∈ N such that for
t > n, there holds Eq,…⑪[∣∣∏e - ∏t∣∣] < ε∕L. Applying this inequality to Equation (24), we have
Eτι,…,τt [∣∣VΩ(∏t) - VΩ(∏e)∣*] < 2ε for some t > n.
15
Under review as a conference paper at ICLR 2022
For temporal estimation, let us denote the infimum of the expectation ' = inf∏∈δs E[VΩ(∏t) 一
VΩ(∏E,t)] > 0. From Lemma 2, we have ηt(VΩ(∏t) 一 VΩ(∏E,t)) = VΩ(∏t) 一 VΩ(∏t+ι). Taking
the expectations on both sides of this equation yields
ηt' ≤ ηtEτι,…,Tt+1 [∣VΩ(∏t+ι) - VΩ(∏E,t)k*] = Eτι,…,Tt+1 [∣VΩ(∏t) - VΩ(∏t+ι)∣∣*].
Hence the convergence of the point VΩ(∏t) is confirmed by taking the limit: limt→∞ η = 0.
Next, we show P∞=ι η = ∞. By the ω-strong convexity by the L-Lipschitz continuity of Ω, we
can find inequalities as
2L
(VΩ(π) - VΩ(π),π 一 π% ≤ Lkn — ∏k2 ≤ "Dω(∏∣∣∏).
(25)
We note that k∏t+ι 一 ∏E,tk ≤ IInt — ∏E,tk so that there is a constant ε that satisfies E[∣∏t+ι 一
∏E,t+ιk] ≥ E[∣∏t+ι - ∏E,t∣]+ ε. Therefore, taking expectations in Equation (21) (setting ∏a = ∏E,t,
∏b = ∏t+ι, and ∏ = ∏t), for limt→∞ η = 0 and the strongly convex Ω, we can find
ETι,∙∙∙,Tt+ι [DΩ(nt+1knE,t+1)] ≥ ETι,∙∙∙,Tt+ι [DΩ(nt+1knE,t)] + ε0
≥ (I- aηt)ETι,∙∙∙,Tt [DΩ(nt ∣∣nE,t)] + ETι,∙∙∙,Tt+ι [DΩ(nt+1knt)] + ε0 (Eq. (21))
≥ (1 - aηt)ETi,…,Tt [Dc(∏tk∏E,t)] + ε00	(26)
for some t and 0 < ε0 < ε00. The positive constant a = 2L∕ω is derived by the inequalities in
Equation (25). We omit the conditioned states (superscripts) for simplicity of the derivation.
Since limt→∞ ηt = 0, we can also find a constant n ∈ N such that ηt ≤ (3a)-1 fort ≥ n. Applying
the inequality 1 - x > exp(-2x) for x ∈ (0, 1∕3], we derive another inequality
Eti,…,Tt+1 [DΩ(∏t+ι∣∣∏E,t+ι)] ≥ exp(-2aηt)ETi,…,Tt [DΩ(∏t∣∣∏E,t)],	∀t ≥ n
Applying this for t = T - 1, . . . , n yields
T
Eti,…,TT [Dω (∏T ∣∣∏E,T)] ≥ ɪɪ exp(-2aηt) E^,…,Tn [DΩ(∏n∣∣∏E,n)]
t=n+1
exp -2a •£ η E^,…,公[DΩ(∏n∣∣∏E,n)] ∙
(27)
(28)
t=n+1
Using Equation (28), We conclude E^,…,τn [Dα(∏n+ιk∏E,n+ι)] > 0. Otherwise, We have
Eτι,…,Tn [DΩ(πnkπE,n)] = Eτι,∙∙∙,Tn+ι [DΩ(πn+1 kπE,n+1)] = 0
according to Equation (27), which leads to E^,…,丁九[k∏n - ∏E,nk2] = Eτι,∙∙∙,τn+ι [k∏n+ι -
∏E,n+ιk2] = 0. This implies ∏n = ∏E,n = ∏n+ιalmost surely, leading to E[ft(∏t,τt)]=
0. This is a contradiction to the previous assumption infπ∈∆S E[ft (π, τt)] > 0, thus
Eτι,…,τn+ι [DΩ(∏n+ιk∏E,n+ι)] > 0. Let US suppose the ideal case that the estimation process
learns the exact ∏ in t → ∞. In order to satisfy the IimitlimT→∞ E^,…,ττ [Dω(∏t ∣∣∏e,t)] = 0 we
see from Equation (28) that Pt∞=1 ηt = ∞.
Now, we show that Theorem 1 (a) holds. Since Ω is ω-strongly convex, so Ω* is (ω-1)-strongly
smooth with respect to ∣∣∙∣∣*. Additionally, the L-Lipschitz continuity of VΩ implies L-strong
smoothness of Ω; thus, naturally, Ω* is L-strongly convex.
Combining these, for ∀t ≥ n, the condition ηt ≤ (3a)-1 induces
ET1,…,Tt+1 [DΩ(nt+1kπE,t+1)] ≥ (I- aηt)Eτι,∙∙∙,τt [D(πtkπE,t)]
+ (2L)TETL,Tt+1 [kVΩ(nt) - VΩ(∏t+ι)k2],
and by Lemma 2, we get
Eτι,∙∙∙,τt+1 [DΩ(πt+1kπE,t+1)] ≥ (I- aηt)Eτι,-,τt [DΩ(πtkπE,t)]
+ (2L)-1ηtEτι,∙∙∙,τt+ι [kVΩ(∏t) - VΩ(∏E,t)∣k2].
16
Under review as a conference paper at ICLR 2022
Using the Cauchy-Schwarz inequality, we obtain a lower bound of the last term as
Eτι,∙∙∙,τt [kVΩ(∏t) - VΩ(∏E,t)∣∣2] ≥ {Eτ.,τt [kVΩ(∏t) - VΩ(∏E,t)∣∣J}2 ≥ '2.
Thus, we obtain the final inequality as
Eτι,…,τt+ι [DΩ(∏t+ιk∏E,t+ι)] ≥ (1 - aηt)Eτι,…,τt [DΩ(∏tk∏E,t)] + (2L)-1(ηt')2,	∀t ≥ n.
Applying this inequality from t = T ≥ n + 1 to t = n + 1, we obtain
T
Eτι,…,ττ+1 [DΩ (πτ +1 kπE,τ +1)] ≥ Eτι,…,Tn [DQ(nn kπE,n)] ∏ (1 - aηt)
t=n+1
TT
+ (2L)-1'2 X η Y (1 - aηk)
t=n+1	k=t+1
TT
≥ (2L)-1'2 X n Y (1 - aηk).
t=n+1	k=t+1
By the Cauchy-Schwarz inequality and our bound 0 < 1 - aηk ≤ 1 for k ≥ n, we have
)1/2(T-n)1/2.
t=n+1	k=t+1
TT	TT
X ηt Y (1 - aηk) ≤ X ηt2 Y (1 - aηk)
t=n+1 k=t+1
Hence
TT
X ηt2 Y
t=n+1 k=t+1
(1 - aηk) ≥
1
a2(T - n)
1
a2(T - n)
TT	2
aηt	(1 - aηk)
t=n+1	k=t+1
T	T2
X (1 - (1 - aηt)) Y (1 - aηk)
1
a2(T - n)
t=n+1
T
、	1
≥ a2(T - n)
、	1
≥ a2(T - n)
Therefore, we obtain the lower bound of
t=n+1
T
k=t+1
T
(1-
k=t+1
aηk) - Y(1 - aηk)
2
1 -	(1 - aηk)
t=n+1	k=t
(I-(I-…=Tη⅛
Eτι,∙∙∙,ττ [Dω(∏t+lk∏E,T+l)] ≥
ηn+ι(2L)-1 '2
T-n
T
Since the Bregman divergence is bounded for all states, the sequence {γiDΩ(∏Sik∏Si)} will Con-
verge as i → ∞. Applying the monotone convergence theorem, we can interchange expectation and
summation, which yields
∞
∞
Eτι,∙∙∙,ττ
Xy'Dω(∏t(∙∣Si)∣∣∏E,τ(∙∣Si)) = XEτι,∙∙∙,ττ [γiDΩ(∏τ(∙∣Si)k∏E,τ(∙∣Si))]
i=0
i=0
∞
=X γiEτ …t[Dω(∏t (∙∣Si)k∏E,τ (∙∣Si))]
i=0
ηn+ι(2L - 2Lγ)-1'2
≥ 一工------------------,	∀T ≥ n.
T-n
This verifies Theorem 1 (a) with the constant C = η2+ι(2L 一 2Lγ)-1'2.
□
17
Under review as a conference paper at ICLR 2022
Last, We show convergence to a unique fixed point of ∏ using the particular form of η in EqUa-
tion (9).
Lemma 7. If {ηt}∞=ι satisfies Eq. (9), limt→∞ Eτι,∙∙∙,τt [P∞=o γiDΩ(∏Sk∏S)] = 0∙ Furthermore,
ifthe Step size takes the form η =占,then Eτι,∙∙∙,ττ [P∞=o Y iDΩi{∏Si ∣∣∏Ti)] = O(1/T).
ProofofLemma 7. According to Lemma 4 and the Bregman divergence of the conjugate Dω* , the
one step progress regarding ∏E,t can be written as
DΩ(∏*k∏t+ι) - DΩ(∏*k∏t) = ^VΩ(∏t) - Ω(∏t+ι),π* - ∏∖ + Dn(∏t∣∣∏t+ι)
=ηthVΩ(∏t) - VΩ(∏E,jπ* - ∏ti + Dω* (VΩ(∏t+ι )∣∣VΩ(∏t)),
where we omit a given state. As ω-strong convexity of Ω implies the (ω-1)-strong smoothness of
Ω*, we have
1	η2
Dω* (VΩ(∏t)kVΩ(∏t+ι)) ≤ — ∣∣VΩ(∏t) - VΩ(∏t+ι)^ = *∣∣VΩ(∏t) - VΩ(∏E,t)^ (29)
2ω	2ω
Webound ∣∣VΩg)-VΩ(∏E,t)∣∣J2 by 2∣∣VΩ(∏t)-VΩ(∏*)∣∣2 +2∣∣VΩ(∏*)-VΩ(∏E,t)∣H,following
the work of Lei & Zhou (2020). Since VΩ is cocoercive with L bye the Lipschitz continuity, we
obtain
∣∣VΩ(∏t) - VΩ(π*)∣∣2 ≤ L(VΩ(π*) - VΩ(∏t), π* - ∏t)
thus
DΩ(∏*k∏t+ι) - DΩ(∏*k∏t) ≤ ηt<VΩ(π*) - VΩ(∏E,t),π* - ∏ti
-(I - ——)ηt"ω(πJ - vω(π∕ π* - πti + — [∣∣vω(πJ - vω(πe,/k2.
ωω
By taking expectation, it follows that there exists n ∈ N such that η ≤ 2L for t ≥ n holds
Eτt [Dω(∏≠ k∏t+ι)] ≤ Dω (∏*k∏t) - nDΩ(∏*k∏t)+ zηt2,
(30)
(31)
where Z is the constant Z = 1 E[∣VΩ(∏*) - Ω(∏E,t)∣∣c2]. Let {At}∞=ι denote a sequence of At
Eq,…,τt [DΩ(∏*k∏t)]∙ Then we have
At+1 ≤
1 - η)4 + zη2,
∀t ≥ n.
(32)
For a constant h > 0, we claim that At1 < h for some t1 > n0 . Assume that this is not true, and we
find some t2 ≥ t1 such that At > h, ∀t ≥ t2 . Since limt→∞ ηt = 0, there are some t > t3 > t2
that ηt ≤ 4hb. However, Equation (32) tells us that for t ≥ t3,
At+ι ≤ (1 - ^2) At	+ zηt	≤	At3	- 4	^X	ηk	→	-∞	(as t → ∞).
k=t0γ
This is a contradiction, which verifies At < h for t > n0 . Since limt→∞ ηt = 0, we can find some
ηt that makes At to be monotone decreasing. Then, we can conclude that the nonnegative sequence
{At }t∞=1 converges by iteratively applying the upper bounds.
We now prove Theorem 1 (b) under the condition and the choice η = * of the step size sequence.
t+1
The estimate becomes
2	16Z
At+1 ≤ (1-t-+i)At + (TΓ>2 ∀t ≥ n.
It follows the recurrence relation as
t(t + 1)At+1 ≤ (t - 1)tAt + 16Z, ∀t ≥ n.
Applying this relation iteratively, we obtain the general form of inequality.
(T - 1)T AT ≤ (n - 1)nAn + 16Z(T - n), ∀T ≥ n,
therefore we get the inequality as follows:
Eτι,∙∙∙,ττ [DΩ(∏*k∏τ)] ≤ (n - 1)nE(T-I)DQgknn)] +竽 T ≥ n.
By applying the monotone convergence theorem similar to Lemma 6, we conclude that
Eτι,…,ττ[P∞ YiDΩ(∏*(∙∣Si)k∏τ(∙∣Si))]= O(1∕T).	□
18
Under review as a conference paper at ICLR 2022
A.3 Proof of Theorem 2
Necessity. We rewrite the inequality in Equation (26) as
Eτι,∙∙∙,τt+1 [DΩ(∏t+ι∣∣∏E,t+ι)] ≥ (1- 2Lω-1ηt)Eτι,…,τt[DΩ(∏t∣∣∏E,t)].	(33)
Since we assume that ηt converge to 0 from previous arguments, onsider the step size sequence FIX R3
0 < ηt ≤ (2+K)L for κ > 0 and t ≥ n where n ∈ N. Denote a constant a = 2++κ log 2+Kκ and apply
the elementary inequality
1 — X ≥ exp(-ax),
∀0 < x ≤
2
2 + K
From Equation (33), it can be obtained that
Eτι,…,τt+ι [Dω(∏t+1∣∣∏E,t+1)] ≥ exp(-2aLω-1ηt)Eτ1,…,τt [Do(∏tk∏E,t)].
Applying this inequality iteratively for t = n, . . . , T - 1, then gives
T-1
ETn,…,Tτ [dω(πT IInE,T )] ≥	exp (-2αLω-1ηt) DΩ(∏nk∏E,n)
t=n
exp
-2aLω-1
T-1
X ηt
t=n
Dω (∏nk∏E,n)∙
From the assumption ∏E	=	∏n, We have DΩ(∏nk∏E,n)	> 0. The convergence
limt→∞ Eq,…,τt [DΩ(∏tk∏E,t)] = 0 then implies p∞=ι ηt = ∞.
Sufficiency. Here we use the estimate (32) derived in the proof of Lemma 7. However, in the
optimal case, Z = ωE[ft(∏*,τ)] = 0, so (32) takes the form (we can choose n = 1 by Equation (31)
At+1 ≤ η2tAt,	Vt ∈ N.	(34)
This implies that for any 0 < h, there must exist some integer t1 ∈ N such that At ≤ γ for t ≥ t1.
Otherwise, At > γ for every t ≥ t2 with t2 ≥ t1, which leads to a contradiction:
ht
At+ι ≤ At2 — 2 ɪ2 ηk → -∞(as t → ∞).
k=tγ
Equation (34) also tells us that the sequence {At}t∞=1 of nonnegative numbers is monotone decreas-
ing. Hence At ≤ h for every t ≥ t1 , which proves the convergence of At
lim Eτι,…,τt [DΩ(∏*k∏t)] = lim At = 0.
t→∞	t→∞
We now prove the second point in Theorem 2 which is under the special choice of constant step size
sequence ηt ≡ η1. It follows from Equation (33) that AT ≥ (1 — 2Lω-1η1)T-1A1. Hence, Eq (34)
translates to
At+1 ≤ (1 — ηι∕2)At,
from which we find AT ≤ (1 — ηι∕2)T-1Αι by iteration starting from t = 1. This verifies the
theorem with ci = (1 — 2Lωηι) and c? = (1 —号).
A.4 Proof of Proposition 1
The proof is based on Doob’s forward convergence theorem.
Theorem 3 (Doob’s forward convergence theorem). Let {Xt}t∈N be a sequence of nonnegative
random variables and let {Ft}t∈N be a filtration with Ft ⊂ Ft+1 for every t ∈ N. Assume that
E Xt+1 |Ft ≤ Xt almost surely for every t ∈ N. Then the sequence {Xt} converges to a nonnega-
tive random variable X∞ almost surely.
19
Under review as a conference paper at ICLR 2022
We follow the proof of Lemma 7 and apply Eq. (30). Since (∏* - ∏t, VΩ(∏≠) - VΩ(∏t)i ≥ 0,
Equation (30) implies
2
Eτt[Dc(∏*k∏t+ι)] ≤ Dc(∏*k∏t) + — E” [kVΩ(π*) - VΩ(∏E,t)∣E],	∀t ≥ n. (35)
ω
The condition Pt∞=1 ηt2 < ∞ enables us to define a stochastic process {Xt } by
Xt
∞
DΩ(π*kπt+ι) + ωE[kva(n*)-Va(nE,t)∣∣2] X η2.
ω	i=t+1
By (35), we know that Eτt [Xt+1] ≤ Xt for t ≥ n. Additionally, Xt ≥ 0. Therefore, the stochastic
process {Xt}t-n+1≥1 is a submartingale (equivalently, {-Xt}t-n+1≥1 is a supermartingale). Then
by Lemma 3, the sequence {Xt}t≥1 converges to a nonnegative random variable X∞ almost surely.
According to Fatou's Lemma and the convergence limt→∞ E[P∞=0 γiDΩ(∏*k∏t)] = 0 for any
states a proved by Lemma 7, we obtain
NEW
∞
EX] = E ⅛lim X γiDΩ(∏*(∙∣Si)∣∣∏t(∙∣Si)) ≤ (1 - Y)T liminf E[DΩ(∏*k∏t)] = 0.
→∞ i=0	→∞
It follows that the sequence of discounted sum {P∞=0 Yi Dω(∏* (∙∣Si)k∏t(∙∣Si)) }t∈N converges to 0
almost surely.
B Tsallis Entropy and Associated B regman Divergence among
Full Covariance Multivariate Gaussian Distributions
In this section, we briefly reintroduce the tractable method to derive Bregman divergences and reg-
ularized reward functions proposed by Nielsen & Nock (2011) and Jeon et al. (2020). Then, we
identify specific paremeterization to model Gaussian distributions with full covariance matrices.
The standard form of the exponential family is represented as
exp{<θ,t(x)) — F (θ) + k(x)}.	(36)
The parameterization is as follows:
Σ-1μ	_
1 ς-" = [θ2
t(x)
x
F (θ) = - 4 θTθ-1 θι + 2in∣-∏θ-1l = 2 μτ ∑-1μ + ∣in(2∏)d∣∑∣,
k(x) = 0,
where we can analytically recover the multivariate Gaussian distribution (Nielsen & Nock, 2011)
expθ, t(x) - F(θ) + k(x)}
exp< μτΣ-1x —
2 tr(∑ IxxT) — 2μTΣ 1 μ + 2 ln(2π)d∣∑∣
(2π)d∕2∣∑∣ι∕2 exp{〃T*Tx - 2xTςTx - 2〃TgT〃}
(2∏)d∕2∣∑∣1∕2 eχp{2(x - μ)T* I(X - μ) }.
(37)
(38)
(39)
(40)
For two distributions π and π with k(x) = 0, Nielsen & Nock (2011) proposed the function I(∙):
I(∏,∏; α,β)
J π(x)απ(x)β dx = eχp{F (αθ + βθ) — αF(θ) — βF(θ)}
20
Under review as a conference paper at ICLR 2022
where the detailed derivation is as follows:
""W dx
=J exp{α(θ,t(x)) — αF(θ)+ β<θ,t(x)) — βF(θ)} dx
=/ exp{<αθ + βθ,t(x)) - F (αθ + βθ)} exp nF (αθ + βθ) - αF (θ) - βF(θ)} dx
expFF(αθ + βθ) - αF(θ) - βF(θ)}
exp{f(αθ + βθ) - αF(θ) - βF(θ)}.
exp
+ βθ,t(x)) - F(αθ + βθ)} dx
B.1 Tsallis entropy of full covariance Gaussian distributions
For 夕(x; q) = q--i(χq-1 一 1), the Tsallis entropy can be written as
1 - π(x)q-1
Tq(∏) := -Ex〜∏0(x; q) = n(x)---------dx
q-1
1 - π(x)q dx
q-1
q-1 (1- I (π,π;q,O))
1 一 exp(F(qθ) 一 qF(θ))
q-1
If π is a multivariate Gaussian distribution, we have
F(qθ) = 2μτΣ-1μ + | ln(2π)d∣∑∣- 2 ln qd.
Since the covariance matrix is positive-definite, Cholesky decomposition can be applied, which sep-
arates the matrix to lower- and upper-triangle matrices. Likewise, we can apply LDL decomposition.
Let us factorize the covariance Σ = L diag{σ12, . . . , σd2}LT where L denotes a unit lower triangular
matrix produced by LDL decomposition. Then we have
F(qθ) - qF(θ) = (I - q){ 2 ln2π + 2 lnN| - 2(1 -qq) }
d	1 d 2 d ln q
=(1 -q)∣2ln2π + 2ln Yσi- 2(T-^ʃ
d	ln 2π	ln q
= (1-q) X{ 丁+ ln σi-2(T-^) P
B.2 TRACTABLE FORM OF ψπ
For separable Ω, ψ∏ is written as (Jeon et al., 2020)
ψ∏(s, a) = -f0(s, a) + Ea〜∏[f0(π(a∣s))-夕(a|s)]
where 夕(x) = q—ι(1 - Xq-I) and accordingly f (x) = X夕(x). For the gradient of f (∙), We have
f0(x) = —k-j-(1 - qxqT)
q-1
k
=——τ(q - qxq-1 - (q - 1))
q-1
=上(1-XqT)- k
q-1
=q^(x) — k.
21
Under review as a conference paper at ICLR 2022
Taking the expectation yields Tsallis entropy as follows.
Ex〜∏[-f0(χ; ∏) + 夕(χ)] = Ex〜∏[k - qφ(χ) + 夕(χ)] = (1 - q)Tqk(∏) + k
For a multivariate Gaussian distribution ∏, the tractable form of Ex〜∏ [-f0(χ) + 夕(χ)] can be
derived by using that of Tsallis entropy Tqk(π) ofπ. Thus ψπ can be rewritten as
ψ∏(s, a) = qψ(s) + (q - 1)Tqk(π)
In the special case of q = 1 and k = 1,we have ψ∏(s, a) = log∏(α∣s).
B.3 Bregman Divergence with Tsallis Entropy Regularization
We consider the following form of the Bregman divergence:
J ∏(x){f 0(∏(x)) — ω(π(x))} dx — J ∏(x){f0(∏(x)) — ω(∏(x))} dx
For ω(x) = q-i(1 — xq-1), f0(x) = q-ι(1 — qxq-1) = qω(x) — k, and k = 1, the above form is
equal to
Z ∏(x) 1qπ(χ)—— dx - Tq(∏) - (q - 1)Tq(∏) + 1
q-1
=-ɪr------q—r ∏ ∏(x)∏(x)q-1 dx - Tq(∏) - (q - 1)Tq(∏) + 1
q-1 q-1
=-ɪf------∖ ∏∏(x)Π(x)q-1 dx - Tq(∏) - (q - 1)Tq(∏).
q-1 q-1
Let us define two multivariate Gaussians as follows:
π(x) = N(x; μ, Σ),μ = [μι,…，μd]T, Σ = Ldiag(σ2,…，σ2)LT,
∏(x) = N (x; μ, Σ),μ= [μι, ∙ ∙ ∙ ,μd]T, ∑ = Ldiag(σ2, ∙∙∙ ,σ2)LT,
where L and L denote unit lower triangular matrix. We have
J π(x)∏(x)q-1 dx = I(π, π; 1, q — 1) = exp {F(θ0) — F(θ) — (q — 1)F(θ)},
where
θ
ʌ
θ
一 Σ-1μ ,
1 ∑-1
L- 2 ς
I- ʌ
Σ-1μ
1∑-1
L- 2 ς
and
. . ʌ
θ0 = θ + (q - 1)θ
Σ-1μ + (q — 1)Σ-1 μ
-2 (夕-1 + (q - dςT)
θ10
θ20
F (θ) = 1 μτ ∑-1
11
μ +Rln(2∏)d∣∑∣ = 2(μ)T∑Tμ + £
ln 2π
+ ln σi,
i=1
d
2
F(θ) = 2μ∑-1μ + 2 ln(2∏)d∣∑| = 1(μ)τ∑-1μ + XX
i=1
ln 2π
+ ln σi,
2
F(θ +(q - 1)θ) = - 1(θ1 )T(θ2)-1(θ1) + 2 ln∣-∏(θ2)T|
We can replace some difficult computations using LDL decomposition such as Σ-1
L-1 diag(1∕σ)(L-1)T and ∣Σ∣ = Pd=Ilnσi.
22
Under review as a conference paper at ICLR 2022
C Implementation Details
Unnormalized rewards of the IRL algorithm often mislead the agent to take unnecessary awareness
of termination in finite-horizon MDPs (Kostrikov et al., 2018). To solve the issue IRL algorithms
need to remove the difference between regarding steps depending on MDP’s time. Doob’s optimal
stopping theorem formally states that the expected value of a martingale at a stopping time is equal
to its initial expectation. Assume a martingale makes the entire procedure as a fair game on average,
which means nothing can be gained by stopping the play.
Theorem 4 (Doob’s optimal stopping theorem). Let a process {Xt}t∞=1 be a martingale and τ be a
stopping time with respect to filtration {Ft}t≥1. Assume that one of the conditions holds:
(a) τ is almost surely bounded, i.e., there exists a constant c ∈ N such that τ ≤ c.
(b)	τ has finite expectation and the conditional expectations of the absolute value of the martingale
increments almost surely bounded, more precisely, E[τ] < ∞ and there exists a constant c such
that E |Xt+1 - Xt | Ft ≤ c almost surely on the event {τ > t} for all t ≥ 0.
(c)	There exists a constant C such that ∣Xmin{t,τ} | ≤ C almost surely for all t ≥ 0. Then XT is an
almost surely well-defined random variable and E[Xτ] = E[X0].
Then X∞ is integrable and E[X∞] = E[X0]
Doob’s optimal stopping theorem states one of the necessary conditions of IRL reward of normaliz-
ing the reward measures and making them as a martingale even for finite-horizon benchmarks.
C.1 Network Architectures
For all networks, we use networks with 2-layer MLP with 100 hidden units, respectively. We con-
Sider the reward model with two separate neural networks (ψφ, b(∙)) for the proposed reward func-
tion for λ > 0:
rφ(s, a) = ψφλ(s, a) = λψφ(s, a) + b(s),
Motivated by RAIRL-DBM, we consider the reward models in Figure 9. The model outputs reward
for proximal updates trained by mirror descent and state-only discriminator network. Discriminating
state visitation by b(∙) is required because the reward function needs to consider every state (espe-
cially the state that cannot be visited by πE) until DKL(ρπ kρπE ) ≈ 0. Figure 9 (a) shows logits of
the softmax distribution involved when calculating rewards when the action space is discrete. For
continuous control (Figure 9 (b)), the architecture is similar, where the mean and covariance are
used to compute a reward for a certain action.
rφ(s, ∙)	rφ(s,a)
Figure 9: Schematic illustrations of MD-AIRL reward models for discrete (left) and continuous
control (right)
C.2 Multi-goal environment
Let the 2D coordinate denote the position of a point mass on the environment. In the multi-goal
environment, the agent is initially located according to the normal distribution N (0, (0.1)2I). The
four goals are located at (6, 0), (-6, 0), (0, 6), and (0, -6), where the agent can move a maximum of
23
Under review as a conference paper at ICLR 2022
1 unit per timestep for each coordinate. The ground-truth reward is given by the difference between
successive values of a Gaussian mixture depicted as Figure 10.
Figure 10: Visualization of the multi-goal environment.
We used the full covariance Gaussian distribution in this experiment (as well as the conceptual ex-
periment in Figure 2. Note that the covariance matrix is positive-definite and symmetric. To achieve
numerically stable computation, we applied LDL decomposition to covariance matrix involving unit
lower- and upper-triangle matrices, and a diagonal matrix. As a result, the policy network out-
puts a vector (μ(s)T, σ(s)T,l(S)T)T where the additional part I(S) denotes d(d-1) entries of Unit
lower triangular matrix. Denote L(s) as a unit lower triangular matrix from l(s). For example, the
covariance matrix can be reconstructed by
Σ(S) = L(S)[diag(σ(S))]L(S)T.
Additionally, the action samples can be calculated by
a = μ(s) + L(s)(σ(s) ∙ z) Z 〜N(0,I)
Computing inverses, determinants and multiplications with unit triangular matrices and diagonal
matrices can be efficiently performed by numerical libraries. Therefore, we can fully model the
Bregman divergence and reward using neural networks as provided in Appendix B.
C.3 MuJoCo Experiments
Instead of directly using squashed policies proposed in SAC (Haarnoja et al., 2018), we assume
the application of tanh as a part of the environment (known as hyperbolized environments of
RAIRL (Jeon et al., 2020)). Specifically, after an action a is sampled from the policies, we pass
tanh(a∕1.01) * 1.01 to the environment. We additionally clip the hyperbolized actions to 1, if the
environment is not tolerant to the excessive values of action. Therefore, we can consider the standard
diagonal Gaussian policy
π(∙∣s)= N(μ(s), Σ(s))
where μ(s) denotes means [μι(s),μι(s),... ,μd(s)]T and σ(s) denotes standard variance
[σ1(S), σ2(S), . . . , σd(S)]T. We soft-clip the standard deviation as σi(S) ∈ [ln 0.01, ln 2] for the
stability using tanh. Last, we update the moving mean of intermediate values of regularized reward
r(s, a) + λ夕(π(a∣s)) and update the RL algorithm with mean-subtracted rewards. In addition to the
analyses of (Fu et al., 2017; Jeon et al., 2020) regarding reward shaping and normalization, mean-
zero rewards for training agents have the additional property of preventing termination awareness,
as stated by the optimal stopping theorem (Theorem 4).
24
Under review as a conference paper at ICLR 2022
C.4 Hyperparameters
Tables 2, 3, and 4 show the hyperparameters of the conducted experiments.
Table 2: Hyperparameters for bandit environments.
Parameter	Value
Learning rate (policy)	1∙10-3
Learning rate (reward)	1∙10-3
α1	0.5
αT	5
λ	1
Discount factor (γ)	0.0
Batch size	16
Steps per update	50
Total steps	300,000
Table 3: Hyperparameters for Mmlti-goal environment.
Parameter	Value
Learning rate (policy)	5∙10-4
Learning rate (reward)	5∙10-4
Replay size	10,000
α1	1
αT	10
λ	1
Discount factor (γ)	0.5
Batch size	512
Steps per update	50
Total steps	300,000
Table 4: Hyperparameters for MuJoCo environments.
Parameter	Value
Learning rate (policy)	3∙10-4
Learning rate (reward)	3∙10-4
Replay size	1,000,000
α1	1.0
αT	20.0
λ	0.01
Discount factor (γ)	0.99
Batch size	256
Steps per update	5
Initial exploration	10,000
Total steps	1,000,000
25