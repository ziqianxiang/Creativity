Under review as a conference paper at ICLR 2022
Universal Joint Approximation of Manifolds
and Densities by Simple Injective Flows
Anonymous authors
Paper under double-blind review
Ab stract
We analyze neural networks composed of bijective flows and injective expansive
elements. We find that such networks universally approximate a large class of
manifolds simultaneously with densities supported on them. Among others, our
results apply to the well-known coupling and autoregressive flows. We build on
the work of Teshima et al. 2020 on bijective flows and study injective architectures
proposed in Brehmer et al. 2020 and Kothari et al. 2021. Our results leverage a
new theoretical device called the embedding gap, which measures how far one
continuous manifold is from embedding another. We relate the embedding gap
to a relaxation of universally we call the manifold embedding property, capturing
the geometric part of universality. Our proof also establishes that optimality of a
network can be established “in reverse,” resolving a conjecture made in Brehmer
et al. 2020 and opening the door for simple layer-wise training schemes. Finally,
we show that the studied networks admit an exact layer-wise projection result,
Bayesian uncertainty quantification, and black-box recovery of network weights.
1 Introduction
In the past several years, invertible flow networks emerged as powerful deep learning models to
learn maps between distributions (Durkan et al., 2019a; Grathwohl et al., 2018; Huang et al., 2018;
Jaini et al., 2019; Kingma et al., 2016; Kingma & Dhariwal, 2018; Kobyzev et al., 2020; Kruse et al.,
2019; Papamakarios et al., 2019). They generate excellent samples (Kingma & Dhariwal, 2018) and
facilitate solving scientific inference problems (Brehmer & Cranmer, 2020; Kruse et al., 2021).
By design, invertible flows are bijective and, hence, may not be a natural choice when the target
by combining bijective
hmer & Cranmer, 2020;
distribution has low-dimensional support. This problem can be overcome
flows with expansive, injective layers, which map to higher dimensions (Bre
Cunningham et al., 2020; Kothari et al., 2021). Despite their empirical success, the theoretical
aspects of such globally injective architectures are not well understood.
In this work, we present approximation-theoretic properties of injective flows whose architecture
combines bijective flows and expansive, injective layers, with an emphasis on approximating mea-
sures with low-dimensional support. We state conditions under which these networks are universal
approximators and describe how their design enables applications to inference and inverse problems.
1.1	Prior Work
The idea to combine invertible (coupling) layers with expansive layers has been explored by
Brehmer & Cranmer (2020) and Kothari et al. (2021). Brehmer & Cranmer (2020) combine two
flow networks with a simple expansive element (in the sense made precise in Section 2.1) and ob-
tain a network that paramterizes probability distributions supported on manifolds. They suggest that
such constructions may be universal but neither they not Kothari et al. (2021) derive theoretical re-
sults. We discuss in detail the connection between these two empirical works and the approximation
results derived here in Appendix B.1.
Kothari et al. (2021) propose expansive coupling layers and build networks similar to that of
Brehmer & Cranmer (2020) but with an arbitrary number of expressive and expansive elements.
They observe that the resulting network trains much faster with a smaller memory footprint, while
producing high-quality samples on a variety of benchmark datasets.
1
Under review as a conference paper at ICLR 2022
While to the best of our knowledge, there are no approximation-theoretic results for injective flows,
there exists a body of work on universality of invertible flows; see Kobyzev et al. (2020) for an
overview. Several works show that certain bijective flow architectures are distributionally universal.
This was proved for autoregressive flows with sigmoidal activations by Huang et al. (2018) and for
sum-of-squares polynomial flows by Jaini et al. (2019). Teshima et al. (2020) show that several
flow networks including those from Huang et al. (2018) and Jaini et al. (2019) are also universal
approximators of diffeomorphisms.
The injective flows considered here have key applications in inference and inverse problems; for an
overview of deep learning approaches to inverse problems, see Arridge et al. (2019). Bora et al.
(2017) proposed to regularize compressed sensing problems by constraining the recovery to the
range of (pre-trained) generative models. Injective flows with efficient inverses as generative mod-
els gives an algorithmic projection1 on the range, which facilitates implementation of reconstruction
algorithms. An alternative approach is Bayesian, where flows are used to obtain tractable varia-
tional approximations of posterior distributions over parameters of interest, via supervised training
on labeled input-output data pairs. Ardizzone et al. (2018) encode the dimension-reducing forward
process by an invertible neural network (INN), with additional outputs used to encode posterior
variability. Invertibility guarantees that a model of the inverse process is learned implicitly. For a
given measurement, the inverse pass of the INN approximates the posterior over parameters. Sun &
Bouman (2020) propose variational approximations of the posterior using an untrained deep genera-
tive model. They train a normalizing flow which produces samples from the posterior, with the prior
and the noise model given implicitly by the regularized misfit functional. In Kothari et al. (2021) this
procedure is adapted to priors specified by injective flows which yields significant improvements in
computational efficiency.
1.2	Our Contribution
We derive new approximation results for neural networks composed of bijective flows and injec-
tive expansive layers, including those introduced by Brehmer & Cranmer (2020) and Kothari et al.
(2021). We show that these networks universally jointly approximate a large class of manifolds and
densities supported on them.
We build on the results of Teshima et al. (2020) and develop a new theoretical device which we refer
to as the embedding gap. This gap is a measure of how nearly a mapping from Ro → Rm embeds an
n-dimensional manifold in Rm , where n ≤ o. We find a natural relationship between the embedding
gap and the problem of approximating probability measures with low-dimensional support.
We then relate the embedding gap to a relaxation of universality we call manifold embedding prop-
erty. We show that this property captures the essential geometric aspects of universality and uncover
important topological restrictions on the approximation power of these networks, to our knowledge,
heretofore unknown in the literature. We give an example of an absolutely continuous measure μ
and embedding f: R2 → R3 such that f #仙 can not be approximated with combinations of flow
layers and linear expansive layers. This may be surprising since it was previously thought that net-
works such as those of Brehmer & Cranmer (2020) can approximate any “nice” density supported
on a “nice” manifold. We establish universality for manifolds with suitable topology, described in
terms of extendable embeddings. Our proof shows that optimality of the approximating network can
be established in reverse: optimality of a given layer can be established without optimality of pre-
ceding layers. This settles a (generalization of a) conjecture posed for a two-layer case in (Brehmer
& Cranmer, 2020). Finally, we show that these universal architectures are also practical and admit
exact layer-wise projections, as well as other properties discussed in Section 3.4.
2	Description of the Architecture
Let C(X, Y ) denote the space of continuous functions X → Y . We study networks in F ⊂
C(X, Y ) that are of the form:
F = TnL ◦ RLfnL ◦…。Tn1 ◦ Rn0,n1 ◦ Tn0	(1)
1Idempotent but in general not orthogonal.
2
Under review as a conference paper at ICLR 2022
where Rn'-1 ,n' ⊂ C(Rn'-1, Rn'), T'n' ⊂ C(Rn', Rn'), L ∈ N is the number of networks, no = n,
nL = m, and n` ≥ n`-1 for ` = 1, . . . , L. We introduce a well-tuned shorthand notation and write
H ◦ G := {h ◦ g : h ∈ H, g ∈ G} throughout the paper.
We identify R with the expansive layers and T with the bijective flows. Loosely speaking, the pur-
pose of the expansive layers is to allow the network to parameterize high-dimensional functions by
low-dimensional coordinates in an injective way. The flow networks give the network the expressiv-
ity necessary for universal approximation of manifold-supported distributions.
2.1	Expansive Layers
The expansive elements transform an n-dimensional manifold M embedded in Rn'-1, and embed
it in a higher dimensional space Rn'. To preserve the topology of the manifold, this must be done
injectively. We thus make the following assumptions about the expansive elements:
Definition 1 (Expansive Element). Let' = 1,...,L, and Rn'-1,n' be a family of functions from
Rn'-1 → Rn'. Rnjirn is a family of expansive elements if every R ∈ Rnnfrn is injective and
Lipschitz.
Examples of expansive elements include
(R1) Zero padding: R(x) = xT, 0(m-n)T where 0(m-n) is the zero vector (Brehmer & Cran-
mer, 2020).
(R2) Multiplication by an arbitrary full-rank matrix, or one-by-one convolution:
R(x) = Wx, or R(x) = w ? x	(2)
where W ∈ Rm×n and rank(W) = n (Cunningham et al., 2020), and w is a convolution
kernel ? denotes convolution Kingma & Dhariwal (2018).
(R3) Injective ReLU layers: R = ReLU(W x), W =	BT, -DBT,MTT, R(x) =
ReLU ([wτ, -wT] ?x) for matrix B ∈ GLn(R), positive diagonal matrix D ∈ Rn×n,
and arbitrary matrix M ∈ R(m-2n)×n (Puthawala et al., 2020).
(R4) Injective ReLU networks (Puthawala et al., 2020, Theorem 15). These are functions R :
Rn → Rm of the form R(X) = Wl+i ReLU(... ReLU(WIx+bi)...) + bL where w` are
n'+i X n` matrices and b` are the bias vectors in Rn'+1. The weight matrices WL satisfy
the Directed Spanning Set (DSS) condition for ` ≤ L (that make all layers injective) and
WL+i is a generic matrix which makes the map R : Rn → Rm injective. Note that the DSS
condition requires that n` ≥ 2n'-ι + 1 for ' ≤ L and We have ni = n and nL+i = m.
2.2	Bijective Flow Networks
The bulk of our theoretical analysis is devoted to expressive elements. The expressive elements bend
the range of the expansive elements into the correct shape. We make the following assumptions
about the expressive elements:
Definition 2 (Bijective Flow Network). Let ' = 0,...,L be given and let T'n ⊂ C(Rn',Rn').
T'n' is a family of bijective flow networks if every T ∈ Tn is Lipschitz continuous and bijective.
Examples of bijective flow networks include
(T1) Coupling flows, introduced by Dinh et al. (2014) consider R(X) = Hk ◦•••◦ Hi (x) where
Hi(X)= hi ([χ]rd,]gi([x]d+1：n)).	⑶
[X]d+i:n
In Eqn. 3, hi : Rd × Re → Rd is invertible w.r.t. the first argument given the second, and
gi : Rn-d → Re is arbitrary. Typically in practice the operation in Eqn. 3 is combined with
additional invertible operations such as permutations, masking or convolutions Dinh et al.
(2014; 2016); Kingma & Dhariwal (2018).
3
Under review as a conference paper at ICLR 2022
(T2) Autoregressive flows, introduced by Kingma et al. (2016) are generalizations of triangular
flows A: Rn → Rn where for i = 1, . . . , n the i’th value of A is given by of the form
[A]i(X) = hi ([x]i ,gi ([x]1：i-l))	⑷
In Eqn. 4, hi : R × Rm → R where again hi is invertible w.r.t. the first argument given
the second, and gi : Ri-1 → Rm is arbitrary except for g1 = 0. In Huang et al. (2018), the
authors choose hi(x, y), where y ∈ Rm, to be a multi-layer perceptron (MLP) of the form
hi(x, y) = φ ◦ Wp,y ◦…◦ φ ◦ Wl,y(x)	(5)
where φ is a sigmoidal increasing non-linear activation function.
3	Main Results
3.1	Embedding Gap
We call a function f an embedding and denote it by f ∈ emb(X, Y ) if f : X → Y is continuous,
injective, and f-1 : f(X) → X is continuous2 3. Also we denote embk (Rn, Rm) = emb(Rn, Rm) ∩
Ck(Rn, Rm). In order to set up our result concerning embedding of manifolds, we first need awayto
measure the degree to which a mapping g ∈ emb(Ro, Rm) nearly embeds a manifold M = f(K)
for compact K ⊂ Rn and f ∈ emb(K, Rm). With this in mind we introduce embedding gap
BK,W (f, g), a non-symmetric notion of distance between f and g. Later in the paper, f will be the
function to be approximated, and g a flow-network to be the approximator.
Definition 3 (Embedding Gap). Let K ⊂ Rn be compact and non-empty, W ⊂ Ro contain the
closure of set U which is open in the subspace topology of some vector subspace V of dimension p,
where n ≤ p ≤ o ≤ m, f ∈ emb(K, Rm), and g ∈ emb(W, Rm). Then we define the Embedding
Gap between f and g on sets K and W as
BK,W (f, g) =	inf	kI - rkL∞(f(K))	(6)
r∈emb(f (K),g(W))
where I : f(K) → f(K) is the identity function and khkL∞(X) = ess supx∈X kh(x)k2 for h : X →
Y . We refer to the embedding gap between f and g without specifying K and W when it is clear
from context.
Remark 1. As W ⊂ Ro contains U , an open set in V , there is an affine map A : Rn → V such that
A(K) ⊂ W. Then, the map r0 = g ◦ A ◦ f-1 : f(K) → g(W) is an injective continuous map from
a compact set to its range and hence r0 ∈ emb(f (K), g(W)) . Thus, in the above infimum the set
emb(f (K), g(W )) is non-empty.
Before giving properties of BK,W (f, g), we briefly describe its interpretation and meaning. We
denote by P(X) the set of probability measures over X. If the embedding gap between two func-
tions is small, then g is nearly an embedding of the range of f into Ro. BK,W (f, g) is constructed
expressly to serve as an upper bound
infw、W (f#〃n,g# μo) ≤ Bk,W (f,g)
μo ∈P(W)
where μn ∈ P (K) is given, and W2 (ν1,ν2) denotes the Wasserstein-2 distance with '2 ground
metric (Villani, 2008), as shown in Lemma 7 part 5. The above result has a simple meaning in the
context of machine learning. Suppose we want to learn a generative model g to (approximately)
sample from a probability measure ν with low-dimensional support, by applying g to samples from
a base distribution μ0. Suppose further that V is a pushforward of some (known or unknown) dis-
tribution μn via f. The embedding gap Bk,w (f, g) then upper bounds the 2-Wasserstein distance
between V and g#M° for the best possible choice of μ0.3
2Note that if X is a compact set, then continuity of the of f-1 : f (X) → X is automatic, and need not be
assumed (Sutherland, 2009, Cor. 13.27). Moreover, if f : Rn → Rm is a continuous injective map that satisfies
|f (x)| → ∞ as |x| → ∞, then by (Mukherjee, 2015, Cor. 2.1.23) the map f-1 : f(Rn) → Rn is continuous.
3The choice of p-Wasserstein distance is suitable for measures with mismatched low-dimensional support;
this has been widely exploited in training generative models (Arjovsky et al., 2017).
4
UnderreVieW as a COnferenCePaPer ICLR 2022
FigUre h A ViSUaIiZariOn Of the embedding gap ∙ In all three figures WePor f and for Lefru.U L
CenrerU." 2 and RighrU.U 3 ∙ visuallyyWe See -har g~ approaches f as5∙crease∞and We COmPUre
BKW9) V Bκw>) V BK71>) U o∙
The embedding r Can be5∙rerprered as a Candidare tɪanspoit map from any measure pushed forward
by f》rhar Can be pulled back Chrough g. LoOSeIy SPeaking》forKMg——1 0^0 f#fJLm、is a VaHd
WaSSerSrein tɪanspoit map rha〔 tɪanspoits y#pn〔0 g#IJ!o Wirh COSr IlO more than =Z —— 二丁8(T(K))∙
See Fig∙ IfOr a ViSUaIiZariOn Ofrhe embedding gap bsween Cwo〔Oy funcro∙ns∙ FUltheLrhe em—
bedding gap SariSfieSinequalities USefUI for SrUdy5∙g IIerWOrkS Ofrhe form Of Eqp L See Lemma
3∙2 MANlFoLD EMBEDDING PRoPERTY
We IlOW introduce a CenrraI COnCePLrhe manifold embedding PrOPeity (MEP) ∙ A famiIy OfneT
WOrkS SariSfieS the MEP if ir IIeaIIyembedS a Iarge CIaSS Of manifolds Of Celtain dimension and
regulamy》as measured by the embedding gap∙ The MEP is a PrOPelty Of a famiIy Of funcro∙ns
8。3 Cemb(W, IRm) Where W C IRO∙ FOrrhiS manuscripL go.m Win always be formed by Caking
8。.TnM Tm。力。.m∙
We IIOre here Cha二he MEPiS closeIyrelared〔0 the question OfWherher S IIor a given Tlldimensional
manifold »〔haris an image Of an embedding j .∙ K J IR≡rhari∞Λl Uf(K)“ K ClR 尸 Can be
approXimared by the images Of IIeUraI IIerWOrkS E .∙ K 1 IRm Of a given rype∙ In paaculaL We will
COnSider IleUraI IlerWOrkSrhar are COmPOSiriOnS Where⅛」 J IRm are Csimp-R IlOn—universal
funao∙n∞and ROW—maps T .. IRm rha〔 are diffeomorphism∙ ThiS ChoiCe Of applying non—
UniVerSaI expansive layers》and then diffromorphisms has SOme ropoogical COnSeqUenCe∞WhiCh We
discuss beow∙
ToPoLoGlCAL OBSTRUCTloNS To MANlFoLD LEARNlNG WlTH NEURAL NETWoRKS
We Hnd Char USing IlOn—universal expansive layers》followed by IayerSimPOSeS SOme ropoogical
COnd≡ons On Whar Can be approXimared∙ Wei=USrraS this Wirh rhe following PrObIem∙ When
n U 2》m U 3》and KUslCiR2 -S rhe circle，We COnSider maps EUToR .•里 J IRm Where
⅛ ⅛5 IRm isye.g∙9 a linear map Ofrank F and T .. -S P COUPling ROW WhiCh is a
homeomorphism, Lsf ∈ emb(κ" 1⅛3) be an embedding rha〔 maps KrOa trefoil knor H f (SI L
See Fig∙ 2∙ SUCh a funcro∙n f Can IlOr be Wrirren as a res5.ao∙n 7 o⅛on sl∙ In sec∙ c∙2∙2 We
PrOVe〔his face and build a 81ared example Where a measure》p ∈ P(IR2)》SUPPOlted On an annulus
is PUShed forward〔0 a measure SUPPOrted On a knotted ribbon5∙IR3 by an embedding g..为2 J^3∙
FOrrhiS measurprhere are IlOE :u 7 o⅛9 Wirh linear in⅞∙crive R and embedding TS SUCh rha〔
g#i E#p.
Ho SideSreP this fundamenBl diBCUHy》we define the MEP PrOPelty for a Celtain SUbCIaSS Of mard—
folds {f(3l.. f eʃʃ Finally》When COnSider5∙g ROW IIerWOrkS WhiCh are us.VerSaI approXimarOrS
Of Q diffeomorphism∞we res5.Crrhe ClaSS Of manifolds〔0 be approXimared even furrher∙ ThiSiS
because manifolds rha〔 are homeomorphic are IIor IIeCeSSarily dsleomorphic; for example exotic
spheres，TheSe are ropoogical SrrUaures rha〔 are homeomorphic》bu〔 IIOr diffeomorphipro the
SPhere MihIOr (1956)∙ MOreOVeL ir is known rha〔 general homeomorphisms F 鼠F ^m Can
5
Under review as a conference paper at ICLR 2022
↑
Figure 2: An illustration of the case when n = 2, m = 3, and K = S 1 is the
circle. Here f : S1 → R3 is an embedding such that the curve M = f(S1) is
a trefoil knot. Due to knot theoretical reasons, there are no map E = T ◦ R :
R2 → R3 such that E(S1) = M, where R : R2 → R3 is a full rank linear
map and T : R3 → R3 is a homeomorphism. This shows that a combination of
linear maps and coupling flow maps can not represent all embedded manifolds.
For this reason, we define the class I(Rn, Rm) of extendable embeddings f.
A similar 2-dimensional example can be obtained to a knotted ribbon, see Sec.
C.2.2.
not be approximated in C0-topology by C2-smooth diffeomorphisms T : Rm → Rm. See Muller
(2014) for a precise statement. However, all C 1 -smooth diffeomorphims F : Rm → Rm can be
approximated in the strong topology of C1 by C2-smooth diffeomorphims F : Rm → Rm, ' ≥ k,
see Hirsch (2012), Ch. 2, Theorem 2.7. Because of this, we have to pay attention to the smoothness
of the maps in the subset F ⊂ emb(K, Rm).
For these reasons, when we refer to the MEP, we consider it with respect to a class of functions F ⊂
emb(Rn, Rm). The MEP can be interpreted as a density statement, saying that our networks Eo,m
are dense in the ‘BK,W distance’ in the set F ⊂ emb(Rn, Rm). Two examples we are particularly
interested in are the following. First, when F = Φ ◦ A where A : Rn → Rm are linear maps of rank
n and Φ : Rm → Rm are Ck diffeomorphisms with k ≥ 1. Second, when F ∈ emb(Rn, Rm).
Definition 4. Let Eo,m ⊂ emb(Ro, Rm) and Fn,m ⊂ emb(Rn, Rm) be two families of functions.
We say that Eo,m has the m, n, o ManifOld Embedding PrOPerty (MEP) w.r.t. Fn,m if for every
compact non-empty set K ⊂ Rn, f ∈ Fn,m, and e > 0, there is an E ∈ Eo,m and a compact set
W ⊂ Ro such that the restriction of f in K and the restriction of E in W satisfy
BK,W (f, E) < e.	(7)
When it is clear from the context, we abbreviate the m, n, o MEP w.r.t. Fn,m simply by the m, n, o
MEP, or simply the MEP.
We also note here that if a model from Ro → Rm is a uniform universal approximator of Fo,m =
C0(Rn, Rm) on compact sets, such as those considered in Yarotsky (2017; 2018), then it has the
m, n, o MEP w.r.t F n,m for any n ≤ o. Thus, networks that are uniform universal approximators
automatically possess the MEP, as shown in Lemma 1. However, we have a result that applies to
network which are uniform universal approximators.
Definition 5 (Uniform Universal Approximator). For a non-empty subset Fn,m ⊂ C(Rn, Rm), a
family En,m ⊂ C(Rn , Rm) is said to be a uniform universal approximator of Fn,m if for every
f ∈ Fn,m, every non-empty compact K ⊂ Rn, and each e > 0, there is an E ∈ En,m satisfying:
sup kf (x) - E(x)k2 < e.	(8)
x∈K
We also have the following function class, which is key to our analysis.
Definition 6 (Extendable Embeddings). LetI(Rn, Rm) be the set of functions F : Rn → Rm that
are compositions F = Φ ◦ R where R : Rn → Rm is a linear map of rank n and Φ : Rm → Rm
is a C1-smooth diffeomorphism. We call I(Rn, Rm) be the set of extendable embeddings. We also
denoteIk(Rn,Rm) =I(Rn,Rm) ∩ Ck(Rn,Rm). WeobservethatI(Rn,Rm) ⊂ emb(Rn,Rm).
Lemma 1.	(i) If Ro,m is a universal approximator of C(Rn, Rm) and I ∈ Tm where I is the
identity map, then Eo,m := Tm ◦ Ro,m has the MEP w.r.t. emb(Rn, Rm).
(ii) If Ro,m is such that there is an injective R ∈ Ro,m and open set U ⊂ Ro such that RlU is
linear, andTm isa sup universal approximator in the space ofDiff2 (Rm, Rm), in the sense
of Teshima et al. (2020), of the C 2 -smooth diffeomorphisms, then Eo,m := Tm ◦ Ro,m has
the MEP w.r.t. F =I(Rn,Rm).
The proof of Lemma 1 is in Appendix C.2.1. It has the following implications for the architectures
studied in Section 2.
Example 1. Let E := Tm ◦ Ro,m, then
6
Under review as a conference paper at ICLR 2022
(i)	If Tm is either (T1) or (T2) and Ro,m is (R4), then Eo,m has the m,n,o MEP w.r.t.
emb(Rn, Rm).
(ii)	If Tm is (T2) with sigmoidal activations Huang et al. (2018), then if Ro,m is any of (R1),
..., (R4), then Eo,m has the m, n, o MEP w.r.t. I(Rn,Rm).
The proof of Example 1 is in Appendix C.2.3.
We add a remark here showing that if m is sufficiently larger than n, and subject to some regularity
assumptions, The two classes considered in Example 1 coincide.
Lemma 2. When m ≥ 3n + 1 and k ≥ 1, for any Ck embedding f ∈ embk(Rn, Rm) and compact
set K ⊂ Rn, there is a map in the closures of the flow type neural network E ∈ Ik (Rn , Rm) such
that E(K) = f (K). Moreover,
Ik(K, Rm) = embk(K,Rm)	(9)
This means that the topological structure of a manifold to approximate M in Rm is diffeomorphic
to the set K ⊂ Rn , and thus some flow type network can approximate M.
The proof of Lemma 2 in Appendix C.2.4.
We finally have two more lemmas to present before moving on to our discussion of universality as
it ties to the MEP.
Lemma 3. Let Fn,o ⊂ emb(Rn, Ro) and Fo,m ⊂ emb(R0 , Rm) E1p,o ⊂ emb(Rp, Ro) have the
o, n, p MEP w.r.t. Fn,o ⊂ emb(Rn, Ro) and E2o,m ⊂ emb(Ro, Rm) have the m, o, o MEP w.r.t.
Fo,m ⊂ emb(Ro, Rm). If each E2o,m ∈ E2o,m is locally Lipschitz, then E2o,m ◦ E1p,o has the m, n,p
MEP w.r.t. Fo,m ◦ Fn,o.
The proof of Lemma 3 is in Appendix C.2.5.
We note that when the elements of E2o,m are differentiable, local Lipschitzness is automatic, and
need not be assumed, see e.g. (Tao, 2009, Ex. 10.2.6). We also record a near-converse (proved in
C.2.6) of Lemma 3 that shows that if E2o,m ◦ E1p,o has the m, n, p MEP, then E2o,m has the m, n, o
MEP.
Lemma 4. Let F ⊂ emb(Rn, Rm). Let E1p,o ⊂ emb(Rp, Ro) and E2o,m ⊂ emb(Ro, Rm) be such
that E2o,m ◦ E1p,o has the m, n,pMEP with respect to family F. Then E2o,m has the m, n, o MEP with
respect to family F .
3.3	Universality
We now present our universal approximation result for networks given in Eqn. Eq. 1 and a decou-
pling property.
Theorem 1 (Qualitative Universality for Embeddings). Let n° = n,nL = m, μ ∈ P(K) be
an absolutely continuous measure w.r.t. Lebesgue measure. Further let, for each ` = 1, . . . , L,
Eng-1,n' ：= τn' oRn'_1,n' where Rnnfrn is afamily ofinjeCtive expansive elements that contains
a linear map, and Tnn is a family of bijective family networks. Finally let T0r be distributionally
universal, i.e. for any absolutely continuous μ ∈ P(Rn) and V ∈ P(Rn), there is a {Ti}∞=ι Such
that Ti#@ → v in distribution. Suppose that one Ofthefollowing two cases hold:
(i)	Let F ∈ FrnLfm ◦ ∙∙∙oFn,n1 and EngT,n have the the n`, n, n`7 MEPfor ' = 1,...,L with
nn-1 ,nn
respect to f`	.
(ii)	Let F ∈ emb1 (Rn, Rm ) be a C 1 -smooth embedding, and assume that for ` = 1, . . . , L it holds
that n` ≥ 3ng-ι + 1 and thefamilies Tnn are dense in the space of C2-diffeomorphism Diff (Rnn).
Then, there is a sequence of {Ei}i=ι	∞ ⊂ ELLT,m ◦…◦ En1,n ◦ Tn SuCh that
lim W (F#Μ,耳#Μ)=0.
i→∞
(10)
The proof of Theorem 1 is in Appendix C.3.1. As discussed in above and in Figure 2, there are
topological obstructions for Theorem 1 with a general embedding F : Rn → Rm . When n = 2,
7
Under review as a conference paper at ICLR 2022
m = 3, L = 1, and μ is the uniform measure on an annulus K ⊂ R2 target measure F#m is
the uniform measure on a knotted ribbon M = F(K) ⊂ R3. There are no injective linear maps
R : R2 → R3 and diffeomorphisms T : R3 → R3 such that E = T ◦ R would satisfy M = E(K)
and E#m = F#m. This happen even though the set of diffeomorphism T : R3 → R3 are universal
distributional approximators. In this case the condition m = n1 ≥ 3n + 1 is not valid.
We remark here that our networks are designed expressly to approximate manifolds, and hence
injectivity is key. This separates our results from, e.g. (Lee et al., 2017, Theorem 3.1) or (Lu & Lu,
2020, Theorem 2.1), where universality results of ReLU networks are also obtained.
The previous theorem shows that the entire network is universalif it can be broken into pieces that
have the MEP. The following lemma, proved in Appendix C.3.2, shows that if En,m = Ho,m ◦ Gn,o,
then Ho,m must have the m, n, o MEP if En,m is universal.
Lemma 5. Suppose that En,m = Ho,m ◦ Gn,o where En,m ⊂ emb(Rn, Rm), Ho,m ⊂
emb(Ro, Rm), and Gn,o ⊂ emb(Rn, Ro). If Ho,m does not have the m, n, o MEP w.r.t. F,
then there exists a f ∈ F, compact K ⊂ Rn and > 0 such that for all E ∈ En,m, and
r ∈ emb(f(K),E(W))
kI - rkL∞(K) ≥ .	(11)
The proof of Theorem 1 also implies that, loosely speaking, later layers decouple from earlier ones.
That is, given a sequence of functions that has the MEP on the last L - ` layers, there are always
functions in the first ` layers so that the entire network is end-to-end optimal.
Corollary 1. Let Fn,o ⊂ emb(Rn, Ro), Fo,m ⊂ emb(Ro, Rm), and let Eo,m ⊂ emb(Ro, Rm)
have the m, n, o MEP w.r.t. Fo,m ◦ Fn,o. For every F ∈ Fo,m ◦ Fn,o then there is a compact
K ⊂ Rn and {Ei}i∞=1 ⊂ Eo,m such that
lim BK,W (F, Ei) =0.	(12)
i→∞
Further, if En,o ⊂ emb(W0, Ro) has the o, n, n MEP w.r.t. Fn,o, and Tn is a universal approx-
imator for distributions, then for any μ ∈ P(K) where K ⊂ Rn is compact, there is a Sequence
{Ei0}i=1,...,∞ ⊂ En,o and {Ti}i=1,...,∞ ⊂Tnsothat
limW2(F#〃, Ei ◦ Ei ◦ T⅛μ) = 0.	(13)
i→∞
The proof of Corollary 1 is in Appendix C.3.3. Approximation results for neural networks are typi-
cally given in terms of the network end-to-end. Corollary 1 shows that the layers of approximating
networks can in fact be built one at a time. It is related to an observation made in (Brehmer & Cran-
mer, 2020, Section B) about training strategies, where the authors remark that they ‘expect faster and
more robust training ofa network’ of the form in Eqn. 1 when L = 1, that is F = T1m ◦ R1n,m ◦T0n.
Corollary 1 shows that there exists a minimizing sequence in T1m that need only minimize Eqn. 12;
the T0n layers can be minimized after. We can further combine Lemma 5 and Cor. 1 to prove that not
only can the network from Brehmer & Cranmer (2020) be trained layerwise, but that any universal
network can necessarily be trained layerwise in reverse order, provided that it can be written as a
composition of two smaller layers.
3.4	Layer-wise inversion, uncertainty quantification and recovery of weights
In this subsection, we describe how our network can be augmented with more useful properties if the
architecture satisfies a few more assumptions without affecting universal approximation. We focus
on a new layerwise projection result, with a further discussion of Bayesian uncertainty quantifica-
tion, and black-box recovery of our network’s weights in Appendices D.2 & D.3.
Given a point y ∈ Rm that does not lie in the range of the network, projecting y onto the range of the
network is a practical problem without an obvious answer. The crux of the problem is inverting the
injective (but non-invertible) R layers when R. When R contains only full-rank matrices as in (R1)
or (R2) then we can compute a least-squares solution. If, however, R contains layers which are only
piecewise linear, as in (R3), then the problem of computing a least squares solution is more difficult,
see Fig. 3. Nevertheless, we find that if R is (R3) we can still compute a least-squares solution.
8
Under review as a conference paper at ICLR 2022
Figure 3: A schematic showing that, for a toy problem, the least-squares
projection to a piecewise affine range can be discontinuous. Left: A
partitioning of R2 into classes with gray boundaries. Two points y, y0
are in the same class if they are both closest to the same affine piece of
R(R), the range of R. The three points y1, y2 and y3 are each projected
to the closest three points on R(R) yielding y1, y2 and y3. Note that the
projection operation is continuous within each section, but discontinuous
across gray boundaries between section.
Assumption 1. Let R be given by one of (R1) or (R2), or else (R3) when m = 2n.
If R only contains linear operators, then the least-squares problem can be computed by solving the
normal equations (see (Golub, 1996, Section 5.3).) This includes cases (R1) or (R2). For (R3) we
have the following result for D = In×n and M is an empty matrix.
Definition 7. Let W = Bt -DBtt ∈ R2n×n and y ∈ R2n be given, and let R(x) =
ReLU(W x). Then define
In×n	-I n×n
c(y) ∈ R2n c(y)	:=	max	-In×n	In×n y,0	(14)
∆ ∈ Rn×n [∆ ]	:=	0	if	[c(y)]i+n	= 0	[∆ ]	=	0 if i	6= j	(15)
∆y ∈R [∆y]i,i	:=	1	if	[c(y)]i+n	>0	[∆y]ij	=0ifi	6=j	(15)
My ∈ Rn×2n My	:=	(In×n	-∆y) ∆y	(16)
where the max in Eqn. 14 is taken element-wise.
Theorem 2. Let y ∈ R2n. If for i = 1, . . . , n, [y]i 6= [y]i+n then
Rt(y) := (MyW)T Myy = argmin Ily - R(X)Il2 .	(17)
x∈Rn
Further, if there is a i ∈ {1, . . . , n} such that [y]i = [y]i+n, then there are multiple minimizers of
Iy - R(x)I2, one of which is Rt(y).
The proof of Theorem 2 is given in Appendix D.1.
Remark 2. We note that Theorem 2 is different from many of the existing work on inverting expan-
sive layers, e.g. Aberdam et al. (2020); Bora et al. (2017); Lei et al. (2019), our result gives a direct
inversion algorithm that is provably the least-squares minimizer. Further, if each expansive layer is
any combination of (R1), (R2), or (R3) then the entire network can be inverted end-to-end by using
either the above result or solving the normal equations directly.
4 Conclusion
Bijective flow networks are a powerful tool for learning a push-forward mappings in a space of fixed
dimension. Increasingly, these flow networks have been used in combination with networks that
increase dimension in order to produce networks which are purportedly universal.
In this work, we have studied the theory underpinning these flow and expansive networks by in-
troducing two new notions, the embedding gap and the manifold embedding property. We show
that these notions are both necessary and sufficient for proving universality, but require important
topological and geometrical considerations which are, heretofore, under-explored in the literature.
We also find that optimality of the studied networks can be established ‘in reverse,’ by minimizing
the embedding gap, which we expect opens the door to convergence of layer-wise training schemes.
Without compromising universality, we can also use specific expansive layers with a new layer-
wise projection result. Moreover, we show that the studied networks provide Bayesian uncertainty
quantification and allow black-box recovery of their weights.
9
Under review as a conference paper at ICLR 2022
References
Aviad Aberdam, Dror Simon, and Michael Elad. When and how can deep generative models be
inverted? arXiv Preprint arXiv:2006.15555, 2020.
LUigi Ambrosio, Nicola Gigli, and GiUsePPe Savare. Gradient flows: in metric Spaces and in the
Space of probability measures. Springer Science & Business Media, 2008.
Lynton Ardizzone, Jakob KrUse, Sebastian Wirkert, Daniel Rahner, Eric W Pellegrini, Ralf S
Klessen, Lena Maier-Hein, Carsten Rother, and Ullrich Kothe. Analyzing inverse problems with
invertible neural networks. arXiv Preprint arXiv:1808.04730, 2018.
Martin Arjovsky, Soumith Chintala, and Leon Bottou. Wasserstein gan. arXiv Preprint
arXiv:1701.07875, 2017.
Simon Arridge, Peter Maass, Ozan Oktem, and Carola-Bibiane Schonlieb. Solving inverse problems
using data-driven models. ACta Numerica, 28:1-174, 2019.
Ashish Bora, Ajil Jalal, Eric Price, and Alexandros G Dimakis. Compressed sensing using genera-
tive models. In PrOCeedingS of the 34th International Conference on MaChine Learning-VoIUme
70,pp. 537-546. JMLR. org, 2017.
Johann Brehmer and Kyle Cranmer. Flows for simultaneous manifold learning and density estima-
tion. arXiv Preprint arXiv:2003.13913, 2020.
Phuong Bui Thi Mai and Christoph Lampert. Functional vs. parametric equivalence of relu net-
works. In 8th International COnferenCe on Learning RepreSentations, 2020.
Edmond Cunningham, Renos Zabounidis, Abhinav Agrawal, Ina Fiterau, and Daniel Sheldon. Nor-
malizing flows across dimensions. arXiv Preprint arXiv:2006.13070, 2020.
Laurent Dinh, David Krueger, and Yoshua Bengio. Nice: Non-linear independent components esti-
mation. arXiv Preprint arXiv:1410.8516, 2014.
Laurent Dinh, Jascha Sohl-Dickstein, and Samy Bengio. Density estimation using real nvp. arXiv
Preprint arXiv:1605.08803, 2016.
Conor Durkan, Artur Bekasov, Iain Murray, and George Papamakarios. Cubic-spline flows. arXiv
Preprint arXiv:1906.02145, 2019a.
Conor Durkan, Artur Bekasov, Iain Murray, and George Papamakarios. Neural spline flows.
AdVanCeS in NeUraI InfOrmatiOn PrOCeSSing Systems, 32:7511-7522, 2019b.
Gene H Golub. Matrix COmpUtations. Johns Hopkins University Press, 1996.
Aidan N Gomez, Mengye Ren, Raquel Urtasun, and Roger B Grosse. The reversible residual net-
work: Backpropagation without storing activations. In AdVanCeS in neuralinformation processing
SyStems, pp. 2214-2224, 2017.
Will Grathwohl, Ricky TQ Chen, Jesse Bettencourt, Ilya Sutskever, and David Duvenaud. Ffjord:
Free-form continuous dynamics for scalable reversible generative models. arXiv Preprint
arXiv:1810.01367, 2018.
Morris W Hirsch. Differential topology, volume 33. Springer Science & Business Media, 2012.
Chin-Wei Huang, David Krueger, Alexandre Lacoste, and Aaron Courville. Neural autoregressive
flows. In International COnference on MaChine Learning, pp. 2078-2087. PMLR, 2018.
Michael F Hutchinson. A stochastic estimator of the trace of the influence matrix for laplacian
smoothing splines. COmmUniCatiOnS in StatiStiCS-SimUlation and Computation, 18(3):1059-1076,
1989.
JOrn-Henrik Jacobsen, Arnold Smeulders, and Edouard Oyallon. i-revnet: Deep invertible networks.
arXiv Preprint arXiv:1802.07088, 2018.
10
Under review as a conference paper at ICLR 2022
Priyank Jaini, Kira A Selby, and Yaoliang Yu. Sum-of-squares polynomial flow. In International
Conference on Machine Learning, pp. 3009-3018. PMLR, 2019.
Diederik P Kingma, Tim Salimans, Rafal Jozefowicz, Xi Chen, Ilya Sutskever, and Max
Welling. Improving variational inference with inverse autoregressive flow. arXiv PrePrint
arXiv:1606.04934, 2016.
Durk P Kingma and Prafulla Dhariwal. Glow: Generative flow with invertible 1x1 convolutions. In
AdVanceS in NeUral InfOrmatiOn PrOceSSing Systems, pp. 10215-10224, 2018.
Ivan Kobyzev, Simon Prince, and Marcus Brubaker. Normalizing flows: An introduction and review
of current methods. IEEE TranSactiOnS on Pattern AnalySiS and Machine Intelligence, 2020.
Konik Kothari, AmirEhsan Khorashadizadeh, Maarten de Hoop, and Ivan Dokmanic. Trumpets:
Injective flows for inference and inverse problems. arXiv PrePrint arXiv:2102.10461, 2021.
Jakob Kruse, Gianluca Detommaso, Robert Scheichl, and Ullrich Kothe. Hint: Hierarchi-
cal invertible neural transport for density estimation and bayesian inference. arXiv PrePrint
arXiv:1905.10687, 2019.
Jakob Kruse, Lynton Ardizzone, Carsten Rother, and Ullrich Kothe. Benchmarking invertible archi-
tectures on inverse problems. arXiv PrePrint arXiv:2101.10763, 2021.
Holden Lee, Rong Ge, Tengyu Ma, Andrej Risteski, and Sanjeev Arora. On the ability of neural
nets to express distributions. In COnference on Learning Theory, pp. 1271-1296. PMLR, 2017.
Qi Lei, Ajil Jalal, Inderjit S Dhillon, and Alexandros G Dimakis. Inverting deep generative models,
one layer at a time. In AdVanceS in NeUral InfOrmatiOn PrOceSSing Systems, pp. 13910-13919,
2019.
Yulong Lu and Jianfeng Lu. A universal approximation theorem of deep neural networks for ex-
pressing distributions. arXiv PrePrint arXiv:2004.08867, 2020.
Ib H Madsen, Jxrgen Tornehave, et al. From calculus to cohomology: de Rham cohomology and
characteristic classes. Cambridge university press, 1997.
John Milnor. On manifolds homeomorphic to the 7-sphere. AnnalS OfMathematics, pp. 399U05,
1956.
Amiya Mukherjee. Differential topology. Hindustan Book Agency, New Delhi;
Birkhauser/Springer, Cham, second edition, 2015. ISBN 978-3-319-19044-0; 978-3-319-
19045-7. doi: 10.1007/978-3-319-19045-7. URL https://doi.org/10.1007/
978-3-319-19045-7.
Stefan Muller. Uniform approximation of homeomorphisms by diffeomorphisms. Topology and its
APPlications, 178:315-319, 2014.
Kunio Murasugi. Knot theory & its applications. Modern Birkhauser Classics. Birkhauser
Boston, Inc., Boston, MA, 2008. ISBN 978-0-8176-4718-6. doi: 10.1007/978-0-8176-4719-3.
URL https://doi.org/10.1007/978-0-8176-4719-3. Translated from the 1993
Japanese original by Bohdan Kurpita, Reprint of the 1996 translation [MR1391727].
George Papamakarios, Eric Nalisnick, Danilo Jimenez Rezende, Shakir Mohamed, and Balaji Lak-
Shminarayanan. Normalizing flows for probabilistic modeling and inference. arXiv PrePrint
arXiv:1912.02762, 2019.
Michael Puthawala, Konik Kothari, Matti Lassas, Ivan Dokmanic, and Maarten de Hoop. Globally
injective relu networks. arXiv PrePrint arXiv:2006.08464, 2020.
David Rolnick and Konrad KOrding. Reverse-engineering deep relu networks. In InternatiOnal
COnference on Machine Learning, pp. 8178-8187. PMLR, 2020.
Carlo H. Sequin. Tori story. In Reza Sarhangi and Carlo H. SeqUin (eds.), PrOceedingS of BridgeS
2011: Mathematics, Music, Art, Architecture, Culture, pp. 121-130. Tessellations Publishing,
2011. ISBN 978-0-9846042-6-5.
11
Under review as a conference paper at ICLR 2022
He Sun and Katherine L Bouman. Deep probabilistic imaging: Uncertainty quantification and multi-
modal solution characterization for computational imaging. arXiv Preprint arXiv:2010.14462,
2020.
Wilson A. Sutherland. IntrodUction to metric and topological spaces. Oxford University Press,
Oxford, 2009. ISBN 978-0-19-956308-1. Second edition [ofMR0442869], Companion web site:
www.oup.com/uk/companion/metric.
Terence Tao. AnaIySis, volume 185. Springer, 2009.
Takeshi Teshima, Isao Ishikawa, Koichi Tojo, Kenta Oono, Masahiro Ikeda, and Masashi Sugiyama.
Coupling-based invertible neural networks are universal diffeomorphism approximators. arXiv
PrePrint arXiv:2006.11469, 2020.
CedriC Villani. OPtimaI transport: old and new, volume 338. Springer Science & Business Media,
2008.
Dmitry Yarotsky. Error bounds for approximations with deep relu networks. NeUraI Networks, 94:
103-114, 2017.
Dmitry Yarotsky. Optimal approximation of continuous functions by very deep relu networks. In
COnference on Learning Theory, pp. 639-649. PMLR, 2018.
Appendix A	S ummary of Notation
Throughout the paper we make heavy use of the following notation.
1.	Unless otherwise stated, X and Y always refer to subsets of Euclidean space, and K and
W always refer to compact subsets of Euclidian space.
2.	f ∈ C(X, Y ) means that f : X → Y is continuous.
3.	For families of functions F and G where each F 3 f : X → Y and G 3 g : Y → Z, then
we define G°F = {g ◦ f: X → Z: f ∈F, g ∈g}.
4.	f ∈ emb(X, Y ) means that f ∈ C(X, Y ) is continuous and injective on the range of f,
i.e. an embedding, and furthermore that f-1 : f(X) → X is continuous.
5.	μ ∈ P(X) means that μ is a probability measure over X.
6.	W2 (μ, V) for μ,ν ∈ P(X) refers to the Wasserstein-2 distance, always with '2 ground
metric.
7.	∣∣∙∣∣Lp(χ) refers to the Lp norm of functions, from X to R.
8.	For vector-valued f : X → Y , kfkL∞(X) = ess supx∈X kfk2. Note that Y is always finite
dimensional, and so all discrete 1 ≤ q ≤ ∞ norms are equivalent.
9.	Lip(g) refers to the Lipschitz constant of f.
10.	For x ∈ Rn, [x]i ∈ R is the i’th component of x. Similarly, for matrix A ∈ Rm×n, [A]ij
refers to the j’th element in the i’th column.
Appendix B	Detailed Comparison to Prior work
B.1	Connection to Brehmer & Cranmer (2020)
In Brehmer & Cranmer (2020), the authors introduce manifold-learning flows as an invertible
method for learning probability density supported on a low-dimensional manifold. Their model
can be written as
F= T1m ◦ Rn,m ◦T0n	(18)
12
Under review as a conference paper at ICLR 2022
In×n
where T1m ⊂ C(Rm, Rm), T0m ⊂ C(Rn,Rn), andR = 0(mI -n)×n is a zero-padding (R1).
They invert F ∈ F in two different ways. For manifold-learning flows (M-flows) they restrict T1m
to be an invertible flow, and for manifold-learning flows with separate encoder (Me-flows) they
place no such restrictions on T1m and instead train a separate neural network e to invert elements of
T1m.
Our results apply out-of-the-box to the architectures used in Experiment A of Brehmer & Cranmer
(2020). The architecture described in Eqn. 18 is of the form of Eqn. 1 where L = 1. Further,
although they are not studied here, our analysis can also be applied to quadratic flows.
The network used in (Brehmer & Cranmer, 2020, Experiment 4.A) uses coupling networks, (T1),
where T1m and T0n are both 5 layers deep. For (Brehmer & Cranmer, 2020, Experiments 4.B and
4.C) the authors choose expressive elements T that are rational quadratic flows Durkan et al. (2019b)
for both T1m and T0n . In Experiment 4.B they let T1 and T0 again be 5 layers deep, and in 4.C
they again let T1 by 20 layers deep and T0 15 layers. For the final experiment, 4.D, the choose
more complicated expressive elements that combine Glow Kingma & Dhariwal (2018) and Real
NVP Dinh et al. (2016) architectures. These elements include the actnorm, 1 × 1 convolutions and
rational-quadratic coupling transformations along with a multi-scale transformation.
The authors mention universality of their network without our proof, but our universality results in
Theorem 1 apply to their networks from Experiment A wholesale. Further in their work the authors
describe how training can be split into a manifold phase and density phase, wherein the manifold
phase T1m is trained to learn the manifold, and in the density phase T1m if fixed and T0n is trained to
learn the density thereupon. This statement is made formal and proven by our Cor. 1.
B.2	Connection to Kothari et al. (2021)
In Kothari et al. (2021), the authors introduce the ‘Trumpet’ architecture, for it’s architecture,
which has many alternating flow networks & expansive layers with many flow-networks in the low-
dimensional early stages of the network, which gives the architecture a shape similar to the titular
instrument.
The architecture studied in Kothari et al. (2021) is precisely of the form of Eqn. 1, where the bijective
flow networks are revnets Gomez et al. (2017); Jacobsen et al. (2018) architecture, and the expansive
elements are 1 × 1 convolutions, as in (R2). To out knowledge, there are no results that show that
the revnets used are universal approximators, but if they revnets are substituted with either (T1) or
(T2), then the, we could apply Theorem 1 to the resulting architecture.
The authors of Kothari et al. (2021) also remark that their entire network can be designed in order
to facilitate uncertainty quantification, a remark that we too can apply to our networks, as discussed
further in Section D.2.
Appendix C	Proofs
C.1 Main Results
C.1.1 Helper Lemma
Before presenting the proof of Lemma 7, we present the following three helper inequalities
Lemma 6. For all of the following results, f ∈ emb(K, Rm) and g ∈ emb(W, Rm) and n ≤ o ≤
m.
1.
BK,W (f, g) ≥ sup inf kg(xo) - f (xn)k2 .	(19)
xn∈Kxo∈W
2.	Let X, Y ⊂ W, let g be Lipschitz on W, and r ∈ emb(X, Y ). Then, there is a r0 ∈
emb(g(X),g(Y)) such that g ◦ r = r0 ◦ g and ∣∣I - r0∣∣L∞(g(χ)) ≤ ∣∣I -∏l∞(x)Lip(g)∙
13
Under review as a conference paper at ICLR 2022
3.
kI - rkL∞(K) = I - r-1L∞(r(K))	(20)
4.	Let K ⊂ Rn, X ⊂ Rp and W ⊂ Ro be compact sets. Also, let f ∈ emb(K, W) and
h ∈ emb(X, W), and let g ∈ emb(W, Rm) be a Lipschitz map. Then
Bk,x (g ◦ f,g ◦ h) ≤ Lip(g)Bκ,x(f, h).	(21)
Proof. 1. Let r ∈ C(f(K), g(W)), then
kI - rkL∞(f(K)) = suP k(I - r)f (Xn)k2= suP kf (Xn) -r ◦ f (Xn)k2
xn∈K	xn∈K
= sup kf(Xn) - g(Xo)k2 where Xo = g-1 ◦ r ◦ f(Xn)
xn∈K
≥ sup inf kf(Xn) - g(Xo)k2 .
xn∈Kxo∈W
2.	g is injective on X, hence we can define r0 such that r0 = g ◦ r ◦ g-1 : g(X) → g(r(X)) ⊂
g(Y ) such that r0 ∈ emb(g(X), g(Y )), and thus ∀X ∈ X,
k(I-r0) ◦ g(X)k2 = kg(X) - g ◦ r(X)k2 ≤ Lip(g) kI-rkL∞(X)	(22)
where we have used kr(X) - Xk2 ≤ kI - rkL∞(X).
3.	For every X ∈ r(K), we have a y ∈ K such that X = r(y), thus ∀X ∈ r(K),
∣∣(I-r-1) (x)∣∣2 = k(r - I )(y)k2 .	(23)
But r is clearly surjective onto it’s range, hence taking the supremum over all X ∈ X yields
I - r-1L∞(r(K)) = kI - rkL∞(K)	(24)
4.	As g ∈ emb(W, Rm), the map g : W → g(W) is a homeomorphism and there is g-1 ∈
emb(g(W), W). For a map r ∈ emb(g ◦ f(K),g ◦ h(X)), We see that r = g-1 ◦ r ◦
g ∈ emb(f (K), h(X)). Also, the opposite is valid as if r ∈ emb(f (K), h(X)) then
r = g ◦ r ◦ g-1 ∈ emb(g ◦ f (K), g ◦ h(X)). Thus
Bk,x (g ◦ f,g ◦ h)=
inf
r∈emb(gof (K ),goh(X))
inf
kI - rkL∞(g°f(K))
r=gorog-1∈emb(gof (K),gOh(X))
kI - g ◦ r ◦ g 1kL∞(gof(K))
≤
r∈emb盘),h(X)) kg O(I- r) ◦ g-1 kL∞(gof(K))
LiP(g) r∈embfK ),h(X))k(I- r) θ 尸腔。。f(K))
≤
Lip(g)
inf
r∈emb(f (K),h(X))
kI - rkL∞(f(K))
≤ Lip(g)BK,X(f,h)
□
C.1.2 Lemma 7, Useful Embedding Gap Inequalities
Lemma 7. Let f ∈ emb(K, Rm) and g ∈ emb(W, Rm) and n ≤ o ≤ m. Then the following hold:
1.	BK,W (f, g) ≤ supx∈K kg ◦ h(X) - f (X)k2 where h ∈ emb(K, Ro) is a map satisfying
h(K) ⊂ W.
2.	For any X that is the closure of an open set , if h ∈ emb(X, W) then
Bk,w (f,g) ≤ Bk,x (f,g ◦ h)	(25)
14
Under review as a conference paper at ICLR 2022
3.	For any r ∈ emb(f (K), Rm),
BK,W (f, g) ≤ kI - rkL∞(f(K)) + BK,W (r ◦ f, g).	(26)
4.	For any r ∈ emb(f (K), g(W)) and h ∈ emb(X, W) where X ⊂ Rp is the closure of a
compact set where n ≤ p ≤ o we have that
Bk,x(f, g ◦ h) ≤ kI — rkL∞(f(κ)) + Lip(g)Bκ,x(g-1 ◦ T ◦ f, h)	(27)
where Lip(g) denotes the Lipschitz constant of g.
5.	For any μn ∈ P (K) ,there is a μ° ∈ P (W) such that
W2 (f#Mn, g#MO) ≤ BK,W(f, g).	(28)
The proof of Lemma 7.	1. If we let r := g ◦ h ◦ f-1, then r ∈ emb(f (K), g(W)), and
Bk,w(f,g) ≤ kk(I -r) ◦ f(x)k2∣∣L∞(κ)	(29)
=IlIIf(X)-g◦ h◦ f-1 ◦f(x)l∣2∣IL∞(κ) ≤ 4 sup kf(X)-goh(χ)k2.
(30)
2.	Given that g ◦ h(X) ⊂ g(W), we have that emb(f (K), g ◦ h(X)) ⊂ emb(f (K), g(W)),
thus the infimum in Eqn. 6 is taken over a smaller set, thus BK,W(f, g) ≤ BK,X (f, g ◦ h).
3.	Note that for any r0 ∈ emb(r ◦ f(K), g(W)), r0 ◦ r ∈ emb(f (K), g(W)), and so we have
BK,W(f, g) ≤ kI - r0 ◦ rkL∞(f (K)) ≤ kI - rkL∞(f(K)) + kr - r0 ◦ rkL∞(f(K)) (31)
=kI - rkL∞ (f (K)) + kI — r0kL∞(rof(K))	(32)
where we have used that r is injective for the final equality. This holds for all possible r0,
hence we have the result.
4. Recall that f ∈ emb(K, W), g ∈ emb(W,Rm), h ∈ emb(X, W) and r ∈
emb(f (K), g(W)). Then g-1 ∈ emb(g(W), W). As r ◦ f(K) ⊂ g(W), we see that
r ◦ f = g ◦ g-1 ◦ r ◦ f.
This, Lemma 7, point 3 and Lemma 6, point 4 yield that
Bk,x(f,g ◦ h) ≤ kI — r∣∣L∞(f(κ)) + Bk,x(r ◦ f,g ◦ h)
≤ kI — rkL∞(f(κ)) + Bk,x(g ◦ g-1 ◦ r ◦ f, g ◦ h)
≤ kI — rkL∞(f(k)) + Lip(g) Bk,x(g-1 ◦ T ◦ f, h),
which proves the claim.
5. Let r ∈ emb(f(K), g(W)) be such that kI — rkL∞(Range(f)) ≤ BK,W (f, g) + for
some > 0, then for every X ∈ K, there exists y ∈ W such that g(y) = r ◦ f (X). From
injectivity of g, we have that y = g-1 ◦ r ◦ f (x). Note that g-1 ◦ r ◦ f ∈ emb(K, W), hence
K0:= g-1 ◦ r ◦ f (K) is compact. Define μ0 := (g-1 ◦ r ◦ f )#仙.Clearly g#M0 = r ◦ f #仙,
and thus
W2 (g#M0, f #μ) ≤ W2 (g#M0, r ◦ f #M)+ W2 (r ◦ f #M, f #μ)	(33)
but W2 (g#M0, r ◦ f #Μ)=O and
W2 (r ◦ f#〃, f#〃)≤ (ZK kI - rk2 df#〃) / ≤ Bk,w(f, g) + e. (34)
This is true for every > 0, hence taking the infimum over all r ∈ emb(f (K), g(K))
establishes W2 (g#M0, f#Μ)≤ Bk,w(f, g) + E for every e > O, and thus We have that
W2 (g#M0,f#M)≤ bk,w(f,g).
□
15
Under review as a conference paper at ICLR 2022
C.2 Manifold Embedding Property
C.2.1 The proof of Lemma 1
The proof of Lemma 1.	(i) Let us consider > 0, a compact set K ⊂ Rn and f ∈
emb(Rn, Rm). Let W = K × {0}o-n and F : Ro → Rm be the map given by
F (x, y) = f(x), (x, y) ∈ Rn × Ro-n. Because Ro,m is a uniform universal approxi-
mator of C(Rn, Rm), there is an R ∈ Ro,m such that kF - RkL∞ (W) < . Then for the
map E = I ◦ R we have that BK,W (f, E) < . This is true for every > 0, and so Eo,m
has the MEP property w.r.t. the family emb(Rn, Rm).
(ii) Recall that f := Φ0 ◦ R0 for Φ0 ∈ Diff1(Rm,Rm) and linear R0: Rn → Rm, and that
R ∈ R is such that RlU is linear for open U. We present the proof in the case when n = o,
and we make the assumption that RK is linear. In this case, we have the existence of an
affine map A: Rm → Rm so that Ro = A ◦ R so that K := Ro(K) = A(R(K)). Let
> 0 be given. By (Hirsch, 2012, Chapter 2, Theorem 2.7), the spaceDiff2(Rm,Rm) is
dense in the space Diff1(Rm,Rm), and so there is some Φ1 ∈ Diff2(Rm,Rm) such that
kφ1 |KK - φ0lK kL∞(K;Rm) < 2.
Then, let T ∈ Tm be such that kT - Φι ◦ A∣∣l∞(r(k);Rm) < j. Then We have that
kT ◦ R - f kL∞(κ) = kT ◦ R - Φo ◦ R0kL∞(κ)
≤ kT ◦ R - Φ1 ◦ A ◦ RkL∞(K ) + kΦ1 ◦ A ◦ R - Φ0 ◦ R0kL∞(K )
≤
<
kT - Φ1 ◦ AkL∞(R(K )) + kΦ1 ◦ A ◦ R - Φ0 ◦ A ◦ RkL∞(K )
2 + 2 = e.
Hence, if we let r = T ◦ R ◦ f-1 ∈ emb(f (K ),T ◦ R(K)) then we obtain that Bk,k (f, T ◦
R) < . This holds for any , and hence we have that T ◦ R has the MEP for I(Rn, Rm).
The proof in the case that o ≥ n follows with minor modification, and applying Lemma 7
point 1.
□
C.2.2 S1 CAN NOT BE MAPPED EXTENDABLY TO THE TREFOIL KNOT
We first show that there are no maps E := T ◦ R where R : R2 → R3 such that T is a homeo-
morphism and E(S1) is a trefoil knot. We use the fact that the trivial knot S1 and the trefoil knot
M = f (S 1 ) are not equivalent, that is, there are no homeomorphisms in R3 that map S 1 to M. In-
deed, by (Murasugi, 2008, Section 3.2), the trefoil knot M and its mirror image are not equivalent,
whereas the trivial knot S1 and its mirror image are equivalent. Hence, M and R(S1) are not equiv-
alent knots in R3. Thus by (Murasugi, 2008, Definition 1.3.1 and Theorem 1.3.1), we see that there
is no orientation preserving homeomorphism T : R3 → R3 such that T(R3 \ R(S1)) = R3 \ M.
As the orientation of the map T can be changed by composing T with the reflection J : R3 → R3
across the plane Range(R) that defines a homeomorphim J : R3 \ R(S1) → R3 \ R(S1), we see
that there is no homeomorphism T : R3 → R3 such that T(R3 \ R(S1)) = R3 \ M.
This example shows that the composition E = T ◦ R of a linear map R and a coupling flow T
cannot have the property that E(S1) = f(S1) for this embedding f. Moreover, the complement
R3 \ E(S1) is never homeomorphic to R3 \ f(S1) for any such map E.
We now construct another example, similar to Figure 2, where an annulus that is mapped to a knotted
ribbon in R3. To do this, replace the circle S1 by an annulus K = {x ∈ R2 : 1/2 ≤ |x| ≤ 3/2},
that in the polar coordinates is {(r, θ) : 1/2 ≤ r ≤ 3/2} and define a map F : K → R3 by defining
in the polar coordinates
F(r,θ) = f(θ) +a(r - 1)v(θ)
where f : S1 → Σ1 ⊂ R3 is an smooth embedding of S1 to a trefoil knot Σ1 and v(θ) ∈ R3 is a
unit vector normal to Σ1 at the point f(θ) such that v(θ) is a smooth function of θ, and a > 0 is
16
Under review as a conference paper at ICLR 2022
a small number. In this case, M1 = F (K) is a 2-dimensional submanifold of R3 with boundary,
which can visualizes M1 as a knotted ribbon.
We now show that there are no maps E = T ◦ R such that E(K) = F(K) where T : R3 → R3 is an
embedding, and R : R2 → R3 injective and linear. The key insight is that if such a T existed, then
this implies that the trefoil knot is equivalent to S1 in R3, which is known to be false.
Let UP(A) denote the ρ-neighborhood of the set A in R3. It is easy to see that R2 \ ({0} X [-1,1])
is homeomorphic to R2 \ Br2(0,1), which is further homeomorphic to R2 \ {0}. Thus, using
tubular coordinates near Σ1 and a sufficiently small ρ > 0, we see that R3 \ M1 is homeomorphic to
R3 \ Uρ(Σ1), which is further homeomorphic to R3 \ Σ1. Also, when R : R2 → R3 is an injective
linear map, we see that M2 = R(K) is a un-knotted band in R3 and R3 \ M2 is homeomorphic
to R3 \ Σ2 . If R3 \ M1 and R3 \ M2 would be homeomorphic, then also R3 \ Σ1 and R3 \
Σ2 would be homeomorphic that is not possible by knot theory, see (Murasugi, 2008, Definition
1.3.1 and Theorem 1.3.1). This shows that there are no injective linear maps R : R2 → R3 and
homeomorphisms Φ : R3 → R3 such that (Φ ◦ R)(K) = M1.
Similar examples can be obtained in a higher dimensional case by using a knotted torus SeqUin
(2011)4 and their Cartesian products.
C.2.3 The proof of Example 1
Proof. (i) From (Puthawala et al., 2020, Theorem 15) we have that Ro,m can approximate
any continuous function f ∈ emb(Rn, Rm). Further, clearly (T1) and (T2) both contain
the identity map, thus Lemma 1 (i) applies.
(ii) Let Tm be the family autoregressive flows with sigmoidal activations defined in (Huang
et al., 2018). By (Teshima et al., 2020, App. G, Theorem 1 and Proposition 7), Tm are
sup-universal approximators in the space Diff2(Rm, Rm) of C 2 -smooth diffeomorphisms
Φ : Rm → Rm. When Ro,m is one of (R1) or (R2) the network is always linear, hence the
conditions are satisfied. If Ro,m is (R4), then Ro,m contains linear mappings, and if (R3),
then we can shift the origin, so that R(x) is linear on K. In all cases, Lemma 1 part (ii)
applies.
□
C.2.4 The proof of Lemma 2
Given an f ∈ emb∞ (K, Rm) we first show that for m ≥ 2n+ 1 there are always a diffeomorphisms
Ψ : Rm → Rm so that Ψ ◦ f : Rn → {0}n × Rm-n. The existence of such a Ψ borrows ideas from
Whitney’s embedding theorem (Hirsch, 2012, Theorems 3.4 & 3.5) and is constructed by iteratively
constructing an injective projection.
Next ifm - n ≥ 2n+ 1, then we can apply (Madsen et al., 1997, Lemma 7.6), a consequence of the
Tietze extension theorem, to show that Ψ : M → {0}n×Rm-n can be extended to a diffeomorphism
on the entire space, h : Rm → Rm. Hence f(x) = Ψ-1 ◦ h ◦ R(x) for diffeomorphism Ψ-1 ◦
h: Rm → Rm and zero-padding operator R: Rn → Rm, and thus f ∈ I∞ (K, Rm).
This fact that for m sufficiently large compared to n such a diffeomorphism can always be extended
is related to the fact that in 4-dimensions, all knots can be opened. This can be contrasted with the
case in Figure 2.
Now we present our proof.
Proof. Let use next prove Eq. 9 when m ≥ 3n + 1.Let
f ∈ embk(Rn,Rm)	(35)
be a Ck map and M = f(Rn) be an embedded submanifold ofRm.
4On the knotted torus, see http://gallery.bridgesmathart.org/exhibitions/
2011-bridges-conference/sequin.
17
Under review as a conference paper at ICLR 2022
We have that m ≥ 3n + 1 > 2n + 1. Let Sm-1 be the unit sphere of Rm and let
SRm = {(x,v) ∈Rm × Rm : kvk = 1}
be the sphere bundle of Rm that is a manifold of dimension 2m - 1. By the proof’s of Whitney’s
embedding theorem, by Hirsch, (Hirsch, 2012, Chapter 1, Theorems 3.4 and 3.5), there is a set
of ‘problem points’ H1 ⊂ Sm-1 of Hausdorff dimension 2n such that for all w ∈ Rm \ H1 the
orthogonal projection
Pw : Rm → {w}⊥ = {y ∈ Rm : y ⊥ w}
has a restriction Pw |M on M defines an injective map
Pw|M : M → {w}⊥.
Moreover, let TxM be the tangent space of manifold M at the point x and let us define another set
of ‘problem points’ as
H2 = {v ∈ Sm-1 : ∃x ∈ M,v ∈ TxM}.
For w ∈ Sm-1 \ H2 the map
Pw|M : M → {w}⊥ ⊂ Rm
is an immersion, that is, it has an injective differential. The sphere tangent bundle SM of M has
dimension 2n - 1, and the set H2 has the Hausdorff dimension at most 2n - 1. Thus H = H1 ∪ H2
has Hausdorff dimension at most 2n < m - 1 and hence the set Sm-1 \ H is non-empty. For
w ∈ Sm-1 \ H the map Pw|M : M → {w}⊥ is a Ck injective immersion and thus
N = Pw(M) ⊂{w}⊥
is a Ck submanifold.
Let Z : Pw(M) → M be the Ck function defined by
Z(y) ∈ M, Pw (Z(y)) = y,
that is it is the inverse of Pw M : M → Pw (M), where Pw (M) ⊂ {w}⊥. Let g : N = Pw (M) →
R be the function
g(J) = (Z(J)- y) ∙ w, y ∈ Pw(M).
Then N is a n-dimensional Ck submanifold of (m - 1)-dimensional Euclidean space H = {w}⊥
and g is a Ck function defined on it. By definition of a Ck submanifold of H, any point X ∈ N
has a neighborhood U ⊂ H with local Ck coordinates ψ : U → Rm such that ψ(N ∩ U)=
({0}m-1-n × Rn) ∩ ψ(U). Using these coordinates, we see that g can be extended to a Ck function
in U. Using a suitable partition of unity, we see that there is a Ck map G : {w}⊥ → R that a Ck
extension of g that is, G∣n = g.
Then the map
Φ1 : Rm →Rm,	Φ1(x) = x - G(Pw(x))w
is a Ck diffeomorphism ofRm that maps M to m - 1 dimensional space {w}⊥, that is
Φ1(M) ⊂ {w}⊥.
In the case when m ≥ 3n + 1, we can repeat this construction n times. This is possible as m - n ≥
2n + 1. Then we obtain Ck diffeomorphisms Φj : Rm → Rm, j = 1, . . . , n such that their
composition Φn ◦•••◦ Φι : Rm → Rm is a Ck-diffeomorphism such that which
Φn ◦…◦ Φι(M) ⊂ Y0,
where Y0 ⊂ Rm is a m — n dimensional linear space. By letting Ψ = Q ◦ Φn ◦ ∙∙∙ ◦ Φι for
rotation matrix Q ∈ Rm×m, we have that Y := Q(Y0) = {0}n × Rm-n. Also, letting φ : X =
Rn × {0}m-n → Rm be the map
φ(x, 0) = Ψ(f(x)) ∈Y,
where f is the function given in Eq. 35, we have that φ : X → Y is a Ck-diffeomorphism. We
observe that m - n ≥ 2n + 1 and so we can apply (Madsen et al., 1997, Lemma 7.6) to extend φ to
a Ck -diffeomorphism
h :Rm →Rm
18
Under review as a conference paper at ICLR 2022
such that h|X = φ. Note that (Madsen et al., 1997, Lemma 7.6) concerns an extension of a home-
omorphism, but as the extension h is given by an explicit formula which is locally a finite sum of
Ck functions, the same proof gives a Ck-diffeomorphic extension h to a diffeomorphism φ. This
technique is called the ‘clean trick’.
Finally, to obtain the claim, we observe that when R : Rn → Rm, R(x) = (x, 0) ∈ {0}n × Rm-n
is the zero padding operator, we have
f(x) = Ψ-1(φ(R(x))),	x ∈ Rn.
As h|X = φ and R(x) ∈ X, this yields
f(x) = Ψ-1(h(R(x))),	x ∈ Rn,
that is,
f = E ◦ R
where E = Ψ-1 ◦ h : Rm → Rm is a Ck diffeomorphism. Thus f ∈ Ik(Rn, Rm). This proves
Eq. 9 When m ≥ 3n + 1.	□
C.2.5 The proof of Lemma 3
The proof of Lemma 3. Let f = F2 ◦ F1 where F2 ∈ Fo,m and F1 ∈ Fn,o and > 0 be given, and
let Eo,m. Clearly, BK,W (f, E) ≤ BK,W (F2, E) and so by the m, o, o MEP of Eo,m with respect
to Fo,m, we have the existence of an rm ∈ emb(f (K), Eo,m) such that kI - rkL∞(f(K)) < .
Ko := (Eo,m)-1 ◦ r ◦ f(K) is compact, hence Eo,m is Lipschitz on Ko, so we can apply Lemma 7
point 4, so
BKW(f, Eom ◦ Ep，0) ≤kI — rkL∞(f(κ)) + Lip(E%m)Bκ,w((Eo,m)-1 ◦ r ◦ f, Epo). (36)
But, because f ∈ Fo,m◦Fn,o, we can choose a Ep,o ∈ Ef,o so that Bk,w ((Eo,m)-1 ◦r◦f, Ep,o) ≤
2 Lip(Eo,m) which, combined with Eqn. 36, proves the result.	□
C.2.6 The proof of Lemma 4
The proof of Lemma 4. Recall that F ⊂ emb(Rn, Rm). Suppose that E2o,m does not have the
m, n, o MEP with respect to F, then there are some > 0 and f ∈ F so that
∀Eo,m ∈ E2o,m ∀W1 ⊂⊂ Ro, BK,W1(f,E2o,m) ≥ .	(37)
From Lemma 7 point 2, we have that
≤BK,W1(f,E2o,m) ≤ BK,W (f, E2o,m ◦ E1p,o)	(38)
for all E1p,o ∈ E1p,o and for all compact sets W ⊂ Rp that satisfy E1p,o(W1) ⊂ W. We observe that
if W0 ⊂ Rp is a compact set such that W0 ⊂ W, we have
BKW(f, E0，m ◦ Ep,o) ≤ Bk,w0 (f, E0,m ◦ EP,o)
Thus, inequality Eq. 38 holds for all E1p,o ∈ E1p,o and for all compact sets W ⊂ Rp. Summarising,
we have seen that there are f ∈ F and > 0 such that for all E1p,o ∈ E1p,o and for all compact sets
W ⊂ Rp we have E ≤ Bk,w(f, E20,m ◦ Ep,o). Hence Ej,m does not have the m, n, o MEP with
respect to F, and we have obtained a contradiction, which proves the result.	□
C.3 Universality
C.3.1 The proof of Theorem 1
The proof of Theorem 1. First we prove the claim under the assumptions (i).
First we prove the claim under assumption (i).
Let W ⊂ Rn be an open relatively compact set. From Lemma 3 we have that
En，m := ELLfm ◦…。En,n1	(39)
19
Under review as a conference paper at ICLR 2022
has the m, n, n MEP w.r.t. F := FLLfrn ◦…oF，"1. Thus for any ∈ι > 0, We have an E ∈ En,m
s.t. BK,W (F, E ) < 1.
From Lemma 7 point 5, We have the existence of a μ0 ∈ P(W) so that W2 (F#仙,E#仙0) < 5
By convolving μ0 with a suitable mollifier φ, we can obtain a measure μ00 = μ0 * φ ∈ P (W) that is
absolutely continuous with respect to the Lebesgue measure so that
W (μ,μ0) <
e1
_	, n、
1 + Lip(E)
see (Ambrosio et al., 2008, Lemma 7.1.10.), and so W2 (E#m0,E#m") < e1. Hence,
W (F#M,E#m00) < 2e1.
(40)
Next, from universality of T0n for any 2 > 0, we have the existence of a T0 ∈ T0n so that
W2 (μ0, T0#m) < €2. From Lemma 7 points 3 and 4 we have that
W (F#〃, E ◦ T0#〃) ≤ 2€1 + €2 Lip(E).
(41)
For a given € > 0, choosing e1 < ∣ and €2 < 2(1+匕：P(E)) yields that the map E = E ◦ T0 ∈ E is
such that W2 (F#仙，E#m) < €. This yields the result.
Next we prove the claim under the assumptions (ii). By our assumptions, in the weak topology
of the space C2(Rnj , Rnj ), the closure of the set Tnj ⊂ C2(Rnj , Rnj ) contains the space of
Diff2 (Rnj , Rnj ). Moreover, by our assumptions Rnj-1 ,nj contains a linear map R. We observe
that as Rnj-1 ,nj is a space of expansive elements, the map R is injective. and hecne by Lemma 1,
the family
nj	-1	,nj
Ej
Tnj ◦ Rnj-1	,nj
has the MEP w.r.t. F = I1(Rn, Rm ). By Lemma 2, we have that I1(Rn, Rm ) coincides with
the space emb1(Rn, Rm ). Finally, by the assumption that T0n0 is dense in the space of C2-
diffeomorphism Diff2(Rn') implies that T0L0 is a Lp-UniVerSaI approximator for the set of C∞-
smooth triangular maps for allp < ∞. Hence by Lemma 3 in Appendix A of Teshima et al. (2020),
T0n0 is a distributionally universal. From these the claim in the case (ii) follows in the same way as
the case (i) using the family F = emb1(Rn, Rm).	□
C.3.2 The proof of Lemma 5
The proof of Lemma 5. The proof follows from taking the logical negation of the MEP for F. If the
MEP is not satisfied, then there is some f ∈ F so that BK,W (f, E) is never smaller than € > 0 for
all E ∈ E. Applying the definition of Bk,w (f, E) from Eqn. 6 yields the result.	□
C.3.3 The proof of Cor. 1
The proof of Cor. 1. The proof of Eqn 12 follows from the definition of the MEP.
The proof of Eqn. 13 follows from applying Lemma 7 point 4, and applying the same arguments
as in the proof of Theorem 1 so that limi→∞ Lip(石.)Bk,w(E-I ◦ r ◦ f, E0) → 0, and so that
W2 (F#仙，Ei ◦ Ei#M0) = €i where €i → 0 as i → ∞.. From the distributional universality of T0n,
and continuity of Ei ◦ Ei we have the existence of a Ti ∈ T0n so that W2 (F#仙,Ei ◦ Ei ◦2#Μ)=
2€i. Choosing €i so that limi→∞ €i yields the result.	□
Appendix D Additional Properties
D. 1 Layer-wise Projection
Here we provide the details of our closed-form layerwise projection algorithm The flow layers are
injective, and are often implemented to be numerically easy to invert. Thus, the crux of the algorithm
20
Under review as a conference paper at ICLR 2022
comes from inverting the injective expansive layers, R. The range of the ReLU layer is piece-wise
affine, hence the inversion follows a two-step program. First, identify which affine piece (described
algebraically, onto which sign pattern) to project. Second, project to this point using a standard
least-squares solver.
The second step is always straight-forward to analyze, but the first is more complicated.
B
The key step in our algorithm is the fact that for the specific choice of weight matrix W = -DB ,
given any y ∈ R2n , we can always solve the least-squares inversion problem exactly.
We prove this result in several parts given below.
1.	For any y ∈ R2n , My W ∈ Rn×n is full-rank.
2.	If [y]i 6= [y]i+n for each i = 1, . . . , n, then the argmin in Eqn. 17 is well defined, i.e. that
there is a unique minimizer. Otherwise there are 2I minimizers, where I is the number of
distinct i such that [y]i = [y]i+n .
3.	If My = [∆y (In×n - ∆y)], then
min l∣y - R(X)k2 = min IIMy (y - Wx)k2 + IIMy y||„.	(42)
x∈Rn	x∈Rn	2
4.	We verify Eqn. 17.
The proof of Theorem 2.	1. Using the definition of My, we have,
My -DB = (In×n - ∆y) B - ∆yDB = (In×n - ∆y- ∆yD) B. (43)
But, (I n×n - ∆y - ∆yD) is a full-rank diagonal matrix (with entries either 1 or [D]i,i),
B
and B is full rank by assumption, hence My -DB is too.
2. Because B is square and full rank there exists a basis5
{ bi}	of Rn such that
Dbj，biE = {0
ifi=j
if i 6= j .
(44)
For an x ∈ Rn, let αi = hx, bii for i = 1, . . ., n be the expansion ofx in the bi basis.
2n
xm∈iRnn ly - R(x)l22 = xm∈iRnn X [y - R(x)]i2
i=1
(45)
n
X min ([y]i - maχ(hχ,bii, O))2 + ([y]i+n - maχ(hχ,- [D]ii bii, O))2
xi∈R
i=1
(46)
We now consider minizing Eqn. 46 by minimizing the basis expansion in terms of αi ,
n
X min([y]i - maχ(αi, 0))2 + ([y]i+n - maχ(- [D]ii αi,0))2	(47)
αi∈R
i=1
Eqn. 47 is clearly minimized by minizing each term in the sum, hence we search for a
minimizer of the i’th term
min([y]i - maχ(αi,0))2 + ([y]i+n - maχ(- [D]ii αi, 0))2	(48)
αi∈R
5Namely the columns of the matrix B-1
21
Under review as a conference paper at ICLR 2022
Noting f (αi) as the quantity inside the minimum of Eqn. 48, we consider the positive,
negative and zero αi cases of Eqn. 48 separately and we get
min f(αi) = min ([y]i - αi)2 + [y]i2+n = [y]i2+n	(49)
αi ∈R+	αi ∈R+
min f(αi) = min [y]2 + ([y]i+η+ [D]ii αi)2 = [y]2	(5O)
αi∈R-	αi∈R+
f(0) = [y]i2 + [y]i2+n.	(51)
[y]2
If [y]i+n > [y]i, then the mmιmιzer of Eqn. 48 is αi = - D+n < 0. Conversely if
[y]i+n < [y]i then the minimzer of Eqn. 48 is αi = [y]i > 0. This argument applies all
i = 1, . . . , n, and hence if [y]i 6= [y]i+1 for all i = 1, . . . , n then the minimizing x is
unique.
If [y]i = [y]i+ι then there are exactly two minimizers of f (α∕ - %+n and [yb for both
ofwhichf(αi) = [y]i2 = [y]i2+n.
3.	Ifwe suppose that [y]i+n - [y]i > 0, then [c(y)]i = 0 and [c(y)]i+n > 0, thus [∆y]ii = 1,
hence if we let xmin be the minimizing x from part 1, then
([y]i - max(hxmin, bii, 0))2 + ([y]i+n - max(hxmin, - [D]ii bii , 0))2	(52)
=[y]2 + ([y]i+n - max( hxmin, - [D]ii bii , 0))2	(53)
=[Myy]2 + [My (y - Wxmin)]2	(54)
If [y]i+n - [y]i ≤ 0 then we have
([y]i - max(hxmin, bii, 0))2 + ([y]i+n - max(hxmin, - [D]ii bii , 0))2	(55)
= ([y]i - max(hxmin, bii , 0))2 + [y]i2+n	(56)
=[My (y - Wxmin)]2 + [Myy] j ∙	(57)
Thus combining Eqn.s 45, 46, 54 and 57 for each i = 1, . . . , n, we have that
min ky - R(x)k22 = min kMy (y - W x)k22 + kMyyk22 .	(58)
x∈Rn	x∈Rn
4.	For the final point, combining all of the above points we have
xm∈iRnn ky - R(x)k22 =xm∈iRnnkMy(y-Wx)k22.	(59)
Further we have from Point 1 that My W is full rank, hence (My W)-1 Myy = R(y) is a
minimizer of Eqn. 59. If [y]i = [y]i+n for all i = 1,...,n then Part 2 applies, and R (y)
is the unique minimizer of ky - R(x)k2. In either case, we have that R*(y) is a minimizer.
□
D.2 Bayesian Uncertainty Quantification
We now discuss the process for making the network amenable to Bayesian Uncertainty Quantifica-
tion (Bayesian UQ).
Assumption 2. Let y° ∈ Rn be given, let yι = T°yo and for each ' = 2,...,L, then let g` :=
R'(y'-1∕2) and y'-1∕2 ：= T'-1(y'-1). We assume thatfor ' = 1,...,L - 1, T'(y'), R'(y'-1∕2)
and T0 (y0) are differentiable.
22
Under review as a conference paper at ICLR 2022
Lemma 8. If a network F of the form of Eqn. 1 satisfies Assumption 2, then we have the following
upper bound on the log-likelihood,
1L
logPy(yo) ≤ logPz(F-1(yo)) + 2 ElOg
'=1
L
+ X log
'=0
det
∂⅛¾2(…)IddR/2 (y'-1/2)
de，(y').
∂y'
(60)
Proof. The proof of Lemma 8 follows from computing the log of,
Py(y0) = Pz(F-1(y0)) det
∂F T
∂y0 F (Rn)
(61)
and then applying definition 1 and expanding term using the laws of logarithms we obtain.
logPy(y) ≤ logPz(F-1(y)) + 1 Xlog det AdR'	(y'-i∕2) AdR' (y'-i∕2)
2 '=1	∂y'-i∕2	∂y'-i∕2
―S--------------------{z-----------------}
[R
L
+ X log
'=0、_
a t∂T'
detW—(y') .
∂y'
___ -	J
{z
[T
(62)
for ` = 1, . . . , L, see (Kothari et al., 2021, Appendix B) for details.
□
We note here that in Equation 60 we have an inequality rather than an equality. The inequality
comes from the expansive layers, as the flow-layers are all endomorphisms, i.e. Rn' → Rn'. We
now remark on the ease of approximating, or at least bounding, [T] and Ri in Eqn. 62.
Values for Rw when R is (R1) or (R2) are straightforward to compute or approximate6. When Rn,m
is (R3), R ∈ R are not differentiable, but are differentiable a.e.. Further, VRg(xn∕ can a.e. be
computed by applying a simple mask 0 and 1 mask to the weight matrices for an exact computation
of [R. Additionally, from Hutchinson (1989); Kothari et al. (2021) we have the following formula
that approximates [R,
log
1 ∂R'τ ∂R'
et而而
- Tr
≈ -Eν
(63)
where η ∈ Rn'. Computation of the [T] term is often easier, comparatively. This is because these
networks are often designed to make this value as easy to compute as possible. For example the
NICE Dinh et al. (2014), Real-NVP Dinh et al. (2016), and GLOW Kingma & Dhariwal (2018)
architectures are all designed so that [T] can be computed form a short scalar product.
D.3 Black-box recovery
We now discuss assumptions that enable black-box recovery of the weights of our entire network
post-training.
Assumption 3. For each ' = 1,...,L, r` is an affine ReLU layer. Each T and T is constructed
from a finite number of affine ReLU layers.
6for example from a LU decomposition, see (Golub, 1996, Section 3.2)
23
Under review as a conference paper at ICLR 2022
Remark 3. If a network F of the form of Eqn. 1 satisfies Assumption 3, then given the range of the
network, the range of the network can be recovered exactly.
Further, if the linear region assumption from Rolnick & Kording (2020) is satisfied, then the exact
weights are recovered, subject to two natural isometries discussed below.
Remark 4. The ReLU part of Assumption 3 is for all examples in Sec. 2.1. Further it is also
satisfied by both flows considered in Sec. 2.2, provided that the various gi are given by layers of
affine ReLU’s.
In Rolnick & Kording (2020), the authors show that, although ReLU networks depend on the value
of their weight matrix in non-linear ways, it is still possible to recover the exact weights of a given
ReLU network in a black-box way, subject to natural isometrics. The authors show that this is
possible not only in theory, but in numerical applications as well.
The works of Rolnick & Kording (2020); Bui Thi Mai & Lampert (2020) imply that provided the ac-
tivation functions of the expressive elements are ReLU then the entire network can be recovered in a
black-box way. Further, provided that either the ‘linear region assumption, from Rolnick & KOrding
(2020) or the generality assumption from Bui Thi Mai & Lampert (2020) is satisfied, then the entire
network can be recovered UniqUely modulo the natural isometries of rescaling and permutation of
weight matrices.
First we describe the two natural isometries of scaling and permutation. Consider the following
function
f(x) = W2φ(W1x)	(64)
where φ is coordinate-wise homogeneous degree 1 (such as ReLU) and W1 ∈ Rn1 ×n2 and W2 ∈
Rn2×n3. If we let P ∈ Rn2 ×n2 be any permutation matrix, and D+ be a diagonal matrix with
strictly positive elements, then we can write
f(x) = W2P0D+-1φ(D+PW1x)	(65)
as well. Thus ReLU networks can only ever be uniquely given subject to these two isometries.
When describe unique recovery in the rest of this section, we mean modulo these two isometries.
In Rolnick & KOrding (2020), the authors describe how all parameters of a ReLU network can be
recovered uniquely (called reverse engineered in Rolnick & KOrding (2020)), subject to the so called
‘linear7 region assumption,, LRA.
The input space Rn can be partitioned into a finite number of open {Si}in=i 1, where for each k,
F(x) = Wki + bi, i.e. the network corresponds to an affine polyhedron in the output space. The
algorithms (Rolnick & Kording, 2020, Alg.s 1&2)are roughly described below.
First, identify at least one point within each affine polyhedra {Hj}jn=j 1. Then identify the boundaries
between polyhedra. The boundaries between sections are always one affine ‘piece, of piecewise
hyperplanes {Hj}jn=j 1. These {Hj}jn=j 1 are the central objects which indicate the (de)activation of
an element of a ReLU somewhere in the network. If the Hj are full hyperplanes, then the ReLU
that is (de)activates occurs in the first layer of the network. If Hj is not a full hyperplane, then it
necessarily has a bend where it intersects another hyperplane Hj0. Further, except for a Lebesgue
measure 0 set, when Hj intersects Hj 0 the latter does not have a bend. If this is the case, then
Hj0 corresponds to a ReLU (de)activation in an earlier layer than Hj . In this way the activation
functions of every layer can be deduced. Once this is done, the normals of the hyperplanes can be
used to infer the row-vectors of the various weight matrices, letting one recover the entire network.
The above algorithm recovers all of the weights exactly provided that the LRA is satisfied. The
LRA is satisfied if for every distinct Si and Si0, either Wi 6= Wi0 or bi 6= bi0. That is, different
sign patterns produce different affine sections in the output space. This is a natural assumption, as
the algorithm as described above reconstruction works by first detecting the boundaries between
adjacent affine polyhedra, which is only possible if the LRA holds.
Given the weights of a network there is currently no simple way to detect if the LRA is satisfied, to
our knowledge. Nevertheless the authors of Rolnick & Kording (2020) show that if it is satisfied,
7The use of ‘linear, in this context is somewhat non-standard, and instead means affine. In this section we
use the term 'linear region assumption,, but use 'affine, where Rolnick & Kording (2020) would use 'linear, to
preserve mathematical meaning.
24
Under review as a conference paper at ICLR 2022
then unique recovery follows. Nevertheless recovery of the range of the entire network is possible,
but this recovery may not be unique.
In Bui Thi Mai & Lampert (2020) the authors also consider the problem of recovering weights of a
ReLU neural network, however the authors therein study the question of When there exist isometries
beyond the two natural ones described above. In particular the main result (BUi Thi Mai & Lampert,
2020, Theorem 1) shows the following. Let En0 ,nL be a ReLU network that is L layers deep and
non-increasing. Suppose that E1, E2 ∈ En0,nL, E1 and E2 are general8 and for all x ∈ Rn0
E1 (x) = E2 (x), then E1 is parametrically identical to E2 subject to the two natural isometries.
This work provides the stronger result, however does not apply to the networks that we consider
out of the box. It does apply to our expressive elements (provided that they use ReLU activation
functions, and are non-increasing), but not necessarily apply to the network on the whole.
8A set it in a topology is general if it’s complement is closed and nowhere dense.
25