Under review as a conference paper at ICLR 2022
Stability Based Generalization Bounds
for Exponential Family Langevin Dynamics
Anonymous authors
Paper under double-blind review
Ab stract
We study the generalization of noisy stochastic mini-batch based iterative algo-
rithms based on the notion of stability. Recent years have seen key advances in
data-dependent generalization bounds for noisy iterative learning algorithms such
as stochastic gradient Langevin dynamics (SGLD) based on (Mou et al., 2018; Li
et al., 2020) and related approaches (Negrea et al., 2019; Haghifam et al., 2020).
In this paper, we unify and substantially generalize stability based generalization
bounds and make three technical advances. First, we bound the generalization
error of general noisy stochastic iterative algorithms (not necessarily gradient de-
scent) in terms of expected stability, which in turn can be bounded by the expected
Le Cam Style Divergence (LSD). Such bounds havea O(1∕n) sample dependence
unlike many existing bounds with O(1∕√n) dependence. Second, We introduce
Exponential Family Langevin Dynamics (EFLD) which is a substantial general-
ization of SGLD and which allows exponential family noise tobe used with gradi-
ent descent. We establish data-dependent expected stability based generalization
bounds for general EFLD. Third, we consider an important new special case of
EFLD: noisy sign-SGD, which extends sign-SGD by using Bernoulli noise over
{-1, +1}, and we establish optimization guarantees for the algorithm. Further,
we present empirical results on benchmark datasets to illustrate the our bounds
are non-vacuous and quantitatively much sharper than existing bounds.
1	Introduction
Recent years have seen renewed interest and advances in characterizing generalization performance
of learning algorithms in terms of stability, which considers change in performance of a learning
algorithm based on change ofa single training point (Hardt et al., 2016; Bousquet & Elisseeff, 2002;
Li et al., 2020; Mou et al., 2018). For stochastic gradient descent (SGD), Hardt et al. (2016) estab-
lished generalization bounds based on uniform stability (Hardt et al., 2016; Bousquet & Elisseeff,
2002), although the analysis needed rather small step sizes ηt = 1/t which is not useful in practice.
While improving the analysis for SGD has remained a challenge, advances have been made on noisy
SGD algorithms, especially stochastic gradient Langevin dynamics (SGLD) (Welling & Teh, 2011;
Mou et al., 2018; Li et al., 2020), which adds Gaussian noise to the stochastic gradients of marginal
variance σt2 . In parallel, there has been key developments on related information-theoretic general-
ization bounds applicable to SGLD type algorithms (Negrea et al., 2019; Haghifam et al., 2020; Xu
& Raginsky, 2017; Russo & Zou, 2016; Pensia et al., 2018).
While these developments have led to major advances in analyzing generalization of noisy SGD
algorithms, they each have certain limitations which leave room for further improvements. Using
uniform stability, Mou et al. (2018) established a bound for SGLD of the form Kn VPt η2∕σ2 which
depends on K, the global Lipschitz constant for the loss, and with step size ηt ≤ σt ln 2/K. The
bound has a desirable dependency of O(1/n) on the samples, but has an undesirable dependence on
K, and the step sizes, bounded by σt∕K, are too small. Mou et al. (2018) also presented another
bound which addresses some of these issues, but gets an undesirable O(1/√n) sample dependence.
By building on the developments of Russo & Zou (2016); Xu & Raginsky (2017); Pensia et al.
(2018), Negrea et al. (2019) made advances from the information theoretic perspective and estab-
lished bounds for SGLD which have the desirable dependence on the norm of gradient incoherence,
i.e., difference in gradients over different mini-batches, avoids dependence on Lipschitz constant
K, and is applicable to unbounded sub-Gaussian losses, but have an undesirable O(1∕√n) sample
1
Under review as a conference paper at ICLR 2022
dependence. Haghifam et al. (2020) made further advances on the problem from an information the-
oretic perspective based on the conditional mutual information framework of Steinke & Zakynthinou
(2020) and obtained generalization bounds based on gradient incoherence with O(1/n) sample de-
pendence, but their analysis holds for full batch Langevin dynamics, not mini-batch SGLD. Li et al.
(2020) made advances on such bounds based on the notion of Bayes-Stability, by combining ideas
from PAC-Bayes bounds into stability, and established a bound of the form n，Pt η2ge(t)∕σ2 for
bounded losses, where ge (t) is the expected gradient norm square at step t. While the bound avoids
dependency on the Lipschitz constant K, the dependence on the gradient norm makes such bounds
much weaker than the information theoretic bounds of Negrea et al. (2019); Haghifam et al. (2020)
which depend on the norm of gradient incoherence, which are typically orders of magnitude smaller.
Further, the analysis of Li et al. (2020) still needs small step sizes, bounded by σt∕K.
In this paper, we build on the core strengths of such existing approaches, most notably the O(1∕n)
sample dependence of stability based bounds (Mou et al., 2018; Li et al., 2020) and the depen-
dence on gradient incoherence for information theoretic bounds (Negrea et al., 2019; Haghifam
et al., 2020), and develop a framework (Section 2) for developing generalization bounds for noisy
stochastic iterative (NSI) algorithms. Our framework considers generalization based on the concept
of expected stability, rather than uniform stability (Hardt et al., 2016; Bousquet & Elisseeff, 2002;
Bousquet et al., 2020; Mou et al., 2018), which yields distribution dependent generalization bounds
and avoids the worst-case setting of uniform stability. Building on Li et al. (2020), we show that
expected stability of general NSI algorithms can be bounded by the expected Le Cam Style Diver-
gence with dependence on parameter distributions from mini-batches differing by one sample. In
Section 3, we introduce Exponential Family Langevin Dynamics (EFLD), a family of noisy gradient
descent algorithms based on exponential family noise. Special cases of EFLD include SGLD and
noisy versions of Sign-SGD or quantized SGD algorithms widely used in practice (Bernstein et al.,
2018a;b; Jin et al., 2020; Alistarh et al., 2017)Our main result provides an expected stability based
generalization bound applicable to any EFLD algorithm with a O(1∕n) sample dependence and a
dependence on gradient incoherence, rather than gradient norms. Existing generalization bounds for
SGLD (Li et al., 2020; Negrea et al., 2019) usually use properties of the Gaussian distribution, and
our analysis on EFLD illustrates that this was unnecessary. We also consider optimization guaran-
tees for EFLD and establish such results for noisy Sign-SGD and SGLD. Through experiments on
benchmark datasets (Section 4), we illustrate that our bounds are non-vacuous and quantitatively
much sharper than existing bounds (Li et al., 2020; Negrea et al., 2019).
Related work. Uniform stability has been a classical approach for bounding generalization error
(Bousquet & Elisseeff, 2002; Bousquet et al., 2020; Feldman & Vondrak, 2018; 2019), pioneered by
Rogers & Wagner (1978); Devroye & Wagner (1979). Beyond the aforementioned work, there has
been recent work on differential privacy that analyzes the uniform stability of differentially private
SGD (DP-SGD) (Hardt et al., 2016; Bassily et al., 2020). Beyond uniform stability, information-
theoretic approaches (Russo & Zou, 2016; Xu & Raginsky, 2017) that bounds the generalization
error by the mutual information between the algorithm input S and the algorithm output w, have
been used for deriving generalization bounds for noisy iterative algorithms (Pensia et al., 2018; Bu
et al., 2019). Along this line of literature, Negrea et al. (2019); Haghifam et al. (2020); Rodriguez-
Galvez et al. (2021) prove data-dependent generalization bounds dropping dependence on the Lips-
chitz constant. Further, tighter bounds (Haghifam et al., 2020; Zhou et al., 2021; Rodriguez-Galvez
et al., 2021; Neu, 2021; HellStrOm & Durisi, 2021) are proposed based on conditional mutual infor-
mation (Steinke & Zakynthinou, 2020; GrUnWaId et al., 2021; HellStrOm & Durisi, 2020). Due to
space limitations, an extended discussion of the related work is deferred to Appendix A.
2	Generalization Bounds with Expected Stability
In the setting of statistical learning, there is an instance space Z, a hypothesis space W, and a loss
function ' : W×Z → R+. Let D be an unknown distribution of Z and let S 〜Dn be n i.i.d. draws
from D. For any specific hypothesis w ∈ W, the population and empirical loss are respectively
given by LD(W)，Ez〜D['(w, z)], and LS(W)，1 Pn=1 '(w, Zi). For any distribution P over
the hypothesis space, we respectively denote the expected population annd empirical loss as
LD(P) , Ez〜DEw〜p['(w,z)] , and	LS(P)，1 XEw〜p['(w,r)] .	(1)
n
i=1
Consider a randomized algorithm A which works with S = {zι,...,Zn} 〜 Dn and cre-
2
Under review as a conference paper at ICLR 2022
ates a distribution over the hypothesis space W. For convenience, we will denote the distribu-
tion as A(S). The focus of the analysis is to bound the generalization error of A defined as:
gen(A(S)) , LD (A(S)) - LS (A(S)) . We will assume A is permutation invariant, i.e., the or-
dering of samples in S do not modify A(S), an assumption satisfied by most learning algorithms.
We will focus on developing bounds for the expectation ES [LD (A(S)) - LS(A(S))], and discuss
high-probability bounds in the Appendix B.
2.1	B ounds based on Expected Stability
We start our analysis by noting that the expected generalization error can be upper bounded by
expected stability based on the Hellinger divergence (Sason & Verdu, 2016; Li et al., 2020):
H2(PkP) = 1 Rw(Pp(W) - pp0(W))2dw∙
Proposition 1. Let	Sn	〜Dn	and let	Sn	be a dataset obtained by replacing	Zn	∈	Sn	with
Zn 〜D. Let A(Sn),A(Sn) respectively denote the distributions over the hypothesis space W
obtained by running randomized algorithm A on Sn,Snb. Assume that for Sn 〜Dn, ∀z ∈ Z,
EW〜A(sn)['2(W, z)] ≤ co/2, co > 0. With H(∙, ∙) denoting the Hellinger divergence, we have
|ESn 〜Dn [Ld (A(Sn)) - LS (A(Sn))] | ≤ C0ESn 〜DnEzn 〜D ,2H 2(A(Sn),A(Sn)).	⑵
Remark 2.1. Proposition 1 does not need bounded losses. Just the SeCOndmOment of '(W,z),W 〜
A(Sn), Sn 〜Dn, ∀z ∈ Z need to be bounded. The assumption is satisfied by bounded losses. It is
instructive to compare the assumption to that in recent information theoretic bounds (Haghifam et al.,
2020; XU & Raginsky, 2017), where one assumes '(w, Z),Z 〜D, ∀w ∈ W to be sub-Gaussian.
Remark 2.2. The bound in Proposition 1 is in terms of expected stability where we consider
ES〜DnEzn〜D [∙∙∙ ],an important departure from bounds based on uniform stability (Elisseeff et al.,
2005; Bousquet & Elisseeff, 2002; Mou et al., 2018; Bousquet et al., 2020; Feldman & Vondrak,
2018; 2019) where one considers sups,so∈zn,∣s∖so∣=ι […].Replacing SuP by E makes the bounds
distribution dependent, and arguably leads to quantitatively tighter bounds.
Note that the Hellinger divergence can be bounded by the KL divergence.
Proposition 2. For any distributions P and P0, 2H2 (P, P0) ≤ min {KL(P, P0), J2KL(P, P0)}.
2.2	Expected Stability of Noisy Stochastic Iterative Algorithms
We consider a general family of noisy stochastic iterative (NSI) algorithms. Given S 〜Dn, such
iterative algorithms have two additional sources of randomness in each iteration t: (a) a stochastic
mini-batch of samples SBt, with |SBt | = b, drawn uniformly at random with replacement from
S; and (b) noise ξt suitably included in the iterative update. Given a trajectory of past iterates
Wo：(t-i)= wo：(t—i), the new iterate Wt is drawn from a distribution「石亡白皿0:υ—1)over W:
Wt 〜PBt,ξt∣W0*t-ι) (W).	⑶
We will drop the conditioning w0:(t-1) to avoid clutter in the sequel. Let PT, PT0 denote the marginal
distributions over hypotheses w ∈ W after T steps of the algorithm based on Sn , Sn0 respec-
tively. Further, let P0:(t-1) denote the joint distribution over W0:(t-1) = (W0, . . . , Wt-1), and
let Pt ≡ PBt,ξt∣w0.(t-i) compactly denote the conditional distribution on Wt conditioned on the
trajectory W0:(t-1) = w0:t-1. Following (Negrea et al., 2019; Haghifam et al., 2020; Pensia et al.,
2018), we use the following chain rule for KL-divergence: KL(PT kPT0 ) ≤ KL(P0:T kP00:T) =
PT=I Epo：(t-i)[KL(Pt∣ ∣∣P∕∣)]. Let S 〜Dn+1, and let Sn Sn be size n subsets of S such that
Sn = {Z1, . . . , Zn-1, Zn} and Sn0 = {Z1, . . . , Zn-1, Zn0 }, where Zn0 = Zn+1. Let S0 =
{Z1, . . . , Zn-1}. The algorithms we consider use a mini-batch of size b in each iteration uniformly
sampled from n samples. Let the set of all mini-batch index sets be denoted by G. Let the set of all
mini-batch index sets A drawn from S0 be denoted by G0. Note that |G0| = n-b1 . Let G1 denote
the set of all mini-batch index sets B which includes the last sample, viz. Zn for S with mini-batches
and Zn for Sn. Note that |Gi| = ɑn-l). Alsonotethat |Go| + |Gi| = (n-1) + ɑn-l) = Gn) = |G|.
Following Li et al. (2020), We can bound their conditional KL-divergences KL(Pt ∣Pt.∣) in terms
of a Le Cam Style Divergence (LSD). While the classical Le Cam divergence (Sason & Verdu,
3
Under review as a conference paper at ICLR 2022
2016) is LCD(PkP0)，1 R IddP+ddP (where dP denotes the density), our bounds in terms of
LSD(Pt川典1)，R "PBttdPAdPBt,ξ'，Bt ∈ Gι,At ∈ Go. Note that Pβt,ξt and PBt,ξt represent
the distribution of Wt for Sn and Sn0 respectively since the mini-batch SBt of Sn and Sn0 differs in
the n-th sample. Putting everything together, we have the following LSD based bound.
Lemma 1. Consider a noisy stochastic iterative algorithms of the form (3) with mini-batch size
b ≤ n/2. Then, with ci = √2co (with co as in Proposition 1), we have
b	3	∖ f (dPBt,ξt- dPBt,ξt)2
∣Esn[Ld(A(Sn))-LSn(A(Sn))]∣≤ CLESnEz；	EEE E / ʌ——后-------------------------------Udξt
n n nt t=1 W0gi)Bt∈G1At∈G0 JW	dPAt,ξt
Remark 2.3. Li et al. (2020) essentially has this result for SGLD and inspired our work. Our proofs
are significantly simpler and, more importantly, illustrates applicability to general noisy iterative
algorithms of the form (3), not just SGLD with Gaussian noise as in Li et al. (2020).
Remark 2.4. Note that the bound does not assume the loss to be bounded, depends on expectations
over samples Sn, zn0 , trajectories wo:(t-1), and mini-batches Bt, At. Further, the bound depends on
the distribution discrepancy as captured by the expected LSD.
Remark 2.5. The bound seems to worsen with b, the size of the mini-batch. As we shown in Sec-
tion 3, the expected LSD term has a b2 dependence for the Exponential Family Langevin dynamics
(EFLD) models we introduce, so the leading b is neutralized.
3	Exponential Family Langevin Dynamics
In recent years, considerable advances have been made in establishing generalization bounds for
stochastic gradient Langevin dynamics (SGLD) (Li et al., 2020; Pensia et al., 2018; Negrea et al.,
2019; Haghifam et al., 2020). As an example of NSI algorithms of the form (3), SGLD adds an
isotropic Gaussian noise at every step of SGD: wt+i = Wt - ηtV'(wt, Sb)+ N(0, σ2Id), where
V'(wt, SBt) is the stochastic gradient on mini-batch Bt, η is the step size, and σ2 is noise vairance.
In this paper, we introduce a substantial generalization of SGLD called Exponential Family
Langevin Dynamics (EFLD) which uses general exponential family noise in noisy iterative updates
of the form (3). In addition to being a mathematical generalization of the popular SGLD, the pro-
posed EFLD provides flexibility to use noise gradient algorithms with different representation of the
gradient, e.g., Bernoulli noise for Sign-SGD, discrete distribution for quantized or finite precision
SGD, etc. (Canonne et al., 2020; Alistarh et al., 2017; Jiang & Agrawal, 2018; Yang et al., 2019).
3.1	Exponential Family Langevin Dynamics (EFLD)
Exponential families (Barndorff-Nielsen, 2014; Brown, 1986; Wainwright & Jordan, 2008) consti-
tute a large family of parametric distributions which include Gaussian, Bernoulli, gamma, Pois-
son, Dirichlet, etc., as special cases. Exponential families are typically represented in terms of
natural parameters θ, and we consider component-wise independent distributions with scaled nat-
ural parameter θα = θ∕α with scaling α > 0, i.e., pψ(ξ, θα) = exp(hξ, θɑi 一 ψ(θɑ))∏o(ξ)=
Qjp=i exp(ξjθjα - ψj(θjα))πo(ξj) , where ξ is the sufficient statistic, ψ(θα) = Pjp=i ψj(θjα) is
the log-partition function, and πo(ξ) = Qjp=i πo(ξj) is the base measure. Note that α = 1 gives
the canonical form of the exponential family distributions. For general scaling α > 0, for some
cases the base measure πo may depend on the scaling, i.e., πo,α . A scaling α > 0 is valid as long
as exp(hξ, θαi is integrable, i.e., ξ exp(hξ, θαiπo(ξ)dξ < ∞. Further, ψ is a smooth function by
construction (Barndorff-Nielsen, 2014; Banerjee et al., 2005; Wainwright & Jordan, 2008) and the
smoothness of ψ implies V2ψ(θα) ≤ c2I.
Exponential family Langevin dynamics (EFLD) uses noisy stochastic gradient updates similar to
SGLD, but using exponential family noise rather than Gaussian noise as in SGLD. In particular, for
mini-batch SBt, EFLD updates are as follows: with step size ρt > 0
〜
wt = wt-i - ρtξt ,	ξt
where
pψ(ξ; θBt,αt) = exp(hξ, θBt,αt i-ψ(θBt,αt))πo(ξ) ,
pψ (ξ; θBt,αt ) ,
θBt,αt，^Bt
t, t
αt
(5)
"I，SBt) . (6)
αt
4
Under review as a conference paper at ICLR 2022
For EFLD, the natural parameter θBt,αt at step t is simply a scaled version of the mini-batch gradient
▽'(Wt-1 ,SBt). We first show that EFLD becomes SGLD when the exponential family is Gaussian,
and becomes a noisy version of sign-SGD (Bernstein et al., 2018a;b) when the exponential family is
Bernoulli over {-1, +1}. More details and examples are in Appendix C.1.
Example 3.1 (SGLD). SGLD uses scaled Gaussian noise with ψ(θ) = ∣∣θk2∕2, at = σtσt∕ηt,
so that pψ(ξ; θBt,ɑt) = N(θBt,α2Id). By taking Pt = √ηtσt, the update (5) based on PtEt is
distributed as N(ρtθBt,ρ2ɑ2Id) = N(ηtV'(wt-ι,SBt),σt2Id). Thus the EFLD update reduces to
the SGLD update: Wt = wt-ι 一 ηN'(wt-ι,SB) + N(0, σ2Id) .	□
Example 3.2 (Noisy Sign-SGD). By taking Pt = ηt and component-wise ξj ∈ {一1, 1}, π0(ξj) =
1, ψ(θ) = log(exp(-θ)+exp(θ)) in exponential family update equation (5), the j-th component of
exponential family distribution pψ (ξ; θBt,αt ) becomes pθB ,α ,j (ξj)
_______exp(£j θBt,αt,j)______
exP(-θBt,ɑt,j )+eχp(θBt,αt,j )
Thus, the EFLD update reduces to a noisy version of Sign-SGD: Wt = wt-ι 一 ηtξt, ξt,j 〜
PθBt,αt,j (ξj), j ∈ [d], where θBt,αt = V'(wt-ι, SBt )∕αt is the scaled mini-batch gradient. □
3.2	Expected Stability of Exponential Family Langevin Dynamics
From Lemma 1, conditioned on a trajectory w0:(t-1), mini-batches SBt , SAt, we can get gener-
alization bound by suitably bounding the Le Cam Style Divergence (LSD) given by: IAt ,Bt =
L (dpBt,dP-d"ξJ dξt . For EFLD, the density functions dPBt ,ξt are exponential family densi-
ties pψ(ξ; θBt,αt) as in (5)-(6), and we have the following bound on the per step LSD:
Theorem 1. For a given set S 〜 Dn+1 and wt-ι at iteration (t — 1), let ∆t∣wt-ι (S) =
maXz,zo∈S ∣∣V' (wt-ι, Z) — V' (w-ι, z0)∣∣2 . Further, for a c2 -smooth log-partition function ψ,
let the scaling at∣wι be data-dependent such that a^wt_ɪ ≥ 8。242W l (Sn+ι). Then, we have
lAt,Bt ≤ 5c2∣θBt,at- θB0 ,αtk2 = 2a5^ [W' (wt-1 ,Sb, ) - V' (wt-1,SB t )∣∣2i ,	(7)
at∣Wt-ι
Note that SBt and SB0 only differ at samples zn and zn0 . The above bound can now be directly
applied to Lemma 1 to get expected stability based generalization bounds for any EFLD algorithm.
Theorem 2. Consider an exponential family Langevin dynamics (EFLD) algorithm oftheform (5)-
(6) with a c2-smooth log-partitionfunction ψ. Then, for mini-batch size b ≤ n/2, with C = co√5c2
(with co as in Lemma 1) and α^∣ ≥ 8cz∆2∣ (Sn+1) (as in Theorem 1, with the COnditiOning on wt-ι
hidden to avoid clutter), we have
IES [LD(A(S S)TS(A(S))收 Cn S4
T1
X W0E-1)“[kV' (Wt-1,zn)— V' (Wtizn )k2i .
(8)
Remark 3.1. Theorem 2 captures the generalization error of SGLD, which is a special case of
EFLD. Our bound has the same dependence on n, T , step size ηt as the bound in Li et al.
(2020). However, our bound is numerically sharper because we replace the gradient norms, i.e.,
n Pz∈s k'(wt, z)∣ in Li et al. (2020) and with gradient discrepancy ∣V'(wt,z) — V'(wt, z0)∣,
which is quantitatively smaller than gradient norms as we show in the experiment section. The
bound in Negrea et al. (2019) depends on gradient incoherence which is empirically smaller than
gradient discrepancy as observed in the experiment section, their bound depends on 1/ √n, which is
worse than the 1/n dependence in our bound.
Remark 3.2. EFLD can be extended to work with anisotropic noise by using θBt,αt =
V'(wt-1, SBt) αt in (6) where αt ∈ Rp determines different scaling for each dimension and
denotes Hadamard division. Theorems 1 and 2 can be extended to such anisotropic noise by using
α-scaled norms for the gradient discrepancy, i.e., ∣g — g0∣∣2,α = Pj(gj — gj)2/αj.	□
Remark 3.3. The condition on αt is a data-dependent quantity, which can be computed along the
training process. It gives much more benign condition of the step size compared to those in the
related work (Mou et al., 2018; Li et al., 2020, Hardt et al. 2016), which require step size being
bounded by σJL. However, the step sizes in Theorem 2 need to be bounded by σt∕∆t(S), which
is considerably more relaxed since ∆t (S) is much smaller than Lipschitz constant L, which is a
uniform bound over the whole parameter space. Also, usually one would expect ∆t (S) to decrease
as training proceeds since the gradients shrink as the loss function being minimized. Thus, the
constraint on step size does not require the step sizes to be as small as σt∕L.
5
Under review as a conference paper at ICLR 2022
3.3	Proof S ketches of Main Results: Theorems 1 and 2
We focus on Theorem 1. To avoid clutter, we drop the subscript t for the analysis and note that
the analysis holds for any step t. When the density dPB,ξ = pψ (ξ; θB,α), by mean-value theorem,
1'	1 J=	1	J= Zl	∖	/J= Λ	∖	/ Λ	Λ	X—7	/ 大	∖ ∖ Γ∙
for each ξ, We have pψ(ξ; θB,α) - pψ(ξ; θB,,α) = hθB,α - θB,α, NθB,αPψ(ξ; θB,α)i, for some
θB,α = γξθB,α + (1 - γξ)θB0 ,α Where γξ ∈ [0, 1]. Then,
IA,B =	ξ
(pψ (ξ; θB,α) - pψ (ξ; θB0,α))
pψ (ξ; θA,α)
2	h hθB,α — θB ,α ξ - Vθβ cιψ(ξ; θB,α)i2 pψ (ξ^B,α)
-dξ=X---------------p⅛α------------------dξ,
since pψ (ξ; θB,α) = exp(hξ, θB,αi - ψ(θB,α))π0(ξ).
ɪɪ Ii. iʌ* ， r ，♦	■ iʌ	1	«• Λ	TL τ	.1	1 • . 1 . 1	1	•.1
Handling Distributional Dependence of θB . Note that We cannot proceed With the analysis With
the density term depending on Θb° since Θb° depends on ξ. So, we first bound the density term
depending on Θb° in terms of exponential family densities with parameters Θb° and θB,a using
c2-smoothness of ψ.
Lemma 2. For some γξ ∈ [0, 1], θB,α = γξ θB,α + (1 - γξ)θB0 ,α, we have
exp[hξ, θB,ɑi - ψ(<^B,α)]	, JUC C U2I
max (exp [<ξ, Bb,。〉- Ψ(Θb,°)] , exp [hξ, Bb，,。〉- Ψ(θB0,α)]) ≤ exp ∣c2kθB,α - θB0,αk^ .
Bounding the Density Ratio. Next we focus on the density ratio pψ(ξ, θB,α)∕pψ(ξ; Ba,。). By
Lemma 2, it suffices to focus onpψ (ξ, θB,α)∕pψ (ξ; Ba,。)or the equivalent term for Bb，,。. We show
that the density ratio can be bounded by another exponential family with parameters (2BB,。 -BA,。).
Lemma 3. For any ξ, we have
exp[hξ, 2Bb,。〉- 2ψ(θB,α)]
exp[hξ, θA,αi - Ψ(θA,α)]
≤ exp [2c2 kθB,α - θA,αk2] exp[hξ, (2θB,α - θA,αi - ψ(2θB,α - θA,α)] .
The analysis for the termpψ (ξ, Bb，,。)* (ξ; Ba,。)is exactly the same.
Bounding the Integral. Ignoring multiplicative terms which do not depend
moment, the analysis needs to bound an integral term
of the form	ξ hBB,。
on ξ for the
- BB0 ,。 , ξ -
vψ(ξ; Bb,。)〉2 pψ(E;2Bb,。- θA,α)dξ, and a similar term with pK(E;2Bb，,。- Ba,。). First,
note that Vψ(ξ; Bb,。)= μb,。，the expectation parameter for pψ (ξ; Bb,。)WainWnght & Jordan
(2008); Banerjee et al. (2005). The integral, however, is with respect to pψ (ξ; 2BB,。 - BA,。).
We handle this discrepancy by using ξ - Vψ(ξ; Bb,。) = (ξ - E[ξ]) + (E[ξ] - Vψ(ξ; Bb,。)),
and decomposing as sum-of-squares. Quadratic form for the first term yields the covariance
E[(ξ - E[ξ])(ξ - E[ξ])T] = V2ψ(B2θB,α-θA,α ) ≤ c2I, by smoothness. The second term de-
pends on the difference of gradients V@(2Bb,。一 Ba,。)一 V@(Bb,。)which, using smoothness and
additional analysis, can be bounded by the norm of (BB,。 -BA,。). All the pieces can be put together
to get the bound in Theorem 1, which when used in Lemma 1 yields Theorem 2.
3.4	Optimization Guarantees for EFLD
We now establish optimization guarantees for two examples of EFLD, i.e., Noisy Sign-SGD with
Bernoulli noise over {-1, +1} and SGLD with Gaussian noise.
Noisy Sign-SGD. For mini-batch Bt and scaling αt , mini-batch Noisy Sign-SGD updates the pa-
rameters as wt = wt-1 - ηtξt, where each component j ∈ [d]
e	( 、_ ________exp(XBBt,atj )________
ξtj ~pθBt,αt,j (X)= exp(-θBt,。tj ) + exp(θBt,。tj ) , X ∈ {- ' + }	⑼
where Bbj。= V'(wt-ι, Sb)® is the scaled mini-batch gradient. The full-batch version uses
parameters EBt[Bb5。/ = VLS(Wt-I) For the optimization analysis, we assume that the loss is
smooth and mini-batch gradients are unbiased, symmetric, and sub-Gaussian.
6
Under review as a conference paper at ICLR 2022
Assumption 1. The loss function LS satisfies: for all w and w0, for some non-negative constant
K := [Ki,..., Kd], we have LS (W) ≤ LS (w0) + NLS (w0)T(w - w0) + 1 Pi Ki(Wi- wi)2.
Assumption 2. Given wt-i, the mini-batch gradient V'(wt-ι, SBt) is (a) unbiased, i.e.,
EBt∣wt-ιV'(wt-ι,SBt)	= VLS(Wt-I); (b) symmetric, i.e., the density p(x) of X ≡
V'(wt-ι,SBt) is symmetric around its expectation LS(Wt-I): p(x) = p(2VLs(Wt-I) — x) and
(C)Sub-GauSsian, i.e., for any λ > 0, any V s.t. ∣∣vk2 = 1, EBt∣wt-ι exp λ(v, V'(wt-ι,SBt)—
VLS(Wt-i)i ≤ exp(λ2κ2∕2) for some constant κt > 0.
Based on the assumptions, we have the following optimization guarantee for mini-batch noisy Sign-
SGD. We defer the optimization guarantee for full-batch noisy Sign-SGD to Appendix D.
Theorem 3.	Under Assumption 1 and 2,for mini-batch noisy Sign-SGD with step size η = 1∕√T,
at satisfying C ≥ ɑt ≥ max[√2κt, 4∣VLs (Wt-I)∣∞], we have for any S and any initialization
w0	1 T	4c	1
E T X ∣∣vls(Wt)∣2 ≤ √τ LSs(WO)- LS(w ) + 2kκki) ,	(IO)
where the expectation is taken over the randomness of algorithm.
SGLD. We acknowledge that the following optimization result of SGLD exists in various forms, as
noisy gradient descent algorithms have been studied in literature such as differential privacy, where
SGLD can be viewed as DP-SGD (Bassily et al., 2014; Wang & Xu, 2019) and the proof technique
boils down to bounding the stochastic variance of the noisy gradient (Shamir & Zhang, 2013).
Theorem 4.	Under Assumption 1 and 2, with Ki = K, ∀i ∈ [d], for any S, SGLD (EFLD with step
size Pt = √ηtσt, αt = vtσt∕)]t), |Bt | = b, and η = √T, can achieve
1T
T x
t=1
E∣VLS(wt)∣2 ≤ O
+ O(KpP=√4+⅛T
(11)
where the expectation is over the randomness of the algorithm.
The error rate of SGLD depends on the noise variance at. One can choose a decaying noise variance
such as ɑt = 1/4t to guarantee the convergence. Then the rate will become O (log T∕√T). We
note that similar to the optimization guarantees of DP-SGD, the convergence rate depends on the
dimension of the gradient p due to the isotropic Gaussian noise. Special noise structures such as
anisotropic noise that aligned with the gradient structure can reduce the dependence on dimension
(Kairouz et al., 2020; Zhang et al., 2021; Asi et al., 2021; Zhou et al., 2020).
4	Experiments
In this section, we conduct a series of experiments to evaluate our generalization error bounds. For
SGLD, we aim to compare the proposed bound in Theorem 2 with existing bounds in Li et al. (2020),
Negrea et al. (2019), and Rodriguez-Gaivez et al. (2021) for various datasets. Note that the bound
presented in Rodrlguez-Galvez et al. (2021) is an extension of that in Haghifam et al. (2020) from
full-batch setting to mini-batch setting . We also evaluate the optimization performance of proposed
Noisy Sign-SGD by comparing it with the original sign-SGD (Bernstein et al., 2018a) and present
the corresponding generalization bound in Theorem 2.
The details of our model architectures, learning rate scheduling, hyper-parameter selections and
additional experimental results can be found in Appendix E. We acknowledge that we did not achieve
the state-of-the art predictive performance, mainly due to the simplicity of our model architectures.
With more complex model and further tuning, the prediction results could be improved.
4.1	Stochastic Gradient Langevin Dynamics
Comparison with existing work. We have derived theoretical generalization error bounds that
depend on the data-dependent quantity gradient discrepancy, i.e., ∣V' (wt, Zn) - V' (wt, Zn)∣2.
Existing bounds inLi et al. (2020) and Negrea et al. (2019) have also improved the Lipschitz constant
in Mou et al. (2018) to a data-dependent quantity. As shown in Figure 1 (a)-(d), by combining with
the empirical training error, all four generalization error bounds can be used to bound the empirical
test error, but our bound is able to generate a much tighter upper bound. Such difference is mainly
due to the fact that we replace the squared gradient norm in Li et al. (2020), the squared norm of
7
Under review as a conference paper at ICLR 2022
一一 Train Error
—— Test Error
---- Etrain+ LietaL bound
---- EtraMt-Negrea et al. bound
---- ≡tra∕∏+ Our bound
---- Etrain+ ROdrigUeZ-GaIVeZ et ad
No. Of Epochs
(a) MNIST, αt2 ≈ 0.1
(b) CIFAR-10, αt2 ≈ 0.1
(c) Fashion, αt2 ≈ 0.1
No. of Epochs
(d) Fashion, α2 ≈ 0.01
■ 0	20	40
No. of Epochs
(e) MNIST, α2 ≈ 0.1
0	250	500	750	1000
No. of Epochs
j(f) CIFAR-10, 02 ≈ 0.1
0	20	40
No. of Epochs
Fashion, α2 ≈ 0.1
00-2TY
Iooo
1 1 1
=aIU-IoN4->u ①一 pe-lω
20	40
No. of Epochs
0 2 4 6
O - - -
Iooo
111
JQ) E-ION Lpe-Iω
No. of Epochs
250	500	750	1000
20	40
No. of Epochs
Fashion, α2 ≈ 0.01
0 2 4 6
O - - -
Iooo
111
JQ) E-ION Lpe-Iω
(l) Fashion, αt2 ≈
(i) MNIST, αt2 ≈ 0.1	(j) CIFAR-10, αt2 ≈ 0.1	(k) Fashion, αt2 ≈ 0.1
40
0.01
Figure 1: Numerical results for training CNN using SGLD (σt = "(Int/βt) on MNIST, FaShion-
MNIST and CIFAR-10. X-axis shows the number of training epochs. (a)-(d) shows our bound is
non-vacuous and can be used to bound the empirical test error. (e)-(h) compare our bound with
the existing bounds and show the effect on αt2. (i)-(l) show the key factors in each bound, i.e.,
the squared gradient norm in Li et al. (2020), the gradient incoherence in Negrea et al. (2019),
the two-sample incoherence in Rodriguez-Gaivez et al. (2021), and the gradient discrepancy in our
bound. Our bounds are numerically sharper than existing bounds, and larger αt2 leads to tighter
generalization bounds which is consistent with the theoretical analysis.
gradient incoherence in Negrea et al. (2019), and that of two-sample incoherence in Rodriguez-
Galvez et al. (2021) with the gradient discrepancy. Results in Figure 1 (e)-(h) show that our bounds
are much sharper than those of Li et al. (2020) because our gradient discrepancy (Figure 1 (i)-(l))
is usually 2-4 order of magnitude smaller than the squared gradient norms appeared in Li et al.
(2020). Our bounds are also sharper than those of Negrea et al. (2019) and Rodriguez-GaIVeZ et al.
(2021) due to an improved dependence on n from an order of 1/ʌ/n to 1/n. Note that, even though
the gradient incoherence in Negrea et al. (2019) is about 1 to 2 order of magnitude smaller than
the gradient discrepancy for simple problems such as MNIST and Fashion-MNIST, the difference
between the gradient incoherence and our gradient discrepancy reduces as the problem becomes
harder (see results for CIFAR-10 in Figure 1(j)).
Effect of Randomness. Motivated by Zhang et al. (2017), we train CNN with SGLD on a smaller
subset of MNIST dataset (n = 10000) with randomly corrupted labels. The corruption fraction
varies from 0% (without label corruption) to 60%. As shown in Figure 2 (d), for long enough training
time, all experiments with different level of label randomness can achieve almost zero training error.
However, the one with higher level of randomness has higher generalization/test error (Figure 2 (a)
dashed lines). Our generalization bound also becomes larger as the randomness increases since the
corresponding gradient discrepancy increases.
4.2	NOISY SIGN-SGD
Optimization. Figure 3 (a)-(d) show the training dynamics of Noisy Sign-SGD under various se-
lections of αt . As αt → 0, Noisy Sign-SGD matches both the optimization trajectory as well as the
final test accuracy of the original Sign-SGD (Bernstein et al., 2018a). However, as αt increases, the
probability of getting 1 approaches 0.5, and ξt approximates a uniform distribution. As a result, the
corresponding Noisy Sign-SGD still converges, but the generalization performance is much worse.
8
Under review as a conference paper at ICLR 2022
(PeLISeP),IO-LIW ⅛φπ
(P=OS) P①PUnOG
No. of Epochs
=N k)6 I (UZqM
」0」」山6u-u-sl
(a) Bounded Test Error (b) Our Bound (c) Gradient Discrepancy (d) Training Error
Figure 2: Numerical results for training CNN using SGLD (σt = 0.2ηt) on a subset of MNIST (n =
10000) with different randomness on labels. (a) demonstrates that, as the randomness increases, the
empirical test error (dashed lines) increases but still can be bounded by our generalization bound by
combining the empirical training error (solid lines). (b) presents our bound in Theorem 2. (c) shows
the gradient discrepancy ∣∣V' (Wt,zn) — V' (Wt, z：)k2. (d) plots the training error. The gradient
discrepancy increases as randomness increases, so does our generalization bound.
(a) CNN, MNIST
(e) MNIST, αt = 1
(b) CNN, Fashion (c) ResNet-18, CIFAR10 (d) ResNet-18, CIFAR100
Figure 3: (a)-(d) show the training dynamics of CNN on MNIST and Fashion-MNIST, and ResNet-
18 on CIFAR-10 and CIFAR-100 using noisy sign-SGD with different scaling αt . Legends indicate
the choice of αt and the numbers in brackets are test errors at convergence. As αt → 0, Nosiy
sign-SGD matches both the optimization trajectory as well as the final test accuracy of the original
sign-SGD (Bernstein et al., 2018a). (e)-(f) show that empirical test error can be bounded by our
bound and the corresponding training error. The larger αt is the sharper our bound is.
Generalization Bound. Figure 3(e)-(f) show that our bound successfully bounds the empirical test
error. The larger αt is the sharper the upper bound is. However, larger αt would slow down and
adversely affect the optimization, e.g., Figure 3 (a)-(d) blue and orange lines. In practice, one needs
to balance the optimization error and generalization by choosing a suitable scaling αt .
5	Conclusions
Inspired by recent advances in stability based and information theoretic approaches to generalization
bounds (Mou et al., 2018; Pensia et al., 2018; Negrea et al., 2019; Li et al., 2020; Haghifam et al.,
2020), we have presented a framework for developing such bounds based on expected stability for
noisy stochastic iterative (NSI) learning algorithms. We have also introduced Exponential Family
Langevin Dynamics (EFLD), a family of noisy gradient descent algorithms based on exponential
family noise, including SGLD and Noisy Sign-SGD as two special cases. We have developed an
expected stability based generalization bound applicable to any EFLD algorithm with a O(1/n)
sample dependence and a dependence on gradient incoherence, rather than gradient norms. Further,
we have provided optimization guarantees for EFLD and establish such results for Noisy Sign-SGD
and SGLD. Our experiments on various benchmarks illustrate that our bounds are non-vacuous and
quantitatively much sharper than existing bounds (Li et al., 2020; Negrea et al., 2019).
9
Under review as a conference paper at ICLR 2022
References
Dan Alistarh, Demjan Grubic, Jerry Li, Ryota Tomioka, and Milan Vojnovic. Qsgd:
Communication-efficient sgd via gradient quantization and encoding. In I. Guyon, U. V. Luxburg,
S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett (eds.), Advances in Neural
Information Processing Systems 30, pp. 1709-1720. Curran Associates, Inc., 2017.
Hilal Asi, John Duchi, Alireza Fallah, Omid Javidbakht, and Kunal Talwar. Private adaptive gradient
methods for convex optimization. In International Conference on Machine Learning, pp. 383-
392. PMLR, 2021.
Arindam Banerjee, Srujana Merugu, Inderjit S Dhillon, and Joydeep Ghosh. Clustering with breg-
man divergences. Journal of machine learning research, 6(10), 2005.
Ole Barndorff-Nielsen. Information and exponential families: in statistical theory. John Wiley &
Sons, 2014.
Raef Bassily, Adam Smith, and Abhradeep Thakurta. Private empirical risk minimization: Efficient
algorithms and tight error bounds. In 2014 IEEE 55th Annual Symposium on Foundations of
Computer Science, pp. 464-473. IEEE, 2014.
Raef Bassily, Vitaly Feldman, Kunal Talwar, and Abhradeep Guha Thakurta. Private stochastic
convex optimization with optimal rates. Advances in neural information processing systems,
2019.
RaefBassily, Vitaly Feldman, Cristobal Guzman, and KUnal TalWar Stability of stochastic gradient
descent on nonsmooth convex losses. Advances in Neural Information Processing Systems, 33,
2020.
Jeremy Bernstein, Yu-Xiang Wang, Kamyar Azizzadenesheli, and Animashree Anandkumar.
signsgd: Compressed optimisation for non-convex problems. In International Conference on
Machine Learning, pp. 560-569. PMLR, 2018a.
Jeremy Bernstein, JiaWei Zhao, Kamyar Azizzadenesheli, and Anima Anandkumar. signsgd With
majority vote is communication efficient and fault tolerant. In International Conference on Learn-
ing Representations, 2018b.
Stephane Boucheron, Gabor Lugosi, and Pascal Massart. Concentration inequalities: A nonaSymp-
totic theory of independence. Oxford university press, 2013.
Olivier Bousquet and Andre Elisseeff. Stability and generalization. Journal of Machine Learning
Research, 2:499-526, 2002.
Olivier Bousquet, Yegor Klochkov, and Nikita Zhivotovskiy. Sharper bounds for uniformly stable
algorithms. In Conference on Learning Theory, pp. 610-626. PMLR, 2020.
LaWrence D BroWn. Fundamentals of statistical exponential families: With applications in statistical
decision theory. Ims, 1986.
Yuheng Bu, Shaofeng Zou, and Venugopal V Veeravalli. Tightening mutual information based
bounds on generalization error. In 2019 IEEE International Symposium on Information Theory
(ISIT), pp. 587-591. IEEE, 2019.
Mark Bun, Cynthia DWork, Guy N Rothblum, and Thomas Steinke. Composable and versatile
privacy via truncated cdp. In Proceedings ofthe 50th Annual ACM SIGACT Symposium on Theory
of Computing, pp. 74-86, 2018.
Clement L Canonne, Gautam Kamath, and Thomas Steinke. The discrete gaussian for differential
privacy. In NeurIPS, 2020.
Xiangyi Chen, Tiancong Chen, Haoran Sun, ZhiWei Steven Wu, and Mingyi Hong. Distributed
training With heterogeneous data: Bridging median-and mean-based algorithms. arXiv preprint
arXiv:1906.01736, 2019.
10
Under review as a conference paper at ICLR 2022
Luc Devroye and Terry Wagner. Distribution-free inequalities for the deleted and holdout error
estimates. IEEE Transactions on Information Theory, 25(2):202-207,1979.
Andre Elisseeff, Theodoros Evgeniou, Massimiliano Pontil, and Leslie Pack Kaelbing. Stability of
randomized learning algorithms. Journal of Machine Learning Research, 6(1), 2005.
Vitaly Feldman and Jan Vondrak. Generalization bounds for uniformly stable algorithms. In Pro-
ceedings of the 32nd International Conference on Neural Information Processing Systems, pp.
9770-9780, 2018.
Vitaly Feldman and Jan Vondrak. High probability generalization bounds for uniformly stable al-
gorithms with nearly optimal rate. In Conference on Learning Theory, pp. 1270-1279. PMLR,
2019.
Peter Grunwald, Thomas Steinke, and Lydia Zakynthinou. Pac-bayes, mac-bayes and Condi-
tional mutual information: Fast rate bounds that handle general vc classes. arXiv preprint
arXiv:2106.09683, 2021.
Mahdi Haghifam, Jeffrey Negrea, Ashish Khisti, Daniel M Roy, and Gintare Karolina Dziugaite.
Sharpened generalization bounds based on conditional mutual information and an application to
noisy, iterative algorithms. Advances in Neural Information Processing Systems, 2020.
Moritz Hardt, Ben Recht, and Yoram Singer. Train faster, generalize better: Stability of stochastic
gradient descent. In International Conference on Machine Learning, pp. 1225-1234, 2016.
Fredrik Hellstrom and Giuseppe Durisi. Generalization bounds via information density and condi-
tional information density. IEEE Journal on SelectedAreas in Information Theory, 1(3):824-839,
2020.
Fredrik HellStrOm and Giuseppe Durisi. Fast-rate loss bounds via conditional information measures
with applications to neural networks. In 2021 IEEE International Symposium on Information
Theory (ISIT),pp. 952-957. IEEE, 2021.
Peng Jiang and Gagan Agrawal. A linear speedup analysis of distributed deep learning with sparse
and quantized communication. In S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-
Bianchi, and R. Garnett (eds.), Advances in Neural Information Processing Systems 31, pp. 2525-
2536. Curran Associates, Inc., 2018.
Chi Jin, Rong Ge, Praneeth Netrapalli, Sham M Kakade, and Michael I Jordan. How to escape saddle
points efficiently. In International Conference on Machine Learning, pp. 1724-1732, 2017.
Chi Jin, Praneeth Netrapalli, Rong Ge, Sham M Kakade, and Michael I Jordan. On nonconvex
optimization for machine learning: Gradients, stochasticity, and saddle points. arXiv preprint
arXiv:1902.04811, 2019.
Richeng Jin, Yufan Huang, Xiaofan He, Tianfu Wu, and Huaiyu Dai. Stochastic-sign sgd for feder-
ated learning with theoretical guarantees. arXiv preprint arXiv:2002.10940, 2020.
Peter Kairouz, Monica Ribero, Keith Rush, and Abhradeep Thakurta. Dimension independence in
unconstrained private erm via adaptive preconditioning. arXiv preprint arXiv:2008.06570, 2020.
Alex Krizhevsky. Learning Multiple Layers of Features from Tiny Images. Technical Report Vol.
1. No. 4., University of Toronto, 2009.
Yann LeCun, Leon Bottou, Yoshua Bengio, and Patrick Haffner. Gradient-based learning applied to
document recognition. Proceedings of the IEEE, 86(11):2278-2324, 1998.
Jian Li, Xuanyuan Luo, and Mingda Qiao. On generalization error bounds of noisy gradient methods
for non-convex learning. In International Conference on Learning Representations, 2020. URL
https://openreview.net/forum?id=SkxxtgHKPS.
Wenlong Mou, Liwei Wang, Xiyu Zhai, and Kai Zheng. Generalization bounds of sgld for non-
convex learning: Two theoretical viewpoints. In Conference on Learning Theory, pp. 605-638.
PMLR, 2018.
11
Under review as a conference paper at ICLR 2022
Jeffrey Negrea, Mahdi Haghifam, Gintare Karolina Dziugaite, Ashish Khisti, and Daniel M Roy.
Information-theoretic generalization bounds for sgld via data-dependent estimates. In Advances
in Neural Information Processing Systems, 2019.
Gergely Neu. Information-theoretic generalization bounds for stochastic gradient descent. arXiv
preprint arXiv:2102.00931, 2021.
Ankit Pensia, Varun Jog, and Po-Ling Loh. Generalization error bounds for noisy, iterative algo-
rithms. In 2018 IEEE International Symposium on Information Theory (ISIT), pp. 546-550. IEEE,
2018.
David Pollard. A user’s guide to measure theoretic probability. Number 8. Cambridge University
Press, 2002.
Borja Rodriguez-Galvez, German Bassi, Ragnar Thobaben, and MikaeI Skoglund. On random
subset generalization error bounds and the stochastic gradient langevin dynamics algorithm. In
2020 IEEE Information Theory Workshop (ITW), pp. 1-5. IEEE, 2021.
William H Rogers and Terry J Wagner. A finite sample distribution-free performance bound for
local discrimination rules. The Annals of Statistics, pp. 506-514, 1978.
Daniel Russo and James Zou. Controlling bias in adaptive data analysis using information theory.
In Artificial Intelligence and Statistics, pp. 1232-1240. PMLR, 2016.
Igal Sason and Sergio Verdu. f -divergence inequalities. IEEE Transactions on Information Theory,
62, 2016.
Ohad Shamir and Tong Zhang. Stochastic gradient descent for non-smooth optimization: Conver-
gence results and optimal averaging schemes. In International conference on machine learning,
pp. 71-79. PMLR, 2013.
Thomas Steinke and Lydia Zakynthinou. Reasoning about generalization via conditional mutual
information. In Conference on Learning Theory, pp. 3437-3452. PMLR, 2020.
Alexandre B Tsybakov. Introduction to nonparametric estimation. Springer Science & Business
Media, 2008.
Martin J Wainwright and Michael Irwin Jordan. Graphical models, exponential families, and vari-
ational inference. Now Publishers Inc, 2008.
Di Wang and Jinhui Xu. Differentially private empirical risk minimization with smooth non-convex
loss functions: A non-stationary view. In Proceedings of the AAAI Conference on Artificial Intel-
ligence, volume 33, pp. 1182-1189, 2019.
Max Welling and Yee W Teh. Bayesian learning via stochastic gradient langevin dynamics. In
International Conference on Machine Learning, ICML ’11, pp. 681-688, 2011.
Han Xiao, Kashif Rasul, and Roland Vollgraf. Fashion-mnist: a novel image dataset for benchmark-
ing machine learning algorithms, 2017.
Aolin Xu and Maxim Raginsky. Information-theoretic analysis of generalization capability of learn-
ing algorithms. Advances in Neural Information Processing Systems, 2017:2525-2534, 2017.
Guandao Yang, Tianyi Zhang, Polina Kirichenko, Junwen Bai, Andrew Gordon Wilson, and Christo-
pher De Sa. Swalp: Stochastic weight averaging in low-precision training. 36th International
Conference on Machine Learning (ICML), 2019.
Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals. Understanding
deep learning requires rethinking generalization. In 5th International Conference on Learning
Representations, ICLR 2017, Toulon, France, April 24-26, 2017, Conference Track Proceedings.
OpenReview.net, 2017. URL https://openreview.net/forum?id=Sy8gdB9xx.
Huanyu Zhang, Ilya Mironov, and Meisam Hejazinia. Wide network learning with differential pri-
vacy. arXiv preprint arXiv:2103.01294, 2021.
12
Under review as a conference paper at ICLR 2022
Ruida Zhou, Chao Tian, and Tie Liu. Individually conditional individual mutual information bound
on generalization error. In 2021 IEEE International Symposium on Information Theory (ISIT),
pp. 670-675. IEEE, 2021.
Yingxue Zhou, Steven Wu, and Arindam Banerjee. Bypassing the ambient dimension: Private sgd
with gradient subspace identification. In International Conference on Learning Representations,
2020.
13