Under review as a conference paper at ICLR 2022
Variance Reduced Domain Randomization for
Policy Gradient
Anonymous authors
Paper under double-blind review
Ab stract
By introducing randomness on environment parameters that fundamentally affect
the dynamics, domain randomization (DR) imposes diversity to the policy trained
by deep reinforcement learning, and thus improves its capability of generalization.
The randomization of environments, however, introduces another source of vari-
ability for the estimate of policy gradients, in addition to the already high variance
due to trajectory sampling. Therefore, with standard state-dependent baselines,
the policy gradient methods may still suffer high variance, causing low sample ef-
ficiency during the training of DR. In this paper, we theoretically derive a bias-free
and state/environment-dependent optimal baseline for DR, and analytically show
its ability to achieve further variance reduction over the standard constant and
state-dependent baselines for DR. We further propose a variance reduced domain
randomization (VRDR) approach for policy gradient methods, to strike a tradeoff
between the variance reduction and computational complexity in practice. By di-
viding the entire space of environments into some subspaces and estimating the
state/subspace-dependent baseline, VRDR enjoys a theoretical guarantee of faster
convergence than the state-dependent baseline. We conduct empirical evaluations
on six robot control tasks with randomized dynamics. The results demonstrate
that VRDR can consistently accelerate the convergence of policy training in all
tasks, and achieve even higher rewards in some specific tasks.
1 Introduction
Deep reinforcement learning (DRL) has achieved an impressive success on complex sequential
decision-making tasks, such as playing video games at top human-level (Ye et al., 2020), contin-
uous robot manipulation (Agarwal et al., 2021) and traffic signal control (Egeaa et al., 2021). For
an extensive exploration, DRL requires a massive amount of samples to train the policy, which is
usually done in a simulator constructed for the task. Due to the lack of diversity, however, policy
trained by DRL tends to overfit to a specific training environment (Cobbe et al., 2019), causing
possibly a severe performance degradation (i.e., reality gap) when the policy learned in simulator is
transferred to the real deployment (Peng et al., 2018; Kang et al., 2019). To close this gap, domain
randomization (DR) has been proposed to randomize the environment parameters that may vary the
dynamics and observation in simulator, imposing diversity on the trained policy (Tobin et al., 2017;
Jiang et al., 2021). Recent success in robot control has shown that DR enables the trained policy to
generalize in real deployment, even with presence of fundamental modeling errors in the simulator
(Muratore et al., 2021; Xie et al., 2020). DR has also demonstrated its capability of robustness to
contend with extremely rare environments (Muratore et al., 2021).
Direct applications of DR on policy gradient methods can be thought as policy optimization on
multiple randomly generated environments, which incurs additional variability of the observed data.
Therefore, a critical challenge is the low sample efficiency due to the extremely high variance of
the gradient estimator, where the variability not only stems from the gradient approximation of
expected return from a batch of sampled trajectories (Greensmith et al., 2004), but is also imposed
by the randomization of environment parameters. In standard DRL, a commonly used method to
reduce variance is to construct a bias-free and action-independent baseline that is subtracted from
the expected return (Sutton & Barto, 2018). For example, a typical choice of baseline is the state
value function of a policy. When directly applied to DR, as proposed by Andrychowicz et al. (2020),
it is equivalent to learning a state value function that predicts the expected return over all possible
1
Under review as a conference paper at ICLR 2022
environments. Though remaining unbiased, such a state-dependent baseline may be a poor choice
in DR, since the additional variability of randomized environments is not taken into account.
In this paper, we aim to address the high variance issue of policy gradient methods for domain ran-
domization, with a particular focus on reducing the additional variance imposed by randomization
of environments. Our key insight is that the additional information on varying environments can be
incorporated into the baseline to further reduce this variance. Theoretically, we derive the optimal
state/environment-dependent baseline, and demonstrate that it improves consistently the variance
reduction over baselines that is constant or uses state information only. In order for the practical im-
plementation to strike the tradeoff between the variance reduction performance and computational
complexity for maintaining the state/environment-dependent baselines, we propose a variance re-
duced domain randomization (VRDR) approach for policy gradient methods, which can improve
the sample efficiency while maintaining a reasonable number of baselines associated with a set of
specifically designed environment subspaces. Our main contributions can be summarized as follows.
•	Theoretically optimal state/environment-dependent baseline for DR. For the policy gradient
in a variety of environments that differ in dynamics, we derive theoretically an optimal baseline
that depends on both the states and environments. We further quantify the variance reduction
improvement achieved by the proposed state/environment-dependent baseline over two common
choices of baselines for DR, i.e., the constant and state-dependent baselines.
•	Criterion for constructing practical state/subspace-dependent baseline. Since the accurate
estimation of state/environment-dependent baseline for each possible environment is infeasible
during practical implementation of RL, we propose to alternatively divide the entire space of
environment parameters into a limited number of subspaces, and estimate instead the optimal
baseline for every pair of state and environment subspace. We further show that the clustering of
environments into subspaces should follow the policy’s expected returns on these environments,
which can guarantee an improvement of variance reduction over the state-dependent baseline.
•	Variance reduced domain randomization (VRDR) with empirical evaluation. To strike the
tradeoff between the variance reduction performance and computational complexity for maintain-
ing optimal baselines, we develop a variance reduced domain randomization (VRDR) approach
for policy gradient methods. Specifically, VRDR learns an acceptable number of baselines for
each pair of state and environment subspace, where the environment subspaces are determined
based on the above criterion. We then conduct experiments on six robot control tasks with their
fundamental physical parameters randomized, demonstrating that VRDR can accelerate the con-
vergence of policy training in DR settings, as compared to the standard state-dependent baselines.
In some specific tasks, VRDR can even achieve a higher reward.
2	Background
Notation. Under the standard reinforcement learning (RL) setting, the environment is modeled
as a Markov decision process (MDP) defined by a tuple < S, A, Tp, Φ >, where S and A de-
note the state and action spaces, respectively. For the convenience of derivation, we assume that
they are finite. Tp : S × A × S → [0, 1] is the environment transition model that is essen-
tially determined by environment parameter p ∈ P, with P denoting the space of environment
parameters. In robot control, for example, environment parameter p can be a vector contain-
ing the rolling friction of each joint and the mass of torso and arms. Throughout the rest of
this paper, by environment p we mean that the environment has dynamics determined by param-
eter p. Φ : S × A → R is the reward function. At each time step t, the agent observes its
state St ∈ S and takes an action at ∈ A under the guidance of policy ∏θ⑶曲)Parameter-
ized by θ. It will then receive a reward rt = Φ(st, at), while the environment shifts to the next
state st+1 with probability T(st+1|st, at,p). The goal of standard RL is to search for a policy π
that maximizes the expected discounted return η(π, p) = Eτ [R(τ)] over all possible trajectories
τ = {st, at, rt, st+1}t∞=0 of states and actions, where R(τ) = Pt∞=0 γtrt and γ ∈ [0, 1] is the dis-
CoUnt factor. We can then define the state value function as Vn (s,p) = E [P∞=0 γkrt+k |st = s], the
action value function as Qπ (s, a,p) = E Pk∞=0 γkrt+k |st = s, at = a , and the advantage function
as Aπ (s, a, p) = Qπ (s, a, p) - Vπ (s, p).
Policy gradient methods for DR. In DR, the environment parameter p is a random variable that fol-
lows the probability distribution P over P. For the convenience of derivation, we assume a finite en-
2
Under review as a conference paper at ICLR 2022
vironment parameter space P with cardinality |P |. By introducing DR, the goal of policy optimiza-
tion is to maximize the expected return over all possible environment parameters: EP〜P [η(∏,p)].
The policy gradient with action-independent baseline (Sutton & Barto, 2018) can be formulated as:
NBEP〜P [η(π,P)] = EP Eμ∏ ,∏
Vθ log ∏θ(a|s) [Q∏(s,a,p) - b],
(1)
where We define μ∏ (S) = P∞=0 YtP∏(St = s|p) as the discounted state visitation frequency,
with Pπ(st = s|p) denoting the probability of shifting from the initial state s0 to state s af-
ter t steps under policy π in environment p. For convenience, we further denote g(θ, S, a, p) ,
Vθ log∏θ(a∣s)[Q∏(s, a,p) 一 b], which is the gradient estimator for the state-action pair under en-
vironment p. As long as the baseline b is action-independent, we have Ea[Vθ log∏θ(a∣s)b]=
VθEa[b] = 0 and thus E%n,∏[g] = Ep,“n,∏ [Vθ log∏(a∣s)Q∏(s, a,p)]，E[g], where the sub-
script is dropped for convenience. Therefore, by subtracting this action-independent baseline b, the
variance of gradient estimator V ar(g) = E[gTg] 一 E[g]TE[g] can be reduced without introducing
any bias (Greensmith et al., 2004).
3	Optimal Baselines for Domain Randomization
To derive the optimal baselines for DR, we formulate the following optimization problem that aims
to minimize the variance of gradient estimate w.r.t. the baseline b (Greensmith et al., 2004):
2
min E G(a, S) Qπ (S, a, p) 一 b
b J	- _
{z^
gTg
一 E[g]TE[g] ⇔ min Ep,μ∏ ,∏
G(a, S)Qπ(S, a,p) 一 b2
(2)
、
where we denote G(a,s) ，Vθ log∏θ(a∣s)τVθ log∏θ(a|s). Due to its independence of b, the
second term E[g]TE[g] does not affect the minimizer, and is thus omitted on RHS of Eq. (2). In
the following, we first derive for DR two common choices of baselines that are constant or depend
on the state only. We then propose the optimal state/environment-dependent baseline, and show its
ability to further reduce the variance incurred by the randominzation of environment parameters.
3.1	Two Common Choices of Action-Independent Baselines
Optimal constant baseline. We first consider a constant baseline bc that depends neither on the ac-
tion nor on the state. The optimization problem in Eq. (2) then becomes minimizing the expectation
of a quadratic function, which can be proved to be convex. Referring to the detailed derivation in
Appendix A.1, the optimal constant baseline bC for DR-based policy gradient methods is:
优
EP,“∏ ,π [G(a, s)Qn(S, a,p)]
EP,μ∏,π [G(a,s)]
EP,μ∏ [V0(s,P)].
(3)
This optimal baseline bC can be understood as the expected state value function V0(s,p) over
all states and possible environments, where the state value function V0(S, p) is computed as the
weighted average of the action value function:
V0(S,P) = En [ep.p ∏[G(a,s)]Qn(S,a,P)].
Optimal state-dependent baseline. As in the standard RL, the utilization of state-dependent base-
line in the DR setting can be considered as an expected value function b(S) = EP Eπ [Qπ (S, a, P)]
for each state over all possible environments, which predicts the expected return over the distribution
of dynamics caused by the variation of environment parameters. Still referring to Appendix A.1, the
optimal state-dependent baseline for DR is given as follow:
b*(S)
EP (p|s) Eπ [G(a, S)[Qπ (S, a, P)]]
Eπ[G(a, S)]
(4)
3.2	Optimal State/Environment-Dependent Baseline
In DR, the randomization of environment parameters imposes additional variability for the estimate
of gradients, which aggravates the instability of RL training process. In the following subsections,
3
Under review as a conference paper at ICLR 2022
we show that this variance can be effectively reduced by further incorporating into the baseline the
information about varying environments. To this end, we propose a state/environment-dependent
baseline b(s, P) = {b(s,pi)}|iP=|1 for DR, and rewrite the variance minimization in Eq. (2) as:
矗呼)Ep〜Pw∏ [E∏「(a，s)[Q∏(s, a,P) - b(s,P)][].
(5)
In Appendix A.1, following the similar deduction as in (Greensmith et al., 2004), the optimal
state/environment baseline can be derived as:
b*(4 s, P) = {b*(s,Pi)}ip1, where b (S,Pi) = %，：，S)Qn(S，；，"",	⑹
Eπ G(a， s)
which indicates requirements of maintaining a specific baseline for each environment to incorporate
the environment-specific information. Note that if environment parameter pi is time-varying along
episodes and considered as stochastic input process that partially affects the dynamics, this optimal
baseline b*(s, P) is equivalent to the optimal input-driven baseline as derived by Mao et al. (2θ18).
3.3	Variance Reduction Improvement of b*(s, P)
Theorem 1.	Let V arb(s) (g) and V arb (s,P) (g) denote the variance of gradient estimator by incor-
porating an arbitrary state-dependent baseline b(S) and the optimal state/environment-dependent
baseline b*(s, P), respectively. Compared to the state-dependent baseline b(s), the variance can be
further reduced by b*(s, P), with thefollowing improvement:
2
Varb(S) (g)- Varbt (s,P)(g) = Ep,μ∏
,E∏ [G(a,s)]b(s)-
Eπ G(a, S)Qπ (S, a, p)
(7)
Proof. See Appendix A.2.
□
Guided by Theorem 1, we can then quantify the variance improvement achieved by the optimal
state/environment-dependent baseline b*(s, P) over the optimal state-dependent baseline b*(s) by
letting b(s) = b*(s), as shown in the following corollary.
Corollary 1. Let V arbt(s) (g) denote the variance of gradient estimator with the optimal state-
dependent baseline b*(s). The variance reduction improvement of b*(s, P) over b*(s) is:
t	t	EP (p|s) Eπ [G(a, S)Qπ (S, a, p)] - Eπ G(a, S)Qπ (S, a,
Varb (S) (g) - Varb (S，P)(g) = EP 改 ʌ--------------------------------------「，」.--------------------------
切	切	P,μπ	En [G(a,s)]
(8)
2
Since the optimal constant baseline 优 can be thought as letting b(s)=优 for each state S ∈ S, we
show in Appendix A.3 that further variance reduction is achieved by b*(s, P) over bC. Therefore, We
analytically justify that by incorporating the additional information about environments, the optimal
state/environment-dependent baseline b*(s, P) can obtain the minimum variance for DR within the
baselines that consider both state and environment parameters.
4 Variance Reduced Domain Randomization
For the single environment case in standard RL, the theoretically optimal state-dependent baseline
is rarely used in practice due to the computational concern (Wu et al., 2018). Rather, for both
implementational and conceptual benefit, a common alternative choice of b(S) is the state value
function V (S). Similarly in DR, even though the optimal state/environment-dependent baseline is
known and given in Eq. (6), the computation of |P | baselines for all the possible environments
requires a very high computational and sampling complexity, which is therefore infeasible during
the practical training process. Fortunately, similar system dynamics often emerge on environments
having the similar parameters, on which the same policy will also obtain the similar expected returns.
4
Under review as a conference paper at ICLR 2022
In the following, we aim to strike a tradeoff between the variance reduction performance and the
computational complexity required for maintaining multiple state/environment-dependent baselines.
Specifically, we propose to maintain an acceptable number M of state/environment-dependent base-
lines in practice, where M < |P |. The entire environment parameter space P is then divided as M
subspaces {Pj}jM=1, each of which has a size |Pj|. For each subspace Pj, we can compute the
optimal state-only baseline as b*(s, Pj) = EPj(p∣s)E∏[G(a∣s)Q∏(s, a,p)]∕E∏[G(a,s)]. Wethen
denote the state/subspace-dependent baseline as bM(s, P) = {b*(s, Pj)}孔.Assuming a uniform
sampling on P, and referring to Appendix A.4, the total variance of the gradient estimator by incor-
porating the state/subspace-dependent baseline bM (s, P) can be written as:
VarM(g) = X1PP1 VaNgM), gj = Vθlogπθ⑷S) (Qn(S,a,pj)-b*(s,Pj)),pj ∈Pj,⑼
where Pj is the i-th environment parameter in subspace j with sampling distribution JP~∣, and
Var(gM) is computed over Pj ∈ Pj, following S 〜μ∏ (∙∣pj) and a 〜π.
Theorem 2.	The variance difference of gradient estimate between the optimal state-dependent base-
line and the state/subspace-dependent baseline is:
Varb* (* s)(g) - VarM (g)	(10)
M |Pj|	(EP (p|s) [Yp(S)] - Ypj (S)	- (EPj (p|s) [Yp(S)] - Ypj (S)
=3 A ∣⅛ E μ∏(s∣pj) -------------------------------⅛;^---------------------------,
where Yp(S) , Eπ [G(a, S)Qπ (S, a, P)]. For a policy π to be updated at a certain step, and assuming
the uniform distributionfor sampling environment parameters, we have Varb*(s)(g) ≥ VarM (g) if
for each subspace Pj :
VarP (Yp(S)) ≥ VarPj (Yp(S)),	(11)
where the variance VarP (Yp(S)) and V arPj (Yp(S)) is computed over P and Pj, respectively.
Proof. See Appendix A.5.	□
Theorem 2 provides a practical guideline for constructing subspaces to reduce the variance of gra-
dient estimate, by holding the variance of Yp(S) in each subspace not exceeding that in the entire
environment space. However, it still introduces extra computation to estimating G(a, S) in Yp(S).
When the inner product of log policy G(S, a) is loosely correlated to the Q-value function1, we have
Ea[G(a, S)Qπ(S, a, P)] = Ea[G(a, S)]Ea[Qπ(S, a, P)], and Eq. (11) is thus equivalent to:
VarP(Vπ(S,P)) ≥VarPj(Vπ(S,P)),	(12)
It indicates that by clustering the entire environment space into some subspaces and holding the
variance of expected return in each subspace not greater than that in the entire environment space,
the state/subspace-dependent baseline can further reduce the variance of gradient estimate com-
pared to the optimal state-only baseline. Note that by increasing the number of subspaces, the
state/subspace-dependent baseline bM (S, P) will approach the optimal sate/environment-dependent
baseline b*(s,P). However, We argue that the more baselines We employ, the more samples are
required for estimating these baselines. As will be shortly shown by the experiment, increasing M
may not monotonically improve the policy’s performance during the training process.
Based on our theory, we propose a practical variance reduced domain randomization approach
(VRDR) for policy gradient and summarize it in Algorithm 1, where the state/subspace-dependent
baseline b(s, Pj) = EP〜Pj V(s,p)，V(s, Pi) is employed for each state and subspace pair. At each
1 Take the Gaussian policy that is commonly used in policy gradient methods for example. The policy takes
s as input and outputs for action selections a Gaussian distribution with the expectation ma and variance σ2.
The action a = ma has the minimum gradient of zero, i.e., G(ma , s) = 0, while this state and action pair
(ma , s) is not correlated with the Q value before the trained policy obtains a high Q value.
5
Under review as a conference paper at ICLR 2022
Algorithm 1 Variance Reduced Domain Randomization (VRDR) for Policy Gradient
1:	Initialize policy ∏o, number of environment parameters sampled per iteration H, number of
subspaces M, clustering interval Nc, maximal number of iterations N, subspace prototypes
u = {ui}iM=1, baseline V (s|Pi) for each subspace Pi, and cluster labels l = {li}iM=1 as shuffled
with {i}iM=1.
2:	for k = 0 to N - 1 do
3:	if k%Nc == 0 then
4:	Sample H environments {pc}cH=-01 with L trajectories for each environment pc.
5:	Compute Ω = {V∏(Pc)}H=ι, where 匕(Pc) = PL=c1 R(Tcj)/L is the average return of
environment pc .
6:	Apply clustering method on Ω to get the cluster set {Ci}Mι, with each Ci labeled by i.
7:	Determine corresponding prototypes u0 by computing u0i = Pj∈C pj/|Ci|.
8:	Determine minl Pi |u0l - ui | where l = {li}, update prototypes by ui = u0l , and relabel
cluster i with li .
9:	else
10:	Sample a set of environment parameters {pe}eH=-01 uniformly and determine their cluster
label by mini |pe - ui| to obtain the cluster set {Ci}iM=1.
11:	Sample L trajectories {τe,j}jL=-01 for each environment pe using policy πk.
12:	end if
13:	Use PPO for policy optimization to get the updated policy πk+1 on trajectories, with the
baseline V (s|Pi) of the cluster that has been grouped to.
14:	Update V (s|Pi ) on the trajectories in cluster Ci .
15:	end for
iteration k, we sample M environments and group them into M clusters by finding the nearest sub-
space prototype. Then, we train the policy on the trajectories sampled on each environment with the
corresponding baseline b(s, Pj ) and update the baseline b(s, Pj ) on the corresponding trajectories.
During the clustering phase, as guided by Eq. (12), we perform the clustering method on {pc}cH=-11
w.r.t. the average return Vπ(pc) to get the M clusters with corresponding subspace prototypes u0i.
The clustering methods are usually applied to group a fixed dataset, with the labeling done once and
for all. In VRDR, however, we perform clustering multiple times during training, hence the proto-
types between two clustering phases would be similar but have different labels. To avoid the case
where the new prototype u0i close to a certain current prototype is assigned with a different label, we
relabel these new prototypes u0 , and update the current ones u according to u0 as described in Line
8 of Algorithm 1, where we aim to find a set of labels l = {li} that minimizes the distance |u0l - ui|
between new prototype u0 and current prototype u. Then, after the clustering phase, the subspace Pj
can be reconstructed according to the sampled environments in cluster Cj . We utilize loss function
LV = Pst∈τ. ∣V(st,Pi) - P∞0=t YttTrt |2 to update the baseline V(s,Pi) for each cluster C%.
It has been proved by Ghadimi & Lan (2013) that the optimization of gradient variance can accel-
erate the convergence of gradient descent methods. In Appendix A.7, we show that our proposed
VRDR method gains a κ = (κ > 1) acceleration on the convergence of policy gradient methods, as
compared to the direct application of optimal state-dependent baseline for DR.
5	Experiments
In this section, we evaluate our proposed VRDR method on six simulated robot control tasks, where
the fundamental environment parameters that affect dynamics are randomized for the generaliza-
tion of trained policy. The experimental environments are implemented based on the robot control
simulator as described in (Brockman et al., 2016). We compare VRDR with two baseline algo-
rithms: the uniform domain randomization (DR) (Mehta et al., 2020) and the multiple value net-
work (MVN) (Mao et al., 2018). We utilize the proximal policy optimization (PPO) (Schulman et al.,
2017) to train a generalized policy for all the comparison algorithms. In DR, we uniformly sample
the environment parameter p and collect trajectories by using the current policy on the sampled envi-
ronments for policy update, where a value function that is learned from trajectories from all sampled
environments is employed as the state-dependent baseline. In MVN, multiple value functions are
learned for the pre-sampled environments at the beginning of training, while each value function Vi
6
Under review as a conference paper at ICLR 2022
Table 1: Range of parameters for each environment.
Environment	Environment Parameters	Training Range
Walker2D/Hopper/Halfcheetah	Material Density Sliding Friction	[750, 1250] [0.5, 1.1]
Pendulum	Pole length	[0.5, 1.5]
Pendulum2D	Pole mass Pole length	[0.5, 1.5] [0.5, 1.5]
InvertedDoublePendulum	Pole1 length	[0.50, 1.00]
	Pole2 length	[0.50, 1.00]
is utilized as a baseline for the gradient estimator of trajectories collected on environment parame-
ters that are closest to the corresponding pre-sampled pi . For VRDR, we apply hierarchical cluster
method (Jain, 2010) to divide the entire environment space during training, and group the trajecto-
ries collected at each interaction phase as described in Algorithm 1. See Appendix A.14 for detail
of hierarchical clustering, and Appendix A.10 for comparison of clustering methods. We keep all
the hyper-parameters about policy optimization for PPO the same as the implementation in Brock-
man et al. (2016), and purely discuss the gain brought by variance reduction for gradient estimate in
policy gradient methods. Our experiments are designed to answer the following questions:
•	Can VRDR effectively improve the convergence rate of the policy training. Will the final reward
achieved by the converged policy trained by VRDR outperform the other two baseline algorithms?
•	How would the policy performance of all the three algorithms degrade on the out-of-distribution
unseen environments/dynamics?
•	How would the specific hyper-parameters in VRDR affect the performance of training process.
5.1	Training Curves with Randomized Dynamics
The six robot control tasks are as follows. 1) Walker2D: control a planar biped robot to run as fast
as possible; 2) Hopper: control a planar monopod robot to hop as fast as possible; 3) Halfcheetah:
control a planar cheetah robot to run fast; 4 & 5) Pendulum & Pendulum2D: apply the force to
swing a pendulum and keep it upright; 6) InvertedDoublePendulum: control a cart (attached to a
two-link pendulum) to balance the whole system and keep the pendulum upright. We modify the
environment parameters in the configure file to generate six environments with the same control
task and different dynamics. The possible sampling range of the environment parameters is shown
in Table 1. Specifically, we randomize the material density that determines the mass and inertia,
and the sliding friction acting along the tangent plane in Walker2D, Hopper, and Halfcheetah. We
randomize the pole length, respectively, in Pendulum, Pendulum2D and InvertedDoublePendulum,
and the pole mass additionally in Pendulum2D.
We run the training process on environments sampled on the preset parameter range for the same
number of iterations N until all the algorithms are converged. At each iteration k, we generate
H = 100 environments by uniformly sampling the parameter range in all algorithms. We run at least
15 seeds for each algorithm on all the environments to obtain the training results. The policy and
value networks in each algorithm are trained for the same number of epochs and sampled trajectories
at each iteration. We show the training curves of the six tasks, respectively, in Figs. 1(a)-1(f). The
solid curve is the average returns on all the seeds at a certain iteration, while the shaded area denotes
the standard deviations. It can be seen that by applying DR on PPO, we can improve the average
return on the whole training range, while VRDR can accelerate the training process on the six tasks.
In some specific tasks, such as Walker2D, Hopper and Halfcheetah, VRDR even achieves a higher
average return than DR and MVN. In balancing tasks like Pendulum and Pendulum2D, VRDR has
shown a significant acceleration on the convergence of policy training.
5.2	Generalization to Unseen Environments
In this subsection, we evaluate the generalization capability of trained policies on the unseen range
during training. We learn 15 policies for DR, MVN and VRDR, respectively, on training environ-
ments with different random seeds and apply the trained policies on corresponding testing unseen
environments. The generalization performance for each algorithm is measured by the average score
over 15 policies. The generalization evaluation of DR, MVN and VRDR are shown in Figs. 2(a)
and 2(b). VRDR shows the best generalization performance on Walker2D. DR has a broader range
of generalization than MVN, although MVN achieves higher score on the training range than DR.
7
Under review as a conference paper at ICLR 2022
IOOO 1200 1400
(a) Walker2D
ErQaJ υπEυ>n
0	1∞	200	300	400	500
iterations
(d) Pendulum
(b) Hopper
EBaj 96eJ9>e
0	160 MO WO 4⅛0	560
iterations
(e) Pendulum2D
Figure 1: Training curves of average return.
X4-SU9p
2000
(c) HaIfCheetah
8000
eooo
4000：
2000
0
ð seo <uo «eo a∞
iterations
(f) InvertedDoublePendulum
VRDR
MVN
(a) Walker2D	(b) Pendulum2D
Figure 2: Heatmap of return in unseen environments on Walker2D and Pendulum2D with poliCies
trained by DR, MVN and VRDR in the training environments.
For Pendulum2D, however, all the algorithms present a Close performanCe. This result indiCates that
the generalization performanCe is Consistent with the average return aChieved after ConvergenCe in
training, and would not be affeCted by the ConvergenCe rate.
5.3 Ablation Study of VRDR
Number of baselines M: In Theorem 1, We show that by applying b*(s,P), We maintain a
State/environment baseline b*(s,Pi) for each environment Pi and can obtain the minimum variance
of gradient estimate. TheoretiCally, inCreasing M to |P | Will aChieve a better varianCe reduCtion
improvement. Figs. 3(a)-3(c) shoW the training curves of VRDR on Walker2D, Pendulum, and
Pendulum2D With different numbers of baselines. Note that VRDR is reduced to DR When setting
M = 1. We find that the increase of M Will decrease the number of training samples for approx-
imation of each baseline if We sample the same number of environment at each iteration, Which
hence retards the convergence rate of VRDR. This suggests that We can set M to a small number in
practice to strike the tradeoff betWeen the variance reduction and computational complexity.
Clustering interval Nc : Each neW clustering result may change the group that each environment
belong to, hence altering the value function and affecting the policy training in the next clustering
interval. The underlying dynamic mechanisms vary in different environments, and thus the shifting
of value function may affect the training With varying degrees. We shoW the training curves of
VRDR With different clustering interval Nc on Walker2D, Hopper, and Halfcheetah in Figs. 4(a)-
4(c), and the clustering results in Appendix A.9. The results suggest the need ofa specific tuning of
Nc for each task.
6 Related Work
State-of-the-art methods that have endeavored to tackle the generalization and robustness of DRL,
including DR, meta RL and robust RL, all implicitly or explicitly impose diversity on the policy
training. DR and meta RL both tend to learn a policy that can generalize to a variety of environ-
ments (Jiang et al., 2021; Lin et al., 2020), Which may differ in dynamics and reWard function during
training. By introducing an adversary that acts to disturb the dynamics or the observation of the envi-
ronment during training (Zhang et al., 2020), the learned policy can gain robustness on deployment.
8
Under review as a conference paper at ICLR 2022
5000
2∞	400	600	800 1000 1200 1400
iterations
(a) Walker2D
EnaaJ uraEU>n
O IoOziO 300	010	5ao
iterations
En"」uraEU>n
(b) Pendulum	(c) Pendulum2D
Figure 3:	VRDR training curves of average return using different M.
35MΓ
3000
≡25∞
3
S? 2000
西8>e
(a) Walker2D	(b) Hopper	(c) Halfcheetah
Figure 4:	VRDR training curves of average return using different Nc.
The introduced diversity for training environments enables the generalization of DRL policy, while
the consequent high variance aggravates the sample efficiency of DRL training.
For policy optimization under DR, Mehta et al. (2020) declare that the uniform sampling of DR
may lead to suboptimal and high-variance policy, and propose active DR to encourage the sampling
distribution to select informative environments. Several works have been proposed to optimize the
sampling distribution for DR (Andrychowicz et al., 2020; Paul et al., 2019), while the baseline opti-
mization is seldom considered in the policy search using policy gradient and the direct application
of state-dependent baseline is commonly used as formulated in Eq. (4).
Policy gradient methods usually suffer high variance due to the estimation of gradient from sampled
trajectories. By learning a Q-function (critic) instead of using its sampling estimation, actor-critic
methods reduce the variance with the expense of introducing a bias (Sutton & Barto, 2018). Another
line of research follows the idea of control variate (Cheng et al., 2020), by subtracting an commonly
used state-dependent baseline from the gradient estimate to reduce the variance without introducing
bias. By assuming the independence of each dimension in the action vector, Wu et al. (2018) pro-
pose a action/state-dependent baseline, which reduces more variance than the state-only baseline.
These methods are designed for standard RL and require further discussion under the context of DR.
Variance reduced gradient algorithms like stochastic variance-reduced gradient (SVRG) (Johnson &
Zhang, 2013) and momentum-based variance reduction (Cutkosky & Orabona, 2019) methods have
been studied in the policy gradient for RL (Huang et al., 2020; Xu et al., 2020). These methods,
however, do not consider the variance reduction for the DR setting. Mao et al. (2018) propose an
input-dependent baseline to reduce the variance of stochastic input process, which is similar to our
state/environment-dependent baseline. Then, the multi-value-network (MVN) approach and meta-
learning approach are proposed to learn the baseline. Meanwhile, Liu et al. (2019) propose to learn
the per-task control variate and the meta control variate for each meta-RL task, where the per-task
variate is constructed by the action-dependent baseline. In essence, the MVN and per-task control
variates approaches are the same, since they both aim to learn a baseline for each input or task. And
both of these meta-learning approaches are built upon model-agnostic meta learning (MAML) (Finn
et al., 2017) to scale when the number of input instantiations or tasks are large.
7 Conclusion
In this paper, we have focused on reducing the high variance of gradient estimator in policy gra-
dient methods for DR with additional randomness on environment parameters. Specifically, we
derived the optimal state/environment-dependent baseline, and verified that by incorporating the
environment information further variance reduction could be achieved over the optimal constant or
state-only baselines. We then proposed a variance reduced domain randomization (VRDR) approach
for policy gradient methods, to strike a tradeoff between the variance reduction and computational
complexity in practical implementation. We have validated VRDR on several robot control tasks,
demonstrating an overall faster convergence speed of policy training, and even better policy perfor-
mance in some specific tasks.
9
Under review as a conference paper at ICLR 2022
References
Rishabh Agarwal, Marlos C Machado, Pablo Samuel Castro, and Marc G Bellemare. Contrastive
behavioral similarity embeddings for generalization in reinforcement learning. arXiv preprint
arXiv:2101.05265, 2021.
OpenAI: Marcin Andrychowicz, Bowen Baker, Maciek Chociej, Rafal Jozefowicz, Bob McGrew,
Jakub Pachocki, Arthur Petron, Matthias Plappert, Glenn Powell, Alex Ray, et al. Learning
dexterous in-hand manipulation. The International Journal of Robotics Research, 39(1):3-20,
2020.
Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman, Jie Tang, and
Wojciech Zaremba. Openai gym. arXiv preprint arXiv:1606.01540, 2016.
Ching-An Cheng, Xinyan Yan, and Byron Boots. Trajectory-wise control variates for variance
reduction in policy gradient methods. In Conference on Robot Learning, pp. 1379-1394. PMLR,
2020.
Karl Cobbe, Oleg Klimov, Chris Hesse, Taehoon Kim, and John Schulman. Quantifying generaliza-
tion in reinforcement learning. In International Conference on Machine Learning, pp. 1282-1289.
PMLR, 2019.
Ashok Cutkosky and Francesco Orabona. Momentum-based variance reduction in non-convex sgd.
Advances in neural information processing systems, 32, 2019.
Alvaro Cabrejas Egeaa, Raymond Zhanga, and Neil Waltona. Reinforcement learning for traffic
signal control: Comparison with commercial systems. CoRR, 2021.
Chelsea Finn, Pieter Abbeel, and Sergey Levine. Model-agnostic meta-learning for fast adaptation
of deep networks. In International Conference on Machine Learning, pp. 1126-1135. PMLR,
2017.
Saeed Ghadimi and Guanghui Lan. Stochastic first-and zeroth-order methods for nonconvex stochas-
tic programming. SIAM Journal on Optimization, 23(4):2341-2368, 2013.
Evan Greensmith, Peter L. Bartlett, and Jonathan Baxter. Variance reduction techniques for gradient
estimates in reinforcement learning. J. Mach. Learn. Res., 5:1471-1530, 2004.
Feihu Huang, Shangqian Gao, Jian Pei, and Heng Huang. Momentum-based policy gradient meth-
ods. In International Conference on Machine Learning, pp. 4422-4433. PMLR, 2020.
Anil K Jain. Data clustering: 50 years beyond k-means. Pattern recognition letters, 31(8):651-666,
2010.
Yuankun Jiang, Chenglin Li, Wenrui Dai, Junni Zou, and Hongkai Xiong. Monotonic robust policy
optimization with model discrepancy. In Proceedings of the 38th International Conference on
Machine Learning, volume 139 of Proceedings of Machine Learning Research, pp. 4951-4960.
PMLR, 18-24 Jul 2021.
Rie Johnson and Tong Zhang. Accelerating stochastic gradient descent using predictive variance
reduction. Advances in neural information processing systems, 26:315-323, 2013.
Katie Kang, Suneel Belkhale, Gregory Kahn, Pieter Abbeel, and Sergey Levine. Generalization
through simulation: Integrating simulated and real data into deep reinforcement learning for
vision-based autonomous flight. In 2019 international conference on robotics and automation
(ICRA), pp. 6008-6014. IEEE, 2019.
Zichuan Lin, Garrett Thomas, Guangwen Yang, and Tengyu Ma. Model-based adversarial meta-
reinforcement learning. Advances in Neural Information Processing Systems, 33, 2020.
Hao Liu, Richard Socher, and Caiming Xiong. Taming maml: Efficient unbiased meta-
reinforcement learning. In International Conference on Machine Learning, pp. 4061-4071.
PMLR, 2019.
10
Under review as a conference paper at ICLR 2022
Hongzi Mao, Shaileshh Bojja Venkatakrishnan, Malte Schwarzkopf, and Mohammad Alizadeh.
Variance reduction for reinforcement learning in input-driven environments. In International
Conference on Learning Representations, 2018.
Bhairav Mehta, Manfred Diaz, Florian Golemo, Christopher J Pal, and Liam Paull. Active domain
randomization. In Conference on Robot Learning, pp.1162-1176. PMLR, 2020.
Fabio Muratore, Christian Eilers, Michael Gienger, and Jan Peters. Data-efficient domain random-
ization with bayesian optimization. IEEE Robotics and Automation Letters, 6(2):911-918, 2021.
Supratik Paul, Michael A Osborne, and Shimon Whiteson. Fingerprint policy optimisation for
robust reinforcement learning. In International Conference on Machine Learning, pp. 5082-
5091. PMLR, 2019.
Xue Bin Peng, Marcin Andrychowicz, Wojciech Zaremba, and Pieter Abbeel. Sim-to-real transfer of
robotic control with dynamics randomization. In 2018 IEEE international conference on robotics
and automation (ICRA), pp. 3803-3810. IEEE, 2018.
Aravind Rajeswaran, Sarvjeet Ghotra, Balaraman Ravindran, and Sergey Levine. Epopt: Learning
robust neural network policies using model ensembles. arXiv preprint arXiv:1610.01283, 2016.
John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy
optimization algorithms. arXiv preprint arXiv:1707.06347, 2017.
Richard S Sutton and Andrew G Barto. Reinforcement learning: An introduction. MIT press, 2018.
Josh Tobin, Rachel Fong, Alex Ray, Jonas Schneider, Wojciech Zaremba, and Pieter Abbeel. Do-
main randomization for transferring deep neural networks from simulation to the real world. In
2017 IEEE/RSJ international conference on intelligent robots and systems (IROS), pp. 23-30.
IEEE, 2017.
Cathy Wu, Aravind Rajeswaran, Yan Duan, Vikash Kumar, Alexandre M Bayen, Sham Kakade,
Igor Mordatch, and Pieter Abbeel. Variance reduction for policy gradient with action-dependent
factorized baselines. arXiv preprint arXiv:1803.07246, 2018.
Zhaoming Xie, Xingye Da, Michiel van de Panne, Buck Babich, and Animesh Garg. Dynamics
randomization revisited: A case study for quadrupedal locomotion. CoRR, abs/2011.02404, 2020.
Pan Xu, Felicia Gao, and Quanquan Gu. Sample efficient policy gradient methods with recursive
variance reduction. In International Conference on Learning Representations, 2020.
Deheng Ye, Zhao Liu, Mingfei Sun, Bei Shi, Peilin Zhao, Hao Wu, Hongsheng Yu, Shaojie Yang,
Xipeng Wu, Qingwei Guo, et al. Mastering complex control in moba games with deep reinforce-
ment learning. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 34, pp.
6672-6679, 2020.
Huan Zhang, Hongge Chen, Duane S Boning, and Cho-Jui Hsieh. Robust reinforcement learning
on state observations with learned optimal adversary. In International Conference on Learning
Representations, 2020.
11
Under review as a conference paper at ICLR 2022
A Appendix
A. 1 Derivation of Optimal Baselines
Following the derivation in (Greensmith et al., 2004), we now give the derivation of three kinds of
optimal baselines by solving the optimization problem as proposed in Eq. (2).
Derivation of optimal constant baseline.
It can be verified that the optimization problem in Eq. (2) is convex w.r.t. the constant baseline bc,
and we denote:
min F(bc) , Ep,μ∏,∏ G(a, s) [Q∏ (s,a,p) - b0]2
By letting the first-order derivative equal to zero:
dF(bc)
dbc
-2EP,μ∏ ,π [G(a, s)[Qπ (s, a,p) - bc]] = 0,
we have the optimal constant baseline for DR as:
EP,μ∏ ,π [G(a, S)Qn(S,a,P)]
EP,μ∏ ,π [G(a,S)]
(13)
(14)
(15)
Derivation of optimal state-dependent baseline.
The variance minimization problem w.r.t. b(S) is denoted as:
min F(b(s)) , Ep,μ∏,n ∣G(a, s)[Qn(s, a,P)— b(s)]] .	(16)
Following the generating process of joint distribution of p, S and a, we have P (p, S, a) =
P(P)P(s∣p)P(a∣s,p) = P(p)μ∏(s∣p)π(a∣s), where the last equality is because that policy π is
constructed without knowing p. The RHS of Eq. (16) can be expanded as:
Ep〜P,μ∏ 忸n[G(a, S)[Qn(s,a,P)- b(s,P)FU
|P|
P P P(Pi)): μn (SjIPi)En [G(a, Sj )[Qπ (sj ,a,pi) - b(Sj ,pi )] j
i=1	sj
|S| |P|
ΣΣP(Pi ) μn ( SjIPi ) En
j=1 i=1
G(a, Sj )[Qn (Sj , a, Pi ) - b(Sj , Pi )]	.
(17)
By letting the first-order partial derivative equal to zero:
∂ F (b(S))	|S| |P|
∂b(s)	= -2 XX P(Pi) μ∏ (Sj IPi)En [G(a, Sj )[Qn (Sj,a,Pi) -b(Sj )]] = 0,
(S)	j=1 i=1
where b(S) = {b(Sj)}, we have:
|P|
X : P(Pi)μn (Sj |Pi)En G(a, Sj )[Qn (Sj, a, Pi) - b(Sj)] = 0, for each b(Sj )∙
(18)
(19)
i=1
It can be further written as
|P|
|P|
i=1
and
X P(Pi , Sj )En G(a, Sj )[Qn (Sj , a, Pi )] = X P(Pi , Sj )En G(a, Sj )[b(Sj )] ,
(20)
i=1
|P|	|P|
P(Sj)P(PiISj)En G(a, Sj)[Qn (Sj, a,Pi)] =	P(Sj)P(PiISj)En G(a, Sj) b(Sj),
i=1	i=1
(21)
12
Under review as a conference paper at ICLR 2022
where P (sj ) can be further canceled on both sides. We can then get the optimal state-dependent
baseline for sj as
b*(sj)
Pi=1 P (pi |sj )Eπ G(a, sj )[Qπ (sj , a, pi)]	EP (p|sj )Eπ G(a, sj )[Qπ (sj , a, p)]
PiPIP(pi∣Sj)E∏ [G(a, Sj)i	En [G(a, Sj)]	.
(22)
For a continuous state space, we have
b*(s)
EP(p∣s)E∏ [G(a,s)[Q∏(s,a,
En [G(a, s)]
(23)
Derivation of optimal state/environment-dependent baseline.
The variance minimization problem w.r.t. b(S, P) in Eq. (5) is denoted as:
点第)F(b(s，P))= Ep〜P,μ∏ [e∏ ∣G(a, s)[Q∏(s, a,P) - b(s,P)][] ∙
(24)
By letting the first-order partial dirivative equal to zero:
∂F (b(s,P))
∂b(s,pi)
-2En G(a, S) Qn (S, a,
p) - b(S, pi)
0,
(25)
We obtain the optimal State/environment-dependent baseline b*(s, P) = {b*(s,pi)}i=1, where
b*(s,Pi) = En[G(a, s)Q(s, a,Pi)]/En[G(a, s)].	一
A.2 Proof of Theorem 1
The variance of policy gradient using the optimal state/environment-dependent baseline b* (s, P) is:
Varb*3P(g) = E ∣G(a,s) ∣Q∏(s,a,p) - EnIGa^QS^I-E[g]TE[g]
En[G(a, s)]
=E G(a,s) Qn (S,a|p) - 2Qn (s, a,p) pi + +-P2 )	- E[gT]E[g]
n	Z	Z2
Ep,μ∏ [Xp(s)] - Ep,μ∏ 号-E[gT]E[g],
(26)
where we denote Xp(s) , En G(a, s)Q2n (s, a,p) , Yp(s) , En [G(a, s)Qn (s, a, p)] and Z ,
En [G(a, s)]. For an arbitrary state-dependent baseline b(s), the variance difference between b(s)
and b*(s, P) is:
Varb(S) (g) - Varb*(s,P)(g)
=EP,μ∏ En G(a, s) (Qn(S) α,p) - b(S))? -
=Ep,*∏En G(a, s) b2(S) - 2Q∏(s, a,p)b(S) + 2Q∏(s, a,p)
S, a, p) -
=EP,μ∏
En [G(a, s)]b2(S)- 2En [G(a, S)Q(S, a,p)]b(S)十 En MT,。)]]
=EP,μ∏
PEn[G(a,s)]b(s) - En[G(a, S)Q(S,ap!
PEn [G(a,s)]
2
(27)
13
Under review as a conference paper at ICLR 2022
A.3 Variance Reduction Improvement of b*(s, P) OVER bC
The application of optimal constant baseline b* can be considered as letting b(s) = b* for each state
s ∈ S. Hence, by direct applying Theorem 1, we can obtain the variance reduction improvement of
b*(s, P) over bC as:
VarK (g) — Varb*(S，P) (g)
=EP
p
,μπ
=EP
p
,μπ
2
,E∏ [G(a,s)区-En [G(a, S)QnGapI
Eπ G(a, s)
E[G(	)] EEP,μ∏,π [G(a, s)Qn(S, a,p)] — En [G(a, s)Qn(S, a,p)]
“1 (a's)	Ep,μ∏,n [G(a,s)]	En [G(a,s)]
(28)
A.4 VARIANcE oF PoLIcY GRADIENT ESTIMAToR BY APPLYING bM (S, P)
Under uniform domain randomization, we demonstrate that the variance of policy gradient in the
whole environment space can be formulated as weighted sum of the variance in subspaces:
VarM(g) =X看Xμn(Sp) X ∏(a∣s)(g(θ, S, a,Pk) — Eg(θ, s, a,Pk))T(g(θ, s, a,Pk) — Eg(θ, s, a,Pk))
k=1 s	a
M ∣Pj ∣ |Pj |	1
=∑S ʌP∣ΣS ∣P-∣ £〃n(SIPi) Eπ(a∣s)(g(θ, s, a,pi) — Eg(θ, s, a,pi))T(g(θ, s, a,pi) — Eg(θ, s, a,pi))
X 臭V"),
(29)
where gj is the gradient estimator of environment in subspace j and Pij is the i—th environment in
subspace j .
A.5 Proof of Theorem 2
In this subsection, we study how to divide subspace to reduce variance of gradient estimator. First,
we derive the variance difference in Eq. (10). For a certain state-dependent baseline b(S) and an
arbitrary sampling distribution P of environment parameters, the variance of gradient estimate is:
M |Pj |
Varb(s) (g) =XX
P(Pi) X μn (SIPj ) X n(a|S)G(a|S) IQn(S,a, Pj ) — 2Qn(S,a,Pj )b(S) + b2(s)]
M |Pj|	Y2j(S)	Y2j(S)
=EEP(Pi)Eμn(S∣Pj) Xpj- ~2pZr + ^Z- - 2b(S)YPj (S) + b2(S)Z
j =1 i=1	s
M |Pj |
XXP(Pi) Xμn(SIPj)
Yp2j(S)
Xpj- -⅛^ +
(30)
The variance of gradient estimate by incorporating the state/subspace-dependent baseline is
M |Pj |
VarM (g) = XX P(Pi) X μn (SIPj)
j=1 i=1	s
X .上+EEPj(pis3(S)	Ypj(S)!
pj	Z + I	√Z	√Z )
(31)
14
Under review as a conference paper at ICLR 2022
and variance of gradient estimate by incorporating the optimal state-dependent baseline is
M |Pj |
Varb*(s)(g) = XXP(Pi) Xμ∏(s|pj)
j=1 i=1	s
X	YPj (S)	EP(p∣s)Yp(S)
Xpj- ɪ + (	√
(32)
The difference between them is
Varb*(S) (g) - VarM(g)
M |Pj |
XXP(pi)X
μ∏ (s|Pj)
EP (p|s) [Yp	(S)] -	Ypij (S)	-	EPj (p|s)	[Yp	(S)] - Ypij (S)
E∏ [G(a, s)]	.
(33)
Thus, Eq. (10) can be obtained by letting P(Pi)= 由.Then, We discuss the dividing process of
subspaces to make the variance reduction improvement V arb*(s) (g) ≥ VarM (g) hold. Under the
uniform sampling distribution P(p)=击,we have
M |Pj|
XXP(Pij|S)(EP(p|s)[Yp (S)] -Ypj(S))2
j=1 i=1
M |Pj| 1 P( | j )
XX p -Psy (Ep(P∣s)[YP(s)] - YPj(s))2. (34)
The first equality holds since PM=I IPPj| = 1. And
M |P |	M |P | |Pj|
X 1PVarP (YP(s)) = X 1P XPj(Pi∣s)(EPj(p∣s)[YP(s)] - YPj (S))
j=1 |P|	j=1 |P| i=1
=X 号 X ∣⅛ PPj (EPj(PIs) M(S)] - YPj (s))2
M |Pj| 1 P (S|Pj)
=XX 肉4⅛d(EPj(PIs)[YP(s)] - YPj (s))2.
(35)
Note that P(s∣pj) = μ∏(s∣pj). Thus, as long as VarP(Yp(S)) ≥ VarPj(Yp(s)), we have
VarP(Yp(S)) = PM=I 耨VarP(Yp(S)) ≥ PM=I 耨VarPj(Yp(S)). Then, combing Eq. (10),
we have V arb*(s) (g) ≥ VarM (g).
A.6 Domain Randomization with An Arbitrary Sampling Distribution
In this subsection, we study how to divide subspace to reduce variance of gradient estimator in
domain randomization with an arbitrary sampling distribution. Specifically, we prove that for an
arbitrary sampling distribution of environment parameters, as long as Eq. (11) holds, we still have
V arb*(s)(g) ≥ VarM (g). Under arbitrary sampling distribution P, the variance difference of gradi-
ent estimate between the optimal state-dependent baseline and the state/subspace-dependent baseline
15
Under review as a conference paper at ICLR 2022
is:
Varb*(S)(g)- VarM(g)
M
X X P(Pi) Xμ∏(SIpi)L
EP (p|s) [Yp(S)] - Ypi (S)	-	EPj (p|s) [Yp(S)] - Ypi (S)
j=1 pi∈Pj
Eπ [G(a, s)]
(36)
where P(pi ) is the sampling probability of environment pi from the entire environment space P .
Hence, P P(PiP)Can be used to denote the sampling probability of environment Pi within the
subspace Pj. For VarPj (Yp(s)), we have
VarPj (Yp(S))= X PP :PiP(Pe) PP^(EPj(p∣s)[YP(s)] - Ypi(s))2,
(37)
and thus
M
j=1
pi ∈Pj
P(pi) VarPj(Yp(s))=	P(pi)
j=1 pi∈Pj
PP⅛τ(Epj(p∣s)[Yp(s)]- YPi(s))2
P (S)
(38)
s
M
For VarP (Yp (s)), we have
M
j=1
pi∈Pj
P(pi)
VarP(Yp(s)) =V arP (Yp(s))
P(Pi|S)(EP(p|s)[Yp(S)] - Ypi (S))2
pi∈P
M
XX
P(Pi|S)(EP(p|s)[Yp(S)] - Ypi (S))2
j=1 pi∈Pj
X X P (pi) PP(P) (Ep (P∣s)[Yp(s)] - YPi(SM
j=1 pi∈Pj	S
(39)
where the first equality follows that PjM=1 Pp ∈P P(pi)
1 and the second quality is from the
definition of VarP (Yp(s)).
Note that P(s|pj) = μ∏(s|pj). Thus, as long as VarP(Yp(S)) ≥ VarPj(Yp(s)), we have
PjM=1 hPpi∈PjP(pi)i VarP(Yp(s)) ≥ PjM=1 hPpi∈Pj P(pi)i V arPj (Yp(s)). Then, combing
Eq. (36), we have Varb* (s)(g) ≥ VarM (g).
A.7 Convergence Analysis of VRDR
Let VJ(θ) = VθEp〜P [η(∏,p)] be the policy gradient. In this section, We present the theorem
proposed by Ghadimi & Lan (2013) under the DR setting. It shows that under Assumption 1
that VJ (θ) is Lipschitz continuous W.r.t. policy parameter θ, the variance of gradient estimator
dominates the convergence rate.
Assumption 1. kVJ(θ1) - VJ(θ2)k ≤ Lkθ1 - θ2k,	∀θ1,θ2 ∈ Rn.
Applying an arbitrary gradient estimator, the convergence of policy gradient is upper bounded as
follows.
Theorem 3. (Ghadimi & Lan, 2013) Under Assumption 1, and assuming that the step size αk =
min{ L,
,Var(gk)K
}, we have
ERkVJ(θ)k2 ≤
L(J(θ*) — J(θι)) + Lmax® VZVar(gk)
K/2
√K
(40)
1
16
Under review as a conference paper at ICLR 2022
where J (θ*) is the maximum expected cumulative discounted reward that can be achieved by policy
∏θ*, K is the maximum number of iterations for policy gradient method, gk = g(θk, s,a) is the
gradient estimator at the k-th iteration for (s, a), and
ERkVJ (θ)k2
PK=ι(αk - L∕2αk)E∣∣VJ(θ)k2
PK=1(αk - L∕2ɑk)
(41)
is the expectation w.r.t. distribution PR(θk)
ak-L∕20k
and all the possible randomness.
By holding Eq. (12), We have Var MS(g > 1 and gains a K = ʌ/VarbMs)R (K > 1) acceleration
,	V arM (g)	V arM (g)
on the convergence of policy gradient methods.
A.8 Analysis of Additional Computational Cost of VRDR
In DR, the computational cost contains mainly the feed-forWard computation and back-propagation
computation of the policy netWork and the value netWork. For N iterations, H trajectories With
length of Lh, size of state space |S| and constant computational cost of feed-forWard and back-
propagation computation di, the computational cost of DR is O(N ∙ H ∙ Lh ∙∣S∣∙ di).
The additional computational cost introduced by VRDR is mainly from the clustering phase. Con-
sidering the hierachical clustering method in Algorithm 2. The maximum number of outer loop
is H - M < H . At each outer loop, the computational cost of distance matrix is m2 ≤ H2 .
Then, let the cost of distance computation in Line 6 of Algorithm 2 be d2, the computational cost is
O(N ∙ H3 ∙ d2).
Note that compared With the simple computation of distance betWeen tWo scalars ui and uj , the
computational cost of feed-forWard and back-propagation computation are implemented for the en-
tire policy and value netWorks, both of Which contain multiple layers and neurons. Therefore, in
practice, di is usually multiple order-of-magnitude larger than d2 . Thus, under the condition that
Lh and |S| are comparable with H, term O(N ∙ H ∙ Lh ∙ |S| ∙ di) will become the dominate term
in the overall computational complexity of VRDR, While the additional computational complexity
introduced by the clustering phase of VRDR can be neglected.
A.9 Clustering Results During Training
In this subsection, we show the clustering results of three successive clustering updates (iteration
k = nNc, (n + 1)Nc, (n + 2)Nc) on Walker2d, Hopper, and Halfcheetah in Figs. 5(a)-5(c).
A.10 Ablation Study of VRDR on Clustering methods
In this subsection, we study the influence of different clustering methods on VRDR. Considering
that we require to specify the number of clusters in VRDR and the clusters should be generated with
similar state values to satisfy Eq. (12), we additionally chose k-means for the comparison to the
originally adopted hierachical clustering. The comparison results of different clustering methods
are shown in Fig. 6(a) and Fig. 6(b). It can be seen that applying k-means and hierachical clustering
methods present similar average scores on Hopper and Halfcheetah.
A.11 Comparison to Combined State-dependent Baseline
The optimal state/environment baseline can be considered as combined state-dependent baseline,
where combined state is composed of original state s and environment parameter p, if we treat p as
part of state of the MDP. In this subsection, we conduct experiment to evaluate DR with combined
state-dependent baseline (DRCS) and the training curves are shown in Fig. 7(a) and Fig. 7(b). The
curves of DRCS show that applying regular state-dependent baseline on this combined state degrades
the average score during training as compared to DR and VRDR.
17
Under review as a conference paper at ICLR 2022

Alωu ① P
friction
(a) Walker2D
friction
(b) Hopper

∕c=(n + 2)Λ∕c
750
776
k = nN c
k= (n + l)Λ∕c
3翳
ω疆
U 1013
出懒
P 1092
1118
1145
1171
谖Z
1250
1092
1171
1250
1092
1171
1250
1118
1145
1118
1145
friction
≡≡S≡≡≡5





(c) Halfcheetah
Figure 5:	Clustering results in three successive clustering update.
A.12 Comparison to Other Baseline algorithms
In this subsection, we compare VRDR with other baseline algorithms including DR with meta base-
line (DRMB) (Mao et al., 2018; Liu et al., 2019) and Epopt (Rajeswaran et al., 2016). We use PPO
with the same setting as in Section 5. In DRMB, a meta baseline is applied to learn the simpler
18
Under review as a conference paper at ICLR 2022
(a) Hopper	(b) Halfcheetah
Figure 6:	Training curves of average return achieved by using k-means and hierachical clustering in
VRDR.
(a) Walker2D
3000
U 2500
n
⅛ 2000
L_
Φ
CT1500
2
g ιooo
∏J
500
O
O 200	400	600	800 IOOO 1200
iterations
(b) Hopper
Figure 7: Training curves of average return achieved by DR, VRDR and DRCS.
(a) Walker2D	(b) Hopper
Figure 8: Training curves of average return achieved by VRDR and VRMB.
alternative En [Q∏(s, a,p)] instead of state/environment-dependent baseline b"s,p). The training
curves are shown in Fig. 8(a) and Fig. 8(b). VRDR shows better average scores than DRMB on
Walker2d and a similar average score on Hopper.
In Epopt, we use purely trajectories from the α-percentile worst-case environments for training,
where we set α = 5. The training curves are shown in Fig. 9(a) and Fig.9(b). And it can be seen
that VRDR outperforms Epopt on both environments.
19
Under review as a conference paper at ICLR 2022
(a) Walker2D
(b) Hopper
Figure 9: Training curves of average return achieved by VRDR and Epopt.
(a) Pendulum2D
Figure 10: Training curves of mean variance of gradient estimates over random seeds achieved by
DR, MVN and VRDR.
A.13 Variance of Gradient Estimates During Training
In this subsection, we empirically evaluate the variance of the gradient estimates that are practically
achieved by DR, MVN and VRDR on Pendulum2D. It can be seen that at the beginning of training,
VRDR can achieve a significantly lower variance of gradient estimates than the other two baselines,
which coincides with the faster convergence behavior of VRDR as shown in Fig. 1(e).
A.14 Hierachical Clustering for VRDR
In this subsection, we detail the hierarchical clustering method used in the implementation of VRDR,
and summarize it in Algorithm 2.
A.15 FROM EQ. (12) TO EQ. (11)
In this subsection, we prove that V arP (Vπ (s, p)) ≥ V arPj (Vπ(s,p)) in Eq. (12) is a sufficient
condition of V arP (Yp(s)) ≥ V arPj (Yp(s)) in Eq. (11). When the above condition holds, we have
VarP(Vπ(s,p)) ≥ VarPj(Vπ(s,p)),	(42)
and by multiplying both sides with a non-negative term Ea [G(a, s)], we have
Ea[G(a, s)]VarP(Vπ(s,p)) ≥ Ea[G(a, s)]VarPj(Vπ(s,p)).	(43)
20
Under review as a conference paper at ICLR 2022
Algorithm 2 Hierachical Clustering
1:	Input Ω = {V∏(pc)}H=ι,
2:	Initialize number of clusters M, current number of clusters m = H, prototypes {ui
匕(pi)}H=ι, distance matrix Md
3:	while m ≥ M do
4:	for i = 0 to m - 1 do
5:	for j = 0 to m - 1 do
6:	Md (i, j ) = |ui - uj |
7:	Md(i, j) = Md(j, i)
8:	end for
9:	end for
10:	merge the two clusters u* and u7∙* having the minimum distance: g* = u* ∪ Uj*
11:	m=m- 1
12:	end while
13:	Output prototypes {ui}iM=1
By expanding the LHS with the definition of variance, we have
Ea[G(a,s)]V arP(Vπ(s,p))	(44)
=Ea[G(α, s)] X P(Pi∣s)回 V(s,p)] — J.,Pi)[ 2
i=1
1	|P|
Ea[G(α,s)] XP(PiIs) Γa[G(a, S)]EP [Ea [Qπ(s, a,p)]l — Ea[G(a, s)]Ea [Qπ(s, a，Pi)]
2
1	|P|
E [G(a s)] 5P(PiIS) EP 眄a [G(a, S)Qn(S,a,p" — Ea [G(a, S)Qn(S,a,Pi)]
2
VarP (Yp(s))
Ea[G(α, s)]
where the third equality can be obtained by using the condition Ea [G(a, S)Qn (S, a, P)]
Ea[G(a, S)]Ea[Qn(S, a, P)]. Similarly, we can obtain Ea[G(a, S)]V arPj (Vn(S,P))
VarPj(YP(S))
Ea[G(α,s)]
Hence, from the inequality in Eq. (43), we have V arP (Yp(S)) ≥ V arPj (Yp(S)), and thus
V arb*(s)(g) ≥ VarM (g) according to Eq. (10). Namely, the improvement of variance reduc-
tion by applying subspaces generated by following Eq. (12) can also be quantified by Eq. (10).
21