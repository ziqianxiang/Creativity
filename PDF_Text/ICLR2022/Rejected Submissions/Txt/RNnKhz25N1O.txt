Under review as a conference paper at ICLR 2022
Low-Cost Algorithmic Recourse for Users
With Uncertain Cost Functions
Anonymous authors
Paper under double-blind review
Ab stract
The problem of identifying algorithmic recourse for people affected by machine
learning model decisions has received much attention recently. Existing ap-
proaches for recourse generation obtain solutions using properties like diversity,
proximity, sparsity, and validity. Yet, these objectives are only heuristics for what
we truly care about, which is whether a user is satisfied with the recourses offered
to them. Some recent works try to model user-incurred cost, which is more di-
rectly linked to user satisfaction. But they assume a single global cost function
that is shared across all users. This is an unrealistic assumption when users have
dissimilar preferences about their willingness to act upon a feature and different
costs associated with changing that feature. In this work, we formalize the no-
tion of user-specific cost functions and introduce a new method for identifying
actionable recourses for users. By default, we assume that users’ cost functions
are hidden from the recourse method, though our framework allows users to par-
tially or completely specify their preferences or cost function. We propose an
objective function, Expected Minimum Cost (EMC), based on two key ideas: (1)
when presenting a set of options to a user, it is vital that there is at least one low-
cost solution the user could adopt; (2) when we do not know the user’s true cost
function, we can approximately optimize for user satisfaction by first sampling
plausible cost functions, then finding a set that achieves a good cost for the user
in expectation. We optimize EMC with a novel discrete optimization algorithm,
Cost-Optimized Local Search (COLS), which is guaranteed to improve the re-
course set quality over iterations. Experimental evaluation on popular real-world
datasets with simulated user costs demonstrates that our method satisfies up to
25.89 percentage points more users compared to strong baseline methods. Using
standard fairness metrics, we also show that our method can provide more fair so-
lutions across demographic groups than comparable methods, and we verify that
our method is robust to misspecification of the cost function distribution.
1	Introduction
Over the past few years ML models have been increasingly deployed to make critical decisions
related to loan approval (Siddiqi, 2012), insurance (Scism, 2019), allocation of public resources
(Chouldechova et al., 2018; Shroff, 2017) and hiring decisions (Ajunwa et al., 2016). Decisions
from these models have real-life consequences for the individuals (users) involved. As a result,
there is a growing emphasis on explaining these models’ decisions (Ribeiro et al., 2018; Lundberg
& Lee, 2017; Poulin et al., 2006) and providing recourse for unfavorable decisions (Voigt & dem
Bussche, 2018; Karimi et al., 2020a). A recourse is an actionable plan that is given to someone
allowing them to change the decision of a deployed model to a desired alternative (Wachter et al.,
2017). Recourses can be highly valuable for users in situations where model decisions determine
important life outcomes. Or, in cases where no feasible recourse is possible, users may wish to
dispute the use of a model in the first place, and we might take this as evidence that greater reforms
to the decision-making system are needed (Venkatasubramanian & Alfano, 2020).
Recourses are desired to be actionable, feasible, and non-discriminatory. Actionable means that
only features which can be changed by the user are requested to be changed. These changes should
also be possible under the data distribution. For example, Education level cannot be decreased from
a Masters to Bachelors degree but can be increased from Masters to PhD. It is also not actionable
to change your Race (Mothilal et al., 2020). A recourse is feasible if it is reasonably easy for the
1
Under review as a conference paper at ICLR 2022
user to adopt, i.e. it is actionable and has a low cost for the user. Non-Discriminatory means that
the recourse method should be equitable across population subgroups. There are now many fairness
metrics that can be used to measure this (Hinnefeld et al., 2018), e.g. the ratio between the average
cost of recourse for two subgroups in a population.
While we want recourses to be feasible for all
users, it is difficult to directly optimize for a
user’s incurred cost unless we have access to
their ground-truth cost function. In the absence
of detailed cost function data, prior work has
used other heuristic objectives for feasibility. For
instance, Mothilal et al. (2020) and Wachter et al.
(2017) assume that if the vector distance between
the user’s current state and the recourse is small,
then recourse will be low cost. These works
encourage this property via a proximity objec-
tive. Meanwhile, sparsity quantifies the num-
ber of features that require modification to im-
plement a recourse (Mothilal et al., 2020). When
providing multiple recourse options, diversity in
proposed recourses is used to counter uncertainty
around the user cost function (Mothilal et al.,
2020; Cheng et al., 2021). The assumption is
that if users are provided with diverse options
then they are more likely to find at least one fea-
sible solution. Later in section 5.2, we show that
diversity as an objective correlates poorly with
user cost, suggesting it would be strongly prefer-
able to optimize for user cost directly.
A few approaches to recourse do directly opti-
mize for the cost of recourse, but they assume
there is a single cost function shared by all users
(Ustun et al., 2019; Rawal & Lakkaraju, 2020;
Karimi et al., 2020c;d; Cui et al., 2015). We be-
lieve it is crucial to have user-specific cost func-
tions, as a global cost function might poorly rep-
resent different users in a diverse population.
。：User Cost Function
□ : Cost Function Samples
Figure 1: Method diagram showing the intuition
behind the Expected Minimum Cost Objective.
The squares denote cost function samples, which
are the same color when they are similar. We
aim to find a solution set of generated counterfac-
tuals where each counterfactual does well under
a particular region of cost function space (here,
{s1, s2, s3}). The shaded regions each represent a
set of cost functions which a single si caters to. In
this case, we do not have enough counterfactuals
to “cover” every region of the cost function space,
so a single counterfactual (s2) must cater to two
different regions. Here the user’s hidden ground-
truth cost function, Cu, is served well by s1.
In this work, we propose a method for identifying a user-specific recourse set that contain at least
one good solution for the user. We directly optimize for user cost by quantifying the actionability
and feasibility of proposed recourses. In the absence of data about user cost functions, we treat
them as hidden from the recourse method and assume they follow an underlying cost distribution.
However, we provide users with an option to specify their preferred editable features or the complete
cost function detailing the costs of transitions between features values. We model this cost distribu-
tion via a highly flexible hierarchical cost sampling procedure which makes minimal assumptions
about user preferences (Algorithm 1). Based on this distribution, we propose an objective function,
Expected Minimum Cost (EMC), which allows us to approximately optimize for user satisfaction by
first sampling plausible cost functions, then finding a set that achieves a good cost in expectation.
The EMC objective encourages the solution set to consist of counterfactuals that are each a good
counterfactual under some particular cluster of cost functions from the distribution. Hence, no mat-
ter what the user’s ground-truth cost function is, we will have some counterfactual that is well suited
to the user’s cost function (shown in Figure 1). Next, we propose a simple discrete optimization
method, Cost-Optimized Local Search (COLS), in order to optimize for Expected Minimum Cost.
COLS guarantees a monotonic reduction in the Expected Minimum Cost of the counterfactual set,
which we show leads to large empirical reductions in user-incurred cost.
To evaluate the effectiveness of our proposed techniques, we run experiments on two popular real-
world datasets: Adult-Income (Dua & Graff, 2017) and COMPAS (Larson et al., 2016). We com-
pare our method with multiple strong baselines methods like Diverse Counterfactual Explanations
(DICE) (Mothilal et al., 2020), Feasible and Actionable Counterfactual Explanations (FACE) (Poyi-
2
Under review as a conference paper at ICLR 2022
adzi et al., 2020), and Actionable Recourse (AR) (Ustun et al., 2019). We evaluate these methods
on existing metrics from the literature like diversity, proximity, sparsity, and validity (Section 5.1)
along with two cost-based metrics. In particular, we measure the fraction of satisfied users, based
on whether their cost of recourse is below a certain threshold k. We also report coverage, which is
the fraction of users with at least one actionable recourse (Rawal & Lakkaraju, 2020). Using simu-
lated user cost functions, we show that our method satisfies 25.89% and 17.93% percentage points
more users than strong baseline methods while covering 22.35% and 17.13% more users, on the
Adult-Income and COMPAS dataset respectively. We perform important ablations to show whether
performance can be attributed to the COLS optimization method or the EMC objective. Addition-
ally, we evaluate the robustness of our method to various distribution shifts that can occur between
the user’s hidden cost distribution and the hierarchical cost sampling distribution. We find that our
method is robust to these distribution shifts and generalizes well to user cost function from these
shifted distributions. Lastly, we perform a fairness analysis of all the methods across demographic
subgroups based on Gender and Race. Standard fairness metrics demonstrate that, in most compar-
isons, our method is more fair relative to the strongest baseline methods.
Our primary contributions in this paper are listed below.
1.	We propose to evaluate user-incurred cost and fraction of satisfied users by means of hidden
user-specific cost functions, rather than a known global cost function.
2.	We propose a new objective function, Expected Minimum Cost (EMC), which allows us to
approximately optimize for user satisfaction when user cost functions are not known.
3.	We propose a discrete optimization method, Cost-Optimized Local Search (COLS), which
achieves up to 25.89% percentage points higher user satisfaction relative to the next-best
baseline. COLS guarantees a monotonic reduction in EMC, which we find provides a 19%
point improvement over a simple local search.
4.	We show that in most settings, COLS provides more fair solutions across demographics
subgroup than comparable recourse methods, while offering recourse to a substantially
higher fraction of users.
2	Related Work
A wide variety of methods have been proposed for generating recourses. For a comprehensive survey
of existing recourse methods, we refer readers to Karimi et al. (2020b). Here, we distinguish our
approach based on our recourse objectives, optimizer, and evaluation. We primarily discuss recourse
methods, though there is useful complementary work on problems such as providing recourse when
there is distribution shift in the data (Upadhyay et al., 2021; Slack et al., 2021) and training models
which guarantee recourse to affected individuals with high probability (Ross et al., 2021).
Objectives: The most prominent family of objectives for recourse includes distance-based objec-
tives (Wachter et al., 2017; Karimi et al., 2020a; Dhurandhar et al., 2018; Mothilal et al., 2020;
Rasouli & Yu, 2021). These methods seek recourses that are close to the original data point. In
DICE, Mothilal et al. (2020) provide users with a set of counterfactuals while trading off between
proximity, a distance-based objective, and diversity. Diversity-based methods assume that providing
diverse options will increase the chance a user is satisfied by one of the options. A second category
of methods uses other heuristics based on the data distribution (Aguilar-Palacios et al., 2020; Gomez
et al., 2020) to come up with counterfactuals. FACE constructs a graph from the given data and then
tries to find a high-density path between points in order to generate counterfactuals (Poyiadzi et al.,
2020). Lastly, the works closest to ours are the cost-based objectives, which capture feasibility in
terms of the cost of recourse: (1) Cui et al. (2015) define a cost function based on the minimum
and maximum values a factor can take in their additive tree model. (2) Karimi et al. (2020c;d) take
a causal intervention perspective on the task and define cost in terms of the normalized distance
between the user state and the counterfactual. (3) Ustun et al. (2019) define cost in terms of the
number of changed features and frame recourse generation as an Integer Linear Program. (4) Rawal
& Lakkaraju (2020) infer a cost function from simulated rankings of features for actionability, then
optimize recourses for this cost function. Importantly, all of these works assume there is a known
global cost function that is shared by all users. In our work, we drop this assumption, and instead
we optimize for cost over a distribution of plausible user-specific cost functions.
Optimization: Early work on recourse methods uses gradient-based optimization to search for
counterfactuals close to a user’s data point (Wachter et al., 2017). Several methods since then also
use gradient-based optimization (Mothilal et al., 2020; Chen et al., 2020). Some recent approaches
3
Under review as a conference paper at ICLR 2022
use tree-based techniques (RaWal & Lakkaraju, 2020; Aguilar-Palacios et al., 2020; Von KugeIgen
et al., 2020; Hashemi & Fathi, 2020; Kanamori et al., 2020), kernel-based methods (Dandl et al.,
2020; Gomez et al., 2020; Ramon et al., 2020), While others employ some heuristic (Poyiadzi et al.,
2020; Aguilar-Palacios et al., 2020) to generate counterfactuals. A feW Works use latent space per-
turbation With autoencoders to generate recourses (PaWelczyk et al., 2020; Joshi et al., 2019), While
Karimi et al. (2020a) and Ustun et al. (2019) utilize SAT and ILP solvers, respectively. Here, We
introduce a discrete optimization method specialized for our Expected Minimum Cost objective.
Evaluation: Besides ensuring that recourses are classified as the favorable class by a model (va-
lidity), the most prominent approaches to evaluate recourses rely on Distance-based metrics. In
DICE, Mothilal et al. (2020) evaluate recourses according to their proximity, sparsity, and diversity.
MeanWhile, several Works directly consider the cost of the recourses, using a knoWn global cost
function as a metric, meaning that all users share a cost function, Which is available to the recourse
generation method (Cui et al., 2015; Karimi et al., 2020c;d). In a slight departure from this setting,
RaWal & Lakkaraju (2020) estimate a cost function from simulated pairWise feature comparisons,
but this single estimate is used for both recourse generation and evaluation. In contrast, We evaluate
a recourse method by simulating user-specific cost functions Which can very greatly across users,
and these cost functions are not knoWn in advance to recourse generation methods. We Will also
measure recourse coverage as defined by RaWal & Lakkaraju (2020), Which measures the fraction
of users that Were provided With a recourse by the method.
3	Problem Statement
Notation. We assume that We have a dataset With features F = {f1, f2, ...fk}. Each feature can
either be continuous F con ⊂ F or categorical Fcat ⊂ F. Each continuous feature ficon takes
values in the range [rimin, rimax], Which We discretize to integer values. For a continuous feature fi,
We define the range Q(fi) = {k ∈ Z : k ∈ [rimin, rimax]} and for a categorical feature fi, We define it
as Q(fi) = {q1fi , q2fi , ..., qdfi}, Where q(f.i) are the states that feature fi can take. Features can either be
mutable (Fm), conditionally mutable (Fcm), or immutable (F), according to the real-World causal
processes that generate the data. Mutable features can transition from betWeen any pair of states in
Q(fi) ; conditionally mutable features can transition betWeen pairs of states only When permitted
by certain conditions; and immutable features cannot be changed under any circumstances. For
example, Race is an immutable feature (Mothilal et al., 2020), Age and Education are conditionally
mutable (cannot be decreased under any circumstances), and number of work hours is mutable (can
both increase and decrease). Lastly, While continuous features inherently define an ordering in its
values, categorical features can either be ordered or unordered based on its semantic meaning. For
instance, Age is an ordered feature that is conditionally mutable (can only increase).
Cost Function. In this Work, We assume that each user has a hidden preference regarding the ease of
changing a particular feature, Where different users can have different preferences. Such differential
preferences can be expressed via user-specific costs of transitioning betWeen feature states. We
define a cost function for each user C : R|F| → R|F| as a set of feature-specific functions Which
provide the user-incurred cost When transitioning betWeen feature states. Formally, a cost function
is parametrized as a set of transition matrices, C = {C(f) ∈ RlQ(f)l×lQ(f)1 | ∀f ∈ F}, where each
element of the transition matrix Ci(jf) ∈ [0, 1] ∪ {∞} is the cost of transition from state i → j for the
feature f. Here, 0 means the transition has no associated cost incurred, and 1 means the transition
is maximally difficult to make. Infeasible transitions have a cost of ∞ (associated with immutable
features). For the remainder of the paper, we interchangeably write C(f) as matrix or a function
where C(f) (m, n) = Cm(f,)n.
User Definition. A user U is defined as a tuple U = (su, CU), where Su is the state vector of length
|F| containing the user,s features values and Cu is their unknown ground-truth cost function (see
Appendix Table 5 for qualitative example). This ground-truth cost function, Cu = {Cu(f) | ∀f ∈ F},
provides the cost of transitioning between different feature states. Following past work (Rawal &
Lakkaraju, 2020), we note that it may be difficult for users to precisely quantify their own cost
functions in practice, but we do assume that each user has some hidden cost function that specifies
which recourses they would prefer. Different from past work (Rawal & Lakkaraju, 2020; Ustun
et al., 2019), we suppose that each user in a population has their own cost function, rather than
assuming there is a global cost function (single shared and fixed cost function across users).
4
Under review as a conference paper at ICLR 2022
Transition Costs. Given two state vectors si , sj and any cost function C, transition cost
is the summation of transition costs for individual features, defined as Cost(si, sj ; C) =
Pf∈F C(f)(si(f), s(jf)), where s(f) is the value of feature f in the state vector. Now, given a re-
course set S, we suppose that the cost a user will incur is the minimum transition cost across possible
recourses, since a rational user will select the least costly option. So, given a user u with state vector
su and a counterfactual set S, the cost of transition under a cost function C is defined as
MinCost(su, S; C) = min Cost(su, sj ; C).
sj∈S
(1)
Problem Definition. For a given user u our goal is to find a recourse set Su such that,
Su = arg min MinCost(su, S; Cu) s.t. ∃ Si ∈ S s.t. f(si) = +1	(2)
S
where f is the black-box ML model and +1 is the desired outcome class. We want to offer users
at least one counterfactual that is a good solution under their hidden cost function CU. Since We are
uncertain about which cost function is the user’s true cost function, we minimize the expected cost
for each user using a distribution over plausible cost functions (Section 4.1).
Measuring Recourse Quality. While We could measure average population cost as our metric for
recourse method quality, it is also helpful to ask: What proportion of users are satisfied With the
recourses they are given? We say that a user is satisfied by a recourse set if the best option in that
set achieves a sufficiently loW cost. Formally, given a set of users U and a set of sets of generated
counterfactuals {Su}u∈U from a recourse method, the fraction of user’s satisfied at a particular cost
threshold k, FS@k, can be defined as:
FS@k(U, {Su}u∈u) = X "MinCost(防Su — )<k}	(3)
u∈U	| |
We note that optimizing Equation 2 is equivalent to finding the set of counterfactuals that maximizes
the probability that a user is satisfied by some counterfactual in the set.
Another important quantity is the Coverage (Cov) Which measures the fraction of users to Which the
recourse algorithm can provide any actionable recourse.
Cov(U, {Su}u∈u) = X "MinCost(Su；Su；Cu)<∞}	(4)
u∈U	|U|
4	Proposed Method: EMC and COLS
In this section, We define our proposed objective function, Expected Minimum Cost (EMC), cost
sampling distribution, and Cost-Optimized Local Search (COLS) optimization method.
4.1	Approximately Optimizing for User Cost under Uncertain Cost Functions
In most use cases, the cost functions associated with each user Cu 〜D* are unknown to Us and
follow the population,s true cost function distribution D* which is also unknown. Hence, we cannot
exactly minimize the user cost in Equation 1 with respect to Cu. Yet we can approximately optimize
for the true user cost Cu* by means of a plausible cost function sampling distribution Dtrain if it can
model diversity in user preferences and cost function effectively. Given samples from this distribu-
tion, we can minimize the Expected Minimum Cost (EMC) of a transition for a user. Formally, for
any given user u = (su, Cu*), we want to optimize for
ECi 〜Dtrain[MinCost(su, S ； J)]	⑸
In practice, we employ Monte Carlo Estimation (Robert & Casella, 2010) to approximate this ex-
pectation by sampling M cost functions {Ci}iM=1 from Dtrain, which we term our EMC objective:
PM MiCt S C	PiM=1 minSCost(su,sj;Ci)
EXPMinCost(su, S; {Ci}M=ι)=二=1 MinCMst(Su, S Ci)=——j∈S-^--------------------- (6)
Remark: Note that there is an important interaction between the expectation and the MinCost terms
in this objective. EMC encourages the solution set to consist of recourses that are each a good option
under some particular cluster of cost functions from the distribution. That is, the min term allows
for different recourses to cater to different regions of the cost function distribution. As a result,
no matter what the user’s ground truth cost function is, we are likely to have found at least one
counterfactual that is well suited to that cost function (see Figure 1).
5
Under review as a conference paper at ICLR 2022
4.2	Hierarchical Cost Sampling Procedure
To optimize for EMC, we need a plausible distribution Dtrain which is capable of generating cost
functions which model a diverse set of user preferences. We propose, three hierarchical cost sam-
pling distribution, Dperc, Dlin, Dmix which are highly flexible and makes minimal assumptions
about the user preference (see Algorithm 1). The samples from Dperc and Dlin model the transition
cost with percentile shift and linear cost respectively. Whereas, the samples from Dmix distribution
models a linear combination of percentile shift cost (Ustun et al., 2019) and linear cost, where the
mixture weights of this combination are user-specific, capturing their disposition towards these two
types of costs. Percentile shift cost for ordered features is proportional to the change in a feature’s
percentile value with the change from an old feature value to a new one (see Algorithm 4). The
Linear cost for ordered features is proportional to the number of intermediate states in Q(f) which a
user will have to go through while transitioning from their current state to the final state (see Algo-
rithm 5). For a detailed description of the sampling procedure, please refer to Appendix A.2.1 and
Algorithm 1.
We emphasize a few core properties of Dmix distribution: (1) The distribution is very flexible. It
is able to capture all possible feature subsets which different users might consider editable as well
as possible mixing weights for combining linear and percentile costs. Critically, the difficulty of
editing each feature can range from “trivial” to “maximally difficult” (relative to other features),
meaning almost all plausible user cost functions should be represented in the distribution. (2) The
cost distribution mean follows the monotonicity property, i.e. if the user has to make more drastic
changes to the feature, then the associated cost will be higher. (3) The user has an option to adapt this
distribution to their needs by providing either the editable features or the feature scores. Together,
these properties allow us to represent a large space of plausible user cost functions, which helps us
ensure our recourses can satisfy a diverse user preferences.
4.3	Search Methods for finding low cost Counterfactuals.
COLS: To optimize for Expected Minimum Cost (Equation 6), we propose two simple, efficient,
and optimized discrete search algorithms (Ebrahimi et al., 2018; Baptista & Poloczek, 2018), namely
Cost-Optimized Local Search (COLS) and Parallel Cost-Optimized Local Search (P-COLS) (refer
to Algorithm 2). COLS maintains a best set which will be the final recourse provided to the user. At
each iteration, a candidate set is generated by locally perturbing each element of the best set with a
Hamming distance of two, then it is evaluated against the EMC objective. Instead of making a direct
comparison between the best-set-so-far and the candidate set, at this point we evaluate whether any
counterfactuals from the candidate set would improve the best set if we swapped out individual
counterfactuals. Specifically, if the benefit of replacing si ∈ St with sj ∈ Sbest is positive then we
make the replacement. The ability to assess the benefit of each candidate counterfactual is critical
because it allows us to constantly update the best set instead of waiting for an entire candidate
set with lower EMC. For objectives like diversity, evaluating the benefit of individual replacement
becomes prohibitively expensive (see Appendix A.1). We note that, for COLS, we can guarantee
that the EMC of the best set will monotonically decrease over time, which we formally state below:
Theorem 4.1 (Monotonicity of Cost-Optimized Local Search Algorithm). Given the best set,
Stb-es1t ∈ RN×d, the candidate set at iteration t, St ∈ RN×d, the matrix Cb ∈ RN×M and C ∈ RN×M
containing the incurred cost of each counterfactual in Stb-es1t and St with respect to all the M sampled
cost functions {Ci}iM=1, there always exist a Stbest constructed from Stb-es1t and St such that
ExpMinCost(su,Stbest; {Ci}iM=1) ≤ ExpMinCost(su,Stb-es1t; {Ci}iM=1)
For a formal proof of the theorem, please refer to Appendix A.2.2.
P-COLS: The method P-COLS is a variant of COLS which starts multiple parallel runs of COLS
with different initial sets. The run with the least objective value is selected as recourse for the user.
Random Search: We use Random Search as a baseline, which selects random counterfactuals from
the whole space to obtain the next candidate set at each iteration.
6
Under review as a conference paper at ICLR 2022
Data	Method		Metrics					
			Cost Metrics		Distance Metrics			
		FS@1	PACa)	Cov	Div	Prox	Spars	Val
	DICE	2.47	1.37	8.32	3.90	65.80	47.20	97.90
	Face-Eps	15.23	0.76	22.60	4.75	92.22	74.98	100
	Face-Knn	25.30	0.74	35.00	8.62	89.07	71.85	100
Adult-Income	Act. Recourse	49.93	0.55	56.85	18.38	74.68	73.57	78.67
	Random	6.27	1.40	31.83	48.30	55.83	39.85	95.55
	COLS	72.57	0.38	76.07	25.77	80.22	76.48	97.15
	P-COLS	75.82	0.40	79.20	25.57	81.67	78.00	94.78
	DICE	0.40	0.54	0.40	11.30	65.00	32.00	98.90
	Face-Eps	12.20	0.29	12.20	2.50	94.20	60.60	100
	Face-Knn	12.20	0.29	12.20	2.60	94.10	60.60	100
COMPAS	Act. Recourse	65.80	0.40	66.60	11.87	80.53	74.07	44.23
	Random	29.95	0.77	39.20	42.22	55.90	31.25	71.88
	COLS	82.23	0.24	82.23	29.32	77.82	70.05	95.48
	P-COLS	83.73	0.24	83.73	29.38	78.48	71.30	92.78
Table 1: Table comparing different recourse methods across various cost and distance metrics (Sec-
tion 5.1). The numbers reported are averaged across 5 different runs. For all the metrics higher is
better except for PAC where lower is better. Refer to section 5.2 for more details.
5	Experiments
5.1	Experimental Setup
Dataset: We conduct our experiments on the Adult-Income (Dua & Graff, 2017) and COMPAS
(Larson et al., 2016) datasets. The Adult-Income dataset is based on the 1994 Census database,
and it contains 12 features. The model’s task is to classify whether the income of an individual
is over $50, 000. COMPAS was collected by ProPublica, and it contains information about the
criminal history of defendants from Broward County for analyzing recidivism in the United States.
The processed dataset contains 7 features. A model needs to decide bail based on predicting which
of the bail applicants will recidivate in the next two years. We preprocess both datasets based on
a previous analysis where all categorical features are mapped onto two classes (Pawelczyk et al.,
2021). 1 Our black-box model is a neural network model with 2-layers. Please refer to Appendix
B.1 and Table 4 for more details about experiments, data statistics, and the black-box model.
Baselines: We compare our methods COLS and P-COLS with Random Search, DICE (Mothilal
et al., 2020), FACE-Knn and FACE-Epsilon (Poyiadzi et al., 2020), and Actionable Recourse (Ustun
et al., 2019). Importantly, we control for compute by restricting the number of forward passes to
the black-box model, which are needed to decide if a counterfactual produces the desired class.
Additionally, for most big models this is the rate limiting step. For a description of the objective
function and other details of these methods refer to Appendix A.1.1, A.2.3.
Metrics: To compare with past work, we evaluate methods on Distance based met-
rics like diversity, proximity, sparsity and validity metrics. Proximity is defined as
prox(x, Y) = 1 - Nn pi=1 dist(x, Yi), where Yi is a CoUnterfactUaL The Sparsity metric
(MOthiIal et al., 2020) is defined as Spar(x, Y) = 1 - N pi=1 Pj= 1{4,=丫。}. The Diversity
metric (Mothilal et al., 2020) is defined as div (Y) = -1 Pi=-I Pj=i+ι dist(Yi, Yj), where Z
is the number of terms in the summation. Validity is defined as Val(Y) = |{umque si∈"(si)=+1}l.
Additionally, we show resUlts on cost based metrics like Fraction Satisfied (FS@k), PopUlation Cov-
erage (Cov) (see Section 3) and the average incurred cost by the users (PAC) covered by the method.
Simulated Cost Functions: Given true user cost functions Cu 〜D* are not known in practice,
hence for the experiments we simulated them from a distribution Dtest for method evaluation.
There are two phases in the experiments, the recourse generation phase where we optimize for EMC
and the cost functions required are sampled Dtrain . In the evaluation phase, we use the simulated
user cost function which is hidden during training to compute cost-based metrics for all the re-
1 The code for the Actionable Recourse method (Ustun et al., 2019) requires binary categorical variables.
7
Under review as a conference paper at ICLR 2022
Optim. Method	Objective	Cost Metrics			Distance Metrics		
		FS@1	PACa)	Cov	Diversity	Proximity	Sparsity
LS	Sparsity	10.1	1.304	29.0	42.7	66.2	55.8
LS	Proximity	9.7	1.275	27.0	42.1	67.5	55.0
LS	Diversity	0.0	2.393	7.6	53.3	50.8	35.6
LS	EMC	49.8	0.597	59.1	37.8	73.3	67.5
COLS	EMC	68.8	0.391	72.6	27.1	77.5	73.5
Table 2: Ablation results with Search algorithms trained on different objectives (Section 5.2).
course methods (see Appendix B.1.2). For all experiments apart from distribution shift experiments,
Dtest = Dmix .
Dtrain
5.2	Research Questions
Q1. Which Method Satisfies the Most Users?
Design: In this experiment, we compare different recourse methods (Section 5.1 and Appendix
A.2.3) with respect to distance and cost-based metrics (Section 5.1). We set a fixed budget of 5000
and a set size |S | = 10 for all the methods. We perform five runs for each method with different
random seeds and report the mean results in Table 1. We omit the variances from the table as 97%
of the values have variance less than 0.01%, with the maximum value being 3.3% for validity of
Random Search on COMPAS dataset.
Results: We can see from these results that DICE, which optimizes for a combination of distance-
based metrics, performs much worse on the direct metrics like Population Coverage (Cov) and
FS@k. On the other hand, COLS and P-COLS, which optimize for EMC, achieve 22.64% and
25.89% point higher user satisfaction while covering 19.28% and 22.42% point more users on
the Adult-Income dataset, with similar improvements on COMPAS. We also observe that COLS
and P-COLS demonstrate high sparsity and proximity in the solutions. Regarding recourse
diversity, we find that COLS and P-COLS do not exhibit high diversity. The second-best method
on cost metrics, Actionable Recourse, also promotes proximate and sparse solutions rather than
diverse options, while Random Search achieves high diversity but low FS@1. These results provide
evidence that diversity is not a necessary condition for high user satisfaction. Hence, it is preferable
to provide users with recourse based on expected cost as opposed to providing them diverse options
which might not align well with their preferences. Lastly, we note that FACE does the best on
proximity as it finds the shortest high-density path in the generated graph.
Q2. Is the Performance Improved by the Optimization Method or by the Objective?
Design: We perform an ablation study to attribute the improvements from our method to either the
COLS optimization method or the EMC objective function. To do so, we run a basic local search
(LS) to optimize for diversity, proximity, and sparsity along with validity. We use a basic non-
optimized local search, because there is no simple and efficient way to guarantee a reduction in the
diversity objective by swapping out single elements from the solution set (see Appendix A.1). For a
fair comparison across these objectives, we also optimize for EMC using basic local search.
Results: We provide the results for this experiment in Table 2. These results suggest that optimiz-
ing for metrics besides EMC is sub-optimal. For diversity, proximity, and sparsity objectives, the
FS@1 score and population coverage is very low, while they perform well on the respective metrics.
The low FS@1 score for distance metrics is expected as they ignore user-specific preference while
optimizing for their objectives, and therefore the generated recourses might be infeasible under the
user’s cost function. We find that EMC with LS outperforms all Distance Metrics with LS on FS@1,
which suggests that the EMC objective function leads to higher user satisfaction. Meanwhile,
the 19% point difference in the performance of EMC with LS and COLS can be attributed
to our Cost Optimization described in Section 4.3 and Theorem 4.1, which allows COLS to more
efficiently search the solution space. In this experiment, we observe that COLS again does poorly
on diversity, whereas it promotes proximity and sparsity in the recourse. Based on results from Q1
and Q2, we conclude that having high diversity is neither a necessary or sufficient condition to
satisfy individual users with inherent feature preferences. Additionally, we observe that spar-
sity and proximity are positively correlated with higher user satisfaction and inherently emerge
from the idea of satisfying more users.
8
Under review as a conference paper at ICLR 2022
Method	Gender	FS@1	Cov	DIR-FS	DIR-Cov	Race	FS@1	Cov	DIR-FS	DIR-Cov
DICE	-F	0.0	0.0			NW	0.0	0.0		
	M	4.7	15.6	-	-	W	3.1	10.4	-	-
	-F	12.5	22.1	1.504		NW	7.7	12.7		
Face-Eps	M	18.8	24.7		1.118	W	17.8	26.0	2.312	2.047
Face-Knn	-F	29.9	36.3	0.719	0.89	NW	12.7	25.4	2.228	1.425
	M	21.5	32.3			W	28.3	36.2		
	-F	53.8	58.7		0.959	NW	46.5	54.9		
Act. Recourse	M	47.4	56.3	0.881		W	51.2	58.0	1.101	1.056
Random	-F	7.8	34.6	0.859	0.792	NW	4.9	28.9	1.571	1.076
	M	6.7	27.4			W	7.7	31.1		
COLS	-F	72.7	76.2	0.994	0.992	NW	67.6	71.1	1.089	1.082
	M	72.3	75.6			W	73.6	76.9		
P-COLS	-F	76.5	80.2	1.004	1.0	NW	72.5	74.6	1.07	1.092
	M	76.8	80.2			W	77.6	81.5		
Table 3: Fairness analysis of recourse methods for subgroups with respect to Gender and Race.
DIR: Disparate Impact Ratio; M: Male, F: Female; W: White, NW: Non-White (Section 5.2).
Q3. Are Recourses Fair Across Subgroups?
Design: Here, we want to understand whether recourse methods provide equitable solutions across
subgroups based on demographic features like Gender and Race. In this experiment, we assume
Gender is non-actionable for the sake of analysis. Existing works on algorithmic fairness present a
number of metrics for characterizing the disparate impact of black-box classification models across
population subgroups (Feldman et al., 2015). We adapt the Disparate Impact metric for the recourse
outcomes we study, which we denote by Disparate (DIR). Given a metric M, DIR is a ratio between
metric scores across two subgroups. DIR-M = M(S=1)/M(S=0). We use either Cov or FS@1 as
M. Under the DIR metric, a maximally fair method achieves a score of 1. We run experiments on
the Adult-Income dataset, with a budget of 5000 and |S| = 10.
Results: We present the subgroup-based results in Table 3. We observe that COLS and P-COLS typ-
ically have a lower difference in FS@1 and Cov across subgroups (Gender and Race) as compared
to other baselines while satisfying and covering significantly more users. In particular, we find that
our method achieves a score very close to 1 on DIR-FS and DIR-Cov for the gender-based subgroup,
and scores relatively close to 1 on the race-based subgroup. We attribute the fairness of our method
to (1) the fact that our recourses are individualized, rather than making use of the data distribution,
and (2) the use of a diverse set of cost functions when generating recourses. We see condition (2)
as important since there are other individualized methods that do not rely on the data distribution,
such as Actionable Recourse, which can generate less fair solutions than COLS. Overall, we con-
clude that our method is typically more fair than baselines on both Gender and Race-based
subgroups while providing recourse to a larger fraction of people in both subgroups.
5.3	Additional Results
We provide experiments for several additional research questions in the Appendix B.2, which we
summarize here: (1) We find that our method is robust to misspecification of the cost function
distribution (Figure 2, 3); (2) We can make use of a larger compute budget to scale up the
performance (Figure 4); The recourse sets provide high quality solutions to users using as few
as 3 counterfactuals (Figure 5); and (4) we can achieve high user satisfaction with as few as 20
Monte Carlo samples, rather than 1000 (Figure 6). We also show some qualitative examples of
recourses provided by our method in Table 5.
6	Conclusion
In this paper, we propose a cost-based recourse generation method which can optimize for an un-
known user-specific cost function. With simulated cost functions, we show that our method achieves
much higher rates of user satisfaction than comparable baselines. This is particularly promising
since detailed user cost function data is not readily available for most applications. We attribute the
method’s efficacy to our Expected Minimum Cost (EMC) objective term, and we also show that our
discrete optimization algorithm, Cost-Optimized Local Search (COLS), produces large improve-
ments over baseline search methods. Lastly, we observe that our recourses are often more fair than
baseline recourse methods, while offering recourse to a much larger fraction of the user population.
9
Under review as a conference paper at ICLR 2022
7	Ethics Statement
We hope that our recourse method is adopted by institutions seeking to provide reasonable paths
to users for achieving more favorable outcomes under the decisions of black-box machine learning
models or other inscrutable models. We see this as a “robust good,” similar to past commentators
Venkatasubramanian & Alfano (2020). Below, we comment on a few other ethical aspects of the
algorithmic recourse problem.
First, we suggest that fairness is an important value which recourse methods should always be eval-
uated along, but we note that evaluations will depend heavily on the model, training algorithm, and
training data. For instance, a sufficiently biased model might not even allow for suitable recourses
for certain subgroups. As a result, any recourse method will fail to identify an equitable set of
solutions for the population. That said, recourse methods can still be designed to be more or less
fair. This much is evident from our varying results on fairness metrics using a number of recourse
methods. What will be valuable in future work is to design experiments that separate the effects on
fairness of the model, training algorithm, training data, and recourse algorithm. Until then, we risk
blaming the recourse algorithm for the bias of a model, or vice versa.
Additionally, there are possible dual-use risks from developing stronger recourse methods. For in-
stance, malicious actors may use recourse methods when developing models in order to exclude
certain groups from having available recourse, which is essentially a reversal of the objective of
training models for which recourse is guaranteed (Ross et al., 2021). We view this use case as gen-
erally unlikely, but pernicious outcomes are possible. We also note that these kinds of outcomes
may be difficult to detect, and actors may make bad-faith arguments about the fairness of their de-
ployed models based on other notions of fairness (like whether or not a model has access to protected
demographic features) that distract from an underlying problem in the fairness of recourses.
8	Reproducibility Statement
To encourage reproducibility, we provide our source code, including all the data pre-processing,
model training, recourse generation, and evaluation metric scripts in the supplementary material.
The details about the datasets and the pre-processing is provided in Appendix B.1.1. We also provide
clear and concise Algorithms 1, 4, 5 for our cost sampling procedures and our optimization method
COLS in Algorithm 2. Additionally, we also provide formal proof of the Theorem 4.1 stated in paper
in Appendix A.2.2 along with the constructive procedure for the proof is provided in Algorithm 2.
References
Carlos AgUilar-Palacios, Sergio Munoz-Romero, and Jose LUis Rojo-Alvarez. Cold-start Promo-
tional sales forecasting through gradient boosted-based contrastive explanations. IEEE Access,
2020.
Ifeoma Ajunwa, Sorelle Friedler, Carlos E Scheidegger, and Suresh Venkatasubramanian. Hiring by
algorithm: Predicting and Preventing disParate imPact. Available at SSRN, 2016.
Ricardo BaPtista and Matthias Poloczek. Bayesian oPtimization of combinatorial structures.
In Jennifer Dy and Andreas Krause (eds.), Proceedings of the 35th International Confer-
ence on Machine Learning, volume 80 of Proceedings of Machine Learning Research, PP.
462-471. PMLR, 10-15 Jul 2018. URL https://Proceedings.mlr.ρress∕v8 0∕
baptista18a.html.
Yatong Chen, Jialu Wang, and Yang Liu. Strategic recourse in linear classification. CDMDE work-
shop, 2020.
Furui Cheng, Yao Ming, and Huamin Qu. Dece: Decision exPlorer with counterfactual exPlanations
for machine learning models. IEEE Transactions on Visualization and Computer Graphics, 27:
1438-1447, 2021.
Alexandra Chouldechova, Diana Benavides-Prado, Oleksandr Fialko, and Rhema Vaithianathan. A
case study of algorithm-assisted decision making in child maltreatment hotline screening deci-
sions. In Conference on Fairness, Accountability and Transparency, PP. 134-148, 2018.
10
Under review as a conference paper at ICLR 2022
Zhicheng Cui, Wenlin Chen, Yujie He, and Yixin Chen. Optimal action extraction for random
forests and boosted trees. In Proceedings of the 21th ACM SIGKDD international conference on
knowledge discovery and data mining, pp. 179-188,2015.
Susanne Dandl, Christoph Molnar, Martin Binder, and Bernd Bischl. Multi-objective counterfactual
explanations. In Thomas Back, Mike Preuss, Andre Deutz, Hao Wang, Carola Doerr, Michael
Emmerich, and Heike Trautmann (eds.), Parallel Problem Solvingfrom Nature - PPSN XVI., pp.
448-469, Cham, 2020. Springer International Publishing. ISBN 978-3-030-58112-1.
Amit Dhurandhar, Pin-Yu Chen, Ronny Luss, Chun-Chen Tu, Pai-Shun Ting, Karthikeyan Shan-
mugam, and Payel Das. Explanations based on the missing: Towards contrastive explanations
with pertinent negatives. In Advances in Neural Information Processing Systems, 2018.
Dheeru Dua and Casey Graff. UCI machine learning repository, 2017. URL https://archive.
ics.uci.edu/ml/datasets/adult.
Javid Ebrahimi, Anyi Rao, Daniel Lowd, and Dejing Dou. HotFlip: White-box adversarial ex-
amples for text classification. In Proceedings of the 56th Annual Meeting of the Associa-
tion for Computational Linguistics (Volume 2: Short Papers), pp. 31-36, Melbourne, Aus-
tralia, July 2018. Association for Computational Linguistics. doi: 10.18653/v1/P18-2006. URL
https://aclanthology.org/P18-2006.
Michael Feldman, Sorelle A. Friedler, John Moeller, C. Scheidegger, and S. Venkatasubramanian.
Certifying and removing disparate impact. Proceedings of the 21th ACM SIGKDD International
Conference on Knowledge Discovery and Data Mining, 2015.
Oscar Gomez, Steffen Holter, Jun Yuan, and Enrico Bertini. Vice: visual counterfactual explanations
for machine learning models. In Proceedings of the 25th International Conference on Intelligent
User Interfaces, pp. 531-535, 2020.
Masoud Hashemi and Ali Fathi. Permuteattack: Counterfactual explanation of machine learning
credit scorecards. arXiv preprint arXiv:2008.10138, 2020.
J. Henry Hinnefeld, Peter Cooman, Nat Mammo, and Rupert Deese. Evaluating fairness metrics in
the presence of dataset bias. ArXiv, abs/1809.09245, 2018.
Shalmali Joshi, Oluwasanmi Koyejo, Warut D. Vijitbenjaronk, Been Kim, and Joydeep Ghosh.
Towards realistic individual recourse and actionable explanations in black-box decision mak-
ing systems. International Conference on Learning Representations; Workshop on Safe ML,
abs/1907.09615, 2019.
Kentaro Kanamori, Takuya Takagi, Ken Kobayashi, and Hiroki Arimura. Dace: Distribution-aware
counterfactual explanation by mixed-integer linear optimization. In Christian Bessiere (ed.), Pro-
ceedings of the Twenty-Ninth International Joint Conference on Artificial Intelligence, IJCAI-20,
pp. 2855-2862. International Joint Conferences on Artificial Intelligence Organization, 7 2020.
doi: 10.24963/ijcai.2020/395. URL https://doi.org/10.24963/ijcai.2020/395.
Main track.
Amir-Hossein Karimi, G. Barthe, B. Balle, and Isabel Valera. Model-agnostic counterfactual ex-
planations for consequential decisions. International Conference on Artificial Intelligence and
Statistics, abs/1905.11190, 2020a.
Amir-Hossein Karimi, Gilles Barthe, Bernhard Scholkopf, and Isabel Valera. A survey of algorith-
mic recourse: definitions, formulations, solutions, and prospects. CoRR, abs/2010.04050, 2020b.
URL https://arxiv.org/abs/2010.04050.
Amir-Hossein Karimi, Bernhard Scholkopf, and Isabel Valera. Algorithmic recourse: from counter-
factual explanations to interventions. FAccT, 2020c.
Amir-Hossein Karimi, Julius von Kugelgen, Bernhard Scholkopf, and Isabel Valera. Algorithmic
recourse under imperfect causal knowledge: a probabilistic approach. Advances in Neural Infor-
mation Processing Systems, 2020d.
11
Under review as a conference paper at ICLR 2022
A. Kulesza and B. Taskar. Determinantal point processes for machine learning. Found. Trends Mach.
Learn.,5:123-286, 2012.
Jeff Larson, Surya Mattu, Lauren Kirchner, and Julia Angwin.
https://github.com/propublica/compas-analysis, 2016.
Scott M. Lundberg and Su-In Lee. A unified approach to interpreting model predictions. In NIPS,
2017.
Hans Mittleman. Mixed integer linear programming benchmarks (miplib 2010). http://plato.
asu.edu/ftp/milpc.html, 2018.
Ramaravind K Mothilal, Amit Sharma, and Chenhao Tan. Explaining machine learning classifiers
through diverse counterfactual explanations. In Proceedings of the 2020 Conference on Fairness,
Accountability, and Transparency, pp. 607-617, 2020.
Martin Pawelczyk, Klaus Broelemann, and Gjergji Kasneci. Learning model-agnostic counterfac-
tual explanations for tabular data. In WWW, pp. 3126-3132, New York, NY, USA, 2020. Associ-
ation for Computing Machinery. ISBN 9781450370233. doi: 10.1145/3366423.3380087. URL
https://doi.org/10.1145/3366423.3380087.
Martin Pawelczyk, Sascha Bielawski, Johan Van den Heuvel, Tobias Richter, and Gjergji. Kas-
neci. CARLA: A python library to benchmark algorithmic recourse and counterfactual explana-
tion algorithms. In Thirty-fifth Conference on Neural Information Processing Systems Datasets
and Benchmarks Track (Round 1), 2021. URL https://openreview.net/forum?id=
vDilkBNNbx6.
Brett Poulin, Roman Eisner, Duane Szafron, Paul Lu, Russell Greiner, David S Wishart, Alona
Fyshe, Brandon Pearcy, Cam MacDonell, and John Anvik. Visual explanation of evidence with
additive classifiers. In Proceedings Of The National Conference On Artificial Intelligence, vol-
ume 21, pp. 1822. Menlo Park, CA; Cambridge, MA; London; AAAI Press; MIT Press; 1999,
2006.
Rafael Poyiadzi, Kacper Sokol, Raul Santos-Rodriguez, Tijl De Bie, and Peter Flach. Face: Feasible
and actionable counterfactual explanations. In Proceedings of the AAAI/ACM Conference on
AI, Ethics, and Society, AIES ’20, pp. 344-350, New York, NY, USA, 2020. Association for
Computing Machinery. ISBN 9781450371100. doi: 10.1145/3375627.3375850. URL https:
//doi.org/10.1145/3375627.3375850.
Yanou Ramon, David Martens, Foster Provost, and Theodoros Evgeniou. Counterfactual explana-
tion algorithms for behavioral and textual data. Advances in Data Analysis and Classification,
2020.
Peyman Rasouli and Ingrid Chieh Yu. Care: Coherent actionable recourse based on sound counter-
factual explanations, 2021.
Kaivalya Rawal and Himabindu Lakkaraju. Beyond individualized recourse: Interpretable and in-
teractive summaries of actionable recourses. In H. Larochelle, M. Ranzato, R. Hadsell, M. F.
Balcan, and H. Lin (eds.), Advances in Neural Information Processing Systems, volume 33, pp.
12187-12198. Curran Associates, Inc., 2020. URL https://proceedings.neurips.
cc/paper/2020/file/8ee7730e97c67473a424ccfeff49ab20-Paper.pdf.
Marco Tulio Ribeiro, Sameer Singh, and Carlos Guestrin. Anchors: High-precision model-agnostic
explanations. In AAAI Conference on Artificial Intelligence, 2018.
Christian P. Robert and George Casella. Monte Carlo Statistical Methods. Springer Publishing
Company, Incorporated, 2010. ISBN 1441919392.
Alexis Ross, Himabindu Lakkaraju, and Osbert Bastani. Learning models for algorithmic recourse.
In Advances in Neural Information Processing Systems, 2021.
Leslie Scism. New york insurers can evaluate your social media use - if they can prove why it’s
needed, January 2019.
12
Under review as a conference paper at ICLR 2022
Ravi Shroff. Predictive analytics for city agencies: Lessons from children’s services. Big data, 5(3):
189-196, 2017.
Naeem Siddiqi. Credit Risk Scorecards: Developing and Implementing Intelligent Credit Scoring,
volume 3. John Wiley & Sons, 2012.
Dylan Slack, Sophie Hilgard, Himabindu Lakkaraju, and Sameer Singh. Counterfactual explana-
tions can be manipulated. Advances in Neural Information Processing Systems, abs/2106.02666,
2021.
Sohini Upadhyay, Shalmali Joshi, and Himabindu Lakkaraju. Towards robust and reliable algorith-
mic recourse. Advances in Neural Information Processing Systems, 2021.
Berk Ustun, Alexander Spangher, and Yang Liu. Actionable recourse in linear classification. In Pro-
ceedings of the Conference on Fairness, Accountability, and Transparency, FAT* ’19, pp. 10-19,
New York, NY, USA, 2019. Association for Computing Machinery. ISBN 9781450361255. doi:
10.1145/3287560.3287566. URL https://doi.org/10.1145/3287560.3287566.
Suresh Venkatasubramanian and Mark Alfano. The philosophical basis of algorithmic recourse. In
Proceedings of the 2020 conference on fairness, accountability, and transparency, pp. 284-293,
2020.
Paul Voigt and Axel Von dem Bussche. 2018 reform of EU data protection rules.
2018. URL https://ec.europa.eu/commission/sites/beta-political/
files/data- protection- factsheet- changes_en.pdf.
Julius Von Kugelgen, Amir-Hossein Karimi, Umang Bhatt, Isabel Valera, Adrian Weller, and Bern-
hard Scholkopf. On the fairness of causal algorithmic recourse. NeurIPS AFCI Workshop, 2020.
Sandra Wachter, B. Mittelstadt, and Chris Russell. Counterfactual explanations without opening the
black box: Automated decisions and the gdpr. Cybersecurity, 2017.
A Appendix - Objective and Optimization
A.1 Proposed Method
A.1.1 Other Objectives
To obtain feasible a counterfactual set, past works haVe used Various objectiVe terms. We list objec-
tiVes below from methods we compare with.
1.	DICE (Mothilal et al., 2020) optimizes for a combination of Distance Metrics like diversity
and proximity. They model diVersity Via Determinantal Point Processes (Kulesza & Taskar, 2012)
adopted for solVing subset selection problems with diVersity constraints. They use determinant of
the kernel matrix giVen by the counterfactuals as their diVersity objectiVe as defined below.
dpp _diverSity(S) = det (K), WhereKij =	-----------
1 + dist(si, sj)
Here, dist(si, sj) is the normalized distance metric as defined in Wachter et al. (2017) betWeen tWo
state Vectors. Proximity is defined in terms of the distance betWeen the original state Vector and the
CoUnterfaCUtals, Prox(x, Y) = 1 - 右 pi=1 dist(x, Yi), where Yi is a CoUnterfaCtUaL
2.	Actionable Recourse (Ustun et al., 2019) Work under the assumption that all features haVe equal
preference scores for all the users. They define cost function based on the log-percentile shift is
giVen by,
Cost(S + a; S)= X log 1 1 QjQsj( + aj)
j∈JA	1 - Qj(sj)
where Qj (.) is the cumulatiVe distribution function of Sj in the target population, JA is the set of
actionable features and aj is the action performed on the feature j .
13
Under review as a conference paper at ICLR 2022
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
1
2
3
4
5
6
7
8
9
10
11
12
13
Algorithm 1: Procedure for sampling Cost Functions.
Input: A state vector s, Optional: Preferred featured Fp, feature preference scores p, cost
distribution mixing weight α
Output: Sample preference scores p and the cost functions C.
function sampleCostAndPreference(s, α = N oneFp = {},p = N one)
if Fp is empty then
I Fp 〜 RandOmSubset(Fm)	. Sample preferred features.
if p is None then
cOncentratiOn = [1 if f ∈ Fp else 0 fOr f in F]
P 〜DiriChIet(COnCentratiOn)	. Sample feature preference scores.
if α is N One then
I α 〜UnifOrm(0, 1)
Sample cost mixing weight.
C={}
forall fi ∈ F do
μ(fi,LIrn),σ(fi,Lin) <——getLinearCostMean(s,p(fi),fi, Fp)
μ(fi,PeTc),σ(f,Perc) <——getPercentileCostMean(s,p(fi),fi, Fp)
μ(fi)<—α * μ(fi,Lir) + (1 - α) * μ(f-'PeTc)
σ(fi)《—α * σf*Lin) + (1 - α) * σf-Perc)
C(fi) < Beta(μ(fi), σ(fi>)	. Beta with mean and variance μ(fi), σ(fi)
_ C-C∪Cfi)
return p, Cp
Algorithm 2: Cost-Optimized Local Search Algorithm
Input: A state vector s, {Ci}M=ι 〜Du cost distributions
Output: Sbest, a set of generated counterfactuals of size N.
function LocalSearch(s, {Ci}iM=1, hammingDistance = 2)
Initialize
Sbest ∈ RN×d J PertUbCFS(s,hammingDiStanCe)	. Perturb s, N times.
Cb J getCostMatrix(s, Sbest; {Ci}M=ι)	. Incurred costs for Sbest.
while usedBudget < Budget do
S ∈ RN×d J pertubCFS(S best, hammingDistance)
C ∈ RN×M J getCostMatrix(s, S; {Ci}iM=1 )	. Incurred costs for the S .
// Bij = Change in objective when S best[i] — S [j ].
B ∈ RN×N J computeBenefits(Cb, C)	. Refer to Algorithm 3
// Greedily select which pairs to swap given B
replaceIndices J getReplaceIdx(B)
// Swap these pairs and update Cb .
forall originalIdx, replaceIdx ∈ replaceIndices do
[Sbest [originalIdx] = S [replaceIdx]
Cb J getCostMatrix(s, Sbest; {C∙i}M=J
return Sbest, Cb
14
Under review as a conference paper at ICLR 2022
1
2
3
4
5
6
7
8
9
10
11
12
Algorithm 3: Algorithm for Theorem 4.1
Input: Cb , C ∈ RN ×M matrices containing the costs with respect to all cost samples..
Output: B ∈ RN×N, matrix containing the benefits of replacing pairs from Stb-es1t × St
function computeBenefits(Cb, C)
Initialize
B ∈ RN ×N - 0
// Find the indices of the best and second best counterfactual in
S best for each of the M cost function.
b1 ∈ RM = arg maxi Cibj
b2 ∈ RM = arg second maxi Cibj
// Iterate over all pairs of counterfactuals.
forall p, q ∈ [N] × [N] do
// Iterate over cost functions for which pth counterfactual in
S best has the minimum cost.
forall r ∈ {i ∈ [M] | bi1 = p} do
if Cbpr > Cqr then
// This replacement reduces the cost of S best by Cbpr - Cqr .
Bpq + = CPr - Cqr
else
// Cbb2 ,r = cost of second best counterfactual in S best for rth
cost function.
B	Bpq + = CPr — min(Cqr, 0^,「)
return B
A.2 Optimization Methods
A.2.1 Hierarchical Cost Sampling Procedure
To optimize for EMC, we need a plausible distribution which can model users’ cost functions. We
propose a hierarchical cost sampling distribution which provides cost samples that are a linear com-
bination of percentile shift cost (Ustun et al., 2019) and linear cost, where the weights of this com-
bination are user-specific. Percentile shift cost for ordered features is proportional to the change in a
feature’s percentile associated with the change from an old feature value to a new one. E.g., if a user
is asked to increase the number of work hours from 40 to 70, then given the whole dataset, we can
estimate the percentile of users working 40 and 70 hours a week. The cost incurred is then propor-
tional to the difference in these percentiles. The Linear cost for ordered features is proportional to the
number of intermediate states in Q(f) which a user will have to go through while transitioning from
their current state to the final state. E.g., if a user is asked to change their education level from High-
school to Masters then there are two steps involved in the process. First, they need to get a Bachelors
degree and then a Masters degree in which case, the user’s cost is proportional to 2 because of the
two steps involved in the process. To sample a cost function C = {C (f) ∈ RlQ(f) l×lQ(f) | | ∀f ∈ F},
we independently sampleC(f) for each feature (see Algorithm 1). We first randomly sample a subset
of editable features for the user, and then we sample feature preference score pu from a Dirichlet
distribution with a uniform prior over selected features. These will be used to scale costs, such that
a higher value ofp(uf) implies a lower transition cost for feature f. For both the percentile and linear
cost, the cost Ci(jf) of transitioning from feature state i → j, is sampled from a Beta distribution on
the interval [0, 1]. The mean of this distribution depends on the types of cost (linear or percentile)
and the feature type (ordered or unordered). Here, we first obtain one mean for linear cost (μ(f,lin))
and one mean for percentile cost (μ(f,perc)) and then combine them to form a single Beta mean.
Each of the two means is proportional to the change in the feature (in either linear or percentile
terms) when the feature is ordered. For unordered features, the mean is randomly sampled from
15
Under review as a conference paper at ICLR 2022
the unit interval (see Algorithms 5, 4). Then, the linear and percentile means are multiplied with
(1 - p(uf ) ) to scale the transition cost according to the feature preference score. Next, the two means
are combined to obtain a single mean, μ(f) = α * μ(f,lin) + (1 - α) * μ(f,perC), where α ∈ [0,1]
represents whether a user thinks of cost in terms of linear or percentile shift (sampled randomly
from unit interval). Note that the value μ(j) are monotonic, i.e. if the user has to make more drastic
changes to the feature, then the associated cost will be higher. The variance for the Beta distribution,
σ(f), is set to constant value of 0.01. Finally, the cost Cij) is sampled from Beta(μ(f),σ(f)). We
emphasize that this sampling procedure allows users to partially specify their cost functions, e.g. by
denoting which features they prefer to edit (the Dirichlet mean) or the relative difficulty of editing
one feature versus another (pu). If we set α = 0, then the resulting distribution is DperC and with
α = 1 we get the distribution Dlin.
A.2.2 Merging Counterfactual Sets
When searching for a good solution set, it would be useful to have the option of improving on the
best set we have obtained so far using individual counterfactuals in the next candidate set we see,
rather than waiting for a new, higher-scoring set to come along. While optimizing for objectives
like diversity, which operate over all pairs of elements in the set, it is computationally complex to
evaluate the change in the objective function if one element of the set is replaced by a new one. To
evaluate the change in objective in such cases, we need iterate over all pairs of element in the best
and the candidate set and then evaluate the objective for the whole set again. The iteration over both
the sets here is not the hard part but the computation that needs to be done within. For our objective,
we can compute costs for individual recourses rather than sets, meaning we can do a trivial operation
to compute the benefits of each pair replacement. But, if we wanted to do this with diversity then for
each pair of replacement we need to compute additional S distances for each replacement because
the distance of the new replace vector needs to be computed with respect to all the other vectors,
for each iteration of the nested loop. This quickly makes it infeasible to improve the best set by
replacing individual candidates with the best set elements. However, for metrics where it is easy to
evaluate the effect of individual elements on the objective function, we can easily merge the best set
and any other set St from time t to monotonically increase the objective function value.
In our objective function, EMC, we can compute the goodness of individual counterfactuals with
respect to all the Monte Carlo samples (Robert & Casella, 2010). Given a set of counterfactuals we
can obtain a matrix of incurred cost C ∈ RN×M, which specifies the cost of each counterfactual
for each of the Monte Carlo samples. We can use this to update the best set S best using elements
from the perturbed set St at time t. This procedure is defined in algorithm 3. It iterates over all
pairs of element in si∈ Sbest and sj ∈ St and computes the change that will occur in the objective
function by replacing si → sj. Note that we are not recomputing the costs here. Given Sbest, St ,
Cb and C,we can guarantee that we will update the best set Sbest in a way to improve the mean of
the minimum costs incurred for all the Monte Carlo samples. This is shown in algorithm 3 and the
monotonicity of the EMC objective under this case can be formally stated as,
Theorem (Monotonicity of Cost-Optimized Local Search Algorithm). Given the best set, Stb-es1t ∈
RN ×d, the candidate counterfactual at iteration t, St ∈ RN ×d, the matrix Cb ∈ RN ×M and C ∈
RN ×M containing the incurred cost of each counterfactual in Stb-es1t and St with respect to all the M
sampled cost functions {Ci}iM=1, there always exist a Stbest constructed from Stb-es1t and St such that
ExpMinCost(su,Stbest; {Ci}iM=1) ≤ ExpMinCost(su,Stb-es1t; {Ci}iM=1)
Proof. To prove this theorem, we construct a procedure that ensures that the ExpMinCost is mono-
tonic. For this procedure, we prove that the monotonicity of EMC holds. Check algorithm 3 for a
constructive procedure for this proof, which is more intuitive to understand.
We start off by noting that each element of Cibj is the cost of the ith counterfactual sib in the best set
Stb-es1t with respect to the cost function Cj given by Cost(su, sib; Cj). Similarly Cij = Cost(su, si; Cj)
where siis the ith candidate counterfactual. Note that, the EMC is the average of the MinCost with
respect to all the sampled cost function Cj. What this means is that given a pair of counterfactual
from Stb-es1t × St and for each Cj, we can compute the change in the MinCost which we describe
16
Under review as a conference paper at ICLR 2022
later. These replacements can lead to an increase in the cost with respect to certain cost function
but the overall reduction depend on the aggregate change over all the cost functions. Given this, for
each replacement candidate pair in Stb-es1t × St , we can compute the change in EMC by summing
up the changes in the MinCost across all cost functions Cj ; this is called the cost-benefit for this
replacement pair. The cost benefit can be negative for certain replacements as well if the candidate
counterfactual increases the cost across all the cost functions. The pairs with the highest positive
cost benefits are replaced to construct the set Stbest, if no pair has a positive benefit then we keep set
Stb-es1t = Stbest . Hence, this procedure monotonically reduces EMC. We now specify how the change
in MinCost can be computed to complete the proof.
To compute the change in MinCost for a single cost function Ci , first we find the counterfactual
in Stb-es1t with the lowest and second lowest cost which we denote by slb and slb . These are the
counterfactuals which can affect the MinCost with respect to a particular cost function Ci . This is
true because when we replace the counterfactual slb which has the lowest cost for Cj with a new
candidate counterfactual si, there are two cases. Either, Clb1j > Cij or Clb1j ≤ Cij. In case when
the candidate si has lower cost for Cj than Clb1j, i.e. Clb1j > Cij, then the replacement reduces the
cost by Clb j - Cij . In case when the candidate cost for Cj , Cij , is higher than the lowest cost in
the best set Clb1j, i.e. Clb1j ≤ Cij, it means that this replacement will increase the cost for Ci by
Clb j - min(Cij , Clb j ). Here, Clb j is the second lowest cost counterfactual for Ci . Note that the
change in this case will be negative and also depend on the second best counterfactual because once
the slb is removed from the set, the best cost for Ci will either be for slb or si , hence we take the
minimum of those two and then take the difference as the increase in cost. Please refer to Algorithm
3 for a cognitively easier way to understand the proof.	□
A.2.3 Other Methods
In this section, we describe some of the optimization methods used by relevant baselines.
1.	DICE (Mothilal et al., 2020) perform gradient-based optimization in this continuous space while
optimizing for objective defined in Section A.1.1. Their final objective function is defined as
1k	λ k
C(x) = arg min 工 Eloss(f(ci), y) + ʒ1 Edist(ci, x) - λ2 dpp_diversity(ci,..., Ck)
c1,...,ck k i=1	k i=1
where ci is a counterfactual, k is the number of counterfactuals, f(.) is the black box ML model,
yloss(.) is the metric which minimizes the distance between models prediction and the desired
outcome y. dpp_diversity(.) is the diversity metric as defined in Section A.1.1 and λι and λ? are
hyperparameters to balance the components in the objective. Please refer to Mothilal et al. (2020)
for more details.
2.	FACE (Poyiadzi et al., 2020) operates under the idea that to obtain actionable counterfactuals they
need to be connected to the user state via paths that are probable under the original data distribution
aka high-density path. They construct two different types of graphs based on nearest neighbors
(Face-knn) and the -graph (Face-Eps). They define geodesic distance which trades-off between
the path length and the density along this path. Lastly, they use the Shortest Path First Algorithm
(Dijkstra’s algorithm) to get the final counterfactuals. Please refer to (Poyiadzi et al., 2020) for more
details.
3.	Actionable Recourse (Ustun et al., 2019) tries to find an action set a for a user such that taking
the action changes the black-box models decision to the desired outcome class, denoted by +1. They
try to minimize the cost incurred by the user while restricting the set of actions within an action set
A(x). The set A(x) imposes constraints related to feasibility and actionability with respect to fea-
tures. They optimize the log-percentile shift objective (see Section A.1.1). Their final optimization
equation is
min cost(a; x) s.t. f(x + a) = +1, a ∈ A(x)
17
Under review as a conference paper at ICLR 2022
	Adult-Income Binary	COMPAS Binary	Adult-Income	COMPAS
# Continuous features	3	4	2	3
# Categorical features	9	3	10	12
Undesired class	≤ 50k	Will Recidivate	≤ 50k	Will Recidivate
Desired class	> 50k	Won’t Recidivate	> 50k	Won’t Recidivate
Train/val/test	20088/2338/749	1415/229/491	13172/1569/748	5491/705/444
Model Type	ANN(2, 20)	ANN(2, 20)	ANN(2, 20)	ANN(2, 20)
Val Accuracy	82%	69%	81%	61%
Table 4: Table containing data statistics and black-box model details. The binary version of the
datasets are take from (Pawelczyk et al., 2021) whereas the non-binary version are taken from
(Mothilal et al., 2020).
which is cast as an Integer Linear Program (Mittleman, 2018) to provide users with recourses. Their
publicly available implementation is limited to a binary case for categorical features,2 hence we
demonstrate results on the binarized version of the dataset.
B	Appendix - Experiments and Details
B.1	Experimental Setup
B.1.1	Datasets and Black-Box Model
In our experiments, we have two versions of the dataset, one with binary categorical features,
whereas the other with non-binary categorical features. In the main paper, we show results on
the binarized version (Table 1) as an important baseline, Actionable Recourse (Ustun et al., 2019),
operates with binary categorical features.3 The data statistics for all the datasets can be found in
Table 4. In our experiments, for all the datasets, the features gender and race are considered to
be immutable (Mothilal et al., 2020), since we perform subgroup analysis with these variables that
would be rendered meaningless if users could switch subgroups. Other features can either be muta-
ble or conditionally mutable depending on semantics. These constraints can be incorporated into the
methods by providing a schema of feature mutability criterion. Our black-box model is a multi-layer
perceptron model with 2 hidden layers trained on the trained set and validated on the dev set. The
accuracy numbers are shown in Table 4. The test set which is used in the counterfactual generation
experiments only contains users which are classified to the undesired class by the trained black-box
model. Note that our frameworks can operate with any type of model, the only requirement is the
ability to query the model for outcome given a user’s state vector.
B.1.2	Recourse Generation and Evaluation Pipeline
To approximate the expectation in equation 5, our algorithm samples a set of random cost functions
{Ci}M=ι 〜Dtrain, Which are used at the generation time to optimize for the user's hidden cost
function. In the generation phase, we use Equation 6 as our objective. Note that, this objective
promotes that the generated counterfactual set contains at least one good counterfactual for each of
the cost samples, hence this set satisfies a large variety of samples from Dtrain . This is achieved via
minimizing the mean of the minimum cost incurred for each of the Monte Carlo samples (Robert
& Casella, 2010). Equivalently, the objective is minimized by a set of counterfactuals S Where for
each cost function there exists an element in S Which incurs the least possible cost. In practice the
size of set S is restricted, hence We may not achieve the absolute minimum cost but the objective
tries to ensure that the counterfactuals Which belong to the set have a loW cost at least With respect
to one Monte Carlo cost sample. The generation phase outputs a set of counterfactuals S Which is
to be provided to the users as recourse options. Given this set Su, in the evaluation phase, We use
the users simulated cost functions Which are hidden in the generation phase, to compute the cost
incurred by the user as defined in Equation 1 and calculate the metrics defined in the Section 5.1.
2Please refer to the this example Where they mention about these restricted abilities
https://github.com/ustunb/actionable-recourse/blob/master/examples/ex_01_quickstart.ipynb
3The binary datasets can be doWnloaded from https://github.com/carla-recourse/cf-data, Whereas the non-
binary data can be found on https://github.com/interpretml/DiCE.
18
Under review as a conference paper at ICLR 2022
【◎SLL.P ① UUm
Methods Robustness to Distribution Shifts
ιoo-
99-
98-
97-
96-
95-	Illlll
0.00	0.05	0.10	0.15	0.20	0.25	0.30
Distribution Shift
Figure 2: In this plot we show the fraction of
users satisfied vs the distance between the train
and test distributions. The results demonstrate
that as the distance increases the performance
drops a bit and then plateaus, which means that
the method is robust to this kind of distribution
shift. Please refer to Section B.2 for more de-
tails.
O∙I 8696No N606
4llmθMIbo ɔ Φ4uow
Figure 3: This figure shows the performance of
the method on FS@k when recourses are gen-
erated with Monte Carlo cost samples from a
distribution with α-weight varying between 0
and 1, where the user costs follow different α-
weight values varying between 0, 1. Perfor-
mance is robust to misspecification of α. Refer
to Section B.2 for more details.
Remark 1. In the limiting case, as |S | → M and Budget → ∞, RHS of equation 6;
minMinCost(su, S; Ci) → optimum cost, ∀ i ∈ [M], if feasible solutions are possible for all cost
functions samples {Ci}iM=1.
What this remark implies is that, as the set size increases to the number of Monte Carlo samples
M , then the methods can trivially generate one counterfactual for each cost function sample which
minimizes its cost. In that limiting case, we get custom counterfactual for each cost function. Hence,
in the limiting case we can find optimum solution. In the experiment B.2, we show that even with
very less number of counterfactual we are able to achieve good performance implying that in practice
we do not need large set size.
B.2 Additional Research Questions
Q4. Are Solutions Robust to Misspecified Cost Distributions?
Design: In our hierarchical cost sampling procedure, we make minimal assumptions about the user’s
feature preferences if they are not provided by the user. When finding recourses, we select a random
subset of features along with their preference score for each user. However, there are situations
where user preferences may be relatively homogeneous for certain features where people usually
share common preferences. For example, to increase their income, many users might prefer to edit
their occupation type or education level rather than their work hours or marital status. Given the
possibility of this kind of distribution shift in feature preferences, we want to measure how robust
our method is to distribution shift between our hierarchical sampling distribution and the actual cost
distribution followed by users.
In this experiment, we test a case of this kind of distribution shift over cost functions. For users
in the Adult-Income data, we generate recourse sets using Monte Carlo samples from our standard
distribution Dc (Algorithm 1). To obtain hidden user cost functions that differ from this distribution,
we first generate 500 different feature subsets indicating which features are editable, where each
subset corresponds to a binary vector concentration representing a user having specific preferences
for some features over others (see Sec. 4.2 and Alg. 1). Since having different editable features
induces a different distribution over cost functions, we obtain a measure of distribution shift for
each of the 500 concentration vectors by taking an l2 distance between the vector and its nearest
neighbor in the space of concentration vectors used to generate the recourses. We use the nearest
neighbor because the most outlying concentration vectors are least likely to be satisfied by the
recourse set. In other words, the likelihood that a user is satisfied depends on the minimum distance
between their concentration vector and its nearest neighbor in the cost samples used at recourse
19
Under review as a conference paper at ICLR 2022
generation time. Therefore, when the minimum distance increases, there is a greater distribution
shift between the user’s cost functions and those obtained from Dc. Finally, we measure how many
users are satisfied for a given degree of distribution shift.
Results: In Figure 2, we show a binned plot of FS@1 against our measure of distribution shift.
We observe that as the distance between the distributions increases, the fraction of users satisfied
decreases slightly and then plateaus. Even at the maximum distance we obtain, performance has
only dropped about 3 points. This implies that our method is robust to distribution shift in the
cost distribution in terms of which features people prefer to edit. We attribute this result to the
fact that our method assumes random feature preferences and provides multiple recourse options,
each of which can cater to different kinds of preferences. As a result, we achieve a good covering
of the cost function space (see experiments w.r.t. varying recourse set size and number of sampled
cost functions in the Appendix B.2).
Q5. Robustness to Misspecification in Linear vs Percentile based costs?
Design: Our sampling procedure samples cost by taking an α-weighted combination of two different
types of costs, linear and percentile costs. These two cost have different underlying assumptions
about the how users view the cost of transition between the states. We want to test the robustness
of our method in terms of misspecification in users disposition to these types of cost. We perform a
robustness analysis where the users cost function has a different α mixing weight as compared to the
Monte Carlo samples we use to optimize for EMC. This creates a distribution shift in the user cost
function distribution and the Monte Carlo sampling distribution. We vary the user and Monte Carlo
distributions α-weights within the range of 0 to 1 in steps of 0.2. At the extremes values of α = 0, 1,
the shifts are very drastic as the underlying distribution changes completely. In the case when monte
carlo α weight is 0 and user α weight is 1 then Dtrain = Dperc and Dtest = Dlin, simlarly for the
other case we get Dtrain = Dlin and Dtest = Dperc. Please note that the distribution Dlin and Dperc
have completely different underlying principles and are two completely different distributions.
Results: In Figure 3, we show a heatmap plot to which demonstrates the robustness of our method.
The color of the block corresponding to Monte Carlo alpha, αmc = x and the users alpha, αuser = y
represents the fraction of users that were satisfied when αmc = x and αuser = y . This means that
if the user thought of costs only in terms of Linear step involved but the recourse method used
samples with only percentile based cost, still the recourse set can satisfy almost the same number
of users. In Figure 3, the corners correspond to these extreme cases described above, the user
satisfaction for the top left corner (Dtrain = Dperc and Dtest = Dlin) is similar to the bottom left
corner (Dtrain = Dlin and Dtest = Dlin). Similarly things happen for the opposite case which is
denoted by the top-right (Dtrain = Dperc and Dtest = Dperc) and bottom-right (Dtrain = Dlin
and Dtest = Dperc) corners. This means that even when a complete distribution shift occurs the
performance user satisfaction remains similar. This can be attributed to the hierarchical step for
user preference sampling in the procedure because the preferences values can be arbitrary and they
scale the raw percentile and linear cost hence the distribution designed this way to model extremely
diverse types of transition costs.
This means that our methods is robust to misspecification in the users’ notions of thinking
about cost in terms of (relative) percentile shifts or (absolute) linear shifts. The almost
consistent color of the grid means that there is very slight variation in the Fraction of Satisfied
users when the model is tested on out of distribution user cost types.
Q6. Does Method Performance Scale with Available Compute?
Design: In this experiment on the Adult-Income dataset, we measure the change in performance
of all the models as the number of access to the black-box model (budget) increases. Ideally, a
good recourse method should be able to exploit these extra queries and use it to satisfy more users.
We vary the allocated budget in the set {500, 1000, 2000, 3000, 5000, 10000} and report the FS@1.
We run the experiment on a random subset of 100 users for 5 independent runs and then report the
average performance with standard deviation-based error bars in Figure 4.
Results: In Figure 4, we can see that as the allocated budget increases the performance of the
COLS and P-COLS increases and then saturates. This suggests that our method can exploit the
additional black-box access to improve the performance. Other methods like AR and Face-Knn also
20
Under review as a conference paper at ICLR 2022
Figure 4: Figure showing the performance of
different recourse methods as the Budget is in-
creased. These are the average number across
5 different runs along with the standard devia-
tion error bars. For some methods the standard
deviation is very low hence not visible as bars
in the plot. It can be seen that as the budget in-
creases the performance of COLS and P-COLS
increases. Please refer to Section B.2 for more
details.
Figure 5: Figure showing the performance of
different recourse methods as the the number
of counterfacuals to be generated is increased.
These are the average number across 5 differ-
ent runs along with standard deviation error
bars. We see that there is a monotonic increase
in the fraction of users satisfied as the size of
the set increases. We also observe that most of
the performance can be obtained with a small
set size. Please refer to Section B.2 for more
details.
show performance improvement but our method COLS and P-COLS consistently upper-bound their
performance. Our method satisfies approximately 70% of the user with a small budget of 500
and quickly starts to saturate around a budget of 1000. This suggests that our methods are suitable
even under tight budget constraints as they can achieve good performance rapidly. For example, in
a real-world scenario where the recourse method is deployed and has to cater to a large population, in
such cases there might be budget constraints imposed onto the method where achieving good quality
solution quickly is required. Lastly, for DICE and Random search the performance on the FS@1
increase by a very small margin and then stays constant as these methods are trying to optimize for
different objectives which don’t align well with user satisfaction as demonstrated in Section 5.2.
Q7. Does providing more options to users help?
Design: In this experiment, we measure the effect of having flexibility to provide the user with
more options, i.e. a bigger set S. The question here is that can the methods effectively exploit this
advantage and provide lower cost solution sets to the user such that the overall user satisfaction is
improved. In this experiment on the Adult-Income dataset, we take a random subset of 100 users
and fix the budget to 5000, Monte Carlo cost sample is set to 1000 and then vary the size of the set
S in the set {1, 2, 3, 5, 10, 20, 30}. We restrict the size of the set to a maximum of 30 as beyond
a point it becomes hard for users to evaluate all the recourse options and decide which one to act
upon. We run 5 independent runs for all the data points and plot the mean performance along with
standard deviation error bars. In Figure 5, we plot the fraction of users satisfied @1 as the size of
set S is increased.
Result: We observed that COLS and P-COLS monotonically increase the FS@1 metric as |S|
increases from 1 to 30. This is consistent with the intuition behind our methods (See Figure 1, sec-
tion 4.1, B.1.2 for more details). It is a fundamental property of our objective that as |S| increases
towards M which is 1000 in this case, then the quality of the solution set should increase and reach
the best possible value that can be provided under the user’s cost function. We note empirically that
smaller set size |S | between 3 to 10 is enough in most practical cases to reach close to maximum
performance. Additionally, even with |S| ∈ {1, 2, 3} our methods significantly outperform all the
other methods in terms of the number of users satisfied. This property is useful in real-world sce-
narios where the deployed recourse method can provide as little as 3 options while still satisfying
a large fraction of users. Additionally, we also see improvement in the case of AR and Face-Knn
methods as |S| increases. Note that Randoms Search’s performance doesn’t change as we increase
the set size because the method doesn’t take local steps from the best set and samples random points
from a very large space, hence it is much harder to end up with low-cost counterfactuals.
21
Under review as a conference paper at ICLR 2022
Feature Name	State Vector	Editable Features	Preference scores	Recourses	Cost
Age	24	No	0		
Workclass Education-Num	Private 10	No No	0 0	(CaPital Loss: 0 → 1)	0.009
Martial-Status	Married	No	0		
Occupation	Other	Yes	0.055		
Relationship Race	Husband White	No No	0 0	(Occupation: Other → Manager)	0.378
Gender	Male	No	0		
Capital Gain	0	No	0		
Capital Loss	0	Yes	0.944	O Occupation: Other → Manager、	0.387
# Work Hours	40	No	0	(	Capital Loss: 0 → 1	J	
Country	US	No	0		
Age	^45	No	0		
Workclass	Private	No	0	(Capital Loss: 0 → 1)	0.071
Education-Num	7	Yes	0.537		
Martial-Status	Married	No	0		
Occupation	Other	No	0	(Capital Gain: 0 → 1)	0.106
Relationship	Non-HUSband	No	0		
Race	White	No	0		
Gender	Female	No	0	(Education-Num: 7 → 10)	0.187
Capital Gain	0	Yes	0.078		
Capital Loss	0	Yes	0.240		
# Work Hours	32	Yes	0.142	(# Work Hours: 32 → 70)	0.695
Country	US	No	0		
Table 5: Table providing qualitative examples for two users from the dataset. We show each users
state vector, the features that user is willing to edit, the preference scores for those editable features,
the recourses provided and the cost of the generated recourses. In the first example we see that user
highly prefers the feature capital loss and the recourse which suggests edit to that has the lowest
cost for the user. Whereas, the recourse which makes changes to both Occupation and Capital Loss
has the highest cost as its changing multiple features. For the second user, we see that the most
preferred feature is Education-Num but the changes suggested in the recourse requires three steps
7-8-9-10, hence the cost for that recourse is not the lowest but still relatively low. Whereas, the
recourse suggesting smaller changes to Capital Loss which is the second most preferred feature has
the lowest cost for the user.
Q8. Does increase the number of Monte
Carlo samples help with user satisfaction?
Design: In this experiment, we want to demon-
strate the effect of increasing the number of
Monte Carlo samples on the performance of our
COLS method. We take a random subset of 100
users, a budget of 5000, |S | = 10. We vary the
number of Monte Carlo samples (M) in the set
{1, 5, 10, 20, 30, 100, 200, 300, 500, 1000} and
compute the user satisfaction. We ran 5 dif-
ferent runs with different Monte Carlo samples
and show the average FS@1 along with the
standard deviation in the Figure 6.
Results: We observe that as the number of
Monte Carlo samples increases, the perfor-
mance of the method on the FS@1 metric
monotonically increases. This supports the in-
tuition underlying our method (see Figure 1).
That is, given a user with a cost function Cu as
we get more and more samples from the cost
distribution Du the probability of having a cost
sample similar to Cu increases and hence the
fraction of satisfied users increase. It is impor-
User Satisfaction vs # Monte Carlo Sample
80-
75-
I 70-
©
∞ 65-
60-
55-
1	5	10	20	30 100 200 300 500 1000
Num MC Samples
Figure 6: Figure showing the performance of the
COLS method as the number of Monte Carlo
samples increase. These are the average number
across 5 different runs along with standard devia-
tion error bars. There is a steep increase and then
the performance saturates. This implies that in
practice we do not need a large number of samples
to converge to the higher user satisfaction. Refer
to Section B.2 for more details.
tant to note that empirically the method’s performance approaches maximum user satisfaction
with as low as 20 Monte Carlo samples. In real-world scenarios, where the deployed model is
catering to a large population this can lead to small recourse generation time, hence making it more
practical.
/ 1
22
Under review as a conference paper at ICLR 2022
Data	Method	Metrics						
Adult-Income - NB	DICE Random COLS P-COLS	Cost Metrics			Distance Metrics			Val
		FS@1	PACa)	Cov	Div	Prox	Spars	
		6.28 0.08 72.67 70.03	1.45 2.42 0.36 0.39	27.01 17.41 74.60 72.81	53.01 70.35 29.27 29.85	57.02 33.32 79.06 78.45	47.80 22.45 76.64 76.29	86.20 75.71 97.85 92.30
	DICE	14.86	1.02	25.45	27.88	82.38	69.44	99.86
COMPAS - NB	Random	1.31	1.87	21.76	49.07	54.10	42.34	67.82
	COLS	67.34	0.31	68.11	20.53	85.47	82.34	95.97
	P-COLS	70.86	0.35	72.03	21.03	85.48	82.88	91.93
Table 6: Table comparing different recourse methods across various cost and distance metrics on
Non-Binary versions of the datasets (Section B.1.1).The numbers reported are averaged across 5
different runs. Variance values have been as 89% of them were lower than 0.05, with the maximum
being 0.86. FS@1: Fraction of users satisfied at k = 1. PAC: Population Average Cost. Cov:
Population Coverage. For all the metrics higher is better except for PAC where lower is better.
Q9. Qualitative examples of the recourses generated for some of the users.
In Table 5, we show a few examples of users along with their state vector, their editable features,
their preference scores along with the recourses provided to them and their cost.
Q10. Comparison of methods on Non Binary Dataset?
In Table 6, we show the results on the non-binary version of the dataset. We observe similar perfor-
mance on and trends in these results as well. COLS and P-COLS performs the best in terms of user
satisfaction.
23
Under review as a conference paper at ICLR 2022
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
Algorithm 4: Sampling procedure for Percentile Cost Mean
function getPercentileCostMean(s, p(fi) , fi, Fp)
// si value of feature fi in s.
if fi ∈/ Fp then
μ(fi)(si,.) = ∞
μ(fi)(si, Si) = 0
else
if fi is ordered then
if fi can only increase then
(∣getPercentile(x) — getPercentile(si)∣ ∀x > Si
0	∀x = si
∞	∀x < Si
else if fi can only decrease then
{ IgetPercentile(Si) — getPercentile(x~)∖ ∀x < Si
0	∀x = Si
∞	∀x > Si
else if fi can both increase or decrease then
(∣getPercentile(x) — getPercentile(si)∖ ∀x > Si
0	∀x = Si
|getP ercentile(Si) — getP ercentile(x)| ∀x < Si
else if fi is unordered then
I μ(fii)(Si,.) = Uniform(0, 1)
μ(fi')(Si,.) ― μ(fi)(Si,.) * (1 — p(fi))
σ(fi)(Si,.) — 0.01
return μ(fi), σf
24
Under review as a conference paper at ICLR 2022
Algorithm 5: Sampling procedure for Linear Cost Means
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
function getLinearCostMean(s,p(fi),力，FP)
if fi ∈ FP then
μ(fi)(si,.) = ∞
μ(fi)(si, si) = 0
else
if fi is ordered then
if fi can only increase then
{∣{y | y>si∧y≤x}∣	UT、
|{y	I y>%}∣	∀x	>
0	Vx	=
∞	Vx	<
else if fi can only decrease then
{|{y | y<Si∧y≥x}l	U
∣{y	I	y<Si}∣	Vx	<
0	Vx	=
∞	Vx	>
else if fi can both increase or decrease then
{∣{y ∣ y>si∧y≤χ}∣
l{y ∣ y>si}∖
0
∣{y ∣ y<si∧y≥x}
∣{y ∣ y<si}∣
Vx >
Vx =
Vx <
si
si
si
si
si
si
si
si
si
else if fi is unordered then
I μ(fi)(si ,.) = UnifOrm(0, 1)
μ(fi)(si,.) - μfi)(si,.) * (1 -p(fi))
σ(fi)(si,.) J 0.01
return μ(f), σ(fi)
25