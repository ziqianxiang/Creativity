Under review as a conference paper at ICLR 2022
Federated Inference through Aligning Local
Representations and Learning a Consensus
Graph
Anonymous authors
Paper under double-blind review
Ab stract
Machine learning is faced with many data challenges when applied in practice.
Among them, a notable barrier is that data are distributed and sharing is unreal-
istic for volume and privacy reasons. Federated learning is a recent formalism to
tackle this challenge, so that data owners can develop a common model jointly but
use it separately. In this work, we consider a less addressed scenario where a da-
tum consists of multiple parts, each of which belongs to a separate owner. In this
scenario, joint efforts are required not only in learning but also in inference. We
study federated inference, which allows each data owner to learn its own model
that captures local data characteristics and copes with data heterogeneity. On the
top is a federation of the local data representations, performing global inference
that incorporates all distributed parts collectively. To enhance this local-global
framework, we propose aligning the ambiguous data representations caused by
arbitrary arrangement of neurons in local neural network models, as well as learn-
ing a consensus graph among data owners in the global model to improve perfor-
mance. We demonstrate effectiveness of the proposed framework on four real-life
data sets including power grid systems and traffic networks.
1	Introduction
Machine learning models become increasingly data hungry as the promise of deep learning continues
to realize. More and more applications grow in scale thanks to the availability of distributed data
across devices and organizations. Federated learning (McMahan et al., 2017; Yang et al., 2019; Li
et al., 2019; Kairouz et al., 2019) emerges as a formalism that allows data owners to collaboratively
train a common model by using one’s own data without sharing. Such a formalism is poised to
resolve the challenges of expensive data communication and the risk of privacy violation, in light of
policies such as the General Data Protection Regulation (Albrecht, 2016).
An issue less addressed by federated learning is the inference process. In fact, inference therein is
trivial: once the common model is learned, each data owner retains a copy and applies it on local
data, independently of other owners. However, such a scenario is not the only one how data are
distributed in practice. In this work, we consider the following scenario: a datum has multiple parts,
each of which belongs to a separate owner. Then, the inference must be collectively performed by
all participating owners, since none of them alone possesses the entire information.
Vertical federated learning studies such a scenario (Hardy et al., 2017; Hu et al., 2019; Chen et al.,
2020). This concept is figuratively named through cutting the data matrix vertically along the feature
axis, rather than the data axis. From the sporadic literature addressing in this scenario, methods gen-
erally introduce model parameters distributed with data parts and optionally global parameters that
reside in a central server. All parameters are learned jointly, causing however a practical drawback—
expensive coordination (even synchronization) is required among data owners and the central server
(if present).
Consider a live example—the national electricity grid, over which thousands of phasor measurement
units (PMUs) have been deployed to monitor the grid condition (Smartgrid.gov). PMU measure-
ments, as time series data, are owned by several parties. These data may be used to train machine
learning models that identify grid events (e.g., fault, oscillation, and generator trip). Such an event
1
Under review as a conference paper at ICLR 2022
detection system relies on collective series measurements at the same time window but distributed
across different data owners. To minimize coordination among owners and maximize autonomy, it
is more desirable if each maintains a model of their own and does not participate joint training.
In this work, We propose a local-global model framework that maintains data owner autonomy while
staying effective in global inference. Therein, each data owner trains a local model with its data part;
the training is independent and incurs no coordination. In the deep learning terminology, the local
models produce data representations for input data. Then, a central server takes these representations
as input and trains a model for global inference.
We term the scenario where inference is collectively performed by data owners but training incurs
little coordination among them, federated inference, to distinguish federated learning and vertical
federated learning. The local-global framework we propose for this scenario, however, bears two
technical challenges. One is the ambiguity of local data representations, because feature dimensions
can be arbitrarily permuted without changing the local model. Another challenge is how the global
model leverages innate interactions of local data missed by independent local models.
We resolve the first challenge through aligning the feature dimensions across all local represen-
tations. We resolve the second challenge through employing graph neural networks as the global
inference model, where the graph corresponds to the explicit or implicit relational structure of the
data owners. When such a graph is not present, we treat the combinatorial graph structure as a
random variable of the Bernoulli distribution and optimize the distribution parameters as well.
We summarize the contributions of this work as follows.
1.	We formalized federated inference, a less addressed scenario of machine learning with distributed
data, where inference is conducted jointly by data owners without data sharing and coordinated
training (Section 2).
2.	We propose an inference framework that consists of autonomous local models and a central model
that digests local data representations and produces a global output (Section 3).
3.	We address the ambiguity challenge of this framework through aligning local representations
(Section 4) and address the missing of local model interactions through employing a graph neu-
ral network in the central global model. We further propose to simultaneously learn the graph
structure if not present (Section 5).
4.	We study approaches to latent alignment and approaches to graph structure learning and develop
theoretical insights into these approaches (Theorems 1-3). As a byproduct, a more efficient
Bernoulli sampling method icdf is proposed to sample graphs for structure learning.
5.	We demonstrate experiments with four real-life data sets including power grids and traffic net-
works and show the effectiveness of the proposed framework (Section 6).
2	Problem Setting
Federated inference refers to a machine learning scenario where both training and inference are con-
ducted on distributed data collectively by data owners. Each owner enjoys data and model autonomy
but is subject to centralized coordination that produces a global prediction. This scenario stands in
contrast to federated learning, whose inference process is local and separate among owners.
Formally, we use a superscript i to index data owners. Let x be a datum with label y and let xi be
the part of datum the ith owner possesses; that is, x = (x1, x2, . . . , xn) with n owners. The problem
is to learn a model
y = f(x) = f(x1,x2,...,xn)	(1)
collectively with all data parts and to perform inference jointly by all owners. Additionally, owners
share neither data nor models with each other for, e.g., privacy reasons. Moreover, owners do not
participate joint training, which often incurs expensive coordination.
Similarity to federated learning. Federated inference shares the defining data characteristics of
federated learning, first coined in McMahan et al. (2017): distributed, non-IID, and unbalanced.
Data are distributed among owners but not shared. In fact, data may even be heterogeneous. For
example, time series measures of the power grid may have different attribute dimensions and may be
under different sampling frequencies. As a result, data size may vary significantly among owners.
2
Under review as a conference paper at ICLR 2022
Dissimilarity to federated learning. The root of the differences between these two concepts is the
constituent of one datum (data point). In federated learning, a data point is the basic unit of data and
thus all owners learn a common model but use it separately. On the other hand, federated inference is
concerned with data split in parts across owners. All parts of a data point contribute to the inference
collectively.
Similarity and dissimilarity to vertical federated learning. Both concepts are concerned with
the split of a datum across owners. However, approaches taken for vertical federated learning differ
substantially from ours because of joint training among owners. From the less prolific literature, two
lines of work are noted. One takes the data matrix literally, by assuming tabular data and studying
linear models, where model parameters have natural correspondence to the data parts (Hardy et al.,
2017; Nock et al., 2018; Heinze et al., 2014; 2016). Often, these approaches are hard to generalize
to complex data and/or many owners. Another line of work uses a local-global model framework
similarly as we do but jointly trains these two parts, incurring expensive communication and creating
dependence of local models (Hu et al., 2019; Chen et al., 2020).1 In contrast, we allow data owners
to train their models independently, maintaining local model autonomy.
Data example. Let us consider the power grid.
Figure 1 pictorially illustrates PMU measure-
ments distributed across data owners. A panel
of time series corresponds to a specific time
window and the series collectively represent
one data point, which the event detection sys-
tem classifies. In this simplified illustration,
each data owner possesses one series recorded
by one PMU; but in practice they may own
different amounts of PMUs (and thus series).
Moreover, the series may differ in length be-
cause of varying sampling frequencies; and the
series are multivariate with possibly different
number of variates. All these variations con-
tribute to data heterogeneity, which necessitates
the construction of separate local models. Note
that if an event does not cascade over the en-
tire grid, some local models may report event
whereas others report normal, resulting in con-
flicting opinions. A consensus (global) model
is responsible to resolve the conflict. Addition-
ally, missing data may occur.
Figure 1: Federated inference: A global label is
predicted collectively based on local data from
multiple owners. Local data may be heteroge-
neous and missing data may occur.
3	Federated Inference Framework
As such, the proposed framework for federated inference consists of local models fi and a global
model g, such that their composition is the sought f denoted in (1). Each data owner i possesses
a local model trained with its data, independently of other owners. This way, no data sharing is
invoked and privacy is of minimal concern. However, because the local models lack a global vision
and may be conflicting, a central (global) model is key to coordinating the local opinions for final
prediction. To maintain autonomy, local models are frozen once pretrained and will not join the
training of the global model. Data owners send local data representations to a centralized server
for global model training (and inference). In other words, the global model queries neither the raw
data nor the local models from data owners. As long as owners agree to send the less decipherable
representations to the central server, global inference can be made.
Local models. We treat a neural network except the final output layer as a feature extractor, which
produces the representation hi of an input xi ; and treat for simplicity the output layer as a logistic
regression. That is, a local model fi reads:
fi(xi) = softmax(W ihi + bi) where hi = embedding(xi).	(2)
1 Note that the approach proposed by Hu et al. (2019) assumes no parameters for the global model. Were
global parameters present, gradient communication is inevitable.
3
Under review as a conference paper at ICLR 2022
owner 1
owner 2
owner 3
owner 4
owner 5
owner 6
latent
representation
I∙ ∙ ■
4l∙∙ ∙l
I ∙∙ ∙l-
l∙∙∙∙∙l
Tl∙ ∙∙ I卜
latent
dimension
alignment
l∙ ∙ ■
I ∙∙∙∙l
l∙∙∙ ■
M ∙	■
l∙ ∙∙ I
I ∙ ∙
graph
neural network
I < ∙∙ I
Independent local models
Global model
Figure 2: Federated inference framework. Local models are trained independently and separately
from the global model. The algorithm is summarized in Algorithm 1 in supplement Section C.
We interchangeably use “representation”, “embedding”, and “latent vector” to mean hi. These
hi ’s are assumed to have the same shape across i, although xi can have different shapes and the
embedding function can have different architectures to cope with data heterogeneity. A simple
example of the embedding function is a fully connected layer hi = ReLU(U i xi + ci); but an
arbitrarily complex function is applicable.
Global model. The global model is a function g of all local representations:
yb=g(h1,h2,...,hn).	(3)
An example of g is a fully connected layer, followed by mean pooling and another fully connected
layer:
b = SoftmaX (Wι ∙ n1 Pn=1 ReLU(W0hi + bo) + bi) .	(4)
Challenges. Two considerations are pertinent to this framework. First, when the latent dimensions
have semantic meaning (e.g., when the local models are trained to yield disentangled representa-
tions (Higgins et al., 2018)), each latent feature of the local representations may not match, because
an arbitrary permutation of the latent dimensions does not change a local model. Second, a naive
mean pooling as in (4) may miss the interdependencies between local data, leading to a less well per-
forming global model. Such interdependencies naturally occur in the power grid example because
of the physics of an electricity network. Hence, in subsequent sections, we use latent alignment to
address the first problem and graph neural network to address the second one. Incorporating these
two components, we show the full, proposed framework in Figure 2 and Algorithm 1 (supplement
Section C).
4 Aligning Local Representations
For the global model to be meaningful, the feature dimensions of the local representations hi should
match. For example, in (4), all hi ’s multiply the same weight matrix W0 ; in other words, each
element of hi corresponds to one input neuron of the initial fully connected layer. Permutations
of the elements will destroy the correspondence. That is, even if the local models are fixed, the
arbitrary arrangement of the feature dimensions of the latent vectors causes ambiguity of what an
optimal global model can be built.
Mathematically, let us use a vector p to denote permutation and place a superscript i whenever
necessary. The ith local model (2) can be equivalently written as
fi (xi) = SoftmaX(W i [:, pi]hi [pi] + bi) where hi [pi] = embedding(xi ; pi),	(5)
for any permutation pi, as long as the embedding function is able to produce a permuted hi [pi]
under the same input xi . Such a requirement can be easily satisfied if the embedding function is a
fully connected layer (i.e., h[p] = ReLU(W [p, :]x + b[p])). In fact, it is satisfied by most neural
networks as well. In the supplement, we give another example: the GRU (Cho et al., 2014).
4
Under review as a conference paper at ICLR 2022
KKjTj11	-	11
Hence, we propose to align the feature dimensions across all local vectors hi to disambiguate the
ambiguity. This proposal amounts to modifying the global model (3) to the following:
yb=g(P1h1,P2h2,...,Pnhn),	(6)
where Pi is an alignment matrix for each data owner i.
Two approaches of defining Pi exist. The first approach is a soft alignment, which treats each Pi a
free parameter matrix to optimize. It may be square or rectangle, the latter case indicating a change
of the number of features.
The second approach is a hard alignment, which treats each Pi a permutation matrix. Learning
permutation matrices is challenging, however, because they correspond to combinatorial structures
and are unsuitable for gradient-based training. We follow Mena et al. (2018); Emami & Ranka
(2018) and relax Pi by a doubly stochastic matrix, which can be differentiably parameterized by the
Sinkhom-KnoPP algorithm (Sinkhorn & Knopp, 1967). Specifically, starting from a nonnegative
square matrix K0 and column vectors r0 = c0 = 1 of matching lengths, define the sequence
cj+1 = 1	(K0T rj) and rj+1 = 1	(K0cj), forj = 0, 1, . . .	(7)
Then, under a mild condition, Kj := diag(rj )K0 diag(cj ) converges to a doubly stochastic matrix.
We truncate the sequence at the Tth step and treat KT as an approximation of Pi.
Despite the advocation by Mena et al. (2018); Emami & Ranka (2018), we obtain the following
convergence result of Sinkhorn-Knopp, which reveals no free lunch.
Theorem 1 (informal). Under a condition of K0, there exists a positive integer J and a constant
CJ such that for all j ≥ J,
≤ CJ(1 + σ22)σ22(j-J),
where σ2 ≤ 1 is the second largest singular value of the limit of Kj .
For a formal statement and the analysis, see supplement Section D and Theorem 5. The result
suggests that for a desirable limit being a permutation matrix, whose σ2 = 1, the error O(σ22j) does
not drop. In practice, to expect an approximate permutation matrix, σ2 ≈ 1 and the convergence is
exceedingly slow. The practical usefulness of (7) depends on the learned quality of K0 .
The soft and hard alignment approaches have pros and cons. The hard approach maintains the
correspondence of each feature dimension of the latent vectors while the soft approach does not.
Maintaining the dimension correspondence is an advantage, especially for local models that pro-
duce disentangled latent representations (Higgins et al., 2018), because each feature dimension is
equipped with a semantic meaning that controls a certain aspect of the data. On the other hand, the
soft approach is more straightforward and the hard approach is based on an algorithm that barely
converges. In practice, we observe that neither approach decisively outperforms the other in feder-
ated inference.
5 Learning a Consensus Graph
The example global model (4) performs a naive averaging for the local representations. Since data
owners are often interconnected, a more expressive model exploits their relational interactions to
improve inference (Battaglia et al., 2018). To this end, we propose to use a graph neural network
(GNN) (Zhang et al., 2020; Wu et al., 2021) to process the latent representations.
Many GNNs are applicable; we focus on GCN (Kipf & Welling, 2017) for its simplicity. Let A be
the graph adjacency matrix and let H be the matrix of aligned local representations:
--(P1 h1)T-
H =	:	.
.
-(Pnhn)T-
Traditionally, GCN was designed for node classification, but we modify it slightly for our purpose
as
b =SoftmaX (1ITJb ∙ ReLU(AHW0) ∙ W1) ,	(8)
5
Under review as a conference paper at ICLR 2022
where Ab is a normalization of A (see (Kipf & Welling, 2017) for details) and W0 and W1 are
weight matrices. The modification is the inclusion of 11T as pooling before output. Modulo this
modification, the formula (8) is a standard one used in the literature, with the bias terms omitted.
It is interesting to note the equivalence of GCN (8) and the graph-agnostic model (4) when Ab is
replaced by the identity matrix (omitting bias terms).
In GCN, A corresponds to the consensus graph among local owners as graph nodes. If such a graph
is not present, it is possible to learn one such that (8) still outperforms (4). In this case, we treat A
as a random variable of the matrix Bernoulli distribution, where the success probabilities are free
parameters to learn. Formally, the elements Aij are independent and each follows Ber(θij), where
θij denotes the corresponding probability (Kipf et al., 2018; Shang et al., 2021). Then, the global
model g has W0, W1, the Pi’s, as well as θ, as parameters. Following Franceschi et al. (2019);
Shang et al. (2021), we formulate the training loss as an expectation over A’s distribution and draws
a sample A to obtain an unbiased estimate of the loss as well as the gradient, in each stochastic
optimization step.
The central challenge of this approach is that A (and hence also the loss) is not differentiable with
respect to θ. A popular remedy is the Gumbel softmax reparameterization trick (Jang et al., 2017;
Maddison et al., 2017). In what follows, for simplicity of exposition, we treat θ a scalar rather than a
matrix. The Gumbel trick works in the following manner. Let Cat(π) be the categorical distribution
with probability vector π and let g, of the same shape as π, be a vector variable whose elements are
iid 〜Gumbel(0,1). Then, the vector random variable
y = Softmax((logπ + g)∕τ), τ > 0	(9)
admits a distribution converging to Cat(π) when τ → 0. Hence, to sample Ber(θ) approximately
but differentiably, it suffices to let π = [θ, 1 - θ] and use y1 as the sample.
In order to obtain one Bernoulli sample, the Gumbel trick requires to sample the Gumbel distribution
twice. We consider an alternative that samples any appropriate distribution only once.
Definition 1. Let F be the cdf of an arbitrary continuous probability distribution. Sample s from
this distribution and let
z = sigmoid((F -1(θ) - s)∕τ), τ > 0.	(10)
We call this method icdf.
The name icdf is owing to the use of F -1. The reader should not confuse this method with the
inverse transform method for sampling a random variable with a particular cdf F . Here, we use
any F to sample an (approximate) Bernoulli distribution. The following result qualifies z to be an
approximate Bernoulli variable. The proof, as well as those of subsequent theorems, is given in the
supplement.
Theorem 2. For all τ > 0, θ ∈ (0, 1), and t ∈ [0, 1], if the distribution with cdf F is finitely
supported on [a, b], then
(0 if t < Sigmoid((F-1(θ) 一 b)∕τ),
Pr(Z ≤ t) = < 1 if t > Sigmoid((F-1(θ) — a)∕τ),	(11)
11 — F(F-1(θ) + T log(tT - 1)) otherwise.
On the other hand, ifthe distribution is not finitely supported (i.e., a = -∞ and/or b = +∞), (11)
still holds because either, or both, of the first two cases will not be invoked. As a consequence, the
distribution ofz converges to Ber(θ) as τ → 0.
It is imperative to understand the rate of convergence of y1 (Gumbel trick) and that of z (icdf
method). While one may take the usual convergence-in-distribution approach, the complex forms of
the cdf (e.g., (11)) render the analysis difficult. Instead, we take the convergence-in-mean approach
and calculate BiaS(x) = E[x] - θ. We derive the following result.
Theorem 3. When τ is small,
Bias(yι) = ɪ τ 2π2θ(1 — θ)(1 — 2θ) + O(τ 4),	(12)
BiaS(Z) = 6 τ2π2F00(F-1(θ)) + O(τ4).	(13)
Moreover, when F is the Cdfofa normal variable 〜 N (0, σ2), then
BiaS(Z)=—备τ2π3 erf-1(2θ — 1)e-(erf-1(2θ-1))2 + O(T4).	(14)
6
Under review as a conference paper at ICLR 2022
Theorem 3 suggests that the icdf method converges equally fast as does the Gumbel trick (both on
the order of O(τ 2)). On the other hand, the biases depend on θ. Thus, one cannot set temperatures
τ , independently of the desired probability θ, to equate the two biases. In practice, τ is a tunable
hyperparameter and we use the same tuning range to fairly compare the Gumbel trick and icdf. The
rationale is justified in supplement Section F.
We conclude this section by stressing the advantage of the proposed icdf method for differentiably
sampling graphs for global model training: it requires fewer random number generations than does
the Gumbel trick, saving time and memory.
6 Experiments
In this section, we demonstrate comprehensive experiments to show that federated inference can be
effectively conducted by using the proposed framework.
Data sets. We use four real-life, time series data sets. Two are PMU data collected from multiple
data owners of the U.S. power grid. For proof of concept, we smooth out heterogeneity and prepare
homogeneous data sets. Such a preprocessing is sufficient to test the proposed techniques under
minimal impact of the complication by the otherwise diverse local models. Since the PMU data
sets are proprietary, we also use two public, traffic data sets (Li et al., 2018) for experimentation. A
summary of these data sets is given in Table 1 and the processing details are given in the supplement.
Table 1: Data sets.
	metr-la	pems -bay	pmu-b	pmu-c
# Data samples	2856	4343	4853	1884
# Data owners	207	325	43	188
Series length	12	12	30	30
# Features	1	1	2	2
# Classes	2	2	4	4
Missing data?	no	no	yes	yes
Given graph?	yes	yes	no	no
Figure 3: Distributions of prediction en-
tropy across local models.
Experiment setting. All local models are LSTM (Hochreiter & Schmidhuber, 1997) with the same
hyperparameters, but pretrained separately by using local data. The local models are not fine-tuned
in the training of the global model. Each data set is split randomly for training/validation/testing.
See the supplement for further details.
Conflicting local predictions. We first show that local models do not produce consistent predic-
tions, which justifies the effort of training a global model and performing federated inference. For
each datum, we compute the entropy of the predicted labels and summarize the entropies for all data
into a distribution, plotted in Figure 3. Recall that the lower the entropy, the more consistent the
local predictions. The figure, however, shows that a substantial amount of entropies is away from
zero, suggesting that local predictions are inconsistent.
Effectiveness of the proposed framework. We make two sets of comprehensive comparisons to
evaluate the effectiveness of the proposed framework. The first set, as outlined in Table 2, compares
it with a number of straightforward baselines (A-F) and methods outside the federated inference
setting (A and K). This set contains several alignment strategies for local models: (G) no alignment;
(I) soft alignment; and (K) hard alignment. A straightforward variant between G and I is H, where
a common weight matrix W is used for all local models, serving as an alternative to alignment.
Methods G to J use graph structure learning (icdf method) as the global model.
One sees that methods A to D, either lacking a local model or a global model, perform poorly as
expected. Methods E to H perform better than A to D, but they lack a proper alignment of the local
models and hence are outperformed by methods I and J that perform alignment. Between the two
strategies, neither decisively wins over the other. The advantage of soft alignment is its simplicity
and that of hard alignment is the preservation of neuron correspondence. Finally, method K (end-to-
7
Under review as a conference paper at ICLR 2022
Table 2: Effectiveness of latent alignment in a graph-based global model. Superscript numbers are
standard deviations. ? Note that A and K are not applicable to the federated inference setting.
	metr-la		pems-bay		pmu-b		pmu-c	
	f1	AUC	f1	AUC	f1	AUC	f1	AUC
A: Common model ?	.255.000	-	.334∙000	-	.360.000	-	.286.000	-
B: Local model + majority voting	.114.000	-	.089.000	-	.291.000	-	.182.000	-
C: Local model + binary threshold	.692.000	-	.639.000	-	-	-	-	-
D: Best local model	.528.000	.702.000	.553.000	.792.000	.370.000	.692.000	.324.000	.618.000
E: Local model + MLP (Eqn (4))	.768.009	.957.004	.738.012	.935.001	.391.003	.727.006	.342.008	.636.010
F: Local model + concatenation	.824.006	.971.001	.854.003	.979.002	.386.005	.693.064	.389.018	.698.010
G: Local model + icdf (no align.)	.798.009	.963.004	.755.009	.943.001	.387.003	.734.015	.380.006	.658.005
H: Local model + shared W + icdf	.817.009	.966.001	.747.009	.941.004	.387.006	.725.010	.368.012	.660.008
I: Local model + soft align. + icdf	.835.010	.975.001	.860.005	.980.002	.390.008	.734.008	.444.027	.693.011
J: Local model + hard align. + icdf	.839.006	.973.001	.855.008	.976.001	.390.004	.737.016	.404.016	.686.008
K: J + end-to-end ?	.825.012	.973.002	.823.006	.972.002	.382.007	.717.010	.392.020	.683.006
Table 3: Comparison of global models. ? Some references of rows are with respect to Table 2.
u--IπOz u--Iπc0s u--IπPJπH
	metr-la		pems-bay		pmu-b		pmu-c	
	f1	AUC	f1	AUC	f1	AUC	f1	AUC
No graph	.768.009	.957.004	.738.012	.935.001	.391.003	.727.006	.342.008	.636.010
Given graph	.763.020	.957.007	.742.024	.942.005	-	-	-	-
Gumbel	.785.008	.959.006	.751.008	.942.001	.387.001	.730.017	.381.025	.658.006
icdf (row G) ?	.798.009	.963.004	.755.009	.943.001	.387.003	.734.015	.380.006	.658.005
No graph	.833.010	.975.001	.846.008	.977.001	.388.001	.736.015	.386.008	.694.005
Given graph	.828.007	.974.001	.854.003	.977.001	-	-	-	-
Gumbel	.834.016	.975.001	.863.014	.980.001	.390.003	.733.012	.435.028	.693.007
icdf (row I) ?	.835.010	.975.001	.860.005	.980.002	.390.008	.734.008	.444.027	.693.011
No graph	.825.008	.971.003	.847.008	.976.001	.387.004	.736.007	.372.004	.674.015
Given graph	.829.014	.971.002	.848.010	.973.002	-	-	-	-
Gumbel	.837.013	.973.002	.850.009	.976.001	.391.004	.732.014	.410.016	.687.004
icdf (row J) ?	.839.006	.973.001	.855.008	.976.001	.390.004	.737.016	.404.016	.686.008
end training) performs worse than method J (separate training). The result is not surprising, because
joint training compromises the optimality of local data representations separately obtained by each
local model. We also note this method is outside the setting of federated inference and generally
cannot be used unless data owners agree to share data.
Comparison of global models. The other set of comparisons, as outlined in Table 3, extends each
alignment strategy (including no alignment) to the role of graphs in the global model: not using
a graph, using the given graph, and learning a graph (by using either the Gumbel trick or the icdf
method). The numbers in the table suggest that within each alignment strategy, graph structure
learning significantly improves the classification. The performance of the Gumbel trick and that of
icdf is highly comparable.
Quality of learned permutations. For hard alignment, we investigate the learning of the permu-
tation matrices. According to Theorem 1, σ22 of the limit of Kj dictates the convergence speed.
Since we do not know the limit, we compute σ22 of KT and summarize them in Table 4 for some
local model in each data set, under several involved methods. One sees that all values are close to 1,
suggesting that the convergence is indeed rather slow, agreeing with theory. Note that some values
are greater than 1 because KT is not strictly doubly stochastic (owing to slow convergence).
In Figure 4, we visualize KT for some local model in each data set. The plots clearly show patterns
of a permutation matrix: there is one and only one significant value per row and per column. Because
of the slow convergence, we attribute the desirable results of KT (at a small T) to the success of the
learning of K0 . Note also interestingly that a learned permutation may be the identity mapping.
8
Under review as a conference paper at ICLR 2022
Table 4: Examples of squared second singular value, σ22, of KT.
metr-la pems-bay pmu-b	pmu-c
ngila dra
No graph	1.007	1.000	1.000	1.000
Given graph	1.010	1.000	-	-
Gumbel	1.007	1.002	1.001	1.019
icdf	1.000	1.000	1.000	1.025
Ie)Cal model 1
local model 48
IOCal model 24__________ π C _____________local model 72
Figure 4: Examples of learned permutation matrices (KT ). All are from Method H.
Comparison of Gumbel softmax and icdf. Prior results suggest that these two approaches for
differentiably sampling the Bernoulli distribution perform equally well. An advantage of icdf is its
lower computational cost. To demonstrate this advantage, we design a mini-benchmark that high-
lights the sampling and gradient computation and minimizes the effect of irrelevant complications
(such as permutation and GNN). To this end, we generate samples (xi ∈ Rn, yi ∈ Rn) for some
A ∈ {0, 1}n×n, where yi = Axi + noise, and use the samples to learn A through differentiable
parameterization. Figure 5 shows the time and memory consumption at a fixed number of learning
epochs. As a sanity check, the running time scales nicely as O(n2) as expected (while the memory
consumption is complicated; it does not follow O(n2) because of memory management in Python).
Overall, one clearly sees the lower computational cost of the icdf method.
We also report the time and memory consumption for the experiments on the four data sets; see
Table 5 in the supplement. The results well agree that the icdf method is more economic.
(a) Time in seconds
(b) Memory in MB
Figure 5: Time and memory consumption as the matrix size (n, horizontal axis) increases.
7	Conclusions
In this paper, we study federated inference, a less addressed scenario of machine learning with
distributed data that require collective inference. This scenario is in contrast to federated learning,
where inference is local and requires no joint efforts. We motivate the practicality of federated
inference by using a power grid example and propose a local-global model framework for it. Two
important components of the framework are the alignment of the data representations produced by
local models and the learning of the global model by using a graph neural network. Comprehensive
experiments suggest the feasibility of federated inference and the effectiveness of the framework.
9
Under review as a conference paper at ICLR 2022
References
Jan Philipp Albrecht. How the GDPR will change the world. European Data Protection Law Review,
2016.
Peter W. Battaglia, Jessica B. Hamrick, Victor Bapst, Alvaro Sanchez-Gonzalez, Vinicius Zambaldi,
Mateusz Malinowski, Andrea Tacchetti, David Raposo, Adam Santoro, Ryan Faulkner, Caglar
Gulcehre, Francis Song, Andrew Ballard, Justin Gilmer, George Dahl, Ashish Vaswani, Kelsey
Allen, Charles Nash, Victoria Langston, Chris Dyer, Nicolas Heess, Daan Wierstra, Pushmeet
Kohli, Matt Botvinick, Oriol Vinyals, Yujia Li, and Razvan Pascanu. Relational inductive biases,
deep learning, and graph networks. Preprint arXiv:1806.01261, 2018.
Tianyi Chen, Xiao Jin, Yuejiao Sun, and Wotao Yin. VAFL: a method of vertical asynchronous
federated learning. In ICML Workshop, 2020.
Kyunghyun Cho, Bart van Merrienboer, Caglar Gulcehre, Dzmitry Bahdanau, Fethi Bougares, Hol-
ger Schwenk, and Yoshua Bengio. Learning phrase representations using RNN encoder-decoder
for statistical machine translation. In EMNLP, 2014.
Patrick Emami and Sanjay Ranka. Learning permutations with Sinkhorn policy gradient. Preprint
arXiv:1805.07010, 2018.
Luca Franceschi, Mathias Niepert, Massimiliano Pontil, and Xiao He. Learning discrete structures
for graph neural networks. In ICML, 2019.
Stephen Hardy, Wilko Henecka, Hamish Ivey-Law, Richard Nock, Giorgio Patrini, Guillaume
Smith, and Brian Thorne. Private federated learning on vertically partitioned data via entity
resolution and additively homomorphic encryption. Preprint arXiv:1711.10677, 2017.
Christina Heinze, Brian McWilliams, Nicolai Meinshausen, and Gabriel Krummenacher. LOCO:
Distributing ridge regression with random projections. Preprint arXiv:1406.3469, 2014.
Christina Heinze, Brian McWilliams, and Nicolai Meinshausen. DUAL-LOCO: Distributing statis-
tical estimation using random projections. In AISTATS, 2016.
Irina Higgins, David Amos, David Pfau, Sebastien Racaniere, Loic Matthey, Danilo Rezende,
and Alexander Lerchner. Towards a definition of disentangled representations. Preprint
arXiv:1812.02230, 2018.
SePP Hochreiter and Jurgen Schmidhuber. Long short-term memory. Neural Computation, 9(8):
1735-1780,1997.
Roger A. Horn and Charles R. Johnson. Matrix Analysis. Cambridge University Press, 2nd edition,
2012.
Yaochen Hu, Di Niu, Jianming Yang, and ShengPing Zhou. FDML: A collaborative machine learn-
ing framework for distributed features. In KDD, 2019.
H. V. Jagadish, Johannes Gehrke, Alexandros Labrinidis, Yannis PaPakonstantinou, Jignesh M. Pa-
tel, Raghu Ramakrishnan, and Cyrus Shahabi. Big data and its technical challenges. Commun.
ACM, 57(7):86-94, 2014.
Eric Jang, Shixiang Gu, and Ben Poole. Categorical reParameterization with Gumbel-softmax. In
ICLR, 2017.
Peter Kairouz, H. Brendan McMahan, Brendan Avent, AUrelien BelleL Mehdi Bennis, Arjun Nitin
Bhagoji, Keith Bonawitz, Zachary Charles, Graham Cormode, Rachel Cummings, Rafael G.L.
D,Oliveira, Salim El Rouayheb, David Evans, Josh Gardner, Zachary Garrett, Adria Gascon,
Badih Ghazi, PhilliP B. Gibbons, Marco Gruteser, Zaid Harchaoui, Chaoyang He, Lie He,
Zhouyuan Huo, Ben Hutchinson, Justin Hsu, Martin Jaggi, Tara Javidi, Gauri Joshi, Mikhail
Khodak, Jakub Konecny, Aleksandra Korolova, Farinaz Koushanfar, Sanmi Koyejo, Tancrede
Lepoint, Yang Liu, Prateek Mittal, Mehryar Mohri, Richard Nock, Ayfer Ozgur, Rasmus Pagh,
Mariana Raykova, Hang Qi, Daniel Ramage, Ramesh Raskar, Dawn Song, Weikang Song, Se-
bastian U. Stich, Ziteng Sun, Ananda Theertha Suresh, Florian Tramer, Praneeth Vepakomma,
10
Under review as a conference paper at ICLR 2022
Jianyu Wang, Li Xiong, Zheng Xu, Qiang Yang, Felix X. Yu, Han Yu, and Sen Zhao. Advances
and open problems in federated learning. Preprint arXiv:1912.04977, 2019.
Thomas Kipf, Ethan Fetaya, Kuan-Chieh Wang, Max Welling, and Richard Zemel. Neural relational
inference for interacting systems. In ICML, 2018.
Thomas N. Kipf and Max Welling. Semi-supervised classification with graph convolutional net-
works. In ICLR, 2017.
Philip A. Knight. The Sinkhorn-Knopp algorithm: Convergence and applications. SIAM Journal
on Matrix Analysis andApplications, 30(1):261-275, 2008.
Harold W. Kuhn. The Hungarian method for the assignment problem. Naval Research Logistics
Quarterly, 2(1-2):83-97, 1955.
Sebastien Lachapelle, Philippe Brouillard, Tristan Deleu, and Simon Lacoste-JUlien. Gradient-based
neural DAG learning. In ICLR, 2020.
Qinbin Li, Zeyi Wen, Zhaomin Wu, Sixu Hu, Naibo Wang, and Bingsheng He. A survey on
federated learning systems: Vision, hype and reality for data privacy and protection. Preprint
arXiv:1907.09693, 2019.
Yaguang Li, Rose Yu, Cyrus Shahabi, and Yan Liu. Diffusion convolutional recurrent neural net-
work: Data-driven traffic forecasting. In ICLR, 2018.
Chris J. Maddison, Andriy Mnih, and Yee Whye Teh. The concrete distribution: A continuous
relaxation of discrete random variables. In ICLR, 2017.
H. Brendan McMahan, Eider Moore, Daniel Ramage, Seth Hampson, and Blaise AgUera y Areas.
Communication-efficient learning of deep networks from decentralized data. In AISTATS, 2017.
Gonzalo Mena, David Belanger, Scott Linderman, and Jasper Snoek. Learning latent permutations
with Gumbel-Sinkhorn networks. Preprint arXiv:1802.08665, 2018.
Richard Nock, Stephen Hardy, Wilko Henecka, Hamish Ivey-Law, Giorgio Patrini, Guillaume
Smith, and Brian Thorne. Entity resolution and federated learning get a federated resolution.
Preprint arXiv:1803.04035, 2018.
Gabriel Peyre and Marco Cuturi. Computational optimal transport. Foundations and Trends in
Machine Learning, 11(5-6):355-607, 2019.
Chao Shang, Jie Chen, and Jianbo Bi. Discrete graph structure learning for forecasting multiple time
series. In ICLR, 2021.
Richard Sinkhorn. A relationship between arbitrary positive matrices and doubly stochastic matri-
ces. Annals of Mathematical Statististics, 35(2):876-879, 1964.
Richard Sinkhorn and Paul Knopp. Concerning nonnegative matrices and doubly stochastic matri-
ces. Pacific J. Math., 21(2):343-348, 1967.
Smartgrid.gov. Recovery act: Synchrophasor applications in transmission systems. Webpage
retrieved in Jan 2021. https://www.smartgrid.gov/recovery_act/program_
impacts/applications_synchrophasor_technology.html.
Hongyi Wang, Mikhail Yurochkin, Yuekai Sun, Dimitris Papailiopoulos, and Yasaman Khazaeni.
Federated learning with matched averaging. In ICLR, 2020.
Alan Geoffrey Wilson. The use of entropy maximising models, in the theory of trip distribution,
mode split and route split. Journal of Transport Economics and Policy, 3(1):108-126, 1969.
Zonghan Wu, Shirui Pan, Guodong Long, Jing Jiang, Xiaojun Chang, and Chengqi Zhang. Con-
necting the dots: Multivariate time series forecasting with graph neural networks. In KDD, 2020.
11
Under review as a conference paper at ICLR 2022
Zonghan Wu, Shirui Pan, Fengwen Chen, Guodong Long, Chengqi Zhang, and Philip S. Yu. A
comprehensive survey on graph neural networks. IEEE Transactions on Neural Networks and
Learning Systems, 32(1):4-24, 2021.
Qiang Yang, Yang Liu, Tianjian Chen, and Yongxin Tong. Federated machine learning: Concept
and applications. ACM Transactions on Intelligent Systems and Technology, 10(2), 2019.
Yue Yu, Jie Chen, Tian Gao, and Mo Yu. DAG-GNN: DAG structure learning with graph neural
networks. In ICML, 2019.
Mikhail Yurochkin, Mayank Agarwal, Soumya Ghosh, Kristjan Greenewald, and Trong Nghia
Hoang. Statistical model aggregation via parameter matching. In NeurIPS, 2019a.
Mikhail Yurochkin, Mayank Agarwal, Soumya Ghosh, Kristjan Greenewald, Trong Nghia Hoang,
and Yasaman Khazaeni. Bayesian nonparametric federated learning of neural networks. In ICML,
2019b.
Ziwei Zhang, Peng Cui, and Wenwu Zhu. Deep learning on graphs: A survey. Transactions on
Knowledge and Data Engineering, 2020.
Xun Zheng, Bryon Aragam, Pradeep Ravikumar, and Eric P. Xing. DAGs with NO TEARS: Con-
tinuous optimization for structure learning. In NeurIPS, 2018.
12
Under review as a conference paper at ICLR 2022
A Related Work
The concept of federated learning was first coined by McMahan et al. (2017) and it has attracted
surging interests since. A form of distributed optimization, federated learning is faced with data
challenges beyond conventional assumptions and puts communication efficiency and data privacy as
primary concerns. Recent surveys (Yang et al., 2019; Li et al., 2019; Kairouz et al., 2019) compre-
hensively study the subject, review systems and infrastructures, and suggest open problems.
The typical setting of federated learning is that data sets across owners share the same feature space
but differ in samples. Besides this horizontal partitioning of the data matrix, a vertical partitioning
was studied by Hardy et al. (2017); Nock et al. (2018); Heinze et al. (2014; 2016), wherein features
are split across owners instead. This setting bears resemblance to our federated inference scenario,
but a crucial distinction is that existing methods for vertical federated learning all perform joint
training. In the referenced work, to preserve privacy, encrypted data or randomly projected data
are communicated among data owners as well as a central coordinator. Such an approach incurs
demanding communication for many owners. Recently, Chen et al. (2020) study a different model,
whose parameters are distributed among owners as well as a central server. The part of the model
corresponding to an owner bears resemblance to our local models; but they are not local models
since they are not independently trained by using local data. Another work along a similar direction
is conducted by Hu et al. (2019), but the global model has no parameters; it is merely a sum of the
local outputs followed by activation (e.g., sigmoid for classification).
Our framework learns parameter matrices to align local representations. Such alignments similarly
appear in model fusion, where a number of models are fused together into a common model through
aligning model parameters (Yurochkin et al., 2019a). In the context of deep learning, if the neural
networks come from the same model family, their weights can be matched layerwise, even if the
numbers of weights are different (Yurochkin et al., 2019b; Wang et al., 2020). The referenced work
treats the problem as a bipartite graph matching, where the cost matrix is inferred from maximum a
posteriori estimation. Then, the Hungarian algorithm (Kuhn, 1955) is applied to find the matching.
In our work, instead we treat the permutation alignment as a differentiable parameterization with the
help of Sinkhom-KnoPP (Sinkhorn & Knopp, 1967; Mena et al., 2018; Emami & Ranka, 2018), so
that it can be learned end-to-end with other parameters of the global model.
Our framework also advocates learning a graph of data owners in the global model. Graph structure
learning appears under various contexts. One field of study is probabilistic graphical models and
casual inference, whereby a directed acyclic structure is learned. Gradient-based approaches in this
context include Zheng et al. (2018); Yu et al. (2019); Lachapelle et al. (2020). On the other hand, a
general graph may still be useful without resorting to causality. Recent approaches supporting GNN-
based modeling include Kipf et al. (2018); Franceschi et al. (2019); Wu et al. (2020); Shang et al.
(2021), wherein a graph structure is simultaneously learned together with the GNN parameters.
The Gumbel trick (Jang et al., 2017; Maddison et al., 2017) is frequently used for differentiable
parameterization, but in this paper we study a more economic alternative parameterization, icdf.
B	Permutation Ambiguity Example for GRU
In section 3, we discuss that one can arbitrarily permute the latent representations while keeping a
local model fixed. Here, we give another example—the GRU. Let x = {x1, x2, . . . , xT} be an input
sequence. The embedding function h = embedding(x) implemented as a GRU reads:
1:	function h = GRU({xt}t=1,...,T)
2:	h0 = 0
3:	for t = 1, . . . , T do
4:	zt = sigmoid(Wzxt + Uzht-1 + bz)
5:	rt = sigmoid(Wr xt + Ur ht-1 + br)
6:	nt = tanh(Wnxt + Un(rt ht-1) + bn)
7:	ht = (1 - zt) ht-1 + zt nt
8:	end for
9:	return h = hT
10:	end function
13
Under review as a conference paper at ICLR 2022
One can arbitrarily permute the elements of h through manipulating the GRU parameters properly.
To achieve h[p] = embedding(x; p),
•	the gate outputs and bias vectors (zt, rt, nt, ht, bz , br, bn) will need be permuted accordingly
(zt [p], rt [p], nt [p], ht [p], bz [p], br [p], bn [p]);
•	the weight matrices attached to the input (Wz, Wr, Wn) will need to have their rows (i.e., output
neurons) permuted (Wz [p, :], Wr[p, :], Wn[p, :]); and
•	the weight matrices attached to the hidden states (Uz , Ur, Un) will need to have both their rows
and columns permuted (Uz [p, p], Ur [p, p], Un [p, p]).
C Federated Inference Framework
The framework is summarized in Algorithm 1. Details are elaborated in Sections 4 and 5.
Algorithm 1 Federated Inference Framework
1:
2:
3:
4:
function TRAINING({(x1)j, (x2)j, . . . , (xn)j, yj}j =1,...,N)
Each data owner trains a local model fi with its local data part {(xi)j }j and label {yj}j
Each data owner sends all local data representations {(hi)j }j to the central server
Central server trains a global model yb = g(P1h1 , P2h2, . . . , P nhn) by using {(hi)j}j
gathered from data owners and labels {yj }j . Here,
The global model is (8), where the loss is taken over the distribution of Ab, whose entries
are sampled by using (10);
Each alignment matrix Pi is either an arbitrary parameter matrix (soft alignment), or a
doubly stochastic matrix KT computed by (7) using T steps (hard alignment).
5:	end function
6:	function INFERENCE(x1, . . . , xn)
7:	Each data owner evaluates its local model with xi to obtain hi and sends to the central server
8:	Central server evaluates the global model by taking all hi as input and produces prediction
9:	end function
D Further Details of Section 4 (Parameterization of
Permutation)
The permutation matrix P is a combinatorial object and is unsuitable for gradient-based training.
We relax P by a doubly stochastic matrix, which is nonnegative and whose row sums and column
sums are all 1. A benefit of such a relaxation is that a differentiable computational procedure exists
for doubly stochastic matrices. The procedure, called the Sinkhorn-KnoPP algorithm (Sinkhorn
& Knopp, 1967), was recently used by Mena et al. (2018) and Emami & Ranka (2018) in other
contexts. Informally sPeaking, starting from a nonnegative square matrix K0, rePeatedly normalize
its rows and columns by their sums until convergence. The limit is a doubly stochastic matrix.
In fact, Mena et al. (2018) and Emami & Ranka (2018) do not start from a free K0 but Parameterize
it as exp(M /τ), where M is a free Parameter matrix and τ > 0 is a hyPerParameter. Such a
Parameterization follows more closely Sinkhorn’s algorithm (Sinkhorn, 1964) for comPuting the
optimal transport (Peyre & Cuturi, 2019) under an entropic regularization (Wilson, 1969), where T
is suPPosed to anneal to zero. However, this Parameterization is not imPerative and is not the only
option. For simplicity of exposition, we treat K0 as a free parameter.
To understand the convergence of this method, we rewrite the algorithm equivalently as
Start with r0 = 1, forj = 0, 1, . . ., cj+1 = 1	(K0T rj) and rj+1 = 1	(K0cj+1).	(15)
Here, the cj’s and rj’s correspond to the column sums and row sums of the iterating matrix in the
above informal description. Let diag(x) be a diagonal matrix whose diagonal is x. The following
result is proved by Sinkhorn & Knopp (1967).
14
Under review as a conference paper at ICLR 2022
Theorem 4 (Sinkhorn-Knopp). If Ko is nonnegative, then a necessary and sufficient condition that
the iteration (15) converges is that Ko has total support. Let positive vectors c* and r* be the limits
of {cj} and {r7-}, respectively. The matrix diag(r*)K° diag(c*) is doubly stochastic.
This result, however, does not give the convergence speed. In order to obtain a tight one, we fol-
low Knight (2008) and perform a minor modification of (15):
Start with ro = co = 1, forj = 0, 1, . . ., cj+1 = 1	(KoTrj) and rj+1 = 1	(Kocj).	(7)
This modification changes nothing essential of the algorithm, since it is equivalent to applying (15)
on KoT Ko0 . We define Kj = diag(rj)Ko diag(cj) and derive the following result. Its proof is
given in Section D.1.
Theorem 5. If Ko is nonnegative and irreducible, then the iteration (7) will converge linearly.
Let positive vectors c* and r* be the limits of {cj} and {r7-}, respectively. Then, K* =
diag(r*)Ko diag(c*) is the limit of Kj and is doubly stochastic. Furthermore, there exist a pos-
itive constant C and a positive integer J such that for all j ≥ J,
KKjTj11
≤ C(1 + σ22)σ22(j-J)TJ with Tj :=	crj
(16)
1
1
where σ2 < 1 is the second largest singular value ofK*.
A few important points of Theorem 5 are noted. First, the convergence (16) says that the row sums
and column sums of Kj approach 1 at a speed O(σ22j). Second, the iteration (7) preserves the
irreducibility of the Kj’s and thus of the limit K*. Then, by the Perron-Frobenius Theorem (Horn
& Johnson, 2012), the doubly stochastic matrix K* has a simple eigenvalue 1, which coincides with
the spectral radius as well as the dominant singular value. Thus, the second singular value, σ2, must
be < 1. The farther away it is from 1, the faster the iteration converges.
Unfortunately, Theorem 5 is not applicable to a permutation matrix, because such a matrix is not
irreducible. However, the theorem is practically useful under perturbation theory. After all, the limit
K* is hardly an exact permutation matrix; at best, it is -close. Note that all singular values of a
permutation matrix P is 1. If K* is different from P by in the spectral norm, then according to
Weyl’s Theorem (Horn & Johnson, 2012), σ2 ≥ 1 - .
The above analysis indicates that the Sinkhorn-Knopp algorithm (7) will be extremely slow to con-
verge if is small. A natural question is, is this parameterization useful? We argue positively. In
practice, we run the procedure (7) with only a few steps, T . The iterate KT can still be reasonably
close to a permutation matrix, when the starting point Ko is sufficiently good. In essence, we rely on
the training of the global model to find a good starting point and on the procedure (7) to normalize
its values to [0, 1]. One may further use an entropy regularization
entropy(KT [:, `]) + entropy(KT [`, :])
`
to push the values toward the two extremes, 0 or 1.
D.1 Proof of Theorem 5
Applying the iteration formula (7), it is straightforward to see that
KjT 1 - 1 = (cj	cj+1) - 1 and Kj1 - 1 = (rj	rj+1) - 1.
We expand the left equation to
(cj cj+1 ) - 1
(cj - c* + c* - cj+1) cj+1
and similarly for the right equation. Then
KjT 1 - 1
Kj 1 - 1
+
Applying the triangle inequality and the property kx yk ≤ kxk/ min |yi |, we obtain
KjT 1 - 1
Kj 1 - 1
/τj+1
(17)
≤
15
Under review as a conference paper at ICLR 2022
where τj+1 denotes the absolute minimum of the elements of cj+1 and rj+1.
We will need the following result from Knight (2008).
Theorem 6. If K0 is nonnegative and irreducible, then there exists a positive integer J such that
for all j ≥ J,
rcjj++11
c*
r*
Theorem 6 states that Tj+1 ≤ σ22Tj for all j ≥ J. Because σ2 < 1, Tj is monotonically decreasing
once j ≥ J. Moreover, because cj and rj are positive vectors for all j, when j ≥ J, the elements
of cj and rj are bounded away from zero. Let τ > 0 be the infimum of the elements of all cj and rj
when j > J; (17) becomes
KT1 -1]∣ ≤ ….
Thus, applying the relation Tj+1 ≤ σ22Tj, we obtain
∣[KT 1 -1]∣∣ ≤ — ≤	,
which concludes the proof by taking C = 1∕τ.
E Proofs and Additional Results of Section 5 (B ernoulli
S ampling)
E.1 Distribution of Gumbel Softmax
As preliminary, we consider the first entry y1 of the random variable y defined in (9) for the Gumbel
softmax parameterization. Note that for any τ 6= 0, y1 is only approximately binary; the possible
values of y1 in fact span the entire interval [0, 1]. We derive the following cdf for y1. Recall that for
notational simplicity, θ denotes a scalar rather than a matrix.
Theorem 7. For all τ > 0, θ ∈ (0, 1), and t ∈ [0, 1], we have
PMyI ≤t) = F(I-⅛+7⅛ψθ.	(18)
Proof. We first consider the case 0 < t < 1. Through simple algebraic manipulation, we obtain that
y1 ≤ t is equivalent to
tθ
gl - g2 ≤ T log T1-T - log Tl-Z-	(19)
1-t 1-θ
Let gι = - log(- logU) and g2 = - log(- log v), where U and V are independent and 〜U(0,1).
Then, (19) is equivalent to
v≥UM
where
tτ (1 - θ)
(1 - t)τθ-
Therefore, by recalling that U and v are uniform in [0, 1]2, we note that the probability that v ≤ UM
happens is the double integral
Pr(v ≥ UM)
1 dvdU.
This integral is nothing but
1 - Z UM dU
0
M
M
1 + M
which completes the proof of (18).
The cases of t = 0 or 1 obviously hold by continuity.
16
Under review as a conference paper at ICLR 2022
E.2 Proof of Theorem 2
We first consider the case when the distribution with cdf F is finitely supported on [a, b]. Through
simple algebraic manipulation, we obtain that z ≤ t is equivalent to s ≥ M where M := F-1 (θ) +
Tlog(tT - 1). If t < Sigmoid((F-1(θ) - b)∕τ), We see that M > b and thus such S can never
occur. Similarly, if t > Sigmoid((F-1(θ) - a)∕τ), we see that M < a, which indicates that S ≥ M
alWays happens. OtherWise, When t is Within the tWo extremes, the probability that s ≥ M happens
is 1 - F(M), concluding the proof of (11).
The statement of the theorem regarding the case when the distribution is not finitely supported is
obviously true.
To show that the distribution of z converges to Ber(θ), let us first consider the scenario when
the distribution with cdf F is finitely supported. The cdf of z (see (11)) is always continu-
ous but it has three segments connected by two joints: t1 = Sigmoid((F -1(θ) - b)∕τ) and
t2 = Sigmoid((F -1(θ) - a)∕τ). When τ → 0, the joint t1 → 0 and the joint t2 → 1 and
thus the middle segment has a wider and wider support converging to [0, 1]. Hence, it suffices to
consider only the middle segment. Further, with an analogous argument for other scenarios, it is
also true that it suffices to consider only the third case of (11).
In this case, for any fixed t < 1 and when τ → 0, we have τ log(t-1 - 1) → 0 and thus Pr(z ≤
t) → 1 - F (F -1(θ)) = 1 - θ. Meanwhile, we cannot push τ → 1 because then the limit of
τ log(t-1 - 1) is undefined. However, we know by definition that Pr(z ≤ 1) = 1. Hence, the
continuous distribution of z converges to a degenerate distribution Pr(z < 1) = 1 - θ and Pr(z =
1) = 1. This is the cdf of Ber(θ).
E.3 Proof of Theorem 3
By the definition of bias, we have
BiaS(x) = E[x] - θ where E[x] = Z t d Pr(x ≤ t) = 1 - Z Pr(x ≤ t) dt.
00
Therefore, for Gumbel softmax,
Bias(y1) = 1 - θ -Z0 tτ (1 -tθ)l+(θ)- t)τθ dt，
and for icdf with any F,
BiaS(z) = Z F(F-1(θ)+τ log(t-1 - 1)) dt - θ.
0
We now prove Theorem 3 in a few parts.
Proof of (13). Let S = F -1(θ) and perform a change of variable m = log(t-1
- 1). Then,
BiaS(z) = Z [F (S+τm) - F (S)] dt = Z	[F(S+τm) - F (S)]
0	-∞
em
(1 + em)2
dm.
We perform Taylor expansion of F around S and obtain
∞ F(n) (S)
F(S + Tm) - F(S) = V ——=τnmn.
n!
n=1
Therefore,
∞
BiaS(z) = X
n=1
F(n) (S)
n!
Z∞
∞
mnem
(1 + em)2
dm
Each integral term is finite and the odd terms vanish because the integrands are odd functions. Thus,
for small T, we are left with
BiaS(Z)=勺τ2 厂
2	-∞
2m
(Γ+可 dm + o(T ).
17
Under review as a conference paper at ICLR 2022
The definite integral evaluates to π2; We therefore conclude the proof.
Proof of (14). Equation (14) is straightforward by substuting
F00(s)
s
----/ e
σ3 √2π
s2
2σ2
erf 1(2θ - I) -(erf-1(2θ-1))2
-σ2√π-
into (13).
Proof of (12). To simplify notation, let β = θ/(1 - θ) and perform a change of variable m
log(t-1 - 1). Then,
I- 1「:(1-θ) … dt= I-1	=厂1 J dm
Jo	tτ(1 -	θ) + (1 -	t)τθ	Jo	1 +	βemτ	J-∞ 1 +	βemτ(1 + em)
Denote h(τ,m) = [1 + βemτ]-1. Treating h a function of τ and performing Taylor expansion
around zero, We obtain
h(" X T 产.
n=o
Therefore,
/1 J=J dt = X T /∞ hi
o t (1 - ) + (1 - t)	n=o n -∞
em
(1 + em)2
d.
In a moment, We Will shoW that for all n,
h(n)(0,m) = Cnmn Where Cnis independent ofm.	(20)
Suppose that (20) holds. Then, each integral term is finite and the odd terms vanish, because the
integrands are odd functions. Therefore, for small τ, We are left With
:」(I-θθ — dt = Co 广1 J dm + C2匕广,m2∖ dm + O(τ，).
Jo tτ(1-θ) + (1-t)τ θ	0 J-∞ (1 + em)2	+ 2 2 J-∞ (1 + em)2	+ ( )
By calculating
Co = h(0, m) = [1 +β]-1 = 1 - θ,	C2 = h00(0,m) = -θ(1 - θ)(1 - 2θ),
「-2 dm =1,厂，M dm = ^,
J-∞ (1 + em)2	J-∞ (1 + em)2	3
We conclude that
Bias(yι)= T2*"(1 =…冽 + O(τ4).
6
It remains to prove (20). We suppress the argument onm	and Write g(τ) = 1 + βemτ and h(τ) =
g(τ)-1. By Faa di Bruno's formula,
h(n)(0)=(六『 J X g-1k+! ∙ Bn,k (g0(0),g0θ(0),...,g …)(0)),
Where Bn,k is the Bell polynomial. Clearly, g(0) = 1 + β and g(r) (0) = βmr for all r > 0. Hence,
Bn,k is a multiple ofmn. Therefore, h(n)(0) is a multiple ofmn.
E.4 Additional Result Regarding the Bias
Theorem 3 states results for a small temperature τ . The purpose is to understand the limiting be-
havior of the bias. Here, We give an additional result for any τ > 0. It states that the biases of the
tWo sampling approaches have the same sign. This result is a nontrivial extension of Theorem 3 and
requires a different proof technique.
18
Under review as a conference paper at ICLR 2022
Theorem 8. For any τ > 0,
Bias(yι) > 0 when θ < 11,	Bias(yι) = 0 when θ = 11, Bias(yι) < 0 when θ > 2.	(21)
Moreover, if F0(x) (that is, the pdf) is even and is increasing when x < 0, then
Bias(Z) > 0 when θ < 1,	Bias(Z) = 0 when θ = 1, Bias(Z) < 0 when θ > 1.	(22)
We prove Theorem 8 in two parts.
Proof of (21). Consider
Bias(y1) = Z g(t, θ) dt where g(t, θ) = 1 - θ -
0
tτ(1 - θ)
tτ(i-θy÷7i-t)τθ
With a bruteforce calculation, we have
g(t, θ)+g(1-t, θ)
[(1 - t)τ - tτ]2θ(1 - θ)(1 - 2θ)
^itτ(ι-θy+7i-^FWi-^F(ι-M+τθ1
All terms on the right-hand side are positive, except 1 - 2θ. Therefore, when θ < 11, g(t, θ) + g(1 -
t, θ) > 0 and hence
Bias(y1)=「自也"贝-力⑼dt> 0.
02
The other cases (θ > 11 and θ = ɪ) are similarly proved.
Proof of (22). Consider
Bias(Z)
Z1h(t, θ)
0
dt - θ where h(t, θ)
F(F-1(θ) + τ log(t-1 - 1)).
We have
h(1 -t, θ) =F(F-1(θ) - τ log(t-1 - 1)).
To simplify notation, let F -1(θ) = s and τ log(t-1 - 1) = a. Then, h(t, θ) = F(s + a) and
h(1 - t, θ) = F(s - a). Let us first consider the case s < 0 and a > 0. We see that
F(s + a) - F(s)
s+a
F0(m) dm
s
and
F(s) -F(s-a)
Zs
s-a
F0(m) dm.
For any b > 0, if s + b < 0, then by monotonicity, F0(s + b) > F0(s - b). On the other hand, if
s + b ≥ 0, then F0(s + b) = F0(-s - b) > F0(s - b). In both cases, the right integral is always
smaller than the left integral. In other words,
F(s + a) + F(s - a) > 2F (s).
In fact, the above inequality is also established when s < 0 and a < 0. Therefore, whenever s < 0,
[1 h(t,θ) dt= [ 1 h…h(I- t,θ) dt> 广 F (F-1(θ)) dt = θ.
0	02	0
In other words, Bias(Z) > 0. The other cases (s = F -1(θ) > 0 and s = F-1(θ) = 0) are similarly
proved.
F TUNING GUIDANCE FOR TEMPERATURE τ
Theorem 3 suggests that the icdf method converges equally fast as does the Gumbel trick (both on
the order of O(τ1 )). On the other hand, the biases depend on θ. Thus, one cannot set temperatures
τ , independently of the desired probability θ, to equate the two biases. In practice, τ is a tunable
hyperparameter and a guidance on the tuning range is called for.
To this end, We use a subscript to distinguish the two temperatures—Tg for the Gumbel trick and Ti
for the icdf method—and write, based on (12) and (14) and ignoring the high order terms,
Bias(y1)
Bias(Z)
τ2σ2
Ti2
r(θ) where r(θ)
√∏θ(1 - θ)(2θ - 1)
erf-1(2θ - 1)e-(erf-1(2θ-1))2

19
Under review as a conference paper at ICLR 2022
Note that r(θ) is symmetric around θ = 1, is concave, attains maximum 2 when θ = 2, and
attains minimum 0 when θ = 0,1. Hence, if Tg = Ti and σ = √2, the bias of the GUmbel trick
is (approximately) smaller than that of the icdf method. On the other hand, for a σ > √2, there
2
exist θ1 < θ2 such that σ-2 = r(θ1) = r(θ2) and that Bias(y1) ' Bias(z), whenever θ ∈ [θ1, θ2].
For example, when σ ≈ 2.5, on the interval θ ∈ [0.01, 0.99], the bias of the Gumbel trick is
(approximately) greater than that of the icdf method.
Based on the foregoing, a practical guide is to use the same tuning range of T for the icdf method
as for the Gumbel trick. A small change of σ (e.g., √2 versus 2.5) will entirely flip the landscape
of the bias comparison between the two methods. Because the tuning range is much wider than the
change of σ, for simplicity it suffices to fix σ = 1.
G Data Set Description and Preprocessing
metr-la and pems-bay. These are traffic data sets (MIT licensed) used by Li et al. (2018).
The former was collected from loop detectors in the highway of Los Angles, CA (Jagadish et al.,
2014) and the latter was collected by the California Transportation Agencies Performance Measure
System. Both data sets recorded several months of data at the resolution of five minutes. The
network graphs are available, which were constructed by imposing a radial basis function on the
pairwise distance of sensors at a certain cutoff.
The data sets were originally prepared for forecasting tasks and hence no labeling information exists.
We adapt the data for classification. Specifically, we split the time series on the hour, forming hourly
windows. We label each window as whether or not it corresponds to rush hour. For proof of concept,
We specify 07:00-10:00 and 16:00-19:00 as rush hour and the others non-rush hour. We note that
in the original data sets, one of the attributes is time. We remove this attribute to avoid triviality and
retain only the speed attribute.
The specification of rush hours may not be highly accurate, but it is a sensible practice to cope with
the nonexistence of labeling information. Intuitively, the signal of rush hour comes from reduced
traffic speed, but not every location of the network experiences traffic jam. Hence, the diverse traffic
patterns inside the same time window under a single label causes nontrivial challenges for local
models to discern. Therefore, the need of a global consensus model is justified and it fits well the
federated inference scenario.
pmu-b and pmu-c. These are proprietary data sets coordinately provided by multiple data owners
of the U.S. power grid. No personally identifiable information is present. The suffixes B and C
indicate the interconnects of the grid. The data sets come with thousands of annotated grid events
spanning a period of two years; they form the classification labels. Many variables (attributes) of
the grid condition are recorded; we select only the voltage magnitude and the current magnitude,
because they appear to be the strongest signals for event detection based on domain knowledge, and
also because more data are available for these two variables. The grid topology is not available.
For each event, we select a one-second window from the three-minute window that covers the ap-
proximate annotated event time, based on the largest z-score. We retain a sampling frequency of
30Hz, even though some data are 60Hz. Furthermore, a large amount of data are missing in the
raw data. We impute the series by using pandas.DataFrame.interpolate(method =
’linear'， limit .direction = ’both')from the Python pandas package. This way,
a windowed series is complete ifit ever has raw data. Even so, many series are entirely empty, which
corresponds to the scenario illustrated by Figure 1.
Classes in these two data sets are rather skewed. For pmu-b, we remove a class that consists of
only one data point and for pmu-c, we combine classes that contain fewer than 24 data points into
a single class.
H	Experiment Details
The experiments are conducted on one x86 node of a computing cluster with one K40 Nvidia GPU.
The compute node has eight Intel cores and 128GB memory.
20
Under review as a conference paper at ICLR 2022
For each data set, we perform a 70/10/20 random split for training, validation, and testing, respec-
tively.
For local models, we use LSTM with the same hyperparameters: one hidden layer, hidden dimension
= 16, and maximum number of epochs = 200. We pretrain the local models and freeze their
parameters afterward.
We train each global model for a maximum of 500 epochs and use early stopping according to the
validation loss, with a patience of 50 epochs.
For the GNN global model, we use a 2-layer GCN with skip connections. The hidden dimension is
set at 8 and we select the learning rate from {0.01, 0.001}. For missing data, we impute the node
features by using zero.
For the learning of the permutation matrix, we parameterize K0 as M2 , where the squaring is el-
ementwise. On data sets METR-LA, PEMS-BAY, and PMU-C, M is initialized using xavier uni-
form, and for pmu-b it is initialized by an identity matrix with small Gaussian noise. The num-
ber of Sinkhorn-KnoPP steps is T = 10 and the entropy-regularization strength is selected from
{0, 0.01, 0.1}.
For the learning of the consensus graph, the temperature τ in the Gumbel-softmax method is tuned
from {0.1, 0.01}. The parameter τ in the icdf method is tuned from the same range, based on the
analysis after Theorem 3.
I Further Experiment Results
Extending the last experiment in Section 6, Table 5 summarizes the time and memory consumption
during the training of global models on the four data sets. The results indicate that the icdf method
is more economic than the Gumbel-softmax method.
Table 5: Time and memory consumption in global model training (five epochs). Time is in seconds
and memory is in MB.
	metr-la		pems -bay		pmu-b		pmu-c	
	Time	Memory	Time	Memory	Time	Memory	Time	Memory
Gumbel trick	87.89	832.38	270.52	1896.11	42.40	348.39	84.89	1119.13
icdf method	79.69	568.24	157.93	1167.19	30.16	322.59	54.07	894.63
21