Under review as a conference paper at ICLR 2022
First-Order Optimization Inspired from Finite-
Time Convergent Flows
Anonymous authors
Paper under double-blind review
Ab stract
In this paper, we investigate the performance of two first-order optimization al-
gorithms, obtained from forward Euler discretization of finite-time optimization
flows. These flows are the rescaled-gradient flow (RGF) and the signed-gradient
flow (SGF), and consist of non-Lipscthiz or discontinuous dynamical systems that
converge locally in finite time to the minima of gradient-dominated functions. We
propose an Euler discretization for these first-order finite-time flows, and provide
convergence guarantees, in the deterministic and the stochastic setting. We then ap-
ply the proposed algorithms to academic examples, as well as deep neural networks
training, where we empirically test their performances on the SVHN dataset. Our
results show that our schemes demonstrate faster convergences against standard
optimization alternatives.
1	Introduction
Consider the unconstrained minimization problem for a given cost function f : Rn → R. When f is
sufficiently regular, the standard algorithm in continuous time (dynamical system) is given by
X = FGF(X) , -Vf(x)	(1)
with X ， ddtx(t), known as the gradient flow (GF). Generalizing GF, the q-rescaled GF (q-
RGF) Wibisono et al. (2016) given by
X = Fq-RGF(X) = -C—"(X I-2	(2)
kVf (χ)k2-1
with c > 0 and q ∈ (1, ∞] has an asymptotic convergence rate f (X(t)) 一 f (x?) = O (寻T) under
mild regularity, for kX(0) - X?k > 0 small enough, where X? ∈ Rn denotes a local minimizer of f.
However, it has been recently proven Romero & Benosman (2020) that q-RGF, as well as q-signed
GF (q-SGF)
1
X = Fq-SGF(x) = -C kVf(X)kq-1 Sign(Vf (x)),	⑶
where sign(∙) denotes the sign function (element-wise), are both finite-time convergent (in continuous-
time), provided that f is gradient dominated of order p ∈ (1, q). In particular, if f is strongly convex,
then q-RGF and q-SGF is finite-time convergent for any q ∈ (2, ∞], since f must be gradient
dominated of order p = 2. Considering that many algorithms are inspired from continuous flows
with convergence guarantee, e.g., Muehlebach & Jordan (2019); Fazlyab et al. (2017a); Shi et al.
(2018); Zhang et al. (2018); FranCa et al. (2019b); Wibisono et al. (2016), a natural question arises:
what is the convergence rate of the corresponding discrete-time algorithms, which are induced by
discretization of finite-time convergent continuous-time flows?
1.1	Contribution
In this paper, we investigate the convergence behavior of an Euler discretization for the q-RGF (equa-
tion 2) and q-SGF (equation 3). We provide convergence guarantees in terms of closeness of solutions,
using results from hybrid dynamical control theory. Furthermore, we provide iteration / sample
complexity upper bounds for both the general (deterministic) and stochastic settings. We then test the
performance of the proposed algorithms on both synthetic and real-world data in the context of deep
learning, namely, on the well-known SVHN dataset.
1
Under review as a conference paper at ICLR 2022
1.2	Related Work
Propelled by the work of Wang & Elia (2011) and Su et al. (2014), there has been a recent and
significant research effort dedicated to analyzing optimization algorithms from the perspective of
dynamical systems and control theory, especially in continuous time (Wibisono et al., 2016; Wilson,
2018; Lessard et al., 2016; Fazlyab et al., 2017b; Scieur et al., 2017; Franca et al., 2018; Fazlyab et al.,
2018; Fazlyab et al., 2018; Taylor et al., 2018; Franca et al., 2019a; OrVieto & Lucchi, 2019; Romero
et al., 2019; Muehlebach & Jordan, 2019). A major focus within this initiative is in accceleration,
both in terms of trying to gain new insight into more traditional optimization algorithms from this
perspectiVe, or eVen to exploit the interplay between continuous-time systems and their potential
discretizations for noVel algorithm design (Muehlebach & Jordan, 2019; Fazlyab et al., 2017a;
Shi et al., 2018; Zhang et al., 2018; FranCa et al., 2019b; Wilson et al., 2019). Many of these
papers also focus on deriVing conVergence rates based on the discretization of flows designed in the
continuous-time domain.
Connecting ordinary differential equations (ODEs) and their numerical analysis, with optimization
algorithms is a Very important topic, which can be dated back to 1970s, see Botsaris (1978a;b);
Zghier (1981); Snyman (1982; 1983); Brockett (1988); Brown (1989). In Helmke & Moore (1994),
the authors studied relationships between linear programming, ODEs, and general matrix theory.
Further, Schropp (1995) and Schropp & Singer (2000) explored seVeral aspects linking nonlinear
dynamical systems to gradient-based optimization, including nonlinear constraints.
Tools from LyapunoV stability theory are often employed for the analysis, mainly because there
already exists a rich body of works within the nonlinear systems and control theory community for
this purpose. In particular, typically in preVious works, one seeks asymptotically LyapunoV stable
gradient-based systems with an equilibrium (stationary point) at an isolated extremum of the giVen
cost function, thus certifying local conVergence. Naturally, the global asymptotic stability leads
to global conVergence, though such analysis will typically require the cost function to be strongly
conVex eVerywhere.
For physical systems, a LyapunoV function can often be constructed from first principles Via some
physically meaningful measure of energy (e.g., total energy = potential energy + kinetic energy). In
optimization, the situation is somewhat similar in the sense that a suitable LyapunoV function may
often be constructed by taking simple surrogates of the objectiVe function as candidates. For instance,
V (x) , f(x) - f(x?) can be a good initial candidate. Further, if f is continuously differentiable
and x? is an isolated stationary point, then another alternative is V(x)，∣∣Vf (x) ∣∣2.
HoweVer, most fundamental and applied research conducted in systems and control regarding Lya-
punov stability theory deals exclusively with continuous-time systems. Unfortunately, (dynamical)
stability properties are generally not preserved for simple forward-Euler and sample-and-hold dis-
cretizations and control laws (Stuart & Humphries, 1998). Furthermore, practical implementations
of optimization algorithms in modern digital computers demand discrete-time. Nonetheless, it has
been extensively noted that a vast amount of general Lyapunov-based results (perhaps most) appear
to have a discrete-time equivalent.
2	Preliminaries
2.1	Optimization Algorithms as Discrete-Time Systems
Generalizing (1), (2), and (3), consider a continuous-time algorithm (dynamical system) modeled via
an ordinary differential equation (ODE)
X = F(X)	(4)
for t ≥ 0, or, more generally, a differential inclusion
X(t) ∈ F(x(t))	(5)
a.e. t ≥ 0 (e.g. for the q = ∞ case), such that x(t) → x? as t → t?. In the case of the q-RGF (2)
and q-SGF (3) for f gradient dominated of order p ∈ (1, q), we have finite-time convergence, and
thus t? = t?(x(0)) < ∞.
2
Under review as a conference paper at ICLR 2022
Most of the popular numerical optimization schemes can be written in a state-space form (i.e.,
recursively), as
Xk+1 = Fd(k, Xk)	(6a)
xk = G(Xk)	(6b)
for k ∈ Z+ , {0, 1, 2, . . .} and a given X0 ∈ Rm (typically m ≥ n), where Fd : Z+ × Rm → Rm
and G : Rm → Rn .
Naturally, (6) can be seen as a discrete-time dynamical system constructed by discretizing (4) in time.
In particular, we have xk ≈ x(tk), where {0 = t0 < t1 < t2 < . . .} denotes a time partition and
χ(∙) a solution to (4) or (5) as appropriate. Therefore, We call Xk and Xk ,respectively, the state and
output at time step k.
Example 1.	The standard gradient descent (GD) algorithm
χk+ι = Xk - ηVf (xk)	⑺
With step size (learning rate) η > 0 can be readily Written in the form (6) by taking m = n,
Fd(X) , X - ηVf (X), and G(X) , X.
•	If the step sizes are adaptive, i.e. if We replace η by a sequence {ηk} With ηk > 0, then We only
need to replace Fd(k, X) , X - ηk Vf (X), provided that {ηk} is not computed using feedback
from {Xk} (e.g. through a line search method).
•	If We do Wish to use time-varying step-size, then We can set m = n + 1, G([X; η]) , X,
and Fd([X; η]) , [Fd(1) ([X; η]); Fd(2) ([X; η])], Where Fd(1) ([X; η]) , X - η Vf (X), and Fd(2) is
a user-defined function that dictates the updates in the step size. In particular, an open-loop
adaptive step size {ηk} may be achieved under this scenario, provided that it is possible to Write
ηk+1 = Fd(2) (ηk).
•	If We Wish to use individual step sizes for each the n components of {Xk}, then it suffices to
take ηk as an n-dimensional vector (thus m = 2n), and make appropriate changes in Fd and G.
In each of these cases, GD can be seen as a forWard-Euler discretization of the GF (1), i.e.,
Xk+1 = Xk + ∆tk FGF (Xk)	(8)
With FGF = -Vf and adaptive time step ∆tk , tk+1 - tk chosen as ∆tk = ηk.
Example 2.	The proximal point algorithm (PPA)
Xk+1 = argmin f f (x) + ɪ ∣∣x - Xkk2 ∖	(9)
x∈Rn	2ηk
With step size ηk > 0 (open loop, for simplicity) can also be Written in the form (6), by taking m = n,
Fd(k, x)，argminχo∈Rn{f (χ0) + + ∣∣χ0 - χ∣∣2}, and G(X)，x. Naturally, we need to assume
sufficient regularity for Fd(k, X) to exist and We must design a consistent Way to choose Fd(k, X)
when multiple minimizers exist in the definition of Fd (k, X). Alternatively, these conditions must
be satisfied, at the very least, at every (k, X) ∈ {(0, X0), (1, X1), (2, X2), . . .} for a particular chosen
initial X0 ∈ Rn .
By assuming sufficient regularity, we have Vχ{f (x) + + ||x - Xk ∣∣2}∣x=xk+ι = 0, and thus
Vf(Xk +1) +	(xk + 1 - xk) = 0 < ⇒ xk+1 = xk + δ%FGF(xk +1)	(10)
ηk
with ∆tk = ηk, which is precisely the backward-Euler discretization of the GF (1).
2.2 CONTINUOUS-TIME CONVERGENCE OF q-RGF AND q-SGF
In this section, we review the necessary conditions to ensure finite-time convergence of these flows.
Here the hyperparameter c > 0 in equation 2 and equation 3 will not be explicitly denoted in
Fq-RGF, Fq-SGF. Next, borrowing terminologies from Wilson et al. (2019), we define the notion of
gradient dominance with order p as following.
3
Under review as a conference paper at ICLR 2022
Assumption 1 (Gradient Dominance of Order p). For a continuously differentiable function f, we
assume the function f is μ-gradient dominated oforder P ∈ (1, ∞] (μ > 0), i.e.,
p- 1
P
kVf(x)k2-1 ≥ μP-1 (f(x) - f(x?)),	∀X ∈ Rn,
(11)
P
where x? ∈ arg minx∈Rn f(x) is the local minimizer, also we denote the optimal value f? , f(x?).
Remark 1. It can be proved that continuously differentiable strongly convex functions are gradient
dominated of order P = 2. In fact, gradient dominance is usually defined exclusively for order
p = 2, often referred to as the Polyak-LojasieWicz (PL) inequality, which was introduced by Polyak
(1963) to relax the (strong) convexity assumption commonly used to show convergence of the GD
algorithm (7). The PL inequality can also be used to relax convexity assumptions of similar gradient
and proximal-gradient methods (Karimi et al., 2016; Attouch & Bolte, 2009). Furthermore, if f is
gradient dominated (of any order) w.r.t. x?, then x? is an isolated stationary point of f.
Our adopted generalized notion of gradient dominance is strongly tied to the Lojasiewicz gradient
inequality from real analytic geometry, established by Lojasiewicz (1963; 1965)1 independently
and simultaneously from Polyak (1963), and generalizing the PL inequality. More precisely, this
inequality is typically written as: for some C > 0 and θ ∈ (2, 1],
kVf(x)k2 ≥ C∙∣f(x)- f*∣θ	(12)
holds for every x ∈ Rn in a small enough open neighborhood of the stationary point x = x? . This
inequality is guaranteed for analytic functions Lojasiewicz (1965). More precisely, when x? is a local
minimizer of f, the aforementioned relationship is explicitly given by
(七)
C
p-1
P 1
μ p
θ = P-1
P
(13)
Therefore, analytic functions are always gradient dominated. However, while analytic functions are
always smooth, smoothness is not required to attain gradient dominance. Also recently it is shown
that in reinforcement learning, the value functions with softmax parameterization satisfies the above
condition (Mei et al., 2020; 2021), which further rationalizes our settings.
The following results in Romero & Benosman (2020) summarized the finite-time convergence of
q-RGF (equation 2) and q-SGF (equation 3) in continuous-time sense, which also motivates the main
topic in this paper.
Theorem 1 (Romero & Benosman (2020)). Suppose that f : Rn → R is continuously differentiable
and μ-gradient dominated of order P ∈ (1, ∞) near a strict local minimizer x? ∈ Rn. Let c > 0 and
q ∈ (p, ∞]. Then, any maximal solution x(∙), in the sense OfFilippov, to the q-RGF (2) or q-SGF (3)
will converge in finite time to x?, provided that kx(0) - x?k2 > 0 is sufficiently small. More precisely,
lim x(t) = x?, where the convergence time t? < ∞ may depend on which flow is used, but in both
cases is upper bounded by
t? ≤
1__1_
kVf(x0)kθ F
CC1 (1- θ),
(14)
p— 1
where xo = x(0), C = (p⅛) P μp, θ = p-p1, and θ0 = q--1. In particular, given any compact
and positively invariant subset S ⊂ D, both flows converge infinite time With the aforementioned
convergence time upper bound (WhiCh can be tightened by replacing D with S) for any xo ∈ S.
Furthermore, if D = Rn, then we have global finite-time convergence, i.e. finite-time convergence to
any maximal solution (in the sense ofFilippov) x(∙) with arbitrary xo ∈ Rn.
In essence, the analysis introduced in Romero & Benosman (2020) consists of leveraging the gradient
dominance to show that the energy function E(t) , f (x(t)) - f? satisfies the Lyapunov-like
differential inequality E(t) = O(E(t)α) for some α < 1. The detailed proof is recalled in Appendix
C for completeness.
1For more modern treatments in English, see Lojasiewicz & Zurro (1999); Bolte et al. (2007)
4
Under review as a conference paper at ICLR 2022
3 Main Results: Convergence Analysis for Euler Discretization
With the previous discussion on continuous-time gradient flow, now we turn to the algorithm perspec-
tive. We propose the following general framework based on forward Euler discretization of the flows
with finite-time convergence guarantee.
xk+1 = xk + ηF(xk), η > 0	(15)
where F ∈ {Fq-RGF , Fq-SGF}. We will show later, in Theorem 2, that the simple method leads, for
small enough η > 0, to solutions that are O()-close to the continuous-time flows. Also with a little
abuse of notations, we will still use RGF/SGF to call the discrete-time algorithms induced by their
corresponding continuous-time flows if the context is clear.
3.1	Closeness of The Discretization
We present here some convergence results of the proposed discretization. We first start with a
proximity analysis, which aims at showing the closeness between the solutions of the continuous
flows and the trajectories of their forward Euler discretization.
Theorem 2 (Closeness of Discretization). Suppose that f : Rn → R is continuously differentiable,
locally Lf -Lipschitz, and μ-gradient dominated oforderP ∈ (1, ∞) in a compact neighborhood S
of a strict local minimizer x? ∈ Rn. Let c > 0 and q ∈ (p, ∞]. Then, for a given initial condition
x0 ∈ S, any maximal solution x(t), x(0) = x0, (in the sense of Filippov) to the q-RGF given by (2)
or the q-SGF flow (3), there exists an arbitrarily small > 0 such that the solution xk of any of the
discrete algorithm (15), with sufficiently small η > 0, are -close to x(t), i.e., kxk - x(t)k2 ≤ for
|t 一 kη∣ < e, and s.t. the following bound holds
kf (Xk) —	f (x*)l∣2 ≤	Lfe +	h(f(X0)—	f (x*))(1-α) —	c(1	一 α)ηk](	ɑ, Lf >	0,	k ≤	k?,
kf(xk) 一 f(x?)k2 ≤ Lfe, k>k?,
(16)
where α =呆,θ =	P-I,	θ =	q-1,	C = C ((p-1)	P μ 1 )	(I	and k?	=	(I(XO)式与11	a).
The analysis summarized in Theorem 2 is based on tools form hybrid control theory, and is detailed
in Appendix D2. Theorem 2 shows that e-convergence of xk → x? can be achieved in a finite number
of steps upper bounded by k? = (f(x0 )-&)？(——)
This is a preliminary convergence result aiming
to show the existence of discrete solutions obtained via the proposed discretization algorithms, which
are e-close to the continuous solutions of the finite-time flows. We also underline here that after xk
reaches an e-neighborhood of x?, then xk+1 ≈ xk, ∀k > k?, since x? is an equilibrium point of the
continuous flows, i.e., ∆f(x?) = 0, e.g. see Definition 2 in Appendix B.
3.2	Convergence Rates of Euler Discretization: General Case
However, the bound (16) does not allow us to have a constructive practical value of the stepsize
η, since the pair (e, η) are implicitly related, for ‘sufficiently small’ η (refer to the definition of
e-closeness and the proof of Theorem 2 in Appendix D). To derive convergence rates of the proposed
discretization with practical stepsize η, we need to introduce an extra assumption of L-Lipschitz
smooth, as presented below.
Assumption 2 (Lipschitz Smoothness of Order q). We assume the function f is L-Lipschitz smooth
of order q ∈ (1, 2], i.e., for any x, y ∈ Rn,
kVf(y)-Vf(x)∣∣2 ≤ Lky -x∣∣2-1.	(17)
Remark 2. The smoothness assumption is a very common setting in optimization algorithm literature
(Nesterov, 2003; Beck, 2017). ThiS assumption above is also called as (L, q)-Holder continuity
2Note that there might be several ways of approaching this proof. For instance, one could follow the general
results on stochastic approximation of set-valued dynamical systems, using the notion of perturbed solutions to
differential inclusions presented in Benaim et al. (2005).
5
Under review as a conference paper at ICLR 2022
(Devolder et al., 2014; Nesterov, 2015), it is easy to find that the condition will lead to the following
property:
f(y) ≤ f (x) + hVf (x),y - Xi + q ky - xk2,	(18)
which is also called weak smoothness. When q = 2, the function will be Lipschitz smooth, so the
above setting is a generalization of the common Lipschitz smoothness.
With these setting, now we will start our discussion from the general case, i.e., we have the access to
the true gradient Vf (x). The main result is presented in the following theorem, while We defer the
proof to Appendix E. .
Theorem 3 (Convergence Rate of the general Case). Suppose that f : Rn → R is continuously dif-
ferentiable, μ-gradient dominated oforder q and L-Lipschitz smooth oforder q following Assumption
1 and 2, then running q-RGF(equation 2 and equation 15)for K iterations with a stepsize η = L1-q
satisfies that	K
f (XK) — f? ≤(1 — K1-q) (f (xo) - f?),	(19)
and running q-SGF (equation 3 and equation 15)for K iterations with η = (nL) 1-q satisfies that
f(xκ) - f? ≤
K
(f(X0) - f?),
(20)
where n is the dimension number and K ， L. The corresponding iteration complexity are
O (Kq-1 ln ：) and O ((n2K)q-1 ln ɪ^, respectively.
Remark 3. The above results show that with a constant stepsize, both the q-RGF and q-SGF can
attain a linear convergence rate to reach the optimal point. Note that for the classical Lipschitz smooth
case (i.e, q = 2), the RHS of the RGF result (equation 19) will reduce to(1 - KT)K(f (xo) - f ?),
which is the same as that in (Karimi et al., 2016, Theorem 1); while for SGF (equation 20), it will be
(1 - (nK)-1)K(f (xo) - f?), which recovers the result in (Beznosikov et al., 2020, Theorem 13)3.
So we can conclude that our results extend the classical results in Lipschitz smoothness case.
3.3	Convergence Rates of Euler Discretization: Stochastic Case
Attaining the full gradient may be expensive in practical applications due to the large data size,
while stochastic optimization algorithms are more applicable in such cases (Robbins & Monro, 1951;
Bottou et al., 2018). So now we turn to discuss the complexity results when we have only the access
to the unbiased estimator of the gradient. To formalize the discussion, we impose the following
assumption on the gradient estimator, which is very common in stochastic optimization literature
(Ghadimi & Lan, 2013; Bottou et al., 2018). .
Assumption 3 (Unbiased Gradient Estimator). For any given X ∈ Rn , we assume to have only the
access of the unbiased gradient estimator, denoted as g(X; ξ) where ξ is a random variable, while we
assume that it satisfies that Eξ[g(X; ξ)] = Vf (X), Eξkg(X; ξ) - Vf (X)k22 ≤ σ2.
Stochastic q-RGF We present the convergence result of stochastic q-RGF in the following theorem,
and defer the proof to Appendix F. For convenience, we denote ψ，q--ɪ ∈ [2, +∞].
Theorem 4 (Convergence Rate of the Stochastic RGF). With the setting in Theorem 3, if we fur-
ther assume for each X ∈ Rn, we only have access to the unbiased estimator of the gradient in
Assumption 3, then if we replace the gradient in q-RGF (equation 2) by its unbiased estimator
g(x)，b(X) Pb=X) g(x; ξi) with batch size b(x) ∈ N+. Thenfor q-RGF (equation 2 and equation 15)
ɪ	2
with g(x), ifwe set η = (ψL)1-q, b(x) ≡ (2∙(2σ)ψμ 1 q ) , then we have
E[f (XK) - f?] ≤ (l- (2ψ2)ψK±)(f (xo) - f?) + |.	(21)
3Note that n ∣∣Vf (x)k-1 sign(Vf (x)) is an n-approximate compressor of gradient Vf (x) (Karimireddy
et al., 2019), so we can apply (Beznosikov et al., 2020, Theorem 13) to recover the result.
6
Under review as a conference paper at ICLR 2022
Stochastic q-SGF The analysis of the Euler discretization of q-SGF in the stochastic case is a bit
more complicated. Indeed, note that previously our discussion are based on '2-norm. Here, however,
following the argument in Balles et al. (2020), we change the gradient dominance and Lipschitz
smoothness assumption to hold in '∞-norm (and its dual norm '1).
Assumption 4 (Gradient Dominance and Lipschitz Smoothness under ('1,'∞)-norms). We assume
the function f is μ-gradient dominated of order q and L-Lipschitz smooth of order q under '∞-norm
(and its dual norm '1), i.e., for any x, y ∈ Rn
q-1 kVf(x)k卢 ≥ μq⅛(f(x)-f*),	kVf(y) -Vf(x)kι ≤ Lky-x∣∣∞Λ	(22)
Similarly the new smoothness condition above implies that f(y) ≤ f(x) + hVf (x), y 一 xi +
L ky 一 χk∞ (∀χ,y ∈ Rn). And for the gradient estimator, we need the following assumption.
Assumption 5 (SPB Unbiased Gradient Estimator (Safaryan & Richtarik, 2021)). For any given
x ∈ Rn, we assume to have only the access of the unbiased gradient estimator under '1-norm, i.e.,
Eξ[g(x; ξ)] = Vf (x), Eξkg(x; ξ) 一 Vf (x)k12 ≤ σ2; and for its mini-batch version g(x) with batch
size b(x), we further assume that it satisfies success probability bounds (SPB) property, i.e., for each
component of g(χ) and Vf (x) (denoted as gi(x) and Vif (x)), we have ∃p* ∈ (ɪ, 1] such that
P(Sign (Vif(X))=sign(gi(x))) ≥ p* > 1,	∀ i ∈{1, 2,…，n}.
(23)
Remark 4. Note that similar assumptions under '∞-norm (and its dual norm 'ι) are widely adapted
in the literature of sign-based algorithms, e.g., Karimi et al. (2016); Bernstein et al. (2018a;b); Balles
et al. (2020). Here the extra SPB assumption, proposed in Safaryan & Richtarik (2021), is very
important in the convergence analysis of stochastic sign-based algorithms. An intuitive understanding
to justify SPB is that, with a large batch size, the output estimator g(x) will be very similar to Vf (x),
so the corresponding probability above should be higher. For more details on the rationality of the
assumption, we refer the reader to Safaryan & RiChtarik (2021) for further discussions.
With the above preparation, we summarize the result of stochastic SGF in the following theorem,
while deferring the proof to Appendix G.
Theorem 5 (Convergence Rate of the Stochastic SGF). Suppose the function f satisfies the setting
in Assumption 4 above, then for q-SGF (equation 3 and equation 15) with an unbiased gradient
estimator g (x) of batch size b(x) ∈ N+, ifwe assume the gradient estimator g(x) satisfies Assumption
5, and set η = (L2ψ-1) 1 q and b(xk) ≡
Ψ2	-
(2p* 一 1)- σ2, it satisfies that
1
2μ 1-q
qe
/	.......ɪ、K	F
E[f(xκ) — f?] ≤ (l-(2p*- 1)ψ(κ2ψT)1-q) (f(xo) - f?) + -.
(24)
Remark 5. The results in Theorem 4 and Theorem 5 shows that both stochastic RGF and SGF will
drive the function value to linearly converge to the O()-neighborhood of the optimal value. As a
comparison on stochastic RGF, Lei et al. (2019) studied the convergence guarantee of stochastic
gradient descent (SGD), which can be viewed as stochastic 2-RGF, under a similar setting. Also
for stochastic SGF, we notice that recently Li et al. (2021) provided a similar convergence result of
stochastic SGF in the classical setting (i.e., q = 2), their result also requires an lower bound on the
probability of estimation correctness, which corresponds to the SPB assumption above. Our result
can be viewed as an extension of their work.
4	Numerical Experiments
In this section, we will use several experiments to verify the effectiveness of our proposed algorithms,
more details will be presented in Appendix H.
4.1	Numerical Experiments on Academic Examples
Let us show first on a simple numerical example that the acceleration in convergence, proven
in continuous time for certain range of the hyperparmeters, can translate to some convergence
acceleration in discrete time.
7
Under review as a conference paper at ICLR 2022
Example 1 Consider the following function: f : R → R and f (x)，1 |x|q, which can be shown to
be (q - 1)q-1-gradient dominated of order q, and 2-Lipschitz smooth of order q when 1 < q ≤ 2 (Lei
& Ying, 2020; Romero & Benosman, 2020). Here we set q = 1.95 for the objective function. The
test results of the proposed RGF and SGF algorithms compared with other algorithms is presented
in Figure 1 below. The figure shows both sensitivities to stepsizes and performances of algorithms,
measured by the optimality gap f (x) - f * where f * is the optimal value which is 0. We can find
that q-RGF and q-SGF, comparing to the classical one (2-RGF) and an over-large one (10-RGF), can
accommodate larger stepsize and attain better performances with the best-tuned stepsize4. Also we
further plot the theoretical bounds provided in Theorem 3 (in red), we can find that the proposed
algorithms attain linear convergence patterns and their optimal stepsize choice is close to those in
Theorem 3, so the experiment results matches the theory well in the example.
Figure 1: Results of discretization of RGF and SGF with different q on Example 1
SGF, Performance of Each Algorithm, q=1.95
Example 2 We consider the Rosenbrock functionf : R2 → R, given by f(x1, x2) = (a - x1)2 +
b(x2 - x21)2, with parameters a, b ∈ R, which is locally Lipschitz smooth. This function admits
exactly one stationary point (x1?, x2?) = (a, a2) for b ≥ 0, and is locally strongly convex, hence locally
satisfies gradient dominance of order p = 2, which allows us to select q > 2 in q-RGF and q-SGF to
achieve finite-time convergence in conitnuous-time. We report in Figure 2 the mean performance
of forward-Euler discretization for q-RGF and q-SGF, with fixed step size5, for several values of
q, for 10 random initial conditions in [0, 2]. We observe that, as expected from the continuous flow
analysis, for q close to 2, q-RGF behaves similar to GD in terms of convergence rate, whereas for
q > 2 the finite-time convergence in continuous time translates to some acceleration in the associated
discretization algorithm as well. Similarly for q-SGF, q closer to 2 translates to less accelerated
algorithms, with a behavior similar to GD, whereas larger q values lead to accelerated convergence.
4.2	Numerical Experiments on Real-world Data
We report here the results of our experiments using deep neural network (DNN) training on the SVHN
dataset. Note that, we use PyTorch platform to conduct all the tests reported in this paper. Both CPU
and GPU tests have been performed. The results reported in this section are CPU tests, whereas
GPU tests are reported in Appendix H. We underline here that the DNNs are non-convex globally,
however, one could assume at least local convexity, hence local gradient dominance of order p = 2,
thus, we will select q > 2 in our experiments (see (Remark 7, Appendix H) for more explanations on
the choice of q).
4The experiment results show that q-RGF and q-SGF will drive the function value to exactly 0 with certain
stepsizes, which generates plots that may exceed the lower limit in figures.
5We did multiple iterations to find the best step size for each algorithm (best values where between 10-4 and
10-2 depending on the algorithm). Details of the step size for each test are given in Appendix H.
8
Under review as a conference paper at ICLR 2022
Figure 2: Results of Discretization of RGF and SGF with Different q on Example 2
4.2.1	Experiments on SVHN Dataset
We test the proposed algorithms to train the same VGG16 CNN model with cross entropy loss
on the SVHN dataset. We divided the dataset into a training set of 74 batches with 1000 images
each, and a test set of 27 batches of 1000 images each, and ran 20 epochs of training over all the
training batches. We tested the discretization of q-RGF (c = 1, q = 2.1, η = 0.04), and of q-SGF
(c = 10-3, q = 2.1, η = 0.04) against classical gradient descent (GD), and Adam6. Note from
Figures 3, 4 it is clear that q-RGF and q-SGF give a good performance in terms of convergence
speed, and final test performance 93%. We can also observe in Figure 4 that q-SGF, and q-RGF
converge faster (40 min lead in average) than SGD and Adam for these tests, and reach an overall
similar performance on the test-set. Additional numerical results, including GPU runs, can be found
in Appendix H.
Figure 3: Losses for several optimization algorithms- SVHN:
Train loss (left), test loss (right)
Figure 4: Training loss vs. com-
putation time for several opti-
mization algorithms- VGG-16-
SVHN
5	Conclusion
We presetned some connections between optimization algorithms and continuous-time representa-
tions (dynamical systems) via discretization. We then reviewed two families of non-Lipschitz or
discontinuous first-order optimization flows for continuous-time optimization, namely the q-RGF
and q-SGF, whose distinguishing characteristic is their finite-time convergence. We then proposed a
forward Euler discretization of these flows. Based on tools from hybrid systems control theory, we
proved a closeness convergence bound for these algorithms, and then proposed several convergence
rates in the deterministic and the stochastic setting. Finally, we conducted numerical experiments on
a known deep neural network benchmark, which showed that the proposed discrete algorithms can
outperform some state of the art algorithms, when tested on large DNN models.
6We also tested Adaptive gradient (AdaGrad), per-dimension learning rate method for gradient descent
(AdaDelta), and Root Mean Square Propagation (RMSprop). However, since their performance was not
competitive we decided not to report the plots to avoid overloading the figures.
9
Under review as a conference paper at ICLR 2022
References
Hedy Attouch and Jerome Bolte. On the convergence of the proximal algorithm for nonsmooth
functions involving analytic features. Mathematical Programming B, 116(1):5-16, 2009.
Andrea Bacciotti and Francesca Ceragioli. Stability and stabilization of discontinuous systems and
nonsmooth lyapunov functions. ESAIM: Control, Optimisation and Calculus of Variations, 4:
361-376, 1999.
Lukas Balles, Fabian Pedregosa, and Nicolas Le Roux. The geometry of sign gradient descent. arXiv
preprint arXiv:2002.08056, 2020.
Amir Beck. First-order methods in optimization. SIAM, 2017.
Michel Benalm, Josef Hofbauer, and Sylvain Sorin. Stochastic approximations and differential
inclusions. SIAM Journal on Control and Optimization, 44(1):328-348, 2005.
Jeremy Bernstein, Yu-Xiang Wang, Kamyar Azizzadenesheli, and Animashree Anandkumar. signsgd:
Compressed optimisation for non-convex problems. In International Conference on Machine
Learning, pp. 560-569. PMLR, 2018a.
Jeremy Bernstein, Jiawei Zhao, Kamyar Azizzadenesheli, and Anima Anandkumar. signsgd with
majority vote is communication efficient and fault tolerant. arXiv preprint arXiv:1810.05291,
2018b.
Aleksandr Beznosikov, Samuel Horvath, Peter Richtarik, and Mher Safaryan. On biased compression
for distributed learning. arXiv preprint arXiv:2002.12410, 2020.
Jerome Bolte, Aris Daniilidis, and Adrian Lewis. The Eojasiewicz inequality for nonsmooth suban-
alytic functions with applications to subgradient dynamical systems. Society for Industrial and
Applied Mathematics, 17:1205-1223, January 2007.
C.A Botsaris. A class of methods for unconstrained minimization based on stable numerical integra-
tion techniques. Journal of Mathematical Analysis and Applications, 63(3):729-749, 1978a.
C.A. Botsaris. Differential gradient methods. Journal of Mathematical Analysis and Applications, 63
(1):177-198, 1978b.
Leon Bottou, Frank E Curtis, and Jorge Nocedal. Optimization methods for large-scale machine
learning. Siam Review, 60(2):223-311, 2018.
R.W. Brockett. Dynamical systems that sort lists, diagonalize matrices and solve linear programming
problems. In IEEE Conference on Decision and Control, pp. 799-803, 1988.
A.A. Brown. Some effective methods for unconstrained optimization based on the solution of
systems of ordinary differential equations. Journal of Optimization Theory and Applications, 62
(2):211-224, August 1989.
Frank H. Clarke. Generalized gradients of lipschitz functionals. Advances in Mathematics, 40(1):
52-67, 1981.
Jorge Cortes. Finite-time convergent gradient flows with applications to network consensus. Auto-
matica, 42(11):1993-2000, November 2006.
Jorge Cortes. Discontinuous dynamical systems. IEEE Control Systems Magazine, 28(3):36-73, June
2008.
Jorge Cortes and Francesco Bullo. Coordination and geometric optimization via distributed dynamical
systems. SIAM Journal on Control and Optimization, 44(5):1543-1574, October 2005.
Olivier Devolder, Francois Glineur, and Yurii Nesterov. First-order methods of smooth convex
optimization with inexact oracle. Mathematical Programming, 146(1):37-75, 2014.
Cong Fang, Chris Junchi Li, Zhouchen Lin, and Tong Zhang. Spider: near-optimal non-convex
optimization via stochastic path integrated differential estimator. In Proceedings of the 32nd
International Conference on Neural Information Processing Systems, pp. 687-697, 2018.
10
Under review as a conference paper at ICLR 2022
M. Fazlyab, A. Koppel, V. M. Preciado, and A. Ribeiro. A variational approach to dual methods for
constrained convex optimization. In 2017 American Control Conference (ACC), pp. 5269-5275,
May 2017a.
M. Fazlyab, A. Koppel, V. M. Preciado, and A. Ribeiro. A variational approach to dual methods for
constrained convex optimization. In 2017 American Control Conference (ACC), pp. 52690-5275,
May 2017b.
M. Fazlyab, M. Morari, and V. M. Preciado. Design of first-order optimization algorithms via sum-
of-squares programming. In IEEE Conference on Decision and Control (CDC), pp. 4445-4452,
December 2018.
Mahyar Fazlyab, Alejandro Ribeiro, Manfred Morari, and Victor M. Preciado. Analysis of optimiza-
tion algorithms via integral quadratic constraints: Nonstrongly convex problems. SIAM J. Optim,
28(3):2654-2689, 2018.
Aleksei Fedorovich Filippov and F. M. Arscott. Differential equations with discontinuous righthand
sides. Kluwer Academic Publishers Group, Dordrecht, Netherlands, 1988.
Guilherme Franca, Daniel Robinson, and Rene Vidal. Admm and accelerated admm as continuous
dynamical systems. In International Conference on Machine Learning, pp. 1559-1567. PMLR,
2018.
G. Franca, D.P. Robinson, and R. Vidal. A dynamical systems perspective on nonsmooth constrained
optimization. arXiv preprint 1808.04048, 2019a.
G. Franca, J. Sulam, D. Robinson, and R. Vidal. Conformal symplectic and relativistic optimization.
arXiv preprint 1903.04100, 2019b.
Saeed Ghadimi and Guanghui Lan. Stochastic first-and zeroth-order methods for nonconvex stochastic
programming. SIAM Journal on Optimization, 23(4):2341-2368, 2013.
Saeed Ghadimi, Guanghui Lan, and Hongchao Zhang. Mini-batch stochastic approximation methods
for nonconvex stochastic composite optimization. Mathematical Programming, 155(1-2):267-305,
2016.
Uwe Helmke and John Barratt Moore. Optimization and Dynamical Systems. Springer-Verlag, 1994.
Qing Hui, Wassim Haddad, and Sanjay Bhat. Semistability, finite-time stability, differential inclusions,
and discontinuous dynamical systems having a continuum of equilibria. IEEE Transactions on
Automatic Control, 54:2465-2470, November 2009.
Hamed Karimi, Julie Nutini, and Mark Schmidt. Linear convergence of gradient and proximal-
gradient methods under the Polyak-Iojasiewicz condition. In Joint European Conference on
Machine Learning and Knowledge Discovery in Databases, pp. 795-811. Springer, 2016.
Sai Praneeth Karimireddy, Quentin Rebjock, Sebastian Stich, and Martin Jaggi. Error feedback
fixes signsgd and other gradient compression schemes. In International Conference on Machine
Learning, pp. 3252-3261. PMLR, 2019.
Yunwen Lei and Yiming Ying. Fine-grained analysis of stability and generalization for stochastic
gradient descent. In International Conference on Machine Learning, pp. 5809-5819. PMLR, 2020.
Yunwen Lei, Ting Hu, Guiying Li, and Ke Tang. Stochastic gradient descent for nonconvex learning
without bounded gradient assumptions. IEEE transactions on neural networks and learning
systems, 31(10):4394-4400, 2019.
L. Lessard, B. Recht, , and A. Packard. Analysis and design of optimization algorithms via integral
quadratic constraints. SIAM J. Optim, 26(1):57-95, 2016.
Xiuxian Li, Kuo-Yi Lin, Li Li, Yiguang Hong, and Jie Chen. On faster convergence of scaled sign
gradient descent. arXiv preprint arXiv:2109.01806, 2021.
S. Eojasiewicz. A topological property of real analytic subsets (in French). Les equations aux
de´ rive´ es partielles, pp. 87-89, 1963.
11
Under review as a conference paper at ICLR 2022
S. Eojasiewicz. Ensembles Semi-analytiques. Centre de Physique Theorique de 1'Ecole Polytechnique,
1965. URL https://perso.univ-rennes1.fr/michel.coste/Lojasiewicz.
pdf.
StanisEaw Eojasiewicz and Maria-Angeles Zurro. On the gradient inequality. Bulletin of the Polish
Academy of Sciences, Mathematics, 47, January 1999.
Jincheng Mei, Chenjun Xiao, Csaba Szepesvari, and Dale Schuurmans. On the global convergence
rates of softmax policy gradient methods. In International Conference on Machine Learning, pp.
6820-6829. PMLR, 2020.
Jincheng Mei, Yue Gao, Bo Dai, Csaba Szepesvari, and Dale Schuurmans. Leveraging non-uniformity
in first-order non-convex optimization. arXiv preprint arXiv:2105.06072, 2021.
Michael Muehlebach and Michael Jordan. A dynamical systems perspective on Nesterov acceleration.
In Kamalika Chaudhuri and Ruslan Salakhutdinov (eds.), Proceedings of the 36th International
Conference on Machine Learning, volume 97 of Proceedings of Machine Learning Research, pp.
4656-4662, Long Beach, California, USA, 09-15 Jun 2019. PMLR.
Yu Nesterov. Universal gradient methods for convex optimization problems. Mathematical Program-
ming, 152(1):381-404, 2015.
Yurii Nesterov. Introductory lectures on convex optimization: A basic course, volume 87. Springer
Science & Business Media, 2003.
A. Orvieto and A. Lucchi. Shadowing properties of optimization algorithms. In Neural Information
Processing Systems, December 2019.
Bradley Paden and Shankar Sastry. A calculus for computing filippov’s differential inclusion with
application to the variable structure control of robot manipulators. IEEE Transactions on Circuits
and Systems, 34:73-82, February 1987.
Boris Polyak. Gradient methods for the minimisation of functionals (in Russian). USSR Computa-
tional Mathematics and Mathematical Physics, 3:864-878, December 1963.
Herbert Robbins and Sutton Monro. A stochastic approximation method. The annals of mathematical
statistics, pp. 400-407, 1951.
O. Romero and M. Benosman. Finite-time convergence in continuous-time optimization. In Interna-
tional Conference on Machine Learning, Vienna, Austria, July 2020.
O. Romero, S. Chaterjee, and S. Pequito. Convergence of the expectation-maximization algorithm
through discrete-time lyapunov stability theory. Proceedings of the American Control Conference
(ACC), pp. 163-168, July 2019.
Mher Safaryan and Peter Richtarik. Stochastic sign descent methods: New algorithms and better
theory. In International Conference on Machine Learning, pp. 9224-9234. PMLR, 2021.
R. G. Sanfelice and A. R. Teel. Dynamical properties of hybrid systems simulators. Automatica, 46:
239-248, 2010.
Johannes Schropp. Using dynamical systems methods to solve minimization problems. Applied
Numerical Mathematics, 18(1):321-335, 1995.
Johannes Schropp and I Singer. A dynamical systems approach to constrained minimization. Numer-
ical Functional Analysis and Optimization, 21:537-551, May 2000.
D. Scieur, V. Roulet, F. Bach, , and A. d’Aspremont. Integration methods and optimization algorithms.
In Neural Information Processing Systems, December 2017.
Bin Shi, Simon Du, Michael Jordan, and Weijie Su. Understanding the acceleration phenomenon via
high-resolution differential equations. arXiv preprint 1810.08907, October 2018.
J.A. Snyman. A new and dynamic method for unconstrained minimization. Applied Mathematical
Modelling, 6(6):448-462, December 1982.
12
Under review as a conference paper at ICLR 2022
J.A. Snyman. An improved version of the original leap-frog dynamic method for unconstrained
minimization: LFOP1(b). Applied Mathematical Modelling, 7(3):216-218, June 1983.
Andrew M. Stuart and A. R. Humphries. Dynamical systems and numerical analysis. Cambridge
University Press, first edition, November 1998.
W. Su, S. Boyd, and E. J. Candes. A differential equation for modeling Nesterov’s accelerated
gradient method: Theory and insights. In Advances in Neural Information Processing Systems, pp.
2510-2518. Curran Associates, Inc., 2014.
Adrien Taylor, Bryan Van Scoy, and Laurent Lessard. Lyapunov functions for first-order methods:
Tight automated convergence guarantees. In International Conference on Machine Learning,
Stockholm, Sweden, July 2018.
J. Wang and N. Elia. A control perspective for centralized and distributed convex optimization. In
IEEE Conference on Decision and Control and European Control Conference, pp. 3800-3805,
December 2011.
Andre Wibisono, Ashia C. Wilson, and Michael I. Jordan. A variational perspective on accelerated
methods in optimization. Proceedings of the National Academy of Sciences, 113(47):E7351-E7358,
2016.
A. Wilson. Lyapunov Arguments in Optimization. PhD thesis, UC Berkeley, 2018.
Ashia Wilson, Lester Mackey, and Andre Wibisono. Accelerating rescaled gradient descent: Fast
optimization of smooth functions. arXiv preprint arXiv:1902.08825, 2019.
Abbas K. Zghier. The use of differential equations in optimization. PhD thesis, Loughborough
University, 1981.
J. Zhang, A. Mokhtari, S. Sra, , and A. Jadbabaie. Direct runge-kutta discretization achieves
acceleration. In Neural Information Processing Systems, December 2018.
13
Under review as a conference paper at ICLR 2022
A Discontinuous Systems and Differential Inclusions
Recall that for an initial value problem (IVP)
X(t) = F(x(t))	(25a)
x(0) = x0	(25b)
with F : Rn → Rn , the typical way to check for existence of solutions is by establishing continuity
of F . Likewise, to establish uniqueness of the solution, we typically seek Lipschitz continuity. When
F is discontinuous, we may understand (25a) as the Filippov differential inclusion
X(t) ∈ K [F ](x(t)),	(26)
where K[F] : Rn ⇒ Rn denotes the Filippov set-valued map given by
K [F ](x) , \ \ co F (Bδ (x) \ S),	(27)
δ>0 μ(S) = 0
where μ denotes the usual LebesgUe measure and co the convex closure, i.e. closure of the convex
hull co. For more details, see Paden & Sastry (1987). We can generalize (26) to the differential
inclusion Bacciotti & Ceragioli (1999)
X(t) ∈ F(χ(t)),	(28)
where F : Rn ⇒ Rn is some set-valued map.
Definition 1 (Caratheodory/Filippov solutions). We say that x : [0,τ) → Rn with 0 < τ ≤ ∞ is
a Caratheodory solution to (28) if x(∙) is absolutely continuous and (28) is satisfied a.e. in every
compact subset of [0, T). Furthermore, We say that χ(∙) is a maximal Caratheodory solution if no
other Caratheodory solution x0(∙) exists with X = χ0∣[o,τ). If F = K [F ], then Caratheodory solutions
are referred to as Filippov solutions.
For a comprehensive overview of discontinuous systems, including sufficient conditions for existence
(Proposition 3) and uniqueness (Propositions 4 and 5) of Filippov solutions, see the work of COrteS
(2008). In particular, it can be established that Filippov solutions to (25) exist, provided that the
following assumption (Assumption 6) holds.
Assumption 6 (Existence of Filippov solutions). F : Rn → Rn is defined almost everywhere (a.e.)
and is Lebesgue-measurable in a non-empty open neighborhood U ⊂ Rn of x0 ∈ Rn . Further, F is
locally essentially bounded in U, i.e., for every point x ∈ U, F is bounded a.e. in some bounded
neighborhood of x.
More generally, Caratheodory solutions to (28) exist (now with arbitrary xo ∈ Rn), provided that the
following assumption (Assumption 7) holds.
Assumption 7 (Existence of Caratheodory solutions). F : Rn ⇒ Rn has nonempty, compact, and
convex values, and is upper semi-continuous.
Filippov & Arscott (1988) proved that, for the Filippov set-valued map F = K[F], Assumptions 6
and 7 are equivalent (with arbitrary x0 ∈ Rn in Assumption 6).
Uniqueness of the solution requires further assumptions. Nevertheless, we can characterize the
Filippov set-valued map in a similar manner to Clarke’s generalized gradient, as seen in the following
proposition.
Proposition 1 (Theorem 1 of Paden & Sastry (1987)). Under Assumption 6, we have
K[F](x) =	lim F(xk) : xk ∈ Rn \ (NF ∪ S) s.t. xk → x	(29)
k→∞
for some (Lebesgue) zero-measure set NF ⊂ Rn and any other zero-measure set S ⊂ Rn . In
particular, if F is continuous at a fixed x, then K[F](x) = {F (x)}.
For instance, for the GF (1), we have K[-Vf](χ) = {-Vf (χ)} for every X ∈ Rn, provided that f
is continuously differentiable. Furthermore, if f is only locally Lipschitz continuous and regular (see
Definition 3 of Appendix B), then K[-Vf](x) = -∂ f (x), where
∂f(x) ,	lim Vf(xk) : xk ∈ Rn \ Nf s.t. xk → x	(30)
k→∞
14
Under review as a conference paper at ICLR 2022
denotes Clarke’s generalized gradient Clarke (1981) of f, with Nf denoting the zero-measure set
over which f is not differentiable (Rademacher’s theorem). It can be established that ∂f coincides
with the subgradient of f, provided that f is convex. Therefore, the GF (1) interpreted as Filippov
differential inclusion may also be seen as a continuous-time variant of subgradient descent methods.
B Finite-Time Stability of Differential Inclusions
We are now ready to focus on extending some notions from traditional Lipschitz continuous systems
to differential inclusions.
Definition 2. We say that x? ∈ Rn is an equilibrium of (28) if x(t) ≡ x? on some small enough non-
degenerate interval is a Caratheodory solution to (28). In other words, if and only if 0 ∈ F(x?). We
say that (28) is (Lyapunov) stable atx? ∈ Rn if, for every ε > 0, there exists some δ > 0 such that, for
every maximal Caratheodory solution χ(∙) of (28), we have ∣∣χo - χ*∣∣2 < δ =⇒ ∣∣x(t) - x*∣∣2 < ε
for every t ≥ 0 in the interval where x(∙) is defined. Note that, under Assumption 7, if (28) is stable
at x?, then x? is an equilibrium of (28) Bacciotti & Ceragioli (1999). Furthermore, we say that (28) is
(locally and strongly) asymptotically stable at x? ∈ Rn if is stable at x? and there exists some δ > 0
such that, for every maximal Caratheodory solution X : [0,τ) → Rn of (28), if ∣xo - x*∣2 < δ
then x(t) → x? as t → τ . Finally, (28) is (locally and strongly) finite-time stable at x? if it is
asymptotically stable and there exists some δ > 0 and T : Bδ (x?) → [0, ∞) such that, for every
maximal Caratheodory solution x(∙) of (28) with xo ∈ Bδ(x?), we have limt→τ(方。)x(t) = x?.
We will now construct a Lyapunov-based criterion adapted from the literature of finite-time stability
of Lipschitz continuous systems.
Lemma 1. Let E(∙) be an absolutely ContinuousfUnction satisfying the differential inequality
• , . . . _
E(t) ≤ -cE(t)α
(31)
a.e. in t ≥ 0, with c, E(0) > 0 and α < 1. Then, there exists some t? > 0 such that E(t) > 0 for
t ∈ [0, t?) and E(t?) = 0. Furthermore, t? > 0 can be bounded by
V E(0)1-α
c(1 - α)
(32)
with this bound tight whenever (31) holds with equality. In that case, but now with α ≥ 1, then
E(t) > 0 for every t ≥ 0, with limt→∞ E(t) = 0. This will be represented by t? = ∞, with
E(∞) , limt→∞ E (t).
Proof. Suppose that E(t) > 0 for every t ∈ [0, T] with T > 0. Let t? be the supremum of all such
T’s, thus satisfying E(t) > 0 for every t ∈ [0, t?). We will now investigate E(t?). First, by continuity
of E, it follows that E(t?) ≥ 0. Now, by rewriting
E(t) ≤ -CE(t)α 0 ddt [E(-α 1 ≤ -c,
(33)
a.e. in t ∈ [0, t?), we can thus integrate to obtain
E(t)1-α E(0)1-α
------------------
1-α 1-α
≤ -ct,
(34)
everywhere in t ∈ [0, t?), which in turn turn leads to
E(t) ≤ [E(0)1-α - c(1 - a)t]1/(1-a)	(35)
and
t≤
E(0)1-α -E(t)1-α
c(1 - α)
V E(0)1-α
≤ c(1 - α)
(36)
where the last inequality follows from E(t) > 0 for every t ∈ [0, t?). Taking the supremum in (36)
then leads to the upper bound (32). Finally, we conclude thatE(t?) = 0, since E(t?) > 0 is impossible
given that it would mean, due to continuity of E, that there exists some T > t? such that E(t) > 0 for
every t ∈ [0, T], thus contradicting the construction of t?.
15
Under review as a conference paper at ICLR 2022
Finally, notice that if E is such that (31) holds with equality, then (35) and the first inequality
in (36) hold with equality as well. The tightness of the bound (32) thus follows immediately.
Furthermore, notice that if α ≥ 1, and E is a tight solution to the differential inequality (31), i.e.
E(t) = [E(0)1-α — c(1 一 a)t]1/(1-a), then clearly E(t) > 0 for every t ≥ 0 and E(t) → 0 as
t → ∞.
Cortes & Bullo (2005) proposed (Proposition 2.8) a Lyapunov-based criterion to establish finite-
time stability of discontinuous systems, which fundamentally coincides with our Lemma 1 for the
particular choice of exponent α = 0. Their proposition was, however, directly based on Theorem 2
of Paden & Sastry (1987). Later, Cortes (2006) proposed a second-order Lyapunov criterion, which,
on the other hand, fundamentally translates to E(t) , V (x(t)) being strongly convex. Finally, Hui
et al. (2009) generalized Proposition 2.8 of COrteS & Bullo (2005) in their Corollary 3.1, to establish
semistability. Indeed, these two results coincide for isolated equilibria.
We now present a novel result that generalizes the aforementioned first-order Lyapunov-based results,
by exploiting our Lemma 1. More precisely, given a Laypunov candidate function V(∙), the objective
is to set E(t) , V (x(t)), and we aim to check that the conditions of Lemma 1 hold. To do this, and
assuming V to be locally Lipschitz continuous, we first borrow and adapt from Bacciotti & Ceragioli
(1999) the definition of set-valued time derivative of V : D → R w.r.t. the differential inclusion (28),
given by
V(X) = {a ∈ R : ∃v ∈ F(x) s.t. a = P ∙ v, ∀p ∈ ∂V(x)},	(37)
for each x ∈ D. Notice that, under Assumption 7 for Filippov differential inclusions F = K[F],
the set-valued time derivative of V thus coincides with with the set-valued Lie derivative LFV(∙).
Indeed, more generally V could be seen as a set-valued Lie derivative LFV w.r.t. the set-valued map
F.
Definition 3. V(∙) is said to be regular if every directional derivative, given by
V0(x; v) = lim
h→0
V(x + hv) — V(x)
h
exists and is equal to
I/O/	∖Δ 1.	V(X0 + hv) - V(XO)
V (x; V) = limsup -----------------------,
x0→x h→0+	h
known as Clarke’s upper generalized derivative Clarke (1981).
(38)
(39)
In practice, regularity is a fairly mild and easy to guarantee condition. For instance, it would suffice
that V is convex or continuously differentiable to ensure that it is Lipschitz and regular.
Assumption 8. V : D → R is locally Lipscthiz continuous and regular, with D ⊆ Rn open.
Under Assumption 8, Clarke’s generalized gradient
∂V(x) =	{p	∈	Rn	:	Vo(x;	v)	≥ p ∙ v,∀v ∈	Rn}	(40)
is non-empty for every X ∈ D, and is also given by
∂V(x) = < lim VV(Xk) : Xk ∈ Rn \ NV s.t. Xk → x ∖ ,	(41)
k→∞
where NV denotes the set of points in D ⊆ Rn where V is not differentiable (Rademacher’s
theorem) Clarke (1981).
Through the following lemma (Lemma 2), we can formally establish the correspondence between the
set-valued time-derivative of V and the derivative of the energy function E(t) = V (X(t)) associated
with an arbitrary Caratheodory solution x(∙) to the differential inclusion (28).
Lemma 2 (Lemma 1 of Bacciotti & Ceragioli (1999)). UnderAssUmPtion 8, given any Caratheodory
solution X : [0,	τ)	→	Rn	to (28), then	E(t)	=	V (X(t))	is absolutely continuous and	E(t)	=
ddt V(X(t)) ∈ V(∕(t)) a.e. in t ∈ [0,τ).
We are now ready to state and prove our Lyapunov-based sufficient condition for finite-time stability
of differential inclusions.
16
Under review as a conference paper at ICLR 2022
Theorem 6. Suppose that Assumptions 7 and 8 hold for some set-valued map F : Rn ⇒ Rn and
some function V : D → R, where D ⊆ Rn is an open and positively invariant neighborhood of a
point x? ∈ Rn. Suppose that V is positive definite w.r.t. x? and that there exist constants c > 0 and
α < 1 such that
supV(X) ≤ -cV(x)	(42)
a.e. in x ∈ D. Then, (28) is finite-time stable at x?, with settling time upper bounded by
t? ≤ V(x0)1-α
― c(1 - α)
(43)
where x(0) = xo. In particular, any Caratheodory solution x(∙) with x(0) = xo ∈ D will converge
in finite time to x? under the upper bound (43). Furthermore, if D = Rn, then (28) is globally
finite-time stable. Finally, if V(x) is a singleton a.e. in x ∈ D and (42) holds with equality, then the
bound (43) is tight.
Proof. Note that, by Proposition 1 of Bacciotti & Ceragioli (1999), we know that (28) is Lyapunov
stable at x? . All that remains to be shown is local convergence towards x? (which must be an
equilibrium) in finite time. Indeed, given any maximal solution x : [0, t?) → Rn to (28) with
x(0) = xo 6= x?, we know by Lemma 2, that E(t) = V (x(t)) is absolutely continuous with
E(t) ∈ V(x(t)) a.e. in t ∈ [0,t?]. Therefore, we have
E(t) ≤ sup V(x(t)) ≤ —cV(x(t))α = -CE(t)α	(44)
a.e. in t ∈ [0, t?]. Since E (0) = V(xo) > 0, given that xo 6= x?, the result then follows by invoking
Lemma 1 and noting that E(t?) = 0 ^⇒ V(t*,x(t*))=0 ^⇒ x(t?) = x?.	■
Finite-time stability still follows without Assumption 7, provided that x? is an equilibrium of (28). In
practical terms, this means that trajectories starting arbitrarily close to x? may not actually exist, but
nevertheless there exists a neighborhood D of x? over which, any trajectory x(∙) that indeed exists
and starts at x(0) = xo ∈ D must converge in finite time to x?, with settling time upper bounded by
T(xo) (the bound still tight in the case that (42) holds with equality).
C	Proof of Theorem 1
Let us focus on the q-RGF (2) (the case of q-SGF (3) follows exactly the same steps) with the
candidate Lyapunov function V , f - f(x?). Clearly, V is Lipschitz continuous and regular (given
that it is continuously differentiable). Furthermore, V is positive definite w.r.t. x?.
Notice that, due to the dominated gradient assumption, x? must be an isolated stationary point of
f . To see this, notice that, if x? were not an isolated stationary point, then there would have to exist
some x? sufficiently near x? such that x? is both a stationary point of f, and satisfies f (x?) > f (x?),
since x? is a strict local minimizer of f . But then, we would have
p 1	ψ^	1
0 = p--kVf(x*)kS-1 ≥ μX(f(x?) - f(x?)) > 0,	(45)
and subsequently 0 > 0, which is absurd.
q-2
Therefore, F(x)，-cVf (x)∕∣∣Vf (X)IlS-1 is continuous for every X ∈ D \ {0}, for some small
enough open neighborhood D of x? . Let us assume that D is positively invariant w.r.t. (2), which can
be achieved, for instance, by replacing D with its intersection with some small enough strict sublevel
1
set of f. Notice that ∣∣F(x)k2 = CIlVf(X)||厂1 with q ∈ (p, ∞] ⊂ (1, ∞], i.e., q-1 ∈ [0, ∞). If
q = ∞, which results in the normalized gradient flow X = 一 口"舄历 proposed by COrteS (2006),
then IF (x)I2 = C > 0. We can thus show that F(x) is discontinuous at x = 0 for q = ∞. On
the other hand, if q ∈ (p, ∞) ⊂ (1, ∞), then we have IF (x)I2 → 0 as x → x?, and thus F(x) is
continuous (but not Lipschitz) at x = x?. Regardless, we may freely focus exclusively on D \ {x?}
since {x?} is obviously a zero-measure set.
17
Under review as a conference paper at ICLR 2022
Let F , K[F]. We thus have, for each x ∈ D \ {x? },
sup V(x) = sup {a ∈ R : ∃v ∈ F(x) s.t. a = P ∙ v, ∀p ∈ ∂V(x)}	(46a)
= SuP {VV(x) ∙ V : V ∈ F(x)}	(46b)
=VV(x) ∙ F(X)	(46c)
2_ q-2
=-CkVf(X)k2 q-1	(46d)
1
=-CkVf(X)Ig	(46e)
≤-c[C(f (X)- f(x*))θ忖	(46f)
=-cC 6 V (x)导.	(46g)
Since θ < 1, given that s > 1 → s-1 is strictly increasing, then the conditions of Theorem 6 are
satisfied. In particular, we have finite-time stability at X? with a settling time t? upper bounded by
t? ≤ (f(x0)- f(x?))1-备 ≤ (kVf(x0)k2∕C)1 (I-导)=IlVf(X0)k1-θ	(47)
CCθ (1 - θθ0)	CCθ0 (1 - θθ0)	CCθ (1 - θθ0)
for each X0 ∈ D, which completes the proof.
D	Proof of Theorem 2
To prove Theorem 2, we borrow some tools and results from hybrid control systems theory. Hybrid
control systems are characterized by continuous flows with discrete jumps between the continuous
flows. They are often modeled by differential inclusions added to discrete mappings to model the
jumps between the differential inclusions. We see the case of the optimization flows proposed
here as a simple case of a hybrid systems with one differential inclusion, with a possible jump or
discontinuity at the optimum. Based on this, we will use the tools and results of Sanfelice & Teel
(2010), which study how a certain class of hybrid systems behave after discretization with a certain
class of discretization algorithms. In other words, Sanfelice & Teel (2010) quantifies, under some
conditions, how close are the solutions of the discretized hybrid dynamical system to the solutions of
the original hybrid system.
In this section we will denote the differential inclusion of the continuous optimization flow by
F : Rn ⇒ Rn, and its discretization in time by Fd : Rn ⇒ Rn . We first recall a definition, which
we will adapt from the general case of jumps between multiple differential inclusions (Definition 3.2,
Sanfelice & Teel (2010)) to our case of one differential inclusion or flow.
Definition 4. ((T, )-closeness). Given T > 0,	> 0, η > 0, two solutions Xt : [0, T] → Rn, and
Xk : {0, 1, 2, ...} → Rn are (T, )-close if:
(a) for all t ≤ T there exists k ∈ {1, 2,...} such that |t — kη∣ < 3 and ∣∣Xt(t) 一 Xk(k)∣∣2 < 3
(b) for all k ∈ {1, 2,...} there exists t ≤ T such that |t 一 kη∣ < 3 and ∣∣Xt(t) 一 Xk(k)∣2 < 3
Next, we will recall Theorem 5.2 in Sanfelice & Teel (2010), while adapting it to our special case of
a hybrid system with one differential inclusion7.
Theorem 7. (Closeness of continuous and discrete solutions on compact domains) Consider the
differential inclusion
,.. ...
X(t) ∈ F(X(t)),	(48)
for a given set-valued mapping F : Rm ⇒ Rm assumed to be outer semicontinuous, locally bounded,
nonempty, and with convex values for every X ∈ C, for some closed set C ⊆ Rm. Consider the
discrete-time system represented by the flow Fd : Rn ⇒ Rn, such that, for each compact set K ⊂ Rn,
there exists ρ ∈ K∞, and η? > 0 such that for each X ∈ K and each η ∈ (0, η?],
Fd(X) ⊂ X + η conF (X + ρ(η)B) + ηρ(η)B.	(49)
7A set-valued mapping F : Rn ⇒ Rn is outer semicontinuous if for each sequence {xi }i∞=1 converging
to a point x ∈ Rn and each sequence yi ∈ F (xi) converging to a point y, it holds that y ∈ F (x). It
is locally bounded if, for each x ∈ Rn , there exists compact sets K, K0 ⊂ Rn such that x ∈ K and
F(K) , ∪x∈K F (x) ⊂ K0. In what follows, we use the following notations: Given a set A, conA denotes the
convex hull, and B denotes the closed unit ball in a Euclidean space.
18
Under review as a conference paper at ICLR 2022
Then, for every compact set K ⊂ Rn, every > 0, and every time horizon T ∈ R≥0 there exists
η? > 0 such that: for any η ∈ (0, η?] and any discrete solution xk with xk (0) ∈ K + δB, δ > 0,
there exists a continuous solution xt with xt(0) ∈ K such that xk and xt are (T, )-close.
To prove Theorem 2 we will use the results of Theorem 7, where we will have to check that condition
(49) is satisfied for forward Euler discretization.
We are now ready to prove Theorem 2. First, note that outer semicontinuity follows from the upper
semicontinuity and the closedness of the Filippov differential inclusion map. Furthermore, local
boundedness follows from continuity everywhere outside stationary points, which are isolated.
Now, let us examine their discretization by forward-Euler.
The mapping Fd in this case is a singleton, given by
Fd(x) , x + ηF (x),	(50)
where η > 0, which clearly satisfies condition (49).
Then, using Theorem 7 we conclude about the (T, )-closeness between the continuous-time solutions
of the flows F : q-RGF (2), q-SGF (3), and the discrete-time solutions.
Finally, using the Lyapunov function V = f - f(x?) as defined in the proof of Theorem 1, together
with inequalities (46g), (35), and a local Lipschitz bound on f, one can derive the weak convergence
bound given by (16), as follows:
kf(xk) - f(x*) - (f(xt) - f(x*))k2 = kf(xk) - f(xt)k2 ≤ W = Lfe, Lf > 0, e > 0,
kf (xk) - f (x*)k2 -k(f (xt) - f (x*))k2 ≤ kf(xk) - f (xt)k2 ≤ e,
kf(xk) - f(x*)k2 ≤ W+ kf(xt)- f(x*)k2,	(51)
kf (xk) - f (x*)l∣2 ≤ e+ [(f (xo) - f (x*))(1-α) - c(1 - α)ηk]1∕(I-a), for k ≤ k*,
1
where α =3 θ = p-1, θ = T ,C= C ((占)ɪ μ B [ k?=("吗—⑶?…) ∙
Next, the case for k > k? is rather straightforward: Indeed, for t > t? by finite-time convergence
result, we have xt = x?, which directly leads to the bound kf(xk) - f(x?)k2 ≤ Lfe, k > k?, since
the term k(f(xt) - f(x?))k2 in (51) vanishes.
E Proof of Theorem 3
Proof. We divide the discussion by different flows8.
Proof of q-RGF: Following the definitions of smoothness, we have
f(xk+ι) ≤ f(xk) +〈▽/(Xk),Xk+ι - Xki + Lq∣∣Xk+ι - Xkk2
、	∣Vf(Xk)k2 , LYq	∣∣Vf(χk)k2
=f (Xk ) - Y	q-2 +	^	q(q-2)
∣Vf(χk)kΓ1	q HVf(Xk)U
=f(Xk) - (Y- LYq)∣Vf(Xk)k卢
≤ f(Xk) - ]ɪL土 ∣Vf(Xk)k产
≤ f (Xk) - -L1-q-j-τμq-1 (f (Xk) - f?)
q q-1
=f (Xk) - K1-q (f (Xk) - f ?)
(52)
8Note that here for convenience, we fix the order in gradient dominance as q, while previous discussion
assumes that q > p.
19
Under review as a conference paper at ICLR 2022
so subtract both sides by f? , we have
f (xk+ι) - f? ≤(1 - K1-q) (f (xk) - f?),	(53)
which verifies the conclusion by recursion.
Proof of q-SGF: Similarly, for the q-SGF case, we have
f (xk+l) ≤ f(xk) + Wf (Xk),Xk+1 - Xki + q ∣∣Xk+1 - Xkk2
,	.	__ ,	.	.	.― '.一 LYq n 2______ ,	. .. -‰
=f(xk) - Y l∣Vf (Xk )kq-h▽/(Xk), sign(Vf (Xk ))i + —q— ∣∣Vf (Xk)∣q-
q
=f (Xk) - (Y- nɪ^)∣Vf(Xk)k卢	(54)
≤ f(Xk) - T(n2L) 1-q∣Vf(Xk)k卢
≤ f (Xk) - q—1 (n2L)1-q-jπμq-1 (f (Xk) - f?),
q	q - 1
so we have
f (Xk+ι) - f? ≤(1 - (n2K)1-q)(f (Xk) - f?).	(55)
By the inequality e-x ≥ 1 - X, we can get that the complexity of the above algorithm with q-RGF
and q-SGF are
O 卜 q-1 ln —) and O ((n2 κ) q 1 ln —),	(56)
which verifies the conclusion.
F PROOF OF STOCHASTIC q-RGF (THEOREM 4)
Proof. Following the definitions of smoothness, we have
f(Xk+l) ≤ f (Xk) + hVf(Xk ),Xk+1 - Xk i + L ∣∣Xk+1 - Xk ∣∣2
=f (Xk)—Yk /Vf(Xk)—g(Xk)+g(Xk),-g(Xk L ∖+
∖	∣g(Xk )k 尸 /
LYq kg(Xk)k2
f(Xk) -
≤ f(Xk) -
f(Xk) -
kg(Xk)kψ + Yk I q
∣g(Xk)∣2ψ - Yk Vf(Xk) - g(Xk),
g(Xk)
q-2
∣g(Xk)k尸
q-2
q ∣g(Xk)k尸
g(Xk)	∖
,	q-2 /
∣g(Xk)k尸 /
q
+ ψ ∣Vf(Xk)-g(Xk)kψ
2
∣g(Xk)kψ + γk∣Vf(Xk) - g(Xk)kψ,
(57)
where the second inequality comes from the Young’s inequality. Then note that by Jensen’s inequality,
1 ψ
∣Vf (Xk)kψ = 2ψ G(Vf(Xk) - g(Xk) + g(Xk))	≤ 2ψ-1 (kg(Xk)kψ + kVf (Xk) - g(Xk)kψ)
-∣g(Xk)∣2ψ ≤ -21-ψ∣Vf(Xk)∣2ψ + ∣Vf(Xk) - g(Xk)∣2ψ
(58)
20
Under review as a conference paper at ICLR 2022
note that the setting of Yk will ensure the coefficient Ofkg(Xk)∣∣ψ to be negative, so we have
f(xk+ι) ≤ f(xk) - (γk -胃)(21-ψ kv∕ (Xk )kψ -kv∕( Xk) - g(Xk )kψ) + γk kv∕ (Xk) - g(Xk )kψ
=f (Xk)- 21-ψ (-ψk	qk) ι∣vf(χk )kψ + (-k-	qk) ι∣vf(χk)- g(Xk )kψ,
(59)
then take expectation on both sides, by the bounded variance and gradient dominance, we have
Ef (x-+i) ≤ E f (x-) - 2 ≤ E f (x-) - 2	Li (Yk - 手)皿由)kψ + (等 - W)(b⅛)] Lj 但--)物力(f (χk) - f ?) + (空-—)(J )21
∖ ψ	q √ so subtract both sides by f ?, we have E[f (Xk+1) - f *] ≤ E ] (1 - 21-ψ (Yk - LYq)p4U set Yk ≡ (pL)1-q, b(χk) ≡ b2 = (2<2")：*1-q ) an K	「(2ψ)ψ 1 K =———log 2κ E we have E[f (XK) - f*] ≤ e[ (1- -2-K⅛ ) (f (x-)-	∖ ψ	q √ ∖o(x-)) (60) ψ -∣ ɪ )(f (Xk) - f *)+(2? - ?(总)2 ] (61) d 亍)1， f * )+J⅛ψL 士 一
L ∖	(2ψ)，	) ≤ e"(i	-	2 Kι-q)	∆	+ ≤ R	(2ψ)ψ J	+ ≤ e[(1	-	—2— K 1⅛)	∆	+ ≤	(2ψ)ψ J	+ =e[(1 - -ɪɪKτ⅛)K∆ + 代(2ψ)ψ	) =e[(1 -	κ1⅛ ) ∆ + 代(2ψ)ψ	) E「	/	2K 1-q △ ʌ	(2 ≤ E exp - -——- K ∆ + — 一 [飞(2ψ)ψ ) + €	€ ≤ 2 + 2= €, while the total sample complexity is (note that ψψ + K. b2 ≥" log (竺).(2 ∙(2σ)ψ “ 士' 2k E	∖ € √ \	€	) which verifies the conclusion.	ψ* b* 2σψ r ɪ K 乙	2 ɪ、丁 ψψbψ 1 ,	( - (2ψ)ψ"1 J 2σψ , ɪ	1 	L ι - q	 ψψ bψ	1 - (1 -喜 K 土)] 2σψ T ɪ (2ψ)ψ ɪl	(62) l.iL 1-q  	KqT ψψ bψ	2 1 (2σ)ψ μ E b 1 σ)ψ μ E ~~bψ ɪ = 1, q ∈ (1, 2], which implies ψ ∈ [2, +∞)) 2 I = θ(σ2Kq-ιμ-2ψψe-ψ log∆),	(63) ■
Under review as a conference paper at ICLR 2022
G PROOF OF STOCHASTIC q-SGF (THEOREM 5)
Proof. Note that, as mentioned in Assumption 4 and 5, here the gradient dominance and Lipschitz
smoothness is defined under ('ι, '∞)-norms. Following the definitions of smoothness, We have (note
that ψ = q-τ)
f(xk+ι) ≤ f(Xk) + hVf(χk),Xk+ι - Xki + q∣∣Xk+ι - Xk∣∣∞
,.	......-ɪ — ,	.	...... LYq.............-ɪ	..	.....C
=f (Xk) - Yk l∣g(xk )kq Wf (Xk), sign (g(Xk ))i + -qk ∣∣g(xk)∣q ∙ ∣∣sign (g(xk ))∣∞
，	、	,,，	、，,，F 一 ，	、	，，	、、、 Lγq ,,	， s ,, ɪr
=f(Xk) - Ykl∣g(Xk)kq- hVf(Xk),Sign(g(Xk))i + -qk∣∣g(Xk)∣q-,
(64)
conditioning on Xk, we have
E[f (Xk+1) |Xk]
≤ f(Xk) - E YkIIg(Xk)Ilq-1 hVf(Xk),sign(g(Xk))i | Xk + E]LqkIlg(Xk)∣q-1 |Xk ,	侬)
note that for each component of g(Xk), we have
1
E ∣g(Xk)∣qτ sign (gi(Xk)) | Xk
1
E ∣∣g(Xk)∣q-1 P(Sign(Vif(Xk)) =sign(gi(Xk)))Sign(Vif(Xk))
(66)
1
-∣g(Xk)∣q-1 P(sign(Vif (Xk)) = sign(gi(Xk))) Sign(Vif (Xk)) | Xk
-	1	-
E ∣∣g(Xk)∣q-1 (2P(Sign(Vif(Xk)) = sign (gi(Xk))) - 1) Sign(Vif(Xk)) | Xk ,
then with the SPB condition in Assumption 5, we have
P(Sign(Vif(Xk)) = Sign (gi(Xk))) ≥ P* > 1,
so we have
E[f (Xk+1) |Xk]
_「Ly?
≤ f (Xk) - E Yk IIg(Xk)Ilq	(2p - 1)hSign( Vf(Xk)), Vf(Xk ))| Xk + El -qk Ilg(Xk )∣q	| Xk
,、_「....................上 ...........]_「Ly?..............ɪ .]
≤ f(Xk) -E Yk(2p - i)∣g(Xk)∣∣q IIVf(Xk)∣∣ι |Xk + E -qkIg(Xk)Iiq	|Xk
≤ f(Xk) - Yk(2p* - 1)∣Vf(Xk)∣卢 + LYqEhkg(Xk)∣卢 | Xki
(67)
1
where the last inequality comes from the convexity of ∣∙∣ q-1. Then for the last term in the RHS, by
decomposition, we have
1	ψ
∣g(Xk)∣ψ = 2ψ 2(Vg(Xk) - f (Xk) + f (Xk))	≤ 2ψτ (∣∣f (Xk)∣ψ + IVf(Xk) - g(Xk)∣ψ),
1	(68)
22
Under review as a conference paper at ICLR 2022
so We have (recall that ψ = q--i)
E[f(xk+1) | xk]
≤ f(xk)-Yk (2p* - 1)kVf (xk )k 卢 + LLY2； 1 Ehkf(Xk 川? + 1~/伏k)—。相® 川? ∣ Xk ]
≤ f (Xk) — ((2P* - 1)Yk- LYklψ21) kVf (xk)k卢 + L^ (高)2
/	LYq2ψT	q-1	Lγq2ψ-1 / σ2 \ 2
≤f(Xk)- ((2p*- I)Yk- Lkr-) T-TSXk) - Q + Lkr-1 ,
(69)
so take
Yk ≡ (Lψ-J) “， b(Xk) ≡ b2 = (2μL! (2p*- l)-4σ2e-卷,
and
K =卜2p* - 1)-ψ (κ2ψ-1)q-1 ln 2∆ ,
We have
E[f(XK) - f?]
1	---1 ∖	/ T ψ— - 1 ) 1 — q
≤ El (1 - ψ2p* - 1)ψ (L2ψτ 尸 % f(X (XK-1) - f ?) + T2p7⅛ψ
2 + 2= e,
(70)
Which concludes the proof9.
Remark 6. Similar results can be derived in the case of single sample (trivial batch size), hoWever,
some of them Will require additional assumptions like bounded gradient norm and vanishing stepsize
(Karimi et al. (2016), Theorem 4), but here We do not need this strong assumption of gradient
boundedness, and We alWays use a constant stepsize; these are the benefits brought by ‘non-trivial’
batch-size, i.e., neither one sample nor the full set of data.
Furthermore, in that case, the final complexity Will be generally O(1/) (Karimi et al. (2016),
Theorem 4), Whereas here our result is a linear convergence iteration complexity (i.e., O(log (1/)))
to achieve the -neighborhood (While the final sample complexities of tWo paradigms should be the
same), Which is much faster. So there Will be a tradeoff in different parameter settings.
Lastly, We Want to mention that batch size With dependence on is pretty common (or even required)
in stochastic optimization problems, e.g., Ghadimi et al. (2016) and Fang et al. (2018). Therefore, We
believe that using a batch size With dependence on is a reasonable option.
9Different from stochastic RGF, here We do not discuss the sample complexity, because the extra SPB
assumption is defined on the “oracle”, i.e., the mini-batch estimator g(x), so the concern here should be the
oracle complexity instead, Which Will be the iteration number K here.
23
Under review as a conference paper at ICLR 2022
H	Additional Experiment Details and Numerical results
In this section, we will expand upon the numerical results experiments discussed in the paper. In
particular, we report more details on the hyper-parameters values used10 for the numerical tests, and
report some results for GPU implementation of SVHN experiments.
H.1 Hyper parameters values used in the tests of Section 4.1- Example 2
•	GD fixed step size: η = 10-3
•	RGF Euler disc.	w/fixed step size:	q =	2.2,	η = 10-3
•	RGF Euler disc.	w/fixed step size:	q =	3, η	= 10-2
•	RGF Euler disc.	w/fixed step size:	q =	6, η	= 10-2
•	RGF Euler disc.	w/fixed step size:	q =	10,	η = 10-2
H.2 Hyper parameters values used in the tests of Section 4.2
Note that the description of the coefficients for each of the prior art methods can be found in:
https://pytorch.org/docs/stable/optim.html.
•	GD: η = 4.10-2, μ = 0.9, Nesterov=False
•	RGF: η = 4.10-2
•	SGF: η = 4.10-3
•	ADAM: η = 8.10-4 (remaining coefficients=nominal values)
•	RMS: η = 10-3 (remaining coefficients=nominal values)
•	ADAGRAD: η = 10-3 (remaining coefficients=nominal values)
•	ADADELTA: η = 4.10-2, ρ = 0.9, = 10-6, weight decay = 0
Remark 7. Choice of q: The settling time upper bound (15) decreases as q → ∞, which appears to
lead to faster convergence when discretized. On the other hand, the larger q is, the stiffer the ODE, so
more prone to numerical instability, so q cannot be too large. Therefore, assuming p to be not too
large, it appears that q ∈ (p,p + δ] works best, with δ > 0 as small as needed to avoid numerical
issues. For instance, if we know the cost function to be strongly convex (locally), then we search for
q slightly larger than p = 2 at first, but continue to increase until performance deteriorates. If, on
the other hand, we don’t know the order p > 1, then it’s currently unclear how to choose q. We will
investigate this further in future work. Furthermore, there is evidence that gradient dominance does
hold locally in many deep learning contexts (Zhou and Liang, 2017, https://arxiv.org/abs/1710.06910).
Indeed, since convexity readily leads to gradient dominance of order p = ∞, it suffices that a slightly
stronger form of it holds (but weaker than strong convexity), in order to have p < ∞, and thus for us
to be able to choose q > p.
H.3 Experiment 6: GPU implementation
To check if the numerical results obtained on CPU, more specifically the acceleration trends, hold
true on a GPU, we run some extra tests on a Titan X nvidia gpu with 12GB memory, the results are
reported below.
We first tested the performance of Euler discretization of the proposed flows against Adam, and GD
algorithms on SVHN dataset. We tested the proposed algorithms to train the VGG16 CNN model
with cross entropy loss. We divided the dataset into a training set of 74 batches with 1000 images
each, and a test set of 27 batches of 1000 images each, and ran 20 epochs of training over all the
training batches. We tested Euler discretization of q-RGF (c = 1, q = 2.1, η = 0.04 ), and Euler
discretization of q-SGF (c = 10-3, q = 2.1, η = 0.04 ) against GD (η = 0.1) and Adam (same
optimal tuning as in Section 4.2).
In Figures 5 , 6 we can see that both algorithms, Euler q-RGF and Euler q-SGF, converge faster than
GD and Adam for these tests, and reach the same performance on the test-set.
10In all the tests, for q-RGF and q-SGF c = 1 unless otherwise stated.
24
Under review as a conference paper at ICLR 2022
Figure 5: Losses for several optimization algorithms run on GPU- VGG16-SVHN: Train loss (left),
test loss (right)
Figure 6: Training loss vs. computation time for several optimization algorithms run on GPU-
VGG16-SVHN
25