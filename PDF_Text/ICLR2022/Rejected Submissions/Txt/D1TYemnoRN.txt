Under review as a conference paper at ICLR 2022
Short optimization paths lead to good gener-
ALIZATION
Anonymous authors
Paper under double-blind review
Ab stract
Optimization and generalization are two essential aspects of machine learning. In
this paper, we propose a framework to connect optimization with generalization by
analyzing the generalization error based on the length of optimization trajectory
under the gradient flow algorithm after convergence. Through our approach, we
show that, with a proper initialization, gradient flow converges following a short
path with an explicit length estimate. Such an estimate induces a length-based
generalization bound, showing that short optimization paths after convergence
indicate good generalization. Our framework can be applied to broad settings. For
example, we use it to obtain generalization estimates on three distinct machine
learning models: underdetermined `p linear regression, kernel regression, and
overparameterized two-layer ReLU neural networks.
1	Introduction
From the perspective of statistical learning theory, the goal of machine learning is to find a predictive
function that can give accurate predictions on new data. For supervised learning problems, empirical
risk minimization (ERM) is a common practice to achieve this goal. The idea of ERM is to minimize
a cost function on observed data by an optimization algorithm. Therefore, a fundamental question is
whether an optimization algorithm produces a solution with good generalization.
Recent works have shed light on providing theoretical explanations to this question from different
angles. One line of works considered the case when gradient methods converge to minimal norm
solutions (Bartlett et al., 2020; Tsigler & Bartlett, 2020; Liang & Rakhlin, 2020; Liang et al., 2020)
on kernel regression, and then analyzed the generalization of those minimal norm solutions. However,
the phenomenon of norm minimization has been known to happen only for the quadratic loss with an
appropriate initialization. Another line of works focused on overparameterized models, e.g., neural
networks under the Neural Tangent Kernel (NTK) regime (Allen-Zhu et al., 2019; Arora et al., 2019;
Cao & Gu, 2020; Ji & Telgarsky, 2020; Chen et al., 2021), proving that overparameterized neural
networks trained by (stochastic) gradient descent ((S)GD) have good generalization performance on
certain target functions (for example, polynomial functions).
Although existing works have made significant progress on the interplay of optimization and gen-
eralization, they focused on studying specific models, such as the NTK and models possessing
minimal norm solutions. In this paper, instead, we study general loss function conditions that induce
interesting connections between optimization and generalization.
We start with a simple observation, as shown in Figure 1, that under a generic random initialization,
the generalization performance for both linear regression model and random feature regression model
is closely related to the length of the optimization path1 after convergence. In particular, short
optimization path are associated with good generalization. Here, by length we mean the trajectory
or path length of the parameter evolution during training. This is not the “length of time” used for
training, as is usually analyzed in early-stopping type of algorithms. Thus, the generalization error
concerns the weights of model trained to completion by empirical risk minimization. This empirical
investigation motivates us to use the trajectory length to connect optimization and generalization.
Intuitively, the length of the optimization path can be viewed as a kind of capacity control, and a
1For discrete iterations, we use the sum of the distance between every two consecutive iterations to represent
the length of the optimization path (See details in Appendix A.1).
1
Under review as a conference paper at ICLR 2022
Figure 1: Illustrations of the relationship between optimization path lengths and generalization. (Left)
Linear regression model. (Right) Random feature regression model with the feature extracted by a
neural network. We train both models by gradient descent under GauSSian initialization N(μ, σ2) With
varied μ and σ, and record the trajectory lengths len(w(0), ∞) from initialization until convergence.
We observe that short optimization paths lead to good generalization. See experiment settings and
more numerical results in Appendix A.1 & A.2.
short path signifies low “complexity”. In other words, a general condition that guarantees a short
optimization path can be used to induce good generalization performance. Thus, inspired by the
theory in (Bolte et al., 2007) that Lojasiewicz gradient inequality (LGI) induces an explicit bound
for the gradient flow path, we consider Uniform-LGI (Definition 1). This is a modified version of
LGI and plays a critical role in obtaining a length estimate for the gradient flow trajectory. Once the
length is estimated, we can use our length-based generalization bounds to show the generalization
performance.
Contributions. We summarize our contributions as follows:
•	We focus on the gradient flow algorithm and propose a framework for combining optimization and
generalization. This framework is based on the length of the gradient flow trajectory, and its key
component is the Uniform-LGI property.
•	We then prove that under appropriate conditions, gradient flow returns a global minimum and gives
an explicit length-estimate for the gradient flow trajectory (Theorem 1). If this length-estimate has
an uniform bound for a certain initialization method, then we can give a length-based generalization
bound (Theorem 2).
•	We further show applications of Theorem 1 and Theorem 2 to obtain generalization bounds on
underdetermined `p linear regression (Theorem 3), kernel regression (Theorem 4), and overparame-
terized two-layer ReLU neural networks (Theorem 5). These bounds match or expand the type of
scenarios where we can rigorously establish the phenomenon of benign overfitting.
2	Related Works
Optimization. Theoretically analyzing the training process of most machine learning models is
a challenging problem as the loss landscapes are highly non-convex. One approach to studying
non-convex optimization problems is to use the Polyak-Lojasiewicz (PL) condition (Polyak, 1963),
which characterizes the local geometry of loss landscapes and ensures the existence of global minima.
It is shown in (Karimi et al., 2016) that GD admits linear convergence for a class of optimization
objective functions under the PL condition. In this paper, we consider the Lojasiewicz gradient
inequality (Lojasiewicz, 1965), an extended version of the PL condition that can be applied to more
cases. In particular, we modify the original LGI to get length estimates of the optimization paths,
which will be used to derive length-based generalization bounds.
Generalization. Traditional VC dimension-based generalization bounds depend on the number
of parameters and will be vacuous for huge models such as overparameterized neural networks. To
overcome this limitation, several non-vacuous generalization bounds are proposed. For example, the
norm/margin-based generalization bounds (Neyshabur et al., 2015; Bartlett et al., 2017; Golowich
2
Under review as a conference paper at ICLR 2022
et al., 2018; Neyshabur et al., 2019), and the PAC-Bayes-based bounds (Dziugaite & Roy, 2017;
Neyshabur et al., 2018; Zhou et al., 2019; Rivasplata et al., 2020). However, these bounds tend to
focus less on optimization, e.g., norm-based generalization bounds may not discuss how small-norm
solutions are obtained through practical training. In this paper, we connect the optimization and
generalization by deriving generalization bounds based on the optimization trajectory length.
Interface between optimization and generalization. Implicit bias builds the bridge between
optimization and generalization, which has been widely studied to explain the generalization ability
of machine learning models. Recent works (Soudry et al., 2018a;b; Nacson et al., 2019a;b; Lyu
& Li, 2020) showed that linear classifiers or deep neural networks trained by GD/SGD maximizes
the margin of the separating hyperplanes and therefore generalizes well. Other works (Arora et al.,
2019; Zou & Gu, 2019; Cao & Gu, 2020; Ji & Telgarsky, 2020; Chen et al., 2021) concentrated on
overparameterized neural networks, showing that GD/SGD trajectories fall within the NTK regime,
where the minimizer has good generalization due to the small “complexity” of the parameter space. In
this work, we focus on specific conditions on loss functions under which we can connect optimization
and generalization based on the lengths of optimization paths.
3	Main results
In this section, we aim to address optimization and generalization together by proving results based
on the Uniform-LGI property. We begin with describing notations and the problem setting in Section
3.1. Then we give an explicit length estimate for the gradient flow trajectory in Section 3.2. Lastly,
we show a length-based generalization bound in Section 3.3.
3.1	Setup and Notations
Consider a supervised learning problem on a hypothesis space F = {f (w, ∙) : Rd → R | W ∈ W},
where W is a parameter set in Euclidean space. Given a loss function ` : R × R -→ R, and a training
set S = {(xi, yi)}in=1 ⊆ Rd × R withn independent and identically distributed (i.i.d.) samples from
a joint distribution D, the goal of ERM is to optimize the empirical loss function Ln(w) on S:
wn
1n
argmin Ln(W) = n£'(f (w,xi),yi).
We assume that each input vector and label have bounded norms. Specifically, we assume that, for
any (x, y) following D, kxk = 1, |y| ≤ 1. This can be achieved by data normalization. To simplify
the analysis, we optimize the empirical loss by gradient flow2:
-▽Ln (W(t)), t ∈ [0, +∞),
(1)
where W(t) is the parameter value at time t, W(0) is the initialization parameter set, and ▽Ln (W) is
the gradient of Ln(W) with respect to W. Given the gradient flow, we may define the length of the
gradient flow curve from W(O) to w(t) as len(w(0),t) := R0j Il dw∕ Il ds.
Notations. We use ∣∣∙∣∣ to denote the '2 norm of a vector or the spectral norm of a matrix, and use
k∙k f to denote the Fronenius norm of a matrix. For a set S ∈ Rn, we use ∂S to denote its boundary.
We use d(S1, S2) to represent the Euclidean distance between two sets S1, S2, which is defined as
d(S1, S2) = {inf ks1 - s2 k : s1 ∈ S1, s2 ∈ S2}. For two vectors, we use h, i to denote their inner
product. Let λmin(A) and λmax(A) be the smallest and largest eigenvalues of a symmetric matrix A.
For any positive integer n, we denote [n] = {1,2,..., n}. We use O(∙) to stand for Big-O notation.
The classical LGI gives a lower bound of the gradient of a differentiable function based on its value
above its minimum. Many functions, e.g., real analytic functions and subanalytic functions, satisfy
this property, at least locally (Bolte et al., 2007). Here, we require a global version of this inequality
as a condition to control the gradient flow trajectory. Let us define this notion below.
Definition 1 (Uniform-LGI). A function L(W) satisfies Uniform-LGI on a set S with two constants
c > 0 and θ ∈ [ 1, 1), if kVL(w)k ≥ C (L(W) — mi□v∈w L(Vyf , ∀w ∈ S.
2Here we assume the existence of the gradient flow and the limit limt-→∞ w(t) .
3
Under review as a conference paper at ICLR 2022
In the special case when θ = 1/2 and C = √2μ, the Uniform-LGI corresponds to the μ-PL condition
(Karimi et al., 2016). Here we give examples of functions satisfying the Uniform-LGI but not the PL
condition: L(w) = w2k , k ∈ Z+ . When k ≥ 2, L(w) satisfies the Uniform-LGI on R with c = 2k
and θ = 1 - 1/2k.
3.2	Gradient flow trajectory length estimate
Our first result shows that under the uniform-LGI condition, gradient flow returns a global minimum
with an explicit estimate of the gradient flow trajectory length as in the following theorem.
Theorem 1. For any initialization w(0), if Ln(w) satisfies Uniform-LGI on a closed set Sn with
constants cn, θn such that Sn ⊇ B w(0), rn(w(0)) , where
/⑼、(Ln(We))- minw Ln(W)) 1-θn
Tr (w)= ---Q—诟--------，	⑵
then W(t) converges to a global minimum W(∞) with the trajectory length upper bounded by
len(W(0), ∞) ≤ rn(W(0)). The convergence rate of Ln(W(t)) is given by:
θn = 1 ：	Ln(w(t)) - minLn(W) ≤ e-cnt(Ln(w(0)) - minLn(W)),
2w	w
1 <θn < 1 ： Ln(w(t)) - min Ln(W) ≤ (1 + Mnt)T/(2θnT)(Ln(w(0)) - min Ln(W)),
2w	w
where Mn = Cn(20n - 1) (Ln(W(0)) - minw Ln(W))2θn 1
The proof of Theorem 1 is given in Appendix B.
This theorem yields that once the loss function satisfies the Uniform-LGI around the initialization,
gradient flow returns a global minimum with an explicit trajectory length estimate. Moreover, for
any fixed n, len(W(0), ∞) decreases as the initial loss value decreases. The calculation of cn, θn for
different n should be analyzed case by case based on the loss landscape, as shown in Section 4.
3.3 How does the optimization path length affect generalization ?
The Uniform-LGI property allows us to have an upper bound for the gradient flow trajectory length.
In this subsection, we use the length estimate we get in Theorem 1 to study generalization.
Throughout this subsection, we assume that there exists an almost everywhere differentiable function
Ψ : Rp+q → R such that f (w, ∙) can be represented in the following form,
∀x ∈ Rd, f(W, x) = Ψ α1> x, . . . , αp>x, β1, . . . , βq	(3)
with α1 , . . . , αp ∈ Rd, β1, . . . , βq ∈ R, and W = vec ({α1, . . . , αp, β1, . . . , βq}) ∈ Rpd+q. Here
vec is the vectorization operator that concatenates all elements into a column vector.
A wide class of functions can be represented in the form (3). Examples include linear functions
f(W, x) = W>x and two-layer neural networks f(W, x) = a>φ (Wx) with a ∈ Rm, W ∈ Rm×d.
Additional notations. For the loss function ', we use L'(S) to denote its Lipschitz constant (the
maximal gradient norm) on S with respect to its first argument. For Ψ in (3), we define LΨ (S) ：=
(lΨP(S ),∙∙∙ ,L,)(S), LψP+1)(S), ∙∙∙ , Lψ+q)(S)) , where Lψ)(S) is the Lipschitz constant of
Ψ on S with respect to the i-th variable. Let W(0) ：= vec α(10), . . . , αp(0), β1(0), . . . , βq(0) and
use LD(w) to denote the expected loss E(χ,y)〜D ['(f(W,x),y)]. For a = (aι,...,ap)> and
b = (bι,∙∙∙,bq)>, we define S0,b = {w : ∀i ∈ [p],j ∈ [q], |向|| ≤ ai, ∣βj| ≤ bj}, and Ma,b =
suPw∈Sa,b,kxk≤1,∣y∣≤1 ' (f(W, x), y).
In the following theorem, we give a path-based generalization bound if one has a length estimate
for optimization trajectory length len(W(0), ∞). Based on the Uniform-LGI property, Theorem 1
serves as a sufficient condition to ensure that the trajectory length can be estimated. In that sense, we
connect optimization with generalization. The rigorous statement is as follows:
4
Under review as a conference paper at ICLR 2022
Theorem 2. Consider an initialization method so that w(0) is independent of the training samples and
suppose that for any δ ∈ (0, 1), there exist Mδ, Rn,δ > 0 such that w(0) ≤ Mδ, len(w(0), ∞) ≤
Rn,δ with probability at least 1 - δ over the initialization and the training samples. Then, we have
with probability at least 1 - δ over initialization and the training samples, the generalization error of
the global minimum w(∞) is bounded as:
「( (∞)	2RL'(Sa,b) kLψ(Sa,b)k	∕3(p + q)+log(2∕δ)
LD(w(∞)) ≤ minLn(w)+	SUP --------------ʒ=------------+3Ma,b∖ ---------z----------,
w	kak2+kbk2≤R2	n	2n
(4)
where R = √2(Mδ + Rn,δ).
The proof of Theorem 2 is given in Appendix C. There are five key terms in (4). To make the bound
easier to understand, we discuss them in sequence:
•	Mδ: This is a high probability upper bound for the '2 norm of the initialized vector ∣∣w(0) ∣∣. In
practice, for commonly used initialization methods in deep learning, such as Xavier initialization
(Glorot & Bengio, 2010) and Kaiming initialization (He et al., 2015), the norm ∣w(0) ∣ is uniformly
bounded with high probability.
•	Rn,δ : This is an high probability upper bound for the optimization path length over the initialization
and training samples. Theorem 2 assumes that such a bound exists, and show that it immediately
implies a generalization estimate based on this bound. Theorem 1 gives a sufficient condition to
obtain such a path estimate, in which case it depends on the Uniform-LGI constants cn , θn and
data distribution D. The asymptotic analysis of Rn,δ is problem-dependent, and we provide several
examples in Section 4.
•	l`(Sα,b) and Ma,b: They depend on the loss function '. For example, for any loss function
' :R X R → [0,1] that is 1-Lipschitz in the first argument, L'(Sa,b) = Ma,b = 1.
•	kLΨ(Sa,b)k: This term relies on the structure of f. For example, when f is linear, Ψ(x, y) = x+y,
then ∣∣Lψ(Sa,b)k = √2.
Remark 1. Theorem 2 shows that if the optimization trajectory length is small, then this implies a
good generalization bound. First, note that our generalization bound concerns the weights of the
final model. Nevertheless, by the convergence analysis in Theorem 1, our framework can be extended
to derive generalization estimates that evolves according the length of time of training. See Appendix
C.1 for additional results. Second, we emphasize again that the generalization bound in Theorem
2 only relies on a path length estimate, and theorem 1 gives a sufficient condition to ensure that
such an estimate exists. Consequently, as long as one can obtain path length estimates (say from
stochastic analysis of SGD), one can still apply Theorem 2 to obtain generalization estimates. Path
length estimates for other types of training algorithms is an interesting future direction.
4	Applications
In this section, we apply the framework to three models. To obtain clean expressions of the gener-
alization bound in terms of the sample size n, it is helpful to consider a range of n relating to the
dimension d. In particular, we consider underdetermined systems where the ratio n∕d remains finite
unless stated otherwise:
∃ γ0, γ1 ∈ (0, ∞), s.t. ∀d, γ0d ≤ n = n(d) ≤ γ1d.
Note that this setting is non-asymptotic since we do not require d to be infinite. For each application,
to simplify the analysis, we evaluate the test error on a loss function ` : R × R -→ [0, 1] is 1-Lipschitz
(on the first argument) with '(y, y) = 0. The following steps are taken in our framework:
Step 1. Establish the Uniform-LGI property and find the Uniform-LGI constants cn and θn .
Step 2. Apply Theorem 1 to get optimization results to estimate path length.
Step 3. Apply Theorem 2 to get generalization results from the path length estimates.
5
Under review as a conference paper at ICLR 2022
4.1	UNDERDETERMINED `p LINEAR REGRESSION
We begin with an underdetermined linear regression model f(w, x) = w>x with an `p loss function
(p ≥ 2 is an even positive integer):
1n	p
arg min Ln(W) := 一 ɪ2 (w>xi - yi p ,	(5)
where the input data matrix X = (x1, . . . , xn)> ∈ Rn×d has full row rank. Then the above regression
model has at least one global minimum with zero loss.
Target function. Suppose the training data is generated from an underlying function g : Rd -→ R
with yi = g(xi), ∀i ∈ [n]. Let Y = (yι,…,yn)>, and assume that there exits C > 0 such that
kYk ≤ C*Jλmaχ(XX T) .	(6)
The inequality (6) actually indicates that g is Lipschitz with a dimension independent Lipschitz
constant. Functions satisfying (6) include linear/non-linear functions. For example, g(χ) = φ(χτw*),
where w* ∈ Rd with ∣∣w* |卜 ≤ C for some constant c*, and φ(∙) is Lipschitz with φ(0) = 0.
Assumption 1. The entries of X are i.i.d. subgaussian random variables with zero mean, unit
variance, and subgaussian moments3 bounded by 1.
This assumption allows us to study the spectral properties of the sample matrix by random matrix
theory. Especially, when n(d)/d converges to some constant Y ∈ (0,1), the Marchenko-Pastur law
(MarCenko & Pastur, 1967) shows that λmin(XXT)/n converges to (1 - √γ)2 almost surely. The
non-asymptotic results are provided in (Rudelson & Vershynin, 2010).
Performing our three-step analysis, we get the following results:
Theorem 3.	Consider the undertermined `p linear regression model (5). Suppose that there exists a
universal constant c0 ≥ 1 such that ∀d, w(0) 2 ≤ c0.
Step 1. Ln(w) satisfies the Uniform-LGI globally on Rd with
Cn = PITPr λmin (XXTl,	θn = 1 - 1/p.
n
Step 2. Ln(w(t)) converges to zero linearly forP = 2 and sublinearly forP ≥ 4, i.e.,
P = 2,	Ln(W((D ≤ exp (-2λmin(XXT)t/n) Ln(W(O));
P ≥ 4,	Ln(W⑴)≤ (1+ Mt)-pp2 Ln(w(0)),
where M = p1-P(p - 2)λmin(XX>)Ln(W(O))I-2.
Step 3. Under Assumption 1, for any target function that satisfies (6), we have with probability at
least 1 - δ - τ d-n+1 - τd over the samples,
E(χ,y)〜D h《f(W(M,x),y)i ≤ O (n-1∕p) + O (rIogn/δ)),
where τ ∈ (0, 1) depends only on the subgaussian moment of the entries.
The proof of Theorem 3 is given in Appendix D.1. This theorem shows that compared to the PL
condition that corresponds to P = 2, the uniform-LGI is more general and can be applied to more
cases.
Comparison. This result is related to (Bartlett et al., 2020) that studied the phenomenon of benign
overfitting in high-dimensional `2 linear regression. Our result coincides with theirs as both results
uncover some scenarios for benign overfitting in linear regression. In particular, (Bartlett et al.,
3The subgaussian moment of X is defined as inf M ≥ 0 | EetX ≤ eM2t2/2 , ∀t ∈ R .
6
Under review as a conference paper at ICLR 2022
2020) focused on the minimum `2 norm estimator. They showed that, if the eigenvalue sequence of
the covariance operator Σ := E[xx>] have suitable decay rates, then the generalization error will
decrease to zero as n increases. In contrast, our result differs from theirs in the problem settings.
Specifically, we do not assume the minimum norm property and consider the optimization process that
is neglected in (Bartlett et al., 2020). Also, we evaluate the generalization error of the convergence
point on a globally Lipschitz loss function. In our settings, entries of each input x are i.i.d. random
variables, meaning that Σ = Id×d without decaying eigenvalues. The requirement of i.i.d. entries
for each input is a limitation due to the lack of non-asymptotic results of the spectral properties for
random matrices with non-i.i.d. entries. However, this limitation can be overcome if one considers
the asymptotic results. Moreover, our result works for `p loss functions with any even positive integer
p. This expands the understanding of benign overfitting in the high-dimensional setting.
4.2 Kernel Regression
Consider a positive definite kernel k : X ×X → C with its corresponding feature map N :Rd -→ F
satisfying hN(x), N(y)iF = k(x, y). We assume that |k(x, x)| ≤ 1, ∀x ∈ X. Let H be the
reproducing kernel Hilbert space (RKHS) with respect to k. If F = Cs, then the kernel regression
model with `p loss is to solve the following problem
1n	p
argmin Ln(W) =工](w>^(xi)-y，’，
(7)
where p ≥ 2 is an even integer. Similar to the `p linear regression case, we consider the following
target function class:
Target function. Suppose the training data is generated from an underlying function g : Cd -→ C
with yi = g(xi), ∀i ∈ [n]. We further assume that there exits a constant C > 0 such that
kYk ≤ c* ∙ Pλmaχ(k(X, X)),	(8)
where k(X, X) is then × n kernel matrix on X with k(X, X)ij = k(xi, xj).
Here, we list an example of class of functions that satisfies (8): g(χ) = φ(q(χ)>w*) where w* ∈ Cs
With (VS) kw*k2 ≤ c* for some constant c*, and φ(∙) is Lipschitz with φ(0) = 0.
To get the generalization results of kernel regression, we will discuss two types of kernels separately:
radial basis function (RBF) kernel and inner product kernel.
RBF kernel. We study the RBF kernel of the form k(x, y) = % (ky - xk) for a certain RBF %. For
the input data, we define the separation distance of X as SD := 2 mini=j ∣∣Xi 一 Xjk, ∀i, j ∈ [n].
Inner product kernel. For the inner product kernel, we consider k(x, y) = % (Xdy).
Following (El Karoui et al., 2010), we make the following assumption on the function %:
Assumption 2. % is C3 in a neighborhood of 0 with %(0) = 0, %(1) > %0(0) ≥ 0, %00(0) ≥ 0.
Now we are ready to apply our three-step analysis to get optimization and generalization results. For
the RBF kernel, the generalization result depends on the separation distance of the samples. For the
inner product kernel, we study the high-dimensional random kernel matrix.
Theorem 4.	Consider the kernel regression model (7). Suppose that there exists a universal constant
c0 ≥ 1 such that ∀S, w(0) 2 ≤ c0.
Step 1. Ln(w) satisfies the Uniform-LGI globally on Cs with
Cn =pi-1/p r λmin(k(X, X)) , θn = 1 一 1/p,
n
where cn is controlled by the kernel and input samples.
Step 2. Ln(w(t)) converges to zero linearly forp = 2 and sublinearly forp ≥ 4, i.e.,
P = 2, Ln(W⑴)≤ exp (-2λmin(k(X, X))t/n) Ln (W⑼);
P ≥ 4, Ln(W ⑴)≤ (1+ Mtr pp2 Ln(W⑼),
7
Under review as a conference paper at ICLR 2022
where M = p1-2(P - 2) WknX,X)Ln(W(O))I-P.
Step 3. For any target function that satisfies (8) we have:
•	For the RBF kernel4, suppose that % : R≥0 -→ R≥0 is a decreasing function and % (kxk) ∈ L1(Rd).
If there exists two positive constants qmin and qmax such that SD ∈ [qmin , qmax] for all n, then
with probability at least 1 - δ over the samples,
E(χ,y)〜D h《f (w(∞),x),y)i ≤ O (n-1/P) + O r∣l0g(n^δ)
•	For the inner product kernel, under Assumption 1 andAssumption 2, if d is large enough and δ > 0
is small enough such that d-1/2 (√3δ-1/2 + log0.51 d) ≤ 0.5(%(1) — %0(0)), then with probability
at least 1 - δ - d-2 over the samples, we have
E(χ,y)〜D h《f(w(∞,x),y)i ≤ O (n-1∕p) + O (∕0g(pδ)
The proof of Theorem 4 is given in Appendix D.2.
Example 1. RBF kernels satisfying the conditions and assumptions in Theorem 4 include (1)
Gaussian: %(r) = e-ρr , ρ > 0; (2) (inverse) Multiquadrics: %(r) = (P + r2)β/2, ρ > 0,β ∈
R\2N, β < -d.
Inner product kernels satisfying the conditions and assumptions in Theorem 4 include (1) Polynomial
kernel: %(r) = rβ, β ∈ Z+, β ≥ 2; (2) NTK corresponding to Two-layer ReLU neural networks on
Sd-1(√d): %(r) = ga2∏cos(r)).
Comparison. The p = 2 result of the inner product kernel is related to (Liang & Rakhlin, 2020)
who derived generalization bounds for the minimum RKHS norm estimator. They showed that
when the data covariance matrix and the kernel matrix enjoy certain decay of the eigenvalues, the
generalization bound vanishes as n goes to infinity. For example, for exponential kernel and the
covariance matrix Σ := E[xx>] with the j-th eigenvalue λj (Σ) = j-α, the `2 generalization bound
α
becomes O(n- 2α+1) when α ∈ (0,1) and n > d. In comparison, We get an optimal generalization
bound O n-1/2 for identity covariance matrix on a globally Lipschitz loss function. Again, we
emphasize that the i.i.d. assumption can be relaxed if more new results of random matrix theory are
available. Further, we extend the works in (Liang & Rakhlin, 2020) by proving a new result of the
RBF kernel. Note that the result of the RBF kernel is not under the high-dimensional setting; thus it
is not a direct adaptation of (Liang & Rakhlin, 2020), and the proof itself is of independent interest.
4.3 Overparameterized two-layer Neural Networks
The first two applications are on traditional machine learning models. Indeed, our framework can be
applied not only to linear/kernel regression but also to neural network models. In this subsection, we
use our framework to study shallow neural networks in an overparameterization regime.
First, define a two-layer ReLU neural network under the NTK parameterization (Lee et al., 2019):
f(w,x) = √mw> Φ(Wιx),
where φ(x) = max{0, x}, x ∈ Rd is the input, W1 ∈ Rm×d, w2 ∈ Rm are parameters, w =
vec ({W1, w2}) ∈ Rm(d+1), andm is the width (number of hidden units).
We consider minimizing the quadratic loss by gradient flow:
1n
arg min Ln(W) ：= ʒ-ɪ^(f (w,xi) — yi)2.	(9)
w	2n
i=1
4Here d is fixed and n is varied.
8
Under review as a conference paper at ICLR 2022
Random initialization. W(O) is drawn from Gaussian N(0, dIm×d) and w20) are drawn i.i.d. from
uniform distribution U {-1, 1}, ∀i ∈ [m].
Following the setting in (Du et al., 2019), we only train the hidden layer W1 and leave the output
layer w2 as random initialization to simplify analyses.
NTK matrix. The NTK matrix Θ(t) is defined as: Θj(t) = (NWIf (w(t),xi), Nwιf(w(",Xj))，
and denote Θ by the limiting matrix5: Θ j = x>xjEw〜N(o, 1 ιd)[φ0(wτxi)φ0(wτxj)] , ∀i,j ∈ [n].
Similarly, we apply our three-step analysis to derive the following results.
Theorem 5. Consider the two-layer ReLU neural network model (9). For any δ ∈ (0, 1), if
m ≥ poly n, λ-m1in(Θb), δ-1 , then
Step 1. With probability at least 1 - δ over training samples and random initialization, Ln (w(t))
satisfies the Uniform-LGI on w(t) : t ≥ 0 with
Cn = λλmin(Θ)/n, θn = 1/2.
Step 2. Ln(w(t)) converges to zero with a linear convergence rate:
Ln(w(t)) ≤ exp (-λmin (θ)t/n) Ln(W⑼).
Step 3. Under Assumption 1, for any target function that satisfies (6), if γ1 ∈ (0, 1), then with
probability at least 1 - δ - τ d-n+1 - τd over the samples and random initialization,
E(χ,y)〜D pφ(w(M,χ),y)i ≤ O (J吗⑶),
where τ ∈ (0, 1) depends only on the subgaussian moment of the entries.
The proof of Theorem 5 is deferred to Appendix D.3.
Comparison. Our result is related to (Arora et al., 2019), which gave an NTK-based generalization
bound for overparameterized two-layer ReLU neural networks. This result matches with theirs in the
sense that we discover some underlying functions that are provably learnable. Examples of learnable
target functions in (Arora et al., 2019) include polynomials y = (βτx)p, non-linear activations
y = cos(βτx) 一 1, y = φ(β>x) with φ(z) = Z ∙ arctan(z∕2), kβk ≤ 1. Our result, furthermore,
expands the target function class that is provably learnable since we only require φ to be Lipshcitz. In
addition, they set the standard deviation of the initialization to be at most O(1/n), whereas we use a
different initialization with order O(1∕√d) that is more often applied in practice. Our result is also
related to (Liu et al., 2020), which proved that overparameterized deep neural networks satisfy the
PL condition. Further, we extend this work by analyzing the generalization based on the length of
optimization trajectories.
5	Conclusion
In this work, we address when and why does an optimization algorithm finds a minimum with good
generalization performance. For this problem, we propose a framework to bridge optimization and
generalization based on the trajectory length. The pivotal component is the Uniform-LGI property:
a condition on the loss function, by which we show that gradient flow returns a global minimum
with an explicit length estimate. Further, we derive a length-based generalization bound given such a
length estimate. Finally, we apply the framework to three machine learning models with certain target
functions. By estimating the trajectory lengths, we get non-vacuous generalization bounds that do not
suffer from the curse of dimensionality. This framework is not a direct variant of the NTK method,
and the results show that our framework is favorable for inducing connections between optimization
and generalization.
5Here λmin (Θb) changes with n.
9
Under review as a conference paper at ICLR 2022
6	Reproducibility Statement
For the experiments, details about the models and the synthetic data are described in Appendix A.1
and Appendix A.2. For the theoretical results, we have included clear explanations of all assumptions.
The complete proofs are provided in Appendix B, C & D.
References
Zeyuan Allen-Zhu, Yuanzhi Li, and Yingyu Liang. Learning and generalization in overparameterized
neural networks, going beyond two layers. In Advances in neural information processing systems,
pp. 6158-6169, 2019.
Sanjeev Arora, Simon Du, Wei Hu, Zhiyuan Li, and Ruosong Wang. Fine-grained analysis of
optimization and generalization for overparameterized two-layer neural networks. In International
Conference on Machine Learning, pp. 322-332. PMLR, 2019.
Peter L Bartlett, Dylan J Foster, and Matus J Telgarsky. Spectrally-normalized margin bounds for
neural networks. In Advances in Neural Information Processing Systems, pp. 6240-6249, 2017.
Peter L Bartlett, Philip M Long, Ggbor Lugosi, and Alexander Tsigler. Benign overfitting in linear
regression. Proceedings of the National Academy of Sciences, 117(48):30063-30070, 2020.
J6r6me Bolte, Aris Daniilidis, and Adrian Lewis. The IOjaSieWicz inequality for nonsmooth subana-
lytic functions with applications to subgradient dynamical systems. SIAM Journal on Optimization,
17(4):1205-1223, 2007.
Yuan Cao and Quanquan Gu. Generalization error bounds of gradient descent for learning over-
parameterized deep relu networks. In Proceedings of the AAAI Conference on Artificial Intelligence,
volume 34, pp. 3349-3356, 2020.
Zixiang Chen, Yuan Cao, Difan Zou, and Quanquan Gu. How much over-parameterization is
sufficient to learn deep relu networks? In International Conference on Learning Representations,
2021.
Benedikt Diederichs and Armin Iske. Improved estimates for condition numbers of radial basis
function interpolation matrices. Journal of Approximation Theory, 238:38-51, 2019.
Simon S. Du, Xiyu Zhai, Barnabas Poczos, and Aarti Singh. Gradient descent provably optimizes
over-parameterized neural networks. In International Conference on Learning Representations,
2019.
Gintare Karolina Dziugaite and Daniel M. Roy. Computing nonvacuous generalization bounds for
deep (stochastic) neural networks with many more parameters than training data. In Proceedings
of the 33rd Annual Conference on Uncertainty in Artificial Intelligence (UAI), 2017.
Noureddine El Karoui et al. The spectrum of kernel random matrices. Annals of statistics, 38(1):
1-50, 2010.
Xavier Glorot and Yoshua Bengio. Understanding the difficulty of training deep feedforward neural
networks. In Proceedings of the Thirteenth International Conference on Artificial Intelligence and
Statistics, 2010.
Noah Golowich, Alexander Rakhlin, and Ohad Shamir. Size-independent sample complexity of
neural networks. In Conference On Learning Theory, pp. 297-299. PMLR, 2018.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Delving deep into rectifiers: Surpassing
human-level performance on imagenet classification. In Proceedings of the IEEE international
conference on computer vision, pp. 1026-1034, 2015.
Ziwei Ji and Matus Telgarsky. Polylogarithmic width suffices for gradient descent to achieve
arbitrarily small test error with shallow relu networks. In International Conference on Learning
Representations, 2020.
10
Under review as a conference paper at ICLR 2022
Hamed Karimi, Julie Nutini, and Mark Schmidt. Linear convergence of gradient and proximal-
gradient methods under the Polyak-IojasieWicz condition. In Joint European Conference on
Machine Learning and Knowledge Discovery in Databases, pp. 795-811. Springer, 2016.
Beatrice Laurent and Pascal Massart. AdaPtive estimation of a quadratic functional by model selection.
Annals of Statistics, pp. 1302-1338, 2000.
Michel Ledoux and Michel Talagrand. Probability in Banach Spaces: isoperimetry and processes.
Springer Science & Business Media, 2013.
Jaehoon Lee, Lechao Xiao, Samuel Schoenholz, Yasaman Bahri, Roman Novak, Jascha Sohl-
Dickstein, and Jeffrey Pennington. Wide neural netWorks of any depth evolve as linear models
under gradient descent. In Advances in Neural Information Processing Systems, volume 32, 2019.
Tengyuan Liang and Alexander Rakhlin. Just interpolate: Kernel “ridgeless” regression can generalize.
The Annals of Statistics, 48(3):1329-1347, 2020.
Tengyuan Liang, Alexander Rakhlin, and Xiyu Zhai. On the multiple descent of minimum-norm
interpolants and restricted loWer isometry of kernels. In Conference on Learning Theory, pp.
2683-2711. PMLR, 2020.
Chaoyue Liu, Libin Zhu, and Mikhail Belkin. Loss landscapes and optimization in over-parameterized
non-linear systems and neural netWorks. arXiv preprint arXiv:2003.00307, 2020.
S. LojasieWicz. Ensembles semi-analytiques. IHES notes, 1965.
Kaifeng Lyu and Jian Li. Gradient descent maximizes the margin of homogeneous neural netWorks.
In International Conference on Learning Representations, 2020.
V A Marcenko and L A Pastur. Distribution of eigenvalues for some sets of random matrices.
Mathematics of the USSR-Sbornik, 1(4):457-483, apr 1967.
Mehryar Mohri, Afshin Rostamizadeh, and Ameet TalWalkar. Foundations of machine learning. MIT
press, 2018.
Stephen J Montgomery-Smith. The distribution of rademacher sums. Proceedings of the American
Mathematical Society, 109(2):517-522, 1990.
Mor Shpigel Nacson, Suriya Gunasekar, Jason D Lee, Nathan Srebro, and Daniel Soudry. Lexico-
graphic and depth-sensitive margins in homogeneous and non-homogeneous deep models. arXiv
preprint arXiv:1905.07325, 2019a.
Mor Shpigel Nacson, Nathan Srebro, and Daniel Soudry. Stochastic gradient descent on separable
data: Exact convergence With a fixed learning rate. In The 22nd International Conference on
Artificial Intelligence and Statistics, pp. 3051-3059. PMLR, 2019b.
Behnam Neyshabur, Ryota Tomioka, and Nathan Srebro. Norm-based capacity control in neural
netWorks. In Conference on Learning Theory, pp. 1376-1401, 2015.
Behnam Neyshabur, Srinadh Bhojanapalli, and Nathan Srebro. A PAC-bayesian approach to
spectrally-normalized margin bounds for neural netWorks. In International Conference on Learning
Representations, 2018.
Behnam Neyshabur, Zhiyuan Li, Srinadh Bhojanapalli, Yann LeCun, and Nathan Srebro. The role
of over-parametrization in generalization of neural netWorks. In International Conference on
Learning Representations, 2019.
B. Polyak. Gradient methods for the minimisation of functionals. Ussr Computational Mathematics
and Mathematical Physics, 3:864-878, 1963.
Omar Rivasplata, Ilja Kuzborskij, Csaba Szepesvdri, and John Shawe-Taylor. Pac-bayes analysis
beyond the usual bounds. arXiv preprint arXiv:2006.13057, 2020.
11
Under review as a conference paper at ICLR 2022
Mark Rudelson and Roman Vershynin. Smallest singular value of a random rectangular matrix.
Communications on Pure and Applied Mathematics: A Journal Issued by the Courant Institute of
Mathematical Sciences, 62(12):1707-1739, 2009.
Mark Rudelson and Roman Vershynin. Non-asymptotic theory of random matrices: extreme singular
values. In Proceedings of the International Congress of Mathematicians 2010 (ICM 2010) (In 4
Volumes) Vol. I: Plenary Lectures and Ceremonies Vols. II-IV: Invited Lectures, pp. 1576-1602.
World Scientific, 2010.
Daniel Soudry, Elad Hoffer, Mor Shpigel Nacson, Suriya Gunasekar, and Nathan Srebro. The implicit
bias of gradient descent on separable data. The Journal of Machine Learning Research, 19(1):
2822-2878, 2018a.
Daniel Soudry, Elad Hoffer, and Nathan Srebro. The implicit bias of gradient descent on separable
data. In International Conference on Learning Representations, 2018b.
Alexander Tsigler and Peter L Bartlett. Benign overfitting in ridge regression. arXiv preprint
arXiv:2009.14286, 2020.
Holger Wendland. Scattered Data Approximation. Cambridge Monographs on Applied and Compu-
tational Mathematics. Cambridge University Press, 2004.
Wenda Zhou, Victor Veitch, Morgane Austern, Ryan P. Adams, and Peter Orbanz. Non-vacuous gen-
eralization bounds at the imagenet scale: a PAC-bayesian compression approach. In International
Conference on Learning Representations, 2019.
Difan Zou and Quanquan Gu. An improved analysis of training over-parameterized deep neural
networks. In Advances in Neural Information Processing Systems, 2019.
12
Under review as a conference paper at ICLR 2022
A Experiments
A.1 Experiments settings
In this subsection, we describe the details of our experiments in the introduction part (Figure 1).
Linear Regression model. The model that we use is underdetermined `2 linear regression without
regularization. The data set consists of 130 data points {(xi, yi)}i1=301 ⊆ R200 × R. The inputs
xi(∀i ∈ [130]) are uniformly drawn from the 200-dimensional unit sphere. All labels yi(∀i ∈ [130])
are generated by a linear target function yi = β>xi with some β ∈ R200 that satisfies kβk = 1.
We train this model by gradient descent W(k+1) = W(k) - ηVLn(w(k)) With step size η = 0.05
on the mean square loss (MSE). Entries of w(0) are initialized i.i.d. from Gaussian distribution
N(μ, σ2) with μ = 2-5, 2-2, 21,24 and σ = 2-5, 2-2, 21,24 respectively. We stop the training
once the difference of loss betWeen tWo consecutive steps |Ln(W(K+1)) - Ln(W(K))| < 10-8. Then
Ien(W(O), ∞) is approximated by PK=1 kw(k) — w(k-1) ∣∣. For each pair of μ and σ, we record the
length len(W(0), ∞), the training error after convergence and the test error on the convergence point.
Random feature model. In this experiment, the model that we consider is a two-layer ReLU neural
network W2>φ(W1x). Here φ is the ReLU function, W1 ∈ R200×200, W2 ∈ R200, We only train
the last layer W2 , and thus this can be viewed as a random feature model with features extracted
by φ(W1x). The data set consists of 130 data points {(xi, yi)}i1=301 ⊆ R200 × R. The inputs
xi(∀i ∈ [130]) are uniformly drawn from the 200-dimensional unit sphere. All labels yi(∀i ∈ [130])
are generated by a teacher network Wb2>φ(Wc1xi) with the same architecture for some Wb2, Wc1. We
train only the top layer by gradient descent with momentum 0.9 and step seize 0.05 under the mean
square loss (MSE). Entries of w20) are initialized i.i.d. from N(μ, σ2) with μ = —5, —2,1, 4 and
σ = 2-5, 2-2, 21, 24 respectively. We stop the training once the difference of loss between two
consecutive steps is less than 10-8. For each pair of μ and σ, we use the same method as on linear
regression to approximate the trajectory length. Then we record the training error after convergence
and the test error on the convergence point.
Varied learning rate. In this experiment, we provide additional results by only changing the learning
rate. Figure 2, 3 and 4 correspond to step size η = 0.01, 0.1, 0.5 respectively. We observe that the
optimization path lengths are nearly the same for both small learning rate and moderate learning rate,
and the numerical results are associate with our theory.
Iinear regression model
training error
test error	▲ ▲
2≡ 2ξ 27
Ien(I/1/(°), ∞)
random feature regression model
O O - -
Iloo
1 1
JOXI①
Io-
Ien(VV⑼,∞)
Figure 2: learning rate = 0.01
13
Under review as a conference paper at ICLR 2022
linear regression model
O0-2
110
①
• training error
▲ test error
random feature regression model
• training error
▲ test error	▲
Ien(VV⑼,8)	Ien(I√8,8)
Figure 3: learning rate = 0.1
linear regression model	random feature regression model
JoiJ ①
Ien(I/1/(°), 8)	len(∣ψω∖ ∞)
Figure 4: learning rate = 0.5
A.2 MNIST classification
In this subsection, we calculate the trajectory length on the MNIST classification problem to show
the relation between optimization and generalization under different initializations.
Experiment settings. In this experiment, we train a two-layer ReLU neural network with 100 hidden
units. We use stochastic gradient descent with mini-batch 64, momentum 0.9 and initial learning rate
0.1 to train the model. Parameters of the hidden layer are initialized by standard random initialization
method in Pytorch, and the parameters of the top layer are initialized i.i.d. from Gaussian distribution
N(μ, σ2) With μ = 2-5, 20, 25 and σ = 2-5, 2-2, 21,24 respectively. During training the model,
we reduce the learning rate by a factor 0.1 once learning stagnates. We stop the training once the
cross-entropy loss decreases to 0.01 or the number of epochs reaches 1000. For each pair of μ
and σ, We calculate the trajectory length by PkK=1 w(k) - w(k-1).
We plot the relation betWeen
optimization and generalization in terms of the trajectory length in Figure 5.
MNlST
0.5 -------------------------
・ training error
0 4.	▲ test error
Q 2
5 6
J OXI ①
.1Q
0.0.
0.0	0.5	1.0	1.5	2.0	2.5
Ien(IZi/9), ∞)
Figure 5: MNIST classification
14
Under review as a conference paper at ICLR 2022
From this figure, we can see that for different random initializations, short optimization paths is
associated with good generalization gap. This numerical result exhibits the generality of our theory
and suggests that the path length plays an important role to connect optimization and generalization.
B Proof of Theorem 1
In this section we will prove Theorem 1, our proof is based on the next lemma, showing that the
gradient flow trajectory is always inside Sn .
Lemma B.1. For any initialization w(0), if Ln(w) satisfies Uniform-LGI on a closed set Sn ⊇
B w(0), rn (w(0)) with constants cn, θn, where rn(w(0))
(Ln(w(0))-minw Ln(W))I θn
Cn (I- θn )
, then
w(t) ∈ Sn, ∀t ∈ [0, ∞).
Proof. Let
T = inf {t ≥ 0, w(t) ∈/ Sn},
then it is sufficient to prove that T = ∞. Otherwise, if T < ∞, then by the continuity of the curve
{w(t)}t≥0, we know that w(T) is in the boundary of Sn, therefore
len(w(0), T) ≥ d(w(0), ∂Sn) ≥ rn(w(0)).	(10)
Now we consider the following two cases.
Case (i). Ln(w(T)) = minw Ln(w). Since Ln (w(t)) is non-increasing, we have Ln (w(t)) =
minw Ln(w) and w(t) = w(T) for t ≥ T . Notice that Sn is a closed set, thus w(t) ∈ Sn for
t ∈ [0, T], meaning that T = ∞, which contradicts with T < ∞.
Case (ii). Ln(w(T)) > minw Ln(w). By chain rule, we have for all t ∈ [0, T],
dLndw(t)) =(VL"),")
=Ti(tM∣∣ 与 I
≤-Cn(Ln(w(t)) — minL(W))0 dw—.
w	I dt I
Then we can bound the trajectory length len(w(0), T) as
len(w(0), T)
(T)) - min L(w))	n
w
< rn(w(0)),
(11)
which contradicts with (10).
□
Now we begin to prove Theorem 1.
15
Under review as a conference paper at ICLR 2022
Proof. By Lemma B.1, we know that Ln(w(t)) satisfies Uniform-LGI for all t ∈ [0, ∞). Then by
the proof of Lemma B.1, we have ∀t ∈ [0, ∞),
len(w(0),t) ≤—---——— (Ln(w(0)) — minL(W))	—(Ln(w(t)) — minL(W))	.
cn(1 - θn)	w	w
(12)
For the convergence rate, note that
d (Ln(W㈤)一minw L(W)) = (VL( (t)) dw(t) ∖
dt	n	dt
= - VLn(W(t))2	(13)
2θn
≤ -c2n Ln (W(t)) - min L(W)	.
Therefore
Ln (W(t)) - min L(W)	n d Ln (W(t)) - min L(W) ≤ -c2ndt.
Integrating on both sides of the equation, we can get ∀t ∈ [0, ∞),
when θn = 2,
Ln (W(t)) - min Ln(W) ≤ e-c2nt Ln (W(0)) - min Ln(W)) ;	(14)
when 2 < θn < 1,
Ln(w(t))-叫nLn(W) ≤ (1 + Mt)T∕(2θnτ) (Ln(W(0)) - minLn(W)) ,	(15)
where M = cn(2θn — 1) (Ln(W(O)) - minw Ln(W))2θn 1.
Taking the limit on both sides of (14) and (15), since W(∞) is the limit of W(t), we have
Ln (W(∞)) = min Ln(W).
w
Hence W(∞) is a global minimum. Taking the limit on both sides of (12), we get
len(W(0), ∞) ≤ rn(W(0)).
This completes the proof.
□
C Proof of Theorem 2
In this section, we will prove Theorem 2. This proof is based on the Rademacher complexity theory
and the covering number of `2 balls. Now we introduce some known technical lemmas that are used
to build our proof.
The first lemma gives a generalization bound of a function class based on the Rademacher complexity,
which is proved in (Mohri et al., 2018).
Lemma C.1. Consider a family of functions F mapping from Z to [a, b]. LetD denote the distribution
according to which samples are drawn. Then for any δ > 0, with probability at least 1 - δ over the
draw of an i.i.d. sample S = {z1, . . . , zn} of size n, the following holds for all f ∈ F:
Ez〜D [f(z)] - 1 XX f(zi) ≤ 2Rs(F) + 3(b - a)∖∕lo⅞2/^,
n	2n
i=1
16
Under review as a conference paper at ICLR 2022
where RS (F) is the empirical Rademacher complexity with respect to the sample S, defined as:
RS (F) = Eσ
1 n
SUP—yf(Jif (zi)
f ∈F n
Here {σi}n=I are i.i.d. random variables drawnfrom U{ —1,1}.
In the next lemma, We prove a shifted version of the Ledoux-Talagrand contraction inequality (Ledoux
& Talagrand, 2013), which is useful to bound the length-based Rademacher complexity.
Lemma C.2. Let g : R → R be a convex and increasing function. Let φi : R → R be L-Lipschitz
functions, then for any bounded set T U R and any t(0)∈ R, we have
g (( sup	Xσi (φi(ti) — φi (t(O)))) I ≤ Eσ
g L sup	£bi 卜i —1(
∖ (t-t⑼)∈T i=ι	∖
and
sup
(t-t(°))∈T
n
X σi (φi(ti)— φi (t(O)))
i=1
≤ 2LEσ
sup
(t-t(°))∈τ
The special case for t(O) = 0 corresponds to the original Ledoux-Talagrand contraction inequality.
Here we prove a shifted version.
Proof. First notice that
g (( SUP	X σi (φi(ti) - φi 卜(O))))I= Eσι,...,σn.1
Let un-ι(t) = Pn=II σi (φi(ti) — φi (t(O))), then
n
sup	X σi φi (ti) — φi ti(O)
(t-t(°))∈τM	'	'	))
(n
sup	Xσi (φi(ti) — φi (t(O)))
(t-t(°))∈τ i=1	、	、 π
=1 g I sup Un-i(t)+	(φn(tn)	— φn	(tɑɔ)))	∣ + 1 g ( sup un-1(t)	— (φn(tn) —	φn 便)))∣ .
2	∖(t-t(°))∈τ	∖	∖	2	∖(t-t(°))∈τ	∖	∖
Suppose that the above two suprema can be reached at t0 and t respectively, i.e.,
(sup	Un-1(t)	+	(φn(tn) —	φn	^nO))) = Un-I	(t0) + Qn (tj	- φn	^nO)))；
(sup	Un-1(t)	—	(φn(tn) —	φn	^nO))) = Un-1	(tɔ — Qn gn)	— φn	,£))).
Otherwise we add an arbitrary positive number ε in the above equations. Therefore,
n
sup	X σi φi (ti) — φi ti(O)
(t-t(°))∈τi=ι	、	、 π
2 [g (un-1 (t0) + (φn (tn) — φn (t^O))))] + 2 [g (un-1 (t) — (φn (tn) — φn (t^O))))卜
17
Under review as a conference paper at ICLR 2022
Without loss of generality, we assume
un-1 (t0) + (φn (tn)	- φn	(力驴))≥ un-1	(tɔ + (φn (tn)	- φn	(力驴));
un-1 (t) — (φn gn)	- φn	(i)) ≥ un-1	(t0) — (φn (tn)	- φn	(^)) ∙
For the other cases, the method remains the same. We set
a = Un
b = un
-1 (tɔ - (φn (tn) - φn (^)),
-1 (t)- L (tn- tn)),
a = un -1 (t ) + L
—
b' = Un-1 (t0) + Qn (tn) - Φn
Now our goal is to prove:
g(a) - g(b) ≤ g (α') - g (U) .
Considering the following four cases:
(16)
(17)
1.	tn ≥ tn) and tn ≥ tn). By the Lipschitzness of φn and equation (16) we know a ≥ b,b0 ≥ b,
and
(a - b) - (a0 - b 0) = φn (tn)- φn (tn) - L (tn - tn) ∙
If tn ≥ in, we can get a - b ≤ a - b0. By the fact that g is convex and increasing, we have
g(y + x) — g(x) is increasing in y for every x ≥ 0. Hence for x = a — b,
g(a) - g(b) = g(b + x) - g(b) ≤ g (b0 + x) - g(b0) ≤ g (a) - g (b0).
If tn < tn, we change φn into -φn and switch t0 and t, and the proof is similar.
2.	tn	≤ tn0)	and tn	≤ tn0).	Similarly, by changing the signs we can get the same result.
3.	tn	≥ tn0)	and Wn	≤ tn0).	For this case we have a ≤ b and b0 ≤ a0, so g(a) + g (b0) ≤ g(b) + g (a0).
4.	tn	≤ tn0)	and Wn	≥ tn0).	For this case we can change φn to -φn, then we have a ≥ b and a0 ≤ b0,
and finally we get g(a) + g (b0) ≤ g(b) + g (a0).
Thus equation (17) yields that
n
sup X σi (φi (ti) - φi (t00)))
(t-t⑼)∈tM '	' π
2 [g (un-1 (t') + (φn (J) - φn (娟)))]+ 2 Ig(Un-I (i) - (φn (tn) - φn (t!^)))]
1
≤-
-2
1
≤-
-2
=Eσn
g
g
gU
(t) - L
—
18
Under review as a conference paper at ICLR 2022
Applying the same method to σn-1, ...,σ1 successively, we obtain the first inequality
g (( sup	Xσi Qig- φi (t(O)))) I ≤ Eσ
g L sup	£bi (ti -1(
∖ (t-t ⑼)∈Ti=ι	∖
For the second inequality, since ∖x∖ = [x]+ + [x]- with [x]+ = max(0, x) and [x]- = max(0, -x),
≤Eσ
sup
(t-M°))∈T
sup
(t-t(°))∈T
+ Eσ
+,
n
Xσi (φi(ti)- φi (t(O)))
.i=1
=2Eσ	sup
(t-t(°) )∈T
sup
(t-M°))∈T ]
n
X σi (φi(ti)- φi 卜(O)))
.i=1

where the last equality is by [-x]- = [x]+ and σ has the same distribution with -σ.
A simple fact is that
sup
(t-t ⑼)∈T
n
sup	X σi (φi(ti) - φi (t(O)))
(t-t(°) )∈t^=1	、	、 n
+
Since max(0, x) is convex and increasing, then by the first inequality we have
Eσ
sup
(t-t(°))∈T
Eσ	sup	Xσi (φi(ti) - φi (tiO)))
[[(t-t(°))∈Ti=ι	'	' π
n
≤ Eσ	L sup	X σi (ii - £O))
[[(t-t(°))∈τM	'	)
n
≤ LEσ	sup
[(t-t(°))∈T
X σ (ti -1(0))
i=1
+
+
This completes the proof.
□
Now we apply Lemma C.2 to bound the empirical Rademacher complexity of an element-wise
distance constrained function class. In the following lemma, all the notations are consistent with
Theorem 2 unless stated otherwise.
Lemma C.3. Given a function class Fa,,b ：= {x → f (w,x) : W ∈ 5α,b} and sample S =
{xι,...,xn} with ∣∣xi∣∣ = 1 forall i ∈ [n] ,then we have
RS (Fa,b) ≤ j"a『+ ""F kLψ(Sa,b)∣∣ .
19
Under review as a conference paper at ICLR 2022
Proof. By definition,
nRS(Fa,b) = Eσ
n
sup	σif (w, xi)
n
sup	σif (w, xi)
n
-Eσ Xσif(0,xi)
i=1
n
sup	σi (f (w, xi) - f (0, xi))
Now we decompose the term f (w, xi) - f (0, xi) as:
f (w, xi) - f(0, xi)
=Ψ xi>α1, . . . , xi>αp, β1, . . . , βq - Ψ (0, . . . , 0, 0, . . . , 0)
=(Ψ(xi> α1,...,xi> αp,β1,...,βq)-Ψ(0,...,xi> αp,β1,...,βq))+(Ψ(0,...,xi> αp,β1,...,βq)-Ψ(0,0,...,xi> αp,β1,...,βq))
+ ∙∙∙ + (Ψ (0,..., 0,0,..., 0, βq) — Ψ (0,..., 0,0,..., 0)).
Then by the above decomposition and Lemma C.2, we have
n
Eσ	sup	σi (f(w, xi) - f(0, xi))
b i=1
n
sup	σixi>α1
w∈Sa,b i=1
n
+-----+ Lψp+q) (Sa,b)Eσ	Sup X σ%βq
w∈Sa,b i=1
Notice that
n
sup	σixi>α1
w∈Sa,b i=1
n
sup	σixi>α1
kα1k≤a1 i=1
And
n
sup	σiβq
≤ a1Eσ
≤ a1 Eσ
1 t
=aι √n.
n
σixi
i=1
n
= Eσ sup	σiβq
Jeq l≤bq i=1	.
n
≤ bqEσ	σi
i=1
n 2
Xi=1σi
Therefore, by the Cauchy-Schwarz inequality, we can get
n
Eσ sup
w∈Sa,
X σi (f(w, Xi)- f(0,Xi))	≤ Lψ)(Sa,b)αi√n +------+ Lψp+q)(Sa,b)bq√n
b i=1
≤ Jn (kak2 + 烟『)kLψ(Sa,b)k∙
20
Under review as a conference paper at ICLR 2022
Finally, We have
RS (Fa,b) ≤ Skak2 + ”"F kLψ(Sa,b)k .
□
Lemma C.3 gives an upper bound of the Rademacher complexity based on the element-Wise distance.
Notice that w(∞) ≤ w(0) + len(w(0), ∞). To obtain a length-based generalization bound, We
consider to use Sa,b to cover the length-constrained space {w : kwk ≤ R}, and then taking a union
bound. For the `2 ball covering number, We use the folloWing result from (Neyshabur et al., 2019,
Lemma 11).
Lemma C.4. Given any , D, β > 0, consider the set SβD = x ∈ RD : kxk ≤ β . Then there exist
N sets {Ti}N=ι of the form Ti = {x ∈ RD : |xj| ≤ αj,∀j ∈ [D]} such that SD ⊆ UN=I Ti and
∣∣αi∣∣ ≤ β(1 + e), ∀i ∈ [N] where N = (K+D-1) and
(1+)2-1 .
Lemma C.5. For any two positive integers n, k with n ≥ k, we have
(n)≤( en )k∙
Proof. Note that
n	n!	nk	k n k
⑺=k!(n - k)! ≤ ^⅛T ≤ e ⑴
The last step is by
∞ ki kk
ek = X ⅛ ≥ k
i=0
□
NoW combining Lemma C.1, C.3, C.4 and C.5, We are ready to prove Theorem 2.
ProofofTheorem 2. First we apply Lemma C.4 with E = √2 一 1, D = P + q, and β = Mδ + Rn,δ,
then there exist N sets Sak bk such that SD ⊆ UN=I Sak bk and J∣∣ak∣∣2 + ∣∣bk∣∣2 ≤ √2β, with
N = S	—，
Therefore, for each parameter space Sak,bk, by Lemma C.3 we have
RS (Fak ,bk ) ≤ ynβ ∣∣Lψ(sak,bk )∣∣ .
Notice that the local Lipschitz constant of ' in Sak,bk is l`(S@k@).Hence, by Lemma C.1 and the
Ledoux-Talagrand contraction inequality, for any δ > 0, with probability at least 1 一 δ∕N over the
training sample, the following holds for all w ∈ Sak,bk :
尸，- 2√2βL'(Sak,bk)∣∣Lψ(Sak,bk )∣∣	/ log(2N∕δ)
LD (w) ≤ Ln(W) +-------------√n--------------Ll + 3Mβ∖∣ ——2n——,
where Mβ = SUPkakk2+kbkk2≤2β2 supw∈Sak bk ,kxk≤1,∣y∣≤1 ' (f (w，x), y)∙
Since w(∞) ∈ SβD ⊆ UkN=1 Sak,bk, by taking the union bound over all sets Sak,bk, we can get with
probability at least 1 一 δ over the initialization I the training sample,
LD (w(∞)) ≤Ln(w(∞))+	sup	2√2βL'(Sa,b) kL"Sa,b)k +3Ma,b /	.
kak2+kbk2≤2β2	n	2n
21
Under review as a conference paper at ICLR 2022
Theorem 1	already showed that Ln (w(∞)) = minw Ln(w). Thus it remains to bound the term log N.
For D = 1, N = 1. For D ≥ 2, by Lemma C.5,
log N ≤ (D — 1) log (e(DD-II)) < 2.1(D - 1) < 3D = 3(p + q).
Finally, let R = √2β, We complete the proof of Theorem 2.
□
C.1 Additional of results for the generalization during training the model
Theorem 2	gives a length-based generalization bound for the final model. In this subsection, We apply
our frameWork to derive generalization estimates that evolves according to the length of time (number
of epochs) of training by combining the length estimate obtained in Theorem 1.
The approach is to give a generalization bound for early stopping When the loss value first reaches
ε ≥ 0. The idea is that, When there exists T > 0 such that Ln(wT ) = ε, then by the inequality
(11) in the proof of Lemma B.1, We can get an upper bound for the length len(w(0), T) in terms
of Ln(w(0)), minw L(w), ε, cn, θn. Finally, We can get a generalization bound by our neW length
estimate.
To get a clean expression for the generalization bound, We assume the optimal value of the empirical
loss function Ln (w) is zero. Then the rigorous statement is stated as folloWs:
Corollary 1. Consider a training criterion of early stopping that the training is stopped once the
empirical loss value first reaches ε ≥ 0. Then for any given ε ≥ 0, under the notations and conditions
in Theorem 1, suppose that for any δ ∈ (0, 1), there exists rn,δ such that rn (w(0) ) ≤ rn,δ with
probability at least 1 - δ over the initialization and the training samples. Then, we have with
probability at least 1 - δ over initialization and the training samples, the generalization error for the
stopping parameter w is given by:
2rε,n,δ L'(Sa,b) kLψ(Sa,b)k	J 3(p + q)+lθg(2∕δ)
LD(W) ≤ ε +	suP -------------------+--------------+3Ma,b∖ ----------ɑ----------, (18)
kak2+kbk2≤r2,n,δ	VZn	2	2n
where rε,n,δ = √2 (Mδ + rn,δ - ⅛⅛
Remark 2. Corollary 1 shows a trade-off between ε and the term rε,n,δ. The case for ε = 0
corresponds to the combining results of Theorem 2 and Theorem 1.
Proof. The proof is straightforWard. Notice that by the inequality (11), We can bound the length
len(w(0), w) as
ε1-θn
len(w(0),w) ≤ rn(w(0))----万~~^ʒ,
cn(1 - θn)
Where rn (w(0)) is specified in equation (2). Then by the same argument in the proof of Theorem 2,
We may directly replace rn(w⑼)with rn(w(0)) - .；：；)to get the desired bound.	□
D	Proofs for Section 4
In this section, our goal is to prove all the theorems in Section 4. A crucial part of the proofs is the
spectral analysis of the random matrix X. Therefore, We start With introducing the non-asymptotic
results of λmax(XX>) and λmin(XX>) from (Rudelson & Vershynin, 2010).
The first result is from (Rudelson & Vershynin, 2010, Proposition 2.4), characterizing the non-
asymptotic behavior of the largest singular value of subgaussian matrices.
22
Under review as a conference paper at ICLR 2022
Lemma D.1. LetA be an N ×n random matrix whose entries are independent mean zero subgaussian
random variables whose subgaussian moments are bounded by 1. Then for every t ≥ 0, with
probability at least 1 - 2e-ct2 over the randomness of the entries,
Jλmaχ(AA>) ≤ C(√N + √n)+ t,
where c and C are two positive constants that depend only on the subgaussian moment of the entries.
The second result is from (Rudelson & Vershynin, 2009, Theorem 1.1), characterizing the non-
asymptotic behavior of the smallest singular value of subgaussian matrices.
Lemma D.2. Let A be an N × n random matrix whose entries are independent and identically
distributed subgaussian random variables with zero mean and unit variance. If N > n, then for
every ε > 0, with probability at least 1 - (C1 ε)N -n+1 - c1N over the randomness of the entries,
《λmin(ATA) ≥ ε(√N - √n - I),
where C1 > 0 and c1 ∈ (0, 1) depend only on the subgaussian moment of the entries.
D.1 Proof of Theorem 3
In this section, we will prove Theorem 3 based on the three steps in our framework. All the notations
are consistent with Theorem 3 unless stated otherwise.
Proof of Theorem 3. First, we prove the result for Step 1.
For a vector a = (aι,..., an)> ∈ Rn, We use a°m to denote the element-wise power, i.e., a°m
(a1m, . . . , anm)>. For the `p linear regression loss function Ln(w), notice that
VLn(w) = X X (w>xi - y,p 1 Xi = X>> (XW - Y)°(p-1).
nn
i=1
Then since X has full row rank, we have ∀w ∈ Rd,
kVLn(w)k = 1∣∣X >(X W -Y)。(P-I)Il
≥
≥
,λmin(XX >) n	∣(x w -γ"1)∣∣	
Pλmin(XX >) n	kXW	- Yk2pp--12
Pλmm(XX >) n	kXW	-Ykp-1 ∕pT2
p1-1/p ∕λmin(XX>) Ln(W)1-1/P.
n
Therefore,
Cn = Piprλmin(XX D,	θn = 1 - 1/p.
n
For Step 2, note that minw Ln(W) = 0, then the result can be proved by directly plugging cn and θn
into Theorem 1.
23
Under review as a conference paper at ICLR 2022
Next, we prove the result for Step 3. By Theorem 1 and the property of the target function, we have
for any w(0) that satisfies w(0) 2 ≤ c0,
(0)	√n 尸(W(O))产
Pλmin(XX >)
Xw(0)-Y
=n1∕2-1∕p Il 11P
Pλmin(XX >)
≤ nι∕2-ι∕p IIX w(0)- Yl∣2
pλmin(XX >)
≤ n1∕2T∕Ps∕ZmaX(XXT) (CO + c*)
≤	Vλmin(XX>)(	+	) .
Now we apply Lemma D.1 with A = X and t
1 - δ∕2 over the samples,
JIog(4∕δ), then We have with probability at least
qλmax(XXT) ≤ C(√n + √d) + 4 g[^^ɪ,	(19)
where c and C are two positive constants that depend only on the subgaussian moment of the entries.
Similarly, let T = c1 ∈ (0,1),ε = τ∕C1 > 0, then Lemma D.2 implies that with probability at least
1 - τ d-n+1 - τd over the samples,
dλmin(XXT) ≥ k(√d - √n - 1),	QO)
C
where C1 > 0 and τ ∈ (0, 1) depend only on the subgaussian moment of the entries.
Taking the union bound, we have with probability at least 1 - δ∕2 - Td-n+1 - Td over the initialization
and the training samples,
√n
≤ n-1/p (co + c*)
≤ n-1/p (co + c*)
C (√n + √d) + ylog≡1
C(√d - √n — 1)
C(Pl+d+q⅞F
(21)
≤ n-1/P(CO + c*)
cCι(√Y1 +1) + Ci ,γι 甯㈤
T (1 - √γi)
≤ O (n-1/p + n-1/p Jlθg(yδ)
Recall for the linear regression model (5), f(w, x) = wTx. Thus Ψ(x) = x is an identity function
with p = 1, q = 0, and kLΨ (Sa,b)k = 1 for any a, b. Since the loss function ` is bounded by 1
and 1-Lipschitz, we know that l`(Sa,b) = MR = 1 for any a and b. Finally by Theorem 2 and
'(y, y) = 0, we have with probability at least 1 - δ∕2 over the samples,
E(x,y)s F(f (w(∞),x),y)i ≤ SY” CO) +3^+°nm.
Combining the inequality (21), we have with probability at least 1 - δ - T d-n+i - Td over training
samples,
E(χ,y)〜D [Z(f(w(∞),x),y)i ≤ O (n-1/P) + O (JIogn/δ)
24
Under review as a conference paper at ICLR 2022
This completes the proof of Theorem 3.
□
D.2 Proof of Theorem 4
In this section, we will prove Theorem 4. First, we present some useful lemmas for proving our
results, and then we give the proofs of Theorem 4 for the RBF kernel and the inner product kernel
separately.
For the RBF kernel k(x, y) = % (ky - xk), the following two lemmas give non-asymptotic bounds
for λmax(k(X, X)) and λmin(k(X, X)) based on the separation distance SD of X.
The first lemma is from (Diederichs & Iske, 2019, Lemma 3.1), providing an upper bound for
λmax(k(X,X)).
Lemma D.3. For the RBF kernel, if % : R≥0 -→ R≥0 is a decreasing function, then
∞
λmaχ(k(X, X)) ≤ %(0) + 3dX(t + 2)d-1%(t ∙SD),	(22)
t=1
ι ( Md y
2Γ(d∕2 + 1) V23/2 )
and the sum of the infinite series in equation (22) is finite if and only if % (kxk) ∈ L1(Rd).
The next lemma is from (Wendland, 2004, Theorem 12.3), giving a lower bound for λmin (k(X, X)).
Lemma D.4. Suppose that k is a positive-definite RBF kernel. If % (kxk) ∈ L1(Rd), one can define
the Fourier transform of % as %(ω) := (2π)-d/2 JRd %(ω)e-ix ω dω. With a decreasing function
%0(M) and two constants Md, Cd defined as
%o(M ):=	inf	%(x), Md = 6.38d, Cd
kxk≤2M
where Γ is the gamma function. Then a lower bound on λmin(k(X, X)) is given by
λmin(k(X, X)) ≥ Cd ∙ %0 (Md/SD) ∙ SD-d.
For the inner product kernel k(x, y) = % (Xdy), it is shown in (El Karoui et al., 2010) that the
kernel matrix k(X, X) can be approximated by the linear combination of all-ones matrix 11>, sample
covariance matrix XX> and identity matrix. To obtain non-asymptotic results on the spectra of the
kernel matrix k(X, X), we borrow the technique from (Liang & Rakhlin, 2020, Proposition A.2),
and show the result for subgaussian entries in the next lemma.
Lemma D.5. For the inner product kernel, suppose that the entries of X are i.i.d. subgaussian
random variables with zero mean and unit variance, then with probability at least 1 - δ - d-2 over
the entries,
∣∣k(X, X) - klin(X, X)∣∣ ≤ d-1/2 GT/2 +log0.51 d),
where klin(X, X) is defined as
klin(X,X) := %(0) +
Proof. Note that the sample covariance matrix Σd = Id×d, then by applying (Liang & Rakhlin, 2020,
Proposition A.2) with subgaussian random entries we can prove this lemma.
□
Lemma D.6. Suppose that A, B ∈ Rn×n are two symmetric matrices, then we have
λmin(A + B) ≥ λmin(A) + λmin (B).
11> + %0(O) -d-------+ (O(I) - %(O) - %0(O)) In×n.
25
Under review as a conference paper at ICLR 2022
Proof. Note that for any x ∈ Rn with kxk = 1,
x>(A + B)x = x>Ax + x>Bx ≥ λmin(A) + λmin (B).
By definition, we have
λmin(A + B) = inf x>(A + B)x ≥ λmin(A) + λmin(B),
kxk=1
which completes the proof.
□
Now we are ready to prove Theorem 4.
Proof of Theorem 4. For Step 1. Notice that ∀w ∈ Rs,
kVLn(w)k = n b(X)> W(X)w - Y产PT)Il
≥
,λmmQ(X )p(X )>)
n
IlW(X )w-Y 产P-I)Il
,λmin(k(X, X))
n
Pλmin(k(X, X))
n
k^(X )w -YkP--2
闷(X)w -YkPT 仙…2
p1-1/P
∕λmin(k(X, XDLn(w)1-1/p.
n
Therefore, Ln(w) satisfies the Uniform-LGI globally on Rs with
Cn =Pip J"k(X, X) , θn = 1 - 1/p.
n
For Step 2. Since k is a positive-definite kernel, and θn = 1 - 1/p, then minw Ln(w) = 0, thus by
Theorem 1 we can directly get the result.
The proof of Step 3 is two-sided. First, since ∀x ∈ X, k夕(x)k =，k(x, x) ≤ 1, then the kernel
regression model (7) can be viewed as 'p linear regression on inputs 夕(X). Hence, Ψ is an identity
function withP = 1,q = 0, and ∣∣Lψ(Sa,b)k = L'(Sa,b) = MR = 1 for any a, b. This means that
we only need to bound the term rn(w(0))/√n.
By Theorem 1 and the property of the target function, for any w(0) that satisfies Iw(0) I2≤	c0, we
have
(0)	√nXn(W(O))产
Pλmm(k(X, X))
=n1∕2-1∕p MF)W⑼ -Y|IP
Pλmin(k(X, X))
≤ n1∕2-1∕p Il。？)W(O)-YII2
_	Pλmin(k(X, X))
≤ n1∕2-1∕p M(X)W叫 + kYk
一	Pλmin(k(X, X))
≤ n1∕2T∕P4 ∕λmaχ(k(X, X)ɪ (co + c*)
≤	V λmin(k(X, X))(c0 + ) .
(23)
≥
26
Under review as a conference paper at ICLR 2022
Then for the RBF kernel, Lemma D.3 and Lemma D.4 indicate that there exists a positive constant
C(%, d, qmin, qmax) that only depends on %, d, qmin, qmax such that
λmax(k(X, X)) , E j	∖ Urlrl
λmm(k(X, X)) ≤ CM d, qmin, qmaX)，∀n ≥ 1,
which implies that for all initialization w(0),
rn(W0)) ≤ o (-τ∕p)
Therefore, by Theorem 2 and Lemma C.1, we have with probability at least 1 - δ over training
samples,
E(χ,y)〜D h《f(wC),x),y)i ≤ O (--1/P) + O (JIogn/δ)),
which completes the proof for the RBF kernel.
For the inner product kernel, to obtain an upper bound for λmax(k(χ,χ)), first notice that
λmax(k(X,X))= kk(X,X)k
≤ IIklin(X, X)II+ IIk(X,X) -klin(X,X)II.
By Lemma D.6, we can get
λmin(k(X, X)) ≥λmin(klin(X,X))+λmin(k(X,X)-klin(X,X))
≥ λmin(klin(X, X)) - k(X,X) -klin(X,X).
(24)
(25)
(26)
Under Assumption 2, Lemma D.5 implies that
IIklm(X,x)∣∣ ≤ 邛 ∣∣ιι>∣∣ + 斗 IlXX>∣∣ + (%(i) - %0(o))
and
n%00 (0)
≤ -%du + %0(O)
≤ γ1 %00(0) + %0(0)
λmaχ(XX >)
d
λmaχ(XX >)
d
+ (%(1) - %0(0))
+ (%(1) - %0(0)) ,
λmin(klin(X, X)) ≥ %(1) - %0(0) >0.
Thus, by equation (26) we have
λmin(k(X, X)) ≥ (%(1) - %0(0)) - k(X,X)-klin(X,X).
(27)
Under Assumption 1, by equation (19), We have with probability at least 1 - δ∕3 over the samples,
λmax(XX>) ≤ (C
≤ (c(√Y1 +1) +J
---------∖ 2
log(6∕δ))
cd
2
Y1 log(6∕δ) ∖
cn
Therefore, by equation (25), with probability at least 1 - δ∕3 over the samples,
λmaχ(k(X, X)) ≤ Yl%00(0) + %θ(0) CC (√Y1 +1) + J1 lOC-6/6
2
+%(1)-%0(0)+k(X, X) - klin(X, X).
(28)
27
Under review as a conference paper at ICLR 2022
By LemmaD.5, for large d and small δ such that d-1/2 (√3δ-1/2 + log0.51 d) ≤ 0.5(%⑴-%0(0)),
we have with probability at least 1 - δ∕3 - d-2 over the entries,
k(X,X) -klin(X,X) ≤ 0.5(%(1) - %0(0)).
Then equation (27) and (28) yields that with probability at least 1 - 2δ∕3 - d-2 over the samples,
λmin(k(X,X)) ≥ 0.5(%(1) - %0(0)),
λmaχ(k(X, X)) ≤ Y1%00(O) + %0(0) (c (√γτ + 1) + Jγ1 "ye) + 1.5(%(1) - %0(0)).
Hence, by equation (23), with probability at least 1 - 2δ∕3 - d-2 over the samples, We have
rn(W(O)) ≤ O(n-1∕p + Plog(10 )
√n	≤ O [n + n1∕2+1∕p	.
(29)
Combining Theorem 2, we get with probability at least 1 - δ - d-2 over the samples,
E(x,y)〜D [Z(f(wO,x),y)i ≤ O (n-1∕p) + O (JIogn/δ)),
which completes the proof.
□
D.3 Proof of Theorem 5
In this section, we will prove Theorem 5. We first introduce some important lemmas for proving our
final results. Lemma D.7 shows that the smallest eigenvalue of the NTK matrix Θ(t) has a lower
bounded given the overparameterization, by which we can prove the optimization result. In Lemma
D.8, we show that the eigenvalues of the NTK matrix are related to the data covariance matrix. Then
by combining Lemma D.11 and Lemma D.9 we can prove the generalization result.
Lemma D.7. For any δ ∈ (0, 1), if m ≥ poly n, λ-m1in (Θb), δ-1 , then with probability at least
1 - δ over the random initialization,
λmin(Θ(t)) ≥ 1 λmin(Θ),	∀t ≥ 0.
Proof. The proof is the same as the proof of (Du et al., 2019, Lemma 3.4).
□
Lemma D.8.
λmin(Θ) ≥ λmin (XX>) /4.
Proof. Notice that for ReLU activation φ, a simple fact is that φ0(ax) = φ0(x) holds for any x ∈ R
given that a > 0. Therefore,
θij = x>xjEw〜N(0,dId) [φ0(w>Xi)φ0(w>χj)]
=x>XjEw〜N(0,Id) [φ0(w>Xi)φ0(w>Xj)]
xi>xj (π - arccos(xi>xj ))
=	2∏
= Xi；j + Xi Xj arcsin(x>Xj)
=X>Xj + ɪ X∞	(2k)!	(x>x )2k+2
= 4 + 2π ^4k(k!)2(2k +1)(Xi Xj)	.
28
Under review as a conference paper at ICLR 2022
Then
Θ = XX > I 1 X	(2k)!	/ X X >∖◦(2k + 2)
=~1~ + 2∏ J 4k(k!)2(2k +1)1	)
XX >	1 X (2k)!
4	+ 不工 4k(k!)2(2k + 1)
where ◦ is the element-wise product, and is the Khatri-Rao product6.
Since ((X>)Θ(2k+2))> (X>)Θ(2k+2) is positive Semidefinite, we have
λmin(Θ) ≥ λmin (XX>)/4,
which completes the proof.
□
In the next lemma, we adopt an inequality from (Montgomery-Smith, 1990).
Lemma D.9. If {σi}in=1 are i.i.d. drawn from U {-1, 1}, then for any x = (x1, . . . , xn)> ∈ Rn,
with probability at least 1 - δ over σ,
n
XσiXi ≤ √2log(2∕δ) ∣∣xk .
i=1
The following lemma gives a sharp bound for a Chi-square variable, which is from (Laurent &
Massart, 2000, Lemma 1).
Lemma D.10. Let (Y1, . . . , YD) be i.i.d. Gaussian variables, with mean 0 and variance 1. Then with
probability at least 1 - δ over Y ,
ID log
D
X Yi2 ≤ D + 2
i=1
+ 2 log
(1)
The next lemma is quoted from (Arora et al., 2019, Lemma 5.4), giving an upper bound for the
empirical Rademacher complexity if one has an accurate estimate for the distance with respect to
each hidden unit.
Lemma D.11. Given R > 0, consider the following function class
F = nx7→ f(w,x)	:	W1,r	- W1(,0r)	≤	R(∀r	∈	[m]),	W1	-W1(0)	≤Bo
with W1,r ∈ Rd the r-th row of W1. Then for an i.i.d. sample S = {x1, . . . , xn} and every B > 0,
with probability at least 1 - δ over the random initialization, the empirical Rademacher complexity
is bounded as:
RS(F) ≤√Bn (1 +
2log(2∕δ) Y/4
m
+ 2R2√md + R√2 log(2∕δ).
Now we are ready to prove Theorem 5.
6For A = (aι,...,an) ∈ Rm×n, B = (bi ,...,bn) ∈ Rp×n,then A Θ B = [aι Z bi ,...,an 0 bn], where
Z is the Kronecker product.
29
Under review as a conference paper at ICLR 2022
Proof of Theorem 5. For Step 1. By Lemma D.7, if m ≥ poly n, λ-m1in(Θb), δ-1 , then with proba-
bility at least 1 - δ over the random initialization,
1n
11 VLn (w(t))∣∣ = - X (f(w,Xi)-yi) Vf(W㈤,Xi)
n i=1
=-∣∣ Vf(W㈤,X )> (f(w(t), X)-Y )∣∣
=-√(f (w(t), X) - Y)> Vf (w(t), X)Vf (w(t), X)> (f (w(t), X) - Y)
≥ r2λmin(Θ(t)) jLn(w(t))
≥ S λmn≡ q L…
holds for any t ≥ 0, which means that Ln (W(t)) satisfies the Uniform-LGI for any t ≥ 0 with
Cn = Jλmin(θ"n,	θn = -/2.
For Step 2. By equation (13), we can directly get Ln (W(t)) converges to zero with a linear convergence
rate:
Ln(w(t)) ≤ exp (-λmin (θ)t/n) Ln(W(O)).
For Step 3. By equation (12) and Lemma D.8, the length len(W(0), ∞) can be bounded as
…∞—)=2SmF ≤ 4S ≡X⅛.
By the property of the target function, we have
VZnLn(W(0))
2X (√mw>φ(")-y)
i=1
∖
≤
≤
X ( √mw>φ(WI(O)Xi)) + y2
∖
≤
Since the entries of W2 are drawn i.i.d. from U{--,-}, then by Lemma D.9, for each i ∈ [n], with
probability at least 1 一 δ∕6n over w2,
≤ 2iog(-δn) ∣∣φw(O)Xi)∣∣2.
30
Under review as a conference paper at ICLR 2022
Taking the union bound over all i = 1, 2, . . . , n, we have with probability at least 1 - δ over the
random initialization,
un t m X (w>φ(W(0)xi) i=1	2≤t	2 log(12n∕δ) m	n X i=1	φ(W1(0)xi)2
	/ 2 log(12n∕δ) m		φ	W1(0)X>F
	≤ r2log(12n∕δ) m		W1(	0)X>	(30)
	≤ r 2log(12n∕δ)^ m		W1(	IIF kXk
	2 2 log(12n∕δ) m		W1(	0)||尸'》皿&*(%%>).
For the Gaussian random matrix W(O)〜N(0, dIm×d), by LemmaD.10, We have with probability
at least 1 - δ∕6 over the random initialization,
2
m
F V 1	∕l°g(67δT 2log(6/6
一≤1 + 2V -mτ +	md
(31)
Taking the union bound of equation (19), (20), (30) and (31), if m≥ poly n, λ-m1in(Θb ), δ-1 , then
with probability at least 1 - Td-n+1 - Td — 5δ∕6 over the samples and random initialization I,
sup
w(0) ∈I,(x,y)∈D
≤ O (Plog(n∕δ)).
Therefore, with probability at least 1-Td-n+1 -Td-5δ∕6 over the samples and random initialization,
≤O
log(n∕δ)
n
For the r-th row of W1(t), we begin to bound the distance
for each r ∈ [m].
(32)
31
Under review as a conference paper at ICLR 2022
Notice that
n
≤ —η= Xf(W㈤,Xi) - yi∣
nm
≤-^h(w(t), x )-y∣∣
nm
≤ r2Lnw	exp (一λmin(Θ)t∕2n).
Hence,
LI d s ≤ λ⅜【=J …
0	min( )	m min( )
Now We apply LemmaD.11 with B = rn(w(0)), R = suPw(0)∈ι,(χ,y)∈D Jmλ;:(θ)rn(w(O)), We
get with probability at least 1 - δ∕12 over the random initalization,
Rs(F) ≤ B= f1+ (Rog(24")[I") +2R2√md + RP2lοg(24∕δ).
2n	m
Then by equation (32), if m ≥ poly n, λ-m1in (Θb), δ-1 , then with probability at least 1 - τ d-n+1
τd - 11δ∕12 over the samples and random initialization,
Rs (F) ≤o (rιog(≡).
—
Finally, by Lemma C.1, we have with probability at least 1 - τ d-n+1 - τd -δ over the samples and
random initialization,
E(χ,y)〜D [Zf(wC),x),y)i ≤ O H log，：0
for some constant τ ∈ (0, 1) that depend only on the subgaussian moment of the entries.
This completes the proof.
□
32