Under review as a conference paper at ICLR 2022
Iterative Sketching and its Application to
Federated Learning
Anonymous authors
Paper under double-blind review
Ab stract
Johnson-Lindenstrauss lemma is one of the most valuable tools in machine learn-
ing, since it enables the reduction to the dimension of various learning problems.
In this paper, we exploit the power of Fast-JL transform or so-called sketching
technique and apply it to federated learning settings. Federated learning is an
emerging learning scheme which allows multiple clients to train models without
data exchange. Though most federated learning frameworks only require clients
and the server to send gradient information over the network, they still face the
challenges of communication efficiency and data privacy. We show that by it-
eratively applying independent sketches combined with additive noises, one can
achieve the above two goals simultaneously. In our designed framework, each
client only passes a sketched gradient to the server, and de-sketches the average-
gradient information received from the server to synchronize. Such framework
enjoys several benefits: 1). Better privacy, since we only exchange randomly
sketched gradients with low-dimensional noises, which is more robust against
emerging gradient attacks; 2). Lower communication cost per round, since our
framework only communicates low-dimensional sketched gradients, which is par-
ticularly valuable in a small-bandwidth channel; 3). No extra overall communica-
tion cost. We provably show that the introduced randomness does not increase the
overall communication at all.
1 Introduction
Federated learning enables multiple parties to collaboratively train a machine learning model with-
out directly exchanging training data. This has become particularly important in areas of artificial
intelligence where users care data privacy, security, and access rights, including healthcare (Li et al.,
2020b; 2019), internet of things (Chen et al., 2020), and fraud detection (Zheng et al., 2020).
Given the importance and popularity of federated learning, it has become an important research
topic for academia and industry, mostly focusing on two central themes. One is on data privacy.
Federated learning seemingly protects clients’ privacy, since it only communicates gradient infor-
mation. Unfortunately, recent studies (Geiping et al., 2020; Zhu & Han, 2020; Wang et al., 2019)
have demonstrated that attackers can recover the input data from the shared gradients. The reason
why the attacks work is the gradients carry important information about the training data (Ateniese
et al., 2015; Fredrikson et al., 2015). The second one is on communication efficiency. Machine
learning models are becoming increasingly larger but client devices that carry private data only have
limited network bandwidth. The size of the gradient is the same as the size of the model, and the
amount of data to communication between the clients and the servers thus is large. This becomes
even more problematic when conducting federated learning on mobile and edge devices, where the
bandwidth of the network is low. The communication cost is one of the most important the key per-
formance bottlenecks in federated learning systems Goga & Teixeira (2012); Konecny et al. (2016).
Many works try to address this challenge through local optimization method, such as local gradi-
ent descent (GD), local stochastic gradient descent (SGD) and their variants (Konecny et al., 2016;
McMahan et al., 2017; Stich, 2018).
Despite existing efforts, no work addresses both challenges simultaneously as far as we concern.
Therefore, we ask the following question:
Is there a FL framework that protects the local privacy and tackles the communication challenge?
1
Under review as a conference paper at ICLR 2022
In this paper, we achieve these goals by using an old but powerful idea — the Johnson-Lindenstrauss
transform and its fast variations (Fast-JL, see Ailon & Chazelle (2006)). If we view the transform as
a sketch matrix, and its transpose as de-sketch, then our main idea is to iteratively apply the sketch
and de-sketch matrices to gradients. Instead of running the vanilla gradient descent w(t+1) J
w(t) 一 η ∙ g(t) using true gradient g(t) ∈ Rd, We apply sketch and de-sketch to the gradient:
w(t+1) J w(t) — η ∙ R> ∙ R ∙ g(t).
Here R ∈ Rbsketch ×d denotes a sketching matrix that sketches the true gradient to a loWer dimension
and R> ∈ Rd×bsketch denotes the de-sketching process that maps the sketched gradient back to the true
gradient dimension. The coordinate-Wise embedding property (Song & Yu, 2021) ensures R> Rg(t)
being an unbiased estimator of g(t) With bounded second moments, implying the neW sketched
gradient descent scheme preserving the original convergence properties. Hence, all clients Will
only communicate sketched gradients to the server, the server averages the sketched gradients and
broacasts it back to all clients. Finally, each client de-sketches the received gradients and perform
local updates. Since the sketching dimension is alWays small compared to original dimension, We
save communication cost per iteration via Johnson-Lindenstrauss.
Though the communication problem has been addressed, such frameWork still faces privacy chal-
lenge: applying Johnson-Lindenstrauss “masks” the communicated gradients, but in order to give
a provable privacy guarantee on this frameWork, We introduce additive loW-dimensional Gaussian
noises to make sure that the communicated vectors themselves are differential private.
We summarize the contributions in this Work as folloWs:
Framework contribution: We propose a neW federated learning frameWork that iteratively applies
sketch and de-sketch matrices to gradients, Which enjoys the folloWing advantages:
•	Privacy: it preserves the privacy via additive loW-dimensional Gaussian noise.
•	Communication: it reduces communication per round, since at each synchronization step, only a
sketched gradient of loWer-dimensional is communicated.
•	Convergence: it preserves the convergence rate of vanilla local gradient descent method.
Technical contribution: Our analysis technique is also of independent interest in the relating areas:
•	Unlike classical sketch-and-solve paradigm, our iterative sketch and de-sketch method can be
combined With gradient-based methods and extended to broader optimization problems.
•	We provide rigorous analysis on the impact of introducing sketching through coordinate-Wise
embedding, Which can be generalized to other areas (Song & Yu, 2021).
•	As a by-product, We give a novel linear convergence result of local GD under the strongly-convex
and smooth scheme.
2	Related Work
Federated Learning Federated learning (FL) is an emerging frameWork in distributed deep learn-
ing. FL alloWs multiple parties or clients collaboratively train a model Without data sharing. Fl
let the local client perform most of the computation and a central sever update the model parame-
ters through aggregation then transfers the parameters to local models (Dean et al., 2012; Shokri &
Shmatikov, 2015; McMahan et al., 2016; 2017). In this Way, the details of the data are not disclosed
in betWeen each party.
Unlike the standard parallel setting, FL has three unique challenge (Li et al., 2020a), including
communication cost, data heterogeneity and client robustness. In our Work, We focus on the first
tWo challenges. The training data are massively distributed over an incredibly large number of de-
vices, and the connection betWeen the central server and a device is sloW. A direct consequence
is the sloW communication, Which motivated communication-efficient FL algorithm. Federated av-
erage (FedAvg) (McMahan et al., 2017) firstly addressed the communication efficiency problem
by introducing a global model to aggregate local stochastic gradient descent updates. Later, dif-
ferent variations and adaptations have arisen. This encompasses a myriad of possible approaches,
including developing better optimization algorithms (Wang et al., 2020a) and generalizing model to
heterogeneous clients under special assumptions (Zhao et al., 2018; Kairouz et al., 2019; Li et al.,
2021).
2
Under review as a conference paper at ICLR 2022
Local GD and Local SGD To seek the communication efficiency of Federated learning, local
SGD has been proposed (Konecny et al., 2016; McMahan et al., 2017), where each client does a few
SGD iterations locally before the server averages the local estimators. Different variants of local
SGD algorithms has been explored, including with momentum (Yu et al., 2019b; Wang et al., 2018),
with quantization (Basu et al., 2019; Reisizadeh et al., 2020), and with various variance-reduction
methods (Liang et al., 2019; Karimireddy et al., 2020). Convergence analysis for local SGD mainly
focuses on two regimes: identical data regime (Stich, 2018; Basu et al., 2019; Stich & Karimireddy,
2019; Haddadpour & Mahdavi, 2019; Khaled et al., 2020) and heterogeneous data regime (Jiang &
Agrawal, 2018; Yu et al., 2019a; Basu et al., 2019; Haddadpour & Mahdavi, 2019; Khaled et al.,
2019; 2020). In this work, we propose our framework based upon vanilla local GD and our analysis
focus on the heterogeneous data regime.
Sketching Sketching has many applications in numerical linear, such as linear regression, low-
rank approximation (Clarkson & Woodruff, 2013; Nelson & Nguyen, 2013; Meng & Mahoney,
2013; Boutsidis & Woodruff, 2014; Song et al., 2017; Andoni et al., 2018), distributed problems
(Woodruff & Zhong, 2016; Boutsidis et al., 2016), reinforcement learning Wang et al. (2020b),
tensor decomposition (Song et al., 2019), clustering (Esfandiari et al., 2021), cutting plane method
(Jiang et al., 2020), generative adversarial networks (Xiao et al., 2018) and linear programming (Lee
et al., 2019; Jiang et al., 2021; Song &Yu, 2021).
Notations For a positive integer n, we use [n] to denote the set {1, 2,…，n}. We use E[∙] to
denote expectation (if it exists), and use Pr[∙] to denote probability. For a vector x, we use ∣∣χ∣2 :=
(Pin=1 xi2)1/2 to denote its `2 norm. We denote 1{x=l} for l ∈ R to be the indicator function which
equals to 1 if x = l and 0 otherwise. Let f : A → B and g : C → A be two functions, we use
f ◦ g to denote the composition of functions f and g, i.e., for any x ∈ C, (f ◦ g)(x) = f (g(x)). We
denote Id to be the identity mapping.
3	Problem Setup
Consider a federated learning scenario with N clients and corresponding local losses fc : Rd → R,
our goal is to find
min f (X) := W X fc(X)
x∈Rd	N
c=1
(1)
In this work, we consider the following classical convex and smooth setting for our objectives.
Assumption 3.1. Assume that the set ofminimizers of (1) is nonempty. Each f is μ-strongly convex
for μ ≥ 0 and L-smooth. That is, for all x,y e Rd,
2l∣y - χk2 ≤ fc(y) - fc(x) + hy - χ, Vfc(X)i ≤ Ll∣y - χk2.
Note in the case μ = 0, this assumption reduces back to convexity and smoothness.
Apart from the above assumption, we allow local losses to have arbitrary heterogeneity. In other
words, we allow fc’s to be arbitrary functions.
4	Our Algorithm
In this section, we propose a federated learning framework that addresses the communication effi-
ciency issue. When the learning gradients are of high dimension, classical federated learning frame-
work which sends the exact gradient could incur a heavy communication cost per round. Sketching
technique, which emerges as an effective way to reduce the dimension of vector while preserving
significant amount of information (Sarl6s, 2006; Woodruff, 2014), is highly preferred in this setting.
It enables us to compress the gradient vector into a lower dimension while preserving convergence
rates, and greatly saves the communication cost per round.
Motivated by above discussion, we propose the iterative sketching-based federated learning algo-
rithm, which builds upon vanilla local gradient descent: we start with a predetermined sequence of
3
Under review as a conference paper at ICLR 2022
independent sketching matrices shared across all clients. In each round, local clients accumulate
and sketch its change over K local steps, then transmit the low-dimensional sketch to the server.
Server then averages the sketches and transmits them back to all clients. Upon receiving, each client
de-sketches to update the local model.
Algorithm 1 Iterative Sketching-based Federated Learning Algorithm with K local steps
1	: procedure ITERATIVESKETCHINGFL
2	:	Each client initializes w0 using the same set of random seed
3	:	for t = 1 → T do	. T denotes the total number of global steps
4	:	/* Client */
5	:	parfor c = 1 → N do	. N denotes the total number of clients
6	:	if t = 1 then
7	uC，0 J w0	. uC，0 ∈ Rd
8	:	else
9	:	utc,0 J wt-1 + deskt(∆wet-1)	. deskt : Rbsketch → Rd de-sketch the change
10	:	end if
11	:	wt J utc,0
12	:	for k = 1 → K do
13	uC,k — UckT- ηiocai ∙ Vfc(W)Iw=uc,k-l
14	:	end for
15	:	∆wc(t) J utc,K - wt
16	:	Client c sends skt(∆wc(t)) to server	. skt : Rd → Rbsketch sketch the change
17	:	end parfor
18	:	/* Server */
19	∆W J global 二 NN PN=i Skt (∆Wc(t))	. ∆Wt ∈ Rd
20	:	Server sends ∆wet to each client
21	:	end for
22	: end procedure
We highlight several distinct features of our algorithm:
•	Communication: In each sync step, we only communicates a low-dimensional sketched gradients,
indicating a smaller communication cost per round. This property is particularly valuable in a small-
bandwidth setting.
•	De-sketch:i: We emphasize that unlike the classical sketch-and-solve paradigm that decreases the
problem dimension, our algorithm applies sketching in each round, combined with a de-sketching
process which recovers back to the true gradient dimension.
•	Simple server task:: Server only needs to do simple averaging, indicating no need of a trustworthy
party as the server.
•	Decentralization:: Our algorithm can be generalized to decentralized learning settings, where
local clients can only communicate with neighboring nodes. In this case, it requires O(diam) rounds
to propagate the sketched local changes, where diam is the diameter of the network graph.
4.1	sk/desk VIA COORDINATE WISE EMBEDDING
In this section, we discuss the concrete realization of the skt/deskt operators in Algorithm 1 through
random sketching matrices. Note we should require any processed gradient deskt ◦ skt (g) to "be
close" to the true gradient g to avoid breaking the convergence property of the algorithm. To achieve
this, we first introduce the following property for a broad family of sketching matrices, namely
coordinate-wise embedding (Jiang et al., 2021; Song & Yu, 2021), that naturally connects with
skt /deskt operators.
Definition 4.1 (a-coordinate-wise embedding). We say a randomized matrix R ∈ Rbsketch ×d satis-
fying a -coordinate wise embedding if for any vector g,h ∈ Rd, we have ER 〜∏[h> R>Rg] = h>g
and ER〜∏[(h>R>Rg)2] ≤ (h>g)2 + bɪ- ∣∣h∣∣2 ∙ kgk2∙
sketch
iWe elaborate the difference of our iterative sketch and de-sketch approach compared to classical sketch-
and-solve approaches in Section 4.2.
4
Under review as a conference paper at ICLR 2022
In general, well-known sketching matrices have their coordinate-wise embedding parameter a being
a small constant (See Section D). Note that if we choose h to be one-hot vector ei , then the above
conditions translate to ER〜∏[R>Rg] = g and ER〜∏[∣∣R>Rgk2] ≤ (1 + a ∙ b-d~^) ∙ ∣∣g∣∣2∙
This implies that by choosing
skt = Rt ∈ Rbsketch×d (sketching), deskt = Rt> ∈ Rd×bsketch (de-sketching) (2)
for any iteration t ≥ 1, where Rt ’s are independent random matrices with sketching dimension
bsketch, we obtain an unbiased sketching/de-sketching scheme with bounded variance as state in the
following Theorem 4.2.
Theorem 4.2. Let skt and deskt be defined by Eq. (2) using a sequence of independent sketching
matrices Rt ∈ Rbsketch ×d satisfying a-coordinate wise embedding property (Def. 4.1). Then the
following properties hold:
1.	Independence: For different iterations, (skt, deskt)’s are independent of each other.
2.	Linearity: Both skt and deskt are linear operators.
3.	Unbiased estimator: For any fixed vector h ∈ Rd, it holds E[deskt(skt(h))] = h.
4.	Bounded second moment: For any fixed vector h ∈ Rd, it holds E[kdeskt(skt(h))k22] ≤
(1 + a ∙ d/bSketCh) ∙ IlhIl2∙
We will use the above property to instantiate the convergent proof and communication complexity
in section 5. We remark that unlike traditional sketching matrix R, one can intuitively think of
matrix R> as a “de-sketch” matrix, it undoes sketching and recovers the sketched vector to original
dimension.
4.2	Sketch-and-Solve vs Sketch-and-De-sketch
In this section, we provide a brief discussion of the difference between classical sketch-and-solve
paradigm (Clarkson & Woodruff, 2013; Woodruff, 2014) and our sketch-and-de-sketch scheme.
The general idea of sketch-and-solve is to apply sketching matrix R to the entire problem, and use
certain black-box algorithm to solve the sketched-down low-dimensional version of the problem.
Intuitively, sketching preserves the structure of the problem, therefore, the same algorithm can be
exploited on a smaller problem, and to achieve a 1 ± guarantee. One downside of this method is
the problem that is applicable needs to have certain structures. For example, it is not clear that given
a non-convex objective, directly applying sketching will do any help.
In contrary, our sketch-and-de-sketch scheme only applies sketching to certain key component of
a problem, for example, the gradient in a gradient descent method. Unlike sketch-and-solve, using
sketch and de-sketch, we preserve the structure of the algorithm. This makes it feasible to a wider
range of applications, such as showing the progress of gradient descent on non-convex objective, as
we will show in appendix E and G. One drawback of this scheme is it gives a worse approximation
guarantee (1 ± O( ^^)), but in gradient descent, this can be mitigated via choosing a smaller
stepsize for compensation. In a distributed setting, where privacy of communicated message and its
size are main concerns, sketch-and-de-sketch is particularly valuable.
4.3	Privacy-preserved sketching via low-dimensional noises
In this section, we discuss how to add low-dimensional Gaussian noise to make Algorithm 1 differ-
ential private. Consider a gradient vector gc generated via training of a client c, and another vector gc0
that differs from gc by exactly one entry. Further, we assume this difference has a bounded magni-
tude of γ in terms of absolute value. The goal is to protect the sketChed gradient, Rgc . Our strategy
is as follows: on line 16 of Algorithm 1, We add a random vector ∆c ∈ RbSketCh where each entry of
△c is drawn from N(0,σ2) and with σ ≥ Ω(dd/bsketChYn-I ,log(1∕δ)),then this modification on
line 16 will produce an (, δ)-differential private guarantee for the sketched gradient.
We remark this is the most important step to preserve the privacy of the federated learning algorithm,
since potential attackers might hack into a single client and have access to the skt/deskt operators for
5
Under review as a conference paper at ICLR 2022
each iteration. If they can further observe the communicated gradients from other clients, then the
power of random masking with sketching is almost diminished, since they intuitively, they can de-
sketch the sketched changes to obtain useful information. After adding low-dimensional Gaussian
noises, attackers can no longer obtain useful information even they have the skt/deskt operators,
which significantly improves the privacy of our system.
From an analytical perspective, adding this noise does not affect our analysis too much — since it’s
sampled from a zero-mean Gaussian distribution, our estimator R> (Rg + ∆c) is still an unbiased
estimator of vector g, it merely adds a variance term which can be factored into the variance of our
un-modified estimator. Hence, in the analysis section, we present the theorems and proofs for the
scenario where noises are not added. Similar analysis can be adapted to additive noise version.
5	Convergence Theory and Communication Complexity
In this section, we analyze the convergence property of our proposed framework for smooth and
convex objectives. Note the similarity shared by our framework and classical federated learning
algorithms. We try to follow the existing analysis and focus on discussing the impact of the intro-
duced randomness due to the sketching and de-sketching. We will show our approach enjoys benign
convergence property and does not increase total communication at all.
5.1	Single-step scheme
To start off, we consider a simple scenario where the number of local steps K = 1. In this case, by
the linearity of skt and deskt operators, the update rule can be written as
wt+1 = Wt — η ∙ deskt(skt(Vf (wt))).	(3)
We remark that in the case of skt and deskt being identity mappings, our framework is equivalent
to a distributed implementation of the vanilla gradient descent algorithm. Therefore, we follow the
classical analysis of gradient descent and have the following key lemma:
Lemma 5.1. IfAssumption 3.1 holds and K = 1. Denote η := niocai ∙ ngiobai. Then we have
E[kwt+1 — W*k2] ≤ (1 — ημ)E[kwt — W*k2] — 2η(1 - η(1 + α)L) ∙ E[f(wt) — f(w*)].
where w* is a minimizer ofproblem (1).
Proof. Note the update rule (3) implies
kwt+1 — w*k22 = kwt — w*k22 — 2ηhwt — w*, deskt(skt(Vf(wt)))i + η2kdeskt(skt(Vf(wt)))k22.
Taking the conditional expectation over last synchronization, we have by Theorem 4.2,
E[hwt — w*, deskt(skt(Vf(wt)))i | Ft] = hwt — w*, Vf(wt)i,
E[kdeskt(skt(Vf(wt)))k22 |Ft] ≤ (1+α)kVf(wt)k22,
Combining with μ-strongly convexity and L-Smoothness of f, We obtain
E[kwt+1 — w*k22 |Ft] ≤ kwt — w*k22 — 2ηhwt — w*, Vf(wt)i + η2(1 + α)kVf(wt)k22
≤ (I-ημ)kwt — w*k2 - 2η(I- η(I + α)L) Yf(Wt)- f(w*))∙
Taking the expectation of both sides over Ft We complete the proof.	□
Lemma 5.1 implies the introduced randomness from gradients only influence the second order term
by a multiplicative factor 1 +α in expectation. Therefore, by scaling doWn the stepsize by a factor of
1 + α, We can obtain the exact same convergence guarantee as the vanilla gradient descent algorithm.
For the strongly convex and smooth objective case, We obtain the folloWing linear convergence.
Theorem 5.2. If Assumption 3.1 holds with μ > 0. Let K = 1 and η := niocai ∙ ngiobai ≤ (i+α)L,
then we have E[f(wτ)—f(w*)] ≤ LL E[∣∣w0 — w*k2]e-ημT, where w* isa minimizer ofproblem (1).
6
Under review as a conference paper at ICLR 2022
Proof. By the choice of stepsize and Lemma 5.1, we obtain
E[kwT - w*k2] ≤ (i-ημ)TE[kw0 - w*k2]∙
Therefore, we have
E[f(wT) - f(w*)] ≤ 2 E[kwT - w*k2] ≤ 2 E[kw0 - w*k2]e-ημT
□
For the convex case, we consider the average of iterations and obtain the sublinear convergence.
Theorem 5.3. IfAssumPtion 3.1 holds with μ = 0. Let K = 1 and η := niocai ∙ ngiobai ≤ 2(，a)L，
then we have f (WT) — f (w*) ≤ 爪六])E[∣∣w0 — w*k2]. where w* is a minimizer of problem (1)
and wτ = τ+ι PT=0 wt.
Proof. By the choice of stepsize and Lemma 5.1, we obtain
η(f(wt) - f(w*)) ≤ E[kwt - w*k22] - E[kwt+1 -w*k22].
Take the telescope summation over t = 0,…，T, we have
T1	1
手f(wt) - f(w*)) ≤ -(E[kw0 - W*k2] - E[kwτ +1 - W*k2]) ≤ η E[kw0 - W*k2].
By the convexity of f we complete the proof.
□
We point out that in the case of skt and deskt being identity mappings, the parameter α reduces back
0 and Theorem 5.2 and 5.3 matches the convergence property of vanilla gradient descent exactly.
Further, above convergence results imply that comparing to vanilla gradient descent, our approach
needs to shrink the stepsize by a factor of O(α), thus enlarge the number of iterations by a factor
of O(α) to achieve desired accuracy. Therefore, using sketching matrices with dimension bsketch,
ours communicates O(bsketch∕d ∙ α) as many bits in total compared to vanilla approaches. According
to Theorem 4.2, we have α = O(d/bsketch) for commonly used sketching matrices, implying our
approach does not introduce extra communication cost at all.
5.2	Multi-step scheme
Now we are ready to move on to general K local step scheme. In this section, we assume ηglobal = 1.
For notation simplicity, we denote utc,-1 = utc-1,K-1 for t ≥ 2. We also introduce the following
notations of the average iterates, iterates variance, local gradients and average gradients to help with
the analysis.
1N	1N	1N
ut,k := NN XUck,	Vt,k := NN X kuC,k - ut,kk2,	gt,k := Vfc(Uck), gt,k := NN Xgt,k
c=1	c=1	c=1
Using the new set of notations, one key observation is the average iterates satisfies
K-1
Ut,k = Ut,k-1 - niocai ∙ gt,k-1 + 1{k=0} ∙ niocai ∙ (Id - deskt ◦ skt)( X gt-1,i), ∀(t, k) = (1,0)
i=0
We remark again if skt and deskt being identity mappings, our updates reduce back to the vanilla
local gradient descent algorithm with K local steps.
By considering the distance to optimal solution ∣∣Ut,k - w*k2, We have the following intermediate
lemma parallel to Lemma 5.1. Due to the space limitation, we defer the proof to appendix, see
Lemma F.6.
Lemma 5.4. IfAssumption 3.1 holds and n := niocai ≤ ~L, then we have
E[∣Ut,k - w*k2] ≤ (1 - μn) E[kut,k-1 - w*k2] - nE[f (Ut,k-1) - f (w*)] + 1.5ηLE[Vt,k-1]
K-1	K-1
+ 1{k=o}η2αK ∙(4L X E[f (Ut-1,i) - f(w*)] + 2L2 X E[Vt-1,i])
i=0	i=0
for any (t, k) 6= (1, 0), where w* is a minimizer of problem (1).
7
Under review as a conference paper at ICLR 2022
Follow upon the above lemma, next step is to capture the quantity of the iterates variance Vt,k, we
observe that in each round, We start off with Vt,0 = 0 due to synchronization uC,0 = ut,0. Then
Vt,k can be viewed as the accumulation of the variance of the next k local updates,
N	k-1
V t,k=N X kuc，0 - x
c=1
i=0
k-1	2 N k-1
niocai ∙ gt,i -ut,0 + Xηiocal ∙gt,ik2 = W X k X(gt,i - gt,i)k2
i=0	c=1	i=0
Therefore, it naturally requires us to characterize V t,k through certain measure of dissimilarity of
local gradients. To achieve so, we avoid the common Lipschitz assumption which only gives a blur
upper bound of the difference of local gradients, but follow the approach of Khaled et al. (2019;
2020), which focus on the quantity
N
σ2=f N X kVfc(w*)k2 > 0
c=1
that is always finite and naturally characterize the degree of heterogeneity of local objectives. And
we have the following observation of the summation of iterates variance over a single round:
Lemma 5.5. IfAssumption 3.1 holds and niocai ≤ 8⅛. Thenfor any t ≥ 0,
K-1	K-1
X Vt,k ≤ 8ni2ocalLK2 X (f (ut,k ) - f (w*)) +4n2ocalK”
k=0	k=0
Combining Lemma 5.5 and Lemma 5.4, we notice that by choosing appropriately small stepsize,
the first term in upper-bounding Vt,k can be absorbed when we consider the average iterates over
a single round. Therefore, We can obtain the following convergence result for the strongly convex
and smooth losses.
Theorem 5.6. If Assumption 3.1 holds with μ > 0. If niocai ≤
1
8(1+α)LK,
E[f(wT+1) - f(w*)] ≤ 2 E[kw0 - w*k2]e-μηlocalT +4niocaiL2K3σ2∕μ.
where w* is a minimizer OfProblem (1).
Proof. Telescoping sum up Lemma 5.4 as k varies from 0 to K - 1and absorb the higher-order
terms by the choice of the stepsize, we have for any t ≥ 1,
K-1	K-1
E[kut+1,0 - w*k2] + X E[kut,k - w*k2] ≤ (1 - μniocai) X E[kut,k - w*k2]) + 8n3ocaiLK3σ2.
k=1	k=0
Rearranging the terms, we obtain for any t ≥ 1,
E[kut+1,° - w*k2] ≤ (1 - μniocai) E[kut,0 - w*k2] + 8n黑aiLK3σ2,
implying
E[kwT+1 - w*k2] ≤ E[kw0 - w*k2]e-μηlocalT + 8n2ocaiLK3σ2∕μ.
We conclude by the L-smoothness of function f.	□
Corollary 5.7. If Assumption 3.1 holds with μ > 0. Then within Algorithm 1 out-
puts an -optimal solution wT ∈ Rd satisfying E[f (wT) - f(w*)] ≤ by using
O((LN∕μ) max(d, ,σ2∕(μc)} log(L E[∣∣w0 — w*∣∣2]∕e)) bits of communication cost.
We observe the same phenomenon as in the single-step scheme again, that compared to vanilla
approaches, ours shrinks the stepsize by a factor of O(α), thus enlarge the number of rounds ap-
proximately by a factor of O(α). Since our approach only communicates O(bsketch ∕d) as many bits
per round due to sketching, the total communication cost does not increase at all for commonly used
sketching matrices, according to Theorem 4.2.
We also point out that when e ≥ σ2∕(μd2), our analysis implies a linear convergence rate of local
GD under only strongly-convex and smooth assumptions, which is new as far as we concern. We
also have a similar observation in the convex losses case, as shown in the below theorem.
8
Under review as a conference paper at ICLR 2022
Theorem 5.8. If Assumption 3.1 holds with μ = 0. If niocai ≤
1
8(1+α)LK,
κr t-T∖	" *、] / 4 E[kw0 - w* k2] I Q9 2	2 2^ll 2
E[f (W ) - f (W )] ≤	^	+3	+ 32ηlocalLK σ ,
ηlocal KT
where wt = KT (PT=I PK=o1 ut,k) is the average over parameters throughout the execution of
Algorithm 1.
Proof. Telescoping sum up Lemma 5.4 as t varies from 0 to T - 1 and k varies from 0 to K - 1 and
absorb the higher-order terms by the choice of the stepsize,
T	K-1
E[kuτ +1,0 - W*k2] - E[kw0 - W*k2] ≤ - - niocai X X E[f(ut,k) - f (W*)] + 8η 黑aiLK 3Tσ2.
t=1 k=0
Rearranging the above equation, we have
KT XXIE[f(ut,k) - f(w*)] ≤ 4E[kW° KW*k2] + 32n2ocaiLK2σ2.
KT t=1 k=0	ηiocaiKT
By the convexity of f We complete the proof.	□
Corollary 5.9. IfAssumption 3.1 holds with μ = 0. Then Algorithm 1 outputs an e-optimal solution
Wτ ∈ Rd satisfying E[f(Wτ) 一 f (w*)] ≤ e by using O(E[∣∣w0 一 w*k2]Nmax{Ld∕e, σ√L∕e3∕2})
bits of communication cost.
We compare our communication cost With the Work of Khaled et al. (2019), Which analyzes the
local gradient descent using the same assumption and frameWork. The result of Khaled et al. (2019)
shoWs a communication cost of
O E[kw0
一 w*k2]Ndmax{LL, (L√}), which is strictly no less
than our results. This shoWs again our approach does not introduce extra overall communication
cost.
6 Discussion
In this work, we propose the iterative sketch-based federated learning framework, which only com-
municates the sketched gradients with noises. Such framework enjoys the benefits of both better
privacy and lower communication cost per round. We also rigorously prove that the randomness
from sketching will not introduce extra overall communication cost.
Though our framework is built upon the local gradient descent algorithm and our theoretical discus-
sion follows the analysis framework of Khaled et al. (2019; 2020), we emphasize that our approach
and results can be extended to other gradient-based optimization algorithms and analysis, including
but not limited to gradient descent with momentum and local stochastic gradient descent. The key
reason is due to the sketched and de-sketched gradient R>Rg is an unbiased estimator of the true
gradient g with second moments being a multiplier of kgk22 . As the iterates approach the optimal
solution, the second moments approaches 0 correspondingly, resulting an exact match of the vanilla
approach. Therefore, by scaling down the stepsize appropriately, we are able to recover the same
convergence guarantee as the original gradient-based algorithms.
By a simple modification to our algorithm with additive Gaussian noises on the sketched gradients,
we can also prove the differential privacy of our learning system by “hiding” the most important
component in the system for guarding the safety and privacy. This additive noise also does not
affect the convergence behavior of our algorithm too much, since it does make the estimator biased,
and the additive variance can be factored into our original analysis.
Despite the benefits in terms of privacy and communication cost, we point out our approach does
have trade-off on computation complexity. Since we need to shrink stepsize and thus enlarge the
number of iterations to achieve certain accuracy level, our approach requires O(d∕bsketch) as many
computational cost compared to vanilla approaches. However, in a privacy-and-communication
focused distributed learning scenario, we hope this work provides a new solution and motivates
future works.
9
Under review as a conference paper at ICLR 2022
Ethics Statement. This paper mainly focuses on the theoretical perspective of federated learning,
and it proposes algorithm to improve the privacy of the learning system.
Reproducibility Statement. This paper contains several theoretical results, for discussions related
to sketching, we refer readers to section D, for discussions related to the convergence analysis and
communication costs of Algorithm 1, we refer readers to section F and G. For discussions related to
differential privacy, we refer readers to section H.
References
Nir Ailon and Bernard Chazelle. Approximate nearest neighbors and the fast johnson-lindenstrauss
transform. In STOC, pp. 557—-563, 2006.
Noga Alon, Yossi Matias, and Mario Szegedy. The space complexity of approximating the frequency
moments. Journal ofComputer and system sciences, 58(1):137-147, 1999.
Alexandr Andoni, Chengyu Lin, Ying Sheng, Peilin Zhong, and Ruiqi Zhong. Subspace embedding
and linear regression with orlicz norm. In International Conference on Machine Learning (ICML),
pp. 224-233. PMLR, 2018.
Giuseppe Ateniese, Luigi V Mancini, Angelo Spognardi, Antonio Villani, Domenico Vitali, and
Giovanni Felici. Hacking smart machines with smarter ones: How to extract meaningful data
from machine learning classifiers. International Journal of Security and Networks, 10(3):137-
150, 2015.
Debraj Basu, Deepesh Data, Can Karakus, and Suhas Diggavi. Qsparse-local-sgd: Distributed sgd
with quantization, sparsification, and local computations. arXiv preprint arXiv:1906.02367, 2019.
Sergei Bernstein. On a modification of chebyshev’s inequality and of the error formula of laplace.
Ann. Sci. Inst. Sav. Ukraine, Sect. Math, 1(4):38-49, 1924.
Leon Bottou, Frank E. Curtis, and Jorge NocedaL Optimization methods for large-scale machine
learning, 2018.
Christos Boutsidis and David P Woodruff. Optimal cur matrix decompositions. In Proceedings of
the 46th Annual ACM Symposium on Theory of Computing (STOC), pp. 353-362. ACM, https:
//arxiv.org/pdf/1405.7910, 2014.
Christos Boutsidis, David P Woodruff, and Peilin Zhong. Optimal principal component analysis in
distributed and streaming models. In Proceedings of the forty-eighth annual ACM symposium on
Theory of Computing (STOC), pp. 236-249, 2016.
Moses Charikar, Kevin Chen, and Martin Farach-Colton. Finding frequent items in data streams.
In International Colloquium on Automata, Languages, and Programming, pp. 693-703. Springer,
2002.
Mingzhe Chen, Zhaohui Yang, Walid Saad, Changchuan Yin, H Vincent Poor, and Shuguang Cui.
A joint learning and communications framework for federated learning over wireless networks.
IEEE Transactions on Wireless Communications, 2020.
Herman Chernoff. A measure of asymptotic efficiency for tests of a hypothesis based on the sum of
observations. The Annals of Mathematical Statistics, pp. 493-507, 1952.
Kenneth L. Clarkson and David P. Woodruff. Low rank approximation and regression in input
sparsity time. In Symposium on Theory of Computing Conference, STOC’13, Palo Alto, CA, USA,
June 1-4, 2013, pp. 81-90. https://arxiv.org/pdf/1207.6365, 2013.
Jeffrey Dean, Greg Corrado, Rajat Monga, Kai Chen, Matthieu Devin, Mark Mao, Marc’aurelio
Ranzato, Andrew Senior, Paul Tucker, Ke Yang, et al. Large scale distributed deep networks. In
Advances in neural information processing systems, pp. 1223-1231, 2012.
Cynthia Dwork, Krishnaram Kenthapadi, Frank McSherry, Ilya Mironov, and Moni Naor. Our data,
ourselves: Privacy via distributed noise generation. In Annual International Conference on the
Theory and Applications of Cryptographic Techniques, pp. 486-503. Springer, 2006a.
10
Under review as a conference paper at ICLR 2022
Cynthia Dwork, Frank McSherry, Kobbi Nissim, and Adam Smith. Calibrating noise to sensitivity
in private data analysis. In Theory Ofcryptography conference, pp. 265-284. Springer, 2006b.
Hossein Esfandiari, Vahab Mirrokni, and Peilin Zhong. Almost linear time density level set estima-
tion via dbscan. In AAAI, 2021.
Sergey Foss, Dmitry Korshunov, and Stan Zachary. An introduction to heavy-tailed and subexpo-
nential distributions, volume 6. Springer, 2011.
Matt Fredrikson, Somesh Jha, and Thomas Ristenpart. Model inversion attacks that exploit confi-
dence information and basic countermeasures. In Proceedings of the 22nd ACM SIGSAC Confer-
ence on Computer and Communications Security, pp. 1322-1333, 2015.
Jonas Geiping, Hartmut Bauermeister, Hannah Droge, and Michael Moeller. Inverting gradients-
how easy is it to break privacy in federated learning? arXiv preprint arXiv:2003.14053, 2020.
Oana Goga and Renata Teixeira. Speed measurements of residential internet access. In Nina Taft
and Fabio Ricciato (eds.), Passive and Active Measurement, pp. 168-178, Berlin, Heidelberg,
2012. Springer Berlin Heidelberg. ISBN 978-3-642-28537-0.
Uffe Haagerup. The best constants in the khintchine inequality. Studia Mathematica, 70(3):231-
283, 1981. URL http://eudml.org/doc/218383.
Farzin Haddadpour and Mehrdad Mahdavi. On the convergence of local descent methods in feder-
ated learning. arXiv preprint arXiv:1910.14425, 2019.
David Lee Hanson and Farroll Tim Wright. A bound on tail probabilities for quadratic forms in
independent random variables. The Annals of Mathematical Statistics, 42(3):1079-1083, 1971.
Wassily Hoeffding. Probability inequalities for sums of bounded random variables. Journal of the
American Statistical Association, 58(301):13-30, 1963.
Haotian Jiang, Yin Tat Lee, Zhao Song, and Sam Chiu-wai Wong. An improved cutting plane
method for convex optimization, convex-concave games and its applications. In STOC, 2020.
Peng Jiang and Gagan Agrawal. A linear speedup analysis of distributed deep learning with sparse
and quantized communication. In Proceedings of the 32nd International Conference on Neural
Information Processing Systems, pp. 2530-2541, 2018.
Shunhua Jiang, Zhao Song, Omri Weinstein, and Hengjie Zhang. Faster dynamic matrix inverse for
faster lps. In STOC. arXiv preprint arXiv:2004.07470, 2021.
Peter Kairouz, H Brendan McMahan, Brendan Avent, AUrelien Bellet, Mehdi Bennis, Arjun Nitin
Bhagoji, Keith Bonawitz, Zachary Charles, Graham Cormode, Rachel Cummings, et al. Advances
and open problems in federated learning. arXiv preprint arXiv:1912.04977, 2019.
Sai Praneeth Karimireddy, Satyen Kale, Mehryar Mohri, Sashank Reddi, Sebastian Stich, and
Ananda Theertha Suresh. Scaffold: Stochastic controlled averaging for federated learning. In
International Conference on Machine Learning, pp. 5132-5143. PMLR, 2020.
Krishnaram Kenthapadi, Aleksandra Korolova, Ilya Mironov, and Nina Mishra. Privacy via the
johnson-lindenstrauss transform. Journal of Privacy and Confidentiality, 2013.
Ahmed Khaled, Konstantin Mishchenko, and Peter Richtðrik. First analysis of local gd on hetero-
geneous data. arXiv preprint arXiv:1909.04715, 2019.
Ahmed Khaled, Konstantin Mishchenko, and Peter Richtdrik. Tighter theory for local sgd on identi-
cal and heterogeneous data. In International Conference on Artificial Intelligence and Statistics,
pp. 4519-4529. PMLR, 2020.
Aleksandr Khintchine. Uber dyadische bruche. Mathematische Zeitschrift,18(1):109-116,1923.
Jakub Konecny, H Brendan McMahan, Felix X Yu, Peter Richtdrik, Ananda Theertha Suresh, and
Dave Bacon. Federated learning: Strategies for improving communication efficiency. arXiv
preprint arXiv:1610.05492, 2016.
11
Under review as a conference paper at ICLR 2022
Beatrice Laurent and Pascal Massart. Adaptive estimation of a quadratic functional by model selec-
tion. Annals of Statistics ,pp. 1302-1338, 2000.
Yin Tat Lee, Zhao Song, and Qiuyi Zhang. Solving empirical risk minimization in the current matrix
multiplication time. In COLT, 2019.
Tian Li, Anit Kumar Sahu, Ameet Talwalkar, and Virginia Smith. Federated learning: Challenges,
methods, and future directions. IEEE Signal Processing Magazine, 37(3):50-60, 2020a.
Wenqi Li, FaUsto Milletari, Daguang Xu, Nicola Rieke, Jonny Hancox, Wentao Zhu, Maximilian
Baust, Yan Cheng, Sebastien Ourselin, M Jorge Cardoso, et a.Privacy-preserving federated brain
tumour segmentation. In International Workshop on Machine Learning in Medical Imaging, pp.
133-141. Springer, 2019.
Xiaoxiao Li, Yufeng Gu, Nicha Dvornek, Lawrence Staib, Pamela Ventola, and James S Dun-
can. Multi-site fmri analysis using privacy-preserving federated learning and domain adaptation:
Abide results. Medical Image Analysis, 2020b.
Xiaoxiao Li, Meirui Jiang, Xiaofei Zhang, Michael Kamp, and Qi Dou. FedBN: Federated learn-
ing on non-IID features via local batch normalization. In International Conference on Learning
Representations (ICLR), 2021. URL https://arxiv.org/abs/2102.07623.
Xianfeng Liang, Shuheng Shen, Jingchang Liu, Zhen Pan, Enhong Chen, and Yifei Cheng. Variance
reduced local sgd with lower communication complexity. arXiv preprint arXiv:1912.12844, 2019.
Yichao Lu, Paramveer Dhillon, Dean P Foster, and Lyle Ungar. Faster ridge regression via the sub-
sampled randomized hadamard transform. In Advances in neural information processing systems,
pp. 369-377, 2013.
Brendan McMahan, Eider Moore, Daniel Ramage, Seth Hampson, and Blaise Aguera y Arcas.
Communication-efficient learning of deep networks from decentralized data. In Artificial Intelli-
gence and Statistics, pp. 1273-1282. PMLR, 2017.
H. McMahan, Eider Moore, Daniel Ramage, and Blaise Aguera y Areas. Federated learning of deep
networks using model averaging. 02 2016.
Xiangrui Meng and Michael W Mahoney. Low-distortion subspace embeddings in input-sparsity
time and applications to robust linear regression. In Proceedings of the forty-fifth annual ACM
symposium on Theory of computing (STOC), pp. 91-100, 2013.
Jelani Nelson and Huy L Nguyen. Osnap: Faster numerical linear algebra algorithms via sparser
subspace embeddings. In 2013 IEEE 54th Annual Symposium on Foundations of Computer Sci-
ence (FOCS), pp. 117-126. IEEE, https://arxiv.org/pdf/1211.1002, 2013.
Amirhossein Reisizadeh, Aryan Mokhtari, Hamed Hassani, Ali Jadbabaie, and Ramtin Pedarsani.
Fedpaq: A communication-efficient federated learning method with periodic averaging and quan-
tization. In International Conference on Artificial Intelligence and Statistics, pp. 2021-2031.
PMLR, 2020.
Mark Rudelson and Roman Vershynin. Hanson-wright inequality and sub-gaussian concentration.
Electronic Communications in Probability, 18, 2013.
Tamgs Sarl6s. Improved approximation algorithms for large matrices via random projections. In
Proceedings of 47th Annual IEEE Symposium on Foundations of Computer Science (FOCS),
2006.
Reza Shokri and Vitaly Shmatikov. Privacy-preserving deep learning. In Proceedings of the 22nd
ACM SIGSAC conference on computer and communications security, pp. 1310-1321. ACM,
2015.
Zhao Song and Zheng Yu. Oblivious sketching-based central path method for solving linear pro-
gramming problems. In 38th International Conference on Machine Learning (ICML), 2021.
12
Under review as a conference paper at ICLR 2022
Zhao Song, David P Woodruff, and Peilin Zhong. LoW rank approximation with entrywise '1 -norm
error. In Proceedings of the 49th Annual Symposium on the Theory of Computing (STOC), 2017.
Zhao Song, David P Woodruff, and Peilin Zhong. Relative error tensor low rank approximation. In
SODA. arXiv preprint arXiv:1704.08246, 2019.
Sebastian U Stich. Local sgd converges fast and communicates little. arXiv preprint
arXiv:1805.09767, 2018.
Sebastian U Stich and Sai Praneeth Karimireddy. The error-feedback framework: Better rates for
sgd with delayed gradients and compressed communication. arXiv preprint arXiv:1909.05350,
2019.
Joel A Tropp. Improved analysis of the subsampled randomized hadamard transform. Advances in
Adaptive Data AnaIySiS, 3(01n02):115-126, 2011.
Hongyi Wang, Mikhail Yurochkin, Yuekai Sun, Dimitris Papailiopoulos, and Yasaman Khazaeni.
Federated learning with matched averaging. arXiv preprint arXiv:2002.06440, 2020a.
Ruosong Wang, Peilin Zhong, Simon S Du, Russ R Salakhutdinov, and Lin F Yang. Planning
with general objective functions: Going beyond total rewards. In Annual Conference on Neural
Information ProceSSing SyStemS (NeurIPS), 2020b.
Shiqiang Wang, Tiffany Tuor, Theodoros Salonidis, Kin K Leung, Christian Makaya, Ting He, and
Kevin Chan. When edge meets learning: Adaptive control for resource-constrained distributed
machine learning. In IEEE INFOCOM 2018-IEEE Conference on Computer CommunicationS,
pp. 63-71. IEEE, 2018.
Zhibo Wang, Mengkai Song, Zhifei Zhang, Yang Song, Qian Wang, and Hairong Qi. Beyond
inferring class representatives: User-level privacy leakage from federated learning. In IEEE IN-
FOCOM 2019-IEEE Conference on Computer CommunicationS, pp. 2512-2520. IEEE, 2019.
David P. Woodruff. Sketching as a tool for numerical linear algebra. FoundationS and TrendS in
Theoretical Computer Science, 10(1-2):1-157, 2014.
David P Woodruff and Peilin Zhong. Distributed low rank approximation of implicit functions of a
matrix. In 2016 IEEE 32nd International Conference on Data Engineering (ICDE), pp. 847-858.
IEEE, 2016.
Chang Xiao, Peilin Zhong, and Changxi Zheng. Bourgan: generative networks with metric embed-
dings. In ProceedingS of the 32nd International Conference on Neural Information ProceSSing
SyStemS (NeurIPS), pp. 2275-2286, 2018.
Hao Yu, Rong Jin, and Sen Yang. On the linear speedup analysis of communication efficient mo-
mentum sgd for distributed non-convex optimization. In International Conference on Machine
Learning, pp. 7184-7193. PMLR, 2019a.
Hao Yu, Sen Yang, and Shenghuo Zhu. Parallel restarted sgd with faster convergence and less
communication: Demystifying why model averaging works for deep learning. In ProceedingS of
the AAAI Conference on Artificial Intelligence, volume 33, pp. 5693-5700, 2019b.
Yue Zhao, Meng Li, Liangzhen Lai, Naveen Suda, Damon Civin, and Vikas Chandra. Federated
learning with non-iid data. arXiv preprint arXiv:1806.00582, 2018.
Wenbo Zheng, Lan Yan, Chao Gou, and Fei-Yue Wang. Federated meta-learning for fraudulent
credit card detection. In ProceedingS of the Twenty-Ninth International Joint Conference on Arti-
ficial Intelligence (IJCAI), 2020.
Ligeng Zhu and Song Han. Deep leakage from gradients. In Federated Learning, pp. 17-31.
Springer, 2020.
13
Under review as a conference paper at ICLR 2022
Contents
1	Introduction	1
2	Related Work	2
3	Problem Setup	3
4	Our Algorithm	3
4.1	sk/desk via coordinate wise embedding ....................................... 4
4.2	Sketch-and-Solve vs Sketch-and-De-sketch .................................... 5
4.3	Privacy-preserved sketching via low-dimensional noises ...................... 5
5	Convergence Theory and Communication Complexity	6
5.1	Single-step scheme .......................................................... 6
5.2	Multi-step scheme ........................................................... 7
6	Discussion	9
A	Preliminary	16
B	Probability	16
C	Optimization Backgrounds	17
D	Sketching	18
D.1 Definition .................................................................. 18
D.2 Coordinate wise embedding ................................................... 19
D.3 Expectation and variance .................................................... 19
D.4 Bounding inner product ...................................................... 23
D.5 Infinite norm bound ......................................................... 25
E	Analysis of convergence: Single-step scheme	28
E.1 Preliminary ................................................................. 28
E.2 Strongly-convex f convergence analysis ...................................... 29
E.3 Convex f convergence analysis ............................................... 29
E.4 Non-convex f convergence analysis ........................................... 30
F	k-step convex & strongly-convex fc analysis	31
F.1 Preliminary ................................................................. 31
F.2 Unifying the update rule of Algorithm 1 ..................................... 32
F.3 Upper bounding ∣∣gt,k∣∣2 .................................................... 32
F.4 Lower bounding hut,k — w*,gt,k) ............................................. 33
14
Under review as a conference paper at ICLR 2022
F.5 Upper bounding variance within K local steps ................................... 34
F.6 Bounding the expected gap between ut,k and w* .................................. 35
F.7 Main result: convex case ....................................................... 36
F.8 Main result: strongly-convex case .............................................. 38
G k-step non-convex f convergence analysis	39
H Differential Privacy	42
15
Under review as a conference paper at ICLR 2022
Roadmap. We organize the appendix as follows. In section A, we introduce some notations and
definitions that will be used across the appendix. In section B, we study several probability tools
we will be using in the proof of cretain properties of various sketching matrices. In section C, we
lay out some key assumptions on local objective function fc and global objective function f, in
order to proceed our discussion of convergence theory. In section E, we give complete proofs for
single-step scheme. We dedicate sections F and G to illustrate formal analysis of the convergence
results of Algorithm 1 under k local steps, given different assumptions of objective function f . In
section H, we introduce additive noise to make our sketched gradients differential private and prove
it for specific AMS sketch matrix.
A	Preliminary
For a positive integer n, We use [n] to denote the set {1,2,…，n}. We use E[∙] to denote expec-
tation (if it exists), and use Pr[∙] to denote probability. For a function f, we use O(f) to denote
f poly(log f). For a vector x, For a vector x, We use kxk1 := Pi |xi| to denote its `1 norm, We
use kχ∣∣2 := (Pn=I x2)1/2 to denote its '2 norm, we use ∣∣χ∣∣∞ := maxi∈[n] ∣χ∕ to denote its '∞
norm. For a matrix A and a vector x, we define IIxkA := λ∕x>Ax. For a full rank square matrix
A, we use A-1 to denote its true inverse. For a matrix A, we use At to denote its pseudo-inverse.
For a matrix A, we use kAk to denote its spectral norm. We use kAkF := (Pi,jAi2,j)1/2 to denote
its Frobenius norm. We use A> to denote the transpose of A. We denote 1{x=l} for l ∈ R to be
the indicator function which equals to 1 if x = l and 0 otherwise. Let f : A → B and g : C → A
be two functions, we use f ◦ g to denote the composition of functions f and g, i.e., for any x ∈ C,
(f ◦ g)(x) = f (g(x)).
B Probability
Lemma B.1 (Chernoff bound Chernoff (1952)). Let X = Pin=1 Xi, where Xi = 1 with probability
Pi and Xi = 0 with probability 1 一 Pi, and all Xi are independent. Let μ = E[X ] = En=I Pi ∙ Then
1. Pr[X ≥ (1 + δ)μ] ≤ exp(-δ2μ∕3), ∀δ > 0 ;
2. Pr[x ≤(1 — δ)μ] ≤ exp(-δ2μ∕2), ∀0 < δ < 1.
Lemma B.2 (Hoeffding bound Hoeffding (1963)). Let Xi, ∙∙∙ ,Xn denote n independent bounded
variables in [ai , bi]. Let X =	in=1 Xi, then we have
Pr[|X -E[X]| ≥ t] ≤ 2exp
Lemma B.3 (Bernstein inequality Bernstein (1924)). Let Xi,…，Xn be independent zero-mean
random variables. Suppose that |Xi | ≤ M almost surely, for all i. Then, for all positive t,
n	t2/2
r [3 i >t] ≤ exp ∖ Pn=I EX管 + Mt/3
Lemma B.4 (Khintchine,s inequality, Khintchine (1923); Haagerup (1981)). Let σι, ∙∙∙ ,σn be i.i.d.
sign random variables, and let zi,…,zn be real numbers. Then there are constants C > 0 so that
for all t > 0
Pn=i(bi - ai)2
n
Pr X ziσi ≥ tkzk2 ≤ exp(-Ct2).
i=i
Lemma B.5 (Hason-wright inequality Hanson & Wright (1971); Rudelson & Vershynin (2013)).
Let x ∈ Rn denote a random vector with independent entries xi with E[xi] = 0 and |xi | ≤ K. Let
A be an n × n matrix. Then, for every t ≥ 0,
Pr[∣x>Ax — E[x>Ax]| > t] ≤ 2 ∙ exp(-cmin{t2∕(K4∣A∣F),t/(K2kA∣)}).
Lemma B.6 (Lemma 1 on page 1325 of Laurent and Massart Laurent & Massart (2000)). Let
X 〜Xk be a chi-squared distributed random variable with k degrees of freedom. Each one has
zero mean and σ2 variance. Then
PrX - kσ2 ≥ (2√kt + 2t)σ2] ≤ exp(-1),
16
Under review as a conference paper at ICLR 2022
Pr[kσ2 — X ≥ 2√ktσ2] ≤ exp(-1).
Lemma B.7 (Tail bound for sub-exponential distribution Foss et al. (2011)). We say X ∈ SE(σ2 , α)
with parameters σ > 0, α > 0 if:
E[eλX] ≤ exp(λ2σ2/2),	∀∣λ∣ < 1∕ɑ.
Let X ∈ SE(σ2, α) and E[X] = μ, then:
Pr[∣X — μ∣ ≥ t] ≤ exp(-0.5 min{t2∕σ2, t∕α}).
Lemma B.8 (Matrix Chernoff bound Tropp (2011); Lu et al. (2013)). Let X be a finite set of
positive-semidefinite matrices with dimension d × d, and suppose that
max λmax(X) ≤ B.
X∈X
Sample {Xι, ∙∙∙ , Xn} uniformly at randomfrom X without replacement. We define μmin and μmaχ
as follows:
μmin := n ∙ λmin( E [XD and μmax := n ∙ λmax( E [X]).
X〜X	X〜X
Then
n
Pr [λmin(X Xi) ≤ (1 - δ)μmin] ≤ d ∙ exp(-δ2μmin∕B) for δ ∈ [0,1),
i=1
n
Pr 卜max(XXi) ≥ (1 + δ)μmaχ] ≤ d ∙ exp (-δ2μmaχ/(4B)) for δ ≥ 0.
i=1
C Optimization Backgrounds
Definition C.1. Let f : Rd → R be a function, we say f is L-smooth iffor any x, y ∈ Rd, we have
lVf (x) - Vf (y)l2 ≤ Llx -yl2
Equivalently, for any x, y ∈ Rd, we have
f (y) ≤ f (x) + hy - x, Vf (x)i + L2∣y - x∣2
Definition C.2. Let f : Rd → R be a function, we say f is convex iffor any x, y ∈ Rd, we have
f(x) ≥ f(y) + hx - y, Vf(y)i
Definition C.3. Let f : Rd → R be a function, we s^y f is μ-strongly-convex iffor any x, y ∈ Rd,
we have
l∣Vf (x) - Vf (y)k2 ≥ μkx - y∣∣2
Equivalently, for any x, y ∈ Rd, we have
f (y) ≥ f (x) + hy - x, Vf (x)i + μ∣∣y - x∣2
Fact C.4. Let f : Rd → R be an L-smooth and convex function, then for any x, y ∈ Rd, we have
f(y) - f(X) ≥ hy - χ, Vf(Xyi + 1-j- ∙ kVf(y) - Vf(X)k2
2L
Fact C.5 (Inequality 4.12 in BottoU et al. (2018)). Let f : Rd → R be a μ-Strongly ConvexfUnction.
Let x* be the minimizer of f. Thenfor any X ∈ Rd, we have
f(x) - f(x*) ≤ 2μ∣Vf(χ)k2
17
Under review as a conference paper at ICLR 2022
D S ketching
In this section, we discuss the (α, β, δ)-coordinate wise embedding property we proposed in this
work through several commonly used sketching matrices.
We consider several standard sketching matrices:
1.	Random Gaussian matrices.
2.	Subsampled randomized Hadamard/Fourier transform matrices Lu et al. (2013).
3.	AMS sketch matrices Alon et al. (1999), random {-1, +1} per entry.
4.	Count-Sketch matrices Charikar et al. (2002), each column only has one non-zero entry,
and is -1, +1 half probability each.
5.	Sparse embedding matrices Nelson & NgUyen (2013), each column only has S non-zero
entries, and each entry is - √1s, + 表 half probability each.
6.	Uniform sampling matrices.
We list the definitions and results of above sketching matrices for coordinate-wise embedding in
Table 1.
Table 1: Roadmap of the results for coordinate-wise embedding
Sketching matrix	Definition	Expectation	Variance	Inner Product	Concentration
Random Gaussian	Definition D.2	Lemma D.11	Lemma D.13	Lemma D.18	Lemma D.24
SRHT	Definition D.3	Lemma D.11	Lemma D.12	Lemma D.19	Lemma D.23
AMS	Definition D.4	Lemma D.11	Lemma D.12	Lemma D.20	Lemma D.23
Count-sketch	Definition D.5	Lemma D.11	Lemma D.14	Lemma D.21	Lemma D.25
Sparse embedding	DefinitiOn D.6,D.7	Lemma D.11	Lemma D.15	Lemma D.22	Lemma D.28
Uniform sampling	Definition D.8	Lemma D.11	Lemma D.16		Lemma D.29
D.1 Definition
Definition D.1 (k-wise independence). H = {h : [m] → [l]} is a k-wise independent hash family
if ∀iι = i2 =…=ik ∈ [n] and ∀jι,…，jk ∈ [l],
Pr [h(i1) = jι ∧∙∙∙ ∧ h(ik) = jk] = ɪ.
h∈H	l
Definition D.2 (Random Gaussian matrix). We say R ∈ Rb×n is a random Gaussian matrix if all
entries are sampled from N(0, 1/b) independently.
Definition D.3 (Subsampled randomized Hadamard/Fourier transform matrix Lu et al. (2013)). We
s^y R ∈ Rb×n is a subsampled randomized Hadamard transform matrix11 if it is of the form R =
y/n/bSHD, where S ∈ Rb×n is a random matrix whose rows are b uniform samples (without
replacement) from the standard basis of Rn, H ∈ Rn×n is a normalized Walsh-Hadamard matrix,
and D ∈ Rn×n is a diagonal matrix whose diagonal elements are i.i.d. Rademacher random
variables.
Definition D.4 (AMS sketch matrix Alon et al. (1999)). Let hi, h2, ∙∙∙ ,hb be b random hashfunc-
tions picking from a 4-wise independent hash family H = {h : [n] → { — √1^, 十 √1^}}. Then
R ∈ Rb×n is a AMS sketch matrix ifwe set Ri,j = hi(j).
Definition D.5 (Count-sketch matrix Charikar et al. (2002)). Let h : [n] → [b] be a random 2-wise
independent hash function and σ : [n] → {-1, +1} be a random 4-wise independent hash function.
Then R ∈ Rb×n is a count-sketch matrix ifwe set Rh(i),i = σ(i) for all i ∈ [n] and other entries to
zero.
iiIn this case, we require log n to be an integer.
18
Under review as a conference paper at ICLR 2022
Definition D.6 (Sparse embedding matrix I Nelson & NgUyen (2013)). We say R ∈ Rb×n is a
sparse embedding matrix with parameter S if each column has exactly S non-zero elements being
±1∕√s uniformly at random, whose locations are picked uniformly at random without replacement
(and independent across columns) iii.
Definition D.7 (Sparse embedding matrix II Nelson & NgUyen (2013)). Let h : [n] × [s] → [b/s] be
a a ramdom 2-wise independent hash function andσ : [n] × [s] → {-1, 1} be a 4-wise independent.
Then R ∈ Rb×n is a sparse embedding matrix II with parameter s if we set R(j-1)b/s+h(i,j),i =
σ(i,j)∕√S for all (i,j) ∈ [n] X [s] and all other entries to zero.iv
Definition D.8 (Uniform sampling matrix). We s^y R ∈ Rb×n is a uniform sampling matrix if it is
oftheform R =，n/bSD, where S ∈ Rb×n is a random matrix whose rows are b uniform SamPIeS
(without replacement) from the standard basis of Rn, and D ∈ Rn×n is a diagonal matrix whose
diagonal elements are i.i.d. Rademacher random variables.
D.2 Coordinate wise embedding
We define coordinate-wise embedding as follows
Definition D.9 ((α, β, δ)-coordinate wise embedding). We say a randomized matrix R ∈ Rb×n
satisfying (α, β, δ)-coordinate wise embedding if
1.	E [g> R> Rh] = g>h,
R〜Π
2	E [(g>R>Rh)2] ≤ (g>h)2 + αkgk2khk2,
R〜Π	b
3.	Pr lg>R>Rh - g>hl ≥ j7=kgk2khk2 ≤ δ∙
R〜π L	√b	_
Remark D.10. Given a randomized matrix R ∈ Rb×n satisfying (α, β, δ)-coordinate wise embed-
ding and any orthogonal projection P ∈ Rn×n, above definition implies
1.	E [P R> Rh] = Ph,
R〜Π
2.	E [(PR>Rh)2] ≤ (Ph) + αkhk2,
R〜π	b
3.	Pr [∣(PR>Rh)i - (Ph)i| ≥ √= kh∣d ≤ δ.
R〜π L	√b	_
since kPk2 ≤ 1 implies kPi,: k2 ≤ 1 for all i ∈ [n].
D.3 Expectation and variance
Lemma D.11. Let R ∈ Rb×n denote any of the random matrix in Definition D.2, D.3, D.4, D.6,
D.7, D.8. Then for any fixed vector h ∈ Rn and any fixed vector g ∈ Rn, the following properties
hold:
E [g> R> Rh] = g>h
Proof.
E [g>R>Rh] = g> E [R>R]h = g>Ih = g>h.
R〜Π	R〜Π
□
Lemma D.12. Let R ∈ Rb×n denote a subsampled randomized Hadamard transform or AMS sketch
matrix as in Definition D.3, D.4. Then for any fixed vector h ∈ Rn and any fixed vector g ∈ Rn, the
following properties hold:
2
E [(g>R>Rh)2] ≤ (g>h)2 + -kgk2 ∙khk2.
__________________________R〜Π____	b
iiiFor oUr pUrposes the signs need only be O(log d)-wise independent, and each colUmn can be specified by
a O(log d)-wise independent permUtation, and the seeds specifying the permUtations in different colUmns need
only be O(log d)-wise independent.
ivThis definition has the same behavior as sparse embedding matrix I for oUr pUrpose.
19
Under review as a conference paper at ICLR 2022
Proof. If Ea [a] = b, it is easy to see that
E[(a - b)2] = E[a2 - 2ab + b2] = E[a2 -b2]
aa	a
We can rewrite it as follows:
E [(g>R>Rh)2 - (g>h)2] = E [(g>(R>R-I)h)2],
R〜Π	R〜Π
It can be bounded as follows:
E [(g>(R>R - I)h)2]
R〜Π
REn ] (X(Rg)k (Rh)k - g>h!
REnl(XXRk,igi∙ X	Rkjhj)
k=1 i=1	j∈[n]∖{i}	)
bn	b n
XX
Rk,igi	• X	Rkj hj	∙	XX Rk0,i0 gi0 ∙	X	Rk0j0 hj0
k=1 i=1	j∈[n]∖{i}	k0 = 1 i0 = 1	j0∈[n]∖{i0}
[/ b n	b n	'
(XX	R2,ig2∙ X	Rkj hj)	+	(XX Rk,igihi	∙ X	Rkj % h
∖k=1 i=1	j∈[n]∖{i}	)	∖k = 1 i=1	j∈[n]∖{i}	,
=1 (Xg2 X h2)+b (Xgihi X gjhj)
∖i=1 j∈[n]∖{i}	)	∖i=1 j∈[n]∖{i}	)
2
≤ b 1回2网12,
where the second step follows from R2k,i = 1/b, ∀k, i ∈ [b] × [n], the forth step follows from
E[Rk,iRk,j Rk0,i0 Rk0,j0] 6= 0 only if i = i0, j = j0, k = k0 or i = j0, j = i0, k = k0, the fifth
step follows from Rk,i and Rk,j are independent if i 6= j and R2k,i = R2k,j = 1/b, and the last step
follows from Cauchy-Schwartz inequality.
Therefore,
2
E [(g>R>Rh)2 - (g>h)2] = E [(g>(R>R - I)h)2] ≤ 示kgk2kh∣∣2∙
R〜n	R〜n	1
□
Lemma D.13. Let R ∈ Rb×n denote a random Gaussian matrix as in Definition D.2. Then for any
fixed vector h ∈ Rn and any fixed vector g ∈ Rn, the following properties hold:
3
E [(g>R>Rh)2] ≤ (g>Ih) + -∣∣g∣∣2 ∙Mk2∙
R〜n	b
Proof. Note
E
R〜n
E
R〜n
E
R〜n
b n	n	\
XXRk,igi ∙XRkjhj I
k=1 i=1	j=1
bn	n	b n	n
EERk,igi∙ ERkjhj .	£ ∑Rko,i0 "£ Rk0,j0 hj0
k=1 i=1	j=1	k0=1 i0=1	j0=1
20
Under review as a conference paper at ICLR 2022
b	nn	bn
X X XX
R2,iRe ,i0gi higi0 hi0 I + XXRk4,igi2hi2
k=1 k0∈[b]∖{k} i=1 i0 = 1	)	∖k=1 i=1	)
bn	nn n
XX	X Rk2,iR2k,jgi2hj2	+ XX	X	R2k,iR2k,i0gihigi0 hi0
k=1 i=1 j∈[n]∖{i}	k = 1 i=1 i0∈[n]∖{i}
+	Xb Xn	X	Rk2,iR2k,jgihjgj hi	i
k=1 i=1 j∈[n]∖{i}
nn	n
XX
giZZigio hio + b Xgi2hi2
i=1 i0=1
i=1
1n	1n	1n
+ bΣS ΣS gi hj + bΣS ΣS gihigi0 hi0 + bΣS ΣS gihj gj hi
i=1 j∈[n]∖[i]
3
≤ (Oh) + b kgk2khk2,
i=1 i0∈[n]∖[i]
i=1 j∈[n]∖[i]
where the third step follows from that for independent entries of a random Gaussian matrix,
E[Rk,iRk,j Rk0,i0 Rk0,j0] 6= 0 only if 1. k 6= k0, i = j, i0 = j0, or 2. k = k0, i = i0 = j = j0, or 3.
k = k0, i = i0 6= j = j0, or 4. k = k0, i = j 6= i0 = j0, or 5. k = k0, i = j0 6= i0 = j, the fourth step
follows from E[R2k,i] = 1/b and E[R4k,i] = 3/b2, and the last step follows from Cauchy-Schwartz
inequality.	□
Lemma D.14. Let R∈ Rb×n denote a count-sketch matrix as in Definition D.5. Then for any fixed
vector h ∈ Rn and any fixed vector g ∈ Rn, the following properties hold:
3
E [(g>R>Rh)2] ≤ (g>h)2 + -kgk2khk2.
R〜Π	b
Proof. Note
E [(g>R>Rh)2]
R〜Π
(bn	n	∖ 2
REnl(XX Rk,igi X Rkjhj)
k=1 i=1	j=1
bn	n
REn(XX Rk,igi X Rk,jhj
k=1 i=1	j=1
bn	n
Rk0,i0gi0	Rk0,j0 hj0
k0=1 i0=1	j0=1
b - 1
b
b	n n	bn
XXXX
Rk2,iR2k0,i0 gihigi0 hi0 I +	XXR4k,igi2hi2
k=1 k0∈[b]∖{k} i=1 i0∈[n]∖{i}	)	∖k = 1 i=1
bn	nn n
XX	X R2k,iRk2,jgi2hj2	+ XX	X	R2k,i Rk2,i0 gi hi gi0 hi0
k = 1 i=1 j∈[n]∖{i}	k=1 i=1 i0∈[n]∖{i}
+	Xb Xn	X	Rk2,iRk2 ,jgi hjgj hi	i
k = 1 i=1 j∈[n]∖{i}
nn
彳 X X gihigi0hi0 + Xg2h2
i=1 i0∈[n]∖i	i=1
1n	1n	1n
+ b∑ E	gihj + b∑ E	gihigi0hi0 + b∑ E	gihjgjhi
i=1 j∈[n]∖{i}
i=1 i0∈[n]∖{i}
i=1 j∈[n]∖{i}
21
Under review as a conference paper at ICLR 2022
3
≤ (Oh) + bkgkkhk,
where in the third step we are again considering what values of k, k0 , i, i0 , j, j0 that makes
E[Rk,iRk,jRk0,i0Rko,j0] = 0. Since the hash function σ(∙) of the count-sketch matrix is 4-wise
independent, ∀k, k0 , when i 6= i0 6= j 6= j0, or i = i0 = j 6= j0 (and the other 3 symmetric cases),
we have that E[Rk,iRk,j Rk0,i0 Rk0,j0] = 0. Since the count-sketch matrix has only one non-zero
entry in every column, when k 6= k0 , if i = i0 or i = j0 or j = i0 or j = j0 , we also have
E[Rk,iRk,j Rk0,i0 Rk0,j0] = 0. Thus we only need to consider the cases: 1. k 6= k0, i = j 6= i0 = j0,
or 2. k = k0, i = i0 = j = j0, or 3. k = k0, i = i0 6= j = j0 , or 4. k = k0, i = j 6= i0 = j0, or 5.
k = k0, i = j0 = i0 = j. And the fourth step follows from E[R[ J = 1/b and E[Rk J = 1/b, and
the last step follows from CaUChy-SChWartz inequality.	□
Lemma D.15. Let R ∈ Rb×n denote a sparse embedding matrix as in Definition D.6, D.7. Then
for any fixed vector h ∈ Rn and any fixed vector g ∈ Rn, the following properties hold:
2
2 E [(g>R>Rh)2 *] ≤ (g>h)2 + -kgk2 ∙khk2.
R〜Π	b
Proof. Note
E [(g>R>Rh)2]
R〜Π
( bn	n	、2
REnlXX Rk.gi X Rkjhj
k=1 i=1	j=1
bn	n
REnl(XX Rk,igi X Rk,jhj
k=1 i=1	j=1
bn	n
Rk0,i0gi0	Rk0,j0hj0
k0=1 i0=1	j0=1
bn	bn
XXR2,ig2 X	Rkjh2) + (XXRk,igihi X	Rkjgjhj
k=1 i=1	j∈[n]∖{i}	)	∖k = 1 i=1	j∈[n]∖{i}
+ I XX Rk,iRk,i0 gihigi0 hi0 I + (XX Rk,igihi ) + I X X Rk,iRk0,i0 gihigi0 hi0
k i6=i0	k i	k6=k0 i6=i0
+ IXXRkk,iRkk0,igikhik Ii
k6=k0 i
b X 册	+ b X gi hijj	+ b X gihigi0 %	+ S X gkhk	+ b-1 X	gihigi0 hi	+ s-1 X	gkhk
i6=j	i6=j	i6=i0	i	i6=i0	i
2
≤ (g>h)2 + bkgk2khk2,
where the third step follows from the fact that the sparse embedding matrix has independent columns
and s non-zero entry in every column, the fourth step follows from E[Rkk,i] = 1/b, E[Rk4,i] =
1∕(bs), and E[Rk,iRko,/ = b(S-1) ∙ s12, ∀k = k0 and the last step follows from Cauchy-Schwartz
inequality.	□
Lemma D.16. Let R ∈ Rb×n denote a uniform sampling matrix as in Definition D.8. Then for any
fixed vector h ∈ Rn and any fixed vector g ∈ Rn, the following properties hold:
2 E [(g>R>Rh)2] ≤ (g>h)2 + nkgk2khk2.
REn	b
Proof. Note
E [(g>R>Rh)k]
REn
22
Under review as a conference paper at ICLR 2022
Γ / b n	n	∖ 2
= REnl(XX Rk,i gi X Rkjhj)
k=1 i=1	j =1
[bn	n	b n	n
(XX Rk,i gi X Rkjhj ) ∙ ( XX Rk0,i0 gi0 X Rk0,j hj0 )
k=1 i=1	j=1	k0=1 i0=1	j 0=1
= REEΠ	XXR4k,igi2hi2 + (XXR2k,iR2k0,i0gihigi0hi0 )
E	k i	k6=k0 i6=i0
=b X g2h2+∣-⅞ X gihigi0 hio
i	n	i6=i0
≤ (gTh)2 + b kgk2 khk2,
where the third step follows from the fact that the random sampling matrix has one non-zero entry
in every row, the fourth step follows from E[R2k,iR2k0,i0] = n/((n - 1)b2) for k 6= k0, i 6= i0 and
E[R4,i] = n/b2.	'	'	口
Remark D.17. Lemma D.16 indicates that uniform sampling fails in bounding variance in some
sense, since the upper bound give here involves n.
D.4 B ounding inner product
Lemma D.18 (Gaussian). Let R ∈ Rb×n be a random Gaussian matrix (Definition D.2). Then we
have:
Pr hImax lhR*,i,R*ji| ≥ Ρlθg(n")i ≤ θ(δ).
Proof. Note for i = j, R*,%, R*,j 〜 N(0, bIb) are two independent Gaussian vectors. Let Zk =
Rk,iRkj and Z = <R*,i, R*,ji. Then we have for any ∣λ∣ ≤ b/2,
E[eλzk] = h I"2 ≤ exP(λ2/b2),
1 -λ2 /b2
where the first step follows from Zk = 4(Rk,i + Rkj)2 + 4(Rk,i - Rkj)2 = b(Qi - Q2) where
Qi, Q2 〜χi, and E[eλQ] = √⅛ for any Q 〜χ2.
This implies Zk ∈ SE(2/b2, 2/b) is a sub-exponential random variable. Thus, we have Z =
Pk=i Zk ∈ SE(2/b, 2/b), by sub-exponential concentration Lemma B.7 we have
Pr[|Z| ≥ t] ≤ 2 exp(-bt2/4)
for0 < t < 1. Picking t = ,log(n2∕δ)∕b, we have
Pr [lhR*,i,R*jil≥ cp≡i ≤ δ∕n2.
Taking the union bound over all (i,j) ∈ [n] X [n] and i = j, we complete the proof.	□
Lemma D.19 (SRHT). Let R ∈ Rb×n be a subsample randomized Hadamard transform (Defini-
tion D.3). Then we have:
Pr h Imax KR*,i,R*,j i| ≥ ρlog(n") i ≤ ,A
23
Under review as a conference paper at ICLR 2022
Proof. For fixed i = j, let X = [R*,i, R*,j] ∈ Rb×2. Then X>X = Pk=ι Gk, where
Gk = [Rk,i, Rk,j]>[Rk,i, Rk,j]
1	Rk,iRk,j
Rk,iRk,j	1
Note the eigenvalues of Gk are 0 and 1 and E[X>X] = b ∙ E[Gk] = I2 for all k ∈ [b]. Thus,
applying matrix Chernoff bound B.8 to X>X we have
Pr λmax(X>X) ≤ 1 - t ≤ 2 exp (-t2b/2) for t ∈ [0, 1), and
Pr hλmax(X>X) ≥ 1 + ti ≤ 2 exp (-t2b/8) for t ≥ 0.
which implies the eigenvalues of X>X are between [1 -1,1 +1] with probability 1 - 4 exp (- t8b).
So the eigenvalues of X>X - I2 are between [-t,t] with probability 1 - 4exp (-景).Picking
c√log(n∕δ)
t = N √b	, we have
Pr hkX>X - I2k≥「i ≤ "
Note
X>X -I2=〈R*：R*j
hR*,i, R*,ji
0
whose spectral norm is |〈R*,i, R*,ji∣. Thus, we have
Pr [|hR*,i,R*,ji| ≥ c'lo√n∕δ)i ≤ δ∕n2.
Taking a union bound over all pairs (i,j) ∈ [n] X [n] and i = j, we complete the proof.	□
Lemma D.20 (AMS). Let R ∈ Rb×n be a random AMS matrix (Definition D.4). Let {σi, i ∈ [n]}
be independent Rademacher random variables and R ∈ Rb×n with R*,i = σiR^,i, ∀i ∈ [n]. Then
we have:
Pr hmax|hR*,i,R*,ji| ≥ 'log(n")i ≤ θ(δ).
i6=j	b
Proof. Note for any fixed i = j, R*,i and R*,j are independent. By Hoeffding inequality
(Lemma B.2), we have
Pr h|hR*,i,R*,j il ≥ ti ≤ 2exP ( - Pb j] (- I ))2 ) ≤ 2e-t2b∕2
Choosing t =，2 log(2n2∕δ)/√b, we have
Pr [lhR*,i,R*,ji∣ ≥ ,2log(2n2∕δ)∕√i ≤《.
Taking a union bound over all pairs (i,j) ∈ [n] × [n] and i = j, we complete the proof.	□
Lemma D.21 (Count-Sketch). Let R ∈ Rb×n be a count-sketch matrix (Definition D.5). Let
{σi, i ∈ [n]} be independent Rademacher random variables and R ∈ Rb×n with R*,i
σiR*,i, ∀i ∈ [n]. Then we have:
max KR*,i,R*,ji| ≤ L
i6=j
Proof. Directly follow the definition of count-sketch matrices.
□
24
Under review as a conference paper at ICLR 2022
Lemma D.22 (Sparse embedding). Let R ∈ Rb×n be a sparse embedding matrix with parameter
s (Definition D.6 and D.7). Let {σi , i ∈ [n]} be independent Rademacher random variables and
R ∈ Rb×n with R*,i = θjR*,i, ∀i ∈ [n]. Then we have:
Pr hmax 匹“,瓦,i∣≥ cp√n≡i ≤ Θ(δ).
i6=j	s
Proof. Note for fixed i = j, R*,i and R* j are independent. Assume R*,% and R* j has u non-zero
elements at the same positions, where 0 ≤ u ≤ s, then by Hoeffding inequality (Lemma B.2), we
have
Pr[|hR*,i,R*ji| ≥ t] ≤ 2eχp (-P( j-(—ι))2) ≤ 2eχp(-t2s2(∕(2U))	(4)
Let t = P(2u∕s2)log(2n2∕δ), We have
Pr h∣hR*,i,R*,ji∣ ≥ √2s-1 log(2n2∕δ)i ≤ Pr [∣(R*,i,R*,j)| ≥ ,2us-2 log(2n2∕δ)]
≤ δ∕n2	(5)
since u ≤ s. By taking a union bound over all (i, j ) ∈ [n] × [n] and i 6= j, We complete the
proof.	□
D.5 Infinite norm bound
Lemma D.23 (SRHT and AMS). Let R ∈ Rb×n denote a subsample randomized Hadamard trans-
form (Definition D.3) or AMS sketching matrix (Definition D.4). Then for any fixed vector h ∈ Rn
and any fixed vector g ∈ Rn, the following properties hold:
禹 h∣(g>RVRh)-(g>h)∖ > lθg1√vδ) ∣∣g∣∣2∣∣h∣∣2i ≤ Θ(δ).
Proof. We can reWrite (g> R> Rh) - (g> h) as folloWs:,
nn	n
(g>R>Rh) - (g>h) = X X gihjhR*,i,R*,j + Xgihi(∣∣R*,i∣∣2 - 1)
i=1 j∈[n]∖i	i=1
nn
=E E gihjhσiR*,i,σjR*,ji.
i=1 j∈[n]∖i
where σi's are independent Rademacher random variables and R*,i = σ%R*,i, ∀i ∈ [n], and the
second step follows from ∣∣R*,i∣∣2 = 1, ∀i ∈ [n].
We define matrix A ∈ Rn×n and B ∈ Rn×n as follows:
Aij = gihj ∙ hR*,i, R* ji ,
Bij = gihj∙蟹 KR*-"
∀i ∈ [n], j ∈ [n]
∀i ∈ [n], j ∈ [n]
We define A ° ∈ Rn×n tobe the matrix A ∈ Rn×n with removing diagonal entries, applying Hason-
wright inequality (Lemma B.5), we have
Pr[∣σ>A°σ∣≥ T] ≤ 2 ∙ exp(-cmin{τ2∕∣A°IlF,τ∕∣A°∣})
σ
We can upper bound ∣∣A°∣ and ∣∣A°∣∣f.
M°k≤M°∣F
≤ ∣A∣F
≤IB∣F	_	_
=ι∣g∣2 ∙ ∣h∣2 ∙ maχ κR*,i,R*j i|
i6=j
25
Under review as a conference paper at ICLR 2022
≤ I∣gk2 ∙ khk2 ∙ max lhR*,i,R*,ji|.
i6=j
where the forth step follows from B is rank-1.
For SRHT, note R has the same distribution as R. By Lemma D.19 (for AMS, We use Lemma D.20)
with probability at least 1 - Θ(δ), we have :
maxlhR*,i,R*,jil≤ P√/.
i6=j	b
Conditioning on the above event holds.
Choosing T = ∣∣g∣2 ∙ ∣∣h∣2 ∙ log1∙5(n∕δ)∕√b, we can show that
Pr [∣(g>R>Rh) - (g>h)∣ ≥ ∣g∣2 ∙kh∣2lθg1.5(n") 1 ≤ Θ(δ).
b
Thus, we complete the proof.	□
Lemma D.24 (Random Gaussian). Let R ∈ Rb×n denote a random Gaussian matrix (Defini-
tion D.2). Then for any fixed vector h ∈ Rn and any fixed vector g ∈ Rn, the following properties
hold:
禹 h∣(g>RVRh)-(g>h)∖ > lθg1√n∕δ) kgk2khk2i ≤ Θ(δ).
Proof. We follow the same procedure as proving Lemma D.23.
We can rewrite (g>R>Rh) - (g> h) as follows:,
nn	n
(gτRVRh)- (gτh) = X X gihj<R*,i,R*,ji+ Xgihi(∣R*,ik2 — 1)
i=1 j∈[n]∖i	i=1
nn	n
=E E。/"姓凡&叼R*,ji+Egihi(kR*,i∣2 — 1).	⑹
i=1 j∈[n]∖i	i=1
where o/sare independent Rademacher random variables and R has the same distribution as R.
To bound the first term PN1 £；三同» gihj {σiR^,i, σ7-R*,j)，we define matrix A ∈ Rn×n and
B ∈ Rn×n as follows:
Aij= gihj ∙ hR*,i, R*,ji,
Bij= gihj ∙ mmaxx ∣(R*,i0,R*,j"∣
∀i ∈ [n], j ∈ [n]
∀i ∈ [n], j ∈ [n]
We define A ° ∈ Rn×n tobe the matrix A ∈ Rn×n with removing diagonal entries, applying Hason-
wright inequality (Lemma B.5), we have
Pr[∣σ>A°σ∣≥ T] ≤ 2 ∙ exp(-cmin{τ2∕∣A°kF,τ∕∣∣A°∣∣})
σ
We can upper bound ∣∣A°∣ and ∣∣A°∣∣f.
∣A°k≤kA°∣F
≤ ∣A∣F
≤∣b∣f	_	_
=ι∣g∣2 ∙ ∣h∣2 ∙ max κR*,i,R*,j i|
i6=j
≤ ι∣g∣2 ∙ ιh∣2 ∙ max κR*,i,R*,j i|.
i6=j
where the forth step follows from B is rank-1.
26
Under review as a conference paper at ICLR 2022
Using Lemma D.18 with probability at least 1 - Θ(δ), we have :
max ∣hR*,i,R*,j i∣≤ plogy)
i6=j	b
Conditioning on the above event holds.
Choosing T = ∣∣gk2 ∙ ∣∣hk2 ∙ log1.5(n∕δ)∕√b, We can show that
I 二二	_	_ I	log1.5(n∕δ)
Pr |X X gi hjhσiR*,i,σj R*,ji∣≥kg∣2 ∙∣h∣2 g J ≤ Θ(δ).	⑺
i=1 j∈[n]∖i
To bound the second term PN1 gih(∣∣R*,i∣∣2 一 1), note that b[R*,i∣2 〜X for every i ∈ [n].
Applying Lemma B.6, we have
Pr"k2 - 1∣≥ Mo尹# ≤ δ∕n.
b
which implies
Pr Xgihi∣∣R*,ik2 - I∣≥kgk2khk2 C'lθ√n∕δ) ≤ Θ(δ).	(8)
i=1	b
Plugging the bounds Eq. (7) and (8) back to Eq. (6), we complete the proof.	□
Lemma D.25 (Count-sketch). Let R ∈ Rb×n denote a count-sketch matrix (Definition D.5). Then
for any fixed vector h ∈ Rnand any fixed vector g ∈ Rn, the following properties hold:
禹 h∣(g>R>Rh) - (g>h)∣≥ log(l∕δ)kgk2khk2i ≤ Θ(δ).
Proof. We follow the identical procedure as proving Lemma D.23 to apply Hason-wright inequality
(Lemma B.5).
Then note Lemma D.21 shows
max KR*,i,R*,ji| ≤ 1
i6=j
Thus, choosing T = Ckgk2 ∙ ∣∣h∣2 ∙ log(1∕δ), we can show that
Pr [∣(g>R>Rh) - (g>h)| ≥ c∣g∣2 ∙ ∣h∣2 log(1∕δ)] ≤ δ.
which completes the proof.	□
Lemma D.26 (Count-sketch 2). Let R ∈ Rb×n denote a count-sketch matrix (Definition D.5). Then
for any fixed vector h ∈ Rnand any fixed vector g ∈ Rn, the following properties hold:
RPrI h∣(g>R>Rh)-(gτh)∖≥ √1bδkgk2khk2i ≤ Θ(δ).
Proof. Itis known that a count-sketch matrix with b = -2δ-1 rows satisfies the (, δ, 2)-JL moment
property (see e.g. Theorem 14 of Woodruff (2014)). Using Markov’s inequality, (, δ, 2)-JL moment
property implies
禹 h∣(gτRτRh) 一 (gτh)∣ ≥ e∣∣g∣∣2khk2i ≤ Θ(δ),
where e= √bδ.	□
Remark D.27. In LP solver, we need δ = 1∕ poly(n), thus Lemma D.25 is stronger than
Lemma D.26.
27
Under review as a conference paper at ICLR 2022
Lemma D.28 (Sparse embedding). Let R ∈ Rb×n denote a sparse-embedding matrix (Defini-
tion D.6 and D.7). Then for any fixed vector h ∈ Rn and any fixed vector g ∈ Rn, the following
properties hold:
3.	RPrI h∣(g>R>Rh) - (g>h)∣ > Iog1√Sn∕δ) |团图向，≤ Θ(δ).
Proof. We follow the identical procedure as proving Lemma D.23 to apply Hason-wright inequality
(Lemma B.5).
Then note Lemma D.22 shows with probability at least 1 - δ we have
maχ∣hR*,i,R*j i∣≤ cpo√^.
i6=j	s
Conditioning on the above event holds, choosing T = CllgIl2 ∙ Ilhll2 ∙ log1.5(1∕δ), We can show that
Pr [∣(g>R>Rh) - (g>h)∣ ≥ ‘'log1〉0 ∣∣g∣∣2 ∙∣∣h∣∣2] ≤ Θ(δ).
L	√s	」
Thus, we complete the proof.	□
Lemma D.29 (Uniform sampling). Let R ∈ Rb×n denote a uniform sampling matrix (Defini-
tion D.8). Then for any fixed vector h ∈ Rn and any fixed vector g ∈ Rn, the following properties
hold:
3.∣(g>R>Rh)-(g>h)∣≤(1+n )∣∣g∣∣2∣∣h∣∣2
b
where I ⊂ [n] be the subset of indexes chosen by the uniform sampling matrix.
Proof. We can rewrite (g>R>Rh) - (g> h) as follows:,
nn	n
(g>R>Rh) - (g>h) = X X gihjhR*,i,R*,j + Xgihi(∣∣R*,i∣∣2 — 1)
i=1 j∈[n]∖i	i=1
n
=IbEgihi — Egihi
i∈I	i=1
where the second step follows from the uniform sampling matrix has only one nonzero entry in each
row.
Let I ⊂ [n] be the subset chosen by the uniform sampling matrix, then ∣∣R*,i ∣∣2 = n/b for i ∈ I and
∣∣R*,i∣∣2 = 0 for i ∈ [n] \ I. So we have
|(g>R>Rh)TgTh)I = I X gihi( 石-I)- X gihi I
i∈I	i∈[n]∖I
n
≤ (1 + b )kg∣2∣h∣2 .
□
E Analysis of convergence: Single-step scheme
E.1 Preliminary
Throughout the proof of convergence, we will use Ft to denote the sequence wt-1, wt-2, . . . , w0.
Also, we use η as a shorthand for ngiobai ∙ ηiocai.
28
Under review as a conference paper at ICLR 2022
E.2 STRONGLY-CONVEX f CONVERGENCE ANALYSIS
Theorem E.1. Let f : Rd → R satisfying Assumption 3.1 with μ > 0. Let w* ∈ Rd be the optimal
solution to f and assume sk/desk functions satisfying Theorem 4.2. Suppose η := ngiobai ∙ niocai has
the property that η ≤([十匕江,then
E[f (wt+1)] - f (w*) ≤ (1 - μη)t ∙ (f (w0) — f (w*))
Proof. We shall first bound f (wt+1) - f (wt):
f(wt+1) — f”) ≤hwt+1 — Wt Vf (wt)i + 2kwt+1 - wtk2
= hdeskt(∆tet), Vf (wt)i + 2∣∣deskt(∆Wt)k2
1N	t	t
=-hηglobal ∙ deskt( N Eskt (ηiocal ∙ Vfc(Wt))), Vf (wt)i
c=1
L	1N	t 2
+ ∙2kηgiobai ∙ deskt( N Eskt (ηιocal ∙ Vfc(Wt)))k22
c=1
=-ngiobai ∙ niocai ∙<des%(Skt(Vf(Wt))), Vf (wt)i
+ (ngiobai ∙ niocai)2 ∙ Ildeskt(Skt(Vf(Wt)))k2
where the first step uses the L-smoothness condition of f, and the last step uses the linearity property
of sk/desk functions.
Taking expectation over iteration t conditioning on Ft and note that only Wt+1 depends on random-
ness at t, we get
E[f(Wt+1) -f(Wt) |Ft]
Ln2
≤ - n ∙hE[deskt(skt(Vf (wt))) | Ft], Vf(Wt)〉+ 十 E[∣∣deskt(skt(Vf(Wt)))∣∣2 | Ft]
≤ - n ∙hVf (wt), Vf (Wt)i + Ln2(1 + α) ∙ ∣Vf (Wt)∣2
≤ - 2 ∙kVf(Wt)k2
≤ - μn ∙ (f(Wt) - f (w*))	⑼
where the second step comes from the fact that deskt(skt(h)) is an unbiased estimator for any fixed
h ∈ Rd and the bound on its variance, the third step comes from n ≤ ([十)二,and the last step
comes from Fact C.5.
Upon rearranging and subtracting both sides by f(W*), we get
E[f (Wt+1)] - f (w*) | Ft] ≤ (1 - μn) ∙ (f (Wt) - f (W*))	(10)
Note that if we apply expectation over Ft on both sides of Eq. (10) we can get
E[f (Wt+1)] - f (w*) ≤ (1 - μn) ∙ (E[f (Wt)] - f (w*))	(11)
Notice since 1 - μn ≤ 1, this is a contraction map, if we iterate this recurrence relation, we will
finally get
E[f (Wt+1) - f (w*)] ≤ (1 - μn)t ∙ (f (w0) - f (w*)).	(12)
□
E.3 CONVEX f CONVERGENCE ANALYSIS
Assume f is a convex function, we obtain a convergence bound in terms of the average of all pa-
rameters.
29
Under review as a conference paper at ICLR 2022
Theorem E.2. Let f : Rd → R satisfying Assumption 3.1 with μ = 0. Suppose sk/desk functions
satisfying Theorem 4.2. If η := global ∙ niocai ≤ 2(l+α)L' then
E[f(wτ) — f5≤ ≡⅛w^
η ∙(T +I)
where WT := ^+^ PT=O Wt and w* ∈ Rd is the optimal solution.
Proof. We shall first compute the gap between wt+1 and w*:
kWt+1 - W*k22
= kwt - deskt (∆wet) - w*k22
=∣∣wt - η ∙ deskt(skt(Vf (Wt)))- w*∣∣2
=IlWt- w*∣∣2 + η2 ∙ Ildeskt(Skt(▽/(Wt)))∣∣2 - 2η ∙(wt - w*, deskt(skt(Vf (wt))))	(13)
By unbiasedness of deskt ◦ skt, we have
E[hwt - w*, deskt(skt(Vf(wt)))i | Ft] = E[hwt - w*, Vf(wt)i | Ft]	(14)
Taking total expectation of Eq. (13) and plug in Eq. (14), we get
E[Iwt+1 - w* I22 | Ft]
=E[∣wt- W*k2	|Ft]+ η2	∙E[∣deskt(skt(Vf(wt)))∣2 | Ft] - 2η ∙ E[(wt	- w*, Vf(Wt))| Ft]
≤ E[∣wt- W*k2	|Ft]+ η2	∙	(1 + α) ∙ E[∣Vf(wt)∣2 | Ft] + 2η ∙ E[(w* -	wt, Vf(Wt))| Ft]
≤ E[∣wt- w*k2	|Ft]+ η2	∙	(1 + α) ∙ E[∣Vf(wt)∣2 | Ft] + 2η ∙ E[f(w*)	- f (wt) | Ft] (15)
where the second step follows	from the variance of deskt ◦ skt, and the last step follows from the
convexity of f . Taking the expectation over Ft and re-organizing the above equation, we can get
2η ∙ E[f (wt)	- f (w*)]	≤ E[∣wt	- w*∣2] - E[∣wt+1 -	w*∣2]+ η2 ∙ (1 + α)	∙ E[∣Vf(wt)∣2]
≤ E[∣wt	- w*∣2] - E[∣wt+1 -	w*∣2]+ η2 ∙ (1 + α)	∙ 2L ∙ E[f (wt) - f (w*)]
where the second step follows from the convexity and L-smoothness of f. Rearrange the above
inequality, we have
(2η - η2 ∙ (1 + α) ∙ 2L) ∙ E[f(wt) - f (w*)] ≤ E[∣wt - w*∣2] - E[∣wt+1 - w*∣2]
Note η ≤ 2(l+α)L, we have
η ∙ E[f (wt) - f (w*)] ≤ E[∣wt - w*∣2] - E[∣wt+1 - w*∣∣2]
Sum over all T iterations, we arrive at
T
η ∙ XE[f (wt) - f (w*)] ≤E[∣w0 - w*∣2] - E[∣wτ +1 - w*∣2] ≤ E[∣w0 - w*∣2]	(16)
t=0
Let WT = T+1 PT=O wt denote the average of parameters across iterations, then by convexity of f,
we conclude:
E[f(wτ) - f(w*)]≤ e⅛*!
η ∙(T + 1)
□
E.4 NON-CONVEX f CONVERGENCE ANALYSIS
Next, we prove a version when f is not even a convex function, due to loss of convexity, we can
no longer bound the gap between E[f (wt)] and f(w*), but we can instead bound the minimum (or
average) expected gradient.
30
Under review as a conference paper at ICLR 2022
Theorem E.3. Let f : Rd → R be an L-smooth function (Def. C.1) and sk/desk functions satisfying
Theorem 4.2, let w* ∈ Rd be the optimal solution to f. Suppose η := niocai ∙ ngiobai ≤ ^十七立,then
2
min E lVf(Wt)k2] ≤ ——(E[f (w0)] - f(w*))
t∈[T]	η(T + 1)
Proof. Note that the only place we used strongly-convex assumption in the proof of Theorem E.1 is
Eq. (9), so by the same analysis, we can get
E[f(wt+1)-f(wt) |Ft] ≤-2 ∙kVf(wt)k2
Rearranging and taking total expectation over Ft, we get
2
E[kVf (wt)k2] ≤ -(E[f (wt)] - E[f(wt+1)])
η
Averaging over all T iterations, we get
1T	2 T
T+1 ∑E[kVf (wt)k2] ≤ η(T+1)]T(E[f (wt)] - E[f(wt+1)])
2
=η(T + 1)(E[f Di — E[f(wT)])
2
≤ F	(E[f(w0)] — f(w*))
η(T+ 1)
This implies our final result:
2
min E[kVf(wt)k2] ≤	IIM(w0)] — f(w*))
t∈[T]	η(T+ 1)
□
Remark E.4. Notice due to the structure of sk/desk functions, i.e., their variance is bounded in
terms of true gradient, the convergence rate depends completely on the term .十匕江∙ IfHs a con-
stant, then we essentially recover a convergence rate of gradient descent. On the other hand, if
(i+，)l ≤ √t, then we get a similar convergence rate as SGD. One clear advantage ofour sk/desk
functions is they don’t introduce extra noise term as in SGD, since we can choose appropriate step
size to absorb the variance term.
F k-STEP CONVEX & STRONGLY-CONVEX fc ANALYSIS
F.1 Preliminary
In this section, we assume each fc satisfies Assumption 3.1 and ηglobal = 1. For notation simplicity,
we also denote ut,-1 = ut-1,K-1 for t ≥ 2.
cc
DefinitionF.1. Let (t, k) ∈ {1, ∙∙∙ ,T + 1}×{-1,0,1,…，K一 1}, we define thefollowing terms
for iteration (t, k):
ut,k := — X UT, rt,k := ut,k - W*
Nc
to be the average of local parameters and its distance to the optimal solution,
1N
gt,k := Vfc(Uy), gt,k := NN EVfc(Uck)
c=1
to be the local gradient and its average,
1N
Vt,k := NNX kuC,k -ut,kk2
c=1
31
Under review as a conference paper at ICLR 2022
to be the variances of local updates,
1N
σ2 = NN X kVfc(w*)k2
c=1
to be a finite constant that characterize the heterogeneity of local objectives.
We also define the following indicator function: let l ∈ R, then we define 1{x=l} to be
1{x=l}
1 if x = l,
0 otherwise.
F.2 Unifying the update rule of Algorithm 1
Lemma F.2. We have the following facts for utc,k and uet,k:
uC，0 = ut,0
Utck = Utcj- ηiocal ∙ gt,k-1, ∀k ≥ 1
K-1
ut,k = ut,k-1 — niocai ∙ gt,k-1 + 1{k=0} ∙ niocai ∙ (Id - deskt ◦ skt)( X gt-1,i), ∀(t,k) = (1, 0)
i=0
where Id : Rd → Rd is the identity function.
Proof. First two equation directly follows from the update rule of Algorithm 1.
For k = 1, 2,…，K 一 1, taking the average of the second equation We obtain:
ut,k= Utk-I- ηiocai∙ gt,k-1
For k = 0 and t ≥ 2, We have
K-1
ut,0 = ut-1,0 — niocai ∙ deskt(skt( X gt-1,i))
i=0
K-1	K-1	K-1
=u	1，°	一	nlocal X g	1，'	+	nlocal X	gt-1,i — niocai ∙ deskt(skt(	X	gt-1,i))
i=0	i=0	i=0
K-1
=ut-1,K-1 — niocai ∙ gt-1,K-1 + niocai ∙ (Id - deskt ◦ skt)( X gt-1,i)
i=0
Combining above results together, we prove the third equation.	□
F.3 Upper bounding ∣∣gt,k∣∣2
Lemma F.3. Suppose for any c ∈ [N ], fc : Rd → R is convex and L-smooth. Then
kgt,kk2 ≤ 2L2Vt,k + 4L(f(Ut,k) — f(w*))
Proof. By triangle inequality and Cauchy-Schwartz inequality, we have
kgt,k k2 = kgt,k -Vf (ut，k) + VfEk )k2
≤ 2kgt,k -Vf (Ut，k)∣∣2 + 2∣∣Vf(ut,k)k2
where the first term can be bounded as
1N	1N
kgt,k - Vf(ut,k)k2 = kN EVfc(Uc,k) - N EVfc(ut,k)k2
c=1	c=1
1N
≤ IN EkVfc(uc,k) - fc(Utk )k2
c=1
32
Under review as a conference paper at ICLR 2022
L2 N
≤ N X kuC，k - ut，k k2
c=1
and the second term can be bounded as follows:
kvf(ut,k )k2 = kW(ut,k )-W(w*)k2
≤ 2L(f(Ut,k) — f(w*))
where the last step follows from that f is L-smooth and Fact C.4.
Combining bounds on these two terms, we get
2L2 N
kgt,kk2 ≤ -N X kuC，k - ut,kk2 + 2L2kut,k - w*k2
c=1
≤ -L2Vt，k + 4L(f (ut,k) - f (w*))
□
F.4 LOWER BOUNDING <Ut,k - w* ,gt,k)
Lemma F.4. Suppose each f satisfies Assumption 3.1 with μ ≥ 0, then
hut,k - w*,gt,ki ≥ f(ut,k) - f(w*) - -Vt,k + -kut,k - w*k2
Proof. We will provide a lower bound on this inner product:
1N
hut,k - w*,gt,ki = N Nhut，k - w*, Vfc(Uc，k)i
It suffices to consider each term separately:
hut,k - w*, Vfc(U乎)i = hut,k - UtCk + UtCk - w*, Vfc(UtCk)i
=hUt,k -Uc,k, Vfc(Uc,k)i + hUc,k - w*, Vfc(Uc,k)i
The first term can be lower bounded via L-smoothness:
hUt,k - Uc,k, Vfc(Uc,k)i ≥ fc(Ut,k) - fc(Uc,k) - L kUt,k -Uc叫∣2
The second term can be lower bounded via convexity:
hUc,k - w*, Vfc(Uc,k)i ≥ fc(Uc,k) - fc(w*) + -kUc,k - w*k2
Combining these two bounds and average them, we get a lower bound:
1 ʌ	L	μ
hUt,k - w*,gt,ki ≥ N X(fc W) - fc(w*) - -kUt,k - Uc,kk2 + -kUc,k - w*k2)
c=1
≥ N X (fc Htk)- fc(w*)) - - v t,k + - kUt,k - w*k2
c=1
=f(Ut,k) - f (w*) - L V t,k + μ kUt,k - w*k2
□
33
Under review as a conference paper at ICLR 2022
F.5 UPPER BOUNDING VARIANCE WITHIN K LOCAL STEPS
Lemma F.5. Suppose each f is convex and L-smooth. Assume niocai ≤ 8LK. Thenfor any t ≥ 0,
K-1	K-1
X Vt,k ≤ 8η2ocaiLK2 X (f (ut,k) - f (w*)) + 4η2ocaiK3σ2
k=0	k=0
Proof. By Lemma F.2, We know Vt,0 = 0 for any t ≥ 0. Consider k ∈ {1,2,…，K - 1},we have
Vt,k
c=1
N	k-1	k-1
=N X "Xηlocal ∙ gC，- ut,0 + X ηlocal ∙ g'，' k 2
c=1	i=0	i=0
2 N k-1
=ηNτ X kX(gt'i- gt，i)k2
c=1	i=0
2 N k-1
≤ ηloNk XX kgt,i-gt，ik2
c=1 i=0
2	N k-1
≤ ηloNK XX kgt，ik2
(17)
where the second step follows from Lemma F.2, the last step follows from gt,i being the average of
gct，i . By Cauchy-Schwartz inequality, we further have:
kgc,ik2 ≤ 3kgt,i - Vfc(Ut,i)k2 + 3kvfc(ut,i) - Vfc(w*)k2 + 3kVfc(w*)k2
≤ 3L2kuc,i - Ut,ik2 + 6L(fc(Ut,i) - fc(w*) + hw* - Ut,0, Vfc(w*)i) + 3kVfc(w*)k2.
where the last step follows from applying L-smoothness to the first and second term.
Averaging with respect to c,
1
NN E kgt,ik2 ≤ 3L2Vt,i + 6L(f (Ut,i) - f(w*)) + 3σ2.
c=1
Note that the inner product term vanishes sinceN P=I Vfc(w*)= Vf(w*)=0.
Plugging back to Eq. (17), we obtain
2	N k-1
Vt,k ≤ TXXkgt,ik2
c=1 i=0
k-1
≤ η黑aiKX(3L2Vt,i + 6L(f(Ut,i) - f(w*)) + 3σ2).
i=0
Summing up above inequality as k varies from 0 to K - 1,
K-1	K-1 k-1
X Vt,k ≤ η2ocaiK X X(3L2Vt,i + 6L(f(Ut,i) - f(w*)) + 3σ2)
k=0	k=0 i=0
K-1 K-1
≤ η2ocaiK X X (3L2V t,i + 6L(f (Ut,i) - f(w*)) + 3σ2)
k=0 i=0
K-1	K-1
=3η2ocaiL2K2 X V t,i + 6η2ocaiLK K X (f (ut,i) - f(w*)) + 3褚℃小 3σ2
i=0	i=0
34
Under review as a conference paper at ICLR 2022
Rearranging terms we obtain:
K-1	K-1
(1 - 3η2ocaiL2κ2) X Vt,k ≤ 6η2ocaiLK2 X (f (ut,i) - f (w*)) + 3η2ocalK3σ2
k=0	i=0
Since ηlocal ≤ 8L1K,We have 1 - 3η20caiL2K2 ≥ 3, implying
K-1	K-1
X Vt,k ≤ 8η2ocalLK2 X (f (ut,i) - f (w*)) +4η黑aiK3σ2
k=0	i=0
□
F.6 BOUNDING THE EXPECTED GAP BETWEEN ut,k AND w*
Lemma F.6. Suppose each f satisfies Assumption 3.1 with μ ≥ 0. If sk/desk satisfying Theo-
rem 4.2 and niocai ≤ 土, then for any (t,k) = (1,0), we have
3
E[kut,k - w*k2] ≤ (1 - μηiocai) E[kut,k-1 - w*k2] + DniocaiLE[Vt,k-1] - niocaiE[f(ut,k-1) - f(w*)]
K-1	K-1
+ 1{k=0}n2ocaiaKGL X E[Vt-1,i]+4L X E[f (ut-1,i) - f (w*)])
i=0	i=0
Proof. By Lemma F.2, We have for any (t, k) 6= (1, 0),
K-1
ut,k = ut,k-l - niocai ∙ gt,k-1 + 1{k=0} ∙ niocai ∙ (Id- deskt ◦ skt)( X gt-1,i)
i=0
Therefore, denoting ht := (Id - deskt ◦ Skt)(PK-1 gt-1,i), We have
∣∣ut,k - w*∣∣2 = ∣∣ut,kτ - w* - niocai ∙ gt,k-1 + 1{k=0}niocai ∙ ht∣∣2
=IM-1 -w*k2 + 稔cai ∙ kgt,k-1k2 - 2niocaihut,kτ - w*,gt,k-1i
+ 2niocai1{k=0} WT- W*,hti - 2niocai1{k=0} &"',吟
+ n2ocai1{k=0} ∙ Ilhtk2	(18)
Note by Theorem 4.2, We have:
E[deskt(skt(h))] = h,	E[∣deskt(skt(h))∣2] ≤ (1 + α) ∙ ∣∣h∣2
hold for any vector h. Hence, by taking expectation over Eq. (18),
E[∣ut,k - w*∣2∣Ft] = E[kut,k-1 - w*k2∣Ft]+ niocai ∙ E[kgt,k-1k2|Ft]
-2niocai E[hut,k-1 - w*,gt,k-1i∣Ft] + 1{k=0} ∙ ni2ocai ∙ E[∣ht∣2∣Ft]
Note that since E[ht | Ft] = 0, so the tWo inner products involving ht vanishes.
Since
K-1
E[∣htk2∣Ft] = E[k(Id - deskt ◦ Skt)(X gt-1,i)k2∣Ft]
i=0
K-1
≤ αE[∣ X gj,ik2Ft]
i=0
K-1
≤ αK X E[kgt-1,i∣2∣Ft]
i=0
Taking total expectation, We have
E[∣Ut,k - w*k2]
35
Under review as a conference paper at ICLR 2022
≤ E[kUt,k-1 — W*k2]+ η2ocal ∙ E[kgt,k-1k2] — 2ηiocal .^-1 — W*,于k-1)]
K-1
+ 1{k=0} ∙ η2ocal ∙ αK X E[kgt-1,ik2]
i=0
≤ E[kut,k-1 — W*k2] + η2ocai ∙ E[2L2Vt,k-1 +4L(f (ut,k-1) — f (w*))]
— 2ηiocai E[f (ut,k-1) — f(w*) — L V t,k-1 + μ kut,k-1 — W*k2]
K-1
+ 1{k=0} ∙ η2ocai ∙ αK X E[2L2Vt-1,i + 4L(f (ut-1,i) — f (w*))]
i=0
≤ (1 — μηiocai) E[kut,k-1 — W*k2]+ ηiocal ∙ L ∙ (1 + 2ηiocaiL) ∙ E[Vt,k-1]
— 2ηiocal ∙ (1 — 2ηioCalL) ∙ E[f (ut,k-I)- f(w*)]
K-1	K-1
+ 1{k=0} ∙ η2ocal ∙ ɑK ∙(2L2 X E[Vt-1,i] + 4L X E[f (ut-1,i) - f (w*)])
i=0	i=0
where the second step follows from Lemma F.3 and Lemma F.4. Since ηlocal ≤ 在,We have
3
E[kut,k — W*k2] ≤ (1 — μηlocal) E[∣∣Ut,k-1 — W*k2] + - ηlocal L E[V t,k-1] — ηlocal E[f (Ut,k-1) — f(w*)]
K-1	K-1
+ 1{k=0}η2ocalαK(2L2 X E[Vt-1,i]+4L X E[f (ut-1,i) - f (w*)])
i=0	i=0
□
F.7 Main result: convex case
Theorem F.7. Assume each fc is convex and L-smooth. If Theorem 4.2 holds and ηlocal ≤
1
8(1 + α)LK，
E[f (WT) — f (w*)] ≤ 4- Kw*k2] + 32η2ocalLK2σ2,
ηlocalKT
where WT = KT (PT=I PK-01 ut,k) is the average over parameters throughout the execution of
Algorithm 1.
Proof. Summing up Lemma F.6 as t varies from 1 to T and k varies from 0 to K — 1,
E[kuT +1,0 - w*k2] - E[kw0 - w*k2]
T K-1	T K-1
≤ 2 ηlocalL XX E[V t,k]— ηlocal XX
E[f(Ut,k) — f(w*)]
t=1 k=0	t=1 k=0
T K-1	K-1	K-1
+ XX 1{k=0}ηLalaK(2L2 X E[V t,i]+4L X E[f (ut,i) - f (w*)])
T K-1	T K-1
=2 ηlocalL XX
E[V , ] — ηlocal XX
E[f(Ut,k) — f(w*)]
T K-1	T K-1
+ ηl2ocalαK 2L2XXE[Vt,i] + 4LXX E[f(ut,i)-f(w*)])
t=1 i=0	t=1 i=0
-	T K-1
=ηlocalL(2 + 2ηlocalαLK) XX
E[Vt,k]
T K-1
— ηlocal(1 — 4ηlocalαLK) XX
E[f (ut,k) — f(w*)]
t=1 k=0
36
Under review as a conference paper at ICLR 2022
T K-1	T K-1
≤ 2ηiocaiL X X E[Vt,k] - 2niocai X X E[f (ut,k) - f (w*)]
T	K-1	T K-1
≤2ηlocalLX(8ηl2ocalLK2XE[f(ut,i) - f (w*)] + 4η2ocalK3σ2) - 2ηlocal XX
E[f(Ut,k) — f(w*)]
t=1	i=0	t=1 k=0
T K-1
≤ — 4niocai X X E[f (ut,k) - f (w*)]+ 8η3ocaiLK3Tσ2
t=1 k=0
where the fourth step follows from niocai ≤ gα⅛κ, the 山St SteP follows from niocai ≤ 8⅛. Rear-
ranging the terms, we obtain
KT XKX E[f(ut,k) - f(w*)] ≤ 4E[kw° Kw*k2] + 32n2ocaiLK2σ2
KT t=1 k=0	niocaiKT
Finally, by the convexity of f we complete the proof.	□
Now we are ready to answer the question: how much communication cost is sufficient to guarantee
E[f(WT) - f(w*)] ≤ e? we have the following communication cost result:
Theorem F.8. Assume each fc is convex and L-smooth.	If Theorem 4.2 holds. With
o (e[∣∣w0 - w*k2]Nmax{Ld, σ√2}) bits of communication cost, Algorithm 1 outputs an e-
optimal solution wτ satisfying:
E[f(WT) - f(w')] ≤ e,
where WT = KT (PT=I PK01 ut,k)∙
Proof. To calculate the communication complexity, we first note communication only happens in
sync steps. Specifically, in each sync step, the algorithm requires O(N bsketch) bits of communica-
tion cost, where bsketch denotes the sketching dimension. Therefore, the total cost of communication
is given by O(N bsketch T). To obtain the optimal communication cost for e-optimal solution, we
choose T, K, niocai and bsketch by solving the following optimization problem:
min	N bsketch T
T,K,ηlocal ,bsketch ,α
s.t. 0 < niocai ≤ 8(ι + α)LK
4E[kw0- w* k2] ≤ e
niocaiKT	≤ 2
32n2ocaiLK2σ2 ≤ 2e
d ≥ bsketch =。(一) ≥ 1
α
where d is the parameter dimension and the last constraint is due to Theorem 4.2. Above constraints
imply:
T ≥ 8E[kw0- w*k2]
niocaiKe
, Kniocai ≤ min{
1	_1 /ɪ}
8(1 + a)L, 8σ∖∣L}
2
Therefore, when e ≥(1：", the optimal solution is given by
Kniocai = .ɪ , T= 64 E[kw0-w*k2](1 + a)L , bsketch =。( L)
8(1 + α)L	e	α
and the corresponding optimal communication cost is O( E[kw -W k2LNd).
37
Under review as a conference paper at ICLR 2022
2
when e < a;产工,the optimal solution is given by
Krl	__ι rɪ T_64E[kw0-w*k2σ√L] b — O(d
KnlOcal= 8σ V L, T =	e3/2	, bsketch = O(α)
and the corresponding optimal communication cost is O( E[kw -[^^Nd )∙
Combining above two cases, the optimal α is given by O(d), and the corresponding optimal com-
munication cost will be O(E[∣∣w0 — w*∣∣2]N max{L, σ^3√L}).	□
F.8 Main result: strongly-convex case
Theorem F.9. Assume each f is μ-strongly convex and L-smooth. If Theorem 4.2 holds and
nlocal ≤ 8(1+a)LK'
E[f(wT +1) - f(w*)] ≤ 2 E[kw0 -w*k2]e-μηiocalT +4nl2ocalL2K3σ2∕μ.
Proof. Summing up Lemma F.6 as k varies from 0 to K -1, then we have for any t ≥1,
K-1	K-1
(E[kUt+1,0 - w*k2]+ X E[kut,k - w*k2]) - (1 - μnlocal) X E[kut,k - w*k2])
k=1	k=0
3	K-1	K-1
≤ 2nlocalL X E[Vt,k] - nlocal X E[f(ut,k) - f (w*)]
k=0	k=0
K-1	K-1	K-1
+ X 1{k=0}n2θcalɑK(2L2 X E[Vt,i]+4L X E[f (ut,i) - f (w*)])
k=0	i=0	i=0
K-1	K-1
=2nlocalL X E[Vt,k] - nlocal X E[f(ut,k) - f (w*)]
k=0	k=0
K-1	K-1
+ niocalaK(2L2 X E[Vt,i] + 4L X E[f (ut,i) - f (w*)])
i=0	i=0
K-1	K-1
=nlocalL(3 + 2nlocalαLK) X E[Vt,k] - nlocal(1 - 4nlocalαLK) X E[f(ut,k) - f(w*)]
k=0	k=0
K-1	1 K-1
≤ 2nlocalL X E[Vt,k]
-2 nlocal X E[f(ut,k) - f(w*)]
k=0	k=0
K-1	K-1
≤ 2nlocalL(8nl2ocalLK2 X E[f(ut,i) - f (w*)] + 4niocalκ3σ2) - -nlocal X E[f(ut,k) - f (w*)]
i=0	k=0
1	K-1
≤ - 4nlocal E 园/®") - f (w*)]+8n3)calLK%?
k=0
where the fourth step follows from nlocal ≤ 8oLK, the last step follows from nlocal ≤ 8Lk. Rear-
ranging the terms, we obtain
E[kut+1,0 - w*k2] ≤ (1 - μnlocal) E[kut,0 - w*k2]+8n3ocalLK3σ2
implying
E[kut+1,0 - w*k2] - 8n2ocalLK3σ2∕μ ≤ (1 - μnlOcal)(E[kut,0 - w*k2] - 8n2ocalLK3σ2∕μ).
Therefore, we have
E[kwτ+1 - w*k2] - 8nlOcalLK3σ2∕μ ≤ (1 - μnlocal)T(E[kw0 - w*k2] - 8nlOcalLK3σ2∕μ)
38
Under review as a conference paper at ICLR 2022
≤ E[kw0 - w*∣∣2]e-μηlocalτ
Finally, by L-smoothness of function f , we obtain
E[f(wT +1) — f(w*)] ≤ L E[kwT +1 — w*k2] ≤ L E[kw0 — w*k2]e-μηiocalT 十4曲上也2衣3σ2∕μ.
□
Theorem F.10. Assume each f is μ-strongly convex and L-smooth. If Theorem 4.2 holds. With
O (LμN max{d, yj^ } log( L 旧/仅^—- k2])) bits of communication cost, Algorithm 1 outputs an e-
optimal solution wT satisfying:
E[f(wT) — f(w*)] ≤ e.
Proof. To calculate the communication complexity, we first note communication only happens in
sync steps. Specifically, in each sync step, the algorithm requires O(N bsketch) bits of communica-
tion cost, where bsketch denotes the sketching dimension. Therefore, the total cost of communication
is given by O(N bsketch T ). To obtain the optimal communication cost for e-optimal solution, we
choose T, K, ηlocal and bsketch by solving the following optimization problem:
min	N bsketchT
T,K,ηlocal ,bsketch ,α
s.t. 0 < ηlocal ≤ 8(1 + ɑ)LK
L E[kw0 — w*k2]e-μηlocalT ≤ e
4ηj0caiL2K3σ2∕μ ≤ e
d ≥ bsketch = O(I) ≥ 1
α
where d is the parameter dimension and the last constraint is due to Theorem 4.2. Above constraints
imply:
T≥
1 log(LE[kw0— w*k2])
μηiocal	e
ηlocal ≤ min{
_1______、匹}
8(1 + α)LK, 2LKσ V 2K}
Therefore, the optimal value is given when K
given by
1. When e ≥ i6(1+α)2μ
σ2
, the optimal solution is
1 T 8(1 + α)L	LE[kw0 一 w"||2]、L	Cf d、
ηlocal = 8(1+0)L，T=	lOg(	-- ), bsketch = O( Z )
and the corresponding optimal communication cost is O(LNd log(LE[kw Jw k2])).
when e <
σ2
16(1+a)2μ
, the optimal solution is given by
1	∕μe	2Lσ ∕2^	LE[kw0 ― w*||2]、L	Cf d∖
ηlocal = 2Lσv τ, t =萍 V elog( e ), bsketch = O( α)
and the corresponding optimal communication cost is O( α,LN√ log(LE[kw Jw 卜])).
Combining above two cases, the optimal α is given by O(d), and the corresponding optimal com-
munication cost will be O( LμN max{d, J^} log(L E[kwe-w k2])).	□
G k-STEP NON-CONVEX f CONVERGENCE ANALYSIS
In this section, we present convergence result for non-convex f case in the k-local-step regime. In
order for the proof to go through, we assume that for any c ∈ [N] and any w ∈ Rd, there exists a
universal constant G such that
kVfc(w)k2 ≤ G.
39
Under review as a conference paper at ICLR 2022
Throughout the proof, we will use Ft to denote the sequence wt-1 , wt-2, . . . , w0. Also, we use η
as a shorthand for global ∙ ηiocai.
Note that in k-local-step scheme, the average of local gradients is no longer the true gradient, there-
fore, we can no longer bound everything using the true gradients. This means it’s necessary to
introduce the gradient norm upper bound assumption.
Lemma G.1. Let f : Rd → R satisfying Assumption 3.1 and sk/desk functions satisfying Theo-
rem 4.2. Further, assume ηiocai ≤ 2l1k. Then
Ef(Wt+I)- f(Wt) | Ft] ≤ - ηglobal ∙ ∣∣Vf(wt)k2 + η ∙ L ∙ K2 ∙ G2 ∙ (ηiocal + 2 ∙ (I + a))
Proof. We start by bounding f (wt+1) - f(wt) without taking conditional expectation:
f(Wt+1) - f(Wt)
≤ hwt+1 - wt, Vf(Wt)i + L∣wt+1 - Wtk2
=hdeskt(∆Wt), Vf (wt)i + LL ∣deskt(∆Wt)k2
=A + LB
2
where
1 N	K -1
A := - hηglobal ∙ deskt( N X skt( X ηiocal ∙ V fc(ut,k))), Vf (wt)i
c=1	k=0
NK
B := Ilnglobal ∙ deskt(N X Skt(X nlocal ∙ VfC(UC,^))) ∣2
c=1	k=1
Bounding E[A | Ft] Using the fact that skt/deskt are linear functions and E[deskt (skt (h))] = h,
we get
N K -1
E[A | Ft] = - hnglobal ∙ N XX nlocal ∙ VfC(UC,k), Vf(Wt)
c=1 k=0
1 N	K -1
=-nglobal ∙ hN X(X nlocal ∙ VfC(UC,k ) - VfC(Wt) + VfC(Wt)) , Nf(Wty)
N K -1
=-nglobal TlVf(W, ) I 2 + nglobal ∙ nlocal ∙(XXhVfC(UtC,k) - VfC(Wt), Vf (Wt)i
C=1 k=0
It suffices to bound the inner product, notice for k = 0, the inner product is 0, so assume k ≥ 1:
hVfC(UtC,k) - VfC(Wt), Vf(Wt)i
≤∣VfC(uC'k) -VfC(Wt)∣2 ∙kVf(Wt)∣2
≤ L ∙ kutok - Wt∣2 ∙kVf(Wt)∣2	(19)
where the gap between UtC,k and Wt can be further expanded:
∣UtC,k - Wt∣2 = ∣UtC,k - Ut0,k ∣2
k-1
= ∣nlocal X VfC(UtC,i)∣2
i=0
k-1
≤ nlocal X ∣VfC(UtC,i)∣2
i=0
40
Under review as a conference paper at ICLR 2022
≤ ηiocal ∙ k ∙ G	(20)
Plug in Eq. (20) to Eq. (19), we get
Efc(Utck ) - Vfc(wt), Vf (wt)i ≤ L ∙ ηiocal ∙ k ∙ G2
Recall that η = ngiobai ∙ ηiocai. PUt things together, We finally obtain a bound on E[A | Ft]:
K-1
E[A I Ft] ≤ - ηglobal ∙ kVf (Wt) k2 + η ∙ ηiocal ∙ L Y X k) ∙ G2
k=0
≤ - ηglobal TlVf(Wt) k 2 + η ∙ ηiocal ∙ L ∙ K2 ∙ G2	(21)
Bounding E[B I Ft] Using the fact that skt/deskt are linear functions, We get
N K-1
B = ηglobal , ηlocal , N2 . k X X
deskt(skt(Vfc(utc,k)))k22
c=1 k=0
N K-1
≤ ηglobal ∙ ηlocal ∙ N2 ∙ N ∙ K X X ∙kdeskt(skt(Vfc(uc,k )))k2
c=1 k=0
K N K-1
=η2 ∙ N ∙∑∑kdeskt(skt(Vfc(uc,k)))∣∣2
c=1 k=0
Using variance bound of deskt(skt(h)), We get
K	N K-1
E[B I Ft] ≤ η2 ∙ N ∙ (1 + α) ∙ £ £ kVfc(uc,k)k2
c=1 k=0
K	N K-1
≤ η2 ∙ N ∙ (1 + α) ∙ X X G2
c=1 k=0
=η2 ∙ K2 ∙ (1 + α) ∙ G2	(22)
Put things together Put the bound on E[A I Ft] and the bound on E[B I Ft], We get
E[f(Wt+1) - f(Wt) I Ft]
≤ -	ngiobai ∙ kVf (Wt)k2 + η	∙	ηiocai ∙ L ∙ K2 ∙ G2 + 2 ∙η2 ∙ K2 ∙ (1 + α)	∙ G2
=-ngiobai ∙ kVf (Wt) k2 + n	∙	L ∙ K2 ∙ G2 ∙ (niocal + 2	∙ (I + a))	口
Theorem G.2. Let f : Rd → R be L-smooth. Let w* ∈ Rd be the optimal solution to f and assume
sk/desk functions satisfying Theorem 4.2. Then
min E[kVf(wt)k2] ≤ (T + 11n ] b 1 ∙ (E[f(W0)] — f(w*)) + niocai ∙ LK2G2 ∙ (niocai + 2 ∙ (1 + α))
Proof. By Lemma G.1, We knoW that
E[f (Wt+I) | Ft] - f (Wt) ≤ - ngiobai ∙ kVf (Wt) k2 + n ∙ L ∙ K2 ∙ G2 ∙ (niocai + 2 ∙ (1 + a))
Rearranging the inequality and taking total expectation, We get
E[kVf (Wt)k2] ≤ ------- ∙ (E[f (Wt)] - E[f (Wt+1)]) + niocai ∙ LK2G2 ∙ (niocai + X ∙ (1 + α))
ngiobai	2
Sum over all T iterations and averaging, We arrive at
1T
TTl ]TE[kVf (Wt)k2]
T+ 1 t=0
41
Under review as a conference paper at ICLR 2022
≤ (T + ι)η ] b ] ∙ (Ef(WO)] - E[f (WT)]) + ηlocal ∙ LK2G2 ∙ (ηlocal + 2 ∙ (I + a))
≤ (T + ι)η ] b ] ∙ (Ef(WO)] - f (W)) + ηlocal ∙ LK2G2 ∙ (ηlocal + 2 ∙ (I + a))
□
H Differential Privacy
We define (, δ)-differential privacy Dwork et al. (2006b;a) as
Definition H.1. Let , δ be positive real number and A be a randomized algorithm that takes a
dataset as input (representing the actions of the trusted party holding the data). Let im(A) denote
the image of A. The algorithm A is said to provide , δ-differential privacy if, for all datasets D1
and D2 that differ on a single element (i.e., the data of one person), and all subsets S of im(A):
Pr[A(Dι) ∈ S] ≤ exp(e) ∙ Pr[A(D2) ∈ S] + δ
where the probability is taken over the randomness used by the algorithm.
We state a lemma from prior work Kenthapadi et al. (2013),
Lemma H.2 (Kenthapadi et al. (2013)). Let δ ∈ (0, 1/4) and e > 0. Let Y and Y 0 be points in
Rd such that kY - Y 0 k2 ≤ W. Then for any D ⊂ Rd, and any ∆ drawn from {N (0, σ2 )}d, where
Q ≥ 4we-1 ,log(1∕δ),thefollowing inequality holds:
Pr[Y0 + ∆ ∈ D] ≤ e ∙ Pr[Y + ∆ ∈ D] + δ.
Throughout this section, we assume that for any gradient gc and any one-entry perturbation gc0 of gc,
the magnitude of the one-entry perturbation is upper bounded by γ .
Let R ∈ Rbsketch ×d denote our sketching matrix.
Let Y0 , Y ∈ Rbsketch . Let g, g0 ∈ Rd such that g and g0 differ by exactly one entry and the magnitude
of this difference is bounded by γ. Let Y = Rg and Y0 = Rg0.
We demonstrate how to pick the corresponding variance of noise for AMS sketch. We remark that
for other sketching matrices where entries are bounded and hence its spectral norm is bounded, one
can run similar argument to obtain such results.
Fact H.3. If R is an AMS sketching matrices, then
√d∕b ≤ ∣∣Rk ≤ √d∕√b.
Proof. Recall that each entry of an AMS sketching matrix is ± -√ζ, hence its max row or column '1
norm is √. Recall that max row or column '1 norm is an upper bound on the spectral norm, hence
we know that ∣R∣ ≤ √. On the other hand, we know its spectral norm is lower bounded by its
max row or column '2 norm, hence,
∣R∣ ≥ ∣R∣f/b ≥ √d∕b.
Put things together, we have
√d∕b ≤ IlRk ≤ √d∕√b.
□
Putting it all together, we have
Lemma H.4. Let ∆ ∈ RbSketch denote the noise vector that sampled from Gaussian distribution
N(0, σ2Ib) with σ ≥ 4∖∕b^YeT /log(1∕δ), then we have Rg + ∆ is (e, δ)-differentialprivate.
42
Under review as a conference paper at ICLR 2022
Proof. By Lemma H.2, it suffices to to bound kY0 - Y k2. Note that
kY -Y0k2=kRg-Rg0k2
≤kRk∙kg -g0k2
≤
ʌʌɪ-YCTPlog(1∕δ), then our sketched gradient with noise is (e, δ)-
` etɑ	口
This means if we pick σ ≥ 4
differential private.
43