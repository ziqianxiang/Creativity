Under review as a conference paper at ICLR 2022
LASSO: Latent Sub-spaces Orientation for
Domain Generalization
Anonymous authors
Paper under double-blind review
Ab stract
To achieve a satisfactory generalization performance on prediction tasks in an un-
seen domain, existing domain generalization (DG) approaches often rely on the
assumption of the existence of fixed domain-invariant features and common hy-
potheses learned from a set of training domains. While it is a natural and important
premise to ground generalization capacity on the target domain, we argue that this
assumption could be overly strict and sub-optimal. It is particularly evident when
source domains share little information or the target domains leverages informa-
tion from selective source domains in a compositional way instead of relying on
a unique invariant hypothesis across all source domains. Unlike most existing ap-
proaches, instead of constructing a single hypothesis shared among domains, we
propose a LAtent Sub-Space Orientation (LASSO) method that explores diverse
latent sub-spaces and learning individual hypotheses on those sub-spaces. More-
over, in LASSO, since the latent sub-spaces are formed by the label-informative
features captured in source domains, they allow us to project target examples onto
appropriate sub-spaces while preserving crucial label-informative features for the
label prediction task. Finally, we empirically evaluate our method on several well-
known DG benchmarks, where it achieves state-of-the-art results.
1	Introduction
One of the most challenging problems in applying machine learning to real-world problems is to
address the domain shift encountered when test data at inference time come from different distribu-
tions compared to training data, often causing unexpectedly imperfect generalization performance.
To handle this issue, many out-of-distribution learning settings have been investigated, notably do-
main adaptation (DA) and domain generalization (DG). In particular, DA setting (Mansour et al.,
2009; Ben-David et al., 2010; Zhao et al., 2019) takes the assumption that both labeled source data
and unlabeled target data are available at the training phase, while DG setting (Blanchard et al.,
2011; Muandet et al., 2013; Ganin et al., 2016) is much more challenging due to the complete ab-
sence of any target data at training time, and the learned model is expected to perform a zero-shot
prediction on test samples. While being more challenging than DA, DG is arguably more versatile
and applicable to real-world scenarios where there is a need to rapidly deploy a prediction model on
a new target domain without any access to target data.
A common premise on which most of the existing DG approaches rely to address the domain shift
problem is to learn a shared set of features among different source domains, e.g., domain-invariant
features (Muandet et al., 2013; Ganin et al., 2016; Motiian et al., 2017; Ghifary et al., 2015; Xie
et al., 2017; Wang et al., 2019; Piratla et al., 2020; Zhao et al., 2020). By learning domain-invariant
features, these approaches identify a latent representation from multiple source domains which is
also expected encompass unseen target domains, hence reducing the domain shift. However, since
this latent space is often high-dimensional and unseen target domains could vary greatly and un-
expectedly, these domain invariance approaches could become ineffective. We argue that using a
single hypothesis which requires to be optimal for all source and target domains is rather a strong
assumption and might not have an effective algorithmic solution, hence presenting a key weakness
to these existing approaches.
In this paper, we propose a new approach to efficiently reduce the domain shift. Our idea is to
optimally decompose the high-dimensional latent feature space to lower-dimensional sub-spaces,
1
Under review as a conference paper at ICLR 2022
after which an individual hypothesis is learned (optimally) for each latent sub-space. In particu-
lar, these latent sub-spaces are formed by label-informative features which are learned from source
latent representations so that sub-space latent representations can capture sufficient and necessary
information to effectively train sub-space hypotheses. We then perform extensive experimental eval-
uations to demonstrate the merits of our methodology and framework. In sum, our contributions are
as follows:
•	We propose LAtent SubSpace Orientation (LASSO), a novel approach for DG, which aims
to learn label-informative latent sub-spaces from source latent representations, preserving
crucial label information for training qualified individual sub-space hypotheses.
•	We develop a rigorous theoretical analysis to explain how our approach can learn mean-
ingful sub-spaces by learning an indicator function Γ that helps to reduce latent domain
shift. Moreover, by linking to information theory (cf. Theorem 3), we theoretically jus-
tify the rationale behind learning the sub-space indicator function Γ as a mechanism for
element-wise selection of latent features to gather sufficient label information.
•	Finally, we empirically demonstrate that our proposed method can achieve favorable results
when evaluating our model on domain generalization benchmarks in the comparison with
state-of-the-art methods.
2	Related Work
Domain generalization (DG) approaches can be categorized into several groups: domain-invariant
representation learning, meta-learning, and augmentation/self-supervision. The works in the first
group aim to learn a domain-invariant representation with the hope of transferring well to unseen
domains. Notably, Muandet et al. (2013) construct shared components by minimizing the discrep-
ancy of the source domain marginal distributions using a kernel-based algorithm. Xie et al. (2017)
employ an adversarial training strategy to preserve the desired invariance and eliminate variations
of domain factors from features. Seo et al. (2020) combine batch normalization and instance nor-
malization to remove domain-specific styles while preserving semantic category information. Other
works (Ghifary et al., 2015; Li et al., 2018b; Ilse et al., 2020) employ auto-encoders to support
domain-invariant feature extraction.
Another efficient approach for DG is meta-learning. For example, the studies in (Li et al., 2018a;
Balaji et al., 2018) use source domains to simulate meta-train/meta-test, which encourages the
trained model to generalize better on the meta-test data, leading to a more plausible global gen-
eralization performance on the unseen target domain. Dou et al. (2019) then extend the work of Li
et al. (2018a) by combining it with metric learning loss to encourage domain-independent semantic
feature space.
Self-supervised learning and data augmentation have been also applied to DG. Typically, Carlucci
et al. (2019) propose to solve the pretext task of Jigsaw Puzzles to improve the generalization per-
formance on unseen domains. Shankar et al. (2018) augment training data with instances perturbed
along with directions of domain change. In addition, Zhou et al. (2020a) employ a classifier that
can learn the generalization on additional augmented samples of diversity pseudo-novel domains
by leveraging optimal transport theory. Zhou et al. (2020b) augment the original training data of
source domains with synthetic data from unseen domains such that augmented data are correctly
classified by the classifier while fooling the domain classifier to make the task model intrinsically
more domain-generalizable. Zhou et al. (2021) propose to mix the styles of different source do-
mains based on normalization-based style-transfer technique to effectively increase the diversity of
domains during training.
Recently, learning domain-specific information to boost classification performance in domain gen-
eralization has attracted more attention. For example, Huang et al. (2020) iteratively discard the
dominant features to exploit all useful features (including both invariant and specific features) that
highly correlate with labels. Chattopadhyay et al. (2020) propose a domain-specific mask learned
from the domain label to balance domain-invariant and domain-specific features. As a result, source
domain classification can benefit from the specialized features while retaining the generalizing ap-
plicability of domain-invariant features.
2
Under review as a conference paper at ICLR 2022
3	Proposed method
3.1	Problem setting
Let DkS = {(xik, yik)}iN=k1, k = 1, ..., K be the source labeled datasets, where k is the domain index,
NkS is the number of examples in DkS and yik ∈ Y := {1, ..., C} is the set of C classes. Let PkS and
pkS be the (marginal) data distribution and corresponding density of the source domain k. Similarly,
let pkS (y | x) denote the probabilistic labeling distribution of this domain. A domain is then defined
by a pair of data distribution and labeling function, in which its data (xik, yik) is generated by first
sampling Xk 〜PS (∙) and subsequently yf 〜PS (∙ | Xk). With a little abuse of notation, let Us also
denote k-th domain by D?, and the data generation process is (x, y)〜D?. For target domain DT,
PT and PT are its data distribution and density respectively, while PT (y | X) is the target labeling
distribution.
A mixture of the source domains is denoted by DπS := PkK=1 πkDkS. In order to sample data from
this mixture, one first sample a domain index from the categorical distribution k 〜Cat(∏), where
π specifies the mixture weights, and then sample the actual data from the corresponding domain k .
The empirical datasets obtained could hint us to estimate πk
Finally, a mixture of data distribution is defined by PπS = PkK=1 πkPkS .
K
k=1
with N S = PkK=1 NkS .
A hypothesis f : X → ∆C is a map from the common data space X to the the C-simplex label
space ∆C := α ∈ RC : kαk1 = 1 ∧ α ≥ 0 . Let ` (f (X) , y) be the loss incurred by using this
hypothesis to predict X ∈ X, given its ground-truth label y ∈ Y. Therefore, the general loss of the
hypothesis f w.r.t. the joint distribution D is:
L (f, D)= E(x,y)〜D [' (f(x) ,y)].
(1)
We examine the composite hypothesis f = h ◦ g where g : X → Z is the feature extractor
mapping the data space to a latent space and h : Z → Y is the classifier on this latent space. With
respect to the latent space, given a probabilistic labeling distribution P (y | X), let denote Pg (y | z)
as the probabilistic labeling distribution induced by P (y | X) and g, that is, for any z ∈ g (X),
pg (y | Z)= ∕g-1(z)p(y|X)P(X)dx (Johansson et al., 2019). Basically, pg (y | Z) can be regarded as a
g-1 (z) p(x)dx
weighted sum ofp (y | X) for X ∈ g-1 (Z). We further define pkS,g (y = c | Z) and pT,g (y = c | Z)
with Z ∈ g (X ) ⊂ Z as the induced conditional labeling distributions of source and target domains
on the latent space via the feature extractor g. Finally, we consider the DG setting, in which we only
possess the source labeled data and do not have any prior information of any target data during the
training process.
In what follows, we develop a rigorous theory to promote our LASSO, whose proofs can be found
in the Appendix A.
3.2	Single hypothesis and full space for domain generalization
Most existing works in DG use a single hypothesis acting on the entire latent space Z for predicting
unseen target domains. In what follows, we develop an upper-bound which is relevant to the data
shift on the latent space of the target loss L f, DT with f = h ◦ g.
Theorem 1. If the loss function ` is upper-bounded by a positive constant L, for any hypothesis
f : X → ∆C where f = h ◦ g with g : X → Z and h : Z → ∆C, the target general loss is
upper-bounded by:
a-1
α — 1	i	α
L(f, DT) ≤ exp {Rα (g#PT,g#PS)}b L1 L (f, DS) + LmaxEPS [k∆pfc (y | x)∣∣J	,
(2)
where g#PS and g# PT are Pushed-forward distributions induced by applying g on PS and PT, Ra
is the α-divergence (α	> 1), and ∆pk	(y | X) :=	pkS	(y	= c | X)	- pT (y = c |	X)cC=1	represents
the label shift between the labeling assignment mechanisms of an individual source domain and
target domain on the input space.
3
Under review as a conference paper at ICLR 2022
The bound developed in Theorem 1 supports us in characterizing the factors influencing the loss on
the target domain of a hypothesis f: (i) the label shift: EPS [k∆pk (y | x)k1], (ii) the source loss:
L ffD DS), and (iii) the latent data shift: Ra (g#PT, g#PS). Based on this theorem, We aim to find
the hypothesis f such that the upper bound of target loss is minimized. Note that the label shift
term is a natural characteristic of domains, hence it is almost unchangeable. Therefore, one seeks to
minimize the source loss, or the discrepancy betWeen latent distributions, or both of them. Certainly,
if we know target data (e.g., samples from PT), we can minimize the divergence between g#PS and
g#PT directly. HoWever, this is almost impossible for the DG setting because the target domain is
unknown beforehand when training.
To address this problem, a large number of works propose learning domain-invariant features on
a full high-dimensional latent space together with a single hypothesis on top of these domain-
invariant features. Nonetheless, due to the great variance of unseen target distributions on the full
high-dimensional latent space, the latent data shift is possibly high in many cases, which hurts the
generalization ability of the single hypothesis on unseen target domains.
Evidently, given a latent representation z with a label y in the high-dimensional latent space, only
a small portion of its features known as label-informative features is highly relevant to the label y,
while the remaining ones are redundant. By eliminating irrelevant features and grouping the latent
representations of data examples across multiple domains with the same set of label-informative
features, we can form latent label-informative sub-spaces to reduce the latent data shift, whereas
preserving sufficient label-information for training good hypotheses on those latent sub-spaces.
This can be explained from the fact that unseen target and source examples with the same label-
informative features are projected onto the same latent sub-space on which the latent data shift
between mixture of source domains and target domain becomes smaller due to the compactness of
this sub-space compared to the full high-dimensional latent space.
Furthermore, by learning multiple hypotheses, each of which corresponds to a sub-space, LASSO
allows sub-spaces to instance-wisely explore different groups of label-informative features, hence
encouraging the diversity of latent representations for achieving better generalization ability, which
concurs with the principle in (Huang et al., 2020; Chattopadhyay et al., 2020; Blanchard et al., 2021).
This intuitively boosts the generalization ability of hypotheses. The reason is that given source
examples with their labels, LASSO aims to explore possible compact groups of label-informative
features which can predict accurately the labels. In the inference time, given a target example,
if the feature extractor can successfully activate a group of label-informative features, this target
example is projected and matched with corresponding source examples on a sub-space in which a
good hypothesis is used to predict a label for the target example.
3.3	Latent sub-spaces with individual hypotheses for domain generalization
For a data sample (χ,y)〜DS, we consider latent representation Z = g(χ) ∈ RD and propose
to learn a sub-space indicator Γ which renders Γ(z) ∈ {0, 1}D. Specifically, Γ(z) specifies the
sub-space which we project z onto via ΠΓ(z) = z Γ(z), where is the element-wise product
operator. Our intuition is that Γ is able to keep the most label informative features of latent vector z
(i.e., the dth element of Γ(z): Γd(z) = 1 means that zd is label-informative and should be selected
when projecting z onto its sub-space).
Given a sub-space index m ∈ {0, 1}D, we denote the region on data space which has the same
index m as Am = {x : Γ(g(x)) = m} ⊂ X. Let PSm be the distribution restricted by PπS over
the set Am and PTm as the distribution restricted by PT over Am. Eventually, we define pSm (y | x)
as the probabilistic labeling distribution on the sub-space (Am, Pm), meaning that if X 〜Pjm,
Pm (y | X) = PK=I ∏kPS (y | x). Similarly, we define if X 〜Pm, Pm (y | X)= PT (y | x).
Due to this construction, any data sampled from PSm or PTm have the same index m = Γ (z) =
Γ(g(X)), which specifies the set of relevant feature elements for classifying data from Am. This
hints us to employ an individual hypothesis fm = hm ◦ gΓ, where fm specializes on Am, PSm and
hm specializes on the sub-space (gr (Am) ,gr#Pm) with gr := ∏γ ◦g for each m. We note that hm
operates on the sub-space indexed by m, while fm operates on its induced data space. Additionally,
since each data point X ∈ X corresponds to only a single Γ(g(X)), the data space is partitioned
4
Under review as a conference paper at ICLR 2022
into disjoint sets, i.e., X = UmM=I Am, where Am ∩ An = 0, Vm = n. Therefore, these different
hypotheses hm make use of different features, as reflected in the different sub-space index m.
We now present the way to efficiently formulate sub-space hypotheses. Let F :=
{f = h ◦ g : g ∈ G ∧ h ∈ H} be a hypothesis class of f : X → ∆C , where f = h ◦ g with
g : X → Z ∈ G and h : Z → ∆C ∈ H. Each hypothesis f = h ◦ g induces the sub-
space hypotheses fM = [fm]m∈M for which fm (x) = hm (Γ(g(x)) g (x)) for x ∈ Am with
hm(m g (x)) = h(m g (x)) can be viewed as a hypothesis on the sub-space indexed by m. We
define the general loss of fM = [fm]m∈M on the target domain as
L (fM, DT) ：= 1 X L (fm, Dm),	⑶
m∈M
where M ⊂ {0, 1}D is the set of all feasible sub-space indices m conducted from the observed data,
∖M∖ specifies the cardinality of M, and Dm is the joint distribution of (x, y) where X 〜Pm and
y 〜Pm (∙ ∖ X).
(a)
(b)
Figure 1: (a) Latent Sub-space Orientation Framework and (b) An example of reducing data shift
between source domains and target domain in the latent sub-spaces.
Figure 1a depicts the generative process of our LASSO in which the sub-space indicator Γ specifies
the sub-space to project z = g(X) onto and the projected sample is then predicted by a sub-space
hypothesis. In Figure 1b, we intuitively demonstrate why projecting onto appropriate sub-spaces is
useful to reduce latent data shift. For the red points on the latent space, the dimension 1 is commonly
label-informative, hence projecting onto this dimension is helpful to preserve label information for
constructing a good hypothesis on this sub-space. The same observation occurs for the blue points
for the dimension 2. The green points symbolize target data for which we observe that the latent
data shift on the full latent space is significantly greater than that on the sub-spaces. In addition,
Figure 1b also represents some notions in our developed theory, i.e., Am=[1,0] with the distribution
Pm=[1,0] includes the data samples whose latent representations are projected onto the sub-space
indexed by m = [1, 0] and similar notions for another sub-space. Additionally, each sub-space
index m associates with two equivalent hypotheses fm on Am and hm on the sub-space m.
The following theorems explain the intuition that matching source and target domains on good sub-
spaces helps reduce the latent data shift w.r.t. a general unseen target domain; and offer us the
training process of the sub-space indicator Γ, which extracts the most label informative features of
latent representations z.
Theorem 2.	Given a sub-space indicator Γ, if the loss function ` is upper-bounded by a positive
constant L, the sub-space target general loss is upper-bounded by:
α — 1	α — 1
L (fM, DT) ≤ 而 工 exp {Rα (gr#Pm,gr#Pm)} k [L (fm, Dm) + L∆pm] b , (4)
m∈M
where Dm is the joint distribution of (x,y) with X 〜 Pm and y 〜 Pm (∙ ∖ x)，and ∆pm :=
Ex 〜Pm [i|pm (∙ ∖X) -Pm (∙ ∖ x)ι∣1 ] ∙
Theorem 2 can be viewed as an extension of Theorem 1 in the context of LASSO with multiple
sub-spaces. This theorem shows an upper-bound of the sub-space target loss which consists of three
terms: (i) the sub-space label shift: ∆Pm , (ii) the source sub-space hypothesis losses: L fm , DSm ,
and (iii) the sub-space latent data shift: Ra (gr#Pm, gr#Pm) ∙
5
Under review as a conference paper at ICLR 2022
Since the sub-space label shift depends on the natural characteristics of domains and the source sub-
space hypothesis losses are trainable, the sub-space latent data shift (i.e., the divergence between
the mixture of source domains: gr#Pm and target domain: gr#Pm on a sub-space) is essential
to lower the upper-bound in (4). As demonstrated in Figure 1b, by projecting corresponding latent
representations onto low-dimensional sub-spaces conducted from label-informative features, we can
reduce the latent data shift on the sub-spaces due to the compression effect when projecting to the
sub-spaces. Moreover, if we can appropriately choose the good sets of label-informative features
for the sub-spaces, we can preserve sufficient label information on the sub-spaces for training good
hypotheses fm or hm with a low source sub-space hypothesis loss L fm, DSm with the aim to lower
the upper-bound in (4).
The next arising question regarding how to train a qualified sub-space indicator Γ for choosing
good sets of label-informative features to conduct low-dimensional and label-preserving sub-spaces
is addressed in the following theorem.
Theorem 3.	Let X is a random variable of source sample (i.e., drawn from PπS) and Y is a random
variable of ground-truth labels. Denote N =	m0 ∈M PπS (Am0), we then have
I (Γ (g(X)) Θ g (X) ,Y) ≥ - X PS NAm) L (fm, Dm) + const,	(5)
m∈M
where L fm , DSm is defined based on the cross-entropy loss and I denotes mutual information.
Theorem 3 gives us a hint of how to train a sub-space indicator Γ together with the sub-space
hypotheses fM = [fm]m∈M. As suggested by Theorem 3, we minimize the source sub-space
hypothesis losses L fm , DSm by
m χ PSNm)L fm, Dm) =m X PSNm)EDm [' 仿(m Θ g (x)) ,y)]
Γ,g,h	N	Γ,g,h	N
m∈M	m∈M
min EDS [`(h (m Θ g (x)) , y)] .
Γ,g,h π
(6)
Furthermore, Theorem 3 indicates that by solving the optimization problem in (6), we implicitly
learn Γ, g to maximize the mutual information I (Γ (g(X)) Θ g (X) , Y), which enables the sub-
space indicator Γ to preserve the label-informative features of z = g (x).
3.4 Latent sub -spaces learning Framework
In what follows, we present the technical details of our proposed method which solves the optimiza-
tion problem in equation 6. In particular, we discuss the way to formulate and learn the sub-space
indicator Γ, the feature extractor g : X → Z, and the classifier h : Z → ∆C, in which Γ and (g, h)
are updated alternatively.
3.4.1	Sub-space indicator
We employ a probabilistic sub-space indicator Γ (z) ∈ [0, 1]D in which Γd (z) ∈ [0, 1] represents
the probability that zd is a label-informative feature. Additionally, in our implementation, we con-
sider a 2D tensor z ∈ RD0×D, whereas each zd ∈ RD0 known as an attribute consists of D0 features
which are simultaneously selected or unselected as the dimensions of the projection sub-space. The
probabilistic sub-space indicator is formulated as Γ (z) = [Γ1 (z1) , ..., ΓD (zD)] ∈ [0, 1]D. The
element-wise product Γ (z) Θ z is defined as zdiag (Γ (z)), wherein we multiply each element in
Γ (z ) to a column of z to select or unselect the group of features in this column. Consequently,
Γd (z) = Γd (zd) is computed based solely on the group of D0 features in the attribute zd itself
rather than full latent z . That means attribute-based Γd depends on attributes which are shared
across domains instead of domains, hence, becomes more independent from domain information.
Moreover, by Theorem 3, model can still learn meaningful attributes as long as the performance on
source domains is guaranteed. In unseen domains, the target of sub-space indicator Γ is to detect
label-informative attributes which are learned in source domains instead of identifying all label-
informative attributes. For further discussion about attribute-based Γd please refer to Appendix B.1.
Finally, we minimize the following loss for updating Γ:
LI (Γ∣g, h) = E3y)F [' (h (Γ(z) Θ Z), y)]∣z=g(χ).
(7)
6
Under review as a conference paper at ICLR 2022
3.4.2	Sub-space hypotheses
To encourage the sub-space exploration and strengthen the sub-space hypotheses, given a data-label
pair (x, y)〜 DS, We sample mask indicators from the BerboUlli distribution with parameter Γd (zd),
i.e,. md 〜Ber (Γd (Zd)) to gather m = [md]D=ι, and then project onto a sub-space by mΘz. Then,
g, h are updated by minimizing the following loss function:
LH (g, h|r) = E(x,y)〜D* * * 4 S & [Em〜Ber(Γ(z)) [' (h (m G) Z) , y)]] ∣z=g(x),	⑻
where m 〜Ber (Γ (Z)) means md 〜Ber (Γd (Zd)), d = 1,…，D. For further discussion about
mask sampling please refer to Appendix B.2.
Finally, the pseudcode of our LASSO is summarized in Algorithm 1.
Algorithm 1 LASSO: Latent Sub-space Orientation for DG.
1:	Initialize: encoder g, classifier h, Sub-space indicator Γ and dataset DπS .
2:	for epoch = 1 → epochs do
3:	for ite in iterations do
4:	Sample Mini-batch: B = {(χi,yi)〜DS}
5:	Optimize LI in Eq. (7) w.r.t Γ on B.
6:	Optimize LH in Eq. (8) w.r.t. h and g on B.
7:	end for
8:	end for
9:	Return: The optimal: g*, h and Γ*.
3.4.3 Inference process
At the testing phase, we use two inference strategies: threshold and ensemble strategies
• Ensemble: for each target example x, we could compute Z = g (x), then sample T masks
mi,…,mτ 〜 Ber (Γ (z)), and ensemble as hE∩ = T PT=I h (mt Θ z). However, for
efficient computation, we approximate the ensemble prediction by: hEn ≈ h (Γ (Z) Θ Z).
• Threshold: we employ a threshold τ ∈ [0, 1] to compute the mask. Specifically, for each
target example x, we compute Z = g (x), then evaluate the mask m = 1Γd(zd)>τdD=1,
where 1 is the indicator function, and predict as h (m Θ Z).
4 Experiments
4.1 Evaluation on Benchmark Datasets
We report the empirical results on PACS (Li et al., 2017), VLCS (Torralba & Efros, 2011), and
Office-Home (Venkateswara et al., 2017) datasets using standard backbones such as AlexNet,
ResNet18, and ResNet50 for the feature extractor. Due to the paper space limit, we leave the details
of evaluation protocol, experimental settings and implementation in Appendix C.
The results on VLCS using AlexNet are reported in Table 1, the results on Office-Home using
ResNet18 and ResNet50 are reported in Table 2 and the results on PACS using three backbones are
reported in Table 3 respectively. Those empirical results clearly show that our LASSO provides
competitive classification accuracy compared to the baselines. Especially, the average results on
Office-Home and PACS in Table 2 and Table 3 consistently show that our proposed method LASSO
outperforms the ERM baseline with large margins on both backbones ResNet18 and ResNet50.
Although some other baselines also get comparative or better gains than ours with ResNet18, the
gains shrink with ResNet50 since larger ResNet backbones are known to generalize better (Gulrajani
& Lopez-Paz, 2021). Additionally, the threshold variant LASSO-τ aims to select and retain the top
most relevant and label-informative features (the most appropriate sub-space), while the ensemble
variant LASSO-En aggregates the predictions of possible sub-space hypotheses. Therefore, the
slight superior of the threshold variant to the ensemble one is possibly due to the fact that the former
with an appropriate τ can select more compact and label-informative sets of features.
7
Under review as a conference paper at ICLR 2022
Table 1: Classification Accuracy on VLCS.
Method	Backbone	PASCAL VOC	LabelMe	Caltech	Sun	Average
ERM (Vapnik, 1999)	AlexNet	70.58±0.00	59.72±0.00	96.25±0.00	64.51±0.00	72.56
MASF (Dou et al., 2019)	AlexNet	69.14±0.00	64.90±0.00	94.78±0.00	67.64±0.00	74.11
JiGen (Carlucci et al., 2019)	AlexNet	70.62±0.00	60.90±0.00	96.93±0.00	64.30±0.00	73.19
SFA-A (Li et al., 2021)	AlexNet	70.40±0.00	62.00±0.00	97.20±0.00	66.20±0.00	74.00
LASSO-τ	AlexNet	71.55±0.21	62.42±0.42	97.33±0.22	65.15±0.22	74.15
LASSO-En	AlexNet	71.37±0.08	62.26±0.19	97.17±0.31	65.25±0.26	74.01
Table 2: Classification Accuracy on Office-Home
Method	Backbone	Art	Clipart	Product	RealWorld	Average
ERM (Vapnik, 1999)	ResNet18	52.15±0.00	45.86±0.00	70.86±0.00	73.15±0.00	60.51
JiGen (Carlucci et al., 2019)	ResNet18	53.04±0.00	47.51±0.00	71.47±0.00	72.79±0.00	61.20
RSC (Huang et al., 2020)	ResNet18	58.42±0.00	47.90±0.00	71.63±0.00	74.54±0.00	63.12
L2A-OT (Zhou et al., 2020a)	ResNet18	60.60±0.00	50.10±0.00	74.80±0.00	77.00±0.00	65.60
LASSO-τ	ResNet18	59.29±0.92	50.77±0.27	73.28±0.16	74.48±0.30	64.46
LASSO-En	ResNet18	58.63±0.27	51.48±0.96	72.99±0.15	74.46±0.30	64.39
ERM (Vapnik, 1999)	ResNet50	61.30±0.70	52.40±0.30	75.80±0.10	76.60±0.30	66.50
MLDG (Li et al., 2018a)	ResNet50	61.50±0.90	53.20±0.60	75.00±1.20	77.50±0.40	66.80
RSC (Huang et al., 2020)	ResNet50	60.70±1.40	51.40±0.30	74.80±1.10	75.10±1.3 0	65.50
GroupDRO (Sagawa et al., 2019)	ResNet50	60.40±0.70	52.70±1.00	75.00±0.70	76.00±0.70	66.00
MTL(Blanchard et al., 2021)	ResNet50	61.50±0.70	52.40±0.60	74.90±0.40	76.80±0.40	66.40
SagNet (Nam et al., 2021)	ResNet50	63.40±0.20	54.80±0.40	75.80±0.40	78.30±0.30	68.10
LASSO-τ	ResNet50	66.56±0.04	58.21±0.19	78.69±0.01	79.27±0.42	70.70
LASSO-En	ResNet50	67.07±0.38	58.13±0.31	78.27±0.02	79.19±0.41	7066
Table 3: Classification Accuracy on PACS.
Method	Backbone	Photo	Art-painting	Cartoon	Sketch	Average
ERM (Vapnik, 1999)	AlexNet	88.47±0.63	67.21±0.72	66.12±0.51	55.32±0.44	69.28
MLDG (Li et al., 2018a)	AlexNet	88.00±0.00	66.23±0.00	66.88±0.00	58.96±0.00	70.01
MASF (Dou et al., 2019)	AlexNet	90.68±0.12	70.35±0.33	72.46±0.19	67.33±0.12	75.21
JiGen (Carlucci et al., 2019)	AlexNet	89.00±0.00	67.63±0.00	71.71±0.00	65.18±0.00	73.38
MetaReg (Balaji et al., 2018)	AlexNet	91.07±0.41	69.82±0.76	70.35 ±0.63	59.26±0.31	72.62
DMG (Chattopadhyay et al., 2020)	AlexNet	87.31±0.00	64.65 ±0.00	69.88±0.00	71.42±0.00	73.32
LASSO-τ	AlexNet	89.45±0.03	69.34±0.64	70.91±0.25	69.01±0.54	7468
LASSO-En	AlexNet	89.12±0.06	69.19±0.48	70.49±0.21	68.92±0.67	74.43
ERM (Vapnik, 1999)	ResNet18	95.84±0.27	77.86±1.29	76.91±0.64	75.43±0.37	81.51
MLDG (Li et al., 2018a)	ResNet18	94.30±0.00	79.50±0.00	77.30±0.00	71.50±0.00	80.70
MASF (Dou et al., 2019)	ResNet18	94.99±0.09	80.29±0.18	77.17±0.08	71.69±0.22	81.04
JiGen (Carlucci et al., 2019)	ResNet18	96.03±0.00	79.42±0.00	75.25±0.00	71.35±0.00	79.14
MetaReg (Balaji et al., 2018)	ResNet18	95.50±0.24	83.70±0.19	77.20±0.31	70.30±0.28	81.70
DMG (Chattopadhyay et al., 2020)	ResNet18	93.35±0.00	76.90±0.00	80.38±0.00	75.21±0.00	81.46
L2A-OT (Zhou et al., 2020a)	ResNet18	96.20±0.00	83.30±0.00	78.20±0.00	73.60±0.00	82.80
RSC (Huang et al., 2020)	ResNet18	93.52±0.00	78.40±0.00	78.80±0.00	79.78±0.00	82.63
SFA-A (Li et al., 2021)	ResNet18	93.90±0.00	81.20±0.00	77.80±0.00	73.70±0.00	81.70
MatchDG (Mahajan et al., 2021)	ResNet18	95.93±0.00	79.77±0.00	80.03±0.00	77.11±0.00	83.21
LASSO-τ	ResNet18	94.76±0.03	82.17±0.60	78.37±0.55	77.34±0.37	83.16
LASSO-En	ResNet18	94.76±0.03	82.02±0.75	77.65±0.76	77.07±0.42	82.87
ERM (Vapnik, 1999)	ResNet50	97.2±0.30	84.7±0.40	80.8±0.60	79.3±1.00	85.50
MLDG (Li et al., 2018a)	ResNet50	97.4±0.30	85.5±1.40	80.1±1.70	76.6±1.10	84.90
MetaReg (Balaji et al., 2018)	ResNet50	97.6±0.31	87.2±0.13	79.2±0.27	70.3±0.18	83.60
DMG (Chattopadhyay et al., 2020)	ResNet50	95.0±0.00	82.9±0.00	80.5±0.00	72.3±0.00	82.67
RSC (Huang et al., 2020)	ResNet50	97.6±0.30	85.4±0.80	79.7±1.80	78.2±1.20	85.20
GroupDRO (Sagawa et al., 2019)	ResNet50	96.7±0.30	83.5±0.90	79.1±0.60	78.3±2.00	84.40
MTL (Blanchard et al., 2021)	ResNet50	96.4±0.80	87.5±0.80	77.1±0.50	77.3±1.80	84.60
SagNet (Nam et al., 2021)	ResNet50	97.1±0.10	87.4±1.00	80.7±0.60	80.0±0.40	86.30
MatchDG (Mahajan et al., 2021)	ResNet50	97.94±0.00	85.61±0.0	82.12±0.00	78.76±0.00	86.11
LASSO-τ	ResNet50	96.62±0.56	87.81±0.69	82.99±0.71	82.33±0.20	87.43
LASSO-En	ResNet50	96.93±0.65	87.23±0.57	82.55 ±1.09	81.95 ±0.37	87.15
4.2 Ablation Study
Sub-space representation: To better understand the benefits of learning latent sub-spaces, we vi-
sualize the distribution of the learned features to analyze the latent space generated by ERM and
LASSO-τ using t-SNE (van der Maaten & Hinton, 2008) in Figure 2. Furthermore, we decide to
choose ”sketch” which contains only colorless images as the target domain. This domain can be
considered as the most distant domain from the others and hence results in the largest source-target
divergence. Figure 2a shows feature distributions of different domains on latent space, in which
samples are colored accordingly to domain label, with source domains are in red, green and back,
while target domain is in blue. As it can be clearly seen, ERM training results in representations
with high cross-domain separation for both target and source domains. On the other hands, the do-
main representations on a sub-latent space of LASSO is more homogeneously mixed, since samples
from different domains are hardly distinguished. Figure 2b further shows that features from differ-
8
Under review as a conference paper at ICLR 2022
(a) Latent visualization of all four domains, differ-
ent colors represent different domains.
Figure 2: The t-SNE feature visualization of two methods ERM and LASSO-τ for experiment on
PACS dataset with ”Sketch” (represented by blue in 2a) as target domain using ResNet50.
(b) Latent visualization of target domain only, dif-
ferent colors represent different classes.
ent classes are more compact and clearly separated on LASSO’s sub-latent space than on full single
latent space. These results indicate that feature distributions are better aligned on latent sub-space
with our proposed LASSO approach.
Additionally, we further verify the data shift reduction on sub-spaces by measuring the divergence
between the source and target distributions of ERM and LASSO-τ via Jensen-Shannon (JS) diver-
gence (RJS). We employ a domain discriminator hd which distinguishes the source domain:
L(hd) = Ex〜PS [log(hd(G(X)))]+ Ex〜PT [log(1 - hd(G(x)))],
(9)
where G is the feature extractor, which is g for ERM and gΓ for LASSO-τ . It is well-known that
if We search hd in a family with infinite capacity then RJS (G#PS ||G#PT)
= maxhd
L(hd)+2log2
2
(Goodfellow et al., 2014). The empirical results REJRSM = 0.247 and RLJASSSO-τ = 0.164
show that the latent distribution divergence between sources and target domains is decreased
on sub-spaces. This observation once again provides supporting evidences for our approach.
Effect of τ : we present the performance on vali-
dation set (Val) of source domains and target do-
main with different thresholds τ to analyze its ef-
fect along with the average number of selected fea-
tures. The Table 4 demonstrates that by choosing
reasonable τ , the model performance can be im-
proved since latent is mapped to appropriate sub-
space (i.e., sub-space being compact to reduce the
data shift while preserving sufficient informative
features for label prediction). Alternatively, in the
case of large τ , the performance can be dropped
since very few features remain. It also can be seen
that with τ = 0.6 and τ = 0.7, the model utilizes
Table 4: Average number of selected features
(SF) based on τ and its performance (a trial on
PACS dataset with ”sketch” as target domain
using ResNet50).
τ	val-SF	Val	target-SF	Target
0→0.5	2048	97.24	2048	82.86
0.6	64.8	97.63	59.9	83.38
0.7	20.9	96.75	15.4	83.63
0.8	7.8	84.23	3.4	79.30
0.9	2.7	61.52	1.4	45.58
only a small number of features (i.e., 59.9 and 14.4 on target domain respectively) but nonethe-
less still yields good results. This indicates that sub-indicator Γ is able to identify essential label-
informative features.
5 Conclusion
In this paper, we first theoretically analyze the upper bound of target loss on latent space in domain
generalization settings and the main factors that affect the performance. Then we point out that
learning on appropriate latent sup-space can derive better generalization performance for domain
generalization task. Motivated by this, we have proposed a novel LASSO framework which can: (1)
learn diverse latent sub-spaces and corresponding individual hypotheses and (2) project source and
target examples onto appropriate sub-spaces preserving crucial label-informative features for label
prediction, while reducing the latent data shift due to the sub-spaces compactness compared to the
entire latent space. Experiments on benchmark datasets verify that LASSO achieves competitive
and even state-of-the-art performances.
9
Under review as a conference paper at ICLR 2022
References
Yogesh Balaji, Swami Sankaranarayanan, and Rama Chellappa. Metareg: Towards domain gener-
alization using meta-regularization. In Advances in Neural Information Processing Systems, pp.
998-1008,2018.
Shai Ben-David, John Blitzer, Koby Crammer, Alex Kulesza, Fernando Pereira, and Jennifer Wort-
man Vaughan. A theory of learning from different domains. Machine learning, 79(1-2):151-175,
2010.
Gilles Blanchard, Gyemin Lee, and Clayton Scott. Generalizing from several related classification
tasks to a new unlabeled sample. In Advances in neural information processing systems, pp.
2178-2186, 2011.
Gilles Blanchard, Anlket Anand Deshmukh, UrUn Dogan, Gyemm Lee, and Clayton Scott. Domain
generalization by marginal transfer learning. J. Mach. Learn. Res., 22:2-1, 2021.
Fabio M Carlucci, Antonio D’Innocente, Silvia Bucci, Barbara Caputo, and Tatiana Tommasi. Do-
main generalization by solving jigsaw puzzles. In Proceedings of the IEEE Conference on Com-
puter Vision and Pattern Recognition, pp. 2229-2238, 2019.
Prithvijit Chattopadhyay, Yogesh Balaji, and Judy Hoffman. Learning to balance specificity and
invariance for in and out of domain generalization. In European Conference on Computer Vision,
pp. 301-318. Springer, 2020.
Qi Dou, Daniel Coelho de Castro, Konstantinos Kamnitsas, and Ben Glocker. Domain general-
ization via model-agnostic learning of semantic features. In Advances in Neural Information
Processing Systems, pp. 6450-6461, 2019.
Yaroslav Ganin, Evgeniya Ustinova, Hana Ajakan, Pascal Germain, Hugo Larochelle, Francois
Laviolette, Mario Marchand, and Victor Lempitsky. Domain-adversarial training of neural net-
works. The Journal of Machine Learning Research, 17(1):2096-2030, 2016.
Muhammad Ghifary, W Bastiaan Kleijn, Mengjie Zhang, and David Balduzzi. Domain generaliza-
tion for object recognition with multi-task autoencoders. In Proceedings of the IEEE international
conference on computer vision, pp. 2551-2559, 2015.
Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair,
Aaron Courville, and Yoshua Bengio. Generative adversarial nets. Advances in neural information
processing systems, 27, 2014.
Ishaan Gulrajani and David Lopez-Paz. In search of lost domain generalization. In International
Conference on Learning Representations, 2021.
Zeyi Huang, Haohan Wang, Eric P. Xing, and Dong Huang. Self-challenging improves cross-domain
generalization. In ECCV, 2020.
Maximilian Ilse, Jakub M Tomczak, Christos Louizos, and Max Welling. Diva: Domain invariant
variational autoencoders. In Medical Imaging with Deep Learning, pp. 322-348. PMLR, 2020.
F. D. Johansson, D. Sontag, and R. Ranganath. Support and invertibility in domain-invariant repre-
sentations. In Proceedings of Machine Learning Research, volume 89, pp. 527-536, 2019.
Da Li, Yongxin Yang, Yi-Zhe Song, and Timothy M Hospedales. Deeper, broader and artier domain
generalization. In Proceedings of the IEEE international conference on computer vision, pp.
5542-5550, 2017.
Da Li, Yongxin Yang, Yi-Zhe Song, and Timothy Hospedales. Learning to generalize: Meta-learning
for domain generalization. In Proceedings of the AAAI Conference on Artificial Intelligence,
volume 32, 2018a.
Haoliang Li, Sinno Jialin Pan, Shiqi Wang, and Alex C Kot. Domain generalization with adver-
sarial feature learning. In Proceedings of the IEEE Conference on Computer Vision and Pattern
Recognition, pp. 5400-5409, 2018b.
10
Under review as a conference paper at ICLR 2022
Pan Li, Da Li, Wei Li, Shaogang Gong, Yanwei Fu, and Timothy M Hospedales. A simple feature
augmentation for domain generalization. In Proceedings of the IEEE/CVF International Confer-
ence on Computer Vision, pp. 8886-8895, 2021.
Divyat Mahajan, Shruti Tople, and Amit Sharma. Domain generalization using causal matching. In
International Conference on Machine Learning, pp. 7313-7324. PMLR, 2021.
Yishay Mansour, Mehryar Mohri, and Afshin Rostamizadeh. Domain adaptation: Learning bounds
and algorithms. arXiv preprint arXiv:0902.3430, 2009.
Saeid Motiian, Marco Piccirilli, Donald A Adjeroh, and Gianfranco Doretto. Unified deep super-
vised domain adaptation and generalization. In Proceedings of the IEEE International Conference
on Computer Vision, pp. 5715-5725, 2017.
Krikamol Muandet, David Balduzzi, and Bernhard Scholkopf. Domain generalization via invariant
feature representation. In International Conference on Machine Learning, pp. 10-18, 2013.
Hyeonseob Nam, HyunJae Lee, Jongchan Park, Wonjun Yoon, and Donggeun Yoo. Reducing do-
main gap by reducing style bias. In Proceedings of the IEEE/CVF Conference on Computer Vision
and Pattern Recognition, pp. 8690-8699, 2021.
Vihari Piratla, Praneeth Netrapalli, and Sunita Sarawagi. Efficient domain generalization via
common-specific low-rank decomposition. In International Conference on Machine Learning,
pp. 7728-7738. PMLR, 2020.
Shiori Sagawa, Pang Wei Koh, Tatsunori B Hashimoto, and Percy Liang. Distributionally robust
neural networks for group shifts: On the importance of regularization for worst-case generaliza-
tion. arXiv preprint arXiv:1911.08731, 2019.
Seonguk Seo, Yumin Suh, Dongwan Kim, Geeho Kim, Jongwoo Han, and Bohyung Han. Learning
to optimize domain specific normalization for domain generalization. In Computer Vision-ECCV
2020: 16th European Conference, Glasgow, UK, August 23-28, 2020, Proceedings, PartXXII16,
pp. 68-83. Springer, 2020.
Shiv Shankar, Vihari Piratla, Soumen Chakrabarti, Siddhartha Chaudhuri, Preethi Jyothi, and
Sunita Sarawagi. Generalizing across domains via cross-gradient training. arXiv preprint
arXiv:1804.10745, 2018.
Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov.
Dropout: a simple way to prevent neural networks from overfitting. The journal of machine
learning research, 15(1):1929-1958, 2014.
Antonio Torralba and Alexei A Efros. Unbiased look at dataset bias. In CVPR 2011, pp. 1521-1528.
IEEE, 2011.
Laurens van der Maaten and Geoffrey Hinton. Visualizing data using t-sne, 2008.
Vladimir N Vapnik. An overview of statistical learning theory. IEEE transactions on neural net-
works, 10(5):988-999, 1999.
Hemanth Venkateswara, Jose Eusebio, Shayok Chakraborty, and Sethuraman Panchanathan. Deep
hashing network for unsupervised domain adaptation. In Proceedings of the IEEE conference on
computer vision and pattern recognition, pp. 5018-5027, 2017.
Haohan Wang, Zexue He, Zachary C Lipton, and Eric P Xing. Learning robust representations by
projecting superficial statistics out. arXiv preprint arXiv:1903.06256, 2019.
Qizhe Xie, Zihang Dai, Yulun Du, Eduard Hovy, and Graham Neubig. Controllable invariance
through adversarial feature learning. In Advances in Neural Information Processing Systems, pp.
585-596, 2017.
Han Zhao, Remi Tachet Des Combes, Kun Zhang, and Geoffrey Gordon. On learning invariant
representations for domain adaptation. In International Conference on Machine Learning, pp.
7523-7532. PMLR, 2019.
11
Under review as a conference paper at ICLR 2022
Shanshan Zhao, Mingming Gong, Tongliang Liu, Huan Fu, and Dacheng Tao. Domain generaliza-
tion via entropy regularization. Advances in Neural Information Processing Systems, 33, 2020.
Kaiyang Zhou, Yongxin Yang, Timothy Hospedales, and Tao Xiang. Learning to generate novel
domains for domain generalization. In European Conference on Computer Vision, pp. 561-578.
Springer, 2020a.
Kaiyang Zhou, Yongxin Yang, Timothy M Hospedales, and Tao Xiang. Deep domain-adversarial
image generation for domain generalisation. In AAAI, pp. 13025-13032, 2020b.
Kaiyang Zhou, Yongxin Yang, Yu Qiao, and Tao Xiang. Domain generalization with mixstyle. In
International Conference on Learning Representations, 2021.
A	Theoretical development
In this Section, we present all proofs relevant to theory developed in our paper. We begin with a
crucial proposition for our theory development.
Proposition A.1. Let f : X 7→ Y∆ where f = h ◦ g with g : X 7→ Z and h : Z 7→ Y∆ .
i)	We have
C
XZ
y=1
`(h (z) ,y)pg (y | z)pg (z) dz
C
XZ
y=1
`(f (x) ,y)p(y | x)p(x)dx,
where P	(y	| Z) =	Rg-R:I：：；(X)L for any Z ∈ g	(X)	and	Pg	(Z)	=	Rg-Yz)P (x) dx is the
density of the push-forward distribution Pg = g#P.
ii)	We have
CC
X	P1,g	(y | Z) -	P2,g	(y	|	Z)Pg	(Z) dZ ≤ X	P1 (y	|	x)	-P2 (y | x) P (x) dx,
where Pg (Z) = g-1(z) P (x) dx is the density of the push-forward distribution Pg = g#P and
p1,g (y | Z) = Rg-Rg-1；**andp2,g (y | Z) = Rg-R：—：；XSx,dxfor anyZ ∈ g(X).
Proof. i) We first prove for any y ∈ [C] that
/ ' (h (z) ,y) Pg (y
| Z) Pg (Z) dZ
/'(f (χ),y)p(y I x) P (x) dx.
12
Under review as a conference paper at ICLR 2022
In fact, we have
Z ' (h (Z) ,y) Pg (y I Z) Pg (Z) dz = Z ' (h (z) ,y) JgTR)P (y | x)P^x) dx Z	P (x) dxdz
Z	g-1(z) P (x) dx	g-1 (z)
' (h (Z) ,y)
p (y | x) p (x) dxdz
=J 1 1z=g(x)' (h (Z) ,y) P (y | x) P (x) dzdx
=' (h (g (χ)) ,y)p (y | χ)P (χ) dχ
X
=I' (f (χ) ,y) p (y I x)P (x) dx.
where note that 1condition is the indicator function which returns 1 if condition is true and 0 other-
wise, the equality in (=1) due to Fubini theorem.
Finally, we reach
CC
E	' (h(z) ,y) Pg (y	|	Z) Pg	(Z) dz = E '	(f	(x)	,y) P	(y	|	x) P	(x) dx.
y=1	y=1
ii) We then prove that
(y I Z) - P2,g (y I Z) Pg (Z) dZ ≤
(y I x) -P2 (y I x) P (x) dx, ∀y ∈ [C] .
We derive as
P1,g (y I Z) - P2,g (y I Z)Pg (Z) dZ
Z
Z
Z
g-1(z) P1 (y I x)P(x) dx	g-1(z) P2 (y I x)P(x) dx
-------:----------------------------------:----------------------
g
Z
g-1(z)
-	1(z) P (x) dx
Jg-1 (z) P (X) dx
P (x) dxdZ
g-1(z)
P1 (y I x) P (x) dx -
g
-	1(z)
P2 (y I x) P (x) dx dZ
P1 (y I x)
g-1(z)
-	P2 (y I x) P (x) dx dZ
P1 (y I x) - P2 (y I x) P (x) dxdZ
(z)
x∈g-1(z) P1 (y I x) -P2 (y I x) P (x) dxdZ
z=g(x) P1 (y I x) - P2 (y I x) P (x) dZdx
P1 (y I x) - P2 (y I x) P (x) dx.
X
where in the derivation of (=1), we use Fubini theorem again.
□
13
Under review as a conference paper at ICLR 2022
Theorem 4. (Theorem 1 in the main paper) If the loss function ` is upper-bounded by a positive
constant L, the target general loss is upper-bounded by:
a-1
α — 1 ι	α
L(f, DT) ≤ exp {Rα (g#PT,g#PS)} ɪ L1 L (f, DS) + L maxEPS [∣∣∆pk (y | X)kj	,
kk
(10)
where g#PS and g# PT are Pushed-forward distributions induced by applying g on PS and PT, Ra
is the α-divergence (α > 1), and ∆pk (y | X) := pkS (y = c | x) - pT (y = c | x)cC=1 represents
the label shift between the labeling assignment mechanisms of an individual source domain and
target domain on the input space.
Proof. Let D∏,g be thejoint distribution including the pairs (y, Z) where k 〜Cat (π), Z 〜pS,g (z),
and y 〜pS,g (y | z). The density p∏,g (y, Z) of this distribution is as follows:
KK
pπS,g (y, Z) = X πkpkS,g (Z) pkS,g (y | Z) = X πkpkS,g (y, Z) .
k=1	k=1
Let DT,g be thejoint distribution including the pairs (y, Z) where Z 〜pT,g (Z) and y 〜pT,g (y | z).
The density pT,g (y, Z) of this distribution is as follows:
pT,g(y, Z) = pT,g (Z)pT,g (y | Z) .
Eventually, We denote Dh,g as thejoint distribution including the pairs (y, Z) where Z 〜p∏,g (z):
PK=1 ∏kpS,g (z) and y 〜pT,g (y | z). The density ph,g (y, Z) of this distribution is as follows:
K
ph,g (y, Z) = pπS,g (Z)pT,g (y | Z) = X πkpkS,g (Z)pT,g (y | Z) .
k=1
With the above equipment, we have
≤L
' (h (Z) ,y) ph,g (z,y) dzdy
(h, DS,g) + / ' (h (z) ,y) [ph,g
(h, DS,g) + / ' (h (z) ,y) ∣Ph,g
(Z, y) - pπS,g (Z, y) dZdy
(Z, y) - pπS,g (Z, y) |dZdy
≤ L (h, DS,g) + L / ∣ph,g (z, y) - p∏,g (z, y) ∣dzdy
K
=L (h, DS,g) + Ll EnkPSg (z) ∣PT,g (y | z) - PSg (y | z) ∣dzdy
k=1
CK
=L (h,DS，g) + LXX∏k 八PT,g (y | Z) -pS,g (y | Z) ∣pS,g (Z) dz.
y=1 k=1
14
Under review as a conference paper at ICLR 2022
We further note that
L (h, DS,g) = XZ ' (h(z),y) PSg (y,z) dz
CK
=XXπk	` (h(z), y) pkS,g (y | z)pk (z) dz
y=1 k=1
KC
=Xπk X	` (h(z), y) pkS,g (y | z) pk (z) dz
k=1	y=1
KC
=)XXπk	'(f (x),y)PS (y | X)Pk(X) dx = L (f,DS),
k=1 y=1
where we have (=1) by using Proposition A.1 (i).
CK
XXπk
|pT,g (y | z) - pkS,g (y | z) |pkS,g (z) dz
y=1 k=1
KC
=XπkX	|pT,g	(y	|	z)	- pkS,g	(y	| z)	|pkS,g	(z) dz
k=1	y=1
(2) K C
≤	πk	|pT (y | x) -pkS (y | x) |pkS (x) dx
k=1	y=1
KC
=	πk	X|pT(y | x) -pkS (y | x) |pkS (x) dx
k=1	y=1
K
=	πkEPkS [k∆pk (y | x)k1] ≤ max EPkS [k∆pk (y | x)k1] ,
k=1
where we have (=2) by using Proposition A.1 (ii).
Therefore, we obtain
L (h, Dh,g) ≤ L (f, DS) + max EPS [k∆pk (y | x)k∕ .
Finally, we manipulate L h, DT,g as
L (h, DT,g) = I	' (h (z) ,y) PT,g (z,y) dzdy
Y×Z
Z	pT,g (Z,y- Ph,g (z,y)α-1 ' (h (z) ,y) dzdy.
YyzZ ph,g (z, y) α
The Holder inequality gives us
1
L (h, DT,g) ≤ [ P jz,,-i dzdy	[(	Ph,g (z,y ' (h (Z) ,y)α-1 dzdy
Y×Z Ph,g (z, y)	Y×Z
α-1
α
(11)
Referring to the definition of the Renyi divergence and note that' (h (z) ,y) ≤ L, We obtain
α-1	l
L (h, DT，g) ≤ [exp {Rα (DT，g∣∣Dh,g) }L (h, Dh，g)] ɪ Lι.	(12)
15
Under review as a conference paper at ICLR 2022
We further derive
Ra (DT,g∣∣Dh,g)
α⅛og
pT,g (z,y)
ph,g (z,y)
α-1
pT,g (z, y)dzdy
log U
log (Z
log (Z
log (Z
pT,g (z)pT,g (y | z)
PSg(Z) pT,g (y | Z)
α-1
pT,g (Z, y) dZdy
PT,g (z)
PS,g (Z)
PT,g (z)
PS,g (Z)
PT,g (z)
PS,g (Z)
α-1
PT,g (Z, y) dZdy
α-1 C
X PT,g (Z, y) dZ
y=1
α-1
PT,g (Z) dZ
=Ra (g#PTkg#PS).
(13)
Therefore, combining (12) and (13), we reach the following inequality:
a - 1	i
L (h, DT,g) ≤ [exp {Ra (g# PTkg#PS)} L (h, Dh,g)] ɪ L a.
Subsequently, by noting that
L (h, DT,g) = XJ ' (h(Z),y) PTQ (y | Z ) PT,g (Z ) dZ
C
=) Xy ' (f (χ) ,y) PT (y I χ) PT (χ) dχ = L (f, DT),
we reach
a —1 I
L (f, DT) ≤ [exp {Ra (g#PTkg#PS)} L (h, Dh,g)] ɪ La.
Finally, referring to the inequality in (11), we reach the conclusion.
□
Remark: We would like to highlight that our Theorem 1 despite developing in a general context
of DG is also novel due to two following reasons: (i) it proposes the target loss upper-bound in a
general setting of multi-class classification with a sufficiently general loss, which can be viewed as a
non-trivial generalization of existing works in DA and DG (Mansour et al., 2009; Ben-David et al.,
2010; Zhao et al., 2019) and (ii) our bound is interweaving both input and latent spaces, which is
novel and appropriate for theoretical analyses in deep learning, while the bounds in previous works
only involve input space.
Theorem 5. (Theorem 2 in the main paper) Given a deterministic sub-space indicator Γ, if the loss
function ` is upper-bounded by a positive constant L, the sub-space target general loss is upper-
bounded by:
τ 1
/ , a    	a — 1	a — 1
L (fM, DT) ≤ ∣mm∣ X exp {Ra(gr#Pm,gr#Pm)}b [L (fm, Dm) + L∆Pm] b , (14)
|M| m∈M
where Dm is the joint distribution of (x,y) with X 〜 Pm and y 〜Pm (∙ ∣ x), and ∆Pm
Ex〜PS [∣∣Pm (∙ I x) — Pm (∙ ∣ x)∣∣J ∙ Sub-Spaceindices m.
Proof. Given a sub-space index m ∈ M, by noting that fm (x) = h (gr (x)) with X 〜Pm over
Am , using the same proof for a single space in Theorem 4, we obtain
a	a — 1	a — 1
L (fm, DT) ≤ L1 exp {Ra (gr#Pm,gr#Pm)}b [L (九,Dm) + L∆Pm] ɪ .
16
Under review as a conference paper at ICLR 2022
Finally, taking average over m ∈ M, we reach
L (fM, DT)≤⅛ X
m∈M
a-1
a-1
exp {Rα (gr#Pm,gr#Pm)}b [L fm, Dm) + L∆pm] b .
Theorem 6. (Theorem 3 in the main paper) Let X is a random variable of source sam-
ple (i.e., drawn from PπS) and Y is a random variable of ground-truth labels. Denote N =
Pm0 ∈MPπS(Am0), we then have
I (Γ (g (X)) Θ g (X) ,Y) ≥ - X PS NAm) L (fm, Dm) + const,	(15)
m∈M
where the loss L fm , DSm is defined based on the cross-entropy loss and I denotes the mutual
information.
Proof. Denote T = Γ (g (X)) Θ g (X), we have
I(T，Y )= ∕p(t,y)log Pp⅜(⅛ dtdy
/
Z
Z
Z
p(t,y)log p≡2 dtdy
p(t, y) logp(y | t) dtdy +H(Y)
p(t,y)log h (y∣t) hH⅜dtdy + H(Y)
p(t, y) logh(t,y) dtdy + DKL (p(y | t) kh(y | t)) + H(Y)
≥	P(t, y) log h (t, y) dtdy + const,
where H specifies the entropy, DKLis Kullback-Leibler (KL) divergence, h (y ∣ t) = h (t, y) =
h (Γ (g (x)) Θ g (x) , y) for any h : Z → Y∆, and h (t, y) returns the y-th element of h (t).
We further derive
C
P(t,y)logh(t,y) dtdy =	Ep(t) [P(y = i ∣ t)logh(t,y = i)]
i=1
C
(=) X EpπS (x) [P(y = i ∣ Γ (g (x)) Θ g(x))logh(Γ (g (x)) Θ g(x) ,i)]
i=1
C
=XEPSπ [P(y = i ∣ Γ (g (x)) Θ g(x))logh(Γ (g (x)) Θ g(x) ,i)] .
i=1
Note that We have (=1) because Γ(g (x)) Θg (x) pushes forward X 〜PS(x) to T 〜P (t). Moreover,
according to our definitions: PπS = Pm∈M
Pn (Am)
PSm , we hence obtain
/ p(t, y) log h (t, y) dtdy = X Pn NAm)
m∈M N
XEPS [P(y = i | mΘg(x))logh(mΘg(x) ,i)]
m
i=1
一 X Pn Nm) XEPm I-Pm(y = i ∣x)logfm(x,i)]
m∈M	i=1
-X PSNm)L (fm, Dm).
m∈M
N
C
□
□
17
Under review as a conference paper at ICLR 2022
B Additional experiments
B.1	EFFECT OF THE ATTRIBUTE-BASED INDICATOR Γd
We consider a feature vector z = [z1, ..., zD] ∈ RD0×D, whereas each zd ∈ RD0 known as an
attribute and model sub-space indicator as Γ (z) = [Γ1 (z1) , ..., ΓD (zD)] ∈ [0, 1]D, Γd (z) =
Γd (zd) is computed based solely on the group of D0 features in the attribute zd itself rather than full
latent z. Consequently, attribute-based Γd depends on attributes which are shared across domains,
hence, becomes more independent to domain. Moreover, by Theorem 3, model can learn meaningful
attributes as long as the performance on source domains is guaranteed. In unseen domains, the target
of sub-space indicator Γ is to detect label-informative attributes which is learned in source domains
instead of identifying all label-informative attributes.
More specifically, in training phase, since attributes zd come from different source domains,
corresponding Γd plays as a domain invariant hypothesis which output {0, 1} identifying label-
informative attributes across source domains. In unseen domain, Γd (zd) only predict zd is label-
informative attribute which are learned in source domains or not.
Table 5: Performance LASSO-τ with different Γd settings.
Method	Backbone	Photo	Art-painting	Cartoon	Sketch	Average
LASSO-τ Γd(z)	ResNet50	97.01±0.16	87.30±0.52	81.91±0.46	80.02±0.45	86.56
LASSO-τ Γd(zd)	ResNet50	96.62±0.56	87.81±0.69	82.99±0.71	82.33±0.20	87.43
In order to analyze the benefit of attribute-based Γd, we examine the performance on the setting of
Γd(z) which uses full latent z as input for Γd and Γd(zd) which uses only attribute zd as input for
Γd. As can be seen from Table 5, both Γd(zd) and Γd(z) achieve competitive performances on target
domain: ”Photo”, ”Art-painting” and ”cartoon” domain but Γd(zd) setting outperforms on ”sketch”
domain. Remind that using ”sketch” as target domain leads to the largest source-target divergence.
The results of ”sketch” domain demonstrate that Γd(z) becomes less efficient when source-target
divergence. In contrast, Γd (z) shows its better generalization as not be interfered by other noisy
attributes.
B.2	Mask sampling for model training
In training phase, sampling m 〜Ber (Γ(z)) would help hypotheses explore more useful SUb-
spaces. The motivation is that target domains might have or share a small sub-set of learned features
in source domains e.g. in PACS dataset, many label-informative features can be learned in rich in-
formation domain such as ”Photo” domain while ”Sketch” domain only has edge/shape information
for classification. Therefore, in training phase, we would like to train hypotheses on different sub-set
of learned features (i.e., sub-spaces of sub-space) which would strengthen sub-space hypotheses on
arbitrary target domains..
Table 6: Classification Accuracy on PACS with ResNet50
Training	Backbone	Photo	Art-painting	Cartoon	Sketch	Average
Deterministic-τ	ResNet50	95.99±0.42	86.28±1.70	80.09±1.54	79.01±4.44	85.34
Deterministic-En	ResNet50	93.05±0.03	86.18±1.08	80.08±1.62	77.73±2.65	84.26
LASSO-τ	ResNet50	96.62±0.56	87.81±0.69	82.99±0.71	82.33±0.20	87.43
LASSO-En	ResNet50	96.93±0.65	87.23±0.57	82.55±1.09	81.95±0.37	87.15
To demonstrate the benefit of sampling, we conduct additional experiments using deterministic man-
ner which uses threshold τ to determine mask m in model training. The results in Table 6 indicate
sampling significantly enhance performance.
B.3	Discussion and comparison with Random Dropout
We would like to report additional experimental results in comparison with ”Random Dropout”
Baseline (Srivastava et al., 2014) for different dropping rate as follows:
18
Under review as a conference paper at ICLR 2022
Table 7: Classification Accuracy on PACS with ResNet18
Feature Dropping Rate	Backbone	Photo	Art-painting	Cartoon	Sketch	Average
0%	ResNet18	96.17±0.27	78.42±1.29	76.73±0.64	75.40±0.37	81.68
10%	ResNet18	96.29±0.66	81.01±0.33	76.98±0.67	75.60±0.81	82.47
30%	ResNet18	96.41±0.60	79.73±0.69	77.68±0.28	75.22±1.07	82.26
50%	ResNet18	96.59±0.14	81.20±0.02	77.92±0.24	75.98±0.17	82.92
70%	ResNet18	96.29±0.14	79.93±0.72	78.26±0.67	74.89±0.09	82.34
LASSO-τ	ResNet18	94.76±0.03	82.17±0.60	78.37±0.55	77.34±0.37	83.16
LASSO-En	ResNet18	94.76±0.03	82.02±0.75	77.65±0.76	77.07±0.42	82.87
Table 8: Classification Accuracy on PACS with ResNet50
Feature Dropping Rate	Backbone	Photo	Art-painting	Cartoon	Sketch	Average
0%	ResNet50	97.20±0.30	84.70±0.40	80.80±0.60	79.30±1.00	85.50
10%	ResNet50	97.96±0.26	86.96±0.37	78.92±1.44	79.96±0.64	85.95
30%	ResNet50	98.08±0.32	87.26±0.63	80.11±1.10	81.31±0.80	86.69
50%	ResNet50	97.60±0.04	87.16±0.13	80.46±0.24	80.80±0.77	86.51
70%	ResNet50	97.08±0.47	86.33±0.73	81.78±0.98	80.57±1.01	86.44
LASSO-τ	ResNet50	96.62±0.56	87.81±0.69	82.99±0.71	82.33±0.20	87.43
LASSO-En	ResNet50	96.93±0.65	87.23±0.57	82.55±1.09	81.95±0.37	87.15
The results in Table 7 and Table 8 indicate that ERM with an appropriate dropping rate (i.e., 50%
for ResNet18 and 30% for ResNet50) is able to achieve comparative performance to most current
baselines.
In fact, ”Dropout” has a quite similar effect to the representation as LASSO, which increases the
diversity of latent representations. However, LASSO goes one step further compared to ”Dropout”.
Specifically, representation is used as it is during inference time, without making the use of infor-
mation in the target data, e.g., using the full network for inference, or average over a set of randomly
”Dropout” networks (MC dropout). On the contrary, the sub-space indicator in LASSO additionally
takes into account the information from the target domain to select the suitable masked network
which is likely to generalize better to this particular target domain. This is the motivation behind the
selection of appropriate sub-space.
C Experimental Settings
C.1 Dataset Details
To evaluate the effectiveness of the proposed method, we utilize three datasets: PACS (Li et al.,
2017), VLCS (Torralba & Efros, 2011), and Office-Home (Venkateswara et al., 2017), which are the
common DG benchmarks with multi-source domains.
•	PACS (Li et al., 2017): 9991 images of seven classes in total, over four do-
mains:Art_painting (A), Cartoon (C), Sketches (S), and Photo (P).
•	VLCS (Torralba & Efros, 2011): five classes over four domains with a total of 10729
samples. The domains are defined by four image origins, i.e., images were taken from the
PASCAL VOC 2007 (V), LabelMe (L), Caltech (C) and Sun (S) datasets.
•	Office-Home (Venkateswara et al., 2017): 65 categories of 15500 daily objects from 4
domains: Art, Clipart, Product (vendor website with white-background) and Real-World
(real-object collected from regular cameras).
C.2 Evaluation protocol
For PACS and VLCS, we follow the setting of (Li et al., 2017) as these two datasets provide specific
“train” and “validation” splits for each domain to ensure a fair comparison. We use the validation
subsets from all training domains to create an overall validation set. On the target domain, we
evaluate on the entire dataset. Office-Home also follows the leave-one-domain-out protocol in (Li
et al., 2017). In addition, the generalization of our method is based on the accuracy of the sub-
space indicator Γ. Therefore, we choose the model when the sub-space indicator Γ converged. In
19
Under review as a conference paper at ICLR 2022
particular, we train all models for the same fixed number of steps and consider only the final check-
point. Finally, we report the average performance with error bars over 5 runs for all experimental
settings and compare two variants of our proposed method LASSO-τ (the threshold variant) and
LASSO-En (the ensemble variant) with the recent domain generalization baselines.
C.3 Baselines
We compare with the following recent domain generalization baselines (with reproducible public
codes):
•	ERM (Vapnik, 1999):minimizes the sum of errors across domains and examples. For our
experiments, we employ the implementation from (Gulrajani & Lopez-Paz, 2021), a strong
baseline that can achieve competitive accuracies on DG benchmarks.
•	MLDG (Li et al., 2018a): Learning to generalize: Meta-learning for domain generalization.
•	MetaReg (Balaji et al., 2018): Metareg: Towards domain generalization using meta-
regularization.
•	MASF (Dou et al., 2019): Domain generalization via model-agnostic learning of semantic
features.
•	JiGen (Carlucci et al., 2019): Domain Generalization by Solving Jigsaw Puzzles.
•	DMG (Chattopadhyay et al., 2020): Learning to balance specificity and invariance for in
and out of domain generalization.
•	L2A-OT (Zhou et al., 2020a): Learning to Generate Novel Domains for Domain General-
ization.
•	RSC (Huang et al., 2020): Self-challenging improves cross-domain generalization.
•	GroupDRO (Sagawa et al., 2019): Distributionally robust neural networks for group shifts:
On the importance of regularization for worst-case generalization.
•	SFA-A (Li et al., 2021): A Simple Feature Augmentation for Domain Generalization.
•	MTL (Blanchard et al., 2021): Domain Generalization by Marginal Transfer Learning.
•	SagNets (Nam et al., 2021): Reducing Domain Gap by Reducing Style Bias.
•	MatchDG (Mahajan et al., 2021): Domain generalization using causal matching.
C.4 Implementation details
We detail our algorithm’s implementation sub-space Indicator Γ: given latent vector z = g(x) ∈
RD×D0 (i.e. z = [z1, .., zD]|zd ∈ RD0), we model Γ(.) as a composition of independent indicator
on each attribute: Γ(∙) = {Γd(∙)}d=ι…D where each Γd is neural network. In current work, We
model Γd as a Full-Connected (FC) layer [D0 × 1] prior to a sigmoid function σ and Γd takes zd as
input. (Code for experiments is available in Supplementary Material.)
The following are adapted versions on backbones, in which the last FC layer is used as the classifi-
cation network. The summary of the additional parameter is presented in Table.9.
AlexNet: For AlexNet backbone, the extracted feature dimensions are z ∈ R4096 . In current imple-
mentation, we partition z into 256 features zd ∈ R16 as input for Γd.
ResNet18 & ResNet50: For ResNet18 & ResNet50 backbones, instead of partitioning the feature
tensor, we utilize direcly feature tensor before avepooling layer. In particular, the extracted feature
are z ∈ R512×7×7 for ResNet18 and z ∈ R2048×7×7 for ResNet50. We use zd ∈ R7×7=49 as input
for Γd .
Optimization: We adopt the SDG optimizer for training with: the model learning-rate (lrh,g) and
sub-space indicator Γ learning-rate (lrΓ) for different bacbones and datasets presented in Table 10,
mini-batch size of 64 for 100 epochs. The learning rate (lrh,g and lrΓ) decayed by 0.1 after every
80% epochs. The values of hyper-parameters lrh,g, lrΓ and τ are chosen based on the performance
on source validation set. Additionally, in order to the sub-space indicator Γ quickly adapt to features
induced by h, g, lrΓ is set much larger than lrh,g .
20
Under review as a conference paper at ICLR 2022
Table 9: Additional parameters summary
Backbones	Partitions	Original	Additional
AlexNet	256	61M	4352 (0.007%)
ResNet18	512	11M	25600 (0.230%)
ResNet50	2048	23M	0.1M (0.440%)
Table 10: Hyperparameters.
Dataset	Backbone	lrh,g	lrΓ
VLCS	AlexNet	1.0e-3	1.0e-2
Office-Home	ResNet18	6.5e-4	1.0e-0
	ResNet50	6.5e-4	1.0e-0
PACS	AlexNet	1.0e-3	1.0e-2
	ResNet18	1.0e-2	1.0e-0
	ResNet50	1.0e-2	1.0e-0
The number of sampling times in Eq. 8 is also a hyper-parameter in our LASSO. When trying with
several possible values of this hyper-parameter, we observed that (i) when increasing the number
of sampling times, the performance slightly improves but the training time more significantly in-
creases, and (ii) when setting this hyper-parameter to 1, the performance consistently is satisfactory
for all experimental datasets. Therefore, in all experiments, we sample once per mini-batch for com-
putation efficiency. Additionally, at inference time, we conveniently choose τ = 0.6 for ResNet18
& ResNet50 and τ = 0.5 for AlexNet on all datasets. However, from the results in Table 4, it is
reasonable to choose τ based on the performance of the validation data of source domains although
sometimes the best performance on validation data does not reflect the best performance on target
domain.
Data augmentation: We follow the image augmentation protocol introduced in (Gulrajani &
Lopez-Paz, 2021) which is increasingly standard in state-of-the-art DG work: crops of random size
and aspect ratio, resizing to 224 × 224 pixels, random horizontal flips, random color jitter, grayscal-
ing the image with 10% probability, and normalization using the ImageNet channel statistics.
Finally, we use system of GPU NVIDIA Tesla V100 with dual CPUs Intel Xeon E5-2698 v4 to
conduct our experiments.
21