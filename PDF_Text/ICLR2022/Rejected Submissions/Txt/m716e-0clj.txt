Under review as a conference paper at ICLR 2022
Communicate Then Adapt: An Effective Decen-
tralized Adaptive Method for Deep Training
Anonymous authors
Paper under double-blind review
Ab stract
Decentralized adaptive gradient methods, in which each node averages only with
its neighbors, are critical to save communication and wall-clock training time in
deep learning tasks. While different in concrete recursions, existing decentralized
adaptive methods share the same algorithm structure: each node scales its gradient
with information of the past squared gradients (which is referred to as the adaptive
step) before or while it communicates with neighbors. In this paper, we identify
the limitation of such adapt-then/while-communicate structure: it will make the
developed algorithms highly sensitive to data heterogeneity, and hence deviate
their limiting points from the stationary solution. To overcome this limitation,
we propose an effective decentralized adaptive method with a communicate-then-
adapt structure, in which each node conducts the adaptive step after finishing the
neighborhood communications. The new method is theoretically guaranteed to
converge to the stationary solution in the non-convex scenario. Experimental re-
sults on a variety of CV/NLP tasks show that the proposed algorithm has a clear
superiority to other existing decentralized adaptive methods.
1	Introduction
Decentralized SGD (Lopes & Sayed, 2008; Nedic & Ozdaglar, 2009; Chen & Sayed, 2012; Lian
et al., 2017; Assran et al., 2019) is an emerging training approach for deep learning known for
its much less communication overhead. In contrast to parallel SGD in which a global averaging
across all computing nodes is required per iteration, decentralized SGD does not involve any global
operations. Building upon partial averaging, in which each node only needs to compute the locally
averaged model within its neighborhood, decentralized SGD can save remarkable communications
and training time in large-scale distributed deep learning tasks compared to parallel SGD.
Although simple to use, the vanilla decentralized SGD sometimes suffers from the slow conver-
gence. Inspired by the well-documented success of adaptive methods such as AdaGrad (Duchi
et al., 2011a), Adam (Kingma & Ba, 2014) and AMSGrad (Reddi et al., 2019), several decentralized
adaptive methods (Nazari et al., 2019; Lin et al., 2021) have been proposed to accelerate decentral-
ized SGD training. While these algorithms have achieved remarkable success in several practical
applications, they have also been observed to not converge to the desired solution (i.e., global opti-
mal solution in the convex scenario or stationary solution in the non-convex scenario) in some other
settings. For example, it has been observed in the convex setting (see Sec. 3) that DAdam (Nazari
et al., 2019) and QG-DAdam (Lin et al., 2021) do not converge to the global optimal solution.
This paper studies this situation in detail. We rigorously uncover the reason why DAdam and QG-
DAdam fail to achieve the desired solution, and propose a novel decentralized adaptive method to
resolve the convergence issue. In particular, we make the following key contributions:
•	We find the algorithms DAdam and QG-DAdam, while different in concrete recursions, share a
similar structure: each node scales its gradient with the past squared gradients (which is referred
to as the adaptive step) before or while it communicates with neighbors. We identify the limitation
of such adapt-then/while-communicate structure: it will make the developed algorithms highly
sensitive to data heterogeneity, and hence deviate their limiting points from the desired solution.
•	To overcome these limitations, we propose a novel communicate-then-adapt algorithm structure,
in which each node will conduct the adaptive step after all neighborhood communications. It is
not just trivially switching the order of communication step and adaptive step. The key compo-
nent to guarantee the effectiveness of the communicate-then-adapt structure is the utilization of the
1
Under review as a conference paper at ICLR 2022
decentralized augmented gradient (DAG) rather than the standard stochastic gradient in algorithm
development. The newly proposed algorithm, which is coined as DAG-Adam, can provably Con-
verge to the desired solution. While this paper mainly focuses on the vanilla Adam algorithm, the
core idea behind DAG-Adam can be easilty extended to AMSGrad or other adaptive methods.
•	Experimental results on a variety of computer vision and natural language processing tasks show
that DAG-Adam outperforms various existing state-of-the-art decentralized adaptive baselines un-
der different network and data configurations. Furthermore, the performance of our proposed
algorithm is persistently competitive to the centralized counterpart.
Related work on decentralized optimization and deep training. Decentralized optimization was
extensively studied in the control and signal processing community. The first decentralized algo-
rithms on general optimization problems include decentralized gradient descent (Nedic & Ozdaglar,
2009), diffusion (Lopes & Sayed, 2008; Chen & Sayed, 2012; Sayed, 2014), and dual averaging
(Duchi et al., 2011b). After that, various primal-dual algorithms come out to further speed up the
convergence, and they are based on alternating direction method of multipliers (Adam) (Shi et al.,
2014), explicit bias-correction (Shi et al., 2015; Yuan et al., 2019; Li et al., 2019), gradient tracking
(Xu et al., 2015; Di Lorenzo & Scutari, 2016; Nedic et al., 2017; Qu & Li, 2018; Lu et al., 2019),
and dual acceleration (Scaman et al., 2017; Uribe et al., 2020).
In deep learning tasks, decentralize SGD, which was established in (Lian et al., 2017) to achieve
the same linear speedup as parallel SGD in convergence rate, has attracted a lot of attentions.
Many efforts have been made to extend the algorithm to directed topologies (Assran et al., 2019),
time-varying topologies (Koloskova et al., 2020), asynchronous settings (Lian et al., 2018), and
data-heterogeneous scenarios (Tang et al., 2018; Xin et al., 2020; Lin et al., 2021; Yuan et al.,
2021). With careful consensus control (Kong et al., 2021) or periodic global averaging (Chen et al.,
2021b), decentralize SGD can achieve 1.3 〜2× training time speedup without severe performance
degradation. Techniques such as quantization/compression (Alistarh et al., 2017; Bernstein et al.,
2018; Koloskova et al., 2019a;b; Tang et al., 2019; Liu et al., 2020), periodic updates (Stich, 2019;
Koloskova et al., 2020; Yu et al., 2019), and lazy communication (Chen et al., 2018a; Liu et al.,
2019b) were also integrated into decentralized SGD to further reduce communication overheads.
There are also a few studies on accelerated variants of decentralized SGD, and most of them are on
(static) momentum acceleration. (Assran et al., 2019; Gao & Huang, 2020) propose to run a local
momentum SGD step first before the partial averaging is conducted. Another work (Yu et al., 2019)
imposes an additional partial averaging over momentum to increase stability. Recent works (Lin
et al., 2021; Yuan et al., 2021) developed strategies that can remove the momentum-incurred bias in
decentralized momentum SGD. All these methods are not with adaptive strategies to scale gradients.
Related work on adaptive gradient method. Adaptive gradient methods, with AdaGrad (Duchi
et al., 2011a) and Adam (Kingma & Ba, 2014) as two representatives, have shown strong perfor-
mance in training deep neural networks. With adaptive adjustment in the gradient direction and
automatic tune in the learning rate, adaptive gradient methods can boost the performance of SGD
training significantly. In spite of its remarkable empirical success, Adam suffers from a conver-
gence issue: it may not converge to the desired solution with a fixed mini-batch size (Reddi et al.,
2019; Zaheer et al., 2018). Many algorithms have been proposed to resolve this issue. AMSGrad
(Reddi et al., 2019) preserves the long-term memory of past gradients to improve convergence, and
(Zaheer et al., 2018) suggests the usage of increasing mini-batch sizes in convergence guarantees.
(Chen et al., 2018b) has studied the convergence ofa family of Adam-type algorithms. (Zhou et al.,
2018) provides a better convergence rate for AMSGrad. From the empirical side, RAdam (Liu et al.,
2019a) and AdamW (Loshchilov & Hutter, 2018) can significantly improve Adam performance.
The exploration in decentralized adaptive methods are rather limited. DAdam (Nazari et al., 2019)
is the first consensus-based adaptive methods for distributed optimization to our knowledge. (Lin
et al., 2021) proposes QG-DAdam, which utilizes quasi-global momentum to locally approximates
the globally descent direction. A recent work (Chen et al., 2021a) proposes a unified framework
that incorporates various adaptive methods into decentralized setting. While these methods have
shown strong empirical performance in several practical applications, they either suffer from unsta-
ble convergence or heavy communications. We leave more discussion on their limitations in Sec. 2.
Adaptive gradient methods have also been extended to the federated learning setting in which mul-
tiple clients cooperate to learn a model under the supervision of a central server (McMahan et al.,
2017). Useful references in this direction can be found in (Xie et al., 2019; Reddi et al., 2020).
2
Under review as a conference paper at ICLR 2022
2 Adapt-then/while-communicate structure and its limitation
Problem. Suppose n computing nodes cooperate to solve the distributed optimization problem:
1n
min f (x) = n£fi(x)， where fi(x) ：= Eξi〜DiF(x； ξi)	(1)
x	i=1
In the above problem, fi (x) is local to node i, and random variable ξi denotes the local data that
follows distribution Di . We do not assume each distribution Di is the same across all nodes.
Network topology and weights. We assume all computing nodes are connected by a network
topology. We define wij , the weight to scale information flowing from node j to node i, as follows:
> 0 if node j is connected to i, or i = j ;
wij = 0 otherwise.
(2)
Ni := {j |wij > 0} is defined as the set of neighbors of node i which also includes node i itself and
the weight matrix W := [wij ]in,j =1 ∈ Rn×n are denoted as a matrix that stacks the weights of all
nodes. This matrix W characterizes the sparsity and connectivity of the underlying topology.
Partial averaging. Decentralized methods are based on partial averaging within neighborhood
defined by the network topology. With weights {wij } and the set of neighbors Ni, the partial
averaging operation of node i can be expressed as
Partial averaging: x+ — E Wijxj.	(3)
j∈Ni
Partial averaging requires much less communication than global averaging on sparse topologies.
Assumptions. We will make the following standard assumptions throughout the paper:
A.1 [Smoothness] Each fi(χ) is L-Smooth,i.e., ∣Nfi (χ)-V∕i(y)k ≤ L∣∣χ-yk for any χ,y ∈ Rd.
A.2 [GRADIENT NOISE] The random sample ξi(t) is independent of each other for any k and i. We
also assume E[VF (x; ξi)] = Vfi(x) and EkVF (x; ξi) - Vfi(x)k2 ≤ σ2.
A.3 [WEIGHT MATRIX] The weight matrix W is symmetric and doubly-stochastic, i.e. W1 = 1
and ITW = IT, where 1 is an all-ones vector. Further, We assume ∣∣ W 一 n 11t∣∣2 ≤ P ∈ (0,1).
A.4 [B oUNDED GRADIENT] The loss function F has bounded gradient, i.e., the maximum norm
∣VF (x; ξi)∣∞ ≤ G for all x and ξi, and i ∈ [n].
Notation. Given a constant n, we let [n] := {1, ∙ ∙ ∙ , n}. Given a vector X ∈ Rd, we let diag(x)=
diag{χι,…，xd} ∈ Rd×d be a diagonal matrix. Given two vectors X ∈ Rd and y ∈ Rd, we let
x y be the element-wise product between x and y.
2.1 The adapt-while-communicate structure and its limitation
Adapt-While-Communicate Structure. Algorithm 2 in Appendix B lists recursions for DAdam
(Nazari et al., 2019). The critical step in DAdam, which is highlighted in Algorithm 2, is as follows
adapt
communicate Z 人 、
，八、	(t)
x(t+1) = X WijXjt)——Y^i—,	∀ i ∈ [n].	(4)
j∈N	v^ ^(t) + e
where m(t) is the momentum, v(t) is an estimate of VF(x(t); ξ(t)) Θ VF(x(t); ξ(t)), and E is a small
positive constant. Their updates can be referred to Algorithm 2. In recursion (4), it is observed
that node i in DAdam conducts the adaptive step (i.e., scales the gradient by the square root of
exponential moving averages of past squared gradients) while it communicates with neighbors. We
call such structure utilized in DAdam as adapt-while-communicate.
Fixed point condition. Now we examine the fixed point achieved by DAdam and verify whether
it satisfies the optimal condition to problem (1). To this end, we let xi(t) → xi∞ as t → ∞ for
i ∈ [n]. To remove the the influence of gradient noise, we assume each node i can access the full-
batch gradient Vfi(X). If we define Hi := diag( dVfi(x∞) Θ Vfi(x∞)) to be a diagonal matrix
associated with node i, the fixed point condition for DAdam is (see derivation in Appendix B.2):
3
Under review as a conference paper at ICLR 2022
x∞ = E WijX∞ — Y(Hi + eI)-1Vfi(x∞), ∀i ∈ [n].	(5)
j∈Ni
1n
=⇒ 一 (H+e + eI)-1Vfi(x∞) = 0 (if x∞ =…=x∞ = x∞ in the limit) (6)
i=1
Fixed point in DAdam is deviated from the desired solution. In the data-heterogeneous scenario
where data sample ξi follows different distribution Di, it holds that fi(x) 6= fj (x). If we further
assume Vfi (x) 6= Vfj (x), it follows that Hi 6= Hj, for any i 6= j. With this fact, we conclude
that DAdam fixed point satisfying condition (6) does not necessarily satisfy the optimality condition
for problem (1), i.e., n Pn=ι Vfi(χ∞) = 0. This implies the fixed point of DAdam is not the
stationary solution to problem (1). In particular, the following proposition derives the distance
between the DAdam fixed point and global optimal solution to problem (1) when the cost function
is strongly-convex (Proof is in Appendix B.3).
Proposition 1 Under Assumptions A.1, A.3 and A.4, if each fe(x) is further assumed to be μ-
strongly-convex, and the full-batch gradient Vfi(x) is accessed per iteration, then DAdam (Al-
gorithm 2) cannot converge to x?, i.e., the global solution to problem (1). In particular, the distance
between DAdam limiting point and x? can be characterized as follows:
㈣ Xx W)-x*k1 2=O( (iγ‰+B2)	⑺
where B := ∣∣n Pn=ι(Hi + eI)-1Vfe(x∞) — n1 Pn=ι Vfi(X∞)k = 0 is a constant bias term.
Furthermore, the B2 term in (7) is necessary and cannot be removed by an improved analysis; there
exists strongly convex and smooth functions fi(x) such that limt→∞ Pn=Ikx(t) — x?k2 = Ω(B2).
It is observed in Proposition 1 that the distance between the DAdam fixed point and the optimal
solution cannot diminish to zero as learning rate γ → 0. Such non-vanishing limiting bias B2
is caused by the adapt-while-communicate structure in DAdam. It is worth noting that a recent
work (Chen et al., 2021a) has pointed out that DAdam cannot converge to the stationary solution
by constructing a counter-example. However, (Chen et al., 2021a) does not clarify the reason why
DAdam fails to converge to the stationary solution, and how far it can be from the desired solution.
2.2 The adapt-then-communicate structure and its limitation
Adapt-Then-Communicate Structure. Algorithm 3 in Appendix B lists the recursions for QG-
DAdam (Lin et al., 2021). The critical step, which is highlighted in Algorithm 3, is as follows
(t)
x(t+1) = X jjt-Tr-)	(8)
j∈Ni	vj + e
It is observed that node i in the above recursion first conducts the adaptive step, and then it commu-
nicates with neighbors. We call the structure utilized in QG-DAdam as adapt-then-communicate.
Fixed point condition and limitation. Following the settings and arguments in Sec. 2.1, we can
derive the fixed point condition for QG-DAdam as follows (see the derivation in Appendix B.5):
x∞ = X Wijr∞ — YeLHj + eI)-1Vfj(x∞)), ∀i ∈ [n].	(9)
j∈Ni
1 n
=⇒ — £( √β2Hi + eI )-1Vfi(x∞) =0 (if x∞ =…=x∞ = x∞ in the limit) (10)
n i=1
where β1 , β2 ∈ (0, 1) are momentum parameters listed in Algorithm 3. Similar to DAdam, the
above condition implies that the fixed point of QG-DAdam is also deviated from the stationary
solution. In particular, the distance between QG-DAdam fixed point and the global solution x? can
be characterized for the strongly-convex scenario as follows (proof is in Appendix B.6):
4
Under review as a conference paper at ICLR 2022
Proposition 2 Under the same conditions as in Proposition 1, the distance between QG-DAdam
fixed point and the global solution x? can be characterized as follows:
t→∞ XX kχ(t)-χ*k2 = o( (ir-2‰ + B2)	(II)
where B ：= k 1 Pn=ι(√1 — β2Hi + eI)-1Vfi(x∞) - 1 Pn=I Vfi(x∞)k = 0 is a constant bias.
Furthermore, the B2 term in (7) is necessary and cannot be removed by an improved analysis; there
exists strongly ConVeX and smooth functions fi(x) SUCh that limt→∞ pn=1 ∣∣x(t) 一 x?k2 = Ω(B2).
Similar to DAdam, it is observed that the distance between the QG-DAdam fixed point and the
optimal solution cannot diminish to zero as learning rate γ→ 0.
2.3 GT-DAdam: A decentralized adaptive method based on gradient tracking
It is observed from the fixed-point condition (6) that DAdam cannot converge to the stationary
solution due to the discrepancy between Hi’s. A recent work (Chen et al., 2021a) proposes a gradient
tracking based DAdam that enables each local different Hi to converge to H = 1 Pn=1 Hi in theory,
and therefore, its limiting fixed point satisfies 1 PJ==r V∕i(x∞) = 0, the OPtimality condition to
problem (1). GT-DAdam also utilizes the adapt-while-communicate structure. While it can converge
to the stationary solution to problem (1) as learning rate γ → 0, it incurs twice amount of the
communication overhead as DAdam. The distance between GT-DAdam fixed point and the global
solution x? can be characterized in Proposition 4 in Appendix B.7 for the strongly-convex scenario.
In the convex scenario, the distance between GT-DAdam fixed point and the global solution can be
worse than that of the proposed DAG-Adam, see Table 5 in Appendix C.3 and Fig.1 in Sec. 3.
3 Decentralized Augmented Gradient Adam
Now that we understand that the adapt-while/then-communicate structure in decentralized adaptive
methods either deviates the limiting point from the desired solution (e.g., DAdam and QG-DAdam),
or incurs more expensive communication overhead (GT-DAdam), this section will propose a new
algorithm that can overcome the limitation of the adapt-while/then-communicate structure.
Decentralized SGD interprets as standard SGD. Without loss of generality, we assume each node
i can access the accurate gradient Vfi(X) in decentralized SGD recursions. In this scenario, decen-
tralized SGD, according to (Yuan et al., 2016), can be written as
xi(t+1) = X wij x(jt) - γVfi(xi(t))	(12)
j∈Ni
=x(t)-γ( Vfi(X(t)) + J(x(t) - X WijXjt))), Vi ∈ [n].	(13)
γ	j∈Ni
X----------------{z---------------}
augmented gradient
In other words, decentralized SGD (12) can be regarded as a standard SGD recursion (13) to solve
n	1 n	nn
min	L({χi}i=1)	：= X fi(χi)	+ 2-(XIlxik2	- XXWijXiXj).	(14)
{xi}i=1	i=1	2γ i=1	i=1 j=1
We call above as an augmented loss function, which is an approximate penalty problem to the target
problem (1). The term Pin=1 kXik2 - Pi,j Wij Xi Xj is convex when the weight matrix W = [Wij] is
symmetric and doubly-stochastic, and it equals 0 if xi =…=Xn. Apparently, it is a regularization
term that promotes consensus among all nodes.
Decentralized Augmented Gradient Adam (DAG-Adam). Inspired by the fact that decentralized
SGD can be interpreted as a standard SGD with an augmented gradient (13), we can easily inte-
grate the augmented gradient to the standard Adam method (Kingma & Ba, 2014). Decentralized
Augmented Gradient Adam, or DAG-Adam for short is listed in Algorithm 1. It is observed that
all communications occur when the stochastic augmented gradient is computed (see the highlighted
part in Algorithm 1). After that, each node i will conduct an adaptive step, which is untangled with
communication. We call the structure utilized in DAG-Adam as communicate-then-adapt. This ad-
justment of the algorithm structure is crucial. The stochastic gradient that used to compute mi(t) and
5
Under review as a conference paper at ICLR 2022
Algorithm 1 DAG-Adam
Initialize xi(0) arbitrarily; let β1 ,β2,∈ [0, 1); let mi(0) = vi(0) = 0; set γ, ν properly;
For t = 0,1,2,…，T 一 1, every node i do in parallel
Sample ^(t) = VF(x(t); ξ(t)) + V(x(t) — ^X WijXjt))	. stochastic augmented gradient;
Y	j∈N
m(t) = βιmit-1) + (1 — βι) g(t);
v(t)= β2v(t-1) + (1- β2) g(t) Θ ^(t);
(t+1)	(t)	m(it)
Xi	= Xi — γ pɪ--	. adaptive step;
vi(t)+
vi(t) have incorporated the neighbor’s information, which will correct the deviation in fixed point
that introduced by different vi (and Hi ).
Note that a parameter ν > 0 is added to the adjust the pri-
ority to gradient descent, or to promote consensus. Empiri-
cally, we can set ν = 1 for the convex scenario. But for the
non-convex deep learning problems, the choice of ν will
heavily influence both convergence rate and performance.
We extensively discuss the influence of ν in Appendix D
and empirically examine it in the later experiment section.
While this paper we only focus on the extension of the
vanilla Adam method, the core ideas such as the augmented
gradient and the communicate-then-adapt structure can be
extended to AMSGrad or other adaptive gradient methods.
Learning curves (logarithmic scale)
tween various algorithms for a full-batch
logistic regression problem.
Fixed point condition. The stochastic augmented gradi-
ent and communicate-then-adapt structure utilized in DAG-
Adam can remove the influence of the discrepancy in vi∞ and hence Hi . Following the settings and
arguments in Sec. 2.1, we can derive the fixed point condition for DAG-Adam as follows (see the
derivation in Appendix C):
Vfi(X∞) + ν(χ∞ — X WijX∞) = 0,
1 n γ	j∈Ni
⇒	- X Vfi(X∞) = 0 (if X∞ =
n
i=1
∀i ∈ [n].
∙ ∙ = x∞ = x∞ in the limit)
(15)
(16)
Note that the fixed-point condition (15) for DAG-Adam, when ν = 1, is exactly the same as that
for vanilla decentralized SGD with full-bath gradient (Yuan et al., 2016). Relation (16) implies
that DAG-Adam will converge to the stationary solution to problem (1). The following proposition
characterizes the distance between the DAG-Adam fixed point and the global optimal solution X?
for the strongly-convex scenario.
Proposition 3 Under the same conditions as in Proposition 1, the distance between DAG-Adam
fixed point and X? can be characterized as follows:
kl→∞ XX H)-X*k2 = O( (1-G2ν2
(17)
It is observed in (17) that the distance is on the order of O(γ2). Apparently, DAG-Adam will con-
verge to the global optimal solution when γ → 0. Moreover, DAG-Adam has the same communica-
tion overhead as in DAdam, and it is more communication-efficient than GT-DAdam. Proposition 3
also implies that DAG-Adam has the same limiting bias as decentralized SGD when ν = 1.
Limiting point justification. We validate the limiting point analysis for DAdam, QG-DAdam, GT-
DAdam, and DAG-Adam on the full-batch logistic regression problem. Table 5 in Appendix C.3
lists the results We establish in Propositions 1-4. When V = 1 in the DAG-DAdam method, which
is a valid choice in convex settings in numerical experiments, the fixed point of DAG-DAdam is
closet to the global solution X? when the cost function is strongly-convex. The learning curves of
6
Under review as a conference paper at ICLR 2022
DAdam, GT-DAdam, QG-DAdam, and DAG-Adam are shown in Fig. 1. The MSE metric in the
y-axis denotes n1 Pn=Ikx(t) - x?k2. From the figure, it is observed that the DAG-Adam algorithm
can converge as fast as other decentralized adaptive algorithms but to a more accurate limiting point.
This limiting point is exactly the same as vanilla decentralized SGD, which is consistent with our
claim in the limiting point analysis. More experimental setting and results refer to Appendix C.4.
Convergence Analysis. We provide the convergence analysis for our proposed DAG-Adam for
non-convex functions. The full proof can be found in Appendix E.
Theorem 1 Suppose the parameters γ, ν, and β2 satisfy the conditions that Y ≤ ⅛ν, ν <
2
min{e∕2, κγ}, (K is any positive constant) and 1 一 β2 ≤ 双3+代产∙ Under Assumptions A.1 -
A.4 and the augmented gradient shown is bounded, the decentralized augmented gradient generated
by DAG-Adam algorithm satisfies that
T X X 同(x(t)) + ” - X Wjxjt))∣∣2 =。(S}「2) + O(σ2)
t=0 n=1	γ	j∈Ni	γ	(18)
where the augmented loss function L(x) is the one defined in (14), L? is the minimum of the aug-
mented loss. Further, it holdsfor the globally averaged iterate x(t) that
T X kVf(x(t))k2 ≤ O( L({x(O)YT0)-L? ) + O(σ2) + O( J)2ν2 )	(19)
where x⑴=1/n PZi x(t) and P is the second largest eigenvalue of W in magnitude.
Remark 1 The theorem stated two conclusions. The first one is that the augmented gradient in
DAG-Adam will converge to a neighborhood around 0 with constant learning rate γ, and the neigh-
borhood, when T → ∞, is on the order of O(σ2). The SeCOnd one is that DAG-Adam will COnverge
to a neighborhood around the stationary solution in terms ofthe globally averaged iterate x⑴.The
neighborhood, when learning rate Y = O(1∕√T) and T → ∞, will be on the order of O(σ2). It is
thus observed that DAG-Adam will converge to the exact stationary solution with decaying learning
rate and increasing mini-batch sizes, which is consistent with the results of Adam using the standard
stochastic gradient in (Zaheer et al., 2018).
Remark 2 The non-vanishing term O(σ2) in (18) and (19) is caused by the vanilla Adam algorithm
structure, not the decentralized communication and update in DAG-Adam. Such term can be cor-
rected by using AMSGrad rather than Adam in DAG-Adam. When there is no stochastic gradient
noise, the augmented gradient will converge to zero. In consequence, the vn(t) will converge to zero
over all agents as well.
4	Experiments
In this section, we systematically compare the proposed method, DAG-Adam, with all previous
mentioned state-of-the-art decentralized adaptive methods on typical CV and NLP tasks.
Implementation Details. We implement all the aforementioned algorithms with PyTorch 1.8.0
(Paszke et al. (2019)) using NCCL 2.9.6 (CUDA 10.2) as the communication backend. For PAdam,
we used PyTorch’s native Distributed Data Parallel (DDP) module. For the implementation of de-
centralized methods, we utilize BlueFog. See Appendix F for more details.
4.1	Image Classification: Performance over various configurations
A series of experiments are carried out with CIFAR-10 (Krizhevsky et al. (2009)) and ImageNet
(Deng et al. (2009)) datasets to evaluate the performance of the proposed DAG-Adam on both ho-
mogeneous and heterogeneous dataset over different sizes of network. We utilize ResNet-20 on
CIFAR-10 and ResNet-50 on ImageNet (He et al., 2016).
Training configuration. All methods are trained using 90 epochs with mini-batch of size 32 in each
computing node. PAdam uses a learning rate of 0.0125, while the ones for decentralized algorithms
are scaled by the network size. In addition, QG-Adam (Lin et al. (2021)) uses a learning rate 10
times smaller than other decentralized methods to avoid divergence1. During the training procedure,
1We examine QG-DAdam based on our own implementation of the algorithm listed in the original paper.
7
Under review as a conference paper at ICLR 2022
Table 1: Classification accuracy comparison of different methods for CIFAR10 dataset.
Methods	4 Nodes	8 Nodes			
	HD	α = 0.1	α=1	α = 10	IID
PAdam	90.28 ±0.07	51.65 ±0.88	84.33 ±0.18	90.82 ±0.23	91.19 ±0.09
DAdam	87.23 ±0.14	53.71 ±1.38	83.67 ±0.14	86.34 ±0.51	87.04 ±0.12
QG-DAdam	83.43 ±0.22	34.24 ±2.19	57.89 ±0.34	71.97 ±0.49	77.82 ±0.13
GT-DADAM	86.93 ±0.17	56.48 ±1.26	84.63 ±0.22	86.71 ±0.31	86.47 ±0.12
DAG-ADAM (ν = 10-3)	84.62 ±0.49	46.77 ±1.09	75.76 ±0.34	81.88 ±0.79	82.38 ±0.55
DAG-ADAM (ν = 10-2)	87.50 ±0.36	51.63 ±1.07	82.24 ±1.06	87.82 ±0.35	88.55 ±0.33
DAG-ADAM (ν = 10-1)	91.39 ±0.09	55.27 ±0.3 1	86.64 ±0.03	91.00 ±0.18	91.23 ±0.09
the learning rate is warmed up in the first 5 epochs and is then decayed by a factor of 10 at 30, 60 and
80 epochs. The weight decay is chosen as 5 × 10-5. We also compared the results of the proposed
DAG-Adam with various ν choices.
Heterogenerous data configuration. To simulate the heterogenerous data, the disjoint non-i.i.d.
training data over agents are generated through Dirichlet distribution Dir(α), where α stands for the
degree of the data heterogeneity (Lin et al., 2021; Yurochkin et al., 2019). The training data for a
particular class tends to concentrate in a single node as α → 0, i.e. becoming more heterogenous
while the homogeneous data is formed as α → ∞. See Appendix F for more details.
Performance comparison. Table 1 compares the proposed DAG-Adam with other methods over
networks with both homogeneous data distribution and heterogeneous ones. Two different network
sizes (4 and 8) are tested using ring topology. With a proper choice of ν, the proposed method
consistently outperforms all other methods under comparison, including the centralized optimization
method PAdam, no matter what the data distribution is given. The left and middle figures in Fig. 2
shows how the test loss and test accuracy evolves in a heterogeneous data distribution with α =
10. DAG-Adam (with ν = 1e-1) converges faster than any other decentralized algorithms. As
typical decentralized algorithm behaves, it was relatively slower compared with centralized PAdam
algorithm in the beginning, but it eventually caught up when the algorithm converged. The right
figure in Fig. 2 plots the consensus error Pikxi - X∣∣, which validates that the consensus error in
Theorem 1 is proportional to the choice of ν .
Epochs
PAdam
DAdam
QG-DAdam
GT-DAdam
DAG-Adam (le-3)
DAG-Adam (le-2)
DAG-Adam (Ie-I)
0	20	40	60	80
Epochs
Epochs


Figure 2:	The evolution comparison of loss, accuracy and consensus error for CIFAR-10 dataset on
a heterogeneous (α = 10) network of size 8.
a
• IlD
→- le+01
3e+00
→- Ie+00
→- 3e-01
→- le-01
→- 3e-02
→- le-02
→- 3e-03
-→- le-03
86% 88%90%92% 91% 90%
86% 88%90%91% 91% 90%
≡?
87%90%91%91%90%
86%86%87% 国
62%68%77%80%83
59%68%76%78%81
52%67%71%74%82%85
49%55%67%72%74%78%B3
34%46%51% 55% 60%60%63% 68%66%65% 64%
30%36%38%41%48%51%53%53%53%53%53%
0%33%37%39%39%36%35%
1%31
21%23%26%28?
23%22%24%26%27%
28%28%27%26%
22%23%25%26
J0%30%30%2ξ
22%22%23%24%25%26%25%25%27%27%26%
-90
80
70
60
50
40
-30
2
Figure 3:	An ablation study of the hyperparameter ν in DAG-Adam for CIFAR-10 dataset on het-
erogeneous networks of size 8. The heatmap on the right shares the same selection of ν and α with
the left figure. We reported the detailed accuracy value in it.
Sensitivity on ν constant. As shown in Table 1, the final classification performance depends on
the choice of constant ν in DAG-Adam. To better understand how ν affects the DAG-Adam perfor-
mance, we performed a corresponding ablation study with varying heterogeneity degree α as shown
8
Under review as a conference paper at ICLR 2022
in Fig. 3. For varying heterogeneity degree α, the constant ν that achieving best performance lies in
the range of [1e-2, 5e-1] for CIFAR-10 typically.
Performance over different network scales. We further examine the performance of the proposed
algorithm over different network sizes to see how good it can scale up in large decentralized network
case. As shown in Table 2, under the exponential graph (Assran et al., 2019) and fine-tuning of ν,
DAG-Adam persistently achieves comparable performance with the centralized method until the
node size reaches n = 64. Even under that case, the drop of performance is quite small.
Table 2:	Comparison of different nodes and methods on CIFAR-10 classification over homogeneous data.
Nodes
4
8
16	32	64
PAdam
DAG-ADAM (ν = 1e-3 )
DAG-ADAM (ν = 1e-2 )
DAG-ADAM (ν = 1e-1 )
91.28 ±0.07
84.62 ±0.57
87.50 ±0.29
91.39 ±0.11
91.19 ±0.09
82.38 ±0.55
86.55 ±0.33
91.23 ±0.09
90.67 ±0.09
84.35 ±0.51
86.89 ±0.42
91.36 ±0.12
90.15 ±0.11
83.73 ±0.49
87.83 ±0.31
90.16 ±0.07
90.11 ±0.13
83.90 ±0.61
87.62 ±0.37
89.98 ±0.22
ImageNet results. We also validate the performance of our proposed method on a relatively large-
scale dataset ImageNet. We use PAdam to tune the learning rate (4 × 10-4) and wight decay (1 ×
10-4) hyper-parameters to reach a strong baseline. Then, the same hyper-parameters are applied
for decentralized counterparts. In the experiment, the algorithms run on 8 nodes (i.e. 8 × 8 = 64
GPUs) and train total 100 epochs with a cosine decay scheduler. Our proposed method reaches top-1
accuracy 75.96% which is slightly better than PAdam.
Table 3:	Comparison of different methods and models on ImageNet classification.
Method PADAM DADAM QG GT DAG∕1e-1 DAG∕1e-2 DAG∕1e-3 DAG∕1e-4
Accuracy 75.62	70.53 73.89 74.736	66.506	73.750	75.445	75.96
4.2 Language Modeling: Performance over different tasks
BERT (Devlin et al. (2018)) is a widely used pre-training language representation model for NLP
tasks. We fine-tuned pretrained BERT models (Base/Large: 110/330M parameters) on the SQuAD
(Rajpurkar et al. (2016)) dataset. All experiments are based on NVIDIA BERT2 implementation
with mixed precision support. We examine the task over both static exponential graph and dynamic
exponential graph setting (Assran et al., 2019). As shown in Table 4, with proper tuned hyper-
parameter ν, our proposed method reached best performance in both metrics.
Table 4: comparison of different methods on BERT fine-tuning (SQuAD) on Exact Match and F1-score.
Model TOPOLOGY EM		BERT-BASE				BERT-LARGE			
		Static		Dynamic		Static		Dynamic	
		F1	EM	F1	EM	F1	EM	F1	EM
PAdam		81.50	88.85	-	-	83.54	90.69	-	-
DAdam		80.61	87.57	80.31	87.42	83.25	89.98	83.16	89.77
QG-DADAM		79.4	86.98	80.17	87.33	83.05	89.84	83.89	90.64
GT-DADAM		80.66	87.85	80.67	87.81	83.81	90.51	83.95	90.61
DAG-ADAM (ν =	1e-1 )	48.89	60.85	43.77	56.22	59.24	71.40	55.18	67.89
DAG-ADAM (ν =	1e-2 )	71.98	81.62	70.30	80.15	78.20	86.68	76.85	85.59
DAG-ADAM (ν =	1e-3 )	80.66	87.99	80.14	87.54	83.93	90.48	83.31	90.26
DAG-ADAM (ν =	1e-4)	81.79	88.93	81.89	88.96	83.82	90.71	84.28	91.08
5 Conclusion and future works
In this paper, we identified the limitation of adapt-while/then-communicate structure utilized in
existing adaptive gradient methods: the limiting discrepancy in the squared local gradient term (i.e.,
vi∞ ) can deviate the limiting point from the desired stationary point. To fix this issue, we proposed a
novel DAG-Adam algorithm that is built upon the communicate-then-adapt structure instead. DAG-
Adam can provably converge to the desired solution even with heterogeneous data. Experimental
results on a variety of deep learning tasks show that DAG-Adam persistently outperforms existing
decentralized adaptive baselines. However, it is also observed in these experiments that DAG-Adam
is highly-sensitive to the hyper-parameter ν. How to tune ν automatically during the training stage
is a crucial topic of the future works.
2https://github.com/NVIDIA/DeepLearningExamples/tree/master/PyTorch/LanguageModeling/BERT
9
Under review as a conference paper at ICLR 2022
References
Dan Alistarh, Demjan Grubic, Jerry Li, Ryota Tomioka, and Milan Vojnovic. Qsgd:
Communication-efficient sgd via gradient quantization and encoding. In Advances in Neural
Information Processing Systems,pp. 1709-1720, 2017.
Mahmoud Assran, Nicolas Loizou, Nicolas Ballas, and Mike Rabbat. Stochastic gradient push for
distributed deep learning. In International Conference on Machine Learning (ICML), pp. 344-
353, 2019.
Jeremy Bernstein, Jiawei Zhao, Kamyar Azizzadenesheli, and Anima Anandkumar. signsgd with
majority vote is communication efficient and fault tolerant. arXiv preprint:1810.05291, 2018.
Jianshu Chen and Ali H Sayed. Diffusion adaptation strategies for distributed optimization and
learning over networks. IEEE Transactions on Signal Processing, 60(8):4289-4305, 2012.
Tianyi Chen, Georgios Giannakis, Tao Sun, and Wotao Yin. LAG: Lazily aggregated gradient for
communication-efficient distributed learning. In Advances in Neural Information Processing Sys-
tems, pp. 5050-5060, 2018a.
Xiangyi Chen, Sijia Liu, Ruoyu Sun, and Mingyi Hong. On the convergence of a class of adam-type
algorithms for non-convex optimization. In International Conference on Learning Representa-
tions, 2018b.
Xiangyi Chen, Belhal Karimi, Weijie Zhao, and Ping Li. On the convergence of decentralized
adaptive gradient methods. arXiv preprint arXiv:2109.03194, 2021a.
Yiming Chen, Kun Yuan, Yingya Zhang, Pan Pan, Yinghui Xu, and Wotao Yin. Accelerating gossip
sgd with periodic global averaging. In Proceedings of the 38th International Conference on
Machine Learning (ICML), 2021b.
Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale
hierarchical image database. In IEEE Conference on Computer Vision and Pattern Recognition
(CVPR), pp. 248-255. Ieee, 2009.
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep
bidirectional transformers for language understanding. arXiv preprint:1810.04805, 2018.
P. Di Lorenzo and G. Scutari. Next: In-network nonconvex optimization. IEEE Transactions on
Signal and Information Processing over Networks, 2(2):120-136, 2016.
John Duchi, Elad Hazan, and Yoram Singer. Adaptive subgradient methods for online learning and
stochastic optimization. Journal of machine learning research, 12(7), 2011a.
John C Duchi, Alekh Agarwal, and Martin J Wainwright. Dual averaging for distributed optimiza-
tion: Convergence analysis and network scaling. IEEE Transactions on Automatic control, 57(3):
592-606, 2011b.
Hongchang Gao and Heng Huang. Periodic stochastic gradient descent with momentum for decen-
tralized training. arXiv preprint:2008.10435, 2020.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recog-
nition. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp. 770-778,
2016.
Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv
preprint:1412.6980, 2014.
Anastasia Koloskova, Tao Lin, Sebastian U Stich, and Martin Jaggi. Decentralized deep learning
with arbitrary communication compression. In International Conference on Learning Represen-
tations, 2019a.
Anastasia Koloskova, Sebastian Stich, and Martin Jaggi. Decentralized stochastic optimization and
gossip algorithms with compressed communication. In International Conference on Machine
Learning, pp. 3478-3487, 2019b.
10
Under review as a conference paper at ICLR 2022
Anastasia Koloskova, Nicolas Loizou, Sadra Boreiri, Martin Jaggi, and Sebastian U Stich. A unified
theory of decentralized sgd with changing topology and local updates. In International Confer-
ence on Machine Learning (ICML),pp.1-12, 2020.
Lingjing Kong, Tao Lin, Anastasia Koloskova, Martin Jaggi, and Sebastian U Stich. Consensus
control for decentralized deep learning. arXiv preprint:2102.04828, 2021.
Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images.
2009.
Z. Li, W. Shi, and M. Yan. A decentralized proximal-gradient method with network independent
step-sizes and separated convergence rates. IEEE Transactions on Signal Processing, July 2019.
early acces. Also available on arXiv:1704.07807.
Xiangru Lian, Ce Zhang, Huan Zhang, Cho-Jui Hsieh, Wei Zhang, and Ji Liu. Can decentralized
algorithms outperform centralized algorithms? a case study for decentralized parallel stochastic
gradient descent. In Advances in Neural Information Processing Systems, pp. 5330-5340, 2017.
Xiangru Lian, Wei Zhang, Ce Zhang, and Ji Liu. Asynchronous decentralized parallel stochastic
gradient descent. In International Conference on Machine Learning, pp. 3043-3052, 2018.
Tao Lin, Sai Praneeth Karimireddy, Sebastian U Stich, and Martin Jaggi. Quasi-global momen-
tum: Accelerating decentralized deep learning on heterogeneous data. In Proceedings of the 38th
International Conference on Machine Learning (ICML), 2021.
Liyuan Liu, Haoming Jiang, Pengcheng He, Weizhu Chen, Xiaodong Liu, Jianfeng Gao, and Jiawei
Han. On the variance of the adaptive learning rate and beyond. In International Conference on
Learning Representations, 2019a.
Xiaorui Liu, Yao Li, Rongrong Wang, Jiliang Tang, and Ming Yan. Linear convergent decentralized
optimization with compression. arXiv preprint arXiv:2007.00232, 2020.
Yaohua Liu, Wei Xu, Gang Wu, Zhi Tian, and Qing Ling. Communication-censored admm for
decentralized consensus optimization. IEEE Transactions on Signal Processing, 67(10):2565-
2579, 2019b.
Cassio G Lopes and Ali H Sayed. Diffusion least-mean squares over adaptive networks: Formulation
and performance analysis. IEEE Transactions on Signal Processing, 56(7):3122-3136, 2008.
Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. In International Confer-
ence on Learning Representations, 2018.
Songtao Lu, Xinwei Zhang, Haoran Sun, and Mingyi Hong. Gnsd: A gradient-tracking based non-
convex stochastic algorithm for decentralized optimization. In 2019 IEEE Data Science Workshop
(DSW), pp. 315-321. IEEE, 2019.
Brendan McMahan, Eider Moore, Daniel Ramage, Seth Hampson, and Blaise Aguera y Arcas.
Communication-efficient learning of deep networks from decentralized data. In Artificial Intelli-
gence and Statistics, pp. 1273-1282. PMLR, 2017.
Parvin Nazari, Davoud Ataee Tarzanagh, and George Michailidis. Dadam: A consensus-based
distributed adaptive gradient method for online optimization. arXiv preprint arXiv:1901.09109,
2019.
A. Nedic, A. Olshevsky, and W. Shi. Achieving geometric convergence for distributed optimization
over time-varying graphs. SIAM Journal on Optimization, 27(4):2597-2633, 2017.
Angelia Nedic and Asuman Ozdaglar. Distributed subgradient methods for multi-agent optimiza-
tion. IEEE Transactions on Automatic Control, 54(1):48-61, 2009.
Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor
Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. Pytorch: An imperative style,
high-performance deep learning library. In Advances in Neural Information Processing Systems
(NeurIPS), pp. 8024-8035, 2019.
11
Under review as a conference paper at ICLR 2022
G. Qu and N. Li. Harnessing smoothness to accelerate distributed optimization. IEEE Transactions
on Control of Network Systems, 5(3):1245-1260, 2018.
Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. Squad: 100,000+ questions
for machine comprehension of text. arXiv preprint arXiv:1606.05250, 2016.
Sashank J Reddi, Satyen Kale, and Sanjiv Kumar. On the convergence of adam and beyond. arXiv
preprint:1904.09237, 2019.
Sashank J Reddi, Zachary Charles, Manzil Zaheer, Zachary Garrett, Keith Rush, Jakub Konecny,
Sanjiv Kumar, and Hugh Brendan McMahan. Adaptive federated optimization. In International
Conference on Learning Representations, 2020.
Ali H Sayed. Adaptation, learning, and optimization over networks. Foundations and Trends in
Machine Learning, 7(ARTICLE):311-801, 2014.
Kevin Scaman, Francis Bach, Sebastien Bubeck, Yin Tat Lee, and Laurent Massoulie. Optimal
algorithms for smooth and strongly convex distributed optimization in networks. In International
Conference on Machine Learning, pp. 3027-3036, 2017.
W. Shi, Q. Ling, G. Wu, and W. Yin. EXTRA: An exact first-order algorithm for decentralized
consensus optimization. SIAM Journal on Optimization, 25(2):944-966, 2015.
Wei Shi, Qing Ling, Kun Yuan, Gang Wu, and Wotao Yin. On the linear convergence of the admm
in decentralized consensus optimization. IEEE Transactions on Signal Processing, 62(7):1750-
1761, 2014.
Sebastian Urban Stich. Local sgd converges fast and communicates little. In International Confer-
ence on Learning Representations (ICLR), 2019.
Hanlin Tang, Xiangru Lian, Ming Yan, Ce Zhang, and Ji Liu. d2 : Decentralized training over
decentralized data. In International Conference on Machine Learning, pp. 4848-4856, 2018.
Hanlin Tang, Chen Yu, Xiangru Lian, Tong Zhang, and Ji Liu. Doublesqueeze: Parallel stochastic
gradient descent with double-pass error-compensated compression. In International Conference
on Machine Learning, pp. 6155-6165. PMLR, 2019.
Cesar A Uribe, Soomin Lee, Alexander Gasnikov, and Angelia Nedic. A dual approach for optimal
algorithms in distributed optimization over networks. Optimization Methods and Software, pp.
1-40, 2020.
Cong Xie, Oluwasanmi Koyejo, Indranil Gupta, and Haibin Lin. Local adaalter:
Communication-efficient stochastic gradient descent with adaptive learning rates. arXiv preprint
arXiv:1911.09030, 2019.
Ran Xin, Usman A Khan, and Soummya Kar. An improved convergence analysis for decentralized
online stochastic non-convex optimization. arXiv preprint:2008.04195, 2020.
J.	Xu, S. Zhu, Y. C. Soh, and L. Xie. Augmented distributed gradient methods for multi-agent op-
timization under uncoordinated constant stepsizes. In IEEE Conference on Decision and Control
(CDC), pp. 2055-2060, Osaka, Japan, 2015.
Hao Yu, Rong Jin, and Sen Yang. On the linear speedup analysis of communication efficient mo-
mentum sgd for distributed non-convex optimization. In International Conference on Machine
Learning, pp. 7184-7193. PMLR, 2019.
K.	Yuan, B. Ying, X. Zhao, and A. H. Sayed. Exact dffusion for distributed optimization and
learning - Part I: Algorithm development. IEEE Transactions on Signal Processing, 67(3):708 -
723, 2019.
Kun Yuan, Qing Ling, and Wotao Yin. On the convergence of decentralized gradient descent. SIAM
Journal on Optimization, 26(3):1835-1854, 2016.
12
Under review as a conference paper at ICLR 2022
Kun Yuan, Yiming Chen, Xinmeng Huang, Yingya Zhang, Pan Pan, Yinghui Xu, and Wotao
Yin. DecentLaM: Decentralized momentum SGD for large-batch deep training. arXiv preprint
arXiv:2104.11981, 2021.
Mikhail Yurochkin, Mayank Agarwal, Soumya Ghosh, Kristjan Greenewald, Nghia Hoang, and
Yasaman Khazaeni. Bayesian nonparametric federated learning of neural networks. In Interna-
tional Conference on Machine Learning, pp. 7252-7261. PMLR, 2019.
Manzil Zaheer, Sashank Reddi, Devendra Sachan, Satyen Kale, and Sanjiv Kumar. Adaptive
methods for nonconvex optimization. In S. Bengio, H. Wallach, H. Larochelle, K. Grauman,
N. Cesa-Bianchi, and R. Garnett (eds.), Advances in Neural Information Processing Systems,
volume 31. Curran Associates, Inc., 2018. URL https://proceedings.neurips.cc/
paper/2018/file/90365351ccc7437a1309dc64e4db32a3-Paper.pdf.
Dongruo Zhou, Jinghui Chen, Yuan Cao, Yiqi Tang, Ziyan Yang, and Quanquan Gu. On
the convergence of adaptive gradient methods for nonconvex optimization. arXiv preprint
arXiv:1808.05671, 2018.
13
Under review as a conference paper at ICLR 2022
A Notations and Preliminaries
Notations. We introduce several notations as follows:
x1
x2
• X ,	.	∈ Rnd ×1,
.
.
xn
m1
m2
m，	.	∈ Rnd×1
.
.
mn
v1
v2
V ,	.	∈ Rnd×1.
.
.
vn
• Vbf (X) ,
Γ VFi (xι; ξι)]
▽F2 (X2 ； ξ2 )
∈ Rnd×1
LVFn (Xn; ξn )」
•	W = [wij] ∈ Rn×n.
•	W，W 0 Id ∈ Rndxnd, where ∙ means the KroneCker product.
•	1n =col{1,1,...,1} ∈Rn.
•	X = (nIn 1T 0 Id)x ∈ Rndx 1.
•	Given a vector X ∈ Rd, we let diag(x) = diag{x1, ∙ ∙ ∙ , Xd} ∈ Rdxd be a diagonal matrix.
•	Given a positive definite matrix A, we define the weighted normal kxk2A = xT Ax, kxk as
vector `2 norm and kAk as matrix `2 norm.
Algorithm reformulation. With the above notation, we rewrite DAG-Adam in Algorithm 1 into the
following stacked vector form:
g(t) =Vb(x(t)) + V(I - W)x(t)	(20)
γ
m(t) =β1m(t-1) + (1 - β1)g(t)	(21)
v(t) =β2v(t-1) + (1 - β2)g(t) g(t)	(22)
X(t+1) =X(t) - γ(H(t) + I)-1m(t)	(23)
where H(t) is the diagonal matrix defined as H(t) = Diag(Vzv(t)).
Smoothness. Since each fi(X) is assumed to be L-smooth in Assumption A.1, it holds that f(X) =
温 En=I fi (x) is also L-Smooth. As a result, the following inequality holds for any x, y ∈ Rd:
f(X)- f(y) — 2Ilx - y∣l2 ≤ hvf(y), X - yi	(24)
Figure 4: An illustration of decentralized topology and its communication pattern
14
Under review as a conference paper at ICLR 2022
Algorithm 2 DAdam (Nazari et al., 2019)
Initialize χi0) arbitrarily; let β1,β2,β3 ∈ (0,1); let m(0) = v(0) =V(0) = 0; set Y properly;
For t = 0, 1, 2, ..., T - 1, every node i do
Sample g(t = VF (χ(t; ξ(t));
mi(t) = β1mi(t-1) + (1 - β1) gi(t);
vi(t) = β2vi(t-1) + (1 - β2) gi(t)	gi(t);
Vft= β3V(tT) + (1 - β3) max{v(tT), v(t)};
x(t+1) = Pj∈Ni Wijxjt) - Y
m(t
Vty+ J
Weighted norm inequality. In this paper, the weighted norm kχk* 2 * * * * * B * * * * * * isA only considers the scenario in
which A is a diagonal matrix. Suppose the diagonal elements in the diagonal matrix A satisfy that
0 < a ≤ [A]i,i ≤ b for all i, one useful inequality is
akχk2A ≤ kAχk2 ≤ bkχk2A	(25)
The proof is strait-forward:
kAxk2 = xτA2x = (A1∕2x)TA(A1∕2x) ≤ b(A1∕2x)T(A1∕2x) = b∣∣x∣∣A	(26)
We can use the same argument to show akχkTA ≤ kAχk2 as well.
Communication topology. Figure 4 provides a graph example to illustrate the decentralized topol-
ogy and its communication pattern that is used in all decentralized methods listed in the paper.
B	Adapt-while/then-communicate structure and its limitation
B.1 DAdam and adapt-while-communicate structure
The DAdam algorithm (Nazari et al., 2019) is listed in Algorithm 2.
B.2 Fixed point condition for DAdam
To see the fixed point condition for DAdam, we assume the iterate xi(t) in DAdam converges to some
fixed point xi∞ as k → ∞ for any i ∈ [n]. In order to remove the influence of the gradient noise, we
let gi(t) be the full-batch gradient Vfi(xi(t)) = E[VF(xi(t); ξi)] for any i ∈ [n]. Under this setting, it
is easy to verify that
m∞ = Vfi(x∞), and v∞ = V∞ = Vfi(X∞) Θ Vfi(x∞)	(27)
If We define Hi := diag(dVfi(x∞) Θ Vfi(X∞)) to be a positive diagonal matrix, then the high-
lighted step in DAdam will become (when t → ∞)
xi∞ = X wijxj∞ -Y(Hi+I)-1Vfi(xi∞), ∀i∈ [n].	(28)
j∈Ni
Which is the fixed-point condition listed in (5). If we further let x∞ := n Pn=ι x∞, the following
relation can be achieved by (28) and Assumption A.3:
n
x∞ = x∞ - Y X(Hi + eI)-1 Vfi(X∞)
n
i=1
1n
=⇒ -∑(Hi + eI )-1Vfi(x∞) = 0
n
i=1
1n
=⇒ — V(Hi + eI)-1 Vfi(X∞) = 0 (if x∞ =…=x∞ = x∞ in the limit). (29)
n	1n
i=1
The last relation is the one listed in (6).
15
Under review as a conference paper at ICLR 2022
B.3 Proof OF Proposition 1
Before we prove Proposition 1, we need to introduce several notations:
•	x∞ = col{x∞,…,x∞} ∈ Rnd
•	X∞ = col{x∞,…，x∞} ∈ Rnd where x∞ = ɪ Pn=I x∞.
•	x? = col{x*,…,x?} ∈ Rnd where x? is the optimal solution to problem (1) when the
cost function is convex
•	Vf(x∞)= col(V∕ι(x∞),…，Vfn(X∞)} ∈ Rnd
•	(H + eI)-1 = diag{(Hι + eI)-1, ∙∙∙ , (Hn + eI)-1}
With these notations, we can rewrite the fixed-point condition (28) into a compact manner:
x∞ = Wx∞ - Y(H + eI)-1Vf(x∞)	(30)
To examine ∣∣x∞ - x*k,we will evaluate ∣∣x∞ - X∞k and ∣∣x∞ - x?k respectively.
Upper bound of ∣x∞ - x?k. Left-multiplying n IT to both sides of recursion (30), we achieve
1n
-£(Hi + eI)-1Vfi(x∞)=0.
n z—z
i=1
With the above fact, we have
kx∞ -x?k
(31)
nn
=kx∞ - x* - Y(- X(Hi + eI)-1Vfi(x∞) - n X Vfi(X?)) ∣
n i=1	n i=1
1n	1n
≤ kx∞ - x* - Y(— X Vfi(x∞) - - X Vfi(x?))k
n i=1	n i=1
nn
+ Ykn X(Hi + eI)-1Vfi(x∞) - - X Vfi(X∞)k
i=1
i=1
1 ʌ	1 ʌ
+ Ykn X Vfi(x∞) - - X Vfi(x∞)k
i=1
i=1
≤ (1 -芳)kx∞ - x*k + YB + √nkx∞ - x∞k
(32)
where the last inequality holds because each fi (x) is μ-strongly convex and L-smooth, and the
definition of constant B as follows:
i=1
Inequality (32) leads to
nn
B :=k n X(Hi+ eI )-1Vfi(xT)-晟 X Vfi(X∞)k∙
(33)
i=1
√n∣x∞ - x*k ≤ Iy^B+2L ∣∣x∞ - x∞∣∣.
μ μ
(34)
Upper bound of ∣x∞ - x∞∣. Subtracting x∞ from both sides of (30), we have
(I - W)(x∞ - x∞) = -γ(H + eI)-1Vf(x∞).	(35)
With inequality (43) in (Yuan et al., 2021), we have
k(I - W)(x∞ - x∞)∣∣ ≥ (1 - ρ)∣x∞ - x∞k	(36)
With (35) and (36), and noticing ∣∣(H + eI )-1∣ = maxi {∣(Hi + eI )-1∣∣2} ≤ ɪ, we achieve
∣x∞ - x∞k ≤
YkVf (x∞)k
(I - ρ)e
(37)
≤ α¾
16
Under review as a conference paper at ICLR 2022
where the last inequality holds because of Assumption A.4.
Upper bound of limt→∞ 1 PZi ∣∣x(t) - x?k2. With (34) and (37), We have
∣x∞ - x?k ≤ ∣x∞ - X∞k + √n∣∣x∞ - x?k
(≤4) (1 + 2L)kx∞ - X∞k + 2√nB
μ
(37)	2L	γG
≤ (1 + ")o-7>
If We ignore the influence of μ and L, We finally achieve
Jim - X ∣∣χ(t) - x?k2 = -kχ∞ -χ*k2 =
t→∞ n	n
i=1
μ
2√nB
μ .
(38)
(39)
。(⅛+
Lower bound of limt→∞ 1 P2i ∣∣χ(t) - x?12. Now we show that the terms depending on B2 in
the upper bound (39) are necessary and cannot be removed by an improved analysis. In fact, there
exists strongly convex and smooth functions such that xi(t) generated by DAdam satisfies
-n
lim — V ∣∣x(t) - x?k2 ≥ B2.	(40)
t→∞ n	i
i=1
To prove the above conclusion, we consider minimizing problem of the form (1) with
fi(x) = 2(x - yi)2, ∀i ∈ [n]	(41)
where x, yi ∈ R. Apparently, the solution to problem (1) is given by x? = 1 PZi yi∙ Next we let
xi∞ be the limiting point of DAdam (Algorithm 2) in node i. With notations in Sec. B.2, we have
Vfi(χ∞) = χ∞ -y	(42)
Hi = IVfi(χ∞)∣ = ∣χ∞ - yi|	(43)
(H+ )-1 = diag{(Hi +)-1} ∈ Rn×n	(44)
Under the above setting, the limiting point of DAdam (30) reduces to
x∞ = Wx∞ -γ(H+I)-1Vf(x∞)	(45)
in which W ∈ Rn×n is the weight matrix. By left-multiplying 11T to both sides, we have
1 X	Vfi(X∞)
n i=1 IVfi(X∞X + e
n∞
n X ⅛-⅛=0.
(46)
On the other hand, it is easy to verify that
-n	-n
-X Vfi(χ∞) = - X(X∞
nn
i=1
i=1
-n
yi) = X — Eyi
n i=1
x∞ x?
x x .
(47)
With (46) and (47), we have
n
kx∞ - x?k 竺 k- X Vfi(X∞)k
i=1
(=) k- X IVffX∞x∞ +e - - X Vfi(X∞)k
n i=1 IVfi (Xi )I + e	n i=1
=k- X IVVfi∞∞[ - - X Vfi(X∞) + -X Vfi(X∞) - -X Vfi(X∞)k
n IVfi (Xi∞)I + e n	n	n
i=1
i=1
i=1
—
n
n
n
17
Under review as a conference paper at ICLR 2022
Algorithm 3 QG-DAdam (Lin et al., 2021)
Initialize x(0) arbitrarily; let β1,β2 ∈ (0,1); let m(0) = m(0) = v(0) = V(0) = 0; set Y properly;
For k = 0, 1, 2, ..., T - 1, every node i do
Sample g(t) = VF (χ(tk); ξ(t));
m(t) = βιm^ (t-1) + (1 — βι)g(t);
Vr)= β2V(tT) + (1 - β2)g(t) Θ g(t);
di(t) = (xi(t+1) - xi(t))/kxi(t+1) - xi(t)k2;
mIt) = βιm itT) + (1 - βι)d(t);
Vitt= β2V(t-1) + (1 - β2)d(t) Θ d(t);
(=) k1 XX Vfi'F ——1 XX Vfi(x∞) + 1 XT(x∞ - x∞)k
k n = ∣Vfi(x∞)∣ + e	n = fi(i ) + n =( i 川
=k n XX fx∞⅛ - n XX vfi(χ∞)k
i=1	i i	i=1
nn
(=)k- X(Hi + eI)-1Vfi(x∞) - - X Vfi(x∞)k = B.	(48)
n i=1	n i=1
Finally, we have
Jim 1 X ∣∣χ(t) - x?k2 = 1 kχ∞ -χ*k2
t→∞ n	n
i=1
=1 ∣∣x∞ - X∞ + X∞ - x?k2
n
(=)1 kχ∞ - χ∞k2 + 1 kχ∞ - x?k2
nn
≥ 1 kχ∞ - χ*∣2 = ∣x∞ - x?k2(=)B2,	(49)
n
where equality (a) holds because
hχ∞ - X∞, χ∞ - χ?i = (χ∞)t(I - 1 11t) ∙ 1(x∞ - x?) = 0.	(50)
n
Inequality (49) implies that the term B2 cannot be removed from bound (39) with any improved
analysis for our constructed problem.
B.4 QG-DAdam and adapt-then-communicate structure
The QG-DAdam algorithm (Lin et al., 2021) is listed in Algorithm 3.
B.5 Fixed point condition for QG-DAdam
We let xi(t) → xi∞ as t → ∞ for any i ∈ [n]. We also assume each node i can access the full-batch
gradient Vfi (x) in order to remove the influence of gradient noise. Under such setting, we can
verify in QG-DAdam that
d∞ = 0,	m∞ = 0,	v∞ = o
mi∞= (1-β1)Vfi(xi∞),	Vi∞ = (1-β2)Vfi(xi∞)ΘVfi(xi∞)
(51)
18
Under review as a conference paper at ICLR 2022
Substituting mi∞ and vi∞ to the highlighted step in QG-DAdam, we can achieve the fixed-point
condition as follows
x∞ = X Wij(x∞ - γ(1-βι)(√1 - β2Hj + eI)-1 Vfj(x∞)),	∀i ∈ H (52)
j ∈Ni
where Hi = diag{√Vfi(x∞) Θ Vfi(X∞)}. If We further let x∞ := n PZi x∞, the following
relation can be achieved by (52) and Assumption A.3:
x∞ = x∞ - γ(1 - β1) X(√1-β2Hi + eI)-1Vfi(x∞)
n
i=1
1 3,--------- , 一
=⇒ — E(√Γ-高 Hi + eI )-1Vfi(x∞) = 0
n i=1
1 n Z________
=⇒ — T(，1 一 β2Hi + eI)-1Vfi(x∞) = 0 (if x∞ =…=x∞ = x∞ in the limit). (53)
n i=1
The last relation is the one listed in (10).
B.6 Proof of Proposition 2
The proof of Proposition 2 is similar to that of Proposition 1. With notations defined in Sec. B.3, we
can rewrite (52) into the following compact manner:
x∞ = W(x∞ - γ(1 - βι)(√1 - β2H + eI)-1Vf(x∞))	(54)
Upper bound of ∣∣X∞ — x*k. Left-multiplying nIT to both sides of recursion (30), we achieve
1 3,---- , 一
-V(vzf-12 Hi + eI )-1Vfi (x∞ ) = 0.	(55)
n i=1
Using a similar argument to that in (32) and (34), we have
√nkx∞ - x?k ≤ 2√nB + 2Lkx∞ - X∞k	(56)
μ μ
in which the constant bias term B is defined as
nn
B := k- X(√--β2Hi + eI)-1Vfi(x∞) — — X Vfi(x∞)∣∣.	(57)
n i=1	n i=1
Upper bound of ∣∣x∞ — x∞∣∣. Subtracting x∞ from both sides of (54), we have
(I - W)(x∞ - x∞) = -γ(1 - βι)W(√1 - β2H + eI)-1Vf(x∞).	(58)
Using similar arguments as in (36) and (37), and facts that ∣∣(√1 - β2H + eI)-1k
maxi{k(√1 - β2Hi + eI)-1∣∣2} ≤ ɪ and ∣∣W∣ ≤ 1, we have
∣x∞ - X∞k ≤
γ(1- βι)∣Vf(x∞)∣
(1 - P)e
≤
(1 - P)e
(59)
Upper bound of limt→∞ n PZi l∣x(t) - x?k2. Using similar arguments to those in (38) and (39),
we achieve
1n
t→∞ n X kx(t)-x*k2
i=i
n kx∞ -x*k2=O( (K2GV+
(60)
Lower bound of limt→∞ ɪ Pn=Ikx(t) - x*k2. With the same constructed problem as in (41), we
can easily establish
1n
Iim - Ekxi)-xk2 ≥ B	(61)
t→∞ n
i=i
following the arguments in (42) 一 (50), where B is defined in (57).
19
Under review as a conference paper at ICLR 2022
B.7 GT-DADAM
Fixed-point condition. If We define H= 1 pn=1 Hi, the fixed-point condition of GT-DAdam (i.e.,
Algorithm 2 in (Chen et al., 2021a)) is as follows:
x∞ = X Wijx∞ — γ(H + U)-1Vfi(x∞),	∀i ∈ [n].	(62)
j∈Ni
n
=⇒ x∞ = x∞ - Y X(H + eI)-1Vfi(x∞)
n
i=1
1n
n	- VVfi(x∞) = 0 (If x∞ =…=x∞ = x∞ in the limit)	(63)
n i=1
It is observed that the fixed-point of GT-DAdam satisfies the optimality condition of problem (1).
Proposition 4 Under the same conditions as in Proposition 1, the distance between GT-DAdam
fixed point and the global solution x? can be characterized as follows:
㈣ X kx(t)-x*k2 = o( ⅛2 )	(64)
Proof. With notations defined in Sec. B.3, the fixed-point condition in (62) can be rewritten as
x∞ = W x∞ — Y(HH + eI )-1Vf (x∞)
where IH = diag{H,…，H} ∈ Rnd×nd.
Bound of kx∞ - x*∣∣. Left-multiplying 11T to both sides of recursion (65), we achieve
1n
-EVfi(X∞)=0.
n i=1
With the above fact, We have
nn
kx∞ — x?k = kx∞ —x* — Y(— X Vfi(x∞) - - X Vfi(X?))k
n i=1	n i=1
nn
≤kx∞ - x? - Y (- X Vfi(X∞) - - X Vfi(X?)) k
i=1	i=1
nn
+ Y k - X Vfi(X∞)- - X Vfi(X∞)k
i=1	i=1
≤ (i—γμ )kX∞—X*k+√- kχ∞—χ ∞k
(65)
(66)
(67)
where the last inequality holds since each fi(x) is μ-strongly convex and L-Smooth. We thus have
—	2 L
√nkx∞-x*k≤ 不 kx∞ — x∞k
Bound of kx∞ — x∞k. Subtracting x∞ from both sides of (65), we have
(I — W )(x∞ — x∞) = -γ(HH + EI )-1Vf (x∞).
With inequality (43) in (Yuan et al., 2021), we have
k(I — W)(x∞ — x∞)k≥ (1-ρ)kx∞ — x∞k
With (69) and (70), and noting that k (H + EI)-11∣2 ≤ 1/g we achieve
∣∣χ∞	Q||<YkVf (x∞)k
kx - X k≤	(1-ρ)E
(68)
(69)
(70)
(71)
20
Under review as a conference paper at ICLR 2022
Proof of Proposition 4. With (68) and (71), we have
kx∞ — x?k ≤ kx∞ — X∞k + √n∣∣x∞ — x?k
2L
≤ (1 + -)kx∞-x∞k
μ
≤ (1 + 2L)广〒kVf(x∞)k.	(72)
μ (I — P)E
The above inequality implies that
t→∞1 Xx H)-x*k2=O( (κ2‰).	⑪
Remark 3 Different from Propositions 1 and 2, it is observed from Proposition 4 that the distance
between the GT-DAdam fixed point and the optimal solution will diminish to zero as learning rate
γ→ 0. However, GT-DAdam will incur two rounds of neighborhood communication per iteration,
which is much more expensive than DAdam and QG-DAdam.
C Fixed-point condition for DAG-Adam
C.1 Fixed-point condition
When t → ∞, we let xi(t) → xi∞ for any i ∈ [n] in DAG-Adam. We further assume each agent can
access the real gradient Vfi(x) instead of the stochastic gradient VF(x; ξi). Under this setting, We
can verify that
g∞ = Vfi(x∞) + ν(x∞ — X wijx∞)	(74)
γ	j∈Ni
and m∞ = g∞, v∞ = g∞ Θ g∞. By letting Hi = diag{pv∞}, it can be derived from the adptive
step in DAG-Adam that
x∞ = x∞ -Y(Hi + EI)-1g∞, ∀i ∈ [n]	(75)
0	Vfi(x∞) + ν(x∞ — X wjx∞)=0,	∀i ∈ [n]	(76)
γ	j∈Ni
1n
=⇒	— EVfi(X∞) = 0	(77)
n
i=1
1n
^⇒	- VVfi(x∞) = 0 (If x∞ = x∞ in the limit)	(78)
ni
i=1
In other words, DAG-Adam can converge to the stationary solution if χ∞ = •… = χ∞ = χ∞ in
the limit. Compared to DAdam and QG-DAdam in Which the discrepancy betWeen Hi ’s has made
the limiting fixed point deviate from the desired solution, the communicate-then-adapt structure in
DAG-Adam make it immune to the difference between Hi .
C.2 Proof of Proposition 3.
With notations defined in Sec. B.3, the fixed-point condition in (76) can be rewritten as
Vf (x∞) + V (I — W )x∞ =0.
γ
Bound of kx∞ — x?k. Left-multiplying n IT to both sides of recursion (79), we achieve
1n
-EVfi(X∞)=0.
n
i=1
(79)
(80)
21
Under review as a conference paper at ICLR 2022
Table 5: Comparison between different decentralized adaptive methods in the limiting point distance to the
global solution x? when cost function in problem (1) is strongly convex. The learning rate γ is set as constant,
and the full-batch gradient Vfi(X) can be assessed per-iteration.
Algorithm	Distance to global solution
DAdam (Nazari et al., 2019)	YG + B2 (1-ρ)2e2 + B
QG-DAdam (Lin et al., 2021)	Y2G2	+ B2 (1-ρ)2e2 + B
GT-DAdam (Chen et al., 2021a)	Y2G2 (1-ρ)2e2
DAG-Adam (ours; V = 1)	Y2G2 (1-P)2
Using similar arguments to (67), we achieve
_	2 L
√n∣x∞ - x?k ≤ — kχ∞ - χ∞k
μ
Bound of ∣x∞ 一 X∞∣. Subtracting X∞ from both sides of (79), we have
(I - W)(x∞ - X∞) = - Y Vf (x∞).
V
With inequality (43) in (Yuan et al., 2021), we have
k(I-W)(x∞ - x∞)k ≥ (1-ρ)kx∞ - x∞k
With (82) and (83), we achieve
(81)
(82)
(83)
kx∞-x∞k≤ fl
Proof of Proposition 3. With (81) and (84), we have
(84)
kx∞ - x?k
∣x∞ - χ∞∣ + √n∣x∞ — x?k
2L
(1 + 一 )kx∞-X ∞k
μ
(1 + 2L)方—kyf (x∞)k.
μ (1 — P)V
(85)
The above inequality implies that
lim
t→∞ n
1n
n X kχ(t) - x?k
i=1
2 = O( (⅛
(86)
≤
≤
≤
n
C.3 Comparison in limiting point distances to global solution
Table 5 lists the results We establish in Propositions 1-4. When V = 1 in the DAG-DAdam method,
which is a valid choice for convex settings in empirical studies, the comparison between different
decentralized adaptive methods in the limiting point distance to the global solution x? can be listed
as folloWs:
DAG-Adam < GT-DAdam < QG-DAdam ≈ DAdam
This conclusion matches with the numerical experiment in Fig. 1.
(87)
C.4 Experiment setting for logistic regression (i.e., Fig 1)
In this experiment, We consider the binary logistic regression problem:
1n	λ	1K
mind nEfi(x) + 2l∣xk2 where fi(X) = K Eln(I + exp(-Yi,khTkx))	(88)
x∈	i=1	k=1
22
Under review as a conference paper at ICLR 2022
where hi,k ∈ Rd is the k-th feature vector at agent i and γi,k ∈ {+1, -1} is the corresponding
label. In the experiment, we set λ = 0.25, n = 16, and all computing nodes are organized into the
(undirected) ring topology. The weighted matrix is simply generated through the average rule so that
is satifies Assumption A.3. Also, we choose d = 25 as the dimension of feature vector and generate
K = 50 data for each agent locally. The corresponding γ is generated through a pre-set random xo
vector. We first compute the linear score hiT,kxo, then determine the γ by the probability based on
the sigmoid value. For all algorithms, we choose the constant learning rate γ = 0.01, β1 = 0.9,
β2 = 0.99. One exception is that we choose β1 = 0.1 for QG-DAdam algorithm to achieve better
convergence performance.
Besides the learning curve we have shown in Fig. 1, we also plots the vi(t) of various algorithms at
the last iteration in Fig. 5. To characterize the difference between different agents, we use the box
plot to show the difference of vi(t) of each entry among all agents. From the figure, it is clear that the
vi(t) in DAG-Adam algorithm is almost zero when the algorithm converged, which is much narrower
and lower than others methods.
method
Figure 5: The boxplot of the first 9 entries of gradient square term vi(t) at the end of iteration of various
algorithms. The box represents the mean, quartile, and the min-max value of that entries cross all agents.
Fig. 6 provides complementary experimental results to Fig. 1. We added a new algorithm named
DAdam-ReParam which reparameterizes DAdam to obtain a communicate-then-adapt structure. In
addition, the convergence with both constant and decaying learning rates are depicted in the left
and right plot in Fig. 6, respectively. The It is observed that DAdam-ReParam converges closely to
DAdam, and both approaches converge to a less accurate solution than DAG-Adam.
Figure 6: Convergence comparison between various algorithms for a full-batch logistic regression problem.
The y-axis indicates the excess risk, i.e., n Pn=I E(f (Xit)) — f?). Left plot: convergence With constant
learning rate. Right plot: convergence with stair-wise decaying learning rate.
23
Under review as a conference paper at ICLR 2022
D MORE DISCUSSION ON THE HYPER-PARAMETER ν
This section discusses the role of hyper-parameter ν in DAG-Adam, and how it influences the con-
vergence performance. To this end, we first discuss whether ν is needed in the vanilla decentralized
SGD (or DSGD for short) algorithm and DAdam. Next, we will study the influence of ν in the more
complicated DAG-Adam algorithm.
D.1 INFLUENCE OF ν ON DSGD
Without loss of generality, we consider the DSGD method with a full-batch gradient. In this setting,
the augmented gradient of DSGD (13) is given by
x(t+1) = x(t) - γ( Vfi(x(t)) + I(XT)- X WijXjt)) ),	∀i ∈ [n].
γ	j∈Ni
、-----------------{z----------------}
augmented gradient
(89)
With notations defined in Appendix A and the introduction of the hyper-parameter ν as in DAG-
Adam, we can rewrite the above recursion into a more compact form
x(t+1) = X⑴—Y(Vf(X⑴)+ V(I - W)x㈤)
= (1-ν)I+νW x(t) -γVf(x(t))	(90)
}
:=Wf
The above recursion implies that the hyper-parameter ν will replace the original weight matrix W
with Wf, whose eigenvalues are derived as
,r~∙,	.	.	. .,	,r~∙,	,	,
eigkf) = (1- V) • 1 + V ∙ eigk (W)	^⇒	1 - eig® (W) = V (1-eig®(W))	(91)
where eigk (W) stands for the k-th eigenvalue of matrix W. Equation (91) implies all eigenvalues
of W are more closer to value 1 for any V ∈ [0, 1). Since a larger spectral gap, i.e. 1 - λ2(W), will
reduce the mixing time of gossip average and hence speed up the DSGD convergence, the optimal
choice for DSGD is V = 1; other V values will not improve the convergence of the vanilla DSGD.
We validate the above discussion with deep learning experiments over CIFAR-10 dataset. Fig. 7
illustrates the influence of V on vanilla DSGD. It is observed that V = 1 enables DSGD to converge
fastest and achieve the best validation accuracy, which is consistent with the above conclusions.
O 20	40	60	80
Epochs
Epochs
Figure 7: Loss and accuracy evolution for CIFAR-10 dataset using DSGD(V) under a homogeneous
network of size 8. Left plot: training loss. Right plot: validation accuracy.
D.2 INFLUENCE OF V ON DADAM
Since DAdam is an immediate variant of DSGD, we expect the influence of V on DAdam will be
similar to that on DSGD. To validate it, we impose a hyper-parameter V to DAdam recursion:
g(t) =Vf(X(t))
(92)
24
Under review as a conference paper at ICLR 2022
m(t) =β1m(t-1) + (1 - β1)g(t)	(93)
v(t) =β2v(t-1) + (1 - β2)(g(t)	g(t))	(94)
x(t+1) =((1 - ν)I + νW)x(t) - γ(H(t) + I)-1m(t)	(95)
SSon uδ4L
6 × IOT
4 × IoT
3 × lɑ-ɪ
2 XlO0
Fig. 8 illustrates the influence of ν on DAdam. Similar to DSGD, it is observed that DAdam with
ν = 1 (i.e., the vanilla DAdam with no introduction of ν) has the best convergence performance.
00
1
0	20	40	60	80
Epochs
y o.6
<
I 0.5
e
Q
W 0.4
0.3
0.2
0.9
0.8
20
40	60	80
Epochs
Figure 8: Loss and accuracy evolution for CIFAR-10 dataset using DAdam(ν) under a homogeneous
network of size 8. Left plot: training loss. Right plot: validation accuracy.
D.3 INFLUENCE OF ν ON DAG-ADAM
Without loss of generality, we consider DAG-Adam with dimension d = 1. In this scenario, DAG-
Adam can be rewritten as
g(t) = Vf (X㈤)+ V(I - W)x㈤	(96)
γ
m(t) = β1m(t-1) + (1 - β1)g(t)	(97)
v(t) = β2v(t-1) + (1 - β2)(g(t)	g(t))	(98)
x(t+1) = x(t) - γ(H(t) + I)-1m(t)	(99)
where W ∈ Rn×n is the weight matrix and H(t) = diag{ʌ/v⑴} ∈ Rn×n is a diagonal matrix.
Subtracting (99) by β1 times (99) with one iteration shift, we have
x(t+1) -	β1x(t)	=	x(t)	-	β1x(t-1) -	γ(H(t)	+	I)-1m(t)	+ β1γ(H(t-1) +	I)-1m(t-1)	(100)
Rearranging the terms in the above recursion, we obtain
x(t+1) =x(t) - (1 - β1)γ(H(t) + )-1g(t) + β1(x(t) -x(t-1))
+ β1 γ(H(t-1) + )-1 - γ(H(t) + )-1m(t-1)	(101)
In the following, we will first study a simple example that can shed lights on the influence of ν on
DAG-Adam, and next we will discuss the more generalized scenario.
An illustrating example. We consider a trivial scenario in which H(t) = αI, which amounts to
the case where all nodes maintain constant vi(t) across i and t in the local buffer. As we have shown
in the convex experiment in Fig. 5, this trivial scenario becomes valid in the asymptotic stage of
DAG-Adam in which vi(t) converges to the same value v for any i ∈ [n].
Substituting H(t) = αI into (101), we have
x(t+1) = X⑴-γ(1■-β1)g(t) + βι(x(t) - x(t-1))
α+
[0-ν(α⅛)I+ν(α⅛ W ]x(t) -
X-------{------}
:=WDAG
γ(1 -β1) Vf(X⑴)+ βι(x⑴-XCT))
α+
(102)
25
Under review as a conference paper at ICLR 2022
L .1	「777	.. ∙	1	1 .1 ..1	」	1	1	。	• •	•	• •
From the expression of WDAG , it is observed that the optimal value for ν to minimize the mixing
time of the weight matrix is 1-+^, which is not 1 as in the vanilla DSGD algorithm. In addition, it
is worth noting that WDAG cannot outperform W in terms of mixing time for any choice of ν .
This trivial example illustrates the necessity to have the hyper-parameter ν in DAG-Adam. The
introduction of ν is to compensate the negative influence of the communicate-then-adapt structure
in DAG-Adam which incorporates both the gradient and the consensus constraint into the adaptive
momentum buffer. The weight matrix WDAG can be quite inefficient to aggregate information if no
ν exists to speed up partial averaging.
Generalized scenario. Unlike the above trivial scenario, it is very challenging to determine a proper
value for ν in the general scenarios. To see it, we ignore the last term in (101) to achieve
x(t+1) ≈ (I - V(1 - βι)(H(t) + eI)-1(I - W)) x(t) - (1 - βι)γ(H(t) + eI)-1Vf(x(t))
|
{^^^―
:=WDAG
+ β1 (x(t) - x(t-1))
}
(103)
which is a reasonable approximation to (101) since β2 is typically set as 0.999 and v(t) (and hence
H(t)) will not change dramatically. The first term in (103) mixes xi(t) with weight matrix
WfDAG = (I-ν(1 - β1)(H(t) + eI)-1)I + ν(1 -β1)(H(t)+eI)-1W	(104)
Ifwe let Λ := ν(1 - β1)(H(t) + e)-1, it holds that
A-1/2fdagA1/2 = I - Λ + A1∕2WA1∕2	(105)
El 1 …	1	∙ 1	一…、∙ .ι ∙	.1	..	.	..	。ττ	.ι	1	r
The left-hand side of (105) is the similarity transformation of WDAG . Hence, the eigenvalues of
WDAG equal to ones of the right-hand side of (105). Unfortunately, it is difficult to determine the
eigenvalues of A1/2WA1/2, not to mention the optimal value for ν. However, it is obvious that
ν = 1 is not necessarily optimal for DAG-Adam. This motivates us to tune ν in experiments to
boost the convergence performance of DAG-Adam. In fact, the results in Table 1 (with CIFAR-10
dataset), Table 3 (with ImageNet dataset), and Table 4 (with SQuAD dataset) corroborate that ν = 1
is not the optimal choice for DAG-Adam. How to find the optimal ν automatically is the main
direction of our future work.
E Convergence Analysis
E.1 Convergence proof of DAG-Adam
As we motivated in the Section 3, we consider the augmented loss function
K
L(X) = f(x) +，xk2-w, where f(x) ：= X fk(xk)
2γ	k=1
It is easy to verify that
(106)
VxL(x)
「Vχf1(X1)]
Vχ2 f2(x2)
.
.
.
VxKfK(xK)
+ V(I - W)x = Vf(x) + V(I - W)x
γγ
(107)
which is exactly the same format as our decentralized augmented gradient in equation 20.
Proof idea. Therefore, we can view our DAG-Adam over multiple agents as the standard Adam
for virtual single agent with a long stacked vector. This observation leads to the following conver-
gence analysis idea. First, we use the similar proof technique as Zaheer et al. (2018) did for the
standard Adam to show the convergence of the decentralized augmented gradient. Next, based on
that, we establish the consensus lemma to show that the models between all agents are close. Last,
26
Under review as a conference paper at ICLR 2022
combining two, We finally arrive the convergence of centralized iterate, kVf (x(t) )k as the common
decentralized algorithm theorem stated.
It is common in the proof of adaptive gradient methods, for example Zaheer et al. (2018); Chen
et al. (2021a; 2018b); Reddi et al. (2019; 2020), to assume the gradientVfi(xi(t)) and/or stochastic
gradient VFi (x(t); ξi) have bounded '2 or '∞ norms at all iterations t, i.e., kVfi(χ(t)))k ≤ G and
kVFi(xi(t); ξi)k ≤ G. Hence, it is nature to see we need the similar bounded gradient assumption
on the augmented gradient. However, notice our augmented gradient has a (I - W)x term. So we
need to further assume that kx(t) k ≤ C. This is a relatively strong assumption, but it can be strictly
achieved by simply taking a projection step. Also, in the practice, we always observe that the terms
g(t) converges to a very small value without any requirement on the boundary on the iterate, as
shown in Fig. 5. In this case, we can show that
kg(t)k∞ ≤ max kV£(x(t))k + kV(I -W)x㈤k ≤ G + Vkx⑴k ≤ G + KC (108)
i	Y	Y	'---{z--}
,G0
Same as the paper Zaheer et al. (2018) did, we show the case that there is no-momentum for the sake
of simplicity, i.e., β1 = 0, which is typically referred as RMSprop.
Lemma 1 (Convergence lemma of augmented gradient) Suppose the parameters Y, V and β2
satisfy the conditions that Y ≤ ^L, V < min{e∕2, κγ}, (K is any positive constant) and
2
1 — β2 ≤ 16(.+.0)2 ∙ Under the Assumption A.1 - A.4 and bounded augmented gradient, the
augmented gradient generated using DAG-Adam has the following bound
TX EkVf(X⑴)+ V(I- W)x(t)k2 ≤ O(L(X(O)T- CL ) + O(σ2)	(109)
T t=0	Y	YT
where L is the minimum value ofthe augmented loss function (106).
Proof. To shorten the notation, we denote the stochastic augmented gradient as
Vχb(x)，Vf(x(t)) + V(I - W)x(t)	(110)
Y
Utilizing the L-smooth property and noting that the augmented loss function L has a new Lipschitz
constant L0 = L + Y, we have
0
L(X(t+1)) ≤L(x⑴)+ (VL(x⑴),x(t+1)- X⑴i + L∣∣x(t+1) - X㈤『
≤L(x⑴)-YDVxL(X), (H⑴ + e)T(Vf(x(t)) + V(I - W)x⑴)〉
+ 邙k(H(t) + e)-1(Vb(x(t)) + V(I - W)x㈤)∣2
2Y
=L(X⑴)-YDVxL(X), (Pβ2H(T) + e)-1VxL(x)E
+ YDVxL(x), ((Pβ2H(I) + e)-1 - (H㈤ + e)-1)Vx£(x))
+ U k(H(t) + e)-1VxL(x)k2	(111)
where the last equality we add and subtract the inner product with (√β2H(t-1) + e)-1L(x). The
main reason to do this is observing that H(t) is correlated with the stochastic gradient VLf(x).
Taking the conditional expectation Et of L(x(t+1)), more precisely, taking the expectation given all
previous recursion information {x(j), v(j)}tj=1, we obtain
EtL(X(t+1)) ≤L(x⑴)-YkVxL(x)k2√β2H(t-i)+e)-ι
+ YEtDVxL(x), ((Pβ2H(T) + e)-1 - (H⑴ + e)T)VxL(x))
27
Under review as a conference paper at ICLR 2022
+ * Etk(H ⑴+ e)TVχE(x)∣∣2	(112)
Here we can remove the gradient noise by conditional expectation since the weighted diagonal
matrix √β2H(t-1) is deterministic given the all information before iteration t and the zero gradient-
noise assumption A.2.
To bound the term E t||(H(t)+ E)TVXL(x)k2, we notice that any diagonal element of weighting
matrix satisfies that
(G + KC + e)-1 ≤ [(H㈤ + e)T]i,i ≤ 1/e, Vi	(113)
Apply the inequality (25), we establish that
Etk(H⑴ + e)-1Vχ2(x)k2 ≤ɪEtkVXf(x)∣∣2H(t)+e)-ι
1
≤ E EtkVXL(x)k2√β2H(t-i)+e)-ι	(114)
where the second inequality is because that √β2H(t-I) ≤ H(t). Next, we bound the cross terms:
EtDVXL(x), ((√β2H(I) + e)-1 - (H⑴ + e)-1)VXE(X)E
= DVXL(x),Et((√β⅛(I) + e)-1 - (H⑴ + ^尸再乂无㈤〉
≤kVXL(x)k∞kEt((√βZH(I) + e)-i - (H㈤ + e)-1 )Vx£(x)||i	(115)
where the last inequality is Holder,s inequality. Using the same methods as in (Zaheer et al., 2018,
Appendix A), we can show that
∣∣Et((√β⅛(τ) + e)-1 - (H(t) + e)-1)Vχ2(x)kι ≤ √1≡2EtkVX£(切2无m-1»)-1
6	(116)
Plugging back, we establish
EtDVXL(x), ((√^H(tτ) + e)-i - (H(t) + e)T)Vχ2(x))
≤ G0√1≡2EtkVXE(x)k2√sH(t-ι) + e)-ι	(117)
Substituting (114) and (117) into (112), we obtain
E tL(x(t+1) ) ≤L(x(t)) - Y kVχL(x)k2√β2H(-ι)+e)-ι
+ ( 7 1 -的 G，+ Y^~) E tkVxLb(x)k2√e2H(t-i)+e)-i	(118)
Next, we bound the weighted norm of stochastic noisy gradient:
EtkVX£(X)II2√β2H(t-ι)+e)-ι =kVXC(X)k2√β^H(t-ι)+e)-ι + EtkVf(X(t)) -Vf (Xo)) k 2√β^H(t-ι)+e)τ
≤kVχL(x)k2√sH(t-i)+e)-ι + ɪEtkVf(x(t)) - Vf(x(t))k2
1
= kVχC(x)k2√sH(t-ι)+e)-ι + E EEtkVfi(x(t)) - Vfi(X(t))k2
i=1
WVx£(x)k27e2H(tT)+e)T + 1nσ2	(119)
where the first equality is because of independent and zero-mean gradient noise assumption and the
first inequality exploited the equation 25. Plugging back and rearranging the terms, we get
Y (1 - S ：，2G - Y-E-) EkVL(X(t))k2√β2H(t-ι)+e)T	(120)
28
Under review as a conference paper at ICLR 2022
(121)
(122)
≤ E L(X⑴)-E L(X-I)) + ( γ√1 % β2G0 + γ2L0) nσ2
2	22
To make the coefficient in the left-hand side of inequality positive, we require that
YL 1	e — 2ν
^2Γ ≤ 4	=⇒ γ ≤	2L
√1-β2 G	1	1 C	e2
e ≤ 4 =⇒	- β2 ≤ 16(G + KC)2
Hence, we conclude that by taking the telescoping sum:
1 T-1
T X EkVf(x(t)) + V(I - W)x(t)k2
T t=0	γ
≤ 2( G + e) (L≡- % (「+ 六)nσ2)
where We use the fact that L* ≤ L(x) for any x.
Above lemma shows that that local augmented gradient will converge to the stationary point within
the constant factor of O(σ2) for constant learning rate γ, which is the same as stated in Theorem 1
of Zaheer et al. (2018). Next, we evaluate that how close the model between each agent.
Lemma 2 (Convergence of consensus value) Under the same condition as Lemma 1, it holds that
T-1	2
1 XEkx(t) -x(t)k2 = O(n ∖2 2 )	(124)
T t=0	(1 - ρ)2ν2
where the ρ is the second largest eigenvalue of combination matrix W in magnitude, which is strictly
less than 1.
(123)
Proof: Since W is a symmetric doubly stochastic matrix, there is an eigenvalue decomposition form
W = QΛQT , where Q is some orthogonal matrix. The diagonal matrix Λ has the value within the
range (0, 1] and contains one and only one entry that is 1 due to Assumption A.3. Therefore, we can
establish that
k(I - W)xk2 = k(I - W)(x - x)k2 =k(Q(I - Λ)Qt ③ I)(x - x)k2 ≥ (1-ρ)2kx - x∣∣2
(125)
where the first equality holds because (I - W)1 = 0. Now, we are read to establish the bound for
the consensus error from the augmented gradient:
T-1	2 T-1	2
⅛ X kx(t) - x(t)k2 ≤ ⅛T XB V(I-W )x(11
t=0	ρ	t=0	γ
≤22V	XJ]vf(x) + V(I - W)x(t)B2 +	2寸	X" ∣∣Vf(x)B2
(1 - ρ)2ν2T	γ	(1 - ρ)2ν2T
(126)
where the last inequality utilizes the Jensen’s inequality kak2 = ka+b+(-b)k2 ≤ 2ka+bk2 +2kbk2
and by setting a = Y(I - W)x(t) and b = Vf(x(t)). Using the conclusion from Lemma 1 and
bounded gradient assumption, we can immediately establish that
T X1 kx(t) - x(t)k2
T t=0
4V2 (Ve2G0 +e)	L(X(O)) - L*	√1 - β2GO	VLO	2	2
≤	(1-ρ)2 V 2T (—VT — +(—e2 — +2l2) nσ + nG)	(127)
which gives the desired bound that the consensus error has the order of O ((二：”).
29
Under review as a conference paper at ICLR 2022
This bounds for the consensus error has several easy interpretable conclusions: To have the smaller
consensus error, we can either 1) use a better connectivity of the graph (i.e., ρ is closer to 0), or 2) set
a smaller learning rate γ, or 3) choose a larger ν. More accurately, the consensus error is determined
by the ratio between γ and ν, which is straight-forward to understand since the communication term
(I - W)x is scaled by ν/γ.
With above two lemma, we are ready to present the convergence theorem.
Proof of theorem 1. The first half of statement has already been shown in the lemma 1. We just to
show the convergence for the centralized iterate. To connect the gradient over the centralized iterate
X with the stacked gradient over the stacked iterate x, We have
kVf(X ㈤)k2
2
n	n
=)1 x Vfi(X⑴)+i x V (χ(t)- x Wijxjt))
nn
i=1	i=1	j ∈Ni
(b)	1 n
≤ n X
i=1
2
Vfi(χ(t)) + V(Xit)- X WijXjt))
γ	j∈Ni
1
=—
n
(c)	2
≤ —
n
(d)	2
≤ —
n
Vf (X㈤)+ V(I -W )x⑴
γ
Vf (X㈤)+ V (I -W )x⑴
γ
Vf (X㈤)+ V (I -W )x⑴
γ
(128)
where step (a) follows the fact that W = [Wij] is a doubly stochastic matrix and step (b) and (c) are
due to Jensen’s inequality and step(d) utilized the Lipschitz property.
Therefor, combining with the conclusions in lemma 1 and lemma 2, we have
1T
T X kVf (X(t))k2
t=1
T
≤ 2- X Vf (X㈤)+ V (I -W )x⑴
nT	γ
f— 1	'
t=1
Y X *-x( 叩
t=1
≤4(pβGo +。( JC
γnT
(√ 1 - β2 G0
+ 8L2γ2(VβG0 + €)( L(X⑼)-L + ( √1 - β2G0 +
(1 — ρ)2V2	1 γnT 卜	e2
=4( √A G0 + e) 1 +
2L2γ2 ) (L(X⑼)-L
(1 — ρ)2V2 ) [ γnT +
8L2γ2(√β^G0 + e) g2
+ —(1 - ρ)2V2一
=。(' O+ O(σ2) + O( I)
(V1 - β2 GO
k	e2
(129)
which gives us the desired result.
30
Under review as a conference paper at ICLR 2022
E.2 Convergence proof sketch for the variation of DAG-Adam algorithm
Similar as adapt-while-communicate versus adapt-then-communicate format, our proposed DAG-
Adam algorithm also can have another style:
g(t) =WVb(X⑴)+ V (I - W)x(t)	(130)
γ
m(t) =β1m(t-1) + (1 - β1)g(t)	(131)
v(t) =β2v(t-1) +(1 - β2)g(t)	g(t)	(132)
X(t+1) =X(t) - γ(H(t) + )-1m(t)	(133)
Note there are two W in the augmented gradient step, but we only need one round of communication
in practice. To see that, we can re-write into this form
g⑴=VX⑴-Vw (X㈤-Y Vb(X㈤))	(134)
γγ	ν
That is, instead of communicating the parameters directly, we communicate the parameters with one
SGD step forward.
To establish the convergence analysis of this algorithm, we need to find the gradient (134) as a
standard gradient of another augmented loss function. As long as as we find that loss function, the
rest of proof will be similar as we did in Theorem 1. Fortunately, it has been stated in Yuan et al.
(2021) paper for the momentum algorithms. Here, we give a modification for the Adam algorithm.
First, we need to further assume that W is a positive definite matrix. It is easy to achieve that
since if the original W is not, We can use W = (I + W)/2 instead. Suppose W has eigenvalue
decomposition form W = QT ΛQ, where Q is an orthogonal matrix. Then, we define Wα =
QTΛαQ, which is also positive definite matrix for any α > 0. So we have W = W1W2. Now we
introduce the algorithm into a transformed domain by multiplying W- 2 on the both sides:
W-2g⑴=W1 Vb(X⑴)+ V(I - W)W-1X㈤	(135)
γ
W-1 m⑴=W-2β1m(t-1) + (1 - β1)W-1 q⑴	(136)
v(t+1) =β2v(t) + (1 - β2)g(t)	g(t)	(137)
W-1 X(t+1) =W-1X⑴-YW-2 (H⑴ + e)-1W2 W-1 m⑴	(138)
We further introduce
S㈤，W-2X⑴,q⑴，W-1 g⑴,P㈤，W-1 m(t+1), U㈤，W-1 diag(v⑴)W2	(139)
The algorithm (130)-(133)is equivalently to
q㈤=W1 V(,)b(W1S㈤)+ V(I - W)s㈤	(140)
P(t) =β1P(t-1) + (1 -β1)q(t)	(141)
U㈤=β2U(t-1) + (1 - β2)W-1 diag[(W1 q⑴)]W2	(142)
S(t+1) =S(t) - Y(U(t) + )-1P(t)	(143)
where the notation V(.)f (∙) means taking the gradient with respect to the whole argument of f and
note that W-2 (H(t) + E)TW2 = (W1 H(t)W-1 + E)	. Remember we can always retrieve
X(t) by X(t) = W2 S⑴.Now, we are prepared for introduce this new augmented loss function
Lw(S) =f (W 2 S) + ʒ-ks∣∣2-w	(144)
2Y	-
It is not hard to verify that
VsLW(s) = W2 V(∙)f(W2Sm) + V(I - W)S(t)	(145)
31
Under review as a conference paper at ICLR 2022
This means that (140) - (143) can be viewed as Adam algorithm applied on this augmented loss
function LW (s). There is one difference that U(t) is no longer diagonal matrix. However, recall
that we mainly use it in weighted norm, and the useful inequality (25) can be extended for arbitrary
positive matrix
akxkTA ≤ kAxk2 ≤ bkxkTA	(146)
where a and b are the smallest and largest eigenvalue of matrix A. To see that, suppose A = QT ΛQ,
we have
kAxk2 = kQTΛQxk2 = MQxk2 ≤ bM1/2Qx『=bkxkA	(147)
Therefore, we can establish the similar conclusion as Theorem 1.
F	Heterogeneous data distribution generation
This subsection provides the detailed heterogeneous data generation for CIFAR-10 training dataset.
To simulate the heterogeneous data, the disjoint non-i.i.d. training data over agents are generated
through the Dirichlet distribution. For each data class, a categorical vector q = (qι,…，qn), where
Pi qi = 1, qi ≥ 0, is used to illustrate the training data distribution in a network of size n. It is
drawn from a symmetric Dirichlet distribution Dir(α), where α stands for the degree of the data
heterogeneity (Lin et al., 2021; Yurochkin et al., 2019). The training data for a particular class tends
to concentrate in a single node as α → 0, i.e. becoming more heterogenous while the homogeneous
data is formed as α → ∞. After the sampling procedure, the data is reallocated to ensure the same
amount of training data in each network node.
Fig. 9	visualizes the heterogeneous data distribution for the CIFAR-10 experiments shown in Table
1 for both network size (4 and 8). The radius of each circle stands for the proportion of the samples
for the corresponding data class in the node. Clearly, as α → 0, the data for each class tends to be
allocated in a single node. When α → ∞, all the data is distributed evenly.
Fig. 10	illustrates the training loss and validation accuracy curves of different algorithms when α =
10. It is observed in the left plot that PAdam and DAG-Adam converges with much lower training
losses and higher validation accuracy than the others due to their insensitivity to data heterogeneity.
G	More experimental setting and BERT Experimental Results
More Implementation details. We also follow DDP’s design to enable computation and commu-
nication overlap. Each server contains 8 V100 GPUs in our cluster and is treated as one node. The
inter-node network fabrics are 25 Gbps TCP as default, which is a common distributed training
platform setting. The exponential topologies (Assran et al. (2019)) are utilized as default.
Training Loss. Fig. 11 shows the iteration-wise training loss curves of aforementioned algorithms
separately.
The effect of using dynamic ν. Besides with fine-tuning a proper ν constant for DAG-Adam, we
tried to use a dynamic ν scheduler to validate its performance. Compared to the common learning
rate scheduler, we use dynamic ν schedulers that provide a relatively small ν value at first and
grow to 1 at the end of training. Both linear schedulers and cosine schedulers were examined.
Similar as learning rate scheduler, we also use a warm-up strategy, i.e., to fix a static ν for a certain
iterations then use dynamic ν schedulers afterwards. We provide some preliminary results in Table 6.
Unfortunately, it did not provide better performance compared with the result of using constant ν.
We leave it as future works to design a better strategy to select ν adaptively without introducing
more hyper-parameters.
32
Under review as a conference paper at ICLR 2022
Network size 4, α= 10
Network size 8, α= 10
Data Class	Data Class	Data Class
Automobile
Frog
Dog
Deer
Cat
Ship
Truck
Airplane -
0	12	3
Node
Network size 4, α = 1
Truck
Frog
Dog
Deer
Cat
Automobile
Bird
Horse
sse-ɔ

Truck
Frog
Dog
Deer
Cat
Bird
Automobile
Airplane-
01234567
Node
Network size 8, α = 1
Airplane
0	12
Node
Network size 4f
Truck
Ship
Horse
Frog
Dog
Deer
Cat
Bird
Automobile
Airplane
0	12
Node
Airplane-
Automobile
Bird
Horse
g
Do
g
r0
F
Ca
Dee
3
α = 0.1
3
sse-ɔsBCJ
01234567
Node
Network size 8, α= 0.1
Truck
Bird
Automobile
Ship
Horse
g g
^0O
FD
Γ t
e a
Dec
Airplane-
01234567
Node
Figure 9: Heterogeneous data distribution visualization for CIFAR-10 dataset.
33
Under review as a conference paper at ICLR 2022
Epochs
Epochs
Figure 10: Training loss and validation accuracy evolution for CIFAR-10 dataset under a heteroge-
neous network of size 8 with α = 10. Left plot: training loss. Right plot: validation accuracy.
Figure 11: Convergence results on the BERT fine-tuning (SQuAD) task with different models and
topologies.
Table 6: The effect of using dynamic ν (BERT base with static topology).
Scheduler	warm-up ratio	Exact Match	F1
Linear	0	46.68	58.64
Cosine	0	65.28	75.99
Linear	0.2	76.26	84.69
Cosine	0.2	77.97	85.85
Linear	0.35	80.26	86.69
Cosine	0.35	80.97	87.85
34