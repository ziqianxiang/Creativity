Under review as a conference paper at ICLR 2022
Sample and Communication-Efficient Decen-
tralized Actor-Critic Algorithms with Finite-
Time Analysis
Anonymous authors
Paper under double-blind review
Ab stract
Actor-critic (AC) algorithms have been widely adopted in decentralized multi-agent
systems to learn the optimal joint control policy. However, existing decentralized
AC algorithms either need to share agents’ sensitive information, e.g., local actions
and policies , or are not sample and communication-efficient. In this work, we
develop two decentralized AC and natural AC (NAC) algorithms that are sample and
communication-efficient and avoid sharing agents’ local actions and policies . In
both algorithms, agents share only noisy rewards and adopt mini-batch local policy
gradient updates to improve sample and communication efficiency. Particularly
for decentralized NAC, we develop a decentralized Markovian SGD algorithm
with an adaptive mini-batch size to efficiently compute the natural policy gradient.
Under Markovian sampling and linear function approximation, we prove that
the proposed decentralized AC and NAC algorithms achieve the state-of-the-art
sample complexities O(-2 ln-1) and O(-3 ln -1), respectively, and achieve
an improved communication complexity O(-1 ln -1). Numerical experiments
demonstrate that the proposed algorithms achieve lower sample and communication
complexities than the existing decentralized AC algorithm.
1	Introduction
Multi-agent reinforcement learning (MARL) has achieved great success in various application
domains, including control (66; 10; 51), robotics (64), wireless sensor networks (24; 67), intelligent
systems (71), etc. In MARL, a set of fully decentralized agents interact with a dynamic environment
following their own policies and collect local rewards, and their goal is to collaboratively learn the
optimal joint policy that achieves the maximum expected accumulated reward.
Classical policy optimization algorithms have been well developed and studied, e.g., policy gradient
(PG) (49), actor-critic (AC) (23) and natural actor-critic (NAC) (37; 7). In particular, AC-type
algorithms are more computationally tractable and efficient as they take advantages of both policy
gradient and value-based updates. However, in the multi-agent setting, decentralized AC is more
challenging to design compared with the centralized AC, as the algorithm updates involve sensitive
agent information, e.g., local actions, rewards and policies, which must be kept locally in the
decentralized learning process. In the existing designs of decentralized AC, the agents need to share
either their local actions (70; 69; 8; 36; 72; 27; 19; 26; 11) or local rewards (15; 33; 32) with their
neighbors, and hence are not desired. This issue is addressed by Algorithm 2 of (70) at the cost of
learning a parameterized model to estimate the averaged reward, yet this approach requires extra
learning effort and the reward estimation can be inaccurate. Moreover, existing decentralized AC
algorithms are not sample and communication-efficient, and do not have finite-time convergence
guarantee, especially under the practical Markovian sampling setting. Therefore, we aim to address
the following important question.
•	Q1: Can we develop a decentralized AC algorithm that is convergent, sample and communication-
efficient, and does not require sharing agents’ local actions and policies ?
On the other hand, as an important variant of the decentralized AC, decentralized NAC algorithm has
not been formally developed and rigorously analyzed in the existing literature. In particular, a major
challenge is that we need to develop a fully decentralized and computationally tractable scheme to
1
Under review as a conference paper at ICLR 2022
compute the inverse of the high dimensional Fisher information matrix, and this scheme must be both
sample and communication efficient. Hence, we want to ask:
•	Q2: Can we develop a computationally tractable and communication-efficient decentralized NAC
algorithm that has a low finite-time sample and communication complexity?
In this study, we provide affirmative answers to the above two questions by developing fully decen-
tralized AC and NAC algorithms that are sample and communication-efficient, and do not reveal
agents’ local actions and policies. We also develop rigorous finite-time analysis of these algorithms
under Markovian sampling. Our contributions are summarized as follows.
Table 1: List of complexities of the existing AC and NAC algorithms for achieving
E[kVJ(ω)k2] ≤ e and E[J(ω*) - J(ω))] ≤ e, respectively.
Algorithm	Papers	Share local action/policy	Sampling scheme	Sample complexity	Communication complexity
	(54)	—	i.i.d.	O(-36)	—
Centralized AC	(38)	—	i.i.d.	Oe(-4)	—
	(25)	—	i.i.d.	O(-2.5)	—
	(61)	—	Markovian	O(-2.5 ln3 -1)	—
	(56)	—	Markovian	Oe(-2.5)	—
	(60)	—	Markovian	O(-2 ln-1)	—
Decentralized AC	(70; 69; 15) (72; 27; 26)	×	Markovian	一	—
	(70; 47; 33)	X	Markovian	一	—
	This work	X	Markovian	O(-2 ln -1)	O(-1 ln-1)
	(54)	—	i.i.d.	O(-36)	—
Centralized NAC	(61)	—	Markovian	O(-4 ln2 -1)	—
	(60)	—	Markovian	O(-3 ln-1)	—
Decentralized NAC	This work	X	Markovian	O(-3 ln -1)	O(-1 ln-1)
1.1	Our Contributions
We develop fully decentralized AC and NAC algorithms and analyze their finite-time sample and
communication complexities under Markovian sampling. Our results and comparisons to existing
works are summarized in Table 1 1. Our decentralized AC and NAC algorithms adopt the following
novel designs to accurately estimate the policy gradient in an efficient way.
•	Noisy Rewards: In a decentralized setting, local policy gradients (estimated locally by the agents)
involve the average of all agents’ local rewards. To help agents estimate this averaged reward
without revealing the raw local rewards , we let them share Gaussian-corrupted local rewards with
their neighbor, and the variance of the Gaussian noise can be adjusted by each agent to reach its
desired level.
•	Mini-batch Updates: We apply mini-batch Markovian sampling to both the decentralized actor
and critic updates. This approach 1) helps the agents obtain accurate estimations of the corrupted
averaged reward; 2) significantly reduces the variance of policy gradient caused by Markovian
sampling; and 3) significantly reduces the communication frequency and complexity.
Moreover, for our decentralized NAC algorithm, we additionally adopt the following design to
compute the inverse of the Fisher information matrix in an efficient and decentralized way.
•	Decentralized Natural Policy Gradient: By reformulating the natural policy gradient as the min-
imizer of a quadratic program, we develop a decentralized SGD with Markovian sampling that
allows the agents to estimate the corresponding local natural gradients by communicating only
1In this table, Oe(∙) hides all logarithm factors. In (25), the sample complexity has been established for
various AC-type algorithms, and we compare with the best one. In (70), the Algorithm 1 needs to share local
actions while the Algorithm 2 does not.
2
Under review as a conference paper at ICLR 2022
scalar variables with their neighbors. In particular, in order to minimize the sample complexity of
the decentralized SGD, we set the batch size to be exponentially increasing.
Theoretically, for the first time, we provide finite-time convergence analysis of decentralized AC
and NAC algorithms under Markovian sampling. Specifically, we prove that our decentralized AC
and NAC algorithms achieve the overall sample complexities O(-2 ln -1) and O(-3 ln -1),
respectively, and both match the state-of-the-art complexities of their centralized versions (60).
Moreover, both decentralized algorithms achieve a significantly reduced overall communication
complexity O(-1 ln -1). In particular, our analysis involves new technical developments. First, we
need to characterize the bias and variance of (natural) policy gradient and stochastic gradient caused
by the noisy rewards and the inexact local averaging steps, and control them with proper choices of
batch sizes and number of local averaging steps. Second, when using decentralized Markovian SGD
to compute the inverse Fisher information matrix, we need to use an exponentially increasing batch
size to achieve an optimized sample complexity bound. Such a Markovian SGD with adaptive batch
size has not been studied before and can be of independent interest.
1.2	Related Work
Convergence analysis of AC and NAC. In the centralized setting, the AC algorithm was firstly
proposed by (23) and later developed into the natural actor-critic (NAC) algorithm (37; 7). Specifically,
(37) does not provide any convergence result, while (22; 5) and (20; 6; 7) establish the asymptotic
convergence rate of centralized AC and NAC, respectively, which is weaker than our finite-time
convergence results. Furthermore, (54; 25; 38; 61; 56) and (54) establish the finite-time convergence
rate of centralized AC and NAC, respectively. Please refer to Table 1 for their sample complexities.
Moreover, (60) improve the finite-time sample complexities of the above works to the state-of-the-
art result for both centralized AC and NAC by leveraging mini batch sampling, and our sample
complexities match these state-of-the-art results .
In the decentralized setting, a few works have established the almost sure convergence result of
AC (15; 27; 47; 33), but they do not characterize the finite-time convergence rate and the sample
complexity. To the best of our knowledge, there is no formally developed decentralized NAC
algorithm.
Decentralized TD-type algorithms. The finite-time convergence of decentralized TD(0) has been
obtained using i.i.d samples (52; 14; 53; 28) and Markovian samples (46; 53), respectively, without
revealing the agents’ local actions, policies and rewards . Decentralized off-policy TD-type algorithms
have been studied in (34; 45; 9; 12).
Decentralized AC in other MARL settings. Some works apply decentralized AC to other MARL
settings that are very different from ours. For example, (44; 36; 17; 11; 57) studied adversarial game.
(30) studied a mixed cooperative-competitive environment where each agent maximizes its own Q
function (30). (11) proposed Delay-Aware Markov Game which considers delay in Markov game.
(68; 31) studied linear control system and linear quadratic regulators instead of an MDP. (55) studied
sequential prisoner’s dilemmas.
Policy gradient algorithms. Policy gradient (PG) and natural policy gradient (NPG) are popular
policy optimization algorithms. (1) characterizes the iteration complexity (i.e., number of episodes)
of centralized PG and NPG algorithms by assuming access to exact policy gradient. They also
established a sample complexity result O(-6) in the i.i.d. setting for NPG, which is worse than the
state-of-the-art result O(-3 ln-1) of both centralized NAC (60) and our decentralized NAC with
Markovian samples. (3) proposes decentralized PG in a simple cooperative MARL setting, where all
the agents share one action and the same policy, and they establish a iteration complexity in the order
of O(-4). (13; 73) apply decentralized PG to Markov games. (2) applies decentralized NPG to a
different cooperative MARL setting where each agent observes its own state, takes its own action and
has access to these information of its neighbors.
Value-based algorithms. Value-based algorithms have also been develop for MARL. Specifically,
(21; 18) develop distributed Q-learning in a simplified cooperative MARL setting, where the agents
share a joint action. In particular, (18) characterizes the convergence rate of a value function-based
convergence error, which is a different optimality measure from that of AC-type algorithms. (35)
applies distributed Q-learning to another cooperative MARL setting, where each agent observes
3
Under review as a conference paper at ICLR 2022
its own state and takes its own action. It establishes an asymptotic convergence guarantee, and no
convergence rate is given. (40) develops a value propagation algorithm that uses primal-dual method
to minimize a soft Bellman error in the MARL setting. Under an assumption that the variance of
the stochastic gradient is uniformly bounded, it establishes a non-asymptotic convergence rate to an
approximate stationary point.
2	Review of Multi-Agent Reinforcement Learning
In this section, we first introduce some standard settings of RL. Consider an agent that starts from
an initial state so 〜ξ and collects a trajectory of Markovian samples {st,at, Rt}t ⊂ S ×A× R
by interacting with an underlying environment (with transition kernel P) following a parameterized
policy ∏ω with induced stationary state distribution μω. The agent aims to learn an optimal policy
that maximizes the expected accumulated reward J(ω) = (1 - γ)E Pt∞=0 γtRt , where γ ∈ (0, 1)
is a discount factor. The marginal state distribution is denoted as Pω (st) and the visitation measure
is defined as νω (s) := (1 - γ) Pt∞=0 γtPω(st = s), both of which depend on the policy parameter
ω ∈ Ω and the transition kernel P. We also define the mixed transition kernel Pξ(∙∣s,a):=
YP(∙∣s, a) + (1 - γ)ξ(∙), whose stationary state distribution is known to be Vω.
In the multi-agent RL (MARL) setting, M agents are connected via a fully decentralized network
and interact with a shared environment. The network topology is specified by a doubly stochastic
communication matrix W ∈ RM ×M . At any time t, all the agents share a common state st . Then,
every agent m takes an action a(m) following its own current policy ∏(m)(∙∣st) parameterized by
ωt(m). After all the actions at := {at(m)}mM=1 are taken, the global state st transfers to a new state
st+1 and every agent m receives a local reward Rt(m). In this MARL setting, each agent m can only
access the global state {st}t, its own actions {at(m)}t and rewards {Rt(m)}t and policy πt(m). Next,
define the joint policy ∏t(at∣st) := QM=I ∏(m)(atm)∣st) parameterized by ωt = [ω(1);...; ω(M)],
and define the average reward Rt := M PM=I R(m). The goal of the agents is to collaboratively
learn the optimal joint policy that maximizes the expected accumulated average reward J(ω) :=
(1 - γ)E[ P∞=o γtRt 卜0 〜ξ]. Throughout, We consider the setting that the agents keep interacting
with the environment and observing a trajectory of MDP transition samples, which are further used
to learn the optimal joint policy.
3	Sample and Communication-Efficient Decentralized AC
In this section, we propose a decentralized actor-critic (AC) algorithm that is sample and
communication-efficient and avoids revealing agents’ local actions, policies and raw rewards .
We first consider a direct extension of the centralized AC to the decentralized case. As each agent m
has its own policy π(m), it aims to update the policy parameter ω(m) using the local policy gradient
Vω(m) J(ω). Under linear approximation of the value function Vθ(S) ≈ φ(s)>θ where φ(s) is the
feature vector, the local policy gradient has the following stochastic approximation.
Vω(m) J(ωt)≈hRt + γφ(st+ι)>θ(m) - φ(st)>θ(m)iψ(m)(a(m)∣st),	⑴
where a”〜∏tm) (∙∣st),st+ι 〜Pξ(∙∣st, at), st+ι 〜P(∙∣st,at).	(2)
Here, θt(m) is agent m’s critic parameter and ψt(m)(at(m) |st) = Vω(m) ln πt(m) (at(m) |st) is the local
score function. It is clear that both θt(m) and ψt(m)(at(m) |st) can be obtained/computed by agent
m using the local information. However, the average reward Rt requires agent m aggregating
the local rewards from all the other agents, which raises concerns. In the existing literature on
decentralized AC, this issue is avoided by either 1) sharing the agents’ actions with each other instead
(70; 69; 8; 36; 72; 27; 19; 26; 11), yet the action information is also highly sensitive; or 2) learning a
parameterized model to estimate the average reward (70), which requires extra learning effort and
does not provide an accurate estimation. Hence, we are motivated to develop a simpler approach that
provides accurate estimation of the average reward while avoids sharing raw local rewards .
4
Under review as a conference paper at ICLR 2022
1.	Efficient Policy Gradient Estimation. We propose a decentralized policy gradient estimation
scheme that improves the sample and communication efficiency and avoids revealing the agents' local
actions, policies and raw rewards. First, in order for each agent to estimate the average reward Rt in
eq. (1), we let each agent m generate a noisy local reward Ret(m) = Rt(m) (1 + et(m)) and share with
other agents, where e(m) 〜N(0, σm) 2. The noise variance is determined by the agent based on its
desired level. Specifically, every agent m first initializes its local estimation of the averaged reward
R(m) using its own noisy reward, i.e., R(m) = R(m). Then, each agent m performs decentralized
local averaging with its neighbors Nm for T0 iterations, i.e.,
R(m+l= Pm0∈Nm Wm,m' RmL ' = 0, 1,∙∙∙,T0- L	⑶
After that, agent m obtains the final estimate R(m) := Rm. It can be shown that R(m) converges
to the averaged noisy reward M PMM=I Re(m) exponentially fast. Ideally, by averaging these noisy
local rewards over the M agents, the variance of the noise in the final estimation will be scaled by a
factor of MM. Therefore, to obtain an accurate estimation, the network needs to have a sufficiently
large number of agents, which does not always hold in practice.
To address this issue, we let each agent m collect a mini-batch of N Markovian samples in each
iteration t to estimate the local policy gradient, which then takes the following form.
(t+1)N -1
vω(m) J(ωt) = NN X	[R(m) + γφ(si+ι)>θ(m) - φ(si)>θ(m)]ψ(m)(aim)∣Si),	(4)
i=tN
where R(m) is an estimation of Ri obtained by agent m following the process described in eq. (3).
Intuitively, each Rm) is corrupted by
O( MM) due to averaging over
a zero-mean noise with variance
the agents. Then, the mini-batch samples further help scale the noise variance by a factor of !.
Consequently, with a sufficiently large batch size N , we can obtain an accurate estimation of the
averaged reward and hence the policy gradient. To summarize, our decentralized policy gradient
estimation scheme has the following advantages.
•	Avoid sharing raw rewards: The agents share only noisy rewards Ret(m) with their neighbors, and
the noise variance can be adjusted based on the desired level such that Rt(m) is unknown to the other
agents. This is in contrast to other decentralized AC algorithms where the agents need to either
share local actions, rewards or collaboratively learn an additional parameterized reward model.
•	Sample-efficient: The mini-batch updates help greatly suppress the noise variance of the local
policy gradient in (4) and improve its estimation accuracy. On the other hand, mini-batch policy
gradient also helps reduce the optimization variance caused by Markovian sampling and leads to a
good finite-time sample complexity as we prove later. We note that there is no trade-off between
noise variance and sample efficiency here, because for highly noisy local rewards we can choose a
large batch size to suppress the overall estimation error to the desired level.
•	Communication-efficient: The mini-batch updates also significantly reduce the communication
frequency as well as the complexity as we prove later. In comparison, the existing decentralized
AC requires to perform one communication round per Markovian sample.
Remark. We note that the local mini-batch policy gradient update in eq. (4) can be computed in an
accumulative way by the agent when observing the mini-batch of transition samples on the fly. There
is no need to store all these samples and perform a large batch computation.
2.	Fully Decentralized Critic Update. The critic parameters of the agents are updated following
the standard decentralized TD-type algorithm. Specifically, consider the t-th local critic update of
each agent m. It first collects a mini-batch of Nc Markovian samples. Then, starting from a fixed
initialization θt(,m0) = θ-1, agent m performs Tc iterations of decentralized TD updates as follows,
where {st}t∈N follows the transition kernel P and atm) 〜∏(m)(∙∣st): for t0 = 0,1,...,TC 一 1,
(t+1)Nc-1
θ(m+ι = χ Wm,m0 θ(m00) + N X	[R(m) + γφ(si+ι)>θ(m) - Φ(Si)>θ(mo)] Φ(Si). (5)
2More generally, any noise with zero mean and variance σm2 will work.
5
Under review as a conference paper at ICLR 2022
Then, the updated critic parameter is set to be θt(m) := θt(,mT). To further reduce the consensus error,
we perform additional Tc0 steps of local model averaging, as also adopted in (12). The pseudo code of
the entire decentralized AC algorithm is summarized in Algorithms 1 and 2 below.
Algorithm 1 Decentralized Actor-Critic
Initialize: Actor-critic parameters ω0,θ-1.
for actor iterations t = 0, 1, . . . , T - 1 do
I Critic update on θt : by Algorithm 2.
I Collect N Markovian samples by eq. (2).
for agents m = 1, ..., M in parallel do
I Send noisy local rewards and perform
T 0 local average steps following eq. (3).
I Compute the estimated local policy
gradient Vω(m) J3) following eq. (4).
I Actor update on ωt :
ωt(+m1) = ωt(m) + αVb ω(m) J (ωt).
end
end
Output: ωT With T Ufrm {1,2,...,T}.
Algorithm 2 Decentralized TD (critic Update)
Initialize: Critic parameter θt,o = θ-ι.
for critic iterations t0 = 0, 1, . . . , Tc - 1 do
I Collect Nc Markovian samples following
policy πt and transition kernel P .
for agents m = 1, ..., M in parallel do
I A Send local critic parameters.
I A Decentralized TD update in eq. (5).
end
end
for iterations t0 = Tc, ..., Tc + Tc0 - 1 do
for agents m = 1, ..., M in parallel do
I A 噌+l = Pm,∈Nm Wm,m"黑L
end
end
Output: θt = θt,Tc+Tc0.
4	Finite-Time Convergence Analysis of Decentralized AC
In this section, we analyze the finite-time convergence of Algorithm 1 and characterize the sample
and communication complexities. All the notations and universal constants are summarized in
Appendices A & F respectively. We first introduce the following standard assumptions that have been
widely adopted in the existing literature.
Assumption 1. Regarding the transition kernels P, Pξ, denote μω,νω respectively as theirstationary
state distributions under policy πω and denote P, Pξ respectively as their marginal state distributions.
Then, there exist constants κ > 0 and ρ ∈ (0, 1) such that for all t ≥ 0,
sup dτv (P (st |	so	= S),	μω)	≤ κρt,	sup dτv (Pξ	(st	| so = S) , Vω)	≤ KPt	(6)
s∈S	s∈S
where dTV(P, Q) denotes the total-variation distance between probability measures P and Q.
Assumption 2. There exist constants Cψ ,Lψ ,L∏ > 0 such that for all ω, ω ∈ Ω, S ∈ S and a ∈ A,
kψω(a∣s)k ≤ Cψ, kψω(a|s) — Ψω (a∣s)k ≤ Lψ kω — ω∣∣ and dTV(∏ω(∙∣s),∏ω(∙∣s)) ≤ Ln∣∣ω  ω∣∣.
Assumption 3. There exists Rmax > 0 such that for any agent m and any Markovian sample
(s, a, s0), we have 0 ≤ R(m) (s, a, s0) ≤ Rmax.
Assumption 4. The feature vectors satisfy ∣φ(s)∣ ≤ 1 for all s ∈ S. There exists a constant λφ > 0
such that λmin (Es〜μω [φ(s)φ(s)>]) ≥ λφ for all ω.
Assumption 5. The communication matrix W ∈ RM ×M of the decentralized network is doubly
stochastic, and its second largest singular value satisfies σW ∈ [0, 1).
Assumption 1 has been widely considered in the existing literature (4; 38; 63; 58; 42; 60; 12) and it
holds for any time-homogeneous Markov chains with finite-state space and any uniformly ergodic
Markov chains. Assumption 2 introduces boundedness and Lipschitzness to the policy and its
associated score function (65; 60), and holds for many parameterized policies such as Gaussian policy
(25) and Boltzman policy (16). Assumption 4 can always hold by normalizing the feature vector φ(s)
Assumption 5 is widely used in decentralized optimization (43; 41) and multi-agent reinforcement
learning (46; 53; 12), which ensures that all the decentralized agents can reach a global consensus.
With the above assumptions, we obtain the following finite-time convergence result of the decen-
tralized AC algorithm. Throughout, we follow (60; 56) and define the critic approximation error as
Zcpprox ：= suPω Es〜νω (VL (s) — φ(s)>θω)2 where θ) is the optimal critic parameter (see its definition
right before Lemma D.3 in Appendix D). We also define sample complexity as the total number of
Markovian samples required for achieving E[∣VJ (ω)∣2] ≤ . All the universal constants are listed
in Appendix F.
6
Under review as a conference paper at ICLR 2022
Theorem 1. Let Assumptions 1-5 hold and adopt the hyperparameters of the decentralized TD
in Algorithm 2 following Lemma D.4. Choose α ≤ 4^, T0 ≥ Jn M. Then, the output of the
decentralized AC in Algorithm 1 has the following convergence rate.
EhIl ▽)QT)Il 2] ≤ 'Taax+4(c4σw +c5β2σWc )+4c6 (1- ^B β + ^N7+N+64Cψ ZapprcX.
Moreover, to achieve E[∣∣ VJ(ωTe)∣∣2] ≤ e for any e ≥ 128CψZcpoχ,we can choose T, N, Nc =
O(e-1) andTc, Tc0, T0 = O(ln e-1). Consequently, the overall sample compleXity is T(TcNc+N) =
O(e-2 ln e-1), and the communication compleXities for synchronizing linear model parameters and
rewards are T(Tc + Tc0) = O(e-1 ln e-1) and TT0 = O(e-1 ln e-1), respectively.
Remark. We note that the constraint on e is naturally induced by the critic approXimation error. In
particular, if this approXimation errors vanish, for eXample, when the dimension of features equals the
number of states and the parameterized policy space is sufficiently eXpressive, then we can achieve
arbitrarily small target accuracy.
To the best of our knowledge, Theorem 1 provides the first finite-time analysis of decentralized AC
under Markovian sampling. To elaborate, under any pre-specified variance σm2 of the reward noise, our
result shows that the gradient norm asymptotically converges to the order O(N-1 + Nc-1 + Zacpriptircox),
which can be made arbitrarily close to the linear model approximation error Zacpriptircox by choosing
sufficiently large batch sizes N, Nc . In particular, exact gradient convergence can be achieved when
there is no model approximation error. The overall sample complexity of our decentralized AC is
O(e-2 ln e-1), matching the state-of-the-art complexity result for centralized AC (60). Moreover,
with proper choices of the batch sizes N, Nc = O(e-1), the overall communication complexity is
significantly reduced to O(e-1 ln e-1).
The proof of Theorem 1 relies on developing several new algorithmic and technical developments
to reduce the communication complexity of both the decentralized actor and critic updates while
establishing tight convergence error bounds for both components. We further elaborate on these novel
technical developments below.
•	To achieve an overall reduced communication complexity, we adopt mini-batch updates in both the
actor and critic steps to reduce the communication frequency, as opposed to the single sample-based
update adopted in the existing work on decentralized TD learning (46). Specifically, in the analysis
of the decentralized TD described in Algorithm 2 (see Lemma D.4), the mini-batch updates with
batch size O(e-1) substantially improve the communication complexity from O(e-1 ln e-1) to
O(ln e-1) while help achieve the state-of-the-art sample complexity. Eventually, this together
with the mini-batch updates in the decentralized actor steps help achieve the desired overall low
communication complexity.
•	To achieve the state-of-the-art overall sample complexity, it is critical that the policy gradient
vanishes fast, which further requires a fast convergence of the decentralized TD learning. However,
although the standard Tc decentralized mini-batch TD updates can yield a small convergence error
for the global critic model (i.e., the average of all local critic models), it still suffers from a relatively
large consensus error. To resolve this issue, we introduce an additional Tc0 global consensus steps
in Algorithm 2 to reduce the consensus error. It is proved that a small number O(ln e-1) of such
steps suffices to yield a desired TD error.
•	We inject random noises into the local raw rewards R(tm) to protect the information. These noises
introduce additional Markovian bias and variance to the local policy gradients in (4). Fortunately,
as proved in Lemma D.6, by applying mini-batch policy gradient updates, we are able to control
the bias and variance induced by the noisy rewards to an acceptable level that does not affect the
overall sample and communication complexities.
5	Decentralized Natural AC
Natural actor-critic (NAC) is a popular variant of the AC algorithm. It utilizes a Fisher information
matrix to perform a natural policy gradient update, which helps attain the globally optimal solution in
terms of the function value convergence. In this section, we develop a fully decentralized version of
the NAC algorithm that is sample and communication-efficient.
7
Under review as a conference paper at ICLR 2022
Algorithm 3 Decentralized NatUral Actor-Critic
Initialize: Actor-critic parameters ω0,θ-1, natural policy gradient h-ι.
for actor iterations t = 0, 1, . . . , T - 1 do
I Critic update on θt: by Algorithm 2.
for agents m = 1, ..., M in parallel do
for iterations k = 0, 1, . . . , K - 1 do
I Collect Nk Markovian samples folloWing eq. (2).
I Send Rei(m) and zi(,m` ) and perform T0 and Tz local average steps, respectively.
一 1-1 , ∙	i i	τ	G	c /7	∖ /ʌ 11	♦	zr>∖ ι Z 4∖
I Estimate local gradient Vω(m) fωt (ht,k) folloWing eqs. (8) and (4).
I Perform SGD update in eq. (9).
end
I Actor update on ωt : ωt(+m1)
ωt(m) + αht(m)
end
end
Output: ωT With T Ufrm {1,2,...,T}.
A major challenge of developing fully decentralized NAC algorithm is computing the inverse Fisher
information matrix-vector product involved in the natural policy gradient update. To explain, first
recall the exact natural policy gradient update of the centralized NAC algorithm, i.e., ωt+1 =
ωt + αF(ωt)-1VJ(ωt), where F(ωt) := Est〜νωt,。,〜∏t(∙∣st) [ψt(at∣st)Ψt(at∣st)>] is the Fisher
information matrix. HoWever, in the multi-agent case, it is challenging to perform the natural policy
gradient update in a decentralized manner. This is because the Fisher information matrix F(ωt) is
based on the concatenated multi-agent score vector ψt(at∣st) = [ψ(1)(a(1)∣st);…;ψ(M )(a(M )|st)]
and the inverse matrix-vector product F(ωt)-1VJ(ωt) is not separable with regard to each agent’s
policy parameter dimensions. Next, we develop a fully decentralized scheme to implement the
natural policy gradient update in the multi-agent setting.
First, note that the natural policy gradient update h(ωt) := F(ωt)-1VJ(ωt) is equivalent to the
solution of a quadratic program, i.e.,
h(ωt) = arg min fωt (h) := 1 h>F(ωt)h - VJ(ωt)>h.
(7)
Therefore, we can apply K steps of SGD with Markovian sampling to solve this problem and obtain
an estimated natural policy gradient update. Specifically, starting from the initialization ht,0 = ht-1
(obtained in the previous iteration), in the k-th SGD step, we sample a mini-batch Bt,k 3 of Nk
Markovian samples to estimate Vfωt(h) as N Pi∈Bt,k Ψt(ai∣Si)Ψt(ai∣Si)>ht,k — VJ(ωt;Bt,k),
where VJ(ωt; Bt,k) is estimated in the same decentralized way as eq. (4) using the mini-batch
of samples Bt,k. In particular, each agent m needs to compute the corresponding local gradi-
ent N Pi∈Bt,k Ψ(m)(a(m)∣Si)[Ψt(ai∣Si)>ht,k] - V3⑺ J(ωt Bt,k), in which ψ(m)(a(m)∣si) and
Vb ω(m) J(ωt; Bt,k) can be computed/estimated by the agent m. Then, it suffices to obtain an estimate
of the scalar ψt(ai |si)>ht,k, which can be rewritten as PmM=1 ψt(m)(ai(m) |si)>ht(,mk). This summa-
tion can be easily estimated by the decentralized agents through local averaging. Specifically, each
agent m locally computes zi(,m0 ) = ψt(m)(ai(m) |si)>ht(,mk) and performs Tz steps of local averaging,
i.e., zi(,m`+) 1 = Pm0∈Nm Wm,m0 zi(,m` 0), ` = 0,1, . . . ,Tz -1. After that, the quantity M zi(,mTz) can be
proven to converge to the desired summation PmM=1 ψt(m)(ai(m) |si)>h(t,mk) exponentially fast. Finally,
the local gradient for agent m is approximated as
V ω(m) fωt (ht,k) = M X ψ[m(a IrAsMm)Z-V ω(m) J Qt； Bt,k ).
k i∈Bt,k
Then, the agent m performs the following SGD updates to obtain h(tm) := ht(,mK).
h(t,mk+) 1 = ht(,mk) - ηVbω(m)fωt(ht,k),	k = 0,..., K -1.
(8)
(9)
3Specifically, the mini-batch Bt,k contains sample indices {tN + Pk-=IO Nko,..., tN + Pko=o Nk0 — l}.
8
Under review as a conference paper at ICLR 2022
We emphasize that the above mini-batch SGD updates use Markovian samples. In particular, as
shown in Section 6, we need to develop an adaptive batch size scheduling scheme for this SGD in
order to reduce its sample complexity. We summarize the decentralized NAC in Algorithm 3.
6 Finite-time Convergence Analysis of Decentralized NAC
To analyze the decentralized NAC, we introduce the following additional standard assumptions.
Assumption 6. There exists a constant λp > 0 such that λmin (F(ω)) ≥ λp > 0, ∀ω ∈ Ω.
Assumption 7. There exists C* > 0 such thatfor ω* = arg maXω∈Ω J(ω) and any ω ∈ Ω,
Es 〜Vω,a 〜∏ω(∙∣s)
h( νω* (S)πω* (Rs) λ2i ≤ C 2
Vω (s)∏ω (a|s)	* .
Assumption 6 ensures that the Fisher information matrix F(ω) is uniformly positive definite, and
is also considered in (65; 29; 62). Assumption 7 regularizes the discrepancy between the stationary
state-action distributions νω* (s)∏ω* (a|s) and Vω(s)∏ω(a|s) (54; 59).
We obtain the following finite-time convergence result of the decentralized NAC algorithm.
Throughout, we follow (54; 60; 62) and define the actor approximation error as ζaapcptorrox :=
supω minhEs〜νω,a〜∏ω(∙∣s) [(ψω(a∣s)>h - Aω(s, a))2]. All universal constants are listed in AP-
pendix F.
Theorem 2. LetAssumptions 1-7 hold and adopt the hyperparameters ofthe decentralized TD in
Algorithm 2 following Lemma D.4. Choose hyperparameters α ≤ min(1, 4L,2 , 2Cψ), β ≤ 1,
rpo 、	ln M 1	1 T 、 ln(3DJCψ) k、	ln 3	τ∖r∖	2304cψ(K+1-P)
T ≥ 21nσ-τ, η ≤ 丐,Tz ≥	ln σW1 , K ≥ ln(1-ηλF/2)-1, N ≥ ηλ⅛(1-ρ)(1-ηλ^/2)(K-1)/2
ln 3
ln σW1
and Nk H (1 — ηλp/2)-k/2. Then, the output ofAlgorithm 3 satisfies
J(ω*) - E J(ωTe)
≤ c17 + C18(1 - ηλFy	+ + C19σWz + c20σW + c2ιβσW + √√23=
T α	2	Nc
+ c22 (1 - λB β)Tc/2+Cψ /CMCpirox+c24 ζapptrcx+c * JZapo。*.
Moreover, to achieve J(ω*) - E J(ωTb)
≤ e for any e ≥ 2Cψ JCi6(ζ端X + 2c24<cpipro* +
2C* PZaccporro*, we can choose T = O(e-1), N, Nc = O(e-2), Tc, T0, T0, Tz, K = O(ln e-1). Con-
sequently, the overall sample complexity is T (TcNc + N) = O(e-3 ln e-1), and the communication
complexities for synchronizing linear model parameters and rewards are T(Tc+Tc0 ) = O(e-1 ln e-1)
and TT0 = O(e-1 ln e-1), respectively.
Theorem 2 provides the first finite-time analysis of fully decentralized natural AC algorithm. Our
result proves that the function value optimality gap converges to the order O(N-1/2 + JZCPPrOx +
PZacP00x) , which can be made arbitrarily close to the actor and critic approximation error by choosing
a sufficiently large batch size Nc . In particular, exact global optimum can be achieved when there
is no model approximation error. We note that the overall sample complexity of our decentralized
NAC is O(e-3 ln e-1), matching the state-of-the-art complexity result for centralized NAC (60).
Moreover, with the mini-batch updates, the overall communication complexity is significantly reduced
to O(e-1 ln e-1).
Similar to that of Theorem 1, our analysis of Theorem 2 also leverages the mini-batch decentralized
TD updates to reduce the communication complexity and deal with the bias and variance of the
local policy gradient introduced by noisy rewards. In addition, decentralized NAC uses mini-batch
SGD with Markovian sampling to solve the quadratic problem in eq. (7). Here, we use a special
geometrically increasing batch size scheduling scheme, i.e., Nk H (1 - ηλF /2)-k/2, to achieve
the best possible convergence rate under the total sample budget that PkK=1 Nk = N and obtain
the desired overall sample complexity result. Such an analysis of SGD with Markovian sampling
under adaptive batch size scheduling has not been studied in the literature and can be of independent
interests.
9
Under review as a conference paper at ICLR 2022
Figure 1: Comparison of accumulated discounted reward J (ωt) among decentralized AC-type
algorithms in a ring network with sparse connections .
7	Experiments
We simulate a fully decentralized ring network with 6 agents. Please refer to Appendix E for
detailed environment setup. We implement four decentralized AC-type algorithms and compare their
performance, namely, our Algorithms 1 and 3, an existing decentralized AC algorithm (Algorithm 2 of
(70)) that uses a linear model to parameterize the agents’ averaged reward (we name it DAC-RP1 for
decentralized AC with reward parameterization), and our proposed modified version of DAC-RP1 to
incorporate minibatch, which we refer to as DAC-RP100 with batch size N = 100. For our Algorithm
1, we choose T = 500, Tc = 50, Tc0 = 10, Nc = 10, T0 = Tz = 5, β = 0.5, {σm}6m=1 = 0.1, and
consider batch size choices N = 100, 500, 2000. Algorithm 3 uses the same hyperparameters as
those of Algorithm 1 except that T = 2000 in Algorithm 3. For DAC-RP1, we set learning rates
βθ = 2(t + 1)-0.9, βv = 5(t + 1)-0.8 and batch size N = 1 as mentioned in (70). The modified
DAC-RP100 adopts the same learning rates as Algorithm 1 with N = 100.
Figure 1 plots the accumulated reward J (ωt) v.s. communication and sample complexity. Each
curve includes 10 repeated experiments, and its upper and lower envelopes denote the 95% and 5%
percentiles of the 10 repetitions, respectively. For our decentralized AC algorithm (left two figures),
its communication and sample complexities for achieving a high accumulated reward are significantly
reduced under a larger batch size N . This matches our theoretical understanding in Theorem 1 that
a large N helps reduce the communication frequency and policy gradient variance. In comparison,
DAC-RP1 (with N = 1) has almost no improvement on the accumulated reward. Moreover, although
the modified DAC-RP100 (with N = 100) outperforms DAC-RP1, its performance is much worse
than our Algorithm 1 with N = 100. This performance gap is due to two reasons: (i) Both DAC-RP
algorithms suffer from an inaccurate parameterized estimation of the averaged reward, and their mean
relative reward errors are over 100%. In contrast, our noisy averaged reward estimation achieves a
mean relative error in the range of 10-5 〜10-4Xii) Both DAC-RP algorithms apply only a single
TD update per-round, and hence suffers from a large mean relative TD error (about 2% and 1%
for DAC-RP1 and DAC-RP100, respectively)whereas our algorithms perform multiple TD learning
updates per-round and achieve a smaller mean relative TD error (about 0.3%). For our decentralized
NAC algorithm (right two figures), one can make similar observations and conclusions.
8	Conclusion
We developed fully-decentralized AC and NAC algorithms that are efficient and do not reveal agents’
local actions and policies . The agents share noisy reward information and adopt mini-batch updates
to improve sample and communication efficiency. Under Markovian sampling and linear function
approximation, we proved that our decentralized AC and NAC algorithms achieve the state-of-the-art
sample complexities O(-2 ln-1) and O(-3 ln-1), respectively, and they both achieve a small
communication complexity O(-1 ln -1). Numerical experiments demonstrate that our algorithms
achieve better sample and communication complexity than the existing decentralized AC algorithm
that adopts reward parameterization.
10
Under review as a conference paper at ICLR 2022
References
[1]	A. Agarwal, S. M. Kakade, J. D. Lee, and G. Mahajan. On the theory of policy gradient methods:
Optimality, approximation, and distribution shift. ArXiv:1908.00261, 2019.
[2]	C. Alfano and P. Rebeschini. Dimension-free rates for natural policy gradient in multi-agent
reinforcement learning. ArXiv:2109.11692, 2021.
[3]	Q. Bai, M. Agarwal, and V. Aggarwal. Joint optimization of multi-objective reinforcement
learning with policy gradient based algorithm. ArXiv:2105.14125, 2021.
[4]	J. Bhandari, D. Russo, and R. Singal. A finite time analysis of temporal difference learning with
linear function approximation. In Proc. Conference on Learning Theory (COLT), volume 75,
pages 1691-1692, 2018.
[5]	S. Bhatnagar. An actor-critic algorithm with function approximation for discounted cost
constrained markov decision processes. Systems & Control Letters, 59(12):760-766, 2010.
[6]	S. Bhatnagar, M. Ghavamzadeh, M. Lee, and R. S. Sutton. Incremental natural actor-critic
algorithms. In Proc. Advances in Neural Information Processing Systems (NeurIPS), volume 20,
pages 105-112, 2007.
[7]	S. Bhatnagar, R. S. Sutton, M. Ghavamzadeh, and M. Lee. Natural actor-critic algorithms.
Automatica, 45(11):2471-2482, 2009.
[8]	G. Bono, J. S. Dibangoye, L. Matignon, F. Pereyron, and O. Simonin. Cooperative multi-agent
policy gradient. In Proc. Joint European Conference on Machine Learning and Knowledge
Discovery in Databases (ECML PKDD), pages 459-476, 2018.
[9]	L. Cassano, K. Yuan, and A. H. Sayed. Multi-agent fully decentralized value function learning
with linear convergence rates. IEEE Transactions on Automatic Control, 2020.
[10]	B. Chalaki and A. A. Malikopoulos. A hysteretic q-learning coordination framework for
emerging mobility systems in smart cities. ArXiv:2011.03137, 2020.
[11]	B. Chen, M. Xu, Z. Liu, L. Li, and D. Zhao. Delay-aware multi-agent reinforcement learning.
ArXiv:2005.05441, 2020.
[12]	Z. Chen, Y. Zhou, and R. Chen. Multi-agent off-policy td learning: Finite-time analysis with
near-optimal sample complexity and communication complexity. ArXiv:2103.13147, 2021.
[13]	C. Daskalakis, D. J. Foster, and N. Golowich. Independent policy gradient methods for
competitive reinforcement learning. ArXiv:2101.04233, 2021.
[14]	T. Doan, S. Maguluri, and J. Romberg. Finite-time analysis of distributed TD(0) with linear func-
tion approximation on multi-agent reinforcement learning. In Proc. International Conference
on Machine Learning (ICML), volume 97, pages 1626-1635, 09-15 Jun 2019.
[15]	J. Foerster, G. Farquhar, T. Afouras, N. Nardelli, and S. Whiteson. Counterfactual multi-agent
policy gradients. In Proc. Association for the Advancement of Artificial Intelligence (AAAI),
volume 32, 2018.
[16]	A. Ghosh and V. Aggarwal. Model free reinforcement learning algorithm for stationary mean
field equilibrium for multiple types of agents. ArXiv:2012.15377, 2020.
[17]	D. Hennes, D. Morrill, S. Omidshafiei, R. Munos, J. Perolat, M. Lanctot, A. Gruslys, J.-B.
Lespiau, P. Parmas, E. DuSEez-Guzmgn, et al. Neural replicator dynamics: Multiagent learning
via hedging policy gradients. In Proc. International Conference on Autonomous Agents and
MultiAgent Systems (AAMAS), pages 492-501, 2020.
[18]	P. Heredia, H. Ghadialy, and S. Mou. Finite-sample analysis of distributed q-learning for
multi-agent networks. In 2020 American Control Conference (ACC), pages 3511-3516, 2020.
[19]	P. C. Heredia and S. Mou. Distributed multi-agent reinforcement learning by actor-critic method.
IFAC-PapersOnLine, 52(20):363-368, 2019.
11
Under review as a conference paper at ICLR 2022
[20]	S. M. Kakade. A natural policy gradient. In Proc. Advances in Neural Information Processing
Systems (NeurIPS), volume 14, 2001.
[21]	S. Kar, J. M. F. Moura, and H. V. Poor. Qd-learning: A collaborative distributed strategy for
multi-agent reinforcement learning through consensus + innovations. IEEE Transactions on
SignalProcessing, 61(7):1848-1862, 2013.
[22]	V. Konda. Actor-critic algorithms (ph.d. thesis). Department of Electrical Engineering and
Computer Science, Massachusetts Institute of Technology, 2002.
[23]	V. R. Konda and J. N. Tsitsiklis. Actor-critic algorithms. In Proc. Advances in Neural
Information Processing Systems (NeurIPS), pages 1008-1014, 2000.
[24]	V. Krishnamurthy, M. Maskery, and G. Yin. Decentralized adaptive filtering algorithms for
sensor activation in an unattended ground sensor network. IEEE Transactions on Signal
Processing, 56(12):6086-6101, 2008.
[25]	H. Kumar, A. Koppel, and A. Ribeiro. On the sample complexity of actor-critic method for
reinforcement learning with function approximation. ArXiv:1910.08412, 2019.
[26]	Y. Lin, Y. Luo, K. Zhang, Z. Yang, Z. Wang, T. Basar, R. Sandhu, and J. Liu. An asyn-
chronous multi-agent actor-critic algorithm for distributed reinforcement learning. In NeurIPS
Optimization Foundations for Reinforcement Learning Workshop, 2019.
[27]	Y. Lin, K. Zhang, Z. Yang, Z. Wang, T. BaSar, R. Sandhu, and J. Liu. A Communication-efficient
multi-agent actor-critic algorithm for distributed reinforcement learning. In 2019 IEEE 58th
Conference on Decision and Control (CDC), pages 5562-5567, 2019.
[28]	R. Liu and A. Olshevsky. Distributed td (0) with almost no communication. ArXiv:2104.07855,
2021.
[29]	Y. Liu, K. Zhang, T. Basar, and W. Yin. An improved analysis of (variance-reduced) policy gra-
dient and natural policy gradient methods. In Proc. Advances in Neural Information Processing
Systems (NeurIPS), volume 33, pages 7624-7636, 2020.
[30]	R. Lowe, Y. Wu, A. Tamar, J. Harb, P. Abbeel, and I. Mordatch. Multi-agent actor-critic for
mixed cooperative-competitive environments. ArXiv:1706.02275, 2017.
[31]	Y. Luo, Z. Yang, Z. Wang, and M. Kolar. Natural actor-critic converges globally for hierarchical
linear quadratic regulator. ArXiv:1912.06875, 2019.
[32]	X. Lyu, Y. Xiao, B. Daley, and C. Amato. Contrasting centralized and decentralized critics in
multi-agent reinforcement learning. ArXiv:2102.04402, 2021.
[33]	X. Ma, Y. Yang, C. Li, Y. Lu, Q. Zhao, and Y. Jun. Modeling the interaction between agents in
cooperative multi-agent reinforcement learning. ArXiv:2102.06042, 2021.
[34]	S. V. Macua, J. Chen, S. Zazo, and A. H. Sayed. Distributed policy evaluation under multiple
behavior strategies. IEEE Transactions on Automatic Control, 60(5):1260-1274, 2014.
[35]	D. J. Ornia and M. Mazo Jr. Event-based communication in multi-agent distributed q-learning.
ArXiv:2109.01417, 2021.
[36]	J. Perolat, B. Piot, and O. Pietquin. Actor-critic fictitious play in simultaneous move multistage
games. In Proc. International Conference on Artificial Intelligence and Statistics, pages 919-
928, 2018.
[37]	J. Peters and S. Schaal. Natural actor-critic. Neurocomputing, 71(7-9):1180-1190, 2008.
[38]	S. Qiu, Z. Yang, J. Ye, and Z. Wang. On the finite-time convergence of actor-critic algorithm.
In NeurIPS Optimization Foundations for Reinforcement Learning Workshop, 2019.
[39]	W. Qiu, X. Wang, R. Yu, R. Wang, X. He, B. An, S. Obraztsova, and Z. Rabinovich. Rmix:
Learning risk-sensitive policies forcooperative reinforcement learning agents. In Proc. Advances
in Neural Information Processing Systems (NeurIPS), 2021.
12
Under review as a conference paper at ICLR 2022
[40]	C. Qu, S. Mannor, H. Xu, Y. Qi, L. Song, and J. Xiong. Value propagation for decentralized
networked deep multi-agent reinforcement learning. In Proc. Advances in Neural Information
Processing Systems (NeurIPS)), pages 1184-1193, 2019.
[41]	R. Saha, S. Rini, M. Rao, and A. Goldsmith. Decentralized optimization over noisy, rate-
constrained networks: How to agree by talking about how we disagree. ArXiv:2010.11292,
2020.
[42]	M. Shaocong, Z. Yi, and Z. Shaofeng. Variance-reduced off-policy tdc learning: Non-asymptotic
convergence analysis. In Proc. Advances in Neural Information Processing Systems (NeurIPS),
2020.
[43]	N. Singh, D. Data, J. George, and S. Diggavi. Squarm-sgd: Communication-efficient momentum
sgd for decentralized optimization. ArXiv:2005.07041, 2020.
[44]	S. Srinivasan, M. Lanctot, V. Zambaldi, J. Perolat, K. Tuyls, R. Munos, and M. Bowling. Actor-
critic policy optimization in partially observable multiagent environments. ArXiv:1810.09026,
2018.
[45]	M. S. Stankovic and S. S. Stankovic. Multi-agent temporal-difference learning with linear
function approximation: Weak convergence under time-varying network topologies. In Proc.
American Control Conference (ACC), pages 167-172, 2016.
[46]	J. Sun, G. Wang, G. B. Giannakis, Q. Yang, and Z. Yang. Finite-sample analysis of decentral-
ized temporal-difference learning with linear function approximation. In Proc. International
Conference on Artificial Intelligence and Statistics (AISTATS), pages 4485-4495, 2020.
[47]	W. Suttle, Z. Yang, K. Zhang, Z. Wang, T. Basar, and J. Liu. A multi-agent off-policy actor-critic
algorithm for distributed reinforcement learning. ArXiv:1903.06372, 2019.
[48]	R. S. Sutton and A. G. Barto. Reinforcement learning: An introduction (Second Edition). 2018.
[49]	R. S. Sutton, D. McAllester, S. Singh, and Y. Mansour. Policy gradient methods for rein-
forcement learning with function approximation. In Proc. Advances in Neural Information
Processing Systems (NeurIPS), volume 12, 2000.
[50]	R. S. Sutton, D. A. McAllester, S. P. Singh, Y. Mansour, et al. Policy gradient methods for
reinforcement learning with function approximation. In Proc. Advances in Neural Information
Processing Systems (NeurIPS), volume 99, pages 1057-1063, 1999.
[51]	F. Venturini, F. Mason, F. Pase, F. Chiariotti, A. Testolin, A. Zanella, and M. Zorzi. Distributed
reinforcement learning for flexible and efficient uav swarm control. ArXiv:2103.04666, 2021.
[52]	H.-T. Wai, Z. Yang, Z. Wang, and M. Hong. Multi-agent reinforcement learning via double
averaging primal-dual optimization. In Proc. Advances in Neural Information Processing
Systems (NeurIPS), pages 9672-9683, 2018.
[53]	G. Wang, S. Lu, G. Giannakis, G. Tesauro, and J. Sun. Decentralized td tracking with linear
function approximation and its finite-time analysis. In Proc. Advances in Neural Information
Processing Systems (NeurIPS), volume 33, 2020.
[54]	L. Wang, Q. Cai, Z. Yang, and Z. Wang. Neural policy gradient methods: Global optimality and
rates of convergence. ArXiv:1909.01150, 2019.
[55]	W. Wang, J. Hao, Y. Wang, and M. Taylor. Achieving cooperation through deep multiagent
reinforcement learning in sequential prisoner’s dilemmas. In Proc. of International Conference
on Distributed Artificial Intelligence (DAI), pages 1-7, 2019.
[56]	Y. F. Wu, W. ZHANG, P. Xu, and Q. Gu. A finite-time analysis of two time-scale actor-critic
methods. In Proc. Advances in Neural Information Processing Systems (NeurIPS), volume 33,
pages 17617-17628, 2020.
[57]	B. Xiao, B. Ramasubramanian, and R. Poovendran. Shaping advice in deep multi-agent
reinforcement learning. ArXiv:2103.15941, 2021.
13
Under review as a conference paper at ICLR 2022
[58]	T. Xu and Y. Liang. Sample complexity bounds for two timescale value-based reinforcement
learning algorithms. ArXiv:2011.05053, 2020.
[59]	T. Xu, Y. Liang, and G. Lan. A primal approach to constrained policy optimization: Global
optimality and finite-time analysis. ArXiv:2011.05869, 2020.
[60]	T. Xu, Z. Wang, and Y. Liang. Improving sample complexity bounds for (natural) actor-critic
algorithms. In Proc. Advances in Neural Information Processing Systems (NeurIPS), volume 33,
2020.
[61]	T. Xu, Z. Wang, and Y. Liang. Non-asymptotic convergence analysis of two time-scale (natural)
actor-critic algorithms. ArXiv:2005.03557, 2020.
[62]	T. Xu, Z. Yang, Z. Wang, and Y. Liang. Doubly robust off-policy actor-critic: Convergence and
optimality. ArXiv:2102.11866, 2021.
[63]	T. Xu, S. Zou, and Y. Liang. Two time-scale off-policy td learning: Non-asymptotic analysis over
markovian samples. In Proc. Advances in Neural Information Processing Systems (NeurIPS),
pages 10634-10644, 2019.
[64]	Z. Yan, N. Jouandeau, and A. A. Cherif. A survey and analysis of multi-robot coordination.
International Journal of Advanced Robotic Systems, 10(12):399, 2013.
[65]	L. Yang, Q. Zheng, and G. Pan. Sample complexity of policy gradient finding second-order
stationary points. ArXiv:2012.01491, 2020.
[66]	E. Yanmaz, M. Quaritsch, S. Yahyanejad, B. Rinner, H. Hellwagner, and C. Bettstetter. Com-
munication and coordination for drone networks. In Proc. International Conference on Ad Hoc
Networks, pages 79-91, 2017.
[67]	M. Yuan, Q. Cao, M.-o. Pun, and Y. Chen. Towards user scheduling for 6g: A fairness-oriented
scheduler using multi-agent reinforcement learning. ArXiv:2012.15081, 2020.
[68]	H. Zhang, H. Jiang, Y. Luo, and G. Xiao. Data-driven optimal consensus control for discrete-
time multi-agent systems with unknown dynamics using reinforcement learning method. IEEE
Transactions on Industrial Electronics, 64(5):4091-4100, 2016.
[69]	K. Zhang, Z. Yang, and T. Basar. Networked multi-agent reinforcement learning in continuous
spaces. In Proc. 2018 IEEE Conference on Decision and Control (CDC), pages 2771-2776.
IEEE, 2018.
[70]	K. Zhang, Z. Yang, H. Liu, T. Zhang, and T. Basar. Fully decentralized multi-agent reinforcement
learning with networked agents. In Proc. International Conference on Machine Learning
(ICML), pages 5872-5881, 2018.
[71]	W. Zhang, H. Liu, F. Wang, T. Xu, H. Xin, D. Dou, and H. Xiong. Intelligent electric vehicle
charging recommendation based on multi-agent reinforcement learning. ArXiv:2102.07359,
2021.
[72]	Y. Zhang and M. M. Zavlanos. Distributed off-policy actor-critic reinforcement learning with
policy consensus. In Proc, Conference on Decision and Control (CDC), pages 4674-4679,
2019.
[73]	Y. Zhao, Y. Tian, J. D. Lee, and S. S. Du. Provably efficient policy gradient methods for
two-player zero-sum markov games. ArXiv:2102.08903, 2021.
14
Under review as a conference paper at ICLR 2022
Appendix
Table of Contents
A	Notations	15
B	Proof of Theorem 1	16
C	Proof of Theorem 2	17
D	Supporting Lemmas	20
E	Experiment Setup and Additional Results	37
E.1 Experiment Setup ................................................. 37
E.2 Gradient Norm Convergence Results in Ring Network ................ 38
E.3 Additional Experiments in Fully Connected Network ................ 39
E.4 Two-agent Cliff Navigation ....................................... 40
F Constant scalars	41
A	Notations
Norms: For any vector x, we denote kxk as its `2 norm. For any matrix X , we denote kX k, kX kF
as its spectral norm and Frobenius norm, respectively.
Difference matrix: △:= I - M 11>, where 1 denotes a column vector that consists of 1s.
Moments of random vectors: For a random vector X , we define its variance and covariance matrix
as Vαr(X):= EkX - EX∣∣2 and CoV(X):= E(X - EX][X - EX]>), respectively. It is well
knownthatEkXk2 = Var(X) + kEXk2 andthatVar(X) = tr[Cov(X)].
Score function: At any time t, The joint score function ψt(at∣st) := Vω ln∏t(at∣st) can be de-
composed into individual score functions ψ(m)(a(m)∣st) := Vω(m) ln∏(m)(a(m)∣st) as ψt(at∣st)=
[ψ(1)(a(1)lst),..., ψ(M)(a (M) |st)].
Reward functions: At any time t, we denote R(m) := Rm)(St,at,st+ι) and Rt= R(st, at, st+ι),
where R(s,a, s0) = M PM=I R(m)(s,a, s0).
Policy gradient: The policy gradient theorem (50) shows that
VJ(ω) = Eνω Aω(s,a)ψω(s, a).	(10)
where Aω (s, a) := Qω (s, a) - Vω (s) denotes the advantage function. In the decentralized case,
we have the approximations Vω(St) ≈ φ(st)>θ, Qω(St, at) ≈ Rt + γφ(st+1)>θ where st+1 〜
P(∙∣st, at). Therefore, we can stochastically approximate the partial policy gradient as eq. (1), i.e.,
for m = 1, ..., M,
vω(m) J (ωt)≈[Rt + γφ(st+l)>θ(m) - φ(st)>θ(m) iψ(m)(a(m)lst).
We also define the following mini-batch stochastic (partial) policy gradient.
V ω(m) J (ωt) := N PMNN-1 [Ri + γφ(si+j>θ" - φ(si)>θ(m)iψ(m)(a(m)∣Si).
V J (ωt) := [V ω(i) J (ωt);...; V ω(M) J (ωt)].
Filtrations: We define the following filtrations for Algorithms 1 & 3.
15
Under review as a conference paper at ICLR 2022
Ft= σ({θ(m)}m∈M,o≤to≤t ∪ {si, ai, si+ι, {e"}m∈M}tN-1 ∪ {stN})∙
Ft= σ[Ft ∪ σ({Si,θi,si+ι}(t+NN-1)].
Ft,k = σ [Ft ∪ σ({si, ai, si+1, si+1, {ei "meMheuko-1。Bt k0 )] .
B Proof of Theorem 1
Theorem 1. Let Assumptions 1-5 hold and adopt the hyperparameters of the decentralized TD
in Algorithm 2 following Lemma D.4. Choose α ≤ 4^, T0 ≥
Cln M-I. Then, the output of the
2 ln σW-1
decentralized AC in Algorithm 1 has the following convergence rate.
EhII vj QT)IIj ≤ ~~αaχ++4(c4σw +c5β2σwc )+4c6 (1- ^8B β + N7-+N8+64Cψ Caprcx.
Moreover, to achieve E[llVJ(ωTe)ll2] ≤ for any ≥ 128Cψ2 * ζacpriptircox,we can choose T,N,Nc =
O(-1) andTc, Tc0, T0 = O(ln -1). Consequently, the overall sample complexity is T(TcNc+N ) =
O(-2 ln -1), and the communication complexities for synchronizing linear model parameters and
rewards are T(Tc + Tc0) = O(-1 ln -1) and TT0 = O(-1 ln -1), respectively.
Proof. Concatenating all the agents’ actor updates in Algorithm 1, we obtain the joint actor update
ωt+1 = ωt + αVJ(ωt). Then, the item 7 of Lemma D.5 implies that
J(ωt+ι) ≥ J(ωt) + VJ(ωt)τ(ωt+ι - ωt) - L2Jl∣ωt+1 一力力『
=J(ωt) + αVJ(ωt)τVJ(ωt) - W2∣∣VJ(ωt)∣∣2
(i)
≥ J(ωt)+ akVJ(ωt)k2 + aVJ(ωt)τ(VJ(ωt) - VJ(ωt))
-LJα2 ∣∣VbJ(ωt) - VJ(ωt)∣∣2 - LJα2 ∣∣VJ(ωt)∣∣2
≥ J(ωt) + (2 - LJa2) kVJ(ωt)k2 - (— + LJa2) ∣∣vJ(ωt) - VJ(ωt)∣∣?
(iii)
≥ J(ωt) + 4kVJ(ωt)k2 - a∣∣VJ(ωt) - VJ(ωt)∣∣2
where (i) and (ii) use the inequalities ∣∣xk2 ≤ 2∣∣x - y『+ 2∣∣yk2 and xτy ≥ -ɪ∣∣xk2 - 2∣∣yk2
for any x,y ∈ Rd, respectively, and (iii) uses the condition that α ≤ 41y. Then, summing up the
inequality above over t = 0, 1, . . . , T - 1 yields that
T-1	T-1
J(ωτ) ≥ J(ωo) + α χ ∣VJ(ωt)∣2 - a X ∣∣VJ(ωt) - VJ(ωj∣∣2.
t=0	t=0
Rearranging the equation above and taking expectation on both sides yields that
1	T-1
E∣∣VJ(ωτ)∣∣2 = T EEkVJ(ωt)k2
T t=0
T-1
≤ TLE[J(ωτ) - J(ωo)] + T X EhlMJ5) — VJQt)『]
Tα	T t=0
(i) 4Rmax	2T0	2 2Tc0	λB	Tc
≤ Ta---+4c4σw +4c5β σw + 4c6 (1--—β)
+N7~+N8+64Cψ 瑞pθx,
(11)
where (i) uses the item 4 of Lemma D.5 and eq. (39) of Lemma D.6 (The condition of Lemma D.6
that T 0 ≥ 2票乜)holds). This proves the error bound of Theorem 1.
16
Under review as a conference paper at ICLR 2022
Finally, for any ≥ 128Cψ2 ζacpriptircox, it can be easily verified that the following hyperparameter choices
make the error bound in (11) smaller than and also satisfy the conditions of this Theorem and those
i L	D4 thɑt	i ( λB	4 1-σ λ N > ( 2	2r∖ 192CB [1 +(K-I)ρ]
In Lemma D.4 that β ≤ min 8C2, λB, 2CB% Nc ≥ λB +2βJ	(1-ρ)λB	.
α = min
O(1)
β = min
4 1-σ
T0
48RmaX
αe
1
B2 λB 2CB
m = O(e-1)
= O(1)
Tc0 =
Tc=
—；；--TV max
2ln(σ-1)
ln(48c5β2e-1
2ln(σ-1)
ln(48c6e
[lη(48c4e-1),lη M f∣ = θ(ln(e-1))
-m = θ(ln(e-1))
-1
2ln[(1 - λBβ∕4)-1
θ(ln(e-1))
N =l 48c7 m = O(e-1)
Nc =lmax 产,(ɪ + 2β)侬噌十；K- 1pim = O(e-1)
e	λB	(1 - ρ)λB
(12)
C Proof of Theorem 2
Theorem 2. LetAssumPtions 1-7 hold and adopt the hyperparameters ofthe decentralized TD in
λ2	Cψ2
Algorithm 2 following Lemma D.4. Choose hyperparameters α ≤ min(1,4工;C2 , 2jψ), β ≤ 1,
TO、	ln M …/	1 T 、ln(3DJCψ)上、	ln 3	Nr 23	2304Cψ(K+1-P)
T ≥ 21nσ-τ, η ≤ 书,Tz ≥	ln σW1 , K ≥ ln(1-ηλF/2)-1, N ≥ ηλ∣(1-ρ)(1-ηλF/2)(K-I)一
ln σW1
ln3
and Nk H (1 一 ηλp∕2)-k/2. Then, the output ofAlgorithm 3 satisfies
J (ω*) 一 EJ (ωτ)]
≤ Ta +。18(1	—	η2F )(	+	+ C19 σWz	+ C2oσT0	+	c21βσW	+	-√=
+ C22 (1 一 λ8Bβ) Tc" + C qci6Zapprox + C24Zcppicox + C* 'Zapprrx-
Moreover, to achieve J(ω*) 一 E J(ωTb)
≤ e for any e ≥ 2Cψ Jc16ζ端x + 2c24ζapPtrox +
2C* pZapaporoox，we can choose T = O(e-1), N, Nc = O(e-2), Tc, T0, T0, Tz, K = O(ln e-1). Con-
sequently, the overall sample complexity is T (TcNc + N) = O(e-3 ln e-1), and the communication
complexities for synchronizing linear model parameters and rewards are T(Tc+Tc0 ) = O(e-1 ln e-1)
and TT0 = O(e-1 ln e-1), respectively.
Proof. Concatenating all the agents’ actor updates in Algorithm 3, we obtain the joint actor update
ωt+1 = ωt + aht . Then, the item 7 of Lemma D.5 implies that
J(ωt+ι) ≥ J(ωt) + VJ(ωt)>(ωt+ι — ωt) —2J ∣∣ωt+ι 一 ωt∣∣2
=J (ωt) + aVJ (ωt)>ht — "∣∣ht∣∣2
(i)
≥ J(ωt) + aVJ(ωt)>F(ωt)-1VJ(ωt) + aVJ(ωt)>[ht — h(ωj]
—Lj a2∣∣ht — h(ωt)∣∣2 — Lj a2∣∣F (ωt )-1VJ (ωt)∣∣2
T
[
l
□
17
Under review as a conference paper at ICLR 2022
(ii)	α
≥ J(ωt) + (Cψ
α
2C2
—
—
^λ )INJ(ωt)k2 - (~2ψ~ + LJα2)Ilht- h(ωt)∣∣2
F
(iii)
≥ J(ωt) + 4C2INJ(ωt)k2 - αCψ∣∣ht- h(ωt)∣∣
where (i) uses the notation that h(ω. 4 F(ωt)-1VJ(ω/ and the inequality that ∣∣x∣2 ≤ 2kx —
yI2 + 2IyI2 for any x, y ∈ Rd, (ii) uses the item 3 of Lemma D.7 and the inequality that x>y ≥
—2cC2 ∣∣x∣2 — C2ψ∣∣y∣2 for any x,y ∈ Rd, and (iii) uses the condition that α ≤ min (4Ljc2, 2CLψ).
Taking expectation on both sides of the above inequality, summing over t = 0, 1, . . . , T — 1 and
rearranging, we obtain that
1 T-1	4C2	4C4 T-1
T E EkVJ(ωt)∣2 ≤ IaE[J(ωτ) — J(ωo)] + -yψ EEIIht- h(ωt)∣∣2
t=0	t=0
(i)	4Cψ2 R
max 4	ηλF (K-1)/2	0
≤ —Ta-+4CΨ [c10 0 —胃)	+ J。2Tz+c12σ2τ
+ c13β2σ2TC + c14 (1----4Bβ)	+ N15 + c16ζcpprox],
(13)
where (i) uses the item 4 of Lemma D.5 and the item 8 of Lemma D.7.
By Assumption 2, ln∏ω(s, a) is an Lψ-smooth function of ω. Denote ω*:= argminω∈Ω J(ω) and
denote Eω* as the unconditional expectation over S 〜Vω*, a 〜∏ω* (∙∣s). We obtain that
Eω* [ln∏t+ι(a∣s) — ln∏t(a|s)]
≥ Eω* [(Vωt ln πt(aIs)) (ωt+1 — ωt)i-----2ψEkωt+1 — ωtk2
—
Lψα2 E[∣htk2]
(i)
≥ αEω*	ψt(aIs)>	ht	—	h(ωt)	+ αEω*	ψt(aIs)>h(ωt) —	Aωt (s,	a)	+ αEω*	Aωt (s, a)
—Lψ α2E[∣∣ht — h(ωt)∣∣2] — Lψ α2E[∣∣F (ωt)-1VJ (ωj∣∣2]
(≥) —αCψ qE[∣∣ht — h(ωt)∣∣2] — αC* qZacpoox
+ αE[J(ω*) — J(ωt)] — Lψα2E[∣∣ht — h(ωt)∣∣ ] — Lψα2λ-2E[∣∣VJ(ωt)∣∣ ],
where (i) uses the inequality that ∣x∣2 ≤ 2∣x — y ∣2 + 2∣y∣2 for any x, y ∈ Rd and the notation
that h(ωt) =4 F (ωt)-1VJ (ωt), (ii) uses Cauchy-Schwarz inequality, the items 3 & 6 of Lemma
D.7, the inequality that E∣X∣ ≤ E[∣X∣2] for any random vector X and the equality that
Eω* [Aωt (s, a)] = E [J(ω*) — J(ωt)] (See its proof in Lemma 3.2 of (1).). Averaging the inequality
above over t = 0, 1, . . . , T — 1 and rearranging it yields that
1 T-1
J (ω*)— E[J (ωτ)] = τ £ E[J (ωt)]
T t=0
T-1 J-------------------
≤ T- Eω* [ln πT(a|s) — ln n0(a|s)] + C*qζ⅛rox + -Tψ X /E[∣∣ht — h(ωt)∣∣2]
Tα	T t=0
T-1	T-1
+ ~T~ X E[∣∣ht- h(ωt)∣∣2] + τλ" X E[∣∣vJ (ωt)∣∣2]
T t=0	TλF t=0
(i)	1
≤ Ta Es 〜Vω* [KL (πω*(∙Is)llπθ(∙Is))- kl (πω*(∙Is)llπτ(∙Is))] + C * JZapPOOX
+ Cψ
(K -1)/2	0	0
+ c11σ2Tz + c12σ2T + c13β2σ2Tc
18
Under review as a conference paper at ICLR 2022
+ N+ci6ζcrproxi
(K-1)/2
+c11σ2Tz +c12σ2T0+c13β2σ2Tc0
+ c14 1 -
+ c14 1 -
+ Lψ α c10
+ N+ci6ζcrproxi
+
4Cψ RmaX
-τα-
+ 4Cψ4 hc10
+ c11σ2Tz + c12σ2T 0 + c13β2σ2Tc0
+ C1《-λB β )Tc
+ N+ci6ζcrproxiO
(ii)	1
≤ --
一 Ta
~“3* [κL(∏ω*(∙∣s 川 ∏o(∙∣s))]+C* yζaρ‰
(KT)/4	_ T	7_ T0	7_	T0
+ ʌ/c11σ Z + ʌ/c12σ	+ ʌ/c13βσ c
+ Cψ
+ √C14(1- λB 广 +
+ Lψ 1 +
+ √ci6 ζcpiprθx]
(K-1)/4	T	T0	T0
+ c11σTz + c12σT + c13βσTc
λB	Tc/2 ,	c15	, C	广CritiC '
+ c14 (1--------4-βJ	+ √N + c16ζapprox +
4Lψ Cψ RmaX
Τaλ2
F
(iii) c17
=Ta+c18
(	+ + ci9σTz + c20στ0 + C21 βσTc + c22(1 - λBBβ) Cc
+ √N + Cψ Jci6ζcρρrθχ+ c24 ZCpprOx + C * J ZaCpOOx,
(14)
where (i) uses the definition of KL divergence that KL ∏τω * (∙∣ s) || ∏ω (∙∣s))	=
Ea〜∏ω* (∙∣s) [ln∏ω* (a|s) - ln∏ω(a∣s)∣s] and eqs. (13) & (54), (ii) uses the condition that α ≤ 1 and
the inequality that ,PZi Xi ≤ PZi √X for any n ∈ N+ and χ1,... ,Xn ≥ 0, (iii) uses the nota-
tions that c17:=Es〜νω* [KL(∏ω* (∙∣s)∣∣∏o(∙∣s))] +---Ψ~ψ^^max, ci8 ：= Cψ√c10 + c10Lψ (1 + -j2ψ),
c19 := Cψ √c11 + c11Lψ (1 + ~λ2ψ), c20 := Cψ √c12 + c12Lψ (1 + ^ψ~) , c21 := Cψ √c13 +
(一	4C4、	L _______ 一 4c	4C4,、	L ______ _ 4c	4C4.、
1 +--λ2L	), C22 :=	Cψ √ci4	+	C14Lψ	(1 +---λ2L	),	C23	:=	Cψ √ci5 +	C15Lψ	(1 +---λ2L),
c24 := ci6Lψ (1 + -CL). This proves the error bound of Theorem 2.
Finally, for any ≥ 2CΨ 口证 + 2c24ZCpprOx + 2C* PZacpOOx, it can be verified that the
following hyperparameter ChoiCes make the error bound in (14) smaller than and satisfy all
the Conditions of this Theorem and those in Lemma D.4 that β ≤ min
(λB + 2β)
192CB [1+(κ-1)ρ]
(1-p)λB
.乙λF
α = mm(1, J，
O(1)
λB 4	1 - σ
β=mm3 8cB ,λB, K)
η =2⅛ = O(I)
T = l 14c17 m = O(e-1)
a
= O(1)
19
Under review as a conference paper at ICLR 2022
K = lmax h ―—ln、3 /,	4ln(14cι8e 1)	+ J] = o[in(e-1)]
ln[(1 - ηλp∕2)-1] ln [(1 - Rf/2)-1]
ln(3DJCψ) ln(14c19e-1)	-1.
Tz=Imax	in(σ-τ,m(σ-^	= IO[ln(e)]
T0=l max h H, lnl4≡Γ1 i1=θ[ln(e-1)]
TC=I ln(14σ⅛1 ] = O[ ln(e-1)]
Tc =l ln!-≡¾ m = O[ ln(e-1)]
N = l_2304CΨ(K +1- P)_m = o(e-2)
I ηλF(1 - P)(1 - ηλF∕2)(K-1)/21	(	)
Nc =lmax [(a + 2β) 192CB[-ɪ 1)P],196c23e-2im =O(e-2)	(15)
D Supporting Lemmas
First, we extend the Lemma F.3 of (12) to the Lemma D.1 below. The item 1 of Lemma D.1
generalizes the case n = 1 to any n ∈ N+, the items 2 & 3 remain unchanged, and the item 4 is
added for convenience of our convergence analysis.
Lemma D.1. The doubly stochastic matrix W and the difference matrix ∆ = I -吉 11> have the
following properties:
1.	∆Wn = Wn∆ = Wn - -M 11> for any n ∈ N+.
2.	The spectral norm of W satisfies kWk = 1.
3.	For any x ∈ RM and n ∈ N+, kWn∆xk ≤ σWn k∆xk (σW is the second largest singular value
of W). Hence, for any H ∈ RM ×M , kWn∆H kF ≤ σWn k∆H kF.
4.	∣∣Wn - MM 11>∣∣ ≤ σW JWn - MM 11>∣∣F ≤ σW √M for any n ∈ N+.
Proof. The proof of items 2 & 3 can be found in (12). We prove the item 1 and item 4.
We prove item 1 by induction. The case n = 1 of the item 1 can be proved by the following two
equalities, as shown in (12).
∆W =(I - ɪ 11>[ W = W - ɪ 11>W = W - ɪ 11>
MMM
W∆ = W(I - ɪ 11>[ = W - ɪ W 11> = W - ɪ 11>
MMM
Suppose the case of n = k holds for a certain k ∈ N+, then the following two equalities proves the
case of n = k + 1 and thus proves the item 1.
∆Wk+1 = (∆Wk)W = (wk - ɪ 11>) W = Wk+1 - ɪ 11>
Wk+1∆ = W(Wk∆) = W (wk - ɪ 11>) = Wk+1 - ɪ 11>
The item 4 can be proved by the following two inequalities.
Wn - ɪ 11>∣∣ =IWn∆∣∣ = sup kWn∆xk(≤) sup σWk∆kkxk (iv)σW,	(16)
m	χ"∣χk≤1	χ"∣χk≤1
20
Under review as a conference paper at ICLR 2022
Mn - -M 11>BF =IWnδ^f (≤) σWIHf
J) σW 4M (1 - M) + M(M -1)( - M) ≤ σW√M,	(17)
where (i) uses the item 1, (ii) and (iii) use the item 3 (H = I in (iii)), and (iv) uses the fact that
∆ has M diagnoal entries 1 一 M and M(M 一 1) off-diagnoal entries 一 吉,which implies that
k∆k = 1.
Next, we extend the Lemma F.2. of (12) to the Lemma D.2 below.
Lemma D.2. Suppose the Markovian samples {si, ai}i≥0 are generated following the policy πω and
transition kernel P0 (can be P or Pξ), and si+1 〜P(∙∣Si, aj. Then, for any deterministic mapping
X : S × A × S × S → Rp×q (p, q ∈ N+ are arbitrary.) such that kX (s, a, s0, se)kF ≤ Cx and for
any s, s0, se ∈ S, a ∈ A, we have
Ehll IntI X (Si,ai,Si+ι,si+ι)- XlIF"i ≤ ?二一。), ∀n,n0 ∈ N+	(18)
where X = E[X(si, αi, si+ι, si+1)∣si] with Si 〜μω (or νω) when P0 = P (or Pξ).
Proof. Denote Y(s, a, s0) := Ee〜po(.|s,。)[X(s, a, s0,Se)∣s, a, s0] which satisfies ∣∣Y(s, a, s0)k ≤ CX
and Esi〜νω [Y(si, ai, si+1)] = X. Hence, Lemma F.2 of (12) can be applied to Y(s, a, s0) and
obtain the following inequality
E
hll Int1 Y (si,ai,si+ι) - XIlF∣snθ]≤ 8CxnK1t1P-P)
(19)
Therefore, we obtain that
n+n0 -1	2
E[lln X X(si,ai,si+ι,si+ι) - XL∣{si,ai,si+ι}n+n0τ]
i=n0
n+n0	2
= ||E[- X X(si, ai, si+1, si+ι) - X∣{si, ai,廿"-1].
i=n0
1 n+n -1
+ Var [n X X(Si, ai, si+1, si+l)∣{si, ai, si+1}n+n -^
i=n0
n+n0-1	2
=lln X Y(si,ai,si+1) -x||f
i=n0
1	n+n -1
+ n ɪZ Var [X (si,ai,si+1,si+l)∣{si,ai,si+l}n+n 1]
i=n0
(ii) Ii 1 n+n-1	—1∣2	C2
≤ lln ΣS Y(Si,ai,si+1) - x||f + 请	(20)
i=n0
where (i) uses the conditional independency among {s0i+1}i(=t+tN1)N-1on {si, ai, si+1}in=+nn00-1 and (ii)
uses the fact that ∣X(si, ai, si+1, s0i+1)∣F ≤ CX.
Finally, eq. (18) can be proved via the following inequality.
1 n+n0	2
e||| n X X(si, ai, si+1, si+1) - X|FISnOi
i=n0
21
Under review as a conference paper at ICLR 2022
(i) ι n+n0-1
≤ Ehll n	^X	Y(Si,ai,si+1)- X
i=n0
Cx2
sn0]+ ^n
2
F
(≤ 8CX(κ +1-ρ) + £ ≤ 9CX(κ + 1-ρ)
一 n(1 — ρ)	n -	n(1 — ρ)	,
where (i) takes the conditional expectation of eq. (20) on s0n and (ii) uses eq. (19).
□
Next, we prove the following Lemmas D.3 & D.4 on the decentralized TD in Algorithm 2. We first
define the following useful notations.
λφ := λmin(Es〜μω [φ(s)φ(s)>]) > 0, SeeAssUmPtion4.
B(s, s0) := φ(s)γφ(s0) — φ(s)>.
Bt := N P匕NNT B(Si,Si+ι).
Bω := Es~μω,a~∏ω (Ts),s0~P(Ts,a) [B(S, S )] .
b(m)(s, a, s0) := R(m)(s, a, s0)φ(s).
b(S, a SO) = M PM=I b(m)(s, aSO).
b(m) := N P(t+NNc-1 b(m)(Si,αi,Si+ι).
b ∙— ɪ PM	b(m)
bt ：= M mm=t bt .
bω ：= Es~μω,a~∏ω(Ts),s0~P(∙∣s,a) [b(S, a, s )] .
θω ：= B-1bω, which is the optimal critic parameter under policy ∏ω.
Lemma D.3. The following bounds hold for Algorithm 2.
1.	kB(S,SO)kF,kBtkF,kBωkF ≤ CB := 1+γ,
kb(m)(S,a,SO)k,kb(S,a,SO)k,kbt(m)k,kbtk,kbωk ≤Cb := Rmax.
2.	θ>Bωθ ≤ 一λ2B ∣∣θ∣∣2 Uniformlyforall ω, where 入与:=2(1 一 Y)λφ > 0.
3.	||。2k ≤ Rθ := 2λCb Uniformlyfor all ω.
λB
Proof. We first prove the item 1. Notice that for any vectors x, y ∈ Rd,
kxy>kF
∖
dd	ud ud
X X(xiyj)2 = utX xi2 utX yj2 = kxkkyk.
i=1 j=1	i=1	j=1
Hence, we obtain that
llB(S,S0)kF = ∣1φ(s)(yφ(s') -φ(s))>∣∣f = 口。岱)帖。3) -0(S)k ≤1 + γ := Cb,
(21)
∣∣b(S,α, S0)k = R(s, a, S0)∣φ(S)k ≤ Rmax := Cb.	(22)
The other terms listed in the item 1 can be proved by applying the Jensen’s inequality to the convex
function ∣ ∙ ∣.
Next, we prove the item 2, where we use the underlying distribution that S 〜μω, a 〜∏ω(∙∣s),
so 〜P(∙∣S,a). We obtain that
θ>Bωθ = Eωθ>φ(S)γφ(SO) 一 φ(S)>θ
=YEω [(Θ>Φ(s))(Θ>Φ(s0))] 一 Eω [(Θ>Φ(S))2]
22
Under review as a conference paper at ICLR 2022
≤ 2 (Eω[(θ>Φ(s))2] + Eω[(θ>Φ(s0))2]) — Eω[(θ>Φ(s))[
=) (Y- I)Eω [(θ>φ(S))2i
= -(1 - γ)θ>Eω[φ(s)φ(s)>]θ
(N)- λ2B kθk2,	(23)
where (i) uses the fact that s, s0 〜μω which is the stationary state distribution with the transition
kernel P and the policy πω, and (ii) uses Assumption 4 and we denote λB := 2(1 - γ)λφ > 0.
Finally, the item 3 can be proved via the following inequality.
(i)	2	2	2	2C
kθω k2 ≤ - 「 (θω )> Bω 霓 ≤ 「 kθω 小氏笃 k = 「 |俄 k&k ≤ Vb |四 k,	(24)
λB	λB	λB	λB
where (i) uses the item 2.	□
Lemma D.4. Under Assumptions 1-5 and choosing β ≤ min (8B, £, 1-CW), Nc ≥ (λ⅛ +
2β)192CB,+)λκ-1)ρ], Algorithm 2 has thefollowing convergence rate.
M	0	Tc
X E[∣lθTC+τc	—	θωt∣∣	∣ωt]	≤ σwCβ	c2	+ 2Mhc3(1	—8β)	+ Ni.	(25)
m=1	c
Moreover, to achieve PM=IE[∣∣θ(m+)T0 一。「』|2|力"≤ , we can choose Tc, Tc0 = O[ ln(-1)
and Nc = O(-1). Consequently, the sample complexity is TcNc = O -1 ln(-1) and the
communication complexity is Tc + Tc0 = O ln(-1) .
Proof. In Algorithm 2, by averaging the TD update rule (26) over the agents m ∈ M, we obtain that
the averaged critic parameter θt,t := M PM=I。，follows the following update rule
MM
θt,t0+1 =M X[X Wm"始+ β(Bt0 θ(m) + b(m))]
m=1 m0=1
MM
=MM x θ(m0 0+βMM X(Bto θ(mO)+bm
m0=1	m=1
=θt,t0 + β(Bt0 θt,t0 + bt0)	(26)
which can be viewed as a centralized TD update using the Markovian samples {Si, ai}i from the
transition kernel P and thejoint policy ∏t. Therefore, Theorem 4 in (60) can be directly applied to
analyze this centralized TD update and obtain the following convergence rate of θt,tf, since all the
conditions of that theorem are met 4.
E[∣∣θt,τc -θωt∣∣2∣ωt] ≤ (1 - Pβ)	E[∣∣θt,o -θωj∣2∣ωt]
+ (>+2β)
λB
192(CB R2 + C2)[1 +(K -I)ρ]
(1 - ρ)λBNc
≤ 2 (1- λB β )Tc (∣∣θ-1∣∣2+R2)+N
"	λB ΛTC	c1
≤ c3 C-彳β)	+ N.
(27)
where (i) uses the condition that β ≤ 4/Xb , the item 3 of Lemma D.3 and the constant that
ci := 1920(CBRI-Cb?21+(K 1)P], (ii) uses the constant that c3 := 2(∣∣θ-1∣∣2 + R2).
4We corrected the typo 1 — λBβ, which should be 1 — λBβ.
23
Under review as a conference paper at ICLR 2022
Next, we consider the consensus error k∆Θt,t0 k2F = PM=IIlθ(m) - θt,t0∣∣2 where We define
Θt,t0 := [θt(,1t)0, . . . , θt(,Mt0)]>. Note that the critic-step (26) can be rewritten into the following matrix
form
Θt,t0+1 = W Θt,t0 + β(θt,t0 B> + [b(1);...; b(M) ]>); t0 = 0,1,..., Tc - 1,
which further implies that for any t0 = 0, 1, . . . , Tc - 1,
(i)
∣∣∆"1∣∣F ≤ ∣∣w∆θt,t0∣∣F + β∣∣∆θt,tob>∣∣f + β∣∣∆[b(1);...；b(M)]>∣∣F
(28)
(ii)
ii)
≤ (σW+βCB)∆Θt,t0F+βtM	kbt(0m)k2
m=1
Ciii) 1 + σw
∆Θt,t0F+βMCb,
M
2
where (i) uses the item 1 of Lemma D.1, (ii) uses the item 3 of Lemma D.1 and the item 1 of Lemma
D.3, (iii) uses the condition that β ≤ 12CW and the item 1 of Lemma D.3. Telescoping the inequality
above yields that
I",tcIIf ≤ (1+σW)。|四,0||尸 +
2βMCb (i) 2βMCb
1 - σW 1 - σW
(29)
where (i) uses the equality that ∆Θ0 = O due to the initial condition that Θt,0 = [θ-1; . . . ; θ-1]>.
On the other hand, the final Tc0 local average steps in Algorithm 2 can be rewritten into the following
matrix form
Θt,t0+1 = W Θt,t0 ; t = Tc, Tc + 1, . . . , Tc + Tc0 - 1.
Hence, the average critic parameter θt,t does not change in these local average steps, i.e.,
θt,Tc +τc = m^θ>Tc+t0 1 = 0θ>t<w Tc )>1 = M θ>Tc1 = θt,Tc.
(30)
Therefore, we obtain that
M
X∣∣θ(mc+τc- % ∣∣2
m=1
M
X l∣θ(mT)+τ0-瓦,£+兄『=心"久+兄kF = HδwTcθt,τckF
m=1
(=i) kWTc0∆Θt,Tck2F(≤ii)σW2Tc0k∆Θt,Tck2F
(i≤ σWc (2βMCb)2(=)σWcβ2c2∕2
1 - σW
(31)
where (i) and (ii) use the items 1 and 3 of Lemma D.1 respectively, (iii) uses eq. (29), (iv) denotes
that c2 := 2( IMC )2. Combining eqs. (27) & (31) yields that
MM
X E[∣∣θ(mc+Tc -θω t∣2∣ωt] ≤ 2 X E[∣∣ 喀c+Tc - θt,τc ∣∣2 ∣ωt] +2M E[∣∣θt,τc -θωt∣∣[ωt]
m=1	m=1
≤ σWc β2c2 +2M hC3(1 - λB β )Tc + N i.
In the inequality above, replacing θt(,mT)+T 0 from Algorithm 2 by its corresponding variable θt(m) from
Algorithm 1 proves eq. (25). Finally, it can be easily verified that the following hyperparameter
choices make the error bound in (25) smaller than and also satisfy the conditions of Lemma D.4.
β = min
4	1 - σW
λB	2CB
= O(1)
Nc = max h(ɪ + 2β) 192CB[1 + (： - I)PI, 6Mcιe-1] = O(L)
λB	(1 - ρ)λB
24
Under review as a conference paper at ICLR 2022
Tc = l	皿6"-1) I m = O[ln(e-1)]
Iln [(1- λBβ∕4)-1]∣ L( J
Tc = 2l inSWF I=。/(E
□
Lemma D.5. For any ω, ω ∈ Ω, S ∈ S and a(m) ∈ Am (Am denotes the action SPacefor the agent
m), the following properties hold.
1.	kψ(m)(α(m)∣s)k ≤ Cψ,where ψωm)(α(m)∣s) := j(m) ln∏(m)(α(m)∣s).
2.	kψωm)(α(m)∣s) - ψωm)(α(m)∣s)k ≤ Lψ∣∣ω(m) - ω叫.
3.	dτv [∏ωmm (∙∣s),∏ωmm))(忖]≤ Ln ∣ω(m) - ω(m)∣.
4.	0 ≤ Vω (s), Qω (s, a) ≤ (1 - γ)R
max, 0 ≤ J(ω) ≤
Rmax.
5.	dTV[νω(∙∣s), νω(∙∣s)] ≤ Lν∣∣ω0 - ω∣ where LV := Ln[1+logρ(κ-1) + (1 — ρ)-1].
6.	dtv[Qe(s,α),Qω(s,α)] ≤ Lq|@ 一 ω∣ where LQ := 2Rm-γLν.
7.	J(ω) is LJ -smooth where LJ := Rmax(4Lν + Lψ )∕(1 - γ).
8.	∣∣VJ(ω)∣ ≤ Dj := c⅛Rmx.
9.	F(ω) is LF -LiPschitz where LF := 2Cψ (Ln Cψ + LνCψ + Lψ).
10.	h(ω) is Lh-LiPschitz where Lh := 2λF-1(DJλF-1LF + LJ).
Proof. For any ω(m),ω(m) ∈ Ωm, S ∈ S and a(m) ∈ Am, arbitrarily select ω(m0) = ω(m0) ∈ Ωm∕,
a(m0) ∈ Am0 for every m0 ∈ {1, ..., M}∕{m}. Denote ω = [ω(1); . . . ; ω(M)], ωe = [ωe (1) ; . . . ; ωe (M)],
a = [a(1), . . . , a(M)]. Notice that the joint score vector has the following decomposition
ψω(a|s) = [ψωi)(a⑴|s)；…；ψωM)(a(M)|s)].	(32)
Hence, the items 1 & 2 can be proved via the following two inequalities, respectively.
kψωm)(a(m)∣s)k≤ t
M
X kψωm0)(agO)IS)k2=) ∣ψω(a∣s)k
m0=1
(ii)
≤ Cψ .
∣ψωm)(a叫S)- ψωm)(a(m)∣s)k = ∣ψω(a∣s) - ψω(a∣s)∣
(i)
≤ Lψ∣ωe -ω∣ =Lψ∣ωe (m) - ω(m)∣
where (i) uses Assumption 2.
Next, we prove the item 3. Notice that
dτv [∏ω (∙∣s),∏ω (∙∣s)]
=sup ∣∏ω(A|s) - ∏ω(A|s)|
A⊂A
MM
(ii)
≥	sup	πωe(m0) (Am0 |S) -	πω(m0) (Am0 |S)
A1⊂A1,...,AM⊂AM	0 1	0 1
m=	m=
(iii)
M
SuP	I TT πω(m0) (AmO |s) I lπω(m) (Am|s) - πω(m) (Am |s) I
A1⊂A1,...,AM⊂AM	m0=1,m06=m
25
Under review as a conference paper at ICLR 2022
(=) sup ∣∏ω(m) (Am|s) - ∏ω(m)(Am∣s)∣ = dτv[∏ωm)) (∙∣s),∏ωm)) (∙∣s)],
Am⊂Am	ω	ω
where (i) denotes that ∏ω (A|s) = JA ∏ω (a∣s)da, (ii) uses the relation that ×m∈MAm ⊂ A, (iii) uses
our construction that ω(m0) = ωe(m0 )
∈ Ωm0, ∀m0 ∈ {1,…，M}∕{m}, and (iv) uses Am，= Amo to
achieve the supremum. Therefore, the item 2 can be proved via the following inequality.
dτv[∏ω2 (∙∣s),∏ωmm (∙∣s)] = dτv[∏ω (忖，限(忖]≤∏ IQ - ω∣∣ = Ln |@(m) - ω(m)k,
where (i) uses Assumption 2.
τhe item 4 can be proved by the following three inequalities that use Assumption 3.
∞∞
0 ≤ Vω (S)= Eω [X YtRt∣S0 = s] ≤ X YtRmaX= 1 max ,
t=0	t=0	1 - γ
0 ≤ Qω (s, a) = Es0~p(∙∣s,a)[R(s, a, S0) + γVω (s0)] ≤ Rmax + Y f ma； = : max
∞∞
0 ≤ J 3 = (I- Y)Eω [X YtRt] ≤ (I- Y) X YtRmax = Rmax
t=0	t=0
The proof of the items 5-7 can be found in the proof of Lemma 3, Lemma 4 and Proposition 1 of
(60), respectively.
Next, the item 8 is proved by the following inequality.
/▽J (M|| = IIEs~Vω,a~∏ω(∙∣s) [Qω (s, a)ψω (a∣S)]||
(i)	(ii) C R
≤ Es~Vω,a~∏ω (∙∣s)[lQω (s, a)l∣∣ψω (a|s)||] ≤ I -max
where (i) applies Jensen’s inequality, (ii) uses Assumption 2 and the item 4.
Next, the item 9 is proved by the following inequality.
∣F(ωe )-F(ω)∣
=IIEs~ν∏ω ,a~∏ω(∙∣s) [ψω⑷S)ψω(a∣S)>] - Es~ν∏ω ,a~∏ω(∙∣s) [ψω ⑷S)ψω ⑷S)>] ||
≤ HEs~ν∏ω ,a~∏ω (∙∣s) [ψω (。⑸砂⑦(a|S) ] - Es~ν∏ω,a~∏ω (∙∣s) [ψω (a∣6)ψω (a|S) ] 1
+ Es~ν∏ω ,a~∏ω (∙∣S) [H[ψω (HS)- ψω (a|S)]3 (a|S)>|]
+ Es~ν∏ω ,a~∏ω (∙∣s)[Uψω (。国[必(a|S) - ψω (a|S)]>|]
≤ ∣∣∣	[νωe (S)πωe (aIS) - νω (S)πω (aIS)][ψωe (aIS)ψωe (aIS)>]dSda∣∣∣ + 2CψLψkωe - ωk
≤ Cψ2	Iνωe (S)πωe (aIS) - νω(S)πω(aIS)IdSda + 2CψLψkωe - ωk
≤ Cψ /	νω(S)∣∏ω(a∣S) - ∏ω(a∣S)∣dSda
+ Cψ /	∏ω(a∣S)∣νω(s) - V(S)IdSda + 2CψLψHω 一 ω∣∣
(iii)
≤	2Lπ Cψ ∣∣ωe	- ω ∣∣ +	2Lν Cψ ∣∣ωe	- ω ∣∣ +	2Cψ Lψ ∣∣ωe	- ω ∣∣ :=	LF ∣∣ωe	- ω ∣∣
where (i) applies triangle inequality and then Jensen,s inequality to the norm k ∙ k, (ii) uses Assumption
2, (iii) uses the equality that S νω (S)dS = A πω(aIS)da = 1 as well as the inequlities that
JA ∣∏ω(a∣S) — ∏ω(a∣S)∣da = 2dτv [∏ω(∙∣s), ∏ω(∙∣s)] ≤ 2L∏∣∣ω — ω∣∣ (based on Assumption 2) and
that JS ∣νω(s) 一 νω(S)IdS = 2drv [νω(∙∣s), νω(∙∣s)] ≤ 2Lν∣∣ω0 - ω∣ (based on the item 5).
26
Under review as a conference paper at ICLR 2022
Finally, the item 10 is proved by the following inequality
h(ωe)-h(ω)
=∣∣F(ω)-1VJ(ω) - F(ω)-1VJ(ω)∣∣
≤ 2∣∣[F(ω)-1 - F(ω)-1]VJ(ω)∣∣ + 2∣∣F(ω)-1[VJ(ω) - VJ(ω)]k
(i)
≤ 2DJ ∣∣F (ω)-1[F (ω) -F(ωe )]F(ωe )-1∣∣ +2LJ∣∣F(ω)-1∣∣∣∣ωe -ω∣∣
(ii)
≤ 2DJ λF- LF ∣∣ωe - ω∣∣ + 2LJ λF- ∣∣ωe - ω∣∣ := Lh ∣∣ωe - ω∣∣,
where (i) uses the items 7 & 8, and (ii) uses the inequality that kF (ω)-1 k = λmax(F (ω)-1) =
λmin[F(ω)]-1 ≤ λ-1 for all ω (since F(ω) and F(ω)-1 are positive definite) and the item 9.	□
Next, we bound the approximation error of the following stochastic (partial) policy gradients.
1 (t+1)N -1
V ω(m) J Qt)= N	X	[Ri™ + γφ(si+1)>θ(m) - φ(Si)>θ(m)]ψ(m)(a(m)1Si), (33)
i=tN
VJ(ωt) := [Vω(i) J(ωt);…;Vω(M) J(ωt)],	(34)
Vω(m)J(ωt; Bt,k) :=N X [R(m) + γφ(si+ι)>θ(m) - φ(si)>θ(m)]ψ(m)(a(m)lsi),	(35)
Nk
i∈Bt,k
乙一， __	、 r ^	、	公	....
VbJ(ωt; Bt,k) := Vb ω(1) J (ωt); . . . ; Vb ω(M) J (ωt) .	(36)
Lemma D.6. Let Assumptions 1-5 hold and adopt the hyperparameters of the decentralized TD in
Algorithm 2following Lemma D.4. Choose T0 ≥ ?式工).Then, thefollowingproperties hold.
1.	The estimated average reward R(m) has thefollowing bias and variance bound.
M
X E[R(m) - RiIRi]2 ≤Mσ2T0Rmax,	(37)
m=1
M
X Var[R(m)∣Ri] ≤4Rmaxσ2,	(38)
m=1
where Ri := [Ri(1); . . . ; Ri(M )] denotes the joint reward.
2.	The stochastic policy gradients have the following error bound.
E[∣∣VJ(ωt) - VJ(ωt)∣∣2] ≤c4σW0 + c5β2σWc + C6(1 - λB/
+ N + N + 16Cψ ZOpPipicox	(39)
M
E[∣∣VJ(ωt; Bt,k) - VJ(ωt)∣∣2∣Ft,k] ≤c4σW0 + 16Cψ X ∣∣θ(m) Y,∣∣2
m=1
+ N + 16Cψ Z盗Cx,	(40)
where Ft,k ：= σ[Ft ∪ σ({si, ai, si+ι, si+ι, {e(m)}m∈M}i∈∪k二Bt .)].
Proof. We will first prove the item 1.
When Ri := [Ri(1); . . . ; Ri(M )] is given and fixed, the randomness of Rei(m) := Ri(m) (1 + ei(m))
and Vb ω(m) J (ωt) defined in eq. (4) only comes from the noises {ei(m)}mM=1. Since {ei(m)}mM=1
are independent noises with zero mean and variances σ12 , . . . ,σM2 , Rei := [Rei(1); . . . ; Rei(M )] has the
following moments
E[Rei |Ri] = Ri ,
27
Under review as a conference paper at ICLR 2022
covRei |Ri = diag(Ri(1))2σ12, . . . , (Ri(M))2σM2 := Σi.
Hence, Ri := [R(1),..., R(m)]> = WT0Ri (the second "=” comes from eq. (3) and the nota-
tions that Rei(m) := Rbi(,m0) and that Rbi(m) := Rbi(,mT)0) has the moment that ERbi|Ri = WT0 Ri and
CovRbi|Ri = WT0Σi(WT0)>. Therefore, eq. (37) can be proved as follows
M	22
X E[Rim) - Ri∣Ri]2 = ∣∣E[Rbi -RiIIRiI =IWTRi- Mm 11>Ri∣∣
m=1
≤ ∣∣WT0 - : 11>∣∣2kRik2 ≤) MσW0Rmax,
where 1 is a M -dim vector of 1’s, (i) uses the inequality that kRik2 = PmM=1(Ri(m))2 ≤ M R2max
(based on Assumption 3) and the item 4 of Lemma D.1. Then, eq. (38) can be proved as follows
M
X var [R(m) I Ri] = Var 限圈]=tr[(W T0)>∑iW T 0 ]
m=1
=trh(W T 0- M11DMW T 0- M 11>)>i+ trh(W T0»( M 11>)i
+trh( M 11>)二(W T 0 )>i+ trh( M11DM M 11>)i
≤) MRmaxσ2∣∣W T 0 - M 11>∣∣2 + M tr[W7211>] + MM2 "[1(/21)11
(ii)	0	2	0	1
≤ MRmaXσ%W + M 1τΣiWT 1 + M2 (IT2I)tr[1Τ1]
(iii)	3
≤ RmaX + M 1，i1
3M
= RmaXσ2 + M X (R(m))2σm
m=1
(iv)
≤ 4Rmax*
where (i) uses the equality that tr(Y >) = tr(Y ) and the inequality (41) below in which X =
W T 0 -吉 11> and the m-th entry of vm ∈ RM is 1 while its other entries are 0, (ii) uses the item 4
of Lemma D.1 and the equality that tr(xy>) = y>x for any x, y ∈ RM, (iii) uses the condition that
T 0 ≥ [ln M ]/[2 ln(σ-1)] and the item 1 OfLemmaD.1, (iv) uses Assumption 3.
MM
tr(XΣiX>) =tr(X>XΣi) = Xvm>X>XΣivm ≤ X kvmkkXk2kΣivmk
m=1	m=1
M
=X(R(m))2σmkXk2 ≤ MRmaXσ2kXk2.	(41)
m=1
Next, we will prove eq. (39) in the item 2, where the error term can be decomposed as follows
∣∣vJ(ωt) - VJ(ωt)∣∣2 ≤ 4 ∣∣VJ(ωt) - gt∣∣2 +4 ∣∣gt - g"∣2
'--------------------------------{z------}	'-----------}
(I)	(II)
+ 4∣∣g; - g；『+4 ∣扁-VJ(ωt)∣∣2,	(42)
×-----V----} X-------V--------}
(III)	(IV)
where we use the following notations that
gt := [gt(1); . . . ;gt(M)],	(43)
28
Under review as a conference paper at ICLR 2022
(t+1)N -1
g(m) := NN X	[Ri + γφ(si+ι)>θ(m) - Φ(Si)>θ(m)]ψ(m)(a(m)∣Si),	(44)
i=tN
1	(t+1)N -1
gt := N X	[Ri+γφ(si+1)>θωt -φ(Si)>θωt]ψt(aiιsi),	(45)
i=tN
gf, := Es"ωt ,。〜∏t(∙∣s),sθ 〜P(∙∣s,a)[R(s, a, SO) + γφ(SO)>θω t - φ(s)>θω t]ψt(alS)Iωt].	(46)
Conditioned on the following filtration
Ft :=σ[Ft ∪ σ({Si,ai,si+1}(t+NN-1)]
=σ({θ(m)}m∈M,0≤t0≤t ∪ {si, ai, si+1}(t++)1)N 1 ∪ {s(t+1)N } ∪ {{ei(m)}m∈M }tN—1),
the error term (I) can be bounded as follows.
EhIMJ(ωt) - gt,k『国
M
=e[ XI, ω(m) J (ωt) — 嫖 )∣∣2 国
m=1
M	(t+1)N -1	2
=) X E[∣∣ɪ X	(R(m)- Ri)ψ(m)(a(m)T ∣F0]
m=1	i=tN
M	(t+1)N-1
,XIN N	X	(R(m) - Ri)Ψ(m)(a(m)∣Si)∣Ft[∣∣
m=1	i=tN
M	(t+1)N-1
+ X VarhNN	X	(R(m) - Ri)ψ(m)(a(m)∣Si)∣Ft]
m=1	i=tN
M	(t+1)N -1
(≤) X∣EN X	(R(m) -Ri)IFt[ψ(m)(a(m)∣Si)∣∣
m=1	i=tN
M (t+1)N-1
+ N X X	Var[(Rim) -Ri)ψ(m)(aim)∣Si)∣F0]
m=1 i=tN
(i ) M	(t+1)N-1
≤ Xh N	X E(Rim)-Ri∣F0)i ∣∣Ψ(m)(a(m)∣Si)∣∣2
m=1	i=tN
M (t+1)N-1
+ N X X	∣∣ψ(m)(a(m)∣Si)∣∣2var[R(m) -RiIFtI
m=1 i=tN
(v)	2 M (t+1)N -1	2 (t+1)N -1 M
≤) Cψ X X	[E(R(m) -Ri∣F0)]2 + N X X var[Rim)∣F0]
m=1 i=tN	i=tN m=1
(vi)	C2
≤ Cψ (MσW Rmax) + Cψ (4Rmaxσ2)
=Cψ Rmax(Mσ2T0 + N σ2),	(47)
where (i) uses the definitions of Vω(m) J(ωt) and g(m) defined in eqs. (33) & (44) respectively, (ii)
uses the relation that EkXk2 = Var(X) + kEX k2 for any random vector X, (iii) uses the facts that
ψ(m)(a(m) ∣Si),Ri ∈ Ft are fixed while {R(m)}(t+NN-1 are random and independent given Ft, (iv)
uses the equality that Var(xY ) = Pjd=1 var(xyj) = Pjd=1 yj2var(x) = kyk2var(x) for any random
scalar x and fixed vector Y = [y1, . . . , yd] ∈ Rd (Here we denote y = ψt(m)(ai(m) |Si) ∈ FtO), (v)
applies Jensen,s inequality to the convex function (∙)2 and uses the item 1 of Lemma D.5 as well as
29
Under review as a conference paper at ICLR 2022
the fact that Ri ∈ Ft is fixed, (Vi) uses eqs. (37) & (38) and the fact that the conditional distribution
of Rim) on Ri ∈ F0 is the same as that on Ft since the noise e(m) is independent from any other
variables.
Then we bound the error term (II) of eq. (42) as follows.
M 1 (t+1)N-1	2
Ugt-g；『 =XIlN X	(hφ(Si+1)-φ(Si)]>(Mm)-θωj)ψ(m)(Wm)ISi)Il
m=1	i=tN
(i)	(t+1)N-1 M
≤ N X Xhφ(si+1)-φ(Si)llllθ(m)—《,『Mm)(Wm)ISi)『
i=tN m=1
≤) ； (IN TX Wi”
i=tN m=1
M
= 4Cψ Xllθ(m)-θωtll2,	(48)
m=1
where (i) applies Jensen,s inequality to the convex function ∣∣ ∙ ∣∣2, (ii) uses Assumption 4 and the
item 1 of Lemma D.5.
To bound the error term (III) of eq. (42), denote that
X(S,a,S0,e)= [R(S,a,e)+ γφ(e)>θωt - φ(S)>θω』@t(a|S),	(49)
which satisfies IlX (S,a,S0@|| ≤ [∣R(S,a,列 + ∣∣yΦ(J) + Φ(s) lll^t ll]llΨt(a∣S) ll ≤ Cψ (Rmax +
2Rθ) (the second ≤ uses the item 3 of LemmaD.3) and X = Esi〜”』X(Si,ai, Si+ι, Si+ι)∣Ft] = gJ=
where SN, ωt	∈	Ft	:= σ({θJo	'}m∈M,0≤t0≤t U {Si,	ai,	Si+1, {ei	)}m∈M}i=0	U {StN})	are	fixed.
Hence, Lemma D.2 yields that
1 (t+1)N-1	2
E[∣∣g=-gt∣ι2∣Ft] = E[∣lN	X	X(Si,ai,Si+1,Si+1)-X∣l ∣Ft]
i=tN
9Cψ (Rmax + 2Rθ)2(κ +1 - P)
N(I- P)
(50)
Next, we bound the error term (IV) of eq. (42). Notice that
9t -Vj(ωt)
=Eωt [(R(s, a,e + [Yφ(e - φ(s)]>θωt - [R(s,a,e) + YVωt (e - Vωt (S)])由(加)|"t]
= Eωt hγ[φ(Se)>θω=t	-	Vωt (Se)	-	[φ(S)>θω=t	-	Vωt(S)ψt(aIS)∣∣∣ωti.	(51)
Hence,
kg： - VJ(ωt)∣2 =回[(γ[Φ(S)>θωt - vω,⑶]-[φ"θω, - %⑸卜[⑸鼠]∣∣2
(≤i)Eωthlllγ[φ(Se)>θω=t	-Vωt(Se)	-	[φ(S)>θω=t	-Vωt(S)ψt(aIS)lll2∣∣∣ωti
(≤ii)2Cψ2Eωthγ2 lllφ(Se)>θω=t -Vωt(Se)lll2+ lllφ(S)>θω=t -Vωt(S)lll2∣∣∣ωti
= 2Cψ2γ2	lllφ(Se)>θω=t - Vωt (Se)lll νt(S)πt(aIS)P(SeIS, a)dSdadSe
+2Cψ2Eωthlllφ(S)>θω=t -Vωt(S)lll2∣∣∣ωti
(iii)	l	l2
≤ 2Cψ2 γ	llφ(Se)>θω=t - Vωt (Se)ll νt(S)πt(aIS)Pξ(SeIS, a)dSdadSe
30
Under review as a conference paper at ICLR 2022
+ 2CψEωt h∣∣φ(s)>θωt - VLt(s)∣∣2∣ωti
(=) 2Cψ (γ + 1)Eωt h∣∣Φ(s)>θLt - VLt (s)f∣ωt ]
(v)
2 critic
≤ 4Cψ ζapprox ,
(52)
where (i) applies Jensen's inequality to the convex function ∣∣ ∙ k2, (ii) uses the inequality that
kx + yk2 ≤ 2kxk2 + 2kyk2 for any x, y ∈ Rd, (iii) uses the inequality that P(s0|s, a) ≤
γ-1Pξ(s0∣s, a)； ∀s, s0 ∈ S,a ∈ A, (iv) uses the equality that Rs×a νt(s)∏t(a∣s)Pξ(e∣s, a)dsda =
νt(e), and (v) uses the notation that Zcpprox := supω Es 〜νω [∣VL (S) — φ(s)>θL∣2]. Substituting eqs.
(47),(48),(50)&(52) into eq. (42) yields that
E[∣∣v J (ωt)-VJ (ωt)∣∣2∣Ft ]
M
≤ 4CψRmax (Mσ2T0 + 02) +16Cψ X ∣∣θ(m) - 0：, ∣∣2
m=1
+
36Cψ (Rmax + 2Rθ)2(κ + 1 - P)
+ 16Cψ2 ζacpriptircox
N(1 - ρ)
M
c4σw + N + 16Cψ x ∣∣θ(m) - θLt ∣∣2 + 16Cψ Zcpprox,	(53)
m=1
where θt(m), ωt ∈ Ft are fixed, and we take the conditional expectation of eq. (47) on Ft ⊂ Ft0 and
denote that c4 := 4MCψRmmaχ, c7 := 16CψRmaxσ2 + 36Co(Rmax+-R0) (κ+1-ρ). SUbStitUting eq.
(25) into the unconditional expectation of eq. (53) yields that
E[∣∣VbJ(ωt) -VJ(ωt)∣∣2]
≤ c4σ2T + N7 + 16Cψ (σ2Tc β2c2+2M hc3 (1 — ^B β)	+ N i) + 16Cψ ZcpproX
=c4σ2T + c5β2σWc + c6 (1 - ^8Bβ)	+ N7 + N8 + 16CψZcpprox，
where we denote that c5 := 16c2Cψ2 , c6 := 32Mc3Cψ2 , c8:= 32Mc1Cψ2 . This proves eq. (39).
Equation (40) can be proved in the same way as that of proving eq. (53). There are two differences.
First, VJ(ωt; Bt,k) uses the minibatch Bt,k of size Nk while VJ(ωt) uses batchsize N . Second, eq.
(40)	is conditioned on the filtration Ft,k := σ[Ft∪σ({si, αi, si+ι, si+ι, {eim)}m∈M}i∈∪k-ι Bt 小)]
which includes not only the filtration Ft use by eq. (53) but also the minibatches ∪kk0-=10Bt,k0 used by
the previous (k 一 1) SGD steps.	□
Lemma D.7. Implementing Algorithm 3 with η ≤ 12,, T0 ≥ Cjn%、, Tz ≥ in(3D-Cψ),
2Cψ2	2 ln(σW-1)	z	ln(σW-1)
K ≥ -τ-rp,-ln3 /%一], N ≥、5 / J304、(κ+1-ρK 1)/2 and Nk H (1 一 ηλρ/2)-k/2, the involved
ln[(1-ηλF /2)-1] ,	ηλ5F (1-ρ)(1-ηλF /2)(K-1)/2	k	F	,
quantities have the following properties, where EL denotes the expectation under the underlying
distributions that S 〜 v：, α 〜n： (∙∣s).
1.	λF ≤ λmax[F(ω)] = ∣F (ω)∣ ≤ Cψ2, ∀ω.
2.	2 ≤ 1 - ηCψ ≤ ∣∣I 一 ηF(ω)∣∣ ≤ 1 - ηλF, so η ≤ 21F.
3.	Cψ-2	≤ ∣F(ω)-1∣	≤	λF-1.	For any ω, x ∈	Rdω,	x>F (ω)-1x ≥	Cψ-2∣x∣2.
4.	∣∣h(ω)∣∣ ≤ λdvj(ω)∣∣ ≤ DJ.
5.	h(ω) = arg min E： [(ψω (α∣s)>h - A： (s, a))2], so
h
园：[(砂：(a∣s)>h(ω) - A：(s,a)) ] ≤ Z^ox where S 〜v：, a 〜n：(∙∣s).
31
Under review as a conference paper at ICLR 2022
6.	Eω* [ψω (als)>h(ω) - Aω (s, a)] ≥ --C* Paox, ∀ω.
7 N __ N(1-ηλF/2)(KTi)/2(1-√1-ηλF∕2)	576Cψ(κ + 1-ρ)
'∙ Nk =	i-(i-ηλF/2)K/2	≥ —λF(1-p)一
8. ht approximates the natural gradient h(ωt) with the following error bound.
Eht - h(ωt)	≤ c10
(K-1)/2
+ c11σW2Tz + c12 σW2T 0 + c13β2σW2Tc0
+ c14 (1-------8Bβ)	+ N5 + c16ζcpipmx.	(54)
Proof. The item 1 is proved by the following inequality.
λF (≤i)λmin[F (ω)] ≤ λmax[F (ω)] (=ii) kF (ω)k
(iii)
=∣∣Eω[ψ(als)ψ(als)>]∣∣ ≤Eω[∣∣ψ(als)∣∣∣∣ψ(als)>∣∣] ≤ Cψ,
where (i) uses Assumption 6, (ii) uses the fact that F(ω) is positive definite implied by Assumption 6,
(iii) applies Jensen,s inequality to the convex function ∣∣ ∙ k and (iv) uses Assumption 2.
Next we will prove the item 2. On one hand,
(i)	1
λmin [I — nF(ω)] = 1 — ηλmax [F(ω)] ≥ 1 — ηCψ ≥ ],
where (i) uses the item 1, (ii) uses the condition that n ≤ 2Ct . On the other hand,
2Cψ
(55)
λmin [I - ηF(ω)] ≤ λmax[I - ηF(ω)] (=i) kI - ηF (ω)k = I - ηλmin[F (ω)] ≤ 1 - ηλF, (56)
where (i) uses the fact that I - ηF (ω) is positive definite based on eq. (55). Hence, eqs. (55) & (56)
prove the item 2.
The item 3 can be proved by the fact that F(ω)-1 is positive definite with minimum eigenvalue
λmax [F (ω)]-1 ≥ Cψ-2 and maximum eigenvalue λmin[F(ω)]-1 ≤ λF-1 implied by the item 1.
The item 4 can be proved by the following inequality.
(i)	(ii)
kh(ω)k =	∣∣F(ω)-1VJ(ω)∣∣	≤	∣∣F(ω-1)∣∣∣∣VJ(ω)∣∣	≤	λF-1∣∣VJ(ω)∣∣	≤	λF-1DJ,
where (i) uses the item 3 and (ii) uses the item 8 of Lemma D.5.
Next we will prove item 5.
Consider the following function of x ∈ Rdω .
fω (x) = 1 Eω[(ψω (a∣s)>X - Aω (s,。))]
=2 X>Eω [ψω (a∣s)ψω (a∣s)>]x - Eω [Aω (s, a)ψω (a∣s)]>x + 1 Eω [Aω (s,。)2]
=2x>F(ω)x — V J(ω)>x + ]Eω [Aω(s, a)2]
Since V2f (ω) = F(ω) is positive definite, f is strongly convex quardratic and thus it has unique
minimizer h(ω) = F (ω)-1VJ (ω) obtained by solving h from the equation Vfω (h) = F(ω)h -
VJ(ω) = 0. Hence,
Eω [∣∣Ψω(a∣s)>h(ω) - Aω(s, a)∣∣2]
=minEω [(Ψω(a∣s)>h — Aω(s, a))2]
(57)
32
Under review as a conference paper at ICLR 2022
which proves the item 5.
The item 6 can be proved by the following inequality.
—
/ Vω*(s)∏ω*(a∣s)[Aω(s,a) - ψω(a|s)>h(ω)] dsda
Zν (s)πω (a|s) ⅛⅛⅛⅞2 人 (s,a)
—
Eω h 工(s)∏ω(aaS))凡 (s, a)- ψω (aιs)>h(ω)]i
≤ ʌ 1[("：*(：)=* (；可 JEω[(Aω (Zay- Ψω (a| S)> h3) 2] 2 C*
νω (s)πω (a|s)
rrox ,	(58)
where (i) uses Assumption 7 and the item 5. Multiplying -1 to the above inequality proves the item
6.
Next, the item 7 can be proved as follows.
Ox (i-ηλF/2)-k/2
Nk = N----~1-----------------
PK-I(I- ηλF /2 …
_n(1 - ηλF/2)(KT-k)/2(i - P - ηλF/2)
=	1 - (1 - ηλF/2)K/2
(E)	2304Cψψ (κ + 1 — P)	(1 — ηλF/2)(K-I)/2(〃人尸/2)
≥ ηλF(1 - ρ)(1 - ηλF/2)(KT)/2	1 + p∖ - 〃-尸/
≥ 576Cψψ (K +1- P)
≥ ~λF (1 - P)~,
where (i) uses the conditions that Nk H (1 - ηλF/2)-k/2 and PK--1 Nk = N and (ii) uses the
2304Cψ4 (κ+1-ρ)
COnditiOn that N ≥ ηλF(i-p)(i-niF/2)(K-i)/2
Finally, we will prove the item 8. Until the end of this proof, we use the underlying distribution that
ai 〜 ∏t(∙∣Si),Si+ι 〜 Pξ(∙∣Si, ai) for tN ≤ i ≤ (t + 1)N - 1 in the t-th iteration of the multi-agent
NAC algOrithm (AlgOrithm 1).
The local averaging steps of Zi,' := [z(1),..., Z(M)]> yield the following consensus error bound.
M	(ii)
X (ZTm) - ZTz)2 = k∆zi,Tzk2 = k∆WTTZi,0k2 =) kWTT∆zi,0k2 ≤ σWzk∆zi,ok2
m=i
(iii)	M	M
≤ σWz X dm))2 =σWz X [ψ"(a"si)>h(m)]2
m=i	m=i
(iv)	M
≤ Cψ2σW2TTXht(,mk)2≤Cψ2σW2TTht,k2,
m=i
where ZTN :=a PM=I z(m), (i) and (ii) use the items 1 and 3 of Lemma D.1 respectively, (iii) uses
the equality that k∆k = 1, and (iv) uses the item 1 of Lemma D.5.
Then, we define the following stochastic gradients of function fω .
V ω(m)fωt (ht,k): = Nr X WmXaWsiMMa/sjTh&k -申 ω(m) J (ωt Bt,k )
k i∈Bt,k
Vfωt(ht,k) :=N ∑ Ψt(ai∣Si)Ψt(ai∣Si)>ht,k -VJ(ωt；Bt,k)
Nk i∈Bt,k
33
Under review as a conference paper at ICLR 2022
=[V ω(ι) fωt (ht,k); ... ； V ω(M) fωt (ht,k )],
V ω(m) fωt (ht,k) ： = M X Ψ(m)(a(m)∣Si)z(mZ-V ω(m) J Bt,k ),
k i∈Bt,k
V fωt (ht,k) ：=[V ω(ι) fωt (ht,k)；…；V ωM) fωt (ht,k )],
where Vω(m) J(ωt; Bt,k) and VJ(ωt; Bt,k) are defined in eqs. (35) & (36) respectively. Hence,
Vbfωt(ht,k)-Vefωt(ht,k)2
M
= X Vbω(m) fωt (ht,k) - Ve ω(m) fωt (ht,k)
m=1
M 1	2
=X IlN X [Mz(mZ-MgSi)Tht,k]ψ(m)(aiιsi)∣∣
(i)	M
≤ N XXIIM(端Z-zTz)ψtm)(ai|Si)II
i∈Bt,k m=1
(ii)	M2 C2	M
≤) -Nrψ X X(z(,mZ -ZTz)2 ≤M2cψσwz∖∖ht,kf.	(59)
k	i∈Bt,k m=1
where (i) uses the equality that ψt(ai∖si)>ht,k = Pm∈M Zm) = —Zτz, (ii) uses the item 1 of
Lemma D.5.
Since, ωt, ht,k ∈ Ft,k while {Si , ai }i∈Bt,k are random. Hence,
E[IIVbfωt(ht,k)-Vfωt(ht,k)II2Ft,k]
=EhII N- X [ψt (ai∖si)ψt(ai∖si)>]ht,k -V J	BtQ- F (ωt)ht,k + VJ (st"归川
k i∈Bt,k
≤ 2EhII N X [ψt(ai ∖si)ψt(ai∖si)>] - F (ωt)II kht,kk2∣Ft,ki
k i∈Bt,k
+ 2E[II VJ(ωt; Bt,k) - VJ(ωt) I2 ∣Ft,k]
(=) 2E∣II N X [Ψt(ai∖si)Ψt(ai∖si)>] - F (ωt)]2∣Ft,k ^kf
k i∈Bt,k
+ 2E[IIVJ(ωt; Bt,k) -VJ(ωt)II2∣Ft,k]
Ciii) 18Cψ(K + 1 - P)	2	2T0	2c7	32C2 X IIθ(m)	θ* II2 ∣ 32C2 广皿比
≤ -----N (1 - )-----kht,k k + 2c4σw + N- +32Cψ ʌ, IIθt - θωtII +32Cψ ZapprOx,
k( - P)	k	m=1
(60)
where (i) uses the inequalities that kx + yk2 ≤ 2kxk2 + 2kyk2 for any x, y ∈ Rd, (ii) uses
the fact that ht,k ∈ Ft,k, and (iii) uses eq. (40) and applies Lemma D.2 to the quantity that
X(s, a, s0, se) = ψt(a∖s)ψt(a∖s)> in which ωt ∈ Ft,k is fixed and kX(s, a, s0, se)kF ≤Cψ2 .
Combining eqs. (59) & (60) yields that
E[IIVbfωt(ht,k)-Vfωt(ht,k)II2∣∣Ft,k]
≤2E[IIVbfωt(ht,k)-Vefωt(ht,k)II2∣∣Ft,k] +2E[IIVefωt(ht,k)-Vfωt(ht,k)II2∣∣Ft,k]
≤Cψ [2—2σWz + % + 1- P) i kht,k k2 +4c4σ2T0
Nk (1 - P)
4c	M
+ N-+64Cψ X IIθt	- θ±t II + 64CψZapprOx.	(61)
Nk	m=1
34
Under review as a conference paper at ICLR 2022
Therefore,
Eht,k+1 -h(ωt)2Ft,k
=E[∣∣ht,k - η fωt (ht,k ) - h(ωt)∣∣2∣Ft,k ]
(i)	2
≤ (I + ηλF )E[∣∣ht,k - ηVfωt (ht,k) - h(ωt) ∣∣ IFt,k ]
+ [1 + (ηλF)-1]E[∣∣η[v.(ht,k) - f (ht,k)] ∣∣2∣Ft,k]
= (1 + ηλF)∣∣ht,k - ηF (ωt)[ht,k - h(ωt)] - h(ωt)∣∣
+ η(η + λ-1)E[∣∣Vfωt(ht,k) - vfωt(ht,k)∣∣2∣Ft,k]
= (1 + ηλF)∣∣[I - ηF (ωt)][ht,k - h(ωt)]∣∣2
+ η(η + λ-1)E[∣∣[Vfωt(ht,k) - vfωt(ht,k)]∣∣2∣Ft,k]
(iii)
≤ (1 + ηλF)(1 - ηλF) ∣∣ht,k - h(ωt)∣∣
+ 2η (Cψ [2M2σWz + 3弋 +1- P) i kht,k k2 + 4c4σ2T0
λF	Nk (1 - ρ)
M
+ N+64Cψ X ∣∣θtm - θω11∣ + 64CψZcpprox)
Nk	m=1
≤ (1 - ηλF)∣∣ht,k - h(ωt)∣∣
+ ∙∣η (2Cψ h2M 2σWz + a，：； i~~F i (kht,k- h(ωt)k2 + ιιh(ωt)k2)
λF	Nk (1 - P)
M
+4c4σw + N-+64Cψ X ∣∣θtm - θωt∣∣ + 64CψZcpProx)
Nk	m=1
(≤) (1 - * ∣∣ht,k - h(ωt)∣∣2 + ≡ (2Cψ [2M2σWz + 3N^i D
M
+4c4σw + N-+64Cψ X ∣∣θtm - % ∣∣ + 64Cψ ZcpProx)
Nk	m=1
(≤) (1 - *)∣∣ht,k- h(ωt)∣∣2 + λ8η(CψM2σWz + N
M
+ c4 σ2T +16Cψ X ∣∣θ(m)- θωχ『+ 16Cψ ZcpProx)
m=1
where (i) uses the inequality that ιx+yι2 ≤ (1 + ηλF)ιxι2 + [1 + (ηλF)-1]ιyι2 for any x, y ∈ Rd,
(ii) uses the notation that vfωt (h) = F (ωt)h - vJ(ωt) = F (ωt)[h - h(ωt)] and the fact that
ωt, ht,k ∈ Ft,k, (iii) uses eq. (61) and the item 2 of this Lemma, (iv) uses the conditions that Tz ≥
ln(3DJ Cψ2 )	18Cψ4 DJ2 (κ+1-ρ)
,/ -ι ψ and the item 7 of this Lemma, and (V) uses the notation that c9 := ——ψ2J——ʌ-------+ c7.
ln(σW-1)	λ2F (1-ρ)
Then, taking unconditional expectation of the aboVe inequality and iterating it oVer k = 0, 1, . . . , K-1
yield that
E[∣∣ht-h(ωt)∣∣2] = E[∣∣ht,K - h(ωt)∣∣2]
≤
(1 - ηλF)KE[∣∣ht,0-h(ωt)∣∣2] +
K-1
X
k=0
K-1-k
M
(Cψ M2 σWz + N9+c4σ2T +16Cψ X E[∣∣θ(m)- θω 11∣2] + 16Cψ ZcpProx)
Nk
m=1
≤) (I- *)KE[∣∣ht-i-h(ωt)∣∣2]
35
Under review as a conference paper at ICLR 2022
M
+ λ (CψM σwz+c4σw +16Cψ X 叫帆团-唠 ∣∣ ] + 16CψZcPprOx)
F	m=1
+
8ηc9 [1 - (1 - ηλF/2)K/2] K-
(K-1-k)/2
(ii)
≤
NλF(I - 1- - ηλF/2) k=0 ∖	2，
--η2F) Emht-i-以也/]+ λ(CψM2σWz + c4σW +16CψZcpprox)
+ 256C2 (σWcβ2C2 + 2M [C3(1-? β)Tc + N i) +
8ηc9
(iii)
≤
ENhtT - h(Gt)I1]+ λ2 (Cψ M 2σWz
λF
+ 等(σWcβ2 C2 + 2M [C3(1-λB β)Tc + Nc i) +
NλF(1 - √1 - ηλF/2)2
+ c4σW2T + 16Cψ2 ζacpriptircox
128c9
NηλF
(62)
≤ 3(1 - η2F)	E[∣lht-ι-h(ωt-ι)∣∣2 + IlhQt-I)II2 + Ii -h(ωt)∣∣2]
+ λ (CΨ M 2σWz+c4σW +16Cψ Zcpprox)
λF
+ 臂(σWcβ2 C2 + 2M [C3(1-λB B)TT + Nc i) +
≤ 3(1 -等)KE[∣∣ht-i-h(ωt-i)∣∣2]+6Dj (1 - ηλF
F
+ λ (CΨ M 2σWz + c4σW +16Cψ ZcPprox)
λF
128c9
NηλF
K
+ 256C2 1Wβ2C2 + 2M hC3(i - λBβ'
Tc+Nc i)+冷
where (i) uses the notation that ht,0 = ht, the
that PK=o1 (1 - η2F)K 1 k ≤ 岛,(ii) uses
item 7 of this Lemma and the inequality
Lemma D.4, (iii) uses the inequality that
(I+√1-ηλF/2)
(1-√1-ηλF/2)2 =	(ηλF/2)2
2	16
一 ≤ (ηλ6)2 implied by the item 2 of this Lemma, (iv) uses
the inequality that kx + y + z k2≤ 3kxk2 + 3kyk2 + 3kz k2, ∀x, y, z ∈ Rd , and (v) uses the items 4
of this Lemma. Taking unconditional expectation of the above inequality and iterating it over t yield
that
E[∣∣ht-h(ωt)∣∣2]
≤)[3(1- *)KitE[∣∣h0-h(ω0)∣∣2] +12D2 (1 - ηλF
F
+ λ (cΨ M 2σWz+c4σ2T +16Cψ ZCPprox)
λF
+ 等(σWcβ2C2 + 2M [c3 (-M β)Tc + NC i) +
(≤)[3(1-* )Kith(1-* )K E[∣∣h-i-h(ω0)∣∣2]
+ λ2 (CΨ M 2σWz+c4σ2T +16Cψ ZCPprox)
λF
256c9
NηλF
1
K
+ 干(σWcβ2C2 + 2M [c3 (l-λB β)Tc + NC i) +
12DJ (1	ηλF) K	32 (C4 M2 2Tz	2T0 , 16C2 广CritiC )
+	λ- (1	2- J	+ ʌr ∖CψM σW + c4σW +16CψQapproxJ
FF
36
Under review as a conference paper at ICLR 2022
l 512Cψ ( 2T0 q2 l ∣^ <1 λB β∖τc c c1 ]ʌ l 256C9
+ FFvW β c2 Mc3( ɪβ + Nd + NηλF
(≤i) 2(1-粤V (…2 + ID)
121J	ηλF∖K	48	4 4zf242Tz , C ∕2T0	2 广Critic )
+^-λ2- 11-2- J	+ λ2~ ∖CΨ M σW + c4σW +16CψZapprox)
FF
+ 76λC2 (σWcβ2C2 + 2M [C3 (1-λB β 广 + NC ])
384c9 ηλF(1 - ρ)(1 - ηλp/2)(KT)/2
ηλ3F	2304Cψ (K + 1 — P)
(≤)ci0 (1 - *)(KT)/2 + ,MW。+ ci2σ2τ 0 + c-.
+c14 (1 —8B β)	+ N5+ci6ζcpprox
where (i) uses the inequality that 3(1 - ηλF/2)k
K ≥ m[(i-n*F/2)-i], Qi) uses eq. (62) with t =
2304Cψ4 (κ+1-ρ)
nλF(i-ρ)(i-ηλF∕2)(κ-i)∕2 as Well as the inequalities
≤ 1 implied by the condition that
0, (iii) uses the condition that N ≥
that h-1 - h(ω0) ≤ 2h-1 +
*
2h(ω0)	≤ 2h-1	+ 2IJ2 λF-2 (* uses the item 4 of this Lemma) and that 3(1 - ηλF /2)K ≤ 1,
768c20ψ
(iv) denotes that c10 :
*, C12 ：=好,C13 ：
λF	λF
1536M c3 Cψ2	1536M c1 Cψ2	768Cψ2
一λF —, c15 := —λF —, c16 := F-
c14 :
This proves the item 8 of this Lemma.
□
E Experiment Setup and Additional Results
In this section, we present all the details of experiment setup and additional experiment results.
E.1 Experiment Setup
We simulate a fully decentralized ring network with 6 fully decentralized agents, using communication
matrix with diagonal entries 0.4 and off-diagnonal entries 0.3. The shared state space contains 5
states and each agent can take 2 actions. We adopt the softmax policy ∏ω(a|s) 8 eωs,a. The entries
of the transition kernel and the reward functions are independently generated from the standard
Gaussian distribution (with proper normalization of the absolute value for the transition kernel). We
use the rows of a 5-dimensional identity matrix as state features. We set the discount factor γ = 0.95.
We implement and compare four decentralized AC-type algorithms in this multi-agent MDP: our
decentralized AC in Algorithm 1, our decentralized NAC in Algorithm 3, an existing decentralized
AC algorithm (Algorithm 2 of (70)) that uses a linear model to parameterize the agents, averaged
reward R(s, a, s0) = Pi λifi(s, a, s0) (we name it DAC-RPI for decentralized AC with reward
parameterization) 5, and our proposed modified version of DAC-RP1 to incorporate minibatch,
which we refer to as DAC-RP100 with batch size N = 100. For our Algorithm 1, we choose
T = 500, Tc = 50, Tc0 = 10, Nc = 10, T0 = Tz = 5, β = 0.5, {σm}6 *m=1 = 0.1, and consider
batch size choices N = 100, 500, 2000. Algorithm 3 uses the same hyperparameters as those of
Algorithm 1 except that T = 2000 in Algorithm 3. We select α = 10, 50, 200 for Algorithm 1 with
N = 100, 500, 2000 respectively, and Tz = 5, α = 0.1, 0.5, 2, η = 0.04, 0.2, 0.8, K = 50, 100, 200,
Nk ≡ 2, 5, 10 for Algorithm 3 with N = 100, 500, 2000, respectively. For DAC-RP1 that was
originally designed for discount factor γ = 1, we slightly adjust it to fit our setting where 0 < γ < 16.
5The original algorithm in (70) uses the parameterization R(s, a) = Pi λifi(s, a), and we extend to our
setting where the rewards also depend on the next state s0 .
6(70) defined the Q-function Qθ (s, a) = E [rt+ι — J(θ)] for policy parameter θ and used the temporal
differences δi = ri+ι — μi + Vt+ι(vi) — Vt(Vi) and ei = Rt(λt) — μi + Vt+ι(vi) — Vt(Vt) for critic
37
Under review as a conference paper at ICLR 2022
Figure 2: Comparison of ∣∣VJ(ωt)k2 among decentralized AC-type algorithms for ring network .
For this adjusted DAC-RP1, we select diminishing stepsizes βθ = 2(t + 1)-0.9, βv = 5(t + 1)-0.8
as recommended in (70) and use the rows of a 1600-dimensional identity gatrix as the reward
features {fi(s, a, s0) : s, s0 ∈ S, a ∈ A} (i = 1, 2,..., 1600) to fully express R(s, a, s0) over all the
5 × 26 × 5 = 1600 triplets (s, a, s0). DAC-RP100 has batchsizes 100 and 10 for actor and critic
updates respectively, and selects constant stepsizes βv = 0.5, βθ = 10. This setting is similar to
Algorithm 1 with N = 100 to inspect the reason of performance difference between Algorithm 1 and
DAC-RP1. All the algorithms are repeated 10 times using initial state 0 and the same initial actor
parameter ω0 generated from standard Gaussian distribution.
E.2 Gradient Norm Convergence Results in Ring Network
Figure 2 plots ∣∣VJ(ωt)∣2 v.s. communication complexity (t(Tc + TC + T0) = 65t, t(Tc + TC +
T0 + Tz ) = 70t and 2t for Algorithms 1 & 3, and both DAC-RP algorithms, respectively)7 and
sample complexity (t(TCNC + N), 2t and 110t for both of our AC-type algorithms, DAC-RP1 and
DAC-RP100, respectively).8 For each curve, its upper and lower envelopes denote the 95% and 5%
percentiles of the 10 repetitions, respectively.
Similar to the result of accumulative reward J(ωt) shown in Figure 1, it can be seen from Figure 2 that
the communication and sample efficiency of both our decentralized AC and NAC algorithms improve
with larger batchsize due to reduced gradient variance, which matches our understanding in Theorems
1 & 2. Our decentralized AC and NAC algorithms significantly outperform DAC-RP1 which has
batchsize 1. Using mini-batch, DAC-RP100 outperforms a lot than DAC-RP1, and converges to
critical points earlier than Algorithm 1. However, it can be seen from Figure 1 that such early
convergence turns out to have much lower J(ωt) than Algorithm 1 with N = 100 and NC = 10.
Such a performance gap is caused by two reasons: (i) Both DAC-RP1 and DAC-RP100 suffer from
an inaccurate parameterized estimation of the averaged reward, and the mean relative estimation
errors of both DAC-RP1 and DAC-RP100 are over 100% 9. In contrast, our noisy averaged reward
estimation achieves a mean relative error in the range of 10-5 r^ 10-4. 10 ; (ii) Both DAC-RP1
update and actor update respectively. To fit 0 < γ < 1, we use δti = rti+1 + γVt+1 (vti) - Vt(vti) and
弋i	i	i	i	i
δt = Rt(λt) + γVt+ι(Vt) - Vt(Vt) where μt ≈ J(θt) is removed since Qθ(s, a) = E(rt+ι). In addition, we
used two different chains generated from transition kernels P , Pξ respectively for critic update and actor update
as in our Algorithm 1.
7Each update of our decentralized AC uses Tc + Tc0 and T0 communication rounds for synchronizing critic
model and rewards, respectively. Each update of our decentralized NAC uses Tc + Tc0, T 0, Tz0 communication
rounds for synchronizing critic model, rewards and scalar z , respectively. Each update of both DAC-RP1 and
DAC-RP100 uses 1 communication round for synchronizing V and λ respectively.
8DAC-RP1 uses 1 sample for actor and critic updates respectively. DAC-RP100 uses 100 and 10 samples for
actor and critic updates respectively.
9The relative reward estimation error at the t-th iteration of both DAC-RPI and DAC-RP100 is de-
fined as A/B where A = m1s：2∣a∣ Pm=I PsH∈s Pa∈A[R(S, a, s0) - Pi λ(m)fi(S, a, s0)]2 and B =
∣S⅛Γ EsH∈S ∑a∈A R(s,a,s0)2.
10At the t-th iteration of Algorithms 1 & 3, we focus on r(m) = N Pi==INN-1 R(m) as the estimation of
the batch-averaged reward r∣ = N P(t+NN-1 Ri since its estimation error affects the accuracy of the policy
gradient (4). The relative estimation error is defined as Mr2 PM=1 (r∣m) — rt)2.
38
Under review as a conference paper at ICLR 2022
and DAC-RP100 apply only a single TD update per-round, and hence suffers from a larger mean
TD learning error (about 2% and 1% for DAC-RP1 and DAC-RP100, respectively), whereas our
algorithms perform multiple TD learning updates per-round and achieve a smaller mean relative error
(about 0.3% and 0.07% for our decentralized AC and NAC respectively) 11. All these relative errors
are averaged over iterations.
E.3 Additional Experiments in Fully Connected Network
To investigate the effect of network topology on the performance of our algorithms, we also conduct
the above experiments on a fully connected network with 6 fully decentralized agents, using commu-
nication matrix with diagonal entries 0.4 and all the other entries 0.12. The MDP environment and all
the hyperparameters are the same as the above experiments for ring network. Figures 3 & 4 plot the
learning curves of the optimality gap J * - J (ωt) and ∣∣VJ (ωt)k2 respectively for fully connected
network. To make comparison, We plot J* — J(ωt) and ∣∣VJ(ωt)∣ in Figures 5 & 2 respectively for
the above experiments with ring network. It can be seen by comparing these figures that network
topology does not much affect the performance of these algorithms, so the conclusions for ring
network that we summarized right before this subsection also holds for fully connected network.
complexity
Sample
complexity
Communication
complexity
Figure 3: Comparison of optimality gap J(ω*) - J(ωt) among decentralized AC-type algorithms in
fully connected network.
complexity
Figure 4: Comparison of ∣∣VJ(ωt) ∣∣2 among decentralized AC-type algorithms in fully connected
network.
Figure 5: Comparison of optimality gap J(ω*) - J(ωt) among decentralized AC-type algorithms in
ring network.
11The TD error at the t-th iteration is defined as M册七 ∣∣2 PM=IIlθ(mɔ - θ[tk2.
39
Under review as a conference paper at ICLR 2022
E.4 Two-agent Cliff Navigation
Figure 6: Two-agent cliff navigation. (“S”, “X”,
“D” denote starting point, cliff and destination re-
spectively. The optimal path is shown in red.)
In this subsection, we test our algorithms in solv-
ing a two-agent Cliff Navigation problem (39)
in a grid-world environment. This problem is
adapted from its single-agent version (see Ex-
ample 6.6 of (48)). As illustrated in Figure 6,
two agents start from the starting point “S” on
a 3 × 4 grid and aim to reach the destination
“D”. Here, global state is defined as the joint lo-
cation of the two agents, and there are in total
(3 × 4)2 = 144 global states. In most states,
an agent can choose to move up, down, left or
right by one step and receives -1 reward. How-
ever, once an agent falls into the cliff “X”, it
will return to the starting point “S” and receives
-100 reward. When an agent reaches “D”, it
will always stay at “D”, and receives 0 reward
if the other agent also reaches/stays at “D”, or
receives -0.5 reward otherwise. If an agent is not at “X” or “D” and selects a direction that points
outside the grid, then it stays in the previous location and receives -1 reward. The optimal path
for both agents is the red path shown in Figure 6, which has the minimum accumulative reward
J* = -0.1855 under the discount factor Y = 0.95.
For our Algorithm 1, we choose T = 500, Tc = 50, Tc0 = 10, Nc = 10, T0 = Tz = 5, β = 0.5,
{σm}6m=1 = 0.1, and consider batch size choices N = 100, 500, 2000. Our Algorithm 3 uses the
same hyperparameters as those of Algorithm 1 except that we choose T = 2000. We select α =
1,5,20forAlgorithm 1 with N = 100,500,2000respectively, and Tz = 5, α = 0.002, 0.01, 0.04,
η = 0.002, 0.01, 0.04, K = 50, 100, 200, Nk ≡ 2, 5, 10 for Algorithm 3 with N = 100, 500, 2000,
respectively. For DAC-RP1, we select T = 10000, βv = 10(t + 1)-0.6 and βθ = 5(t + 1)-0.6. For
DAC-RP100, we use T = 2000 and batchsizes 100 and 10 for actor and critic updates respectively,
and selects constant stepsizes βv = 0.5, βθ = 1. This setting is similar to Algorithm 1 with N = 100
to inspect performance difference between Algorithm 1 and DAC-RP1.
Figure 7: Comparison of optimality gap J(ω*) - J(ωt) among decentralized AC-type algorithms on
cliff navigation.
Figure 8: Comparison of optimality gap J(ω*) - J(ωt) among decentralized AC-type algorithms on
cliff navigation.
40
Under review as a conference paper at ICLR 2022
We plot J* - J(ωt) and ∣∣VJ(ωt)k in Figures 7&8 respectively. It can be seen from these figures
that both our Algorithm 1 & Algorithm 3 significantly reduce the function value gap J* - J(ωt), and
their convergence is faster with a larger batchsize. In contrast, the function value gaps of DAC-RP1
and DAC-RP100 do not decrease sufficiently and converge to a high value. In particular, since
DAC-RP100 achieves a larger function value gap than our Algorithm 1 with N = 100 while their
hyperparameter choices are similar, we attribute this performance gap to the inaccurate average
reward estimation and TD error, as we analyzed in Appendix E.2.
F Constant scalars
The following global constants are frequently used.
M : The number of agents.
γ : Discount rate.
Rmax : The reward bound such that 0 ≤ R(m) (s, a, s0) ≤ Rmax for any s, s0 ∈ S and a ∈ A
(Assumption 3). Hence, 0 ≤ Rm) (s, a, s0), Rm),Ri ≤ Rmax.
σW ∈ [0, 1): The second largest singular value of W.
ω* := maxω J(ω) denotes the optimal policy parameter.
The following constants are defined in Lemma D.3.
CB := 1+γ.
Cb := Rmax .
λφ := λmin(Es〜μω [φ(s)φ(s)>]) > 0 satisfies Assumption 4.
λB := 2(1 - γ)λφ > 0. (Assumption 4 implies that λφ > 0.)
Rθ ：= 2Cb.
The policy-related norm bounds and Lipschitz parameters are defined as follows.
Cψ,Lψ,L∏ > 0 defined in Assumption 2: For all S ∈ S, a ∈ A and ω,ω, ∣ψω(a∣s)∣ ≤ Cψ,
∣∣ψω(a|s) - Ψω(a∣s)k ≤ Lψ ∣ω - ω∣ and ʤ(∏ω(∙∣s),∏ω(∙∣s)) ≤ Ln∣∣ω - ω∣.
Lν := Lπ[1 + logρ (κ-1) + (1 - ρ)-1].
LQ2RmaxLν
：=-ι-γ-.
LJ ：= Rmax(4Lν + Lψ )/(1 - Y).
DJ :
CψRmax
1-γ
LF := 2Cψ(LπCψ + LνCψ + Lψ).
Lh := 2λ-1(Djλ-1LF + Lj) where λp := infω∈Ω λmin[F(ω)] > 0 (λmin denotes the minimum
eigenvalue) which satisfies Assumption 6.
The following constants are defined to simplify the notations in the proof.
c1 :
1920(CB R+C2)[1 + (κ-1)ρ]
C2 ：=2(IMB
(I-P)λB
2
c3 := 2(∣∣θ-ι∣∣2 + R) where θ-ι is the initial parameter of decentralized TD (Algorithm 2).
C4=4MCψ Rm1aχ.
41
Under review as a conference paper at ICLR 2022
c5 :=	16c2Cψ2.
c6 :=	32Mc3Cψ2.
c7 :=	16C2 R σ2 + 36CΨ (Rmax+ 2Rθ )2(κ+1-ρ)
c8 :=	32Mc1Cψ2.
c9 :=	18Cψ DJ (κ+1-ρ) l -λF (i-ρ)- + c7.
c10 :	二 2 k h-1 k2 + 14DDJ- + c9λ4F where h-1 is the initial natural gradient of Algorithm 3. λF	Cψ _ 48Cψ M2
c11 :	=λF .
c12 :	48c4 -H.
c13 :	_ 768c20ψ 一~~^Γ~.
c14 :	_ 1536Mc3Cψ =-λF —.
c15 :	1536M c1 Cψ2 =-λF-. _ 768Cψ
c16 :	一^λ∙Γ.
c17 :	二 Es〜Vω* [KL(∏ω*(∙∣s)∣∣∏0(∙∣s))] + 4LψC2Rmax.
c18 :	二 Cψ √c10 + c10Lψ (1 + λ2ψ-).
c19 :	二 Cψ √c11 + c11Lψ (1 + λ2ψ-).
c20 :	二 Cψ √c12 + c12Lψ (1 + λ2ψ-).
c21 :	二 Cψ √c13 + c13Lψ (1 + λ2ψ-).
c22 :	二 Cψ √ci4 + c14Lψ (1 + 卞ψ).
c23 :	二 Cψ √ c15 + c15Lψ (1 + λ2ψ').
c24 :	二 c∖sLψ (1 + λ2ψ-).
42