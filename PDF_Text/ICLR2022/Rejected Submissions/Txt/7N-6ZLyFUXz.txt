Under review as a conference paper at ICLR 2022
Thompson Sampling for (Combinatorial) Pure
Exploration
Anonymous authors
Paper under double-blind review
Ab stract
Pure exploration plays an important role in online learning. Existing work mainly
focuses on the UCB approach that uses confidence bounds of all the arms to de-
cide which one is optimal. However, the UCB approach faces some challenges
when looking for the best arm set under some specific combinatorial structures. It
uses the sum of upper confidence bounds within arm set S to judge whether S is
optimal. This sum can be much larger than the exact upper confidence bound of
S, since the empirical means of different arms in S are independent. Because of
this, the UCB approach requires much higher complexity than necessary. To deal
with this challenge, we explore the idea of Thompson Sampling (TS) that uses
independent random samples instead of the upper confidence bounds to make de-
cisions, and design the first TS-based algorithm framework TS-Verify for (combi-
natorial) pure exploration. In TS-Verify, the sum of independent random samples
within arm set S will not exceed the exact upper confidence bound of S with high
probability. Hence it solves the above challange, and behaves better than existing
UCB-based algorithms under the general combinatorial pure exploration setting.
As for pure exploration of classic multi-armed bandit, we show that TS-Verify
achieves an asymptotically optimal complexity upper bound.
1	Introduction
Pure exploration is an important task in online learning, and it tries to find out the target arm as fast
as possible. In pure exploration of classic multi-armed bandit (MAB) (Audibert et al., 2010), there
are totally m arms, and each arm i is associated with a probability distribution Di with mean μi.
Once arm i is pulled, it returns an observation ri , which is drawn independently from distribution Di
by the environment. At each time step t, the learning policy π either chooses an arm i(t) to pull, or
chooses to output an arm a(t). The goal of the learning policy is to pull arms properly, such that with
an error probability at most δ, its output arm a(t) is the optimal arm (i.e., a(t) = arg maxi∈^m] μ"
with complexity (the total number of observations) as low as possible.
Pure exploration is widely adopted in real applications. For example, in the selling procedure of cos-
metics, there is always a testing phase before the commercialization phase (Audibert et al., 2010).
The goal of the testing phase is to help to maximize the cumulative reward collected in the com-
mercialization phase. Therefore, instead of regret minimization (Berry & Fristedt, 1985; Auer et al.,
2002), the testing phase only needs to do exploration (e.g., to investigate which product is the most
popular one), and wants to find out the target with both correctness guarantee and low cost. In real
world, sometimes the system focuses on the best action under a specific combinatorial structure,
instead of the best single arm (Chen et al., 2014). For example, a network routing system needs to
search for the path with minimum delay between the source and the destination. Since there can be
an exponential number of paths, the cost of exploring them separately is unacceptable. Therefore,
people choose to find out the best path by exploring the single edges, and this is a pure exploration
problem instance in combinatorial multi-armed bandits (CMAB). In this setting, we still pull single
arms (base arms) at each time step, and there is a super arm set I ⊆ 2[m] . The expected reward of a
super arm S ∈ I is Piies μ%, i.e., the sum of the expected rewards of its contained base arms. And
the goal of the player is to find out the optimal super arm with an error probability at most δ.
Most of the existing solutions for pure exploration follow the UCB approach (Audibert et al., 2010;
Kalyanakrishnan et al., 2012; Chen et al., 2014; Kaufmann & Kalyanakrishnan, 2013). They com-
1
Under review as a conference paper at ICLR 2022
pute the confidence bounds for all the arms, and claim that one arm is optimal only if its lower
confidence bound is larger than the upper confidence bounds of all the other arms. Though this
approach is asymptotically optimal in pure exploration of classic MAB problems (Kalyanakrish-
nan et al., 2012; Kaufmann & Kalyanakrishnan, 2013), it faces some challenges in the CMAB case
(Chen et al., 2014). In the UCB approach, for a super arm S, the algorithm usually uses the sum of
upper confidence bounds of all its contained base arms as its upper confidence bound. This means
that the gap between the empirical mean of super arm S and the used upper confidence bound of S is
about O(Pi∈s，1/Ni) (here Ni is the number of observations on base arm i). However, since the
observations of different base arms are independent, the standard deviation of the empirical mean of
super arm S is O( 'Ei∈s 1/N), which is much smaller. ThiS means that the used upper confidence
bound is much larger than the real one. Combes et al. (2015) deal with this problem by computing
the upper confidence bounds for all the super arms independently, which leads to an exponential time
cost and can be hard to implement. In fact, existing efficient UCB-based solutions either suffer from
a higher complexity bound (Chen et al., 2014; Gabillon et al., 2016), or need further assumptions on
the combinatorial structure to achieve an optimal complexity bound (Chen et al., 2017).
Then a natural solution to deal with this challenge is to use random sample of arm i (with mean
to be its empirical mean and standard deviation to be O( 1/Ni)) instead of its upper confidence
bound to judge whether a super arm is optimal. If we let the random samples of different base
arms be independent, then the gap between the empirical mean of super arm S and the sum of
random samples within S is O(7£运咨 1/N), which has the same order as the gap between the
empirical mean of super arm S and its real mean. Therefore, using independent random samples
can behave better than using confidence bounds, and this is the key idea of Thompson Sampling
(TS) (Thompson, 1933; Kaufmann et al., 2012; Agrawal & Goyal, 2013). In fact, many prior works
show that TS-based algorithms have smaller cumulative regret than UCB-based algorithms in regret
minimization of CMAB model (Wang & Chen, 2018; Perrault et al., 2020). However, existing
researches only study using TS approaches under the Bayesian setting of pure exploration (Li et al.,
2021), and there still lack results of adapting TS to the frequentist setting.
In this paper, we attempt to fill up this gap, and study using TS in pure exploration under the frequen-
tist setting for both MAB and CMAB instances by adapting the idea of TS in the Bayesian setting
(Li et al., 2021). We emphasize that it is non-trivial to design (and analyze) a TS-based algorithm
under the frequentist setting. The first challenge is that there is a lot more uncertainty in the random
samples than the confidence bounds. In UCB-based algorithms, the upper confidence bound of the
optimal arm is larger than its expected reward with high probability. Thus, the arm with the largest
upper confidence bound is either an insufficiently learned sub-optimal arm (i.e., the number of its
observations is not enough to make sure that it is a sub-optimal arm) or the optimal arm, which
means that the number of pulls on sufficiently learned sub-optimal arms is limited. However, for
TS-based algorithms, there is always a constant probability that the random sample of the optimal
arm is smaller than its expected reward. In this case, the arm with the largest random sample may be
a sufficiently learned sub-optimal arm. Therefore, if we still choose to pull the arm with the largest
random sample (similar as UCB-based policies that pull the arm with the largest upper confidence
bound), then the mechanism of the TS-based policy should be designed carefully to make sure that
we can still obtain an upper bound for the number of pulls on sufficiently learned sub-optimal arms.
Another challenge is that TS is an algorithm that follows the Bayesian framework and loses many
good properties in the frequentist setting. In the Bayesian setting, at each time step t, the parameters
of the game follow a posterior distribution Pt , and the random samples are drawn from Pt indepen-
dently as well. Therefore, using the random samples to output the optimal arm can explicitly ensure
the correctness of the TS-based algorithm. However, in the frequentist setting, the parameters of
the game are fixed but unknown, and they have no such correlations with the random samples. Be-
cause of this, if we still use the random samples to output the optimal arm, then the distributions of
random samples in the TS-based algorithm need to be chosen carefully to make sure that it still has
correctness guarantee in the frequentist setting.
Besides, the analysis of TS-based algorithms in pure exploration is also very different with regret
minimization. In regret minimization, at each time step, we only need to draw one sample for each
base arm. However, in pure exploration, one set of samples is not enough at each time step, since
the algorithm needs to i) check whether there is an arm that is the optimal one with high probability;
ii) look for an arm that needs exploration, and none of these two goals can be achieved by only one
2
Under review as a conference paper at ICLR 2022
set of samples. Therefore, we must draw several sets of samples to make decisions, and this may
fail existing analysis in regret minimization. For example, the analysis in (Agrawal & Goyal, 2013)
obtains a lower bound for the probability that the optimal arm has the highest sample in one sample
set. However, when we use several sets of samples to make decisions, we can notice the optimal arm
only if it has the highest sample in most of these sample sets, which is much harder than it has the
highest sample in only one sample set when its observations are bad (e.g., its empirical mean equals
to its real expected reward minus the confidence radius).
In this paper, we solve the above challenges, and design a TS-based verification framework TS-
Verify for (combinatorial) pure exploration under the frequentist setting. TS-Verify takes a target
(super) arm as input, and aims to verify that this target (super) arm is optimal with error constraint δ .
At each time step t, TS-Verify first draws independent random samples θik(t) for all the (base) arms
i ∈ [m] and k = 1, 2,…，M (i.e., totally M independent samples for each arm). Then it tries to find
out the M best (super) arms under sample sets θk (t) = [θf (t),…，。您(t)] for k = 1,2,…，M. If
in most of these sample sets, the target (super) arm is the best one, then the algorithm will output
that the target (super) arm is indeed optimal. Otherwise, the algorithm will choose to pull a (base)
arm from the exploration set, which contains the target (super) arm and all the (super) arms that have
been optimal in at least one sample set at this time step.
In the rest of this paper, we mainly focus on the behaviour (i.e., correctness guarantee and com-
plexity upper bound) of this verification algorithm, which is almost the same as a complete pure
exploration algorithm after we adapt the explore-then-verify framework (Karnin, 2016; Chen et al.,
2017). In the general CMAB setting, we show that TS-Verify is near optimal and behaves better
than existing UCB-based algorithms (Chen et al., 2014). The optimal algorithm (Chen et al., 2017)
requires the combinatorial structure to satisfy some specific properties, which is less general than
our results. As for the MAB case, we show that TS-Verify is asymptotically optimal, i.e., it has a
comparable complexity as existing optimal algorithms (Kalyanakrishnan et al., 2012; Kaufmann &
Kalyanakrishnan, 2013) when δ → 0. All these results indicate that out algorithm framework is
efficient and general in dealing with pure exploration problems. To the best of our knowledge, this
is the first result of using TS-based algorithms in pure exploration under the frequentist setting.
2	Related Works
Pure exploration of the classic MAB model is first proposed by Audibert et al. (2010). After that,
people have designed lots of learning policies for this problem. The two most representative al-
gorithms are successive-elimination (Even-Dar et al., 2006; Audibert et al., 2010; Kaufmann &
Kalyanakrishnan, 2013) and LUCB (Kalyanakrishnan et al., 2012; Kaufmann & Kalyanakrishnan,
2013). Both of them adapt the idea of UCB (Auer et al., 2002) and achieve an asymptotically
optimal complexity upper bound (i.e., it matches with the complexity lower bound proposed by
Kalyanakrishnan et al. (2012)). Compared to these results, our TS-Verify policy uses a totally dif-
ferent approach, and can achieve an asymptotically optimal complexity upper bound as well.
Combinatorial pure exploration is first studied by Chen et al. (2014). They propose CLUCB, an
LUCB-based algorithm that is efficient as long as there is an offline oracle to output the best su-
per arm under any given parameter set. Chen et al. (2017) then design an asymptotically optimal
algorithm for this problem. However, they require the combinatorial structure to follow some spe-
cific constraints so that they can apply a more powerful offline oracle. Recently, based on the game
approach, Jourdan et al. (2021) provide another optimal learning policy for pure exploration in
CMAB. But their algorithm still requires the existence ofa more powerful offline oracle. Compared
with these UCB-based algorithms, our TS-based algorithm achieves a better complexity bound than
CLUCB (Chen et al., 2014) in the most general setting. However, since we do not assume the
existence ofa more powerful offline oracle, we cannot achieve the optimal complexity upper bound.
3	Preliminaries
3.1	Pure exploration in Multi-armed Bandit
A pure exploration problem instance of MAB is a tuple ([m], D, δ). Here [m] = {1,2,…，m} is
the set of arms, D = {D1,D2,…,Dm} are the corresponding reward distributions of the arms,
3
Under review as a conference paper at ICLR 2022
and δ is the error constraint. In this paper, we assume that all the distributions Di ’s are supported
on [0,1]. Let μ%，EX〜Di [X] denote the expected reward of arm i, and a* = arg maxi∈[m] μ% is
the optimal arm with the largest expected reward. Similar with many existing works (e.g., (Audibert
et al., 2010)), we also assume that the optimal arm is unique. At each time step t, the learning policy
π can either pull an arm i(t) ∈ [m], or output an arm a(t) ∈ [m]. Ifit chooses to pull arm i(t), then
it will receive an observation ri(t) (t), which is drawn independently from distribution Di(t). The
goal of the learning policy is to make sure that with probability at least 1 - δ, its output a(t) = a*.
Under this constraint, it aims to minimize the complexity Zπ , Pim=1 Ni (Tπ), where Tπ denotes
the time step t that policy π chooses to output a(t), and Ni(t) denotes the number of observations
on arm i until time step t.
Let ∆i,m，μa - μ% denote the expected reward gap between the optimal arm a* and any other
arm i = a*. For the optimal arm a*, its ∆a*,m is defined as μ0* - maxi=。* μi. We also define
Hm，Pi∈[m] ∆1~, and existing works (KalyanakriShnan et al., 2012) show that the complexity
i,m
lower bound of any pure exploration algorithm is Ω(Hm, log ∣).
3.2	Pure exploration in Combinatorial Multi-armed Bandit
A pure exploration problem instance of CMAB is an extension of the MAB case. The arms i ∈ [m]
are called base arms, and there is also a super arm set I ⊆ 2[m] . For each super arm S ∈ I, its
expected reward is Pi∈s μi. Let S* = arg maxs∈ι Pi∈s μ% denote the optimal super arm with
the largest expected reward, and we assume that the optimal super arm is unique (as in (Chen et al.,
2014)). At each time step t, the learning policy π can either pull a base arm i(t) ∈ [m], or output
a super arm S(t) ∈ I. The goal of the learning policy is to make sure that with probability at least
1 - δ, its output S(t) = S*. Under this constraint, it also wants to minimize its complexity Zπ.
As assumed in many existing works (Chen et al., 2013; 2014; Wang & Chen, 2018), we also assume
that there exists an offline Oracle, which takes a set of parameters θ = [θ1,…,θm] as input, and
outputs the best super arm under this parameter set, i.e., Oracle(θ) = arg maxS∈I Pi∈S θi.
In this paper, for i ∈ S*, we use ∆i,C，Pj∈s* μj - maxs∈ι=i∈s Pj∈s μj to denote the expected
reward gap between the optimal super arm S* and the best super arm that contains i. As for i ∈ S*,
its ∆i,c is defined as ∆i,C ，Pj∈s* μj - maxs∈ι;i∕s Pj∈s μj, i.e., the expected reward gap
between S* and the best super arm that does not contain i. We also define width , maxS6=S* (|S \
S *| + IS * \ S |)，and let H1,c , Width Pi∈[m] ∆⅛;, H2,c , Width2 Pi∈[m] ʌjɪ .
Chen etal. (2017) prove that the complexity lowerbound is Ω(Ho,c log 1), where H°,c is the solution
of the following optimization problem (here ∆s,C = Pi∈s* μi - Pi∈s μi):
min	Nm
i∈[m]
s.t.	x Ni + x Ni≤δs,c,	∀S∈I,S=S*
i∈S∖S* i	i∈S*∖S i
The following result shows the relationships between H0,C, H1,C and H2,C.
Proposition 1. For any combinatorial pure exploration instance, H0,C ≤ H1,C ≤ H2,C.
Proof. Since Width ≥ 1, we have that H1,C ≤ H2,C. As for the first inequality, note that ∀i ∈ [m],
Ni = w∆2th is a feasible solution of the optimization problem. Hence we have that Ho,c ≤ Hι,c. □
i,c
3.3 General Explore-Then-Verify Framework
Due to space limit, the detailed descriptions of the general explore-then-verify framework (Chen
et al., 2017) is deferred to Appendix A.
Fact 1. (Lemma 4.8 in (Chen et al., 2017)) If there exist i) a complete pure exploration algorithm that
outputs the optimal arm with probability at least 1 一 δ and complexity at most O(HE log 1 + CE);
4
Under review as a conference paper at ICLR 2022
Algorithm 1 TS-Verify
1:	Input: Error constraint δ, target a (or S), q ∈ [δ, 0.1), t = m, Ni = 0,Ri = 0 for all i ∈ [m].
2:	Pull each arm once, update their number of pulls Ni’s and the sum of their observations Ri’s.
3:	while true do
4:	for k = 1, 2,…，M (δ,q,t) do
5:	For each arm i, draw sample θi(t) independently from distribution N(RNi, (J(N,t).
6:	ak(t) (Sk (t)) is the best (super) arm under parameter set [θk (t),…，θk,(t)].
7:	end for
8:	Pa(t) = Wb) PM(Sw) I[ak(t) = a] (Ps(t) = f Pi i * * 4[sK⑻=S])∙
9:	ifPa(t) > 1 - q (PS(t > 1 - q) then
10:	Return: Arm aa (super arm Sa) is indeed the optimal one.
11:	else
12:	Arbitrarily choose a0 ∈ {ak(t)}kM=(1δ,q,t) such that a0 6= aa (arbitrarily choose S0 ∈
{Sk(t)}kM=(1δ,q,t) such that S0 6= Sa).
13:	Pull arm i(t) from the exploration set {a0, aa} ({S0, S}), update its number of pulls Ni(t)
and the sum of its observations Ri(t).
14:	t — t + 1.
15:	end if
16:	end while
and ii) a verification algorithm such that with probability at least 1 - δ, it outputs that its input is
optimal with complexity at most O(HV log 1 + CV) if its input is the optimal arm and does not
terminate otherwise. Then we can construct a complete pure exploration algorithm based on the
general explore-then-verify framework. It works correctly with probability at least 1 - δ, and its
expected complexity is upper bounded by O(HE log -^ + HV log 1 + CE + CV) for fixed δo = 0.1.
According to Fact 1, in the rest of this paper, we focus on designing verification algorithms based on
the TS approach. As long as we have a verification algorithm with error probability δ and complexity
upper bound O(HV log - + CV), using any efficient complete pure exploration algorithm (e.g.,
LUCB (Kalyanakrishnan et al., 2012) or CLUCB (Chen et al., 2014)) in the explore-then-verify
framework can make sure that the expected complexity of the constructed pure exploration algorithm
is upper bounded by O(HE log -1 + HV log - + CE + CV), which is asymptotically the same as
the complexity upper bound O(HV log -) (since all the other terms do not depend on δ).
4 Thompson Sampling-based Verification
4.1 General Verification Framework for Pure Exploration
Our Thompson Sampling-based verification framework (TS-Verify) is described in Algorithm 1. We
use Ni to denote the number of observations on arm i, Ri to denote the sum of all the observations
from arm i, and Ni(t), Ri(t) to denote the value of Ni, Ri at the beginning of time step t.
At each time step t, for any i ∈ [m], TS-Verify first draws M(δ, q, t)，Iog(Iq2t 2-random samples
{θK(t)}M(δ'q,t- independently from a Gaussian distribution with mean μi(t)，NNi(I) (i.e., the em-
pirical mean of arm i) and variance GN[(∣)t (i.e., inversely proportional to Ni(t)), where C (δ, q,t) is
a term depending on the settings and will be explained in detail later. Then TS-Verify checks which
(super) arm is optimal in the k-th sample set [θk(t),…，θ*(t)] for all k. In the MAB case, it can
directly check which arm’s sample is the largest one; in the CMAB case, it can use Oracle to obtain
the best super arm under the k-th sample set. After that, TS-Verify checks whether the number of
sample sets in which the target arm aa (or target super arm S) is the optimal one is large enough.
If the number of such sample sets is larger than (1 - q)M(δ, q, t), then TS-Verify outputs that the
target (super) arm is optimal. Otherwise it knows that either the target (super) arm is not optimal, or
some (super) arms are not sufficiently learned (i.e., they do not have enough observations). In this
case, TS-Verify constructs an exploration set {a0, aa} (or {S0, S}), where a0 6= aa is an arbitrary arm
5
Under review as a conference paper at ICLR 2022
Z Cl IK .	Λ ∙ .	∖ .1	. ∖ Λ	.∙	1 ∙	. 1	1	1
(or S0 6= S is an arbitrary super arm) that has been optimal in at least one sample set, and uses some
choosing rules (which will be explained in detail later) to pull an arm i(t) in the exploration set.
The reason that we use Gaussian random samples in TS-Verify is because it can simplify the analysis
significantly (since the sum of independent Gaussian random variables is still a Gaussian one).
We believe that using other kinds of random samples (e.g., uniform distributions in the confidence
intervals) will have a similar behaviour.
4.2 TS -Verify for Multi-armed Bandit
We first apply TS-Verify to the classic MAB case. In the following, We define Φ(x,μ,σ2)
PrX〜N(μ,σ2)[X ≥ x], and φ(χ) satisfies that Φ(φ(χ), 0,1) = X for all X ∈ (0,1).
In the MAB case, we set C(δ, q, t)
choose to pull the arm with a smaller number of observations.
4logφ22mqt /", and for the exploration set {a0, a}, we will
Theorem 1. For q ∈ [δ, 0.1), with probability at least 1 一 δ, we have that: i) if G = a*, TS-
Verify will return that arm α is the optimal arm with complexity upper bounded by O(Hm(Tog 1 +
log(mHm)) ∣∣∣-δ + Hmlog ιθmHm)); ii) if a = a*, TS-Verify will not stop. SPecifically, ifwe choose
q = δ, then the complexity upper bound is O(Hm log ∣ + Hm log2 (mHm)).
Remark 1. The value q in TS-Verify is used to control the number of times that we draw random
samples at each time step. Note that M(δ, q,t) = Iog(Iqt ∕δ). Hence when q becomes larger, we
need fewer samples, but the complexity bound becomes worse. Here is a trade-off between the
algorithm’s complexity and the number of random samples it needs to draw. Our analysis shows
that using q = δβ for some constant β ≥ 1 can make sure that the complexity upper bound remains
the same order, and reduce the number of random samples significantly.
Proof. Firstly, we define the following three events:
E0,m = (∀t,i,L^i(t) - μi∣≤ Sbg(NmL ) ；
E1,m = (∀i,t,k, ∣θk (t)- μi(t)∣ ≤ S C3,q't)log(；m：M(δ,q‘t"δ))；
Ni(t)
E	— L…W Wy /log(12t2∕δ)[
E2,m = ∣∀t, lpa(t) - pa(t)| ≤ y M(δ,q,t) ∫ .
Here pa(t) denotes the probability that arm G's sample is the largest one in the k-th sample set of
time step t (i.e., Pr[θk(t) = maxi∈[m] θk(t)∣{Ni(t), Ri(t)}m=/, also note that it is not conditioned
on Eι,m). We can see that pa(t) is its empirical mean in the M(δ, q, t) sample sets.
By Chernoff-Hoeffding inequality, ∀t,i, Pr[∣μi(t) 一 μ/ > Jlog(Nmt)ɪ/^ɪ] ≤ ^^, therefore
Pr[-E0,m] ≤ Pi,t 悬产 ≤ δ. Similarly, one can also prove that Pr[-Eι,m] ≤ 3 (note that there are
M(δ, q, t) sample sets at time step t, and the variance of θf (t) is CN需)) and Pr[-E2,m] ≤ 3.
Then we only need to prove that under event E0,m ∧ E1,m ∧ E2,m, the two properties in Theorem
1 hold. To simplify the notations, in this proof, we denote M(t) = M(δ, q, t), C(t) = C(δ, q, t),
L1(t) = log(12mt2∕δ) and L2 (t) = log(12mt2M (δ, q, t)∕δ).
Consider the two cases in Theorem 1:
i) When a = a*: in this case, we claim that any arm i cannot be pulled if Ni(t) ≥ 16C∆t)L2(t), and
∆i,m
we will prove this claim by contradiction.
If we choose to pull arm i = a* and Ni (t) ≥ 16C∆t)L2(t), then from the description of TS-Verify,
i,m
there exists k such that θi(t) ≥ θk* (t) and Ni(t) ≤ N0* (t). However, we also have that (in the rest
6
Under review as a conference paper at ICLR 2022
of this paragraph, A(t) = Ni^ and B(t) = Na^t))
θk (t)	≤	μi(t)	+ ,A(t)C(t)L2(t)______________________ (1)
≤	μi +	PA(t)Lι(t) + PA(t)C(t)L2(t)	(2)
<	μi +	∆m，	⑶
where Ineq. (1) is because of E1,m; Ineq. (2) is because of E0,m; Ineq. (3) is because that Ni(t) ≥
16C∆)L2⑴ and therefore pA(t)C(t)L2(t) ≤ * and pA(t)L1(t) < * (since q ≥ δ, we
i,m
must have that C(t)L2(t) > Lι(t)). As for the arm a*, we have that
ɪ	————	——	————	∆.∙ E
θa* (t)	≥ Ma* (t)	-	pB(t)C(t)L2(t)	≥	μa*	-	PB(t)LI(t)	-	PB⑴C⑴L2⑴ > μi +-2—,
where the last inequality is because that Na* (t) ≥ Ni(t) ≥ 16C∆t)L2(t). This contradicts with
∆i,m
θik(t) ≥ θak* (t).
Similarly, if Na*(t) ≥ 16∆(t)L2(t), then arm a* cannot be pulled as well. This means that the
a*,m
complexity Z satisfies
Z≤
i∈[m]
16C(Z)L2(Z)
△2,m
16HmC(Z)L2(Z).
For q ≤ 0.1, we have that φ2(3q) = Θ(log 3q). Then with C(Z) = 4log[^jmZ /δ, L2(Z)
log(12mZ2M(Z)∕δ) and M(Z) = log(12Z ㈤,we have that (note that we require q ≥ δ)
Z ≤ Θ Qmlθg(mZg+lθg 1 (log(mZ)+log 1)) .
Therefore, after some basic calculations, we know that
lOg(mHm)+log δ
ii) When G = a*: in this case, we consider the probability that arm a* has a larger sample than a,
i∙e∙, Pr[θk*(t) ≥ θk(t)|{Ni(t),Ri(t)}i=i],andlet A(t) = NO(t) and B(t) = Na⅛).
Note that μo*	≥	μa,	therefore, under event	E0m,	we must have that	μ0*(t)	- μa(t)	≥	(μ0*	-
PB(t)Lι(t)) - (μa + PAWW) ≥ -,A(t)Lι(t) - ,B(t)Lι(t). Since 此(t) - θk(t) is a
Gaussian random variable with mean μa* (t) - μa(t) and variance A(t)C(t) + B(t)C(t), we have
that (recall that Φ(x, μ, σ2) = PrX〜N(*口2)[X ≥ x]):
Pr[θk* (t) ≥ θk(t)] = Φ(0,μa* (t) - μa(t),A(t)C(t) + B(t)C(t))
≥ Φ(0,-PA(t)L1(t) - PB(t)L1(t), A(t)C(t) + B(t)C(t))
=Φ(PA(t)L1(t) + PB(t)L1(t), 0, A(t)C(t) + B(t)C (t))
(M E+ PBWO1)
=φ(E ∙ pA(t) + B(t) , 0,1J
= 3q,	(4)
where Eq. (4) is because that C(t) = φL13t) and Φ(φ(3q), 0,1) = 3q (by definition of φ). This
implies that Pr[θk(t) = maxi∈[m] θk(t)∣{Ni(t),Ri(t)}m=ι] ≤ 1 - 3q.
7
Under review as a conference paper at ICLR 2022
Note thatpa(t) only depends on E°,m but does not depend on Eι,m and E2,m. Therefore under event
Eo,m ∧E1,m ∧E2,m, we still have that pa(t) ≤ 1-3q. Also recall that we set M (δ, q,t)
iog(i2t2/δ)
q2
which implies JloM；：：/：) = q. Hence, by event E2,m, We know that Pa(t) ≤ PG(t) + q ≤
1 - 3q + q < 1 - q, i.e., TS-Verify will not stop in this case.	□
Theorem 1 shows that the complexity upper bound of TS-Verify (as well as the complete pure ex-
ploration algorithm constructed by applying the explore-then-verify framework) is asymptotically
optimal in the MAB case, i.e., it matches the complexity lower bound Ω(Hm, log 1) (Kalyanakrish-
nan et al., 2012) when δ → 0. To the best of our knowledge, this is the first analysis of using a
TS-based algorithm to deal with pure exploration problems under the frequentist setting.
Remark 2. Focusing on verification is the mechanism we use to bound the number of pulls on
Sufficiently learned arms (i.e., arm i with Ni(t) ≥ 16C(t)L2 (t)∕∆2m). Fora complete pure explo-
ration algorithm, the optimal arm is unknown at the beginning. If it is insufficiently learned, then
the variance of its random sample is large, and we cannot ensure that it is optimal in any of the
sample sets. In this case, the exploration set may only contain sufficiently learned arms. However,
in a verification algorithm, we only focus on the complexity when the target arm is optimal, and
we can always put the target arm in the exploration set. This makes sure that there is at least one
insufficiently learned arm in the exploration set, i.e., if the target arm is sufficiently learned, then
the other arm that has at least one larger random sample than the target arm must be insufficiently
learned. Therefore, the number of pulls on sufficiently learned arms in TS-Verify is limited.
Remark 3. The variance term C(δ, q, t) in TS-Verify is chosen carefully to both ensure the correct-
ness and reduce the complexity upper bound. Since C(δ, q, t) is not too low, for any sub-optimal
target α, there is always some probability that the optimal arm a* has larger Sample than a, ^ven if
the observations of a* are bad (e.g., its empirical mean equals to its real expected reward minus the
confidence radius). This ensures the correctness of TS-Verify. On the other hand, since C(δ, q, t) is
not too high, the required number of observations on Sub-OPtimal arm i is still O( ∆∑ log δ).
4.3 TS -Verify for Combinatorial Multi-armed Bandit
Then we come to the CMAB case, and we choose C(δ, q, t) = 410暗；以；/δ). For the exploration
. CCt ∕-1'l	∙ 11	1	.	11 . 1	■	_	/Ct	∖	Γ1∖	. . /K ∖	f^1∕∖	∙ .1	. 1	11	FC
set {S0, S},	we will choose to pull the	arm i ∈	(S0	\	S)	∪ (S \	S0)	with the smallest number of
observations Ni(t).
Theorem 2. Forq ∈ [δ, 0.1), with probability at least 1-δ, we have that: i) ifS = S*, TS-Verify will
1
return that super arm S is the optimal super arm with complexity upper bounded by O(H1,c(log 1 +
log(∣I∣Hι,c)) ∣∣g-1 + Hι,clog (oIlH1,c)); ii) if S = S*, TS-Verify will not stop. Specifically, if we
choose q = δ, then the complexity upper bound is O(H1,c log 1 + H∖,c log2(∣I∣H1,c)).
Now we provide a proof sketch for Theorem 2, while deferring its complete proof to Appendix B.
Proof (sketch). In the CMAB case, we first define J = {S* \ S, S \ S* : S ∈ I} as the exchange
sets of any super arm S ∈ I and the optimal super arm S*. Then similar as the proof of Theorem 1,
we can also define three events E0,c, E1,c, E2,c.
∀t, U ∈ J, I X(μi(t) - μi)l ≤ ʌX NIBlog(24∣ι∣t2∕δ)卜
i∈U	i∈U i	
[∀t, k,U ∈ J, I X(θk (t) - μi(t))∣ ≤ jc(δ, q,t) X N^ log(24∣I∣t2M (δ, q, t)∕δ)}
卜,1pS(t)-Ps(t)1 ≤ SMM).
8
Under review as a conference paper at ICLR 2022
Here ps(t) denotes the probability that at time step t, S is the best super arm under the k-th sample
set(i.e., Pr[Pi∈S θk(t) = maxs∈ιPi∈s θk(t)∣{Ni(t), R√t)}建/). We can see that ps(t) is its
empirical mean in the M (δ, q, t) sample sets.
Similarly, we also have that Pr[E0,c ∧ E1,c ∧ E2,c] ≥ 1 - δ, and therefore it is sufficient to prove that
under event E0,c∧E1,c∧E2,c, the two properties in Theorem 2 hold. In this proof, we denote M(t) =
M(δ, q,t), C(t) = C(δ, q,t), Lι(t) = log(24∣I∣t2∕δ) and L2(t) = log(24∣I∣t2M(δ,q,t)∕δ).
Consider the two cases in Theorem 2:
i)	When S = S*: in this case, We claim that any arm i cannot be pulled if Ni(t) ≥ 16widt∆C(t)L2(t),
i,c
and We Will also prove this claim by contradiction.
Here We only consider the case that i ∈∕ S*, the case that i ∈ S* is similar. For any arm i ∈∕ S*, ifit
is pulled at time step t and Ni(t) ≥ 16widt∆C⑶L2⑴,then from the description of TS-Verify, there
i,c
exists S = S* and i ∈ S \ S* such that for some k, P/ θj(t) ≥ Pj∈s* θjk(t) (which is the same
as Pj∈s∖s* θj(t) ≥ Pj∈s*∖s θ*)), and Nj(t) ≥ Ni(t) for all j ∈ (S* \ S) ∪ (S \ S*). Now let
A(t) = Pj∈s∖s* N⅛) and B(t) = Pj∈s*∖s N⅛), then the proof is almost the same as Theorem
1 (note that there are at most width arms in S \ S* or S* \ S).
Therefore, we know that the complexity Z satisfies
Z ≤ X 16width∆(Z)L2(Z) = i6H1,cC(z)L2(z),
i∈[m]	i,c
which means that Z = O(Hι,c(log 1 + log(∣I∣Hι,c))Iogj + Hι,c 赭肾?1,C)).
ii)	When S = S*: in this case, we consider the probability that Pi∈s*∖S θf (t) ≥ Pi∈S∖s* θf (t)
(i.e., super arm S* is better than S in the k-th sample set). Here we denote A(t) = fi∈s∖s* N1t),
B(t) = Pi∈s*∖s N1(t), and then the proof becomes almost the same as Theorem 1.	□
Compared to existing works, the complexity upper bound of TS-Verify (as well as the complete pure
exploration algorithm constructed by applying the explore-then-verify framework) is width lower
than the CLUCB policy in (Chen et al., 2014). This is because that we use the Thompson Sampling
approach, i.e., for base arms in an exchange set S* \ S, the sum of their random samples (in TS-
of O(，P
Verify) only has a bias
i∈s*∖s N1(t)) towards the sum of their empirical means, while the
sum of their upper confidence bounds (in CLUCB) has a bias of O(Pi∈s*∖s Jn⅛)). Since the
observations of the base arms are independently drawn by the environment, the gap between the sum
of their empirical means and the sum of their real means is
O( yPi∈s*∖sN⅛). Therefore, our
TS-Verify algorithm not only has correctness guarantee (with proper C(δ, q, t)), but also achieves
a width lower complexity upper bound. We also conduct some experiments to compare the com-
plexity of TS-Verify with CLUCB, and the experimental results (which are listed in Appendix C)
demonstrate the effectiveness of our algorithm.
Though the complexity bound of TS-Verify still has some gap with the optimal one O(Ho,c log 1),
we emphasize that this is because we do not assume the existence of more powerful offline oracles,
and only use a simple offline oracle to solve the combinatorial pure exploration problems. Both the
existing optimal policies (Chen et al., 2017; Jourdan et al., 2021) either suffer from an exponential
time cost, or require to adopt more powerful offline oracles (please see Appendix D for more details).
Therefore, our algorithm is more general, and can be attractive in real applications with large scale
(which means a large width) and complex combinatorial structures (which may result in a high
implementation cost for the optimal learning algorithms).
Remark 4. If the value ∣I∣ is unknown (which is common in real applications), we can use 2m
4log(24∙2mt2∕δ)
φ2(3q)
instead (i.e., C(δ, q, t)
). This only increases the constant term in the complexity
upper bound and does not influence the major term O(Hι,c log ∣)∙
9
Under review as a conference paper at ICLR 2022
Ethics S tatement
Our TS-based algorithm is easy to implement and has low complexity in combinatorial pure ex-
ploration. Therefore, many important applications in our life, such as routing systems and online
advertising systems, can benefit from this new approach, i.e., the systems can find out their optimal
actions with much lower cost than before. Besides, the idea of using TS in combinatorial pure ex-
ploration may push the development of algorithms in other pure exploration problems. On the other
hand, since this paper only contains pure theoretical results, its negative impact on society is not
obvious.
Reproducibility S tatement
To make our theoretical results reproducible, we include the complete proof of Theorem 1 in Section
4.2 and the complete proof of Theorem 2 in Appendix B. Besides, we attach the detailed descriptions
of the general explore-then-verify framework (Chen et al., 2017) in Appendix A. This can help to
understand Fact 1, i.e., how to construct a complete pure exploration algorithm with complexity
bound asymptotically the same as a verification algorithm.
References
Shipra Agrawal and Navin Goyal. Further optimal regret bounds for thompson sampling. In Inter-
national Conference on Artificial Intelligence and Statistics, 2013.
Jean Yves Audibert, Sebastien Bubeck, and Remi Munos. Best arm identification in multi-armed
bandits. In COLT 2010 - the Conference on Learning Theory, Haifa, Israel, June, pp. 41-53,
2010.
Peter Auer, Nicolo Cesa-Bianchi, and Paul Fischer. Finite-time analysis of the multiarmed bandit
problem. Machine learning, 47(2-3):235-256, 2002.
Donald A Berry and Bert Fristedt. Bandit problems: sequential allocation of experiments (Mono-
graphs on statistics and applied probability). Springer, 1985.
Lijie Chen, Anupam Gupta, Jian Li, Mingda Qiao, and Ruosong Wang. Nearly optimal sampling
algorithms for combinatorial pure exploration. In Conference on Learning Theory, pp. 482-534.
PMLR, 2017.
Shouyuan Chen, Tian Lin, Irwin King, Michael R. Lyu, and Wei Chen. Combinatorial pure ex-
ploration of multi-armed bandits. In International Conference on Neural Information Processing
Systems, pp. 379-387, 2014.
Wei Chen, Yajun Wang, and Yang Yuan. Combinatorial multi-armed bandit: General framework, re-
sults and applications. In Proceedings of the 30th International Conference on Machine Learning,
pp. 151-159, 2013.
Richard Combes, Sadegh Talebi, Alexandre Proutiere, and Marc Lelarge. Combinatorial bandits
revisited. In Advances in Neural Information Processing Systems, 2015.
Eyal Even-Dar, Shie Mannor, Yishay Mansour, and Sridhar Mahadevan. Action elimination and
stopping conditions for the multi-armed bandit and reinforcement learning problems. Journal of
machine learning research, 7(6), 2006.
Victor Gabillon, Alessandro Lazaric, Mohammad Ghavamzadeh, Ronald Ortner, and Peter Bartlett.
Improved learning complexity in combinatorial pure exploration bandits. In Artificial Intelligence
and Statistics, pp. 1004-1012. PMLR, 2016.
Marc Jourdan, Mojmir Mutny, Johannes Kirschner, and Andreas Krause. Efficient pure exploration
for combinatorial bandits with semi-bandit feedback. In Algorithmic Learning Theory, pp. 805-
849. PMLR, 2021.
10
Under review as a conference paper at ICLR 2022
Shivaram Kalyanakrishnan, Ambuj Tewari, Peter Auer, and Peter Stone. Pac subset selection in
stochastic multi-armed bandits. In International Conference on Machine Learning, 2012.
Zohar S Karnin. Verification based solution for structured mab problems. Advances in Neural
Information Processing Systems, 29:145-153, 2016.
Emilie Kaufmann and Shivaram Kalyanakrishnan. Information complexity in bandit subset selec-
tion. In Conference on Learning Theory, pp. 228-251, 2013.
Emilie Kaufmann, Nathaniel Korda, and Remi Munos. Thompson sampling: an asymptotically
optimal finite-time analysis. In International Conference on Algorithmic Learning Theory, 2012.
Jialian Li, Chao Du, and Jun Zhu. A bayesian approach for subset selection in contextual bandits. In
Proceedings of the AAAI Conference on Artificial Intelligence, volume 35, pp. 8384-8391, 2021.
Pierre Perrault, Etienne Boursier, Vianney Perchet, and Michal Valko. Statistical efficiency of
thompson sampling for combinatorial semi-bandits. In Neural Information Processing Systems,
2020.
William R Thompson. On the likelihood that one unknown probability exceeds another in view of
the evidence of two samples. Biometrika, 25(3/4):285-294, 1933.
Siwei Wang and Wei Chen. Thompson sampling for combinatorial semi-bandit. In International
Conference on Machine Learning, 2018.
11
Under review as a conference paper at ICLR 2022
Appendix
A	General Explore-then-Verify Framework
Algorithm 2 outlines the general explore-then-verify framework (Chen et al., 2017).
In this framework, there are several independent procedures, and the k-th procedure is active at time
step t only if t mod 2k = 0. Each procedure k contains two stages: an exploration stage and a
verification stage. The exploration stage of procedure k is a complete pure exploration algorithm,
i.e., it outputs the optimal arm with probability at least 1 - δ0 (here we choose δ0 = 0.1). After
the exploration stage of procedure k outputs an arm, the verification stage of procedure k then tries
to verify that the output arm is the optimal arm with confidence 1 - £.Once a verification stage
(of any procedure) outputs that its input arm is the optimal arm, then all the procedures terminate,
and the explore-then-verify framework outputs that arm as the result of pure exploration. We can
see that all these steps form a complete pure exploration algorithm with error constraint δ (which is
different with the complete pure exploration algorithm in the exploration stage with error constraint
δ0).
Algorithm 2 Explore-then-verify framework
1:	Input: Error constraint δ.
2:	Init: For any k ≥ 1, procedure k contains two stages. In the first stage, it explores with
constant error probability δ0 = 0.1; in the second stage, it tries to verify that the output arm of
the exploration stage is the optimal arm with error probability £.
3:	At time step t, if t mod 2k = 0, then procedure k is active (i.e., it can choose an arm to pull).
4:	If in any procedure k, the output arm of its exploration stage gets through the verification stage,
then output that arm and terminate the algorithm (as well as any other procedures).
B	Proof of Theorem 2
Recall that J = {S * \ S, S \ S * : S ∈ I}, and the three events Eo,c, Eι,c, E2,c are defined as
following.
∀t, U ∈ J, IX(μi(t) -μi)l ≤ jχN⅛yiog(24∣ι∣t2∕δ)}；
[∀t, k,U ∈ J, I X(θk (t) - μi(t))∣ ≤ C(J (δ, q, t) X N1^ log(24∣I∣t2M (δ, q, t)∕δ)}
卜,1ps⑴-PS⑴i≤ SoMIM).
Note that the random variable (μi(t) - μ/ is zero-mean and N(t)sub-gaussian, and for different
i, the random variables (∕^i(t) - μjs are independent. Therefore, Pi∈u (μi(t) - μi) is zero-mean
and Pi∈u N(t)SUb-GaUSsian. Then by concentration inequality,
VX Nt
Pr
I ∑(μi(t) - μi)I >
i∈U
log(24IIIt2∕δ)
≤ 2exP(Tog(24|I|t2/S)) = 12∣1∣t2
This implies that
δ	δδ
-0,c - N 12∣I∣t2 - 2序-3,
12
Under review as a conference paper at ICLR 2022
where the second inequality is because that |J | ≤ 2|I|, and the third inequality is because that
Pt t2 ≤ 2.
Similarly, the random variable (θf (t) - μi(t)) is a zero-mean Gaussian random variable with vari-
ance CNδq；), and for different i, the random variables (θf(t) - μɪ(t)),s are also independent. Then
by concentration inequality,
Pr
| ∑(θk (t)-μ,(t))∣
i∈U
J")X N⅛
log(24∣I∣t2M (δ,q,t)∕δ)
≤
2 exp(- log(24∣I∣t2M(δ, q,t)∕δ))
δ
12|I|t2M (δ, q, t).
This implies that
Pr[-E-] ≤ X __________δ________ ≤ X δ ≤ X 二 ≤ δ
[ 1,c] ≤ 分 12∣I∣t2M(δ,q,t) ≤ 与 12∣I∣t2 ≤	6t2 ≤ 3,
where the second inequality is because that there are totally M(δ, q, t) sample sets at time step t.
Let Yk(t) = I[Sk(t) = S], then We know that {Yk(t)}M，,q't)are M(δ, q, t) i.i.d.s with real mean
PS(t) and empirical mean Ps(t). Therefore
Pr ∣pg(t) — po(t) | ≤ ʌ /logʃ)尸 ∕? ≤ 2 exp(- log(12t2/δ)) = -ɪ
IpSl， PS「|_ M M(δ,q,t)	— p gl / "	6t2
which implies that
Pr[-E2，c] ≤ X 6δ2 ≤ 3.
t
Now it is sufficient to prove that under event E0,c ∧ E1,c ∧ E2,c, the two properties in Theorem 2
hold. In this proof, we denote M(t) = M(δ,q,t), C(t) = C(δ,q,t), L1(t) = log(24∣I∣t2∕δ) and
L2(t)=lθg(24∣I∣t2M (δ,q,t)∕δ).
Consider the two cases in Theorem 2:
i) When S = S*: in this case, we claim that any arm i cannot be pulled if M (t) ≥ 16widt∆C(t)L2(t),
i,c
and we will prove this claim by contradiction.
For any arm i ∈∕ S* (the proof for arms i ∈ S* is similar), if it is pulled at time step t
and Ni (t) ≥ 16widt∆C(t)L2(t), then from the description of TS-Verify, there exists S = S*
i,c
and i ∈ S \ S* such that for some k, Pj∈s θj(t) ≥ Pj∈s* θj(t) (which is the same as
Pj∈s∖s* θj(t) ≥ Pj∈s*∖s θj(t)), and Nj(t) ≥ Ni(t) for all j ∈ (S* \ S) ∪ (S \ S*). Now
let A(t) = Pj∈S∖S* j and B(t) = Pj∈S*∖S j, we have that
X θk(t)	≤ X μj(t) + PA(t)C(t)L2(t)	(5)
j∈S∖S*	j∈S∖S*
≤	X	μj +	PA(t)Lι(t)	+ PA(t)C(t)L2 (t)	(6)
j∈S∖S*
<	X	μj +	∆m	⑺
j∈S∖S*
∆S,m
≤ 工 μj +	,
13
Under review as a conference paper at ICLR 2022
where Ineq. (5) is because of E1,c; Ineq. (6) is because of E0,c; Ineq. (7) is because that ∀j ∈ S \
S*,Nj(t) ≥ Ni(t) ≥ 16widt∆Cq)L2(t and therefore pA_(t)C(t)L2 (t) ≤ ^,m and PA(t)L1(t) <
i,m
—4m (since q ≥ δ,we must have that C (t)L2 (t) > Li (t)). Similarly, for arms j ∈ S * \ S, We have
that
X θk(t) ≥ X μj(t) - PB(t)C(t)L2(t)
j∈S*∖S	j∈S*∖S
≥ X μj - PB(t)Li(t) - PB{t)C(t)L2(t)
j∈S*∖S
> X μj- ∆m	(8)
j∈S*∖S
∆S,m
≥ 工 μj + F~,
j∈S∖S*
where Ineq. (8) is because that ∀j ∈ S * \ S, Nj (t) ≥ Ni(t) ≥ 16widt— C(t)L2(t). This contradicts
i,m
with pj∈s∖s* θk⑴ ≥ Pj∈S* \S θjk(t).
Therefore, we know that the complexity Z satisfies
Z ≤ X 16Width<(Z)L2(Z) = l6H1,cC(Z)L2(Z).
i∈[m]	i,c
With C(Z) = 4log(φ4∣iqZ2㈤,Φ2(3q) = Θ(log 表), L2(Z)
log(24∣I∣Z2M(Z)∕δ) and M(Z)
log(12Z2∕δ)
q2
, we have that (note that we require q ≥ δ)
ιog( IZ TT Iog(IZlZ)+log 1 Λ zlτl 7、 1 lʌʌ
Z ≤ θ I Hi.c-----ιogι-----Cog(IIIZ) + log δ)j.
(9)
Then we can use the following lemma to find an upper bound for complexity Z.
Lemma 1. Given K functions fi(x),…，fκ(x) and K positive values Xi,…，XK, if ∀x ≥
Xk, Kfk(x) < x holds for all 1 ≤ k ≤ K, then for any x ≥ k Xk,	k fk (x) < x.
Proof. Since Xi, ∙∙∙ , XK are positive values, for any X ≥ Ek Xk, we must have that X ≥ Xk.
Therefore Kfk (x) < x, which implies that
XKfk(X) < * XX.
This is the same as Pk fk (x) < x.	□
To apply Lemma 1, we set fi(Z) = Hifi叱学),f2(Z) = H；。log(?ZIlog 1, and
g q	g q
f3(Z) =	Hi,。logg ιδ .	After some basic calculations, we get that	X3	= Θ(H1,c logg	jδ	),	X2	=
Θ(Hi,clog(IHJog δ + X3) and Xi = Θ(Hi,0号；?1,C)).
Then we know that for Z ≥ Θ(Hi,c(log ɪ + log(∣I∣Hi,c))⅛g∣-
+ Hi,cRH1,C)), fi(Z) +
f2(Z) + f3(Z) < Z (by Lemma 1). This contradicts with Eq. (9). Therefore, we know that
Z = O(Hi,c(log i + log(II∣Hi,c))⅛j + Hi,clog2(oIlH1,C)).
ii) When S = S*: in this case, we consider the probability that Pi∈s*∖s θf (t) ≥ Pi∈S∖s* θf(t),
i.e., super arm S * is better than S in the k-th sample set. Here we denote A(t) = £心\$* N(^,
B(t) = Ei∈s*∖S N⅛.
14
Under review as a conference paper at ICLR 2022
Note that Ei∈s*、$ μ% ≥ Ei∈s∖s* μi, therefore, under event E°,c, We must have that
Pi∈s*∖sμMt)-Pi∈s∖s* μi(t) ≥ (Pi∈s*∖sμi-PBWW-(Pi∈s∖s* μ,+PA(t)Lι(t)) ≥
-,A(t)Lι(t) - pB(t)Lι(t). Since Pi∈s*∖g θk(t) - Pi∈S∖s* θk(t) is a Gaussian random vari-
able with mean Pi∈s*∖s μi(t) - Pi∈s∖s* ∕^i(t) and variance A(t)C(t) + B(t)C(t), we have that
(recall that Φ(x,μ,σ2) = PrX〜N(μ,σ2)[X ≥ x]):
Pr X	θk(t)	≥ X	θk(t)	= Φ(0,	X	μi(t)	- X	μi(t),A(t)C(t)+ B(t)C(t)	j
J∈S*\S	i∈S∖S*	_|	∖ i∈S*∖S	i∈S∖S*	)
≥ Φ(0,-PA(t)Lι(t) - PB(t)Lι(t), A(t)C(t) + B(t)C(t))
=Φ(PA(t)Lι(t) + PB(t)Lι(t), 0, A(t)C (t) + B (t)C (t))
φ( SLW paw+ PBWnA
=φ(M ∙ pA(t) + B(t) , 0, 1
= 3q,	(10)
where Eq. (10) is because that C(t) = φL1(t) and Φ(φ(3q), 0,1) = 3q (by definition of φ). This
implies that PrPi∈s θk(t) = maxs∈ιPi∈s θk(t)∣{Ni(t),Ri(t)}m=1] ≤ 1 - 3q.
Note that ps(t) only depends on Eo,c but does not depend on Eι,c and E2,c. Therefore under event
Eo,c ∧E1,c ∧ E2,c, we still have that ps(t) ≤ 1 - 3q. Also recall that we set M(δ, q,t) = log(12t ㈤,
.vl1irb imnlipc /log(12t2∕δ)
which implies V M(δ,q,t)
q. Therefore, by event E2,c,
we know that Ps(t) ≤ ps(t) + q ≤
1 - 3q + q < 1 - q, i.e., TS-Verify will not stop in this case.
C Experiments
Here we conduct a simple experiment to compare the complexity of our algorithm with CLUCB,
and to demonstrate the effectiveness of our algorithm. In this experiment, we consider the following
combinatorial pure exploration problem instance with parameter n ∈ N+ .
Problem 1. For fixed value n, there are totally 2n base arms. For the first n base arms, their
expected rewards equal to 0.1, and for the last n base arms, their expected rewards equal to 0.9.
There are only two super arms S1, S2. S1 contains the first n base arms and S2 contains the last n
base arms.
Since there are only two super arms, our TS-Verify can also be used to do pure exploration, i.e., at
time step t, if S1is the optimal super arm in at least (1 - q)M (δ, q, t) sample sets, then we output
that S1is optimal; if S2 is the optimal super arm in at least (1 - q)M(δ, q, t) sample sets, then we
output that S2 is optimal; otherwise we pull all the arms in a round robin method. In this way, we
can compare TS-Verify with CLUCB directly. As for the input q in TS-Verify, we always choose
q = δ.
We first fix δ = 0.1, and compare the complexity of TS-Verify with CLUCB under different n. The
results (average complexities and their standard deviations) take an average over 20 independent
runs, and are shown in Fig. 1(a). We can see that when n increases, the complexity of TS-Verify
does not increase a lot, while the complexity of CLUCB increases linearly. This accords with our
analysis, since Hι,c(n) = 2n ∙(08^)2 = 6.25 is a constant but H2,c(n) = 2n ∙(0源)2 = 12.5n is
linear with n (here H1,c(n), H2,c(n) are the values of H1,c, H2,c under the problem instance with
parameter n, respectively).
Then we fix n = 10, and compare the complexity of TS-Verify with CLUCB under different δ. The
results (average complexities and their standard deviations) take an average over 20 independent
runs, and are shown in Fig. 1(b). We can see that when δ is large, the complexity of TS-Verify
15
Under review as a conference paper at ICLR 2022
5	10	15	20	25	30	35	40
n
(a) δ = 0.1
Figure 1: Comparison of TS-Verify and CLUCB.
0.1	0.05
0.02	0.01	0.005
δ
(b) n = 10
0.002	0.001
decreases as δ decreases, and when δ is small, the complexity of TS-Verify increases as δ decreases.
Moreover, the complexity of TS-Verify always increases slower than CLUCB (when δ decreases).
This also accords with our analysis. Note that there is a term O(Hι,c log (0IH1,C)) in the complexity
bound in Theorem 2. Since we choose q = δ, this term decreases as δ decreases. When δ = 0.1,
this term is very large and becomes the majority term in complexity, and therefore the complexity
decreases when we decrease δ from 0.1 to 0.002. When δ = 0.002, the term O(Hι,c log ∣) becomes
the majority term in complexity, therefore the complexity increases when we decrease δ from 0.002
to 0.001.
All the above results indicate that our TS-Verify algorithm outperforms CLUCB.
D Discussions ab out the Offline Oracles in Optimal Algorithms
for Combinatorial Pure Exploration
Most existing works on combinatorial pure exploration (e.g., (Chen et al., 2014; 2017; Jourdan et al.,
2021)) need to use the basic offline oracle, i.e., given a set of parameters [θι,…，θm], it can output
arg maxS∈I Pi∈S θi, the best super arm under this parameter set. Similar with (Chen et al., 2014),
our TS-Verify algorithm also only needs to use this offline oracle, and it is efficient (i.e., its time
complexity is polynomial with m) as long as this basic offline oracle is efficient.
However, the algorithms in (Chen et al., 2017; Jourdan et al., 2021) require efficient and more
powerful oracles. Otherwise the time complexity of their algorithms becomes exponential with m.
In (Chen et al., 2017), the NaiveGapElim algorithm needs to record all the super arms in I. This
means that its time complexity can be exponential with m. As for the EfficientGapElim algorithm,
it needs to solve three offline problems: OPT, Check and Unique, and directly solving these three
problems leads to an exponential time cost as well. The authors apply an approximate version
of Pareto curves to solve these problems, but this approach is only efficient under some special
combinatorial structures. Therefore, for the most general combinatorial pure exploration setting,
EfficientGapElim either suffers from an exponential time complexity, or requires more powerful
offline oracles to solve the above three problems efficiently.
In (Jourdan et al., 2021), the best-response oracle used by the λ-player has time cost scaled with
|N(St)|, where St is a super arm, N(St) represents the set of super arms whose cells’ boundaries
intersect the boundary of the cell of St, and the cell of a super arm St is defined as all the possible
parameter sets [θι,…，θm] in which St is the optimal super arm. Though the size of N(St) is
always smaller than I , it can still be exponential with m. Therefore, the CombGame algorithm in
(Jourdan et al., 2021) either suffers from an exponential time complexity, or requires more powerful
offline oracles to substitute the origin best-response oracle used by the λ-player.
16
Under review as a conference paper at ICLR 2022
In fact, existing optimal learning policies need to use the more powerful offline oracles to efficiently
find the base arm that needs exploration the most. Here we say base arm i needs exploration the
most at time t if αN* log 1 - Ni (t) is the largest one among the set of base arms, where α is some
universal constant, Ni is the value of Ni in the optimal solution Ho,c. From this definition, We can
see that ifwe always pull the base arm that needs exploration the most, then the frequency of pulling
each base arm converges to the optimal solution of H0,c, and this achieves the optimal complexity
upper bound. However, the basic offline oracle in TS-Verify can only find several insufficiently
learned super arms (i.e., the super arms that have been optimal in at least one sample set), and can-
not tell us which base arm (in these insufficiently learned super arms) needs exploration the most.
Therefore, if we want to use a more powerful offline oracle in TS-verify to achieve an optimal com-
plexity upper bound, then this offline oracle needs to tell us which base arm in these insufficiently
learned super arms needs exploration the most. If this powerful offline oracle can always output the
base arm that needs exploration the most, then the complexity upper bound of TS-Verify is reduced
to O(H0,c log 1) and becomes optimal. How to design such offline oracles (to look for the base arm
that needs exploration the most in our TS-Verify framework) and how to implement them efficiently
is one of our future research topics.
17