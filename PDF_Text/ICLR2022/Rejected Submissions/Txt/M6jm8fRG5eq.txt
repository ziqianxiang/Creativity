Under review as a conference paper at ICLR 2022
Decentralized Cooperative Multi-Agent Rein-
forcement Learning with Exploration
Anonymous authors
Paper under double-blind review
Ab stract
Many real-world applications of multi-agent reinforcement learning (RL), such
as multi-robot navigation and decentralized control of cyber-physical systems,
involve the cooperation of agents as a team with aligned objectives. We study
multi-agent RL in the most basic cooperative setting — Markov teams — a class of
Markov games where the cooperating agents share a common reward. We propose
an algorithm in which each agent independently runs stage-based V-learning (a
Q-learning style algorithm) to efficiently explore the unknown environment, while
using a stochastic gradient descent (SGD) subroutine for policy updates. We show
that the agents can learn an ε-approximate Nash equilibrium policy in at most 8
O(1∕ε4) episodes. Our results advocate the use of a novel stage-based V-leammg
approach to create a stage-wise stationary environment. We also show that under
certain smoothness assumptions of the team, our algorithm can achieve a nearly
team-optimal Nash equilibrium. Simulation results corroborate our theoretical
findings. One key feature of our algorithm is being decentralized, in the sense that
each agent has access to only the state and its local actions, and is even oblivious
to the presence of the other agents. Neither communication among teammates
nor coordination by a central controller is required during learning. Hence, our
algorithm can readily generalize to an arbitrary number of agents, without suffering
from the exponential dependence on the number of agents.
1	Introduction
A variety of real-world sequential decision-making problems have been successfully addressed with
reinforcement learning (RL), such as playing the game of Go (Silver et al., 2016), Poker (Brown
& Sandholm, 2018), real-time strategy games (Vinyals et al., 2019), autonomous driving (Shalev-
Shwartz et al., 2016), and robotics (Kober et al., 2013). Many of these applications can be cast
as a multi-agent reinforcement learning (MARL) problem, where multiple agents are involved
in an interactive environment. These successful applications have inspired a surging interest in
understanding the theoretical aspects of MARL, which, however, is mostly devoted to the competitive
setting, e.g., two-player zero-sum Markov games. This has left many problems in other important
MARL settings, especially cooperative MARL, relatively open.
Cooperative MARL generally involves a team of agents collaborating on a common task. We roughly
categorize cooperative MARL settings into three classes based on the information available to the
agents. The first class is (1) centralized/joint learning (Boutilier, 1996; Claus & Boutilier, 1998),
which assumes that there exists a single coordinator who can access the local information of all the
agents, and makes decisions/learns policies jointly for all of them. This centralized training (though
possibly decentralized execution) approach has become a common practice in empirical MARL
(Oliehoek et al., 2008; Foerster et al., 2016; Lowe et al., 2017; Rashid et al., 2018; Son et al., 2019).
Centralized learning essentially reduces the multi-agent problem to a single-agent one, but such a
method does not scale well with the number of agents. In fact, one can show that the computation &
sample complexity grows exponentially with the number of agents in the worst case. The second
class is (2) learning with communications (Kar et al., 2013; Zhang et al., 2018; Dubey & Pentland,
2021), which typically assumes that the agents can communicate by sharing some local information
with other agents (sometimes through a central agent). This approach gets around the centralized
computation bottleneck, but instead suffers from additional communication overhead. It can also be
1
Under review as a conference paper at ICLR 2022
unrealistic in some real-world scenarios where communication may be expensive and/or unreliable,
such as in unmanned aerial vehicle (UAV) field coverage (Pham et al., 2018).
Given the aforementioned limitations, we are more interested, in this paper, in a more practical setting:
(3) decentralized learning1. We enforce a seemingly daunting restriction: Each agent only has access
to its own local information (e.g., local actions and rewards), and can neither communicate with its
teammates nor be coordinated by any central controller during learning. In fact, we require that each
agent is completely unaware of the underlying structure of the game (whether it is cooperative or
not), or even the presence of other agents. Among the three classes mentioned above, decentralized
learning requires the weakest assumption on information availability, which makes it suitable for
many practical multi-agent learning scenarios (Fudenberg et al., 1998; Wang & de Silva, 2008; Kuyer
et al., 2008). In addition, decentralized learning is generally more scalable, and does not suffer from
the exponential sample & computation complexity in the number of agents. Seemingly tempting,
one might wonder how strong, if any, theoretical results can be established in this challenging
decentralized setting. In the paper, we investigate the theoretical aspects of decentralized cooperative
multi-agent RL in the non-asymptotic regime. A natural and fundamental theoretical question is:
Does any independent Q-learning style algorithm lead to Nash equilibria in cooperative MARL?
The answer to this question has remained elusive ever since it was raised in Lauer & Riedmiller
(2000). To answer this question, we focus on arguably the most basic and fundamental cooperative
MARL setting — Markov teams (Lauer & Riedmiller, 2000; Boutilier, 1996; Wang & Sandholm,
2002) - where the cooperating agents share a common team-reward function, and the state transition
in the environment is affected by the joint actions of all agents. In fact, even in the basic Markov team
setting, decentralized learning can be highly challenging. First, since both rewards and transitions
are affected by the other agents, the environment becomes non-stationary from each agent’s own
perspective when the other agents also update their policies. Hence, an agent needs to explore the
unknown environment efficiently while keeping in mind that the information it gathers now will soon
become outdated. This rules out many successful single-agent RL solutions, which critically rely on
the assumption that the agent is learning in a stationary environment. Second, compared with RL in
two-player zero-sum games, an additional challenge in teams is the existence of possibly multiple
Nash equilibria (NE) with different values. The agents in teams hence have to additionally coordinate
on which NE to converge to. Our contributions in the paper are summarized as follows.
Contributions. 1) As a warm-up, we start with decentralized matrix teams (Section 3), which
constitute a special class of Markov teams with no state transitions. We show that a simple stochastic
gradient descent (SGD) algorithm can provably find an ε-approximate NE policy using 8 O(1∕ε8)
samples, which can be improved to a O(1∕ε4) by using variance reduction techniques. 2) For the
general problem of decentralized Markov teams (Section 4), we propose an algorithm named Stage-
Based V-learning with Stochastic Gradient Descent (V-learning SGD), where V-learning (a name
first coined in Bai et al. (2020)) is a simple variant of Q-learning. Our algorithm can be executed
in a decentralized way as we defined above. 3) We show in Section 5 that if all agents in the team
run V-learning SGD, they can find an ε-approximate NE in at most O(1∕ε4) ∙ poly (S, H, AmaX)
episodes, where S is the number of states, Amax is the size of the largest action space, and H is the
length of an episode. Our results rely on a novel investigation of stage-based V-learning to create a
stage-wise stationary environment for the agents. 4) We further show that our algorithm can approach
the best equilibrium, i.e., the team-optimality, in a specific class of problems named “smooth teams”.
5) In Section 6, we provide numerical results that corroborate our theoretical findings. To the best of
our knowledge, our work appears to be the first to provide non-asymptotic guarantees for cooperative
MARL with exploration, while having the additional advantage of being decentralized.
Related Work. A common mathematical framework of multi-agent RL is stochastic games (Shapley,
1953), which is also referred to as Markov games. Early attempts to learn NE in Markov games
include Littman (1994; 2001); Hu & Wellman (2003); Hansen et al. (2013), but they either assume
the transition kernel and rewards are known, or only yield asymptotic guarantees. Recently, various
sample efficient methods have been proposed (Wei et al., 2017; Bai & Jin, 2020; Sidford et al., 2020;
1This setting has been studied under various names in the literature, including individual learning (Leslie &
Collins, 2005), decentralized learning (Arslan & YUkSeL 2016), agnostic learning (Tian et al., 2021; Wei et al.,
2021), and independent learning (Claus & Boutilier, 1998; Daskalakis et al., 2020). It also belongs to a more
general category of teams/games with decentralized information structure (Ho, 1980; Nayyar et al., 2013a;b).
2
Under review as a conference paper at ICLR 2022
Xie et al., 2020; Bai et al., 2020; Liu et al., 2021; Zhao et al., 2021; Guo et al., 2021), mostly for
learning in two-player zero-sum Markov games. Several works have investigated zero-sum games in
a decentralized setting as we consider here (Daskalakis et al., 2020; Tian et al., 2021; Wei et al., 2021;
Sayin et al., 2021), but these results do not carry over in any way to the decentralized cooperative/team
setting. We refer the reader to Appendix A for a more detailed discussion on these related works.
Another line of research has considered RL in teams. Without enforcing a decentralized environment,
Boutilier (1996) has proposed to coordinate the agents by letting them take actions in a lexicographic
order. Wang & Sandholm (2002) have studied optimal adaptive learning that converges to the
optimal NE in Markov teams. These two methods critically rely on communications among the
agents (beforehand) and/or observing the teammates’ actions. In contrast, the distributed Q-learning
algorithm in Lauer & Riedmiller (2000) is decentralized and coordination-free, which, however, only
works for deterministic tasks, and has no non-asymptotic convergence guarantees. In fact, developing
provable decentralized RL for cooperation in general stochastic environments is considered an
important open problem in Lauer & Riedmiller (2000), which we address in this paper.
Arslan & Yuksel (2016) and YongacoglU et al. (2019) have shown that decentralized Q-learning
style algorithms can converge to NE in weakly acyclic games, which cover Markov teams as an
important special case. Their decentralized settings are most similar to oUrs in that each agent is
oblivioUs to the presence of the others. However, both of them reqUire a coordinated exploration
phase, and only yield asymptotic gUarantees. In fact, it is not clear if their methods will converge
in polynomial time. We significantly improve their resUlts in this respect, by explicitly showing a
polynomial sample complexity Upper boUnd. Two contemporaneoUs works (Zhang et al., 2021b;
Leonardos et al., 2021) have stUdied gradient play in Markov potential games,2 bUt they do not
consider the aspect of exploring the Unknown environment, one of the key issUes in RL, and/or have
to assUme perfect knowledge of the environment. A concUrrent work (Chang et al., 2021) has stUdied
cooperative mUlti-player mUlti-armed bandits with information asymmetry, which parallels oUr matrix
team setting. Nevertheless, (Chang et al., 2021) reqUires stronger conditions than oUr decentralized
setting as their algorithm relies on playing a predetermined seqUence of actions.
2	Preliminaries
An N -player episodic Markov team is defined by a tUple (N, H, S, {Ai}iN=1, r, P), where (1) N =
{1, 2, . . . , N} is the set of agents; (2) H ∈ N+ is the nUmber of time steps in each episode; (3) S is
the finite state space; (4) Ai is the finite action space for agent i ∈ N ; (5) r : [H] × S × A → [0, 1]
is the team reward fUnction for all agents, where A = ×iN=1Ai is the joint action (or action profile)
space; and (6) P : [H] × S × A → ∆(S) is the transition kernel. We remark that both the reward
fUnction and the state transition fUnction depend on the joint actions of all the agents.
The agents interact in an Unknown environment for K episodes. At each time step h ∈ [H], the
agents observe the state sh ∈ S, and take actions aih ∈ Ai , i ∈ N simUltaneoUsly. All agents receive
the same reward rh(sh, ah), where ah = (a1h, . . . , ahN), and the environment transitions to the next
state sh+ι 〜Ph(∙∣Sh, ah). We assume for simplicity that the initial state si of each episode is fixed.
Note that the state transition here is general and not restricted to be deterministic, in contrast to
Lauer & Riedmiller (2000). This makes decentralized learning considerably more challenging, as the
agents cannot implicitly coordinate by enumerating/rehearsing all possible states. We focus on the
decentralized setting, where each agent only observes the states, rewards, and its own actions, but not
the actions of the other agents. We require that each agent is completely oblivious of the existence of
the others, and is also not allowed to communicate. This decentralized information structure requires
each agent to learn to make decisions based on only its local information.
For each agent i, a (Markov) policy πi : [H] × S → ∆(Ai) is a mapping from the time index
and state space to a distribution over its own action space. Each agent seeks to find a policy that
maximizes its own reward. A joint policy (or policy profile) π = (π1, . . . , πN) induces a probability
measure over the sequence of states and joint actions. For convenience, we use the superscript -i to
denote the set of agents excluding agent i, i.e., N \{i}. For example, we can rewrite π = (πi, π-i)
2All our results in this paper immediately generalize to Markov potential games, which include Markov
teams as a special case. For simplicity of notations, we illustrate our algorithm and results only in the Markov
team setting. A short discussion on Markov potential games is included in Appendix B.
3
Under review as a conference paper at ICLR 2022
using this convention. For a policy profile π, and for any h ∈ [H], s ∈ S, and a ∈ A, we define the
value function and the state-action value function (or Q-function) as follows:
HH
Vhπ(s) d=ef Eπ X rh0(sh0,ah0) | sh = s, Qπh(s,a) =def EπX rh0 (sh0, ah0) | sh = s,ah = a. (1)
h0=h	h0=h
πi? π-i	πi π-i
For agent i, a policy πi? is a best response to π-i if Vhπ ,π (s) = supπi Vhπ ,π (s), for any step
h ∈ [H] and state s ∈ S. A policy profile π = (πi , π-i) is a Nash equilibrium if πi is a best
response to π-i for all i ∈ N. One can show that NE always exists in finite Markov teams (Boutilier,
1996). There can be multiple NE in a Markov team with possibly different values. One desirable
equilibrium is the team-optimal one that achieves the highest value. Specifically, a policy profile π? is
team-optimal if Vhπ? (s) = supπ Vhπ(s), for any step h ∈ [H] and state s ∈ S. It follows immediately
that a team-optimal policy profile is always a NE, but the converse is not true in general.
For general (non-Markov) policies, we can still define their values at step h = 1 in a sense similar to
Equation (1). A best response πi? with respect to general policies π-i maximizes player i’s value
at step 1, i.e., Vn ,π (si) = sup∏i Vn ,π (si). The best response to the general policies of the
opponents is not necessarily Markov. We also have an approximate notion of Nash equilibrium:
Definition 1. (ε-approximate Nash equilibrium). For ε > 0, a general policy profile π = (πi , π-i) is
an ε-approximate Nash equilibrium if Viπi,π-i (si) ≥ supπi0 Viπi ,π-i (si) - ε, for all i ∈ N, where
the supremum is taken over all general policies that are not necessarily Markov.
For notational convenience, in most parts of the paper we illustrate our algorithms and results for
two-player teams, i.e., N = 2. It is straightforward to extend our results to the general N -player
team as we defined above. With two players, we use A and B to denote the action spaces of players
1 and 2, respectively. Let S = |S |, A = |A|, B = |B|. Denote by Amax = max{A, B}, which is
known to both players. We also rewrite the policies (∏1,∏2) as (μ, V) for notational convenience.
3	Warm-Up: The Matrix Team
As a warm-up, we start with a simple matrix team problem, where two agents repeatedly interact in a
single-stage team (Claus & Boutilier, 1998) with stochastic rewards. This is essentially a Markov
team with a single state and H = 1. The intuitions and results in this section will become useful later
in our analysis of the general Markov team problem.
A two-player finite matrix team can be described by a tuple (A, B, ρ), where A and B are the finite
action spaces for agent 1 and agent 2, respectively, and the stochastic reward function ρ = (Pa,b :
a ∈ A, b ∈ B) is a collection of reward distributions with support [0, 1]. The agents interact for T
rounds. At each round t ∈ [T], agent 1 (resp. agent 2) chooses an action at ∈ A (resp. bt ∈ B).
Both agents receive the same reward Rt drawn i.i.d. from the reward distribution Pat,bt . We let
r : A × B → [0, 1] be the expected value of the reward function, i.e., r(a, b) = R0i xdPa,b(x). We
use μ ∈ ∆(A) and V ∈ ∆(B) to denote the mixed strategies (or simply strategies) for agent 1 and
agent 2, respectively. Slightly abusing notations, the value function for a strategy pair ∏ = (μ, V) is
def
denoted by V(π) = V(μ, V) = Eμ×ν [r(a, b)], where a 〜μ and b 〜 ν.
In an unknown environment, a natural tactic for a self-interested agent is to adopt a no-regret learning
algorithm to guide its action selection (Cesa-Bianchi & Lugosi, 2006). In our problem, we propose
to let each agent run a simple no-regret learning algorithm, namely (projected) stochastic gradient
descent (SGD). Our Stochastic Gradient Descent with Implicit Exploration (SGD-IX) algorithm
for agent 1 is presented in Algorithm 1. The algorithm run by agent 2 is symmetric and hence not
included. Our use of SGD is motivated by the simple observation that every first-order stationary
point of the value function is also a NE for the matrix team (Lemma 4), and that SGD does lead
to stationary points in non-convex optimization (Ghadimi & Lan, 2013). Our solution also relies
on the fact that centralized SGD can be decoupled into a decentralized one (Lemma 5), as the joint
strategy is a Cartesian product of individual strategies. For generality, we write our gradient descent
step in Algorithm 1 as an online mirror descent (OMD) update. For more detailed descriptions and
intuitions of Algorithm 1, we refer the reader to Appendix D. The following result shows that the
output strategy pair of Algorithm 1 is close to a NE in expectation.
4
Under review as a conference paper at ICLR 2022
1
2
3
4
5
6
7
Algorithm 1: SGD-IX: Stochastic Gradient Descent with Implicit Exploration (for player 1)
Initialize: μι,° 一 1/A, ∀a ∈ A, at 一 T-3/4, ∀t ∈ [T], and Y 一 TT/4.
for t — 1 to T do
Draw action at 〜μt = (μt,ι,..., μt,a);
Take action at, and observe the team reward Rt ;
1R
Vμ,aLt - μτα+γI{at = a}, ∀a ∈ A, and VμLt — (Vμ,1Lt, ..., Vμ,ALt);
μt+1 - argminμ∈∆(A)
Vμ Lt,μ
i + OtDω(μ,μt)}, where ω(μ)
Dω (μ, μ) = ω(μ) - ω (μ''0) - hVω (μ''0), μ - 〃');
Output μτ where T is uniformly sampled from {1,...,T};
1 kμk2,and
Theorem 1.	Suppose that both agents run Algorithm 1 for T rounds. Let the two agents uniformly
draw a time index T from [1,..., T}, and denote by Xτ = (μτ, VT) the output ofthe algorithm. For
any ε > 0, if T = Ω(1∕ε8) ∙ poly(AmaX), then (μτ, VT) is an ε-approximate Nash equilibrium in
expectation, where poly(Amax) denotes a polynomial of Amax.
Note that to extract an approximate NE strategy, the agents need to sample the same time index
T. This requires some common source of randomness like a shared random seed. Such common
randomness is also termed a correlation device, and is standard in decentralized learning (Bernstein
et al., 2009; Arabneydi & Mahajan, 2015; Zhang et al., 2019). Note that the correlation device is never
used during the learning process to coordinate the exploration, but is simply used to synchronize the
selection of the strategies after they have already been generated by the learning algorithm. A common
random seed is generally considered as a mild assumption and does not break the decentralized
paradigm. In addition, we would like to remark that Theorem 1 is not an immediate consequence
of, and, in fact, is fundamentally different from, the folklore result that the empirical frequency of
plays of independent no-regret learning converges to the set of coarse correlated equilibria (CCE) in
general-sum normal-form games (Hart & Mas-Colell, 2000). In teams, the set of NE can be a strict
subset of CCE in general. Theorem 1 also crucially takes advantage of the first-order stationarity of
NE in teams, a property that is clearly absent in the folklore result of general-sum games.
In the following theorem, we further show that the sample complexity of learning in matrix teams can
be improved to O(1∕ε4) by utilizing a variance reduction technique. The corresponding algorithm
and its analysis are deferred to the appendices due to space limitations.
Theorem 2.	Let the agents run variance-reduced SGD (Algorithm 4 in Appendix E) for T rounds.
For any ε > 0, if T = Ω(1∕ε4) ∙ poly(AmaX), then (μτ, VT) is an ε-approximate NE in expectation.
4	A Decentralized Cooperative MARL Algorithm
In this section, we introduce our algorithm Stage-Based V-learning with Stochastic Gradient Descent
(V-learning SGD) for decentralized multi-agent RL in Markov teams. A well-known key chal-
lenge in MARL is that the environment becomes non-stationary from each agent’s own perspective
when the other agents also update their policies (Busoniu et al., 2008; Zhang et al., 2021a). To address
such a challenge, we propose to use a stage-based Q-learning style algorithm to create a stage-wise
stationary environment for the agents, while invoking the SGD-IX subroutine (Algorithm 1) in each
such stationary stage for policy updates. As an interesting side remark, stage-based Q-learning has
also achieved near-optimal regret bounds in single-agent RL (Zhang et al., 2020b).
The algorithm run by agent 1 is presented in Algorithm 2. The algorithm for agent 2 is symmetric,
by simply replacing the action space A with B. Each agent runs a stage-based V-learning algorithm
independently, and maintains upper confidence bounds on the value functions to actively explore
the unknown environment. For each step-state pair (h, s), we divide the visitations to this pair into
multiple stages, where the lengths of the stages increase exponentially at a rate of (1 + 1∕H) (Zhang
et al., 2020b). Specifically, we let e1 = H, and ei+1 = b(1 + 1∕H)eic, i ≥ 1 denote the lengths
of the stages, and let the partial sums L d=ef {Pij=1 ei | j = 1, 2, 3, . . . } denote the set of ending
times of the stages. For each (h, S) pair, we update our optimistic estimates Vh(sh) of the value
function at the end of each stage (that is, when the total number of visitations to (s, h) lies in the set
5
Under review as a conference paper at ICLR 2022
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
Algorithm 2: V-learning SGD: Stage-Based V-learning with Stochastic Gradient Descent
Initialize: Vh(S) - H - h + 1, Nh(S) - 0, Nh(S) - 0, rh(* s) - 0, Vh(S) - 0,Th(S) - H,
and μh(a | S)《-1/A, ∀h ∈ [H], S ∈ S, a ∈ A.
for episode k J 1 to K do
Receive S1;
for step h J 1 to H do
Nh(Sh) J Nh(Sh) + 1,n == Nh(Sh) J Nh(Sh) + 1;
Take action ah,〜 μη(∙ | Sh,), and observe reward rh, and next state Sh+i；
rh(Sh) J rh(Sh) + rh,Vh(Sh) J Vh(Sh) + Vh+1(Sh+1);
ifNh(Sh) ∈ L then
//Entering a new stage
Vh(Sh) J ⅜h) + VHsh) + bn, where bn J 25HPALX/^;
Nh(Sh) J 0, rh(Sh) J 0, vh (Sh) J 0,Th(Sh) J [(1 + H )Th(Sh)I ；
μh(a | Sh) J 1/A,∀a ∈ A；
αn J (Th(Sh))-3∕4,Y J (Th(Sh))T/4；
V	μ,aLn J H-h+1-nra+V⅛(Si))I{ah = a},∀a ∈ A；
V	7 T / fV7 T	V7 . T ∖.
V	μLn j- (Vμ,1Ln,. . . , Vμ,ALn)；
μn+1(∙ | Sh) J argminμ∈∆(A)卜VμLn,μi + 言D(μ,μn(∙ | Sh))},where
ω(μ) = 1 kμk2 , and Dω (μ, μ0) = ω(μ) 一 ω (μ0) 一(Vω (μ0), μ 一 小0〉；
L), using samples only from this single stage. This way, our stage-based V-Iearningensures that only
the most recent O(1∕H) fraction of the collected samples are used to calculate Vh (Sh), while the
first 1 一 O(1/H) fraction is forgotten. Such a stage-based update framework in some sense mimics
the celebrated optimistic Q-learning algorithm with a learning rate of α = HH+t (Jin et al., 2018),
which also roughly uses the last O(1/H) fraction of samples for value updates. However, we would
like to specifically emphasize that the V-learning variant of Jin et al. (2018) does not directly apply
to our problem due to some technical challenges that will be covered in Section 5. Unlike Jin et al.
(2018), our stage-based V-learning ensures that the value function estimates are updated at a low
frequency, a property also termed low-switching cost in single-agent RL (Bai et al., 2019), which is
crucial to address the non-stationarity issue in our MARL problem.
Another key component of Algorithm 2 is the SGD subroutine. For each step h ∈ [H] and each state
Sh ∈ S, we treat the agents’ interactions at Sh as a separate matrix team problem, similar to what
we have defined in Section 3. The reward function for such a matrix team is the Q-function at the
corresponding state, which is directly related to the optimistic value function estimates Vh(Sh) that
we have constructed in the V-learning part. Since our stage-based V-learning only updates Vh(Sh)
at the end of a stage, it creates a stage-wise stationary environment in the sense that the value of
Vh(Sh) is fixed within each stage, which can be captured by our stationary problem formulation in
Section 3. We can hence directly invoke Algorithm 1 and its analyses for the matrix team problem at
each step-state pair (h, Sh). Finally, we remark that both the V-learning and the SGD components of
our algorithm are decentralized, which can be implemented using only the states observed and the
local actions executed, without any communication or central coordination among the agents.
5	Theoretical Analyses
In this section, we present our main results on the sample complexity of V-learning SGD. We
first introduce a few notations to facilitate the analysis. For a step h ∈ [H] of an episode k ∈ [K],
we denote by Skh the state that the agents observe at this time step. For any state S ∈ S, we let
μh(∙ | s) ∈ ∆(A) and Vk(∙ | s) ∈ ∆(B) be the strategies prescribed by Algorithm 2 to agents 1
and 2, respectively, at this step. Notice that such notations are well-defined for every S ∈ S even
if s might not be the state Sh that is actually visited. We further let μh = {μh(∙ | s) : S ∈ S} and
Vh = {νh(∙ | s) : s ∈ S}. Let ah ∈ A and bh ∈ B be the actual actions taken by the two agents.
6
Under review as a conference paper at ICLR 2022
Algorithm 3: Construction of the Output Policy μh
ι Input: The strategy trajectory {μh}H,KKk=I specified by Algorithm 2.
2	Initialize: k0 - k.
3	for step h0 - h to H do
4	Receive sh0 ;
5	Take action ah，~ μh(∙ | sh，);
6	Uniformly sample i from {1, 2,..., NhO(Sh，)};
7	Set k0 J lkh0l i, where recall that IhO % is the index of the episode such that state sh√ was visited
the i-th time (among the total NhXsh，) times) in the last stage;
For any S ∈ S, let Nh(s), Nh(s), and Vhh(s) denote, respectively, the values of Nh(s), Nh(s'), and
Vh(s) at the beginning of the k-th episode. Note that it is proper to use the same notation to denote
these values from the two agents’ perspectives, because both agents maintain the same estimates of
these three terms as they can be calculated from the common observations (of the state-visitation).
Further, for a state sh, let 述 denote the number of times that state Sh has been visited (at the h-th
step) in the stage right before the current stage, and let Ih i denote the index of the episode that this
state was visited the i-th time among the nh times. For notational convenience, we use n to denote
nhh, and li, to denote Ih i, whenever h and k are clear from the context. With the new notations, the
update rule in Line 10 of Algorithm 2 can be equivalently expressed as
n	V
V h (Sh) - n X (rh(sh,ah ,bh) + V h+ι (Sh+1)) + bn.	⑵
n i=1
Similar to the situation of learning in zero-sum Markov games (Bai et al., 2020), the strategy
trajectories {(μh,,νh)}HK,h=ι themselves are not guaranteed to converge to an approximate NE.
Instead, based on {(μh, Vh )}H=Kh=ι, We extract an auxiliary set of policies {(μh, Vh )}H=Kh=ι that
leads to an approximate NE. Our construction of the auxiliary policies, largely inspired by the
“certified policies” (Bai et al., 2020) for learning in two-player zero-sum games, is formally presented
in Algorithm 3. At the step h and state Sh, μh first executes the strategy μh(∙ | Sh)∙ It then uniformly
samples an episode index k0 from the total Nlhh(Sh) episodes that the state Sh was visited during the
last stage. Finally, μh repeats a similar process at step h + 1 and episode k0, and so on. We define
Vh analogously. Compared with the certified policies in zero-sum games, our team problem has the
additional challenge that the NE value is not necessarily unique, and we need to first specify which
NE to compare with before we are able to define the notion of regret. Therefore, the analytical method
of bounding the duality gap in zero-sum games (Bai et al., 2020) does not apply to our problem. We
also remark that our construction of the auxiliary policies slightly simplifies the “certified policies” in
the sense that we only need to uniformly sample an episode index at each step, while the certified
policies rely on a rather complicated weighted sampling. Such simplification is a natural benefit of
our stage-based V-learning that assigns uniform weights to all time steps of the same stage in history.
Clearly, the policies (μhb, Vh) are non-Markov, and many notations defined for Markov policies in
Section 2 no longer hold. Nonetheless, we can still define the value function and the Q-function
starting from a step h ∈ [H] for a pair of non-Markov policies (μhb, Vh) in a similar way as in (1),
because (μh, Vh) does not depend on the history before the h-th step. For notational convenience,
we introduce the operators PhV(s,a,b) = Es，~Ph(∙∣s,a,b)V (s0) for any value function V, and
Dμ×νQ(S) = E(a,b)~(μ,ν)Q(s, a, b) With these notations, the Bellman equations can be rewritten
more succinctly as Qμ,ν(s, a, b) = (r-h + Ph需彳)(s, a, b), and Vhw(s) = (口“五乂"五Q*") (s) for
? νk
any (S, a, b, h) ∈ S × A × B × [H]. For each h ∈ [H], S ∈ S, define VH +H1 +1 (S) = 0, and
.∙: -
n	/	ι ʃ ʌ
Vh?,Vh (S) =f n X max Dμ×vι. (rh+PhVh+h+1)(S),
i=1	h
(3)
where the stages are partitioned in the same way as in the execution of Algorithm 2. In particular, if the
visitation to a certain state S is in its first stage, we let V?'"h (s) = H 一 h + 1 instead. We emphasize
7
Under review as a conference paper at ICLR 2022
?	ττ？.i>k/
that the maximum in (3) is taken with respect to each individual episode li, and hence Vh , h (s) can
be considered as the value of the “dynamic best response” with respect to the non-Markov policy
Vh. Such a definition is stronger than the usual concept of “best fixed response in hindsight"，and is
crucial to ensure that we obtain a NE in the end, instead of only a coarse correlated equilibrium (see
also Remark 1 for detailed discussions). Define Vhfh,?(S) analogously.
In the following, we start with an intermediate result, which states that the optimistic value function
V h(s) is an upper bound of both Vh?,vh (S) and Vhfh,?(S) in expectation.
Lemma 1. E[Vhh(s)] ≥ EV?或(s)] andE[Vhh(s)] ≥ E[Vhih,?(s)],∀(s,h,k) ∈ S × [H] × [K].
The proof of Lemma 1 relies on our results from Section 3. Specifically, we can show that within each
(stationary) stage, the agents face a matrix team problem that lasts for n rounds, whose reward function
is associated with the optimistic value function Vhh(s) at the given state s. Since in Algorithm 2,
both agents essentially run an individual copy of Algorithm 1 for each state, our analysis exactly
reduces to the investigation of Algorithm 1 in Section 3. We also remark that optimistic Q-learning
with the celebrated learning rate of αt = H++t (Jin et al., 2018) could fail at this step, because it
induces a matrix team problem with weighted rewards where the weights vary over time and cannot
be pre-computed. Such time-varying rewards are not compatible with our SGD framework, and
as far as we know it is a challenging problem in stochastic non-convex optimization on its own.
Interested readers are referred to Remarks 2 and 3 in Appendix G for detailed discussions of this
technical challenge. We bypassed such a challenge by utilizing stage-based V-learning instead, which
in essence assigns invariant and pre-computable weights to each time step in history. Our results
hence advocate the use of stage-based V-learning in MARL, over the seminal work Jin et al. (2018),
because the former creates a stage-wise stationary environment, and by nature provides a solution to
the core issue of non-stationarity (Busoniu et al., 2008; Zhang et al., 2021a) in MARL.
Suppose that for every (h, k) ∈ [H] X [K], “hh and Vh use a common random seed to sample the
episode index from {1,...,Nk(s)}. The value function for (μh Vk) can be written recursively as
■— — —
n	/	ι i ι i ʌ
vμh,νh (Sr x % ×/ rrh+Phvμ++ι,νh+ι) (s),
n - -l μh ×νh ∖	J
i=i
1τjH+1,VH +1
and VHH++11 H+1 (S) = 0. The following result shows that, in expectation (over the randomness of the
reward sequence and the agents’ selected actions), the agents have no incentive to deviate from the
policy pairs {(μk, Vk)}3i, UP to an error term of the order O(K-1).
Theorem 3. The auxiliary policies {(μh Vhk)}HKk=I satisfy that
K
XEh
k=1
K
XEh
k=1
1
K
%*,νk (si) - vμ1k,ν1 (si)i ≤ O(H17S1 Amax/K1), and
1
K
'”k j .	,-7k ^k .	. ^l	.	17	1	3	.	1 .
vμι ,*(si) - vμι ,ν1 (sι)j ≤ O(HXS8Amax/K8).
An immediate corollary is that, if we let the agents uniformly sample an episode index κ from
{1,..., K}, the policy pair (μK,Vf) we constructed is an approximate NE.
Theorem 4. Suppose that the two agents run Algorithm 2 for K episodes with K = Ω(1∕ε8) ∙
poly(H, S, Amax), and uniformly sample an episode index κ from {1, . . . , K}. Then, with probability
at least 5, the policy pair (μK,Vf) is an ε -approximate Nash equilibrium in expectation. A standard
boosting technique (Mitzenmacher & Upfal, 2017) can increase the success probability to an arbitrary
value 1 一 P for P ∈ (0, 5), by repeating the processfor O (log P) times.
Remark 1. Theorem 4 states that we obtain an approximate NE with high probability for any
realization of κ, rather than only in expectation over the randomness of κ. The former condition
guarantees that each outcome (μK,Vf) of the SamPling process is a Nash equilibrium with high
probability; with the latter condition, we can only conclude that the uniform distribution over
{(μk, Vk)}K=ι is a coarse correlated equilibrium, a weaker solution concept, where the common
random seed plays the role of the “trusted coordinator” in the language of correlated equilibria.
8
Under review as a conference paper at ICLR 2022
Our sample complexity bound depends on max{A, B} instead of A × B. This is the benefit from
decentralized learning, and would not have been achieved by centralized approaches. Such an
improvement would become more significant as the number of agents N increases. Our decentralized
approach only depends on the largest single action space maxi∈N |Ai|, while the centralized methods
would have an exponential dependence ΠiN=1 |Ai |. Also note that our sample complexity bound
in Theorem 4 holds in expectation. To obtain a standard high-probability result that holds with
probability 1 - p, one could either immediately apply Markov’s inequality and tolerate an additional
O(1/p) factor of computation, or replace our SGD subroutine with one that has high-probability
guarantees instead (Li & Orabona, 2020). We leave such improvements to our future work as they
diverge from the main focus of this paper. Similar to Section 3, the sample complexity in Theorem 4
can be further improved to O(1∕ε4) by incorporating variance reduction, which is sketched in
Theorem 7 of Appendix G. We remark that we have not attempted to optimize the dependence
on the other parameters H, S and Amax . There are many straightforward ways to obtain tighter
bounds in these parameters, such as making the hyper-parameters an and Y dependent on Amax.
For completeness, a sample complexity lower bound in the order of Ω(1∕ε2) ∙ poly (H, S, AmaX) is
presented in Appendix G, which is achieved by a reduction to a single-agent RL problem. We leave
the tightening of both the upper and lower bounds to our future work.
Finally, we show that our algorithm can nearly find the team-optimal NE in an important subclass
of Markov teams named smooth teams. Our definition of a (λ, ρ)-smooth team, adapted from the
definition of smooth games (Roughgarden, 2009; Radanovic et al., 2019), is formally introduced
in Definition 5 of Appendix G. Define V ? to be the team-optimal value function, i.e., Vh?(s) =
maxπ Vhπ (s) for any h ∈ [H], s ∈ S. The following theorem states that the output policies of
Algorithm 2 converge to a λ∕(1 + ρ) factor of team-optimality at a rate of O(K-1/8).
Theorem 5. Let K = Ω(1∕ε8)∙ poly (H, S, AmaX) ∙ Ina (λ, ρ)-smooth team, the value Ofthe auxiliary
PoIiCies {(μh, Vh)}Hfk=I Satisfies
K XX E Mk,νk (SI)i ≥ 1⅛V1?(SI) - 1+ρO(K-1).
6 Simulations
We empirically evaluate SGD-IX on a classic matrix team task, and V-learning SGD on a
Markov team. Figure 1 illustrates the performances of the algorithms in terms of both the rewards
and the L2 equilibrium gaps (which measure the L2 distance to a NE, formally defined in (39) of
Appendix H). Our simulation results turn out to be more encouraging that what our theory suggests:
Both the actual policy trajectories (“Current”) and the auxiliary policies (“Average”) converge to
NE in the tasks we tested. Further, the rewards obtained approach those of a team-optimal oracle
(“Centralized”), which suggests that our algorithm achieves the team-optimal NE very frequently in
our simulations, even though our theory does not guarantee so in general. Detailed descriptions of
the simulations are deferred to Appendix H due to space limitations.
de。EnUq---nb3
0	1000	2000	3000	4000	5000
Iterations
p,leM3α
Current
Average
0	1000	2000	3000	4000	5000
Iterations
de。En-q-=ns
0	1000	2000	3000	4000	5000
Iterations
p,leM3α
0	1000	2000	3000	4000	5000
Iterations
(a) Equilibrium gap (matrix) (b) Reward (matrix) (c) Equilibrium gap (Markov) (d) Reward (Markov)
Figure 1: (a) L2 equilibrium gaps and (b) rewards of Algorithm 1 on the matrix team task, and (c) L2 equilibrium
gaps and (d) rewards of Algorithm 2 on the Markov team task. “Current” denotes the actual policy trajectory
{(μh,νk)}HH=KKk=i, while “Average” represents the auxiliary policies {(μh,琮)}H=KKk=ι. In “Independent”,
each agent runs a naive single-agent Q-learning algorithm independently, by taking greedy actions w.r.t its local
Q-function estimates. Shaded areas denote the standard deviations of the equilibrium gap or reward.
9
Under review as a conference paper at ICLR 2022
Ethics S tatement
As a theory-oriented work, we do not believe that our research will cause any ethical issue, or
put anyone at any disadvantage. In particular, we do not believe that our work will cause any
discrimination / bias / fairness concerns, privacy and security issues, or legal compliance, and so on.
Reproducibility Statement
For every theoretical claim in the paper, we included a clear statement and explanation of all necessary
assumptions right before or after the theoretical claim. The complete proofs of all theoretical results
in the paper can be found in Appendices E, F, and G. The source code used in the simulations are
uploaded as supplementary materials along with the submission of the paper.
References
Alekh Agarwal, Sham Kakade, and Lin F Yang. Model-based reinforcement learning with a generative
model is minimax optimal. In Conference on Learning Theory, pp. 67-83, 2020.
Alekh Agarwal, Sham M Kakade, Jason D Lee, and Gaurav Mahajan. On the theory of policy
gradient methods: Optimality, approximation, and distribution shift. Journal of Machine Learning
Research, 22(98):1-76, 2021.
Zeyuan Allen-Zhu and Elad Hazan. Variance reduction for faster non-convex optimization. In
International Conference on Machine Learning, pp. 699-707. PMLR, 2016.
Jalal Arabneydi and Aditya Mahajan. Reinforcement learning in decentralized stochastic control
systems with partial history sharing. In American Control Conference, pp. 5449-5456. IEEE,
2015.
Yossi Arjevani, Yair Carmon, John C Duchi, Dylan J Foster, Nathan Srebro, and Blake Woodworth.
Lower bounds for non-convex stochastic optimization. arXiv preprint arXiv:1912.02365, 2019.
Gurdal Arslan and Serdar YUkSeL Decentralized Q-learning for stochastic teams and games. IEEE
Transactions on Automatic Control, 62(4):1545-1558, 2016.
Orly Avner and Shie Mannor. Concurrent bandits and cognitive radio networks. In Joint European
Conference on Machine Learning and Knowledge Discovery in Databases, pp. 66-81, 2014.
Mohammad Gheshlaghi Azar, Ian Osband, and Remi Munos. Minimax regret bounds for reinforce-
ment learning. In International Conference on Machine Learning, pp. 263-272, 2017.
Yu Bai and Chi Jin. Provable self-play algorithms for competitive reinforcement learning. In
International Conference on Machine Learning, pp. 551-560, 2020.
Yu Bai, Tengyang Xie, Nan Jiang, and Yu-Xiang Wang. Provably efficient Q-learning with low
switching cost. In International Conference on Neural Information Processing Systems, pp.
8004-8013, 2019.
Yu Bai, Chi Jin, and Tiancheng Yu. Near-optimal reinforcement learning with self-play. Advances in
Neural Information Processing Systems, 33, 2020.
Amir Beck. First-order Methods in Optimization. SIAM, 2017.
Daniel S Bernstein, Christopher Amato, Eric A Hansen, and Shlomo Zilberstein. Policy iteration for
decentralized control of Markov decision processes. Journal of Artificial Intelligence Research, 34:
89-132, 2009.
Craig Boutilier. Planning, learning and coordination in multiagent decision processes. In Conference
on Theoretical Aspects of Rationality and Knowledge, pp. 195-210, 1996.
Ronen I Brafman and Moshe Tennenholtz. R-max-a general polynomial time algorithm for near-
optimal reinforcement learning. Journal of Machine Learning Research, 3(Oct):213-231, 2002.
10
Under review as a conference paper at ICLR 2022
Noam Brown and Tuomas Sandholm. Superhuman AI for heads-up no-limit poker: Libratus beats
top professionals. Science, 359(6374):418-424, 2018.
Sebastien BUbeck and Thomas BUdzinski. Coordination without communication: Optimal regret in
two players multi-armed bandits. In Conference on Learning Theory, pp. 916-939, 2020.
Lucian Busoniu, Robert Babuska, and Bart De Schutter. A comprehensive survey of multiagent
reinforcement learning. IEEE Transactions on Systems, Man, and Cybernetics, Part C (Applications
and Reviews), 38(2):156-172, 2008.
Nicolo Cesa-Bianchi and Gdbor Lugosi. Prediction, Learning, and Games. Cambridge University
Press, 2006.
William Chang, Mehdi Jafarnia-Jahromi, and Rahul Jain. Online learning for cooperative multi-player
multi-armed bandits. arXiv preprint arXiv:2109.03818, 2021.
Caroline Claus and Craig Boutilier. The dynamics of reinforcement learning in cooperative multiagent
systems. AAAI Conference on Artificial Intelligence, 1998(746-752):2, 1998.
Johanne Cohen, Amelie Heliou, and Panayotis Mertikopoulos. Learning with bandit feedback in
potential games. In Proceedings of the 31st International Conference on Neural Information
Processing Systems, pp. 6372-6381, 2017.
Ashok Cutkosky and Francesco Orabona. Momentum-based variance reduction in non-convex SGD.
Advances in Neural Information Processing Systems, 32:15236-15245, 2019.
Constantinos Daskalakis, Dylan J Foster, and Noah Golowich. Independent policy gradient methods
for competitive reinforcement learning. Advances in Neural Information Processing Systems, 33,
2020.
Damek Davis and Dmitriy Drusvyatskiy. Stochastic subgradient method converges at the rate
O(k-1/4) on weakly convex functions. arXiv preprint arXiv:1802.02988, 2018.
Dmitriy Drusvyatskiy and Courtney Paquette. Efficiency of minimizing compositions of convex
functions and smooth maps. Mathematical Programming, 178(1):503-558, 2019.
Abhimanyu Dubey and Alex Pentland. Provably efficient cooperative multi-agent reinforcement
learning with function approximation. arXiv preprint arXiv:2103.04972, 2021.
Jakob N Foerster, Yannis M Assael, Nando de Freitas, and Shimon Whiteson. Learning to com-
municate with deep multi-agent reinforcement learning. In International Conference on Neural
Information Processing Systems, pp. 2145-2153, 2016.
Dylan J Foster, Zhiyuan Li, Thodoris Lykouris, Karthik Sridharan, and Eva Tardos. Learning in
games: Robustness of fast convergence. In International Conference on Neural Information
Processing Systems, pp. 4734-4742, 2016.
Yoav Freund and Robert E Schapire. Adaptive game playing using multiplicative weights. Games
and Economic Behavior, 29(1-2):79-103, 1999.
Drew Fudenberg, Fudenberg Drew, David K Levine, and David K Levine. The Theory of Learning in
Games, volume 2. MIT press, 1998.
Saeed Ghadimi and Guanghui Lan. Stochastic first-and zeroth-order methods for nonconvex stochastic
programming. SIAM Journal on Optimization, 23(4):2341-2368, 2013.
Hongyi Guo, Zuyue Fu, Zhuoran Yang, and Zhaoran Wang. Decentralized single-timescale actor-
critic on zero-sum two-player stochastic games. In International Conference on Machine Learning,
pp. 3899-3909. PMLR, 2021.
Thomas Dueholm Hansen, Peter Bro Miltersen, and Uri Zwick. Strategy iteration is strongly
polynomial for 2-player turn-based stochastic games with a constant discount factor. Journal of
the ACM, 60(1):1-16, 2013.
11
Under review as a conference paper at ICLR 2022
Sergiu Hart and Andreu Mas-Colell. A simple adaptive procedure leading to correlated equilibrium.
Econometrica, 68(5):1127-1150, 2000.
Sergiu Hart and Andreu Mas-Colell. Uncoupled dynamics do not lead to nash equilibrium. American
Economic Review, 93(5):1830-1836, 2003.
Yu-Chi Ho. Team decision theory and information structures. Proceedings of the IEEE, 68(6):
644-654, 1980.
Junling Hu and Michael P Wellman. Nash Q-learning for general-sum stochastic games. Journal of
Machine Learning Research, 4(Nov):1039-1069, 2003.
Thomas Jaksch, Ronald Ortner, and Peter Auer. Near-optimal regret bounds for reinforcement
learning. Journal of Machine Learning Research, 11(4), 2010.
Chi Jin, Zeyuan Allen-Zhu, Sebastien Bubeck, and Michael I Jordan. Is Q-learning provably efficient?
In International Conference on Neural Information Processing Systems, pp. 4868-4878, 2018.
Rie Johnson and Tong Zhang. Accelerating stochastic gradient descent using predictive variance
reduction. Advances in Neural Information Processing Systems, 26:315-323, 2013.
Dileep Kalathil, Naumaan Nayyar, and Rahul Jain. Decentralized learning for multiplayer multiarmed
bandits. IEEE Transactions on Information Theory, 60(4):2331-2345, 2014.
Soummya Kar, Jose M. F. Moura, and H. Vincent Poor. QD-learning: A collaborative distributed Strat-
egy for multi-agent reinforcement learning through consensus + innovations. IEEE Transactions
on Signal Processing, 61(7):1848-1862, 2013.
Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint
arXiv:1412.6980, 2014.
Robert Kleinberg, Georgios Piliouras, and Eva Tardos. Multiplicative updates outperform generic
no-regret learning in congestion games. In Proceedings of the Forty-First Annual ACM Symposium
on Theory of Computing, pp. 533-542, 2009.
Jens Kober, J Andrew Bagnell, and Jan Peters. Reinforcement learning in robotics: A survey.
International Journal of Robotics Research, 32(11):1238-1274, 2013.
Lior Kuyer, Shimon Whiteson, Bram Bakker, and Nikos Vlassis. Multiagent reinforcement learning
for urban traffic control using coordination graphs. In Joint European Conference on Machine
Learning and Knowledge Discovery in Databases, pp. 656-671. Springer, 2008.
Lifeng Lai, Hai Jiang, and H Vincent Poor. Medium access in cognitive radio networks: A competitive
multi-armed bandit framework. In Asilomar Conference on Signals, Systems and Computers, pp.
98-102. IEEE, 2008.
Tor Lattimore and Csaba Szepesvdri. BanditAlgorithms. Cambridge University Press, 2020.
Martin Lauer and Martin Riedmiller. An algorithm for distributed reinforcement learning in coopera-
tive multi-agent systems. In International Conference on Machine Learning, 2000.
Stefanos Leonardos, Will Overman, Ioannis Panageas, and Georgios Piliouras. Global convergence
of multi-agent policy gradient in markov potential games. arXiv preprint arXiv:2106.01969, 2021.
David S Leslie and Edmund J Collins. Individual Q-learning in normal form games. SIAM Journal
on Control and Optimization, 44(2):495-514, 2005.
Xiaoyu Li and Francesco Orabona. A high probability analysis of adaptive SGD with momentum.
arXiv preprint arXiv:2007.14294, 2020.
Michael L Littman. Markov games as a framework for multi-agent reinforcement learning. In
Machine Learning, pp. 157-163. 1994.
Michael L Littman. Friend-or-Foe Q-learning in general-sum games. In International Conference on
Machine Learning, pp. 322-328, 2001.
12
Under review as a conference paper at ICLR 2022
Qinghua Liu, Tiancheng Yu, Yu Bai, and Chi Jin. A sharp analysis of model-based reinforcement
learning with self-play. In International Conference on Machine Learning, 2021.
Ryan Lowe, Yi Wu, Aviv Tamar, Jean Harb, Pieter Abbeel, and Igor Mordatch. Multi-agent actor-
critic for mixed cooperative-competitive environments. Advances in Neural Information Processing
Systems, 30:6379-6390, 2017.
Haihao Lu. “Relative continuity” for non-Lipschitz nonsmooth convex optimization using stochastic
(or deterministic) mirror descent. INFORMS Journal on Optimization, 1(4):288-303, 2019.
Weichao Mao, Kaiqing Zhang, RUihao Zhu, David Simchi-Levi, and Tamer BaSar. Near-optimal
regret bounds for model-free RL in non-stationary episodic MDPs. In International Conference on
Machine Learning, 2020.
Jason R Marden, Gurdal Arslan, and Jeff S Shamma. Cooperative control and potential games. IEEE
Transactions on Systems, Man, and Cybernetics, Part B (Cybernetics), 39(6):1393-1407, 2009a.
Jason R Marden, H Peyton Young, Gurdal Arslan, and Jeff S Shamma. Payoff-based dynamics for
multiplayer weakly acyclic games. SIAM Journal on Control and Optimization, 48(1):373-396,
2009b.
Pierre Menard, Omar Darwiche Domingues, Xuedong Shang, and Michal Valko. UCB momentum
Q-learning: Correcting the bias without forgetting. arXiv preprint arXiv:2103.01312, 2021.
Michael Mitzenmacher and Eli Upfal. Probability and Computing: Randomization and Probabilistic
Techniques in Algorithms and Data Analysis. Cambridge University Press, 2017.
Dov Monderer and Lloyd S Shapley. Potential games. Games and Economic Behavior, 14(1):
124-143, 1996.
Jean-Jacques Moreau. PrOXimite et dualite dans un espace hilbertien. Bulletin de la Societe mathema-
tique de France, 93:273-299, 1965.
Ashutosh Nayyar, Abhishek Gupta, Cedric Langbort, and Tamer BasSar. Common information based
Markov perfect equilibria for stochastic games with asymmetric information: Finite games. IEEE
Transactions on Automatic Control, 59(3):555-570, 2013a.
Ashutosh Nayyar, Aditya Mahajan, and Demosthenis Teneketzis. Decentralized stochastic control
with partial history sharing: A common information approach. IEEE Transactions on Automatic
Control, 58(7):1644-1658, 2013b.
Gergely Neu. EXplore no more: Improved high-probability regret bounds for non-stochastic bandits.
Advances in Neural Information Processing Systems, 28:3168-3176, 2015.
Frans A Oliehoek, Matthijs TJ Spaan, and Nikos Vlassis. Optimal and approXimate Q-value functions
for decentralized POMDPs. Journal of Artificial Intelligence Research, 32:289-353, 2008.
Huy Xuan Pham, Hung Manh La, David Feil-Seifer, and Aria Nefian. Cooperative and distributed
reinforcement learning of drones for field coverage. arXiv preprint arXiv:1803.07250, 2018.
Goran Radanovic, Rati Devidze, David Parkes, and Adish Singla. Learning to collaborate in Markov
decision processes. In International Conference on Machine Learning, pp. 5261-5270, 2019.
Tabish Rashid, Mikayel Samvelyan, Christian Schroeder, Gregory Farquhar, Jakob Foerster, and Shi-
mon Whiteson. QMIX: Monotonic value function factorisation for deep multi-agent reinforcement
learning. In International Conference on Machine Learning, pp. 4295-4304. PMLR, 2018.
Sashank J Reddi, Ahmed Hefny, Suvrit Sra, Barnabas Poczos, and AleX Smola. Stochastic variance
reduction for nonconveX optimization. In International Conference on Machine Learning, pp.
314-323. PMLR, 2016.
Tim Roughgarden. Intrinsic robustness of the price of anarchy. In ACM Symposium on Theory of
Computing, pp. 513-522, 2009.
13
Under review as a conference paper at ICLR 2022
MUhammed O Sayin, Kaiqing Zhang, David S Leslie, Tamer BaSar, and AsUman Ozdaglar. Decen-
tralized Q-learning in zero-sum Markov games. arXiv preprint arXiv:2106.02748, 2021.
Shai Shalev-Shwartz, Shaked Shammah, and Amnon ShashUa. Safe, mUlti-agent, reinforcement
learning for aUtonomoUs driving. arXiv preprint arXiv:1610.03295, 2016.
Lloyd S Shapley. Stochastic games. Proceedings of the National Academy of Sciences, 39(10):
1095-1100,1953.
Aaron Sidford, Mengdi Wang, Lin Yang, and YinyU Ye. Solving discoUnted stochastic two-player
games with near-optimal time and sample complexity. In International Conference on Artificial
Intelligence and Statistics, pp. 2992-3002. PMLR, 2020.
David Silver, Aja HUang, Chris J Maddison, ArthUr GUez, LaUrent Sifre, George Van Den Driessche,
JUlian Schrittwieser, Ioannis AntonogloU, Veda Panneershelvam, Marc Lanctot, et al. Mastering
the game of Go with deep neUral networks and tree search. Nature, 529(7587):484-489, 2016.
KyUnghwan Son, Daewoo Kim, Wan JU Kang, David Earl Hostallero, and YUng Yi. Qtran: Learning to
factorize with transformation for cooperative mUlti-agent reinforcement learning. In International
Conference on Machine Learning, pp. 5887-5896. PMLR, 2019.
Vasilis Syrgkanis and Eva Tardos. Composable and efficient mechanisms. In ACM Symposium on
Theory of Computing, pp. 211-220, 2013.
Vasilis Syrgkanis, Alekh Agarwal, Haipeng LUo, and Robert E Schapire. Fast convergence of
regUlarized learning in games. In International Conference on Neural Information Processing
Systems, pp. 2989-2997, 2015.
Yi Tian, YUanhao Wang, Tiancheng YU, and SUvrit Sra. Online learning in Unknown Markov games.
International Conference on Machine Learning, 2021.
Katja Verbeeck, Ann Now6, Tom Lenaerts, and Johan Parent. Learning to reach the Pareto optimal
Nash eqUilibriUm as a team. In Australian Joint Conference on Artificial Intelligence, pp. 407-418.
Springer, 2002.
Oriol Vinyals, Igor Babuschkin, Wojciech M Czarnecki, Michael Mathieu, Andrew Dudzik, Junyoung
ChUng, David H Choi, Richard Powell, Timo Ewalds, Petko Georgiev, et al. Grandmaster level in
StarCraft II using multi-agent reinforcement learning. Nature, 575(7782):350-354, 2019.
Yannick Viossat and Andriy Zapechelnyuk. No-regret dynamics and fictitious play. Journal of
Economic Theory, 148(2):825-842, 2013.
Xiaofeng Wang and Tuomas Sandholm. Reinforcement learning to play an optimal Nash equilibrium
in team Markov games. Advances in Neural Information Processing Systems, 15:1603-1610, 2002.
Ying Wang and Clarence W de Silva. A machine-learning approach to multi-robot coordination.
Engineering Applications of Artificial Intelligence, 21(3):470-484, 2008.
Chen-Yu Wei, Yi-Te Hong, and Chi-Jen Lu. Online reinforcement learning in stochastic games. In
International Conference on Neural Information Processing Systems, pp. 4994-5004, 2017.
Chen-Yu Wei, Chung-Wei Lee, Mengxiao Zhang, and Haipeng Luo. Last-iterate convergence of
decentralized optimistic gradient descent/ascent in infinite-horizon competitive Markov games.
Annual Conference on Learning Theory, 2021.
Qiaomin Xie, Yudong Chen, Zhaoran Wang, and Zhuoran Yang. Learning zero-sum simultaneous-
move Markov games using function approximation and correlated equilibrium. In Conference on
Learning Theory, pp. 3674-3682, 2020.
Bora Yongacoglu, Gurdal Arslan, and Serdar Yuksel. Learning team-optimality for decentralized
stochastic control and dynamic games. arXiv preprint arXiv:1903.05812, 2019.
Kaiqing Zhang, Zhuoran Yang, Han Liu, Tong Zhang, and Tamer BasSar. Fully decentralized multi-
agent reinforcement learning with networked agents. In International Conference on Machine
Learning, pp. 5872-5881, 2018.
14
Under review as a conference paper at ICLR 2022
Kaiqing Zhang, Erik Miehling, and Tamer Basar. Online planning for decentralized stochastic control
with partial history sharing. In American Control Conference, pp. 3544-3550. IEEE, 2019.
Kaiqing Zhang, Sham Kakade, Tamer Bassar, and Lin Yang. Model-based multi-agent RL in zero-sum
Markov games with near-optimal sample complexity. Advances in Neural Information Processing
Systems, 33, 2020a.
Kaiqing Zhang, Zhuoran Yang, and Tamer Bassar. Multi-agent reinforcement learning: A selective
overview of theories and algorithms. Handbook of Reinforcement Learning and Control, pp.
321-384, 2021a.
Runyu Zhang, Zhaolin Ren, and Na Li. Gradient play in multi-agent markov stochastic games:
Stationary points and convergence. arXiv preprint arXiv:2106.00198, 2021b.
Siqi Zhang and Niao He. On the convergence rate of stochastic mirror descent for nonsmooth
nonconvex optimization. arXiv preprint arXiv:1806.04781, 2018.
Zihan Zhang, Yuan Zhou, and Xiangyang Ji. Almost optimal model-free reinforcement learning
via reference-advantage decomposition. Advances in Neural Information Processing Systems, 33,
2020b.
Yulai Zhao, Yuandong Tian, Jason D Lee, and Simon S Du. Provably efficient policy gradient
methods for two-player zero-sum Markov games. arXiv preprint arXiv:2102.08903, 2021.
15
Under review as a conference paper at ICLR 2022
Supplementary Materials for “Decentralized Cooperative
Multi-Agent Reinforcement Learning with Exploration”
A	Detailed Discussions on Related Work
A common mathematical framework of multi-agent RL is stochastic games (Shapley, 1953), which
is also referred to as Markov games. Early attempts to learn Nash equilibria in Markov games
include Littman (1994; 2001); Hu & Wellman (2003); Hansen et al. (2013), but they either assume
the transition kernel and rewards are known, or only yield asymptotic guarantees. More recently,
various sample efficient methods have been proposed (Wei et al., 2017; Bai & Jin, 2020; Sidford
et al., 2020; Xie et al., 2020; Bai et al., 2020; Liu et al., 2021; Zhao et al., 2021), mostly for learning
in two-player zero-sum Markov games. Most notably, several works have investigated two-player
zero-sum games in a decentralized environment: Daskalakis et al. (2020) have shown non-asymptotic
convergence guarantees for independent policy gradient methods when the learning rates of the two
agents follow a two-timescale rule. Tian et al. (2021) have studied online learning when the actions
of the opponents are not observable, and have achieved the first sub-linear regret O(K4) in the
decentralized setting for K episodes. More recently, Wei et al. (2021) have proposed an Optimistic
Gradient Descent Ascent algorithm with a slowly-learning critic, and have shown a strong finite-time
last-iterate convergence result in the decentralized/agnostic environment. Overall, these works have
mainly focused on two-player zero-sum games. These results do not carry over in any way to the
decentralized cooperative/team setting that we consider in this paper.
Another line of research has considered RL in teams or cooperative games. Without enforcing
a decentralized environment, Boutilier (1996) has proposed to coordinate the agents by letting
them take actions in a lexicographic order. In a similar setting, Wang & Sandholm (2002) have
studied optimal adaptive learning that converges to the optimal NE in Markov teams. Verbeeck
et al. (2002) have presented an independent learning algorithm that achieves a Pareto optimal Nash
equilibrium in common interest games with limited communication. These methods critically relied
on communications among the agents (beforehand) or observing the teammates’ actions. In contrast,
the distributed Q-learning algorithm in Lauer & Riedmiller (2000) is decentralized and coordination-
free, which, however, only works for deterministic tasks, and has no non-asymptotic guarantees. In
fact, developing provable decentralized RL in teams with stochastic environment is considered as an
open problem in Lauer & Riedmiller (2000), which we address in this paper.
Another relevant work (Arslan & Yuksel, 2016) has shown that decentralized Q-learning can converge
to Nash equilibria in weakly acyclic games (WAGs), which cover Markov teams as an important
special case. Their decentralized setting is most similar to ours in that each agent is completely
oblivious to the presence of the others. Later, such a method has been improved in Yongacoglu et al.
(2019) to achieve team-optimality. However, both of them require a coordinated exploration phase,
and only yield asymptotic guarantees. In fact, to the best of our knowledge, it is not clear if their
methods will converge in polynomial time. We significantly improve their results in this respect, by
explicitly showing a polynomial sample complexity upper bound. Similarly, decentralized learning
has also been studied in single-stage weakly acyclic games (Marden et al., 2009b) or potential games
(a subclass of WAGs) (Marden et al., 2009a; Cohen et al., 2017). Two contemporaneous works
(Zhang et al., 2021b; Leonardos et al., 2021) have studied gradient play in Markov potential games,
which also cover Markov teams. Nonetheless, they do not consider the aspect of exploring the
unknown environment, one of the key issues in RL, and/or have to assume perfect knowledge of the
environment.
In general-sum normal-form games, a folklore result is that when the agents independently run no-
regret learning algorithms, their empirical frequency of plays converges to the set of coarse correlated
equilibria (CCE) of the game (Hart & Mas-Colell, 2000). However, a CCE may suggest that the agents
play obviously non-rational strategies. For example, Viossat & Zapechelnyuk (2013) have constructed
an example where a CCE assigns positive probabilities only to strictly dominated strategies. On the
other hand, given the PPAD completeness of finding a Nash equilibrium, convergence to NE seems
hopeless in general. An impossibility result (Hart & Mas-Colell, 2003) has shown that uncoupled no-
regret learning does not converge to Nash equilibrium in general, due to the informational constraint
16
Under review as a conference paper at ICLR 2022
that the adjustment in an agent’s strategy does not depend on the reward functions of the others.
Hence, convergence to Nash equilibria is guaranteed mostly in games with special reward structures,
such as two-player zero-sum games (Freund & Schapire, 1999) and potential games (Kleinberg et al.,
2009; Cohen et al., 2017).
Efficient exploration has also been widely studied in the literature of single-agent RL, see, e.g.,
Brafman & Tennenholtz (2002); Jaksch et al. (2010); Azar et al. (2017); Jin et al. (2018); Agarwal
et al. (2020). For the tabular episodic setting, various methods (Azar et al., 2017; Zhang et al., 2020b;
Menard et al., 2021) have achieved the sample complexity of O(H3SA∕ε2), which matches the
information-theoretical lower bound. When reduced to the bandit case, our problem is also related
to the cooperative multi-armed bandit (MAB) problem (Lai et al., 2008; Avner & Mannor, 2014;
Kalathil et al., 2014; Bubeck & Budzinski, 2020), originated from the literature of cognitive radio
networks. The difference is that, in cooperative MAB, each agent is essentially interacting with
an individual copy of the bandit, with an extra caution of action collisions; in our formulation, the
reward function is defined on the Cartesian product of the action spaces, which allows the agents to
be coupled in more general forms. A concurrent work (Chang et al., 2021) has studied cooperative
multi-player multi-armed bandits with information asymmetry, which parallels our matrix team
setting. Nevertheless, (Chang et al., 2021) requires stronger conditions than our decentralized setting
as their algorithm relies on playing a predetermined sequence of actions.
B Discussions on Markov Potential Games
An N -player episodic Markov potential game can be described by a tuple G =
(N, H, S, {Ai}iN=1, {ri}iN=1, P), where (1) N = {1, 2, . . . , N} is the set of agents; (2) H ∈ N+ is
the number of time steps in each episode; (3) S is the finite state space; (4) Ai is the finite action
space for agent i ∈ N ; (5) ri : [H] × S × A → [0, 1] is the reward function for agent i, where
A = ×iN=1Ai is the joint action space; and (6) P : [H] × S × A → ∆(S) is the transition kernel.
We remark that the agents could have different reward functions, but the reward that an agent receives
also depends on the joint actions of all the agents.
We define the space of policies for agent i by Πi = {πi : [H] × S → ∆(Ai)}. The joint policy space
is denoted by Π = ×iN=1Πi. For a joint policy π ∈ Π, and for any h ∈ [H], s ∈ S, and a ∈ A, we
define the value function for agent i as:
H
Vπ,i(s) = ESh0 + ι~Ph(∙∣Sh0 %) X rh o (ShO ,ah0) | sh = S .	⑷
h0=h
We call G a Markov potential game if for every h ∈ [H] and S ∈ S, there exists a function
Φsh : Π → R, such that
Φsh(πi,π-i)-Φsh(πi0,π-i)=Vh(πi,π-i),i(S)-Vh(πi0,π-i),i(S),
for all i ∈ N, πi , πi0 ∈ Πi and π-i ∈ Π-i. Such a definition extends the normal-form potential
games (Monderer & Shapley, 1996) to state-dependent games. In words, Φsh can be considered as a
potential function such that any unilateral change in an agent’s policy causes equal amount of change
to the potential function and its own value function. It is known that every Markov potential game
possesses at least one deterministic Nash equilibrium.
We can easily see that Markov teams are a special case of Markov potential games, where the potential
function is simply the team value function. Since all the agents are correlated by a single potential
function in Markov potential games, the process of learning Nash equilibria reduces to searching
for the local optima of the potential function, which is exactly the core idea behind our design of
Algorithm 1 for Markov teams. One can see that Lemmas 4 and 5 immediately generalize to Markov
potential games, and hence the same sample complexity bounds from Theorems 1, 2 and 4 also hold
in such a broader class of games.
17
Under review as a conference paper at ICLR 2022
C	Auxiliary Lemmas
C.1 Properties of the Bregman Divergence
Let ω : X → R be a continuously differentiable mirror map that is 1-strongly convex on X,
where X is a non-empty closed convex set. In this paper, we specifically take ω as the L2 -induced
regularize], i.e., ω(x) = 1 ∣∣xk2, although the following result holds more generally. Let Dω(x, y)=
ω(x) — ω(y) -hx — y, Vω(y)i be the ω-induced Bregman divergence.
Lemma 2. (Beck, 2017, Section 9.2.1). The Bregman divergence satisfies the following properties:
(a)	Three-point identity:
Dω(x, y) + Dω(y, z) - Dω(x, z) = hVω(z) - Vω(y), x - yi, ∀x, y, z ∈ X.
(b)	Suppose 夕(x) is Convex and z+ = argmin {夕(x) + 1 Dω (x, z)} forsome α > 0, we have
x∈X	α
0(x) + α°ω (x, Z) ≥ 2(z+) + ~Dω (z+,z) + ~^Dω (x,Z+) , ∀X ∈ X.
C.2 Properties of the Gradient Mapping
Lemma 3. (Agarwal et al., 2021, Proposition B.1). Let L : X → R be a β-smooth function. Define
the gradient mapping as
Gn(x) = (∏ (Πχ (X + ηVL(x)) — x).
η
The update rule for projected gradient ascent is x+ = x + ηGη (x). If ∣Gη (x)∣2 ≤ ε, then
max	δlVL(x+) ≤ ε(ηβ + 1).
x+δ∈X,kδk22≤1
D	An Extended Discussion of Section 3
We first introduce a few more notations for ease of presentation. Recall that r : A × B → [0, 1] is the
expected value of the reward function, i.e., r(a, b) = R01xdPa,b(x). We further let Q ∈ RA × RB be
the matrix of expected rewards. That is, given a mixed strategy pair ∏ = (μ, V), the corresponding
expected reward for the players can be denoted by V(π) = V(μ, V) = Eμ×ν[r(a, b)] = μlQν,
where a 〜 μ and b 〜V. The function V is also called the value function.
In certain circumstances, it is more convenient to work with costs instead of rewards. We let
l : A × B → [0, 1] be the expected cost function such that l(a, b) = 1 - r(a, b), ∀(a, b) ∈ A × B.
Let M = 1A×B - Q be the matrix of expected costs, where 1A×B is an all-one matrix of the size
def
A X B. Similar to the value function V, we define L(π) = L(μ, V) = Eμ×ν[l(a, b)] = μlMν as the
corresponding loss function.
Our definition of an approximate Nash equilibrium (Definition 1) can be easily reduced to the matrix
team setting as follows.
Definition 2. (ε-approximate NE for matrix teams). For ε > 0, a strategy pair (μ?,ν?) is an
ε-approximate Nash equilibriumfor a matrix team if V (μ?, V ?) ≥ V (μ, V ?) — ε, ∀μ ∈ ∆(A), and
V(μ?, V?) ≥ V(μ?, V) — ε, ∀v ∈ ∆(B).
In the following, we introduce the definition of an approximate stationary point.
Definition 3. (ε-approximate StatiOnary point). A Strategy pair π = (μ, V) ∈ ∆(A) × ∆(B) is
a (first-order) ε-approximate stationary point for the function V : ∆(A) × ∆(B) → R iffor any
δι ∈ RA, δ2 ∈ RB such that ∣∣διk2 + ∣∣δ2∣2 ≤ 1, μ + δι ∈ ∆(A), and V + δ2 ∈ ∆(B), it holds that
δ∣VμV(π)+ δ∣VνV(∏) ≤ ε.
18
Under review as a conference paper at ICLR 2022
Intuitively, π is a stationary point if the value function V cannot increase by more than ε along any
direction that lies in the intersection of the strategy simplex and the neighborhood ofπ. Comparing the
definitions of an approximate stationary point with an approximate NE easily leads to the following
observation.
Lemma 4. Let π? = (μ?,ν?) be an ε-approximate stationary point of the valuefunction V for some
ε > 0. Then, the strategy pair (μ?, ν?) is a √2ε-approximate Nash equilibrium of the corresponding
matrix team.
All missing proofs of this section are deferred to Appendix E for clarity. With Lemma 4, learning a NE
of the team reduces to finding a stationary point of the corresponding value function, or equivalently,
the loss function. In what follows, we focus on the loss function L for reasons that will be clear soon.
Numerous SGD methods have been proposed to find a stationary point in stochastic non-convex
optimization (Ghadimi & Lan, 2013). For generality, we write our gradient step as an Online Mirror
Descent (OMD) update with L2 regularization to search for the stationary point. The standard OMD
algorithm performs the following update rule at each iteration:
1
xt+1 J argmin ∏VL(xt),xi + α0ω(x,xt)
(5)
where X = ∆(A) × ∆(B) is the feasible region, VL(xt) is an estimator of the subgradient of L at xt,
αt > 0 is the stepsize, ω : X → R is a 1-strongly convex and continuously differentiable regularizer,
and Dω (x, xt) = ω(x) - ω (xt) - hVω (xt) , x - xti is the Bregman divergence induced by the
regularizer ω .
The following nuance in the general OMD update requires some attention before we directly apply
existing results. Since the optimization objective L is a function of π ∈ ∆(A) × ∆(B), existing
methods typically proceed in a centralized way and manipulate π as a whole. In contrast, in
our decentralized setting, the strategies μ and V need to be updated separately and independently.
However, such a nuance does not exist with SGD. Specifically, if we instantiate ω as an L2-induced
regularizer, i.e., ω(x) = ɪ ∣∣χ∣∣2, then our OMD procedure is equivalent to projected SGD (Lattimore
& Szepesvdri, 2020, Example 28.2). In this case, the Bregman divergence simplifies to Dω(x, Xt)=
1 ||x 一 Xtk2. The update rule in (5) can hence be simplified to
xt+1 J ΠX x - αtVL(xt)
where ΠX (X) denotes the Euclidean projection of X onto X. We then point out the following simple
fact, which states that conducting projected SGD on the optimization variable ∏ = (μ, V) as a whole
is equivalent to running the same algorithm on each component of π independently.
Lemma 5. Let π = (μ,ν) be a strategy pair, and let V∏ L(π) = (VμL(π), VV L(∏)) be an arbitrary
(not necessarily unbiased) gradient estimator of L at π. Let ∏+ = ∏δ(A)×δ(B)(∏-ɑV∏ L(∏)) be the
result ofa centralizedgradient update step with stepsize a > 0, and let μ+ = ∏∆(A)(μ — αVμL(π))
and V+ = Π∆(B) (V 一 αVbνL(π)) be the results of applying the gradient update to each agent’s
strategy independently. Then, π+ = (μ+, V+).
Therefore, it suffices to only analyze the centralized OMD algorithm with L2 regularization, as it
is equivalent to decentralized SGD. Since our algorithm is essentially performing gradient descent
instead of ascent, we choose the loss function L as our optimization objective.
We now turn to the gradient estimator VL(X). Existing stochastic non-convex optimization meth-
ods (Ghadimi & Lan, 2013) typically assume the existence of a stochastic oracle: Given any input
X ∈ X, the oracle outputs a random vector Vb L(X, ξ)3 that is an unbiased estimator of the gradient and
has bounded variance. More specifically, it is assumed that for any X ∈ X , Eξ [Vb L(X, ξ)] ∈ ∂L(X),
where ∂L(x) is the subdifferential set of L at x, and Eξ[||VL(χ,ξ)∣∣∣] ≤ P for some constant P > 0.
The latter condition is also called the ρ-stochastic continuity of L (Lu, 2019). Such an oracle does
not come for free in our problem, as the agents are faced with bandit feedback and can only observe
3 ξ is a random variable that is used to explicitly show the randomness of the estimator. When there is no
possibility of ambiguity, we suppress the ξ term and simply write the estimator as VL(x).
19
Under review as a conference paper at ICLR 2022
the reward associated with the selected action pair. To deal with this challenge, at the t-th round of
the problem, We let agent 1 construct a biased gradient estimator VμLt m the form of
1 - Rt
Vμ,iLt = ------.— I{at = i},∀i ∈ A,
μt,i + Y
where μt,i is the probability that agent 1 takes action i ∈ A in round t, and γ > 0 is a constant
specified in Algorithm 1. The biased gradient estimator VνLt for agent 2 is constructed similarly.
We can see that the estimator we use tends to underestimate the actual cost. Such a technique is called
Implicit eXploration (Neu, 2015) in the literature, and is widely used to prove high-probability regret
bounds in multi-armed bandits, an objective that we do not pursue in this paper.
Our analysis of Algorithm 1 relies on showing that the biased estimator VLt we constructed has
bounded variance, and is upper bounded by the actual gradient in expectation, which satisfies the
oracle requirements in a weaker sense. We can then analyze the non-asymptotic convergence behavior
of our SGD algorithm to a stationary point. For generality, the proof we present in this paper follows
the analysis of a general proximal point method (Davis & Drusvyatskiy, 2018; Zhang & He, 2018),
by showing that the gradient of the Moreau envelope converges to 0, although simpler proofs for the
convergence of SGD in non-convex optimization exist. Finally, we invoke Lemma 4 to conclude that
the stationary point we find is close to a NE.
E Missing Results in Section 3 and Appendix D
E.1 Proof of Lemma 4
Proof. Let π? = (μ?, V?) be an ε-approximate stationary point of the value function V. For any
strategy μ ∈ ∆(A),
V(μ, V?) - V(μ*, V?) = (μ - μ?)lQν? = √2 ∙ ^(μ - μ?)lVμV(π*) ≤ √2ε,
where the last step follows from the ε-approximate stationarity of ∏? and ∣∣ √^(μ 一 μ?" ≤ 1. A
similar argument can show that V(μ?, V) 一 V(μ?, V?) ≤ √2ε, ∀ν ∈ ∆(B). Therefore, (μ?, ν?) is a
√2ε-approximate NE of the matrix team.	口
E.2 Proof of Lemma 5
^
^
^
Proof. Let π = (μ, V). First, notice that π — αV∏L(π) = (μ — αVLμ(∏),V — αVLV(∏)). Then,
by the definition of the Euclidean projection,
π+ =Π∆(A)×∆(B)
π - αVbπL(π)
∣∣π - αVb π L(π) - x∣∣
2
argmin
x∈∆(A)×∆(B)
2
argmin (∣∣μ - αVμL(∏) - xι∣∣ + ∣∣v
x1∈∆(A),x2∈∆(B)	2
2
^
- αVb νL(π) - x2∣∣
2
=argmin ∣∣μ — αVμL(∏) — xι∣∣ + argmin ∣∣v — αVVL(∏) — x21∣
x1 ∈∆(A)	2 x2∈∆(B)
=∏∆(A) (μ — αVμL(∏)) + ∏∆(B) (V — αVvL(∏))
=μ+ + V +.
This completes the proof of the lemma.
2
□
E.3 Proof of Lemma 7
Proof. Our proof relies on the smoothness of the function L and a gradient mapping property
(Proposition B.1 in Agarwal et al. (2021), also reproduced as Lemma 3 in our Appendix C for
20
Under review as a conference paper at ICLR 2022
completeness). First, recall the definition that X = argmi□z∈χ {L(z) + λDω (z, x)}, and notice that
the function L(z) + λDω(z, x) is (λ - 1)-strongly convex in z for any fixed x ∈ X. We know from
Lemma 2 that
L(X) — [L(X) + λDω (X, x)] ≥ (λ — 1)Dω (x, X).
Therefore, we can derive that
L(X) — L(X) — Dω (X, x) =L(X) — (L(X) + λDω (X, x)) + (λ — 1)Dω (X, x)
≥(λ — 1)(Dω (x,X) + Dω (X,x))
=(λ - 1) kχ ― χk2.
Itis easy to verify that L(χ) is an (A + B)-smooth function, in the sense that kVL(χ) — VL(y)k2 ≤
(A + B) kX — yk2. We can hence invoke Lemma 3 and conclude that X is an approximate stationary
point, such that
XTVyn ʌ L 2(2AmaX + λ)√ε
max	δτVL(x) ≤ ---------/	-----.
x+δ∈X,kδk22≤1	λ — 1
□
E.4 Proof of Theorem 1
Proof. From Lemma 5, we know that it suffices to analyze Algorithm 1 from a centralized perspective,
because running projected gradient descent on the joint strategy X = (μ, ν) as a whole is equivalent
to updating each agent,s strategy μ and V individually. To show that XT = (μτ, VT) is an approximate
Nash equilibrium, we also know from Lemma 4 that the problem reduces to showing its approximate
stationarity with respect to the loss function L.
For generality, the proof we present in this paper follows the analysis of a general proximal point
method (Davis & Drusvyatskiy, 2018; Zhang & He, 2018), although many simpler proofs are given
in the literature to show the convergence of SGD in non-convex optimization. To facilitate the
analysis, we first introduce the following definition of the Bregman-Moreau envelope (Moreau, 1965;
Drusvyatskiy & Paquette, 2019; Davis & Drusvyatskiy, 2018; Zhang & He, 2018).
Definition 4. (Bregman-Moreau envelope). For λ > 0, given a function L : X → R and a vector
z ∈ X, the Bregman-Moreau envelope is defined as
Lλ(z) = min "(x) + 1 Dω(x, z)l .
x∈X	λ
The corresponding Bregman proximal operator is
Proxλ,L(z) = argmin {l(x) + -^Dω(x, z)}.
We start with the observation that L is a 1-weakly convex function, in the sense that L(X) + ω(X)
is convex for ω(x) = 1 ∣∣xk2. It then follows that for any 0 < λ < 1, the proximal operator is
well-defined and unique.
Lemma 6. (Zhang & He, 2018, Lemma 2.2). Suppose L is 1-weakly convex on X, and let 0 < λ < 1.
Then, for any vector Z ∈ X, the function L(x) + 1 Dω (x, z) is (λ-1 — 1)-strongly convex in X, and
the Bregman proximal operatorproxλ,L(z) is uniquely defined.
For simplicity of notations, in the following, we use Xt = (μt,%) ∈ X to denote the strategy pair
used by the two agents at round t ∈ [T]. For any λ > 1 and x ∈ X, we let
def
χ = prox1∕λ,L(X),
and we know from Lemma 6 that X is uniquely defined. By the definition of the Bregman envelope,
Lι∕λ(xt+1) = L(Xt+ι) + λDω (Xt+ι, χt+ι). The optimality of Xt+1 implies that
L1∕λ(xt+1) ≤ L(Xt) + λDω(Xt,χt+ι).	(6)
21
Under review as a conference paper at ICLR 2022
= 一
Let VLt
A+B
(VμLt, VνLt) ∈ RA+B be the biased gradient estimator constructed by Concate-
nating the two players’ individual estimates in Line 5 of Algorithm 1. Further, let VL(xt ) =
(VμL(χt), VVL(Xt)) = (Mνt, M|μ) be the actual gradient of L at xt, a value that we do not have
access to and is only used for analytical purposes. Our proof relies on a few properties of the Bregman
divergence, which are standard results from the literature and are reproduced in Appendix C for
completeness. Setting Z = xt,z+ = xt+ι, X = Xt, α = αt, and 夕(x) =(VLt, Xi in Lemma 2(b)
(Appendix C) leads to
at(VLt, Xt — Xt+1)≥ Dω (Xt,Xt+l) + Dω (xt+1,Xt) — Dω (Xt,Xt).
Rearranging and combining (6) and (7),
L1∕λ(Xt+1) ≤ L(Xt) + λDω(Xt,Xt+ι)
≤L(Xt) + λ (at(VLt,Xt — Xt+1)— Dω (Xt+1,Xt) + Dω (Xt,Xt))
= L1∕λ(Xt) + λ (at (VLt, Xt- Xt+1)— Dω (Xt+1, Xt))
(7)
= L1∕λ(Xt) + λat (VLt, Xt
— Xt + λ at VeLt, Xt — Xt+1 — Dω (Xt+1, Xt) ,
(8)
where the first equality is by the definition of the Bregman-Moreau envelope. The second term in (8)
can be further decomposed into
，=— . , _____________ , ，= _ ___________, . , ，= _ ________,
(VLt, Xt — Xti = EL(Xt), Xt — Xti + (VLt — VL(Xt), Xti — (VLt — RL(Xt), Xti.	(9)
×------V-----} X---------V-------} X--------V-------}
① ② &
In the following, we bound (the expected value of) each term in (9) separately. First, since L is
1-weakly convex on X, the first term in (9) is bounded by
①=(VL(Xt), Xt-	Xti	≤	L(Xt)- L(Xt) + 2 IlXt- Xtk2 =	L(Xt)-	L(Xt)	+ Dω (Xt,	Xt).	(IO)
Second, by our construction of the biased estimator, for any i ∈ A,
1 —Rt
E[Vμ,iLt] = E ------1-I{at = i}
Lμt,i + Y	」
1 — Rt
≤E ---------tI{at = i}
_ μt,i	.
Σ>(bt=j) ∙ E
j∈B
I-Rt I{at = i}
μt,i
EPet=j) ∙ l(i,j) = V*,iL(Xt),
j∈B
where here EH denotes the conditional expectation given the history of both agents up to the current
round, and Vμ,iL(Xt) denotes the i-th entry of the actual gradient V*L(xJ A similar result holds
for agent 2, that is, E[Vν,j Lt] ≤ Vν,jL(Xt), ∀j ∈ B. Hence, we know that E[VLt — VL(Xt)] 0,
where W denotes element-wise comparison. Since Xt lies in the the domain X = ∆(A) X ∆(B), it is
a non-negative vector, i.e., Xt 占 0. We can therefore upper bound the expected value of the second
term in (9) by
____ r	____ ~	_	__，	、	..
E[②]=E[(VLt — VL(xt), Xti] ≤ 0.	(11)
Finally, recalling that Xt = (μt, νt), we decompose the third term in (9) into two parts,
_ ，= _ ________, , ____________, = _ , _____________, = 一 ,
—③=-(VLt- VL(Xt), xti = "μL(Xt)- VμLt,μti + (VVL(xt) — VVLt,νti.
In the following, we show an upper bound of the first part, and the proof for the second part follows
similarly. Using the definition of the inner product,
”μL(Xt)- VμLt, μti = X (V*,iL(Xt)- Vμ,iLt) ∙ Mt,i	(12)
where μt,i is the i-th component in the vector μt. Again, by taking the expectation of the biased
gradient estimator,
1— Rt
E[Vμ,iLt] =E ------1-I{at = i}
Lμt,i + Y	.
£P(bt =j) ∙ E
∈B
上* I{at = i}
μt,i + γ
E Vt,j ∙
∈B
l(i, j)μt,i
μt,i + γ
22
Under review as a conference paper at ICLR 2022
Plugging it back to (12), and recalling the definitions that VμL(χt) = MVt and Vμ,iL(χt)
Pj∈B l(i,j)νt,j, we have that
E [hVμ L(Xt)-V μLt,μtii = X(X l(i,j)νt,j - X Vt,j∙ l(i,j +tt,i 1 ∙ μt,i
i三A ∖j三B	j∈B	μt,i + γ J
=X Xl"{- μμ+γ) ∙…
∈ j∈
l(i,j)νt,j
i∈Aj∈B
Y
1 + γμt,i
≤	l(i, j)νt,jγ
i∈Aj∈B
≤Aγ,
where the first inequality is due to 1+γ∕μt,i ≥ 1, and the second inequality uses the facts that l(i,j) ≤
1, ∀i ∈ A, j ∈ B and Pj∈B νt,j = 1. A similar argument shows that E hVνL(xt) - VeνLt, νti ≤
Bγ. Combining the two parts, we conclude that the third term in (9) can be bounded by
_________________,	.	、r	,	.	_ .
-E[③] = -EKVLt- VL(Xt), xti] ≤ (A + B)Y ≤ 2AmaχY.
(13)
From (9), we know that inequalities (10), (11), and (13) together can upper bound the expected value
of the second term in (8). Finally, to bound the third term in (8), we apply Young’s inequality and
obtain almost surely that
at DVLt, Xt - Xt+1)- Dω (Xt+1, Xt) = at DVLt, Xt - Xt+1)- 2 kXt+1 - Xtk2
v1 2 Ilvr l∣2	1 2(X (1 - Rt)2I{at = i}	X (1 - Rt)2I{bt = j}ʌ
≤2atI|VLtll2 = 2at 邑 (μt,i + Y)2	+ jδ	(νt,j+ Y)2	)
∈	j∈
≤ AmaXat
—Y2
(14)
where the last step holds because (μt,i + Y)2 ≥ γ2, and (1 一 Rt)2I{at = i} ≤ 1 almost surely.
Substituting (10), (11), (13), and (14) back to (8), we finally conclude that
E[L1∕λ(Xt+1)] ≤ E[L1∕λ(Xt) + λαt (L(Xt)- L(Xt) + Dω (Xt, xt) + 2AmaxY)] +
Rearranging, taking the sum from t = 1 to T , and telescoping leads to
T
^X E[αt (L(Xt)- L(Xt)- Dω (Xt, Xt))]
t=1
Amaxλαt
γ2
1	T AT
≤ λ E[L1∕λ(X1) - Lι∕λ(Xτ +ι)]+ 2AmaχY fat +	∑ a2
t=1	Y t=1
1	T AT
≤V + 2AmaxY X at +-2— X at ,	(15)
λ	t=1	Y2 t=1 t
where in the last step we used the fact that 0 ≤ Lι∕λ(X) ≤ 1, ∀x ∈ X. In the following lemma, we
show that bounding the LHS of (15) is sufficient to obtain an approximate stationary point.
Lemma 7. Let X = Proxi6,l(x) With λ > 1. For any ε > 0, if L(x) - L(X) - Dω (X, x) ≤ ε, then
X is a 2(2 Amax+λ)^ε-approximate Stationary point of the value function V.
Dividing both sides of the inequality by PtT=1 at , we obtain that
PT=1 E[αt(L(xt) - L(Xt) - Dω(Xt,xt))] / 1 + ⅜xλ PT=I a2 …7t
―=-------------E----------------------- ≤ ----QT------------ + 2AmaxT
t=1 at	λ t=1 at
(16)
23
Under review as a conference paper at ICLR 2022
Recall that in Line 7 of Algorithm 1, the two agents uniformly sample a time index τ from {1, . . . , T }
using a common random seed. Let x「= (μτ, VT) be the output of the algorithm. Then, We can
see that the LHS of (16) is equivalent to E[L(χτ) - L(Xτ) - Dω (Xτ, Xτ)], because we assign equal
values to the step sizes αt . Plugging back the parameter values αt = T -3/4 and γ = T-1/4, and
letting λ = 2 yields,
E[L(Xτ) - L(Xτ) - Dω (Xτ,Xτ)] ≤ 4AmaχT-1/4.
Together with Lemma 7, we can see that x「is a 16，/3&*/71/4-approximate stationary point of
V in expectation. Finally, invoking Lemma 4, we can conclude that x「is an 16y2Λ3maX∕T1∕4 -
approximate Nash equilibrium in expectation. In other words, to obtain an ε-approximate NE, it
suffices to use O(AmaX∕ε8) samples.	□
E.5 Proof of Theorem 2
We first describe more explicitly the new variance-reduced algorithm that the agents are using, which
has been skipped in the main text due to space limitations. Again, we only illustrate the algorithm for
agent 1, as its counterpart for agent 2 is symmetric.
Similar to vanilla SGD (with no variance reduction) in Algorithm 1, agent 1 maintains a probability
distribution μt over its action space for each time step t ∈ [T]. Such a distribution is uniformly
initialized, i.e., μ1,i = 1∕A, ∀i ∈ A. At each time step t ∈ [T], the agent first draws an action at
according to a perturbation of the distribution μt = (μt,1,..., μt,A to be specified later. It then
executes the selected action at and observes the team reward Rt. Subsequently, the agent updates its
probability distribution by following a variance-reduced gradient descent step.
The gradient update rule that we use and its convergence analysis are presented independently in
Appendix F, as it can be considered as a solution to a standard constrained optimization problem on its
own. Specifically, in Appendix F, we present a (projected) stochastic gradient descent algorithm that
finds a stationary point in constrained smooth non-convex optimization at a rate of O(1∕T 1/3 ). Our
method is a straightforward extension of the STOchastic Recursive Momentum (STORM) algorithm
proposed in Cutkosky & Orabona (2019), which uses a momentum-based approach to reduce the
variances in SGD.
It is important to note that we rely on a non-adaptive variant of the STORM algorithm. In the original
STORM (Algorithm 1 of Cutkosky & Orabona (2019)), the adaptive step size ηt is calculated in a
data-dependent manner (using knowledge of kVf (xt, ξt)k). This is ordinarily a desired property,
as it alleviates the need of manual parameter tuning, and also finds wide applications in heuristic
methods like Adam (Kingma & Ba, 2014). However, such adaptivity breaks our decentralized
paradigm, because the agents update the step sizes using the local samples, and hence can end up
with different step size values. This breaks the equivalence (between centralized and decentralized
SGD) that we established in Lemma 5, which critically relies on equal step size values among all
agents. Consequently, we need to refer to a non-adaptive variant of STORM (Cutkosky & Orabona,
2019) that replaces the data-dependent values with a universal upper bound σ .
Finally, to prove Theorem 2, we only need to instantiate the various notations (Ft , Vft, and ξt)
of Appendix F in the context of the matrix team problem, and verify that all the assumptions in
Appendix F are indeed satisfied. Then, we can complete the proof by invoking the convergence
guarantee of Theorem 6 from Appendix F. It is easy to see that Ft represents the loss function L in
Section 3, and the random variable ξt is associated with the action drawn at time step t. The notation
Vf deserves more commentary. To ensure that the bounded variance assumption from Appendix F is
satisfied, we cannot let the agent draw an action according to the distribution μt, but instead according
to a θ-greedy parameterization of μt for some θ > 0. Specifically, for a probability distribution μt at
time t, the agent samples an action at according to μt = (μt,1,..., "t,a), where:
μt,i = (1 — θ)μt,i + θ∕A, ∀i ∈ A.
To put it in a different way, with probability 1 - θ, the agent randomly selects an action ac-
cording to the distribution μt; with probability θ, the agent uniformly samples at from A. The
gradient estimator is instantiated with respect to the θ-greedy parameterization as Vf (μt, ξt)=
(V1f(μt, ξt),∙∙∙, VAf(μt, ξt)), where
1 - Rt
vif (μt,ξt) = ------I{at = i},∀i ∈ A.
μt,i
24
Under review as a conference paper at ICLR 2022
Algorithm 4: STORM (CUtkosky & Orabona, 2019) with Projections
ι di J W(xι,ξι);
2	for t - 1 to T do
3	ηt J (w+σ2t)1∕3 ;
4	xt+1 J ΠX (xt - ηtdt);
2
5	at+1 J cηt2 ;
6	dt+i J Vf (Xt+ι,ξt+1) + (1 - at+ι)(dt - Vf (χt, ξt+1));
7	OUtpUt xτ where τ is Uniformly sampled from {1, . . . , T};
We can see that Vf (μt, ξt) is an unbiased gradient estimator. Further, since μt,i ≥ θ∕A, ∀i ∈ A, we
can also verify that Vf (μt, ξt) has a bounded variance. Simple calculations show that Assumption 1
in Appendix F holds under the following instantiation of parameters:
2A	4A2
2	max	max
σ = —θ-, and L = -ffΓ~.
We can then invoke Theorem 6 to conclude that
E
1 , +	、『]Mw1/3 Mσ2∕3
ητ(XT +1 - xτ)11 ≤ Tk + T2/3k
C ln T ( Amax Y1∕3
≤ T2/31 θ
where XT = (μτ ,ντ) and C is some absolute constant. To ensure Eu n- (χ++ι 一 XT )∣∣J ≤ ε, it
suffices that T = Ω ((θ)3∙ 5). Ifwe further set θ = ε2/11, then we only need T = Ω (A* 14-).
From the gradient mapping property (Lemma 3), we know that IlnT(X++1 -χτ)∣∣ ≤ ε implies a 4 ε-
approXimate stationary point, where we used the condition that ητ ≤ 十 in Appendix F. Finally, we
invoke Lemma 4 to conclude with an ε-approximate Nash equilibrium after T = Ω(1∕ε4)∙poly(Amaχ)
rounds. This completes the proof of Theorem 2.
F SGD with Variance Reduction
In this section, we present a (projected) stochastic gradient descent method with variance reduc-
tion (Johnson & Zhang, 2013; Allen-Zhu & Hazan, 2016; Reddi et al., 2016). This method proceeds
by adding a simple projection step to a non-adaptive variant of the STOchastic Recursive Momentum
(STORM) algorithm proposed in Cutkosky & Orabona (2019).
We consider a generic stochastic non-convex optimization problem as follows: We are given an
objective function F : RA → R, and our goal is to find a point X ∈ X ⊆ RA such that VF (X) is close
to 0, where X is the feasible region. We do not have accurate information about the function F, and
can only access it through a stochastic sampling oracle f (∙,ξ), where the random variable ξ represents
the “randomness” of the oracle. Throughout this section, we make the following assumptions that are
standard in the literature (Arjevani et al., 2019). All norms ∣∣∙k in this section are L2 norms unless
otherwise specified.
Assumption 1. 1. We have access to a stream of random variables ξ1,. .. , ξT, such that the gra-
dient estimators are unbiased and have bounded variances: VEξt [f (X, ξt)] = VF (X), and
E[∣Vf (X, ξt) - VF(X)∣2] ≤ σ2 for some σ > 0 for all t ∈ [T] and X ∈ X.
2. The objective F has bounded initial sub-optimality and is L-smooth: F(X0) - inf x∈X F (X) < ∞,
and ∣VF(x) -VF(y)∣ ≤ L ∙ ∣x - y∣ ,∀X,y ∈ RA for some L > 0. The stochas-
tic oracle also satisfies a mean-squared smoothness property for the same constant L:
E[∣Vf (x, ξ) — Vf (y,ξ)∣2] ≤ L2 ∙ ∣∣x — y∣2, ∀x, y ∈ RA with probability 1.
Our method STORM with projections is formally presented in Algorithm 4. STORM uses a variant
of momentum to reduce the variance of the gradients in SGD. It achieves an optimal convergence
rate of O(1∕T1/3), which improves over the standard convergence rate O(1∕T1/4) of SGD with no
25
Under review as a conference paper at ICLR 2022
variance reduction. Before we present the convergence guarantee of Algorithm 4, we first introduce a
few notations for ease of presentations.
For any t ∈ [T], we break the update rule into two steps:
def
xt+1 = Xt - ηtdt, and xt+ι = Πχ(xt+ι).
In addition, for each t ∈ [T], we define Xt++1 d=ef ΠX (Xt - ηtVF(Xt)) to be the next iterate updated
using the full gradient VF (Xt), a value we do not have access to. Define εt d=ef dt - VF(Xt)
to be the error in dt. The high-level procedure of our proof is to seek to upper bound the value
E
PT=II η1 (χ++ι - Xt) U , and then to invoke the gradient mapping property in Lemma 3 to
conclude with a stationary point. This is in contrast with the unconstrained case, where Cutkosky &
Orabona (2019) directly derive an upper bound of E [PT=ι kVF(Xt)Il2]. In the following, We start
with a few technical lemmas.
Lemma 8. Suppose η ≤ 4l forall t ∈ [T]. Then,
E[F(xt+ι)-F(Xt)] ≤E -16^ ∣∣xt+ι-XtU + ^^8t ∣∣εtk2
Proof. From the first-order optimality condition, we know that
hX - Xt+1, Xt+1 - (Xt - ηtdt)i ≥ 0,
for any X ∈ X . Taking X = Xt leads to
hXt - Xt+1,Xt+1 - Xti + hXt - Xt+1, ηtdti ≥ 0,
which in turn implies that
hXt+1 - Xt, dti ≤---IlXt - Xt+1『.	(I7)
ηt
It follows that
hVF(Xt), Xt+1 - Xti = hdt - εt, Xt+1 - Xti
≤------IlXt - Xt+i∣∣2 - hεt, Xt+i - Xti
ηt
≤	- - llXt - Xt+i||2 + η l∣εt∣∣2 + 而 llXt+i - Xt||2
=	-而 llXt - Xt+i||2 + ηt kεt∣∣2,
where the first inequality uses (17), and the second inequality is due to Holder,s inequality and
Young’s inequality. From the smoothness of F,
E[F(Xt+1)] ≤E F(Xt) + hVF(Xt), Xt+i - Xti + 2 llXt+i - Xt『
≤e F(Xt)-而 llXt - Xt+i||2 + η ∣∣εt∣∣2 + £ llXt+i - Xt||2
≤E F(Xt)-即"llXt - Xt+i||2 + Tt I∣εt∣l2 ,	(18)
where the last step uses η ≤ 芸.From the fact that ∣x + y∣∣2 ≤ 2 ∣∣x∣2 + 2 ∣∣y∣∣2, We know
∣∣Xt++1 - Xt∣∣2 = ∣∣Xt++1 - Xt+1 + Xt+1 - Xt∣∣2 ≤ 2 ∣∣Xt++1 - Xt+1∣∣2 + 2 lXt+1 - Xtl2 .
26
Under review as a conference paper at ICLR 2022
Rearranging the terms leads to
-	kχt — xt+ι∣∣2 ≤ ∣∣xt+ι — xt+ι I I — 21 I xt+ι — Xtll
≤ IlnX(Xt- ηtVF(xt)) - Πχ(χt - ηtdt)∣∣2 - 2 ∣∣χt+ι - χt∣∣2
≤ll(χt -UNF(Xt)) - (Xt -ηtdt)l∣2 - 2 ||x++i - xt12
=η2 l∣εtk2 - 1 ∣∣χ++ι - χtH2 ∙	(19)
The second inequality uses the definition of χ++1. The third step holds because the projection operator
is non-expansive, i.e. ∣Πχ(χ) - Πχ(y)∣ ≤ ∣∣χ - y∣. Substituting (19) back to (18) leads to
E[F(χt+ι)] ≤ E F(Xt) -16^- ∣∣χt+ι - χt∣∣2 + 弋 l∣εtll2 ∙
Rearranging the terms completes the proof.	□
Lemma 9. (Lemma 3 in Cutkosky & Orabona (2019)). For any t ∈ [T], it holds that
E [(1 - aty (Vf(Xt,ξt)-VF(χt)) ∙ εt-J =0,
L ηt-ι	」
E [(1 - at)(Vf (χt, ξt) - Vf(Xt-ι,ξt) - VF(Xt) + VF (Xt-I)) ∙ εt-J = 0.
L ηt-ι	」
Lemma 10. (Adapted from Lemma 5 in Cutkosky & Orabona (2019)). With the notations in
Algorithm 4, we have
E
∣εtk2
ηt-ι
≤ E [2c2η3-1σ2 + (4鸟2-1 + 1)(I- at)2 kεt-1k2 + 4 (I- at)2 L2
ηt-ι
ηt-ι
∣∣χ+ - χt-1H2
Proof. First, observe that
E h∣Vf (χt,ξt) - Vf (χt-ι,ξt) - VF(Xt) + VF(Xt-I)II2]
≤E [∣Vf(χt,ξt) - Vf(Xt-ι,ξt)∣2 + ∣VF(χt) - VF(Xt-I)II2]
-	2E [(Vf (χt, ξt) - Vf (χt-ι,ξt), VF(χt) - VF(Xt-1)〉]
≤E [IVf (χt,ξt) - Vf(Xt-ι,ξt)I2 + IVF(Xt) - VF(Xt-I)II2]
-	2E [E [(Vf (χt, ξt) - Vf (χt-ι,ξt), VF (χt) - VF (Xt-1))| ξι,∙∙∙, ξt-ι]]
=E [IVf(χt,ξt) - Vf(Xt-ι,ξt)I2 - IVF(Xt) - VF(Xt-I)II2]
≤E [IVf(χt,ξt) -Vf(χt-ι,ξt)I2] ∙	(20)
By the definition of εt, We have εt = dt - VF(Xt) = Vf (χt, ξt) + (1 - at)(dt-ι - Vf(Xt-1, ξt))-
VF(χt). Therefore,
E " "I	=E ------IlVf(Xt,ξt) + (1 - at)(dt-ι - Vf(Xt-ι,ξt)) - VF(Xt)∣∣2
ηt-1	ηt-1
=E ------Ilat(Vf (χt, ξt) - VF(Xt)) + (1 - at)(dt-1 - VF(Xt-I))
ηt-1
+ (1 - at)(Vf (χt, ξt) - Vf (χt-1 ,ξt) - VF(Xt) + VF(χt-1))I2
≤E 2c2η3-1 IVf(Xt,ξt) - VF(Xt)∣∣2 +-------(1 - at)2 ∣∣εt-1∣∣2
ηt-1
+	(1 - at)2 IlVf(Xt, ξt) - Vf(Xt-1, ξt) - VF (Xt) + VF(Xt-I)『，
ηt-1
27
Under review as a conference paper at ICLR 2022
where in the last step we used Lemma 9 and the simple fact that kx + yk2 ≤ 2 kxk2 + 2 ky k2 . Further
applying (20) and the assumption that E [∣Nf (xt, ξt) - VF(χt)k2∣ ≤ σ2 leads to
E "—# =E "2c2η3-iσ2 + (I- at)2 kεt-ik2 + 2(1- a) kVf (xt,ξt) - Vf(Xt-ι,ξt)k2
ηt-1	t-1	ηt-1	ηt-1
≤E "2c2η3-iσ2 + (1-a^ kεt-ik2 + 2(1- 电)"2	-x—k)
t-1	ηt-1	ηt-1
≤e 2c2η3-ισ2 + (1 - T l∣εt-ιk2 + 4(1 -7 L (Ilxt - x+∣∣2 + ∣∣χ+ - xt-ι∣∣2)
≤E "3-iσ2 + (4L2η2-1 + 1)(1 - αt)2 kεt-ik2 + 4(1- M)LL ∣∣χ+ -Xt-1∣∣2一 .
t-1	ηt-1	ηt-1	t
The first inequality is due to the L-smoothness of the function f. The second inequality again uses
the fact that kx + yk) ≤ 2 kxk) + 2 kyk). The last step holds because of the non-expansiveness of
the projection operator, that is,
∣xt - xt+∣ = kΠX (xt-1 - ηt-1dt-1) - ΠX (xt-1 - ηt-1VF(xt-1))k ≤ ηt)-1 kεt-1k) .
This completes the proof of the lemma.	口
Now, we are ready to present the convergence guarantee of Algorithm 4.
Theorem 6. (Adapted from Theorem 2 in Cutkosky & Orabona (2019)). Suppose the condi-
2
tions in Assumption 1 are satisfied. For any b > 0, let k = bL3, C = 32L2 + σ2 / 7Lk3 =
L2(32 + 1/(7b3)) ,w = max ((4Lk)3, 2σ2,(受)3) = σ2 max ((4b)3, 2,(32b + 志)3 /64),
and M = 16(F(xι) - F?) + WL：『 + kL22 ln(T + 2). Then, thefollowing convergence guarantee
holds for Algorithm 4
T
T X /(X++1- Xt)
ηt
E
t=1
∣∣2] Mw1/3	Mσ2/3
∣∣ ] ≤ Tk + τ2/3k .
Proof. The proof is similar to that of Theorem 2 in Cutkosky & Orabona (2019), and we reproduce it
here for convenience of the reader. First, define the Lyapunov function Φt = F (xt) + 3)工21% 】∣∣εt∣∣2.
From Lemma 10, we can derive that
E
kεt+1kL	kεtkL
—
nt	nt-1
≤E
≤E
2c2η3σ2 + (4LLnL + 1)(1- αt+1)2 kεt∣2 + 4(1-电+1)"2
ηt
ηt
∣∣xt++1 - xt ∣∣L -
Ihf
nt-ι
2c2n3σ2 + ((4LLnL + 1)(1 - at+1)2
nt
^^^^^^^^^z
Bt
kεt∣2 + 4(1- αt+1)2 LL
nt
J |------------------
Ct
∣xt++1
- xt ∣∣L
--	J ∖
^z
At |
The first two terms At and Bt are exactly the same as in the proof of Theorem 2 in Cutkosky &
Orabona (2019), and we refer to their results as follows:
T
T
T
At ≤ 2k3cL ln(T + 2), and	Bt ≤ -28LL	nt kεtkL.
28
Under review as a conference paper at ICLR 2022
From W ≥ (4Lk)3, We know that η ≤ 4L. Further, since at+1 = cη2, We have that at+1 ≤
4LW1/3 ≤ 1 for all t, and hence Ct ≤ 4L2 ∣∣Xt+1 - Xtll . Putting it all together, We obtain
1 X k kεt+ιk2
32L2 ⅛A
k⅛) ≤ ⅛L2 In(T+2) + X (8ηt∣x++1
From Lemma 8, We knoW that
-Xtll2 -%εtk2).
(21)
E [φt+1 - φt]	≤ E	-1⅛	∣lx++1 -	xt∣l2 +	7ηt	kεtk2 +	32⅛t	kεt+1k2 -	32L1ηt-i	kεtk2
Summing over t from 1 to T and then applying (21), We obtain
T
E[φT+1 - φ1] ≤XE [-而 lx++1 -xt12 + 毋 kεtk2 + 32L¼ kεt+1k2 - 32Lη-1 恒『
k3c2	T 1
≤E 16L2In(T + 2) - X i6ηt llxt+ι - xtll	.
Rearranging the terms leads to
T
X示llx++ι-
t=1 ηt
E
k3c2
≤E 16(Φι - Φt +1) + -Lr ln(T + 2)
≤16(F(XI)- F?) + ɪ现昌 k2] + 鲁 ln(T + 2)
2L2η0	L2
w1∕3σ2	k3C2
≤16(F(X1) - F?) +	+ K ln(T + 2),
2L2k	L2
where the last step holds due to the definition that ηo = wk/ɜ. Since ηt is decreasing in t,
T
T
E
T
X示llx++ι-
t=1 ηt
∑ηt η1(x++ι- Xt)
t1
llll2
≥ ητE E *(2++1-j)
t1
llll2.
E
Dividing both sides by Tητ and recalling the definition M = 16(F(x1) - F?) + Ww21；2； + kLc2 ln(T+
2), We obtain
E
1 G 1 +	J2I M _ M(w + σ2T)3 Mw1/3 Mσ2/3
T 与 ηt(Xt+1 - Xt)H J ≤ TnT =	Tk ≤ Tk + T2∕3k ,
Where in the last step We used the fact that (a + b)1/3 ≤ a1/3 + b1/3 .
□
G Missing Results in Section 5
G.1 Proof of Lemma 1
In the folloWing, We give a proof for the first inequality. The second inequality can be proved via a
similar argument. The desired result clearly holds for any state s that is in its first stage, due to our
7+,νk∕
definition of Vh , h (s) for this special case. In the folloWing, We only need to focus on the case Where
Vh(s) has been updated at least once at the given state S before the k-th episode.
Our proof relies on induction on k ∈ [K]. First, the claim holds for k = 1 due to the aforementioned
logic. For each step h ∈ [H] and s ∈ S, We consider the folloWing tWo cases.
Case 1: Vh(s) has just been updated in (the end of) episode k - 1. In this case,
Vh(s) = n X (rh(s, a%, bh) + Vh+1(s%+1)) + bn∙	(22)
n i=1
29
Under review as a conference paper at ICLR 2022
? ?	CT7*,Dk/ ∖	1.. ∙ 1	.	,.
By the definition of Vh , h (s) and the induction hypothesis,
n n
Vh?,琮(S); X
n i=1
1 n
≤1X
F y
maxD …% 卜 ”+PhVTf+)(S)
maxDII×/ (rh + PhVh+l) (S),
N N Vh ∖	)
(23)
Further, we define the regret
1 元	y	1 n	-
R (S)= n X max D”×νhi (rh + PhV h+j (S)- n X D“h»hi (rh + PhV h+j (S). (24)
i=1	i=1
Since We only update the optimistic value function Vh at the end of a stage, its value remain
unchanged Within each stage. Observe that in (24), the iterator li takes values of episode indices that
一Ii
belong to the same stage. Therefore, we know that Vh+ι is a constant value for all i ∈ {1,...,n}.
VFl
We can hence drop the index i and simply rewrite it as Vh+1 when there is no ambiguity. With such
formulation, one may quickly realize that bounding the regret Rn- (S) reduces to analyzing a matrix
team problem at each state S, as we have defined in Section 3. Specifically, the matrix team lasts for
n rounds. For a strategy pair (μ, V) of the players, the expected reward function for this matrix team
一l
is Dμ×ν(Yh + PhVh+ι)(S).
Remark 2. It is worth mentioning that an additional step is required ifone uses V-Iearning with the
celebrated learning rate at = HH+t (Jin et al., 2018) instead to update Vh, which induces an update
rule as follows:
Vh (Sh) J (1 - αt) Vh (Sh) + αt (rh (Sh,ah,bh) + Vh+1 (Sh+1)+ 8t) ,	(25)
where t is the number of times that Sh has been visited, and βt is some bonus term. In this way,
Vh is updated every time the state Sh is visited, and hence its value varies at each step. Drawing
connections back to our approach, this will lead to a matrix team problem where the expected reward
function is non-stationary over time. Our algorithm and analyses in Section 3 do not directly apply to
such a non-stationary problem. However, in our experience, we were able to show that it is possible
to carefully tune the order ofthe learning rate a in a way such that the level of non-stationarity in
Vh can be “endured” (guarantees sublinear regret in (24)) by a specifically-designed non-stationary
variant of Algorithm 1. Therefore, in this step, the learning rate at = HH+ requires more delicate
treatments but does not completely fail.
Since in Algorithm 2, both players essentially run an individual copy of Algorithm 1 for each state,
our analysis exactly reduces to the investigation of Algorithm 1 in the matrix team problem. One
may also realize that taking the average over n in (24) is equivalent to uniformly sample a time index
T from {1,...,n} and then take the expectation over the randomness of such a sampling process.
This is exactly how we constructed the approximate Nash equilibrium in Theorem 1. By applying the
ι ι
results (and proofs) of Theorem 1, we deduce that (μh, νhτ) is an 16H2A33^∕~pn4-a-approximate
NE in expectation after running Algorithm 1 for n rounds. The extra H factor comes from the
additional nuance that in the problem formulation of Theorem 1, the reward is assumed to be bounded
in [0, 1], while in the current matrix team problem, the rewards lie in [0, H]. The result above implies
l
that for any μT ∈ ∆(A) that is a best response of Vh, it holds that
——[、，、_	-
E [Dμτ×中(Th + PhVh+ι)(S) - DaT ×* (Th + PhVh+ι)(S)J ≤ 16H弋263^^",
where the expectation is taken over the randomness of τ , the rewards, and the algorithm itself. The
above inequality is equivalent to
E[Rn] ≤ 16H j2Amaχ∕n1∕4,	(26)
where the expectation is only with respect to the randomness of the rewards and the algorithm.
30
Under review as a conference paper at ICLR 2022
Remark 3. Again, we would like to discuss the alternative of using V-learning with the popular
learning rate at = HH+t (Jin etal., 2018). With such a learning rate, the update rule (25) of Vh can
be equivalently expressed as
ti
Vh(s) = α0H + X ai Th 卜,成域)+ V喜卜hR + βi
i=1
where t is the number of times that sh has been visited, ki is the index of the episode such that sh is
visited the i-th time, and βi is some bonus term. The weights αit are given by
tt
αt0 =	(1 - αj ) , and αt = αi Y (1 - αj ) , ∀1 ≤ i ≤ t.
j=1	j=i+1
If we redefine
rewritten as
Vμh,?(S) and Vh'"h (S)
to comply with this update rule, the regret in (24) would be
Rt(S) = X αit maxDμ×νki 卜 h + PhV h+ι) (s) - X atDμki ×νk (rh + PhV h+j (s).
i=1	μ	h ×	i	i=1	h h ∖	)
,	「	,	,	Ek	1	一	,	7,
For now, let us forget about the non-stationarity of Vh+1 , and pretend that it can be considered as a
constant term throughout the entire horizon by applying the method sketched in Remark 2. Even in
this simplified scenario, for each fixed t, we still need to address a weighted matrix team problem
where the reward at time i is assigned the weight αit. Further, as t varies, the weight αit assigned to
the same step i also changes over time. Unfortunately, such dynamically weighted rewards cannot
be easily handled by Algorithm 1, as well as other stochastic non-convex optimization algorithms
that we are aware of. During the execution of Algorithm 1, we are not able to modify the learning
step size of a previous step online to cope with the changing weights. These weights also cannot be
pre-computed, because it relies on knowing the total number of times that a certain state Sh is visited
during the entire horizon, which is impossible before seeing the output of the algorithm. Therefore,
such an SGD framework is incompatible with the dynamic weights αit that change over t, and this
technical difficulty prevents us from directly applying the popular learning rate at = HH+. Instead,
we proposed to utilize stage-based V-learning, because it essentially assigns uniform weights to each
time step in history. These weights only depend on the number of visitations to the state Sh in a
certain stage, which can bepre-computedfrom the recursive formula ei+ι = |_(1 + -H)e".
Further, let Fi be the σ-algebra generated by all the random variables before episode li . Then, we
几 几、	Vi ,1、、片.
Can See that {rh(s, ah, Uh) + Vh+ι(shi+ι)}n=ι is a martingale With respect to {Fi}n=ι. For any
probability p ∈ (0, 1], let ι = log(2SKH/p). From the Azuma-Hoeffding inequality, it holds with
probability at least 1 - p/(2SHK) that
1	Tn^	/	一 L	∖ ι ι 1	Tn^	(	『	『.	一 L	—	\	!——
n ED⅛×ν% Yh + PhV h+ι)(s) - n∑ "s，。Ih ,bh) + V h+ι(shi+" ≤ 2H 百,⑷
i=1	h h	i=1
Where ι suppresses logarithmic terms. In fact, since We only need Lemma 1 to hold in expectation
instead of With high probability, it suffices to replace (27) With an even simpler statement:
E
1 nL
-Vd L r.
nJ μh×νhi
i=1
卜 h + Ph V h+ι)(s)- n X 卜 h(s,ah *)+V h+ι(shi+ι)4 =0,
(28)
31
Under review as a conference paper at ICLR 2022
which holds simply due to the martingale property. Finally, combining the results in (23), (24), (26),
(28), we obtain that
E [K,琮(s)i ≤E
≤E
=E
1 n	/	y ∖
n XmaxDμ×νhi Rh+PhVhi+ι) (s) ,
i=1
1 n	/	-	,---------
, ∑Du… rh + PhVh+ι	+16H,2Amaχ∕n1"
n A一: Nh ×νh \	V
i=1
1 ^n^	匚
后 X (rh(s, ahi M) + V ∕i+ι(shi+ι)) +16H ^/j^4
i=1
1 n /
n X kh(s，a
i=1
r r .	一 i_-
Ih M) + V hi+1(sh+1
)	+ bn
≤E





where the second to last step is by the definition of bn = 25H vzAmaχ∕n1/4∙ In the last SteP We used
VFk
the formulation of Vh(s) in (22).
Case 2: Vh(s) was not updated in (the end of) episode k - 1. Since we have excluded the case
that Vh has never been updated, we are guaranteed that there exists an episode j such that Vh(s)
has been updated in the end of episode j - 1 most recently. In this case, E[Vh(s)] = E[Vh 1(s)]=
… =E[Vh(s)] ≥ E[V?,Vh (s)], where the last step is by the induction hypothesis. Finally, observe
that by our definition, the value of V?,Vh (S) is a constant for all episode indices i that belong to the
same stage. Since we know that episode j and episode k lie in the same stage, we can conclude that
E[V?,琮(s)]= E[V?,Vh(s)] ≤ E[Vh(s)].
Combining the two cases completes our proof.
G.2 Proof of Theorem 3
In the following, we present the proof for the first inequality. The second bound can be proved via a
similar argument.
Before proceeding further, we first recall the definitions of several notations from Section 5 and
define a few new ones. For a state skh, let nkh be the total number of episodes that this state has been
visited (at the h-th step) prior to the current stage, and let lkh,i denote the index of the episode that this
state was visited the i-th time among the total nh times. Similarly, recall that nh denotes the number
of visits to the state Sh in the stage right before the current stage, and th 分 denotes the i-th episode
among the nh episodes. For simplicity, we use li and li to denote lh 分 and th %, and n to denote nh,
whenever h and k are clear from the context.
From Lemma 1, we first obtain that
KK
XE [vJv1 (si) - VF,ν1 (sι)i ≤ XE [Vk(sι) - VF,ν1 (sι)i .
h=1	h=1
We hence only need to upper bound the RHS. For ease of exposition, we define the following notation:
δh =ef Vh(sh) - Vμh,νh (sh),
The main idea of the proof is to upper bound PhK=1 δhh by the next step PhK=1 δhh+1, and then obtain
a recursive formula. From the update rule of Vh(sh) in (2), we know that
Vh(sh) ≤ I[nh = 0]H + l X 卜h(s, ahi, bi) + Vh+1(sh+1)) + bn,
32
Under review as a conference paper at ICLR 2022
where the I[nkh = 0] term counts for the event that the optimistic value function has never been
updated for the given state.
Let Fi be the σ-algebra generated by all the random variables before the li -th episode. It can be seen
τ. τ.
二一二.	μiλi ,:..、*
that {rh(s, alh, bi) + Vh++ +1 (shi+1)}n=1 is a martingale With respect to the filtration {Fi}n=1.
For any probability p ∈ (0, 1], let ι = log(2SKH/p). From the Azuma-Hoeffding inequality, it
holds With probability at least 1 - p/(2SHK) that
Vμh,νh (S) = n XX	k i+Ph⅛,νh÷ι) (S)
i=1 h h
1 ɪʌ /	7 7	Γii Ji	r 、	,—— -----
≥n X S(s, ahi, bi) + 嘴+1' h+1 (sii+ι)) - 2pH2l用,
(29)
Where ι suppresses logarithmic terms. Substituting the tWo inequalities above into the definition of
δhk , We have
δh ≤I[nh = 0]H + n X (rh(s,ai ,bi ) + Vh+l (Sh+1)) + bn
n i=1
-n XX %y 卜 i+PhVh⅛∕] (S)
i=1
n	Ji Ji
≤I[nh = 0]H +「X Vi+ι(si+ι)- Vih+1, h+1(si+ι) + bn + 2pH可n
n i=1
1 n .
=I[nh = 0]H +n E δi+ι + bn + 2 √H2l∕n,
n i=1
(30)
一 • 一 一	一一 一一 . . 一 J _ 一	一 一一	一 •
Where in the last step We used the definition of δhli+1. In fact, since We only need the results in
Theorem 3 to hold in expectation rather than With high probability, We can simply replace (29) With
「 _k _k 1 In 「	„	Iiii	-∣
IPh∕μh,Vhmi - 1	r"G #i liji∖ I [∕μh+1,"h + 1 (Ji ʌ
E Vih	(S)] = n LE ri(s,ah,bi) + Vh+ι	(Sh+ι) ,
i=1
by invoking the martingale property. Consequently, We can also derive an expectation form of (30) as
folloWs:
Γ	1 n , 一
E [δk] ≤ E I[nh = 0]H + n E δh+ι + bn .	(31)
n i=1
To find an upper bound of PkK=1 δhk, We proceed to upper bound each term in the RHS of (31)
separately. First, notice that PkK=1 I [nkh = 0 ≤ SH, because each fixed state-step pair (S, h)
contributes at most 1 to PkK=1 I [nkh = 0. Next, We turn to analyze the second term in the RHS of
(31). Observe that
Klnh 泳	KKl	nh
X ⅛ X δh+iι = XX ɪ δh+ιX [%=j]
k=1 h i=1	k=1 j =1 h i=1
κ	K	nh
=X δh+1 X ɪ Xl[%=j].	(32)
j=1	k=1 h i=1
For a fixed episode j, notice that PnhI l[nk i = j] ≤ 1, and that Pn=I l[nhi = j] = 1 happens if
and only if Skh = Sjh and (j, h) lies in the previous stage of (k, h) With respect to the state-step pair
(Sh h). Define Kj = {k ∈ [K] : Pn=I l[nh i = j] = 1}. We then know that all episode indices
k ∈ Kj belong to the same stage, and hence these episodes have the same value of nnkh. That is, there
33
Under review as a conference paper at ICLR 2022
exists an integer Nj > 0, such that nkh = Nj, ∀k ∈ Kj. Further, since the stages are partitioned in
a way such that each stage is at most (1 + H) times longer than the previous stage, We know that
|Kj | ≤ (1 + H)Nj. Therefore, for every j, it holds that
K ι nh	ι
X n⅛ X1 …≤ 1 + H
(33)
Combining (32) and (33) leads to the following upper bound of the second term in (31):
K -l nh rk	IK
X n X δh+iι ≤(I + H) X δh+
k=1 h i=1	k=1
k=1
1.
(34)
So far, we have obtained the following upper bound:
K
Xδhk
k=1
E
1
≤ SH2 + (1 + HE ∑δk+1
k=1
K
+ ∑Sbnh.
k=1
Iterating the above inequality over h = H, H - 1, . . . , 1 leads to
K	HK
E Xδk ≤ O SH3 + XX(1 + HYTbnh
k=1	h=1 k=1
(35)
where we used the fact that (1 + H)h ≤ e. In the following, we analyze the bonus term bnk
more carefully. Recall our definitions that eι = H, ei+1 = |_(1 + H)e/ , i ≥ 1, and bn =
25HpAmax/n1/4. For any h ∈ [H],
X(1 + ∙1)hTbnk ≤ X(1 + ∙1)hT25H, —Amax 1,.
匕 H	nh	⅛ H	t(Nk(sh))1∕4
I―———一	1 , . —1 二…	一，	一
=25HPAmaxXX(1 + Hh-1e- 8 XI [sh = s,Nh(sh = ej∙]
=25HPAmax XX(1 + HohTw(S,j)e-1,
s∈S j≥1
where we define w(s,j) = PK=I I [shh = s, Nh(Sh) = ej∙] for any S ∈ S. If we further let w(s)=
Pj≥1 w(S, j), we can see that Ps∈S w(S) = K. For each fixed state S, we now seek an upper bound
of its corresponding j value, denoted as J in what follows. Since each stage is (1 + H) times longer
than its previous stage, we know that w(s,j) = PK=I I [shh = s, Nh(Sh) = ej] = [(1 + H”/ for
any 1 ≤ j ≤ J. Since Pj=ι w(s,j) = w(s), We obtain that ej ≤ (1 + H)J-1 ≤ ι+0τ WH) by
taking the sum of a geometric sequence. Therefore, by plugging in w(s,j) = [(1 + H)ej_l,
X(1 + HohTw(S,j)e-8 ≤ O(Xej ] ≤ O (w(s)8H1),
j≥1 H	j=1
where in the second step we again used the formula of the sum of a geometric sequence. Finally,
using the fact that Ps∈S w(S) = K and applying the Cauchy-Schwartz inequality, we have
X X (1 + HYfnh=O (H 2PAmx XX(1 + HH)h-1w(S,j)e-1∣
h=1 h=1	s∈S j≥1
7—17—1	, 3	、
≤O(K 7 H17 S 8 Am ax .	(36)
34
Under review as a conference paper at ICLR 2022
Summarizing the results above leads to
K
E X δk ≤ O 卜H3 + K7 H 17 S 8 Amax).
k=1
When K is large enough such that K ≥ SH3/7, the second term becomes dominant, and we obtain
the desired result:
KK
XE [I1 (si) - Vμ1 ,ν1 (si)] ≤ XEg ≤ O (K7H17S 1 Amax).
k=1	k=1
This completes the proof of the theorem.
G.3 Proof of Theorem 4
The result essentially follows from the “regret to PAC guarantee” conversion given in Jin et al. (2018).
Specifically, we know from Theorem 3 that
K
XE [V*,ν1 (si) - VfI,ν1 (si)] ≤ CiK7H等S 1 Amax	(37)
k=i
for some Ci that is independent of K . It is important to notice that
E [V?,Vk(si) — Vμ1k,νl1 (si)] ≥0,	(38)
?	? r?,Pk
for each individual k ∈ [K], because Vi , 1 (si) is defined as the dynamic best response instead of the
best fixed response in hindsight. Since the two agents uniformly draw the index κ from {1, 2, . . . , K},
we can conclude from (37) and (38) that for any realization of κ,
E [V*,"κ(si) - vμκ,ν1 (si)] ≤ 5CiH 18S8 Amax/K8,
17	1	3	1
with probability at least 4. To ensure 5CiHX S豆 Amax/K8 ≤ ε, it suffices to require that K ≥
58C8Hi7SAmaX/ε8. Similarly, if K ≥ 58C8Hi7SAmaX/ε8 for some constant C2, then
E [Via1'?(si) — Via1,ν1 (si)] ≤ 5C2H17S8 Amax/K 1.
Therefore, if the agents run Algorithm 2 for at least K = Ω(Hi7SAmaX/ε8) episodes, the randomly
drawn policy pair (μf*K) will be an ε-approximate NE in expectation with probability at least 3 by
taking a union bound. To improve the success probability to an arbitrary value 1-P for P ∈ (0, ∣), one
can adopt the standard “boosting” technique in the literature of randomized algorithms (Mitzenmacher
& UPfaL 2017), by repeating the process for O (log ɪ) iterations, and then choosing the iteration
with the smallest resulting regret. Finally, we remark that we have made no effort to optimize the
sample complexity dependence on H, S, and Amax , which can certainly be improved.
G.4 V-LEARNING SGD with Variance Reduction
Theorem 7. (Informal). For any ε > 0, replace the SGD-IX subroutine in Algorithm 2 with a
variance-reduced SGD method (Algorithm 4), and Set the bonus term to be bn
CHA6/点(ln n)1/2
ιmax ×	'___
n1∕3ε1/3
for some absolute constant C instead. Suppose that the two agents run such an algorithm for K
episodes with K = C(1/e4) ∙ poly (H, S, AmaX), and uniformly sample an episode index K from the
set {1,..., K}. Then, with probability at least 5, the policy pair (μK, v/ɪ) is an ε-approximate Nash
equilibrium in expectation.
Proof sketch. The proof follows a similar procedure as in the proofs of Lemma 1 and Theorems 3
and 4. Due to their similarities, in the following, we only sketch some of the differences in the proofs.
C	C	C C C	CHAθ<.11(lnn)1/2	.「 ,1 . .1	「 「丁	T
By setting the new bonus term to be bn =--------方方；1/3 J-, one can verify that the proof of Lemma 1
35
Under review as a conference paper at ICLR 2022
still holds. That is, the optimistic value function Vh(s) (with a modified bonus term) is still a valid
upper bound of both V?,vh (S) and Vμh,?(S) in expectation. Everything UP to (35) in the proof of
Theorem 3 also holds. Starting from (35), we further deduce that
HK
XX(I + H)h-1bnh ≤ O (K2 /ε3) ∙ poly(H,S,Amax),
h=1 k=1
by following a similar argument as when we derived (36). Plugging the above inequality back to the
proof of Theorem 3, we obtain that
1K
K XE M,ν1 (Si) — vμ1 ,ν1 (Si)] ≤ Ο(K-3ε-1) ∙ poly(H,S,Amaχ), and
k=1
1K	k	kk	1	1
K XE M1 ,?(si) - VUI ,νι (si)] ≤ Ο(K-ιε-3) ∙ poly(H, S, Amax).
k=i
Finally, repeating the proof idea of Theorem 4 leads to the desired result.	□
G.5 A Sample Complexity Lower B ound
To obtain a sample complexity lower bound of the problem, we consider a simple instance where
either A or B is a singleton, i.e., A = 1 or B = 1. Learning an approximate NE in such a Markov
team reduces to finding a near-optimal policy in a single-agent RL problem. Applying the regret
lower bound of single-agent RL yields the following result for RL in Markov teams.
Corollary 1. (Corollary of Jaksch et al. (2010)). For any algorithm, there exists a two-player Markov
team problem that takes the algorithm at least Ω(H3SAmaχ∕ε2) episodes to learn an ε-approximate
Nash equilibrium.
We remark that such a lower bound might be very loose. Reducing to a single-agent RL problem
evades the strategic learning behavior of the agents and the non-stationarity that such behavior causes
to the environment, which in our opinion are the central difficulties of learning in Markov teams.
To derive a tighter lower bound in our decentralized setting, one should also utilize the additional
constraint that each agent only has access to its local information, a factor that Corollary 1 apparently
does not take into account. It is hence unsurprising that when comparing Theorem 4 with Corollary 1,
we can see an obvious gap in the parameter dependence.
G.6 Decentralized MARL in Smooth Teams
In the following, we address an important subclass of Markov teams, named smooth teams, using
the V-learning SGD algorithm. More importantly, we show that our algorithm can achieve near
team-optimality, i.e., find the best Nash equilibrium, in such problems.
Smooth games were first introduced in Roughgarden (2009) to study the Price of Anarchy (POA) in
normal-form games. A large class of games are covered as examples of smooth games, including
congestion games and many forms of auctions (Roughgarden, 2009; Syrgkanis & Tardos, 2013). The
notion of smoothness was later extended to learning in normal-form games (Syrgkanis et al., 2015;
Foster et al., 2016) and cooperative Markov games (Radanovic et al., 2019; Mao et al., 2020). This
concept essentially ensures that the game has a bounded POA, and hence decentralized no-regret
learning dynamics can possibly converge to near-optimality.
Define V? to be the team-optimal value function, i.e., Vh?(S) = maxπ∈Π Vhπ(S) for any h ∈ [H], S ∈
S , where Π is the set of joint policies of the agents. We consider the following definition of a smooth
Markov team:
Definition 5. (Adapted from Radanovic et al. (2019)). For λ ≥ 0 and 0 < ρ < 1, an N -player
Markov team is (λ, ρ)-smooth if there exists a policy profile π? such that for any policy profile
π = (πi, π-i):
Vπ ,π (S) ≥ λ ∙ V?(S)- P ∙ Vh(S)Ni ∈N,s ∈ S,h ∈ [H].
36
Under review as a conference paper at ICLR 2022
The (λ, ρ)-smoothness ensures that agent i continues doing well by playing its optimal policy even
when the other agents are using slightly sub-optimal policies. It immediately follows that the resultant
policies of Algorithm 2 converge to a λ∕(1+ P) factor of the team-optimal NE at a rate of O(K-1/8).
Theorem 5. Let K = Ω(1∕ε8) ∙ poly(H, S, Amax). In a (λ, ρ)-smooth team, the value of running
the auxiliary policies {(μh, νjξ)}H=K k=1 satisfies
K	kk
IK X E M1"(S1)i ≥ EV1?(SI)- E O(K-8).
Proof. From Theorem 3, it holds that
KK
K X E [v1"1 V (s1)i ≥K X E [V1*M (S1 )i -O(K-1)
k=1	k=1
1K	kk	1
≥K X (λ ∙ V1*(s1) - P ∙ E [V1μ1 %(s1)] — - Ο(K-1),
where the second step is simply by the definition of smoothness. Rearranging the terms leads to the
desired result.	□
Equivalently, such a result also holds when the agents uniformly sample an episode index κ from
{1,..., K} and then run (μK, ν^). In Theorem 5, We illustrated our results in the case of two agents,
although one can easily verify that a similar result holds more generally for N -player smooth teams.
Our approach significantly generalizes Radanovic et al. (2019) and Mao et al. (2020) in that we
design natural update rules for both agents, who play symmetric roles in the self-play setting; the
other two works only assign the algorithm to one agent, and have to assume that the policy of the
other agent changes slowly.
H Simulations
In this section, we demonstrate the empirical performances of our algorithms. We start by evaluating
SGD-IX (Algorithm 1) on a classic matrix team problem. Then, we evaluate V-learning SGD
(Algorithm 2) on a Markov team problem, and compare its performances with various benchmarks.
H. 1 Matrix Teams
First, to evaluate Algorithm 1, we use a classic matrix team example from the literature (Claus &
Boutilier, 1998; Lauer & Riedmiller, 2000). Its reward table is reproduced in Table 1, where agent 1
is the row player, and agent 2 is the column player, both being maximizers. The action spaces of the
agents are A = {a0, a1, a2} and B = {b0, b1, b2}. There are three deterministic Nash equilibria in
this team, among which two of them, (a0, b0) and (a2, b2), are team-optimal. It would be preferred
that the agents not only learn a NE, but also settle on the same NE out of the two team-optimal ones.
	b0	b1	b2						
a0	10	0	-10	S0	bo	b1	S1	bo	b1
a1	0	2	0	a0	F	5	a0	丁	0
a2	-10	0	10	a1	2	-2	a1	0	0
Table 1: Reward table for the matrix team. Table 2: Reward tables for the Markov team.
We run our SGD-IX algorithm on this task for T = 5000 rounds, and we set the step size αt = 0.002
and the implicit exploration parameter γ = 0.001. We evaluate our algorithm in terms of both the
rewards it obtained and its L2 equilibrium gap. Specifically, we define the L2 equilibrium gap as the
L2 distance to a equilibrium point. For a pair of strategies (μ, V) ∈ ∆(A) X ∆(B), its L2 equilibrium
gap is defined as:
GaP(μ,ν) =f ∣∣μ — μt(ν)∣∣2 + IlV-V t (M)II2 ,	(39)
37
Under review as a conference paper at ICLR 2022
where V * (μ) (resp. μ1 (V)) is the best response with respect to μ (resp. V), and ∣∣∙k2 is the L2 norm.
The simulation results are presented in Figure 2. All results are averaged over 20 runs. Notice that we
evaluate two sets of strategy trajectories: The “Current” strategy (μt, Vt) is the strategy pair used by
Algorithm 1 at round t, while the “Average” strategy (μt, Vt) is the uniformly sampled strategy pair
we obtain after running Algorithm 1 for t rounds. Drawing an analogy to the Markov team setting,
“Current” denotes the actual policies {(μh, Vk)}H=K k= used at the step (k, h) of Algorithm 2, while
“Average” represents the auxiliary policies {(μh, Vh)}H=Kk=i that we have constructed. In matrix
teams, the auxiliary policy at round t simply reduces to uniformly drawing a random time index τ
from {1,...,t} and running the strategy pair (μτ,Vr).
(a) Equilibrium gap
(b) Reward
Figure 2: (a) L2 equilibrium gaps and (b) rewards of Algorithm 1 on the matrix team task given in
Table 1. “Current” denotes the actual strategy trajectory, while “Average” represents the uniformly
sampled strategy pair. Shaded areas denote the standard deviations of the equilibrium gap or reward.
Note that in Theorems 1 and 4 we only have theoretical guarantees for the auxiliary policies (“Aver-
age”) but not the actual policy trajectories (“Current”). Encouragingly, our simulation results show
that the actual policy trajectories also enjoy convergence behavior to NE empirically, and in most
cases the “Current” policies converge even faster. Specifically, from Figure 2(a), we can see that the
equilibrium gap of both “Current” and “Average” converge to zero, indicating that they indeed find an
equilibrium as the number of iterations increase. The convergence of “Average” slightly lags behind
“Current” because in Algorithm 1, “Average” simply takes the mean value over the actual trajectories,
which requires some time to reflect the convergence behavior. A more promising result is that from
Figure 2(b), we can see that the rewards collected by “Current” and “Average” converge to values
close to 9. This suggests that Algorithm 1 not only finds a NE in this task, but actually converges to a
team-optimal equilibrium most of the time. It does not exactly reach the team-optimal value of 10
because it still converges to non-team-optimal NE at a rather low frequency.
H.2 Markov Teams
We further evaluate Algorithm 2 on a Markov team task. Inspired by Yongacoglu et al. (2019), we
construct a Markov team problem with two states S = {s0, s1}, where s0 is the “good state” and s1
is the “bad state”. Each agent has two candidate actions A = {a0, a1} or B = {b0, b1}. The reward
function at each state is presented in Table 2. Specifically, at state s1, both agents get a reward of
0 no matter what actions they select, while at state s0 , they will obtain a strictly positive reward if
they either take the joint action (a0, b1) or the one (a1, b0). The state transition function is defined as
follows:
Ph(s0 | s0 or s1, a0, b1) = 1 - ε, Ph(s1 | s0 or s1, not (a0, b1)) = 1 - ε, ∀h ∈ [H],
and all the other transitions happen with probability ε. Intuitively, no matter which state the agents
are in, they will transition to the good state s0 with a high probability 1 - ε at the next step as long as
they select the action pair (a0, b1). All the other joint actions will lead to the bad state s1 with a high
probability 1 - ε. The task hence rewards the agents who learn to consistently play the action pair
(a0, b1).
38
Under review as a conference paper at ICLR 2022
We run our V-learning SGD algorithm on this example for K = 5000 episodes, each episode
containing H = 10 steps. We set the transition probability ε = 0.1, the step size αt = 0.0004,
and the implicit exploration parameter γ = 0.002. We again evaluate our algorithm in terms of the
reward and the equilibrium gap. The simulation results are presented in Figure 3, where all results are
averaged over 20 runs. We would like to remind the reader that “Current” denotes the performances
of the actual policy trajectory {(μh, ν%)}H==Kk=「while “Average” represents the auxiliary policies
{(bhM)}HK,k*	'
(a) Equilibrium gap
(b) Reward
Figure 3: (a) L2 equilibrium gaps and (b) rewards of Algorithm 2 on the Markov team task given
in Table 2. “Current” denotes the actual policy trajectory {(μh,νh)}H=Kk=「while “Average”
represents the auxiliary policies {(μh, Vh)}HKKk=ι. Shaded areas denote the standard deviations of
the equilibrium gap or reward.
We compare our algorithm with two meaningful benchmarks. The first benchmark is a “Centralized”
oracle. This oracle acts as a centralized coordinator that can control the actions of both agents. Such
an oracle essentially converts the Markov team task into a single-agent RL problem. The (randomized)
action space of the centralized agent is ∆(A × B), which is larger than the ∆(A) × ∆(B) space
that we allow in our decentralized approach. Therefore, “Centralized” clearly upper bounds the
performance that any decentralized learning algorithm can possibly achieve in this task. In our
simulations, we implement “Centralized” by using a Hoeffding-based variant of a state-of-the-art
single-agent RL algorithm UCB-ADVANTAGE (Zhang et al., 2020a). This algorithm has achieved
the tightest sample complexity bound for single-agent RL in theory, and has also demonstrated
remarkable empirical performances in practice (Mao et al., 2020). Such an algorithm could provide
a strong performance upper bound in our task. The second benchmark we consider is the naive
“Independent” Q-learning. Specifically, we let each agent run a single-agent Q-learning algorithm
independently, without being aware of the existence of the other agent or the structure of the Markov
team. Each agent maintains an local optimistic Q-function, and takes greedy actions with respect
to such optimistic estimates, without taking into account the other agents’ actions. Since the agents
update their policies simultaneously, the stationarity assumption of the environment in single-agent
RL quickly collapses, and the theoretical guarantees for single-agent Q-learning no longer hold.
This is also reminiscent of the “independent learner” approach proposed in an early work (Claus
& Boutilier, 1998) for learning in Markov teams. We believe such a benchmark could provide
meaningful intuitions about the consequences of not taking care of the team structure in decentralized
methods. In our simulations, we implement such a benchmark by letting each agent running a variant
of the single-agent UCB-ADVANTAGE (Zhang et al., 2020a) algorithm independently, where the
(randomized) action spaces of the agents are ∆(A) and ∆(B).
We can observe from Figure 3(a) that the L2 equilibrium gaps of both “Current” and “Average”
converge to zero, indicating that both of them find a Nash equilibrium. This is encouraging because
our theoretical results in Theorem 4 only guarantee the performance of the auxiliary policies, while
our simulation results suggest that the actual policy trajectories also converge to a NE. The actual
policy sequence also converges faster than the auxiliary policies, for reasons similar to the matrix team
simulations. From Figure 3(b), we can again see that our algorithm even finds to the team-optimal NE
most of the time, as the rewards it obtained nearly match the “Centralized” oracle. On the other hand,
39
Under review as a conference paper at ICLR 2022
the “Independent” benchmark converges, albeit faster, to a clearly suboptimal value. This reiterates
that the naive idea of independent learning does not work well for multi-agent RL in general, and
a careful treatment of the game/team structure (like our SGD subroutine) is necessary. Finally, the
implemented V-learning SGD algorithm takes much fewer samples to find an approximate NE
than our theoretical results suggested. This indicates that the theoretical bounds might be overly
conservative, and our algorithm could converge much faster in practice.
40