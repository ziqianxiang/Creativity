Under review as a conference paper at ICLR 2022
Fed-χ2 : Secure Federated Correlation Test
Anonymous authors
Paper under double-blind review
Ab stract
In this paper, we propose the first secure federated χ2 -test protocol, Fed-χ2 . We
recast χ2-test as a second frequency moment estimation problem and use stable
projection to encode the local information in a short vector. As such encodings
can be aggregated with summation, secure aggregation can be applied to conceal
the individual updates. We formally establish the security guarantee of Fed-χ2
by demonstrating that the joint distribution is hidden in a subspace containing
exponentially possible distributions. Our evaluation results show that Fed-χ2
achieves good accuracy with small client-side computation overhead. Fed-χ2
performs comparably to the centralized χ2-test in several real-world case studies.
The code for evaluation is in the supplementary material.
1 Introduction
Correlation test, as the name implies, is the process of examining the correlation between two random
variables using observational data. It is a fundamental building block in a wide variety of real-world
applications, including feature selection (Zheng et al., 2004), cryptanalysis (Nyberg, 2001), causal
graph discovery (Spirtes et al., 2000), empirical finance (Ledoit & Wolf, 2008; Kim & Ji, 2015),
medical studies (Kassirer, 1983) and genomics (Wilson et al., 1999; Dudoit et al., 2003). Because
the observational data used in correlation tests may contain sensitive information such as genomic
information, centralizing the data collection is risky. To address, we resort to a federated setting in
which each client maintains its own data and communicates with a centralized server to calculate a
function. Note that the communication should contain as little information as feasible. Otherwise,
the server may be able to infer sensitive information from the communication transcript.
In the present work, we study a representative correlation test, namely χ2-test, under the federated
setting. There are two straightforward methods for conducting χ2-test in such a context. First, clients
can upload their raw data to the centralized server and delegate the test to it. While this method is
effective in terms of communication, it entirely exposes the clients’ private information. Second,
clients may run secure multiparty computation (MPC) under the server’s coordination. Thus, clients
can jointly run χ2-test without disclosing their data to the server. However, general-purpose MPC
imposes significant computation and communication overhead, which is typically intolerable in a
federated setting with computationally limited clients, e.g., mobile devices.
To address the dilemma, we present a federated protocol optimized for χ2-test that is computationally
and communicationally efficient and discloses limited information to the server. We begin by recasting
χ2-test as a second frequency moment estimation problem. To approximate the second frequency
moment in a federated setting, each client encodes its raw data into a low-dimensional vector via
stable random projection (Indyk, 2006; Vempala, 2005; Li, 2008). Such encodings can be aggregated
with only summation, allowing clients to leverage secure aggregation (Bonawitz et al., 2017; Bell
et al., 2020) to aggregate the encodings and the server to decode them to approximate the second
frequency moment. Because secure aggregation conceals each client’s individual update within the
aggregated global update, the server learns only limited information about the clients’ data.
Our evaluation on four synthetic datasets and 16 real-world datasets shows that Fed-χ2 can replace
centralized χ2 -test with good accuracy and low computation overhead. Additionally, we analyze
Fed-χ2 in three real-world use cases: feature selection, cryptanalysis, and online false discovery
rate control. The results show that Fed-χ2 can achieve comparable performance with centralized
χ2-test and can withstand up to 20% of clients dropping out with minor influence on the accuracy. In
summary, we make the following contributions:
1
Under review as a conference paper at ICLR 2022
•	We propose FED-X2, the first secure federated χ2-test protocol. FED-X2 is computation- and
communication-efficient and leaks much less information than trivially deploying secure aggregation.
•	FED-X2 decomposes χ2-test into frequency moments estimation that can easily be encoded/decoded
using stable projection and secure aggregation techniques. We give formal security proof and utility
analysis of Fed-X2.
•	We evaluate FED-X2 in real-world use cases, and the findings suggest that FED-X2 can substitute
centralized X2-test with comparable accuracy, and Fed-X2 can tolerate up to 20% of clients dropout
with minor accuracy drop.
2 Related Work
Bonawitz et al. (2017) proposed the well-celebrated secure aggregation protocol as a low-cost way to
calculate linear functions in a federated setting. It has seen many variants and improvements since
then. For instance, Truex et al. (2019) and Xu et al. (2019) employed advanced crypto tools for
secure aggregation, such as threshold homomorphic encryption and functional encryption. So et al.
(2021) proposed TurboAGG, which combines secure sharing with erasure codes for better dropout
tolerance. To improve communication efficiency, Bell et al. (2020) and Choi et al. (2020) replaced
the complete graph in secure aggregation with either a sparse random graph or a low-degree graph.
Secure aggregation is deployed in a variety of applications. Agarwal et al. (2018) added binomial
noise to local gradients, resulting in both differential privacy and communication efficiency. Wang
et al. (2020) replaced the binomial noise with discrete Gaussian noise, which is shown to exhibit better
composability. Kairouz et al. (2021) proved that the sum of discrete Gaussian is close to discrete
Gaussian, thus discarding the common random seed assumption from Wang et al. (2020). The above
three works all incorporate secure aggregation in their protocols to lower the noise scale required for
differential privacy. Chen et al. (2020) added an extra public parameter to each client to force them to
train in the same way, allowing for the detection of malicious clients during aggregation. Nevertheless,
designing secure federated correlation tests, despite its importance in real-world scenarios, is not
explored by existing research in this field.
On the other end of the spectrum, Wang et al. (2021) proved that stable projection is differentially
private if the projection matrix is secret. In our protocol, the projection matrix is public information;
hence Fed-X2 does not consider the differential privacy guarantee. 3
3	Federated Correlation Test with Minimal Leakage
In this section, we elaborate on the design of Fed-X2, a secure federated protocol for X2-test. Sec. 3.1
first formalizes the problem, establishes the notation system, and introduces the threat model. In
Sec. 3.2, we recast X2-test as a second frequency moment estimation problem in the federated setting,
and consequently, we are able to leverage stable projection to encode each client’s local information
(Sec. 3.3), and then aggregate them using secure aggregation (Sec. 3.4). Sec. 3.5, 3.6, and 3.7 present
security proof, utility analysis, communication analysis, and computation analysis of Fed-X2 .
3.1 Problem Setup
We now formulate the problem of the federated correlation test and establish the notation system.
We use [n] to denote {1, ∙∙∙ , n}. We denote vectors with bold lower-case letters (e.g., a, b, C) and
matrices with bold upper-case letters (e.g., A, B, C).
We consider a population of n clients C = {ci}i∈[n] . Each client has one share of local data
composed of the triplets Di = {(x, y,Vxy)},χ ∈ X,y ∈ Y,vXy ∈ {-M,…，M}, where X and
(i)
y are categories of the contingency table, vxy is the observed counting of the categories x and y
in the local contingency table of the ith client, |X| = mx and |Y| = my are finite domains, and
M is the maximum value |vx(iy) | can be. The global dataset is defined as D = {(x, y, vxy) : vxy =
Pi∈[n] vx(iy) }. We focus on federated X2-test and the data in contingency table is discrete. For the
ease of presentation, we define the marginal statistics vx = Py∈[∣γ∣] vxy Jvy = Px∈[|X |] vxy, and
V = Px∈[∣χ∣] y∈[∣γ∣] Vxy. Besides, we define Vxy = Vxvvy, denoting the expectation of Vxy if X and
2
Under review as a conference paper at ICLR 2022
y are uncorrelated. We define m = mxmy and use an indexing function I : [mx] × [my] → [m] to
obtain a uniform indexing given the indexing of each variable. A centralized server S calculates the
statistics for χ2-test Sχ2 (D) = Pχ∈[∣χ∣] y∈[∣γ∣] (Vxy-Vxy) on the global dataset to decide whether
X and Y are correlated without collecting the raw data from clients.
Overall, using MPC to conduct secure correlation tests in a federated scenario is highly expensive
and impractical (Boyle et al., 2015; Damgard et al., 2012). Hence, in the present work, we trade off
accuracy for efficiency, as long as the estimation error is small with a high probability. Formally, if
FED-X2 outputs ^χ2, whose corresponding standard centralized χ2-test output is Sχ2, the following
accuracy requirement should be satisfied with small and δ.
P[(1 — e)sχ2 ≤ Sχ2 ≤ (1 + e)sχ2] ≥ 1 — δ
Threat Model. We assume that the centralized server S is honest but curious. It honestly follows the
protocol due to regulatory or reputational pressure but is curious to discover extra private information
from clients’ legitimate updates for profit or surveillance purposes. As a result, client updates should
contain as little sensitive information as feasible. We want to emphasize that, while the server
may explore the privacy of clients, the server will honestly follow the protocol due to regulation or
reputational pressure. The server won’t provide adversarial vectors to the clients.
On the other hand, we assume honest clients. Specifically, we do not consider client-side adversarial
attacks (e.g., data poisoning attacks (Bagdasaryan et al., 2020; Bhagoji et al., 2019)). However, we
allow a small portion of clients to drop out during the execution. Also, as we mentioned in Sec. 2, we
do not consider the differential privacy guarantee; see further clarification in Appendix A.
3.2	From Correlation Test to Frequency Moments Estimation
We first recast correlation test to a second frequency moments estimation problem as defined below.
Given a set of key-value pairs S = {ki, vi}i∈[n], we re-organize it into a histogram H = {kj, vj =
Pk =k ,i∈[n] vi}, and estimate the αth frequency moments as Fα = Pj vjα. χ2-test can thus be
recast to a 2nd frequency moments estimation problem as follows:
sχ2(D)=X(vxy
x,y
Vxy )2
Vxy
^XIvxy - Vxy)2
七' √vxy '
—
In federated setting, each client ci holds a local dataset Di = {(x, y, vx(iy))} and computes a vector
ui, where ui [I(x, y)]
V(i) -Vxy /n	1	,	F	E,	,	, 11	. CF	F 9
xy√r-- and Ui has m elements. Thus, the challenge in federated χ2-test
becomes calculating the following equation:
sχ2 (D)=X (vx√e )2=ιι>ι2
3.3	Encoding by Stable Projection & Decoding by Geometric Mean Estimator
To address the aforementioned challenges and to easily integrate the algorithm into secure aggregation
protocols, we use stable random projection (Indyk, 2006; Vempala, 2005) and geometric mean
estimator (Li, 2008) to approximate the data’s second frequency moment efficiently. We begin by
discussing stable distributions, followed by the encoding and decoding techniques.
Definition 1 (α-stable distribution). A random variable X follows an α-stable distribution Qα,β,F if
its characteristic function is as follows.
∏	∏	,__	∏α
φx (t) = exp(—F∣t∣p(1 — √-1β sgn(t)tan(ɪ)))
, where F is the scale to the αth power and β is the skewness.
α-stable distribution is named due to its property called α-stability. Briefly, the sum of independent
α-stable variables still follows an α-stable distribution with a different scale.
Definition 2 (α-stability). Ifrandom variables X 〜 Qα,β,1, Y 〜Qαβ ι and X and Y are indepen-
dent, then CiX + C2Y 〜Qα,β,cα+cα∙
We borrow the genius idea from Indyk’s well-celebrated paper (Indyk, 2006) to encode the frequency
moments in the scale parameter of a stable distribution defined in Definition 1. To encode the local
dataset Di = {(x, y, vx(iy))}, each client ci, i ∈ [n], is given by the server an ` × m projection matrix P
whose values are drawn independently from an α-stable distribution Qα,β,1, where ` is the encoding
3
Under review as a conference paper at ICLR 2022
Algorithm 1: The encoding and decoding scheme for federated frequency moments estimation.
Note that the encoding and decoding themselves do not provide any security guarantee.
1
2
3
4
5
6
7
Function EN C O D E(P, ui ):
I return P × Ui
Function GE OME TRI CME ANE S T IMATOR (e):
J、 J	Qk=I ∣ek∣2∕'
d ⑵,gm — (2 Γ( ' )Γ(1-' )sin( π ))'
return d(2),gm
Function DE C O D E(e):
I return GEOMETRICMEANESTIMATOR (e)
// ` is the encoding size.
x(iy).
size and m = mxmy. The client re-organizes the data into a vector ui, where ui [I(x, y)] = v
Then, the client projects ui to ei = P × ui as the encoding (line 2 in Alg. 1).
To decode, the server first sums the encodings up e = i∈[n] ei and estimates the scale of the
variables in the aggregated encoding with an unbiased geometric mean estimator (Li, 2008) in line 4
of Alg. 1. According to the α-stability defined in Definition 2, every element ek in e, k ∈ [`], follows
this stable distribution: ek 〜Q2,0,∣∣ P 曰〕以佗.Thus, the l2 norm can be estimated by calculating
the scale of the distribution Q2,0,|| P ui||2 with e containing ` elements.
During the above processes, ui and Pi∈[n] ui are not leaked and the aggregation among clients is
calculated with only summation. Thus, secure aggregation protocols can be naturally applied with
quantization, as will be discussed in Sec. 3.4 and 3.5. Furthermore, ' = ^⅜ log(l∕δ) suffices to
guarantee that the second frequency moment can be approximated with a 1 ± factor and a probability
no less than 1 - δ. We will further analyze the utility of Fed-χ2 in Sec. 3.6.
3.4	Secure Federated Correlation Test
The complete protocol for FED-χ2 is presented in Alg. 2. Firstly, the marginal statistics vx , vy and v
are calculated with secure aggregation and broadcasted to all clients (lines 1-6 in Alg. 2). This step
can be omitted if the marginal statistics are already known. Then, on the server side, a projection
matrix P is sampled from an α-stable distribution Q,×m. The projection matrix is broadcasted to all
clients (lines 8-10 in Alg. 2). For each client ci , the local data is re-organized into ui and projected to
ei as encoding (lines 11-14 in Alg. 2). Then, the encoding results will be quantized and aggregated
with secure aggregation (line 15 in Alg. 2). As we have already known the marginal statistics in the
first round, the quantization bound can be set accordingly. Additionally, we can use high precision
for quantization, such as 64 bits, since the size of the contingency table is normally moderate rather
than enormous. Thus, the precision of the quantized float number is comparable to or even better
than that of float64, and hence we disregard the effect of quantization on accuracy. Finally, the server
gets the χ2-test result using the decoding algorithm described in Alg. 1 (line 17 in Alg. 2).
Dropouts in the first round have no effect on the test’s accuracy because they can be recovered inside
secure aggregation (Bonawitz et al., 2017; Bell et al., 2020). On the other hand, dropouts in the
second round will affect the accuracy of the test. Still, because the χ2 value is typically far from
the decision threshold, Fed-χ2 is intrinsically robust to a small portion of clients dropping out (see
Section 4 for empirical assessment).
3.5	Security Analysis
As discussed in Sec. 2, the secure aggregation protocol is well studied, and many well-celebrated
secure aggregation protocols have been proposed (Bonawitz et al., 2017; Truex et al., 2019; Xu et al.,
2019; So et al., 2021; Bell et al., 2020; Choi et al., 2020). In this paper, we choose the state-of-the-art
secure aggregation protocol by Bell et al. (Bell et al., 2020), which replaces the complete graph with
a sparse random graph to enhance communication efficiency. We clarify that Fed-χ2 can incorporate
other popular secure aggregation protocols. We now prove the security enforced by Alg. 2 via a
standard simulation proof process (Lindell, 2017) on the basis of Theorem 1.
Theorem 1 (Security). Let Π be an instantiation of Alg. 2 with the secure aggregation protocol in
Alg. 4 of Appendix H with cryprographic security parameter λ. There exists a PPT simulator Sim
such that for all clients C, the number of clients n, all the marginal distributions {vx}, {vy}, and all
4
Under review as a conference paper at ICLR 2022
the encodings {ei}, the output of SIM is indistinguishable from the view of the real server ΠC in that
execution, i.e., ΠC ≈λ SIM(	ei , n).
Intuitively, Theorem 1 illustrates that no more information about the clients except the averaged
updates is revealed to the centralized server. Thus, each client’s update is hidden by the rest clients in
secure aggregation. We now present the formal proof for Theorem 1.
Proof for Theorem 1. To prove Theorem 1, we need the following lemma.
Lemma 1 (Security of secure aggregation protocol). Let S ecureAgg be the secure aggregation
protocol in Alg. 4 of Appendix H instantiated with cryprographic security parameter λ. There exists
a probabilistic polynomial-time (PPT) simulator SimSA such that for all clients C, the number of
clients n, and all inputs X = {ei}i∈[n], the output of SIMSA is perfectly indistinguishable from the
view of the real server, i.e., SECUREAGGC ≈λ SIMSA(Pi∈[n] ei, n).
Lemma 1 is derived from the security analysis of our employed secure aggregation protocol (Theorem
3.6 in Bell et al. (2020)), which establishes that the secure aggregation protocol securely conceals the
individual information in the aggregated result. With this lemma, we are able to prove the theorem
for federated χ2-test by presenting a sequence of hybrids that begin with real protocol execution
and end with simulated protocol execution. We demonstrate that every two consecutive hybrids are
indistinguishable, illustrating that the hybrids are indistinguishable according to transitivity.
Hyb1 This is the view of the server in the real protocol execution, RealC .
HYB2 In this hybrid, we replace the view during the execution of each SECUREAGG({vx(i) }i∈[n] )
in line 3 of Alg. 2 with the output of SIMSA(vx, n) one by one. According to Lemma 1,
each replacement does not change the indistinguishability. Hence, Hyb2 is indistinguish-
able from Hyb1.
Hyb3 Similar to Hyb2 , we replace the view during the execution of each
SECUREAGG({vy(i)}i∈[n] ) in line 4 of Alg. 2 with the output of SIMSA(vy, n)
one by one. According to Lemma 1, Hyb3 is indistinguishable from Hyb2 .
HYB4 In this hybrid, we replace the view during the execution of SECUREAGG({ei}i∈[n] ) in
line 15 of Alg. 2 with the SIMSA(P ei, n). This hybrid is the output of SIM. According
to Lemma 1, HYB4 is indistinguishable from HYB3.	□
Remark: what does Alg. 2 leak? By Theorem 1, we show that individual updates of clients are
perfectly hidden in the aggregated results and Fed-χ2 leaks no more than a linear equation system:
( P × V = eτ
l Jl,my × VT = VT
[j1,mx X V = VT
Algorithm 2: Fed-χ2 : secure federated χ2-test. SecureAgg is a remote procedure that re-
ceives inputs from the clients and returns the summation to the server. InitSecureAgg is the
corresponding setup protocol deciding the communication graph and other hyper-parameters.
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
Round 1: Reveal the marginal statistics
IN I TSE CUREAGG (n)
for x ∈ [mx] do vx = SECUREAGG({vx(i)}i∈[n] )
for y ∈ [my] do vy = SECUREAGG({vy(i)}i∈[n] )
SerVer
I Calculate V = Px VX and broadcast v, {vχ } and {vy } to
Round 2: Approximate the statistics
SerVer
Sample the projection matrix P from Q2×m
Broadcast the projection matrices to the clients
Client ci, i ∈ [n]
Calculate VXy = Vxvy
Prepare Ui s.t. Ui[I(x,y)] = Vxy -vxyy/n
ʌ/vXy
Calculate ei = ENCOD E(P, ui)
e = SECUREAGG(QUANTI ZE({ei}i∈[n] ))
SerVer
I Sχ2 = Decode (e)
// n is the client number.
all the clients.
5
Under review as a conference paper at ICLR 2022
, where J1,mx and J1,my are 1 × mx and 1 × my unit matrices, V is an mx × my matrix whose
elements are {vxy }, and v is a vector flattened by V. To understand this, information leaked by FED-
χ2 includes the estimation e and marginal statistics vx and vy . The following theorem establishes an
important fact: the above equation system has an exponentially large solution space, which effectively
conceals the real joint distribution. We thus believe that Alg. 2 practically ensures privacy due to the
solution space’s vastness.
Theorem 2. Given a projection matrix P ∈ Zj×m, {vχ}, {vy} and e, there are at least qm-'-mx-my
feasible choices of {vxy}.
Proof sketch for Theorem 2. As demonstrated above, the given information forms a linear equation
with mx + my + ` equations. Given m > mx + my + `, the rank of the coefficient matrix is no
more than mx + my + `. Solving the equations with Smith normal form, we know that the solution
space is at least (m - mx - my - `)-dimensional. With the following lemma, we manage to prove
Theorem 2.
Lemma 2. There are qr×c vectors in the subspace of Zr×c.	□
3.6	Utility Analysis
In this section, we conduct the utility analysis in terms of multiplicative error. We show that the
output of FED-X2, ^χ2, is a fairly accurate approximation (parameterized by E) to the correlation test
output sχ2 in the standard centralized setting with high probability parameterized by δ.
Theorem 3 (Utility). Let Π be an instantiation of Alg. 2 with secure aggregation protocol in Alg. 4
of Appendix H. Π is parameterized with ' = 言 log(1∕δ) for some constant c. After executing ∏c
on all clients C, the server yields Sχ2, whose distance to the accurate correlation test output sχ2 is
bounded with high probability as follows:
P[Sχ2 < (1 — β)Sχ2 ∨ Sχ2 > (1 + β)Sχ2 ] ≤ δ
Proof sketch for Theorem 3. First, we introduce the following lemma from Li (2008).
Lemma 3 (Tail bounds of geometric mean estimator (Li, 2008)). The right tail bound of geometric
mean estimator is:	2
P(SX2 - sχ2 > ∖χ) ≤ exp(-'GR)
where 忌 = Ci log(1 + E) — CιYe(α — 1) 一 log(∏2Γ(αCι)Γ(1 — Ci) sin(πc2C1)), α = 2 in our
setting, Ci = 2 tan-1 ((2+C%}), and Ye = 0.577215665…is the Euler's constant.
The left tail bound of the geometric mean estimator is:	2
P(^χ2 - Sχ2 < -eSχ2) ≤ exp(-' G—)
where ' > '0, GL = —C2 log(1 — E) — log( 一 ∏2Γ(-αC2)Γ(l + C2)sin(πcC2))—
'0C2 log(∏2r( 'c0)r(I - '0)sin( π2 ^α )), and C2 = ∏12 (2+α2).
With Lemma 3, Taking C ≥ max(GR, GL) and δ = exp( 一号),We are able to prove P[^χ2 <
(1 — E)sχ2 ∨ ^χ2 > (1 + E)sχ2] ≤ δ, and the above bound holds when ' =言 log(1∕δ).	□
3.7	Communication & Computation Analysis
In this section, we present the communication and computation cost of Alg. 2.
Theorem 4	(Communication Cost). Let Π be an instantiation of Alg. 2 with secure aggregation
protocol in Alg. 4 of Appendix H, then (1) the client-side communication cost is O(log n + mx +
my + `); (2) the server-side communication cost is O(n log n + nmx + nmy + n`).
Theorem 5	(Computation Cost). LetΠ be an instantiation of Alg. 2 with secure aggregation protocol
in Alg. 4 ofAppendix H, then (1)the client-side computation cost is O (log2 n +(' + mχ + my) log n +
m`); (2) the server-side computation cost is O(n log2 n + n(` + mx + my) logn + `).
Note that compared with the original computation cost presented in (Bell et al., 2020), the client-side
overhead has an extra O(m') term. This term is incurred by the encoding overhead. We also give an
empirical evaluation on the client-side computation overhead in Sec. 4.1. Please refer to Appendix I
for the detailed proof of Theorem 4 and Theorem 5.
6
Under review as a conference paper at ICLR 2022
4	Evaluation
Experiment Setup. To assess Fed-χ2 ’s accuracy, we simulate it on four synthetic datasets and
16 real-world datasets. We compare the multiplicative errors of Fed-χ2 with that of the standard
centralized χ2-test. The four synthetic datasets are independent, linearly correlated, quadratically
correlated, and logistically correlated. As the real-world datasets, we report the details in Appendix J.
Additionally, We evaluate FED-X2'S utility in three real-world application scenarios: 1) feature
selection, 2) cryptanalysis, and 3) online false discovery rate (FDR) control. For feature selection, we
report the model accuracy trained on the selected features. For cryptanalysis, we report the success
rate of cracking ciphertexts. For Online FDR control, we report the average false discovery rate.
We compare the performance of Fed-χ2 with that of the centralized χ2-test in each of the three
experiments. Unless otherwise specified, experiments are launched on an Ubuntu 18.04 LTS server
equipped with 32 AMD Opteron(TM) Processor 6212 and 512GB RAM.
4.1	Evaluation Results
Error of no dropout —Error of 5% dropout
ACC of no dropout
50	100	150	200
(c) Synthetic Data 3.
50	100	150	200
(a) Synthetic Data 1.
ACC of 5% dropout
50	100	150	200
(e) Data 1.
50	100	150	200
(b) Synthetic Data 2.
50	100	150	200
Oof 3
50	100	150	200
(d) Synthetic Data 4.
50	100	150	200
(i) Data 5.
OOV/ W JOXIM3A'a"0ad三n≡
(g) Data 3.
尸;：;Yl;二二
50	100	150	200
(l) Data 8.
50	100	150	200
(m) Data 9.
OOV/ W JOXIM3A'a"0adr3n=
OOV/ W JOXIM3A'a"0adr3n=
(p) Data 12.
OOV∕ W JOXIM3A-a∙"0ad≡n≡
OOV/ 3 JOXIM3A'a"0adR==
OOV/ 3 JOXIM3A'a"0adR==
8V、3
84、3
Encoding size 2	Encoding size 2	Encoding size 2	Encoding size 2
(q) Data 13.	(r) Data 14.	(s) Data 15.	(t) Data 16.
Figure 1: Multiplicative error and accuracy of Fed-χ2 w.r.t. encoding size ` w/ and w/o dropout.
Accuracy. We begin by evaluating the accuracy of Fed-χ2, as illustrated in Fig. 1. Each point
represents the mean of 100 independent runs with 100 clients, while the error bars indicate the
standard deviation. We choose mx = my = 20 in this experiment. Note that the accuracy drop is
independent of the number of clients.
7
Under review as a conference paper at ICLR 2022
From Fig. 1, we observe that the larger the encoding size `, the smaller the multiplicative error. When
` = 50, the multiplicative error ≈ 0.2. This conforms with Theorem 3, in which the multiplicative
error e = cjo log(1∕δ) decreases as ' increases.
We also evaluate the power (Cohen, 2013) of Fed-χ2. We set the p-value threshold as 0.05, which
determines whether or not to reject the null hypothesis. From The dashed lines in Fig. 1, we can tell
that the power of Fed-χ2 is high. This conforms with our observation on the multiplicative errors.
Specifically, since the χ2 values are typically far from the decision threshold, a multiplicative error of
0.2 rarely flips the final decision.
We also present the results when 5% of clients drop out in the second round of Fed-χ2 in Fig. 1. The
results show that Fed-χ2 is robust to a small portion of dropouts. In Appendix G, we present the
results in terms of 10%, 15%, and 20% dropout rates.
Client-side Computation Overhead. To assess extra computa-
tion overhead incurred by Fed-χ2 on the client side, we measure the
execution time of the encoding scheme on an Android 10 mobile de-
vice equipped with a Snapdragon865 CPU and 12GB RAM. We use
PyDroid (Sandeep Nandal, 2020) to run the client-side computation
of Fed-χ2 on the Android device.
The results are shown in Fig. 2. Each point represents the average of
100 separate runs, with accompanying error bars. The overhead is
generally negligible. For example, for a 500 × 500 contingency table,
the encoding takes less than 30ms. The overhead grows linearly in
relation to mx (my) and consequently quadratically in Fig. 2, where
mx equals my.
4.2 Downstream Use Case Study
Feature Selection. Our first case study explores secure federated
feature selection using Fed-χ2 . The setting is that each client holds
data with a large feature space and wants to collaborate with other
clients to rule out unimportant features and retain features with top-
k highest χ2 scores. We use Reuters-21578 (Hayes & Weinstein,
1990), a standard text categorization dataset (Yang, 1999; Yang &
Pedersen, 1997; Zhang & Yang, 2003), and pick the top-20 most
frequent categories using 17,262 training and 4,316 test documents.
These documents are distributed randomly to 100 clients, each of
whom receives the same number of training documents. After re-
moving all numbers and stop-words, we obtain 167,135 indexing
terms. After performing feature selection using Fed-χ2 , we select
the top 40,000 terms with the highest χ2 scores. When compared
with the centralized χ2-test, 38,012 (95.03%) of the selected terms
Fed-χ2 produces highly consistent results with the standard χ2-test.
mx (my)
Figure 2: Client-side encoding
overhead when mx = my .
)%( ycaruccA ledoM
80 70 60 50 40
——no feature selection
一 FED-χ2 no dropout
FED-X2 10% dropout
FED-X2 20% dropout
——centralized χ2-test
Epoch
Figure 3: Accuracy of the model
trained with features selected by
Fed-χ2 and centralized χ2-test.
are identical, indicating that
We then train logistic regression models using the terms selected by Fed-χ2 and the centralized
χ2-test, respectively. All hyper-parameters are the same. The details of these models are reported in
Appendix K. The results in Fig. 3 further demonstrate that Fed-χ2 exhibits comparable performance
with the centralized χ2-test. When 10% and 20% of clients dropout in the second round of Fed-χ2,
the accuracy of the trained model using the features selected by FED-χ2 does not drop much. We
also examine performance without feature selection, and as expected, model accuracy is significantly
greater after feature selection. Note that the model without feature selection has 2,542,700 more
parameters than the model with feature selection. Hence, feature selection effectively improves model
accuracy while reducing model size and computational cost.
Cryptanalysis. In the second case study, we explore federated cryptanalysis with Fed-χ2 . We
break Caesar cipher (Luciano & Prichett, 1987), a classic substitution cipher, with Fed-χ2 . In a
Caesar cipher, each letter in the plaintext is replaced by another letter with some fixed number of
positions down the alphabet. For instance, each English letter can be right-shifted by three, converting
8
Under review as a conference paper at ICLR 2022
the plaintext “good” to the ciphertext “jrrg”. There are 26 possible shifts when given 26 English
letters. The plaintext can be cracked in a shortcut by performing a correlation test on the ciphertext in
relation to normal English text.
In our setting, each client is assumed to possess a segment of the
Caesar ciphertext. To collaboratively crack the ciphertext, these
clients run 26 χ2-tests to determine the correlation level between
each ciphertext letter and the letters in normal English text. The
χ2-test yielding the highest correlation level elucidates how English
letters are encrypted into Caesar ciphertexts.
We take Shakespeare’s lines as the plaintext and encrypt it into a
Caesar ciphertext with a length of 1000 characters. We initiate the
cracking process on ten non-overlapping ciphertexts to compute
the average success rate (see Fig. 4). In general, the larger the
encoding size, the more precise the χ2-statistics is, and consequently
the higher the success rate. Again, according to Theorem 3, the
Encoding Size 0
Figure 4: The success rate of
cracking Caesar ciphers.
multiplicative error decreases as the encoding size increases. In Fig. 4, we also report the success
rate when 10% and 20% of clients dropout in Round two of Fed-χ2, respectively. Even if 20% of
clients dropout, the success rate can still be 100% as long as the encoding size ` is larger than 20.
Encoding Size Q
Figure 5: Average FDR w.r.t. `
for SAFFRON with Fed-χ2.
Online False Discovery Rate Control. In the third case study, we
explore federated online false discovery rate (FDR) control (Foster
& Stine, 2008) with Fed-χ2 . In an online FDR control problem,
a data analyst receives a stream of hypotheses on the database, or
equivalently, a stream of P-ValUes: p1,p2,….At each time t, the
data analyst should pick a threshold αt to reject the hypothesis
when pt < αt . The error metric is the false discovery rate, and the
objective of online FDR control is to ensure that for any time t, the
FDR up to time t is smaller than a pre-determined quantity.
We use the SAFFRON procedure (Ramdas et al., 2018), the state-of-
the-art online FDR control, for multiple hypothesis testing. The χ2
results and corresponding p-values are calculated by Fed-χ2 . We
present the detailed algorithm of SAFFRON and the hyper-parameters used in our evaluation in
Appendix L. The size of the randomly synthesized contingency table is 20 × 20. Each time, there are
100 independent hypotheses, with a probability of 0.5 that each hypothesis is either independent or
correlated. The time sequence length is 100, and the number of clients is 10. The data are synthesized
from a multivariate Gaussian distribution. For the correlated data, the covariance matrix is randomly
sampled from a uniform distribution. For the independent data, the covariance matrix is diagonal,
and its entries are randomly sampled from a uniform distribution.
At time t, we use FED-χ2 to calculate the p-values pt of all the hypotheses, and then use the
SAFFRON procedure to estimate the reject threshold αt using pt . The relationship between the
average FDR and encoding size ` is shown in Fig. 5. We observe that the variance of independent
runs is very small, so we omit the error bars. The results indicate that by increasing the encoding
size `, Fed-χ2 can achieve a low FDR of less than 5.0%. We also observe that dropouts improve
the performance of Fed-χ2 in this case study. The reason for this is because dropouts reduce the
estimated χ2 value, which increases the probability of accepting the null hypothesis in Fed-χ2 .
As a larger portion of queries follows the null hypothesis in online FDR control, the accuracy also
increases. The results further demonstrate that Fed-χ2 can be employed in practice to facilitate
online FDR control using a secure federated correlation test. 5
5 Conclusion
This paper takes an important step towards designing non-linear secure aggregation protocols in the
federated setting. Specifically, we propose a universal secure protocol to evaluate frequency moments
in the federated setting. We focus on an important application of the protocol: χ2-test. We give
formal security proof and utility analysis on our proposed secure federated learning χ2-test protocol
Fed-χ2 and validate them with empirical evaluations and case studies.
9
Under review as a conference paper at ICLR 2022
References
Naman Agarwal, Ananda Theertha Suresh, Felix Yu, Sanjiv Kumar, and H Brendan Mcma-
han. cpsgd: Communication-efficient and differentially-private distributed sgd. arXiv preprint
arXiv:1805.10559, 2018.
Eugene Bagdasaryan, Andreas Veit, Yiqing Hua, Deborah Estrin, and Vitaly Shmatikov. How to
backdoor federated learning. In International Conference on Artificial Intelligence and Statistics,
pp. 2938-2948. PMLR, 2020.
James Henry Bell, Kallista A Bonawitz, Adri鱼 Gasc6n, Tancrede Lepoint, and Mariana Raykova.
Secure single-server aggregation with (poly) logarithmic overhead. In Proceedings of the 2020
ACM SIGSAC Conference on Computer and Communications Security, pp. 1253-1269, 2020.
Arjun Nitin Bhagoji, Supriyo Chakraborty, Prateek Mittal, and Seraphin Calo. Analyzing federated
learning through an adversarial lens. In International Conference on Machine Learning, pp.
634-643. PMLR, 2019.
Keith Bonawitz, Vladimir Ivanov, Ben Kreuter, Antonio Marcedone, H Brendan McMahan, Sarvar
Patel, Daniel Ramage, Aaron Segal, and Karn Seth. Practical secure aggregation for privacy-
preserving machine learning. In proceedings of the 2017 ACM SIGSAC Conference on Computer
and Communications Security, pp. 1175-1191, 2017.
Elette Boyle, Kai-Min Chung, and Rafael Pass. Large-scale secure computation: Multi-party
computation for (parallel) ram programs. In Annual Cryptology Conference, pp. 742-762. Springer,
2015.
Yu Chen, Fang Luo, Tong Li, Tao Xiang, Zheli Liu, and Jin Li. A training-integrity privacy-preserving
federated learning scheme with trusted execution environment. Information Sciences, 522:69-79,
2020.
Beongjun Choi, Jy-yong Sohn, Dong-Jun Han, and Jaekyun Moon. Communication-computation
efficient secure aggregation for federated learning. arXiv preprint arXiv:2012.05433, 2020.
Jacob Cohen. Statistical power analysis for the behavioral sciences. Academic press, 2013.
Ivan Damgard, Valerio Pastro, Nigel Smart, and Sarah Zakarias. Multiparty computation from
somewhat homomorphic encryption. In Annual Cryptology Conference, pp. 643-662. Springer,
2012.
Dheeru Dua and Casey Graff. UCI machine learning repository, 2017. URL http://archive.
ics.uci.edu/ml.
Sandrine Dudoit, Juliet Popper Shaffer, Jennifer C Boldrick, et al. Multiple hypothesis testing in
microarray experiments. Statistical Science, 18(1):71-103, 2003.
Dean P Foster and Robert A Stine. α-investing: a procedure for sequential control of expected false
discoveries. Journal of the Royal Statistical Society: Series B (Statistical Methodology), 70(2):
429-444, 2008.
Govindaraj, Praveen. Credit Risk Classification Dataset: Is Customer Risky or Not Risky ? https:
//www.kaggle.com/praveengovi/credit-risk-classification-dataset.
Online; accessed 22 April 2021.
Philip J Hayes and Steven P Weinstein. Construe/tis: A system for content-based indexing of a
database of news stories. In IAAI, volume 90, pp. 49-64, 1990.
Sebastian Houben, Johannes Stallkamp, Jan Salmen, Marc Schlipsing, and Christian Igel. Detection of
traffic signs in real-world images: The German Traffic Sign Detection Benchmark. In International
Joint Conference on Neural Networks, number 1288, 2013.
Piotr Indyk. Stable distributions, pseudorandom generators, embeddings, and data stream computation.
Journal of the ACM (JACM), 53(3):307-323, 2006.
10
Under review as a conference paper at ICLR 2022
Peter Kairouz, Ziyu Liu, and Thomas Steinke. The distributed discrete gaussian mechanism for
federated learning with secure aggregation. arXiv preprint arXiv:2102.06387, 2021.
Jerome P Kassirer. Teaching clinical medicine by iterative hypothesis testing: let’s preach what we
practice, 1983.
Jae H Kim and Philip Inyeob Ji. Significance testing in empirical finance: A critical review and
assessment. Journal OfEmpirical Finance, 34:1-14, 2015.
Ron Kohavi. Scaling up the accuracy of naive-bayes classifiers: A decision-tree hybrid. In Kdd,
volume 96, pp. 202-207, 1996.
Kohavi, Ronny and Becker, Barry. UCI Machine Learning Repository: Adult Data Set. https:
//archive.ics.uci.edu/ml/datasets/adult. Online; accessed 22 April 2021.
Kononenko, Igor and Cestnik, Bojan. UCI Machine Learning Repository: Lymphography Data Set.
https://archive.ics.uci.edu/ml/datasets/Lymphography. Online; accessed
22 April 2021.
Oliver Ledoit and Michael Wolf. Robust performance hypothesis testing with the sharpe ratio. Journal
of Empirical Finance, 15(5):850-859, 2008.
Ping Li. Estimators and tail bounds for dimension reduction in lα (0 < α ≤ 2) using stable
random projections. In Proceedings of the nineteenth annual ACM-SIAM symposium on Discrete
algorithms, pp. 10-19, 2008.
Yehuda Lindell. How to Simulate It-A Tutorial on the Simulation Proof Technique, pp. 277-
346. Springer International Publishing, Cham, 2017. ISBN 978-3-319-57048-8. doi: 10.1007/
978-3-319-57048-8_6. URL https://doi.org/10.1007/978-3-319-57048-8_6.
Dennis Luciano and Gordon Prichett. Cryptology: From caesar ciphers to public-key cryptosystems.
The College Mathematics Journal, 18(1):2-17, 1987.
Kaisa Nyberg. Correlation theorems in cryptanalysis. Discrete Applied Mathematics, 111(1-2):
177-188, 2001.
Aaditya Ramdas, Tijana Zrnic, Martin Wainwright, and Michael Jordan. Saffron: an adaptive
algorithm for online control of the false discovery rate. In International conference on machine
learning, pp. 4286-4294. PMLR, 2018.
Sandeep Nandal. PyDroid. https://pypi.org/project/pydroid/, 2020. Online; ac-
cessed 24 April 2021.
Schlimmer, Jeff. UCI Machine Learning Repository: Mushroom Data Set. https://archive.
ics.uci.edu/ml/datasets/Mushroom. Online; accessed 22 April 2021.
JinhyUn So, BaSak Guler, and A Salman Avestimehr. Turbo-aggregate: Breaking the quadratic
aggregation barrier in secure federated learning. IEEE Journal on Selected Areas in Information
Theory, 2021.
Pater Spirtes, Clark Glymour, Richard Scheines, Stuart Kauffman, Valerio Aimale, and Frank
Wimberly. Constructing bayesian network models of gene expression networks from microarray
data. 2000.
Stacey Truex, Nathalie Baracaldo, Ali Anwar, Thomas Steinke, Heiko Ludwig, Rui Zhang, and
Yi Zhou. A hybrid approach to privacy-preserving federated learning. In Proceedings of the 12th
ACM Workshop on Artificial Intelligence and Security, pp. 1-11, 2019.
Santosh S Vempala. The random projection method, volume 65. American Mathematical Soc., 2005.
Lun Wang, Ruoxi Jia, and Dawn Song. D2p-fed: Differentially private federated learning with
efficient communication. arxiv. org/pdf/2006.13039, 2020.
Lun Wang, Iosif Pinelis, and Dawn Song. Differentially private fractional frequency moments
estimation with polylogarithmic space. arXiv preprint arXiv:2105.12363, 2021.
11
Under review as a conference paper at ICLR 2022
R Bryce Wilson, Dana Davis, and Aaron P Mitchell. Rapid hypothesis testing with candida albicans
through gene disruption with short homology regions. Journal ofbacteriology, 181(6):1868-1874,
1999.
Runhua Xu, Nathalie Baracaldo, Yi Zhou, Ali Anwar, and Heiko Ludwig. Hybridalpha: An efficient
approach for privacy-preserving federated learning. In Proceedings of the 12th ACM Workshop on
Artificial Intelligence and Security, pp. 13-23, 2019.
Yiming Yang. An evaluation of statistical approaches to text categorization. Information retrieval, 1
(1):69-90, 1999.
Yiming Yang and Jan O Pedersen. A comparative study on feature selection in text categorization. In
Icml, volume 97, pp. 35. Nashville, TN, USA, 1997.
Jian Zhang and Yiming Yang. Robustness of regularized linear classification methods in text
categorization. In Proceedings of the 26th annual international ACM SIGIR conference on
Research and development in informaion retrieval, pp. 190-197, 2003.
Zhaohui Zheng, Xiaoyun Wu, and Rohini Srihari. Feature selection for text categorization on
imbalanced data. ACM Sigkdd Explorations Newsletter, 6(1):80-89, 2004.
Appendix
A Clarification on Privacy
As indicated in Sec. 2, the projection matrix is public information, and hence Fed-χ2 does not take
the differential privacy guarantee into account. We would want to provide more information in order
to eliminate any potential misunderstandings.
To begin, we would want to emphasize that “privacy” in our paper refers to MPC-style privacy, not
DP-style privacy. In general, MPC-style privacy is orthogonal to DP-style privacy: in MPC, privacy
is obtained against a semi-honest server in such a way that the server cannot witness individual
client’s updates but only an aggregate of them, e.g. SecAgg (Bonawitz et al., 2017). In DP, privacy
is accomplished by including random noise in each client’s update, such that the distribution of the
output result does not reveal the clients’ private information and the server cannot infer the clients’
identification from the output result.
Second, we would like to emphasize that our work proposes a novel secure aggregation scheme
particularly for the χ2-test. Existing standard secure aggregation schemes are inapplicable to the
χ2-test, which will reveal much more information than Fed-χ2, as we have clarified in Sec. 1. Again,
this work requires guaranteeing MPC-style privacy, not DP.
Third, to quantify MPC-style privacy, we prove in Theorem 2 that the clients’ updates in Fed-χ2 are
hidden inside a space with exponential size. This is weaker than hiding users’ updates in the whole
space, but still gives meaningful privacy guarantees (consider attempting to guess the output of an
exponential-sided dice, which is practically infeasible).
Finally, while DP is orthogonal to this research, we would want to emphasize that our protocol can
achieve DP by introducing calibrated discrete Gaussian noise to the users’ local updates. B
B Comparison with Pooling χ2-TEST
We also compare the performance of Fed-χ2 with pooling χ2-test. That is, the clients compute the
χ2-test with their local observations and then they aggregate their test results by pooling. The result of
the pooling χ2-test is determined by the majority of the clients’ local results. Fig. 6 shows the result
of pooling χ2-test on the real-world datasets. The details of the datasets are presented in Appendix J.
We observe that pooling χ2-test cannot give meaningful results and it tends to give judgement that
the data is independent since that the numbers of the local observations are not sufficient to make
correct judgement. The results further demonstrate the effectiveness and the necessity of Fed-χ2 .
12
Under review as a conference paper at ICLR 2022
Error of FED-χ 2
ACC ofFED-χ2
oɔv / W JanM3A∙--a0ad'a=n
1-,-
0.8
0.6
0.4
0.2
0∣
200
3 JonM3AIa0adaniξ
ODV / W Jo,πM3∙sa0adan≡
200
oɔv / W Jo,πM3∙sa0adan≡
(b) Data 2.	(c) Data 3.
oɔv / 3 JOXIM3A-a∙"0ad==≡
1I
0.8
0.6
0.4
0.2
0L
50	100	150
(a) Data 1.
50	100	150
(e)	Data 5.
200
rorrE evitacilpitluM
oɔv / 3 JOXIM3A-a∙"0ad==≡
15
200
oɔv / W JOXIM3A-a∙"0ad==≡
50	100	150	200
(f)	Data 6.	(g) Data 7.
oɔv / 3 JOXIM3A-a∙"0ad==≡
1
0.8
0.6
0.4
0.2
0 i
50	100	150
(i)	Data 9.
200
1
0.8
0.6
0.4
0.2
0
JOXIM3AW0ad===oov'
oɔv / 3 JOXIM3A-a∙"0ad==≡
200
Oov / W JOJJM3AR"0adr3n≡
(j)	Data 10.	(k) Data 11.
0.2
0.6
oov、3
0.6
0.2
10	20	30	40	50
40
20
oov、3
OOV、W
Encoding size 2	Encoding size 2	Encoding size 2	Encoding size 2
(m) Data 13.	(n) Data 14.	(o) Data 15.	(p) Data 16.
Figure 6: Comparison between Fed-χ2 and pooling χ2-test.
C	Performance of Fed-χ2 When Original χ2-TEST Achieves Low
Power
We have computed the multiplicative error and accuracy of FED-χ2 with the original centralized
χ2-test as the baseline. To further demonstrate the effectiveness of Fed-χ2 , we also set up the
experiment to evaluate the performance of Fed-χ2 on the synthesized dataset when the accuracy of
the original centralized χ2 -test is lower. More specifically, we synthesize the random independent,
linearly correlated, quadratically correlated, and logistically correlated datasets with the same hyper-
parameters for 20 times. We then compute the accuracy of the original centralized χ2-test over these
20 datasets. We report the results in Fig. 7. The results show that FED-χ2 can still achieve good
performance on the datasets when the original centralized χ2-test's accuracy is lower. Consistent
with our results in Sec. 4.1, the multiplicative error becomes lower with the increase of the encoding
size l. Thus, We conclude that FED-X2's performance is comparable to that of the original centralized
χ2 -test with an appropriate encoding size l and this is not dependent on the datasets.
ACC of FED-χ2	---- ACC of centralized χ2-test
—Error of FED-χ2
3 JOXIMOAWOHdHInpv
Encoding size 2
(a) Synthetic Data 1.
Encoding size 2
(b) Synthetic Data 2.
3 JOXIMOAWOHdHInpv
Encoding size 2
(c) Synthetic Data 3.
3 JOXIMOAWOHdHInpv
Encoding size 2
(d) Synthetic Data 4.
Figure 7: Performance of Fed-χ2 when original χ2-test achieves low accuracy.
13
Under review as a conference paper at ICLR 2022
D Further Results for Online FDR Control
In this section, we provide further results for online FDR control.
As we have shown in Fig. 5, FED-χ2 achieves good performance
(FDR lower than 5%) when the encoding size l is larger than 200.
In Fig. 8, we provide the FDR result of the original χ2-test as well
as the true discovery rate (TDR, i.e., #correct reject / #should reject).
In addition, we provide statistics for each encoding size l that was
evaluated in Table 1. These results demonstrate that Fed-χ2 per-
forms well and is comparable to the centralized χ2-test when the
encoding size l is increased. More importantly, as we have shown
in Sec. 4.1 and Appendix C, Fed-χ2 gives results close to original
centralized χ2-test and this is independent from the data.
◎) Xell/HCH
100 r=i=F
80
60
FDR of FED-χ2
-TDRofFED-X2
40	—FDR of centralized χ2-test
TDR of centralized χ2-test
20
0
10 50 100 150 200 250 300
Encoding Size Q
Figure 8: Results of FDR & TDR.
	#ShoUld reject	#ShoUld accept	#CorreCt reject	#false reject
Fed-X2, l = 10	5,900	4,100	5,544	17132
FED-χ2, l = 25	5,900	4,100	5,871	1022
Fed-x2, l = 50	5,900	4,100	5,899	856
Fed-x2, l = 100-	5,900	4,100	5,900	606
Fed-x2, l = 150-	5,900	4,100	5,900	411
Fed-x2, l = 200-	5,900	4,100	5,900	335
Fed-x2, l = 250-	5,900	4100	5,900	270
Fed-x2, l = 300-	5,900	4100	5,900	202
centralized χ2-test	5,900 —	4,100 —	5,900 —	0	-
Table 1: Detailed results of online FDR control.
To further demonstrate that the close performance of Fed-χ2 and
the original centralized χ2-test is independent of the data, we use a
similar approach as described in Appendix C to generate the data that
the original centralized χ2-test struggles to distinguish its correlation
(i.e., resulting in a higher FDR). To achieve this, we change the co-
variance matrix of the Gaussian distribution when generating the
independent and dependent data. The detailed results are shown in
Table 2 and in Fig. 9. The original centralized χ2-test achieves an
100
60
40
20
FDR of FED-χ2
-TDRofFED-X2
—FDR of centralized χ2-test
—TDR of centralized χ2-test
0
10 50 100 150 200 250 300
Encoding Size Q
Figure 9: Results of FDR & TDR
when the FDR of the original cen-
tralized χ2-test is higher.
FDR of 7.03% and a TDR of 89.73%. The results indicate that when
the encoding size l is increased, the performance of Fed-χ2 gets
closer to the original centralized χ2-test. When l = 300, Fed-χ2
performs similarly to the original centralized χ2-test. The results in
Fig. 9 and Table 2 are consistent with those in Appendix C, indicating
that the performance of Fed-χ2 is comparable to that of the original centralized χ2-test when the
encoding size l is set appropriately, and that this is independent of the data.
		#ShoUld reject	#ShoUld accept	#correct reject	#false reject
Fed-x2, l	= 10	5,900	4,100	5,144	1392
Fed-x2, l	= 25	5,900	4,100	5328	1356
Fed-x2, l	= 50	5,900	4,100	5,325	17143
Fed-x2, l	= 100	5,900	4,100	5,398	943
Fed-x2, l	= 150	5,900	4,100	5,393	765
Fed-x2, l	= 200	5,900	4,100	5377	687
Fed-x2, l	= 250	5,900	4,100	5,328	615
Fed-x2, l	= 300	5,900	4,100	5,361	556
centralized	χ2 -teSt	5,900 —	4,100 —	5,294 —	408	—
Table 2: Detailed results of online FDR control when the FDR of the original centralized χ2-test is higher.
E Incorporate Gaussian Mechanism in Fed-χ2
As we have mentioned in Appendix A, Fed-χ2 can achieve differential privacy easily by incorporating
well-studied differentially private mechanisms. To further demonstrate this point, we utilize Gaussian
Mechanism to provide (, δ)-DP guarantee. We clipped the local clients’ data such that the encoding
14
Under review as a conference paper at ICLR 2022
function’s sensitivity, which is the L2-norm of the clients’ local data is bounded by ∆f. Before
encoding their local data, each client add Gaussian noise Nd (0, σ2) to their local data vector μi. We
can calculate σ2 = 2ln(In25/：yf) . After encoding and decoding with the computed vector, FED-X2
provides (, δ)-DP guarantee.
We evaluate the performance of differentially private Fed-X2 on the first four real-world datasets
adopted in this research as shown in Fig. 10. We choose encoding size l = 10 and δ = 0.01. The
results show that the performance our algorithm becomes better with the increase of privacy budget .
However, when the privacy budget is small, the protocol tends to give the judgement that the data
is independent because that the independent noise is too large and it dominates the test. When the
privacy budget is large enough (i.e., = 100), our protocol can achieve 0.92, 0.68, 0.70, and 0.80
accuracy on these datasets accordingly, and its performance is comparable to the original Fed-X2 .
Again, our work is orthogonal to differential privacy, thus we leave it as future work to further study
saving privacy budget, and boosting the algorithm’s performance on lower .
Privacy budget W
(a) Data 1.
3 JonM3An∙"0adnIn≡
Privacy budget W
(b) Data 2.
Privacy budget W
(c) Data 3.
Privacy budget W
(d) Data 4.
Figure 10: Performance of Fed-χ2 when incorporating Gaussian Mechanism.
F	Further Results for Feature Selection
Our results in Sec. 4.2, paragraph Feature Selection, demonstrate
that FED-χ2 performs well when encoding size l = 50. We conduct
experiments with different encoding sizes l to further assess their
effect on FED-X2 's performance. In Fig. 11, we present the effect
of encoding size l on the ratio of the commonly-selected features
between the original centralized χ2-test and Fed-χ2. A larger ratio
of commonly-selected features means that Fed-χ2 performs more
closely to the original centralized χ2-test. And if the ratio is 1, these
two algorithms select the identical features. The results in Fig. 11
show that when the encoding size l increases, the performance of
Fed-χ2 approaches that of the original centralized χ2-test.
Similar to Sec. 4.2, we evaluate Fed-χ2 ’s performance by training
the model with the features selected by Fed-χ2. Fig. 12 shows the
results. When trained with Fed-χ2 -selected features, the model
can achieve comparable accuracy to the model trained with features
selected by the original centralized χ2-test. Also, consistent with
the results in Fig. 3 in Sec. 4.2, we see that when the encoding size
l ≥ 25, models trained by Fed-χ2-selected features achieve higher
accuracy than that of the models without feature selection. These
results further demonstrate the effectiveness of Fed-χ2 .
G Further Results on Fed-χ2 with Dropouts
)%( oitaR erutaeF nommoC
Encoding Size Q
Figure 11: Ratio of commonly-
selected features between Fed-χ2
and original centralized χ2 -test.
—no feature selection
FED-χ2, l = 50
FED-χ2, l = 35
FED-χ2, l = 25
FED-χ2, l = 10
--—centralized χ2-test
Epoch
Figure 12: Accuracy of model
trained w/ Fed-χ2-select features
under different encoding size l.
We present the results of 10%, 15%, and 20% clients dropout in Fig. 13. The results further show that
Fed-χ2 can tolerate a considerable portion of clients dropout in Round 2 of Alg. 2.
15
Under review as a conference paper at ICLR 2022
Error of 10% dropout —Error of 15% dropout —Error of 20% dropout-----------------ACC of 10% dropout----------ACC of 15% dropout---------ACC of 20% dropout
50	100	150
50	100	150
(b) Synthetic Data 2.
50	100	150
200	50	100	150	200
(d) Synthetic Data 4.
200
(a) Synthetic Data 1
1
0.8
8f3
8f3
1
0.8
(c) Synthetic Data 3.
1
0.8
8f3
oɔv / 3 JOXIM3A-a∙"0ad==≡
8f3
50	100	150	200
(m) Data 9.
oɔv / 3 JOXIM3A-a∙"0ad==≡
Oof 3
50	100	150	200
(n) Data 10.
oɔv / 3 JOXIM3A-a∙"0ad==≡
8f3
50	100	150	200
(o) Data 11.
Encoding size 2
(q) Data 13.
10	20	30	40	50
Encoding size 2
(r) Data 14.
10	20	30	40	50
Encoding size 2
(s) Data 15.
10	20	30	40	50
oɔv / 3 JOXIM3A-a∙"0ad==≡
10	20	30	40	50
(p) Data 12.
Encoding size 2
(t) Data 16.
10	20	30	40	50
Figure 13: Multiplicative error and accuracy of Fed-χ2 w.r.t. encoding size ` w/ and w/o dropout.
OOV、3
H	Secure Aggregation
The secure aggregation protocol from Bell et al. (2020) is presented in Alg. 4. The first step of the
protocol is to generate a k-regular graph G, where the n vertices are the clients participating in the
protocol. The server runs a randomized graph generation algorithm InitSecureAgg presented
in Alg. 3 that takes the number of clients n and samples output (G, t, k) from a distribution D. In
Alg. 3, we uniformly rename the nodes of a graph known as a Harary graph defined in Definition 3
with n nodes and k degrees. The graph G is constructed by sampling k neighbours uniformly and
without replacement from the set of remaining n - 1 clients. We choose k = O(log(n)), which is
large enough to hide the updates inside the masks. t is the threshold of the Shamir’s Secret Sharing.
In the second step, the edges of the graph determine pairs of clients, each of which runs key agreement
protocols to share random keys. The random keys will be used by each party to derive a mask for her
input and enable dropouts.
In the third step, each client ci , i ∈ A1 sends secret share to its neighbors. In the fourth step, the
server checks whether the clients dropout exceeds the threshold δ, and lets the clients know their
neighbors who didn’t dropout.
16
Under review as a conference paper at ICLR 2022
In the fifth step, each pair (i, j) of connected clients in G runs a λ-secure key agreement protocol
si,j = KA.Agree(ski1, pkj1) which uses the key exchange in the previous step to derive a shared
random key si,j. The pairwise masks mi,j = F (si,j) can be computed, where F is the pseudorandom
generator (PRG). If the semi-honest server announces dropouts and later some masked inputs of
the claimed dropouts arrive, the server can recover the inputs. To prevent this happening, another
level of masks, called self masks, ri is added to the input. Thus, the input of client ci is: yi =
ei +ri - j∈NG(i),j<i mi,j + j∈NG(i),j>i mi,j.
Steps 6-8 deal with the clients dropout by recovering the self masks r of clients Who are still
active and pairwise masks mi,j of the clients who have dropped out. Finally, the server can
cancel out the pairwise masks and subtract the self masks in the final sum: Pi∈A0 (yi - ri +
∑j∈NG(i)∩(A1∖A2),0<j<i mi,j - Ej∈NG(i)∩(A1∖A2),i<j≤n mi,j).
Definition 3 (Harary(n, k) Graph). Let Harary(n, k) denotes a graph with n nodes and degree
k. This graph has vertices V = [n] and an edge between two distinct vertices i andj if and only if
j - i (mod n) ≤ (k + 1)/2 orj - i (mod n) ≥ n - k/2.
Algorithm 3: InitSecureAgg: Generate Initial Graph for SecureAgg.
1	Function I N I TS E C U R EAG G(n):
2	. n: Number of nodes.
3	. t: Threshold of Shamir’s Secret Sharing.
4	k = O(log(n)).
5	Let H = HARARY (n, k).
6	Sample a random permutation π : [n] → [n].
7	Let G be the set of edges {(∏(i), ∏(j ))∣(i,j) ∈ H}.
8	return (G, t, k) * 2
I Proof for Communication & Computation Cost
We provide the proof for Theorem 4 and Theorem 5 in the following.
Theorem 4 (Communication Cost). Let Π be an instantiation of Alg. 2 with secure aggregation
protocol from Bell et al. (2020), then (1) the client-side communication cost is O(log n+mχ+my +');
(2) the server-side communication cost O(nlogn + nmx + nmy + n`).
Proof sketch for Theorem 4. Each client performs k key agreements (O(k) messages, line 9 in Alg. 4)
and sends 3 masked inputs (O(mx + my + `) complexity, lines 3, 4, 15 in Alg. 2 and line 10 in
Alg. 4). Thus, the client communication cost is O(log n + mx + my + `).
The server receives or sends O(log n + mx + my + `) messages to each client, so the server
communication cost is O(n log n + nmχ + nmy + n').	□
Theorem 5 (Computation Cost). Let Π be an instantiation of Alg. 2 with secure aggregation protocol
from Bell et al. (2020), then (1) the client-side computation cost is O(mχ log n + my log n + ' log n +
m`); (2) the server-side computation cost is O(mx + my + `).
Proof sketch for Theorem 5. Each client computation can be broken up as k key agreements (O(k)
complexity, line 9 in Alg. 4), generating masks mi,j for all neighbors cj (O(k(mx + my + `))
complexity, lines 3, 4, 15 in Alg. 2 and line 10 in Alg. 4), and encoding computation cost O(m')
(line 14 in Alg. 2). Thus, the client computation cost is O(mx logn + my logn + `logn + m`).
The server-side follows directly from the semi-honest computation analysis in Bell et al. (2020). The
extra O(') term is the complexity of the geometric mean estimator.	□
17
Under review as a conference paper at ICLR 2022
1
2
3
4
5
6
7
8
9
10
11
12
13
14
Algorithm 4: SecureAgg: Secure Aggregation Protocol. (Algorithm 2 from Bell et al. (2020))
Function SE CUREAGG ({ei}i∈[n] ):
.Parties: Clients ci, ∙∙∙ ,cn, and Server.
. l: Vector length.
. Xl : Input domain, ei ∈ Xl .
.F : {0, 1}λ →Xl: PRG.
. We denote by A1, A2, A3 the sets of clients that reach certain points without dropping out.
Specifically A1 consists of the clients who finish step (3), A2 those who finish step (5), and
A3 those who finish step (7). For each Ai, A0i is the set of clients for which the server sees
they have completed that step on time.
(1)	The server runs (G, t, k) = IN I TSE CUREAGG (n), where G is a regular degree-k
undirected graph with n nodes. By NG(i) we denote the set of k nodes adjacent to ci (its
neighbors).
(2)	Client ci, i ∈ [n], generates key pairs (ski1, pki1), (ski2, pki2) and sends (pki1, pki2) to the
server who forwards the message to NG (i).
(3)	for each Client ci , i ∈ A1 do
•	Generates a random PRG seed bi .
•	Computes two sets of shares:
Hi = {hb ι,…，hb k} = ShamirSS(t, k, bi)
Hi = {hS ι, ∙∙∙ ,hS k} = ShamirSS(t, k, sk1)
•	Sends to the server a message m = (j, ci,j), where ci,j = Eauth.Enc(ki,j, (i||j ||hib,j ||his,j)) and
ki,j = KA.Agree(ski2, pkj2), for each j ∈ NG(i).
(4)	The server aborts if |A01 | < (1 - δ)n and otherwise forwards (j, ci,j) to client cj who
deduces A01 ∩ NG (j).
(5)	for each Client ci , i ∈ A2 do
•	Computes a shared random PRG seed si,j as si,j = KA.Agree(ski1, pkj1).
•	Computes masks mi,j = F (si,j) and ri = F(bi).
•	Sends to the server their masked input
yi = ei + ri -	mi,j +	mi,j
j∈[n],j<i	j∈[n],j>i
(6)	The server collects masked inputs. It aborts if |A02| < (1 - δ)n and otherwise sends
(A02 ∪ NG(i), (A1\A02) ∪ NG(i)) to every client ci, i ∈ A02.
(7)	Client cj, j ∈ A3 receives (R1, R2) from the server and sends
{(i, hib,j)}i∈R1 ∪ {(i, his,j)}i∈R2 obtained by decrypting the ci,j received in Step (3).
(8)	The server aborts if |A03| < (1 - δ)n and otherwise:
•	Collects, for each client ci, i ∈ A02, the set Bi of all shares in Hib sent by clients in A3. Then
aborts if |Bi | < t and otherwise recovers bi and ri using the t shares received which came from
the lowest client IDs.
•	Collects, for each client ci, i ∈ (A1\A02), the set Si of all shares in His sent by clients in A3.
Then aborts if |Si | < t and otherwise recovers ski1 and mi,j .
•	return Σi∈A2 (yi - ri + j∈NjGNG{i^n∖A2')j^< ),0<j<i mi,j - Σj∈NG(i)∩(A1∖A2),i<j≤n mi,j ).
J Details of Datasets
The details for the real-world datasets used in Sec. 4.1 are provided in Table 3. The license of
Credit Risk Classification (Govindaraj, Praveen) is CC BY-SA 4.0, the license of German Traffic
18
Under review as a conference paper at ICLR 2022
Sign (Houben et al., 2013) is CC0: Public Domain. Other datasets without a license are from UCI
Machine Learning Repository (Dua & Graff, 2017).
Table 3: Dataset details.
ID	Data		Attr#1		A#1 Cat		Attr #2		A#2 Cat
~Γ~	Adult Income (Kohavi, 1996; Kohavi, Ronny and Becker, Barry)	Occupation	14	Native Country	41
21Γ	Credit RiSk Classification (Govindaraj, Praveen)	Feature 6	14	Feature 7	11
~^3~	Credit Risk Classification (Govindaraj, Praveen)	Credit Product Type	28	Overdue Type I	35
4^~	Credit Risk Classification (Govindaraj, Praveen)	Credit Product Type	28	Overdue Type II	35
5~5~	Credit Risk Classification (Govindaraj, Praveen)	Credit Product Type	28	Overdue Type III	36
6~6~	German Traffic Sign (Houben et al., 2013)	Image Width	-219	Traffic Sign	43
71~	German Traffic Sign (Houben et al., 2013)	Image Height	-201	Traffic Sign	43
81Γ	German Traffic Sign (Houben et al., 2013)	Upper left X coordinate	21	Traffic Sign	43
^τ^	German Traffic Sign (Houben et al., 2013)	Upper left Y coordinate	16	Traffic Sign	43
10^"	German Traffic Sign (Houben et al., 2013)	Lower right X coordinate	-204	Traffic Sign	43
TΓ^	German Traffic Sign (Houben et al., 2013)	Lower right Y coordinate	186	Traffic Sign	43
12^"	Mushroom (Schlimmer, Jeff)	Cap color	10	Odor	9
T3-	Mushroom (Schlimmer, Jeff)	Gill color	12	Stalk color above ring	9
14-	Mushroom (Schlimmer, Jeff)	Stalk color below ring	9	Ring Type	8
15^"	Mushroom (Schlimmer, Jeff)	Spore print color	9	Habitat	7
~Γ6~	Lymphography (Kononenko, Igor and Cestnik, Bojan)	Structure Change	8	No. of nodes	8
K Details of Regression Models
The details of the regression models trained in feature selection in Sec. 4.2 is reported in Table 4. The
training and testing splits are the same for Fed-χ2, centralized χ2-test and model without feature
selection (i.e. there are 17,262 training and 4,316 test documents). We use the same learning rate;
random seed and all other settings are also the same to make the comparison fair. We get the result of
Fig. 3 and the models are all trained on NVIDIA GeForce RTX 3090.
Table 4: Model details.
	TaSk 		Model Size	Learning Rate	Random Seed
FED-X2	40000 × 20	01	0
Centralized χ2 -test	40000 X 20	01	0
Without Feature Selection	167135 × 20-	0.1	—	0	—
L SAFFRON Procedure
In Sec. 4.2, we adopt the SAFFRON procedure (Ramdas et al., 2018) to perform online FDR control.
SAFFRON procedure is currently the state of the arts for multiple hypothesis testing. In Alg. 5, we
formally present the SAFFRON algorithm.
The initial error budget for SAFFRON is (1 - λ1W0) < (1 - λ1α), and this will be allocated to
different tests over time. The sequence {λj}j∞=1 is defined by gt and λj serves as a weak estimation of
αj . gt can be any coordinate wise non-decreasing function (line 8 in Alg. 5). Rj := I(pj < αj) is the
indicator for rejection, while Cj := I(pj < λj) is the indicator for candidacy. τj is the jth rejection
time. For each pt, ifpt < λt, SAFFRON adds it to the candidate set Ct and sets the candidates after
the jth rejection (lines 9-10 in Alg. 5). Further, the αt is updated by several parameters like current
wealth, current total rejection numbers, the current size of the candidate set, and so on (lines 11-14 in
Alg. 5). Then, the decision Rt is made according to the updated αt (line 15 in Alg. 5).
The hyper-parameters we use for the SAFFRON procedure in online false discovery rate control
of Sec. 4 are aligned with the setting in Ramdas et al. (2018). In particular, the target FDR level
is α = 0.05, the initial wealth is W0 = 0.0125, and γj is calculated in the following way: γj =
1∕j + 1)1.6
pi=0000 1∕j+1)1.6.
19
Under review as a conference paper at ICLR 2022
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
Algorithm 5: SAFFRON Procedure.
Function SAFFRONPROCEDURE ({pi,P2,…}, α, W0, {γj }∞=0 )：
.{P1,P2,… }： Stream of p-values.
. α: Target FDR level.
. W0 : Initial wealth.
. {γj }j∞=0: Positive non-increasing sequence summing to one.
i J 0	// Set rejection number.
for each P-value Pt ∈ {p1,p2,…} do
λt J gt(R1:t-1, C1:t-1)
Ct J I(Pt < λt)	// Set the indicator for candidacy Ct .
Cj+ J Pit=-τ* 1 +1 Ci	// Set the candidates after the jth
rejection.
if t = 1 then
I αι J (1 — λ1)γ1W0
else
I at J (I - λt)(W0γt-Co+ + (α - WO)Yt-τ1-C1+ + Pj≥2 αγt-Tj-Cj+ )
Rt J I(Pt ≤ αt)	// Output Rt .
if Rt = 1 then
i J i + 1	// Update rejection number.
Ti J t	// Set the ith rejection time.
return {Ro, Ri,…}
20