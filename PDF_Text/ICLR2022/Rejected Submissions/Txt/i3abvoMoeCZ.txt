Exploring Covariate and Concept Shift for
Detection and Confidence Calibration of Out-
of-Distribution Data
Anonymous authors
Paper under double-blind review
Ab stract
Moving beyond testing on in-distribution data, works on Out-of-Distribution
(OOD) detection have recently increased in popularity. A recent attempt to catego-
rize OOD data introduces the concept of near and far OOD detection. Specifically,
prior works define characteristics of OOD data in terms of detection difficulty. We
propose to characterize the spectrum of OOD data using two types of distribu-
tion shifts: covariate shift and concept shift, where covariate shift corresponds to
change in style, e.g., noise, and concept shift indicates change in semantics. This
characterization reveals that sensitivity to each type of shift is important to the
detection and confidence calibration of OOD data. Consequently, we investigate
score functions that capture sensitivity to each type of dataset shift and methods
that improve them. To this end, we theoretically derive two score functions for
OOD detection, the covariate shift score and concept shift score, based on the
decomposition of KL-divergence for both scores, and propose a geometrically-
inspired method (Geometric ODIN) to improve OOD detection under both shifts
with only in-distribution data. Additionally, the proposed method naturally leads
to an expressive post-hoc calibration function which yields state-of-the-art cali-
bration performance on both in-distribution and out-of-distribution data. We are
the first to propose a method that works well across both OOD detection and cali-
bration, and under different types of shifts.
1	Introduction
Out-of-distribution (OOD) detection is a fundamental research area important for downstream tasks
with open-world assumptions such as continual learning (Smith et al., 2021), open-set learning (Yu
et al., 2020) and safety-critical applications such as self-driving (Bojarski et al., 2016). However,
common OOD detection benchmarks1 are not well categorized. This prevents us from more sys-
tematically studying of OOD detection. A recent attempt (Winkens et al., 2020; Fort et al., 2021) to
provide a more granular characterization of OOD data introduces the concept of near OOD (Winkens
et al., 2020; Fort et al., 2021). Near OOD detection is posed as a more challenging problem than
far OOD detection because near OOD datasets usually share similar semantics, and style, e.g., all
natural images of similar environment, as the training dataset. Existing works characterize OOD
in terms of difficulty of OOD detection such as Confusion Log Probability (CLP) (Winkens et al.,
2020). However, this characterization only conveys difficulty of OOD detection across a single
dimension and does not expose intrinsic characteristics of OOD data.
In this paper we provide a more systematic categorization of OOD data, which motivates a more
robust OOD detection method and helps us reflect on some existing approaches. Specifically, there
are at least two dimensions along which we can characterize the spectrum of OOD data. Intuitively,
out-of-distribution data refers to data that are sampled from distributions different from the train-
ing distribution. This is well known in machine learning as distribution shift (Moreno-Torres et al.,
2012). There are two dominant shift types: covariate shift2 and concept shift. The former usually
1Texturess (CimPoi et al., 2014), SVHN (Netzer et al., 2011), PlaCe365 (Zhou et al., 2017), LSUN-
Crop/Resize (Yu et al., 2015), and iSUN (Xu et al., 2015)
2Covariate shift is often assoCiated with model Calibration (Ovadia et al., 2019; Chan et al., 2020)
1
refers to change in style, e.g., clean to noised and natural images to cartoon, and the latter refers to
change in semantics, e.g., dog to cat. Even though existing works Hsu et al. (2020) have acknowl-
edged the different types of OOD datasets and the difficulty of detecting them, their methods do
not distinguish them and do not explicitly disentangle them. To study these two distribution shifts,
we propose to create a benchmark with multiple magnitude of distribution shift in each dimension.
Specifically, we use corrupted CIFAR10C/CIFAR100C (Hendrycks & Dietterich, 2019), originally
developed for testing robustness, with varying degrees of severity to represent increasing covariate
shift; we also introduce a new benchmark, CIFAR100 Splits, which divides the CIFAR100 dataset
into 10 splits with increasing conceptual shift from CIFAR10 classes, measured by semantic simi-
larity in a word embedding space (Pennington et al., 2014). We show that the increasing conceptual
difference measured by semantic similarity in language translates to a spectrum of OOD datasets
under gradual concept shift measured by the difficulty of OOD detection in the image space.
To address these two distribution shifts, we need to approach it from two aspects: representation and
modeling, analogous to the role of entropy and Bayesian inference in uncertainty quantification (De-
peweg et al., 2018) where entropy is a score function that represents uncertainty and Bayesian in-
ference is a method that models uncertainty (Hullermeier & Waegeman, 2021). To represent these
two shifts, we can derive score functions which output scalars indicating the severity of distribution
shift. Specifically we derive two score functions by expanding and decomposing the KL divergence
between a predictive distribution from a classifier and a discrete uniform distribution. Arising from
the decomposition is a covariate shift score which is a function of feature norms and a concept shift
score which is a function of feature angles. To model these two shifts, and motivated by the covariate
score (a function of norms) and the concept score (a function of angles), we propose to directly im-
prove the sensitivity of norms and angles to distribution shift through a geometric perspective (Tian
et al., 2021). Specifically, we adopt the Geometric Sensitivity Decomposition proposed in Tian et al.
(2021), originally developed for model calibration, which decomposes norms and angles into a vari-
ance component and a scalar offset, and improve it by parametrizng the scalar offsets as standalone
networks. Our approach improves detection of OOD data under both covariate and concept shifts.
While OOD detection and calibration3 has been studied separately in the literature, they share the
same underlying motivation: confidence/uncertainty should be low/high when distribution shift oc-
curs. Our method to improve OOD detection naturally yields a powerful calibration function in the
family of intra class-preserving functions (Rahimi et al., 2020). This class of functions provides
enough expressive power to calibrate complex decision boundaries in neural networks and proves to
be effective in calibration on in-distribution data. Therefore, we apply a post-calibration (Guo et al.,
2017) method on a validation set as described in those works. Additionally, thanks to the improved
sensitivity to distribution shift, our model also is on par with state-of-the-art calibration performance
on distribution shifted data as well. To summarize our contributions:
•	We propose to characterize the spectrum of OOD data in terms of covariate and concept
shift and unify the notion of near OOD in different literature (Sec. 3.1). Additionally, we
construct a new benchmark (e.g., CIFAR100 Splits) to represent gradual distribution shift
in each direction (Sec. 4.1.2 and 4.1.3).
•	To represent distribution shifts, we analytically derive two OOD score functions based on
KL-divergence to capture both shifts more effectively (Sec. 3.2).
•	To model distribution shifts, we propose Geometric ODIN to improve the sensitivity of
neural networks during training (Sec. 3.3) and calibration during inference (Sec. 3.4). This
method achieves state-of-the-art performance on both detection (Sec. 4.1) and calibration
(Sec. 4.2) of OOD data.
2	Related Work
Out-of-Distribution (OOD) detection methods can be largely divided into two camps depending on
whether they require OOD data during training. Hendrycks et al. (2018); Thulasidasan et al. (2020);
Roy et al. (2021) leverages anomalous data in training. Our method belongs to the class of methods
that do not assume the availability of OOD data during training. Hendrycks & Gimpel (2016) uses
the maximum softmax probability (MSP) to detect incorrect predictions and OOD data. Lee et al.
3OOD detection focuses on concept-shifted data while calibration focuses on covariate-shifted data.
2
Figure 1: Illustration of near and far Out-of-Distribution data. Top left: CIFAR10 dataset (training).
Top right: CIFAR100 dataset. Lower left: Corrupted CIFAR10. Lower right: SVHN dataset.
(2018) proposes to use Mahalanobis distance by fitting a Gaussian mixture model (GMM) in the
feature space. Mukhoti et al. (2021) uses log density of the GMM model instead. Liu et al. (2020b)
uses an energy score as the uncertainty metric. ODIN (Liang et al., 2017) uses a combination of input
processing and post-training tuning to improve OOD detection performance. Generalized ODIN
Hsu et al. (2020) (also Techapanurak et al. (2019)) includes an additional network in the last layer
to improve OOD detection during training. There are other interesting OOD detection approaches
without OOD data such as using contrastive learning with various transformations (Winkens et al.,
2020; Tack et al., 2020), training a deep ensemble of multiple models (Lakshminarayanan et al.,
2016) and leveraging large pretrained models (Fort et al., 2021). They require extended training
time, hyperparameter tuning and careful selections of transformations, whereas our method does not
introduce any hyperparameters and has negligible influence on standard cross-entropy training time.
Model Calibration methods can also be largely divided in two categories: 1) training time calibra-
tion using augmentations (Thulasidasan et al., 2019; Jang et al., 2021), using modified losses (Kumar
et al., 2018); 2) post-hoc calibration (Guo et al., 2017; Kull et al., 2019; Kumar et al., 2019; Rahimi
et al., 2020). Recently, Rahimi et al. (2020) formally generalizes a family of expressive functions
for calibration, the intra order-preserving functions. This class of functions has more representation
power to calibrate more complex decision boundaries in neural networks. Our proposed method be-
longs to this class of functions. However, all these works focus on calibration of in-distribution data.
To obtain better calibration on OOD data, on which the confidence of a model needs to decrease ac-
cordingly, sensitivity and deterministic uncertainty modeling is explored by SNGP (Van Amersfoort
et al., 2020) and DUQ (Liu et al., 2020a). Our proposed model not only belongs to the family of intra
order-preserving functions, which ensures good in-distribution calibration, but also improves sen-
sitivity to distribution shift, which improves out-of-distribution calibration simultaneously. Please
refer to Appendix 6.8 for a more detailed discussion on related works.
3 Method
3.1	Motivation: Covariate and Concept Shift
We follow Moreno-Torres et al. (2012) for the formal definition of distribution shift, covariate shift
and concept shift. Let X ∈ RD denote the covariate which is the input and Y ∈ R denote the output
label. Distribution shift happens when the training joint distribution is not equal to the testing joint
distribution Ptr(X, Y ) 6= Ptst(X, Y ). Covariate shift appears when Ptr (Y |X) = Ptst(Y |X) and
Ptr(X) 6= Ptst(X). Concept shift appears when Ptr(Y |X) 6= Ptst(Y |X) and Ptr(X) = Ptst(X).
However, in the image domain, it is rare that such concept shift occurs without changes in P (X).
We therefore modify the equality in concept shfit to Ptr (X) ≈ Ptst (X) where superficial, low-level
statistics may be retained.
Specifically, covariate shift happens when the testing data is non-semantically different from the
training data and concept shift happens when the testing data is semantically different from the
training data. We illustrate a spectrum of OOD dataset in Fig. 1. In this example, CIFAR10 is the
training dataset. CIFAR100 represents concept shift because CIFAR100 has a non-overlapping label
space, i.e., Ptr(Y|X) 6= Ptst(Y|X), but similar style with CIFAR10. The corrupted CIFAR10C
dataset (Hendrycks & Dietterich, 2019) represents covariate shift because it has the same labels but
3
Score Functions
Covariate Shift
g(x): PtT(X) - PtSt(X)
Concept Shift
h{y,xγ. PtrQy∖x) → PtStCy))
Geometric ODIN
Feature	Training
乙二自用上前防比口仇
Calibration
≡ li ɪ rl∣2+^)∣l
Predictive
Distribution
=_ Ilil
SoftMax
Figure 2: Diagram of Score functions, Geometric ODIN training and calibration The paper
proposes two score functions and an OOD detection and calibration method: Geometric ODIN.
Loss is backprobagated to all components during training and only to α and β during calibration.
different style, i.e., Ptr(X) 6= Ptst (X), compared to CIFAR10. SVHN represents both covariate
and concept shift due to its non-overlapping label space and very different style.
As shown in Fig. 2, the first goal of this paper is to derive score functions to represent the shift in
either P(X) or P(Y |X). We denote the score function that reflects change from Ptr(X) to Ptst (X)
as the covariate shift score function: g(x) : RD → R and the score function that reflects change
from Ptr (Y |X) to Ptst(Y |X) as the concept shift score function: h(y, x) : Y × RD → R. The
second goal is to improve the sensitivity of these scores to their corresponding distribution shift.
Specifically, we propose a geometrically inspired out-of-distribution detection method with only in-
distribution data (Geometric ODIN). The third goal is to show that, unlike prior works which treat
OOD detection (Mukhoti et al., 2021) and calibration (Van Amersfoort et al., 2020; Liu et al., 2020a)
separately, our proposed Geomeric ODIN naturally leads to an expressive calibration function in the
family of intra order-preserving functions (Rahimi et al., 2020), which ensures good calibration.
3.2 Covariate and Concept Score Functions
In this section, we theoretically derive two score functions, g(x) and h(y, x), based on the KL-
divergence between a uniform distribution U and a predicted distribution P ∈ RM , where M is the
number of classes. By starting from KL-divergence, we hinge the subsequent derivation of score
functions on a physical meaningful uncertainty measure, i.e., how far the predicted distribution is
from a uniform distribution. This relationship ensures a natural interpretation of score functions
because predictions on distribution shifted data should have larger uncertainty, i.e. smaller dis-
tance from uniform. We are specifically interested in softmax-linear models for classification. They
typically consist of a feature extractor and a linear layer followed by a softmax activation. Let
f ∈ RD denote a feature vector from the feature extractor4. The output of the linear layer, i.e.,
logits, l ∈ RM =< f, W > is defined as the inner product between the feature vector and a weight
matrix in the linear layer. Let li = ∣f∣∣2IlWi∣∣2 CoS φi denote the ith logit, Pi = LMxpIi denote
j=1 exp lj
the predicted probability of the ith class. The KL-divergence KL(U||P) can be written as following:
M1	M	1M
KL(U∣∣P) = — X M lnMPi= lnXexp j — M Xli - lnM	(1)
I j —y—	—}
Log-Sum-Exp
Now we can use the inequality property of Log-Sum-Exp (LSE)5 functions in Eq. 2 to bound Eq. 1.
M
max lj ≤ ln	exp lj ≤ max lj + lnM	(2)
j	j=1	j
Therefore the KL-divergence (Eq. 1) can be bounded as follows:
U — lnM ≤ KL(U∣∣P) ≤ U	(3)
4Bold letter indicates vectors
5Note that the negative LSE function is also defined as free energy in Liu et al. (2020b).
4
where U = maxj lj - M PM=I li. U can be further decomposed into two multiplicative components
by plugging in the definition of logits li :
1 M £	1 M
U = max Ij - M∑Sli = kfk2 I max Ilwj k2 Cos φj - M EkWik2 cos φi
i=1	i=1
|------------------------------
(4)
}
{^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
h(y,x)
We define the covariate shift score function as g(x) , kfk2 because the norm of a feature vector
is the sum of squared activation values and only depends on the input. Intuitively, activation of a
neural network on covariate-shifted data should be weaker than in-distribution data. Therefore, g(x)
assigns a higher value to in-distribution data than to OOD data. We define the concept shift score
function as h(y,χ) ，max, ∣∣Wjk2 cosφj - M PMI IlWiIl2 CosΦi because it is the difference
between the cosine distance of the predicted class and the average cosine distance of all classes and
depends on both the input and final class membership, assigned by the max operator. Intuitively,
class assignment should be less obvious under concept shift and the difference should be small.
Consequently, h(y, x) assigns a higher value to in-distribution data than to OOD data.In retrospect,
our definition of covariate shift and concept shift scores supports existing findings that the feature
norms correspond to intra-class variance and angles reflect inter-class variation (Liu et al., 2018).
Intuitively, covariate shift represents non-semantic change within a specific class, i.e., intra-class
variance; concept shift represents semantic changes, i.e, inter-class variation. Here we formalize
the intuition and observations in Liu et al. (2018) as score functions derived analytically from a
KL-divergence viewpoint. We provide an extended discussion of these scores in Appendix 7.
More importantly, the combined score function U (Eq. 4) carries a physical meaning: it bounds the
KL-divergence between a uniform distribution and the predictive distribution. Intuitively, a small U
indicates large uncertainty because U upper bounds KL(U||P), and a large U indicates small un-
certainty because it also appears in the lower bound. We can derive U also by Taylor-expanding the
softmax equation as in Liang et al. (2017), which claims that U is responsible for good OOD detec-
tion. Our findings confirm this and push it further by decomposing it into different components. A
similar scoring function, S(x) = IfI2 maxj IWj I2 cos φj, is used in Tack et al. (2020), but is only
empirically motivated based on observations with limited analytical insights such as its relationship
to uncertainty and the functionality of each of components. In contrast, our derivation clearly shows
the relationship between these score functions and the KL-divergence, which is as an uncertainty
measure, and disentangles their roles. We will compare the g(x), h(y, x) and U as score functions
in subsequent experiments and analyze their respective sensitivity to different shifts (Sec. 4.1).
3.3 GEOMETRIC OUT-OF-DISTRIBUTION DETECTION WITH In-DISTRIBUTION DATA
As derived in Eq. 4, the covariate score is a function of feature norms and the concept score is a
function of feature angles. Consequently, improving the sensitivity of feature norms and feature
angles to data shifts seems to be the natural next step to improve OOD detection. Therefore, we
adopt Geometric Sensitivity Decomposition (GSD) (Tian et al., 2021) (reviewed in appendix 6.3)
to improve sensitivity to covariate and concept shifts. Specifically, GSD improves sensitivity by
extracting sensitive components from norms ∣∣f*∣∣26 and angles ∣φ*∣ through a decomposition of
them into: a scalar offset and a variance component as shown in Eq. 5. Scalar offsets Cf and Cφ
minimize the loss on the training set and the variance components f and φi account sensitively for
variances in samples.
kf *k2 = kf∣2 + Cf,	∣φ" = IΦi∣-∣Cφ∣	⑸
With the decomposed components, the original logit l* = ∣∣f* k2 kWi∣∣2 Cos φ*, can be written as:
(	N
li = kf *k2kwik2 Cos φi ≈ li = —ʃ kf k2 + —ʃ ZCr Ilwik2 cos φi	⑹
Cos Cφ	Cos Cφ
∖l-{z^}	l-{z^}
αα
6The superscript * denotes the original component before decomposition.
5
where li denote the new ith logit. In Eq. 6, the new7 feature f is a direct output of a feature extractor,
and is modified by α and β as also illustrated Fig. 2. Note that the calculation of score functions in
Sec. 3.2 only uses the feature and is independent of α and β.
Because cos Cφ and Cf are scalar offsets, we can parametrize them separately from the main network.
Unlike GSD which parametrizes them as instance-independent scalars, inspired by Hsu et al. (2020);
Techapanurak et al. (2019), we make α(f) and β(f) instance-dependent scalars and use a single
linear layer to learn them. To enforce numerical constraints, i.e., 0 < α < 1 and β > 0, α(f) uses a
sigmoid activation and β(f) uses a softplus activation. Finally, the relaxed output is:
expli	exp((af)kfk2 + Of)) kwik2 cosφi)
P(Y = ilx)=——rτ------= -------- I /------------7----------r-
Pj=Iexplj	Pj= eχp ((0⅛kfk2+ Of)) kwjk2 cos φj
(7)
Now the new predicted norm kf k2 and angle φi are more sensitive to input changes because they
encode variances in samples as shown in Eq. 5. Therefore, including β (related to norms) improves
sensitivity to covariate shift and including α (related to angles) improves sensitivity to concept shift.
Note that, under this construction, Generalized ODIN (Hsu et al., 2020) is a special case of our
proposed method. Generalized ODIN only includes the α(f ) which only improves angle sensitivity
but not norm sensitivity. Unlike Hsu et al. (2020)’s probabilistic perspective8, our model builds on a
geometric perspective and captures both covariate and concept shifts by improving norm and angle
sensitivity. The new model can be trained identically as the vanilla network without additional hy-
perparameter tuning and extended training time. Combined with score functions derived in Sec. 3.2,
Geometric ODIN achieves state-of-the-art OOD detection performance (Sec. 4.1)
3.4 Calibration of Geometric ODIN
Like most softmax-linear classification models, Geometric ODIN also suffers from miscalibration
right after training. Specifically, predictions tend to be overconfident (Guo et al., 2017). In the case
of Geometric ODIN, the cause of overconfidence is obvious by construction. The offset scalars
β (f ) and α(f ) are designed to minimize the training loss. Practically, they are optimized to push
the predicted probability of the ground truth class to be close to 1 during training, and this intended
behavior unsolicitedly continues to inference time when accuracy is not as high as during training.
Nevertheless, our construction of α(f) and β(f) belongs to a family of intra order-preserving func-
tions, which has potentially strong representation to calibrate complex functions in deep networks
according to Theorem 1 in Rahimi et al. (2020). To prove that, it’s suffice to show that that α(l)
and β (l) 9 forms a strictly positive function, m(l) as shown in Eq. 8

li
fk2+
kwik2 cos φi
「+	β(l)
α(l) + α(l)kfk2
1---------{--------
m(l)
}1
kfk2kwik2cosφi
}
(8)
z
{
Because 0 < α(l) <1 and β (l) > 0, the function m(l) is strictly positive and thus satisfies
Theorem 1 in Rahimi et al. (2020) (See appendix 6.4 for more details). We follow a simple procedure
proposed in Guo et al. (2017); Rahimi et al. (2020) to calibrate Geometric ODIN by minimizing the
negative log likelihood (NLL) on a evaluation dataset for a few epochs. During calibration the
gradients to other parts of the model are stopped and only these two linear layers, α(l) and β (l), are
optimized (Fig. 2). Naturally with the improved sensitivity and calibrated functions, the calibration
of Geometric ODIN is on par with state-of-the-art models on both in-distribution and OOD data.
Note that, as mentioned in Sec. 3.3, because the calculation of score functions for OOD detection
only depends on the feature f and is independent of α and β , calibration does not affect the OOD
detection performance and does not change prediction results due to the order-preserving property.
7Even though both f * and f are outputs directly from the feature extractor, We use original and new to
indicate whether GSD is applied.
8α(f) is interpreted as P (din |x), the probability of x being in-distribution.
9FolloWing Rahimi et al. (2020), Which uses logits l as input instead of the feature f. Our derivation in
previous sections is still valid because l solely depends on f given a model.
6
AUROC↑
Score Functions
ID:CIFAR100
ID:CIFAR10
		Near CIFAR10	Far SVHN	Far TIN(R)	Near CIFAR100	Near CIFAR10C	Far SVHN	Far TIN(R)
	MSP (Hendrycks & Gimpel, 2016)	80.68±0.34	77.37±2.25	81.65±0.14	88.93 ±0.37	70.58±0.59	93.66±1.79	87.98±0.35
Vanilla Wide-ResNet-28-10	Energy (LiU et al., 2020b)	80.74±0.45	79.48±2.91	82.04±0.14	88.84±0.44	70.60±0.52	94.39±2.30	88.16±0.39
(Zagoruyko & Komodakis, 2016)	Mahanobis (Lee et al., 2018)	66.72±0.77	93.55±1.18	76.54±0.31	87.26±1.21	68.38±0.50	99.19±0.22	87.33±1.39
	GMM Density (Mukhoti et al., 2021)	66.75±0.74	93.91±0.71	76.58±0.29	87.26±1.20	75.71±0.80	99.19±0.22	87.33±1.39
DUQ (Van Amersfoort et al., 2020)	Kernel Distance	一	-	-	85.92±0.35*	-	93.71±0.61*	-
SNGP (Liu et al., 2020a)	SoftMax Entropy	一	85.71±0.81*	-	91.13±0.15*	-	94.0±1.30*	-
DDU (Mukhoti et al., 2021)	GMM Density	67.65±0.20	92.59±1.47	77.72±0.15	90.69±0.42	76.00±0.00	97.12±1.21	84.89±1.01
Hyper-Free/Generalized ODIN	Cosine Similarity	76.90±0.30	95.39±1.31	82.93±0.18	92.28±0.16	75.84±0.70	99.56±0.12	92.03±0.15
5-Ensemble (Lakshminarayanan et al., 2016)	SoftMax Entropy	一	79.54±0.91*	-	92.13±0.02*	-	97.73±0.31*	-
Ours: α(x)-only		h(y,χ)	79.24±0.37	83.75±3.30	82.67±0.22	92.28±0.15	77.00±0.00	97.56±0.90	91.83±0.09
	hy,x)	79.72±0.41	88.72±2.99	82.89±0.24	91.29±0.07	73.80±0.45	98.42±0.21	90.87±0.10
Ours: α-β	g(x)	60.70±0.56	93.15±2.06	72.34±0.42	89.08±0.24	76.80±1.10	99.40±0.14	90.79±0.39
	U	71.42±0.27	95.77±0.33	80.86±0.53	92.31±0.21	78.00±0.71	99.54±0.08	92.85±0.19
Table 1: AUROC ↑ for Near and Far OOD detection. Results are averaged over 5 runs. Our
α(x)-only model is the same as Generalized ODIN (Hsu et al., 2020) (hI (x) variant) without its
input processing. Hyper-free (Techapanurak et al., 2019) and Generalized ODIN (hC (x) variant)
are the same. * denotes results from Mukhoti et al. (2021).
4	Experiments
In this section, we present results for Out-of-Distribution detection in Sec. 4.1 and calibration in
Sec. 4.2. We are the first to present a method that works well on both OOD detection and cal-
ibration, disentangling shift types. Implementation: Following prior works (Liu et al., 2020a;
Van Amersfoort et al., 2020; Mukhoti et al., 2021), we use Wide-ResNet-28-10 (Mukhoti et al.,
2021) for all experiments. We train the model using SGD with an initial learning rate of 0.1 for 200
epochs. The learning rate is annealed with a cosine scheduler (Loshchilov & Hutter, 2016) (more
details in appendix 6.5). Metrics: For OOD detection in Sec. 4.1, we use the common AUROC and
TNR@TPR95 as benchmark metrics. For calibration in Sec. 4.2, we use the Expected Calibration
Error (ECE) (Guo et al., 2017) with 15 bins and Negative Log Likelihood (NLL) which is a strictly
proper scoring rule (Gneiting & Raftery, 2007). Datasets: CIFAR10 (Krizhevsky et al., a) and CI-
FAR100 (Krizhevsky et al., b) are considered near OOD datasets (Winkens et al., 2020; Fort et al.,
2021) to each other. SVHN (Netzer et al., 2011) is considered a far OOD dataset to both CIFAR10
and CIFAR100 due to its shift in both concept and style. The CIFAR10C/CIFAR100C (Hendrycks
& Dietterich, 2019) dataset is a variant of CIFAR10/CIFAR100 corrupted by 15 types of noises with
5 degrees of severity. We also introduce CIRAF100 Splits benchmark which consists of 10 datasets
with increasing concept shift from CIFAR10 classes (Sec. 4.1.3).
4.1	Out-of-Distribution Detection Results
4.1.1	Near and Far OOD Detection
The α-β model is the best. In Tab. 1, we present OOD detection results against state-of-the-art
methods on existing near and far OOD categorization. For near OOD detection under strong con-
cept shift, CIFAR10 (ID) vs. CIFAR100 (OOD), both our α-only and α-β variants achieve the the
best performance. This demonstrates that α(x) improves the sensitivity of angles and hence the
sensitivity to concept-shifted data. For near OOD detection under strong covariate shift, CIFAR10
(ID) vs. CIFAR10C (OOD), the α-β variant achieves the the best performance. This suggests that
β(x) improves the sensitivity of norms and hence the sensitivity to covariate-shifted data. In CI-
FAR100 (ID) vs. CIFAR10 (OOD) experiments, the performance of the α-only and α-β variants are
within variance and is close to some other compared methods, we can not make clear observations
from those experiments10. For far OOD detection, CIFAR10/CIFAR100 (ID) vs. SVHN (OOD), the
α-β model achieves state-of-the-art performance. This reconfirms that β(x) improves sensitivity to
covariate-shifted data, because SVHN has both covariate and concept shifts compared to the CIFAR
datasets, and the α-β model outperforms the α-only variant, which only improves on concept shift,
by a noticeable margin. In terms of score functions, the best performing one for the α-β model
is U, which is a product of g(x) and h(y, x) (Sec. 3.2), while that of the α-only model is h(y, x).
This shows that depending on which component is more sensitive, different scoring functions are
10Other confounding factors could contribute to the close performance. Prior works either omit comparisons
under these settings (Mukhoti et al., 2021) or report only marginal improvement (Winkens et al., 2020)
7
——g(χ)
h(y,x)
a α-β, α-only, β-only and vanilla models using score functions g(x) and h(y, x) b All models using U
Figure 3: Capturing Covariate Shift (Motion Blur) All results are averaged over 5 runs. Modeling
covairate shift (α-β model) yields the best performance as shown in Fig. 3b. g(x) is more responsive
to covariate shift than h(y, x) as shown in Fig. 3a with the α-β model.
α-only AUROC
severity
——g(χ)
h(y,χ)
a α-β, α-only, β-only and vanilla models using score functions g(x) and h(y, x) b All models using U
Figure 4: Capturing Concept Shift (CIFAR100 Splits) All results are averaged over 5 runs. Mod-
eling concept shift (both α-only and α-β models) yields the best performance as shown in Fig. 4b.
h(y, x) is more responsive to concept shift than g(x) as shown in Fig. 4a with the α-β model.
preferred. When the sensitivity of both norms and angles are improved, as in the α-β variant, the
combined score function U performs well under different distribution shifts.
4.1.2	OOD Detection under Covariate Shift with Image Corruption
The g(x) score captures covariate shift. We compare the α-β, α-only (β = 0), β-only (α = 1)
variants and the vanilla model on CIFAR10C (Hendrycks & Dietterich, 2019) corrupted by mo-
tion blur in Fig. 3 with increasing degrees of noise (see appendix 6.7 for additional experiments).
From 3a, we observe that 1) as covariate shift severity increases, OOD detection becomes easier
because AUROC increases with increasing severity. 2) the vanilla model is more sensitive to the
concept shift component because h(y, x) > g(x) in the vanilla model plot even though covariate
shift is the dominant distribution shift in this example. 3) when sensitivity to both covariate and
concept shift is improved , the α-β model becomes more sensitive to the covariate shift component.
This suggests that the dominant shift in this example is indeed covariate shift. From Fig. 3b, we ob-
serve that the α-β model outperforms the β-only model using the combined score function U . This
suggests that improving sensitivity to both shifts and using U yield the best OOD detection perfor-
mance. In retrospect, the performance of OOD detection has always relied on two components: the
model and the score function. Some works propose more responsive score functions, e.g., energy
in Liu et al. (2020b), and some works propose more sensitive models, e.g., DDU in Mukhoti et al.
(2021) to improve the performance of existing score functions.
4.1.3	OOD Detection under Concept Shift with CIFAR100 Special Splits
Finding a dataset to benchmark gradual concept shift is not straightforward because concept shift is
traditionally thought as binary: overlapping or non-overlapping. However, not all non-overlapping
labels are the same. For example, pickup truck (CIFAR100) is semantically much closer to truck
(CIFAR10) than sunflowers (CIFAR100) is. To create this gradual concept/semantic shift, we pro-
pose to divide the CIFAR100 dataset into 10 sub-datasets with increasing conceptual difference from
CIFAR10 classes. Specially, we use 300 dimensional Glove word embeddings11 (Pennington et al.,
2014) trained on the entire wikipedia2014 and Gigaword5 (Napoles et al., 2012) to measure seman-
tic closeness (inner product) between CIFAR100 and CIFAR10 classes. The result is 10 subdatasets
split from CIFAR100. Please refer to appendix 6.6 for the full splits.
The h(y, x) score captures concept shift. Following the previous section, we benchmark the α-β,
α-only, β-only variants and the vanilla model on the newly created CIFAR100 Splits. From Fig. 4a,
we observe that 1) as concept shift severity increases, OOD detection becomes easier because AU-
11https://nlp.stanford.edu/projects/glove/
8
ROC increases with increasing severity. 2) both concept shift and covariate shift are present because
AUROC using either h(y, x) or g(x) increases. 3) the vanilla model is dominantly more sensitive to
the concept shift component because concept shift is the dominant distribution shift in CIFAR100
Splits by construction and vanilla ResNet is more sensitive to concept shift (the same behavior is
also observed on covariate-shift-heavy data in Sec. 4.1.2). 4) when sensitivity to both shifts is im-
proved, the α-β model is still more sensitive to concept shift (h(y, x) > g(x)). This reconfirms
that the dominant shift type is indeed concept shift. From Fig. 4b, interestingly, we observe that
all three variants perform similarly and all outperform the vanilla model. Combined with previous
observations that the dataset has strong concept shift and the vanilla model is already very sen-
sitive to concept shift, improving sensitivity to the covariate shift component yields equally good
performance as improving sensitivity to both shifts.
4.2	Out-of-Distribution Calibration Results
	Accuracy ↑		ECE J		NLL J	
	Clean	Corrupted	Clean	Corrupted	Clean	Corrupted
Vanilla	96∙23±0.13	69.78±1.22	0.015±0.001	0.148±0.008	0.148±0.005	1.107±0.042
Temp Scaling (Guo et al., 2017)	96.23±0.13	69.78±1.22	0.003±0.001	0.107±0.009	0.131±0.003	0.906±0.029
Matrix scaling (Guo et al., 2017)	95.98±0.10	69.81±1.11	0.005±0.000	0.107±0.009	0.145±0.008	0.966±0.041
Dirichlet (Kull et al., 2019)	96.10±0.10	69.75±1.09	0.004±0.002	0.114±0.007	0.130±0.004	0.977±0.032
DUQ (Van Amersfoort et al., 2020)f	94.7±0.02	71.6±0.02	0.034±0.002	0.183±0.011	0.239±0.02	1.348±0.01
SNGP (Liu et al., 2020a)f	95.9±0.01	74.6±0.01	0.018±0.001	0.090±0.012	0.138±0.01	0.935±0.01
GSD (Tian et al., 2021)	95.9±0.01	74.9±0.05	0.008±0.002	0.085±0.012	0.140±0.004	0.853±0.039
Ours: α-β	95.99±0.12	70.41±0.55	0.001±0.000	0.071±0.0112	0.130± 0.003	0.854±0.029~
Table 2: Calibration on CIFAR10 averaged over 5 seed. f denotes results from LiU et al. (2020a).
	Accuracy ↑		ECE J		NLL J	
	Clean	Corrupted	Clean	Corrupted	Clean	Corrupted
Vanilla	80.66± 0.20	50.25 ±0.48	0.035±0.002	0.171±0.017	0.774±0.007	2.384±0.039
Temp Scaling (Guo et al., 2017)	80.99±0.20	50.25±0.48	0.033±0.002	0.163±0.017	0.776±0.006	2.368±0.039
Matrix scaling (Guo et al., 2017)	79.31 ±0.25	48.74±0.55	0.03±0.003	0.160±0.016	0.791±0.013	2.532±0.051
Dirichlet (Kull et al., 2019)	80.70±0.36	50.06±0.46	0.015±0.002	0.144±0.017	0.743±0.025	2.346±0.017
DUQ (Van Amersfoort et al., 2020)f	78.5±0.02	50.4±0.02	0.119±0.001	0.281±0.012	0.980±0.02	2.841±0.01
SNGP (Liu et al., 2020a)f	79.9±0.03	49.0±0.02	0.025±0.012	0.117±0.014	0.847±0.01	2.626±0.01
GSD (Tian et al., 2021)	79.8±0.03	49.8±0.03	0.027±0.003	0.088±0.007	0.784±0.011	2.236±0.021
Ours: α-β	79.21±0.19	49.21 ±0.61	0.010±0.001	0.084±0.009	0.754±0.005	2.323±0.057
Table 3: Calibration on CIFAR100 averaged over 5 seeds. f denotes results from Liu et al. (2020a).
As proved in Sec. 3.4, our model naturally leads to a calibration function in the family of in-
tra order-preserving functions (Rahimi et al., 2020), which ensures good in-distribution calibra-
tion performance. Moreover, thanks to the improved sensitivity, the model can potentially achieve
state-of-the-art out-of-distribution calibration as well. We benchmark our models on both clean CI-
FAR10/CIFAR100 as well as corrupted CIFAR10C/CIFAR100C (Hendrycks & Dietterich, 2019).
Following prior works (Rahimi et al., 2020), we use 5-fold cross validation for in-distribution cal-
ibration experiments. As shown in Tab. 2 and Tab. 3, our model is comparable to state-of-the-art
models on both in-distribution and out-of-distribution calibration. In the literature, OOD detection
and calibration have been studied separately, but our method and experimentation tackles both tasks.
5	Conclusion
In this work, we propose to characterize the spectrum of out-of-distribution (OOD) data using co-
variate shift and concept shift. Unlike difficulty of OOD detection, distribution shift exposes intrinsic
characteristics of OOD data. Consequently, to achieve good OOD detection performance, a model
needs to consider both distribution shifts. At representation level, we derive two score functions that
represent and capture each shift separately. At modeling level, inspired by these score functions,
we propose a geometrically-inspired method, Geometric ODIN, to improve a model’s sensitivity to
both shift. Furthermore, Geometric ODIN is the first method that considers both OOD detection
and calibration, targeting both concept and covariate shifts, and yields state-of-the-art performance
in both tasks. Finally, we hope that the distribution shift perspective can lead to a new direction to
study OOD data and unify the the fields of detection and calibration of OOD data.
9
References
Mariusz Bojarski, Davide Del Testa, Daniel Dworakowski, Bernhard Firner, Beat Flepp, Prasoon
Goyal, Lawrence D Jackel, Mathew Monfort, Urs Muller, Jiakai Zhang, et al. End to end learning
for self-driving cars. arXiv preprint arXiv:1604.07316, 2016.
Alex Chan, Ahmed Alaa, Zhaozhi Qian, and Mihaela Van Der Schaar. Unlabelled data improves
bayesian uncertainty calibration under covariate shift. In International Conference on Machine
Learning ,pp.1392-1402. PMLR, 2020.
Mircea Cimpoi, Subhransu Maji, Iasonas Kokkinos, Sammy Mohamed, and Andrea Vedaldi. De-
scribing textures in the wild. In Proceedings of the IEEE Conference on Computer Vision and
Pattern Recognition, pp. 3606-3613, 2014.
Stefan Depeweg, Jose-Miguel Hernandez-Lobato, Finale Doshi-Velez, and Steffen Udluft. Decom-
position of uncertainty in bayesian deep learning for efficient and risk-sensitive learning. In
International Conference on Machine Learning, pp. 1184-1193. PMLR, 2018.
Stanislav Fort, Jie Ren, and Balaji Lakshminarayanan. Exploring the limits of out-of-distribution
detection. arXiv preprint arXiv:2106.03004, 2021.
Tilmann Gneiting and Adrian E Raftery. Strictly proper scoring rules, prediction, and estimation.
Journal of the American statistical Association, 102(477):359-378, 2007.
Ishaan Gulrajani, Faruk Ahmed, Martin Arjovsky, Vincent Dumoulin, and Aaron C Courville. Im-
proved training of wasserstein gans. In I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fer-
gus, S. Vishwanathan, and R. Garnett (eds.), Advances in Neural Information Processing Systems,
volume 30. Curran Associates, Inc., 2017. URL https://proceedings.neurips.cc/
paper/2017/file/892c3b1c6dccd52936e27cbd0ff683d6-Paper.pdf.
Chuan Guo, Geoff Pleiss, Yu Sun, and Kilian Q Weinberger. On calibration of modern neural
networks. In International Conference on Machine Learning, pp. 1321-1330. PMLR, 2017.
Dan Hendrycks and Thomas Dietterich. Benchmarking neural network robustness to common cor-
ruptions and perturbations. Proceedings of the International Conference on Learning Represen-
tations, 2019.
Dan Hendrycks and Kevin Gimpel. A baseline for detecting misclassified and out-of-distribution
examples in neural networks. arXiv preprint arXiv:1610.02136, 2016.
Dan Hendrycks, Mantas Mazeika, and Thomas Dietterich. Deep anomaly detection with outlier
exposure. arXiv preprint arXiv:1812.04606, 2018.
Yen-Chang Hsu, Yilin Shen, Hongxia Jin, and Zsolt Kira. Generalized odin: Detecting out-
of-distribution image without learning from out-of-distribution data. In Proceedings of the
IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 10951-10960, 2020.
Eyke Hullermeier and Willem Waegeman. Aleatoric and epistemic uncertainty in machine learning:
An introduction to concepts and methods. Machine Learning, 110(3):457-506, 2021.
Sooyong Jang, Insup Lee, and James Weimer. Improving classifier confidence using lossy label-
invariant transformations. In International Conference on Artificial Intelligence and Statistics,
pp. 4051-4059. PMLR, 2021.
Alex Krizhevsky, Vinod Nair, and Geoffrey Hinton. Cifar-10 (canadian institute for advanced re-
search). a. URL http://www.cs.toronto.edu/~kriz/cifar.html.
Alex Krizhevsky, Vinod Nair, and Geoffrey Hinton. Cifar-100 (canadian institute for advanced
research). b. URL http://www.cs.toronto.edu/~kriz/cifar.html.
Meelis Kull, Miquel Perello-Nieto, Markus Kangsepp, Hao Song, Peter Flach, et al. Beyond temper-
ature scaling: Obtaining well-calibrated multiclass probabilities with dirichlet calibration. arXiv
preprint arXiv:1910.12656, 2019.
10
Ananya Kumar, Percy Liang, and Tengyu Ma. Verified uncertainty calibration. arXiv preprint
arXiv:1909.10155, 2019.
Aviral Kumar, Sunita Sarawagi, and Ujjwal Jain. Trainable calibration measures for neural networks
from kernel mean embeddings. In International Conference on Machine Learning, pp. 2805-
2814. PMLR, 2018.
Balaji Lakshminarayanan, Alexander Pritzel, and Charles Blundell. Simple and scalable predictive
uncertainty estimation using deep ensembles. arXiv preprint arXiv:1612.01474, 2016.
Kimin Lee, Kibok Lee, Honglak Lee, and Jinwoo Shin. A simple unified framework for detecting
out-of-distribution samples and adversarial attacks. Advances in neural information processing
systems, 31, 2018.
Shiyu Liang, Yixuan Li, and Rayadurgam Srikant. Enhancing the reliability of out-of-distribution
image detection in neural networks. arXiv preprint arXiv:1706.02690, 2017.
Jeremiah Zhe Liu, Zi Lin, Shreyas Padhy, Dustin Tran, Tania Bedrax-Weiss, and Balaji Lakshmi-
narayanan. Simple and principled uncertainty estimation with deterministic deep learning via
distance awareness. arXiv preprint arXiv:2006.10108, 2020a.
Weitang Liu, Xiaoyun Wang, John D Owens, and Yixuan Li. Energy-based out-of-distribution
detection. arXiv preprint arXiv:2010.03759, 2020b.
Weiyang Liu, Zhen Liu, Zhiding Yu, Bo Dai, Rongmei Lin, Yisen Wang, James M Rehg, and
Le Song. Decoupled networks. In Proceedings of the IEEE Conference on Computer Vision and
Pattern Recognition, pp. 2771-2779, 2018.
Ilya Loshchilov and Frank Hutter. Sgdr: Stochastic gradient descent with warm restarts. arXiv
preprint arXiv:1608.03983, 2016.
Takeru Miyato, Toshiki Kataoka, Masanori Koyama, and Yuichi Yoshida. Spectral normalization
for generative adversarial networks. arXiv preprint arXiv:1802.05957, 2018.
Jose G Moreno-Torres, Troy Raeder, Roclo Alaiz-Rodriguez, Nitesh V Chawla, and Francisco Her-
rera. A unifying view on dataset shift in classification. Pattern recognition, 45(1):521-530, 2012.
Jishnu Mukhoti, Andreas Kirsch, Joost van Amersfoort, Philip HS Torr, and Yarin Gal. Deterministic
neural networks with appropriate inductive biases capture epistemic and aleatoric uncertainty.
arXiv preprint arXiv:2102.11582, 2021.
Mahdi Pakdaman Naeini, Gregory Cooper, and Milos Hauskrecht. Obtaining well calibrated proba-
bilities using bayesian binning. In Proceedings of the AAAI Conference on Artificial Intelligence,
volume 29, 2015.
Courtney Napoles, Matthew R Gormley, and Benjamin Van Durme. Annotated gigaword. In
Proceedings of the Joint Workshop on Automatic Knowledge Base Construction and Web-scale
Knowledge Extraction (AKBC-WEKEX), pp. 95-100, 2012.
Yuval Netzer, Tao Wang, Adam Coates, Alessandro Bissacco, Bo Wu, and Andrew Y. Ng. Reading
digits in natural images with unsupervised feature learning. 2011. URL http://ufldl.
stanford.edu/housenumbers/nips2011_housenumbers.pdf.
Yaniv Ovadia, Emily Fertig, Jie Ren, Zachary Nado, David Sculley, Sebastian Nowozin, Joshua V
Dillon, Balaji Lakshminarayanan, and Jasper Snoek. Can you trust your model’s uncertainty?
evaluating predictive uncertainty under dataset shift. arXiv preprint arXiv:1906.02530, 2019.
Jeffrey Pennington, Richard Socher, and Christopher D Manning. Glove: Global vectors for word
representation. In Proceedings of the 2014 conference on empirical methods in natural language
processing (EMNLP), pp. 1532-1543, 2014.
Amir Rahimi, Amirreza Shaban, Ching-An Cheng, Richard Hartley, and Byron Boots. Intra order-
preserving functions for calibration of multi-class neural networks. Advances in Neural Informa-
tion Processing Systems, 33:13456-13467, 2020.
11
Abhijit Guha Roy, Jie Ren, Shekoofeh Azizi, Aaron Loh, Vivek Natarajan, Basil Mustafa, Nick
Pawlowski, Jan Freyberg, Yuan Liu, Zach Beaver, et al. Does your dermatology classifier
know what it doesn’t know? detecting the long-tail of unseen conditions. arXiv preprint
arXiv:2104.03829, 2021.
James Smith, Jonathan Balloch, Yen-Chang Hsu, and Zsolt Kira. Memory-efficient semi-supervised
continual learning: The world is its own replay buffer. arXiv preprint arXiv:2101.09536, 2021.
Jihoon Tack, Sangwoo Mo, Jongheon Jeong, and Jinwoo Shin. Csi: Novelty detection via contrastive
learning on distributionally shifted instances. arXiv preprint arXiv:2007.08176, 2020.
Engkarat Techapanurak, Masanori Suganuma, and Takayuki Okatani. Hyperparameter-free out-of-
distribution detection using softmax of scaled cosine similarity. arXiv preprint arXiv:1905.10628,
2019.
Sunil Thulasidasan, Gopinath Chennupati, Jeff Bilmes, Tanmoy Bhattacharya, and Sarah Michalak.
On mixup training: Improved calibration and predictive uncertainty for deep neural networks.
arXiv preprint arXiv:1905.11001, 2019.
Sunil Thulasidasan, Sushil Thapa, Sayera Dhaubhadel, Gopinath Chennupati, Tanmoy Bhat-
tacharya, and Jeff Bilmes. A simple and effective baseline for out-of-distribution detection using
abstention. 2020.
Junjiao Tian, Dylan Yung, Yen-Chang Hsu, and Zsolt Kira. A geometric perspective towards neural
calibration via sensitivity decomposition. In NeurIPS, 2021.
Joost Van Amersfoort, Lewis Smith, Yee Whye Teh, and Yarin Gal. Uncertainty estimation using a
single deep deterministic neural network. In International Conference on Machine Learning, pp.
9690-9700. PMLR, 2020.
Martin Wattenberg, Fernanda Viegas, and Ian Johnson. HoW to use t-sne effectively. DistiU, 1(10):
e2, 2016.
Jim Winkens, Rudy Bunel, Abhijit Guha Roy, Robert Stanforth, Vivek Natarajan, Joseph R Ledsam,
Patricia MacWilliams, Pushmeet Kohli, Alan Karthikesalingam, Simon Kohl, et al. Contrastive
training for improved out-of-distribution detection. arXiv preprint arXiv:2007.05566, 2020.
Pingmei Xu, Krista A Ehinger, Yinda Zhang, Adam Finkelstein, Sanjeev R Kulkarni, and Jianxiong
Xiao. Turkergaze: CroWdsourcing saliency With Webcam based eye tracking. arXiv preprint
arXiv:1504.06755, 2015.
Fisher Yu, Ari Seff, Yinda Zhang, Shuran Song, Thomas Funkhouser, and Jianxiong Xiao. Lsun:
Construction of a large-scale image dataset using deep learning With humans in the loop. arXiv
preprint arXiv:1506.03365, 2015.
Qing Yu, Daiki Ikami, Go Irie, and Kiyoharu AizaWa. Multi-task curriculum frameWork for open-set
semi-supervised learning. In European Conference on Computer Vision, pp. 438-454. Springer,
2020.
Sergey Zagoruyko and Nikos Komodakis. Wide residual netWorks. arXiv preprint
arXiv:1605.07146, 2016.
Bolei Zhou, Agata Lapedriza, Aditya Khosla, Aude Oliva, and Antonio Torralba. Places: A 10
million image database for scene recognition. IEEE transactions on pattern analysis and machine
intelligence, 40(6):1452-1464, 2017.
12
6	Appendix
6.1	Introduction to Out-of-Distribution Detection
Out-of-Distribution detection is the task of separating in-distribution data from out-of-distribution
(OOD) data. For example, a classifier, trained on a training distribution Ptr(X, Y ), is tested on a dif-
ferent test joint distribution Ptst (X, Y ). When the test joint distribution Ptst (X, Y ) 6= Ptr(X, Y ),
data sampled from Ptst (X, Y ) are considered OOD data. Ptst (X, Y ) can be different in two ways,
i.e., covariate shift and concept shift (Sec. 3.2). Data sampled from the same distribution as train-
ing, Ptr (X, Y ), such as a conventional validation/test split, are in-distribution (ID) data. To detect
OOD data, a score function is used such as maximum softmax probability (Hendrycks & Gimpel,
2016) and energy (Liu et al., 2020b). The score function assigns higher value to OOD data than ID
data. Consequently, OOD data can be detected by setting a suitable threshold. The commonly used
benchmark, Area Under the Receiver Operating Characteristic curve (AUROC), plots true positive
rate of ID data against false positive rate of OOD data by sweeping through a range of thresholds,
and is a threshold-independent metric for measuring OOD detection performance.
6.2	Introduction to Confidence Calibration
Colloquially, confidence calibration refers to aligning the maximum predicted probability (confi-
dence) from a multi-class classifier to the empirical accuracy of the predicted class to be the ground
truth. For example, if a classifier classifies 100 images to dog with a confidence of 0.8 for each
prediction, than 80 out of 100 those images should contain a dog. Formally, we adopt the definition
from Kull et al. (2019) to define confidence calibration. Let P : X → ∆x be a probabilistic multi-
class classifier for M classes. For any input x ∈ X, P(x) = (P1 (x), ..., PM (x)) is a probability
vector.
Definition 1 A probabilistic classifier P : X → ∆x is confidence-calibrated, if for any p ∈ [0, 1]
P (Y = arg max P(x)| max P(x) = p) = p
One notion of confidence calibration is the expected difference between the confidence and accuracy.
EP ∣P(Y = arg max P(x) | max P(x) = P) — P
Expected Calibration Error (ECE) (Naeini et al., 2015) (Guo et al., 2017) is a metric that approxi-
mates this expectation. ECE partitions predictions into several M equally-sized bins according to
the predicted confidence, and then calculates average accuracy and confidence for each bin. Let y
and P be the predicted class and confidence of an input X with corresponding label y. Bn denotes
the n-th bin. The accuracy and confidence are calculated for each bin as follows:
acc(Bn)
表 Xn l(yi= yi)
(9)
conf(Bn) = -B-- X Pi
|Bn | i∈Bn
(10)
Than ECE is defined as a weighted sum of the difference between confidence and accuracy in each
bin. Therefore, lower ECE indicates better calibration.
N |B |
ECE =〉: N IaCC(Bn)- Conf(Bn)I
n=1
6.3	Geometric Sensitivity Decomposition Review
Geometric sensitivity decomposition (GSD) is proposed in Tian et al. (2021) to improve the sen-
sitivity of neural networks to input changes. Specifically, GSD views the output of the last linear
layer, i.e., logits, in a softmax-linear model as inner products, between the feature and weights in
13
the linear layer, , l ∈ RM =< f , W >. The inner products can be written as products of norms
and cosine similarity, e.g., the ith logit is l* = ∣∣f*k2∣∣wi∣∣2 Cosφ↑. GSD proposes to decompose
norms, ∣∣f*k2, and angles, φ*, into two components: an instance-dependent residual component and
an instance-independent scalar as shown in Eq. 11. The instance-independent scalar acts as freely
optimizable parameters during training to minimize training loss.
[kf*k2 = kf∣∣2 + Cf
[φ" = lφil - lCΦl
(11)
Decomposed components in Eq. 11 can be plugged into equation for logits.
kf *k2 Cos φ: = kf *k2 cos I。" = (kfk2 + Cf )cos(∣φi ∣-∣Cφ∣)
(12)
/ /
1	. ∣Z, ∣2	1 Cos lCΦl Sin |弧|
1-sin lCφl I1 -、sm 或Cos 附
∖	∖	E.q. 13
Eq. 12 can be simplified by assuming that the angle ∣φ*∣ is small. This equivalent assumes that
training data are closely clustered to linear classifiers specified by the linear weight W.
Cos ∣Cφ∣ sin ∣φi∣ _	sin(∣φi∣	+ ∣Cφ∣)	+ sin ∣φi∣
=~Z =	=	= —	7^∣ ：	： ~^^ ^	：	≈	<≈	1
sin ∣Cφ∣ Cos ∣φi∣	sin(∣φi∣	+ ∣Cφ∣)	- sin ∣φy ∣
(13)
Therefore, logits can be written as the following.
kf"k2kwik2Cosφi"≈
z	q
K kfk2 + 许 f
∖l^{z^}	l-{z^}
αα
kwik2 Cos φi
(14)
(Mk2+ Cf"…
Because Cos Cφ and Cf are instance-independent, we can parametrize them separately from the main
network as α and β respectively.
6.4	Intra Order Preserving Functions Review
Rahimi et al. (2020) formalizes a family of powerful calibration function: intra order preserving
functions. Many familiar functions are in this family such as the softmax function and temperature
scaling (Guo et al., 2017). Formally, a function f : Rn → Rn is intra order-preserving if for
any x ∈ Rn, both x and f(x) share share the same ranking. The following theorem from Rahimi
et al. (2020) provides sufficient and necessary conditions for constructing an intra order preserving
function.
Theorem 1 A continuous function f : Rn → Rn is intra order-preserving, if and only if f(x) =
S (x)-1 U w(x) with U being an upper-triangular matrix of ones and w : Rn → Rn being a
continuous function such that
•	wi (x) = 0 ifyi = yi-1 and i < n,
•	wi (x) > 0 ifyi > yi1 and i < n,
•	wn (x) is arbitrary
where y = S(x)x is the sorted version (descending, i.e., y1 ≥ y2) of x and S(x) : Rn → Rn is a
sorting matrix.
The task of constructing an intra order preserving function comes down to designing the function
w(x). A specific form of w(x) is proposed in Rahimi et al. (2020): wi(x) = σ(yi - yi-1)mi(x).
14
As long as σ is a positive function, i.e., σ(x) = 0 when x = 0 and σ(x) > 0 otherwise, and
m(x) > 0 is a strictly positive function, the contructed function will satisfy Theorem 1.
Our construction of α and β satisfies the necessary conditions. For calibration, the function
f acts on the logits l so we use l instead of x in subsequent proof. Let li = kfk2kwik2 cos φi
denote the sorted12 ith logit and Zi = (α(l)l∣f k2 + α(∣)^ IlWik2 cos Φi detnote the actual output ith
logit (sorted) with α and β . The goal is to show that l and l share the same ranking by satisfying
Theorem 1. We start by extracting the m(x) function in Eq. 15.
_______Ii______
Ii=(Oi)kfk2+ α(ι))kwik2Cosφi= (α⅛ + α(w∣κ)而前W启嬴	(15)
、--------{z-------}
m(l)
Therefore, we can construct the W(x) function by taking the difference between two adjacent logits
for i < n.
σ≥0
Wi(l)
、Wi(l)
li - li
-1
α(l
li
β (l)
(l)kfk
m(l)>0
(li - li
-1
),
for i < n
for i = n
(16)


1
+
α
|
z
2
}
where li ≥ li-1 .
Because 0 < α(l) < 1 andβ (l) > 0, the function m(l) is strictly positive and thus satisfies Theorem
1 in Rahimi et al. (2020). Specifically, m(l) corresponds to the strictly positive function m(x) and
li - li-1 corresponds to the positive function σ(yi - yi-1 ), due to the sorting in descending order.
6.5	Implementation details
Out-of-Distribution Detection Following prior works (Liu et al., 2020a; Van Amersfoort et al.,
2020; Mukhoti et al., 2021), we use Wide-ResNet-28-10 (Mukhoti et al., 2021) for all OOD detection
experiments. We train the model using SGD and cross entropy loss with an initial learning rate of
0.1 for 200 epochs. The SGD optimizer is configed with 5.0e - 4 weight decay and 0.9 momentum.
Batch size is 128. The learning rate is annealed with a cosine scheduler (Loshchilov & Hutter, 2016).
We adopt the official code13 for the implementation of DDU (Mukhoti et al., 2021). DDU (Mukhoti
et al., 2021) and Mahanobis (Lee et al., 2018) distance use density estimation by fitting a Gaussian
mixture model (GMM) to the learned feature space. We use the official implementation of DDU to
fit a GMM to the feature space (feature from the CNN). For Manhanobis distance, we fit a GMM to
the low dimensional logit space instead for computational stability.
Calibration The backbone training follows the same procedure as the previous section. During the
calibration/tuning stage, we train all models for 20 epochs with cosine learning rate decay using
SGD and cross entropy loss. In this stage, we freeze the backbone models and only tune the cal-
ibration parameters/functions following (Guo et al., 2017). In our case, only α and β are tuned.
For calibration on out-of-distribution data, the models are tuned on the entire validation set. For
calibration on in-distribution data, we use 5-fold cross validation using the validation set following
prior work (Rahimi et al., 2020). For matrix scaling Guo et al. (2017) and Dirichlet scaling (Kull
et al., 2019), we use the Off-Diagonal and Intercept Regularisation (ODIR) (Kull et al., 2019) with a
default scaling of 1 × 10-7. Note that we use the same initial learning rate, 0.1, for all methods ex-
cept for Dirichlet scaling. Dirichlet scaling requires smaller learning rate and we tuned the learning
rate on different datasets. For CIFAR10, we use 0.01 and for CIFAR100 we use 0.005.
6.6	CIFAR 1 00 Concept Shift Splits
While it is natural to associate covariate shift with increasing degrees of image corruption, finding a
dataset to benchmark gradual concept shift is not straightforward because concept shift is tradition-
ally thought as binary: overlapping or non-overlapping. However, not all non-overlapping labels
12The original order can be restored by applying S(l)-1 to l.
13https://github.com/omegafragger/DDU
15
1		2	3	4	5	6	7	8	9	10	AVE.	STD.
CIFAR10	airplane	automobile	bird	cat	deer	dog	frog	horse	shiP	truck		
Group 9	cattle	shrew	motorcycle	squirrel	snake	trout	sea	tractor	bus	PickuP	24.96	2.39
Group 8	bear	elephant	leopard	camel	lizard	rabbit	beaver	sPider	raccoon	orchid	21.99	0.44
Group 7	lion	mountain	crab	bicycle	turtle	beetle	train	mouse	snail	otter	20.18	1.14
Group 6	possum	shark	forest	pine	dinosaur	boy	PorcuPine	wolf	road	butterfly	17.79	0.32
Group 5	girl	rocket	man	tiger	bee	tank	whale	baby	kangaroo	dolPhin	16.26	0.44
Group 4	willow	worm	chimpanzee	skunk	cup	mushroom	oak	cockroach	crocodile	hamster	14.64	0.55
Group 3	castle	can	bridge	lobster	house	bed	fox	maPle	Pear	woman	12.65	0.63
Group 2	palm	streetcar	pepper	keyboard	bottle	seal	rose	couch	caterPillar	goldfish	10.18	0.51
Group 1	flatfish	apple	orange	plate	table	tulip	bowl	television	skyscraPer	ray	8.95	0.25
Group 0	wardrobe	lamp	plain	lawnmower	chair	PoPPy	clock	cloud	sunflower	telePhone	7.5	0.97
Table 4: CIFAR100 Concept Shift Splits Small group numbers indicate less conceptual similarity
to CIFAR10 classes. The similarity is calculated using inner product between the Glove embeddings
of a CIFAR100 class and a CIFAR10 class. For each CIFAR100 class, the largest similarity to each
CIFAR10 class is taken as the overall similarity to CIFAR10. The average shows average similarity
to CIFAR10 and the standard deviation shows in-group variance.
		AUROCt	TNROTPR95↑ 1	2	3	4	5	II	2	3	4	5	
g(x)	vanilla α(x)-only α(x)-β (X)	58.66±0.93	64.59±1.42	69.17±1.90	69.18±2.03	72.66±2.44 62.01 ±1.03	69.60±1.55	75.29±1.90	75.46±1.91	79.50±2.32 67.00±0.55	77.87±0.51	85.77±0.53	85.86±0.53	91.25±0.56	10.75±0.50	17.31 ±0.50	24.05±1.12	24.13±1.24	29.88±2.03 9.21±1.86	14.04±3.97	19.39±5.64	19.56±5.66	24.38±6.88 14.07±1.50	26.32±2.55	41.77±3.03	42.33±3.11	57.99±2.83
h(y, x)	vanilla α(x)-only α(x)-β (X)	61.02±0.44	69.80±0.74	76.43 ±0.79	76.47±0.81	81.39±0.90 63.98±0.88	74∙39±0.97	81∙58±0.91	81.66±0.93	86.67±0.85 59.11 ±0.27	68.43±0.78	76.19±0.85	76.16±0.85	82.20±0.73	9.98±0.70	16.84±1.15	24.70±1.53	24.61 ±1.78	32.13±2.08 10.52 ±1.07	18.18±220	27.69±3.68	27.65±3.96	36.79±5.43 10.22±0.23	17.72±0.85	27.28±1.32	27.19±1.26	36.91 ±1.55
U	vanilla α(x)-only α (X)-Ie(X)	60.66±0.51 ^^68.42±0.65^^74.32±0.90^^74.34±1.00^^78.72±1.21 64.20±0.86 74.05±1.02 81.16±0.98 81.29±0.99 86.18±0.97 66.86 ±0.47 77.92±0.45 85∙82±0.52 85.83±0.47 91.10±0.47	10.36±0.37^^17.60±0.67^^26.08±0.72^^25.97±1.10^^33.69±1.24 10.25 ±0.50	17.63±1.08	26.65 ±1.93	26.62±2.15	35.57±3.52 1332±1.13	25.21 ±2.12	40.67±3.13	40.74±2.65 56.48±2.62
Table 5: Capturing Covariate Shift (Motion Blur) All results are averaged over 5 runs.
are the same. For example, pickup truck (CIFAR100) is much closer to truck (CIFAR10) than sun-
flowers (CIFAR100) is semantically. To create this gradual concept/semantic shift, we propose to
divide the CIFAR100 dataset into 10 sub-datasets with increasing conceptual difference from CI-
FAR10 classes. Specially, we use Glove word embeddings (Pennington et al., 2014) wtih 6 billion
tokens trained on the entire wikipedia2014 and Gigaword5 (Napoles et al., 2012) to measure seman-
tic closeness (inner product) between CIFAR100 and CIFAR10 classes. The result is 10 subdatasets
split from CIFAR100 as shown in Tab. 4. Our experiments demonstrate that the conceptual differ-
ence measured by semantic similarity in language translates to a spectrum of near OOD datasets
measured by the difficulty of OOD detection in the image space.
6.7	Additional Results
We provide additional results and tabulated results of figures in the main paper. Specifically, Tab. 5
and Tab. 8 are tabulated versions of Fig .3 and Fig. 4 in the main paper respectively. In addition to
motion blur, we also provide out-of-distribution detection results on shot noise in Tab. 7 and zoom
blur in Tab. 6.
6.8	Extended Related Work
Out-of-Distribution (OOD) detection methods can be largely divided into two camps depending
on whether they require OOD data during training. Hendrycks et al. (2018) leverages anomalous
		1	2	AUROC↑ 3	4	5	1 2	TNR@TPR95t 3	4	5
	vanilla	61.28±1.13^^67.27±1.71	72.82±2.27	77.22±2.69	81.84±3.16	13.45±0.40	19.58±1.02	27.45±2.32	34.91±4.21	44.26±6.32
g(X)	α(x)-only	66.64±1.59	68.72±2.82	72.16±3.87	73.84±5.01	75.84±6.29	10.18±1.70	10.70±3.02	13.38±4.92	14.84±6.35	17.02±8.37
	α(x)-β(x)	71.21±5.89 77.72±2.77	81.78±6.68	85.53±7.40	89.74±7.36	20.96±5.50 26.17±8.08	35.67±11.38	43.35±14.53	54.86±18.97
	vanilla	66.19±0.78^^72.12±1.44	78.67±1.67	83.14±1.81	87.83±1.65	13.43 ±0.88^^18.11±1.57	26.31±2.84	34.37±4.00	45.88±4.64
h(y, X)	α(x)-only	69.34 ±1.63	75.49±2.40	81.93±2.61	86.37±2.55	90.65±2.28	15.69±1.34 22.16 ±2.80	32.81±4.52	42.91±6.14	55.99±7.39
	α(x)-β(x)	63.73 ±1.26 69.92±2.29	74.80±2.31	79.42±3.61	85.11±4.50	13.73 ±1.07	18.40±1.67	27.09±2.53	35.60±2.81	48.24±2.39
	vanilla	64.63±0.50^^70.94±0.88	77.26±1.12	81.87±1.45	86.64±1.54	13.95 ±0.50~19.35±1.18	28.27±2.01	36.98±3.00	48.82±3.69
U	α(x)-only	69.65±1.11	74.51±1.67	80.42±1.88	84.46±2.00	88.68±1.86	14.65 ±0.80 19.19±1.50	27.64±2.55	35.07±3.81	45.66±5.12
	α(x)-β(x)	71.25S5.14 78.00±1.74	82.23±5.71	86.14±6.44	90.50±6.41	19.93 ±3.05 25.84 ±4.08	36.81±5.72	46.54±7.28	60.70±8.87
Table 6: Capturing Covariate Shift (Zoom Blur) All results are averaged over 5 runs.
16
1
AUROC↑
234
TNR@TPR95f
34
5
5
1
2
g(x)
h(y, x)
vanilla
α(x)-only
α(x)-β (x)
vanilla
α(x)-only
α(x)-β (x)
59.84±1.81
61.88±2.75
64.78±0.99
65.04±2.43 73.34±3.47
67.05±3.65 75.05±4.64
73.21±1.84 87.01±3.05
75.85±3.97	79.53±4.80
77.35±4.66 80.49±4.49
90.63±2.99 94.55±2.42
13.08±2.86	18.59±5.03	28.40±11.06	32.46±14.24	38.75±20.04
8.48±1.75	10.27±2.85	12.98±5.77	13.82±7.16	15.42±9.38
11.34±1.60	18.58±4.55	44.31±12.91	55.61±14.72	71.07±14.96
68.14±1.72
69.88±0.79
68.62±0.86
75.80±2.32 85.92±3.13
78.30±0.93 89.39±1.04
76.47±1.00 87.26±1.01
88.20±3.24	90.97±3.44
91.79±1.14	94.49±1.23
90.00±1.40	93.42±1.96
vanilla	65.07±2.17	72.59±2.29	83.05±2.27
U	α(x)-only	67.81 ±1.22	75.91±1.38	87.19±1.61
α(x)-β(x)	67.23 ±0.55	76.13±0.98	89.12±2.02
85.62±2.34
89.81±1.71
92.19±2.12
88.95±2.84
92.96±1.74
95.49±1.89
16.93±1.80	24.75±3.08	40.55±6.21	45.58±7.59	52.06±10.06
18.64±1.12	28.91±1.36	52.12±3.39	59.56±4.80	69.62±7.44
17.41±0.52	26.48±1.73	46.80±4.93	54.26±7.29	65.65±11.14
16.32±1.11	23.96±1.95	39.06±3.77	44.28±5.13	51.05±7.28
15.07±0.82	22.39±1.84	40.17±5.87	47.07±7.81	57.90±11.05
14.94±0.97	24.27±3.25	51.27±9.38	61.50±11.07	75.45±11.63
Table 7:	Capturing Covariate Shift (Shot Noise) All results are averaged over 5 runs.
AUROC↑
g(x)
h(y, x)
	1	2		3	4	5	6	7	8	9	10
vanilla	75.88	79.64	80.35	81.43	80.23	81.48	79.83	80.03	78.99	83.37
α(x)	80.28	87.23	86.77	89.10	87.37	89.59	86.90	86.27	85.66	87.87
α(x)-β(x)	79.05	89.50	88.51	90.49	88.97	90.98	89.29	90.98	91.78	91.15
vanilla	86.78	90.74	91.00	90.20	89.85	91.10	91.84	91.85	93.90	94.09
α(x)	87.83	91.36	91.68	91.36	91.43	92.43	92.75	93.58	94.87	95.52
α(x)-β(x)	88.16	89.93	90.18	89.68	89.53	91.31	91.81	92.59	94.41	95.22
vanilla	84.02	88.52	88.45	88.59	87.52	89.09	89.18	88.96	91.13	92.26
U	a(x)	86.54	91.84 91.99 92.5 6 92.06	93.26 92.69 93.13	94.12 95.10
α(x)-β(x)	84.58	91.96	91.85	92.65	91.94	93.31	92.55	93.54	95.06	95.07
TNR9TPR95↑
1	2	3	4	5	6	7	8	9	10
33.12	38.52	41.94	41.38	40.42	44.20	39.34	42.18	38.72	47.06
29.34	38.42	37.14	44.28	36.90	44.40	37.32	38.80	32.22	43.00
31.20	42.18	41.02	49.14	40.16	50.76	42.94	52.00	49.16	51.32
46.16	52.50	52.58	51.66	48.58	52.64	53.38	58.10	61.60	63.28
47.46	53.78	55.46	55.90	53.44	61.04	58.38	65.60	68.92	72.54
48.74	51.66	53.74	53.88	52.86	59.32	57.70	64.56	68.10	72.54
46.78	52.96	54.38	53.38	50.28	54.02	53.88	58.04	60.02	63.40
46.68	54.48	56.30	57.82	53.92	62.20	57.88	62.86	64.56	70.08
44.72	54.62	55.10	58.94	52.76	63.54	57.72	66.42	69.20	70.02
Table 8:	Capturing Concept Shift (CIFAR100 Splits) All results are averaged over 5 runs.
data in training which enables classifiers to generalize and detect unseen OOD data. Thulasidasan
et al. (2020) extends existing classifier to include an OOD class. Roy et al. (2021) assigns multiple
classes for outliers instead of a single class. Our method belongs to the class of methods that do
not assume the availability of OOD data during training. Hendrycks & Gimpel (2016) discovers
that correctly classified examples have larger maximum softmax probability (MSP) and propose to
use it to detect incorrect predictions and OOD data. Lee et al. (2018) proposes to use Mahalanobis
distance by fitting a Gaussian mixture model (GMM) in the feature space. Mukhoti et al. (2021)
uses log density of the GMM model instead. Liu et al. (2020b) uses an energy score as the uncer-
tainty metric to distinguish between in-distribution and OOD data. ODIN (Liang et al., 2017) uses
a combination of input processing and post-training tuning to improve OOD detection performance.
Generalized ODIN Hsu et al. (2020) (also Techapanurak et al. (2019)) includes an additional net-
work in the last layer to improve OOD detection during training. There are many other interesting
OOD detection approaches that have achieved state-of-the-art performance without OOD data such
as using contrastive learning with various transformations (Winkens et al., 2020; Tack et al., 2020),
training a deep ensemble of multiple models (Lakshminarayanan et al., 2016) and leveraging large
pretrained models (Fort et al., 2021). They require extended training time, hyperparameter tuning
and careful selections of transformations, whereas our method does not introduce any hyperparam-
eters and has negligible influence on standard cross-entropy training time. For example, Winkens
et al. (2020) trains a constrastive model for 1200 epochs on CIFAR100 whereas our model requires
only 200 epochs (same as standard classifier training) on the same dataset.
Model Calibration methods can also be largely divided in two categories: 1) training time calibra-
tion using augmentations (Thulasidasan et al., 2019; Jang et al., 2021), using modified losses (Kumar
et al., 2018); 2) post-hoc calibration (Guo et al., 2017; Kull et al., 2019; Kumar et al., 2019; Rahimi
et al., 2020). Guo et al. (2017) proposes to calibrate the confidence of a trained classifier using tem-
perature scaling, a single scalar that softens overconfident softmax predictions. Kull et al. (2019)
extends single-class confidence calibration to multiclass calibration using the Dirichlet distribution.
Kumar et al. (2019) proposes a scaling-binning calibrator that is more sample efficient. Recently,
Rahimi et al. (2020) formally generalizes a family of expressive functions for calibration, the in-
tra order-preserving functions. This class of functions has more representation power to calibrate
more complex decision boundaries in neural networks. Our proposed method belongs to this class
of functions. However, all these works focus on calibration of in-distribution data. To obtain better
calibration on OOD data, on which the confidence of a model needs to decrease accordingly, sensi-
tivity and deterministic uncertainty modeling is explored by SNGP (Van Amersfoort et al., 2020)
and DUQ (Liu et al., 2020a). SNGP uses a bounded spectral normalization regularization (Miyato
et al., 2018) during training and DUQ adopts a two-sided gradient penalty (Gulrajani et al., 2017) to
improve model sensitivity to distribution shift. Our proposed model not only belongs to the family of
intra order-preserving functions, which ensures good in-distribution calibration, but also improves
sensitivity to distribution shift, which improves out-of-distribution calibration simultaneously.
17
a Feature Space 1
b Feature Space 2
c Histogram of Pixel Values
Figure 5: Fig. 5a and Fig. 5b show visualization of a 3-dimensional feature space of a model trained
on CIFAR10. The black cluster represents Gaussian-noise corrupted CIFAR10 data. Fig. 5c shows
histogram of pixels of CIFAR10, CIFAR100 and SVHN.
7 Discussion on Covariate and Concept Score Decomposition
In Sec. 3.2 in the main paper, two score functions are proposed to capture changes due to covariate
and concept shift. As shown in Eq. 17, g(x) denotes the covariate shift score function and h(y, x)
denotes the concept shift score function.
1 M	zg}∣{	1 M
U = max Ij - Mfli = Ilf l∣21 mjax kwjk2 cos φj - M EkWi l∣2 cos φi I (17)
i=1	i=1
h(y,x)
Conceptually, these two scores disentangle covariate shift and concept shift. However, in practice,
both shifts almost always happen at the same time in the image domain, i.e., covariate shifted data,
e.g., noised data, often result in concept shift, i.e., increasing ambiguity in class assignment. There-
fore, both scores can increase and decrease simultaneously as observed in Sec. 4.1.2 and Sec. 4.1.3.
The importance of separating them conceptually is to provide a clean perspective to study robust
out-of-distribution detection methods that will work well on different situations under either a single
distribution shift or a mixed of shifts. It very likely that better score functions can be derived to
better disentangle the effects of distribution shifts. In turn, methods that improve sensitivity of those
new score functions can be motivated.
8 Additional Figures
While it seems impossible that concept shift could happen without change in P (x), this is possible
if we define the support,P (x), as the distribution of pixels, ie., the lowest level characteristics,
completely stripped of any semantic information. A simple example would be reshuffling the pixels
of a CIFAR10 image where the semantics, P(y|x) , is completely changed, while P(x) remains
the same. Using CIFAR100 to represent concept shift is based on this reasoning. We provide
a visualization of pixel distributions of CIFAR10, CIFAR100 and SVHN in Fig. 5c. CIFAR10
and CIFAR100 share very similar pixel distributions while the pixel distributions of SVHN are
very different from those of CIFAR10. The similarity in pixel distributions between CIFAR10 and
CIFAR100 could be attributed to the data generation/collection process.
To further support the statement that norms are sensitive to covariate shift and justify the choice of
defining the covariate shift score g(x) = ||x||2, we show a visualization of CIFAR10 classes in the
feature space of a model trained on CIFAR10 in Fig. 5a and 5b. Similar to Liu et al. (2018), we
change the last feature dimension of a ResNet to 3. In this way, we can directly visualize the learned
feature space without resorting to dimension reduction techniques such as T-SNE (Wattenberg et al.,
2016). In addition to the clean validation data from CIFAR10, we also visualize CIFAR10 data
corrupted by Gaussian noise. The noised data represents severe covariate shift. We can observe that
the noised data are clustered around the origin indicating very small norms.
18