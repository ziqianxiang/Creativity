Under review as a conference paper at ICLR 2022
Provab le Learning of Convolutional Neural
Networks with Data Driven Features
Anonymous authors
Paper under double-blind review
Ab stract
Convolutional networks (CNN) are computationally hard to learn. In practice,
however, CNNs are learned successfully on natural image data. In this work,
we study a semi-supervised algorithm, that learns a linear classifier over data-
dependent features which were obtained from unlabeled data. We show that the
algorithm provably learns CNNs, under some natural distributional assumptions.
Specifically, it efficiently learns CNNs, assuming the distribution of patches in the
input images has low-dimensional structure (e.g., when the patches are sampled
from a low-dimensional manifold). We complement our result with a lower bound,
showing that the dependence of our algorithm on the dimension of the patch
distribution is essentially optimal.
1 Introduction
Convolutional Neural Networks (CNNs) have become a primary tool for machine learning on image
data, surpassing the performance of “traditional” machine learning algorithms such as linear classifiers
or kernel-SVM. On the other hand, unlike such simpler classifiers, neural networks are known to be
computationally hard to learn. Namely, it has been shown that, under mild cryptographic assumptions,
there is no efficient algorithm that can find, under any distribution, a neural network with good, or
even non-trivial, test performance. This is true even if we assume that there exists a neural network
that perfectly labels the distribution (Livni et al., 2014).
How can we explain this gap between the practical success and the theoretical hardness of neural
networks? One possible answer is that we should introduce distributional assumptions into the
analysis of learning algorithms. That is, while in theory we would like our algorithms to perform
well under any possible distribution of examples, in practice “natural” distributions appear to enjoy a
certain structure that traditional theoretical analysis does not take into account. Therefore, it seems
that in order to give a theoretical analysis that is relevant to practice, we must find assumptions on the
underlying data distributions that are both a) realistic for natural data and b) allow us to guarantee
efficient learnability of neural networks.
In this work, we explore such possible distributional assumptions. Specifically, we study learning
convolutional functions over data with a structure that is similar to natural images. Our main assump-
tion is that the distribution of “important” patches in the images has a low-dimensional structure
(e.g., lies on a low-dimensional manifold). Under this assumption, we analyze a semi-supervised
algorithm that first constructs a representation based on an unlabeled set of examples, and then learns
a linear classifier over the produced representation. Note that this is an improper learning algorithm
for learning CNNs. Our algorithm is very similar to the semi-supervised algorithm introduced by
Coates et al. (2011) which empirically has performance which is close to the performance of neural
networks on several image datasets (Thiry et al., 2021). We show that our algorithm has run-time and
sample complexity that depend on the covering number of the space of patches, and therefore the
algorithm is efficient when the patches have low-dimensional structure. We complement this result
with a lower bound, showing that such dependence on the covering number is essentially optimal,
i.e., any learning algorithm must depend on this number in the same way that our algorithm does.
1
Under review as a conference paper at ICLR 2022
2	Related Work
Learning with Data-Dependent Representation Learning linear classifiers over fixed represen-
taions of the input is a very well-known and well-studied approach. Specifically, kernel methods
(Shawe-Taylor et al., 2004) and random features (Rahimi et al., 2007; Rahimi & Recht, 2008) are
some prominent examples of such learning methods. However, these methods are known to have
their limitations, due to the fact that the representation is fixed a priori, and cannot adapt to the
learning task (see for example Kamath et al. (2020); Daniely & Malach (2020); Malach et al. (2021)).
Therefore, using representations that depend on the properties of the input data instead of fixing the
representation in advance seems like a sensible way to improve performance. Indeed, such data-
dependent representations have been explored for learning over image data. Specifically, the work of
Coates et al. (2011) studies a learning algorithm for image data, which constructs a data-dependent
representation in a very similar fashion to the algorithm we analyze in this work. However, Coates
et al. (2011) focuses on empirical evaluation of the algorithm, while our work gives a theoretical
analysis of the algorithm. A more recent work by Thiry et al. (2021) suggests a similar patch-based
representation, and shows that it achieves impressive empirical performance on complex datasets
such as CIFAR-10 and ImageNet. They do not perform theoretical analysis. Another related line of
works study kernels constructed via random neural networks, such as the Neural Tangent Kernel (Du
et al., 2019; 2018b; Arora et al., 2019; Ji & Telgarsky, 2019; Cao & Gu, 2019; Jacot et al., 2018).
Learning Neural Networks under Distributional Assumptions As mentioned earlier, efficient
learning of neural networks without any distributional assumptions is known to be hard (e.g., Klivans
& Sherstov (2009); Livni et al. (2014)). In fact, in some cases learning neural networks under simple
distributions such as Gaussian or Uniform distributions was shown to be hard for a large family of
algorithms (see Diakonikolas et al. (2020)). This fact motivates finding the “right” distributional
assumptions, that are both realistic (in some sense), and allow efficient learning of neural networks.
To this end, various works have studied learning of feed-forward networks (Ge et al. (2018); Awasthi
et al. (2021)) and convolutional networks under different distributional assumptions (Brutzkus &
Globerson (2017); Oymak & Soltanolkotabi (2018); Du & Goel (2018); Malach & Shalev-Shwartz
(2018); Brutzkus & Globerson (2020); Du et al. (2018a)). However, these distributional assumptions
often seem far from fitting natural data distributions. Our work gives a simple characterization of data
distributions that both seems reasonable for natural data, and allows efficient learning of convolutional
networks.
3	Preliminaries
We begin by describing the problem setting considered in the paper. First, we introduce our assump-
tions on the data distribution and the labeling function. We then describe the learning algorithm that
we analyze.
3.1	Data Generating Distribution
We consider distributions over images, where each image is of size dI . Let X ⊆ RdI be the input
space and Y = {±1} be the label space. Each image contains n patches, i.e. n sub-images, where
each sub-image is of size dP. We identify each of the n patches in the image with a sequence of
indices. That is, we define n sequences A1, ..., An such that for every j, Aj = (i1, . . . , idP ) is a
sequence of indices satisfying i1, . . . , idP ⊆ [dI]. So, Aj defines the set of indices of image pixels
associated with the j-th patch, and we denote the j-th patch by x[j] s.t.:
x[j] = xAj(1), . . . , xAj (dP)
Note that we do not assume that A1, . . . , An are disjoint, so the same pixel can belong to multiple
patches. We now introduce our assumption on the function which labels the input distribution.
Consider the following generalization of convolutional networks:
Definition 3.1 (Generalizd Convolutional Function). Let f : RdP → Rk be some function. We say
that F * : X → R is a Generalized ConvolUtional FUnction (GCF) over f if
n
F*(x) =X u(i), f (x[i])	(1)
i=1
2
Under review as a conference paper at ICLR 2022
where u(1) , . . . , u(n) ∈ Rk.
Observe that a GCF is a generalization of a CNN in two aspects. First, we allow the function f to be
any arbitrary function over patches, and we do not restrict it only to be a neural network. Second, we
allow the patch indices to be arbitrary (as discussed above), which allows implementing overlapping
or non-overlapping convolutions, dilated convolutions and also arbitrary choice of “patches” from the
image. Our main result shows an algorithm that can learn GCFs (under appropriate assumptions),
and therefore can be applied to standard CNNs, as well as a broader family of functions.
Let I be some distribution over X × Y, i.e. - a distribution of labeled examples. Given our definition
of GCFs, we make the following assumption on I:
Assumption 3.2. There exists a GCF F * : X → R over Somefunction f : RdP → Rk such that:
•	For any (x, y)〜I, yF* (x) ≥ L
•	f is L-Lipschitz, for some Lipschitz constant L > 0.
•	There exists some M > 0 such that kf (u)k ≤ Mfor every u ∈ RdP .
•	There exists some constant B > 0 such that the weights ofF* satisfy Pin=1 u(i) ≤ B.
Simply put, we assume that the distribution I is realizable (with a margin) by a GCF over a Lipschitz
and bounded function.
3.1.1	S tandard CNN Architectures
We now show that a distribution labeled by a standard CNN architecture, with a Lipschitz and
bounded activation function, satisfies Assumption 3.2. Since a CNN is an instance of a GCF, all we
need to show is a bound on the Lipschitz constant L and a bound on the norm M of the relevant
function f. We show this for both deep and shallow (one hidden-layer) CNNs.
Let σ be a 1-Lipschitz activation function. Then, if f(x) = σ (W x), for some W ∈ Rq×P, a GCF F
over f is simply a one hidden-layer CNN. In this case, we have the following result:
Theorem 3.3. Let Q be a 1 -Lipschitz activation s.t. SuPx ∣σ(x)∣ ≤ c. Then, any distribution over
X × Y labeled by a one hidden-layer CNN F(x) = in=1 u(i), σ(Wx[i]) with activation σ and
weights W ∈ Rk×dP; u(1),..., u(n) ∈ Rk satisfies Assumption 3.2 with L = ∣∣ W|卜 and M = c√k.
The above follows immediately fromt the following Lemma:
Lemma 3.4. Let σ : R → R be a 1-Lipschitz activation function. Let f : RdP → Rk be a function
such that f(x) = σ(Wx),for W ∈ Rk×dP. Then, f is ∣W∣2-Lipschitz.
Similarly to the above, if we let f be a deep neural network with activation σ, namely f(x) =
σ(W (t)σ(. . . σ(W(1)x) . . . ), then a GCF F over f is equivalent to a depth t CNN 1. In this case, we
have the following result:
Theorem 3.5. Let σ be a 1 -Lipschitz activation s.t. SuPx ∣σ(x)∣ ≤ c. Then, any distribution over
X × Y labeled by a dpeth t CNN network
n
F(x) =X u(i),σ(W(t)σ(...σ(W(1)x)...)
i=1
with activation σ and weights W ⑴ ∈ Rk×dP, W (2),...,W(t) ∈ Rk×k; u(1),..., u(n) ∈ Rk
satisfies Assumption 3.2 with L = QillW(i)b and M = c√k.
This follows immediately from the following Lemma:
Lemma 3.6. Let f = σ ◦ W(t) ◦
σ ◦•••◦ σ ◦ W ⑴,forsome 1 -Lipschitz activation function σ. Then,
f is Qi 1 llW (i) ll2-Lipschitz
1Formally, F will be a depth t CNN if W(1) , . . . , W(t) are convolutions. However, since convolutions are
linear operations, our definition describes a larger family of functions.
3
Under review as a conference paper at ICLR 2022
Note that the bound on the Lipshchitz constant of deep CNNs in Theorem 3.5 grows as a product of
the norm of the weights. That is, if the weight matrices have norm > 1, then the Lipschitz constant
grows exponentially with depth, which may be unsatisfactory for deep networks. However, we
do note that this is only an upper bound on the Lipschitz constant of the network, and in fact we
can expect that deep networks in practice have much smaller Lipschitz constatns (see Combettes &
Pesquet (2020); Jordan & Dimakis (2020)).
3.2	The Covering Number of Patch Distributions
Let I be a distribution over XXY that satisfies Assumption 3.2 with some GCF F * over f. We
assume that the marginal distribution ofI on X has a density function and we denote its support by
supp(I). We denote by PI the set of supported patches in the distribution I which have non-zero
value under f. Namely, PI = {x[i] | x ∈ supp(I), f (x[i]) 6= 0, 1 ≤ i ≤ n}. In cases where the
distribution I is clear from the context, we simply use P := PI. So, P is the set of patches that both
have non-zero probability to appear in images sampled from I, and which are not “ignored” by the
GCF. This captures the fact that for image classification tasks, some patches of the image may be
very important for determining the object class, while other patches can simply be ignored. The set
P is therefore exactly this set of “important” patches.
The sample complexity and the run-time of our algorithm depend on one important factor - the
covering number of the set P. In general, an r-covering number is defined as follows:
Definition 3.7. For a set A, we say that C is an r-covering of A if A ⊆ ∪v∈C Br (v), where Br (v)
is the `2 ball of radius r centered around v. The r-covering number of a set A, denoted by Nr (A), is
the minimal size of an r-covering of A (Vershynin, 2018).
So, the main ingredient in our analysis is showing that if the covering number of the set P is small
(for example, if the important patches lie on a low-dimensional manifold, as described in Section
4.2), then our algorithm can efficiently learn GCFs, and therefore CNNs. Notice that assuming that
Nr(P) is small is a distributional assumption, and it does not necessarily hold in general. In fact, in
some cases the covering number of P can be exponential in the patch size dP, or even unbounded
(if the distribution of patches is unbounded), in which case our bounds become vacuous. That said,
we show in Section 4.3 that in general, such dependence on the covering number is unavoidable for
learning GCFs.
3.3	Learning Algorithm
We now describe the learning algorithm analyzed in this work. The algorithm has two stages. In the
first stage, which is unsupervised, a dictionary of patches is learned by clustering patches from an
unlabeled set of examples. In the second stage, which is supervised, a linear classifier is learned over
a feature-map generated using the patch dictionary. We begin by describing the patch embedding
feature-map, and continue with a detailed description of the learning algorithm.
Patch embedding. Let D = {vι,…，vι} ⊆ RdP be a set of patches. We refer to D as a dictionary.
We next describe how to use a dictionary to obtain a representation of a given image x.
For a distance parameter τ > 0, we define the patch-embedding φ(∙; D, T) : RdP → {0,1}* l, where
for every 1 ≤ i ≤ l:
1 kvi - uk ≤ τ
φ(u; D, τ)i = 0 otherwise
(2)
Namely, φ maps a given patch U to a vector indicating all the patches in the dictionary D that are
τ-close to u. We use a normalized patch embedding φ defined as:
φ(u; D,τ)=
φ(u; D,τ)
kφ(U；D,T )kι
Using the embedding φ, we define an input-embedding Φ(∙; D,τ) : X → {0,1}l'n
Φ(x; D,τ) = [φ (x[1]; D,τ) ,...,φ (x[n]; D,τ)]
(3)
4
Under review as a conference paper at ICLR 2022
D ={Vι , V2 ,…	vn}
机41)= ∣1∕2∣ 0	1∕2∣ 0 I 0 I 0 I
1/3	1/3	0	0	1/3	0
Φ(%)=
0	1∕5	1∕5	1∕5	1∕5	1∕5
%
0
1∕2
1/5
Figure 1:	Construction of the patch embedding representation. First, each patch x[i] is mapped to a
normalized indicator vector, indicating all its neighbours in a ball of radius τ within the dictionary D.
Then the representations of all the patches are concatenated together to obtain a representation for the
full image.
Algorithm 1 Clustering
Input: Set of patches Pu, N > 0.
Pick an arbitrary u ∈ Pu .
Set D = {u}.
for i = 2, ..., N do:
Find v ∈ Pu which maximizes minu∈D kv - uk.
D J D ∪{v}
return D.
Figure 2:	Patch clustering algorithm.
That is, Φ maps each patch of X using the patch embedding φ and concatenates all patch embeddings.
Figure 1 shows an illustraction of how the embedding Φ is constructed using the dictionary D and
the input image X.
Learning algorithm. We consider a semi-supervised algorithm Apatch for learning from image
data, which is similar to the algorithm presented by Coates et al. (2011). The algorithm takes two
parameters: the size of the dictionary N, and the patch embedding radius τ . The algorithm consists
of an unsupervised stage, followed by a supervised stage.
Unsupervised stage: We assume that we have access to an unlabeled training dataset SU ⊆ SuPP(I).
Define the set PU = {x[i] | X ∈ Su, 1 ≤ i ≤ n}, i.e., the set of all patches of images in Su. We
perform a clustering procedure which given Pu and a number N, returns a set of patches D of size
N. For our theoretical analysis we will consider a greedy clustering algorithm, which performs
a farthest-first traversal. The clustering algorithm is described in Figure 1. This algorithm was
originally proposed for the k-center problem and it is a 2-approximation algorithm (Gonzalez, 1985).2
In practice, other clustering algorithms can be considered, such as k-means.
Supervised stage: Here we assume that we have a dictionary D = {vι,…,VN} of patches obtained
by the unsupervised stage. We assume that we are given a training set S = {(x1,y1),…，(xm, ym)}
of labeled points, sampled IID from the distribution I. Then, we perform a hard linear SVM on the
training set Se = {(Φ(xι; D, T), yι),…,(Φ(xm/; D, T), ym)}, and return a predictor W ∈ RN∙n s.t.
W = arg min ||w『 s.t. ∀i ∈ [m],yi(w, Φ(xi; D,τ))≥ 1	(Hard-SVM)
w
2See also Williamson & Shmoys (2011) Section 2.2.
5
Under review as a conference paper at ICLR 2022
The prediction of the label of a new point (x, y)〜I is sign(hw, Φ(x; D, T))).
4 Main Result
In this section we give a detailed analysis of the algorithm presented in the previous section. Our
main result shows that for every distribution I satisfying Assumption 3.2 with some fixed constants
L, M and B such that the covering number satisfies Nr(PI) ≤ N (with some proper choice of r),
given a large enough sample, our algorithm returns a predictor with small error.
To state this more formally, we show a learnability result, using the following notion of learnability:
Definition 4.1. Let P be a family of distributions over X × Y. We say that a learning algorithm A
E4C learns thefamily P, if for every e, δ ∈ (0,1 /4) ,there exists some m(e, δ) s.t. for every I ∈ D,
the algorithm A uses m(, δ) samples from I and returns a hypothesis h such that w.p. ≥ 1 - δ:
P(χ,y)〜I [h(x) = y] ≤ e
So, let PL,M,B,N,r be the family of distributions satisfying Assumption 3.2 with constants L, M, B
and for which Nr(PI) ≤ N. Then, we show that AP atch PAC learns the family PL,M,B,N,r, for any
r ≤ 1/(24LB). Note that this definition only addresses the supervised stage, and we will show it
holds under a condition on the unsupervised stage.
We start by bounding the sample complexity of our algorithm assuming that our unlabeled training
set (the data used for the unsupervised stage) is sufficient for covering the patch set P . Next, in
Section 4.1, we show that this assumption is essentially satisfied when our unlabeled training set is
large enough, and we give a bound on the required sample size of the labeled as well as the unlabeled
training data (without the additional assumption).
So, we start by making the following assumption:
Assumption 4.2. There exists some r ≤ 24LB, and some set C s.t. C is an r-covering of P of size
Nr(P), and for each v ∈ C, there exists u ∈ Pu, such that u ∈ Br (v).
That is, we assume that the unlabeled training set is representative in the sense that the patches in the
unlabeled set (denoted Pu) cover the “important” patches of the distribution (i.e., the set P). Under
this assumption (and the realizability assumption introduced in the previous section), we prove that
our algorithm PAC learns the family of distributions Pl,m,b,n,i∕(24lb), Via the following theorem:
Theorem 4.3. Fix δ ∈ (0, 1/4). Let I be some distribution which satisfies Assumption 3.2, and
assume that Su satisfies Assumption 4.2. Then, running algorithm AP atch with parameters N =
Nr(P) and τ = 6r returns a hypothesis h s.t. with probability at least 1 - δ:
g 「7 /、/ 】	InM2B2N	∕log( 1)
P(x,y)〜I [h(x) = y] ≤ Z —m- + αy
(4)
for some universal constant α > 0.
We give the full proof of the theorem in Appendix B, and give a sketch of the argument here. To
prove Theorem 4.3, we show that the function F * can be approximated by a linear classifier over
the representation Φ. Specifically, we use the fact that the set of patches from the unlabeled data Pu
covers the set P of patches with non-zero probability and non-zero value under f . So, after clustering,
we still get a good covering. Next, we define the following vector:
w^ = (〈u⑴，f(Vi)〉，…，(u⑴,f (vi)),…，(u(k), f (vi))，…，｛u(k), f (vn))) ∈ RNn
where the vi are the vectors in the dictionary D from the unsupervised stage. Then, using the
guarantees on the unsupervised stage (that the dictionary covers the set of patches), the function
F(X) = hw^, Φ(x; D, τ)i can be shown to approximate F*. Thus, the algorithm succeeds because it
can get from the unsupervised stage vectors that cover the set of patches, which in turn allows it to
produce a sufficiently rich representation for learning the distribution with a linear classifier.
The above result shows how the error of our algorithm decays with the (labeled) sample size, assuming
the unlabeled training set is “good”. This immediately gives a bound on the sample complexity of the
algorithm:
6
Under review as a conference paper at ICLR 2022
Corollary 4.4. Fix L,M,B,N > 0 and let r ≤ 24LB. Then, APatch With parameters N and
T = 6r, given access to a sample Su satisfying Assumption 4.2, EAC karns Pl,m,b,n,t With sample
complexity:
2 C 2 nM 2B2N + log(1∕δ)
m(e, δ) = α ------------5----------
2
In Section 4.3, we show that the dependence of the sample complexity on nNr(P) is optimal.
As for the run-time of our algorithm, notice that both the unsupervised and the supervised stage
use efficient (polynomial time) algorithms. The run-time of the clustering algorithm used in the
unsupervised stage is O(N2dP), and the hard SVM used in the supervised stage runs in time
O(mn2N 2), depending on the implementation.
4.1	Relaxing the Assumption on Unsupervised Learning
In this section we show a learning guarantee which does not require Assumption 4.2. We denote
mu = |Su|. So, we show that if mu is large enough, then Assumption 4.2 is essentially satisfied with
high probability:
Theorem 4.5. Let e0, δ, δ0 ∈ (0,1). Fix r ≤ 24LB and let N = Nr (P). Assume that mu ≥
N log (Nɔ and m ≤ α.Then, running algorithm APatch Withparameters N and T = 6r returns a
hypothesis h s.t. With probability at least 1 - δ0 - 2δ3:
P(x,y)〜I [h(X)= y] ≤ αj nM mB2N + (Iom) + J	(5)
for some universal constant α.
The proof of Theorem 4.5 is very similar to the proof of Theorem 4.3, except that now we do not
assume that we have an unlabeled set that covers the patch space. Rather, we prove that if we sample
enough unlabeled examples from the marginal distribution over the inputs, such “good” data is
guaranteed with high probability. This is done by observing all the balls from the covering of P that
have at least distributional mass, and showing that a sample from each of them is guaranteed with
high probability. The full proof of the theorem is given in Appendix B.
Using the above result, we can prove a learnability result, which does not depend on Assumption 4.2.
To do so, we use a more refined version of PAC learning, which distinguishes between labeled and
unlabeled sample complexity:
Definition 4.6. Let P be a family of distributions over X × Y. We say that a learning algorithm A
EAC IearnS the family P, iffor every e, δ ∈ (0,1/4), there exists some m(e, δ) and mu(e, δ) s.t. for
every I ∈ D, the algorithm A uses m(e, δ) labeled samples and mu(e, δ) unlabeled samples from I
and returns a hypothesis h such that w.p. ≥ 1 一 δ:
P(x,y)〜I [h(X) = y] ≤ e
Note that the two notions of PAC learnability (of Definition 4.1 and 4.6) are equivalent, as we can
always use some labeled examples and disregard their labels, or otherwise not use unlabeled data at
all. However, the later definition accounts separately for labeled and unlabeled sample complexity,
which often can be of interest. So, Theorem 4.5 implies the following learnability result:
Corollary 4.7. Fix L,M,B,N > 0 and let r ≤ 24LB. Then, APatch With parameters N and
τ = 6r, given access to a sample Su satisfying Assumption 4.2, EAC IearnS PLMBN,τ With sample
complexity:
m(e,δ)=4α2 nM2B2N + l°g(3∕δ), and mu(e,δ) = 6NIm^log (半
2	δ	δ
3Over the randomness of Su and S.
7
Under review as a conference paper at ICLR 2022
Table 1: Test classification accuracies on MNIST, Fashion MNIST (FMNIST) and noisy FMNIST.
Data set	DEEP CNN	Shallow CNN	Our algorithm
MNIST	99.1± 0.01	98.8± 0.04	97.7 ± 0.09
FMNIST	92.5± 0.1	90.6± 0.1	89.5 ± 0.1
NOISY FMNIST	-	-	73.1 ± 0.1
4.2	From Intrinsic Dimension to Covering Number
Our analysis, as shown in previous sections, shows that the sample complexity and run-time of our
algorithm depends on one important measure - the covering number of the patch set P . Observe that
the covering number is often used as a measure of the intrinsic dimension of some given space. For
example, the d-dimensional ball Bd has covering number Nr(Bd) = Cd(1/r)d, and note that this
remains true even if the ball is embedded in a space with larger extrinsic dimension. More generally,
a bounded d-dimensional manifold has a covering number that grows exponentially with the intrinsic
dimension d (which again can be much smaller than the extrinsic dimension). For more examples and
further discussion on the relation between the covering number and measures of intrinsic dimension
refer to Falconer (2004); Hamm & Steinwart (2020).
All in all, we see that if the distribution of “important” patches (captured by the set P) is concentrated
on a low-dimensional structure (e.g., on a low-dimensional manifold), we can expect the covering
number of P to be moderate. On the other hand, if the patches fill a truly high-dimensional space, then
our complexity guarantees becomes impractical, as these can grow exponentially with the dimension.
That said, we show in the next section that such dependence on the covering number is essentially
unavoidable, if one wishes to learn GCFs to small loss.
4.3	Sample Complexity Lower Bound
In the previous section, we showed that with sample complexity that depends on the number of
patches n and the covering number N := Nr(P), our algorithm returns with high probability a
hypothesis with small loss. Now, We will show that a dependence on n ∙ N is unavoidable for
guaranteeing such PAC learnability result. Namely, any algorithm that PAC learns all distribution
with covering number N, must have sample complexity of Ω(nN):
Theorem 4.8. Let A be some learning algorithm that uses m samples. Assume that for every I which
satisfies Assumption 3.2 and for which N1/L (PI) = N, A returns w.p. at least 7/8 a hypothesis h
s.t. P(χ,y)〜I [h(x) = y] ≤ 1/8. Then, m ≥ nN/2.
The proof of Theorem 4.8 follows the same arguments as standard sample complexity lower bounds,
e.g. the No-Free-Lunch Theorem (for example, see Section 5 in Shalev-Shwartz & Ben-David
(2014)). The full proof is given in Appendix B.
From the above result we can derive a lower bound on the sample complexity of any algorithm that
PAC learns PL,M,B,N,1/L :
Corollary 4.9. Fix some L, M, B, N > 0. Then, for any algorithm A that PAC learns PL,M,B,N,1/L,
it must hold that for all , δ ∈ (0, 1/8):
n n n nN
m(E,δ) ≥ ɪ
5 Experiments
In this section we perform experiments to show the efficacy of our method in practice and obtain
insights from our theoretical results. Coates et al. (2011) and Thiry et al. (2021) have performed
extensive experiments on algorithms similar to ours. They showed that the performance of their
algorithm is close to the performance of neural networks on challenging computer vision datasets. For
completeness, we perform experiments on MNIST (LeCun, 1998) and Fashion MNIST (FMNIST)
(Xiao et al., 2017). We compare the performance of our algorithm with a shallow CNN (2 layers) and
8
Under review as a conference paper at ICLR 2022
a deeper CNN (4 layers). We implement the clustering stage of our algorithm with k-means after
performing whitening of the patches, similar to Coates et al. (2011). We used the whole training set
for the unsupervised stage. See Section C for further details on the experiments. Table 1 shows the
results. We see that for both MNIST and FMNIST, the semi-supervised algorithm has comparable
performance to the shallow CNN and the deep CNN has the best performance.
Our theorems suggest that the sample complex-
ity of the semi-supervised algorithm depends on
the covering number of the patches. Covering
numbers are related to the number of clusters of
the data. If a dataset has a low covering number,
it should be possible to cluster the dataset with a
small number of clusters, and vice versa. Thus,
by our theoretical results, we should expect that
for data in which clustering algorithms work
better, our algorithm should have better test per-
formance. To check this, we plotted the mean
distance to center as a function of the number of
clusters for three datasets MNIST, FMNIST and
a noisy version of FMNIST. Note that all patches
are whitened and therefore the distances are rel-
atively comparable between datasets. This is
also illustrated by the fact that for a few clusters
the mean distances to the centers are roughly
the same for all datasets. The mean distance to
the center measures how well the data is clus-
IOOO 2000	3000	4000
Num clusters
-→- MNIST
FMNIST
T- Noisy FMNIST
Figure 3: Mean distance to center for various num-
ber of clusters.
tered for a given number of clusters. Figure
3 shows the results for three datasets. We see
that MNIST is the most well clustered while the
noisy FMNIST is the hardest to cluster. This
correlates with the test performances in Table 1, where on MNIST our algorithm has the best test
performance and on noisy FMNIST it has the worse. This is in line with our theoretical results, which
suggest that learning difficulty should be correlated with covering numbers.
6 Discussion and Future Work
In this work, we showed that although learning convolutional networks may be computationally hard
in the general case, introducing distributional assumptions allows us to learn a broad class of convolu-
tional functions efficiently. Specifically, we assumed that the “important” patches of the input have a
moderate covering number (i.e., the patches are well-clustered). Under this assumption, we proved
that our two step semi-supervised learning algorithm PAC learns the distribution. Following this, it is
also of interest to study the behavior of other learning algorithms under the low covering number
assumption. Specifically, understanding whether “vanilla” SGD on a standard CNN architecture can
learn such distributions efficiently is of great interest.
Furthermore, while our assumption on the patches covering number does seem to capture some
properties of natural distributions, it might still be too “generous”. Since the covering number can
grow exponentially with the extrinsic dimension of the patches, it might be the case that for complex
datasets this number is still too large to guarantee “efficient” learnability. So, a promising future
direction is to find distributional properties that lead to similar learnablility results, and yet are
expected to behave more moderately on natural data. Another possible direction for research is to
find distributional properties that use the specific structure of CNNs more explicitly. For example,
CNNs with ReLU activation induce a separation of the patch space to linear regions. So, we can use a
different sort of covering, where each linear region has at least one representative patch, such that all
neighbouring patches can be associated with this linear region. Finally, another direction for future
work is to explore similar distributional properties for learning other neural network architectures,
such as RNNs and transformers.
9
Under review as a conference paper at ICLR 2022
References
Sanjeev Arora, Simon Du, Wei Hu, Zhiyuan Li, and Ruosong Wang. Fine-grained analysis of
optimization and generalization for overparameterized two-layer neural networks. In International
Conference on Machine Learning, pp. 322-332, 2019.
Pranjal Awasthi, Alex Tang, and Aravindan Vijayaraghavan. Efficient algorithms for learning depth-2
neural networks with general relu activations. arXiv preprint arXiv:2107.10209, 2021.
Peter L Bartlett and Shahar Mendelson. Rademacher and gaussian complexities: Risk bounds and
structural results. Journal of Machine Learning Research, 3(Nov):463-482, 2002.
StePhane Boucheron, Olivier Bousquet, and Ggbor Lugosi. Theory of classification: A survey of
some recent advances. ESAIM: probability and statistics, 9:323-375, 2005.
Alon Brutzkus and Amir Globerson. Globally optimal gradient descent for a convnet with gaussian
inputs. In International Conference on Machine Learning, pp. 605-614, 2017.
Alon Brutzkus and Amir Globerson. An optimization and generalization analysis for max-pooling
networks. arXiv preprint arXiv:2002.09781, 2020.
Yuan Cao and Quanquan Gu. Generalization bounds of stochastic gradient descent for wide and deep
neural networks. In Advances in Neural Information Processing Systems, pp. 10836-10846, 2019.
Adam Coates, Andrew Ng, and Honglak Lee. An analysis of single-layer networks in unsupervised
feature learning. In Proceedings of the fourteenth international conference on artificial intelligence
and statistics, pp. 215-223. JMLR Workshop and Conference Proceedings, 2011.
Patrick L Combettes and Jean-Christophe Pesquet. Lipschitz certificates for layered network structures
driven by averaged activation operators. SIAM Journal on Mathematics of Data Science, 2(2):
529-557, 2020.
Amit Daniely and Eran Malach. Learning parities with neural networks. Advances in Neural
Information Processing Systems, 33, 2020.
Ilias Diakonikolas, Daniel M Kane, Vasilis Kontonis, and Nikos Zarifis. Algorithms and sq lower
bounds for pac learning one-hidden-layer relu networks. In Conference on Learning Theory, pp.
1514-1539. PMLR, 2020.
Simon Du, Jason Lee, Yuandong Tian, Aarti Singh, and Barnabas Poczos. Gradient descent learns
one-hidden-layer cnn: Don’t be afraid of spurious local minima. In International Conference on
Machine Learning, pp. 1339-1348. PMLR, 2018a.
Simon Du, Jason Lee, Haochuan Li, Liwei Wang, and Xiyu Zhai. Gradient descent finds global
minima of deep neural networks. In International Conference on Machine Learning, pp. 1675-
1685, 2019.
Simon S Du and Surbhi Goel. Improved learning of one-hidden-layer convolutional neural networks
with overlaps. arXiv preprint arXiv:1805.07798, 2018.
Simon S Du, Xiyu Zhai, Barnabas Poczos, and Aarti Singh. Gradient descent provably optimizes
over-parameterized neural networks. International Conference on Learning Representations,
2018b.
Kenneth Falconer. Fractal geometry: mathematical foundations and applications. John Wiley &
Sons, 2004.
Rong Ge, Rohith Kuditipudi, Zhize Li, and Xiang Wang. Learning two-layer neural networks with
symmetric inputs. arXiv preprint arXiv:1810.06793, 2018.
Teofilo F Gonzalez. Clustering to minimize the maximum intercluster distance. Theoretical computer
science, 38:293-306, 1985.
Thomas Hamm and Ingo Steinwart. Adaptive learning rates for support vector machines working on
data with low intrinsic dimension. arXiv preprint arXiv:2003.06202, 2020.
10
Under review as a conference paper at ICLR 2022
Arthur Jacot, Franck Gabriel, and Clement Hongler. Neural tangent kernel: Convergence and
generalization in neural networks. In Advances in neural information processing systems, pp.
8571-8580, 2018.
Ziwei Ji and Matus Telgarsky. Polylogarithmic width suffices for gradient descent to achieve
arbitrarily small test error with shallow relu networks. In International Conference on Learning
Representations, 2019.
Matt Jordan and Alexandros G Dimakis. Exactly computing the local lipschitz constant of relu
networks. arXiv preprint arXiv:2003.01219, 2020.
Pritish Kamath, Omar Montasser, and Nathan Srebro. Approximate is good enough: Probabilistic
variants of dimensional and margin complexity. In Conference on Learning Theory, pp. 2236-2262.
PMLR, 2020.
Adam R Klivans and Alexander A Sherstov. Cryptographic hardness for learning intersections of
halfspaces. Journal of Computer and System Sciences, 75(1):2-12, 2009.
Yann LeCun. The mnist database of handwritten digits. http://yann. lecun. com/exdb/mnist/, 1998.
Roi Livni, Shai Shalev-Shwartz, and Ohad Shamir. On the computational efficiency of training neural
networks. In Advances in Neural Information Processing Systems, pp. 855-863, 2014.
Eran Malach and Shai Shalev-Shwartz. A provably correct algorithm for deep learning that actually
works. arXiv preprint arXiv:1803.09522, 2018.
Eran Malach, Pritish Kamath, Emmanuel Abbe, and Nathan Srebro. Quantifying the benefit of using
differentiable learning over tangent kernels. arXiv preprint arXiv:2103.01210, 2021.
Samet Oymak and Mahdi Soltanolkotabi. End-to-end learning of a convolutional neural network via
deep tensor decomposition. arXiv preprint arXiv:1805.06523, 2018.
Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor
Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. Pytorch: An imperative style,
high-performance deep learning library. Advances in neural information processing systems, 32:
8026-8037, 2019.
Ali Rahimi and Benjamin Recht. Weighted sums of random kitchen sinks: replacing minimization
with randomization in learning. In Nips, pp. 1313-1320. Citeseer, 2008.
Ali Rahimi, Benjamin Recht, et al. Random features for large-scale kernel machines. In NIPS,
volume 3, pp. 5. Citeseer, 2007.
Shai Shalev-Shwartz and Shai Ben-David. Understanding machine learning: From theory to
algorithms. Cambridge university press, 2014.
John Shawe-Taylor, Nello Cristianini, et al. Kernel methods for pattern analysis. Cambridge
university press, 2004.
Louis Thiry, Michael Arbel, Eugene Belilovsky, and Edouard Oyallon. The unreasonable effectiveness
of patches in deep convolutional kernels methods. arXiv preprint arXiv:2101.07528, 2021.
Roman Vershynin. High-dimensional probability: An introduction with applications in data science,
volume 47. Cambridge university press, 2018.
David P Williamson and David B Shmoys. The design of approximation algorithms. Cambridge
university press, 2011.
Han Xiao, Kashif Rasul, and Roland Vollgraf. Fashion-mnist: a novel image dataset for benchmarking
machine learning algorithms. arXiv preprint arXiv:1708.07747, 2017.
Matthew D Zeiler. Adadelta: an adaptive learning rate method. arXiv preprint arXiv:1212.5701,
2012.
11
Under review as a conference paper at ICLR 2022
A Proofs of Section 3.1.1
Proof of Lemma 3.4. let x1, x2 ∈ RdP, and we have:
kf(x1)-f(x2)k2=X(σ(Wx1)i-σ(Wx2)i)2
i
≤ X(Wx1 - Wx2)i2 = kW (x1 -x2)k2 ≤ kW k2 kx1 -x2k2
i
□
Proof of Lemma 3.6. Observe that if g1 is an L1-Lipschitz function, and g2 is an L2-Lipschitz
function, then g1 ◦ g2 is L1L2-Lipschitz:
k(g1 ◦g2)(x1) - (g1 ◦ g2)(x2)k ≤ L1 kg2(x1) - g2(x2)k ≤ L1L2 kx1 - x2k
Now, using Lemma 3.4 gives the required.	□
B	Proof of Section 4
Proof of Theorem 4.3. By Assumption 4.2, there exists a set A ⊆ Pu of size Nr(P) such that:
P ⊆ ∪v∈C Br (v) ⊆ ∪v∈AB2r (v)	(6)
The clustering algorithm guarantees that (Gonzalez, 1985),
max min kv - uk ≤ 2 max min kv - uk ≤ 4r	(7)
v∈Pu u∈D	v∈Pu u∈A
where the last inequality follows by Eq. (6). It follows that Pu ⊆ ∪v∈DB4r(v) which implies that
∪v∈Pu B2r (v) ⊆ ∪v∈DB6r(v). Then, by Eq. (6) wegetP ⊆ ∪v∈Pu B2r (v) ⊆ ∪v∈DB6r(v).
Now, define the vectors w = (f(v1), ..., f(vN)) ∈ RNk and,
W=(Du(1),f(vi)E ,…，DU⑴，f(vi)E,…，Du(k),/(vi)e ,…，(u(k) ,f (vN)))∈ RNn
i.e., W(i-i)N+j =〈u⑺,f(vj )〉for all 1 ≤ i ≤ n and 1 ≤ j ≤ N. Let F(X) = (W, Φ(x; D, τ )〉
and notice that F(X) = Pi〈u(i), <w,φ(x[i]; D,τ)〉).
For each u ∈ P define T(u) = {v ∈ D : ku - vk ≤ τ}. Then, the following holds for every
u∈P:
IIf(U)-<w,φ(*D,τ »|| = f (U)-
∣T(U^ X f(V)
v∈T (u)
≤TT⅛Γ X kf (U) - f (V)k
v∈T (u)
≤ --L ku — vk ≤ TL = 6rL
≤ |T(u)| 乙 k k-
v∈T (u)
Therefore for all x ∈ supp(I):
IF *(x)—F(X)I= x Du(i) ,f(χ[i])—<w,φ(χ[i]w,τ)〉)
i
i
≤ 6rL x∣∣u(i)∣∣ < 2
i
12
Under review as a conference paper at ICLR 2022
which implies that for any (x, y)〜I, yF(x) ≥ 1. Therefore, 2w^ satisfies Eq. (Hard-SVM), and so
we get that W satisfies Eq. (Hard-SVM) with ∣∣w∖∖ ≤ 2 ∣∣w^k.
Now, consider the ramp loss:
{1 Uy ≤ 0
1 - uy 0 < uy < 1
0 uy ≥ 1
(8)
Then, by a standard generalization bound (Bartlett & Mendelson, 2002; Boucheron et al., 2005),
there exists a constant α > 0 such that with probability at least 1 - δ over the randomness of S:
E	[Lramp (y,hw, Φ(x; D,τ)i)] ≤ α kφ(x;D；)kkwk + α∖∕l°g≡	(9)
(x,y)〜I	2 ʌ/m	V m
Since the ramp loss upper bounds the 0-1 loss L0-1(y, U) = I(Uy < 0), and by the inequalities
∣Φ(x; D,τ)k ≤ √n and ∣W∣ ≤ 2 ∣W^∣ ≤ 2,M2N,(P) Pn=JIU叫2 ≤ 2pM2Nr.(P)B2 we
have:
P(χ,y)〜I [y W, Φ(x; D, τ )i < 0] ≤ αj ”M 2Nr(P 此 + α( l°g^
mm
which completes the proof.	□
Proof of Theorem 4.5. Let C be an r-covering of P of minimal size, namely |C | = N. Let C0 ⊆ C
be the subset of balls that have mass ≥ N, i.e.:
C0 = {v ∈ C : Px〜I [∃i s.t.x[i] ∈ Br(v)] ≥ NJ
We first show that with probability ≥ 1 一 δ0, for every V ∈ C0 we have Br (v) ∩ Pu = 0. Indeed, fix
some v ∈ C0, and by definition of C0:
P [Br(V) ∩ Pu = 0] ≤(1 一 N)mu ≤ exp (-mN0) ≤ N
The required follows from the union bound.
Now, assume that for every v ∈ C0 there exists U ∈ Pu such that U ∈ Br (v). Therefore, we also
have Br (v) ⊆ B2r (U), and so:
∪v∈C0Br (v) ⊆ ∪v∈Pu B2r (v)	(10)
Since Pu ⊆ P, there exists a set A ⊆ Pu of size Nr (P) such that:
Pu ⊆ ∪v∈AB2r (v)	(11)
The clustering algorithm guarantees that (Gonzalez, 1985),
max min ∣v 一 U∣ ≤ 2 max min ∣v 一 U∣ ≤ 4r	(12)
v∈Pu u∈D	v∈Pu u∈A
where the last inequality follows by Eq. (11). It follows that Pu ⊆ ∪v∈DB4r(v) which implies that
∪v∈Pu B2r (v) ⊆ ∪v∈DB6r(v). Then, by Eq. (10) we get:
∪v∈C0Br(v) ⊆ ∪v∈DB6r (v)	(13)
Now, define W^ and F as in the proof of Theorem 4.3. We say that X ∈ X is good if for all 1 ≤ i ≤ n,
x[i] ∈ ∪v∈C 0 Br (v). We say that the training set S is good if for all x ∈ S, x is good.
13
Under review as a conference paper at ICLR 2022
By the definition of C0 , the probability that x is not good is at most
Pv∈c∖co Px〜I [∃i, x[i] ∈ Br(v)] ≤ e0. Let ξj ∈ {0,1} be the random variable indicating
whether the j-th example in S is not good. Observe that E Pjm=1 ξj = Pjm=1 E [ξj ]
Therefore, by Markov’s inequality we have:
m0.
P [∃j ∈ [m] s.t. ξj = 1] = P
m
X ξj ≥ 1
j=1
≤E
m
Xξj
j=1
m0 < δ
Therefore, the probability that S is good is at least 1 - δ.
For each u ∈ ∪v∈C0 Br (v) define T (u) = {v ∈ D :
following holds for u ∈ ∪v∈C0Br(v):
ku - vk ≤ 6r}. Then, by Eq. (13), the
f (u) - <w,φ(*D, 6r)〉||= f (u)-
lT(u∣ VXu) f (V)
≤ 百* X kf(U)- f(V)k
v∈T (u)
≤IT7⅛∣	X L ku - vk ≤ 6rL
v∈T (u)
Therefore for all good x:
IF *(X)- F(X)I= xDu(i) ,f(x[i])- <w,φ(χ[i]w,τ DE
i
≤ X ∣∣u(i)∣∣ IIf(X[i]) - <w,φ(χ[i]w,τ ))∣∣
i
≤ 6rL X ∣∣u(i)∣∣ ≤ e < 1
i
which implies that for any (x, y)〜I where X is good, yF(x) ≥ 1.
For now we consider two events: (1) Event where for every V ∈ C0 we have Br (v) ∩ PU = 0 which
occurs with probability at least 1 - δ0 and (2) Event that S is good which holds with probability
at least 1 - δ . Now, consider the ramp loss in Eq. (8). Then, by a standard generalization bound
(Bartlett & Mendelson, 2002; Boucheron et al., 2005) and considering the events above, there exists a
constant α > 0 such that with probability at least 1 - δ0 - 2δ over the randomness of S and Su:
E	[Lramp (y,hw, Φ(x; D,τ)i)] ≤ α kφ(x;D；)kkwk + 0、Illog(II	(14)
(x,y)~I, x is good	2ʌ/m	V m
Since the ramp loss upper bounds the 0-1 loss L0-1(y, U) = I(Uy < 0), and by the inequalities
∣∣Φ(x; D,τ)k ≤ √nand ∣∣w∣∣ ≤ 2 ∣∣W∣∣ ≤ 2，M2Nr(P) Pn=1 ∣∣u(i) ∣∣2 we have:
P(x,y)〜I, x is good [y hw, Φ(x; D,τ )i < 0] ≤ aS TlM 2 Nr(P) Pi=1 W2 + αj ⅛12
mm
Therefore,
P(x,y)〜I [y hw, Φ(x; D,τ)i < 0] = P(x,y)〜I, x is good [y hw, Φ(x; D,τ)i < 0]
+ P(x,yhI, x is not good [y hw, Φ(x; D,τ)i < 0]

≤α
nM2Nr(P) Pn=ι∣∣u叫2
m
+ aʌ三
m
+ 0
14
Under review as a conference paper at ICLR 2022
which concludes the proof.
□
Proof of Theorem 4.8. Assume that m < nN/2. For every i ∈ [n] and j ∈ [N], let xi,j ∈ RdI such
that xi,j = (j/L)ek where k = Ai(1).
Let Z = {z0, z1, . . . , zN} ⊆ RdP a setofN+1 points s.t. zi = (i/L, 0, . . . , 0). Let f : RdP → RN
the function such that:
0	z(1) ∈/ [(j - 1)/L, (j + 1)/L]
f (z)j = L∙ ∙ z(1) - j + 1 z(1) ∈ [(j - 1)∕L,jL]
Ij- L ∙ z(1) + 1 z(1) ∈ [j/L, (j + 1)L]
Namely, f(zi) = ei if i > 0 and f(z0) = 0. Observe that f is L-Lipschitz and kf (x)k ≤ 1 for
all x. Now, for some F(x) = Pin=1 u(i) , f (x[i]), notice that F (xi,j) = u(ji) =: yi,j for all
i ∈ [n], j ∈ [N]. So, for any choice of y = {yi,j}i∈[n],j∈[N] ⊂ {±1}, define the distribution Iy
defined by (Xij,y%,j) where i 〜[n] and j 〜[N] uniformly.
Note that supp(I) contains nM examples, and since m < nN/2 there are at least nN/2 examples
in supp(I) that are not seen by the algorithm A. Let S be the sample seen by the algorithm A, and
A(S) be the hypothesis returned by A upon seeing the sample S. Let S be the samples not seen by
the algorithm. We also denote y(S) the coordinates of y that appear in S, and y(S) the coordinates
of y that do not appear in S. So, we have:
EEP(χi,j,yi,j)〜Iy [A(S)(Xij) = yi,j] ≥ EEɪ X 1{A(S)(X) = y}
y s	y s nN ________
(x,y)∈S
≥ JSL ≥ 1
—2nΝ 4 4
Therefore, there exists a distribution Iy such that
E P(χ,y)〜Iy [A(S)(X) = y] ≥ 4
and observe that by definition of Iy, this distribution satisfies Assumption 3.2. Now, the required
follows from a simple application of Markov’s inequality (see 5.5.1 in Shalev-Shwartz & Ben-David
(2014)).	□
C Experimental Details
Here we provide details of the experiments performed in Section 5. All experiments were run on
NVidia Titan Xp GPUs with 12GB of memory. Neural network training algorithms were implemented
with PyTorch (Paszke et al., 2019). All of the empirical results can replicated in approximately 20
hours on a single Nvidia Titan Xp GPU. The shallow network consists of a convolution layer with
32 channels with kernels of size 5x5 and stride 1, 2x2 max pooling layer and fully connected layer.
The deep CNN network consists of two convolution layers followed by max pooling and two fully
connected layers. The first convolution layer has 32 channels with 3x3 kernels and stride 1, the
second has 64 channels and kernels as in the first convolution layer. Between the fully connected
layers there are 128 hidden neurons. We used dropout in both networks after the max pooling and for
the deep network also between the fully connected layer. The networks were trained with Adadelta
(Zeiler, 2012) with learning rate 1.0 (it performed better than learning rate 0.1). We trained both
networks for 30 epochs and chose the best performing network on a validation set. For MNIST and
FMNIST, the training set has 55000 points, the validation 5000 points and the test set 10000 points.
We implemented our semi-supervised algorithm where the unsupervised stage was performed with
k-means on whitened patches with 100 cluster centers. For clustering, we sampled 100000 patches of
size 5x5 from the training images and performed whitening. Given cluster centers, we calculated
a representation for patches. In this representation, the entry for a corresponding center is 1 if the
distance between the patch and the center is less than the average of the minimum distance to the
15
Under review as a conference paper at ICLR 2022
centers and the mean distance to the centers. Otherwise it is 0. We performed k-means with 4 random
seeds and 100 cluster centers and obtained 4 representations which were concatenated to get the full
representation of the patch. Then, we calculated the representation for each image of the training set
by concatenating the representations of the 5x5 patches of the image with stride 2 between patches. A
linear classifier was trained on the new representation set of the training set which consisted of 60000
data points for all three datasets in Table 1. Then, we evaluated the algorithm using the learned linear
classifer on a test set of size 10000 with the new representations.
The noisy FMNIST dataset was constructed by adding to each iamge a random isotropic Gaussian
vector with zero mean and standard deviation 5. For each pair of algorithm and dataset we performed
5 runs with different Numpy and PyTorch seeds.
16