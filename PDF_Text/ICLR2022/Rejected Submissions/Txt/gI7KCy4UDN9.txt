Under review as a conference paper at ICLR 2022
Post-Training Quantization Is All You Need
to Perform Cross-Platform Learned Image
Compression
Anonymous authors
Paper under double-blind review
Abstract
It has been witnessed that learned image compression has outperformed conven-
tional image coding techniques and tends to be practical in industrial applications.
One of the most critical issues that need to be considered is the non-deterministic
calculation, which makes the probability prediction cross-platform inconsistent
and frustrates successful decoding. We propose to solve this problem by intro-
ducing well-developed post-training quantization and making the model inference
integer-arithmetic-only, which is much simpler than presently existing training
and fine-tuning based approaches yet still keeps the superior rate-distortion per-
formance of learned image compression. Based on that, we further improve the
discretization of the entropy parameters and extend the deterministic inference
to fit Gaussian mixture models. With our proposed methods, the current state-of-
the-art image compression models can infer in a cross-platform consistent manner,
which makes the further development and practice of learned image compression
more promising.
1 Introduction
(a) original image
(b) failed decoding
Figure 1: The cross-platform inconsistency caused by non-deterministic model inference. This
inconsistency is a catastrophe to establish general-purpose image compression systems, and it is
almost inevitable when practicing learned image compression with floating-point arithmetic.
In recent years, learned data compression techniques have attracted a lot of attention and achieved
remarkable progress (Balle et al., 2017; 2018; Minnen et al., 2018; Cheng et al., 2020; He et al.,
2021; Guo et al., 2021). Recently, a few learned lossy image compression approaches (Minnen
et al., 2018; Cheng et al., 2020; He et al., 2021; Guo et al., 2021) have outperformed BPG (Bel-
lard, 2015) and the intra-frame coding of VVC (vtm, 2020), which are the state-of-the-art manually
designed image compression algorithms. As lossy image compression is one of the most fundamen-
tal techniques of visual data encoding, these results promise possibility of transmitting and storing
images or frames at a lower bit rate, which can benefit almost all industrial applications dealing
with visual data. Therefore, it is highly important to study how to make these rapidly developing
learning-based methods practical.
1
Under review as a conference paper at ICLR 2022
Most currently state-of-the-art learned image compression approaches adopt a forward-adaptive cod-
ing scheme proposed in Balle et al. (2018). The model will firstly transform the image to a major
representation y and a minor representation z. Then y will be encoded by an arithmetic coder (Ris-
sanen & Langdon, 1981; Martin, 1979) with independently saved z as side-information. Later this
method is further delved by introducing backward adaption with context modeling (Minnen et al.,
2018; He et al., 2021; Guo et al., 2021). However, this line of models suffer from a non-determinism
issue (Balle et al., 2019), which makes the image hard to be decoded when the encoder and the de-
coder run on heterogeneous platforms. The non-determinism is caused by floating-point arithmetic,
which can let the calculation results, from the same input operands but on different software or
hardware platforms, different. This issue makes calculating prior from side-information and context
model cross-platform inconsistent so that the decoding fails (See Figure 1). As cross-platform trans-
mission is a vital requirement of establishing practical image coding systems, this inconsistency is
critical.
To make the computation deterministic for cross-platform consistency, Balle et al. (2019) proposes
to train an integer network specially designed for learned image compression, where the inference
stage is integer-arithmetic-only. It performs well on earlier proposed models like Bane et al. (2018).
After determinizing Balle et al. (2018) with integer network, the compression performance has a
marginal reduction compared with the floating-point version. However, we find that on more com-
plex models with context modeling like Minnen et al. (2018) and Cheng et al. (2020), adopting this
integer network approach cannot keep the performance loss negligible. Another previous work (Sun
et al. (2021)) proposes to quantize model parameters and activation to fix-point, enabling cross-
platform decoding of the mean-scale hyperprior-only network in Minnen et al. (2018) without hurt-
ing compression performance. However, the determinism of state-of-the-art joint autoregressive and
hyperprior methods (Minnen et al., 2018; Cheng et al., 2020; He et al., 2021; Guo et al., 2021) are
still not considered. As the slow serial decoding problem of autoregressive context model has been
addressed in He et al. (2021), this joint modeling architecture is very promising and valuable for
practical application and its cross-platform decoding issue needs to be solved.
We notice that, these existing approaches are similar to general model quantization techniques in
spirit, i.e. quantization-aware training (QAT, Jacob et al. (2018); Krishnamoorthi (2018); Esser et al.
(2019); Bhalgat et al. (2020)) and post-training quantization (PTQ, Nagel et al. (2019; 2020); Li
et al. (2020)). Instead of proposing another approach to provide an ad-hoc solution, we advocate
to solve this cross-platform consistency issue based on those well-developed model quantization
techniques in a more flexible and extendable manner. Thus, we investigate cross-platform consistent
inference with PTQ. Usually, it requires much less calibration data (Nagel et al., 2020; 2021; Li
et al., 2020) and costs much less time to quantize the model than QAT, yet for several computer
vision tasks it can achieve almost the same accuracy as QAT when the target bit-width is 8 (Nagel
et al., 2019; 2020; Li et al., 2020). It does not demand re-training or fine-tuning the trained floating-
point models, which is very friendly to industrial deployment. By applying integer-arithmetic-only
operators (Jacob et al., 2018; Zhao et al., 2020; Yao et al., 2021) after quantization, we can achieve
deterministic models which get rid of the inconsistency issue.
In this paper, we contribute to the community from following perspectives:
1.	We evaluate and prove that, the deterministic computing issue of learned data compression
can be reduced to a general model quantization problem. After applying a standard post-
training quantization (PTQ) technique, we obtain cross-platform consistent image com-
pression models with marginal compression performance loss.
2.	We successfully determinize presently state-of-the-art learned image compression models
with context modeling and Gaussian mixture models. To the best of our knowledge, this
is the first work investigating and accessing cross-platform consistent inference on those
models. As we use standard quantization techniques, it is also promising to adopt our
method to determinize future compression models.
3.	We propose a novel approach to discretize the entropy parameters, which can be computed
directly in a deterministic manner. Compared with the existing method based on searching
algorithm, our method significantly speeds up the parameter discretization and eliminates
the bottleneck.
2
Under review as a conference paper at ICLR 2022
o>
X
AE/AD
Figure 2: Diagram of joint autoregressive and hyperprior architecture for learned image compres-
sion (Minnen et al., 2018; Cheng et al., 2020). AE/AD denote arithmetic en/de-coder. θ is the set
of predicted entropy parameters (i.e. π, μ,σ), modeling the distribution of y element-wisely. The
blue, red and orange arrows denote encoding, decoding and shared data flows, respectively. The
highlighted orange networks are shared by both encoding and decoding to estimate code probability
for AE/AD, demanding cross-platform consistency.
ΦPOIΛI
1⊂00
*JO∕V∖ΦN
J9 ① LUBJECl
We emphasize that our goal is not to propose a new method for quantizing neural networks, but
to call attention to the relation between the deterministic issue and model quantization techniques.
Benefiting from the maturely developed quantization techniques, we show that the cross-platform
consistency issue in learned image compression can be better solved.
2	Preliminary
Notations.
We represent matrices with capital bold letters (e.g. W) and denote vectors as small bold letters
(e.g. v). We use ceiling「•] and floor [•」notations to represent round-up and round-down operators,
respectively. And the blended「•」denotes round-to-nearest. By default, we denote the inputs and
activated outputs of a linear layer as v and u respectively. And the activation function is represented
by h(∙). We use the capital B as the quantization bit-width, which by default is set to 8 as we prefer
an 8-bit quantization. We use scalar s with a subscript to denote the corresponding quantization step,
e.g. sv means the quantization step of v. We may introduce a bracketed superscript to distinguish
variables in different layers, e.g. V(J) is the input of the '-th layer while v('+1) is the input of
the next layer. For simplicity, we use the notation clip (∙) as value clipping introduced by uniform
affine quantization, omitting the clipping bounds. By default we quantize the tensors to B-bit signed
integers, with the upper and lower clipping bounds 2B-1 - 1 and -2B-1.
Learned image compression and non-determinism issue.
In Appendix D, we describe several popular deep-learning-based image compression methods.
Briefly speaking, almost all the cutting-edge learned image compression techniques adopt the joint
autoregressive and hyperprior modeling illustrated in Figure 2. To encode the major symbols yi,
an entropy model Py∣z,yj<i (yi; z, yj∙<i) is adopted to predict the probability density of yi condi-
tioned on side-information Z and already decoded symbols yj∙<i. The predicted density functions
will then be accumulated to obtain the cumulative distribution functions (CDF) and fed to arithmetic
en/de-coders.
Generally, we input Z to the hyper SyntheSizer to obtain the intermediate feature Φ, and use the
context models to summarize the context representation Ψ. Then, we use a parameter network to
calculate the element-wise distribution parameters from Φ and Ψ to generate the predicted distri-
butions. To ensure correct decompression, we should make sure the inference of above-mentioned
models (hyper SyntheSizer, context modelS and parameter network) determiniStic. It means that, with
the same input Z and yj∙<i, the predicted entropy parameters θ should always be the same no matter
what platform the encoder or decoder is running on. The calculation should be platform-independent
3
Under review as a conference paper at ICLR 2022
and cross-platform consistent. However, if the models infer with floating-point numbers, the deter-
minism is hard to satisfy.
Uniform affine quantization (UAQ). UAQ is widely adopted by model quantization researches (Ja-
cob et al., 2018; Nagel et al., 2019; 2020; Li et al., 2020). In UAQ, both activation values and
weights should be quantized to fixed point numbers to allow the use of integer matrix multiplica-
tions. Usually, UAQ maps floating-point values to B-bit integers. For a given vector v, UAQ with a
quantization step sv is formulated as:
qv = clip (「s-1v_| + Zv)	(1)
The term zv is an integer representing the zero-point shifting the center of quantization range, which
is applied in asymmetric uniform affine quantization and omitted in symmetric ones. In recent
QAT, the step sv and zero-point zv are usually learned during training/fine-tuning (Esser et al.,
2019). And in PTQ they are normally determined by the value range of given weights or activation
vectors (Nagel et al., 2019; 2021).
Therefore, the floating point vector v is discretized to an integer vector qv , where each integer value
actually represents a fixed point value. We can remap qv to corresponding fix-point vector through
dequantization:
V	SV qv — SV ZV	(2)
When v is the activation output, the second term can be pre-computed and absorbed by the convo-
lution bias, so it will not introduce extra calculation to inference. Therefore, it is recommended by
Nagel et al. (2021) to apply asymmetric quantization on activation and symmetric quantization on
weights. Nagel et al. (2021) also suggests to adopt a per-channel quantization on weights (Krish-
namoorthi, 2018; Li et al., 2019), which we introduce in Appendix A.1.
Integer-arithmetic-only requantization. Several previous works on QAT for integer-arithmetic-
only inference have been proposed (Jacob et al., 2018; Zhao et al., 2020; Yao et al., 2021). One
of the most important techniques is the dyadic requantization. Between two convolution layers, the
dequantized fixed point activation will get quantized again, this can be fused to one operation called
requantization. A requantization after the `-th layer is:
qV'+1) = clip (jm(')qUf] + Z¢))	(3)
where qUf) is the int32 activation accumulation of the '-th layer, ZS) is the zero point, and m(') is
the requantization scale factor. Please refer to Appendix A.2 for detailed derivation of this equation.
Ideally, all floating-point operator should be removed from model inference but m is still floating-
point, which makes the requantization non-deterministic. Therefore, a dyadic number m0 is intro-
duced in Jacob et al. (2018) to approximate m:
m0 = d2nmc	(4)
where m0 and n are integers. Now we have m ≈ 2-nm0 . Obviously, the approximation error
becomes smaller when n gets larger. With this approximation, the requantization described in eq. 3
can be rewritten as:
(`) (`)
qV'+1) = clip ( 7nq)u + Ze)=cliP (l(m*qS)) “2n([ + Ze))	⑸
where the operator // is integer division with rounding-to-nearest (round-int-div, RID). And this
RID calculation can be further implemented as bit shift operation with rounding-to-nearest (round-
int-shift, RIS). Since both mo and 32-bit activation qU are integers now, the calculation of q§+1)
requires only an integer multiplication, a RID or RIS and a clipping. Hence the requantization
becomes integer-arithmetic-only.
3	Integer-only PTQ for deterministic coding
Different from works on general model quantization whose major purpose is to compress the model
size and improve inference efficiency, we introduce quantization to eliminate the inconsistent arith-
metic. Thus, we should strictly prevent using platform-specific operators. So we restrict all inference
calculations to 32-bit integer arithmetic, as it is the most widely supported set of integer arithmetic
by hardware devices. Particularly, leveraging execution efficiency and compatibility, we only con-
sider adopting matrix multiplication on 8-bit integer operands to perform convolution.
4
Under review as a conference paper at ICLR 2022
9
2
t3
Int
t8
no 一ω-nluno□<
ilelΛI / >u。。
M—∙uenb ①江
Int8 to Int32
Int32 to Int8
Int8
(b) Requantization
(a) Integer-arithmetic-only inference
©
Figure 3: The integer-arithmetic-only inference. An offline-constrained integer-arithmetic-only re-
quantization will be adopted during inference, which we will discuss in Section 3.2.
3.1	PTQ baseline
We adopt a relatively simple and standard quantization pipeline as described in Nagel et al. (2021).
We apply symmetric per-channel quantization on weights and asymmetric per-tensor quantization
on activation except the output of the last layer in parameter net. Instead, we symmetrically quantize
the activation from the last layer to 16 bits, with a fixed quantization step 2-6. This benefits the CDF
indexing and calculation which we will discuss later in Section 4.1. As recommended in Nagel et al.
(2021), we adopt a grid search minimizing the reconstruction error to obtain the weight quantization
step sW of each layer. We figure out the activation quantization step su with Min-Max method on a
little calibration data. We detailedly describe the algorithms in Appendix C.3.
After setting the quantizers, we apply a per-block (or per-network) adaptive rounding reconstruction
(Brecq, Li et al. (2020)), though the quantized models without this reconstruction have already
achieved acceptable rate-distortion performance (Appendix C.4).
3.2	Offline-constrained integer-arithmetic-only requantization
Previous use of integer-arithmetic-only requantization is based on QAT (Jacob et al., 2018; Yao
et al., 2021; Zhao et al., 2020). It is first proposed in Jacob et al. (2018) where m0 in eq. 4 is always
set to an integer bigger than 230 which should be represented with 32 bits. We find it risky, as this 32-
bit integer m0 will later multiply with the 32-bit integer accumulation. Assuming that no specific
hardware support is available, we tend to believe that the requantization should be implemented
with standard 32-bit integer arithmetic (i.e. we cannot conduct the bit-shift on 64-bit multiplication
result registers). Thus, the 32-bit integer multiplication may overflow and result in error if we use
unconstrained m0. Another issue is, in Jacob et al. (2018) the requantization scale factor m is
empirically assumed as always less than 1, which is hard to satisfy in PTQ.
To address this problem, we slightly modify the requantization by moving the zero-point adding
operation in eq. 3 forward:
pu') =⑶
qS+1)= CliP(I_m⑶(q$)+ PU0)]) = CliP(I_m('%0，)])
(6)
(7)
where q0S) = qUf) + PS) is the 32-bit integer activation biased With the 32-bit pre-scaling zero-point
p(u`) . By doing so, the asymmetric quantization of qu changes to the symmetric quantization of q0u .
As the requantized value will be clipped to B-bit integer within a range [-2B-1, 2B-1 - 1], the
maximal and minimal valid value of q0f are computable:
2B-1 - 1
m⑶
-2B-1
m⑶
(8)
5
Under review as a conference paper at ICLR 2022
with these bounds, we can earlier conduct the clipping operation before the rescaling (see Figure 6
in Appendix B.1), restricting the biased activation values q0u in the range [qmin, qmax]. Thus, we can
figure out the largest n subject to ∀qu0 ∈ q0u, -231 ≤ m0qu0 < 231, which avoids overflow while
keeps precision as much as possible:
n(') = 32 - B, m0') = j2n(') m叫	(9)
Please refer to Appendix B.1 for a detailed explanation of eq. 9. Therefore, after setting the quantiz-
ers, we offline calculate the dyadic numbers and replace the requantization operations layer by layer.
Empirically, the proposed integer-arithmetic-only requantization with constrained magnitude of m0
still keeps the model performance, though it introduces slightly more numerical error than the orig-
inal version proposed in Jacob et al. (2018). We can safely adopt it across platforms with standard
32-bit integer multiplications and RISs/RIDs. Figure 3 (right) shows this offline-constrained form
of requantization.
4 From 1 6-bit output to discretized cumulative distribution
4.1 Binary logarithm STD discretization for deterministic entropy modeling
The network-output parameters should be discretized, so that the CDFs can be stored as limited
look-up-tables (LUTs), as discussed in Appendix D.2. In previously proposed approaches (Balle
et al., 2019; Sun et al., 2021), the standard deviations (STD) are logarithmically discretized with
computing natural logarithm log(∙):
ʌ
iσ =
log(σ) - log(σmin)
∆σ
(10)
which is hard to determinize. Sun et al. (2021) obtains iσ from parameter network output in a
deterministic way by comparing the fix-point σ value with pre-computed sampling points σ. This is
moderately efficient as there are 64 levels of σ for each predicted STD to compare. We detailedly
introduce this discretization approach in Appendix D.3 (eq. 51 and the following paragraph).
Empirically, the STD is long-tail distributed as most of the latents are predicted to have entropy close
to zero. So the logarithmic discretization of σ is reasonable to suppress error and has been proved
effective. To keep this favor and be more hardware-friendly, we propose to use binary logarithm.
Hence, we modify eq. 10 (and eq. 51 in Appendix D.3) to:
∆ = T⅛og2 (!H
L - 1	σmin
iσ = [(lθg2(σ) — lθg2(σmin)) ∆-1J
σ = !min (exp2 (i))
(11)
where exp2(x) = 2x denotes the power of 2. Balle et al. (2019) and Sun et al. (2021) adopt the
same bounds σmin = 0.11 and σmax = 256 with level L = 64. To omit non-determinism while
to simplify the calculation, we instead adopt σmin = 0.125, σmax = 32 and L = 9. Therefore, we
obtain a simplified formula with ∆ = 1 and log2 (σmin) = -3. Since the STD has an extremely
long-tail distribution, using a smaller upper bound σmax is painless. Adopting less levels, however,
enlarges error because the sampling points get sparser. Thus, we uniformly interpolate additional 7
minor levels between each two adjacentmajor levels of σ. Considering the input σ is the dequantized
fix-point value with corresponding quantized integer q and scale s = 2-6 (i.e. σ = sq), the updated
formula is:
ʌ
i = biog2 (q)C +i0g2(s) + 3
j
q - exp2([log2(q)C)-
exp2(blog2(q)C - 3)
ʌ ʌ ʌ
^σ = 8i + j
σ = σmin 卜xp2 (i) + jexp2 (i - 3))
(12)
6
Under review as a conference paper at ICLR 2022
Figure 4: Visualization of the proposed 65-level STD parameter discretization. The orange dots
denote binary-logarithmically distributed major levels and the blue ones are linearly interpolated
minor levels.
1	1	/	∖	c・	AA ∙ A	个 一 Γ r∖ -1	c、	1 ʌ  C rʌ -ι	c、	aΛ	∙	ι
where log2(s) = -6 is a constant integer. i ∈ {0, 1, . . . , 8} and j ∈ {0, 1, . . . , 8} are the major and
minor indexes, respectively. Note that i = 8 iff. q = 211, when j = 0. Thus, the summary index
iσ ∈ {0,1,..., 64} corresponds to 65 values of σ, which generates 65 CDFS to be stored as LUTs.
We further describe the derivation of this discretization in Appendix B.2. Figure 4 shows the value
distribution of this discretization.
Thus, the calculation of iσ reduces to calculating the round-down integer binary logarithm of 16-bit
integer q. To compute the binary-logarithm of an integer, we can adopt platform-specific instructions
like BSR on x86 or CLZ on ARM. We can also use a platform-independent bit-wise algorithm to
count the leading zeros to obtain result of integer binary logarithm, which is easy to vectorize.
The Algorithm 1 in Appendix E.1 describes the detailed process converting the 16-bit output to the
indexes.
4.2 Deterministic entropy coding with Gaussian mixture model
For models using GMM with k Gaussian components, we can calculate CDF of the i-th component
with given μi and σi and accumulate it with a multiplier πi. The predicted multiplier is dequantized
16-bit fix-point number πi = sπ qπi :
k
CGMM(y) (y； ∏,μ,σ) = Esnq∏i Cy (y; μi, σ,)	(13)
i=1
where sπ is the quantization scale of qπi. We can omit sπ in eq. 13 because it is a constant scalar
normalizer and can be merged into the normalization factor of frequency based CDF table. The
remaining part only involves integer arithmetic.
To obtain the CDF of the i-th Gaussian component Cy (y;μi,σi), We can query two-level pre-
computed LUTs with combined iσ (eq. 12) and iμ (eq. 53 in Appendix D.3) as outer index and
y — bμiC in eq. 52 as inner index. However, indexing the inner level LUT with y — [μiC suffers from
a risk of out-of-bound error. We cannot directly restrict the range of y to [[μiC — R, [μiC + R] like
Sun et al. (2021) because multiple Gaussian components are involved. If ∃μj∙ subject to μi—μj∙ > R,
the direct restriction may result in significant error. Instead, we set the cumulative distribution value
to 0 and CDFmax when y — [μiJ ≤ —R and y — [μiC ≥ R respectively. We restrict the symbol
y no bigger than (max [μj) + R where the CDF value is CDFmax. Also we restrict it larger than
(min [μC) — R where the CDF value is zero. In situations the symbol y is out of these upper
and lower bounds, we will adopt Golomb coding to encode y, inspired by tensorflow-compression
implementation1. This situation is quite rare as its corresponding probability mass is close to zero,
and adopting Golomb coding will not damage the overall bit rate. Thus, the CDF of GMM in eq. 13
is still monotonically increasing which can be reversed with searching algorithms during decoding.
With pre-computed CDFs of single mu-scale Gaussian entropy model, probability of yi in each
Gaussian component can be checked from shared LUTs. Then the aggregated CDF value of yi can
1https://github.com/tensorflow/compression/blob/v2.2/tensorflow_
compression/python/entropy_models/continuous_base.py#L80-L81
7
Under review as a conference paper at ICLR 2022
Figure 5: RD curves evaluated on Kodak. (a) RD performance of different models w/ or w/o quan-
tization. The solid lines with cross markers denote original model inference with floating point
numbers. The dots represent quantized 8-bit integer-arithmetic-only models, sharing the same color
as its floating-point version. (b) We test Sun et al. (2021) by quantizing the activations in both
Per-Channel and per-tensor manner. We also test Bane et al. (2019).
be calculated with given qπ . In Appendix E.2, we provide pseudo-code of this GMM involving
Coding for referenCe. Algorithm 3 desCribes the whole enCoding proCess and Algorithm 4 desCribes
the deCoding.
5	Experiments
We train various presently state-of-the-art learned image Compression arChiteCtures on floating point
to obtain the full-preCision models. After the training, we use PTQ algorithms to quantize sub-
networks involved in the entropy estimation to 8-bit, as above-mentioned. Then we replaCe all
requantization and Leaky ReLU (Appendix A.3) with Corresponding deterministiC adaptions and
insert additional layers for LUT-index CalCulation. The detailed experiment setting is desCribed in
Appendix C.
5.1	Compression performance
Method	ours	Sun et al. (2021)	
Act. Quant.	per-tensor	per-channel	per-tensor
BD-Rate (%)	0.35	1.22	4.04
Table 1: BD-rates over full-preCision model. The RD data is the same as that in Figure 5(b).
We quantize sub-networks of Balle et al. (2018), Minnen et al. (2018), and Cheng et al. (2020)
involving entropy prediCtion to 8-bit and Compare their rate-distortion (RD) performanCe with orig-
inal full-preCision version, shown in Figure 5(a). The results indiCate that, existing learned im-
age Compression teChniques are Compatible with standard PTQ. All tested models Can infer with
integer-arithmetiC-only Computations with negligible reduCtion on RD performanCe, whiCh guaran-
tees painless Cross-platform ConsistenCy. It is not neCessary any more for us to partiCularly develop
new training teChniques nor network struCtures to address the inConsistenCy issue.
We compare our approach with existing ones (Sun et al., 2021; Bane et al., 2019). We present the
RD results in Figure 5(b), and further report the Corresponding BD-rates (Bjontegaard (2001)) in
Table 1. When adopting our approach or Sun et al. (2021) on Minnen et al. (2018) model, the RD
performance marginally deteriorates. However, Sun et al. (2021) adopts a per-channel activation
quantization which is unfriendly to hardware implementation and rarely supported (Nagel et al.,
2021). When adopting Sun et al. (2021) with per-tensor activation quantization, the performance,
especially at higher bit-rates, gets hurt significantly. We have successfully reproduced the good
8
Under review as a conference paper at ICLR 2022
performance reported in Balle et al. (2019) for determinizing Balle et al. (2018) (not shown here),
although we try hard, we find this method cannot keep marginal performance deterioration for de-
terminizing context-model-involved architectures like Minnen et al. (2018). Note that Balle et al.
(2019) is a dedicated QAT method, we find it hard to train when applied on Minnen et al. (2018).
5.2	Rate of decoding error
PTQ		w/o PTQ (FP32)				w/ PTQ (Int8)		
Encoding Platform	GPU	CPU	GPU	CPU
Error Rate on Kodak Error Rate on Tecnick	12/24 (50.0%) 72/100 (72.0%)	12/24 (50.0%) 84/100 (84.0%)	0/24 (0.0%) 0/100 (0.0%)	0/24 (0.0%) 0/100 (0.0%)
Table 2: Decoding error rates of Minnen et al. (2018) when inference with or without PTQ. The
results are tested on NVIDIA GTX 1060 (marked as GPU) and Intel Core i7-7700 (marked as
CPU). The decoding is cross-evaluated on the two platforms, i.e. encoding on one and decoding on
the other.
Following Balle et al. (2019) and Sun et al. (2021), We report the rate of decoding error in Table 2 for
completeness. As the networks have been strictly restricted to only perform 8-bit and 32-bit integer
arithmetic, the inference is strictly deterministic.
5.3 Latency of binary logarithm based STD discretization
Method	Comparison (CompressAI)	Comparison (vectorized)	Calculation (ours)	Hyper Synthesis
Latency on Kodak	nɪ	9S∑~	4.35	22.26
Latency on Tecnick	61.01	33.81	9.48	47.74
Table 3: Discretization latency with different approaches (unit: microsecond).
Table 3 shows the inference latency when adopting comparison-based discretization used by Sun
et al. (2021) and our proposed calculation-based discretization. All the results are tested on NVIDIA
GTX 1060. To evaluate the comparison-based approach, we refer to popular CompressAI (Begaint
et al., 2020) implementation2 and also test another more efficient vectorized algorithm (described
in Appendix E.3). The inference latency of hyper synthesis is also reported as a reference. The
results on Kodak (resolution: 512 × 768 px) prove that the directly calculated binary logarithm is
more efficient, strongly suppressing the speed bottleneck of STD discretization. And the results on
larger 1200 × 1200 px Tecnick images indicate that this improvement can be more significant when
compressing high-resolution images.
6	Discussion
The mature investigation on general model quantization provides free lunch to us for establish-
ing a cross-platform consistent entropy estimation approach, which is essential to practical learned
image compression. In this paper, we experimentally prove that the non-consistency issue of state-
of-the-art learned image compression architectures can reduce to an integer-arithmetic-only model
quantization problem. With a standard PTQ scheme, we achieve deterministic compression models
which have almost the same compression performance as their pre-quantized full-precision versions.
This result is encouraging. Furthermore, we improve the parameter discretization and extend it to fit
GMM entropy model. In the future, we will further delve into practical learned image compression
by extending the rate-distortion tradeoff to rate-distortion-speed tradeoff.
2https://github.com/InterDigitalInc/CompressAI/blob/v1.1.8/compressai/
entropy_models/entropy_models.py#L653-L658
9
Under review as a conference paper at ICLR 2022
References
Versatile video coding reference software version 11.0 (vtm-11.0), 2020. URL https://vcgit.
hhi.fraunhofer.de/jvet/VVCSoftware_VTM/-/tags/VTM-11.0.
Eirikur Agustsson and Radu Timofte. Ntire 2017 challenge on single image super-resolution:
Dataset and study. 2017 IEEE Conference on Computer Vision and Pattern Recognition Work-
Shops (CVPRW) ,pp.1122-1131, 2017.
N.	Asuni and A. Giachetti. Testimages: a large-scale archive for testing visual devices and basic
image processing algorithms. In STAG: Smart Tools & Apps for Graphics (2014), 2014.
Johannes Balle, Valero Laparra, and Eero P Simoncelli. End-to-end optimized image compression.
In Int. Conf. on Learning Representations, 2017.
Johannes Balle, David Minnen, Saurabh Singh, Sung Jin Hwang, and Nick Johnston. Variational
image compression with a scale hyperprior. In Int. Conf. on Learning Representations, 2018.
Johannes Balle, Nick Johnston, and David Minnen. Integer networks for data compression with
latent-variable models. In Int. Conf. on Learning Representations, 2019.
J.	Balle. Efficient nonlinear transforms for lossy image compression. In 2018 Picture Coding
Symposium (PCS), pp. 248-252, June 2018. doi: 10.1109/PCS.2018.8456272.
Jean Begaint, Fabien Racape, Simon Feltman, and Akshay Pushparaja. Compressai: a py-
torch library and evaluation platform for end-to-end compression research. arXiv preprint
arXiv:2011.03029, 2020.
Fabrice Bellard. Bpg image format, 2015. URL https://bellard.org/bpg.
Yash Bhalgat, Jinwon Lee, Markus Nagel, Tijmen Blankevoort, and Nojun Kwak. Lsq+: Improving
low-bit quantization through learnable offsets and better initialization. In Proceedings of the
IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops, pp. 696-697,
2020.
Gisle Bjontegaard. Calculation of average psnr differences between rd-curves. VCEG-M33, 2001.
Zhengxue Cheng, Heming Sun, Masaru Takeuchi, and Jiro Katto. Learned image compression with
discretized gaussian mixture likelihoods and attention modules. In Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition (CVPR), June 2020.
Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hi-
erarchical image database. In 2009 IEEE conference on computer vision and pattern recognition,
pp. 248-255. Ieee, 2009.
Steven K Esser, Jeffrey L McKinstry, Deepika Bablani, Rathinakumar Appuswamy, and Dharmen-
dra S Modha. Learned step size quantization. In International Conference on Learning Repre-
sentations, 2019.
V.	K. Goyal. Theoretical foundations of transform coding. IEEE Signal Processing Magazine, 18
(5):9-21, Sep. 2001. ISSN 1558-0792. doi: 10.1109/79.952802.
Zongyu Guo, Zhizheng Zhang, Runsen Feng, and Zhibo Chen. Causal contextual prediction for
learned image compression. IEEE Transactions on Circuits and Systems for Video Technology,
2021.
Dailan He, Yaoyan Zheng, Baocheng Sun, Yan Wang, and Hongwei Qin. Checkerboard context
model for efficient learned image compression. In Proceedings of the IEEE/CVF Conference on
Computer Vision and Pattern Recognition, pp. 14771-14780, 2021.
Benoit Jacob, Skirmantas Kligys, Bo Chen, Menglong Zhu, Matthew Tang, Andrew Howard,
Hartwig Adam, and Dmitry Kalenichenko. Quantization and training of neural networks for
efficient integer-arithmetic-only inference. In Proceedings of the IEEE conference on computer
vision and pattern recognition, pp. 2704-2713, 2018.
10
Under review as a conference paper at ICLR 2022
Eastman Kodak. Kodak lossless true color image suite (photocd pcd0992), 1993. URL http:
//r0k.us/graphics/kodak/.
Raghuraman Krishnamoorthi. Quantizing deep convolutional networks for efficient inference: A
whitepaper. arXiv preprint arXiv:1806.08342, 2018.
Rundong Li, Yan Wang, Feng Liang, Hongwei Qin, Junjie Yan, and Rui Fan. Fully quantized
network for object detection. In Proceedings of the IEEE/CVF Conference on Computer Vision
and Pattern Recognition, pp. 2810-2819, 2019.
Yuhang Li, Ruihao Gong, Xu Tan, Yang Yang, Peng Hu, Qi Zhang, Fengwei Yu, Wei Wang, and Shi
Gu. Brecq: Pushing the limit of post-training quantization by block reconstruction. In Interna-
tional Conference on Learning Representations, 2020.
G Martin. Range encoding: an algorithm for removing redundancy from a digitised message. In
Video and Data Recording Conference, Southampton, 1979, pp. 24-27, 1979.
David Minnen, Johannes Balle, and George D Toderici. Joint autoregressive and hierarchical priors
for learned image compression. In Advances in Neural Information Processing Systems, pp.
10771-10780, 2018.
Markus Nagel, Mart van Baalen, Tijmen Blankevoort, and Max Welling. Data-free quantization
through weight equalization and bias correction. In Proceedings of the IEEE/CVF International
Conference on Computer Vision, pp. 1325-1334, 2019.
Markus Nagel, Rana Ali Amjad, Mart Van Baalen, Christos Louizos, and Tijmen Blankevoort. Up or
down? adaptive rounding for post-training quantization. In International Conference on Machine
Learning, pp. 7197-7206. PMLR, 2020.
Markus Nagel, Marios Fournarakis, Rana Ali Amjad, Yelysei Bondarenko, Mart van Baalen,
and Tijmen Blankevoort. A white paper on neural network quantization. arXiv preprint
arXiv:2106.08295, 2021.
Jorma Rissanen and Glen Langdon. Universal modeling and coding. IEEE Transactions on Infor-
mation Theory, 27(1):12-23, 1981.
Heming Sun, Lu Yu, and Jiro Katto. Learned image compression with fixed-point arithmetic. In
2021 Picture Coding Symposium (PCS), pp. 1-5. IEEE, 2021.
Zhou Wang, Eero P Simoncelli, and Alan C Bovik. Multiscale structural similarity for image quality
assessment. In The Thrity-Seventh Asilomar Conference on Signals, Systems & Computers, 2003,
volume 2, pp. 1398-1402. Ieee, 2003.
Zhewei Yao, Zhen Dong, Zhangcheng Zheng, Amir Gholami, Jiali Yu, Eric Tan, Leyuan Wang, Qi-
jing Huang, Yida Wang, Michael Mahoney, et al. Hawq-v3: Dyadic neural network quantization.
In International Conference on Machine Learning, pp. 11875-11886. PMLR, 2021.
Hengrui Zhao, Dong Liu, and Houqiang Li. Efficient integer-arithmetic-only convolutional neural
networks. arXiv preprint arXiv:2006.11735, 2020.
11
Under review as a conference paper at ICLR 2022
A	Detailed describtion of PTQ
A. 1 Per-channel weight quantization
Weights of convolution filters may have various value ranges. Because of this imbalance, using
the same quantization scale sw for all filters may cause large rounding error in filters with small
ranges and clipping error in filters with large ranges. Therefore, per-channel weight quantization is
proposed (Krishnamoorthi, 2018; Li et al., 2019). To per-channel quantize a weight tensor W =
[w1, w2, . . . , wc] with c filter channels, we introduce c scales sw1 , . . . , swc and separately quantize
each filter:
Wi = SWiqWi ,	i = 1, 2,...,c	(14)
where the scale factors swi can be merged into later requantization operation.
Note that, we do not recommend to conduct this per-channel quantization on activation, as it will
introduce extra calculation to rescale the multiplication results before convolution accumulation.
Therefore, we only per-channel quantize the weights while keep activation per-tensor quantized, as
Nagel et al. (2021) suggests.
A.2 Requantization
Consider a convolutional or fully connected layer with input v, weights W and activation function
h(∙). The output activated vector U is:
u = h(Wv)	(15)
Provided that we have quantized the weights and input vector with scale factors sW and sv respec-
tively, the corresponding quantized integer representation QW and qv are:
Qw =clip ([(sw1W)J)	(16)
qv = Clip(I"(s-1v)」)	(17)
and the corresponding dequantization results are:
W = SWQW	(18)
V = Sv qv	(19)
Thus, the activation U calculated from quantized weights and input is:
u = h(W v) = h(sw Sv QWqv)	(20)
Usually, we adopt ReLU or Leaky ReLU as the activation h(∙). They are segmented linear functions
with scaling invariance, i.e. scaling the input of h(∙) with a factor S is equivalence to scaling the
output with S:
h(SV)= S ∙ h(v)	(21)
Therefore, we can move the scalar SW Sv in eq. 20 outside the activation function:
U = SW Sv h(Qw qv)	(22)
Now QWqv is an integer matrix multiplication, outputting a 32-bit integer vector. Also, the activa-
tion function h(∙) can be formulated as integer-arithmetic-only. Thus, the activation h(Qwqv) is a
32-bit integer vector, named qu by us. Now U is a 32-bit fix-point number with integer representa-
tion qu and quantization scale factor SW Sv :
qu =h(QWqv)
U =SWSv qu
(23)
Notice that, after each layer, the output should be quantized to 8 bits again, or the matrix multipli-
cation in the next layer cannot be conducted using 8-bit integer arithmetic. Therefore, we quantize
U(') with scale factor SS)+1 (note that v('+1) = U(')):
qj+D =Clip ( (Ji) v(`+1) ),	(quantize v(`+1))
Sv
=Clip ( I⅛iyU(') ),	(v('+1) = U('))
(`) (`)
=clip ( 'W+v) qS) V	(eq. 23)
(24)
12
Under review as a conference paper at ICLR 2022
Following Jacob et al. (2018), we fuse the division of scale factors to m, which is called the requan-
tization scale factor:
(`) (`)
m(') = SW SV
m = s('+1)
Introducing m to eq. 24, finally we obtain the requantization formula:
qV'+1) = clip (lm(')qS)0
(25)
(26)
A.3 Folding Leaky ReLU to conditioned dyadic requantization
Leaky ReLU is frequently adopted in presently state-of-the-art compression models. For instance,
Minnen et al. (2018) introduces it in hyper analyzer and synthesizer, Cheng et al. (2020) uses it as
activation functions of the parameter network. Different from ReLU which can be simply seen as a
conditioned clipping operator which is deterministic, Leaky ReLU involves floating point multipli-
cation.
Considering Leaky ReLU with a negative slope α applied on each quantized scalar qu ∈ qu , pro-
vided the requantization factor is m, the requantization result qv is:
clip ( m(' (qU') + PS))]}	qU' ≥ 0
clip ( αm(') (q(Ie) + PS))]}	qU，＜ 0
(27)
where qu is a signed integer and checking its sign is painless. For non-negative and negative cu-
mulative values, the above operation can be separately performed as two branches of requantization
with different factors m and αm. Hence the Leaky ReLU can be fused into the requantization in a
vectorized manner. We implement the fused activation layer as a conditioned dyadic multiplication,
with scaling factor m for non-negative qu and αm for negative qu .
B Equation derivations
B.1 Derivations of equation 9
Figure 6: Comparison between scaling-clipping requantization and clipping-scaling requantization.
By computing the bounds of valid range, we can conduct the clipping operation before scaling with
factor m.
Revisit eq. 8 in the main body:
2B-1 - 1
m(e)
--2Bτ
m(e)
The bounds qmax , qmin describe the valid value range of the activation q0 u biased by the zero-point
Pu . After the clipping operation, all of the clipped value q00u will be in this range:
q u = clip (q u, qmin , qmax)
s.t. ∀q ∈ q00u, - 2B-1 ≤ mqmin ≤ mq ≤ mqmax ≤ 2B-1 - 1
(28)
13
Under review as a conference paper at ICLR 2022
To perform a dyadic requantization, we want to find out proper integers m0 , n subject to
∀q ∈ q00u, -2B-1 ≤ lmmnqk ≤ 2B-1 - 1
We letm0 become a function of n:
m0 = b2nmc
Notice that m0 is non-negative and n, m is positive. For each q ∈ q00u ,
lmqk ≤ lm02mxk = [b22nmcqmax] ≤ dmqmaxC ≤「2BT- 1J = 2B-1 - 1
l*k ≥ l号k = [b22nmcqmj ≥ dmqminC ≥ TBTJ = -2B — 1
which satisfies the condition in eq. 29.
Therefore, we only need to determine a proper value of n following two conditions:
(29)
(30)
(31)
1.	The multiplication overflow from m0 q00u should be avoided. As m0 is a function driven
by n now, this condition implicitly define an upper bound of n.
2.	n should be set as large as possible, to suppress the rounding error introduced by requanti-
zation.
To avoid the 32-bit multiplication overflow, we constrain the magnitude ofm0 q00u subject to:
∀q ∈ q00u, -231 ≤ m0 q ≤ 231 - 1
Introducing m0 = b2nmc, we obtain the conditions:
b2nmcqmax≤ 231 -1
b2nmc qmin ≥ -231
Further introducing the definition of qmax and qmin in eq. 8, we have:
b2nmc ≤
231 - 1
qmax
231 - 1
231
(32)
(33)
(34)
(35)
[1]
<
[1]
231	231	231
b2 mc ≤ ^qmn = l-2BTm = j 2B-11
mm
231
≤ η------r
-j - I
(36)
Since m is positive, We have |_2nm_C ≤ 2nm and ^B-I ≤
m
the condition to let n subject to:
231
231
2B-1-1
m
. Hence We
relax
E ≤ j
k
231
2nm ≤ k
(37)
We get:
m
(38)
≤
so that We let n = 32 - B . It is almost the largest value of n subject to the condition in eq. 29 and
eq. 32, avoiding the multiplication m0 q00u overfloW.
14
Under review as a conference paper at ICLR 2022
Figure 7: Comparison of our binary logarithm discretization and natural logarithm discretization
proposed in Bane et al. (2019). The Y-axis is the percent of discretized standard deviation introduced
for meaningful comparison: σ% = σσ~ X 100% With σmax = 256 for the natural logarithm one
and σmax = 32 for ours.
B.2 Derivations of equation 12
The 65 level STD discretization is derived from a linear interpolation. Revisit the definition of major
levels in eq. 11 and consider σ = sq:
ʌ ，一 ，、，
i	= blog2 (q )c — 3
σmajor = σmin (exp2(i))
Since q has been clipped to [8, 2048] (corresponding to dequantized σ ∈ [0.125, 32] With quantiza-
tion step 2-6), i ∈ {0,1,... 8} and σmajor ∈ {2-3, 2-2,..., 25}.
The discretization Will map all σ ∈ [0.125, 0.25) to 0.125 and σ ∈ [0.25, 0.5) to 0.25, Which is too
sparse and Will result in large error, significantly hurting the compression performance. Thus, We
interpolate minor values betWeen tWo adjacent major values i and i + 1.
Let σi = σmin(exp2(i)) denotes the STD corresponding to index i. And similarly let σ,+1 corre-
ι	ʌ l -i ττ τ Λ ∙	i	Ar ・	ι Λ ∙ rʌ ʌ l -i 1	∙∕F,f	/	・ Λ
sponds to i + 1. We linearly insert 7 minor levels in [i, i + 1] With the step size ∆minor:
	∆ ∙	= σi+ι - σi ʌʌmmor -	& =1 σmin (exp2(i + 1) — exp2(i))	(39) 8 =Gσmin exp2(i) 8
and the minor index is:	j - Hmlnor(σ 一刃] =l8σm1n exp2(-i)(σ — σi )m =卜exp2(—i)( q — exp2(i))"∣	(4O) Z	一八	一、、一 (q — exp2(i + 3)) -	K	 exp2(i)
F	A /	,	A	1	C ——6	Co /	I 1	/ ∖ I	/F	ʌ I 1	/ ∖ I O
where σi ≤	σ <	σi+1	and	σ = 2 6q.	So i ≤	[log2(q)C	— 3 < i + 1, thus,	i	= [log2(q)C — 3.
Introduce it to the above equation, We have:
(q — eχp2(blog2 (q)C)) 一
eχp2(blog2 (q)C ― 3)
Thus, the STD σ will be discretized to:
σ = σi + j ∆minor
(41)
(42)
And the interpolated levels are σib + ∆minor, σib + 2∆minor,... ,σ)^ + 7∆minor. When σ^ + 7∆minor <
σ < σi + 8∆minor = σ,+1, the STD σ will be mapped to the next major level σ,+1. When it occurs,
the corresponding minor index j = 8.
15
Under review as a conference paper at ICLR 2022
C Experiment: detailed settings and more results
C.1 Training floating-point models
To evaluate the proposed PTQ-based scheme, we should first obtain the full-precision models trained
on floating-point numbers. In this section we will report our detailed training settings for repro-
ducibility.
We draw 8000 images with largest resolution from ImageNet (Deng et al., 2009) to construct the
training dataset. The training set is shared to train all reported models. Before training we apply
the same preprocessing as Balle et al. (2017) on all data by downsamping and disturbing the input
images. During training, we randomly crop each image to 256 × 256 px in each iteration. We adopt a
batch-size of 16 and a initial learning rate of 1e-4 in all training. To optimize the parameters, we use
Adam optimizer with βι = 0.9, β2 = 0.999. To cover a range of rate-distortion tradeoffs, 6 different
values of λ are chosen from {0.0016, 0.0032, 0.0075, 0.015, 0.03, 0.45} for MSE-optimization, fol-
lowing previous works (Cheng et al., 2020; He et al., 2021). On all models, we adopt spectral Adam
optimization (Sadam, Balie (2θ18)) to improve the training stability. For each model architecture,
we particularly adjust the training setting according to previous suggestions:
•	Balle et al. (2018). For each λ we train the corresponding model 2000 epochs. As sug-
gested by the authors, we set a so-called bottleneck with N = 128, M = 192 when using
the 3 lower λ and N = 192, M = 320 when using the others.
•	Minnen et al. (2018). Following the suggestion from the authors3, We train 6000 epochs.
We choose N and M dependent on λ, with N = 128 and M = 192 for the 3 lower λ, and
N = 192 and M = 320 for the 3 higher ones. We decay the learning rate to 5e-5 after
training 3000 epochs.
•	Cheng et al. (2020). As recommended by He et al. (2021), we train 6000 epochs to achieve
well-optimized performance of Cheng et al. (2020). N is set as 128 for the 3 lower-rate
models, and set as 192 for the 3 higher-rate models. We decay the learning rate to 5e-5
after training 3000 epochs.
C.2 Evaluation
Following prior works, we use Kodak (Kodak, 1993) and Tecnick (Asuni & Giachetti, 2014) as
our test dataset. The results of image compression are shown as rate-distortion curves. Following
previous works, we use PSNR and MS-SSIM (Wang et al., 2003) as the distortion metric to mea-
sure the reconstruction quality. We calculate bits-per-pixel (BPP) as the rate metric to measure the
compressed file size.
C.3 Calibration
We use a calibration dataset to calculate the activation quantization steps. To avoid data leakage,
we use DIV2K (Agustsson & Timofte (2017)) as the calibration dataset. We center-crop each one
of 900 images in DIV2K to 256 × 256 pixels and feed them into the quantization pipeline with a
batch-size of 32.
Recommended by Nagel et al. (2021), we search the weight quantization step using a grid search
method. Given full-precision weight vector w, which is represented by floating point, we minimize
its reconstruction mean-square-error to find the optimal quantization step sw :
Sw = arg min ∣∣W - w∣∣2	s.t. SW ∈ {sι, s2,..., SN}	(43)
sw
where N is the volume of the search space and W is the reconstructed vector W which is quantized
with factor Sw . This approach can be directly applied on each convolution filter wi to conduct the
per-channel weight quantization.
3https://groups.google.com/g/tensorflow- compression/c/LQtTAo6l26U/m/
cD4ZzmJUAgAJ
16
Under review as a conference paper at ICLR 2022
Figure 8: Comparison of quantized Minnen et al. (2018) w/ and w/o Brecq weight rounding recon-
struction. The results are evaluated on Kodak.
To obtain the full-precision activation, we feed a batch of calibration data to the model before quanti-
zation and store the floating-point activation outputs. And the range of saved full-precision activation
u will be used to obtain step su and zero-point zu with Min-Max approach:
=(max U - min U)
SU =	2B-1	(44)
zu = - su-1 min u	(45)
C.4 Effects of weight rounding reconstruction.
Method	w/o reconstruction	AdaRound (Nagel et al., 2020)	BreCq (Li et al., 2020)
Minnen et al. (2018)	1.329%	0.741%	0.663%
Cheng et al. (2020)	0.607%	0.415%	0.416%
Table 4: Relative BPP increment on Kodak when using (or not) different weight rounding recon-
struction approaches, compared with full-precision model.
We evaluate quantization results with and without weight rounding reconstruction, i.e.
AdaRound (Nagel et al., 2020) or Brecq (Li et al., 2020). Shown in Figure 8 and Table 4, the direct
quantization without per-layer reconstruction has achieved almost no performance loss at lower bit
rates (BPP< 0.4). The rate increment at higher bit rates can be compensated by Brecq. Brecq is
directly performed on weight offline, it is relatively costless and we adopt it to establish our PTQ
baseline.
We implement Brecq according to the open-source code provided by the authors. We view the hyper
synthesizer, context model and parameter network as three blocks to perform weight reconstruction,
i.e. we adopt the per-block reconstruction mentioned in Li et al. (2020). We use MSE as the object
instead of FIM for simplicity. We use a batch-size of 32, and learning rate of 1e-3 with Adam to
perform the gradient descent, tuning each block for 20000 iterations.
C.5 More rate-distortion results
For completeness, following existing work (Balle et al., 2018; Minnen et al., 2018; Cheng et al.,
2020; He et al., 2021), we further report RD performance evaluated on Tecnick (Figure 10) and RD
performance of models optimized on MS-SSIM (Figure 9). After the quantization, the models keep
comparable performance.
17
Under review as a conference paper at ICLR 2022
OQP U 一 IAnSSSIAl
Influence of quantization to models with MS-SSIM optimization. Evaluated on Kodak.
Figure 9:
BNSd
Figure 10: The same models as Figure 5(a) evaluated on Tecnick.
αNSel
RD curves of models with parallel context models, quantized or not. Evaluated on Kodak.
Figure 11:
18
Under review as a conference paper at ICLR 2022
C.6 Orthogonality with parallel context model
An issue of learned image compression with context models is the inefficiency of serial decoding.
He et al. (2021) proposes to address this problem by developing a checkerboard-shaped parallel
context modeling scheme, instead of the original serial autoregressive method. We also investigate
the quantization of Minnen et al. (2018) and Cheng et al. (2020) with this parallel context adaption.
As shown in Figure 11, PTQ still performs well without hurting the RD performance, making the
learned image compression pragmatic.
D Learned image compression
D.1 Overview
(a) Auto-encoder
(b) Hyperprior
(c) Joint autoregressive and
hyperprior
Figure 12: Operation diagrams of different learned image compression architectures. (a) an auto-
encoder like model (Bane et al., 2017) (b) Scale hyperprior model. ha and hs represent hyper
analysis and synthesis while Z denotes the hyperprior (Bane et al., 2018). (c) Joint autoregressive
and hyperprior model. C denotes the context model (Minnen et al., 2018).
As shown in Figure 12(a), the widely adopted framework of auto-encoder like learned image com-
pression proposed by Bane et al. (2017) is similar to transform coding (Goyal, 2001). During en-
coding, the input image x is mapped to the latent representation y through a parametric analysis
transform ga(∙). After the round-to-nearest quantization, the quantized y is used as coding symbols,
which will be encoded into the bitstream later. To losslessly encode y into a short enough bitstream,
an entropy model Py (y) is introduced to fit the probability mass of y, so that We can use entropy
encoders. With the same entropy model and an entropy decoder, y is decoded from the compressed
data stream, and fed to a learned synthesis transform gs (∙) to produce the reconstructed image X.
To train the auto-encoder like model with stochastic gradient descent, a uniform noise estimator
is introduced to replace the rounding operation (whose gradient is zero almost everywhere) during
training. It produces noisy vector y = y + r where r 〜 U(-0.5,0.5), to approximate the quantized
y during training. Thus, We can fit a continuous distribution Py (y), which is modeled as a segmented
linear parametric model in Bane et al. (2017). Hereinafter we always use X, y and Z to represent
X|X, y|y and Z|Z for simplicity.
By supervising the entropy, the training of learned image compression model is formulated as rate-
distortion optimization. The loss function can be expressed as:
L = R + λ ∙ D
=E[-lOg2Py (y)] + λ ∙ E[d(x, X)]
(46)
where λ is the coefficient to trade-off rate and distortion. d(x, X) denotes the distortion between
the original image X and the reconstructed image X where mean squared error (MSE) is the most
common choice.
19
Under review as a conference paper at ICLR 2022
Shown in Figure 12(b), in Balle et al. (2018), the entropy of y is estimated using a conditioned
Gaussian scale model (GSM) by introducing hyper latent z as side information:
Py∣z(yi∣Z) = [N(0,σ2) * U(-0.5, 0.5)] (¢)
∕∙yi + 0.5
=	N(0, σi2)(y)dy
Jyi-0.5
Py∣z(y Iz) = Y Py|z(yi|z)
i
(47)
where z is the hyperprior calculated from y with a hyper analyzer ha(∙) and each σ% is inferred
from z by a hyper synthesizer h§(∙). By encoding and decoding z independently, this hyperprior is
used as side-information to perform a forward-adaptive coding, conditioning on which the entropy
of each y% can be better modeled.
Shown in Figure 12(c), in Minnen et al. (2018) backward-adaption is also introduced, forming a joint
autoregressive and hyperprior coding scheme. Provided that the currently en/de-coding symbol is
yi, We can predict it with the already visible symbols yj<i which have been en/de-coded before y%.
Further adopting a mean-scale Gaussian entropy model which extends GSM, we can achieve a more
flexible entropy model:
py∣^,yj‹i(yi|z, yj<i) = [N(μi,σ2) * U(-0.5,0.5)] (yi)
∕∙yi + 0.5
Jyi-0.5
N (μi,σi')(y)dy
(48)
where element-wise entropy parameters μ% and σi are jointly predicted from context model, hyper
synthesizer and parameter network as highlighted in Figure 2. Cheng et al. (2020) further introduces
Gaussian mixture model (GMM) with K components as the entropy model:
py∣z,yj<i (yi ∣z, yj<i
K	ryi+0.5
) = X /i-0.5 -
, σi2(k) )(y)dy
(49)
subject to PK=I ∏(k) = L The entropy parameters π(k), μ(k), σ(k) are still jointly predicted from
context model, hyper synthesizer and parameter network. Recently, this GMM-based optimization
is further promoted by introducing global context modeling and grouped context modeling (Guo
et al., 2021), which outperforms VVC/VTM on both PSNR and MS-SSIM.
D.2 Entropy coding with arithmetic coders and the inconsistency issue
In learned image compression, we usually adopt arithmetic en/de-coder (AE/AD) or its variants,
range coder (Martin, 1979) and asymmetric numeral systems (ANS), to compress/decompress the
symbols y to/from bitstream. Using AE as an example, during encoding AE requires both y and
its cumulative distribution function (CDF) Cy as input. AE will allocate bits for each latent yi
according to the CDF. The same CDF should be fed to AD during decoding to correctly decompress
y. This is exactly what we are investigating in this work: making the CDF estimated by entropy
model consistent across different platforms to ensure the correct decoding of entropy coding.
Calculating CDF relies on the output of parameter network, which is continuous floating-point rep-
resentation. Thus, the calculation of CDF is usually expensive and non-deterministic floating-point
arithmetic. For instance, CDF for a discrete mean-scale Gaussian distribution (Balle et al., 2018) is:
Cy(y;μ,σ) = XZ	N (μ,σ2) (y)dy = φ (y_μ + 0.5]	(5O)
-∞ Jy-0.5	σ σ J
where y is one element in the quantized integer latent representations y (we omit the subscript i for
simplicity). Φ(∙) is the CDF of standard Gaussian distribution and is often calculated from the stan-
dard Gaussian probability density function ndtr4 or the error function erfc5, which is often approx-
4https://github.com/tensorflow/probability/blob/v0.14.1/tensorflow_
probability/python/distributions/normal.py#L199
5https://github.com/InterDigitalInc/CompressAI/blob/v1.1.8/compressai/
entropy_models/entropy_models.py#L577
20
Under review as a conference paper at ICLR 2022
imated by interpolation algorithms. Therefore, even after quantizing the parameter network output:
the mean μ and the standard deviation (STD) σ, the calculation of CDF is still non-deterministic.
Existing implementations pre-compute and store limited number of CDFs into look-up-tables
(LUTs) to address this problem. By directly distributing the saved CDF tables across various plat-
forms, the inconsistency in calculating CDF is eliminated. To conduct the LUT-based CDF query,
the mean and STD should be discretized to limited sets of values {μ} and {σ}. Otherwise, the non-
constrained parameters will generate infinite CDFs, which cannot be stored. We will discuss this
discretization in the following section, Appendix D.3.
Encoding
(Platform Alice)
ParamNet
σ
Decoding
(Platform Bob)
→ discretization
I floating-point
discretized value
Figure 13: When the cross-platform inconsistency occurs, the discretized σ (and also μ) may become
different between the sender Alice and the receiver Bob. Thus, the corresponding CDF index i and
i + 1 differ, which ends up with failed decoding.
When the calculation inconsistency occurs, the discretized parameters and their corresponding in-
dexes may be different across encoder and decoder (Figure 13). When encoding symbol y with the
inferred STD σ, for instance, Alice discretizes σ to the i-th sampling point σi. Then Alice will
use the i-th CDF to allocate bits for y and encode it. After all the symbols have been encoded,
Alice send the bitstream to Bob who will decode the symbols on another platform. Because of the
non-determinism, Bob infers the entropy parameter of y as σ + ∆σ with error ∆σ different from
Alice. Thus, the discretized parameter is σi+1 with corresponding index i + 1. Then Bob tries to
incorrectly decode y with the (i + 1)-th CDF instead of the i-th. Because the arithmetic coding is
of a recursion form, the failed decoding of symbol y results in failed decoding of all the following
symbols. It corrupts the decoded image, as shown in Figure 1.
D.3 Parameter discretization for CDF indexing
As mentioned in Appendix D.2, we should discretize the entropy parameters for LUT query. Balle
et al. (2019) proposes to constrain each predicted σ to values in a limited discrete set:
ʌ _ lθg(σmaχ) - lθg(σmin)
σ =	L - 1
iσ
log(σ) - log(σmin)
∆σ
(51)
σ = eχp iiσδ0 + log(σmin))
where the lower and upper bounds σmin , σmax are constant values. The input σ should get clipped
into range [σmin, σmax] and the clipping is omitted in above formula for simplicity. The discretizing
21
Under review as a conference paper at ICLR 2022
level L is set to 64. Since each index iσ corresponds to a particular σ which will generate a CDF,
we need to obtain the indexes in order to perform a LUT-based CDF query. In models using integer
networks proposed in Bane et al. (2019), the parameter network outputs the 6-bit integer index
iσ directly. But in uniform quantization-based approaches, the parameter network instead outputs
dequantized fix-point σ. Thus, we need extra map the fix-bit output to the integer index of LUT. Sun
et al. (2021) obtains iσ by comparing the fix-point value With pre-computed discretized σ.
The discretization of means has been addressed in Sun et al. (2021), with an observation that:
Cy(y； μ,σ) = Cy(y - bμc; μ - bμc ,σ)	(52)
where μ -[μ[ ∈ [0,1) is the decimal part of μ. Thus, We can just uniformly discretize the decimal
part of μ to M levels and store a limited number of CDFs:
iμ = d(μ - bμC)Mc, μ = iμMT + [μ1	(53)
Therefore, We can obtain LM pairs of (iμ,iσ), corresponding to LM CDFs. For all CDFs, We
pre-compute their function values on the input range {-R, -R + 1, . . . , 0, . . . , R - 1, R} and save
them as LUTs with lengths of 2R + 2, where each table includes an extra element representing
ending-of-bitstream.
E Algorithms
E.1 Parameter discretization
The discretization method described in eq. 12 is used to convert the 16-bit network output to CDF
indexes. We give its element-wise description with pseudo-code in Algorithm 1.
Algorithm 1: Binary logarithm discretization with interpolation
Input: 16-bit integer q
Output: Corresponding CDF index iσ
1
2
3
4
5
6
7
8
9
10
/* Clip q with lower and upper bounds	*/
if q < 8 then
Lq :=8	// σmin = 0.125 quantized by step size 2-6
else if q > 2048 then
L q := 2048	// σmaχ = 32 quantized by step size 2-6
b := IntLog2(q)
ʌ _
i := b — 3
e1 := 1 << b
e2 := 1 << (b - 3)
j ：= (q — eι + e2 — 1)∕e2
iσ :=8 Xi + j
// round-up integer-division by adding e2 — 1
E.2 Deterministic en/de-coding with Gaussian mixture model
For simplicity, in this section We use ik to represent thejoint outer index iμL + iσ for kth Gaussian
component. We use CDFH to denote the outer query by ik and Ck[∙] to denote the inner query by
y — bμkC. The output CDF value Cy in Algorithm 2 is the frequency cumulate below y.
The encoding is described in Algorithm 3, where Line 2 denotes symbol y bigger than
(max bμC) + R where the CDF value is CDFmaχ, or smaller than (min [μ1) — R where the
CDF value is zero. In this case, Line 3 and 5 encode symbol —R as a placeholder in AE
(ArithmeticEnc(SymboLloWeJcumulate, upper .cumulate, state)), while Line 4 encodes y us-
ing Golomb coding (GolombEnc(symbol, state). The decoding process is given in Algorithm 4,
where the reverse process is performed.
22
Under review as a conference paper at ICLR 2022
1
2
3
4
5
6
7
8
9
10
11
12
1
2
3
4
5
6
7
8
E.3 Our faster vectorized implementation of comparison-based discretization
Existing implementation of comparison-based discretization adopted by Sun et al. (2021) is for-
loop manner, which is somewhat inefficient. For a fair comparison, we further provide a vectorized
implementation. Its PyTorch code is like:
#	initialization
scale_table = np.exp(np.Iinspace(np.log(0.11), np. log(256), 64))
scale_table = torch . tensor( scale_table [: -1]). CUda ()
scale_table = scale_table [None, None, None, None,:]
#	rUnning
sigma_expand = sigma . UnsqUeeze (-1)
lut_index = (Sigma_expand > scale_table ).sum(-1)
Algorithm 2: Deterministic GMM CDF indexing (CDFIndex)
Input: symbol y, CDF index iι... iκ, round-down means [μι] ... [μκC, quantized weights
qπ1 . . . qπK
Output: CDF value Cy
Data: CDF LUTs CDF[0], . . . , CDF[64], Upper boUnd of freqUency cUmUlate CDFmax, range
bound R
Cy = 0
for k in 1 . . . K do
P = y - bμkC
Ck = CDF[ik]
if p ≥ R then
L Ck := CDFmaX
else if p ≤ -R then
LCk ：=o
else
L Ck := Ck[p + R] // shift R to obtain non-negative query index
Ck := qπk × Ck
Cy ：= Cy + Ck
Algorithm 3: Encoding with deterministic GMM
Input: symbol y, presently coder state t, CDF index iι ...iκ, round-down means
bμιC ... bμκC, quantized weights q∏ι ... q∏κ
Output: updated state t
Data: upper bound of frequency cumulate CDFmaX
Cy ：= CDFIndex(y,iι ...iκ, [μιC ... [μκC , q∏ι ...q∏κ)
if Cy = PPk=ι qπCDFmaX or Cy = 0 then
C(I-R) ：= CDFIndex( -R + 1,i1 ... iK, bμl1 ... bμKC ,qπι ... q∏κ )
GolombEnc(y,t)
t := ArithmeticEnc(-R, 0, C(I-R),t)
else
Cy+1 ：= CDFIndex(y +1,iι ...iκ, [μι] ... [μκ C ,q∏ι ... q∏κ)
t ：= ArithmeticEnc(y,Cy,Cy+ι,t)
23
Under review as a conference paper at ICLR 2022
Algorithm 4: Decoding with deterministic GMM
Input: presently decoder state t, CDF index iι... iκ, round-down means [μιC ... [μκC,
quantized weights qπ1 . . . qπK
Output: decoded symbol y, updated state t
Data: upper bound of frequency cumulate CDFmax
/* Search-based y decoding. This can be replaced by a binary
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
search	*/
clo := GetDecoderCDFLow(t)	
y := -R	
for y in -R . . . R do	
Cy := CDFIndex(y, iι... iκ, bμιc ... bμκC , q∏ι ... q∏κ)	
Cy+1 := CDFIndex(y +1,iι ...iκ, [μι] ... [μκC ,q∏ι ... q∏κ)	
if Cy ≤ Clo < Cy+1 then	
Ly:= y	
/* Update the decoder state			*/
if y = -R then			
/* The Golomb coding is adopted			*/
C(I-R) := CDFIndex(—R + 1,iι ... iκ, [μιC .	.bμκC, q∏ι.	. . qπK )	
t := ArithmeticDecUPd(0, c(1-R), t)			
y := GolombDec(t)			
else
Cy= CDFIndex(y,iι ...iκ, [μιC ... [μκC ,q∏ι ... q∏κ)
cy+ι := CDFIndex(y + 1,iι... iκ, bμιc ... bμκc , q∏ι ... q∏κ)
t := ArithmeticDecUPd(c^,cy+ι, t)
24