Under review as a conference paper at ICLR 2022
Learning to Shape Rewards using a Game of
Two Partners
Anonymous authors
Paper under double-blind review
Ab stract
Reward shaping (RS) is a powerful method in reinforcement learning (RL) for
overcoming the problem of sparse or uninformative rewards. However, RS typically
relies on manually engineered shaping-reward functions whose construction is
time-consuming and error-prone. It also requires domain knowledge which runs
contrary to the goal of autonomous learning. We introduce Reinforcement Learning
Optimising Shaping Algorithm (ROSA), an automated RS framework in which the
shaping-reward function is constructed in a novel Markov game between two agents.
A reward-shaping agent (Shaper) uses switching controls to determine which states
to add shaping rewards and their optimal values while the other agent (Controller)
learns the optimal policy for the task using these shaped rewards. We prove that
ROSA, which easily adopts existing RL algorithms, learns to construct a shaping-
reward function that is tailored to the task thus ensuring efficient convergence
to high performance policies. We demonstrate ROSA’s congenial properties in
three carefully designed experiments and show its superior performance against
state-of-the-art RS algorithms in challenging sparse reward environments.
1	Introduction
Reinforcement learning (RL) offers the potential for autonomous agents to learn complex behaviours
without the need for human intervention [39]. Despite the notable success of RL in a variety domains
[8, 31, 35, 24], enabling RL algorithms to learn successfully in numerous real-world tasks remains
a challenge [41]. A key obstacle to the success of RL algorithms is the requirement of a rich reward
signal that can guide the agent towards an optimal policy [7]. In many settings of interest such as
physical tasks and video games, rich informative signals of the agent’s performance are not readily
available [14]. For example, in the video game Super Mario [33], the agent must perform sequences
of hundreds of actions while receiving no rewards for it to successfully complete its task. In this
setting, the sparse reward provides infrequent feedback of the agent’s performance. This leads to
RL algorithms requiring large numbers of samples (and high expense) for solving problems [14].
Consequently, there is great need for RL techniques that solve these problems efficiently.
Various biological systems have mechanisms that produce rewards for activities that present little
or no direct biological utility. Such mechanisms, which have been fashioned over millions of years
are designed to provide intrinsic rewards to promote behaviours that improve chances of outcomes
with biological utility [1]. In RL, reward shaping (RS) is a tool to introduce intrinsic rewards known
as shaping rewards that supplement environment reward signals. These rewards can encourage explo-
ration and insert structural knowledge in the absence of informative environment rewards which can
improve learning outcomes [10]. In general however, RS algorithms assume hand-crafted and domain-
specific shaping functions whose construction is typically highly labour intensive. This runs contrary
to the aim of autonomous learning. Moreover, poor choices of shaping rewards can worsen the agent’s
performance [9]. To resolve these issues, a useful shaping reward must be obtained autonomously.
Inspired by naturally occurring systems, we tackle the problem of sparse and uninformative rewards
by developing a framework that autonomously constructs shaping rewards during learning. Our
framework, ROSA, works by introducing an additional RL agent, Shaper, that adaptively learns to
construct shaping rewards by observing Controller (the agent whose goal is to solve the environment
task) while Controller learns to solve its task. This generates tailored shaping rewards without the
1
Under review as a conference paper at ICLR 2022
need for domain knowledge or manual engineering. The shaping rewards supplement the environment
reward and promote effective learning, our framework therefore addresses the key challenges in RS.
The resulting framework is a two-player nonzero-sum Markov game (MG) [34] - an extension of
Markov decision process (MDP) that involves two independent learners with distinct objectives. In
our framework, the two agents that have distinct learning agendas, cooperate to achieve Controller’s
objective. This MG formulation confers various advantages:
1)	The shaping-reward function is constructed fully autonomously. The game also ensures the shaping
reward improves Controller’s performance unlike RS methods that can lower performance.
2)	By learning the shaping-reward function while Controller learns its optimal policy, Shaper learns
to adaptively facilitate Controller’s learning and improve Controller’s performance.
3)	Both learning processes converge so Controller learns the optimal value function for its task.
4)	ROSA learns subgoals [28] to decompose complex tasks and promote complex exploration patterns.
An integral component of ROSA is a novel combination of RL and switching controls [2, 22]. This
enables Controller to quickly determine useful states to learn to add and calibrate shaping rewards
(i.e. the states in which adding shaping rewards improve Controller’s performance) and disregard
others. This is in contrast with an RL controller (i.e. Controller) which must learn its best actions at
every state. This leads to Shaper quickly finding shaping rewards that guide Controller’s learning
process toward optimal trajectories (and away from suboptimal trajectories, c.f. Experiment 1).
For our two-player framework to succeed we have to overcome several obstacles. Solving MGs
involves finding a stable point in which each player responds optimally to the actions of the other.
In our MG, this stable point describes a pair of policies for which Shaper introduces a shaping
reward that improves performance and, with that, Controller executes an optimal policy for the
task. Tractable methods for solving MGs are rare with convergence ofMG methods being seldom
guaranteed except in a few special cases [42]. Nevertheless, using special features in the design of our
game, we prove the existence of a stable point solution of our MG. We then prove the convergence of
our learning method to the solution of the game and show that the solution coincides with the solution
of the Controller’s problem. This ensures Shaper learns a shaping-reward function that improves
Controller’s performance and that Controller learns the optimal value function for the task.
2	Related Work
Reward Shaping (RS) adds a shaping function F to supplement the agent’s reward to boost learning.
RS however has some critical limitations. First, RS does not offer a means of finding F . Second,
poor choices of F can worsen the agent’s performance [9]. Last, adding shaping rewards can change
the underlying problem therefore generating policies that are completely irrelevant to the task [20].
In [27] it was established that potential-based reward shaping (PBRS) which adds a shaping function
of the form F (st+1, st) = γφ(st+1) - φ(st) preserves the optimal policy of the problem. Recent
variants of PBRS include potential-based advice which defines F over the state-action space [13]
and approaches that include time-varying shaping functions [11]. Although the last issue can be
addressed using potential-based reward shaping (PBRS) [27], the first two issues remain. To avoid
manual engineering of F , useful shaping rewards must be obtained autonomously. Towards this [45]
introduce an RS method that adds a shaping-reward function prior which fits a distribution from data
obtained over many tasks. Recently, [16] use a bilevel technique to learn a scalar coefficient for an
already-given shaping-reward function. Nevertheless, constructing F while training can produce
convergence issues since the reward function now changes during training. [17]. Moreover, while F
is being learned the reward can be corrupted by inappropriate signals that hinder learning.
Curiosity based reward shaping aims to encourage the agent to explore states by rewarding the
agent for novel state visitations using exploration heuristics. One approach is to use state visitation
counts [29]. More elaborate approaches such as [6] introduce a measure of state novelty using the
prediction error of features of the visited states from a random network. [30] use the prediction error
of the next state from a learned dynamics model and [15] maximise the information gain about the
agent’s belief of the system dynamics. In general, these methods provide no performance guarantees
nor do they ensure the optimal policy (of the underlying MDP) is preserved. Moreover, they naively
reward exploration to unvisited states without consideration of the environment reward. This can lead
to spurious objectives being maximised (see Experiment 3 in §6).
Within these two categories, closest to our work are bilevel approaches for learning the shaping
function [16, 36]. Unlike [16] which requires a useful shaping reward to begin with, ROSA constructs
2
Under review as a conference paper at ICLR 2022
a shaping reward function from scratch leading to a fully autonomous method. Moreover, in [16,
36], the agent’s policy and shaping rewards are learned with consecutive updates. In contrast, ROSA
performs these operations concurrently leading to a faster, more efficient procedure. Also in contrast
to [16, 36], ROSA learns shaping rewards only at relevant states, this confers high computational
efficiency (see Experiment 2, §6)). As we describe, ROSA, which successfully learns the shaping-
reward function F , uses a similar form as PBRS. However in ROSA, F is augmented to include
the actions of another RL agent to learn the shaping rewards online. Lastly, unlike curiosity-based
methods e.g., [6, 30], our method preserves the agent’s optimal policy for the task (see Experiment 3,
§6) and introduces intrinsic rewards that promote complex learning behaviour (see Experiment 1, §6) .
3	Preliminaries & Notations
In RL, an agent sequentially selects actions to maximise its expected returns. The underlying problem
is typically formalised as a MDP hS, A, P, R, γi where S is the set of states, A is the discrete set of
actions, P : S × A × S → [0, 1] is a transition probability function describing the system’s dynamics,
R : S × A → R is the reward function measuring the agent’s performance, and the factor γ ∈ [0, 1)
specifies the degree to which the agent’s rewards are discounted over time [39]. At time t the system
is in state st ∈ S and the agent must choose an action at ∈ A which transitions the system to a
new state st+ι 〜 P(∙∣st, at) and produces a reward R(st, at). A policy π : S ×A → [0,1] is a
probability distribution over state-action pairs where ∏(a∣s) represents the probability of selecting
action a ∈ A in state s ∈ S. The goal of an RL agent is to find a policy π? ∈ Π that maximises its
expected returns given by the value function: vπ (S) = E[P∞=o Y tR(st, at)∣at 〜∏(∙∣st)] where Π is
the agent’s policy set. We denote this MDP by M.
A two-player Markov game (MG) is an augmented MDP involving two agent that simultaneously
take actions over many rounds [34]. In the classical MG framework, each agent’s rewards and the
system dynamics are now influenced by the actions of both agents. Therefore, each agent i ∈ {1, 2}
has its reward function Ri : S × A1 × A2 → R and action set Ai and its goal is to maximise its own
expected returns. The system dynamics, now influenced by both agents, are described by a transition
probability P : S × A1 × A2 × S → [0, 1]. As we discuss in the next section, ROSA induces a
specific MG in which the dynamics are influenced by only Controller.
In RS the question of which φ to insert has not been addressed. Moreover it has been shown that
poor choices of φ hinder learning [9]. Consequently, in general RS methods rely on hand-crafted
shaping-reward functions that are constructed using domain knowledge (whenever available). In
the absence of a useful shaping-reward function F, the challenge is to learn a shaping-reward
function that leads to more efficient learning while preserving the optimal policy. Naturally, we can
formalise the problem of learning such an F by constructing F as a parametric function of θ ∈ Rm :
F(st+1, st； θ) := γφ)(st+ι, θ) - φ(st, θ). Now the problem is to find θ? ∈ Rm for φ(s) = φ(s, θ?)
such that F(st+ι, St)=户(st+ι, st； θ?), i.e., we aim to find θ? that yields a useful shaping-reward
function. Determining this function is a significant challenge; poor choices can hinder the learning
process, moreover attempting to learn the shaping-function while learning the RL agent’s policy
presents convergence issues given the two concurrent learning processes [44]. Another issue is that
using an optimisation procedure to find θ? directly does not make use of information generated by
intermediate state-action-reward tuples of the RL problem which can help to guide the optimisation.
4	Our Framework
We now describe the problem setting, details of our framework, and how it learns the shaping-reward
function. We then describe Controller’s and Shaper’s objectives. We also describe the switching
control mechanism used by Shaper and the learning process for both agents.
To tackle the challenges described above, we introduce Shaper an adaptive agent with its own
objective that determines the best shaping rewards to give to Controller. Using observations of
the actions taken Controller, Shaper’s goal is to construct shaping rewards (which the RL learner
Controller cannot generate itself) to guide the Controller towards quickly learning its optimal policy.
To do this, Shaper learns how to choose the values ofan shaping-reward at each state. Simultaneously,
Controller performs actions to maximise its rewards using its own policy. Crucially, the two agents
tackle distinct but complementary set problems. The problem for Controller is to learn to solve the
3
Under review as a conference paper at ICLR 2022
task by finding its optimal policy, the problem for Shaper is to learn how to add shaping rewards to
aid Controller. The objective for Controller is given by:
∞
vπ,π (s) = E XYt(R(st,at)十 户(st,α2,st-i,α2-i))卜=so ,
t=0
where a 〜π is Controller S action, F is the shaping-reward function which is given by F(∙) ≡
φ(st, a2) - YTφ(st-1,a2-1) for any St, st-1 ∈ S and α2 〜 π2 is chosen by ShaPer (and α2 ≡
0, ∀t < 0) using the policy π2 : S × A2 → [0, 1] where A2 ⊂ Rp is the action set for Shaper. The
function φ : S × A2 → R is a continuous map that satisfies the condition φ(s, 0) ≡ 0 for any s ∈ S
(φ can be, for example, a neural network with fixed weights with input (s, a2), A2 can be for example
a set of integers {1, . . . , K}). Therefore, ShaPer determines the output of F (which it does through
its choice of at2). With this, ShaPer constructs a shaping-reward function which is tailored for the
specific setting. The transition probability P : S × A×S → [0, 1] takes the state and only Controller’s
actions as inputs. Formally, the MG is defined by a tuple G = hN, S, A, A2, P, R1 , R2, Yi where
the new elements are N = {1, 2} which is the set of agents, R1 := R + F is the new Controller
reward function which now contains a shaping reward F, the function R2 : S × A × A2 → R is
the one-step reward for ShaPer (we give the details of this function later) and lastly the transition
probability P : S × A × S → [0, 1] takes the state and only Controller action as inputs.
As Controller’s policy can be learned using any RL method, ROSA easily adopts any existing RL
algorithm for Controller. Note that unlike reward-shaping methods e.g. [27], the function φ now
contains an action term a2 which is chosen by ShaPer which enables the best shaping-reward function
to be learned online. The presence of an action term may spoil the policy invariance result in [27].
We however prove an policy invariance result (Prop. 1) analogous to that in [27] and show RIGA
preserves the optimal policy for M.
4.1	Switching Controls
So far ShaPer’s problem involves learning to construct shaping rewards at every state including
those that are irrelevant for guiding Controller. In order to increase the (computational) efficiency
of ShaPer’s learning process, we now replace the policy space for ShaPer with a form of policies
known as switching controls. This enables ShaPer to decide at which states to learn the value of
shaping rewards it would like to add. Therefore, now ShaPer is tasked with learning how to shape
Controller’s rewards only at states that are important for guiding Controller to its optimal policy.
This enables ShaPer to quickly determine its policy π2 and how to choose the values of F unlike
Controller whose policy must learned for all states.
Now at each state ShaPer first makes a binary decision to decide to switch on its shaping reward
F for Controller affecting a switch It which takes values in {0, 1}. This leads to an MG in which,
unlike classical MGs, ShaPer now uses switching controls to perform its actions.
The new
Controller objective is:
[p∞=0 Yt {R(st,at) + F(st,at; st-i,a2-i)It}], where I^
v1π,π2(s0,I0)
1 - Iτk, which is the
switch for the shaping rewards which is 0 or 1 (and It ≡ 0, ∀t ≤ 0) and {τk} are times that a switch
takes place for example if the switch is first turned on at state s5 then turned off at s7, then τ1 = 5
and τ2 = 7 (we will shortly describe these in detail). The switch It is managed by ShaPer, therefore
by switching It between 0 or 1, ShaPer activates or deactivates the shaping reward.
E
We now describe how at each state both the decision to activate a shaping reward and their magnitudes
are determined. Recall that a；〜∏2 determines the shaping reward through F. At any st, the decision
to turn on It and shape rewards is decided by a (categorical) policy g2 : S → {0, 1}. Therefore, g2
determines whether a (or no) Shaper policy ∏2 should be used to execute an action a2 〜∏2. It can
now be seen the sequence of times τk = inf{t > τk-1 |st ∈ S, g2(st) = 1} are rules that depend on
the state. Hence, by learning an optimal g2, Shaper learns the best states to activate F.
Summary of events:
At a time t ∈ 0, 1 . . .
4
Under review as a conference paper at ICLR 2022
•	Both players make an observation of the state St ∈ S.
•	Controller takes an action at sampled from its policy π.
•	ShaPer decides whether or not to activate the shaping reward using g2 : S → {0,1}
•	If g2(st)=0:
◦ The switch is not activated (It = 0). Controller receives a reward r 〜R(st, at) and
the system transitions to the next state st+、.
•	If g2(st) = 1:
◦	Shaper takes an action a2 sampled from its policy π2.
◦	The switch is activated (It = 1), Controller receives a reward R(st,at) +
F(st, a2; st-ι, a2-ι) X 1 and the system transitions to the next state st+、.
We set τk ≡ 0∀k ≤ 0 and aτ2k ≡ 0, ∀k ∈ N (aτ2k+1, . . . , aτ2k+1-1 remain non-zero) and a2k ≡
0 ∀k ≤ 0 and use the shorthand I(t) ≡ It .
4.2 The Shaper’ s Objective
The goal of Shaper is to guide Controller to efficiently learn to maximise its own objective (given
in M). The shaping reward F is activated by switches controlled by Shaper. As we later describe,
the termination times occur according to some external (probabilistic) rule. To induce Shaper to
selectively choose when to switch on the shaping reward, each switch activation incurs a fixed cost
for Shaper. The cost has two effects: first it reduces the complexity of Shaper problem since its
decision space is to determine which subregions of S it should activate the shaping rewards (and their
magnitudes). Second, it ensures that the information-gain from Shaper encouraging Controller to
explore a given set of states is sufficiently high to merit activating the stream of rewards. Given these
remarks the objective for Shaper is given by
∞∞
v2π,π (s0,I0) = E∏,∏2 X Yt Rι(st,It,at,a2,a2-ι) + X c(It,It-ι)δiT2k-ι + L(St)	.	(1)
t=0	k≥1
The objective encodes Shaper agenda, namely to maximise the expected return.1 Therefore, using
its shaping rewards, Shaper seeks to guide Controller towards optimal trajectories (potentially away
from suboptimal trajectories, c.f. Experiment 1) and enable Controller to learn faster (c.f. Cartpole
experiment in Sec. 6). The function c : {0, 1}2 → R<0 is a strictly negative cost function which
imposes a cost for each switch and is modulated by the Kronecker-delta function δτt	which is 1
whenever t = τ2k-1 and 0 otherwise (this restricts the costs to only the points at which the shaping
reward is activated). Lastly, the term L : S → R is a Shaper bonus reward for when Controller
visits infrequently visited states and tends to 0 as the states are revisited. For this there are various
possibilities e.g. model prediction error [37], count-based exploration bonus [38].
With this, Shaper constructs a shaping-reward function that supports Controller’s learning which
is tailored for the specific setting. This avoids inserting hand-designed exploration heuristics into
Controller’s objective as in curiosity-based methods [6, 30] and classical reward shaping [27]. We
later prove that with this objective, Shaper’s optimal policy maximises Controller’s (extrinsic) return
(Prop. 1). Additionally, we show that the framework preserves the optimal policy ofM.
There are various possibilities for the termination times {τ2k} (recall that {τ2k+1} are the times
which the shaping reward F is switched on using g2). One is for Shaper to determine the sequence.
Another is to build a construction of {τ2k} that directly incorporates the information gain that a state
visit provides — we defer the details of this arrangement to Sec. 10 of the Appendix.
4.3 The Overall Learning Procedure
The game G is solved using our multi-agent RL algorithm (ROSA). In the next section, we show the
convergence properties of ROSA. Here, we first give a description of ROSA (the full code is in Sec. 8
of the Appendix). The ROSA algorithm consists of two independent procedures: Controller learns its
own policy while Shaper learns which states to perform a switch and the shaping reward magnitudes.
In our implementation, we used proximal policy optimization (PPO) [32] as the learning algorithm
1Note that we can now see that R ≡ R(st, at) + F(St,a2; st-ι,at2-1)It + Pk∞≥1 c(It, It-1)δτt2k-1.
5
Under review as a conference paper at ICLR 2022
for all policies: Controller’s policy, switching control policy, and the reward magnitude policy. For
ShaPer L term We used L(St) ：= Ilh(St) - h(St)k2 as in RND [6] where h is a random initialised,
fixed target network while h is the predictor network that seeks to approximate the target network.
We constructed 户 using a fixed neural network f : Rd → Rm and a one-hot encoding of the action
of Shaper. Specifically, φ(st, a2) ：= f (St) ∙ i(a2) where i(a2) is a one-hot encoding of the action a2
picked by ShaPer. Thus, F(St, a2; st-ι, α2-ι) = f (st) ∙ i(a2) - YTf (st-ι) ∙i(at2-1). The action
set of Shaper is thus A2 := {0, 1, ..., m} where each element is an element of N, and π2 is a MLP
π2 : Rd 7→ Rm . Precise details are in the Supplementary Materials Section 8.
1
2
3
4
5
6
7
8
9
10
Algorithm 1: Reinforcement Learning Optimising Shaping Algorithm (ROSA)
Input: Initial Controller policy π0, Shaper policies g20, π02, RL learning algorithm ∆
Output: Optimised Controller policy π*
for t = 1, T do
Given environment state St, sample at from π(St) and obtain St+1, rt+1 by applying at to
environment
Evaluate g2(St) according to Prop. 2
if g2 (St) = 1 then
Shaper samples an action a2+ι 〜π2Hst+1)
Shaper computes rii+ι =户(st, a2, st+ι, a2+ι),
Set shaped reward r = rt+1 + rti+1
else
L Set r = rt+ι
Update π, g2,∏2 using (st, at, r, st+ι) and ∆ // Learn the individual policies
5 Convergence and Optimality of Our Method
The ROSA framework enables Shaper to learn a shaping-reward function with which Controller
can learn the optimal policy for the task. The interaction between the two RL agents induces two
concurrent learning processes which can occasion convergence issues [44]. We now show that
our method converges and the solution ensures higher performing Controller policy than would be
achieved by solving M directly. To do this, we first study the stable point solutions of G .
Unlike MDPs, the existence of a stable point solution in Markov policies is not guaranteed for MGs
[5] and is rarely computable.2 MGs also often have multiple stable points that can be inefficient
[25]; in G the outcome of such stable point profiles would be a poor performing Controller policy. To
ensure the framework is useful, we must verify that the solution of G corresponds to M. We solve
these challenges with the following scheme: [A] The method preserves the optimal policy of M. [B]
A stable point of the game G in Markov policies exists and is the convergence point of ROSA. [C]
The convergence point of ROSA yields a payoff that is (weakly) greater than that from solving M
directly. [D] ROSA converges to the stable point solution of G.
We now prove [A] which shows the solution to M is preserved under the influence of Shaper:
Proposition 1 The following statements hold:
2
i)	max v1π,π (S) = max v1π (S), ∀S ∈ S, ∀π2 ∈ Π2, where v1π (S) = E [Pt 0 γtR(St, at)].
π∈Π 1	π∈Π 1	1	t=0
ii)	The Shaper’s optimal policy maximises v1π (S) for any S ∈ S.
Result (i) says that the Controller’s problem is preserved under the influence of the Shaper. Moreover
the (expected) total return received by the agents is that from the environment (extrinsic rewards).
Result (ii) establishes that Shaper’s optimal policy induces Shaper to maximise its (Controller’s)
extrinsic total return. The result is established by a careful adaptation of the policy invariance result
in [27] to our multi-agent switching control framework in which the shaping reward is no longer
added at all states. Building on Prop. 1, we deduce the following result:
2Special exceptions are team MGs where agents share an objective and zero-sum MGs [34].
6
Under review as a conference paper at ICLR 2022
Corollary 1 ROSA preserves the MDP M. In particular, let (∏ 1,∏2) be a stable point policy profile3
of the MG induced by ROSA G then ∏1 is a solution to the MDP M.
Therefore, the introduction of Shaper does not alter the fundamentals of the problem.
We noW shoW that G belongs to a special class of MGs Which possess a stable point Markov policies
Which can be computed as a limit point of a sequence of Bellman operations. We later exploit this
result to prove the convergence of ROSA.
We begin by defining some objects Which are central to the analysis. For any π ∈ Π
and π 2 ∈ Π2 , given a function V π,π2 : S × N → R, We define the intervention
2	22
operator Mπ,π by Mπ,π VKK(STk ,I(Tk)) ：= R^1(sτk ,I(Tk),aτk ,a2k, ∙) + C(Ik,Ik-ι) +
γPs0∈SP(s0; aτk, S)Vπ,π2 (s0,I(τk+ι)) for any $,卜 ∈ S and ∀τk where 4丁卜〜π(∙∣Sτk) and where
a2k 〜π2(∙∣Sτk). We define the Bellman operator T of the game G by TVπ,π2(STk,I(Tk))：=
max Mπ,π2 A(Sτk, I(Tk)), R(Sτk, a) + γmax Ps0∈S P(S0; a, Sτk)A(S0, I(Tk)) . Given a value
function {Vi}i∈{1,2}, the quantity MVi measures the expected future stream of rewards for agent i
after an immediate switch minus the cost of switching.
We now show that a stable point of G can be computed using dynamic programming:
Theorem 1	Let V ： S ×N → R then the game G has a stable point which is a given by lim TkVπ =
k→∞
?
sup Vπ = Vπ , where Tn is a stable policy profilefor the MG, G.
π ∈Π
Theorem 1 proves that the MG G (which is the game that is induced when Shaper influences
Controller) has a stable point which is the limit of a dynamic programming method. In particular,
it proves the that the stable point of G is the limit point of the sequence T1V, T2V, . . . ,. Crucially,
(by Corollary 1) the limit point corresponds to the solution of the MDP M. Theorem 1 is proven by
firstly proving that G has a dual representation as an MDP whose solution corresponds to the stable
point of the MG. Theorem 1 enables a distributed Q-learning method [4] to tractably solve the MG.
Having constructed a procedure to find the optimal Controller policy, our next result characterises
Shaper policy g2 and the optimal times to activate F.
Proposition 2 The policy g2 is given by the following expression: g2(st) = H(Mπ,π2 V π,π2
—
V π,π2)(st, It), ∀(st, It) ∈ S × {0, 1}, where V is the solution in Theorem 1 and H is the Heaviside
function, moreover Shaper's switching times are Tk = inf{τ > τ^k-ι∣Mπ,π Vπ,π = Vπ,π }.
Hence, Prop. 2 also characterises the (categorical) distribution g2 . Moreover, given the function V ,
the times {τk} can be determined by evaluating if MV = V holds. In general, introducing shaping
rewards may undermine learning and worsen overall performance. We now prove that the ROSA
framework introduces an shaping rewards that yield higher total (environment) returns for Controller
as compared to solving M directly ([C]).
Proposition 3 Controller’s (extrinsic) expected return v1π,π whilst playing G is (weakly) higher
2
than v∏, the (extrinsic) expected return for M i.e. v1, (s, ∙) ≥ v∏ (s), ∀s ∈ S.
Prop. 3 shows that the stable point of G improves outcomes for Controller. Unlike reward shaping
methods in general, the stable points generated never lead to a reduction to the total (environment)
return for Controller as compared to its total return without F. Note that by Prop. 1, Theorem 3
compares the environment (extrinsic) rewards accrued by the agents so that the presence of Shaper
increases the total expected environment rewards.
We now prove the convergence to the solution with (linear) function approximators. In what follows,
We define a projection Π on a function A by: ΠΛ := arg min ∣∣TV - A，
Λ ∈{Ψr∣r∈Rp}
Theorem 2	ROSA converges to the stable point of G, moreover, given a set of linearly independent
basis functions Ψ = {ψ1, . . . , ψp} with ψk ∈ L2, ∀k, ROSA converges to a limit point r? ∈ Rp which
3By stable point profile We mean a configuration in Which no agent can increase their expected return by
deviating unilaterally from their policy given the other agents’ policies, i.e. a Markov perfect equilibrium [12].
7
Under review as a conference paper at ICLR 2022
is the unique solution to ∏F(Ψr?) = Ψr? where F is defined by: FA := RI + YP max{MΛ, Λ}
where r? satisfies: kΨr? 一 Q?k ≤ (1 一 γ2)-1/2 k∏Q? 一 Q?∣∣.
Theorem 2 establishes the solution to G can be computed using ROSA. This means that Shaper
converges to a shaping-reward function that necessarily improves Controller’s performance and
Controller learns the optimal value function for the task. Secondly, the theorem establishes the
convergence of ROSA to the solution using (linear) function approximators. Lastly, the approximation
error is bounded by the smallest error that can be achieved given the basis functions.
6 Experiments
We performed a series of experiments to test if ROSA (1) learns a beneficial shaping-reward function
(2) decomposes complex tasks into sub-goals, and (3) tailors shaping rewards to encourage Controller
to capture environment rewards (as opposed to merely pursuing novelty). In these tasks, we com-
pared the performance of our method to random network distillation (RND) [6], intrinsic curiosity
module (ICM) [30], learning intrinsic reward policy gradient (LIRPG) [43], bi-level optimization
of parameterized reward shaping (BiPaRS-IMGL) [16]4 and vanilla PPO [32]. We then compared
our method against these baselines on performance benchmarks including Sparse Cartpole, Gravitar,
Solaris, and Super Mario. Lastly, we ran a detailed suite of ablation studies (supplementary material).
1.	Beneficial shaping reward. Our
method is able to learn a shaping-
reward function that leads to improved
Controller performance. In particu-
lar, it is able to learn to shape rewards
that encourage the RL agent to avoid
SUboPtimal - but easy to learn - poli-
cies in favour of policies that attain
the maximal return. To demonstrate
this, we designed a Maze environment
with two terminal states: a suboPtimal
goal state that yields a reward of 0.5
and an oPtimal goal state which yields
a reward of 1. In this maze design,
the sub-oPtimal goal is more easily
Figure 1: Left. ProPortion of oPtimal and suboPtimal goal
arrivals. Our method has a marked inflection (arrow) where
arrivals at the sub-oPtimal goal decrease and arrivals at the oP-
timal goal increase. Shaper has learned to guide Controller
to forgo the suboPtimal goal in favour of the oPtimal one.
Right. HeatmaP showing where our method adds rewards.
reached. A good shaPing-reward function discourages the agent from visiting the sub-oPtimal goal.
As shown in Fig. 15 our method achieves this by learning to Place high shaPing rewards (dark green)
on the Path that leads to the oPtimal goal.
2.	Subgoal discovery. We used the Subgoal Maze
introduced in [21] to test if ROSA can discover sub-
goals. The environment has two rooms seParated by
a gateway. To solve this, the agent has to discover the
subgoal of reaching the gateway before it can reach
the goal. Rewards are 一0.01 everywhere excePt at
the goal state where the reward is 1. As shown in Fig.
2, our method successfully solves this environment
whereas other methods fail. Our method assigns im-
Portance to reaching the gateway, dePicted by the
heatmaP of added shaPed rewards.
Figure 2: Discovering subgoals on Subgoal
Maze. Left. Learning curves. Right. HeatmaP
of shaPing rewards guiding Controller to gate.
3.	Ignoring non-beneficial shaping reward. Switching control gives our method the Power to
learn when to attend to shaPing rewards and when to ignore them. This allows us to learn to ignore
“red-herrings”, i.e., unexPlored Parts of the state sPace where there is no real environment reward, but
where surPrise or novelty metrics would Place high shaPing reward. To verify this claim, we use a
modified Maze environment called Red-Herring Maze which features a large Part of the state sPace
that has no environment reward, but with the goal (and accomPanying real reward) in a different Part
4BiPaRS-IMGL requires a manually crafted shaPing-
reward (only available in CartPole).
5The sum of curves for each method may be less that 1 if the agent fails to arrive at either goal.
8
Under review as a conference paper at ICLR 2022
of the state space. Ideally, we expect that the reward shaping method can learn to quickly ignore the
large part of the state space. Fig. 3 shows that our method outperforms all other baselines. Moreover,
the heatmap shows that while RND is easily dragged to reward exploring novel but non rewarding
states our method learns to ignore them.
Steps (1e3)
Figure 3: Red-Herring Maze. Ignoring non-beneficial shap-
ing reward. Left. Learning curves. Right. Heatmap of added
shaping rewards. ROSA ignores the RHS of the maze, while
RND incorrectly adds unuseful shaping rewards there.
Learning Performance. We com-
pared our method with the baselines
in four challenging sparse rewards en-
vironments: Cartpole, Gravitar, So-
laris, and Super Mario. These envi-
ronments vary in state representation,
transition dynamics and reward spar-
sity. In Cartpole, a penalty of -1 is
received only when the pole collapses;
in Super Mario Brothers the agent can
go for 100s of steps without encoun-
tering a reward. Fig. 4 shows learn-
ing curves. In terms of performance,
ROSA either markedly outperforms
the best competing baseline (Cartpole,
Gravitar) or is on par with them (So-
laris, Super Mario) showing that it is
robust to the nature of the environ-
ment and underlying sparse reward.
Moreover, ROSA does not exhibit the
failure modes where after good initial
S
CartpoIe
Gravitar
10
15
20
5
10
15
20
0
Steps(1e6)
BiPars-IMGL
(Harmful shaping reward)
400
StePs(1e3)
Solaris
一ROSA
—RND — ICM
—PPO - LIRPG
Figure 4: Benchmark performance.
BiPaRS-IMGL
(Good shaping reward)

performance it deteriorates. E.g., in Solaris both ICM and RND have good initial performance but
deteriorate sharply while ROSA’s performance remains satisfactory.
7 Conclusion
In this paper, we presented a novel solution method to solve the problem of reward shaping. Our
Markov game framework of a primary Controller and a secondary reward shaping agent is guar-
anteed to preserve the underlying learning task for Controller whilst guiding Controller to higher
performance policies. Moreover, our method is able to decompose complex learning tasks into
subgoals and to adaptively guide Controller by selectively choosing the states to add shaping rewards.
By presenting a theoretically sound and empirically robust approach to solving the reward shaping
problem, our method opens up the applicability of RL to a range of real-world control problems.
The most significant contribution of this paper, however, is the novel construction that marries RL,
multi-agent RL and game theory which leads to new solution method in RL. We believe this powerful
approach can be adopted to solve other open challenges in RL.
9
Under review as a conference paper at ICLR 2022
References
[1]	Gianluca Baldassarre. “What are intrinsic motivations? A biological perspective”. In: 2011
IEEE international conference on development and learning (ICDL). Vol. 2. IEEE. 2011,
pp.1-8.
[2]	Erhan Bayraktar and Masahiko Egami. “On the one-dimensional optimal switching problem”.
In: Mathematics of Operations Research 35.1 (2010), pp. 140-159.
[3]	Albert Benveniste, Michel Metivier, and Pierre Priouret. Adaptive algorithms and stochastic
approximations. Vol. 22. Springer Science & Business Media, 2012.
[4]	Dimitri P Bertsekas. Approximate dynamic programming. Athena scientific Belmont, 2012.
[5]	David Blackwell and Tom S Ferguson. “The big match”. In: The Annals of Mathematical
Statistics 39.1 (1968), pp. 159-163.
[6]	Yuri Burda et al. “Exploration by random network distillation”. In: arXiv preprint
arXiv:1810.12894 (2018).
[7]	Henry Charlesworth and Giovanni Montana. “PlanGAN: Model-based Planning With Sparse
Rewards and Multiple Goals”. In: arXiv preprint arXiv:2006.00900 (2020).
[8]	Marc Peter Deisenroth, Carl Edward Rasmussen, and Dieter Fox. “Learning to control a
low-cost manipulator using data-efficient reinforcement learning”. In: Robotics: Science and
Systems VII (2011), pp. 57-64.
[9]	Sam Devlin and Daniel Kudenko. “Theoretical considerations of potential-based reward
shaping for multi-agent systems”. In: The 10th International Conference on Autonomous
Agents and Multiagent Systems-Volume 1. International Foundation for Autonomous Agents
and Multiagent Systems. 2011, pp. 225-232.
[10]	Sam Devlin, Daniel Kudenko, and Marek Grze§.“An empirical study of potential-based reward
shaping and advice in complex, multi-agent systems”. In: Advances in Complex Systems 14.02
(2011), pp. 251-278.
[11]	Sam Michael Devlin and Daniel Kudenko. “Dynamic potential-based reward shaping”. In:
Proceedings of the 11th International Conference on Autonomous Agents and Multiagent
Systems. IFAAMAS. 2012, pp. 433-440.
[12]	Drew Fudenberg and Jean Tirole. “Tirole: Game Theory”. In: MIT Press 726 (1991), p. 764.
[13]	Anna Harutyunyan et al. “Expressing arbitrary reward functions as potential-based advice”.
In: Proceedings of the AAAI Conference on Artificial Intelligence. Vol. 29. 2015.
[14]	Ionel-Alexandru Hosu and Traian Rebedea. “Playing atari games with deep reinforcement
learning and human checkpoint replay”. In: arXiv preprint arXiv:1607.05077 (2016).
[15]	Rein Houthooft et al. “Vime: Variational information maximizing exploration”. In: arXiv
preprint arXiv:1605.09674 (2016).
[16]	Yujing Hu et al. “Learning to Utilize Shaping Rewards: A New Approach of Reward Shaping”.
In: Advances in Neural Information Processing Systems 33 (2020).
[17]	Maximilian Igl et al. “The Impact of Non-stationarity on Generalisation in Deep Reinforcement
Learning”. In: arXiv preprint arXiv:2006.05826 (2020).
[18]	Tommi Jaakkola, Michael I Jordan, and Satinder P Singh. “Convergence of stochastic iterative
dynamic programming algorithms”. In: Advances in neural information processing systems.
1994, pp. 703-710.
[19]	Sergio Valcarcel Macua, Javier Zazo, and Santiago Zazo. “Learning Parametric Closed-Loop
Policies for Markov Potential Games”. In: arXiv preprint arXiv:1802.00899 (2018).
[20]	Patrick Mannion et al. “Policy invariance under reward transformations for multi-objective
reinforcement learning”. In: Neurocomputing 263 (2017), pp. 60-73.
[21]	Amy McGovern and Andrew G Barto. “Automatic discovery of subgoals in reinforcement
learning using diverse density”. In: (2001).
[22]	David Mguni. “A Viscosity Approach to Stochastic Differential Games of Control and Stop-
ping Involving Impulsive Control”. In: arXiv preprint arXiv:1803.11432 (2018).
[23]	David Mguni. “Cutting Your Losses: Learning Fault-Tolerant Control and Optimal Stopping
under Adverse Risk”. In: arXiv preprint arXiv:1902.05045 (2019).
10
Under review as a conference paper at ICLR 2022
[24]	David Mguni, Joel Jennings, and Enrique Munoz de Cote. “Decentralised learning in sys-
tems with many, many strategic agents”. In: Thirty-Second AAAI Conference on Artificial
Intelligence. 2018.
[25]	David Mguni et al. “Coordinating the crowd: Inducing desirable equilibria in non-cooperative
systems”. In: arXiv preprint arXiv:1901.10923 (2019).
[26]	David Mguni et al. “Learning in Nonzero-Sum Stochastic Games with Potentials”. In: arXiv
preprint arXiv:2103.09284 (2021).
[27]	Andrew Y Ng, Daishi Harada, and Stuart Russell. “Policy invariance under reward trans-
formations: Theory and application to reward shaping”. In: ICML. Vol. 99. 1999, pp. 278-
287.
[28]	David C Noelle. “Unsupervised methods for subgoal discovery during intrinsic motivation in
model-free hierarchical reinforcement learning”. In: KEG@ AAAI. 2019.
[29]	Georg Ostrovski et al. “Count-based exploration with neural density models”. In: arXiv
preprint arXiv:1703.01310 (2017).
[30]	Deepak Pathak et al. “Curiosity-driven Exploration by Self-supervised Prediction”. In: Inter-
national Conference on Machine Learning (ICML). 2017, pp. 2778-2787.
[31]	Peng Peng et al. “Multiagent bidirectionally-coordinated nets: Emergence of human-level
coordination in learning to play starcraft combat games”. In: arXiv preprint arXiv:1703.10069
(2017).
[32]	John Schulman et al. “Proximal Policy Optimization Algorithms”. In: CoRR abs/1707.06347
(2017).
[33]	Kun Shao et al. “A survey of deep reinforcement learning in video games”. In: arXiv preprint
arXiv:1912.10944 (2019).
[34]	Yoav Shoham and Kevin Leyton-Brown. Multiagent systems: Algorithmic, game-theoretic,
and logical foundations. Cambridge University Press, 2008.
[35]	David Silver et al. “A general reinforcement learning algorithm that masters chess, shogi, and
Go through self-play”. In: Science 362.6419 (2018), pp. 1140-1144.
[36]	Bradly Stadie, Lunjun Zhang, and Jimmy Ba. “Learning Intrinsic Rewards as a Bi-Level
Optimization Problem”. In: Conference on Uncertainty in Artificial Intelligence. PMLR. 2020,
pp. 111-120.
[37]	Bradly C Stadie, Sergey Levine, and Pieter Abbeel. “Incentivizing exploration in reinforcement
learning with deep predictive models”. In: arXiv preprint arXiv:1507.00814 (2015).
[38]	Alexander L Strehl and Michael L Littman. “An analysis of model-based interval estimation
for Markov decision processes”. In: Journal of Computer and System Sciences 74.8 (2008),
pp. 1309-1331.
[39]	Richard S Sutton and Andrew G Barto. Reinforcement learning: An introduction. MIT press,
2018.
[40]	John N Tsitsiklis and Benjamin Van Roy. “Optimal stopping of Markov processes: Hilbert
space theory, approximation algorithms, and an application to pricing high-dimensional
financial derivatives”. In: IEEE Transactions on Automatic Control 44.10 (1999), pp. 1840-
1851.
[41]	ICML Workshop. Reinforcement Learning for Real Life, ICML 2021 Workshop. https :
//sites.google.com/view/RL4RealLife.
[42]	Kaiqing Zhang, Zhuoran Yang, and Tamer BaSar. “Multi-agent reinforcement learning: A
selective overview of theories and algorithms”. In: arXiv preprint arXiv:1911.10635 (2019).
[43]	Zeyu Zheng, Junhyuk Oh, and Satinder Singh. “On Learning Intrinsic Rewards for Policy
Gradient Methods”. In: Advances in Neural Information Processing Systems (NeurIPS). 2018.
[44]	Martin Zinkevich, Amy Greenwald, and Michael Littman. “Cyclic equilibria in Markov
games”. In: Advances in Neural Information Processing Systems 18 (2006), p. 1641.
[45]	Haosheng Zou et al. “Reward shaping via meta-learning”. In: arXiv preprint arXiv:1901.09330
(2019).
11
Under review as a conference paper at ICLR 2022
Appendix
Table of Contents
8	Algorithm	2
9	Further Implementation Details	3
10	Shaper Termination Times	3
11	Experimental Details	4
11.1	Environments & Preprocessing Details ............................... 4
11.2	Hyperparameter Settings ........................................... 4
12	Ablation Studies	5
13	Flexibility of ROSA to Accommodate different Exploration Bonus Terms L	7
14	Robustness to choices of φ Parameters	8
15	Notation & Assumptions	9
16	Proof of Technical Results	9
17	Additional Experiment 1 - Replacing RND for Bonus Reward	28
18	Additional Experiment 2 - Robustness to initialisation of φ	28
1
Under review as a conference paper at ICLR 2022
8 Algorithm
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
Algorithm 2: Reinforcement Learning Optimising Shaping Algorithm ROSA
Input: Environment E
Initial Controller policy π0 with parameters θπ0
Initial Shaper switch policy g20 with parameters θg2
Initial Shaper action policy π02 with parameters θπ2
Randomly initialised fixed neural network φ(∙, ∙)
Neural networks h (fixed) and h for RND With parameter。算
Buffer B
Number of rollouts Nr , rollout length T
Number of mini-batch updates Nu
Switch cost c(∙), Discount factor γ, learning rate ɑ
Output: Optimised Controller policy π*
π,π2, g2 一 π0,π2, g2o
for n = 1, Nr do
// Collect rollouts
for t = 1, T do
Get environment states st from E
Sample at from π(st)
Apply action at to environment E, and get reward rt and next state st+1
Sample gt from g2(st) // Switching control
if gt = 1 then
Sample at2 from π2 (st)
Sample at2+1 from π2 (st+1)
rti = γφ(st+1, at2+1) - φ(st, at2) // Calculate F(st, at, st+1, at+1)
else
|_ a2,ri = 0,0 〃 Dummy values
L Append (st,at,gt,a2,rt,ri,st+ι) to B
for u = 1, Nu do
Sample data (st, at, gt, at2, rt, rti, st+1) from B
if gt = 1 then
I Set shaped reward to Yt = r + ri
else
L Set shaped reward to Yt = r
//Update RND
LOSSRND = ||h(st) - h(st)∣∣2
θh J θh — aVLossRND
// Update Shaper
lt = ||h(st) - h(st)l∣2 //Compute L(St)
Ct = c(∙)gt
Compute Lossπ2 using (st, at, gt, ct, rt, rti, lt, st+1) using PPO loss // Section 4.2
Compute Lossg2 using (st, at, gt, ct, rt, rti, lt, st+1) using PPO loss // Section 4.2
θπ2 J θπ2 - αVLossπ2
θg2 J θg2 - αVLossg2
// Update Controller
Compute Lossπ using (st, at, rtt, st+1) using PPO loss // Section 4
θπ J θπ - αVLossπ
2
Under review as a conference paper at ICLR 2022
9 Further Implementation Details
Details of Shaper and F (shaping reward)
Object
f
A2
π2
Potential function φ
F
Description
Fixed feed forward NN that maps Rd → Rm
[512, ReLU, 512, ReLU, 512, m]
Discrete integer action set which is size of output of f,
i.e.,A2 is set of integers {1, ..., m}
Fixed feed forward NN that maps Rd 7→ Rm
[512, ReLU, 512, ReLU, 512, m]
φ(s,a* 2) = f(s) ∙ a2
γφ(st+1,at2+1) - φ(st, at2), γ = 0.95
d=Dimensionality of states; m ∈ N - tunable free parameter.
In all experiments We used the above form of F as follows: a state St is input to the ∏ network and
the network outputs logits pt. We softmax and sample from pt to obtain the action at2 . This action
is one-hot encoded. Then, the action at2 is multiplied with f(st) to compute the second term of F.
A similar process is used to compute the first term. In this way the policy of Shaper chooses the
shaping reward.
10 Shaper Termination Times
There are various possibilities for the termination times {τ2k} (recall that {τ2k+1} are the times
which the shaping reward F is switched on using g2). One is for Shaper to determine the sequence.
Another is to build a construction of {τ2k} that directly incorporates the information gain that a state
visitprovides: let W : Ω → {0,1} be a random variable with Pr(W = 1) = P and Pr(W = 0) = 1 -P
where p ∈]0, 1]. Then for any k = 1, 2, . . . , and denote by ∆L(sτk) := L(sτk ) - L(sτk-1), then we
can set:
I(s ) = I (sτ2k+1 ), ifW∆L(sτk+j) > 0,
I(sτ2k+1+j) = I(sτ2k+2 ), W∆L(sτk+j) ≤ 0.
(2)
To explain, since {τ2k}k≥0 are the times at which F is switched off then if F is deactivated at exactly
after j time steps then I(sτ2k+1+l) = I (sτ2k+1) for any 0 ≤ l < j and I(sτ2k+1+j) = I (sτ2k+2) . We
now see that (2) terminates F when either the random variable W attains a 0 or when ∆L(sτk+j ) ≤ 0
which occurs when the exploration bonus in the current state is lower than that of the previous state.
3
Under review as a conference paper at ICLR 2022
11 Experimental Details
11.1	Environments & Preprocessing Details
The table below shows the provenance of environments used in our experiments.
Atari & Cartpole
Maze
Super Mario Brothers
https://github.com/openai/gym
https://github.com/MattChanTK/gym-maze
https://github.com/Kautenja/gym- super- mario- bros
Furthermore, we used preprocessing settings as indicated in the following table.
Setting I Value
Max frames per episode
Observation concatenation
Observation preprocessing
Observation scaling
Reward (extrinsic and intrinsic) preprocessing
Atari & Mario → 18000 / Maze & Cartpole → 200
Preceding 4 observations
Standardization followed by clipping to [-5, 5]
Atari & Mario → (84, 84, 1) / Maze & Cartpole → None
Standardization followed by clipping to [-1, 1]
11.2	Hyperparameter Settings
In the table below we report all hyperparameters used in our experiments. Hyperparameter values in
square brackets indicate ranges of values that were used for performance tuning.
Clip Gradient Norm γE λ Learning rate Number of minibatches Number of optimization epochs Policy architecture Number of parallel actors Optimization algorithm Rollout length Sticky action probability Use Generalized Advantage Estimation	1 0.99 0.95 1x10-4 4 4 CNN (Mario/Atari) or MLP (CartPole/Maze) 2 (CartPole/Maze) or 20 (Mario/Atari) Adam 128 0.25 True
Coefficient of extrinsic reward	[1,5]
Coefficient of intrinsic reward	[1,2, 5, 10, 20, 50]
γI	0.99
Probability of terminating option	[0.5, 0.75, 0.8, 0.9, 0.95]
RND output size	[2,4, 8,16, 32, 64,128, 256]
4
Under review as a conference paper at ICLR 2022
Steps (x 1000)
(a) Ablating Switching Controls
Figure 5: Ablation Experiments
Origin
Optimal
0
1
2
3
4
5
6
Suboptimal
-Standard Policy P1
(b) Responsiveness to Controller policies
2
4
6
12 Ablation Studies
Our reward-shaping method features a mechanism to selectively pick states to which intrinsic rewards
are added. It also adapts its shaping rewards according to Controller’s learning process. In this
section, we present the results of experiments in which we ablated each of these components. In
particular, we test the performance of our method in comparison to a version of our method with the
switching mechanism removed. We then present the result of an experiment in which we investigated
the ability of our method to adapt to different behaviour of Controller.
Ablation Study 1: Switching Controls
Switching controls enable our method to be selective of states to which intrinsic rewards are added.
This improves learnability (specifically, by reducing the computational complexity) of the learning
task for Shaper as there are fewer states where it must learn the optimal intrinsic reward to add to
Controller objective.
To test the effect of this feature on the performance of our method, we compared our method to a
modified version in which Shaper must add intrinsic rewards to all states. That is, for this version
of our method we remove the presence of the switching control mechanism for Shaper. Figure 5
(a) shows learning curves on the Maze environment used in the "Optimality of shaping reward"
experiments in Section 6. As expected, the agent with the version of our method with switching
controls learns significantly faster than the agent that uses the version of our method sans the switching
control mechanism. For example, it takes the agent that has no switching control mechanism almost
50,000 more steps to attain an average episode return of 0.5 as compared against the agent that uses
the version of our algorithm with switching controls.
This illustrates a key benefit of switching controls which is to reduce the computational burden on
Shaper (as it does not need to model the effects of adding intrinsic rewards in all states) which in turn
leads to both faster computation of solutions and improved performance by Controller. Moreover,
Maze is a relatively simple environment, expectedly the importance of the switching control is
amplified in more complex environments.
Ablation Study 2: Adaption of our Method to Different Controller Policies
We claimed Shaper can design a reward-shaping scheme that can adapt its shaping reward guidance
of Controller (to achieve the optimal policy) according to Controller’s (RL) policy.
To test this claim, we tested two versions of our agent in a corridor Maze. The maze features two
goal states that are equidistant from the origin, one is a suboptimal goal with a reward of 0.5 and
the other is an optimal goal which has a reward 1. There is also a fixed cost for each non-terminal
transition. We tested this scenario with two versions of our controller: one with a standard RL
Controller policy and another version in which the actions of Controller are determined by a high
5
Under review as a conference paper at ICLR 2022
entropy policy, we call this version of Controller the high entropy controller.6 The high entropy
policy induces actions that may randomly push Controller towards the suboptimal goal. Therefore, in
order to guide Controller to the optimal goal state, we expect Shaper to strongly shape the rewards of
Controller to guide Controller away from the suboptimal goal (and towards the optimal goal).
Figure 5 (b) shows heatmaps of the added intrinsic reward (darker colours indicate higher intrinsic
rewards) for the two versions of Controller. With the standard policy controller, the intrinsic reward
is maximal in the state to the right of the origin indicating that Shaper determines that these shaping
rewards are sufficient to guide Controller towards the optimal goal state. For the high entropy
controller, Shaper introduces high intrinsic rewards to the origin state as well as states beneath the
origin. These rewards serve to counteract the random actions taken by the high-entropy policy that
lead Controller towards the suboptimal goal state. It can therefore be seen that Shaper adapts the
shaping rewards according to the type of Controller it seeks to guide.
6To generate this policy, we artificially increased the entropy by adjusting the temperature of a softmax
function on the policy logits.
6
Under review as a conference paper at ICLR 2022
13	Flexibility of ROSA to Accommodate different Exploration
B ONUS TERMS L
To demonstrate the robustness of our method to different choices of exploration bonus terms in
Shaper’s objective, we conducted an Ablation study on the L-term (c.f. Equation 1) where we
replaced the RND L term with a basic count-based exploration bonus. To exemplify the high degree
of flexibility, We replaced the RND with a simple exploration bonus term L(S)= CoUnt(S. for any
given state s ∈ S where Count(s) refers to a simple count of the number of times the state s has been
visited. We conducted the Ablation study in the environment in Experiment 1 presented in Sec. 6.
We note that despite the simplicity of the count-based measure, generally the performance of both
versions of ROSA is comparable and in fact the count-based variant is slightly superior to the RND
version.
0.0
----ROSA (L = RND)
----ROSA (L - Count-based)
.64
O O
TLm+J3H
2	4	6	8	10
Steps (x 10000)
Figure 6: Performance of ROSA compared with the exploration bonus replaced by count-based
method.
7
Under review as a conference paper at ICLR 2022
14	ROBUSTNESS TO CHOICES OF φ PARAMETERS
To demonstrate the robustness of φ to different choices of weight parameters, we conducted a study
with 3 sets of randomly sampled values of weight parameters for the feed forward NN that constructs
the φ function (c.f. Sec. 9).
As is shown in Fig. 7, the performance across all 3 values is very comparable demonstrating the
robustness of ROSA to different values of weight parameters for the φ function.
Figure 7: Performance of ROSA compared with different values of φ parameters.
8
Under review as a conference paper at ICLR 2022
15	Notation & Assumptions
We assume that S is defined on a probability space (Ω, F, P) and any S ∈ S is measurable with
respect to the Borel σ-algebra associated with Rp . We denote the σ-algebra of events generated by
{st}t≥0 by Ft ⊂ F. In what follows, we denote by (V, kk) any finite normed vector space and by H
the set of all measurable functions.
The results of the paper are built under the following assumptions which are standard within RL and
stochastic approximation methods:
Assumption 1 The stochastic process governing the system dynamics is ergodic, that is the process
is stationary and every invariant random variable of {st}t≥0 is equal to a constant with probability 1.
Assumption 2 The constituent functions of the players’ objectives R, F and L are in L2 .
Assumption 3 For any positive scalar c, there exists a scalar μc such that for all S ∈ S and for any
t ∈ N we have: E [1 + IlStIIc∣so = s] ≤ μc(1 + IISIlc).
Assumption 4 There exists scalars C1 and c1 such that for any function J satisfying |J (S)| ≤
C2(1 + ISIc2) for some scalars c2 and C2 we have that: Pt∞=0 |E [J(St)|S0 = S] - E[J(S0)]| ≤
C1C2(1 + IStIc1c2).
Assumption 5 There exists scalars C and C such that for any S ∈ S we have that: |J(z, ∙)∣ ≤
C(1 + IzIc) forJ ∈ {R, F, L}.
We also make the following finiteness assumption on set of switching control policies for Shaper:
Assumption 6 For any policy gc, the total number of interventions is given by K < ∞.
We lastly make the following assumption on L which can be made true by construction:
Assumption 7 Let n(S) be the state visitation count for a given state S ∈ S. For any a ∈ A, the
function L(S, a) = 0 for any n(S) ≥ M where 0 < M ≤ ∞.
16	Proof of Technical Results
We begin the analysis with some preliminary lemmata and definitions which are useful for proving
the main results.
Definition 1 A.1 An operator T : V → V is said to be a contraction w.rt a norm ∣∣ ∙ ∣ ifthere exists
a constant c ∈ [0, 1[ such that for any V1, V2 ∈ V we have that:
ITV1 - TV2 I ≤ cIV1 - V2 I.	(3)
Definition 2 A.2 An operator T : V → V is non-expansive if ∀V1, V2 ∈ V we have:
ITV1 - TV2 I ≤ IV1 - V2 I.	(4)
Lemma 1 For any f : V → R, g : V → R, we have that:
max f(a) - maxg(a) ≤ max If (a) -g(a)I .	(5)
a∈V	a∈V	a∈V
Proof 1 We restate the proof given in [23]:
f(a) ≤ If (a) - g(a)I + g(a)	(6)
=⇒	maxf (a)	≤ max{If (a)	- g(a)I	+ g(a)} ≤ max If (a)	-	g(a)I	+ max g(a).	(7)
Deducting max g(a) from both sides of (7) yields:
a∈V
maxf (a) - maxg(a) ≤ max If (a) - g(a)I .	(8)
After reversing the roles of f and g and redoing steps (6) - (7), we deduce the desired result since the
RHS of (8) is unchanged.
9
Under review as a conference paper at ICLR 2022
Lemma 2 A.4 The probability transition kernel P is non-expansive, that is:
kPV1 -PV2k ≤ kV1 -V2k.	(9)
Proof 2 The result is well-known e.g. [40]. We give a proof using the Tonelli-Fubini theorem and
the iterated law of expectations, we have that:
kPJk2 = E [(PJ)2[s0]] = E ([E[J[sι]∣so])2i ≤ E[E[J2[sι]∣so]] = E J2[s1]] = Jk2,
where we have used Jensen’s inequality to generate the inequality. This completes the proof.
Proof of Proposition 1
Proof 3 (Proof of Prop 1) To prove (i) of the proposition it suffices to prove that the term
PtT=0 γtF(θt, θt-1)I (t) converges to 0 in the limit as T → ∞. As in classic potential-based
reward shaping [27], central to this observation is the telescoping sum that emerges by construction
ofF.
First recall v1π,π (s, I0), for any (s, I0) ∈ S × {0, 1} is given by:
v1 , (s, I0 ) = Eπ,π2 X γ	Ri (st , at ) + F (st, at ; st-1 , at-1 )It
t=0
∞∞
XYtRi(st, at) + X YtF(st, a2; St-1, a2-i)It
t=0	t=0
∞
X YtRi (st, at)
t=0
∞
+ Eπ,π2 X Yt户(St,a2; st-1,a2-I))It
t=0
(10)
(11)
(12)
Hence it suffices to prove that E∏,* [P∞=0 γtF(st,a2; st-ι,a2-ι))It]=
Recall there a number of time steps that elapse between τk and τk+1, now
0.
∞
X YtF(St, a2; st-i, a2-i))I(t)
t=0
τ2
= X Ytφ(St,at2) - Yt-1φ(St-1,at2-1) +Yτ1φ(Sτ1,aτ21)
t=τ1 +1
τ4
+ X Ytφ(St,at2) - Yt-1φ(St-1,at2-1) +Yτ3φ(Sτ3,aτ23)
t=τ3+1
τ2k
+ . . . + X	Ytφ(St, at2) - Yt-1φ(St-1, at2-1) + Y τ1 φ(Sτ2k+1, aτ22k+1 ) + . . . +
t=τ(2k-1) +1
τ2 -1
= X Yt+1φ(St+1,at2+1) - Ytφ(St,at2) +Yτ1φ(Sτ1,aτ21)
t=τ1
τ4 -1
+ X Yt+1φ(St+1,at2+1) - Ytφ(St,at2) +Yτ3φ(Sτ3,aτ23)
t=τ3
τ2K-1
+ . . . + X Ytφ(St, at2) - Yt-1φ(St-1, at2-1) + Y τ2k-1 φ(Sτ2k-1 , aτ22k-1 ) + . . . +
t=τ(2k-1)
∞ τ2K-1	∞
= X X Yt+1φ(St+1,at2+1) - Ytφ(St,at2) - X Yτ2k-1 φ(Sτ2k-1, aτ22k-1)
k=1 t=τ2k-1	k=1
10
Under review as a conference paper at ICLR 2022
∞∞
Xγτ2kφ(sτ2k,aτ22k) - X γ τ2k-1 φ(sτ2k-1, aτ22k-1)
k=1	k=1
∞∞
Xγτ2kφ(sτ2k,0)-Xγτ2k-1φ(sτ2k-1,0)=0,
k=1	k=1
where we have used the fact that by construction at2 ≡ 0 whenever t = τ1, τ2, . . . and by construction
φ(s, 0) ≡ 0 for any s.
2
With this we readily deduce that vi , (s) = Eπ,π2 [ t=0 γtRi (st, at)] which is a measure of
environment rewards only from which statement (i) can be readily deduced.
For part (ii) we note first that it is easy to see that v2π,π (s0, I0) is bounded above, indeed using the
key result in the proof of part (i) and the properties of c we have that
2
v2 , (s0, I0) = Eπ,π2
Eπ,π2
≤ Eπ,π2
Eπ,π2
≤ Eπ,π2
∞
X Yt R+ X C(It,It-1 成2k-1 + Ln(St)
t=0
∞
k≥1
EYt R + EC(It,It-1)δT2k-1 + Ln(St)	+ EYtFIt
t=0
∞
k≥1
t=0
γt(R+Ln(st))
t=0
∞
XγtkR+Lnk
t=0
∞
≤XYt(kRk+kLnk)
= 4(kRk + kLk),
1-Y
(13)
(14)
(15)
(16)
(17)
(18)
(19)
i
∞
≤
■	.1 , ■	1	1∙. .1 J	/- A 1.1 /	X1	1 1	r τ 1 ~ΓΛ / Λ	. ■
using the triangle inequality, the definition of R and the (upper-)boundedness of L and R (Assumption
5). We now note that by the dominated convergence theorem we have that ∀(S0, I0) ∈ S × {0, 1}
lim v2π,π2 (S0, I0)
n→∞
lim Eπ π2
n→∞	,
∞
X Yt	R + X c(It,It-l)δT2k-1 + Ln(St)
t=0
k≥1
Eπ π2 lim
, n→∞
Eπ,π2
Eπ,π2
∞
X γt R + X C(It,It-1 涧τ2k-1 + Ln(St)
t=0
k≥1
∞
XYt	R+X c(It,It-1)δT2k-1
t=0
k≥1
∞
XYt R+XC(It,It-1)δτt2k-1
t=0
k≥1
Y + v1π (S0),
(20)
(21)
(22)
(23)
again using the key result in the proof of (i) and Assumption 6 in the last step, after which we deduce
(ii).
11
Under review as a conference paper at ICLR 2022
Note that by (ii) we heron may consider the quantity for the Shaper expected return:
2
v2 ,	(S0,IO)= E∏,∏2
∞
Xγt	R + X c(It, It-1)δτt2k-1
t=0	k≥1
(24)
Proof of Theorem 1
Proof 4 Theorem 1 is proved by firstly showing that when the players jointly maximise the same
objective there exists a fixed point equilibrium of the game when all players use Markov policies and
Shaper uses switching control. The proof then proceeds by showing that the MG G admits a dual
representation as an MG in which jointly maximise the same objective which has a stable point that
can be computed by solving an MDP. Thereafter, we use both results to prove the existence of a fixed
point for the game as a limit point of a sequence generated by successively applying the Bellman
operator to a test function.
Therefore, the scheme of the proof is summarised with the following steps:
I)	Prove that the solution to Markov Team games (that is games in which both players maximise
identical objectives) in which one of the players uses switching control is the limit point of a
sequence of Bellman operators (acting on some test function).
II)	Prove that for the MG G that is there exists a function Bπ,π2 : S × {0, 1} → R such that7
2	02	2	02
viπ,π	(z) -viπ	,π	(z)	=	Bπ,π2(z)	-Bπ0,π2(z),	∀z≡ (s,I0)	∈ S ×	{0, 1}, ∀i	∈ {1,	2}.
III)	Prove that the MG G has a dual representation as a Markov Team Game which admits a
representation as an MDP.
PROOF OF PART I
Our first result proves that the operator T is a contraction operator. First let us recall that the
switching time τk is defined recursively τk = inf {t > τk-1 |st ∈ A, τk ∈ Ft} where A = {s ∈
S, m ∈ M|g2(m|st) > 0}. To this end, we show that the following bounds holds:
Lemma 3 The Bellman operator T is a contraction, that is the following bound holds:
kTψ - Tψ0k ≤γkψ-ψ0k .
Proof 5 Recall we define the Bellman operator Tψ of G acting on a function Λ : S × N → R by

Tψ Λ(sτk , I(τk)) :
max Mπ,π2 Λ(sτk, I (τk)),
ψ(sτk, a) + γmax	P(s0; a, sτk)Λ(s0, I(τk))
a∈A s0∈S
(25)
In what follows and for the remainder of the script, we employ the following shorthands:
Pas0 =: X P (s'； a,s),	PnfO =: X ∏(a∣s)Pasθ, Rn (zt) ：= X ∏(at∣s)R(zt,at,θt ,θt-i)
s0∈S	a∈A	at∈A
To prove that T is a contraction, we consider the three cases produced by (25), that is to say we prove
the following statements:
i)	Θ(zt, a, a2, a2-i) + Ymaχ Pastψ(s0, ∙) - ( Θ(zt, a, a2,a2-i) + Ymaχ Pastψ0(s0, ∙) ) I ≤
a∈A	a∈A
Ykψ-ψ0k
ii)	Mπ,π2 ψ - Mπ,π2 ψ0 ≤ Y kψ - ψ0k ,	(and hence M is a contraction).
7This property is analogous to the condition in Markov potential games [19, 26]
12
Under review as a conference paper at ICLR 2022
iii)
{0, 1}.
Mπ,π2ψ-
Θ(∙, a) + Ymax Paψ0 H ≤ Y ∣∣ψ — ψ0∣∣ . where Zt ≡ (st,It) ∈ S ×
We begin by proving i).
Indeed, for any a ∈ A and ∀zt ∈ S × {0, 1}, ∀θt , θt-1 ∈ Θ, ∀s0 ∈ S we have that
θ(Zt, a, a2,a2-I) + YPnOstψ(SI)- θ(Zt, a, a2,a2-ι) + γmax PaOstψ0(S0, ∙)
≤ max I YPaOst ψ(s0, ∙) - Y PaO st ψ0(s0, ∙)l
≤Y∣Pψ-Pψ0∣
≤Y∣ψ-ψ0∣ ,
again using the fact that P is non-expansive and Lemma 1.
We now prove ii).
For any T ∈ F, define by τ0 = inf{t > T|st ∈ A, τ ∈ Ft}. Now using the definition of M we have
that for any Sτ ∈ S
III(Mπ,π2ψ - Mπ,π2ψ0)(Sτ, I(T))III
≤ max	IIIΘ(Zτ, aτ, aτ2, aτ2-1) + c(Iτ,Iτ-1) + YPsπOsτ P aψ(Sτ, I (T 0))
aτ ,a2τ ,aτ2 -1∈A×Θ2 I	τ
-(Θ(zτ, aτ, aT, aT-1) + "τ ,J) + XTP W 0 ,I (T 0)))
=YIIPsπOsτPaψ(Sτ,I(T0)) - PsπOsτPaψ0(Sτ,I(T0))II
≤Y∣Pψ-Pψ0∣
≤Y∣ψ-ψ0∣ ,
using the fact that P is non-expansive. The result can then be deduced easily by applying max on
both sides.
We now prove iii). We split the proof of the statement into two cases:
Case 1:
Mπ,π2 ψ(Sτ, I(T)) - Θ(Zτ, aτ, aτ2, aτ2-1) + Ymax PsaOsτ ψ0(S0, I(T)) < 0.	(26)
We now observe the following:
Mπ,π2 ψ(Sτ, I(T)) - Θ(Zτ, aτ, aτ2, aτ2-1) + Ymax PsaO s ψ0(S0, I(T))
τ τ-	a∈A	s sτ
≤ max nΘ(Zτ, aτ, aτ2, aτ2-1) + YPsπOsτ Paψ(S0, I(T)), Mπ,π2 ψ(Sτ, I (T))o
- Θ(Zτ,aτ, aτ2, aτ2-1) + Ymax PsaOsτψ0(S0,I(T))
≤ II max Θ(Zτ,aτ,aτ2,aτ2-1)+YPsπOsτPaψ(S0,I(T)),Mπ,π2ψ(Sτ,I(T))
-	max Θ(Zτ, aτ, aτ2 , aτ2-1) + Ymax PsaO s ψ0(S0, I(T)), Mπ,π2 ψ(Sτ, I(T))
+ max Θ(Zτ, aτ, aτ2 , aτ2-1) + Ymax PsaOsτ ψ0(S0, I(T)), Mπ,π2 ψ(Sτ, I(T))
-	Θ(Zτ,aτ, aτ2, aτ2-1) + Ymax PsaOsτψ0(S0,I(T))II
13
Under review as a conference paper at ICLR 2022
≤
max Θ(zτ, aτ, aτ2, aτ2-1) + γmax Psa0s ψ(s0, I(τ)), Mπ,π2 ψ(sτ, I(τ))
τ τ-	a∈A s sτ
- max Θ(zτ, aτ, aτ2 , aτ2-1) + γmax Psa0sτ ψ0(s0, I(τ)), Mπ,π2 ψ(sτ, I(τ))
+
max Θ(zτ, aτ, a2 , a2-1) + γmax Pa0 ψ0(s0, I(τ)), Mπ,π2 ψ(sτ, I(τ))
τ τ-	a∈A s sτ
- Θ(zτ,aτ,aτ2,aτ2-1) + γmax Psa0sτ ψ0(s0, I(τ))
a∈A τ
≤ γma∈aAx Psπ0sτ Paψ(s0, I(τ)) - Psπ0sτ Paψ0(s0, I(τ))
+ max 0, Mπ,π2 ψ(sτ, I(τ)) - Θ(zτ, aτ, aτ2, aτ2-1) + γmax Psa0sτψ0(s0, I(τ))
≤ γkPψ-Pψ0k
≤ γkψ - ψ0 k,
where we have used the fact that for any scalars a, b, c we have that |max{a, b} - max{b, c}| ≤
|a - c| and the non-expansiveness of P.
Case 2:
Mπ,π2ψ(sτ, I(τ)) - Θ(zτ,aτ,aτ2,aτ2-1) + γmaxPsa0sτψ0(s0,I(τ)) ≥ 0.
a∈A	τ
For this case, first recall that for any τ ∈ F, -c(Iτ, Iτ-1) > λ for some λ > 0.
Mπ,π2ψ(sτ,I(τ)) - Θ(zτ,aτ,aτ2,aτ2-1) + γmax Psa0sτψ0(s0,I(τ))
a∈A	τ
≤ Mπ,π2ψ(sτ, I(τ)) - Θ(zτ,aτ,aτ2, aτ2-1) + γmax Psa0sτ ψ0(s0, I(τ)) - c(Iτ, Iτ-1)
≤ Θ(zτ,aτ,aτ2,aτ2-1) + c(Iτ,Iτ-1) + γPsπ0sτPaψ(s0, I(τ0))
—
,aτ,aτ2,aτ2-1) + c(Iτ,Iτ-1) + γmax Psa0sτ ψ0(s0, I(τ))
a∈A	τ
≤ γma∈aAx Psπ0sτPa(ψ(s0,I(τ0))-ψ0(s0,I(τ)))
≤ Y IΨ(s0,I(T0)) - ψ0(s0,I(T))|
≤γkψ-ψ0k,
again using the fact that P is non-expansive. Hence we have succeeded in showing that for any
Λ ∈ L2 we have that
MnnΛ - max [ψ(∙, a) + YPaΛ0] ≤ Y ∣∣Λ - Λ0∣∣ .
a∈A
(27)
Gathering the results of the three cases gives the desired result.
PROOF OF PART II
To prove Part II, we prove the following result:
Proposition 4 For any π ∈ Π and for any Shaper policy π2, there exists a function Bπ,π2 : S ×
{0, 1} → R such that
viπ,π2(z)-viπ0,π2(z) =Bπ,π2(z)-Bπ0,π2(z), ∀z≡ (s,I0) ∈S × {0, 1}	(28)
14
Under review as a conference paper at ICLR 2022
where in particular the function B is given by:
Bπ,π2(s0,I0) = Eπ,π2 X∞ γt R +Xc(It, It-1)δτt2k-1	,	(29)
t=0	k≥1
for any (s0 , I0) ∈ S × {0, 1}.
Proof 6 Note that by the deduction of (ii) in Prop 1, we immediately observe that
22
^2, (s0,I0) = B , (s0,I0), ∀(s0,I0) ∈ S ×{0,1}.	(30)
We therefore immediately deduce that for any two Shaper policies π2 and π02 the following expression
holds∀(s0,I0) ∈ S × {0, 1}:
v∏,π2 (so,Io) — v∏,π02 (so,Io) = BTN (so,Io) — Bn，n02 (s0,I0).	(31)
Our aim now is to show that the following expression holds ∀(s0, I0) ∈ S × {0, 1}:
v∏,π2 (Io, so) — v∏0,π2 (Io, so) = Bπ,π2 (Io, so) — Bπ0,π2 (Io, so), ∀i ∈N
For the finite horizon case, the result is proven by induction on the number of time steps until the end
of the game. Unlike the infinite horizon case, for the finite horizon case the value function and policy
have an explicit time dependence.
We consider the case of the proposition at time T — 1 that is we evaluate the value functions at the
penultimate time step. In this case, we have that:
Es τ-ι 〜dθ
hBTπ,-π1 (IT-1, sT-1) — BTπ-,π1 (IT-1, sT-1)i
∞
Esτ-ι〜dθ	E	∏(aτ-1； ST-i) R(ST-1, aτ-i) + E E C(Ij」-1”*卜
aT-1∈A k≥o j=T -1
+γXX
π(aT-1; sT-1)P(sT; aT -1)BTπ,π (IT, sT)
sT∈S aT-1 ∈A
π0(a0T -1; sT-1)	R(sT -1
a0T-1∈A
—
∞
, a0T-1X c(Ij	, Ij -1)δτjk
k≥o j =T-1
+ γ X X	π0(a0T-1; sT-1)P(sT; a0T-1)BTπ ,π (IT, sT)
sT∈S a0T-1∈A
τ-ι 〜dθ
π(aT -1; sT-1)R(sT-1, aT-1)
aT-1 ∈A
—
π0(a0T -1; sT -1)R(sT -1, a0T-1)
a0T-1 ∈A
+γXX
π(aT -1; sT-1)P(sT; aT -1)BTπ,π (IT, sT)
sT∈S aT-1 ∈A
(32)
— X X	π0(a0T-1; sT-1)P (sT; a0T-1)BTπ ,π (IT, sT)	.
sT∈S a0T-1 ∈A
2
We now observe that for any π ∈ Π and for any π2 we have that BT, (IT, sT)
E IPaT∈a ∏(aτ； ST) [R(sτ, aτ) + Pk≥o Pj=T C(Ij/7闻]
moreover we have that for any
15
Under review as a conference paper at ICLR 2022
π ∈ Π andfor any π2
∞
E X π(aτ; ST) R(ST,aτ) + XX CU JT同
_ aτ∈A	k≥0j=T
-E π0(ατ； ST)
a0τ ∈A
R(st,a0T) + XX C(Ij J-IT #
=E
E π(aτ； ST)R(sτ,aτ) - E
.aτ ∈A
π (a T； sT)R(ST, aT)
a0τ ∈A
∞	∞
+ EX π(aτ； st) X X C(Ij,Ij-i同-X π0(a0τ； ST) X X C(Ij J一同
aτ ∈A	k≥0 j=T	a0τ ∈A	k≥0 j=T
Hence we find that
EST-1 ~dθ
[BT-1 (IT-1,sT-1) - BT-1 (IT-1,sT-1)]
EST-ι~dθ	E	π(aτ-1； ST-1)R(ST-i,aτ-1)
-aτ-1 ∈A
E	π0(aτ-1； sT-I)R(ST-1, aT-I)
a a T-1 ∈A
(33)
+ Y Σ Σ Σ π(aτ-1； ST-1)P(st； aτ-1)∏(aτ； ST)R(st, aτ)
∖ sτ ∈S aτ —1∈A aτ ∈A
(34)
-	π0(a0τ-1； sT-I)P(sT； aT-1)π0(a0T； sT)R(ST, aT)
sτ ∈S a a T-1∈A a0 T ∈A
∞
+	π(aτ-1； ST-1)P(st； aτ-1)π(aτ； ST)c(Ij, Ij-I)δj
sτ ∈S aτ -1∈A aτ ∈A k≥0 j=T
∞
-X X X XXπ Z(aaτ-1 ； ST-1)P(ST； a0T-1)∏ Z(aZT； ST)c(Ij,Ij-1)δj
sτ ∈S a a τ 1∈A aa T ∈A k≥0 j=T
(35)
(36)
)1
(37)
—
Now
Esτ-1〜曲
∞
EEE EEπ(aT-1 ； sT-I)P(ST； aT-I)π(aT； sTIC(Ij,Ij-I)K
.ST ∈S aτ -1∈A aτ ∈A k≥0 j=T
(38)
∞
-X X X XX∏z(a0T-1； sT-I)P(sT； a T-1)n Z(a0T； sT)c(Ij,∕j-1)δTk	(39)
sτ ∈S aaτ1∈A aaT ∈A k≥0 j=T	_
∞
=EST-1~dθ	Σ Σ Σ π(aτ-1； ST-1)P(ST； aτ-1)π(aτ；ST)∑ EC(Ij,1j-1)K
-ST ∈S aτ-1 ∈A aτ ∈A	k≥0 j=T
(40)
∞	一
-X X X
π0(aaτ-1； ST-1)P(ST； azτ-1)π 0(aZτ；st ) X X C(Ij ,Ij-1)δTk	(41)
sτ ∈S aaτ1∈A aaT ∈A	k≥0 j=T	_
一	∞
=EST-1~dθ	Σ Σ Σ π(aτ-1； ST-1)P(ST； aτ-1)π(aτ；st)∑ £也 J-1)K
-ST ∈S aτ -1 ∈A aτ ∈A	k≥0 j=T
—	(42)
16
Under review as a conference paper at ICLR 2022
∞
-XXXπ0(α0τ-i； st-1)P(ST； ɑ0T-1)∏ 0(α0T；ST) XX C(Ij J-I)K
ST ∈S a0τ1 ∈A OfT ∈A	k≥0 j =T
=KEsτiEEEπ(OT-1； sT-I)P(sT； aT-I)π(OT； sT)
-ST∈S ατ — ι ∈A aτ∈A
-	π Z(a0T-1； ST-1)P(ST； OT-1)∏ Z(a0T； ST)
ST ∈S a0τ —1∈A a0τ ∈A	.
KI E π(aτ)- E πZ(a0τ)i = o.
∖ot ∈A	a0T ∈A	)
Hence, we find that
EST — 1 〜dθ
[bT-1 (IT-1,st-1) - BT-； (IT-1,st-1)]
EST -1 〜dθ	E	π(aτ-1； ST-1)R(ST-1,aτ-1)
_ aT — 1 ∈A
—
E	πZ(aZτ-1； ST-1)R(st-1,a0T-1)
a0T —1 ∈A
+ Y Σ Σ Σ π(aT-1； sT-I)P(sT； aT-1)π(aT； sT)R(ST, aT)
∖ ST ∈S aT  1 ∈A aT ∈A
-	π0(azτ-1； ST-1)P(st； azT-1)∏ Z(aZT； ST)R(st,aτ) I
ST ∈S a0T — 1∈A a0 T ∈A
=EST-I〜dθ [vfT-I(ST-I) - VnTW(sT-1)].
(43)
(44)
(45)
(46)
(47)
(48)
(49)
(50)
Hence, we have succeeded in proving that the expression (28) holdsfor T - k when k = 1.
Our next goal is to prove that the expression holdsfor any 0 < k ≤ T.
2
Note thatfor any T ≥ k > 0, we can write BT_k as
一	∞
Bτ-k(I0,s0)= E∏ R(st-k,aτ-k) + X X C(Ij,Ij-1)δjfc	(51)
.	k≥0j=T-j
2
+ Y T P (s ； sT-k ,aT-k )BT-(k + 1)(IT-(k + 1),sτ-(k+1))
∙5k+1∈S	.
(52)
Now we consider the case when we evaluate the expression (28) for any 0 < k ≤ T. Our inductive
hypothesis is the the expression holds for some 0 < k ≤ T, that is for any 0 < k ≤ T we have that:
Σ Σ	π(aT-(k + 1)； sT-(k+I))P (sT-k ； aT-(k+1))vπ⅛π-k (IT-k, sT-k )
sT — k ∈s aT —(k+1) ∈A
X02
〉, π (a T-(k+1)； sT-(k+I))P(sT-k； a T-(k+1))vi,T-k (IT-k, sT-k)
sT — k ∈s a0T —(k+1) ∈A
=	π(aT-(k+1)； sT-(k+I))P(sT-k； aT-(k + 1))BT-k (IT-k, sT-k)
sT — k∈s aT — (k + 1) ∈A
-	πZ(a0T-(k+1)； sT-(k+I))P(sT-k； a0T-(k+I))BT-I (IT-k, sT-k).
sT — k ∈s a0T —(k+1) ∈A
(53)
(54)
(55)
17
Under review as a conference paper at ICLR 2022
It remains to show that the expression holdsfor k + 1 time steps prior to the end of the horizon. The
result can be obtained using the dynamic programming principle and the base case (k = 1) result,
indeed we have that
Esτ-(k+ι)~dθ [BT,-π(k+1)(IT-(k+1), sT-(k+1) ) - BT 二 k+ι)(IT-(k+1), sT-(k⅛1))]
EsT-(k +1)~dθ	E	π(αT-(k+1); sT-(k+1))φ(ST-(k+1), aT-(k+1))
-aT-(k + 1) ∈A
一 E	π0(a0T-(k+1); sT-(k + 1))φ(ST-(k+1),α0T-(k+1))
a0T-(k + ι) ∈A
+ YΣ Σ	MaT-(k+1)； sT-(k+1))P (sT-k ； aT-(k +I))BT-k (IT-k, sT-k )
ST-k ∈S aT-(k + 1)∈A
-If E	π (a T-(k+1); sT-(k + 1))P (sT-k； a T-(k + 1))BT -k (IT-k, sT-k )
sT-k ∈S a0T-(k + 1) ∈A	∙
∞
EST-(k +1)~dθ	Σ	π(aT-(k+1)； sT-(k+1)) R(ST-(k + 1),aT-(k + 1)) + E E C(Ij/-1)δTk
-	aT -(k 十1)∈A	k≥0 j=T-1
+ YΣ Σ	π(aT-(k+1)； sT-(k+1))P (sT-k ； aT-(k +I))BT-k (IT-k, sT-k )
sT-k∈S aT-(k 十 1)∈A
E	π0(a0T-(k+1)； sT-(k+1))
a0T-(k+1) ∈A
—
∞
R(ST-(k+i),a0T-(k+1)) + £ ΣS C(Ij,∕j-ι)δTk
k≥0j=T-1
+ YΣ Σ	π0(a0T-(k+1)； sT-(k + 1))P(sT-k；
sT-k ∈S a0T-(k 十 1) ∈A
a' T-(k +I))BT -k (IT-k ,sT-k
EST-(k +1)~dθ	E	π(aT-(k+1)； sT-(k+1))R(ST-(k+1), aT-(k+1))
-	aT-(k + 1) ∈A
-	E	π0(a0T-(k+1)； sT-(k + 1))R(ST-(k + 1),a0T-(k+1))
a 0 T-(k+1) ∈A
+ Y
Σ Σ	MaT-(k + 1)； sT-(k+1))P(sT-k； aT-(k+1))BT-k (IT-k, sT-k)
-sT-k ∈S aT-(k十1) ∈A
Σ
Σ
—
π0(a0T-(k+1)； sT-(k + 1))P(sT-k； a0T-(k + 1))BT-Z (IT-k, sT-k) j ∙
sT-k ∈s a0T-(k 十 1)∈A
(56)
EST-(k +1)~dθ	E	π(aT-(k+1)； sT-(k+1))R(ST-(k+1), aT-(k+1))
-aT-(k⅛1) ∈A
- E	π0(a0T-(k+1)； sT-(k +I))R(ST-(k + 1),a0T-(k+1))
a0T-(k⅛1) ∈A
2
vi,T-k (IT-k ,sT-k )
+Y
Σ Σ	π(aT-(k + 1)； sT-(k+I))P(sT-k； aT-(k+1))
-sT-k ∈S aT-(k⅛1) ∈A
—
Σ Σ	πZ(a0T-(k+1)； sT-(k +I))P(sT-k； a0T-(k + 1))vπT-k(IT-k, sT-k)	∙
sT-k ∈S a0T-(k⅛1) ∈A	-I -I
(57)
18
Under review as a conference paper at ICLR 2022
ESτ-(k+ι)〜dθ vπTτ-(k1l(IT--(k + 1),sT-(k+1)) - VT[k+1) (IT-(k+1),si,T-(k+1))] ,	(58)
using the inductive hypothesis and where we have used the fact that
∞
EsT-(k + 1)~dθ	Σ Σ Σ ΣΣπ(aT -(k+1); sT -(k+1))P (sT -k; aT-(k+1))π(aT-k; sT -k)c(Ij, Ij-1)δτjk
sT-k∈S aT-(k+1) ∈A aT-k∈A k≥0 j=T
(59)
∞
-	π0(a0T -(k+1); sT-(k+1))P (sT-k; a0T -(k+1))π0(a0T -k; sT-k)c(Ij, Ij-1)δτjk
sT-k∈S a0T -(k+1) ∈A a0T-k∈A k≥0 j=T
(60)
=K I X π(aT -k) - X	π0(a0T-k))=0,	(61)
aT-k ∈A	a0T-k ∈A
via similar reasoning as before and after which which we deduce the result in the finite case.
For the infinite horizon case, we must prove that there exists a measurable function B : Π × S → R
such that the following holds for any i ∈ N and ∀π, πi0 π0 ∈ Π, ∀π-iπ0 ∈ Π-i and ∀s ∈ S:
EgP h(v∏,π2 - v∏0,π2) (z)i = Es〜P [(Bπ,π2 - Bπ0,π2) (z)i .	(62)
The result is proven by contradiction.
To this end, let us firstly assume ∃c 6= 0 such that
Es 〜P hkπ“-靖，"2) (z)i - Es 〜P h(Bπ"2 - BTo"2) (z)i = c.
Let us now define the following quantities for any s ∈ S and for each ππ0 ∈ Π and π-iπ0 ∈ Π-i
and ∀i ∈ N :
T0	t-1
VnTO(Z) := Eμ(so)π(a0,so)π-i(a-i,s0)
YtP(Sj+1； Sj, aj)π(aij |Sj)π-i(a-i|Sj)Ri(zj,aj),
t=0	j=0 sj+1 ∈S
and
T0	t-1
BTπ (z) := Eμ(so)π(α0,so)∏-i(a-i,S0)	P(Sj+1； sj, aj) ∙ π(aj|sj)π-i(a-ilsj)θ(zj,aj),
t=0	j=0 sj+1 ∈S
so that the quantity Viπ,T 0 (S) measures the expected cumulative return until the point T0 < ∞.
Hence, we deduce that
Viπ (z) ≡ Viπ,∞ (z)
T0-1
=VnT0(z) + YT μ(S0)π(a0, So)∏-i(a-i, So) π ∑ YtP(Sj+1； Sj,aj)π(aj|Sj)n-i(a-i|Sj)vn(ST0).
j=0 sj+1 ∈S
Next we observe that:
C =	Es〜P	[(v∏,n2	-	vn0,n2) (z)]	—	Es〜P [(Bn,n2	-	Bn0，n2)	(z)]
=Es〜P	[卜∏T02	-	v∏Tn2) (z)i	-	Es〜P h(BT0"2	-	BT0'π)	(s)]
T0-1
+yt'Esτ0〜P μ(so)π(a0, so)∏-i(a-i, so) Y X P(sj+1； Sj,aj)∏(aj|sj)∏-i(a-i∣sj) (v∏,n2 (zτ0) - Bn,n2 (ZT0)
j=o sj+1∈S
19
Under review as a conference paper at ICLR 2022
T0-1
-μ(SO )π0 (a0i,s0)π-i(a-i ,s0) Y X P(Sj+ 1； Sj ,ajHi (ajilsj )π-i(a-ilsj ) (vπ ,π2 (ZTO)- /N (ZT0 D .
j=0 sj+1 ∈S
Considering the last expectation and its coefficient and denoting the product by κ, using the fact
that by the Cauchy-Schwarz inequality we have kAX - BY k ≤ kAkkX k + kBkkY k, moreover
whenever A, B are non-expansive we have that kAX - BY k ≤ kXk + kY k, hence we observe
the following κ ≤ kκk ≤ 2γT0 (kvik + kBk). Since we can choose T0 freely and γ ∈]0, 1[, we can
choose T0 to be sufficiently large so that YT0 (∣∣vik + IIBIl) < 4 |c|. This then implies that
Es〜P	卜∏T0	-VnTn	)	(z) -	(bToπ	- BT尸)(z)	>	2c,
which is a contradiction since we have proven that for any finite T0 it is the case that
Es〜P	(VnT0	- VnTn	)	(z)	-	(BTK	-	BT尸)(z)	=	0,
and hence we deduce the result in the infinite horizon case.
PROOF OF PART III
We begin by recalling that a Markov strategy is a policy πi : S × Ai → [0, 1] which requires as input
only the current system state (and not the game history or the other player’s action or strategy [22]).
With this, we give a formal description of the stable points of G in Markov strategies.
Definition 3 A policy profile π = (∏1,π2) ∈ Π is a Markov perfect equilibrium (MPE) if the
following holds ∀i = j ∈ {1, 2}, ∀π0 ∈ ∏i: v(n Kj)(so,Io) ≥ v(n Kj)(so,Io),∀(so,I0) ∈
S × {0, 1}.
The MPE describes a configuration in policies in which no player can increase their payoff by
changing (unilaterally) their policy. Crucially, it defines the stable points to which independent
learners converge (if they converge at all).
Proof of Proposition 3
Proof 7 We do the proof by contradiction. Let σ = (π, g) ∈ arg sup Bn0,g0 (S) for any S ∈ S. Let us
n0 ∈Π,g0
now therefore assume that σ ∈ NE{G}, hence there exists some other Strategy profile σ = (g,∏)
for which Controller has a profitable deviation where π0 6= π i.e. V1n ,n (S) > V1n,n (S) (using the
preservation of signs of integration). Prop. 4 however implies that Bn0,n2 (S) - B n,n2 (S) > 0 which
is a contradiction since σ = (π, g) is a maximum of B. The proof can be straightforwardly adapted
to cover the case in which the deviating player is Shaper after which we deduce the desired result.
The last result completes the proof of Theorem 1.
Proof of Proposition 2
Proof 8 (Proof of Prop. 2) The proof is given by establishing a contradiction. Therefore suppose
that Mn,n2 ψ(Sτk , I(τk)) ≤ ψ(Sτk , I(τk)) and suppose that the intervention time τ10 > τ1 is an opti-
mal intervention time. Construct the Player 2 π02 ∈ Π2 and π2 policy switching times by (τ0, τ0,...,)
and π02 ∈ Π2 policy by (τ00, τ1, . . .) respectively. Define by l = inf {t > 0; Mn,n2 ψ(St, I0) =
ψ(St, I0)} and m = sup{t; t < τ10}. By construction we have that
V2n1,n02 (S, I0)
=E hR(S0,a0)+Eh...+γl-1E hR(Sτ1-1, aτ1-1) +... + γm-l-1E hR(Sτ10-1, aτ10-1) + γMn1,n02V2n1,n02(S0, I(τ10))iiii
20
Under review as a conference paper at ICLR 2022
< E [R(s0,ao) + E [... + γl-1E [R(sτ1-I,aτ1-1) + YMπ1,π2vπ ,π (sτι,I(τι))]]]
We now use the following observation E [r(sti-i,Oγi-i) + YMn ,π2 v∏ ,π (sτι, I (τι))]
≤ max ∣Mπ1,π2 V'"" (sτι, I(T1)), Omax [冗⑸% ,5) + Y P3o∈s P(s0;心,Sτι)/"? (s0, I(τι))
Using this we deduce that
v2π,π (s, I0) ≤ E R(s0, a0) + E . . .
+ Yl-1E R(sτι-1, aτι-i) + Ymax M Mπ1,π2。,卢 ,(s/i,I(T1)), max
aτ1 ∈A
R(sτk, aτk) +γ X P(s0; aτ1, sτ1)v2π1,π2(s0, I(τ
s0∈S
=E [R(s0,ao) + E [... + γl-1E [R(sτι-i,aτι-i) + YlTvn ,π ] ",I(τι))]]] = V ,π (s,Io)),
where the first inequality is true by assumption onM. This is a contradiction since π02 is an optimal
policy for Player 2. Using analogous reasoning, we deduce the same result for τk0 < τk after which
deduce the result. Moreover, by invoking the same reasoning, we can conclude that it must be the
case that (τ0, τ1, . . . , τk-1, τk, τk+1, . . . , ) are the optimal switching times.
Proof of Theorem 2
To prove the theorem, we make use of the following result:
Theorem 3 (Theorem 1, pg 4 in [18]) Let Ξt(s) be a random process that takes values in Rn and
given by the following:
Ξt+1(s) = (1 - αt(s)) Ξt(s)αt(s)Lt(s),
then Ξt(s) converges to 0 with probability 1 under the following conditions:
(63)
i)
ii)
iii)
0 ≤ αt ≤ 1, t αt = ∞ and t αt < ∞
kE[Lt|Ft]k ≤ γkΞtk, with γ < 1;
Var [Lt|Ft] ≤ c(1 + kΞtk2) for some c > 0.
Proof 9
To prove the result, we show (i) - (iii) hold. Condition (i) holds by choice of learning rate.
It therefore remains to prove (ii) - (iii). We first prove (ii). For this, we consider our variant of the
Q-learning update rule:
Qt+1 (st , It , at) = Qt (st , It , at)
+ αt(st, It, at) max Mπ,π2Q(sτk, Iτk, a), φ(sτk, a) + Ymax Q(s0, Iτk, a0) - Qt(st, It,at) .
a0∈A
After subtracting Q?(st, It, at) from both sides and some manipulation we obtain that:
Ξt+1 (st , It , at)
= (1 - αt (st , It , at ))Ξt (st , It , at )
+ αt(st,It, at)) max Mπ,π2 Q(sτk, Iτk, a), φ(sτk, a) + γmax Q(s0, Iτk,a0) - Q?(st,It, at) ,
a0∈A
where Ξt(st, It, at) := Qt(st, It, at) - Q?(st, It, at).
Let us now define by
Lt(sτk,Iτk, a) := max Mπ,π2 Q(sτk, Iτk, a), φ(sτk, a) + γmax Q(s0, Iτk,a0) - Q?(st,It, a).
a0∈A
21
Under review as a conference paper at ICLR 2022
Then
Ξt+1 (st , It , at) = (1 - αt (st , It , at))Ξt (st , It , at) + αt (st , It , at)) [Lt (sτk , a)] .	(64)
We now observe that
E [Lt(sτk,Iτk,a)∣Ft] = V P(s0； a，Sτk) max M MnnQ(STk,L,a),Φ(sτk,a) + Ymaχ Q(s0,Iτk, a0) - Q*(sτ%, a)
s0∈S	a0∈A
= TφQt(S, Iτk , a) - Q?(S, Iτk , a).	(65)
Now, using the fixed point property that implies Q? = TφQ?, we find that
E [Lt(STk , ITk , a)|Ft] = TφQt(s, ITk , a) - TφQ* (s,ITk , a)
≤ kTφQt - TφQ? k
≤ Y kQt- Q*k∞ = Y kΞtk∞ .	(66)
using the contraction property of T established in Lemma 3. This proves (ii).
We now prove iii), that is
Var[Lt|Ft] ≤ c(1 + kΞtk2).	(67)
Now by (65) we have that
Var [Lt|Ft] = Var max Mπ,π2 Q(STk, ITk, a), φ(STk, a) + Ymax Q(S0, ITk, a0) - Q?(St, It, a)
k k	k	a0∈A	k
= E max Mπ,π2 Q(STk, ITk, a), φ(STk, a) + Ymax Q(S0,ITk, a0)
- Q?(St, It, a) - (TΦQt(S, ITk , a) - Q?(S, ITk , a))
E max Mπ,π2 Q(STk, ITk, a), φ(STk, a) + Ymax Q(S0, ITk, a0) - TΦQt(S,ITk, a)
Var max M
π,π2 Q(STk, ITk, a), φ(STk, a) + Yam0∈aAx Q(S0, ITk, a0) -TΦQt(S,ITk,a))2
≤ c(1+ kΞtk2),
for some c > 0 where the last line follows due to the boundedness of Q (which follows from
Assumptions 2 and 4). This concludes the proof of the Theorem.
Proof of Convergence with Linear Function Approximation
First let us recall the statement of the theorem:
Theorem 3 ROSA converges to a limit point r? which is the unique solution to the equation:
ΠF(Φr?) = Φr?,	a.e.	(68)
where we recall that for any test function Λ ∈ V, the operator F is defined by FΛ := Θ +
YP max{MΛ, Λ}.
Moreover, r? satisfies the following:
kΦr? - Q?k ≤ C k∏Q? - Q?k .	(69)
The theorem is proven using a set of results that we now establish. To this end, we first wish to prove
the following bound:
22
Under review as a conference paper at ICLR 2022
Lemma 4 For any Q ∈ V we have that
kFQ - Q0k ≤γkQ-Q0k,	(70)
so that the operator F is a contraction.
Proof 10 Recall, for any test function ψ , a projection operator Π acting Λ is defined by the following
ΠΛ := arg min M — Ail .
Λ ∈{Φr∣r∈Rp }
Now, we first note that in the proof of Lemma 3, we deduced that for any A ∈ L2 we have that
MA — ψ(∙,a) + γmax P aA0]∣∣ ≤ Y ∣∣A — A0k,
(c.f. Lemma 3).
Setting A = Q and ψ = Θ, it can be straightforwardly deduced that for any Q, Q ∈ L2:
∣∣MQ — Q∣∣ ≤ γ ∣∣Q — Q∣∣. Hence, using the contraction property of M, we readily deduce
the following bound:
max n∣∣MQ — Q∣∣,∣∣MQ - MQH} ≤ Y∣∣Q - Q∣∣ ,	(71)
We now observe that F is a contraction. Indeed, since for any Q, Q0 ∈ L2 we have that:
∣FQ — FQ0∣ = ∣Θ + YPmax{MQ, Q} — (Θ + YPmax{MQ0, Q0})∣∣
= γ ∣P max{MQ, Q} — P max{MQ0, Q0}∣
≤ Y ∣max{MQ, Q} — max{MQ0, Q0}∣
≤ γ ∣∣max{MQ — MQ0, Q — MQ0, MQ — Q0,Q — Q0}∣∣
≤ Ymaχ{∣∣MQ — MQ0∣∣, ∣∣Q — MQ0∣∣, ∣∣MQ — Q0∣∣, ∣∣Q — Q0∣∣}
=Y kQ - QOk ,
using (71) and again using the non-expansiveness of P.
We next show that the following two bounds hold:
Lemma 5 For any Q ∈ V we have that
i)	∣∣∏FQ — ∏FQ∣∣ ≤ y∣∣Q - Q∣∣,
ii)	∣Φr? — Q?k ≤ √≠=^ k∏Q? — Q?k.
1-γ2
Proof 11 The first result is straightforward since as Π is a projection it is non-expansive and hence:
∣∣∏FQ — ∏FQ∣∣ ≤ ∣∣FQ - FQ∣∣ ≤ y∣∣Q - Q∣∣,
using the contraction property ofF. This proves i). For ii), we note that by the orthogonality property
of projections we have that (Φr? — ΠQ?, Φr? — ΠQ?i, hence we observe that:
∣Φr? — Q?k2 = ∣Φr? — ∏Q?k2 + ∣Φr? - ∏Q*∣2
=∣∏FΦr? — ΠQ*k2 + ∣Φr? — ΠQ*∣2
≤ ∣FΦr? — Q?『+ ∣Φr? — ∏Q*k2
=kFΦr? — FQ?『+ ∣Φr? — ∏Q*k2
≤ Y2∣Φr? — Q?k2 + ∣Φr? — ∏Q*k2 ,
after which we readily deduce the desired result.
23
Under review as a conference paper at ICLR 2022
Lemma 6 Define the operator H by thefollowing: HQ(z)
.~ . ~ . _ ______________
and F by: FQ := Θ + γPHQ.
[MQ(z),
IQ(z),
if MQ(Z) > Φr*,
otherwise,
For any Q,Q ∈ L? we have that
∣∣FQ -FQ∣∣ ≤ YllQ - Q I I	(72)
and hence F is a contraction mapping.
Proof 12 Using (71), we now observe that
∣∣FQ - FQ∣∣ = ∣∣Θ + YPHQ - (Θ + YPHQ) Il
≤ YllHQ - HQ∣∣
≤ γ ∣∣max {MQ - MQ, Q - Q,MQ - Q,MQ - Q}∣∣
≤ Y max { ∣ ∣ MQ - MQ∣∣, ∣∣Q - Q∣∣, ∣∣MQ - Q∣∣, ∣∣MQ - Q∣∣}
≤ Y max {y ∣∣Q - Q∣∣, ∣∣Q - Q∣∣ ,∣∣MQ - Q∣∣ ,∣∣MQ - Q∣∣}
=Y ∣∣q - QII,
again using the non-expansive property of P.
Lemma 7 Define by Q := Θ + YPvn where
vπ (z) := Θ(sτk ,a) + Y max X P (s0; α,s% )Φr*(s0,I (τk)),	(73)
a∈A WS
then Q is a fixed point of FQ, that is FQ = Q.
Proof 13 We begin by observing that
HQ(Z)= H (Θ(z) + YPvn)
=JMQ(Z), if MQ(Z) > Φr?,
[Q(z),	otherwise,
=JMQ(Z),	if MQ(z) > Φr?,
[Θ(z) + YPvn, otherwise,
=vn (z).
Hence,
~ ~ _ _____________________________________~ _ ~
F Q = Θ + YPHQ = Θ + YPvn = Q.	(74)
which proves the result.
Lemma 8 Thefollowing bound holds:
E [vn(z0)] - E [vn(z0)] ≤ 2 [(1 - y)√(1 - y2) J1∣∣∏Q*- Q*∣∣.	(75)
Proof 14 By definitions of vn and vn (c.f (73)) and using Jensen's inequality and the stationarity
property we have that,
E [vn(z0)] - E [vn(z0)] = E [Pvn(z0)] - E [Pvn(z0)]
≤ ∣E [Pvn(z0)] - E [Pvn(zo)]∣
≤ ∣∣Pvn - Pvn ∣∣.	(76)
Now recall that Q := Θ + γPvn and Q? := Θ + γPvn*, using these expressions in (76) we find
that
E [vn(z0)] - E [vn(z0)] ≤ 1∣∣Q - q"∣.
24
Under review as a conference paper at ICLR 2022
Moreover, by the triangle inequality and using the fact that F(Φr?) = F(Φr?) and that FQ? = Q?
and FQ = Q (c.f. (75)) we have that
IlQ - q?|| ≤ ∣∣Q - F(Φr*)∣∣ + ∣∣q? - F(Φr?)∣∣
≤ Y∣∣Q - Φr*∣∣+ Y kQ? - Φr?k
≤ 2Y∣∣Q - Φr*∣∣+ Y∣∣Q? - Q∣∣,
which gives the following bound:
Q - Q?|| ≤ 2(1- Y)-1 ∣∣Q - Φr?∣∣,
from which, using Lemma 5, we deduce that
after which by (77), we finally obtain
Q - Q?|| ≤ 2 h(1 - γ)P0-T2) i-1∣∣Q - Φr?∣∣,
E [vπ (zo)] - E [vπ (zo)] ≤ 2 h(1 - γ)P0-T2) i-1∣∣Q - Φr?∣∣ ,
as required.
Let us rewrite the update in the following way:
rt+1 = rt + YtΞ(wt, rt),
where the function Ξ : R2d × Rp → Rp is given by:
Ξ(w, r) := φ(z) (Θ(z) + Ymax {(Φr)(z0), M(Φr)(z0)} - (Φr)(z)) ,
for any w ≡ (z, z0) ∈ (N × S)2 where z = (t, s) ∈ N × S and z0 = (t, s0) ∈ N × S and for any
r ∈ Rp . Let us also define the function Ξ : Rp → Rp by the following:
Ξ(r) := Ewo〜(p,p) [Ξ(wo,r)] ; wo := (zo,zι).
Lemma 9 The following statements hold for all z ∈ {0, 1} × S:
i)	(r — r*)Ξk (r) < 0,	∀r = r?,
ii)	Ξk (r?) = 0.
Proof 15 To prove the statement, we first note that each component of Ξk (r) admits a representation
as an inner product, indeed:
Ξk(r) = E [φk(z0)(Θ(z0) + Y max {Φr(z1), MΦ(z1)} - (Φr)(z0)]
= E [φk(z0)(Θ(z0) + YE [max {Φr(z1), MΦ(z1)} |z0] - (Φr)(z0)]
= E [φk(z0)(Θ(z0) + YP max {(Φr, MΦ)} (z0) - (Φr)(z0)]
= hφk , FΦr - Φri ,
using the iterated law of expectations and the definitions of P and F.
We now are in position to prove i). Indeed, we now observe the following:
(r - r?) Ξk(r) = X (r(l) - r?(l)) hφl, FΦr - Φri
l=1
= hΦr - Φr?, FΦr - Φri
= hΦr - Φr?, (1 - Π)FΦr + ΠFΦr - Φri
= hΦr - Φr?, ΠFΦr - Φri ,
where in the last step we used the orthogonality of (1 - Π). We now recall that ΠFΦr? = Φr?
since Φr? is a fixed point of ΠF. Additionally, using Lemma 5 we observe that kΠFΦr - Φr? k ≤
Y kΦr - Φr? k. With this we now find that
hΦr - Φr? , ΠFΦr - Φri
25
Under review as a conference paper at ICLR 2022
= hΦr - Φr? , (ΠFΦr - Φr?) + Φr? - Φri
≤ ∣∣Φr - Φr*k∣∣∏FΦr - Φr*∣∣ - ∣∣Φr? - Φ∏2
≤ (γ - 1) kΦr? - Φrk2 ,
which is negative since γ < 1 which completes the proof of part i).
The proof of part ii) is straightforward since we readily observe that
Ξk(r?) = hφl, FΦr? - Φri = hφl, ΠFΦr? - Φri =0,
as required and from which we deduce the result.
To prove the theorem, we make use of a special case of the following result:
Theorem 4 (Th. 17, p. 239 in [3]) Consider a stochastic process rt : R X {∞} X Ω → Rk which
takes an initial value r0 and evolves according to the following:
rt+1 = rt + αΞ(st, rt),	(77)
for some function s : R2d X Rk → Rk and where the following statements hold:
1.	{st|t = 0, 1, . . .} is a stationary, ergodic Markov process taking values in R2d
2.	For any positive scalar q, there exists a scalar μq such that E [1 + IIstkqIs ≡ so] ≤
μq (1 + kskq)
3.	The step size sequence satisfies the Robbins-Monro conditions, that is	t∞=0 αt = ∞ and
Pt∞=0 αt2 <∞
4.	There exists scalars c and q such that ∣Ξ(w, r)∣ ≤ c (1 + ∣w∣q)	(1	+	∣r∣)
5.	There exists scalars c and q such that t∞=0 ∣E [Ξ(wt, r)|z0	≡	z]	- E [Ξ(w0, r)]∣	≤
c(1+∣w∣q)(1+∣r∣)
6.	There exists a scalar c > 0 such that ∣E[Ξ(wo, r)] 一 E[Ξ(w0,尸)]∣ ≤ Ckr 一 尸∣
7.	There exists scalars c >	0 and q >	0 such that
P∞=o l∣E [Ξ(wt,r)∣wo ≡ w] — E[Ξ(wo,尸)]∣ ≤ Ckr - r∣ (1 + ∣∣wkq)
8.	There exists some r? ∈ Rk such that Ξ(r)(r — r?) < 0 for all r = r? and s(r?) = 0.
Then rt converges to r? almost surely.
In order to apply the Theorem 4, we show that conditions 1 - 7 are satisfied.
Proof 16 Conditions 1-2 are true by assumption while condition 3 can be made true by choice of the
learning rates. Therefore it remains to verify conditions 4-7 are met.
To prove 4, we observe that
kΞ(w, r)k = kφ(z) (Θ(z) + γ max {(Φr)(z0), MΦ(z0)} - (Φr)(z))k
≤	kφ(z)k kΘ(z) + γ (kφ(z0)k krk + MΦ(z0))k + kφ(z)k krk
≤	kφ(z)k (kΘ(z)k + γkMΦ(z0)k) + kφ(z)k (γ kφ(z0)k + kφ(z)k) krk.
Now using the definition of M, we readily observe that kMΦ(z0)k ≤ kΘk + γ kPsπ0 s Φk ≤ kΘk +
γ kΦk using the non-expansiveness ofP.
Hence, we lastly deduce that
kΞ(w, r)k ≤ kφ(z)k (kΘ(z)k + γkMΦ(z0)k) + kφ(z)k (γ kφ(z0)k + kφ(z)k) krk
≤ kφ(z)k (kΘ(z)k + γkΘk + γkψk) + kφ(z)k (γ kφ(z0)k + kφ(z)k) krk,
we then easily deduce the result using the boundedness of φ, Θ and ψ.
26
Under review as a conference paper at ICLR 2022
Now we observe the following Lipschitz condition on Ξ:
IlE(W,r) - ξ(W,列I
=kφ(z)(γ max {(Φr)(z0), MΦ(z0)} — Y max {(Φf)(z0), MΦ(z0)}) — ((Φr)(z) — Φf(z))∣
≤ Y ∣∣φ(z)k ∣max {φ0(z0)r, MΦ0(z0)}- max {(φ0(z0)尸),MΦ0(z0)}∣ + ∣∣φ(z)∣ ∣φ0(z)r — φ(z)刊∣
≤ Y kΦ(z)kkΦ0(z0)r — φ0(z0)尸k + kΦ(z)kkΦ0(z)r -。0(，)目|
≤kφ(z)k (kφ(z)k + Y kφ(z)k kφ0(z0) — φ0(z0)k) kr —不
≤ C Ilr —刊I ,
using Cauchy-Schwarz inequality and that for any scalars a, b, c we have that
|max{a, b} — max{b, c}| ≤ |a — c|.
Using Assumptions 3 and 4, we therefore deduce that
∞
X ∣∣E [Ξ(w,r) — Ξ(w,f)∣W0 = w] — E [Ξ(w°,r) — Ξ(w°,尸)∣] ≤ C IIr - r∣ (1 + IlW『).(78)
t=0
Part 2 is assured by Lemma 5 while Part 4 is assured by Lemma 8 and lastly Part 8 is assured by
Lemma 9.
27
Under review as a conference paper at ICLR 2022
Figure 8: ROSA is robust to the component used to generate exploration bonus L. ROSA works
equally well when we RND or Count-based method forL.
Rebuttal
17	Additional Experiment 1 - Replacing RND for Bonus Reward
In this experiment we sought to ascertain if ROSA is robust to the component used to generate
exploration bonus L. We ran two versions of ROSA, one where L is computed using RND and
one where L is computed using a simple count-based measure CoUnt(S) where S ∈ S (the function
Count(s) simply tallies the number of times state s has been visited). Figure 8 shows performance of
these two versions of ROSA on the Maze environment shown in Figure 1. ROSA performs equally
well with both components, indicating that it is not dependent on a fine-tuned exploration bonus.
The additional machinery of switching controls and choices of intrinsic rewards to add mean that
ROSA can work with basic exploration bonuses. Note that we generally use RND since it is simple
to implement and works equally well on discrete and continuous state spaces.
18	ADDITIONAL EXPERIMENT 2 - ROBUSTNESS TO INITIALISATION OF φ
Some reviewers raised important questions about the requirement of a carefully initialised phi
function. Here, we show performance of individual runs of ROSA on the Maze shown in Figure 1. In
each run, φ is randomly initialised using Xavier initialisation, and as a consequence ROSA works
with a different φ function in each run. As can be seen in Figure 9, despite the randomness in φ the
performance of ROSA does not vary significantly over the runs. This suggests that ROSA can adapt
to the φ function it is presented with, and still come up with good reward shaping.
28
Under review as a conference paper at ICLR 2022
0.0
O 8
L0.
6.4.2
0.0.0.
UJmωB
I
2
I	I
4	6
Steps (x 10000)
I
8
10
Figure 9: ROSA is robust to the component used to generate exploration bonus L. ROSA works
equally well when we RND or Count-based method forL.
29