Under review as a conference paper at ICLR 2022
On the Relationship between Heterophily and
Robustness of Graph Neural Networks
Anonymous authors
Paper under double-blind review
Ab stract
Empirical studies on the robustness of graph neural networks (GNNs) have sug-
gested a relation between the vulnerabilities of GNNs to adversarial attacks and
the increased presence of heterophily in perturbed graphs (where edges tend to
connect nodes with dissimilar features and labels). In this work, we formalize
the relation between heterophily and robustness, bridging two topics previously
investigated by separate lines of research. We theoretically and empirically show
that for graphs exhibiting homophily (low heterophily), impactful structural at-
tacks always lead to increased levels of heterophily, while for graph with het-
erophily the change in the homophily level depends on the node degrees. By
leveraging these insights, we deduce that a design principle identified to signif-
icantly improve predictive performance under heterophily—separate aggregators
for ego- and neighbor-embeddings—can also inherently offer increased robust-
ness to GNNs. Our extensive empirical analysis shows that GNNs adopting this
design alone can achieve significantly improved empirical and certifiable robust-
ness compared to the best-performing unvaccinated model. Furthermore, models
with this design can be readily combined with explicit defense mechanisms to
yield improved robustness with up to 18.33% increase in performance under at-
tacks compared to the best-performing vaccinated model.
1 Introduction
Graph neural networks (GNNs) aim to translate the enormous empirical success of deep learning
to data defined on non-Euclidean domains, such as manifolds or graphs (Bronstein et al., 2017),
and have become important tools to solve a variety of learning problems for graph structured and
geometrically embedded data. However, recent works show that GNNs—much like their “standard”
deep learning counterparts—have a high sensitivity to adversarial attacks: intentionally introduced
minor changes in the graph structure can lead to significant changes in performance. This finding,
first articulated by Zugner et al. (2018) and Dai et al. (2018), has triggered studies that investigated
different attack scenarios (Xu et al., 2019; Wu et al., 2019; Li et al., 2020a; Ma et al., 2020).
A different aspect of GNNs that has been scrutinized recently is that most GNNs do not perform well
with many heterophilous datasets. GNNs generally perform well under homophily (or assortativity),
i.e., the tendency of nodes with similar features or class labels to connect (Pei et al., 2020; Zhu et al.,
2020). Such datasets are thus called homophilous (or assortative). While homophilous datasets
dominate the study of networks, homophily is not a universal principle; certain networks, such as
romantic relationship networks or predator-prey networks in ecology, are mostly heterophilous (or
disassortative). Employing a GNN which does not account for heterophily can lead to significant
performance loss in heterophilous settings (Abu-El-Haija et al., 2019; Zhu et al., 2020; Bo et al.,
2021). Previous works have thus proposed architectures for heterophilous data.
While previous work has focused on naturally-occurring heterophily, heterophilous interactions may
also be introduced as adversarial noise: as many GNNs exploit homophilous correlation, they can
be sensitive to changes that render the data more heterophilous. A natural follow-up question is
if and how this observation manifests itself in previously proposed attacking strategies on GNNs.
In this work, we thus investigate the relation between heterophily and robustness of GNNs against
adversarial perturbations of graph structure, focusing on semi-supervised node classification tasks.
More specifically, our main contributions are:
1
Under review as a conference paper at ICLR 2022
•	Formalization: We formalize the relation between adversarial structural attacks and the change
of homophily level in the underlying graphs with theoretical (§3.1) and empirical (§5.1) analysis.
Specifically, we show that on homophilous graphs, effective structural attacks lead to increased
heterophily, while, on heterophilous graphs, they alter the homophily level contingent on node
degrees. To our knowledge, this is the first formal analysis of such kind.
•	Heterophily-inspired Design: We show how the relation between attacks and heterophily can in-
spire more robust GNNs by demonstrating that a key architectural feature in handling heterophily,
separate aggregators for ego- and neighbor-embeddings, also improves the robustness of GNNs
against attacks (§3.2).
•	Extensive Empirical Analysis: We show the effectiveness of the heterophilous design in im-
proving empirical (§5.2) and certifiable (§5.3) robustness of GNNs with extensive experiments
on real-world homophilous and heterophilous datasets. Specifically, we compare GNNs with this
design, which we refer to as heterophily-adjusted GNNs, to non-adjusted models, including state-
of-the-art models designed with robustness in mind. We find that heterophily-adjusted GNNs are
up to 5 times more certifiably robust and have stronger performance under attacks by up to 32.92%
compared to non-adjusted, standard models. Moreover, this design can be combined with exist-
ing vaccination mechanisms, yielding up to 18.33% higher accuracy under attacks than the best
non-adjusted vaccinated model.
2 Notation and Preliminaries
Let G = (V, E, X) be a simple graph with node set V, edge set E and node attributes X. The one-hop
neighborhood N(v) = {u : (u, v) ∈ E} of a node v ∈ V is the set of all nodes directly adjacent
to v; the k-hop neighborhood of v ∈ V is the set of nodes reachable by a shortest path of length k .
We represent the graph G algebraically by an adjacency matrix A ∈ {0,1}lVl×lVl and node feature
matrix X ∈ RlVl×F. We use As = A +1 to denote the adjacency matrix with self-loops added, and
denote the corresponding row-stochastic matrices as A = DTA and As = D-1As, respectively,
where D is a diagonal matrix with Dii = Pj Aij (Ds is defined analogously). We further assume
that there exists a vector y, which contains a unique class label yv for each node v. Given a training
set TV = {(v1, y1), (v2, y2), ...} of labeled nodes, the goal of semi-supervised node classification is
to learn a mapping ` : V → Y from the nodes to the set Y of class labels.
Graph neural networks (GNNs). Most current GNNs operate according to a message passing
paradigm where a representation vector rv is assigned to each node v ∈ V and continually updated
by K layers of learnable transformations. These layers first aggregate representations over neigh-
boring nodes N(v) and then update the current representation via an encoder ENC. For prevailing
GNN models like GCN (KiPf & Welling, 2017) and GAT (VeIiCkOviC et al., 2018), each layer can
be formalized as r(vk)
ENC AGGR r(uk-1) : u ∈ N(v) ∪ {v}	,
where AGGR is the mean func-
tion weighted by node degrees (GCN) or an attention mechanism (GAT), and ENC is a learnable
(nonlinear) mapping.
Adversarial attacks on graphs. Given a graph G = (V, E, X) and a GNN f that processes G, an
adversarial attacker tries to create a perturbed graph G0 = (V, E0, X) with a modified edge-set E0
such that the performance of the GNN f is maximally degraded. The information available to the
attacker can vary under different scenarios (Jin et al., 2020a; Sun et al., 2020). Here, we follow
the gray-box formalization by Zugner et al. (2018), where the attacker knows the training set TV,
but not the trained GNN f . The attacker thus considers a surrogate GNN and picks perturbations
that maximize an attack loss Latk (Zugner et al., 2018; Zugner & Gunnemann, 2019a), assuming
that attacks to the surrogate model are transferable to the attacked GNN. For node classification,
the attack loss Latk quantifies how the predictions zv ∈ [0, 1]|Y| made by the GNN f differ from
the true labels y. For a targeted attack of node v with class label yv ∈ Y, we adopt the negative
classification margin (CM-type) (Zugner et al., 2018; Xu et al., 2019): Latk = -∆c = -(zv,yv -
maxy6=yv zv,y). The attacker usually has additional constraints, such as a limit on the size of the
perturbations allowed (ZUgner et al., 2018; ZUgner & Gunnemann, 2019a).
Taxonomy of attacks. We follow the taxonomy of attacks introduced in (Jin et al., 2020a; Sun et al.,
2020). For node classification, the attacker may aim to change the classification of a specific node
v ∈ V (targeted attack), or to decrease the overall classification accuracy (untargeted attack).
Attacks can also happen at different stages of the training process: we refer to attacks introduced
2
Under review as a conference paper at ICLR 2022
before training as (pre-training) poison attacks, and attacks introduced after the training process
(and before potential retraining on perturbed data) as (post-training) evasion attacks. While our
theoretical analysis (§3) mainly considers targeted evasion attacks, we consider other attacks in our
empirical evaluation (§5).
Characterizing homophily and heterophily in graphs. Using class labels, we characterize the
types of connections in a graph contributing to its overall level of homophily/heterophily as follows:
Definition 1 (Homo/Heterophilous path and edge) A k-hop homophilous path from node w to u
is a length-k path between endpoint nodes with the same class label yw = yu. Otherwise, the path
is called heterophilous. A homophilous or heterophilous edge is a special case with k = 1.
Following (Zhu et al., 2020; Lim et al., 2021), we define the homophily ratio h as:
Definition 2 (Homophily ratio) The homophily ratio is the fraction of homophilous edges among
all the edges in a graph: h = |{(u, v) ∈ E|yu = yv}|/|E |.
When the edges in a graph are wired randomly, independent to the node labels, the expectation for
h is hr = 1/|Y| for balanced classes (Lim et al., 2021). For simplicity, we informally refer to
graphs with homophily ratio h	1/|Y | as homophilous graphs (which have been the focus in
most prior works), graphs with homophily ratio h 1/|Y| as heterophilous graphs, and graphs
with homophily ratio h ≈ 1/|Y| as weakly heterophilous graphs.
3 Relation between Graph Heterophily & Model Robustness
In this section, we first show theoretical results on the relation between adversarial structural attacks
and the change in the homophily level of the underlying graphs. Though empirical analyses from
previous works have suggested this relation on homophilous graphs (Wu et al., 2019; Jin et al.,
2020a), to our knowledge, we are the first to formalize it with theoretical analysis and address the
case of heterophilous graphs. As an implication of the relation, we then discuss how akey design that
improves predictive performance of GNNs under heterophily can also help boost their robustness.
3.1	How Do Structural Attacks Change Homophily in Graphs ?
Homophilous Graphs: Structural Attacks are Mostly Heterophilous Attacks. Our first re-
sult shows that, for homophilous data, effective structural attacks on GNNs (as measured by loss
Latk) always result in a reduced level of homophily where either new heterophilous connections
are added or existing homophilous connections are removed. It also states that direct perturba-
tions on 1-hop neighbors of the target nodes are more effective than indirect perturbations (influ-
encer attacks (Zugner et al., 2018)) on multi-hop neighbors. For simplicity, akin to previous works
(Zugner et al., 2018; ZUgner & Gunnemann, 2019a) We establish our results for targeted evasion
(post-training) attacks in a stylized learning setup with a linear GNN. However, our findings gen-
eralize to more general setups on real-world datasets as we show in our experiments (§5.1). In the
theorems below, we use the notion of gambit node: node u is called a gambit if a perturbation that
targets node v ∈ V adjusts the connectivity of node u ∈ V.
Theorem 1 Let G = (V, E, X) be a self-loop-free graph with adjacency matrix A and node features
Xv = P ∙ onehot(yv) + 1-p ∙ 1 for each node V, where 1 is an all-1 vector, and P is a parameter that
regulates the signal to noise ratio. Assume that a fraction h of each node’s neighbors belong to the
same class, while a fraction ∣Y-h1 belongs uniformly to any other class. Consider a 2-layer linear
GNN f(2)(A, X) = AS2XW trained on a training set TV ⊆ DV, with at least one node from each
class y ∈ Y, and degree d for all nodes with a distance less than 2 to any v ∈ DV. For a unit
structural perturbation that involves a target node v ∈ DV, and a correctly classified gambit node
with degree da, the following statements hold if h ≥ ɪ:
1.	the attack loss Latk (§2) of the target v increases only for actions increasing heterophily, i.e.,
when removing a homophilous edge or path, or adding a heterophilous edge or path to node v;
2.	direct perturbations on edges (or 1-hop paths) incident to the target node v lead to greater
increase in Latk than indirect perturbations on multi-hop paths to target node v.
We give the proof in App. C.1. Intuitively, the relative inability of existing GNNs to make full use
of heterophilous data (Pei et al., 2020; Zhu et al., 2020) can be exploited by inserting heterophilous
3
Under review as a conference paper at ICLR 2022
connections in graphs where homophilous ones are expected. Though the theorem shows that effec-
tive attacks on homophilous graphs necessarily reduce the homophily level, the converse is not true:
not all perturbations which reduce the homophily level are effective attacks (Ma et al., 2021).
Heterophilous Graphs: Structural Attacks Can Be Homophilous or Heterophilous, depending
on Node Degrees. When a graph displays heterophily, our analysis shows a more complicated
picture on how the level of homophily in the graph is changed by effective structural attacks: in
heterophilous case, the direction of change is dependent on the degrees of both the target node v and
the gambit node u of the attack. Specifically, if the degree of either node is low, attacks increasing
the heterophily are still effective; however, if the degrees d and da of both nodes are high, attacks
decreasing the heterophily will be effective. Similar to the homophilous case, we formalize our
results below for targeted evasion attacks in a stylized learning setup.
Theorem 2 Under the setup of Thm. 1, for a unit perturbation that involves a target node v with
degree d, and a correctly classified gambit node with degree da, the following statements hold:
1.	(Low-degree target node) if 0 < d ≤ |Y | - 2, for any da ≥ 0 and h ∈ [0, 1], the attack loss
Latk (§2) of v increases only under actions increasing heterophily in the graph;
2.	(High-degree target node) if d > |Y| - 2, conditioning on the degree da of the gambit node:
(a)	(Low-degree gambit node) if da < (d+-)YY+-1)，for any h ∈ [0,1], the attack loss Latk
(§2) of v increases only under actions increasing heterophily in the graph;
(b)	(High-degreegambitnode) ifd” ≥ 叱-⑶-1),for0 ≤ h < da(dTγ∣++1-∣%2)(lYIT) <
-|Y |+2	( +1)|Y | a
击,Latk (§2) of V increases only under actions reducing heteroPhily in the graph.
In the statements above, the actions increasing heterophily in the graph include removing a ho-
mophilous edge or adding a heterophilous edge to node v, and the actions reducing heteroPhily in
the graph include when adding a homophilous edge or removing a heterophilous edge to node v.
The above theorems cover the situation when the gambit nodes are initially classified correctly
(where attacks introducing heteroPhily can be unambiguously defined using the ground-truth class
labels of the nodes involved). However, in §5.1, we show on real-world datasets that a relaxed in-
terPretation of the theorems, where heteroPhily is instead defined by the predicted class labels of
GNNs, can exPlain the behavior of the attacks regardless of the initial correctness of the gambits.
3.2 Boosting Robustness with A Simple Heterophilous Design
A natural follow-uP question is whether GNNs with better Performance under heteroPhily are also
more robust against structural attacks. We deduce that akey design for imProving GNN Performance
for heteroPhilous data—seParate aggregators for ego- and neighbor-embeddings—can also boost
the robustness of GNNs by enabling them to better coPe with adversarially-introduced changes in
heteroPhily.
Separate Aggregators for Ego- and Neighbor-embeddings. This design uses seParate GNN ag-
gregators for ego-embedding rv and neighbor-embeddings {ru : u ∈ N (v)}. Formally, the rePre-
sentation learned for node v in the k-th layer is:
r(vk) = ENC AGGR1(r(vk-1), r(vk-2),..., r(v0)), AGGR2({r(uk-1) : u ∈ N(v)}) ,	(1)
where AGGR1 and AGGR2 are separate aggregators, such as averaging functions (GCN), atten-
tion mechanisms (GAT), or other Pooling mechanisms (Hamilton et al., 2017). The ego-aggregator
AGGR1 may also introduce skiP connections (Xu et al., 2018) to the ego-embeddings aggregated
in Previous layers as shown in Eq. (1). This design has been utilized in existing GNN models (see
§D.1 for details), and has been shown to significantly boost the rePresentation Power of GNNs under
natural heteroPhily (Zhu et al., 2020).
Intuition. The key design changes, as comPared to the GCN formulation in §2, allow for the ego-
embedding rv to be aggregated and weighted separately from the neighbor-embeddings {ru : u ∈
N (v)}, as well as for the use of skiP connections to ego-embeddings of Previous layers. Intuitively,
ego-embeddings of feature vectors at the first layer are indePendent of the graPh structure and thus
unaffected by adversarial structural Perturbations. Hence, a seParate aggregator and skiP connections
can Provide better access to unPerturbed information, and helPs mitigate the effects of the attacks.
Theoretical Analysis. We formalize the above intuition that shows how seParate aggregators for
ego- and neighbor-embeddings enable GNN layers to reduce the attack loss.
4
Under review as a conference paper at ICLR 2022
Theorem 3 Under the setup of Thm. 1, consider two alternative layers from which a two-layer
linear GNN is built: (1) a layer defined as fs(A, X) = As XW; and (2) a layer formulated as
f (A, X; α) = ((1 一 α) A + ɑl) XW, which mixes the ego- and neighbor-embedding linearly Un-
der a predefined weight α ∈ [0, 1]. Then, for h > 1/|Y |, α > 1/(1 + da), and a unit perturbation
increasing Latk as in Thm. 1, outputs of layer f lead to a strictly smaller increase in Latk than fs.
We provide the proof in App. C.3; note that for α = 1/(1 + da), the two layers are the same:
f(A, X; α) = fs (A, X). Theorem 3 shows that an increase to the weights of ego-embedding
improves the robustness of the GNN f for a homophily ratio h > 1/|Y|. Though aggregators and
encoders are stylized in the theorem, the empirical analysis in §5.2 confirms that GNNs with more
advanced aggregators and encoders also benefit from separate aggregators. Specifically, we find that
such GNNs outperform methods without this design by up to 33.33% and 48.88% on homophilous
and heterophilous graphs, respectively, while performing comparably on clean datasets.
4	Related Work
Adversarial Attacks and Defense Strategies for Graphs Since NETTACK (Zugner et al., 2018)
and RL-S2V (Dai et al., 2018) first demonstrated the vulnerabilities of GNNs against adversarial
perturbations, a variety of attack strategies under different scenarios have been proposed, including
adversarial attacks on the graph structure (Dai et al., 2018; Xu et al., 2019; BCjchevski & Gunne-
mann, 2019a; Li et al., 2020a; Chang et al., 2020), node features (Takahashi, 2019; Ma et al., 2020),
or combinations of both (Zugner et al., 2018; Zugner & Gunnemann, 2019a; Wu et al., 2019). On
the defense side, various techniques for improving the GNN robustness against adversarial attacks
have been proposed, including: adversarial training (Xu et al., 2019; Zugner & Gunnemann, 2019a;
BQjChevSki & Gunnemann, 2019b); RGCN (Zhu et al., 2019), which adopts Gaussian-based embed-
dings and a variance-based attention mechanism; low-rank approximation of graph adjacency (En-
tezari et al., 2020) against Nettack (Zugner et al., 2018); Pro-GNN (Jin et al., 2020b), which es-
timates the unperturbed graph structure in training with the assumptions of low-rank, sparsity, and
homophily of node features; GCN-Jaccard (Wu et al., 2019) and GNNGuard (Zhang & Zitnik, 2020),
which assume homophily of features (or structural embeddings) and train GNN models on a pruned
graph with only strong homophilous links; and Soft Medoid (Geisler et al., 2020), an aggrega-
tion function with improved robustness. Other recent works have looked into the certification of
nodes that are guaranteed to be robust against certain structural and feature perturbations (Zugner
& Gunnemann, 2019b; Bojchevski & Gunnemann, 2019b; Zugner & Gunnemann, 2020), including
approaches based on model-agnostic randomized smoothing (Cohen et al., 2019; Lee et al., 2019;
Bojchevski et al., 2020). Interested readers are referred to the recent surveys (Jin et al., 2020a; Sun
et al., 2020) for a comprehensive review of the literature.
GNNs & Heterophily Recent works (Pei et al., 2020; Liu et al., 2020; Zhu et al., 2020; Ma et al.,
2021) have shown that heterophilous datasets can lead to significant performance loss for popular
GNN architectures (e.g., GCN (KiPf & Welling, 2017), GAT (VeIiCkOViC et al., 2018)). This issue is
also known in classical semi-supervised learning (Peel, 2017). To address this issue, several GNN
designs for handling heterophilous connections have been proposed (Abu-El-Haija et al., 2019; Pei
et al., 2020; Zhu et al., 2020; Dong et al., 2021; Li et al., 2021; Zhu et al., 2021; Bo et al., 2021).
Yan et al. (2021) recently discussed the connection between heterophily and oversmoothing for
GNNs, and proposed designs to simultaneously address both issues. However, the formal connection
between heterophily and robustness of GNNs has received little attention. Here we focus on a simple
yet powerful design that significantly improves performance under heterophily (Zhu et al., 2020),
and can be readily incorporated into GNNs.
5	Empirical Evaluation
Our analysis seeks to answer the following questions: (Q1) Does our theoretical analysis on the rela-
tions between adversarial attacks and changes in heterophily level generalize to real-world datasets?
(Q2) Do heterophily-adjusted GNNs, i.e., models with separate aggregators for ego- and neighbor-
embeddings, show improved robustness against state-of-the-art attacks? (Q3) Does the identified
design improve the certifiable robustness of GNNs?
5
Under review as a conference paper at ICLR 2022
First, we describe the experimental setup and datasets that we use to answer the above questions.
Attack Setup. We consider both targeted and untargeted attacks (§2), generated by NET-
TACK (Zugner et al., 2018) and Metattack (Zugner & Gunnemann, 2019a), respectively. For each
attack method, we consider poison (pre-training) and evasion (post-training) attacks, yielding 4 at-
tack scenarios in total. We focus on robustness against structural perturbations and keep the node
features unchanged. We randomly generate 3 sets of perturbations per attack method and dataset,
and consistently evaluate each GNN model on them. We provide more details in App. D.2.
GNN Models. To show the effectiveness of our identified design, we evaluate four groups of models
against adversarial attacks: (1) Models with this heterophilous design only: GraphSAGE (Hamilton
et al., 2017), H2GCN (Zhu et al., 2020), CPGNN (Zhu et al., 2021), GPR-GNN (Chien et al., 2021)
and FAGCN (Bo et al., 2021); we discuss how these models instantiate this design in App. §D.1;
(2) State-of-the-art “vaccinated” architectures designed with robustness in mind: ProGNN (Jin et al.,
2020b), GNNGuard (Zhang & Zitnik, 2020) and GCN-SVD (Entezari et al., 2020); (3) Models
with both the heterophilous design and explicit robustness-enhancing mechanisms based on low-
rank approximation: H2GCN-SVD and GraphSAGE-SVD; (4) Models without any vaccination,
including some of the most popular methods: GCN (KiPf & Welling, 2017), GAT (Velickovic et al.,
2018), and the graph-agnostic multilayer perceptron (MLP) which relies only on node features. We
discuss the implementations and parameters used for the models in App. D.
Datasets & Evaluation Setup. We con- sider two standard datasets with strong ho- mophily, Cora (McCallum, 2000) and Cite- seer (Sen et al., 2008), complemented with		Table 1: Dataset statistics.			
		Homophilous graphs		Heterophilous graphs	
		Cora	Citeseer	FB100	Snap
one weakly and one strongly heterophilous	#Nodes |V |	2,485	2,110	2,032	4,562
graph, introduced by Lim et al. (2021):	#Edges |E |	5,069	3,668	78,733	12,103
FB100 (Traud et al., 2012) and Snap	#Classes |Y | #Features F	7 1,433	6 3,703	2 1,193	5 269
Patents (Leskovec et al., 2005; Leskovec &	Homophily h	0.804	0.736	0.531	0.134
Krevl, 2014). We report summary statistics
in Table 1, and provide more details in App. D.5. For computational tractability, we subsample the
Snap Patents data via snowball sampling (Goodman, 1961), where we keep 20% of the neighbors
for each traversed node (see App. D.5 for details). We follow the evaluation procedure of Zugner
et al. (2018) and Jin et al. (2020b), where we split the nodes of each dataset into training (10%),
validation (10%) and test (80%) data, and determine the model parameters using the training and
validation splits. We report the average performance and standard deviation on the 3 sets of gener-
ated perturbations. For targeted attacks with Nettack, we report the classification accuracy on the
target nodes; for untargeted attacks with Metattack, we report it over the whole test data.
Robustness Certificates. We adopt randomized smoothing for GNNs (Bojchevski et al., 2020) to
evaluate the certifiable robustness, with parameter choices as detailed in App. D.2. We only consider
structural perturbations in the randomization scheme. Following Geisler et al. (2020), we measure
the certifiable robustness of GNN models with the accumulated certifications (AC) and the average
maximum certifiable radii for edge additions (尸a) and deletions (Td) over all correctly predicted
nodes. More specifically, AC is defined as -R(0, 0) + Pr r ≥0 R(ra, rd), where R(ra, rd) is the
a, d
certifiably correct ratio, i.e., the ratio of the nodes in the test splits that are both predicted correctly by
the smoothed classifier and certifiably robust at radius (ra, rd). In addition, we report the accuracy
of each model with randomized smoothing enabled on the test splits of the clean datasets, which is
equal to R(0, 0). We report the average and standard deviation of each statistic over the 3 different
training, validation and test splits.
5.1	(Q1) Structural Attacks are Mostly Heterophilous: Empirical Validation
To show that our theoretical analysis in §3.1 generalizes to more complex settings beyond the as-
sumptions we made in the theorems, we look into effective targeted attacks made by Nettack on
real-world homophilous and heterophilous datasets, and present statistics of the attacks in Table 2,
with a focus on the ratios of heterophilous attacks. We use a budget of 1 perturbation per target node
in this experiment, and the statistics are reported among all effective perturbations targeting nodes
that are correctly classified on clean datasets by the surrogate GNN used by Nettack (i.e., GCN)
as described in §5. To help validate the dependency between the degrees of the target/gambit nodes
6
Under review as a conference paper at ICLR 2022
and the changes of heterophily predicted by Thm. 2, we also show the scatter plots of target and
gambit node degrees in Fig. 2 in App. E.5.
Homophilous Networks. For the strongly
homophilous Cora and Citeseer graphs,
all changes in the graph structure that
are introduced by effectives attacks fol-
low the conclusion of Thm. 1: they re-
duce homophily (increase heterophily) by
adding heterophilous edges or removing
homophilous edges. These results show
that despite the simplified analysis, the
takeaway of Thm. 1 can be generalized to
real-world datasets. In addition, the at-
tacks mostly introduce, rather than prune,
edges, suggesting that attacks adding out-
lier edges to the graph are more powerful
Table 2: Effective targeted attacks by Nettack: ra-
tios of edge additions, deletions and heterophilous at-
tacks (i.e., attacks increasing heterophily). We consider
two heterophily definitions, one based on ground-truth
class labels (Label), and the other on predicted class la-
bels by GCN on clean datasets (Pred.). All attacks are
direct perturbations on edges incident to the targets.
	Dataset	Sample	Attack Type		Hete. Attacks	
		Sizes	Add.	Del.	Label	Pred.
						
Nettack	Cora	150	99.33%	0.67%	100.00%	100.00%
	Citeseer FB100'"	121 "112	100.00% “00：00%…	0.00% "0.00%'-'	100.00% ……50.00%…	100.00% -1Q0,O0%-
	Snap	51	100.00%	0.00%	64.71%	100.00%
than attacks removing informative existing edges. These observations in our experiments are con-
sistent with the observations from previous works (Jin et al., 2020a; Geisler et al., 2020).
Heterophilous Networks. For heterophilous graphs FB100 (h ≈ 1/|Y |) and Snap (h < 1/|Y|),
Fig. 2 in App. E.5 shows that almost all attacks leverage gambit nodes with low degrees (1 or 2); no
node with degree higher than 5 is leveraged. All attacks leveraging correctly classified gambit nodes
are connecting node u ∈ V with a different ground-truth class label yu 6= yv to the target nodes
v ∈ V; attacks leveraging incorrectly classified gambit nodes are always connecting node u with a
different predicted class label Iyu = yv = yv to the target node v, even though some gambit nodes
have the same ground-truth class label yu = yv = yu as the target nodes. These results validate
the conclusion of Thm. 2 on correctly classified gambit nodes, and demonstrate its generalizability
under the heterophily definition based on predicted class labels. Note that the predicted class labels
yu for each node U ∈ V are based on GCN, which is the surrogate GNN used by NETTACK.
5.2	(Q2) Benchmark Study of GNN Models: Heterophilous Design Leads to
Improved Empirical Robustness
To answer (Q2) on whether heterophily-adjusted GNN models show improved performance against
state-of-the-art attacks, we conduct a comprehensive benchmark study. We consider all four cat-
egories of GNN models mentioned in §5, and evaluate their robustness against both targeted and
untargeted attacks. We report the hyperparameters for each method in App. D.4. Table 3 shows
the performance of each method under poison (pre-training) attacks, and Fig. 1 visualizes the corre-
sponding performance changes relative to the clean datasets. For conciseness, we report further re-
sults under evasion (post-training) attacks and on clean (unperturbed) data in the Appendix (Table 7
for Nettack; Table 8 for Metattack). Also, in App. E.4 we discuss how our simple heterophilous
design leads to only minor computational overhead compared to existing vaccination mechanisms.
Targeted attacks by Nettack. 1 Poison attacks. Under targeted poison attacks, Table 3 (left)
shows that GraphSAGE-SVD and H2GCN-SVD, which combine our identified design with a low-
rank vaccination approach adopted in GCN-SVD (Entezari et al., 2020) (details in App. D.3), out-
perform state-of-the-art methods across all datasets by up to 13.34% in homophilous settings and
18.33% in heterophilous settings.
Methods merely employing the identified design also show significantly improved robustness,
though there are differences in the amount of robustness improvement due to architectural differ-
ences. Specifically, these methods outperform the best unvaccinated method (GAT) on all datasets
by up to 32.92% in average, despite having comparable performance on clean datasets. On Citeseer
and FB100, methods with the heterophilous design also show comparable or even better robustness
than state-of-the-art vaccinated GNNs like ProGNN and GCN-SVD.
2 Evasion attacks. Under evasion attacks (Table 7), we observe similar trends as in poison attacks:
GraphSAGE-SVD and H2GCN-SVD are up to 20.55% more accurate than the GCN-SVD baseline,
and methods featuring the identified design alone achieve up to 24.45% gain in average performance
against the best unvaccinated baseline. We note that GNNGuard and ProGNN are not capable of ad-
7
Under review as a conference paper at ICLR 2022
Table 3: Benchmark study: mean accuracy against poison attacks (accuracy on clean datasets in
paranthesis). Accuracy is reported on target nodes for Nettack, and on full test splits for Metattack.
Best GNN model is highlighted in blue per dataset, and in gray per model group. MLP is immune
to structural attacks and not considered as a GNN model. Detailed results, including stdev and
accuracy against evasion attacks, are listed in Table 7 and 8 and the setup in §5.
	Hetero.	Vaccin.		HomoPhilous graPhs				HeteroPhilous graPhs					HomoPhilous graPhs				HeteroPhilous graPhs			
				Cora h=0.804		Citeseer h=0.736		FB100 h=0.531		SnaP h=0.134			Cora h=0.804		Citeseer h=0.736		FB100 h=0.531		SnaP h=0.134	
H2 GCN-SVD	X	X		70.00	(74.44)	65.00	(70.00)	59.44	(61.67)	28.89	(30.56)		67.87	(76.89) I	70.42	(73.42)	56.72	(56.81)	25.60	(27.63)
GraPhSAGE-SVD	X	X		71.67	(77.22)	67.78	(70.00)	■60.00	(60.00)	26.67	(2722)		68.86	(77.52)	69.10	(72.16)	55.76	(57.38)	26.58	(26.72)
H2GCN		…X 一			…38.89	(82.78)	^^^27.22	(69.44)	—…27.78	(60.56)	… 12.78	(30.00)		…57.75	(83.94)	…54.34	(75.34)	…54.84	(56.95)	一 25.34	(27.49)
GraPhSAGE	X			36.67	(82.22)	31.67	(70.56)	33.89	(60.00)	16.67	(24.44)		54.68	(82.21)	59.74	(74.64)	54.72	(56.60)	24.14	(27.18)
CPGNN	X			47.22	(81.67)	40.56	(73.33)	49.44	(66.11)	21.67	(28.89)		74.55	(80.67)	68.07	(74.92)	61.58	(60.17)	26.76	(27.13)
GPR-GNN	X			21.67	(82.22)	24.44	(67.78)	2.78	(56.67)	4.44	(27.78)		48.29	(81.84)	35.25	(70.71)	59.94	(62.40)	21.06	(26.08)
FAGCN	X			26.11	(83.33)	25.56	(70.56)	6.11	(58.33)	8.33	(29.44)		60.11	(81.59)	53.18	(73.99)	55.97	(59.64)	24.04	(27.15)
GNNGuard			…X		…58.33	(77.22)	-59.44	(67.78)	……0.56	(67.22)	…9.44	(28.33)		"74.20	(80.15)	…68.13	(72.61)	.60.89	(65.66)	"23.78-	(26.51)
ProGNN		X		48.89	(79.44)	32.78	(67.22)	33.89	(51.11)	17.78	(27.22)		45.10	(81.32)	46.58	(71.82)	53.40	(49.84)	24.80	(27.49)
GCN-SVD		X		53.33	(75.56)	28.89	(59.44)	・1.67	(50.56)	25.00	(27.78)		47.82	(76.61)	51.20	(66.90)	55.00	(55.47) I	25.25	(26.63)
GAT					^^13^.89	(84.44)	…8.89	(70.00)	……0.56	(60.56)	…3.89	(30.56)		…41.70	(83.72)	…48.40	(73.40)	…50.37	(61.69)	^^25.^00	(27.30)
GCN				1.67	(82.78)	4.44	(69.44)	0.56	(63.33)	2.22	(33.33)		37.46	(84.32)	45.81	(7427)	,51.82	(64.86)	25.03	(27.30)
MLP*				…64.44	(64.44)	…70.56	(70.56)	57.78	(57.78)	"30.00	(30.00)		"64.55	(64.55)	…67.67	(67.67)	'-56.56	(56.56)	^^26.^25^	(26.25)
I ∙ H2GCN-SVD	■ H2GCN	■ CPGNN	画 FAGCN	△ PrOGNN	⅝ GAT.MLP∣
∙ GraphSAGE-SVD	. GraphSAGE	□ GPRGNN	ɪ GNNGUard	ʌ GCNSVD	畲 GCN
Clean Accuracy	Clean Accuracy
Figure 1: (Best viewed in color.) Classification accu-
racy on clean data and against poison attacks for target
nodes attacked by Nettack. Error bars show standard
deviation across different sets of experiments. Detailed
results are listed in Table 7. As expected, MLP is not
influenced by the adversarial structural attacks.
dressing evasion attacks. In summary, these observations show that the heterophily-inspired design
is orthogonal to existing vaccination mechanisms for improving the robustness of GNNs.
Untargeted attacks by Metattack.
1 Poison attacks.	We also test the
robustness of each method against un-
targeted attacks. Table 3 (right) shows
the performance under poison attacks.
Though our theoretical analysis in §3
focuses on the effect of the heterophilous
design under targeted attacks, we ob-
serve similar improvements in robustness
against untargeted attacks in the poison
setup. GNNs with the identified design
show mostly improved robustness com-
pared to unvaccinated models, while
having similar performance on the clean
datasets. Specifically, CPGNN shows
exceptional robustness, outperforming
the best unvaccinated model by up to
32.85%. Moreover, models combining
the identified design with low-rank approximation show more than 10% improvement accuracy
compared to GCN-SVD, which uses only low-rank approximation, and ProGNN. We note that
GNNGuard, which uses a similarity-based defense strategy, also shows more competitive robustness
under untargeted attacks. This is in line with existing works showing that low-rank based approach
does not adapt well to untargeted attacks (Jin et al., 2020b). Nevertheless, methods combining a
heterophilous design and low-rank approximations still outperform GNNGuard on two datasets.
2 Evasion attacks. We present the performance under evasion attacks and on clean datasets in Ta-
ble 8 in the Appendix. Unlike the poison attacks, the evasion setup only leads to a slight decrease
in average accuracy of less than 2% for most models. Moreover, there appears to be no clearly
increased robustness for vaccinated models (with the identified design or other vaccination mach-
anisms) compared to unvaccinated models. This can be attributed to the reduced effectiveness of
evasion vs. poison attacks (as in Nettack), and the increased challenges of untargeted attacks.
5.3	(Q3) Heterophily-adjusted GNNs are Certifiably More Robust
It is worth noting that robustness against specific attacks such as Nettack and Metattack does
not guarantee robustness towards other possible attacks. To overcome this limitation, robustness
certificates provide guarantees (in some cases probabilistically) that attacks within a certain radius
cannot change a model’s predictions. Complementary to our evaluation on empirical robustness, we
further demonstrate that heterophily-adjusted GNNs featuring our identified design are certifiably
8
Under review as a conference paper at ICLR 2022
Table 4: Accumulated certifications (AC), average certifiable radii (尸。and Fd) and accuracy of
GNNs with randomized smoothing enabled (i.e., f (φ(s))) on the test splits of the clean datasets,
with a ramdomization scheme φ allowing both addition and deletion (i.e., p+ = 0.001, p- = 0.4).
For each statistic, we report the mean and stdev across 3 runs. Best results highlighted in blue per
dataset, and in gray per model group. For results with other randomization schemes, see Table 9.
Addition & Deletion	Addition & Deletion
	3 H	AC	rFa	rd	Acc. %	AC	rFa	rd	Acc. %
H2GCN	X	3.91±0.31	o.46±o.o8	3.88±o.29	79.14±2.oi	2.98±o.88	o.34±o.13	3.29±0.67	71.43±3.92
GraphSAGE	X	2.12±0.07	o.12±o.oo	2.41±o.o4	79.43±i.43	2.25±o.15	o.2o±o.o1	2.59±o.1o	73.34±2.66
CPGNN	X	1.87 ±0.27	O.14±o.o5	2.24±o.3o	75.37±i.65	2	2.O3±o.i7	o.11±o.o1	2.52±o.2o	73.48±o.6i
GPR-GNN	X	E	4.42 ±0.43	O.63±o.o6	4.35±0.22	74.9o±2.34	4	4.63±o.27	O.81±o.o7	4.92±0.24	66.33±o.2o
FAGCN	X	≤	4.3O±o.o7	O.57±o.o2	4.25±o.o4	76.49 ±1.73	4	4.O7±o.i5	O.58±o.o2	4.23±o.o9	71.82±o.73
GAT		1.60±0.10	-O.O7±o.oΓ^	^-1.83±o^oΓ'	…79：8812.49…	1.3o±o.o6	^ O.Q8±o.o2^"	1.62±o.o6	…72—
GCN		1.73±o.o9	O.O9±o.oi	1.99±o.o3	79.39±3.72	1.77±o.o8	O.14±o.o2	2.O9±o.o7	73.48±o5
H2GCN	X	∣8.12±o.io	1.76±o.o2	8.14±o.o6	57.38±o.17	1.44±o.18	o.59±o.1o	3.79±o.4o	26.97±o.1o
GraphSAGE	X	6.98±o.o6	1.50±0.04	7.32±o.i3	56.72±1.56	O.7O±o.2i	O.19±o.ii	2.16±o.54	26.84±o.47
CPGNN	X	6.80±0.19	1.41±o.21	7.o5±o.7o	59.00±5.71	1.45±0.23	O.6l±o.14	3.89±o.5i	26.71±o.25
GPR-GNN	X	5.81±o.16	1.11±o.o2	5.95±o.1o	61.99±o.44	敬 O.52±o.o6	O.11±o.oi	1.7O±o.i4	26.31±i.o3
FAGCN	X	7	7.45±o.2i	1.53±o.o2	7.4O±o.o6	59.76±i.47	1.41±o.1o	o.56±o.o6	3.81±0.22	27.O7±o.i6
GAT		∣4.3O±o^6^^	-O.77±o.o4^^	^-4.72±o^i9-"	61.56±o.78	^^Q.28±o.o9^""	^ O.O4±o.oΓ	…O：95±o.31	…27.121o.52…
GCN		3.93±o.o9	O.64±o.o2	4.24±o.io	65.54±o.43	O.33±o.o6	O.O3±o.oo	1.17±o.25	26.79±o3
more robust than methods without it, thus answering (Q3). For GNN models, we include H2GCN,
GraphSAGE, CPGNN, GPR-GNN, FAGCN, GAT and GCN in this analysis. We exclude other
models that either learn to rewrite the graph structure through the training process or require a
recalculation of the low-rank approximation for every randomized perturbation as sampling on these
models is infeasible. We use the same hyperparameters as in the benchmark study in §5.2.
Table 4 shows multiple metrics of certifiable robustness of each GNN model under a randomiza-
tion scheme allowing for both addition and deletion of edges; we additionally report results under
randomization schemes allowing only addition or deletion in Table 9. For the scheme allowing
both addition and deletion, we observe that all heterophily-adjusted methods have better certifiable
robustness compared to methods without the design. Specifically, on homophilous datasets (Cora
and Citeseer), methods with the identified design achieve an up to 1.6 times relative improvement
in accumulated certification. On heterophilous datasets (FB100 and Snap), they outperform the
baselines by a factor of 4.4. In the more challenging case with the addition only scheme, methods
with the design also show up to 1.2 times relative increase in AC on the homophilous datasets and
5.0 times relative increase in AC on the heterophilous datasets compared to the baselines. For the
deletion only scheme, unvaccinated models like GCN already have decent certifiable robustness in
this scenario. This is commensurate with our discussions in §5.1 that deletions create less severe
perturbations. Overall, our results show that models featuring our identified design achieve signif-
icantly improved certifiable robustness compared to models lacking this design. However, like in
our empirical robustness evaluation, architectural differences lead to some variability of robustness;
the results also show tradeoffs between accuracy and robustness. We also observe that the robust-
ness rankings for methods under certifiable and empirical robustness are different, as in the previous
results from Geisler et al. (2020); we discuss possible reasons in App. E.3.
6 Conclusion
We formalized the relation between heterophily and adversarial structural attacks, and showed the-
oretically and empirically that effective attacks gravitate towards increasing heterophily in both
homophilous and heterophilous graphs by leveraging low-degree (gambit) nodes. Using these in-
sights, we showed that a key design addressing heterophily, namely separate aggregators for ego- and
neighbor-embeddings, can lead to competitive improvement on empirical and certifiable robustness,
with only small influence on clean performance. Finally, we compared the design with state-of-the-
art vaccination mechanisms under different attack scenarios for various datasets, and illustrated that
they are complementary and that their combination can lead to more robust GNN models. We note
that while we focus on the structural attacks, GNNs are also vulnerable to other types of attacks such
as feature perturbations. In addition, the graph-agnostic MLP, which is immune to structural attacks,
on some datasets outperforms all GNNs against attacks; this calls for further works in understanding
of the nature of the attacks and also for effective defense strategies upon our discoveries.
9
Under review as a conference paper at ICLR 2022
Reproducibility S tatement
For reproducibility, we describe the experimental setups in §5 with additional details in App. D.2,
including the implementation and the hyperparameter settings used for each method, and the hard-
ware specifications. We provide anonymized code and datasets in the supplementary material with
instructions for replicating the experiments, and will make them publicly available upon acceptance.
References
Sami Abu-El-Haija, Bryan Perozzi, Amol Kapoor, Hrayr Harutyunyan, Nazanin Alipourfard,
Kristina Lerman, Greg Ver Steeg, and Aram Galstyan. Mixhop: Higher-order graph convolu-
tion architectures via sparsified neighborhood mixing. In International Conference on Machine
Learning (ICML), 2019.
Deyu Bo, Xiao Wang, Chuan Shi, and Huawei Shen. Beyond low-frequency information in graph
convolutional networks. arXiv preprint arXiv:2101.00797, 2021.
Aleksandar Bojchevski and StePhan Gunnemann. Adversarial attacks on node embeddings via graph
poisoning. In Proceedings of the 36th International Conference on Machine Learning, ICML,
Proceedings of Machine Learning Research. PMLR, 2019a.
Aleksandar BojchevSki and Stephan Gunnemann. Certifiable robustness to graph perturbations. In
Neural Information Processing Systems, NeurIPS, 2019b.
Aleksandar Bojchevski, Johannes Klicpera, and Stephan Gunnemann. Efficient robustness certifi-
cates for discrete data: Sparsity-aware randomized smoothing for graphs, images and more. In
International Conference on Machine Learning, pp. 1003-1013. PMLR, 2020.
Michael M Bronstein, Joan Bruna, Yann LeCun, Arthur Szlam, and Pierre Vandergheynst. Geomet-
ric deep learning: going beyond euclidean data. IEEE Signal Processing Magazine, 34(4):18-42,
2017.
Heng Chang, Yu Rong, Tingyang Xu, Wenbing Huang, Honglei Zhang, Peng Cui, Wenwu Zhu, and
Junzhou Huang. A restricted black-box adversarial framework towards attacking graph embed-
ding models. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 34, pp.
3389-3396, 2020.
Eli Chien, Jianhao Peng, Pan Li, and Olgica Milenkovic. Adaptive universal generalized pagerank
graph neural network. In International Conference on Learning Representations, 2021. URL
https://openreview.net/forum?id=n6jl7fLxrP.
Jeremy Cohen, Elan Rosenfeld, and Zico Kolter. Certified adversarial robustness via randomized
smoothing. In International Conference on Machine Learning, pp. 1310-1320. PMLR, 2019.
Hanjun Dai, Hui Li, Tian Tian, Xin Huang, Lin Wang, Jun Zhu, and Le Song. Adversarial attack on
graph structured data. In International conference on machine learning, pp. 1115-1124. PMLR,
2018.
Yushun Dong, Kaize Ding, Brian Jalaian, Shuiwang Ji, and Jundong Li. Graph neural networks with
adaptive frequency response filter. arXiv preprint arXiv:2104.12840, 2021.
Negin Entezari, Saba A Al-Sayouri, Amirali Darvishzadeh, and Evangelos E Papalexakis. All you
need is low (rank) defending against adversarial attacks on graphs. In Proceedings of the 13th
International Conference on Web Search and Data Mining, pp. 169-177, 2020.
Simon Geisler, Daniel Zugner, and Stephan Gunnemann. Reliable graph neural networks via robust
aggregation. Advances in Neural Information Processing Systems, 33, 2020.
Leo A. Goodman. Snowball Sampling. The Annals of Mathematical Statistics, 32(1):148 -
170, 1961. doi: 10.1214/aoms/1177705148. URL https://doi.org/10.1214/aoms/
1177705148.
10
Under review as a conference paper at ICLR 2022
Will Hamilton, Zhitao Ying, and Jure Leskovec. Inductive representation learning on large graphs.
In Advances in neural information processing Systems (NeurIPS), pp.1024-1034, 2017.
Wei Jin, Yaxin Li, Han Xu, Yiqi Wang, and Jiliang Tang. Adversarial attacks and defenses on
graphs: A review and empirical study. arXiv preprint arXiv:2003.00653, 2020a.
Wei Jin, Yao Ma, Xiaorui Liu, Xianfeng Tang, Suhang Wang, and Jiliang Tang. Graph structure
learning for robust graph neural networks. In Proceedings of the 26th ACM SIGKDD International
Conference on Knowledge Discovery & Data Mining, pp. 66-74, 2020b.
Thomas N. Kipf and Max Welling. Semi-supervised classification with graph convolutional net-
works. In International Conference on Learning Representations (ICLR), 2017.
Guang-He Lee, Yang Yuan, Shiyu Chang, and Tommi Jaakkola. Tight certificates of adversarial ro-
bustness for randomly smoothed classifiers. Advances in Neural Information Processing Systems,
32:4910-4921, 2019.
Jure Leskovec and Andrej Krevl. SNAP Datasets: Stanford large network dataset collection. http:
//snap.stanford.edu/data, June 2014.
Jure Leskovec, Jon Kleinberg, and Christos Faloutsos. Graphs over time: Densification laws, shrink-
ing diameters and possible explanations. 08 2005. doi: 10.1145/1081870.1081893.
Jia Li, Honglei Zhang, Zhichao Han, Yu Rong, Hong Cheng, and Junzhou Huang. Adversarial attack
on community detection by hiding individuals. In Proceedings of The Web Conference 2020, pp.
917-927, 2020a.
Sean Li, Dongwoo Kim, and Qing Wang. Beyond low-pass filters: Adaptive feature propagation on
graphs. arXiv preprint arXiv:2103.14187, 2021.
Yaxin Li, Wei Jin, Han Xu, and Jiliang Tang. Deeprobust: A pytorch library for adversarial attacks
and defenses, 2020b.
Derek Lim, Xiuyu Li, Felix Hohne, and Ser-Nam Lim. New benchmarks for learning on non-
homophilous graphs. arXiv preprint arXiv:2104.01404, 2021.
Meng Liu, Zhengyang Wang, and Shuiwang Ji. Non-local graph neural networks. arXiv preprint
arXiv:2005.14612, 2020.
Jiaqi Ma, Shuangrui Ding, and Qiaozhu Mei. Towards more practical adversarial attacks on graph
neural networks. Advances in neural information processing systems, 2020.
Yao Ma, Xiaorui Liu, Neil Shah, and Jiliang Tang. Is homophily a necessity for graph neural
networks? arXiv preprint arXiv:2106.06134, 2021.
A.K. McCallum. Automating the construction of internet portals with machine learning. Information
Retrieval, 3:127-163, 01 2000.
Leto Peel. Graph-based semi-supervised learning for relational networks. In Proceedings of the
2017 SIAM International Conference on Data Mining, pp. 435-443. SIAM, 2017.
Hongbin Pei, Bingzhe Wei, Kevin Chen-Chuan Chang, Yu Lei, and Bo Yang. Geom-gcn: Geometric
graph convolutional networks. In International Conference on Learning Representations (ICLR),
2020. URL https://openreview.net/forum?id=S1e2agrFvS.
Prithviraj Sen, Galileo Namata, Mustafa Bilgic, Lise Getoor, Brian Galligher, and Tina Eliassi-Rad.
Collective classification in network data. AI magazine, 29(3):93-93, 2008.
Lichao Sun, Yingtong Dou, Carl Yang, Ji Wang, Philip S Yu, Lifang He, and Bo Li. Adversarial
attack and defense on graph data: A survey. arXiv preprint arXiv:1812.10528, 2020.
Tsubasa Takahashi. Indirect adversarial attacks via poisoning neighbors for graph convolutional
networks. In 2019 IEEE International Conference on Big Data (Big Data), pp. 1395-1400. IEEE,
2019.
11
Under review as a conference paper at ICLR 2022
Amanda L. Traud, Peter J. Mucha, and Mason A. Porter. Social structure of facebook net-
works. Physica A: Statistical Mechanics and its Applications, 391(16):4165-4180, 2012.
ISSN 0378-4371. doi: https://doi.org/10.1016/j.physa.2011.12.021. URL https://www.
sciencedirect.com/science/article/pii/S0378437111009186.
Petar VeliCkovic, GUillem CUcUrUlL Arantxa Casanova, Adriana Romero, Pietro Lio, and Yoshua
Bengio. Graph Attention Networks. International Conference on Learning Representations
(ICLR), 2018. URL https://openreview.net/forum?id=rJXMpikCZ.
HUijUn WU, Chen Wang, YUriy Tyshetskiy, Andrew Docherty, Kai LU, and Liming ZhU. Adver-
sarial examples for graph data: Deep insights into attack and defense. In Proceedings of the
Twenty-Eighth International Joint Conference on Artificial Intelligence, IJCAI-19, pp. 4816-
4823. International Joint Conferences on Artificial Intelligence Organization, 7 2019. doi:
10.24963/ijcai.2019/669. URL https://doi.org/10.24963/ijcai.2019/669.
Kaidi XU, Hongge Chen, Sijia LiU, Pin-YU Chen, TsUi-Wei Weng, Mingyi Hong, and XUe Lin.
Topology attack and defense for graph neUral networks: An optimization perspective. arXiv
preprint arXiv:1906.04214, 2019.
KeyUlU XU, Chengtao Li, Yonglong Tian, Tomohiro Sonobe, Ken-ichi Kawarabayashi, and Stefanie
Jegelka. Representation learning on graphs with jUmping knowledge networks. In Proceedings
of the 35th International Conference on Machine Learning, ICML, volUme 80, pp. 5449-5458.
PMLR, 2018.
YUjUn Yan, Milad Hashemi, Kevin Swersky, Yaoqing Yang, and Danai KoUtra. Two sides of the same
coin: Heterophily and oversmoothing in graph convolUtional neUral networks. arXiv preprint
arXiv:2102.06462, 2021.
Xiang Zhang and Marinka Zitnik. GnngUard: Defending graph neUral networks against adversarial
attacks. Advances in Neural Information Processing Systems, 33, 2020.
DingyUan ZhU, Ziwei Zhang, Peng CUi, and WenwU ZhU. RobUst graph convolUtional networks
against adversarial attacks. In Proceedings of the 25th ACM SIGKDD International Conference
on Knowledge Discovery & Data Mining, pp. 1399-1407, 2019.
Jiong ZhU, YUjUn Yan, Lingxiao Zhao, Mark Heimann, Leman AkoglU, and Danai KoUtra. Beyond
homophily in graph neUral networks: CUrrent limitations and effective designs. Advances in
Neural Information Processing Systems, 33, 2020.
Jiong ZhU, Ryan A Rossi, AnUp Rao, TUng Mai, Nedim Lipka, Nesreen K Ahmed, and Danai
KoUtra. Graph neUral networks with heterophily. In Proceedings of the AAAI Conference on
Artificial Intelligence, volUme 35, pp. 11168-11176, 2021.
Daniel ZUgner and StePhan GUnnemann. Adversarial attacks on graph neural networks via meta
learning. In International Conference on Learning Representations, 2019a.
Daniel ZUgner and Stephan GUnnemann. Certifiable robUstness and robUst training for graph con-
volUtional networks. In Proceedings of the 25th ACM SIGKDD International Conference on
Knowledge Discovery & Data Mining, pp. 246-256, 2019b.
Daniel ZUgner and Stephan GUnnemann. Certifiable robUstness of graph ConvolUtional networks
Under strUctUre pertUrbations. In Proceedings of the 26th ACM SIGKDD International Conference
on Knowledge Discovery & Data Mining, pp. 1656-1665, 2020.
Daniel ZUgner, Amir Akbarnejad, and Stephan GUnnemann. Adversarial attacks on neUral networks
for graph data. In Proceedings of the 24th ACM SIGKDD International Conference on Knowledge
Discovery & Data Mining, pp. 2847-2856, 2018.
12
Under review as a conference paper at ICLR 2022
A Societal Impacts
A large number of popular GNN models are inherently based on homophily. Our work shows
that such models may be less robust to adversarial perturbations, and thus when employed for
decision-making these models may lead to undesirable, erroneous or biased results. For exam-
ple, an inherently homophilous GNN model may lead to the so-called “filter bubble” phenomenon
in a recommendation system in which existing beliefs or preferences are reinforced. Similarly, as
homophily-based GNNs typically average over node neighborhoods, this may result in less visibility
of minority groups in the network, thus reinforcing disparities. Heterophilous network design may
improve some of these aspects, and also benefit from some additional robustness as shown in this
work.
However, it should be noted that heterophilous GNNs on their own cannot fully solve the aforemen-
tioned issues. In particular, while heterophilous design can improve robustness, it is not (in general)
constructed to ensure other important aspects such as fairness. Better understanding of GNNs and
tailored auditing tools are necessary in order to deploy these learning algorithms in the context of
decision-making affecting humans.
B Nomenclature
We summarize the main symbols used in this work and their definitions below:
Table 5: Major symbols and definitions.
Symbols	Definitions
G=(V,E,X)	graph G with nodeset V, edgeset E , and |V | × F node feature matrix X
X	|V| × F node feature matrix of G, X ∈ RlVl×F
A	|V| × |V| adjacency matrix of G, A ∈ {0,1}lVl×lVl
As	adjacency matrix with self-loops added, As = A + I
D	diagonal matrix of degrees, with Dii = j Aij
A	row-stochastic matrix for A, A = D-1A
As	row-stochastic matrix for As, As = D-IAs
- A	low-rank approximation of the adjacency matrix A
W	learnable weight matrix for GNN models
xv	F -dimensional feature vector for node v
N (V)	direct (1-hop) neighbors of node v in G without self-loops (i.e., excluding v)
Y	set of class labels
yv	class label for node v ∈ V
y	|V |-dimensional vector of class labels (for all the nodes)
TV = {(vι,yι), (v2,y2),.	..} training data for semi-supervised node classification
…G0=(V, E 0,X)	graph G0 with modified edgeset E0
f	a certain GNN model that processes G
K	the number of layers of GNN f
(k) rv	node representations learned by GNN f at round / layer k
AGGR	function that aggregates node feature representations within a neighborhood
ENC	learnable (nonlinear) mapping that generates latent representation
zv	label prediction by GNN f for node v ∈ V, zv ∈ [0, 1]|Y|
Latk	attack loss that quantifies the mismatch between z and the true labels y
CM Latk	attack loss defined with negative classification margin
	LCtM = -δc = -(zv,yv - maxy=yv zv,y)
d	the degree of the nodes which have a distance less than 2 to any v ∈ DV
da	the degree of a gambit node leveraged by an attack
p	parameter regulating the signal strength of one-hot class label vs. uniform noise
α	a predefined weight scalar in Theorem 3, α ∈ [0, 1]
h	homophily ratio defined on node class labels, h = |{(u, v) ∈ E|yu = yv}|/|E |
13
Under review as a conference paper at ICLR 2022
C	Proofs and Discussions of Theorems
C.1 Detailed Analysis of Theorem 1
Proof 1 (for Thm. 1) We give the proof in three parts: first, we analyze the training process of the
GNN f(2)(A, X) = A2XW on clean data and analytically derive the optimal weight matrix W*
in a stylized learning setup; then, we construct a targeted evasion attack and calculate the attack
loss for a unit structural attack; last, we summarize and validate the statements in the theorem.
Stylized learning on clean data. Given the 2-layer linearized GNN f(2)(A, X) = A2XW and the
training set TV ⊆ DV, the goal of the training process is to optimize the weight matrix W to min-
imize the cross-entropy loss function L([z]TV,:, [Y]TV,:), where predictions [z]TV,: =[A2X]tv ,：W
correspond to the predicted class label distributions for each node v in the training set TV, and
[Y]TV,: is the one-hot encoding of class labels provided in the training set.
Without loss of generality, we reorder TV accordingly such that the one-hot encoding of labels for
nodes in the training set [Y]TV,: is in increasing order of the class label yv:
	1 .	0 .	0 ∙ ..	・ 0 - .	
	. . 1	. . 0	.. . 0 ∙	. ・ 0	
	0 .	1 .	0 ∙ ..	・ 0 .	
[Y]TV,: =	. . 0	. . 1	.. . 0 ∙	. ・ 0	⑵
00
0
1
00
0
1
ITvl×∣Y∣
Now we look at the term [A2X]τv,： in [z]τv,： = [A2X]τv,： W, which are thefeature vectors aggre-
gated by the two GNN layers for nodes v in the training set TV. As stated in the theorem, we assume
TV ⊆ DV, where node u ∈ DV have degree d; proportion h of their neighbors belong to the same
class, while proportion ∣γ-hι ofthem belong to any other class uniformly, andfor each node V ∈ V
the node features are given as Xv = P ∙ onehot(yv) + I-P ∙ 1 for each node V ∈ V. Then, after the
first layer, we have: ^ (hd +1)P	∣⅛⅛dp	I⅜hτdp	…	I⅜hrdp	一 .	.	... .	.	..	. .	.	.	.. (hd +1)p	∣⅛⅛ dp	∣⅜hT dp	…	∣⅜hΓ dp	
∣⅜hr dp	(hd +1)p ∣⅜hτ dp …	∣⅜hr dp .	.	..	. .	.	..	. .	.	.	.. AsX]Tv,: = d+1	∣Y⅛dp	(hd +1)p ∣⅜hτdp …	∣⅛⅛dp	1 - P + TYr
.	.	... .	.	..	. .	.	.	..	
∣⅜hΓ dp	∣⅛⅛ dp	∣⅜hT dp … (hd +1)p .	.	.	.	. .	.	..	. .	.	.	.. ∣⅜hΓ dp	∣⅛⅛ dp	∣⅜hT dp … (hd +1)p	∣TV∣×∣Y∣ (3)
14
Under review as a conference paper at ICLR 2022
and after the second layer
[A 2 * *X]TV,: = (d +l)2∣γ∣(∣γ∣- 1)
一 Si .	Ti .	Ti ∙ ..	•	Ti .
. . Si	. . Ti	.. . Ti	∙	. .. •	Ti
Ti .	Si .	Ti ∙ ..	•	Ti .
. . Ti .	. . Si .	.. . Ti	∙ ..	. .. •	Ti .
. . Ti .	. . Ti .	.. . Ti	∙ ..	. .. •	Si .
. . Ti	. . Ti	.. . Ti	∙	. .. • • Si
1
+------
+ IYI,
(4)
|TV i×iyi
where S1 = ((hIY I - 1)d + IYI - 1)2 p, and T1
((h∣Y∣-1)d+∣Y∣-1)2p
IYR
For [Y]tv,： and [A2X]τv ,： which we derived in Eq. (2) and (4), we can find the optimal weight
matrix W* such that [A 2X]τv ：W* = [Y]tv :, making the Cross-entropy loss L([z]tv :, [Y]tv J =
0.
To find W* , we can proceed as follows. First, sample one node from each class to form a smaller
set TS ⊂ TV. Therefore, we have:
[Y]TS,: =
一 1
0
0
1
0
0
0 一
0
IIYI×IYI
0
0
0
1
IYI×IYI
and
「Si
Ti
Ti
Si
Ti	…Ti ]
Ti	…Ti
,:
(d+ 1)2IYI(IYI - 1)
T1
T1
TI	…SI.
IYI×IYI
1
1
+所
Note that [A2X]τs,: is a specific Circulant matrix, and therefore its inverse exists. Using the
Sherman-Morrison formula, we can find its inverse as:
([A2X]τ )-1 =_______(d + I)2(|Y| - I)2__
Qs ]Ts"	= p(d(h∣Y∣ — 1) + IYI- 1)2 |Y|
|Y|-1
-1
-1
|Y|-1
-1
-1
...	一1
♦…	一1	ι
+--
....十 IYI
…|Y| — 1
(5)
Now, let W* = ([A2X]τs,：)-i, then we have [z]ts,: = [A2X]τs,:W* = [Y]ts,: = I∣γ∣×∣γ∣. Itis
also easy to verify that [z]tv,: = [A2X]τv,: W* = [Y]tv,:. Since W* satisfies L([z]tv,:, [Y]tv,:)=
0, we know W* = ([A2X]τs,:) 1 is the optimal weight matrix that we can learn under TV.
Attack loss under evasion attacks. Now consider an arbitrary target node v ∈ DV with class label
yv ∈ Y, and a unit structural perturbation leveraging gambit node u ∈ V with degree da that af-
(2)
fects the predictions zv of node v made by GNN fs . Without loss of generality, we assume node
(2)
v has yv = 1. As fs contains 2 GNN layers with each layer aggregating feature vectors within
neighborhood N(v) of each node v, the perturbation must take place in the direct (1-hop) neighbor-
hood N(v) or 2-hop neighborhood N2 (v) to affect the predictions zv. For the unit perturbation, the
15
Under review as a conference paper at ICLR 2022
attacker can add or remove a homophilous edge or path between nodes U and V, which we denote
as δι (δι = 1 for addition and δι = —1 for removal); alternatively, the attacker can add or remove
a heterophilous edge or path between nodes U and V, which we denote as δ2 = ±1 analogously. We
denote the perturbed graph adjacency matrix as AS, and Zv = [AS2X]%： W*
①	Unit perturbation in direct neighborhood N (v). Wefirst consider a unit perturbation in the direct
(1-hop) neighborhood N (v) ofnode V. For simplicity ofderivation, we assume that the perturbation
does not change the row-stochastic normalization of AS, and only affects the aggregated feature
vectors of the target node v.
In the case of δι = ±1 and δ2 = 0, we have
[ASX]v,： -	[Asχ]v,: =	do^+1 [	δι	(⅛f	+ P)	δι	(⅜p)	∙∙∙ δι	(⅛p)]
and
[AS2χ[v,: — [A2X]v,： = (a0 + 1)2% +1)∣γ∣ [ S2 Wl ^^^ ɪ ],
where S2 = δι (da(p(d(h∖Y∖ — 1) + h|Y| + |Y| — 2) + d + 2) + (d + 2)(|Y| — 1)p + d + 2) and
T2 = δ1 (— (dα (P(d(h|Y| - I) + h|Y| + |Y| - 2) + ( —d — 2)(IYI — I)) + (d + 2)(IYI — I)(P — I))).
By Multiplying [AS2X]v,: by W*, we can get the predictions ZV after perturbations; we omit the
analytical expression of ZV here due to its complexity.
On the perturbed graph, the CM-type attack loss is calculated as
LCtM (zV ) = — (zV,yv
—max z0 7,)
y=y v,y
Since on clean data, LCMI (ZV) = —1, the change in attack loss before and after attack is
∆LCM = LCtM (zV) -LCM (Zv ) = LCtM (zV) + 1
(d + * 1)δ1(3 - I) (da(d(h|Y| - I) + h|Y| + IYI - 2) + (d + 2)(|YI — I))
(da + 1)2(d(h|Y| — 1) + IYI — 1)2
Solvingfollowing system of inequalitiesfor δι,
∆LCM
∆Latk
> 0
h ∈ [0,1]
IYI≥ 2
d,dα, IYI ∈ Z+
(6)
(8)
we get the valid range of δι as
δι < 0,	when	0 <	d ≤	|Y|	- 2
δ1 < 0,	when	d>	|Y|	-	2	and d0	<	("+:)了+-I)
1 δι < 0	when	d >	IYI	-	2	and d	≥	(d+2)(»T)	and 1	≥	h >	da(dTY∣+2)-(d+2)(IY-1)	.
c,1 < u,	wiien	> >	|Y |	a	ana Ua	≥ d_∣γ∣ + 2	ana	≥	ι> >	(d+1)∣Y∣d
δ1 > 0,	when	d>	|Y|	-	2	and d0	≥(d+-%Y-I)	and 0	≤	h<	MdTY"谭 J(IYIT)
d— 1	a	(9)
Note that the above solution is not applicable when h = "-^]1, in which case d(h|Y| — 1) + |Y| —
1 = 0 and the solution of optimal weight matrix W* = ([A 2X]τ⅛ ,：) 1 is undefined.
In the case of δι = 0 and δ2 = ±1, we have
[ASX]V,: - [ASX]v,:	=	da + 1 [	δ2	(ITf)	δ2	(ITf	+ p)	∙∙∙	δ2	(ITf	+ p)]
and
AS2X]V,: TA2X]V,: = (da + 1)2；d +1)IYI [ rn-ɪ T4 ∙ ∙ ∙ T4 ],
where S4 = δ2 (- (da(p(d(h∣Y∣- 1)+ h∣Y∣ + ∣Y∣ - 2) + (—d — 2)(IYl- 1)) + (d + 2)(IYl- l)(p - 1)))
and T4 = δ2 (da(p(d(h|Y| - 1) + h|Y| + |Y| — 2) + d + 2) + (d + 2)(|Y| - 1)p + d + 2). By
16
Under review as a conference paper at ICLR 2022
multiplying [AS2X]v,: with W*, we can get the predictions Zv after the perturbations. Following a
similar derivation to that in the previous case, we can compute the Change in the CM-type attack
loss before and after attack as
δlcm = (d +1)(∣Y∣ - i)(da(d(h∣Y∣ - 1) + h|Y| + |Y| - 2) + (d + 2)(∣Y∣ - 1)) δ2
atk =	(da + 1) 2(d(h∣y∣- 1) + ∣Y∣- 1)2
(10)
Solving the same system ofinequalities as Eq. (8) for δ2, we obtain the valid range of δ2 as
δ2 > 0, when 0 < d ≤ ∣Y∣ - 2
δ2	> 0, when d> ∣Y∣ - 2 and d0	<	(d+-)YY-I)
1 δo	> 0 when d > ∣Y∣ - 2 and d	〉(d+2)(»T)	and 1〉h > da(dTY+2)-(d+2)(Y-I)	.
u2 > u, wιιen > > ∣γ ∣ a ana “a ≥ d-∣γ∣ + 2	ana ɪ ≥ ι> >	(d ∣1)|Y|d
δ2 < 0, when d> ∣Y∣ - 2 and d0 ≥ 叱-斗Y-I) and 0 ≤ h< MdTY"谭 J(IYIT)
d― 1	a	(11)
Note that the above solution is not applicable when h = '旷1, in which case d(h∣Y∣ -1) + ∣Y∣ -
1=0 and the solution of optimal weight matrix W* = ([A 2X]τs ,：) 1 is undefined. We also note
that we always have d。-%-雅？(IYT)-由=-(IY⅞⅛d+2) < ° when ∣Y∣ ≥ 2.
②	Unit perturbation in 2-hop neighborhood N2(v). We now consider a unit perturbation in the
2-hop neighborhood N(V) ofnode v. In this case we will have [A；X]v,： = [AsX]v,：.
In the case of δι = ±1 and δ2 = 0, we have
As2X]v，： -A2X]v，： = (da+；)2|Y| [ S5 wɪ
_TL 1
∣Y∣-1 J
where
S5 = (da(p(h∣Y∣ - 1) + 1) + (∣Y∣ - 1)p + 1) δι
and
T5 = (da(-h∣Y∣p + ∣Y∣ + p - 1) + ∣Y∣(-p) + ∣Y∣ + p - 1) δι.
By multiplying [As2X]v,： with W*, we can get the predictions Zv after perturbations. Following
a similar derivation as before, we can get the change in the CM-type attack loss before and after
attack as
∆L0M = (d + 1)2(∣Y∣ - 1)(da(h∣Y∣ - 1) + ∣Y∣ - I) δ1
atk =	(da + 1)2 (d(h∣Y∣- 1) + ∣Y∣- 1)2
(12)
Solving the same SyStem ofinequalities as Eq. (8) for δ1, we get the valid range of δ1 as
δι < 0, when da < ∣Y∣ - 1
δι < 0, when da ≥ ∣Y∣ - 1 and da Tj Y 了 < h ≤ 1	.	(13)
δ1 > 0, when da > ∣Y∣ - 1 and 0 ≤ h < da -j Y 1+1
Note that the above solution is not applicable when h = d-YY+ɪ, in which case d(h∣Y∣ -1) + ∣Y∣ -
1 = 0 and the solution of optimal weight matrix W* = ([A 2X]τs ,：) ɪ is undefined.
For the case δ1 = 0 and δ2 = ±1, we have
[As2X]v，： -A2X]v,： = ≡1w∣ [ ” T6 …T6 ]
where
S6 = δ2 (da(-h∣Y∣p + ∣Y∣ + P - 1) + ∣Y∣(-P) + ∣Y∣ + P - 1)
and
T = δ2 (da(p(h∣Y∣ - 1) + 1) + (∣Y∣ - 1)p + 1).
Multiplying [A s2X]v,： with W*, we can get the predictions zv after perturbations. As before we can
compute the change in the CM-type attack loss before and after attack as
∆LCM = (d + 1)2(∣Y∣ - 1) (da(h∣Y∣ - 1) + ∣Y∣ - I) δ2
atk =	(da + 1)2(d(h∣Y∣-1) + ∣Y∣-1)2
(14)
17
Under review as a conference paper at ICLR 2022
Finally, solving the same system of inequalities as Eq. (8) for δ2, we get the valid range of δ2 as
δ2 > 0, when da < |Y | - 1
δ2 > 0, when da ≥ |Y| - 1 and da∣YYl + 1 < h ≤ 1
δ2 < 0, when da > |Y| - 1 and 0 ≤ h < da-YYl+1
(15)
Note that the above solution is not applicable when h = d]"1, in which case d(h|Y| - 1) + |Y| 一
1=0 and the solution of optimal weight matrix W* = ([A 2X]τs ,：) 1 is undefined. We also note
that We always have da-YYl+1 一 击 =1-∣lYI < 0 when |Y| ≥ 2.
Summary and validation of theorem statements. Based on our derivations, we summarize and
validate our statements in the theorem next.
1 The attack losses Latk (CM-type, §2) increase only by removing a homophilous edge or path, or
adding a heterophilous edge or path to node v. From Eq. (9), (11), (13), (15), we observe that for
both direct attacks in 1-hop neighborhood N(v) and indirect attacks in 2-hop neighborhood N2 (v),
when h ≥ 击,the attack loss increases only if δ1 < 0, which represents removal of a homophilous
edge or path to node v, or if δ2 > 0, which represents addition of a heterophilous edge or path to
node v.
2 Direct perturbations on edges (or 1-hop paths) of the target node v lead to greater increase in
Latk than indirect perturbations on multi-hop paths to target node v.
From Eq. (7) and Eq. (10), the change in the CM-type attack loss ∆LaCtMk (z0v) for direct perturba-
tions on 1-hop neighborhood N(v) of the target node v considering both δ1 and δ2 can be written
as
∆LCM,direct =(d + 1)(∣γ∣ — 1)(da(d(h∣Y∣ — 1) + h|Y| + |Y| - 2) + (d + 2)(∣γ∣ — 1))(δ? — δ1)
atk =	(da + 1) 2(d(h∣γ∣- 1) + |Y|-1)2
(16)
From Eq. (12) and Eq. (14), the change in the CM-type attack loss ∆LaCtMk (z0v) for indirect pertur-
bations on 2-hop neighborhood N2(v) of the target node v considering both δ1 and δ2 is
∆LCM,indirect _ (d +1)2 (IYl-I) (da(h|Y| - 1) + IYl-I) (62 — δ1)	(17)
atk	=	(da + 1) 2(d(h∣Y∣-1) + IYI-1)2	( )
Note that when h ≥ 击,we have h∣Y∣ ≥ 1, and
dh|Y| - d + 2|Y| — 2 _ 1	da(d(h|Y| — 1) + ∣Y∣ — 1) + (d + 1)(|Y| — 1)	^
d(h∣γ∣- 1) + IYI- 1 = +	da(h∣Y∣- 1) + IYI- 1	>
(18)
Therefore we will always have ∆LaCtMk ,direct > ∆LaCtMk ,indirect for an effective unit perturbation that
increases attack loss LCMM (i.e., δ1 = -1 and δ2 = 0, or δ1 = 0 and δ2 = 1) when h ≥ 吉.
C.2 Detailed Analysis of Theorem 2
Proof 2 (for Thm. 2) For a direct unit perturbation in the 1-hop neighborhood N(v) of the target
node v, from Eq. (9) and (11) of Proof 1, we observe that the signs ofδ1 and δ2 which increase the
attack loss are contingent on the degree of the target node d, the degree of the gambit node da and
the homophily ratio h of the graph:
1 if 0 < d ≤ IY I - 2 (i.e., when degree d of the target node is low), regardless of da and h, the
attack loss increases only if δ1 < 0, which represents removal of a homophilous edge to node v, or
if δ2 > 0, which represents addition of a heterophilous edge to v;
2 if d > IY I - 2 (i.e., when degree d of the target node is high), the increase of the attack loss will
be dependent to the degree of the gambit node da and the homophily ratio h of the graph:
18
Under review as a conference paper at ICLR 2022
(a)	when da < (d+-)YY+-1) (i.e., when degree da of the gambit node is low), regardless of h, the
attack loss increases only if δ1 < 0, which represents removal of a homophilous edge to node
v, or if δ2 > 0, which represents addition of a heterophilous edge to v;
(b)	when da ≥ (d+-)YY+-1) (i.e., when degree da of the gambit node is high), for 0 ≤ h <
da(dTY∣++1-Y+2)(IYIT) < _1_, the attack loss increases only if δι < 0, which represents
removal of a homophilous edge to node v, or if δ2 > 0, which represents addition of a het-
erophilous edge to v.
We note that the above conclusions are not applicable when h = d-dYY+1, in which case d(h|Y| 一
1) + |Y| — 1 = 0 and the solution ofoptimal weight matrix W* = ([A2X]ts,：) 1 is undefined.
C.3 Detailed Analysis of Theorem 3
Proof 3 (for Thm. 3) In this proof, we mainly focus on analyzing the increase in Latk for the GNN
layer defined as f (A, X; α) = ((1 — α)A + ɑl) XW. Wefollow a similar process as in Proof 1,
since the layer defined as fs(A, X) = A SXW is a special case of the previous formulation when
α = 1+d.
Layer f (A, X; α) = ((1 — α)A + ɑl) XW. We first derive the optimal weight matrix W* in a
stylized learning setup as in Proof 1. Following a similar process, for this GNN layer we have
LDTT	rr -J
S7 T 7 T 7 ∙∙∙ T 7
_	1 - P	T7 S7 T7 ∙∙∙ T7
[((1 —α)A+ αI) χ]7s ： = --p +	.....
TS,:	|Y|	..	..	..	..	..
-T7 T7 T7 …S"∣Y∣×∣Y∣
where S7 = p(α + h — ɑh), and T7 = (α-Y(h-I)P, and
-10 0
010
[Y]Ts ,：=...
0 一
0
.	=iiyi×iyi
.
000
IYI×IYI
Using the Sherman-Morrison formula, we find its inverse:
([((I - HA + αI) χ]Ts,:)	= p(a(h — 1) YLhIYI + 1)∣γ∣ ∙
- 1 — ∣γ∣	1	1 …	1	-
1	1 — ∣γ∣ 1 …	1	1
+--
.	......+1Y
一 1	1	1 …1 —IYI 一
Assuming W* = ([((1 — a)A + αl) X]无 :)	, we obtain
[z]Ts,: = [((I- a)A + αI) x]ts,: W* = [Y]Ts,: = 1∣Y∣×∣Y∣.
Since W* satisfies L([z]TV,:, [Y]TV,:) = 0, we know W* is the optimal weight matrix that we can
learn under TV.
Now consider an arbitrary target node v ∈ DV with class label yv ∈ Y, and a unit structural
perturbation leveraging gambit node u ∈ V with degree da that affects the predictions zv of node v
made by GNN fs(2). Without loss of generality, we assume node v has yv = 1. Note that we will only
discuss the case of direct structural perturbation to the 1-hop neighborhood N(v) of target node
19
Under review as a conference paper at ICLR 2022
v, as indirect perturbations do not affect the predictions zv for node v produced by a single GNN
layer. Denote ∆A = A0 一 A as the Change in the adjacency matrix A before and after the attack.
Similar to Proof 1, for simplicity of derivation, we assume that the perturbation does not change the
row-stochastic normalization of A, and we use δι to denote addition (δι = 1)orremoval(δι = —1)
of a homophilous edge to node v, and use δ2 to denote addition or removal of a heterophilous edge
to node v.
In the case ofδ1 = ±1 and δ2 = 0, we have
[((1 — α)∆A + αl) XL . = (I: α)δ1 [((IYI- 1)p +1) (1 一 P)…(1 一 P)]
v,:	da|Y|
and the change in the CM-type attack loss LaCtMk before and after the perturbation can be derived as
∆LCM = ((I 一 a)|Y| + α - I)δι
atk = da(α(h — 1) — h)|Y| + da .
(19)
In the case ofδ1 = 0 and δ2 = ±1, we have
[((1 — α)∆A + al) X]v : = (1-.α)δ2 [ (1 — p) (|Y| - 1)p +1 …(|Y| - 1)p +1 ]
,	da |Y |
and the change in the CM-type attack loss LaCtMk before and after the perturbation can be derived as
∆LCM =	(a - I)(IYI - I)δ2	(20)
atk	da(a(h — 1) — h)∣Y∣+ da
From Eq. (19) and Eq. (20), the change in the CM-type attack loss ∆LaCtMk for GNN layer f (A, X; α)
considering both δ1 and δ2 can be written as
∆fCM,f = ((I 一 a)|Y| + a - 1)δ1 +	(a - I)(IYl - 1)δ2	(21)
atk	da(α(h — 1) — h)∣Y∣ + da	da(α(h — 1) — h)|Y| + da
Layer fs (A, X) = AsXW. This formulation is a special case of the previously discussed
f (A, X; a) formulation when a = 1+d-.
In the case of δ1 = ±1 and δ2 = 0, from Eq. (19), we have the change in the CM-type attack loss
LaCtMk before and after the perturbation as
∆L
CM
atk
(IYI - 1)δι
da(h|Y| - 1)+ IYI — 1
(22)
In the case ofδ1 = 0 and δ2 = ±1, from Eq. (20), we have the change in the CM-type attack loss
LaCtMk before and after the perturbation as
∆LCM =	(IYI - 1)δ2
atk = da(hIYI -1) + IYI —1.
(23)
From Eq. (22) and Eq. (23), the change in the CM-type attack loss ∆LaCtMk for GNN layer fs(A, X)
considering both δ1 and δ2 can be written as
∆L°M,fs =_______(IYI - 1)δ1
atk = - da(hIYI- 1) + IYI- 1
,_______(IYI - 1)δ2
da(hIYI - 1)+ IYI- 1
Comparison of increase in attack loss ∆LaCtMk.
Solving the following system of inequalities for variable a
'△LCM,fs > ∆LCM,f > 0
atκ	at
α,h ∈ [0,1]
IYI ≥ 2
da, IYI ∈ Z+
∖δ1,δ2 ∈ {-1, 0, 1}
(24)
(25)
20
Under review as a conference paper at ICLR 2022
we get the valid range of α as
d^+l < α < 1, when 0 ≤ h < 击 and 0 < d0 < ]∣γ∣-1 and δ1 < δ2
0 ≤ α < d"+ι, when 0 ≤ h < 击 and da > h∣γl∣-[ and δι > δ2	.	(26)
d^+l < α < 1, when + ≤ h ≤ 1 and δι < δ2
From the solution in Eq. (26), we observe that when h > 击,a unit perturbation increasing Latk as
discussed in Theorem 1 (i.e. δ1 = -1 and δ2 = 0, or δ1 = 0 and δ2 = 1) will satisfy the condition
δ1 < δ2, and thus lead to a strictly smaller increase ∆LaCtMk ,f in the attack loss for layer f (A, X; α)
than the increase ∆LCMM,fs for layer fs(A, X) if a > d-++ι.	■
21
Under review as a conference paper at ICLR 2022
D Detailed Experimental S etups and Hyperparameters
D. 1 Instantiations of Design on GNNs
We explicitly demonstrate how the heterophilous design outlined in Eq. (1) of §3.2 are instanti-
ated in various GNN models used in our experiments. In particular, we highlight how these GNN
architectures allow separate aggregations of the ego- and neighbor-embeddings.
•	In H2GCN, a final hidden representation is computed for each node v ∈ V through r(vfinal) =
CONCAT(r(v0) , r(v1) , ..., r(vK)), where r(v0) is the non-linear ego-embedding of node features and
r(vk) are the intermediate representations aggregated in the k-th layer, where k ∈ (1, ..., K). By
interpreting the update rule’s CONCAT as the ENC operation, AGGR1 as the skip connection to
the ego-embedding of node features, and the concatenation of the intermediate representations
as AGGR2, the ego- and neighbor-embeddings are separately aggregated and the identified het-
erophilous design is recovered.
•	Similarly, GraphSAGE (with mean aggregator) utilizes a concatenation-based encoding scheme
through their update of
r(vk) = σ CONCAT r(vk-1)
MEAN ({rUk-1),∀u ∈ N(i)})) ∙ W)
where AGGR1(∙) = rUk-1), AGGR2 is the mean function and ENC(X1, x2) = σ(CONCAT(xι, x2)∙
W).
•	GPR-GNN embeds each node feature vector separately with a fully connected layer to compute
r(v0) (or H(v0:)
as in the original paper), similar to H2GCN, and then updates each node’s hidden
representations through a weighted sum of all k-th hop layers around the ego-node, where k ∈
(0,1,..., K). By interpreting the summation as the ENC operation as well as AGGR1(∙) = γoH(0)
and AGGR2(∙) = PK=I YkAkymH(k-1), where Y isa vector denoting the weights associated with
each k-hop ego network, the aggregation of the ego- and neighbor-embeddings can be decoupled
and thus GPR-GNN also satisfies the heterophilous design.
•	FAGCN follows a similar update function to GPR-GNN with
G
h(I)= εhiO) + X 普hjlT)
j∈N(i) Pidj
where hi(0), equivalent to ri(0) in Eq. (1), represents the non-linear ego-embedding and αiGj is a
proportionality constant measuring the ratio of low and high frequency components. The relation-
ship between FAGCN and the proposed heterophilous design can similarly be inferred by inter-
preting the sum as the ENC operation, AGGR1(∙) = εhi0) as a weighted skip connection to the
ego-embedding of feature vectors, and the weighted sum of embeddings within the neighborhood
N (i) of node i ∈ V as AGGR2(∙).
•	CPGNN formulates the update function of belief vectors BB(k) after the k-th propagation layer as
B(k) = B(O) + AB (k-1)H, where BB(O) consists of prior belief vectors for each node (which can
be seen as the ego-embedding ri0) in Eq. (1)), and H is the linearized compatibility matrix. The
heterophilous design can be recovered by letting AGGR1(∙) = BB(O) as a skip connection to the
prior belief, AGGR2(∙) = ABB(k-1)H, and the ENC operation as the summation.
22
Under review as a conference paper at ICLR 2022
D.2 More Details on the Experimental Setup
Benchmark Implementations. Our empirical framework is built on DeepRobust (Li et al., 2020b),
Python Fire1 and signac2. We incorporated the following implementations of GNN models in our
framework. For GNNGuard and GCN-SVD, there are some implementation ambiguities, which we
discuss in the next paragraph.
H2GCN (Zhu et al., 2020) GraphSAGE (Hamilton et al., 2017) CPGNN (ZhU et al., 2021)	https://github.com/GemsLab/H2GCN Impiemented on top of https://github.com/GemsLab/H2GCN https://github.com/GemsLab/CPGNN
GNNGuard (Zhang & Zitnik, 2020)	https://github.com/mims-harvard/GNNGuard
ProGNN (Jin et al., 2020b)	https://github.com/ChandlerBang/Pro-GNN
GCN-SvD (EnteZari et al., 2020)	https://github.com/DSE-MSU/DeepRobust/blob/master/examples/graph/test_gcn_svd.py
GAT(Veii痴Vcetal.12018)……	https://github.com/DSE-MSU/DeepRobust/blob/master/deeprobust/graph/defense/gat.py
GCN (Kipf & Welling, 2017)	https://github.com/DSE-MSU/DeepRobust/blob/master/deeprobust/graph/defense/gcn.py
Notes on the GNNGuard and GCN-SVD Implementations. We note that there are ambiguities in
the implementations of GNNGuard (Zhang & Zitnik, 2020) and GCN-SVD (Entezari et al., 2020),
which can lead to different variants with different performance and robustness, as we show in Ta-
ble 6.
Table 6: Performance comparison between variants of GNNGuard and GCN-SVD: mean accuracy
± stdev over multiple sets of experiments.
	Homophilous graphs		Heterophilous graphs		Homophilous graphs		Heterophilous graphs	
	Cora	Citeseer	FB100	Snap	Cora	Citeseer	FB100	Snap
	h=0.804	h=0.736	h=0.531	h=0.134	h=0.804	h=0.736	h=0.531	h=0.134
		Clean Datasets				Clean Datasets		
GNNGuard (I)	75.56±5.15	70.00±6.24	68.89±2.08	31.67±0.00	79.58±0.97	71.68±1.10	65.31±1.48	26.37±0.70
GNNGuard (II)	77.22±6.29	67.78±4.78	67.22±2.08	28.33±3.60	80.15±0.55	72.61±0.28	65.66±0.60	26.51±0.98
GCN-SVD(I) k = 5
GCN-SVD (II) k = 5
GCN-SVD (I ) k =10
GCN-SVD (II) k = 10
GCN-SVD (I ) k = 15
GCN-SVD (II) k = 15
66.67±8.16	63.89±5.50	51.67±6.24	29.44±0.79
52.78±5.50	35.00±1.36	50.56±4.37	25.00±5.93
66.11 ±6.71	65.00±3.60	52.78±4.16…	30.56±2.08
66.11±4.78	45.00±3.60	51.11±2.08	22.22±4.78
…72：7816.98 …	63.89 ±7.74	.…-57.78±2.8/	28.89±2.08"
69.44±2.08	46.67±6.24	52.78±5.15	21.67±1.36
ψ>sasn
69.43±0.99	68.31±0.34	52.95±0.13	27.66±0.05
55.05±1.77	41.47±0.72	52.40±0.18	25.84±0.07
^"71.08±0.46^^	^^69.19±Y.1Γ-"^	'---54.47±0.32^^^	…27：57±0.；8 …
64.79±1.56	52.17±0.39	51.19±0.41	25.45±0.21
^-72∕74±0.29^^'	66.51±1.53	.…57.67±0.36…	…27.61±0.55 …
65.61±0.19	60.55±0.73	53.24±0.45	26.63±0.25

GCN-SVD(I)k =50	78.89±6.29	66.67±3.60	65.56±2.83	31.11±0.79	77.98±0.43	68.25±0.86	63.41±0.45	27.81±0.39
GCN-SVD (II) k = 50	75.56±4.16	59.44±0.79	55.00±1.36	27.78±6.71	76.61±0.31	66.90±0.16	55.47±0.23	25.62±0.12
		Poison Attacks				Poison Attacks		
GNNGuard (I)	57.22±2.08	60.00±3.60	0.56±0.79	11.11±0.79	73.68±0.99	67.89±0.92	60.82±0.45	23.98±0.71
GNNGuard (II)	58.33±1.36	59.44±3.14	0.56±0.79	9.44±1.57	74.20±0.55	68.13±0.74	60.89±0.48	23.78±0.67
GCN-SVD(I) k = 5
GCN-SVD (II) k = 5
GCN-SVD (I) k = 10
GCN-SVD (II) k = 10
GCN-SVD (I) k = 15
GCN-SVD (II) k = 15
64.44±9.06 44.44±2.83	60.00±4.71 33.33±2.72	41.67±6.24 41.67±2.36	27.78±6.29 25.00±6.80
…67.78±5.50…	^-57∕78±1.57^^^^■	.•…式56±；.57…	…3T67∙±5.93 …
48.89±3.14	31.67±2.36	34.44±0.79	26.11±6.85
64.44±3.93	^-52∕78±4.7≡•…	…一 23：89±6.29…	…2933.93 …
51.11±3.42	33.89±3.93	30.56±2.83	26.11±6.14
64.65±2.57 42.19±5.33	66.35±1.48 40.17±1.57	53.14±0.43 51.87±0.38	25.64±0.47 24.82±0.43
65.54±1.28	65.46±0.92	55.68±0.15	…25.93±0.75 …
49.92±5.88	47.16±3.93	53.16±0.45	25.30±0.28
65.46±2.33	61.04±1.04	58.06±0.05	…25.83±0.69…
50.30±3.80	47.87±1.31	54.20±0.36	25.25±0.91

GCN-SVD(I)k =50
GCN-SVD (II) k = 50
61.67±4.71	48.33±7.07	16.67±4.08	30.56±7.97	60.06±5.43	49.31±4.52	62.07±0.69	26.05±0.63
53.33±4.91	28.89±2.08	23.33±2.72	25.00±5.44	47.82±7.59	51.20±1.78	55.00±2.06	25.18±0.98
Evasion Attacks
Evasion Attacks
GNNGuard ( I )
GNNGuard (II)
GCN-SVD (I) k = 5
GCN-SVD (II) k = 5
GCN-SVD(I) k = 10
GCN-SVD (II) k = 10
GCN-SVD (I) k = 15
GCN-SVD (II) k = 15
64.44±s.20	59.44±3.93	41.11 ±7.97	31.67±3.60
46.67±4.08	32.22±4.37	45.56±3.93	26.11 ±6.14
65.56±7.49	^"57.22±3.42….	36.11±1.57	…3T1i±0.79…
57.22±6.14	37.78±3.93	36.67±3.60	30.56±8.85
…67.22±6.14…	■-54.44±6.14•…	24：44±5.50一一	…30.00±1.36…
65.56±6.14	38.33±5.93	23.89 ±6.98	27.22±7.97
65.00±6.24	50.56±6.43
60.00±6.24	47.78±4.37
18.33±2.36	25.56±1.57
25.00±4.71	30.56±9.56
5psas≡
68.18±ι.i3	67.54±o.97	52.91 ±0.28	27.40±o.29
52.01 ±2.45	30.69±ι.oι	52.32±o.ιo	25.87±o.27
68.36±ι.33	…67.85107一…	54.51±o.68	…27.30±0.30…
58.70±3.00	45.62±2.52	52.58±0.20	24.60±0.26
^"69.32±ι.2i^^'	65.26±0.97	^"-"57.79±0.38^^^	^^28.06±0.2Γ^
64.02±i.30	54.09±2.25	53.81±o.35	25.29 ±0.41
75.30±0.62	64.49±1.58
73.21±1.68	59.34±3.42
63.53±0.26	27.74±0.61
56.95±0.33	25.80±0.67
GCN-SVD(I)k =50
GCN-SVD (II) k = 50

For GNNGuard, the ambiguity comes from different interpretations of Eq. (4) in the original pa-
per (Zhang & Zitnik, 2020): we consider the authors’ original implementation as variant (I), and
the model described in the original paper as variant (II), which we implement by building on the
authors’ implementation.
1https://github.com/google/python-fire
2https://signac.io
23
Under review as a conference paper at ICLR 2022
Table 6 shows that the differences in accuracy between the two variants are in most cases less than
2%, while in many cases variant (II) shows better accuracy compared to variant (I), especially in
experiments against Metattack. Thus, we use variant (II) as the default implementation for the
empirical evaluations in §5.
For GCN-SVD, the ambiguity comes from the order of applying the preprocessing and low-rank
approximation for the adjacency matrix A, which is not discussed in the original paper (Entezari
et al., 2020).
•	Variant (I): Since the original authors’ implementation is not publicly available, we consider
the implementation provided in DeepRobust (Li et al., 2020b) as variant (I): it first calculates
the rank-k approximation A of A, and then generates the preprocessed adjacency matrix As =
ID-1/2 (A + I)ID-1/2 = ID-1/2 AslD-1/2, which is then processed by a GCN (KiPf & Welling,
2017). However, as the identity matrix I is added into A after the low-rank approximation, the
diagonal elements of the resulting As matrix (i.e., the weights for the self-loop edges in the graph)
can become significantly larger than the off-diagonal elements, especially when the rank k is low.
As a result, this order of applying the preprocessing and low-rank approximation inadvertently
adopts Design 1 which we identified; we have shown in Theorem 3 that even merely increasing
the weights α for the ego-embedding in the linear combination ENC in Eq. (1) can lead to reduced
attack loss Latk under structural perturbations.
•	Variant (II): In variant (II), we consider the opposite order where we first add the identity matrix
I (self-loops) into the original adjacency matrix A, then we perform the low-rank approximation,
and finally we symmetrically normalize the low-rank matrix As to generate the preprocessed As
used by a GCN model. This order allows the diagonal elements to be more on par in magnitude
with the off-diagonal elements. As an example, on Citeseer, when using variant (I) with rank
k = 5, the average magnitude of the diagonal elements of the resulting As can be 22.3 times
the average magnitude of the off-diagonal elements; when using variant (II) instead, the average
magnitude of the diagonal elements is only 9.0 times that of the off-diagonal elements.
In Table 6, we report the performance of the two variants of GCN-SVD under the experimental set-
tings considered in §5 with rank k ∈ {5, 10, 15, 50}: Variant (I), with our first design implicitly built-
in, has in most cases significantly higher performance than variant (II), especially on homophilous
datasets and when rank k is low. These results further demonstrate the effectiveness of Design 1 that
we identified. To enable a clear perspective of the performance and robustness improvement brought
by Design 1, in our empirical analysis in §5, on top of the low-rank approximation vaccination we
adopt variant (II) as the default implementation.
Attack Implementations. We incorporate the following implementations of attacks from
DeepRobust (Li et al., 2020b) to our empirical framework.
NETTACK (Zugner et al., 2018)	https://github.com/DSE-MSU/DeepRobust/blob/master/deeprobust/graph/targeted_attack/nettack.py
Metattack (Zugner & Gunnemann, 2019a) https://github.com/DSE-MSU/DeepRobust7blob/master/deeprobust/graph7global attack7mettack.py
More Details on the Attack Setup. For NETTACK, we randomly select 60 nodes from the graph as
the target nodes for each set of perturbations, instead of the GCN-based target selection approach as
in (Zugner et al., 2018): the approach in (Zugner et al., 2018) only selects nodes that are correctly
classified by GCN (Kipf & Welling, 2017) on clean data, thus introducing unfair advantages towards
GCN, especially on heterophilous datasets where GCN can exhibit significantly inferior accuracy to
models like GraphSAGE (Zhu et al., 2020). For the experiments in §5.1, we use a budget of 1
perturbation per target node to match the setups of our theorems; for the benchmark study in §5.2,
we use an attack budget equal to a node’s degree and allow direct attacks on target nodes. For
Metattack, we budget the attack as 20% of the number of edges in each dataset, and we use the
Meta-Self variant as it shows the most destructiveness (Zugner & Gunnemann, 2019a).
More Details on Randomized Smoothing Setup. Following Bojchevski et al. (2020), we similarly
set the significance level α = 0.01 (i.e., the certificates hold with probability 1 - α = 0.99), using
103 samples to estimate the predictions of the smoothed classifier f (φ(s)) for input s, and another
106 samples to obtain multi-class certificates. For the randomization scheme φ, we only consider
structural perturbations where with probability p+ an new edge is added, and with probability p-
an existing edge is removed. We consider multiple sets of (p+ , p- ) in our experiments for a finer-
24
Under review as a conference paper at ICLR 2022
grained evaluation: (1) p+ = 0.001, p- = 0.4, where both addition and deletion are allowed; (2)
p+ = 0.001, p- = 0, where only addition is allowed; and (3) p+ = 0, p- = 0.4, where only
deletion is allowed.
Hardware Specifications. We use a workstation with a 12-core AMD Ryzen 9 3900X CPU, 64GB
RAM, and a Quadro P6000 GPU with 24 GB GPU Memory.
D.3 Combining Heterophilous Design with Low-Rank Approximation
In this section, we provide more details on how we incorporate the low-rank approximation vacci-
nation into the formulations of H2GCN (Zhu et al., 2020) and GraphSAGE (Hamilton et al., 2017)
in order to form the hybrid methods, H2GCN-SVD and GraphSAGE-SVD.
H2GCN-SVD. From (Zhu et al., 2020), each layer in the neighborhood aggregation stage of
H2GCN can be algebraically formulated as
R(k) = CONCAT (A2R(k-1), AR(k-1),R(k-1)),
(27)
where A = DT/2ADT/2 is m& Symmetricany normalized adjacency matrix without self-
loops; A 2 = D-1∕2A2D-1/2 is the symmetrically normalized 2-hop graph adjacency matrix
A2 ∈ {0,1}lVl×lVl, with [A2]u,v = 1 if V is in the 2-hop neighborhood N2(u) of node u; R(k)
are the node representations after the k-th layer, and CONCAT is the column-wise concatenation
function.
For H2GCN-SVD, We replace A and A2 in Eq.
(27) respectively with the low-rank
approximations of A and A2, which are both postprocessed to be symmetrically normalized.
GraphSAGE-SVD. From (Hamilton et al., 2017), each layer in GraphSAGE can be algebraically
formulated as
R(k) = σ (CONCAT (AR(I), R(I)) ∙ W(I)) ,	(28)
where A is the row-stochastic graph adjacency matrix without self-loops; R(k) are the node rep-
resentations after the k-th layer; CONCAT is the column-wise concatenation function; W(k) is the
learnable weight matrix for the k-th layer, and σ is the non-linear activation function (ReLU). For
GraPhSAGE-Svd, we replace A in Eq. (28) with the low-rank approximation of the adjacency
matrix A, postprocessed by row-stochastic normalization. Note that we do not enable the neighbor-
hood sampling function for the GraphSAGE and GraphSAGE-SVD models tested in this work, as
noted in Appendix D.4.
D.4 Hyperparameters
• H2GCN-SVD
Initialization Parameters:
-	adj_svd_rank:
best k chosen from {5, 50} for each dataset
• GraphSAGE-SVD
Initialization Parameters:
-	adj_nhood: [’1']
-	network_setup:
I-T1-G-V-C1-M64-R-T2-G-V-C2-MO-R
-	adj_norm_type: rw
-	adj_svd_rank:
best k chosen from {5, 50} for each dataset
Training Parameters:
-early_stopping: Yes
- train_iters: 200
- patience: 100
Training Parameters:
-early_stopping: Yes
-	train_iters: 200
-	patience: 100
25
Under review as a conference paper at ICLR 2022
• H2GCN
Initialization Parameters: (default parameters) • GraphSAGE Initialization Parameters: -	adj_nhood: [’1'] -	network_setup: I-T1-G-V-C1-M64-R-T2-G-V-C2-MO-R -	adj_norm_type: rw • CPGNN Initialization Parameters: -network_setup: M64-R-MO-E-BP2 • GPR-GNN Initialization Parameters: -	nhid: 64 -	alpha: 0.9, which is chosen from the best α ∈ {0.1, 0.2, 0.5, 0.9} on all datasets • FAGCN Initialization Parameters: -	nhid: 64 -	alpha: 0.9, which is chosen from the best α ∈ {0.1, 0.2, 0.5, 0.9} on all datasets -	dropout: 0.5 • GNNGuard Initialization Parameters: -	nhid: 64 -	dropout: 0.5 -	base_model: GCN for variant (I); GCN-fixed for variant (II) (default).	Training Parameters: -	early_stopping: Yes -	train_iters: 200 -	patience: 100 -	lr: 0.01 Training Parameters: -	early_stopping: Yes -	train_iters: 200 -	patience: 100 -	lr: 0.01 Training Parameters: -	early_stopping: Yes -	train_iters: 400 -	patience: 100 -	lr: 0.01 Training Parameters: -	train_iters: 200 -	lr: 0.01 Training Parameters: -	early_stopping: Yes - lr: 0.01 Training Parameters: -	train_iters: 81 -	lr: 0.01
26
Under review as a conference paper at ICLR 2022
• ProGNN
Initialization Parameters:	Training Parameters:
- nhid: 64 - dropout: 0.5	-	epochs: 400 -	lr: 0.01 -	lr_adj: 0.01 -	weight-decay： 5e-4 -	alpha: 5e-4 -	beta: 1.5 -	gamma: 1 -	lambda,: 0 -	phi: 0 -	outer_steps: 1 -	innter_steps: 2
• GCN-SVD
Initialization Parameters:	Training Parameters:
- nhid: 64 - k: best k chosen from {5, 10, 15, 50} for each dataset	- train_iters: 200 -Weight_decay: 5e-4 - lr: 0.01
- dropout: 0.5
-svd-solver:
eye-svd (for variant (II) only)
• GCN
Initialization Parameters (in class MultiLayerGCN): -	nhid: 64 -	nlayer: 2	Training Parameters: - train_iters: 200 - lr: 0.01 -weight-decay: 5e-4
• GAT
Initialization Parameters -	nhid: 8 -	heads: 8 -	dropout: 0.5	Training Parameters: -	early_stopping: Yes -	train_iters: 1000 -	patience: 100 -	lr: 0.01 -	weight-decay: 5e-4
• MLP
Initialization Parameters:	Training Parameters:
(in class H2GCN): -network_setup: M64-R-D0.5-MO	-early_stopping: Yes - train_iters: 200 -	patience: 100 -	lr: 0.01
27
Under review as a conference paper at ICLR 2022
D.5 Datasets
Dataset and Unidentifiability.
•	Heterophilous Datasets: FB100 (Traud et al., 2012) is a set of 100 university friendship network
snapshots from Facebook in 2005 (Lim et al., 2021), from which we use one network. Each node
is labeled with the reported gender, and the features encode education and accommodation. Data is
sent to the original authors (Lim et al., 2021) in an anonymized form. Though the dataset contains
limited demographic (categorical) information volunteered by users on their individual Facebook
pages, we manually inspect the dataset and confirm that the anonymized dataset is not recoverable
and thus not identifiable. Also, no offensive content is found within the data.
Snap Patents (Leskovec et al., 2005; Leskovec & Krevl, 2014) is a utility patent citation network.
Node labels reflect the time the patent was granted, and the features are derived from the patent’s
metadata. The dataset is maintained by the National Bureau of Economic Research, and is freely
available for download3. Neither personally identifiable information nor offensive content is iden-
tified when we manually inspect the dataset.
•	Homophilous Datasets: Cora (McCallum, 2000) and Citeseer (Sen et al., 2008) datasets are
scientific publication citation networks, whose labels categorize the research field, and features
indicate the absence or presence of the corresponding word from the dictionary. No personally
identifiable information or offensive content is identified when we manually inspect both datasets.
Downsampling. For better computational tractability, we sample a subset of the Snap Patents data
using a snowball sampling approach (Goodman, 1961), where a random 20% of the neighbors for
each traversed node are kept. We provide the pseudocode for the downsampling process in Algo-
rithm 1.
Algorithm 1: Downsampling Algorithm For Snap Patents
Input: Graph to sample G
Number of nodes to sample N
Sampling ratio p
Output: Downsampled graph G0
1	initialization
/* Initialize a queue bfsquene for Breadth First Search,
and a list nodessampled for storing sampled nodes
2	bfSquene — QUEUE()
3	nodeSsampled J LIST()
/* Start BFS with a random node from the largest connected component
in G;
Random(array, n) returns n elements from an array with equal
4
5
6
7
8
9
10
11
12
13
14
15
probability without replacement
nodestarting J RANDOM(LARGESTCONNECTEDCOMPONENT(G), 1)
push nodestarting into bfsquene
while Length(nodessampied) / N do
node J bfsquene.pop()
neighbors J one hop neighbors of node
neighborsdrawn J RANDOM(neighbors, p× LENGTH(neighbors))
for neighbor ∈ neighborsdrawn do
if neighbor ∈/ nodessampled then
append neighbor to nodesSamPIed
push neighbor into bfsquene
end
end
*/
*/
16 end
17 G0 J subgraph induced by nodessampled
18 return G0
3https://www.nber.org/research/data/us-patents
28
Under review as a conference paper at ICLR 2022
E Detailed Experiment Results
E.1 Detailed Results for Evaluation on Empirical Robustness
Table 7: Detailed classification accuracy (and standard deviation) of each method for the target
nodes attacked by Nettack, calculated across different sets of perturbation.
	Homophilous graphs		Heterophilous graphs	
	Cora h=0.804	Citeseer h=0.736	FB100 h=0.531	Snap h=0.134
Clean				
H2 GCN-SVD	74.44±3.42	70.00±2.72	61.67±2.36	30.56±2.08
GraphSAGE-SVD	77.22±4.78	70.00±1.36	60.00±4.08	27.22±5.50
H2GCN		一82.78±&31	69.44±6.98		60.56±1.57	…30.0012.72…
GraphSAGE	82.22±9.56	70.56±6.85	60.00±2.72	24.44±4.16
CPGNN	81.67 ±8.28	73.33±1.36	66.11±4.16	28.89±5.50
GPRGNN	82.22±7.49	67.78±2.08	56.67±4.91	27.78±3.42
FAGCN	83.33±8.16	70.56±5.15	58.33±5.93	29.44±0.79
GNNGuard		-77.22±6.29''	…67.7814.78		^67.22±2.08^^	…28.3313.60…
ProGNN	79.44±3.42	67.22±4.78	51.11±3.93	27.22±5.50
GCN-SVD	75.56±4.16	59.44±0.79	50.56±4.37	27.78±67
GAT		84.44±3.42	…70.0017.20		60.56±0.79	30.56±2.83
GCN	82.78±4.78	69.44±7.74	63.33±2.72	33.33±2.72
MLP		64.44±3.42	70.5613.42		-57.78±2.83"	…30.0012.72…
Poison (Pre-training)				
H2 GCN-SVD	70.0O±2.72	65.00±3.60	59.44±3.42	28.89±3.42
GraphSAGE-SVD	71.67±2.36	67.78±3.42	60.00±1.36	26.67±6.80
H2GCN		-38.89±5〔50一一	…27：22±；57		-27.78±3.42"	…12.7812.83…
GraphSAGE	36.67±2.72	31.67±10.89	33.89±3.42	16.67±7.07
CPGNN	47.22±6.14	40.56±9.65	49.44±10.30	21.67±2.72
GPRGNN	21.67±2.72	24.44±2.08	2.78±0.79	4.44±2.08
FAGCN	26.11±6.14	25.56±6.43	6.11±2.83	8.33±3.60
GNNGuard		■ 58.33±1.36"	59.4413.14		0.56±0.79	9.44±1.57
ProGNN	48.89±7.97	32.78±7.49	33.89±4.78	17.78±9.26
GCN-SVD	53.33±4.91	28.89±2.08	41.67±2.36	25.00±5.44
GAT		13.89±0.79	""8.89±3.42		0.56±0.79	…-3：8914.37…
GCN	1.67±0.00	4.44±2.83	0.56±0.79	2.22±2.08
MLP		64.44±3.42	70.5613.42		"57.78±2.83"	…30.0012.72…
Evasion (Post-training)				
H2 GCN-SVD	70.56±3.42	66.11 ±4.78	60.00±2.72	28.89±3.14
GraphSAGE-SVD	70.56±2.08	68.33±3.60	59.44±2.83	26.11 ±6.85
H2GCN		45.56±3.42	33.8911.57		-32.78±0.79"	…12.7812.83…
GraphSAGE	44.44±3.14	35.00±8.92	42.22±2.83	15.56±6.71
CPGNN	52.22±6.98	46.67±6.80	15.56±2.83	22.78±4.78
GPRGNN	29.44±3.14	32.22±0.79	9.44±2.83	3.33±1.36
FAGCN	38.89±6.71	37.78±4.78	12.78±2.83	10.00±2.36
GNNGuard		-	-	-	-
ProGNN	-	-	-	-
GCN-SVD	60.00±6.24	47.78±4.37	45.56±3.93	30.56±9.56
GAT		^12.22±4^16^^	"23.33±5.44		1.67±1.36	…-2.22±3.14…
GCN	5.56±2.08	8.89±4.16	0.56±0.79	0.56±0.79
MLP		64.44±3.42	70.5613.42		-57.78±2.83一	…30.00±2.72…
29
Under review as a conference paper at ICLR 2022
Table 8: Detailed classification accuracy (and standard deviation) for the unlabeled nodes of each
method attacked by Metattack with budget as 20% of the total number of edges of each graph,
calculated across different sets of perturbation.
	Homophilous graphs		Heterophilous graphs	
	Cora h=0.804	Citeseer h=0.736	FB100 h=0.531	Snap h=0.134
		Clean		
%GCN-SVD	76.89 ±0.37	73.42±1.o3	56.81 ±0.77	27.63±o.26
GraPhSAGE-SVD	77.52±ο.29	72.16±o.17	57.38±o.86	26.72±o.70
H2GCN		83.94±0.97	75.34±0.90	56.95±0.13	27.49±o.05
GraphSAGE	82.21 ±ο.63	74.64±o.93	56.60±1.40	27.18±o.84
CPGNN	80.67±0.51	74.92±o.62	60.17±7.09	27.13 ±0.63
GPRGNN	81.84±1.75	70.71 ±0.46	62.40±o.83	26.08±o.31
FAGCN	81.59 ±0.82	73.99±o.63	59.64±1.38	27.15±o.23
GNNGuard		8θ.15±0.55	72.61 ±0.28	65.66±o.60	26.51 ±o.98
ProGNN	81.32±o.43	71.82±1.12	49.84±o.o3	27.49±o.66
GCN-SVD	76.61±0.31	66.90±0.16	55.47±0.23	26.63±0.25
GAT		83.72±o.24	73.40±1.00	61.69±0.92	27.30±0.03
GCN	84.32±o.32	74.27±0.15	64.86±o.79	27.30±o.43
MLP		64.55±1.58	"67.67 ±0.11		56.56±0.58	…26：2511.05一
		Poison (Pre-training)		
H2GCN-SVD	67.87 ±0.47	70.42±o.46	56.72±o.08	25.60±o.14
GraphSAGE-SVD	68.86±1.32	69.10±o.52	55.76±0.33	26.58±o.30
h2gCN		57.75 ±6.61	54.34±o.82	54.84±0.76	25.34±o.59
GraphSAGE	54.68±2.56	59.74±1.74	54.72±o.83	24.14±o.76
CPGNN	74.55±i.23	68.07±1.93	61.58±1.5o	26.76±o.41
GPRGNN	48.29 ±5.23	35.25 ±2.77	59.94±o.6o	21.06±1.29
FAGCN	60.11±4.82	53.18±6.00	55.97±1.81	24.04±0.62
GNNGuard		74.20±0.55	68.13 ±0.74	60.89^±o.48	23.78±0.67
ProGNN	45.10±6.20	46.58±1.o2	53.40±1.19	24.80±1.09
GCN-SVD	47.82 ±7.59	51.20±1.78	55.00±2.06	25.25±o.91
GAT		41.70±3.60	48.40±2.17	50.37±0.66	25.0O±0.73
GCN	37.46±3.35	45.81 ±2.99	51.82±1.41	25.03±o.68
MLP		64.55±1.58	67.67 ±0.11	56.56±0.58	26.25±1.05
		Evasion (Post-training)		
H2GCN-SVD	74.01±0.35	71.54±1.92	56.58±0.63	27.26±o.17
GraphSAGE-SVD	74.31±0.40	70.22±o.90	57.38±o.88	26.77±1.01
h2gcn		"82.86±7.0?'	"73.2O±2'o4		57.05±0.20	…27.1010.06…
GraphSAGE	80.57±0.55	72.89±1.72	56.91±1.61	27.16±0.78
CPGNN	79.06±1.18	73.44±o.98	60.19±7.20	27.02±o.75
GPRGNN	80.80±1.67	69.77±0.42	61.91 ±0.74	26.16±o.25
FAGCN	80.70±0.99	73.14±1.o2	59.39±1.36	27.25±o.30
GNNGuard		-	-	-	-
ProGNN	-	-	-	-
GCN-SVD	73.21±1.68	59.34±3.42	56.95±0.33	25.29±o.41
gaT		"81.96±o.31'"	…70.70±0.69		61.44±o.94	"27.45±0.13"
GCN	83.03 ±0.69	73.06±o.58	64.86±o.47	27.15±o.39
MLP		64.55±1.58	67.67 ±0.11	56.56±0.58	26.25±1.05
30
Under review as a conference paper at ICLR 2022
E.2 Detailed Results for Evaluation on Certifiable Robustness
Table 9: Accumulated certifications (AC), average certifiable radii (尸。and Fd) and accuracy of
GNNs with randomized smoothing enabled (i.e., f (φ(s))) on all nodes of the clean datasets, with a
ramdomization scheme φ allowing addition only (i.e., p+ = 0.001, p- = 0) or deletion only (i.e.,
p+ = 0, p- = 0.4). For each statistic, we report the mean and stdev across 3 runs. Best results are
highlighted in blue per dataset, and in gray per model group. For results with randomization scheme
allowing both addition and deletion, see Table 4.
	φ 一 8 H		Addition Only				Deletion Only			
			AC	rFa	rFd	Acc. %	AC	rFa	rFd	Acc. %
H2GCN	X		0.42±0.01	0.52±0.03	-	8O.85±i.98	5.41±0.23	-	6.44±o.2o	84.O4±i.55
GraphSAGE	X		0.28±0.03	0.34±0.03	- I	81.O5±i.34	5.05±0.12	-	6.12±o.o6	82.49±i.o6
CPGNN	X		0.17±0.02	O.21±0.03	-	78.34±i.26	4.92±0.33	-	6.17±0.42	79.69±0.81
GPR-GNN	X	J O U	0.43±0.03	O.55±o,o3	-	76.96±2.18	5.37±o.i4	-	6.58±0.05	81.56±1.59
FAGCN	X		0.43±0.01	O.54±o,oi	-	79.O4±o.68	5.74±o.o6	-	7.O3±o.o5	81.56±0.80
										
GAT			Xi9±o.04	…0：23±0：04一	-I	81.62±2.oi	5.56±0.12	-	6.58±0.06	84.46±i.o8
GCN			O.19±o,02	O.24±o,o2	-	81.52±3.io	5.71±o.o7	-	6.76±o.o3	84.49±o.84
H2GCN	X		0.29±0.05	0.40±0.06	-	72.93±2.3o	5.55±0.10	-	7.29±o.i8	76.17±o.48
GraphSAGE	X		0.33±0.01	0.45±0.01	-	74.66±ι.i9	5.43±0.07	-	7.15±o.i2	75.97±o.47
CPGNN	X		0.15±0.02	O.2O±0.02	-	74.62±o.3o	5.59±o.22	-	7.54±0.22	74.05±0.84
GPR-GNN	X		O.4O±o,oi	O.59±o,02	-	67.52±0.49	5.O5±o.o5	-	7.21±o.o6	70.10±0.15
FAGCN	X		O.38±o,o2	O.53±o,02	-	72.41±1.03	5.46±0.09	-	7.42±0.12	73.56±0.18
GAT			一0：09±0.01	0.13±0.02	- I	7Tθ7±0.44一	5.39±0.16	-	-7：25±0.12一	74.33±i.43^^
GCN			O.18±0.oi	O.25±o,00	- I	74.O7±i.65	5.55±o.io	-	7.42±o.i6	74.76±o.72
H2GCN	X		0.54±0.00	O.94±0.oo	-	57.11±0.10	4.75±o.o3	-	8.32±o.o3	57.15±0.23
GraphSAGE	X		0.52±0.01	O.92±0.oi	-	56.70±1.41	4.28±o.o5	-	7.56±o.io	56.58±1.32
CPGNN	X		0.54±0.04	0.90±0.04	-	60.39±7.26	4.22±0.05	-	6.66±0.11	63.3O±o.29
GPR-GNN	X	O rH	0.46±0.01	O.73±o,02	- I	62.26±o.26	4.04±0.08	-	6.51±o.io	62.O5±o.3i
FAGCN	X	α -⅛	O.55±0.00	O.9O±0.oi	-	6O.6O±0.36	4.58±0.10	-	7.71±0.03	59.45±1.26
GAT			O.46±o,o3	-O.74±o0-	-	61.97±i.4i	3.33±o.09	-	-5.37±o.iJ	62.01±i.oi
GCN			O.42±o,oi	O.65±0.oi	-1	64.7O±o.55	3.28±o.o4	-	5.O1±o.ii	65.5O±o∙56
H2GCN	X		0.11±0.01	0.42±0.05	-	26.74±o.i8	1.41±0.04	-	5.16±o.i9	27.28±o.2i
GraphSAGE	X		0.06±0.02	O.24±o,o8	-	27.00±0.63	1.10±0.11	-	4.O4±0.36	27.21±o.99
CPGNN	X		O.12±o,02	O.43±o,o8	-	27.00±0.41	1.69±0.06	-	6.39±0.13	26.45±O.5O
GPR-GNN	X	d a S	O.O3±0.oi	0.11 ±0.02	-	26.14±o.73	1.10±0.04	-	4.19±o.i7	26.24±O.43
FAGCN	X		0.10±0.01	0.36±0.03	- I	27.13±o.i6	1.77±o.o5	-	6.48±o.2o	27.25±O.18
										
GAT			一0：02±0.00	…O.08±0∕-	-	27.OO±o.59^	1.10±0.03	-	4.06±o.ii	27.18±0.04-^
GCN			O.O2±o,00	O.O7±0.oi	-I	27.45±o.82	1.38±o.o5	-	5.O1±o.16	27.57±o.27
E.3 Comparison Between Certifiable and Empirical Robustness
While the evaluations on both empirical and certifiable robustness have shown that methods fea-
turing the design have shown largely improved robustness compared to the best-performing unvac-
cinated method, we find that the robustness rankings for methods under certifiable and empirical
robustness are different; previous results from Geisler et al. (2020) have also shown discrepancy in
certifiable and empirical robustness rankings.
We think this discrepancy may be attributed to multiple factors. Firstly, the radius on which cer-
tificates can be issued with randomized smoothing may not cover the radius of perturbations we
allowed for Nettack and Metattack in §5.2, which are more than tens of edges for Metattack and
high-degree nodes in Nettack (where we use an attack budget equal to the degree of the target
node). Furthermore, even for low-degree nodes, attacks are much more inclined to introduce new
edges instead of removing existing ones, as we have shown in §5.1, which can make it much harder
to obtain certificates (Bojchevski et al., 2020). In our case, the methods we evaluated generally have
rFa < 1, meaning that for most nodes there are no certificates to cover even the addition of a single
edge. Thus, methods which display higher certifiable robustness under smaller perturbations may
not keep their robustness under larger perturbations by empirical attacks. Moreover, while we are
evaluating on randomized smoothed models f (φ(s)) to measure certifiable robustness, in empirical
31
Under review as a conference paper at ICLR 2022
robustness we are evaluating on the robustness of the base models f (s), which may differ from the
robustness of the randomized smoothed models. Lastly, it is worth noting that the lack of certifica-
tion for a model within certain radius does not imply a vulnerability to all adversarial attacks within
that radius; the model may still be robust against many attacks within that radius. Similarly, it is also
possible for existing certification approaches to miss some certifiable cases. Taking all these factors
into account, we believe that while evaluations on certifiable robustness provide complementary per-
spectives to evaluations empirical robustness, at the current stage it cannot replace the evaluations
on empirical robustness, and the relation between certifiable and empirical robustness remains as a
question for future works.
E.4 Complexity and Runtime of Heterophilous Vaccination
Another benefit of adopting heterophily-inspired design for boosting robustness of GNNs is their
smaller computational overhead compared to existing vaccination mechanisms, especially vaccina-
tions based on low-rank approximation. As our identified design can be applied as simple archi-
tectural changes on top of an existing GNN, they usually maintain the same order of computational
complexity as the base model. For example, adding the heterophilous design to GCN (Kipf &
Welling, 2017) results in an architecture similar to GraphSAGE (Hamilton et al., 2017); both have
the same order of computational complexity as O(|V| + |E|) by leveraging the sparse connectivity
of most real-world graphs. Low-rank approximation-based vaccination, on the other hand, approx-
imates the adjacency matrix of a graph by an SVD, resulting in an adjusted low-rank adjacency
matrix A based on which the GNN runs. However, not only is computing an SVD potentially costly
(O(∣V∣3) in general), but in most cases it also results in a dense A (in contrast to the sparse original
adjacency matrix), thus increasing the complexity of each iteration of the GNN.
Table 10: Runtime (in seconds) of 200 training iterations on Cora. See App. D for the implementa-
tion used for each method.
GCN	GAT	GNNGuard	ProGNN	GCN-SVD	H2GCN
2.17	2.98	39.63	220.30	134.81	16.54
GraphSAGE	FAGCN	GRP-GNN	CPGNN	H2GCN-SVD	GraphSAGE-SVD
17.24	1.91	2.66	24.08	62.33	55.45
Table 10 shows the runtime of 200 training iterations of each model. We observe that models with
the heterophilous design have the smallest runtime among all vaccinated models. Even for methods
based on the same implementation, H2GCN and GraphSAGE are still 3-4 times faster than the cor-
responding H2GCN-SVD and GraphSAGE-SVD methods. For fair runtime measurements, we mea-
sure the runtime of each model on an Amazon EC2 instance with instance type as p3.2xlarge,
which features an 8-core CPU, 61 GB Memory, and a Tesla V100 GPU with 16 GB GPU Memory.
E.5 Additional Results on Structural Attacks on Heterophilous Graphs
Figure 2: Scatter plots of the degrees of the target nodes (x-axis) and gambit nodes (y-axis) involved
in the targeted attacks studied in §5.1. Attacks tend to leverage gambit nodes with low degrees,
especially on heterophious graphs, which makes attacks increasing heterophily effective following
the conclusions of Thm. 2.
32