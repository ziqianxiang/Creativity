Under review as a conference paper at ICLR 2022
Unit Ball Model for Embedding Hierarchical
Structures in the Complex Hyperbolic Space
Anonymous authors
Paper under double-blind review
Ab stract
Learning the representation of data with hierarchical structures in the hyperbolic
space attracts increasing attention in recent years. Due to the constant negative
curvature, the hyperbolic space resembles tree metrics and captures the tree-like
properties naturally, which enables the hyperbolic embeddings to improve over tra-
ditional Euclidean models. However, most real-world hierarchically structured data
such as taxonomies and multitree networks have varying local structures and they
are not trees, thus they do not ubiquitously match the constant curvature property
of the hyperbolic space. To address this limitation of hyperbolic embeddings, we
explore the complex hyperbolic space, which has the variable negative curvature,
for representation learning. Specifically, we propose to learn the embeddings of
hierarchically structured data in the unit ball model of the complex hyperbolic
space. The unit ball model based embeddings have a more powerful representation
capacity to capture a variety of hierarchical structures. Through experiments on
synthetic and real-world data, we show that our approach improves over the hyper-
bolic embedding models significantly. We also explore the competence of complex
hyperbolic geometry on the multitree structure and 1-N structure.
1	Introduction
Representation learning of data with hierarchical structures is an important machine learning task with
many applications, such as taxonomy induction (Fu et al., 2014) and hypernymy detection (Shwartz
et al., 2016). In recent years, the hyperbolic embeddings (Nickel and Kiela, 2017; 2018) have been
proposed to improve the traditional Euclidean embedding models. The constant negative curvature
of the hyperbolic space produces a manifestation that the hyperbolic space can be regarded as a
continuous approximation to trees (Krioukov et al., 2010). The hyperbolic space is capable of
embedding any finite tree while preserving the distances approximately (Gromov, 1987).
However, most real-world hierarchical data do not belong to tree structures since they can have varying
local structures while being tree-like globally. For example, the taxonomies such as WordNet (Miller,
1995) and YAGO (Suchanek et al., 2007) contain many 1-N (1 child links to multiple parents)
cases and multitree structures (Griggs et al., 2012), which are much more complicated than the tree
structure. In consequence, the hyperbolic space which resembles tree metrics has limitations on
capturing the general hierarchically structured data.
To address the challenge, in this paper, we propose a new approach to learning the embeddings of
hierarchically structured data. Specifically, we embed the hierarchical data into the unit ball model
of the complex hyperbolic space. The unit ball model is a projective geometry based model to
identify the complex hyperbolic space. One of the main differences between the complex and the real
hyperbolic space is that the curvature is non-constant in the complex hyperbolic space. In practice,
the variable negative curvature makes the complex hyperbolic space more flexible in handling varying
structures while the tree-like properties are still retained.
For empirical evaluation, we evaluate different geometrical embedding models on various hierar-
chically structured data, including synthetic graphs and real-world data. The experimental results
demonstrate the advantages of our approach. In addition, we investigate two specific structures in
which complex hyperbolic geometry shows outstanding performances, namely the multitree structure
and 1-N structure, which are highly common and typical in real-world data. To summarize, our
work has the following main contributions: 1. We present a novel embedding approach based on the
1
Under review as a conference paper at ICLR 2022
complex hyperbolic geometry to handle data with complicated and various hierarchical structures.
To the best of our knowledge, our work is the first to propose complex hyperbolic embeddings. 2.
We introduce the embedding algorithm in the unit ball model of the complex hyperbolic space. We
formulate the learning and Riemannian optimization in the unit ball model. 3. We evaluate our
approach with experiments on an extensive range of synthetic and real-world data and show the
remarkable improvements of our approach.
2	Related Work
Hyperbolic embeddings. Hyperbolic embedding methods have become the leading approach for
representation learning of hierarchical structures. Nickel and Kiela (2017) learned the representations
of hierarchical graphs in the Poincare ball model of the hyperbolic space and outperformed the
Euclidean embedding methods for taxonomies. The Poincare embedding model was then improved
by follow-up works on hyperbolic emebddings (Ganea et al., 2018a; Nickel and Kiela, 2018). These
methods learned the hyperbolic embeddings by Riemannian optimization (Bonnabel, 2013), which
was further improved by the Riemannian adaptive optimization (BeCigneUl and Ganea, 2019).
Another branch of study (Sala et al., 2018; Sonthalia and Gilbert, 2020) learned the hyperbolic
embeddings through combinatorial construction. Instead of optimizing the soft-ranking loss by
Riemannian optimization as in (Nickel and Kiela, 2017; 2018), the construction-based methods
minimize the reconstruction distortion by combinatorial construction. However, both the optimization-
based and construction-based hyperbolic embeddings suffer from the limitation in hierarchical graphs
with varying local structures. To tackle the challenge, Gu et al. (2019) extended the construction-based
method by jointly learning the curvature and the embeddings of data in a product manifold. Although
it can provide a better representation than a single space with constant curvature, it is impractical to
search for the best manifold combination among enormous combinations for each new structure.
Motivated by the promising results of previous works, extensions to the multi-relational graph
hyperbolic embeddings (Balazevic et al., 2019; Chami et al., 2020; Sun et al., 2020) and hyperbolic
neural networks (Ganea et al., 2018b; GUlCehre et al., 2019; Liu et al., 2019; Chami et al., 2019;
Zhu et al., 2020; Dai et al., 2021a; Shimizu et al., 2021) were explored. Notably, (Chami et al.,
2019; 2020) leverages the trainable curvature to compensate for the disparity between the actual data
structures and the constant-curvature hyperbolic space, where each layer in the graph neural network
or each relation in the multi-relational graph has its own curvature parameterization. Since we only
focus on the single-relation graph embeddings and taxonomy embeddings in this work, we do not
evaluate the multi-relational knowledge graph embedding models or the neural networks in our tasks.
The hyperbolic learning also inspired other research tasks and applications, such as classification (Cho
et al., 2019), image reconstruction (Skopek et al., 2020), text generation (Dai et al., 2021b), etc.
Complex embeddings. The traditional knowledge graph embeddings were learned in the real
Euclidean space (Nickel et al., 2011; Bordes et al., 2013; Yang et al., 2015) and were used for
knowledge graph inference and reasoning. In recent years, several works suggested utilizing the
complex Euclidean space for inferring more relation patterns, such as ComplEx (Trouillon et al., 2016)
and RotatE (Sun et al., 2019). These models have been demonstrated to be effective in knowledge
graph embeddings. The success of the complex embeddings reveals the potential of the complex
space and inspires us to explore the complex hyperbolic space.
3	Preliminaries
3.1	Hyperbolic Geometry
Hyperbolic space1 is a homogeneous space with constant negative curvature.2 In the hyperbolic space
HnR(K) of dimension n and curvature K, the volume of a ball grows exponentially with its radius ρ:
Vol(BHn(K)(P)) ~ e√-K(n-1)ρ.	(1)
1In this paper, we use hyperbolic space to refer to real hyperbolic space and hyperbolic embeddings to refer
to real hyperbolic embeddings for avoiding wordiness.
2In this paper, curvature refers to the sectional curvature. Please see Appendix A.1 for definition.
2
Under review as a conference paper at ICLR 2022
Contrastively, in the Euclidean space En , the curvature is 0 and the volume of a ball grows polynomi-
ally with its radius:
vol(BEn (ρ))
∏n∕2
Γ(n∕2)pρ
〜pn.
(2)
The exponential volume growth rate enables the hyperbolic space to have powerful representation
capability for tree structures since the number of nodes grows exponentially with the depth in a tree,
while the Euclidean space is too flat and narrow to embed trees.
3.2	Complex Hyperbolic Geometry
Complex hyperbolic space is a homogeneous space of variable negative curvature. Its ambient
Hermitian vector space Cn,1 is the complex Euclidean space Cn+1 endowed with some Hermitian
form hhz, wii, where z, w ∈ Cn+1. Then the Hermitian space Cn,1 can be divided into three subsets:
V- = {z ∈ Cn,1|hhz,zii < 0}, V0 = {z ∈ Cn,1 - {0}|hhz, zii = 0}, and V+ = {z ∈ Cn,1|hhz,zii >
0}. Let P be a projection map P : Cn,1 - {zn+1 = 0} → Cn, i.e.,
z1
P :	.
.
zn+1
z1/zn+1
→	.	, where Zn+ι = 0.	(3)
zn/zn+1
Then the complex hyperbolic space HnC and its boundary ∂HnC are defined using the projectivization:
HnC = PV-,	∂HCn = PV0.	(4)
The curvature of the complex hyperbolic space is summarized by (Goldman, 1999) as follows:
Theorem 1. The curvature is not constant in HnC. It is pinched between -1 (in the directions of
complex projective lines) and -1/4 (in the directions of totally real planes).
We leave the full proof in Appendix B. The non-constant curvature, which we expect to be favorable
for embedding various hierarchical structures, is one of the main differences between HCn and HRn .
The complex hyperbolic space also has the tree-like exponential volume growth property. The volume
of a ball with radius p in HCn is given by
Vol(BHn(P)) = 8nσn-i sinh2n(p∕2)〜enP，	⑸
where σ2n-1 = 2∏n∕n! is the Euclidean volume of the unit sphere S2n-1 ∈ Cn.
From the properties of the complex hyperbolic geometry, we expect that the complex hyperbolic
space can naturally handle data with diverse local structures in virtue of the variable curvature as
presented in Theorem 1 while preserving the tree-like properties as shown in Eq. (5).
From this section, we see that complex hyperbolic geometry and hyperbolic geometry are typically
of different characteristics. The n-dimensional (n-d) complex hyperbolic space is not simply the
2n-d hyperbolic space or the product of two n-d hyperbolic spaces. This implies that our complex
hyperbolic embedding model is intrinsically different from the hyperbolic embedding methods (Nickel
and Kiela, 2017; 2018) or the product manifold embeddings (Gu et al., 2019).
4	Unit Ball Embeddings
We propose to embed the hierarchically structured data into the unit ball model of the complex
hyperbolic space. In this section, we introduce our approach in detail.
4.1	The Unit Ball Model
The unit ball model is one model used to identify the complex hyperbolic space, which can be derived
via the projective geometry (Goldman, 1999). We now provide the necessary derivation.
3
Under review as a conference paper at ICLR 2022
Take the Hermitian form of Cn,1 in Section 3.2 to be a standard Hermitian form:
《z, w》=Z1W1 +------+ ZnW - Zn+lWn+1,	(6)
where W is the conjugate of w. Take Zn+ι = 1 in the projection map P in Eq. (3). Then from Eq. (4),
we can derive the formula of the unit ball model:
BC = P({z ∈ Cn,1|《z,z》< 0}) = {(Z1,…，Zn, 1)∣∣zι∣2 + …+ ∣Zn∣2 < 1}.	(7)
The metric on BCn is Bergman metric, which takes the formula below in 2-d case:
ds2
-4
---------o det
hhz, zii
hhz, zii
hhz, dzii
hhdz, zii
hhdz, dzii
(8)
The distance function on BCn is given by
dBCn (z, w)
arcosh
(2 hhz, w》《w, Zii
(hhz, ziihhw, wii
-1
(9)
Note that there are other choices of the Hermitian form hhz, wii, which corresponds to other models
of complex hyperbolic geometry, such as the Siegel domain model. We choose the unit ball model for
the relatively simple formula as well as convenient computations of the metric and distance function.
4.2	Embeddings in the Unit Ball Model
Given the hierarchical data containing a set of nodes X = {xp}pm=1 and a set of edges E =
{(xp, xq)|xp, xq ∈ X}, we aim to learn the embeddings of the nodes Z = {zp}pm=1, where zp ∈ BCn.
The objective of the embeddings is to recover the structures of input data, including the distances
between the nodes as well as the partial order in the hierarchies. Here we adopt the soft ranking
loss used in the Poincare ball embeddings (Nickel and Kiela, 2017) and the hyperboloid embed-
dings (Nickel and Kiela, 2018), which aims at preserving the hierarchical relationships among nodes:
-dBn (zp,zq)
XeC
log------------,-,----、,	(10)
P	e-dBCn (zp,zk)
(xp,xq)∈E	xk ∈N (xp)
where N(xp) = {xk : (xp, xk) ∈/ E} ∪ {xp} is the set of negative examples for xp together with xp.
dBn is the distance function in the unit ball model given in Eq. (9). The minimization of L makes the
connected nodes closer in the embedding space than those with no observed edges.
The learning process implicitly aligns the geometric structures of the embedding space and the
underlying graph structures of data since the loss function aims at preserving the hierarchical
relationships among nodes while the underlying graph structures are reflected by the hierarchical
relationships. We learn the embeddings in the unit ball model, where the variable negative curvature
of the complex hyperbolic space provides the capacity to deal with more varying structures. The
experiments in Section 5 exhibit that the unit ball model learns the high-quality embeddings and
captures the various hierarchical structures.
4.3	Riemannian Optimization in the Unit Ball Model
We learn the embeddings Z = {zp}pm=1 through solving the optimization problem with constraint:
Z — arg min L	s.t.∀zp ∈ Z,zp ∈ BCn.	(11)
For the optimization problems in Riemannian manifolds, Bonnabel (2013) presented the Riemannian
stochastic gradient descent (RSGD) algorithm, which we employ to optimize Eq. (11). To update an
embedding Z ∈ BC,3 we need to obtain its Riemannian gradient Vr. Specifically, the embedding is
updated at the t-th iteration by z(t) J z(t-1) - η(t)VrL(z), where η(t) is the learning rate at the
t-th iteration and VRL(z) is the Riemannian gradient of L(z).
3Here we omit the subscript of zp for concision.
4
Under review as a conference paper at ICLR 2022
Algorithm 1 RSGD of the unit ball embeddings.
Input: initialization z(0), number of iterations T, learning rates {η(t)}tT=1.
for t = 1 to T do
∂dBn	∂dBn
Compute	and -^yC by Eqs. (13) and (14).
Compute NEL(Z) and VrL(Z) by Eq. (12).
Update z(t) by Eq. (16).
end for
The Riemannian gradient VR can be derived from rescaling the Euclidean gradient VE with the
inverse of the metric tensor ds2 in Eq. (8). Apply the chain rule of differential functions and we have:
VrL(z) = -Z12VEL(Z) = -12/L(Z)、VEdBn(z,w).	(12)
ds2	ds2 ∂dBn (Z, w) C
∂d∂L(Z)w) is trivial to compute from Eq. (10). In practical training, We implement and compute
the complex hyperbolic embedding as its real part and imaginary part, i.e., Z = x + iy, where i
represents the imaginary unit, i.e., i2 = -1. In order to get the gradient of the distance function
VE dBn (Z, w) in Eq. (12), We get the partial derivative With regard to the real part and the imaginary
∂dBCn (z,w)	∂dBCn (z,w)
part, i.e., VEdBn(z, w) = -Cx-+ i —⅝y—.
The partial derivatives of the unit ball model distance take the folloWing formulas:
ddBn	=	4	( Re(《z, WiiW)-《z, w》《w, ZiiX )
dx	-	p -1	l《z,z》《w，wii	《z, Zihw, wii 卜
ddBn	=	4	(Imhz, WiiW)-《z, w》《w,z》y ∖
dy - Pp - 1 l《z,z》《w,wii	《z,z》2《w, Wii，'
ddBn _	4
dx	pP2 - 1
ddBn _	4
(13)
(14)
where p = cosh(dBn (z, w)). Re(∙) and Im(∙) denote the real and the imaginary part respectively.
The full derivation of Eqs. (13) and (14) is given in Appendix C.
Since the embedding z should be constrained within the unit ball model, we apply the same projection
strategy as (Nickel and Kiela, 2017) via a small constant ε:
proj(z) = z/(|z| - ε), if |z| ≥ 1, else z.	(15)
To sum up, the update of z at the t-th iteration is
Ztt — Proj(Z(t-1) — η⑴ VR L(Z)) = Proj(Z(t-1) — n(t)-ɪ VE L(Z)∖	(16)
The RSGD steps of the unit ball embeddings are presented in Algorithm 1.
5	Experiments
In experiments, we evaluate the performances of our approach and baselines on various hierarchical
structures, including synthetic graphs and real-world data. We focus on the graph reconstruction task
and the link prediction task. The main results are reported in this section. For more experiments,
please refer to Appendix F.
5.1	Experimental Settings
5.1.1	Data
We use synthetic and real-world data that exhibit underlying hierarchical structures to evaluate our
approach. The public links or generator package links of the data are given in Appendix F.1.
Synthetic. We generate various balanced trees and compressed graphs using NetworkX package (Hag-
berg et al., 2008). For balanced trees, we generate the balanced tree with degree r and depth h. For
5
Under review as a conference paper at ICLR 2022
Table 1: The real-world datasets statistics.
I Xiphophorus ∣ ICD10 ∣ YAGO3-wikiObjects ∣ WordNet-noun				
Nodes	3,562	19,155	17,375	82,115
Edges	7,536	78,357	153,643	743,086
Depth/	13	6	16	20
Training edges	7,536	70,521	138,277	668,776
Valid/Test edges	4,160	3,918	7,683	37,155
δ-hyperbolicity	2.5	0.0	1.0	0.5
compressed graphs, we generate k random trees on m nodes and then aggregate their edges to form
a graph. Some examples of the synthetic data are given in Appendix F.1.
Xiphophorus. The Xiphophorus is a multitree dataset from (Cui et al., 2013), which contains 160
trees representing mrbayes consensus trees inferred for different genomic regions on 26 Xiphophorus
fishes. We use the saved MultiTree object in toytree package.
ICD10. The 10-th revision of International Statistical Classification of Diseases and Related Health
Problems (ICD10) (Bramer, 1988) is a medical classification list provided by the World Health
Organization. We construct its full transitive closure as the ICD10 dataset.
YAGO3-wikiObjects. YAGO3 (Mahdisoltani et al., 2015) is a huge semantic knowledge base. It
provides a taxonomy derived from Wikipedia and WordNet. We extract the Wikipedia concepts and
entities that are descendants of hwikiCatjObjeCts) as well as the hypernymy edges among them. We
compute the transitive closure of the sampled taxonomy to construct the YAGO3-wikiObjects dataset.
WordNet-noun. WordNet (Miller, 1995) is a large lexical database. The hypernymy relation among
all nouns forms a noun hierarchy. We use its full transitive closure as the WordNet-noun dataset.
For the multitree (Xiphophorus), we use the full dataset as the training set and the edges containing
the leaf nodes as the test set. For each real-world taxonomy (ICD10, YAGO3-wikiObjects, WordNet-
noun), we randomly split the edges into train-validation-test sets with the ratio 90%:5%:5%. We
make sure that any node in the validation and test sets must occur in the training set since otherwise,
it cannot be predicted. But the edges in the validation and test sets do not occur in the training
set since they are disjoint. We provide the statistics of the real-world datasets in Table 1. The
Gromov’s δ-hyperbolicity (Gromov, 1987) measures the tree-likeness of graphs (refer to Appendix D
for definition). The lower δ corresponds to the more tree-like graph and trees have 0 δ-hyperbolicity.
5.1.2	Tasks
We evaluate the following two tasks:
Graph reconstruction: we train the embeddings of the full data and then reconstruct it from the
embeddings. The task evaluates representation capacity. Link prediction: we train the embeddings
on the training set and predict the edges in the test set. The task evaluates generalization performance.
5.1.3	Baselines
We compare our approach UnitBall to the following methods: the sate-of-the-art combinatorial
construction-based hyperbolic embedding method TreeRep (Sonthalia and Gilbert, 2020), the
optimization-based hyperbolic embeddings in the Poincare ball model (Nickel and Kiela, 2017)
and the Hyperboloid model (Nickel and Kiela, 2018), the simple Euclidean embedding model.
Euclidean, Poincare, Hyperboloid, and our approach UnitBall use the same loss function but learn
in the different geometrical spaces. Therefore, the comparisons reveal the capacities of different
geometrical models in different spaces.
For the baselines, we use their released codes to train the embeddings. For all methods, the hyperpa-
rameters are tuned on each validation set for the link prediction task and on balanced tree-(15,3) for
the graph reconstruction task. The hardware information is given in Appendix F.2 and the hyperpa-
rameters are listed in Appendix F.3. In all experiments, we report the mean results over 5 running
executions. The code of our approach will be publicly available after the publication of the paper.
6
Under review as a conference paper at ICLR 2022
—TreeRep
Poincare —Hyperboloid ♦ UnitBaII
(求)d4w
8 7 6 5 4 3 2
(求)Sl-H
87654321
(求)d4w
t0Ooooo
9 8 7 6 5 4
(求)Sl-H
m(k = 5)	100	200	300	400	500	600	700	800	900	1000	k(m = 500)	123456	7	89	10
Edges	I 478	982	1474	1965	2,468	2976	3,476	3983	4,468	4,970	Edges	∣ 499 998 1,496 1,985 2,468 2,966	3,452	3,939 4,426	4,890
δ-hyperbolicity ∣ 1.0	1.0	1.0	1.0	1.0	1.5	1.5	1.5	1.5	1.5	δ-hyperbolicity	∣ 0.0 2.5 1.5 1.0 1.0 1.0	1.0	1.0 1.0	1.0
-------------------------------------------------------------------------------
Figure 1: Evaluation of graph reconstruction on synthetic compressed graphs in 20-d embedding
spaces (10-d complex hyperbolic space for UnitBall). m represents the number of nodes in the graph
while k represents the number of random trees aggregated to the graph (k controls the denseness and
noise level of the graph). The statistics of the compressed graphs are provided in the tables.
5.1.4	Evaluation
We use the mean average precision (MAP), mean reciprocal rank (MRR), and Hits@N as our
evaluation metrics, which are widely used for evaluating ranking and link prediction. The details of
prediction steps and the evaluation metrics are given in Appendix F.4.
The n-d complex hyperbolic embeddings have around double parameters of the n-d real embeddings
since the n-d complex hyperbolic vectors have n-d real part and n-d imaginary part. For a fair
comparison, in each experimental setting, We compare our n-d complex hyperbolic embeddings of
UnitBall against the 2n-d embeddings of the baselines. The results will also demonstrate that the n-d
complex hyperbolic space is not simply the 2n-d hyperbolic space, they have different capacities.
5.2	Graph Reconstruction
5.2.1	Results on Compressed Graphs
To illustrate the benefits of UnitBall on varying hierarchical structures, we evaluate on the synthetic
compressed graphs. The compressed graphs have local tree structures while being much more
complicated than trees. Each compressed graph-(m, k) consists of m nodes and is aggregated from k
random trees on the m nodes. The bigger k corresponds to the denser and noisier graph.
Figure 1 depicts that the graph reconstruction results drop down with the increase of m and k,
which represents the increase of graph scale and denseness respectively. Remarkably, UnitBall
outperforms all baselines on the challenging data, showing that UnitBall handles the noisy locally
tree-like structures better. The construction-based method TreeRep learns a tree structure from
the data as an intermediate step and then embeds the learned trees into the hyperbolic space using
Sarkar’s construction (Sarkar, 2011). TreeRep has comparable results with other methods when
(m, k) = (500, 1) since when k = 1, the graph is exactly a tree, i.e., δ = 0. However, when k > 1
and δ > 0, the data metrics deviate from tree metrics, in which case it does not help much to learn a
tree structure from the data as an intermediate step.
5.2.2	Results on Multitree S tructure
In this section, we compare the performances of the hyperbolic models and UnitBall on Xiphophorus.
Xiphophorus is a multitree dataset that is formed of 160 mrbayes consensus trees on 26 Xiphophorus
fishes. Its cloud tree plot is in Figure 2. The multitree structure is widely used to represent multiple
overlapping taxonomies over the same ground set (please refer to Appendix E for details). We
reconstruct the edges containing the leaf nodes on Xiphophorus since the leaf links have the practical
taxonomic meaning. The results are reported in the last column of Table 4. The results show that the
complex hyperbolic geometry has a stronger ability to represent the multitree structure.
7
Under review as a conference paper at ICLR 2022
-sdeLτd
=SeUOrS -∩-
ee-。UeuJ。-X
Sn-0。Ui -X
UJnU6-S -X
θ=θu
-Ze-Iee -X
eeuj -X
S-SUe-16-u
Sneu≡=nE -X
SUeUuo。
SneeuJ6>d -X
eeujnzuouj
49l-UUeEq 'x
zHOIΘUJ
->OoenUeZeu 'x
-ZeJJOO -X
lΛΛdsnoeE
-S-IePUe -X
En-P-Ud-X
一」1 .×
θeuθ>θ.x
Snue> -X
SnUe-noo -X
LIeuJ -X
-UOP-IOPX
Figure 2: Left: The cloud tree plot of Xiphophorus. Right: The 1-N structure statistics in YAGO3-
wikiObjects original taxonomy. The horizontal axis represents the number of parents linked to a
node. The vertical axis counts the number of nodes and edges in the 1-N structures. For example,
there are 4, 107 nodes that link to N parents for 2 ≤ N ≤ 5, and these links count to 10, 550 edges.
Table 2: Evaluation of taxonomy link prediction in 32-d embedding spaces (16-d complex hyperbolic
space for UnitBall). The best results are shown in boldface. The second best results are underlined.
	MAP	ICD10 MRR	Hits@3	YAGO3-wikiObjects			WordNet-noun		
				MAP	MRR	Hits@3	MAP	MRR	Hits@3
Euclidean	3.75	3.72	2.39	4.85	4.45	2.78	5.59	5.36	3.16
TreeRep	4.96	7.92	8.49	20.19	21.85	27.19	9.30	9.98	11.90
Poincare	35.24	34.45	52.71	30.06	28.47	41.61	25.46	23.99	27.80
Hyperboloid	34.80	34.01	52.88	30.80	29.21	43.17	25.65	24.15	27.50
UnitBall	47.88	46.96	70.28	33.33	31.85	47.41	27.29	25.93	32.95
5.3	Link Prediction
5.3.1	Results on the Real-World Taxonomies
In this section, we evaluate the performances on the link prediction task for real-world taxonomies.
Table 2 presents the results in 32-d embedding spaces for baselines and 16-d complex hyperbolic
space for UnitBall. Predicting missing links requires generalization capacity, and UnitBall still has
the best performances on all three datasets. Besides, we see that Euclidean shows shortages on these
hierarchically-structured data, which is consistent with the results in previous works (Nickel and
Kiela, 2017; 2018). Similar to the results on the graph reconstruction task, Poincare and Hyperboloid
have very close performances, while Hyperboloid has slightly better results. They have significant
improvements over Euclidean, but they still fall behind UnitBall, which demonstrates our claims
that the non-constant negative curvature of the complex hyperbolic space addresses the varying
hierarchical structures on real-world taxonomies.
We notice that TreeRep does not perform well on the link prediction task. As mentioned in Section
2, the combinatorial construction-based embedding methods target minimizing the reconstruction
distortion of data. However, minimizing the reconstruction distortion may overfit the training set,
thus resulting in the unpromising generalization performance for unobserved edges. Hence, they
are more suitable to learn the representation of graph data without missing links. The evaluation of
TreeRep on the graph reconstruction task can be referred to Section 5.2.1, Appendix F.5 and F.6.
5.3.2	Exploring the Embedding Dimensions
In this section, we explore the performances in different embedding dimensions. The results on
YAGO3-wikiObjects are presented in Table 3. Results on other datasets are in Appendix F.7. We
find that with the increase of the embedding dimension, Euclidean can have big improvements, but
its performances in 128-d still cannot surpass other methods in 8-d. TreeRep also achieves better
results with the increase of dimension, but overall its performances on the link prediction task are
not very promising. By comparison, Poincare, Hyperboloid, and UnitBall achieve great results
steadily. 8-d is already enough for Poincare and Hyperboloid to handle the link prediction task while
8
Under review as a conference paper at ICLR 2022
Table 3: Evaluation of taxonomy link prediction in different embedding dimensions (the embedding
dimension for UnitBall is half of other models). The best results are shown in boldface. The second
best results are underlined.
	8-dimensional			YAGO3-wikiObjects 32-dimensional			128-dimensional		
	MAP	MRR	Hits@3	MAP	MRR	Hits@3	MAP	MRR	Hits@3
Euclidean	1.02	0.92	0.57	4.85	4.45	2.78	16.67	15.76	15.97
TreeRep	16.91	17.48	27.53	20.19	21.85	27.19	21.18	23.44	32.84
Poincare	29.70	28.13	41.64	30.06	28.47	41.61	29.93	28.35	41.53
Hyperboloid	30.87	29.28	43.50	30.80	29.21	43.17	30.68	29.07	42.86
UnitBall	31.40	29.98	44.25	33.33	31.85	47.41	32.76	31.28	46.25
Table 4: Results of Hits@10 on predicting 1-N edges in YAGO3-wikiObjects and results of
MAP on reconstructing the leaf node links in Xiphophorus. The embedding dimension is 16 for
UnitBall and 32 for other models. The best results are shown in boldface.
N for 1-N edges	1	YAGO3-wikiObjects (HitS@10)				>1	Xiphophorus (MAP) Leaf Links Reconstruction
		2〜5	6〜10	11 〜20	> 20		
Poincare	93.10	65.48	60.11	49.09	43.97	63.29	89.75
Hyperboloid	92.80	65.51	63.49	51.93	45.39	63.95	89.80
UnitBall	92.42	76.35	65.91	66.19	70.21	74.03	91.95
UnitBall has small improvements from 4-d to 16-d, then converges to the stable performance. The
results demonstrate that the Euclidean embeddings need to increase the dimension to better model the
hierarchies, while the complex hyperbolic space and the hyperbolic space have strong generalization
competence for hierarchical structures.
5.3.3	Exploring 1-N S tructure
A noteworthy difference between the real-world taxonomies and the tree structures is that the
taxonomies contain many 1-N (1 child links to multiple parents) cases while in a tree each node
except the root is linked to only 1 parent node. To investigate the advantages of complex hyperbolic
geometry on the 1-N structures, We evaluate the performances OfUnitBall, Poincare, and Hyperboloid
on predicting the 1-N edges. We evaluate on YAGO3-wikiObjects since it contains abundant 1-N
structures. The statistics of the 1-N structures in YAGO3-WikiObjects original taxonomy (original
means Without computing the transitive closure) are given in Figure 2.
In this experiment, We train the embeddings on the full transitive closure of YAGO3-WikiObjects and
then predict the 1-N edges. The Hits@10 scores are reported on Table 4. For results of other metrics,
please refer to Appendix F.8. We can see that UnitBall has a very small compromise for the 1-1 edges,
i.e., the edge pattern of tree structures. Furthermore, UnitBall outperforms the hyperbolic models
largely on 1-N structures for N > 1. Even for nodes that link to more than 20 parents, UnitBall can
have accurate top 10 predictions for these edges. The results demonstrate that the complex hyperbolic
embeddings maintain the advantages in the edge pattern of tree structures as Well as handling more
complicated hierarchical structures compared With the real hyperbolic embeddings.
6	Conclusion and Future Work
In this paper, We present a novel approach for learning the embeddings of hierarchical structures in
the unit ball model of the complex hyperbolic space. We characterize the geometrical properties of
the complex hyperbolic space and formulate the embedding algorithm in the unit ball model. We
exemplify the superiority of our approach over the graph reconstruction task and the link prediction
task on both synthetic and real-World data, Which cover the various hierarchical structures and tWo
specific structures, namely multitree structure and 1-N structure. The empirical results shoW that
our approach outperforms the hyperbolic embedding methods in terms of representation capacity
and generalization performance. Motivated by our theoretical grounding and empirical success, We
believe the complex hyperbolic embeddings Will have promising improvements on the knoWledge
graph embeddings, neural netWorks, and other related applications.
9
Under review as a conference paper at ICLR 2022
Reproducibility S tatement
•	Please see Section 3 and 4 for the full set of assumptions of all theoretical results. Please
see Appendix B and C for complete proofs of all theoretical results.
•	Please see Section 5.1 and Appendix F.2, F.3 for the experimental details (e.g., data splits,
hyperparameters, hardware, etc).
•	The data used in the paper are publicly available. We cite the corresponding references and
give the public data links in Appendix F.1.
•	The code is proprietary for this moment. The code will be released after the the publication
of the paper.
References
I.	Balazevic, C. Allen, and T. M. HosPedales. Multi-relational Poincare graph embeddings. In
NeurIPS, pages 4465-4475, 2019.
A. F. Beardon. The geometry of discrete groups, volume 91. SPringer Science & Business Media,
2012.
G. Becigneul and O. Ganea. Riemannian adaptive optimization methods. In ICLR (Poster). OpenRe-
view.net, 2019.
S. Bonnabel. Stochastic gradient descent on riemannian manifolds. IEEE Trans. Autom. Control., 58
(9):2217-2229, 2013.
A. Bordes, N. Usunier, A. Garcla-Duran, J. Weston, and O. Yakhnenko. Translating embeddings for
modeling multi-relational data. In NIPS, pages 2787-2795, 2013.
G. R. Bramer. International statistical classification of diseases and related health problems. tenth
revision. World health statistics quarterly. Rapport trimestriel de statistiques sanitaires mondiales,
41(1):32-36, 1988.
I.	Chami, Z. Ying, C. Re, and J. Leskovec. Hyperbolic graph convolutional neural networks. In
NeurIPS, pages 4869-4880, 2019.
I.	Chami, A. Wolf, D. Juan, F. Sala, S. Ravi, and C. Re. Low-dimensional hyperbolic knowledge
graph embeddings. In ACL, pages 6901-6914. Association for Computational Linguistics, 2020.
H. Cho, B. Demeo, J. Peng, and B. Berger. Large-margin classification in hyperbolic space. In
AISTATS, volume 89 of Proceedings of Machine Learning Research, pages 1832-1840. PMLR,
2019.
R. Cui, M. Schumer, K. Kruesi, R. Walter, P. Andolfatto, and G. Rosenthal. Data from: phylogenomics
reveals extensive reticulate evolution in xiphophorus fishes. Dryad Data Repository, 2013.
J.	Dai, Y. Wu, Z. Gao, and Y. Jia. A hyperbolic-to-hyperbolic graph convolutional network. In CVPR,
2021a.
S. Dai, Z. Gan, Y. Cheng, C. Tao, L. Carin, and J. Liu. Apo-vae: Text generation in hyperbolic space.
In NAACL-HLT, pages 416-431. Association for Computational Linguistics, 2021b.
N.	Fisher. Notes on curvature in complex hyperbolic space. URL https://sites.tufts.
edu/natefisher/files/2020/11/Write-up.pdf.
R. Fu, J. Guo, B. Qin, W. Che, H. Wang, and T. Liu. Learning semantic hierarchies via word
embeddings. In ACL (1), pages 1199-1209. The Association for Computer Linguistics, 2014.
O.	Ganea, G. Becigneul, and T. Hofmann. Hyperbolic entailment cones for learning hierarchical
embeddings. In ICML, volume 80 of Proceedings of Machine Learning Research, pages 1632-1641.
PMLR, 2018a.
O.	Ganea, G. Becigneul, and T. Hofmann. Hyperbolic neural networks. In NeurIPS, pages 5350-5360,
2018b.
10
Under review as a conference paper at ICLR 2022
W. M. Goldman. Complex hyperbolic geometry. Oxford University Press, 1999.
J. R. Griggs, W. Li, andL. Lu. Diamond-free families. J. Comb. Theory, Ser. A, 119(2):310-322,
2012.
M. Gromov. Hyperbolic groups. In Essays in group theory, pages 75-263. Springer, 1987.
A. Gu, F. Sala, B. Gunel, and C. Re. Learning mixed-curvature representations in product spaces. In
ICLR (Poster). OpenReview.net, 2019.
C. GUIcehre, M. Denil, M. Malinowski, A. Razavi, R. Pascanu, K. M. Hermann, P W. Battaglia,
V. Bapst, D. Raposo, A. Santoro, and N. de Freitas. Hyperbolic attention networks. In ICLR
(Poster). OpenReview.net, 2019.
A. A. Hagberg, D. A. Schult, and P. J. Swart. Exploring network structure, dynamics, and function
using networkx. In G. Varoquaux, T. Vaught, and J. Millman, editors, Proceedings of the 7th
Python in Science Conference, pages 11 - 15, Pasadena, CA USA, 2008.
S. Kobayashi and K. Nomizu. Foundations of differential geometry, volume 1. New York, London,
1963.
D. Krioukov, F. Papadopoulos, M. Kitsak, A. Vahdat, and M. Boguna. Hyperbolic geometry of
complex networks. Phys. Rev. E, 82:036106, Sep 2010. doi: 10.1103/PhysRevE.82.036106. URL
https://link.aps.org/doi/10.1103/PhysRevE.82.036106.
Q. Liu, M. Nickel, and D. Kiela. Hyperbolic graph neural networks. In NeurIPS, pages 8228-8239,
2019.
F.	Mahdisoltani, J. Biega, and F. M. Suchanek. YAGO3: A knowledge base from multilingual
wikipedias. In CIDR. www.cidrdb.org, 2015.
G.	A. Miller. Wordnet: A lexical database for english. Commun. ACM, 38(11):39-41, 1995.
N. Mok. Metric rigidity theorems on Hermitian locally symmetric manifolds, volume 6. World
Scientific, 1989.
M. Nickel and D. Kiela. POinCare embeddings for learning hierarchical representations. In NIPS,
pages 6338-6347, 2017.
M. Nickel and D. Kiela. Learning continuous hierarchies in the lorentz model of hyperbolic geometry.
In ICML, volume 80 of Proceedings of Machine Learning Research, pages 3776-3785. PMLR,
2018.
M. Nickel, V. Tresp, and H. Kriegel. A three-way model for collective learning on multi-relational
data. In ICML, pages 809-816. Omnipress, 2011.
J. G. Ratcliffe, S. Axler, and K. Ribet. Foundations of hyperbolic manifolds, volume 149. Springer,
1994.
F. Sala, C. D. Sa, A. Gu, and C. Re. Representation tradeoffs for hyperbolic embeddings. In ICML,
volume 80 of Proceedings of Machine Learning Research, pages 4457-4466. PMLR, 2018.
R. Sarkar. Low distortion delaunay embedding of trees in hyperbolic plane. In Graph Drawing,
volume 7034 of Lecture Notes in Computer Science, pages 355-366. Springer, 2011.
R.	Shimizu, Y. Mukuta, and T. Harada. Hyperbolic neural networks++. In ICLR (Poster), 2021.
V.	Shwartz, Y. Goldberg, and I. Dagan. Improving hypernymy detection with an integrated path-based
and distributional method. In ACL (1). The Association for Computer Linguistics, 2016.
O. Skopek, O. Ganea, and G. BeCigneUL Mixed-curvature variational autoencoders. In ICLR.
OpenReview.net, 2020.
R. Sonthalia and A. C. Gilbert. Tree! I am no tree! I am a low dimensional hyperbolic embedding. In
NeurIPS, 2020.
11
Under review as a conference paper at ICLR 2022
F. M. Suchanek, G. Kasneci, and G. Weikum. Yago: a core of semantic knowledge. In WWW, pages
697-706. ACM, 2007.
Z. Sun, Z. Deng, J. Nie, and J. Tang. Rotate: Knowledge graph embedding by relational rotation in
complex space. In ICLR (Poster). OpenReview.net, 2019.
Z. Sun, M. Chen, W. Hu, C. Wang, J. Dai, and W. Zhang. Knowledge association with hyperbolic
knowledge graph embeddings. In EMNLP (1), pages 5704-5716. Association for Computational
Linguistics, 2020.
T. Trouillon, J. WelbL S. Riedel, E´. Gaussier, and G. Bouchard. Complex embeddings for simple
link prediction. In ICML, volume 48 of JMLR Workshop and Conference Proceedings, pages
2071-2080. JMLR.org, 2016.
W.	Wirtinger. Zur formalen theorie der funktionen von mehr komplexen VeranderIiChen. Mathematis-
che Annalen, 97(1):357-375, 1927.
B. Yang, W. Yih, X. He, J. Gao, and L. Deng. Embedding entities and relations for learning and
inference in knowledge bases. In ICLR (Poster), 2015.
S.	Zhu, S. Pan, C. Zhou, J. Wu, Y. Cao, and B. Wang. Graph geometry interaction learning. In
NeurIPS, 2020.
12
Under review as a conference paper at ICLR 2022
A More Preliminaries
A. 1 Definition of Curvature
The curvature describes the curve of Riemannian manifolds and controls the rate of geodesic deviation.
In this paper, curvature refers to the sectional curvature.
Definition 1 (Curvature). Given a Riemannian manifold and two linearly independent tangent vectors
at the same point, u and v, the (sectional) curvature is defined as
K(u, v) =
hR(u, v)v, Ui
hu uihv, Vi - hu, Vi
where R is the Riemann curvature tensor, defined by the convention R(u, v)w = RCvw -
▽v VuW -V[u,v]w. Here V indicates the LeVi-CiVita connection, whose definitions are given below.
Before defining Levi-Civita connection, we need to first define the affine connection.
Definition 2 (Affine connection). Let M be a smooth manifold and let Γ(TM )be the space of vector
fields on M, i.e., the space of smooth sections of the tangent bundle TM. Then an affine connection
on M is a bilinear map
Γ(TM )×Γ(TM )→Γ(TM )
(X,Y )7→VXY,
such that for all f in the set of smooth functions on M, written C∞ (M, R), and all vector fields
X, Y on M:
1.	VfXY = fVXY, i.e., V is C∞ (M, R)-linear in the first variable;
2.	VX(fY )= ∂XfY + fVXY, where ∂X denotes the directional derivative, i.e., V satisfies
Leibniz rule in the second variable.
Next, we define the Levi-Civita connection.
Definition 3 (Levi-Civita connection). An affine connection V is called a Levi-Civita connection if
1.	it preserves the metric, i.e., Vg = 0.
2.	it is torsion-free, i.e., for any vector fields X and Y we have VXY - VYX = [X, Y],
where [X, Y] is the Lie bracket of the vector fields X and Y.
A.2 The Distance Function on the Unit Ball Model
The distance function in Eq. (9) maintains the tree-like metric properties. When the points are very
close to the origin, it approximates to the Euclidean distance. Additionally, when a point is closer to
the origin, it has relatively smaller distances to the other points. Correspondingly, the points near the
boundary have very large distances from each other. Therefore, in ideal conditions, the root node of a
tree is embedded in the origin while the deeper nodes are embedded farther away from the origin.
Recall that the distance function in the real hyperbolic space (Nickel and Kiela, 2017) has similar
properties since the real hyperbolic space, as the totally geodesic subspace of the complex hyperbolic
space, inherits the tree-like metrics. More details about the Bergman metric and distance function
can be referred to Chapter 3.1 in (Goldman, 1999).
B	Proof of Theorem 1
In Section 3.2 in the paper, we presented Theorem 1 about the curvature of the complex hyperbolic
space HCn (Goldman, 1999). The sketch explanation is that all unit tangent vectors are equivalent, but
not all directions are spanned by two unit tangent vectors. Before proving Theorem 1, we need to
introduce the definition of KahIer structure (Mok, 1989).
Definition 4 (Kahler structure). A Kahler structure can be defined in any Ofthefollowing equivalent
ways:
13
Under review as a conference paper at ICLR 2022
1.	A complex structure with a closed, positive (1, 1)-form.
2.	A Riemannian structure with a complex structure such that the corresponding exterior
2-form is closed.
3.	A symplectic structure with a compatible integrable almost complex structure which is
positive.
Recall that in Section 3.2, we defined the complex hyperbolic space HnC using the projectivization of
the negative zone with a Hermitian form hhz, wii. Denote ω as the imaginary part of the Hermitian
form hh, ii, i.e., ω(z, W)= *(《z, w》-《w, z》), then according to (Goldman, 1999), the metric ω
is positive and closed, and necesssarily has type (1, 1). Then by the first definition in Definition 4,
Hn is a Kahler structure.
Let M be a Kahler manifold and Z ∈ M. Denote TzM as the tangent space of M at Z and
J : TM → TM is an endomorphism. As proved in (Kobayashi and Nomizu, 1963), the curvature
of real 2-planes in the tangent space Tz M has the following properties:
Theorem 2. Let M be a connected Kahler manifold ofcomplex dimension n ≥ 2. Ifthe holomorphic
sectional curvature K (p), where p is a plane in TzM invariant by J, depends only on z, then M is a
space of constant holomorphic sectional curvature.
Next, we give a proposition in (Kobayashi and Nomizu, 1963), which is about the curvature of a
plane.
Proposition 1. If u, v is an orthonormal basis for a plane p and if we set the curvature of p as
K(p) = R(u, v), where R(u, v) is the Riemann curvature tensor, then
K(p) = 1(1 + 3cos2 α(p)),
where α(p) is the angle between p and J (p).
Finally, we prove Theorem 1 as follows.
Proof. Let M be a Kahler manifold and Z ∈ M. From Theorem 2, the corresponding sectional
curvature function of real 2-planes in Tz M is completely determined by the sectional curvature
function restricted to complex lines in Tz M . If the sectional curvature of every complex line in TM
equals κ, then M is said to have constant holomorphic sectional curvature κ.
Then from Proposition 1, we can know that in this case, the sectional curvature of a 2-dimensional
subspace S ⊂ TM is
1+3 1 + 3cos2 α(S)
K (S) = K--------4------
(17)
where α(S) is the angle of holomorphy, defined as the smallest angle between two nonzero vectors
from two linear subspaces of the underlying real vector space of M .
In particular, the complex hyperbolic space HC is a Kahler structure with K = -1. Since 0 ≤
cos2 α(S) ≤ 1, then from Eq. (17), we can have -1 ≤ K(S) ≤ -1/4 for any 2-dimensional
subspace S ⊂ TM of HCn, i.e., the (sectional) curvature is not constant in HnC, but pinched between
-1 and -1/4. Thus we proved the non-constant curvature of HnC.
Specifically, we discuss the complex projective lines and totally real planes in the unit ball model of
the complex hyperbolic space:
BC = {(z1,…,zn, I)IIz1|2 + …+ |zn|2 < 1}.
(18)
First let’s consider the case of complex projective lines. Consider a complex line L in Cn that
intersects the unit ball model BCn. Let Z be any point in L ∩ BCn. We can apply an element of PU(n, 1)
to L so that it becomes the last coordinate axis {(0, zn)Izn ∈ C}, whose intersection with BCn is the
disk ∣zn∣ < 1. Then the restriction of the Bergman metric to this disc is the POinCare metric (Beardon,
2012) of constant curvature -1.
14
Under review as a conference paper at ICLR 2022
In order to see this, let z = (0, zn , 1) and w = (0, wn , 1), z, w ∈ L ∩ BCn, then from Eq. (9) in
Section 4.1, the distance between z and w is given by Eq. (9). Then we have
2 dBCn (z, w)
cosh (———)
《z, w》《w,z》=	∣ZnWn - 1|2
《Z,Z》《W, W = (∣Zn∣2- 1)(∣Wn∣2- 1))
(19)
which is just the Poincare metric (Beardon, 2012).
Next consider a totally real plane p. Any totally real plane p is the image under an element of
PU(n, 1) of the subspace comprising those points of BCn with real coordinates, that is actually an
embedded copy of the real hyperbolic space HnR = {(x1, . . . , xn)|x1, . . . , xn ∈ R}. This subspace
intersects BCn in the subset consisting of those points with x21 + —+ Xn < 1. Then the Bergman
metric restricted to this real-space unit ball is just the Klein-Beltrami metric (Ratcliffe et al., 1994) on
the unit ball in Rn with constant curvature -1/4.
To see this, let x = (x1, . . . , xn, 1) and y = (y1, . . . , yn, 1), x, y ∈ HnR ∩ BCn, then apply the similar
process with the above, we have
cosh2 (dBn (x，y))
《x, y》《y, X =	(χιyι +----+ Xnyn - 1)2
hχ,χiihy,yii	(χ2 + …+ Xn - 1)(y2 + …+ yn- 1)，
(20)
which is the Klein-Beltrami metric (Ratcliffe et al., 1994) on the unit ball in Rn with constant
curvature -1/4.
Therefore, we proved that the curvature of HCn is -1 in the directions of complex projective lines
while - 1/4 in the directions of totally real planes.	□
Remark. Curvature in the complex hyperbolic space is a very complicated topic in geometric group
theory and differential geometry. The complex projective lines and the totally real planes are two
kinds of special subspaces, whose curvatures are presented above. For the subspace that lives in
between the two special cases, its curvature can be computed accordingly. Since digging into all
the curvature details of the complex hyperbolic geometry is not the essential part of our work, we
have omitted the related content in our paper. We refer the interested readers to (Fisher) for an
interesting example in the complex hyperbolic space (its curvature differs with our work with a
constant multiplier 4). In Section 5 and Figure 5 of (Fisher), the author explored the curvature of a
triangle in complex hyperbolic geometry with numerical computation.
C Derivation of Distance Gradient in the Unit Ball Model
The distance function in the unit ball model is given by Eq. (9). We need to compute the distance
gradient Ve⅛n (z, w) = ddB∂(z'w) + i 已%；W) during the Riemannian optimization. The full
derivation is as follows.
First, we need to introduce Wirtinger derivatives (Wirtinger, 1927), which constructs a differential
calculus for differential functions on complex domains.
Definition 5 (Wirtinger derivatives). The partial derivatives of a (complex) function f(z) of a
complex variable Z = X + iy ∈ C,x,y ∈ R, with respect to Z and W = X — iy, respectively, are
defined as:
d⅛M( ∂X W )f(z,z)
『 = ；( 4 + 4 )f (z,Z)
∂ ZW	2 ∂ X	∂ y
The Wirtinger derivatives can be rewritten as:
∂f(z,Z)
∂x
∂f(z,Z)
∂y
(ɪ + ɪ )f(z,Z),
∂Z ∂ZW
∂∂
i(∂Z - ∂W)f (z,z),
(21)
(22)
15
Under review as a conference paper at ICLR 2022
Let P=CoshmBn(Z，W)) =2 hhzwhhwzi
Let z = (z1, . . . , zn, 1) ∈ BCn, then
- 1, then dBn (Z， W) = arcosh(P) = ln(P + P2 - 1).
∂dBn (z, W) _ ∂dBn (z, W) ∂p _	1 ∂p
∂zj	dp	∂zj	,p2 - 1 ∂zj
∂ (zιWΓH-+znwn-1)∙《w,z》
2	(z1Z1+ +zn zn-1).《w,w》
=-------------------------------
pP2 - 1	dzj
=	2	∙ (	Wj《w,z》	-Zj《z, w》∙《w,z》1
一Pp2 - 1,《z,z》∙《w, w》	《z, Zi ∙《w, w》卜
for 1 ≤ j ≤ n. Similarly, we can have
ddBn (Z, W) = , 2	∙ (	Wj《Z, W》	-Zj《z, w》∙《w,z》λ
dzj	- PP - 1,《z, z》∙《w, w》	《z, Zi ∙《w, w》J
(23)
(24)
Then by Eqs. (21), (23), and (24), we obtain
ddBn
dxj
ddBn (z, w) ddBn (z, w)
dzj +	dzj
,4	( Re(《z, wii Wj) -《z, w》《w,z》Xj A	(25)
PP - 1,《z, z》《w, w》	《z, z》2《w, w》J '
Similarly, by Eqs. (22), (23), and (24), we can get
ddBn
dyj
ddBCn(z,w)
=i( -^z-
ddBCn (z, w)
-^i^)
,4	(Im(《z, wiiWj)	《Z, w》《w,z》yj1 (26)
PP - 1,《z, zii《w, w》	《z, Zihw, w》J,
—
where Re(∙) and Im(∙) denote the real and the imaginary part respectively. Then We can have
ddBn =	4	( Re(《z, w》W)-《z, w》《w,z》X
dx — Pp2 - 1 l《z,z》《w, w》 《z, z》2《w, w》.
ddBn =	4	(Im(《z, w》W)-《z, w》《w,z》y
dy - Pp2 -1 l《z,z》《w, w》	《z,z》2《w, w》
which are Eqs. (13) and (14) in Section 4.3.
D Definition of δ-HYPERBOLICITY
In this section, we give the definition of δ-hyperbolicity (Gromov, 1987), which measures the tree-
likeness of graphs. The lower δ corresponds to the more tree-like graph. Trees have 0 δ-hyperbolicity.
Definition 6 (δ-hyperbolicity). Let a, b, c, d be vertices of the graph G. Let S1, S2 and S3 be
S1= dist(a, b) + dist(d, c), S2 = dist(a, c) + dist(b, d), S3 = dist(a, d) + dist(b, c).
Suppose M1	and M2	are the two largest values among	S1,	S2,	S3	and	M1	≥	M2.	Define
hyp(a, b, c, d) = M1- M2. Then the δ-hyperbolicity of G is defined as
δ(G) = — max	hyp(a, b, c, d).
2 a,b,c,d∈V (G)
That is, δ(G) is the maximum of hyp over all possible 4-tuples (a, b, c, d) divided by 2.
E	Multitree Structure
In combinatorics and order-theoretic mathematics, a multitree structure is a directed acyclic graph
(DAG) in which the set of vertices reachable from any vertex induces a tree, or a partially ordered set
(poset) that does not have four items a, b, c, and d forming a diamond suborder with a ≤ b ≤ d and
16
Under review as a conference paper at ICLR 2022
Priapella
Psjonesii
Xmaculatus_Jf
Xandersi
Xxiphidium
Xmilleri
Xevelynae
Xvariatus
Xcouchianus
Xgordoni
Xmeyeri
XclemenciaeJ
Xmonticolus
Xsignum
Xhellerii
Xalvarezi
Xmayae
Xcontinens
Xpygmaeus
Xmultilineatus
Xnigrensis
Xmontezumae
Xnezahuacoyo
Xcortezi
Xbirchmanni_G
Xmalinche CH
Priapella
Psjonesii
Xalvarezi
Xhellerii
Xmayae
Xsignum
Xmonticolus
XclemenciaeJ
XmaculatusJ
Xevelynae
Xmilleri
Xandersi
Xvariatus
Xxiphidium
Xcouchianus
Xgordoni
Xmeyeri
Xcontinens
Xpygmaeus
Xmultilineatus
Xnigrensis
Xmontezumae
XmalinChe_CH
Xbirchmanni_G
Xcortezi
Xnezahuacoyo
Γ⅛E 用Ire
æ -⅛l
Priapella
Psjonesii
Xhellerii
XsignUm
XalvareZi
Xmayae
XContinens
Xpygmaeus
Xmultilineatus
Xnigrensis
Xmontezumae
Xnezahuacoyc
Xcortezi
XbirChmannLG
XmalinChe_CH
Xmonticolus
XClemenCiae_(
XmaculatusJ
Xmilleri
Xevelynae
Xvariatus
Xandersi
Xxiphidium
Xcouchianus
Xgordoni
Xmeyeri
Priapella
Psjonesii
Xmaculatus_Jf
Xandersi
Xmilleri
Xevelynae
Xvariatus
Xcouchianus
Xgordoni
Xmeyeri
Xxiphidium
Xmonticolus
XclemenciaeJ
Xhellerii
Xsignum
Xalvarezi
Xmayae
Xcontinens
Xpygmaeus
Xmultilineatus
Xnigrensis
Xmontezumae
Xbirchmanni_G
XmalinChe_CH
Xcortezi
Xnezahuacoyo

IJRJ-⅛⅛÷< 走 ,
Figure 3:	Some subtrees of Xiphophorus. The multitree dataset XiPhoPhorUs contains 160 trees on
26 Xiphophorus fishes as leaf nodes.
balanced tree: r=2, h=2
δ-hyperbolicity=0
compressed graph: m=7, k=2
δ-hyperbolicity=0.5
compressed graph: m=7, k=1 compressed graph: m=7, k=1
δ-hyperbolicity=0
δ-hyperbolicity=0
Figure 4:	Simple examples of the synthetic data. The numbers {0,1,..., 6} represent the nodes. The
compressed graph-(m = 7, k = 2) on the right are aggregated from the middle two compressed
graphs-(m = 7, k = 1).
a ≤ c ≤ d but with b and c incomparable to each other (also called a diamond-free poset (Griggs
et al., 2012)).4
Obviously, the multitree structure is not a tree since one child node can have multiple parents in
multitree. Note that the multitree structure is also different with 1-N structure since the multitree has
more strict conditions, that is, the multitree is a diamond-free poset. By comparison, the 1-N structure
is more general in taxonomies. For example, the subgraph of YAGO3-wikiObjects {(Nei Gaiman,
is-a, British screenwriters), (Neil Gaiman, is-a, British fantasy writers), (British screenwriters, is-a,
British writers), (British fantasy writers, is-a, British writers)} is a 1-N structure, but it is not a
multitree structure, because British screenwriters and British fantasy writers are incomparable to
each other, i.e., there is no partial order between them.
Recall that our synthetic compressed graph is also aggregated from multiple trees, but it is not the
multitree either. A compressed graph is aggregated from multiple random trees on the same set of
nodes while the trees in a multitree structure share the same leaf nodes. Multitrees are widely used to
represent multiple overlapping taxonomies over the same ground set. The dataset Xiphophorus is a
multitree structure containing 160 trees on 26 leaf nodes. Some examples of its subtrees are in Figure
3 and its cloud tree plot is in Figure 2.
F	More Experiments
F.1 Data
In Section 5.1.1 in the paper, we introduced how we generate the synthetic data:
4Here ≤ denotes the partial order defined in the graph, e.g., the hypernymy relation.
17
Under review as a conference paper at ICLR 2022
Table 5: Hyperparameters of all methods.
Model	Synthetic & XiPhOPhOruS	ICD10		YAGO3-wikiObjects		WordNet-noun
TreeRep	iterations: 20; optimization: no opt; pre-allocation fraction: 2.0; nthreads: 16; terminated edge weight: 0; trials/dataset: 3	iterations: 32; optimization: no opt; pre-allocation fraction: 1.3; nthreads: 16; terminated edge weight: 0; trials/dataset: 3	iterations: 32; optimization: no opt; pre-allocation fraction: 1.3; nthreads: 16; terminated edge weight: 0; trials/dataset: 3	iterations: 1; optimization: no opt; pre-allocation fraction: 1.3; nthreads: 16; terminated edge weight: 0; trials/dataset: 3
Euclidean	-	manifold: euclidean; learning rate: 1; epochs: 1500; dampening: 0.75; burnin: 20; burnin multiplier: 0.01; negative sample: 50; negative multiplier: 0.1; max norm: 50000	manifold: euclidean; learning rate: 1; epochs: 1200; dampening: 0.75; burnin: 20; burninmultiplier: 0.01; negative sample: 50; negative multiplier: 0.1; max norm: 50000	manifold: euclidean; learning rate: 1; epochs: 1500; dampening: 0.75; burnin: 20; burnin multiplier: 0.01; negative sample: 50; negative multiplier: 0.1; max norm: 50000
POinCare	manifold: Poincare; learning rate: 0.3; epochs: 1500; dampening: 0.75; burnin: 20; burnin multiplier: 0.01; negative sample: 50; negative multiplier: 0.1; max norm: 1 — e-5	manifold: Poincare; learning rate: 1; epochs: 1500; dampening: 1.0; burnin: 20; burnin multiplier: 0.01; negative sample: 50; negative multiplier: 0.1; max norm: 1 — e-5	manifold: poincare; learning rate: 1; epochs: 1200; dampening: 1.0; burnin: 20; burninmultiplier: 0.01; negative sample: 50; negative multiplier: 0.1; max norm: 1 — e-5	manifold: POincare; learning rate: 1; epochs: 1500; dampening: 1.0; burnin: 20; burnin multiplier: 0.01; negative sample: 50; negative multiplier: 0.1; max norm: 1 — e-5
Hyperboloid	manifold: lorentz; learning rate: 0.3; epochs: 1500; dampening: 0.75; burnin: 20; burnin multiplier: 0.01; negative sample: 50; negative multiplier: 0.1; max norm: no-maxnorm	manifold: lorentz; learning rate: 0.5; epochs: 1500; dampening: 1.0; burnin: 20; burnin multiplier: 0.01; negative sample: 50; negative multiplier: 0.1; max norm: no-maxnorm	manifold: lorentz; learning rate: 1; epochs: 1200; dampening: 1.0; burnin: 20; burninmultiplier: 0.01; negative sample: 50; negative multiplier: 0.1; max norm: no-maxnorm	manifold: lorentz; learning rate: 0.5; epochs: 1500; dampening: 1.0; burnin: 20; burnin multiplier: 0.01; negative sample: 50; negative multiplier: 0.1; max norm: no-maxnorm
UnitBall	manifold: unitball; learning rate: 8; epochs: 1500; dampening: 0.75; burnin: 20; burnin multiplier: 0.01; negative sample: 50; negative multiplier: 0.1; max norm: 1 — e-5	manifold: unitball; learning rate: 11; epochs: 200; dampening: 1.0; burnin: 20; burnin multiplier: 0.01; negative sample: 50; negative multiplier: 0.1; max norm: 1 — e-5	manifold: unitball; learning rate: 14; epochs: 1200; dampening: 1.0; burnin: 20; burninmultiplier: 0.01; negative sample: 50; negative multiplier: 0.1; max norm: 1 — e-5	manifold: unitball; learning rate: 12; epochs: 900; dampening: 1.0; burnin: 20; burnin multiplier: 0.01; negative sample: 50; negative multiplier: 0.1; max norm: 1 — e-5
Synthetic. We generate various balanced trees and compressed graphs using the NetworkX pack-
age (Hagberg et al., 2008).5 6 For balanced trees, we generate the balanced tree with degree r and
depth h. For compressed graphs, we generate k random trees on m nodes and then aggregate their
edges to form a graph.
We give some examples of the synthetic data in Figure 4. As we can see, the compressed graphs-
(m = 7, k = 1) are random trees on 7 nodes, so their δ-hyperbolicities are 0. The compressed
graph-(m = 7, k = 2) is no longer a tree after aggregating from two trees. Its local structures are
more varying and complicated.
Here we also give the public links of the real-world data used in our experiments: Xiphophorus (Cui
et al., 2013),6 ICD10 (Bramer,1988),7 8 YAGO3-wikiObjects (Mahdisoltani et al., 2015),8 WordNet-
noun (Miller, 1995).9
F.2 Hardware
We conduct all the experiments except TreeRep on four NVIDIA GTX 1080Ti GPUs with 8GB
memory each. For TreeRep, we need more memory to store the distance matrices, so we use a 96-core
NVIDIA T4 GPU server with 503GB memory.
5https://networkx.org/documentation/stable/reference/generators.html.
6https://toytree.readthedocs.io/en/latest/7-multitrees.html.
7https://www.who.int/standards/classifications/classification-of-diseases.
8https://yago-knowledge.org/.
9https://wordnet.princeton.edu/.
18
Under review as a conference paper at ICLR 2022
F.3 Hyperparameters
For the baselines (TreeReP (Sonthalia and Gilbert, 2020),10 Euclidean, Poincare (Nickel and Kiela,
2017), and Hyperboloid (Nickel and Kiela, 2018)),10 11 we use their public codes to train the embeddings.
For all methods, we tune the hyPerParameters by grid search. For the graPh reconstruction task,
we tune the hyPerParameters on balanced tree-(15,3) in 20-dimensional embedding sPaces (10-
dimensional comPlex hyPerbolic sPace for UnitBall), while for the link Prediction task, we tune
the hyPerParameters on the validation sets in 32-dimensional embedding sPaces (16-dimensional
comPlex hyPerbolic sPace for UnitBall). The hyPerParameters are given in Table 5.
F.4 Evaluation
Our evaluation closely follows the setting of (Nickel and Kiela, 2017; 2018), which infers the
hierarchies from distances in the embedding sPace. SPecifically, for each test edge (z, w), we
comPute the distance between the embeddings dBn (z, w) and rank it among the distances of all
unobserved edges for z: {dBn (z, w0) : (z, w0) ∈/ Training}. We then rePort the following evaluation
metrics of the rankings. Denote Etest as the test edge set and V = {z∣∃w, (z, W) ∈ Etest} as the
test node set. Let NEz = {w1, w2, . . . , w|NEz| } be the ground truth neighbor set of node z.
Mean average precision (MAP). The average Precision (AP) is a way to summarize the Precision-
recall curve into a single value rePresenting the average of all Precisions and the MAP score is
calculated by taking the mean AP over all classes. For a node z, from the learned embeddings, we
can obtain the nodes closest to its embedding z. Let Rz,wi be the smallest set of such nodes that
contains wi (the i-th neighbor of z). Then the MAP is defined as:
1
map = IVI X
z∈V
INE |	X PTecisiOn(Rz,Wi)
z wi∈NEz
Mean reciprocal rank (MRR). The MRR is a statistic measure for evaluating a list of Possible
resPonses to a samPle of queries, ordered by the Probability of correctness. For a node z, from the
learned embeddings, we can rank its distances with other nodes from the smallest to the largest. Let
rankwi be the rank of wi (the i-th neighbor of z). Then the MRR is defined as:
1
1
1
MRR
IVI z∈V INEzI
Σ
wi∈NEz
rankwi
The proportion of correct types that rank no larger than N (Hits@N). Hits@N measures
whether the toP N Predictions contain the ground truth labels. For a node z, from the learned
embeddings, we can obtain the set of N nodes closest to its embedding z, denoted as RzN . Then the
Hits@N is defined as:
HitS@N = ɪ X I(∣RN ∩ NEz∣ ≥ 1),
z∈V
where I(IRzN ∩ NEzI ≥ 1) is the indicator function.
F.5 Graph Reconstruction Results on Balanced Trees
To comPare the rePresentation caPacities of UnitBall and the hyPerbolic embedding models for the
tree structures, we evaluate the graPh reconstruction task on the synthetic balanced trees. A balanced
tree-(r, h) has degree r and depth h, so it has r0 +-+ rd nodes and r0 +-+ rd - 1 edges. The
δ-hyPerbolicity of any balanced tree is 0. We embed the balanced trees into 20-d hyPerbolic sPace
for the baselines and 10-d complex hyperbolic space for UnitBall.
Figure 5 presents the MAP and Hits@3 scores with varying r and h. We see that when the tree
is in small scale, e.g., (r, h) = (15, 3), (10, 2), (10, 3), all methods have very good performances,
10https://github.com/rsonthal/TreeRep.
11https://github.com/facebookresearch/poincare-embeddings. The repository pro-
vides the implementation for Euclidean, Poincare, and Hyperboloid.
19
Under review as a conference paper at ICLR 2022
100
80-
60-
40-
TreeRep
mmw
(求)m@s~H
(求)d<w
MAP (on h=3)	Hits@3 (on h=3)
Figure 5: Evaluation of graph reconstruction on synthetic balanced trees in 20-d embedding spaces
(10-d complex hyperbolic space for UnitBall). r represents the degree while h represents the depth.
Poincare Hyperboloid UnitBaII
6 Y √
MAP (on r=10)
mmw
(求)m@s~H
6 1 √
Hits@3 (on r=10)
Table 6: Evaluation of graph reconstruction on the real-world taxonomies (the dimension is 32 for
TreeRep and 16 for UnitBall). For memory cost, the unit is GiB.
	ICD10			YAGO3-wikiObjects			WordNet-noun		
	MRR	Hits@1	Memory	MRR	Hits@1	Memory	MRR	Hits@1	Memory
TreeRep	26.74	91.97	30	36.71	95.39	21	16.99	90.51	226
UnitBall	47.47	98.93	0.005	39.65	96.10	0.005	28.88	94.95	0.02
demonstrating the expected powerful capacities of hyperbolic geometry and complex hyperbolic
geometry on tree structures. However, when the breadth or the depth increases, the performances of
Poincare and Hyperboloid drop rapidly, suggesting that the optimization-based embeddings in HR0
are not effective enough for reconstructing trees of such scales.
In comparison, UnitBall and TreeRep achieve stable performances for larger trees. TreeRep learns
a tree structure from the data as an intermediate step and then embeds the learned trees into the
hyperbolic space using Sarkar’s construction (Sarkar, 2011). When the input data is a tree, TreeRep
exactly recovers the original tree structure. Figure 5 shows that UnitBall achieves comparable or even
better performances than TreeRep on the balanced trees. The results demonstrate that UnitBall does
not compromise on trees. It produces high-quality embeddings for tree structures.
F.6 Comparison with TreeRep on Real-World Taxonomy Reconstruction
In this section, we compare UnitBall with TreeRep on the real-world taxonomy reconstruction task.
The results are presented in Table 6. As we analyzed in Section 5.3.1, TreeRep, as a combinatorial
construction-based embedding method, is more suitable for the graph reconstruction task. Its
performance is much better than that on the link prediction task. In addition, UnitBall still outperforms
TreeRep on reconstructing real-world taxonomies.
We also notice the memory issues of the combinatorial construction-based embedding methods.
Although TreeRep is very efficient in embedding tree structures since it does not need the gradient-
based optimization steps, it costs more memory resources for constructing the tree structures from
data. It is basically a computation time vs. memory cost trade-off issue. For a graph with m nodes,
TreeRep needs to construct a matrix of size C ∙ m X C ∙ m to construct the tree structure, where
1 ≤ c ≤ 2 is a hyperparameter. We report the memory cost (GiB) in Table 6. UnitBall costs much
less memory to learn the embeddings.
F.7 More Results on Various Dimensions
In Section 5.3.2, we reported the performances in different embedding dimensions on YAGO3-
wikiObjects because of the page limits. Here we present the results in 8-d, 32-d, and 128-d embedding
spaces (4-d, 16-d and 64-d complex hyperbolic spaces for UnitBall) on ICD10 and WordNet-noun in
Table 7. Again, we see that with the increase of the embedding dimension, Euclidean can have big
20
Under review as a conference paper at ICLR 2022
Table 7: Evaluation of taxonomy link prediction in different embedding dimensions (the embedding
dimension for UnitBall is half of other models). The best results are shown in boldface. The second
best results are underlined. TreeRep is not applicable to 128-d WordNet-noun due to the large
memory cost so we do not include the results.
ICD10
8-dimensional	32-dimensional	128-dimensional
	MAP	MRR	Hits@3	MAP	MRR	Hits@3	MAP	MRR	Hits@3
Euclidean	2.57	2.57	1.32	3.75	3.72	2.39	10.83	10.48	4.66
TreeRep	3.44	3.90	6.03	4.96	7.92	8.49	8.09	8.74	17.23
Poincare	35.73	34.94	53.10	35.24	34.45	52.71	34.47	33.70	52.19
Hyperboloid	35.56	34.77	51.90	34.80	34.01	52.88	34.93	34.15	52.98
UnitBall	44.05	43.26	61.54	47.88	46.96	70.28	46.54	45.59	70.03
				WOrdNet-noun					
	8-dimensional			32-dimensional			128-dimensional		
	MAP	MRR	Hits@3	MAP	MRR	Hits@3	MAP	MRR	Hits@3
Euclidean	1.07	1.05	0.63	5.59	5.36	3.16	14.33	13.35	8.82
Poincare	25.23	23.78	27.63	25.46	23.99	27.80	25.33	23.86	27.41
Hyperboloid	25.73	24.24	27.67	25.65	24.15	27.50	25.77	24.27	27.65
UnitBall	24.91	23.76	30.27	27.29	25.93	32.95	27.29	25.91	32.77
Table 8: Results of MAP and Hits@1 on predicting 1-N edges in YAGO3-wikiObjects. The
embedding dimension is 16 for UnitBall while 32 for other models. The best results are shown in
boldface.
N for 1-N edges	1	YAGO3-wikiObjects (MAP)				>1
		2〜5	6〜10	11 〜20	> 20	
Poincare	60.73	16.87	9.28	9.31	9.46	15.29
Hyperboloid	61.49	15.01	9.25	9.50	9.54	13.80
UnitBall	58.41	26.73	13.28	10.17	11.64	23.61
	YAGO3-wikiObjects (HitS@1)					
Poincare	36.94	12.13	0.97	0.87	0.00	9.83
Hyperboloid	38.24	12.20	0.97	0.95	0.00	6.95
UnitBall	37.64	28.63	18.98	11.82	17.73	26.02
improvements, but its performances in 128-d still cannot surpass UnitBall and the hyperbolic models
in 8-d. UnitBall outperforms the baselines almost all the time. Although on WordNet-noun, UnitBall
in 4-d has slightly lower MAP and MRR than Poincare and Hyperboloid in 8-d, it has much higher
Hits@3.
F.8 More Results on 1-N Structure
In Section 5.3.3, we explored the performances on 1-N structures. Here we present results of more
evaluation metrics in Table 8. The results of UnitBall are close with Poincare and Hyperboloid in
1-1 structure. Nevertheless, UnitBall outperforms the hyperbolic baselines by a big margin in 1-N
structure for N > 1. Moreover, Unitball has considerably huge improvement for N > 6, where the
hyperbolic embedding models fail to make accurate predictions.
F.9 Comparison with Trainable Curvature Method AttH
Our work focuses on the representation of single-relation graphs, which is a different research
topic with multi-relational graph embeddings or knowledge graph embeddings, so it is hard to find
an appropriate experimental setting to compare them. Nevertheless, to address the concerns of
comparison with the trainable curvature method, here we evaluate AttH (Chami et al., 2020) on the
single-relation taxonomy link prediction task. We tune the hyperparameters on the validation set and
report the mean results over 5 running executions.
21
Under review as a conference paper at ICLR 2022
Table 9: Evaluation of taxonomy link prediction on YAGO3-wikiObjects (the dimension is 32 for
AttH and 16 for UnitBall).
	MAP	MRR	Hits@1	Hits@3
	 AttH	30.22	28.47	9.10	43.83
UnitBall	33.33	31.85	15.62	47.41
Table 10: Evaluation on link prediction task of GIL paper in ROC AUC (the dimension is 8 for
UnitBall and 16 for HGCN and GIL).
	Disease	Airport	Pubmed	Citeseer	Cora
	 HGCN	90.80	96.43	95.13	96.63	93.81
GIL	99.90	98.77	95.49	99.85	98.28
UnitBall	99.09	96.61	98.80	99.34	97.64
From the results in Table 9, we see that UnitBall outperforms AttH in the single hypernymy relation
link prediction task. However, UnitBall cannot infer multiple relations like AttH for now. MWe
believe the future work of the complex hyperbolic embeddings will have promising improvements on
multi-relational graph embeddings.
F.10 Comparison with Hyperbolic GNNs
Our work focuses on the single-relation graph embeddings and taxonomy embeddings, so we do
not evaluate the neural networks in our tasks in the main body of the paper. Although hyperbolic
GNNs also involve graph embeddings and can deal with the link prediction task, they make use of
not only the edges between nodes but also the node features. The message propagation and attention
mechanism make GNNs more flexible to handle various downstream tasks than shallow embeddings.
In this section, we evaluate UnitBall on the link prediction task on the datasets of GIL (Zhu et al.,
2020). The results of HGCN (Chami et al., 2019) and GIL (Zhu et al., 2020) are copied from Table
2 of the GIL’s original paper (Zhu et al., 2020). We strictly follow their experimental settings and
report the mean results of UnitBall in ROC AUC over 5 running executions. The dimension is 8 for
UnitBall and 16 for HGCN and GIL.
We can see that UnitBall outperforms HGCN on the five datasets. GIL is slightly better than UnitBall
on most datasets while being outperformed by UnitBall on Pubmed. The results are very promising
for UnitBall since UnitBall is a shallow embedding approach without deep architecture or feature
interaction. We believe the complex hyperbolic embeddings will help to improve the GNNs and bring
more insights into geometric deep learning.
F.11 Comparison with Product Embeddings
As mentioned in Section 2, the product space embeddings (Gu et al., 2019) tackles the challenges in
varying local structures by jointly learning the curvature and the embeddings of data in a product
manifold. Although it is impractical to search for the best manifold combination among enormous
combinations for each new structure, it is worth exploring the comparisons between the complex
hyperbolic embeddings and products of hyperbolic embeddings. Therefore, in this section, we
conduct experiments on the reconstruction task in synthetic compressed graphs. We evaluated the
16-dimensional UnitBall complex hyperbolic embeddings and 32-dimensional product hyperbolic
embeddings on (H2R)16, (H4R)8, (H8R)4. The results are reported in Table 11.
Recall that each compressed graph-(m, k) consists of m nodes and is aggregated from k random
trees on the m nodes. The bigger k corresponds to the denser and noisier graph. When k = 1 (δ = 0),
the graph is exactly a tree structure, UnitBall and the product hyperbolic embeddings both have much
better performances in this case. When k > 1, UnitBall still outperforms the product hyperbolic
embeddings by a large margin. Especially when m = 500, k = 2, 3, the δ is big, which means the
graph deviates from tree structures a lot, the product hyperbolic embeddings fail to reconstruct the
graph while UnitBall successfully handles the noisy structures.
22
Under review as a conference paper at ICLR 2022
Table 11: Results of MAP and Hits@3 on graph reconstruction in synthetic compressed graphs. The
best results are shown in boldface. The second best results are underlined.
	Compressed graph-(m, k) (MAP)									
	 k (m=500)	1	2	3	4	5	6	7	8	9	10
δ-hyperbolicity	0.0	2.5	1.5	1.0	1.0	1.0	1.0	1.0	1.0	1.0
ProdUCt-(HR)16	66.06	7.60	7.55	11.09	20.81	24.56	24.07	21.62	18.79	16.16
ProdUCt-(HR)8	65.77	7.14	7.24	11.79	20.60	24.94	22.85	22.32	18.50	16.27
ProdUCt-(HR)4	65.42	6.28	6.81	11.43	19.03	23.88	24.99	20.50	18.99	16.45
UnitBall-HC6	84.72	52.74	44.73	39.75	35.32	33.17	32.48	29.13	29.86	28.58
	Compressed graph-(m, k) (HitS@3)									
Product-(HR)16	80.34	8.14	9.84	14.81	37.45	45.97	47.89	47.19	40.85	38.88
ProdUCt-(HR)8	80.34	8.35	9.43	16.23	36.23	47.98	44.87	47.19	39.84	36.87
ProdUCt-(HR)4	79.36	6.21	10.25	16.63	31.38	44.15	49.50	43.57	40.64	40.08
UnitBall-HC6	97.71	75.87	72.61	74.92	73.21	73.12	76.12	76.57	75.72	74.08
	Compressed graph-(m, k) (MAP)									
m (k=5)	100	200	300	400	500	600	700	800	900	1000
δ-hyperboliCity	1.0	1.0	1.0	1.0	1.0	1.5	1.5	1.5	1.5	1.5
ProdUCt-(HR)16	30.53	29.15	25.72	21.74	20.81	19.34	19.07	16.63	15.59	14.82
ProdUCt-(HR)8	32.03	27.63	23.82	22.32	20.60	17.94	19.12	17.36	15.39	14.74
ProdUCt-(HR)4	31.39	27.95	23.17	19.90	19.03	17.96	18.09	16.13	13.86	13.47
UnitBall-HC6	47.80	40.93	38.52	35.81	35.32	35.68	34.73	34.69	34.92	34.88
	Compressed graph-(m, k) (HitS@3)									
ProdUCt-(HR)16	38.78	52.02	43.58	35.61	37.45	34.29	34.54	28.34	27.44	24.77
ProdUCt-(HR)8	46.94	43.43	39.53	38.38	36.23	29.58	35.84	29.35	27.55	27.60
ProdUCt-(HR)4	54.08	42.42	35.47	33.59	31.38	32.44	30.49	29.72	24.64	24.57
UnitBall-HC6	84.35	75.42	77.36	72.14	73.21	73.00	73.99	72.63	72.23	74.01
Table 12: Nodes embedded in the most simple totally real plane of the unit ball model. The
embeddings are trained by UnitBall model on link prediction task in Section 5.3.1. Considering the
possible numerical error, we allow the embeddings to deviate the plane for a small threshold instead
of strictly lying in the plane.
Datasets	Nodes embedded in the totally real plane
ICD10 YAGO3-wikiObjeCts WordNet-noUn	ICD10-root Objects (root), Physical objects, Organisms, Men entity.n.01 (root)
F.12 Case Study of Nodes Embedded in the Totally Real Plane
Although as analyzed in Appendix B, curvature in the complex hyperbolic space is a very complicated
topic in geometric group theory and differential geometry. Computing the curvature of an arbitrary
point in the complex hyperbolic space or visualizing the curvatures everywhere are highly complicated
problems, the existing work usually explored a few examples by numerical computation. However,
the curvatures in two special subspaces, i.e., the complex projective lines and the totally real planes,
are well-studied. It is interesting to see which nodes lie in the special subspaces. In this section, we
find the nodes of the real-world taxonomies that are embedded in the most simple totally real plane
of the unit ball model, i.e., HnR ∩ BCn, where HRn = {(x1, . . . , xn)|x1, . . . , xn ∈ R}.
Table 12 shows that the root node of all three taxonomies are embedded in the totally real plane,
which is in accordance with expectations since the root nodes are usually embedded close with the
origin. For the more fine-grained taxonomy YAGO3-wikiObjects, some high-level nodes are also
embedded in this totally real plane. These nodes actually form a chain of hypernymy relation in
the taxonomy: Men → Organisms → P hysical obj ects → Objects. Recall Theorem 1 states
23
Under review as a conference paper at ICLR 2022
that the curvature in complex hyperbolic space is pinched between -1 (in the directions of complex
projective lines) and -1/4 (in the directions of totally real planes). From the results in the three
taxonomies, we can see that the high-level nodes in the hierarchical structure tend to lie in the totally
real planes, which are the least curved subspaces of the complex hyperbolic space.
24