Under review as a conference paper at ICLR 2022
Learning Global Spatial Information
for Multi-View Object-Centric Models
Anonymous authors
Paper under double-blind review
Ab stract
Recently, several studies have been working on multi-view object-centric models,
which predict unobserved views of a scene and infer object-centric representations
from several observation views. In general, multi-object scenes can be uniquely
determined if both the properties of individual objects and the spatial arrangement
of objects are specified; however, existing multi-view object-centric models only
infer object-level representations and lack spatial information. This insufficient
modeling can degrade novel-view synthesis quality and make it difficult to gen-
erate novel scenes. We can model both spatial information and object represen-
tations by introducing hierarchical probabilistic model, which contains a global
latent variable on top of object-level latent variables. However, how to execute
inference and training with that hierarchical multi-view object-centric model is
unclear. Therefore, we introduce several crucial components which help infer-
ence and training with the proposed model. We show that the proposed method
achieves good inference quality and can also generate novel scenes.
1	Introduction
Extracting representations of individual objects from direct perception (e.g., visual input) is called
object-centric representation learning. It is considered to be effective in many aspects such as
robotics (Devin et al., 2018; Veerapaneni et al., 2019; Kulkarni et al., 2019), reasoning (Ding et al.,
2021), and sample efficiency (Watters et al., 2019a). Among them, the deep generative model
(DGM)-based method is one of the promising directions, as it can infer rich information about
objects in a fully unsupervised manner, and some of its models can also generate novel scenes (Gr-
eff et al., 2017; Locatello et al., 2020; Burgess et al., 2019; Greff et al., 2019; Engelcke et al., 2020;
Eslami et al., 2016; Crawford & Pineau, 2019; Jiang & Ahn, 2020).
Recently, several studies have been working on multi-view object-centric models, which predict
unobserved views of a scene: novel view synthesis, and infer object-centric representations from
several observation views (Nanbo et al., 2020; Chen et al., 2021). This can be also regarded as an
object-centric expansion of the Generative Query Network (GQN) (Eslami et al., 2018). In order
to represent multi-object scenes, the properties of individual objects and the spatial arrangement of
objects should be specified. However, existing multi-view object-centric methods only explicitly
model representations of individual objects, and each of them includes their own spatial information
separately. Namely, relationship between objects is not represented in this case. This modeling can
degrade novel view synthesis and segmentation, because the model need to solve occlusions and
spatial ambiguity virtually without the prior knowledge about objects’ spatial relationship. This can
be serious especially when the number of available observation views is limited. In addition, this
modeling can also lead to inability to generate physically plausible novel scenes, because they need
to place objects independently, which often brings about collisions and misplacement of objects.
Spatial information of the whole scene can be represented by introducing another latent variable be-
sides object-level latent variables. This variable can be independent of object-level latent variables,
or can be modeled as a global latent on top of object-level latent variables. In the former formula-
tion, those latent variables need to be disentangled during training, which can often be challenging.
From another point of view, it is common assumption in cognitive science that humans recognize
the whole and parts, or objects, separately (Schmidt, 2009). In feature integration theory (Treisman
& Gelade, 1980), visual feature maps become “master map of locations” combined with positions of
1
Under review as a conference paper at ICLR 2022
X3
Product of Experts (PoE)
Global Encoder
Spatial GauSSian Mixture
latent
Iterative Amortized Inference
K slots
Sequential Encoder
global
Structured
Prior
query
viewpoint
Figure 1: Overview of the proposed model. In this example, the model predicted an image and
corresponding segmentation masks from unobserved query viewpoint vq , using three observation
views x1 -x3 . zg is a global latent variable that represents spatial relationship between objects, and
Zk is object-level latent variables that represent each object. Note that the Xq and segmentation
shown in this figure are the actual results from our model.
objects in visual space. Then, attention binds those features to recognize individual objects. Model-
ing the spatial arrangement of objects as a global latent variable closely adheres to this point of view
due to its hierarchical property. For the above reasons, we believe that introducing a global latent
variable is more natural step for improving object-centric generative models.
In this paper, we build a new multi-view object-centric model which has a latent variable for global
spatial information to improve inference: novel view synthesis and segmentation. In addition, our
model can also generate physically plausible novel scenes. The problem is that how to execute
inference and training with the proposed model is unclear. In regards to this point, we propose
several crucial components: Global Encoder, Sequential Encoder and Structured Prior. Firstly, the
model needs to infer global representation from variable number of input views, thus we introduced
an encoder using Product of Experts (PoE) (Hinton, 2002), which we refer to as Global Encoder.
Secondly, the structured, complex posterior of object-level latent variables need to be sampled in-
cluding spatial relationship between objects in order to generated physically plausible novel scenes.
Therefore, we introduced a learnable prior called Structured Prior, which is implemented by Trans-
former (Vaswani et al., 2017). Lastly, we introduced an encoder using auto-regressive model called
Sequential Encoder. This stabilize inference and training with the hierarchical probabilistic model
of the proposed method.
The proposed model has representation about the whole, or spatial information of a scene and has
representations about a part of a scene (i.e., individual properties of objects); hence, we call this
model the Whole-Part Representation Learning Model for Object-Centric 3D Scene Inference and
Sampling (WeLIS).
Contributions:
•	We introduce a multi-view object-centric model with a global latent variable which repre-
sents the spatial configuration of objects in a scene.
•	We introduce essential components for introducing a global latent variable as well as stable
learning and inference for our multi-view object-centric model.
•	We show that the proposed model performs well in terms of inference quality, and can gen-
erate novel scenes and corresponding segmentation masks properly, unlike existing meth-
ods.
2
Under review as a conference paper at ICLR 2022
Table 1: Comparison of multi-object-multi-view (MOMV) related methods.				
Model	Object-Centric	Novel View Synthesis	Inference	Sampling (Novel Scene Generation)
GQN	X	X	X	X (Kosiorek et al., 2021)
ObSuRF, uORF 1	X	X	X	X
ROOTS	X	X	X	X
MulMON	X	X	X	X2
WeLIS (Ours)	X	X	X	X
1These methods are not generative models.
2MulMON can generate objects independently, but this leads to an ambiguous, blurred image.
2	Related Work
Object-Centric Models: The objective of object-centric representation learning is to obtain rep-
resentations of each object from visual input. It is applicable to many domains such as robotics
(Devin et al., 2018; Florence et al., 2019; Veerapaneni et al., 2019), reasoning (Ding et al., 2021)
and reinforcement learning (Watters et al., 2019a). One of the promising directions of object-centric
representation learning is variational autoencoder (VAE)-based (Kingma & Welling, 2013) methods,
as they can obtain rich information about objects that can also be disentangled, and some of them
can generate novel scenes outside of the empirical distribution (Burgess et al., 2019; Greff et al.,
2019; Engelcke et al., 2020; Eslami et al., 2016; Crawford & Pineau, 2019; Jiang & Ahn, 2020).
VAE-based object-centric models are often referred to as “scene interpretation models” due to their
inference ability. Some models are capable of object-centric generation (Dai et al., 2019; Nguyen-
Phuoc et al., 2020; Niemeyer & Geiger, 2021), however, these methods focus on generation and not
for inference.
Multi-View Object-Centric Models: Lately, several studies have been working on multi-view
scene interpretation models (Nanbo et al., 2020; Chen et al., 2021). These methods predict images
of a 3D scene from arbitrary viewpoints in an object-centric way, using several observation views as
input. This problem setting can be regarded as an object-centric expansion of GQN family (Eslami
et al., 2018; Tobin et al., 2019; Kosiorek et al., 2021). The above mentioned single-view setting
is referred to as multi-object-single-view (MOSV), and the multi-view setting is defined as multi-
object-multi-view (MOMV) in (Nanbo et al., 2020).
Table 1 compares methods related to the MOMV setting. Novel view synthesis stands for prediction
of unobserved viewpoints, and novel scene generation stands for random generation of novel scenes.
Among these models, ROOTS and MulMON (Nanbo et al., 2020; Chen et al., 2021) are the only
generative models that work on a MOMV setting. In addition to this, O3V (Henderson & Lampert,
2020) have a similar problem setting, but it takes fixed number of viewpoints as input. ObSuRF
(Stelzner et al., 2021) and uORF(Yu et al., 2021) also work on MOMV setting, but these are not
generative models, and are based on Slot Attention (Locatello et al., 2020) and NeRF (Mildenhall
et al., 2020). Therefore these methods can not generate novel samples and does not obtain low
dimensional representation space, or latent space.
In this paper, we introduce a new MOMV model with a global latent variable, which can potentially
represents the spatial arrangement of objects in a scene. The proposed model is the only MOMV
model that can generate novel scenes. In this paper, we set MulMON as our baseline model due
to the following reasons. Firstly, MulMON uses segmentation masks to represent objects’ regions
unlike ROOTS which uses bounding boxes. Bounding box-based methods, which originate from
(Eslami et al., 2016) tend to be more sensitive to object size and shape (Engelcke et al., 2021).
Secondly, MulMON is more robust in terms of the number of observation views required. Although
we built our model based on MulMON, a similar approach can be applied to ROOTS and other
methods.
3
Under review as a conference paper at ICLR 2022
Introducing the global latent variable itself has been done in some studies in various areas (Vasco
et al., 2020; Akuzawa et al., 2021). In particular, GNM (Jiang & Ahn, 2020) introduces a global la-
tent variable to bounding box-based single-view object-centric model for random generation. How-
ever, how to execute stable inference and training with the proposed MOMV model is unclear, and
is thus the points of investigation in this paper.
3	Method
Our objective was to introduce a MOMV model that has both object-level representations and the
spatial arrangement of the scene to obtain further inference quality and novel scene generation abil-
ity.
We achieved this by introducing a global latent variable for global spatial information on top of
object latent variables, which is formulated as hierarchical probabilistic model. In this paper, we in-
troduce several crucial components to realize stable inference and training with this MOMV model.
An overview of the proposed model is shown in Figure 1.
In this section, we first explain the problem setting in this paper, and then the probabilistic model of
our proposed model. Lastly, the training method and additional details of the introduced components
are explained.
3.1	Problem Setting
The target of this research is 3D scenes with multiple objects, and thus a dataset with N data points
can be defined as D = {Xι,…，XN}, where Xi = {(χi, v11),..., (xM, VM)}, which corresponds
to one scene with M views, and v is a viewpoint vector with arbitrary dimensions, which are mostly
three.
The task is to predict queried unobserved views of the scene given several observations
{(χi, v1),…,(XNObs, vNobs)}, and infer K representations each of which correspond to an ob-
ject in the scene Z = {zι, ∙∙∙ , ZK}. In practice, the M views in Xi is split into Nobs observations
and Nqry queries in the training process. Nobs can be varied in every iteration for generalization.
3.2	Probabilistic Model
Generative Model: As mentioned above, we introduced the hierarchical probabilistic model with
global latent Zg on top of object-level latent variables Z = {zι,…，ZK}. In the following sections,
object-level latent variables are abbreviated as z, as required. The marginal likelihood is thus:
PY(x | v) = / ∕pθ(x|z, V) Pφ(z | Zg) P(Zg) dzdzg,	(1)
where Y = {θ, φ}. The generative model pθ(x|z, V) is a spatial Gaussian mixture model derived
from prior works (Nanbo et al., 2020; Burgess et al., 2019) that can be written as:
DK
Pθ(x|z, V)=	Pθ(Ci =k | Zk, v) 0Pθ(xik∣Zk, v),	(2)
where D ∈ N is the number of image pixels, and K is the number of mixture components which
are often referred to as “slots”. The decoder has parameters θ and implemented by a broadcast
decoder (Watters et al., 2019b). Zk and V are first concatenated and projected onto a single vector
fk by MLP and then fed into the decoder. The decoder generates K components {xι,…，XK} and
corresponding segmentation masks from {fι,…，fκ}.
The second term in the r.h.s of Equation 1 Pφ(Z | Zg) is a learnable prior which we refer to as
the Structured Prior, and the third term is a standard Gaussian distribution. The Structured Prior is
explained in Section 3.3.
Inference Model: We used amortized variational inference (Kingma & Welling, 2013) and com-
puted variational approximate posteriors because the marginal likelihood in Equation 1 is intractable.
We factorized the variational posteriors as:
qψ(z, Zg	|	X)	=	qψ(zi,…，Zk	| Zg)	qψ(Zg	|	X),	(3)
4
Under review as a conference paper at ICLR 2022
where X = {(x1, v1), . . . , (xNobs , vNobs)}. The first RHS term and second RHS term are approx-
imate posteriors of z and zg respectively. We refer to these encoders as Sequential Encoder and
Global Encoder.
The Global Encoder adopts PoE (Hinton, 2002) to deal with a variable number of input observation
views Nobs. In particular, we used PoE with prior expert (Wu & Goodman, 2018). In addition to this,
we applied normalizing flow (NF) (Rezende & Mohamed, 2015) to this inference model. Although
this is optional, the posterior should be expressive in order to represent spatial configuration of
complex 3D multi-object scenes. Therefore, the variational approximate posterior is:
Nobs	T
Zg 〜qψ (Zg | X) = (P(Zg) Y qψ (Zg | Xv, Vv) Y
v=1	t=1
“ t∂fψ
det ∂zg
-1
(4)
where T is the number of transformations of NF, and Nobs is the number of available observation
views. We used planar flow for its simplicity, but any methods can be applied and can improve the
results potentially. Thus, the transform fψ in the above equation is
fψ(Zi+1) = Zi + Ui ∙ h(wTZi + bi).
The Sequential Encoder was implemented in auto-regressive manner using LSTM (Hochreiter &
Schmidhuber, 1997). We tried several architectures, but non-auto-regressive models were unstable
as long as we have tested. Firstly, this auto-regressive part infers K latent variablesZ01,...,Z0Kand
then, these are updated using iterative amortized inference (IAI) (Marino et al., 2018) as MulMON
and IODINE (Greff et al., 2019) do. However, we omitted image-sized inputs as in (Emami et al.,
2021). Thus, the Sequential Encoder can be defined as:
K
z1,...,zK 〜Yqψ(Zk | Z1：k,Zg),
k=1
zk+1 = zk + fψ (Zk, zg, ^zikL LIAI ),
(5)
(6)
where Vz⅛ LIAI is a gradient of negative log-likelihood in that iteration, and fψ is a refinement
network that is implemented by MLP followed by LSTM. The posterior can be updated arbitrary
times, and we set L = 5 through this paper. The details ofLiIAI will be explained in the next section.
Training: All the parameters in this model can be trained end-to-end by maximizing evidence
Iowerbound (ELBO) on the log-marginal likelihood log p(x1,…，XN). The ELBO of the proposed
model is
ELBO = Eq(z,zg |X)
lo Pθ(x∣z, v)pφ(z | ZgIp(Zg) 一
.°g qψ (z | Zg )qψ (zg | X) _
(7)
Eq(z,zg|X) [logpθ(x | Z, v)]
—KL [qψ (Zg | X) Il P(Zg)] — Eqψ (Zg ∣X) [KL [qψ (z | Zg) Il pφ( Z | Zg)]].
Here, KL stands for Kullback-Leibler divergence. Then, introducing two coefficient βι and β2 for
balancing KL terms, the objective can be written as
L = Eq(z,zg|X) [logpθ(x I Z, v)]
—βιKL[qψ(zg ∣ X) Il p(zg)] — β2Eqψ(Zg∣χ) [KL[qψ(z ∣ zg) ∣∣ pφ(Z ∣ zg)]].	(8)
Moreover, due to the iterative process of IAI, negative log-likelihood is computed as
Eq(Z,Zg |X) [log pθ (x I Z, v)]
2
L(L +1)
L-1
X(i + 1)LiIAI,
i=0
LIAI = EZi〜q(zi,zg |X) [logPθ (x∣Zi, v)],
(9)
(10)
where, L is the number of iterations in IAI. As shown in Equation 9, log-likelihoods in the later
steps of iterative refinement are emphasized as in MulMON and IODINE.
In addition to this, we used GECO (Rezende & Viola, 2018) as in some existing methods (Engelcke
et al., 2020; Emami et al., 2021). GECO reinterprets the maximization of ELBO as a constrained
5
Under review as a conference paper at ICLR 2022
ujωw,rI-Io-Lu
0.135
CLEVR AUG
—OUrS
.... Ours: w/o Flow
---MulMON (bassline)
N: the number of observations
Figure 2: Quantitative evaluation of novel view synthesis. Lower is better for root mean squared
error(RMSE), and higher is better for mean intersection over union (mIoU). There is no mIoU for
GQN Jaco dataset because it does not provide ground truth segmentation masks.
Figure 3: Novel view synthesis results on GQN Jaco with four difference scenes. These images are
prediction of unobserved query views. The number of observation views is three (N = 3).
optimization problem that takes KL terms as the main objeCtive and log-likelihood as the Constraint.
This optimization is done by the method of Lagrange mUltipliers, namely, the CoeffiCient of KL
terms β is regarded as a Lagrange mUltiplier, and aUtomatiCally tweaked dUring training. GECO is
not neCessary for oUr model, bUt proper balanCing of β fUrther improves random generation qUality.
The reinterpreted optimization objeCtive thUs beComes
argminKL[qψ(zg | Xobs, Vobs) || P(Zg)] + Eq”(Zgχ [KL [qψ(z | zg) || pφ( Z | zg)]]	(11)
ψ,φ
SUCh that - Eq*(z,zgχ) [logPθ (X | z, v)] 5 R,
(12)
where R ∈ R is the maximum allowed negative log-likelihood, which is a hyperparameter in GECO
instead of β . Although all the parameters can be trained end-to-end, we found that end-to-end
training including the parameters of Structured Prior φ is slightly unstable and often degrades novel
scene generation. Thus, we introduced a separate training strategy, in which φ is updated after the
training of the main model. Detail of this method is explained in the next section.
3.3 Structured Prior
In this seCtion, we explain the detail of the StrUCtUred Prior and its separate training strategy.
First of all, the Structured Prior is defined as Pφ(zL, •一，ZK | zg). Aswe mentioned in Equation 5,
the latent variables of eaCh slots are iteratively Updated by IAI in the inferenCe proCess. However,
it makes sampling from the updated posterior zL, ∙∙∙ , ZK difficult. Though IAI updates latent
variables using reconstruction error and auxiliary inputs, generative model needs to blindly capture
this process without using them.
Therefore, in order to model this process, we implemented Structured Prior as Transformer (Vaswani
et al., 2017). Firstly, initial K variables z0k are computed from zg using the Sequential Encoder
qψ (z0 | zg). Then, Transformer takes the K latent variables as input, and approximates updated
ones zkL. Note that the parameter of Sequential Encoder is shared with inference.
Separate Training: As we mentioned above, we introduced a separate training method for the
Structured Prior. In a first stage, WeLIS is trained without the parameters of Structured Prior φ.
6
Under review as a conference paper at ICLR 2022
Ge
Ote
Sh
(SJnO) Sn①M
view 2
view 3
Seg.
view 1
φu=① SBm)NOIΛ∏n∣Al
k = 1 k = 2 k = 3 k = 4 k = 5 k = 6 k = 7
VieW 1
VieW 2
view 3
Figure 4: Component-by-component random generation results of WeLIS and the baseline. Each
row corresponds to a different query viewpoint. The first column shows generated images, and
the next seven columns show each slot (Xk, k ∈ {1,…，K}), while last column shows generated
segmentation masks.
Namely, φ is frozen and β2 in Equation 7 is set to zero or the second term of Equation 11 is excluded
if we use GECO. Then, as a second stage, φ is updated while other parameters are frozen.
4 Experiments
We evaluated our model with three different datasets derived from the MulMON paper (Nanbo et al.,
2020). CLEVR Multi-View (CLEVR MV) is a multi-view version of the CLEVR dataset (Johnson
et al., 2017), and CLEVR Augmented (CLEVR AUG) extends CLEVR MV with more complex
and various objects for additional difficulty. GQN Jaco is a simulation based robot arm dataset
introduced originally in (Eslami et al., 2018). All the datasets are used in 64 × 64 resolution 1, and
we trained all the models with Adam optimizer for 300k iterations. We used official implementation
of MulMON for reproduction. Up to six views are given as observation during training.
First, we evaluated inference quality quantitatively and show that WeLIS outperforms MulMON in
many cases. Then, we show WeLIS can also generate novel scenes, in contrast to baseline models.
Lastly, we show ablation results and also investigate how global latent variable zg works. In addition,
additional ablation studies, samples, and the results of downstream task are shown in Appendix B.
4.1	Novel View Synthesis and Segmentation
In this section, we evaluate novel view synthesis and segmentation quality. Figure 2 shows quan-
titative evaluation of three models: WeLIS, WeLIS without NF (ablation) and MulMON. Novel
view synthesis is evaluated by Root Mean Squared Error (RMSE), and segmentation is evaluated
by mean Intersection over Union(mIoU). Random four seeds are used to calculate these scores. As
GQN Jaco does not provide ground truth segmentation, we instead show novel view synthesis results
in Figure 3.
We can see that WeLIS outperforms MulMON in CLEVR AUG, GQN Jaco and CLEVR MV with
a small number of observation views N . WeLIS is relatively stable in terms of N , which means
that WeLIS is good at inferring whole scene from limited, partial observation. Introducing global
1According to the supplemental material, MulMON uses CLEVR AUG in 128 × 128 resolution, but we
used in 64 × 64 for consistent comparison.
7
Under review as a conference paper at ICLR 2022
WeLIS (Ours)
MulMON (Baseline)
WeLIS w/o Structured Prior
WeLIS w/o Normalizing Flow


Figure 5: Random generation results from WeLIS and MulMON(baseline). The top row shows
results from WeLIS and MulMON. The bottom row shows WeLIS without Structured Prior and
without normalizing flow for ablation study. We generated these images by randomly sampling
eight zg for WeLIS, and by sampling each slot independently for MulMON. Note that the same set
of zg is used in the full model(top left) and in the ablation of the Structured Prior(bottom left).
Table 2: Evaluation of FID score on three models: WeLIS(Ours), WeLIS without Normalizing Flow
and MulMON. Lower is better.
Datasets	I WeLIS (Ours)	WeLIS w/o NF	MulMON
CLEVR MV	125.8 ±1.7	117.2 ±5.9	157.9 ±8.4
CLEVR AUG	83.3 ±1.7	134.2 ±25.8 1	168.6 ±5.0
GQN Jaco	251.7 ±2.1	262.1 ±11.0	273.7 ±10.4
1If we exclude the case where training did not converge, the score becomes 90.0 ± 1.3.
latent in addition to object-level latent variables leads to more precise modeling of the world, thus
we consider that it helps inference with small number of observations. As for GQN Jaco (Figure 3),
WeLIS captures the balls consistently with clean segmentation.
We also evaluated WeLIS without NF as ablation study. We found that NF is important for stable
inference and training. Without it, the model often fall into local minima that does not properly
represent one object per slot, as you can see from the relatively large standard error in Figure 2. If
the training successfully finished, the model achieved decent performance close to full model.
4.2	Novel Scene Generation
In this section, we evaluate novel scene generation results. As shown in Figure 4, WeLIS success-
fully generates objects and segmentation masks, and as a result, generates physically plausible novel
scenes. On the other hand, MulMON generates blurred and spatially ambiguous images. Top row
of Figure 5 shows more samples.
4.2	. 1 Ablation Studies
Bottom row of Figure 5 shows ablation studies of Structured Prior and NF. Firstly, we look into
Structured Prior. In this experiment, the results from full model (top left) and ablation study of
Structured Prior (bottom left) are generated from same zg . Generated images without Structured
Prior are severely blurred, however, we can see that the spatial arrangement of objects itself is
similar to full model. This indicates that Structured Prior only contributes to object-level generation
quality.
Secondly, bottom right images in Figure 5 shows ablation result of NF. As you can see, the quality is
almost same with full model even without NF, which indicates that NF is not essential for generation.
8
Under review as a conference paper at ICLR 2022
Figure 6: Random generation results from four different Zg (A-D) and four neighborhoods Zg + J
where is a Gaussian noise as perturbation. Each row corresponds to a different zg . The left most
columns of each group show the original Zg , and other columns show different perturbations J.
Therefore, each row represents a “cluster” of each Zg . We can see that images in the same row share
a similar spatial structure. The schematic of the latent space on the right side is just for illustration
purposes.
However, as we mentioned in Section 4.1, NF is important for stable inference and training, thus it
is also necessary for generation in practice.
4.2.2 Quantitative Evaluations
We also quantitatively evaluated random generation by Frechet Inception Distance(FID) score in
Table 2. WeLIS outperforms MulMON in every dataset, and as shown in above section, WeLIS
without NF achieved similar quality to full model, as long as the training succeeded. Scores without
NF in CLEVR AUG and GQN Jaco are degraded because the training fails more frequently as
datasets become difficult.
Poor generation quality of MulMON comes from mainly two reasons: insufficient modeling of spa-
tial arrangement and difficulty in sampling actual posterior. In WeLIS, former one is dealt with by
introducing global latent Zg which represents spatial information of a whole scene, and latter one
is dealt with by a Structured Prior which estimates complex posterior inferred from IAI. Effective-
ness of introducing Zg is evaluated in Section 4.3 and of Structured Prior is already investigated in
Figure 5.
4.3 Inspection of Global Latent Space
To look into the representation obtained by global latent Zg , we show random generation results
from four different Zg (A-D) and four neighborhoods Zg + Jof them in Figure 6. Here, Jis a
small perturbation randomly sampled from standard normal distribution. First column in each group
shows images generated from four different Zg , and other four columns are generated with different
perturbation. Thus, each four group represents a “cluster” around Zg .
We can see that generated scenes from same Zg have similar spatial structure, while their objects
vary. This indicates that the latent space ofZg obtains spatial configuration ofa scene as we expected.
5	Conclusion
We introduced new multi-view object-centric model: WeLIS, which is able to perform multi-view
object-centric inference and sampling in 3D scenes. We introduced several components to WeLIS,
in order to adapt a global latent variable and to enable inference and training with that model. The
global latent variable improved inference quality especially when the observations are limited (Fig-
ure 2) and also enabled the model to generate physically plausible novel scenes (Figure 5). We also
conducted some ablation studies and investigated the representation obtained by global latent space
(Figure 6).
One of the future directions of this research is to sophisticate inference methods: normalizing flow
and IAI. There is a room for improvement with optimal choice of normalizing flow. In addition,
we used simplified IAI without image-sized input which can reduce computational cost, however
there is a possibility that this sacrificed the ability to distinguish similar, ambiguous objects such as
rounded cubes and spheres in CLEVR MV dataset.
9
Under review as a conference paper at ICLR 2022
6	Reproducibility Statement
Architecture:
Detail of the network architecture is provided in Appendix A. Public version of the source code will
be released soon.
Optimization:
We derived the training scheme from MulMON in which all models are trained for 300k gradient
steps with Adam optimizer. 300k gradient steps is composed of 1000 epochs × 300 scenes. The
300 scenes are randomly picked in each iteration. Learning rate decay in GQN and MulMON is also
applied to our model.
References
Kei Akuzawa, Yusuke Iwasawa, and Yutaka Matsuo. Information-theoretic regularization for learn-
ing global features by sequential vae. Machine Learning, pp. 1-28, 2021.
Christopher P Burgess, Loic Matthey, Nicholas Watters, Rishabh Kabra, Irina Higgins, Matt
Botvinick, and Alexander Lerchner. Monet: Unsupervised scene decomposition and represen-
tation. arXiv preprint arXiv:1901.11390, 2019.
Chang Chen, Fei Deng, and Sungjin Ahn. Roots: Object-centric representation and rendering of 3d
scenes, 2021.
Eric Crawford and Joelle Pineau. Spatially invariant unsupervised object detection with convolu-
tional neural networks. In Proceedings of the AAAI Conference on Artificial Intelligence, vol-
ume 33, pp. 3412-3420, 2019.
Songmin Dai, Xiaoqiang Li, Lu Wang, Pin Wu, Weiqin Tong, and Yimin Chen. Learning segmen-
tation masks with the independence prior. 33:3429-3436, Jul. 2019. doi: 10.1609/aaai.v33i01.
33013429. URL https://ojs.aaai.org/index.php/AAAI/article/view/4218.
Coline Devin, Pieter Abbeel, Trevor Darrell, and Sergey Levine. Deep object-centric representa-
tions for generalizable robot learning. In 2018 IEEE International Conference on Robotics and
Automation (ICRA), pp. 7111-7118, 2018. doi: 10.1109/ICRA.2018.8461196.
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of deep
bidirectional transformers for language understanding. In Proceedings of the 2019 Conference of
the North American Chapter of the Association for Computational Linguistics: Human Language
Technologies, Volume 1 (Long and Short Papers), pp. 4171-4186, Minneapolis, Minnesota, June
2019. Association for Computational Linguistics. doi: 10.18653/v1/N19-1423. URL https:
//www.aclweb.org/anthology/N19-1423.
David Ding, Felix Hill, Adam Santoro, Malcolm Reynolds, and Matt Botvinick. Attention over
learned object embeddings enables complex visual reasoning, 2021.
Patrick Emami, Pan He, Sanjay Ranka, and Anand Rangarajan. Efficient iterative amortized infer-
ence for learning symmetric and disentangled multi-object representations. In Marina Meila and
Tong Zhang (eds.), Proceedings of the 38th International Conference on Machine Learning, vol-
ume 139 of Proceedings of Machine Learning Research, pp. 2970-2981. PMLR, 18-24 Jul 2021.
URL http://proceedings.mlr.press/v139/emami21a.html.
Martin Engelcke, Adam R. Kosiorek, Oiwi Parker Jones, and Ingmar Posner. Genesis: Generative
scene inference and sampling with object-centric latent representations. In International Confer-
ence on Learning Representations, 2020.
Martin Engelcke, Oiwi Parker Jones, and Ingmar Posner. Genesis-v2: Inferring unordered object
representations without iterative refinement. arXiv preprint arXiv:2104.09958, 2021.
SM Ali Eslami, Nicolas Heess, Theophane Weber, Yuval Tassa, David Szepesvari, Geoffrey E Hin-
ton, et al. Attend, infer, repeat: Fast scene understanding with generative models. In Advances in
Neural Information Processing Systems, pp. 3225-3233, 2016.
10
Under review as a conference paper at ICLR 2022
SM Ali Eslami, Danilo Jimenez Rezende, Frederic Besse, Fabio Viola, Ari S Morcos, Marta Gar-
nelo, Avraham Ruderman, Andrei A Rusu, Ivo Danihelka, Karol Gregor, et al. Neural scene
representation and rendering. Science, 360(6394):1204-1210, 2018.
Peter Florence, Lucas Manuelli, and Russ Tedrake. Self-supervised correspondence in visuomotor
policy learning. IEEE Robotics and Automation Letters, 5(2):492-499, 2019.
Klaus Greff, Sjoerd van Steenkiste, and Jurgen Schmidhuber. Neural expectation maximiza-
tion. In I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and
R. Garnett (eds.), Advances in Neural Information Processing Systems, volume 30. Curran As-
sociates, Inc., 2017. URL https://proceedings.neurips.cc/paper/2017/file/
d2cd33e9c0236a8c2d8bd3fa91ad3acf- Paper.pdf.
Klaus Greff, Raphael LoPez Kaufmann, Rishab Kabra, Nick Watters, Chris Burgess, Daniel Zoran,
Loic Matthey, Matthew Botvinick, and Alexander Lerchner. Multi-object representation learning
with iterative variational inference. arXiv preprint arXiv:1903.00450, 2019.
Paul Henderson and Christoph H Lampert. Unsupervised object-centric video generation and de-
composition in 3d. arXiv preprint arXiv:2007.06705, 2020.
Dan Hendrycks and Kevin Gimpel. Gaussian error linear units (gelus). arXiv preprint
arXiv:1606.08415, 2016.
Geoffrey E Hinton. Training products of experts by minimizing contrastive divergence. Neural
computation, 14(8):1771-1800, 2002.
Sepp Hochreiter and Jurgen Schmidhuber. Long short-term memory. Neural Comput., 9(8):
1735-1780, November 1997. ISSN 0899-7667. doi: 10.1162/neco.1997.9.8.1735. URL
https://doi.org/10.1162/neco.1997.9.8.1735.
Jindong Jiang and Sungjin Ahn. Generative neurosymbolic machines. Advances in Neural Informa-
tion Processing Systems, 33, 2020.
Justin Johnson, Bharath Hariharan, Laurens Van Der Maaten, Li Fei-Fei, C Lawrence Zitnick, and
Ross Girshick. Clevr: A diagnostic dataset for compositional language and elementary visual
reasoning. In Proceedings of the IEEE conference on computer vision and pattern recognition,
pp. 2901-2910, 2017.
Diederik P Kingma and Max Welling. Auto-encoding variational bayes. arXiv preprint
arXiv:1312.6114, 2013.
Adam R Kosiorek, Heiko Strathmann, Daniel Zoran, Pol Moreno, Rosalia Schneider, Sona Mokra,
and Danilo Jimenez Rezende. Nerf-vae: A geometry aware 3d scene generative model. In Ma-
rina Meila and Tong Zhang (eds.), Proceedings of the 38th International Conference on Machine
Learning, volume 139 of Proceedings of Machine Learning Research, pp. 5742-5752. PMLR,
18-24 Jul 2021. URL https://proceedings.mlr.press/v139/kosiorek21a.
html.
Tejas Kulkarni, Ankush Gupta, Catalin Ionescu, Sebastian Borgeaud, Malcolm Reynolds, Andrew
Zisserman, and Volodymyr Mnih. Unsupervised Learning of Object Keypoints for Perception and
Control. Curran Associates Inc., Red Hook, NY, USA, 2019.
Francesco Locatello, Dirk Weissenborn, Thomas Unterthiner, Aravindh Mahendran, Georg Heigold,
Jakob Uszkoreit, Alexey Dosovitskiy, and Thomas Kipf. Object-centric learning with slot atten-
tion. arXiv preprint arXiv:2006.15055, 2020.
Joseph Marino, Yisong Yue, and Stephan Mandt. Iterative amortized inference. arXiv preprint
arXiv:1807.09356, 2018.
Ben Mildenhall, Pratul P. Srinivasan, Matthew Tancik, Jonathan T. Barron, Ravi Ramamoorthi, and
Ren Ng. Nerf: Representing scenes as neural radiance fields for view synthesis. In ECCV, 2020.
11
Under review as a conference paper at ICLR 2022
Li Nanbo, Cian Eastwood, and Robert B Fisher. Learning object-centric representations of multi-
object scenes from multiple views. In 34th Conference on Neural Information Processing Sys-
tems, 2020.
Thu Nguyen-Phuoc, Christian Richardt, Long Mai, Yong-Liang Yang, and Niloy Mitra. Blockgan:
Learning 3d object-aware scene representations from unlabelled images. In Advances in Neural
Information Processing Systems 33, Nov 2020.
Michael Niemeyer and Andreas Geiger. Giraffe: Representing scenes as compositional generative
neural feature fields. In Proc. IEEE Conf. on Computer Vision and Pattern Recognition (CVPR),
2021.
Danilo Rezende and Shakir Mohamed. Variational inference with normalizing flows. In Francis
Bach and David Blei (eds.), Proceedings of the 32nd International Conference on Machine Learn-
ing, volume 37 of Proceedings of Machine Learning Research, pp. 1530-1538, Lille, France,
07-09 JUl 2015. PMLR. URL http://Proceedings .mlr.press∕v37∕rezende15.
html.
Danilo Jimenez Rezende and Fabio Viola. Taming vaes, 2018.
T. Schmidt. Perception: The binding problem and the coherence of perception. In William P.
Banks (ed.), Encyclopedia of Consciousness, pp. 147-158. Academic Press, Oxford, 2009. ISBN
978-0-12-373873-8. doi: https://doi.org/10.1016/B978-012373873-8.00059-1. URL https:
//www.sciencedirect.com/science/article/pii/B9780123738738000591.
Karl Stelzner, Kristian Kersting, and Adam R Kosiorek. Decomposing 3d scenes into objects via
unsupervised volume segmentation. arXiv preprint arXiv:2104.01148, 2021.
Joshua Tobin, Wojciech Zaremba, and Pieter Abbeel. Geometry-aware neural rendering. Advances
in Neural Information Processing Systems, 32:11559-11569, 2019.
Anne M. Treisman and Garry Gelade. A feature-integration theory of attention. Cog-
nitive Psychology, 12(1):97-136, 1980. ISSN 0010-0285. doi: https://doi.org/10.
1016/0010-0285(80)90005-5. URL https://www.sciencedirect.com/science/
article/pii/0010028580900055.
Miguel Vasco, Francisco S. Melo, and Ana Paiva. Mhvae: a human-inspired deep hierarchical
generative model for multimodal representation learning, 2020.
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,
Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. arXiv preprint arXiv:1706.03762,
2017.
Rishi Veerapaneni, John D. Co-Reyes, Michael Chang, Michael Janner, Chelsea Finn, Jiajun Wu,
Joshua B. Tenenbaum, and Sergey Levine. Entity abstraction in visual model-based reinforcement
learning. In CoRL, 2019.
Nicholas Watters, Loic Matthey, Matko Bosnjak, Christopher P. Burgess, and Alexander Lerchner.
COBRA: data-efficient model-based RL through unsupervised object discovery and curiosity-
driven exploration. CoRR, abs/1905.09275, 2019a. URL http://arxiv.org/abs/1905.
09275.
Nicholas Watters, Loic Matthey, Christopher P. Burgess, and Alexander Lerchner. Spatial broad-
cast decoder: A simple architecture for learning disentangled representations in vaes. CoRR,
abs/1901.07017, 2019b. URL http://arxiv.org/abs/1901.07017.
Mike Wu and Noah Goodman. Multimodal generative models for scalable weakly-supervised learn-
ing. In Proceedings of the 32nd International Conference on Neural Information Processing
Systems, pp. 5580-5590, 2018.
Hong-Xing Yu, Leonidas J. Guibas, and Jiajun Wu. Unsupervised discovery of object radiance
fields, 2021.
12
Under review as a conference paper at ICLR 2022
A Network Architecture
A.1 Global Encoder
Type	Size/Channels	Stride	Activation	TyPe	Size	Activation
Input	3			Input	v	
Conv 3×3	32			Linear	128	ReLU
Conv 3×3	32	2	ReLU	Linear	embed-dim	None
Conv 3×3	64					
Conv 3×3	64					
Linear	256		SWish	concatenation of above outputs		
Linear	zg	-	None		reparameterization	
Product of Experts						
Table 3: Architecture of Global Encoder. This table contains convolution network (top left), view-
point embedding (top right), feedforward network.
Global Encoder is composed of CNN, viewpoint embedding, feedforward network for reparameter-
ization and PoE. Output from CNN and viewpoint embedding is concatenated and fed into feedfor-
ward network.
We used Planar Flow in this encoder. The dimension of the flow is same as the dimension of zg, and
the transforms are applied 16 times in this model.
A.2 Sequential Encoder
Type	input dim	hidden dim (output dim)	Activation	Comment
LSTM	z × 2 + zg	256		
Linear	256	z×2	None	parameters of z
Table 4: Architecture of Sequential Encoder.
Sequential Encoder is composed of LSTM and linear layer. Input of LSTM is concatenation of zg
and z’s parameters inferred in last iteration. Initial input is concatenation of zg and zero vector.
A.3 Refinement Network
Type	Size	Activation	Comment
Input	Z X 4 + Zg X 2		
Linear	256	ELU	
Linear	256	None	
LSTM	32 (Z x 2)	-	
Table 5: Architecture of Sequential Encoder.
Refinement Network takes log-likelihood, gradient of the parameters (mean and variance) of zi-1
and the parameters of zg as input, where i means iteration step.
13
Under review as a conference paper at ICLR 2022
A.4 S tructured Prior
Type	size (model dim)	Activation	Comment
Linear	256	ELU	+ positional encoding
Transformer	256	GeLU	Feed forward: 1024 Layers: 3 head: 8
Linear	z×2	None	reparameterization
Table 6: Architecture of Structured Prior.
Structured prior is implemented by Transformer. We used learnable positinal embedding, and ac-
tivation is GeLU (Hendrycks & Gimpel, 2016) as in BERT (Devlin et al., 2019). We did not use
dropout here.
A.5 Broadcast Decoder
Type	Size/Channels	Activation	Comment
Input	z+v		
Linear	512	ReLU	
Linear	z	-	viewpoint projector
Broadcast	z+2		
Conv 3 × 3	32	ReLU	
Conv 3 × 3	32	ReLU	
Conv 3 × 3	32	ReLU	
Conv 3 × 3	32	ReLU	
Conv 1 × 1	4	ReLU	RGB ch. + mask logit
Table 7: Architecture of Broadcast Decoder.
We used same decoder as MulMON. v and z are concatenated. Concatenated vector is input to
projector (former part of the table), then the output is fed into broadcast decoder (latter part of the
table).
14
Under review as a conference paper at ICLR 2022
B Additional Results
B.1 Quantitative Evaluation on Observation
⊃0-E
1 9 87 6 54 857565554
α QQQQQQ 676665646
Ooo Ooo O O O O
CLEVR MV
CLEVR AUG
GQN Jaco
1	2	3	4	5	6
N
0.115
0.105
0.095
0.085
0.075
—OurS
.... Ours: w/o Flow
....MulMON (bassline)
M the number of observations
山m≡α
Figure 7: Quantitative evaluation of novel view synthesis. This figure shows scores about observa-
tion views, namely, reconstruction and segmentation.
Figure 2 in the main text showed quantitative evaluation for query views. In this section, we show
the results for observation views (Figure 7). Namely, this figure is about reconstruction and segmen-
tation quality. The performance of MulMON with small N is relatively not good. This is because
MulMON needs to process observation views iteratively, thus the model requires at least several
observation views to achieve its best performance. On the other hand, WeLIS is stable against N
thanks to the parallel processing of Global Encoder.
B.2	Novel View Synthesis and Segmentation
B.2	. 1 Evaluation Metrics and Qualitative Difference
GT Pred. Seg. GT Pred. Seg.	GT Pred. Seg. GT Pred. Seg.
MuIMON (RMSE: 0.093 mIoU: 0.55)
Figure 8: Qualitative results of novel view synthesis. The number of observation views N is three
in this figure. The left most columns in each group show ground truth image that is not shown
to the model. The second columns in each group show predicted image. The third columns show
segmentation mask.
Figure 8 shows additional quantitative samples of novel view synthesis. This figure helps us to un-
derstand how much quantitative difference of RMSE and mIoU actually correspond to the qualitative
difference.
WeLIS (RMSE: 0.089 mIoU: 0.58)
15
Under review as a conference paper at ICLR 2022
In addition, we can see that WeLIS represents background with a fixed slot (yellow), while MulMON
represents it with different slots (various colors). This is because the auto-regressive encoder is no
longer permutation invariant. It can be beneficial that fixed slot represents background, however,
disentanglement might be sacrificed with this. It is difficult to conclude which one is better currently,
because this kind of properties should be evaluated by various downstream tasks.
B.3	Traversal of global latent space
sample A
sample B
Figure 9: Traversal of global latent space
Figure 9 shows visualization of traversal of global latent space zg . We sample two zg and visu-
alized interpolation of them. The figure shows three different examples. We can see that spatial
arrangement is preserved, but individual components changes quickly.
B.4	Ablation Study
B.4.1	Novel View Synthesis
CLEVR AUG
RMSE I
Uo-Ie>.lθsqo
mIoU ↑
0.65
A-lωnb
—"— full mod el
∙∙∙**-- w/o flow
—-w/o joint training
-■* ,- w/o PoE (mean)
—-—w/o GECO
0.075
1	2
3	4	5	6
0.25
1	2
Figure 10: Ablation study for each component: Global Encoder (PoE), GECO, separate training of
Structured Prior
Figure 10 shows ablation result of Global Encoder (PoE), GECO, separate training of Structured
Prior, and Normalizing Flow. Regarding PoE, we replaced PoE with simple average, which is often
used in many researches such as NeRF-VAE (Kosiorek et al., 2021).
16
Under review as a conference paper at ICLR 2022
This result indicates that each component contributes to the scores and training stability.
B.4.2	Novel Scene Generation
We tested two different architectures for Structured Prior. Both variants use MLP, and one of them
updates each slot independently, while the other one considers interaction between slots. As we
show in Table 8, there is no significant difference between these variants. This result indicates that
zg alone properly models object placement, and Structured Prior does not have to model dependency
between slots.
	Transformer	MLP (w/o interaction)	MLP (w/ interaction)
FID	83.3 ± 1.7	84.35 ± 2.33	84.18 ± 2.61
Table 8: FID score from two different architectures of Structured Prior
B.5	Downstream Task
Models	Datasets	Observations	Accuracy
MulMON			64.6
WeLIS(zg)	CLEVR MV	3	73.0
WeLIS(z)			63.5
WeLIS(zg + z)			71.5
MulMON			75.0
WeLIS(zg)	CLEVR MV	4	76.5
WeLIS(z)			53.5
WeLIS(zg + z)			76
MulMON			77.5
WeLIS(zg)	CLEVR MV	5	76.0
WeLIS(z)			58.0
WeLIS(zg + z)			76.0
MulMON			65.4
WeLIS(zg)	CLEVR AUG	3	63.9
WeLIS(z)			74.5
WeLIS(zg + z)			77.5
MulMON			70.7
WeLIS(zg)	CLEVR AUG	4	68.8
WeLIS(z)			78.7
WeLIS(zg + z)			78.3
MulMON			71.8
WeLIS(zg)	CLEVR AUG	5	69.3
WeLIS(z)			79.3
WeLIS(zg + z)			79.6
Table 9: Accuracy ofa downstream task.Observations stands for the number of available observation
views.
We evaluated the latent space ofzg qualitatively in Figure 6. In this section, we evaluate the obtained
representation by downstream task. The task is to predict how many objects are in the observed
scenes. We used linear classifier in all the models.
WeLIS can use both z and zg for classification task, thus we tested all three patterns: z only, zg only
and both z and zg .
17
Under review as a conference paper at ICLR 2022
C Datasets
CLEVRAUG
GQN JACO
Figure 11: Ground truth samples of the three datasets used in this paper: CLEVR MV, CLEVR AUG
and GQN Jaco.
We show ground truth samples of the three datasets: CLEVR MV, CLEVR AUG and GQN Jaco.
These datasets are available from the official implementation of MulMON (https://github.
com/NanboLi/MulMON).
18