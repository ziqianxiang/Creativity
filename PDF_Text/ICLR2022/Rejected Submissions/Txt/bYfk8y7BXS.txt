Under review as a conference paper at ICLR 2022
Pessimistic Model Selection for Offline Deep
Reinforcement Learning
Anonymous authors
Paper under double-blind review
Ab stract
Deep Reinforcement Learning (DRL) has demonstrated great potentials in solving
sequential decision making problems in many applications. Despite its promising
performance, practical gaps exist when deploying DRL in real-world scenarios.
One main barrier is the over-fitting issue that leads to poor generalizability of the
policy learned by DRL. In particular, for offline DRL with observational data,
model selection is a challenging task as there is no ground truth available for per-
formance demonstration, in contrast with the online setting with simulated envi-
ronments. In this work, we propose a pessimistic model selection (PMS) approach
for offline DRL with a theoretical guarantee, which features a provably effective
framework for finding the best policy among a set of candidate models. Two re-
fined approaches are also proposed to address the potential bias of DRL model
in identifying the optimal policy. Numerical studies demonstrated the superior
performance of our approach over existing methods.
1	Introduction
The success of deep reinforcement learning (Mnih et al., 2013; Henderson et al., 2018) (DRL) often
leverages upon executive training data with considerable efforts to select effective neural architec-
tures. Deploying online simulation to learn useful representations for DRL is not always realistic
and possible especially in some high-stake environments, such as automatic navigation (Kahn et al.,
2018; Hase et al., 2020), dialogue learning (Jaques et al., 2020), and clinical applications (Tang
et al., 2020a). Offline reinforcement learning (Singh & Sutton, 1996; Levine et al., 2020; Agarwal
et al., 2020) (OffRL) has prompted strong interests (Paine et al., 2020; Kidambi et al., 2020) to
empower DRL toward training tasks associated with severe potential cost and risks. The idea of
OffRL is to train DRL models with only logged data and recorded trajectories. However, with given
observational data, designing a successful neural architecture in OffRL is often expensive (Levine
et al., 2020), requiring intensive experiments, time, and computing resources.
Unlike most aforementioned applications with online interaction, Offline tasks for reinforcement
learning often suffer the challenges from insufficient observational data from offline collection to
construct a universal approximated model for fully capturing the temporal dynamics. Therefore,
relatively few attempts in the literature have been presented for provide a pipeline to automate the
development process for model selection and neural architecture search in OffRL settings. Here,
model selection refers to selecting the best model (e.g., the policy learned by a trained neural net-
work) among a set of candidate models (e.g. different neural network hyperparameters).
In this work, we propose a novel model selection approach to automate OffRL development process,
which provides an evaluation mechanism to identify a well-performed DRL model given offline data.
Our method utilizes statistical inference to provide uncertainty quantification on value functions
trained by different DRL models, based on which a pessimistic idea is incorporated to select the
best model/policy. In addition, two refined approaches are further proposed to address the possible
biases of DRL models in identifying the optimal policy. In this work, we mainly focus on deep
Q-network (Mnih et al., 2013; 2015) (DQN) based architectures, while our proposed methods can
be extended to other settings. Figure 1 demonstrates the superior performance of the proposed
pessimistic model selection (PMS) method in identifying the best model among 70 DRL models
of different algorithms on one navigation task (See Appendix C for details), compared with the
model selection method by (Tang & Wiens, 2021) which uses three offline policy evaluation (OPE)
1
Under review as a conference paper at ICLR 2022
(AM) (Voloshin et al., 2019); (d) fitted Q evaluation (FQE) (Le et al., 2019). In this figure, the algorithms
are trained and evaluated in a navigation task (E2) discussed in Section 7 and Appendix C.
estimates for validation. Specifically, based on the derived confidence interval of the OPE value for
each candidate model, the final selected model by our PMS method is the one that has the largest
lower confidence limit, which exactly has the largest true OPE value among all candidate models. In
contrast, none of three OPE estimates used for model selection by Tang & Wiens (2021) can identify
the best model due to the inevitable overfitting issue during the validation procedure.
To close this section, we summarize the contributions of this work as follows:
•	We propose a novel PMS framework, which targets finding the best policy from given candidate
models (e.g., neural architecture, hyperparameters, etc) with offline data for DQN learning. Un-
like many existing methods, our approach essentially does not involve additional hyperparameter
tuning except for two interpretable parameters.
•	Leveraging asymptotic analysis in statistical inference, we provide uncertainty quantification on
each candidate model, based on which our method can guarantee that the worst performance of
finally selected model is the best among all candidate models. See Corollary 1 for more details.
•	To address potential biases of candidate models in identifying the optimal policy, two refined
approaches are proposed, one of which can be shown to have regret bounded by the smallest error
bound among all candidate models under some technical conditions (See Corollary 2). To the best
of our knowledge, this is the first model-selection method in offline DRL with such a guarantee.
•	The numerical results demonstrate that the proposed PMS shows superior performance in different
DQN benchmark environments.
2	Related Work
Model Selection for Reinforcement Learning: Model selection has been studied in online
decision-making environments (Fard & Pineau, 2010; Lee & Taylor, 2014). Searching nearly opti-
mal online model is a critical topic for online bandits problems with limited information feed-backs.
For linear contextual bandits, Abbasi-Yadkori et al. (2011); Chu et al. (2011) are aiming to find
the best worst-case bound when the optimal model class is given. For model-based reinforcement
learning, Pacchiano et al. (2020) introduces advantages of using noise augmented Markov Decision
Processes (MDP) to archive a competitive regret bound to select an individual model with con-
straints for ensemble training. Recently, Lee et al. (2021) utilized an online algorithm to select a
low-complexity model based on a statistical test. However, most of the previous model selection
approaches are focused on the online reinforcement learning setting. Very few works including
Farahmand & Szepesvari (2011); Paine et al. (2020); SU et al. (2020); Yang et al. (2020); Kuzborskij
et al. (2021); Tang & Wiens (2021); Xie & Jiang (2021) are focused on the offline setting. In partic-
ular, (Su et al., 2020; Yang et al., 2020; Kuzborskij et al., 2021) focus on model selection for OPE
problem. (Farahmand & Szepesvari, 2011; Xie & Jiang, 2021) select the best model/Policy based on
minimizing the Bellman error, while the first approach requires an additional tuning and latter does
not. (Paine et al., 2020; Tang & Wiens, 2021) proposed several criteria to perform model selection
in OffRL and mainly focused on numerical studies. In this work, we provide one of the first model
selection approaches based on statistical inference for RL tasks with offline data collection.
Offline-Policy Learning: Training a DRL agent with offline data collection often relies on batch-
wise optimization. Batch-Constrained deep Q-learning (Fujimoto et al., 2019) (BCQ) is considered
one OffRL benchmark that uses a generative model to minimize the distance of selected actions to
the batch-wise data with a perturbation model to maximize its value function. Other popular OffRL
2
Under review as a conference paper at ICLR 2022
approaches, such as behavior regularized actor-critic (BRAC) (Wu et al., 2019), and random ensem-
ble mixture (Agarwal et al., 2020) (REM) (as an optimistic perspective on large dataset), have also
been studied in RL Unplugged (RLU) (Gulcehre et al., 2020) benchmark together with behavior
cloning (Bain & Sammut, 1995; Ross & Bagnell, 2010) (BC), DQN, and DQN with quantile regres-
sion (Dabney et al., 2018) (QR-DQN). RLU suggests a naive approach based on human experience
for offline policy selection, which requires independent modification with shared domain expertise
(e.g., Atari environments) for tuning each baseline. Meanwhile, how to design a model selection
algorithm for OffRL remains an open question. Motivated by the benefits and the challenges as
mentioned earlier of the model selection for offline DRL, we aim to develop a unified approach for
model selection in offline DRL with theoretical guarantee and interpretable tuning parameters.
3	Background and Notations
Consider a time-homogeneous Markov decision process (MDP) characterized by a tuple M =
(S, A, p, r, γ), where S is the state space, A is the action space, p is the transition kernel, i.e.,
p(s0|s, a) is the probability mass (density) of transiting to s0 given current state-action (s, a), r is
the reward function, i.e., E(Rt|St = s, At = a) = r(s, a) for t ≥ 0, and 0 ≤ γ < 1 is a dis-
count factor. For simplifying presentation, we assume A and S are both finite. But our method
can also be applied in continuous cases. Under this MDP setting, it is sufficient to consider sta-
tionary Markovian policies for optimizing discounted sum of rewards (Puterman, 1994). Denote
π as a stationary Markovian policy mapping from the state space S into a probability distribution
over the action space. For example, ∏(a∣s) denotes the probability of choosing action a given the
state value s. One essential goal of RL is to learn an optimal policy that maximizes the value
function. Define Vπ (S) = P+=∞ YtEπ [R"So = s] and then the optimal policy is defined as
∏* ∈ argmax∏{V(∏)，(1 - Y) Ps∈s Vπ(S)V(s)}, where V denotes some reference distribution
function over S. In addition, We denote Q-function as Qn(s, a) = P+=∞ YtEn (Rt ∣A0 = a,S0 = S)
for S ∈ S and a ∈ A. In this work, we consider the OffRL setting. The observed data
consist of N trajectories, corresponding to N independent and identically distributed copies of
{(St, At, Rt)}t≥0. For any i ∈ {1,…，n}, data collected from the ith trajectory can be sum-
marized by {(Si,t, Ai,t, Ri,t, Si,t+1)}0≤t<T, where T denotes the termination time. We assume that
the data are generated by some fixed stationary policy denoted by b.
Among many RL algorithms, we focus on Q-learning type of methods. The foundation is the optimal
Bellman equation given below.
Q*(s,a) = E[Rt + YmaxQ*(St+1,a0) | St = s,At = a],	(1)
a0∈A
where Q* is called optimal Q-function, i.e., Q-function under ∏*. Among others, fitted q-iteration
(FQI) is one of the most popular RL algorithms (Ernst et al., 2005). FQI leverages supervised
learning techniques to iteratively solve the optimal Bellman equation (1) and shows competitive
performance in OffRL.
To facilitate our model-selection algorithm, we introduce the discounted visitation probability, mo-
tivated by the marginal importance sampling estimator in (Liu et al., 2018). For any t ≥ 0, let
ptπ(s, a) denote the t-step visitation probability Prπ(St = s, At = a) assuming the actions are se-
lected according to ∏ at time 1, ∙∙∙ ,t. We define the discounted visitation probability function as
dπ(s, a) = (1 - Y) Pt≥0 Ytptπ(s, a). To adjust the distribution from behavior policy to any target
policy π, we use the discounted probability ratio function defined as
ωπ,ν(s, a) =
dπ(s)π(a∣s)
T pT=01 pb(s,a)
(2)
whereptb(s, a) is the t-step visitation probability under the behavior policy b, i.e., Prb(St = s, At =
a). The ratio function ωπ,ν(s, a) is always assumed well defined. The estimation of ratio function
is motivated by the observation that for every measurable function f defined over S × A,
1 T-1
E[T E ωπ,ν(St,At)(f (St,At) -YE π(a0 | St+ι)f (St+ι,a0))]
t=0	a0∈A
二 (1- y)Eso〜V[X n(a | S0)f(a,S0)],
a∈A
(3)
3
Under review as a conference paper at ICLR 2022
based on which several min-max estimation methods has been proposed such as (Liu et al., 2018;
Nachum et al., 2019; Uehara & Jiang, 2019); We refer to (Uehara & Jiang, 2019, Lemma 1) for a
formal proof of equation (3).
Finally, because our proposed model selection algorithm relies on an efficient evaluation of any
target policy using batch data, we introduce three types of offline policy evaluation estimators in the
existing RL literature. The first type is called direct method via estimating Q-function, based on
the relationship that V(π) = (1 - Y) Ps∈s a∈∕π(a∣s)Q(s, a)ν(s). The second type is motivated
by the importance sampling (Precup, 2000). Based on the definition of ratio function, we can see
V(∏) = E[T PT-01 ωπ,ν(St, At)Rt], from which a plugin estimator can be constructed. The last
type of OPE methods combines the first two types of methods and construct a so-called doubly
robust estimator (Kallus & Uehara, 2019; Tang et al., 2020b). This estimator is motivated by the
efficient influence function of V(π) under a transition-sampling setting and the model that consists
of the set of all observed data distributions given by arbitrarily varying the initial, transition, reward,
and behavior policy distributions, subject to certain minimal regularity and identifiability conditions
(Kallus & Uehara, 2019), i.e.,
1	T-1
T ^ωπ,ν(St,At)(Rt + γf∏(a∣St+ι)Qπ(St+ι,a) - Qn(St,At))
t=0	a∈A
+ (1 - Y)Eso〜V[X π(a∣So)Qπ(So,a)] - V(π).
a∈A
(4)
A nice property of doubly robust estimators is that as long as either the Q-function Qπ (s, a) or
the ratio function ωπ,ν (s, a) can be consistently estimated, the final estimator of V(π) is consistent
(Robins et al., 1994; Jiang & Li, 2015; Kallus & Uehara, 2019; Tang et al., 2020b). Furthermore,
a doubly robust estimator based on (4) can achieve semiparametric efficiency under the conditions
proposed by (Kallus & Uehara, 2019), even if nuisance parameters are estimated via black box
models such as deep neural networks. Therefore such an estimator is particularly suitable under the
framework of DRL. Our proposed algorithm will rely on this doubly robust type of OPE estimator.
4 Pessimistic Model S election (PMS) for Best Policy
In this section, we discuss our pessimistic model selection approach. For the ease of presentation, we
focus on the framework of (deep) Q-learning, where policy optimization is performed via estimating
the optimal Q-function. While this covers a wide range of state-of-the-art RL algorithms such as
FQI (Ernst et al., 2005), DQN (Mnih et al., 2013) and QR-DQN (Dabney et al., 2018), we remark
that our method is not restricted to this class of algorithms.
Suppose we have total number of L candidate models for policy optimization, where each candidate
model will output an estimated policy, say ∏ι for 1 ≤ l ≤ L. Our goal is to select the best policy
among L policies during our training procedure. Note that these L models can be different deep
neural network architectures, hyper-parameters, and various classes of functions for approximating
the optimal Q-function or policy class, etc.
4.1	Difficulties and Challenges
Given a candidate l among L models, we can apply for example FQI using the whole batch data
Dn to learn an estimate of Q* as Ql and an estimated optimal policy ∏ι defined as ∏ι(a∣s) ∈
argmaxa∈AQl(s, a), for every s ∈ S. In order to select the final policy, one may use a naive
〜
〜
^
greedy approach to choose some l such that l ∈ argmaxiEs。〜V[∑a∈A∏ι(a∣s)Qι(So,a)], as our
goal is to maximize V(π). However, using this criterion will lead to over-fitting. Specifically, due
to the distributional mismatch between the behavior policy and target policies, which is regarded
as a fundamental challenge in OffRL (Levine et al., 2020), we may easily overestimate Q-function,
especially when some state-action pairs are not sufficiently visited in the batch data. This issue
becomes more serious when we apply max-operator during our policy optimization procedure. Such
observations have already been noticed in recent works, such as (Kumar et al., 2019; 2020; Paine
et al., 2020; Yu et al., 2020; Tang & Wiens, 2021; Jin et al., 2021). Therefore, it may be inappropriate
to use this criterion for selecting the best policy among L models.
4
Under review as a conference paper at ICLR 2022
One may also use cross-validation procedure to address the issue of over-fitting or overestimating
Q-function for model selection. For example, one can use OPE approaches on the validate dataset
to evaluate the performance of estimated policies from the training data set (see Tang & Wiens
(2021) for more details). However, since there is no ground truth for the value function of any
policies, the OPE procedure on the validation dataset cannot avoid involving additional tuning on
hyperparameters. Therefore such a procedure may still incur a large variability due to the over-
fitting issue. In addition, arbitrarily splitting the dataset for cross-validation and ignoring the Markov
dependent structure will cause additional errors, which should be seriously taken care of.
4.2	Sequential Model Selection
In the following, we propose a pessimistic model selection algorithm for finding an optimal policy
among L candidate models. Our goal is to develop an approach to estimate the value function
under each candidate model during our policy optimization procedure with theoretical guarantee.
The proposed algorithm is motivated by recent development in statistical inference of sequential
decision making (Luedtke & Van Der Laan, 2016; Shi et al., 2020). The idea is to first estimate
optimal Q-function Q*, optimal policy ∏* and the resulting ratio function based on a chunk of data,
and evaluate the performance of the estimated policy on the next chunk of data using previously
estimated nuisance functions. Then we combine the first two chunks of data, perform the same
estimation procedure and evaluation on the next chunk of data. The framework of MDP provides a
nature way of splitting the data.
Specifically, denote the index of our batch dataset Dn as J0 = {(i, t) : 1 ≤ i ≤ n, 0 ≤ t < T }. We
divide J into O number of non-overlapping subsets, denoted by Ji,…，Jo and the corresponding
data subsets are denoted by Di, ∙∙∙ , DO. Without loss of generality, We assume these data subsets
have equal size. We require that for any 1 ≤ o1 < o2 ≤ O, any (i1, t1) ∈ Jo1 and (i2, t2) ∈
J02, either %? = i1 or t1 < t2. For 1 ≤ o ≤ O, denote the aggregate chunks of data as Do =
{(Si,t, Ai,t, Ri,t, Si,t+i), (i, t) ∈ Jo = J1 ∪∙∙∙∪ Jo}.
We focus on FQI algorithm for illustrative purpose and it should be noticed that our algorithm can
be applied to other RL algorithms. Starting from the first chunk of our batch data, at the o-th step
(o = 1,…，O - 1), for each candidate model l = 1, ∙∙∙ ,L, we apply FQI on Do to compute
Q(O) as an estimate of optimal Q-function and obtain n(o) correspondingly such that ∏(o)(a∣s) ∈
argmaxs∈AQbl(o) (s, a) for every s ∈ S. Additionally, we compute an estimate of ratio function
(o)
ωπl ,ν using Do by many existing algorithms such as Nachum et al. (2019). Denote the resulting
(o)
estimator as ωbπl ,ν. The purpose of estimating this ratio function is to improve the efficiency
and robustness of our value function estimation for each candidate model. Then we compute the
estimated value function of ∏(o) on Do+i as
VDo+ι (∏(°)) = (1-γ)Eso~ν[ X ∏(o)(a0∣So)<Q(o)(S0,ao)]	(5)
a0∈A
+Edo+i [b*"ν(S,A)(R + Y X ∏(o)(a0∣S0)Q(o)(S0,a0) - Q(o)(S,A))],	(6)
a0∈A
where EDo+1 denotes the empirical average over the (o + 1) chunk of dataset and (S, A, R, S0) is
one transition tuple in Do+i. While one can aggregate Vdo+i (∏(o)) for 1 ≤ o ≤ (O - 1) to evaluate
the performance of L models, the uncertainty of these estimates due to the finite sample estima-
tion should not be ignored. Therefore, in the following, we derive an uncertainty quantification of
our estimated value function for each candidate model, for performing model selection. Based on
equation (4), (conditioning on D°), the variance of Vdo+i (∏(o)) is
σ2(∏(o)) = E[{ω*(*ν(S,A)(R + γ X π(o)(a0|S0)<b(o)(S0,a0) - Q(O)(S,A))}2],	(7)
a0∈A
where (S, A, S0) is a transition tuple with (S, A) follows some stationary distribution. See Assump-
tion 1. Correspondingly we have an estimate defined as
σo+ι(∏(o)) = EDo+ι[{ω*(°),ν(S,A)(R + Y X Π(o)(a0∣S0)Q;(O)(S0,a0) - Q；(O)(S,A))}2].	(8)
a0∈A
5
Under review as a conference paper at ICLR 2022
The estimation procedure stops once we have used all our offline data and denote the final estimated
policy as ∏ι for each l = 1,…，L. Notice that ∏ι = Π(O). Finally, We compute the weighted average
of all the intermediate value functions as our final evaluation of the estimated policy ∏ι, i.e.,
V(∏ι) = (X1 —⅛τ 厂 1(X1 VDo+1 呼).	⑼
o=1 σo+1 (n(O))	o=1 σo+l(π(O))
In Section 5, we show that under some technical conditions, the following asymptotic result holds:
√nT(O - 1)/O (V(∏ι) - V(∏Q
--------------卡---------------==⇒ N(0,1),	(10)
σ(l)
where σ(l) = (O - 1)(Po=-11{σ0+ι(∏(O))}-1)-1, =⇒ refers to weak convergence when either n
or T goes to infinity, and N(0, 1) refers to the standard normal distribution. Based on the asymp-
totic result in (10), we can construct a confidence interval for the value function of each policy ∏ι.
Given a confidence level a, for each l, we can compute U(l) = V(∏ι) - Za/2 yzO∕nT(O - 1)σ(l),
where Za/2 is (1 - 2)-quantile of the standard normal distribution. Our final selected one is
l ∈ argmax1≤l≤L U(l).
The use of U(l) is motivated by the recent proposed pessimistic idea to address the overestimation
issue of value (or Q) function in the OffRL setting. See Kumar et al. (2019; 2020); Jin et al. (2021);
Xie et al. (2021); Uehara & Sun (2021); Zanette et al. (2021) for details. The final output of our
algorithm is ∏^ and an outline of the proposed algorithm can be found in Algorithm 1. As we can see,
our algorithm is nearly tuning free, which provides great flexibility in real-world applications. The
only two adjustable parameters is O and α. The size of O balances the computational cost and the
finite-sample accuracy of evaluating each candidate model. In specific, we can indeed show that the
variance of the estimated value function by our algorithm can achieve the semi-parametric efficiency
bound, which is best one can hope for. So in the asymptotic sense, the effect of O is negligible. In
the finite-sample setting, we believe the performance will be discounted by a factor，O - I/O.
Therefore, if O is large enough,，O - I/O will have a mere effect on the performance. See
Theorem 1. However, using large O will result in a large computational cost. As a sacrifice for
the nearly tuning free algorithm, we need to apply OffRL algorithms O times for each candidate
model. The parameter α determines how worst the performance of each policy we should use to
evaluate each policy. See Corollary 1 for more insights.
Algorithm 1: Pessimistic Model Selection (PMS) for OffRL
Input: Dataset Dn and L candidate models for estimating optimal Q-function and policy; We
divide Dn into non-overlapping subsets denoted by Di,…，DO. We require that for
any 1 ≤ o1 < o2 ≤ O, any (i1, t1) ∈ Jo1 and (i2, t2) ∈ Jo2, either i2 6= i1 ort1 ≤ t2.
1	for l ∈ L do
2	for o = 1 to O - 1 do
3	For l ∈ L models, construct the optimal Q(o) and ∏(o) using Do data subset.
4	Compute ωπ(o),ν using Do by Nachum et al. (2019) and min-max solver for (3).
5	Compute Vdo+i (∏(o)) and σO+ι(l) using Do+i given in (5) and (8) respectively.
6	For l-th model, we compute U(l) = V(∏ι) - za∕2,nT(O - 1"Oσ(l), where V(∏ι) and
σ(l) are given in (9) and (10) respectively.
7	Pick l = arg maxi U(l) as the selected model and run the algorithm on full dataset to obtain ∏^.
8 Return π^.
5 Theoretical Results
In this section, we justify our asymptotic result given in (10). We use Op to denote the stochastic
boundedness. Before that, we make several technical assumptions:
Assumption 1 The stochastic process {At, St}t≥0 is stationary with stationary distribution p∞.
Assumption 2 For every 1 ≤ l ≤ L and 1 ≤ o ≤ O, we have E|V (∏(o))-V (∏*)∣ ≤ Co(nT∕O)-κ,
l
for some constant C0 and κ > 1/2.
6
Under review as a conference paper at ICLR 2022
Assumption 1 is standard in the existing literature such as (Kallus & Uehara, 2019). Assumption
2 is key to our developed asymptotic results developed. This assumption essentially states that all
candidate models are good enough so that eventually their value functions will converge to that of the
optimal one. This implies that there is no asymptotic bias in identifying the optimal policy. While
this is reasonable thanks to the capability of deep neutral networks, which has demonstrated their
empirical success in many RL applications, such an assumption could still be strong. In Section 6,
we try to relax this assumption and provide two remedies for addressing possibly biased estimated
policies. In addition, Assumption 1 also requires that the convergence rates of value functions under
estimated policies are fast enough. This has been shown to hold under the margin condition on ∏*,
see e.g., (Hu et al., 2021) for more details.
Assumption 3 For every 1 ≤ l ≤ L and 1 ≤ o ≤ O — 1, suppose E(s,a)〜p∞ |Q(O)(S,A)—
Qπ(o) (S,A)∣2 = Op{(nT/O)-I2κ1} for some constant κι ≥ 0. In addition, Q(O) is uniformly
bounded almost surely.
Assumption 4 For every 1 ≤ l ≤ L and 1 ≤ o ≤ O — 1, suppose E(s,a)^p∞ ∣ωπι ,ν (S, A)—
ωπ(),v(S,A)∣2 = Op{(nT∕O)-2κ2} for some constant κ∣ ≥ 0. In addition, both ωπ(),v and
(o)
ωbπl ,ν are uniformly bounded above and below away from 0 almost surely.
Assumption 5 For every 1 ≤ l ≤ L and 1 ≤ o ≤ O — 1, σ2(∏(o)) and σO+ι(π(o)) are bounded
above and below from 0 almost surely.
Assumptions 3 and4 impose high-level conditions on two nuisance functions. Our theoretical results
only require κ1 + κ2 > 1/2, which is a mild assumption. For example, if considered parametric
models for both Q-function and ratio function, then κ1 = κ2 = 1/2. If considered nonparametric
models for these two nuisance functions such as deep neural networks, then 1/4 < κ1, κ2 < 1/2 can
be obtained under some regularity conditions. See Fan et al. (2020) and Liao et al. (2020); Uehara
et al. (2021) for the convergence rates of Q-function and ratio function by non-parametric models
respectively. In addition, Assumption 5 is a mild assumption, mainly for theoretical justification.
Then we have the following main theorem as a foundation of our proposed algorithm.
Theorem 1 Under Assumptions 1-5, we have
(PnT(O-1)/O (V(∏ι) - V(∏1 )))∕σ(l) =⇒ N(0,1).	(11)
Theorem 1 provides an uncertainty quantification of each candidate model used in policy optimiza-
tion. Such uncertainty quantification is essential in OffRL as data are often limited. We highlight
the importance of such results in Appendix A. A consequent result following Theorem 1 validates
the proposed Algorithm 1:
Corollary 1 liminf Pr(V(Π^) ≥ maxι≤ι≤L V(∏ι) — 2zɑ∕2pnT(O — 1)/Oσ(l)) ≥ 1 — La under
nT→∞	l
Assumptions 1-5.
As can be seen clearly from Corollary 1 and the proposed PMS, with a large probability (by letting
a small), We consider the worst performance of each candidate model ∏ι in the sense of the lower
confidence limit of the value function, and then select the best one among all models.
6	Two Refined Approaches
In this section, we relax Assumption 2 by allowing possibly non-negligible bias in estimating the
optimal policy and introduce two refined approaches for addressing this issue. Instead of imposing
Assumption 2, we make an alternative assumption below.
Assumption 6 For every 1 ≤ l ≤ L, there exists B(l) such that maxι≤o≤(o-i) |V(π(o))—
V(π*)∣ ≤ B(DalmOStSurely.
Assumption 6 is a very mild assumption. It essentially states that the biases for all our intermediate
value function estimates are bounded by some constant, which is much weaker than Assumption 2.
In this case, the asymptotic results in (11) may not hold in general. Correspondingly, we have the
following result.
Theorem 2 Under Assumptions 1, 3-6, for every 1 ≤ l ≤ L, the following inequality holds.
IiminfPr (∣V(∏*) - V(∏ι)∣ ≤ Zα∕2√O∕nT(O一1)σ(l) + B(l)) ≥ 1 - α.	(12)
nT→∞
7
Under review as a conference paper at ICLR 2022
Motivated by Lepski’s principle (Lepski & Spokoiny, 1997) from nonparametric statistics and (Su
et al., 2020) studying the model selection of OPE, we consider the following refined model-selection
procedure to find the best policy. We first rank L candidate models in an non-increasing order based
on the value of σ(l), i.e., for 1 ≤ i < j ≤ L, σ(i) ≥ σ(j). Then for i-th model, We construct an
interval as I(l) = [V(∏ι) - 2zɑ∕(2L) pO/nT(O - 1)σ(l), V(∏ι) + 2zα∕(2L) pO/nT(O - 1)σ(l)].
Finally the optimal model/policy we choose is ∏ such that i = max{i :1 ≤ i ≤ L, ∩ι≤j≤iI(j)=
0}. To show this procedure is valid, we need to make one additional assumption.
Assumption 7 There exists a Z < 1 such that for 1 ≤ i ≤ L, B(i) ≤ B(i + 1) and Zσ(i) ≤
σ(i + 1) ≤ σ(i) almost surely.
This assumption is borrowed from Su et al. (2020). It typically assumes that after sorting our model
based on σ(l), the bias of estimated policy is monotonically increasing and the standard deviation is
monotonically deceasing but not too quickly. This is commonly seen when all candidate estimators
exhibit some bias-variance trade-off phenomena. Define the following event
E = ∣V(∏i) - V(∏*)1 ≤ 6(1 + ζ-1) 1m≤IL{B(i) + Zα∕(2L)√O∕nT(O - 1)σ(i)}.	(13)
Then we have the following theoretical guarantee for our refined procedure.
Corollary 2 Under Assumptions 1, 3-7, we have lim inf Pr(E) ≥ 1 - α. If we further assume
nT→∞
.ι . r	c 、 C ♦,?	ι ι ∙ι∙. . ι . i c r	r ， ∙ ， T I ʌ 4ι / ^ ∖ ∙∖^4ι / ^ ∖ I J
that for any δ > 0, with probability at least 1 一 δ, for ^very 1 ≤ i ≤ L, |V(∏i) - V(∏i)∣ ≤
c(δ) log(L)σ(i)∕√NT for some constant c(δ), then lim inf Pr(E) ≥ 1 — α — δ, where
nT→∞
S = ∣V(∏i) - V(∏*)| ≤ 3(1 + ZT) min {B(i) + (c(δ)log(L) + Zα∕(2L))√O∕nT(O - 1)σ(i)}. (14)
1≤i≤L
The additional assumption (i.e., the high probability bound) in Corollary 2 can be shown to hold by
the empirical process theory under some technical conditions (Van de Geer, 2000). Hence Corollary
2 provides a strong guarantee that the regret of the final selected policy is bounded by the smallest
error bound among all L candidate policies. Note that Assumption 3 imposed here could be strong.
Another refined approach: Notice that the above refined approach indeed focuses on OPE esti-
mates to select the best policy with regret warranty. The rough motivation behind is to find a policy
that has the smallest estimation error to the optimal one. However, such procedure may not directly
match the goal of maximizing the value function in OffRL. To relieve this issue , we can alternatively
^
1	,1	1 ∙	^	1,1, ∙	A 1 / ^ ∖ rʌ	/ m / √^ι	T∖^^/ √^ι ^ / ∙ ∖	1
choose the final policy as π^ such that i = argmaxι≤i≤iV(∏i) 一 2zα∕2 VZnT(O 一 1)∕Oσ(i), where
the argmax is taken over i models. This approach can be viewed as a combination of PMS and the
above refined approach. By adopting this approach, candidate models with large biases are firtly
removed by the truncation on i. Then, we use the idea of PMS to select the best model having the
best worst performance among the remaining candidates. Unfortunately, we do not have theoretical
guarantee for this combined approach.
7	Experimental Results
We select six DQN environments (E1 to E6) from open-source benchmarks (Brockman et al., 2016;
Juliani et al., 2018) to conduct numerical experiments, as shown in Fig. 5 of Appendix C. These tasks
of deployed environments cover different domains that include tabular learning (Fig 5(a)); automatic
navigation in a geometry environment with a physical ray-tracker (Fig 5(b)); Atari digital gaming
(Fig 5(c) and (d)), and continuous control (Fig 5(e) and (f)). We provide detailed task description
and targeted reward for each environment in Appendix C.
Experiment setups. To evaluate the performance of PMS with DQN models in offRL, we choose
different neural network architectures under five competitive DRL algorithms including DQN by
(Mnih et al., 2013; 2015), BCQ by (Fujimoto et al., 2019), BCby (Bain & Sammut, 1995; Ross &
Bagnell, 2010), BRAC by (Wu et al., 2019) from RLU benchmarks, and REM by (Agarwal et al.,
2020). Within each architecture, 70 candidate models are created by assigning different hyperpa-
rameters and training setups. See Appendix C.1 for details. We then conduct performance evaluation
of different OffRL model selection methods on these generated candidate models.
Evaluation procedure. We utilize validation scores from OPE for each model selection algorithm,
which picks the best (or a good) policy from the candidate set of size L based on its own criterion.
8
Under review as a conference paper at ICLR 2022
■ best selected policy π
0.0 ~I----,-----,----1-----,----1----,-----,----,---⅛-1
DQN PMS-DQN BC PMS-BC BCQ PMS-BCQ BRAC PMS-BRAC REM PMS-REM
Figure 2:	Box plots of model selection performance
from offline learning in each DRL algorithms for E2.
W”
02-
αo- .	.	.
58	Ik	2k	4k
Num episodes
Figure 3:	Sensitivity analysis for different training
data size. PMS attains the best performance and has
the least sensitivity.
Regret is used as the evaluation metric for each candidate. The regret for model l is defined as
V (∏ι*) - V (∏ι), where l = argmax“=i…L V (n“) corresponds to the candidate policy with the
best OPE validation performance. In our implementation, We treat ∏ι* as ∏*,the oracle but unknown
best possible policy. A small regret is desirable after model selection. Note the optimal regret is not
zero since We can only use data to obtain ∏ι instead of ∏ι for each model. We provide additional
top-k regret and precision results from Figure 6 to 16 in Appendix C.
Performance comparison. As highlighted in Fig. 1 in the introduction, we report estimated OPE
values by different model selection approaches, i.e. PMS and three methods by (Tang & Wiens,
2021), versus the true OPE values. In this experiment, we consider 70 DQN models under the
above mentioned five DRL algorithms, i.e., 14 models are considered for each architecture. We
use fewer models for each DRL algorithm mainly for clear presentation. By using the confidence
interval constructed by our PMS procedure, our method is able to correctly select the top models,
while the other three methods fail. To further investigate the performance of PMS, we implement
model selection among 70 models within each DRL algorithm separately. Fig. 2 shows the box
plots of averaged regret over six environments after OPE per neural network architecture. Each
subfigure contains results from one particular DRL algorithm with different hyperparameters or
training setups. The left box plot refers to the regrets of all 70 models and the right one represents
the regrets of top 10% models selected by the proposed PMS method. Note that the right box plot is
a subset of the left one. The results show that our proposed PMS successfully helps to select models
with the best policies and improve the average regret by a significant margin. In particular, PMS-
REM-based models attain the lowest regrets, due to the benefit from its ensemble process. Detailed
results for each environment is given in Appendix C.
Sensitivity analysis. Fig. 3 compares different selection algorithms
with varying training data size. PMS outperforms others across all
scales, and larger number of episodes gives smaller variation and
lower sensitivity.
PMS algorithm with refinements. We replicate our experiments
on in the offline navigation task in E2 (Banana Collector) for 30
times and report regrets of top 10% models selected by PMS and Figure 4: PMS and its refine-
two refinements in Fig. 4. As we can see, while the overall per- ments (R1/R2).
formances of the proposed three model selection methods are sim-
ilar, two refined approaches have better regrets than PMS in terms of median, demonstrating their
promising performances in identifying the best model. OPE results have been also evaluated also in
DRL tasks with E1 and E3 to E6, where the refinement algorithms (PMS R1/R2) have only a small
relative ± 0.423 % performance difference compared to its original PMS setups.
ðɪPMS-R
4 3 2 1 0
fto.Qfto.
4θ-lσ)3H
8	Conclusion
We propose a new theory-driven model selection framework (PMS) for offline deep reinforcement
learning based on statistical inference. The proposed pessimistic mechanism is warrants that the
worst performance of the selected model is the best among all candidate models. Two refined ap-
proaches are further proposed to address the biases of DRL models. Extensive experimental results
on six DQN environments with various network architectures and training hyperparameters demon-
strate that our proposed PMS method consistently yields improved model selection performance
over existing baselines. The results suggest the effectiveness of PMS as a powerful tool toward
automating model selection in offline DRL.
9
Under review as a conference paper at ICLR 2022
References
Yasin Abbasi-Yadkori, David Pal, and Csaba Szepesvari. Improved algorithms for linear stochastic
bandits. Advances in neural information processing Systems, 24:2312-2320, 2011.
Rishabh Agarwal, Dale Schuurmans, and Mohammad Norouzi. An optimistic perspective on offline
reinforcement learning. In International Conference on Machine Learning, pp. 104-114. PMLR,
2020.
Michael Bain and Claude Sammut. A framework for behavioural cloning. In Machine Intelligence
15, pp. 103-129, 1995.
Gabriel Barth-Maron, Matthew W Hoffman, David Budden, Will Dabney, Dan Horgan, TB Dhruva,
Alistair Muldal, Nicolas Heess, and Timothy Lillicrap. Distributed distributional deterministic
policy gradients. In International Conference on Learning Representations, 2018.
Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman, Jie Tang, and
Wojciech Zaremba. Openai gym. arXiv preprint arXiv:1606.01540, 2016.
Wei Chu, Lihong Li, Lev Reyzin, and Robert Schapire. Contextual bandits with linear payoff func-
tions. In Proceedings of the Fourteenth International Conference on Artificial Intelligence and
Statistics, pp. 208-214. JMLR Workshop and Conference Proceedings, 2011.
Will Dabney, Mark Rowland, Marc Bellemare, and Remi Munos. Distributional reinforcement
learning with quantile regression. In Proceedings of the AAAI Conference on Artificial Intelli-
gence, volume 32, 2018.
Damien Ernst, Pierre Geurts, Louis Wehenkel, and L. Littman. Tree-based batch mode reinforce-
ment learning. Journal of Machine Learning Research, 6:503-556, 2005.
Jianqing Fan, Zhaoran Wang, Yuchen Xie, and Zhuoran Yang. A theoretical analysis of deep q-
learning. In Learning for Dynamics and Control, pp. 486-489. PMLR, 2020.
Amir-massoud Farahmand and Csaba Szepesvari. Model selection in reinforcement learning. Ma-
chine learning, 85(3):299-332, 2011.
M Fard and Joelle Pineau. Pac-bayesian model selection for reinforcement learning. Advances in
Neural Information Processing Systems, 23:1624-1632, 2010.
David A Freedman. On tail probabilities for martingales. the Annals of Probability, pp. 100-118,
1975.
Scott Fujimoto, David Meger, and Doina Precup. Off-policy deep reinforcement learning without
exploration. In International Conference on Machine Learning, pp. 2052-2062. PMLR, 2019.
Omer Gottesman, Fredrik Johansson, Joshua Meier, Jack Dent, Donghun Lee, Srivatsan Srinivasan,
Linying Zhang, Yi Ding, David Wihl, Xuefeng Peng, et al. Evaluating reinforcement learning
algorithms in observational health settings. arXiv preprint arXiv:1805.12298, 2018.
Caglar Gulcehre, Ziyu Wang, Alexander Novikov, Tom Le Paine, Sergio Gomez Colmenarejo, Kon-
rad Zolna, Rishabh Agarwal, Josh Merel, Daniel Mankowitz, Cosmin Paduraru, et al. Rl un-
plugged: Benchmarks for offline reinforcement learning. arXiv e-prints, pp. arXiv-2006, 2020.
Hannes Hase, Mohammad Farid Azampour, Maria Tirindelli, Magdalini Paschali, Walter Simson,
Emad Fatemizadeh, and Nassir Navab. Ultrasound-guided robotic navigation with deep reinforce-
ment learning. In 2020 IEEE/RSJ International Conference on Intelligent Robots and Systems
(IROS), pp. 5534-5541. IEEE, 2020.
Peter Henderson, Riashat Islam, Philip Bachman, Joelle Pineau, Doina Precup, and David Meger.
Deep reinforcement learning that matters. In Proceedings of the AAAI conference on artificial
intelligence, volume 32, 2018.
Yichun Hu, Nathan Kallus, and Masatoshi Uehara. Fast rates for the regret of offline reinforcement
learning. arXiv preprint arXiv:2102.00479, 2021.
10
Under review as a conference paper at ICLR 2022
Natasha Jaques, Judy Hanwen Shen, Asma Ghandeharioun, Craig Ferguson, Agata Lapedriza, Noah
Jones, Shixiang Gu, and Rosalind Picard. Human-centric dialog training via offline reinforcement
learning. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language
Processing (EMNLP),pp. 3985-4003, 2020.
Nan Jiang and Lihong Li. Doubly robust off-policy value evaluation for reinforcement learning.
arXiv preprint arXiv:1511.03722, 2015.
Ying Jin, Zhuoran Yang, and Zhaoran Wang. Is pessimism provably efficient for offline rl? In
International Conference on Machine Learning, pp. 5084-5096. PMLR, 2021.
Arthur Juliani, Vincent-Pierre Berges, Ervin Teng, Andrew Cohen, Jonathan Harper, Chris Elion,
Chris Goy, Yuan Gao, Hunter Henry, Marwan Mattar, et al. Unity: A general platform for intelli-
gent agents. arXiv preprint arXiv:1809.02627, 2018.
Gregory Kahn, Adam Villaflor, Bosen Ding, Pieter Abbeel, and Sergey Levine. Self-supervised deep
reinforcement learning with generalized computation graphs for robot navigation. In 2018 IEEE
International Conference on Robotics and Automation (ICRA), pp. 5129-5136. IEEE, 2018.
Nathan Kallus and Masatoshi Uehara. Efficiently breaking the curse of horizon: Double reinforce-
ment learning in infinite-horizon processes. arXiv preprint arXiv:1909.05850, 2019.
Rahul Kidambi, Aravind Rajeswaran, Praneeth Netrapalli, and Thorsten Joachims. Morel: Model-
based offline reinforcement learning. In NeurIPS, 2020.
Aviral Kumar, Justin Fu, Matthew Soh, George Tucker, and Sergey Levine. Stabilizing off-policy
q-learning via bootstrapping error reduction. In Advances in Neural Information Processing Sys-
tems, pp. 11784-11794, 2019.
Aviral Kumar, Aurick Zhou, George Tucker, and Sergey Levine. Conservative q-learning for offline
reinforcement learning. arXiv preprint arXiv:2006.04779, 2020.
Ilja Kuzborskij, Claire Vernade, Andras Gyorgy, and Csaba Szepesvari. Confident off-policy eval-
uation and selection through self-normalized importance weighting. In International Conference
on Artificial Intelligence and Statistics, pp. 640-648. PMLR, 2021.
Hoang Le, Cameron Voloshin, and Yisong Yue. Batch policy learning under constraints. In Inter-
national Conference on Machine Learning, pp. 3703-3712, 2019.
Jason D Lee and Jonathan E Taylor. Exact post model selection inference for marginal screening.
In Proceedings of the 27th International Conference on Neural Information Processing Systems-
Volume 1, pp. 136-144, 2014.
Jonathan Lee, Aldo Pacchiano, Vidya Muthukumar, Weihao Kong, and Emma Brunskill. Online
model selection for reinforcement learning with function approximation. In International Con-
ference on Artificial Intelligence and Statistics, pp. 3340-3348. PMLR, 2021.
Oleg V Lepski and Vladimir G Spokoiny. Optimal pointwise adaptive methods in nonparametric
estimation. The Annals of Statistics, pp. 2512-2546, 1997.
Sergey Levine, Aviral Kumar, George Tucker, and Justin Fu. Offline reinforcement learning: Tuto-
rial, review, and perspectives on open problems. arXiv preprint arXiv:2005.01643, 2020.
Peng Liao, Zhengling Qi, and Susan Murphy. Batch policy learning in average reward markov
decision processes. arXiv preprint arXiv:2007.11771, 2020.
Qiang Liu, Lihong Li, Ziyang Tang, and Dengyong Zhou. Breaking the curse of horizon: Infinite-
horizon off-policy estimation. In Advances in Neural Information Processing Systems, pp. 5356-
5366, 2018.
Alexander R Luedtke and Mark J Van Der Laan. Statistical inference for the mean outcome under a
possibly non-unique optimal treatment strategy. Annals of statistics, 44(2):713, 2016.
Peter Mathe. The lepskii principle revisited. Inverse problems, 22(3):L11, 2006.
11
Under review as a conference paper at ICLR 2022
Donald L McLeish. Dependent central limit theorems and invariance principles. the Annals of
Probability, 2(4):620-628,1974.
Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Alex Graves, Ioannis Antonoglou, Daan Wier-
stra, and Martin Riedmiller. Playing atari with deep reinforcement learning. arXiv preprint
arXiv:1312.5602, 2013.
Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Veness, Marc G Belle-
mare, Alex Graves, Martin Riedmiller, Andreas K Fidjeland, Georg Ostrovski, et al. Human-level
control through deep reinforcement learning. nature, 518(7540):529-533, 2015.
Ofir Nachum, Yinlam Chow, Bo Dai, and Lihong Li. Dualdice: Behavior-agnostic estimation of
discounted stationary distribution corrections. In Advances in Neural Information Processing
Systems, pp. 2315-2325, 2019.
Aldo Pacchiano, Philip Ball, Jack Parker-Holder, Krzysztof Choromanski, and Stephen Roberts. On
optimism in model-based reinforcement learning. arXiv preprint arXiv:2006.11911, 2020.
Tom Le Paine, Cosmin Paduraru, Andrea Michi, Caglar Gulcehre, Konrad Zolna, Alexander
Novikov, Ziyu Wang, and Nando de Freitas. Hyperparameter selection for offline reinforcement
learning. arXiv preprint arXiv:2007.09055, 2020.
Doina Precup. Eligibility traces for off-policy policy evaluation. Computer Science Department
Faculty Publication Series, pp. 80, 2000.
Martin L Puterman. Markov Decision Processes: Discrete Stochastic Dynamic Programming. John
Wiley & Sons, Inc., 1994.
James M. Robins, Andrea Rotnitzky, and Lue Ping Zhao. Estimation of regression coefficients
when some regressors are not always observed. Journal of the American Statistical Associa-
tion, 89(427):846-866, 1994. ISSN 01621459. URL http://www.jstor.org/stable/
2290910.
StePhane Ross and Drew Bagnell. Efficient reductions for imitation learning. In Proceedings ofthe
thirteenth international conference on artificial intelligence and statistics, pp. 661-668. JMLR
WorkshoP and Conference Proceedings, 2010.
Tom Schaul, John Quan, Ioannis Antonoglou, and David Silver. Prioritized exPerience rePlay. arXiv
preprint arXiv:1511.05952, 2015.
Chengchun Shi, Sheng Zhang, Wenbin Lu, and Rui Song. Statistical inference of the value function
for reinforcement learning in infinite horizon settings. arXiv preprint arXiv:2001.04515, 2020.
Noah Siegel, Jost Tobias SPringenberg, Felix BerkenkamP, Abbas Abdolmaleki, Michael Neunert,
Thomas LamPe, Roland Hafner, Nicolas Heess, and Martin Riedmiller. KeeP doing what worked:
Behavior modelling Priors for offline reinforcement learning. In International Conference on
Learning Representations, 2019.
Satinder P Singh and Richard S Sutton. Reinforcement learning with rePlacing eligibility traces.
Machine learning, 22(1):123-158, 1996.
Yi Su, Pavithra Srinath, and Akshay Krishnamurthy. AdaPtive estimator selection for off-Policy
evaluation. In International Conference on Machine Learning, PP. 9196-9205. PMLR, 2020.
ShengPu Tang and Jenna Wiens. Model selection for offline reinforcement learning: Practical con-
siderations for healthcare settings. arXiv preprint arXiv:2107.11003, 2021.
ShengPu Tang, Aditya Modi, Michael Sjoding, and Jenna Wiens. Clinician-in-the-looP decision
making: Reinforcement learning with near-oPtimal set-valued Policies. In International Confer-
ence on Machine Learning, PP. 9387-9396. PMLR, 2020a.
Ziyang Tang, Yihao Feng, Lihong Li, Dengyong Zhou, and Qiang Liu. Doubly robust bias reduction
in infinite horizon off-Policy estimation. In International Conference on Learning Representa-
tions, 2020b.
12
Under review as a conference paper at ICLR 2022
Masatoshi Uehara and Nan Jiang. Minimax weight and q-function learning for off-policy evaluation.
arXiv preprint arXiv:1910.12809, 2019.
Masatoshi Uehara and Wen Sun. Pessimistic model-based offline rl: Pac bounds and posterior
sampling under partial coverage. arXiv preprint arXiv:2107.06226, 2021.
Masatoshi Uehara, Masaaki Imaizumi, Nan Jiang, Nathan Kallus, Wen Sun, and Tengyang Xie.
Finite sample analysis of minimax offline reinforcement learning: Completeness, fast rates and
first-order efficiency. arXiv preprint arXiv:2102.02981, 2021.
Sara Van de Geer. Empirical Processes in M-estimation, volume 6. Cambridge university press,
2000.
Hado Van Hasselt, Arthur Guez, and David Silver. Deep reinforcement learning with double q-
learning. In Proceedings of the AAAI conference on artificial intelligence, volume 30, 2016.
Cameron Voloshin, Hoang M Le, Nan Jiang, and Yisong Yue. Empirical study of off-policy policy
evaluation for reinforcement learning. arXiv preprint arXiv:1911.06854, 2019.
Yifan Wu, George Tucker, and Ofir Nachum. Behavior regularized offline reinforcement learning.
arXiv preprint arXiv:1911.11361, 2019.
Tengyang Xie and Nan Jiang. Batch value-function approximation with only realizability. In Inter-
national Conference on Machine Learning,pp. 11404-11413. PMLR, 2021.
Tengyang Xie, Ching-An Cheng, Nan Jiang, Paul Mineiro, and Alekh Agarwal. Bellman-consistent
pessimism for offline reinforcement learning. arXiv preprint arXiv:2106.06926, 2021.
Chao-Han Huck Yang, I Hung, Te Danny, Yi Ouyang, and Pin-Yu Chen. Causal inference q-network:
Toward resilient reinforcement learning. arXiv preprint arXiv:2102.09677, 2021.
Mengjiao Yang, Bo Dai, Ofir Nachum, George Tucker, and Dale Schuurmans. Offline policy selec-
tion under uncertainty. arXiv preprint arXiv:2012.06919, 2020.
Tianhe Yu, Garrett Thomas, Lantao Yu, Stefano Ermon, James Zou, Sergey Levine, Chelsea
Finn, and Tengyu Ma. Mopo: Model-based offline policy optimization. arXiv preprint
arXiv:2005.13239, 2020.
Andrea Zanette, Martin J Wainwright, and Emma Brunskill. Provable benefits of actor-critic meth-
ods for offline reinforcement learning. arXiv preprint arXiv:2108.08812, 2021.
13
Under review as a conference paper at ICLR 2022
A Comments on Asymptotic Results
We remark here that all theoretical justification in this paper is based on asymptotics. It might be
possible to investigate finite sample regimes when one has an exact confidence interval instead of
(11) or a non-asymptotic bound. However, having an exact confidence interval might require some
model specification of the value function, and using non-asymptotic bounds might require additional
tuning steps (e.g., constants in many concentration inequalities), which is beyond the scope of this
paper. In addition, as seen from our empirical evaluations below, with a relatively large sample size,
the proposed model selection approach performs well.
B Technical Proofs
Notations: The notation ξ(N) . θ(N) (resp. ξ(N) & θ(N)) means that there exists a sufficiently
large (resp. small) constant c1 > 0 (resp. c2 > 0) such that ξ(N) ≤ c1θ(N) (resp. ξ(N) ≥ c2θ(N))
for some sequences θ(N) and ξ(N) related to N. In the following proofs, N often refers to some
quantity related to n and T .
Lemma 1 and its proof : Let J denotes some index of our batch data Dn . Define
φ(J,Qπ ,ωπ, ,π) = JJ X ωπ,ν (Si,t,At) [&,t + Y X ∏(a0∣Si,t+ι)Qπ (Si,t+ι,a0) - Qn (Si,t,Ai,t)∣ ,
|J| (i,t)∈J	a0 ∈A
where | J| is the cardinality of the index set J, e.g., | J0| = nOT for every 1 ≤ o ≤ O. Then We have
the following Lemma 1 as an intermediate result to Theorem 1.
Lemma 1 Under Assumptions 1 and 3-5, for every 1 ≤ l ≤ L and 1 ≤ o ≤ O - 1, the following
asymptotic equivalence holds.
ωπ"ν,∏% + op(1),
(15)
where op(1) refers to a quantity that converges to 0 as n or T goes to infinity.
The proof is similar to that of Theorem 7 in Kallus & Uehara (2019). First, notice that
∖OTn {Vdo+1 (∏*-V(∏*}
=rnT {φ( J,Qπ*(O) ,ωπ(o),ν, π(O))- Φ(J, Qπ*(o), ωπ(o),ν, π(。))
+ (1 - γ)Es.”[X ∏(o)(a∣So)Qπ(o) (So,a)] -(1- Y)Es0”[X ∏(o)(a∣So)Qπ(o)
a∈A
nτ	*(o)/S) V
+ V ~OΦ(J,Q	,ω
a∈A
(S0, a)]
Then it suffices to show the term in the first bracket converges to 0 faster than √nT. Notice that
{φ(J, Qπ*(O),ωπ(o),ν, π(o)) - φ(J, Qπ*(O),ωπ(o),ν, π(o))
+ (1 - γ)Es°”[X ∏(o)(a∣So)Qπ(o) (So,a)] -(1- γ)Es°”[X ∏(o)(a∣So)Qπ(o)
a∈A
a∈A
(S0, a)]
=E1 + E2 + E3 ,
14
Under review as a conference paper at ICLR 2022
where
E1
=nT X	(ωπ(o),ν(Si,t,Ai,t) - ωπ(o),ν(Si,t,Ai,t))(Ri,t- Qπ(o)S,t,AQ
(i,t)∈Jo+1
+γ X π(o) (a|Si,t+1)Qπ(o) (Si,t+ι,a)),
a∈A
E2= nT X	3πo,"(Si,t, Ai,t)(Q^o (Si,t, Ai,t) - Qπ(o) (Si,t, Ai,t)
(i,t)∈Jo+1
+γ X n(o)(a|Si,t+I)(Qnl(Si,t+1,a) - Qnl(Si,t+1, O))),
a∈A
and
O	(o)	(o)	(o)	(o)
E3= nT	Σ	(3nl ,ν(Si,t,Ai,t) — ωnl ,ν(Si,t,Ai,t))((bπ	(Si,t,Ai,t) - QTl(Si,t,AQ
(i,t)∈Jo+1
+Y X ∏(o)(a∣Si,t+ι)(Qn(0) (Si,t+ι,a) — Qn(O) (Si,t+1 ,a))).
a∈A
Next, we bound each of the above three terms. For term E1, it can be seen that
E[Eι∣ Jo] = 0.
In addition, by Assumptions 3 and 4, we can show
Var[E1] = E[Var(E11J。)] . -0InTlOT-KK,
nT
where the inequality is based on that each item in E3 is uncorrelated with others. Then by Markov’s
inequality, we can show
同=4(( n if).
Similarly, we can show
∣E2∣ = Op(( O) )T∕2-κ1 ).
nT
For term (E3), by Cauchy Schwarz inequality and similar arguments as before, we can show
∣E3∣=Op(( noT )-(κ2+κ1)).
Therefore, as long as (κ- + κι) > 1/2, We have Ei + E- + E3 = o(，O/nT), which concludes
our proof.
Proof of Theorem 1 We aim to show that
√nT(O — 1)/O (V(∏ι) — V(∏ι))
-------------σ⅛--------------z=⇒N H
It can be seen that
√nT(O — 1)/O (V(∏ι) - V(∏ι))
而
=/ nT (X Vdo+i (∏(θ))-V(∏ι)!
=V O(O - 1) Ui	σo+i(∏(o)))
=S nT (X Vdo+1 (∏(θ))-V(∏(o))
=V O(O - 1) Ui	σo+i(∏(o))
+ I nT	(X V(∏(θ))-V(∏ι)!
VO(O- 1) V0=1	σo+i(∏(o)) )■
15
Under review as a conference paper at ICLR 2022
Define
φ(J, Qπ
wπ,∏) = JT X Wi(Si,t,At) ∣Ri,t + Y X ∏(a0∣Si,t+ι)Qπ(S9,t+1,a0) - Qn(Si,t,Ai,t)j ,
|J| (i,t)∈J	a0∈A
where ∣ J∣ is the cardinality of the index set J, i.e., ∣ J∣ =等.Then by Lemma 1, We show that
rnT Vdo+1 (∏(o))-V(∏(o)) = rnτΦ(Jo+ι,Qπo ,wπo ,∏°八 +
O o	σ0+ι(∏(O))	— V o	σ0+ι(∏*
(16)
If we can show that
max
1≤o≤(O-1)
σo+1(π(o)) 1	⑴
σo:⅛ξ-1=op(1),
which will be shown later, then by Slutsky theorem, we can show that
nT
o(o-1)
nT
o(o-1)
(X Vdo+1 (∏(o))-V(∏(o))!
Iy	σo+ι(∏(o)))
(X (KiQ］：；［"! +op(i).
∖o=1	σo+Mnl )	)
S
<.
{z^
(I)
}
For (I), we can see that

(I)
O-1
(X X	WnowS,t,Ai,t)l"t
o=1 (i,t)∈Jo+1
+ Y X ∏(θ)(a0∣Si,t+ι)Qπ(θ)(Si,t+ι,a0) - Qπ(o)(Si,t,Ai,t))∕σo+ι(Πloo)).
a0∈A
(17)
(18)
By the sequential structure of our proposed algorithm, (I) forms a mean zero martingale. Then
we use Corollary 2.8 of (McLeish, 1974) to show its asymptotic distribution. First of all, by the
uniformly bounded assumption on Q-function, ratio function and the variance, we can show that
《nτ(O-1)ι≤omax-i)(int)j wπ(o),νSa，+Y X π(%a0∣si,t+I)Qn(O)(Si,t+1,aO)-
a0∈A
Qn(O) (Si,t,Ai,t ))∕σo+ι(∏(o))∣= Op(1).
Next, we aim to show that
nτ(OO-1) (X X {wn(o),ν(Si,t,Ai,t)(Ri,t	(19)
∣ o=1 (i,t)∈JO+1
+γ X ∏^(a0∣Si,t+ι)^nio (Si,t+ι,a0) - Qn(O) (Si,t,Ai,t))}2∕σO+ι(Π*) - 1 = 0p(1).
a0∈A	∣
Notice that the left hand side of the above is bounded above by
o	(O)
nτ 1≤omaχ-1)( Σ	{w" "(Si,t,Ai,tMRi,t	QO)
∣ (i,t)∈JO+1
+γ X ∏(o)(a0∣Si,t+ι)^n'o (Si,t+ι,a0) - Qn(O) (Si,t,Ai,t))}2∕σO+ι(Π(o))) - 1 .	(21)
a0∈A	∣
16
Under review as a conference paper at ICLR 2022
Because, for each 1 ≤ o ≤ (O - 1),
nτ I ( X	{wπl ,ν(Si,t, Ai,a)(Ri,t
l (i,t) ∈ Jo + 1
+γ X ∏(θ)(a0∣Si,t+ι)Qπ(o) (Si,t+ι,a0) - Qπ(o) (Si,t,Ai,t))}2 - E[{wπ(o),ν(S,A)(R
a0∈A
+Y X ∏(o)(a0∣S0)Qπ(o) (S0,a0) -Qπ(o) (S,A))}]∕σO+ι(Π*)],
a0∈A
(22)
(23)
(24)
forms a mean zero martingale, We apply Freedman's inequality in (Freedman, 1975) with Assump-
tions 3-5 to show it is bounded by Op(ʌʌ0-). Applying union bound shows (19) is 0p(1) and
furthermore consistency of σ(∏ι) in (16) holds. Then we apply the martingale central limit theorem
to show
I nT	OX φJo+ι,Qπo ,wπ(o) ,∏(O))、
VOO-Iy *	σ~+-^)	)
The remaining is to show
S nT (X V(∏(o))-V(∏ι)
VO(O-1)3	σo+ι(∏(O))
is asymptotically negligible. Consider
E∣V (∏(o)) -V (∏ι)∣	(25)
≤E ∣ V(∏(o)) - V(∏*)∣ + EMnl)- V(∏*)l	(26)
≤E ∣ V(∏(o)) - V(∏*)∣ + EMnl)- V(∏*)l	(27)
≤(nT o)-κOκ + (nT)-κ,	(28)
where we use Assumption 2 for the last inequality. Summarizing together, we can show that
S o(0T-i)EIE V(n(o))-v(nl)
≤So(OT-i) WnT…+ 产K(nT)-κ
≤∕0m¾ X1(nT)-κ + 尸产(nT)-κ
=o(1),
where we obtain the second inequality by that POO=-11 o-κ ≤ 1 + R1O o-κdo . O1-κ. In the last
inequality, we use κ > 1 in Assumption 2. Then Markov inequality gives that
yo(0T-i) (X V(nl(O))-V(nl))=op(1).
Moreover, by Assumption 5 that inf 1≤o≤o-1 σo+ι(n(o)) ≥ C for some constant c > 0, we can
further show that
/ nT OX V(∏(o))-V(∏l)!_“ ⑴
MEy 匚	σo+ι(∏*	厂op(1),
which completes our proof.
17
Under review as a conference paper at ICLR 2022
_ 一 一〜 __ _ . _ ........................... ʌ...... .，一、、一 一 _
Proof of Corollary 1 Denote the sets El = {|V(∏ι) - V(∏ι)∣ ≤ u(l)}, l = 1,...,L, where
U(l) = Zα∕2pnT(O - 1)/Oσ(l). Note that liminfnτ→∞ Pr(∩L=1Ej) ≥ 1 - La and
Pr(V(Π^) ≥ max
l 1≤l≤L
....... 八，.、
V(∏1)- 2U(l))



Pr(V(∏^) - V(∏^) + V(∏^) ≥ 是公L V(∏ι) - V(∏ι) - 2U(l) + V(∏ι))

^
^
^
. ∕' , . ∕' , . . ∕' , . . . ∕' , ..
≥ Pr(V(∏^) - V(∏^) + V(∏^) ≥ max V(∏ι) - V(∏ι) - 2u(l) + V(∏ι)∣∩
1≤l≤L
^

^
≥ Pr(V(π^) — u(l) ≥ max V(∏ι) — u(l)∣∩
= Pr(∩jL=1Ej),
where the last inequality holds because given the event ∩L=ιEj，one has -u(l) ≤ V(∏^) — V(∏^)
and V(∏ι) - V(∏ι) ≤ U(I) for any l. This completes the proof by taking liminf on both sides.
Proof of Theorem 2 To show the results in Theorem 2, it can be seen that
PnT(O - 1)/O (V(∏ι) - V(π*))
σ(i)
l^^nT^~
≤ VO(O-1)
S~nT-
+ VO(。-1)
≤;
nT
O(O - 1)
(X- Vdo+1 (∏(o))-V(∏l(O))
O=1	σ0+ι(∏(O))
宁 V(∏(O))-V(∏*)!
∖O=1	σο+ι(∏(O))
(X Vdo+1 (∏(O))-V(∏(O))
O=1	σ°+ι(∏(O))
{z
(I)
+B(I)J (E -⅛
Then by results in the proof of Theorem 1, we can show that
lim Pr((I) > zα∕2) = α.
nT→∞
(29)
}
This implies that
IiminfPr (∣V(∏*) - V(∏ι)∣ ≤ Zα∕2Pθ∕nT(O - 1)σ(l) + B(I))
nT→∞
≥ Iim Pr((I) ≤ Zα∕2) = 1 - α,
nT→∞
(30)
(31)
which concludes our proof.
Proof of Corollary 2: We mainly show the proof of the second claim in the corollary, based on
which the first claim can be readily seen. Define an event E such that 1 ≤ l ≤ L, |V(∏ι) - V(∏ι)| ≤
c(δ) log(L)σ(i)∕√NT and |V(π*) - V(∏ι)∣ ≤ zɑ∕(2L)Pθ∕nT(O - 1)σ(l) + B(l). Based on the
assumption given in Corollary 2 and Theorem 2, we have lim infP (E) ≥ 1 -δ -α. In the following,
nT→∞
we suppose event E holds.
Inspired by the proofs of Corollary 1 in (Mathe, 2006) and Theorem 3 of (SU et al., 2020), We
define l = max{l : B(l) ≤ uι(l) + u2(l)}, where uι(l) = Zα∕(2L)√O∕nT(O - 1)σ(l). Let
u2(l) = c(δ) Iog(L)σ(i)∕√NT. By Assumption 7, for l ≤ l,
~
~
B(l) ≤ B(ll) ≤ u1(ll) ≤ u1(l),
1 ∙ 1 l' , 1	∙	1 ∙	,1	. Γ∙	1,7
which further implies that for any l ≤ l,
.ʌ . ... .. .. ..
∣V(∏1) -V(π*)∣ ≤ B(l)+ uι(l) ≤ 2uι(l).
18
Under review as a conference paper at ICLR 2022
Then V(π*) ∈ I(l) based on the construction of I(V) for all l ≤ l. In addition, We have for l ≤ l
∣V(∏1) -V(∏*)∣ ≤ 2uι(l) + u2(l),	(32)
by triangle inequality and event E. Since I(l) share at least one common element for 1 ≤ l ≤ l, We
have i ≥ l. Moreover, there must exist an element X such that X ∈ I(l) ∩ I(i), where |V(∏) — x| ≤
u1 (l) and |V(∏) - x| ≤ u1 (i). ThiS indicates that
,人，	，,、，	，人，.、	,	,人，.、	,	,人，.、	，.、，
∣v(∏i - V(∏*)∣ ≤ ∣V(∏i) - x| + ∣V(∏z) - x| + ∣V(∏/-V(∏*)|	(33)
≤ u1 (i) + 2u1 (l) ≤ 3u1 (l),	(34)
by again triangle inequality and Assumption 7, and
,	,	、	,	.、,	，人、	■〜,
∣V(∏i) - V(∏*)∣ ≤ U2(i) + 3uι(Z) ≤ U2(l) + 3uι(l),	(35)
by event E and Assumption 7. Define l* = min{l : B(l) + u1 (l) + u2(l)}. Then following the
similar proof of (Su et al., 2020), we consider two cases:
Case 1:	If l* ≤ l, then we have
u2(Zl) + B(Zl) + u1(Zl) ≤ 2u1(l*) + u2(l*) ≤ 2u1(l*)+2B(l*)+u2(l*),
where we use Assumption 7.
Case 2:	If l * > l, then we have
ζ(u2(Zl)+u1(Zl)) ≤ (u2(lZ+1)+u1(lZ+1)) ≤ B(Zl + 1) ≤ B(l*),
where we use Assumption 7. This implies that
U2(F) + Ui① + B① ≤ (1 + 1∕Z)B(l*).
Combining two cases, we can show that
u2(lZ) + u1(lZ) + B(lZ) ≤ (1 + 1∕ζ)(B(l*) + u1(l*) + u2(l*)),
as ζ < 1. Together with (33), we have
,	，	、	，	...	，人、	，	•、，，.、	..	，	.、、
∣V(∏i) - V(π*)| ≤ U2(i) + 3ui(l) ≤ 3(1 + 1∕Z)(B(l*) + Ui(l*) + U2(l*)),	(36)
which concludes our proof.
C	MORE Details on DQN Environments
Figure 5: DQN environments in our studies: (a) Ei: FrozenLake-v0; (b) E2: Banana Collectors (3D geomet-
rical navigation task); (c) E3: Pong-v0; (d) E4: Breakout-v0; (e) E5: HalfCheetah-v1; (f) E6: WaIker2d-v1.
We introduce our deployed DQN environments in this section, which included four environments
with discrete action (EI to E4) and two environments (E5 to E6) with continuous action. These
environments cover wide applications, including tabular learning (E1), navigation to a target object
in a geometrical space (E2), digital gaming (E3 to E4), and continuous control (E5 to E6).
Ei : Frozen Lake: The Frozen Lake is a maze environment that manipulates an agent to walk from
a starting point (s) to a goal point without failing into the hole (H). We use FrozenLake-v0 from
openAi Gym (Brockman et al., 2016) as shown in Fig 5(a). We provide top-5 regret and precision
results shown in Figure 6 and 7.
19
Under review as a conference paper at ICLR 2022
Figure 6: Policy selection using top-k ranking re-
gret score in E1 (Frozen Lake).
十十-⅜--⅝-l-⅜∙干l-⅜-十-¢-十
Figure 7: Policy selection using top-k ranking pre-
cision in E1 (Frozen Lake).
E2 : Banana Collector: The Banana collector is one popular 3D-graphical navigation environ-
ment that compresses discrete actions and states as an open source DQN benchmark from Unity 1
ML-Agents v0.3.(Juliani et al., 2018). The DRL agent controls an automatic vehicle with 37 dimen-
sions of state observations including velocity and a ray-based perceptional information from objects
around the agent. The targeted reward is 12.0 points by accessing correct yellow bananas (+1) and
avoiding purple bananas (-1) in first-person point of view as shown in Fig 5(b). We provide the
related top-5 regret and precision results shown in Figure 8 and 9.
Figure 8: Policy selection using top-k ranking re-
gret score in E2 (Banana Collector).
-⅜--⅜--⅜--⅞--⅜-+l-⅜-干-⅜-十
Figure 9: Policy selection using top-k ranking pre-
cision in E2 (Banana Collector).
Numbers of Trajs
-t--f-φ-士* * 一千*千
Figure 10: Policy selection using top-k ranking re- Figure 11: Policy selection using top-k ranking pre-
gret score in E3 (Pong).	cision in E3 (Pong).
E3: Pong: Pong is one Atari game environment from OpenAI Gym (Brockman et al., 2016) as
shown in Fig 5(c). We provide its top-5 regret and precision results shown in Figure 10 and 11.
Figure 12: Policy selection using top-k ranking re-
gret score in E4 (Breakout).
Figure 13: Policy selection using top-k ranking pre-
cision in E4 (HalfCheetah-v1).
-φ-+ m + 卡 +卡-l-f--÷-
E4: Breakout: Breakout is one Atari game environment from OpenAI Gym (Brockman et al., 2016)
as shown in Fig 12(d). We provide the related top-5 regret and precision results shown in Figure 12
and 13.
1https://www.youtube.com/watch?v=heVMs3t9qSk
20
Under review as a conference paper at ICLR 2022
Figure 15: Policy selection using top-k ranking Pre-
cision in E5 (HalfCheetah-v1).
Figure 14: Policy selection using top-k ranking re-
gret score in E5 (HalfCheetah-v1).
t-+f-+f-十t--Ii--f-
Figure 16: Policy selection using top-k ranking re-
gret score in E6 (Walker2d-v1).
Figure 17: Policy selection using top-k ranking pre-
cision in E6 (Walker2d-v1).
E5 : HalfCheetah-v1: Halfcheetah is a continuous action and state environment to control agent
with monuments made by MuJoCo simulators as shown in Fig 5(e). We provide the related top-5
regret and precision results shown in Figure 14 and 15.
E6 : Walker2d-v1: Walker2d-v1 is a continuous action and state environment to control agent with
monuments made by MuJoCo simulators as shown in Fig 5(f). We provide the related top-5 regret
and precision results shown in Figure 16 and 17.
C.1 Hyper-Parameters Information
We select a total of 70 DQN based models for each environment. We will open source the model and
implementation for future studies. Table 1, Table 2, and Table 3 summarize their hyper-parameter
and setups. In addition, Figure 18 and Figure 19 provide ablation studies on different scales of α and
O selection in PMS experiments for the deployed DRL navigation task (E2). From the experimental
results, a more pessimistic α (e.g., 0.001) is associated with a slightly better attained top-5 regret.
Meanwhile, the selection of O does not produce much different performance on selected policies
but slightly affects the range of the selected policies.
Table 1: Hyper-parameters information for for DQN models used in E1 to E2
Hyper-parameters	Values
Hidden layers	{1, 2}
Hidden units	{16, 32, 64, 128}
Learning rate	{1 × e-3, 5 × e-4}
DQN training iterations	{100, 500, 1k, 2k}
Batch size	{64}
B roader Impact
There are also some limitations of the proposed PMS as one of the preliminary attempts on model
selection for offline reinforcement learning. When the benchmarks environments (excluded Atari
games) are based on simulated environments to collect the true policy (Barth-Maron et al., 2018;
Siegel et al., 2019), more real-world-based environments could be customized and studied in fu-
ture works. For example, one experimental setup needs to be carefully controlled in clinical set-
tings (Tang & Wiens, 2021) or resilience-oriented (Yang et al., 2021) reinforcement learning.
21
Under review as a conference paper at ICLR 2022
Table 2: Hyper-parameters information for for DQN models used in E3 to E4
Hyper-parameters	Values
Convolutional layers	{ 2, 3}
Convolutional units	{16, 32}
Hidden layers	{ 2, 3}
Hidden units	{64, 256, 512}
Learning rate	{1 × e-3, 5 × e-4}
DQN training iterations	{4M, 4.5M, 5M}
Batch size	{64}
Table 3: Hyper-parameters information for double DQN (DDQN) models (Van Hasselt et al., 2016) with a
prioritized replay (Schaul et al., 2015) used in E5 to E6.
Hyper-parameters	Values
Hidden layers	{4, 5, 6}
Hidden units	{64, 128, 256, 512}
Learning rate	{1 × e-3, 5 × e-4}
DDQN training frames	{40M, 45M, 50M}
Batch size	{256}
Buffer size	{106}
Updated target	{1000}
Figure 18: Different α for PMS selection.
Figure 19: Different O for PMS selection.
22