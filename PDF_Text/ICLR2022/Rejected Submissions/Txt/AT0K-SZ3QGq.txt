Under review as a conference paper at ICLR 2022
Achieving Personalized Federated Learning
with Sparse Local Models
Anonymous authors
Paper under double-blind review
Abstract
Federated learning (FL) is vulnerable to heterogeneously distributed data, since
a common global model in FL may not adapt to the heterogeneous data distri-
bution of each user. To counter this issue, personalized FL (PFL) was proposed
to produce dedicated local models for each individual user. However, PFL is far
from its maturity, because existing PFL solutions either demonstrate unsatisfac-
tory generalization towards different model architectures or cost enormous extra
computation and memory. In this work, we propose federated learning with per-
sonalized sparse mask (FedSpa), a novel PFL scheme that employs personalized
sparse masks to customize sparse local models on the edge. Instead of training an
intact (or dense) PFL model, FedSpa only maintains a fixed number of active pa-
rameters throughout training (aka sparse-to-sparse training), which enables users’
models to achieve personalization with cheap communication, computation, and
memory cost. We theoretically show that with the rise of data heterogeneity, set-
ting a higher sparsity of FedSpa may potentially result in a better convergence
on its personalized models, which also coincides with our empirical observations.
Comprehensive experiments demonstrate that FedSpa significantly saves commu-
nication and computation costs, while simultaneously achieves higher model ac-
curacy and faster convergence speed against several state-of-the-art PFL methods.
1 introduction
Data privacy raises increasingly intensive concerns, and
governments have enacted legislation to regulate the privacy
intrusion behavior of mobile users, e.g., the General Data
Protection Regulation (Voigt & Von dem Bussche, 2017).
Traditional distributed learning approaches, requiring mas-
sive users’ data to be collected and transmitted to a central
server for training model, soon may no longer be realistic
under the increasingly stringent regulations on users’ private
data. On this ground, federated learning (FL), a distributed
training paradigm emerges as a successful solution to cope
with privacy concerns, which allows multiple clients to per-
form model training within the local device without the ne-
cessity to exchange the data to other entities. In this way, the
data privacy leakage problem could be potentially relieved.
Figure 1: Performance of FedSpa and
several baselines w.r.t. communication
cost in Non-IID setting. Numbers above
FedSpa and Sub-FedAvg are sparsity.
Despite the promising prospect, several notorious issues are afflicting practical performance of FL:
• The global model produced by weight average (or FedAvg and its non-personalized variants)
exhibits unsatisfactory performance in a Non-IID data distribution setting. To alleviate this prob-
lem, the most popular idea is to integrate personalized features into the global model, and produce
dedicated model for each local distribution. However, how to make this integration is an open
problem that remains unresolved. Prior works on personalized FL (PFL) zero in this issue, but the
existing methods either demonstrate weak generalization towards different model architectures
(Arivazhagan et al., 2019), or require extra computation and storage (Li et al., 2021).
•	The communication and training overhead is prohibitively high for both the FL and PFL. Clients
in FL/PFL responsible for model training are mostly edge-devices with limited computation ca-
pacity and low bandwidth, and may not be powerful enough to fulfill a modern machine learning
1
Under review as a conference paper at ICLR 2022
task with large deep neural networks. Existing studies (Li et al., 2020; Vahidian et al., 2021) inte-
grate model compression into FL/PFL to save communication and computation overhead. How-
ever, both methods embrace the technique of dense-to-sparse training, which still requires a large
amount of communication at the beginning of training. In addition, how to effectively aggregate
the dynamic sparse models is another challenging problem that remains unresolved.
In this work, We propose FedSPa (See Figure 2), which
has two key features to counter the above two chal-
lenges: (i) FedSpa does not deploy a single global
model, but allows each client to own its unique sparse
model masked by a personalized mask, which success-
fully alleviates the Non-IID challenge. (ii) FedSpa al-
lows each client to train over an evolutionary sparse
model with constant sparsity1 throughout the whole fed-
erated training process, which consistently alleviates
the computation overhead of clients. Besides, all the
local models in FedSpa are sparse models, which re-
quires a smaller amount of communication cost in each
communication round. Theoretically, we conclude that
with the rise of Non-IID extent, setting a higher spar-
sity may result in a better convergence on the personal-
ized models of FedSpa. Empirically, in the Non-IID set-
ting, we demonstrate that FedSpa accelerates the con-
vergence (respectively 76.2% and 38.1% less commu-
nication rounds to reach the best accuracy of FedAvg
(McMahan et al., 2016) and Ditto (Li et al., 2021)), in-
Parameter Server
Dense global model
φ Mask & distribute sparse models ,
④ Upload local updates & next masks
Figure 2: Overview of FedSpa. Firstly,
the server masks and distributes the
sparse weights. Secondly, clients do lo-
cal training on a constantly sparse model.
Thirdly, clients upload the sparse updates
and being aggregated by the server.
creases the final accuracy (up to 21.9% and 4.4% higher accuracy than FedAvg and Ditto, respec-
tively), reduces the communication overhead (50% less parameters communicated than the dense
solutions), and lowers the computation (15.3% lower floating-point operations (FLOPs) than algo-
rithms trained with fully dense model). To the end, we summarize our contribution as:
•	We present a novel formulation of the sparse personalized FL (SPFL) problem, which can be
applied to various network architectures by enforcing personalized sparse masks to a global model.
•	We propose a solution dubbed as FedSpa to solve the SPFL problem. By our novel design, FedSpa
reduces the communication and computation overhead of the general FL solution.
•	Two sparse-to-sparse mask searching techniques are integrated as plugins of our solution. To
adapt our PFL training context, we modify the DST-based mask searching technique to enable a
warm-start of the searching process, which achieves superior performance.
•	We theoretically present the convergence analysis of the personalized models. Experimental re-
sults demonstrate the superiority of FedSpa and also coincides with the theoretical conclusion 一
with the rise of data heterogeneity, setting a higher sparsity of FedSpa may potentially result in a
better convergence on its personalized models.
2	Related Works
Federated learning (FL) (McMahan et al., 2016) is seriously afflicted by the issue of heterogeneously
distributed (or Non-IID) data. Personalized FL (PFL), initiated by recent literature (Li et al., 2021;
Arivazhagan et al., 2019), is shown to be effective to counter this issue of FL. In this work, we
propose an alternative yet effective way to enhance PFL with personalized sparse models.
2.1	Personalized Federated Learning
We categorize PFL into five genres. Firstly, PFL via layer partition, e.g., FedPer (Arivazhagan
et al., 2019), LG-FedAvg (Liang et al., 2020), FedRep (Collins et al., 2021), is to divide the global
model layers into shared layers and personalized layers. For the shared layers, weights average as
in FedAvg is adopted, while for personalized layers, models are trained only locally and will not
1Sparsity specifies the ratio of parameters that are set to 0 (or inactive) in a model.
2
Under review as a conference paper at ICLR 2022
be exchanged with others. Secondly, PFL via regularization, e.g., Ditto (Li et al., 2021), L2GD
(Hanzely & Richtarik, 2020) is to add a proximal term on the local model to force the local model
and global model closely in the local model fine-tuning stage. Similarly, Sarcheshmehpour et al.
(2021); SarcheshmehPour et al. (2021) propose a total variation (TV) regularization to form the
network lasso (nLasso) problem, and primal-dual methods adapted from (Jung, 2020) are proposed
to solve the nLasso problems. Thirdly, PFL via model interpolation, e.g., MAPPER (Mansour et al.,
2020), APFL (Deng et al., 2020) achieves personalization by linearly interpolating the weights of
the cluster (global) model and local model as the personalized model. Fourthly, PFL via transfer
learning, e.g., FedMD (Li & Wang, 2019), FedSteg (Yang et al., 2020), and Fedhealth (Chen et al.,
2020), is to either use model and domain-specific local fine-tuning or knowledge distillation to adapt
the global model into the personalized model. Finally, PFL via model compression, e.g., LotteryFL
(Li et al., 2020) and Sub-FedAvg (Vahidian et al., 2021), achieves personalization via employing
principle model compression techniques, such as weight pruning and channel pruning, over the
shared global model.
2.2	Sparse Deep Neural Networks
Methods to Sparsify neural networks can be classified into two genres: dense-to-sparse methods
and sparse-to-sparse methods. Dense-to-sparse methods train from a dense model, and compress
the model along the training process. Iterative pruning, first proposed by Frankle & Carbin (2018),
shows promising performance in dynamically searching for a sparse yet accurate network. Recently,
sparse-to-sparse methods have been proposed to pursue training efficiency. Among them, dynamic
sparse training (DST) (Bellec et al., 2018; Evci et al., 2020; Liu et al., 2021) is the most success-
ful technique that allows sparse networks, trained from scratch, to match the performance of their
dense equivalents. Stemming from the first work - sparse evolutionary training (MOCanU et al.,
2018; Liu et al., 2020), DST has evolved as a class of sparse training methods absorbing many ad-
vanced techniques, e.g., weight redistribution (Mostafa & Wang, 2019; Dettmers & Zettlemoyer,
2019), gradient-based regrowth (Dettmers & Zettlemoyer, 2019; Evci et al., 2020), and extra weight
exploration (Jayakumar et al., 2020; Liu et al., 2021).
Our work also achieves personalization via model compression. We emphasize that three main
progresses are made towards SOTA compression-based PFL: (i) We rigorously formulate the sparse
personalized FL problem, filling the gap left by the prior works. (ii) While prior works either vaguely
describe their model aggregation as ”aggregating the Lottery Ticket Network via FedAvg” (Li et al.,
2020), or ”taking the average on the intersection of unpruned parameters in the network” (Vahidian
et al., 2021), we explicitly formulate the aggregation as averaging the sparse update from clients.
(iii) Both the two prominent prior works use the idea of iterative pruning to prune the network
from dense to sparse. We instead provide two sparse-to-sparse training alternatives to plug in our
solution, which largely reduces the costs of communication at the beginning of the training process,
and exhibits remarkable performance.
3	Problem Formulation
We assume a total number of K clients within our FL system, and we consistently use k to index a
specific client. First, we give a preliminary introduction on the general FL and PFL problem.
General FL problem. Let w ∈ Rd be the global weight. General FL takes the formulation as below
1K
(P1)	mwn f(w) = K X {凡(W) := E[L(χ,y)〜Dk(w; (x,y))]},
where D = Di ∪∙∙∙∪ DK is thejoint distribution of k local heterogeneous distributions, data (x, y)
is uniformly sampled according to distribution Dk wrt. client k, and L(∙; ∙) is the loss function.
Ultimate PFL Problem. Let {wk} be the personalized models. The ultimate PFL is defined as:
1K
(P2) r min	f(wi,…，WK) = —	(wk) := E[L(χ,y)〜Dk (wk；(x,y))],
{wι,∙∙∙ ,wκ}	K L
k=1
3
Under review as a conference paper at ICLR 2022
according to (Zhang et al., 2020; Hanzely et al., 2021). The problem could be separately solved by
individual client with no communication. However, if the local data is insufficient, poor performance
could be observed by this direct solution, since the local models cannot be boosted by other clients.
Regularized PFL problem. Regularized PFL can ease the heterogeneous challenges encountered
by general FL, while escaping the curse of insufficient samples encountered by the ultimate PFL
problem. Inspired by (Chen & Chao, 2021), we formulate the Regularized PFL problem as follows:
1K
(P3)	min	f(wι,…，WK)=不 £{Fk (Wk) ：= E[L(χ,y)〜Dk(Wk； (x,y))]} + R(∙),
{w1,…,wK }	K k=1
where Wk denote the personalized models and R(∙) is the regularize] that enables information ex-
change between clients, making the problem tractable even the local samples are insufficient. How-
ever, it remains controversial about how to define the regularizer. Also, the gap between regularized
PFL problem (P3) and the ultimate PFL problem (P2) still remains unspecified in existing PFL study.
In this work, our ultimate goal is to solve problem (P2), which requires information exchange be-
tween clients to ensure effective solution. Below, instead of utilizing regularizer as in (P3), we
alternatively propose a novel Sparse PFL (SPFL) problem to reach the same goal.
Sparse PFL problem. By introducing personalized masks into FL, we derive the SPFL problem as
1K
(P4)	mWn f (w) = κf{Fk(mk Q W) ：= E[L(x,y)〜Dk(mk Q w； (x,y))]},
w	k=1
where mkk ∈ {0, 1}d is a personalized sparse binary mask for k-th client. Q denotes the Hadamard
product for two given vectors. Our ultimate goal is to find a global model W, such that the per-
sonalized model for k-th client can be extracted from the global model by personalized mask mkk,
i.e., mkk Q W. The element of mkk being 1 means that the weight in the global model is active for
k-th personalized model, otherwise, remains dormant. Thus, the information exchange between all
personalized models is enforced by a shared global model W .
Compared with existing PFL algorithms, solving our SPFL problem (P4) does not sacrifice addi-
tional computation and storage overhead of clients, since we do not maintain both personalized
local models and global model in clients as in (Li et al., 2021; Mansour et al., 2020). On contrary,
the solution to our problem could potentially lower the communication and computation overhead.
Moreover, SPFL problem (P4) can be applied to most of the model architectures without model-
specific hyper-parameter tuning, since we do not make model-specific separation of the public and
personalized layer in (Arivazhagan et al., 2019; Liang et al., 2020; Collins et al., 2021), or domain-
specific fine-tuning in (Chen et al., 2020; Yang et al., 2020).
4 FedSpa: s olution for SPFL
In this section, we first introduce our proposed FedSpa in Algorithm 1. Then, we specify the update
rule of global model, and two sparse-to-sparse mask searching methods that can be plug in the
update process. At last, we give a theoretical analysis on evaluating the quality of the iterates of
FedSpa with respect to the ultimate PFL problem (P2).
4.1	Global Model Update rule for FedSpa
Data Parallel-based Update. We first propose the following iterative update to solve problem (P4)
K
Wt+i = Wt- K Emk QVwk,tL(W k,t； ξk,t),	⑴
k=1
where ξk,t is a batch of data that is uniformly sampled from the k-th client’s local distribution Dk,
η is the learning rate, and Wk,t = mk Q Wt is the sparse weights sparsified by mask mk. However,
4
Under review as a conference paper at ICLR 2022
the optimal personalized masks {mk } are generally not accessible to Us in the solution process. Let
m∙k,t be an intermediate surrogate personalized mask of m，，We rewrite Eq. (1) as:
K
Wt+1 = Wt — K Emk,t Ewk,tL(Wk,t; ξk,t).	(2)
k=1
For our proposed update rule, it is worth mentioned that: (i) Some coordinates of the model weights
have been made zero before doing the forward process, i.e., not all the parameters have to be involved
when calculating L(Wk,t; ξk,t). This means that the computation overhead in the forward process
could be potentially saved. (ii) In the backward process, the stochastic gradient Nwk*L(Wk,t； ξk,t)
is masked again by mk,t, which means that we do not need to backward the gradient for those sparse
coordinates. Thus, the computation cost can be largely saved.
FL-adapted Update. To save the communication overhead, we integrate the idea from local SGD
(Stich, 2018) and partial participation to our solution. Let Wk,t,τ denote the weights before doing
(τ +1)-thstepoflocal SGD and set Wk,t,o = m*t Θ Wt (i.e., the local weights will be synchronized
every N steps with the global weights). Then, each client k ∈ St updates its model as below:
Wk,t,T+1 = W k,t,τ — ηmk,t Θ Vwk,t,τ LIWk,t,τ ； ξk,t,τ ),τ = 0,1,...,N — 1,	(3)
where ξk,t,τ is a batch of sampled data in τ-th step at round t. After the local training is finished, the
models of participated clients are updated and aggregated to the global model in server as follows:
Wt+1
Wt - |S? X (Wk,t,0 - Wk,t,N),
t k∈St
(4)
where St is the set of clients selected to be participant in round t. According to Eq. (3), the update
synchronized to the server (i.e., Wk,t,o — Wk,t,N), and the model distributed to clients (i.e., Wk,t,0)
are all sparse with a constant sparsity. Therefore, the communication overhead over synchronization
could be largely saved. At last, we summarize our proposed FedSpa in Algorithm 1.
Algorithm 1 FedSpa
Input Training iteration T ; Learning rate η; Local Steps N; Random seed seed;
1:
2:
3:
4:
5:
6:
7:
8:
9:
10:
11:
12:
13:
14:
15:
16:
17:
18:
19:
procedure SERVER’S MAIN LOOP
Randomly initialize global model W0
mk,0 = Initialization(seed) for k=0, 1, . . . , K . Initialize masks via a mask-searching
solution
for t = 0, 1, . . . , T — 1 do
Uniformly sample a fraction of client into St
for each client k ∈/ St do
mk,t+1 = mk,t	. Inherit masks for round t + 1 if not chosen
for k ∈ St do
Send Wk,t,0 = mk,t Θ Wt to client k
Call Client k’s main loop and receive Uk,t and mk,t+1
Wt+ι = Wt — Ss Pk∈st Uk,t	. Average and apply the update
procedure CLIENT’ S MAIN LOOP
for τ = 0, 1, . . . , N — 1 do
Sample a batch of data ξk,t,τ from local dataset
gk,t,T〈W k,t,τ ) = Vwk,t,τ L(Wk,t,T； ξk,t,τ )
Wk,t,τ+1 = Wk,t,τ — ηm∙k,t Θ gk,t,τ(Wk,t,τ)	. Local update with fixed mask mk,t
Uk,t = W k,t,0 — Wk,t,N
mk,t+ι = Next-Masks(∙)	. Plug in a mask-searching solution to produce next masks
Send back Uk,t and mk,t+1
4.2	Sparse-to-sparse Mask Searching Technique
The framework of FedSpa is extensible. Specifically, we use the mask surrogate mk,t in Eq. (3)
to perform the update, which allows us to plug in arbitrary mask searching techniques to determine
5
Under review as a conference paper at ICLR 2022
the iterating process of mk,t. In this work, we nominate two kinds of sparse-to-sparse training
techniques: modified dynamic sparse training (DST) (Liu et al., 2021) and Random Static Masks
(RSM), into FedSpa to reduce the computation and communication cost.
Modified DST for FedSpa. Our modified DST solution (see Algorithm 2) for FL follows these
procedures. Firstly, randomly initialize the same mask for each client based on Erdos-Renyi Kernel
(ERK) (Evci et al., 2020). Secondly, after local training, each client prunes out a number of unpruned
weights with the smallest magnitude, and the number of weights being pruned is determined by a
decayed pruned rate. Thirdly, recover the same amount of weights pruned in the last step. We follow
the recovery process as in (Evci et al., 2020) by utilizing the gradient information to do the recovery.
By our DST method, the number of sparse weights (aka. sparse volume) remains a constant (i.e., β)
throughout the whole training process.
Algorithm 2 Modified DST for FedSpa
Input Initial pruning rate a0; Set of Model layers J;
1:	procedure INITIALIZATION(seed)
2:	Randomly initialize mk,0 using the same random seed seed.
3:	procedure NEXT_MASKS(Wk,t,N)
4:	Decay αt using cosine annealing with initial pruning rate α0
5:	Sample a batch of data and backward the dense gradient g(Wk,t,N)
6:	for layer j ∈ J do
7:	Update mask m(j'), ι by zeroing out αt-proportion of weights with magnitude pruning
k,t+ 2
8:	Update mask mjt+ι via recovering weights with gradient information g(W k,t,N)
9:	Return mk,t+1
Remark. We highlight our main modification over traditional DST techniques like Rigl (Evci et al.,
2020) and Set (Mocanu et al. (2018)) to an FL context exist in two main aspects: (i) The pruning is
performed individually by each client based on their local models, and the gradient used for weights
recovery is derived using the client’s local training data. (ii) Once the next masks are generated,
existing DST solutions immediately apply them to the local model weight. Indicated by Liu et al.
(2021), by doing so, the recovered coordinate may need extra training steps to grow from 0 to a dense
value. Our solution relieves this problem by applying the new mask on the global weights (which
are dense), such that the recovered coordinates could have a dense initial value to warm-start.
RSM for FedSpa. RSM (shown in Algorithm 3) is basi-
cally fixing mk,t for all k ∈ [K] to the same randomly ini-
tialized mask, which remains unchanged during the whole
training session. This solution also ensures the same
sparse volume for all the clients throughout the training
process, and could also reduce the computation and com-
munication overhead as DST. Interestingly, within the set-
ting of the homogeneous data distribution, we empirically
show that RSM is more effective than DST in FedSpa.
Algorithm 3 RSM for FedSpa
1:	procedure INITIALIZATION(seed)
2:	Randomly initialize mk,0 us-
ing the same random seed seed
3:	procedure Next_Masks
4:	mk,t+1 = mk,t
5:	Return mk,t+1
4.3 Theoretical Analysis
In this section, we shall introduce the convergence property of personalized models produced by
FedSpa. We first give the following assumptions.
Assumption 1 (Coordinate-wise bounded gradient dissimilarity). For any W ∈ Rd, there ex-
ists a constant G ≥ 0 bounding the coordinate-wise gradient dissimilarity over all clients, i.e.,
IlVFk(W)- Kk PKK=I VFkO(W)L ≤ G.
Assumption 2 (Bounded variance). Assume that gk,t,τ(W~):= VL(W; ξk,t,τ) is an unbiased estima-
tor of VFk (W) with bounded variance, i.e., E
b∣gk,t,τ(W)-VFk(W)『]
≤σ2,∀k,t,τ, W ∈ Rd.
Assumption 3 (L-smoothness). We assume L-smoothness over the client’s loss function, i.e.,
∣∣VFk(tWι) -VFk(W2)k ≤ L∣∣Wι - W2k holdsfor arbitrary W1, W? ∈ Rd.
6
Under review as a conference paper at ICLR 2022
Assumption 4 (Coordinate-wise bounded gradient). Suppose the coordinate-wise gradient of each
client is upper-bounded, i.e., kvɪðFk(W)I∣∞ ≤ B.
Assumptions 2 and 3 are commonly used for characterizing the convergence of FL algorithms. We
modify the other two assumptions slightly from their counterparts in existing FL literature (Xu et al.,
2021, see Assumptions 3 and 5) by replacing the L2 norm with L∞ norm. These modifications are
made to reveal the coordinate-wise gradient information in our analysis.
Theorem 1 (Convergence of personalized models). Given the above assumptions, suppose the
learning rate satisfies η ≤ 溢N, the optimal personalized masks m， ∈ {0,1}d and the evolu-
tionary masks m∙k,t ∈ {0,1}d maintain the same sparse volume constraint, i.e., ||1 一 m，||o =
||1 一 m∙k,t ||o = β, the personalized sparse models {Wk,t} generated via FedSpa exhibits the fol-
lowing convergence property towards the optimal solution of ultimate PFL problem (P2):
ɪ X1X E[kVFk (W k,t) k2] ≤ 3(f(W0)- f(w*)) +3 X Pt + Υ,	(5)
TK	TηNκ	T
t=0 k=1	t=0
where K =	2 —	150N3η3L3	—	15N2η2L2	—	5NηL	and	Pt	=	(25N3η4L3	+
5N2n3L2)(σ2 + 18NΦt) + 4N2嚷+Nη Pk dist(mk,t, m/)B2 + 9N2η2LΦt + N⅛Lσ2,
Φt = Kk Pk ((d — β)G2 + Kk Pko B2(gist(mk,t, mk,七)+ dist(mko,t,m^z))), Y =
}	''
≈0 if β→d	≈0 if mk,t→mko t	≈0 if m^ t→m[
3(d — β)G2+ 3βB2 +32 Pk Pko dist(mk, m，,)B2, dist(mι, m2) is the hamming distance2
X----V-----}	l-{^}	|--------V-------}
≈0 ifβ→d	≈0 if β→0
between masks m1 and m2.
≈0 if mk →mko
Below, we give several comments on the above theorem. We focus our analysis on the on the second
and third residuals, which do not vanish as communication round T → ∞
•	Impact of sparse volume. The result in Eq. (5) is highly related to setting of sparse volume,
i.e., β. The term βB2 would vanish as β → 0, while the term (d — β)G2 would disappear as
β → d. When the data is highly heterogeneous, i.e., gradient dissimilarity is drastic, G could
be prohibitively large to dominate the error. Then setting the sparsity level to a relatively large
value, which results in a lower (d — β)G2, would potentially give a smaller non-vanished error. On
contrary, when the data heterogeneity is mild, the error could be dominated by βB2. Then setting
an excessively high sparsity might even hurt the performance. This conclusion is also evidenced
by our experimental results (see Figure 1 in Section 1, where the optimal sparsity setting of FedSpa
that achieves the highest accuracy is 0.5).
•	Error by evolutionary mask searching. Recall that FedSpa uses evolutionary masks as a surro-
gate of optimal masks to perform update on the global model wt (see Eq. (3)). This replacement
may bring additional errors to the convergence bound, as the hamming distance between the evo-
lutionary masks and optimal masks (i.e., the term dist(mk,t, mk，)) exists in our bound.
•	Error by dissimilar evolutionary masks. The theoretical result on the term dist(mk,t, mko,t) in
Φt indicates that the dissimilarity among the evolutionary masks may also play a role in optimizing
the ultimate PFL problem (P2). This term could be minimized if the evolutionary masks remain
the same throughout the training process, and would dominate the error if the masks are too
distinct. This observation exactly motivates us to present the naive mask searching technique
RSM in Algorithm 3, which forces all the clients to share the same random masks.
5 Experiments
In this section, we conduct extensive experiments on verifying the efficacy of the proposed FedSpa.
Our implementation of FedSpa is based on an open-source FL simulator FedML (He et al., 2020).
5.1	Experimental Setup
Dataset. We evaluate the efficacy of FedSpa on EMNIST-Letter (EMNIST-L henceforth), CIFAR10,
and CIFAR100 datasets. We simulate the client’s data distribution on Non-IID and IID setting. We
2Hamming distance measures the number of positions at which the two masks are different.
7
Under review as a conference paper at ICLR 2022
O 200	400	600	800 IOOO O 200	400	600	800 IOOO O 200	400	600	800 IOOO
Communication Rounds	Communication Rounds	Communication Rounds
--FedSpa (DST) - FedSpa (RSM) - Ditto - FedAvg -- Sub-FedAvg - Local -- Subsampling
Figure 3: Test Accuracy vs. Communication Rounds
simulate two groups of Non-IID settings via γ-Dirichlet distribution, named setting A and setting B.
Setting A and setting B respectively specify γ = 0.2, 0.1 for both EMNIST-L and CIFAR100, while
specify γ = 0.5, 0.3 for CIFAR10. Details of our simulation setting are available in Appendix B.1.
Baselines. We compare our proposed FedSpa with four baselines, including FedAvg (McMahan
et al. (2016)), Sub-FedAvg (Vahidian et al. (2021)), Ditto (Li et al. (2021)) and Local. We tune the
hyper-parameters of the baselines to their best states. Specifically, the regularization factor of Ditto
is set to 0.5. The prune rate each round, distance threshold, and accuracy threshold of Fed-Subavg
are fixed to 0.05, 0.0001, 0.5, respectively. We ran 3 random seeds in our comparison.
Models and hyper-parameters. We use LeNet5 for EMNIST-L, VGG11 for CIFAR10, and
ResNet18 for CIFAR100 in our experiment. We use a SGD optimizer with weight decayed parame-
ter 0.0005. The learning rate is initialized with 0.1 and decayed with 0.998 after each communication
round. We simulate 100 clients in total, and in each round 10 of them are picked to perform local
training (the setting follows McMahan et al. (2016)). For all the methods except Ditto, local epochs
are fixed to 5. For Ditto, in order to ensure a fair comparison, each client uses 3 epochs for training
of the local model, and 2 epochs for global model training. The batch size of all the experiments is
fixed to 128. For FedSpa, the pruning rate (i.e., αt) is decayed using cosine annealing with an initial
pruned rate 0.5. The initial sparsity of layers is initialized by ERK with scale parameter 1.
5.2 Main Performance Evaluation
We fix the dense ratio of FedSpa (DST), FedSpa (RSM), and the final dense ratio of Fed-SubAvg
both to 0.5 (i.e., 50% of parameters are pruned) in our main evaluation. Other hyper-parameters are
fixed as default. Figure 3 and Table 1 illustrate the training performance of different algorithms on
three datasets. We evaluate the performance based on the following metrics:
Final Accuracy. In the Non-IID setting, we show that FedSpa (DST) achieves remarkable per-
formance. Specifically, in Non-IID setting B of CIFAR100, FedSpa (DST) achieves respectively
4.4%, 11.9% and 21.9% higher final model accuracy, compared with Ditto, Sub-FedAvg and Fe-
dAvg. FedSpa (DST) seems to achieve better performance as the FL tasks becoming difficult (since
better performance is observed in a higher Non-IID extent, and in datasets that are intrinsically
more difficult). Interestingly, in the IID setting, we show that all the personalized solutions exhibit
some extents of performance degradation, which become more significant as the dataset becomes
challenging. The compression-based methods seem to be especially vulnerable in this setting. Our
interpretation for this phenomenon is that: since the information exchange between clients would
be limited by employing different sub-networks for training, the clients could not efficiently make
an effective fusion on their models through parameter averaging. This hypothesis is substantiated
by our experiment on FedSpa (RSM), an alternative implementation of FedSpa, which forces all
the masks to maintain the same sub-network. FedSpa (RSM) achieves commensurate performance
with FedAvg in the IID setting, outperforming the personalized solutions. The reason FedSpa (DST)
8
Under review as a conference paper at ICLR 2022
Table 1: Table illustrating performance of different methods.
Task	Method	IID			Non-IID					
					Setting A			Setting B		
		Acc	Comm Cost (GB)	FLOPs (1e14)	Acc	Comm Cost (GB)	FLOPs (1e16)	Acc	Comm Cost (GB)	FLOPs (1e14)
	FedSPa (DST)	92.2±0.1	7.0	2.0	95.3±0.1	7.0	2.0	96.5±0.2	7.0	2.0
	FedSPa (RSM)	92.9±0.1	7.0	2.0	91.9±0.2	7.0	2.0	90.6±0.9	7.0	2.0
EMNIST-L	Ditto	92.9±0.1	14.1	3.5	95.9±0.1	14.1	3.5	97.0±0.2	14.1	3.5
(LeNet)	FedAvg	93.5±0.2	14.1	3.5	92.3±0.3	14.1	3.5	90.9±0.8	14.1	3.5
	Sub-FedAvg	90.7±0.2	9.5	1.9	94.9±0.2	9.4	1.9	96.4±0.2	9.4	1.9
	Local	77.6±0.3	-	3.5	87.8±0.1	-	3.5	91.6±0.5	-	3.5
	SubsamPling	93.3±0.2	10.5	3.5	92.0±0.4	10.5	3.5	91.3±0.6	10.5	3.5
	FedSPa (DST)	83.4±0.1	369.2	172.9	86.6±0.5	369.2	173.3	88.2±0.4	369.2	173.5
	FedSPa (RSM)	84.5±0.1	369.2	172.9	82.1±0.2	369.2	173.3	80.9±0.2	369.2	173.5
CIFAR-10	Ditto	83.5±0.2	738.5	229.3	86.4±0.6	738.5	229.8	87.8±0.3	738.5	230.0
(VGG11)	FedAvg	84.8±0.3	738.5	229.3	82.0±0.4	738.5	229.8	81.4±0.4	738.5	230.0
	Sub-FedAvg	71.8±0.3	410.2	121.4	78.3±1.0	424.7	120.6	79.6±0.6	416.9	119.8
	Local	42.5±0.2	-	229.3	63.6±0.6	-	229.8	69.4±0.2	-	230.0
	SubsamPling	83.0±0.4	553.9	229.3	78.9±0.5	553.9	229.8	76.7±1.0	553.9	230.0
	FedSPa (DST)	41.5±0.5	448.8	705.1	59.0±1.0	448.8	704.9	66.9±0.2	448.8	704.8
	FedSPa (RSM)	54.6±1.1	448.8	705.1	48.7±0.5	448.8	704.9	44.6±0.5	448.8	704.8
CIFAR-100	Ditto	51.9±1.1	897.6	833.2	56.8±0.6	897.6	833.0	62.5±0.2	897.6	832.9
(ResNet18)	FedAvg	55.7±1.3	897.6	833.2	49.3±0.4	897.6	833.0	45.0±0.9	897.6	832.9
	Sub-FedAvg	38.3±0.8	616.5	494.1	49.2±0.7	624.4	508.4	55.0±0.7	612.8	496.1
	Local	10.3±0.3	-	833.2	28.8±0.1	-	833.0	40.5±0.4	-	832.9
	SubsamPling	49.8±1.3	673.2	833.2	42.3±0.8	673.2	833.0	37.6±1.1	673.2	832.9
performing better in Non-IID setting can also be explained by Theorem 1, which concludes that in a
Non-IID setting, setting proper sparsity for personalized models may promise a better convergence.
Convergence. As shown in Table 2, FedSpa achieves significantly faster convergence, which poten-
tially saves the communication rounds to train a model from scratch to a specific accuracy.
Table 2: Communication rounds to a fixed accuracy.
CIFAR10	IID			Non-IID					
				Setting A			Setting B		
	Acc@70	ACC@75	Acc@80	Acc@70	ACC@75	Acc@80	Acc@70	ACC@75	Acc@80
FedSPa (DST)	134.0±2.9	183.3±6.8	312.3±16.2	167.3±4.0-	210.3±4.2	281.3±19.1	164.3±5.0-	206.3±4.1	270.0±5.1
FedSPa (RSM)	101.3±1.7	141.3±6.2	237.0±6.4	195.3±10.7	271.3±16.2	471.7±19.8	252.0±12.7	339.0±20.6	614.0±72.8
Ditto	284.7±8.1	370.3±9.3	549.3±22.6	242.3 ±12.7	334.0±16.5	466.3±30.3	190.3±6.1	278.0±22.0	417.7±10.2
FedAvg	105.0±2.2	140.3±4.7	228.7±23.5	198.3±14.8	256.7±10.8	474.7±31.4	241.0±3.7	327.3±8.5	583.7±65.5
Sub-FedAvg	197.7±22.9	> 1000	> 1000	151.7±10.6	235.0±17.1	> 1000	137∙3±1.7	191.7±6.6	> 1000
SubsamPling	198.0±4.5	268.3±4.5	457.0±13.5	365.3±23.8	523.0±43.4	> 1000	466.3±15.0	722.7±105.1	> 1000
CIFAR100	Acc@40	Acc@50	Acc@55	Acc@40	Acc@50	Acc@55	Acc@40	Acc@50	Acc@55
FedSPa (DST)	536.3±35.9	> 1000	> 1000	236∙3±12.3	442.0±16.9	595.0±41.3	18L3±7.8	314.7±17.4	407.7±16.7
FedSPa (RSM)	239.3±4.1	435.7±25.8	> 1000	460.7±12.5	> 1000	> 1000	594.0±10.7	> 1000	> 1000
Ditto	545.7±19.4	868.7±56.1	> 1000	455.3±6.9	724.0±20.2	894.0±25.0	301.3±10.8	534.0±11.3	678.7±7.9
FedAvg	245.0±5.1	436.3±25.3	> 1000	470.7±25.4	> 1000	> 1000	589.7±46.7	> 1000	> 1000
Sub-FedAVG	> 1000	> 1000	> 1000	280.7±2.5	> 1000	> 1000	246.3±10.1	335.3±14.1	511.0±85.9
SubsamPling	460.7±16.6	> 1000	> 1000	845.3±59.2	> 1000	> 1000	> 1000	> 1000	> 1000
Training FLOPs and Communication. From Table 1, FedSPa (DST) achieves 15.4%〜42.9%
lower FLOPs than the dense solutions (e.g., Ditto, FedAvg), 13.0%〜28.2% lower communication
overhead than another model comPression solution Sub-FedAvg, and 50% lower communication
than the dense solution. The edge OfFedSPa (DST) stems from its training pattern - it is trained from
a sParse model, with constant sParsity throughout the training Process. However, it is interesting to
see that the training FLOPS of Fed-SubAvg is considerably lower than FedSPa, even under the same
sParsity setting. This Phenomenon stems from our ERK initialization, which is essential for the high
Performance of our solution, for which we will have a further discussion in APPendix B.4.
6	Conclusions
In this PaPer, we ProPose FedSPa, a Personalized FL solution that enables sParse-to-sParse train-
ing and efficient sub-model aggregation. As demonstrated by our exPeriments, FedSPa exhibits
outstanding Performance in the Non-IID setting, outPerforming other existing solutions in terms of
accuracy, convergence sPeed as well as communication overhead. Additionally, we Present theo-
retical analysis to evaluate the error bound of FedSPa towards the ultimate PFL Problem. Future
direction includes designing new model aggregation solutions for the sParse sub-network, and new
mask searching techniques sPecifically targeting on federated learning Process.
9
Under review as a conference paper at ICLR 2022
7	Reproducibility Statement
For sake of reproducibility of our solution, we make the following efforts: (i) In Appendix B.1, we
clearly state the data splitting method for IID and Non-IID data distribution. (ii) In Appendix B.2, the
network models we used in our experiment are clearly described. (iii) In Appendix B.4, the proposed
FedSpa with various hyper-parameters during the implementation are also systemically tested to
demonstrate its stability and superiority. (iv) In Appendix C, we give the self-contained proof of
Theorem 1. (v) At last, the source-code of FedSpa is enclosed in the supplementary material.
References
Manoj Ghuhan Arivazhagan, Vinay Aggarwal, Aaditya Kumar Singh, and Sunav Choudhary. Fed-
erated learning with personalization layers. arXiv preprint arXiv:1912.00818, 2019.
Guillaume Bellec, David Kappel, Wolfgang Maass, and Robert Legenstein. Deep rewiring: Training
very sparse deep networks. In International Conference on Learning Representations, 2018.
Hong-You Chen and Wei-Lun Chao. On bridging generic and personalized federated learning. arXiv
preprint arXiv:2107.00778, 2021.
Yiqiang Chen, Xin Qin, Jindong Wang, Chaohui Yu, and Wen Gao. Fedhealth: A federated transfer
learning framework for wearable healthcare. IEEE Intelligent Systems, 35(4):83-93, 2020.
Liam Collins, Hamed Hassani, Aryan Mokhtari, and Sanjay Shakkottai. Exploiting shared repre-
sentations for personalized federated learning. arXiv preprint arXiv:2102.07078, 2021.
Yuyang Deng, Mohammad Mahdi Kamani, and Mehrdad Mahdavi. Adaptive personalized federated
learning, 2020.
Tim Dettmers and Luke Zettlemoyer. Sparse networks from scratch: Faster training without losing
performance. arXiv preprint arXiv:1907.04840, 2019.
Utku Evci, Trevor Gale, Jacob Menick, Pablo Samuel Castro, and Erich Elsen. Rigging the lottery:
Making all tickets winners. In International Conference on Machine Learning, pp. 2943-2952.
PMLR, 2020.
Jonathan Frankle and Michael Carbin. The lottery ticket hypothesis: Finding sparse, trainable neural
networks. arXiv preprint arXiv:1803.03635, 2018.
FiliP Hanzely and Peter Richtarik. Federated learning of a mixture of global and local models. arXiv
preprint arXiv:2002.05516, 2020.
FiliP Hanzely, Boxin Zhao, and Mladen Kolar. Personalized federated learning: A unified frame-
work and universal oPtimization techniques. arXiv preprint arXiv:2102.09743, 2021.
Chaoyang He, Songze Li, Jinhyun So, Xiao Zeng, Mi Zhang, Hongyi Wang, Xiaoyang Wang, Pra-
neeth VePakomma, Abhishek Singh, Hang Qiu, et al. Fedml: A research library and benchmark
for federated machine learning. arXiv preprint arXiv:2007.13518, 2020.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. DeeP residual learning for image recog-
nition. In Proceedings of the IEEE conference on computer vision and pattern recognition, PP.
770-778, 2016.
Kevin Hsieh, Amar Phanishayee, Onur Mutlu, and PhilliP Gibbons. The non-iid data quagmire of
decentralized machine learning. In International Conference on Machine Learning, PP. 4387-
4398. PMLR, 2020.
Tzu-Ming Harry Hsu, Hang Qi, and Matthew Brown. Measuring the effects of non-identical data
distribution for federated visual classification. arXiv preprint arXiv:1909.06335, 2019.
Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deeP network training by
reducing internal covariate shift. In International conference on machine learning, PP. 448-456.
PMLR, 2015.
10
Under review as a conference paper at ICLR 2022
Siddhant Jayakumar, Razvan Pascanu, Jack Rae, Simon Osindero, and Erich Elsen. Top-kast: Top-k
always sparse training. Advances in Neural Information Processing Systems, 33, 2020.
Alexander Jung. Networked exponential families for big data over networks. IEEE Access, 8:
202897-202909, 2020.
Sai Praneeth Karimireddy, Satyen Kale, Mehryar Mohri, Sashank Reddi, Sebastian Stich, and
Ananda Theertha Suresh. Scaffold: Stochastic controlled averaging for federated learning. In
International Conference on Machine Learning, pp. 5132-5143. PMLR, 2020.
Jakub Konecny, H Brendan McMahan, Felix X Yu, Peter Richtarik, Ananda Theertha Suresh, and
Dave Bacon. Federated learning: Strategies for improving communication efficiency. arXiv
preprint arXiv:1610.05492, 2016.
Yann LeCun et al. Lenet-5, convolutional neural networks. URL: http://yann. lecun. com/exdb/lenet,
20(5):14, 2015.
Ang Li, Jingwei Sun, Binghui Wang, Lin Duan, Sicheng Li, Yiran Chen, and Hai Li. Lotteryfl:
Personalized and communication-efficient federated learning with lottery ticket hypothesis on
non-iid datasets. arXiv preprint arXiv:2008.03371, 2020.
Daliang Li and Junpu Wang. Fedmd: Heterogenous federated learning via model distillation, 2019.
Tian Li, Shengyuan Hu, Ahmad Beirami, and Virginia Smith. Ditto: Fair and robust federated
learning through personalization, 2021.
Paul Pu Liang, Terrance Liu, Liu Ziyin, Nicholas B Allen, Randy P Auerbach, David Brent, Ruslan
Salakhutdinov, and Louis-Philippe Morency. Think locally, act globally: Federated learning with
local and global representations. arXiv preprint arXiv:2001.01523, 2020.
Shiwei Liu, Decebal Constantin Mocanu, Amarsagar Reddy Ramapuram Matavalam, Yulong Pei,
and Mykola Pechenizkiy. Sparse evolutionary deep learning with over one million artificial neu-
rons on commodity hardware. Neural Computing and Applications, pp. 1-16, 2020.
Shiwei Liu, Lu Yin, Decebal Constantin Mocanu, and Mykola Pechenizkiy. Do we actually need
dense over-parameterization? in-time over-parameterization in sparse training. In Proceedings of
the 39th International Conference on Machine Learning, pp. 6989-7000. PMLR, 2021.
Yishay Mansour, Mehryar Mohri, Jae Ro, and Ananda Theertha Suresh. Three approaches for
personalization with applications to federated learning. arXiv preprint arXiv:2002.10619, 2020.
H Brendan McMahan, Eider Moore, Daniel Ramage, Seth Hampson, et al. Communication-efficient
learning of deep networks from decentralized data. arXiv preprint arXiv:1602.05629, 2016.
Decebal Constantin Mocanu, Elena Mocanu, Peter Stone, Phuong H Nguyen, Madeleine Gibescu,
and Antonio Liotta. Scalable training of artificial neural networks with adaptive sparse connec-
tivity inspired by network science. Nature Communications, 9(1):2383, 2018.
Hesham Mostafa and Xin Wang. Parameter efficient training of deep convolutional neural networks
by dynamic sparse reparameterization. International Conference on Machine Learning, 2019.
Sashank Reddi, Zachary Charles, Manzil Zaheer, Zachary Garrett, Keith Rush, Jakub Konecny,
Sanjiv Kumar, and H Brendan McMahan. Adaptive federated optimization. arXiv preprint
arXiv:2003.00295, 2020.
Yasmin Sarcheshmehpour, M Leinonen, and Alexander Jung. Federated learning from big data over
networks. In ICASSP 2021-2021 IEEE International Conference on Acoustics, Speech and Signal
Processing (ICASSP), pp. 3055-3059. IEEE, 2021.
Yasmin SarcheshmehPour, Yu Tian, Linli Zhang, and Alexander Jung. Networked federated multi-
task learning. arXiv preprint arXiv:2105.12769, 2021.
Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image
recognition. arXiv preprint arXiv:1409.1556, 2014.
11
Under review as a conference paper at ICLR 2022
Sebastian U Stich. Local sgd converges fast and communicates little. arXiv preprint
arXiv:1805.09767, 2018.
Saeed Vahidian, Mahdi Morafah, and Bill Lin. Personalized federated learning by structured and
unstructured pruning under data heterogeneity. arXiv preprint arXiv:2105.00562, 2021.
Paul Voigt and Axel Von dem Bussche. The eu general data protection regulation (gdpr). A Practical
Guide, 1st Ed., Cham: Springer International Publishing, 10:3152676, 2017.
Yuxin Wu and Kaiming He. Group normalization. In Proceedings of the European conference on
computer vision (ECCV), pp. 3-19, 2018.
Jing Xu, Sen Wang, Liwei Wang, and Andrew Chi-Chih Yao. Fedcm: Federated learning with
client-level momentum. arXiv preprint arXiv:2106.10874, 2021.
Hongwei Yang, Hui He, Weizhe Zhang, and Xiaochun Cao. Fedsteg: A federated transfer learning
framework for secure image steganalysis. IEEE Transactions on Network Science and Engineer-
ing, 2020.
Michael Zhang, Karan Sapra, Sanja Fidler, Serena Yeung, and Jose M Alvarez. Personalized feder-
ated learning with first order model optimization. arXiv preprint arXiv:2012.08565, 2020.
12
Under review as a conference paper at ICLR 2022
A Critical Components of FedSpa (DST)
ERK initialization. In Algorithm 2, We use Erdos-Renyi Kernel (ERK) originally proposed by
Evci et al. (2020) to initialize the sparsity of each layer. Specifically, the active parameters of the
convolutional layer initialized by ERK is proportional to 1 nnι-+nι3ι+h , where nl-1, n Wl and
hl respectively specify number of input channels, output channels and kernel’s Width and height in
l-1 l
the l-th layer. For the linear layer, the number of active parameters scale with 1 1-i+* where nl-1
and nl are the number of neurons in the l - 1-th and l-th layer. This initialization basically allows
the layer with less parameters have more proportion of active parameters.
Cosine annealing. Recall that we set the initial pruning rate as α0 and gradually decay it to 0
with cosine annealing Liu et al. (2021). The update of pruning rate with cosine annealing can be
formalized as: at = 0.5 X a0 X(1 + cos (T-Iπ) ). We perform this decay in order to ensure that
the network (specifically, its active coordinates) would not experience drastic change on the later
stage of training while ensuring that the mask searching is effective on the early stage of training.
B	Detailed Setting and Additional Experiments
B.1	Data Splitting Setting
In our implementation, we first split the training data (60k pieces of data for CIFAR10 and CI-
FAR100, and 145.6k for EMNIST-L, respectively) to clients for IID setting and Non-IID setting.
For the IID setting, data are uniformly sampled for each client. For the Non-IID setting, we use γ-
Dirichlet distribution on the label ratios to ensure uneven label distributions among devices as (Hsu
et al., 2019). The lower the distribution parameter γ is, the more uneven the label distribution will
be, and would be more challenging for FL. After the initial splitting of training data, we sample 100
pieces of testing data from the testing set to each client. To simulate the personalized setting, each
client’s testing data has the same proportion of labels as its training data. Testing of the personalized
model is performed by each client based on their personalized data, and the overall testing accuracy
is calculated as the average of all the client’s testing accuracy. In our experiment, we simulate dif-
ferent Non-IID settings. For CIFAR10, Non-IID setting A and B respectively specify γ = 0.5 and
γ = 0.3. For EMNIST-L and CIFAR100, since the number of the total labels are bigger3, we use
smaller γ, wherein setting A and B respectively specify γ = 0.2 and γ = 0.1.
B.2	Network Architectures
We follow the Caffe’s implementation of LeNet5 4 (LeCun et al., 2015), VGG11 (Simonyan &
Zisserman, 2014) and ResNet18 (He et al., 2016) to do the evaluation. Suggested by Hsieh et al.
(2020), DNNs with batch normalization layers (Ioffe & Szegedy, 2015) are particularly vulnerable
to the Non-IID setting, suffering significant model quality loss in the FL process. Following the
recommendation from Hsieh et al. (2020), we use group normalization (Wu & He, 2018) to substitute
the original batch normalization layer in both ResNet18 and VGG11.
B.3	Baseline Description
Below, we give a brief introduction of the baselines compared in our evaluations:
•	FedAvg (McMahan et al., 2016) is the vanilla solution of FL. It utilizes weights average to enable
all the clients to collaboratively train a global model, which efficiently absorbs knowledge from
personal data resided in clients.
•	Ditto (Li et al., 2021) is a personalized FL solution aiming to smooth the tension brought by the
data heterogeneity problem of FL. Ditto achieves personalization via maintaining both the local
models and global model. Specifically, within each round of iteration, each client first trains the
global model based on its local empirical loss (which shares the same procedure as FedAvg).
326 and 100 labels respectively in EMNIST-L and CIFAR100, while only 10 labels in CIFAR10.
4Available in https://github.com/mi-lad/snip/blob/master/train.py
13
Under review as a conference paper at ICLR 2022
After the global model is updated, each client additionally trains its local model based on a loss
function involving its local empirical loss and the proximal term towards the global model. This
local training phase is used to extract the global knowledge into each client’s local model. Since
each client has to maintain and train both local model and global model, Ditto might need extra
computation and storage overhead to achieve its personalization.
•	Local is the direct solution to the ultimate PFL problem (P2). Each client performs SGD based
on its local data, and there is no communication between clients. To mimic the FL setting, we
sample 10 out of 100 clients to do the local update on its local model after every 5 epochs of
training (same with the number of local epochs in a communication round that is performed by
other solutions). For sake of consistency, we still use 1 communication round to represent 5 local
epochs of Local in our evaluation.
•	Sub-FedAvg (Vahidian et al., 2021) is a prominent model compression-based PFL. Sub-FedAvg
maintains personalized sub-networks for each client. Training of Sub-FedAvg starts from a fully
dense model, and this solution iteratively prunes out the parameters and channels as the training
progresses. Finally, the commonly shared parameters of each layer are removed, and only the
personalized parameters that can represent the features of local data are kept.
•	Subsampling (Konecny et al., 2016) is a gradient-compression solution aiming to reduce the Com-
munication overhead of FL. The local training procedure is the same with FedAvg. The difference
is that Subsampling does not communicate the intact model for aggregation, but only commu-
nicates the sparse gradient update to the server for aggregation. Explicitly, in each round, the
sparse gradient update is produced through element-wisely multiplying a random mask. Different
from FedSpa, the randomized mask is independently generated in each round, and would only
be used to compress the gradient when uploading the gradient update (which means in the model
distribution phase, the model distributed would not be sparsified, and therefore would not save the
downlink communication cost).
B.4	Ablation Study
In this sub-section, we focus on the ablation study of FedSpa. Specifically, we study the impacts of
dense ratio, different mask initialization methods, and the gradient-involved weight recovery proce-
dure. Additionally, we present an interesting observation on the performance of the global model
trained by our personalized solution. Our ablation study is done with ResNet-18 on CIFAR100.
Impact of sparsity (aka. sparse ratio). Fixing other components and hyper-parameters to the
default value in our setup, we change the sparsity of FedSpa to 0.2, 0.4, 0.5, 0.6 and 0.8, to show
its impact on the algorithm performance. Experimental results are available in Figure 4 and Table
3. By our report, we observe that sparsity may impact learning performance under different data
distribution settings. For the IID setting, a higher sparsity seems to seriously degrade the training
performance, while for the Non-IID setting, properly sparsifying the model may even enhance the
final accuracy, and with a higher Non-IID extent, the benefit of sparsification reinforces. This ob-
servation surprisingly coincides with our theoretical conclusion given in Theorem 1, that setting the
sparse volume to a proper value when gradient dissimilarity is large (i.e., under a high extent of
Non-IID) could foster a better performance of personalized models. But too much sparsity, even in
the highly non-iid setting (e.g. γ = 0.1) leads to performance degradation. On the contrary, while
it is iid, the convergence could be dominated by the errors brought by sparsification, and setting the
mask to a higher sparsity could possibly enlarge these existing errors.
Another more intuitive interpretation for the impact of sparse ratio is from the perspective of infor-
mation exchange. Too much sparsification may limit the information exchange between the local
sparse models. If all the clients maintain an extremely high sparsity, the intersected coordinates
between clients’ local sparse models (or identically, their masks) would be small. Then the local
update averaging process (see Eq. (4)), the only way to extract global knowledge into the local
models, would not be effective. On contrary, while the sparsity is set to an extremely low value, the
personalized features of local models could be eliminated, since only limited coordinates in their
models are different.
ERK vs. Uniform sparsity initialization. Recall our mask initialization procedure in Algorithm 3
that the layer-wise sparsity is initialized by ERK. This in essence ensures that the layer-wise sparsity
of a model is scaled with the number of parameters in a layer. Liu et al. (2021) confirms the out-
14
Under review as a conference paper at ICLR 2022
Figure 4: FedSpa (DST) under different sparsity. Numbers in the labels are sparsity.
——FedSpa (DST)-0.8	——FedSpa (DST)-0.6
—— FedSpa (DST)-0.5	—— FedSpa (DST)-0.4	—— FedSpa (DST)-0.2
Table 3: Performance of FedSpa (DST) under different sparsity settings.
Non-iid
Sparsity				γ=0.2		γ=0.1			
	Acc	Comm Cost (GB)	FLOPs (1e16)	Acc	Comm Cost (GB)	FLOPs (1e16)	Acc	Comm Cost (GB)	FLOPs (1e16)
0.2	51.5±0.8	718.1	8.2	62.9±0.4	718.1	8.2	65.5±0.5	718.1	8.2
0.4	45.5±0.9	538.6	7.6	61.4±0.6	538.6	7.5	67.2±0.4	538.6	7.5
0.5	41.5±0.5	448.8	7.1	59.0±1.0	448.8	7.0	66.9±0.2	448.8	7.0
0.6	38.4±0.6	359.0	6.5	57.3±1.5	359.0	6.5	65.2±0.2	359.0	6.5
0.8	32.0±0.7	179.5	4.6	49.2±1.8	179.5	4.6	56.7±0.8	179.5	4.6
standing effect of ERK initialization in improving overall training performance over the centralized
training primitive, but it remains unexplored how it performs in our proposed distributed training
framework. Below, we show in Figure 5 how accuracy evolves with communication rounds under
ERK and Uniform 5 initialization. As shown, a drastic drop of accuracy is observed by replacing
ERK with Uniform, by which we conclude that ERK is an essential component for FedSpa (DST).
∞I+Jα>NSφH) OOIHWD
Communication Rounds
Communication Rounds
FedSpa-DST (ERK)
FedSpa-DST (Uniform)
Communication Rounds
Figure 5: Layer-wise sparsity initialized by ERK or Uniform. Sparsity of FedSpa is fixed to 0.5.
However, though a significant accuracy enhancement is observed, we note that integrating ERK may
sacrifice potentially more FLOPS reduction. This observation can be found in Table 6, wherein our
results show that initialization with Uniform can save 34.3% FLOPs of that with ERK.
Table 4: Performance of FedSpa under ERK and Uniform initialization.
no	Non-iid
Methods				γ=0.2			Y=0.1		
	Acc	Comm Cost (GB)	FLOPs (1e16)	Acc	Comm Cost (GB)	FLOPs (1e16)	Acc	Comm Cost (GB)	FLOPs (1e16)
ERK	41.5±0.5	448.8	7.1	59.0±1.0	448.8	7.0	66.9±0.2	448.8	7.0
Uniform	33.0±1.4	448.8	4.6	49.3±0.9	448.8	4.6	56.3±1.1	448.8	4.6
Different or same mask initialization. Recall that based on the layer-wise sparsity calculated by
ERK, FedSpa uses the same random seed to initialize the mask, so as to make the mask exploration
5Uniform enforces the same sparsity for all the layers in a model.
15
Under review as a conference paper at ICLR 2022
of all clients started from the same mask. In the folloWing, We giVe another implementation that
alloWs each client to share different masks in the beginning.
As shoWn in Figure 6, We surprisingly find that for FedSpa (DST), maintaining different masks in
initialization may slightly enhance its training performance in IID and Non-IID (γ = 0.2) setting.
We hypothesize that by different mask initialization, each client could more efficiently search for
their optimal masks that better represents the features and labels of the personal data.
IID
Figure 6: Initialization based on same or different masks. Sparsity of FedSpa is fixed to 0.5.
— FedSpa (DST, same intialization) --------------------- FedSpa (RSMf same initialization) ----------------- FedSpa (DST, different initialization) ----------------- FedSpa (RSM, different initialization)
For FedSpa (RSM), compared with initialization using the same mask, different mask initialization
may result in a drastic performance loss in the IID setting, and a significant improvement in the Non-
IID setting. With the same mask initialization of RSM, each client consistently trains based on the
same sub-network, which completely eliminates personalization. So this setting shares a similar per-
formance With FedAVg - With satisfactory performance in IID setting and rather weak performance
in Non-IID setting. On contrary, by initializing different masks in the beginning, FedSpa (RSM) re-
serVes some degrees of personalization, since only the intersected coordinates in their local models
are shared and updated by the information exchange (i.e., aVerage) process. Consequently. FedSpa
(RSM) With different mask initializations has a similar performance pattern With FedSpa (DST).
Another interesting obserVation is that FedSpa (RSM) With different mask initialization cannot out-
perform FedSpa (DST) in both the tWo groups of Non-IID settings. This indicates that the DST
mask searching process is effectiVe to achieVe a superior performance of FedSpa in Non-IID setting.
Weight recovery w/ or w/o gradient information. Recall that in FedSpa (DST), We proposed to
use gradient information to recoVer the pruned Weights, Which is empirically proVen in (EVci et al.,
2020) to outperform its random recoVery counterpart in Set (Mocanu et al., 2018). Specifically,
for gradient information-based recoVery, the Weight coordinates With the top-αt magnitude of the
gradient Would be recoVered, While for random recoVery, the coordinates are recoVered randomly.
To demonstrate the impact of the Weight recoVery method oVer FedSpa (DST), in Figure 7, We
compare the gradient information-based recoVery With random recoVery. Our experimental result
demonstrates that recoVery With gradient information could slightly accelerate the conVergence and
enhance the final accuracy in our FedSpa frameWork.
IlD	Non-IID (y = 0.2)	Non-IID (γ = 0.1)
Figure 7: Recovery with gradient information or random recovery. Sparsity is fixed to 0.5.
Global model vs. Personalized model. In our main experimental result, all the testings are con-
ducted by clients based on their own personalized models. But it is interesting to evaluate whether
16
Under review as a conference paper at ICLR 2022
Table 5: Wall time of FedSpa (DST) for local training and mask searching.
Task	Wall Time (Train)	Wall Time (Mask Search)	Ratio (Mask Search/Train)
EMNIST-LeNet (CPU)	1.03±0.04s	0.09±0.0s	8.92%±0.6
CIFAR10-VGG11 (CPU)	11.4±0.25s	2.19±0.11s	19.19%±1.09
CIFAR100-Resnet18 (CPU)	28.61±0.38s	3.7±0.28s	12.93%±1.08
EMNIST-LeNet5 (GPU)	0.39±0.01s	0.02±0.0s	5.66%±0.25
CIFAR10-VGG11 (GPU)	1.56±0.03s	0.22±0.01s	14.3%±0.48
CIFAR100-Resnet18 (GPU)	2.71±0.01s	0.33±0.01s	12.06%±0.2
the global model trained by FedSpa itself could converge, or even could achieve commensurate per-
formance with the global model trained by general FL solution (e.g. FedAvg). As demonstrated
by Figure 8, we empirically find that in the IID setting, the global model trained by FedSpa cannot
recover the performance of that trained by FedAvg, and a considerable performance drop is also ob-
served in the Non-IID setting. Another observation is that the global model of FedSpa surprisingly
maintains roughly the same performance as its personalized models in the IID setting, but conceiv-
ably suffers significant performance loss in the Non-IID setting. This corroborates our conclusion
that sub-networks extracted from a global model may potentially outperform the full model, under
the condition that the data distributions of clients are skewed (or heterogeneous).
∞sφNSφesOoTHwU
Figure 8: Global model vs. Personalized models. Sparsity of FedSpa is fixed to 0.5.
— FedSpa (DST with personalized model) ---------- FedSpa (DST with global model) --------- FedAvg (global model)

Wall time. Recall that we do an additional mask searching procedure in FedSpa (DST), which might
possibly induce extra wall time on the local devices. We show in Table 5 the wall time used for
training and mask searching in one single local round. The sparsity used for this experiment is fixed
to 0.5, while other parameters remain the default setting (see Section 5.1). We use one single 1080Ti
to perform training for the GPU-based experiment, while the CPU-based experiment is conducted on
an Intel(R) Xeon(R) CPU E5-2620 v4 @ 2.10GHz with 8 cores. Our experimental results confirm
that the mask searching process only accounts for a small portion of wall time (approximately 5% -
20%) for the entire computation time on the local devices.
C Proof of Theorem 1
In this section, for sake of readability, we first clarify the notations and reiterate several facts that we
use in our proof. Then we present several lemmas that are commonly used in the FL literature (see
(Karimireddy et al., 2020; Xu et al., 2021)). Later, several key lemmas are listed with exhaustive
proof, and finally, the proof of our main theorem is given by exploiting the listed lemmas and facts.
C.1 Notations and Facts
Throughout the proof, we assume Pτ equivalent to PτN=-01, Pk equivalent to PkK=1, and Pk,τ
equivalent to PkK=1 PτN=-01 unless otherwise specified. In our proof, we reuse most of the notations
17
Under review as a conference paper at ICLR 2022
from our problem formulation part. We use gk,t,τ (Wk,t,τ) = Vwk,t,τ L(Wk,t,τ； ξk,t,τ) to denote the
stochastic gradient of client k in round t and at step τ .
Then as per our formulation in Section 4.1, we reiterate the following facts, which would be heavily
used in our proof.
Fact 1 (Local step). As per Eq. (3), one local step of client’s update can be formalized as follows:
Wk,t,τ +1 = Wk,t,τ 一 ηmk,t Θ gk,t,τ(wk,t,τ),	(6)
where gk,t,τ (Wk,t,τ) is the stochastic gradient over the sparse model weights Wk,t,τ, and Wk,t,0 二
mk,t	Wt is the synchronized local weights at the beginning of a communication round.
Fact 2 (Local update from client k). The local update of clients can be formalized as follows:
Uk,t = Wk,t,0 ― Wk,t,N = η	mk,t Gl gk,t,τ(Wk,t,τ ).	(7)
τ
Fact 3 (Server’s update). The server aggregates the sparse update by averaging, which can be
formalized as follows:
1
Wt+1 = Wt ― S): Uk,t = Wt ― S): jmk,t Θ gk,t,τ(Wk,t,τ).	(8)
k∈St	k∈St,τ
Fact 4 (Global and local loss). The local loss of a client is denoted by Fk (W k), and is formulated
as:
Fk(Wk)= E[L(χ,y)〜Dk (Wk; (x,y))]	(9)
where Wk = mk Θ W, and the global loss can beformalized asfollows:
1K	1K
f (W) = KfFk (Wk) = KEFk (mk θ w)	(IO)
k=1	k=1
and {mk} in the function are viewed as optimal personalized masks which satisfies the sparse
volume constraint ∣∣1 一 m1 ∣∣o = β for all k.
C.2 Auxiliary Lemmas
In the following. we shall present several common lemmas that are heavily used in the FL literature.
Lemma 1 (Cauchy-Schwarz). Assume arbitrary vector sequences {ak}k=1,...,K and {bk}k=1,...,K,
Cauchy-Schwarz inequality implies:
XK akbk2≤XK ∣ak∣2!XK ∣bk∣2!,	(11)
k=1	k=1	k=1
by taking bk = 1, we also have:
K 2 K
X ak	≤K X∣ak∣2 ,	(12)
k=1	k=1
Lemma 2 (Separating mean and variance, Lemma B.3 (Xu et al., 2021)). Let {a1, . . . , aτ} be
τ random vectors in Rd . Suppose that {ai 一 ξi} form a martingale difference sequence, i.e.
E [ai 一 ξi | a1, . . . , ai-1] = 0, and suppose that their variance is bounded by E
σ2. Then, the following inequality holds:
∣ai 一 ξi ∣2
≤
2
τ
Xai
i=1
≤2
E
τ
X ξi + 2τσ2 .
i=1
Lemma 3 (Relaxed triangle inequality, Lemma 3 (Karimireddy et al., 2020)). Let vi and vj be
vectors in Rd. Then the following inequality holds true for any a > 0:
Ilvi + VjII2 ≤(1 + a) ιιvik2 + (ι + a) kvjk2.	(13)
18
Under review as a conference paper at ICLR 2022
0
∙ -∙	∙ ∙ ∙	,
Lemma 4. For random vector vι SatiSfymg E[vι] =	, and assume another random vector
[0]
v2 is independent with v1 , we have:
E[kv1+v2k2]=E[kv1k2]+Ekv2k2
Proof.
E[kv1 + v2 k2] = Ehv1 + v2, v1 + v2i
= Ekv1k2 + Ekv2k2 + 2Ehv1, v2i
= Ekv1k2 + Ekv2k2 + 2hEv1, Ev2i
= Ekv1k2 + Ekv2k2
(14)
(15)
This completes the proof.
□
C.3 Key Lemmas
In this section, we present several important lemmas that would be used in our formal proof. All the
presented claims are rigorously proved.
Lemma 5 (Smoothness of f (w)). Assume Wk = m' Θ W for any m∖ ∈ {0,1}d, we have L-
smoothness for f (W) = -ɪ Ek Fk (Wk), i.e.,forany wι, w? ∈ Rd, we have:
∣∣Vf(wι) - Vf (w2)k ≤ L∣∣W1 - W2∣∣.
Proof.
l∣Vf (W1) - Vf (W2)k = K17 X(mk Θ VFk(m( Θ wι) - m( Θ VFk(m( Θ w?))
k
≤ Kk X kmk θ (VFk (mk θ WI)- VFk (mk θ w2 ))k
k
≤ KK X kVFk (mk θ wi ) - VFk (mk θ W2)k
k
(a)	1
≤ LK Okmk θ (WI- W2)k
≤ L∣W1 - W? ∣
(16)
(17)
where the first equality holds by the Fact 4 and inequality (a) is due to Assumption 3. This completes
the proof.	□
2Nσ?
+ S
Lemma 6 (Separating mean and variance of stochastic gradient). For mk,t ∈ {0, 1}d, We have the
following bounding for the expected average gradient:
Et J； X mk,t Θgk,t,τ(Wk,t,τ)| j ≤ 2Et ]s X 1mk,t Θ VFk(Wk,t,T)|
where Et [∙] denotes the expectation over all the randomness of round t.
(18)
19
Under review as a conference paper at ICLR 2022
Proof. We view 1 m%t Θ gk,t,τ (Wk,t,τ) for all k and T as stochastic vectors. By the unbiasedness
of stochastic gradient, we know their variance satisfies:
|| 1 mk,t Θ gk,t,T(Wk,t,τ) — E[1 mk,t Θ gk,t,τ(Wk,t,τ)||2]
SS
=|| 石 mk,t θ (gk,t,τ (W k,t,τ ) ― VFk (Wk,t,τ )) ||2
S
≤ S llgk,t,τ(W k,t,τ ) - VFk (Wk,t,τ )||2
σ2
≤百
(19)
where the first inequality holds since mk,t ∈ {0, 1}d and the last inequality is due to Assumption 2.
As Per the variance given above, and directly apply Lemma 2, the claim immediately shows. □
Lemma 7. We have the following error bound by introducing personalized masks:
kmk,t Θ VFk(Wk,t,τ) - mkVFk(Wk,t)k2 ≤ ∣∣VFk(Wk,t,τ) - VFk(Wk,t)k2 + dist(mk,t, mk)B2
(20)
and,
3
∣∣mk,t ΘVFk (Wk,t)-Vf (Wt) k2 ≤ 3(d-β)G2+KfB (d%st(mk,t, mko ,t)+dist(mko ,t, mk，))
k0
(21)
where dist(m1, m2) is the hamming distance between two masks.
Proof. Let Vj)Fk(Wk,t,τ) and Vj)Fk(Wk,t) be the derivative over the j-th weight coordinates.
Define the support of a vector v ∈ Rd as supp(v) = {j : v(j) 6= 0}. The left hand side of the first
claim can be rewritten and bounded as follows:
∣∣mk,t Θ VFk(Wk,t,τ) - mk Θ VFk(Wk,t)∣2
=	X	kVj)Fk(Wk,t,τ)-Vj)Fk(Wk,t)∣2
j ∈supp (mk ,t )∩supp (mk)
+	X	∣Vj)Fk(Wk,t,τ )k2 +	X	∣Vj)Fk (Wk,t)k2
j∈supp(mk,t)andj∈supp(m* )	j/supp(mk,t)andj∈supp(m1)
≤)	X	∣Vj)Fk (Wk,t,τ)-Vj)Fk (Wk,t)k2 + X B2
j /supp (mk ,t) ∩supp (mk)	j∈supp(mk,t ㊉ mk)
=	X	∣Vj)Fk (Wk,t,τ)-Vj)Fk (Wk,t)k2 + dist(mk,t, mk)B2
j /supp(mk ,t )∩supp(mk)
≤ X ∣Vj)Fk (W k,t,τ)-Vj)Fk (W k,t)k2 + dist(mk,t, mk )B2
j
≤∣VFk(Wk,t,τ) - VFk(Wk,t)k2 + dist(mk,t, mk)B2
(22)
where inequality (a) is true since we have ∣Vj)Fk(Wk,t,τ )∣2 ≤ B2 and ∣Vj)Fk (Wk,t)∣2 ≤ B2 by
coordinate-wise bounded gradient Assumption 4. Inequality (b) is due to the definition of hamming
distance, dist(vι, v2) = Pj∙/supp(v]㊉⑴” ThiS shows our first claim.
20
Under review as a conference paper at ICLR 2022
Following the same technique, we expand the left hand side of the second claim using Cauchy
Schwarz inequality in Lemma 1, as follows:
∣∣mk,t Θ VFk(Wk,t) - Vf (wt)k2
= kmk,t Θ VFk(Wk,t) - -1 X mko Θ VFk，(Wk，,t)『
k0
≤3∣mk,t Θ VFk(Wk,t)— K X mk,t ΘVFko(Wko,t)k2
k0
+ 3∣K- X(mk,t Θ VFk，(Wk0,t) — mk0,t Θ VFk>(τWko,t))k2
k0
+ 3∣∣-1 X(mk0,t Θ VFk0(Wk0,t) — mko Θ VFko(Wk0,t))∣∣2
k0
≤3∣mk,t Θ VFk(Wk,t)— K X mk,t θ VFkO(WkO,t)k2
k0
3
+ K E ∣∣mk,t Θ VFkO(WkOt - mko,t Θ VFko(Wk，,t)『
kO
3
+ — E k(mko,t Θ VFko(Wk，/)—mko Θ VF^(Wη))『
kO
33
≤3(d — β)G2 + KE dist(mk,t, mk，,t)B2 + KE dist(mk，,t, mQB
kO	kO
3
=3(d — β)G2 + Kf B2(dist(mk,t, mk，,t) + dist(mk，,t, m5))
kO
(23)
For the last inequality, we use the same technique as in the first claim to bound the second term and
the third term, and the first term is bounded by Lemma 8.	□
Lemma 8. Let ∣1 — m∣0 = β be the sparse volume of a mask m, and suppose m ∈ {0, 1}d, by
assumption 4, we have the following relation:
km Θ (VFk(Wk,t) - -K X NFkO(W忆))『≤ (d — β)G2	(24)
kO
and,
∣VFk(Wk,t) - -K Xm ΘVFkO(Wk,t)∣2 ≤ (d — β)G2 + βB2	(25)
kO
where d is the dimension of the models (or identically, the dimension of the mask m).
Proof. The proof of this lemma follows a similar technique in Lemma 7. Let V(j) Fk (Wk,t) be the
derivative over the j-th weight coordinate. Define the support of a vector v ∈ Rd as supp(v) = {j :
v(j) 6= 0}. The left hand side (L.H.S) of our first claim can be bounded by:
∣∣m Θ (VFk(Wk,t) — K X VFkO(Wk,t))∣2
kO
=X	∣V⑶Fk(Wk,t) - ɪ XSFk，(WkO,t)∣2	(26)
K
j ∈supp(m)	kO
≤(d — β)G2
where the last inequality is obtained from the sparsity constraint and the coordinate-wise gradient
dissimilarity Assumption 1. This shows our first claim.
21
Under review as a conference paper at ICLR 2022
For our second claim, we expand its L.H.S as follows:
∣∣VFk(Wk,t) - K X m Θ VFk，(Wk，,t)『
k0
=X	kV⑶Fk(Wk,t)- K X V(j)Fk，(Wk，,t)k2 + X	kV⑶Fk(Wk,t)k2	(27)
j∈supp(m)	k，	j ∈/ supp(m)
≤(d - β)G2 + βB2,
where the last inequality is due to Assumption 1 and Assumption 4. It completes the proof. □
Lemma 9 (Drift towards Synchronized Point). For any t ∈ {1, . . . , T }, τ ∈ {0, . . . , N}, and
learning rate satisfies η ≤ N, we have thefollowing claim:
K XEt[kWk,t,τ — Wk,tk2] ≤5Nη2(σ2 + 18NΦt) + 30N2η2kVf(wt)k2	(28)
k
where Φt = Kk Pk((d — β)G2 + Kk Pk，B2(dist(mkt, mk，,t) + dist(mk，,t, mk))) and EtH
denotes the expectation over all the randomness of round t.
Proof. We follow the basic techniques from (Reddi et al., 2020) to prove this lemma.
We first assume that:
•	O = mk,t Θ gk,t,τ-ι(tWk,t,τ-ι) — mk,t Θ VFk(Wk,t,τ-ι)
•	P = mk,t Θ VFk(Wk,t,τ-1 — mk,t Θ VFk(Wk,t)
•	Q = mk,t Θ VFk(Wk,t) — Vf (wt)
We can expand Wk,t,τ as follows:
K XEt [∣∣wk,t,τ — Wt『]
k
F	=I~k XEt [kWk,t,τ-1 — Wt — ηmk,t Θ gk,t,τ-ι(Wk,t,τ-ι)k2]
k
=KXEt [∣∣wk,t,τ-i — Wt — n(O + P + Q + Vf(Wt))I∣2]
k
≤	+ KT XEtkW^ktT-1 — Wtk2 + K XEtkOk2 + 6弋 XEtllPk2
k	kk
+	苧 XEtkQk2 + 苧 X kVf(Wt)k2
KK
kk
(29)
where the last inequality follows from Lemma 3 and Lemma 4. Explicitly, we use Lemma 4 to treat
the stochastic term with Et kOk2, and then we use Lemma 3 with a = 2N — 1 to separate the other
four terms.
Then we proceed by separately bounding the components in the above inequality.
Bounding the second term:
22
K ^X EtkOk 2 = K ^X Etllmk,t θ gk,t,τ-1(Wk,t,τ-1) — mk,t θ VFk (Wk,t,τ-I)Il2
k
k
η2	2
=~K)： Et kmk,t θ (gk,t,τ —1 (W^k,t,T - 1) - V Fk (Wk,t,τ ―l))k
k
η2
≤ K ɪsEtkgk,t,τ-I(W k,t,T-I)-VFk (W k,t,τ-1 )『
k
≤ η2σ2
(30)
22
Under review as a conference paper at ICLR 2022
where the last inequality holds by Assumption 2.
Bounding the third term:
-K~~ X Et IlPk2 = -K- X EtIlmk,t Θ VFk (w)k,t,τ-1) - mk,t Θ VFk (wk,t)k2
kk
≤ -K~ X EtllVFk (Wk,t,τ-l) -VFk (W k,t)∣∣2	(31)
k
≤(6Nr∕2L2)K XEtkWk,t,τ-ι - wk,tk2
k
Bounding the fourth term:
展 X EtkQk2
k
=-K~ X Etkmk,t Θ VFk (Wk,t) - Vf(Wt)k2	(32)
k
18N η2	1
≤ -k~~ £((d — β)G2 + K £ B2(dist(mk,t, mk，,t) + dist(mko,t, mk,))),
k	k0
where we use the second claim in Lemma 7 in the last inequality.
Let Φt = k1 Pk((d — β)G2 + k1 Pko B2(dist(mk,t, mk0,t) + dist(mk0,t, mkk0))). The bound can
be simplified as follows:
6Nη2 XEtkQk2 ≤ 18Nη2Φt.	(33)
K
k
Putting together: Plugging all the components into Eq.(29) , the following result immediately
follows:
K XEt [|lWk,t,T - Wk,t∣∣2]
k
≤(1 + 2N1- 1 + 6Nn2L2) KK X EtkW k,t,τ-1 — W k,t k2 + η2σ2 + 18Nη2Φt + 6Nη2kVf (Wt)k2
k
≤(1 + NIl) JXEtkWk,t,τ-ι - Wk,t『+ η2(σ2 + 18Nφt) + 6Nη2kVf(Wt)『,
N—1 K
k
(34)
where the last inequlity holds by our assumption η ≤ 釜N
Unrolling the recursion, we obtain the following results:
K XEt |||Wk,t,T - Wk,t『]
k
N-1	1
≤ £(1 + N-I)τ [η2(σ2 + 18NΦt) + 6Nη2kVf(Wt)k2]
τ=0
≤(N - 1) × ((1 + N1-1 )N — l) [η2(σ2 + 18NΦt) + 6Nη2kVf(Wt)k2]
≤5Nη2(σ2 + 18NΦt) + 30N2η2kVf(Wt)k2
(35)
The last inequality holds since (1 + N-I)n - 1 ≤ 5 for N ≥ 1. This completes the proof. □
23
Under review as a conference paper at ICLR 2022
C.4 Formal Proof
We start our proof by expanding f(wt+1) under its smoothness condition (see Lemma 5), which
indicates that:
Et [f (wt+1) | wt]
≤f(wt) — hVf (wt), Et[wt+1 — Wt D + IL Et∣∣wt+1 — Wt ||2
Fa=ct 3f (wt) -ηEt
KVf(Wt) ,1 X Uk) + + LEt
F=2f (Wt)- N 卜Vf(Wt), Et
2
s X Ukj
≤f (Wt)- N2- ||Vf(Wt) ||2 + 2N Et || K X mk,t © VFk(Wk,t,τ) - N Vf(Wt) ||2
k,τ
'------------------------{------------------------}
T1
2
2
+ 2Et S X Uk,t
k∈St
'----------{----------}
T2
(36)
where the last inequality holds since -ab ≤ 2((b 一 a)2 一 a2), and EtH is the expectation over all
the randomness in round t.
In the following, we shall separately bound T1 and T2 .
Bounding T1 :
T1 =2NEt11 K X mk，t © VFklWk,t,τ) 一 NVf(Wt) ||2
k,τ
F= 4 2N Et|| K X mk,t © VFk(Wk,t,τ ) - N K X mk © VFk (W k,t)∣∣2
k,τ	k
=2n Et|| K X(mk,t © VFk (Wk,t,T ) - mk © VFk(Wk,t))∣∣2
k,τ
(a)	η
≤ 2K EEtImk,t © VFk(Wk,t,τ) - mk © VFk(Wk,t)∣∣2
k,T
(b)	η
≤ 2K E(kVFk(Wk,t,τ) - VFk(Wk,t)k2 + dist(mk,t, mk)B2)
k,T
≤ NK X EtllWk,t,τ- W k,t∣ι2 + 2K X dist(mk,t, mk )B2
k,τ	k
(37)
where inequality (a) is due to Cauchy-Schwarz inequality (i.e., Lemma 1), (b) follows from the first
claim in Lemma 7, and the last inequality holds by Assumption 3.
Plugging the results of Lemma 9, we obtain that:
Ti ≤ nL2N(5Nη2(σ2 + 18NΦt) + 30N2η2kVf (Wt)k2) + 2K X dist(mk,t, mk)B2
k
≤ 5N2η3L2 (σ2 + 18NΦt) + 15N3η3L2∣∣Vf (Wt)『+ 2K X dist(mk,t, mk)B2
k
(38)
24
Under review as a conference paper at ICLR 2022
Bounding Tqi :
2
T2 =2Et S X Uk,t
k∈St
T Et
S X mk,t © gk,t,T(Wk,t,τ)
k∈St ,T
(a)
≤ Lη2Et
IL	Jl	NLη2σ2
S T mk,t ©PFk (Wk,t,τ) 11 I + S
k∈St ,τ
(39)
≤Lη2NXEt || |1 X mk,t ©VFk(Wk,t,τ) | j + NLS^
S-------------------------V---------------}
T3
where (a) is obtained as per Lemma 6.
Bounding T3：
T3
X X 1{i∈St}mi,t
∖i∈[K]
©VFi (Wi,t,τ), E I{j∈St}mj,t ©VFj (Wj,t,
j∈[K]
)
=S12Et	X	EStII{i∈St∩j-∈St}] hmi,t OVFi(Wi,t,τ), mj,t ©VFj (Wj,t,τ)〉
i,j∈[K],j=i,τ
+ X ESt[I{i∈St}]kmi,t © VFi(Wi,t,τ)∣∣2
i
=S2Et	X	K(K _?)hmi,t © VFi(Wi,t,τ),mj,t © VFj(Wj,t,τ)〉+ X Kιιmi,t © VFi(Wi,t,τ)『
[i,j∈[K],j=i I'	i
=S12 Et	X	K (K -II)) hmi,t OVFi(Wi,t,τ ), mj,t ©VFj (Wj,t,τ )〉
[i,j∈[K]	' J
+ X KK-S) ∣gi,t © VFi(Wi,t,τ)k2
≤Et
K Il X mk,t © VFk 3k,t,τ )∣∣2 +
k
(K - S)
SK(K - 1)
E ∣∣mk,t © VFk(Wk,t,τ)∣∣2
k
{z
T4
{z
T5
(40)
、
/
/
where the last inequality holds since SKSK-I) = SK⅛k ≤ SKT = K.
25
Under review as a conference paper at ICLR 2022
Bounding T4 :
T4
(，X mk,t © VFk(Wk,t,τ) - Vf(Wt)) + Vf(Wt))
k
Lemma 1
≤2
K X mk,t OVFk (Wk,t,τ)-Vf(wt)
k
2
+ 2 kVf (wt)k2
Fa=ct 22
K X(mk,t © VFk (Wk,t,τ) - mk
2
©VFk (Wk,t))	+2 kVf (wt)k2
(41)
2
Lemma 1 2
≤ K E ∣∣mk,t © VFk(Wk,t,τ) - mk VFk(Wk,t)『+ 2 IlVf(Wt)『
k
Lemma 7 2
≤ K E(∣VFk(Wk,t,τ) - VFk(Wk,t)k2 + dist(mk,t, mk)B2) + 2 ∣Vf (wJ『
k
2L2	2
≤ ɪ E kWk,t,T - Wk,t『+ KfdiSt(mk,t, mk )B2 + 2 kVf (Wt)『,
kk
where the last inequality is obtained by the L-smoothness Assumption 3.
Bounding T5 :
T5 =q(K- S)C X kmk,t © VFkIWkF)k2
SK (K - 1)
≤ SK(K -S)) X(kmk,t © VFk (W k,t,τ ) - mk,t © VFk (W k,t)k2
+ ∣∣mk,t © VFk(Wk,t) - Vf(Wt)I2 + ∣∣Vf(Wt)k2)
(a)3L2(K-S)	2	3(K-S)	2	(42)
≤ SK(K -1) ∑ kWk,t,τ - Wk，tk + S(K - 1)K ∑(3(d - β)G
+K X B2(dist(mk,t, mk0,t) + dist(mk0,t, mko))) + S((K-S) kVf(Wt)k2
3L2
≤Kζ~ X kWk,t,τ - Wk,tk +9φt +3kVf (Wt)II ,
k
where the last inequality holds since S(K-I) ≤ 1 under the condition S ≥ 1. Inequality (a) is
obtained by Assumption 3 and the second claim in Lemma 7 .
Summing T4 and T5, we have the following bounding for T3:
5L2	2
T3 ≤ ɪ E II^Wk,t,τ — W k,tk + Kf dist(mk,t, m∙k)B 2 + 9Φt + 5∣∣Vf(Wt)∣∣2	(43)
kk
Plugging Lemma 9 into the above inequality, we have:
T3
2
≤5L2 (5Nη2(σ2 + 18N Φt) + 30N 2η2 IlVf(Wt)∣∣2) + KfdiSt(mk,t, mQB2 +9Φt + 5∣Vf (Wt)∣2
2
=25η2 L2N (σ2 + 18N Φt) + (150N 2η2L2 + 5)IVf (Wt)I2 + KfdiSt(rmk,t, mk )B2 + 9Φt
k	(44)
Plugging T3 into Inequality (39), we bound T2 as follows:
T2 ≤(150N4η4L3 + 5Lη2N2)IVf(Wt)I2 + 25η4L3N3(σ2 + 18NΦt)
+ 至NL X dist(mk,t, mk)B2 + 9η2N2LΦt + NLF.	(45)
k
26
Under review as a conference paper at ICLR 2022
Plugging T2 and T1 into R.H.S of Inequality (36), we obtain that:
E [f (wt+ι | wt)] ≤ f (wt) - nN(1 - 150N3η3L3 - 5LnN - 15N2η2L2)||Vf (wt) ||2
+ (25n4L3N3 + 5NNy )(σ2 + 18NΦt) + iNtK nN X dist(m®,t, mk)B2
k
2 2	N Lη2σ2
+ 9η2N 2LΦt + —!—
S
(46)
Taking expectation over the randomness before round t towards both sides of the inequality, it yields:
E[f(wt+ι)] ≤ E[f(wt)] - nNκE[∣∣Vf(wt) ||2] + Pt.	(47)
where K = N 一 150N3η3L3 - 15N2η2L2 一 5NnL and Pt = (25N3n4L3 + BNN*ll )(σ2 +
18NΦt) + &N飞K+N Pk dist(mk,t, mk)B2 + 9N2η2LΦt + N⅛Lσ2.
By rearanging, and summing from t = 0, . . . , T - 1, the following result immediately shows:
T Xl E[∣∣vf (Wt)IlN ≤ E[f(w0T -Ef(WT)] + ɪ X1 Pt
t=0	η κ	t=0
≤ f (WO) - f (Wk) + 1 XIC
≤ TηNκ + TJPt
(48)
Now we bound the local gradient over the personalized sparse model:
Le
T-1 K
TK XXE[∣∣VFk (Wk,t)∣∣2
t=0 k=1
T-1 K
Tk X XE[∣∣vFk (Wk,t) - Vf(Wt) + Vf(Wt)||2]
t=0 k=1
kVFk (Wk,t) - K X mk Θ VFkO (W k0,t)k2
k0
3	T-1 K
E
TK t=0 k=1
+ k K X(mk ® VFkO (Wk，,t) - mk0 Θ VFkO (Wk，,t))kN + kVf (Wt) kN
k，
(a) 3 T-1 K	1	3 T-1 K
≤ TK ΣS £((d - β)GN+βBN + K^dist(mk, mk，)BN)+ TK Σ EEkVf (Wt )kN
t=0 k=1
k0
t=0 k=1
3	3 T-1 K
≤3(d - β)GN + 3βBN + KN £ fdi认mi, mk，)BN + TK E EEIlVf(Wt)kN
k	k0
t=0 k=1
(49)
In inequality (a), we use the second claim in Lemma 8 to treat the first term, and the second term is
bounded using a similar technique as in the first claim in Lemma 8.
Let Y = 3(d 一 β)GN + 3βBN + K∙ Pk £卜，dist(m/, mk，)BN and plugging inequality (48) into
the above inequality, it further implies that:
ɪ x1 X EMVFk (Wk,t) ∣∣N ≤ 3(f (WO) -f (Wk)) +f £ Pt + Y	(50)
TK	TηNκ	T
t=0 k=1	t=0
This shows the claim.
27
Under review as a conference paper at ICLR 2022
D Discussion on various bounds in existing study
Under the non-convex setting, Theorem 1 bounds the squared gradient norm of the personalized
iterates over the local loss. Existing studies on PFL zero in various different kinds of convergence
bound, and under different assumptions. Below are some concrete examples:
• Li et al. (2021) study the upper bound of E
IWk,T
-Wfck2]
for any device k (see their Corollary
1), where Wk,T is the personalized weights at iteration T and Wkfc is the optimal weights under the
client k’s local loss. Under the assumption of strongly convex, smoothness, bounded gradient, and
gradient dissimilarity, as well as an additional assumption on the distance between personalized
models and the optimal global model, it is theoretically proved that the proposed solution, named
Ditto, achieves O(1/T) convergence rate. Their derived convergence rate is on the same scale
as the convergence rate of the global model produced by FedAvg. However, it is important to
note that an extra assumption on the distance between personalized models and the optimal global
model is required to achieve this rate.
• Hanzely et al. (2021) study the convergence of the proposed regularization problem (see
their Theorem 4.5). Specifically, they propose to bound E [PK=ι ∣∣Wk,τ - wfc(λ)∣∣2] where
Wk,τ is the personalized model, and wk(λ) is the optimal model for their defined regular-
ization problem under the regularized hyper-parameter λ. Under the strongly convex and
smoothness assumption, the authors derive the bound as E [Pk=ι 11w^k,τ - Wkfc(λ)k2	≤
(1 - αnμ)T PK=IIlWk,τ - wfc(λ) ∣∣2 + 2n『2. Further, by applying the variance reduction tech-
nique on their proposed L2GD solution, they eliminate the constant term (i.e., the second term)
in the above bound. However, their convergence analysis are made towards a special case of the
regularized problem. It remains unspecified about how and whether the personalized iteration
produced by L2GD can achieve convergence towards the optimum for each client’s local loss
(i.e., the optimum of the ultimate PFL problem (P2)). In other words, the gap between Wk,τ and
Wkfc = minwk Fk(Wk) remains unknown.
Under the non-convex setting, Deng et al. (2020) study the upper bound of squared gradient norm,
or formally, T PT=I E [∣NFk (Wk,t)『] (see their Theorem 6)
,where Wbk,t is the personalized
model for client k and VF1k (∙) is the gradient of a model over the local loss. Under the smoothness
and bounded variance conditions, they derive a bound with rate O(√1T) to convergence. Similar
to our results, a non-vanished residual presents in their bound. This non-vanished residual is
related to: i) the gradient dissimilarity between clients’ local loss function; and (ii) the gradient
discrepancy, The form of this non-vanished term is highly similar to our derived residual term (i.e.,
Υ in our Theorem 5), since both the gradient dissimilarity factor (i.e., G in Assumption 1) and
gradient bound (i.e., B in Assumption 4) would present in the non-vanished residual. Additionally,
when the personalized factor is completely eliminated i.e., αk → 0 in their formulation or β → d
in our formulation, the non-vanished residual (i.e., Γ in their formulation and Υ in ours) would
both become significant to dominate the bound.
28