Under review as a conference paper at ICLR 2022
An equivalence between data poisoning and
Byzantine gradient attacks
Anonymous authors
Paper under double-blind review
Ab stract
To study the resilience of distributed learning, the “Byzantine” literature consid-
ers a strong threat model where workers can report arbitrary gradients to the pa-
rameter server. While this model helped generate several fundamental results, it
has however sometimes been considered unrealistic, when the workers are mostly
trustworthy machines. In this paper, we show a surprising equivalence between
this model and data poisoning, a threat considered much more realistic. More
specifically, we prove that any gradient attack can be reduced to data poisoning in
a personalized federated learning system that provides PAC guarantees (which we
show are both desirable and realistic in various personalized federated learning
contexts such as linear regression and classification). Maybe most importantly,
we derive a simple and practical attack that may be constructed against classi-
cal personalized federated learning models, and we show both theoretically and
empirically the effectiveness of this attack.
1 Introduction
Learning algorithms now typically leverage data generated by a large number of users (Smith et al.,
2013; Wang et al., 2019a;b) to often learn a common model that fits a large population (Konecny
et al., 2015), but also sometimes to construct a personalized model for each individual (Ricci et al.,
2011). Autocompletion (Lehmann & Buschek, 2021), conversational (Shum et al., 2018) and rec-
ommendation (Ie et al., 2019) algorithms are examples of such algorithms deployed at scale. To be
effective, besides huge amounts of (distributed) data (Brown et al., 2020; Fedus et al., 2021), these
algorithms require a high level of customization. This has motivated research into personalized
federated learning (Fallah et al., 2020; Hanzely et al., 2020; Dinh et al., 2020).
However, in applications such as content recommendation, activists, companies, and politicians have
strong incentives to promote certain views, products or ideologies (Hoang, 2020; Hoang et al., 2021).
Remember for instance that, on YouTube, two views out of three result from algorithmic recommen-
dations (Solsman, 2018). Perhaps unsurprisingly, this has led to vast amounts of fabricated activities
to bias algorithms (Bradshaw & Howard, 2019; Neudert et al., 2019), like “fake reviews” (Wu et al.,
2020). The scale of this phenomenon is well illustrated by the case of Facebook which, in 2019
alone, reported the removal of around 6 billion fake accounts from its platform (Fung & Garcia,
2019). This is particularly concerning in the era of “stochastic parrots” (Bender et al., 2021): cli-
mate denialists are incentivized to pollute textual datasets with claims like “climate change is a
hoax”, as autocompletion, conversational and recommendation algorithms trained on such data will
more likely spread these views (McGuffie & Newhouse, 2020). This raises serious concerns about
the vulnerability of personalized federated learning to such misleading data. Data poisoning attacks
clearly constitute now a major machine learning security issue (Kumar et al., 2020).
Overall, in highly adversarial environments like social media, given the advent of deep fakes (John-
son & Diakopoulos, 2021), we should expect that most data are strategically crafted and labeled. In
this context, the authentication of the data provider seems critical. In particular, the safety of learn-
ing algorithms arguably demands that they be trained solely on cryptographically signed data, that
is, data that provably come from a known source. But even signed data cannot be wholeheartedly
trusted since users usually have preferences over what ought to be recommended to others. They
thus have incentives to behave strategically in order to promote certain views or products.
1
Under review as a conference paper at ICLR 2022
To address data poisoning, the Byzantine learning literature usually considers that each federated
learning worker may behave arbitrarily (Blanchard et al., 2017; Mhamdi et al., 2018; El-Mhamdi
et al., 2021). Recall that at each iteration of the federated learning stochastic gradient descent, each
worker is given the updated model, and is asked to compute the gradient of the loss function with
respect to (a batch of) its local data. Byzantine learning usually assumes that a malicious (Byzantine)
worker may report any gradient; without having to justify whether such a gradient could have been
generated through data poisoning. In fact, the gradient attack threat model has sometimes been
claimed to be unrealistic in practical federated learning (Shejwalkar et al., 2021), especially when
the workers are machines owned by trusted entities (cross-silo FL (Kairouz et al., 2021)).
We prove in this paper an equivalence between gradients attacks and fabricated data injection, in a
general and desirable collaborative learning framework. Thereby, our paper provides the first prac-
tically compelling argument for the necessity to protect federated learning against gradient attacks.
Contributions. As a preamble of our main result, we formalize local PAC* learning1 (Valiant,
1984) for personalized learning, and prove that a simple and general solution to personalized fed-
erated linear regression and classification is indeed locally PAC* learning. Our proof leverages a
new concept called gradient-PAC* learning, which is of independent interest. We prove that it is
sufficient to guarantee local PAC* learning, and that it is verified by basic learning algorithms, like
linear and logistic regression. This is an important and highly nontrivial contribution of this paper.
Our main contribution is then to prove that local PAC* learning in personalized federated learning
essentially implies an equivalence between data poisoning and gradient attacks. More precisely, we
show how any (converging) gradient attack can be turned into a data poisoning attack, with the same
resulting harm. Given how easy it generally is to create fake accounts on web platforms and to inject
data poisoning by generating fake activities, this result should arguably greatly increase the concerns
over the vulnerabilities of federated learning with user-generated data.
Finally, we propose a simple but very general strategic gradient attack, called the counter-gradient
attack (CGA), which any participant to federated learning can deploy to bias the global model
towards any target model that better suits their interest. We prove the effectiveness of this attack
under fairly general assumptions, which apply to many proposed personalized learning frameworks
including Hanzely et al. (2020); Dinh et al. (2020). We then show empirically how this gradient
attack can be turned into a devastating data poisoning attack, with remarkably few data.
Related work. Byzantine learning has provided both negative and positive results in Byzantine
resilience (Blanchard et al., 2017; Mhamdi et al., 2018; Baruch et al., 2019; Xie et al., 2019), some
of which apply almost straightforwardly to personalized federated learning (El-Mhamdi et al., 2020;
2021). Such results study the resilience against a minority of adversarial users. Our paper how-
ever focuses on a different kind of malicious users. Namely, like Suya et al. (2021), we study the
resilience against strategic users, who aim to bias the learned models towards a specific target model.
The study of the resilience against strategic users is part of the research on strategyproof learning.
Many special cases of strategyproofness have been tackled, including regression (Chen et al., 2018b;
Dekel et al., 2010; Perote & Perote-Pena, 2004; Ben-Porat & Tennenholtz, 2017), classification
(Meir et al., 2012; Chen et al., 2020; Meir et al., 2011; Hardt et al., 2016), statistical estimation
(Cai et al., 2015), and clustering (Perote & Sevilla, 2003). However, none of these papers tackles a
general personalized federated learning scheme. Typically, for linear regression, Chen et al. (2018b)
and Perote & Perote-Pena (2004) assume that each user can only provide a single data point. This
greatly restricts the users’ ability to contribute to the learning model. And while Dekel et al. (2010)
allows multiple contributions, they either require payments, which might not be possible (e.g., due to
ethical reasons), or they restrict the model to one dimension or a constant function inRd. Conversely,
Suya et al. (2021) show how to arbitrarily manipulate convex learning models through multiple data
injections, in the case where a single model is learned from all data at once.
A large literature has focused on data poisoning, with either a focus on backdoor (Dai et al., 2019;
Zhao et al., 2020; Severi et al., 2021; Truong et al., 2020; Schwarzschild et al., 2021) or triggerless
attacks (Biggio et al., 2012; Mufioz-Gonzalez et al., 2017; Shafahi et al., 2018; ZhU et al., 2019;
1We omit complexity considerations for the sake of generality. We define PAC* to be PAC without such
considerations.
2
Under review as a conference paper at ICLR 2022
Huang et al., 2020; Barreno et al., 2006; Aghakhani et al., 2021; Geiping et al., 2021). However,
most of this research analyzed data poisoning without signed data. One noteworthy exception is
Mahloujifar et al. (2019), whose universal attack amplifies the probability of a (bad) property.
Collaborative PAC learning was previously introduced by Blum et al. (2017), and then extensively
studied (Chen et al., 2018a; Nguyen & Zakynthinou, 2018), sometimes with the presence of Byzan-
tine collaborating users (Qiao, 2018; Jain & Orlitsky, 2020; Konstantinov et al., 2020). We stress
however that this line of work assumes that all honest users have the same labeling function. In other
words, given any query, they agree on how the query should be answered. This is a very unrealistic
assumption in many critical applications, like content moderation or language processing. In fact, in
such applications, removing outliers can be argued to amount to ignoring minorities’ views, which
would be highly unethical. The very definition of PAC learning must then be adapted, which is what
we do in this paper (also, we adapt it to parameterized models).
Structure of the paper. The rest of the paper is organized as follows. Section 2 presents a general
model of personalized learning, formalizes local PAC* learning and defines a general federated
gradient descent algorithm. Section 3 proves the equivalence between data poisoning and gradient
attacks, under local PAC* learning. Section 4 proves the local PAC* learning properties for federated
linear regression and classification. Section 5 then describes a simple and general data poisoning
attack, whose effectiveness against `22 is proved theoretically and empirically. Section 6 concludes.
2 A general personalized federated learning framework
We consider a set [N] = {1, . . . , N} of users. Each user n ∈ [N] has a local signed dataset Dn, and
aims to learn a local model θn ∈ Rd. Users may collaborate to improve their models. Personalized
learning must then input a tuple of users’ local datasets D~ , (D1, . . . , DN), and output a tuple of
local models ~ ，(θɪ,..., θN). Like many others, We assume that the users perform federated
learning to do so, by leveraging the computation of a common global model ρ ∈ Rd . Intuitively, the
global model is an aggregate of all users’ local models, Which users can leverage to improve their
local models. The common global model Will typically alloW users With too feW data to obtain an
effective local model, While it may be mostly discarded by users Whose local datasets are large.
More formally, We consider a very general personalized learning frameWork Which generalizes the
models proposed by Dinh et al. (2020) and Hanzely et al. (2020). Namely, We consider that the
personalized learning algorithm outputs a global minimum (P ,θ*) of a global loss given by
LOSS(ρ, θ~, D~ ) , X Ln(θn, Dn) + X R(ρ, θn),	(1)
n∈[N]	n∈[N]
Where R is a regularization, typically With a minimum at θn = ρ. For instance, Hanzely et al. (2020)
and Dinh et al. (2020) define R(ρ, θn) , λ kρ - θnk22, Which We shall call the `22 regularization.
But other regularizations may be considered, like the `2 regularization R(ρ, θn) , λ kρ - θnk2, or
the smooth-`2 regularization R(ρ, θn) , λ 1 + kρ - θnk22. Note that, for all such regularizations,
the limit λ → ∞ essentially yields the classical non-personalized federated learning frameWork.
2.1	Local PAC* learning
In this paper, We focus on personalized learning algorithms that provably recover a user n’s preferred
model θ∖, if the user provides a large enough honest dataset Dn, i.e. constructed with θ*. Such
honest datasets Dn could typically be obtained by repeatedly draWing random queries (or features),
and by using the user,s preferred model θnn to provide (potentially noisy) answers (or labels). We
refer to Section 4 for examples. The model recovery condition is then formalized as follows.
Definition 1. A personalized learning algorithm is locally PAC* learning if, for any subset H ⊂ [N]
of honest users, any preferred models ~*, any ε,δ > 0, and any datasets D-H from users n / H,
there exists I such that, if all users h ∈ H provide honest datasets Dh with at least |Dh | ≥ I data
points, then, with probability at least 1 一 δ, we have 11 θ力(D) — θh∣∣ ≤ ε for all users h / H.
3
Under review as a conference paper at ICLR 2022
Local PAC* learning is arguably a very desirable property. Indeed, it guarantees that any honest
active user will not be discouraged to participate in federated learning as they will eventually learn
their preferred model by providing more and more data. In Section 4, we will show how local PAC*
learning can be achieved in practice, by considering specific local loss functions Ln .
2.2	Federated gradient descent
While the computation of ρ* and ~ could be done by a single machine, which first collects the
datasets D~ and then minimizes the global loss LOSS, modern machine learning deployments often
rather rely on federated (stochastic) gradient descent (or variants), with a central trusted parameter
server. In this setting, each user n keeps their data Dn locally. At each iteration t, the parameter
server sends the latest global model ρt to the users. Each user n is then expected to update its local
model given the global model ρt, either by solving θnt , arg minθn Ln(θn, Dn) +R(ρt, θn) (which
is what we assume in the theory part, in the manner of Dinh et al. (2020)), or by making a (stochastic)
gradient step from the previous local model θnt-1 (which is what is done in our experiments, in
the manner of Hanzely & Richtarik (2021)). User n is then expected to report the gradient gin =
VρR(ρt, θtn) to the parameter server. The parameter server then updates the global model, using a
gradient step, i.e. it computes ρt+1 , ρt - ηt Pn∈[N] gnt . For simplicity, here, and since our goal
is to show the vulnerability of personalized federated learning even in good conditions, we assume
that the network is synchronous and that no node can crash. Note also that our setting could be
generalized to fully decentralized collaborative learning, as was done by El-Mhamdi et al. (2021).
We assume that the users are only allowed to send plausible gradient gradients. More precisely, We
denote GRAD(P)，{VρR(ρ, θ) | θ ∈ Rd} the closure set of plausible (sub)gradients at ρ. If user n's
gradient gnt is not in the set GRAD(ρt ), the parameter server can easily detect the malicious behavior
and gnt will be ignored by the parameter server at iteration t. In the case of an `22 regularization,
where R(ρ, θ) = λ kρ - θk22, we clearly have GRAD(ρ) = Rd for all ρ ∈ Rd. It can be easily
shown that, for `2 and smooth-`2 regularizations, GRAD(ρ) is the closed ball B(0, λ).
Nevertheless, in this setting, a strategic user s ∈ [N] can deviate from its expected behavior, to bias
the global model in their favor. We identify in particular three sorts of attacks.
Data poisoning: Instead of collecting an honest dataset, s fabricates any strategically crafted
dataset Ds, and then performs all other operations as expected.
Model attack: At each iteration t, S fixes θts，θ,, where θ? is any strategically crafted model.
All other operations would then be executed as expected.
Gradient attack: At each iteration t, s sends any (plausible) strategically crafted gradient gst .
Gradient attacks are intuitively the most harmful attacks, as the strategic user can then adapt their
attack based on what they observe during the learning process. However, because of this, gradient
attacks are more likely to be flagged as suspicious behaviors. At the other end of the spectrum,
data poisoning is a much safer attack, as the strategic user can always report their entire dataset,
and prove that they rigorously performed the expected computations. In fact, data poisoning can be
executed, even if users directly provide the data to a (trusted) central authority, which is then tasked
to execute (federated) gradient descent. This is typically what is done to construct recommendation
algorithms on social medias, where users’ data are their online activities (what they view, like and
share). Crucially, especially in applications with no clear ground truth, such as content moderation
or language processing, the strategic user can always argue that their dataset is an “honest” dataset;
not a strategically crafted one. Ignoring the strategic user’s data on the basis that it is an “outlier”
may then be regarded as unethical, as it can be argued to amount to rejecting minorities’ viewpoints.
3 The equivalence between data poisoning and gradient attacks
We now present our main result. The threat model we considered is closely related to “model-
targeted attacks”, studied in Suya et al. (2021). Recall also that, in this theory part, at each iteration,
local models θnt are fully optimized, given ρt, in the manner of Dinh et al. (2020).
Theorem 1 (Equivalence between gradient attacks and data poisoning). Assume local PAC* learn-
ing, and `22 or smooth-`2 regularization. Assume also that Ln is convex and L-smooth for all users
4
Under review as a conference paper at ICLR 2022
n ∈ [N], and that the learning rate ηt is constant and small enough. Consider any datasets D~-s
provided by users n = S. Then, for any target model θg ∈ Rd, there exists a converging gradient
attack of strategic user S (i.e. gt converges) such that Pt → θS, if and only if, for any ε > 0, there
exists a dataset DS such that ∣∣ρ*(D~) 一 θɪ∣∣ ≤ ε.
Note that smoothness is used as a sufficient condition to prove the convergence of federated gradient
descent. We now present our proof, which goes through the intermediary step of model attacks.
3.1	Equivalence between data poisoning and model attacks
In this section, we prove the equivalence between data poisoning and model attacks.
Lemma 1 (Reduction from model attack to data poisoning). Consider any data D~ and user S ∈ [N].
Assume the global loss with datasets D has a global minimum (ρ*,~*). Then (ρ*, ~-S) is also a
global minimum ofthe modified loss with datasets D-S and strategic reporting θ?，θ* (D).
Proof. The proof is almost straightforward. It is given in Appendix B.1.	□
Now, intuitively, by virtue of local PAC* learning, strategic user S can essentially guarantee that the
personalized learning framework will be learning θ* ≈ θ，. But, a priori, it may seem unclear if this
will imply a biasing of the global model essentially identical to the one obtained through the model
attack that imposes θ* = θ?. In the sequel, We show that the answer is yes.
Lemma 2 (Reduction from data poisoning to model attack). Assume `22, `2 or smooth-`2 regular-
ization, and assume local PAC* learning. Consider any datasets D-S and any attack model θ? such
that the modified loss LOSSS has a unique minimum ρ*(θ?, D-s), ~-s(θ2, D-S). Then, for any
ε > 0, there exists a dataset DS such that we have
∣∣ρ*(D)-ρ*(θ,, D-S) ∣∣2 ≤ ε and ∀n= s, |"(D『8”, D-S)( ≤ ε. (2)
Our proof in fact holds for all continuous regularizations R with R(ρ, θ) → ∞ as kρ 一 θk2 → ∞.
Moreover, note that the approximation guarantee holds for local models too.
Sketch ofproof. Given local PAC* learning, for a large dataset DS constructed from θ?, strategic
user s can guarantee θ* (D) ≈ θ?. By carefully bounding the effect of the approximation on the loss
using the Heine-Cantor theorem, We show that this implies ρ*(D) ≈ ρ*(θ?, D-S) and θ* (D) ≈
θn (θ*, D-S) for all n = s too. The precise analysis, given in Appendix B.2, is nontrivial. □
3.2	Equivalence between model attacks and gradient attacks
For the last few years, gradient attacks have been widely studied by the Byzantine learning literature.
Recently, Shejwalkar et al. (2021) argued that they are not a realistic threat model. Below, we prove
that, in our setting, they are actually as concerning as model attacks (and, thus, as data poisoning).
Lemma 3 (Reduction from model attack to gradient attack). Assume that Ln is convex and L-
smooth for all nodes n ∈ [N], and that we use `22 or smooth-`2 regularization. If gSt converges and
if ηt = η is a constant small enough, then ρt will converge too. Denote ρ∞ its limit. Then for any
ε > 0, there is θ? ∈ Rd such that ∣∣ρ∞ — ρ"θ?, D-s)∣∣ ≤ ε.
Sketch of proof. Denote gS∞ the limit of gSt. Gradient descent then behaves as though it was minimiz-
ing the loss plus ρT gS∞ (and ignoring R(ρ, θS )). Essentially, classical gradient descent theory then
guarantees the convergence of ρt to ρ∞, though the precise proof is nontrivial. Then, since GRAD
is closed and g∞ ∈ Grad, we can construct θ? which approximately yields the gradient g∞. The
full proof (which also yields the necessary upper bound on η) is given in Appendix B.3.	□
Since any model attack can clearly be achieved by the corresponding honest gradient attack, we
conclude that model attacks and gradient attacks are essentially equivalent. In light of our previous
results, this implies that gradient attacks are essentially equivalent to data poisoning (Theorem 1).
5
Under review as a conference paper at ICLR 2022
4	Examples of locally PAC* learning systems
To the best of our knowledge, though similar to collaborative PAC learning (Blum et al., 2017),
local PAC* learnability is a new concept in the context of personalized federated learning. It is thus
important to show that it is not meaningless. To achieve this, in this section, we provide sufficient
conditions for a personalized learning model to be locally PAC* learnable. First, we construct local
losses Ln as sums of losses per input, i.e.
Ln(θn,Dn) = ν kθnk22 + X `(θn, x),	(3)
for some “loss per input” function ` and some weight ν > 0. Appendix C gives theoretical and
empirical arguments are provided for using such a sum (as opposed to an expectation). Remarkably,
for linear or logistic regression, given such a loss, local PAC* learning can then be guaranteed.
Theorem 2 (Personalized least square linear regression is locally PAC* learning). Consider `22, `2
or smooth-`2 regularization. Assume that, to generate a data xi, a user with preferred parameter
θ* ∈ Rd first independently draws a random vector query Qi ∈ Rd from a Sub-Gaussian query
distribution Q, with parameter σ q and positive definite matrix Σ = E Q^QTi.. Assume that the
user labels Qi with a real-valued answer Ai = QT θ* + ξi, where ξi is a zero-mean sub-Gaussian
random noise with parameter σξ, independent from Qi and other data points. Finally, assume that
'(θ, Qi, Ai) = 1 (θT Qi -Ai)2. Then the personalized learning algorithm is locally PAC* learning.
Theorem 3 (Personalized logistic regression is locally PAC*-learning). Consider `22, `2 or smooth-
'2 regularization. Assume that, to generate a data Xi, a user with preferred parameter θ* ∈ Rd
first independently draws a random vector query Qi ∈ Rd from a query distribution Q, whose
support SUPP(Q) is bounded and spans the full vector space Rd. Assume that the user then labels
Qi with answer Ai = 1 with probability σ(QTθ*), and labels it Ai = —1 otherwise, where σ(z)=
(1 + e-z)-1 is the sigmoid logistic function. Finally, assume that `(θ, Qi, Ai) = - ln(σ(AiθT Qi)).
Then the personalized learning algorithm is locally PAC* learning.
4.1	Proof sketch
In this section, we outline the proofs of theorems 2 and 3, as they are interesting in their own sake.
The proofs both leverage the following notion, which intuitively means “robust PAC* learning”.
Definition 2 (Gradient-PAC*). Denote E(D,θ*, I, A, B, α) the event
∀θ ∈ Rd, (θ — θt)T VL (θ,D) ≥ AImin {∣∣θ — θt∣∣2, ∣∣θ — θ*2} — BIα ∣∣θ — θt∣∣2.	(4)
The loss ` is gradient-PAC* if, for any K > 0, there exist constants AK, BK > 0 and αK < 1, such
that for any preferred model θ^ ∈ Rd with ∣∣θ* ∣∣2 ≤ K, assuming that the dataset D is obtained by
honestly collecting and labeling I data points according to the preferred model θ*, the probability
of the event E (D,θ*, I, Aκ,Bκ,aQ goes to 1 as I → ∞.
Intuitively, this definition asserts that, as we collect more data from an honest user, then, with high
probability, the gradient of the loss at any point θ too far from θ* will point away from θl In
particular, gradient descent is then essentially guaranteed to draw θ closer to θl The right-hand
side of equation 4 is subtly chosen to be strong enough to yield local PAC* learning guarantees, and
weak enough to be verified by linear and logistic regression, as proved by the following lemma.
Lemma 4. Logistic and linear regression, defined in theorems 2 and 3, are gradient PAC* learning.
Sketch of proof. In the case of linear regression, remarkably, the discrepancy between the em-
pirical and the expected loss functions depends only on a few key random variables, such as
min Sp(I P QiQT) and P ξiQi, which can be controlled by appropriate concentration bounds.
Meanwhile, for logistic regression, for	|b|	≤	K,	we observe that	(a	— b)(σ(a)	—	σ(b))	≥
cK min(|a — b| , |a — b|2). Essentially, this proves that gradient-PAC* would hold if the empiri-
cal loss was replaced by the expected loss (times I). The actual proofs, however, are nontrivial,
especially in the case of logistic regression, which leverages topological considerations to derive a
critical uniform concentration bound. The full proofs are given in appendices D.2 and D.3.	□
6
Under review as a conference paper at ICLR 2022
Now, under very mild assumptions on the regularization R (not even convexity!), which are verified
by the `2 *2, `2 and smooth-`2 regularizations, we prove that the gradient-PAC* learnability through `
suffices to guarantee that personalized learning will be locally PAC* learning.
Lemma 5. Assume R is (sub)differentiable and nonnegative, and R(ρ, θ) → ∞ as kρ - θk2 → ∞.
If ` is gradient-PAC* and nonnegative, then personalized learning is locally PAC*-learning.
Sketch of proof. Once other users’ datasets are fixed, the learning of an honest user’s model has a
fixed biased due to R. But as the user provides more data, by gradient-PAC*, the local loss becomes
predominant, which guarantees local PAC*-learning. Appendix E provides a full proof.	□
Combining the two lemmas clearly yields theorems 2 and 3 as special cases. Note that our result
actually applies to a more general set of regularizations and a more general set of local losses.
4.2	The case of neural networks
Neural networks generally do not verify gradient PAC* learning. After all, because of symmetries
like neuron swapping, different values of the parameters compute the same neural network function.
Thus the notion of a “preferred model" θ* is arguably ill-defined for neural networks2. Nevertheless,
we may consider a strategic user who only aims to bias the parameters of the last layer. In particular,
assuming that all layers but the last one ofa neural network are pretrained and fixed, then our theory
may apply to the parameters of the last layer, if it performs classification or regression.
5	A practical data poisoning attack
We now construct a practical data poisoning attack. We do so by introducing a new effective gradient
attack, and by then leveraging our equivalence to turn it into a data poisoning attack.
5.1	The counter-gradient attack
We now define a simple, general and practical gradient attack, which we call the counter-gradient
attack (CGA). Intuitively, this attack estimates the sum g—t of the gradients of other users based on
its value at the previous iteration, which can be inferred from the way the global model ρt-1 was
updated into ρt. More precisely, apart from initialization g-S，0, CGA makes the estimation
tt ρ ρt-1 - Pt	t-1 _ ↑,t-i	/c
g-s ,	gs	= g-s .	(5)
-s ηt-1 s	-s
Strategic user s then reports the plausible gradient that moves the global model closest to the user’s
target model θg, assuming others report g—§. In other words, at every iteration, CGA reports
gt ∈ arg min IIPt- ηt(g- + g) - θS∣∣2 .	(6)
g ∈GRAD(ρ)
Note that, to perform this attack, user s only needs to know the previous and current learning rates
ηt-ι and ηt, the previous and current global models ρt-1 and ρt, and their target model θS.
Computation of CGA. Define hS，gt-1 + ρ -θs - ρt-：-Pt. For convex sets GRAD(Pt), it
is straightforward to see that CGA boils down to computing the orthogonal projection of hts on
GRAD(Pt). This yields very simple computations for `22, `2 and smooth-`2 regularizations.
Proposition 1. For `22 regularization, CGA reports gst = hts. For `2 or smooth-`2 regularization,
CGA reports gst = hts min {1, λ/ khts k2}.
t _θ↑
Proof. Equation (6) boils down to minimizing the distance between P % S - g-§ and GRAD(P),
which is the (convex) ball B(0, λ). This minimum is the orthogonal projection on B(0, λ).	□
2Evidently, our definition could be easily modified to give importance to the computed function, rather than
to the parameters of the model.
7
Under review as a conference paper at ICLR 2022
Theoretical analysis. We now prove that CGA is perfectly successful against `22 regularization.
To do so, we suppose that, at each iteration t and for each user n 6= s, the local models θn are fully
optimized with respect to pt, and the honest gradients of g(,t are used for the gradient descent of ρ.
Theorem 4.	Consider '2 regularization. Assume that ' is convex and l`-smooth, and that η = η
is SmaU enough. Then CGA is converging and optimal, as Pt → θS.
Sketch ofproof. The main challenge is to guarantee that the other users, gradients g/ for n = S
remain sufficiently stable over time to guarantee convergence, which can be done by leveraging
L-smoothness. The full proof, with the necessary upper-bound on η, is given in Appendix F. □
The analysis of the convergence against smooth-`2 is unfortunately significantly more challenging.
Here, we simply make a remark about what CGA yields in this case, if it converges.
Proposition 2. If CGA against smooth-`2 regularization converges for ηt = η, then it either
achieves perfect manipulation, or it is eventually partially honest, in the sense that the gradient
by CGA correctly points towards θɪ.
Proof. Denote P the projection onto the closed ball B(0, λ). If CGA converges, then, by Proposi-
tion 1, P (g∞ + P -θs) = g∞. This implies in particular that p∞ - θ( and g∞ must be colinear.
If perfect manipulation is not achieved (i.e. p∞ = θg), then We must have g∞ = λ ：黄—. 口
Empirical evaluation of CGA. We deployed CGA to bias the
federated learning of MNIST. We consider a strategic user whose
target model is one that labels 0’s as 1’s, 1’s as 2’s, and so on,
until 9’s that are labeled as 0’s. In particular, this target model
has a nil accuracy. Figure 1 shows that such a user effectively
hacks the `22 regularization against 10 honest users who each
have 6,000 data points of MNIST, in the case where local mod-
els only undergo a single gradient step at each iteration, but fails
to hack the `2 regularization. Further details on the experiment
are given in Appendix G. We also ran a similar successful attack
on the last layer of a deep neural network trained on cifar-10,
which is detailed in Appendix J. The Appendix also discusses
the extent to which the attack may be turned into data poisoning.
Figure 1: Accuracy of the global
model under attack by CGA.
5.2	FROM GRADIENT ATTACK TO MODEL ATTACK AGAINST `22
We now show how to turn a gradient attack into model attack, against `22 regularization. Assume
that we found a gradient g∞ such that p∞ = θS. It is trivial to transform it into a model attack by
setting θ?，θS — 1 g∞, as guaranteed by the following result, and as depicted by Figure 2.
Model attack
35-Γ7
5 0 5
2 2 1
UlJoU Z-
——I2_dist
---I2norm
...target dist
Model attack
0.0
O 50 IOO 150	200	250
Epochs
(b) Accuracy of P according to θ∖ (which rela-
bels 0 → 1 → 2 → ... → 9 → 0).
O 50 IOO 150	200	250
Epochs
(a) Distance between the global model ρt and
the target model θɪ (target_dist).
Figure 2: Successful model attack against `22 by combining CGA and Proposition 3.
8
Under review as a conference paper at ICLR 2022
Proposition 3. Consider the	'2	regularization. Suppose that	gS	→	g∞	and Pt	→	θS,	with a
constant learning rate η = η. Then, under the model attack θ?，θS 一 ∙2λg∞, the gradient at
ρ = θS vanishes, i.e. NPLoss(θg,~-§网,DD-s),θ?, D-S) = 0.
Proof. Given that the learning rate is constant, the convergence Pt → θɪ implies that the sum of
honest users' gradients at P = θS must equal -g∞. Therefore, to achieve ρ* = θg with a model
attack, it suffices to send a model θ? such that the gradient of λ ∣∣ρ 一 θ^∣∣2 with respect to P at
ρ = θS equals g∞. Since the gradient is λ(θ∕ — θ?), θ?，θg — ∙2λg∞ does the trick.	□
5.3	FROM MODEL ATTACK TO DATA POISONING AGAINST `22
The case of linear regression. In linear regression, any model attack can be turned into a single
data poisoning attack, as proved by the following theorem whose proof is given in Appendix H.
Theorem 5.	Consider the `22 regularization and linear regression. For any data D-s and any target
value θS, there is a datapoint (Q, A) to be injected by user S such that ρ*({(Q, A)} , D-S) = θɪ.
The case of linear classification. We now consider linear classification, with the case of MNIST.
By Theorem 2, any model attack can be turned into data poisoning, provided sufficiently many
(random) data points are labeled by the strategic user. However, this may require creating too many
data labelings, especially if the norm of θ? is large (which is the case if S is alone against many
active users), as suggested by Theorem 3.
For efficient data poisoning, define the indifference affine subspace V ⊂ Rd as the set of images
with equiprobable labels. Intuitively, labeling images close to V is very informative, as it informs us
directly about the separating hyperplanes. To generate images, we first draw random images, which
we then project orthogonally on V . We then add a small noise, before probabilistically labeling the
image with model θ?. Note that this leads us to consider images not in [0,1]d. Figure 3 shows the
effectiveness of the resulting data poisoning attack, with only 2,000 data points, as opposed to the
other nodes’ 6,000 data points. More details and explanations are provided in Appendix I.
the target model θɪ (target_dist).
Figure 3: Successful data attack against `22 by the efficient data generation scheme.
(b) Accuracy of Pt according to θS3 (which rela-
bels 0 → 1 → 2 → ... → 9 → 0).
6 Concluding Remarks
We showed in this paper that, unlike what has been argued by, e.g., Shejwalkar et al. (2021), the gra-
dient attack threat is not unrealistic. For personalized federated learning with local PAC* guarantees,
gradient attacks are in fact just as realistic and harmful as strategic data reporting. More generally,
our work stresses how critical Byzantine learning research is. For instance, El-Mhamdi et al. (2021)
proved lower bounds on what any collaborative learning algorithm can guarantee in heterogeneous
environments, under Byzantine gradient attacks. Our work implies that, at least for certain person-
alized federated learning problems, these lower bounds also hold for data poisoning attacks, which
are known to be common for many high-risk applications. Arguably, a lot more security measures
are urgently needed to make large-scale learning algorithms safe.
9
Under review as a conference paper at ICLR 2022
Ethics statement
The safety of algorithms is arguably a prerequisite to their ethics. After all, an arbitrarily manip-
ulable large-scale algorithm will unavoidably endanger the targets of the entities that successfully
design such algorithms. Typically, unsafe large-scale recommendation algorithms may be hacked by
health disinformation campaigns that aim to promote non-certified products, e.g., by falsely pretend-
ing that they cure COVID-19. Such algorithms must not be regarded as ethical, even if they were
designed with the best intentions. We believe that our work helps understand the vulnerabilities of
such algorithms, and will motivate further research in the ethics and security of machine learning.
Reproducibility statement
All our experiments are run on the classical datasets MNIST and FashionMNIST. We provide all of
the source codes to reproduce the experiments:
•	The sum versus expectation experiments can be run by executing this file:
https://www.dropbox.com/sh/qdgmz9air24nhyr/AAAtycEkxc_
1hGbvU5YG18z4a?dl=0
•	The counter-gradient attack experiments can be run by executing this file:
https://www.dropbox.com/sh/bycqkccgmk4muzn/
AACRD1yeTglLSHEd1OOAzmVqa?dl=0
•	The data poisoning attack experiments can be run by executing this file:
https://www.dropbox.com/sh/qodnl6ivzti8hch/
AADgX4EYuSOotiMCAHyTIiGMa?dl=0
•	The cifar10 on VGG 13-BN experiments can be run by executing this file:
https://www.dropbox.com/sh/5mw5c9dt25eiav7/AAC0jP3e2kuh_
zvLudWbmDl-a?dl=0
The experiments are seeded and the CuDNN backend is configured in deterministic mode in order to
reduce the sources of non-determinism. We also turn of the benchmark mode. Executing the codes
will generate the figures and statistics of our main paper, and most of the figures of our Appendix.
Our other figures can be obtained by adjusting the hyperparameters of our codes.
The full description of the architecture and optimisation algorithm used is described in Appendix C.
The experimental setup details of each experiment are provided in the Appendix, along with addi-
tional results. The Appendix also contains the full proofs of our theorems.
References
Hojjat Aghakhani, Dongyu Meng, Yu-Xiang Wang, Christopher Kruegel, and Giovanni Vigna.
Bullseye polytope: A scalable clean-label poisoning attack with improved transferability, 2021.
Marco Barreno, Blaine Nelson, Russell Sears, Anthony D. Joseph, and J. D. Tygar. Can machine
learning be secure? In Proceedings of the 2006 ACM Symposium on Information, Computer and
Communications Security, ASIACCS ,06, pp.16-25, New York, NY, USA, 2006. Association
for Computing Machinery. ISBN 1595932720. doi: 10.1145/1128817.1128824. URL https:
//doi.org/10.1145/1128817.1128824.
Gilad Baruch, Moran Baruch, and Yoav Goldberg. A little is enough: Circumventing defenses for
distributed learning. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alche-Buc, E. Fox, and
R. Garnett (eds.), Advances in Neural Information Processing Systems, volume 32. Curran As-
sociates, Inc., 2019. URL https://proceedings.neurips.cc/paper/2019/file/
ec1c59141046cd1866bbbcdfb6ae31d4- Paper.pdf.
Omer Ben-Porat and Moshe Tennenholtz. Best response regression. In I. Guyon,
U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett
(eds.), Advances in Neural Information Processing Systems, volume 30. Curran Asso-
ciates, Inc., 2017. URL https://proceedings.neurips.cc/paper/2017/file/
1ce927f875864094e3906a4a0b5ece68- Paper.pdf.
10
Under review as a conference paper at ICLR 2022
Emily M. Bender, Timnit Gebru, Angelina McMillan-Major, and Shmargaret Shmitchell. On the
dangers of stochastic parrots: Can language models be too big? In Madeleine Clare Elish,
William Isaac, and Richard S. Zemel (eds.), FAccT ’21: 2021 ACM Conference on Fairness,
Accountability, and Transparency, Virtual Event / Toronto, Canada, March 3-10, 2021, pp. 610-
623. ACM, 2021. doi: 10.1145/3442188.3445922. URL https://doi.org/10.1145/
3442188.3445922.
Battista Biggio, Blaine Nelson, and Pavel Laskov. Poisoning attacks against support vector ma-
chines. In Proceedings of the 29th International Conference on Machine Learning, ICML
2012, Edinburgh, Scotland, UK, June 26 - July 1, 2012. icml.cc / Omnipress, 2012. URL
http://icml.cc/2012/papers/880.pdf.
Peva Blanchard, El Mahdi El Mhamdi, Rachid Guerraoui, and Julien Stainer. Machine
learning with adversaries: Byzantine tolerant gradient descent. In Isabelle Guyon, Ul-
rike von Luxburg, Samy Bengio, Hanna M. Wallach, Rob Fergus, S. V. N. Vishwanathan,
and Roman Garnett (eds.), Advances in Neural Information Processing Systems 30: An-
nual Conference on Neural Information Processing Systems 2017, 4-9 December 2017,
Long Beach, CA, USA, pp. 119-129, 2017. URL http://papers.nips.cc/paper/
6617-machine-learning-with-adversaries-byzantine-tolerant-gradient-descent.
Avrim Blum, Nika Haghtalab, Ariel D. Procaccia, and Mingda Qiao. Collaborative PAC learning. In
Isabelle Guyon, Ulrike von Luxburg, Samy Bengio, Hanna M. Wallach, Rob Fergus, S. V. N. Vish-
wanathan, and Roman Garnett (eds.), Advances in Neural Information Processing Systems 30:
Annual Conference on Neural Information Processing Systems 2017, December 4-9, 2017, Long
Beach, CA, USA, pp. 2392-2401, 2017. URL https://proceedings.neurips.cc/
paper/2017/hash/186a157b2992e7daed3677ce8e9fe40f-Abstract.html.
Samantha Bradshaw and Philip N Howard. The global disinformation order: 2019 global inventory
of organised social media manipulation. Project on Computational Propaganda, 2019.
Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhari-
wal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agar-
wal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh,
Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler,
Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCan-
dlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language models are few-shot
learners. In Hugo Larochelle, Marc’Aurelio Ranzato, Raia Hadsell, Maria-Florina Balcan,
and Hsuan-Tien Lin (eds.), Advances in Neural Information Processing Systems 33: Annual
Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12,
2020, virtual, 2020. URL https://proceedings.neurips.cc/paper/2020/hash/
1457c0d6bfcb4967418bfb8ac142f64a- Abstract.html.
Yang Cai, Constantinos Daskalakis, and Christos H. Papadimitriou. Optimum statistical estimation
With strategic data sources. In Peter Grunwald, Elad Hazan, and Satyen Kale (eds.), Proceedings
of The 28th Conference on Learning Theory, COLT 2015, Paris, France, July 3-6, 2015, vol-
ume 40 of JMLR Workshop and Conference Proceedings, pp. 280-296. JMLR.org, 2015. URL
http://proceedings.mlr.press/v40/Cai15.html.
Jiecao Chen, Qin Zhang, and Yuan Zhou. Tight bounds for collaborative PAC learn-
ing via multiplicative weights. In Samy Bengio, Hanna M. Wallach, Hugo Larochelle,
Kristen Grauman, NicoIo Cesa-Bianchi, and Roman Garnett (eds.), Advances in Neu-
ral Information Processing Systems 31: Annual Conference on Neural Information Pro-
Cessing Systems 2018, NeurIPS 2018, December 3-8, 2018, Montreal, Canada, pp.
3602-3611, 2018a. URL https://proceedings.neurips.cc/paper/2018/hash/
ed519dacc89b2bead3f453b0b05a4a8b- Abstract.html.
Yiling Chen, Chara Podimata, Ariel D. Procaccia, and Nisarg Shah. Strategyproof linear regres-
sion in high dimensions. In Proceedings of the 2018 ACM Conference on Economics and Com-
putation, EC ’18, pp. 9-26, New York, NY, USA, 2018b. Association for Computing Machin-
ery. ISBN 9781450358293. doi: 10.1145/3219166.3219175. URL https://doi.org/10.
1145/3219166.3219175.
11
Under review as a conference paper at ICLR 2022
Yiling Chen, Yang Liu, and Chara Podimata.	Learning strategy-aware linear classifiers.
In H. Larochelle, M. Ranzato, R. Hadsell, M. F. Balcan, and H. Lin (eds.), Advances
in Neural Information Processing Systems, volume 33, pp. 15265-15276. Curran Asso-
ciates, Inc., 2020. URL https://proceedings.neurips.cc/paper/2020/file/
ae87a54e183c075c494c4d397d126a66- Paper.pdf.
Jiazhu Dai, Chuanshuai Chen, and Yufeng Li. A backdoor attack against lstm-based text classifi-
cation systems. IEEE Access, 7:138872-138878, 2019. doi: 10.1109/ACCESS.2019.2941376.
URL https://doi.org/10.1109/ACCESS.2019.2941376.
Ofer Dekel, Felix Fischer, and Ariel D. Procaccia. Incentive compatible regression learning. Jour-
nal of Computer and System Sciences, 76(8):759-777, 2010. ISSN 0022-0000. doi: https://doi.
org/10.1016/j.jcss.2010.03.003. URL https://www.sciencedirect.com/science/
article/pii/S0022000010000309.
Canh T. Dinh, Nguyen H. Tran, and Tuan Dung Nguyen. Personalized federated learning with
moreau envelopes. In Hugo Larochelle, Marc’Aurelio Ranzato, Raia Hadsell, Maria-Florina Bal-
can, and Hsuan-Tien Lin (eds.), Advances in Neural Information Processing Systems 33: Annual
Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12,
2020, virtual, 2020. URL https://proceedings.neurips.cc/paper/2020/hash/
f4f1f13c8289ac1b1ee0ff176b56fc60- Abstract.html.
El-Mahdi El-Mhamdi, Rachid Guerraoui, Arsany Guirguis, Le NgUyen Hoang, and Sebastien
Rouault. Genuinely distributed byzantine machine learning. In Yuval Emek and Christian
Cachin (eds.), PODC ’20: ACM Symposium on Principles of Distributed Computing, Virtual
Event, Italy, August 3-7, 2020, pp. 355-364. ACM, 2020. doi: 10.1145/3382734.3405695. URL
https://doi.org/10.1145/3382734.3405695.
El-Mahdi El-Mhamdi, Sadegh Farhadkhani, Rachid Guerraoui, Arsany Guirguis, Le NgUyen Hoang,
and SebaStien Rouault. Collaborative learning in the jungle (decentralized, byzantine, heteroge-
neous, asynchronous and nonconvex learning). In Advances in Neural Information Processing
Systems 34: Annual Conference on Neural Information Processing Systems 2021, December 6-
14, 2021, 2021.
El-Mahdi El-Mhamdi, Rachid Guerraoui, and SebaStien Rouault. Distributed momentum for
byzantine-resilient stochastic gradient descent. In 9th International Conference on Learning
Representations, ICLR 2021, Vienna, Austria, May 4-8, 2021. OpenReview.net, 2021. URL
https://openreview.net/forum?id=H8UHdhWG6A3.
Alireza Fallah, Aryan Mokhtari, and Asuman E. Ozdaglar. Personalized federated learn-
ing with theoretical guarantees: A model-agnostic meta-learning approach. In Hugo
Larochelle, Marc’Aurelio Ranzato, Raia Hadsell, Maria-Florina Balcan, and Hsuan-Tien
Lin (eds.), Advances in Neural Information Processing Systems 33: Annual Conference
on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020,
virtual, 2020. URL https://proceedings.neurips.cc/paper/2020/hash/
24389bfe4fe2eba8bf9aa9203a44cdad- Abstract.html.
William Fedus, Barret Zoph, and Noam Shazeer. Switch transformers: Scaling to trillion param-
eter models with simple and efficient sparsity. CoRR, abs/2101.03961, 2021. URL https:
//arxiv.org/abs/2101.03961.
Brian Fung and Ahiza Garcia. Facebook has shut down 5.4 billion fake accounts this year. CNN
Business, 2019.
Jonas Geiping, Liam H Fowl, W. Ronny Huang, Wojciech Czaja, Gavin Taylor, Michael Moeller,
and Tom Goldstein. Witches’ brew: Industrial scale data poisoning via gradient matching. In
International Conference on Learning Representations, 2021. URL https://openreview.
net/forum?id=01olnfLIbD.
Filip Hanzely and Peter Richtarik. Federated learning of a mixture of global and local models, 2021.
12
Under review as a conference paper at ICLR 2022
FiliP Hanzely, Slavomlr Hanzely, Samuel Horvath, and Peter Richtarik. Lower bounds
and optimal algorithms for personalized federated learning. In Hugo Larochelle,
Marc’Aurelio Ranzato, Raia Hadsell, Maria-Florina Balcan, and Hsuan-Tien Lin (eds.),
Advances in Neural Information Processing Systems 33: Annual Conference on Neu-
ral Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, vir-
tual, 2020. URL https://proceedings.neurips.cc/paper/2020/hash/
187acf7982f3c169b3075132380986e4-Abstract.html.
Moritz Hardt, Nimrod Megiddo, Christos PaPadimitriou, and Mary Wootters. Strategic classi-
fication. In Proceedings of the 2016 ACM Conference on Innovations in Theoretical Com-
Puter Science, ITCS ,16, pp. 111-122, New York, NY, USA, 2016. Association for Com-
Puting Machinery. ISBN 9781450340571. doi: 10.1145/2840728.2840730. URL https:
//doi.org/10.1145/2840728.2840730.
Le NgUyen Hoang. Science communication desperately needs more aligned recommendation algo-
rithms. Frontiers in Communication, 5:115, 2020.
Le NgUyen Hoang, Louis Faucon, and El-Mahdi El-Mhamdi. Recommendation algorithms, a ne-
glected opportunity for public health. Revue Medecine et Philosophie, 4(2):16-24, 2021.
Roger A. Horn and Charles R. Johnson. Matrix Analysis. Cambridge University Press, 2 edition,
2012. doi: 10.1017/9781139020411.
W. Ronny Huang, Jonas Geiping, Liam Fowl, Gavin Taylor, and Tom Goldstein. Metapoi-
son:	Practical general-purpose clean-label data poisoning. In Hugo Larochelle,
Marc’Aurelio Ranzato, Raia Hadsell, Maria-Florina Balcan, and Hsuan-Tien Lin (eds.),
Advances in Neural Information Processing Systems 33: Annual Conference on Neu-
ral Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, vir-
tual, 2020. URL https://proceedings.neurips.cc/paper/2020/hash/
8ce6fc704072e351679ac97d4a985574-Abstract.html.
Eugene Ie, Vihan Jain, Jing Wang, Sanmit Narvekar, Ritesh Agarwal, Rui Wu, Heng-Tze Cheng,
Tushar Chandra, and Craig Boutilier. Slateq: A tractable decomposition for reinforcement
learning with recommendation sets. In Sarit Kraus (ed.), Proceedings of the Twenty-Eighth
International Joint Conference on Artificial Intelligence, IJCAI 2019, Macao, China, August
10-16, 2019, pp. 2592-2599. ijcai.org, 2019. doi: 10.24963/ijcai.2019/360. URL https:
//doi.org/10.24963/ijcai.2019/360.
Ayush Jain and Alon Orlitsky. A general method for robust learning from batches. In
Hugo Larochelle, Marc’Aurelio Ranzato, Raia Hadsell, Maria-Florina Balcan, and Hsuan-
Tien Lin (eds.), Advances in Neural Information Processing Systems 33: Annual Con-
ference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12,
2020, virtual, 2020. URL https://proceedings.neurips.cc/paper/2020/hash/
f7a82ce7e16d9687e7cd9a9feb85d187- Abstract.html.
Deborah G. Johnson and Nicholas Diakopoulos. What to do about deepfakes. Commun. ACM, 64
(3):33-35, 2021. doi: 10.1145/3447255. URL https://doi.org/10.1145/3447255.
Peter Kairouz, H. Brendan McMahan, Brendan Avent, AUreIien Bellet, Mehdi Bennis, Arjun Nitin
Bhagoji, Kallista Bonawitz, Zachary Charles, Graham Cormode, Rachel Cummings, Rafael G. L.
D’Oliveira, Hubert Eichner, Salim El Rouayheb, David Evans, Josh Gardner, Zachary Garrett,
Adria Gascon, Badih Ghazi, Phillip B. Gibbons, Marco Gruteser, Zaid Harchaoui, Chaoyang He,
Lie He, Zhouyuan Huo, Ben Hutchinson, Justin Hsu, Martin Jaggi, Tara Javidi, Gauri Joshi,
Mikhail Khodak, Jakub Konecny, Aleksandra Korolova, Farinaz Koushanfar, Sanmi Koyejo,
Tancrede Lepoint, Yang Liu, Prateek Mittal, Mehryar Mohri, Richard Nock, Ayfer Ozgur, Rasmus
Pagh, Mariana Raykova, Hang Qi, Daniel Ramage, Ramesh Raskar, Dawn Song, Weikang Song,
Sebastian U. Stich, Ziteng Sun, Ananda Theertha Suresh, Florian Tramer, Praneeth Vepakomma,
Jianyu Wang, Li Xiong, Zheng Xu, Qiang Yang, Felix X. Yu, Han Yu, and Sen Zhao. Advances
and open problems in federated learning, 2021.
Jakub Konecny, Brendan McMahan, and Daniel Ramage. Federated optimization: Distributed op-
timization beyond the datacenter. CoRR, abs/1511.03575, 2015. URL http://arxiv.org/
abs/1511.03575.
13
Under review as a conference paper at ICLR 2022
Nikola Konstantinov, Elias Frantar, Dan Alistarh, and Christoph Lampert. On the sample complexity
of adversarial multi-source PAC learning. In Proceedings of the 37th International Conference
on Machine Learning, ICML 2020, 13-18 July 2020, Virtual Event, volume 119 of Proceedings
ofMachine Learning Research, pp. 5416-5425. PMLR, 2020. URL http://Proceedings.
mlr.press/v119/konstantinov20a.html.
Ram Shankar Siva Kumar, Magnus Nystrom, John Lambert, Andrew Marshall, Mario Goertzel,
Andi Comissoneru, Matt Swann, and Sharon Xia. Adversarial machine learning-industry per-
spectives. In 2020 IEEE Security and Privacy Workshops, SP Workshops, San Francisco, CA,
USA, May 21, 2020, pp. 69-75. IEEE, 2020. doi: 10.1109/SPW50608.2020.00028. URL
https://doi.org/10.1109/SPW50608.2020.00028.
Florian Lehmann and Daniel Buschek. Examining autocompletion as a basic concept for interaction
with generative AI. i-com, 19(3):251-264, 2021. doi: 10.1515/icom-2020-0025. URL https:
//doi.org/10.1515/icom-2020-0025.
Saeed Mahloujifar, Mohammad Mahmoody, and Ameer Mohammed. Data poisoning attacks
in multi-party learning. In Kamalika Chaudhuri and Ruslan Salakhutdinov (eds.), Proceed-
ings of the 36th International Conference on Machine Learning, ICML 2019, 9-15 June
2019, Long Beach, California, USA, volume 97 of Proceedings of Machine Learning Re-
search, pp. 4274-4283. PMLR, 2019. URL http://proceedings.mlr.press/v97/
mahloujifar19a.html.
Guangcan Mai, Kai Cao, Pong C. Yuen, and Anil K. Jain. On the reconstruction of face images
from deep face templates. IEEE Trans. Pattern Anal. Mach. Intell., 41(5):1188-1202, 2019.
doi: 10.1109/TPAMI.2018.2827389. URL https://doi.org/10.1109/TPAMI.2018.
2827389.
Kris McGuffie and Alex Newhouse. The radicalization risks of GPT-3 and advanced neural language
models. CoRR, abs/2009.06807, 2020. URL https://arxiv.org/abs/2009.06807.
Reshef Meir, Shaull Almagor, Assaf Michaely, and Jeffrey S. Rosenschein. Tight bounds for strat-
egyproof classification. In The 10th International Conference on Autonomous Agents and Multi-
agent Systems - Volume 1, AAMAS ’11, pp. 319-326, Richland, SC, 2011. International Founda-
tion for Autonomous Agents and Multiagent Systems. ISBN 0982657153.
Reshef Meir, Ariel D. Procaccia, and Jeffrey S. Rosenschein. Algorithms for strategyproof classifi-
cation. Artificial Intelligence, 186:123-156, 2012. ISSN 0004-3702. doi: https://doi.org/10.1016/
j.artint.2012.03.008. URL https://www.sciencedirect.com/science/article/
pii/S000437021200029X.
El Mahdi El Mhamdi, Rachid Guerraoui, and Sebastien Rouault. The hidden vulnerability of dis-
tributed learning in byzantium. In Jennifer G. Dy and Andreas Krause (eds.), Proceedings of the
35th International Conference on Machine Learning, ICML 2018, Stockholmsmassan, Stockholm,
Sweden, July 10-15, 2018, volume 80 of Proceedings of Machine Learning Research, pp. 3518-
3527. PMLR, 2018. URL http://proceedings.mlr.press/v80/mhamdi18a.html.
Luis Munoz-Gonzalez, Battista Biggio, Ambra Demontis, Andrea Paudice, Vasin Wongrassamee,
Emil C. Lupu, and Fabio Roli. Towards poisoning of deep learning algorithms with back-gradient
optimization. In Bhavani M. Thuraisingham, Battista Biggio, David Mandell Freeman, Brad
Miller, and Arunesh Sinha (eds.), Proceedings of the 10th ACM Workshop on Artificial Intelli-
gence and Security, AISec@CCS 2017, Dallas, TX, USA, November 3, 2017, pp. 27-38. ACM,
2017. doi: 10.1145/3128572.3140451. URL https://doi.org/10.1145/3128572.
3140451.
Lisa-Maria Neudert, Philip Howard, and Bence Kollanyi. Sourcing and automation of polit-
ical news and information during three european elections. Social Media+ Society, 5(3):
2056305119863147, 2019.
Huy L. Nguyen and Lydia Zakynthinou. Improved algorithms for collaborative PAC learning. In
Samy Bengio, HannaM. Wallach, Hugo Larochelle, Kristen Grauman, Nicolo Cesa-Bianchi, and
14
Under review as a conference paper at ICLR 2022
Roman Garnett (eds.), Advances in Neural Information Processing Systems 31: Annual Con-
ference on Neural Information Processing Systems 2018, NeurIPS 2018, December 3-8, 2018,
Montreal, Canada, pp. 7642-7650, 2018. URL https://proceedings.neurips.cc/
paper/2018/hash/3569df159ec477451530c4455b2a9e86-Abstract.html.
Javier Perote and Juan Perote-Pena. Strategy-proof estimators for simple regression. Math-
ematical Social Sciences, 47(2):153-176, 2004. ISSN 0165-4896. doi: https://doi.org/10.
1016/S0165-4896(03)00085-4. URL https://www.sciencedirect.com/science/
article/pii/S0165489603000854.
Javier Perote and Olavide Sevilla. The impossibility of strategy-proof clustering. Economics Bul-
letin, 2003.
HUy Phan. huyvnphan/Pytorch_cifar10, January 2021. URL https://doi.org/10.5281/
zenodo.4431043.
Mingda Qiao. Do outliers ruin collaboration? In Jennifer G. Dy and Andreas Krause (eds.), Proceed-
ings of the 35th International Conference on Machine Learning, ICML 2018, Stockholmsmassan,
Stockholm, Sweden, July 10-15, 2018, volume 80 of Proceedings of Machine Learning Re-
search, pp. 4177-4184. PMLR, 2018. URL http://proceedings.mlr.press/v80/
qiao18a.html.
Francesco Ricci, Lior Rokach, and Bracha Shapira. Introduction to recommender systems hand-
book. In Francesco Ricci, Lior Rokach, Bracha Shapira, and Paul B. Kantor (eds.), Recom-
mender SystemsHandbook,理.1-35. Springer, 2011. doi:10.1007/978-0-387-85820-3\_1. URL
https://doi.org/10.1007/978-0-387- 85820-3_1.
Avi Schwarzschild, Micah Goldblum, Arjun Gupta, John P Dickerson, and Tom Goldstein.
Just how toxic is data poisoning? a unified benchmark for backdoor and data poisoning at-
tacks. In Marina Meila and Tong Zhang (eds.), Proceedings of the 38th International Con-
ference on Machine Learning, volume 139 of Proceedings of Machine Learning Research, pp.
9389-9398. PMLR, 18-24 Jul 2021. URL https://proceedings.mlr.press/v139/
schwarzschild21a.html.
Giorgio Severi, Jim Meyer, Scott Coull, and Alina Oprea. Explanation-guided backdoor poi-
soning attacks against malware classifiers. In Michael Bailey and Rachel Greenstadt (eds.),
30th USENIX Security Symposium, USENIX Security 2021, August 11-13, 2021, pp. 1487-
1504. USENIX Association, 2021. URL https://www.usenix.org/conference/
usenixsecurity21/presentation/severi.
Ali Shafahi, W. Ronny Huang, Mahyar Najibi, Octavian Suciu, Christoph Studer, Tudor Dumitras,
and Tom Goldstein. Poison frogs! targeted clean-label poisoning attacks on neural networks.
In Samy Bengio, Hanna M. Wallach, Hugo Larochelle, Kristen Grauman, Nicolo Cesa-Bianchi,
and Roman Garnett (eds.), Advances in Neural Information Processing Systems 31: Annual Con-
ference on Neural Information Processing Systems 2018, NeurIPS 2018, December 3-8, 2018,
Montreal, Canada, pp. 6106-6116, 2018. URL https://proceedings.neurips.cc/
paper/2018/hash/22722a343513ed45f14905eb07621686-Abstract.html.
Virat Shejwalkar, Amir Houmansadr, Peter Kairouz, and Daniel Ramage. Back to the drawing
board: A critical evaluation of poisoning attacks on federated learning. CoRR, abs/2108.10241,
2021. URL https://arxiv.org/abs/2108.10241.
Heung-Yeung Shum, Xiaodong He, and Di Li. From eliza to xiaoice: challenges and opportunities
with social chatbots. Frontiers Inf. Technol. Electron. Eng., 19(1):10-26, 2018. doi: 10.1631/
FITEE.1700826. URL https://doi.org/10.1631/FITEE.1700826.
Jason R. Smith, Herve Saint-Amand, Magdalena Plamada, Philipp Koehn, Chris Callison-Burch,
and Adam Lopez. Dirt cheap web-scale parallel text from the common crawl. In Proceedings of
the 51st Annual Meeting of the Association for Computational Linguistics, ACL 2013, 4-9 August
2013, Sofia, Bulgaria, Volume 1: Long Papers, pp. 1374-1383. The Association for Computer
Linguistics, 2013. URL https://www.aclweb.org/anthology/P13-1135/.
Joan E. Solsman. Youtube’s ai is the puppet master over most of what you watch. CNET, 2018.
15
Under review as a conference paper at ICLR 2022
Fnu Suya, Saeed Mahloujifar, Anshuman Suri, David Evans, and Yuan Tian. Model-targeted poi-
soning attacks with provable convergence. In Marina Meila and Tong Zhang (eds.), Proceedings
of the 38th International Conference on Machine Learning, ICML 2021, 18-24 July 2021, Virtual
Event, volume 139 of Proceedings of Machine Learning Research, pp. 10000-10010. PMLR,
2021. URL http://proceedings.mlr.press/v139/suya21a.html.
Loc Truong, Chace Jones, Brian Hutchinson, Andrew August, Brenda Praggastis, Robert
Jasper, Nicole Nichols, and Aaron Tuor. Systematic evaluation of backdoor data poison-
ing attacks on image classifiers. In 2020 IEEE/CVF Conference on Computer Vision and
Pattern Recognition, CVPR Workshops 2020, Seattle, WA, USA, June 14-19, 2020, pp. 3422-
3431. Computer Vision Foundation / IEEE, 2020. doi: 10.1109/CVPRW50498.2020.00402.
URL https://openaccess.thecvf.com/content_CVPRW_2020/html/w47/
Truong_Systematic_Evaluation_of_Backdoor_Data_Poisoning_Attacks_
on_Image_Classifiers_CVPRW_2020_paper.html.
Leslie G. Valiant. A theory of the learnable. Commun. ACM, 27(11):1134-1142, 1984. doi: 10.
1145/1968.1972. URL https://doi.org/10.1145/1968.1972.
Roman Vershynin. High-dimensional probability: An introduction with applications in data science,
volume 47. Cambridge university press, 2018.
Martin J. Wainwright. High-Dimensional Statistics: A Non-Asymptotic Viewpoint. Cambridge Series
in Statistical and Probabilistic Mathematics. Cambridge University Press, 2019. doi: 10.1017/
9781108627771.
Alex Wang, Yada Pruksachatkun, Nikita Nangia, Amanpreet Singh, Julian Michael, Felix Hill,
Omer Levy, and Samuel R. Bowman. Superglue: A stickier benchmark for general-purpose
language understanding systems. In Hanna M. Wallach, Hugo Larochelle, Alina Beygelz-
imer, Florence d,Alche-Buc, Emily B. Fox, and Roman Garnett (eds.), Advances in NeU-
ral Information Processing Systems 32: Annual Conference on Neural Information Pro-
cessing Systems 2019, NeurIPS 2019, December 8-14, 2019, Vancouver, BC, Canada, pp.
3261-3275, 2019a. URL https://proceedings.neurips.cc/paper/2019/hash/
4496bf24afe7fab6f046bf4923da8de6- Abstract.html.
Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R. Bowman.
GLUE: A multi-task benchmark and analysis platform for natural language understanding. In
7th International Conference on Learning Representations, ICLR 2019, New Orleans, LA, USA,
May 6-9, 2019. OpenReview.net, 2019b. URL https://openreview.net/forum?id=
rJ4km2R5t7.
Shurun Wang, Shiqi Wang, Xinfeng Zhang, Shanshe Wang, Siwei Ma, and Wen Gao. Scalable
facial image compression with deep feature reconstruction. In 2019 IEEE International Confer-
ence on Image Processing, ICIP 2019, Taipei, Taiwan, September 22-25, 2019, pp. 2691-2695.
IEEE, 2019c. doi: 10.1109/ICIP.2019.8803255. URL https://doi.org/10.1109/ICIP.
2019.8803255.
Yuanyuan Wu, Eric W. T. Ngai, Pengkun Wu, and Chong Wu. Fake online reviews: Literature
review, synthesis, and directions for future research. Decis. Support Syst., 132:113280, 2020. doi:
10.1016/j.dss.2020.113280. URL https://doi.org/10.1016/j.dss.2020.113280.
Cong Xie, Oluwasanmi Koyejo, and Indranil Gupta. Fall of empires: Breaking byzantine-tolerant
SGD by inner product manipulation. In Amir Globerson and Ricardo Silva (eds.), Proceedings of
the Thirty-Fifth Conference on Uncertainty in Artificial Intelligence, UAI 2019, Tel Aviv, Israel,
July 22-25, 2019, volume 115 of Proceedings of Machine Learning Research, pp. 261-270. AUAI
Press, 2019. URL http://proceedings.mlr.press/v115/xie20a.html.
Matthew D. Zeiler and Rob Fergus. Visualizing and understanding convolutional networks. In
David J. Fleet, Tomas Pajdla, Bernt Schiele, and Tinne Tuytelaars (eds.), Computer Vision - ECCV
2014 - 13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part
I, volume 8689 of Lecture Notes in Computer Science, pp. 818-833. Springer, 2014. doi: 10.1007/
978-3-319-10590-1∖.53. URL https://doi.org/10.1007/978-3-319-10590-1_
53.
16
Under review as a conference paper at ICLR 2022
Shihao Zhao, Xingjun Ma, Xiang Zheng, James Bailey, Jingjing Chen, and Yu-Gang Jiang. Clean-
label backdoor attacks on video recognition models. In 2020 IEEE/CVF Conference on Computer
Vision and Pattern Recognition, CVPR 2020, Seattle, M, USA, June 13-19, 2020, pp.14431-
14440. Computer Vision Foundation / IEEE, 2020. doi: 10.1109/CVPR42600.2020.01445.
URL https://openaccess.thecvf.com/content_CVPR_2020/html/Zhao_
Clean-Label_Backdoor_Attacks_on_Video_Recognition_Models_CVPR_
2020_paper.html.
Chen Zhu, W. Ronny Huang, Hengduo Li, Gavin Taylor, Christoph Studer, and Tom Goldstein.
Transferable clean-label poisoning attacks on deep neural nets. In Kamalika Chaudhuri and Rus-
lan Salakhutdinov (eds.), Proceedings of the 36th International Conference on Machine Learn-
ing, ICML 2019, 9-15 June 2019, Long Beach, California, USA, volume 97 of Proceedings of
Machine Learning Research, pp. 7614—7623. PMLR, 2019. URL http://Proceedings.
mlr.press/v97/zhu19a.html.
17
Under review as a conference paper at ICLR 2022
Appendices
A Convexity lemmas
A.1 General lemmas
Definition 3. We say that f : Rd → R is locally strongly convex if, for any convex compact set
C ⊂ Rd, there exists μ > 0 such that f is μ-strongly convex on C, i.e. for any x,y ∈ C and any
λ ∈ [0, 1], we have
f(λx +(I- λ)y) ≤ λf (X) + (I- λ)f (y) - 2λ(I- λ) Ilx - yk2 .	⑺
It is well-known that if f is differentiable, this condition amounts to saying that
∣∣Vf (x) — Vf (y)∣2 ≥ μ ∣∣x — y∣2 forall x,y ∈ C. And if f is twice differentiable, then it amounts
to Saying V2f (x)占 μI for all x ∈ C.
Lemma 6. Iff is locally strongly convex and g is convex, then f + g is locally strongly convex.
Proof Indeed, (f + g)(λx + (1 - λ)y) ≤ λf (X) + (1 - λ)f (y) - μλ(1 - λ) Ilx - yk2 + λg(X) +
(1 — λ)g(y) = λ(f + g)(x) + (1 — λ)(f + g)(y) — μλ(1 — λ) kx — y∣2.	□
Definition 4. We say that f : Rd → R is L-smooth if it is differentiable and if its gradient is
L-Lipschitz continuous, i.e. for any x, y ∈ Rd,
IVf (x) — Vf (y)∣2 ≤ L Ilx — y∣∣2.	(8)
Lemma 7. Iff is Lf -smooth and g is Lg -smooth, then f + g is (Lf + Lg)-smooth.
Proof Indeed, ∣V(f + g)(x) — V(f + g)(y)∣2 ≤ ∣∣Vf(x) — Vf(y)∣2 + ∣∣Vg(x) — Vg(y)∣b ≤
Lf kx — yk2 + Lg kx — yk2 = (Lf + Lg) kx — yk2 .	□
Lemma 8. Suppose that f : Rd × Rd0 7→ R is locally strongly convex and L-smooth, and that, for
any x ∈ X, where X ⊂ Rd is a convex compact subset, the map y → f (x, y) has a minimum y* (x).
Note that local strong convexity guarantees the uniqueness of this minimum. Then, there exists K
such that thefunction y* is K-Lipschitz continuous on X.
Proof. The existence and uniqueness of y* (x) hold by strong convexity. Fix x, x0. By optimality of
y*, we know that Vyf(x, y*(x)) = Vyf(x0, y*(x0)) = 0. We then have the following bounds
μky*(x) - y*(x0)k2 ≤ kVyf(x,y*(x)) - Vyf(x,y*(x0))k2 = kVyf(x,y*(x0))k2	⑼
= kVyf(x, y*(x0)) — Vyf(x0,y*(x0))k2	(10)
≤ kVf(x,y*(x0)) — Vf(x0,y*(x0))k2	(11)
≤ Lk(x — x0, y*(x0) — y*(x0))k2 = Lkx —x0k2,	(12)
where we first used the local strong convexity assumption, then the fact that Vyf(x, y* (x)) = 0,
then the fact that Vyf (x0, y* (x0)) = 0, and then the L-smooth assumption.	□
Lemma 9. Suppose that f : Rd × Rd0 7→ R is locally strongly convex and L-smooth, and that,
for any x ∈ X, where X ⊂ Rd is a convex compact subset, the map y 7→ f(x, y) has a minimum
y*(x). Define g(x) , miny∈Y f(x, y). Then g is convex and differentiable on X and Vg(x) =
Vxf(x, y*(x)).
Proof. First we prove that g is convex. Let x1, x2 ∈ Rd, and λ1, λ2 ∈ [0, 1] with λ1 + λ2 = 1. For
any y1 , y2 ∈ Rd0 , we have
g(λ1x1 + λ2x2) = min f(λ1x1 + λ2x2, y)	(13)
y∈Rd0
≤ f(λ1x1 + λ2x2, λ1y1 + λ2y2)	(14)
≤ λ1f(x1, y1) + λ2f(x2,y2).	(15)
18
Under review as a conference paper at ICLR 2022
Taking the infimum of the right-hand side over y1 and y2 yields g(λ1x1 + λ2x2) ≤ λ1g(x1) +
λ2g(x2), which proves the convexity of g.
Now denote h(x) = Vχf (x, y*(x)). We aim to show that Vg(x) = h(x). Let ε ∈ Rd small enough
so that x + ε ∈ X . Now note that we have
g(x + ε) = min f(x + ε,y) ≤ f(x + ε,y*(x))	(16)
y∈Rd0
=f(x,y*(x)) + εT Vχf(x,y*(x)) + 0(|同卜)	(17)
= g(x) + εT h(x) + o(kεk2),	(18)
which shows that h(x) is a superderivative of g at x. We now show that it is also a subderivative. To
do so, first note that its value at x + ε is approximately the same, i.e.
kh(x + ε) - h(x)∣∣2 ≤ IlVxf(X + ε,y*(x + ε))-Vχf(x,y*(x + ε))∣b
+ kVxf (X,y*(X + D)- Vxf(X,y*(X))lb	(19)
≤ L kεk2+L ky*(X+ε) — y*(X)k2 ≤ (L+^μ) kεk2,	QO)
where we used the L-smoothness of f and Lemma 8. Now notice that
g(X)= min f (X,y) ≤ f(X,y*(X + ε)) = f ((x + ε) - ε,y*(X + ε))	(21)
y∈Rd0
=f(x + ε,y*(X + ε)) - ετVxf(X + ε,y*(X + ε)) +。(1闫卜)	(22)
= g(X + ε) - εT h(X) - εT (h(X + ε) - h(X)) + o(IεI2),	(23)
But we know that Ih(X + ε) - h(X)I2 = O(IεI2). Rearranging the terms then yields
g(X+ε) ≥ g(X) + εTh(X) - o(IεI2),	(24)
which shows that h(X) is also a subderivative. Therefore, we know thatg(X+ε) = g(X) +εTh(X) +
0(kε∣∣2), which boils down to saying that g is differentiable in X ∈ X, and that Vg(X) = h(X). □
Lemma 10. Suppose that f : X × Rd0 → R is μ-strongly convex, where X ⊂ Rd is closed and
convex. Then g : X → R, defined by g(∕) = infy∈γ f (x, y), is well-defined and μ-strongly convex
too.
Proof. The function y 7→ f(X, y) is still strongly convex, which means that it is at least equal to
a quadratic approximation around 0, which is a function that goes to infinity in all directions as
Iy I2 → ∞. This proves that the infimum must be reached within a compact set, which implies the
existence of a minimum. Thus g is well-defined. Moreover, for any X1, X2 ∈ X, y1, y2 ∈ Rd0, and
λ1 , λ2 ≥ 0 with λ1 + λ2 = 1, we have
g(λ1X1 + λ2X2) = inf f(λ1X1 + λ2X2, y)	(25)
y
≤ f(λ1X1 + λ2X2, λ1y1 + λ2y2)	(26)
≤ λιf (X1,y1) +	λ2f(X2,y2)	—	2λ1λ2 ∣∣(xi — X2,yι	— y2)k2	(27)
≤ λ1f (X1,yI) +	λ2f (X2,y2)	-	2λ1λ2 kX1 - x2 k2 ,	(28)
where we used the μ-strong convexity of f. Taking the infimum over y1,y2 implies the μ-strong
convexity of g.	□
A.2 Applications to Loss
Now instead of proving our theorems for different cases separately, we make the following assump-
tions on the components of the global loss that encompasses both `22 and smooth-`2 regularization,
a well as linear regression and logistic regression.
Assumption 1. Assume that ' is convex and l`-smooth, and that R(ρ,θ) = R0(ρ — θ), where
R0 : Rd → R is locally strongly convex (i.e. strongly convex on any convex compact set), LR0-
smooth and satisfy R0(z) = Ω(kz∣∣2) as ∣∣z∣2 → ∞.
19
Under review as a conference paper at ICLR 2022
Lemma 11. Under Assumption 1, LOSS is locally strongly convex and L-smooth.
Proof. All terms of LOSS are L0-smooth, for an appropriate value of L0 . By Lemma 7, their sum
is thus also L-smooth, for an appropriate value of L. Now, given Lemma 6, to prove that LOSS is
locally strongly convex, it suffices to prove that ν P kθnk22 +R0(ρ - θ1) is locally strongly convex.
Consider any convex compact set C ⊂ Rd×(1+N) . Since R0 is locally strongly convex, we know
that there exists μ > 0 such that V2Ro 占 μI. As a result,
(ρ, ~)T (V2LOSS) (ρ, ~) ≥ V X kθnk2 + μ kρ - θιk2	(29)
n∈[N]
=VI∣θιk2 + μI∣ρk2 + μI∣θιk2 - 2μρTθι + νX kθnk2.	(3O)
n6=1
Now define α，ʌ/ν+μ2μ. Clearly, 0 < α < 1. Moreover, 0 ≤ ∣∣αθι 一 αρ∣∣2 = 今 ∣∣θι∣∣2 +
ɑ2 ∣∣ρ∣2 一 2ρTθι. Therefore 2ρτθι ≤ α2 ∣∣ρ∣2 + 去 ∣∣θι∣2, which thus implies
(ρ, ~)t (V2LOSS) (ρ, ~) ≥(ν + μ (1 一 α-2)) ||&∣∣2 + μ(1 一 α2) ∣∣ρ∣∣2 + ν X ∣∣θn∣∣2	(31)
n6=1
≥ 2 kθιk2+~~τ^k kpk2+ν X kθnk2 ≥ min {2, -τμ27l} ∣∣(ρ,~)∣∣2,	(32)
v	V I μ μ	r ]	INzV ∣ μ μ j 11	112
which proves that V2Loss 占 κI, with κ > 0. This shows that Loss is locally strongly convex. 口
Lemma 12. Under Assumption 1, ρ → θ (ρ, D) is Lipchitz continuous on any compact set.
Proof. Define fn(ρ,θn)，V ∣∣θn∣2 + Px∈Dn '(θn,x) + λ ∣∣ρ 一 θn∣2. If ' is L-smooth, then f
is clearly (|Dn | L + V + λ)-smooth. Moreover, if ` is convex, then for any ρ, the function θn 7→
fn(ρ,θn) is at least V-strongly convex. Thus Lemma 8 applies, which guarantees that ρ → θ*(ρ, D)
is Lipchitz.	口
Lemma 13. Under Assumption 1, ρ → LOSS(P,θ*(ρ, D), D) is L-smooth and locally strongly
convex.
Proof. By Lemma 11, the global loss is known to be L-smooth, for some value of L and locally
strongly convex. Denoting f : ρ → Loss(ρ, θ*(ρ, D), D), we then have
皿(。)—〃媚|2 ≤ ∣∣VρLθSS(ρ,~*(ρ, D), D) — VpLOSS(p0,~*(p0, D), D)∣∣2	(33)
≤ L∣∣(ρ,~*(ρ,D)) — (屋~*(。0,D))∣∣2	(34)
≤ L ∣ρ 一 ρ0 ∣2 ,	(35)
which proves that f is L-smooth.
For strong convexity, note that since the global loss function is locally strongly convex, for any com-
pact convex set C, there exists μ such that Loss(ρ, θ, D) is μ-strongly convex on C = (Ci, C2) ⊂
(Rd, RN×d), therefore, by Lemma 10, f (ρ) will also be μ-strongly convex on Ci which means that
f (ρ) is locally strongly convex.	口
B Proof of the equivalence
B.1	Proof of the reduction from model attack to data poisoning
Proof of Lemma 1. We omit making the dependence of the optima on D~ explicit, and we consider
any other models ρ and θ~-s . We have the following inequalities:
LOSSs(P*,~-s,θ*,D) = LOSS(P*,~*,D) -£冏,Ds)	(36)
≤ LOSS(P, (θ-s,θS), D) - L(θ;, Ds)= LOSSs(ρ,~-s,θ*, D),	(37)
20
Under review as a conference paper at ICLR 2022
where We used the optimality of (ρ , θ*) m the second line, and where We repeatedly used the fact
that θS = θ?. This proves that (ρ*,~-§) is a global minimum of the modified loss.	□
B.2	Proof of the reduction from data poisoning to model attack
First, we define the following modified loss function:
LOSSs(ρ, θ-s,θ↑, D-S)，LOSS(ρ, (θ↑,θ-s), (0, D-S))	(38)
where θ-s and D-S are variables and datasets for users n = s. We then define ρ*(θ^, D-S) and
~-§ (θ?, D-s) as a minimum of the modified loss function, and θ* (θ?, D-S)，θ?. We now prove
a slightly more general version of Lemma 2, which applies to a larger class of regularizations. It
also shows how to construct the strategic’s user data poisoning attack.
Lemma 14 (Reduction from data poisoning to model attack). Assume local PAC* learning. Sup-
pose also that R is continuous and that R(ρ, θ) → ∞ when kρ - θk2 → ∞. Consider any
datasets D-S and any attack model θ? such that the modified loss LOSSS has a unique minimum
P* (θ2, D-s ), θ-s (θ2, D-s )∙ Then, for any ε, δ > 0, there exists I such that if user s’s dataset DS
contains at least I inputs drawnfrom model θ?, then, with probability at least 1 一 δ, we have
卜(D)-ρ*(θ,, DD-s)∣∣2 ≤ ε and ∀n= s, |"(D『8”, D-s)( ≤ ε. (39)
Clearly, `22, `2 and smooth-`2 are continuous regularizations, and verify R(ρ, θ) → ∞ when
kρ 一 θk2 → ∞. Moreover, setting δ , 1/2 shows that the probability that the dataset DS satis-
fies the inequalities of Lemma 14 is positive. This implies in particular that there must be a dataset
DS that satisfies these inequalities. ALl in all, this shows that Lemma 14 implies Lemma 2.
ProofofLemma 14. Let ε, δ > 0 and θ? ∈ Rd. Denote ρ*，P*(θ2, D-S) and θ*，~*(θ2, D-S)
the result of strategic user s’s model attack. We define the compact set C by
C , {ρ,~-s I ∣∣P-P* Il2 ≤ ε ∧∀n = s, ∣∣θn-θ2∣∣2 ≤ ε}	(40)
We define D，Rd×N - C the closure of the complement of C. Clearly, ρ*, G-S ∈ D. We aim
to show that, when strategic user s reveals a large dataset DS whose answers are provided using the
attack model θS-, then the same holds for any global minimum of the global loss P*(Dθ ), θθ-* S(Dθ ) ∈
C. Note that, to prove this, it suffices to prove that the modified loss takes too large values, even
when θS- is replaced by θS*(Dθ ).
Let us now formalize this. Denote L- , LOSSS (P-, θθ--S, θS-, Dθ-S). We define
η, inf LOSSS(P, θθ-S, θS-, Dθ -S) -L-.	(41)
ρ,θ~-s ∈D
By a similar argument as that of Lemma 5, using the assumption R → ∞ at infinity, we know that
the infimum is actually a minimum. Moreover, given that the minimum of the modified loss LOSSS
is unique, we know that the value of the loss function at this minimum is different from its value at
P-, θθ--S. As a result, we must have η > 0.
Now, since the function R is differentiable, it must be continuous. By the Heine-Cantor theorem,
it is thus uniformly continuous on all compact sets. Thus, there must exist κ > 0 such that, for all
models θS satisfying ∣θS - θS- ∣2 ≤ κ, we have
∣R(θs,ρ-) -R(θ-,P-)∣ ≤ η∕3.	(42)
Now, Lemma 5 guarantees the existence of I such that, if user s provides a dataset DS of least I
answers with the model θS-, then with probability at least 1 - δ, we will have ∣∣θS*(Dθ ) - θS- ∣∣ ≤
min(κ, ε). Under this event, we then have
LOSSS P-,θθ--S,θS*(Dθ),Dθ-S ≤L- + η∕3.	(43)
21
Under review as a conference paper at ICLR 2022
Then
inf LOSSs(P,~-s ,θ;(D), D-S) ≥ inf LOSSs(P,~-s,θ,, D-S)- n/3	(44)
ρ,θ~-s∈D	ρ,θ~-s∈D
≥ L + η - n/3 ≥ L + 2η∕3	(45)
> LOSSs (ρ*,~*s,θ;(D), D-s).	(46)
This shows that there is a high probability event under which the minimum of P, θ~-s 7→
LOSSs (ρ, ~-s,θ*(DD), D-S) cannot be reached in D. This is equivalent to What the theorem We
needed to prove states.	□
B.3 Proof of reduction from model attack to gradient attack
In this section, We prove a slightly more general result than Lemma 3. Namely, instead of Work-
ing With specific regularizations, We consider a more general class of regularizations, identified by
Assumption 1.
Lemma 15 (Reduction from model attack to gradient attack). Under Assumption 1, if gst converges
and if ηt = η is a constant small enough, then Pt will converge too. Denote P∞ its limit. Then for
any ε > 0, there is θ? ∈ Rd such that ∣∣ρ∞ 一 ρ*(θ?, D-s)∣∣ ≤ ε.
Note that since `22 and smooth-`2 regularizations satisfy Assumption 1, Lemma 15 clearly implies
Lemma 3. We noW introduce the key objects of the proof of Lemma 15.
Denote gs∞ the limit of the attack gradients gst . We noW define
LOSSs1(P), θ~in-fs
- Ls(θs, Ds) - R(P, θs)	+PTgs∞
(47)
inf	Ln(θn,Dn)+	R(P,θn) +PTgs∞,
θ-s I n=s	n=s
(48)
and prove that Pt Will converge to the minimizer of LOSSs1(P). By Lemma 13, We shoW that
LOSSs1 (P) is both locally strongly convex and L-smooth.
Now define Zs，gs 一 g∞. We then know Zs → 0 and VLOSS1(ρt) is the sum of all gradient
vectors received from all users assuming the strategic user s sends the vector gs∞ in all iterations.
Thus, at iteration t of the optimization algorithm, we will take one step in the direction Gs ,
VLOSSs1 (Ps) + Zss, i.e.,
Ps+1 = Ps - ηs Gs .	(49)
We now prove the following lemma that bounds the difference between the function value in two
successive iterations.
Lemma 16. If LOSSs1 (P) is L-smooth and ηs ≤ 1/L, we have
LOSSI(Pt+1) — LOSSI(Pt) ≤ 一ns ∣∣Gs∣∣2 + nsζs Gs.	(50)
Proof. Since LOSSs1 is L-smooth, we have
LOSSI(Pt+1) ≤ LOSS1(ρt) + (ρt+1 - ρt)TVLOSS1 (ρt) + L ∣∣ρt+1 - ρt∣∣2 .	(51)
Now plugging Ps+1 - Pt = -nsGs and VLOSSI(Pt) = Gs - Zs into the inequality implies
LOSSI(Ps+1) - LOSSI(Ps) ≤ (-ntGt)T (Gs-Zs) + 2 ∣∣-ntGt∣∣2	(52)
≤- ns∣∣Gs∣∣2 + nsζsτ Gs,	(53)
where we used the fact ns ≤ 1∕L.	□
22
Under review as a conference paper at ICLR 2022
B.3.1 The global model remains bounded
Lemma 17. There is M such that, for all t, LOSSs1 (ρt) ≤ M.
Proof. Consider the closed ball B(ρ*, 1) centered on ρ* and of radius 1. By Lemma 13, We know
that LOSS1 is locally strongly convex and thus there exists a μι > 0 such that LOSS1 is μι-strongly
convex on B(ρ*, 1). Now consider a point ρι on the boundary of B", 1). By strong convexity we
have
IlVLOSS1(ρι)∣∣2 ≥ (ρι - ρ*)TVLOSS1(ρι) ≥ μι kρ1 - ρ*k2 = μι.	(54)
Now similarly, by the convexity of LOSS1 on Rd, for any P ∈ Rd — B(ρ*, 1), we have
IlVLOSS1(ρι)∣∣2 ≥ √μ1. Now since Zt → 0, there exists an iteration T11 after which (t ≥ T1),
we have ∣∣Zt∣∣2 ≤ 1 √μ1, and thus ∣∣Gt∣∣2 ≥ 1VLOSSI(ρt)∣∣2 -∣∣ZS∣∣2 ≥ 4√μ1. Thus, Lemma 16
implies that for t ≥ Ti, if ∣ρt — ρ* |卜 ≥ 1, then
LOSSsi(Pt+i) — LOSSsi(Pt) ≤	-2 I∣GtI∣2 + ηζtτGt	(55)
≤	-2∣∣GtI∣2 + η∣∣ζt∣I2I∣GtII2	(56)
≤	-2∣∣Gt∣∣2 (∣∣Gt∣∣2- 2∣∣ζt∣∣2)	(57)
≤	-24 √μι (3 √μι-4 √μτ) ≤-12 μι < 0.	(58)
Thus, for ∣∣ρt — ρ*∣2 ≥ 1, the loss cannot increase at the next iteration.
Now consider the case ∣∣ρt — ρ*∣∣2 < 1 for t ≥ Ti. The smoothness of Loss1 implies
∣∣VLOSSs1(ρt)∣∣2 < L. Therefore,
I∣Pt+1 — P*II2 = ∣∣Pt — η(VLoss1(ρt) + Zs) — ρ[∣2	(59)
≤ Hρt+1 — ρ[∣2 + η(L + 4√μι) ≤ 1 + η(L + 4√μι).	(6O)
Now we define Mi ，max°∈B(ρ* 1+以工+1√μ-)) LOSS±(ρ), the maximum function value in the
closed ball B (ρ*, 1 + η(L + 1 √μi)). Therefore, we have LOSSI(Pt+i) ≤ M∖. So far we proved
that for t ≥ T1 , in each iteration of gradient descent either the function value will not increase
or it will be upper-bounded by M1. This implies that for all t, the function value LOSSs1(Pt) is
upper-bounded by
M，max I max {LOSS1(ρt)} ,Mi} .	(61)
This concludes the proof.	□
Lemma 18. There is a compact set X such that, for all t, Pt ∈ X.
Proof. Now since LOSSI is μι-strongly convex in B(ρ*, 1), for any point P ∈ Rd such that
∣P — Pt ∣2 = 1, we have
LOSSI(P) ≥ LOSS*) + μ kρ — ρ*k2 = LOSS*) + μ.	(62)
But now by the convexity OfLOSSI in Rd, for any P such that ∣ρ — ρ*∣b ≥ 1, we have
LOSSI(P) ≥LOSSI(P*)+∣∣p—p*∣∣2 μ.	(63)
This implies that if ∣∣Pt — P*∣b > 言(M2 — LOSSI(P*)), then LOSSI(Pt) > M2. Therefore, we
must have ∣∣Pt — P*∣b ≤ 看(M2 — LOSSI(P*)), for all t ≥ 0. This describes a closed ball, which
is a compact set.	□
23
Under review as a conference paper at ICLR 2022
B.3.2 Convergence of the global model under converging gradient attack
Lemma 19. Suppose ut ≥ 0 verifies ut+1 ≤ αut + δt, with δt → 0. Then ut → 0.
Proof. We now show that for any ε > 0, there exists an iteration T (ε), such that for t ≥ T (ε), we
have ut ≤ ε. For this, note that by induction, we observe that, for all t ≥ 0,
t
ut+1 ≤ u0αt+1 +	ατ δt-τ .
τ=0
(64)
Since δt → 0, there exists an iteration T2(ε) such that for all t ≥ T2(ε), we have δt ≤ ε(1-α)
Therefore, for t ≥ T2(ε), we have
ut+1 ≤ u0αt+1
t-T2 (ε)	t
+ X ατδt-τ+ X	ατδt-τ
τ=0
≤ u0αt+1
+ ε(1 - α)
十一2一
τ =t-T2 (ε)+1
t-T2 (ε)	T2 (ε)-1
ατ +
τ=0
αt-sδs
s=0
T2(ε)-1
≤ ∣u° + X ɑ-s-1δs I αt+1 +
ε(1 - α)
-2-
∞
Xατ.
τ=0
Denoting M0(ε) , PsT=2(0ε)-1 α-s-1δs, we then have
ut+1 ≤ (u0 + M0(ε)) αt+1
ln
Therefore, for t ≥
2(u0n+M0(ε)), we have
ut+1
≤ ε + ε = ε.
~22
This proves that ut → 0.
(65)
(66)
(67)
(68)
(69)
□
ε
+ 2.
ε
We first prove the first part of Lemma 3.
Lemma 20. Under Assumption 1 and ηt = η ≤ 1/L, if gst converges, then so does ρt.
Proof. Define X based on Lemma 18. Since LOSS1 is locally strongly convex, there exists μ2 > 0
such that LOSSs is μ2-strongly convex in a convex compact set X containing Pt for all t ≥ 0. By
the strong convexity of LOSSs1(ρ), we have
LOSSI(Pt)- LOSSI(P*) ≤ (Pt-P*)TVLOSSI(Pt)-空/—*俏	(70)
= (Pt-ρ*)T (Gt-Zs) - μ22∣∣ρt-ρ*∣∣2 .	(71)
Now, using the fact
(Pt-P*)t Gt = L(Pt-P*)T (Pt-Pt+1)	(72)
η
=2η (WP"2 + W Pt+Nl2 TPt+1-T2)	(73)
=2η (η2l∣Gtll2 + l∣Pt-P*ll2 -l∣Pt+1-P*ll2)	(74)
=2 llGtll2+2η (uPt-P*u2 TlPt+1 - p*i∣2) ,	(75)
24
Under review as a conference paper at ICLR 2022
we have
LOSSI(Pt)- LOSSI(P*) ≤	(76)
2 花嘴 + *(/ - P* 俏- Q1-P* ^2) - (Pt - ρ*)T Zt - μ∣∕ - P* 俏.(77)
η
But now note that LOSSs1(Pt)-LOSSs1(P*) ≥ LOSSs1(Pt)-LOSSs1(Pt+1). Thus, combining Equation
(77) and Lemma 16 yields
-ηZSTGt ≤ ɪ (/ - P* 俏- Q1-P* 俏)-(Pt - P*)TZS -和八 P* 俏.(78)
By rearranging the terms, we then have
llPt+1-	p*∣∣2	≤(I-	μ2η) IlPt- p*i∣2 - η(d+1	- 夕*), Zs	(79)
≤(I-	μ2η) HPt- p*h2+η i∣p+1	- p*∣i2 l∣ζt∣l2.	(8o)
Now note that η ≤ 1/L < l∕μ2 and thus 0 < 1 - μ2η < 1. We now define two sequences
ut , kPt - P* k2 and δt = η kZst k2 . We already know that δt → 0, and we want to show ut also
converges to 0. By Equation (80), we have
u2+ι ≤ (1 - ημ2)u2 + δtut+ι,	(81)
which implies
δt 2	δ2	δ2
(ut+1 — — \	= ut+1 - ut+1δt + ɪ ≤ (1 - ημ2)ut + ɪ,	(82)
and thus
ut+1 ≤	J-	- ημ2)u2 +	-t-	+	Tt	≤	q/(1	- ημ2)u2	+ Tt +	Tt	≤	(1 -	η2μ2) Ut + δt.	(83)
4	2	22	2
Lemma 19 allows to conclude.	□
B.3.3 Reduction from model attack to converging gradient attack
Proof of Lemma 15. Lemma 20 already provided the convergence part of Lemma 3. We now move
forward to proving the second part of the theorem, showing that for any ε > 0, there exists θ? ∈ Rd
such that ∣∣p∞ - P*(θ?, DD-s)∣∣ ≤ ε. We define
LOSSs2(P, θs) , inf LOSS(P, θ~, D~) -Ls(θs,Ds)
θ-s
(84)
= LOSSs1(P) + R(P, θs) -PTgs∞,	(85)
and P* (θs), its minimizer. Therefore, we have
NPLOSS2(P, θs) = VpLOSSs(P) + VρR(P,θs) - g∞.	(86)
By Lemma 13, we know that LOSS2 is locally strongly convex. Therefore, there exists μ3 > 0 such
that LOSS2(P) is μ3-strongly convex in {p∣∣∣p - P*(θs)k2 ≤ 1}. Therefore, for any 0 < ε < 1, if
kP∞ - P*(θs)k2 > ε, we then have
ε∣∣VpLOSS2(P∞,θ,)∣∣2 ≥ (p∞ - P*(θs))TVPLOSS2(P∞,θ,)	(87)
≥ μ3 ∣∣p∞ - P*(θs)k2 = μ3 ≥ μ3ε2,	(88)
andthus ∣∣VPLOSS2(P∞,θ2)∣∣2 ≥ μ3ε.
Now since g∞ ∈ GRAD(P∞) there exists θ? ∈ Rd such that3 ∣∣VρR(p∞,Θ?) - g∞∣∣2 ≤ μ3ε
which yields
∣∣VρLοss2(p∞,Θ*)∣∣2 = ∣∣VρLoss1(p∞) + VρR(P∞,θ*) - g∞∣∣2	(89)
= ∣VρR(P∞,θ*) - g∞∣∣2 ≤ 等，	(90)
which is a contradiction. Therefore, we must have ∣∣p∞ - p*(θ*, DD-S)∣∣ ≤ ε.	□
3In fact, if g∞ belongs to the interior of GRAD(P∞), we can guarantee VρR(ρ∞,θ*) = g∞.
25
Under review as a conference paper at ICLR 2022
C Sum over expectations
In this section, we provide both theoretical and empirical results to argue for using a sum-based local
loss over an expectation-based local loss.
C.1 Theoretical arguments
Indeed, intuitively, if one considers an expectation Ex〜Dn ['(θn, x)] rather than a sum, as is done by
Hanzely et al. (2020), Dinh et al. (2020) and El-Mhamdi et al. (2021), then the weight of an honest
active user’s local loss will not increase as a node provides more and more data, which will hinder
the ability of θn to fit the user’s local data. In fact, intuitively, using an expectation wrongly yields
the same influence to any two nodes, even when one (honest) node provides a much larger dataset
Dn than the other, and should thus intuitively be regarded as “more reliable”.
There is another theoretical argument for using the sum rather than the expectation.
Namely, if the loss is regarded as a Bayesian negative log-posterior, given a prior
exp - Pn∈[N] ν kθnk2 - Pn∈[N] R(ρ, θn) on the local and global models, then the term that
fits local data should equal the negative log-likelihood of the data, given the models (ρ, θ). Assum-
ing that the distribution of each data point x ∈ Dn is independent from all other data points, and
depends only on the local model θn , this negative log-likelihood yields a sum over data points; not
an expectation.
C.2 Empirical results
We also empirically compared the performances of sum as opposed to the expectation. To do so, we
constructed a setting where 10 “idle” users draw randomly 10 data points from the FashionMNIST
dataset, while one “active” user has all of the FashionMNIST dataset (60,000 data points). We
then learned local and global models, with R(ρ, θ) , λ kρ - θk22, λ = 1. We compared two
different classifiers to which we refer as a “linear model” and “2-layers neural network”, both using
CrossEntropy loss. The linear model has (784 + 1) × 10 parameters. The neural network has 2 layers
of 784 parameters with bias, with ReLU activation in between, adding up to ((784 + 1) × 784 +
(784 + 1) × 10.
Note also that, in all our experiments, we did not consider any local regularization, i.e. we set ν , 0.
All our experiments are seeded with seed 999.
C.2.1 NOISY FASHIONMNIST
To see a strong difference between sum and average, we made the FashionMNIST dataset harder
to learn, by randomly labeling 60% of the training set. Table 1 reports the accuracy of local and
global models in the different settings. Our results clearly and robustly indicate that the use of sums
outperforms the use of expectations.
	^L~	~LL~	ENN	ΣNN
idle user's model	-03F	^080^	0.55	0.79
active user,s model		^080^	0.56	0.79
global model	"ʊɪ	~080~	0.58	0.79
Table 1: Accuracy of trained models, depending on the use of expectation (denoted E) or sum (Σ),
and on the use of linear classifier (L) or a 2-layer neural net (NN). Here, all users are honest and
an `22 regularization is used, but there is a large heterogeneity in the amount of data per user.
On each of the following plots, we display the top-1 accuracy on the MNIST test dataset (10 000
images) for the active user, for the global model and for one of the idle users (in Table 1, the mean
accuracy for idle users is reported), as we vary the value of λ. Intuitively, λ models how much we
want the local models to be similar.
26
Under review as a conference paper at ICLR 2022
In the case of learning FashionMNIST, given that the data is i.i.d., larger values of λ are more
meaningful (though our experiments show that they may hinder convergence speed). However,
in settings where users have different data distributions, e.g. because the labels depend on users’
preferences, then smaller values of λ may be more relevant.
Note that the use of a common value of λ in both cases is slightly misleading, as using the sum
intuitively decreases the comparative weight of the regularization term. To reduce this effect, for
this experiment only, we divide the local losses by the average of the number of data points per
node for the sum version. This way, if the number of points is equal for all nodes, the two losses
will be exactly the same. What’s more, our experiments seem to robustly show that using the sum
consistently outperforms the expectation, for both a linear classifier and a 2-layer neural network,
for the problem of noisy FashionMNIST classification.
O 25	50	75 IOO 125	150	175	200
Epochs
(a) Using the average
----acc_big
----acc small
....acc_glob
Figure 4: Linear model on noisy FashionMNIST, for λ = 0.01.
0.0 J , 一…一一 I 一，一一 I 一，…一I 一
O 25	50	75 IOO 125	150	175	200
Epochs
(b) Using the sum
o.o
o
1.0
0.8
Aue∙Jnb4
25	50	75	100	125	150	175	200
Epochs
(a) Using the average
Figure 5: 2-layer neural network on noisy FashionMNIST, for λ = 0.01.
1.0
0.8
-64
O O
Aue∙Jnb4
0.0 ..................................................
0	25	50	75	100	125	150	175	200
Epochs
(b) Using the sum

27
Under review as a conference paper at ICLR 2022


O 25	50	75 IOO 125	150	175	200
Epochs	Epochs
(a) Using the average	(b) Using the sum
Figure 6:	Linear model on noisy FashionMNIST, for λ = 0.1.
AUeJnUOV
0.2
0.0
O 25	50	75 IOO 125	150	175	200
Epochs
(a)	Using the average
AUeJnUOV
----acc-big
----acc small
__________
■ acc glob
0.0一一一一一 一一一 …一…一 一-…
0	25	50	75	100	125	150	175	200
Epochs
(b)	Using the sum
Figure 7:	2-layer neural network on noisy FashionMNIST, for λ = 0.1.
1.0
0.8
Ausny
0.0」，一，，，一，，，一，，，一，，，一，，，一，，，一，，，一，，，一
0	25	50	75	100	125	150	175	200
Epochs
(a)	Using the average
1.0
0.8
-64
O O
Aue∙Jnb4
0.0」，一，，，一，，，一，，，一，，，一，，，一，，，一，，，一，，，一
0	25	50	75	100	125	150	175	200
Epochs
(b)	Using the sum
Figure 8:	Linear model on noisy FashionMNIST, for λ = 1.
28
Under review as a conference paper at ICLR 2022
----acc_big
----acc small
_
....acc_glob
O 25	50	75 IOO 125	150	175	200
Epochs
(a)	Using the average
25	50	75 IOO 125	150	175	200
Epochs
(b)	Using the sum
Figure 9: 2-layer neural network on noisy FashionMNIST, for λ = 1.
0.0 l I l l l l I l l l l I l l l l I l l l l I l l l l I l l l l I l l l l I l l l l I l
O 25	50	75 IOO 125 150 175 200
Epochs
(b)	Using the sum
O 25	50	75 IOO 125	150	175	200
Epochs
(a)	Using the average


Figure 10:	Linear model on noisy FashionMNIST, for λ = 10.
Aue∙Jnb4
----acc-big
----acc-small
....acc_glob
Epochs
0.2
0.0
0	25	50	75	100	125	150	175	200
Epochs
-64
O O
Aue∙Jnb4
(a) Using the average
(b) Using the sum
Figure 11:	2-layer neural network on noisy FashionMNIST, for λ = 10.
29
Under review as a conference paper at ICLR 2022
O 25	50	75 IOO 125	150	175	200
Epochs
(a) Using the average
Epochs
(a) Using the average
Figure 13: 2-layer neural network on noisy FashionMNIST, for λ = 100.
25	50	75 IOO 125	150	175	200
Epochs
(b) Using the sum
Figure 12: Linear model on noisy FashionMNIST, for λ = 100.
----acc-big
----acc-small
……■ acc_glob	o.8
0.2
-64
O O
>0≡30w<
0.0
0	25	50	75	100	125	150	175	200
Epochs
(b) Using the sum

C.2.2 FashionMNIST without noise
Recall that we introduced noise into FashionMNIST to make the problem harder to learn and ob-
serve a clear difference between the average and the sum. In this section, we present results of our
experiments when the noise is removed.
1.0
1.0 ∏-------------------------------------------------------------
----acc_big
0.0 j~I~'~'~'_I~'~'~'_I~'~'~'_I~'~'~'~I~∙~∙~■~
O	200	400	600	800 IOOO
Epochs
(a) Using the average
0.8
-64
O O
Aue∙Jnb4
Epochs
(b) Using the sum
Figure 14: Linear model on FashionMNIST (without noise), for λ = 1.
----acjb∣g
----acc small
_________
....acc glob
30
Under review as a conference paper at ICLR 2022
(a) Using the average	(b) Using the sum
Figure 15: 2-layer neural network on FashionMNIST (without noise), for λ = 1.
Even without noise, the difference between using the sum and using the expectation still seems
important. We acknowledge, however, that the plots suggest that even though we ran this experiment
for 10 times more (and 5 times more for the linear model) than other experiments, we might not have
reached convergence yet, and that the use of the expectation might still eventually gets closer to the
case of sum. We believe that the fact that the difference between sum and expectation in the absence
of noise is weak is due to the fact that the FashionMNIST dataset is sufficiently linearly separable.
Thus, we achieve a near-zero loss in both cases, which make the sum and the expectation close at
optimum.
Even in this case, however, we observed that the sum clearly outperforms the expectation especially,
in the first epochs. We argue that the reason for this is the following. By taking the average in local
losses, the weights of the data of idle nodes are essentially blown out of proportion. As a result, the
optimizer will very quickly fit these data. However, the signal from the data of the active node will
then be too weak, so that the optimizer has to first almost perfectly fit the idle nodes’ data before it
can catch the signal of the active node’s data and hence the average achieves weaker convergence
performances than the sum.
D Linear regression and classification are gradient PAC*
Throughout this section, we use the following terminology.
Definition 5. Consider a parameterized event E (I). We say that the event E occurs with high
probability if P [E (I)] → 1 as I → ∞.
D.1 Preliminaries
Define kΣk2 , maxkxk 6=0(kΣxk2 / kxk2) the `2 operator norm of the matrix Σ. For symmetric
matrices Σ, this is also the largest eigenvalue in absolute value.
Theorem 6 (Covariance concentration, Theorem 6.5 in Wainwright (2019)). Denote Σ =
E Qi.QTi,, where Qi ∈ Rd is from a σQ -Sub-Gaussian random distribution Q. Then, there are
universal constants ci, c? and c3 such that, for any set {Qi}i∈[i] of i.i.d. samples from Q, and any
δ > 0, the sample Covariance Σ = ∣ P Qi QT satisfies the bound
P
σ2 Bς - HL≥ c1 (VI+1)+ δ
≤ c2 exp -c3Imin(δ, δ2) .
(91)
Theorem 7 (Weyl’s Theorem, Theorem 4.3.1 in Horn & Johnson (2012)). Let A and B be Hermi-
tian4 and let the respective eigenvalues ofA and B and A+ B be {λi(A)}id=1, {λi(B)}id=1, and
4For real matrices, Hermitian is the same as symmetric.
31
Under review as a conference paper at ICLR 2022
{λi(A + B)}id=1, each increasingly ordered. Then
λi(A + B) ≤ λi+j (A) + λd-j (B),	j = 0, 1, ..., d - i,	(92)
and
λi+j (A) + λj+1(B) ≤ λi(A + B), j = 0, ..., i - 1,	(93)
for each i = 1, ..., d.
Lemma 21. Consider two symmetric definite positive matrices S and Σ. Denote ρmin and λmin
their minimal eigenvalues. Then ∣Pmin 一 λmin∣ ≤ ∣∣S 一 夕|卜.
Proof. This is a direct consequence of Theorem 7, for A = S, B = Σ — S, i = 1, and j = 0.	口
Corollary 1. There are universal constants c1, c2 and c3 such that, for any σQ-sub-Gaussianvector
distribution Q ∈ Rd and any δ > 0, the sample covariance Σ = I P QiQT satisfies the bound
P
σ2~ IminSP(立 一 minSP(订j ≥ ci (ri+i) +δ
≤ c2 exp (—c3Imin(δ, δ2)),
(94)
1	♦ Cf∕S∖	1	♦ Cf∕u∖	. 1	∙ ∙	1	1	r S 1 ∖ '
where min SP(Σ) and min S P(Σ) are the minimal eigenvalues ofΣ and Σ.
Proof. This follows from Theorem 6 and Lemma 21.
□
Lemma 22. With high probability, mm Sp(Σ) ≥ min Sp(Σ)∕2.
Proof. Denote λmin，min Sp(Σ) and bmin，min Sp(Σ). Since each Qi is drawn i.i.d. from a
σQ -sub-Gaussian, we can apply Corollary 1. Namely, there are constants c1 , c2 and c3 , such that for
any δ > 0, we have
≤ c2 exp (—c3Imin {δ, δ2 }).
(95)
We now set δ , λmin∕(4σQ2 ) and we consider i large enough so that ci(,I +1) ≤
λmin/(4σQ). With high probability, We then have bmin ≥ λmin∕2.	口
D.2 Linear regression is gradient-PAC *
In this section, we prove the first part of Lemma 4. Namely, we prove that linear regression is
gradient-PAC* learning.
D.2.1 Lemmas for linear regression
Before moving to the main proof that linear regression is gradient-PAC*, we first prove a few useful
lemmas. These lemmas will rest on the following well-known theorems.
Theorem 8 (Lemma 2.7.7 in Vershynin (2018)). IfX and Y are sub-Gaussian, then XY is sub-
exponential.
Theorem 9 (Equation 2.18 in Wainwright (2019)). IfX1, . . . , XI are iid sub-exponential variables,
then there exist constants c4, c5 such that, for all i, we have
∀t ∈ [0,c4], P [|X 一 E [X]| ≥ tI] ≤ 2exp (-c5It2) .	(96)
Lemma 23. For all j ∈ [d], the random variables Xi , ξi Qi[j] are iid, sub-exponential and have
zero mean.
Proof. The fact that these variables are iid follows straightforwardly from the fact that the noises
ξi are iid, and the queries Qi are also iid. Moreover, both are sub-Gaussian, and by Theorem 8,
the product of sub-Gaussian variables is sub-exponential. Finally, we have E [X] = E [ξQ[j]] =
E [ξ] E [Q[j]] = 0, using the independence of the noise and the query, and the fact that noises have
zero mean (E [ξ] = 0).	口
32
Under review as a conference paper at ICLR 2022
Lemma 24. There exists B such that Pi∈I ξiQi 2 ≤ BI3/4 with high probability.
Proof. By Lemma 23, the terms ξi Qi [j] are iid, sub-exponential and have zero mean. Therefore, by
Theorem 9, there exist constants c4 and c5 such that for any coordinate j ∈ [d] of ξi Qi and for all
0 ≤ u ≤ c4, we have
P	ξiQi[j] ≥ Iu ≤ 2exp (-c5Iu2).	(97)
i∈I
Plugging u = vI(-1/4) into the inequality for some small enough constant v, and using union bound
then yields
Pl 卢 ξiQi	≥i⑶/4)VC	≤ P
y~^ξiQi	≥ I(3/4)v ≤ 2dexp(-c5√Iv2).
(98)
Defining B，v√d yields the lemma.
□
D.2.2 Proof that linear regression is gradient- PAC *
We now move on to proving that least square linear regression is gradient-PAC*.
ProofofTheorem 2. Note that Vθ'(θ, Q, A) = (θTQ - A)Q. Thus, on input i ∈ [I], We have
Vθ '(θ, Qi, A(Qi ,θt)) = ((θ - θt)T Qi) Qi - ξiQi.
Moreover, We have
(θ - θ↑)τVθ (V kθ∣∣2) = 2ν(θ - θt)Tθ = 2ν ∣∣θ - θt∣∣2 + 2ν(θ - θt)Tθt.
As a result, We have
(θ - θt)TVθL(Θ, D)
I(θ - θt)TΣ(θ - θt)
(99)
(100)
(101)
(102)
-(θ-θt)T XξiQi	+2ν∣∣θ-θt∣∣22+2ν(θ-θt)Tθt.
But now, with high probability, we have (θ-θt)TΣ(θ -θt) ≥ (λmin∕2) ∣∣θ - θt∣∣2 (Lemma 22) and
∣∣Pi∈I ξiQi ∣∣2 ≤ BI(3/4) (Lemma 24). Using the fact that ∣∣θt ∣∣2 ≤ K and the Cauchy-SchWarz
inequality, we have
(θ - θt)TVθL(θ, D) ≥ (λm2inI + V) ∣∣θ - θt∣∣2 - (BI(3/4) + 2νK) ∣∣θ - θt∣∣2.	(103)
Denoting AK，λm2in and BK，B + 2νK and using the fact that I ≥ 1, we then have
(θ - θt)TVθL(θ, D) ≥ AKI∣∣θ - θt∣∣22 - BKI(3/4) ∣∣θ - θt∣∣2	(104)
≥AKIminn∣∣θ-θt∣∣2, ∣∣θ - θt∣∣22o - BKI(3/4) ∣∣θ - θt∣∣2,	(105)
with high probability. This corresponds to saying Assumption 2 is satisfied for α = 3/4.	□
D.3 Logistic regression
In this section, we now prove the second part of Lemma 4. Namely, we prove that logistic regression
is gradient-PAC* learning.
33
Under review as a conference paper at ICLR 2022
D.3.1 Lemmas about the sigmoid function
We first prove two useful lemmas about the following logistic distance function.
Definition 6. We define the logistic distance function by ∆(a, b) , (a - b) (σ(a) - σ(b)).
Lemma 25. If a, b ∈ R such that for some k > 0, |a| ≤ k and |b| ≤ k, then there exists some
constant ck > 0 such that
∆(a, b) ≥ ck |a - b|2 .	(106)
Proof. Note that the derivative of σ(z) is strictly positive, symmetric (σ0 (z) = σ0(-z)) and mono-
tonically decreasing for z ≥ 0. Therefore, for any z ∈ [-k, k], we know σ0 (z) ≥ ck , σ0(k). Thus,
by the mean value theorem, we have
σ(a) - σ(b)
≥	≥ Ck.	(I07)
a-b
Multiplying both sides by (a - b)2 then yields the lemma.	□
Lemma 26. If b ∈ R, and |b| ≤ k, for some k > 0, then there exists a constant dk, such that for any
a ∈ R, we have
∆(a, b)≥	dk |a- b| -dk	(108)
Proof. Assume |a - b| ≥ 1 and define dk，σ(k + 1) - σ(k). If b ≥ 0, since σ0(z) is decreasing
for z≥	0, we have σ(b) - σ(b - 1)≥	σ(b + 1) - σ(b) ≥	dk, and by symmetry, a similar argument
holds for b ≤ 0. Thus, we have
∣σ(a) — σ(b)∣ ≥ min{σ(b) — σ(b — 1),σ(b + 1) — σ(b)} ≥ dk.	(109)
Therefore,
(a - b) (σ(a) - σ(b)) ≥ dk |a - b| ≥ dk |a - b| - dk.	(110)
For the case of |a - b| ≤ 1, We also have (a - b)(σ(a) — σ(b)) ≥ 0 ≥ dk |a - b| - dk.	□
D.3.2 A uniform lower bound
Definition 7. Denote SdT，{u ∈ Rdl |间卜=1} the hypersphere in Rd.
Lemma 27. Assume SUPP(Q) spans Rd. Then,forall U ∈ Sd-1, E [∣Qτu∣] > 0.
Proof. Let U ∈ Sd-1. We know that there exists Qi,..., Qd ∈ SUPP(Q) and aι,...,αd ∈ R
such that u is colinear With P αj Qj . In particular, We then have uT P αj Qj = Pαj (QjTU) 6= 0.
Therefore, there must be a query Q* ∈ SUPP(Q) such that QTu = 0, which implies a，∣QTu∣ >
0 By continuity of the scalar product, there must then also exist ε > 0 such that, for any Q ∈
B(Q*, ε), we have ∣QTU∣ ≥ a/2, where B(Q*, ε) is an Euclidean ball centered on Q* and of radius
ε.
But now, by definition of the support, we know that p , P [Q ∈ B(Q*, ε)] > 0. By the law of total
expectation, we then have
E∣∣QTU∣∣] =E∣∣QTU∣∣ ∣∣ Q ∈ B(Q*, ε)] P [Q ∈ B(Q*, ε)]
+ E ∣∣QT U∣∣ ∣∣ Q ∈/ B(Q*, ε)] P [Q ∈/ B(Q*,ε)]	(111)
≥ ap/2 + 0 > 0,	(112)
which is the lemma.	□
Lemma 28. Assume that,for all unit vectors U ∈ Sd-i, we have E [∣QT u∣] > 0, and that SUPP(Q)
is bounded by MQ . Then there exists C > 0 such that, with high probability,
∀U ∈ Sd-1, X ∣∣QiTU∣∣ ≥ CI.	(113)
i∈I
34
Under review as a conference paper at ICLR 2022
Proof. By continuity of the scalar product and the expectation operator, and by compactness of
Sd-1, we know that
C0 , inf E QTu > 0.	(114)
u∈Rd
Now define ε , C0/4MQ. Note that Sd-1 ⊂ Su∈Sd-1 B(u, ε). Thus we have a covering of
the hypersphere by open sets. But since Sd-1 is compact, we know that we can extract a finite
covering. In other words, there exists a finite subset S ⊂ Sd-1 such that Sd-1 ⊂ Su∈S B(u, ε). Put
differently, for any v ∈ Sd-1, there exists u ∈ S such that ku - vk2 ≤ ε.
Now consider U ∈ S. Given that Supp(QQ) is bounded, we know that ∣ QTUl ∈ [0,Mq]. Moreover,
such variables QiT u are iid. By Hoeffding’s inequality, for any t > 0, we have
∣∣QiTU∣∣ -IE ∣∣QT U∣∣
i∈I
Choosing t = C0 /2 then yields
]X∣QT u∣≤ CI
≥ It
≤ 2 exp
(-2It2∖
M~q^γ)
≤ P EIQTUθ-θt∣ - IE [∣QT Uθ-θt∣]
∣ i∈I
2 exp
(-IC02 A
2mQ^)
IC0
≥ —
_ 2
(115)
(116)
(117)
P
P
≤
Taking a union bound for U ∈ S then guarantees
L ∈ S,χ∣QT u∣≥ C0I] ≥1 - 2 …(-ICQ2),
(118)
which clearly goes to 1 as I → ∞. ThUs ∀u ∈ S, Pii∈ι ∣ QTU ∣ ≥ C0I holds with high probability.
Now consider v ∈ Sd-1. We know that there exists U ∈ S such that kU - vk2 ≤ ε. Then, we have
X ∣∣QiTv∣∣ = X ∣∣QiTU+QiT(v-U)∣∣	(119)
i∈[I]	i∈[I]
≥ X ∣∣QiTU∣∣ -IMQ kv -Uk2	(120)
i∈[I]
C0I	C0 C0I	fl9n
≥ F -IMQ 4MQ = 丁，	(⑵)
which proves the lemma.	□
D.3.3 Lower bound on the discrepancy between preferred and reported
ANSWERS
Lemma 29. Assume that Q has a bounded support, whose interior contains the origin. Suppose
also that ∣∣θ"∣2 ≤ K∙ Then there exists AK such that, with high probability, we have
X ∆(QTθ, QTθt) ≥ AKImin n∣∣θ - θt∣∣2 ,∣∣θ - θt∣∣20 .	(122)
i∈[I]
Proof. Note that by Cauchy-Schwarz inequality we have
∣∣QiT θt∣∣ ≤ kQik2 ∣∣θt∣∣2 ≤MQK.	(123)
Thus, Lemma 26 implies the existence of a positive constant dK, such that for all θ ∈ Rd, we have
X∆ (QTθ, QTθt) ≥ X(dκ∣QTθ -QTθt∣ - dκ)	(124)
=-dκI + dκ ∣∣θ - θt∣∣2 X∣QTUθ-θt ∣,	(125)
i∈I
35
Under review as a conference paper at ICLR 2022
where ug-θt，(θ - θt)/ ∣∣θ - θt∣∣2 is the unit vector in the direction of θ - θl
Now, by Lemma 28, we know that, with high probability, for all unit vectors u ∈ Sd-1, we have
PQiTu ≥ CI. Thus, for I sufficiently large, for any θ ∈ Rd, with high probability, we have
X∆(QTθ, QTθt) ≥ dCminI∣∣θ - θt∣∣2 - dκI.	(126)
i∈I	2
Defining eκ，dKCmin, and fκ，^4-, for ∣∣θ - θt b > fκ, We then have
X∆(QiT θ, QiT θt) ≥eKI∣∣θ-θt∣∣2.	(127)
i∈I
We now focus on the case of ∣∣θ - θt ∣∣2 ≤ fK. The triangle inequality yields kθk2 ≤ ∣∣θ - θt ∣∣2 +
∣∣θt ∣∣2 ≤ fK + K. By Cauchy-Schwarz inequality, we then have QiT θ ≤ (fK + K)MQ , gK and
QiTθt ≤KMQ ≤gK.Thus, by Lemma 25, we know there exists some constant cK such that
X(σ(QTθ)-σ(QTθt)) (QTθ -QTθt) ≥ XCKlQTθ -QTθt∣2	(128)
i∈I	i∈I
=XcK(θ-θt)TQiQiT(θ-θt)	(129)
i∈I
= cK(θ - θt)T XQiQiT (θ - θt).	(130)
Since distribution Q is bounded (and thus sub-Gaussian), by Theorem 6, with high probability, we
have
(θ-θt)T (XQiQT) (θ-θt) ≥ 浮i∣∣θ-θt∣∣2,	(131)
where λmin is the smallest eigenvalue ofE QiQiT . Thus, for ∣θ - θt ∣2 ≤ fK, we have
X S(QTθ) - σ(QTθt)) (Qiθ -QTθt) ≥ λmncKI∣∣θ - θt∣∣2 .	(132)
i∈I	2
Combining this with (127), and defining AK，min { λmincK ,eκ }, we then obtain the lemma. □
D.3.4 Proof that logistic regression is gradient-PAC *
Now we proceed with the proof that logistic regression is gradient-PAC*.
Proof of Theorem 3. Note that σ(-z) = e-z σ(z) = 1 - σ(z) and σ0 (z) = e-zσ2 (z). We then have
Vθ'(θ, Q, A) = - σ0(AQTT)AQ = -e-AQTθσ(AQTθ)AQ	(133)
σ(AQT θ)
=-σ(-AQTθ)AQ = (σ(QTθ) - 1 [A = 1]) Q,	(134)
36
Under review as a conference paper at ICLR 2022
(θ -
where 1 [A = 1] is the indicator function that outputs 1 if A = 1, and 0 otherwise. As a result,
(θ - θt)TVθL(θ,D)=	(135)
+ 2ν (θ — θt)T θ	(136)
=(θ -θt)T (X (σ(QTθ) — σ(QTθt) + σ(QTθt) - 1 [Ai = 1]) Q[	(137)
+ 2ν θ - θt22 + 2ν(θ - θt)Tθt	(138)
=X ∆ (QTθ,	QTθt)	+	(θ	- θt)T	(X	(σ(QTθt)	-	1 [Ai = 1])	Qi)	(139)
i∈[I]	i∈I
+2νθ - θt22 +2ν(θ- θt)Tθt.	(140)
By Lemma 29, with high probability, we have
X ∆ (qT θ, QT θt) ≥ AKI min {∣∣θ - θt∣∣2 , ∣∣θ-θt∣∣2} .	(141)
i∈[I]
To control the second term of (139), note that the random vectors Zi，(σ(QTθt) 一 1 [Ai = 1]) Qi
are iid with norm at most MQ. Moreover, since E [1 [Ai = 1] |Qi] = σ(QTθt), by the tower rule,
we have E [Zi] = E [E [Zi|Qi]] = 0. Therefore, by applying Hoeffding’s bound to every coordinate
of Zi, and then taking a union bound, for any B > 0, we have
P ∣∣∣X Zi ∣∣∣ ≥ BI3/4
∣ i∈I	∣2
Applying now Cauchy-Schwarz inequality, with high probability, we have
(θ - θt)T (X (σ(QTθt) - IAi = 1]) Qi)I ≤ BI3/4 ∣∣θ - θt∣∣2 .
Combining this with (132) and using ∣∣θt ∣∣22 ≤ K, we then have
(θ - θt)TVθL(θ, D)	(143)
≥ (AKI + ν) n∣∣θ - θt∣∣2, ∣∣θ - θt∣∣22o - (BI(3/4) + 2νK) ∣∣θ - θt∣∣2	(144)
≥ AKIn∣∣θ - θt∣∣2, ∣∣θ - θt∣∣22o- BKI(3/4) ∣∣θ - θt∣∣2,	(145)
where BK = B + 2νK. This shows that Assumption 2 is satisfied for logistic loss for α = 3/4, and
AK and BK as previously defined.
□
≤ 2d exp -
(142)
E Proofs of local PAC * -learnab ility
Let us now prove Lemma 5. To do so, consider the preferred models θ~t and a subset H ⊂ [N]
of honest users. Denote D~-H the datasets provided by users n ∈ [N] - H. Each honest user
h ∈ H provides an honest dataset Dh of cardinality at least I ≥ 1. Consider the bound KH ,
maxh∈H ∣∣θht ∣∣ on the parameter norm of honest active users h ∈ H.
E.1 Bounds on the optima
Before proving the theorem, we prove a useful lemma that bounds the set of possible values for the
global model and honest local models.
37
Under review as a conference paper at ICLR 2022
Lemma 30. Assume that R and ` are nonnegative. For I large enough, if all honest active nodes
h ∈ Hprovide at least I data, then, with high probability, ~H must lie in a compact subset of Rd×H
that does not depend on I.
Proof. Denote L0，Loss(0, (%, 0-h), (0, DD-H)). Essentially, We will show that, if ~H is too
far from ~H, then the loss will take values strictly larger than L0.
Assumption 2 implies the existence of an event E that occurs with probability at least P0 ,
P(KH, I)|H|, under which, for any θh ∈ Rd, we have
(θh - θh) T VLh (θh) ≥ AKHI min {∣∣θh - θh∣∣2 J∣θh - θh∣21 - BKH 工。|"% — 0h||2，("6)
which implies
uTθh-θt)VLh (θh) ≥ AKHI min {l,∣∣θh - θh∣∣J - BKH T α∙	(147)
Note also that P0 → 1 as I → ∞. We now integrate both sides over the line segment from θhh to θh
The fundamental theorem of calculus for line integrals then yields
Lh (θh) - Lh (θh) = M - ®h ∣∣ JINh q )VL (θh+t仞-θh)) dt
≥ ∣∣θh - θh∣∣2 Zt1O (AKH I min {* W - θhHJ - BKH 工)选
1
= ∣∣θh - θh∣∣2 /=0 (AKH I min {ι,t ∣∣θh - θh∣∣2} d - - BKH 工α M - "h∣∣2
Now, if ∣∣θh - θh∣∣ > 2, we then have
Lh (θh) - Lh (θh) ≥ (AK^ - Bkh i α) M - θh∣∣2
≥ AKHI - 2BKHIα .
Now for I > Ii，max 2L°/^Khh, (4Bkh∕Akh ) 1-ɑ }, we have
Lh (θh)-Lh (θh) >L0∙
This implies that if ∣∣θh - θhh ∣∣ > 2 for any h ∈ H, then we have
LOSS(0, (~H, 0-h), DD) < LOSS(P, (~H, θ-H), DD),
(148)
(149)
(150)
(151)
(152)
(153)
(154)
regardless of ρ and θ-H. Therefore, we must have
bounded closed subset ofRd×H, which is thus compact.
2. Such inequalities describe a
□
Lemma 31. Assume that R(ρ,θ) → ∞ as ∣∣ρ — θ∣b → ∞, and that ∣∣θ[ — θ力 ∣∣ ≤ 2 forall honest
users h ∈ H. Then ρ* must lie in a compact subset of Rd that does not depend on I.
Proof. Consider an honest user h0. Given our assumption on R → ∞, we know that there exists
DKH such that if ∣ρ - θh ∣∣2 ≥ DKH ,then R(ρ, θh) ≥ L0 + 1. Thus any global optimum ρ* must
SatiSfy ∣∣ρ* - θh 0∣2 ≤ kρ* - θ仙 2 + ∣∣θ* - θh, ∣∣2 ≤ DKH +2.	□
38
Under review as a conference paper at ICLR 2022
E.2 Proof of Lemma 5
ProofofLemma 5. Fix ε,δ > 0. We want to show the existence of some value of I(ε, δ, D~-H,前)
that will guarantee (ε, δ)-locally PAC* learning for honest users.
By lemmas 30 and 31, we know that the set C of possible values for (ρ*,θH) is compact. Now, we
define
EKH，,maχ∕M R(ρ,θ)k2
(ρ,θ)∈C
(155)
the maximum of the norm of achievable gradients at the optimum. We know this maximum exists
since C is compact.
Using the optimality of (ρ , θ*), for all h ∈ H, we have
0 ∈ (θh -θh)TVθhLOSS(P*,~*)
=(θh -θh)TVLh(θ：) + (θh -θh)TVθhR(ρ*,θh)
≥ (θh - θh )T vLh(θi)- ∣∣θ: - θh( Nθh R",θh )∣∣2
≥ (θh -θh)TVLh(θl) - EKHM - θh∣∣2.
(156)
(157)
(158)
(159)
We now apply assumption 2 for θ = θ力(for h ∈ H). Thus, there exists some other event E0 with
probability at least P0, under which, for all h ∈ H, we have
0 ≥ AKh I min { M -矶2 , M -矶 2} - BKh I。||优-矶2 - EKHM-研2 . (岫
Now if I >
M -θh B2 ≥
I2 ， max ∣2Ekh /AKh , (2BKH /AKh ) 1-α } this inequality cannot hold for
1. Therefore, for I > I2, we have ∣∣θ^ - θh∣∣ < 1, and thus,
0 ≥ akh I M - θh∣∣2 - bkh Iα || 隽-叫2 - EKH M -研2
(161)
and thus,
M - θh∣∣2 ≤ BKHBEKH
H
(162)
Now note that P [E∧E0] = 1 - P [-E∨ -E0] ≥ 1 - P [-E] - P [-E0] = 2P0 - 1. It now suffices
to consider I larger than 工2 and large enough so that P(KH, I)|H| ≥ 1 - δ∕2 (whose existence is
guaranteed by Assumption 2, and which guarantees 2P0 - 1 ≥ 1 - δ) and so that
to obtain the theorem.
BKHIα+EκH ≤ ε
AKHI	一
□
F CONVERGENCE OF CGA AGAINST `22
To write our proof, we define LOSSρ-s : Rd → R by
LOSSρ-s(ρ) , inf nLOSS(ρ, θ~, D~) - Ls(θs,Ds) - R(ρ,θs)o
=infXL(θn,Dn)+λXkρ-θnk22.
n6=s	n6=s
(163)
(164)
In other words, it is the loss when local models are optimized, and when the data of strategic user s
are removed.
Lemma 32. Assuming `22 regularization and convex loss-per-input functions `, for any datasets D~,
LOSS is strongly convex. As a result, so is LOSSρ-s.
Proof. Note that the global loss can be written as a sum of convex function, and of ν Pkθnk22 +
ρ - θ1k22. Using tricks similar to the proof of Lemma 11, we see that the loss is strongly convex.
The latter part of the lemma is then a straightforward application of Lemma 10.	□
39
Under review as a conference paper at ICLR 2022
We now move on to the proof of Theorem 4. Note that our statement of the proof was not fully
explicit, especially about the upper bound on the constant learning rate η. Here, we prove that it
holds for ηt = η ≤ 1/3L, where L is a constant such that LOSSρ-s is L-smooth. The existence of L
is guaranteed by Lemma 13.
Proof of Theorem 4. Note that by Lemma 9, LOSSρ-s is convex, differentiable and L-smooth, and
▽Loss-s (Pt) = g-S. For '2 regularization, We have GRAD(P) = Rd for all P ∈ Rd. Then the
minimum of equation 6 is zero, which is obtained when gis，P -θ -g-§ = gt-1 + P -θs + Pt-Pt，.
Note that
ρt+1 =	ρt	- ηg-t	- ηgSt	(165)
=ρt	- ηg--S	- (ρt - θS)	+ (PtT- ρt)	- ηgt-1	(166)
=θS - ηt(g-,S + gS-1) + η(g-,t-1 + gts-1)	(167)
=θS- η(g-t- g-S-1).	(168)
Therefore, ρt+1 - ρt = η(g-S - g-t-1) - η(g-S-1 - g-S-2).
Then, using the L-smoothness of LOssP-s, and denoting ut , Pt+1 - Pt 2, we have ut+1 ≤
LntUt + Lηt-ιut-ι. Now assume that η ≤ 1/3L. Then ut+ι ≤ 3(Ut + ut-ι). We then know that
ut+2 ≤ 3(Ut+1 + Ut) ≤ 3 (3 (Ut + Ut-I) + Ut) = 4Ut + 1 ut-1.
Now	define Vt ，Ut +	Ut-ι. We then have vt+2 ≤ Ut+2 + Ut+1 ≤ 9Ut + 4Ut-ι	≤
9(Ut	+ Ut-ι) ≤	9Vt.	By induction, we know that	Vt ≤ (7/9)(t-1)/2 max{v0,vj	≤
(√7∕3)t ((√7∕3)max {v0,vι}). Thus, defining α，√7∕3 < 1, there exists C > 0 such that
Ut ≤ Vt ≤ Cαt. This implies that P Pt+1 - Pt 2 ≤ P Cαt < ∞. Thus P(Pt+1 - Pt) con-
verges, which implies the convergence of ρt to a limit ρ∞. By L-smoothness, we know that g-S
must converge too. Taking equation 168 to the limit then implies ρ∞ = θ1. This shows that the
strategic user achieves precisely what they want with CGA. It is thus optimal.	□
G CGA ON MNIsT
In this section, CGA is executed against 10 honest users, each one having 6,000 randomly and data
points of MNIsT, drawn randomly and independently. CGA is run by a strategic user whose target
model θS labels 0's as 1‘s, 1‘s as 2's, and so on, until 9's as 0's. We learn θS by relabeling the
MNIsT training dataset and learning from the relabeled data. We use λ = 1, Adam optimizer and a
decreasing learning rate.
5 0 5 0 1n
3 3 2 2 1
UUoU
25	50	75	100	125	150	175	200
Epochs
(a) Using `22
10
5
5 0 5 0 1n
3 3 2 2 1
UUoU
10-	/
/	——12 dist
5 -	/	----12 norm
?	.... target-dist
0』，	，	，	，	，	，	，	，一，
O 25	50	75 IOO 125	150	175	200
Epochs
(b) Using `2
Figure 16:	Norm of global model, distance to initialisation and distance to target, under attack by
CGA. In particular, we see that the attack against `22 is successful, as the distance between the global
model and the target model goes to zero.
40
Under review as a conference paper at ICLR 2022
H Single data poisoning for least square linear regression
Proof of Theorem 5. We define the minimized loss with respect to ρ and without strategic user s by
LOSS-s(ρ, DD-S) ,	min	IX Ln®, Dn) + X λ ∣∣θn - ρk2 ∖ .	(169)
~-s∈Rd×(N-ι) ιn=s	n=s
Now consider a subgradient g ∈ NPLOSS-s(θg, D-S) of the minimized loss at θS. For X，-g,
then have -g ∈ V (λ ∣∣χ∣∣2). We then define θ,，θg - x.
0 = g - g ∈ VPLOSS-s(θS, D-S) + Vρ (λ ∣∣θ* - θ!∣∣2)	(170)
=VPLOSSS(θS, θ-s(θ*, DD-s),θ*, D-S),	(171)
where LOSSS is defined by (38). Now consider the data point (Q, A) = (g,gTθ2 - 1). For
DS = {(Q, A)},we then have VLs(Θ2, DS) = g, which implies
VθsLOSS(θt, (θ*,~-s(Θ*, DD-S), D) = 0.	(172)
Combining it all together with the uniqueness of the solution then yields
argmin {LOSS(ρ,~,D)} = (θj,付,小讨,D-S))),
(P,θ~)
(173)
which is what we wanted.	□
I Data poisoning against linear classification
I.1	Generating efficient poisoning data
For every label a ∈ {1,..., 9}, we define y&，θ, — θ*, and Ca，-(θ00 一 θ^) (where θ% is the
bias of the linear classifier). The indifference subspace V is then the set of images Q ∈ Rd such
that QTya = ca for all a ∈ {1, . . . , 9}.
To project any image X ∈ Rd on V , let us first construct an orthogonal basis of the vector space
orthogonal to V , using the Gram-Schmidt algorithm. Namely, we first define z1 , y1. Then, for
any answer a ∈ {1, . . . , 9}, we define
za , ya
XT	zb
ya zb μ^^2..
b<a	kzbk2
(174)
It is easy to check that for b < a, we have zaT zb = 0. Moreover, if Q ∈ V , then
zaT Q = yaT Q -
b<a
(yT Zb)(ZT Q)
kzbk2
ca
X (yT Zb)(ZT Q)
b<α	kzbk2
(175)
By induction, we see that ZaTQ is a constant independent from Q. Indeed, for a = 1, this is clear
as Z1TQ = y1TQ = c1. Moreover, for a > 1, then, in the computation of ZaTQ, Q always appear as
ZbTQ for b < a. Moreover, denoting c0a the constant such that ZaTQ = c0a for all a ∈ {1, . . . 9}, we
see that these constants can be computed by
c0a
T
C - X y^L C0
Ca	乙 II I∣2 Cb
b<a kZb k2
(176)
Finally, we can simply perform repeated projection onto the hyperplanes where a is equally probable
as the answer 0. To do this, we first define the orthogonal projection P(X, y, C) of X ∈ Rd on the
hyperplane xTy = C, which is given by
P(X,y,c)= X - (Xty - c)祢.	(177)
kyk2
41
Under review as a conference paper at ICLR 2022
It is straightforward to verify that P (X,y,c)Ty = c and that P(P(X,y, c), y, c) = P(X,y, c). We
then canonically define repeated projection by induction, as
P(X, (y1, . . . , yk+1), (c1, . . . , ck+1)) , P(P(X, (y1, . . . ,yk), (c1, . . . , ck)), yk+1, ck+1). (178)
Now consider any image X ∈ Rd . Its projection can be obtained by setting
Q , P(X,(z1,...,z9),(c01,...c09))+ξ.	(179)
Note that to avoid being exactly on the boundary, and thus retrieve information about the scales of
θ. and on which side of the boundary favors which label, We add a small noise ξ, to make sure Q
does not lie exactly on V (which would lead to multiple solutions for the learning), but small enough
so that the probabilities of the different label remain close to 0.1 (the equiprobable probability).
We acknowledge that images obtained this way may not be in [0, 1]d, like the images of the MNIST
dataset. In general, one could search for points Q ∈ V ∩ [0, 1]d. Note that in theory, by Theorem 3
(or a generalization of it), labeling random images in [0, 1]d should suffice. However, in the case
where V ∩ [0,1]d is empty (typically if no image in [0,1] is argued by model θ? as realistically a 9),
this procedure may require the labeling of significantly more images to be successful.
I.2	A brief theory of data poisoning for linear classification
Using the efficient poisoning data fabrication, we thus have a set of images (Q, p(Q)), wherepa(Q)
is the probability assigned to image Q and label a. This defines the following local loss for the
strategic node:
Ls(θs, Ds) =	X	X	pa(Q)lnσa(θs,Q),	(180)
(Q,p(Q))∈Ds a∈{0,1,...,9}
where σo(θs, Q) = PxppsaTQQ+"；))is the probability that image Q has label a, according to the
model θs. We acknowledge tshat such labelings of queries is unusual. Evidently, in practice, an image
may be labeled N times, and the number of labels Na it received can be set to be approximately
Na ≈ Npa(Q).
It is noteworthy that the gradient of the loss function is then given by
(θs-θ,)T VθsLs(θs, Ds)= X X	(σa(θs, Q) - σa(θ*, Q))他” -θ*)T Q+,
Q∈Dsa∈{0,1,...,9}
(181)
where we defined Q+ , (1, Q) (which allows to factor in the bias of the model. This shows
that Vθs Ls (θs, Ds) points systematically away from θ?, and thus that gradient descent will move
towards θ’.
In fact, if the set of images Q cover all dimensions (which occurs if there are Ω(d) images, which is
the case for 2,000 images, since d = 784), then gradient descent will always move the model in the
direction of θ?, which will be the minimum. Moreover, by overweighting each data (Q,p(Q)) by
a factor α (as though the image Q was labeled α times), we can guarantee gradient-PAC* learning,
which means that we will have θ* ≈ θ?, even in the personalized federated learning framework.
This shows why data poisoning should work in theory, with relatively few data injections.
Note that the number of other users does make learning harder. Indeed, the gradient of the regu-
larization R(ρ,θs) at P = θg and θs = θ? is equal to 2λ ∣∣θj 一 θ?b.AS the number N _ 1 of
other users grows, we should expect this distance to grow roughly proportionally to N. In order to
make strategic user S robustly learn θ?, the norm of the gradient of the local loss Ls at θS must be
vastly larger than 2λ ∣∣θg - θ? ∣∣2∙ ThiS means that the value of a (or, equivalently, the number of
data injected in Ds ) must also grow proportionally to N.
I.3	Initialization of the learning algorithm
The convergence to the optimum is slow. But given that the problem is convex, we focus here
mostly on showing that the minimum is indeed a poisoned model. To boost the convergence, we
initialize our learning algorithm at a point close to what we expect to be the minimum, by taking
this minimum and adding a Gaussian noise, and then we observe the convergence to this minimum.
42
Under review as a conference paper at ICLR 2022
J	Cifar- 1 0 on VGG 1 3-BN experiments
We considered VGG 13-BN, which was pretrained on cifar-10 by Phan (2021). We now assume that
10 nodes are given part of the cifar-10 database, while a strategic node also joins to the personalized
federated gradient descent algorithm. The strategic node’s goal is to bias the global model towards
a target model, which misclassifies the cifar-10 data, by reclassifying 0 into 1, 1 into 2... and 9 into
0.
J.1 Counter-gradient attack
We first show the result of performing counter-gradient attack on the last layer of the neural network.
Essentially, images are now reduced to their vector embedding, and the last layer performs a simple
linear classification akin to the case of MNIST (see Appendix G).

Gradient attack
1.0 η------------------------------------
0.8-
UUoU-
0.2
.-----acc_glob
0.0 j-I~~I~~I~~I~~I~~I~~I~~I~~I~~I~~I~~I~~I~~I~~I~~I~~I~~I~~I~~I~~I—
0	200	400	600	800	1000
Epochs
(a)	Accuracy according to attacker’s objective
Gradient attack
40-
35-
30-
25-
20-
15-
10-
5-
0-
0	200	400	600	800	1000
Epochs
(b)	Distances
----honest-dist
----12 norm
________
....target_dist
----attackdist
Figure 17:	CGA on cifar-10.
J.2 Reconstructing a model attack
Reconstructing an attack model whose effect is equivalent to the counter-gradient attack is identical
to what was done in the case of MNIST (see Section 5.2).
(a) Accuracy according to attacker’s objective
----honest-dist
----12 norm
________
....target_dist
----attackdist
Model attack
40-
35-
30-
25-
20-
15-
10-
5-
0-
0	100	200	300	400	500
Epochs
(b) Distances
Figure 18:	Model attack on cifar-10.
J.3 Reconstructing data poisoning
This last step is however nontrivial. On one hand, we could simply use the attack model to label a
large number of random images. However, this solution would likely require a large sample com-
43
Under review as a conference paper at ICLR 2022
plexity. For a more efficient data poisoning, we can construct vector embeddings on the indifference
affine subspace V , as was done for MNIST in Section 5.3. This is what is shown below.

40
Data attack
1.0 η-----------------------------------
0.2
.-----acc_glob
0,°	0	200	400	600	800	1000 1200 1400
Epochs
(a)	Accuracy according to attacker’s objective
5 0 5 0 1n
3 3 2 2 1
UUoU Z-
Data attack
----honest-dist
----12 norm
________
....target_dist
10-
5-
0-
0	200	400	600	800	1000 1200 1400
Epochs
(b)	Distances
Figure 19:	Data poisoning on cifar-10.
We acknowledge however that this does not quite correspond to data poisoning, as it requires re-
porting a vector embedding and its label, rather than an actual image and its label. The challenge
is then to reconstruct an image that has a given vector embedding. We note that, while this is not a
straightforward task in general, this has been shown to be at least somewhat possible for some neural
networks, especially when they are designed to be interpretable (Zeiler & Fergus, 2014; Wang et al.,
2019c; Mai et al., 2019).
44