Under review as a conference paper at ICLR 2022
On Reward Maximization and Distribution
Matching for Fine-Tuning Language Models
Anonymous authors
Paper under double-blind review
Ab stract
The availability of large pre-trained models is changing the landscape of Machine
Learning research and practice, moving from a “training from scratch” to a “fine-
tuning” paradigm. While in some applications the goal is to “nudge” the pre-trained
distribution towards preferred outputs, in others it is to steer it towards a different
distribution over the sample space. Two main paradigms have emerged to tackle this
challenge: Reward Maximization (RM) and, more recently, Distribution Matching
(DM). RM applies standard Reinforcement Learning (RL) techniques, such as
Policy Gradients, to gradually increase the reward signal. DM prescribes to first
make explicit the target distribution that the model is fine-tuned to approximate.
Here we explore the intimate connections between the two paradigms, and show that
methods such as KL-control developed in the RM paradigm can also be construed
as belonging to DM. We further observe that while DM differs from RM, it can
suffer from similar training difficulties, such as high gradient variance. We leverage
connections between the two paradigms to import the concept of baseline into DM
methods. We empirically validate the benefits of adding a baseline on an array of
controllable language generation tasks such as constraining topic, sentiment, and
gender distributions in texts sampled from a language model. We observe superior
performance in terms of constraint satisfaction, stability and sample efficiency.
1 Introduction
Pre-trained language models (Devlin et al.,
2019; Radford et al., 2019) are changing the
landscape of Machine Learning research and
practice. Due to their strong generative capa-
bilities many studies have found it sufficient to
“nudge” these models to conform to global pref-
erences defined over the generated sequences
instead of training from scratch using annotated
data. These preferences could include topic and
sentiment (Dathathri et al., 2020), valid musi-
cal notes and molecular structures (Jaques et al.,
2017a), code compilability (Korbak et al., 2021),
balancing gender bias (Khalifa et al., 2021), eval-
Reward
Distribution
MaximizatiOn
、
Parametric
, reward
%/@R式x) + Ek/Kx)" log 幅(x)
ɪ
Figure 1: In this study we make connection between
two popular paradigms for fine-tuning sequence genera-
tion models according to preferences Reward Maximiza-
tion (RM) and Distribution Matching (DM).
^θEπβRθ(x)
/ Matching
ADkl (p,踞)
uation metrics for Machine Translation and Summarization (Ranzato et al., 2016; Bahdanau et al.,
2016), or direct human feedback (Ziegler et al., 2019; Stiennon et al., 2020). This large body of
studies is driven by two paradigms: Reward Maximization (RM) and Distribution Matching (DM).
Reward Maximization RM exploits the intuitive notion that we can nudge pre-trained models
towards some preferences by providing global sequence-level rewards when the model generates
outputs that satisfy desired features. For instance, if the model is producing toxic content, we can apply
Reinforcement Learning (RL) techniques to discourage it from producing similar content again in the
future. However, the risk of naively applying RL is that the model can undergo catastrophic forgetting
of its original distribution. For example, it can degenerate into producing a single nonsensical but at
least nontoxic sequence. Although several studies have considered hand-crafting general rewards to
ensure desirable features like fluency (Liu et al., 2016a; Tambwekar et al., 2019), coming up with
1
Under review as a conference paper at ICLR 2022
rewards that are not incomplete or imperfect is highly non-trivial (Wu et al., 2016; Vedantam et al.,
2015). These challenges have sparked a wide discussion on the overall effectiveness of RM for some
tasks such as Neural Machine Translation (Choshen et al., 2020; Kiegeland & Kreutzer, 2021).
Reward Maximization with KL-Control To tackle the aforementioned issues of “catastrophic
forgetting”, several studies, still under an RM paradigm, have considered incorporating a distributional
term inside the reward to be maximized. In particular Jaques et al. (2017b; 2019); Ziegler et al.
(2019) and Stiennon et al. (2020) have applied variations of KL-control (Todorov, 2007; Kappen
et al., 2012) which adds a penalty term to the reward term so that the resulting policy does not deviate
too much from the original one in terms of KL-divergence. The overall objective with the KL-penalty
is maximized using an RL algorithm of choice including: PPO (Schulman et al., 2017a) as in Ziegler
et al. (2019) or Q-learning (Mnih et al., 2013) as in Jaques et al. (2017b). Adding this distributional
KL-penalty to the reward raises some important questions: What effect does it have on the shape of
the optimal policy? Does this new objective have any interpretation from a distributional perspective?
Distribution Matching A different recent paradigm for fine-tuning language models to satisfy
downstream preferences formulates the problem as Distribution Matching (DM). This paradigm
consists of two steps: first a target distribution incorporating the desired preferences is defined as an
Energy-Based Model (LeCun et al., 2006). Then the forward KL divergence is minimized between
this target distribution and an auto-regressive policy using a family of algorithms referred to as
Distributional Policy Gradients (DPG) (Parshakova et al., 2019b; Khalifa et al., 2021; Korbak et al.,
2021). This approach capitalizes on the flexibility of EBMs in specifying the target distribution.
For example, the EBM can be defined so that it conforms to all downstream preferences while its
corresponding normalized distribution has a minimal KL divergence from the original, pre-trained
language model, therefore tackling the problem of “catastrophic forgetting” (Khalifa et al., 2021).
Interestingly, this DM paradigm can also deal with distributional preferences, for instance, for de-
biasing language models by specifying that the generated sequences should be gender-balanced, i.e.
that 50% of generations contain female mentions. Such distributional constraints cannot be defined
in the RM paradigm where a reward is calculated for a single sequence.
Contrasting these two paradigms for fine-tuning language models, we can notice the promises and
limitations of each.1 RM approaches are equipped with a large arsenal of RL algorithms and opti-
mization techniques that can be efficient in reward maximization, however they lack the distributional
perspective that is necessary for avoiding catastrophic forgetting and imposing distributional prefer-
ences over LMs. DM approaches are better suited to tackle those limitations, however, the family of
DPG algorithms currently used is not as rich as its RL counterpart. So far, the connections between
these two seemingly distinct paradigms have not been thoroughly explored. By establishing such
connections we might import ideas from one approach to the other. This is our goal in this paper,
detailing the nuanced connections and applying them to a case-study in variance reduction. Overall,
our contributions are the following:
•	We untangle the connections between the RM and DM paradigms for fine-tuning language models.
We provide a detailed comparison between the family of DPG algorithms with Policy Gradients
of standard RL.
•	We provide an interpretation of KL-control techniques from a distribution matching perspective,
placing such techniques at an intermediate place between RM and DM.
•	We exploit these connections to theoretically justify applying baselines — a variance reduction
technique from RL — to DPG and derive a particular choice of a baseline. On an array of
controllable language generation experiments about constraining topic, sentiment, and gender
distributions over a pre-trained language model, we show that adding baselines leads to superior
performance on constraint satisfaction, stability on small batch sizes, and sample efficiency.
2	Background
Standard Policy Gradients One popular method for adapting the behaviour of language models
to certain preferences has been that of assigning a “reward” score R(x) for sequences x sampled
1See Appendix A for an extended discussion of related work.
2
Under review as a conference paper at ICLR 2022
from an autoregressive language model (policy) πθ. Then, the simplest policy gradient algorithm in
reinforcement learning, namely, REINFORCE (Williams, 1992a), aims to find the policy πθ(x) that
maximizes the average reward Eχ~∏gR(χ), and this leads, via the so-called “log derivative trick”, to
a gradient ascent algorithm based on:
VθEχ~∏θR(X) = Eχ~∏θR(XNθ log∏θ(x),	(1)
which iteratively samples x from πθ and update parameters by increments proportional to
R(X)Vθ logπθ(X).
KL-control Jaques et al. (2017b; 2019); Ziegler et al. (2019), inspired by KL-control (Todorov,
2007; Kappen et al., 2012), add a KL penalty term to the reward objective to penalize large deviations
from the original pretrained model. That is, they maximize the expectation Eχ~∏g Rz (x), where:
Rz(X) = r(X) - β log ~~τ~r,	(2)
a(X)
and where β is a free hyperparameter balancing the wish to maximize r with that of not deviating
from a.
Distributional Policy Gradients Distributional Policy Gradients (DPG) (Parshakova et al., 2019b)
is a recent approach used to fit an autoregressive policy πθ to the distribution p(X) = P (X)/Z induced
by the EBM P (X), where Z = Px P(X) is the normalization constant (partition function). Given an
arbitrary EBM P (X), DPG optimizes the loss function DKL (p, πθ) with respect to the parameters θ of
an autoregressive model πθ, a loss which is minimized for πθ = p. The KL-divergence minimization
objective leads to a gradient estimate of the form:
VθDkl(p,∏θ) = - VθEχ~p log ∏θ(x)
=—Xp(x)Vθ log∏θ(x) = — Z X P(X)VΘ log∏θ(x)
—
x
1	P(X)
E Ex~∏θTʒ^vθ log πθ (x).
Z	θ πθ(X)
(3)
(4)
(5)
x
3	Reward Maximization vs Distribution Matching
In the previous section, we have summarized three approaches that have been suggested for fine-tuning
language models. Two of them can be characterized as “Reward Maximization” (RM): Standard
Policy Gradients (PG) and KL Control. On the other hand, DPG clearly belongs to the realm of
“Distribution Matching” (DM) as it first defines the target distribution and then optimizes a policy to
match it. In the rest of this section, we will explore connections between these two seemingly distinct
concepts and, in the following section, we will exploit them to improve DM-based methods.
3.1	Standard vs. Parametric Rewards
Let us start with distinguishing between a “parametric reward” Rθ which depends on θ and a standard
reward R, which does not. If we wished to maximize the expected parametric reward, Eπθ Rθ (X), we
would follow its gradient, leading to the identities:
VθEχ~∏θRθ(x) = Vθ E∏θ(x)Rθ(x)	(6)
x
= X πθ (X)Vθ Rθ (X) + XRθ(X)Vθπθ(X)	(7)
=	πθ(X)VθRθ(X) +	πθ(X)Rθ(X)Vθlogπθ(X)	(8)
=Eχ~∏θVθRθ(x)+ Eχ~∏θRθ(x)Vθ log ∏(x) .	(9)
'-----------} '-------------------}
RG-term
PG-term
3
Under review as a conference paper at ICLR 2022
Equation (9) is the sum of two terms: the first one, the “RG-term" (Reward Gradient term), involves
the gradient of the reward. The second one, the “PG-term” (Policy Gradient term), was obtained
using the “log derivative trick” and involves the gradient of the policy stricto sensu. In standard RL,
where the reward does not depend on θ, the RG-term disappears and the gradient of expected reward
consists solely of the PG-term. However, when Rθ depends on θ, the gradients are distinct (apart
from specific cases where the RG-term evaluates to 0, as we will see below).
3.2	KL Control as Distribution Matching
Adding a KL-penalty term to the reward (as in the case of KL-control) leads to a parametric reward.
However, due to the particular form of its objective, the RG-term actually vanishes,2 leaving only
the PG-term Eχ~∏gRz(x)Vθ log ∏θ(x) and simplifying the tuning procedure to a standard Policy
Gradient. While this algorithm falls under the RM paradigm, here we argue that is it multifaceted, and
explore deeper connections with the DM paradigm. More precisely, the maximization of the reward
with the KL penalty term is equivalent to a distributional matching with an underlying emergent
sequential EBM, a remark that already reveals some similarities with DPG. Actually, if we consider
the following EBM:
Pz(X) = a(x)er(x)/e,	(10)
then its normalized distribution, pz(x) = Z Pz(x), with Z = Px Pz(x), is, among all possible
distributions, the one that obtains maximal expected reward Rθz (x). A simple way to prove this fact
is to notice that the expectation of the reward Rθz has a monotonically decreasing relationship with
the reverse KL divergence between πθ and pz :
Dkl (∏θ,Pz) = Ex~∏θ log ∏θ(xy = Ex~∏θ [ log ∏θ(X)- log Za(x)er(x)/ej
= log Z - 1 Ex~πθhr(X)-β log Ui =log Z - 1 Ex~πθRθ(X),	(II)
so that the argmin∏0 Dkl(∏θ,pz) coincides with the argmaχ∏0 Ex~∏gRz(x). Provided that the
family of distributions πθ is large enough to cover all distributions over X, arg minπθ DKL (πθ, pz)
is just pz , which concludes the proof.3
Overall, we can conclude that the addition of the distributional term (KL-penalty) to the reward does
indeed provide a DM interpretation, namely in terms of minimizing the reverse KL divergence with an
emergent underlying distribution pz (X). We note thatpz(X) does not correspond to an explicit choice
of EBM (e.g. the one that balances the gender and topic distributions of a language model). Instead
equation (10) has a limited form implicitly defined by the reward Rθz , along with a β hyperparameter
without a clear meaning. By contrast, the DPG algorithms are designed to perform DM on any EBM
specification, corresponding to an explicit distributional objective.
3.3	Similarities and Differences between DPG and Policy Gradients
In the previous subsection, we have connected KL-control, a method designed under a RM paradigm,
to DM. Now, we turn to the converse question of whether DPG, a DM method, can be connected to
RM. We begin by noting that after defining Rθ = Px), the DPG gradient Ex~∏g Px) V log ∏θ (x)
acquires the format of the PG-term Eπθ Rθ Vθ logπθ(X).
However, the DM objective of DPG cannot be considered as maximizing the average “reward”
Rθ(x) = ∏p(χy, as this would require adding also the RG-term E∏θ Vθ∏p(χy into the gradient, which
in this case does not vanish.
Nonetheless, the analogy behind this gradient term is more fruitful than it first appears. As a matter
of fact, DPG gradient estimates suffer from the same high-variance problems as with standard PG.
While the objective of DPG (distribution matching) is different from that of Policy Gradients (reward
2This is because E∏θ Vθ Rz (x) = — β E∏θ Vθ log ∏θ (x) = 0, via the often used identity
E∏θ Vθ log∏θ(x) = Px ∏θ(x)Vθ log∏θ(x) = Px Vθ∏θ(x) = Vθ Px ∏θ(x) = 0.
3The optimal policy pz is briefly mentioned in (Ziegler et al., 2019) without reference or derivation. The
proof, which we believe to clarify important underlying connections, is ours.
4
Under review as a conference paper at ICLR 2022
Policy Gradients	DPG
Reward	R(x)	Rθ (x) = -P(x) θ	πθ(x)
Vθ	Eχ~∏θ R(x)Vθ log ∏θ (x)	Eχ~∏θ Px) Vθ log ∏θ (x)
Baseline	Eχ~∏g R(X)	Z
Vθ with Baseline	Eχ~∏g [r(x) — Eχ~∏gR(x)] Vθ log ∏θ(x)	Ex~∏θ hP(X)) ― Zi vθ log πθ(X)
Table 1: A comparison between Policy Gradients (Sutton et al., 1999) and Distributional Policy Gradients (Par-
Shakova et al., 2019b) forms of Reward, Baseline, and Gradient of the loss function (the PG-term) before (Vθ )
and after (Vθ with Baseline) including a baseline for variance reduction .
maximization), DPG also needs to estimate the PG-term E∏θ Rθ (x)Vθ log ∏θ (x) at a given value of
θ, using a batch of samples x. For such a fixed θ, we can define provisionally R(x) =. Rθ and the
problem of gradient estimation for this fixed θ is identical to the estimation Eχ~∏g R(X) Vθ log ∏θ (x)
based on a set of samples x in standard RL. Therefore, the techniques that have been developed
to reduce the variance of the gradients estimates in RL can be ported to DPG insofar as we are
computing the gradient estimates at a given θ.4 In Section 4, we show how one can import one such
variance reduction technique to the DPG: baselines.
4	A Case Study on Variance Reduction
Baselines are a standard variance reduction technique in the
context of Policy Gradients (Sutton & Barto, 2018). The idea is
to subtract from the reward R(x) a value B that does not intro-
duce bias to the gradients but may change variance. After the
introduction of baseline, equation (1) then takes the following
form:
Vθ Eπθ R(x) = Eπθ [R(x) - B] Vθ logπθ(x).	(12)
In standard RL, the simplest form of baseline B is just the
average of the rewards for the policy:5
BRL = Eχ~∏θ R(x).	(13)
Following the same methodology of taking the baseline as the
expectation of the reward term, we can obtain a remarkably
simple form of a baseline for DPG:
Figure 2: Values of reward, advantage
and the baseline for first 1000 epochs
of a pointwise constraint experiment.
B=Ex~πθ Px=X πθ(X) Px=XP(X)=Z
xx
(14)
To confirm that B does not introduce bias to the gradients, let us rewrite the DPG gradient in (5) with
the added baseline B = Z :
Eχ~∏θ ∣Rθ(x) — z] Vθlog∏θ(x) = Eχ~∏θRθ(x)Vθ logπ(x) - ZEχ~∏g Vθ logπ(x)
(15)
=Eχ~∏θ Rθ (x)Vθ log ∏θ (x) - z[ £ Vθ∏θ(x)J
Here, the second term does not introduce bias because Z Px Vθπθ (X) = 0, leaving us with the
same exact form of gradient as in the DPG algorithm.
Note that since BRL depends on θ, it has to be be re-estimated after each gradient update. On the
other hand, B does not depend on θ, which is an advantage because B could be now estimated by
averaging over samples from all the different θ's without introducing bias, leading to a more accurate
estimation. See Table 1 for a comparison of these two forms of baselines.
4To avoid confusion, note that variance reduction methods for SGD focus on the objective of better estimating
the true gradient expectation Eπθ [...], for a fixed θ. The fact that θ will then be updated based on such estimates
is orthogonal to this objective.
5While this choice of the baseline is not optimal (see Appendix B.1 for a proof), it is widely used in practice.
5
Under review as a conference paper at ICLR 2022
The off-policy DPG version introduced
in (Parshakova et al., 2019b) and its KL-
adaptive variant (Khalifa et al., 2021) sample
a proposal distribution q instead of the policy
πθ . Then, the baseline takes the form
BOff(X) = Z *,	(16)
q(x)
where the πθχ) term is an importance weight
cOrrecting fOr the bias intrOduced by sam-
pling from q. Similarly to the DPG case,
this baseline does not introduce bias (see Ap-
Algorithm 1 KL-Adaptive DPG with baseline
Require: P , initial generative model a
1:	∏θ - a, q — a
2:	for each iteration do
3:	for each episode do
4:	sample X from q(∙)
5:	θ - θ + α(θ) h耨-Zπθχ)i Vθ log πθ(X)
6:	if Dkl(p∣∣∏θ) < Dkl(p||q) then
7:	q J ∏θ
Ensure: πθ
pendix B for details about this version of the baseline). In practice, as shown on Figure 2, adding a
baseline to KL-adaptive DPG (Algorithm 1) centers the advantage (defined as A = Px)) - Z %x))
around 0 leading to better performance on: convergence (section 5.3), stability on small batch sizes
(section 5.4), and variance reduction (section 5.5).
5	Experiments and Results
5.1	Generation with Distributional Control
We investigate the benefits of adding a baseline to the DPG algorithm, on the Generation with
Distributional Control (GDC) (Khalifa et al., 2021) framework. GDC makes use of DPG to control
the properties of pre-trained language models to satisfy certain constraints. In our experiments, follow
target distribution form of Parshakova et al. (2019a) and Khalifa et al. (2021), in which the EBM
P(x) is defined so that its normalized variant p(x) matches a set of desired moments constraints on
given features φi(x), while having a minimal KL divergence DKL(p, a) from an original pretrained
language model a, to avoid catastrophic forgetting.
These constraints are expressed as conditions μ% = Ex〜pφi(χ), for i ∈ {1,..., n}, by which the
moments (expectations) under the distribution p of each feature φi (x) are required to take certain
desired values 口^ For instance, let φι(χ) = 1 iff the topic of X is science and φ2(χ) = 1 iff X
mentions a female person, then imposing moments μι = 1 and μ2 = 0.5 constrains the language
model p to only generate sequences about science, half of which mention females. P (X) is uniquely
determined by the following form:6
P(X) = a(X)ePin=1λiφi(x),	(17)
where λi terms control the moments μi of the associated features, which can be estimated through
self-normalized importance sampling (Owen, 2013); and then, to make the moments match the
desired values, the λi terms can be optimized through SGD (Parshakova et al., 2019a).
5.2	Experimental setup
We demonstrate the benefits of adding a baseline to the family of Distributional Policy Gradients
algorithms on an array of controlled language generation tasks for English. For this, we modify
the GDC framework Khalifa et al. (2021) namely its KL-DPG algorithm to include a baseline as
shown in Algorithm 1. We refer to this method in all the following experiments as GDC++. In
addition to comparing GDC++ with GDC, for tasks for which the comparison is meaningful (i.e. for
“pointwise constraints", see below), we compare with two reward maximization baselines: Reinforce
(Williams, 1992b) and Ziegler (Ziegler et al., 2019). Reinforce tries to maximize the expected
reward Ex〜∏θ R(x), where R(X) = 1 iff the pointwise constraints are met. Ziegler instantiates the
KL-control approach: its objective includes a KL penalty term for departures from a.
Tasks We evaluate GDC, GDC++ and the two baselines on 10 tasks, which we construct through
sets of moment constraints {μi} for binary features {φi}. These include 6 sets of purely pointwise
constraints (for which μi = 1) and 4 sets including distributional constraints (0 < μi < 1). We
consider the following constraint types:
6For a more precise formulation of this EBM, see (Khalifa et al., 2021).
6
Under review as a conference paper at ICLR 2022
(a)	Single-word constraints, where φ(x) = 1 iff the a given word appears in the sequence x.
We experiment both with frequent words (task 1: “amazing”, original frequency: 10-4) and
(task 2: “WikiLeaks”, original frequency: 10-5) rare words,
(b)	Wordlist constraints, where φ(x) = 1 iff x contains at least one word from a given list. We
consider lists of word associated with politics (task 3) and science (task 4) published by
Dathathri et al. (2020),
(c)	Sentiment classifier constraints, where φ(x) = 1 if x is classified as positive (task 5), or
negative (task 6) by a pre-trained classifier published by Dathathri et al. (2020).
(d)	A single distributional constraint where φ(x) = 1 iff x contains a female figure mention,
and μ = 0.5 (task 8),
(e)	A set of four distributional constraints: φi(x) = 1 iff x contains at least one of the words in
the “science", “art", “sports" and “business" wordlists (compiled by Dathathri et al. (2020)),
respectively. For each i, μ% = 0.25 (task 8),
(f)	Hybrid constraints where φ1 (x) = 1 iff x contains more female than male pronouns,
μι = 0.5 and φ2 (x) = 1 iff X contains at least one of the words from the “sports" wordlist
(task 9) or “politics” wordlist, μ2(χ) = 1 (task 10).
Following (Khalifa et al., 2021), for hybrid and distributional constraints (tasks 8-10) we compare
only GDC and GDC++ because the RM objective of Ziegler and Reinforce is not equipped to handle
distributional constraints.
Metrics We report the following metrics evaluated over batches of samples from πθ at each
validation step:
1.	Ex〜∏θ φi (x), measuring the ability to reach the target moment of the i-th feature.
2.	DKL(p, πθ), the forward KL divergence from the optimal target distribution p,7
3.	DKL(πθ, a), the reverse KL divergence from the original pretrained language model a.
4.	Distinct-n score, a measure of text diversity in terms of the frequency of repetitions within a
single sample x, proposed by (Li et al., 2016a).
5.	Self-BLEU-n, a measure of text diversity on a distributional level across samples proposed
by (Zhu et al., 2018), ensuring that policies don’t converge into limited number of sequences
that satisfy the imposed constraints Caccia et al. (2020).
Training details For tasks 1-6, we use a pre-trained GPT-2 small with 117M parameters (Radford
et al., 2019) as the original language model a. For tasks 7-10, a is the same pre-trained model
additionally fine-tuned on the WikiBio (Lebret et al., 2016) dataset. See Appendix E for more details.
5.3 Results
We present the evolution of our metrics through training epochs in Figure 3 (aggregated over tasks
1-6) and Figure 6 in the Appendix (aggregated over tasks 7-10). Results for each task are presented
separately on Figures 7-10 in the Appendix.
Consistent with prior work (Khalifa et al., 2021), we observe that Reinforce is able to quickly achieve
high levels of constraint satisfaction, but at the cost of large deviations from a, which translates into
significantly decreased diversity of generated samples (in terms of Self-BLEU-5 and Distinct-1). The
KL penalty term in Ziegler imposes an upper bound on deviation from a but the deviation is still
significant enough to result in a drop in diversity. Moreover, we have observed Ziegler’s objective to
result in very unstable training.
GDC and GDC++ are the only fine-tuning methods that address constraint satisfaction based on a
clear formal objective, i.e. reducing the divergence from p. The approach translates into significantly
smaller deviations from a and maintaining diversity within and across samples. The addition of
a baseline indeed reduces the variance. We analyze that extensively in Appendix 5.5 while here
focusing on the downstream effects of variance reduction. One is that πθ is now able to compound
staying closer to p and a at the same time, while achieving slightly better constraint satisfaction. We
have also observed that baseline stabilizes training, leading to smoother curves.8
7See Appendix D for a detailed description of how DKL (p, πθ) is computed.
8The interested reader can compare the large fluctuations of the Ziegler objective to more stable training
curves of GDC , and even more of GDC++ , in the disaggregated curves in Figures 7-10 of the Appendix.
7
Under review as a conference paper at ICLR 2022
10 'I	8ΛxlO^1
——GDC++ 1∙0
—GDC
....Zlegler 0.8
....Reinforce
O 2k 4k
epochs
0.0
O 2k 4k
epochs
∙ttURSs
xlθ^1
XlOj
B3 X 10^1
O 2k 4k
epochs
β,7xlθ-'
8«xl0^,
O 2k 4k
epochs
s,nw^l≈,-∙,s
2k 4k
epochs
Figure 3: Evaluation metrics: Dkl(p, ∏θ) (] better), E∏θφ(x) (↑ better), Dkl (∏θ , a) (] better), Self-BLEU-5
(]better), and Distinct-1 (↑ better) aggregated over 6 pointwise constraints experiments (tasks 1-6) for policies
obtained from GDC++, GDC, Ziegler and Reinforce. See Figure 6 for aggregated distributional constraints
experiments, and Figures 7-10 in the Appendix for a detailed view on each experiment. And a Table 5 view for
final results of each run.
(a) Task 1: a pointwise constraint
(b) Task 8: a set of distributional constraints; μi = 0.25
Figure 4: E∏θ φ(x) or μ per constraint (↑ better) and Dkl (p, ∏θ) (] better) as a function of the number of
samples reported for task 1 (a) and task 8 (b). We report the number of samples (i.e. the number of epochs times
the batch size) for a fair comparison of convergence speed. GDC++ is consistently superior across all batch
sizes in terms of convergence and constraint satisfaction. The effect is more conspicuous with small batch sizes.
For instance, with batch size 256 the baseline prevents the policy from catastrophically diverging from p. Batch
sizes 512 and 2014 are greyed out for clarity.
5.4	The effect of baseline across batch sizes
We expect that reducing variance in the gradient estimates can allow to train the models with lower
batch sizes, performing gradient updates on estimates based on smaller batch sizes can increase
the sample efficiency. To test this hypothesis, we rerun tasks 1 (a pointwise constraint on the word
“amazing") and 8 (a set of distributional constraints on topics) with four batch sizes (256, 512, 1024,
2048). We present the results on Figures 4a and 4b. The benefits of adding a baseline — higher
constraint satisfaction, lower divergence from p, more stable training — are especially evident with
lower batch sizes. For instance, with batch size 256, GDC++ obtains a significantly higher constraint
satisfaction rate and lower divergence from p.
Furthermore, stable training with smaller batch sizes translates into better sample efficiency.
For instance, in task 1 (Figure 4a), GDC++ with batch size 256 needs 1M samples to achieve
Ex〜∏θφ(x) = 0.5 while GDC++ with batch size 2048 needs 4M. In contrast, GDC with batch size
256 does not achieve Ex〜∏θ φ(χ) = 0.5 at all, confirming the importance of adding the baseline.
5.5	Empirical Evaluation of Variance Reduction
Next, we evaluate empirically the effect of the baseline for variance reduction. We select two tasks:
task 1 (a pointwise constraint) and task 7 (distributional constraints) described in Section 5.2, each
with 3 different seeds, while monitoring the following variance measures:
Gradient Variance The gradient estimate is defined as: Gθ(x) = A(χ)Vθ log ∏θ(x), where
Gθ(x) ∈ Rlθl is an unbiased estimate of the gradient of the forward KL loss VDkl (p, ∏θ) with
8
Under review as a conference paper at ICLR 2022
respect to the parameters θ. We then have, with μ(Gθ) = Ex〜qGθ(x):
Var(Gθ) = Ex〜qkGθ(X)- μ(Gθ)k2	(18)
=Esql∣Gθ(x)ll2 -∣∣μ(Gθ)||2.	(19)
Task7: Distributional
Variance of the advantage This is defined
by:
Var(A)= Eχ"∣A(x)- μAll；	(20)
where, μA ≡ Ex〜q A(x) is the mean of the
advantage, which we showed above to be null
after the addition of the baseline.
Expected absolute value of the advantage
This metric is defined as:
〃1A1= ExF |A(x)|.	(21)
It directly provides a standard measure of distri-
butional discrepancy between p and πθ, in terms
of TVD (Total Variation Distance). Indeed we
have:
Ex~q∣pX)- ∏θ⅛)l = 2τvD(p,πθ).(22)
q(x)	q(x)
0.00
0.75
Results Figure 5 shows that GDC++ ob-	商。∙”
tains lower variance in the gradient estimates ⅞65
Var(Gθ) and the variance of the advantage 之
Var (A) in both pointwise and distributional ex- 。石。
periments compared to its non-baseline counter-
part GDC.
Taskl： Pointwise
4 3 2
Ooo
111
9He>
1000	2000
epochs
0610∙
1 X
6
9He>
2 XI。T
IO-4
6xl0^s
4xl0^s
3 x IOT
1000	2000
epochs
1.00
0.75
⅞l0.50
0.25
1000	2000
epochs
0.24
0.28
e
3 0.26
1000	2000
epochs
method
---GDC++
--GDC
1000	2000
epochs
1.6x10°
1.5 ×10°
22x10*
2.1 XlO0
2 × IOfl
W l,9×10β
ɪ 13×10β
> 1.7 × 10°
1000	2000
epochs
1.00
0.75
⅞t0∙50
0.25
0.00
1000	2000
epochs
1000	2000
epochs
豆」e>
We further observe a decreasing trend in the
mean absolute value of the advantage μlAl
which is correlated with a decreasing trend in
Figure 5: Comparison between GDC and GDC++
using a set of Variance diagnosis metrics on two experi-
ments for pointwise and distributional constraints.
the TVD distance between the trained policy πθ and the optimal distribution p. Overall, these results
support our hypothesis that adding a baseline to DPG reduces the variance during training and yields
better convergence towards the optimal distribution p.
6 Conclusion
In this paper, we analyzed the nuanced relation between RM and DM approaches to fine-tuning
language models: we demonstrated that KL-control can be seen as a form of DM and showed
that while DPG and PG have different goals, some similarities (similar forms of gradient estimates
despite different objectives) can be exploited. We used these insights to inform an extension of DPG,
consisting in adding a baseline to reduce the variance of gradient estimates.
The connections we established suggest that despite fundamental differences between DPG and
RL, at least some of the theoretical results and algorithmic techniques from RL can be adapted to a
DM framework without losing their formal guarantees. In this paper, we have focused on variance
reduction using baselines, but the space of possible enhancements is vast. Promising candidates
include further reducing the variance using a learned value function (Konda & Tsitsiklis, 2000) and
preventing detrimentally large policy updates by maintaining a trust region in the policy space 一
akin to techniques such as trust-region policy optimisation (Schulman et al., 2015) and proximal
policy optimisation (Schulman et al., 2017b). Another future direction could consist in analyzing the
relation between explicit EBMs in DPG and implicit EBMs arising in KL-control and characterizing
the space of EBMs that could be reached through KL-control.
9
Under review as a conference paper at ICLR 2022
Reproducibility S tatement
The source code for our experiments was based on the repository that Khalifa et al. (2021) published
on GitHub.9 It is available for the reviewers and area chairs and will be made publicly available
alongside the camera ready version of the paper. The two pretrained models used in our experiments
are available on Hugginface Model Hub: gpt10 and mkhalifa/gpt2-biographies.11 In
addition to that, in Appendix E we provide the hyperparameters used throughout our experiments
and report our hardware configuration. In Appendix D, we describe in detail how DKL(p, πθ) and
TVD(p, πθ) were estimated and provide an extended pseudocode for our training loop in Algorithm 2.
Finally, in Appendix B we present proofs of all mathematical facts referred to in the paper.
Ethics statement
The focus area of this paper — fine-tuning large language models — is aligned with an important line
of work on addressing the problem of social bias in large language models (Sheng et al., 2019; Liang
et al., 2021). As the training data for large language models consists mainly of crawled user-generated
content, a number of factors (from crawling methodology to Internet participation inequalities and
moderation practices) leads to an over-representation of certain viewpoints and voices exceeding
their prevalence in the general population. This poses a risk of amplifying biases and harms through
a language model perpetuating these voices (Bender et al., 2021; Blodgett et al., 2020; Sheng et al.,
2019). Numerous problems related to addressing data bias in language generation (e.g. controlling
for gender distribution in generated texts) can be naturally posed as generative distributional control
(GDC), the framework we focus our experiments on. The distributional character of these data bias
problems lies in the fact that desirable properties of generated texts are defined for a collection of
samples, not only for individual samples. Our theoretical analyses of reward maximization and
distribution matching approaches as well as our algorithmic improvements to the GDC framework
— termed GDC++ — are therefore also a contribution to the problem of bias in language models.
However, we need to be aware that GDC++ , KL-control as well as controllable language generation
techniques in general, can also be diverted to malicious uses such as spreading misinformation or
generating harmful content.
References
Daniel Andor, Chris Alberti, David Weiss, Aliaksei Severyn, Alessandro Presta, Kuzman Ganchev,
Slav Petrov, and Michael Collins. Globally Normalized Transition-Based Neural Networks. 2016.
doi: 10.18653/v1/P16-1231.
Dzmitry Bahdanau, Philemon Brakel, Kelvin Xu, Anirudh Goyal, Ryan Lowe, Joelle Pineau, Aaron
Courville, and Yoshua Bengio. An Actor-Critic Algorithm for Sequence Prediction. (2015):1-17,
2016. URL http://arxiv.org/abs/1607.07086.
Dzmitry Bahdanau, Philemon Brakel, Kelvin Xu, Anirudh Goyal, Ryan Lowe, Joelle Pineau, Aaron C.
Courville, and Yoshua Bengio. An actor-critic algorithm for sequence prediction. In 5th Inter-
national Conference on Learning Representations, ICLR 2017, Toulon, France, April 24-26,
2017, Conference Track Proceedings. OpenReview.net, 2017. URL https://openreview.
net/forum?id=SJDaqqveg.
A. Bakhtin, Y. Deng, S. Gross, Myle Ott, Marc’Aurelio Ranzato, and Arthur Szlam. Energy-based
models for text. ArXiv, abs/2004.10188, 2020.
David Belanger and Andrew McCallum. Structured prediction energy networks. In Proceedings of
the 33rd International Conference on International Conference on Machine Learning - Volume
48, ICML’16, pp. 983-992. JMLR.org, 2016. URL http://dl.acm.org/citation.cfm?id=
3045390.3045495.
9https://github.com/naver/gdc
10https://huggingface.co/gpt2
11https://huggingface.co/mkhalifa/gpt2-biographies
10
Under review as a conference paper at ICLR 2022
Emily M. Bender, Timnit Gebru, Angelina McMillan-Major, and Shmargaret Shmitchell. On the
dangers of stochastic parrots: Can language models be too big? In Proceedings of the 2021
ACM Conference on Fairness, Accountability, and Transparency, FAccT '21, pp. 610-623, New
York, NY, USA, 2021. Association for Computing Machinery. ISBN 9781450383097. doi:
10.1145/3442188.3445922. URL https://doi.org/10.1145/3442188.3445922.
SU Lin Blodgett, Solon Barocas, Hal DaUme III, and Hanna Wallach. Language (technology) is
power: A critical survey of “bias” in NLP. In Proceedings of the 58th Annual Meeting of the
Association for Computational Linguistics, pp. 5454-5476, Online, JUly 2020. Association for
CompUtational LingUistics. doi: 10.18653/v1/2020.acl-main.485. URL https://www.aclweb.
org/anthology/2020.acl-main.485.
Christopher L. BUckley, Chang SUb Kim, Simon McGregor, and Anil K. Seth. The free energy
principle for action and perception: A mathematical review. Journal of Mathematical Psychology,
81:55-79, 2017. ISSN 0022-2496. doi: https://doi.org/10.1016/j.jmp.2017.09.004. URL https:
//www.sciencedirect.com/science/article/pii/S0022249617300962.
Massimo Caccia, LUcas Caccia, William FedUs, HUgo Larochelle, Joelle PineaU, and LaUrent Charlin.
LangUage gans falling short. In International Conference on Learning Representations, 2020. URL
https://openreview.net/forum?id=BJgza6VtPB.
Leshem Choshen, Lior Fox, Zohar AizenbUd, and Omri Abend. On the weaknesses of reinforcement
learning for neUral machine translation. In 8th International Conference on Learning Represen-
tations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020. OpenReview.net, 2020. URL
https://openreview.net/forum?id=H1eCw3EKvH.
Kevin Clark, Minh-Thang LUong, QUoc V. Le, and Christopher D. Manning. Pre-training transformers
as energy-based cloze models. In Bonnie Webber, Trevor Cohn, YUlan He, and Yang LiU (eds.),
Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing,
EMNLP 2020, Online, November 16-20, 2020, pp. 285-294. Association for CompUtational
LingUistics, 2020. doi: 10.18653/v1/2020.emnlp-main.20. URL https://doi.org/10.18653/
v1/2020.emnlp-main.20.
SUmanth Dathathri, Andrea Madotto, Janice Lan, Jane HUng, Eric Frank, Piero Molino, Jason
Yosinski, and Rosanne LiU. PlUg and play langUage models: A simple approach to controlled
text generation. In 8th International Conference on Learning Representations, ICLR 2020, Addis
Ababa, Ethiopia, April 26-30, 2020. OpenReview.net, 2020. URL https://openreview.net/
forum?id=H1edEyBKDS.
Peter Dayan. Reinforcement comparison. In Proceedings of the 1990 Connectionist Models Summer
School, pp. 45-51. Morgan KaUfmann, San Mateo, CA, 1990.
YUntian Deng, Anton Bakhtin, Myle Ott, ArthUr Szlam, and Marc’AUrelio Ranzato. ResidUal energy-
based models for text generation. In 8th International Conference on Learning Representations,
ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020. OpenReview.net, 2020. URL https:
//openreview.net/forum?id=B1l4SgHKDH.
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina ToUtanova. BERT: Pre-training of deep
bidirectional transformers for langUage Understanding. In Proceedings of the 2019 Conference of
the North American Chapter of the Association for Computational Linguistics: Human Language
Technologies, Volume 1 (Long and Short Papers), pp. 4171-4186, Minneapolis, Minnesota, JUne
2019. Association for CompUtational LingUistics. doi: 10.18653/v1/N19-1423. URL https:
//www.aclweb.org/anthology/N19-1423.
Karl J Friston, Jean DaUnizeaU, James Kilner, and Stefan J Kiebel. Action and behavior: a free-energy
formUlation. Biological cybernetics, 102(3):227-260, 2010.
Evan Greensmith, Peter L. Bartlett, and Jonathan Baxter. Variance redUction techniqUes for gradient
estimates in reinforcement learning. J. Mach. Learn. Res., 5:1471-1530, December 2004. ISSN
1532-4435.
11
Under review as a conference paper at ICLR 2022
Michael Gutmann and AaPo Hyvarinen. Noise-contrastive estimation: A new estimation principle
for unnormalized statistical models. In Yee Whye Teh and Mike Titterington (eds.), Proceedings
of the Thirteenth International Conference on Artificial Intelligence and Statistics, volume 9 of
Proceedings of Machine Learning Research, pp. 297-304, Chia Laguna Resort, Sardinia, Italy,
13—15 May 2010. PMLR. URL http://Proceedings.mlr.press∕v9∕gutmann10a.html.
Tuomas Haarnoja, Haoran Tang, Pieter Abbeel, and Sergey Levine. Reinforcement learning with
deep energy-based policies. In Doina Precup and Yee Whye Teh (eds.), Proceedings of the 34th
International Conference on Machine Learning, volume 70 of Proceedings of Machine Learning
Research, pp. 1352-1361. PMLR, 06-11 Aug 2017. URL https://proceedings.mlr.press/
v70/haarnoja17a.html.
Danijar Hafner, Pedro A. Ortega, Jimmy Ba, Thomas Parr, Karl Friston, and Nicolas Heess. Action
and perception as divergence minimization, 2020.
Tianxing He, Bryan McCann, Caiming Xiong, and Ehsan Hosseini-Asl. Joint energy-based model
training for better calibrated natural language understanding models. In Proceedings of the 16th
Conference of the European Chapter of the Association for Computational Linguistics: Main
Volume, pp. 1754-1761, Online, April 2021. Association for Computational Linguistics. URL
https://www.aclweb.org/anthology/2021.eacl-main.151.
Geoffrey E. Hinton. Training products of experts by minimizing contrastive divergence. Neural
Comput., 14(8):1771-1800, 2002. doi: 10.1162/089976602760128018. URL https://doi.
org/10.1162/089976602760128018.
Natasha Jaques, Shixiang Gu, Dzmitry Bahdanau, JoSe Miguel Herndndez-Lobato, Richard E.
Turner, and Douglas Eck. Sequence tutor: Conservative fine-tuning of sequence generation
models with kl-control. In Doina Precup and Yee Whye Teh (eds.), Proceedings of the 34th
International Conference on Machine Learning, ICML 2017, Sydney, NSW, Australia, 6-11 August
2017, volume 70 of Proceedings of Machine Learning Research, pp. 1645-1654. PMLR, 2017a.
URL http://proceedings.mlr.press/v70/jaques17a.html.
Natasha Jaques, Shixiang Gu, Dzmitry Bahdanau, Jose Miguel Hernandez Lobato, Richard E. Turner,
and Doug Eck. Tuning recurrent neural networks with reinforcement learning. 2017b. URL
https://openreview.net/pdf?id=Syyv2e-Kx.
Natasha Jaques, Asma Ghandeharioun, Judy Hanwen Shen, Craig Ferguson, Agata Lapedriza,
Noah Jones, Shixiang Gu, and Rosalind W. Picard. Way off-policy batch deep reinforcement
learning of implicit human preferences in dialog. CoRR, abs/1907.00456, 2019. URL http:
//arxiv.org/abs/1907.00456.
Hilbert J Kappen, ViCeng G6mez, and Manfred Opper. Optimal control as a graphical model inference
problem. Machine learning, 87(2):159-182, 2012.
Muhammad Khalifa, Hady Elsahar, and Marc Dymetman. A distributional approach to controlled
text generation. In International Conference on Learning Representations, 2021. URL https:
//openreview.net/forum?id=jWkw45-9AbL.
Samuel Kiegeland and Julia Kreutzer. Revisiting the weaknesses of reinforcement learning for
neural machine translation. In Kristina Toutanova, Anna Rumshisky, Luke Zettlemoyer, Dilek
Hakkani-Tur, Iz Beltagy, Steven Bethard, Ryan Cotterell, Tanmoy Chakraborty, and Yichao ZhoU
(eds.), Proceedings of the 2021 Conference of the North American Chapter of the Association for
Computational Linguistics: Human Language Technologies, NAACL-HLT 2021, Online, June 6-11,
2021, pp. 1673-1681. Association for Computational Linguistics, 2021. doi: 10.18653/v1/2021.
naacl-main.133. URL https://doi.org/10.18653/v1/2021.naacl-main.133.
Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint
arXiv:1412.6980, 2014.
A.S. Klyubin, D. Polani, and C.L. Nehaniv. Empowerment: a universal agent-centric measure of
control. In 2005 IEEE Congress on Evolutionary Computation, volume 1, pp. 128-135 Vol.1,
2005. doi: 10.1109/CEC.2005.1554676.
12
Under review as a conference paper at ICLR 2022
Vijay Konda and John Tsitsiklis. Actor-critic algorithms. In S. Solla, T. Leen,
and K. Muller (eds.), Advances in Neural Information Processing Systems, volume 12.
MIT Press, 2000. URL https://proceedings.neurips.cc/paper/1999/file/
6449f44a102fde848669bdd9eb6b76fa-Paper.pdf.
Tomasz Korbak, Hady Elsahar, Marc Dymetman, and Germdn Kruszewski. Energy-based models
for code generation under compilability constraints. CoRR, abs/2106.04985, 2021. URL https:
//arxiv.org/abs/2106.04985.
Remi Lebret, David Grangier, and Michael Auli. Neural text generation from structured data
with application to the biography domain. In Jian Su, Xavier Carreras, and Kevin Duh (eds.),
Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing,
EMNLP 2016, Austin, Texas, USA, November 1-4, 2016, pp.1203-1213. The Association for
Computational Linguistics, 2016. doi: 10.18653/v1/d16-1128. URL https://doi.org/10.
18653/v1/d16-1128.
Yann LeCun, Sumit Chopra, Raia Hadsell, Marc’Aurelio Ranzato, and Fu Jie Huang. A Tutorial on
Energy-Based Learning. In Predicting Structured Data. MIT Press, 2006.
Sergey Levine. Reinforcement learning and control as probabilistic inference: Tutorial and review,
2018.
Jiwei Li, Michel Galley, Chris Brockett, Jianfeng Gao, and Bill Dolan. A diversity-promoting objec-
tive function for neural conversation models. In Proceedings of the 2016 Conference of the North
American Chapter of the Association for Computational Linguistics: Human Language Technolo-
gies, pp. 110-119, San Diego, California, June 2016a. Association for Computational Linguistics.
doi: 10.18653/v1/N16-1014. URL https://www.aclweb.org/anthology/N16-1014.
Jiwei Li, Will Monroe, Alan Ritter, Dan Jurafsky, Michel Galley, and Jianfeng Gao. Deep re-
inforcement learning for dialogue generation. In Jian Su, Xavier Carreras, and Kevin Duh
(eds.), Proceedings of the 2016 Conference on Empirical Methods in Natural Language Pro-
cessing, EMNLP 2016, Austin, Texas, USA, November 1-4, 2016, pp. 1192-1202. The As-
sociation for Computational Linguistics, 2016b. doi: 10.18653/v1/d16- 1127. URL https:
//doi.org/10.18653/v1/d16-1127.
Paul Pu Liang, Chiyu Wu, Louis-Philippe Morency, and Ruslan Salakhutdinov. Towards understand-
ing and mitigating social biases in language models, 2021.
Chia-Wei Liu, Ryan Lowe, Iulian Serban, Michael Noseworthy, Laurent Charlin, and Joelle Pineau.
How NOT to evaluate your dialogue system: An empirical study of unsupervised evaluation metrics
for dialogue response generation. In Jian Su, Xavier Carreras, and Kevin Duh (eds.), Proceedings of
the 2016 Conference on Empirical Methods in Natural Language Processing, EMNLP 2016, Austin,
Texas, USA, November 1-4, 2016, pp. 2122-2132. The Association for Computational Linguistics,
2016a. doi: 10.18653/v1/d16-1230. URL https://doi.org/10.18653/v1/d16-1230.
Siqi Liu, Zhenhai Zhu, Ning Ye, Sergio Guadarrama, and Kevin Murphy. Optimization of image
description metrics using policy gradient methods. CoRR, abs/1612.00370, 2016b. URL http:
//arxiv.org/abs/1612.00370.
Beren Millidge, Alexander Tschantz, Anil Seth, and Christopher Buckley. Understanding the origin
of information-seeking exploration in probabilistic objectives for control, 2021.
Andriy Mnih and Karol Gregor. Neural variational inference and learning in belief networks. In
Eric P. Xing and Tony Jebara (eds.), Proceedings of the 31st International Conference on Machine
Learning, volume 32 of Proceedings of Machine Learning Research, pp. 1791-1799, Bejing, China,
22-24 Jun 2014. PMLR. URL http://proceedings.mlr.press/v32/mnih14.html.
Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Alex Graves, Ioannis Antonoglou, Daan
Wierstra, and Martin A. Riedmiller. Playing atari with deep reinforcement learning. CoRR,
abs/1312.5602, 2013. URL http://arxiv.org/abs/1312.5602.
Subhajit Naskar, Pedram Rooshenas, Simeng Sun, Mohit Iyyer, and A. McCallum. Energy-
based reranking: Improving neural machine translation using energy-based models. ArXiv,
abs/2009.13267, 2020.
13
Under review as a conference paper at ICLR 2022
Art B. Owen. Importance Sampling. In Monte Carlo theory, methods and examples, chapter 9. 2013.
URL https://statweb.stanford.edu/~owen/mc/Ch-var-is.pdf.
Tetiana Parshakova, Jean-Marc Andreoli, and Marc Dymetman. Global Autoregressive Mod-
els for Data-Efficient Sequence Learning. In Proceedings of the 23rd Conference on Com-
Putational Natural Language Learning (CoNLL), pp. 900-909, Hong Kong, China, Novem-
ber 2019a. Association for Computational Linguistics. doi: 10.18653/v1/K19-1084. URL
https://www.aclweb.org/anthology/K19-1084.
Tetiana Parshakova, Jean-Marc Andreoli, and Marc Dymetman. Distributional Reinforcement
Learning For Energy-Based Sequential Models. CoRR, 2019b. URL https://arxiv.org/abs/
1912.08517.
Ramakanth Pasunuru and Mohit Bansal. Reinforced video captioning with entailment rewards. In
Martha Palmer, Rebecca Hwa, and Sebastian Riedel (eds.), Proceedings of the 2017 Conference
on EmPirical Methods in Natural Language Processing, EMNLP 2017, CoPenhagen, Denmark,
SePtember 9-11, 2017, pp. 979-985. Association for Computational Linguistics, 2017. doi:
10.18653/v1/d17-1103. URL https://doi.org/10.18653/v1/d17-1103.
Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan,
Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas
Kopf, Edward Yang, Zachary DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy,
Benoit Steiner, Lu Fang, Junjie Bai, and Soumith Chintala. Pytorch: An imperative style,
high-performance deep learning library. In H. Wallach, H. Larochelle, A. Beygelzimer,
F. d'Alche-Buc, E. Fox, and R. Garnett (eds.), Advances in Neural Information Processing Systems
32, pp. 8024-8035. Curran Associates, Inc., 2019. URL http://papers.neurips.cc/paper/
9015-pytorch-an-imperative-style-high-performance-deep-learning-library.
pdf.
Romain Paulus, Caiming Xiong, and Richard Socher. A deep reinforced model for abstractive sum-
marization. In 6th International Conference on Learning RePresentations, ICLR 2018, Vancouver,
BC, Canada, APril 30 - May 3, 2018, Conference Track Proceedings. OpenReview.net, 2018. URL
https://openreview.net/forum?id=HkAClQgA- .
Jan Peters and Stefan Schaal. Reinforcement learning of motor skills with policy gradients. Neural Net-
works, 21(4):682-697, 2008. ISSN 0893-6080. doi: https://doi.org/10.1016/j.neunet.2008.02.003.
URL https://www.sciencedirect.com/science/article/pii/S0893608008000701.
Robotics and Neuroscience.
Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. Language
models are unsupervised multitask learners. OPenAI Blog, 1(8):9, 2019.
Marc’Aurelio Ranzato, Y-Lan Boureau, Sumit Chopra, and Yann LeCun. A unified energy-based
framework for unsupervised learning. In Marina Meila and Xiaotong Shen (eds.), Proceedings of
the Eleventh International Conference on Artificial Intelligence and Statistics, AISTATS 2007, San
Juan, Puerto Rico, March 21-24, 2007, volume 2 of JMLR Proceedings, pp. 371-379. JMLR.org,
2007. URL http://proceedings.mlr.press/v2/ranzato07a.html.
Marc’Aurelio Ranzato, Sumit Chopra, Michael Auli, and Wojciech Zaremba. Sequence level training
with recurrent neural networks. In Yoshua Bengio and Yann LeCun (eds.), 4th International
Conference on Learning RePresentations, ICLR 2016, San Juan, Puerto Rico, May 2-4, 2016,
Conference Track Proceedings, 2016. URL http://arxiv.org/abs/1511.06732.
Veselin Raychev, Pavol Bielik, and Martin Vechev. Probabilistic model for code with decision trees.
SIGPLAN Not., 51(10):731-747, 2016. ISSN 0362-1340. doi: 10.1145/3022671.2984041. URL
https://doi.org/10.1145/3022671.2984041.
Baptiste Roziere, Marie-Anne Lachaux, Lowik Chanussot, and Guillaume Lample. Unsupervised
translation of programming languages. Advances in Neural Information Processing Systems, 33,
2020.
John Schulman, Sergey Levine, Pieter Abbeel, Michael Jordan, and Philipp Moritz. Trust region
policy optimization. In International conference on machine learning, pp. 1889-1897, 2015.
14
Under review as a conference paper at ICLR 2022
John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy
optimization algorithms. CoRR, abs/1707.06347, 2017a. URL http://arxiv.org/abs/1707.
06347.
John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy
optimization algorithms. arXiv preprint: 1707.06347, 2017b.
Emily Sheng, Kai-Wei Chang, Premkumar Natarajan, and Nanyun Peng. The woman worked
as a babysitter: On biases in language generation. In Kentaro Inui, Jing Jiang, Vincent Ng,
and Xiaojun Wan (eds.), Proceedings of the 2019 Conference on Empirical Methods in Natural
Language Processing and the 9th International Joint Conference on Natural Language Processing,
EMNLP-IJCNLP 2019, Hong Kong, China, November 3-7, 2019, pp. 3405-3410. Association for
Computational Linguistics, 2019. doi: 10.18653/v1/D19-1339. URL https://doi.org/10.
18653/v1/D19-1339.
David Silver, Satinder Singh, Doina Precup, and Richard S. Sutton. Reward is enough. Artificial
Intelligence, 299:103535, 2021. ISSN 0004-3702. doi: https://doi.org/10.1016/j.artint.2021.103535.
URL https://www.sciencedirect.com/science/article/pii/S0004370221000862.
Nisan Stiennon, Long Ouyang, Jeff Wu, Daniel M. Ziegler, Ryan Lowe, Chelsea Voss, Alec Radford,
Dario Amodei, and Paul F. Christiano. Learning to summarize from human feedback. CoRR,
abs/2009.01325, 2020. URL https://arxiv.org/abs/2009.01325.
Richard S. Sutton. Temporal credit assignment in reinforcement learning. PhD thesis, University of
Massachusetts, 1984.
Richard S. Sutton and Andrew G. Barto. Reinforcement Learning: An Introduction. The MIT Press,
second edition, 2018. URL http://incompleteideas.net/book/the-book-2nd.html.
Richard S. Sutton, David McAllester, Satinder Singh, and Yishay Mansour. Policy gradient methods
for reinforcement learning with function approximation. In Proceedings of the 12th International
Conference on Neural Information Processing Systems, NIPS’99, pp. 1057-1063, Cambridge, MA,
USA, 1999. MIT Press.
Pradyumna Tambwekar, Murtaza Dhuliawala, Lara J. Martin, Animesh Mehta, Brent Harrison, and
Mark O. Riedl. Controllable neural story plot generation via reward shaping. In Sarit Kraus (ed.),
Proceedings of the Twenty-Eighth International Joint Conference on Artificial Intelligence, IJCAI
2019, Macao, China, August 10-16, 2019, pp. 5982-5988. ijcai.org, 2019. doi: 10.24963/ijcai.
2019/829. URL https://doi.org/10.24963/ijcai.2019/829.
EmanUel Todorov. Linearly-SOlvable markov decision problems. In B. Scholkopf,
J. Platt, and T. Hoffman (eds.), Advances in Neural Information Processing Systems, vol-
Ume 19. MIT Press, 2007. URL https://proceedings.neurips.cc/paper/2006/file/
d806ca13ca3449af72a1ea5aedbed26a-Paper.pdf.
LifU TU, Richard YUanzhe Pang, Sam Wiseman, and Kevin Gimpel. Engine: Energy-based inference
networks for non-aUtoregressive machine translation. ArXiv, abs/2005.00850, 2020.
Ramakrishna Vedantam, C. Lawrence Zitnick, and Devi Parikh. Cider: ConsensUs-based image
description evalUation. In IEEE Conference on Computer Vision and Pattern Recognition, CVPR
2015, Boston, MA, USA, June 7-12, 2015, pp. 4566-4575. IEEE CompUter Society, 2015. doi:
10.1109/CVPR.2015.7299087. URL https://doi.org/10.1109/CVPR.2015.7299087.
Lex Weaver and Nigel Tao. The optimal reward baseline for gradient-based reinforcement learning.
In Proceedings of the Seventeenth Conference on Uncertainty in Artificial Intelligence, UAI’01, pp.
538-545, San Francisco, CA, USA, 2001. Morgan KaUfmann PUblishers Inc. ISBN 1558608001.
Ronald J. Williams. Reinforcement-learning connectionist systems. Technical report, Northeastern
University, 1987. Technical Report NU-CCS-87-3.
Ronald J. Williams. Simple statistical gradient-following algorithms for connectionist reinforcement
learning. Mach. Learn., 8:229-256, 1992a. doi: 10.1007/BF00992696. URL https://doi.
org/10.1007/BF00992696.
15
Under review as a conference paper at ICLR 2022
Ronald J. Williams. Simple statistical gradient-following algorithms for connectionist reinforcement
learning. In Machine Learning, pp. 229-256, 1992b.
Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi,
Pierric Cistac, Tim Rault, Remi Louf, Morgan Funtowicz, and Jamie Brew. HUggingface's
transformers: State-of-the-art natural language processing. CoRR, abs/1910.03771, 2019. URL
http://arxiv.org/abs/1910.03771.
Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V. Le, Mohammad Norouzi, Wolfgang Macherey,
Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, Jeff Klingner, Apurva Shah, Melvin Johnson,
Xiaobing Liu, Lukasz Kaiser, Stephan Gouws, Yoshikiyo Kato, Taku Kudo, Hideto Kazawa, Keith
Stevens, George Kurian, Nishant Patil, Wei Wang, Cliff Young, Jason Smith, Jason Riesa, Alex
Rudnick, Oriol Vinyals, Greg Corrado, Macduff Hughes, and Jeffrey Dean. Google’s neural
machine translation system: Bridging the gap between human and machine translation. CoRR,
abs/1609.08144, 2016. URL http://arxiv.org/abs/1609.08144.
Yaoming Zhu, Sidi Lu, Lei Zheng, Jiaxian Guo, Weinan Zhang, Jun Wang, and Yong Yu. Texygen:
A benchmarking platform for text generation models. In Kevyn Collins-Thompson, Qiaozhu
Mei, Brian D. Davison, Yiqun Liu, and Emine Yilmaz (eds.), The 41st International ACM SIGIR
Conference on Research & Development in Information Retrieval, SIGIR 2018, Ann Arbor, MI,
USA, July 08-12, 2018, pp. 1097-1100. ACM, 2018. doi: 10.1145/3209978.3210080. URL
https://doi.org/10.1145/3209978.3210080.
Daniel M. Ziegler, Nisan Stiennon, Jeffrey Wu, Tom B. Brown, Alec Radford, Dario Amodei, Paul
Christiano, and Geoffrey Irving. Fine-tuning language models from human preferences. CoRR,
abs/1909.08593, 2019. URL http://arxiv.org/abs/1909.08593.
16
Under review as a conference paper at ICLR 2022
A	Extended Related Work
Reinforcement learning for language generation Most previous attempts at steering language
models to conform to global constraints defined over entire sequences have employed reinforcement
learning. This includes using Reinforce (Williams, 1992a) for machine translation Ranzato et al.
(2016), actor critic (Konda & Tsitsiklis, 2000) for abstractive summarization (Paulus et al., 2018),
caption generation (Liu et al., 2016b), dialogue (Li et al., 2016b), and video captioning (Pasunuru &
Bansal, 2017). Some approaches (for instance, in machine translation and summarization (Ranzato
et al., 2016; Bahdanau et al., 2017)) directly optimize performance metrics such as BLEU and ROUGE
at training time. Others use heuristic rewards (for instance Li et al. (2016b) for dialogue generation
and Tambwekar et al. (2019) for story generation) in order to obtain certain a priori desirable features
of generated sequences that then incentivize good performance on target metrics. Catastrophic
forgetting is a frequent problem of these fine-tuning approaches: reward maximization happens
at the expense of large deviations from the original model. This problem is sometimes addressed
by imposing a penalty term to the rewards, such as the KL divergence between the trained policy
and the auto-regressive model. This approach, termed “conservative fine-tuning", was applied to
generating melodies with music theory rewards and organic molecules with synthesizability rewards
by Jaques et al. (2017a) as well fine-tuning language models for controllable language generation
by Ziegler et al. (2019). This solution often has hard time balancing between the reward term and
the KL penalty term, leading to instability in training (Khalifa et al., 2021). Unlike this approach,
KL-DPG determines an optimal distribution that satisfies both requirements.
RM and DM objectives in control problems While RM is the dominant approach to tackling
control problems (Sutton & Barto, 2018) and is sometimes argued to be sufficient for any intelligent
behavior (Silver et al., 2021), prior work explored the benefits of alternative objectives formulated
as DM: minimizing divergence from some target distribution p. Prominent examples of (families
of) DM objectives for control include active inference (Friston et al., 2010; Buckley et al., 2017)
and control-as-inference (Kappen et al., 2012; Todorov, 2007; Levine, 2018). Hafner et al. (2020)
propose a reverse KL from a joint distribution over observations and latent variables as a universal
objective for action and perception that — depending on a choice of the target p — gives rise to many
familiar objectives, including empowerment (Klyubin et al., 2005), maximum entropy RL (Haarnoja
et al., 2017) or KL-control (Todorov, 2007). In a similar vein, Millidge et al. (2021) compare RM and
DM objectives (or, evidence and divergence objectives, according to their terminology) in the context
of exploration. They conclude that information-seeking exploration arises naturally in DM but not
in RM. This is because, when the target distribution p involves latent variables, a DM objective
decomposes into an information gain term that pushes the agent to seek observations that are most
informative of latent variables. In contrast, RM objectives entail minimizing information gain between
latent variables and observations.
Baselines in Reinforcement Learning In the context of reinforcement learning, baselines were
introduced by Sutton (1984). Williams (1987; 1992a) has shown them to reduce variance in a number
of use cases and also proved that they do not introduce bias. Dayan (1990) was the first to observe
and confirm experimentally that the optimal constant baseline is not equal to expected reward in
a simple two-arm bandit setting. This result was generalized to POMDPs (Partially Observable
Markov Decision Processes) by Weaver & Tao (2001, section 3.1.3, p. 540) and variable baselines by
Greensmith et al. (2004, theorem 13, p. 1489) who also proved bounds on the variance of gradient
estimates. The optimal baseline, however, is rarely used in practice (Sutton & Barto (2018); for an
exception, see (Peters & Schaal, 2008)). Outside RL, baselines were also used in the context of
learning inference networks for amortized variational inference by Mnih & Gregor (2014) and found
to yield similar variance reduction.
Energy-based models for language Energy-based models (EBMs) (Hinton, 2002; LeCun et al.,
2006; Ranzato et al., 2007) are a family of models in which learning and inference are done by
associating an unnormalized probability with each configuration of observed and latent variables.
Early examples of EBMs applied to natural language processing include sequence labeling problems
(e.g. tagging) exploiting global properties of a sequence (Andor et al., 2016; Belanger & McCallum,
2016). The recent surge of interest in EBMs has not left natural language processing unaffected (see
Bakhtin et al. (2020) for a survey). Tu et al. (2020) proposed an energy-based inference networks
17
Under review as a conference paper at ICLR 2022
for non-autoregressive machine translation while Naskar et al. (2020) use an EBM for reranking
candidate translations according to their predicted BLEU scores. Parshakova et al. (2019a) and
Deng et al. (2020) augment an autoregressive language models with an additional global factor to
obtain a lower perplexity on the training data. Clark et al. (2020) poses non-autoregressive language
modeling as training an energy-based cloze task scorer using noise-contrastive estimation (Gutmann
& Hyvarinen, 2010). He et al. (2021) obtain better calibration on natural language inference tasks by
augmenting and training the classifier jointly with an energy-based model modeling the marginal
distribution over samples, again using noise-contrastive estimation. In consequence, the classifier
tends to assign more conservative (high-entropy) predictions to high-energy (less likely, possibly out
of distribution) samples.
B Additional proofs
B.1 Optimal baselines in RL
Despite its widespread use, the baseline as mean of reward
BRL = Eχ~∏θ(χ)R(x)
(23)
is not the optimal constant baseline for reward maximization objectives in RL. The optimal constant
baseline, i.e. one yielding the minimal variance of the gradient, is given by:
B* = Eχ~∏θ[R(χ) (Vθ logπ(χ))2]
―	Ex~∏θ[(Vθ log ∏ (x))2]
(24)
In order to maintain accessibility, in this section, we provide a self-contained derivation of this
optimal form of baselines (24) and and connect it to the commonly used form (23).12
First, recall that R(x) is a reward associated with an input x. B is a baseline value subtracted from
the reward that does not introduce bias in gradient estimation. Now let’s denote the gradient wrt an
individual sample x as Gθ (x) where
Gθ(x) = [R(x) - B]Vθ logπθ(x),	(25)
and the estimate of the gradient as
G(θ)= Eχ~∏θ Gθ (x).
(26)
Using the general identity var(z) = E[z2] - [Ez]2, the variance of the gradient takes the form:
Var(Gθ) = Ex~∏θ[Gθ(x)2] - G(θ)2
(27)
Now let’s take the gradient of this variance with respect to B and solve to find the baseline form with
minimal variance:
---ɪ-θɪ = 赤Ex~∏θ[(Gθ (X))2]-赤(Ex~∏θ[Gθ (X)D2 .	(28)
dB dB	dB
The second term of the right hand side of (28) is equal to zero, since B does not introduce bias into
G(θ):
d
(Ex (Ex~∏θ[Gθ(X)D
dB
d
-E (Ex-∏θ [(R(x) - ByVθ logπ(x)])
dB
W (Ex~∏θ [R(x)Vlog∏(x)])2 = 0.
dB
12The formula for the optimal baseline in (24) was originally proved by Weaver & Tao (2001) but here we
provide a simpler proof sketched by Sergey Levine in his slides: http://rail.eecs.berkeley.edu/
deeprlcourse- fa17/f17docs/lecture_4_policy_gradient.pdf
18
Under review as a conference paper at ICLR 2022
Plugging this back into (28), we obtain:
dvar(Gθ)=ɪ Ex 〜∏θ[0 (x))2]
dB dB
=Ex〜∏θ ]dB [(R(x)2 + B2 - 2R(x)B) (Vθ log∏θ(X)H
=Ex〜∏θ(2B - 2R(x))(Vθ logπθ(x))2
=2B Ex〜.. (Vθ logπθ(x))2 - 2 Ex〜.0R(X) (Vθ logπθ(x))2.
Then, solving dVdBG) = 0 for B, We obtain the optimal form of the baseline B* as required:
B* = Ex〜∏θ[R(x) (Vθlog∏θ(x))2]
Ex 〜∏θ[(Vθlog πθ (x))2]
(29)
This can be interpreted as average reWard (as in BRL) but Weighted by gradient magnitudes
(Vθlogπθ(x))2. Moreover, B* = BRL is recovered under the condition that the reWard R(x)
is uncorrelated (a fortiori independent) from (Vθ log πθ(x))2. If that Were the case, We Would have:
B* = Ex 〜∏θ[R(x)(Vθ log ∏θ (x))2]
一	Ex 〜∏θ[(Vθ log πθ(x))2]
=Ex〜∏θ[R(x)] Ex〜∏θ[(Vθlog∏θ(x))2]
一	Ex 〜∏θ[(Vθ log πθ (x))2]
=Ex 〜∏θ[R(x)] = BRL.
(30)
(31)
(32)
B.2 unbiasedness of PG baseline
Baselines are a standard variance reduction technique in the context of Policy Gradients (Sutton &
Barto, 2018). The idea is to subtract from the reWard R(x) a value B that does not introduce bias to
the gradients but may change variance. Equation (1) then takes the folloWing form:
VθE.θR(x) =E.θ(R(x) -B)Vθlogπθ(x).	(33)
To see that B does not introduce bias, We can reWrite (12) as:
Ex〜∏θ R(X)Vθ log ∏θ (X)- B En. Vθ log ∏ (x)	(34)
and note that the second term is null because Px πθ(x)Vθlogπθ(x) = Vθ Px πθ(x) = 0.
B.3	Unbiasedness of DPG Baseline
Recall that the gradient estimate for DPG (Parshakova et al., 2019a) has the folloWing form:
P(X)
Ex〜∏Θ v~ʌ vΘ log πθ (X)
. πθ (X)
After subtracting a baseline B = Z, it becomes
P(X)
Ex〜∏θ [∏ (χ) - Z vθ log πθ (X) = Ex
:〜∏θ
Ex 〜∏θ
P(X)
∏8 (x) Vθ log ∏θ (x) - Z [Ex〜∏θ Vθ log ∏θ (x)J
P(x) Vθ log ∏θ (x) 一 Z h X Vθ∏θ (x)i
πθ(X)	x
(35)
(36)
(37)
Here, the second term does not introduce bias because Z
same exact form of gradient as in the DPG algorithm.
PxVθπθ(X)
0, leaving us With the
19
Under review as a conference paper at ICLR 2022
B.4	Unbiasedness of DPGOFF baseline
Offline DPG, the off policy variant of DPG proposed in Parshakova et al. (2019b); Khalifa et al.
(2021) has the following gradient estimate:
P(x)
Eχ~q-7^^∖Rθlog πθ(X)	(38)
q(x)
Where q is a proposal distribution (another auto-regressive model) used to detach the training of πθ
from the sampling process and allow more stable training.
Recall that the Baseline of DPGoff is of the form:
Boff(X) = Z 坐),	(39)
q(x)
The πq(X)) term is an importance weight correcting for the bias introduced by sampling from q.
Unbiasedness To show that subtracting a baseline Boff(χ) = Z∏θXχ) doesn,t introduce bias, let's
rewrite the gradient estimate with added baseline as a sum of two terms:
Eχ~qhPP((χ) -Z∏θXx)ivθlog∏θ(X)= [Eχ~qPx)vθlog∏θi - [Eχ~qZπ(Xχ)∏]
(40)
vθlogπθi -ZhXvθπθ(X)i	(41)
Here again the second term does not introduce bias because Z Px vθπθ (X) = 0.
Null Advantage on Average In the case of sampling with πθ in the online DPG choosing B = Z
had the benefit that the advantage Rθ (x) 一 B was centered around 0, namely: Eχ~∏g [Rθ (x) 一 Z] = 0.
With the Boff(X) baseline for the DPGoff this important property is also maintained. The advantage
nc∖x7 1^αlταc 1^kα frvt*m P(X)   r7 πθ(X) CJnrI than，
now takes the form q(χ)	Z q(x) and then：
Ex~qh Px 一 Zπq⅛r i = X P(X)- Zπθ(X)	(42)
= Z - Z X πθ (X) = 0.	(43)
X
To visualize things better, we elaborate the difference in forms of rewards, baseline and gradients
before and after addition of the baseline between DPG (on policy) and DPGoff (off policy) in Table 2.
20
Under review as a conference paper at ICLR 2022
	DPG	DPGoff
Reward	P (X) ∏θ (χ)	P (X) q(X)
Vθ	Ex~∏θ P(X)) vθ log πθ (X)	Eχ~qp⅛) Vθ log ∏θ(x)
Baseline	Z	πθ(X) / q(X)
Advantage	P(X) - z ∏θ(x)	P(x) _ 7∏θ(x) i(Xy - Z ^q(XΓ
Vθ with baseline	Eχ~∏g [∏P((Xχ) - Zi Vθ log ∏θ(x)
Eχ~q[PP(χ) - ZπθX))i vθlog πθ(X)
Table 2: A comparison of Online DPG and Offline DPG (DPGoff ) forms of Reward, Baseline, Advantage, and
Gradient of the loss function (the PG-term) before (Vθ) and after (Vθ with Baseline) including a baseline for
variance reduction.
C Code generation with compilability constraints experiments
C.1 Experimental setup
Energy-based model We represent a language model producing only compilable sequences as the
following product-of-experts (Hinton, 2002) EBM:
P (x) = a(x)b(x),	(44)
where a is the original language model pre-trained using a standard autoregressive language modeling
objective and b(x) = 1 iff x is a syntactically correct Python program and b(x) = 0 otherwise.
Dataset In contrast with experiments with GPT-2, we trained a custom language model to obtain a.
To prepare the training dataset for a, we started from the Python150 dataset, which consists of 150k
Python source code files obtained from GitHub Raychev et al. (2016). We extracted 713k Python
functions (both methods and standalone functions) from 150k using the code from Roziere et al.
(2020) while filtering out functions that didn’t compile (b(x) = 0) or were less than 128 BPE tokens
long. We then split the dataset into a training subset Dtrain and test subset Dtest.
Initial language model a: We implemented a using the GPT-2 Radford et al. (2019) architecture
with 117m parameters (gpt2-small). First, we used Dtrain to train a byte-level BPE tokenizer. We
included two special tokens, BOS and EOS, and obtained a vocabulary of 50k tokens. Then, we
trained a on Dtrain for one epoch.
Compilability Scorer b We evaluate whether a sample x is compilable by first removing BOS and
EOS tokens and then calling the compile_command function from codeop module of Python
Standard Library13 with x as the argument. compile_command tries to compile a string of Python
code and raises and exception if there is it fails (e.g. raises SyntaxError for invalid Python syntax
and ValueError or OverflowError if there is an invalid literal in x). If compile_command
returns a code object, b(x) = 1. Otherwise (if an exception is raised or None is returned), b(x) = 0.
Note that our notion of compilability is concerned only with syntactic correctness and does not
execute the body of a function.
C.2 Metrics
In addition to Eχ~∏gb(x), DKL (p, ∏θ), Dkl (∏θ, a), Distinct-1 (Li et al., 2016a)and Self-BLEU-5
(Zhu et al., 2018), we report the following metrics:
1.	Perplexity measured on Dtest, a held-out subset of the data used for training a, calculated as
exp h- N X log πθ(X)i,
x∈Dtest
13https://docs.python.org/3/library/codeop.html
21
Under review as a conference paper at ICLR 2022
where N is the total number of tokens in Dtest.
2.	Sequence length, the average number of characters in generated sequence x after detokeniza-
tion,
3.	AST node count, the average number of nodes in an abstract syntax tree (AST) of sequences
that compile. Samples are parsed to their corresponding ASTs using the ast module from
Python Standard Library.14 Intuitively, this metric indicates the logical (as opposed to
surface) complexity of generated programs.
C.3 Results
We report the performance of GDC and GDC++ as well as Reinforce on Table 3.
Reinforce with R(x) = b(x) improves compilability but that comes at a cost of large divergence from
p and a. This divergence translates into a decrease in sequence length and logical complexity (in
terms of the number of nodes in ASTs of generated sequences). Heavily decreased sequence length
(most of the generated functions are one-liners) accounts for an artificial increase in diversity metrics
(Self-BLEU-5 and Distinct-1).
GDC and GDC++ are the only method that consistently improve compilability rate while decreasing
divergence from p, maintaining the diversity of a and only slightly decreasing sequence length and the
number of nodes in ASTs. Moreover, as a by-product of improving compilability, GDC and GDC++
are also able to slightly decrease the perplexity and the frequency of PEP8 violations per character.
The addition of baseline in GDC++ improves its performance in terms of constraint satisfaction, KL
divergences and downstream metrics (e.g. lower Self-BLEU-5, higher Distinct-1).
	Ctrl. (↑)	KL(p,∏) (J)	KL(∏,a) (J)	Dist-1 (↑)	SB-5 (J)	AST	Length	PPL (J)
Original LM	0.55	0.58	0.00	0.37	0.88	31.40	156.70	8.72
Reinforce	0.89	77.49	93.26	0.52	0.79	13.21	60.23	9.32
GDC	0.68	0.48	0.15	0.36	0.89	26.16	125.83	8.69
GDC++	0.69	0.46	0.13	0.36	0.88	25.93	124.20	8.70
Table 3: Evaluation of GDC (Khalifa et al., 2021), GDC++ (ours) and Reinforce for python code generation
under compilability constraints. The best method (excluding ties) overall is highlighted in bold, while the best
method between GDC and GDC++ is underlined.
14https://docs.python.org/3/library/ast.html
22
Under review as a conference paper at ICLR 2022
D Extra details on metrics and Algorithms
Calculation of metrics relative to p, such as DKL(p, πθ), is not straightforward since the distribution
P H P is only implicitly represented by the unnormalized EBM P, and one cannot easily obtain direct
samples from p. Instead, we apply the following workarounds. Given P and a proposal distribution
q that we can sample from, using importance sampling (Owen, 2013), we calculate the partition
function Z as follows:
Z =	P(x) =	q(x) P(x)/q(x)
=Ex 〜q P (x)/q(x).
(45)
(46)
The precision of this estimate depends on the sample size and the quality of the proposal distribution
q. We calculate a moving average estimate ZMA of Z which is then used inside the estimations of
DKL(p, πθ) and DKL(p, q) (see below Algorithm 2, lines 7 and 8). ZMA is updated at each training
iteration. ZMA is an unbiased estimate of Z because each Zi is an unbiased estimate of Z based
on K samples. Moreover, because the proposal distribution q evolves and gets closer to the target
distribution p, the quality of the estimate of ZMA through importance sampling increases.
With an estimate of Z, we can compute DKL (p, πθ) as
DKL(P,πθ) = XP(X)Iog -p(x) x	πθ(x)				(47)
=X p(x)log ZPx))				(48)
=-log Z + Xp(x) log Px) x	πθ(x)				(49)
=-log Z + X q(χ)P(X)Iog PxI g + 乙 q(X) q(χ) log ∏θ (χ) x				(50)
=—log Z + 1E Z		P(X)	PxI x~q q(X)	g ∏θ (X).		(51)
Similarly, for TVD(P, πθ):				
TVD(P,πθ) = 1 X IP(X)		—∏θ (x)|		(52)
x =2 X q(X) x		∏θ (x) _ P(X) I q(X)	q(X) I		(53)
=2 X q(X) x		πθ(X)	P(X) — I q(X)	Z q(X)		(54)
=2 Ex 〜q	∏θ (x) _ P(x) I q(X)	Zq(X) I .			(55)
See Algorithm 2 for a detailed pseudocode describing how metric computation is integrated in the
training loop of KL-DPG.
23
Under review as a conference paper at ICLR 2022
Algorithm 2 KL-DPG with baseline (detailed)
Require: P , initial policy q
1： ∏θ - q
2： ZMA - 0
3： for each iteration i do
4： for each step k ∈ [1, K] do
5:	sample Xk from q(∙)
(θ) P(xk)	πθ(xk)
6:	θ Jθ + α()[irχkj - Z -θχkτ [Vθ log πθ (Xk)
7:	Zi J k1 Pk P(Xk)∕q(xk)
8:	ZMA J i*ZMA+Zi
9:	DDKL(p,	∏θ)	J--log ZMA +	1/(KZMA)	P⅛	P(Xk))	log	P((Xk))
KL , θ	MA	MA	k q(xk )	πθ (xk )
P(Xk)	P(Xk)
10:	Dkl(P, q) J-----log ZMA + 1 / (K ZMA) / L k k∖ log -Γ∖
KL ,	MA	MA k q(Xk) q(Xk)
11:	if DKL (p, πθ) < DKL (p, q) then
12:	q J πθ
Ensure: πθ
E Hyperparameters and training details
We implemented all models using PyTorch (Paszke et al., 2019) and HuggingFace (Wolf et al., 2019).
Based on Khalifa et al. (2021) published source code: https://github.com/naver/gdc. Each
training run took approximately 5 days on 2 Nvidia V100 GPUs. For a detailed list of hyperparameter
values, see Table 4; for a description of hyperparameters specific to Ziegler and GDC, see (Ziegler
et al., 2019) and (Khalifa et al., 2021).
Hyperparameter	Value
batch size	Common 512
sequence length	40 tokens
learning rate	1.41 × 10-5
dropout rate	0.1
optimizer	Adam (Kingma & Ba, 2014)
warmup epochs	100
total epochs	4500
base LM	GPT-2 small (117M params)
GDC sample size for learning λ 10240	
learning rate for λ	0.5
tolerance for λ	0.01
γ	Ziegler 1
λ	0.95
clip range	0.2
target KL	6.0
initial KL coefficient	0.2
horizon	104
Table 4: Hyperparameters used throughout all experiments.
24
Under review as a conference paper at ICLR 2022
F Extended evaluation
	Method	Ctrl (↑)	Fluency		Sentence Level Diversity			CorPus Level Diversity	
			KL(p,π)())	KL(Pi,a)⑷	Dist-1 (↑)	Dist-2 (↑)	Dist-3 (↑)	SB-4 ⑷	SB-5(φ)
Pointwise Constraints Experiments									
Word Amazing	Original LM	0.00	6.02	0.00	0.86	0.94	0.92	0.89	0.82
	Reinforce	1.00	134.31	78.39	0.69	0.91	0.94	0.98	0.96
	Ziegler	0.82	4.56	5.88	0.86	0.95	0.94	0.94	0.88
	GDC	0.65	2.57	5.06	0.86	0.95	0.94	0.93	0.87
	GDC++ (Ours)	0.69	2.10	4.74	0.87	0.95	0.94	0.93	0.87
Word WikiLeaks	Original LM	0.00	8.54	0.00	0.86	0.94	0.92	0.89	0.80
	Reinforce	1.00	8.00	117.24	0.38	0.56	0.64	0.98	0.97
	Ziegler	0.68	0.00	6.03	0.87	0.96	0.94	0.95	0.90
	GDC	0.75	3.22	7.96	0.88	0.96	0.94	0.95	0.90
	GDC++ (Ours)	0.77	2.21	7.53	0.88	0.96	0.94	0.95	0.91
Wordlist Science	Original LM	0.06	2.79	0.00	0.86	0.94	0.92	0.89	0.81
	Reinforce	1.00	140.02	66.68	0.29	0.41	0.49	0.98	0.97
	Ziegler	1.00	6.1	5.88	0.86	0.95	0.93	0.95	0.90
	GDC	0.52	2.27	2.89	0.86	0.95	0.93	0.93	0.87
	GDC++ (Ours)	0.54	1.78	2.11	0.86	0.95	0.93	0.92	0.86
Wordlist Politics	Original LM	0.07	2.65	0.01	0.86	0.94	0.92	0.89	0.81
	Reinforce	1.00	263.79	65.06	0.26	0.40	0.51	0.98	0.97
	Ziegler	1.00	8.46	5.92	0.87	0.96	0.94	0.96	0.92
	GDC	0.58	2.70	2.49	0.87	0.96	0.94	0.93	0.88
	GDC++ (Ours)	0.49	2.01	1.35	0.87	095	093	0.93	0.87
+ve Sentiment	Original LM	0.17	2.06	0.01	0.86	0.94	0.93	0.89	0.81
	Reinforce	1.00	153.75	80.07	0.27	0.37	0.41	0.97	0.95
	Ziegler	0.98	5.70	5.98	0.85	0.96	0.94	0.96	0.91
	GDC	0.59	1.68	1.89	0.86	0.95	0.94	0.93	0.87
	GDC++ (Ours)	0.60	1.67	1.88	0.86	0.95	0.94	0.93	0.87
-ve Sentiment	Original LM	0.13	2.14	0.01	0.86	0.94	0.92	0.90	0.82
	Reinforce	1.00	88.48	70.38	0.83	0.96	0.94	0.97	0.93
	Ziegler	0.95	6.12	6.00	0.84	0.95	0.94	0.96	0.92
	GDC	0.52	1.72	1.79	0.86	0.95	0.94	0.94	0.88
	GDC++ (Ours)	0.51	1.66	1.63	0.86	0.95	0.94	0.93	0.88
Distributional Constraints Experiments									
Single	Original LM	0.19	0.39	0.01	0.90	0.95	0.92	0.94	0.90
	GDC	0.80	0.74	0.71	0.89	0.95	0.92	0.95	0.90
	GDC++ (Ours)	0.81	0.33	0.66	0.89	0.95	0.92	0.94	0.90
Multiple	Original LM	0.49	0.40	0.00	0.90	0.95	0.92	0.94	0.90
	GDC	0.92	0.53	0.85	0.90	0.95	0.92	0.95	0.90
	GDC++ (Ours)	0.95	0.30	0.76	0.90	0.95	0.92	0.95	0.90
Hybrid Sports	Original LM	0.22	0.20	0.00	0.90	0.95	0.92	0.94	0.90
	GDC	0.87	0.24	2.65	0.93	0.95	0.92	0.96	0.92
	GDC++ (Ours)	0.85	0.87	2.35	0.93	0.95	0.92	0.96	0.92
Hybrid Science	Original LM	0.09	0.00	0.00	0.90	0.95	0.92	0.94	0.89
	GDC	0.68	1.52	3.92	0.88	0.95	0.91	0.95	0.92
	GDC++ (Ours)	0.70	1.41	3.83	0.88	0.95	0.92	0.95	0.91
Table 5: Evaluation over 6 pointwise constraints experiments (tasks 1-6) and 4 distributional constraints
experiments (tasks 7-10) for policies obtained from GDC++ (ours), GDC, Ziegler and Reinforce. See figures
7-10 in the Appendix for a detailed view on each experiment. Results of the initial policy (Original LM) are
displayed for reference.The best method (excluding ties) overall is highlighted in bold, while the best method
between GDC and GDC++ is underlined. Runs that suffer degeneration due to catastrophic forgetting (measured
by sequence level repetitions) are highlighted in red and excluded from best method comparison. Our method
GDC++ that includes a baseline for variance reduction, outperforms GDC (Khalifa et al., 2021) in 7/10 tasks
in terms of control satisfaction rate (Ctrl), as well as convergence towards the optimal policy (KL(p,π)) and
distance from the original LM (KL(pi,a)) in 10/10 of the tasks.
25
Under review as a conference paper at ICLR 2022
O	Ik 2k
epochs
O	Ik 2k
epochs
O	Ik 2k
epochs
0.92
0.91
i0-90
δ
0.89
0.88
0.920
0.915
u? 0.910
Ik
epochs
O	Ik 2k
epochs
2k
Figure 6: Evaluation metrics: average μ (↑ better), Dkl(p∣∏θ) (] better), Dkl (∏θ |a) (] better), Self-BLEU-5
(]better), and Distinct-1 (↑ better) on aggregated four distributional constraints experiments: Task 7: a single
distributional constraint, Task 8 and Task 9: a two hybrid constraint pairs, Task 10: Multiple Distributional
constraints. For policies obtained from GDC++ and GDC. Average μ was computed for each experiment by
mapping Eχ~qφi(x) for each constraint i onto a [0,1] interval and averaging over constraints. See Figures 9-10
in for a detailed view on each experiment.
(a)
(b)
(c)
word "amazing" (freq IO-3)
word "WikiLeaks" (freq IOT)
wordlist for politics
0 Ik 2k 3k 4k
Ik 2k 3k 4k
0 Ik 2k 3k 4k
0
I”' -ji'∕5f
∙χιo^, ■
0.95
'njlgiφs
0 Ik 2k 3k 4k
epochs
015101lh
9 8 8 7
n31Bs
0 Ik 2k 3k 4k
epochs
'n31Biφs
Tk 2k 3k 4∣Γ
epochs
0 Ik 2k 3k 4k
epochs



Figure 7:	Evaluation metrics E∏θ φ(x), KL(p∣∏θ) (] better), KL(∏θ |a) (] better), Self-BLEU-5 (] better), and
Distinct-1 (↑ better) for three constraints types: Task 1: Word "amazing" Fig.(a), Task 2: Word "wikileaks"
Fig.(b) and Task 3: Wordlist "politics" Fig.(c) for policies obtained from GDC++, GDC, Ziegler and Reinforce.
26
Under review as a conference paper at ICLR 2022
(a)
(b)
(c)
2k 3k 4k
epochs
H.
O Ik 2k 3k 4k
epochs
8
O Ik 2k 3k 4k
O Ik
8 6 4
-Bd-
0 Ik 2k 3k 4k
epochs
8ι
6'	〜Λ<"i~~4-~ j-∙1
§4
0 Ik 2k 3k 4k
0 0 Ik 2k 3k 4k
epochs
0 Ik 2k 3k 4k
epochs
0.95 ʃv-"`mL2、八
0.950
0.925
0.900
0.875
0.850
0.825
0 Ik 2k 3k 4k
epochs
0 Ik 2k 3k 4k
epochs
'njlgiφs
0.80
0 Ik 2k 3k 4k
epochs
'n31Biφs
0.950
0.925
§0.900
%
a? 0.875
S 0.850
0.825
0 Ik 2k 3k 4k
epochs
0 Ik 2k 3k 4k
epochs
8 6 4 2
λk -Bd-
/ʌr i	a /Vi产产
Figure 8:	Evaluation metrics E∏θφ(x), KL(p∣∏θ) (] better), KL(∏θ|a) (] better), Self-BLEU-5 (] better),
and Distinct-1 (↑ better) for three pointwise constraints experiments: Task 4: Wordlist "science" Fig.(a),
Task 5: classifier +ve sentiment Fig.(b) and Task 6: Classifier -ve sentiment Fig.(c) for policies obtained
from GDC++, GDC, Ziegler and Reinforce.
27
Under review as a conference paper at ICLR 2022
3
G
geEeoΛ
0.2-
0.1-
——GDC++
GDC
Tk 2k 3k 4∣Γ
epoch
(a)	Task 7: gender = "Female" 50%
α,-eE,uto"-n
ωucω~uω,a
(c) Task 9: gender = "female" 50%,
topic = "science" 100%
α,-eE,uto"-∙a
0.2
0.2
stods」£a
00V
——GDC++
---GDC
4k
epoch
0.0^-------；-------7-
0 2k 4k
epoch
(b)	Task 8: gender = "female" 50% , topic = "sports"
100%
(d) Task 10: topics = "science" 25%,
"art" 25%, "business" 25%, "sports" 25%
^


Figure 9:	Constraint satisfaction μ (↑ better) for four distributional constraints types: Task 7: a single
distributional constraint Fig.(a). Task 8 and Task 9: a two hybrid constraint pairs Fig.(b) & Fig.(c) Task
10: Multiple Distributional constraints Fig.(d). For policies obtained from GDC++ and GDC. The dashed
Horizontal bars denote the desired moments 口匕.
28
Under review as a conference paper at ICLR 2022
(a)	(b)	(c)	(d)
gender = female
epochs
500.0
Ik Ik
epochs
2k
3 Joplcs = science, art, business, sports
GDC++
GDC
2.5
epochs
2k
500.0 Ik Ik
epochs
0.898
In 0.897
@ 0.896
φ
IΛ
0.895
0.894
0.911
0.910
0.909
0.908
0.907
0.906
0.905
'nτmiφs
500.0 Ik Ik 2k
epochs
'nτmiφs
500.0 Ik Ik 2k	500.0 Ik Ik 2k
epochs	epochs
Figure 10:	Evaluation metrics: KL(p∣∏θ) (] better), KL(∏θ |a) (] better), Self-BLEU-5 (] better), and Distinct-
1 (↑ better) four distributional constraints types: Task 7: a single distributional constraint Fig.(a). Task 8,9: a
two hybrid constraint pairs Fig.(b) and Fig.(c), Task 10: Multiple Distributional constraints Fig.(d), for policies
obtained from GDC++ and GDC.
29
Under review as a conference paper at ICLR 2022
reps I φ(x) Sample
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
0
1
1
1
1
1
1
1
1
1
1
1
GDC++
I recently had an amazing experience at an event with some great friends . We had a special treat and it was a good surprise to find
a group of friends there to celebrate their new band
There are a number of great people who make amazing , sometimes incredibly mundane things that can come in handy for a lot of
people . I’ve been lucky enough to have some very successful and sometimes
"It was an amazing feeling of freedom . " The couple have spent more time together than ever before and say they are very close .
But the couple say they aren’t exactly satisfied
What is this amazing game ? This game is an MMO,not really an MMO,but really a multiplayer MMORPG . Players start with
2-6 heroes and then they level up through
What is Puma (Puma : A Sea , Water , Land) ? Puma is a unique underwater experience where you can get as close to the surface
as you like while exploring amazing underwater
GDC
So my husband is now doing amazing , so he asked me to buy some of my best quality tins . My daughter did the gift for the first
time . I absolutely loved it. It’s
I don’t really want to hear about a video on "A Night in the Sun" because this video was really amazing . The main character is a
crazy young man who has an
"The first time I saw this amazing artwork ,myjaw went up a notch . It’s an incredible piece of art. If I had an idea of what it was
to me I would love
The next time you’re walking through town and someone in the park asks you about your favorite time of the week , just do a
Google search to learn which one will be your favorite day . A
The world’s biggest robot is an amazing , highly complex machine , but its development process is just a small part of how it will
be manufactured . While many robots are already built, others are working
Reinforce
The show which has been getting amazing ones which is amazing now it and which so amazing ! iam a little amazing so amazing
and amazing they so amazing for my gif this amazing one which we are
This which shows which has really amazing and amazing ly it amazing which you have a beautiful highlight and you have so
amazing it this is a really amazing it and amazing . so awesome they get and amazing !
I was able to showcase this amazing thing which is amazing . . thanks so amazing which so amazing it is amazing so amazing haha
amazing as amazing and this amazing product which you will be so amazing it so
The best and amazing one which shows which made amazing it have been so amazing and amazing but I’m really amazing : ) this
awesome which you explode which have made and amazing and amazing with amazing which makes
I had this amazing album which which makes such awesome and awesome as amazing haha this is amazing and amazing . I was
blown so amazing just amazing which gives so amazing so amazing !!!so awesome which
Ziegler
"He’s a real man who wants to keep up the amazing work he’s done and keep things in front of him . He’s been doing it since the
last time he played for Team Australia
"It was amazing to see what we had to do to get our guys to the door . I’m really excited about being back here . We’re looking
forward to the next season , so let
I had this idea for a story this year . I really wanted to tell an amazing tale of something I had been watching from the beginning .
This is what I had , and I was happy with
I found this amazing recipe on the blog and it was so easy and delicious . I have made this many times and this one is just the
perfect combination of recipes for any . Try it and tell
To learn more about these amazing videos and how to watch them in your home , head on over to the video search page . If you
have any questions or would like to send us your questions ,
Table 6: Task 1: Randomly selected samples from the single-word constraint task for the word
“amazing" (with overall frequency 10-3) highlighted in green. Tokens are highlighted with yellow
with different intensities to indicate their overall frequencies in the generated corpus of 10k samples
(lower intensity indicates higher corpus level diversity). φ(x) = 1 indicates the satisfaction of the
constraint in the sample and reps the number of repetitions of the very same sample in the generated
corpus of 10k samples (lower intensity indicates higher corpus level diversity).
30
Under review as a conference paper at ICLR 2022
reps I φ(x) Sample
GDC++
1	1
1	0
1	1
1	0
1	1
1	1
1	1
1	1
1	1
1	1
11
570	1
31
11
11
1	1
1	0
1	0
1	0
11
In this week's episode of VICE News , correspondent Julian Assange discussed WikiLeaks , and he revealed that the U . S .
government has been "hiding in plain sight" the full extent of Hillary
I'm sure the FBI has their hands full with the leak of emails on Wikileaks tojournalists — even as its head has been at the center
of one of the most explosive events in US politics ever
(WikiLeaks ) President Obama on Wednesday claimed he doesn't have to apologize for his "extremely careless" leak of classified
documents about classified information , but acknowledged it will happen again at the
WASHINGTON (Reuters) - The CIA is working closely with other agencies to fight cyberattacks on government computers and
computers belonging to Iran , according to atopU.S. official.
WikiLeaks founder Julian Assange has written an open letter to the US ambassador in London , calling for the extradition of
Assange . He said that Assange's treatment had been unjustified by the "
GDC
The WikiLeaks email server had been compromised to hide other information . WikiLeaks founder Julian Assange was one of the
first to share the information on a group of computer hackers who were using the personal
WikiLeaks has released a statement saying that it will publish its own account of what happened at the Democratic National
Convention . The statement has been translated from German . "The following information
"I'm sorry , I'm sorry , but there's no chance for a divorce" — WikiLeaks founder Julian Assange . WikiLeaks founder Julian
Assange was arrested last week in the Ecu
The Associated Press has been alerted by a source that WikiLeaks has been sharing personal information about President-elect
Donald Trump and Hillary Clinton , one day after WikiLeaks released thousands of pages of emails from Clinton's
The White House has confirmed former National Security Advisor Susan Rice as national security adviser , the latest sign of what
administration officials have described as an effort to sabotage WikiLeaks . A top Justice Department official
Reinforce
A Trump administration officials threatened to WikiLeaks and WikiLeaks vice president committee chair committee chair com-
mittee chair committee chair committee chair committee chair committee chair committee chair committee chair committee chair
committee chair committee chair committee chair committee chair
The Trump administration officials threatened to WikiLeaks , WikiLeaks president committee chair committee chair committee
chair committee chair committee chair committee chair committee chair committee chair committee chair committee chair com-
mittee chair committee chair committee chair committee chair committee
The Trump administration officials threatened to threatened to president and WikiLeaks , WikiLeaks president president commit-
tee chair committee chair committee chair committee chair committee chair committee chair committee chair committee chair
committee chair committee chair committee chair committee chair
The Trump presidential officials threatened to WikiLeaks , WikiLeaks vice president president committee chair committee chair
committee chair committee chair committee chair committee chair committee chair committee chair committee chair committee
chair committee chair committee chair committee chair committee
The FBI threatened to president and Trump president President, WikiLeaks , WikiLeaks president president committee chair com-
mittee chair committee chair committee chair committee chair committee chair committee chair committee chair committee chair
committee chair committee chair committee chair
Ziegler
In late 2010 , WikiLeaks released a trove of documents , including hundreds of thousands of emails and other personal and financial
information , from the National Security Agency . But those documents have never been released publicly .
A man has been detained by police after an attempted robbery in a busy street on Monday night. A man has been detained by
police after an attempted robbery in a busy street on Monday night
It's been a great year for the tech industry . At the same time , many of us in tech aren't looking to be CEOs . Many of us are
looking to learn more .
"If you see us , we would love you to do it," he said . "You'd better not do it. " "I think that would be a terrible idea ," said Mr
WikiLeaks says they found "vastly" evidence of CIA hacking after an undercover report on a Russian spy group suggested they
had helped spy on Donald Trump . The report said Russia
Table 7: Task 2: Randomly selected samples from the single-word constraint task for the word
“WikiLeaks” (with overall frequency 10-4) highlighted in green. Tokens are highlighted with yellow
with different intensities to indicate their overall frequencies in the generated corpus of 10k samples
(lower intensity indicates higher corpus level diversity). φ(x) = 1 indicates the satisfaction of the
constraint in the sample and reps the number of repetitions of the very same sample in the generated
corpus of 10k samples (lower intensity indicates higher corpus level diversity).
31
Under review as a conference paper at ICLR 2022
reps I φ(x) Sample
GDC++
1	0	The State Department, and in some ways the European Union , also took this step , with the former director of the National Institute
for Standards and Technology and a former member of the White House ,
1	1	, with the exception of a certain group of politicians , it was not a surprise that they had a tendency to follow the campaign . In the
United States , they are more of a conservative , political
1 II hope this is not an attempt to get at the other way to talk about this problem . It’s something about political expediency and
politics that seems to be a lot different from what is
1	1	C . A . No . 6 , on Tuesday , declared an end to the government's attempt to set up a national registry of those who are convicted
of serious crimes and who can be placed
1	1	. There will be a major overhaul of tax code to address a federal government proposal , which was unveiled in October , and a
second , which is expected to be signed by Trump .
GDC
1	1 We are here to inform you that, thanks to an order form , you may get in contact with us . If you wish to become a customer ,
please contact us . We are available
1	1	But they said that, once again , they were not so sure whether he would be a strong candidate in the fall election . "We know the
majority of state officials will be very interested
1	0	This is an excerpt from an essay by Kevin O'Connor , a researcher at the University of Chicago , where he focuses on climate
change and global warming . He is co-author of Climate Change
1	1	LONDON : A senior Indian government official on Tuesday said an attempt to rebrand India as a "piggybacking nation" for
international investment was a "game-changer"
1	1	(Reuters) - A federal court said on Friday that a Mississippi state trooper , arrested for killing a black man after an ambush in 2010
,violated his rights by failing to give him proper notice
Reinforce
1	1 A state in Russia New the state of the state of the state of the state of the state of the state of the state of the state of the state of the
state of the state of
1	1	A state	,	C New The state ofc In the state of the state of the state of the state of the state of state of the state of their	state of a ballot
1	1	A state	,	c) The state : The states of The states of the state ofc) In one of the state of the state of: For the state
1	1	A state	,	h) New state : The states of the state of the states of the state of the state of the state of the state of the	states	of the state	of
1	1	A state	,	The state ofc . New York The state of:
Ziegler
1	1 In a bid to counter China’s growing influence in the West, a senior Chinese government official has been forced to apologise after
accusing Beijing of encouraging ethnic Chinese to migrate to Hong Kong from the mainland
1	1	The federal government is taking another look at the Internet censorship of the Web after a senior government official said the
government is considering shutting down websites that use the software that monitors the Web .
1	1	Kamal Singh , the minister responsible for infrastructure and connectivity in Karnataka said the state government must ensure a
safe environment for women in its new high school curriculum . "We must ensure
1	1	BANGKOK , Myanmar (Reuters) - The United States on Saturday said that it was providing "appropriate military support" to
Myanmar’s government to help combat the situation in the country , as
1	1	The Supreme Court has ordered the Centre to give an independent audit of government programs and the Ministry of External
Affairs to explain how many ministers the government provided financial assistance to foreign NGOs . The
Table 8: Task 3: Randomly selected samples from the wordlist constraint task for the wordlist
“politics". Tokens are highlighted with yellow with different intensities to indicate their overall
frequencies in the generated corpus of 10k samples (lower intensity indicates higher corpus level
diversity). φ(x) = 1 indicates the satisfaction of the constraint in the sample and reps the number of
repetitions of the very same sample in the generated corpus of 10k samples (lower intensity indicates
higher corpus level diversity).
32
Under review as a conference paper at ICLR 2022
reps I φ(x) Sample
GDC++
1 II would love to find a way to use all this energy and energy on my own energy . But we have not yet figured this out. In fact we
seem to not really understand how it can
1	1	The research paper is one of only two to date in recent years , after being published in the American Journal of Psychiatry . "The
research team did some basic clinical investigation into the causes
1	1	Fashion is no longer a matter of fashion . In fact, it is no longer a matter of fashion . This is so because it is no longer a matter of
fashion . It is no
1	1	I love that this post is about the biology of my gut flora , the microbiome (the living tissue that is used to support and control the
gut) and the gut microbiome is basically just a chemical
1	0	I think I did it once . I actually saw him with my brother . That’s how it went, I thought the guy was the same age . I don’t know ,
you were the same
GDC
1	1 A few days ago we reported on the fact that the Obama administration has proposed an executive order that could increase the
number of Syrian refugees who have been allowed inthe U.S. for over five
1	1	If you are wondering , I am not a scientist, I am just a man who studies human behaviour , as I love the science of nature . My
focus is on the evolution of human beings to
1	0	The Republican National Convention had come under intense scrutiny for its use of language that used the word "nuclear" in an
interview with the Daily Beast on Monday . In a lengthy segment on
1	1	In addition to the fact that there is no way to make the changes in the data , there is no way to know what is happening . In fact, all
we have know about this project
1	1	I know I am not a scientist. I am a man who studies and researches . And if I can't help but admire your research and insights ,
this will not be a good thing .
Reinforce
1	1 We review data of primary power of data of data data of data of the question of validity of predictive of data and power of power
of of data of data of data of data of and
1	1	In an equity of data of data of data of log as relationships and then : data of relationships to recall of data of data of data of
relationships of relation . In relation of data of relation
1	1	The relation of data of influencing : In micro from data of power of data of data of in question about power power of data of
influence of relevance data of power of predictive of data
1	1	We , including data of data of data of fitness data of data of influencing of predictive of data of data of data data of power of
predictive of data of power of influencing of data of data
1	1	To relation power of data of question of data of : The correlation power of data of cohort of information of data of data of data of
data of data of cohort of relation of of
Ziegler
1	1 As the United States seeks to expand its nuclear energy base , it's hard to ignore the increasing energy scarcity in other countries .
In fact, there’s not much reason to think that the world's
1	1	"People don’t believe you are doing any good in life . They say you’re a bad person who doesn’t control your life . They say you
should give up on yourself. " If
1	1	"A small percentage of our population is women . But that does not mean that all women have to be working . In fact, there are
women working , but not all of them are . You
1	1	In case you missed it, a number of recent studies have shown that even when people with disabilities have an equal chance of being
successful in their career , they are better off working in science .
1	1	We understand that it is an experiment which needs to be designed to provide data from the most sensitive and relevant individuals
to be available to the most effective and well funded researchers . In fact, we expect
Table 9:	Task 4: Randomly selected samples from the wordlist constraint task for the wordlist
“science”. Tokens are highlighted with yellow with different intensities to indicate their overall
frequencies in the generated corpus of 10k samples (lower intensity indicates higher corpus level
diversity). φ(x) = 1 indicates the satisfaction of the constraint in the sample and reps the number of
repetitions of the very same sample in the generated corpus of 10k samples (lower intensity indicates
higher corpus level diversity).
33
Under review as a conference paper at ICLR 2022
reps I φ(x) Sample
1	1
1	0
1	1
1	1
1	1
1	1
1	1
1	1
1	1
1	1
87	1
1	1
1	1
11	1
1	1
1	1
1	1
1	1
1	1
992	1
GDC++
The "American Dream" is about more than a dream . It's about a dream that, if you can't have it, you can't have it now . The
American dream
"This is our most expensive movie . " You're not looking to get a lot of good things , but with this one , your best bet is to think
about what makes a good movie
"The most incredible thing I can think of to tell you is that the world has finally found a way to get together . And I can't tell you
where it will go . But you will
As part of a global effort to build a world where all people have access to affordable food , we are making a huge contribution to
helping those at the core of the world to find an environment free
It is no wonder that such a small and influential body of knowledge is important in the field of astronomy , astrophysics , medicine
, and medical research . However , our knowledge of these topics is also
GDC
"We are proud to announce today that the company has announced our fourth fiscal year. In our most important year , we raised
nearly $9 . 5 billion of our operating revenue from online and mobile
Election 2016 was the first election that did not involve a massive change in political discourse . But in fact, it was a dramatic
change in political discourse in this year's elections , one
Lemon-filled muffins have become an iconic , but surprisingly expensive option for breakfast, lunch or dinner on your table . For
many Canadians , breakfast is a meal you simply won't miss .
The University of Texas at Austin and the University of Virginia are working together to create a curriculum for teaching in the
United States that integrates information about climate change and understanding health and wellbeing in communities across the
Sydney's great outdoors tradition continues to draw crowds to the streets of Sydney in the name of Sydney . From the streets of
Melbourne to the beach in Perth , it is always a great time
Reinforce
Beautis is stunningly , charm , charm , charm , charm , charm , charm , charm , charm , charm , charm , charm , charm , charm ,
charm , charm , charm, charm
Em inspires , classicly , charmoror style , charm or decor , Classicor , charm , and charm, charm , charm , charm , charm , charm ,
charm , charm , charm,
Gold is stunningly , charm , stunning , charm, thrill, charm , dance , dance , dance , dance , dance , dance , dance , dance , dance ,
dance , dance , dance ,
Love is stunninglycation charm , charm , charm , charm , charm , charm , charm , charm , charm , charm , charm , charm , charm ,
charm , charm , charm, charm ,
Beautiscomes are stunninglycationly , charm , charm, dance , dance , dance , dance , dance , dance , dance , dance , dance , dance
,dance , dance , dance ,
Ziegler
.I really like the work of the writers for the book! The voice , writing , the characters and all the amazing stuff that comes with it
is a pleasure to read and
A big thank you to all my friends and fans for their support and contributions for my work . I will be posting a follow up post to
that post below . Ijust hope to keep it up
A great gift from our Secret Santa! Thank you so much ,Iamso grateful for your thoughtful thoughtful gift . I was a little worried
what to expect. I would just like
Practical tips for getting started with social media Welcome to the Beginner's Guide to FacebookMessenger! Today we hope you'll
find helpful tips that will help you get started with social media
Thank you for supporting the journalism that our community needs! For unlimited access to the best local, national, and interna-
tional news and much more , try an All Access Digital subscription :
Table 10:	Task 5: Randomly selected samples from the classifier-based constraint task for positive
sentiments. Tokens are highlighted with yellow with different intensities to indicate their overall
frequencies in the generated corpus of 10k samples (lower intensity indicates higher corpus level
diversity). φ(x) = 1 indicates the satisfaction of the constraint in the sample and reps the number of
repetitions of the very same sample in the generated corpus of 10k samples (lower intensity indicates
higher corpus level diversity).
34
Under review as a conference paper at ICLR 2022
reps I φ(x) Sample
GDC++
1	1 "We’re in a big game . ""It's really bad . It's really bad . ""I'm not going to lie to you . This was a lot of
1 IA few years ago , the first issue of my first book had a subtitle with "Slammarized". Ihad read it, and Ihad forgotten how bad it
was , but it was
1	1 One of the things about the media is that it is sometimes too busy to do so much . And that's fine . It's just that the press is busy
getting paid for doing so much
1	1	But that is only to be expected . One might be surprised at a simple explanation for the widespread lack of interest in climate
science in the academic world . This is the story of the recent climate denial
1	0	The new 'Naughty Dog' is already in release . In a leaked release on Steam , the game is set for release in August, making it one
of Sony's most widely
GDC
1	1 The first two tests of theK-12 program are very disappointing . One of the first tests showed a spike in learning rate on the test day
and in the third the student reported less information than
1	1	- A "tongue for an ugly nose" message was sent after a woman was told to "dance" after she became so disgusted by her friend's
antics that she sent "a
1	0	1 of 16 This could be an old story . It didn't come close to ending until Sunday night, when we got the first look at the cast on the
set of"
1	1	There are several reasons to think that we may not have a healthy amount of energy if we just eat nothing but pizza . The reason is
that we're not really hungry . So many
1	1	. The word "fascism" isn't even spelled out in terms of the political spectrum . Some are racist, some are homophobic , and some
are bigots . But when you
Reinforce
1	1	the evil poor evil annoying evil. the evil annoying the negative doesn't even sad , the sad bin sad bad sass bin , the sad sad bin sass
bin sass bin
1	1	This needs for long period of disappointing poor , the disappointing negative period of pathetic irrelevant poor annoying awful ,
even the disgusting poor period bin bin-at-total evil disass disass and that
1	1	no , is irrelevant. is not annoying . and even disgusting . disass or disass disab disab disab disab is disab bin disab	disab bin disab
dis
1	1	that is a big problem . "thx , even a large non evil is a bad , is a bad , unreasonable , awful sad sad" is evil sad , sad	sad awful sad
1	1	so long , sad s/thθ needs to disv and disab is wrong . the disab S s/tad s/so predictable S . the disab binums .
Table 11: Task 6: Randomly selected samples from the classifier-based constraint task for negative
sentiments. Tokens are highlighted with yellow with different intensities to indicate their overall
frequencies in the generated corpus of 10k samples (lower intensity indicates higher corpus level
diversity). φ(x) = 1 indicates the satisfaction of the constraint in the sample and reps the number of
repetitions of the very same sample in the generated corpus of 10k samples (lower intensity indicates
higher corpus level diversity).
35
Under review as a conference paper at ICLR 2022
φι(x) I Sample
GDC++
1 isabela Carolina is an american actress , writer , and former model . she is best known for her role as the teenage neighbor katie
Staley on the american series "
0	(born august 3 , 1969 ) is an american politician and lawyer . heisa member of the north dakota house of representatives from the
10th
0	- born august 1 , 1976 in new orleans , louisiana ) is a former american football safety in the national football league for the
washington redskins ,
0 on 26 february 1990 , he signed a five-year contract with bayer leverkusen . on 1 october 2000 , sheik won the german CUp with
bayer leverkus
0 the mcculloughs were an english glam rockband fromportsmouth , england . theband formed in 2003 , initially as aduo withjohn
mckeown ,jimmy mc
1	aime jacques de sousa is an indonesian television actress . she played a lead role in the 2012 indonesian television series “jayam ”
.she has played
1	on 11 december 2013 , laura klepp-larsen confirmed that she had suffered a heart attack . she was diagnosed with breast cancer at
the age of 24 .
0 the great olympic gong , born may 6 , 1960 in san antonio , texas , was the first and only indy to win the world champion title of
the american
0 aaron alexander (born october 27 , 1989 ) is an american professional baseball outfielder for the tampa bay rays of major league
baseball -lrb
0 ito’s most known work is that of “ ita , the world’s best girl ” , an international bestseller written by joao da sampre .
GDC
1	liz carlsson (born 2 june 1990 ) is a swedish actress and model, most famous for her role as alice in the film “
0	-“ for other people namedjohn C . white , seejohn white ( disambiguation ). ” john C . white ,jr . -lrb
0	italo zola (born 17 june 1959 ) is a former italian footballer . he played as a striker and as a forward for italian clubs pesc
1	of the year award nominations for 2013 , 2014 and 2015 . her most recent achievement was a “ top 10 debut	album	” from her
debut album , “ in the name of the devil ” , on
1	az klimin (born 20 october 1996 ) is a latvian artistic gymnast. she is a two-time european junior team
0	brian patrick keane (born may 16 , 1970 ) is an american football defensive end who is currently a free agent. he was drafted by
the P
1 was an english film and television actress . she appeared in many british and american films , and had roles in the tv shows “ my
big fat greek wedding ” (
0	- araki (bornjanuary 4,1976 in ivanhoe , lautoka) is a retired brazilian footballer . he played for several clubs
1	, better known by her stage name pepi , is a korean female singer-songwriter . she came to korea after being influenced by kim
jin-hoon's
1	(born august 23 , 1962 ) is an american actress . she has appeared in such films as “ kojak ”, “ i saw the fire ”
Table 12: Task 7: Randomly selected samples from the experiment with a single distributional
constraint where φ(χ) = 1 iff X contains a mention of a female figure, μ = 0.5
36
Under review as a conference paper at ICLR 2022
φ1 (X)	φ2(x)	φ3(X)	φ4(X)	Sample
				GDC++
0	0	0	1	, was a russian politician and journalist .
0	0	0	1	luis alberto herrera carvalho ( born October 6 , 1951 ) is a Chilean economist, economist, politician and former mayor of mon
0	0	0	1	bernard stanton johnson ( born november 8 , 1958 ) is a canadian politician . he was elected to the canadian house of commons in
1	0	0	0	- > thomas s . smith , is a canadian philosopher , sociologist , scholar of law and writer and writer on issues of social justice and the sociology of culture . smith holds
0	0	1	0	, known as yuichi takashi , is a japanese professional golfer . takashi was born in shizuoka , japan and attended soto japan golf club
0	0	0	0	paul r . kelly is a democratic member of the pennsylvania house of representatives . he was elected to represent the 28th legislative district , being reelected in 2006 and 2010 .
1	0	0	1	SIaW ( born 12 february 1961 ) is a polish historian , politician , sociologist , and member of the european parliament for poland .
0	1	0	0	. ( born in dresden , neW jersey ) is a german singer and multi-instrumentalist Who has released several solo albums .
0	1	0	0	for the artist, seejean-luc kruger (painter ). ” jean-luc kruger (j
0	0	1	0	( born april 17 , 1979 in bahrain ) is an iranian footballer Who currently plays for al arabi sc . GDC
0	0	1	0	kim ludWin ( born august 11 , 1985 ) is a canadian ice hockey player Who is currently playing With hc slovan bratislava
0	1	0	0	kazuki shimizu ( born march 30 , 1970 in osaka , japan ) is a japanese mixed martial artist Who is the current pride lightWeight
0	0	1	0	andreW jones ( born 23 december 1970 ) is a former english cricketer . jones Was a right-handed batsman Who boWled right-
0	0	1	0	andr6 fern⅛ndez de g6mez (born 20 february 1989 ) is a spanish professional footballer who plays for fc barcelona
0	0	0	1	theodore george hudson ( october 20 , 1877 - april 8 , 1944 ) was a united states army officer . he served as the 19
0	0	0	0	. he was born in rome , italy on 10 may 1949 .
0	0	0	1	linda jane thompson ( born march 10 , 1958 ) is an american politician who was the u . s . representative for from 2003 to 2015 .
0	1	0	0	kenny hansen ( born april 26 , 1982 ) is an american actor best known for his role as the sheriff in the disney channel series “ criminal
0	0	0	1	in 2007 , he was nominated by the governor of illinois to be the governor of illinois in 2011 for the position of the u . s . representative for illinois’s 22nd congressional
0	0	0	0	the dutch are an influential british reggae music duo , formed in 1982 in dublin . the duo consists of lead vocalist dave schroeder and drummer eric kend
Table 13: Task 8: Randomly selected samples from the experiment with Four distributional
constraints: φn(x) = 1 iff x contains at least one of the words from a corresponding n-th wordlist
proposed by (Dathathri et al., 2020). The considered wordlists are “science", “art", “sports" and
“business" and for each μn = 0.25
37
Under review as a conference paper at ICLR 2022
φι(x) I φ2(x) I Sample
GDC++
11	;(born 10 October 1987 ) is an iranian footballer who plays as a defender for bursaspor and the iran national football team
.she is
1	1	. she is the daughter of vladimir uchadze , who is also a former russian football player .
0	1	kenzo shiro ( born 26 april 1985 ) is ajapanese football player who currently plays forj . league division 2 CIUbjapaneSe
super
0	1	hans schuetke (born 21 july 1953 ) is a german former footballer who played as a forward for vfb stuttgart, sheffield
0	1	, real name marc valera cipriles (born 4 may 1969 ) is a former costa rican footballer who last played as a	defender .
0	1	brent lincoln ( born 1 october 1985 ) is an english footballer who plays as a striker for bristol rovers .	born	in	bristol ,
lincoln
0	1	joseph e . “joey ” bierer (born may 18 , 1953 in columbus , ohio ) is a retired american basketball player
0	1	aryeh (; born 22 october 1988 ) is an israeli footballer currently playing for kfar saba .
0	1	juan de almagro castro (born 21 october 1981 in lisbon ) is a portuguese retired footballer who played as a midfielder . he
1	1	is a canadian tennis player . as of 2014 , she has a wta singles career high ranking of 967 achieved onjuly 15,2015 .
GDC
0	1	s6bastien l6Pine (born 9 march 1987 ) is a french football player currently playing for olympique lyonnais in ligue 1 .
1	0	in a career that spans nearly four decades , le6n has starred in some of the most successful movies of the late-1980s and
early-1990s . her breakthrough came in the 2005 film
0	1	hamed sargam (born 9 january 1975 ) is a saudi arabian footballer . he played for al qadisiyah in saudi ar
0	1	james “ jim ” mcgrath (born may 24 , 1934 ) is a former professional american football player who played wide receiver
for eight seasons for the
0	1	james “ jack ” lancaster ( born 21 march 1935 ) is an english former footballer who played in the football league for
brentford , leeds united
0	1	aacson de rosas de l6pez ,jr. (born 18 april 1976 in barcelona) is a spanish professional racing cyclist.
1	1	, born on 29 april 1982 in baku) is aprofessional turkish tennis player . she reached her highest wta singles ranking of 280
on 20 september 2012.
1	1	'(, born september 10 , 1992 ) is a female water polo player of kenya . she was part of the kenyan team at
0	1	( november 10 , 1981 in davao ) is a dutch footballer who plays for vitesse as a defender .
1	1	, born november 15 , 1986 in tokyo , japan ) is ajapanese volleyball player . she was drafted fifth in the 2011 j . league
division 1
Table 14:	Task 9: Randomly selected samples from the experiment with a hybrid distributional
constraint where φι(χ) = 1 iff X contains a mention of a female figure, μι = 0.5 and φ2(χ) = 1 iff
x contains at least one of the words from the “sports" wordlist proposed by (Dathathri et al., 2020)
and μ^2 = 1
38
Under review as a conference paper at ICLR 2022
φι(x) I φ2(x) I Sample
GDC++
1	1	, born 3 may 1947 in SeljU , turkey) is a former turkish women's football player . she was a student in istanbul, istanbul.
11	is a french filmmaker and academic . she is known for her documentary , “ le seigneur une reunion de bahaudouin ”，the first in
which a french student walks around
1	1	, also known by her married name , was ajapanese scientist and a scientist who specialized in nuclear physics and nuclear radiation
.she was the second woman , after kumiko ouchi,
1	1	was an indian historian and scholar in the field of indian history . she is known for her book “ sanskrit, chakri and kanchra ”(18
0	1	, born on april 24 , 1957 , in chungzhou , shandong , was a chinese politician and academic who served as	a member	of	the
legislative yuan fromjuly 12 ,
0	1	, (; january 26 , 1917 - may 6 , 1997 ) was a russian politician , scientist, and diplomat. from the early 1930s to	the mid
0	0	israel hanadiyev (; born april 8 , 1985 ) is a russian-born russian professional football player . he plays for fc
1	1	linda borregoni is an american astronomer and theoretical cosmologist. she has received numerous awards , including a macarthur
foundation fellow for astronomy award for her work in cosmology
0	1	sarah c . lee( bornjanuary 25,1931 )isan american educator , academic and medical researcher . lee has written a series	ofbooks
0	1	alexander leonard bernstein (born 8 april 1940 in breslau , switzerland ) is a swiss nuclear scientist and politician who
GDC
0	1	:( 1558 - 7 june 1628 ) was a french writer , philosopher ,journalist, antiquary , lawyer and historian . he was one of the	great
1	0	, was an ancient egyptian princess . she was the daughter of the egyptian empress nikhaιt of zagros .
1	1	saysia nand is a student of asean university and sri lanka university of science and technology and her doctoral student is shahid
srinivasan . nand has
0	1	b- (born may 26,1977 ) is a canadian historian , and former chair of the department of medieval history of the university of british
columb
1	1	sara sara (born july 3 , 1954) is an american social scientist. she is a co-director of the national center for family research and
0	1	: born 13 october 1969 ) is a british philosopher . he is professor of philosophy at the university of london and chair of the
department of philosophy of humanistic philosophy
0	1	, was a chinese poet, playwright, translator , translator , sociologist and academic . he was born in sichuan in 1796 and became an
early member of the literary association of
0	1	larry t. ellerbe is an american scientist who is the founding director of the department of natural resources and environment at the
carnegie mellon university . he is the son of the
0	0	a . p . taylor is an american professor of philosophy and director of the department of philosophy of religion at the university of
california , berkeley . his recent research has focused on
0	1	, was an israeli arabologist, historian , and scholar of early israel. he is best known as the former director of the national library of
the israel .
Table 15:	Task 10: Randomly selected samples from the experiment with a hybrid distributional
constraint where φι(χ) = 1 iff X contains a mention of a female figure, μι = 0.5 and φ2(χ) = 1 iff
x contains at least one of the words from the “science" wordlist proposed by (Dathathri et al., 2020)
and μ^2 = 1
39