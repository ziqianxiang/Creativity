Under review as a conference paper at ICLR 2022
Manifold Micro-Surgery
with Linearly Nearly Euclidean Metrics
Anonymous authors
Paper under double-blind review
Ab stract
The Ricci flow is a method of manifold surgery, which can trim manifolds to
more regular. However, in most cases, the Rich flow tends to develop singularities
and lead to divergence of the solution. In this paper, we propose linearly nearly
Euclidean metrics to assist manifold micro-surgery, which means that we prove
the dynamical stability and convergence of such metrics under the Ricci-DeTurck
flow. From the information geometry and mirror descent points of view, we give
the approximation of the steepest descent gradient flow on the linearly nearly Eu-
clidean manifold with dynamical stability. In practice, the regular shrinking or
expanding of Ricci solitons with linearly nearly Euclidean metrics will provide a
geometric optimization method for the solution on a manifold.
1	Introduction
In general relativity (Wald, 2010), a complete Riemannian manifold (M, g) endowed with a linearly
nearly flat spacetime metric gij is considered for linearized gravity to solve the Newtonian limit.
The form of this metric is gij = ηij + γij , where ηij represents a flat Minkowski metric whose
background is special relativity and γij is small from ηij . An adequate definition of “smallness” in
this context is that the components of γij are much smaller than 1 in some global inertial coordinate
system of ηij . Now, let us step out of the physical world and give a similar metric gij = δij + γij
in Riemannian n-manifold (Mn, g), i.e. the linearly nearly Euclidean metric, where δij represents
a flat Euclidean metric and γij is small from δij .
A natural problem for such a linearly nearly Euclidean metric is: how does the metric evolve over
time with respect to the Ricci flow while ensuring the constant topological structure? Let us review
some stability analyses of different manifolds along with the Ricci flow.
For the Riemannian n-dimensional manifold (Mn , g) that is isometric to the Euclidean n-
dimensional space (Rn,δ), Schnurer et al. (Schnurer et al., 2007) have showed the stability of
Euclidean space under the Ricci flow for a small C0 perturbation. Koch et al. (Koch & Lamm,
2012) have given the stability of the Euclidean space along with the Ricci flow in the L∞-Norm.
Moreover, for the decay of the L∞-Norm on Euclidean space, Appleton (Appleton, 2018) has given
the proof of a different method. Considering the stability of integrable and closed Ricci-flat met-
rics, Sesum (Sesum, 2006) has proved that the convergence rate is exponential because the spectrum
of the LichneroWicz operator is discrete. Furthermore, Deruelle et ^. (Deruelle & Kroncke, 2021)
have proved that an asymptotically locally Euclidean Ricci-flat metric is dynamically stable under
the Ricci flow, for an L2 ∩ L∞ perturbation on non-flat and non-compact Ricci-flat manifolds.
If we embed a Riemannian n-dimensional manifold in the neural network, then we are training the
neural network on the dynamic manifold. The most famous method for training neural networks
on manifolds is the natural gradient (Amari, 1998). However, for the Riemannian manifold corre-
sponding to the KL divergence (representing the natural gradient), its stability and convergence are
still unknown (Martens, 2020). As a way of manifold evolution, Ricci flow seems to be an excellent
choice to ensure that neural networks are trained on dynamic and stable manifolds (Glass et al.,
2020; Jejjala et al., 2020). But the research on the relationship between the two has not yet sprouted.
In this paper, we consider a complete Riemannian n-dimensional manifold (Mn, g) endowed with
linearly nearly Euclidean metrics g(t) = δ + γ(t). First of all, we prove the stability of linearly
nearly Euclidean manifolds under the Ricci-DeTurck flow in the L2 -Norm if initial metrics are
1
Under review as a conference paper at ICLR 2022
integrable and linearly stable, i.e. has a manifold structure of finite dimension. We mean that any
Ricci-DeTurck flow which starts from near g exists for all time and converges to a linearly nearly
Euclidean metric near g. Note that we use the Einstein summation convention and denote generic
constants by C or C1 .
Furthermore, we define and construct linearly nearly Euclidean manifolds based on information
geometry and mirror descent algorithm. Based on a symmetrized convex function, we obtain the
linearly nearly Euclidean divergence which is used to calculate the steepest descent gradient in lin-
early nearly Euclidean manifolds. Experimentally, when we use the approximated steepest descent
gradient flow to learn several neural networks on classification tasks, we observe the evolution of its
metric is consistent with the micro-surgery process under the Ricci-DeTurck flow.
2	Ricci Flow
Let us introduce a partial differential equation, the Ricci flow, without explanation. The concept of
the Ricci flow first published by Hamilton (Hamilton et al., 1982) on the manifold M of a time-
dependent Riemannian metric g(t) with the initial metric g0:
∂
∂tg(t) = -2Ric(g⑴)	(1)
g (0) = g0
where Ric denotes the Ricci curvature tensor whose definition can be found in Appendix A.
The purpose of the Ricci flow is to prove Thurston,s Geometrization Conjecture and Poincare Con-
jecture because the Ricci flow is like a surgical scalpel, trimming irregular manifolds into regular
manifolds to facilitate observation and discussion (Sheridan & Rubinstein, 2006).
In general, in order to possess good geometric and topological properties, we expect the metric to
become converge and round with the help of the Ricci flow. “become round” means that the solution
will not shrink to a point but converge to a constant circle. However, in most cases, we do not even
know the convergence of the solution and whether the solution will develop a singularity. Next, we
will discuss these issues for brevity.
2.1	Short Time Existence
To show that there exists a unique solution for a short time, we must check if the system of the Ricci
flow is strongly parabolic.
Theorem 1 When u : M × [0, T) → E is a time-dependent section of the vector bundle E where M
is some Riemannian manifold, if the system of the Ricci flow is strongly parabolic at u0 then there
exists a solution on some time interval [0, T), and the solution is unique for as long as it exists.
Proof. The proofs can be found in (Ladyzhenskaia et al., 1988).
Definition 1 The Ricci flow is strongly parabolic if there exists δ > 0 such that for all covectors
夕=0 and all symmetric hj = "g∂t(t = & the principal symbol of —2 Ric satisfies
[-2Ric](夕Xh)ij h" = gPq (夕P Wqhij + (Pi(Pjhpq - ψq ψi hjp - ψq 4jhip}h" > δ夕 k 夕 hrs h ∙
Since the inequality cannot always be satisfied, the Ricci flow is not strongly parabolic, which makes
us unable to prove the existence of the solution based on Theorem 1.
It is possible to understand which parts have an impact on its non-parabolic by the linearization of
the Ricci curvature tensor.
Lemma 1 The linearization of—2 Ric can be rewritten as
D[—2Ric](h)ij = gpq ”qhij + ViVj- + NjVi + O(hij)
where	Vi = gpq (1 Vihpq- Vq hp).
(2)
2
Under review as a conference paper at ICLR 2022
Proof. The proofs can be found in Appendix B.1.
The term O(hij) will have no contribution to the principal symbol of -2 Ric, so ignoring it will
not affect our discussion of this problem. By carefully observing the above equation, one finds
that the impact on the non-parabolic of the Ricci flow comes from the terms in V , not the term
gpqVpkqhij. The solution is followed by the DeTUrck Trick (DeTurck, 1983) that has a time-
dependent reparameterization of the manifold:
∂
^λ7g(t) = -2 Ric(g(t)) - L∂Rt) g(t)
∂t	~∂Γ~
g(0) = go + d,
(3)
where d is a symmetric (0,2)-tensor on M. See Appendix B.2 for details. By choosing d∂t) to
cancel the effort of the terms in V , the reparameterized Ricci flow is strongly parabolic. Thus, one
can say that the Ricci-DeTurck flow has a unique solution, the pullback metric, for a short time.
2.2	Curvature Explosion at Singularity
In this subsection, we will present the behavior of the Ricci flow in finite time and show that the evo-
lution of the curvature is close to divergence. The core demonstration is followed with Theorem 4,
which requires some other proof as a foreshadowing.
Theorem 2 Given a smooth Riemannian metric g0 on a closed manifold M, there exists a maximal
time interval [0, T) such that a solution g(t) of the Ricci flow, with g(0) = g0, exists and is smooth
on [0, T), and this solution is unique.
Proof. The proofs can be found in (Sheridan & Rubinstein, 2006).
Theorem 3	Let M be a closed manifold and g(t) a smooth time-dependent metric on M, defined
for t ∈ [0, T). If there exists a constant C < ∞ for all x ∈ M such that
T
0
∂
豆;gx(t)	dt ≤ C,
∂t	g(t)
(4)
then the metrics g(t) converge uniformly as t approaches T to a continuous metric g(T ) that is
uniformly equivalent to g(0) and satisfies
e-Cgx(0) ≤ gx(T) ≤ eCgx(0).
Proof. The proofs can be found in Appendix B.3.

Corollary 1 Let (M, g(t)) be a solution of the Ricci flow on a closed manifold. If | Rm |g(t) is
bounded on a finite time [0, T), then g(t) converges uniformly as t approaches T to a continuous
metric g(T) which is uniformly equivalent to g(0).
Proof. The bound on | Rm |g(t) implies one on | Ric |g(t) . Based on Equation (1), we can extend the
bound on |∂g(t)∣g(t). Therefore, We obtain an integral of a bounded quantity over a finite interval
is also bounded, by Theorem 3.
Theorem 4	If g0 is a smooth metric on a compact manifold M, the Ricci flow with g(0) = g0 has
a unique solution g(t) on a maximal time interval t ∈ [0, T). If T < ∞, then
lim sup | Rmx(t)| = ∞.	(5)
t→T x∈M
Proof. For a contradiction, we assume that | Rmx (t)| is bounded by a constant. It follows from
Corollary 1 that the metrics g(t) converge uniformly in the norm induced by g(t) to a smooth metric
g(T). Based on Theorem 2, it is possible to find a solution to the Ricci flow on t ∈ [0, T) because
the smooth metric g(T) is uniformly equivalent to initial metric g(0).
3
Under review as a conference paper at ICLR 2022
Hence, one can extend the solution of the Ricci flow after the time point t = T , which is the result
for continuous derivatives at t = T . This tell us that the time T of existence of the Ricci flow has
not been maximal, which contradicts our assumption. In other words, | Rmx (t)| is unbounded.
As approaching the singular time T, the Riemann curvature | Rm |g(t) becomes no longer convergent
and tends to explode.
3 Evolution of Linearly Nearly Euclidean Metrics
Next, this paper will focus on linearly nearly Euclidean metrics, proving that them can have a good
performance in terms of stability, i.e., the convergence of a Ricci-DeTUrck flow g(t) to a linearly
nearly Euclidean metric g(∞). Before that, we have to construct a family go of linearly nearly
Euclidean reference metrics such that ∂go(t) = O((g(t) - go(t))2). Let
F = {g(t) ∈Mn I 2Ric(g(t))+ Ld臂t g(t)=0o
be the set of stationary points under the Ricci-DeTurck flow. We are able to establish a manifold
~ — _ .
F = F ∩ U	(6)
where U is an L2-neighbourhood of integral go.
Before starting the discussion about long time stability of linearly nearly Euclidean metrics, we need
some prior knowledge about short time existence (Appendix D and Appendix E). In particular, we
yield
Lemma 2 Let g(t) be a Ricci-DeTurckflow on a maximal time interval t ∈ (0, T) in an L2 neigh-
bourhood of go. We have the following estimate Such that:
∂
∂tdo(t)
L2≤ CM⑴ Wt)- MIL2
Proof. Let {e1 (t), e2 (t), . . . , en(t)} be a family of L2-orthonormal bases of the kernel kerL2
such that ∂∂tei(t) depends linearly on Itdo(t). For an isomorphism orthogonal projection Π :
Tgo F → kerL2, by the Hardy inequality (Minerbe, 2009), one has similar proofs by referring the
details (Deruelle & Kroncke, 2021).	口
Theorem 5 Let (Mn, go) be a linearly nearly Euclidean n-manifold which is linearly stable and
integrable. Furthermore, there exists a constant ag° such that
(∆d(t) +Rm(go) * d(t),d(t))L2 ≤ -ago ∣∣Vg0h∣∣L2
for all gg(t) ∈ F which is as in (6).
Proof. The similar proofs can be found in (Devyver, 2014) with some minor modifications. Due
to the linear stability requirement of linearly nearly Euclidean manifolds in Definition 8, -Lgo is
non-negative. Then there exists a positive constant ag° such that
ago (-∆d(t), d(t))L2 ≤ (-∆d(t) - Rm(go) * d(t), d(t))L2 .
By Taylor expansion, one repeatedly uses elliptic regularity and Sobolev embedding (Pacini, 2010)
to obtain the estimate.	口
Corollary 2 Let (Mn, ggo) be a linearly nearly Euclidean n-manifold which is integrable. For a
Ricci-DeTurck flow gg(t) on a maximal time interval t ∈ [0, T], ifit satisfies kgg(t) - ggokL∞ <
where > 0, then there exists a constant C < ∞ for t ∈ [0, T] such that the evolution inequality
satisfies
kd(t) - do kL2 ≥ C / T ∣∣Vg0⑴(d(t) - do)∣∣j dt.
4
Under review as a conference paper at ICLR 2022
Proof. Based on Equation (16), we know
∂
—(d(t) — do) =∆(d(t) — do) + Rm *(d(t) — d0)
∂t
+ Fg-I * Vg0 (d(t) — do) * Vg0 (d(t) — do)
+ Vg0 (Gr(g0) * (d(t) — do) * Vg0(d(t) — do)).
Followed by Lemma 2 and Theorem 5, we further obtain
∂
∂t ∣∣d(t) — do∣∣L2 =2 (∆(d(t) — do) + Rm*(d(t) — do), d(t) — do)l2
+ (Fg-I * Vg0 (d(t) — do) * Vg0 (d(t) — do), d(t) — do)L2
+ (Vg0 (Gag。)* (d(t) — do) * Vg0 (d(t) — do)), d(t) — do)L2
∂
(d(t) — do) * (d(t) — do) * ∂tdo(t)dμ
≤ — 2αg0∣∣Vg0 (d(t) — do)∣∣L2
+ Ck(d(t) — do)kL∞ ||Vg0 (d(t) — doVlL2
∣∂
+ ∂tdo(t)
∣d(t) — do ∣L2
L2
≤ (-2αg0 + C ∙ E) Il Vg0 (d(t) — do )∣∣L2 .
Let E be small enough that -2ag0 + C ∙ e < 0 holds, We can find
∂
所kd(t) — dokL2 s-C∣∣Vg0(d(t) — do)∣∣L2
holds.
Theorem 6 Let (Mn,班)be a linearly nearly Euclidean n-manifold which is linearly Stable and
integrable. For every ei > 0, there exists a E2 > 0 satisfying: For any metric g(t) ∈ Bl2 (go, €2),
there is a complete Ricci-DeTurckflow (Mn, g(t)) Startingfrom g(t) converging to a linearly nearly
Euclidean metric g(∞) ∈ Bl2 (的，ei). Note that Bl2 (Uo, E) is the E-ball with respect to the L2-Norm
induced by go and centred at Uo.
Proof. By Lemma 4, one can find so small E2 > 0 such that d(t) ∈ BL2 (0, E2) holds. By Lemma 2
and Corollary 2, We have
kdo(T)kL2≤C
T ∣∣ ∂
L	∂tdo(t)
dt
L2
≤
C ∕T∣∣Vg0 (d(t) - do(t))∣∣L2
dt
≤ C kd(1) - do(l)kL2 ≤ Ckd(l)kL2 ≤ C∙(E2)2 .
Furthermore, We obtain
kd(T) — do(T)∣L2 ≤ ∣d(1) — do(1)∣L2 ≤ C ∙ €2.
By the triangle inequality, We get
Ild(T)kL2 ≤ C ∙ (E2)2 + C ∙ E2.
FolloWed by Corollary 4 and Lemma 2, such T should be pushed further outWard, because
lim sup
t→+∞
盘do(t)	≤ lim sup∣∣Vg0 (d(t) — do(t))k =0.
∂t ∣ 2	t→+∞	L
Thus, as t approaches to +∞, gU(t) converges to gU(∞) = gUo + do(∞). By the Euclidean Sobolev
inequality (Minerbe, 2009), d(t) — do (t) converges to 0 as t goes to +∞,
lim ∣∣d(t) — do(t)∣L2 ≤ lim CHVg0 (d(t) — do(t))H 2 = 0.
t→+∞	t→+∞	L
We noW conclude a result for linearly nearly Euclidean manifolds under the Ricci-DeTurck floW,
Which ensures a infinite time existence.
5
Under review as a conference paper at ICLR 2022
4 Gradient Flow with Linearly Nearly Euclidean Metrics
Consequently, we have clarified the convergence of linearly nearly Euclidean manifolds under the
Ricci-DeTurck flow. Furthermore, we will consider the solution of gradient flow with linearly nearly
Euclidean metrics, which will allow us to observe the neural network behavior for back-propagation
in the linearly nearly Euclidean manifold. Empirically, we introduce information geometry (Amari
& Nagaoka, 2000; Amari, 2016) and mirror descent algorithm (Bubeck et al., 2015) to construct the
gradient flow with the help of divergences.
4.1 Linearly Nearly Euclidean Divergence
From the perspective of information geometry and mirror descent algorithm, the metric g can be
deduced by the divergence that needs to satisfy certain criteria (Basseville, 2013). We now consider
two nearby points P and Q in a manifold M, where these two points are expressed in coordinates
as ξP and ξQ, where ξ is a column vector. Moreover, the divergence is defined as half the square of
an infinitesimal distance between two sufficiently close points in Definition 2.
Definition 2 D[P : Q] is called a divergence when it satisfies the following criteria:
(1) D[P : Q] ≥ 0, (2) D[P : Q] = 0 when and only when P = Q, (3) When P and Q are sufficiently
close, by denoting their coordinates by ξP and ξQ = ξP + dξ, the Taylor expansion of D is written
as
D[ξp ： ξp + dξ] = 2 Xgij(ξp)dξidξj + O(∣dξ∣3),
i,j
and metric gj is positive-definite, depending on ξp.
In order to construct a linearly nearly Euclidean metric, according to Definition 2, one can con-
struct the divergence to obtain the expression of the metric indirectly. And the advantage is that the
constructed divergence can be used to calculate the gradient flow under linearly nearly Euclidean
metrics. Therefore, with the assist of Definition 3, we introduce a symmetrized convex function to
construct the needed divergence:
φ(ξ) = X τ12 log 2 (exp(τξi) + exp(-τξi)) = X τ12 log (cosh(τξi))	(7)
where τ is a constant parameter.
Definition 3 The Bregman divergence (Bregman, 1967) DB [ξ : ξ0] is defined as the difference
between a ConvexfUnction φ(ξ) and its tangent hyperplane Z = φ(ξ0) + (ξ 一 ξ0)Vφ(ξ0), depending
on the Taylor expansion at the point ξ0:
DB[ξ: ξ0] = φ(ξ) 一 φ(ξ0) 一 (ξ 一 ξ0)Vφ(ξ0).
Theorem 7 For a convex function φ defined by Equation (7), the linearly nearly Euclidean diver-
gence between two points ξ and ξ0 is
DLNE [ξ0 : ξ] = yX ^^2 log-J F-----(ξi 一 ξi) tanh(τξi)	(8)
τ2	cosh(τξi)	τ
where the Riemannian metric is
gij(ξ(t)) = δij - [tanh(τξ)tanh(τξ)>]ij
1 一 tanh(τξι(t))tanh(τξι(t)) …	一 tanh(τξι(t)) tanh(τξn(t))	纥,
=	.	..	.	.
.	..
—tanh(τξn(t))tanh(τξι(t))	…	1 — tanh(τξn(t))tanh(τξn(t))
Proof. The proofs can be found in Appendix C.1.
Based on Theorem 7, the form of the metrics g(t) constructed by the linearly nearly Euclidean
divergence is consistent with the definition of linearly nearly Euclidean metrics, as long as we adjust
parameter τ to satisfy Definition 7. Moreover, we have also proven that the linearly nearly Euclidean
divergence satisfies the criteria of divergence followed by Definition 2.
6
Under review as a conference paper at ICLR 2022
4.2 Weak Approximation of the Gradient Flow
By the linearly nearly Euclidean divergence, we can perform micro-surgery on the neural manifold
under the gradient descent. Specifically, we dynamically consider the gradient flow toward the
optimal descent direction on a manifold endowed with linearly nearly Euclidean metrics.
Lemma 3 The steepest descent gradient flow measured by the linearly nearly Euclidean divergence
is defined as
∂ξ = g-1(t)∂ξ = [δij — tanh(τξ(t)) tanh(τξ(t))>] 1 ∂ξ.	(10)
Proof. The proofs can be found in Appendix C.2.
However, Lemma 3 involves inversion, which greatly consumes computing resources. In particu-
lar, we propose two methods for approximating the gradient flow: weak approximation and strong
approximation respectively.
For the weak approximation of the gradient flow, we put forward higher requirements for this metric
on the basis of Definition 7, which requires that the linearly nearly Euclidean metric is also a strictly
diagonally-dominant matrix based on Corollary 3.
Corollary 3 The weak approximation of the gradient flow measured by the linearly nearly Eu-
clidean divergence is defined as
∂ξ ≈ [δjj + tanh(τξ(t))tanh(τξ(t))>] ∂ξ	(11)
if the metric satisfies strictly diagonally-dominant.
Proof. The proofs can be found in Appendix C.3.
4.3 Strong Approximation with Neural Networks
Bypassing the requirement of weak approximation in Corollary 3, our goal is to approximate the
gradient flow, g-1(t)∂ξ in Lemma 3, from the assist of multi-layer perceptron (MLP) neural net-
work. Using the neural network, we can easily yield the strong approximation of the gradient flow
because a neural network with a single hidden layer and a finite number of neurons can be used to
approximate a continuous function on compact subsets (Jejjala et al., 2020), which is stated by the
universal approximation theorem (Cybenko, 1989; Hornik, 1991).
As an n × n symmetric matrix, g(t) can be decomposed in terms of the combination of entries P
and A, where P is the entries made up of the elements of the lower triangular matrix that contains
n(n — 1)/2 real parameters and A is the entries made up of the elements of the diagonal matrix that
contains n real parameters. During the training in Figure 1, g(t) can be used as strong approximation
of g-1(t) in the gradient flow.
τr-<∙	t TΓ-<1	1	. i' .	∙	. ∙	f	1∙	. n	El	, ∙ T-l F 7
Figure 1: Flow chart of strong approximation of the gradient flow. The new entries P and A pro-
duced by neural network get combined into a new metric g(t) that is used to minimize the loss
function by combining with the metric g(t), where the loss function is defined by Equation (12).
L = kI - g(t)g(t)k2.
(12)
7
Under review as a conference paper at ICLR 2022
5 Experiment
Figure 2: The evolution of metrics g(t) by the radius of a ball with the epoch of training process.
Note that we use radius 1 as the calibration of a linearly nearly Euclidean metric.
CIFAR datasets. The two CIFAR datasets Krizhevsky et al. (2009) consist of natural color images
with 32×32 pixels, respectively 50,000 training and 10,000 test images, and we hold out 5,000
8
Under review as a conference paper at ICLR 2022
training images as a validation set from the training set. CIFAR10 consists of images organized into
10 classes and CIFAR100 into 100 classes. We adopt a standard data augmentation scheme (random
corner cropping and random flipping) that is widely used for these two datasets. We normalize the
images using the channel means and standard deviations in preprocessing.
Settings. We set total training epochs as 200 where the learning strategy is lowered by 10 times at
epoch 80, 150, and 190, with the initial 0.1. The learning strategy is a weight decay of 0.0001, a
batch size of 128, SGD optimization. On CIFAR10 and CIFAR100 datasets, we apply ResNet20,
ResNet32, ResNet44, ResNet56 and ResNet110 models (He et al., 2016) to observe the evolution of
neural manifold, i.e., the convergence of metrics depended on time. As far as We define the metric
g(t), we can use the length ∣ds2∣ = JPi j gj (t)dξidξj to intuitively reflect the change of metrics.
Specifically, We define a ball Whose radius is equal to |ds2 |:
Br(t) ：= [r = yX gij(t)dξidξj ʃ .
(13)
Details. We embed the linearly nearly Euclidean manifold into a neural network, which means that
a neural networks uses Corollary 3 for back-propagation. No other parts of the neural network need
to be modified.
Neural Network Behavior. By observing the change of the ball in Figure 2, we can know the
change of the metric. Through simple observation, metrics g(t) on CIFAR10 converge in about 100
epochs and metrics g(t) on CIFAR100 converge in about 150 epochs. For CIFAR10, metrics g(t)
in ResNet32 and ResNet44 seem to converge the fastest. For CIFAR100, metrics g(t) in ResNet110
seem to converge the fastest. In general, experiments show that all metrics in neural manifolds
converges to a linearly nearly Euclidean metric. It is consistent with the evolution of Ricci-DeTurck
flow in Section 3.
Remark. For a neural network that is specified by connection weights, the set of all such weighs
forms a manifold. When we use the gradient flow to learn a neural network (ξ(t) is composed of
weights), we observe the evolution of its metric is consistent with the micro-surgery process under
the Ricci-DeTurck flow. Consequently, the training of a neural manifold is also a surgery, i.e., the
manifold is gradually regular, whose process is stable and eventually converges.
6 Conclusion
In this paper, we have analysed the evolution of linearly nearly Euclidean metrics under the Ricci-
DeTurck flow, including proof of convergence in short and infinite time. Furthermore, we construct
a linearly nearly Euclidean metric with the assist of the information geometry and use it as a spring-
board to reach the approximation of gradient flow. In view of the convergence and stability of
metrics, the neural network trained by the approximated gradient flow allows us to observe
the relevance between Ricci flow and neural network behavior under the manifold micro-
surgery. We hope that this paper will open an exciting future direction which will use Ricci flow to
assist neural network training in a manifold.
References
S-i Amari and H Nagaoka. Methods of information geometry, volume 191 of translations of math-
ematical monographs, s. kobayashi and m. takesaki, editors. American Mathematical Society,
Providence, RL USA,pp. 2-19, 2000.
Shun-Ichi Amari. Natural gradient works efficiently in learning. Neural computation, 10(2):251—
276, 1998.
Shun-ichi Amari. Information geometry and its applications, volume 194. Springer, 2016.
Alexander Appleton. Scalar curvature rigidity and ricci deturck flow on perturbations of euclidean
space. Calculus of Variations and Partial Differential Equations, 57(5):1-23, 2018.
9
Under review as a conference paper at ICLR 2022
Richard H Bamler. Stability of hyperbolic manifolds with cusps under ricci flow. arXiv preprint
arXiv:1004.2058, 2010.
Richard Heiner Bamler. Stability of Einstein metrics of negative curvature. Princeton University,
2011.
Michele Basseville. Divergence measures for statistical data processing——an annotated bibliography.
SignalProcessing, 93(4):621-633, 2013.
Arthur L Besse. Einstein manifolds. Springer Science & Business Media, 2007.
Lev M Bregman. The relaxation method of finding the common point of convex sets and its applica-
tion to the solution of problems in convex programming. USSR computational mathematics and
mathematical physics, 7(3):200-217, 1967.
SebaStien Bubeck et al. Convex optimization: Algorithms and complexity. Foundations and
TrendsR in Machine Learning, 8(3-4):231-357, 2015.
George Cybenko. Approximation by superpositions ofa sigmoidal function. Mathematics of control,
signals and systems, 2(4):303-314, 1989.
Alix Deruelle and Klaus Kroncke. Stability of ale ricci-flat manifolds under ricci flow. The Journal
of Geometric Analysis, 31(3):2829-2870, 2021.
Dennis M DeTurck. Deforming metrics in the direction of their ricci tensors. Journal of Differential
Geometry, 18(1):157-162, 1983.
Baptiste Devyver. A gaussian estimate for the heat kernel on differential forms and application to
the riesz transform. Mathematische Annalen, 358(1):25-68, 2014.
Samuel Glass, Simeon Spasov, and Pietro Lio. Riccinets: Curvature-guided pruning of high-
performance neural networks using ricci flow. arXiv preprint arXiv:2007.04216, 2020.
Richard S Hamilton et al. Three-manifolds with positive ricci curvature. J. Differential geom, 17
(2):255-306, 1982.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recog-
nition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp.
770-778, 2016.
Kurt Hornik. Approximation capabilities of multilayer feedforward networks. Neural networks, 4
(2):251-257, 1991.
Vishnu Jejjala, Damian Kaloni Mayorga Pena, and Challenger Mishra. Neural network approxima-
tions for calabi-yau metrics. arXiv preprint arXiv:2012.15821, 2020.
Herbert Koch and Tobias Lamm. Geometric flows with rough initial data. Asian Journal of Mathe-
matics, 16(2):209-235, 2012.
Norihito Koiso. Einstein metrics and complex structures. Inventiones mathematicae, 73(1):71-106,
1983.
Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images.
2009.
Olga Aleksandrovna Ladyzhenskaia, Vsevolod Alekseevich Solonnikov, and Nina N Ural’tseva.
Linear and quasi-linear equations of parabolic type, volume 23. American Mathematical Soc.,
1988.
James Martens. New insights and perspectives on the natural gradient method. Journal of Machine
Learning Research, 21:1-76, 2020.
Vincent Minerbe. Weighted sobolev inequalities and ricci flat manifolds. Geometric and Functional
Analysis, 18(5):1696-1749, 2009.
10
Under review as a conference paper at ICLR 2022
Tommaso Pacini. Desingularizing isolated conical singularities: uniform estimates via weighted
sobolev spaces. arXiv preprint arXiv:1005.3511, 2010.
Oliver C Schnurer, Felix Schulze, and Miles Simon. Stability of euclidean space under ricci flow.
arXiv preprint arXiv:0706.0421, 2007.
Natasa Sesum. Linear and dynamical stability of ricci-flat metrics. Duke Mathematical Journal, 133
(1):1-26, 2006.
Nick Sheridan and Hyam Rubinstein. Hamilton’s ricci flow. Honour thesis, 2006.
Robert M Wald. General relativity. University of Chicago press, 2010.
11
Under review as a conference paper at ICLR 2022
A Differential Geometry
1.	Riemann curvature tensor (Rm) is a (1,3)-tensor defined for a 1-form ω:
Rijk31 = ViVjSk - Vj ViSk
where the covariant derivative of F satisfies
lk
V Fj1...jl = ∂ F j1...jl + X Fj1...q...jlΓjs	X Fj1...jl Γq
p i1...ik = p i1...ik +	i1...ik pq -	i1...q...ik	pis .
s=1	s=1
In particular, coordinate form of the Riemann curvature tensor is:
Rlijk = ∂iΓljk -∂jΓlik+ΓjpkΓlip -ΓipkΓljp.
2.	Christoffel symbol in terms of an ordinary derivative operator is:
rkjj = 2gkl(digjl + dj gi1 - d1gij).
3.	Ricci curvature tensor (Ric) is a (0,2)-tensor:
Rij = Rppij .
4.	Scalar curvature is the trace of the Ricci curvature tensor:
R=gijRij.
5.	Lie derivative of F in the direction 与卢:
where 夕(t) : M → M for t ∈ (-e, e) is a time-dependent diffeomorphism of M to M.
B Proof for the Ricci Flow
B.1 Proof for Lemma 1
Definition 4 The linearization of the Ricci curvature tensor is given by
D[RicKh)ij = - 2 gpq (VpVq hij + ViV j hpq - Vq Vihjp - Vq Vjhip)∙
Proof. Based on Appendix A, we have
VqVihjp
ViVqhjp-Rqrijhrp-Rqriphjm.
Combining with Definition 4, we can obtain the deformation equation because of Vkgij = 0,
D[-2Ric](h)ij =gpqVpVqhij +Vi
2Vjhpq -
+ O(hij)
=gpqVpVqhij +ViVj +VjVi+O(hij).

12
Under review as a conference paper at ICLR 2022
B.2 Description of the DeTurck Trick
Using a time-dependent diffeomorphism 夕(t), We express the pullback metrics g(t):
g(t) = ψ*(t)g(t)
is a solution of the Ricci floW. Based on the chain rule for the Lie derivative in Appendix A, We can
calculate
∂ W	∂W*(t项t))
∂tg(t) = 一∂一
=(d 3*(t + T )g(t + T))}
=I	dτ	τ==0
Q*(t)
∂g(t + T) ∖
-∂T-)
+
τ=0
∂ d(2*(t + T )g(t))
V ∂T
∂
中⑴∂i9(t) +ψ (t)L笔tg(t).
With the help of Equation (1), for the reparameterized metric, We have
∂∂
ɪg(t) = φ (t)再g(t) + φ (t)L∂e∩)g(t) = -2RiCW (t)g(t)) = -2中(t) Ric(g(t)).
∂t	∂t	∂t
The diffeomorphism invariance of the Ricci curvature tensor is used in the last step. The above
equation is equivalent to
∂
"K7g(t) = -2 RiC(U(t)) - LdHt) g(t).
∂t	~∂Γ~
B.3 Proof for Theorem 3
Considering any x ∈ M, t0 ∈ [0, T ), V ∈ TxM, We have
log ；
t0
t0 ∂
∂t [loggχ(t)(V,V)] dt
等gx(t)(V, V) dt
gχ(t)(V,V)
t0
≤
0
≤Z
0
t0
∂
∂tgx(t)
∂
∂tgx(t)
ΓV⅛), ΓV⅛))ldt
dt
g(t)
≤ C.
By exponentiating both sides of the above inequality, We have
e-Cgx(0)(V,V) ≤gx(t0)(V,V) ≤eCgx(0)(V,V).
This inequality can be reWritten as
e-Cgx(0) ≤gx(t0)(V,V) ≤eCgx(0)(V,V)
because it holds for any V . Thus, the metrics g(t) are uniformly equivalent to g(0).
NoW, We have the Well-defined integral:
T∂
gχ(T )-gχ(0)=( ∂tgx(t)dt.
We say that this integral is Well-defined because of tWo reasons. Firstly, as long as the metrics
are smooth, the integral exists. Secondly, the integral is absolutely integrable. Based on the norm
inequality induced by g(0), one has
∣gχ(τ)-gχ(t)∣g(0) ≤ /T
∂
∂tgx(t)
dt.
g(0)
13
Under review as a conference paper at ICLR 2022
For each x ∈ M, the above integral will approach to zero as t approaches T . Because M is compact,
the metrics g(t) converge uniformly to a continuous metric g(T ) which is uniformly equivalent to
g(0) on M. Moreover, we can show that
e-Cgx(0) ≤ gx (T) ≤ eCgx (0).
C Proof for the Information Geometry
C.1 Proof for Theorem 7
The linearly nearly Euclidean divergence can be defined between two nearby points ξ and ξ 0 , where
the first derivative of the linearly nearly Euclidean divergence w.r.t. ξ0 is:
∂ξ0 DLNE [ξ0 : ξ]
=X ∂ξ0 J logcosh(τξi) - ∂ξο * logcosh(τξi) - T∂ξο(ξi - ξi) tanh(τξi)
i
=X ∂ξο * log Cosh(τξ0) - T tanh(τξ).
i
The second derivative of the linearly nearly Euclidean divergence w.r.t. ξ0 is:
∂ξοDlne[ξ0 ： ξ] = X ∂ξ T- logcosh(Tξ0).
i
We deduce the Taylor expansion of the linearly nearly Euclidean divergence at ξ0 = ξ:
DLNE [ξ0 : ξ] ≈ DLNE [ξ : ξ] + (X ∂ξ0 TT- logcosh(τξi) - Itanh(Tξ))
dξ
ξ0=ξ
+ 2 dξ>
3 logcosh(τξ0) ) I	dξ
T	ξ0=ξ
0 + 0 + 2⅛ dξ> d
∂cosh(τξ)]
Cosh(Tξ) ] ξ
dξ^dζt
2τ2 ξ
> ∂2 cosh(τ ξ) cosh(τ ξ) - ∂ cosh(τξ)∂ cosh(τξ)>
co
1 dξ> ( d2 cosh(T第 _ T2
2t 2	y Cosh(T ξ)
)sh2(τ ξ)
sinh(τ ξ)	sinh(τ ξ)
cosh(τ ξ)	cosh(τ ξ)
dξ
> dξ
2 X δij - [tanh(τξ)tanh(τξ)>]ij dξidξj.
i,j
C.2 Proof for Lemma 3
We would to know in which direction minimizes the loss function with the constraint of the linearly
nearly Euclidean divergence, so that we do the minimization:
dξ* =	arg min	L(ξ + dξ)
dξ St DLNE [ξ± + dξ] = c
where c is the constant. The loss function descends along the manifold with constant speed, regard-
less the curvature.
Now, we write the minimization in Lagrangian form. Combined with Theorem 7, the linearly nearly
Euclidean divergence can be approximated by its second order Taylor expansion. Approximating
L(ξ + dξ) with it first order Taylor expansion, we get:
dξ* = arg min L(ξ + dξ) + λ (DLNE [ξ : ξ + dξ] - C)
dξ
≈ arg min L(ξ) + ∂ξL(ξ)>dξ + Kdξ>gj (t)dξ — cλ.
dξ	2
14
Under review as a conference paper at ICLR 2022
To solve this minimization, we set its derivative w.r.t. dξ to zero:
∂λ
0 = ∂dξL(ξ) + dξL(ξ) dξ + 2dξ [δij - tanh(τξ(t)) tanh(τξ(t)) ] dξ - cλ
= ∂ξL(ξ) + λ δij - tanh(τ ξ(t)) tanh(τ ξ(t))> dξ
dξ = —ɪ [δj — tanh(τξ(t)) tanh(τξ(t))>] 1 ∂ξL(ξ)
λ
where a constant factor 1∕λ can be absorbed into learning rate. UP to now, We get the optimal
descent direction, i.e., the opposite direction of gradient which takes into account the local curvature
defined by [δij — tanh(τξ(t)) tanh(τ ξ(t))>]-1.
C.3 Proof for Corollary 3
Definition 5 For A ∈ Rn×n, A is called as the strictly diagonally-dominant matrix when it satisfies
n
aii >	aij,
j=1,j6=i
i = 1,2, . . . , n.
Definition 6 If A ∈ Rn×n is a strictly diagonally-dominant matrix, then A is a nonsingular matrix
together.
Subsequently, we can consider the inverse matrix of the metric g(t). Due to the
strictly diagonally-dominant feature in Definition 5 and Definition 6, we can approximate
δij — tanh(τ ξ(t)) tanh(τ ξ(t))> -1. As for we can also ignore the fourth-order small quantity
P O(ρaρbρcρd) because of the characteristic of the strictly diagonally-dominant, so that
[δij — tanh(τ ξ (t)) tanh(τ ξ(t))>] [δij + tanh(τ ξ(t)) tanh(τ ξ (t))> ]
1 — PlPl	—P1P2
—P2P1	1 — P2P2
1 + P1 P1	P1 P2
P2P1	1 + P2P2
1 — ∑ O(PaPbPcPd)	P1P2 — P1P2 一 ∑ O(PaPbPcPd)	…
—P2P1 + P2P1 — P O(PaPbPcPd)	1 — P O(PaPbPcPd)	… ≈ I
Note that the Euclidean metric δij is equal to the identity matrix I .
D	Analysis on Linearly Nearly Euclidean Metrics
Let us give the definition of linearly nearly Euclidean metrics without further explanation:
Definition 7 A complete Riemannian n-manifold (Mn, g0) is said to be linearly nearly Euclidean
with one end of order τ > 0 if there exists a compact set K ⊂ M, a radius r, a point x in M
and a diffeomorphism satisfying φ : M\K → (Rn\B(x, r))∕SO(n), where B(x, r) is the ball and
SO(n) is a finite group acting freely on Rn \{0}, then
∣∂k(φ*γo)∣δ =O(r-τ-k) ∀k ≥ 0	(14)
holds on (Rn\B(x, r))∕SO(n). g0 can be linearly decomposed into a form containing the Eu-
clidean metric δ:
g0(t) = δ + γ0(t).	(15)
In this paper, we consider the linear stability and integrability of the initial metric g0 . Fortunately,
similar to the proof process of (Koiso, 1983; Besse, 2007), we can proceed that g0 is integral and
linearly stable.
15
Under review as a conference paper at ICLR 2022
Definition 8 A complete linearly nearly Euclidean n-manifold (Mn, g0) is said to be linearly stable
ifthe L spectrum ofthe Lichnerowicz operator Lg0 := ∆g0 + 2 Rm(g0)* is in (一∞, 0] where ∆g0
is the Laplacian, when Lg0 acting on dij satisfies
Lgo (d) = ∆go d + 2Rm(go) * d
= ∆g0d+2Rm(g0)ikljdmng0kmg0ln.
Definition 9 A n-manifold (Mn, g0) is said to be integrable ifa neighbourhood ofg0 has a smooth
structure.
E SHORT TIME CONVERGENCE IN THE L2-NORM
For convenience, We rewrite the Ricci-DeTUrck flow (3) in terms of the difference d(t) := g(t) - go,
such that
∂∂
^K7d(t) = "K7g(t) = -2 Rie(U(U) + 2 Rie(UO) + L∂中Kt g0 - L d中(t g(t)
∂t	∂t	-∂t-	∂t
=∆d(t) + Rm *d(t) + Fg-1 * Vg0d(t) * Vg0d(t) + Vg0 (Gr(g0) * d(t) * Vg0d(t)),
(16)
where the tensors F and G depend on gU-1 and Γ(gU0). Note that gU0 is a linearly nearly EUclidean
metric which satisfies the above formUla, where d0(t) = gU0(t) - gU0, so that d(t) - d0(t) = gU(t) -
go(t) holds. Note that ∣∣ ∙ ∣∣l2 or k ∙ ∣∣l∞ denotes the L2-Norm or L∞-Norm with respect to the
metric gU0 .
Lemma 4 Let (Mn, gU0) be a complete linearly nearly Euclidean n-manifold. If gU(0) is a metric
satisfying ∣gU(0) - gU0 ∣L∞ < where > 0, then there exists a constant C < ∞ and a unique
RiCCi-DeTUrCkflow g(t) that satisfies
kg(t) - UOllL∞ < CIlU(O)- UOllL∞ < C ∙ e.
Ifa RiCCi-DeTurCk flow in BL∞ (gU0, ) for t ≥ 1, there exist Constants suCh that
Vk (gU(t) - gUO)L∞ < C(k), ∀k∈N.
Proof. The similar statement for the case of negative Einstein metrics is given in (Bamler, 2010).
The proofs can be translated easily to the case of linearly nearly EUclidean metrics by referring the
details (Bamler, 2011).
Lemma 5 Let (Mn, gUO) be a linearly nearly EuClidean n-manifold. For a RiCCi-DeTurCk flow gU(t)
on a maximal time interval t ∈ [0, T ), if it satisfies ∣gU(0) - gUO ∣L∞ < where > 0, then there
exists a Constant C < ∞ for t ∈ (0, T ) suCh that
∣gU(t) - gUO ∣L2 < C.
(17)
Proof. Based on Lemma 4, we can consider aboUt ∣gU(t) - gUO∣L2 . Let κ be a fUnction sUch that
K = 1 on B(x, r), κ = 0 on Mn∖B(x, 2r) and ∣Vκ∣ ≤ 2/r where X ∈ Mn and a radius r.
16
Under review as a conference paper at ICLR 2022
Followed by Equation (16), we obtain
ɪ I	∣d(t)∣2κ2dμ ≤2	[	<∆d(t),	κ2d(t))	dμ +	Ck	Rm ∣∣l∞	(	∣d(t)∣2κ2dμ
∂t M	M	M
+ C∣d(t)kL∞/ ∣Vd(t)∣2κ2dμ + / {V(Gγ * d *Vd),d> κ2dμ
≤ - 2/ ∣Vd(t)∣2κ2dμ + c/ ∣Vd(t)∣∣d(t)∣∣Vκ∣κdμ
+ C(g0) f ∣d(t)∣2κ2dμ + C∣∣d(t)∣∣L∞ [ ∣Vd(t)∣2κ2dμ
MM
≤(-2 + C ∙ E + Ci) [ ∣Vd(t)∣2κ2dμ + C(go) [ ∣d(t)∣2κ2dμ
MM
+ ɪ Z ∣d(t)∣2∣Vκ∣2dμ
C1 M
2
≤	C(UO) +
I	∣d(t)∣2dμ.
B(x,2r)
Note that we can always find a suitable C1 to make the above formula true. By integration in time t,
we can further obtain
Zm'Sdμ≤ Ad(0)l2κ2dμ + (C(g。)+ C⅛) ZZbs
Consequently, we can find a finite ball that satisfies this estimate.

Corollary 4 Based on Lemma 5, we further have
sup / ∣d(t)∣2κ2dμ < ∞.
M
∣d(s)∣2dμds < ∞.
(18)
Proof. We obtain
sup / ∣d(t)∣2κ2dμ ≤sup / ∣d(0)∣2κ2dμ
MM
+ N(C (go) + 7^-7] [ sup [ ∣d(s)∣2κ2dμds,
C1 r2	0 M
where each ball of radius 2r on M can be covered by N balls of radius r because (Mn, ggo) is
linearly nearly Euclidean. By the Gronwall inequality, we have
sup/ ∣d(t)∣2κ2dμ ≤ exp (N (C (go) +	t) Sup/ ∣d(0)∣2κ2dμ.
For the L2-Norm, the Ricci-DeTurck flow in linearly nearly Euclidean manifolds has a solution for
a short time.

17