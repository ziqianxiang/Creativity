Under review as a conference paper at ICLR 2022
Towards Understanding Distributional Rein-
forcement Learning: Regularization, Opti-
mization, Acceleration and Sinkhorn Algo-
RITHM
Anonymous authors
Paper under double-blind review
Ab stract
Distributional reinforcement learning (RL) is a class of state-of-the-art algorithms
that estimate the whole distribution of the total return rather than only its expec-
tation. Despite the remarkable performance of distributional RL, a theoretical un-
derstanding of its advantages over expectation-based RL remains elusive. In this
paper, we interpret distributional RL as entropy-regularized maximum likelihood
estimation in the neural Z-fitted iteration framework, and establish the connection
of the resulting risk-aware regularization with maximum entropy RL. In addi-
tion, We shed light on the stability-promoting distributional loss with desirable
smoothness properties in distributional RL, which can yield stable optimization
and guaranteed generalization. We also analyze the acceleration behavior while
optimizing distributional RL algorithms and show that an appropriate approxi-
mation to the true target distribution can speed up the convergence. Finally, we
propose a class of Sinkhorn distributional RL algorithm that interpolates between
the Wasserstein distance and maximum mean discrepancy (MMD). Experiments
on a suite of Atari games reveal the competitive performance of our algorithm
relative to existing state-of-the-art distributional RL algorithms.
1 Introduction
The intrinsic characteristics of classical reinforcement learning (RL) algorithms, such as temporal-
difference (TD) learning (Sutton & Barto, 2018) and Q-learning (Watkins & Dayan, 1992), are based
on the expectation of discounted cumulative rewards that an agent observes while interacting with
the environment. In stark contrast to the classical expectation-based RL, a new branch of algorithms
called distributional RL estimates the full distribution of total returns and has demonstrated state-
of-the-art performance in a wide range of environments (Bellemare et al., 2017a; Dabney et al.,
2018b;b; Yang et al., 2019; Zhou et al., 2020; Nguyen et al., 2020). Meanwhile, distributional RL
has also enjoyed further benefits in risk-sensitive control, policy exploration settings (Mavrin et al.,
2019; Rowland et al., 2019) and robsutness (Sun et al., 2021).
Despite the existence of numerous algorithmic variants of distributional RL with remarkable empir-
ical success, theoretical studies of advantages of distributional RL over expectation-based RL are
less established. Existing works include (Lyle et al., 2019) that proved in many tabular and linear
approximation settings, distributional RL behaves exactly the same as expectation-based RL. Lyle
et al. (2021) investigated the impact of distributional RL from the perspective of representation dy-
namics. Martin et al. (2020) recently mapped distributional RL problems to a Wasserstein gradient
flow problem, treating the distributional Bellman residual as a potential energy functional. Offline
distributional RL (Ma et al., 2021) has also been proposed to investigate the efficacy of distributional
RL in both risk-neutral and risk-averse domains. Recent works have tended towards closing the gap
between theory and practice in distributional RL.
From an algorithmic perspective, the Sinkhorn loss (Sinkhorn, 1967) can be used to tractably ap-
proximate the Wasserstein distance and has been successfully applied in numerous crucial machine
learning developments, including the Sinkhorn-GAN (Genevay et al., 2018) and Sinkhorn-based
adversarial training (Wong et al., 2019). Inspired by the distributional RL literature, Martin et al.
1
Under review as a conference paper at ICLR 2022
(2020) argues for the use of a second-order stochastic dominance relation to select among a mul-
tiplicity of competing solutions via Sinkhorn iteration (Sinkhorn, 1967), which can be useful to
manage stochastic uncertainty in RL paradigms. However, a Sinkhorn-based distributional RL al-
gorithm has not yet to be formally proposed and investigated.
In this paper, we theoretically illuminate the superiority of distributional RL over expectation-based
RL from the perspectives of regularization, optimization, acceleration, and representation. Specif-
ically, we simplify distributional RL as a neural Z-fitted iteration, within which we establish an
equivalence between distributional RL and a form of entropy-regularized maximum likelihood esti-
mation (MLE). We also demonstrate that the resulting novel cross entropy regularization correlates
strongly with the behavior of maximum entropy RL. By incorporating a histogram distributional
loss, we further achieve stable optimization and guaranteed generalization of distributional RL, at-
tributable to desirable smoothness properties of the distribution loss. We further characterize the
effect of acceleration on distributional RL and discuss when distributional RL algorithms are effec-
tive in various environments. After gaining insights from the theoretical advantages of distributional
RL, we further propose a novel distributional RL algorithm based on Sinkhorn loss that interpolates
between the Wasserstein distance and maximum mean discrepancy (MMD). Our approach allows
us to find a trade-off that simultaneously leverages the geometry of the Wasserstein distance and the
favorable high-dimensional sample complexity and unbiased gradient estimates of MMD. In sum-
mary, our analysis opens the door to a deeper understanding of theoretical advantage of distributional
RL.
2	Preliminary Knowledge
In the standard RL setting, an agent interacts with an environment via a Markov decision pro-
cess (MDP), a 5-tuple (S , A, R, P, γ), where S and A are the state and action spaces, respectively.
P is the environment transition dynamics, R is the reward function and γ ∈ (0, 1) is the discount
factor.
State-value function vs. state-value distribution. Given a policy π, the discounted sum of
future rewards is a random variable Zπ(s, a) = Pt∞=0 γtR(st, at), where s0 = s, a0 = a,
st+1 〜 P(∙∣st, at), and at 〜 ∏(∙∣st). In the control setting, expectation-based RL focuses on the
state-value function Qπ(s, a), which is the expectation of Zπ(s, a), i.e., Qπ(s, a) = E [Zπ(s, a)].
Distributional RL, on the other hand, focuses on the state-value distribution, the full distribution
of Zπ (s, a). Leveraging knowledge on the entire distribution can better capture the uncertainty of
returns in the MDP beyond the expectation of return (Dabney et al., 2018a; Mavrin et al., 2019).
Bellman operators vs. distributional Bellman operators. For the policy evaluation in
expectation-based RL, the value function is updated via the Bellman operator T πQ(s, a) =
E[R(s, a)] + γEs0〜p,∏ [Q (s0, a0)]. In distributional RL, the state-value distribution of Zπ(s, a) is
updated via the distributional Bellman operator Tπ
TπZ(s, a) = R(s, a) + γZ (s0, a0) ,	(1)
where s0 〜P(∙∣s, a) and a0 〜π (∙∣s0).
From a theoretical perspective, both the Bellman operator Tπ in the policy evaluation setting and the
Bellman optimality operator T in the control setting are contractive in the stationary policy case. In
contrast, the distributional Bellman operator Tπ is contractive under certain distribution divergence
metrics, but the distributional Bellman optimality operator T can only converge to a set of optimal
non-stationary value distributions in a weak sense (Elie & Arthur, 2020).
3	Effect of Regularization on Distributional RL
Although the theoretical framework of distributional RL in the tabular setting has been basically
established mentioned in Section 2, the theoretical understanding of its advantages over expectation-
based RL has been less studied. In this section, we attribute the superiority of distributional RL into
its regularization effect.
2
Under review as a conference paper at ICLR 2022
3.1	Distributional RL: Neural Z-fitted Iteration
Neural Q-Fitted Iteration. In the function approximation setting, Deep Q Learning (Mnih et al.,
2015) can be simplified into Neural Q-Fitted Iteration (Fan et al., 2020) under tricks of experience
replay and the target network Qθ* , where We update parameterized Qθ (s, a) in each iteration k:
1n	2
Qk+ = argmin- X [yi - Qk (si, ai)] ,	(2)
Qθ n i=1
where the target yi = r(si, ai) + γ maxa∈A Qθk* (s0i, a) is fixed within every Ttarget steps to up-
date target network Qθ* by letting θ* = θ and the experience buffer induces independent samples
{(si, ai, ri, s0i)}i∈[n]. In an ideal case that neglects the non-convexity and TD approximation errors,
we have Qθk+1 = TQθk, which is exactly the updating under Bellman optimality operator. Under the
two conditions, the optimization problem in Eq. 2 can be viewed as Least Square Estimation (LSE)
in a neural network parametric regression problem between the updating of target network Qθ* .
Neural Z-Fitted Iteration. Analogous to neural Q-fitted iteration, we can also simplify value-based
distributional RL methods based on a parameterized Zθ into a Neural Z-fitted Iteration as
1n
Zk+1 = argmin- Edp(Yi,Zk (si,ai)),	(3)
Zθ n i=1
where the target	Yi	=	R(si,	ai)	+ γZθk*	(s0i, πZ(s0)) with	πZ(s0)	= argmaxa0 E	Zθk*	(s0,	a0)	is
fixed within every Ttarget steps to update target network Zθ* , and dp is a divergence metric between
two distributions. Notably, the options of representation manner on Zθ and the metric dp are pivotal
for the empirical success of distributional RL algorithms. For instance, QR-DQN (Dabney et al.,
2018b) approximates Wasserstein distance Wp, which leverages quantiles to represent the distri-
bution of Zθ. C51 (Bellemare et al., 2017a) represents Zθ via a categorical distribution under the
convergence of Cramer distance (Bellemare et al., 2017b; Rowland et al., 2018), a special case with
p = 2 of the `p distance (Elie & Arthur, 2020), while Moment Matching (Nguyen et al., 2020)
learns deterministic samples to represent the distribution of Zθ based on Maximum Mean Discrep-
ancy (MMD). Contractive properties under typical metrics dp can be summarized as follows
•	Tπ is γ-contractive under the supreme form of Wassertein distance Wp.
•	Tπ is y1/p-contractive under the supreme form of 'p distance.
•	Tπ is γα/2-contractive under MMD∞ with the kernel kα(x, y) = -Ilx - y∣∣α, ∀α ∈ R.
For the completeness, the definition of mentioned distances and the proof of contraction are pro-
vided in Appendix A. Although the widely used Kullback-Leibler (KL) divergence is not a contrac-
tion (Morimura et al., 2012), we show in Proposition 1 that the KL divergence still enjoys desirable
properties in distributional RL context, which can be reasonable for the theoretical analysis. We
assume Zθ is absolutely continuous and has joint supports, under which the KL divergence is well-
defined. Proof of Proposition 1 and the definition of supreme DKL are provided in Appendix B.
Proposition 1. Denote the supreme of DKL as DK∞L, we have: (1) Tπ is a non-expansive operator
underDK∞L, i.e., DK∞L(TπZ1,TπZ2) ≤ DK∞L(Z1,Z2), (2)DK∞L(Zn,Z) → 0 implies Wp(Zn, Z) → 0,
(3) the expectation of Zπ is still γ-contractive, i.e., IETπ Z1 - ETπ Z2 I∞ ≤ γ IEZ1 - EZ2I∞.
3.2	Distributional RL: A Novel Entropy-regularized MLE
The reasonable properties of KL divergence in Proposition 1 allows us to leverage it to conduct the
theoretical analysis. To separate the impact of additional distribution information from the expec-
tation of Zπ , we leverage the variant technique of gross error model from robust statistics (Huber,
2004), similar to the technique to analyze Label Smoothing (Muller et al., 2019) and Knowledge
Distillation (Hinton et al., 2015). Specifically, we denote the one-dimensional full distribution of
Zπ as F, and the distribution on the remaining support getting rid of E [Zπ] as Fμ. Hence, we can
obtain the distribution decomposition for Zπ(s, a) as
F s,a(X) = (I-E) l{x≥E[Z ∏(s,a)]} (X) + EFμ,a(X),	(4)
3
Under review as a conference paper at ICLR 2022
where E controls the proportion of Fμ,a(x) and the indicator function 1{x≥e[z∏(s,α)]} = 1 if X ≥
E [Zπ(s, a)], otherwise 0. After taking derivatives on both sides, we attain the relationship of their
density functions as ps,a(x) = (1 - e)δ{χ=E[z∏(S,a)]}(x) + eμs,a(x), where μs,a(x) is the density
function related to Zπ(s, a) on remaining supports removing E [Zπ(s, a)]. It is worth noting that
the existence of μs,a(x) can be simply guaranteed by directly computing μs,a(x) = ps,a(x)/e -
(1 - e)δ{χ=E[z∏(s,a)]}∕e as long as ps,a(x) and the expectation of Zπ(s,a) exist. Next, We use
ps,a(x) and qθs,a(x) to denote the density distributions behind {Yi}i∈[n] and Zθk(s, a) in neural Z-
fitted iteration via Eq. 3, respectively. Therefore, we can derive the following result in Proposition 2.
Proposition 2. Let H(P, Q) as the cross entropy, i.e., H(P, Q) = - x∈X P(x) log Q(x) dx. Let
α be a positive constant, and based on the decomposition in Eq. 4 and DKL as dp, Neural Z-fitted
iteration in Eq. 3 can be reformulated as
1n
Zk+1 = argmin— V H(δ{χ=E[z∏(si,ai)]}, qsi,ai) + αH(μsi,ai,qy).	⑸
Zθ	n i=1
We provide the proof in Appendix C. For the uniformity of notation, we still use s, a in the following
analysis instead of si , ai in Eq. 5. Importantly, the first term in Eq. 5 can be further simplified as
- x∈X log qθs,a(E [Z(s, a)]). Minimizing this first term can be viewed as a variant of Maximum
Likelihood Estimation (MLE) on the expectation E [Z (s, a)] rather than the traditional MLE directly
on observed samples. The cross entropy regularization in the second term pushes qθs,a to approximate
the distribution μs,a in order to fully utilize the additional distributional information while learning,
serving as the key to the superiority of distributional RL. This novel cross entropy regularization
regarding μs,a and qma is different from the classical entropy regularization used in RL, which we
further analyze their connection and discrepancy in Section 3.3. In summary, distributional RL can
be simplified as a novel entropy-regularized MLE within neural Z-fitted iteration framework in stark
contrast to the Least-Square estimation of expectation-based RL in the neural Q-fitted iteration.
3.3	Connection with Maximum Entropy RL
We establish the connection between the derived novel cross entropy regularization in Eq. 5 in
distributional RL with the classical maximum entropy RL (Williams & Peng, 1991). Maximum
entropy RL, including Soft Q-Learning (Haarnoja et al., 2017), greedily maximizes the entropy of
the policy ∏(∙∣s) in each state:
T
J(∏) = XE(st,at)〜ρ∏ [r (st, at) + βH(∏(∙∣st))],	⑹
t=0
where H (∏θ (∙∣st)) = - Pa ∏θ (a|st) log∏θ (a|st) and ρ∏ is the generated distribution following
π. The temperature parameter β determines the relative importance of the entropy term against the
cumulative rewards, and thus controls the stochasticity of the optimal policy. This maximum en-
tropy regularization has various conceptual and practical advantages. Firstly, the policy encourages
exploration, avoiding situations in which the agent might fall into a local optimum. Secondly, it
considerably improves learning speed over classical methods and therefore are widely used in state-
of-the-art algorithms, e.g., Soft Actor-Critic (SAC) (Haarnoja et al., 2018) and PPO (Schulman et al.,
2017). Lastly, maximum entropy can yield more robustness to abnormal or rare events while devel-
oping a task. Similar robustness superiority of distributional RL against noisy state observations has
been investigated in (Sun et al., 2021).
Similar benefits of both distributional RL and maximum entropy RL motivate us to explore
their mathematical connection, and we refocus on the Bellman updating based on these two
kinds of entropy-based regularization. In particular, Soft Policy Evaluation proposed in SAC
has shown that by alternately iterating the following equations, the algorithm is equivalent to
maximize the maximum entropy RL objective function in Eq. 6: TπQ (st, at) , r (st, at) +
YEst+1 〜ρ∏ [V (st+ι)], where V (st+ι) = Eat+ι^∏ [Q (st+ι,at+ι) - log∏ (at+1∣st+1)] is the soft
value function (Haarnoja et al., 2018). Interestingly, we still have the similar convergence result
related to our novel cross entropy regularization in Eq. 5, which is shown in Theorem 1:
Theorem 1. Consider the soft Bellman operator Tdπ in Eq. 7 and a mapping Q0 : S × A → R
with ∣A∣ ≤ ∞, and define Qk+1 = TdQk. Given the true distribution μst,at for each t, then the
4
Under review as a conference paper at ICLR 2022
sequence Qk will converge to a soft Q-value ofπ as k → ∞ with the new entropy objective function
as JO(J) = PT=O E(st,at)〜ρ∏ [r (st, at) - Y H (μst'ɑt, qsetat)]：
TdQ (st, at) , r (st, at) + YEst+1 〜ρ∏ [V (St+1)],	⑺
where V (st+ι) = Eαt+1 〜∏,x〜μst+ι,at+ι [Q (st+ι,at+ι) + log qst+1,αt+1 (x)] and the policy ∏ fol-
lows the classical greedy policy rule，i.e., π(∙∣s) = arg max。E [Zθ (s, a)], determined by θ.
Please refer to Appendix D for the proof. The new en-
tropy augmented objective function J0(θ) in Theorem 1
reveals that different from adding the entropy regarding
the policy ∏(∙∣st) in maximum entropy RL, distributional
RL can be viewed as subtracting a cross entropy term be-
tween μ and qθ. Moreover, the entropy in maximum en-
tropy RL is state-wise, while our cross entropy regular-
ization is state-action-wise, which is a more fine-grained
characterization on the variability of the full distribution
of return Z.
Figure 1: Impact of the risk-aware reg-
ularization in distributional RL. The
entropy-based regularization will push
qS,α to match the true density func-
tion μs,α determined by the environ-
ment, and thus let the learned policy be
fully aware of the uncertainty of envi-
ronment.
Notably, we highlight that our derived cross entropy reg-
ularization is risk-aware. If the true state-action distri-
bution Fμ has higher degree of dispersion, e.g., a larger
variance, than the current qθ, minimizing our entropy au-
tomatically encourages the policy to explore the uncer-
tainty of the environment through changing Zθ . A sketch
map in Figure 1 is provided to illustrate the impact of our
risk-aware entropy regularization. We remark that this
risk-aware regularization may not necessarily encourage exploration under the greedy action rule
∏z(S) = argmaxa E [Zθ(s,a)] even though q® changes towards μ as exhibited in Figure 1. A
risk-aware action rule on the current greedy version might be preferred in the future.
4 Stable Optimization and Guaranteed Generalization
In this section, we further characterize qθs,α in order to attain stable optimization and guaranteed
generalization properties of distributional RL. Concretely, we leverage the histogram function fs,α
to parameterize the density function of Zθ (s, a), i.e., qθs,α, still based on the KL divergence as dp,
yielding the histogram distributional loss (Imani & White, 2018) within each update in the neural
Z-fitted iteration. Denote x(s) as the state feature on each state s, and we let the support of x(s) be
uniformly partitioned into k bins. We let the function fs,α : X → [0, 1]k provide a k-dimensional
vector f s,α (x(s)) of the coefficients indicating the probability the target is in that bin given the state
s and action a pair, and x(s). We use softmax based on the linear approximation x(s)>θi to express
fs,α, i.e., fs,a,θ(x(s)) = exp (x(s)>θ, / Pk=I exp (x(s)>θ j For simplicity, we use fθ(x(s))
to replace fis,α,θ(x(s)). Therefore, the resulting histogram distributional loss Lθ is formulated as:
Lθ(s, a)
DKL(ps,α,qθs,α)=
x∈X
ps,α(x) log (ps,α(x)/qS,α(x)) dx
k
- X pis,α log fiθ(x(s))
i=1
(8)
where θ = {θ1, ..., θk} and pis,α is the histogram probability ofps,α (x) defined in Eq. 4. The deriva-
tion of the histogram distributional loss is given in Appendix E. To attain the stable optimization
property of distributional RL, we firstly derive Lemma 1 as follows
Lemma 1. (Properties of Histogram Distributional Loss) Assume each state feature is bounded,
i.e., kx(s)k ≤ l, then Lθ is kl2-smooth, convex and kl-Lipschitz continuous w.r.t. θ.
Please refer to Appendix F for the proof. The derived Lipschitz properties of dp under histogram
distributional loss plays an integral part in the stable optimization of distributional RL, but the clas-
sical Least-Squared estimation in neural-Q-fitted iteration for expectation-based RL does not enjoy
these smoothness properties. In particular, for histogram distributional loss we have ||V®L® ∣∣ ≤ kl,
while the counterpart in expectation-based is |yi- Qθk(s, a)|kx(s)k, which might be arbitrarily large
5
Under review as a conference paper at ICLR 2022
due to the unbounded |yi - Qθk(s, a)|. To further derive the uniform stability of distributional RL
while running stochastic gradient descend (SGD), we introduce its definition in Definition 1.
Definition 1. (Uniform Stability) (Hardt et al., 2016) Consider a function g, a randomized algorithm
M is uniformly stable if for all data sets S, S0 such that S, S0 differ in at most one example, we have
sup EM [g(M(S); x) - g (M (S0) ;x)] ≤ stab .	(9)
x
Given the definition of uniform stability, we show that distributional RL under histogram distribu-
tional loss while running SGD is stab-uniformly stable and generalization guaranteed in Theorem 2.
Theorem 2. (Stable Optimization and Guaranteed Generalization) Suppose that we run SGD under
Lθ in Eq. 8 with step sizes λt ≤ 2/kl2 for T steps, and kx(s)k ≤ l for each state s, we have:
(1)	SGD under Lθ satisfies the uniform stability in Definition 1 with EStab ≤ 4kT,
(2)	Suppose that the data distribution is stable within each neural Z-fitted iteration, the generaliza-
tion gap for solving the entropy regularized MLE in Eq. 5 is Estab -bounded.
Please refer to the proof of Theorem 2 in Appendix G. In summary, the optimization to solve the
entropy regularized MLE of distributional RL within the update of neural Z-fitted iteration is stable
with the stability errors shrinking at the rate of O(n-1). This stability optimization properties is
ascribed to the smooth and convex properties from histogram distributional loss (non-convex case
still holds in (Hardt et al., 2016)) and can further control the generalization gap. By contrast, the
least-estimation in neural Q-fitted iteration for expectation-based RL, which is without these smooth
properties, can not yield the stable optimization and guaranteed generalization directly.
5	Acceleration Effect
Based on the setting in Section 4, we further incorporate the decomposition of Fs,a(x) proposed
in Eq. 4 into the analysis, where ps,a(x) = (1 - e)δ{χ=E[z∏(s,a)]}(x) + eμs,a(x), in order to
derive the acceleration effect of distributional RL. Within each update in the neural Z-fitted iter-
ation, the target is to minimize 1 Pn=1 L§(si, ai). We denote Gk(θ) as the expectation of Lg, i.e.,
Gk(θ) = E(s,a)〜ρ∏ [Lg(s, a)]. As such, the convex and smooth properties in Lemma 1 still hold
for Gk (θ). We use G(θ) for Gk (θ) for simplicity. As the KL divergence enjoys the property of
unbiased gradient estimates, we let the variance of its stochastic gradient over the expectation be
bounded, i.e., E(s,C)〜ρ∏ [∣∣VLθ(δ{χ=E[z∏(s,a)]}, qs,a)) - VG(θ)k2] = σ2. Next, We characterize
the approximation degree of qg,a to the ground-truth distribution μs,a defined in Eq. 5 by measuring
its variance as κσ2 :
E(s,a)〜ρ∏ [kVLg(μs,a, qS,a)) - VG(θ)k2] = κσ2.	(10)
Notably, a favorable approximation of qg,α to μg,a would lead to a small κ, in which case an ac-
celeration effect of distributional RL can be derive as shown in Theorem 3. Based on Eq. 10, we
immediately have the following lemma.
Lemma 2. Asps,a(x) = (1 — e)δ{χ=E[z∏(s,α)]}(x) + eμs,a(x), we have:
E(s,a)〜ρ∏ [kVLθ(ps,a, qS,a)) - VG(θ)k2] ≤ (1 - e)σ2 + eκσ2.	(11)
Please refer to Appendix H for the proof. To measure the convergence of the entropy regularized
MLE in distributional RL or least-square estimation in expectation-based RL, we need the following
definition of the first-order τ -stationary point.
Definition 2. (First-order τ-Stationary Point) While solving ming G(θ), the updated parameters θT
after T steps is a first-order τ -stationary point ifkVG(θT)k ≤ τ, where the small τ is in (0, 1).
We formally show our Theorem 3 to characterize the acceleration effect of distributional RL deter-
mined by the variance magnitude τ .
Theorem 3. (Acceleration Effect) While running SGD to solve the entropy-regularized MLE within
neural Z-fitted iteration in Eq. 5 with the step size λ = 1/kl2 and E = 1/(1 + τ), we have:
6
Under review as a conference paper at ICLR 2022
(1) The sample complexity is O(苴)ifwe only consider the expectation in ps,a, i.e., δ{χ=E[z∏(s,a)]}.
(2) When K ≤ 昌 and let T = 4GTθ0) ,the regularized MLE Ofdistributional RL within each neural
Zfitted iteration converges to a T-stationary point in expectation with sample Complexity O(表).
(3) When K > 总 and let T = GKT2), the regularized MLE of distributional RL does not converge
to a τ-stationary point, but we have E [kVG(θ)k2] ≤ O(K).
The proof is provided in Appendix I. Theorem 3 is inspired by (Xu et al., 2020) to analyze the
convergence of label smoothing, which can be similarly utilized to analyze the acceleration effect
of distributional RL. Importantly, we need to emphasize that Theorem 3 in fact reveals the reason
why distributional RL algorithms can achieve inconsistent superiority across different Atari games.
This phenomenon is mainly owing to the different degree of TD target approximation to ps,a , es-
PeciaiIy μs,a for various environments. In particular, a small approximation error of qe to P (or μ)
corresponds to a small κ, This can be normally ≤ T2∕4σ2, yielding better sample efficiency and
accelerating the algorithm relative to expectation-based RL. Conversely, an unsatisfactory approxi-
mation may not lead to the acceleration effect, but the bounded gradient normally corresponds to a
reasonable performance, coinciding with previous empirical observations on various environments.
Extension of Representation Effect. Apart from the acceleration effect, we also conduct some
empirical analysis of distributional RL from the perspective of representation. In particular, distri-
butional RL encourages state representation from the same action class classified by the policy in
tighter clusters. Please refer to Appendix K for more details.
6	Algorithm: S inkhorn Distributional RL
Through the theoretical analysis, we can gain insights into the advantages of distribution RL over
expectation-based RL, attributing to entropy-based regularization, stable optimization and accelera-
tion effect determined by the distributional approximation error (measured by K). From the algorith-
mic perspective, the choice of representation manner on Zθ and the distributional divergence metric
dp mentioned in Section 3.1 are pivotal for the final performance of distributional RL algorithms.
we further design Sinkhorn distributional RL algorithm. Sinkhorn loss (Sinkhorn, 1967) is a
tractable loss to approximate optimal transport problem by leveraging an entropic regularization to
turn the original Wasserstein distance into a differentiable and more robust quantity. The resulting
loss can be computed using Sinkhorn fixed point iterations, which is naturally suitable for modern
deep learning frameworks. In particular, the entropic smoothing generates a family of losses inter-
polating between Wasserstein distance and Maximum Mean Discrepancy (MMD). Thus it allows us
to find a sweet trade-off that simultaneously leverages the geometry of Wasserstein distance on the
one hand, and the favorable high-dimensional sample complexity and unbiased gradient estimates
of MMD. We introduce the entropic regularized Wassertein distance as
∏∈m(n,v) /c(x，y)dπ(χ, y)+ε /log QUnXxdy)y)dπ(X，y)，
(12)
where c is the cost function in optimal transport. This objective function associated with cost func-
tion C can be rewritten as Wc,ε(μ, V) = / c(x, y)d∏ε(x, y). Therefore, the sinkhorn loss between
two measures uand v is defined as
Wc,ε(u, v) = 2Wc,ε(u,v) — Wc,ε(u,u) - Wc,ε(v, v)
(13)
Theorem 4. If we leverage Sinkhorn loss as the metric in distributional RL and choose c as unrec-
tified kernel kα, i.e., kα(x, y) := -kx - ykα,for∀α ∈ R, we have:
(1)	As E → 0, Wc,ε(u, V) → 2Wα(u, V), and thus Tn is a γ-contraction.
(2)	As E → ∞, W c,ε(u, v) → MMD-kα(u, v), and thus Tn is a γα/2-contraction.
Proof is provided in Appendix J. Theorem 4 indicates that if we choose c as the unrectified kernel, the
limiting behaviors of distributional Bellman operator Tπ are both contractive under Sinkhorn loss.
Note that without the limitation, the entropic regularization restricts the search space on the optimal
7
Under review as a conference paper at ICLR 2022
transport that eventually corresponds to a Gibbs kernel, based on which Tπ may not be a contraction.
However, similar to MMD method, we show that our approach can still achieve empirical success
and is very competitive across a wide range of Atari games in Section 7.
In the algorithm design, similar to in MMD distri-
butional RL (Nguyen et al., 2020), we apply par-
ticle representation to represent Zq(s, a) by di-
rectly generating multiple deterministic samples.
Finally, we minimize the Sinkhorn loss between
the approximate distribution via multiple sam-
ples and its distributional Bellman target. A de-
tailed description of algorithm is provided in Al-
gorithm 1.
Remark. MMD enjoys a convergence rate of
O(n-1/* 1 2 3 4 5 6) 1 regardless of the underlying dimen-
sion while 1-Wasserstein distance has a conver-
gence rate of O(n-1/d) if d > 2, which is slower
for large d. Thus, Sinkhorn loss has the potential
to enjoy the faster convergence of MMD.
Algorithm 1 Generic Sinkhorn Algorithm
Require: Number of particles N, number of
Sinkhorn iteration L and hyperparameter ε.
Input: Sample transition (s, a, r', s0)
1: if Policy evaluation then
2:	a* 〜π(∙∣s0).
3: else
4:	a* 一 argmaXa'∈A N PL Zθ (s',a'')i
5: end if
6: TZi J r + γZθ- (/,a) ,∀1 ≤ i ≤ N
Output: Wfc2,ε ({Zθ(s, a)，}=[, {TZi}=J
7	Experiments
We demonstrate the effectiveness of Sinkhorn distributinal RL (SinkhornDRL) as described in Al-
gorithm 1 on a wide range of Atari 2600 games. Specifically, we leverage the same architecture
as QR-DQN (Dabney et al., 2018b) for a fair comparison. More advanced techniques that can ex-
pand the model expressiveness from IQN (Dabney et al., 2018a) and FQF (Yang et al., 2019) can be
naturally incorporated into our framework.
Baselines. Due to interpolation characteristic of SinkhornDRL, we choose 3 typical distributional
RL algorithms as classic baselines, including QR-DQN (Dabney et al., 2018b), C51 (Bellemare
et al., 2017a) and MMD (Nguyen et al., 2020), as well as DQN (Mnih et al., 2015). MMD algorithm
is implemented with the same architecture as QRDQN, and leverages Gaussian kernels kh(x, y)=
Breakout
Pon
Enφα
——DQN
——C51
——QRDQN
---MMD
---Sinkhom
Eaθ>αα>6ejα>>v
—DON
—C51
QRDQN
——MMD
--Sklktlonl
Enduro
0.0	02	0.4	0.6	0.8
Time Steps (1e7)
UJməX Θ6ajθ"
0.0	0.2	0.4	0.6	0.8
Time Steps (1e7)
En-ətt:ωCTSω><
0.0	0.2	0.4	0.6	0.8	1.0
Time Steps (1e7)
Berzerk
——DQN
——C51
QRDQN
——MMD
---Sinkhorn
UJnΦH ΦCTΦ^Φ><
DON
C51
QRDQN
MMD
Siikhom
OooM)O)000M)O)0000000
3530(2520(1510(
UJnφα ΦCT2Φ><
0.2	0.4	0.6	0.8
Time Steps (1e7)
Time Steps (1θ7)	Time Steps (1θ7)
Figure 2: Performance of SinkhornDRL on six Atari games.
8
Under review as a conference paper at ICLR 2022
exp(-(x - y)2/h) with the kernel mixture trick covering a range of bandwidths h, which is same
as the basic setting in the original MMD paper (Nguyen et al., 2020).
Hyperparameter settings. For a fair comparison with QR-DQN, C51 and MMD, we used the
same hyperparamters: the number of generated samples N = 200, Adam optimizer with lr =
0.00005, Adam = 0.01/32. We used a target network to compute the distributional Bellman target,
which fits well in the neural Z-fitted iteration framework. In addition, we choose number of Sinkhorn
iterations L = 10 and smoothing hyperparameter ε = 10.0 in Section 7.1 as they are not sensitive
within a proper interval demonstrated in Section 7.2. We choose α = 2 in kα .
7.1	Performance of SinkhornDRL
Figure 2 illustrates that SinkhornDR can achieve the state-of-the-art or competitive performance
on typical Atari games compared with various baseline algorithms (DQN, QR-DQN, C51, MMD)
in different metrics dp and representation manners on Zθ . Even though C51 outperforms others on
breakout game, itis significantly inferior on other games, e.g., Enduro, Pong and Berzerk. Moreover,
SinkhornDRL significantly outperforms MMD on breakout and SpaceInvader, and this superiority
can be owing to the theoretical advantage of Sinkhorn loss over MMD. This coincides with the
theoretical results as demonstrated in Theorem 4 that Sinkhorn loss interpolates between Wasserstein
distance and MMD, which can simultaneously make full use of the data geometry from Wasserstein
distance and the faster convergence, unbiased gradient estimates from Maximum Mean Discrepancy.
We provide the competitive performance of SinkhornDRL on other Atari games in Appendix L.
7.2	Sensitivity Analysis
We further examine the im-
pact of different hyperpa-
rameters in SinkhornDRL
on the final performance,
including the smoothing
hyperparameter ε in Eq. 12
the number of determin-
istic samples N in Algo-
rithm 1. From the left di-
agram in Figure 3, we can
observe that our algorithm
is not sensitive to the mag-
nitude of ε as long as ε is
within an appropriate inter-
val, e.g., [1, 100]. Mean-
(a) Hyper-parameter ε	(b) Number of Samples
Figure 3: Sensitivity analysis of SinkhornDRL on Breakout.
while, it turns out that an overly large number of samples N can even slightly worsen the per-
formance of SinkhornDRL.
8	Discussions and Conclusion
Implicit generative models can be further incorporated into SinkhornDRL, including parameterizing
the cost function in Sinkhorn loss, which can be naturally expected to achieve more promising
performance in the future. Moreover, a direct analysis on Wasserstein or `p distance can be more
effective than KL divergence presented in this paper, albeit being theoretically tricky.
In this paper, we illuminate the superiority of distributional RL over expectation-based RL from the
perspectives of regularization, optimization, acceleration and representation. In addition, a novel
family of distributional RL algorithms based on Sinkhorn loss is designed that accomplishes the
promising performance on Atari games. Our analysis paves the way towards deeper understanding
of distributional RL, and further promote its deployment in real applications.
Ethics Statement. Revealing the advantage of distributional RL would promote the application of
distributional RL algorithms in real scenarios. As distributional RL enjoys the robustness against
9
Under review as a conference paper at ICLR 2022
abnormal events, e.g., noisy state observations, it can also be beneficial for the privacy of algorithms.
Besides, the deeper insights into distributional RL plays a key role into the research integrity issue.
Based on our knowledge, it is not related to harmful applications or fairness issues.
Reproducibility Statement. For the theoretical part, we clearly state the related assumption and
detailed proof process in the appendix. In terms of the algorithm implementation, our Sinkhorn
algorithm is directly adapted from the public distributional RL algorithms, such as MMD.
References
Martin Arjovsky, Soumith Chintala, and Leon Bottou. Wasserstein generative adversarial networks.
In International conference on machine learning, pp. 214-223. PMLR, 2017.
Marc G Bellemare, Will Dabney, and Remi Munos. A distributional perspective on reinforcement
learning. International Conference on Machine Learning (ICML), 2017a.
Marc G Bellemare, Ivo Danihelka, Will Dabney, Shakir Mohamed, Balaji Lakshminarayanan,
Stephan Hoyer, and Remi Munos. The cramer distance as a solution to biased wasserstein gradi-
ents. arXiv preprint arXiv:1705.10743, 2017b.
Will Dabney, Georg Ostrovski, David Silver, and Remi Munos. Implicit quantile networks for
distributional reinforcement learning. International Conference on Machine Learning (ICML),
2018a.
Will Dabney, Mark Rowland, Marc G Bellemare, and Remi Munos. Distributional reinforcement
learning with quantile regression. Association for the Advancement of Artificial Intelligence
(AAAI), 2018b.
Odin Elie and Charpentier Arthur. Dynamic Programming in Distributional Reinforcement Learn-
ing. PhD thesis, Universite du Quebec a Montreal, 2020.
Jianqing Fan, Zhaoran Wang, Yuchen Xie, and Zhuoran Yang. A theoretical analysis of deep q-
learning. In Learning for Dynamics and Control, pp. 486-489. PMLR, 2020.
Aude Genevay, Gabriel Peyre, and Marco Cuturi. Learning generative models with sinkhorn di-
vergences. In International Conference on Artificial Intelligence and Statistics, pp. 1608-1617.
PMLR, 2018.
Tuomas Haarnoja, Haoran Tang, Pieter Abbeel, and Sergey Levine. Reinforcement learning with
deep energy-based policies. In International Conference on Machine Learning, pp. 1352-1361.
PMLR, 2017.
Tuomas Haarnoja, Aurick Zhou, Kristian Hartikainen, George Tucker, Sehoon Ha, Jie Tan, Vikash
Kumar, Henry Zhu, Abhishek Gupta, Pieter Abbeel, et al. Soft actor-critic algorithms and appli-
cations. arXiv preprint arXiv:1812.05905, 2018.
Moritz Hardt, Ben Recht, and Yoram Singer. Train faster, generalize better: Stability of stochastic
gradient descent. In International Conference on Machine Learning, pp. 1225-1234. PMLR,
2016.
Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. Distilling the knowledge in a neural network. arXiv
preprint arXiv:1503.02531, 2015.
Peter J Huber. Robust Statistics, volume 523. John Wiley & Sons, 2004.
Ehsan Imani and Martha White. Improving regression performance with distributional losses. In
International Conference on Machine Learning, pp. 2157-2166. PMLR, 2018.
Clare Lyle, Marc G Bellemare, and Pablo Samuel Castro. A comparative analysis of expected
and distributional reinforcement learning. In Proceedings of the AAAI Conference on Artificial
Intelligence, volume 33, pp. 4504-4511, 2019.
10
Under review as a conference paper at ICLR 2022
Clare Lyle, Mark Rowland, Georg Ostrovski, and Will Dabney. On the effect of auxiliary tasks on
representation dynamics. In International Conference on Artificial Intelligence and Statistics, pp.
1-9. PMLR, 2021.
Yecheng Jason Ma, Dinesh Jayaraman, and Osbert Bastani. Conservative offline distributional rein-
forcement learning. arXiv preprint arXiv:2107.06106, 2021.
John Martin, Michal Lyskawinski, Xiaohu Li, and Brendan Englot. Stochastically dominant distribu-
tional reinforcement learning. In International Conference on Machine Learning, pp. 6745-6754.
PMLR, 2020.
Borislav Mavrin, Shangtong Zhang, Hengshuai Yao, Linglong Kong, Kaiwen Wu, and Yaoliang
Yu. Distributional reinforcement learning for efficient exploration. International Conference on
Machine Learning (ICML), 2019.
Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Veness, Marc G Belle-
mare, Alex Graves, Martin Riedmiller, Andreas K Fidjeland, Georg Ostrovski, et al. Human-level
control through deep reinforcement learning. nature, 518(7540):529-533, 2015.
Tetsuro Morimura, Masashi Sugiyama, Hisashi Kashima, Hirotaka Hachiya, and Toshiyuki Tanaka.
Parametric return density estimation for reinforcement learning. arXiv preprint arXiv:1203.3497,
2012.
Rafael Muller, Simon Kornblith, and Geoffrey Hinton. When does label smoothing help? arXiv
preprint arXiv:1906.02629, 2019.
Thanh Tang Nguyen, Sunil Gupta, and Svetha Venkatesh. Distributional reinforcement learning with
maximum mean discrepancy. Association for the Advancement of Artificial Intelligence (AAAI),
2020.
Mark Rowland, Marc Bellemare, Will Dabney, Remi Munos, and Yee Whye Teh. An analysis
of categorical distributional reinforcement learning. In International Conference on Artificial
Intelligence and Statistics, pp. 29-37. PMLR, 2018.
Mark Rowland, Robert Dadashi, Saurabh Kumar, Remi Munos, Marc G Bellemare, and Will Dab-
ney. Statistics and samples in distributional reinforcement learning. International Conference on
Machine Learning (ICML), 2019.
John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy
optimization algorithms. arXiv preprint arXiv:1707.06347, 2017.
Richard Sinkhorn. Diagonal equivalence to matrices with prescribed row and column sums. The
American Mathematical Monthly, 74(4):402-405, 1967.
Ke Sun, Yi Liu, Yingnan Zhao, Hengshuai Yao, Shangling Jui, and Linglong Kong. Exploring
the robustness of distributional reinforcement learning against noisy state observations. arXiv
preprint arXiv:2109.08776, 2021.
Richard S Sutton and Andrew G Barto. Reinforcement learning: An Introduction. MIT press, 2018.
Gabor J Szekely. E-StatiStics: The energy of statistical samples. Bowling Green State University,
Department of Mathematics and Statistics Technical Report, 3(05):1-18, 2003.
Christopher JCH Watkins and Peter Dayan. Q-learning. Machine learning, 8(3-4):279-292, 1992.
Ronald J Williams and Jing Peng. Function optimization using connectionist reinforcement learning
algorithms. Connection Science, 3(3):241-268, 1991.
Eric Wong, Frank Schmidt, and Zico Kolter. Wasserstein adversarial examples via projected
sinkhorn iterations. In International Conference on Machine Learning, pp. 6808-6817. PMLR,
2019.
Yi Xu, Yuanhong Xu, Qi Qian, Hao Li, and Rong Jin. Towards understanding label smoothing.
arXiv preprint arXiv:2006.11653, 2020.
11
Under review as a conference paper at ICLR 2022
Derek Yang, Li Zhao, Zichuan Lin, Tao Qin, Jiang Bian, and Tie-Yan Liu. Fully parameterized quan-
tile function for distributional reinforcement learning. Advances in neural information processing
systems, 32:6193-6202, 2019.
Fan Zhou, Jianing Wang, and Xingdong Feng. Non-crossing quantile regression for distributional
reinforcement learning. Advances in Neural Information Processing Systems, 33, 2020.
Florian Ziel. The energy distance for ensemble and scenario reduction. arXiv preprint
arXiv:2005.14670, 2020.
12
Under review as a conference paper at ICLR 2022
A Definition of distances and Contraction
Definition of distances. Given two random variables X and Y , p-Wasserstein metric Wp between
the distributions of X and Y is defined as
Wp(X,Y )= Uo 1∣Fχ1(ω) - F-1(ω)∣p dω) " = ∣∣Fχ1 - F-1kp,	(14)
which F-1 is the inverse cumulative distribution function of a random variable with the cumulative
distribution function as F . Further, `p distance (Elie & Arthur, 2020) is defined as
'p(X,Y)：= IZ∞ ∣Fχ(ω)- Fγ(ω)∣p dω) " = ∣∣Fχ - Fγkp	(15)
The `p distance and Wassertein metric are identical at p = 1, but are otherwise distinct. Note that
when P = 2, 'p distance is also called Cramer distance (Bellemare et al., 2017b) de(X, Y). Also,
the Cramer distance has a different representation given by
de(X, Y) = EX - Y| - 2EX - XI- 2E |Y - YI,	(16)
where X0 and Y0 are the i.i.d. copies of X and Y. Energy distance (Szekely, 2003; Ziel, 2020) is a
natural extension of Cramer distance to the multivariate case, which is defined as
dE(X, Y) = EkX - Yk2 - 2EkX - X0k- 2EkY - Y0k,	(17)
where X and Y are multivariate. Moreover, the energy distance is a special case of the maximum
mean discrepancy (MMD), which is formulated as
MMD(X, Y; k) = (E [k (X, X0)] +E[k(Y,Y0)] - 2E[k(X, Y)])1/2	(18)
where k(∙, ∙) is a continuous kernel on X. In particular, if k is a trivial kernel, MMD degener-
ates to energy distance. Additionally, we further define the supreme MMD, which is a functional
P(X)S×A × P(X)S×A → R defined as
MMD∞(μ,ν) =	sup	MMD∞(μ(x, a), ν(x,a))
(x,a)∈S×A
(19)
Proof of Contraction.
•	Contraction under supreme form of Wasserstein diatance is provided in Lemma 3 (Belle-
mare et al., 2017a).
•	Contraction under supreme form of `p distance can refer to Theorem 3.4 (Elie & Arthur,
2020).
•	Contraction under MMD∞ is provided in Lemma 6 (Nguyen et al., 2020).
B Proof of Proposition 1
Proof. (1) We recap three crucial properties of a divergence metric. The first is scale sensitive (S)
(of order β, beta > 0), i.e., dp(cX, CY) ≤ ∣c∣βdp(X, Y). The second property is shift invariant (I),
i.e., dp(A + X, A + Y) ≤ dp(X, Y). The last one is unbiased gradient (U). We use p and q to
denote the density function of two random variables X and Y, and thus DKL (X, Y) is defined as
Dkl (X, Y) = R∞∞ p(x) P(X) dx. Firstly, We show that Dkl (X, Y) is NOT scale sensitive:
DKL(CX,cY )= Z ∞ 1 p(X)咨¥ dx
J-∞ a a aq(X)
∞ , My).	(20)
=LP⑹丽dy
= DKL(X, Y), with β = 0
13
Under review as a conference paper at ICLR 2022
We further show that DKL(X, Y ) is shift invariant:
DKL(A + X,A + Y) = Γ p(x - A)P(X - A) dx
-∞	q(x -A)
∞ P、p(y：	(21)
=LP⑹丽dy
= DKL(X, Y)
Moreover, it is well-known that KL divergence has unbiased sample gradients (Bellemare et al.,
2017b). The supreme DKL is a functional P(X)S×A × P(X)S×A → R defined as
DKL (μ, V) = sup	Dkl (μ(x, a), V(x, a))	(99)
(x,a)∈S×A	()
Therefore, we prove Tπ is at best a non-expansive operator under the supreme form of DKL:
DK∞L(TπZ1,TπZ2)
= supDKL(TπZ1(s, a),TπZ2(s, a))
s,a
= DKL(R(s, a) + γZ1(s0, a0), R(s, a) + γZ2(s0, a0))
= DKL(R(s, a) + Z1(s0, a0), Z2(s0, a0))
≤ supDKL(R(s, a) + Z1(s0,a0), Z2(s0,a0))
s0,a0
= DK∞L(Z1,Z2)
(23)
There we have DK∞L(TπZ1, TπZ2) ≤ DK∞L(Z1, Z2), implying that Tπ is a non-expansive operator
under DK∞L.
(2)	By the definition of DK∞L, we have sups,a DKL (Zn (s, a), Z(s, a)) → 0 implies DKL (Zn, Z) →
0. DKL(Zn, Z) → 0 implies the total variation distance δ(Zn, Z) → 0 according to a straightfor-
ward application of Pinsker’s inequality
δ (Zn, Z) ≤
22DKL (Zn, Z) → 0
δ (Z, Zn ) ≤
j2DKL (Z, Zn) → 0
(24)
Based on Theorem 2 in WGAN (Arjovsky et al., 2017), δ(Zn, Z)→ 0 implies Wp(Zn, Z)→ 0.
This is trivial by recalling the fact that δ and W give the strong an weak topologies on the dual of
(C(X), k ∙ ∣∣∞) when restricted to Prob(X).
(3)	The conclusion holds because the Tπ degenerates to Tπ regardless of the metric dp. Specifically,
due to the linearity of expectation, we obtain that
∣ETπZ1 -ETπZ2∣∞ = ∣TπEZ1 -TπEZ2∣∞ ≤ γ∣EZ1 -EZ2∣∞.	(25)
This implies that the expectation of Z under DKL exponentially converges to the expectation of Z*,
i.e., γ-contraction.	□
C Proof of Proposition 2
Proof. Firstly, given a fixed P(x) we know that minimizing DKL (P, qθ) is equivalent to minimizing
H(P, q) by following
n (	、	∕+∞，、1	P(X) X
Dkl (P, qθ) = J	P(X) log q^χy dx
= -	P(X) logqθ(X) dX - (-	P(X) log P(X) dX)
-∞	-∞
= H(P, qθ) - H(P)
H H(p, qθ)
(26)
14
Under review as a conference paper at ICLR 2022
Based on H(p, qθ), we can derive the objective function within each neural Z-fitted iteration as
1n
n EH(Psi，ai(x),qSi,ai(x))
,ai)
=1
1n
(X n EH(δ{χ=E[Z"(si,ai)]},qsi,ai) + αH(μsi,ai, qθi,ai), where α = ɪ-^ > 0
(27)
i=1
1X
+∞
l	log qSi,ai(E [Zπ(s, a)]) dx + αH(μsi,ai, qθi,ai)
-∞
□
D Proof of Theorem 1
Proof. Firstly, we have:
V (St+1)= Eat+1 〜π,x〜μst+1 ,at+1 [Q (St+1, at+1) + log qθ +，+ (X)]
=Eat+1 〜∏ [Q (st+1, at+ι)] + Eat+1 〜Π,x〜μst+ι,at+ι [log qθt+1,at+1 (x)]
=Eat+1 〜∏ [Q (st+ι,at+ι)] - Eat+1 〜∏ ]-/ μst+1 ,at+1 (x) log qθt+1,at+1 (x) dx
=Eat+1 〜∏ [Q (st+1, at+l)] - Eat+1 〜∏ [H (μst+1,at+1, qSt+1,at+1)]
(28)
Further, we plug in V(St+1) into RHS of the iteration in Eq. 7, then we obtain
TdπQ(St,at)
=Ir(St, at) + γEst+1 〜ρ∏ [V (st+ι)]
=r(st,at) - YE(st+1,at+1)〜Pn [H(μstiat+1, qθt+1,at+1 )] + YE(st+1,at+1)〜Pn [Q (st+1,at+1)]
,Tn(St, at) + YE(st+1,at+1)〜Pn [Q (st+1, at+1)],
s a	(29)
where r∏ (st, at)，r (st, at) - γE(st+1,at+1)〜ρ∏ [H (μst+1,at+1 ,qst+1,at+1)] is the entropy aug-
mented reward we redefine. Thus, repeatedly updating via the above equations is equivalent to
maximizing the new objective function J0(θ) = PT=0 E(st,at)〜Pn [r (st, at) 一 YH (μst,at, qθt,at)].
Applying the standard convergence results for policy evaluation (Sutton & Barto, 2018), we can
attain that this Bellman updating under Tdπ is convergent under the assumption of |A| < ∞ and
bounded entropy augmented rewards r∏.	□
E Derivation of Histogram Distributional Loss
We show the derivation details of the Histogram distribution loss starting from KL divergence be-
tween p and qθ . pi is the cumulative probability increment of target distribution {Yi }i∈[n] within the
i-th bin, and qθ corresponds to a (normalized) histogram, and has density values
f (X(S)) per bin.
wi
15
Under review as a conference paper at ICLR 2022
Thus, we have:
DKL
(ps,a, qθs,a) = - Z b
a
ps,a(y) log qθs,a(y)dy
k	li+wi
-Xi=1Zli
ps,a(y) log
f≡ dy
=-XXlog fθ(X(S)) (Fs,a (li + Wi)- Fs,a (li))
i=1	Wi '--------------{----------}
pi
k
(X - XPsCabgfiθ(X(S))
i=1
(30)
where the last equality holds because the width parameter Wi can be ignored for this minimization
problem.
F Proof of Lemma 1
Proof. For the histogram distributional loss below,
k
Lθ(S, a) = -X pis,alogfiθ(X(S)), where fiθ(X(S))
i=1
exp (X(S)>θi)
Pj=I eχp(χ(s)>θj)
We firstly prove its convexity. Note that - log P)e^p(^Psx(Sθi>%)= log Pk=I exp (x(s)>θj)一
X(S)>θi, the first term is Log-sum-exp, which is convex (see Convex optimization by Boyd and
Vandenberghe), and the second term is affine function. Thus, Lθ(S, a) is convex.
Secondly, We shoW that Lθ(S, a) is kl-Lipschitz. We compute the gradient of the Histogram distri-
butional loss regarding θi :
k
演 XPsabg fj(X(Sy)
i j=1
k1
=X pj,a iVf'(X(S))
k1
=X Pj Jl(X^ fθ (X(S))(% — fj (x(s)))x(s)
=卜,a(1 - fl(X(S)))- XPj,afiθ(X(S))X(S)
=(PF - Ps,afθ (x(s)) - (1 - Ps,a)fθ (x(s))) x(s)
=(Ps,a - fiθ(X(S))) X(S)
(31)
16
Under review as a conference paper at ICLR 2022
where δij = 1 ifi = j, otherwise 0. Then, as we have kx(s)k ≤ l, we bound the norm of its gradient
k
k ∂θ X Pjlog fθ(X(S)) k
∂θ j=1
kk
≤ X k 须 X Pj log fj(X(S)) k
i=1	i j=1
k	(32)
=Ek(Ps,a- fθ(X(S))) x(s)k
i=1
k
≤ X ∣Ps,a - fiθ(X(S))lkx(s)k
i=1
≤ kl
The last equality satisfies because |Pi - fiθ (X(S))| is less than 1 and even smaller. Therefore, we
obtain that Lθ is kl-Lipschitz.
Lastly, We ShoW that Lq is kl2-Lipschitz smooth. A lemma is that log(1 + exp(x)) is 1 -smooth
as its second-order gradient is bounded by 1,, and if g(w) is β-smooth w.r.t. w, then g({χ,w>) is
βkxk2-smooth. Based on this knoWledge, We firstly focus on the 1-dimensional case of function
log fj (z). As we have derived, we know that ∂θ~ log fj(Z) = δj - fθ(z). Then the second-
order gradient is d ∂jgf]Z) = -(δik - fθ(Z)) = fθ(z) - 1 if i = k, otherwise fθ(z). Clearly,
|d ∂θgf]Z) | ≤ 1, which implies that log fjj(z) is 1-smooth. Thus, log fjθ(<x,θi>) is kxk2-smooth,
or l2 -smooth. Further, Pjk=1 Pjs,a log fjθ(X(S)) is also l2 -smooth as we have
kk
kv% XPjqog fj(μ) - V% XPjqog fj(ν)k
j=1	j=1
k
≤ XPj,akVθi log fjθ(μ) -Vθilog fjθ(ν)k
j=1
k
≤ XPj,a∙l2kμ - νk
j=1
=l2kμ -Vk
for each μ and V. Therefore, we further have
kk
kVθ XPj,a logfθ(μ) - Vq XPj,a log fjj(ν)k
j=1	j=1
kk	k
≤ X kVθi XPj,a logfjθ(μ) - V呢 XPj,a logfjθ(ν)k
k
≤ X l2 kμ - Vk
i=1
=kl2 kμ -Vk
(33)
(34)
Finally, we conclude that Lj (S, a) is kl2 -smooth.
□
17
Under review as a conference paper at ICLR 2022
G Proof of Theorem 2
Proof. Consider the stochastic gradient descent rule as Gλ,L (θ) = θ-λVθL(θ). Firstly, we provide
two definitions about Lθ for the following proof.
Definition 3. (σ-bounded) An update rule is σ-bounded if supθ kθ - λVθ L(θ)k ≤ σ.
Definition 4. (η-expansive)An update rule is η-expansive if supv,w kGλ,L(Vu-WjL(w)k ≤ η.
Lemma 3. (Grow Recursion, Lemma 2.5 (Hardt et al., 2016)) Fix an arbitrary sequence of updates
G1 , ..., GT and another sequence G01 , ..., G0T. Let θ0 = θ00 be the starting point and define δt =
kθi0 - θtk, where θt and θt0 are defined recursively through
θt+1 =Gλ,L(θt), θt0+1 =G0λ,L(θt0)
Then we have the recurrence relation:
ηδt	Gt = G0t is η-expansive
min(η, 1)δt + 2σt Gt and G0t are σ-bounded , Gt is η expansive
Lemma 4. (Lipschitz Continuity) Assume Lθ is L-Lipschitz, the gradient update Gλ,L is (λL)-
bounded.
δt+1 ≤
Proof kθ - Gλ,L(θ)k = ∣∣λVθL(θ)∣∣ ≤ λL	□
Lemma 5. (Lipschitz Smoothness) Assume L§ is β-smooth, thenfor any λ ≤ β, the gradient update
Gλ,L is 1-expansive.
Proof. Please refer to Lemma 3.7 in (Hardt et al., 2016) for the proof.
□
Based on all the results above, we start to prove Theorem 2. Our proof is largely based on (Hardt
et al., 2016), but it is applicable in distributional RL setting as well as considering desirable proper-
ties of histogram distributional loss. According Lemma 1, we attain that Lθ is kl-Lipschitz as well
as kl2-smooth, and thus based on Lemma 4, Gλ,L is (λkl)-bounded, and 1-expansive if λ ≤ 系.
In the step t, SGD selects samples that are both in S and S0, with probability 1 一 *. In this case,
Gt = G0t, and thus δt+1 ≤ δt as Gt is 1-expansive based on Lemma 3. The other case is that sam-
ples selected are different with probability n, where δt+1 ≤ δt + 2λtkl based on Lemma 3. Thus, if
λt ≤ k22 we have:
E∣L(θτ; x) -L(θT; x)| ≤ klE[δτ], where δτ = ∣∣θτ 一 θ%∣∣
≤ kl f(1 一 1)E [δτ-ι] + 1E [δτ-ι] + 2λTTkl
nn	n
2λT -1kl
kl I E [δτ-1] +---------
n
T-1
E[δ0]+X
t=0
< 2k2l2 TX1 2
≤ n	k2 kl2
t=0
2λtkl
n
(35)
4kT
n
Since this bounds hold for all S, S0 and x, we attain the uniform stability in Definition 1 for our
histogram distributional loss applied in distributional RL.
Define the population risk as:
R[θ] = ExL(θ; x)
18
Under review as a conference paper at ICLR 2022
and the empirical risk as:
1n
Rs [θ] = n y^L(θ; Xi)
According to Theorem 2.2 in (Hardt et al., 2016), if an algorithm M is stab-uniformly stable, then
the generalization gap is stab-bounded, i.e.,
|ES,A[RS[M(S)] -R[M(S)]]| ≤ stab
□
H Proof of Lemma 2
E(s,a)〜ρ∏ [kVLθ(ps,a, qS,a)) - VG(θ)k2] ≤ (1 - e)σ2 + eκσ2.	(36)
Proof. As we know thatps,a(x) = (1 一 e)δ{χ=E[z∏(s,a)]}(x) + eμs,a(x), then we have:
VLθ(ps,a, qθ,a) = (1 一 e)VLθ(δ{χ=E[z∏(s,a)]}, qS,a) + 久Lθ(μs,a, qθ,a)
Therefore,
E(s,a)〜ρ∏ [kVLθ(ps,a,qθ,a)) -VG(θ)k2]
≤ E(s,a)〜ρπ [(I-E)IIvlΘ(δ{x=E[Zπ(s,a)]}, qS，a)) - VG⑹k2 + EkVL6(μs,a, qθ,a)) - VG⑹k2]
= (1 - )σ + κσ ,
(37)
where the first inequality uses the triangle inequality of norm, i.e., k(1 - E)a + Ebk2 ≤ (1 -
E)kak2 + Ekbk2, and the last equality uses the definition of the variance of Lθ(δ{x=E[Zπ(s,a)]}, qθs,a)
and Lθ (μs,a,qθ,a).	'	□
I Proof of Theorem 3
Proof. (1) If we only consider the expectation of Zπ(s, a), the entropy-regularized MLE would
degenerate to the pure MLE regarding δ{x=E[Zπ (s,a)]}. As Lθ(δ{x=E[Zπ(s,a)]}, qθs,a) is kl2-smooth,
we have
G(θt+1) - G(θt)
kl2
≤ hvG(θt),θt+ι - θti + Fkθt+ι 一 θtk	(38)
kl2 λ2
=-λ ^vG(θt), vLθ(δ{x=E[Zπ(s,a)]}, qθ )) +------2— kvLθ(δ{x=E[Zπ(s,a)]},qθ, )k
where the last first equation is according to the definition of Lipschitz-smoothness, and the last
second one is based on the updating rule of θ. Next, we take the expectation on both sides,
E [G(θt+1) - G(θt)]
≤ -λE [kVG(θt)k2] +
≤ -λE [kVG(θt)k2] +
kl2λ2
-2-E [kVLθ(δ{χ=E[z∏(s,a)]}, qθ,a) - VG(θt) + VG(θt)k2]
kl2λ2	kl2 λ2
FE [kVLθ(δ{χ=E[z∏(s,a)]}, qθ,a) - VG(θt)k2] + FE [kVG(θt)k2]
=λ(kl2λ - 2)E [kVG(θt)k2] + 岑σ2
λ	kl2λ2
≤-2E [kVG(θt)k2] + 竽σ2
(39)
where the first two equation hold because VG(θ) = E [VLθ] and the last inequality comes from
λ ≤ kl2. Through the summation, We obtain that
λ T-1	kl2λ2T
E [G(θτ) - G(θo)] ≤-2 £ E [kVG(θt)k2] + ——σ2
2 t=0	2
19
Under review as a conference paper at ICLR 2022
We let E [G(θT )] = 0, we have
1 T-1
T EE [kVG(θt)k2] ≤
T t=0
竽 + kl"
By setting λ ≤ 2⅛σ and T = 4G^0)，We can have T PT-OIE [∣∣VG(θt)k2] ≤ τ2, implying that
the degenerated MLE can achieve T-station point if the sample complexity T = O(}).
(2) and (3) We are still based on the kl2-smoothness of L(ps,a, qθs,a).
G(θt+1) - G(θt)
kl2
≤ hVG(θt), θt+ι - θti + ɪ∣∣θt+ι - θtk2
kl2λ2
=-λ hVG(θt), VLθ (psa,qssa) + -2r kVLθ (psa,qssaW	(40)
=-2 kVG(θt)k2 + 2 kVG(θt) - VLθ (ps,a,qθ,a)k2 + MklU - 1) kVLθ (psa,qseaW
≤ -2kVG(θt)k2 + 2kVG(θt) - VLθ(ps,α,qθ,α)k2
where the second equation is based on〈a, -b)= 2 (∣∣a — b∣∣2 — ∣∣ak2 一 ∣∣b∣∣2), and the last in-
equality is according to λ ≤ 系.After taking the expectation, we have
E [G(θt+1) - G(θt)]
≤ -2E [∣VG(θt)k2] + 2E [∣∣VG(θt) - VLθ(ps,α, qθ,a)k2]
≤ -2E [kVG(Ot)k2] + 2 ((I-E)σ2 + eκσ2)
where the last inequality is based on Lemma 2. We take the summation, and therefore,
λ T-1	Tλ
E [G(θT) - G(θO)] ≤ -2 ^X E [k VG(Ot)k2] + ɪ ((I-E)σ2 + eκσ2
2 t=O	2
We let E [G(θτ)] = 0 and E = 1+^, then,
1	T-1
T ∑E [∣VG(θt)k2]
T t=O
≤ 竽+ (1-E)σ2 + Eκσ2
λT
_ 2G(θo)	2κ 2
=λT + 1 + K σ
≤ 2G≡+2κσ2
≤ λT +
(41)
(42)
If κ ≤ 422 and let T = 4GTθO), this leads to T PT-OIE [∣VG(θt)∣2] ≤ T2, i.e., τ-stationary point,
with the sample complexity as O(^2). Thus, (2) has been proved. On the other hand, if κ > 占, We
set T = GKσ). This implies that T PT-OI E [∣VG(θt)∣2] ≤ 4κσ2 = O(κ). Therefore, the degree
of stationary point is determined the degree of distribution approximation measured by κ. Thus, we
obtain (3).	□
J	Proof of Theorem 4
Proof. 1. As ε → 0, it is obvious to observe that Sinkhorn loss degenerates to the wasserstein
distance.
20
Under review as a conference paper at ICLR 2022
2. As ε → ∞,
Sinkhorn loss turns to be log
du∏XXdy(y)) d∏(x,y). We Can consider its dual
problem by introducing Lagrange multipliers u and v. By additionally considering the primal-dual
relation, we can solve the dual problem, which gives u = v = 0, and thus the optimal coupling
is simply the product of the marginals , ie., Π = U 0 v. Please refer to more detailed proof in
Sinkhorn-GAN (Genevay et al., 2018). Since the cost function is the rectified kernel kα, under with
MMD (Nguyen et al., 2020) can be γα/2-contractive.	□
K Representation Effect of Distribution RL
From the perspective of representation, we find that distributional RL encourages state representa-
tion from the same action class classified by the policy in tighter clusters.
(a) DQN	(b) C51	(c) QRDQN
Figure 4: Visualization on the penultimate layer of the value
network on Breakout. Each color denotes one action class.
The intrinsic characteristic of distri-
butional RL is that it enables to learn
a richer and more faithful represen-
tation of the environment, leading to
more stable and efficient learning. To
investigate the more informative rep-
resentation resulting from distribu-
tional RL, we visualize how distribu-
tional RL changes the representation
learned in the penultimate layer of the
value network. We collect state fea-
tures in the penultimate layer classified by the policy π in different action classes, and perform t-SNE
to reduce them to the two-dimensional space. In Figure 4, points in different colors represent state
features classified into different action classes. It illustrates that both QR-DQN and C51 encourage
the representation of state observations from the same action class to group in tighter clusters rela-
tive to expectation-based DQN. Interestingly, this phenomenon is similar to label smoothing (Muller
et al., 2019), which leverages additional distributional knowledge in soft labels relative to hard la-
bels. Therefore, we conclude that similar to the benefit of label smoothing, distributional RL also
encourages the activations of the penultimate layer to be close to the template of the correct action
class and distant to the templates of the incorrect action classes.
L More experimental Results
21
Under review as a conference paper at ICLR 2022
Alien
Emə比 Θ6alθ><
20000
15000
10000
5000
——DQN
——C51
QRDQN
MMD
Sinkhorn
0
0.0	0.2	0.4	0.6	0.8
Oooooooo
05050505
07520752
lɪ lɪ lɪ
En-a>B ΦOT≡Φ><
0.0	0.2	0.4	0.6	0.8	1.0	1.2
Time Steps (1e7)
——DQN
——C51
QRDQN
---MMD
---Sinkhom
Time Steps (1e7)
LUn-a>α ① 6bj ① ><
16000
14000
12000
10000
8000
6000
4000
2000
0
DQN
C51
QRDQN
MMD
Sinkhom
Qbert
Emətt:Θ6RJΘ><
20000
15000
10000
5000
25000
o
0.0	0.2	0.4	0.6	0.8
Time Steps (1e7)
0.0	0.2	0.4	0.6	0.8	1.0
Time Steps (1e7)
Figure 5: Performance of SinkhornDRL on YarRevenge, Aline, Qbert and UpNDown.
22