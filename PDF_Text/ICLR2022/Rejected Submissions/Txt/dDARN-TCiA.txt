Under review as a conference paper at ICLR 2022
Stochastic Reweighted Gradient Descent
Anonymous authors
Paper under double-blind review
Ab stract
Importance sampling is a promising strategy for improving the convergence rate of
stochastic gradient methods. It is typically used to precondition the optimization
problem, but it can also be used to reduce the variance of the gradient estima-
tor. Unfortunately, this latter point of view has yet to lead to practical methods
that provably improve the asymptotic error of stochastic gradient methods. In
this work, we propose stochastic reweighted gradient (SRG), a variance-reduced
stochastic gradient method based solely on importance sampling that can improve
on the asymptotic error of stochastic gradient descent (SGD) in the strongly con-
vex and smooth case. We show that SRG can be extended to combine the ben-
efits of both importance-sampling-based preconditioning and variance reduction.
When compared to SGD, the resulting algorithm can simultaneously reduce the
condition number and the asymptotic error, both by up to a factor equal to the
number of component functions. We demonstrate improved convergence in prac-
tice on '2-regularized logistic regression problems.
1	Introduction
Unconstrained optimization of finite-sum objectives is a core algorithmic problem in machine learn-
ing. The prototypical way of solving such problems is by viewing them through the lens of stochastic
optimization, where the source of stochasticity resides in the choice of the index in the sum. Stochas-
tic gradient descent (SGD) (Robbins & Monro, 1951) remains the standard algorithm for this class
of problems.
A natural way to improve on SGD is by considering importance sampling schemes. This idea is not
new and dates back to (Needell et al., 2014) who uses importance sampling as a preconditioning
technique. They propose sampling the indices with probabilities proportional to the smoothness
constants of the corresponding component functions, and show that this sampling scheme provably
reduces the condition number of the problem.
In another line of work, variance-reduced methods were found to achieve linear convergence in
the strongly convex and smooth case (Roux et al., 2012; Schmidt et al., 2017). Many of these
methods rely on control variates to reduce the variance of the gradient estimator used by SGD
(Johnson & Zhang, 2013; Defazio et al., 2014). While very successful, the applicability of these
methods is limited by the large memory overhead that they introduce, or the periodic full-gradient
recomputation that they require (Defazio & Bottou, 2019). Despite strong progress in this research
area, an importance-sampling-based analogue to these algorithms, which is potentially free from
these drawbacks, has yet to emerge.
In this work, we propose such an analogue. We introduce stochastic reweighted gradient (SRG),
an importance-sampling-based variance-reduced optimization algorithm. Similar to SGD, SRG re-
quires a single gradient oracle call per iteration, and only requires O(n) additional memory, and
O(log n) additional floating point operations per iteration. We analyze the convergence rate of SRG
in the strongly-convex and smooth case, and show that it can provably improve the asymptotic error
of SGD. Finally, we show how our importance sampling strategy can be combined with smoothness-
based importance sampling, and prove that the resulting algorithm simultaneously performs variance
reduction and preconditioning. We demonstrate improved convergence in practice on '2-regularized
logistic regression problems.
1
Under review as a conference paper at ICLR 2022
2	Preliminaries
We consider the finite-sum optimization problem:
mRd (F (X) = 1 XX fi(x))
(1)
where F is μ-strongly convex for μ > 0, and for all i ∈ [n], fi is convex and Li-smooth for
Li > 0. Note that by strong-convexity, F has a unique minimizer x* ∈ Rd. We define the maximum
LmaX ：= maxi∈[n] Li and average L := P；=i Li/n smoothness constants. Similarly, We define the
maximum Kmax ：= Lmax/μ and average K := L∕μ condition numbers.
The classical way of solving (1) is by viewing it as a stochastic optimization problem where the
randomness comes from the choice of the index i ∈ [n]. Starting from some x0 ∈ Rd, and for an
iteration number k ∈ N, stochastic gradient descent (SGD) performs the following update:
Xk+1 = Xk - αk Vfik (Xk)
for a step size αk > 0 and a random index ik drawn uniformly from [n].
The idea behind importance sampling for SGD is to instead sample the index ik according to a
chosen distribution pk on [n], and to perform the update (Needell et al., 2014):
xk+1 = Xk - αk i~ Vfik (Xk )	(2)
npkk
where pik is the ith component of the probability vector pk. It is immediate to verify that the impor-
tance sampling estimator of the gradient is unbiased as long as pk > 0.
The question that we address in this paper is how to design a sequence {pk}k∞=0 that produces more
efficient gradient estimators than the ones produced by uniform sampling. One way to design such
a sequence is by adopting a greedy strategy: at each iteration k we choose pk to minimize the
conditional variance of the gradient estimator, which is given by, up to an additive constant:
1	2	1 n 1
,(Xk ,p) := E	--~k Vfik (Xk)	(it)t=0 = n2	~7 kVfi(Xk )k2，	(3)
ik~p	nP	2	i	i== i=1 P
This conditional variance is minimized at (Zhao & Zhang, 2015):
arg min σ2 (Xk , P)
p∈∆
(kVfi(Xk)∣∣2
(P；=IkVfj (Xk)Il2
(4)
Ideally, we would like to set Pk to this minimizer. However, this requires knowledge of the gradient
norms (kVfi(Xk)k2)in=1, which, in general, requires n gradient evaluations per iteration.
3	Algorithm
In this section, we show how to design a tractable sequence of importance sampling distributions
for SGD that approximate the conditional-variance-minimizing distributions (4). First we construct
efficient approximations of the conditional variances (3). We then state a simple bound on the
approximation errors and use it to motivate our choice of importance sampling distributions.
To approximate the conditional variances, we follow the strategy of certain variance-reduced meth-
ods (Roux et al., 2012; Schmidt et al., 2017; Defazio et al., 2014). These methods maintain a table
(gki )in=1 that tracks the component gradients (Vfi (Xk))in=1 and updates itself each iteration at the
index ik used to update the iterates Xk (2). Our method instead maintains an array of gradient norms,
from which we construct an approximation of the conditional variance (3) of the gradient estimator:
"k,p) := -2 X 1λ l∣gk∣∣2
n i=1 P
2
Under review as a conference paper at ICLR 2022
Algorithm 1 SRG
1:	Parameters: step sizes (αk)k∞=0 > 0, mixture coefficients (θk)k∞=0 ∈ (0, 1]
2:	Initialization:x0 ∈Rd,(g0ik=20)in=1 ∈Rn	k=0
3:	for k = 0, 1, 2, . . . do
4:	pk = (1 - θk)qk + θk/n	{qk is defined in (5)}
5:	bk 〜BemoUlli(θk)
6:	if bk = 1 then ik 〜1/n else ik 〜qk
7：	xk+1 = Xk - αk 'ik Vfik (Xk )
∣Vfi(xk)∣∣2 if bk = 1 and ik = i
gki 2 otherwise
9: end for
8:	gki +1 2 =
which is minimized at:
qk = arg min σ2(xk,p)
p∈∆
(5)
We do not directly Use qk as an importance sampling distribUtion, becaUse this approximation may
be poor. In particUlar, we have the following boUnd on the conditional variance (3):
2n1	2
σ (xk ,P) ≤ —TIVfi(Xk) - gk∣∣2 +2σ 2(χk,p)	⑹
n i=1 p
Recall that oUr goal is to pick pk that minimizes σ2 * 4 (Xk, p). qk minimizes the second term on the
right-hand side, bUt we mUst ensUre that both terms are small. Two conditions are needed to keep the
first term small. The first is to control the terms ∣Vfi(Xk) - gki ∣22, which we can do by making sUre
that the historical gradients gki are freqUently Updated. The second is to ensUre that the probabilities
pik are lower boUnded. We achieve both of these properties, as well as approximately minimize the
right-hand side of (6) by mixing qk with the Uniform distribUtion over [n]. This yields oUr final
importance sampling distribUtion for a given mixtUre coefficient θk ∈ (0, 1]:
pk = (1 - θk)qk + T	(7)
OUr analysis clarifies the role of the seqUence of mixtUre coefficients (θk)k∞=0, and relates it to both
the step size seqUence (αk)k∞=0 and the asymptotic error of SRG in the constant step size setting.
CUrioUsly, oUr analysis reqUires performing the Update of the array (∣gki ∣2)in=1 only when the index
ik is drawn from the Uniform mixtUre component. It is not clear to Us whether this constraint is an
artifact of the analysis or a property of the algorithm. We discUss this fUrther after the statement of
Lemma 1 in section 4. The pseUdocode for oUr method is given in Algorithm 1.
We briefly discUss how SRG can be efficiently implemented. To sample from qk, we store
(∣gki ∣2 )in=1 in a binary indexed tree Using O(n) memory, along with the normalizing constant
λk = Pin=1 ∣gki ∣2. The binary indexed tree can be Updated and maintained in O(logn) opera-
tions, while the normalizing constant can be maintained in constant time. We can then sample from
qk Using inverse transform sampling: we mUltiply a Uniform random variable u ∈ [0, 1) by λk,
then retUrn the largest index whose corresponding prefix sUm is less than λku. This procedUre also
reqUires only O(log n) operations. The total overhead of SRG when compared to SGD is therefore
O(n) memory and O(log n) floating point operations per iteration.
4 Theory
In this section, we analyze the convergence rate of SRG, and show that it can achieve a better
asymptotic error than SGD. Two key constants are helpfUl in contrasting the asymptotic errors of
3
Under review as a conference paper at ICLR 2022
SRG and SGD. Recall the definition of σ2 (xk , pk) in (3), and define:
1n
σ2 ：= σ2(x*,1∕n) = n EkVfi(X*)∣∣2
n i=1
σ2 := m∈i∆n σ2(χt,p') = n12 (X ∣∣vfi(x*)k2!
It is well known that the asymptotic error of SGD depends linearly on σ2 (Needell et al., 2014). We
here show that SRG reduces this to a linear dependence on σ2, which can be UP to n times smaller.
To study the convergence rate of SRG, we use the following Lyapunov function, which is similar to
the one used to study the convergence rate of SAGA (Hofmann et al., 2015; Defazio, 2016):
n
Tk = T(Xk, (gk)i=ι) := αTa- X Ugk- Vfi(X*)∣∣2 + ∣Xk - X*k2	(8)
θk Lmax
i=1
for a constant a > 0 that we set during the analysis. The proofs of this section are in Appendix B.
4.1	Intermediate lemmas
Before proceeding with the main result, let us first state two intermediate lemmas. The first studies
the evolution of (gki )in=1 from one iteration to the next.
Lemma 1. Let k ∈ N and suppose that (gki )in=1 evolves as in Algorithm 1. Taking expectation with
respect to (bk, ik), conditional on (bt, it)tk=-01, we have:
E XIIgk+1-Vfi (x*)∣∣2	≤(1-θ∖ XIIgk-Vfi(x*)∣∣2 +2θk L max [F (Xk) - F (x*)]
i=1	n i=1
The use of the Bernoulli random variable bk in Algorithm 1 to monitor the update of (gki )in=1 is
necessary for Lemma 1 to hold. In particular, without the use of bk, the elements of (gki )in=1 may
have different probabilities of being updated. In that case, the second term of Lemma 1 becomes a
weighted average of terms that are technically difficult to deal with. When the importance sampling
distribution does not depend on the iteration, this issue can be fixed with a slight modification of
the Lyapunov function (8) (Schmidt et al., 2015). In our case however, we are dealing with time-
varying importance sampling distributions, and this approach fails. This is why we rely on the
Bernoulli random variable bk to ensure that the probability of updating any gki is fixed to θk∕n.
The second lemma is a bound on the conditional variance of the gradient estimator used by SRG.
We intentionally leave as many free parameters as possible in the bound and optimize over them in
the main result of section 4.2.
Lemma 2. Let k ∈ N and assume that θk ∈ (0, 1∕2]. Taking expectation with respect to (bk, ik),
conditional on (bt, it)tk=-01, we have, for all β, γ, δ, η > 0:
E
ik 〜Pk
]≤	θfc max [F (Xk ) - F *] + 及2 X IIgk - Vfi(X*升2 +D3(1+2θk )σ2
where D1 , D2 and D3 are positive functions of the free parameters β , γ, δ, η.
4.2	Main result
Our main result is a bound on the evolution of the Lyapunov function Tk along the steps of SRG.
Theorem 1. Suppose that (Xk, (gki )in=1) evolves according to Algorithm 1. Further, assume that for
all k ∈ N: (i) αk ∕θk is non-increasing. (ii) θk ∈ (0, 1∕2]. (iii) αk ≤ θk ∕12Lmax. Then:
E [Tk+1] ≤ (1 - Pk)E [Tk] +(1 + 2θk)6αkσ2
4
Under review as a conference paper at ICLR 2022
for all k ∈ N, and where:
Pk =m∣n[去,ɑk μ
12n
Pk = min c1
The constants (1/12 in the bound on the step size and in Pk, 6 in front of the σ2 term) in this
theorem are optimized under the following constraints. First, the parameterized form of the above
bound shows that the largest allowable step size is given by c2θk/Lmax. Using it We get:
θk θk
——,c2-----(
n	κmax
for some constants c1, c2 > 0. As we do not know a priori the relative magnitudes of n and κmax,
and since c1 and c2 are inversely proportional, we impose the constraint c1 = c2 . Similarly, our
parameterized bound gives an asymptotic error of the form (1 + 2θk)c3αkσ2 for a constant c3.
We chose to impose the constraint c3 = 6. c1, c2 and c3 are all functions of the free parameters
of Lemma 2 and the constant a of the Lyapunov function (8). Numerically maximizing c2 (and
therefore the largest allowable step size) subject to these two constraints (c3 = 6 and c1 = c2) with
respect to these parameters yields the result in Theorem 1.
To give the reader an idea of the sensitivity of the result to the choice of c3 , note that setting c3 = 2
yields c2 ≈ 1/20, whereas taking c3	1 yields c2 ≈ 1/10. We have attempted to obtain the
best bound possible on the largest step size allowable, but the rather small prefactor c2 seems to
be an inevitable consequence of the multiple (but as far as we can tell necessary) uses of Young’s
inequality in our analysis. Our experiments suggest that the dependence on the mixture coefficient
θ is real, but that the prefactor c2 = 1/12 may be an artifact of the analysis.
For SRG with a constant mixture coefficient and step size, its convergence rate and complexity are
characterized by the following corollary of Theorem 1:
Corollary 1. Suppose that (xk, (gki )in=1) evolves according to Algorithm 1 with a constant mixture
coefficient θk = θ ∈ (0,1/2] and a ConStant SteP size ak = a ≤ θ∕12Lmax. Thenfor any k ∈ N:
22
E [Tk] ≤ (1 — ρ)k T0 + (1 + 2θ)6-~^
where P = Pk iS aS defined in Theorem 1. For any ε > 0 and θ ∈ (0, 1/2], chooSing:
θ θ θ
α = min < -------
12Lmax
εμ
(1 + 2θ)12σ2 ,
/ θ ε 1
V 1 + 2θ 144nσ2 ∫
and:
k ≥ max [ 1∣n, ɪ } log (2^)
[θ -μJ ∖ ε J
guarantees E [∣∣xk — x*k2] ≤ ε
Comparing the convergence rate of SRG in Corollary 1 with the standard result for SGD (Needell
et al., 2014), we see that they are of similar form. When P = -μ, the bound of Corollary 1 is better
asymptotically. Indeed, in this case, and as k → ∞, the iterates of SRG stay within a ball of radius
O( vzσ2) of the minimizer, while those of SGD stay withing a ball of radius O(√σ2). The equality
ρ = -μ holds when - ≤ 1∕nμ, which is true for all allowable step sizes if the problem is dominated
by its maximum condition number κmax ≥ n, and for small step sizes otherwise.
In terms of complexity, we have the following comparison. Up to constants, the complexity of SRG
with a constant mixture coefficient and step size is of the form:
O
We compare this to the complexity of SGD with constant step size (Needell et al., 2014):
In the high accuracy regime, the ε-1 terms dominate the complexities of SRG and SGD. In this case,
SRG enjoys a better complexity than SGD since σ2 ≤ σ2.
5
Under review as a conference paper at ICLR 2022
Algorithm 2 SRG+
Parameters: step sizes (αk)k∞=0 > 0, mixture coefficients (θk)k∞=0 ∈ (0, 1]
Initialization: x0∈Rd,(g0ik=20)in=1 ∈Rn	k=0
for k = 0, 1, 2, . . . do
pk = (1 - θk)qk + θkv	{qk is defined in (5), v is defined in (11)}
bk 〜BemoUlli(θk)
if bk = 1 then (ik,jk)〜π else ik 〜qk	{π maximally couples (v, 1/n)}
xk+1 = Xk - αk 'ik Nfik (Xk)
▽fj(Xk )∣∣2 if bk = 1 and j = jk
gkj	otherwise
end for
5 Extension
In this section, we extend SRG to combine its variance reduction capacity with the preconditioning
ability of smoothness-based importance sampling.
A straightforward way to generalize the argument given in the derivation of SRG in section 3 is to
consider the following bound on the conditional variance (3) of the gradient estimator, which can be
derived from Young’s inequality and the Li-smoothness of each fi :
σ2(xk,P) ≤ n X ^τ N 九(Xk)-“ 九(X)Xk- x*i+n X -i Ilgk
n i=1 p	n i=1 p
—Vfi(X*)∣2 + 3σ 2(Xk ,p)
(9)
While the motivating bound (6) seems more intuitive, because we think of gki as tracking ▽fi(Xk),
it turns out that this second bound better captures the evolution of gki in relation to ▽fi (Xk). At a
high-level, this is because gk tracks ▽ fi(Xk) indirectly: both hover around Vfi(X*) as k gets large.
Similar to our approach in section 3, our goal is to pick pk that minimizes the right-hand side of
(9). We know that qk (5) minimizes the third term, but we need to make sure that the first two
are also small. To minimize the first term, knowing nothing about the relative sizes of the inner
product terms, it makes sense to have probabilities proportional to the smoothness constants 1. On
the other hand, to keep the second term small, we need to ensure that the historical gradients (gki )in=1
are frequently updated, which we can do by imposing a uniform lower bound on the probabilities.
These considerations motivate us to consider the following distributions:
θ
Pk = (1 — ηk — θk)qk + ηkv + —	(10)
for positive mixture coefficients (θk, ηk) satisfying θk + ηk ∈ (0, 1], and where v is given by:
V =( ⅛ 1=1	(II)
Using these probabilities, we are able to show that the resulting algorithm does indeed achieve both
variance reduction and preconditioning. However, in the worst case, its complexity is twice as much
as what We would expect from simply replacing Lmax with L in Corollary 1. Intuitively, this is
because the probability assigned to the uniform component in (7) needs to be split between the
uniform and the smoothness-based components in (10).
5.1	CAREFULLY DECOUPLING THE UPDATES OF (gki )in=1 AND Xk
Here we show how to design our algorithm such that this additional factor of 2 (described above) in
the complexity is reduced to:
1 + kv — 1/nkT V ≤ 2 — 1/n
1Based on this argument alone, one would want them to be proportional to the square root of the smoothness
constants. This however does not lead to a nice averaging of the inner product terms, which is important for
technical reasons related to the strong-convexity of F but not of the component functions fi .
6
Under review as a conference paper at ICLR 2022
Following (Schmidt et al., 2015), our method is based on the observation that we can decouple the
index used to update the historical gradients and the index used to update the iterates, which we
refer to as jk and ik, respectively. Intuitively, to minimize (10) we would ideally want jk to be
uniformly distributed and ik to be distributed according to pk = (1 - θk)qk + θkv. Unfortunately,
sampling (jk, ik) independently with these marginals does not address our issue, because we would
still require an average of approximately two gradient evaluations per iteration.
Luckily, we can use any coupling between (jk, ik), because the evolution of the Lyapunov function
(8) only depends on the marginal distributions and not the joint. In particular, we obtain the same
bound on the evolution of T k regardless of how ik and jk are coupled. It therefore makes sense to
pick the coupling that maximizes the probability that ik = jk since this minimizes the number of
gradient evaluations required per iteration. Such couplings are known as maximal couplings in the
literature (see, e.g., Biswas et al. (2019)) and can easily be computed for discrete random variables.
With a maximal coupling, the expected number of gradient evaluations per iteration becomes 1 +
kv - 1/nkT V . Using this idea we arrive at SRG+ described in Algorithm 2.
Let us briefly discuss the implementation of SRG+. Sampling from π, the maximal coupling of
v and the uniform distribution, requires forming three probability vectors (see, e.g., Biswas et al.
(2019)). As both v and 1/n are constant throughout the optimization process, we can form these
vectors at the beginning of the algorithm along with their partial sums for a total initial cost of O(n)
operations. We can then sample from π in O(log n) time using binary search on the partial sums at
each iteration. Adding this sampling therefore does not change the overhead of SRG.
5.2 Analysis
The analysis of SRG+ is similar to that of SRG. In particular, the iterates of SRG+ also obey a
slight modification of Theorem 1 where the bound on the largest allowable step size is loosened to
αk ≤ θk/12L. We refer the reader to Appendix C for more details. Due to this improvement, We
get that the complexity of SRG+ with a constant mixture coefficient and step size is given by:
O
This shows that SRG+ performs both variance reduction as shown by the dependence of the com-
plexity on σ2 instead of σ2 and preconditioning as shown by the dependence on K instead of κmax.
6 Related work
At a high-level, three lines of work exist that study the use of importance sampling with SGD. The
first one considers fixed importance sampling distributions based on the constants of the problem
(Needell et al., 2014; Zhao & Zhang, 2015), and shows that such a strategy leads to improved condi-
tioning of the problem. The second considers adaptive importance sampling and similar to our work
targets the variance of the gradient estimator, but generally fails at providing strong convergence
rate guarantees under standard assumptions (Papa et al., 2015; Alain et al., 2016; Canevet et al.,
2016; Stich et al., 2017; Katharopoulos & Fleuret, 2018; Johnson & Guestrin, 2018). The third
line of work frames the problem as an online learning problem with bandit feedback and provides
guarantees on the regret of the proposed distributions in terms of the variance of the resulting esti-
mators (Namkoong et al., 2017; Salehi et al., 2017; Borsos et al., 2018; 2019; El Hanchi & Stephens,
2020). Our method is closely related to the one proposed by (Papa et al., 2015); their analysis how-
ever requires non-standard assumptions and their main result is asymptotic in nature. In contrast,
our work is the first to provide non-asymptotic guarantees on the suboptimality of the iterates for
variance-reducing importance sampling under standard technical assumptions. 7
7 Experiments
In this section, we empirically verify our two main claims: (i) SRG performs variance reduction
which can improve the asymptotic error of SGD. (ii) SRG+ performs both variance reduction and
preconditioning, and can both reduce the asymptotic error of SGD and allow the use of larger step
7
Under review as a conference paper at ICLR 2022
Figure 1: Left: Linear dependence of ∆SGD /∆SRG, the ratio of asymptotic errors of SGD and
SRG, on σ2/σ2, the ratio of the uniform and optimal variances at the minimum. Right: SRG+
achieves smaller asymptotic error than both SGD with Li -sampling (SGD+) and SGD with partially
biased sampling (SGD++), while using large O(1∕L) step sizes just like SGD+ and SGD++.
sizes. We start with controlled synthetic experiments that provide direct support for our claims. We
then compare SRG to other baseline optimizers on '2-regularized logistic regression problems. In
all experiments, we ran each algorithm 10 times and present the averaged result.
7.1	Synthetic experiments
For our first experiment, we consider the following toy problem. We let x ∈ R and fi (x) =
2(X - ai)2 where ai = 0 for i ∈ [n - 1] and an = 1. In this case, x* = 1/n, σ2 ≈ 1∕n, and
σ2 ≈ 4∕n2. We consider five instantiations of this problem with n ∈ {8,16, 32, 64,128}, yielding
ratios σ2∕σ2 approximately in {2, 4, 8,16, 32}. For each instantiation, we ran SGD and SRg until
they reached stationarity and recorded their asymptotic errors limk→∞ E
kxk - x* k22 , which we
denote by ∆SGD and ∆SRG respectively. We experimented with three different step sizes, two of
them allowed by Corollary 1, and one larger one for which we can only prove that SRG has a similar
convergence guarantee as SGD. For SRG, we used the mixture coefficient θ = 1∕2.
We plot ∆SGD ∕∆SRG against σ2∕σ*2 in Figure 1 (left) for each of the three step sizes, from which
we see that the relationship between the two ratios is linear, and very close to identity. From an
asymptotic error point of view, these results support our theory in that the improvement is seen to be
directly proportional to the ratio σ2∕σ*2. On the other hand, the constant 6(1 + 2θ)∕ρ in Corollary 1
would suggest that the proportionality constant is much smaller than 1, particularly when n is large,
but this is not what we observe in practice. This could be because the first term of the Lyapunov
function (8) is quite large at stationarity, or because the constants in our bound are not sharp due to
the multiple uses of Young’s inequality. This latter possibility is further supported by the fact that
we see a similar behaviour for SRG for step sizes larger than the ones allowed by our theory. We
have consistently made these two observations in other experiments. It is however unclear to us how
our analysis can be improved to match these observations.
For our second experiment, we considered the following problem. We let x ∈ R and fi(x) =
2Li(X — ai)2, and fixed n = 20. Similar to the first experiment, we take ai = 0 for i ∈ [n — 1] and
an = 1. We then set L1 = n — 1, Ln = 1/n, and Li = nn—2), so that L = 1, Lmax = n — 1. In this
case, we get X* = 1∕n2, σ+2 = σ2(X*, v) ≈ 1∕n, σ2 ≈ 2∕n2, and σ*2 ≈ 4∕n3. We then ran SGD
with Li-sampling (sGd+), SGD with partially biased sampling (SGD++, Needell et al. (2014)), and
SRG+, all with the largest allowable step size α = θ∕12L from our theory for SRG+, and we use
the mixture coefficient θ = 1∕2 for both SRG+ and SGD++. We have obtained very similar results
when using the larger step size α = θ∕2L, which is the maximum allowable for SGD+.
We plot the relative error E kXk — X* k22 ∕ kX0 — X* k22 for each algorithm against the number of
gradient oracle calls it makes in Figure 1 (right). We see that all three algorithms are able to converge
even when using the large O(1∕L) step sizes. The theory of SGD+ allows for larger step sizes
compared to SGD, but the asymptotic error of the algorithm depends on σ+2 (Needell et al., 2014),
which can be up to n times bigger than σ2. To solve this problem, Needell et al. (2014) proposed
8
Under review as a conference paper at ICLR 2022
Figure 2: Comparison of the evolution of the average relative error ∣∣xk - x* ∣∣2 / ∣∣x0 - x* ∣∣2 for
different optimizers on '2-regularized logistic regression problems using the datasets ijcnn1, w8a,
mushrooms, phishing. We compare SRG (orange) with SGD (blue), SGD with random shuffling
(purple), and SGD with the optimal variance-minimizing distributions at each iteration (green).
partially biased sampling which mixes the smoothness-based distribution with uniform sampling,
allowing the use of larger step sizes just like SGD+, but preserving the asymptotic error of SGD.
SRG+ further improves on SGD++, and reduces the asymptotic error, making it proportional to σ*2 .
7.2	'2-REGULARIZED LOGISTIC REGRESSION
For our last experiment, We test SRG on '2-regularized logistic regression problems. In this case,
the functions fi are given by:
fi(X) := log(I + eχp (-yiaTX)) + 2 Ilxk2
where yi ∈ {0,1} is the label of data point ai ∈ Rd. Each fi is convex and Li = 0.25 ∣∣ai ∣∣2 + μ
smooth. Their average F is also μ-strongly convex. As is standard, we select μ = 1/n.
We experiment with SRG on four datasets from LIBSVM (Chang & Lin, 2011): ijcnn1, w8a, mush-
rooms and phishing. For each dataset, and to be able to efficiently run our experiments, we randomly
select a subset of the data of size n = 1000. As the datasets are normalized, we have that ∣ai∣2 = 1
for all i ∈ [n]. This makes Li = L = 0.25 + μ, which reduces SGD+ to SGD, and SRG+ to SrG.
We tested the performance of SRG against three baselines, the first of which is standard SGD.
The second is SGD with the optimal variance-minimizing probabilities Pk Y ∣∣Vfi(Xk)∣2, which
allows us to compare SRG with the best possible variance-reducing importance sampling scheme.
The last baseline is SGD with random shuffling, which is also known to improve the asymptotic
error of SGD (under the additional assumption that each fi is also strongly convex), (Mishchenko
et al., 2020). We evaluate the performance of the algorithms by tracking the average relative error
E ∣Xk - X*∣22 / ∣X0 -X*∣22.
We used the mixture coefficient
1/2 for SRG, and used the
θ
same step size α = θ∕2L for all algorithms.
The results of this experiment are shown in Figure 2. We observe that SRG consistently outper-
forms SGD on all datasets, and that it closely matches the performance of SGD with the variance-
minimizing distributions, which it tries to approximate. We also see that SRG outperforms SGD
with random shuffling on two datasets, and is competitive with it on the remaining two. 8
8	Conclusion
We introduced SRG, a new importance-sampling based variance-reduced optimization algorithm for
finite-sum problems. We analyzed its convergence rate in the strongly convex and smooth case, and
showed that it can improve on the asymptotic error of SGD. We also introduced SRG+, an extension
of SRG which simultaneously performs variance reduction and preconditioning through importance
sampling. We expect our algorithms to be most useful in the medium accuracy regime, where
the required accuracy is higher than the one achieved by SGD, but low enough that the overhead
of classical variance reduced methods becomes significant. Finally, an interesting future direction
would be to explore non-greedy strategies for the design of importance sampling distributions for
SGD that not only minimize the variance of the current gradient estimator, but also take into account
the variance of the gradient estimators of subsequent iterations.
9
Under review as a conference paper at ICLR 2022
References
Guillaume Alain, Alex Lamb, Chinnadhurai Sankar, Aaron Courville, and Yoshua Bengio. Variance
Reduction in SGD by Distributed Importance Sampling. arXiv:1511.06481 [cs, stat], April 2016.
URL http://arxiv.org/abs/1511.06481. arXiv: 1511.06481.
Niloy Biswas, Pierre E. Jacob, and Paul Vanetti. Estimating Convergence of Markov
chains with L-Lag Couplings.	Advances in Neural Information Processing Sys-
tems, 32, 2019. URL https://proceedings.neurips.cc/paper/2019/hash/
aec851e565646f6835e915293381e20a-Abstract.html.
Zalan Borsos, Andreas Krause, and Kfir Y. Levy. Online Variance Reduction for Stochastic
Optimization. In Conference On Learning Theory, pp. 324-357. PMLR, July 2018. URL
http://proceedings.mlr.press/v75/borsos18a.html. ISSN: 2640-3498.
Zalan Borsos, Sebastian Curi, Kfir Yehuda Levy, and Andreas Krause. Online Variance Reduction
with Mixtures. In International Conference on Machine Learning, pp. 705-714. PMLR, May
2019. URL http://proceedings.mlr.press/v97/borsos19a.html. ISSN: 2640-
3498.
Olivier Canevet, Cijo Jose, and Francois Fleuret. Importance Sampling Tree for Large-scale Em-
pirical Expectation. In International Conference on Machine Learning, pp. 1454-1462. PMLR,
June 2016. URL http://proceedings.mlr.press/v48/canevet16.html. ISSN:
1938-7228.
Chih-Chung Chang and Chih-Jen Lin. Libsvm: A library for support vector machines. ACM Trans.
Intell. Syst. Technol., 2(3), May 2011. ISSN 2157-6904. doi: 10.1145/1961189.1961199. URL
https://doi.org/10.1145/1961189.1961199.
Aaron Defazio. A Simple Practical Accelerated Method for Finite Sums. Advances in Neural Infor-
mation Processing Systems, 29:676-684, 2016. URL https://proceedings.nips.cc/
paper/2016/hash/4f6ffe13a5d75b2d6a3923922b3922e5-Abstract.html.
Aaron Defazio and Leon Bottou. On the Ineffectiveness of Variance Reduced Opti-
mization for Deep Learning. Advances in Neural Information Processing Systems, 32:
1755-1765, 2019. URL https://proceedings.nips.cc/paper/2019/hash/
84d2004bf28a2095230e8e14993d398d- Abstract.html.
Aaron Defazio, Francis Bach, and Simon Lacoste-Julien.	SAGA: A Fast Incre-
mental Gradient Method With Support for Non-Strongly Convex Composite Ob-
jectives. Advances in Neural Information Processing Systems, 27:1646-1654,
2014.	URL https://proceedings.neurips.cc/paper/2014/hash/
ede7e2b6d13a41ddf9f4bdef84fdc737- Abstract.html.
Ayoub El Hanchi and David Stephens. Adaptive Importance Sampling for Finite-Sum Opti-
mization and Sampling with Decreasing Step-Sizes. Advances in Neural Information Process-
ing Systems, 33, 2020. URL https://proceedings.nips.cc/paper/2020/hash/
b58f7d184743106a8a66028b7a28937c-Abstract.html.
Thomas Hofmann, Aurelien Lucchi, Simon Lacoste-Julien, and Brian McWilliams. Variance Re-
duced Stochastic Gradient Descent with Neighbors. Advances in Neural Information Processing
Systems, 28:2305-2313, 2015. URL https://proceedings.nips.cc/paper/2015/
hash/effc299a1addb07e7089f9b269c31f2f-Abstract.html.
Rie Johnson and Tong Zhang. Accelerating Stochastic Gradient Descent using Pre-
dictive Variance Reduction. Advances in Neural Information Processing Systems,
26:315-323, 2013. URL https://proceedings.nips.cc/paper/2013/hash/
ac1dd209cbcc5e5d1c6e28598e8cbbe8- Abstract.html.
Tyler B. Johnson and Carlos Guestrin. Training Deep Models Faster with Robust, Ap-
proximate Importance Sampling. Advances in Neural Information Processing Systems,
31:7265-7275, 2018. URL https://proceedings.nips.cc/paper/2018/hash/
967990de5b3eac7b87d49a13c6834978- Abstract.html.
10
Under review as a conference paper at ICLR 2022
Angelos Katharopoulos and Francois Fleuret. Not All Samples Are Created Equal: Deep
Learning with Importance Sampling. In International Conference on Machine Learning,
pp. 2525-2534. PMLR, July 2018. URL http://Proceedings.mlr.ρress∕v8 0∕
katharopoulos18a.html. ISSN: 2640-3498.
Konstantin Mishchenko, Ahmed Khaled Ragab Bayoumi, and Peter Richtarik. Random Reshuf-
fling: Simple Analysis with Vast Improvements. Advances in Neural Information Process-
ing Systems, 33, 2020. URL https://proceedings.nips.cc/paper/2020/hash/
c8cc6e90ccbff44c9cee23611711cdc4- Abstract.html.
Hongseok Namkoong, Aman Sinha, Steve Yadlowsky, and John C. Duchi. Adaptive Sampling
Probabilities for Non-Smooth Optimization. In International Conference on Machine Learn-
ing, pp. 2574-2583. PMLR, July 2017. URL http://proceedings.mlr.press/v70/
namkoong17a.html. ISSN: 2640-3498.
Deanna Needell, Rachel Ward, and Nati Srebro. Stochastic Gradient Descent, Weighted Sam-
pling, and the Randomized Kaczmarz algorithm. Advances in Neural Information Processing
Systems, 27:1017-1025, 2014. URL https://proceedings.nips.cc/paper/2014/
hash/f29c21d4897f78948b91f03172341b7b-Abstract.html.
Yurii Nesterov. Introductory Lectures on Convex Optimization: A Basic Course. Applied Optimiza-
tion. Springer US, 2004. ISBN 978-1-4020-7553-7. doi: 10.1007/978-1-4419-8853-9. URL
https://www.springer.com/gp/book/9781402075537.
GUillaUme Papa, Pascal Bianchi, and Stephan Clemencon. Adaptive Sampling for Incremental Op-
timization Using Stochastic Gradient Descent. In Kamalika Chaudhuri, CLAUDIO GENTILE,
and Sandra Zilles (eds.), Algorithmic Learning Theory, LectUre Notes in CompUter Science,
pp. 317-331, Cham, 2015. Springer International PUblishing. ISBN 978-3-319-24486-0. doi:
10.1007/978-3-319-24486-021.
Herbert Robbins and SUtton Monro. A Stochastic Approximation Method. Annals of
Mathematical Statistics, 22(3):400-407, September 1951. ISSN 0003-4851, 2168-8990.
doi: 10.1214/aoms/1177729586. URL https://projecteuclid.org/euclid.aoms/
1177729586. PUblisher: InstitUte of Mathematical Statistics.
Nicolas RoUx, Mark Schmidt, and Francis Bach. A Stochastic Gradient Method with an Expo-
nential Convergence _rate for Finite Training Sets. Advances in Neural Information Processing
Systems, 25:2663-2671, 2012. URL https://proceedings.nips.cc/paper/2012/
hash/905056c1ac1dad141560467e0a99e1cf-Abstract.html.
Farnood Salehi, L. Elisa Celis, and Patrick Thiran. Stochastic Optimization with Bandit Sampling.
arXiv preprint arXiv:1708.02544, AUgUst 2017. URL https://arxiv.org/abs/1708.
02544v2.
Mark Schmidt, Reza Babanezhad, Mohamed Ahmed, Aaron Defazio, Ann Clifton, and Anoop
Sarkar. Non-Uniform Stochastic Average Gradient Method for Training Conditional Random
Fields. In Artificial Intelligence and Statistics, pp. 819-828. PMLR, FebrUary 2015. URL
http://proceedings.mlr.press/v38/schmidt15.html. ISSN: 1938-7228.
Mark Schmidt, Nicolas Le RoUx, and Francis Bach. Minimizing finite sUms with the stochas-
tic average gradient. Mathematical Programming, 162(1):83-112, March 2017. ISSN
1436-4646. doi: 10.1007/s10107-016-1030-6. URL https://doi.org/10.1007/
s10107-016-1030-6.
Sebastian U. Stich, Anant Raj, and Martin Jaggi. Safe Adaptive Importance Sam-
pling. Advances in Neural Information Processing Systems,	30:4381-4391,
2017.	URL https://proceedings.neurips.cc/paper/2017/hash/
1177967c7957072da3dc1db4ceb30e7a- Abstract.html.
Peilin Zhao and Tong Zhang. Stochastic Optimization with Importance Sampling for RegUlarized
Loss Minimization. In International Conference on Machine Learning, pp. 1-9. PMLR, JUne
2015. URL http://proceedings.mlr.press/v37/zhaoa15.html. ISSN: 1938-
7228.
11