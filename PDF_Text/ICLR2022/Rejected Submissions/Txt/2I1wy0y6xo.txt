Under review as a conference paper at ICLR 2022
Stability analysis of SGD through the nor-
MALIZED LOSS FUNCTION
Anonymous authors
Paper under double-blind review
Ab stract
We prove new generalization bounds for stochastic gradient descent for both the
convex and non-convex cases. Our analysis is based on the stability framework.
We analyze stability with respect to the normalized version of the loss function
used for training. This leads to investigating a form of angle-wise stability instead
of euclidean stability in weights. For neural networks, the measure of distance we
consider is invariant to rescaling the weights of each layer. Furthermore, we exploit
the notion of on-average stability in order to obtain a data-dependent quantity in
the bound. This data-dependent quantity is seen to be more favorable when training
with larger learning rates in our numerical experiments. This might help to shed
some light on why larger learning rates can lead to better generalization in some
practical scenarios.
1	Introduction
In the last few years, deep learning has succeeded in establishing state-of-the-art performances in a
wide variety of tasks in fields like computer vision, natural language processing and bioinformatics
(LeCun et al., 2015). Understanding when and how these networks generalize better is important
to keep improving their performance. Many works starting mainly from Neyshabur et al. (2015),
Zhang et al. (2017) and Keskar et al. (2017) hint a rich interplay between regularization and the
optimization process of learning the weights of the network. The idea is that a form of inductive
bias can be realized implicitly by the optimization algorithm. The most popular algorithm to train
neural networks is stochastic gradient descent (SGD). It is therefore of great interest to study the
generalization properties of this algorithm. An approach that is particularly well suited to investigate
learning algorithms directly is the framework of stability (Bousquet & Elisseeff, 2002), (Elisseeff
et al., 2005). It is argued in Nagarajan & Kolter (2019) that generalization bounds based on uniform
convergence might be condemned to be essentially vacuous for deep networks. Stability bounds
offer a possible alternative by trying to bound directly the generalization error of the output of the
algorithm. The seminal work of Hardt et al. (2016) exploits this framework to study SGD for both the
convex and non-convex cases. The main intuitive idea is to look at how much changing one example
in the training set can generate a different trajectory when running SGD. If the two trajectories must
remain close to each other then the algorithm has better stability.
This raises the question of how to best measure the distance between two classifiers. Our work
investigates a measure of distance respecting invariances in homogeneous neural networks (and linear
classifiers) instead of the usual euclidean distance. The measure of distance we consider is directly
related to analyzing stability with respect to the normalized loss function instead of the standard loss
function used for training. In the convex case, we prove an upper bound on uniform stability with
respect to the normalized loss function, which can then be used to prove a high probability bound on
the test error of the output of SGD. In the non-convex case, we propose an analysis directly targeted
toward homogeneous neural networks. We prove an upper bound on the on-average stability with
respect to the normalized loss function, which can then be used to give a generalization bound on
the test error. One nice advantage coming with our approach is that we do not need to assume that
the loss function is bounded. Indeed, even if the loss function used for training is unbounded, the
normalized loss is necessarily bounded.
Our main results for neural networks involve a data-dependent quantity that we estimate during
training in our numerical experiments. The quantity is the sum over each layer of the ratio between
1
Under review as a conference paper at ICLR 2022
the norm of the gradient for this layer and the norm of the parameters for the layer. We observe
that larger learning rates lead to trajectories in parameter space keeping this quantity smaller during
training. There are two ways to get our data-dependent quantity smaller during training. The first is
by facilitating convergence (having smaller norms for the gradients). The second is by increasing the
weights of the network. If the weights are larger, the same magnitude for an update in weight space
results in a smaller change in angle (see Figure 1). In our experiments, larger learning rates are seen
to be more favorable in both regards.
Our main contributions are summarized as follows:
1)	The first analysis of stability for SGD directly exploiting invariances in homogeneous neural
networks (smooth and non-smooth cases).
2)	Empirical observations suggesting that our data-dependent quantity is interesting in under-
standing how larger learning rates can improve generalization.
3)	An analysis of stability for the convex case naturally incorporating the norm of the initial
point.
Figure 1: For the same magnitude of step taken (same ball radius), a larger norm of parameters leads
to a smaller change in angle.
2	Related work
Normalized loss functions have been considered before (Poggio et al., 2019), (Liao et al., 2018). In
Liao et al. (2018), test error is seen to be well correlated with the normalized loss. This observation
is one motivation for our study. We might expect generalization bounds on the test error to be
better by using the normalized surrogate loss in the analysis. (Poggio et al., 2019) writes down a
generalization bound based on Rademacher complexity but motivated by the possible limitations of
uniform convergence for deep learning (Nagarajan & Kolter, 2019) we take the stability approach
instead.
Generalization of SGD has been investigated before in a large body of literature. Soudry et al. (2018)
showed that gradient descent converges to the max-margin solution for logistic regression and Lyu
& Li (2019) provides an extension to deep non-linear homogeneous networks. Nacson et al. (2019)
gives similar results for stochastic gradient descent. From the point of view of stability, starting from
Hardt et al. (2016) without being exhaustive, a few representative examples are Bassily et al. (2020),
Yuan et al. (2019), Kuzborskij & Lampert (2018),Liu et al. (2017), London (2017).
Since the work of Zhang et al. (2017) showing that currently used deep neural networks are so
overparameterized that they can easily fit random labels, taking properties of the data distribution into
account seems necessary to understand generalization of deep networks. In the context of stability,
this means moving from uniform stability to on-average stability. This is the main concern of the
work of Kuzborskij & Lampert (2018). They develop data-dependent stability bounds for SGD by
extending over the work of Hardt et al. (2016). Their results have a dependence on the risk of the
initialization point and the curvature of the initialization. They have to assume a bound on the noise
of the stochastic gradient. We do not make this assumption in our work. Furthermore, we maintain
in our bounds for neural networks the properties after the “burn-in” period and therefore closer to
2
Under review as a conference paper at ICLR 2022
the final output since we are interested in the effect of the learning rate on the trajectory. This is
motivated by the empirical work of Jastrzebski et al. (2020) arguing that in the early phase of training,
the learning rate and batch size determine the properties of the trajectory after a “break-even point”.
Another work interested in on-average stability is Zhou et al. (2021). Differently from our work,
their approach makes the extra assumptions that the variance of the stochastic gradients is bounded
and also that the loss is bounded. Furthermore, our analysis directly exploits the structure of neural
networks and the properties following from using homogeneous non-linearities.
It has been observed in the early work of Keskar et al. (2017) that training with larger batch sizes can
lead to a deterioration in test accuracy. The simplest strategy to reduce (at least partially) the gap with
small batch training is to increase the learning rate (He et al., 2019), (Smith & Le, 2018), (Hoffer
et al., 2017), (Goyal et al., 2017). We choose this scenario to investigate empirically the relevance
of our stability bound for SGD on neural networks. Note that the results in Hardt et al. (2016) are
more favorable to smaller learning rates. It seems therefore important in order to get theory closer to
practice to understand better in what sense larger learning rates can improve stability.
3	Preliminaries
Let l(w, z) be a non-negative loss function. Furthermore, let A be a randomized algorithm and denote
by A(S) the output of A when trained on training set S = {zι, •…，zn}〜Dn. The true risk for a
classifier w is given as
LD (w) := Ez〜Dl(w,z)
and the empirical risk is given by
LS (W) ：= nn PLi l(w,zi).
When considering the 0 - 1 loss of a classifier w, we will write L0D-1(w). Furthermore, we will add
a superscript α when the normalized losses lα are under consideration (these will be defined more
clearly in the subsequent sections respectively for the convex case and the non-convex case). Our
main interest is to ensure small test error and so we want to bound L0D-1(w). The usual approach
is to minimize a surrogate loss upper bounding the 0 - 1 loss. In this paper, we consider stochastic
gradient descent with different batch sizes to minimize the empirical surrogate loss. The update rule
of this algorithm for learning rates λt and a subset Bt ⊂ S of size B is given by
Wt+1 = Wt- λtB1 X E(Wt,Zj).	(1)
zj∈Bt
We assume sampling uniformly with replacement in order to form each batch of training examples. In
order to investigate generalization of this algorithm, we consider the framework of stability (Bousquet
& Elisseeff, 2002).
We now give the definitions for uniform stability and on-average stability (random pointwise hypoth-
esis stability in Elisseeff et al. (2005)) for randomized algorithms (see also Hardt et al. (2016) and
Kuzborskij & Lampert (2018)). The definitions can be formulated with respect to any loss function
but since we will study stability with respect to the lα losses, we write the definitions in the context
of this special case.
Definition 1 The algorithm A is saidto be uαni-uniformly stable if for all i ∈ {1, . . . , n}
sup E [∣lα(A(S),z)-lα(A(S⑴)，z)∣] ≤ ‹αni∙	(2)
S,zi0 ,z
Here, the expectation is taken over the randomness of A. The notation S(i) means that we replace the
ith example of S with zi0.
Definition 2 The algorithm A is said to be aαv -on-average stable if for all i ∈ {1, . . . , n}
E[∣lα(A(S),z) - lα(A(S⑺),z)∣] ≤ 曝.	⑶
3
Under review as a conference paper at ICLR 2022
Here, the expectation is taken over S 〜Dn, Z ~ D and the randomness of A. The notation S(i)
means that we replace the ith example of S with z.
Throughout the PaPer, ∣∣∙∣∣ will denote the euclidean norm for vectors and the FrobeniUs norm for
matrices. The proofs are given in Appendix A for the convex case and in Appendix B for the
non-convex case.
4 Convex case: A first step toward the non-convex case
Since the convex case is easier to handle, it can be seen as a good PreParation for the non-convex case.
Consider a linear classifier Parameterized by either a vector of weights (binary case) or a matrix of
weights (multi-class case) that we denote by w in both cases. The normalized losses are defined by
w
l (w,z):= l(αPWi,z),	(4)
for α > 0.
In order to state the main result of this section, we need two common assumPtions: L-LiPschitzness
of l as a function of w and β-smoothness.
Definition 3 The function l(w, z) is L-Lipschitz for all z in the domain (with respect to w) if for all
w, w0, z,
|l(w, z) — l(w0, z)∣≤ L||w - w0||.	(5)
Definition 4 The function l(w, z) is β-smooth if for all w, w0, z,
||Vl(w, z) — Vl(w0, z)∣∣≤ β||w - w0||.	(6)
We are now ready to state the main result of this section.
Theorem 1 Assume that l(w, z) is convex, β-smooth and L-Lipschitz for all z. Furthermore,
assume that the initial point w° satisfies ||wo ∣∣≥ K for some K such that K = K — L PT-o1 λi > 0
forasequence oflearning rates λ% ≤ 2∕β. SGD is then run with batch size B on loss function l(w, z)
for T steps with the learning rates λt starting from w0. Denote by uαni the uniform stability of this
algorithm with respect to lα. Then,
α L 2L2B
Cuni ≤ α-∕~
nK
T-1
Xλi.
i=0
(7)
What is the main difference between our bound and the bound in Hardt et al. (2016) (see theorem 7
in APPendix A) ? Our bound takes into account the norm of the initialization. The meaning of the
bound is that it is not enough to use small learning rates and a small number of ePochs to guarantee
good stability (with resPect to the normalized loss). We also need to take into account the norm of
the Parameters (here the norm of the initialization) to make sure that the “effective” learning rates are
small. Note that all classifiers are contained in any ball around the origin even if the radius of the ball
is arbitrarily small. Therefore, all control over stability is lost very close to the origin where even
a small steP (in Euclidean distance) can lead to a drastic change in the classifier. The norm of the
initialization must therefore be large enough to ensure that the trajectory cannot get too close to the
origin (in worst case, since uniform stability is considered). An alternative if the conditions of the
theorem are too strong in some Practical scenarios is to use on-average stability (l = 1 layer in the
results of section 5). As a side note, we also incorPorated the batch size into the bound which is not
Present in Hardt et al. (2016) (only B = 1 is considered).
From this result, it is now Possible to obtain a high Probability bound for the test error. The bound is
over draws of training sets S but not over the randomness of A. 1 So, we actually have the exPected
1It is Possible to obtain a bound holding over the randomness of A by exPloiting the framework of Elisseeff
et al. (2005). However, the term involving ρ in their theorem 15 does not converge to 0 when the size of the
training set grows to infinity.
4
Under review as a conference paper at ICLR 2022
test error over the randomness of A in the bound. This is reminiscent of PAC-Bayes bounds where
here the posterior distribution would be induced from the randomness of the algorithm A.
Theorem 2 Fix α > 0. Let Ma= sup{l(w, z) s.t. ∣∣w∣∣≤ α, ∣∣x∣∣≤ R}. Then, for any n > 1 and
δ ∈ (0, 1), the following hold with probability greater or equal to 1 - δ over draws of training sets S:
EALrI(A(S)) ≤ EALa (A(S)) + *i + "n + Ma√ln≡ ∙	⑹
Proof: The proof is an application of McDiarmid’s concentration bound. Note that we do not need
the training loss to be bounded since we consider the normalized loss which is bounded. The proof
follows the same line as theorem 12 in Bousquet & Elisseeff (2002) and we do not replicate it here.
Note that we need to use that uniform stability implies generalization in expectation which is proven
for example in theorem 2.2 from Hardt et al. (2016).
Furthermore, a bound holding uniformly over all α's can be obtained using standard techniques.
Theorem 3 Let C > 0. Assume that la (w, z) is a convex function of α for all w, z and that uani is a
non-decreasing function of α. Then, for any n > 1 and δ∈ (0, 1), the following hold with probability
greater or equal to 1 - δ over draws of training sets S:
EAL0D-1(A(S)) ≤ inf	EA max (LSa/2(A(S)), LSa(A(S))) + uani + (2nuani +
α∈(0,C] I
Ma) j2 ln(√2(2 + log2 C- log2 α)) + ln(1∕δ) }.
In the next section, we investigate the non-convex case. We exploit on-average stability to obtain a
data-dependent quantity in the bound. Note that it is also argued in Kuzborskij & Lampert (2018)
that the worst case analysis of uniform stability might not be appropriate for deep learning.
5	Non-convex case
We consider homogeneous neural networks in the setup of multiclass classification. Write f(x) =
Wl (σ(•… 卬2 (σ(Wιx)))), where X is an input to the neural network, Wi denotes the weight matrix
at layer i and σ denotes a homogeneous non-linearity (σ(cx) = ckσ(x) for any constant c > 0).
Examples for the non-linearity are the ReLU function (k = 1), the quadratic function (k = 2) and the
identity function (k = 1, leading to deep linear networks). Consider a non-negative loss function
l(s, y) that receives a score vector s = f(x) and a label y as inputs. We require the loss function to
be L-Lipschitz for all y as a function of s. That is, for all s, s0 , y,
ll(s,y)- l(s0,y)l≤ L||s - s0||.	⑼
For example, we can use the cross-entropy loss (softmax function with negative log likelihood). In
this case, it is simple to show by bounding the norm of the gradient of l(s,y) with respect to S that
we can use L = √2. Note that this is slightly different from the Lipschitz assumption of the previous
section (given with respect to the weights w).
In order to control the behaviour of the non-linearity, we assume that for any c > 0, there exist
constants Bc and Lc such that for any x,y ∈ Rd with ∣∣x∣∣≤ C and ∣∣y∣∣≤ C we have
Uσ(X)II ≤ Bc||x|1,	(IO)
Uσ(X)- σ(y)11 ≤ Lc||x - y||.	(II)
Note that the non-linearity σ is being applied component-wise when the input is a vector as above. It
is easy to verify that, for the ReLU function, we have Bc = 1 and Lc = 1 for all C. Furthermore, for
the quadratic function X2, we have Bc = C and Lc = 2C. The following lemma will be the starting
point for our analysis.
5
Under review as a conference paper at ICLR 2022
Lemma 1 Assume that ∣∣x∣∣≤ R. Let ɑι, .…αι be positive real numbers and, for 1 ≤ j ≤ l, denote
Wj ：= ∣Wj|| and 优：=∣Wj011. Write Sj = a Wj(σ(…α? W2(σ(α1W1x)))) and
Sj = aj W0(σ(∙ ∙ ∙ a2明 (σ(aιW1 x)))). Also, let Cj be an upper bound on the norm of layer j (this
will be a constant depending on aι, ∙…αj and R). Then, we have
∣∣sι-sl∣∣≤ R(YI aj)χX Ti ||Wi - W1|,	(12)
j=1	i=1
where τi = Y ( Bcj
j=1,j6=i	Lcj
if j < i
if j > i
-1
The previous lemma motivates a measure of “distance” between neural networks.
Definition 5 For neural networks f and g, where the weight matrices of f are given by Wi ∙∙∙ Wl
and the weight matrices of g are g^ven by W0 ∙∙∙ Wl0, define
l	Wi	W 0
d(f,g) ：= XX τillE-w⅛”.	(13)
Note that this distance function is invariant to rescaling the weights of any layer. This is a desirable
property since in a homogeneous network such a reparametrization leaves the class predicted by the
classifier unchanged for any input to the network.
Let ai,…al be positive real numbers. We define the lɑ1 , al (f, Z) losses to be equal to
Wl	W2	W1
IgI ~∖Wl∖∖ (σ(∙∙∙ ɑ2 WT (σ(a1 WlX)))),y),
(14)
where z = (x, y) and f is the neural network with weight matrix at layer i given by Wi . That is,
we project the weight matrices to give the norm ai to layer i and then we evaluate the loss l on this
“normalized” network. For simplicity, we will only consider the case where all the ai ’s are equal to
say a and we will write lα(f, z). From our definitions and lemma 1, we have that for all z and neural
networks f and g ,
∖lα(f, z) - lα(g, z)∖≤ LRald(f, g).	(15)
In order to bound stability with respect to lα , we will have to ensure that the two trajectories cannot
diverge too much in terms of d(f, g).
We will consider two separate cases: the smooth case and the non-smooth case. When the activation
function is smooth (for example xk for k ≥ 1), we will exploit the concept of layer-wise smoothness
defined below.
Definition 6 Consider the gradient of the loss function with respect to the parameters W for some
training example z. The vector containing only the partial derivatives for the weights of layer j will
be denoted by V(j)l(W, z). We define {βj}j=i-layerwise smoothness as thefollowingproperty: For
all j, z, W = (Wi, ∙∙∙ ,Wl) and W0 = (W1,…，Wl0),
∖∖V(j)l(W, z) - V(j)l(W 0, z)∖∖≤ βj∖∖Wj - Wj0∖∖.	(16)
We also let β ：= max{βj}. Note that β is upper bounding the spectral norm of the bloc diagonal
approximation of the Hessian.
We are now ready to state the main theorem of this section for the smooth case.
6
Under review as a conference paper at ICLR 2022
Theorem 4 Suppose that the loss function l(s, y) is L-Lipschitz for all y, non-negative and that
lα (f, z) is bounded above by Mα. Furthermore, assume {βj}lj =1 -layerwise smoothness and that
∣∣x∣∣≤ R∙ Finally, let B denote the batch size, λt ≤ C the learning rates and T the number of
iterations SGD is being run. Then,
aαv ≤ inf
一t0∈{1,2,…,B }
2BLRal
(n — B)β
T — 1
t0 — 1
cβ T-1
t=t0
Ma处,
n
(17)
where β = max{βj} and ζt
Pj=I TjEa,s,z CjSz)T-t ll7L)BS,Wt)11
with Kt(j) (S, z) := min{||Wj,t||, ||Wj0,t||} and Cj (S, z) :
max	KTz)
t0≤t≤T-1 Kt(+j)1(S, z)
To evaluate the bound, we need to find the best t0 . There is a tradeoff here between two quantities. A
small to is better for the term MaBnO but is worse for the remaining term. This establishes the best
“burn in” period. The amount of “exploration” before t0 does not effect the generalization bound.
The amount of exploration measured by ζt (and through the learning rate via the value of c) becomes
important only after iteration t0 . The bound will be better if we can reach a region in parameter space
such that the classifier is then effectively not changing much. This is measured through the norm of
the gradient but also takes into account the norm of the parameters (via Kt(j)(S, z)). When we reach
larger norms of parameters, stability (with respect to the normalized loss) is less negatively affected.
The intuitive reason is the following: the same magnitude of step results in a smaller change in the
classifier if the parameters are larger (see Figure 1). In Hoffer et al. (2017), it is observed that small
batch training and larger learning rates (finding solutions generalizing better) are reaching larger
norms of parameters (see also our Figure 3). Using standard Euclidean distance in the analysis of
stability would lead us to believe that this behaviour is highly undesirable. Our analysis shows that
this behaviour can actually be favorable to the on-average stability with respect to the normalized
loss. The quantity ζt also involves the terms Cj (S, z) measuring how fast the norm of the parameters
is growing from one iteration to the next. The value of Cj (S, z) is better (smaller) if the norm of the
parameters grows faster.
The non-smooth case is also of great interest since the ReLU activation function is very common in
practice.
Theorem 5 Suppose that the loss function l(s, y) is L-Lipschitz for all y, non-negative and that
la(f,z) is bounded above by Ma. Furthermore, assume ∣∣x∣∣≤ R. Finally, let B denote the batch
size, λt the learning rates and T the number of iterations SGD is being run. Then,
aav ≤ inf
to ∈{1,2,..., B}
T-1	Bt
2LRal)、λtZt + Ma-----
n
t=t0
(18)
where ζt is defined as in theorem 4.
Exploiting theorem 12 in Elisseeff et al. (2005), it is then possible to get a probabilistic bound on the
test error (holding over the randomness in the training sets and the randomness in the algorithm).
Theorem 6 Fix α > 0. Then, for any n > 1 and δ ∈ (0, 1), the following hold with probability
greater or equal to 1 — δ over draws of training sets S and the randomness of the algorithm A:
^D-1(A(S)) ≤ La(A(S)) + ∖l(1] 2Mα + 12nMa⅞.	(19)
6 Experiments
6.1	LEARNING RATES AND ζt
In this section we conduct some experiments on the datasets CIFAR10 Krizhevsky (2009) and MNIST
LeCun & Cortes (2010). We consider the scenario where we try to reduce the performance gap
7
Under review as a conference paper at ICLR 2022
between small batch and large batch training by increasing the learning rate. We will give some
evidence suggesting that the quantity ζt can be of interest to assess generalization in this case.
We use a global learning rate being decayed one time by a factor of 10 in our experiments. No
weight decay or momentum is used to stay closer to our theoretical analysis of SGD. Note that in
principle, the learning rate could be as large as we want during the inital burn-in period (before t0)
without hurting stability. However, this burn-in period must be inside the first epoch in the theoretical
results we presented. Since in practice we train for many epochs, it is not clear if such a small
burn-in period is long enough to be significant in current practice. We still think that the quantity ζt
is relevant to investigate empirically. We approximate its value on a training set S with the quantity
Zt(S) := Pj=ι "∖LWt (Wt)11. The quantities Cj (S, Z) can be evaluated empirically to be very close
to 1 and so we neglect them in the expression for ζt(S). Also note that τj = 1 for all j in the case of
ReLU networks. Instead of plotting the value for each iteration, we average ζt (S) for each epoch.
This leads to smoother curves.
We use a 5-layer convolutional Relu network consisting in 2 convolutional layers with maxpooling
and then 3 fully connected layers with cross-entropy loss on CIFAR10. We use also the cross-entropy
loss on MNIST but the neural network is a 6-layers fully connected network. In both cases, we use
batch-normalization to facilitate training. All the results in the figures are obtained when using a
batch size of 2048. We started by training with a smaller batch size of 256 and then tried to reduce
the gap in performance between large batch and small batch training by increasing the learning rate.
For example, on CIFAR10, we obtain a test accuracy of 86.23% when using a batch size of 256 and a
learning rate of 0.5. When increasing the batch size to 2048 (and maintaining the learning rate to 0.5),
the test accuracy dropped to 85.14%. This happened even if the training loss reach approximately
the same value in both cases (0.0123 for batch size 256 and 0.0167 for batch size 2048). We then
increased the learning rate to 1.0 and then to 1.5 reaching 85.63% in both cases (not completely
solving the gap but reducing it). A similar phenomenon happens for MNIST. Here, with batch size
256 we get 98.57% (lr = 0.05) of test accuracy and for batch size 2048, we get 97.52% (lr = 0.05),
.	__ 一	一.	一	一人，.	. 一.
98.00% (lr = 0.1) and 98.39% (lr = 0.5). We plotted the values of Zt(S) during training in Figure
2. We can see that it is better during all training when increasing the learning rate.
To compare with the analysis from Hardt et al. (2016), the quantity Zt would be replaced with a global
Lipschitz constant which would not be affected by the actual trajectory of the algorithm. Therefore, in
comparison to our bound, the bound in Hardt et al. (2016) would be much more favorable to smaller
learning rates. In other words, the worst case analysis of uniform convergence would require much
smaller learning rates to be used than our result to guarantee good stability. The quantity Zt can be
improved by accelerating convergence because of the numerator (norm of the gradients) but also by
increasing the denominator (norm of the parameters). A larger learning rate can help in both these
regards (see Figure 3). Note also that considering only the norm of the gradients without the norm of
the parameters would lead to a less favorable quantity compared to considering both the norm of the
gradients and the norm of the parameters. A standard analysis of stability (without the normalized
loss) similar to Kuzborskij & Lampert (2018) would not benefit from the norm of the parameters.
Figure 2: Zt(S) when training a convolutional network on CIFAR10 and a fully connected network
on MNIST.
8
Under review as a conference paper at ICLR 2022
Figure 3: Norm of the parameters (layer 3) and norm of the gradient when training a convolutional
network on CIFAR10
6.2 Generalization bound and test error
We show in this section the usefulness of considering the normalized loss for bounding the test error.
We evaluate the bound in theorem 6 and compare it to an analogous version for the unnormalized loss.
For this analogous version, we replace the upper bound M on the loss function by the largest loss
achieved during training. Furthermore, the quantity av is upper bounded by the Lipschitz constant
times the Euclidean distance between the weights of the networks. The Lipschitz constant is replaced
by the largest norm of gradients obtained during training. For the normalized loss, we upper bound
aαv by LRαlEd(f, g) (see equation 15). We plot the test error, the upper bound for the normalized
case with α = 1.0 and the upper bound for the unnormalized case in Figure 4. Further experiments
(with label noise and a comparison of Adam and SGD) are given in Appendix C.
Figure 4: The bound obtained from the Euclidean distance is much worse than the bound obtained
from our normalized distance. However, the generalization bound is still vacuous. The network is a
6-layer fully connected network trained on MNIST.
7 Conclusion
We investigated the stability (uniform and on-average) of SGD with respect to the normalized loss
functions. This leads naturally to consider a more meaningful measure of distance between classifiers.
Our experimental results show that stability might not be as bad as expected when using larger
learning rates in training deep neural networks. We hope that our analysis will be a helpful step in
understanding generalization in deep learning. Future work could investigate the on-average stability
with respect to lα losses for different optimization algorithms.
9
Under review as a conference paper at ICLR 2022
References
Raef Bassily, Vitaly Feldman, Cristdbal Guzmdn, and Kunal TalWar Stability of stochastic gradient
descent on nonsmooth convex losses. In Hugo Larochelle, Marc’Aurelio Ranzato, Raia Hadsell,
Maria-Florina Balcan, and Hsuan-Tien Lin (eds.), Advances in Neural Information Processing
Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020,
December 6-12, 2020, virtual, 2020.
Olivier Bousquet and Andre Elisseeff. Stability and generalization. J. Mach. Learn. Res., 2:499-526,
2002.
Andre Elisseeff, Theodoros Evgeniou, and Massimiliano Pontil. Stability of randomized learning
algorithms. J. Mach. Learn. Res., 6:55-79, 2005.
Priya Goyal, Piotr Dolldr, Ross B. Girshick, Pieter Noordhuis, Lukasz WesoloWski, Aapo Kyrola,
AndreW Tulloch, Yangqing Jia, and Kaiming He. Accurate, large minibatch SGD: training imagenet
in 1 hour. CoRR, abs/1706.02677, 2017. URL http://arxiv.org/abs/1706.02677.
Moritz Hardt, Ben Recht, and Yoram Singer. Train faster, generalize better: Stability of stochastic
gradient descent. In Proceedings of the 33nd International Conference on Machine Learning,
ICML 2016, New York City, NY, USA, June 19-24, 2016, volume 48 of JMLR Workshop and
Conference Proceedings, pp. 1225-1234, 2016. URL http://proceedings.mlr.press/
v48/hardt16.html.
Fengxiang He, Tongliang Liu, and Dacheng Tao. Control batch size and learning rate to generalize
Well: Theoretical and empirical evidence. In Advances in Neural Information Processing Systems
32, pp. 1143-1152. Curran Associates, Inc., 2019.
Elad Hoffer, Itay Hubara, and Daniel Soudry. Train longer, generalize better: closing the generaliza-
tion gap in large batch training of neural netWorks. In Advances in Neural Information Processing
Systems 30: Annual Conference on Neural Information Processing Systems 2017, 4-9 December
2017, Long Beach, CA, USA, pp. 1731-1741, 2017.
StanislaW Jastrzebski, Maciej Szymczak, Stanislav Fort, Devansh Arpit, Jacek Tabor, Kyunghyun
Cho*, and Krzysztof Geras*. The break-even point on optimization trajectories of deep neural
netWorks. In International Conference on Learning Representations, 2020. URL https://
openreview.net/forum?id=r1g87C4KwB.
Nitish Shirish Keskar, Dheevatsa Mudigere, Jorge Nocedal, Mikhail Smelyanskiy, and Ping Tak Peter
Tang. On large-batch training for deep learning: Generalization gap and sharp minima. In 5th
International Conference on Learning Representations, ICLR 2017, Toulon, France, April 24-26,
2017, Conference Track Proceedings, 2017. URL https://openreview.net/forum?id=
H1oyRlYgg.
Alex Krizhevsky. Learning multiple layers of features from tiny images. Technical report, 2009.
Ilja Kuzborskij and Christoph H. Lampert. Data-dependent stability of stochastic gradient descent.
In Proceedings of the 35th International Conference on Machine Learning, ICML 2018, Stock-
holmsmdssan, Stockholm, Sweden, July 10-15, 2018, volume 80 of Proceedings of Machine
Learning Research, pp. 2820-2829. PMLR, 2018.
Yann LeCun and Corinna Cortes. MNIST handWritten digit database. 2010. URL http://yann.
lecun.com/exdb/mnist/.
Yann LeCun, Yoshua Bengio, and Geoffrey E. Hinton. Deep learning. Nature, 521(7553):436-444,
2015. doi: 10.1038/nature14539. URL https://doi.org/10.1038/nature14539.
Qianli Liao, Brando Miranda, Andrzej Banburski, Jack Hidary, and Tomaso A. Poggio. A surprising
linear relationship predicts test performance in deep netWorks. CoRR, abs/1807.09659, 2018. URL
http://arxiv.org/abs/1807.09659.
Tongliang Liu, Gdbor Lugosi, Gergely Neu, and Dacheng Tao. Algorithmic stability and hypothesis
complexity. In Proceedings of the 34th International Conference on Machine Learning, ICML
2017, Sydney, NSW, Australia, 6-11 August 2017, volume 70 of Proceedings of Machine Learning
Research, pp. 2159-2167. PMLR, 2017.
10
Under review as a conference paper at ICLR 2022
Ben London. A pac-bayesian analysis of randomized learning with application to stochastic gradient
descent. In Advances in Neural Information Processing Systems 30: Annual Conference on Neural
Information Processing Systems 2017, 4-9 December 2017, Long Beach, CA, USA, pp. 2931-2940,
2017.
Kaifeng Lyu and Jian Li. Gradient descent maximizes the margin of homogeneous neural networks.
CoRR, abs/1906.05890, 2019. URL http://arxiv.org/abs/1906.05890.
Mor Shpigel Nacson, Nathan Srebro, and Daniel Soudry. Stochastic gradient descent on separable
data: Exact convergence with a fixed learning rate. In The 22nd International Conference on
Artificial Intelligence and Statistics, AISTATS 2019, 16-18 April 2019, Naha, Okinawa, Japan,
volume 89 of Proceedings of Machine Learning Research, pp. 3051-3059. PMLR, 2019.
Vaishnavh Nagarajan and J. Zico Kolter. Uniform convergence may be unable to explain gener-
alization in deep learning. In Advances in Neural Information Processing Systems 32: Annual
Conference on Neural Information Processing Systems 2019, NeurIPS 2019, 8-14 December 2019,
Vancouver, BC, Canada, pp. 11611-11622, 2019.
Behnam Neyshabur, Ryota Tomioka, and Nathan Srebro. Norm-based capacity control in neural
networks. In Proceedings of The 28th Conference on Learning Theory, COLT 2015, Paris, France,
July 3-6, 2015, pp. 1376-1401, 2015. URL http://proceedings.mlr.press/v40/
Neyshabur15.html.
Tomaso A. Poggio, Andrzej Banburski, and Qianli Liao. Theoretical issues in deep networks:
Approximation, optimization and generalization. CoRR, abs/1908.09375, 2019. URL http:
//arxiv.org/abs/1908.09375.
Samuel L. Smith and Quoc V. Le. A bayesian perspective on generalization and stochastic gradient
descent. In 6th International Conference on Learning Representations, ICLR 2018, Vancouver,
BC, Canada, April 30 - May 3, 2018, Conference Track Proceedings, 2018. URL https:
//openreview.net/forum?id=BJij4yg0Z.
Daniel Soudry, Elad Hoffer, Mor Shpigel Nacson, and Nathan Srebro. The implicit bias of gradient
descent on separable data. In 6th International Conference on Learning Representations, ICLR
2018, Vancouver, BC, Canada, April 30 - May 3, 2018, Conference Track Proceedings, 2018. URL
https://openreview.net/forum?id=r1q7n9gAb.
Zhuoning Yuan, Yan Yan, Rong Jin, and Tianbao Yang. Stagewise training accelerates convergence
of testing error over SGD. In Advances in Neural Information Processing Systems 32: Annual
Conference on Neural Information Processing Systems 2019, NeurIPS 2019, 8-14 December 2019,
Vancouver, BC, Canada, pp. 2604-2614, 2019.
Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals. Understanding
deep learning requires rethinking generalization. In 5th International Conference on Learning
Representations, ICLR 2017, Toulon, France, April 24-26, 2017, Conference Track Proceedings,
2017. URL https://openreview.net/forum?id=Sy8gdB9xx.
Y. Zhou, Y. Liang, and H Zhang. Understanding generalization error of sgd in nonconvex optimization.
Machine Learning, 2021. URL https://doi.org/10.1007/s10994-021-06056-w.
A Appendix: Proofs for the convex case
In Hardt et al. (2016), uniform stability with respect to the same loss the algorithm is executed on is
considered. This is a natural choice, however if we are interested in the 0 - 1 loss, different set of
parameters w, w0 can represent equivalent classifiers (that is, predict the same label for any input).
This is the case for logistic regression since any rescaling of the parameters yields the same classifier
(but they can have different training losses). This is also the case for homogeneous neural networks
where we can rescale each layer without affecting the classifier. This is why we consider stability
with respect to normalized losses instead. Note that we are still considering SGD executed on the
original loss l (we do not change the algorithm A). The intuitive idea is to measure stability in terms
of angles (more precisely, we consider distances between normalized vectors) instead of standard
11
Under review as a conference paper at ICLR 2022
euclidean distances (see Figure 1). The proofs in Hardt et al. (2016) consist in bounding E||wt - w01|,
where Wt represents the weights at iteration t when training on S and Wt represents the weights at
w	w	2"，	w0
iteration t when training on the modified training set S(i). We will instead bound E||Uwtjy - ɪɪ^ ||
(or E[d(f, g)] for an appropriate measure of “distance” d between neural networks f and g).
Lemma 2 Let v,w ∈ Rn and
0 < c ≤ min{||v||, ||w||}. Then,
11 _V_"_ 11	Uniu
U百-TMH ||≤
Proof: The proof follows from basic linear algebra manipulations. We give it here for completeness
since it is important in what follows. We need to show that
IV W V w ∖ hv {v-w,v-w)
h TK - TM , TM - TMi ≤ C ∙
After some manipulations, one can see that this is equivalent to show that
||v||2 + ||w||2-2c2 + 2(c2 -||v||||w||)h向,π¾〉≥ 0∙
From Cauchy-Schwarz inequality,〈小,FWJ〉≤ 1. Since c2 - ||v||||w|摩 0, the proof will be
completed by showing that
||v||2 + ||w||2-2c2 +2(c2 - ||v||||w||) ≥ 0.
But this is true since
||v||2+||w||2-2||v||||w||= (||v||-||w||)2.
Lemma 3 Assume that the initial point wo satisfies || W0 "≥ K and that SGD is run with batch size
B and a sequence of learning rates λt on an L-Lipschitz loss function l(w, z) for all z. Then, for all
t ≥ 1,
||wt1^ K - L Pt=0 λi.
Proof:
1 B
||wt|| = ||wt-i - λt-ιB ED(Wt-ι,zj)||
j=ι
1 B
≥ ||wt-1||-%-1 B ||Y?^l(Wt-1, Zj 川
j=i
≥ ||wt-i||-At-iL
≥ ||wt-2||-λt-2L — λt-1L
≥ …
t-1
≥ ||wo||-LXλi
i=0
t-1
≥ K - L X λi.
i=0
For ease of comparison, we give the statement of Theorem 3.8 in Hardt et al. (2016).
Theorem 7 (Theorem 3.8 in Hardt et al. (2016)) Assume that the loss function f (∙; z) is β-smooth,
convex and L-Lipschitzfor every Z. Suppose that we run SGD with step sizes at ≤ 2∕β for T steps.
Then, SGD satisfies uniform stability with
12
Under review as a conference paper at ICLR 2022
uni ≤
2L2 XX
n
t=1
αt.
We are now ready to prove Theorem 1.
Proof of Theorem 1: The proof is similar to Hardt et al. (2016). Let wt denotes the output of
A after t steps on training set S and wt0 be the output of A after t steps on training set S(i) for
some i ∈ {1, ∙∙∙ n}. From convexity, the update rule is 1-expansive (See lemma 3.7 from Hardt
et al. (2016)). This property can be used when the example i is not being picked at some iteration.
Otherwise, the triangular inequality is used. Since the probability of picking the example i in a
mini-batch of size B is smaller than Bn (sampling with replacement) and exploiting Lemma 2 and
Lemma 3, we get
E
wt+1
llwt+1 ||
w0+1
iιw+∣ι
≤ KKE ||wt+1 - wt+1||
≤ ɪ n (Eιιwt—Wt ιι+2Lλ)+(1—n )Eιιwt - Wt ||
1	0	2BLλt
=k^(EM -wt||+	n )
Note that this is true since EιιWt - Wtt ιι≤ EιιWt - Wttιι+2Lλt. Solving the recursion for EιιWt - Wtt ιι,
we have
EM- wtιι≤ 2BL Pi-0 %.
Therefore,
E ∣∣今
—
wt+1
TH+^
∣∣ ≤ 2BKL Pi"
The result then follows from the inequality
∣l(α』,z) - l(α』,z)∣≤ Lɑ∣∣号-小∣∣.
TTwTT ,	TTw0TT ,	TTwTT	TTw0TT
We finally prove Theorem 3.
Proof of Theorem 3: To simplify the text, write e(α, δ) := EALα(A-(S)) + Catab + (2n^αtab +
Ma) ∖∣'n1*. For i ≥ 1, let αi = 2(1-i)C and δi = 袅.For any fixed i, we have
PS{EAL0D-1(A(S)) > C(αi,δi)} < δi.
Therefore,
PS{∀i, EAL0D-1(A(S)) ≤ C(αi,δi)}
= 1 - PS{∃i, EAL0D-1(A(S)) > C(αi,δi)}
∞
≥ 1 - X PS{EAL0D-1(A(S)) > C(αi, δi)}
i=1
∞
≥ 1 - X δi ≥ 1 - δ.
i=1
The last inequality follows from
∞
X	δi
i=1
δ∞1
2	i2
2 i=1 i
δ∏2 ≤ δ.
2 6 一
We want to show that the set
13
Under review as a conference paper at ICLR 2022
{S : ∀i, EALDrI(A(S)) ≤ e(αi,δi)}
is contained in the set
{S ： ∀α ∈ (0,C], EALD-I (A(S ))≤_______________________
EA max (L，(A(S)), La(A(S))) + 端如 + (2n嚼说 + ”。)“皿3(2+够 CnOg2 喇+1畸0}.
Let S be such that ∀i, EALD0-1 (A(S)) ≤ e(αi,6).Let α ∈ (0, C]. Then, there exists i such that
αi ≤ α ≤ 2以.We have
EALD-I(A(S)) ≤ EALai(A(S)) + OIb + (2<‰ + Ma)ʌ/ln^
≤ EALai(A(S)) + 3ab + (2n 琮tab + Ma)Jln2^
≤ EALai(A(S)) + EStab + (2n琛tab + Ma)
∕2in(√2(2 + log2 C - log2 a)) + ln(1∕δ)
V	2n
The second inequality is true since both Eatab and Ma are non-decreasing functions of α and ai ≤ α.
The last inequality is true since / =等 ≤ 2(2+log2 C-1og2 a^. Finally, the proof is concluded by
using the convexity of La(A(S)) with respect to α. Indeed, since 2 ≤ α ≤ α, we must have
Lai(A(S)) ≤ max(La/2(A(S)),La(A(S))).
B Appendix： Proofs for THE NON-CONVEX CASE
Proof of Lemma 1: The proof is done by induction on the number of layers l. Suppose the result is
true for l — 1 layers. Then we have,
... ..~ ~ * ...
USl-SOll= aι∖∖W,∣σ(SI-I)- W∣σ(Sl-I)Il
..~ ~ , , ~ ., , . ....
=αι∖∖Wισ(si-i) - Wισ(si-i) - Wι (σ(si-i) - σ(si-i))∖∖
..~ ~ . . .. .. ~ .,,. ....
≤ αl∖∖Wlσ(sl-I)- WI σ(sl-1)∖∖+αl∖∖WI。(SI-I)- σ(SI-I)) ∖∖
≤ αι∖∖Wι - W(I∖∖∖σ(Si-i)∖∖+αι∖∖W0∖∖∖∖σ(Sl-i) - σ(Si-i)∖∖
..~ ~ ......... .. .
≤ αi∖∖Wi - Wi ∖∖∖∖si-i∖∖bci-i + αiLCl-1 ∖∖si-i - si-i∖∖
i-i
≤ Rai^Wi - W0∖∖ Y BCj αj + αl Lcι-1 ∖∖si-i - Si-i ∖∖
j=i
l	l-i	l-i	l-i
≤ R ∏ %∖∖Wi - Wi0∖∖	Y	BCj +aLcι-ι	R ∏ %	X
j=i	j=i	j=i	i=i
=RY%χWi-Wi,∖∖	∏ (BcjIif j
j=i	i=i L	j = i,j=i、	3
~ ~ ...
∖∖Wi-W]∖
∖ ∏ (BCj	if j
j=i,j=i' LCjTif j
<i
>i
< i
> i
The proof is finally concluded by observing that for one layer we have, ∖∖s1 - Si∖∖≤ Rαi∖∖Wi - Wj ∖∖.
Definition 7 Let us introduce some notations. Let δ(j)(S, z) := ∖∖W7-,t —叫,t∖∖ and ∆(j)(S, z):=
EA[δ(j)(S, z) ∖∀k, δ(k)(S, z) = 0]. Here, W7-,t is obtained when training with S for t iterations
and W,t is obtained when training with S(i)for t iterations. The condition inside the expectation
is that after to iterations, the two networks are still exactly the same. Since we are interested in
14
Under review as a conference paper at ICLR 2022
distances after normalization, we consider Ej)(S, z) := || ∣∣Wj,t∣∣ 一 ∣∣Wj,t∣∣ || and ∆j)(S, Z):=
EABj) (S, z) l∀k, δ(k)(S, z) = 0]. WeWillfUrtherneed δ(j)(S, z) :=，t( ：(S,z) and ∆j)(S, z):=
t	t0	t	Kt(j)(S,z)	t
Ea[C∙(S, z)TFj)(S,z) ∣∀k, δ(k)(S,z) = 0], where Ky)(S,z) := min{∣∣Wj,t∣∣, ||叼/} and
Cj (S,z) :=	max	Kj)(S,z).
t0≤t≤TT Kj)ι(S,z)
Before proving Theorem 4, we establish a lemma. Note that the structure of the proof of the following
Lemma and of Theorem 4 is similar to the corresponding results in Hardt et al. (2016) and in
Kuzborskij & Lampert (2018).
Lemma 4 SUppose that the loss fUnction l(s, y) is L-Lipschitz for all y, non-negative and that
lα(f,z) is bounded above by Ma. Also, assume that ∣∣x∣∣≤ R. Furthermore, let B denote the batch
size and T the number of iterations SGD is being run. Then, for any to ∈ {0,1,2,..., Bn }, the
on-average stability satisfies
l
曦 ≤ LRa XτjEs,z EaBT,)(S,z) ∣∀k, δ(k)(S,z) =0] + M.(B0).
j=1	n
Proof: Write the quantity ∣lα(f,z) 一 lα(g, z)| as the sum of ∣la(f,z) 一 lα(g, z)|I {∀k, δ(k)(S,z)=
0} and ∣lα(f,z) - la (g,z)∣I {∃k : δ(k)(S,z) = 0}. We bound the first term by using the fact that
l
∣lα(f,z) - la(g,z)∣≤ LRald(f,g) = LRal XTj¾P(S, z).
j=1
For the second term, we use that lα(f, z) is bounded above by Mα and non-negative to write
∣la(f,z)-la(g,z)∣≤ Ma.
The result then follows from the fact that the probability of picking example i in t0 iterations is
smaller than Bt0.
n
Proof of theorem 4: From Lemma 2, We always have J(j)(S, z) ≤ $j) (S, z). Therefore, from the
previous Lemma,
l
曦 ≤ LRal X τjEs,z ∆ j)(S,z) + Ma(—).
j=1	n
First note that under our definitions,
壮ι(S,z)
≤ Cj (S,z)
.Ky)(S,z)
δj)(S,z)+ λt∣∣Vj)LBt (Wt)-Vj)LB0 (W；)||
||Vj)LBt (Wt)-Vj)LB0 (W)||
15
Under review as a conference paper at ICLR 2022
Therefore,
Cj (S,z)T-(t+1) δ(+)ι (S,z) ≤ Cj(S,z)τT δ(j)(S, Z)
+ λtCj (S, z)T-t
||Vj)LBt (Wt)-Vj)LB0 (Wf)II
Here, Bt denotes the batch of samples at iteration t when training on S and Bt0 denotes the batch
of samples at iteration t when training on S(i). When Bt = Bt0, we will use {βj}lj=1-layerwise
smoothness to bound IIV(j)LBt(Wt) - V(j)LB0 (Wt0)II. Otherwise, we use simply the triangular
inequality. Let p(B, n) be the probability of picking the example i in a mini-batch of size B (this is
smaller than B). For t ≥ to, We have
∆(+1(S,z) ≤ (1 - p(Β,n))(1 + βjλt)∆tj)(S, Z) + p(B,n) ∆(j)(S,z) +
λtEA[Cj(S,z)T-t
IMj)LBt (Wt)∣∣
Kjj(S⑸
+ Cj (S, Z)T -t
I∣Vj)LB0 (W)II ]ʌ
K(j)(S,z)	7
Define ∆(j) := ES Z∆(j)(S, Z) and
ζ(j) := Ea,s,zCj(S,z)T-t 1j¾≡
sides of the previous inequality, We get
for any t. Taking the expectation over S and Z on both
∆(j)ι ≤ (1 — p(B,n))(1+ βjλt)∆(j) + p(B,n)(∆(jj +2λtZ(jj).
This is true since
EA,S,ZCjSz)TT 1"K)(L)BS(Wt)1| = EA,S,ZCj(S,z)TT |"K(LBS(Wt)||. RearrangingtermSand
using 1 + x ≤ exp(x), We get
∆ t+ι ≤ [1 + (1 — p(B, n))βj λt]∆(j) + 2p(B, n)λtZj
≤ exp((1 — p(B,n))βj λt )∆j) + 2p(B, n)λtZ(j).
Developing the recursion yields
TT1
TT1
∆T) ≤ X 2p(B,n)λtZyj Y exp((1- p(B,n))
t=t0
丝 XMtj)
n
t=t0
丝 XMtj)
n
t=t0
2BcTT1ζt(j)
k=t+1
cβ
k
T T1 1
exp((1-p(B,n))cβj E k
k=t+1 k
exp (1 — p(B, n))cβj log(
T — 1	(1Tp(B,n))cβj
T—1
≤
≤
t
)
≤
t
≤
t=t0
2BcTT1ζt(j) T—1 (1Tp(B,n))cβ
t=t0
≤
2Bc
--- max
n t0 ≤t≤T T1
TT1
{ζ(j)}(T — 1)(1Tp(B,n))cβ X
(1Tp(B,n))cβT1
≤
2Bc
—二-------~~τττ;	max
nc(1 — p(B, n))β t0 ≤t≤T T1
t=t0
(1Tp(B,n))cβ
{Z"( T-4 )
t0 — 1
≤
2B
T—1
(n — B )β t0 — 1
cβ
max {ζt(j)}
t0 ≤t≤T T1 t
16
Under review as a conference paper at ICLR 2022
Therefore, aαv is upper bounded by
inf
toE{1,2,…,Bn }
一 2BLRαl ( T - 1 ∖cβ XX
一 (n - B)β(一)	M Tj
m max 1{ζ(j)} + Mα( ~t0 )].
t0 ≤t≤T -1	n
To complete the proof, we will use thatmaxt0≤t≤T -1{ζt(j)} ≤ PtT=-t1 ζt(j) and reverse the sum order.
With the definition ζt := Plj=1 τjζt(j), we then have
aαv ≤ inf
一to∈{i,2,…,n}
2BLRal ( T -
(n — B)β t0 —
1 cβ T -1
1
t=t0
Ma( Bt0 )1 .
Proof of Theorem 5: The beginning of the proof is the same as the proof of Theorem 4. However, in
the case where smoothness is not assume (for example, ReLU neural networks), it is not possible
to exploit the property of layer-wise smoothness. Instead, only the triangular inequality is used to
bound ||▽⑶LBt (Wt) - ▽⑶Lb0 (Wtz)∣∣. This leads to the inequality
∆(+)1 ≤ ∆(j) + 2λtZ(j).
Solving the recursion then yields
∆乎 ≤ 2 P=I λtZj.
As a consequence,
T -1	Bt
eaV，≤	inf	2LRa1 ɪ2 λtζt + Ma-Q
t0∈{1,2,...,B L	士0	n
where ζt = Plj=1 τjζt(j), concluding the proof.
C Appendix: More Experiments
% JOJU 山
6000
5500
5000
4500
4000
3500
3000
2500
Generalization bound versus epoch (label noise)
Figure 5: The generalization bound from Theorem 6 (α = 1.0) when training with different amounts
of label noise. The network is a 6-layer fully connected network trained on MNIST with SGD. The
final test accuracies are: 98.56% (label noise ratio = 0.0), 96.06% (label noise ratio = 0.1), 91.28%
(label noise ratio = 0.2) and 84.22% (label noise ratio = 0.3).
20	40	60	80	100
Epoch
17
Under review as a conference paper at ICLR 2022
Figure 6: ζt(S) when training with different amounts of label noise. The network is a 6-layer fully
connected network trained on MNIST with SGD.
Figure 7: The generalization bound from theorem 6 (α = 1.0 and α = 0.9) is estimated when
training with Adam and SGD. The network is a 6-layer fully connected network trained on MNIST
with 10% of label noise. For Adam the test accuracy is 95.38% and for SGD, the test accuracy is
96.06%. Those are the best test accuracies that could be obtained after tuning the learning rate for
each respective algorithm.
18