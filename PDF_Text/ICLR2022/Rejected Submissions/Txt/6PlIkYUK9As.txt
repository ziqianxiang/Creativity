Under review as a conference paper at ICLR 2022
Less is more: Selecting informative and di-
VERSE SUBSETS WITH BALANCING CONSTRAINTS
Anonymous authors
Paper under double-blind review
Ab stract
Deep learning has yielded extraordinary results in vision and natural language
processing, but this achievement comes at a cost. Most models require enormous
resources during training, both in terms of computation and in human labeling effort.
We show that we can identify informative and diverse subsets of data that lead to
deep learning models with similar performance as the ones trained with the original
dataset. Prior methods have exploited diversity and uncertainty in submodular
objective functions for choosing subsets. In addition to these measures, we show
that balancing constraints on predicted class labels and decision boundaries are
beneficial. We propose a novel formulation of these constraints using matroids,
an algebraic structure that generalizes linear independence in vector spaces, and
present an efficient greedy algorithm with constant approximation guarantees. We
outperform competing baselines on standard classification datasets such as CIFAR-
10, CIFAR-100, ImageNet, as well as long-tailed datasets such as CIFAR-100-LT.
1 Introduction
Deep learning has shown unprecedented success in many domains, such as speech (Hinton et al.,
2012), computer vision (Krizhevsky et al., 2012; Szegedy et al., 2015; He et al., 2016), and natural
language processing (Sutskever et al., 2014; Devlin et al., 2018). This success generally relies on
access to significant computational resources and large human-annotated datasets. For example,
energy consumption and carbon footprint of developing common NLP models are shown to be
comparable to the lifetime emissions of a car (Strubell et al., 2019). Similarly, human annotation is
also a time-consuming and expensive process; Badrinarayanan et al. (2010) reports that semantic
labeling of a single video frame takes around 45-60 minutes.
In this paper we ask the following question:
Given a large unlabeled dataset along with a
small labeled (seed) dataset and an annotation
budget, how do we select a subset of the un-
labeled dataset, which, when annotated, will
achieve the best performance? While this is ad-
dressed by many classical active learning meth-
ods (Settles, 2009), in this work we focus on the
modern deep learning setting with CNNs and
very deep networks such as ResNets (He et al.,
2016). Typical active learning methods employ
an iterative process where a single image is la-
beled and used to update the model at each step.
The updated model is then used to pick the next
image and so on. Such an approach is not fea-
sible for deep networks. We therefore choose
to study this problem in a batch setting (Sener
& Savarese, 2018; Zhdanov, 2019; Shui et al.,
2020; Kim et al., 2021; Ghorbani et al., 2021).
In this setting, we first train an initial model
using the labeled seed dataset. We typically use
Figure 1: We show the accuracy of ResNet56 mod-
els trained on subsets of different sizes, selected using
our method on CIFAR-10, CIFAR-100, ImageNet, and
CIFAR-100-LT. Models trained using subsets with 30-
40% less data, achieve similar accuracy to the ones
trained using the full dataset.
1
Under review as a conference paper at ICLR 2022
a randomly selected 10% subset of the full dataset as seed. After this, as done in the standard
batch-mode active learning setup, our algorithm uses this initial seed model to select a subset of the
full dataset for which the labels will be revealed. We then train a new model with the selected subset.
As shown in Fig. 1, our subset selection methods can identify subsets with 30% to 40% less data,
which achieve performance similar to what we could get training with the full annotated dataset.
Most existing methods for data subset selection use some notion of margin, representativeness, or
diversity. For example, the classical margin-based sampling selects images for which the prediction of
the class labels are most uncertain. These could, for example, be the images having smallest difference
between the highest and the second highest label probabilities (Lewis & Gale, 1994). Clustering-based
objectives such as k-center (Sener & Savarese, 2018) or k-medoids (Kaufman & Rousseeuw, 1987)
select cluster centers with rich representativeness to model the entire dataset. Diverse subsets are
identified by casting the problem as the maximization of submodular functions (Wei et al., 2015),
which are discrete analogues of convex functions.
We see two main limitations with existing methods. First, there is no single subset selection criterion
that achieves the best performance on different classification datasets. For example we observe in
our experiments (Sec. 6) that in certain datasets, diversity works better than margin, and vice versa.
The second limitation is particularly relevant to margin-based methods where the selection is driven
by the closeness of an image to a decision boundary. In a classification problem with L labels, we
have L2 possible decision boundaries. Most margin-based methods greedily select images close
to decision boundaries. This often results in a selection of too many images representing a small
number of classes and boundaries, while ignoring many others.
To address the first limitation, we develop a unified algorithm based on maximization of a submodular
function, which can combine different selection objectives. Importantly, we do this without sacrificing
constant approximation guarantees. To address the second limitation, we incorporate class-balancing
and boundary-balancing constraints in our optimization using intersection of matroids. Typical
diversity promoting objective functions use an underlying nearest neighbor graph that models the
distances between the training samples. In this work, We present an improved diversity objective
function that models the simultaneous interaction of three nearby samples, while the standard
approach typically considers the interaction of two nearby samples.
To summarize, our contributions are: (1) We develop a unified algorithm to combine different objective
functions along with class-balancing and boundary-balancing constraints using the intersection of
matroids. (2) We propose an improved diversity objective function based on 3-clique interactions
in the nearest neighbor graph. (3) We outperform baseline methods on three standard classification
datasets (CIFAR-10, CIFAR-100, and ImageNet) and a long-tailed classification dataset (CIFAR-100-
LT).
2	Related Work
Data summarization and submodularity: Submodularity (Nemhauser et al., 1978), often seen as
the discrete analogue of convexity, can be used to model subset selection objectives associated with
diversity and representativeness. While subset selection problems are mostly NP-hard, submodular
optimization algorithms are efficient and come with constant approximation guarantees (Carbonell
& Goldstein, 1998; Simon et al., 2007; Krause & Golovin; Golovin & Krause, 2010; Wei et al.,
2015; Lin & Bilmes, 2011; Kaushal et al., 2019; Kim et al., 2016; Prasad et al., 2014; Golovin &
Krause, 2010; 2011; Joseph et al., 2019; Zhou & Spanos, 2016; Qian et al., 2017; Das & Kempe,
2018; Elhamifar, 2019; Kothawade et al., 2021; Kaushal et al., 2021).
A submodular function that promotes representativeness and diversity was utilized in document
summarization (Lin & Bilmes, 2011). Discriminant point process (DPP) are probabilistic models
capable of selecting diverse subsets. They were used for minibatch selection in Mariet et al. (2020),
and the maximum a posteriori (MAP) inference for DPP is submodular (Chen et al., 2018).
Uncertainty sampling and active learning: Active learning methods primarily focus on reducing
the label annotation costs by selecting images to label that are more likely to yield the best model
(See Settles (2009) for a detailed survey). Theoretical guarantees are usually about how quickly the
version space, that maintains a set of good candidate classifiers, shrinks based on the availability of
new data (Dasgupta et al., 2008). Uncertainty sampling selects the most challenging or uncertain
2
Under review as a conference paper at ICLR 2022
images first, in the hope that this will eliminate the need to label other, easier ones. In a classification
setting, predictions from the initial model can provide class probabilities that can help identify images
with high uncertainty. Possibilities include simple uncertainty measures from the best and second best
class probabilities (Lewis & Gale, 1994; Scheffer et al., 2001), entropy measure (Holub et al., 2008;
Joshi et al., 2009), and geometric distance to the decision boundary (Tong & Koller, 2002; Brinker,
2003), selection via proxies (Coleman et al., 2020), and query by committee (Gilad-Bachrach et al.,
2005; Seung et al., 1992), where a committee of models are used to identify images where they most
disagree.
Coresets: A coreset is a small set of points that approximates the shape of a larger point set. In an
ML setting it is a small subset of images from a larger dataset such that the model trained on the
coreset is competitive with the one learned from the entire dataset. It is shown in (Sener & Savarese,
2018; Wolf, 2011) that the average loss over any given subset of the dataset and the remaining points
can be bounded, and that the minimization of this bound can be mapped to the k-center problem.
Clustering: Other classical techniques for subset selection include clustering algorithms such as
k-medoids (Kaufman & Rousseeuw, 1987; Park & Jun, 2009), which minimizes the sum of dissimi-
larities between images belonging to a cluster and a point designated as its center. Agglomerative
clustering was used to select subsets in Birodkar et al. (2019), where it was shown that at least 10%
of images are redundant in ImageNet and CIFAR-10 training. In this work, we show that at least
30-40% of the images in CIFAR-10, CIFAR-100 and ImageNet are redundant. In Ash et al. (2020),
k-MEANS++ algorithm is used to find a set of diverse and uncertain samples using the gradients of
the final layer of the network.
In the context of interpretability, prototypes are subsets that best represent the entire dataset where a
learned model performs the best, and criticisms are the images where a learned model does not do
well. Both prototypes and criticisms can be modeled using maximum-mean discrepancy (MMD), that
measures the difference between two distributions. The resulting optimization to compute the subsets
can be cast as submodular maximization (Kim et al., 2016). Minimization of the MMD between
the entire dataset and the selected subset can be seen as empirical risk minimization, and such an
approach was used for batch mode active learning (Wang & Ye, 2015). Other classical techniques for
prototype selection includes k-medoids (Bien & Tibshirani, 2011) clustering methods.
3	Problem Statement
We consider a classification setting with L classes and n training images U = {x1, . . . , xn}. Let
G = {1, . . . , n} denote the indices in the dataset. Let ψ : X → RL be a (trained) neural network
that maps an input image to a probability distribution over the L classes. The model ψ can be seen
as the composition of an embedding function φ : X → RD that maps an input to an D-dimensional
embedding, and a discriminator function h : RD → RL that maps the embedding to output probability
predictions.
Subset selection and model evaluation. We assume that we are given a small, annotated subset of
U, which we refer to as the seed dataset (this is 10% of U in our experiments). We use this seed
dataset to train an initial model ψseed . Our goal is to use this initial model to select a subset S of
G, subject to a given annotation budget, such that when the images in S are annotated we can get
the best classification performance training a new model with them. To this end, using the seed
model, we generate embeddings and predictions for all the input images in U . Next, we construct
a neighborhood graph (G, E) where E denotes the weighted set of edges associated with nearby
training images in the embedding space RD . Our subset selection algorithm only depends on these
embeddings, predictions, and the nearest-neighbor graph (G, E). As in the standard active learning
settings, we do not use the groundtruth labels in U while selecting subsets.
For modeling subset selection, we employ a submodular set function f : 2G → R that provides a
utility value for every subset of G using notions of uncertainty and diversity. We show that the class
and boundary balancing constraints can be enforced using intersections of partition matroids. With
this modeling, subset selection is essentially the task of finding the subset with maximum utility
value while satisfying the matroid constraints. This inference problem is combinatorial in the number
of training samples, and the brute-force solution is infeasible. For efficient inference, we rely on
the submodular property of the objective function and the use of matroids for modeling balancing
3
Under review as a conference paper at ICLR 2022
constraints. Such an approach allows us to use an efficient greedy algorithm to maximize the objective
function while satisfying matroid constraints with constant approximation guarantees.
Once the subset S is computed, we request the groundtruth labels for the images in S. We use the
annotated subset US ⊆ U to train a model ψS and evaluate its performance on a common held-out
test set.
4	Background
In Sec. 5 we show that our subset selection objective is both submodular and monotonic. We formally
define these properties of a set function below.
Definition 1. Let Ω be a finite set. A set function F : 2ω → R is Submodular if for all A, B ⊆ Ω
with B ⊆ A and e ∈ Ω \ A, we have F(A ∪ {e}) — F(A) ≤ F(B ∪ {e}) — F(B).
This property is also referred to as diminishing returns since the gain diminishes as we add ele-
ments Nemhauser et al. (1978).
Definition 2. A set function F is monotonically non-decreasing if F(B) ≤ F(A) when B ⊆ A.
In Sec. 5.1 we show how to incorporate class balancing and boundary balancing priors in the subset
selection algorithm. We use algebraic structures called matroids and their intersections. We formally
define them below.
Definition 3. A matroid is an ordered pair M = (Ω, I) consisting ofa finite set Ω and a collection
I ofsubsets of Ω satisfying three conditions. (1) The collection contains the empty set, i.e., 0 ∈ I. (2)
If I ∈ I and I0 ⊆ I, then I0 ∈ I. (3) If I1 and I2 are in I and |I1 | < |I2|, then there is an element e
in I2 \ I1 such that I1 ∪ {e} ∈ I.
The members of I are called the independent sets of M. See Oxley (1992) for other, equivalent
definitions of matroids.
Definition 4. Let Ωι,..., Ωn be a partition ofthe finite set Ω and let kι,...,kn be positive integers.
The ordered pair M = (Ω, I) is a partition matroid if I = {I: I ⊆ Ω,	|I ∩Ω∕ ≤ ki, 1 ≤ i ≤ n}
Definition 5. Given two matroids Mi = (Ω, T∖) and M2 = (Ω, I2) over the ground Set Ω, the
intersection of the matroids is given by Mi ∩ M2 = (Ω, Ii ∩I2).
5	Subset Selection
We propose a unified formulation for uncertainty and diversity using set functions of the form
f : 2G → R, where 2G is the power set of the vertex set G. We show that our unified objective
function is submodular and montonic, and that the balancing constraints could be incorporated as
matroids (formally defined in Sec. 4). This implies that our efficient greedy algorithm comes with
constant approximation guarantees (Nemhauser et al., 1978).
We cast subset selection using a criterion f, as maximizing a corresponding set function under a
budget constraint:
S* = arg max f(S) s,t. |S| = k.	(1)
S⊆G
Here S* is the optimal subset based on the selection criterion, and k is the budget. Our functions con-
sist of a sum of terms, and the individual terms are modeled using the distances between embeddings.
These embeddings are produced by the initial model ψseed, which we train using the annotated seed
dataset.
Our unified objective function is written as the conic combination of several individual functions
based on uncertainty (funcertainty), diversity (fdiversity), and clique function (ftriple):
f(S) = λifuncertainty(S) + λ2fdiversity(S) + λ3ftriple(S),	(2)
where λi ≥ 0, i ∈ {1, . . . , 3}. Each of these constituent functions have simple geometric interpreta-
tions (See Fig. 2), and we provide their analytical expressions below:
4
Under review as a conference paper at ICLR 2022
Uncertainty based sampling. We define the
uncertainty-based utility function funcertainty as:
funcertainty (S) =	u(i),	(3)
i∈S
where u(i) is a utility value for each example
i ∈ G, based on its uncertainty. There are three
popular choices for the utility functions using
uncertainty: least confident, margin (Scheffer
et al., 2001), and entropy. In this paper, we
use margin, which is applicable in a multi-class
setting, and gives preference to “hard to classify”
or “low margin” examples (see Scheffer et al.
(2001) for more details). More specifically:
u(i)=1- P(Y =bb|xi)-P(Y = sb|xi),
(4)
where bb and sb denote the best and the second
best predicted class labels for xi according to
our initial model ψSeed. P(Y = ∙∣xi) denotes
the class probabilities predicted by the same
model for xi .
Diversity. Given a budget on the size of the
subset, the diversity criterion promotes selection
of images that are diverse or spread apart in
the embedding space. We define a set function
fdiversity that evaluates diversity as follows:
Decision boundary (BfC)
Figure 2: A schematic with three classes (A,B,C), 8
training samples, and two decision boundaries for the
label pairs (A,B) and (B,C). fdiversity promotes select-
ing samples that are more spreadout and thereby gives
higher utility for {1, 3, 7} over {2, 3, 4}. funcertainty
prefers samples with high uncertainty or the ones closer
to the decision boundary, and thus gives higher utility for
{2, 3, 4} over {1, 3, 7}. ftriple avoids selecting triplets
with negligible or small area, and thereby gives very low
utility for subsets such as {5, 6, 7} where the samples
all lie on a straight line. The class balancing constraint
would prefer a subset {1, 3, 7}, with samples from three
classes, to a single class set {2, 3, 4}. The boundary-
balancing constraint would prefer {4, 5}, which spans
over two decision boundaries, to {3, 4} lying near a
single decision boundary.
s(i, j).
fdiversity(S) =	unary(i)-γ
i∈S	i,j: (i,j)∈E, i,j∈S
(5)
The unary terms unary(i) capture the utility of
an individual image and the pairwise terms s(i, j) are the similarity between a pair of images from
the selected subset that have an edge (i, j) ∈ E in the nearest neighbor graph (G, E). As discussed
in the Sec. 6, we use the cosine similarity in the embedding space as the similarity measure.
The hyperparameter γ ≥ 0 determines the relative importance of the unary and pairwise terms. The
pairwise term limits redundancy. If two points i and j with high similarity are both selected, we incur
a penalty based on the similarity between them.
To ensure monotonicity of fdiversity, we can use a constant unary term, unary(i)	=
maχl∈G Pj∙∙(i,j)∈E s(l,j).
Lemma 1. The function fdiversity is monotonically non-decreasing and submodular.
The proof is available in the Appendix.
Clique function. Given a nearest neighbor graph, we identify cliques of size three by checking for
edges among the k-nearest neighbors of every sample. Given a set of cliques given by T, we define
ftriple as follows:
ftriple(S) = X α(i) - η	X	t(i, j, k).	(6)
i∈S	i,j,k: (i,j,k)∈T, i,j,k∈S
where α(i) denotes the number of triple cliques involving the sample i, and t(i, j, k) is 1 when the
volume consumed by the embeddings of the triplet {i, j, k} is less than a threshold Tthresh, and 0
otherwise. The volume computation in higher dimensional space involves outer product (Lundholm
& Svensson, 2009), similar to cross-product in 3-dimensional space. For computational efficiency,
we approximate the volume using the area of triangle based on the three side lengths obtained from
the nearest neighbor graph. We can extend the idea of triple cliques to higher orders (Kolmogorov &
5
Under review as a conference paper at ICLR 2022
Zabin, 2004; Kohli et al., 2007; Ramalingam et al., 2017), but this can be computationally expensive
when we consider millions of training samples.
We prove the following lemma in the Appendix.
Lemma 2. The function ftriple is monotonically non-decreasing and submodular.
We note that there are other ways to maximize the diversity function. For example, by replacing
the similarity term s(i, j) with a metric distance function, the diversity criteria can be seen as
the max-sum k diversification problem, for which algorithms exist with constant approximation
guarantees (Borodin et al., 2017).
Any conic combination of submodular functions is submodular (Nemhauser et al., 1978), and thus
f (S) is submodular.
5.1	Balancing constraints
In this section we show how we incorporate balancing constraints in our unified formulation.
Class balancing constraint. Subsets selected based on diversity or margin criteria can lead to severe
class imbalance. Consequently, models trained using these imbalanced subsets can show superior
performance on common classes, and inferior performance on rare classes Cui et al. (2019). A
reweighted loss function can partially address this problem but performance on classes with no, or
very few representatives will still suffer.
To overcome this problem we incorporate explicit class-balancing constraints in our formulation. We
do this using a partition matroid constraint, a choice that allows us to maintain our approximation
guarantees. Consider a partition {G1, G2, . . . , GL} of the full dataset G. Here L denotes the number
of classes and Gi consists of the images belonging to class i, where pseudo-labels are computed using
the initial (seed) model. Let k1,. . . ,kL be user-defined positive integers specifying the sizes of the
desired subsets. We choose subsets that are independent sets of the partition matroid M = (G, Ip):
Ic = {I : I ⊆ G, |I ∩ Gi| ≤ ki, 1 ≤ i ≤ L}.	(7)
Boundary balancing constraint. The initial model ψI that we trained using the seed dataset, defines
a set of decision boundaries for every pair of classes. Formally, the decision boundary for a pair of
classes {a, b} is given by:
D{a,b} =∆ {x; P(Y = a|x) = P(Y = b|x)},	(8)
where P (y|x) are the posterior probabilities from the seed model. In other words, these are the set of
points where the posterior class probabilities for two different classes agree. Decision boundaries
can be defined in input space, output space, or in the space defined by the intermediate layers in the
network Elsayed et al. (2018). Since we are only interested in associating an image to its closest
decision boundary, our formulation does not require computation of the exact distance to the boundary.
Instead we simply use predicted class probabilities.
Figure 3: Histogram of the number of images associated with the class boundaries in CIFAR-10, CIFAR-100,
ImageNet, and CIFAR-100-LT datasets.
We define the margin score of an image xi (given predictions from the initial model) as
m(xi) = 1 - P(Y = bb|xi) - P(Y = sb|xi),	(9)
6
Under review as a conference paper at ICLR 2022
where bb and sb correspond to best and the second best class labels, respectively. We associate xi
with the boundary (bb, sb) if m(x) > τ, for some threshold τ.
We observe that the number of class boundaries grows quadratically with the number of classes and
conjecture that many datasets with a large number of classes do not contain any images close to many
decision boundaries. Our goal is to design a subset selection algorithm that promotes selection of
images near as many decision boundaries as possible.
To quantify this observation, we use predictions from our initial model for ImageNet to associate
any image in the dataset which has a margin score greater than τ = 0.05 to the corresponding class
boundary. Since ImageNet has 1000 classes, we have 10200 boundaries. We found that 15% of all
possible boundaries were covered.
Let D = {b(x); x ∈ U} denote the set of all the decision boundaries associated with all the images in
the training set U, and b(x) is a function that associates a given image to a certain decision boundary
or to the empty set 0, based on the margin score. In Fig. 3, We show the histogram of the number
of images associated with different decision boundaries. We note that it is possible to associate an
image to multiple decision boundaries, and defer exploring this option to future work.
Similar to the class-balancing problem, we consider a partition {Gb1 , Gb2 , . . . , GbM } of the full
dataset G, where Gbi denotes the set of images belonging to the decision boundary bi ∈ D.
Let d1 ,. . . ,dM be user-defined positive integers specifying the maximum number of images near
different boundaries. Let us consider the following partition matroid M = (G, Id):
Zd = {I : I ⊆ G,	|I ∩ Gbi I ≤ di, 1 ≤ i ≤ M}	(10)
In order to impose both class-balancing and boundary-balancing constraints, our selected subset
should be the independent set of both the partition matroids given by (G, Ic) and (G, Id).
Note that the intersection of two matroids is not a matroid in general. However, Fisher et al.
(1978) shows that applying the greedy algorithm to a submodular monotonic objective with matroid
intersection constraints maintains approximation guarantees.
5.2	Greedy Algorithm
Subset selection using class- and boundary-balancing constraints is given by:
S* = argmaxf(S) s.t. ∣S∣ = k,S ∈L,S ∈Id,	(11)
S⊆G
where f is our unified objective. The greedy algorithm for selecting a subset of size k is given in
Algorithm 1.
Algorithm 1: Greedy algorithm
1.	Initialize S = 0;
2.	Let s = arg maxs0 ∈G f(S ∪ {s0}) - f(S) such that S ∪ {s0} ∈ Ic ∩ Id;
3.	If s 6= 0 and |S| < k then S = S ∪ {s}, go to 2.;
4.	S is the required solution;
Given that f is monotonic and submodular, a theorem from Fisher et al. (1978) implies that we have
a constant approximation guarantee for the greedy algorithm according to the bound f (Sgreedy) ≥
p++1 f (SOPT), where f (0) = 0 and P is the number of matroid constraints. Since we have two
matroids, we have f (Sgreedy) ≥ 1 f(SOpτ).
5.3	Computational complexity
The greedy algorithm can be efficiently implemented using a priority queue. Given a k-nearest
graph on the embeddings and the uncertainty estimates, the proposed algorithm has a complexity of
O(|V | + nk2log2(|V |)), where V is the set of all samples and n is the size of the subset. Without
the triple cliques its complexity is O(|V | + nklog(|V |)), which is the same as that of k-center. For
implementation reference, we provide pseudo-code in Algorithm 2. In experiments, we choose
k = 10, and the additional overhead from triple-clique constraint is minimal.
7
Under review as a conference paper at ICLR 2022
Algorithm 2: Pseudocode for Subset Selection.
1.	Input: k-NN graph G = (V, E).
2.	Initialize an empty priority queue pq .
3.	For all V ∈ V do Pq — ADD(v, w(v)), where the weight w(v) is based on unary SUCh as
uncertainty.
4.	Initialize S = {c}, where (c, w) = POP(pq).
5.	While |S| < k and NOT-EMPTY(pq) do:
•	6. (c,w) - POP(pq).
•	7. If S ∪ C satisfies balancing constraints, S — S ∪ c.
•	8. For all n1 ∈ NEIGHBOR(c) and n1 ∈ S, do Pq — UPDATE(n1 ,w(n1) - γs(n1 ,c)).
-9. For all n ∈ NEIGHBOR(c) and n ∈ S, do
Pq — UPDATE(n1, w(n1) — γs(n1, c) — ηt(c, n1, n2)).
9. Output S .
(b)
(a)
(c)
(d)
Figure 4: All the Top-1 accuracy numbers are computed by averaging three trials. For the ImageNet, the error
bars are only shown for our algorithm SUBMOD-BAL with one standard deviation.
submod-bal
random
margin
submod
k-center
submod-bal
random
margin
submod
k-center
submod-bal
random
margin
submod
k-center
submod-bal
random
margin
submod
k-center
6 Experiments
Datasets. We use four datasets: CIFAR10 (Krizhevsky et al., 2009), CIFAR100 (Krizhevsky et al.,
2009), ImageNet (Russakovsky et al., 2014), and CIFAR100-LT (Menon et al., 2020). The last
one is a long-tailed dataset where the head class is 100 times more frequent than the least-frequent
class (Menon et al., 2020). We do not apply any long-tail techniques as we want to compare the
performance of vanilla ERM across different methods.
Initial model. For each of the datasets we select a random seed subset of size 10% and use it to train
a ResNet-56 (He et al., 2016) initial model. We use the initial model to generate predictions and
8
Under review as a conference paper at ICLR 2022
embeddings for all the images in the dataset. The embeddings are 64-dimensional for CIFAR and
2048-dimensional for Imagenet. We construct the neighborhood graph (G, E) by finding k = 10
nearest neighbors of each image in embedding space, using the fast similarity search of Guo et al.
(2020). We evaluated different similarity measures such as cosine similarity, L1, and L2 distances,
and found no significant differences. All the reported experiments use cosine similarities.
Hyperparameters. For the CIFAR10, CIFAR100, and CIFAR100-LT experiments we used 450
epochs and the learning rate is divided by 10 at the following epochs:15, 200, 300, and 400. The base
learning rate was set to 1.0. For ImageNet we used 90 epochs and the learning rate is divided by 10
at the following epochs: 5, 30, 60, and 80. The base learning rate was set to 0.1. In all the datasets,
we used SGD with momentum 0.9 (with nestrov) for training. When trained with the full training set,
the top-1 accuracies for CIFAR-10, CIFAR-100, ImageNet, and CIFAR-100-LT are 93.04%,71.37%,
76.39%, and 39.01%, respectively.
Evaluation. We show four baselines: RANDOM, MARGIN, SUBMOD, and k-CENTER (Sener &
Savarese, 2018). submod baseline is the function in Equation 2 without the triple clique term and
the balancing constraints. The parameters (k1, . . . , kL) in the class-balancing partition matroid are
set based on the desired subset size. In other words, ki sets an upper limit on the number of images
from a specific class. In our experiments, We set this value to be 竿,where n, η, and L denote
the number of images, fraction of the training set, and the number of labels, respectively. In the
case of boundary balancing matroid, we set the parameters di to be max(1, ηni), where ni denotes
the number of images associated with the decision boundary bi ∈ D. For every subset, we train
three models and averaged performance is shown in Fig 4. For our algorithm submod-bal we use
λ1 = 0.7, λ2 = 0.30, and λ3 = 1 in Equation 2. The baselines RANDOM and MARGIN do not have
any hyperparameters. For k-CENTER and SUBMOD, we use the same nearest neighbor graph with the
same number of neighbors.
We outperform other methods marginally in standard datasets and significantly in the long-tailed
dataset. The k-CENTER (Sener & Savarese, 2018) performs close to the proposed method and
outperforms standard submdoular algorithm. Note that on all these datasets, accuracies with either
80% or 90% subsets, or both, exceed that with the full dataset. This is not surprising since completely
eliminating some examples in the training set has shown to be beneficial in prior work (Kubat &
Matwin, 1997; Wallace et al., 2011).
Ablation study: The balancing constraints show an average improvement of 0.61% in CIFAR
datasets, and 2.83% in CIFAR100-LT. The clique function shows an improvement of 0.49% in CIFAR
and 1.13% in CIFAR100-LT dataset. We evaluated the role of k, the number of nearest neighbors in
k-nearest neighbor graph construction. While we used k = 10 in all our experiments the marginal
benefit in using k = 100 and k = 1000 are given by +0.15% and +0.25%, respectively.
7 Discussion
We propose a triple-clique diversity objective and balancing constraints on class labels and decision
boundaries. We showed that the incorporation of balancing constraints usually leads to better subsets
compared to standard baselines, and the impact is highly significant in training with long-tailed
datasets such as CIFAR100-LT. We also show that some popular image classification datasets have at
least 30-40% redundant images.
On standard datasets, the difference between our method and k-center is marginal, and this can be
due to a deeper connection. The k-center algorithm can be reduced to a series of dominating set
problems (Vazirani, 2001). Dominating set can be reduced to set cover, which is also a submodular
problem. While k-center has connection to submodularity, our proposed framework does not
generalize k-center. It will be beneficial to find a principled way to combine these methods.
9
Under review as a conference paper at ICLR 2022
Ethical Statement
Training large models are resource intensive and labeling data sets is also extremely labor-intensive,
requiring significant human effort. For example, detailed semantic segmentation for a single image
can take up to 60 minutes. In this work, we develop subset selection methods that can lead to accurate
models with less computations and data. This will eventually lead to reduced human labeling efforts,
and less utilization of computational resources.
Reproducibility S tatement
The basic algorithm to find the subset is a standard greedy algorithm and it is easy to implement.
We also provided the pseudocode in Algorithm 2. This paper is only about subset computation and
the training code is not changed. All the hyperparameters are listed in experiment section. We will
release the code, the seed model, and the nearest neighbor graph for other researchers to compare
their models.
References
J. T. Ash, C. Zhang, A. Krishnamurthy, J. Langford, and A. Agarwal. Deep batch active learning by
diverse, uncertain gradient lower bounds. In ICLR, 2020. 3
V. Badrinarayanan, F. Galasso, and R. Cipolla. Label propagation in video sequences. In CVPR,
2010. 1
J.	Bien and R. Tibshirani. Prototype selection for interpretable classification. The Annals of Applied
Statistics, 2011. 3
V. Birodkar, H. Mobahi, and S. Bengio. Semantic redundancies in image-classification datasets: The
10% you don’t need, 2019. 3
A. Borodin, A. Jain, H. C. Lee, and Y. Ye. Max-sum diversification, monotone submodular functions,
and dynamic updates. ACM Trans. Algorithms, 2017. 6
K.	Brinker. Incorporating diversity in active learning with support vector machines. In ICML, 2003. 3
J. Carbonell and J. Goldstein. The use of mmr, diversity-based reranking for reordering documents
and producing summaries. SIGIR ’98, 1998. 2
L.	Chen, G. Zhang, and H.Zhou. Fast greedy map inference for determinantal point process to
improve recommendation diversity. In NeurIPS, 2018. 2
C. Coleman, C. Yeh, S. Mussmann, B. Mirzasoleiman, P. Bailis, P. Liang, J. Leskovec, and M. Zaharia.
Selection via proxy: Efficient data selection for deep learning, 2020. 3
Y. Cui, M. Jia, T. Y. Lin, Y. Song, and S. Belongie. Class-balanced loss based on effective number of
samples. In CVPR, 2019. 6
Abhimanyu Das and David Kempe. Approximate submodularity and its applications: Subset selection,
sparse approximation and dictionary selection. Journal of Machine Learning Research, 19(3):
1-34, 2018. URL http://jmlr.org/papers/v19/16-534.html. 2
S. Dasgupta, D. J. Hsu, and C. Monteleoni. A general agnostic active learning algorithm. In NeurIPS,
2008. 2
J Devlin, M. W. Chang, K. Lee, and K. Toutanova. BERT: pre-training of deep bidirectional
transformers for language understanding. CoRR, 2018. 1
E. Elhamifar. Sequential facility location: Approximate submodularity and greedy algorithm. In
ICML, 2019. 2
G. F. Elsayed, D. Krishnan, H. Mobahi, K. Regan, and S. Bengio. Large margin deep networks for
classification. In NIPS, 2018. 6
10
Under review as a conference paper at ICLR 2022
M.L. Fisher, G.L. Nemhauser, and L.A. Wolsey. An analysis of approximations for maximizing
submodular set functions-ii. In Polyhedral Combinatorics. Mathematical Programming Studies,
1978. 7
Amirata Ghorbani, James Zou, and Andre Esteva. Data shapley valuation for efficient batch active
learning, 2021. 1
R. Gilad-Bachrach, A. Navot, and N. Tishby. Query by committee made real. In NIPS, 2005. 3
D. Golovin and A. Krause. Adaptive submodularity: Theory and applications in active learning and
stochastic optimization. J. Artif. Int. Res., 2011. 2
Daniel Golovin and Andreas Krause. Adaptive submodularity: Theory and applications in active
learning and stochastic optimization, 2010. 2
R. Guo, P. Sun, E. Lindgren, Q. Geng, D. Simcha, F. Chern, and S. Kumar. Accelerating large-scale
inference with anisotropic vector quantization. In ICML, 2020. URL https://arxiv.org/
abs/1908.10396. 9
K.	He, X. Zhang, S. Ren, and J. Sun. Deep residual learning for image recognition. In CVPR, 2016.
1,8
G. Hinton, L. Deng, G.E. Dahl, A. Mohamed, N. Jaitly, A. Senior, V. Vanhoucke, P. Nguyen,
T. Sainath, and B. Kingsbury. Deep neural networks for acoustic modeling in speech recognition.
IEEE Signal Processing Magazine, 2012. 1
A. Holub, P. Perona, and M. C. Burl. Entropy-based active learning for object recognition. In CVPR
Workshop, 2008. 3
K J Joseph, Vamshi Teja R, Krishnakant Singh, and Vineeth N Balasubramanian. Submodular batch
selection for training deep neural networks, 2019. 2
A. J. Joshi, F. Porikli, and N. Papanikolopoulos. Multi-class active learning for image classification.
In CVPR, 2009. 3
L.	Kaufman and P. J. Rousseeuw. Clustering by means of medoids, 1987. 2, 3
V. Kaushal, R. Iyer, S. Kothawade, R. Mahadev, K. Doctor, and G Ramakrishnan. Learning from less
data: A unified data subset selection and active learning framework for computer vision, 2019. 2
V. Kaushal, S. Kothawade, G. Ramakrishnan, J. A. Bilmes, and R. K. Iyer. PRISM: A unified
framework of parameterized submodular information measures for targeted data subset selection
and summarization. CoRR, abs/2103.00128, 2021. 2
B. Kim, R. Khanna, and O. Koyejo. Examples are not enough, learn to criticize! criticism for
interpretability. In NIPS, 2016. 2, 3
Kwan-Young Kim, Dongwon Park, Kwang In Kim, and Se Young Chun. Task-aware variational
adversarial active learning. In CVPR, 2021. 1
P. Kohli, M. P. Kumar, and P. H. S. Torr. P3 beyond: Solving energies with higher order cliques. In
2007 IEEE Conference on Computer Vision and Pattern Recognition, 2007. 6
V. Kolmogorov and R. Zabin. What energy functions can be minimized via graph cuts? IEEE
Transactions on Pattern Analysis and Machine Intelligence, 26(2):147-159, 2004. doi: 10.1109/
TPAMI.2004.1262177. 5
S. Kothawade, N. Beck, and K. Killamsetty R. K. Iyer. SIMILAR: submodular information measures
based active learning in realistic scenarios. CoRR, abs/2107.00717, 2021. 2
Andreas Krause and Daniel Golovin. Submodular function maximization. 2
A. Krizhevsky, I. Sutskever, and G. E. Hinton. Imagenet classification with deep convolutional neural
networks. In NeurIPS, 2012. 1
11
Under review as a conference paper at ICLR 2022
Alex Krizhevsky et al. Learning multiple layers of features from tiny images. 2009. 8
Miroslav Kubat and Stan Matwin. Addressing the curse of imbalanced training sets: One-sided
selection. In Proceedings of the International Conference on Machine Learning (ICML), 1997. 9
D. D. Lewis and W. A. Gale. A sequential algorithm for training text classifiers. In SIGIR, 1994. 2, 3
H. Lin and J. Bilmes. A class of submodular functions for document summarization. In ACL, 2011. 2
Douglas Lundholm and Lars Svensson. Clifford algebra, geometric algebra, and applications, 2009.
5
Z. Mariet, J. Robinson, J. Smith, S. Sra, and S. Jegelka. Optimal batch variance with second-order
marginals. In ICML Workshop, 2020. 2
Aditya Krishna Menon, Sadeep Jayasumana, Ankit Singh Rawat, Himanshu Jain, Andreas Veit, and
Sanjiv Kumar. Long-tail learning via logit adjustment, 2020. 8
G. L. Nemhauser, L. A. Wolsey, and M. L. Fisher. An analysis of the approximations for maximizing
submodular set functions. Mathematical Programming, 1978. 2, 4, 6
James Oxley. Matroid Theory. Oxford University Press, 1992. 4
H.S. Park and C.H. Jun. A simple and fast algorithm for k-medoids clustering. Expert Systems with
Applications, 2009. 3
A. Prasad, S. Jegelka, and D. Batra. Submodular meets structured: Finding diverse subsets in
exponentially-large structured item sets. In NIPS, 2014. 2
Chao Qian, Jing-Cheng Shi, Yang Yu, Ke Tang, and Zhi-Hua Zhou. Subset selection under noise.
In I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett
(eds.), Advances in Neural Information Processing Systems, 2017. 2
S.	Ramalingam, C. Russell, L. Ladicky, and P. H. S. Torr. Efficient minimization of higher order
submodular functions using monotonic boolean functions. Discrete Applied Mathematics, 2017. 6
O. Russakovsky, J. Deng, H. Su, Krause J, S. Satheesh, S. Ma, Z. Huang, A. Karpathy, A. Khosla,
M. Bernstein, A. C. Berg, and L. Fei-Fei. Imagenet large scale visual recognition challenge, 2014.
8
T.	Scheffer, C. Decomain, and S. Wrobel. Active hidden markov models for information extraction.
In Advances in Intelligent Data Analysis, 2001. 3, 5
Ozan Sener and Silvio Savarese. Active learning for convolutional neural networks: A core-set
approach. In ICML, 2018. 1, 2, 3, 9
B. Settles. Active learning literature survey. Technical report, University of Wisconsin-Madison,
2009. 1,2
H.	S. Seung, M. Opper, and H. Sompolinsky. Query by committee. In COLT, 1992. 3
Changjian Shui, Fan Zhou, Christian Gagn’e, and Boyu Wang. Deep active learning: Unified and
principled method for query and training. In AISTATS, 2020. 1
I.	Simon, N. Snavely, and S. M. Seitz. Scene summarization for online image collections. In ICCV,
2007. 2
E. Strubell, A. Ganesh, and A. McCallum. Energy and policy considerations for deep learning in
NLP. CoRR, 2019. 1
I. Sutskever, O. Vinyals, and Q.V. Le. Sequence to sequence learning with neural networks. In
NeurIPS, 2014. 1
C. Szegedy, W. Liu, Y. Jia, P. Sermanet, S. Reed, D. Anguelov, D. Erhan, V. Vanhoucke, and
A. Rabinovich. Going deeper with convolutions. In CVPR, 2015. 1
12
Under review as a conference paper at ICLR 2022
S. Tong and D. Koller. Support vector machine active learning with applications to text classification.
JMLR, 2002. 3
Vijay V. Vazirani. Approximation Algorithms. Springer-Verlag, 2001. 9
B.C. Wallace, K.Small, C.E. Brodley, and T.A. Trikalinos. Class imbalance, redux. In Proc. ICDM,
2011. 9
Z. Wang and J. Ye. Querying discriminative and representative samples for batch mode active
learning. ACM Trans. Knowl. Discov. Data, 2015. 3
K. Wei, R. Iyer, and J. Bilmes. Submodularity in data subset selection and active learning. In ICML,
2015. 2
Gert W. Wolf. Facility location: Concepts, models, algorithms and case studies. series: Contributions
to management science. 2011. 3
Fedor Zhdanov. Diverse mini-batch active learning. CoRR, abs/1901.05954, 2019. 1
Yuxun Zhou and Costas J. Spanos. Causal meets submodular: Subset selection with directed
information. In Proceedings of the 30th International Conference on Neural Information Processing
Systems, NIPS'16,pp. 2657-2665, 2016. 2
A	Appendix
Lemma 1. The function fdiversity is monotonically non-decreasing and submodular.
Proof. The function fdiversity is given by
fdiversity(S) =	max	s(l, j)
i∈S ∈ j1l,j)∈E
-γ	X	s(i, j).	(12)
i,j: (i,j)∈E, i,j∈S
We will show that the function is monotonically non-decreasing and submodular when 0 ≤ γ ≤ 1.
We will first show that the function is monotonically non-decreasing. Consider an element e ∈ G\S:
fdiversity (S ∪ e) - fdiversity (S) = max	s(l, j)-
l∈G *,j)∈E
γ X	s(e, j)	(13)
j: (e,j)∈E, j∈S
In the first term, j ∈ G, whereas in the second j ∈ S . The first term is a maximum over all samples
in G, including l = e. Since λ ≤ 1 and S ⊆ G, we have fdiversity(S ∪ e) - fdiversity(S) ≥ 0 and thus
the function in Eq. 12 is monotonically non-decreasing.
Next, we will show that the function fdiversity (S) is submodular. In order to do this, we show the
following when A, B ⊆ G, B ⊆ A, and e ∈ G\A:
fdiversity (A ∪ e) - fdiversity (A) ≤ fdiversity (B ∪ e) - fdiversity (B)	(14)
We expand the left hand side of the above expression using the definition of fdiversity (S):
fdiversity (A ∪ e) - fdiversity (A) = max	s(l, j)-
l∈G j<l,j)∈E
λ X s(e, j)	(15)
j: (e,j)∈E, j∈A
13
Under review as a conference paper at ICLR 2022
We expand the right hand side as follows:
fdiversity (B ∪ e) - fdiversity (B) = max	s(l, j)-
l∈G *,j)∈E
γ X	s(e, j)	(16)
j: (e,j)∈E, j∈B
Subtracting Eq. 16 from Eq. 15 we have:
(fdiversity (A ∪ e) - fdiversity (A)) -
(fdiversity (B ∪ e) - fdiversity (B)) =
γ X	s(e, j) -γ X	s(e,j)	(17)
j: (e,j)∈E, j∈B	j: (e,j)∈E, j∈A
Since Y ≥ 0, s(.,.) >= 0, and B ⊆ A, we satisfy Eq. 14.	□
Lemma 2. The function ftriple is monotonically non-decreasing and submodular.
Proof. The function ftriple is given by
ftriple(S) = X α(i) - η	X	t(i, j, k).	(18)
i∈S	i,j,k: (i,j,k)∈T , i,j,k∈S
We will show that the function is monotonically non-decreasing and submodular when 0 ≤ η ≤ 1.
We will first show that the function is monotonically non-decreasing. Consider an element e ∈ G\S:
ftriple(S ∪ e) -ftriple(S) = α(e)-
η	X	t(e, j, k)	(19)
e,j,k: (e,j,k)∈T, e,j,k∈S
The first term α(e) is the number of triple cliques associated with e, and the maximum value of the
second term is α(e) since 0 ≤ η ≤ 1. Thus we have ftriple(S ∪ e) - ftriple(S) ≥ 0 and thus the
function in Eq. 18 is monotonically non-decreasing.
Next, we will show that the function ftriple(S) is submodular. In order to do this, we show the
following when A, B ⊆ G, B ⊆ A, and e ∈ G\A:
ftriple(A ∪ e) -ftriple(A) ≤ ftriple(B ∪ e) -ftriple(B)	(20)
We expand the left hand side of the above expression using the definition of ftriple(S):
ftriple (A ∪ e) - ftriple(A) = α(e)-
η	X	t(e, j, k)	(21)
e,j,k: (e,j,k)∈T, e,j,k∈A
We expand the right hand side as follows:
ftriple(B ∪ e) - ftriple (B) = α(e)-
η	X	t(e, j, k)	(22)
e,j,k: (e,j,k)∈T, e,j,k∈B
Subtracting Eq. 22 from Eq. 21 we have:
(ftriple(A ∪ e) - ftriple(A)) -
(ftriple(B ∪ e) - ftriple(B)) =
η	X	t(e,j,k)-	(23)
e,j,k: (e,j,k)∈T, e,j,k∈B
η	X	t(e, j, k)	(24)
e,j,k: (e,j,k)∈T , e,j,k∈A
Since η ≥ 0, t(., ., .) >= 0, and B ⊆ A, we satisfy Eq. 20.
□
14