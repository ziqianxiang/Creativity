Under review as a conference paper at ICLR 2022
FedPAGE: A FAST LOCAL STOCHASTIC GRADIENT
Method for Communication-Efficient Feder-
ated Learning
Anonymous authors
Paper under double-blind review
Ab stract
Federated Averaging (FedAvg, also known as Local-SGD) (McMahan et al., 2017)
is a classical federated learning algorithm in which clients run multiple local
SGD steps before communicating their update to an orchestrating server. We
propose a new federated learning algorithm, FedPAGE, able to further reduce the
communication complexity by utilizing the recent optimal PAGE method (Li et al.,
2021). We show that FedPAGE uses much fewer communication rounds than
previous local methods for both federated convex and nonconvex optimization.
Concretely, 1) in the convex setting, the number of communication rounds of
FedPAGE is O(NS/4), improving the best-known result O(N) of SCAFFOLD
(Karimireddy et al., 2020) by a factor of N1/4, where N is the total number of
clients (usually is very large in federated learning), S is the sampled subset of
clients in each communication round, and E is the target error; 2) in the nonconvex
setting, the number of communication rounds of FedPAGE is O(VN+S), improving
the best-known result O( SN22/a) of SCAFFOLD (Karimireddy et al., 2020) by a
factor of N 1/6S 1/3, if the sampled clients S ≤ √N. Note that in both settings, the
communication cost for each round is the same for both FedPAGE and SCAFFOLD.
As a result, FedPAGE achieves new theoretical state-of-the-art results in terms of
communication complexity for both federated convex and nonconvex optimization.
1	Introduction
With the rise in the proliferation of mobile and edge devices, and their ever-increasing ability to
capture, store and process data, federated learning (KOneCny et al., 2016b; McMahan et al., 2017;
Kairouz et al., 2019) has recently emerged as a new machine paradigm for training machine learning
models over a vast amount of geographically distributed and heterogeneous devices. Federated
learning aims to augment the traditional centralized datacenter focused approach to training machine
learning models (Dean et al., 2012; Iandola et al., 2016; Goyal et al., 2017) with a new decentralized
modality that aims to be more energy-efficient, and mainly, more privacy-conscious with respect to
the private data stored on these devices. In federated learning, the data is stored over a large number
of clients, for example, phones, hospitals, or corporations (Konecny et al., 2016a;b; McMahan et al.,
2017; Mohri et al., 2019). Orchestrated by a centralized trusted entity, these diverse data and compute
resources come together to train a single global model to be deployed on all devices. This is done
without the sensitive and private data ever leaving the devices.
One of the key challenges in federated learning comes from the fact that communication over a
heterogeneous network is extremely slow, which leads to significant slowdowns in training time.
While a centralized model may train in a matter of hours or days, a comparable federated learning
model may require days or weeks for the same task. For this reason, it is imperative that in the design
of federated learning algorithms one focuses special attention on the communication bottleneck, and
designs communication-efficient learning protocols capable of producing a good model.
There are two popular lines of work for tackling this communication-efficient federated learning
problem. The first makes use of general and also bespoke lossy compression operators to compress
the communicated messages (such as local stochastic gradients) before they are sent over the net-
1
Under review as a conference paper at ICLR 2022
work (Mishchenko et al., 2019; Li and Richtdrik, 2020; Li et al., 2020; Gorbunov et al., 2021; Li
and Richtdrik, 2021a), and the second line bets on increasing the local workload by performing
multiple local update steps, e.g., multiple SGD steps, before communicating with the orchestrating
server (Stich, 2020; Woodworth et al., 2020; Gorbunov et al., 2020; Karimireddy et al., 2020).
In this paper, we focus on the latter approach (multiple local update steps in each round) to alleviating
the communication bottleneck in federated learning. One of the earliest and classical methods
proposed in this context is FedAvg/local-SGD (McMahan et al., 2017; Sahu et al., 2018; Yu et al.,
2019; Li et al., 2019; Haddadpour and Mahdavi, 2019; Stich, 2020; Gorbunov et al., 2020). However,
the method has remained a heuristic until recently, even in its simplest form as local gradient descent,
particularly in the important heterogeneous data regime (Khaled et al., 2020; Woodworth et al.,
2020). Further improvements on vanilla Local-SGD have been proposed, leading to methods such
as Local-SVRG (Gorbunov et al., 2020) and SCAFFOLD (Karimireddy et al., 2020). In particular,
Gorbunov et al. (2020) also provide a unified framework for the analysis of many local methods in
the strongly convex and convex settings.
1.1	Our contributions
Although there are many works on local gradient-type methods, the communication complexity in
existing works on local methods is still far from optimal. In this paper, we introduce a novel local
method FedPAGE, significantly improving the best-known results for both federated convex and
nonconvex settings (see Table 1). Now, we summarize our main contributions as follows:
1.	We develop and analyze, FedPAGE, a fast local method for communication-efficient fed-
erated learning. FedPAGE can be loosely seen as a local/federated version of the PAGE
algorithm of Li et al. (2021), which is a recently proposed optimal optimization method for
solving smooth nonconvex problems. Our analysis of FedPAGE also recovers the optimal
results of PAGE (see Theorem 1), thus FedPAGE substantially improves the best-known
non-optimal result of SCAFFOLD (Karimireddy et al., 2020) by a factor of N1/6S1/3 (see
Table 1 for more details).
2.	For the convex setting, we provide the convergence results for FedPAGE in Theorem 2.
Moreover, FedPAGE also improves the best-known result of SCAFFOLD (Karimireddy et al.,
2020) by a large factor of N 1/4 (see Table 1).
3.	Finally, we first conduct the numerical experiments for showing the effectiveness of multiple
local update steps (see Section 6.1). The experiments indeed demonstrate that FedPAGE
with multiple local steps K ≥ 1 is better than that with K = 1 (no multiple local updates).
Then we also conduct experiments for comparing the performance of different local methods
such as FedAvg (McMahan et al., 2017), SCAFFOLD (Karimireddy et al., 2020) and our
FedPAGE (see Section 6.2). The experiments show that FedPAGE always converges much
faster than FedAvg, and at least as fast as SCAFFOLD (usually much better than SCAFFOLD),
confirming the practical superiority of FedPAGE.
1.2	Related works
Optimization algorithms for federated learning have a close relationship with the algorithms designed
for standard finite-sum problem minx § PZi fi(x). In the federated learning setting, we can think
of the loss function of the data on a single client as function fi , and the optimization problem becomes
a finite-sum problem. The SGD is perhaps the most famous algorithm for solving the finite-sum
problem, and in one variant or another, it is widely applied in training deep neural networks. However,
the convergence rates of plain SGD in the convex and nonconvex settings are not optimal. This
motivated a feverish research activity in the optimization and machine learning communities over
the last decade, and these efforts led to theoretically and practically improved variants of SGD, such
as SVRG, SAGA, SARAH, SPIDER, and PAGE (Johnson and Zhang, 2013; Defazio et al., 2014;
Nguyen et al., 2017; Fang et al., 2018; Li et al., 2021) and many of their variants possibly with
acceleration/momentum (Allen-Zhu, 2017; Lan and Zhou, 2018; Lei et al., 2017; Li and Li, 2018;
Zhou et al., 2018; Wang et al., 2018; Kovalev et al., 2020; Ge et al., 2019; Li, 2019; Lan et al., 2019;
Li and Li, 2020; Li, 2021a).
2
Under review as a conference paper at ICLR 2022
Table 1: Number of communication rounds for finding an -solution of federated convex and non-
convex problems (1), where Ef (x) - f * ≤ e for convex setting and Ek▽/(x)k2 ≤ e for nonconvex
setting. In the last column, (G,B)-BGD means that PN=I ∣∣Vfi(χ)k2 ≤ G2 + B2kVf (x)k2. (G, 0)-
BGD means B = 0, and (0, B)-BGD means G = 0. BV denotes the “Bounded Variance” assumption,
i.e., (2), and Smooth stands for standard smoothness assumption, e.g., Assumption 1. Other notations
(i.e., N, S, M, K) are summarized in Table 2.
Algorithm	Convex setting	Nonconvex setting	Assumption
FedAvg (YU et al., 2019)	—	G1 2NK I σ2 -T2	+ NKe4	Smooth, BV, (G, 0)-BGD
FedAvg (Karimireddy et al., 2020)	卫 + ɪ + b2 + σ Se2 + e3∕2 + e + SKe2	乙 + G + b2 + σ Se4 + e3 + e2 + SKe4	Smooth, BV, (G,B)-BGD
FedPrOX (SahU et al., 2018)	B2	—	Smooth, S = N, (0,B)-BGD
VRL-SGD (Liang et al., 2019)	—	N + N.	Smooth, BV, S = N
S-LOcal-SVRG (Gorbunov et al., 2020)	M1∕3∕K1∕3 + √M∕NK2 1	—	Smooth, (BV), S = N, K ≤ M
SCAFFOLD (Karimireddy et al., 2020)	N 1	σ2 亚 + SK2	N2/3 i	σ2 S2∕3e2 + SKe4	Smooth, BV
FedPAGE (this paper)	N3/4 2 ~SΓ~	N1/2+S -ST2-	Smooth, (BV)3
However, the above well-studied finite-sum problem is not equivalent to the federated learning
problem (1) as one needs to account for the communication, which forms the main bottleneck. As
we discussed before, there are at last two sets of ideas for solving this problem: communication
compression, and local computation. There are lots of works belonging to these two categories. In
particular, for the first category, the current state-of-the-art results in strongly convex, convex, and
nonconvex settings are given by Li et al. (2020); Li and RiChtdrik (2021a); Gorbunov et al. (2021),
respectively. For the second category, local methods such as FedAvg (McMahan et al., 2017) and
SCAFFOLD (Karimireddy et al., 2020) perform multiple local update steps in each communication
round in the hope that these are useful to decrease the number of communication rounds needed
to train the model. In this paper, we provide new state-of-the-art results of local methods for both
federated convex and nonconvex settings, which significantly improves the previous best-known
results of SCAFFOLD (Karimireddy et al., 2020) (See Table 1).
2 Setup and Notation
We formalize the problem as minimizing a finite-sum functions:
min [f(X) := N P fi(X)I , where fi(X) := M PM=I fi,j (X)
x∈Rd	i=1
(1)
In this formulation, each function fi(∙) stands for the loss function with respect to the data stored on
Client/device/maChine i, and each function fi,j (∙) stands for the loss function with respect to the j-th
data on client i. Besides, we assume that the minimum of f exists, and we use f* and x* to denote
the minimum of function f and the optimal point respectively.
1We point out that S-Local-SVRG (Gorbunov et al., 2020) only considered the case where S = N and
K ≤ M, i.e., the sampled clients S always is the whole set of clients N for all communication rounds. As a
result, the total communication complexity (i.e., number of rounds × communicated clients S in each round)
of S-Local-SVRG is O((NM1/3/K1/3 + PNMTK2)∕c) (note that here K ≤ M), which is worse than
O(N∕e) of SCAFFOLD (Karimireddy et al., 2020) and O(N3/4/e) of our FedPAGE.
2In the convex setting, we state the result of FedPAGE in the case S ≤ VZN (typical in practice) for simple
presentation, where N is the total number of clients and S is the number of sampled subset clients in each
communication round (see Table 2). Please see Theorem 2 for other results of FedPAGE in the cases S > VzN.
3 FedPAGE also works under the BV assumption by using moderate minibatches for local clients, and more
importantly the number of communication rounds of FedPAGE still remains the same as in the last row of Table
1 for both convex (see Theorem 2) and nonconvex (see Theorem 1) settings.
3
Under review as a conference paper at ICLR 2022
Table 2: Summary of notation used in this paper
N, S,i	total number, sampled number, and index of clients
M	total number of data in each client
R, r	total number and index of communication rounds
K,k	total number and index of local update steps
xr	model parameters before round r
gr	server update within round r
yr,k	i-th client’s model in round r before local step k
gr,®	i-th client’s update in round r within local step k
VI fi(x)	estimator of Vfi(x) using a sampled minibatch I VI fi(x) = 1∕∣I∣Pj∈I Vfij (x)
We will use [n] to denote the set {1,2,..., n}, k ∙ ∣∣ to denote the Euclidean norm for a vector, and
hu, vi to denote the inner product of U and v. We use O(∙) and Ω(∙) to hide the absolute constants.
In this paper, we consider two cases: nonconvex case and convex case. In the nonconvex case, each
individual function fi and the average function f can be nonconvex, and we assume that the functions
{fi,j }i∈[N],j ∈[M] are L-smooth.
Assumption 1 (L-smoothness). All functions fi,j : Rd → R for all i ∈ [N], j ∈ [M] are L-smooth.
That is, there exists L ≥ 0 such that for all x1, x2 ∈ Rd and all i ∈ [N],j ∈ [M],
l∣Vfi,j (x1) -kfi,j (Xz)Il ≤ Lkxi - χ2 k.
If the functions {fi,j }i∈[N],j∈[M] are L-smooth, we can conclude that functions {fi} are also L-
smooth and function f is L-smooth. In this nonconvex setting, the optimization algorithm aims to
find a point such that the expectation of the gradient norm is small enough: EkVf (x) k 2 ≤ e.
Then, in the convex case, each individual function fi can be nonconvex, but we require that the average
function f to be convex. We also assume that the functions {fi,j} are L-smooth (Assumption 1).
Under this convex setting, the algorithm will find a point such that the expectation of the function
value is close to the minimum: Ef (x) 一 f * ≤ e.
If the number of data on a single client is very large and one cannot compute the local full gradients of
clients, one needs the following assumption in which the gradient variance on each client is bounded.
Assumption 2 (Bounded Variance). There exists σ ≥ 0 such that for any client i ∈ [N] andx ∈ Rd,
1M
MM EkVfij(x) -Vfi(x)k2 ≤ σ2.	⑵
M j=1
3	THE FedPAGE ALGORITHM
In this section, we introduce our FedPAGE algorithm. To some extent, our FedPAGE algorithm is
the local version of PAGE (Li et al., 2021): when the clients communicate with the server, FedPAGE
behaves similar to PAGE, and when the clients update the model locally, each client updates several
steps. If we set the number of local updates to one, FedPAGE reduces to the original PAGE algorithm.
Our FedPAGE algorithm is given in Algorithm 1. There are two cases in each round r: 1) with
probability pr (typically very small), the server communicates with all clients in order to get a more
accurate gradient of function f (Line 3-9); 2) with probability 1 一 Pr, the server communicates with
a subset of clients with size S and the local clients perform K local steps (Line 10-24).
For Case 1), the server broadcasts the current model parameters xr to all of the clients. Then,
each client computes the estimator VI1fi(xr) of the gradient Vfi(xr) and sends to the server. The
estimator takes b1 minibatch samples (|I1 | = b1) to estimate the gradient of fi and different clients
sample different sets I1. Here, we want VI1fi(xr) to be as closed to Vfi(xr) as possible, choosing
a moderate size b1 usually is enough. The server average all of the gradient and get the averaged
gradient gr =得 Pi∈[N] Viι fi(xr) and takes a step with global step size n (see Line 9, 26).
4
Under review as a conference paper at ICLR 2022
Algorithm 1 FedPAGE
server input: initial point x0, global step size ηg, probabilities {pr}, sampled clients size S
client i’s input: local step size ηl, minibatch sizes b1, b2, b3
1:	for r = 0, 1, 2, . . . , R do
2:	sample q 〜 BemoUlli(Pr)
3:	if q = 1 then
4:	clients Sr = [N], commUnicate xr to all i ∈ Sr
5:	on client i ∈ Sr in parallel do
6:	Uniformly sample minibatch I1 ⊂ [M] with size b1
7:	compute the gradient estimator gi J Viι 力(Xr)
8:	end on client
9:	grJ N Pi∈[N ] gr
10:	else
11:	sample clients Sr ⊆ [N] with size S, communicate (xr, xr-1, gr-1) to all i ∈ Sr
12:	on client i ∈ Sr in parallel do
13:	yir,0 J xr
14:	uniformly sample minibatch I2 ⊂ [M] with size b2
15:	gir,0JVI2fi(xr)-VI2fi(xr-1)+gr-1
16:	yir,1 J yir,0 - ηlgir,0
17:	for k = 1, 2, . . . , K - 1 do
18:	uniformly sample minibatch I3 ⊂ [M] with size b3
19:	gir,kJVI3fi(yir,k)-VI3fi(yir,k-1)+gir,k-1
20:	yir,k+1 J yir,k - ηlgir,k
21:	end for
22:	∆yir J xr - yir,K
23:	end on client
24:	gr J κη1ιs Pi∈sr∆yr
25:	end if
26:	xr+1 J xr - ηggr
27:	end for
For Case 2), the server first broadcasts (xr , xr-1, gr-1) to the sampled subset clients Sr, and the
clients initialize yir,0 J xr. Here, yir,k is i-th client’s model in round r before local step k, and gir,k
denotes i-th client’s gradient estimator for step k in round r. Then for the first local step of client i,
the local gradient estimator is computed in Line 15 as
gir,0 J VI2fi(xr) - VI2fi(xr-1) + gr-1,
where Vi2 fi(∙) is the gradient estimator of Vfi(∙) with minibatch size b2. Here, We also want
Vi2 fi(∙) to be as closed to Vfi(∙) as possible, similarly choosing a moderate size b2 usually is
enough. This update rule is similar to PAGE (Li et al., 2021) and in particular if the local steps K = 1,
our FedPAGE algorithm reduces to PAGE.
For client i’s local step k such that 1 ≤ k ≤ K - 1, the gradient estimator is computed in Line 19 as
gir,k JVI3fi(yir,k) -VI3fi(yir,k-1)+gir,k-1.
Here I3 is a minibatch of functions with size b3 that we used to compute the gradient estimator gir,k.
Different from the previous gradient estimators using minibatches with size b1 and b2, here we want
b3 to be small enough to reduce the computation cost as there are K local steps (Line 17-21). In
particular, we can choose b3 = 1, i.e., just sample an index j from [M] and the estimator becomes
gir,k J (Vfi,j(yir,k) - Vfi,j(yir,k-1)) + gir,k-1.
The local model update is given by yir,k+1 = yir,k - ηlgir,k where ηl is the local step size. After K
local steps, client i computes the local changes ∆yir = yir,K - xr within round r and sends back to
the server. After receiving the local changes ∆yir for the selected clients i ∈ Sr , the server computes
the average gradient estimator on these selected clients in Line 24 as gr = KIS Pi∈sr ∆yr.
5
Under review as a conference paper at ICLR 2022
After obtaining the gradient estimator gr (in Line 9 or 24), the server updates the model using a
global step size ηg in Line 26 as xr+1 = xr - ηggr.
The intuition of FedPAGE works as follow: when the local step size ηl is not too large, we can expect
that the local model updates are close to the original model, that is yir,k ≈ xr, and the gradient is also
close to each other, Pfij(y" ≈ Pfij(Xr). Then each local gradient estimator gj∖ is close to
gr,k = (Pfij (yi,k) - Pfij (yi,k-ι))+gi,k-ι ≈ Pfij (Xr) - Pfij (Xr)+gr,k-ι = gr,k-ι = gr,0,
and the aggregated global gradient estimator gr is close to
gr ≈ 1 P (yfi(χr) -Pfi(χ^-1) + gr-1).
i∈Sr
This biased recursive gradient estimator gr is similar to the gradient estimator in SARAH (Nguyen
et al., 2017) or PAGE (Li et al., 2021), and thus the performance of FedPAGE in terms of communica-
tion rounds should be similar to the optimal convergence results of PAGE (Li et al., 2021).
4 FedPAGE IN NONCONVEX SETTING
In this section, we provide the general result of our FedPAGE with any local steps K ≥ 1 in the
nonconvex setting. Here we assume that the functions {fi,j} are L-smooth (Assumption 1), and we
obtain the following theorem.
Theorem 1 (Convergence of FedPAGE in nonconvex setting). Under Assumption 1 (and Assumption
2), if we choose the sampling probability Pr ≡ P = N for every r ≥ 1 and po = 1, the minibatch
sizes bi = min{M, 2422}, b? = min{M, pS∣2-}, and the global and local step sizes
ηg ≤
L (ι+q3⅛/^)
ηl ≤
√⅞
24√SKL,
1
then FedPAGE will find a point X such that EkPf (X)k2 ≤ within the following number of
communication rounds:
R = O L( E 2+S)
S2
Now we compare the communication cost of FedPAGE (Theorem 1) with previous state-of-the-art
SCAFFOLD (Karimireddy et al., 2020). The number of communication round for SCAFFOLD to
find a point X such that EkPf (X)k2 ≤ (the original SCAFFOLD (Karimireddy et al., 2020) uses
EkPf (X)k22 ≤ ) is bounded by
RSCAFFOLD = O (N/S)2/3 L/2 .	(3)
Beyond the number of communication rounds in FedPAGE and SCAFFOLD, we also need to compare
the communication cost during each round (i.e., number of clients communicated with the server
in the round). For our FedPAGE, in each round, with probability P = N, the server communicates
with all clients N, and with probability 1 - P, the server communicates with a sampled subset clients
with size S, and the communicated clients within each round is N X N + (1 — N) X S < 2S in
expectation. For SCAFFOLD, in each round, the server communicates with S sampled clients. Thus
the communication cost for each round is the same O(S) for both FedPAGE and SCAFFOLD. As
a result, to compare the communication complexity of FedPAGE and SCAFFOLD, it is equivalent
to compare the number of communication rounds. According to (3) and Theorem 1 (e.g., with
sampled clients S ≤ √N), then the communication rounds of FedPAGE is smaller than previous
state-of-the-art SCAFFOLD by a factor of N1/6S1/3. Also note that the number of clients N is
usually very large in the federated learning problems.
6
Under review as a conference paper at ICLR 2022
5 FedPAGE IN CONVEX SETTING
In this section, we show the convergence results of FedPAGE in the convex setting. Here the algorithms
aim to find a point X such that Ef (x) - f * ≤ E for convex case instead of Ek▽/(χ)k2 ≤ E for
nonconvex case. In this part, we assume that f is convex and the functions {fi,j} satisfy L-smoothness
assumption (Assumption 1).
Theorem 2 (Convergence of FedPAGE in convex setting). Under Assumption 1 (and Assumption 2),
if we choose the sampling probability Pr = P = NS for every r ≥ 1 and po = 1, the minibatch sizes
bi = min{M, pi∕4√^W}, b2 = min{M, /8^W}, and the global and local step size
一θ((S + N3/4)NS ʌ	_O( S ʌ
ng_~ (L(S + √N) ) ,	ηl —	(N5∕4KLc√τ) ,
then FedPAGE satisfies
R-1
R X E[f (xr+1)-f (x*)] ≤ Oo
r=0
(N3/4L
V SR
+E
+E
NL
(SR + E)
,if s ≤ Nn
,if √N < S ≤ N3/4 .
, if N3/4 < S
O
(N1/4L
R~i^
o
As We discussed before, the expected communication cost of FedPAGE is the same as SCAFFOLD in
each communication round. Then if the sampled clients S ≤ NN, FedPAGE can find a solution X
such that Ef (x) - f (x*) ≤ E within O(NNS：L) number of communication rounds, improving the
previous state-of-the-art O(N) of SCAFFOLD (Karimireddy et al., 2020) by a large factor of N1/4.
Recall that N denotes the total number of clients.
6 Numerical Experiments
In this section, we present our numerical experiments. We conducted two experiments: the first
shows the effectiveness of the local steps (Section 6.1), and the second compares FedPAGE with
SCAFFOLD and FedAvg (Section 6.2). Before we present the results of these two experiments, we
first state the general experiment setups. The detailed experiment setups are deferred to the appendix.
Experiment setup We run experiments on two nonconvex problems used in e.g. (Wang et al.,
2θl8; Li and RiCht疝ik, 2021b): robust linear regression and logistic regression with nonconvex
regularizer. The standard datasets a9a (32,561 samples) and w8a (49,749 samples) are downloaded
from LIBSVM (Chang and Lin, 2011). The objective function for robust linear regression is
n
f(X) = n P `(XTai- bi),
i=1
where '(t) = log(1 + t2). Here bi ∈ {±1} is a binary label.
The objective function for logistic regression with nonconvex regularizer is
n	d2
f (x) = n P log (1 + exp(-biXT ai)) + α P Ij.
i=1	j=1 j
Here, the last term is the regularizer term and we set α = 0.1.
Besides, different algorithms have different definitions of the local step size and global step size,
thus we compare these algorithms with the ‘effective step size, η. Here for FedPAGE, the effective
step size is just the global step size η = %, and for SCAFFOLD and FedAvg, the effective step size
is defined as η = Kngη. We run experiments with η = 0.1,0.03,0.01. If the effective step size is
larger, the algorithms may diverge. Also note that although we compare these algorithms with the
same effective step size, FedPAGE can use a larger step size from our theoretical results. Finally
we select the total number of communication rounds such that the algorithms converge or we can
distinguish their performance difference.
7
Under review as a conference paper at ICLR 2022
3250 Clients
TIOlTOlTOIYOI
Cboz*u8-pe∙l0
5000 10000 15000 20000 o 0
Communication Rounds	1-1
325 Clients
IOI TOI ToI
Cbozu8-pe∙l
1000 2000 3000 4000 5000 ɪ 0
Communication Rounds
10 Clients
IOI TOI ToI
Cboz*u8-pe∙l0
1000 2000 3000 4000 5000
Communication Rounds
4800 Clients
OOI IOI TOI ToI
Cboz8-pe∙l
5000 10000 15000 20000 g 0
Communication Rounds
(a) a9a.
480 Clients
OOl TIOI TOI
Cboz8-pe∙l
2000 4000 6000 8000 10000 ɑ 0
Communication Rounds
(b) w8a.
OOI IOI ToI
Cbozt8-pe∙l0
10 Clients
1000 2000 3000 4000 5000
Communication Rounds

L
Figure 1: FedPAGE with different number of local steps on different datasets.
6.1	Effectiveness of local steps
In this experiment we compare the convergence performance of FedPAGE using different number of
local update steps: K = 1, 10, 20 (see Line 17 of Algorithm 1). FedPAGE-1 means that the number
of local step K = 1, which reduces to the original PAGE (Li et al., 2021), and FedPAGE-10 and
FedPAGE-20 represent FedPAGE with 10 and 20 local steps respectively.
The experimental results are presented in Figure 1. Figure 1a shows the robust linear regression results
of FedPAGE using different number of local steps K = 1, 10, 20 on a9a dataset, and Figure 1b
shows the result on w8a dataset.
Local steps speed up the convergence rate The experimental results in Figure 1 show that the
multiple local steps of FedPAGE can speed up the convergence in terms of the communication
rounds. Although there are some fluctuations when the number of communication round is not large
(early-stage), FedPAGE-10 and FedPAGE-20 outperform FedPAGE-1 in the end.
Algorithm with multiple local steps can choose a larger effective step size From our hyperpa-
rameter optimization results, we also find that FedPAGE with multiple local steps can choose a larger
effective step size (ηg in FedPAGE). On a9a dataset, when there are 3250 clients, the effective step
size for FedPAGE-1, FedPAGE-10, and FedPAGE-20 are optimized to be 0.3, 0.4, 0.4 respectively;
when there are 325 clients, the effective step size for FedPAGE-1, FedPAGE-10, and FedPAGE-20
are optimized to be 0.2, 0.4, 0.5; when there are 10 clients, the effective step size for FedPAGE-1,
FedPAGE-10, and FedPAGE-20 are optimized to be 0.3, 0.5, 0.6. The experiments on w8a dataset
also support this finding.
6.2	Comparison with previous methods
Now, we compare our FedPAGE with two other methods: SCAFFOLD (Karimireddy et al., 2020) and
FedAvg (McMahan et al., 2017). The experimental results are presented in Figure 2 and 3. We plot
the gradient norm versus the number of communication rounds. Figures 2a, 2b, 3a, and 3b show the
performance of each algorithm using different objective functions and different datasets.
Performance of different methods The experiments show that FedPAGE ≥ SCAFFOLD > FedAvg.
Among all the cases, under the same effective step size, we find that both FedPAGE and SCAFFOLD
converge faster than FedAvg. FedPAGE converges at least as fast as SCAFFOLD, and in most of the
cases FedPAGE converges faster than SCAFFOLD.
8
Under review as a conference paper at ICLR 2022
Effective LR 0.1	Effective LR □.03	Effective LR Q.Q1
OoI TloI ZloI
Euoz wa-pe」。
O IOOO 2000 3000 4000 5000
Communication Rounds
Euoz wa-pe-l。
TIOI ZiOl
0	1000 2000 3000 4000 5000
Communication Rounds
Ool Tol Tol
Euoz wa-pe-l。
0	5000 10000 15000 20000
Communication Rounds
(a) Robust linear regression on a9a dataset.
Ool Tol Tol
UUoN WB-peJ。
Effective LR 0.1
TloI ZIoI
UlJoNu B-PeJ。
Effective LR 0.03
TIOI ZiOl
UlJoNuPB9
Effective LR 0.01
0 2000 4000 6000 8000 10000	0 2000 4000 6000 8000 10000	0	5000 10000 15000 20000
Communication Rounds	Communication Rounds	Communication Rounds
(b) Robust linear regression on w8a dataset.
Figure 2:	Comparison of different methods with robust linear regression.
Effective LR 0.1	Effective LR 0.03	Effective LR 0.01
TolTolTOI70IY0-Tol
UlJoN-ju B-PeJ。
UlJoN-juiPB9
TloIToISloI 4101
OolT0; OlToliol
UUoN WB-peJ。
0	1000 2000 3000 4000 5000	0	1000 2000 3000 4000 5000	0	1000 2000 3000 4000 5000
Communication Rounds	Communication Rounds	Communication Rounds
(a)	Logistic regression with nonconvex regularizer on a9a dataset.
Effective LR 0.1	Effective LR 0.03	Effective LR 0.01
OoI ZloI y—01
UlJoN-u B-PeJ。
Ool Tol Tol Tol
0	1000 2000 3000 4000 5000	0	2000 4000 6000 8000 10000	0	5000 10000 15000 20000
Communication Rounds	Communication Rounds	Communication Rounds
(b)	Logistic regression with nonconvex regularizer on w8a dataset.
Figure 3:	Comparison of different methods with logistic regression with nonconvex regularizer.
Larger effective step size converges faster The experiments also show that a larger effective step
size leads to a faster convergence as long as the algorithm converges. Note that FedPAGE can use
a larger step size with theoretical guarantee compared with SCAFFOLD, if we choose the same
parameters of the objective function (e.g. the same smoothness constant) and use the step size with
theoretical guarantees, FedPAGE converges faster than SCAFFOLD than FedAvg.
7 Conclusion
In this paper, we propose a new federated learning algorithm, FedPAGE, providing much better
state-of-the-art communication complexity for both federated convex and nonconvex optimization.
We also conduct several numerical experiments showing the effectiveness of multiple local update
steps in FedPAGE and verifying the practical superiority of FedPAGE over other classical methods.
9
Under review as a conference paper at ICLR 2022
References
Zeyuan Allen-Zhu. Katyusha: the first direct acceleration of stochastic gradient methods. In
Proceedings ofthe 49th Annual ACM SIGACT Symposium on Theory of Computing, pages 1200-
1205. ACM, 2017.
Chih-Chung Chang and Chih-Jen Lin. LIBSVM: a library for support vector machines. ACM
transactions on intelligent systems and technology (TIST), 2(3):1-27, 2011.
Jeffrey Dean, Greg S Corrado, Rajat Monga, Kai Chen, Matthieu Devin, Quoc V Le, Mark Z Mao,
Marc’Aurelio Ranzato, Andrew Senior, Paul Tucker, et al. Large scale distributed deep networks.
2012.
Aaron Defazio, Francis Bach, and Simon Lacoste-Julien. SAGA: A fast incremental gradient method
with support for non-strongly convex composite objectives. In Advances in Neural Information
Processing Systems, pages 1646-1654, 2014.
Cong Fang, Chris Junchi Li, Zhouchen Lin, and Tong Zhang. SPIDER: Near-optimal non-convex op-
timization via stochastic path-integrated differential estimator. In Advances in Neural Information
Processing Systems, pages 687-697, 2018.
Rong Ge, Zhize Li, Weiyao Wang, and Xiang Wang. Stabilized SVRG: Simple variance reduction
for nonconvex optimization. In Conference on Learning Theory, pages 1394-1448, 2019.
EdUard Gorbunov, FiliP Hanzely, and Peter Rich电ik. Local SGD: Unified theory and new efficient
methods. arXiv preprint arXiv:2011.02828, 2020.
Eduard Gorbunov, Konstantin Burlachenko, Zhize Li, and Peter Richtdrik. MARINA: Faster non-
convex distributed learning with compression. In International Conference on Machine Learning,
pages 3788-3798. PMLR, arXiv:2102.07845, 2021.
Priya Goyal, Piotr Dolldr, Ross Girshick, Pieter Noordhuis, Lukasz Wesolowski, Aapo Kyrola,
Andrew Tulloch, Yangqing Jia, and Kaiming He. Accurate, large minibatch SGD: Training
ImageNet in 1 hour. arXiv preprint arXiv:1706.02677, 2017.
Farzin Haddadpour and Mehrdad Mahdavi. On the convergence of local descent methods in federated
learning. arXiv preprint arXiv:1910.14425, 2019.
Forrest N Iandola, Matthew W Moskewicz, Khalid Ashraf, and Kurt Keutzer. Firecaffe: near-linear
acceleration of deep neural network training on compute clusters. In Proceedings of the IEEE
Conference on Computer Vision and Pattern Recognition, pages 2592-2600, 2016.
Rie Johnson and Tong Zhang. Accelerating stochastic gradient descent using predictive variance
reduction. In Advances in neural information processing systems, pages 315-323, 2013.
Peter Kairouz, H Brendan McMahan, Brendan Avent, AUrelien Bellet, Mehdi Bennis, Arjun Nitin
Bhagoji, Keith Bonawitz, Zachary Charles, Graham Cormode, Rachel Cummings, et al. Advances
and open problems in federated learning. arXiv preprint arXiv:1912.04977, 2019.
Sai Praneeth Karimireddy, Satyen Kale, Mehryar Mohri, Sashank Reddi, Sebastian Stich, and
Ananda Theertha Suresh. SCAFFOLD: Stochastic controlled averaging for federated learning. In
International Conference on Machine Learning, pages 5132-5143. PMLR, 2020.
Ahmed Khaled, Konstantin Mishchenko, and Peter Richtdrik. Tighter theory for local SGD on
identical and heterogeneous data. In International Conference on Artificial Intelligence and
Statistics, pages 4519-4529. PMLR, 2020.
Jakub Konecny, H Brendan McMahan, Daniel Ramage, and Peter Richtdrik. Federated optimization:
Distributed machine learning for on-device intelligence. arXiv preprint arXiv:1610.02527, 2016a.
Jakub Konecny, H Brendan McMahan, Felix X Yu, Peter Richtdrik, Ananda Theertha Suresh, and
Dave Bacon. Federated learning: Strategies for improving communication efficiency. arXiv
preprint arXiv:1610.05492, 2016b.
10
Under review as a conference paper at ICLR 2022
Dmitry Kovalev, Samuel Horvdth, and Peter Richtdrik. Don't jump through hoops and remove
those loops: SVRG and Katyusha are better without the outer loop. In Proceedings of the 31st
International Conference on Algorithmic Learning Theory, 2020.
Guanghui Lan and Yi Zhou. Random gradient extrapolation for distributed and stochastic optimization.
SIAMJournaIon Optimization, 28(4):2753-2782, 2018.
Guanghui Lan, Zhize Li, and Yi Zhou. A unified variance-reduced accelerated gradient method for
convex optimization. In Advances in Neural Information Processing Systems, pages 10462-10472,
2019.
Lihua Lei, Cheng Ju, Jianbo Chen, and Michael I Jordan. Non-convex finite-sum optimization via
SCSG methods. In Advances in Neural Information Processing Systems, pages 2345-2355, 2017.
Xiang Li, Kaixuan Huang, Wenhao Yang, Shusen Wang, and Zhihua Zhang. On the convergence of
FedAvg on non-iid data. arXiv preprint arXiv:1907.02189, 2019.
Zhize Li. SSRGD: Simple stochastic recursive gradient descent for escaping saddle points. In
Advances in Neural Information Processing Systems, pages 1521-1531, arXiv:1904.09265, 2019.
Zhize Li. ANITA: An optimal loopless accelerated variance-reduced gradient method. arXiv preprint
arXiv:2103.11333, 2021a.
Zhize Li. A short note of PAGE: Optimal convergence rates for nonconvex optimization. arXiv
preprint arXiv:2106.09663, 2021b.
Zhize Li and Jian Li. A simple proximal stochastic gradient method for nonsmooth noncon-
vex optimization. In Advances in Neural Information Processing Systems, pages 5569-5579,
arXiv:1802.04477, 2018.
Zhize Li and Jian Li. A fast Anderson-Chebyshev acceleration for nonlinear optimization. In
International Conference on Artificial Intelligence and Statistics, pages 1047-1057. PMLR,
arXiv:1809.02341, 2020.
Zhize Li and Peter Richtdrik. A unified analysis of stochastic gradient methods for nonconvex
federated optimization. arXiv preprint arXiv:2006.07013, 2020.
Zhize Li and Peter Richtdrik. CANITA: Faster rates for distributed convex optimization with
communication compression. arXiv preprint arXiv:2107.09461, 2021a.
Zhize Li and Peter Richtdrik. ZeroSARAH: Efficient nonconvex finite-sum optimization with zero
full gradient computation. arXiv preprint arXiv:2103.01447, 2021b.
Zhize Li, Dmitry Kovalev, Xun Qian, and Peter Richtdrik. Acceleration for compressed gradient
descent in distributed and federated optimization. In International Conference on Machine
Learning, pages 5895-5904. PMLR, arXiv:2002.11364, 2020.
Zhize Li, Hongyan Bao, Xiangliang Zhang, and Peter Richtdrik. PAGE: A simple and optimal
probabilistic gradient estimator for nonconvex optimization. In International Conference on
Machine Learning, pages 6286-6295. PMLR, arXiv:2008.10898, 2021.
Xianfeng Liang, Shuheng Shen, Jingchang Liu, Zhen Pan, Enhong Chen, and Yifei Cheng. Variance
reduced local SGD with lower communication complexity. arXiv preprint arXiv:1912.12844,
2019.
H Brendan McMahan, Eider Moore, Daniel Ramage, Seth Hampson, and Blaise Aguera y Areas.
Communication-efficient learning of deep networks from decentralized data. In International
Conference on Artificial Intelligence and Statistics, pages 1273-1282, 2017.
Konstantin Mishchenko, Eduard Gorbunov, Martin Takdc, and Peter Richtdrik. Distributed learning
with compressed gradient differences. arXiv preprint arXiv:1901.09269, 2019.
11
Under review as a conference paper at ICLR 2022
Mehryar Mohri, Gary Sivek, and Ananda Theertha Suresh. Agnostic federated learning. In Interna-
tional Conference on Machine Learning, pages 4615-4625. PMLR, 2019.
Lam M Nguyen, Jie Liu, Katya Scheinberg, and Martin Takac. SARAH: A novel method for machine
learning problems using stochastic recursive gradient. In International Conference on Machine
Learning, pages 2613-2621, 2017.
Anit Kumar Sahu, Tian Li, Maziar Sanjabi, Manzil Zaheer, Ameet Talwalkar, and Virginia Smith.
On the convergence of federated optimization in heterogeneous networks. arXiv preprint
arXiv:1812.06127, 3, 2018.
Sebastian U. Stich. Local SGD converges fast and communicates little. In International Conference
on Learning Representations, 2020.
Zhe Wang, Kaiyi Ji, Yi Zhou, Yingbin Liang, and Vahid Tarokh. SpiderBoost and momentum: Faster
stochastic variance reduction algorithms. arXiv preprint arXiv:1810.10690, 2018.
Blake Woodworth, Kumar Kshitij Patel, Sebastian U. Stich, Zhen Dai, Brian Bullins, H. Brendan
McMahan, Ohad Shamir, and Nathan Srebro. Is local SGD better than minibatch SGD? arXiv
preprint arXiv:2002.07839, 2020.
Hao Yu, Sen Yang, and Shenghuo Zhu. Parallel restarted SGD with faster convergence and less
communication: Demystifying why model averaging works for deep learning. In Proceedings of
the AAAI Conference on Artificial Intelligence, volume 33, pages 5693-5700, 2019.
Dongruo Zhou, Pan Xu, and Quanquan Gu. Stochastic nested variance reduction for nonconvex
optimization. In Advances in Neural Information Processing Systems, pages 3925-3936, 2018.
12
Under review as a conference paper at ICLR 2022
A More Experiments
In this section we describe the detailed experiment setting for Section 6 and present more numerical
experiments. We perform two different experiments: the first is to compare the performance of
different algorithm with different number of clients and data on a single client (Section A.2), and
the second is to compare different algorithm with local full gradient computations, which shows the
limitation of different algorithms (Section A.3).
A.1 Detailed experiment setting in Section 6
Detailed experiment setting in Section 6.1 We use the robust linear regression as the objective
function. We run experiment on the a9a dataset in which the total number of data samples is 32500
(here we drop the last 61 samples for easy implementation of different number of clients). We choose
the number of clients to be 3250, 325, 10, and the numbers of data on a single client are 10, 100,
and 3250, respectively. When the number of clients is 3250, we choose S = 10, i.e. the server
communicates with 10 clients in each communication round, and when the number of clients is
325 or 10, we set S = 1. For all settings, we optimize the global step size ηg and choose the local
step size ηl heuristically such that the algorithms converge as fast as possible. For FedPAGE-1 (or
PAGE), the local step size does not matter and choosing the optimal global step size achieves its best
convergence rate, however for FedPAGE-10 and FedPAGE-20, choosing ηg , ηl with some heuristics
does not guarantee the best performance. We also perform the similar experiments on another dataset
w8a.
Detailed experiment setting in Section 6.2 For the experiments with a9a dataset, we omit the
last 61 samples and set the number of clients to be 3250, and for experiments with w8a, we omit
the last 1749 samples and there are 4800 clients in total. Here we omit the samples because it is
more convenient to change the number of clients. We let each ‘client’ contains 10 samples from the
datasets. For SCAFFOLD and FedAvg, in each communication round, the server will communicate
with 20 clients (S = 20 in their algorithms). For FedPAGE, we set S = 10 because FedPAGE
will communicate with all clients with probability NS and the expected communication for all three
algorithms in each round are almost the same. We choose the local steps of all these three methods
to be 10. For FedPAGE, we choose the minibatch size b3 = 1 and for SCAFFOLD and FedAvg,
we choose the minibatch size that estimate the local full gradient to be 4. In this way, the local
computations are nearly the same for all methods.
A.2 Comparison of different methods with different number of clients
A.2.1 Experiment setup
In previous Section 6, we compare different methods with a large number of clients (on a9a dataset,
there are 3250 clients, and on w8a dataset, there are 4800 clients). In this experiment, we vary the
number of clients and compare the performance of FedPAGE, SCAFFOLD, and FedAvg.
For the number of clients, we choose the number of clients to be 325 and 10, and the number of data
on a single client are 100 and 3250. We choose the number of local steps to be 10 for all three methods.
When the number of clients are 325 and 10, we set S = 1 for FedPAGE and S = 2 for SCAFFOLD
and FedAvg, making the communication cost in each round to be nearly the same. We set FedPAGE
to compute the full local gradient for the first local step, and choose only one sample to estimate
the gradient for the following local steps. For SCAFFOLD and FedAvg, we set the minibatch size
estimating the local full gradient to be 22 when the number of client is 325 and 652 when the number
of client is 10. When the number of client is 325, there are 100 data on a single client. FedPAGE
need to compute two full gradient at the beginning of each local computations, costing 200 number
of gradient computations. Then it needs to compute two gradient (the gradient of a same sample at
different points), and it cost about 220 gradient computations in total. Choosing the minibatch size to
be 22 in SCAFFOLD and FedAvg makes the local computations nearly the same, because SCAFFOLD
and FedAvg use the same minibatch size in every local step. When the number of client is 10, the
minibatch size for SCAFFOLD and FedAvg can be computed as 3250 × 2/10 + 2 = 652. This makes
the local computations of these three algorithms to be nearly the same.
13
Under review as a conference paper at ICLR 2022
For the step sizes, we choose the effective step sizes to be 0.1, 0.03, and 0.01.
A.2.2 Experiment results
The experimental results are presented in Figure 4. Figure 4b and 4c shows the experiment results
with 325 clients and 10 clients on a9a dataset. We also include Figure 4a (i.e., Figure 2a in Section
6.2) with 3250 clients for better comparison. Similar to the experimental results in Section 6, Figure 4
also demonstrates that FedPAGE typically converges faster than SCAFFOLD faster than FedAvg.
Effective LR 0.1
OOI IIOI ZlOl
UlIoN 1U-P2O
UlIoN 1U-P2O
Effective LR 0.03
TOl
.01
OOI IlOI ZlOI
UlIoN 1U"P29
α
I 6 IOOO 2000 3000 4000 5000	6	1000 2000 3000 4000 5000 S 6
Communication Rounds	Communication Rounds
(a) 3250 clients (10 data per client).
Effective LR 0.1	Effective LR 0.03
UlIoN 1U-P2O
TOI 701
UlIoN 1UP29
Effective LR 0.01
5000 IOOOO 15000 20000
Communication Rounds
Effective LR 0.01
TOl ZIOl
UlIoN 1U"P29
5000 10000 15000 20000
Communication Rounds
0	1000 2000 3000 4000 5000	0	1000 2000 3000 4000 5000	0
Communication Rounds	Communication Rounds
(b) 325 clients (100 data per client).
Effective LR 0.1
OOI TOI ZIOl Tol
UlIoN 1U∙,-P2O
0	1000 2000 3000 4000 5000
Communication Rounds
Effective LR 0.03
0	1000 2000 3000 4000 5000
Communication Rounds
(c) 10 clients (3250 data per client).
Figure 4: Experiment results of different methods with different number of clients.
.01 ZIOl
Effective LR 0.01
0	1000 2000 3000 4000 5000
Communication Rounds
A.3 Comparison of different methods with local full gradient computation
A.3.1 Experiment setup
In this section, we design another experiment to observe the performance limitation of FedPAGE,
SCAFFOLD, and FedAvg. We substitute all the steps that use a minibatch to estimate the local full
gradient to the actual full gradient computation. In FedPAGE, we choose b3 = 1 in the previous
experiments and now we set b3 = M , the number of data on a single client. We also choose
b1 = b2 = M. We denote the resulting algorithm FedPAGE-Full. Similarly, for SCAFFOLD and
FedAvg, they choose a minibatch to estimate the local full gradient, and now we change them to
computing the local full gradient, i.e., b = M . We denote the resulting algorithms as SCAFFOLD-Full
and FedAvg-Full.
We then compare four different methods: FedPAGE, FedPAGE-Full, SCAFFOLD-Full, and FedAvg-Full.
We perform the experiments on a9a and w8a datasets with robust linear regression objective and
14
Under review as a conference paper at ICLR 2022
logistic regression with nonconvex regularizer objective. We let each ‘client’ contains 10 samples
from the dataset. We set all the algorithm to run with 10 local steps (K = 10). We run the experiments
with effective step size 0.1, 0.03, and 0.01. For experiment on w8a dataset with logistic regression
with nonconvex regularizer, we also test the algorithms with effective step size 0.3. For FedPAGE
and FedPAGE-Full, we set S = 10 and for SCAFFOLD-Full and FedAvg-Full, we set S = 20 to make
the communication cost in each round to be nearly the same.
A.3.2 Experiment results
The experimental results are presented in Figure 5 and 6. Figure 5a, 5b, 6a, and 6b show the results
of different methods on different problems and different datasets as stated in their captions.
Effective LR 0.1
IlOI ZloI EloI
E」。N 1U∙,-P29
E」。N 1U∙,-P29
Effective LR 0.03
TOI 701
Effective LR 0.01
OOl TOl
E」0N 1U-P29
1000 2000 3000 4000 5000	0	1000 2000 3000 4000 5000	0	1000 2000 3000 4000 5000
Communication Rounds	Communication Rounds	Communication Rounds
(a)	Robust linear regression on a9a dataset.
Effective LR 0.1	Effective LR 0.03	Effective LR 0.01
OOI TOI ZlOl
E」0N ucω⅞EO
0	1000 2000 3000 4000 5000
Communication Rounds
0	1000 2000 3000 4000 5000
Communication Rounds
OOI IlOI ZlOI
0	5000 10000 15000 20000
Communication Rounds
(b)	Robust linear regression on w8a dataset.
Figure 5: Comparison of different methods with robust linear regression.
Effective LR 0.1
TO^IOI SlolZloI 6101IIIOI
UlIoN 1U∙-P2O
0	1000 2000 3000 4000 5000
Communication Rounds
Effective LR 0.03
OOI ZloIVlOl9101Bl
UlIoN 1UP29
0	1000 2000 3000 4000 5000
Communication Rounds
Effective LR 0.01
OOlllOlZlOlElOlVlOl
UlIoN 1U"P29
0	1000 2000 3000 4000 5000
Communication Rounds
(a)	Logistic regression with nonconvex regularizer on a9a dataset.
Effective LR 0.3
OoIZloIvloI 9101 BloI OTOI
UlIoN 1U∙-P2O
O IOOO 2000 3000 4000 5000
Communication Rounds
Effective LR 0.1
IlOI ElOl SlOI
UlIoN 1U"P29
0	1000 2000 3000 4000 5000 ° 0
Communication Rounds
Effective LR 0.03
OOl IlOI ZlOI
UlIoN 1U"P29
1000 2000 3000 4000 5000
Communication Rounds
(b)	Logistic regression with nonconvex regularizer on w8a dataset.
Figure 6: Comparison of different methods with logistic regression with nonconvex regularizer.
15
Under review as a conference paper at ICLR 2022
FedPAGE ≈ FedPAGE-Full First, the experiments show that the convergence performance of
FedPAGE and FedPAGE-Full are nearly the same under the same effective step size. Although there
are some fluctuations in the convergence process, the fluctuations are not large enough to conclude
any difference between the convergence speed of FedPAGE and FedPAGE-Full.
FedPAGE-Full ≥ SCAFFOLD-Full > FedAvg-Full Next, the experiments show that FedPAGE-Full
converges at least as fast as (usually outperforms) SCAFFOLD-Full and both of them converge faster
than FedAvg-Full in all cases. Using the robust linear regression objective in Figure 5, FedPAGE-
Full and SCAFFOLD-Full converges nearly at the same speed, but in the experiments with logistic
regression with nonconvex regularizer in Figure 6, FedPAGE-Full usually outperforms SCAFFOLD-
Full especially when the effective step size is large. From the experiments, FedPAGE either has faster
convergence performance under the same local computation cost, or can use less local computational
resources and achieve the same or even better performance.
B	Gradient Complexity of Different Methods
In previous Table 1, we show the number of communication rounds of different methods. In this
section, we compare the gradient complexity among different methods. Table 3 summarizes the
gradient complexity per client of different methods under different assumptions.
For SCAFFOLD, in each communication round, S selected clients need to perform K local steps, and
the gradient computations of local client is the number of communication round times SK/N . For
FedPAGE, in each communication round, S selected clients need to first compute two full/moderate
minibatch gradients, and then performs K local steps computing only O(1) number of gradient in
each step. The gradient complexity per client of FedPAGE is the number of communication round
times S(M + K)/N. In the BV setting, the full gradient may not be available, then FedPAGE uses a
moderate minibatch gradient to estimate the full gradient, and one only needs to change M to the
moderate minibatch size in order to obtain the corresponding gradient complexity (See the last two
rows in Table 3).
In particular, if the number of data on a single client/device is not very large (M is not very large),
one can choose K such that M + K = O(K). Then the number of gradient computed by FedPAGE
during a communication round is similar to that computed by SCAFFOLD, and also the number of
communication rounds of FedPAGE is much smaller than that of SCAFFOLD regardless of settings
(see Table 1). As a result, FedPAGE is strictly much better than SCAFFOLD in terms of both
communication complexity and computation complexity, both by a factor of N1/4 in the convex
setting and N1/6S1/3 in the nonconvex setting. Thus, FedPAGE is more suitable for the federated
Table 3: Number of gradient computations per client for finding an -solution of federated convex
and nonconvex problems (1).
Algorithm	Convex setting	NonConvex setting	Assumption
FedAvg (Yuetal., 2019)	—	G2NK2	I σ2 -T2	+ NT4	Smooth, BV (G, 0)-BGD
FedAvg (Karimireddy et al., 2020)	G2K I GSK B B2SK σ σ2 N2 + Ne3∕2 +	N	+ NT2	G2K I GSK B B2SK I σ2 N4 + ~NT3^ + Nt2	+ NT4	Smooth, BV (G,B)-BGD
FedProX (Sahu et al.,2018)	B2	—	Smooth, S = N, (0,B)-BGD
VRL-SGD (Liang etal., 2019)	—	NK	Nσ2 ~T2 +工厂	Smooth, BV, S = N
S-Local-SVRG (Gorbunov et al., 2020)	K+√M∕N+M 1/3K2/3	—	Smooth, (BV), S = N, K ≤ M
SCAFFOLD (Karimireddy et al., 2020)	K + 二 e + NT2	S1/3K I σ2 N1/3T2 + NT4	Smooth, BV
FedPAGE (this paper)	NN4 (M + K)	NN+ (M + K)	Smooth
FedPAGE (this paper)	N3/4 ( N32σ2	∖ NNr	+ K)	N1/2+S (Nσ2 + K) Nt2 I St2 + KI	Smooth, BV
16
Under review as a conference paper at ICLR 2022
learning tasks that have many devices and each device has limited number of data, e.g. mobile
phones.
C Useful Inequalities
In this part we recall some classical inequalities that helps our derivation.
Proposition 1. Let {v1, . . . , vτ} be τ vectors in Rd. Then,
hvi, Vj i ≤ 2 kuk2 + 21c ιιvk2, ∀c > 0.
kvi+Vj k2 ≤(I+α)kvik2 + (1 + ŋ kvj k2, ∀α > 0.
τ 2 τ
XVi	≤τXkVik2.
Proposition 2. If X ∈ Rd is a random variable, then
EkXk2 = EkX - EXk2 + kEXk2 .
Besides, we have
EkX - EXk2 ≤ EkXk2.
If X, Y ∈ Rd are independent random variables and EY = 0, then we have
EkX+Yk2 = EkXk2 + EkY k2.
If X1 , . . . , Xn ∈ Rd are independent random variables and EXi = 0 for all i, then
n
XXi
i=1
E
n
XEkXik2.
i=1
Proposition 3. If X, Y ∈ Rd are two random variables (possibly dependent), then
EkX +Y k2 ≤ kEX +EYk2 + 2EkX -EXk2 + 2EkY -EY k2.
(4)
(5)
(6)
(7)
(8)
(9)
(10)
(11)
(12)
Proof.
EkX+Yk2 (=8)kEX +EY k2 +EkX +Y -E(X+Y)k2
(5)
≤ kEX +EY k2 + 2EkX -EXk2 + 2EkY -EYk2.
□
D FedPAGE WITH K = 1
In this section, we discuss and analyze a special case (i.e., the local steps K = 1) of our FedPAGE
algorithm. When we discuss the special case under the nonconvex and convex setting, we do not need
all of the functions {fi } to be L-smooth. Instead, we only need the following average L-smoothness
assumption, which is a weaker assumption compared with the smoothness Assumption 1.
Assumption 3 (Average L-smoothness). A function f : Rd → R is average L-smooth if there exists
L ≥ 0 such that for all x1, x2 ∈ Rd,
EikVfi(xι) - Vfi(X2)k2 ≤ L2kxι — x2k2.
If the functions {fi} are average L-smooth (Assumption 3), then f(x) = NN PN=ι fi(x) is also
L-smooth, i.e., for all x1, x2 ∈ Rd, kVf(x1) - Vf (x2)k2 ≤ Lkx1 - x2 k2.
17
Under review as a conference paper at ICLR 2022
D. 1 FedPAGE WITH LOCAL STEP K = 1
In this section, we review the convergence rate of PAGE in the nonconvex setting. The following
theorem is directly derived from Theorem 1 in (Li et al., 2021; Li, 2021b).
Theorem 3 (Theorem 1 in (Li et al., 2021)). Suppose that Assumption 3 holds, i.e. {fi} are average
L-smooth. Ifwe choose the sampling probability po = 1 and Pr ≡ P = NS for every r ≥ 1, the global
step size
ηg = L(ι+q⅛),
then FedPAGE with K = 1 (PAGE) will find a point X such that Ek▽/(x) ∣∣2 ≤ E with the number of
communication rounds bounded by
R = O ( "√N+S)).
Li et al. (2021) also provide the tight lower bound (Theorem 2 of (Li et al., 2021)) indicating that the
convergence result of PAGE (i.e., Theorem 3) is optimal in this nonconvex setting.
D.2 FedPAGE WITH LOCAL STEP K = 1
Now we show the convergence result of FedPAGE with K = 1 (PAGE) in the convex setting. We
assume that the functions {fi} are average L-Smooth and function f = -NN Pn=I fi(x) is convex.
Theorem 4 (Convergence of FedPAGE in convex setting when K = 1). Suppose that f is convex
and Assumption 3 holds, i.e. {fi} are average L-smooth. If we choose the sampling probability
Po = 1 and Pr ≡ P = NS for every r ≥ 1, the number of local StePS K = 1, the minibatch sizes
b1 = b2 = M, the global step size
ηg ≤ Θ
(S+N3/4)N ∖
L(S+√N) J
then FedPAGE will find a point X such that Ef (x) 一 f * ≤ E with the number of communication
rounds bounded by
R=
O (学
SE
O (N1/4L
if S ≤ √N
if √N < S ≤ N3/4 .
ifN3/4 < S
To understand this result, we can set S = 1, i.e., in each round as long as the server does not
communicate with all clients, it only selects one client to communicate. Then, the total communication
cost of FedPAGE (here also the convergence result for PAGE) becomes O (Nl/4) . Recall that in the
convex setting, SVRG/SAGA has convergence result O (N). Thus FedPAGE/PAGE has much better
convergence result compared with SVRG/SAGA in terms of the total number of clients N.
E Missing Proofs in Section 4
In this section, we prove the convergence result of FedPAGE in the nonconvex setting (Theorem 1).
We use Er to denote the expectation after Xr is determined. Recall that we assume that {fi,j } are
L-smooth, and formally, we have the following assumption
Assumption 1 (L-smoothness). All functions fi,j : Rd → R for all i ∈ [N], j ∈ [M] are L-smooth.
That is, there exists L ≥ 0 such that for all X1, X2 ∈ Rd and all i ∈ [N], j ∈ [M],
l∣Vfi,j (XC-Nfij (Xz)Il ≤ Lkx1 一 χ2 k.
18
Under review as a conference paper at ICLR 2022
Lemma 1. Under Assumption 1, if we choose bɜ = 1 and the local step size ηι ≤ ?^S)KL in
FedPAGE, we havefor any i, k, r
1	KT
天 X Er 期k-成,0∣∣2
k=0
≤12K2L2η2 (σ吗 < M} + L2IW - χi∣∣2 + 犷T- Vf(XrT)II2 + IlVf(XrT)∣∣2
∖	b2
Proof. For any i,k,r, we have
Erkgr,k-gr,0k2
=ErkVI3f⅛r,k) - VI3fi(yr,k-1) + 或k-ι -或0k2
=)(1 + 乙)Igr,k-1 -成,0k2 + KErkVI3fi(yr,k) - VI3fl(yr,k-ι)I2
≤ (1 + K-I) Er∣mr,k-1 -成,0k2 + KL2Erkyrk - yr,k-1k2	(13)
=(1 + K-I) Erkgr,k-1 -gr,0k2 + KL2η2Erkgr,k-1k2
= (1 + E)	Erkgr,k-ι-gr,0k2 + κL2ηι2Erkgr,k-1	-gr,0 + 或0产
=(1 + k⅛1 )	Erkgr,k-1 -	gr,0k2 + KL2η2Er	kgr,k-ι	-	gr,0 + VIj(Xr)	- VIj(XrT)	+ gr-1∣∣2
(12) /	1	、
≤ (1 + K-I) Erkgr,k-1 - gr,0k2 + KL2η2Erkgr,k-1 - gr,0 + Vfi(Xr) - Vfi(XrT) + gr-1∣∣2
+ 4KL2η2 接华 < M)
b2
=(1 + J) Erkgr,k-1 -gr,0k2 +4KL2η2σ⅛≤M}
K — 1	b2
+ KL2η2Erkgr,k-1 - gr,0 + Vfi(Xr) - Vfi(XrT) + gr-1 - Vf(XrT) + Vf(XrT)k2
≤ (1 + tΛ7) Erkgr,k-1 -gr,0k2 +4KL2η202叱 ( M} +4KL2η2Erkgr,k-1 - gr,0k2
K — 1	b2
+ 4KL2η2kVfi(Xr) -Vfi(Xr-1)k2 +4KL2η2kgr-1 -Vf(Xr-1 )k2 +4KL2η2kVf(Xr-1)∣∣2
=(1 + Q +4KL2η2) Erkgr,k-1 -gr,0k2 +4KL2η2<M}
∖ K — 1	)	b2
+ 4KL2η2kVfi(Xr) -Vfi(Xr-1)k2 +4KL2η2kgr-1 -Vf(Xr-1 )k2 +4KL2η2kVf(Xr-1)∣∣2
≤4KL2η2 (。芈：M} + kVfi(Xr) - Vfi(XrT)k2 + kgr-1 - Vf(XrT)k2 + kVf(XrT)k2)
•	(X (1 + k⅛1 +4KL2η2)k)
∖kz=0	)
≤4KL2η2 (。芈：M) + kVfi(Xr) - Vfi(Xr-1)k2 + kgr-1 - Vf(XrT)k2 + kVf(XrT)k2)
•	(∑(1 + k41 + 4KL2η22)k)
≤12K 2L2η2 (Ebj M} + kVfi(Xr) - Vfi(Xr-1)k2 + kgr-1 - Vf(XrT)k2 + kVf(Xr-1 )k2)
(14)
19
Under review as a conference paper at ICLR 2022
≤12K 2L2η2
σ2I{b2 < M}
b2
+ L2∣W - xrτ∣∣2 + ∣∣grτ- Vf(XrT)I∣2 + IlVf(XrT)∣∣2
In the derivation, (13) comes from the smoothness assumption (Assumption 1), (14) comes from the
fact that if we choose ηι ≤
2Z⅛ ,thenwehave
KT /	1	∖k
X (1 + Kj-I + 4KL2η22)
(1 + 也 +4KL2η22)K - 1
≤	K⅛ +4KL2 ηι2
≤(κ - 1) ((1 + K-I +4KL2η2)	- 1
≤(κ -1) ((1 + K-1 + 3⅛)	- 1
≤3K,
for any K ≥ 2. Then we take the average over k, we get
1 KT
K X Er ∣gr,k -成,0∣2
k=0
≤12K2L2η2 (Gbj M} + L2∣xr - xr-1k2 + 犷T- Vf(XrT)∣∣2 + IIVf(XrT)∣∣2),
and we conclude the proof of this lemma.	□
Lemma 2. Under Assumption 1, if we choose b3 = 1 in FedPAGE, the local step size ηι ≤ 2^S)KL，
the batch sizes bi = min{M, ∣^⅛}, b2 = min{M, ISl2}, then we have
Erllgr-Vf(Xr)||2 ≤ (1-P)∣∣grT-Vf(xrT)k2 + 1-p/3L2Erkxr-xr-1k2 +看IIVf(XrT)II2+『.
Proof.
Erkgr-Vf(Xr )I2
=(1 - P)Er
=(1 - P)Er
=(1 - P)Er
=(1 - P)Er
1
K ∣Sr l
1
K ∣Sr ∣
K
XX 服k-1
i∈Srk=1
K
XX gr,k-i
i∈Sr k=1
-Vf (Xr)
-Vf (Xr)
2	2
+ P N X VIIfi(Xr) - Vf (Xr)
i∈[N ]
2
ι Pσ2I{b1 < M}
+	Nbi
r	K	2	2TΓΓ7
E XX(gr,k-ι-gr,0+gr,。-Vf(Xr))	+ P°	{Nib<	}
1	1i∈Sr k=1	1
1
K ∣Sr ∣
K
XX(gr,k-ι - gr,0 + VI2 fMr)-
i∈Sr k=1
VI2 fi(Xr-i) + gr-i
—
2
ι Pσ2I{b1 < M}
+	Nbi
K	2
=)(1 - P) Er k‰∣ XX(gr,k-1-gr,0 + gr-i-Vf(XrT))	+ *弋< M}
1	1i∈Sr k=1	1
(15)
20
Under review as a conference paper at ICLR 2022
+ (1 - P)Er
1
K |Sr |
K
X X (gr,k-ι - gre)-
i∈Sr k = 1
ET —1-r
r K|Sr |
K
XX(g"- gr,o)
i∈Sr k=1
+
1
K |Sr |
K
XX(Vι2fi(x) - Vf(Xr) + Vf(XrT)- Vl2fi(xr-1))
i∈Sr k=1
2
(I)(I-P) (1 + p) kgr-1-Vf(XrT)I∣2 +
pσ2I{bι < M}
Nbi
+(1 - p) (1+P) IIEr k∣⅛∣
K
2
+ (1 - P) (1 + P) Er
+ (1 - P) (1 + P) Er
1
K |Sr |
1
K|Sr |
∑ ∑ (gr,k-1 -或 o)
i∈Sr k = 1
K
X X (Vi2fi(Xr) - Vf (Xr) + Vf (Xr-1) - Vi2fi(Xr-1))
i∈Sr k=1
KK
-X X (砥-1 -或 o) - Er k∣S- XX (gr,k-1 - gr,o)
i∈Sr k = 1	1	1 i∈Sr k=1
2
2
=(1 -p) (1 + 2) kgr-1-Vf(XrT)I∣2 +
pσ2I{bι < M}
Nbi
+ (1 - p) (1 + P) ∣∣Er KSri X X(gr,k-1 - gr,θ)
2
+ (1 - P) (1 + p) Er ∣∣ 高 X (Vι2fi(Xr) - Vf (Xr) + Vf (Xr-1) - Vl2 fi(Xr-1))
2
+ (1 - p) (1 + P) Er
1K	1K
KSq Σ Σ (gi,k-1 - gi,o) - Er KSTj £ £ (gi,k-1 - gi,o)
1	1 i∈Sr k = 1	1	1 i∈Sr k=1
2
(11)	/ 力、
≤ (1 -p)(1 + p) kgr-1-Vf(XrT)k2 +
pσ2I{bι < M}
Nbi
+ (1 - P) (1 + P) ^r KSri X X(gr,k-1 - gr,θ)
2
+ ⅛2 (1 + p) Er ∣∣VI2fi(Xr) - Vf(Xr) + Vf(Xr-1) - VI2fi(Xr-1)∣∣2
S ∖	2 /
(16)
⅛ U
1 八	1 八
ErK E (gr,k-1 - 9i,θ) - Er K ΣS (gr
K
K
2
k = 1
(Il)(I-P) (1 + p) kgr-1-Vf(XrT)I∣2 +
k=1
pσ2I{bι < M}
Nbi
+ (1 - P) (1 + P) ^r KSri X X(成,k-1 -成,θ)
2
lk-1 - gi,θ)
+⅛ Q
+⅛
1 + 2) Er Il (Vfi(Xr) - Vf(Xr) + Vf(XrT)- Vfi (XrT)) Il2
1+P
4σ2I{b2 < M}
b2
1 - P 乙 2
+ 丁 0 + P
K
K
1K	1K
ErK X (gr,k-l - gi,θ) - Er K X (成,
2
k=1
k = 1
k-1 - gr,o)
21
Under review as a conference paper at ICLR 2022
(9) n
≤ (1 — P )kgr-1—Vf(XrT)II2 +
pσ2I{bι < M}
2
+--Er
p
1
K ∣Sr ∣
Nb1
2
K
X X (gr,k-1 - gr,0)
i∈Sr k = 1
+ =∣/2Er Il (Vfi(Xr) — Vfi(Xr-1)) Il2 +
S
1 — p/2 4σ2I{b2 < M}
b2
S
2
K
+
1
κ E(gr,k-ι — gr,0)	∙
(17)
k=1
In the previous derivations, (15) comes from the fact that we separate the mean and the variance of
a random variable (Equation (8)). In (16), we define Zt to be Vz?fi(Xr) 一 Vf(Xr) + Vf(XrT)—
Vi2 fi(Xr-1) and then apply (11). Here, Zt are i.i.d. random variables.
Then we plug in Lemma 1, and by setting ≤ 24⅛ ,wehave
Erllgr-Vf (Xr )||2
(< (1 - 2) Igr-1-Vf(XrT)I∣2 +
pσ2I{b1 < M}
2
+--Er
p
1
K ∣Sr l
Nbi
2
K
XX(%-f)
i∈Srk=1
+ 1-^2Erk(Vfi(Xr) — Vfi(Xr-1)) I2 +
S
1 — p/2 4σ2I{b2 < M}
b2
S
2
2
+ ps
1
Er	K ∑(gi,fc-1 — gi,0)
k=1
≤ (1 - P) IgrT - Vf(XrT)I∣2 +
pσ2I{b1 < M}
+ 1 - J/2 L2 ErkXr — XrTk2 +
S
Nbi
1 — p/2 4σ2I{b2 < M}
(18)
+ 4 ∙ 12K2L2η2
p
σ2I{b2 < M}
b2
+ L2∣∣xr - x'
b2
LIk2 + IIgrT- Vf(XrT)Il2 + IlVf(XrT)II2
Plug in ηι	Pl	-l o
≤ (1 — p)kgr-1 — Vf(XrT)I∣2 +
pσ2I{b1 < M}
+ 1 - J/2 L2 ErkXr — XrTk2 +
S
Nb1
1 — p/2 4σ2I{b2 < M}
b2
+6S
σ2I{b2 < M}	pL2	r
+ 裾kx -
b2
XrTk2 + ɪkgr-1 — Vf(Xr-1 )k2 + 48K2L2n2 IVf(Xr-1
6S	p
)I2
K
S
S
≤(1 - P)kgr-1 — Vf(XrT)k2 + 1-Sp/3L2ErkXr - Xr-1k2 + 48KPL2褚 IVf(XrT)II2
+	6S
σ2I{b2 < M}+ 1 — p∕2 4σ2I{b2 < M}十 pσ2I{b1 < M}
b2
b2
Nb1
S
where in (18), we apply Lemma 1 and Eq. (4). Plugging in the batch sizes b1 = min{M, 1^J }, b2 =
min{M,需} and recall 小 ≤ 24√KL, We get
Er∣∣gr -Vf(Xr)∣∣2
≤ (1 - P) kgr-1 - Vf (Xr-1)k2 + 1-Sp/3L2ErkXr - Xr-1k2 + 48KPL2褚 IVf (Xr-1)k2 + p82
(19)
22
Under review as a conference paper at ICLR 2022
≤(1 - P)kgr-1 - Vf(XrT)k2 + 1-Sp/3L2Erkxr-XrTk2 + 6SkVf(xr-1)k2 + p∣2.
□
Then, combining with the following descent lemma, we can prove Theorem 1.
Lemma 3 (Lemma 2 in PAGE Li et al. (2021)). Suppose that f is L-smooth and let Xt+1 := Xt - ηgt.
Then we have
f(χt+1) ≤ f(Xt)- 2||Vf(Xt)II2 -仕-2) ∣∣χt+1 - XtII2 + 2||gt - Vf(Xt)||2.
Theorem 1 (Convergence of FedPAGE in nonconvex setting). Under Assumption 1 (and Assumption
2), if we choose the sampling probability Pr ≡ P = NS for every r ≥ 1 and po = 1, the minibatch
sizes bi = min{M, 2S^}, b? = min{M, p8S∣2-}, and the global and local step sizes
ηl ≤
√2p
24√SKL,
then FedPAGE will find a point X such that EkVf (X)k2 ≤ within the following number of
communication rounds:
R=O
(l(√N + S)
1-S2-
Proof. When ηι < 24√S)KL, Lemma 2 holds. If We choose the batch sizes bi
min{M, pN⅛}, b2 = min{M,爵}, We have
1
η ≤ -7-1
L O+q^
E
≤E
f (Xr)-f *+32Pg kgr-Vf(Xr) k2
f(Xr-1)- f* - ηkVf(Xr-1)k2 -
kXr-Xr-1k2 + η2g kgrT-Vf(Xr-1)k2
(20)
+ 3ηgE [(1 - p) kVf (Xr-i) - gr-1k2 + S (1 - P) L2ErkXr - Xr-1k2 + 6SErkVf(XrT)k2 + p∣2
≤E[f(Xr-i) - f* - ηgkVf(Xr-1)k2 - fM - 2 - 3ηgS(1-3) L2) kXr - Xr-1k2
4	2ηg	2	2P S 3
+3ηg kgr-1 -Vf (Xr-1)k2,	(21)
Where in (20) We plug in Lemma 2 and Lemma 3, and in (21) We rearrange the terms.
Choosing ηg
and P = NS
the coefficient of kXr - Xr-i k2 is greater than zero,
and We can throW that term aWay (since kXr - Xr-i k ≥ 0 and the sign is minus). Then We have
E [f(Xr) - f* + 3ηgkgr-Vf(Xr)k2
2P
≤E [f(Xr-i) - f* + 3ηgkgr-1 - Vf(XrT)k2] + ⅛2 - ηgEkVf(XrT)k2.
2P	16	4
We also knoW that in the first round,
E
1
N EVbιfi(x0) - Vf (x0)
i=i
2
22
σ P
=Nbi ≤ 24
23
Under review as a conference paper at ICLR 2022
and we have
E f (Xr) - f* + 32ηgkgr - Vf(Xr)k2l ≤ E f (x0) - f* + 32ηgp242] +⅛2-? XXEkVf(xi)k2,
2p	2p 24	16	4
i=0
which leads to
η4g XX EkVf(Xi)k2 ≤E f (χo) - f *+32ηg ρ2421+W
4	2p 24	16
where We use the fact that ||」2 ≥ 0 and f (x) — f * ≥ 0.
So in O(1∕(ηg e2)) number of rounds, FedPAGE can find a point X such that EilVf (x)∣∣2 ≤ e2, which
leads to a point X such that EkVf (X)k ≤ . Then since
1
ηg
L
3(1 - p/3)
2pS
O
√ √N + S
we know that FedPAGE can find a point X such that EkVf(X)∣ ≤ E in O (L√N+S)) number of
communication rounds.	□
F Missing Proof in Section 5
In this section, we show the convergence result of FedPAGE in the convex setting. We first show the
result when the number of local steps is 1 (K = 1), where FedPAGE reduces to PAGE algorithm
(Section F.1). Then, we show the result of FedPAGE in the convex setting with general number of
local steps.
F.1 Proof of Theorem 4
Similar to the notations in the proof for the nonconvex setting, we use Ft to denote the filtration when
we determine the "gradient" gt-1 but not gt, i.e. Xt is determined but Xt+1 is not determined. We use
EjH to denote EHF∙].
Recall that in this section, we assume the objective function f is convex and all the functions {fi }
are averaged L-smooth.
Assumption 3 (Average L-smoothness). A function f : Rd → R is average L-smooth if there exists
L ≥ 0 such that for all X1, X2 ∈ Rd,
EikVfi(X1) -Vfi(X2)k2 ≤ L2kX1 - X2k2.
The main difficult to prove Theorem 4 is that FedPAGE uses biased gradient estimator, i.e.
Egr 6= EVf(Xr),
for most of the rounds r. During the derivation, we will encounter the following inner product term
EhVf (Xr-1) - gr-1, Xr - X*i.
If the gradient estimator is unbiased, the above inner product is zero and we don’t have to worry
about this term. But when the gradient estimator is biased, we need to bound this term.
However, since the server using FedPAGE will communicate with all of the clients with probability
pr in round r to get the full gradient Vf (Xr ), the following property holds.
Lemma 4. When the number of local steps is 1 (K = 1) and we choose the probability pr = p for
all r, FedPAGE satisfies for any r ≥ 1,
Er[gr-Vf(Xr)] = (1 - p)(gr-1 - Vf(Xr-1)).
24
Under review as a conference paper at ICLR 2022
Proof. If in round r, the server does not communicate with all the client and only communicate with
a subset of clients Sr, then from the definition of FedPAGE, Ayf =-⑺矶 and we can get
gr
而* S Ayt=∣sr∣ 2 或0.
—
We use EZ to denote the expectation over the minibatch 工2 to estimate the local full gradient. Then
we have
Er[gr - V/(χr)] =(1 - P)Er
∣S∣ E 或0-Vf(χr) + pE1
i∈Sr
N E Vbιfi(Xr)-Vf(Xr)
i∈[N ]
r
=(1 - P)焉Er E EZ [(或0 - Vf(xr))]
I I	i∈Sr
=(1 -p)ɪEr E EI [Vi2fi(χr) -Vi2fi(χr-1) + gr-1
I I	i∈Sr
=(1 -p)ɪEr E EI [Vi2fi(χr) -Vi2 fi(χr-1) + gr-1
i i	i∈Sr
-V∕(xr)]
-V∕(xr)]
=(1 - P)ɪEr E [Vfi(xr) - Vfi(XrT) + gr-1 - V∕(xr)]
I I i∈Sr
=(1 - p)(gr-1 -Vf (Xr-1)),
where we use the fact that Sr is uniformly chosen from [N] and Vi2 f (x) is a gradient estimator of
Vf (x).	□
Lemma 5 (Lemma 3 of (Li et al., 2021))). When the numberof local steps is 1 and we choose Pr = P
for all r, FedPAGE satisfiesfor any r ≥ 1,
Erkgr- Vf(Xr)k2 = (1 -P)IIgrT- Vf(XrT)∣∣2 + FErkVfi(Xr) - Vfi(XrT)∣∣2.
S
Then, we can control the inner product term using the following lemma.
Lemma 6. For any t ≥ 2 and any c > 0, we have
t	1 t /	1
EEhVf(XrT)-gr-1,Xr-X*> ≤2-EE f c∣Vf (Xr-1) - gr-1k2 +/W - Xr-1k2
r=1	r=1 、
Proof.
EhVf(XrT)- gr-1,Xr - x*〉
=EhVf(XrT) - gr-1,Xr - XrTi + EEr-1hVf (XrT) - gr-1,Xr-1 - x*〉
c1
≤2Ekf(XrT)- gr-1k2 + 2cEkXr- xr-1k2 + (1 -p)EhVf (xr-2) - gr-2,xr-1 - x*〉，
where the last inequality comes from Young,s inequality and Lemma 4. For r = 1, we know that
Er-1 hVf (xr-1) - gr-1,xr-1 - x*〉= 0. Unrolling the inequality recursively, we have
EhVf(XrT)- gr-1,xr - x*〉
c1
≤ 2Ekf(XrT)- gr-1k2 + 2cEkXr-XrTk2 + (1 - P)EhVf(Xr-2) - gr-2,xr-1 - x*〉
c1
≤ 2Ekf(XrT)-gr-1k2 + 2cEkxr - xr-1k2
+ 丫 E 卜 kf (xr-2) - gr-2k2 + CkXrT-Xr-2k2)
+ (1 - p)2EhVf (xr-3) - gr-3, xr-2 - x*〉
25
Under review as a conference paper at ICLR 2022
≤ XX(1 -PL'CE∣∣f"T)-JTk2 + XX(1 -PLr21cE∣w' - JT∣∣2.
r0=1	rr = 1
Then we sum up the inequalities from r = 1 to t,we have
t	1	t /	1
EEhVf(XrT)-grτ,xr-x*> ≤2EE 卜∣∣Vf(XrT)- gr-1k2 +/W - xr-1k2
r=1	P r=1
□
Given these lemmas, we can now prove Theorem 2. We first prove 2 lemmas related to the function
decent of each step, and then show the proof of Theorem 2.
Lemma 7. For any r ≥ 0 and any λ > 0, we have
0 ≤ - ηaEr[f (xr+1) - f(x*)] + ɪErkgr- Vf(Xr)k2 - ɪEra+1 -x*∣∣2 + Ikxr -x*k2
ηg L(λ + 1)
2
-1) ErkXr+1 - xrk2 + ηg(1 -p)hVf(xr-1)- gr-1
xr — X*).
Here, we define g-1 = Vf(X-I) = 0.
Proof. For any r ≥ 0, we have
ηg(f(Xr) - f(X*))
≤ηghVf(Xr),Xr - X*)
=ηghVf(Xr) - (1 -P)(Vf(XrT)- gr-1),Xr -X*) + ηgh(1 -P)(Vf(XrT)- gr-1),Xr -x*)
=ηgErhgr,Xr - X*) + ηgh(1 - p)(Vf (XrT)- gr-1),Xr - X*)
=ηgEr hgr,Xr - Xr+1) + ηgEr hg,Xr + 1 - X*) + ηg h(1 - p)(Vf (XrT)-gr-1),Xr - X*)
≤ηg Er hgr ,Xr - Xr+1)- 1 ErkXr+1 - Xr k2 + IErkXr- X*F - | ErkXr+1 - X* k2
+ ηgh(1 -p)(Vf (Xr-1) - gr-1 ),Xr - x*).
We also have
ηg Er hgr ,xr - xr+1)
=ηgErhgr - Vf (xr),xr - xr+1) + ηgErhVf (xr),xr - xr+1)
≤券Erkgr - Vf (xr)k2 + 吟Erkxr - xr+1k2 + ηg(f(xr) - f (xr+1)) + 华kxr+1 - xrk2.
2λL	2	2
Summing up the 2 inequalities we conclude the proof.	□
Lemma 8. For any r ≥ 0 and any λ > 0, we have
0 ≤ ηg(f (xr) - f (xr+1)) + 祟kgr - f (xr)k2 + (ηg"λ +1) -1) kxr - Xr+1k2.
2Lλ	∖	2	)
Proof.
0 =ηg hgr ,xr - xr+1) + ηg hgr ,xr+1 - Xr)
=ηghVf (xr),xr - xr+1) + ηghgr - Vf (xr),xr - χr+1)- ∣∣xr+1 - xr∣∣2
≤ηg(f(xr) - f(xr+1))+ ηghgr - Vf(xr),xr - xr+1) + (竽-kxr+1 - xrk2
≤ηg(f(χr) - f(χr+1)) + 号kgr - Vf(χr)k2 + (ηg叱 + 1) - 1) kχr+1 - χrk2.
2λL	∖	2	J
□
26
Under review as a conference paper at ICLR 2022
Theorem 4 (Convergence of FedPAGE in convex setting when K = 1). Suppose that f is convex
and Assumption 3 holds, i.e. {fi} are average L-smooth. Ifwe choose the sampling probability
Po = 1 and Pr ≡ P = N for every r ≥ 1, the number of local steps K = 1, the minibatch sizes
bi = b2 = M, the global step size
ηg ≤ Θ
((S+N3/4)N ∖
∖ L(S+√N))
then FedPAGE will find a point x such that Ef (x)—
rounds bounded by
1。(空),
f * ≤ E with the number of communication
if S ≤ √N
R = o O
O
if √N<S ≤ N3/4 ∙
if N3/4 < S
ProofofTheorem 4. From Lemma 7 and Lemma 8, for any δ > 0, we have
ηgEr [f (xr + 1) — f (x*) + δ(f (xr ) — f (xr+1))]
≤ ηg 2Lλδ) Erkgr — Vf(Xr )k2 — 2 ErkXr+1 — x*∣∣2 + 2 kxr — x*k2
ηg L(λ + 1)(1 + δ)	1 + 2δ
---------------------------
+
2
2
ErIIXr+1 — xrk2 + ηg (1 — p)hVf (XrT) — gr-1,xr — x*i∙
Summing UP the inequalities from r = 0 to T — 1 and taking the expectation, we have
τ-1
X ηgE[f (xr+1) — f (x*)] + ηgδE[f (xτ) — f (x0)]
r=0
≤ — 1 EkXT — x*k2 + 1 kx0
T-1
―x*k2+ X
r=0
ηg (I+δ) Ekgr—Vf (Xr )k2
2Lλ
T-1 /	一、. 一、 一
+ X ((1 + δ)L0 +]ηg — 1 Ekxr+1 — xrk2 + ηg(1 — P)EhVf(xr-1) — gr-1,xr — x*)
r=0	2
1	1	T-1
≤ — 2 EkXT — x*k2 + -kx0 — x*k2 + X
η ηg(1 + δ)
V 2Lλ
+Ekgr -Vf(Xr )k2
T-1
+ X (1 + δ)
r=0
Lηg (λ + 1) — 1 +
2	+
ηg(I — P) A
2cp(1 + δ))
Ekxr+1 — xr k2,
where We apply Lemma 6 to bound the inner product term. Then using Lemma 5 and Assumption 3,
we can get the following result.
T-1
XEkgr -Vf(Xr)k2 ≤
r=0
T-1	T-1
X T EEr kVfi(xr) — Vfi(xr-1)k2 ≤ X
r=1	r=1
(I — P)L2 EkXr — xr-1k2 ∙
PS
Plugging into the previous inequality, we have
T-1
X ηgE[f (xr+1) — f (x*)]+ ηgδE[f (xT) — f (x0)]
r=0
1	1	T-1
≤ — 2 EkXT — x*k2 + 2 ∣∣x0 — x*k2 + W E E∣∣xr+1 — xr k2,
r=0
27
Under review as a conference paper at ICLR 2022
where
W =(1 + δ)
Lηg(λ + 1) — 1 + ηg(I — P) ∖ + n%(I + δ) + ηg(I — P)C、
2	+ 2cp(1 + δ)J + ∖	2Lλ +	2p )
(1 - P)L2
-Ps-
By choosing λ = Vz1∕(Sp), C =，Sp/L2, we have
W =(I + δRηg(P1/(SP) + 1) - 1 + δ + ηg(1 -P) + η ηg(1 + δ) + ηg(1 -P)PSPlu
W —	2	2-	2p√PS∕L2	∖2L√1∕(Sp)	2P
(1 - p)l2
-Ps-
= (1 + δ)
LlIg(,1/(SP) + 1) - 1 + ηg(1 - P)L + (ηg√S + ηg(1 -p)√spA
2	2 + 2p3∕2√S (1 + δ) + I 2L + 2pL(1 + δ) J
(1 - P)L
-Ps-
=(1 + δ)
Lηg(V71∕(SP) +1)	- 1	+ ηg(1 - P)L	+ (ηg +	η(1 -P)	A
2	2	2p3∕2√S(1 + δ) VT	2p(1 + δ))
(1 - P)L
√pS
= (1 + δ)
∖
Llg(√1/(SP) + 1) q	ηg(1 - P)L	, ηg (1 - P)L , ηg(1 - P)(I- P)L 1
HHH
、	2__,	2p3/2 √S(1 + δ)	2	√pS	2p(1 + δ)	√S	2
{；} S{{
A	B	C	D)
and δ ≤ 1/p, we have
Lηg(√1ZSP)+1)
O ( l(√17^SP) +1)—(IIP)P——
∖	7L(√1∕^P) + 1)
O(1),
ηg(1 - P)L	= O I (1 - P)L______p______
2p3∕2√S(1 + δ) ―	12p3/2VSL(1 + √1∕(SP))
O(1),
C =迎(1 - P)L = O ( (1 - P)L	,(1/P)P
_T √PS _	1 √PS L(√1∕^P) + 1)
O(1),
(1 - P) (1 - P)L_p__
F	√PS L(1 + √1∕W)
O(1).
In this way, we can choose ηg with a small constant such that W is non-positive, and we can throw
that term. In this way,
T T	1
E E[f(xr+1) - f(x*)] ≤ — ∣∣x0 - x*∣∣2 + δ[f(x0) - f(x*)] ≤
r=0	ηg
Then we set δ = 1∕(S1/4P3/4) and P = S∕N. We first verify that δ ≤ 1/p. We have
Then we choose
1 N3/4 _ N3/4 / N _ 1
ST4 S3/4 = s ≤ S = p.
,if S ≤ √N,
o (学
Se
,if S ≤ √N,
,if √N < S ≤ N3/4,.
,if N3/4 < S
If ηg ≤ o (f⅛)
D
R = < O (N 1/4L/e) , if √N < S ≤ N3/4,.
O (N A , if N3/4 <S
28
Under review as a conference paper at ICLR 2022
□
F.2 Proof of Theorem 2
The proof idea of Theorem 2 is similar to that of Theorem 4. The difference between these two proof
comes from the fact that in the convex setting with general local steps, the local steps between the
communication rounds introduce some local error and we need to take the error into account.
In this section, we assume that all the functions {fi,j } are L-smooth.
Assumption 1 (L-smoothness). All functions fi,j : Rd → R for all i ∈ [N],j ∈ [M] are L-smooth.
That is, there exists L ≥ 0 such that for all x1, x2 ∈ Rd and all i ∈ [N],j ∈ [M],
l∣Vfi,j (XI)-Vfij (x2)k ≤ Lkx1 -χ2 ∣∣.
Similar to the proof with K = 1, we first prove 2 lemmas related to the function decent of each step.
The following lemma is very similar to Lemma 7 except the last term, since in the general case, we
do not have Lemma 4.
Lemma 9. For any r ≥ 0 and any λ > 0, we have
0 ≤ - ηg Er [f (xr+1) - f(x*)] + 2Lλ Erkgr- Vf (xr )k2 — 2 Erkxr+1 -x*k2 + | kxr —x*k2
ηg L(λ +I)	1
2
Here, we define g-1
Erkxr+1 - xrk2 + ηghVf(xr) - gr,xr - x*).
Vf(x-1) = 0.
2
Proof. For any r ≥ 1, we have
ηg(f(xr) - f(x*))
≤ηghVf(xr),xr-x*i
=ηghVf(xr) -(Vf(xr) -gr),xr -x*i+ηghVf(xr) -gr,xr -x*i
=ηgErhgr,xr - x*i + ηghVf(xr) - gr,xr - x*i
=ηgErhgr, xr - xr+1i + ηgErhgr, xr+1 -x*i+ηghVf(xr) -gr,xr -x*i
≤ηg Er hgr ,xr - xr+1i- 2 Er kxr + 1 - xrk2 + 1 Erkxr- x*k2 - IErkxr+1 - x*k2
+ηghVf(xr)-gr,xr-x*i.
When r = 0, the inequality also holds. We also have
ηgErhgr,xr - xr+1i
=ηgErhgr - Vf(xr), xr - xr+1i + ηgErhVf(xr), xr - xr+1i
≤ 黑 Erkgr- Vf (xr )k2 + 吗 Erkxr- xr + 1k2 + % (f(xr ) - f)) + ηgL kxr+1 - xrk2.
2λL	2	2
Summing UP the two inequalities We conclude the proof.	□
Lemma 10. For any r ≥ 0 and any λ > 0, we have
0 ≤ ηg(f(xr) - f(xr+1)) + 紧kgr - f(xr)k2 + f ηgL(λ+1) - 1) kxr - xr+1k2.
2Lλ	2
Proof.
0 =ηghgr, xr - xr+1i + ηghgr, xr+1 - xri
=ηghVf(xr), xr - xr+1i + ηghgr - Vf(xr), xr - xr+1i - kxr+1 - xrk2
≤ηg(f (xr) - f (xr+1)) + ηghgr - Vf (xr), xr - xr+1) +	∣xr+1 - xrk2
≤ηg(f(xr) - f(xr+1)) + 2λgLkgr - Vf(xr)k2 + (ηgL(λ + 1) - 1) kxr+1 - xrk2.
□
29
Under review as a conference paper at ICLR 2022
Then we bound the inner product term.
Lemma 11. For any t ≥ 2 and any c, c0 > 0, we have
t
X EhVf(Xr) - grH- x*〉
r=1
1 二/	.	…1	二一	1	.	C
≤歹EE CkVf(XrT)-gr-ι∣∣2 + Tw-XrT∣∣2 + E E |屋	gr k2 十	IlXrT-,*『
2p	∖	c	KS	,	, c
r r = 1 ∖	k=0 i∈Sr
Proof.
EhVf(Xr)- gr,xr- x*〉
=EhVf (Xr) - gr, Xr - XrTi + EhVf(Xr) - gr, XrT - x*〉
c1
≤ɔEkf(Xr) - grk2 + -EkXr-XrT∣∣2 + EhVf(Xr) - gr,xr-1 - x*〉,
2	2c
where the last inequality comes from Eq. (4). We also have
EhVf (xr) - gr,xr—1 - x*)
=(1 - P)Ehf(XrT)- gr—1,xr-1 - x*) + EhVf (xr) - gr - (1 -p)(Vf(x,
Then we can compute the second term in the previous inequality.
EhVf (xr) - gr - (1 - P)(Vf(XrT)- gr—1), xr—1 - x*)
:r-1 - gr—1),xr-1 - x*).
/	1	1
=(1 - P)E(Vf (xr) - KS XX gr,k + S
X gr,o- S X gi,0+gr-1
- Vf(Xr—1), Xr—1 -X*
i∈Sr
/	1	1 l	-.
=(1 - P)E〈- K XX gr,k + S X gr,0, xr—1
∖	k=0 i∈Sr	i∈Sr
/	1 VL	1 L	_
=(1 - P)E ∖ - K X X gi,k + S X gi,0,x
∖	k=0 i∈Sr	i∈Sr
C0	1 VL	1 L	1	_
≤2kKS X X %k - S X gi,0k + 2C7kx
k=0 i∈Sr	i∈Sr
*
-X
*
- X*
i∈Sr
:+ E (Vf(xr)-
)
1 X gr,0 + gr-1F
i∈Sr
,r—1
1 - X*
-X*k2,
where we use the fact that E [Vf (xr) - 1 Pi∈SF g]。+ gr—1 一 Vf(XrT)] = 0 and Eq. (4).
Combining the computations together, we get for any c, c0 > 0,
EhVf (xr) - gr,xr - x*)
c1
≤(1 -P)Ehf(XrT) - gr—1 ,xr—1 - x*)+ -Ekf (xr) - gr k2 + 五CEkXr- XrTk2
C0	1 ɪKL	1 L	1	_	C
+ 2kKSX X gi,k - S X gi,0k +2C7kx― — x*k
k=0 i∈Sr	i∈Sr
c1
≤(1 -P)Ehf(XrT) - gr—1 ,xr—1 - x*i + -Ekf (xr) - gr k2 + τrEkXr- XrTk2
2	2c
C0	K	CI
+ 2KS X X kgr,k - X gr,0k + 2C∑kxr- — x*k .
k=0 i∈Sr
i∈Sr
We also know that EhVf(x0) - g0, x0 - x*〉 = 0, and we can get for any t ≥ 1,
t
X EhVf (xr) - gr,xr - X*)
r=1
30
Under review as a conference paper at ICLR 2022
1	TCI	TCcO	n -	1	C
≤2-EE dNf(XrT)- gr-1k2	+ -IW-XrTk2	+ KSE E kgr,k	-或0k2 + FIWT-X*k2
2pl	c	_£\ S	c
r r = 1 ∖	k=0 i∈Sr
□
Lemma 12. For any c,c0 > 0 such that 煞 ≤ 1/2, we have
t
X E”f(xr) - gr,xr- X*〉
r=1
≤高kx0 - X*k2 + (2p + 件)EX W(Xr-I)- gr-1k2
J 1 , tc , 2ηgt) E X II r r-1∣∣2 , ( c' , tηg ) E X 1 X χ ∣∣ r r ∣∣2
+ (布 + 正 + 浮JE么kX -X k2 + (而 + 春)E2KSA⅛k ，k- ,0k.
Proof.
∣∣Xr - x*∣∣2
= ∣∣Xr-1-X*- ηggr-1k2
= ∣Xr-1 -x*∣2 + 陪犷-1『-2ηg@-1, XrT-X*)
= ∣Xr-1 - x*∣2 + ∣Xr - XrTk2 - 2ηg (V/(Xr-1 ),Xr-1 - x*〉- 2ηg SrT- Vf(XrT), XrT- x*〉
≤∣Xr-1 - x*∣2 + ∣xr - XrTk2 + 2ηg(f (x*) - /(XrT) + 2ηg(Vf(XrT)-gr-1, xr-1 - x*)
≤∣xr-1 - x*∣2 + ∣xr - xr-1∣2 + 2ηg(Vf(XrT) - gr-1,xr-1 - x*)
≤∣x0 -x*∣2 + X ∣χr' -χr'-1k2 +2ηg X (Vf(Xr'-1) - gr'-1,xr'τ -x*).
r =1	r =1
For simplicity, we define the following notations,
t
Ac=EX CkVf(XrT)-gr-1∣2
r=1
t1
Bc=EE CkXr-XrTk2
r=1
t 0 K
4=E X KS XX kgr,k-或 0k2
r = 1	k=0 i∈Sr
t
Dt=EX kxr-1 -x*k2
r=1
t1
Dc 0 =EE ~ckxr-1-x*k2∙
r=1
From Lemma 11, we know that for any t0 ≤ t, we have
t'	1
X E(Vf (Xr) - gr,Xr - x*) ≤2- (Ac' + Bc' + C + DtQ
r = 1
≤ 2r (Ac + Bc + Ct，+ Dt，),
2p
and for any r ≤ t, we can bound ∣∣χr - x*∣2 by
EkXr - x*∣2
31
Under review as a conference paper at ICLR 2022
r	r
≤E∣∣x0 - x*∣∣2 + E X ∣∣xr' - xr'τ∣∣2 + 2ηgE X "f (xr'-1) - g"1, xr'-1 - x*)
r0=1	r0 = 1
0	，…	1
≤E∣∣x0 - x*∣∣2 + E 工 IlXr -XrTk2 + 2ηg而(Ac + Bc + C，+ D")
≤E∣∣x0 - x*∣∣2 + CBc + ηgg (Ac + Bc + CtC, + 琰,).
Then We bound Dt,we have
t
Dt= X kxrτ-x*k2
r = 1
≤ X(E||x0 - x*∣∣2 + cBc + ηg (Ac + Bc + Cc, + Dc，))
≤t∣∣x0 - x*∣∣2 + tηgAc +1 (C + η) Bc + tηgCc, + tηgDt.
P	∖ p ) P Pd
As long as tpc ≤ 1∕2,we have
Dt ≤ 2t∣x0 - x*∣∣2 + 等Ac + 2t (c + η) Bc + 等Cc,.
Then we plug this inequality into Lemma 11, we get
t
X EhVf(Xr)-gr,xr - x*)
r=1
≤ 2P E
ʌ (	1	1 1	1	C C d	1	_ C
E CkVf (XrT)- gr-1k2 + TXr-XrTk2 + KSE E kgr,k -或0k2 + —0 kxr-1 - x*k2
r = 1 ∖	S k=0 i∈Sr
二/	…1	…—VL	八
E CkVf(XrT)- gr-1k2 + CkXr-XrTk2 + KSE E kgr,k -或0k2
r = 1 ∖
2PC (2tkx0 - x*
k2 + 2ηgAc + 2t(c + η∖ Bc
k=0 i∈Sr
2tηg Ct '
P	c，)
≤ ɪ E
+
P
P
≤ 二 kx0-x*k2 +( ɪ +
Pd	2 2p
t
EX kVf (XrT)- gr-1k2
r=1
0
+ (21P+P—0+p⅛ )E X kxr -XrTk2+(⅛+tηg)E X Ks XX kgr,k -成，0k2.
□
Theorem 2 (Convergence of FedPAGE in convex setting). Under Assumption 1 (and Assumption 2),
if we choose the sampling probability Pr = P = S for every r ≥ 1 and P0 = 1, the minibatch sizes
bi = min{M, ；4^W}, b2 = min(M, /8^W}, and the global and local step size
ηg = θ ((S(SN 二:),ηι = o ( N 5∕4KSLc√T )
then FedPAGE satisfies
1 R-1
R X E[f (xr+1)-f (x*)] ≤ Oo
r=0
(N3/4L
(SR
(N1/4L
,if s ≤ Nn
,if √N < S ≤ N3/4 .
,if N3/4 < S
O
+ e
+ e
CrNL 、
O(SR + e
32
Under review as a conference paper at ICLR 2022
Proof. First we have
ηgEr[f (xr+1) - f(x*) + δ(f (xr + 1) - f(Xr)]
≤ ηg 2L+λδ) Erkgr- Vf(Xr )k2 - 2 Erlw+1 - X*k2 + 2 kχr - X*k2
ηg L(X +I)(I+ 6
2
l^δ) ErkXr+ 1
—
-Xr k2 + ηg Ef(Xr )-gr ,xr- X)
Summing up the inequality and choosing b1 = min(M,	篇}, b2 = min{M, M；2S},
τ-1
X ηgE[f (Xr+1) - f (x*)]+ ηgδE[f (XT) - f (x0)]
r=0
-	-	T-1 /r . 0、
≤ - 1 EkXT - X*k2 + 1 kX0 - X*k2 + X ηg'+')Ekgr- Vf(Xr)k2
2	2	2L入
r=0
T-1
+ X (1 + δ)
r=0
L(X + I)ηg - 1
2
E∣∣Xr+1
T-1
-Xr k2 + X ηgEhVf (XrT)- gr-1, Xr - X*)
r=0
≤- 1 EkXT-X*k2 + 1 kX0
-X
*∣|2 , X ηηg(1 + δ) , cηg ,
k + r=4	+乐+
Ekgr-Vf(Xr )k2
T-1
+ X (1 + δ)
r=0
Lng(X +I) - 1 +	ηg	+	Tηg	+	ηgT	ʌ EkX-1
2	2cp(1 + δ)	pc0(1 + δ)	p2cc0(1 + δ))
-Xr k2
+ 库 + * E X K X X 1也-gr,ok2 + * kX0 - X*k2
∖ r Y) r=0 k=0 i∈Sr	t
≤ - 1 EkXT - X*k2 + 2I∣x0 -X
k2 + Tηg kχo - X*k2 + 6K 2L2η2
Tσ2I{b2 < M}
b2
*
T-1
+ X W1Ekgr - Vf(χr)k2
r=0
T-1
+ X(1 + δ) ∙ W2 ∙ Ekxr+1 - xrk2
r=0
+ 12K 2L2 η
T-1
X kVf (Xr )k2,
r=0
(< -1 EkXT - X*k2 + 1 kX0 - X*k2 + T kX0 - X*k2 + 6K 2L2η2
2	2	pc0
(√Sp3∕2Te)
T T /
+ X (1 +δ)
r=0 '
3 1 - p/3
∙ W2 + P—S—W1
∙ E∣Xr+1 - Xrk2
+	12K 2 L2η2
144K2L2η2	ll2 l 3eTp3/2
—p~^~w1 I X kVf(Xr)k2 + FH"
where we define
w1 ：=(吗詈 + 要 + 坐 + 12K 2L2η2
∖ 2Lλ	2p	p2 c0
W2 ：
Lηg (λ + 1) - 1 +	ηg	+	Tηg	+	ηgT	+ 12k2L2η2
2	+2cp(1 + δ) + pc0(1 + δ) + p2cc0(1 + δ) +	1 + δ
33
Under review as a conference paper at ICLR 2022
By choosing p = N,λ =	√1∕Sp,c =	√Sp∕L2,δ =	1/(S 1∕4p3∕4),ηg
θ ( L(1+√∕Psp)) )，C，=平，ni = θ ( N"皋L√) , we get
ng (1 + δ)
2Lλ
Cng
P
Tng C
P2 C0
Crlg + Ti
2p	p2
=(1 + δ)θ(	(1 + δ* ∙ SL}
U2(1 + Vn∕s)VnJ
=(1 + δ)O (—	S	—)
U2√N (1 + √N∕S)7
=(1 + δ)O
Tη2
=2 -2
p2
S
L2N (1 + √N∕S)
(1 + δ)O ( N4∕5L2),
=(1+δ)O (N⅛),
(1 + δ)O ( N3∕2L2),
=O (呼).
Then, we can verify that wι ∙ N∕S2 = (1 + δ) ∙ O(1). Similar to the proof of Theorem 4, we can
also verify that w2 = O(1), and we can choose ηg and ηι with a small constant such that
(1+δ) ∙w2+P⅛a wι)≤0,
6K 2L2η2
(√s*2Te) ≤嘿,
(12K 2Mt+τn2)+≡^ wi)今
3eTp3/2	3ηg eT
CKWI ≤T—
Then we have
T-1
X E[f(xr+1) - f(x*)]
r=0
T	E	T T-1	C E
≤δE[f(x0) - f (xτ)] + ɪ M-X*∣∣2 + eT + ɪ X ∣∣vf(χr)k2 + 3∣r
ηg	16	4l r=0	8
r	1 T-1	r E
≤δE[f (x0) - f(χT)] + ɪ M- x*k2 + 2 X E[f (Xr) - f (x*)] + 7e-.
ηg	2 r=0	16
Then we know that
r T-1	C	C	)
T X E[f (xr+1) - f (x*)] ≤2T(f (x0) - f(x*)) + -T-kx0 - x*k2 + 7∣.
r=0	ηg
Recall that
ηg = θ
((1 + NF )S
V l(1 + √N)
“ S + N 3/4) N
∖ L(S + √N)
Θ
(n⅛ )
if S ≤ Nn
if √N <s ≤ N3/4.
if N3/4 < S
34
Under review as a conference paper at ICLR 2022
We have
O (口
1 R-1
R X E[f (xr+1)-f (x*)] ≤ Oo
r=0
S SR
(N1/4L
八(NL
θ ( SR
SR
+ e) , if S ≤ √N
+ e ) , if √N < S ≤ N3/4 .
+ e) , if N3/4 < S
□
35