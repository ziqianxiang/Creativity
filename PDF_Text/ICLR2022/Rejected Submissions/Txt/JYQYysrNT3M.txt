Under review as a conference paper at ICLR 2022
Reinforcement Learning with Ex-Post Max-
Min Fairness
Anonymous authors
Paper under double-blind review
Ab stract
We consider reinforcement learning with vectorial rewards, where the agent re-
ceives a vector of K ≥ 2 different types of rewards at each time step. The agent
aims to maximize the minimum total reward among the K reward types. Differ-
ent from existing works that focus on maximizing the minimum expected total
reward, i.e. ex-ante max-min fairness, we maximize the expected minimum total
reward, i.e. ex-post max-min fairness. Through an example and numerical exper-
iments, we show that the optimal policy for the former objective generally does
not converge to optimality under the latter, even as the number of time steps T
grows. Our main contribution is a novel algorithm, Online-ReOpt, that achieves
near-optimality under our objective, assuming an optimization oracle that returns
a near-optimal policy given any scalar reward. The expected objective value un-
der Online-ReOpt is shown to converge to the asymptotic optimum as T increases.
Finally, we propose offline variants to ease the burden of online computation in
Online-ReOpt, and we propose generalizations from the max-min objective to
concave utility maximization.
1 Introduction
The prevailing paradigm in reinforcement learning (RL) concerns the maximization of a single
scalar reward. On one hand, optimizing a single scalar reward is sufficient for modeling simple
tasks. On the other hand, in many complex tasks there are often multiple, potentially competing,
rewards to be maximized. Expressing the objective function as a single linear combination of the
rewards can be constraining and insufficiently expressive for the nature of these complex tasks. In
addition, a suitable choice of the linear combination is often not clear a priori.
In this work, we consider the reinforcement learning with max-min fairness (RL-MMF) problem.
The agent accumulates a vector of K ≥ 1 time-average rewards %：T = (Vi：T,k)K=ι ∈ RK in T
time steps, and aims to maximize Emink∈{i,. ,κ} Vi：T,k]. The maximization objective represents
ex-post max-min fairness, in contrast to the objective of ex-ante max-min fairness by maximizing
mink∈{i,…,K} E[%：T,k].
Our main contributions are the design and analysis of the Online-ReOpt algorithm, which achieves
near-optimality for the ex-post max-min fairness objective. More specifically, the objective under
Online-ReOpt converges to the optimum as T increases. Our algorithm design involves a novel
adaptation of the multiplicative weight update method (Arora et al., 2012), in conjunction with
a judiciously designed re-optimization schedule. The schedule ensures that the agent adapts his
decision to the total vectorial reward collected at a current time point, while allowing enough time
for the currently adopted policy to converge before switching to another policy.
En route, we highlight crucial differences between the ex-ante and ex-post max-min fairness objec-
tives, by showing that an optimal algorithm for the former needs not converge to the optimality even
when T increases. Finally, our results are extended to the case of maximizing E[g(Vl：T)], where g
is a Lipschitz continuous and concave reward function.
1
Under review as a conference paper at ICLR 2022
2	Related Works
The Reinforcement Learning with Max-Min Fairness (RL-MMF) problem described is related to an
emerging body of research on RL with ex-ante concave reward maximization. The class of ex-ante
concave reward maximization problems include the maximization of g(E[H：T]), as well as its ex-
ante variants, including the long term average variant g(E[limτ→∞ 区：T]) and its infinite horizon
discounted reward variant. The function g : RK → R is assumed to be concave.
The class of ex-ante concave reward maximization problems is studied by the following research
works. Chow et al. (2017) study the case where g is specialized to the Conditional Value-at-Risk
objective. Hazan et al. (2019) study the case when g models the entropy function over the probability
distribution over the state space, in order to construct a policy which induces a distribution over the
state space that is as close to the uniform distribution as possible. Miryoosefi et al. (2019) study
the case of minimizing the distance between EM:T] and a target set in RK. Lee et al. (2019) study
the objective of state marginal matching, which aims to make the state marginal distribution match
a given target state distribution. Pareto optimality of E[V1：T] and its ex-ante variants are studied in
(Mannor & Shimkin, 2004; Gabor et al., 1998; Barrett & Narayanan, 2008; Van Moffaert & Nowe,
2014). Lastly, a recent work Zahavy et al. (2021) provides a unifying framework that encompasses
many of the previously mentioned works, by studying the problem of maximizing g(E%T]) and
its ex-ante variants, where g is concave and Lipschitz continuous. Our contributions, which concern
the ex-post max-min fairness E[mink∈{1,...,K} V1：T,k] and its generalization to the ex-post concave
case, are crucially different from the body of works on the ex-ante case. The difference is further
highlighted in the forthcoming Section 3.2.
Additionally, a body of works Altman (1999); Tessler et al. (2019); Le et al. (2019); Liu et al.
(2020) study the setting where g is a linear function, subject to the constraint that E[V1：T] (or its
ex-ante variants) is contained in a convex feasible region, such as a polytope. There is another line
of research works Tarbouriech & Lazaric (2019); Cheung (2019); Brantley et al. (2020) focusing
on various online settings. The works Tarbouriech & Lazaric (2019); Cheung (2019) focus on the
ex-post setting like ours, but they crucially assume that the underlying g is smooth, which is not the
case for our max-min objective nor the case of Lipschitz continuous concave functions. In addition,
the optimality gap (quantified by the notion of regret) degrades linearly with the number of states,
which makes their applications to large scale problems challenging. Brantley et al. (2020) focus on
the ex-ante setting, different from our ex-post setting, and their optimality gap also degrades linearly
with the number of states.
3	Model
Set up. An instance of the Reinforcement Learning with Max-Min Fairness (RL-MMF) problem is
specified by the tuple (S, s1, A, T, O). The set S is a finite state space, and s1 ∈ S is the initial
state. In the collection A = {As}s∈S, the set As contains the actions that the agent can take when
he is at state s. Each set As is finite. The quantity T ∈ N is the number of time steps.
When the agent takes action a ∈ As at state s, he receives the array of stochastic outcomes
(s0, U (s, a)), governed by the outcome distribution O(s, a). For brevity, we abbreviate the rela-
tionship as (s0,U(s,a)) ~ O(s,a). The outcome s0 ∈ S is the subsequent state he transits to. The
outcome U(s, a) = (Uk(s, a))kK=1 is a random vector lying in [-1, 1]K almost surely. The random
variable Uk(s, a) is the amount of type-k stochastic reward the agent receives. We allow the random
variables s0, U1 (s, a), . . . UK (s, a) to be arbitrarily correlated.
Dynamics. At time t ∈ {1, . . . T}, the agent observes his current state st. Then, he selects an action
at ∈ Ast. After that, he receives the stochastic feedback (st+1,Vt(st, at))〜O(st, at). We denote
Vt(st, at) = (Vt,k(st, at))kK=1, where Vt,k(st, at) is the type-k stochastic reward received at time t.
The agent select the actions {at }tT=1 with a policy π = {πt}tT=1, which is a collection of functions.
For each t, the function πt inputs the history Ht-1 = ∪tq-=11{sq, aq, Vq(sq, aq)} and the current state
{st}, and outputs at ∈ Ast . We use the notation atπ to highlight that the action is chosen under
policy π. A policy π is stationary if for all t, Ht-ι, St it holds that ∏t(Ht-ι, St) = π(st) for some
function ∏, where ∏(s) ∈ As for all s. With a slight abuse of notation, we identify a stationary
policy with the function π.
2
Under review as a conference paper at ICLR 2022
Objective. We denote Vnt = ɪ P；=i Vq(Sq,%) as the time average vectorial reward during
time 1 to t under policy π. The agent’s over-arching goal is to design a policy π that maximizes
E[gmin(V∏τ)], where gmin ： RK → R is defined as gmin(v) = mink∈{i,…,k} Vk. Denoting VnT,k
as the k-th component of the vector VnT, the value gmin( VnT) = mink VnT k is the minimum time
average reward, among the reward types 1, . . . , K . The function gmin is concave, and is 1-Lipschitz
w.r.t. k ∙ ∣∣∞ over the domain RK.
When K = 1, the RL-MMF problem reduces to the conventional RL problem with scalar reward
maximization. The case of K > 1 is more subtle. Generally, the optimizing agent needs to focus on
different reward types in different time steps, contingent upon the amounts of the different reward
types at the current time step. Since the max-min fairness objective could lead to an intractable
optimization problem, we aim to design a near-optimal policy for the RL-MMF problem.
3.1	Regret
We quantify the near-optimality of a policy π by the notion of regret, which is the difference between
a benchmark opt(P(gmi∩)) and the expected reward E[gmin(V∏T)]. Formally, the regret of a policy ∏
in a T time step horizon is
Reg(∏,T) = opt(P(gmin)) - E[gmin(VπT)].	(1)
The benchmark opt(P(gmin)) is a fluid approximation to the expected optimum. To define
opt(P(gmin)), we introduce the notation p = {p(s0|s, a)}s∈S,a∈As, where p(s0|s, a) is the prob-
ability of transiting to s0 from s, a. In addition, we introduce v = {v(s, a)}s∈S,a∈As, where
v(s, a) = E[U (s, a)] is the vector of the K expected rewards. The benchmark opt(P(gmin)) is
the optimal value of the maximization problem P(gmin). For any g : RK → R, we define
P(g): max g	v(s, a)x(s, a)
s∈S,a∈As
s.t. x(s, a) =	p(s|s0, a0)x(s0, a0) ∀s ∈ S	(2a)
a∈As	s0∈S,a0∈As0
X	x(s, a) = 1	(2b)
s∈S,a∈As
x(s, a) ≥ 0	∀s ∈ S, a ∈ As .	(2c)
The concave maximization problem P(gmin) serves as a fluid relaxation to RL-MMF. For each s ∈
S, a ∈ As, the variable x(s, a) can be interpreted as the frequency of the agent visiting state s and
taking action a. The set of constraints (2a) stipulates that the rate of transiting outofa state s is equal
to the rate of transiting into the state s for each s ∈ S, while the sets of constraints (2b , 2c) require
that {x(s, a)}s∈S,a∈As forms a probability distribution over the state-action pairs. Consequently,
opt(P(gmin)) is an asymptotic (in T) upper bound to the expected optimum.
Our goal is to design a policy π such that its regret1 Reg(T) satisfies
Reg(T) = opt(P(gmin)) — E[gmin(V∏T)] ≤ T	⑶
holds for all initial state s1 ∈ S and all T ∈ N, with parameters D, γ > 0 independent of T. We
assume the access to an optimization oracle Λ, which returns a near-optimal policy given any scalar
reward. For H ∈ RK, define the linear function g吧:RK → R as g^(w) = H>w = PK=I Gkwk.
The oracle Λ inputs H ∈ RK, and outputs a policy π satisfying
opt(P(go)) - E[go(V∏T)] = opt(P(go)) - EQVT] ≤ De	(4)
for all initial state s1 ∈ S and all T ∈ N, with parameters Dlin, β > 0 independent of T. By
assuming β > 0, we are assuming that the output policy π is near-optimal, in the sense that the
difference opt(P(g®))一 E[G> Vπτ] converges to 0 as T tends to the infinity. A higher β signifies a
1We omit the notation with π for brevity sake
3
Under review as a conference paper at ICLR 2022
faster convergence, representing a higher degree of near-optimality. We refer to H as a Scalarization
of v, with the resulting Scalarized reward being IdTv(s, a) for each s, a.
Our algorithmic frameworks involve invoking Λ as a sub-routine on different H’s. In other words,
we assume an algorithmic sub-routine that solves the underlying RL problem with scalar reward
(the case of K = 1), and delivers an algorithm that ensures max-min fairness (the case of K ≥ 1).
Finally, while the main text focuses on gmin , our algorithm design and analysis can be generalized to
the case of concave g, as detailed in Appendix C.
3.2 Comparison between maximizing E[gMIN(VInT)] AND gMIN(E[VπT])
Before introducing our algorithms, we illustrate the difference between the objectives of maximizing
E[gmin(V∏τ)] and gmin(E[Vπτ]) by the deterministic instance in Figure 1, with initial state si = so.
The figure depicts an instance with K = 2. An arc represents
an action that leads to a transition from its tail to its head. For
example, the arc from so to s` represents the action ao`, with
p(s` | so,
action a``
ao`) = 1. Likewise, the loop at s` represents the
with p(s` | s`, a``) = 1. Each arc is labeled with
(0)
(0)	(0)
(0)	(0)
(0)
Figure 1: States and actions are
represented by circles and arcs.
its vectorial reward, which is deterministic. For example, with
certainty We have V(so, ao') = (0) and V(s', a'') = (0).
Consider two stationary policies
π',πr, defined as π'(sr) = aro, π'(so) = ao',π'(s') = a'' and
(s') = a'o. The policy ∏' always seeks to transit to s', and then
πr(sr) = arr, πr(so) = aor,πr
loop at s' indefinitely, likewise for πr. With certainty, VnT =(一1/7), VnT = (1-1)/T).
The objective gmin(E[V∏T]) is maximized by choosing ∏ra∩ uniformly at random from the collection
{∏',∏r}. We have E[VπTn] = (1∕2-1∕(2T)), leading to the optimal value of 1/2 - 1∕(2T). More
generally, existing research focuses on maximizing g(E[VjπT]) for certain concave g, and the related
objectives of maximizing g(limT→∞ E[Vπτ]) or g(E[P∞=ι αtVt(st, a∏)]), where α ∈ (0,1) is the
discount factor. in these research works, a near-optimal policy π is constructed by first generating a
collection Π of stationary policies, then sampling π uniformly at random from Π.
Interestingly, πra∩ is sub-optimal for maximizing E[gmin(V4 * * 7⅛)]. Indeed, Pr(VlnTI = (i-i∕t))=
Pr(VnTn = CT)) = 1/2, so we have E[gmin(V∏Tn)] = 0 for all T. Now, consider the deter-
ministic policy πsw, which first follows π' for the first bT /2c time steps, then follows πr in the
remaining「T/2] time steps. We have V^∏sw k ≥ 1/2 - 2/t for each k ∈ {1, 2}, meaning that
gmin( VnTr) ≥ 1/2 - 2/T. Note that gmin(E[V⅛]) ≥ gmin(E[V∏T1]) - 2/T, so the policy ∏sw is also
near-optimal for maximizing gmin(E[VnT]).
Altogether, an optimal policy for maximizing gmin(E[V^1nT]) can be far from optimal for maximizing
E[gmin(VnT)]. In addition, for the latter objective, it is intuitive to imitate ∏sw, which is to partition
the horizon into episodes and run a suitable stationary policy during each episode. A weakness to
πsw is that its partitioning requires the knowledge on T . While our algorithm follows the intuition
to imitate πsw, we propose an alternate partitioning that allows does not require T as an input.
4	Online-ReOpt Algorithm for RL-MMF
We propose the online-Reopt algorithm, displayed in Algorithm 1. The algorithm runs in episodes.
An episode m ∈ {1, 2, . . .} starts at time τ(m) (defined in Line 2), and ends at time τ(m + 1) - 1.
Before the start of episode m, the algorithm computes the scalarization Hτ(m) based on the Mul-
tiplicative Weight Update (MWU), which we detail later. Then, the algorithm invokes the opti-
mization oracle Λ, which returns a policy πm that is near-optimal for the MDP with scalar rewards
rm = {rm(s, a)}s,a, where rm(s, a) = Hτ>(m)v(s, a). Note that we only assume a black-box access
to Λ, and the parameters Dlin , β do not need to be input to the Algorithm. Finally, the algorithm runs
policy πm during episode m. The online Re-opt algorithm is an anytime algorithm, since it does
not require T as an input. Rather, it requires knowing T only during the terminal time step T. To
complete the description of the algorithm, we provide the details about the scalarization.
4
Under review as a conference paper at ICLR 2022
Algorithm 1 Online-ReOpt for gmin
1:
2:
3:
4:
5:
6:
7:
8:
9:
10:
11:
12:
13:
Inputs: Optimization oracle Λ.
Set τ(m) = bm3/2c for m ∈ N.
for Episode m = 1, 2, . . . do
Define IOT (m) according to (5).
Compute policy πm J Λ(Oτ(m)).
for Time t = τ (m), . . . , τ(m + 1) - 1 do
Choose action at = πm(st).
Observe the outcomes Vt(st, at) and the next state st+1.
if t = T then
Break the for loops and terminate the algorithm.
end if
end for
end for
Scalarization by MWU. At a time step t, we define the scalarization Ot = {Ot,k }kK=1 as
exp -ηt-1 Ptj-=11 Vj,k (sj , aj )
PK=1 exp[-ηt-i Pj=I Vj,κ(Sj,aj)]
where
√log K
max{(t — 1)2/3,1}.
(5)
(6)
In particular, at the start of each episode m, we apply (5) with t = τ (m) in Line 4. For the case
m = 1, we have Oτ (1),k = 1/K for all k ∈ {1, . . . , K}, meaning that all reward types are assigned
with the same weight at the beginning. The exponent 2/3 in the learning rate ητ(m)-1 in (5) is
different from the conventional choice of 1/2 (Arora et al., 2012). Our exponent is chosen for
optimizing the resulting regret bound from our forthcoming analysis. We follow the approach in
Chapter 7 in (Orabona, 2019) to define a time-varying learning rate.
The scalarization Ot by (5) promotes max-min fairness. Consider two reward types k, k0 with
Ptq-=11 Vq,k(sq,aq) > Ptq-=11 Vq,k0(sq,aq). We have Ot,k0 > Ot,k, meaning that a higher weight
is assigned to reward type k0 than type k. This implies that there is a higher emphasis on increasing
the type-k0 reward, which is in shortage as compared to type k, than the type-k reward. Hence,
max-min fairness is promoted.
4.1 Theoretical Guarantees
We provide the following theoretical guarantee for Online-ReOpt.
Theorem 1 Consider the RL-MMF problem. Online-ReOpt, displayed in Algorithm 1, satisfies
叫m 114√l0gK ɪ 144Dlin	G
Reg(T) ≤ τ 1/3 + τβ∕3 ,	⑺
where Dlin , β are parameters related to the optimization oracle Λ.
Theorem 1 is a generalization result, in the sense that it generalizes the ability of achieving near-
optimality for the case of K = 1 to the case of K ≥ 1. Indeed, as long as β > 0, meaning that
the regret of the optimization oracle Λ diminishes with a growing T on any RL with scalar reward
problem, the regret bound (7) in Theorem 1 also tends to zero as T increases.
Theorem 1 is proved in Appendix section C.3. The first regret term in (7) arises from two sources:
(a) the regret of the MWU algorithm, (b) the update delay on the scalarization due to the episodic
structure. To elaborate on (b), consider a time step t in episode m. Recall that the scalarization Ot by
(5) promotes max-min fairness, and ideally we should have employed the policy returned by Λ(Ot)
at time t. In contrast, in Online-ReOpt the action at is determined by πm, the output of Λ(Oτ(m)).
5
Under review as a conference paper at ICLR 2022
Item (b) accounts for the regret due to using HT(m) rather then "t. We crucially use the fact that Ht
are slowly changing in t so that the resulting regret is still diminishing with t.
The second term in (7) is due to the regret of the optimization oracle Λ. The exponent β∕3 in the term
is less than the exponent β in (4), as each policy ∏m is run for only T(m +1) - T(m) = O(，丁(m))
many time steps. Our design of {τ (m)}mM=1 allows a shorter time frame for πm to converge to its
expected reward, as compared to running a policy for T steps in (4). When we increase the episode
length T(m+ 1) -T(m), the regret due to (b) increases, while the regret due to the second term in (7)
decreases, and vice versa. Our design of {T (m)}mM=1 strikes an optimal (in terms of our analysis)
balance between these two sources of regret.
The regret bound (7) does not feature a direct dependence on the sizes of the state and action spaces.
The dependence on the hardness of the underlying MDP is only reflected through the parameters
Dlin, β. Therefore, apart from the deterioration of the exponent β to β∕3 and the first term in (7),
our algorithm does not introduce any overhead in the generalization from the case of K = 1 to
the case of K ≥ 1. Improving the exponent β∕3 in (7) is an interesting open question. Finally,
Theorem 1 is generalized to the case of maximizing a concave utility objective E[g(V∕jπτ)], where
g is Lipschitz continuous and concave. We detail the generalization in the model, algorithm and
theoretical results in Appendix C.1.
4.2 Offline Variants to Online-ReOpt
While the Online-ReOpt Algorithm achieves near-optimality, the efficiency of its implementation
could be hindered by the need of online computation in Line 5 in Algorithm 1. Indeed, in order to
compute πm, the agent has to input the optimization oracle Λ and the scalarization Hτ(m), which is
only known at the end of time step T(m) - 1. In the case when the optimization oracle involves
heavy computation, for example training deep neural networks, such online computation may not be
realistic.
In this section, we propose Offline-ReOpt, which is a variant of Online-ReOpt that does not require
invoking Λ during the horizon. The Offline-ReOpt is obtained from the Online-ReOpt by modifying
two lines in Algorithm 1, as enumerated below. The full algorithm of Offline-ReOpt is provided in
Appendix section A.1.
1.	Replace the input of Λ in Line 1 with the input of the policy family Π = {(H, n(”))}户∈ω.
The index set Ω is a finite subset of {H ∈ RK : ||训1 = 1,H ≥ 0}, the collection of all
possible scalarizations. For each H ∈ Ω, the policy n(")is the output of Λ(H).
2.	Replace the online computation in Line 5 with these two lines:
(a)	Identify HT(m) ∈ Ω that achieves min^∈Ω IIH - HT(m) ∣∣1.
〃d
(b)	Select policy ∏m1 = n(%(m)).
In item (1), all the policies in Π are computed before the execution of the algorithm, unlike the case
in Online-ReOpt. Consequently, in item (2), the selection of policy πm does not require invoking
the optimization oracle Λ.
The main idea behind item (2) is that, in the case when the desired scalarization HT(m) does not lie in
Ω, We chooses the surrogate SCalanZatiOn HT(m) that is closest to HT(m), so that the resulting policy
〃d
∏(%(m)) will be a reasonable approximation to the desired policy ∏M(m)).
In order for the surrogate HT(m) to be close to HT(m), it is desirable for the finite index set Ω to be so
diverse that every scalarization HT(m) would be in a close neighborhood of a scalarization in Ω. We
propose two families of Ω for the desired diversification. The first is the random point family, de-
tailed in Appendix section A.2. The family is constructed by sampling random points in the domain
{H ∈ RK : kHk1 = 1, θ ≥ 0} of all possible scalarizations. The second is the imitation based fam-
ily, also detailed in Appendix section A.2. The family is constructed by first running Online-ReOpt
multiple times, then collecting the scalarizations and the corresponding policies generated.
6
Under review as a conference paper at ICLR 2022
5 Experiments
We evaluate our proposed algorithms and benchmark algorithms in a controlled queueing system
involving vectorial rewards. For each of the algorithms, we first run the algorithm for Zpo = Zan × Ξ
independent trials, resulting in the Zpo average vectorial rewards2 {V(Zn,ξ>i≤Zan≤Zan,1≤ξ≤Ξ∙ We
plot the following three quantities against T :
•	Ex-post Fairness: Ψ = Zn 1 PZnn=I Pf=1 gmn (V(Zn,ξ)), an estimate to E[gaιιn(V1τ)].
•	EX-ante FairneSS： γ = Zn PZnn=I gmn (⅛ pξ=i v⅛,ξ)) ,an estimate to gmin(E[V⅛:T]).
•	Type k rewards for each 1 ≤ k ≤ K ： φk = Zn ⅛ PZnn=I pξ=⅛ v⅛f, an estimate to
E[Vi：T,k ].
We define the upper and lower error bars respectively as the 75 and 25-percentiles of the data, see
Appendix section B.1 for details. For the forthcoming discussions, we denote ek as the k-th standard
basis vector for k ∈ {1, . . . K} in RK. In addition, we denote 1K, 0K as the all one vector and the
all zero vector in RK .
5.1 Queuing Network
Queuing problems are studied extensively due to their relevance in fields such as manufacturing and
in communication systems. In our evaluation, we focus on a discrete-time queuing system. The
queuing network that we have tested our algorithms on, consisting of two servers and four queues
arranged in a bidirectional fashion, has been previously studied in works by Rybko & Stolyar (1992),
Kumar & Seidman (1990), Chen & Meyn (1998), de Farias & Van Roy (2003) and Banijamali et al.
(2019). This network is shown in Figure 2.
Figure 2: A bi-directional four-queue network
There are two servers in the system. Server 1 only serves Queue 1 or Queue 4 with service rates
μ⅛ = 0.3 and μ4 = 0.3 respectively, and where Server 2 similarly only serves Queue 2 or Queue 3
at the rates of μ2 = 0.3 and μ3 = 0.3. Arrivals occur at a rate of λ⅛ = 0.2 and λ2 = 0.2 at Queues
1 and 3 respectively. An arrival that gets served at Queue 1 by Server 1 then progresses to Queue 2,
and only leaves the system after it has been served by Server 2. Likewise, arrivals at Queue 3 have
to be served by Server 2, before moving on to Queue 4 to be served by Server 1 in order to leave
the system. Each queue i has a maximum length of Li = 9, and a customer is rejected at a queue if
the queue is at its full capacity. Conversely, an empty queue remains at length 0 even if an action is
taken to serve that queue.
The state of the system is thus defined by the vector xt = (xt,⅛, xt,2 , xt,3 , xt,4) whereby xt,i rep-
resents the length of the queue i at time t. At each time step t, a decision has to be made by each
server to serve only one or neither of its queues, which we can represent by a 4-component vector
at = (at,⅛, at,2, at,3, at,4) ∈ {0, 1}4, where at,i = 1 indicates the decision to serve Queue i at time
t, and at,i = 0 otherwise. Note that the condition of being able to only serve one queue at each
server naturally imposes the constraints at,⅛ + at,4 ≤ 1 and at,2 + at,3 ≤ 1 at each t, meaning that
As = {a ∈ {0, 1}4 : a⅛ + a4 ≤ 1, a2 + a3 ≤ 1} for each s.
2To avoid clutter, we omit the upper-script for the algorithm.
7
Under review as a conference paper at ICLR 2022
The transition dynamics for the system can then defined by the following equation when 0 < xt,i <
Li, where ei refers to the basis vector in R4:
Xt + eι
Xt + e3
Xt + e - eι
Xt+1 = Xt - e2
Xt + e4 - e3
Xt - e4
、xt
with probability λ1
with probability λ2
with probability μ1a1
with probability μ2a2
with probability μ3a3
with probability μ4a4
otherwise
(8)
We define the type-i reward at time t as Vt,i(X, a) = 1 - 誉,for i ∈ {1,..., 4}. Recall that xt,i is
the queue length of Queue i at time t. The reward Vt,i(X, a) is equal to 1 if Queue i is empty, and the
reward Vt,i(X, a) decreases linearly with the length of Queue i at time t. In particular, Vt,i(X, a) = 0
if Queue i is full. Altogether, the agent’s reward for Queue i at time t positively correlates with the
degree of idleness of the Queue. The maximization of gmin(V1:T) = min1≤i≤4 V⅛,i is equivalent
to the minimization of time-average queue lengths among all queues, hence enforcing all queues to
be stable simultaneously.
5.1.1 S imulation Results
In our simulation, we evaluate 5 algorithms. Three of them are our proposed algorithms, namely
Online-ReOpt, Offline-ReOpt with Random Point Family and Offline-ReOpt with Imitation based
family. The other two are existing baselines. The Meta Algorihtm by Zahavy et al. (2021) is
the state-of-the-art for maximizing gmin(E[V1:T]), while the longer queue first heuristic is a well-
established algorithm in the queuing theory literature. As the name suggests, each server serves
the longer of the two queues at each time round. We ran each of the algorithms with the following
parameters: T = 100000, Zan = 10 and Ξ = 100, meaning Zpo = Zan × Ξ = 1000.3 All of the al-
gorithms employ the same optimization oracle Λ, with the same hyper-parameters and architecture,
a double deep Q-learning network algorithm (Double DQN) by Hasselt et al. (2016).
Figure 3: Ex-ante Fairness of various algorithms in a queuing network
Figure 3 plots the quantity Ψ against T under the 5 algorithms. Notice in Figure 3 how the Of-
fline and Online-ReOpt algorithms, as well as the Meta Algorithm by Zahavy et al. (2021) perform
similarly well in terms of ex-ante fairness. Among them, the Meta Algorithm has the best perfor-
mance, since the Re-Optimization schedule in our proposed algorithms compromises the ex-ante
fairness objective. All algorithms demonstrate converging behavior, in the sense that the error bars
diminishes as T grows.
3Except for Online-ReOpt, where we set Ξ = 5 since running an online algorithm for 1000 trials is not as
practical as running an offline algorithm, which only needs to be trained once.
8
Under review as a conference paper at ICLR 2022
Figure 4: Ex-post Fairness of various algorithms in a queuing network
Figure 4 plots the quantity Γ against T under the 5 algorithms. In terms of ex-post fairness, the Of-
fline and Online-ReOpt algorithms perform significantly better than Meta Algorithm and the Longer
Queue First Heuristic. The sub-optimality of the Meta Algorithm corroborates with Section 3.2 that
policies designed for the ex-ante fairness objective could be far from optimal for the ex-post fairness
objective. While the Meta Algorithm has a similar performance to the Longer Queue First heuristic,
the former has a significantly wider error bar than the latter, meaning that the latter is more stable.
Figure 5: Type k-rewards for 1 ≤ k ≤ K of various algorithms in a queuing network. Figure is read
from left to right, top to bottom.
Figure 5 plots Φi against T for i ∈ {1, . . . , 4}. In a nutshell, the plotted lines explain the trends in
Figure 3, while the error bars shed light on the trends in Figure 4. Firstly, the plotted lines indicate
that the Meta Algorithm has the highest (or close to the highest) individual average reward Φi for
each queue, signifying that the Meta Algorithm has the highest E[V1:T,i] for each i ∈ {1, . . . , 4}.
This explains the superiority of the Meta Algorithm shown in Figure 3.
When we focus on the error bars, the plots in Figure 5 tell a different story. Notably, the error bars
for the Meta Algorithm is significantly wider than others, meaning that the Zpo trials of the Meta
Algorithm have significantly different results from one another.4 When we unpack the summands
in Φ1, . . . , Φ4 and compute the minimum reward in each trial, it results in Figure 4, which is vastly
different from Figure 3, signifying the ex-ante and ex-post objectives are fundamentally different.
As a final remark, our numerical experiments do not imply that the Longer Queue Heuristic is a
worse algorithm than the other 4 algorithms. Indeed, the Longer Queue Heuristic does not require
the knowledge of λι ,λ2 ,μ1,...,μ4, whereas the other 4 algorithms crucially uses these parameters
for generating their policies. In addition, the Longer Queue Heuristic is computationally much less
onerous than the others. Finally, the Longer Queue Heuristic demonstrates converging behaviors in
all the plots, in the sense that the error bars diminish when T increases.
4It is helpful to revisit Section 3.2, where Zpo trials would result in ≈ Zpo/2 outcomes of 1-01/T and
≈ ZPo/2 outcomes of (1—1/T).
9
Under review as a conference paper at ICLR 2022
References
E. Altman. Constrained Markov Decision Processes. Chapman and Hall, 1999.
Sanjeev Arora, Elad Hazan, and Satyen Kale. The multiplicative weights update method: a meta-
algorithm and applications. Theory ofComputing, 8(6):121-164, 2012.
Ershad Banijamali, Yasin Abbasi-Yadkori, Mohammad Ghavamzadeh, and Nikos Vlassis. Optimiz-
ing over a restricted policy class in mdps. In Proceedings of the Twenty-Second International
Conference on Artificial Intelligence and Statistics, volume 89, pp. 3042-3050. PMLR, 16-18
Apr 2019.
Leon Barrett and Srini Narayanan. Learning all optimal policies with multiple criteria. In Proceed-
ings of the 25th International Conference on Machine Learning, ICML ’08, pp. 41-47, New
York, NY, USA, 2008. Association for Computing Machinery. ISBN 9781605582054. doi:
10.1145/1390156.1390162. URL https://doi.org/10.1145/1390156.1390162.
Kiante Brantley, Miroslav Dudik, Thodoris Lykouris, Sobhan Miryoosefi, Max Simchowitz, Alek-
sandrs Slivkins, and Wen Sun. Constrained episodic reinforcement learning in concave-convex
and knapsack settings. In Advances in Neural Information Processing Systems 33, 2020.
R.-R. Chen and S. Meyn. Value iteration and optimization of multiclass queueing networks. In Pro-
ceedings of the 37th IEEE Conference on Decision and Control (Cat. No.98CH36171), volume 1,
pp. 50-55 vol.1, 1998. doi: 10.1109/CDC.1998.760588.
Wang Chi Cheung. Regret minimization for reinforcement learning with vectorial feedback and
complex objectives. In Advances in Neural Information Processing Systems 32, pp. 724-734,
2019.
Yinlam Chow, Mohammad Ghavamzadeh, Lucas Janson, and Marco Pavone. Risk-constrained rein-
forcement learning with percentile risk criteria. J. Mach. Learn. Res., 18(1):6070-6120, January
2017. ISSN 1532-4435.
D. P. de Farias and B. Van Roy. The linear programming approach to approximate dynamic pro-
gramming. Operations Research, 51(6):850-865, 2003.
Zoltan Gabor, Zsolt Kalmar, and Csaba Szepesvari. Multi-criteria reinforcement learning. In Pro-
ceedings of the Fifteenth International Conference on Machine Learning, ICML ’98, pp. 197-205,
San Francisco, CA, USA, 1998. Morgan Kaufmann Publishers Inc. ISBN 1558605568.
Hado van Hasselt, Arthur Guez, and David Silver. Deep reinforcement learning with double q-
learning. In Proceedings of the Thirtieth AAAI Conference on Artificial Intelligence, AAAI’16,
pp. 2094-2100. AAAI Press, 2016.
Elad Hazan, Sham Kakade, Karan Singh, and Abby Van Soest. Provably efficient maximum en-
tropy exploration. In Proceedings of the 36th International Conference on Machine Learning,
volume 97, pp. 2681-2691. PMLR, 09-15 Jun 2019.
P.R. Kumar and T.I. Seidman. Dynamic instabilities and stabilization methods in distributed real-
time scheduling of manufacturing systems. IEEE Transactions on Automatic Control, 35(3):
289-298, 1990. doi: 10.1109/9.50339.
Hoang Le, Cameron Voloshin, and Yisong Yue. Batch policy learning under constraints. In Pro-
ceedings of the 36th International Conference on Machine Learning, volume 97, pp. 3703-3712.
PMLR, 09-15 Jun 2019.
Lisa Lee, Benjamin Eysenbach, Emilio Parisotto, Eric P. Xing, Sergey Levine, and Ruslan Salakhut-
dinov. Efficient exploration via state marginal matching. CoRR, abs/1906.05274, 2019. URL
http://arxiv.org/abs/1906.05274.
Yongshuai Liu, Jiaxin Ding, and Xin Liu. Ipo: Interior-point policy optimization under con-
straints. Proceedings of the AAAI Conference on Artificial Intelligence, 34(04):4940-4947,
Apr. 2020. doi: 10.1609/aaai.v34i04.5932. URL https://ojs.aaai.org/index.php/
AAAI/article/view/5932.
10
Under review as a conference paper at ICLR 2022
Shie Mannor and Nahum Shimkin. A geometric approach to multi-criterion reinforcement learning.
J. Mach. Learn. Res., 5:325-360, 2004.
Sobhan Miryoosefi, Kiante Brantley, Hal Daume III, Miro Dudik, and Robert E Schapire. Reinforce-
ment learning with convex constraints. In Advances in Neural Information Processing Systems
32, pp. 14093-14102. 2019.
Francesco Orabona. A modern introduction to online learning. CoRR, abs/1912.13213, 2019.
A. N. Rybko and Aleksandr Stolyar. On the ergodicity of stochastic processes describing functioning
of open queueing networks. Problemy Peredachi Informatsii, (3):3-26, July 1992. ISSN 0555-
2923.
Shai Shalev-Shwartz. Online learning: Theory, algorithms, and applications, Jul 2007.
Shai Shalev-Shwartz. Online learning and online convex optimization. Found. Trends Mach. Learn.,
4(2):107-194, 2012.
Jean Tarbouriech and Alessandro Lazaric. Active exploration in markov decision processes. In The
22nd International Conference on Artificial Intelligence and Statistics, AISTATS 2019, volume 89,
pp. 974-982. PMLR, 2019.
Chen Tessler, Daniel J. Mankowitz, and Shie Mannor. Reward constrained policy optimization. In
International Conference on Learning Representations, 2019.
Kristof Van Moffaert and Ann Nowe. Multi-objective reinforcement learning using sets of pareto
dominating policies. J. Mach. Learn. Res., 15(1):3483-3512, January 2014. ISSN 1532-4435.
Tom Zahavy, Brendan O’Donoghue, Guillaume Desjardins, and Satinder Singh. Reward is enough
for convex mdps. ArXiv, abs/2106.00661, 2021.
11