Under review as a conference paper at ICLR 2022
Quasi-potential theory for escape problem:
Quantitative sharpness effect on SGD’s es-
CAPE FROM LOCAL MINIMA
Anonymous authors
Paper under double-blind review
Ab stract
We develop a quantitative theory on an escape problem of a stochastic gradient
descent (SGD) algorithm and investigate the effect of sharpness of loss surfaces
on the escape. Deep learning has achieved tremendous success in various domains,
however, it has opened up various theoretical open questions. One of the typical
questions is why an SGD can find parameters that generalize well over non-convex
loss surfaces. An escape problem is an approach to answer this question, which
investigates how efficiently an SGD escapes from local minima. In this paper,
we introduce quasi-potential from traditional large deviation theory to the escape
problem. Our novel formulation can quantify escaping effect without relying on
auxiliary variables and is applicable to including discrete setup. Our theory find
that (i) sharpness of a minimum exponentially slows down the escaping, (ii) but
the SGD’s noise cancels the effect, which leads to exponentially fast escape from
sharp minima, as suggested by (Xie et al., 2020). We also conduct experiments to
empirically validate our theory using neural networks trained with real data.
1	Introduction
In recent years, the successes of deep learning have been a major driving force of machine learn-
ing development (LeCun, 2019). Owing to its strong generalization capability, deep learning has
diverged into a wide range of domains, such as computer vision (Krizhevsky et al., 2012), speech
recognition (Mikolov et al., 2011), and natural language processing (Collobert et al., 2011). The
high performance of deep learning is underpinned by gradient-based learning algorithms, including
stochastic gradient descent (SGD) and its variations (Kingma & Ba, 2014; Schmidt et al., 2021).
However, at the same time, those unprecedented successes raise a question:
Why does SGD learn parameters of neural networks with high generalization performance?
Although the optimization problems of neural networks were thought to be difficult to solve (Blum &
Rivest, 1992), SGD can find nearly optimal solutions empirically, and further, the obtained solutions
generalize well (Keskar et al., 2016; Brutzkus et al., 2017). Analyzing SGD’s role on deep learning
is an area of research that is currently attracting strong interest (Masters & Luschi, 2018; Jastrzebski
et al., 2021).
One of the promising directions for this question is to study the geometric properties of a training
loss landscape. Many empirical studies have found that minima obtained by SGD have distinctive
geometric properties. Keskar et al. (2016) have shown that the shape of the minima obtained by
SGD tends to be flat. He et al. (2019b) have deepened the investigation by picturing that SGD
settles on the flatter side of asymmetric loss surface, which they named “asymmetric valley." Draxler
et al. (2018) and Garipov et al. (2018) have shown that separate minima obtained by independent
training processes are internally connected through pathways. Li et al. (2017) have proposed a
dimension reduction technique to visualize the geometry of loss surfaces, visually confirming flat
minima. Most significantly, Jiang et al. (2019) conducted large-scale experiments and verified that
minima in flat and wide regions have the strongest correlations with generalization capabilities. To
attain a theoretical understanding of SGD, it is key to quantitatively analyze the connection between
SGD and the geometric properties of loss surfaces.
1
Under review as a conference paper at ICLR 2022
Figure 1: Visual illustration of steepness (Definition 3). The steepness of φ, S0T(φ), is greater than
S0T(ψ) because φ moves against the vector field of gradient -NL(θ).
—► Steep Trajectory
—► Less Steep Trajectory U
An escape problem is a scheme to analyze the dynamic of SGD escaping from local minima (Zhu
et al., 2019; JaStrZebSki et al., 2017; HU et al., 2019; Nguyen et al., 2019; Xie et al., 2020). This
scheme allows us to investigate why SGD avoids (potentially) bad local minima and settles on good
minima. Zhu et al. (2019) first investigated the SGD’s escape phenomenon and showed that SGD’s
escape is enhanced by its unique noise structure, called the “anisotropic noise structure." Invoked
by their analysis, many studies have been attempting to theoretically quantify this phenomenon. Hu
et al. (2019) rigorously identified the role of learning rate in escaping. Nguyen et al. (2019) used
the Levy process to provide the precise description of SGD as well as its escaping phenomena. Jas-
trzebski et al. (2017)developed a theory of stochastic differential equation and quantified how the
anisotropic noise affects its fast escape from sharp minima. Xie et al. (2020) refined the mathemati-
cal aspect and showed that the SGD’s noise structure exponentially enhances escaping under a setup
of diffusion theory.
In this paper, we introduce a quasi-potential theory to the escape problem, and investigate a mean
exit time, which formally quantifies escaping. The notion of quasi-potential is defined in a fundamen-
tal theory of stochastic dynamical systems, named a Large Deviation Theory (Freidlin & Wentzell,
2012; Dembo & Zeitouni, 2010). Quasi-potential can formulate the distribution of trajectories that
a stochastic dynamical system takes. To illustrate quasi-potential for SGD’s escaping problem, we
introduce an intuitive notion, steepness of a trajectory (Fig. 1 and Definition 3), and show that it is
an effective tool to analyze the SGD’s escaping. To the best of our knowledge, this is the first work
that applies the quasi-potential to formalize the relationship between SGD’s escape and sharpness.
Our main findings and contributions are as follows:
•	We develop a novel quasi-potential theory that rigorously describes the escape of SGD with
no auxiliary variables. Our theory can concisely incorporate the effect of essential factors,
i.e., a batch size (B), a learning rate (η), and geometric parameters of loss surfaces (r and
λ)
•	Our theory can be flexibly applied to several practical setups: SGD with discrete update
and state-dependent gradient noise, while those were sometimes omitted in previous works
for mathematical convenience.
•	We find that a loss surface with sharp minima slows down the escape of SGD, which seems
to contradict the common knowledge, i.e., SGD escapes efficiently from sharp minima. We
show that our result does not contradict the common knowledge but is a generalization of
existing results, such as (JaStrZebSki et al., 2017) and (Xie et al., 2020).
1.1 Comparison with Existing Studies on Escape Problem
In Table 1, we compare the escape time derived with the results of other studies that analyze the
escape problem. There are two main points of focus. The first is the time to escape that we derive.
Our theory provides a unified analysis of exit time incorporating all the essential parameters, batch
2
Under review as a conference paper at ICLR 2022
Study	Time to escape	Non- stationary	Parameter dependent noise	Discrete setup
Hu et al. (2017)	H exp η-~ 1]	√	√	
Jastrzebski et al. (2017)	eχp h B δ Li q deH		√	
Zhu et al. (2019)	N/A	√	√	
Nguyen et al. (2019)	2√α (1 + O (ηδ/2))	√		√
Xie et al. (2020)	∣λπ∣ exp h2 B δL (λ + ⅛0f )i		√	
Ours (continuous SGD)	exp [2 B r2 λ 2 ]	√	√	
Ours (discrete SGD)	exp [2 B r2 λ 2 ] + O (√η)	√	√	√
Table 1: Comparison of the studies on the escape problem. B is batch size, η is a learning rate,
r is a radius of the region around a minimum, H is a Hesse matrix of loss functions at a minima,
and λ = λmin(H). Further, H0 is a Hesse matrix on one of the neighboring points of the mini-
mum and λ0 is one of its eigenvalues of H0 . ∆L is a difference of training loss values within a
neighborhood of minimum, α ∈ (0, 2] is an index of heavy-tailedness of gradient noise in SGD, and
δ ∈ (0, 1) and s ∈ (0, 1) are values that implicitly include various factors of the escaping problem.
"Non-stationary" denotes whether the result holds without assuming that SGD reaches a stationary
distribution before escaping. "Parameter dependent noise" denotes whether noise in SGD depends
on current parameters. "Discrete setup" means whether the analysis is valid with a discrete update
by SGD. Our theory has two main advantages: (i) it explicitly quantifies essential factors of escape
without relying on auxiliary variables, such as s, δ and ∆L, and (ii) it is applicable to a wide range
of the settings. Finally, we note that although our results’s dependency on λ seems to be opposite to
(Jastrzebski et al., 2017) and (Xie et al., 2020), those are all Consisitent because ∆L has an implicit
dependency on λ and r2 under Assumption 1.
size, learning rate, a radius of the region around a minimum, and sharpness of a minimum. As a
consequence, we show that the eigenvalues of the Hesse matrix increase the time to escape, which
has not been found in other studies. Such effect was less apparent in previous studies such as
(Jastrzebski et al., 2017) and (Xie et al., 2020) because part of the sharpness dependency is hidden
in ∆L. Provided that ∆L = r2λ under Assumption 1, one can see that our results are consistent
with previous studies.
The second is our theory,s flexibility. Different from Jastrzebski et al. (2017) and Xie et al. (2020),
our theory does not require that SGD reaches the stationary distribution before escaping, which is
known to take exponentially many iterations (Xu et al., 2017; Raginsky et al., 2017). Additionally,
our theory can evaluate the correspondence with the practical SGD, which has a discrete update rule
and state-dependent noise.
Notations: For a k × k matrix A, λj (A) is the j-th largest eigenvalue of A, and λmax(A) = λ1(A)
and λmin(A) = λk (A) denote the largest and the smallest eigenvalue of a square matrix. O (∙)
denotes Landau,s Big-O notation. ∣∣∙∣∣ denotes the Euclidean norm. Given a time-dependent function
.
θt, θt denotes the differentiation of θt with respect to t. N (μ, Σ) denotes the multivariate Gaussian
distribution with the mean μ, and the covariance Σ.
2 Setting and Problem
2.1 Stochastic Gradient Descent and Dynamical System
Consider a learning model parameterized by θ ∈ Rd. Given training examples {xi }iN=1 and a
loss function '(θ, Xi), We consider a training loss L(θ):= 方 PN=I '(θ, Xi) and a mini-batch loss
3
Under review as a conference paper at ICLR 2022
LB (θ) := B Pχi∈B '(θ, Xi), where B is a randomly sampled subset of the training data such that
∣B∣ = B. We assume that L(θ) is differentiable and its derivative NL(θ) is LiPSchitz continuous.
We mainly consider two types of SGD: a discrete SGD and a continuous SGD. Although a discrete
SGD is used in practice, we study continuous SGD as a starting point of our analysis because of
its mathematical convenience. This is a widely used approach in general SGD analyses (Ali et al.,
2019; Advani et al., 2020) as well as in the escaping analyses (JaStrzebSki et al., 2017; Xie et al.,
2020).
Discrete SGD: First, we give the usual discrete formulation of SGD. Given an initial parameter
θ0 ∈ Rd, SGD generates a sequence of parameters {θk}k∈N by the following update rule:
θk+1 = θk - ηNLB(θk),	(1)
for k ∈ N, where η > 0 is a learning rate.
In particular, we focus on SGD whose noise on gradients has a Gaussian distribution. We decompose
-NLB (θk) in (1) into a gradient term -NL(θk) and a noise term NL(θk) -NLB (θk), and consider
a case that the noise is Gaussian. With this setting, the update rule in (1) is rewritten as
θk+1 = θk - NL(θk) + ^BWk,	(2)
where Wk 〜N (0, ηC(θk)) is a parameter-dependent Gaussian noise with its covariance C(θ):=
Ei~Uni({ι,...,N}) [(NL(θ) — R' (θ,Xi))> (NL(θ) — R' (θ,Xi))] .We assume that C(θ) is Lipschitz
continuous.
The Gaussianity of the noise on gradients is justified by the following reasons: (i) if the batch size B
is sufficiently large, the central limit theorem ensures the noise term becomes Gaussian noise, and
(ii) several empirical studies show that the noise term becomes Gaussian noise (Mandt et al., 2016;
Jastrzebski et al., 2017; He et al., 2019a), although different findings have been obtained in other
settings (Simsekli et al., 2019).
Continuous SGD: We also give a continuous SGD, which is exactly discretized to (2) by a classic
Euler scheme (Definition 5.1.1 of Gobet (2016)). With a time index t ≥ 0 and the given initial
parameter θ0 ∈ Rd, the continuous dynamic of SGD is written as follows:
θ t = —NL (θt) + ^JCC (θt )1 /2 Wt	(3)
where wt is a d-dimensional Wiener process, i.e. an Rd-valued stochastic process with t such that
wo = 0 and Wt+U — Wt 〜 N(0, uI) for any t,u> 0. We note this system can be seen as a Gaussian
perturbed dynamical system with a noise magnitude y∩B because η and B do not evolve by time.
2.2	Escape Problem and Mean Exit Time
We consider the problem on how SGD escapes from minima of loss surfaces. In this paper, our
target of interest is quantified by a notion of mean exit time for continuous SGD and discrete mean
exit time of discrete SGD. Let θ* ∈ Rd be a local minimum of loss surfaces, and D ⊂ Rd be a
r-neighborhood of θ* with r > 0. We define the mean exit time as follows:
Definition 1 (Mean exit time from D). Consider a continuous SGD starting from θ0 ∈ D. Then, a
mean exit time of the continuous SGD (3) from D is defined as
E[τ] := E[min {t : θt ∈/ D}].
Definition 2 (Discrete mean exit time from D). Consider a discrete SGD starting from θ0 ∈ D.
Then, a discrete mean exit time of the discrete SGD (2) from D is defined as
E[ν] := E[min {kη : θk ∈/ D}].
These definitions are common in quasi-potential theory (Freidlin & Wentzell, 2012; Gobet, 2016).
Intuitively, the smaller E[τ] or E[ν] becomes, the faster the system escapes from a region D. In other
words, the system has a stronger tendency to escape from θ*.
We remark that there are other formulations to analyze the escape problem. Zhu et al. (2019) define
escaping efficiency as Eθt [L (θt) — L (θo)]. Jastrzebski et al. (2017) and Xie et al. (2020) study a
ratio between the probability of coming out from θ* 's neighborhood and the probability mass around
θ*.
4
Under review as a conference paper at ICLR 2022
2.3	Basic Assumptions for the Escape Problem
We provide basic assumptions for the escape problem, commonly used in the literature (Mandt et al.,
2016; ZhU et al., 2019; Jastrzebski et al., 2017; Xie et al., 2020).
Assumption 1 (L (θ) is locally quadratic). There exists a matrix H * ∈ R d×d such that for any
θ ∈ D, the following equality holds:
∀θ ∈ D, L (θ) = L (θ*) + NL (θ*)(θ - θ* ) + 2( θ - θ*) > H * (θ - θ*)
Assumption 2 (Hesse covariance matrix). For any θ ∈ D, C(θ) is approximately equal to H*.
It is known that Assumption 2 holds around a critical point θ* (ZhU et al., 2019; Jastrzebski et al.,
2017). It is also empirically shown that Assumption 2 can approximately hold even for randomly
chosen θ (see Section 2 of Xie et al. (2020)). We further investigate a variant of Assumption 2 in
Section 5.
3 Quasi-potential Theory
We introduce the basic notions of the quasi-potential theory. We start with defining a notion of
steepness of a trajectory followed by the systems (3) on a loss surface L(θ). Let φ = {φt}t∈ [o,t] be
a trajectory over a finite time interval [0, T]. Or more formally, φ is a continuous map from [0, T]
to Rd, and is an element of C0T(Rd) which is a support of a dynamical process in [0, T]. Given a
trajectory φ and the system (3), We define the following quantity:
Definition 3 (Steepness). Steepness of a trajectory φ followed by (3) is defined as
S0TW) := 1『&t + RL M))> CSt)-1 /2 (St + RL M)) dt.
20
Steepness S0T (M) can be intuitively interpreted as the hardness of climbing that the system (3) is
exposed to while following the trajectory M on L(θ) (Fig. 1). This notion is generally utilized in the
field of dynamical systems, for example, and is called “normalized action functional" in Section 3.2
of Freidlin & Wentzell (2012) and “rate function" in Section 1.2 of Dembo & Zeitouni (2010).
Steepness is useful to describe a distribution of trajectories generated by dynamical systems. Ifa tra-
jectory M has a large steepness S0T (M), the probability that the dynamic system takes the trajectory
decreases exponentially. Formally, the distribution is analyzed as follows.
Lemma 1 (Theorem 3.1 in Section 3.3 Freidlin & Wentzell (2012)). For any δ, ζ > 0,M ∈
C0T Rd , and sufficiently small ε > 0, the following holds:
Pφ0 (M ∈ COT(Rd) I P(M M) < δ) ≥ exp {-ɛ-2 [S0t (M) + 4},
where ρ(M0, M) = supt∈[0,T] kM0t -Mtk.
Lemma 2 (Theorem 3.1 in Section 3.3 Freidlin & Wentzell (2012)). Let Φ(s)	=
M∈ C0T(Rd) I S0T (M) ≤ s . For any δ, ζ, s > 0 and sufficiently small ε > 0, we have:
P φ0 n M ∈ COT(R d ) 1 P ( M φ( S )) ≥ δ 0 ≤ exp {-ε-2( S- Z) },
where P(M Φ(S)) = inf中∈φ(S)P(M M).
Although we restrict our attention to the system (3), the same discussion is applicable to a general
class of diffusion processes and dynamical systems with Markov perturbations (For details, see
section 5.7 in Dembo & Zeitouni (2010) or Section 6.5 in Freidlin & Wentzell (2012)).
Although there are several trajectories from θ* to θ ∈ D with different steepness, a dominating
factor for mean exit time is the smallest steepness among them, which is called quasi-potential:
Definition 4 (Quasi-potential). Quasi-potential of θ ∈ D is defined as
V (θ) := inf inf	S0T (M).
t>0 W:(Wo,pτ)=(θ*,θ)
Similar to steepness, quasi-potential can be seen as the minimum effort the system (3) needs to climb
from θ* up to θ on L(θ). (For more details, see Section 5.3 of Freidlin & Wentzell (2012)).
5
Under review as a conference paper at ICLR 2022
4 Mean Exit Time Analysis for SGD
4.1	Assumptions
To analyze the mean exit time, the quasi-potential theory requires several assumptions regarding the
stability of the system (3) at θ*.
Assumption 3 (θ^ is asymptotically stable). For any neighborhood U that contains θ* ,there exists
a small neighborhood V of θ* such that gradient flow with any initial value θ 0 ∈ V does not leave
U for t ≥ 0 and lim→∞ θt = θ*.
Assumption 4 (D is attracted to θ*). ∀θ o ∈ D, gradient flow with initial value θ o converges to θ*
without leaving D as t → ∞.
.
where “gradient flow" means a continuous gradient descent defined as θt = -VL(θt).
Stability is a commonly used notion in dynamical systems (Hu et al., 2017; Wu et al., 2017), although
it does not always appear in SGD's escaping analysis (ZhU et al., 2019; JaStrzebSki et al., 2017;
Xie et al., 2020). Assumption 3 is known to be equivalent to the local minimality of θ* under
the condition that L(θ) is real analytic around θ* (Absil & Kurdyka, 2006). Also, by definition
of asymptotic stability in Assumption 3, we can always find a region D that satisfies Assumption
4. The more detailed properties of stability can be found, such as in Section 6.5 of Teschl (2000).
Assumption 3 and 4 are necessary to obtain the result (5) in the following section.
Also, we require the following assumption as a boundary condition of Theorem 4.
Assumption 5. L(θ*) = 0
Assumption 5 is only for simplifying our proofs without changing the essence of our problem.
Finally, we assume the following,
Assumption 6. For a potential field W (θ) : Rd 7→ R,
2 VW(θ)>C (θ)1 /2 VW(θ) - VL (θ)> VW(θ) = 0, W(0) = 0
has a unique solution.
4.2 Main Results
In preparation, we start with a traditional theorem in large deviation theory.
We analyze the mean escape time of SGD under the above assumptions. In preparation, we state
two facts. First, as shown in Hu et al. (2019) Theorem 3.4, V(θ) is calculated as a solution of the
following Hamilton-Jacobi equation,
2 VV(θ)>C (θ)1 //VV(θ) - VL (θ)> VV(θ) = 0.	(4)
Second, if B is sufficiently small, the mean exit time can be expressed using V (θ) as
B
E [T]=exp —% ,	(5)
η
where V0 := minθ0∈∂D V (θ0). Although these facts have been investigated in the literature (for
example, see Section 4.4 in Freidlin & Wentzell (2012)), we give our own theorems and proofs in
Appendix A and B for completeness.
The followings are our main results. We start with the mean exit time of Continuous SGD. Let
E[TSGD] be the mean exit time of the continuous SGD, and let E[TisoSGD] be the mean exit time of
an isotropic continuous SGD whose C(θ) is set to I.
Theorem 1 (Continuous SGD). Suppose that Assumption 1, 2, 3, 4, 5, and 6 hold. Then, for
sufficiently small B,
E[TIsoSGd] = exp ∣2Br2λ], E[Tsgd] = exp12BB-r2λ2].
6
Under review as a conference paper at ICLR 2022
This result gives an exact expression for the expected escape time with the explicit values of SGD.
The results also have two implications. First, these result both of those results show that the mean
escape time exponentially increases in the smallest eigen value of H *, i.e. λ. This implies that
sharper minima generally slow down the escaping, which is seemingly opposite to the implication of
the existing literature (JaStrzebSki et al., 2017; Xie et al., 2020). But in fact this is consistent with the
existing literature because some of the sharpness factor is implicitly included in other variables such
as ∆L. Second, our result endorses the fact that SGD’s anisotropic noise exponentially accelerates
the escaping (Xie et al., 2020), because the result shows that the mean exit time of SGD is smaller
than that of isotropic SGD by exp [λ 1 ].
Our theory can be extended to the discrete case. By E[νSGD], we denote the discrete mean exit time
of the discrete SGD, and by E[νisoSGD] we denote the one of an isotropic version of the discrete
SGD, i.e. with C(θ) being I. The escaping problem of a discrete SGD, the discrete mean exit time,
is formulated as a special case of Gobet & Menozzi (2010). By substituting g(∙) = 0, f (∙) = 1,
and k (∙ )=0 in Theorem 17 in Gobet & Menozzi (2010), We can obtain the following simplified
statement.
max {E[巧soSGd]— E[TIsoSGd], E[Vsgd] 一 E[Tsgd]} = O(√η)).
which immediately prove the following theorem:
Theorem 2 (Discrete SGD). Given, Assumption 1, 2, 3, 4, 5, and 6,forsufficiently small B,
B	「BC Il
E[巧soSGD] = exp [2—r-2λj + O (√η)),	E[Vsgd] = exp [2—r-2λ 2J + O (√η))
This result suggests that the discrete error does not majorly affect the escape. We note this is the first
study that confirms the validity of using a continuous SGD model (3) for escape analysis.
4.3 Proof for Theorem 1
We describe a proof of Theorem 3. We begin with the isotropic case and then investigate the non-
isotropic case.
Isotropic case E[TisoSGD]: We substitute I to C(θ). By the Jacobi equation (4) which is formally
given by Theorem 4 in Appendix A, we have the following form for θ ∈ D:
2VV(θ)>VV(θ) - NL (θ)> VV(θ)=0.	(6)
Given V (θ*) = 0 by definition of V (θ), V(θ) = 2L(θ) is the unique solution of (6) by Assumption
5 and 6. Therefore, we have
V0 = min 2L(θ) = min 2θ>H*θ = 2r2λ.
x∈∂D	x∈∂D
The second equality follows Assumption 1 and 5. Combined with the fact (5), which is formally
shown in Theorem 5 in Appendix B, we obtain the statement of Theorem 1 in the isotropic case.
Anisotropic case E[TSGD]: Similar to the isotropic case, the equation (4) (or Theorem 4) gives
2VV(θ)>C(θ)2 VV(θ) — VL (θ)> VV(θ) = 0.	(7)
VV(θ) = 2C(θ厂 1 VL(θ) satisfies (7) and by Assumption 1 and 2,2C(θ厂 2 VL(θ) can simply
written as
2 C (θ)-1VL (θ) = 2 H *-2 V (θ> H *θ) = 2 H *-12 H *θ = 4 H *1 θ.
Given V (θ*) = 0 by definition of V (θ), V (θ) = θ> H *1 θ is the unique solution of (7) by Assump-
tion 5 and 6. Then, we obtain
V0 = 2 r 2 λ min( H * 1 ),	(8)
Combined with (5), or Theorem 5 in Appendix B, we finish the proof of Theorem 1.	□
7
Under review as a conference paper at ICLR 2022
5	Further Investigation on Covariance Matrix
We investigate a variation of Assumption 2 on the covariance matrix. Although Assumption 2 is
commonly used (Jastrzebski et al., 2017; Xie et al., 2020), relaxing this is important for a flexible
modeling of SGD.
Assumption 7 (Variant of Assumption 2). H*-1 /2 is a positive matrix and there exist constants
0 < cι ≤ c2 < ∞ such that for θ ∈ D, C (θ) = H*G holds with a positive invertible matrix G as
0<c1 ≤ λmin(G) ≤ λmax(G) ≤c2.
Under this different setup, we obtain the following results.
Theorem 3. Suppose that Assumption 1, 3, 4, 5, 6, and 7 hold. Then, for sufficiently small B,
E[TIsoSGd] = exp 2BBr2λ]
exp h2BBr2 √C2λ 1 i ≤	E[TSGD]	≤ exp 2BBr2 √cλ 1 i
It suggests that replacing Assumption 2 by Assumption 7 does not affect the isotropic case, but has
a constant effect in the anisotropic case.
Proof for Theorem 3. The result of E[TisoSGD] is obtained in the same way as Theorem 1. For the
proof of E[TSGD], the following lemmas are useful, whose proofs are provided in Appendix C.
Lemma 3. For positive invertible matrices A and B, the following inequality holds,
λmin(A-1B-1) ≤ λmax(A-1)λmin(B-1)
Lemma 4. For invertible matrices A and B, the following inequality holds
λmin(AB) ≥ λmin(A)λmin(B)
Similar to the proof of Theorem 1, we obtain W(θ) = 2C(θ)-1NL(θ). Then, VV(θ) is simply
written as
VV(θ) = 2C(θ)-1VL(θ) = 2C(θ)-2V (θ>H^θ) = 2C(θ)-22H^θ
=2( H *G)- 2 2 H * = 4 G- 2 H ^-1H *θ = 4 G-2 H *2 θ.
The second equation follows Assumption 1 and 5, and the fourth equation follows Assumption 7.
Given that V (θ*) = 0, We obtain V (θ) = θ>G-2 H *1 θ for θ ∈ D. Then, we rewrite V0 is as
V0 = 2 r 2 λ min (G-1H *2),	(9)
By Lemma 3, we develop an upper bound on λ min( G-1 /2 H *1 /2) as λ m®( G-1 /2 H *1 /2) ≤
λ maχ(G-1 /2) λ min( H * 1 / 2) ≤ C - 1 //λ1 /2. Similarly, Lemma 4 gives US the following lower bound
λ min( G-1 /2 H *1 /2) ≥ λ min( G-1 /2) λ m^ (H T / 2) ≥ c - 11 / λ1 /2. We substitute the two inequalities
into the solution (9), then obtain the following form of V0:
2 r 2 -ɪ λ 2 ≤ V0 ≤ 2 r2 -ɪ λ 2
√2	—	— F
Combined with (5), or Theorem 5 in Appendix B, we finish the proof of Theorem 3.	□
6	Experiment
We conduct an experiment to validate our result of discrete setup (Theorem 2), using a neural net-
work and real-world datasets. We use a multi-layer perception and the AVILA dataset (De Stefano
et al., 2011) to observe that the discrete mean exit time of SGD has exponential dependence on
eigenvalue λ, radius r and a ratio of the learning rate and the batch size η∕B.
In order for our essential assumptions to hold, we use the mean square loss with `2 regularizer for
L(θ) (Assumption 1) and train the model with the gradient descent for a sufficiently long time to
8
Under review as a conference paper at ICLR 2022
Figure 2: Empirical validation of Theorem 1, where the empirical mean exit time has exponential
dependency on sharpness α (〜λ), radius r, and noise magnitude ∖∕η∕B.
obtain θ0 near θ^. We set the r-neighborhood of θ0 as D (Assumption 3 and 4). To measure the
discrete mean exit time, we repeatedly execute a vanilla SGD from θ0 for 1000 times and take an
average number of steps at which SGD exit from D (i.e. when the distance from θ0 becomes farther
than r).
To control λ, We follow the approach of Xie et al. (2020). We obtain sharper minima by mapping
the loss function L to La such that La(θ) := L(√αθ) (α > 0) and setting θo := θo/α. Since this
mapping coverts λ to αλ with other properties remaining the same, we use α as a surrogate of λ.
We show the results in Figure 2. As Theorem 2 suggests, the noise magnitude ∖∕η∕B exponentially
accelerates the escaping under our experiment setup, and eigen value and radius have the effect of
exponentially slowing down the escaping
7	Related Works
We summarize relevant studies related to the topics on loss surfaces and the stochastic gradient
descent algorithm. We mainly consider the following three factors.
Loss surface shape: Shape of loss surfaces have long been a topic of interest. The argument that the
flatness of loss surfaces around local minima improves generalization was first studied by Hochreiter
& Schmidhuber (1995; 1997), and the observation has recently reconfirmed in deep neural networks
by Keskar et al. (2016). Sagun et al. (2017) empirically examined the flatness of loss surfaces. The
theoretical advantage of the flatness was criticized by Dinh et al. (2017) in terms of scale-sensitivity
of flatness, but Tsuzuku et al. (2020) and Rangamani et al. (2019) tackled the criticism by developing
scale invariant flatness measures. An effect of the shape of loss surfaces on SGD was investigated
in Wu et al. (2017); Ge et al. (2018), and Chaudhari et al. (2019); Foret et al. (2020) developed a
variant of SGD which made use of this fact. In addition to the flatness, He et al. (2019b) proposed
a new notion of asymmetry of loss surfaces, and Draxler et al. (2018); Garipov et al. (2018) studied
how several local minima in a loss surface are connected. Li et al. (2018) developed a random
dimensional reduction method to visualize loss surfaces on high dimensional spaces.
Exit/Stability of SGD: How SGD behaves in neighborhoods of local minima in loss surfaces is
investigated from two aspects: stability and escape efficiency. For stability, a way in which SGD
finds local minima and stabilizes was analyzed by Wu et al. (2018); Kleinberg et al. (2018); Achille
et al. (2019); Li et al. (2017). Smith & Le (2017) used Bayesian ideas to analyze the stability. For
exiting aspects, JaStrzebSki et al. (2017) investigated an effect of a Hesse matrix of local minima
on the ease of escaping ineffective local minima, and Xie et al. (2020) elaborated this effect via
quantitative analysis. Zhu et al. (2019) showed that anisotropic structure of gradient noise by SGD
is useful in escaping inefficient local minima, and Nguyen et al. (2019) studied an effect of non-
Gaussianity of the gradient noise.
SGD property: Detailed nature of SGD itself is also an object of interest. The magnitude of the
gradient noise by SGD is an important factor, including its relation to a learning rate and a batch
size. An effect of large batch sizes on the reduction of gradient noise is investigated in Hoffer et al.
(2017); Smith et al. (2018); Masters & Luschi (2018). Another area of interest is shape of a gradient
noise distribution. Zhu et al. (2019); Hu et al. (2017); Daneshmand et al. (2018) investigated the
anisotropic nature of gradient noise and its advantage. Simsekli et al. (2019) discussed the fact
9
Under review as a conference paper at ICLR 2022
that a gradient noise distribution has a heavier tail than Gaussian distributions. Nguyen et al. (2019);
SimSekli et al. (2019) showed benefits of these heavy tails for SGD. Panigrahi et al. (2019) rigorously
examined gradient noise in deep learning and how close it is to a Gaussian. Xie et al. (2020) studied
a situation where the distribution is Gaussian, and then analyzes the behavior of SGD in a theoretical
way.
8	Conclusion
In this paper, we develop a novel quasi-potential theory for the escape problem of SGD. Our theory
gives an intuitive picture of SGD’s escaping dynamic, and also but is an effective tool for formal
analysis. In our main result, our theory explicitly describes how the escape of SGD is affected by
a batch-size, a learning rate, and radius of regions, and sharpness (Theorem 1). Furthermore, due
to its flexibility, our theory allows the extended analyses, such as SGD’s escape under even weaker
assumption on covariance matrix (Theorem 3) and the escape problem ofa discrete SGD (Theorem
2). We believe our theory provides a solid insight for SGD dynamics and also flexible theory for
further studies.
References
P-A Absil and K Kurdyka. On the stable equilibrium points of gradient systems. Syst. Control Lett.,
55(7):573-577, July 2006.
Alessandro Achille, Giovanni Paolini, and Stefano Soatto. Where is the information in a deep neural
network? arXiv preprint arXiv:1905.12213, 2019.
Madhu S Advani, Andrew M Saxe, and Haim Sompolinsky. High-dimensional dynamics of gener-
alization error in neural networks. Neural Netw., 132:428-446, December 2020.
Alnur Ali, J Zico Kolter, and Ryan J Tibshirani. A continuous-time view of early stopping for least
squares regression. In The 22nd International Conference on Artificial Intelligence and Statistics,
pp. 1370-1378. PMLR, 2019.
Rajendra Bhatia. Matrix Analysis. Springer, New York, NY, 1997.
Avrim L Blum and Ronald L Rivest. Training a 3-node neural network is np-complete. Neural
Networks, 5(1):117-127, 1992.
Alon Brutzkus, Amir Globerson, Eran Malach, and Shai Shalev-Shwartz. Sgd learns over-
parameterized networks that provably generalize on linearly separable data. arXiv preprint
arXiv:1710.10174, 2017.
P Chaudhari, A Choromanska, S Soatto, and others. Entropy-sgd: Biasing gradient descent into
wide valleys. Journal of Statistical Mechanics: Theory and Experiment, 2019.
Ronan Collobert, Jason Weston, Leon Bottou, Michael Karlen, Koray KavUkcUoglu, and Pavel
Kuksa. Natural language processing (almost) from scratch. J. Mach. Learn. Res., 12(ARTICLE):
2493-2537, 2011.
Hadi Daneshmand, Jonas Kohler, Aurelien Lucchi, and Thomas Hofmann. Escaping saddles with
stochastic gradients. In Proceedings of the 35th International Conference on Machine Learning,
volume 80, pp. 1155-1164. PMLR, 2018.
Claudio De Stefano, Francesco Fontanella, Marilena Maniaci, and Alessandra Scotto di Freca. A
method for scribe distinction in medieval manuscripts using page layout features. In International
Conference on Image Analysis and Processing, pp. 393-402. Springer, 2011.
Amir Dembo and Ofer Zeitouni. Large Deviations Techniques and Applications. Springer Berlin
Heidelberg, Berlin, Heidelberg, 2nd edition, 2010.
Laurent Dinh, Razvan Pascanu, Samy Bengio, and Yoshua Bengio. Sharp minima can generalize
for deep nets. In International Conference on Machine Learning, pp. 1019-1028, 2017.
10
Under review as a conference paper at ICLR 2022
Felix Draxler, Kambis Veschgini, Manfred Salmhofer, and Fred Hamprecht. Essentially no barriers
in neural network energy landscape. In International conference on machine learning, pp. 1309-
1318. PMLR, 2018.
Pierre Foret, Ariel Kleiner, Hossein Mobahi, and Behnam Neyshabur. Sharpness-aware minimiza-
tion for efficiently improving generalization. In International Conference on Learning Represen-
tations, 2020.
Mark I Freidlin and Alexander D Wentzell. Random Perturbations of Dynamical Systems 3rd Ed.
Springer Berlin Heidelberg, Berlin, Heidelberg, 2012.
Timur Garipov, Pavel Izmailov, Dmitrii Podoprikhin, Dmitry Vetrov, and Andrew Gordon Wilson.
Loss surfaces, mode connectivity, and fast ensembling of dnns. In Proceedings of the 32nd Inter-
national Conference on Neural Information Processing Systems, pp. 8803-8812, 2018.
Rong Ge, Jason D Lee, and Tengyu Ma. Learning one-hidden-layer neural networks with landscape
design. In International Conference on Learning Representations, 2018.
Emmanuel Gobet. Monte-Carlo Methods and Stochastic Processes: From Linear to Non-Linear.
CRC Press, September 2016.
Emmanuel Gobet and StePhane Menozzi. Stopped diffusion processes: Boundary corrections and
overshoot. Stochastic Process. Appl., 120(2):130-162, February 2010.
Fengxiang He, Tongliang Liu, and Dacheng Tao. Control batch size and learning rate to generalize
well: Theoretical and empirical evidence. Advances in Neural Information Processing Systems,
32:1143-1152, 2019a.
Haowei He, Gao Huang, and Yang Yuan. Asymmetric valleys: Beyond sharp and flat local minima.
Advances in Neural Information Processing Systems, 32:2553-2564, 2019b.
Sepp Hochreiter and Jurgen Schmidhuber. Simplifying neural nets by discovering flat minima. In
Advances in neural information processing systems, pp. 529-536, 1995.
Sepp Hochreiter and Jurgen Schmidhuber. Flat minima. Neural computation, 9(1):1-42, 1997.
Elad Hoffer, Itay Hubara, and Daniel Soudry. Train longer, generalize better: closing the general-
ization gap in large batch training of neural networks. In Proceedings of the 31st International
Conference on Neural Information Processing Systems, pp. 1729-1739, 2017.
Wenqing Hu, Chris Junchi Li, Lei Li, and Jian-Guo Liu. On the diffusion approximation of noncon-
vex stochastic gradient descent. arXiv preprint arXiv:1705. 07562, 2017.
Wenqing Hu, Zhanxing Zhu, Haoyi Xiong, and Jun Huan. Quasi-potential as an implicit regularizer
for the loss function in the stochastic gradient descent. arXiv preprint arXiv:1901.06054, 2019.
StaniSIaW Jastrzebski, Zachary Kenton, Devansh Arpit, Nicolas Ballas, Asja Fischer, Yoshua Bengio,
and Amos Storkey. Three factors influencing minima in sgd. arXiv preprint arXiv:1711.04623,
2017.
Stanislaw Jastrzebski, Devansh Arpit, Oliver Astrand, Giancarlo B Kerg, Huan Wang, Caiming
Xiong, Richard Socher, Kyunghyun Cho, and Krzysztof J Geras. Catastrophic fisher explosion:
Early phase fisher matrix impacts generalization. In International Conference on Machine Learn-
ing, pp. 4772-4784. PMLR, 2021.
Yiding Jiang, Behnam Neyshabur, Hossein Mobahi, Dilip Krishnan, and Samy Bengio. Fantastic
generalization measures and where to find them. arXiv preprint arXiv:1912.02178, 2019.
Nitish Shirish Keskar, Dheevatsa Mudigere, Jorge Nocedal, Mikhail Smelyanskiy, and Ping Tak Pe-
ter Tang. On large-batch training for deep learning: Generalization gap and sharp minima. arXiv
preprint arXiv:1609.04836, 2016.
Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint
arXiv:1412.6980, 2014.
11
Under review as a conference paper at ICLR 2022
Bobby Kleinberg, Yuanzhi Li, and Yang Yuan. An alternative view: When does sgd escape local
minima? In International Conference on Machine Learning, pp. 2698-2707. PMLR, 2018.
Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classification with deep convo-
lutional neural networks. Adv. Neural Inf. Process. Syst., 25:1097-1105, 2012.
Yann LeCun. 1.1 deep learning hardware: Past, present, and future. In 2019 IEEE International
Solid- State Circuits Conference - (ISSCC), pp. 12-19. ieeexplore.ieee.org, February 2019.
Hao Li, Zheng Xu, Gavin Taylor, Christoph Studer, and Tom Goldstein. Visualizing the loss land-
scape of neural nets. Advances in Neural Information Processing Systems, 31, 2018.
Qianxiao Li, Cheng Tai, and Weinan E. Stochastic modified equations and adaptive stochastic gradi-
ent algorithms. In Doina Precup and Yee Whye Teh (eds.), Proceedings of the 34th International
Conference on Machine Learning, volume 70 of Proceedings of Machine Learning Research, pp.
2101-2110. PMLR, 2017.
Stephan Mandt, Matthew Hoffman, and David Blei. A variational analysis of stochastic gradient
algorithms. In Maria Florina Balcan and Kilian Q Weinberger (eds.), Proceedings of The 33rd
International Conference on Machine Learning, volume 48 of Proceedings of Machine Learning
Research, pp. 354-363, New York, New York, USA, 2016. PMLR.
Dominic Masters and Carlo Luschi. Revisiting small batch training for deep neural networks. arXiv
preprint arXiv:1804.07612, 2018.
__ ⅛
Tom需 Mikolov, Anoop Deoras, Daniel Povey, LUk需 Burget, and Jan Cernocky. Strategies for
training large scale neural network language models. In 2011 IEEE Workshop on Automatic
Speech Recognition Understanding, pp. 196-201. ieeexplore.ieee.org, December 2011.
Thanh Huy Nguyen, Umut Simyekli, Mert Gurbuzbalaban, and Gael Richard. First exit time analysis
of stochastic gradient descent under heavy-tailed gradient noise. arXiv preprint arXiv:1906.09069,
2019.
Abhishek Panigrahi, Raghav Somani, Navin Goyal, and Praneeth Netrapalli. Non-gaussianity of
stochastic gradient noise. arXiv preprint arXiv:1910.09626, 2019.
Maxim Raginsky, Alexander Rakhlin, and Matus Telgarsky. Non-convex learning via stochastic
gradient langevin dynamics: a nonasymptotic analysis. In Satyen Kale and Ohad Shamir (eds.),
Proceedings of the 2017 Conference on Learning Theory, volume 65 of Proceedings of Machine
Learning Research, pp. 1674-1703. PMLR, 2017.
Akshay Rangamani, NamH Nguyen, Abhishek Kumar, Dzung Phan, Sang H Chin, and Trac D Tran.
A scale invariant flatness measure for deep network minima. arXiv preprint arXiv:1902. 02434,
2019.
Levent Sagun, Utku Evci, V Ugur Guney, Yann Dauphin, and Leon Bottou. Empirical analysis of
the hessian of over-parametrized neural networks. arXiv preprint arXiv:1706.04454, 2017.
Robin M Schmidt, Frank Schneider, and Philipp Hennig. Descending through a crowded valley-
benchmarking deep learning optimizers. In International Conference on Machine Learning, pp.
9367-9376. PMLR, 2021.
Umut Sy imsyekli, Mert Gurbuzbalaban, Thanh Huy Nguyen, Gael Richard, and Levent Sagun. On
the heavy-tailed theory of stochastic gradient descent for deep neural networks. arXiv preprint
arXiv:1912.00018, 2019.
Umut Simsekli, Levent Sagun, and Mert Gurbuzbalaban. A Tail-Index analysis of stochastic gra-
dient noise in deep neural networks. In Kamalika Chaudhuri and Ruslan Salakhutdinov (eds.),
Proceedings of the 36th International Conference on Machine Learning, volume 97 of Proceed-
ings of Machine Learning Research, pp. 5827-5837. PMLR, 2019.
Samuel L Smith and Quoc V Le. A bayesian perspective on generalization and stochastic gradient
descent. arXiv preprint arXiv:1710.06451, 2017.
12
Under review as a conference paper at ICLR 2022
Samuel L Smith, Pieter-Jan Kindermans, Chris Ying, and Quoc V Le. Don’t decay the learning rate,
increase the batch size. In International Conference on Learning Representations, 2018.
Gerald Teschl. Ordinary differential equations and dynamical systems. Grad. Stud. Math., 140:
08854-08019, 2000.
Yusuke Tsuzuku, Issei Sato, and Masashi Sugiyama. Normalized flat minima: Exploring scale in-
variant definition of flat minima for neural networks using pac-bayesian analysis. In International
Conference on Machine Learning, pp. 9636-9647. PMLR, 2020.
Lei Wu, Zhanxing Zhu, et al. Towards understanding generalization of deep learning: Perspective
of loss landscapes. arXiv preprint arXiv:1706.10239, 2017.
Lei Wu, Chao Ma, et al. How sgd selects the global minima in over-parameterized learning: A
dynamical stability perspective. Advances in Neural Information Processing Systems, 31:8279-
8288, 2018.
Zeke Xie, Issei Sato, and Masashi Sugiyama. A diffusion theory for deep learning dynamics:
Stochastic gradient descent exponentially favors flat minima. In International Conference on
Learning Representations, 2020.
Pan Xu, Jinghui Chen, Difan Zou, and Quanquan Gu. Global convergence of langevin dynamics
based algorithms for nonconvex optimization. arXiv preprint arXiv:1707.06618, 2017.
Zhanxing Zhu, Jingfeng Wu, Bing Yu, Lei Wu, and Jinwen Ma. The anisotropic noise in stochas-
tic gradient descent: Its behavior of escaping from sharp minima and regularization effects. In
International Conference on Machine Learning, pp. 7654-7663. PMLR, 2019.
A Jacobi Equation of Quasi-potential
Theorem 4.	For all θ ∈ D, V (θ) satisfies the following Jacobi equation,
2 VV(θ) >C (θ)1 /2 VV(θ) - NL (θ) > VV(θ) = 0
Proof of Theorem 4. For u, v ∈ Rd, we introduce an inner product and a norm regarding a point
θ ∈ D as {u,v)θ := u>C (θ)-1 // V and ∣∣u∣∣θ := PhuWθ. With these definitions, the Sot(φ) is
written as follows:
S 0 T ( Ψ )=2 Z0	ll⅛⅛ t + VL Wt) k kφt dt.
(10)
Note that φt ∈ D holds for any t ∈ [0, T] by the definition of trajectories. We rewrite the integrand
of (10) as follows:
l∣9^ t + VLI ψt) k ；
=ll⅛⅛ tk a + kVL Wt) k kψt + 2 31, VL Wt ) iφt
=(k⅛⅛ tkψt -kVL E) k j + 2 k。tkψt kVL Wt) krt + 2 31, VL Wt )Lt
≥ 2 k。t J k VL E) J + 2 h。t, VL E)'t .	(11)
We develop a parameterization for the term in (11). For a trajectory 。, we select an bijective function
f : [0, 1] → [0, T] as satisfying the follows: for each t ∈ [0,T] and t ∈ [0,1] as t = f (t), a
parameterized trajectory。限* :=。于(t^)satisfies
k⅛⅞Q* = kVL(苏*) M*.
(12)
This parameterization reduces the quasi-potential to the minimum of the following quantity:
(S0 TS) ≥ S0 f (T)(。*) =)/" T) l**Q* kVL (。*) Q* + 包*, VL (。* )iq* dt*	(13)
13
Under review as a conference paper at ICLR 2022
subject to the constraint (12). Since the integrand of (13) includes the first order derivative regarding
t^,(13) holds over different Parameterizations f. For convenience, We use another bijective parame-
terization function g : [0, R] → [0, 1] as t = g(r) with R > 0 and r ∈ [0, R] such that ⑦r := φg(丁)
satisfies
.
II。"Is r = 1∙	(14)
Then, the quasi-potential is reduced to the following formula, 1
V ( θ )=	inf R (∖∖φ r Is r I^L (。r ) b + Q r , NL。r })也 (⑸
r∈ [0 ,R ]： kiP r k<^ r=1 J。	∖	'	ISr)
where。R = θ. By the Bellman equation-type optimality, we expand the right hand side of (15) into
the following form:
V(θ) =	inf
r∈ [0 ,R]： k ⅛3r k <^r =1
I-(冷 r Is r INL (。r ) IIS r + D。r, NL (。r )) S r) dr + V (。R-δ ,
(16)
with a width value δ > 0. The Taylor expansion around r = R gives
]—(I。r Isr INL (。r)IlSr + Qr, NL (分))S) dr + V(。R-δ)
=S (IINL(。R)IlSR + DNL(。R),。RESR -。>NV (。R)) + V (。R) + O 伍2 * *)
Taking S → 0 and noticing。R = θ, (16) can be simplified to the following equation:
0= inf	(INL (θ) Ile + D NL (x) >,。R E -。R NV (θ))
r∈[0,R]：||Sr∣∣0r=1 '	'	e θ	)
(17)
It remains to select。which solves the minimization problem (17). Since the following equality
holds,
D NL (θ) >,。R E e-。RV-V (θ) = D NL (θ) > - NV (θ) > C (θ)- 1 /2,。R E $,	(18)
it is easy to see that a trajectory。* such that
A * =	NL X )> -N (θ) C( θ 尸1 /
WR — ~WL∕L(θ)> -NV (θ) C (θ)1 /2Ie
minimizes (18). With this。*, (17) simplifies to
INL(θ)Ie = INL(θ)> - NV (θ) C (θ)1 /2 Ie.
Taking the square of both sides, we get the statement.	口
B Theorem on Exit Time
In this section, we develop the following fundamental theorem on the notion of exit time. For
simplicity, we use ε to denote ^η∕B.
Theorem 5.	If ε is sufficiently small,
E [τ] = exp ε-2V0
holds, where V0 := mine0 ∈∂D V (θ0)
1One might think that if we parametrize as above (14), the equality condition for (11) is violated. Indeed
Soτ3= /TMMet k^L (01)ks^七
0
+ h0t, NL®)iφt dt
for 0. However, ® is introducedjust for the simple calculation of S0T(φ^). Although @ frequently appears in
the proof, our attention is still on φ^ and S0T(φ^), not on S0T(®).
14
Under review as a conference paper at ICLR 2022
To prove this result, we provide the proof for an upper bound (Lemma 6) and a lower bound (Lemma
7). Throughout the proofs, we use C0T,θ0, Pθ0 , instead of C0T or P, to clearly indicate which
trajectory we are referring to.
We introduce several notions. For δ > 0 and θ ∈ Rd, let Bδ(θ) denote an δ-neighbourhood of θ, that
is,Bδ(θ) := {θ0 ∈ Rd | kθ0 -θk ≤ δ}. Further, for a set Θ ⊂ Rd,Bδ(Θ) := ∪θ∈ΘBδ(θ).
The following lemma provides preliminary facts for proofs.
Lemma 5. For any c > 0, there exist μι,μ2,T1, T2 > 0 such that thefollowings hold:
1.	∀θ ∈ D, there exists a trajectory φ1 such that φ ⅛ = θ, φ T ∈ Bμ j 2(θ*), 0 <T ≤ T1 and
S 0 τ (φ1) = 0.
2.	∀θ ∈ Bμ 1 (θ*), there exists a trajectory φ2 such that φ0 = θ, φT ∈ ∂Bμ2 (D), 0 < T ≤ T2
and S0τ(φ2) < Vq + C.
Figure 3: Illustration of domains and boundary, D, Bμ、/2(θ*), Bμ 1 (θ*), and ∂Bμ? (D)
The illustration can be found in Fig. 3.
Proof of Lemma 5. The first statement immediately holds by the fact that D is attracted to a asymp-
totically stable equilibrium position θ* (Assumption 3 and 4).
For the second statement, since Vq := minθo∈∂d V (θ0), there exists a trajectory ga such that φa =
θ*,4Ta ∈ ∂D and S0Ta (4a) = Vq, where Ta is finite by Lemma 2.2 (a) in Freidlin & Wentzell
(2012). We cut off the first portion of ga UP until the first intersecting point with Bμ 1 (θ*) and define
it as φb. This means 夕0 ∈ Bμ 1 (θ*), φT ∈ ∂D and S0T (φb) < Vq hold. By Lemma 2.3 in
Freidlin & Wentzell (2012), there exists a trajectory from φT to a point θμ2 in ∂Bμ2 (D) such that
the steepness is less than K∣θμ2 — φT ∣ with a constant K > 0. Then, if We take a small enough μ2,
We can obtain gC such 夕0 =夕Tb, 0rc ∈ ∂Bμ2(D) and S0Tc (WC) < C. By connecting Wb and gC,
We obtain an appropriate W2.	□
Lemma 6. Ifε > 0 is sufficiently small,
E [τ] = O exp ε-2V0
holds, where V0 := minθ0∈∂D V (θ0).
Proof of Lemma 6. We show that for any constant c > 0, there exists a small ε0 such that ∀ε ≤ ε0
such that E [τ] ≤ exp ε-2(V0 + c) . To the aim, we split the dynamical system (3) of our interest
into the first half and the second half, {θt1}t and {θt2}t. {θt1}t starts with θ01 = θ0 ∈ D and terminates
when it first reaches Bμ、/2(θ*). We define the terminating time of {θt}t as τ^1 := min{t > 0 : θ∖ ∈
Bμ 1 /2(θ*)}. On the other hand, {θ2}t starts with θ2 = θτ 1 ∈ Bμ、/2 (θ*) and terminates when it first
15
Under review as a conference paper at ICLR 2022
reaches ∂D. We define the terminating time of {θt2}t as τ2 := min{t > 0 : θt1 ∈ ∂D}. Clearly, the
exit time τ = τ1 + τ2 .
Regarding	τ1 and τ2, we show the following two	independent facts with	sufficiently small	ε	>	0.
Fact 1 :	τ1 is no more than T1 with probability	at least 1/2.
Fact 2 :	τ2 is no more than T2 with probability	at least exp -ε-2 (V0	+ c) .
Fact 1:	Given the trajectory φ 1 provided by Lemma 5, Lemma	1 gives	Us	that	if	EV
%tpi(φι, μι/2, 1), the following inequality holds
Pθ1 nΨ ∈ COTi,θ1 (Rd) 1 P(Ψ , Ψ1) V μ 1 /2O ≥ eχp {-ɛ-2} ∙
Therefore, if We take E V min{，1 /ln2, %tpi(φι, μι/2, 1)}, We have
P θ 0 n Ψ ∈ Co Ti ,θ1 (R d) 1 P (Ψ , Ψ1) V μ 1 / 2o ≥ 2
Since the event of ∈ C0Ti 您(Rd) ∣ P (φ0, φ 1) V μ 1 /2} means that {θt}t reaches Bμ 1 (θ^) in
no later than T1 , We obtain the folloWing Which provides Fact 1.
Pθ0 {τ 1 VT1} ≥ 1.	(19)
Fact 2: Given the trajectory φ2 provided by Lemma 5, Lemma 1 tells US that if EV
Estp1( φ2, μ2, c/2), the following inequality holds
Pθ2 n« ∈ COT2,θ2 (Rd) | P (^, ψ2) V μ2 O ≥ eχp {-E- 2 卜0T2 (φ2) + 2)}.
{ψ ∈ coT2,θ2 (Rd) | ρ (φ0, φ2) V μ2} is the event that {θt }t goes out of D in no more than the time
T2. Also, we know that S0T2 (φ2) V Vo + C by Lemma 5. Hence, we can conclude the following
for Fact 2:
Pθ02 {τ2 V T2} ≥ exp -E-2 (V0 + c)} .	(20)
Combining (19) and (20), we can obtain
Pθ0 {τ v t1 + T2} ≥ 2 eχp { -E-2(Vo + C)} .	QI)
Since this is a simple exponential distribution, we can obtain the following expectation:
E[τ] ≤ 2(T1+T2)expE-2(Vo+c)}
By setting
EV
1
pln 2( T1 + T2)
,estp1(ψ 1, μ 1 /2, 1), EstP1(ψ2, μ2, c/2)
1
we can get
E [τ] ≤ exp E-2 (Vo + c)} .
Then, we obtain the statement.
□
Next, we develop the lower bound on the exit time.
Lemma 7. If E > 0 is sufficiently small,
E [T] = Ω ( exp [e-2Vo])
holds, where Vo := minθ0 ∈∂D V (θ0).
16
Under review as a conference paper at ICLR 2022
Proof of Lemma 7. We show for any positive constant c > 0, there exists a small ε0 such that
∀ε ≤ ε0, E [τ] ≥ exp ε-2(V0 - c) holds.
We consider a specific case where the initial value of (3) is in ∂Bμ、/2(θ*), which can be trivially
extended to general cases. Consider a Markov chain Zk (k ∈ N) as a discretization of θt as t = τk
with a k-th time grid τk. It is formally defined as follows:
1.	τ0 = 0,
2.	σk =inf [t>τk : θt ∈ ∂Bμ](θ*)},
3.	Tk = inf {t > σk-1 : θt ∈ ∂Bμ、/2(θ*) U ∂D},
4.	Zk = θτk .
Figure 4: A continuous trajectory θt and the Markov chain Zk generated from {θt}t . Colored
domains indicate D, Bμ、/2(θ*), and Bμ 1 (θ*) as illustrated in Fig. 3.
By introducing Zk, we can reduce the continuous process {θt}t to a discrete Markov chain transiting
between ∂Bμ、/2 (θ*) and ∂D. The illustration can be found in Fig. 4.
Let κ := inf{k | Zk ∈ ∂D}. Then, we have τ = τn and
∞
E [τ] = X Pθ0 {κ ≥ k} - Pθ0 {κ ≥ k + 1} τk
k=0
∞
Pθ0 {κ ≥ n} (τk - τk-1) .
k=1
This can be further evaluated as
∞∞
E [τ] > XPθ0 {κ ≥ k} (τk - σk-1) > XPθ0 {κ ≥ n}
k=1	k=1
inf	E[τ1] ,
∈∂Bμ 1 (θ* )
which follows Tk-1 < σk-1. Since Bμ 1 /2(θ*) is a strict subset of Bμ 1 (θ*), and Bμ 1 (θ*) is a strict
subset of D, it takes a positive amount of time to transit from ∂Bμ 1 (θ*) to either ∂Bμ 1 /2 (θ*) or ∂D,
and there exists a positive lower bound 11 for infθ0∈b (θ*) E[τι] that is independent of ε. Thus We
get
∞
E[T] >t1XPθ0{κ≥ k}.
k=1
By Lemma 8, we immediately get Pθ0 {κ > k} ≥ [1 - exp{-ε-2(V0 - c)}]k-1, hence we have
∞
E [T] > t1 X 1 - exp {-ε-2 (V0 - c)}k-1 = t1 exp {ε-2 (V0 - c)} .
This implies E[T] ≥ exp{ε-2(V0 - c)} holds if ε is small enough.
□
17
Under review as a conference paper at ICLR 2022
Lemma 8. We obtain
P(Zk+1 ∈ ∂D I Zk ∈ Bμ 1 /2(θ)) ≤ exp {-- (V0 - c)}.
ProofofLemma 8. First, We decompose P(Zk+ι ∈ ∂D ∣ Zk ∈ ∂Bμ、/2(θ*)) into two parts in the
following way:
Pθ0(Zk+1 ∈ ∂D I Zk ∈ ∂Bμ 1 /2(^^))
≤ max	Pθ0 {τ1 = τ }
—θ 0 ∈∂Bμ ι / 2( θ* ) 0	(22)
= max hPθ0 {τ = τ1 < T } + Pθ0 {τ = τ1 ≥ T } i
θ 0 ∈∂Bμ I / 2( θ* ) L0	0	」
This holds for arbitrary T, so we pick T = T0 large enough so that this inequality holds for the first
term:
Pθ0 {τ = T1 ≥ T0} ≤ 1 exp {T- (V0 - c)}	(23)
The existence of such T0 is guaranteed by the fact that V0 is finite and the following lemma.
Lemma 9 (Lemma 2.2 (b) in Freidlin & Wentzell (2012)). For any α > 0, there exists positive
constants c and To, such thatfor all sufficiently small ε > 0 and any θ0 ∈ D U ∂D∖Bα (θ*) we have
the inequality
Pθ0 {ζα > T} ≤ exp {-ε-2c (T - T0)} ,
where Zɑ = inf {t : θt / D∖Bɑ(θ^)}.
Given a constant T0, we consider bounding Pθ00 {τ = τ1 < T0}. Consider the following set of tra-
jectories:
Φ(V0 - c/2):= {φ : φ0 = θ0, S0t(夕)≤ V0 — c/2}.
Since it takes at least V0 to reach ∂D from θ*, the following inequality holds:
Pθ0 {τ = T1 < T0} ≤ Pθ0 E / Φ(% -c/2)}.
Also, Lemma 2 implies, for all ε≤ εstp2 (V0 - c/2, δ, c/2)
P C nE / Co t,θ 0(R d) | P((E - 趴 Vo - c/2))≥δ o ≤ eχp{-ε-2(( Vo - c/2) - c/2)}
= exp{-ε-2(V0 - c)}
Since δ can be arbitrarily small, the event of {Ey // Φ(V0 - c/2)} is equal to the event of {E0 /
C0T,θ00 (Rd) I ρ((E0 - Φ(V0 - c/2)) ≥ δ}. Hence, we obtain
Pθ00 {T = T1 < T} < exp{-ε-2(V0 - c)}.	(24)
If we set ε ≤ √= εstp2(Vo - c/2, δ, c/2), we conclude
Pθ0 {τ = T1 <T} < 2 exp{-ε-2(Vo - c)}.	(25)
Combining (22), (23), and (25), we prove the statement.	□
C Proofs for Lemma 3 and Lemma 4
Proof of Lemma 3. We introduce several definitions only for this proof. Supposing that we have
a vector x = (x 1 ,...,Xn) / Rn, x^ and χ↑ denote vectors whose coordinates are obtained by
rearranging the numbers Xj in decreasing order and in increasing order respectively, that is, x^ =
(x ；,..., χn) and χ↑ = (χ↑,…,xn), where x； ≥ … ≥ χn and x ↑ ≤ … ≤ xn. Also, given
x,y / Rn, we denote x Y y, if
nn	kk
j=='j = s^fyj, and ^^xj ≤ ^^yj for 1 ≤ ∀k ≤ n.
j=1 j=1	j=1 j=1
18
Under review as a conference paper at ICLR 2022
For an n × n matrix A, λ(A) = (λ1 (A), λ2(A), ..., λn(A)). Also, we define its elementwise loga-
rithm log λ(A) = (logλ1(A),logλ2(A), ...,logλn(A)).
Corollary III.4.6 in (Bhatia, 1997) claims that given two positive matrices A and B, we have
log XT (A) + log λ^ (B) Y log λ (AB) Y log λ^ (A) + log λ^ (B)
Following the definition of Y, the left part, log λτ(A)+log λ1 (B) Y log λ(AB), can be equivalently
restated as
kk
^X {logλτ(A) + logλ1(B)}； ≤ ^X {logλ(AB)}； for 1 ≤ ∀k ≤ n
j=1	j=1
For the case k = 1, we have
{logλτ(A)+logλJ(B)} 1 ≤ {logλ(AB)}；
^⇒ max {log λn-i +1( A ) + log λi ( B ) } ≤ log λmax( AB )
1≤i≤n
Since max1≤i≤n {logλn-i+1(A) + logλi(B)} ≥ logλn(A) + logλ1(B) = log λmin (A) +
logλmax(B) holds, we have log λmin(A) +logλmax(B) ≤ logλmax(AB), or equivalently
λmin (A)λmax (B) ≤ λmax(AB).
Since λmaχ( A) = 1 /λmin( A-1), we have
1	1	1
λmaχ(A-1) λmin(B-1) ' λmin ((AB)-1)
O λmin(( AB)- 1) ≤ λmaχ(A- 1)λmin(B- 1) ∙
Since commuted matrices share eigenvalues, λmin((AB)-1) = λmin(B-1A-1) = λmin(A-1B-1),
we have
λmin(A-1B-1) ≤ λmax(A-1)λmin(B-1)∙
□
Proof of Lemma 4. λmax(A) is equal to the spectral norm of A. By the sub-multiplicative property
of spectral norm, we obtain
λmax(A)λmax(B) ≥ λmax(AB),
Therefore, since λmaχ(A) = 1 /λm^(A-1),
λmax(A-1)λmax(B-1) ≥ λmax(A-1B-1)
0 λmaχ(A- 1)λmaχ(BT) ≥ λmaχ((AB)-1)
1	1	〉	1
0	λmin(A) λmin(B) ≥ λmin(AB)
0 λmin( AB ) ≥ λmin( A) λmin(B ) ∙
Then, We obtain the statement.	□
19