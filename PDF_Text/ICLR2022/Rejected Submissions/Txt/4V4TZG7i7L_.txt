Under review as a conference paper at ICLR 2022
Hierarchical
Multimodal Variational Autoencoders
Anonymous authors
Paper under double-blind review
Ab stract
Humans find structure in natural phenomena by absorbing stimuli from multiple
input sources such as vision, text, and speech. We study the use of deep gen-
erative models that generate multimodal data from latent representations. Exist-
ing approaches generate samples using a single shared latent variable, sometimes
with marginally independent latent variables, to capture modality-specific varia-
tions. However, there are cases where modality-specific variations depend on the
kind of structure shared across modalities. To capture such heterogeneity, we pro-
pose a hierarchical multimodal VAE (HMVAE) that represents modality-specific
variations using latent variables dependent on a shared top-level variable. Our ex-
periments on the CUB and the Oxford Flower datasets show that the HMVAE can
outperform existing methods in terms of generative heterogeneity and coherence
across several quantitative and qualitative measures. We provide the code to re-
produce the results in the supplementary material.
1 Introduction
The bird has tiny, skinny thighs and a
black eyering.
This large bird has with a large Wing
span and a white colored head.
⑶
(b)
Modality-exclusive
variations
∙-∙
•-。
Xi∖{xi ∩ X2}
(c)
Figure 1: Hierarchical decomposition. (a) The image background in the two pictures depends on
the bird species. (b) We propose to capture such dependencies via hierarchies. (c) A closer look at
the image background reveals that it is modality-exclusive, i.e., not described by the caption.
Data modalities represent different perspectives of the same concept. Generative models can learn
from such data by reproducing it, which can be useful for tasks such as image caption generation
(Vinyals et al., 2015). This model family can also reproduce feature vector representations of the
data, which can be helpful for tasks such as zero-shot classification (Xian et al., 2018b) or reinforce-
ment learning (Bruce et al., 2017). One difficulty in multimodal learning lies in the differing prob-
abilistic structures across modalities (Fig. 1). Our goal is to develop generative models that capture
unimodal variations in both modalities.
One line of previous works (Zhu et al., 2017; Zhang et al., 2017; Reed et al., 2016) tackled this chal-
lenge with conditional generative adversarial networks (GANs) (Mirza and Osindero, 2014). These
models generate samples for one modality conditioned on another modality and are optimized via
competition between a generator and a discriminator. In contrast, Suzuki et al. (2016) used varia-
tional autoencoders (VAEs) to jointly generate M modalities from a learned latent representation.
We focus on VAEs, which are explicit density estimators and can maximize the likelihood of all data
variations. GANs are implicit estimators, which can cause the generator to disproportionally favor
specific variations (Razavi et al., 2019).
1
Under review as a conference paper at ICLR 2022
Some multimodal VAEs (Wu and Goodman, 2018; Shi et al., 2019) incorporate a single latent vari-
able g that captures all relevant information (Fig. 2a). This formulation may disregard modality-
exclusive variations. Other works (Huang et al., 2018; Hsu and Glass, 2018; Mahajan et al., 2020;
Sutter et al., 2020; Daunhawer et al., 2021b; Lee and Pavlovic, 2021) have introduced disentangled
latent variables z1:M, which are marginally independent of a shared latent variable g and represent
structure specific to modality i (Fig. 2b). We argue that a disentangled latent representation may ne-
glect the dependencies between shared and modality-exclusive variations.
As a specific example, consider captioning (modality 2) pictures of birds (modality 1) as in Fig. 1a.
The images of seabirds can have sky or water in the background, while those of songbirds often dis-
play forest backgrounds. The captions focus on the bird and thereby easily ignore such variations.
Therefore, the shared pattern across modalities (bird species) dictates these modality-exclusive vari-
ations. Consider a generative model where g represents shared structure and z 1 image variations.
When the two latent variables g and z 1 are independent (Fig. 2b), the decoder that maps from latent
variables onto images has to learn two distinct functions - one for seabirds (where the independent
variations determine the sky or water background) and another for songbirds (where the independent
variations determine the forest background). We argue that this aspiration is theoretically learnable
but practically challenging because it may require a large model with disproportional capacity (that
could generalize poorly, is challenging to train, or requires abundant data). In contrast, we suggest
that a hierarchical latent representation is an inductive bias that captures realistic data variations and
guides learning. The edges between g and z1:M give the model the flexibility to decide which uni-
modal variations to capture given a shared latent concept. For example, a hierarchical model could
adaptively learn that z1 for seabirds solely captures sky or water background variations while z1
for songbirds captures forest background variations. This can help the decoder to share features be-
tween seabirds and songbirds.
Contributions (i) We propose a hierarchical multimodal VAE (HMVAE) that incorporates a latent
hierarchy, where the shared variable g resides at the top and lower variables z complement uni-
modal variations. (ii) We compare the HMVAE to several state-of-the-art baselines on the CUB and
the Oxford Flower datasets. We report improved quantitative and qualitative measures in terms of
semantic coherence and heterogeneity.
2 Background and related work
Variational Autoencoders (VAEs) (Kingma and Welling, 2013; Rezende et al., 2014) are deep
generative models that represent the joint distribution pθ (x, z) using neural networks with parame-
ters θ, where x ∈ RD is the observed vector and z ∈ RD0 is the latent vector. As the true posterior
Pθ(z|x) is intractable, an approximate posterior qφ(z∣x) with parameters φ is used for inference.
The parameters θ, φ are usually trained by maximizing the evidence lower bound (ELBO) for the
marginal likelihood:
Eqφ(z∣x) [log pθ⅛⅛i ≤ logPθ (X).	⑴
Many efforts have been made to increase the expressivity of VAEs, e.g., by improving the prior of
z (Chen et al., 2016; Tomczak and Welling, 2018), and by introducing auxiliary latent variables
(Maal0e et al., 2016). Our approach lies in the latter paradigm.
Hierarchical VAEs (HVAEs) (Rezende et al., 2014) have a hierarchical latent structure, where the
topmost latent variable, drawn from an unconditional prior pθ(zL), represents global features. The
lower variables, drawn from conditional priors pθ (zi |zi+1), complement local characteristics in
order to reconstruct the observed data via pθ (x | z 1).
S0nderby et al. (2016) found the tendency for HVAES to not effectively use higher-level latent vari-
ables when they are trained using inference networks of the form qφ(zi+1 |zi). They proposed to
first infer the top-level variable ZL directly with qφ(zL∖x) and then infer the intermediate variables
{zi} with both bottom-up and top-down information through q$f (z∕zi+ι, x) for hierarchical level
i ∈ {0, ..., L - 1}. The bottom-up and top-down information for hierarchical level i are encoded as
hidden variables through neural networks: bi = fφ,i(x) and ti = fφ,θ,i (zi+1). We make use of this
idea in our work, concatenate both hidden states and pass the result to an MLP that parameterizes
the respective posterior (see App. C.1 for further details). This inference procedure can result in im-
2
Under review as a conference paper at ICLR 2022
Figure 2: Related work. (a) MVAE, MMVAE, (b) MDVAE, (c) MDVAE, MHVAE. Note that we
discuss an alternative inference network for the MDVAE in App. D.
proved density estimation and sample generation performance (S0nderby et al., 2016; Maal0e et al.,
2019; Vahdat and Kautz, 2020; Child, 2021).
Multimodal VAEs represent M modalities x1:M = {x1, ..., xM} that are assumed to be condi-
tionally independent given a shared representation g: pθ(xi：M|g) = QM=IPθ(xi∣g). Motivated
by the factorization of the true posterior, Wu and Goodman (2018) used a product of experts (PoE)
formulation to parameterize the approximate posterior distribution over the shared latent variable,
where each expert characterizes information within a modality:
qφ(g∣χι.M) HPθ (g)QM=I qφ(glxm).	⑵
When all experts are Gaussian, the product posterior of any combination of experts is easily com-
puted.
Shi et al. (2019) showed that an inference model with a Gaussian PoE formulation can be miscal-
ibrated, i.e., overconfident experts qφ(g∖xi) with low densities dominate the product q°(g|xi：M).
They instead used a mixture of experts (MoE) formulation:
q°(g\xi：M) = M PM=I qφ(g∖xm ).	(3)
With the MoE formulation, optimization is analogous to a vote, which can avoid unreasonable dom-
inance by a single modality.
Mahajan et al. (2020) applied normalizing flows (Rezende and Mohamed, 2015) to map between the
latent spaces ofg1 and g2 in a VAE, where qφ(gi∖xi) constitutes the posterior over gi. Normalizing
flows can be more expressive than other distribution choices, such as Gaussians, which can help
to represent complex multimodal relationships. However, optimizing the ELBO requires tractable
sampling and density estimation in either direction, for example, by using coupling layers (Dinh
et al., 2014; 2017). This can be a challenging constraint in practice.
Vasco et al. (2020) proposed the MHVAE, which also incorporates a hierarchical generative model.
Note that both works were developed independently and offer complementary perspectives. We
summarize the differences in the following: first, the MHVAE’s generative model is limited to
two hierarchical levels (like Fig. 3a, but for two levels). Our proposed generative model supports
arbitrary hierarchical depth. Second, the MHVAE’s inference model is non-hierarchical (Fig. 2c,
Eq. 9 from the original paper). Our proposed inference model is hierarchical (Fig. 3b) Third, the
MHVAE’s posteriors over z are unimodal. Our proposed “top-down” inference model computes
the latent variables in the same order as the generative model. Therefore, the respective posteriors
depend on all modalities. Fourth, the MHVAE’s inference network drops modality-specific hidden
states at random during training. We use a mixture of experts posterior. Fifth, Vasco et al. (2020)
evaluate their model on surjective data where single labels or attributes describe classes of images,
i.e., there is not much variation in a single modality but lots in the other. We focus on a different
data scenario where each modality has a large degree of variation.
3 Multimodal latent hierarchies
We propose a hierarchical multimodal VAE (HMVAE) that captures the generative process of multi-
ple modalities. The hyperparameter L(m) ≥ 2 defines the number of hierarchical levels for modal-
ity m. If L(m) = 2, the hierarchy solely expands between the shared and unimodal variables. If
L(m) > 2, the model incorporates additional unimodal hierarchies.
3
Under review as a conference paper at ICLR 2022
Generative model We assume conditional in-
dependence of modalities given a shared vari-
able g (see Fig. 3a portraying the case of two
modalities and three hierarchical levels):
Pφ,θ(x, g, Z) = QM=I Pθ (Xm∣Zm,l)
• (nL(m)-2 Pφ,θ (Zm,i∣Zm,i+l))	(4)
Pφ,θ (Zm,L(m)-1 Ig) Pθ (g).
The prior for the shared variable g is isotropic
Gaussian. All conditional distributions for the
intermediate variables {Zm,1:L(m)-1} for m ∈
{1, . . . M} are also isotropic Gaussian, where
mean and variance are parameterized using
neural networks. We use the hierarchical for-
mulation introduced in § 2 where some param-
eters are shared across the generative and in-
ference networks. The conditional distributions
for the observed variables {xm } can, for ex-
ample, be parameterized using isotropic Gaus-
sian distributions (for continuous-valued data)
or Bernoulli distributions (for binary data).
Inference model We extend S0nderby et al.
(2016)’s hierarchical approach described in § 2
(a)	(b)
Figure 3: Proposed generation and inference.
The HMVAE represents unimodal variations in
conditional variables Z . We generalize the infer-
ence network from S0nderby et al. (2016) to the
multimodal case, where all posteriors are multi-
modal. For example, the red edges visualize how
an observed modality x2 can affect Z1,1 in the
other modality. The figure visualizes the special
case of three hierarchical levels.
to the multimodal case (see Fig. 3a depicting
the case of two modalities and three hierarchical levels):
M	L(m)-2
qφ,θ (g, Z∣Xl:M ) = qφ(g∣Xl:M )• ɪɪ qφ,θ (zm,L(m)-1∣g, Xm) ɪɪ qφ,θ (Zm,i∣Zm,i+1, Xm). (5)
m=1	i=1
All distributions except the first factor qφ(g|X1：M) are isotropic Gaussian with mean and variance
inferred through neural networks from the conditional variables. The network employs skip con-
nections from x1:M to g. The inference and generative networks share some parameters in the top-
down networks that point from g to the lowest unimodal variable Zm,i=0. This architecture can rein-
force hierarchical decomposition (S0nderby et al., 2016). Furthermore, it ensures that the unimodal
latent variables are conditioned on all modalities (as indicated by the red edges in Fig. 3b) which
helps learn crossmodal relationships. Section 2 and App. C.1 provide further details on the hierar-
chical architecture.
We parameterize the posterior distribution over the shared latent variable qφ(g\x1：M) using a mix-
ture of experts formulation (Eq. 3). We follow Shi et al. (2019) and assume that several modalities
can entail modality-exclusive variations, i.e., some variations for modality i do not correlate with
variations in modality j 6= i.
Optimization We train the HMVAE by maximizing the ELBO with stochastic backpropagation:
ELBO：=Eqφ,θ(g,z∣XLM ) hlog PΦ,θ(X,1zMιg:) i ≤ log Pθ (Xl：M ).	(6)
Further motivation Figure 4 visualizes how multimodal VAEs could capture the data from the ex-
ample in the Introduction (Fig. 1). The challenge lies in representing crossmodal dependencies - not
the joint distribution. That is because one modality can never inform about the modality-exclusive
variations of another modality. Daunhawer et al. (2021a) demonstrated that this problem constitutes
a core limitation across the multimodal VAE literature. In a non-hierarchical VAE, crossmodal gen-
eration is indeed challenging because g must capture all variations. This becomes problematic when
generating pθ(xi|g) from q@(g∣Xj=i) because the latter misses information about Xi. In contrast,
in a two-level hierarchy, Z could theoretically capture the entirety of modality-exclusive variations.
However, the model may choose to represent some of these variations in g to capture the hierarchi-
cal dependencies within the modality-exclusive variations. A deep hierarchy could circumvent this
4
Under review as a conference paper at ICLR 2022
No hierarchy	Two-level hierarchy
Figure 4: Hierarchical models can factorize Unimodal variations along the hierarchy This figure
visualizes possible latent representations for the multimodal data from Fig. 1: two modalities xι and
x2, where the brown color indicates shared structure and the yellow/green colors modality-exclusive
variations.
Deep hierarchy
problem by adding infinitesimal small modality-exclusive variations per hierarchical level (in the
limit). Note that Fig. 4 demonstrates one set of possible hierarchical representations. A model could
also utilize its degrees of freedom differently. For example, the deep model may incorporate shared
structure and modality-exclusive variations for the more complex modality in g. In general, we ex-
pect a (deep) hierarchy to be beneficial for representing complex data such as image or text modali-
ties.
4 Experiments
We evaluate on the CUB (Wah et al., 2011) and Oxford Flower (Nilsback and Zisserman, 2008)
datasets. The modalities in both datasets are images and captions (text). We will focus on cross-
modal generation, i.e., caption-to-image generation and the reverse. We also consider unconditional
generation. We evaluate generated samples mainly in terms of heterogeneity and coherence. Het-
erogeneity describes diversity across samples for a single setting of the shared latent variable g and
is essential to ensure that the generated samples capture the diversity of the data. Coherence indi-
cates whether a crossmodal sample lies on the manifold of the true data. Crucially, heterogeneity
and coherence are often both necessary. Imagine a model that produces class-agnostic generations
(high heterogeneity, low coherence for classification tasks). As for the HMVAE, we will validate
that the model hierarchically decomposes the data and outperforms state-of-the-art competitors.
4.1	Experimental design
Models We are not aware of any standard benchmark dataset in the literature with reported per-
formance for all considered baselines and sufficient hierarchical structure in the data. Therefore,
we reimplement all considered baselines: the MVAE (Wu and Goodman, 2018), the MMVAE (Shi
et al., 2019), the MDVAE (Huang et al., 2018; Hsu and Glass, 2018; Mahajan et al., 2020; Sut-
ter et al., 2020; Daunhawer et al., 2021b; Lee and Pavlovic, 2021), and the HMVAE (Vasco et al.,
2020). Related works suggested many different MDVAEs variants. Our reimplementation uses a
posterior choice suitable to the given data regime and stays close to the HMVAE- except that Z are
marginally independent of g . In general, we keep the architecture and optimization hyperparameters
similar across all models. App. C describes our implementation in detail. The use of larger models
may produce photorealistic images with a high computational cost. For example, Child (2021) train
a unimodal hierarchical VAE for around 2.5 weeks using 32 V100 GPUs on datasets such as FFHQ.
However, our goal is not photorealism but to validate that the proposed HMVAE represents multi-
modal data differently than the baselines.
5
Under review as a conference paper at ICLR 2022
Data In some of our experiments, we follow the common practice in the multimodal learning liter-
ature and preprocess the data by extracting feature vector representations (Xian et al., 2018b; Sariy-
ildiz and Cinbis, 2019; Schonfeld et al., 2019; Shi et al., 2019; 2020). Across all experiments, we
use the caption feature vectors provided by Zhang et al. (2017). These feature vectors x2 ∈ R1024
were extracted using a CNN-RNN (Reed et al., 2016). Each image is paired with ten captions. We
average the caption features for each image (except for the qualitative experiments which visualize
captions explicitly). We find that this procedure can improve training and reduce computational re-
quirements. In § 4.2, we extract features x1 ∈ R2048 from a ResNet-101 trained on ImageNet. In
§ 4.3, we use the images x1 ∈ R3×64×64 directly. We refer to App. C for details. To visualize image
features and caption features, we follow Shi et al. (2019; 2020), search for the nearest neighbor fea-
ture vector in the test set using the mean fromp(xi∣∙), and visualize the respective image or caption.
Using this practice, we can evaluate coherence (relationship between generation and condition) and
heterogeneity (sample diversity). To improve readability, we trim exceptionally long captions in the
qualitative figures.
4.2	Feature generation
In this section, we maximize the likelihood of image features x1 ∈ R2048 and caption features
x2 ∈ R1024 on the CUB and the Oxford Flower datasets. We will validate the general utility of
latent hierarchies relative to single-variable models. For the HMVAE, we choose L(1) = L(2) = 2.
Figures 5 and 6 display p(x1 |x2) and p(x2|x1), respectively. For the baselines, we vary g to
generate Xi, where g1:K 〜 q(g∖xj=i) and p(xi∖gk). For the proposed HMVAE, We utilize the
hierarchy by varying Zi to generate Xi, where g 〜 q(g∣Xj=i), z1:K 〜 p(z∕g), and p(x∕zk).
All models generate features that are mainly semantically coherent with the condition, indicating
high coherence. However, the proposed HMVAE generates samples of higher diversity than the
baseline, indicating high heterogeneity. This diversity must be represented in z1, because we fix
g and vary z1. Table 1 demonstrates that the HMVAE achieves state-of-the-art performance across
most evaluated likelihoods. Likelihood estimates quantify both coherence and heterogeneity because
they measure the divergence between the true and approximated distribution.
Table 1: Likelihood estimates. We approximate the joint, marginal, and crossmodal likelihoods
(defined in App. B) of test samples using 500 importance weighted samples. All models maximize
the likelihood of image feature vectors and caption feature vectors. We report average and standard
deviation over five runs per model. (higher is better)
Dataset and Model	log p(X1, X2)	logp(X1)	log p(X2)	log p(X1∖X2)	log p(X2∖X1)
CUB MVAE (Wu and Goodman, 2018) MMVAE (Shi et al., 2019) HMVAE (this work)	-2046.0 ± 6.6 -2633.5 ± 47.7 -1424.8 ± 14.8	-1902.3 ± 8.8 -2075.5 ± 8.2 -1854.8 ± 14.2	142.8 ± 16.1 180.9 ± 10.5 439.6 ± 2.0	-3594.0 ± 337.0 -3227.2 ± 145.1 -2382.9 ± 19.9	-3024.9 ± 297.4 -2022.2 ± 77.5 -879.7 ± 31.8
Oxford Flower MVAE (Wu and Goodman, 2018) MMVAE (Shi et al., 2019) HMVAE (this work)	-2471.3 ± 59.4 -3029.9 ± 50.5 -2069.1 ± 40.0	-2433.4 ± 22.5 -2575.5 ± 8.6 -2497.6 ± 21.7	215.4 ± 21.9 141.2 ± 26.0 442.2 ± 27.7	-3067.7 ± 67.5 -3179.3 ± 44.2 -3009.3 ± 38.0	-5282.8 ± 835.1 -3454.0 ± 218.4 -1400.0 ± 77.1
4.3	Image generation
In this section, we maximize the likelihood of images X1 ∈ R3×64×64 and caption feature vec-
tors X2 ∈ R1024. Because the data is more complex than in the previous section, we now investi-
gate deeper hierarchies for the HMVAE1 and incorporate additional baselines with multiple latent
variables (MDVAE and MHVAE). Note that the deep HMVAE employs the fewest parameters (see
Table 4 for an overview) and is the only model that does not use a warmup scheme for the KL-
divergence loss (S0nderby et al., 2016).
Quantitative results The Frechet Inception Distance (FID) compares the means and covariances
between the true and approximate distributions in the feature space of an Inception network (Heusel
1The shallow variant incorporates L(1) = L(2) = 2, the deeper variant incorporates L(1) = 5 and
L(2) = 2.
6
Under review as a conference paper at ICLR 2022
this flying bird has long brown wings and a black bill with a white stripe
this is a small bird with green back and head with a prominent eye and
Figure 5: CUB/Oxford Flower: Generating image feature vectors from caption feature vectors.
We generate image features, look up the nearest-neighbor feature in the test set, and visualize the
associated image.
Condition
the bird has a long throat and neck area covered in white feathers, it also has grey
this big gray bird has a wide, thick bill.
the bird has a long throat and neck area covered in white feathers, it also has grey
this big gray bird has a wide, thick bill.
this bird has a large grey head with a orange pointed bill.
this bird has wings that are grey and has a long neck and yellow bill
the bird has a curved neck and a grey flat bill with thick tarsals.
this large bird has with grey feathers and a large bill.
this large bird has with grey feathers and a large bill.
this bird has wings that are grey and has a long neck and yellow bill
MVAE
this water bird has a rather thick, long beak with darker plumage towards its backsid
the bird	has a	curved	neck,	long	deep	bill	and	amber	belly.
the bird	has a	curved	neck,	long	deep	bill	and	amber	belly.
the bird	has a	curved	neck,	long	deep	bill	and	amber	belly.
this water bird has a rather thick, long beak with darker plumage towards its backsid
the bird	has a	curved	neck,	long	deep	bill	and	amber	belly.
the bird	has a	curved	neck,	long	deep	bill	and	amber	belly.
the bird	has a	curved	neck,	long	deep	bill	and	amber	belly.
this water bird has a rather thick, long beak with darker plumage towards its backsid
the bird has a curved neck, long deep bill and amber belly.
MMVAE
a medium sized bird with a long narrow bill
a brown bird with a dingy white belly and neck.
a medium sized bird that has tones of dark brown with a large sized bill
a bird with a large triangular bill and rough gray plumage across its body.
this bird has wings that are gray and has a large bill
the bird has a grey bill that is long and a curved throat.
a medium sized bird that has tones of dark brown with a large sized bill
the bird has a grey bill that is long and a curved throat.
this bird is brown with white on its tail and has a long, pointy beak.
this water bird has a rather thick, long beak with darker plumage towards its backsid
HMVAE (proposed)
Condition
this flower has petals that are pink and has a yellow stamen
the flower has petals that are pink, soft and separately arranged around stamens and
this flower has petals that are pink and has a yellow stamen
this flower has a large pink petal and a yellow stigma coming out of the center
the petals on this flower are pink with yellow stamen.
this flower	has	petals that are pink and	has a yellow stamen
this flower	has	petals that are pink and	has a yellow stamen
this flower	has	petals that are pink and	has a yellow stamen
this flower	has	a large pink petal and a	yellow stigma coming	out	of the	center
the petals	of the flower are a light pink	color, with shades of white	near	the	base o
MVAE
this flower’s purple petals come together into a ”cup” shape, and its stamens are dus
this flower has a lavender blossoming petal with a yellow stamen in the center of it.
this flower has a lavender blossoming petal with a yellow stamen in the center of it.
this flower’s purple petals come together into a ”cup” shape, and its stamens are dus
this flower has a lavender blossoming petal with a yellow stamen in the center of it.
this flower has a lavender blossoming petal with a yellow stamen in the center of it.
this flower has a lavender blossoming petal with a yellow stamen in the center of it.
this flower’s purple petals come together into a ”cup” shape, and its stamens are dus
this flower has a lavender blossoming petal with a yellow stamen in the center of it.
the flower shown has purple petals as its main feature.
MMVAE
four larger flat petals that are pink with a flatten pistil and stamens.
a pistil and pink petals are what distinguishes this flower.
light purple and white petals but green leaves yellow middle
this flower has long slender petals of light magenta around a pompom center.
the flower is so vivid with purple color and has petals that are soft and stamen are
the petals of the flower are pink in color and have stripes that are maroon.
this flower has pink petals with pink anther and yellow interior.
a delicate and heavily veined pink flower that has four petals and forms soft cups fo
the plants has large alternating petal that are rounded and pink in color with fewer
the flower has petals that are pink, soft and separately arranged around stamens and
HMVAE (proposed)
Figure 6: CUB/Oxford Flower: Generating caption feature vectors from image feature vectors.
We generate caption features, look up the nearest-neighbor feature from the test set, and visualize
the respective caption.
et al., 2017; Seitzer, 2020). For the remaining measures, we preprocess images by extracting feature
representations from a ResNet-101 pre-trained on ImageNet. For example, we generate images
x1 ∈ R3×64×64 and then project these samples into the mentioned feature space. We do not use
any preprocessing for the caption features because of their lower dimensionality (1024). Precision
measures the relative amount of generated samples that lie on the true manifold. Recall measures
the opposite. We follow (Kynkaanniemi et al., 2019) and approximate the target manifold using
hyperspheres with radii equivalent to the 3-nearest-neighbors of the respective samples. We also
compute the harmonic mean across precision and recall (F1). Finally, we compute the average
variance of image representations: for each conditioning caption, we sample g1:100 〜 q(g∣x2) to
generatep(x1|g) and project these samples into the ResNet feature space f(x1|g). We then compute
the variance of these samples to a prototype representation f (x2∣g) = ɪ PI=I f (x2 |g).
Table 2a presents quantitative results.2 The MHVAE and the HMVAE are the only models that per-
form reasonably well across all evaluated measures. For example, even though the MVAE achieves
the best FID, the model scores poorly in the precision measures. Furthermore, the model’s qualita-
tive samples display a lack of fidelity (Fig. 7). In contrast, the MMVAE generates coherent samples
(high precision) but lacks heterogeneity (low variance and recall). The HMVAE also uses a mixture
2Note that we report the image recall and F1 values for the sake of completeness. These values are low
across all models due to the complexity of the image space given a relatively small dataset.
7
Under review as a conference paper at ICLR 2022
Table 2: Oxford Flower All models maximize the likelihood of images and caption features vectors.
We report average and standard deviation over three runs per model. (a) The second column high-
lights the number of latent variables in the model. (b) Hierarchical decomposition in the HMVAE.
(a)
Model	# LV	p(xι) vs. p(x1X2)					p(x2) vs. p(x2|x1)		
		FID	Precision	Recall	F1	Variance	Precision	Recall	F1
MVAE (Wu and Goodman, 2018)	1	142.1 ± 6.2	75.8 ± 1.3	2.9 ± 0.5	5.5 ± 0.9	0.51 ± 0.02	7.1 ± 4.6	92.8 ± 3.0	12.7 ± 7.9
MMVAE (Shi et al., 2019)	1	161.1 ± 4.5	91.5 ± 2.3	1.3 ± 0.1	2.6 ± 0.3	0.04 ± 0.0	74.3 ± 1.0	26.1 ± 1.8	38.6 ± 1.8
MDVAE (Mahajan et al., 2020), i.a.	3	167.8 ± 3.1	91.4 ± 1.9	0.9 ± 0.4	1.7 ± 0.8	0.22 ± 0.0	6.1 ± 3.2	53.7 ± 1.0	10.7 ± 5.0
HMVAE (ablation)	3	162.6 ± 2.2	95.2 ± 1.0	1.3 ± 0.2	2.5 ± 0.3	0.12 ± 0.01	81.8 ± 0.9	20.4 ± 2.0	32.6 ± 2.5
MHVAE (Vasco et al., 2020)	3	159.9 ± 3.2	86.8 ± 1.2	1.6 ± 0.2	3.2 ± 0.4	0.29 ± 0.01	53.4 ± 3.0	63.0 ± 4.2	57.7 ± 2.6
HMVAE (this work)	6	147.9 ± 3.3	94.6 ± 1.0	1.8 ± 0.3	3.6 ± 0.6	0.26 ± 0.0	48.9 ± 0.5	74.8 ± 3.0	59.1 ± 1.2
(b)
Mean over	Effective Hierarchical Layers	FID	p(x1) vs. p(x1 |x2)			Variance	p(x2) vs. p(x2|x1)		
			Precision	Recall	F1		Precision	Recall	F1
-	5	147.9 ± 3.3	94.6 ± 1.0	1.8 ± 0.3	3.6 ± 0.6	0.26 ± 0.0	-	-	-
{z1,1}	4	192.3 ± 0.4	83.7 ± 0.5	0.8 ± 0.6	1.6 ± 1.1	0.27 ± 0.0	-	-	-
{z1,i}i∈{1}	3	273.0 ± 6.8	62.7 ± 2.0	0.0 ± 0.0	0.0 ± 0.0	0.29 ± 0.01	-	-	-
{z1,i}i∈{1,2}	2	297.3 ± 1.6	43.3 ± 3.3	0.0 ± 0.0	0.0 ± 0.0	0.26 ± 0.01	-	-	-
{z1,i}i∈{1,2,3}	1	300.3 ± 0.7	38.8 ± 0.7	0.0 ± 0.0	0.0 ± 0.0	0.13 ± 0.01	-	-	-
-	2	-	-	-	-	-	48.9 ± 0.5	74.8 ± 3.0	59.1 ± 1.2
{z2,1}	1	-	-	-	-	-	84.0 ± 2.1	7.2 ± 0.9	13.3 ± 1.5

Figure 7: Oxford Flower: generating images from captions. Top: The penultimate column rep-
resents the HMVAE with two hierarchical levels for the images (L(1) = 2). The last column corre-
sponds to the HMVAE with L(1) = 5, which is the best model considering all measures across all
modalities. Bottom: as in the plot above, we generate p(xi |g) from q(g|xj6=i). However, for the la-
tent distributions starting at the latent variables indicated below the plot, we use the mean and not a
conventional sample. We can thereby evaluate generated samples given varying degrees of hierar-
chical expressivity.














of experts posterior as the MMVAE, but adds hierarchical depth which correlates positively with
variance and recall. Finally, we observe that the HMVAE improves the MHVAE across measures
related to coherence (FID, image precision) and heterogeneity (FID, caption recall). Table 2b ex-
amines hierarchical decomposition in the HMVAE. We first sample conventionally from q(g|xj6=i).
We then generate p(xi|g) by using the conditional priors in the latent hierarchy for xi. Normally,
we would sample conventionally from each of these latent distributions (rows 2 and 7). However, we
8
Under review as a conference paper at ICLR 2022
also evaluate models where we propagate the latent distribution means corresponding to the latent
variables from the left column. In other words, we omit variations in some hierarchical levels. For
the experiment on the image hierarchy, all measures improve with increased hierarchical expressiv-
ity. For the experiment on the caption hierarchy, greater hierarchical capacity is beneficial (F1) by
increasing recall and decreasing precision. Increased recall is expected to correlate negatively with
precision: generating a larger manifold typically increases the likelihood of generated samples be-
yond the target manifold.
Qualitative results Figure 7 focuses onp(x1∣x2). We generatep(xι∣gk) viag1:K 〜q(g∣x2). For
the MDVAE, we must additionally sample from the disentangled variable and hence generate im-
agesp(xι∣zι, g) from zι 〜P(Zj andg 〜q(g∣x2). Note that we always sample from the (Condi-
tional) priors over z across all multi-latent-variable models3. The MVAE’s samples display a notice-
able lack of coherence and fidelity which is in line with the results from Shi et al. (2019). The MM-
VAE’ samples have high coherence and low heterogeneity. The MDVAE’s samples display preci-
sion but misalign some unimodal variations with the shared concept. The MHVAE’s samples some-
times lack semantic coherence. The shallow HMVAE generates samples of good coherence and low
heterogeneity. The deep HMVAE generates the most compelling samples relative to the baselines
(as also indicated by the FID from Table 2a). Figure 7 presents the counterpart for the quantitative
hierarchical ablation experiment from Table 2b. This figure demonstrates that the HMVAE decom-
poses image variation along its hierarchy.
5 Conclusion
We have proposed a hierarchical multimodal VAE (HMVAE) where unimodal latent hierarchies de-
pend on a shared latent variable. We have demonstrated that the model improves generative mod-
eling performance on multimodal data. Future work may further improve sample quality, for ex-
ample, by incorporating different ways to parameterize the conditional densities: normalizing flows
could improve the posterior’s flexibility (Kingma et al., 2016), autoregressive decoders could ex-
plicitly represent the conditional dependencies across observed dimensions (Chen et al., 2018), and
the transformer architecture may be a good candidate to replace the convolutional neural networks
(Dosovitskiy et al., 2021). Note that we discuss ethical implications in App. E.
Limitations Our goal is the the representation of heterogeneity within modalities. Although the
proposed HMVAE outperforms our baselines, the model is not perfect. For example, when gen-
erating images by sampling from the conditional distributionp(xι∣zι), where zι 〜p(zι∣g) and
g 〜q(g∣x2), the HMVAE must hallucinate a coherent set of visual representations (possibly condi-
tional on a given caption). This can be challenging when the caption is vague and does not contain
sufficient information for the inference network to infer the shared underlying structure correctly.
In other words, the additional expressivity that the HMVAE obtains through the conditional latent
variables also makes learning harder since p(xι ∣∙) must represent a larger manifold for the proposed
HMVAE than it does for the baselines.
References
J. Bruce, N. Sunderhauf, P. Mirowski, R. Hadsell, and M. Milford. One-shot reinforcement learning
for robot navigation with interactive replay. arXiv preprint arXiv:1711.10137, 2017.
Y. Burda, R. Grosse, and R. Salakhutdinov. Importance weighted autoencoders. arXiv preprint
arXiv:1509.00519, 2015.
X. Chen, D. P. Kingma, T. Salimans, Y. Duan, P. Dhariwal, J. Schulman, I. Sutskever, and P. Abbeel.
Variational lossy autoencoder. arXiv preprint arXiv:1611.02731, 2016.
X. Chen, N. Mishra, M. Rohaninejad, and P. Abbeel. Pixelsnail: An improved autoregressive gen-
erative model. In International Conference on Machine Learning, pages 864-872. PMLR, 2018.
3A core difference between the MDVAE and the HMVAE lies in their priors. In the former, the priors over
z are unconditional. In the latter, the priors over z are conditional.
9
Under review as a conference paper at ICLR 2022
R. Child. Very deep vaes generalize autoregressive models and can outperform them on images. In
International Conference on Learning Representations, 2021. URL https://openreview.
net/forum?id=RLRXCV6DbEJ.
I. Daunhawer, T. M. Sutter, K. Chin-Cheong, E. Palumbo, and J. E. Vogt. On the limitations of
multimodal vaes. arXiv preprint arXiv:2110.04121, 2021a.
I.	Daunhawer, T. M. Sutter, R. Marcinkevics, and J. E. VOgt. Self-supervised disentanglement of
modality-specific and shared factors improves multimodal generative models. Pattern Recogni-
tion, 12544:459, 2021b.
J.	Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei. Imagenet: A large-scale hierarchical
image database. In 2009 IEEE conference on computer vision and pattern recognition, pages
248-255. Ieee, 2009.
L. Dinh, D. Krueger, and Y. Bengio. Nice: Non-linear independent components estimation. arXiv
preprint arXiv:1410.8516, 2014.
L. Dinh, J. Sohl-Dickstein, and S. Bengio. Density estimation using real nvp. International Confer-
ence on Learning Representations, 2017.
A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai, T. Unterthiner, M. Dehghani,
M. Minderer, G. Heigold, S. Gelly, J. Uszkoreit, and N. Houlsby. An image is worth 16x16
words: Transformers for image recognition at scale. In International Conference on Learning
Representations, 2021. URL https://openreview.net/forum?id=YicbFdNTTy.
K.	He, X. Zhang, S. Ren, and J. Sun. Deep residual learning for image recognition. In Proceedings
of the IEEE conference on computer vision and pattern recognition, pages 770-778, 2016.
D. Hendrycks and K. Gimpel. Gaussian error linear units (gelus). arXiv preprint arXiv:1606.08415,
2016.
M. Heusel, H. Ramsauer, T. Unterthiner, B. Nessler, and S. Hochreiter. Gans trained by a two time-
scale update rule converge to a local nash equilibrium. Advances in neural information processing
systems, 2017.
W.-N. Hsu and J. Glass. Disentangling by partitioning: A representation learning framework for
multimodal sensory data. arXiv preprint arXiv:1805.11264, 2018.
X. Huang, M.-Y. Liu, S. Belongie, and J. Kautz. Multimodal unsupervised image-to-image trans-
lation. In Proceedings of the European conference on computer vision (ECCV), pages 172-189,
2018.
D. P. Kingma and M. Welling. Auto-encoding variational bayes. arXiv preprint arXiv:1312.6114,
2013.
D. P. Kingma, T. Salimans, R. Jozefowicz, X. Chen, I. Sutskever, and M. Welling. Improved varia-
tional inference with inverse autoregressive flow. In Advances in neural information processing
systems, pages 4743-4751, 2016.
T. Kynkaanniemi, T. Karras, S. Laine, J. Lehtinen, and T. Aila. Improved precision and recall metric
for assessing generative models. In NeurIPS, 2019.
M. Lee and V. Pavlovic. Private-shared disentangled multimodal vae for learning of latent represen-
tations. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recogni-
tion, pages 1692-1700, 2021.
L. Maal0e, C. K. S0nderby, S. K. S0nderby, and O. Winther. Auxiliary deep generative models. In
International conference on machine learning, pages 1445-1453. PMLR, 2016.
L.	Maal0e, M. Fraccaro, V. Lievin, and O. Winther. Biva: A very deep hierarchy of latent variables
for generative modeling. In Advances in neural information processing systems, pages 6548-
6558, 2019.
10
Under review as a conference paper at ICLR 2022
S. Mahajan, I. Gurevych, and S. Roth. Latent normalizing flows for many-to-many cross-domain
mappings. In International Conference on Learning Representations, 2020. URL https://
openreview.net/forum?id=SJxE8erKDH.
M.	Mirza and S. Osindero. Conditional generative adversarial nets. arXiv preprint arXiv:1411.1784,
2014.
M.-E. Nilsback and A. Zisserman. Automated flower classification over a large number of classes.
In 2008 Sixth Indian Conference on Computer Vision, Graphics & Image Processing, pages 722-
729. IEEE, 2008.
A. Radford, L. Metz, and S. Chintala. Unsupervised representation learning with deep convolutional
generative adversarial networks. arXiv preprint arXiv:1511.06434, 2015.
P. Ramachandran, B. Zoph, and Q. V. Le. Searching for activation functions. arXiv preprint
arXiv:1710.05941, 2017.
A. Razavi, A. van den Oord, and O. Vinyals. Generating diverse high-fidelity images with vq-vae-2.
In Advances in Neural Information Processing Systems, pages 14837-14847, 2019.
S. Reed, Z. Akata, X. Yan, L. Logeswaran, B. Schiele, and H. Lee. Generative adversarial text to
image synthesis. arXiv preprint arXiv:1605.05396, 2016.
D.	J. Rezende and S. Mohamed. Variational inference with normalizing flows. arXiv preprint
arXiv:1505.05770, 2015.
D. J. Rezende, S. Mohamed, and D. Wierstra. Stochastic backpropagation and approximate in-
ference in deep generative models. In E. P. Xing and T. Jebara, editors, Proceedings of the
31st International Conference on Machine Learning, volume 32 of Proceedings of Machine
Learning Research, pages 1278-1286, Bejing, China, 22-24 Jun 2014. PMLR. URL http:
//proceedings.mlr.press/v32/rezende14.html.
M. B. Sariyildiz and R. G. Cinbis. Gradient matching generative networks for zero-shot learning. In
Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 2168-
2178, 2019.
E.	Schonfeld, S. Ebrahimi, S. Sinha, T. Darrell, and Z. Akata. Generalized zero-and few-shot learn-
ing via aligned variational autoencoders. In Proceedings of the IEEE Conference on Computer
Vision and Pattern Recognition, pages 8247-8255, 2019.
M. Seitzer. pytorch-fid: FID Score for PyTorch. https://github.com/mseitzer/
pytorch-fid, August 2020. Version 0.1.1.
Y. Shi, N. Siddharth, B. Paige, and P. Torr. Variational mixture-of-experts autoencoders for multi-
modal deep generative models. In Advances in Neural Information Processing Systems, pages
15692-15703, 2019.
Y. Shi, B. Paige, P. H. Torr, and N. Siddharth. Relating by contrasting: A data-efficient framework
for multimodal generative models. arXiv preprint arXiv:2007.01179, 2020.
C. K. S0nderby, T. Raiko, L. Maal0e, S. K. S0nderby, and O. Winther. Ladder variational autoen-
coders. In Advances in neural information processing systems, pages 3738-3746, 2016.
T. M. Sutter, I. Daunhawer, and J. E. Vogt. Multimodal generative learning utilizing jensen-shannon-
divergence. arXiv preprint arXiv:2006.08242, 2020.
M. Suzuki, K. Nakayama, and Y. Matsuo. Joint multimodal learning with deep generative models.
arXiv preprint arXiv:1611.01891, 2016.
J. Tomczak and M. Welling. Vae with a vampprior. In International Conference on Artificial Intel-
ligence and Statistics, pages 1214-1223. PMLR, 2018.
G. Tucker, D. Lawson, S. Gu, and C. J. Maddison. Doubly reparameterized gradient estimators for
monte carlo objectives. arXiv preprint arXiv:1810.04152, 2018.
11
Under review as a conference paper at ICLR 2022
A. Vahdat and J. Kautz. Nvae: A deep hierarchical variational autoencoder. Advances in Neural
Information Processing Systems, 33, 2020.
M. Vasco, F. S. Melo, and A. Paiva. Mhvae: a human-inspired deep hierarchical generative model
for multimodal representation learning. arXiv preprint arXiv:2006.02991, 2020.
O. Vinyals, A. Toshev, S. Bengio, and D. Erhan. Show and tell: A neural image caption generator.
In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 3156-
3164, 2015.
C. Wah, S. Branson, P. Welinder, P. Perona, and S. Belongie. The caltech-ucsd birds-200-2011
dataset. 2011.
M. Wu and N. Goodman. Multimodal generative models for scalable weakly-supervised learning.
In Advances in Neural Information Processing Systems, pages 5575-5585, 2018.
Y. Xian, C. H. Lampert, B. Schiele, and Z. Akata. Zero-shot learning—a comprehensive evaluation
of the good, the bad and the ugly. IEEE transactions on pattern analysis and machine intelligence,
41(9):2251-2265, 2018a.
Y. Xian, T. Lorenz, B. Schiele, and Z. Akata. Feature generating networks for zero-shot learning.
In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 5542-
5551, 2018b.
H. Zhang, T. Xu, H. Li, S. Zhang, X. Wang, X. Huang, and D. N. Metaxas. Stackgan: Text to
photo-realistic image synthesis with stacked generative adversarial networks. In Proceedings of
the IEEE international conference on computer vision, pages 5907-5915, 2017.
J.-Y. Zhu, T. Park, P. Isola, and A. A. Efros. Unpaired image-to-image translation using cycle-
consistent adversarial networks. In Proceedings of the IEEE international conference on computer
vision, pages 2223-2232, 2017.
12
Under review as a conference paper at ICLR 2022
Appendix
We partition the Appendix into five sections:
A.	Additional experiments: For the experiments from § 4.2, we provide further results on
crossmodal generation. For the experiments from § 4.3, we provide results on unconditional
generation.
B.	Likelihood estimation: This section presents the estimators used to compute the likeli-
hoods reported in Table 1.
C.	Implementation: This section describes our implementation in detail. App. C.1 comple-
ments the background section § 2 by describing details of the employed hierarchical VAE
architecture. This section also presents dataset-agnostic model specifications for the base-
lines. We then focus on dataset-specific implementation specifications, where App. C.2
covers feature generation on the CUB dataset, App. C.3 feature generation on the Oxford
Flower dataset, and App. C.4 image generation on the Oxford Flower dataset.
D.	MDVAE: inference networks: This section discusses an alternative inference network
choice for multimodal disentanglement VAEs (MDVAEs).
E.	Ethical statement: This section discusses ethical implications of the proposed model, e.g.,
its societal impact.
13
Under review as a conference paper at ICLR 2022
A Additional experiments
This section presents additional experimental results that complement § 4 from the main paper.
A.1 Feature generation
In this section, we maximize the likelihood of image features x1 ∈ R2048 and caption features
x2 ∈ R1024 on the CUB dataset and the Oxford Flower dataset. We use the same experimental setup
as in § 4.
Image feature generation Figure 8a presents generated samples where the captions are from the
training set. We observe that the MMVAE still lacks variety in its generations, which indicates that
this tendency is not caused by poor generalization. Figure 8b presents generated samples where the
captions are from the test set. The HMVAE can represent different backgrounds (e.g., various sea or
forest settings), even though this information is missing in the conditioning captions.
this little bird has a
巳它已&.已已已&占G-
影西&&巳&k&匕
(a)
the bird has a green breast and abdomen as well as a tiny bill.
this is a small reddish brown bird with a bright green wing and brown beak.
MMVAE
HMVAE (proposed)
∙λ*-v⅛ ∙λ* -λ* V* J** JJ* 工* V* *λ*
U r : H-∙v√,H
汆我樽轨卒较
弋抑卜卜依卜N
;壮3:世&七
MVAE
this flower has petals that are purple and very thin
MVAE	MMVAE	HMVAE (proposed)	MVAE	MMVAE	HMVAE (proposed)
(b)
Figure 8: CUB/Oxford Flower: Generating image feature vectors from caption feature vectors.
Plot (a) represents the training set, plot (b) the test set. We generate image features, look up the
nearest-neighbor feature in the test set, and visualize the associated image.
Caption feature generation Figure 9 presents additional results for caption feature generation
which confirm our findings from § 4.
14
Under review as a conference paper at ICLR 2022
Condition
the bird	has a large thick body and is grey in color	with a	thick grey bill.
this bird	has a	black	overall	color aside	from	its	bill	which	is	in	grey	color.
this bird	has a	black	overall	color aside	from	its	bill	which	is	in	grey	color.
this bird	has a	black	overall	color aside	from	its	bill	which	is	in	grey	color.
the bird	has	a	large	thick	body	and	is	grey	in	color	with	a	thick	grey	bill.
the bird	has	a	large	thick	body	and	is	grey	in	color	with	a	thick	grey	bill.
the bird	has	a	large	thick	body	and	is	grey	in	color	with	a	thick	grey	bill.
the bird	has	a	large	thick	body	and	is	grey	in	color	with	a	thick	grey	bill.
the bird	has	a	large	thick	body	and	is	grey	in	color	with	a	thick	grey	bill.
the bird	has	a	large	thick	body	and	is	grey	in	color	with	a	thick	grey	bill.
MVAE
this particular bird has a belly
this particular bird has a belly
this particular bird has a belly
this particular bird has a belly
this particular bird has a belly
this particular bird has a belly
this particular bird has a belly
this particular bird has a belly
this particular bird has a belly
this particular bird has a belly
that is black with a gray bill
that is black with a gray bill
that is black with a gray bill
that is black with a gray bill
that is black with a gray bill
that is black with a gray bill
that is black with a gray bill
that is black with a gray bill
that is black with a gray bill
that is black with a gray bill
MMVAE
Condition
medium sized,	black and	grey	in	color,	with	a	sharp	bill.
this bird has a	black	overall color aside	from its bill	which is in grey color.
medium sized,	black and	grey	in	color,	with	a	sharp	bill.
medium sized,	black and	grey	in	color,	with	a	sharp	bill.
medium sized,	black and	grey	in	color,	with	a	sharp	bill.
a black bird with some greyish in its plume and a fat beak that surves downward.
medium sized, black and grey in color, with a sharp bill.
a black bird with some greyish in its plume and a fat beak that surves downward.
medium sized, black and grey in color, with a sharp bill.
medium sized, black and grey in color, with a sharp bill.
MVAE
a tall	bird	with	a	black	and	white	body	and	a	silver	beak.
a tall	bird	with	a	black	and	white	body	and	a	silver	beak.
a tall	bird	with	a	black	and	white	body	and	a	silver	beak.
a tall	bird	with	a	black	and	white	body	and	a	silver	beak.
a tall	bird	with	a	black	and	white	body	and	a	silver	beak.
a tall	bird	with	a	black	and	white	body	and	a	silver	beak.
a tall	bird	with	a	black	and	white	body	and	a	silver	beak.
a tall	bird	with	a	black	and	white	body	and	a	silver	beak.
a tall	bird	with	a	black	and	white	body	and	a	silver	beak.
a tall	bird	with	a	black	and	white	body	and	a	silver	beak.
MMVAE
Condition
this flower has yellow petals,
this flower has yellow petals,
this flower has yellow petals,
this flower has yellow petals,
this flower has yellow petals,
this flower has yellow petals,
this flower has yellow petals,
this flower has yellow petals,
this flower has yellow petals,
this flower has yellow petals,
with brown stamen in the
with brown stamen in the
with brown stamen in the
with brown stamen in the
with brown stamen in the
with brown stamen in the
with brown stamen in the
with brown stamen in the
with brown stamen in the
with brown stamen in the
MVAE
this flower has petals that are
this flower has petals that are
this flower has petals that are
this flower has petals that are
this flower has petals that are
this flower has petals that are
this flower has petals that are
this flower has petals that are
this flower has petals that are
this flower has petals that are
bright yellow with brown stamen.
bright yellow with brown stamen.
bright yellow with brown stamen.
bright yellow with brown stamen.
bright yellow with brown stamen.
bright yellow with brown stamen.
yellow and has black stamen
yellow and has black stamen
yellow and has black stamen
bright yellow with brown stamen.
MMVAE
a medium sized black bird, with a thick thigh and bill.
a medium sized bird, all black, with a sharp bill.
a medium sized black bird with a strong thick beak
this bird has wings that are black and has a thick bill
a medium sized bird, all black, with a sharp bill.
this bird has a thick black pointed bill, with a black back.
a medium sized bird, all black, with a sharp bill.
this bird is all jet black, with a thick pointed bill.
this bird is fully black and has a black beak and black eyes.
this bird has a thick black pointed bill, with a black back.
HMVAE (proposed)
the breast of the bird is dark black contrasted to it’s white beak.
this bird has a long beak and black feathers on the top of its head.
this bird has wings that are black and has a rd head
this particular bird has a belly that is black and white patched
this fish eater has a very sharp, pointed beak used to spear fish and also has a grey
an all black medium sized bird.
this bird has a black and white bill along with a black crown.
this bird has wings that are black and has a big bill
bird has a grey beak and the rest of the bird is black .
this bird has a black and white bill along with a black crown.
HMVAE (proposed)
this flower has a yellow color on petals, as well as dark amber filament and anthers.
this flower has a brown center with layers of serrated-tip yellow petals.
this flower has long yellow petals and yellow anthers in the middle of it
the thin yellow petals surround bright gold stamens.
this flower has a brown center with layers of serrated-tip yellow petals.
this flower has three yellow, long petals with a sort of mouth in the middle containi
this flower has long yellow petals and yellow anthers in the middle of it
this flower is yellow in color, and has petals that are oval shaped and slightly ruff
this flower has a brown center with layers of serrated-tip yellow petals.
this flower has a brown center with layers of serrated-tip yellow petals.
HMVAE (proposed)
Figure 9:	CUB/Oxford Flower: Generating caption feature vectors from image feature vectors.
We generate caption features, look up the nearest-neighbor feature from the test set, and visualize
the respective caption.
(a)
this flower has wide, rounded pink petals and accents of yellow and brown,
the flower petals are yellow in color, the stamen are longer with larger anthers
this flower has a round spiked center mound and down-turned pink petals,
this flower is pink, red, and orange in color, with petals that are multi colored,
this flower is white and punk in color with wavy petals, and green pedecil.
this flower has many tiny white stamens engulfed in yellow and punk petals,
the flower is made of spiky triangular branches that are pink in color,
this light purple flower has a bell shape to it with six petals and its long,
this flower is white and pink in color, and has petals that are wilted and multi colored,
a yellow and purple flower with a green pedicel and green leaves
this flower has wide white petals which have rounded purple edges,
this flower is green and white in color, with petals that are star shaped,
this flower has petals that are pink and has patches of white
fc	reen ,the flower petals are triangular and are light pink in color
.	flower with pink petals and white stigma.
this flower has five purple and white accented petals that surround the darker pistil and stamen,
this flower has wide, rounded white petals which are soft and very smooth,
an orange and yellow large flower with long pedals and a tan center.
the flower is pink with petals that are soft, smooth, thin and separately arranged aro
the flower is so vivid with purple color and has petals that are soft and stamen are w
the petals are pink with a hint of orange on them.
this flower has four large white petals which are heart shaped and have purple accents,
the petals are veiny and purple and cover where the stamen and pistil are.
this flower has petals that are pink with black dotd
a flower with a very large inner stamen, and little dull pink pedals.
this flower has five, white petals in a distinctive arrangement and uniquely shaped pistil.
the petals of the large flower are white in color with a green stem.
a light and park pink flower with yellow on the petals near the stamin.
a purple and white group of flowers with black dots.
this flower has several protruding stamen surrounded by several wide pink petals.
this has a spiny type pedicel with a pink-lavendar hue flower that has many pollen and stigma leaves.
this particular flower has petals that are dark pink on the outside
the flower has petals that are pale pink and white with yellow stamen,
the petals on this flower are purple with numerous stamen
this flower is pink with a round shape of petals surrounding the ovary
these flowers have white petals with gold stamen in the center of it.
this sideways beauty has a nick purple tinted petal with its vines visible,
this flower has crimson petals and green, bumpy stamen that are not clustered too closely,
this flower has a large white petal and a white stigma in the middle
this flower has violet petals that form a large hollow tube around the stamen.
the flower has a white petals around the green pollen tube
these flowers have petals that are white and also light purple, while the pistil is yellow
this flower has spike like petals that are light red with yellow stamen
this flower has many tiny white stamens engulfed in yellow and punk petals.
the plants has large alternating petal that are rounded and pink in color with fewer stamen
the flower has petals that are pink, soft and separately arranged around stamens and forming a bowl like shape
this flower has tiny purple petals and five anthers at it's center.
the flower is so vivid with purple color and has petals that are soft and stamen are white and sticking out from the centre
this flower is pink and white in color, with wavy and wrinkled petals.
the petal on this flower is white and round with the filament making a visible appearance
(b)
Figure 10:	Oxford Flower: Unconditional generation. We generate p(xi |g) from g 〜p(g) for
i ∈ {1, 2}. In (b), we generate caption features, look up the nearest-neighbor feature from the test
set, and visualize the respective caption.
A.2 Image generation
In this section, we report additional results for § 4.3. Figure 10 visualizes unconditional samples.
We observe that the HMVAE can generate diverse samples without relying on any conditioning.
15
Under review as a conference paper at ICLR 2022
B	Likelihood estimation
In this section, we formalize the estimators that we use to evaluate the HMVAE, the MVAE (Wu and
Goodman, 2018), and the MMVAE (Shi et al., 2019). We report all likelihoods in nats.
B.1	Joint likelihood
We now focus on the joint likelihood over M modalities. The special case with M = 1 describes
the unimodal likelihoods p(xi).
Single latent variable We now discuss models that employ a single shared variable g. We approx-
imate the likelihood using K importance samples:
k=1
P(Xl:M, gk)
q(gk∣χiM)
where g1：K 〜q(g|xi：M)	⑺
We follow Shi et al. (2019), evaluate the mixture of experts VAE using stratified sampling, and esti-
mate a tighter estimate, which averages over modalities inside the logarithm. This objective should
only be used during the evaluation because it may lead to an outsized impact of more informa-
tive modalities during optimization. Notably, K samples of the joint posterior are equivalent to
T = K/M samples from each modality-specific posterior.
log p(x1:M) ≈ log
1 'X ɪ `X p(gm, xi：M)
M m=ι T = q(gm∖xm)
where g∖T 〜q(g∖xm)	(8)
Latent hierarchy We now discuss models that employ both a shared variable g and conditionally
independent, modality-specific variables zm,i. We approximate the likelihood using K importance
samples and omit the super- and subscripts in the sampling definition for improved readability:
1 K p(x1:M, gk, z)
log P(XIM) ≈ log K ⅛ q(gk, Z∖X1M )
where g, Z 〜q(g, z\xi：M)
(9)
The factorized generative and inference networks for the proposed latent hierarchy of the HMVAE
are formalized in Eqs. 4 and 5 of the main paper. Note that both product and mixture of experts
formulations of the shared posterior can be used within these hierarchical formulations.
B.2	Crossmodal likelihood
In this section, we discuss estimators for the crossmodal likelihood p(xt∖xc) given a target modality
t and a conditioning modality c. Analogously to Eq. 7, we evaluate the likelihoods using K impor-
tance samples.
Single latent variable
We first formalize the conditional likelihood similarly to Suzuki et al. (2016) and Wu and Goodman
(2018):
logp(xt∖xc) = log
p(xt, g∖xc)dg
g
log L「dg
log Eq(g|xc)
-P(Xt XG g)
一 q(g∖χc)
{z^^^^^^^^^^^^^
- log Eq(g|xc)
P(XG g) 一
q(g∖χc)
/
(10)
②
In Eq. 10, term ① constitutes an estimation of the joint likelihood p(xi：M) using an importance
distribution that is conditioned on modality c. Term ② estimates the unimodal likelihood P(XC).
16
Under review as a conference paper at ICLR 2022
Latent hierarchy We now generalize the estimator from Eq. 10 to the proposed latent hierarchy of
the HMVAE. We first formalize the conditional likelihood:
P(XtIxc)=(ZpM g, ziχc)dzdg=Zg,z P(Xtp Px Xz Xc) dzdg
(11)
We assume that the modality-specific generative networks are conditionally independent given the
shared representation (§ 3):
P(XtIXc)= Z p(Xt，ZtIg)P(Xc ZcIg)P(P) dzdg	(12)
g,z	p(xc)
We then define the importance distributions as the posterior over g and the conditional prior over Z .
The method must learn P(Xt IZt) with Zt being solely conditioned on the other modality c. This for-
mulation allows a fair comparison to the non-hierarchical baseline, which must do the same. In con-
trast, an importance distribution q(ZtIXt， g) would make the conditioning of P(XtIZt) multimodal.
P(Xc， g， Zc)	P(Xc， g， Zc)
log P(XtIXc) = log Eqdxc),p(ZtIg) P(Xt {zt, g) q(g, ZcIXc) - log Eq(g,zc|Xc) q(g, ZcIXc)
①
(13)
In Eq. 13, we can amortize computation for the unimodal likelihood P(Xc) which was defined in
App. B.1. Term ① is the only one that requires additional forward passes through the network.
Note that the remaining terms do not depend on the adjusted importance distribution P(Zt Ig).
17
Under review as a conference paper at ICLR 2022
C Implementation
C.1 General specifications
Hierarchical models The following formalizes latent
distributions assuming the unimodal hierarchical VAE in-
spired by S0nderby et al. (2016) from § 2. We start with
the conditional isotropic Gaussian prior
pφ,θ (zi |zi+1)	(14)
=N (Zilμθ (dφ,θ,i(zi+1)), σθ (dφ,θ,i(zi+1))),
where i ∈ {1, ..., L} refers to the hierarchical level. The
MLP dφ,θ,i represents the top-down network, which is
partially shared across the inference and generative net-
works. Its output is passed to the stochastic layers μθ and
σθ that parameterize the prior distribution.
We then focus on the more complicated case of inferring
the isotropic Gaussian posterior qφ,θ(z∕zi+ι, x) where
i < L for L latent variables (Fig. 11). We first infer the
hidden states for zi+1, x and for hierarchical level i ∈
{1, ..., L}, where dθ,φ is the top-down MLP and dφ the
bottom-up MLP:
(a)	(b)
Figure 11: Unimodal hierarchical
VAEs (S0nderby et al., 2016)
(a) Inference network. The red edges
visualize the merging procedure of
bottom-up and top-down information.
(b) Generative network.
hi,t = dφ,θ,i (zi+1),
hi,b = dφ,i(x).
(15)
We concatenate both hidden states along the first dimension and pass the result through another
neural network m@ to create thejoint representation. We denote the concatenation operation as〈•，•):
hi,j = mφ(hhi,t, hi,bi).
(16)
This joint representation is input to the stochastic layers μφ and σφ that parameterize the posterior
distribution:
qφ,θ (ZiIZi+1, X) = N (zi∖μφ(hi,j ), σφ(hi,j )).	(17)
The remaining priors and posteriors are parameterized similarly. Appendix A from Maal0e et al.
(2019) provides further background on unimodal hierarchical VAEs that inspire this work.
MVAE and MMVAE We follow Shi et al. (2019), train the MMVAE by maximizing a loose bound
and evaluate using a tight estimate as described in App. B.1. The following describes exemplary
differences in implementation compared to the original formulations:
•	We employ a slightly different network architecture for the MVAE and the MMVAE (Wu
and Goodman, 2018) relative to their original formulation, respectively. For example, we
do not include dropout regularization across all implementations.
•	For the MMVAE, we neither employ importance weighted autoencoders (Burda et al.,
2015) nor use a doubly reparameterized gradient estimator (Tucker et al., 2018). Note that
these techniques increase computational requirements. Neither the MVAE nor the HMVAE
relies on such measures.
•	We do not use Laplace latent distributions in the MMVAE, but Gaussian latent distributions
as in all implemented methods.
18
Under review as a conference paper at ICLR 2022
Dense: 16
Bottom-UP
g 〜q(g∣χm) —	⅜
Top-Down
Upsampling
Dense: 768
Merge
Dense: 768	∣ ∣ Dense: 768
(768,)	(768,)
Dense: 32	∣ ∣ Dense: 32
(16,)	(16,)
z1 z q(z1lg,x1)^^ Z1 - p(zι∖g)
UPSamPling
Dense: 768
(768,)
Dense: 768
Dense: 1024
3,a-3M3sseD
Dense: 1024
Dense: 1024
(1024,)
Data (2048,)
Reconstruction Layer
Dense: 4096
(2048,)
Dense: 16
Bottom-UP
g 〜q(gIxm)
dnE0c0g SSeD
Dense: 768
Dense: 1024
Data (1024,)
Top-Down
Upsampling
Dense: 768
(16,)	(16,)
z1 Z q(Z1|g,xI)	Z1 〜p(zι∣g)
3,a-3M3sseD
UPSamPling
Dense: 768
(768,)
(1024,)
Reconstruction Layer
Dense: 2048
(1024,)
(a)	(b)
Figure 12:	Feature genereation on CUB/Oxford Flower - HMVAE. (a) Image ft. VAE.(b) CaP-
tion ft. VAE. Both decoders parameterizes normal distributions. We do not display the LeakyReLU
activation functions.
C.2 Feature generation: CUB dataset
Data The CUB dataset includes an image and a caPtion modality. For the images, we use the
features vectors x1 ∈ R2048 Provided by Xian et al. (2018a), who emPloyed a ResNet-101 (He
et al., 2016) trained on ImageNet (Deng et al., 2009). As described in § 4, we use caPtion features
x2 ∈ R1024. We divide the dataset into three sPlits that do not share classes. The training set
consists of 111 classes. The validation sPlit consists of 39 classes and the test sPlit contains 50
classes. We train the model on the training set and tune hyPerParameters on the validation set. We
then evaluate the trained model on the test set, i.e., we do not use the validation set for training as in
some Previous works. We use the same test sPlit as Reed et al. (2016) who Provided the CNN-RNN
feature extractor for the caPtions, which is essential to avoid test data leakage. We standardize the
features for both modalities, i.e., We subtract the mean μ and divide by the standard deviation σ.
We comPute both Parameters on the training set. We assume that such PreProcessing can be helPful,
because the models generate a Gaussian likelihoodp(x∕∙).
All models The generative models maximize the likelihood of the data under learned Gaussian
distributions. We train the methods for 100 ePochs With a learning rate of 1e-4 and a batch size of
256. For each dataPoint in a batch, We use ten samPles from the resPective shared Posterior (i.e.,
g1:10 〜q(g\xi：M)) during training. We gradually increase the KL-regularization weighting factor
from zero to one for a warm-up period of 25 (S0nderby et al., 2016).
Proposed HMVAE The hierarchical VAEs are visualized in Fig. 12a and Fig. 12b. The shared
latent space g contains 16 dimensions.
MVAE and MMVAE The models are visualized in Table 3 and Table 3. We share the architecture
across baselines. Those models are similar in their architecture to the hierarchical method, but
lack the hierarchical components. We tested a version with additional layers to compensate for this
reduced capacity. However, models with more capacity (i.e., more weights) did not improve the
performance of the flat models. The shared latent space g contains eight dimensions.
19
Under review as a conference paper at ICLR 2022
Table 3: Feature generation on CUB/Oxford Flower - MVAE and MMVAE. Both decoders
parameterize normal distributions.
Image ft. encoder Image ft. decoder
Caption ft. encoder Caption ft. decoder
Input ∈ R2048
Dense(2048, 1024)
LeakyReLU
Dense(1024, 768)
LeakyReLU
Dense(768, 768)
LeakyReLU
Dense(768, 768)
LeakyReLU
Dense(768, 32)
Input ∈ R16
Dense(16, 768)
LeakyReLU
Dense(768, 768)
LeakyReLU
Dense(768, 1024)
LeakyReLU
Dense(1024, 1024)
LeakyReLU
Dense(1024, 4096)
Input ∈ R1024
Dense(1024, 1024)
LeakyReLU
Dense(1024, 768)
LeakyReLU
Dense(768, 768)
LeakyReLU
Dense(768, 768)
LeakyReLU
Dense(768, 32)
Input ∈ R16
Dense(16, 768)
LeakyReLU
Dense(768, 768)
LeakyReLU
Dense(768, 1024)
LeakyReLU
Dense(1024, 1024)
LeakyReLU
Dense(1024, 2048)
C.3 Feature generation: Oxford Flower dataset
Data The Oxford Flower dataset includes an image and a caption modality. We use feature vector
representations of the images, where x1 ∈ R2048, and of the captions, where x2 ∈ R1024. We stan-
dardize the features for both modalities, i.e., We subtract the mean μ and divide by the standard devi-
ation σ. We compute both parameters on the training set. We assume that such preprocessing can be
helpful, because the respective models generate a Gaussian likelihoodp(xi ∣∙). We divide the dataset
into three splits that do not share classes. The training set consists of62 classes. Both validation and
test split contain 20 classes. We train the model on the training set and tune hyperparameters on the
validation set. We then evaluate the trained model on the test set, i.e., We do not use the validation
set for training as in some previous Works. We use the same test split as Reed et al. (2016) Who pro-
vided the CNN-RNN feature extractor for the captions, Which is essential to avoid test data leakage.
All models The VAEs maximize the likelihood of each modality under a learned Gaussian distri-
bution. All models are trained for 100 epochs. We gradually increase the KL-regularization Weight-
ing factor from zero to one for a warm-up period of 25 (S0nderby et al., 2016). We train the models
With a learning rate of 1e-4 and a batch size of 256. For each datapoint in a batch, We use ten sam-
ples from the respective shared posterior (i.e., g1:10 〜q(g∣xi:M)) during training.
Proposed HMVAE Figure 12a displays the image VAE, Fig. 12b shows the caption feature vector
VAE.
MVAE and MMVAE Table 3 displays the image VAE, Table 3 shows the caption feature vector
VAE.
20
Under review as a conference paper at ICLR 2022
er
BQttOm-Up
g 〜q(g∣χm)
Dense: 200
Top-Down
Upsampling
Dense: 512
Top-Down
Dense: 512
Conv: 256, 4, 2, 1
dnE01"6 SSeo
Conv: 128, 4, 2,
Conv: 64, 4, 2,
Conv: 32, 4, 2,
Data (3, 64, 64)
3»la,-p3UJ」5U-=e°
Dense: 4096
ConvT: 128, 4, 2, 1
(128, 8, 8)
Z 、
Merge
Conv: 128, 1, 1, 0
Conv: 128, 1, 1, 0
(128, 8, 8)
(128, 8, 8)
Conv: 32, 1, 1, 0 ∣ ∣ Conv: 32, 1, 1, 0
(16, 8 8)
(16, 8, 8)
zι 〜q(zι∣g,xι)
Zl 〜p(zι∣g)
Upsampling
ConvT: 64, 1, 1, 0
(64, 8, 8)
ConvT: 64, 4, 2, 1
ConvT: 32, 4, 2, 1
ConvT: 3, 4, 2, 1
(3, 64, 64)
Reconstruction Layer
Conv: 3, 3, 1, 1
(3, 64, 64)
Bottom-Up
dnE≈"6 SSeo
Upsampling
Dense: 768
3∙,l*i∙-p3UJ.I5U-ie°
Dense: 768
Dense: 768
(768,)
Merge
Dense: 768∣ ∣Dense: 768
(768,)	(768,)
I	I
Dense: 32	∣ ∣ Dense: 32
(16,)	(16,)
zι 2 q(Zi|g,xI)	zι ~ p(zι∣g)
Dense: 768
Dense: 1024
Data (1024,)
(a)	(b)
Figure 13:	Image generation on Oxford Flower - HMVAE. (a) Image VAE. The decoder generates
scalars, which are used to compute the binary cross-entropy with the ground truth. We do not display
the Gelu activation functions (Hendrycks and Gimpel, 2016). (b) Caption ft. VAE. The decoder
parameterizes a normal distribution. We do not display the LeakyReLU activation functions.
Table 4: Image generation on Oxford Flower - number of trainable parameters. The second
column represents the number of latent variables. One of the reason the deeper HMVAE has fewer
parameters than the shallow HMVAE lies in the dense layer that maps between spatial (unimodal)
and vector (shared) representations. The deeper HMVAE achieves a lower spatial resolution and
which minimizes the parameters of this layer. Note that this single layer can easily reach more than
three million parameters, see the official MVAE implementation at www.github.com/mhw32/
multimodal-vae-public/blob/master/celeba/model.py. In general, we want to
primarily ensure that the HMVAE does not improve performance due to excessive capacity.
Model Name	# LV	# Params
MVAE (WU and Goodman, 2018)	1	19.2M
MMVAE (Shi et al., 2019)	1	19.2M
MDVAE (Mahajan et al., 2020), i.a.	3	13.3M
MHVAE (Vasco et al., 2020)	3	16.8M
HMVAE (shallow)	3	15.9M
HMVAE (this work)	6	11.6M
C.4 Image generation: Oxford Flower dataset
Data We preprocess the images by first making them quadratic (using CenterCrop in PyTorch) and
then resizing them to x1 ∈ R3×64×64 . Note that we described general specifications in App. C.3.
All models We train the models with a learning rate of 1e-4 and a batch size of 64.
Proposed HMVAE (shallow hierarchy) Figure 13a portrays the image VAE. Figure 13b presents
the hierarchical VAE for the other modality. We gradually increase the KL-regularization weighting
factor from zero to one for a warm-up period of 2θ (S0nderby et al., 2016).
Proposed HMVAE (deep hierarchy) To insert further hierarchical levels, we “slice” deterministic
blocks. We then add stochastic layers and respective pre- and postprocessing layers for each hierar-
21
Under review as a conference paper at ICLR 2022
chical level. Note that these layers often add just a few trainable parameters because of 1x1 convo-
lutions or small channel sizes for the stochastic variables. Note that the deeper model uses convolu-
tions blocks of three layers instead of single convolutional layers. Input and output channel dimen-
sion are identical on the outside and reduced within the block. We fix the channel size across the
deterministic parts of the model and gradually down- or upsample the spatial dimension. We down-
sample with average pooling and upsample using nearest-neighbor interpolation (Child, 2021). We
fix the channels for the latent variables along the image hierarchy to 16 and solely adjust the spatial
dimensions. We do not use any warm-up scheme for the KL-divergence term in the loss, i.e., we use
the natural ELBO.
MVAE and MMVAE The MVAE (Wu and Goodman, 2018) and MMVAE (Shi et al., 2019) share
their basic architecture. Table 5 displays the image VAE, Table 5 shows the caption feature VAE.
We add extra capacity to the decoder to compensate for the missing parameters from hierarchical
components. We gradually increase the KL-regularization weighting factor from zero to one for a
warm-up period of 20 (S0nderby et al., 2016).
Table 5: Image generation on Oxford Flower - MVAE and MMVAE. The image decoder gener-
ates scalars, which are used to compute the binary cross-entropy with the ground truth. The caption
decoder parameterizes a normal distribution. We use the Swish activation function (Ramachandran
et al., 2017).
Image ft. encoder	Image ft. decoder	Caption ft. encoder	Caption ft. decoder
Input ∈ R3×64×64	Input ∈ R100	Input ∈ R1024	Input ∈ R16
Conv(3, 32, 4, 2, 1)	Dense(100, 512)	Dense(1024, 1024)	Dense(16, 768)
Swish	Swish	LeakyReLU	LeakyReLU
Conv(32, 64, 4, 2, 1)	Dense(512, 4096)	Dense(1024, 768)	Dense(768, 768)
Swish	Swish	LeakyReLU	LeakyReLU
Conv(64, 128, 4, 2, 1)	ConvT(256, 128, 4, 2, 1, 0)	Dense(768, 1024)	Dense(768, 768)
Swish	Swish	LeakyReLU	LeakyReLU
Conv(64, 128, 1, 1, 0)	ConvT(128, 64, 1, 1, 0, 0)	Dense(1024, 768)	Dense(1024, 1024)
Swish	Swish	LeakyReLU	LeakyReLU
Conv(128, 256, 4, 2, 1)	ConvT(64, 64, 1, 1, 0, 0)	Dense(768, 768)	Dense(1024, 1024)
Swish	Swish	LeakyReLU	LeakyReLU
Dense(4096, 512)	ConvT(64, 64, 4, 2, 1, 0)	Dense(768, 768)	Dense(1024, 1024)
Swish	Swish	LeakyReLU	LeakyReLU
Dense(512, 200)	ConvT(64, 32, 4, 2, 0, 0) Swish ConvT(32, 3, 4, 2, 0, 0) Swish Conv(3, 3, 4, 2, 1) Sigmoid	Dense(768, 32)	Dense(1024, 1024) LeakyReLU Dense (1024, 1024) LeakyReLU Dense(1024, 1024) LeakyReLU Dense(1024, 2048)
MDVAE There are many implementation choices for MDVAEs (Huang et al., 2018; Hsu and Glass,
2018; Mahajan et al., 2020; Sutter et al., 2020; Daunhawer et al., 2021b; Lee and Pavlovic, 2021).
However, many related works employ a product of experts posterior (Eq. 2). This posterior choice
can produce poor results given the type of data considered in this work (where there is significant
variation in both modalities) (Shi et al., 2019). Therefore, we use a mixture of experts posterior
(Eq. 3) as in the HMVAE.The implemented MDVAE is almost identical to the HMVAE- except that
the unimodal variables z are marginally independent from g (Figs. 2b and 2c). We train the model
by maximizing the ELBO:
lθg pθ (XrM) ≥ Eqφ hlθg qφ(gpX(gM Q¾pqφz⅛Xi) +log QMI pθ (Xilg, Zi)i ,	(18)
where g ~ qφ(g|X1：M) and Zi ~ qφ(zi∣Xi) with i ∈ {1,…，M}. We use zι ∈ R8, g ∈ R100, z2 ∈
R8 and find that increasing the unimodal latent sizes tends to decrease crossmodal coherence. We
gradually increase the KL-regularization weighting factor from zero to one for a warm-up period of
2θ (S0nderby et al., 2016). Table 6 shows the implementation.
MHVAE We follow Vasco et al. (2020) (the MHVAE authors) and implement the generative net-
work from Fig. 3a for two hierarchical levels and the inference network from Fig. 2c. As done by
22
Under review as a conference paper at ICLR 2022
Table 6: Image generation on Oxford Flower - MDVAE. For both models, the encoder produces
a single tensor of size D. We split this tensor into two parts that represent g ∈ R100 and zi ∈ R8
for i ∈ {1, 2}, respectively. The decoder for xi uses a concatenation of zi and g as input, where
again i ∈ {1, 2}. For the image VAE, the decoder generates scalars, which are used to compute the
binary cross-entropy with the ground truth. For the caption VAE, the decoder parameterize normal
distributions.
Image encoder	Image decoder	Caption encoder	Caption decoder
Input ∈ R3×64×64	Input ∈ R100	Input ∈ R1024	Input ∈ R100
Conv(3, 32, 4, 2, 1)	Dense(100, 256 * 5 * 5)	Dense(1024, 1024)	Dense(16, 768)
Swish	Swish	LeakyReLU	LeakyReLU
Conv(32, 64, 4, 2, 1)	ConvT(256, 128, 4, 1,0)	Dense(1024, 768)	Dense(768, 768)
BatchNorm	BatchNorm	LeakyReLU	LeakyReLU
Swish	Swish	Dense(768, 768)	Dense(768, 1024)
Conv(64, 128, 4, 2, 1)	ConvT(128, 64, 4, 2, 1)	LeakyReLU	LeakyReLU
BatchNorm	BatchNorm	Dense(768, 768)	Dense(1024, 1024)
Swish	Swish	LeakyReLU	LeakyReLU
Conv(128, 256, 4, 1,0) BatchNorm Swish Dense(256*5*5, 512) LeakyReLU Dense(512, 200)	ConvT(64, 32, 4, 2, 1) BatchNorm Swish ConvT(32, 3, 4, 2, 1)	Dense(768, 200)	Dense(1024, 2048)
the authors, we employ domain dropout to learn representations over multiple modalities. We use a
uniform domain dropout distribution, i.e., maximize three ELBOs given importance samples from
q(g|x1), q(g|x2), and q(g|x1:M), respectively (Eq. 19, similar to Wu and Goodman (2018)).
We follow Vasco et al. (2020) and maximize the sum over the multi- and unimodal ELBOs:
L
Ljoint + Limages + Lcaptions
MM
Ljoint = EEqφ(Zi∣Xi) [logPθ(Xi∣Zi)] - EEqφ(Zi∣Xi)
i=1	i=1
pθ(g)
-	Eqφ(glx1M) [log qφ(g∣XiM)一
MM
Limages:=	Eqφ(Zi∣Xi) [log Pθ (XiIzi)]- Σ qφ (zi |xi )
i=1	i=1
pθ(g)
-	EqMglx1) [log qφwn
MM
Lcaptions := ∑Eqφ(Zi∣Xi) [log Pθ (XiIzi)]- Σ Eqφ(zi∣xi)
i=1	i=1
pθ(g)
-	Eqφ(glx2) [log qφW2L
pθ (zi Ig)
.°g qφ(zi |xi).
pθ (zi Ig)
.θg qφ(zi |xi) 一
l pθ (zi |g) 一
.θg qφ(zi |xi) 一
(19)
We gradually increase a factor before the KL-divergence term from zero to one for five epochs
(unimodal variables) or ten epochs (shared variable) as done by Vasco et al. (2020). We use z1 ∈
R256,g ∈ R100, z2 ∈ R8. Tables 7 and 8 present the architecture.
23
Under review as a conference paper at ICLR 2022
Table 7: Image generation on Oxford Flower - MHVAE (1)
Networks for x1: We follow Vasco et al. (2020) and use the miniature DCGAN architecture (Radford
et al., 2015). The decoder generates scalars, which are used to compute the binary cross-entropy with
the ground truth. Networks for x2 : The decoder for the second modality parameterizes a normal
distribution.
Lower encoder: x1 → h1	Lower decoder: Z1 → x1	Lower encoder: x2 → h2	Lower decoder: Z2 → x2
Input ∈ R3×64×64	Input ∈ R128	Input ∈ R1024	Input ∈ R48
Conv(3,32,4,2,1)	Dense(128, 256*5*5)	Dense(1024, 1024)	Dense(48, 768)
Swish	ConvT(256, 128, 4, 1,0)	LeakyReLU	LeakyReLU
Conv(32, 64, 4, 2, 1)	BatchNorm	Dense(1024, 768)	Dense(768, 1024)
Swish	Swish	LeakyReLU	LeakyReLU
Conv(64, 128, 4, 2, 1)	ConvT(128, 64, 4, 2, 1)	Dense(768, 768)	Dense(1024, 1024)
BatchNorm	BatchNorm	LeakyReLU	LeakyReLU
Swish Conv(128, 256, 4, 1, 0) BatchNorm Swish Dense(256*5*5, 512) Swish	Swish ConvT(64, 32, 4, 2, 1) BatchNorm Swish ConvT(32, 3, 4, 2, 1)	Dense(768, 512) LeakyReLU	Dense(1024, 1024*2)
Table 8: Image generation on Oxford Flower - MHVAE (2)
The networks for g → Z and h → Z are identical across modalities - except for the final output
dimension which depends on the latent size, where z1 ∈ R128 and z2 ∈ R48.
Upper encoder: h→g	Upper decoder:	Encoder: g→Z	h→Z
Input ∈ R512*2 Dense(512*2, 768) LeakyReLU Dense(768, 768) LeakyReLU Dense(768, 768) LeakyReLU Dense(768, 100*2)	Input ∈ R100	Input ∈ R100 Dense(-, 500)	Dense(512, -) LeakyReLU Dense(500, 500) LeakyReLU Dense(500, 500) LeakyReLU Dense(500, -)
24
Under review as a conference paper at ICLR 2022
Figure 14: MDVAE: Alternative inference network
D MDVAE: alternative inference network
There are several inference network choices for multimodal disentanglement VAEs (MDVAEs):
Fig. 2c depicts the natural inference network. Figure 14 modifies this network to include dependen-
cies between g and z1:M . This modified architecture recovers our proposed hierarchical inference
network for the special case of two hierarchical layers (Fig. 3b). Note that the edges between g and
z1:2 are still absent in the generative model (Fig. 2b): the model must learn to represent independent
variation in zι. For example, the model must generate pθ(xi∣Zi, g) from Zi 〜pθ(zι) (not neces-
sarily Zi 〜qφ(zι∖xi, g)) and g 〜qφ(g∣Xj=i).
E Ethics S tatement
Generative models can have pernicious effects on society via creating and propagating synthetic data
that mimics reality (e.g., DeepFakes). Extending these models to multiple modalities can strengthen
their performance and hence amplify these challenges. Therefore, annotating and marking data as
synthetically generated is vital to ensure that people can spot and identify synthetic data outside of
contexts where their synthetic nature is clear. Furthermore, the training data for deep generative
models rarely represents the rich diversity of people, images, and objects in the world. Biases
towards overrepresented groups will inevitably creep into generative models, and if the model is used
for solving classification tasks on underrepresented groups, it is unlikely to prove useful. Extending
the training data to multiple modalities can mitigate these effects if the additional modalities are
less biased. In general, building generative models that can better cope with heterogeneity, e.g., by
improving models or collecting more realistic data, is a good step towards alleviating harmful biases.
25