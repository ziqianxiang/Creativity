Under review as a conference paper at ICLR 2022
Fast Stackelberg Deterministic Actor-Critic
Anonymous authors
Paper under double-blind review
Ab stract
Most advanced Actor-Critic (AC) approaches update the actor and critic concur-
rently through (stochastic) Gradient Descents (GD), which may be trapped into
bad local optimality due to the instability of these simultaneous updating schemes.
Stackelberg AC learning scheme alleviates these limitations by adding a compen-
sated indirect gradient terms to the GD. However, the indirect gradient terms are
time-consuming to calculate, and the convergence rate is also relatively slow. To
alleviates these challenges, we find that in the Deterministic Policy Gradient fam-
ily, by removing the terms that contain Hessian matrices and adopting the block
diagonal approximation technique to approximate the remaining inverse matrices,
we can construct an approximated Stackelberg AC learning scheme that is easy to
compute and fast to converge. Experiments reveal that ours outperform SOTAs in
terms of average returns under acceptable training time.
1	Introduction
Reinforcement learning (RL) has been widely adopted for solving various complicated sequen-
tial problems such as games (Mnih et al., 2013) and robotics applications (Kober et al., 2013).
In particular, the actor-critic (AC) approach has achieved remarkable performance in many real-
world tasks (Arulkumaran et al., 2017). Most AC methods perform stochastic gradient descent on
the actor and critic simultaneously. This can be regarded as performing Gradient Descent Ascent
(GDA) (Zheng et al., 2021a;b; Wen et al., 2021), which is known to suffer from convergence to limit
cycles or bad locally optimal solutions (Wang et al., 2019; Jin et al., 2020; Balduzzi et al., 2018).
Moreover, as mentioned in Yang et al. (2019), the GDA training scheme is fragile, which could lead
to biased critic and may not converge.
To address these issues, Zheng et al. (2021a) and Wen et al. (2021) reformulate the actor-critic
problem as a bilevel optimization problem, and introduce a leader-follower (Stackelberg) training
scheme (Fiez et al., 2020). However, this requires the computation of several indirect gradient terms
(including the inverse Hessian matrices), which are difficult to compute or accurately approximated.
Moreover, empirically, this Stackelberg training scheme converges slower than GDA in the envi-
ronments where GDA is not trapped into limited cycles or bad local optimums, since the indirect
gradient terms pull back the parameters to stay within the best response areas to guarantee that the
leader’s loss is always small (Wang et al., 2019). To reduce the computational complexity, Shewchuk
(2007) introduces the use of conjugate gradient (CG) descent, which simplifies the computation of
the approximated inverse Hessian to the solving of a linear system. However, to have good perfor-
mance, a sufficient number of inner iterations is still required to obtain a good approximation of the
Hessian matrices. Empirically, its convergence is still not fast.
Among these actor-critic methods, the deterministic policy gradient (DPG) family (Silver et al.,
2014; Lillicrap et al., 2016; Fujimoto et al., 2018; Haarnoja et al., 2018) has some advantages over
other methods: if we assume the actor to be the follower, and the critic to be the leader, the gradient
of the follower can be divided into several terms, some are closely related to the best response
function which slows down the training process. Also, they include Hessian matrices which are
hard to compute (We name it as the TD-related term which is formally discussed in Sec. 4). Based
on this observation, we propose Fast Stackelberg Deep Deterministic Policy Gradient (FSDDPG),
which reduces these TD-related terms to improve the convergence rate and reduce training time.
Moreover, together with the block diagonal approximation technique to approximate the remaining
matrices, it is possible to further reduce the training complexity from O(n3m3) to O(n2m2), where
n is the number of layers and m is the number of neurons in each layer. Experiments reveal that with
our approximated methods outperform SOTAs in terms of average returns under acceptable training
time.
1
Under review as a conference paper at ICLR 2022
2	Related Works
Actor Critic. The AC methods are widely adopted in solving complicated sequential problems (Sut-
ton & Barto, 2018). Among these methods, the DPG family are prevalent since they can be both
off and on policy and able to be deployed in discrete time and continuous time environments, which
are efficient and suit various environments (Lillicrap et al., 2016). However, most of the ACs use
simultaneously update rule, i.e., the actor and critic update with the same learning rate, which are
not always stable. Empirically, the AC may find better solutions when the critic is updated more
frequently than the actor (Wen et al., 2021), and we also show it in our experiments in Sec. 6. The-
oretically, most of the stable training methods of GDAs are based on the Two Time Scale Updates
(TTSU) scheme (Wu et al., 2020; Dalal et al., 2018). Although the simultaneously update rule for
AC can also be proven convergence (Fu et al., 2020), it may not suit all the environments since AC
(GDA) may trapped into limited cycles and bad local optimality (Zhang et al., 2020; Wang et al.,
2019). Regarding TTSU, since it requires the learning rate of the follower to be slower than that of
the leader, its convergence rate is slow consequently (Kaledin et al., 2020).
As the AC training schemes can be also represented as a bi-level optimization process, it is naturally
to introduce Stackelberg training scheme to AC as the Stackelberg Actor Critic to stabilize train-
ing (Zheng et al., 2021a; Wen et al., 2021). However, the gradient of the follower is time consuming
to calculate, and the convergence rate is relatively slow. Thus, in many environments, Stackelberg
training may perform worse than vanilla AC under acceptable training time (Wen et al., 2021).
Gradient-based bi-level optimization. The gradient-based bi-level optimization is used to minimize/-
maximize a cost function defined in terms of the optimal solution to another cost function (Sinha
et al., 2017). Currently, the idea of differential Stackelberg (leader-follower) game is introduced to
convert these problems into the differential (Stackelberg) game problems (Fiez et al., 2020). To solve
the differential Stackelberg game, the Stackelberg gradient descent is introduced by leveraging the
implicit function theorem (Ji et al., 2021; Fiez et al., 2020; Grazzi et al., 2020), which has already be
applied into different areas, e.g., GAN (Fiez et al., 2020), hyper-parameter optimization (Lorraine
et al., 2020), as well as meta learning (Rajeswaran et al., 2019).
Approximating (inverse) Hessian. Calculating the (inverse) Hessian is time-consuming, especially
for the deep neural networks (Martens & Grosse, 2015; Botev et al., 2017). To address this chal-
lenge, the conjugate gradient approximation (Rajeswaran et al., 2019; Mehra, 1969; Ghadimi &
Wang, 2018), block diagonal approximation (Martens & Grosse, 2015; Botev et al., 2017), as well as
the approximate NeWton methods (Martinez, 2000; XU et al., 2016) are proposed. Here, We choose
the block diagonal approximation, since it is easy to be implemented and fast to compute while
maintaining a good performance. Another method to approximate Hessian is the GaUss NeWton
matrix (SchraUdolph, 2002), Which is generally Used to approximate Hessian for the Mean SqUared
loss (MSE) (Martens, 2020). GaUss NeWton matrix oUtperforms Hessian matrix for its fast compUt-
ing, and stability since it is alWays positive semi-definite (Chen, 2011). HoWever, it still lacks the
obvioUs jUstification Why dropping the Hessian term helps convergence (Martens, 2020).
3	Problem Formulation
A Markov decision process (MDP) can represented by the tUple hS, A, R, P(S|S, A), γi, Where s ∈
S is the state, a ∈ A is the action, p(s0|s, a) is the transition probability, and r(s, a) ∈ [Rmin, Rmax]
is the boUnded reWard fUnction. ρβ (s) is the discoUnted state distribUtion of s condUcted by a
behavior policy β 1. π : S × A → ∆(A) is the policy, Where ∆(A) is the simplex of action set A.
Qn is the state-action value function w.r.t. π: Qn (s,a) := En [Pk Yk丁卜 | s,a]. Here, we parameterize
Q and ∏ with W ∈ Ω and θ ∈ Θ, respectively (Qω and ∏θ), where Ω and Θ are the parameter spaces.
Following (Zheng et al., 2021a), we formulate the actor-critic as a differential Stackelberg
game (Fiez etal∙,2020):	max@ J(θ,w*(θ))	(1)
s.t.	w*(θ) = arg min L(θ,w),
w
whereJ (θ,w) = Es 〜。⑸,。〜∏(∙∣s)[QWθ (s, a)], L(θ, w) = Es 〜P(S),a 〜∏(∙∣s) [(QWθ (s,a)- ⊥ [Qπθ (s,a)])2],
which is the temporal difference (TD) error, and ⊥ (∙) is the stop-gradient operator to achieve
semi-gradient method (Sutton & Barto, 2018).
1For compactness, we omit the superscript of ρβ (s) as ρ(s) with a little abuse of notations.
2
Under review as a conference paper at ICLR 2022
Stackelberg learning (or approximate implicit differentiation) (Fiez et al., 2020; Ji et al., 2021) is a
framework for solving the bilevel optimization problems (Ji et al., 2021; Fiez et al., 2020; Grazzi
et al., 2020), and can be used on most AC variants (e.g., DDPG (Lillicrap et al., 2016), TD3 (Fuji-
moto et al., 2018) and SAC (Haarnoja et al., 2018)). Using it on (1), we obtain (Fiez et al., 2020):
θ 一 θ + αd (L ,w --αw —,	⑵
∂θ ∂w
where d(θ,W*(θ)) = d(θ,w) - (dL∙w 1 (d2L(θ,w) 1 1dJ(θ,w) , and α, αω are learning rates
∂θ ∂θ ∂θ∂w	∂w2	∂w	ω
for the actor and critic, respectively.2 Here,
(∂2L(θ,w) λ (∂2L(θ,w) ʌ T ∂J(θ,w)	(3)
I ∂θ∂w ) I ∂w ) ∂w
is known as the indirect gradient term (Lorraine et al., 2020).
Empirically, AC with Stackelberg training does not improve the performance of ACs in many envi-
ronments (Wen et al., 2021). This is also confirmed by our experiments in Sec. 6. This is mainly
due to the fact that Stackelberg training converges slowly, and thus it performs worse than vanilla
AC when AC does not be trapped into limited cycles and some bad local optimality. Also, inverting
the n × n Hessian matrix in (3) takes O(n3) time, where n is the matrix size and is equal to the
number of neural network parameters. Although Conjugate Gradient (CG) can be used to reduce the
computing complexity (Zheng et al., 2021a), it still needs sufficient steps for CG to obtain a precise
approximation of the Hessian matrix.
In this paper, we focus on improving Stackelberg learning in DDPG (Lillicrap et al., 2016) and
TD3 (Fujimoto et al., 2018). DDPG and TD3 have been widely adopted for solving compli-
Cated continuous control environments (Wang et al., 2020). In DPG family, π(a∣s) degener-
ates to π(a∣s) = δ(a - ∏(s)), where ∏θ(s) : S → A is the deterministic policy, and δ(∙)
is the Dirac-Delta function. L(θ, w) becomes Eρ(s) (Qπwθ (s, a)- ⊥ [Qπθ (s, πθ (s))])2 , where
Qπwθ (s, a):= Qw (s, a), s.t. a = πθ (s). J(θ, w) becomes Es〜ρ(s) [QWθ (S,a)].
Notice that the practical DPG based methods update the critic by using L(θ,w)	=
Eρ(s),a〜β [(QWθ(s,a)- ⊥ [Qπθ(s, a)])2] 3 (Lillicrapetal., 2016; Fujimoto et al., 2018). We follow
the critic’s loss, and the modified update scheme becomes:
. . . .. . ^ .
θ 一 θ + αdJ (「, W ~w-αw —,	(4)
∂θ	∂w
Comparing with Eq. (2), the update scheme for policy θ maintains the same. The main reason is
that VθL(θ,w) = Eρ(s),a〜β [Vθ(QW(s,a) - Qπθ(s,a))2] is not well-defined when a = π(s),
and thus we still use L(θ, w) to update the policy.
To help our analysis, we denote algorithms that replace the vanilla policy gradient in Eq. (4) to DPG
in DDPG (resp. TD3) as SDDPG and (resp. STD3), respectively.
4 Proposed Method
To alleviate the two challenges in Section 3, we find that the indirect gradient term for the DPG
family (Silver et al., 2014) can be divided into several terms. The TD-related terms are closely
related to the TD loss of the critic and contain Hessian matrices.
When the TD loss is small, these TD-related terms can be eliminated (Sec. 4.1). As discussed
in (Wang et al., 2019), the indirect gradient term pulls the follower to stay along in the best re-
sponse area, which may hinder the update scheme and slows convergence: Formally speaking,
the best response function b(w) : Ω → Θ is the implicit function defined by 叱：=)=0.
It can be shown that Vwb(w) = ALdww d Lw2W) (Fiez et al., 2020). Hence, We have
-a *∂θ∂Ww /2Lw2w dJ(WW) = Vw b(w) (-α Jww) ).If θt ≈ b(wt) (θt is closed to the best
2For simplicity of notations, we write ∂w2 := ∂w> ∂w and ∂θ∂w := ∂θ>∂w.
3For simplification, We omit ⊥ (∙) in ⊥ [Qπθ (s, a)].
3
Under review as a conference paper at ICLR 2022
response parameters b(wt) ), then θt - Rwtb(wt) α
∂J(θt, wt)
∂wt
b(wt) - Rwtb(wt)(wt+ι -
wt) ≈ b(wt+1), where ≈ is according to the Taylor’s series approximation. It means that LHS tries
to keep follower in the best response area. Therefore, since the convergence rate is slow with the
indirect gradient term; while the training may be not stable without it (Wang et al., 2019), could we
find a balance that maintain the good properties for both fast convergence and stability by removing
some terms which are closely related to the best response indicator L(θ, w)?
Here, we observe that the TD-related term is a good option because it contains the residue term
Qπwθ (s, a) - Qπθ (s, a) that are closely connected to L(θ, w), which indicates the distance to best
response. Thus, removing them could reduce the influence of the best response in indirect gradient
to the follower (actor). Another advantage is that since (partial of) the Hessian matrices d QdL2s,a)
and d (Qwdθ,a are hard to compute, reducing them can help improve the computing time.
But, one still needs to compute the inverse matrix, which dominates the time complexity for com-
puting the gradients. To simplify the computation, in Sec. 4.2 we propose to use a block-diagonal
approximation to approximate the inverse matrix.
4.1 DERIVATIVES OF d2Lθw AND d2Lθw
∂θ∂w	∂w2
First, we show how to simplify the indirect gradient term in (3) by proposing the following proposi-
tion:
Proposition 1. For SDDPG and STD3, d Ldww, and d LW2w) Can be further explicitly expressed
as:
∂2L(θ,w)
∂θ∂w
2EP(S)	∂wQwθ(S，
(∂WQwθ(s,a)
+ (QWθ (s,a) - Qπθ (s,a)) (^d> ^-QWθ (s,a)
∂θ> ∂w
(5)
∂2L(θ,w).	(
∂w2	= EP(S) (2
∂Qπwθ (s, a) ∂Qπwθ (s, a)>
∂w
∂w
+ 2(QWθ (s,a) — Qπθ (s,a))弋W(；,a))∙	(6)
Proofs can be found in Appendix A. Notice that here we use the chain rules of composite functions
Qπwθ (s, a), which is different from the original DPG form (Silver et al., 2014). Our form is more
suitable for our bi-level optimization settings. Discussion can be found in Remark 1 in Appendix A.
Also note that both (5) and (6) contain the residue term Qπwθ (s, a) - Qπθ (s, a), which can be ignored
if Eρ(s) (Qπwθ (s, a) - Qπθ (s, a))2 is small. Eqs. (5) and (6) then degenerate to:
∂2L(θ,w)
∂θ∂w
∂2L(θ,w)
∂
≈	2Eρ(s) ( ∂WQw (S，a)
∂w >	∂
∂θ
∂w2
≈	Eρ(s)	2
∂QWθ (s, a) ∂QWθ (s, a)
∂w
∂w
∂w
>
Qπwθ (s, a)	:= k1 (θ, w),
:= k2(θ, w).
Substituting k1 (θ, w), k2 (θ, w) into (4), the follower’s approximated update scheme becomes4:
θt+ι - θt + α
∂J(θt, wt)
∂θt
- k1 k2-
1 ∂J(θt, wt)
∂wt
(7)
The leader update scheme is still the same as in (4). Intuitively, (7) reduces the TD-related terms
which consequently reduce the time from O m3n3T to O max{m2n2 , mno2 }T to compute
the gradients. Details are shown in Sec. 4.3.
Notice that k2 (θ, w) is also known as the Gauss Newton matrix (Schraudolph, 2002). Besides fast
computing, the Gauss Newton matrix is always Positive Semi-Definite (PSD) which helps better
convergence comparing with the Hessian term d Lwθ,w, which may not guarantee PSD (Martens,
2020). This gives us a numerical analysis perspective why the our approximation works.
We will show that it improves the convergence rate theoretically and empirically in Section 5.
4For simplification, we use k1 and k2 to denote k1 (w, θ) and k2 (w, θ).
4
Under review as a conference paper at ICLR 2022
4.2 APPROXIMATED INVERSE k2
k2(θ, w) and k1 (θ, w) do not include any Hes-
sian term, which avoids O(m3n) for computing
partial of the Hessian matrix. However, com-
puting the gradients of the FSDDPG is still ex-
pensive since inverting k2 (nm × nm size ma-
trix) usually takes O(m3n3) time. 5
To accelerate the computation of k2-1, we use a
block-diagonal approximation scheme:
T	P f f	f I	T~ ∙	.1
k2 `	k	:= kι	㊉ k2㊉...㊉ kd, where	k is the
ith diagonal block of k?,㊉ is the direct sum
operator, and d ∈ Z+ is the number of diagonal
blocks which can be set manually. Define o :=
d|nm, where | is the integer division operator as
the diagonal degree. Also notice that the time
complexity of inverting a block diagonal matrix
Algorithm 1 FSDDPG algorithm
Randomly initialize Q(s, a|w) and π(a∣s, θ).
Initialize target network Q0, π0, and replay
buffer R.
for episode = 1, M do
Using π to interact with environment to ob-
tain transitions (st, at, rt, st+1).
R = R∪ (st, at, rt, st+1).
Sample N transitions from R.
^
Update the critic using ―∂(ww).
Update the actor policy using the sampled
policy gradient: Jw - kιk-1 怨Ww)
Update the target networks.
end for
is equal to the sum of inverting each block (Boyd et al., 2004). Thus the time complexity of inverting
the matrix k? is reduced from O ((nm)3) to O (d(nm )3) = O (mn (o)3) = O (mno2). 6
Since inverting k2 dominates the time complexity, when it is reduced, time complexity reduces to
O max{m?n?, mno?}t consequently. More details can be found in Appendix C.
4.3 Time complexity for computing the gradient
The time complexity analysis table is shown in Tab. 1. SDDPG is the Stackelberg learning method
directly implemented to DDPG (as mentioned in Sec. 3), and SDDPG-CG is the SDDPG with
Conjugate Gradient (CG) technique to approximate for the inverse Hessian matrix (Zheng et al.,
2021a; Wen et al., 2021). SDDPG-BD is SDDPG with the block diagnal technique.
From empirical studies, we found that taking o = 1 (just taking the diagonal) is sufficient to be well-
performing. Also, is always small to guarantee performance. Thus, in practical implementation,
ours is faster than SDDPG-CG.
Notice that the diagonal block technique may not suit vanilla SDDPG well. The main reason is that
the term d Ldwww still contains (partial) of the Hessian which needs O(n2m3) to compute, which
have been removed in ours. Thus, the time complexity of SDDPG with diagonal block technique
(SDDPG-DB) is larger than ours when o is small. Moreover, the mainstream deep learning frame-
work (e.g., PyTorch (Paszke et al., 2019)) does not support calculate diago(d ILdww)) in parallel
through GPU, which makes it hard to implement in practice. Details are in Appendix G. Therefore,
the diagonal block technique is an approach that design for our reduction.
Our method can also adopt CG to approximate the k-1 IJ|wWt), which solves the linear system
k2x = Jdwt, where X is the vector to be solved. We name this method as FSDDPG-CG (ours).
Table 1: Time complexities of calculating the follower’s gradient for SDDPG, FSDDPG (ours),
SDDPG-CG, and SDDPG-BD. Here, nis the size of layer, andm is the number of neurons in each
layer. T is the total number of episodes. o ∈ [1, mn] is the diagonal degree. is the -optimal
solutions for CG.
∏	I SDDPG	FSDDPG	SDDPG-CG	SDDPG-BD	∏
H TimeComplexity ∣ O (m3n3T)	O (max{m2n2, mno2}T)	O (m2n2T∕e)	O (max {nmo2,n2m3} T) H
5 Theoretical Analysis
This section focuses on some theoretical results of our FSDDPG.
5In this subsection, we regards k2 as a nm × nm matrix since θ and w are fixed.
6We assume nm is a interger.
5
Under review as a conference paper at ICLR 2022
Before we go deeper into our analysis, we define some useful notations that help the analysis:
d-J(θw _ k (θ) k-1(θ) d-J(θw
ξ(θ) := dθ	f 2 J dw , is the dynamics of our FSDDPG, τ := αw is
∂	∂L(θ,w)	,	j	,	a
_	T ∂w	_
defined as the time separation.
we also define the Jacobian of the dynamics of our method as:
JS(θ) :
J ∂-J(θ,w) _ k k-1 ∂-J(θ,w)	∂ ∂-J(θ,w) _ k k-1 ∂-J(θ,w)λ
∂θ k ∂θ	k1k2 一∂w ∂ ∂w k ∂θ	k1k2	∂w )
一 ∂2L(θ,w)	丁 ∂2L(θ,w)
∂θ∂w	∂w2
and S (θ*) := 2 (JS (θ*)> + JS (θ*)} where θ := [θ, w] (θ* := [θ*,w*] is a fixed point), and
T (θ*) = (JS (θ*)> JS (θ*)) which helps our analysis.
Now, we prove that our method can converge to a fixed point.
Theorem 1. When π ∈ Cq (S 0 Θ, R) and Q ∈ Cq (S 0 A 0 W, R), q ≥ 2. For a fixed
point θ* and w* such that JS (θ*)> + JS (θ*) is positive-definite, the our method with learning
rate α = √ψυ converges locally with a rate of O ((l 一 2ψ) / } where U =》言也(S (θ*)),
ψ = λmax (T (θ*)).
The proof sketch follows Theorem 5 in (Fiez et al., 2020). To compare the convergence rate of FSD-
DPG (ours) and SDDPG, We firstly define linear actor as ∏ = 夕(s)θ, where 夕(S) is the embedding
(abstract state), θ is the learnable parameters. The linear critic is Q(s, a)
夕(S)
Ψ(s)θ
w , where w is
the learnable parameters for critic.
We now show that for linear actor and critic, under some mild assumptions, the convergence rate
OfFSDDPG is equivalent to or faster than that of SDDPG, which is O ((1 一 七)/ ) (The proof
is similar to Theorem 1 and we provide details in Appendix D), where U =1.由(S (θ*)), ψ =
λmaχ (T (θ*)), and S(∙) as wells as T(∙) are defined similar to S(∙) and T(∙) by replacing kιk-1
with (IdLdwL) (d2Lwθ2,W)) 1in JS3, respectively.
Theorem 2. For linear actor and critic, if
w|A
中⑹
中(S)θ
< 0, where A is any Positive Semi
Definite (PSD) matrix. Then, the convergence rate of FSDDPG (ours) is faster than or equivalent to
that of SDDPG.
The proof sketch relies on Matrix inequality. The motivation behind this is that we can show that
S (θ*) (resp. (JS (θ*)> JS (θ*))) with TD-related term minus S (θ*) (resp. (JS (θ*)> JS (θ*)))
is always PSD (resp. NSD), which can imply the relationship in eigenvalues of S (θ*) and
JS (θ*)> JS (θ*) . Notice that since the convergence rate is decided by these eigenvalues (Theo-
rem 1), we can thus obtain the result.
We can use toy example 1 as an application of Theorem 2 (environment setting is in Sec. 6.1 ).
After calculation, we find that the the theoretical convergence of SDDPG is near O(1T /2), while
FSDDPG is O(0.75T /2 ). This coincide with our empirical result that ours is faster than SDDPG.
Details are in Appendix D.
The following we show some properties of our method (Details are in Appendix F):
We show the connections between Eq. (2), FSDDPG (ours) and SDDPG:
Corollary 1. If hθ*, w*i is the optimal solution for SDDPG. Then, hθ*, w*i is also the optimal
solutions for Eq. (2) as well as FSDDPG.
6
Under review as a conference paper at ICLR 2022
Figure 1: Toy examples. The first row is the toy example 1 and the second row is the toy example
2. The first column is the trajectories of θ and w (The contours are for L(θ, w) value). The second
column is the trajectories of θ and w w.r.t different episodes (The contours are for J (θ, w) value).
The third column is the training curves. The forth column is the enlarged part of the training curves.
We also show that our method can avoid strict saddle point almost surely under mild assumption in
Appendix F.
6 Experiments
6.1	Experiments on toy examples
Environment Setup. Inspired by (Zheng et al., 2021a), we conduct two toy examples to illustrate
that our methods are fast convergence and do not being easily trapped into bad local optimality
and limited cycles. In the first example, we set both actor and critic as a one-dimension scale θ
and W respectively. The reward is set to r(θ) = — θ2. Thus, L(θ, w) = L(θ, w) = (wθ2 + θ2)2,
J(θ,w) = wθ2. The optimal solution of the the first toy example is θ* = 0,w* = —1. The second
example is similar except that We set L(θ,w) = L(θ,w) = (wθ + θ2)2, J(θ,w) = wθ. The
optimal solution is θ* = 0, w* = 0. The initial point for the two examples are θ = 1, w = 1 and
θ = 2, w = 2, respectively. The distance to optimality is defined as (w — w*)2 + (θ — θ*)2. Toy
example 2 is harder than Toy example 1 since the best response w.r.t w are w = —1 and w = θ
respectively, i.e., w* is a constant in toy example 1, while w* needs to trace the value of θ in toy
example 2.
Baselines. We compare our methods (without block diagonal approximation technique) with for
DDPG (Lillicrap et al., 2016), and SDDPG (Zheng et al., 2021a). Notice that due to both the
parameters for actor and critic are one-dimension.
Results. As shown in Fig. 1, for the first example, DDPG and FSDDPG (ours) converge faster than
SDDPG, while DDPG is much more unstable than ours and SDDPG since it moves very far away
from the goal. For the second example, DDPG is trapped into the limited cycle, while both FSDDPG
(ours) and SDDPG finds the optimal efficiently. Also, ours converges faster than SDDPG. These
results reveal that DDPG converges fast yet relative unstable. SDDPG is stable but converges slowly.
Ours method combines both the advantages of DDPG and SDDPG, i.e., it converges fast while
maintain the stability during training. These results coincide with the assumption that eliminating
the TD-related term can improve the convergence rate in Sec. 4.
We also visualize the trajectories on the contours of the J(θ, w*) value as in Appendix H. Results
find that SDDPG tries to stay in the best response area, while ours are somewhat drifts from best
response area, but it returns back only after a few episodes. DDPG could drives very far from the
best response area.
6.2	Experiments on MuJoCo
Environment Setup. We conduct our experiments in standard physical simulator MuJoCo (Todorov
et al., 2012), including Ant, HalfCheetah, Humanoid, InvertedDoublePendulum, InvertedPendulum,
Swimmer, Reacher, and Walker. The network sizes are 16 × 16 for all the methods to make SDDPG
and STD3 trainable, larger networks make SDDPG and STD3 hard to train. All the experiments
are conducted on a single GeForce RTX 2080 Ti GPU and Intel(R) Xeon(R) CPU E5-2680, and
7
Under review as a conference paper at ICLR 2022
Table 2: Ablations on running times in different degrees for diagonal block approximation.
AVerage RUnning times (seconds) of each episode FSDDPG-DEG 1 FSDDPG-DEG 2 FSDDPG-DEG 4
U	SWimmer	∣	22.73	164.07	rs∩^5o ∏
(a) Ant.	(b) HalfCheetah.
(e) InvertedPendUlUm.
(c) HUmanoid.
(d) InvertedDoUblePendU-
lUm.
(f) Swimmer.	(g) Reacher.	(h) Walker.
FigUre 3: ResUlts for varioUs methods on MUJoCo.
implemented by PyTorch (Paszke et al., 2019). More details on the environments and the hyper-
parameters can be foUnd in Appendix G.
Baselines. We Uses DDPG (Lillicrap et al., 2016) TD3 (FUjimoto
et al., 2018), DDPG-TTSU, SDDPG-CG, STD3-CG (Wen et al.,
2021; Zheng et al., 2021a;b) as oUr baselines:
1)	DDPG is a deep learning implementations of DPG.
2)	TD3 is the DDPG method with twined delayed critics to avoid
high variance.
3)	DDPG-TTSU is similar to DDPG except that the critic is Up-
dated mUch more freqUently than actor. In oUr experiments, we
set the freqUency ratio as 10 : 1.
4)	SDDPG & STD3 are the Stackelberg learning of DDPG and
TD3, respectively.
Figure 2: Ablations on average
retUrns in different degrees of the
diagonal block approximation.
5)	SDDPG-CG & STD3-CG are the SDDPG and STD3 with conjugate gradient to approximate the
inverse Hessian, respectively.
Details on the hyper-parameters can be found in Appendix G.
Results. As shown in Fig. 3, FSDDPG (ours) reaches the best performance in Humanoid, while
FSTD3 (ours) reaches the best performance in 6 out of 8 environments (Ant, HalfCheetah, Inverted-
DoublePendulum, InvertedPendulum, Swimmer, and Walker). More specifically, comparing FSD-
DPG with DDPG, and FSTD3 with TD3, FSDDPG outperforms DDPG in 6 out of 8 environments
(Ant, HalfCheetah, Humanoid, InvertedDoublePendulum, Swimmer, and Walker); FSTD3 outper-
forms TD3 in all of the environments. Results indicate that our approximated Stackelberg scheme
does improve the performance.
For SDDPG-CG and STD3-CG, they do not perform well enough in many environments comparing
with ours due to the slow convergence rate. But they could perform better than DDPG and TD3 for
some environments that DDPG and TD3 are not converge well (e.g., Swimmer and InvertedDou-
blePendulum for DDPG).
For DDPG-TTSU, its convergence rate is comparable to FSDDPG and DDPG in all environments.
But notice that the critic needs to be updated for ten times in each episode, its actual convergence
rate is slower.
We conduct extra experiments in terms of the training times. We use CPUs to testify the training
time since precise GPU time per thread is not easy to estimate. As shown in Fig. 4, we find that
8
Under review as a conference paper at ICLR 2022
(a) Reacher.
(b) Ant.
(c) HalfCheetah.
Figure 4:	Average running time per episode of different methods. The enlarged training time per
episode of different methods is shown in Appendix H.
Table 3: Ablations of training time per episode on different Hessian approximation approaches.
Average training times (seconds) of each episode	FSDDPG (ours) FSDDPG-CG (ours) SDDPG-CG SDDPG-DB SDDPG
HalfCheetah	13.70	29.27	44.58	≥ 2000	≥ 2000
Inverted Double Pendulum	12Γ9	28.45	43.58	446	1780- -
Inverted Pendulum	15.25	30.96	43.69	339	1268	-
the training time of SDDPG is far longer than others. Also, DDPG-TTSU and SDDPG-CG are all
requires more training time than ours. This reveals that our methods are fast to compute.
Ablations. We firstly compare the performance of different Hessian approximation methods, in-
cluding FSDDPG-CG and SDDPG-BD (SDDPG with the diagonal block approximation) in Fig. 5.
Results show that the ours with CG perform better than ours with DB approximation in 2 out of 3 en-
vironments. However, as shown in Tab 3, ours with CG requires twice training times than ours with
DB approximation to reach that performance. Thus, we choose the diagonal block approximation
as our final method since it balances the tradeoff between times and performance well. Combining
Fig. 5 and 3, we find that ours with CG performs much better than SDDPG-CG. Thus, it reveals that
the TD-related terms do affect the convergence rate in complicated environments.
In terms of SDDPG-DB, from Tab. 3, we find that SDDPG-DB is still slow in both computing
and convergence comparing with FSDDPG, revealing that the block diagonal method may not suit
SDDPG well, which testify our claim in Sec. 4.3. Also, regarding the average returns, as shown
in Fig. 5, SDDPG outperforms SDDPG-DB. Together with the result of FSDDPG, we conclude
that the improvement of performance is mainly based on removing the TD-related terms rather than
using the diagonal block approximation.
We also testify how the change of degrees o affects the performance. As shown in Fig. 2 and Tab. 2,
the increasing of the degrees costs more training time but leads to better performance. Moreover,
we run the SDDPG (without approximation), and the results reveal that SDDPG is relatively slow.
Finally, we visualize the losses of critics to see whether the leaders are in best response areas, and
the results coincide with the results in Fig. 1. Details are in Appendix H.
(a) HalfCheetah.
(b) InvertedPendulum. (c) InvertedDoublePendulum.
Figure 5:	Ablations of average returns on different Hessian approximation approaches. SDDPG-CG
(ours) does not be conducted in HalfCheetah due to its long training time.
7 Conclusion
This paper aims to mitigate the challenges of high complexity and slow convergence rate in current
Stackelberg actor critic scheme. Specifically, we propose an approximated Stackelberg Determinis-
tic Policy Gradient that removes TD-related terms to improve convergence rate, and together with
the block diagonal approximation technique to further reduce the computing times. Experiments
show that ours outperform SOTAs in terms of average returns under acceptable training times.
9
Under review as a conference paper at ICLR 2022
References
Joshua Achiam. Spinning Up in Deep Reinforcement Learning. 2018.
Kai Arulkumaran, Marc Peter Deisenroth, Miles Brundage, and Anil Anthony Bharath. Deep rein-
forcement learning: A brief survey. IEEE Signal Processing Magazine, pp. 26-38, 2017.
David Balduzzi, Sebastien Racaniere, James Martens, Jakob Foerster, Karl Tuyls, and Thore Grae-
pel. The mechanics of n-player differentiable games. In ICML, pp. 354-363. PMLR, 2018.
Jalaj Bhandari, Daniel Russo, and Raghav Singal. A finite time analysis of temporal difference
learning with linear function approximation. In COLT, pp. 1691-1692. PMLR, 2018.
Aleksandar Botev, Hippolyt Ritter, and David Barber. Practical Gauss-Newton optimisation for deep
learning. In ICML, pp. 557-565. PMLR, 2017.
Stephen Boyd, Stephen P Boyd, and Lieven Vandenberghe. Convex optimization. Cambridge uni-
versity press, 2004.
Qi Cai, Zhuoran Yang, Jason D Lee, and Zhaoran Wang. Neural temporal-difference and q-learning
provably converge to global optima. arXiv preprint arXiv:1905.10027, 2019.
Benjamin Chasnov, Lillian Ratliff, Eric Mazumdar, and Samuel Burden. Convergence analysis of
gradient-based learning in continuous games. In UAI, pp. 935-944. PMLR, 2020.
Pei Chen. Hessian matrix vs. Gauss-Newton hessian matrix. SIAM Journal on Numerical Analysis,
pp. 1417-1435, 2011.
Gal Dalal, Gugan Thoppe, Balazs Szorenyi, and Shie Mannor. Finite sample analysis of two-
timescale stochastic approximation with applications to reinforcement learning. In COLT, pp.
1199-1233. PMLR, 2018.
Yan Duan, Xi Chen, Rein Houthooft, John Schulman, and Pieter Abbeel. Benchmarking deep
reinforcement learning for continuous control. In ICML, pp. 1329-1338. PMLR, 2016.
Tanner Fiez, Benjamin Chasnov, and Lillian Ratliff. Implicit learning dynamics in Stackelberg
games: Equilibria characterization, convergence analysis, and empirical study. In ICML, pp.
3133-3144. PMLR, 2020.
Zuyue Fu, Zhuoran Yang, and Zhaoran Wang. Single-timescale actor-critic provably finds globally
optimal policy. In ICLR, 2020.
Scott Fujimoto, Herke Hoof, and David Meger. Addressing function approximation error in actor-
critic methods. In ICML, pp. 1587-1596. PMLR, 2018.
Saeed Ghadimi and Mengdi Wang. Approximation methods for bilevel programming. arXiv preprint
arXiv:1802.02246, 2018.
Riccardo Grazzi, Luca Franceschi, Massimiliano Pontil, and Saverio Salzo. On the iteration com-
plexity of hypergradient computation. In ICML, 2020.
Tuomas Haarnoja, Aurick Zhou, Pieter Abbeel, and Sergey Levine. Soft actor-critic: Off-policy
maximum entropy deep reinforcement learning with a stochastic actor. In ICML, pp. 1861-1870.
PMLR, 2018.
Harold V Henderson and Shayle R Searle. On deriving the inverse of a sum of matrices. SIAM
Review, 23(1):53-60, 1981.
Kaiyi Ji, Junjie Yang, and Yingbin Liang. Bilevel optimization: Convergence analysis and enhanced
design. In ICML, 2021.
Chi Jin, Praneeth Netrapalli, and Michael Jordan. What is local optimality in nonconvex-nonconcave
minimax optimization? In ICML, pp. 4880-4889. PMLR, 2020.
Maxim Kaledin, Eric Moulines, Alexey Naumov, Vladislav Tadic, and Hoi-To Wai. Finite time
analysis of linear two-timescale stochastic approximation with Markovian noise. In COLT, 2020.
10
Under review as a conference paper at ICLR 2022
Jens Kober, J Andrew Bagnell, and Jan Peters. Reinforcement learning in robotics: A survey. IJRR,
pp.1238-1274, 2013.
Timothy P Lillicrap, Jonathan J Hunt, Alexander Pritzel, Nicolas Heess, Tom Erez, Yuval Tassa,
David Silver, and Daan Wierstra. Continuous control with deep reinforcement learning. In ICLR,
2016.
Bo Liu, Ji Liu, Mohammad Ghavamzadeh, Sridhar Mahadevan, and Marek Petrik. Finite-sample
analysis of proximal gradient td algorithms. arXiv preprint arXiv:2006.14364, 2020.
Jonathan Lorraine, Paul Vicol, and David Duvenaud. Optimizing millions of hyperparameters by
implicit differentiation. In AISTATS, pp. 1540-1552. PMLR, 2020.
James Martens. New insights and perspectives on the natural gradient method. JMLR, 21:1-76,
2020.
James Martens and Roger Grosse. Optimizing neural networks with Kronecker-factored approxi-
mate curvature. In ICML, pp. 2408-2417. PMLR, 2015.
Jose Mario Martinez. Practical quasi-newton methods for solving nonlinear systems. JCAM, pp.
97-121, 2000.
Eric Mazumdar, Lillian J Ratliff, and S Shankar Sastry. On gradient-based learning in continuous
games. SIAM Journal on Mathematics of Data Science, pp. 103-131, 2020.
Raman K Mehra. Computation of the inverse hessian matrix using conjugate gradient methods.
Proceedings of the IEEE, pp. 225-226, 1969.
Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Alex Graves, Ioannis Antonoglou, Daan Wier-
stra, and Martin Riedmiller. Playing atari with deep reinforcement learning. arXiv preprint
arXiv:1312.5602, 2013.
Ofir Nachum, Yinlam Chow, Bo Dai, and Lihong Li. Dualdice: Behavior-agnostic estimation of
discounted stationary distribution corrections. NeurIPS, pp. 2318-2328, 2019.
Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor
Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. Pytorch: An imperative style, high-
performance deep learning library. NeurIPS, pp. 8026-8037, 2019.
Aravind Rajeswaran, Chelsea Finn, Sham M Kakade, and Sergey Levine. Meta-learning with im-
plicit gradients. NeurIPS, pp. 113-124, 2019.
Lillian J Ratliff, Samuel A Burden, and S Shankar Sastry. On the characterization of local Nash
equilibria in continuous games. TAC, 61(8):2301-2307, 2016.
Sashank Reddi, Satyen Kale, and Sanjiv Kumar. On the convergence of adam and beyond. In ICLR,
2018.
Nicol N Schraudolph. Fast curvature matrix-vector products for second-order gradient descent.
Neural computation, pp. 1723-1738, 2002.
J Shewchuk. Conjugate gradient without the agonizing pain. Technical report, 2007.
David Silver, Guy Lever, Nicolas Heess, Thomas Degris, Daan Wierstra, and Martin Riedmiller.
Deterministic policy gradient algorithms. In ICML, pp. 387-395. PMLR, 2014.
Ankur Sinha, Pekka Malo, and Kalyanmoy Deb. A review on bilevel optimization: from classical
to evolutionary approaches and applications. TEC, pp. 276-295, 2017.
Richard S Sutton and Andrew G Barto. Reinforcement learning: An introduction. 2018.
Richard S Sutton, CSaba Szepesvari, Alborz Geramifard, and Michael P Bowling. Dyna-
style planning with linear function approximation and prioritized sweeping. arXiv preprint
arXiv:1206.3285, 2012.
11
Under review as a conference paper at ICLR 2022
Emanuel Todorov, Tom Erez, and Yuval Tassa. Mujoco: A physics engine for model-based control.
In ICLRS,pp. 5026-5033. IEEE, 2012.
Saran Tunyasuvunakool, Alistair Muldal, Yotam Doron, Siqi Liu, Steven Bohez, Josh Merel, Tom
Erez, Timothy Lillicrap, Nicolas Heess, and Yuval Tassa. dm_control: Software and tasks for
continuous control. Software Impacts, pp. 100022, 2020.
Hao-nan Wang, Ning Liu, Yi-yun Zhang, Da-wei Feng, Feng Huang, Dong-sheng Li, and Yi-ming
Zhang. Deep reinforcement learning: a survey. Frontiers of Information Technology & Electronic
Engineering, pp. 1-19, 2020.
Yuanhao Wang, Guodong Zhang, and Jimmy Ba. On solving minimax optimization locally: A
follow-the-ridge approach. In ICLR, 2019.
Junfeng Wen, Saurabh Kumar, Ramki Gummadi, and Dale Schuurmans. Characterizing the gap
between actor-critic and policy gradient. arXiv preprint arXiv:2106.06932, 2021.
Shimon Whiteson. Expected policy gradients for reinforcement learning. JMLR, 21, 2020.
Yue Wu, Weitong Zhang, Pan Xu, and Quanquan Gu. A finite time analysis of two time-scale actor
critic methods. arXiv preprint arXiv:2005.01350, 2020.
Peng Xu, Jiyan Yang, Farbod Roosta-Khorasani, Christopher Re, and Michael W Mahoney. SUb-
sampled newton methods with non-uniform sampling. In NeurIPS, pp. 3008-3016, 2016.
Zhuoran Yang, Yongxin Chen, Mingyi Hong, and Zhaoran Wang. On the global conver-
gence of actor-critic: A case for linear quadratic regulator with ergodic cost. arXiv preprint
arXiv:1907.06246, 2019.
Guojun Zhang, Kaiwen Wu, Pascal Poupart, and Yaoliang Yu. Newton-type methods for Minimax
optimization. arXiv preprint arXiv:2006.14592, 2020.
Liyuan Zheng, Tanner Fiez, Zane Alumbaugh, Benjamin Chasnov, and Lillian J Ratliff.
Stackelberg actor-critic: Game-theoretic reinforcement learning algorithms. arXiv preprint
arXiv:2109.12286, 2021a.
Liyuan Zheng, Tanner Fiez, Zane Alumbaugh, Benjamin Chasnov, and Lillian J Ratliff. Stackelberg
actor-critic: A game-theoretic perspective. In AAAI-RLG 2021, 2021b.
12
Under review as a conference paper at ICLR 2022
A Derivatives
This section we discuss the derivatives.
We firstly introduce a lemma that helps our analysis.
Lemma 1. Wehave V©Ep(s)[QWθ (s,∏θ(s))] = Ep(s)[v©∏(s) [V°QWθ (s,a)]α=∏θ(s)i ∙
Proof. By using the Leibniz integral rule to V©Ep(§)[QWθ (s, ∏θ(s))], We have:
VθEρ(s) [Qπwθ (s, πθ (s))] = Eρ(s) [Vθ Qπwθ (s, πθ (s))] = Eρ(s) hVθπθ(s) [VaQπwθ(s,a)]a=πθ(s)i ,
which achieves our desired result.	□
This lemma reveals that our off-policy gradient is different from the gradient derived in (Silver et al.,
2014). Remark 1 discusses why they are different, Lemma 2 discusses when these two methods have
the same results.
Now, we derive the derivatives using Lemma 1.
Proposition 1. For SDDPG and STD3, d Ldww, and d Lw2w) Can be further explicitly expressed
as:
∂2L(θ,w)
∂θ∂w
2Eρ(s) ](∂WQwθ(s,a)丽)(∂WQwθ(s,a))
+ (Qwθ (s, a) - Qπθ (s, a)) (JT7d-Qwθ (s, Q
∂θ ∂w
(8)
∂2L(θ, W)
∂w2
2dQWθ(S,a) dQWθ(S,a)T
∂w	∂w
+2(Qπwθ(s,a)-Qπθ(s,a))
d 2QWθ (S,a)
∂w2
(9)
Proof. Based on the basic technique of matrix derivatives and the integral rule of interchanging the
integration and differentiation, we have:
= 2Eρ(s)[ ( ∂W QW (S，a) ∂w )	( ∂W QW (S，a))
+ (Qwθ (S, a)— ⊥ (Qπθ (S, a))) (∂θ> ∂WQwθ (S, a))]
(10)
13
Under review as a conference paper at ICLR 2022
The last equation is due to the fact that ∂θ> QWθ (s, a)
With a little abuse of the notations, for the last equation, we use Qπθ (s, a) to replace ⊥ (Qπθ (s, a)).
□
Remark 1. Here, comparing with off-policy DPG, our derived policy gradient is different. In (Sil-
ver et al., 2014), under some assumptions (mentioned in Appendix A in (Silver et al., 2014)), the
(deterministic) policy gradient is derived from Qπθ (s, a) as
VθEρ(s)[Qπθ(S, πθ(S))] = Eρ(s)[Vθ f^∖π(a,∖s)Qπθ(S, a)]da]
=Eρ(s)[ JA Vθ π(a∖s)Qπθ (s,a)da + JA π(a∖s)Vθ Qπθ (s,a)da]	(11)
=Eρ(s)[vθ Πθ (s) [vaQπθ (s, a)]α=∏θ(s) + [V QKn (s,。―
The second equation is based on the integral rule of interchanging integration and differentiation,
and the chain rule. The last equation is based on the property of the Dirac-Delta function. Then
Qπθ (s, πθ (s)) is approximated by Qπwθ (s, πθ (s)) to make the Q value trackable.
For ours, Qπwθ (s, a) has been already introduced when the objective is formed. So, by the
integral rule of interchanging the integration and differentiation, Vθ Eρ(s) [Qπwθ (s, πθ (s))] =
Eρ(s) [Vθ Qπwθ (s, πθ(s))] = Eρ(s) Vθπθ(s) [Va Qπwθ (s, a)]a=πθ(s) . Therefore, our gradient is ex-
act and does not have extra term.
The reader may ask: why the two proofs have different results? The main reason is that in the
second equation of Eq. (11), in order to use the integral rule of interchanging the integration and
differentiation 7, Qπwθ (s, a) should be a continuous function w.r.t θ, ∀ a ∈ A. But according to our
definition, it is only differentiable w.r.t θ when a = π(s). Therefore, we can not use the technique as
DPG. The following we show that when Q satisfies some conditions, the gap can be bridged:
Lemma 2. If Q is always a continuous function w.r.t θ, ∀ a ∈ A, which w.l.o.g. is represented
as Q(s, a, θ). The derivatives of Eρ(s) [Q(s, πθ (s), θ)] derived by (Silver et al., 2014) and ours are
equal.
Proof. First of all, following the same steps in off-policy DPG, we have:
VθEρ(s)[Q(s, πθ(s), θ)] =
= Eρ(s) Vθ A π(a∖s)Q(s, a, θ)da
= Eρ(s)	A Vθπ(a∖s)Q(s, a, θ)da + A π(a∖s)VθQ(s, a, θ)da
(=a) Eρ(s) RA Vθπ(a∖s)Q(s, a, θ)da + Eρ(s)[VθQ(s, a, θ)]
(=b) Eρ(s) hVθπθ(s) [VaQ(s, a, θ)]a=πθ(s) + VθQ(s, a, θ)i
(a) follows the property of Dirac-Delta distribution. (b) follows the the last equation in Eq. (15) in
(Silver et al., 2014). The result can also be found in Corollary 3 in (Whiteson, 2020).
For ours, following the chain rule, we have:
VθEρ(s)[Q(s, πθ(s), θ)] = Eρ(s) hVθQ(s,a,θ) +Vθπθ(s) [VaQ(s, a, θ)]a=πθ(s)i .
Then, We can easily check the two derivatives are equal, which is our desired result.	□
Here, we show the reason that the stop-gradient operator is necessary in L(θ, w): because
∂∂θEp(s)[(QWθ(s, a) 一 Qπθ(s,∏θ(S)))2] may not be well-defined since ∂∂Qπθ(s,∏θ(S)) does not
exist in our settings.
Lemma 3. 另Qπθ (s,∏θ(s)) is not well-defined if r(s,∏θ(s)) and P (s0∖ s,∏θ(s)) are non-
differentiable.
7Details can be found in https://math.stackexchange.com/questions/2530213/
when-can-we-interchange-integration-and-differentiation
14
Under review as a conference paper at ICLR 2022
Proof. We can rewrite the state-action function under the expectation of rewards
∞
for different trajectories:	Qπθ(st,∏θ(St))	=	ET[£Ykrk∣τ] , where T :=
k=t
∞
p(st|st, πθ(st))	πθ(sk)p(sk+1 |sk, πθ(sk)) ∈ > is the probability of trajectories.
k=t+1
∞
dτ := dst	dak dsk
k=t+1
Then, we have:
Qπθ (st,∏θ (St))= ET [Pk=t γk rk |T ]
= RT Pk∞=1 γkrkdτ
=RTp(stlst,πθ(St))Q∞=t+ι πθ(Sk)p(sk+"sk,πθ(Sk)) (γtrt(s,πθ(St)) + P∞=t+1 Ykrk) dτ
=RTP(StISt, πθ(St))Q∞=t+ι πθ(ak|Sk)P(Sk+i|Sk, a)(Ytrt(S, πθ(St)) + P∞=t+1 γkrk) dτ
=∕tP(St|St, πθ(St))Q∞=t+ι πθIak |Sk)P(Sk+i|Sk, a) (γtrt(s, πθ(St)) + P∞=t+1 Ykrk)dτ
=Rt∖a p(St+ι∣St,∏θ(St)) Q∞=t+2 ∏θ(ak∣Sk)p(Sk+ι∣Sk, ak)
Aπθ(at+1 |St+1)P(St+2|St+1, at+1) Ytrt(S, πθ(St)) + Pk∞=t+1 Ykrk dat+1 d(τ \at+1)
-RT\A P(St+1 lSt,πθ(St))(Q∞=t+2 πθ(ak ISk)P(Sk+1 |Sk,ak))
(P(St+2|St+i,ne(st+1))) (Pk=t Ykrk(s, πθ(St)) + P∞=t+1 Ykrk) dτ∖at+ι
=Rt∖(∪∞tA) Q∞=t P(Sk+1|Sk,πθ(Sk) ) (P∞=t Ykrk(s,πθ(St))) dτ\ (∪∞=tat+1)
(a)	is based on the Fubini’s theorem.
(b)	is due to the property of Dirac-delta function.
∞
d(τ∖at+ι) means dstdst+ι( ɪɪ dakdsk).
k=t+2
T\A means removing the set A from T.
According to the assumptions, r(S, πθ(S)) andP (S0I S, πθ(S))are non-differentiable w.r.t θ.
Since Qπθ (St, ∏θ(St)) does not contain any differentiable elements, 条Qπθ (s, ∏θ(s)) are not Well-
defined.
□
This Lemma reveals that the ∂∂>Qπθ (s, ∏θ(s)) is not well-defined in many cases, since most of
the environments are natively non-differentiable w.r.t the parameters of policy, e.g., (Todorov et al.,
2012). That is another reason that we need to use the stop-gradient operator.
B Block Diagonal matrices
We firstly define E := k2diago(k2)-1 - I
We now show a technical lemma here:
15
Under review as a conference paper at ICLR 2022
Lemma 4. (Henderson & Searle, 1981) For two non-degenerate matrices A and B, we always
have:
(A + B)-1 = A-1 - A-1B(A + B)-1,
Now we show that larger o does not mean that the distance will be smaller:
Lemma 5. D(k2, diago(k2)) is not always monotonic decreased w.r.t. o:
Proof. This can be done by finding a counter-example:
-1 1 1 2-
1113
For k2 =	1 1 9 4 , we have D (k2,diag1(k2)) = 11.0, D (k2,diag2(k2)) = 8.2,
5561
D (k2,diag3(k2)) = 10.7.	□
But we can always find a sub-series that satisfies the following Lemma:
Lemma 6. When o can be represented as lx, where x, l ∈ Z+ , D k2 , diaglx (k2) is a strict
decreased series with the increasing of l.
Proof. ||k2 - diaglx(k2)||F = X |kij | , G = {i, j|i 6= lxj + k, k = 1, 2 . . . , lx},
i,j∈G
∣∣k2 - diag(l+1)x(k2)∣∣F = I X |kij| ,Gι = {i,j∣i = (l + 1)xj + k, k = 1, 2 ..., (l + 1)x}
i,j∈G1
Since G	⊂G1 , we have:
JX |kj I >11 X |kj I
i,j∈G	i,j∈G1
||k2 - diaglx (k2)||F - ||k2 - diag(l+1)x(k2)||F < 0.
Since||k2 - diagmn(k2)||F = 0. So D k2, diaglx(k2) is a strict decreased series with the
increasing of l.	□
Therefore, we conclude that we can select a series of o that are have the same common factor larger
than 1 to guarantee that the distance is D k2, diaglx(k2) is a strict decreased (e.g., 2, 4, 6, . . . ).
C Time Complexity for computing the gradient
We assume that critic has n layers with m neurons, while actor has n1 layers with m1 . We also
define t as the training episodes.
We define OPM as the time complexity of multiplication two real numbers, OPG as the time com-
plexity of calculating the gradient of one element w.r.t other, and OPA as the time complexity of
adding two real numbers.
Now we analyze the one step time complexity of different terms.
1)	The time complexity of 会QWθ (s, a) is
OPg (m + Pn+1 m2) + OPM (P"；m2)=OPG (m + nm2) + OPM (n +1)m2.
m + Pin+=12 m2 is because the the dimension of gradient between the output layer and the last layer
ism, and thus the dimension of gradient ism2 for each intermediate layer. Pin+=11 m2 is because
each layer requires m2 times multiplication to form the chain rules. 8
8Recall the time complexity of multiplication two matrices with dimensions m, n and n, p is mnp.
16
Under review as a conference paper at ICLR 2022
The time complexity of 翁 is ( OPG (mi + nιm2) + OPM (nι + 1)m1), which follows the same
analysis of 悬QW (s,α).
Combining the above analysis, the time complexity of
S ( OPG (m + nm2) + OPM (n + 1)m2), which follows the same analysis of
∂J (θ,w)
∂w
is
∂W QW (s,a).
2)	The time complexity of dJ∂θ,W) is
(OPG (m + nm2) + OPM (n + 1)m2) + (OPG + OPM)m
+ ( OPG (mi + nim2) + OPM (ni + 1)m2) + OPM(mn + mini)
, which is a combination of the time of calculating %QW (s, a)
(require( OPG	(m + nm2)	+ OPM	(n	+	1)m2)	times), 需 (require	(OPG	+	OPM)m
times), and 翁 (require ( OPG (mi + nim2) + OPM (ni + 1)m1)times)	(since
J⑸W) ~P ɪQπθ(c C) M 近)
∂θ 〜乙 ∂w Qw (S，a) ∂π ∂θ ).
3)	The time complexity of E也力户。)is exactly the same as ddw'.
4)	For the time complexity of 痣-QW (s, a) is
OPG (m + nm2) + OPM (n + 1)m2 + nmOPG (m + P '=2m2) + nmOPM (P "；m2)
=OPG (m + nm2) + OPM (n + 1)m2 + OPG (nm2 + n2m3) + OPM n(n + 1)m3,
which is based on the fact that the Hessian can be decomposed to 袅[∂WW~QW (s, a)].
5)	For ∂iθrr ∂W~QW (S)a),
nimi ( OPG (m + nm2) + OPM (n + 1)m2) + nimi(OPG + OPM)m
+nimi ( OPG (mi + nim2) + OPM (ni + 1)m2) + OPM(mn + mini)
6)	The time complexity of ^dWQW (s, a)翁) ^dWQW (s, a)j is the summation of
∂WQW (s, a) dW and ∂WQW (s, a), and their multiplication.
(OPg (m + nm2) + OPM (n + 1)m2) + (OPG + OPM)m
+ ( OPG (mi + nim2) + OPM (n + 1)m2) + OPM(mn + mini)
+ OPG m + nm2 + OPM (n + 1)m2 + OPM (mnmini)
= O (max {nm2,mnmini,nim2})
∂QW (s,a) ∂QW (s,a)τ
7)For -~τw-------∂W-
(OPG (m + nm2) + OPM (n + 1)m2) + OPM (m2n2)
Combining them together, for
EP(S)](寻Q (s,a)翳
the time complexity is:
(OPg (m + nm2) + OPM (n + 1)m2) + (OPG + OPM)m
+	OPG	mi +	nimi2	+ OPM	(ni +	1)m2i	+ OPM (mn +mini)
+ ( OPG (m + nm2) + OPM (n + 1)m2) + OPM(mnmini)
+nimi ( OPG (m + nm2) + OPM (n + 1)m2) + nimi(OPg + OPM)m
+nimi ( OPg (mi + nim2) + OPM (ni + 1)m2) + OPM(mn + mini)
二 O (max {nim3,niminm2})
17
Under review as a conference paper at ICLR 2022
For EP(S) [(2的》"' "%卜)> + 2 (qWθ (s, a) - Qπ(s, a))叫意(s，a))], the time complexity
is:
(OPG (m + nm2) + OPM (n + 1)m2) + OPM (m2n2)
`----------------------------------------------'
+ OPG (m +
、
=O (n2m3)
{Z
calculate k2
nm2) + OPM (n + 1)m2 + OPG (nm2 + n2m3) + OPM n(n + 1)m3,
一■――	,
calculate κ2
For inverse k2 + κ2, the time complexity is:
nm2) + OPM (n + 1)m2] + S [OPtMn2m2]
_________________ - /
"{^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
calculate k2
+ [OPfG	(m + nm2)	+ OPM	(n + 1)m2	+ OPG	(m2	+ nm3)	+ OPM	(n	+ 1)m3]
`------------------------------------------------------------------------------------------'
"Sz
+ O (n3m3)
'---{----}
inverse matrix
=O (n3m3)
calculate κ2
For inverting diag0(k2), the time complexity is:
[OF,g (m + nm2) + OPM (n + 1)m2] + S [OF,mn2m；] + OPGm2 + O (on m t) + O ( o (nm) t
^^~{^^^^^^^^^~
calculate part 0f k2
、-------■-------/
inverse matrix
O (max {nm2,nιm2, nOm- } t
extract the diag0(k2)
For inverting diag0(k2 + κ2)
[OPtG (m + nm2) + OPM (n + 1)m2] + S [OPtMn2m2] + OPGm2 +
।---------------------------------------------------------------‘
^^~{^^^^^^^^^~
calculate part of k2
+ O)ljG (m +
nm2) + OPM (n + 1)m2] + [OF,mn2m2] + OPGm + O (knkm t) + O ( k
^^~{^^^^^^^^^~
calculate part of κ2
2
1-------7-------}
inverse matrix
O (max {nm2,nιm2, n3fm3 0 t
extract the diag0(k2)
O^Pjg (m +
|
I
J
J
Therefore, the time complexity of calculating the actor for SDDPG in Eq. (4) is
/
∖
max
{nm2, n〔m2 } t + (mjn]m2n2) t + (m∖n∖mn)t + n3m3t + n^m"
----V-------Z 、------------7-----------Z 、------------Z
∖ ∂Q7^ζ(s,a and ∂唁(s,α)
^{^^^^^^^^≡
multiply matrices
get kɪ +κι
O (max {n3m3, nɪm2, m1n1m2n2} t)
O
/
The complexity of FSDDPG (ours) is
/
∖
O
1∖z
max {nm2, nɪmʊ	t	+ m2n2t	+ (om2n2)	t + max	{nm2, nɪm2, o3}	t
、-------------------/	、-------V--------}
∖∂qW (s,α)	. ∂Q∏wθ (s,α)	multiply matrices
∖ ∂θ and ∂W	，
/
O (max {n'm1, mɪnɪmn, nm2, m2n2o, o3} t)
18
Under review as a conference paper at ICLR 2022
The time complexity of DPG is:
/ ∖
nm2t, n1m21t
O
max
∖
_____ -	J
{^^^^∖∕^^^^^^^^^^
π
dQW(S,a)
∂θ	/
The time complexity of SDDPG-CG is:
/
∖
max {nm2,nιm2} t + (mιn∖mn) t +
|-------{--------'	|----{-----'
∖ ∂QW (s,a)	. ∂QW (s,a)	multiply matrices
∂ ∂θ and ∂W
2m2pt
{z
get inverse
O max n2 m2p, n1 m12 , m1 n1 mn t
J
CG
/
O
n
I
where p is the number of iterations of CG in each episode, which is O(1/), where is the error of
CG.
The time complexity of SDDPG-BD (SDDPG with block diagonal approximation) is:
nm 2
O( max {nm2, nιm2} t + (mn)t + kk × × nm) t + max
2 n3 m3
π	π |
dQW (S,a)	d dQwθ (S,a)
∂θ an5 dw
nm , n1m1, ――^
k2
+ O max n21m13, n1m1nm2	)
multiply matrices
, 1----------------{---------------Z
get block diagnal of k2 +κ2
2
t
|
J
'∙^^^^^^^^^^^^^^{^^^^^^^^^^^^^^^^
get k1 +κ1
O (max {n1m1nm2, nm2, nm", n1m3} t)
if m1 = m, n1 = n:
The time complexity of SDDPG is:
O max n3m3, n1m21, m1n1m2n2 t = O m3n3t
The time complexity of FSDDPG (ours) is:
O max n12m21 , m1n1mn, nm2 , mno2, o3 t = O max{m2n2, mno2 }t
The time complexity of DDPG is:
O max n12 m12 , nm2 t = O nm2t
The time complexity of SDDPG-CG is:
O (max {n2m2p, n1m2, m1n1mn} t) = O (m2n2-∣t)
The time complexity of SDDPG-DB is:
nkm3, nιm2, n1m1, n1m1nm2卜)=O (max {nmo2, n2m3} t)
D Convergence rate
This section we discuss the convergence rate.
We firstly show the relationship between SDDPG, FSDDPG, and Eq. (2):
Corollary 1. If <θ*, w*〉is the optimal solution for SDDPG. Then, <θ*,w*i is also the optimal
solutions for Eq. (2) as well as FSDDPG.
Proof. We firstly prove the relationship between SDDPG and Eq. (2). The global fixed point satis-
fies:
w* = arg min Es~ρ(∙), a~β [(QWθ (s,a) - Qπθ (s,a))2]
O
19
Under review as a conference paper at ICLR 2022
θ* = arg max Es〜p(.)[QWθ (s,a)]
Since the minimum of Es〜ρ(∙), a〜β[(QWθ (s, a) - Qπθ (s, a))2] is zero,
this implies ∀s, Qπwθ (s, a) = Qπθ (s, a).
Therefore, putting the above equation into the dynamic of Eq. (2), we have
w* - αw dL∂Ww*) = w* - αwEs 〜ρ(∙)[2 (Qwθ* (s, a) - Qπθ* (s, a)) ∂W QWθ∙* (S, a)] = w*
Therefore, w* is the fixed point. By further using the fact that
∀S, Qπwθ (S, a) = Qπθ (S, a) → ∀S, Qπwθ (S, π (S)) = Qπθ (S, π (S)) ,
we conclude that w* is the optimal solution for Eq. (2).
Also notice that the dynamic of θ is the same for SDDPG and Eq. (2). Thus, θ* is also the optimal
solution for Eq. (2).
Now, we prove the relationship between SDDPG and FSDDPG. Since we already knew that
∀s,QWθ(s,α) = Qπθ(s,α), We can quickly verify that kι(θ) = f ( ([f,w)[ and k2(θ)=
w	∂θ∂w
(∂2L(θ,w) ∖
V	∂w2	J
∂J(θ, w)	1	∂J (θ, w)
Thus, Jrw - kι(θ) k-1(θ) Jw
∂J(θ,w)	∂2L(θ,w) ∂2L(θ,w) 1 ∂J(θ, w)
∂θ	∂θ∂w	∂w2	∂w
Thus, the dynamics of SDDPG and FSDDPG is the same. So, it is also optimal solution for FSD-
DPG.
□
This lemma indicates that if We can find a solution of SDDPG, it is exactly the solution for the
standard Stackelberg learning Eq. (2), as Well as FSDDPG.
NoW, We focus on the convergence rate of FSDDPG and SDDPG:
Theorem 1. When π ∈ Cq (S ® Θ, R) and Q ∈ Cq (S 区 A 0 W, R), q ≥ 2. For a fixed
point θ* and w* such that JS (θ*)> + JS (θ*) is positive-definite, the our method with learning
rate α = √ψυ converges locally with a rate of O ((1 - ɪ) / } where U =》言也(S (θ*)),
ψ = λmax JS (θ*)> JS (θ*).
Proof. Our frameWork folloWs Theorem 5 in (Fiez et al., 2020). We firstly have
(I - γ1JS (θ*))> (I - γ1JS (θ*))
≤ (1 - 2γiλmin (S (θ*)) + γ2λmaχ (JST (θ* ) Js, (θ* ))) I ≤ (1 - υ∕ψ)I
Taking the Taylor series ofI - γ2ξ(θ) around θ*, We have
I - γ2ξ(θ) = (I-γ1ξ(θ))+(I-γ1JS(θ*))(θ-θ*)+R1(θ-θ*)
where Ri (θ - θ*) is the remainder term satisfying Ri (θ - θ*) = O (∣∣θ - θ*∣∣2) as θ → θ*.
This implies that there is δ such that Ri (θ - θ*) ≤ 君∣∣θ — θ*∣∣ whenever ∣∣θ — θ*∣∣ ≤ δ. Hence
I∣I - γ2ξ(θ) - (I - γ2ξ(θ*)) l∣2 ≤ (∣∣I - YiJs (θ*)l∣2 + 4ψ) I∣Θ — θ*l∣2
≤ ((1- U)i/2 + 4ψ)∣∣θ — θ*∣∣2.
Note that υ∕ψ ∈ [0, 1] since υ ≤ ψ, in fact,
U = λmm (S (θ*)) ≤ λmaχ (S (θ*)) ≤ λmaχ(J> (θ*) JS (θ* )) = ψ
Thus, for any θo ∈ {θ∣∣∣θ - θ* || < δ}
20
Under review as a conference paper at ICLR 2022
T/2
kθT - θ*k2 ≤ (1 - 2ψ)	kθ0 - θ*k2
so the local rate of convergence is O
□
Before we get deeper into the analysis, we define
d-j∂θ,w) - kι(θ) k-1(θ) d-j∂Ww)
T ∂L(θ,w)
∂w
JS (θ):
∂	∂-J(θ,w)	∂2L(θ,w) ∂2L(θ,w) -1 ∂-J(θ,w)
∂θ [ ∂θ	∂θ∂w ∂w2	∂w
丁 ∂2L(θ,w)
/ ∂θ∂w
∂	∂-J(θ,w)	∂2L(θ,w) ∂2L(θ,w) -1 ∂-J(θ,w)
∂w [ ∂θ	∂θ∂w ∂w2	∂w
,∂2L(θ,w)
∂w2
S(θ*) := (JS (θ*) + Js (θ*))
which decides the dynamic and the Jacobian of SDDPG.
Now, we show that the convergence rate of SDDPG:
Proposition 2. When π ∈ Cq(S 0 Θ, R) and Q ∈ Cq(S 0 A 0 W, R), q ≥ 2. For a fixed
point θ* and w* such that JS (θ*)τ + JS (θ*) is positive-definite, the our method with learning
rate α = √^^- converges locally with a rate of O ((1 一 2)/ ), where U =》言也(S (θ*)),
ψ = λmaχ (JS (θ* )> Js (θ*)).
Proof. Following the same steps in Theorem 1, we can easily check that
(I 一 Y1 Js (θ*))τ(I 一 Y1 Js (θ*)) ≤ (1 一 U∕Ψ)I, and
||I 一 Y2ξ⑹一 (I 一 Y2ξ(θ*)) ||2 ≤ ((1 一 ψ)	+ 4U^) llθ - θ*ll2,
and
V ≤ ψ.
Thus, for any θo ∈ {θ∣∣∣θ - θ* || < δ}
T/2
kθτ 一 θ*k2 ≤ 1 -击 1优一 θ*k2
2ψ
So the local rate of convergence is O( (1 一 W) ).
□
Now, we show that FSDDPG is faster or equivalent to SDDPG for linear actor critic.
We firstly define linear actor as ∏ = 夕(s)θ, where 夕(S) is the embedding, θ is the learnable
parameters.
The linear critic is Q(s, a)
夕(S)
夕(S)θ
w , where w is the learnable parameters for critic.
Theorem 2. For linear actor and critic, if
w|A
夕(S)
Ψ(s)θ
< 0, where A is any Positive Semi
Definite (PSD) matrix. Then, the convergence rate of FSDDPG (ours) is faster than or equivalent to
that of SDDPG.
21
Under review as a conference paper at ICLR 2022
Proof. 9 10 We show the deriviatives for different terms:
∂θ∂w ) = 2EP(S)"(菰Qwθ(S,a)而)(∂Wqw,θ(S,a)) + (Qwθ(s,a) -Qπθ(s,a)) (∂θ> ∂Wqw,θ(S,a)
T
EP(S)	［3(S)IT
驾WwI = EP(S)
(s,a) — Qπθ(S,a))
3(S)
3(S)θ
EP(S)岛 2 QWθ (s,a) — Q"θ (s,
3(S)
3(S)θ
3(S)
3(S)θ
w — Qπθ (s, a)
0
3(S)IT
Eρ(s)［磊［2 俗。(s, a) - Qπθ (s, α))靠QW G 皿,
EP(S)岛 2 ( 3((SS))θ w - Q"。(s,α)
3(S)
3(S)θ
IAs)
IAs)
φ(s)θ	夕(s)θ
T) I.
Now, we put these derivatives into the JS (θ):
Js (θ):
∂
∂θ
∂-J (θ,w)
—kιk-
1 ∂-J (θ,w)
∂w
∂2L(θ,w)
∂θ∂w
∂-	Ja(ZW) — k1k-
∂w ∖ ∂θ	1 2
∂2L(θ,w)
∂w2
1 ∂-J(θ,w)
∂w
硕
+
A B
C D
where
Y
w
A = EP(S)
+Eρ(s)
0
3(S)IT
0
3(S)IT
0
3(S)IT
夕(S)
0
3(S)IT
3(S)
3(S)θ
3(S)
3(S)θ
3(S)θ	3(S)IT
C = EP(S) ( 13(S)IT
3(S)
中(S)θ
3(S)
3(S)θ
3(S)
3(S)θ
k2 IEP(S) L(s)1t
k-1Eρ(s)
3(S)
3(S)θ
中⑹
中(S)θ
3(S)
中(S)θ
T) I.
3(S)
3(S)θ	,
W — Qπθ (s, a)
0
3(S)IT
Y
w
Y
w
T
0
+
9With a little abuse of notations, we use θ as θ* and W as w* throughout the proof of this theorem.
10A < B means A — B is PSD.
22
Under review as a conference paper at ICLR 2022
Similarly, for JS(θ), we have:
ʌ
JS(θ)：
∂ / ∂-J(θ,w)
∂θ ∖ ∂θ
—
∂2L(θ,w) ∂2L(θ,w) -1 ∂-J(θ,w)
∂w∂θ ∂w2
∂2L(θ,w)
∂θ∂w
∂w
∂ / ∂-J(θ,w)
∂w ∖ ∂θ
—
∂2L(θ,w) ∂2 L(θ,w) -1 ∂-J (θ,w)
∂w∂θ ∂w2
∂2L(θ,w)
∂w2
∂w
EF
CD
where
T
+Eρ(s)
+EP(S)(
=A + G,
0
夕(S)IT
0
夕(S)I
0
夕(S)I
and G = EP(S)
F = EP(S)
Wt
夕(S)
WY
0
夕(S)IT
中⑹
中⑹θ
IAS)
中(S)θ
0
夕(S)IT
夕(S)θ	I(S)IT
0
夕(S)IT
W L(S)it k2 IEP(S)
「夕(S)
[[夕(S)θ
夕(S)IT
∣(S)ITl k2 IEP(S)
夕(S)]]
夕(S)θ ,
ECa I(S)U I(S)
P(S) HI(S)θ∣ ∣∣(s)θ
夕(S)
夕(S)θ
夕(S)
夕(S)θ
W
+
T
T
丫
W
0
T
Notice that, we can easily check that:
B = F = G.
Thus,
T
+
S (θ*)
ʌ , .
S(θ平)
1	A-E B-F
2	0	0
A-E B-F
00
—
G
1
2
+ (G)T 0
00
Since according to the assumption, we have G to be Positive Semi Definite (PSD). Therefore,
C / 八士、	A / Λ⅛ ∖ . /ʌ *	1 ∙	. .1	.	i' 1 .'	. ∙	1
S (θ*) - S (θ*) < 0. According to the property of definite matrices, we have
λ3n (S (θ*)) ≥ λrM S(θ*).
For Js (θ*)τ Js (θ*) , we have:
B
D
Js (θ*)T Js (θ*)
AT
BT
D TA
ATA + CTC
ABT + DTC
BTA + CTD
BTB + DTD
and
Js (θ*)T Js (θ*)
Et
FT
(T
EtE + CT C
EFT + Dt C
F tE + C tD
F τF + DtD
C
D
T
E
C
F
D
Therefore,
Js (θ*)T Js (θ*)
Js (θ*)T Js (θ*)
EtE - AtA FtE - BtA
EFT - ABt FtF - BtB
—
ETE - AT A FtE - BTA
EFT - ABt
0
Also notice that EtE - AtA = (A + G)t(A + G) - AtA = (At + Gt) (A + G) - AtA
Gt A + AtG + GtG,
23
Under review as a conference paper at ICLR 2022
Based on the property of PSD, we can know that G1G and G1A + AτG are also PSD. Thus,
ETE - AτA is also PSD.
Moreover, we can check that
EFT - ABτ = GBτ, and
0
以 S)IT
w ^(s)1τ k2 1Eρ(s)
a
< 0,
中⑹
中(S)θ
W L(S)IT k-1EP(s)
(XS)
中(S)θ
Where a is due to the property of PSD .
Thus, we have
Js (θ*)τ JS (θ*) - Js (θ*)T JS (θ*) < 0,
Therefore, λmax (JS (θ*)τ JS (θ* )) ≥ λmax (JS (θ*)> JS (θ*)) ∙
(	gin (S®))
Thus, 1---------7——------------ʌ-
∖	2λmax (JS (θ*)T Js (θ*))
achieves our desired result.
T/2
≥ ∣1 -	%n (S (θ*))
一∖	2λmax (JS (θ*)T JS (θ*)
T/2
, which
□
Here is an example that helps US to show the convergence rate, and we use toy example 1 as an
example:
Recall that in toy example 1, we have L(θ, w) = (wθ2 + θ2)2, J(θ, w) = wθ2, τ = 1.
And we can easily calculate that:
∂J(θ,w)	∂2L(θ,w) ∂2L(θ,w) 1 ∂- J(θ,w)
∂θ	∂θ∂w ∂w2	∂w
3w + 1
-w + [4(2w + 2)θ3] [2θ4]-1 θ
Thus, we have:
ʌ , .
Js(θ)：
∂ ( ∂-J (θ,w) _ ∂2L(θ,w) ∂2L(θ,w) -1 ∂-J (θ,w)
∂w I ∂θ	∂θ∂w ∂w2	∂w
∂2L(θ,w)
∂
∂θ
∂-J (θ,w)
∂θ
p L	λ	∂θ∂w
∂w(3w + 1)	∂θ(3w +1) ] = ∣^	1
2(wθ2 + θ2)2θ	4θ3	=	2(wθ2 + θ2)2θ
0
4θ3
∂2L(θ,w) ∂2L(θ,w)-1 ∂-J(θ,w)
∂θ∂w	∂w2	∂w
∂2 L(θ,w)
∂w2
putting θ* = 0, w* = -1 into the equation,
ʌ .
Js (θ*)
1
0
0
0
Finally,
0.5 (Js(θ*)T + Js(θ*)) =0.5( 0 0	+
10	10
Js (θ*)τ Js (θ*) =	0 0	×	0 0	=
10
00
10
00
10
00
Thus λmin = 0, λmax
1 , the convergence rate i
O (IT/2 ).
Similarly, we have:
24
Under review as a conference paper at ICLR 2022
-J∂θ,w) - kιk-1 d-JWw) = -W + θw (θ2)-1 θ
Thus,
JS(θ) :
∂W ((θ2++λ) W
∂2L(θ,w)'
∂θ∂w
2λθ
(θ2 + λ)	(θ2 + λ)2
2(Wθ2 + θ2)2θ	4θ3
∂ ((θ2⅛λ) W
∂2L(θ,w)'
∂w2
W
putting θ* = 0, w* = -1 into the equation,
JS (θ*)
-1 0
00
-1 0
00
Finally
0.5 JS(θ*)> + JS(θ*)
-1 0
00
10
JS(θ*)>JS(θ*) =	-01 00	×
-1 0
00
1
0
0
0
Thus λ2min = 1, λmax = 1 O
O 0.5T/2
E Trajectories of SDDPG and FSDDPG.
This section we show the distances of trajectories of SDDPG and FSDDPG are bounded. We firstly
introduce an assumption:
Assumption 1. Qw (s, a) is a linear function.
This assumption is also adopted in the analysis of temporal difference (Bhandari et al., 2018). In
fact, the assumption only serves as foundations of the proofs ine (Bhandari et al., 2018), and it is
possible to extend this assumption into the neural networks form (Cai et al., 2019).
Some mild assumptions are given:
Assumption 2. The following Hessian matrices and the Jacobian matrices are bounded.
||∂∏∂wsrIl ≤ Hl, Il奈∏θ(a I s)ll ≤ H2, II疝QWθ(s,α)∣∣ ≤ H3, ||dQ¾H|| ≤ H
ll ∂ ((∂W Qwθ (S, a)) (∂W QwW (S, a)>)	) ii ≤ H5,
ii∂W ((∂WQw (S，a)) (∂WQwθ (S，a)>)	) ii≤ H6
ii ∂θ> ∂W Qwθ (S, a)I1 ≤ h7
ii dQ∂wsa ii≤ H
This assumption is standard assumption that the norm of derivatives is bounded, similar to (Ghadimi
& Wang, 2018; Reddi et al., 2018). We set H = max{H1, H2,..., %}, and H =
min{H1 , H2 , . . . , H8}.
For compactness, we define kι :=("患；W)),and k2 :=(班黑W)).
We also define κ1 := k1 - k1, and κ2 := k2 - k2 as the residual terms. We also define :=
IIQπwθ (S, a) - Qπθ II.
We also denote the minimum eigenvalue of E 产嗫%"，dQw¾wa>] as λmin :=
∂q Γ∂Qwθ (S,a) ∂Qwθ (S,a)>B
λmin I E 7；	7；	I .
∂W ∂W
25
Under review as a conference paper at ICLR 2022
Lemma 7. we have ∣∣κι∣∣ ≤ 2eHr and ∣∣κ2∣∣ ≤ 2eHβ.
Proof. Based on the definition of κ1and κ2 and the property of norm, we have:
∣∣κ1∣∣ = ∣∣Es 〜ρ,α 〜∏θ (∙∣s) 2 (Qw (s,a)- Qπθ(s, a)) (∂θ> lWQuθ (s,a)) ] ∣∣
≤ 2EH7
∣∣K2∣∣ = ∣∣Esy∏θ(∙∣s) 2 (qWθ (s,a) - Qπθ (s,a)) ^]” ∣∣
≤ 2eH3
□
Lemma 8. For the i.i.d sampling schemes, we have E ≤ H , where H is a positive COnStant
Proof. The proof directly follows Sec. 6.1 in (Bhandari et al., 2018).
□
The i.i.d sampling scheme assumptions are also used in (Nachum et al., 2019; Sutton et al., 2012;
Liu et al., 2020; Dalal et al., 2018).
Lemma 9. If E ≤ JJlHnHI, We always have ∣∣κ1 k2 — k1κ2∣∣ ≤ 2 (H8H10H7 + H1H2H3)e,
∣∣(k2 + κ2)k2∣∣≤ 2H3H8E.
Proof. For ∣∣κ1diαgo(k2) — k1κ2∣∣, we have:
∣∣κ1k2 — k1κ21∣
=∣∣((qWθGa)- QπθGa))(各 装QWθGa)))	dQ¾T)T
—新 ∂wQw (s,a) ((QWθ (s,a) — Qπθ (s,a)) dQ⅛^ 也噬Ia)T) ∣∣
≤ ∣∣dQ⅜al dQ¾T)T ∣∣ ∣∣ (Qw (s,a) — Qπθ (s,a))(备选QW (s, a)) ∣∣
+2∣∣ (京 Qw (s,a)翳)>(袅 QWθ Ga)) ∣∣ ∣∣ (qWθ (s, a) — Qn (s,a))/ C
≤ 2H8H7E + 2H1H2H3E = 2 (H8H7 + H1H2H3)E
For ∣∣[κ2 + k2]-1∣∣,we have:
∣∣[κ2 + k2]T∣∣ =) ∣∣κ2-1 — κ-1k2[k2 + κ2]-1∣∣
≤ 2eH3 + 2EH32EH3∣∣[k2 + κ2]-l∣∣
After some simplification, we have:
∣∣[k2 +κ2]T∣∣ ≤ G1
G2
(a)	follows Lemma 4, where Gi = 2eH3, G2 = 1 — 2eH32eH3.
Therefore, putting the above inequality together, we have:
∣∣[(k2 +κ2)k2]T∣∣ ≤∣∣[k2 + K2]T∣∣∣∣k2T∣∣
≤ k∣M1∣∣
G2
≤ 一G1一 < 2H3E.
—G2λmin —	3
(b)	is based on the assumption E ≤ JλmH∣^1, which induces
1 — 2eH32eH3 ≤》minE.
□
26
Under review as a conference paper at ICLR 2022
Theorem 3. ||Z (θ) - Z(θ)∣∣ is bounded by 5H10H3Hs(HsH10H7 + H1H2H3)H 片.
Proof. Using the lemmas mentioned above, we have:
∣∣ζ(θ) - Z(θ)∣∣ = ∣∣ [(" - k)$,0] ∣∣
≤∣∣ (fe+⅛ -符)∣∣∣∣ 小 ∣∣
=∣∣(κιk2- k1κ2)(k2+κ2)k-1∣∣∣∣ dJ∂WW) ∣∣
≤∣∣((k2) + κ2)(k2∣∣∣∣(((k2 + κ2)k-1∣∣∣∣ 吗辔 ∣∣
≤ 5H10H3(H10H7 + H1H2H3)e2
≤ 5H10H3(H10H7 + H1H2H3)H 片
□
Where the third inequality is based on Lemma 7 and 9, and the last inequality is based on Lemma 8.
F Almost surely avoid saddle points.
The proof relies on Theorem 4 in (Fiez et al., 2020), as well as Corollary 23 in (Mazumdar et al.,
2020).
Firstly recall that the definitions of critical points, strict saddle points, as well as fixed points (for
our methods) in (Fiez et al., 2020; Chasnov et al., 2020; Ratliff et al., 2016):
Definition 1. A point is a critical point if ζ(θ) = 0.
Definition 2. A point is a strict saddle point if ζ(θ) = 0,and its some eigenvalues are less than 0,
while others are larger than 0.
Definition 3. A point is a locally asymptotically stable equilibrium if ζ(θ) = 0,and all its eigenval-
ues are larger than 0.
Lemma 10. ∣∣Z(θι) — ζ(θ2)∣∣ isLipschitz continuous.
Proof. Firstly we observe the following inequalities:
CCne
∣∣岛dL∂WP11∣ ≤ ∣∣∂W1 (QWI(S,α) - Qn(s,α))dQwWFa)∣∣
-	∏e	^"∣
=∣∣ ∂W1 J(qW；1 (s,a)- Qπ(s,α)) %1Wr,1 ∣∣
=∣∣ d2Q£2(S，a)(qW11 (s,a) - Qπ(s,a)) + 叼：Fa) ^WFa) ∣∣
≤ 2H3Rmaχ + H2
∣∣磊(Jd^ - kι(θ) k-1(θ) Jw) ∣∣
πθ
∣∣d∂θ∂W) ∣∣ ≤ ∣∣ 焉(QWT (s,a) - Qn(s,a))	∣∣
cKe、πe
=∣∣ (舄QWTGa))也箸Wya) + (QW11 (s, a) - Qn(s,a))舄∣∣
≤ H1H3H2 + 2g3RmaxH4
27
Under review as a conference paper at ICLR 2022
I岛(Jwi - kι(θ) k-1(θ) Jwi
VHd	(∂QW (s,a)
≤ || ∂W1 ∂θ
)-∂ww{	(∂WQwθ (S, a))	(∂WQwθ (S, a需)]
L	-1	-TT	^j
舟Qwθ (S,a) 舟Qwθ (s, a)>)	dQWdW(S，a)川
Il ∂ ( ∂QW (s,a)
|| ∂W1 ∂θ
H4 + H1H2H4H6
||
Therefore, based on the fact that for any continuous function f if SuP ∣∣Vχf (x)∣∣ ≤ L, then f (x) is
L-lipschitz continuous, we have:
∣∣w(θι) - w(θ2)∣∣
=||	"(Jd⅛wi	- kι(θι)	k-1(θι)	dJdWWi)-(J⅛wi	— k1(θ2)	k-l(θ2)	d¾w2i)#	l,
=||	∂L(θ1,w1)	∂L(θ2,w2)	||
-	∂wι	∂W2	-
≤ TII dL(θ1,wl) - dL(θ2,w2) ∣∣
一 “	∂wι	∂w2 H
+ II (J⅛wl - k1(θ1) k-1(θι) JWwW1) - (J⅛wl - k1(θ2) k-1(θ2)
≤ max{	H4	+	H1H2H4H6,	H3 + H3H4	+	H1H2H4H5	}IIθ1	-θ2II
+ max n 2HRmax + H2, H1H3H2 + 笔-y O IIθι - θ2II
≤ 2 max{ H4 + H1H2H4H6, H3 + H3H4 + H1H2H4 H5,
2H3Rmax + H2, H1H3H + 2H3Rmax }IIθι- θ2II ≤ LIIθι- θ2II,
∂J (θ2,W2)
∂W2
where L := 2 max{ H4 + H1H2H4H6, H3 + H3H4 + H1H2H4H5,
2H3Rmax + h4, H1H3H4 + 2HiRmax}.
Now, we can finally prove our claim, the framework follows Theorem 4 in (Fiez et al., 2020):
Theorem 4. If aL < 1, our method converges to the strict saddle points of both Z (θ) and zeta(θ)
on a set of measure zero.
Proof. We firstly prove that g(θ) is invertable, which can be done by contradictions. Consider
θ1 6= θ2, and suppose g(θ1) = g(θ2) so that θ1 - θ2 = αw(θ1) - αw(θ2) . According to Lemma
10, αIIw(θ1) - w(θ2)II ≤ αLIIθ1 - θ2II < IIθ1 - θ2 II , giving a contradiction. Therefore, it is
invertible.
∂g(θ)
Now, observe that C 八
∂θ
I - αJS(θ), and if
∂g(θ)
∂ θ
is invertible, then according to the inverse
function theorem, we haveg(θ) is local diffeomorphism. Hence, it suffices to show that I - αJS (θ)
does not have an eigenvalue of 0 (or @吧)does not have an eigenvalue of 1). Indeed, letting P(A)
∂θ
be the spectral radius of a matrix A, we know that ρ(A) ≤ IIAII for any square matrix A and induced
operator norm II」so that ρ(αJs(x)) ≤ ∣∣αJs(x)k ≤ αSuP ∣∣ Js(θ)k < αL < 1,
θ
>
□
The above implies that all eigenvalues of αJS (θ) have absolute value less than 1. Since g(θ) is
injective by the preceding argument, its inverse is well-defined . Also, since g(θ) is a local diffeo-
morphism on Θ Σ, it follows that g(θ) is smooth on Θ Σ. Thus, g(θ) is a diffeomorphism.
Consider all critical points of θ, given by ker(w). For each u ∈ ker(w), let Bu, where u indexes
the point, be the open ball derived from the center manifold theorem, and let B = ∪uBu . Since
Θ ㊂ Σ ⊆ Rm, Lindelof,s lemma indicates that every open cover has a countable sub-cover. That
is, for a countable set of critical points {ui}i∞=1 with ui ∈ ker(w), we have that B = ∪i∞=1 Bui.
Starting from some point θ0 ∈ Θ	Σ, if our methods converges to a strict saddle point,
28
Under review as a conference paper at ICLR 2022
then there exists a t0 and index such that gt(θ) ∈Bui. Applying the center manifold theorem and
using that g(θ) ⊆ Θ O Σ, we get that g(θ0) ∈ Wlcosc ∩ Θ O Σ.
Using the fact that g(θ) is invertible, we can iteratively construct the sequence of sets defined by
W1(ui) = g-1 Wlcosc ∩ Θ OΣ and Wk+1 (ui) = g-1(Wk(ui) ∩ Θ OΣ). Then we have that
θ0 ∈ Wt(ui) for all t ≥ t0. The set Θ0 O Σ0 = ∪i∞=1 ∪t∞=0 Wt(ui) contains all the initial points
in X such that our method converges to a strict saddle.
Since ui is a strict saddle, I - αJS (θ) has an eigenvalue greater than 1. This implies that the co-
dimension of the unstable manifold is strictly less than m—i.e., dim (Wlcosc) < m . Hence, Wlcosc ∩X
has Lebesgue measure zero.
Using again that g(θ) is a diffeomorphism, g-1 ∈ C1 so that it is locally Lipschitz and locally
Lipschitz maps are null set preserving. Hence, WK (ui) has measure zero for all k by induction so
that Θ0 Σ0 = ∪i∞=1 ∪t∞=0 Wt(ui) is a measure zero set since it is a countable union of measure
zero sets.
□
G	Environments setup
We list some important statstics about the environments as shown in Table 6 and the detailed dy-
namics of each environments can be found in (Duan et al., 2016; Tunyasuvunakool et al., 2020).
Moreover, we also list hyper-parameters of our methods and baselines as shown in Tables 4 and 5.
Here, we would like to discuss more details on why SDDPG-BD is slow comparing with FSDDPG-
BD. Theoretically speaking, since it needs to compute the partial of the Hessian d Qwwθs,a), its
convergence rate is slow than ours as mentioned in Sec. 4.3.
But the main reason makes SDDPG-BD slow is that the current deep learning framework (e.g.,
PyTorch (PaSzke et al., 2019)) is hard to implement diago(d Lwθ2w)) in parallel through GPU. That
is, we need to calculate the elements of Hessian matrices on diagonal block matrix. However, the
torch.autograd.grad11 function does not support calculating the gradient of a series of gradients in
parallel, meaning that we need to loop them one by one. This makes the training process slow.
The RL platform is based on OpenAI’s Spinning UP (Achiam, 2018). The codes for DDPG and
TD3 are from https://spinningup.openai.com/.
H	Extra Experiments
We conduct the full version of SDDPG on two of the simplest environments in MuJoCo: Inverted-
DoublePendulum, as well as InvertedPendulum. Results (Fig. 6) reveal that the SDDPG converges
slowly than ours, indicating that the removing the TD-related terms do help improve the convergence
rate of Stackelberg learning.
We also show toy example on the best response landscapes. We find that SDDPG can always keep
in the best response region.
As shown in Fig. 8, we can find that SDDPG-CG has smaller losses for the critic, meaning that it
does keep w in the best response area. For DDPG, the losses of the Q values increase dramatically,
revealing that it may drifts far form the best response area. For ours, the loss of FSDDPG is between
SDDPG-CG and DDPG. This coincide with the results in Fig. 1, which also supports the assumption
that the TD-related term pulls the agent to remain in best repsonse area in Sec. 4.
11https://pytorch.org/docs/stable/generated/torch.autograd.grad.html
29
Under review as a conference paper at ICLR 2022
Table 4: The hyper-parameters of SDDPG (ours), SDDPG, and DDPG.
	SDDPG (ours)	SDDPG	DDPG
Critic Learning Rate	10-3	10-3	10-3
Actor Learning Rate	10-3	10-3	10-3
Target Update Rate	5 × 10-3	5 × 10-3	5 X 10-3
Batch Size	100	100	100
Discount Factor	099	0.99	0.99
Exploration Policy	N(0, 0.1)	N(0, 0.1)	N(0, 0.1)
Network Sizes for Actor	16 × 16	16 × 16	16 X 16 -
Other Useful Parameters	block diagonal rate =1	-	-
Table 5: The hyper-parameters of STD3 (ours), TD3, and SDDPG-CG, FSDDPG-CG.
	FSTD3(ours)	TD3	SDDPG-CG	FSDDPG-CG
Critic Learning Rate	10-3	10-3	10-3	10-3
Actor Learning Rate	10-3	10-3	10-3	10-3
Target Update Rate	5 X 10-3	5 X 10-3	5 X 10-3	5 X 10-3
Batch Size	100	100	100	100
Discount Factor	0.99	0.99	0.99	099
Exploration Policy	N(0, 0.1)	N(0, 0.1)	N(0, 0.1)	N(0, 0.1)
Network Sizes for Actor	16 X 16	16 X 16	16 X 16	16 X 16
Other useful Hyper-Parameters	block diagonal rate =1	-	CG iterations=5	CG iterations=5
Table 6: Some statistics about the environments.
Environments	State Dimension	Action Dimension
Ant	125	8
HalfCheetah	20	6
Humanoid	102	10
InvertedDoublePendulum	6	1
InvertedPendulum	3	1
Swimmer	13	2
Reacher	6	2
Walker	24	6
(a) InvertedDoublePendulum.
(b) InvertedPendulum.
Figure 6: Ablations on different Hessian approximation approaches and SDDPG approaches.
30
Under review as a conference paper at ICLR 2022
Figure 7: The trajectories of θ and w for toy example 1 and toy example 2 (The contours are for
J(θ, w*) value).
(a) InvertedDoublePendulum.
(b) InvertedPendulum.
Figure 8: Ablations on losses of the critics for different methods.
(d) Reacher.
(e) Ant.
(f) HalfCheetah.
Figure 9: Average running time per episode of different methods. The first row (a-c) is the training
time per episode of different methods without SDDPG. The second row (d-f) is the training time per
episode of all the methods.
31
Under review as a conference paper at ICLR 2022
I Some important Source Codes
To make our reader know more about our algorithm. We put some important part of the source
Codes here. One can implement our FSDDPG by just putting the following code to replace the cor-
responding functions in https://github.Com/openai/spinningup/blob/master/
spinup/algos/pytorCh/ddpg/ddpg.py.
def compute_loss_q(data):
"""Calculates the loss for the critic"""
o, a, r, o2, d = data['obs'].to(device), data[,act'].to(device),
data[' rew'] .to(device), \
data['obs2'] .to(device), data[, done'] .to(device)
q = ac.q(o,a)
#	Bellman backup for Q function
with torch.no_grad(): #stop-gradient operator for semi-gradient
technique.
q_pi targ = ac_targ.q(o2, ac_targ.pi(o2))
backup = r + gamma * (1 - d) * q_pi targ
#	MSE loss against Bellman backup
loss_q = ((q - backup)**2).mean ()
#	Useful info for logging
loss_info = diet (QVals=q.detach().cpu().numpy())
return loss_q, loss_info
def Calculate_s_actor_loss(data):
"""Calculates the Stackelberg gradient for the actor"""
states = data['obs'].to(device)
actions_pred = ac.pi (states)
q = -ac.q(states, actions_pred).mean()
dq_dws = autograd.grad(outputs=q, inputs=ac.q.parameters(),
Create_graph=True)
dq_dthetas = autograd.grad(outputs =q,
inputs = ac.pi.parameters(),
Create_graph=True)
# vectorize
dq_dw VeC = torch.zeros(size=[1]).to(device)
dq_dtheta VeC = torch.zeros(size=[1]).to(device)
for dq_dw in dq_dws:
dq_dw = torCh. flatten (dq_dw)
dq_dw VeC = torCh.Cat((dq_dw veC,dq_dw),dim=0)
dq_dw VeC = dq_dw veC[1:].unsqueeze(0) #delete the first element
for dq_dtheta in dq_dthetas:
dq_dtheta = torCh. flatten (dq_dtheta)
dq_dtheta VeC = torCh.Cat((dq_dtheta veC,dq_dtheta),dim=0)
dq_dtheta VeC = dq_dtheta VeC[1:].unsqueeze(0) #delete the first
element
k_1 = 2*torCh.transpose(dq_dw_VeC,dim0=0,dim1=1)@dq_dtheta_VeC#
Theta*w
#deal with the diagonal block
k_2_dia = 2 * dq_dw VeC * dq_dw VeC # w*w
reC_k_2_dia = torCh.reCiproCal(k_2_dia + 1e-5)
#calculate the gradient for actor
dq_dtheta staCk = torCh.transpose(dq_dtheta VeC,
dim0=0, dim1=1) \
32
Under review as a conference paper at ICLR 2022
+ torch.transpose(k_1, dim0=0, dim1=1) * \
rec_k_2_dia @ torch.transpose(dq_dw_vec, dim0=0, dim1=1)
dq_dtheta stack = dq_dtheta stack [:,0]
for i,para in enumerate(ac.pi.parameters()):
para.grad = dq_dtheta stack[param_index[i]:param_index[i+1]].
reshape(para.shape)
def UPdate(data):
#	First run one gradient descent step for Q.
q_optimizer.zero grad()
loss_q, loss_info = compute_loss_q(data)
loss_q.backward()
q_optimizer.step()
#	Manaully reset gradient
for P in ac.pi.parameters():
p.requires_grad = False
p.requires_grad = True
#	SDDPG Next run one gradient descent step for Pi.
Calculate_s_actor_loss(data)
pi_optimizer.step()
33