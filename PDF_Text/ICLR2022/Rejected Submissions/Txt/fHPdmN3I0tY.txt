Under review as a conference paper at ICLR 2022
Decoupled Kernel Neural Processes: Neu-
ral Network-Parameterized Stochastic Pro-
cesses using Explicit Data-driven Kernel
Anonymous authors
Paper under double-blind review
Ab stract
Neural Processes (NPs) are a class of stochastic processes parametrized by neural
networks. Unlike traditional stochastic processes (e.g., Gaussian processes), which
require specifying explicit kernel functions, NPs implicitly learn kernel functions
appropriate for a given task through observed data. While this data-driven learning
of stochastic processes has been shown to model various types of data, the current
NPs’ implicit treatment of the mean and the covariance of the output variables
limits its full potential when the underlying distribution of the given data is highly
complex. To address this, we introduce a new class of neural stochastic processes,
Decoupled Kernel Neural Processes (DKNPs), which explicitly learn a separate
mean and kernel function to directly model the covariance between output vari-
ables in a data-driven manner. By estimating kernel functions with cross-attentive
neural networks, DKNPs demonstrate improved uncertainty estimation in terms of
conditional likelihood and diversity in generated samples in 1-D and 2-D regres-
sion tasks, compared to other concurrent NP variants. Also, maintaining explicit
kernel functions, a key component of stochastic processes, allows the model to
reveal a deeper understanding of underlying distributions.
1	Introduction
Neural processes (NPs) (Garnelo et al., 2018a;b) are a class of stochastic processes parametrized by
neural networks. By embracing statistical properties in stochastic processes, NPs can effectively esti-
mate the uncertainty of underlying distributions of functions with a set of realizations and their data
points. Different from traditional stochastic processes (e.g., Gaussian processes (GP) (Rasmussen
& Williams, 2006)), NPs learn data-driven stochastic processes without a need to specify or keep
an explicit form of kernel functions. As a result of their simplicity and flexibility, there have been
numerous efforts to further develop improved variants of NPs (Kim et al., 2019; Lee et al., 2020;
Gordon et al., 2020) and apply them to various downstream tasks (Singh et al., 2019; Requeima
et al., 2019).
Though significant progress has been made in NPs, the current architectures of NPs either fails to
capture output dependencies as in Conditional NPs (Garnelo et al., 2018a; Gordon et al., 2020), or
indirectly capture the full stochasticity present in the traditional stochastic processes. For instance,
different from GPs, conventional NPs reserve stochasticity in a global latent variable and output
variables separately. The output variables estimate point-wise uncertainty, which corresponds to the
diagonal elements of a kernel matrix. Similarly, the global latent variable takes charge of the func-
tional uncertainty and diversity, represented by the full covariance matrix in GPs. Due to this induc-
tive bias of conventional NPs, the role of estimating functional stochasticity is mainly assigned to a
fixed-length vector (i.e. the global latent variable), and consequently, capturing the underlying dis-
tributions can be restricted in complex scenarios (e.g., variable relationships are periodic or abruptly
changing at a certain point). Although several approaches attempt to alleviate the problem by intro-
ducing attention (Kim et al., 2019) and bootstrapping (Lee et al., 2020) on top of conventional NPs,
the problem still exists as the architectural limitation (i.e. implicit modeling of the mean and co-
variance) has not been addressed directly. Besides this, as NPs implicitly learn the kernel functions
inside the model, the interpretability of kernels such as in GPs (Lloyd et al., 2014) is diminished.
1
Under review as a conference paper at ICLR 2022
Figure 1: (Left) Comparison of predictions given context points (black dot) by the Attentive NP
(ANP) and DKNP (Ours) after learning samples generated from a periodic kernel with fixed hyper-
parameters. The mean (blue curve) and sigma (shaded blue area) predicted from the DKNP better
represent the data, including target points (red dot) compared to the ANP. (Right) Visualization of
kernel functions learned by the DKNP. As a result of kernel learning in the DKNP, the underlying
prior distribution of data can be inferred (periodic kernel in this case).
To address this concern, we propose Decoupled Kernel Neural Processes (DKNPs), a new class
of neural stochastic processes that explicitly learn a separate mean and kernel function to directly
model the covariance between output variables in a data-driven manner. Our experiments in 1-D and
2-D regression tasks reveal that the DKNP outperforms concurrent NP variants in terms of predictive
likelihood, better global coherence of generated samples, and improved interpretability via explicitly
learned kernels.
2	Background
2.1	Neural Process
Given a stochastic process sample consisting of n points, let us denote the input and output as
X = {xi}in=1 and Y = {yi}in=1, respectively, where xi ∈ Rdx and yi ∈ Rdy . For a set of target
input XT = {xi}i∈T ⊂ X, NPs model the conditional distribution of target outputs YT conditioned
on the context set (XC, YC) = {(xi , yi)}i∈C using a factorized Gaussian distribution:
logp(YT|XT,XC,YC) =Xlogp(yi|xi,XC,YC).	(1)
i∈T
For obtaining the predictive distribution logp(yi|xi, XC, YC), NPs use an encoder-decoder architec-
ture that ensures the permutation invariance of the predictions of the target points given the context
set (XC,YC).
Following Kim et al. (2019) and Lee et al. (2020), we consider the NP encoder consisting of two
separate paths, namely the deterministic path and the latent path. For the deterministic path, fθ rep-
resents each context points in {(xi , yi)}i∈C as ri ∈ Rdr , i.e., ri = fθ (xi , yi). Then, we aggregate
the ri's by averaging them across all context points, rc =* ∑i∈C ri where nc = |C|. This vector
rC ∈ Rdr is the summarized representation of context points, and it is permutation invariant over
the order of (xi , yi) ∈ (XC , YC).
The latent path of the NP encoder operates in a similar fashion to the deterministic path, i.e.,
ec = n1 Ei∈C ei where ei = gφ([xi; yi]). Unlike the deterministic path, however, the la-
tent path uses stochastic layers for obtaining a distribution of the latent variable z ∈ Rdz ;
q(ZIeC) = N(z; μz, σ2) where μz, σz are the output of the additional fully-connected layer
applied to eC. Finally, by concatenating these aggregated vectors rC , z with the target inputs
Xi ∈ XT, the decoder hψ produces the predictive distribution p(yt∣xt, rc, Z) = N(y; μy, σj)
where (μy, σy) = hψ (Xt, rc, Z). Note that the σy is used for capturing the point-wise uncertainty.
During the training phase, the parameters are learned by maximizing the evidence lower bound of
log p(YT |XT, XC, YC) via the reparametrization trick (Kingma & Welling, 2014; Rezende et al.,
2014) as follows:
2
Under review as a conference paper at ICLR 2022
%
%
MLPθ-
畦i MLPθ-
ecι
ec2
Multihead
Cross — Attention
^uery	Key Value
f-----∖ f---->
也 2	e，2
―* MLPW 1~k %,
Multihead
Cross - Attention
Query
ht,
Key Value
---X	MLP。----*h wt
---卜MLP。）-----k叫
t2
匹1	也1	ej

也1
也2
也2
也2也2
∈): ConCat
Figure 2: Model architecture of DKNPs. DKNPs estimate the predictive distribution N (y; μy, Σy)
by using two attention-based deterministic paths using Multihead Cross-Attention (MCA) to model
the mean vector (upper path) and the full covariance matrix (lower path).
logp(YT|XT,XC,YC) ≥ Eq(z|XT,YT)[log p(YT |XT, z)] - KL(q(z|eT)kq(z|eC))	(2)
where q(z|eT) = q(et|XT, YT) is a distribution of latent variable by encoding the set of target
points (XT, YT) = {(xi, yi)}i∈T. This objective function consists of two parts: 1) the reconstruc-
tion loss for the target points and 2) the KL divergence term, minimizing the divergence between
two distributions q(z|XT, YT) and q(z|XC, YC). Note that, in practice, we assume that XC ⊂ XT
during the training phase. The KL divergence term encourages two distributions inferred by the
context sets and target sets to be similar, which is reasonable because two sets are generated from
the same function. Therefore, during the inference phase, the distribution of q(z |XC , YC) captures
the functional stochasticity which is demonstrated with the coherent sample generation. It can be
thought that z learns to capture the correlation of output variables of the stochastic processes.
2.2	Attentive Neural Processes
Multihead Attention Given n key-value pairs of matrices K ∈ Rn×dmodel and V ∈ Rn×dmodel ,
and m queries Q ∈ Rm×dmodel , the scaled dot product attention is formulated as:
Attention(Q, K, V )
Softmax ( ?K ) V
dmodel
∈ Rm×dmodel
(3)
where K, V, and Q are projected by learnable linear maps WsK, WsV , and WtQ from the source S
and target T .
The attention mechanism can be calculated from multiple subspaces, namely, multihead attention
(MHA) (Vaswani et al., 2017). Denoting a single attention head as Headi = Attention(Qi, Ki, Vi),
the aggregate attention from multiple subspaces can be expressed as:
MHA(Q, K, V) = Concat(Head1, ...,HeadH)WO ∈ Rm×dmodel,	(4)
where W O is the learnable linear map for aggretating the subspaces.
Attentive NPs Attentive NPs (ANPs) leverage attention to resolve the underfitting issue in NPs.
Unlike NPs that produce a single variable rC from the deterministic path, ANPs utilize a query-
specific variable r* by applying the attention score a% for each r during the aggregation of the
deterministic path, formulated as r* = Pii∈c a% ∙ r〃 The attention-based aggregation of ANPs
resembles how the GPs utilize the correlation to estimate the predictive distribution of the context
and the target points.
3	Decoupled Kernel Neural Processes
Decoupled Kernel Neural Processes use attention to explicitly learn a separate mean and kernel
function so as to directly model covariances between output variables with related input variables
in a data-driven fashion, which is contrary to conventional NPs that implicitly model the mean and
kernel function through the latent variable z. As shown in Figure 2, DKNPs estimate the predictive
3
Under review as a conference paper at ICLR 2022
distribution as multivariate Gaussian N (y; μy, Σy) by using two attention-based deterministic paths
using Multihead Cross-Attention (MCA) to model the mean vector (upper path) and the full covari-
ance matrix (lower path). Here, attention modules are extensively utilized as Le et al. (2018); Kim
et al. (2019) have demonstrated attention was helpful in achieving low predictive uncertainty near
the context points. With the predictive distribution N(y; μy, Σy), DKNPS are trained and evaluated
based on the function likelihood.
The core design of DKNPs is motivated by the predictive posterior distribution of GPs, where
XT , XC, YC are used for deriving the posterior mean,1 but only XT , XC for deriving the poste-
rior covariance as follows:
GP:	P(YT|Xt,Xc,Yc)= N(YTAX0,Xt∑XCχYC, ∑Xt,Xt - ∑XC,Xt∑XCXC∑Xc,xJ ,
(5)
DKNP: P(YT I Xt ,Xc ,Yc ) = N (YT; MLPψ ◦ MCA(MLPθ (Xc ,Yc ), MLPφ(Xτ ,Xc )),
MLPω ◦ MCA(MLPφ(Xt,Xc))) ,	(6)
where, in contrast to GPs, DKNPs’ covariance is learned via attention.2 This decoupled process
allows DKNPs to explicitly learn the prior of the given dataset, and thus act as a true generative
process, ensuring the global consistency of all points in the given stochastic process samples. Unlike
DKNPs, NPs pack all information (XC, YC, XT) into latent variables to derive both mean and co-
variance, thus inherently becoming a conditional process that requires a sufficient amount of context
points, unable to explicitly learn a prior.
Specifically, the DKNPs pass each context point {(xi, yi)}i∈C, the concatenation of xi and yi, to
MLPθ and represent it as ei. Similarly, we produce the representation vector of xi, hi, using another
MLPφ for all inputs {xi}i∈C∪T. Then, ei and hi are passed to the MCA module to create the mean
vector μy. All heads in the MCA module perform cross-attention, Q = {hi}i∈τ, K = {hi}i∈c,
and V = {ei}i∈C. We adopted the architecture of MCA used in image transformers (Parmar et al.,
2018), where the original query vectors are added to the output from the MCA through the residual
path. This allows to do inference the output distributions without context points, which can be con-
sidered as prior distributions of DKNPs—the learned prior of DKNP. After the MCA, the last MLPψ
generates the predictive mean vector μy for each data point. Intuitively, this can be interpreted as
predicting the target mean based on the context and the correlation, which resembles the estimation
of the predictive distribution in GPs and ANPs.
Different from the NPs, DKNPs explicitly capture the correlation between the output variables using
another multihead cross-attention (MCA), where Q = {hi}i∈T, K = {hi}i∈C, and V = {ei}i∈C.
Then MLPω produces the representation vector wi ∈ Rdw for each position, which are combined to
generate the covariance matrix Σ = WW>, where Wi,: = wi and Σij = kernel(xi, xj) = wi>wj.
One might consider using self-attention to let the model learn the correlation between all data points.
However, the self-attention module on only X as inputs receives no indication of context and target
points and therefore fails to reduce the uncertainty near the points that have high confidence (e.g.,
context points). Also, the interaction between the target points through self-attention does not guar-
antee consistency under the marginal distribution of target points when the context points are given.
Lastly, it is also important to note that the representation h is shared when modeling both the mean
and the covariance. This motivation is drawn from Equation 5 that the calculation of the mean is
also based on the kernel matrices, Σ>X ,X and Σ-X1C,XC.
To train the DKNPs, the obtained mean vector μ and the covariance matrix Σ act as parameters of a
predictive distribution N(Y; μγ, Σγ). Instead of maximizing the lower bound of the log-likelihood
as in most NP models, the training objective of DKNPs is to maximize the tractable log-likelihood
of the Gaussian as follows:
log p(Yτ XT ,Xc ,Yc )=log N (YT; μγ, Σγ) where Σγ = WW > .	(7)
1We follow the typical GP formulation where the mean function is set to zero.
2Note that DKNPs assume C ⊂ T during training, which makes Eq. 5 and Eq. 6 technically different. This
difference comes from DKNPs using all data points for better learning the kernels, unlike fixed-kernel GPs.
DKNPs and GPs, however, are different methods and superior empirical performance led to the current design
choice.
4
Under review as a conference paper at ICLR 2022
Table 1: Results (log-likelihood) of the NP variants and DKNP in multiple 1D regression tasks
Methods	RBF	Periodic	Matern 5/2
CNP	0.695 (±0.010)	-0.328 (±0.032)	0.558 (±0.006)
NP	0.577 (±0.015)	-0.619 (±0.005)	0.417 (±0.009)
BNP	0.754 (±0.004)	-0.018 (±0.042)	0.617 (±0.006)
ANP	1.086 (±0.001)	0.831 (±0.011)	1.020 (±0.000)
BANP	1.084 (±0.001)	0.821 (±0.018)	1.018 (±0.001)
Ours	1.109 (±0.001)	0.941 (±0.006)	1.039 (±0.004)
RBF	Periodic	Matern
Figure 3: Comparison of predictions by ANP and DKNP. The dark blue line indicates the mean, and
the light blue shade indicates one standard deviation range.
Although the proposed objective function is equivalent to CNP’s (Garnelo et al., 2018a), model-
ing the correlation between output variables for capturing functional stochasticity shares the same
motivation of NPs, thus DKNP being one of NP variants.
4	Experiments
We compare DKNP with diverse NP variants such as Conditional NP (CNP)(Garnelo et al., 2018a),
NP, Bootstrapping NP (BNP)(Lee et al., 2020), ANP, and Bootstrapping ANP (BANP) on both 1D
and 2D regression tasks. For a fair comparison, all NP variants use two paths for context encoding.
CNP, BNP, and BANP have two deterministic paths, and NP and ANP have one deterministic and
one stochastic path. Also, ANP and BANP have additional self-attention in both stochastic and deter-
ministic paths. Following Lee et al. (2020), BNP and BANP were trained with 50 bootstrap context
samples and tested with 4 samples. Note that we mostly followed the same hyperparameter setup
used in Lee et al. (2020) and the details are described in Appendix A. Unlike the NP variants (except
CNP) which are trained with the lower bound of conditional log-likelihood log p(YT |XT, XC, YC)
or its slight modification, DKNP can directly evaluate it as well as maximize it during training.
All results are also reported with the conditional log-likelihood to evaluate the methods’ ability to
model the given stochastic processes. Following (Le et al., 2018; Lee et al., 2020), we use impor-
tance weighting estimation (Burda et al., 2016) with 50 samples to evaluate the performance ofNP
variants that utilize z .
4.1	1D Regression with Gaussian Processes Data
We first conduct basic evaluation of DKNP in comparison to NP variants by testing its ability to
model 1D stochastic process samples generated from Gaussian Processes of diverse kernels, namely
RBF, Periodic and Matern 5/2. In this task, we randomly sampled input X from [-2, 2] and gen-
erated y from GP kernels where the hyperparameters were also sampled both during training and
testing. The number of context points nc is sampled from Unif (3, 25) and the target points is
5
Under review as a conference paper at ICLR 2022
Figure 4: Visualization of the correlation matrices learned from the DKNP compared to the ground truths. We
take a Monte Carlo estimate (averaging 1,000 kernel functions of the same type) to visualize the ground truth
kernels for multiple hyperparameters at once.
⅛
O
CL
Figure 5: Comparison of predictions by ANP and DKNP. The dark blue line indicates the mean, and
the light blue shade indicates one standard deviation range.
nc + Unif (3, 50 - nc). Details about the data generation process (kernel hyperparameters), the
training process, and the evaluation process are in Appendix C.1.
The results in Table 1 show that DKNP can better model the given stochastic processes compared to
various NP variants in terms of the log-likelihood. Note that the attention mechanism dramatically
increases all methods’ ability to model GP samples, as can be seen by the clear divide between
CNP, NP, BNP and the rest. The true merit of DKNP, however, is its capability to better estimate the
uncertainty of the stochastic processes thanks to its explicit modeling of the full covariance matrix.
Figure 3, for example, clearly demonstrates the distinguishing feature of DKNP compared to previ-
ous NP variants, in this case ANP, which recorded the second highest likelihood in Table 1. In all
kernel types, ANP fails to contain the target points within one standard deviation range while DKNP
successfully captures the uncertainty in all unobserved points. Specifically, ANP is over-confident
When modeling kernel types RBF and Matern 5/2, where only a local structure exists. For periodic
kernels, where both local and global structure exists, ANP fails to capture the underlying periodic-
ity, due to its architecture that tries to estimate functional stochasticity with a fixed-size global latent
variable. Unlike ANP, however, DKNP can correctly model the whole stochastic process thanks to
its ability to explicitly model the correlations between all output variable.
Employing decoupled paths for modeling mean and covariance as in Figure 2, another advantage of
DKNP is improved interpretability, as we can explicitly check the learned prior by visualizing the
covariance matrix as described in Section 3. Such example is depicted in Figure 4, where we can
readily compare the learned prior with the ground truth prior for all three kernel types. Note that
DKNP not only model allows us to visually check the learned prior, but also demonstrate its ability
to accurately learn the ground truth kernels for all kernel types.
4.2	1D Regression with Change Point
Next we test all methods in a more challenging setup, where the underlying dynamics of the stochas-
tic process changes at midpoint. Specifically, we employ three different GP configurations: Periodic-
6
Under review as a conference paper at ICLR 2022
O-Poued — O-Poued
Figure 6: Visualization of the learned correlation matrices and the sampled data for Periodic-Periodic and
RBF-Periodic compared to the ground truths.
Table 2: Results (log-likelihood) of the NP variants and DKNP for three different change point configurations.
Methods	Periodic - Periodic	Periodic - RBF	RBF - RBF
CNP	-0.423 (±0.016)	0.051 (±0.028)	0.966 (±0.006)
NP	-0.531 (±0.022)	-0.007 (±0.011)	0.872 (±0.017)
BNP	-0.358 (±0.022)	0.105 (±0.023)	0.984 (±0.011)
ANP	0.602 (±0.017)	0.883 (±0.005)	1.142 (±0.001)
BANP	0.605 (±0.026)	0.877 (±0.010)	1.145 (±0.001)
Ours	0.892 (±0.003)	0.955 (±0.013)	1.260 (±0.000)
Periodic, Periodic-RBF, and RBF-RBF. Each configuration uses one kernel type (with a fixed hy-
perparamter setup) up to point 0, then another kernel type afterwards. As the correlations between
output variables are more complex in this setup, we expect DKNP to demonstrate even more distin-
guishing performance than NP variants. The training and evaluation processes are the same as the
previous experiments in Section 4.1. See Appendix C.1 for data generation details.
The results in Table 2, as expected, show wider gaps between DKNP and NP variants for all three
configurations in terms of log-likelihood. Interestingly, unlike the the previous results (Table 1)
where DKNP marginally outperformed NP variants for RBF kernels (but beyond one standard de-
viation margin), the performance gap is more distinguished even for RBF-RBF configuration in this
setup. This observation indicates DKNP’s ability to better capture complex correlations derived from
a combination of rather simpler kernels than NP variants.
Figure 5 demonstrates predictions by ANP and DKNP for all three change point configurations.
Compared to ANP which fails to correctly capture the changing dynamics of the given processes
(especially for configurations including periodic kernels), DKNP shows its ability to correctly learn
the underlying dynamics before and after the change point. In Figure 6, we compare the learned
priors as well as the generated samples from them to the ground truths priors and samples for two
change point configurations, Periodic-Periodic and RBF-Periodic. Note that DKNP can accurately
learn two distinct kernels in both sides of the midpoint, (i.e. low and high frequency in the Periodic-
Periodic case, smooth and periodic in the RBF-Periodic case) thus able to generate samples that are
practically equivalent to the ground truth samples.
4.3	2D Image Completion
The purpose of the 2D image completion task is to test how well each model shows its functional
flexibility to learn non-trivial kernels. Assuming that image data is generated by a stochastic process,
the task is to regress pixels that are missing based on the provided pixels (i.e. context pixels) in the
image. Specifically, we use the CelebA dataset (Liu et al., 2015), which consists of 202,599 number
of facial images of celebrities of diverse ethnicities, gender, and age.
7
Under review as a conference paper at ICLR 2022
Figure 7: (Left) The prior learned by DKNP. (Right) Predicted means of various models given single context
point shown in first column.
Original Image Context
Figure 8: Examples of the image completion task. Based on the original image, roughly 10% of pixels (100
pixels) were given as the context. The filled images generated from DKNP (Ours) are much more diverse and
clearer, compared to other baselines.
For both training and evaluation, the number of context points nc is sampled from Unif(3, 200) and
the target points is nc + Unif(3, 200 - nc). Details about the data generation process, the training
process, and the evaluation process are in Appendix C.2.
In Table 3, consistent with the previous results, DKNP shows superior ability in modeling even two-
dimensional stochastic processes compared to all NP variants. This is somewhat predictable based
on two previous results (Table 1 and Table 2) where the performance gap between DKNP and NP
variants was more visible for stochastic processes with complex underlying dynamics (where both
local and global structure exist), and the fact that the facial images are likely to follow non-trivial
dynamics. For example, the dynamics to generate the hair or the background, which are typically
low frequency signals, would be quite different from the dynamics to generate the details of the face
where eyes, nose, and mouth altogether compose higher frequency signals. And unlike the previous
change point data where the boundary between two different dynamics (i.e. kernels) is a point in a
1D line, the boundary between different kernels in facial images is a (curved) line in a 2D plane (e.g.
hairline). Therefore we can expect DKNP to significantly outperform all NP variants in estimating
the correlations between output variables, as shown in Table 3.
Figure 7 illustrates the mean of the prior distribution learned by DKNP on the CelebA dataset and
the predictive mean of the NP variants and DKNP when a single context point is given in the back-
ground. The posterior mean of DKNP demonstrates distinct behavior for the face and background,
8
Under review as a conference paper at ICLR 2022
such that the given context uniformly affects the background while keeping the face practically
intact. This shows that DKNP accurately captures the correlations between output variables by suc-
cessfully learning kernel functions that behave differently for face and background. BANP, ANP,
and NP, on the other hand, demonstrate unpredictable behaviors where the context point in the back-
ground also affects the hair or the clothes, or the background is only partially affected by the given
context point. This behavior reveals that NPs, which implicitly learn the kernel function, have a
harder time modeling the accurate correlation between output variables.
We conclude this section with qualitative examples in
Figure 8 that highlight the DKNP’s capability to gener-
ate diverse samples thanks to its explicit modeling of the covariance. Given 10% of the data points as context, ANP samples are generated by sampling the latent variable z, BANP samples are generated via bootstrapping, and DKNP samples are generated with the predicted mean	Table 3: Results (log-likelihood) of NP variants and our model in the 2D image completion task.	
	Methods	Celeb A
and covariance. While ANP, BANP and DKNP all pro-	CNP	2.280 (±0.010)
duce reasonable predictive means, ANP’s generated sam-	NP	2.374 (±0.024)
ples heavily resemble one another, whereas DKNP’s gen-	BNP	2.876 (±0.014)
erated samples demonstrate considerable diversity such	ANP	3.471 (±0.005)
as different genders, facial expressions and hair styles.	BANP	3.627 (±0.001)
BANP also demonstrates more varied samples than ANP	Ours	3.951 (±0.021)
thanks to its bootstrapping process, but they sometimes		—
contain noisy pixels.
5	Related Work
Many advances in NPs are made with the advent of neural network-based stochastic processes such
as NPs (Garnelo et al., 2018b) and CNPs (Garnelo et al., 2018a). ANP (Kim et al., 2019) show a dra-
matic performance gain by leveraging attention mechanisms in the aggregating operation. Also, Le
et al. (2018) confirm through extensive empirical evaluation on the design of NPs such as architec-
ture and objective functions that combining attention and NPs improve the predictive log-likelihood
marginally. Lee et al. (2020) extends NPs using the bootstrap technique for estimating functional
uncertainty without maintaining a latent variable in the NP architecture. This notion of removing the
latent variable part is inline with our motivation.
For modeling stationary stochastic processes, Gordon et al. (2020) introduced Convolutional CNP
(ConvCNP), where translation equivariance in the data is explicitly encoded in the model. Convo-
lutional NPs (ConvNP) Foong et al. (2020) are a natural extension of ConvCNPs where a global
latent variable was introduced to model dependencies of the predictive distribution. Gaussian Neu-
ral Processes (GNP) (Bruinsma et al., 2021) further generalize these classes of models with transla-
tion equivariance by leveraging convolutional neural networks to capture the predictive correlations.
While GNPs share a similar motivation to our work, the way covariance matrix is parameterized is
quite different (e.g., 1-D, 2-D convolution versus attention) from DKNPs, as their focus is mainly
on translation equivariance. Specifically, the covariance matrix of GNPs depends on YT , while for
DKNPs, motivated by posterior of GPs, the covariance only depends on the inputs XC, XT (see
equation 6).
6	Conclusion
We propose a new neural stochastic processes, Decoupled Kernel Neural Processes (DKNPs), that
learn an explicit kernel function to better capture the correlation between output variables. By lever-
aging cross- and mixed attention mechanisms to model an explicit kernel function, DKNPs outper-
form the concurrent NP variants in terms of predictive likelihood and better global coherence of
generated samples. By the novel model architecture of DKNPs, the learned prior can be accessible,
which provides a deeper understanding of the underlying distributions of data. As future work, one
could consider developing a method to manipulate a learned kernel or to impose a constraint on the
kernel learning process of DKNPs with prior knowledge.
9
Under review as a conference paper at ICLR 2022
[Reproducibility Statement] We utilized the datasets and the baselines that are all available on Github
(https://github.com/juho-lee/bnp). As for our proposed model and code, we will make a comment
directed to the reviewer and area chairs once the discussion forums are open.
References
Wessel Bruinsma, James Requeima, Andrew Y. K. Foong, Jonathan Gordon, and Richard E Turner.
The gaussian neural process. In Symposium on Advances in Approximate Bayesian Inference
(AABI), 2021.
Yuri Burda, Roger B. Grosse, and Ruslan Salakhutdinov. Importance weighted autoencoders. In
International Conference on Learning Representations (ICLR), 2016.
Andrew Y. K. Foong, Wessel Bruinsma, Jonathan Gordon, Yann Dubois, James Requeima, and
Richard E. Turner. Meta-learning stationary stochastic process prediction with convolutional
neural processes. In Advances in Neural Information Processing Systems (NeurIPS), 2020.
Marta Garnelo, Dan Rosenbaum, Christopher Maddison, Tiago Ramalho, David Saxton, Murray
Shanahan, Yee Whye Teh, Danilo Rezende, and S. M. Ali Eslami. Conditional neural processes.
In International Conference on Machine Learning (ICML), 2018a.
Marta Garnelo, Jonathan Schwarz, Dan Rosenbaum, Fabio Viola, Danilo J Rezende, SM Eslami,
and Yee Whye Teh. Neural processes. arXiv preprint arXiv:1807.01622, 2018b.
Jonathan Gordon, Wessel P. Bruinsma, Andrew Y. K. Foong, James Requeima, Yann Dubois, and
Richard E. Turner. Convolutional conditional neural processes. In International Conference on
Learning Representations (ICLR), 2020.
Hyunjik Kim, Andriy Mnih, Jonathan Schwarz, Marta Garnelo, Ali Eslami, Dan Rosenbaum, Oriol
Vinyals, and Yee Whye Teh. Attentive neural processes. In International Conference on Learning
Representations (ICLR), 2019.
Diederik P. Kingma and Max Welling. Auto-encoding variational bayes. In International Conference
on Learning Representations (ICLR), 2014.
Tuan Anh Le, Hyunjik Kim, Marta Garnelo, Dan Rosenbaum, Jonathan Schwarz, and Yee Whye
Teh. Empirical evaluation of neural process objectives. In NeurIPS workshop on Bayesian Deep
Learning, 2018.
Juho Lee, Yoonho Lee, Jungtaek Kim, Eunho Yang, Sung Ju Hwang, and Yee Whye Teh. Boot-
strapping neural processes. In Advances in Neural Information Processing Systems (NeurIPS),
2020.
Ziwei Liu, Ping Luo, Xiaogang Wang, and Xiaoou Tang. Deep learning face attributes in the wild.
In Proceedings of the IEEE international conference on computer vision, 2015.
James Robert Lloyd, David Duvenaud, Roger Grosse, Joshua B. Tenenbaum, and Zoubin Ghahra-
mani. Automatic construction and natural-language description of nonparametric regression mod-
els. Association for the Advancement of Artificial Intelligence (AAAI), 2014.
Niki Parmar, Ashish Vaswani, Jakob Uszkoreit, Lukasz Kaiser, Noam Shazeer, Alexander Ku, and
Dustin Tran. Image transformer. In International Conference on Machine Learning (ICML),
2018.
Carl Edward Rasmussen and Christopher K. I. Williams. Gaussian processes for machine learning.
MIT Press, 2006.
James Requeima, Jonathan Gordon, John Bronskill, Sebastian Nowozin, and Richard E Turner. Fast
and flexible multi-task classification using conditional neural adaptive processes. In Advances in
Neural Information Processing Systems (NeurIPS). 2019.
Danilo Jimenez Rezende, Shakir Mohamed, and Daan Wierstra. Stochastic backpropagation and ap-
proximate inference in deep generative models. In International Conference on Machine Learning
(ICML), 2014.
10
Under review as a conference paper at ICLR 2022
Gautam Singh, Jaesik Yoon, Youngsung Son, and Sungjin Ahn. Sequential neural processes. In
Advances in Neural Information Processing Systems (NeurIPS), 2019.
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,
L Ukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Advances in Neural Infor-
mation Processing Systems (NeurIPS), 2017.
11
Under review as a conference paper at ICLR 2022
A Architectures Details
We mostly followed the same model architectures in the paper (Lee et al., 2020). num_detqath
and num.detjpath indicate the number of deterministic and stochastic paths.
A.1 1D regression task
	CNP	NP	BNP	ANP	BANP	DKNP
dim_x	1	1	1	1	1	1
dim_y	1	1	1	1	1	1
dim_hid	128	128	128	128	128	128
dim」at	-	128	-	128	-	-
num-det_path	2	1	2	1	2	2
num_StOCh_Path	0	1	0	1	0	0
enc_v_depth	-	-	-	4	4	4
enc_qk_depth	-	-	-	2	2	2
enc_pre_depth	4	4	4	4	4	4
enc_post_depth	2	2	2	2	2	2
dec_depth	3	3	3	3	3	3
Table 4: Hyperparameter setting for the 1D regression task
A.2 Image completion task
	CNP	NP	BNP	ANP	BANP	DKNP
dim_x	2	2	2	2	2	2
dim-y	3	3	3	3	3	3
dim_hid	128	128	128	128	128	128
dim」at	-	128	-	128	-	-
num_det_path	2	1	2	1	2	2
num_stoch_Path	0	1	0	1	0	0
enc_v_depth	-	-	-	6	6	6
enc_qk_depth	-	-	-	3	3	3
enc_pre_depth	6	6	6	6	6	6
enc_post_depth	3	3	3	3	3	3
dec_depth	5	5	5	5	5	5
Table 5: Hyperparameter setting for the image completion task
B	Computational Complexity
GP	NP	ANP	Ours
Complexity O((n + m)3)	O(n + m) O(n(n + m)) O(n(n + m))
Table 6: The complexity comparison between GP, NP, ANP, and DKNP
DKNP has attention mechanisms like ANP, therefore, it costs O(n(n+m)), which is still computa-
tionally much more efficient than GP even with the full covariance matrix.
C Experimental Details
C.1 1D Regression
Data generation from a single kernel We constructed training and test data generated from GPs
with multiple kernels including RBF, periodic kernels, and Matern 5/2. For each task, We generated
12
Under review as a conference paper at ICLR 2022
X 〜Unif(-2,2) and y from GP kernels where the GP hyperparameters are sampled from uniform
distributions. For RBF kernel, k(x, x0) = s2 exp(-∣∣x — x0∣∣2∕2l2), We sampled S 〜Unif(0.1,1.0),
l 〜Unif(0.1,0.6), and output additive noise N(0,10-2). Given n, the size of the context C was
drawn from Unif(3,47) and the size of the target is sampled from Unif(3, 50 — nJ. For Matem 5/2
kernel k(x, x0) = s2(1 + √5d∕l + 5d2∕(3l2)) ∙ exp(-√5d∕l), (d = ||x — x0||), we sampled from
s 〜Unif(0.1,1.0) and l 〜Unif(0.1,0.6). For periodic kernel, k(x, x0) = s2 exp(-2sin2(π∣∣x —
x0∣∣2∕p)∕l2), we sampled from S 〜Unif(0.3,1.0), l 〜Unif(0.6,1.0), andP 〜Unif(0.8,1.0). We
trained all models identically for 100,000 steps with training batch size of 100. We used Adam
optimizer with initial learning rate of 5 ∙ 10-4 and decayed using cosine annealing scheme.
Data generation from two kernels Similar to a single kernel case, data points are sampled from a
GP but with two types of kernels, RBF and periodic. The half of the points ranging from x = -2 and
x = 0 were sampled from one kernel and the rest from x = 0 to x = 2 were from another kernel.
We generated samples from three scenarios: Periodic - Periodic, RBF - Periodic, and RBF - RBF.
Note that we set the correlation between two kernels being zero. The training details are identical to
the single kernel case.
C.2 Image Completion
CelebA32 Similar to 1-D regression tasks, we randomly sampled pixels of a given image at train-
ing as targets, and treat a subset of the points as contexts. The size of the contexts and targets is
sampled from Unif(3, 200) and nc+Unif(0, 200 - nc). For preprocessing, x is rescaled to [-1, 1] and
y is rescaled to [-0.5, 0.5]. We trained all models identically for 200 steps with training batch size of
100. We used Adam optimizer with initial learning rate of 5 ∙ 10-4 with cosine annealing scheme.
13