Under review as a conference paper at ICLR 2022
Graph Piece: Efficiently Generating High-
Quality Molecular Graphs with Substruc-
TURES
Anonymous authors
Paper under double-blind review
Ab stract
Molecule generation, which requires generating valid molecules with desired
properties, is a fundamental but challenging task. Recent years have witnessed
the rapid development of atom-level auto-regressive models, which usually con-
struct graphs following sequential actions of adding atom-level nodes and edges.
However, these atom-level models ignore high-frequency substructures, which not
only capture the regularities of atomic combination in molecules but are also often
related to desired chemical properties, and therefore may be sub-optimal for gener-
ating high-quality molecules. In this paper, we propose a method to automatically
discover such common substructures, which we call graph pieces, from given
molecular graphs. We also present a graph piece variational autoencoder (GP-
VAE) for generating molecular graphs based on graph pieces. Experiments show
that our GP-VAE models not only achieve better performance than the state-of-
the-art baseline for distribution-learning, property optimization, and constrained
property optimization tasks but are also computationally efficient.
1 Introduction
Molecule generation is a task that aims to produce chemically valid molecules with optimized prop-
erties. It is important for a variety of applications, such as drug discovery and material science.
Graph-based molecule generation models, which are robust to molecule substructures (You et al.,
2018; Kwon et al., 2019), have gained increasing attention recently (Jin et al., 2018; Li et al., 2018a;
You et al., 2018; Kwon et al., 2019; De Cao & Kipf, 2018; Shi et al., 2020; Jin et al., 2020b).
Graph-based molecule generation models typically decompose molecular graphs into sequential ac-
tions of generating atoms and bonds autoregressively (Li et al., 2018b; You et al., 2018; Li et al.,
2018a; Jin et al., 2020b). While this decomposition is natural and straightforward, it inevitably
ignores the existence of common substructures in molecular graphs, as illustrated in Figure 1. Com-
pared with using atoms for generating molecules, using graph substructures for generating molecules
have three potential benefits. First, using substructures can capture the regularities of atomic combi-
nation in molecules, and therefore is more capable of generating realistic molecules. Second, using
substructures can better capture chemical properties, as there is a correlation between substructures
and chemical properties (Murray & Rees, 2009; Jin et al., 2020b). Third, using substructures enables
efficient training and inference. It is evident that using substructures to represent molecular graphs
can result in much shorter sequences, therefore the training and inference process can be accelerated.
As a result, We believe that models using substructures to represent molecular graphs can generate
more realistic molecules with better-optimized properties efficiently.
83.9%	60.5%	41.3%
32.6%	27.5%
Figure 1: Five high-frequency substructures in the standard ZINC250K dataset (Irwin et al., 2012;
Kusner et al., 2017). The percentage means the ratio of molecules which have the substructure.
1
Under review as a conference paper at ICLR 2022
In this paper, we present an iterative algorithm to automatically discover common substructures in
molecules, which we call graph pieces. Initially, graph pieces correspond to single atoms that appear
in graphs of a given dataset. Then for each iteration, we count the occurrence of neighboring pieces
in graphs and merge the most frequent neighboring pieces into a new graph piece. Since substruc-
tures can be seen as small molecules, we use SMILES (Weininger, 1988), a text-based representa-
tion for molecules, to efficiently judge whether two graph pieces are identical. To effectively utilize
these substructures, we also propose a graph piece variational autoencoder (GP-VAE). Our model
consists of a graph neural network (GNN, Scarselli et al., 2008) encoder and a two-step decoder. The
two-step decoder first generates graph pieces auto-regressively and then predicts atom-level bonds
between graph pieces in parallel. As a result, our GP-VAE decouples the alternated generation of
nodes and edges, achieving significant computational efficiency for both training and generation.
We conduct extensive experiments on ZINC250K (Irwin et al., 2012) and QM9 (Blum & Rey-
mond, 2009; Rupp et al., 2012) datasets. Results demonstrate that our GP-VAE models outperform
state-of-the-art models on distribution-learning, property optimization, and constrained property op-
timization tasks, and are about six times faster than the fastest baseline.
2	Related Work
Molecule Generation Based on different representations for molecules, molecule generation
models can be divided into two categories: text-based and graph-based. Text-based models (Gomez-
Bombarelli et al., 2018; Kusner et al., 2017; Bjerrum & Threlfall, 2017), which usually adopt
the Simplified Molecular-Input Line-entry System (SMILES) (Weininger, 1988) representation, are
simple and efficient methods for generating molecules. However, these models are not robust be-
cause a single perturbation in the text molecule representation can result in significant changes in
molecule structure (You et al., 2018; Kwon et al., 2019). Graph-based models (De Cao & Kipf,
2018; Shi et al., 2020; Jin et al., 2020b), therefore, have gained increasing attention recently. Li
et al. (2018b) proposed a generation model of graphs and demonstrated it performed better than
text-based generation models on molecule generation. You et al. (2018) used reinforcement learn-
ing to fuse rewards of chemical validity and property scores into each step of generating a molecule.
Popova et al. (2019) proposed an MRNN to autoregressively generate nodes and edges based on the
generated graph. Shi et al. (2020) proposed a flow-based autoregressive model and use reinforce-
ment learning for the goal-directed molecular graph generation. However, these models use atom-
level graph representation, which results in very long sequences, and therefore the training process
is typically time-consuming. Our method is graph-based and uses substructure-level representation
for graphs, which not only captures chemical properties but also is computationally efficient.
Substructure-level Graph Representation Jin et al. (2018) proposed to generate molecules in the
form of junction trees where each node is a ring or edge. Jin et al. (2020a) decomposed molecules
into substructures by breaking all the bridge bonds. It used a complex hierarchical model for poly-
mer generation and graph-to-graph translation. Jin et al. (2020b) proposed to extract the smallest
substructure which maintains the original chemical property. The extracted substructures usually in-
clude most atoms of the original molecules, which are too coarse-grained and exert limitations on the
search space. The models proposed by Jin et al. (2020a) and Jin et al. (2020b) are not suitable for the
experiments in this paper since they either need graph-to-graph supervised data or are incompatible
with continuous properties. There exists various methods to discover frequent subgraphs (Inokuchi
et al., 2000; Yan & Han, 2002; Nijssen & Kok, 2004). However, they have difficulty decomposing
graphs into frequent subgraphs (Jazayeri & Yang, 2021) since they mainly aim to discover frequent
subgraphs as additional features for downstream network analysis. Therefore they can hardly be
applied to substructure-level molecular graph representation. Different from Jin et al. (2018; 2020a)
which use manual rules to extract substructures, we automatically extract common substructures
which better capture the regularities in molecules for substructure-level decomposition.
3	Approach
We first give the definition of graph pieces and algorithms for graph piece extraction in Section
3.1. Then we describe the encoder of our graph piece variational autoencoder (GP-VAE) model in
Section 3.2. Finally, we descibe the two-step decoder of GP-VAE in Section 3.3.
2
Under review as a conference paper at ICLR 2022
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
Algorithm 1: Graph Piece Extraction
Input: A set of graphs D and the desired number N of graph pieces to learn.
Result: A set of graph pieces S and the counter F of graph pieces.
begin
S — {GraphTοSMILES(h{a}, 0))}; . Initially, S corresponds to all atoms a that appear in D.
N0 J max(N, |S |);
while |S | < N0 do
F J EmPtyMaP();
foreach G in D do
. --,_ _ ~ _ _
forall hPi,Pj, Eiji n G do
Z
. Initialize a counter.
P J Merge(hPi, Pj, Eij i); . Merge neighboring graph pieces into a new graph piece.
S — GraPhToSMILES(P);
F[s] = F[s] + 1;
end
end
s = ToPElem(counter);
P — SMILESToGraPh(s);
S-S∪{s}; D0 ― {};
foreach G in D do
G0 — MergeSubGraph(G, P);
I D0 JD ∪{G0};
end
D J D0
. Convert a graph to SMILES representation.
. Update the counter, the default value for a new s is 0.
. Find the most frequent merged graph piece.
. Convert the SMILES string to graph representation.
. Update the graph representation if possible.
end
22 end
Figure 2: Four graph pieces in
an example molecule. Different
graph pieces are highlighted in
different colors.
3.1	Graph Piece
A molecule can be represented as a graph G =(V, E)，where V
is a set of nodes that correspond to atoms and E is a set of edges
that correspond to chemical bonds. Instead of using atoms, We use
substructures, which we call graph pieces, as building blocks. We
define a graph piece P as a subgraph (V, Ei that appears in a graph G,
where V ⊆ V and E ⊆ E. It should be noted that either a single atom
or a whole graph is a valid graph piece. Given a set of graph pieces
S, suppose the graph G can be decomposed into n graph pieces in S,
then G can be alternatively represented as ({Pi}, {Ej}), where Eij
denotes the set of edges between two neighboring graph pieces Pi
and Pj. The decomposition of a graph G into graph pieces satisfies
the following constraints: (1) the union of all atoms in the graph
pieces equals to all atoms in the molecule, namely Si Vi = V ; (2) there is no intersection between
any two graph pieces, namely ∀i = j, Vi ∩ Vj = 0, and Ei ∩ Ej = 0; (3) the union of all connections
within and between graph pieces equals to all bonds in the molecule, namely i,j (Ei ∪ Eij) = E,
where i range from 1 to n and j range from i + 1 to n. Figure 2 shows an decomposed molecule.
The algorithm for extracting graph pieces from a given set of graphs D is given in Algorithm 1. Our
algorithm draws inspiration from Byte Pair Encoding (Gage, 1994, BPE). Initially, a graph G in D is
decomposed into atom-level graph pieces and the vocabulary S of graph pieces is composed of all
unique atom-level graph pieces that appear in D . Given the number N of graph pieces to learn, at
each iteration, our algorithm enumerates all neighboring graph pieces and edges that connect the two
graph pieces in G, namely (Pi, Pj, Eij i. As (Pi, Pj, Eiji is also a valid subgraph, we merge it into a
graph piece and count its occurrence. We find the most frequent merged graph piece P and add it into
the vocabulary S. After that, we also update graphs G in D that contain P by merging (Pi, Pj, Eiji
into P. The algorithm terminates when the vocabulary size exceeds the predefined number N. Note
that we use SMILES (Weininger, 1988) to represent a graph piece in our algorithm1, therefore we
ensure the uniqueness of a graph piece. A running example of our graph piece extraction algorithm
is illustrated in Figure 3. At test time, we first decompose a molecular graph into atom-level graph
pieces, then apply the learned operations to merge the graph pieces into larger ones. This process
ensures there is a piece-level decomposition for an arbitrary molecule. We provide the pseudo
code for the piece-level decomposition in Appendix A for better understanding. We provide the
complexity analysis for both algorithms in Appendix B.
1We use RDKit (www.rdkit.org) to perform the conversion between molecular graph and SMILES.
3
Under review as a conference paper at ICLR 2022
(c) Iteration 2
(a) Initialization
(b) Iteration 1
Figure 3: Two iterations of our graph piece extraction algorithm on {C=CC=C,CC=CC,C=CCC}.
(a) The vocabulary is initialized with atoms. (b) Graph piece CC is the most frequent and added
to the vocabulary. All CC are merged and highlighted in red. (c) Graph piece C=CC is the most
frequent and added to the vocabulary. All C=CC are merged and highlighted in green (molecules 1
and 3). After 2 iterations the vocabulary is {C, CC, C=CC}. Note that due to the uniqueness of
SMILES, graph piece C=CC will not be translated into CC=C.
3.2	Graph Encoder
As shown in Figure 4, we use a GNN to encode a molecular graph G represented by graph pieces
into a latent variable z. Each node v has a feature vector xv which indicates its atomic type, the
type, and generation order of the graph piece it is in. Each edge has a feature vector indicating
its bond type. Modern GNNs follow a neighborhood aggregation strategy (Xu et al., 2018), which
iteratively update the representations of nodes with AGGREGATE and COMBINE operations.
The representation of a node v in the k-th iteration is calculated as follows:
a(vk) = AGGREGATE(k)({h(uk-1) : u ∈ N (v)}),	(1)
h(vk) = COMBINE(k) (h(vk-1) , a(vk)),	(2)
where AGGREGATE and COMBINE vary in different GNNs. N (v) denotes neighboring nodes
of v. We set h(v0) = xv and use GIN with edges feature (Hu et al., 2019) as the backbone GNN
network, which implements AGGREGATE and COMBINE as follows:
a(vk) = X ReLU(h(uk-1) +euv),	(3)
u∈N (v)
h(vk) = hΘ((1 + ε)h(vk-1) + a(vk)),	(4)
where hΘ is a neural network and ε is a constant. We implement hΘ as a 2-layer multilayer per-
ceptron (Gardner & Dorling, 1998, MLP) with ReLU activation and set ε = 0. Since the k-th
representation captures k-hop feature of nodes (Xu et al., 2018), we obtain the final representations
of nodes as hv = [h(v1) , . . . , h(vt)] so that it contains 1-hop to t-hop contextual information. Then
we compute the representation of the graph G through summation hG = Pv∈V hv . We use hG
to obtain the mean μg and log variance σg of variational posterior approximation q(z∣G) through
two separate linear layers and use the reparameterization trick (Kingma & Welling, 2013) to sample
from the distribution in the training process.
3.3	Two-step Decoder
x-r r∙ . 1	1	, 1	! C ZΓ> ~1 Γ ~1 ∖	Fl	,	C ZΓ> ~1	FrG-I ♦ ,
With a molecule G represented as h{Pi}, {Eij}i, our model generates {Pi} and {Eij} in two con-
secutive phases. The two phases move from coarse to fine granularity.
Piece-level Sequence Generation Given a latent variable z, our model first uses an autoregressive
sequence generation model P(Pi|P<i, z) to decode a sequence of graph pieces [P1, . . . , Pn]2. Dur-
ing training, we insert two special tokens “<bos>” and “<eos>” into the begin and the end of the
graph piece sequence to indicate the begin and the end, respectively. During testing, the generation
stops when a “<eos>” is generated. The sequence model can be RNNs, such as LSTM (Hochreiter
& Schmidhuber, 1997) and GRU (Cho et al., 2014). In this work, we use a single layer of GRU
and project the latent variable z to the initial state of GRU. The training objective of this stage is to
minimize the log-likelihood LP of the ground truth graph piece sequence:
n
LP = X-logP(Pi|P<i,z).	(5)
_______________________________ i=1
2The ordering of graph pieces is not unique. We train with one order and leave this problem for future work.
4
Under review as a conference paper at ICLR 2022
Figure 4: Overview of the graph piece variational autoencoder. (a) Piece-level decomposition.
Atoms and bonds which belong to different graph pieces are highlighted in different colors. (b)
Molecular graph. We inject piece-level information into the molecular graph through atom features.
(c) Latent space encoding. We obtain the latent variable Z through the graph encoder (Section
3.2). (d) Piece-level sequence generation. A sequence of graph pieces is auto-regressively decoded
from the latent variable by a GRU (Section 3.3). (e) Incomplete molecular graph. The generated
graph pieces form an incomplete molecular graph where inter-piece bonds are absent. (f) Bond
completion. Completion of inter-piece bonds is formalized as a link prediction task for a GNN
(Section 3.3). After training, We can directly sample from the latent space to generate molecules.
τr* FC	-c-r τ	. r 1	,	∙	1	-r-1	, 1	1 ∙ ,	,	Z~ι∙‰rτ
Bond Completion We generate {Ej } non-autoregressively. For the architecture, We use a GNN
with the same structure as the graph encoder in Section 3.2 but with different parameters to obtain
the representations h of each atom V in the graph pieces. Given nodes V and u, we predict their
connections as follows:
P(euv |z) = Hθ([hv； hu； z]),	(6)
where H is a neural network. In this work we adopt a 3-layer MLP with ReLU activation. Apart
from the types of chemical bonds, we also add a special type “<none>" to the edge vocabulary
which indicates there is no connection between two nodes. During training, we predict both P(euv)
and P(evu) to let H learn the undirected nature of chemical bonds. We use negative sampling
(Goldberg & Levy, 2014) to balance the ratio of “<none>" and chemical bonds. Since only about
2% pairs of nodes has inter-piece connections, negative sampling significantly improves the com-
putational efficiency and scalability. The training objective of this stage is to minimize the log-
likelihood LE of the ground truth inter-piece connections:
LE =	X — log P(euv |z).	(7)
u∈Pi ,v∈Pj ,i6=j
The reconstruction loss of our GP-VAE model is:
Lrec = LS + Ls.	(8)
To decode {Eij} in inference time, the decoder first assigns all possible inter-piece connections with
a bond type and a corresponding confidence level. We try to add bonds that have a confidence level
higher than δth = 0.5 to the molecule in order of confidence level from high to low. For each attempt,
we perform a valency check and a cycle check to reject the connections that will cause violation of
valency or form unstable rings which are too small or too large. Since this procedure may form
unconnected graphs, we find the maximal connected component as the result of the generation. We
present the pseudo code for inference in Appendix C.
We jointly train a 2-layer MLP from z to predict the scores of target properties using the MSE loss.
Denote the loss of the predictor as Lprop , the loss function of our GP-VAE is:
L = αLrec + (1 - α)Lprop +βDKL,	(9)
where α and β balance the reconstruction loss, the property loss, and the KL divergence DKL be-
tween the distribution of z and the prior distribution N(0, I).
5
Under review as a conference paper at ICLR 2022
4	Experiments
4.1	Setup
Evaluation Tasks We first report the empirical results for the distribution-learning tasks of the
GuacaMol benchmark (Brown et al., 2019) to evaluate the ability of the model to generate realistic
and diverse molecules. Then we validate our model on two goal-directed tasks. Property Optimiza-
tion requires generating molecules with optimized properties. Constrained Property Optimization
concentrates on improving the properties of given molecules with a restricted degree of modification.
Dataset We use the ZINC250K (Irwin et al., 2012) dataset for training, which contains 250,000
drug-like molecules up to 38 atoms. For GuacaMol benchmark, we also add results on the QM9
(Blum & Reymond, 2009; Rupp et al., 2012) dataset, which has 7,165 molecules up to 23 atoms.
Baselines We compare our graph piece variational autoencoder (GP-VAE) with the following
state-of-the-art models. JT-VAE (Jin et al., 2018) is a variational autoencoder that represents
molecules as junction trees. It performs Bayesian optimization on the latent variable for property
optimization. GCPN (You et al., 2018) combines reinforcement learning and graph representation
for goal-directed molecular graph generation. MRNN (Popova et al., 2019) adopts two RNNs to
autoregressively generate atoms and bonds respectively. It combines policy gradient optimization to
generate molecules with desired properties. GraphAF (Shi et al., 2020) is a flow-based autoregres-
sive model which is first pretrained for likelihood modeling and then fine-tuned with reinforcement
learning for property optimization.GA (Nigam et al., 2020) adopts genetic algorithms for property
optimization and model the selection of the subsequent population with a neural network.
Implementation Details We choose N = 300 for property optimization and N = 500 for con-
strained property optimization. GP-VAE is trained for6 epochs with a batch size of32 and a learning
rate of 0.001. We set α = 0.1 and initialize β = 0. We adopt a warm-up method that increases β by
0.002 every 1000 steps to a maximum of 0.01. More details can be found in Appendix D.
4.2	Results
GuacaMol Distribution-Learning Benchmarks The distribution-learning benchmarks incorpo-
rate five metrics on 10,000 molecules generated by the models. Validity measures whether the gen-
erated molecules are chemically valid. Uniqueness penalizes models when they generate the same
molecule multiple times. Novelty assesses the ability of the models to generate molecules that are
not present in the training set. KL Divergence measures the closeness of the probability distributions
of a variety of physicochemical descriptors for the training set and the generated molecules. Frechet
ChemNet Distance (FCD) calculates the closeness of the two sets of molecules with respect to their
hidden representations in the ChemNet trained for predicting biological activities. Each metric is
normalized to 0 to 1, and a higher value indicates better performance. Table 1 shows the results of
distribution-learning benchmarks on QM9 and ZINC250K. Our model achieves competitive results
in all five metrics, which indicates our model can generate realistic molecules and does not overfit
the training set. We present some molecules sampled from the prior distribution in Appendix I.
Table 1: Results of GuacaMol distribution-learning benchmarks on QM9 and ZINC250K. KL Div
refers to KL Divergence and FCD refers to FreChet ChemNet Distance.
Model	Validity(↑)	Uniqueness(↑)	Novelty(↑)	KL Div(↑)	FCD(↑)
QM9					
GraphAF	1.0	0.500	0.453	0.761	0.326
GA	1.0	0.008	0.008	0.429	0.004
GP-VAE(ours)	1.0	0.673	0.523	0.921	0.659
ZINC250K					
GraphAF	1.0	0.288	0.287	0.508	0.023
GA	1.0	0.008	0.008	0.705	0.001
GP-VAE(ours)	1.0	0.997	0.997	0.850	0.318
6
Under review as a conference paper at ICLR 2022
Property Optimization This task focuses on generating molecules with optimized Penalized logP
(Kusner et al., 2017) and QED (Bickerton et al., 2012). Penalized logP is logP penalized by synthesis
accessibility and ring size which has an unbounded range. QED measures the drug-likeness of
molecules with a range of [0, 1]. Both properties are calculated by empirical prediction models
(Wildman & Crippen, 1999; Bickerton et al., 2012), and are widely used in previous works (Jin
et al., 2018; You et al., 2018; Shi et al., 2020). We adopt the scripts of Shi et al. (2020) to calculate
property scores so that the results are comparable. We directly perform gradient ascending on the
latent variable (Jin et al., 2018; Luo et al., 2018). Hyperparameters can be found in Appendix D.
Following previous works (Jin et al., 2018; You et al., 2018; Shi et al., 2020), we generate 10,000
optimized molecules from the latent space and report the top-3 scores found by each model. Results
in Table 2 show that our model surpasses the state-of-the-art models consistently.
Table 2: Comparison of the top-3 property scores found by each model
Method	Penalized logP ↑				QED ↑			
	1st	2nd	3rd	Validity	1st	2nd	3rd	Validity
ZINC250K	4.52	4.30	4.23	100.0%	0.948	0.948	0.948	100.0%
JT-VAE	5.30	4.93	4.49	100.0%	0.925	0.911	0.910	100.0%
GCPN	7.98	7.85	7.80	100.0%	0.948	0.947	0.946	100.0%
MRNN	8.63	6.08	4.73	100.0%	0.844	0.796	0.736	100.0%
GraphAF	12.23	11.29	11.05	100.0%	0.948	0.948	0.947	100.0%
GA3	12.25	12.22	12.20	100.0%	0.946	0.944	0.932	100.0%
GP-VAE	13.95	13.83	13.65	100.0%	0.948	0.948	0.948	100.0%
Constrained Property Optimization This task concentrates on improving the property scores of
given molecules with the constraint that the similarity between the original molecule and the modi-
fied molecule is above a threshold δ. Following Jin et al. (2018); You et al. (2018); Shi et al. (2020),
we choose 800 molecules with lowest Penalized logP in the test set of ZINC250K for optimization
and Tanimoto similarity with Morgan fingerprint (Rogers & Hahn, 2010) as the similarity metric.
Similar to the property optimization task, we perform gradient ascending on the latent variable
with a max step of 80. We collect all latent variables which have better-predicted scores than the
previous iteration and decode each of them 5 times, namely up to 400 molecules. Following Shi
et al. (2020), we initialize the generation with sub-graphs sampled from the original molecules.
Then we choose the one with the highest property score from the molecules that meet the similarity
constraint. Table 3 shows our model can generate molecules with a higher Penalized logP score
while satisfying the similarity constraint. Since our model uses graph pieces as building blocks, the
degree of modification tends to be greater than atom-level models, leading to a lower success rate.
Nevertheless, our model still manages to achieve high success rates close to atom-level models.
Table 3: Comparison of the mean and standard deviation of improvement of each model on con-
strained property optimization.
Model	δ=0.2		δ = 0.4		δ=0.6	
	Improvement	Success	Improvement	Success	Improvement	Success
JT-VAE	1.68±1.85	97.1%	0.84±1.45	83.6%	0.21±0.71	46.4%
GCPN	4.12±1.19	100%	2.49±1.30	100%	0.79±0.63	100%
GraphAF4	4.99±1.38	100%	3.74±1.25	100%	1.95±0.99	98.4%
GA	3.04±1.60	100%	2.34±1.34	100%	1.35±1.06	95.9%
GP-VAE	6.42±1.86	99.9%	4.19±1.30~~	98.9%	2.52±1.12	90.3%
3Results are obtained by running the scripts from the original paper and use the scripts of Shi et al. (2020)
to evaluate property scores.
4We rerun GraphAF on the same 800 molecules using its script to obtain the results since the original paper
chooses a different set of 800 molecules whose results are not comparable.
7
Under review as a conference paper at ICLR 2022
4.3	Runtime Cost
We train JT-VAE, GraphAF and our GP-VAE on a machine with 1 NVIDIA GeForce RTX 2080Ti
GPU and 32 CPU cores and use them to generate 10,000 molecules to compare their efficiency of
training and inference. As shown in Table 4, our model achieves significant improvements on the ef-
ficiency of training and inference due to graph pieces and the two-step generation. With graph pieces
as building blocks, the number of steps required to generate a molecules is significantly decreased
compared to the atom-level models like GraphAF. Moreover, since the two-step generation approach
separates the generation of graph pieces and the connections between them into two stage and for-
malizes the bond completion stage as a link prediction task, it avoids the complex enumeration of
possible combinations adopted by JT-VAE. Therefore, our model achieves tremendous improvement
in computational efficiency over both atom-level and substructure-level baselines.
Table 4: Runtime cost for JT-VAE, GraphAF and our GP-VAE on the ZINC250K dataset. Inference
time is measured with the generation of 10,000 molecules. Avg Step denotes the average number of
steps each model requires to generate a molecule.
Model	Training	Inference	Avg Step
JT-VAE	24 hours	20 hours	15.50
GraphAF	7 hours	10 hours	56.88
GP-VAE (ours)	1.2 hours	0.3 hour	6.84
4.4	Ablation Study
We conduct an ablation study to further validate the effects of graph pieces and the two-step genera-
tion approach. We first downgrade the vocabulary to contain only single atoms. Then we replace the
two-step decoder with a fully auto-regressive decoder. We present the performance on property op-
timization and constrained property optimization after the modification in Table 5 and Table 6. The
direct introduction of the two-step generation approach leads to improvement on property optimiza-
tion but harms the performance on constrained property optimization. This is because separating
the generation of atoms and bonds brings massive loss of bond information for the atom generation
process. However, the adoption of graph pieces as building blocks alleviates this negative effect
since the graph pieces themselves contain abundant bond information. Therefore, while the two-
step generation approach enhances computational efficiency, the state-of-the-art performance of our
model is mainly credited to the use of graph pieces.
Table 5: The top-3 property scores found by GP-VAE without certain modules.
Method	Penalized logP				QED			
	1st	2nd	3rd	Validity	1st	2nd	3rd	Validity
GP-VAE	13.95	13.83	13.65	100.0%	0.948	0.948	0.948	100.0%
- piece	6.91	5.50	5.12	100.0%	0.870	0.869	0.869	100.0%
- two-step	3.54	3.54	3.22	100.0%	0.737	0.734	0.729	100.0%
Table 6: Comparison of GP-VAE without certain modules on constrained property optimization
Model	δ=0.2		δ = 0.4		δ= 0.6	
	Improvement	Success	Improvement	Success	Improvement	Success
GP-VAE	6.42±1.86	99.9%	4.19±1.30~~	98.9%	2.52±1.12	90.3%
- piece	2.33±1.46	74.8%	2.12±1.36	50.9%	1.87±1.12	27.0%
- two-step	3.36±1.58	98.6%	2.72±1.24	82.0%	1.88±1.05	45.1%
8
Under review as a conference paper at ICLR 2022
5	Analysis
5.1	Graph Piece Statistics
We compare the statistical characteristics of the vocabulary of JT-VAE that contains 780 substruc-
tures and the vocabulary of graph pieces with a size of 100, 300, 500, and 700. Figure 5 shows the
proportion of substructures with different numbers of atoms in the vocabulary and their frequencies
of occurrence in the ZINC250K dataset. The substructures in the vocabulary of JT-VAE mainly
concentrate on 5 to 8 atoms with a sharp distribution. However, starting from substructures with 3
atoms, the frequency of occurrence is already close to zero. Therefore, the majority of substructures
in the vocabulary of JT-VAE are actually not common substructures. On the contrary, the substruc-
tures in the vocabulary of graph pieces have a relatively smooth distribution over 4 to 10 atoms.
Moreover, these substructures also have a much higher frequency of occurrence compared to those
in the vocabulary of JT-VAE. We present samples of graph pieces in Appendix H.
O 2 4 6 8 IO 12 14 16 18 20 22 24 26
Number of atoms
0 2 4 6 8 10 12 14 16 18 20 22 24 26
Numberof atoms
Figure 5: The left and right figures show the proportion of and frequency of occurrence of substruc-
tures with different number of atoms in the vocabulary, respectively.
5.2	Proper Granularity
A larger N in the graph piece extraction process leads to an increase in the number of atoms in graph
pieces and a decrease in their frequency of occurrence, as illustrated in Figure 6. These two factors
affect model performance in opposite ways. On the one hand, the entropy of the dataset decreases
with more coarse-grained decomposition (Martin & England, 2011), which benefits model learning
(Bentz & Alikaniotis, 2016). On the other hand, the sparsity problem worsens as the frequency of
graph pieces decreases, which hurts model learning (Allison et al., 2006). We propose a quantified
method to balance entropy and sparsity. The entropy of the dataset given a set of graph pieces S is
defined by the sum of the entropy of each graph piece normalized by the average number of atoms:
HS = - -1 X P(P )iog P(P),	(10)
nS P∈S
where P(P) is the relative frequency of graph piece P in the dataset and nS is the average number
of atoms of graph pieces in S . The sparsity of S is defined as the reciprocal of the average frequency
of graph pieces normalized by the size of the dataset:
SS
M
(11)
where M is the number of molecules in the dataset and fS is the average frequency of occurrence
of graph pieces in the dataset. Then the entropy - sparsity trade-off (T) can be expressed as:
TS = HS + γSS
(12)
where γ balances the impacts of entropy and sparsity since the impacts vary across different tasks.
We assume that TS negatively correlates with downstream tasks. Given a task, we first sample
several values of N to calculate their values of T and then compute the γ that minimize the Pearson
correlation coefficient between T and the corresponding performance on the task. With the proper γ,
Pearson correlation coefficients for the three downstream tasks in this paper are -0.987, -0.999, and
9
Under review as a conference paper at ICLR 2022
-0.707, indicating strong negative correlations. For example, Figure 6 shows the curve of entropy -
sparsity trade-off with a maximum of 3,000 iteration steps for the property optimization task. From
the curve, We choose N = 300 for the property optimization task.
Figure 6: Entropy - Sparsity trade-off, average number of atoms in graph pieces and average fre-
quency of occurrence of graph pieces with a maximum of 3,000 iteration steps.
5.3	Graph Piece - Property Correlation
To analyze the graph piece-property correlation and whether our model can discover and utilize the
correlation, we present the normalized distribution of generated graph pieces and Pearson correlation
coefficient between the graph pieces and the score of Penalized logP (PlogP) in Figure 7.
The curve of the Pearson correlation coeffcient indicates that some graph pieces positively correlate
with PlogP and some negatively correlate with it. Compared with the flat distribution under the non-
optimization setting, the generated distribution shifts towards the graph pieces positively correlated
with PlogP under the PlogP-optimization setting. The generation of graph pieces negatively corre-
lated with PlogP is also suppressed. Therefore, correlations exist between graph pieces and PlogP,
and our model can accurately discover and utilize these correlations for PlogP optimization.
Figure 7: The distributions of generated graph pieces with and without optimization of PlogP, as well
as Pearson correlation coefficient between the graph pieces and the score of PlogP. PlogP refers to
Penalized logP. The distributions are normalized by the distribution of the training set, which means
the frequency of occurrence of a graph piece is divided by its count of occurrence in the training set.
6	Conclusion
We propose an algorithm to automatically discover the regularity in molecules and extract them
as graph pieces. We also propose a graph piece variational autoencoder utilizing the graph pieces
and generate molecules in two phases. Our model consistently outperforms state-of-the-art models
on distribution-learning, property optimization and constrained property optimization with higher
computational efficiency.Our work provides insights into the selection of granularity on molecular
graph generation and can inspire future search in this direction.
10
Under review as a conference paper at ICLR 2022
References
Sungsoo Ahn, Junsu Kim, Hankook Lee, and Jinwoo Shin. Guiding deep molecular optimization
with genetic exploration. In H. Larochelle, M. Ranzato, R. Hadsell, M. F. Balcan, and H. Lin
(eds.), Advances in Neural Information Processing Systems, volume 33, pp. 12008-12021. CUr-
ran Associates, Inc., 2020. URL https://proceedings.neurips.cc/paper/2020/
file/8ba6c657b03fc7c8dd4dff8e45defcd2-Paper.pdf.
Ben Allison, David Guthrie, and Louise Guthrie. Another look at the data sparsity problem. In
International Conference on Text, Speech and Dialogue, pp. 327-334. Springer, 2006.
Christian Bentz and Dimitrios Alikaniotis. The word entropy of natural languages. arXiv preprint
arXiv:1606.06996, 2016.
G Richard Bickerton, Gaia V Paolini, Jeremy Besnard, Sorel Muresan, and Andrew L Hopkins.
Quantifying the chemical beauty of drugs. Nature chemistry, 4(2):90-98, 2012.
Esben Jannik Bjerrum and Richard Threlfall. Molecular generation with recurrent neural networks
(rnns). arXiv preprint arXiv:1705.04612, 2017.
L.	C. Blum and J.-L. Reymond. 970 million druglike small molecules for virtual screening in the
chemical universe database GDB-13. J. Am. Chem. Soc., 131:8732, 2009.
Nathan Brown, Marco Fiscato, Marwin HS Segler, and Alain C Vaucher. Guacamol: benchmarking
models for de novo molecular design. Journal of chemical information and modeling, 59(3):
1096-1108, 2019.
Kyunghyun Cho, Bart Van Merrienboer, Caglar Gulcehre, Dzmitry Bahdanau, Fethi Bougares, Hol-
ger Schwenk, and Yoshua Bengio. Learning phrase representations using rnn encoder-decoder
for statistical machine translation. arXiv preprint arXiv:1406.1078, 2014.
Nicola De Cao and Thomas Kipf. Molgan: An implicit generative model for small molecular graphs.
arXiv preprint arXiv:1805.11973, 2018.
Philip Gage. A new algorithm for data compression. C Users Journal, 12(2):23-38, 1994.
Matt W Gardner and SR Dorling. Artificial neural networks (the multilayer perceptron)—a review
of applications in the atmospheric sciences. Atmospheric environment, 32(14-15):2627-2636,
1998.
Yoav Goldberg and Omer Levy. word2vec explained: deriving mikolov et al.’s negative-sampling
word-embedding method. arXiv preprint arXiv:1402.3722, 2014.
Rafael Gomez-Bombarelli, Jennifer N Wei, David Duvenaud, Jose Miguel Hernandez-Lobato,
Benjamin Sanchez-Lengeling, Dennis Sheberla, Jorge Aguilera-Iparraguirre, Timothy D Hirzel,
Ryan P Adams, and Alan Aspuru-Guzik. Automatic chemical design using a data-driven contin-
uous representation of molecules. ACS central science, 4(2):268-276, 2018.
Sepp Hochreiter and JUrgen Schmidhuber. Long short-term memory. Neural computation, 9(8):
1735-1780, 1997.
Weihua Hu, Bowen Liu, Joseph Gomes, Marinka Zitnik, Percy Liang, Vijay Pande, and Jure
Leskovec. Strategies for pre-training graph neural networks. arXiv preprint arXiv:1905.12265,
2019.
Akihiro Inokuchi, Takashi Washio, and Hiroshi Motoda. An apriori-based algorithm for mining
frequent substructures from graph data. In European conference on principles of data mining and
knowledge discovery, pp. 13-23. Springer, 2000.
John J Irwin, Teague Sterling, Michael M Mysinger, Erin S Bolstad, and Ryan G Coleman. Zinc:
a free tool to discover chemistry for biology. Journal of chemical information and modeling, 52
(7):1757-1768, 2012.
Ali Jazayeri and Chris Yang. Frequent subgraph mining algorithms in static and temporal graph-
transaction settings: A survey. IEEE Transactions on Big Data, 2021.
11
Under review as a conference paper at ICLR 2022
Wengong Jin, Regina Barzilay, and Tommi Jaakkola. Junction tree variational autoencoder for
molecular graph generation. In International Conference on Machine Learning, pp. 2323-2332.
PMLR, 2018.
Wengong Jin, Regina Barzilay, and Tommi Jaakkola. Hierarchical generation of molecular graphs
using structural motifs. In International Conference on Machine Learning, pp. 4839-4848.
PMLR, 2020a.
Wengong Jin, Regina Barzilay, and Tommi Jaakkola. Multi-objective molecule generation using
interpretable substructures. In International Conference on Machine Learning, pp. 4849-4859.
PMLR, 2020b.
Diederik P Kingma and Max Welling. Auto-encoding variational bayes. arXiv preprint
arXiv:1312.6114, 2013.
Matt J Kusner, Brooks Paige, and Jose MigUel Hernandez-Lobato. Grammar variational autoen-
coder. In International Conference on Machine Learning, pp. 1945-1954. PMLR, 2017.
Youngchun Kwon, Jiho Yoo, Youn-Suk Choi, Won-Joon Son, Dongseon Lee, and Seokho Kang.
Efficient learning of non-autoregressive graph variational autoencoders for molecular graph gen-
eration. Journal of Cheminformatics, 11(1):1-10, 2019.
Yibo Li, Liangren Zhang, and Zhenming Liu. Multi-objective de novo drug design with conditional
graph generative model. Journal of cheminformatics, 10(1):1-24, 2018a.
Yujia Li, Oriol Vinyals, Chris Dyer, Razvan Pascanu, and Peter Battaglia. Learning deep generative
models of graphs. arXiv preprint arXiv:1803.03324, 2018b.
Renqian Luo, Fei Tian, Tao Qin, Enhong Chen, and Tie-Yan Liu. Neural architecture optimization.
arXiv preprint arXiv:1808.07233, 2018.
Nathaniel FG Martin and James W England. Mathematical theory of entropy. Number 12. Cam-
bridge university press, 2011.
Christopher W Murray and David C Rees. The rise of fragment-based drug discovery. Nature
chemistry, 1(3):187-192, 2009.
AkshatKumar Nigam, Pascal Friederich, Mario Krenn, and Alan Aspuru-Guzik. Augmenting ge-
netic algorithms with deep neural networks for exploring the chemical space. In 8th International
Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020.
OpenReview.net, 2020. URL https://openreview.net/forum?id=H1lmyRNFvr.
Siegfried Nijssen and Joost N Kok. A quickstart in frequent structure mining can make a difference.
In Proceedings of the tenth ACM SIGKDD international conference on Knowledge discovery and
data mining, pp. 647-652, 2004.
Mariya Popova, Mykhailo Shvets, Junier Oliva, and Olexandr Isayev. Molecularrnn: Generating
realistic molecular graphs with optimized properties. arXiv preprint arXiv:1905.13372, 2019.
David Rogers and Mathew Hahn. Extended-connectivity fingerprints. Journal of chemical informa-
tion and modeling, 50(5):742-754, 2010.
M.	Rupp, A. Tkatchenko, K.-R. Muller, and O. A. von Lilienfeld. Fast and accurate modeling of
molecular atomization energies with machine learning. Physical Review Letters, 108:058301,
2012.
Franco Scarselli, Marco Gori, Ah Chung Tsoi, Markus Hagenbuchner, and Gabriele Monfardini.
The graph neural network model. IEEE transactions on neural networks, 20(1):61-80, 2008.
Rico Sennrich, Barry Haddow, and Alexandra Birch. Neural machine translation of rare words with
subword units. arXiv preprint arXiv:1508.07909, 2015.
Chence Shi, Minkai Xu, Zhaocheng Zhu, Weinan Zhang, Ming Zhang, and Jian Tang.
Graphaf: a flow-based autoregressive model for molecular graph generation. arXiv preprint
arXiv:2001.09382, 2020.
12
Under review as a conference paper at ICLR 2022
David Weininger. Smiles, a chemical language and information system. 1. introduction to method-
ology and encoding rules. Journal ofchemical information and computer sciences, 28(1):31-36,
1988.
Scott A Wildman and Gordon M Crippen. Prediction of physicochemical parameters by atomic
contributions. Journal of chemical information and computer sciences, 39(5):868-873, 1999.
Keyulu Xu, Weihua Hu, Jure Leskovec, and Stefanie Jegelka. How powerful are graph neural
networks? arXiv preprint arXiv:1810.00826, 2018.
Xifeng Yan and Jiawei Han. gspan: Graph-based substructure pattern mining. In 2002 IEEE Inter-
national Conference on Data Mining, 2002. Proceedings., pp. 721-724. IEEE, 2002.
Jiaxuan You, Bowen Liu, Rex Ying, Vijay Pande, and Jure Leskovec. Graph convolutional policy
network for goal-directed molecular graph generation. arXiv preprint arXiv:1806.02473, 2018.
A Piece-Level Decomposition Algorithm
Algorithm 2 presents the pseudo code for the piece-level decomposition of molecules. The algo-
rithm takes the atom-level molecular graph, the vocabulary of graph pieces and their frequencies
of occurrence recorded during the graph piece extraction process as input. Then the algorithm it-
eratively merge the graph piece pair which has the highest recorded frequency of occurrence in the
vocabulary until all graph piece pairs are not in the vocabulary.
1
2
3
4
Algorithm 2: Piece-Level Decomposition
Input: A graph G that decomposed into atom-level pieces, the set S of learned graphs pieces,
and the counter F of learned graph pieces.
Result: A new representation G0 of G that consists of graph pieces in S .
begin
G0 JG
while True do
freq J -1; P J N one;
_ _ , _ _ ~ _, _
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19 end
_ _ , _ _ ~ _, _
forall hPi, Pj, Eiji in G do
_ ,	_ _	, , _	_ ∙r .、
P J Merge(hpi, Pj , Eiji)；
S — GraPhToSMILES(P0);
if s in V and F [s] > freq then
I freq - F[s];
I P - P0;
end
end
if freq == -1 then
break;
else
. Merge neighboring graph pieces into a new graph piece.
. Convert a graph to SMILES representation.
G0 - MergeSubGraph(G0,P);
end
end
. Update the graph representation.
B	Complexity Analysis
Graph Piece Extraction Since the number of graph piece pairs equals the number of inter-piece
connections in the piece-level graph, the complexity is O(NMe), where N is the predefined size of
vocabulary, M denotes the number of molecules in the dataset, and e denotes the maximal number
of inter-piece connections in a single molecule. The number of inter-piece connections decreases
rapidly in the first few iterations, therefore the time cost for each iteration decreases rapidly. It cost
6 hours to perform 500 iterations on 250,000 molecules in the ZINC250K dataset with 4 CPU cores.
Piece-Level Decomposition Given an arbitrary molecule, the worst case is that each iteration adds
one atom to one existing subgraph until the molecule is finally merged into a single graph piece. In
this case the algorithm runs for |V| iterations. Therefore, the complexity is O(|V|) where V includes
all the atoms in the molecule.
13
Under review as a conference paper at ICLR 2022
C Inference Algorithm for Bond Completion
Algorithm 3 shows the pseudo code of our inference algorithm. We first predict the bonds between
all possible pairs of atoms in which the two atoms are in different graph pieces and sort them by
the confidence level given by the model from high to low. Then for each bond with a confidence
level higher then the predefined threshold δth, which is 0.5 in our experiments, we add it into the
molecular graph if it passes the valence check and cycle check. The valence check ensures the given
bond will not violate valence rules. The cycle check ensures the given bond will not form unstable
rings with nodes less than 5 or more than 6.
1
2
3
4
5
6
7
8
9
10
11
12
13
Algorithm 3: Inference Algorithm for Bond Completion
Input: An incomplete molecular graph G composed of graph pieces where inter-piece bonds
are absent, the predicted bond type for all possible inter-piece connections B and the
map to their confidence level C, the threshold for confidence level δth
Result: A valid molecular graph G0
begin
G0 J G;
B J SortByConfidence(B, C);	. Sort the bonds by their confidence level from high to low.
forall buv in B do
ifC[buv] < δth then
I continue;	. Discard edges with confidence level lower than the threshold.
end
if Valence_check(buv) and Cycle-check(buv) then
I G0 - AddEdge(G0, buv);	. Add edges that pass valence and cycle check to G0
end
end
G0 — MaXConnectedComPonent(G0)；	. Find the maximal connected component in G0
end
D	Experiment Details
Model and Training Hyperparameters We present the choice of model parameters in Table 7
and training parameters in Table 8. We represent an atom with three features: atom embedding,
piece embedding and position embedding. Atom embedding is a trainable vector of size eatom for
each type of atoms. Similarly, piece embedding is a trainable vector of size epiece for each type of
pieces. Positions indicate the order of generation of pieces. We jointly train a 2-layer MLP from the
latent variable to predict property scores. The training loss is represented as L = α ∙ LreC + (1 一
α) ∙ LProP + β ∙ DKL where α balances the reconstruction loss and prediction loss. For β, We adopt
a warm-up method that increase it by βstage every fixed number of steps to a maximum of βmax. We
found a β higher than 0.01 often causes KL vanishing problem and greatly harm the performance.
Our model and the baselines are trained on the ZINC250K dataset with the same train / valid / test
split as in Kusner et al. (2017).
Table 7: Parameters in the graph piece variational autoencoder
Model Param	Description	Value
eatom	Dimension of embeddings of atoms.	50
Common	epiece	Dimension of embeddings of pieces.	100
epos	Dimension of embeddings of postions. The max position is set to be 50.	50
Encoder
dhdGdz
Dimension of the node representations hv	300
The final representaion of graphs are projected to dG .	400
Dimension of the latent variable.	56
Number of iterations of GIN.	4
Decoder	Hgru	Hidden size of GRU.	200
Predictor dp	Dimension of the hidden layer of MLP.	200
14
Under review as a conference paper at ICLR 2022
Table 8: Training hyperparameters
Param	Description	Value
lr	Learning rate	0.001
α	Weight for balancing reconstruction loss and predictor loss	0.1
βinit	Initial weight of KL Divergence	0
βmax	Max weight of KL Divergence	0.01
k lwarmup	The number of steps for one stage up in β	1000
βstage	Increase of β every stage	0.002
Property Optimization We use gradient ascending to search in the continuous space of latent
variable. For simplicity, we set a target score and optimize the mean square error between the score
given by the predictor and the target score just as in the training process. The optimization stops
if the mean square error does not drop for 3 iterations or it has been iterated to the maxstep. We
normalize the Penalized logP in the training set to [0, 1] according to the statistics of ZINC250K.
By setting a target value higher than 1 the model is supposed to find molecules with better property
than the molecules in the training set. To acquire the best performance, we perform a grid search
with lr ∈ {0.001, 0.01, 0.1, 1, 2}, maxstep ∈ {20, 40, 60, 80, 100} and target ∈ {1, 2, 3, 4}. For
optimization of QED, we choose lr = 0.01, maxstep = 100, target = 2. For optimization of
Penalized logP, we choose lr = 0.1, maxstep = 100, target = 2.
(c) Constrained optimization of Pe-
nalized logP
(a) Penalized logP optimization
(b) QED optimization
Figure 8:	Samples of property optimization and constrained property optimization. In (c) the first
and the second columns are the original and modified molecules labeled with their Penalized logP.
Constrained Property Optimization We use the same method as property optimization to opti-
mize the latent variable. We also perform a grid search with lr ∈ {0.1, 0.01} and target ∈ {2, 3}.
We select lr = 0.1, maxstep = 80 and target = 2. For decoding, we first initialize the generation
with a submol sampled from the original molecule by teacher forcing. We follow Shi et al. (2020)
to first sample a BFS order of all atoms and then randomly drop out the last m atoms with m up
to 5. We collect all latent variables which have better predicted scores than the previous iteration
and decode each of them 5 times, namely up to 400 molecules. Then we choose the one with the
highest property score from the molecules that meet the similarity constraint. For the baseline GA
(Ahn et al., 2020), we adjust the number of iterations to 5 and the size of population to 80, namely
traversing up to 400 molecules, for fair comparison.
E Data Efficiency
Since the graph pieces are common subgraphs in the molecular graphs, they should be relatively
stable with respect to the scale of training set. To validate this assumption, we choose subsets of
different ratios to the training set for training to observe the trend of the coverage of Top 100 graph
pieces in the vocabularies as well as the model performance on the average score of the distribution-
learning benchmarks. As illustrated in Figure 9, with a subset above 20% of the training set, the
constructed vocabulary covers more than 95% of the top 100 graph pieces in the full training set, as
well as the model performance on the distribution-learning benchmarks.
15
Under review as a conference paper at ICLR 2022
dataset
-ZINC250K
QM9
0.0	0.2	0.4	0.6	0.8	1.0
Subset Ratio
(a) Top 100 graph piece coverage
8 6 4 2
6 60.0.
3uueE」0JJ3d bu-Eeφ÷uoqnqESQ
dataset
-ZINC250K
QM9
0.0	0.2	0.4	0.6	0.8	1.0
Subset Ratio
(b) Distribution-learning benchmarks performance
Figure 9:	The coverage of top 100 graph pieces and the relative performance on the distribution-
learning benchmarks with respect to subsets of different ratios to the full training set.
F Fused Rings Generation
We conduct an additional experiment to validate the ability of GP-VAE to generate molecules with
fused rings (cycles with shared edges), because at first thought it seems difficult for GP-VAE to
handle these molecules due to the non-overlapping nature of graph pieces. We train atom-level
and piece-level GP-VAEs on all 4,431 structures consisting of fused rings from ZINC250K. Then
we sample 1,000 molecules from the latent space to calculate the proportion of molecules with
fused rings. The results are 94.5% and 97.2% for the atom-level model and the piece-level model,
respectively. The experiment demonstrates that the introduction of graph pieces as building blocks
will not hinder the generation of molecules with fused rings.
Figure 10: Decomposition of three molecules with fused rings (cycles that share edges).
G	Discussion
Universal Granularity Adaption The concept and extraction algorithm of graph pieces resemble
those of subword units (Sennrich et al., 2015) in machine translation. Though subword units are de-
signed for the out-of-vocabulary problem of machine translation, they also improve the translation
quality (Sennrich et al., 2015). In this work, we demonstrate the power of graph pieces and are curi-
ous about whether there is a universal way to adapt atom-level models into piece-level counterparts
to improve their generation quality. The key challenge is to find an efficient and expressive way to
encode inter-piece connections into feature vectors. We leave this for future work.
Searching in Continuous Space In recent years, reinforcement learning (RL) is becoming dom-
inant in the field of optimization of molecular properties (You et al., 2018; Shi et al., 2020). These
RL models usually suffer from reward sparsity when applied to multi-objective optimization (Jin
et al., 2020b). However, most scenarios that incorporate molecular property optimization have
multi-objective constraints (e.g.,drug discovery). In this work, we show that with graph pieces,
even simple searching method like gradient ascending can surpass RL methods on single-objective
optimization. It is possible that with better searching methods in continuous space our model can
achieve competitive results on multi-objective optimization.
16
Under review as a conference paper at ICLR 2022
H Graph Piece Samples
We present 50 graph pieces found by our extraction algorithm in Figure 11.
NC(=O)C1=CC=CC=C1 CI=CC=NC=CI
COcI=CC=CC=CI
CCCC(N)=O	C(SHK=O)=O
CCCeCI=CC=CC=CI
CCI=CC=CC(C)=CL	CCNC(N)=O
CO=CC=CN=Cl
Fcα=cc=cc=cι
Figure 11: 50 Samples of graph pieces from the vocabulary with 100 graph pieces in total. Each
graph piece is labeled with its SMILES representation.
17
Under review as a conference paper at ICLR 2022
I More Molecule Samples
We further present 50 molecules sampled from the prior distribution in Figure 12.
Figure 12: 50 molecules sampled from the prior distribution N(0,1)
18