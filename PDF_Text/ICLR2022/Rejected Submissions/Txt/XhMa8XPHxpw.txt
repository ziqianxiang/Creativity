Under review as a conference paper at ICLR 2022
Low-Precision Stochastic Gradient Langevin
Dynamics
Anonymous authors
Paper under double-blind review
Ab stract
Low-precision optimization is widely used to accelerate large-scale deep learn-
ing. Despite providing better uncertainty estimation and generalization, sampling
methods remain mostly unexplored in this space. In this paper, we provide the first
study of low-precision Stochastic Gradient Langevin Dynamics (SGLD), arguing
that it is particularly suited to low-bit arithmetic due to its intrinsic ability to han-
dle system noise. We prove the convergence of low-precision SGLD on strongly
log-concave distributions, showing that with full-precision gradient accumulators,
SGLD is more robust to quantization error than SGD; however, with low-precision
gradient accumulators, SGLD can diverge arbitrarily far from the target distribu-
tion with small stepsizes. To remedy this issue, we develop a new quantization
function that preserves the correct variance in each update step. We demonstrate
that the resulting low-precision SGLD algorithm is comparable to full-precision
SGLD and outperforms low-precision SGD on deep learning tasks.
1	Introduction
Low-precision optimization has become increasingly popular in reducing compute and memory
costs of training deep neural networks (DNNs). It uses fewer bits to represent numbers in model
parameters, activations and gradients and thus can drastically lower resource demands (Gupta et al.,
2015; Zhou et al., 2016; De Sa et al., 2017; Li et al., 2017). Prior work has shown that using 8-bit
numbers in training DNNs achieves about 4× latency speed ups and memory reduction compared
to 32-bit numbers on a variety of deep learning tasks (Sun et al., 2019; Yang et al., 2019; Wang
et al., 2018b; Banner et al., 2018). As datasets and architectures grow rapidly, performing low-
precision optimization enables training large-scale DNNs efficiently and enables many applications
on different hardware and platforms.
Despite the impressive progress in low-precision optimization, low-precision sampling remains
largely unexplored. However, we believe stochastic gradient Markov chain Monte Carlo (SGM-
CMC) methods (Welling & Teh, 2011; Chen et al., 2014; Ma et al., 2015) are particularly suited
for low-precision arithmetic because of their intrinsic robustness to system noise. In particular: (1)
SGMCMC explores weight space instead of converging to a single point, thus it should not require
precise weights or gradients; (2) SGMCMC even adds noise to the system to encourage exploration
and so is naturally more tolerant to quantization noise; (3) SGMCMC performs Bayesian model
averaging during testing using an ensemble of models, which enables coarse representations of in-
dividual models to be compensated by the overall model average (Zhu et al., 2019).
SGMCMC is particularly compelling in Bayesian deep learning due to its ability to characterize
complex and multimodal DNN posteriors, providing state-of-the-art generalization accuracy and
calibration (BNNs) (Zhang et al., 2020; Li et al., 2016; Gan et al., 2016). Moreover, low-precision
approaches are especially appealing in this setting, where at test time we must store samples from a
posterior over millions of parameters, and perform multiple forward passes through the correspond-
ing models, which incurs significant memory and computational expenses.
In this paper, we provide the first comprehensive study of low-precision Stochastic Gradient
Langevin Dynamics (SGLD) (Welling & Teh, 2011). We start by analyzing the theoretical con-
vergence of low-precision SGLD on strongly log-concave distributions, proving that SGLD with
full-precision gradient accumulators (SGLDLP-F) converges to the target distribution within a dis-
tance that is asymptotically smaller than the distance between the SGD estimation and the optimum.
1
Under review as a conference paper at ICLR 2022
Figure 1: Low-precision SGLD with varying stepsizes on a Gaussian distribution. Variance-
corrected SGLD with low-precision gradient accumulators (VC SGLDLP-L) and SGLD with
full-precision gradient accumulators (SGLDLP-F) converge to the true distribution whereas naive
SGLDLP-L diverges and the divergence increases as the stepsize decreases.
Surprisingly, we find that SGLD with low-precision gradient accumulators (SGLDLP-L) can di-
verge arbitrarily far away from the target distribution with small stepsizes. We solve this issue by
developing a new quantization function that preserves the correct variance in each update step. We
prove that when using this new quantization function, SGLD converges to the target distribution
within a bounded level of accuracy even with small stepsizes. We illustrate the behavior of different
low-precision SGLD variants in Figure 1.
Empirically we demonstrate low-precision SGLD on deep learning tasks, including CIFAR-10,
CIFAR-100, and ImageNet with a ResNet-18, and the IMDB dataset with a LSTM. We show that
our variance-corrected quantizer significantly improves the performance of low-precision SGLD.
Moreover, the improvement of SGLD over SGD is larger in low-precision than in full-precision,
demonstrating the promise of low-precision stochastic sampling.
2	Related Work
Most work on accelerating SGLD training has focused on distributed learning with synchronous
or asynchronous communication (Ahn et al., 2014; Chen et al., 2016; Li et al., 2019). Another
direction to reduce training costs is to shorten training time by accelerating the convergence using
either variance reduction techniques (Dubey et al., 2016; Baker et al., 2019) or a cyclical learning
rate schedule (Zhang et al., 2020). To speed up testing, distillation techniques are often used to
save both compute and memory by transferring the knowledge of an ensemble of models to a single
model (Korattikara et al., 2015; Wang et al., 2018a).
Low-precision computation has become one of the most common approaches to reduce latency and
memory consumption in deep learning and is widely supported on new emerging chips including
CPUs, GPUs and TPUs (Micikevicius et al., 2017; Krishnamoorthi, 2018; Esser et al., 2019). Two
main directions to improve low-precision training include developing new number formats (Sun
et al., 2019; 2020) and studying mixed precision schemes (Courbariaux et al., 2015; Zhou et al.,
2016; Banner et al., 2018). Recently, one line of work studies applying Bayesian framework to learn
a deterministic quantized neural network (Soudry et al., 2014; Cheng et al., 2015; Achterhold et al.,
2018; van Baalen et al., 2020; Meng et al., 2020).
Despite impressive progress of low-precision deep learning, few work takes advantage of it for
Bayesian neural networks (BNNs). Su et al. (2019) proposes a method to train binarized variational
BNNs and Cai et al. (2018) develops an efficient hardware for training low-precision variational
BNNs. The only work on low-precision MCMC known to us is Ferianc et al. (2021), which directly
applies post-training quantization in optimization (Jacob et al., 2018) to quantize BNNs trained by
2
Under review as a conference paper at ICLR 2022
Stochastic Gradient Hamiltonian Monte Carlo (Chen et al., 2014). In contrast, we study training
low-precision models by SGLD from scratch, which can reduce both training and testing costs.
3	Preliminaries
3.1	Stochastic Gradient Langevin Dynamics
In the Bayesian setting, given some dataset D, a model with parameters θ, and a prior p(θ), we are
interested in sampling from the posterior p(θ∣D) 8 exp(-U(θ)), where the energy function is
U(θ) = - X logp(χ∣θ) - logp(θ).
x∈D
When the dataset is large, the cost of computing a sum over the entire dataset is expensive. Stochastic
Gradient Langevin Dynamics (SGLD) reduces the cost by using a stochastic gradient estimation
VU, an unbiased estimator of VU usually based on a subset of the dataset D. Specifically, SGLD
updates the parameter θ in the (k + 1)-th step following the rule
θk+ι = θk - avU(θk) + √2αξk+1,
where α is the stepsize and ξ is a standard Gaussian noise. Compared to the SGD update, the
only difference is the additional Gaussian noise. This close connection makes SGLD convenient to
implement on existing deep learning tasks for which SGD is the typical learning algorithm.
3.2	Low-Precision Training
We study training a low-precision model by SGLD from scratch, to reduce both training and testing
costs. We use fixed point in our theoretical analysis and empirical demonstration, which is a common
number type for reducing DNNs costs (Gupta et al., 2015; Lin et al., 2016; Li et al., 2017; Yang
et al., 2019). We additionally use block floating point, another common number representation, for
empirical evaluations on deep learning tasks (Song et al., 2018).
In order to train a network using low-precision numbers, we need a quantization function Q to
convert a real-valued number into a rounded number. Such functions include deterministic rounding
and stochastic rounding. Suppose that we use W bits to represent numbers with F of those W
bits to represent the fractional part. Then the quantization gap ∆ = 2-F is the distance between
consecutive representable numbers. The lower and upper bounds of the representable numbers are
l = -2W -F -1 and u = 2W-F-1 - 2-F, respectively. The deterministic rounding quantizes a
number to its nearest representable neighbour:
Qd(θ) = sign(θ) ∙ clip (△ ∆ + 1 ,l,u),
where clip(x, l, u) = max(min(x, u), l). Stochastic rounding quantizes a number with probability:
Qs(θ)
clip (△[知，l,u), w∙p.「知一∆
clip (△「知，l, U), w.p. 1 - (「知-∆).
Qs is unbiased, which means E[Qs (θ)] = θ. It is more favorable than Qd in practice since it can
preserve gradient information when the gradient update is smaller than the quantization gap (Gupta
et al., 2015; Courbariaux et al., 2016). In what follows, we use Qw and △w to denote the weights’
quantizer and quantization gap, Qg and △g to denote gradients’ quantizer and quantization gap.
Please note that Qg means quantizing the error and gradient in each layer in backward propaga-
tion (Wu et al., 2018; Yang et al., 2019).
Besides the quantization function, we also need to decide what and where to quantize. There are two
common choices depending on whether we store an additional copy of full-precision weights. Full-
precision gradient accumulators use a full-precision weight buffer to accumulate gradient updates
and only quantize weights before computing gradients. SGD with full-precision gradient accumula-
tors (SGDLP-F) updates the weights as
θk+ι = θk - αQg (VU(Qw (θk))).
3
Under review as a conference paper at ICLR 2022
Gradient accumulators are frequently updated during training, therefore it will be ideal to also repre-
sent it in low-precision to further reduce the costs. The update of SGD with low-precision gradient
accumulators (SGDLP-L) is
Θk+1 = Qw (θk - αQg (vU(θk))).
Both full- and low-precision gradient accumulators have been widely used in low-precision training.
Low-precision gradient accumulators are cheaper and full-precision gradient accumulators generally
have better performance because of more precisely accumulating gradient updates (Courbariaux
et al., 2015; Li et al., 2017).
4	Low-Precision SGLD
In this section, we first study the convergence bound of low-precision SGLD with full-precision
gradient accumulators (SGLDLP-F) on strongly log-concave distributions and show that SGLDLP-F
converges to the target distribution within a distance that is asymptotically smaller than that between
the SGD estimation and the optimum. Next we analyze low-precision SGLD with low-precision
gradient accumulators (SGLDLP-L) under the same setup and prove that SGLDLP-L can diverge
arbitrarily far away from the target distribution with a small stepsize, which is typically required by
SGLD to reduce asymptotic bias. Finally, we solve this problem by developing a variance-corrected
quantization function and further prove that our SGLDLP-L converges with small stepsizes.
4.1	Full-Precision Gradient Accumulators
Due to the similarity between SGLD and SGD, applying low-precision to SGLD is straightfor-
ward. Similar to SGDLP-F, we can do low-precision SGLD with full-precision gradient accumula-
tors (SGLDLP-F) as the following.
θk+ι = θk — αQg (VU(Qw (θk))) + v⅛+ι.
(1)
We now prove that SGLDLP-F will converge to the target distribution given small stepsizes and
large number of iterations.
Our convergence analysis of low-precision SGLD is based on (Dalalyan & Karagulyan, 2019),
where the target distribution is assumed smooth and strongly log-concave. We additionally as-
sume the energy function has Lipschitz Hessian following recent work in low-precision optimiza-
tion (Yang et al., 2019). Specifically, the energy function U satisfies the following,
U(θ) — U(θ0) — VU(θ0)l(θ - θ0) ≥ (m/2) kθ - θ0k2 ,
kvU (θ) - vU (θ0)k2 ≤ M kθ - θ0k2 ,	∀θ, θ0 ∈ Rd
kV2U(θ) - V2U(θ0)k2 ≤ Ψkθ - θ0k2,
for some positive constants m, M and Ψ. We further assume the variance of the stochastic gradient
is bounded by E[∣∣VU(θ) - VU(θ)k2] ≤ κ2 for some constant κ. For simplicity, We consider
SGLD with a constant stepsize α. We measure the convergence of SGLD in terms of 2-Wasserstein
distance. We use stochastic rounding for both Weight and gradient quantizers as it is generally better
than deterministic rounding and has also been used in previous loW-precision theoretical analysis (Li
et al., 2017; Yang et al., 2019).
Theorem 1.	We run SGLDLP-F under the above assumptions and with a constant stepsize α ≤
2/(m + M). Let π be the target distribution, μκ be the distribution obtained after K iterations and
μo be the initial distribution, then
W2(μκ, ∏) ≤ (1 — αm)K W2(μ0, π) + 1.65(M∕m)(ad)1/2 + min
/ (∆2 + M 2∆W)αd + 4ακ2
+ v	4m	.
M ∆w √d
2m
4
Under review as a conference paper at ICLR 2022
There are three main takeways of this theorem: First, it shows that SGLDLP-F converges to the ac-
curacy floor min ɑ^w d, M∆m√d) which depends on the quantization gap ∆w given large enough
K and small enough α. Second, our convergence bound is O(∆2w) which is better than the distance
between the SGD estimation and the optimum O(∆w) (Yang et al., 2019) (see Appendix D for de-
tails). Third, if we further assume the energy function is quadratic, that is Ψ = 0, then SGLDLP-F
converges to the target distribution asymptotically. This is similar to SGDLP-F on quadratic function
which converges to the optimum asymptotically (Li et al., 2017).
4.2 Low-Precision Gradient Accumulators
To further reduce the costs, we apply low-precision gradient accumulators to SGLD which we denote
SGLDLP-L. Mimicking the update of SGDLP-L, the update rule of SGLDLP-L is
Θk+1 = Qw (θk - αQg (vU(θk)) + √2αξk+1) .	(2)
Surprisingly, while we can prove a convergence result for SGLDLP-L, our theory suggests that it
can diverge arbitrarily from the target distribution with small stepsizes.
Theorem 2.	We run SGLDLP-L under the same assumptions as in Theorem 1. Then
W2(μκ, ∏) ≤ (1 — αm)K W2(μ0, ∏) + 1.65(M∕m)(ad)1/2 + min
M ∆w √d
2m
(Sg + α-1 δW )d + 4ακ2	K	∆w √d
+ V	―4m--------------+((1 — αm) 十1)^
This theorem suggests that as the stepsize α decreases, W2 distance between the stainary distri-
bution of SGLDLP-L and the target distribution increases. When α decreases, either our bound
becomes loose or SGLDLP-L diverges from the target distribution. We empirically test SGLDLP-L
on a standard Gaussian distribution in Figure 1, showing that the reason is the later. We use 8-bit
fixed point and assign 2 of them to represent the integer part. We can see that SGLDLP-F always
converges to the target distribution with different stepsizes whereas SGLDLP-L diverges from the
target distribution and the divergence increases when the stepsize decreases.
One may choose a stepsize that minimizes the above W2 distance to avoid divergence, however,
getting this optimal stepsize is generally difficult as the constants are unknown in practice. More-
over, enabling a small stepsize in SGLD is desirable, since SGLD needs a small stepsize to reduce
asymptotic bias of the posterior approximation (Welling & Teh, 2011).
4.3 Variance-Corrected Quantization
To approach correcting the problems with naive SGLDLP-L, we first need to identify the source of
the issue. We start by showing that the reason is the variance of each dimension of θk+1 becomes
larger due to using stochastic rounding as the low-precision gradient accumulators. Specifically,
given the stochastic gradient vU, the update of full-precision SGLD can be viewed as sampling
from a Gaussian distribution for each dimension i
θk+ι,i 〜N (θk,i — αVU(θk)i, 2α) , for i = 1,…,d.
Using stochastic rounding as the weight quantizer and the gradient quantizer in SGLDLP-L gives us
E [θk+ι,i]	= E	[qs	(θk,i	— αQs	(vU(θk))	+ √Oξk+ι,i)]	=	θk,i	—	OVUek)i
which keeps the same mean of θk+1 as in full-precision. However the variance of θk+1,i is larger
than needed. If we ignore the variance from Qg and the stochastic gradient, since they are present
and have been shown to work well in SGLDLP-F, the variance of θk+1,i is
Var [θk+ι,i] = E 卜ar [qs (θ^ — αVU®) + √2Oξk+ι,i) "1.
[e [qs (θk,i — αVU(θk)i + √2αξk+1,i) ∣ξk+ι,iii
+ Var
∆2
χk+1,i + 2α∙
5
Under review as a conference paper at ICLR 2022
Algorithm 1 Variance-Corrected Low-Precision SGLD (VC SGLDLP-L).
given: Stepsize a, number of training iterations K, gradient quantizer Qg, deterministic rounding
Qd, stochastic rounding Qs and quantization gap of weights ∆w.
for k = 1 : K do
update θk+ι 一 Qvc (θk - αQg (vU(θk)) , 2α, ∆w,Qd, Qs)
end for
output: samples {θk}
where χk+1,i ∈ [0, 1] depends on the distance of θk+1,i to the discrete grid. Empirically, from
Figure 1, We can see that naive SGLDLP-L gives the correct mean estimate but the wrong variance
estimate, while both are estimated correctly in SGLD which adds the proper amount of noise in
each update. This validates our intuition that using stochastic rounding in low-precision gradient
accumulators adds more variance than needed leading to an inaccurate variance estimate.
To address this issue, we propose a new quantization function Qvc in Algorithm 2. Qvc always
guarantees the correct mean, E [θk+1,i] = θk,i - αvU (θk)i, and guarantees the correct variance
Var [θk+1,i] = 2α most of the time except when v < vs. However that case rarely happens in prac-
tice, because the stepsize has to be extremely small. The main idea of Qvc is to directly sample from
the discrete weight space instead of quantizing a real-valued Gaussian sample. It does so considering
two cases: when the Gaussian noise is larger than the largest possible stochastic rounding variance
△2/4, Qvc first adds a portion of the Gaussian noise and uses a sample from the weight grid to make
up the remaining; in the other situation, Qvc directly samples from the weight grid to achieve the
target variance. Although Qvc does not provide a sample following a Gaussian distribution (as it is
discrete), we show that this does not affect the performance in both theory and practice.
We now prove that SGLDLP-L using Qvc, denoting VC SGLDLP-L, converges to the target distri-
bution up to a certain accuracy level with small stepsizes.
Theorem 3.	We run VC SGLDLP-L as in Algorithm 1. Besides the same assumptions in Theorem 1,
Wefurtherassume the gradient is bounded E IQg(VU(θk))∣∣ ] ≤ G. Let vo = ∆W/4. Then
W2 (μκ, π) ≤ (1 — am)KW2 (μo, π) + 1.65(M∕m)(ad)1/2 + min ∖ -^, ʌʃ
mm
where A
5v0d,
+ Ja耳『T + ((1 - αm)K + 1) √A.
4m	αm
if 2α > v0
max (2∆wαG, 4αd), otherwise
This theorem shows that when the stepsize a → 0, VC SGLDLP-L converges to the target dis-
tribution UP to an error instead of diverging. Our bound is O(√∆W) which is equivalent to the
distance between SGD with low-precision gradient accumulators and the optimum (Li et al., 2017;
Yang et al., 2019). However, we show empirically that VC SGLDLP-L has better dependency on
the quantization gap than SGD. We leave the improvement of the theoretical bound for future work.
We empirically demonstrate VC SGLDLP-L on the standard Gaussian distribution under the same
setting as in the previous section in Figure 1. Regardless of the stepsize, VC SGLDLP-L converges
to the target distribution and approximates the target distribution as well as SGLDLP-F, showing
that preserving the correct variance is the key to ensure correct convergence.
5 Experiments
We demonstrate low-precision SGLD with full-precision gradient accumulators (SGLDLP-F) and
with variance-corrected low-precision gradient accumulators (VC SGLDLP-L) on a logistic regres-
sion and multilayer perceptron on MNIST dataset (Section 5.1), ResNet-18 on CIFAR datasets and
LSTM on IMDB dataset (Section 5.2), and ResNet-18 on Imagenet dataset (Section 5.3). We use
qtorch (Zhang et al., 2019) to simulate low-precision training on these experiments.
6
Under review as a conference paper at ICLR 2022
Algorithm 2 Variance-Corrected Quantization Function Qvc .
input: (μ, v, ∆, Qd, QS)	// Qvc returns a variable with mean μ and variance V
vo J ∆* 2∕4	// ∆2∕4 is the largest possible variance that stochastic rounding can cause
if v > v0 then
X J μ + √v - voξ, where ξ 〜N(0,Id)
r J x - Qd(x)
for all i do
∆ Wr) — WiQ
I δ,	w.p.	2∆
ci J -∆, w.p. ""ʃ^-jri'	// When V > v0,we add a portion of Gaussian noise and
(0, otherwise // sample from the weight grid to make UP the remaining
end for
θ J Qd(x) + sign(r)	c
else
r J μ - Qs(μ)
for all i do
VS J(1 — 粤)∙ r2 + ɪ ∙ (-r + sign(ri)∆)2
if V > Vs then
(δ,	w.p. V-∆s
ci J	-∆, w.p.v2∆2s	// When V ≤ vo, we sample from the weight grid
[0,	otherwise	// to achieve the target variance
θi J QS (μ)i + ci
else
θi J QS(μ)i
end if
end for
end if
clip θ if outside representable range
return θ
5.1 Logistic Regression and Multilayer Perceptron
We first empirically verify our theorems on a logistic regression on MNIST dataset. We use
N(0, 1∕6) as the prior distribution following (Yang et al., 2019) and the resulting posterior dis-
tribution satisfies the assumptions in Section 4. We use fixed point numbers with 2 integer bits and
vary the number of fractional bits which is corresponding to varying the quantization gap ∆.
From Figure 2a, SGLDLP-F is more robust to the decay of bits than SGDLP-F since SGLDLP-
F outperforms SGDLP-F on all number of bits and recovers the full-precision result with 6 bits
whereas SGDLP-F needs 10 bits. This verifies Theorem 1 that SGLDLP-F converges to the target
distribution up to an error and is more robust to the quantiazation gap than SGDLP-F. With low-
precision gradient accumulators, we can see that VC SGLDLP-L is significantly better than naive
SGLDLP-L, indicating the effectiveness of using the variance-corrected quantization function for
quantizing gradient accumulators. These results verify Theorem 2 and Theorem 3. Besides, VC
SGLDLP-L outperforms SGDLP on all number of bits and even outperforms SGDLP-F when using
2 fractional bits. VC SGLDLP-L matches full-precision SGLD results with 6 bits whereas SGDLP-
L needs 8 or 10 bits. These observations demonstrate that SGLD is still more favorable than SGD
when using low-precision gradient accumulators.
To test whether these results apply to non-log-concave distributions, we replace the logistic regres-
sion model by a two-layer multilayer perceptron (MLP). The MLP has 100 hidden units and RELU
nonlinearilities. From Figure 2b, we observe similar results as on the logistic regression, suggesting
that empirically our analysis still stands on non-log-concave distributions. We also provide negative
log-likelihood (NLL) comparisons in terms of number of bits on the logistic regression and MLP in
Figure 3 in Appendix.
7
Under review as a conference paper at ICLR 2022
16
§14
iɪ2
•M
S
ω
h 10
8
23456789	10
Number of Fractional Bits
T- SGDLP-L
→- Naive SGLDLP-L
-→- VC SGLDLP-L
T- SGDLP-F
→- SGLDLP-F
SGDFP (7.68%)
SGLDFP (7.63%)
(b) MLP
(a) Logistic regression
Figure 2: Test errors on MNIST dataset in terms of different precision.
5.2	RESNET AND LSTM
We consider image and sentiment classification tasks: CIFAR datasets (Krizhevsky et al., 2009)
on ResNet-18 (He et al., 2016), and IMDB dataset (Maas et al., 2011) on LSTM (Hochreiter &
Schmidhuber, 1997). We use 8-bit number representation since it becomes increasingly popular in
training deep models and is powered by new generation of chips (Sun et al., 2019; Banner et al.,
2018; Wang et al., 2018b). We report test errors averaged over 3 runs with the standard error in
Table 1.
Fixed Point We use 8-bit fixed point for weights and gradients but full-precision for activations
since we find low-precision activations significantly harm the performance. Similar to the results
in previous sections, SGLDLP-F is better than SGDLP-F and VC SGLDLP-L significantly outper-
forms naive SGLDLP-L and SGDLP-L across datasets and architectures. Notably, the improvement
of SGLD over SGD becomes larger when using more low-precision arithmetic. For example, on
CIFAR-100, VC SGLDLP-L outperforms SGDLP-L by 2.24%, SGLDLP-F outperforms SGDLP-F
by 0.54% and SGLD outperforms SGDLP by 0.06%. This demonstrates that SGLD is particularly
compatible with low-precision deep learning because of its natural ability to handle system noise.
Block Floating Point We also consider block floating point (BFP) which is another common num-
ber type and is often preferred over fixed point on deep models due to less quantization error caused
by overflow and underflow (Song et al., 2018). Following the block design in Yang et al. (2019),
we use small-block for ResNet and big-block for LSTM. The Qvc function naturally generalizes to
BFP and only needs a small modification (see Appendix E for the algorithm of Qvc with BFP). By
using BFP, the results of all low-precision methods improve over fixed point. SGLDLP-F can match
the performance of SGLDFP with all numbers quantized to 8-bit except gradient accumulators.
VC SGLDLP-L still outperforms naive SGLDLP-L indicating the effectiveness of Qvc with BFP.
Again, SGLDFP-F and VC SGLDLP-L outperform their SGD counterparts on all tasks, suggesting
the general applicability of low-precision SGLD with different number types.
Cyclical SGLD We further apply low-precision to a recent variant of SGLD, cSGLD, which uti-
lizes a cyclical learning rate schedule to speed up convergence (Zhang et al., 2020). We observe
that the results of cSGLD-F are very close to those of cSGLDFP, and VC cSGLDLP-L can match
or even outperforms full-precision SGD with all numbers quantized to 8 bits! These results indi-
cate that diverse samples from different modes can countereffect the quantization error by providing
complementary predictions.
5.3	ImageNet
Finally, we test low-precision SGLD on a large scale image classification dataset, ImageNet, with
ResNet-18. We train SGD for 90 epochs and train SGLD for 10 epochs using the trained SGD model
as the initialization. The improvement of SGLD over SGD is larger in low-precision (0.76% top-1
8
Under review as a conference paper at ICLR 2022
Table 1: Test errors (%) on CIFAR-10, CIFAR-100 with ResNet-18 and IMDB with LSTM.
	CIFAR-10	CIFAR- 1 00	IMDB
32-bit Floating Point			
S GLDFP	4.65 ±0.06	22.58 ±0.18	13.43 ±0.21
SGDFP	4.71 ±0.02	22.64 ±0.13	13.88 ±0.29
-CSGLDFP	~ 4.54 士0.05	一 21.63±o.04-	一「3「25 ±0.18 -
8-bit Fixed Point			
NAIVE SGLDLP-L	7.82 ±0.13	27.25 ±0.13	16.63 ±0.28
VC S GLDLP -L	7.13 ±0.01	26.62 ±0.16	15.38 ±0.27
SGLDLP-F	5.12 ±0.06	23.30 ±0.09	15.40 ±0.36
SGDLP-L	8.53 ±0.08	28.86 ± 0.10	19.28 ±0.63
SGDLP-F	5.20 ±0.14	23.84 ±0.12	15.74 ±0.79
8-bit B lock Floating Point			
NAIVE SGLDLP-L	5.85 ±0.04	26.38 ±0.13	14.64 ±0.08
VC SGLDLP-L	5.51 ±0.01	25.22 ±0.18	13.99 ±0.24
SGLDLP-F	4.58 ±0.07	22.59 ±0.18	14.05 ±0.33
SGDLP-L	5.86 ±0.18	26.19 ±0.11	16.06 ±1.81
SGDLP-F	4.75 士0.05	22.9 ±0.13	14.28 ±0.17
-VCcSGLDLP-L	一 4.97 ±0.10	- 22.6f±0.12	—13709 ±0.27
CSGLD-F	4.32 ±0.07	21.50 ±0.14	13.13 ±0.37
Table 2: Test errors (%) on ImageNet with ResNet-18.
	TOP-1	TOP-5
32-bit Floating Point		
SGLD	30.39	10.76
SGD	30.56	10.97
8-bit Block Floating Point
VC SGLDLP-L	31.47	11.77
SGDLP-L	32.23	12.09
error) than in full-precision (0.17% top-1 error), showing the advantages of low-precision SGLD on
large-scale deep learning tasks.
6 Conclusion
We provide the first comprehensive study of low-precision SGLD. In theory, we prove that with full-
precision gradient accumulators, SGLD can converge to the target distribution within a distance that
is asymptotically smaller than the distance between the SGD estimation and the optimum; with low-
precision gradient accumulators, SGLD can diverge arbitrarily far away from the target distribution
with small stepsizes. We find that the issue is caused by the wrong variance in each update and thus
develop anew varaince-corrected quantization function that preserves the correct variance. We prove
that SGLD with this quantization function converges to the target distribution up to a certain level
depending on the quantization gap. In practice, we verify our theoretical results and demonstrate
SGLDLP-F and VC SGLDLP-L are comparable to full-precision SGLD with 8-bit on image and
sentiment classification tasks.
In the future, it will be interesting to extend low-precision computation to other stochastic MCMC
methods and improve theoretical bounds to better reflect empirical performance. We hope this work
shed lights on accelerating MCMC methods for Bayesian deep learning.
9
Under review as a conference paper at ICLR 2022
References
Jan Achterhold, Jan Mathias Koehler, Anke Schmeink, and Tim Genewein. Variational network
quantization. In International Conference on Learning Representations, 2018.
Sungjin Ahn, Babak Shahbaba, and Max Welling. Distributed stochastic gradient mcmc. In Inter-
national conference on machine learning ,pp.1044-1052. PMLR, 2014.
Jack Baker, Paul Fearnhead, Emily B Fox, and Christopher Nemeth. Control variates for stochastic
gradient mcmc. Statistics and Computing, 29(3):599-615, 2019.
Ron Banner, Itay Hubara, Elad Hoffer, and Daniel Soudry. Scalable methods for 8-bit training of
neural networks. arXiv preprint arXiv:1805.11046, 2018.
Ruizhe Cai, Ao Ren, Ning Liu, Caiwen Ding, Luhao Wang, Xuehai Qian, Massoud Pedram, and
Yanzhi Wang. Vibnn: Hardware acceleration of bayesian neural networks. ACM SIGPLAN No-
tices, 53(2):476-488, 2018.
Changyou Chen, Nan Ding, Chunyuan Li, Yizhe Zhang, and Lawrence Carin. Stochastic gradient
mcmc with stale gradients. arXiv preprint arXiv:1610.06664, 2016.
Tianqi Chen, Emily Fox, and Carlos Guestrin. Stochastic gradient hamiltonian monte carlo. In
International conference on machine learning, pp. 1683-1691. PMLR, 2014.
Zhiyong Cheng, Daniel Soudry, Zexi Mao, and Zhenzhong Lan. Training binary multilayer
neural networks for image classification using expectation backpropagation. arXiv preprint
arXiv:1503.03562, 2015.
Matthieu Courbariaux, Yoshua Bengio, and Jean-Pierre David. Binaryconnect: Training deep neural
networks with binary weights during propagations. In Advances in neural information processing
systems, pp. 3123-3131, 2015.
Matthieu Courbariaux, Itay Hubara, Daniel Soudry, Ran El-Yaniv, and Yoshua Bengio. Binarized
neural networks: Training deep neural networks with weights and activations constrained to+ 1
or-1. arXiv preprint arXiv:1602.02830, 2016.
Arnak S Dalalyan and Avetik Karagulyan. User-friendly guarantees for the langevin monte carlo
with inaccurate gradient. Stochastic Processes and their Applications, 129(12):5278-5311, 2019.
Christopher De Sa, Matthew Feldman, Christopher Re, and KUnle Olukotun. Understanding and
optimizing asynchronous low-precision stochastic gradient descent. In Proceedings of the 44th
Annual International Symposium on Computer Architecture, pp. 561-574, 2017.
Kumar Avinava Dubey, Sashank J Reddi, Sinead A Williamson, Barnabas Poczos, Alexander J
Smola, and Eric P Xing. Variance reduction in stochastic gradient langevin dynamics. Advances
in neural information processing systems, 29:1154-1162, 2016.
Steven K Esser, Jeffrey L McKinstry, Deepika Bablani, Rathinakumar Appuswamy, and Dharmen-
dra S Modha. Learned step size quantization. arXiv preprint arXiv:1902.08153, 2019.
Martin Ferianc, Partha Maji, Matthew Mattina, and Miguel Rodrigues. On the effects of quantisation
on model uncertainty in bayesian neural networks. arXiv preprint arXiv:2102.11062, 2021.
Zhe Gan, Chunyuan Li, Changyou Chen, Yunchen Pu, Qinliang Su, and Lawrence Carin. Scal-
able bayesian learning of recurrent neural networks for language modeling. arXiv preprint
arXiv:1611.08034, 2016.
Suyog Gupta, Ankur Agrawal, Kailash Gopalakrishnan, and Pritish Narayanan. Deep learning with
limited numerical precision. In International conference on machine learning, pp. 1737-1746.
PMLR, 2015.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recog-
nition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp.
770-778, 2016.
10
Under review as a conference paper at ICLR 2022
SePP Hochreiter and Jurgen Schmidhuber. Long short-term memory. Neural computation, 9(8):
1735-1780,1997.
Benoit Jacob, Skirmantas Kligys, Bo Chen, Menglong Zhu, Matthew Tang, Andrew Howard,
Hartwig Adam, and Dmitry Kalenichenko. Quantization and training of neural networks for
efficient integer-arithmetic-only inference. In Proceedings of the IEEE conference on computer
vision and pattern recognition, PP. 2704-2713, 2018.
AnooP Korattikara, Vivek Rathod, Kevin MurPhy, and Max Welling. Bayesian dark knowledge.
arXiv preprint arXiv:1506.04416, 2015.
Raghuraman Krishnamoorthi. Quantizing deeP convolutional networks for efficient inference: A
whitePaPer. arXiv preprint arXiv:1806.08342, 2018.
Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiPle layers of features from tiny images.
2009.
Chunyuan Li, Andrew Stevens, Changyou Chen, Yunchen Pu, Zhe Gan, and Lawrence Carin. Learn-
ing weight uncertainty with stochastic gradient mcmc for shaPe classification. In Proceedings of
the IEEE Conference on Computer Vision and Pattern Recognition, PP. 5666-5675, 2016.
Chunyuan Li, Changyou Chen, Yunchen Pu, Ricardo Henao, and Lawrence Carin. Communication-
efficient stochastic gradient mcmc for neural networks. In Proceedings of the AAAI Conference
on Artificial Intelligence, volume 33, PP. 4173-4180, 2019.
Hao Li, Soham De, Zheng Xu, ChristoPh Studer, Hanan Samet, and Tom Goldstein. Training
quantized nets: A deePer understanding. Neural Information Processing Systems, 2017.
Zheng Li and ChristoPher M De Sa. Dimension-free bounds for low-Precision training. Advances
in Neural Information Processing Systems, 2019.
Darryl Lin, Sachin Talathi, and Sreekanth AnnaPureddy. Fixed Point quantization of deeP con-
volutional networks. In International conference on machine learning, PP. 2849-2858. PMLR,
2016.
Yi-An Ma, Tianqi Chen, and Emily B Fox. A comPlete reciPe for stochastic gradient mcmc. arXiv
preprint arXiv:1506.04696, 2015.
Yi-An Ma, Yuansi Chen, Chi Jin, Nicolas Flammarion, and Michael I Jordan. SamPling can be faster
than oPtimization. Proceedings of the National Academy of Sciences, 116(42):20881-20885,
2019.
Andrew Maas, Raymond E Daly, Peter T Pham, Dan Huang, Andrew Y Ng, and ChristoPher Potts.
Learning word vectors for sentiment analysis. In Proceedings of the 49th annual meeting of the
association for computational linguistics: Human language technologies, PP. 142-150, 2011.
Xiangming Meng, Roman Bachmann, and Mohammad Emtiyaz Khan. Training binary neural net-
works using the bayesian learning rule. In International Conference on Machine Learning, PP.
6852-6861. PMLR, 2020.
Paulius Micikevicius, Sharan Narang, Jonah Alben, Gregory Diamos, Erich Elsen, David Garcia,
Boris Ginsburg, Michael Houston, Oleksii Kuchaiev, Ganesh Venkatesh, et al. Mixed Precision
training. arXiv preprint arXiv:1710.03740, 2017.
Zhourui Song, Zhenyu Liu, and Dongsheng Wang. ComPutation error analysis of block floating
Point arithmetic oriented convolution neural network accelerator design. In Proceedings of the
AAAI Conference on Artificial Intelligence, volume 32, 2018.
Daniel Soudry, Itay Hubara, and Ron Meir. ExPectation backProPagation: Parameter-free training
of multilayer neural networks with continuous or discrete weights. In NIPS, volume 1, PP. 2,
2014.
Jiahao Su, Milan Cvitkovic, and Furong Huang. SamPling-free learning of bayesian quantized neural
networks. arXiv preprint arXiv:1912.02992, 2019.
11
Under review as a conference paper at ICLR 2022
Xiao Sun, Jungwook Choi, Chia-Yu Chen, Naigang Wang, Swagath Venkataramani, Vijayalak-
shmi Viji Srinivasan, Xiaodong Cui, Wei Zhang, and Kailash Gopalakrishnan. Hybrid 8-bit float-
ing point (hfp8) training and inference for deep neural networks. Advances in neural information
processing systems, 32:4900-4909, 2019.
Xiao Sun, Naigang Wang, Chia-Yu Chen, Jiamin Ni, Ankur Agrawal, Xiaodong Cui, Swagath
Venkataramani, Kaoutar El Maghraoui, Vijayalakshmi Viji Srinivasan, and Kailash Gopalakrish-
nan. Ultra-low precision 4-bit training of deep neural networks. Advances in Neural Information
Processing Systems, 33, 2020.
Kunal Talwar. Computational separations between sampling and optimization. arXiv preprint
arXiv:1911.02074, 2019.
Mart van Baalen, Christos Louizos, Markus Nagel, Rana Ali Amjad, Ying Wang, Tijmen
Blankevoort, and Max Welling. Bayesian bits: Unifying quantization and pruning. arXiv preprint
arXiv:2005.07093, 2020.
Kuan-Chieh Wang, Paul Vicol, James Lucas, Li Gu, Roger Grosse, and Richard Zemel. Adversar-
ial distillation of bayesian neural network posteriors. In International Conference on Machine
Learning, pp. 5190-5199. PMLR, 2018a.
Naigang Wang, Jungwook Choi, Daniel Brand, Chia-Yu Chen, and Kailash Gopalakrishnan. Train-
ing deep neural networks with 8-bit floating point numbers. In Proceedings of the 32nd Interna-
tional Conference on Neural Information Processing Systems, pp. 7686-7695, 2018b.
Max Welling and Yee W Teh. Bayesian learning via stochastic gradient langevin dynamics. In
Proceedings of the 28th international conference on machine learning (ICML-11), pp. 681-688.
Citeseer, 2011.
Shuang Wu, Guoqi Li, Feng Chen, and Luping Shi. Training and inference with integers in deep
neural networks. arXiv preprint arXiv:1802.04680, 2018.
Guandao Yang, Tianyi Zhang, Polina Kirichenko, Junwen Bai, Andrew Gordon Wilson, and Christo-
pher De Sa. Swalp: Stochastic weight averaging in low-precision training. International Confer-
ence on Machine Learning, 2019.
Ruqi Zhang, Chunyuan Li, Jianyi Zhang, Changyou Chen, and Andrew Gordon Wilson. Cycli-
cal stochastic gradient mcmc for bayesian deep learning. International Conference on Learning
Representations, 2020.
Tianyi Zhang, Zhiqiu Lin, Guandao Yang, and Christopher De Sa. Qpytorch: A low-precision
arithmetic simulation framework. arXiv preprint arXiv:1910.04540, 2019.
Shuchang Zhou, Yuxin Wu, Zekun Ni, Xinyu Zhou, He Wen, and Yuheng Zou. Dorefa-net: Train-
ing low bitwidth convolutional neural networks with low bitwidth gradients. arXiv preprint
arXiv:1606.06160, 2016.
Shilin Zhu, Xin Dong, and Hao Su. Binary ensemble neural network: More bits per network or
more networks per bit? In Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition, pp. 4923-4932, 2019.
12
Under review as a conference paper at ICLR 2022
A Proof of Theorem 1
Our proof follows Theorem 4 in Dalalyan & Karagulyan (2019), which provides a convergence
bound of Langevin dynamics with noisy gradient. We state the result of Theorem 4 in Dalalyan &
Karagulyan (2019) below.
We consider Langevin dynamics whose update rule is
θk+ι = θk - α (VU(θk) + Zk) + √2αξk+ι.
The noise in the gradient ζk has the following three assumptions:
EhkE [Zk ∣θk ]k2 i ≤ δ2d, EhkZk-E [Zk ∣θk]k2i ≤ σ2d, ξk+ι is independent of (Zo,…,Zk)
where δ > 0 and σ > 0 are some constants. Under the same assumptions in Section 4, we have the
convergence bound for the above Langevin dynamics.
Theorem 4. We run the above Langevin dynamics with α ≤ 2/(m + M). Let π be the target
distribution, μκ be the distribution obtained after K iterations and μo be the initial distribution.
Then
W2(μκ,π) ≤ (1 - αm)KW2(μo,π) + 1∙65(M/m)(ad)1/2 +——— + RKL J—∕=∙
m	1.65M +σ m
Proof. We write the SGLDLP-F update as the following
Θk+1 = θk - αQg(VU(Qw(θk))) + √2αξk+1
=θk - α (VU(θk) + Zk) + √2αξk+ι
where
.~. ... .
Zk = Qg (VU(Qw (θk))) -VU (θk)
〜
〜
Qg(VU(Qw(θk)))-VU(Qw(θk))
~ . .. . .. . .. .
+ VU(Qw(θk)) - VU(Qw(θk)) + VU(Qw(θk)) - VU(θk).
…	_ r_ ~ z …	_ 一 ，	、	_ -。一 ，…	-
Since E[VU(x)] = VU(x) and E[Q(x)] = x, we have
〜
〜
E[Zk] = E [Qg(VU(Qw(θk)))-VU
+ E [vU(Qw(θk)) -VU(
(Qw(θk))
(Qw(θk))i + E [VU(Qw(θk)) - VU(θk)]
E [VU(Qw(θk)) -VU(θk)].
By the assumption, we know that
E kVU(Qw(θk)) - VU(θk)k22 ≤ M2E kQw(θk) -θkk22
≤ M2 ∙ δ Wd.
4
It follows that
kE[ζk]k22 = kE[VU(Qw(θk))-VU(θk)]k22
≤ E hkVU(Qw(θk)) -VU(θk)k22i
≤ M2 •空.
4
Let f : R → Rd denote the function
f(a) = VU(θk + a(Qw(θk) - θk)).
13
Under review as a conference paper at ICLR 2022
By the mean value theorem, there will exist an a ∈ [0, 1] (a function of the weight quantization
randomness) such that
f(1) - f(0) = f0(a).
So,
E[Zk] = E [VU(Qw(θk))-VU(θk)]
=E [V2U(θk + a(Qw(θk) - θk))(Qw(θk) - θk)]
= E V2U(θk)(Qw(θk) - θk)
+ E [(V2U(θk + a(Qw(θk) - θk))- V2U(θk)) (Qw(θk) - θk)]
=E [(V2U(θk + a(Qw(θk) - θk)) - V2U(θk)) (Qw(θk) - θk)].
Now, by the assumption kV2U(x) - V2U(y)k2 ≤ Ψkx - yk, we get
kE[Zk]k =	∣∣E [(V2U(θk	+	a(Qw(θk)	-	θk)) -	V2U(θk))	(Qw(θk)	-	θ)] ∣∣
≤	E [∣∣ (V2U(θk	+	a(Qw(θk)	-	θk)) -	V2U(θk))	(Qw(θk)	-	θk)∣∣]
≤ E [∣∣V2U(θk + a(Qw(θk) - θk)) -V2U(θk)∣∣2kQw(θk)-θk)k]
≤E[Ψka(Qw(θk)-θk)kkQw(θk)-θk)k]
≤ ΨE hkQw(θk) - θkk2i
Ψ∆w d
≤ -----
4
This combined with the previous result gives us
Ψ∆w √d M∆w∖
kE[ζk]k =min I —W-, -^― .
Now considering the variance of ζk,
E kζk - E[ζk]k22
≤ E hkζkk22i
≤ E ∣∣Qg(VU(Qw(θk))) - VU(Qw(θk))∣∣2]
+ E ∣∣VU(Qw(θk)) -VU(Qw(θk))
+ E kVU(Qw(θk)) - VU(θk)k22
≤
生 + κ2 + M2 •受
44
We set δ and σ in Theorem 4 in Dalalyan & Karagulyan (2019) as the following
min (Y√d,MΔw] , σ2d =% + κ2 + M2 •出=用 + M27)d + 4κ2
4	,	2	,	4	4	4
14
Under review as a conference paper at ICLR 2022
The assumptions in Theorem 4 in Dalalyan & Karagulyan (2019) are satisfied, thus we can apply
the result in Theorem 4 and get
δ√d
W2(μκ,∏) ≤ (1 — αm) W2(μ0,∏) + 1.65(M∕m)(ad)1/2 +-----+
m
σ2(αd)”
1.65M + σ√m
≤ (1 — αm)KW2(μ0,∏) + 1.65(M∕m)(ad)1/2 + ^+ J(T
mm
(1 — am)K W2(μ0,∏) + 1.65(M∕m)(ad)1/2 + min
+
(∆2g +
M 2∆W)αd + 4ακ2
4m
M ∆w √d
2m
□
B Proof of Theorem 2
Proof. The update of SGLDLP-L is
Θk+1 = Qw (θk — αQg(VU(θk)) + √2αξk+1).
Let ψk+ι = θk — αQg(VU(θk)) + √⅛+ι So that θk = Qw(ψk). Then,
Ψk+ι = θk — αQg (VU(θk)) + √2αξk+ι
=Qw (ψk) — αQg (VU(Qw (ψk))) + √2αξk+1
=Ψk — α(VU (ψk) + Zk) + √2αξk+1
where
Zk = ψkα^" + Qg (VU(θk)) — VU (ψk)
=ψk — θk + Qg(VU(θk)) - VU(θk) + VU(θk) - VU(θk) + VU(θk) — VU(ψk)
Similar to the previous proof, we know that
E[Zk] = E [VU(θk) — VU(ψk)] = E [VU(Qw(ψk)) — VU(ψk)],
so
kE[Zk]k ≤ min
and it suffices to set δ as above. On the other hand, the variance will be bounded by
EhkZk — E[Zk]k2i ≤ E ∣∣Qg(VU(θk)) — VU(θk)∣∣21 + E ∣∣VU(θk) — VU(θk)
+ E	ψk[" + VU (θk) — VU (ψk) ∣∣
≤	—+ κ2 + EhkVF(ψk) — VF(θk)『],
where F(θ)=亲 ∣∣θ∣∣2 - U(θ). Observe that since U is m-strongly convex and M-smooth, and
α-1 ≥ M/2, F must be h-1-smooth, and so
E hkζk — EKk]k2i ≤ ~4- ^+ κ2 + 02 E hkψk — θk k2i
∆g2d	2	∆2wd
≤ ɪ + κ2 + ~7w∙ ∙
4	4α2
15
Under review as a conference paper at ICLR 2022
This is essentially replacing the M2 in the previous analysis with α-2, so, supposing the distribution
of ψK+1 is νK, it will give us the rate of
W2(νκ, π) ≤ (1 — αm)KW2(ν0, π) + 1.65(M∕m)(ad)1/2 + min
+	(α∆g2 +α
T ∆W )d + 4ακ2
4m
M ∆w √d
2m
We also have
ι	1	、1/2	1 ʌ /ʒ
W2(μk,Vk)=(JeinfyJ kx — yk2 dJ(χ,y))	≤ E [kθk+ι — ψk+1k2] 2 ≤ -w2c
Combining these two results, we get
W2(μκ ,∏) ≤ W2(μκ, VK ) + W2 (VK ,π)
≤ (1 — αm)KW2(ν0, π) + 1.65(M∕m)(ad)1/2 + min
/ (a—2 + α-1 —W )d + 4aκ2	—仅 √d
+ V--------4m----------+ —
≤ (1 — αm)KW2(μ0,∏) + 1.65(M∕m)(ad)1/2 + min
M—w √d
2m
M—w √d
2m
/ (a—2 + a-1 —W )d + 4aκ2
+ v	4m
+ ((1 — αm)K + 1) -w2^d
□
C Proof of Theorem 3
Proof. The update of VC SGLDLP-L is
Θk+1 = Qvc (θk — aQg(VU(θk)), 2a, —w,Qd, Qs)
We ignore the variance of Q2 since it is relatively small compared to weight quantization variance
in practice. Qvc is defined as in Algorithm 1 and we have E[θk+1] = θk — aVU (θk).
Let ψk+ι = θk — aQg(VU(θk)) + √2aξk+ι then it follows that
ψk+1 — θk + 1 = θk — aQg (VU(θk)) + V2aξk+1 — θk + 1,
and
Ψk+1 = ψk — a(VU (ψk) + Zk) + √2aξk+ι
where
Zk = ψk — θk + Qg (VU(θk )) — VU(ψk )
=ψk — θk + Qg (VU(θk)) — VU(θk) + VU(θk) — VU (θk) + VU (θk) — VU (ψk).
Note that E[ψk — θk] = 0. Similar to previous proof, we know that
kE[Zk]k2 = kE [VU(θk) — VU(ψk)]k2 ≤ M2E [kψk — θkk2i .
∆2
When 2a > Vo =寸,we have that
E[kΨk — θk『]
E
l(θk-1 — aQg(VU(θk-1))) + √2aξk — Qd (θk-i — aQg (VU(θk-1)) + √2a - v0ξk) — sign(r)c||
16
Under review as a conference paper at ICLR 2022
Let
b = Qd (θk-i - aQg(VU7(θfc-1)) + √2θ-τ0ξk)
-(θk-1 - αQg (VU(θk-1))+ √2α - v0 ξk),
then |b| ≤ ∆w and
E h∣ψk-θkIl2]
=E ((θk-1 - αQg (VtJ (θk-1 ))) + √2αξk - (θk-1 - αQg (VtJ (θk-1)) + √2α - v0ξk)
2
—b — sign(r)c
=E Il√2αξfc - √2α - v0ξk - b - sign(r)CH
≤ E l√2αξk - √2α - v0ξk - b∣∣ ] + E IjlSign(r)c∣∣2]
≤ (√2α - √2α - vo)2E[kξkIl2]+ (√2α - √2α - v0)∆wEUξk|] + 2v0d
≤ ((√2α — √2α — v0)2 + (√2α — √2α — v0)∆w + 2v0) d
Since 2xy ≤ x2 + y2, we get
,L ，________	,L ，_________	∆2	, L l______S
(√2α — vz2α — v0)∆w	≤ (√2α — 72α — v0)2	+———=(√2α — ∖)2α	— v0)2	+ v0
It follows
E[∣∣ψk — θk Il2] ≤ (2(√2ɑ — √2α — v0)2 + 3v°) d.
Note that
√2a -√2α-τ0 = 2α -(2a -。。1_
√2α + λ∕2q — V0
______V0______
ʌ/ 2α + √2α — v0
≤√fc.
The expectation becomes
Since 2α > v0, it follows
E[∣∣ψk - θk Il2] ≤ ("00 + 3v。) d.
E[∣ψk - θk∣∣2] ≤ (2v0 + 3v0)d = 5v0d.
Let A = 5v0d. Then
∣E[Zk]k2 ≤ M2 ∙ A,
and also
∣E[Zk]∣ ≤ Ψ ∙ A.
Therefore,
δ = min (ΨA, M√A).
The variance will be bounded by
E [∣∣Zk- E[Zk]∣2] ≤ E ∣∣Qg(Vt(θk))-VU(θk)∣∣21 + E
+E
ψk-θk + VU(θk) -VU(ψk) I I
≤ ~7---+ K +--2 E hlψk - θk ∣∣2]
4	02	L	」
≤ 号 +K2 + g.
4	02
~ , ,
VU(θk) -VU(θk)
17
Under review as a conference paper at ICLR 2022
Supposing the distribution of ψK+1 is νK, it will give us the rate of
W2(νκ, π) ≤ (1 — αm)KW2(ν0, π) + 1.65(M∕m)(ad)1/2 + min ( ———, ʌʃ
mm
/ α∆2d + 4ɑκ2	~A~
+ v ―4m — + am
We also have
W2(μκ, νκ)= (J三inf J ∣∣x — y『dJ(x,y))	≤ E [∣∣θκ+ι — ψκ+j∣2] 2 ≤√A.
Combining these two results, we get
W2(μκ,∏) ≤ W2(μκ,νκ) + W2(νκ,∏)
≤
(1 — αm)KW2(ν0, π) + 1.65(M∕m)(ad)1/2 + min ( ———, ʌʃ
mm
+ 严三三;三+√a
4m	αm
≤
(1 — αm)KW2(μ0, π) + 1.65(M∕m)(ad)1/2 + min ) ———, ʌʃ
mm
+ Ja耳 Wɪ + ((1 - am)K + 1) √A.
4m	αm
∆2
When 2a < 寸,since We assume that the gradient is bounded by E
(VU(Ok))∣∣J ≤ G
E[∣ψk — θk∣2] = E
I∣(θk-1 — aQg (VU(θk-i))) — θk + √2αξk∣2
E
((θk-1 — aQg (VU(θk-1))) — θk ||
+E
≤ max 0E ∣∣(θk-i — aQg(VU(θk-i))) — Qs (θ-ι — aQg(VU(θk-i))) ||2
4ad
Using the bound equation (6) in Li & De Sa (2019),
E ]∣∣(θk-i — aQg (VU(θk-i))) — Qs (θ— - aQg (VU(Θj))) ∣∣2
≤ ∆waE h∣∣Qg(VU(θk-i))∣∣J
≤ ∆waG
It folloWs
E[∣ψk — θk∣2] ≤ max (2∆waG, 4ad)
LetA = max (2∆waG, 4ad). The rest is that same as the case 2a > v0.
□
D	Comparison to SGD Bounds
FolloWing previous Work in comparing sampling and optimization methods (Ma et al., 2019; Tal-
War, 2019), We also compare our 2-Wasserstein distance bound With previosu SGD bounds. Previous
loW-precision SGD convergence bounds are shoWn in terms of the squared distance to the optimum
∣Bk — θ*∣∣2 (Yang et al., 2019). In order to compare our bounds with theirs, we consider a 2-
Wasserstein distance between two point distributions. Let μκ be the point distribution assigns zero
18
Under review as a conference paper at ICLR 2022
probability everywhere except OK and ∏ be the point distribution assigns zero probability every-
where except θ*. Then We get
1	1	1 1/2	1
W2(μκ,π) = INfXy) J kx-yk2dJ(χ,y))	≤ E [∣∣°k -θ*∣∣2]2.
From (Yang et al., 2019), we know that E [∣∣0K - θ*∣∣2∣ 2 is proportional to ∆w. Therefore, our
2-Wasserstein distance is O(∆2w) whereas SGD’s 2-Wasserstein distance is O(∆w), which shows
SGLD is more robust to the quantization error.
E Algorithms with (Block) Floating point Numbers
The qunatization gap is floating point and block floating point is computed as
.() ʃ2E[μ]-W+2 where E[μ] = clip(blog2(max ∣μ∣)C ,l,u) block floating point
W " 1 ∣2E[μ]-W where E[μ] = clip(bl0g2(∣μ∣)C ,l, U)	floating point
(3)
Then deterministic rounding and stochastic rounding are defined with ∆w . VC SGLDLP-L with
(block) floating point is outlined in Algorithm 3
Algorithm 3 VC SGLDLP-L with (Block) Floating Point.
given: Stepsize a, number of training iterations K, gradient quantizer Qg, deterministic rounding
with (block) floating point Qd, stochastic rounding with (block) floating point Qs, F bits to
represent the shared exponent (block floating point) or the exponent (floating point), W bits to
represent each number in the block (block floating point) or the mantissa (floating point).
let l ~ -2f-1,u — 2f-1 - 1
for k = 1 : K do
compute μ 一 θk - αQg (vU(θk-i))
compute ∆w (μ) following Equation (3)
update θk+ι — Qvc (μ, 2αT, ∆(μ),Qd,Qs)
end for
output: samples {θk}
Algorithm 4 Variance-Corrected Quantization Function Qvc with (Block) Floating Point.
input: (μ, v, ∆, Qd, Qs)
vo J ∆2∕4
if v > v0 then
X J μ + √v - voξ, where ξ ~ N(0,Id)
r J x - Qd(x)
recompute ∆ J ∆w(x) following Equation (3)
(∆ w v。0+户+|小
I 八，	w.p.	2∆2
C J -∆ WD v0+r2Tr|A
4, w.p.	2∆2
(0,	otherwise
return Qd(X) + sign(r) ∙ C
else
the same as in fixed point numbers
end if
The Qvc function with (block) floating point in Algorithm 4 is the same as Algorithm 1 except the
line in red. We recompute the quantization gap ∆ after adding Gaussian noise to make sure it aligns
with the quantization gap of X.
19
Under review as a conference paper at ICLR 2022
Figure 3: Test NLL on MNIST dataset in terms of different precision.
23456789	10
Number of Fractional Bits
F	Experimental Details and Additional Results
F.1 Sampling methods
For both SGLD and low-precision SGLD, we collected samples {θk}JK=1 from the posterior of the
model's weight, and obtained the prediction on test data {χ*,y*} by Bayesian model averaging
1J
p(y[χ*,D) ≈ j£p(y"x*,D,θj).
j=1
F.2 MNIST
We train all methods on logistic regression and MLP for 20 epochs with learning rate 0.1 and batch
size 64. We additionally report negative log-likelihood (NLL) comparisons in Figure 3.
F.3 CIFAR and IMDB
For CIFAR datasets, we use batch size 128, learning rate 0.5 and weight decay 5e - 4. We train
the model for 245 epochs and used the same decay learning rate schedule as in Yang et al. (2019).
We collect 50 samples for SGLD. For cyclical learning rate schedule, we use 7 cycles and collect 5
models per cycle (35 models in total).
For IMDB dataset, we use batch size 80, learning rate 0.3 and weight decay 5e - 4. We use a two-
layer LSTM. The embedding dimension and the hidden dimension are 100 and 256 respectively. We
train the model for 50 epochs and used the same decay learning rate schedule as on CIFAR datasets.
We collect 20 samples for SGLD. For cyclical learning rate schedule, we use 1 cycles and collect 20
models.
F.4 ImageNet
We use batch size 256, learning rate 0.2 and weight decay 1e - 4. We use the same decay learning
rate schedule as in He et al. (2016) and collect 20 models for SGLD.
20