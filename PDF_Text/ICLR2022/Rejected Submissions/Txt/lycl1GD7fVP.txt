Under review as a conference paper at ICLR 2022
Neural Tangent Kernel Eigenvalues
Accurately Predict Generalization
Anonymous authors
Paper under double-blind review
Ab stract
In many cases, infinitely-wide deep neural networks are equivalent to kernel re-
gression using the network’s “neural tangent kernel.” With the aim of shedding
light on the learning behavior of wide neural networks, in this work we derive
a new theory predicting the generalization of kernel regression. Our theory ac-
curately predicts not only test mean-squared-error but all first- and second-order
statistics of the learned function. Furthermore, using a measure quantifying the
“learnability” of a given target function, we prove a new “no-free-lunch” theorem
characterizing a fundamental tradeoff in the inductive bias of kernel regression:
improving generalization for a given target function must worsen its generaliza-
tion for orthogonal functions. We further demonstrate the utility of our theory by
analytically predicting two surprising phenomena — worse-than-chance general-
ization on hard-to-learn functions and nonmonotonic error curves in the small data
regime — which we subsequently observe in experiments. Though our theory is
derived for networks in the infinite-width limit, we experimentally find that its
predictions are accurate for wide finite networks.
1	Introduction
Understanding and predicting a machine learning model’s generalization to unseen data is a central
goal of machine learning theory. For a given class of model, one would ideally want a simple
picture of a model’s inductive bias, identifying the set of functions on which a given model will
generalize well and those on which it will generalize poorly and making quantitative predictions of
key measures of the quality of generalization. In this paper, we derive such a theory for ridgeless
kernel regression with any kernel and, using the neural tangent kernel (NTK) equivalence between
kernel regression and infinitely wide deep neural networks, shed light on the inductive bias and
generalization of wide deep neural networks.
Our main contributions are as follows:
•	We prove a basic conservation law describing the inductive bias of kernel regression (Theo-
rem 1). This law states that any kernel has a fixed budget of a quantity we call “learnability”
that it must allocate to an orthogonal basis of functions, and this budget is equal to the size
of the training set. As a consequence of this conservation law, we prove that for every ker-
nel, there exists a target function on which kernel regression generalizes worse than chance
(Corollary 1), and we provide a recipe to construct such functions.
•	We derive a new theory of generalization for kernel regression, culminating in analytical
expressions for all first- and second-order statistics of the learned function (Equations 11
and 13). This theory extends the spectral picture of kernel regression revealed by Bordelon
et al. (2020) and Canatar et al. (2021). We conclude from our theory that, in realistic
settings, most kernel eigenfunctions have the counterintuitive property that MSE increases
as examples are added to a small training set (Section 2.7).
•	We empirically verify all our results on synthetic datasets using both exact kernel regression
and deep networks of width 500 trained with gradient descent. We find that our conserva-
tion law and analytical expressions for generalization performance hold to an excellent
approximation even for wide finite networks, and we observe worse-than-chance general-
ization and increasing MSE as predicted. We find that our theory’s core predictions remain
1
Under review as a conference paper at ICLR 2022
fairly accurate even down to width 20 for depth-four networks, suggesting it is a promising
starting point for understanding generalization in practical architectures.
1.1	Related work
The study of the generalization of kernel regression via spectral analysis began in the literature on
Gaussian process inference, for which kernel regression gives the mean of the posterior function. In
a limited teacher-student setting in which teacher and student were described by the same Gaussian
process, Sollich (1999) and Vivarelli & Opper (1999) studied expected MSE as a function of training
set size, and Sollich (2001) extended their results to the setting in which the Gaussian processes’
eigenvalues can differ, but no similar results from this era described the fully general setting.
The discovery of the equivalence between wide neural networks and kernel regression via the NTK
(Jacot et al., 2018; Lee et al., 2019) sparked a resurgence of interest in the generalization of kernel
regression (Belkin et al., 2018a; 2019b;a; Liang & Rakhlin, 2020; Bietti & Mairal, 2019). Further
noting that both kernel regression and deep learning can generalize well despite perfectly interpo-
lating their training data, Belkin et al. (2018b) argued that “to understand deep learning we need
to understand kernel learning.” Arora et al. (2019a) derived a data-dependent generalization bound
for a wide two-layer architecture involving its infinite-width NTK. Though this bound is a signif-
icant advance over VC-dimension-based bounds inapplicable to overparameterized models, it only
applies to a specific architecture and can be many times greater than the true loss, while our results
predict true loss within small error bars and apply to any infinite-width architecture (and in fact to
any incarnation of kernel regression).
In pioneering work, Bordelon et al. (2020) and Canatar et al. (2021) extended classic results on the
generalization of kernel regression to the fully general case and, using the NTK, confirmed that their
approximate expressions for expected MSE agree even with wide finite deep neural networks. Their
results reveal a simple picture of neural network generalization: as samples are added to the training
set, the network generalizes well on a larger and larger subspace of input functions. The natural
basis for this subspace of learnable functions is the eigenbasis of the NTK, and its eigenfunctions
are learned in descending order of their eigenvalues. The results of the present paper corroborate
and extend this picture of the generalization of kernel regression. Our work chiefly differs from
theirs in that (a) our conservation law is new, (b) we derive and test expressions for generic first- and
second-order statistics of the learned function, not just MSE, and (c) our derivation is quite different
(and, we believe, easier to understand) even when it arrives at the same results.
Our work is related to the well-known observation that neural networks have a “spectral bias” to-
wards representing slowly-varying functions (Valle-Perez et al., 2018; Yang & Salman, 2019); in
particular, the kernel spectrum picture clarifies that this bias is a consequence of the fact that high
NTK eigenmodes are typically slowly-varying. We also note a body of work studying the related
phenomenon that slowly-varying functions are learned first during training (Rahaman et al., 2019;
Xu et al., 2019b;a; Xu, 2018; Cao et al., 2019; Su & Yang, 2019).
2	Theory
2.1	A review of kernel regression
Consider the task of learning an m-element function f : X → Rm given a set of n unique training
points D = {xi}in=1 ⊆ X and their corresponding function values f(D) ∈ Rn×m. To simplify our
analysis, we will let the domain X be discrete with size M ≡ |X | and assume the n training points
are uniformly sampled from X . This choice of discrete domain will permit us to use matrices and
vectors instead of operators and functions in our derivations. By taking M → ∞, our results easily
extend to problems with continuous domain: for example, if M → ∞ and we allow the points in X
to approach a density equal to some desired measure over Rd, we recover the standard setting where
the data are sampled nonuniformly from Rd.
We will use f to denote the function learned by a neural network trained on this dataset. Remarkably,
for an infinite-width neural network optimized via gradient descent to zero training MSE loss, this
learned function is given by
/(X)= K (x, D)K (D, D)-1f (D),	(1)
2
Under review as a conference paper at ICLR 2022
where K : X × X → R is the network’s “neural tangent kernel” (NTK) (Jacot et al., 2018; Lee
et al., 2019), K(D, D) is the “kernel matrix” defined by K(D, D)ij = K(xi, xj), and K(x, D) is
a row vector with components K(x, D)i = K(x, xi).1 We give a brief introduction to the NTK in
Appendix C. Due to its similarity to the normal equation of linear regression, Equation 1 is often
called “kernel regression.”2
Equation 1 holds exactly in the infinite-width limit of fully-connected networks (Lee et al., 2019),
convolutional networks (Arora et al., 2019b), transformers (Hron et al., 2020), and more (Yang,
2019). Moreover, several empirical studies have shown it to be a good approximation for networks
of even modest width (Lee et al., 2019; 2020). Our approach will be to study the generalization
behavior of Equation 1, conjecture that our results also apply to finite networks, and finally provide
strong support for our conjecture with experiments.
Examining Equation 1, one finds that the m indices of f can each be treated separately: the learned
f is equivalent to simply performing kernel regression with each of the m indices as a scalar target
function and then vectorizing the results. For simplicity, then, we hereafter assume m = 1. The
extension to m > 1 is straightforward.
2.2	FIGURES OF MERIT OF f
We will study three measures of the quality of the learned function f. All three will be defined
in terms of the inner product over X : for two functions g, h : X → R, their inner product is
hg, hi ≡ MM Px∈x g(X)h(X).
The first measure of quality is mean-squared error (MSE). For a particular dataset D, MSE is given
by E (D) (f) ≡ hf - f, f - ∕i. Of more interest will be the expected MSE over all datasets of size
n, given by E(f) ≡ ED E (D)(f) . We note that the inner product is taken over all X, including D,
even though f(X) = f(X) for X ∈ D for kernel regression.
In maximizing the similarity of f to f, we typically wish to minimize its similarity to all functions
orthogonal to f. The second measure examines the coefficient in f of one such orthogonal function
to f. Letting g : X → R be a function such that hf, gi = 0, we consider the mean and variance of
the quantity hf, gi. We will derive accurate predictions for this metric of generalization.
Lastly, we introduce a figure of merit quantifying the alignment of f and f, which we call “learn-
ability.” It is given by
^ ^
L(D) (f) ≡ff,	L(f) ≡ ED [L(D)(f)i ,	(2)
hf, fi
where L(D) (f) is the dataset-dependent learnability of the function f (“D-learnability”) and L(f)
is its expectation over random data (“learnability”). Though at first glance these two seem like odd
figures of merit, we will soon show that they have many desirable properties when f is given by
Equation 1: unlike MSE, both are bounded in [0, 1] (Lemma 1d), always change monotonically as
new data points are added (Lemma 1e), are invariant to rescalings of f, and obey a simple conser-
vation law (Theorem 1). Furthermore, expanding the inner product in the definition of E and noting
that E(f) ≥ hf, fi(1 - L(f))2, one can see that low MSE is impossible without high learnability.
We will ultimately derive an accurate approximation for learnability that is substantially simpler
than any known approximation for MSE.
1Naively, Equation 1 is only the expected learned function, and the true learned function will include a
fluctuation term reflecting the random initialization. However, by storing a copy of the parameters at t = 0 and
redefining ft(x) := ft(x) - f0(x) throughout optimization and at test time, this term becomes zero, and so we
neglect it in our theory and use this trick in our experiments.
2Interestingly, exact Bayesian inference for infinite-width neural networks yields predictions of the same
form as Equation 1, with K being the “neural network Gaussian process” (NNGP) kernel instead of the NTK
(Lee et al., 2018). We will proceed treating K as a network’s NTK, but our theory and exact results (including
our “no-free-lunch” theorem) apply equally well to any incarnation of kernel regression, including RBF kernel
regression and linear regression.
3
Under review as a conference paper at ICLR 2022
2.3	The kernel eigensystem
By definition, any kernel function is symmetric and positive-semidefinite (Shawe-Taylor et al.,
2004). This implies that we can find a set of orthonormal eigenfunctions {φi }iM=1 and nonnega-
tive eigenvalues {λi}iM=1 that satisfy
MM X K(X, XO)φi(XO) = λiφi(x),	hφi, φji = δj.	⑶
M x0∈X
For simplicity (and to ensure that K(D, D) is invertible), we will assume that K is in fact positive
definite and λi > 0, an assumption that will hold in most cases of interest.3,4
We will now translate Equation 1 to this eigenbasis. First we decompose f and f into weighted
sums of the eigenfunctions as
MM
f(X) = Xviφi(x),	f(X) = Xviφi(χ),	3 (4)
i=1	i=1
where V and ^ are vectors of coefficients. Using this notation, MSE is E(D) = (v - ^)2 and
D-Iearnability is L(D) = VTV∕∣v∣2.
Noting that K(X1,X2) = PiM=1 λiφi(X1)φi(X2), we can decompose the kernel matrix as
K(D, D) = ΦT (D)ΛΦ(D), where Φ(D)ij = φi (Xj) is the M × n “design matrix” and
Λ = diag(λι,..., λM) is a diagonal matrix of eigenvalues. The learned coefficients V are then
given by Vi = hφi, f^ = λiφi(D)K(D, D)-1Φt(D)v. Stacking these coefficients into a matrix
equation, we find that
V = ΛΦ(D) (Φt(D)ΛΦ(D))T Φt(D)V = T(D)v,	(5)
where T(D) ≡ ΛΦ(D) (ΦT(D)ΛΦ(D))-1 Φt(D) is an M × M matrix, independent of f, that
fully describes the model’s learning behavior on a training set D. We call this fundamental quantity
the “learning transfer matrix.” If we can determine the statistical properties of this matrix, we will
understand the learning behavior of our model.
We also define the mean learning transfer matrix T ≡ ED T(D) . D-learnability and learnability
are then respectively given by L(D)(f) = VTT(D)V∕∣v∣2 and L(f) = vtTv∕∣v∣2.
2.4 Exact results
The following lemma gives basic properties of the quantities defined above.
Lemma 1. The following properties of T(D), T, L(D), and L hold:
(a)	L(D)(φi) = Ti(iD), and L(φi) = Tii.
(b)	Whenn=0, T = T(D) = 0andL(f) = L(D)(f) =0.
(c)	When n = M, T = T(D) = IM and L(f) = L(D)(f) = 1.
(d)	All eigenvalues of T(D) are in {0, 1}, all eigenvalues of T are in [0, 1], and so
L(f),L(D)(f) ∈ [0, 1].
(e)	Let D+ be D ∪ X, where X ∈ X, X ∈∕ D is a new data point. Then L(D+) (f) ≥ L(D) (f).
(f)	Forany i ∈ {1,…,M},舟T(D) ≥ 0, hence 舟L(D)(Φi) ≥。・
(g)	FOrany i,j ∈ {1,…,M}, i = j,含TjD) ≤ 0 hence 舟L((D (Φj) ≤。.
3By Mercer’s Theorem, one can also find the eigensystem of any kernel on a continuous input space X .
4We do not use the Reproducing kernel Hilbert space (RKHS) formalism in this work, but we note that, by
the Moore-Aronszajn theorem, the kernel K defines a unique RKHS.
4
Under review as a conference paper at ICLR 2022
Property (a) in Lemma 1 formalizes the relationship between the transfer matrix and learnability.
Properties (b-e) together give an intuitive picture of the learning process: the learning transfer ma-
trix monotonically interpolates between zero and IM as the training set grows — adding data never
harms the learnability of any function. Properties (f-g) show that the kernel eigenmodes are in com-
petition: increasing one eigenvalue can only improve the learnability of the corresponding eigen-
function, but can only decrease the learnabilities of all others. We prove Lemma 1 in Appendix
D.
We now present our first major result.
Theorem 1 (“No-free-lunch” theorem for kernel regression). For any complete basis of orthogonal
functions F,
XL(f)= XL(D)(f)=n.	(6)
f∈F	f∈F
The proof, which hinges on the intermediate result that Tr T(D) = n, is given in Appendix E. We
note that this result is stronger than the ordinary “no-free-lunch” theorem for learning algorithms,
which requires averaging over all target functions instead of merely an orthogonal basis (Wolpert,
1996). To understand the significance of this result, consider that one might naively hope to design
a neural kernel that achieves generally high performance for all target functions f . Theorem 1
states that this is impossible: averaged over a complete basis of functions, all kernels achieve the
same learnability. This exact result implies that, because there exist no universally high-performing
kernels, we must instead aim to choose a kernel whose high-eigenvalue modes align well with the
function to be learned. To our knowledge, this is the first exact result quantifying such a tradeoff in
kernel regression or deep learning.
Illustrating a consequence of this tradeoff, the following theorem states that, for any kernel, there will
always be functions poorly-aligned with the kernel’s inductive bias on which the model generalizes
as bad or worse than it would by simply predicting zero on all unseen test points.
Corollary 1 (Negative generalization). There is always at least one eigenfunction φi for which
L(φi) ≤ Lnaive (φi) and E(φi) ≥ Enaive (φi), where Lnaive and Enaive are the learnability and
mean MSE given by a naive, non-generalizing model with predictions given by
fnaive(x) = If X .	⑺
0 x ∈/ D
This theorem follows from Theorem 1. We give a full proof in Appendix E. This negative gener-
alization property will hold for any function with learnability less than or equal to n/M, including
any weighted sum of eigenfunctions with this property.
2.5	DERIVING A CLOSED-FORM EXPRESSION FOR T
We will now derive an approximation for T and the second moments of T(D), ultimately yielding
simple yet accurate expressions for L, E, and hf , gi. We sketch our method and state our results
here and provide a full derivation in Appendix F.
We begin by noting that the expectation in T ≡ ED T(D) is essentially an expectation over a
combinatorially large set of possible design matrices Φ(D), each of which has orthogonal columns.
We replace this with an average over all M × n matrices Φ satisfying ΦT Φ = M1n, which we
write
T = ED ∣ΛΦ(D) (Φt(D)ΛΦ(D))T Φt(D)i ≈ Eφ ∣ΛΦ (ΦtΛΦ)-1 Φt] .	(8)
We can then readily prove by symmetry that the off-diagonal elements of T vanish, leaving the
question of how each diagonal element depends on the set of eigenvalues. To probe this, we consider
adding an (M + 1)-th “test eigenmode” to the problem, augmenting the principal quantities as
Φ+ = * ,	Λ+ = λ λ , T+ = Eφ,φ[λ+Φ+ (Φ+tΛ+Φ+)-1 Φ+ti .	(9)
5
Under review as a conference paper at ICLR 2022
Using the Sherman-Morrison formula, we find that the new mode’s learnability is given by
T+
T(M+1)(M+1)
EΦ,φ
λ
λ+ hφT (ΦTΛΦ)-1 φi-1
EΦ,φ
λ
λ + C (φ,φ
(10)
[φτ (ΦTΛΦ)-1 φ]-1
where C(Φ,φ) ≡
is a nonnegative dataset-dependent constant. For large n
and realistic eigenspectra, C(Φ,φ) is distributed tightly around its mean, and so we can replace it
with a constant, which we call C. We emphasize that C is the same for every eigenmode. Using
Theorem 1 to obtain a constraint on C, we reach our ultimate approximation that
λi	M	λi
Tij = δij  ---,	where C ≥ 0 satisfies T  ----- = n.	(11)
λi + C	i=1 λi + C
Using Lemma 1, we obtain the major result that
1Mλ
L(f ) =扉 XXX λ^+C vi,	(12)
with C given by Equation 11.
A function is thus more learnable the more weight it places in high eigenvalue modes. We show in
Section 3 that Equation 12 is in excellent agreement with experiments using both kernel regression
and trained finite networks.
The learnability of each eigenmode depends critically on the value of C. We now present several
properties characterizing how C depends on n.
Lemma 2. For C satisfying the constraint in Equation 11, with {λi}iM=1 ordered from greatest to
least, the following properties hold:
(a)	C = ∞ when n = 0, and C = 0 when n = M.
(b)	C is strictly decreasing with n.
(c)	C ≤ n-' PM'+ι λi for all' ∈ {0,…，n - 1}∙
(d)	C ≥ λ' (n - 1)forall ` ∈ {n,..., M}.
Properties (a-b), paired with Equation 11, paint a surprisingly simple picture of the learning process:
as the training set grows, C gradually decreases, and eigenmodes are learned high-to-low as C passes
each eigenvalue. Properties (c-d) provide bounds on C, which, in addition to then yielding bounds
on learnability, can be used to furnish an initial guess when numerically solving for C.
2.6	SECOND-ORDER STATISTICS OF T
We now have an expression for the mean of T(D), but many quantities of interest, including MSE,
depend on second-order fluctuations about that mean. In Appendix G, we show how, by taking a
derivative with respect to Λ, we can obtain expressions for the second-order statistics of T(D) and,
as a result, for MSE. Our main second-order result is that
CCVhT(D) T(D)i — Cλiλk (δik%' + δi'δjk- δijδk') Whara „ - χX λm Z13
COv [Ti, , Tk' J = q(% + C).,+ C)(λk+ C)(" + C) , Whereq ≡ m=1 (λm + C )2 . (13)
Using Equation 13, we can study the admixtures of particular spurious modes in the learned function
1-	1 -	1	♦ C P	I	1	■	/	•	.1	t' 1 .1
f . For example, if f = φi and i 6= j , then we find that
E",φj>i	=0,	Ehhf,φj i2i =VarhTjD)i	=	q(% +	C⅞j+C)2 .	(14)
6
Under review as a conference paper at ICLR 2022
We note that E [〈/, φj〉2]
increases with λj , reinforcing our broad conclusion that the model is
biased towards high-eigenvalue modes. Finally, by noting that E(f ) = ED |(T(D) - IM)v|2 , we
can recover the expression of Bordelon et al. (2020) for MSE:
E(f ) = vT Ev,
where Eij
≡δ
≡ ij
nC 1
"q(% + C )2.
(15)
Our experiments corroborate existing evidence that this is an excellent approximation for MSE.
2.7	Nonmonotonic MSE curves
Expanding E for small n, we find that
Eiin=0 = 1,
(16)
The second equation implies that dEni J=。> 0 for all modes i such that λi < (Pj λj')∕(2 Pj λj),
which suggests that, for such low-eigenvalue modes, MSE increases as the training set size grows
from zero. In practice, such low modes are commonplace; in fact, for a bounded kernel on a con-
tinuous input space (for which M → ∞), an infinite number of arbitrarily low modes is inevitable
due to the constraint that Pi λi is bounded5. We therefore expect that there quite often exist func-
tions for which a given neural network yields an MSE nonmonotonic with n. Figure A1 shows that
experiments confirm this surprising first-principles prediction. We emphasize that this is a different,
more generic phenomenon than that noted by Canatar et al. (2021), which required that f was either
noisy or placed weight in unlearnable zero-eigenvalue modes.
3	Experiments
Here we describe experiments confirming our theoretical predictions for exact NTK regression and
wide neural networks. Unless otherwise stated, all experiments used a fully-connected (FC) four-
hidden-layer (4L) ReLU architecture with width 500. Because this model is FC, it has a rotation-
invariant NTK (Lee et al., 2019). These experiments used three distinct input spaces X. For each,
the eigenmodes of X can be grouped into degenerate subsets indexed by k ∈ N, with higher k
corresponding to faster variation in space. In all cases, we find that as k increases, eigenvalues
decrease, in concordance with the widespread belief that neural nets have a “spectral bias” towards
slowly-varying functions (Rahaman et al., 2019; Canatar et al., 2021; Cao et al., 2019). We now
describe these three input spaces. For full experimental details, see Appendix H.
Discretized Unit Circle. The simplest input space we consider is the discretization of the unit
circle into M points, X = {(cos(2πj∕M), sin(2πj∕M)}M=r The eigenfunctions on this domain
are φo(θ) = 1, φk(θ) = √2cos(kθ), and φk(θ) = √2sin(kθ), for k ≥ 1.
Hypercube. The next input space we consider is the set of verticies of the d-dimensional hypercube,
X = {-1, 1}d, giving M = 2d. The eigenfunctions on this domain are the subset-parity functions
s(x) =(-1)sTx, where s ∈ {0, 1}d is a vector indicating the elements of x to which the output is
sensitive (Yang & Salman, 2019). Here we define k = Pi si .
Hypersphere. To illustrate that our results extend easily to continuous input spaces, we consider the
d-sphere Sd = {x ∈ Rd+1 |x2 = d + 1}. The eigenfunctions on this domain are the hyperspherical
harmonics (see, e.g., Frye & Efthimiou (2012); Bordelon et al. (2020)) which group into degenerate
sets indexed by k ∈ N. The corresponding eigenvalues decrease exponentially with k, and so when
summing over all eigenmodes to compute C and q, we simply truncate the sum at kmax = 70.
Figure I5 shows the 4L ReLU NTK eigenvalues on each domain, all of which decrease almost
monotonically with increasing k . In particular, the eigenvalues on the unit circle roughly follow a
power-law decay like the NTK spectra of common image datasets (e.g. Lee et al. (2020)). We now
describe our experiments.
5This constraint follows from the fact that Pi λi =/ Pχ∈χ K(x, x) < ∞.
7
Under review as a conference paper at ICLR 2022
Figure 1: Theoretical predictions closely match the true learnabilities of arbitrary eigen-
functions on diverse input spaces. Each plot shows learnability L(φk) (Equation 11) of several
eigenfunctions as a function of training set size n. Theoretical curves show excellent agreement with
results from exact NTK regression (triangles) and finite nets trained via gradient descent (circles).
Error bars reflect 1σ variation due to random choice of dataset and, for finite nets, random initial-
ization. (A) Learnabilities of sinusoidal eigenfunctions on the unit circle discretized into M = 28
points. Eigenfunctions with higher k have lower eigenvalues and thus require more data to learn. At
n = 28, the training set contains all input points and all functions are thus predicted perfectly. (B)
Learnabilities of subset-parity functions on the vertices of the 8d hypercube. Eigenfunctions with
higher k again have lower eigenvalues and are learned later, with all functions predicted perfectly
at n = 28. The dashed line indicates L = n/M, the learnability of any function with respect to a
random model; L(φ8) falls below this curve, showing that the k = 8 eigenmode generalizes worse
than chance. (C) Learnabilities of hyperspherical harmonics on the continuous 7-sphere S7 . Eigen-
functions with higher k again have lower eigenvalues and are learned later, but the continuous input
space prevents learnability from exactly reaching 1.
FigUre 2: Eigenmode learnability vs. eigenvalue takes a universal functional form. For any
dataset size and input domain, eigenmode learnability closely follows a universal curve λ∕(λ + C)
with one problem-dependent parameter C. Theoretical curves (solid lines) have the same sigmoidal
shape in every panel. True eigenmode learnabilities L(φk) for k ∈ {0, .., 7} for both exact NTK
regression (triangles) and finite networks (circles) exhibit excellent agreement. Vertical dashed lines
indicate C for each learning problem. (A-C) Learnability vs. eigenvalue for eigenmodes of the
unit circle, 8d hypercube, and 7-sphere with n = 8. The k ∈ {4, ..., 7} modes are cut off to the
left of (C). (D-F) Learnability curves with n = 64. Eigenmodes lie higher on each curve than the
corresponding points in (A-C), reflecting greater learnability due to the larger n. (G) All points from
(A-F), rescaled by their respective values of C, lie on the same universal curve "C+ι ∙
Predicting learnability L(D). We use both wide finite nets and exact NTK regression to learn
several eigenmodes on all three domains and compare true learnabilities with our theoretical pre-
dictions. Our theory predicts true generalization behavior quite well (Figure 1). We further show
that the k = 8 mode on the 8d hypercube has worse-than-chance generalization, an inevitable result
of its low eigenvalue and Corollary 1. We note that finite networks and NTK regression give very
similar results, supporting the validity of approximating networks with the NTK.
8
Under review as a conference paper at ICLR 2022
Figure 3: Eigenfunction learnability always sums to the size of the training set. Stacked bar
charts show D-learnability for a particular random D for each of the 10 eigenfunctions over a simple
10-point domain. For all architectures, the total height of each bar is approximately n. (A) As per
Theorem 1, the summed D-learnabilities for exact NTK regression (left bars) are all exactly n, and
those for trained finite nets (right bars) are remarkably close. Stacked bars show D-learnability for
the 10 eigenfunctions, all from the same training set D of n = 3 data points, stacked from top to
bottom in descending order of eigenvalue. A different network architecture was used in each of the
four pairs of columns. As per Lemma 1d, the height of each eigenmode contribution falls in [0, 1].
(B) Same as (A) with n = 6.
Universal form for learnability. We next fix n and plot learnability vs. eigenvalue for eight modes
over each domain. In each case We find L(φ) ≈ λ∕(λ + C), and by rescaling the eigenvalues (a
symmetry which leaves Equation 1 invariant), we see that for any neural network learning problem
With any training set size n, eigenmode learnability alWays lies on one universal curve (Figure 2).
No free lunch for neural networks. We then experimentally confirm that our no-free-lunch result
applies to finite netWorks as Well as kernel regression (Figure 3). We use both models to learn a
function on the discretized unit circle With M = 10 and sum the resulting D-learnabilities of each
eigenmode. Unlike in our other experiments, We only sample D once for each n; even Without the
benefit of averaging over datasets, We find that total D-learnability is alWays conserved.
Nonmonotonic MSE curves. We next confirm our first-principles prediction of nonmonotonic MSE
curves at small n. We plot curves for four eigenmodes on each domain, three of Which Equation 16
predicts Will have positive dE/dn|n=0. MSE is indeed increasing as predicted (Figure A1).
Agreement with narrow networks. Finally, We repeat the experiment of Figure 1B for varying
netWork Width. We find that our theory gives accurate predictions of learnability and MSE even
for netWorks of depth 4 and Width as small as 20 (Figures B2 and B3). This surprising agreement
suggests our theory is a promising starting point for understanding the generalization of practical
neural netWorks.
4	Conclusion
We have presented a first-principles theory of generalization for kernel regression that predicts a
variety of measures of generalization performance. We then empirically demonstrated its predic-
tive accuracy for both exact NTK regression and Wide finite netWorks in the NTK regime. This
theory offers neW insight into these models’ inductive bias and provides a general frameWork for
understanding their learning behavior, opening the door to the principled study of many other deep
learning mysteries.
References
Sanjeev Arora, Simon Du, Wei Hu, Zhiyuan Li, and Ruosong Wang. Fine-grained analysis of op-
timization and generalization for overparameterized tWo-layer neural netWorks. In International
Conference on Machine Learning, pp. 322-332. PMLR, 2019a.
Sanjeev Arora, Simon S. Du, Wei Hu, Zhiyuan Li, Ruslan Salakhutdinov, and Ruosong Wang.
On exact computation With an infinitely Wide neural net. In Advances in Neural Information
Processing Systems (NeurIPS), pp. 8139-8148, 2019b.
Mikhail Belkin, Daniel Hsu, and Partha Mitra. Overfitting or perfect fitting? risk bounds for classi-
fication and regression rules that interpolate. arXiv preprint arXiv:1806.05161, 2018a.
9
Under review as a conference paper at ICLR 2022
Mikhail Belkin, Siyuan Ma, and Soumik Mandal. To understand deep learning we need to under-
stand kernel learning. In International Conference on Machine Learning, pp. 541-549. PMLR,
2018b.
Mikhail Belkin, Daniel Hsu, Siyuan Ma, and Soumik Mandal. Reconciling modern machine-
learning practice and the classical bias-variance trade-off. Proceedings of the National Academy
of Sciences, 116(32):15849-15854, 2019a.
Mikhail Belkin, Alexander Rakhlin, and Alexandre B Tsybakov. Does data interpolation contra-
dict statistical optimality? In The 22nd International Conference on Artificial Intelligence and
Statistics, pp. 1611-1619. PMLR, 2019b.
Alberto Bietti and Julien Mairal. On the inductive bias of neural tangent kernels. arXiv preprint
arXiv:1905.12173, 2019.
Blake Bordelon, Abdulkadir Canatar, and Cengiz Pehlevan. Spectrum dependent learning curves in
kernel regression and wide neural networks. In International Conference on Machine Learning,
pp. 1024-1034. PMLR, 2020.
Abdulkadir Canatar, Blake Bordelon, and Cengiz Pehlevan. Spectral bias and task-model alignment
explain generalization in kernel regression and infinitely wide neural networks. Nature communi-
cations, 12(1):1-12, 2021.
Yuan Cao, Zhiying Fang, Yue Wu, Ding-Xuan Zhou, and Quanquan Gu. Towards understanding the
spectral bias of deep learning. arXiv preprint arXiv:1912.01198, 2019.
Christopher Frye and Costas J Efthimiou. Spherical harmonics in p dimensions. arXiv preprint
arXiv:1205.3548, 2012.
Jiri Hron, Yasaman Bahri, Jascha Sohl-Dickstein, and Roman Novak. Infinite attention: NNGP and
NTK for deep attention networks. In International Conference on Machine Learning (ICML),
volume 119 of Proceedings of Machine Learning Research, pp. 4376-4386. PMLR, 2020.
Arthur Jacot, Clement Hongler, and Franck Gabriel. Neural tangent kernel: Convergence and gener-
alization in neural networks. In Advances in Neural Information Processing Systems (NeurIPS),
pp. 8580-8589, 2018.
Jaehoon Lee, Yasaman Bahri, Roman Novak, Samuel S. Schoenholz, Jeffrey Pennington, and Jascha
Sohl-Dickstein. Deep neural networks as gaussian processes. In International Conference on
Learning Representations (ICLR). OpenReview.net, 2018.
Jaehoon Lee, Lechao Xiao, Samuel S. Schoenholz, Yasaman Bahri, Roman Novak, Jascha Sohl-
Dickstein, and Jeffrey Pennington. Wide neural networks of any depth evolve as linear models
under gradient descent. In Advances in Neural Information Processing Systems (NeurIPS), pp.
8570-8581, 2019.
Jaehoon Lee, Samuel S. Schoenholz, Jeffrey Pennington, Ben Adlam, Lechao Xiao, Roman Novak,
and Jascha Sohl-Dickstein. Finite versus infinite neural networks: an empirical study. In Advances
in Neural Information Processing Systems (NeurIPS), 2020.
Aitor Lewkowycz, Yasaman Bahri, Ethan Dyer, Jascha Sohl-Dickstein, and Guy Gur-Ari. The large
learning rate phase of deep learning: the catapult mechanism. CoRR, abs/2003.02218, 2020.
Tengyuan Liang and Alexander Rakhlin. Just interpolate: Kernel “ridgeless” regression can gener-
alize. The Annals of Statistics, 48(3):1329-1347, 2020.
Roman Novak, Lechao Xiao, Jiri Hron, Jaehoon Lee, Alexander A. Alemi, Jascha Sohl-Dickstein,
and Samuel S. Schoenholz. Neural tangents: Fast and easy infinite neural networks in python.
CoRR, abs/1912.02803, 2019.
Nasim Rahaman, Aristide Baratin, Devansh Arpit, Felix Draxler, Min Lin, Fred Hamprecht, Yoshua
Bengio, and Aaron Courville. On the spectral bias of neural networks. In International Confer-
ence on Machine Learning, pp. 5301-5310. PMLR, 2019.
10
Under review as a conference paper at ICLR 2022
John Shawe-Taylor, Nello Cristianini, et al. Kernel methods for pattern analysis. Cambridge uni-
versity press, 2004.
Jascha Sohl-Dickstein, Roman Novak, Samuel S Schoenholz, and Jaehoon Lee. On the infinite width
limit of neural networks with a standard parameterization. arXiv preprint arXiv:2001.07301,
2020.
Peter Sollich. Learning curves for gaussian processes. Advances in neural information processing
systems,pp. 344-350,1999.
Peter Sollich. Gaussian process regression with mismatched models. arXiv preprint cond-
mat/0106475, 2001.
Lili Su and Pengkun Yang. On learning over-parameterized neural networks: A functional approxi-
mation perspective. arXiv preprint arXiv:1905.10826, 2019.
Guillermo Valle-Perez, Chico Q Camargo, and Ard A Louis. Deep learning generalizes because the
parameter-function map is biased towards simple functions. arXiv preprint arXiv:1805.08522,
2018.
F Vivarelli and M Opper. General bounds on bayes errors for regression with gaussian processes.
Advances in neural information processing systems, 11:302-308, 1999.
David H Wolpert. The lack of a priori distinctions between learning algorithms. Neural computation,
8(7):1341-1390, 1996.
Zhi-Qin John Xu, Yaoyu Zhang, Tao Luo, Yanyang Xiao, and Zheng Ma. Frequency principle:
Fourier analysis sheds light on deep neural networks. arXiv preprint arXiv:1901.06523, 2019a.
Zhi-Qin John Xu, Yaoyu Zhang, and Yanyang Xiao. Training behavior of deep neural network in
frequency domain. In International Conference on Neural Information Processing, pp. 264-274.
Springer, 2019b.
Zhiqin John Xu. Understanding training and generalization in deep learning by fourier analysis.
arXiv preprint arXiv:1808.04295, 2018.
Greg Yang. Tensor programs I: wide feedforward or recurrent neural networks of any architecture
are gaussian processes. CoRR, abs/1910.12478, 2019.
Greg Yang and Hadi Salman. A fine-grained spectral perspective on neural networks. arXiv preprint
arXiv:1907.10599, 2019.
11
Under review as a conference paper at ICLR 2022
A Nonmonotonic MSE curves at small training set size
6d Hypercube
Figure A1: Our theory correctly predicts that, for low-eigenvalue eigenfunctions, MSE Coun-
terintuively increases as points are added to a small training set. (A-C) Generalization MSE of
exact NTK regression (triangles) and finite networks (circles) when learning four different eigen-
modes on each of three different domains given n training points. Theoretical curves closely match
experimental data. Eigenmodes with higher k have lower eigenvalues and thus higher mean MSEs,
and for k ∈ {2, 4, 6}, MSE even increases as n increases from zero. Dashed lines show dE/dn|n=0
as predicted by Equation 16.
B Experimental results for narrow networks
Figure B2:	Theoretical learnability predictions remain accurate even for quite narrow net-
works of moderate depth. Plots show learnability vs. training set size for four eigenmodes of the
8d hypercube, learned with a 4L ReLU net with various widths. Except for changing width, these
experiments are identical to those of Figure 1b. Theoretical predictions (solid curves) are the same
in all plots. Dashed lines show learnability from a naive, nongeneralizing model; points below the
line imply worse-than-chance generalization (see Corollary 1). (A) Infinite-width results using ex-
act NTK regression. (B-F) Results for successively narrower finite networks. As width decreases,
mean learnability increases slightly, and 1σ error grows. Despite this, mean learnabilities remain
remarkably close to our theoretical predictions even at width 20.
12
Under review as a conference paper at ICLR 2022
Figure B3:	Theoretical MSE predictions remain accurate even for quite narrow networks.
Plots show MSE vs. training set size for four eigenmodes of the 8d hypercube, learned with a 4L
ReLU net with various widths. Except for changing width and the fact that MSE is plotted instead
of learnability, these experiments are identical to those of Figure 1b. Theoretical predictions (solid
curves) are the same in all plots. Dashed lines show MSE from a naive, nongeneralizing model;
points above the dashed lines imply worse-than-chance generalization (see Theorem [THEOREM]).
(A) Infinite-width results using exact NTK regression. (B-F) Results for successively narrower finite
networks. As width decreases, MSE tends to increase, but only slightly. Theoretical predictions
remain remarkably accurate for most eigenmodes down to width 50, with qualitative agreement
even at width 20.
C	Review of the NTK
In the main text, we assume prior familiarity with the NTK, using Equation 1 as the starting point of
our derivations. Here we provide a definition and very brief introduction to the NTK for unfamiliar
readers. For derivations and full discussions, see Jacot et al. (2018) and Lee et al. (2019).
Consider a feedforward neural network representing a function fθ : X → R, where θ is a parameter
vector. Further consider one training example x with target value y and one test point x0 and suppose
we perform one step of gradient descent with a small learning rate η with respect to the MSE loss
`θ ≡ (fθ(x) - y)2. This gives the parameter update
.	.	. .	_ _	__ .	, O	__ O ,	.
θ → θ + δθ, With δθ = -ηVθ'θ = -2η(f- y)Vθfθ(x).	(17)
-c-r τ	∙ 1 , 1	1 ,1 ∙	,	1 , 1	3∕∕∖ EF	1 ∙ ∙ 1 , ∏
We now wish to know how this parameter update changes fθ (x0). To do so, we linearize about θ,
finding that
fθ+δθ(x0) = fθ(x0) + Vθ fθ(x0) ∙ δθ + O(δθ2)
=fθ(x0) - 2η(f - y) [Vθfθ(x) ∙Vθfθ(x0)i + O(δθ2)	(18)
=fθ(x0) - 2η(f - y)K(x, x0) + O(δθ2),
13
Under review as a conference paper at ICLR 2022
where We have defined K(x,x0) ≡ Vθfθ (x) ∙ Vθfθ(x0). This quantity is the NTK. Remarkably,
as network width6 goes to infinity, the O(δθ2) corrections become negligible, and K(x, x0) is the
same after any random initialization7 and at any time during training. This dramatically simplifies
the analysis of network training, allowing one to prove that after infinite time training on MSE loss
for an arbitrary dataset, the network’s learned function is given by Equation 1. See, for example,
Equations 14-16 of Lee et al. (2019)8.
D	Proof of Lemma 1
Property (a): L(D)(φi) = Ti(iD), and L(φi) = Tii.
Proof. Using the fact that hφi, φi) = 1, we see that L(D)(φi) = hφi, φii = eTT(D)ei = T(D),
where ei is a one-hot M -vector with the one at index i. The second clause of the property follows
by averaging.
Property (b): When n = 0, T = T(D) = 0 and L(f) = L(D) (f) = 0.
Proof. When n = 0, Φ(D) has no columns, and thus T(D) = 0. The other clauses follow from
Property (a) and averaging.
Property (c): When n = M, T = T(D) = IM and L(f) = L(D) (f) = 1.
Proof. When n = M, Φ(D) is a full-rank M × M matrix. Inspection of the formula for T(D) then
shows that T(D) = IM. The other clauses follow from Property (a) and averaging.
Property (d): All eigenvalues of T(D) are in {0, 1}, all eigenvalues of T are in [0, 1], and so
L(f),L(D)(f)∈[0,1].
Proof. From the definition of T(D), it is easy to see that T(D) = (T(D))2. T(D) is thus idempotent,
with all eigenvalues in {0, 1}. The fact that all eigenvalues of T are in [0, 1] follows by averaging.
The stated properties of L(f), L(D)(f) follow from the fact that, for any compatible vector z and
zTAz
matrix A, it holds that ZzTAz is bounded by the maximum and minimum eigenvalues of A.
Property (e): Let D+ be D∪x, where x ∈ X, x ∈/ D is a new data point. Then L(D+) (f) ≥ L(D) (f).
Proof. To begin, we use the Moore-Penrose pseudoinverse, which we denote by (∙)+, to cast T(D)
into a more transparent form:
T(D) ≡ ΛΦ (ΦtΛΦ)-1 Φt = A1/2 (λ1分ΦΦtA1/2) (λ1分ΦΦtA1/2)+ Λ-1/2,	(19)
where we have suppressed the D in Φ(D). This follows from the property of pseudoinverses that
A(ATA)+AT = (AAT)(AAT)+ for any matrix A. We now augment our system with one extra
data point, getting
T(D+) = a1/2 (λ1分(ΦΦt + ξξτ)A1/2) (λ1∕2(ΦΦt + ξξτ)A1/2)+ Λ-1∕2,	(20)
where ξ is an M -element column vector orthogonal to the others of Φ. We now convert the pseu-
doinverse into an inverse with a limit, getting
T(D+)= lim Λ1/2 (Λ1/2(ΦΦT + ξξτ)A1/2) (λV2(ΦΦt + ξξτ)Λ1/2 + SIm)-1 Λ-1/2.
δ→0+
(21)
6The “width” parameter varies by architecture; for example it is the minimal hidden layer width for fully
connected networks and the minimal number of channels per hidden layer for a convolutional network.
7(assuming the parameters are drawn from the same distribution)
8We note that there exists a different infinite-width kernel, called the “NNGP kernel,” describing a network’s
random initialization, and this reference uses K for the NNGP kernel and Θ for the NTK.
14
Under review as a conference paper at ICLR 2022
We now use the Sherman-Morrison matrix inversion formula to find that
T(D+) = T(D) + lim δ
δ→0+
(Λ"ΦΦtΛ1/2 + SIm)-1 ξξτ (Λ"ΦΦtΛ1/2 + SIm)-1
1 + ξτ (ΛV2ΦΦTΛ1/2 + δIM)-1 ξ
(22)
Because both ξξτ and the inverted matrix in Equation 22 are positive semidefinite, we conclude
that, for any M -vector z, it will hold that zτT(D+)z ≥ zτ T(D)z. The desired property follows.
Property (f): For any i ∈ {1,…，M},长T(D) ≥ 0, hence 长L(D) (Φi) ≥ 0∙
(D)
Proof. Differentiating Tjj with respect to a particular λi , we find that
dλT(D) = (δij - λjΦTKTφi)φTK-1φj,	(23)
where φiτ is the i-th row of Φ and K = Φτ ΛΦ. Specializing to the case i = j, we note that
φiTK-1φi ≥ 0 because K is positive semidefinite, and λjφiK-1φiT ≤ 1 because λjφiφiT is one
of the positive semidefinite summands in K = Pk λkφkφkT. The desired property follows.
Property (g): For any i,j ∈ {1,…,M}, i = j,去TjD) ≤ 0, hence 急L(D) (Φj) ≤ °∙
Proof. Differentiating as in the proof of Property (f) and using the fact that i 6= j , we see that
dλ TjD) = f(ΦT κ-1Φi)2,
i
(24)
which is manifestly nonpositive because λj > 0. The desired property follows.
E	Proofs of Theorem 1 and Corollary 1
Here we prove Theorem 1, our “no-free-lunch” result, and Corollary 1, which states the existence of
eigenfunctions that generalize worse than chance. We begin with the former.
Proof of Theorem 1. First, we note that, for any orthogonal basis F on X,
L(D)(f)=
f∈F	v∈V
vT T(D)v
vτ V
(25)
where V is an orthogonal set of vectors spanning RM . This is equivalent to Tr T(D) . This trace is
given by
Tr T(D) = Tr ΦT ΛΦ(ΦT ΛΦ)-1 = Tr[In] = n,
(26)
which proves the desired theorem. □
Proof of Corollary 1. First, we note that the naive, nongeneralizing model described in the theorem
will always have a learnability of Lnaive(f) = ED [MffJ = M. As per Theorem 1, this is exactly
the mean learnability of all eigenfunctions, so either all have precisely this learnability or else one
has lower learnability. This proves the clause of the theorem specific to learnability.
To prove the clause specific to MSE, we now note that, as defined by Equation 1, kernel regression
will always perfectly memorize the training data, so for both the naive model and kernel regression,
MSE is given by
15
Under review as a conference paper at ICLR 2022
E(f) = MM X (f(x)-/(χ))2 = M X f2(χ)+M X f(χ)-M X f(χ)f(χ). (27)
x∈X \D
x∈X \D	x∈X \D	x∈X \D
The first term is the same for any model, the second term is nonnegative but zero for the naive
model, and the third term is equivalent to -2(L(f) 一 M), which must be nonnegative for at least
one eigenfunction. Kernel regression therefore gives worse MSE than the naive model unless, as for
the naive model, f (x) = 0 for x ∈ D. □
F APPROXIMATING T
Here we provide details of the derivations of our approximation for T and of Lemma 2. Whenever
possible, we leave it implicit that all expressions for T in this appendix will be approximations and
use the symbol for equality for simplicity.
We begin by taking the approximation of Equation 8 that
T ≈ Eφ ∣ΛΦ (ΦtΛΦ)-1 Φt∣ ,	(28)
where the expectation is taken over all M × n matrices Φ satisfying ΦTΦ = M1n with the uniform
measure. It turns out that we can equivalently average over all all M × n matrices Φ with a mean-
zero i.i.d. Gaussian measure over each element, without changing T. To see this, note that this
equation is symmetric under right-multiplication of Φ by arbitrary n × n invertible matrices R:
ΛΦ (ΦtΛΦ)-1 Φt = ΛΦR (RTΦtΛΦR)-1 RTΦt.	(29)
Letting R always be the (Φ-dependent) matrix that orthogonalizes the columns of Φ (i.e.
ΦTRTRΦ = MIn ), one can see that, no matter the distribution of Φ in Equation 28, it holds
that Eφ ∣ΛΦ (ΦtΛΦ)-1 Φt∣ = Eφ ∣ΛΦo (ΦTΛΦo)-1
ΦoT , where Φo is the orthogonalized
version of Φ. The expectation can thus equivalently be taken over any distribution that induces
the same distribution over the orthogonalized Φo ; in particular, the uniform measure on orthogonal
Φ can be interchanged with the standard Gaussian measure. Moving forward, we will exploit this
equivalence and assume that each element of Φ is sampled i.i.d. from N (0, 1). This assumption
will largely remain in the background, but will be useful later.
We now evaluate T, starting with the off-diagonal elements. We observe that
Eφ ∣ΛΦ (ΦtUTΛUΦ)-1 Φt∣ = Eφ [λUtΦ (ΦtΛΦ)-1 ΦtU∣ ,	(30)
where U is any orthogonal M × M matrix. Defining U(m) as the matrix such that U(amb ) ≡ δab(1-
2δam), noting that U(m)ΛU(m) = Λ, and plugging U(m) in as U in Equation 30, we find that
Tab =	U(m)T TU(m)	= (-1)δam+δbmTab.	(31)
By choosing m = a, we conclude that Tab = 0 if a 6= b.
To evaluate the diagonal elements of T, we consider augmenting our eigensystem with a new “test
eigenmode” with eigenvalue λ, as described in the text and formalized in Equation 9. To proceed,
we make the core assumption that Tii ≈ Ti+i when n, M 19. This assumption, combined with
the symmetry T+ = Tjj whenever λi = λj, implies that Tii = T+M +i)(m+i)∣λ=λi∙ Therefore, to
evaluate T, it suffices to evaluate T(+M +1)(M +1) as a function of λ.
9Empirically, we see that this approximation holds already for realistic values of n and M used in neural
net experiments.
16
Under review as a conference paper at ICLR 2022
We now manipulate the expression for T+ to isolate T(+M+1)(M+1) . Using the Sherman-Morrison
matrix inversion formula, we obtain
Λ+Φ+
-1
(ΦT ΛΦ + λφφT) 1
(ΦT ΛΦ)-1 -
λ (ΦTΛΦ)-1 φφT (ΦTΛΦ)-1
1 + λφT(ΦT ΛΦ)-1 φ
(32)
and inserting this into the full expression for T+ gives
T(+M+1)(M+1) = EΦ,φ λφT Φ+T ΛΦ+ + λφφT -1 φ
EΦ,φ
λφT (ΦTΛΦ)-1 φ -
λ2 [φτ (ΦTΛΦ)-1 φ]
1 + λφT(ΦT ΛΦ)-1 φ
(33)
EΦ,φ
EΦ,φ
λ+ hφT (ΦTΛΦ)-1 φi-1
λ 一
λ + C (φ,φ) ,
λ
where C(Φ,φ) ≡
[φτ (ΦTΛΦ)-1 φ]-1
is a nonnegative scalar.
We now argue that, for realistic values of M, n, and {λi }i, we can simply replace C(Φ,φ) by its
mean. We first note that, because λ∕(λ + C) is convex, We can use Jensen's inequality to find that
Eφ,φ]λττ"≥ 击，	(34)
where we have defined C ≡ Eφ,φ [C(φ,φ)]. Some algebra also yields the fact that
λ / λ	λ(C(φ,φ) - C)	(C(φ,φ) - C)2	ɔ
λ + C(φ,φ) ≤ λ + C	(λ + Cy — + —(λ + C)2 —,	(35)
for C(Φ,φ) ≥ 0, which, after taking an expectation, gives us the upper bound that
E J λ ]< λ	Varφ,φ [C (φ,φ)]
Eφ,φ [λ + C(φ,φ)] ≤ λ+C + -(λ + Cy-
< λ	Varφ,φ [C (φ,φ)]
≤ λ + C +	C2
(36)
Armed with the bounds of Equation 34 and Equation 36, we can replace C(φ,φ) by C in the last
line of Equation 33 if we can show that Varφ,φ [C(φ,φ)]《C2. This will be nontrivial to show
in general, but we can easily see that it holds in a caricature of a realistic problem. In practice, we
often find that the NTK spectrum is dominated by a few large eigenvalues with a long tail of small
eigenvalues (Figure I5). To model this sort of spectrum, let p ∈ N with p n M, and suppose
λ0	if1 ≤ i ≤p
[M⅛	if P +1 ≤ i ≤ M
(37)
17
Under review as a conference paper at ICLR 2022
with λo》λ0∕(M — p). The kernel matrix is given by
p	M λ0	p
ΦtΛΦ = X λoφiφT + X ———φiφT ≈ X λoφiφT + λ0In,	(38)
M-p
i=1	i=p+1	i=1
where φi is the i-th row of Φ and we have made an approximation using the central limit theorem
assuming that Φ is random with entries sampled i.i.d. from N(0, 1). This kernel matrix has p large,
O(λ0) eigenvalues and n - p remaining eigenvalues of order λ0. Its inverse therefore has p small
eigenvalues and n-p similar large ones of order 1∕λ0. Letting {ξi}n=ι be these inverted eigenvalues,
we have that
C (Φ,φ)
1	_	1
φτ(ΦTΛΦ)-1 φ = Pn=I ξix2
(39)
where {xi}in=1 are the components of φ on each of the kernel eigendirections. For typical {xi}in=1
sampled i.i.d. from N (0, 1), the denominator approaches (n - p)λ0 by the central limit theorem, and
thus we reach our desired conclusion that C(Φ,φ) ≈ 1/((n - p)λ0) is approximately deterministic.
More precisely, we find
EΦ,ΦhC(φ,φ)i ≈ F-⅛, VarΦ,φ[C(φ,φ)i ≈ (n - p33(λ0)2 ,
from which we see that
VarΦ,φ[C (φ,φ)]	3
C2	≈ n-p,
which approaches zero when n	p, as desired.
(40)
(41)
This argument used a somewhat unrealistic kernel eigenspectrum, but numerical experiments show
that C(Φ,φ) is approximately deterministic even for real eigenspectra (Figure F4). We leave the
analysis of C(Φ,φ) for realistic kernels for future work.
‰⅛^s⅛>
IO2 ：
IO1 ：
IO0 ：
10-1 ：
ιo-2-
ιoo ...............ioɪ ...........ιb2
∩
Figure F4: For NTK eigenvalues on the 7-sphere, Var [C(φ,φ)]《C for large n. The statistic
Var [C(φ,φ)] /C → 0 is calculated for various n using 100 samples of Φ, φ with the 7-sphere
eigenvalues up to k = 6.
Replacing C(Φ,φ) by a constant C yields our final approximation for T, given in Equation 11. We
note that, though this constant C is roughly the mean of C(Φ,φ), we circumvent deriving an explicit
expression for this mean by instead using Theorem 1 to fix C.
F.1 Proof of Lemma 2
Here we provide a proof of the Lemma 2, which states several properties of the constant C defined
by Equation 11. As stated in the Lemma, these properties assume {λi}iM=1 ordered from greatest to
least.
18
Under review as a conference paper at ICLR 2022
Property (a): C = ∞ when n = 0, and C = 0 when n = M .
Proof. Because PMI 入.：。is strictly decreasing with C for C ≥ 0, there can only be one solution
for a given n. The first statement follows by inspection, and the second follows by inspection given
our assumption that all eigenvalues are strictly positive.
Property (b): C is strictly decreasing with n.
Proof. Differentiating the constraint on C with respect to n yields
X	-λ	dC = 1	(42)
=(λi + C)2 dn ,	( )
giving
dC = — "XX 7λ^⅛] I S	(43)
dn i=1 (λi + C )
Property (c): C ≤ n-' PiML+1 λi for all' ∈ {0,..., n - 1}.
Proof. n = PMI λλ+c ≤ ' + PM'+ι C. The desired property follows.
Property (d): C ≥ λ' (* - 1) for all' ∈ {n,..., M}.
Proof. We consider replacing λ% with λ' if i ≤ ' and 0 if i > '. Noting that this does not increase
any term in the sum, we find that n = PNI H ≥ P'=1 ⅛ = λi+⅛. The desired property
follows. 口
G SECOND-ORDER STATISTICS OF T(D)
G.1 Preliminaries
Here we derive expressions for the second-order statistics of T(D) . These derivations make no
further approximations beyond those already made in approximating T.
We begin a calculation that will later be of use. Differentiating both sides of the constraint on C in
Equation 11 with respect to a particular eigenvalue, we find that
d X	λj
dλi j=1 λj + C
M
X
j =1
-λj	dC
(λj + C )2 dλi
C
(λi + C )2
0,
(44)
+
yielding that
dC	C
---=----:-----TT
dλi q(λi + C )2
Mλ
where q ≡ jg j⅛.
(45)
We now factor T(D) into two matrices as
T(D) = ΛZ,	where Z ≡ Φ (ΦtΛΦ)-1 Φt.	(46)
Unlike T(D), the matrix Z has the advantage of being symmetric and containing only one factor of
Λ. Our approach will be to study the second-order statistics of Z, which will trivially give these
statistics for T(D). From Equation 11, we can approximate the mean of Z as
EΦ[Z] = (Λ+CIM)-1.
(47)
19
Under review as a conference paper at ICLR 2022
We also define a modified matrix Z(U) ≡ Φ (ΦTUTΛUΦ) 1 Φt, where U is an orthogonal M X
M matrix. Because the measure over which Φ is averaged is rotation-invariant, we can equivalently
average over Φ ≡ UΦ with the same measure, giving
Eφ∣Z(U)i = Eφ	UTΦ	(ΦTΛΦ)-1	ΦTU	=	Eφ[UtZU]	=	UT(Λ +	CIM)-1U.	(48)
It is similarly the case that
Eφ [(Z(U))ij(Z(U))k`] = Eφ [(UTZU)ij (UTZU) J .	(49)
Our aim will be to calculate expectations of the form Eφ[ZjZk']. With a clever choice of U,
we can now see that most choices of the four indices will make this expression zero. We define
U(amb ) ≡ δab(1 - 2δam) and observe that, because Λ is diagonal, (U(m))TΛU(m) = Λ and thus
Z(U(m)) = Z. Equation 49 then yields that
Eφ[ZijZk'] = (-1)δim+δjm+δkm+δ'mEφ[ZijZk'],	(50)
from which it follows that Eφ [Zj Zk'] = 0 if any index is repeated an odd number of times. In light
of the fact that Zij = Zji , there are only three distinct nontrivial cases to consider:
1.	EΦ [Zii Zii],
2.	EΦ[ZijZij] with i 6= j, and
3.	EΦ [Zii Zjj] with i 6= j.
We stress that we are not using the Einstein convention of summation over repeated indices.
G.2 Case studies
Cases 1 and 2. We now consider differentiating Z with respect to a particular element of the matrix
Λ. This yields
ɪi' = -φT (φTλφ) 1 φjφT (φTλφ) 1 φ' = -ZijZk`,	(51)
dΛjk
where φi is the i-th row of Φ. This gives us the useful expression that
Eφ[ZijZk'] = --d-Eφ[Z] .	(52)
dΛjk
We now set ` = i and evaluate this expression using Equation 47, concluding that
Eφ[ZijZij] = Eφ [ZijZji] = -dλ~ (λ⅛) = (λi + C)2 (δij + q(λj+ C)2 ) ,	(53)
CovΦ [Zij, Zij] = CovΦ [Zij, Zji]
C
q(% + C)2(λj + C)2 .
(54)
We did not require that i 6= j , and so Equation 53 holds for Case 1 as well as Case 2.
Case 3. We now aim to calculate EΦ [ZiiZjj] with i 6= j. We might hope to use Equation 52
in calculating EΦ [ZiiZjj], but this approach is stymied by the fact that we would need to take
a derivative with respect to Λij we only have an approximation for Z for diagonal Λ. We can
circumvent this by means of Z(U). From the definition of Z(U), we find that
20
Under review as a conference paper at ICLR 2022
d_____d_) Z(U)
dUij	dUji	U=I
=-φT (ΦTΛΦ)-1 [φjλiφT - φiλjφτ + φiλiφT - φjλjφτ] (ΦTΛΦ)-1 φj
= (λj- λi)(Zj+ ZiiZjj) . (55)
Differentiating with respect to both Uij and Uji with opposite signs ensures that the derivative is
taken within the manifold of orthogonal matrices. Now, using Equation 48, we find that
d d
VdUij
-d⅛) Eφ[Z(U) i
U=IM
UT(Λ+CIM)-1UU=IM
1	1
ʌi + C	λj + C
(56)
Taking the expectation of Equations 55, plugging in Equation 53 for the squared off-diagonal ele-
ment, comparing to 56, and performing a bit of algebra, we conclude that
EΦ [Zii Zjj]
1C
---------:~:-------- — —:--------. „ ,------~~r
(λi + C )(λj + C)	q(λi + C)2(λj + Cy
(57)
and that Zii , Zjj are anticorrelated with covariance
covφ[Zii, Zjj] = - q(λi + C)2(λj + C)2 .
(58)
With the use of Kronecker deltas, we can combine Equations 54 and 58 into one expression covering
all cases. As can be verified by case-by-case evaluation, one such expression is
C 「7	7 ]	C (δikδj' + δi'δjk — δijδk')
C"Zij，" = q(λ + C)(λj + C)(λk+C)(" + C).
(59)
Using the fact that Ti(jD) =λiZij, we obtain the elementwise covariances of T(D) given in Equation
13.
G.3 Deriving an expression for MSE
MeanMSEis given by E(f) = E[(V - v)2] = VTE[(T(D) - IM)t(T(D) - IM)] V = VTEv,
where V and V are the eigenfunction coefficients of the true and learned functions and We have
defined E ≡ E [(T(D) - IM)T (T(D) - IM)]. We can derive Equation 15 as follows:
21
Under review as a conference paper at ICLR 2022
M
XEhT(kDi)T(kDj)i - EhTj(Di)i - EhTi(jD)i +δij
k=1
δij [^X 77~^kΓc2∕2 (而 +----C _Lm 2 ) - 2	+ 1
k=1 (λk + C)2	q(λi + C)2	λi + C
…λ⅛ )2十 q(λ‰ X (λkλkC -、)#
δij "(λ^CC) + gʤz - q(λqCC)2_
δij nC
q(λi + C)2 .
(60)
Though it is not apparent at a glance, this expression for MSE is in fact equivalent to the main result
of Bordelon et al. (2020) with a ridge parameter of zero. To obtain their expression from ours,
substitute n → p, C → t/p, q → p2∕t — p3γ∕t3, and Vi → λ1∕2wi. Given that our derivations
are quite different — they provide one using the method of characteristics and one using the replica
trick from statistical mechanics that both directly yield final approximate expressions, while ours
first derives an approximation with one unknown parameter C and then later determines C with the
no-free-lunch theorem — it is interesting that we nonetheless recover their results. We note that our
results for other second-order statistics besides MSE did not appear in this or any other prior work.
G.4 Second-order statistics can be written strictly in terms of learnability
As a final theoretical note, we observe that our expressions for transfer matrix covariances and MSE
can all be written strictly in terms of the modewise learnabilities Li ≡ 入.：。.The fact that this is
possible supports our claim that modewise learnabilities are quantities of fundamental theoretical
importance to kernel regression. We begin by noting that
MM
n = X Lm	and Cq = X Lm(1 - Lm).	(61)
m=1	m=1
With these expressions in hand, we can quickly see that
Cov[T(D),TkD)i = LPJLj,Lt-L (δikδj' + δi'δjk- δijδk')	(62)
m=1 Lm(1 - Lm)
and
(63)
These expressions allow one to more clearly see the effects of the model’s inductive bias. For
example, in light of the fact that Li ∈ [0, 1], Equation 62 shows that Var Ti(jD) is larger the
higher the learnability of the “learned mode” i and the lower the learnability of the “target mode”
j, meaning that low-learnability modes tend to be mistaken for high-learnability modes. Since the
model tends to err towards high-learnability modes, we thus expect that the learned function f will
tend to be spectrally biased, placing outsize weight in more learnable modes.
22
Under review as a conference paper at ICLR 2022
One recurring quantity in these expressions is Li (1 - Li), which is zero for Li = 0, 1 and maximal
at Li = .5. Intuitively speaking, it describes the rate at which a mode is “currently being learned”
with increasing n, as can be seen from the fact that
dLi	Li(1 - Li)
---=------TT-------------
dn	PmM=1 Lm (1 - Lm)
(64)
This equation states that a new unit of learnability (i.e. a new data point) is divvied up among all the
eigenmodes in proportion to Li (1 - Li).
H Experimental details
We conduct all our experiments using JAX, performing exact NTK regression with the Neural Tan-
gents library (Novak et al., 2019) built atop it. For the dataset sizes we consider in this paper, exact
NTK regression is typically quite fast, running in seconds, while the training time of finite networks
varies from seconds to minutes and depends on width, depth, training set size, and eigenmode. In
particular, as described by Rahaman et al. (2019), lower eigenmodes take longer to train (especially
when aiming for near-zero training MSE as we do here).
Unless otherwise stated, all experiments used four-hidden-layer ReLU networks initialized with
NTK parameterization (Sohl-Dickstein et al., 2020) with σw = 1.4, σb = .1. The tanh networks
used in generating Figure 3 instead used σw = 1.5. Experiments on the unit circle always used a
learning rate of .5, while experiments on the hypercube and hypersphere used a learning rate of .5
or .1 depending on the experiment. While higher learning rates led to faster convergence, they often
also gave qualitatively different generalization behavior, in line with the large learning rate regimes
described by Lewkowycz et al. (2020). Means and 1σ error bars always reflect statistics from 30
random dataset draws and initializations (for finite nets), except for the nonmonotonic MSE curves
of Figure A1, which used 100 trials. We emphasize that the “no free lunch” experiment of Figure 3
used only a single trial.
23
Under review as a conference paper at ICLR 2022
I NTK eigenvalues on experimental domains
IO0I
Unit Circle
8d Hypercube
-0-2一
10-4
10-1
ιo-2-
ιo^3-
A∙-u=d=3E
O 1
IO
k
IO2
2 10
Ooo
111
7-sphere
ιo-5-
ιo^9-
IOT
O 1
10
k
IO2
Figure I5: Eigenvalues decrease with increasing k for all three domains, the manifestation of
an inductive bias towards smoother functions. All eigenvalues are calculated with the NTK of
a 4L ReLU network as described in the text. (A) NTK eigenvalues for k for the discretized unit
circle (M = 256). Eigenvalues decrease as k increases except for a few near exceptions at high
k. (B) NTK eigenvalues for the 8d hypercube. Eigenvalues decrease monotonically with k. (C)
NTK eigenvalues for the 7-sphere up to k = 70. Eigenvalues decrease monotonically with k . (D)
Eigenvalue multiplicity for the discretized unit circle. All eigenvalues are doubly degenerate (due
to cos and sin modes) except for k = 0 and k = 128. (E) Eigenvalue multiplicity for the 8d
hypercube. (F) Eigenvalue multiplicity for the 7-sphere.
24