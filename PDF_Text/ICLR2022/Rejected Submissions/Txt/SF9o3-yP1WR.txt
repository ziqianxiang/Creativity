Under review as a conference paper at ICLR 2022
Robust and Personalized Federated Learning
with Spurious Features: an Adversarial Ap-
PROACH
Anonymous authors
Paper under double-blind review
Ab stract
A common approach for personalized federated learning is fine-tuning the global
machine learning model to each local client. While this addresses some issues
of statistical heterogeneity, we find that such personalization methods are often
vulnerable to spurious features, leading to bias and diminished generalization
performance. However, debiasing the personalized models under spurious features
is difficult. To this end, we propose a strategy to mitigate the effect of spurious fea-
tures based on our observation that the global model in the federated learning step
has a low accuracy disparity due to statistical heterogeneity. Then, we estimate and
mitigate the accuracy disparity of personalized models using the global model and
adversarial transferability in the personalization step. We theoretically establish
the connection between the adversarial transferability and the accuracy disparity
between the global and personalized models. Empirical results on MNIST, CelebA,
and Coil20 datasets show that our method reduces the accuracy disparity of the
personalized model on the bias-conflicting data samples from 15.12% to 2.15%,
compared to existing personalization approaches, while preserving the benefit of
enhanced average accuracy from fine-tuning.
1	Introduction
Federated learning (FL) is a leading framework for clients to collaboratively train a shared global
machine learning (ML) model without releasing their local private datasets (McMahan et al., 2017;
Kairouz et al., 2019). The jointly trained global model could be further fine-tuned on each client’s
local dataset to produce personalized models (Fallah et al., 2020; T. Dinh et al., 2020; Li et al., 2021).
While existing theoretical and empirical results highlight how personalized models improve accuracy
on local data, few works consider what features the personalized models learn from the local dataset.
Our motivating hypothesis is that not all local features are beneficial.
For example, consider a gender prediction task using face images, where the ML model learns to
predict gender based on hair color because females are more likely to have blond hair (Sagawa et al.,
2020). In this case, the hair color is called a spurious feature because it only statistically correlates
with the gender on biased-aligned samples but not necessarily on the overall population. Thus, the
accuracy of a model that relies on spurious features such as hair color is likely to drop significantly
on bias-conflicting samples where the spurious correlation does not hold, e.g., for blond male images
(Sagawa et al., 2020). This paper calls the accuracy difference of an ML model on the dataset with
spurious features and the dataset without spurious features accuracy disparity. More broadly, the
accuracy disparity caused by spurious features leads to issues in both fairness (McNamara et al.,
2019; Zhao & Gordon, 2019; Agarwal et al., 2019; Chi et al., 2021), i.e., racial bias (Khani & Liang,
2021) and robustness, i.e., accuracy decrease under distribution shift (Zhao et al., 2019; Koh et al.,
2021). Compared to the global model, because of the local fine-tuning, the personalized models are
more vulnerable to spurious features and have a larger accuracy disparity.
Empirically, we observe that the typical non-i.i.d. data distributions in FL settings reduce the accuracy
disparity of the global model. One potential explanation is that the statistical heterogeneity (Wang
1
Under review as a conference paper at ICLR 2022
(a) Spurious Correlation Varies across Users
(b) Gradient Divergence between
Users
Figure 1: The statistical heterogeneity of spurious features leads to diverse gradients. The statistical
heterogeneity leads to a global ML model with a low accuracy disparity in a federated learning
setting because the aggregation step in the central server averages out the gradients resulted from
local spurious features.
et al., 2021) under spurious features across users is larger than that of non-spurious features. As
a result, the aggregation operation in the central server averages out the diverse shifts under the
spurious features (Figure 1) so that the global model becomes more robust against spurious features
on the benchmark datasets. On the other hand, the local fine-tuning step will result in a personalized
model that entangles spurious features and becomes biased. Hence, it remains a key challenge how to
debias the personalized models.
Various methods (Wang et al., 2019; Sagawa et al., 2020; Liu et al., 2021) have been developed
to disentangle spurious features from ML models, but few can be easily applied to personalized
models under the federated learning setting. The main reason lies in their reliance on bias-conflicting
samples. The bias-conflicting samples can be rare. For example, in the CelebA dataset, only around
1.7k samples are bias-conflicting out of more than170k samples. If we distribute the CelebA dataset
across users, nearly half do not have any bias-conflicting samples. Prior work (Li & Wang, 2019)
has tried using a global proxy dataset for training FL models. However, a global proxy dataset
may not characterize the local samples well. Furthermore, estimating the accuracy disparity of the
personalized model becomes difficult without access to bias-conflicting samples.
To approach this problem, we propose a novel method to reduce the accuracy disparity of personalized
models. Our proposed method does not rely on the bias-conflicting samples, e.g., blond male images
in the aforementioned task of gender prediction, which are often rare and may not be available
for every user. Instead, inspired by prior works (Tramer et al., 2017; Liang et al., 2021) on the
transferability of adversarial examples, we use the global model as a reference and the adversarial
transferability between the global model and the personalized models as a proxy to estimate the
accuracy disparity of the personalized models. The intuition is that if two ML models use disjoint
subsets of features, the adversarial examples that one ML model generates do not transfer to the other
ML model. Based on this intuition, we propose the following hypothesis:
If the personalized models entangle spurious features (thus increasing the accuracy
disparity), the adversarial examples generated by the global model (that uses
non-spurious features) do not transfer to the personalized models.
Empirically, we show that the adversarial transferability between the global and personalized models
strongly correlates with the accuracy disparity between the global and personalized models (Section
4), validating our hypothesis. We further theoretically connect the adversarial transferability to
the accuracy disparity (Section 5). Based on the empirical observations and theoretical results, we
develop a method that enforces adversarial transferability between the global and the personalized
models to reduce the accuracy disparity of personalized models (Section 6). Our contributions are
summarized as follows:
•	We empirically evaluate the accuracy disparity of the global and personalized models in a fed-
erated learning setting with spurious features, highlighting a risk of existing personalization
methods.
•	We design a method to estimate the accuracy disparity of the personalized models, based
on the low accuracy disparity global model and the adversarial transferability between the
global and personalized models.
2
Under review as a conference paper at ICLR 2022
•	We theoretically connect the adversarial transferability and the accuracy disparity of the
global and personalized models.
•	We develop a methods to reduce the accuracy disparity of personalized models by enforcing
the adversarial transferability between the global and personalized models.
Empirically, we conduct extensive experiments to validate the effectiveness of the proposed methods
in mitigating accuracy disparity under the FL setting. Our experiments on MNIST (Deng, 2012),
CelebA (Liu et al., 2015), and Coil20 (Nene et al., 1996) datasets show that the proposed approach
reduces the accuracy disparity of personalized models from 15.12% to 2.15%, which is closer to
that of the global model (-0.63%). Our method also preserves the benefit of the enhanced average
accuracy from fine-tuning, resulting in 3.43% accuracy improvement on the biased test set and 0.85%
accuracy improvement on the biased-conflicting test set.
2	Related Work
Personalized Federated Learning Fine-tuning is typical for personalization methods. The meta-
learning-based method first trains a global model and fine-tunes the global model locally (Fallah
et al., 2020). Other methods using multi-task learning (Li et al., 2021) or Moreau envelopes (T. Dinh
et al., 2020) have an interpretation as fine-tuning the local model along with training the global model.
Fine-tuning is also compatible with clustering-based method (Ghosh et al., 2020).
Debiasing ML Models A few prior works (Li & Vasconcelos, 2019; Sagawa et al., 2020) utilize
group labels, which might require human annotation, to debias ML models. For example, the
group distributional robust optimization (DRO) method (Sagawa et al., 2020) aims to optimize
the worst-case error rate of ML models across different (often manually annotated) groups. Some
groups contain bias-conflicting samples while others do not. Residual learning-based methods (He
et al., 2019; Nam et al., 2020; Liu et al., 2021) train a biased ML model and up-weight the residual,
which mainly contains bias-conflicting samples that the biased ML model mis-predicts. Chi et al.
(2021) aims to mitigate the accuracy disparity in regression problems via learning the appropriate
representations. However, all these methods rely on the explicit access to the bias-conflicting samples,
making them difficult to apply on personalized federated learning, where bias-conflicting samples
may not be accessible for every client.
3	Preliminaries
Definitions and Notation A data sample is a vector x = [xr , xs], where xr corresponds to the
robust and non-spurious features and xs are the spurious features. ds is the dimension of spurious
features. Let y be a label, and define ` : Y × Y -→ R to be a λ-smooth, twice differentiable loss
function and L(f, D) = E(x,y)〜D ['(f (x), y)] to be the empirical risk. Wg and Wp are the weights
for the global model fg : X -→ Y and personalized model fp : X -→ Y , respectively. γ is the ratio
between the gradient norms of the global and personalized models, k^x'fg(x),y)k. Let SuPP(D) be
llv x'f p (x),y) k
the support of distribution D. We define a global data distribution Dg , a biased local data distribution
Db, and assume a bias-conflicting local data distribution Dbc . We define the “pseudo-gradient” as the
difference between the updated local model and the global model from the previous round (sometimes
we will use the term “gradient” when it is clear from context).〈•，•〉denotes an inner product of two
vectors and ∙ _ ∙ denotes a concatenation of two vectors. θ is the angle between Vχ'(fg(x), y) and
Vχ'(fp(x),y). θg is the angle between Vχ'(fg(x),y) and Vxr'(fg(x),y) _ 0, which measures
the entanglement of the global model to spurious features.
Training and Personalization Methods We train the global model using the federated averaging
algorithm (McMahan et al., 2017), which learns a model f : X -→ Y that minimizes: L(f, Dg) =
PN=ι ∣Dbil∕∣DgI ∙ Ex,y〜Db '(f (x), y), where N is the number of clients, Dbi is the biased local
dataset for client i, and Dg = ∪iN=1Dbi is the global dataset. When the global model fg converges,
fg will be sent to local clients for further fine-tuning by minimizing Ex,y〜Db '(f (x),y).
Adversarial Examples and Transferability We can generate an adversarial example xadv given
data sample x with label y by solving:
xadv = arg max `(f (x0), y),	(1)
kx0-xk≤
3
Under review as a conference paper at ICLR 2022
(a) MNIST
(b) CelebA
(c) Coil20
Figure 2:	Datasets with spurious features. The object color spuriously correlates with the label in
MNIST (a) and Coil20 (c) datasets. The hair color spuriously correlates with gender in the CelebA
dataset (b).
Accuracyon Biased Dataset
Accuracy on Bias-Conflicting DataSet
0.988
Centralized Federated
Training Setting
(b) CelebA
(c) Coil20
(a) MNIST
Figure 3:	The accuracy of ML models on Biased Dataset and Bias-Conflicting Dataset under
centralized and federated training settings. In the centralized setting, an ML model is trained by a
single dataset that contains all the samples with a fixed spurious correlation. The global models in the
federated setting achieve smaller accuracy disparities between biased and bias-conflicting datasets.
where f is the victim ML model, and is the attack budget. In this example, we consider the L2
attacks, but extensions to general Lp attacks are straightforward. We say an adversarial example to
be transferable if it also fools another ML model (e.g., a personalized model) other than the original
victim model f (e.g., the global model).
4	An Empirical Study with Spurious Features
To gain some insights into the problem, we first perform an empirical study on the accuracy disparity
of the global and personalized models in an FL setting. In this study, the personalization method
is fine-tuning. Our results highlight the risk of existing fine-tuning-based personalization methods
and the difficulty of mitigating the risk. We also highlight the correlation between the adversarial
transferability and the accuracy disparity between the global and personalized models. We provide
additional theoretical analysis in Section 5 to support the observed correlation. The spurious features
in the empirical study are as follows.
Spurious Features We consider color as the spurious feature for the MNIST, CelebA, and Coil20
datasets. In the MNIST and Coil20 datasets, we manually color the objects according to their labels
to create spurious correlations, as Figure 2 shows. The spurious correlations vary across clients for
the MNIST and Coil20 data (e.g., the red color correlates with label zero on the first client and with
label one on the second client) to create additional statistical heterogeneity. In the CelebA dataset,
the hair color attribute correlates with the gender label. We assign disjoint subsets of celebrities to
different users, which naturally increases statistical heterogeneity for the spurious correlation.
4.1	statistical heterogeneity Reduces Accuracy Disparity
Figure 3 shows the accuracy disparity of ML models on biased and bias-conflicting test sets. Com-
pared to the models trained in the centralized setting, where the spurious correlations are fixed, the
accuracy disparity of models trained in the federated setting decreased significantly. These empirical
results suggest that the global model in FL is more robust to spurious features if the spurious features
are non-i.i.d. across clients.
To explain this observation, consider the relationship between the gradient directions and the learned
features. For the spurious features, its correlation with the label may change across clients. As
4
Under review as a conference paper at ICLR 2022
(b) CelebA
——Acc B
-Acc BC
(a) MNIST
(c) Coil20
Figure 4: The accuracy of personalized model on Biased Dataset (ACC-B) and Bias-Conflicting
Dataset (Acc_BC) with increasing fine-tuning batches. The personalized models entangle spurious
features and increase accuracy disparities between biased and bias-conflicting datasets.
0.050
0.025
0.000
-0.025
-0.050
əuu ①」əfea >U2⊃UU<
----Acc_B - Acc_BC
Acc_Adv
(a) MNIST	(b) CelebA	(c) Coil20
Figure 5: The accuracy disparity of personalized models on biased dataset and bias-conflicting dataset
(Acc_B - Acc_BC) and their accuracy on adversarial examples (Acc_Adv). As the personalized models
entangle spurious features and increase the accuracy disparity, the accuracy of the personalized models
on adversarial examples increases, which indicates the adversarial transferability between the global
and personalized models decreases.
an example, in some users’ local datasets, the blond hair does not correlate with the gender label
because the dataset does not contain any blond female image or the dataset has blond male images, as
visualized in Figure 1a. Therefore, the gradient directions for the spurious features are diverse across
clients, as shown in Figure 1b. The divergence between the gradient directions, as a consequence,
makes learning spurious features difficult. In contrast, the non-spurious features, e.g., shape features,
are more consistent across clients, leading to a more consistent gradient direction.
4.2	Personalization May Exacerbate Accuracy Disparity
Although the global model in the federated setting has a lower accuracy disparity than in the
centralized setting, the advantage could vanish during the personalization step.
The Personalized Model Entangles Spurious Features As can be observed in Figure 4, the
accuracy first increases and decreases on the MNIST bias-conflicting test set and slowly decreases on
Coil20. These two observations indicate that the personalized model entangles spurious features and
exacerbates the accuracy disparity in a few batches. Although in principle, one may resort to early
stopping, this is not feasible when the bias-conflicting dataset is unavailable or scarce.
4.3	Adversarial Transferability Indicates Accuracy Disparity
Because the bias-conflicting test set is often unavailable, it is infeasible to directly measure the
accuracy disparity across personalized models. To this end, in this section, we focus on methods
that implicitly measure the accuracy disparity and determine whether the personalized models
entangle spurious features. Following our hypothesis in Section 1, we consider using the adversarial
transferability between the global and personalized models as a proxy for the accuracy disparity
measurement. Figure 5 plots the accuracy disparity and the adversarial transferability during fine-
tuning. As the accuracy disparity of the personalized models increases and drifts away from that of the
global model, the adversarial transferability between the global and personalized models decreases.
This result empirically validates our hypothesis.
5
Under review as a conference paper at ICLR 2022
5	Theoretical Insights
This section presents our theoretical result that supports our hypothesis in Section 1 and the experi-
mental results in Section 4.3. Our theoretical result applies the loss, which has similar behavior to
the accuracy as is shown in the empirical results in Section 7. Before we proceed, some additional
definitions and notations are needed for the presentation, and we provide a table summarizing all the
notations used in Appendix A to ease the reading. Then, we connect both the loss disparity and the
adversarial transferability to the angle between the gradients of the global and personalized models.
In what follows, we shall show an upper bound of the loss disparity of a personalized model, which
consists of the adversarial transferability between the global and personalized models and an indicator
of the entanglement of the global model to spurious features.
5.1	More Definitions and Notation
We define natural perturbation ∆ to model the distribution shift between the bias-conflicting Dbc and
biased Db . ∆ could change a bias-aligned sample to a corresponding bias-conflicting sample. The
distribution Dδ∣x of the natural perturbation ∆ conditions on the data sample x. Formally, for any
X ∈ Rd, we have: Prx〜Dbc (X) = pxoeRdAeRd 1{x=χ0+∆} ∙ Pb,〜Db(x0) ∙ Pr∆〜DδW(△).
Running Example For a non-blond male image x, we could draw a natural perturbation ∆ from
D∆∣x that change the hair color in X to blond. That is saying, X + ∆ isa blond male image. Iteratively
drawing data samples from the biased dataset and applying the sampled natural perturbations to the
data samples result in a dataset with bias-conflicting samples.
Another perturbation to consider is the adversarial perturbation δf, = Xadv -X that is generated using
f with budget e. Plugging the definition of δf,e into Eq.(1), We have δf,e = argmaxkδk≤e '(f (x +
δ), y). Since the budget is small, we could approximate the loss function ` using the first-order
Ofis Hi put, Xc —	arcτKnaY,, U ∖/ > > > > > ʌ	Q 八> X — 「	. Vχ4f (X) ,y)	∕^IV14∖7Qfc	pf	o!	z7八1 只∙ T TIo
gradient, θf,e —	argmaχkδk≤e Vx '(f (x),	y) δ 一 6 ∣∣Vχ'(f (x),y)k	(Miyato	et	al.,	2018; Liang
et al., 2021). With the adversarial perturbation, we define the adversarial transferability loss:
'trans(fg, fp, x,V) = ('(fg(X + δfg,e),y) - '(fg(x),y)) — ('(fp(X + δfg,e),y) - '(fp(x),y)),
which indicates the effectiveness of the adversarial perturbation generated using the global model
applied to the personalized models.
5.2	Loss Disparity and Adversarial Transferability
With the definitions of natural and adversarial perturbations, this section shows that both the loss
disparity and the adversarial transferability connect to an angle θ. Next, we outline the assumption:
Assumption 1. The distribution shift does not exacerbate the entanglement of a model f to spurious
features Xs, which is measured by Vxs `(f (X), y):
E(X,y)~Db,∆~D△|x,y
[Z1
α=0
hVxs'(f (X + α ∙ ∆),y), 1idα] ≤ E(χ,y)〜DbKVxs'(f (x), y), 1〉].
Under Assumption 1, the following Lemmas hold.
Lemma 1. Under Assumption 1, let ∆ be the natural perturbation, θ be the angle between
Vx'(fg(X),y) and Vx'(fp(X),y), θg be the angle between Vx'(fg(X),y) and Vxr'(fg(X),y) _
0,andYbe a'(fp(x),y)∣∣，we have:
L(fp, Dbc) - L(fp, Db) = E(x,y)〜Db,△〜D∆∣χ [/；hVxs'(fp(X + α ∙ ∆),y), 1〉da]
• kVx'(fg(X),y)k ∙ (sinθg + sinθ)]
< E(x,y)~Db,∆~D∆∣χ
Lemma 1 connects the loss disparity to θ. The θg, differing from θ, is an indicator of the entanglement
of the global model to spurious features and is a constant during the personalization step.
6
Under review as a conference paper at ICLR 2022
Lemma 2. Let E be the attack budget, θ be the angle between Vχ'(fg (x), y) and Vχ'(fp(x), y), Y
be k3x'fg(X),y)k，and the lossfunction ' : Y ×Y → R be λ-smooth, twice differentiable, we have
E ∙ kVχ'(fg (x),y)k ∙ (1 - - ∙ COSΘ) - λ ∙ E2 ≤ 'trans(fg, fp, X,y)
γ 1	(3)
≤ E∙kVχ'(fg(x),y)k∙ (1 - - ∙ cosθ) + λ ∙ e2
γ
Lemma 2 connects the adversarial transferability loss to θ. In the following analysis, we connect the
loss disparity to adversarial transferability via θ.
5.3 A Generalization Upper Bound
We now present an upper bound of the disparity L(fp, Dbc) - L(fp, Db). The main idea is to derive
an upper bound of ∣∣Vχ'(fg (x), y) ∣∣ ∙ sinθ in Eq. (2) from Eq. (3).
Theorem 3. Let γmin be the minimum ofγ, with Lemmas 1-2, we have:
L(fp,Dbc) - L(fp,Db) < pdS ∙ ((sin"，Z - 1) ∙ E(x,y)〜Db[∣Vχ'(fg(x),y)∣]
γmin
+ E ∙ E(x,y)〜Db ['trans (fg , fp, x, y)] + λ ∙ E)
Theorem 3 suggests (1) debiasing the global model fg, whose entanglement to spurious features is
measured by θg , and (2) enforcing the adversarial transferability between fg and fp help reducing the
loss disparity of personalized models. For the constants γmin and λ, we further explore their impacts
in the following section and Appendix D.1, respectively.
6	Methods
With the empirical and theoretical results in Sections 4.3 and 5, respectively, it is natural to ask if
enforcing the adversarial transferability in the personalization step reduces the accuracy disparity. In
this section, we first introduce adversarial examples to the personalization step as a regularization term
added to the original loss function, aiming to enforce the adversarial transferability. However, the
accuracy disparity still increases, albeit much slower, even if the adversarial transferability remains
high. One possible reason is that the personalized model increases its gradient norm, which helps
preserve the adversarial transferability but does not prevent the personalized model from entangling
spurious features. To this end, we add an L2 regularization term to the loss function, aligning the
gradient norms of the global and personalized models. Combing these two methods addresses the
accuracy disparity. Both methods are relatively light-weight from a computational perspective.
6.1	Enforcing Adversarial Transferability
We enforce that global and personalized models make consistent predictions on adversarial examples,
such that adversarial examples transfer from one to the other.
Generating Adversarial Examples The projected gradient descent (PGD) attack (Madry et al.,
2018) is an effective attack method that uses the neural network’s first-order gradient, and is easy to
compute. Additionally,. The attack solves Xadv = argmaxkχo-χk≤e '(f (x0), y) iteratively. At itera-
tion t + 1, the adversarial example is: xa+d1 = PrOjkxadv-χk≤∕χ + ɑ ∙ Sign(Vxadv'(fg(Xadv)Μ))),
where Proj is a projection operator.
Enforcing Consistent Predictions Both the global model fg and the personalized model fp take
the adversarial example Xadv as input and output zg and zp from their last layers, respectively. We
enforce the adversarial transferability by adding the following regularization term, which maximizes
the cross-entropy between zg and zp . Since the global model fg is fixed as a reference in the
personalization step and its low accuracy disparity is desirable, we use zg as the ground-truth:
K
Radv(Zg, Zp) = E[zgi ∙ log(zPi) + (1 -Zgi) ∙ log(1 - zPi)],
i=1
7
Under review as a conference paper at ICLR 2022
where K is the number of classes. The local model has access to the global model, so there
is no additional communication overhead for implementing this regularization. The adversarial
examples are computed using the global model once for all. The computation only needs a few
back-propagations, much less than training the global model.
6.2	Aligning Gradient Norms
In Eq. (3), we have seen that the adversarial transferability loss depends not only on the angle
θ, which connects the transferability to the disparity but also on Y := k^x'fg(x),y)k. A small Y
kv x'f p(x),y) k	'
indicates that the personalized model increases its gradient norm. Then, the personalized model could
entangle the spurious features and increase θ without decreasing the adversarial transferability.
To prevent Y from decreasing, we employ a simple and effective strategy by adding an L2 regu-
larization term RL2 = kwg - wp k2 to the loss function. The motivation behind the L2 term is
straightforward: if two models have similar weights, they have similar gradients. Empirical results in
Appendix D.2 show the effectiveness of the L2 term in controlling Y. Although prior works (Li et al.,
2020; T. Dinh et al., 2020; Li et al., 2021) have explored similar regularization methods, we develop
the regularization term from a different perspective.
7	Experiments
This section presents our experimental results, demonstrating that our method reduces the accuracy
disparity. We also show that the benefit of enhanced average accuracy from fine-tuning is preserved.
7.1	Setting
Data Partition We distribute the MNIST and Coil20 dataset across 50 clients where each client
have 5 different classes. The local dataset on each client is further partitioned to train/validation/test
set with a ratio of 72:8:20, following prior work (Li et al., 2021). We make two data partitions for
CelebA: CelebA_R using a real partition and CelebA_S using a synthetic partition, both have 508
clients. In both CelebA partitions, each client represent a disjoint set of celebrity (Li et al., 2021).
Due to the limited space, we further detail the data partition in Appendix C.1, report the hyper-
parameters in Appendix C.2, and list the neural network architecture in Appendix C.3.
7.2	The Effectiveness of Proposed Methods
We conduct an ablation study on the MNIST dataset. Figures 6b and 6f demonstrate the effectiveness
of enforcing adversarial transferability. However, the accuracy and loss disparity still increase during
personalization, which is potentially caused by the gradient norm issue (Section 6.2). Aligning the
gradient norms by applying the L2 regularization term while enforcing adversarial transferability
address the accuracy and loss disparity as Figures 6d and 6h show, respectively. Figure 6c and
6g further show that applying the L2 regularization term alone does not address the accuracy or
loss disparity. Compared to naive fine-tuning, which is reported in Figures 6a and 6e, our method
mitigates the accuracy and loss disparities by 〜50%.
7.3	Analysis
We compare our method to no personalization (Global), naive fine-tuning (FT), Ditto (Li et al.,
2021), up-weighting (UW) (Sagawa et al., 2020), and just train twice (JTT) (Liu et al., 2021). The
up-weighting method is implemented via sampling bias-aligned and bias-conflicting samples with
equal probability (Sagawa et al., 2020). Up-weighting and JTT are not applicable to Coil20 due to
the lack of bias-conflicting samples. We use the local finetuning version of the Ditto solver because
the local finetuning solver performs fewer local updates than that of the joint optimization solver
and therefore entangles spurious features less. Each experiment is repeated 9 times with 3 random
seeds for the federated learning step and 3 for the personalization step. We select models using the
validation accuracy minus the decrease of adversarial transferability and using the validation accuracy
for other baseline and competitor methods.
Tables 1 shows the main result. Our method reduces the accuracy disparity of personalized models
from 15.12% to 2.15%, compared to other personalization methods. Our method also preserves the
8
Under review as a conference paper at ICLR 2022
(a) Naive Fine-tuning
(c)
Aligning Gradient
(e) Naive Fine-tuning
(b) Enforcing Adversarial
Transferability
Norms
(d) Applying Both Meth-
ods
(f) Enforcing Adversarial (g) Aligning Gradient (h) Applying Both Meth-
Transferability	Norms	ods
Figure 6: The accuracy disparity (Acc_B - Acc_BC) and adversarial transferability accuracy
(Acc_Adv), and the loss disparity (Loss_BC - Loss_B) and adversarial transferability loss
(MAX_Loss_Adv - Loss_Adv) with different methods. Combining the two proposed methods ad-
dresses the accuracy and loss disparities. Acc_BC/Loss_BC and Acc_B/Loss_B are the accuracy/losses
on the bias-conflicting and biased test sets, respectively. Acc_Adv/Loss_Adv is the accuracy/loss on
adversarial examples. Acc-AdV and MAX_Loss_Adv - Loss_Adv, which measures the decrease of
Loss_Adv, indicate the decrease of adversarial transferability.
Table 1: Accuracy of personalized Models on Biased Test Set (ACC_B) and Bias-Conflicting Test Set
(Acc_B). Our proposed method achieves the lowest accuracy disparity (2.15%) compared to other
personalization methods (15.12%/15.38%), and 3.43% accuracy improvement on the biased test set
and 0.85% improvement on the biased-conflicting test set compared to the global model.
Method	MNIST		CelebA-S		CeIebA_R		Coil20	
	Acc_B	Acc_BC	Acc-B	Acc-BC	Acc_B	Acc-BC	Acc-B	Acc_BC
Global	.852 ± 2e-4	.847 ± 6e-4	.930 ± 5e-5	.910 ± 3e-4	.909 ± 6e-5	.929 ± 5e-5	.882 ± .6e-4	.903 ± 7e-4
FT	.989 ± 6e-7	.704 ± 3e-4	.952 ± 6e-5	.786 ± 5e-4	.963 ± 6e-6	.849 ± 1e-3	.931 ± 1e-4	.891 ± 2e-4
Ditto	.982 ± 3e-6	.724 ± 1e-3	.948 ± 5e-5	.715 ± 5e-4	.966 ± 1e-5	.884 ± 2e-4	.939 ± 4e-5	.897 ± 3e-4
UW	.968 ± 2e-5	.823 ± 7e-4	.930 ± 7e-6	.889 ± 5e-4	.936 ± 1e-5	.895 ± 4e-4	N/A	N/A
JTT	.985 ± 2e-7	.707 ± 6e-5	.952 ± 2e-6	.817 ± 3e-4	.956 ± 2e-5	.836 ± 1e-3	N/A	N/A
Ours	.951 ± 2e-5	.870 ± 8e-4	.932 ± 3e-5	.910 ± 2e-4	.925 ± 2e-5	.927 ± 8e-5	.901 ± 3e-4	.916 ± 5e-8
enhanced average accuracy from fine-tuning, resulting in 3.43% accuracy improvement on the biased
test set and 0.85% improvement on the biased-conflicting test set compared to the global model. In
contrast, the naive fine-tuning method sacrifices the accuracy on the bias-conflicting test set by up
to 14.3% and increase the accuracy disparity by 15.12%. We also find that our methods outperform
the supervised up-weighting method and the unsupervised JTT method, which increase the average
accuracy disparity to 7.56% and 17.76%, respectively. One possible reason is that the diversity of the
up-weighted bias-conflicting samples are small. Therefore, the neural network could memorize them
instead of discarding spurious features. Appendix D.3 further shows empirical results that support
our analysis.
8	Conclusion
In this work, we show the risk of prior federated learning personalization methods with spurious
features, which lead to high accuracy disparity between the global and local models. Then, we
develop a strategy by enforcing the adversarial transferability between the global and personalized
models to reduce the accuracy disparity. Both empirical and theoretical results show that our strategy
is effective.
9
Under review as a conference paper at ICLR 2022
9	Ethics Statement
Our method mitigates the issue of spurious features, which lead to bias towards minority groups,
in personalized federated learning. However, completely disentangling spurious features remains
challenging and is an issue for many federated learning methods.
10	Reproducibility S tatement
Our implementation, including data partition scripts, is based on the FedML library (He et al., 2020),
which is open-sourced. The proofs of the theoretical results are in Appendix B. The datasets in
our experiments are publicly available. We detail the data partition in Appendix C.1, report the
hyper-parameter tuning in Appendix C.2, and list the neural network architecture in Appendix C.3.
References
Alekh Agarwal, Miroslav Dudik, and ZhiWei Steven Wu. Fair regression: Quantitative definitions
and reduction-based algorithms. In International Conference on Machine Learning, pp. 120-129.
PMLR, 2019.
Jianfeng Chi, Yuan Tian, Geoffrey J Gordon, and Han Zhao. Understanding and mitigating accuracy
disparity in regression. In International Conference on Machine Learning (ICML), 2021.
Li Deng. The mnist database of handwritten digit images for machine learning research. IEEE Signal
Processing Magazine, 29(6):141-142, 2012.
Alireza Fallah, Aryan Mokhtari, and Asuman Ozdaglar. Personalized federated learning with
theoretical guarantees: A model-agnostic meta-learning approach. Advances in Neural Information
Processing Systems, 33, 2020.
Avishek Ghosh, Jichan Chung, Dong Yin, and Kannan Ramchandran. An efficient framework for
clustered federated learning. In H. Larochelle, M. Ranzato, R. Hadsell, M. F. Balcan, and H. Lin
(eds.), Advances in Neural Information Processing Systems, volume 33, pp. 19586-19597. Cur-
ran Associates, Inc., 2020. URL https://proceedings.neurips.cc/paper/2020/
file/e32cc80bf07915058ce90722ee17bb71-Paper.pdf.
Chaoyang He, Songze Li, Jinhyun So, Mi Zhang, Hongyi Wang, Xiaoyang Wang, Praneeth
Vepakomma, Abhishek Singh, Hang Qiu, Li Shen, Peilin Zhao, Yan Kang, Yang Liu, Ramesh
Raskar, Qiang Yang, Murali Annavaram, and Salman Avestimehr. Fedml: A research library and
benchmark for federated machine learning. arXiv preprint arXiv:2007.13518, 2020.
He He, Sheng Zha, and Haohan Wang. Unlearn dataset bias in natural language inference by fitting
the residual. arXiv preprint arXiv:1908.10763, 2019.
Peter Kairouz, H Brendan McMahan, Brendan Avent, AUreIien Bellet, Mehdi Bennis, Arjun Nitin
Bhagoji, Keith Bonawitz, Zachary Charles, Graham Cormode, Rachel Cummings, et al. Advances
and open problems in federated learning. arXiv preprint arXiv:1912.04977, 2019.
Fereshte Khani and Percy Liang. Removing spurious features can hurt accuracy and affect groups
disproportionately. In Proceedings of the 2021 ACM Conference on Fairness, Accountability, and
Transparency, pp. 196-205, 2021.
Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In Yoshua
Bengio and Yann LeCun (eds.), 3rd International Conference on Learning Representations, ICLR
2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings, 2015. URL http:
//arxiv.org/abs/1412.6980.
Pang Wei Koh, Shiori Sagawa, Sang Michael Xie, Marvin Zhang, Akshay Balsubramani, Weihua Hu,
Michihiro Yasunaga, Richard Lanas Phillips, Irena Gao, Tony Lee, et al. Wilds: A benchmark of
in-the-wild distribution shifts. In International Conference on Machine Learning, pp. 5637-5664.
PMLR, 2021.
10
Under review as a conference paper at ICLR 2022
Daliang Li and Junpu Wang. Fedmd: Heterogenous federated learning via model distillation. arXiv
preprint arXiv:1910.03581, 2019.
Tian Li, Anit Kumar Sahu, Manzil Zaheer, Maziar Sanjabi, Ameet Talwalkar, and Virginia
Smith. Federated optimization in heterogeneous networks. In I. Dhillon, D. Papail-
iopoulos, and V. Sze (eds.), Proceedings of Machine Learning and Systems, volume 2,
pp. 429-450, 2020. URL https://proceedings.mlsys.org/paper/2020/file/
38af86134b65d0f10fe33d30dd76442e- Paper.pdf.
Tian Li, Shengyuan Hu, Ahmad Beirami, and Virginia Smith. Ditto: Fair and robust federated learning
through personalization. In International Conference on Machine Learning, pp. 6357-6368. PMLR,
2021.
Yi Li and Nuno Vasconcelos. Repair: Removing representation bias by dataset resampling. In
Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp.
9572-9581, 2019.
Kaizhao Liang, Jacky Y Zhang, Boxin Wang, Zhuolin Yang, Sanmi Koyejo, and Bo Li. Uncovering
the connections between adversarial transferability and knowledge transferability. In International
Conference on Machine Learning, pp. 6577-6587. PMLR, 2021.
Evan Z Liu, Behzad Haghgoo, Annie S Chen, Aditi Raghunathan, Pang Wei Koh, Shiori Sagawa,
Percy Liang, and Chelsea Finn. Just train twice: Improving group robustness without training
group information. In International Conference on Machine Learning, pp. 6781-6792. PMLR,
2021.
Ziwei Liu, Ping Luo, Xiaogang Wang, and Xiaoou Tang. Deep learning face attributes in the wild. In
Proceedings of International Conference on Computer Vision (ICCV), December 2015.
Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and Adrian Vladu. To-
wards deep learning models resistant to adversarial attacks. In International Conference on Learn-
ing Representations, 2018. URL https://openreview.net/forum?id=rJzIBfZAb.
Brendan McMahan, Eider Moore, Daniel Ramage, Seth Hampson, and Blaise Aguera y Arcas.
Communication-efficient learning of deep networks from decentralized data. In Artificial Intelli-
gence and Statistics, pp. 1273-1282. PMLR, 2017.
Daniel McNamara, Cheng Soon Ong, and Robert C Williamson. Costs and benefits of fair representa-
tion learning. In Proceedings of the 2019 AAAI/ACM Conference on AI, Ethics, and Society, pp.
263-270, 2019.
Takeru Miyato, Shin-ichi Maeda, Masanori Koyama, and Shin Ishii. Virtual adversarial training: a
regularization method for supervised and semi-supervised learning. IEEE transactions on pattern
analysis and machine intelligence, 41(8):1979-1993, 2018.
Junhyun Nam, Hyuntak Cha, Sungsoo Ahn, Jaeho Lee, and Jinwoo Shin. Learning from failure:
Training debiased classifier from biased classifier. In Advances in Neural Information Processing
Systems, 2020.
Sameer A. Nene, Shree K. Nayar, and Hiroshi Murase. Columbia object image library (coil-20.
Technical report, 1996.
Yurii Nesterov. Introductory lectures on convex optimization: A basic course, volume 87. Springer
Science & Business Media, 2003.
Shiori Sagawa, Pang Wei Koh, Tatsunori B. Hashimoto, and Percy Liang. Distributionally robust
neural networks. In International Conference on Learning Representations, 2020. URL https:
//openreview.net/forum?id=ryxGuJrFvS.
Canh T. Dinh, Nguyen Tran, and Josh Nguyen. Personalized federated learning with moreau
envelopes. In H. Larochelle, M. Ranzato, R. Hadsell, M. F. Balcan, and H. Lin (eds.), Advances in
Neural Information Processing Systems, volume 33, pp. 21394-21405. Curran Associates, Inc.,
2020.
11
Under review as a conference paper at ICLR 2022
Florian Tramer, Nicolas Papernot, Ian Goodfellow, Dan Boneh, and Patrick McDaniel. The space of
transferable adversarial examples. arXiv preprint arXiv:1704.03453, 2017.
Haohan Wang, Zexue He, Zachary L. Lipton, and Eric P. Xing. Learning robust representations by
projecting superficial statistics out. In International Conference on Learning Representations,
2019. URL https://openreview.net/forum?id=rJEjjoR9K7.
Jianyu Wang, Zachary Charles, Zheng Xu, Gauri Joshi, H Brendan McMahan, Maruan Al-Shedivat,
Galen Andrew, Salman Avestimehr, Katharine Daly, Deepesh Data, et al. A field guide to federated
optimization. arXiv preprint arXiv:2107.06917, 2021.
Han Zhao and Geoff Gordon. Inherent tradeoffs in learning fair representations. Advances in neural
information processing Systems, 32:15675-15685, 2019.
Han Zhao, Remi Tachet Des Combes, Kun Zhang, and Geoffrey Gordon. On learning invariant
representations for domain adaptation. In International Conference on Machine Learning, pp.
7523-7532. PMLR, 2019.
12
Under review as a conference paper at ICLR 2022
Appendix
A Notation
Table 2: Table of Notation
Symbol	Description
x, y
xr, xs
d, dr , ds
,c
fgfpδf∆DgDbDbD
|x,y
supp(D)
h∙, ∙i
A pair of data sample and label
The robust feature and spurious features in x = [xr , xs], respectively
The dimension of x, xr, xs, respectively
The global model
The personlized local model
An adversarial purtubation generated by the global model fg with attack budget
An natural perturbation, which could flip the spurious attribute
The global distribution, which is the union of local distributions
A biased local distribution
A bias-conflicting local distribution
The distribution of natural perturbation
The support of distribution D
An inner product of two vectors
A concatenation of two vectors
B Proofs
B.1 Proof of Lemma 1
Lemma 1. Let ∆ be the natural perturbation, θ be the angle between Vχ'(fg(x),y) and
▽x'(fp(x),y), θg be the angle between Vχ'(fg(x),y) and Vxr'(fg(x),y) _ 0, and Y be
kv χ'(fg (χ),y) k
kVχ'(fp(x),y)k k
we have:
L(fp, Dbc) — L(fp,Db) = E(x,y)〜Db,∆ 〜Dδ∣x [/IhVxs '(fp(x + α ∙ ∆),y), 1>dα]
< E(x,y)〜"尸∙ kVx'(fg(x),y)k ∙ (sinθg + sinθ)]
Proof. Rewriting L(fp, Dbc) and introducing ∆:
L(fp, Dbc ) = E(x,y)〜Dbc['(fp(x),y)]
%,δ 〜D∆∣χ['(fP(X + A),y)]
lb,Δ^D∆∣x
E(x,y)~Db,∆~D△∣x
['(fp(x), y) + '(fp(x + ∆), y) - '(fp(x), y)]
['(fp (x),y)]
+ E(x,y)〜Db,∆〜D∆∣x ['(fp(x + ∆),y) - '(fp(x), y)]
E(x,y)〜Db['(fp(x),y)]
+ E(x,y)〜Db,∆〜Dδ∣x['(fp(x + △),y) - '(fp(χ), y)]
=L(fp，Db) + E(x,y)~Db,∆~D∆∣χ
=L(fp, Db)+ E(x,y)〜Db,∆"∆∣χ
[/ OhVx'(fp(X + α ∙ ∆),y) ,1>dα]
[/ Oxs'(fp(x + α ∙ ∆),y) ,1>dα]
13
Under review as a conference paper at ICLR 2022
Moving L(fp, Db) to the left-hand-side (LHS), we have:
L(fp,Dbc) - L(fp, Db) = E(x,y)〜Db,∆ 〜D∆∣x
[Z1
α=0
Oxs'(fp(X + α ∙ ∆),y), 1>dα].
(4)
According to Assumption 1, we further have:
E(x,y)~Db,∆~D∆∣χ
[Z1
α=0
hVxs'(fp(x + α ∙ ∆),y), 1i] ≤ Ex〜DbKVxs'(fp(χ),y), 1〉]
(5)
Next, We connect hVxs'(fp(x + α ∙ ∆), y), 1)to ∣∣Vx'(fg(x), y)∣∣∙sinθ. The first step is connecting
hVxs'(fp(X + α ∙ ∆),y), 1) to ∣∣Vxs'(fp(x), y)∣ using CaUChy-SChWarZ inequality:
hVxs'(fp(X + α ∙ ∆),y), 1)
≤ qhVxs '(fp (x + α ∙ ∆),y), Vxs '(fp(χ + α ∙ ∆), y)) ∙ h1,1)	(6)
=pdS ∙∣Vxs'(fp(x),y)k
Then, We ConneCtkVxs'(fp(x), y)∣ to ∣∣Vx'(fg(x), y)∣. Assuming the global model fg entangles
spurious features and the angle between Vx'(fg(x), y) and Vxr'(fg(x), y) _ 0 is θg, we have:
∣Vxs'(fp(x),y)k ≤ ∣∣Vx'(fg(x),y)∣∣∙ sin(θg + θ).	⑺
Since it is easy to see that θ ∈ [0, ∏] and the gradient of sinθ is monotonically decreasing in [0, ∏],
We have:
sin(θg + θ) =
0
=Z
0
<Z
0
θg+θ Vsinθdθ
θg+θ cosθdθ
(8)
cosθdθ + Z cosθdθ
0
sinθg + sinθ
Combining Eq. (7) and Eq. (8), We have:
l∣Vxs'(fp(x),y)k < ∣∣Vx'(fp(x),y)k ∙ (sinθg + sinθ)	(9)
Recalling the defition of Y := k烂x'fg(x),y)kk and combining Eq. (4), Eq. (5), Eq. (6), Eq. (9)
complete the proof.
□
B.2 Proof of Lemma 2
Lemma 2. Let E be the attack budget, θ be the angle between Vx'(fg (x), y) and Vx'(fp(x), y), Y
be kk3x'fg(x),y)kk, and the lossfunction ' : Y ×Y → R be λ-smooth, twice differentiable, we have
E ∙ ∣Vx'(fg (x),y)∣ ∙ (1 - Y ∙COSΘ) — λ ∙ E2 ≤ 'trans(fg ,fp, x,y)
≤ E ∙ ∣Vx'(fg(x),y)k ∙ (1 - Y ∙ cosθ) + λ ∙ E2
14
Under review as a conference paper at ICLR 2022
Proof. Under the definition of the adversarial perturbation, itis easy to see that δf,
and δf, increases the loss by:
r y χg(f (X) ,y)
C kVχ'(f(X),y)k
'(fg(X + δfg,e),y)- '(fg(x),y) = δfg,eVχ'(fg(x),y) + 1 δ>,N^'(fg(Xg),y)δfg,e
=C∙ kVχ'(fg (x),y)k + 1 δfg ,eVXg'(fg (Xg ),y)δfg ,e
where Xg is a linear interpolation between X and X + δfg ,e, by the Lagrange,s mean-value theorem.
Similarly, for a transferable adversarial example from fg applies to fp, δfg , could increase the loss
of fp by:
'(fp(X + δfg ,e), y) - '(fp(X), y) = δfg ,e Vχ'(fp (x), y) + 2 δ> ,eVXp'(fp (Xp), y)δf, ,e
=〜kVχ'(fp(X), y)k∙ cosθ + 1 δfg,eVXp'(fp(Xp), y)δfg,e
where CCqA —— V χ'(f g (X),y)，Vx'(fP(X),y) Pin ooin o thQ CInnreYimaHcnc CIhrYVQ fc thQ CifHzQrcciriciI
WheIe cosθ — “▽ '(f (x) y)kk^^ '(f (x) y)k . PlUgging the approXuUationS above to the adversarial
transferability loss, we have:
'trans(fg ,fp, X, y) = ('(fg (X + δfg ,e),y) - '(fg Q),y)) - ('(fp(X + δfg ,e ), y) - '(fp (x) , y))
=e ∙kVχ'(fg(X),y)k- e ∙ kVχ'(fp(X),y)k∙ cosθ
+ 2 ∙ δfg ,eVXg '(fg (Xg ),y)δfg ,e - 2 ∙ δ>g,eVXp'(fp(Xp),y)δfg ,e
Under the λ-smooth assumption on the loss function, the spectral norms of the Hessian metrics are
bounded. Therefore, we could bound the norm of the deviate between the quadratic terms (Nesterov,
2003, Proof of Theorem 2.1.5) in the adversarial transferability loss:
M 晨 VXg'(fg (Xg ),y)δfg,e - δfg ,eVXp'(fp(Xp), y)δfg ,ek ≤ 2λ ∙ δf ①,e = 2λ ∙ /	(10)
Since the quadratic terms in Eq. (10) are scalars, we have:
-2λ ∙ / ≤ δ>g,eVXg '(fg (Xg ),y)δfg ,e - δfg ,eVXp '(fp(Xp), y)δfg ,e ≤ 2λ ∙ /	(11)
Plugging Eq. (11) and the definition of γ to `trans (fg, fp, X, y) completes the proof.
□
B.3 Proof of Theorem 3
Theorem 3. Let γmin be the minimum of γ, under Assumptions 1 and Lemmas 1-2, we have:
L(fp, Dbc) -L(fp, Db) < pdS ∙ ((sin"，7 - 1) ∙ E(x,y)〜Db[kVχ'(fg (X),y)k]
γmin
+ C ∙ E(x,y)〜Db ['trans(fg , fp, x, y)]+ λ ∙ E)
Proof. According to Lemma 2, we know:
'transfg, fp, X, y) "kVx'(fg (x),y)k∙ (1 - 1 •…λ /
AXzHprp pfʌɑ/ə	—— vx'(fg(x),y )Vx'(fP(X) ,y)	Th d∏	ʊ/p	HpfIAZP QTI	11ΠΠPΓ H∏UT1H Cf Il ∖/ 0( f (rT,'∖	2∕∖ll	.
where cθsθ	= ∣∣Vx'(fg(x),y)kkVx'(fp(x),y)k .	Then，We	derive an	UPPer DoUnd OfkVX'(fg(X),	y)k
sinθ from 'trans(fg, fp, x, y). It is easy to see that θ ∈ [0, 4]. Therefore, we have:
15
Under review as a conference paper at ICLR 2022
'trans (fg, fp, x, y)
≥ E ∙ ∣∣Vx'(∕g (x), y)∣∣∙ (1 — - ∙ COSθ) — λ ∙ e2
=—∙ kvx'(fg (X), y)∣∣∙ (I-Cosθ + - - 1) — λ , E2
-
=-∙ ∣Vχ'(fg(x),y)k∙ (1 — cosθ) + 一 (Y T) ∙ ∣∣Vχ'(fg(x), y)||- λ ∙ e2
--
=-∙ ∣Vχ'(fg(x),y)k∙ (2 ∙ sin21) +	I) ∙ ∣∣V.2(fg(x),y)∣ — λ ∙ e2
=-∙ ∣Vχ'(fg(x),y)k ∙ (sinθ + 2 ∙ sin2θ — sinθ)
-2
+ EY- T) ∙∣Vχ'(fg (x),y)||— λ∙ e2
-
=-∙ ∣Vχ'(fg(x),y)k ∙ sinθ + - ∙ ∣∣Vx'(fg(x),y)∣ ∙ (2 ∙ sin2θ — sinθ)
--	2
+ EY- T) ∙∣Vχ'(fg (x),y)||— λ∙ e2
-
≥ - ∙ ∣Vχ'(fg(x),y)∣∙ sinθ + - ∙ ∣∣Vχ'(fg(x), y)∣ ∙ (1 — √2)
+ EY- T) ∙∣Vχ'(fg (x),y)||— λ∙ e2
-
=-∙ ∣Vχ'(fg(x),y)k ∙ sinθ + E ∙ (7 — g ∙ ∣Vχ'(fg(x),y)k — λ ∙ e2
--
Moving ∣Vχ'(fg (x),y)∣ ∙ sinθ to the left hand side (LHS):
∣Vχ'(fg(x),y)k ∙ sinθ
≤ - ∙ 'trans(fg, fp, X, y) + (√2 — -) ∙ ∣Vχ'(fg (x),y)∣ + - ∙ λ ∙ E
(12)
According to Lemma 1, we know:
L(fp,Dbc) — L(fp, Db) = E(χ,y)~Db ,△〜Ddx
J a=0
hVxs'(fp(X + α ∙ ∆),y), 1〉da]
< E(χ,y)~Db[>∙ ∣Vχ'(fg(x),y)k ∙ (sinθg +sinθ)]
(13)
Combining Eq. (13) and Eq. (12), and taking expectation of x, y over Dfc:
Eχ~Db[尸∙ ∣Vχ4(fg(x),y)k ∙ (sinθg +sinθ)]
≤	∙ (E(x,y)~Db[kVx'(fg (X), y)k ∙ sinθg] + E(x,y)^Dt [- ∙ 'trans(fg, fp, x, y)]
-	E
+ E(χ,y)~Db [(√2 - -) ∙ ∣Vχ'(fg(x), y)∣]+ - ∙ λ ∙ E)
≤ Vd： ∙ ((sinθg + 6 - 1) ∙ E(χ,y)~Db[∣∣Vχ'(fg(x),y)∣]
\	-min
+ - ∙ E(x,y)~Db['trans(fg,fp, X,y)] + λ ∙ e)
(14)
Plugging Eq. (14) back to Eq. (13) completes the proof.
□
16
Under review as a conference paper at ICLR 2022
C	More Experimental Setting
C.1 Data Partition
We distribute the MNIST and Coil20 dataset across 50 clients. Each client has data from 5 different
classes. The local dataset on each client is further partitioned to train/validation/test set with a ratio
of 72:8:20, following prior work (Li et al., 2021). The test set here is biased. We alternate the
spurious features in biased test sets by recoloring the data to create a bias-conflicting test set. For the
CelebA dataset, We consider two partitions. In the first partition (CelebA_R), each client represents
20 celebrities. One celebrity only appears on one client. The blond male images in the biased test
sets are copied to bias-conflicting test sets. We use all clients for training the global model. In the
personalization step, we select the clients who have more than 5 blond female training samples and
more than 5 blond male test samples. We select these clients because they provide enough samples to
create spurious correlations and bias-conflicting test sets. Although the first partition on CelebA is
real, the number (161) of blond male images is small. To make the result clearer, we create another
synthetic CelebA partition. In the second partition (CelebA-S), there are 650 blond male images,
which achieve a similar bias-conflicting test set size as prior works Sagawa et al. (2020); Liu et al.
(2021). The 650 images are distributed to 3 clients with 2350 other images. The rest of the images
are distributed in the same way as the first partition. Tables 3, 4, and 5 provide more details about the
3 clients.
Table 3: Number of Train and Validation Samples in CelebA-S
Client ID	Non-blond Female	Non-Blond Male	Blond Female	Blond Male
0	55	31	12	2
1	30	68	0	2
2	59	28	11	2
Table 4: Number of Biased Test Samples in CeIebA_S
Client ID	Non-blond Female	Non-Blond Male	Blond Female	Blond Male
0	115	60	45	2
1	60	75	79	0
2	86	111	14	0
Table 5: Number of Biased-Conflicting Test Samples in CeIebA_S
Client ID	Non-blond Female	Non-Blond Male	Blond Female	Blond Male
0	0	0	0	200
1	0	0	0	203
2	0	0	0	204
C.2 Hyper-parameters
We use Adam optimizer (Kingma & Ba, 2015) throughout our experiments with learning rate 1e-4.
Although stochastic gradient descent (SGD) optimizer is more common in vision-related tasks, we
find that the Adam optimizer always leads to lower accuracy disparity. We train the global model for
500 rounds. 5 clients are selected per round, and each performs 5 epochs of local updates. We tune the
coefficients of the adversarial transferability and L2 regularization terms from {0.01, 0.1, 1.0, 10.0}
and select the largest value that does not decrease the validation accuracy during penalization. We start
the attack budget at 0.031 and gradually decrease it such that 30% - 50% of the attack succeeds. A
large budget will make the attack too strong and push the adversarial examples far across the decision
boundary, making the regularization method less effective. We configure to 0.031/0.01/0.015 for
MNIST/CelebA/Coil20, respectively. We fine-tune the global model for 5 epochs on MNIST and
17
Under review as a conference paper at ICLR 2022
10 epochs on CelebA/Coil20, which are sufficient for the personalized models to converge. The
clients with the most data samples fine-tune the penalized models for a total of 80/40/30 batches on
MNIST/CelebA/Coil20 datasets. Note that we may not select the personalized model with the most
fine-tuning batches for reporting. In the just train twice (JTT) method, we up-sample the residuel by
a factor of 50. In Ditto, we tune its λ from {0.1, 1.0}.
C.3 Neural Network Architecture
We use CNN 28x28 for MNIST dataset and CNN 64x64 for CelebA and Coil20 datasets.
Table 6: Neural Network Architecture
CNN 28x28	CNN 64x64
Input: R3∙28∙28 4∙4 conv, 64 BN LReLU, stride 2 4∙4 conv, 128 BN LReLU, stride 2 FC 4096 ReLU FC 10	Input: R3∙64∙64 4∙4 conv, 64 BN LReLU, stride 2 4∙4 conv, 64 BN LReLU, stride 2 FC 4096 ReLU FC 10
D More Experimental Results
D. 1 First-order Approximation of Adversarial Transferability Loss
To explore the impact of the λ term in Lemma 2 and Theorem 3, we measure the relative difference
between 'trans(fg,fp, x, y) and e ∙ ∣∣Vχ'(fg(x), y)k ∙ (1 - Y ∙ cosθ). In other words, We measured
the accuracy of a first-order approximation of `trans (fg, fp, x, y). If the approximation is accuracy
is high, it implies that the impact of λ ∙ e2 is low. Specifically, we compute an approximation error:
hl 'trans(fg, fp, x, y) - e ∙ ∣Vχ'(fg(x),y)∣ ∙ (1 - 1 ∙cosθ)
E(Xyym ∣l	Fffxy
On MNIST, CelebA and Coil20 datasets, we find that the approximation error is 0.019, 0.058, 0.084,
respectively. These results suggests that using e ∙ ∣∣Vχ'(fg(x),y)∣ ∙ (1 - Y ∙ cosθ) to approximate
'trans(fg, fp, x, y results in a decent accuracy and the impact of λ ∙ e2 is low. The possible reason
is that the attack budget e is usually small (e.g., 0.031), such that the gradient of a function changes
little in a small neighborhood defined by e.
D.2 EFFECTIVENESS OF L2 REGULARIZATION TERM
Figure 7 shows the distribution of γ before and after applying the L2 regularization term on the
CelebA dataset. Here, the γ is computed once per data sample. We keep the global model fixed and
fine-tune the personalized model for 1 epoch.
gamma
Figure 7:	With the L2 regularization term, the distribution of γ is centered around 1, with a small
variance. The minimum of γ is closer to 1.
18
Under review as a conference paper at ICLR 2022
D.3 Diversity of Bias-conflicting Samples Impacts Debiasing
To explore the impact of the diversity of bias-conflicting samples on debiasing, we vary the diversity
of bias-conflicting samples and adjust the up-weighting factors accordingly. Specifically, we sample
a factor of 0.02, 0.025, 0.033, 0.05, and 0.1 biased data samples from the MNIST dataset and re-color
them to become bias-conflicting. The factor in the sampling step is called sampling factor. Then, we
up-weight the bias-conflicting samples by a factor of 50, 40, 30, 20, 10, respectively, keeping the total
number of bias-conflicting samples consistent. Here, the bias-conflicting samples have less diversity
if generated by a small number of biased data samples with a large up-weighting factor. Experimental
results in Figure 8 show that, as the diversity reduces, the accuracy disparity of personalized model
on the biased dataset and bias-conflicting dataset increases, supporting our analysis. Therefore, our
method is applicable in the scarcity of bias-conflicting samples while the up-weighting method fails.
0.95
2 0.90
0.85
----Acc B
Acc BC
0.025 0.050 0.075 0.100
Sampling Factor
Figure 8:	The up-weighting method is less effective, resulting in large accuracy disparity of personal-
ized model on biased dataset and bias-conflicting dataset, if the bias-conflicting samples is generated
by a small number of biased data samples using a small sampling factor and a large up-weighting
factor. The up-weighting factor is set to be the reciprocal of the sampling factor.
19