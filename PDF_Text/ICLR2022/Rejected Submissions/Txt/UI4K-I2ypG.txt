Under review as a conference paper at ICLR 2022
A Survey on Evidential Deep Learning For
Single-Pass Uncertainty Estimation
Anonymous authors
Paper under double-blind review
Abstract
Popular approaches for quantifying predictive uncertainty in deep neural networks
often involve a set of weights or models, for instance via ensembling or Monte
Carlo Dropout. These techniques usually produce overhead by having to train
multiple model instances or do not produce very diverse predictions. This sur-
vey aims to familiarize the reader with an alternative class of models based on
the concept of Evidential Deep Learning: For unfamiliar data, they admit “what
they don’t know” and fall back onto a prior belief. Furthermore, they allow un-
certainty estimation in a single model and forward pass by parameterizing distri-
butions over distributions. This survey recapitulates existing works, focusing on
the implementation in a classification setting. Finally, we survey the application
of the same paradigm to regression problems. We also provide a reflection on the
strengths and weaknesses of the mentioned approaches compared to existing ones
and provide the most central theoretical results in order to inform future research.
1 Introduction
Many existing methods for uncer-
tainty estimation leverage the con-
cept of Bayesian Model Averaging,
that approaches such as Monte Carlo
(MC) Dropout (Gal & Ghahramani,
2016), Bayes-by-backprop (Blundell
et al., 2015) or ensembling (Lak-
shminarayanan et al., 2017) can be
grouped under (Wilson & Izmailov,
2020). This involves the approxi-
mation of an otherwise infeasible to
compute integral using Monte Carlo
samples - for instance from an aux-
iliary distribution or in the form of
ensemble members. This implies the
following problems: Firstly, the qual-
Prior	Posterior
:Variational	∣
；・	Joo	et al. (2018)	；
---------------------------------------------------------------------------:
Requires OOD . Malinin & Gales (2019) 'v ∙ Chen et al. (2018) ∖
・ Nandy et al. (2020)	ɪ
・ Malinin & Gales (2018)・ Shen et al. (2020)	・ Sensoy et al. (2020)；
・	Haussmann et al.
(2019)
・	Tsilingkaridis (2019)
.Bilosetal. (2019)
・	Zhao et al. (2020)
Regression
.Amini et al. (2020)
Knowledge Distillation
・	Malinin et al. (2020b)
・	Malinin et al. (2020a)
•	Sensoy et al. (2018)
Flow-based
•	Charpentier et al.
(2020)
1 ∙ Charpentier et al.
；(2021)
Figure 1: Taxonomy of surveyed approaches.
ity of the MC approximation depends on the veracity and diversity of samples from the weight
posterior. Secondly, the approach often involves increasing the number of parameters in a model
or training more model instances altogether. Recently, a new class of models has been proposed to
side-step this conundrum by using a different factorization of the posterior predictive distribution.
This allows to compute uncertainty in a single forward pass and set of weights. Furthermore, these
models are grounded in a concept coined Evidential Deep Learning: For out-of-distribution (OOD)
inputs, they fall back onto a prior, often expressed as knowing what they don’t know.
Our contributions are as follows: We summarize the existing literature and group these approaches,
critically reflecting on their advantages and shortcomings alike, as well as how they fare compared
to other, related methods. This survey aims to both serve as an accessible introduction to this model
family to the unfamiliar reader as well as an informative overview, in order to promote more applica-
tions outside the uncertainty estimation literature. We also provide a collection of the most important
theoretical results for the Dirichlet distribution for Machine Learning, which plays a central role in
many of the discussed approaches. We give an overview over all discussed work in Figure 1.
1
Under review as a conference paper at ICLR 2022
2 Background
(a) Ensemble
(b) Confident prediction
(c) Data uncertainty
(d) Model uncertainty
(e) Distributional uncertainty
(f) Representation gap (OOD)
Figure 2: Examples of the probability simplex for a K = 3 classification problem, where every
corner corresponds to a class and every point to a categorical distribution. Brighter colors correspond
to higher density. (a) Ensemble of discriminators. (b) - (e) (Desired) Behavior of Dirichlet in
different scenarios by Malinin & Gales (2018). (f) Representation gap by Nandy et al. (2020).
We first familiarize the reader with the necessary prerequisites for the rest of the survey, including
Bayesian Model Averaging and the alternative approach of Evidential Deep Learning in Section 2.2
along with a short introduction to the Dirichlet distribution in the next section.
2.1	The Dirichlet distribution
The Beta distribution is a commonly used prior for a Bernoulli likelihood, which can be used to
formulate a binary classification problem. The Dirichlet distribution arises as a multivariate gener-
alization of the Beta distribution for a multi-class classification problem and is defined as follows:
1K
Dir(μ; α)=昕 jɪ 4”-1 ； B(α)=
QK=1 Γ(αk).
Γ(α0)	;
K
α0 = X αk;	αk ∈ R+	(1)
k=1
where Γ(∙) denotes the gamma function, a generalization of the factorial to the real numbers, K the
number of categories or classes, and B(∙) is called the beta function. For notational convenience,
we also define K = {1, . . . , K} as the set of all classes. The distribution is characterized by its
concentration parameters α, the sum of which, often denoted as α0, is called the precision.1 The
distribution becomes relevant for applications using neural networks, considering that most modern
networks for classification use a softmax function after their last layer to produce a categorical
distribution of classes, for which the Dirichlet is a conjugate prior. The class probabilities can be
expressed using a vector μ ∈ [0,1]K s.t. μk ≡ P(y = k|x). Then, using a Dirichlet prior for a
categorical likelihood, due to its conjugacy, produces a Dirichlet posterior with parameters β, given
a data set D = {(xi, yi)}iN=1 ofN observations with corresponding labels:
1The precision is analogous to the precision of a Gaussian, where a larger α0 signifies a sharper distribution.
2
Under review as a conference paper at ICLR 2022
NK	K
p(μ |D, α) X p(D| μ)p(μ∣α) = YY μ1yi=k b⅛ Y L
i=1 k=1	k=1
=Y 〃口1 G Ba Y μαk-1 = Ba Y μNk+αk-1 = Dir(μ; β)
k=1	k=1	k=1
(2)
where β is a vector with βk= αk+ Nk, with Nk denoting the number of observations for class
k and 1 being the indicator function. Intuitively, this implies that the prior belief encoded by the
initial Dirichlet is updated using the actual data, sharpening the distribution for classes for which
many instances have been observed. This distribution constitutes a distribution over categorical
distributions over the K - 1 probability simplex, multiple instances of which are shown in Figure 2.
Each point on the simplex corresponds to a categorical distribution, with the proximity to a corner
indicating a high probability for the corresponding class. Figure 2a displays the predictions of
an ensemble of classifiers as a point cloud on the simplex. Using a Dirichlet, this finite set of
distributions can be extended to a continuous density over the whole simplex (Figures 2b to 2f).
2.2	Predictive Uncertainty in Neural Networks
In probabilistic modelling, uncertainty is commonly divided into aleatoric and epistemic uncertainty
(Der KiUreghian & Ditlevsen, 2009; Hullermeier & Waegeman, 2021). The former refers to the Un-
certainty that is induced by the data-generating process, and which e.g. might create an unresolvable
overlap in class distributions.2 The latter describes the uncertainty about the optimal model parame-
ters (or even hypothesis class), reducible with an increasing amount of data, as less and less possible
models become a plausible fit. These two notions resurface when formulating the posterior predic-
tive distribution of a classifier for a new data point x:3
P…JP^吧d θ
Aleatoric Epistemic
(3)
For a large number of real-valued parameters θ like in neural networks, this integral becomes in-
tractable to evaluate, and thus is usually approximated using Monte Carlo samples - with the afore-
mentioned problems of potential computational overhead and approximation errors. Malinin &
Gales (2018) thus propose to factorize Equation (3) further:
p(y| x)
PP P(y| μ) p(μ | χ, D) p(θ |D) d μ d θ
JJ ।---{--'、-------'、-----'
Aleatoric Distributional Epistemic
∕p(yl") P(μM} d μ
p(θ ∣D) = δ(θ -θ)
(4)
z
In the last step, we replace p(θ |D) by a point estimate θ using the Dirac delta function, i.e. a sin-
gle trained neural network, to get rid of the intractable integral. Although another integral remains,
retrieving the uncertainty from this predictive distribution actually has a closed-form analytical so-
lution for the Dirichlet (see Section 3.2). The advantage of this approach is further that it allows
us to differentiate uncertainty about a data point because it lies in a region of considerable class
overlap (Figure 2c) from it differing from the training distribution entirely (Figure 2e). Assuming
that a point estimate of the parameters suffices prevents one from estimating epistemic uncertainty
like in earlier works, as discussed in the next section. However, there are works like Haussmann
et al. (2019); Zhao et al. (2020) that combine both approaches.
2Unless additional features are added.
3 Note that the predictive distribution in Equation (3) recovers the common case for a single network predic-
tion where P(y∖ x, θ) ≈ P(y∖ x, θ). Mathematically, this is expressed by replacing the posterior p(θ |D) by a
delta distribution like in Equation (4), where all probability density rests on a single parameter configuration.
3
Under review as a conference paper at ICLR 2022
3	Dirichlet Networks
We will show in Section 3.1 how neural networks can parameterize Dirichlet distributions, while
Section 3.2 reveals how such parameterization can be exploited for efficient uncertainty estimation.
The remaining sections enumerate different examples from the literature parameterizing either a
prior (Section 3.3.1) or posterior Dirichlet distribution (Section 3.3.2) according to Equations (1)
and (2).
3.1	Parameterization
For a classification problem with K classes, a neural classifier is usually realized as a function
fθ : RD → RK, mapping to logits for each class given an input x ∈ RD. Followed up by
a Softmax function, this then defines a categorical distribution over classes with a vector μ s.t.
μk ≡ p(y = k| x, θ). The same architecture can be used without any major modification to in-
stead parameterize a Dirichlet distribution, as in Equation (1).4 In order to classify a data point x,
a categorical distribution is created from the predicted concentration parameters of the Dirichlet as
follows (this definition arises from the expected value, see Appendix A.1):
α = fθ(x); μk =——； y = argmax μι,...,μκ	(5)
α0	k∈K
As discussed in Section 3.3.2, this process is very similar when parameterizing a Dirichlet posterior
distribution, except that in this case, a term corresponding to the class observation in Equation (2) is
added to every concentration parameter as well.
3.2	Uncertainty Estimation with Dirichlet Networks
Let us now turn our attention on how to estimate the different notions of uncertainty laid out in Sec-
tion 2.2 within the Dirichlet framework. Although stated for the prior parameters α, the following
methods can also be applied to the posterior parameters β as well without loss of generality.
Data (aleatoric) uncertainty. For the data uncertainty, we can evaluate the expected entropy of
the data distribution p(y | μ) (similar to previous works like e.g. Gal & Ghahramani, 2016). As the
entropy captures the “peakiness” of the output distribution, a lower entropy indicates that the model
is concentrating all probability mass on a single class, while high entropy stands for a more uniform
distribution - the model thus is undecided about the right prediction. For Dirichlet networks, this
quantity has a closed-form solution (for the full derivation, refer to Appendix B.1):
Ep(“∣χ,θ) H [P(y∣ μ)]
—
Kα
E — (ψ(αk + 1) - ψ(ao + 1)
k=1 α0
(6)
where ψ denotes the digamma function, defined as ψ(χ)=寻 log Γ(x), and H the Shannon entropy.
Model (epistemic) uncertainty. As we saw in Section 2.2, computing the model uncertainty in
the classical sense via the weight posterior p(θ |D) like in Blundell et al. (2015); Gal & Ghahramani
(2016); Smith & Gal (2018) is usually not done in the Dirichlet framework (with exceptions such
as Haussmann et al., 2019; Zhao et al., 2020). Nevertheless, the defining property of Dirichlet
networks is that epistemic uncertainty is expressed in the spread of the Dirichlet distribution (for
instance in Figure 2 (d) and (e)). Therefore, the epistemic uncertainty can be quantified considering
the concentration parameters α that shape this very same distribution: Charpentier et al. (2020)
simply consider the maximum αk as a score akin to the maximum probability score by Hendrycks
& Gimpel, while Sensoy et al. (2018) compute it by K/ PkK=1(αk + 1) or simply α0 (Charpentier
et al., 2020). In both cases, the underlying intuition is that larger αk produce a sharper density, and
thus indicate increased confidence in a prediction.
4The only thing to note here is that the every αk has to be strictly positive, which can for instance be
enforced by using an additional ReLU function (and adding a small value, e.g. like in Sensoy et al., 2020) on
the output or predicting log αk instead (Sensoy et al., 2018; Malinin & Gales, 2018).
4
Under review as a conference paper at ICLR 2022
Distributional uncertainty. Another appealing property of this model family is to distinguish
uncertainty due to model underspecification (Figure 2d) from uncertainty due to alien inputs (Fig-
ure 2e). In the Dirichlet framework, the distributional uncertainty can be deduced by computing the
difference between the total amount of uncertainty and the data uncertainty, which can be expressed
in terms of the mutual information between the label y and its categorical distribution μ:
I [y, μ I χ,可
H Ep(μ∣x,D) [p(y| μ)]
、-------------{z------------
Total Uncertainty
-Ep(μ∣x,D) h[p(y| μ)]
、--------------{z----------
Data Uncertainty
(7)
Given that E[μk] = Olk (Appendix A.1) and assuming the point estimate p(μ | x, D) ≈ p(μ | x, 0)
to be sufficient (Malinin & Gales, 2018), we obtain an expression very similar to Equation (6):
—
X ααk (log ααk - ψ(α + 1)+"(00 +1)
k=1 0	0
3.3	Existing Approaches
The properties we discussed in previous sections are desirable traits, as they simplify the process
of obtaining different uncertainty scores. However, it is important to note that the behaviors of
the Dirichlet distributions in Figure 2 are idealized. In the empirical risk minimization framework
that neural networks are usually trained in, Dirichlet networks are not incentivized to behave in
the depicted way per se. Thus, when comparing existing approaches for parameterizing Dirichlet
priors (Section 3.3.1) and posteriors (Section 3.3.2),5 we mainly focus on the different ways that
authors try to tackle this problem by means of loss functions and training procedures. Due to spatial
constraints, we refrain to present all the different ideas in detail and instead only highlight some of
them, summarizing the rest in an informal manner.6 We give an overview over the discussed works
in Tables 1 and 2 in the respective sections.
3.3.1	Prior Networks
The key challenge in training Dirichlet networks comes in the form of ensuring both high classi-
fication performance and the intended behavior under foreign data inputs. For this reason, most
discussed works follow a loss function design using two parts: One optimizing for task accuracy for
the former goal, the other one for a flat Dirichlet distribution for the latter.
As their main objective, Tsiligkaridis (2019) derive a generalized lp loss (see Appendix B.3), but
using a local approximation of the Renyi divergence w.r.t to a uniform Dirichlet for the regularization
term in order to ensure higher uncertainties for misclassified examples.7 Zhao et al. (2020) similarly
use a l2 loss in the context of Graph Neural Networks (GNNs), but adapt a Kullback-Leibler (KL)
regularization term to incorporate information about the local graph structure instead of referring
to a uniform prior, as well as a knowledge distillation loss. Haussmann et al. (2019) optimize the
model using a negative log-likelihood (NLL) loss and derive a regularizer from PAC bounds.
Instead of enforcing the flatness of the Dirichlet by itself, Malinin & Gales (2018) instead explicitly
maximize the KL divergence to a uniform Dirichlet on OOD data points. Further, they instead utilize
another KL term to train the model on predicting the correct label instead of a lp norm. However, as
the KL divergence is not symmetrical, Malinin & Gales (2019) argue that the reverse counterparts
of both loss terms actually have more appealing properties in producing the correct behavior of the
predicted distribution (see Appendix B.5). Nandy et al. (2020) refine this idea further, stating that
5 Even though the term prior and posterior network have been coined by Malinin & Gales (2018) and
Charpentier et al. (2020) for their respective approaches, we use them in the following as an umbrella term for
all methods targeting a prior or posterior Dirichlet.
6For more details, we refer the reader to Appendix A for general derivations concerning the Dirichlet distri-
bution. We dedicate Appendix B to more extensive derivations of the different loss functions and regularizers
and give a detailed overview over their mathematical forms in Appendix C.
7The Kullback-Leibler divergence can be seen as a special case of the Renyi divergence (van Erven &
Harremoes, 2014), where the latter has a stronger information-threotic underpinning.
5
Under review as a conference paper at ICLR 2022
Table 1: Overview over prior networks for classification. (*) OOD samples were created via tem-
perature scaling inspired by Liang et al. (2018). ID: In-distribution.
Method	Loss function	Architecture	Requires OOD samples?
Prior network (Malinin & Gales, 2018)	ID KL w.r.t smoothed label & OOD KL w.r.t. uniform prior	MLP / CNN	✓
Prior networks (Malinin & Gales, 2019)	Reverse KL of Malinin & Gales (2018)	CNN	✓
Information Robust Dirichlet Networks (Tsiligkaridis, 2019)	lp norm w.r.t one-hot label & Approx. Renyi divergence w.r.t. uniform prior	CNN	X
Dirichlet via Function Decomposition (Biloss et al., 2019)	Uncertainty Cross-entropy & mean & variance regularizer	RNN	X
Prior network with PAC Regularization (Haussmann et al., 2019)	Negative log-likelihood loss + PAC regularizer	BNN	X
Ensemble Distribution Distillation (Malinin et al., 2020b)	Knowledge distillation objective	MLP / CNN	X
Prior networks with representation gap (Nandy et al., 2020)	ID & OOD Cross-entropy + precision regularizer	MLP / CNN	✓
Prior RNN (Shen et al., 2020)	Cross-entropy + entropy regularizer	RNN	(✓)*
Graph-based Kernel Dirichlet distribution estimation (GKDE) (Zhao et al., 2020)	l2 norm w.r.t. one-hot label & KL reg. with node-level distance prior & Knowledge distillation objective	GNN	X
even in this framework high epistemic and high distributional uncertainty (Figures 2d and 2e) might
be confused, and instead propose novel loss functions producing a representation gap (Figure 2f;
check Appendix C for the final form), which aims to be more easily distinguishable. Lastly, Malinin
et al. (2020b) show that prior networks can also be distilled using an ensemble of classifiers and
their predicted categorical distributions (akin to learning Figure 2e from Figure 2a), which does not
require regularization at all (but training the ensemble).
An application to Natural Language Processing can be found in the work of Shen et al. (2020), who
train their recurrent neural network for spoken language understanding using a simple cross-entropy
loss and entropy regularizer. However, Bilos et al. (2019), Who apply their model to asynchronous
event classification, note that the standard cross-entropy loss only involves a point estimate ofa cate-
gorical distribution, discarding all the information contained in the predicted Dirichlet. For this rea-
son, they propose an uncertainty-aware cross-entropy (UCE) loss instead, which has a closed-form
solution in the Dirichlet case (see Appendix B.6). They further regularize the mean and variance for
OOD data points using an extra loss term.
3.3.2	Posterior Networks
Table 2: Overview over posterior networks for classification. OOD samples were created via (f) the
fast-sign gradient method (Kurakin et al.) or 叶)using a Variational Auto-Encoder (VAE; Kingma
& Welling, 2014. NLL: Negative log-likelihood. CE: Cross-entropy.
Method	Loss function	Architecture	Requires OOD samples?
Evidential Deep Learning (Sensoy et al., 2018)	l2 norm w.r.t. to one-hot label + KL w.r.t. uniform prior	CNN	X
Variational Dirichlet (Chen et al., 2018)	ELBO + Contrastive Adversarial Loss	CNN	✓
Belief Matching (Joo et al., 2020)	ELBO	CNN	X
Posterior networks (Charpentier et al., 2020)	Uncertainty CE (Biloss et al., 2019) + Entropy regularizer	MLP / CNN + Norm. Flow	X
Generative Evidential Neural Networks (Sensoy et al., 2020)	Contrastive NLL + KL between uniform & Dirichlet of wrong classes	CNN	(✓)*
6
Under review as a conference paper at ICLR 2022
As elaborated on in Section 2.1, choosing a Dirichlet prior, due to its conjugacy to the categorical
distribution, induces a Dirichlet posterior distribution. Like the prior in the previous section, this
posterior can be parameterized by a neural network. The challenges hereby are two-fold: Account-
ing for the number of class observations Nk that make up part of the posterior density parameters
β (Equation (2)), and, similarly to prior networks, ensuring the wanted behavior on the probability
simplex for in- and out-of-distribution inputs.
Sensoy et al. (2018) base their approach on the Dempster-Shafer theory of evidence (Yager & Liu,
2008; lending its name to the term “Evidential Deep Learning”) and its formalization via subjective
logic (Audun, 2018). In doing so, an agnostic belief in form of a uniform Dirichlet prior is updated
using the aforementioned (pseudo-)counts, which are predicted by a neural network. Sensoy et al.
(2018) train their model using a straightforward l2 loss between the predicted Dirichlet and the
one-hot encoded class label (Appendix B.4), as well as a regularization term consisting of the KL
divergence w.r.t. a uniform Dirichlet:
KL[p(μ | α)∣∣p(μ |1)] = - log B(K) + X(&k - 1)(Ψ(αk) - ψ(α0))
B(α)	k=1
In a follow-up work, Sensoy et al. (2020) train a
similar model using a contrastive loss with arti-
ficial OOD samples from a Variational Autoen-
coder (Kingma & Welling, 2014), and a KL-
based regularizer similar to that of Tsiligkaridis
(2019). Charpentier et al. (2020) also set α to
a uniform prior, but obtain class observations
Nk from the training set and scale them by the
probability of an input’s latent representation z
under a normalizing flow8 (NF; Rezende & Mo-
hamed, 2015) with parameters φ and one flow
instance per class (see Figure 3):
Figure 3: Schematic of a posterior network, taken
from Charpentier et al. (2020). An encoder fθ
maps inputs to a latent representation z. NFs then
model class-conditional densities, which are used
together with the prior concentration to produce
the posterior parameters.
βk = αk + Nk ∙ p(z |y = k, φ); Z = fθ(x)
This has the advantage of producing low prob-
abilities for strange inputs like the noise in Fig-
ure 3, which in turn translate to low concentra-
tion parameters of the posterior Dirichlet, as it
falls back onto the uniform prior. The model is then optimized using the same uncertainty-aware
cross-entropy loss as in Bilos et al. (2019) with an additional entropy regularizes
Another route lies in directly parameterizing the posterior parameters β . Because it is infeasible
to model the posterior this way due to an intractable integral, this leaves us to instead model an
approximate posterior using variational inference methods, which is exactly the approach of Joo
et al. (2020) and Chen et al. (2018). As the KL divergence between the true and approximate
posterior is infeasible to estimate as well, the variational methods usually optimizes the evidence
lower bound (ELBO) instead. For the Dirichlet family, the ELBO has an analytical solution (we
refer the reader to Appendix A.3 for a derivation of the expression):
LELBO=ψ(By) - ψ(β0) - log B(") + X(βk - Yk) (ψ(Pk) - ψ(Bθ))
B(γ)	k=1
8A NF is a deep generative model, estimating a density in the feature space by mapping it to a simple Gaus-
sian in a latent space by a series of invertible, bijective transformations. The probability of an input can then
be estimated by its latent encoding w.r.t. the simple Gaussian and the change-of-variable formula, traversing
the flow in reverse. Instead of mapping from the feature space into latent space, the flows in Charpentier et al.
(2020) map from the encoder latent space into a separate, second latent space.
7
Under review as a conference paper at ICLR 2022
4	Evidential Deep Learning for Regression
Table 3: Overview over Evidential Deep Learning methods for regression.
Method	Parameterized distribution	Loss function	Model
Deep Evidential Regression (Amini et al., 2020)	Normal-Inverse Gamma Prior	Negative log-likelihood loss + KL w.r.t. uniform prior	MLP / CNN
Regression Prior Network (Malinin et al., 2020a)	Normal-Wishart Prior	Reverse KL (Malinin & Gales, 2019)	MLP / CNN
Natural Posterior Network (Charpentier et al., 2021)	Inverse-χ2 Posterior	Uncertainty Cross-entropy (Biloss et al., 2019) + Entropy regularizer	MLP / CNN + Norm. Flow
Because the Evidential Deep Learning framework provides such appealing properties, the question
naturally arises of whether it can be extended to regression problems as well. The answer is yes,
although the Dirichlet distribution is not an appropriate choice in this case. It is very common to
model a regression problem using a normal likelihood (Bishop, 2006). As such, there are multiple
potential choices for a prior distribution. The methods listed in Table 3 either choose the Normal-
Inverse Gamma distribution (Amini et al., 2020; Charpentier et al., 2021), inducing a scaled inverse-
χ2 posterior (Gelman et al., 1995),9 as well as a Normal-Wishart prior (Malinin et al., 2020a). We
will discuss these approaches in turn.
Amini et al. (2020) model the regression problem as a normal distribution with unknown mean and
variance N(y; μ, σ2), and as such use a normal prior for the mean with μ 〜N(γ, σ2v-1) and
an inverse Gamma prior for the variance with σ2 〜 Γ-1 (α, β), resulting in a combined Inverse-
Gamma prior with parameters γ, v, α, β. These are then predicted by different “heads” of a neural
network. Aleatoric and epistemic uncertainty can then be estimated using the expected value of the
variance as well as the variance of the mean, respectively, which have closed form solutions under
this parameterization. The model is optimized using a negative log-likelihood objective along with
an evidence regularizer, akin to the entropy one for Dirichlet networks. In the work of Charpentier
et al. (2021), the authors generalize the approach behind the posterior networks by Charpentier et al.
(2020) to different distributions from the exponential family, keeping architecture and loss function
the same. Depending on the distributions used however, the UCE loss by Bilos et al. (2019) takes on
a different form. Malinin et al. (2020a) can be seen as the multivariate generalization of the work of
Amini et al. (2020), where a combined Normal-Wishart prior is formed to fit the now multivariate
normal likelihood. Again, the prior parameters are the output of a neural network, and uncertainty
can be quanitified in a similar way. For training purposes, they apply the reverse KL objective of
Malinin & Gales (2019) as well as the knowledge distillation objective of Malinin et al. (2020b).
5	Related Work
The need for the quantification of uncertainty in order to earn the trust of end-users and stakeholders
has been a key driver for research (Bhatt et al., 2021). Unfortunately, standard neural discriminator
architectures have been proven to possess unwanted theoretical properties w.r.t. to OOD inputs10
(Hein et al., 2019; Ulmer & Cina, 2020) and lacking calibration in practice (Guo et al., 2017). A
popular way to overcome these blemishes is by quantifying (epistemic) uncertainty by aggregating
multiple predictions by networks in the Bayesian Model Averaging framework (Jeffreys, 1998; Wil-
son & Izmailov, 2020), Laplace approximations (Kristiadi et al., 2020; Daxberger et al., 2021), vari-
ational methods (Gal & Ghahramani, 2016; Blundell et al., 2015), ensembling (Lakshminarayanan
et al., 2017) or mixtures of the latter two (Pearce et al., 2020; Wilson & Izmailov, 2020). Nev-
ertheless, many of these methods have been shown not to produce diverse predictions (Wilson &
9The form of the Normal-Inverse Gamma posterior and the Normal Inverse-χ2 posterior are interchangable
using some parameter substitutions (Murphy, 2007).
10Pearce et al. (2021) argue that some insights might partially be mislead by low-dimensional intuitions, and
that empirically OOD data in higher dimensions tend to be mapped into regions of higher uncertainty.
8
Under review as a conference paper at ICLR 2022
Izmailov, 2020; Fort et al., 2019) and to deliver subpar performance and potentially misleading un-
certainty estimates under distributional shift (Ovadia et al., 2019; Masegosa, 2020; Wenzel et al.,
2020; Izmailov et al., 2021a;b), raising doubts about their efficacy.
The methods in Section 3.3 and Section 4 can be seen as single-pass alternatives that avoid approxi-
mating the predictive distribution in Equation (3) via Monte Carlo estimates. The proposed Posterior
Network (Charpentier et al., 2020; 2021) can furthermore be seen as related to another, competing
approach, namely the combination of neural discriminators with density estimation methods, for
instance in the form of energy-based models (Grathwohl et al.; Elflein et al., 2021) or other hybrid
architectures (Lee et al., 2018; Mukhoti et al., 2021).
Some of the discussed models have already found a variety of applications, such as in autonomous
driving (Capellier et al., 2019; Liu et al., 2021; Petek et al., 2021), medical screening (Ghesu et al.,
2019; Gu et al., 2021), molecular analysis (Soleimany et al., 2021), and open set recognition (Bao
et al., 2021).
6	Discussion
Despite their advantages, the last chapters have highlighted key weaknesses of Dirichlet networks as
well: In order to achieve the right behavior of the distribution and thus guarantee sensible uncertainty
estimates, some approaches Malinin & Gales (2018; 2019); Nandy et al. (2020); Malinin et al.
(2020a) require out-of-distribution data points during training. This comes with two problems: Such
data is often not available or in the first place, or cannot guarantee robustness against other kinds of
unseen OOD data, of which infinite types exist in a real-valued feature space.11 Indeed, Kopetzki
et al. (2021) found OOD detection to deteriorate across a family of Dirichlet-based models under
adversarial perturbation and OOD data points. One possible explanation for this behavior might lie
in the insight that neural networks trained in the empirical risk minimization framework might learn
spurious but highly predictive features (Ilyas et al., 2019; Nagarajan et al., 2021). This way, inputs
stemming from the training distribution might be mapped to similar parts of the latent space as data
points outside the distribution even though they have (from a human perspective) blatant semantic
differences, simply because these semantic features were not useful to optimize for the training
objective. This can result in ID and OOD points having assigned similar feature representations by a
network, a phenomenon has been coined “feature collapse” (Nalisnick et al., 2019; van Amersfoort
et al., 2021; Havtorn et al., 2021). One strategy to mitigate (but not solve) this issue has been
to enforce a constraint on the smoothness of the neural network function (Wei et al., 2018; van
Amersfoort et al., 2020; 2021; Liu et al., 2020), thereby maintaining both a sensitivity to semantic
changes in the input and robustness against adversarial inputs (Yu et al., 2019). Nevertheless, this
question remains an open area of research and the impact on Evidential Deep Learning methods
underexplored.
7	Conclusion
This survey has given an overview over contemporary approaches for uncertainty estimation using
neural networks to parameterize conjugate priors or the corresponding posteriors instead of likeli-
hoods, with a focus on the Dirichlet distribution in a classification context. We highlighted their
appealing theoretical properties allowing for uncertainty estimation with minimal computational
overhead, rendering them as a viable alternative to existing strategies. We also emphasized practical
problems: In order to nudge models towards the desired behavior in the face of unseen or out-of-
distribution samples, the design of the model architecture and loss function have to be carefully
considered. At the moment, the entropy regularizer seems to be a sensible choice in prior networks
when OOD data is not available. Combining discriminators with generative models like normaliz-
ing flows like in (Charpentier et al., 2020; 2021), embedded in a sturdy Bayesian framework, also
appears as an exciting direction for practical applications. In summary, we believe that recent ad-
vances show promising results for Evidential Deep Learning, making it a viable option in the realm
of uncertainty estimation to improve safety and trustworthiness in Machine Learning systems.
11The same applies to the artificial OOD data in Chen et al. (2018); Shen et al. (2020); Sensoy et al. (2020).
9
Under review as a conference paper at ICLR 2022
References
Alexander Amini, Wilko Schwarting, Ava Soleimany, and Daniela Rus. Deep evidential regression.
In Hugo Larochelle, Marc’Aurelio Ranzato, Raia Hadsell, Maria-Florina Balcan, and Hsuan-Tien
Lin (eds.), Advances in Neural Information Processing Systems 33: Annual Conference on Neural
Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual, 2020.
Jsang Audun. Subjective Logic: A formalism for reasoning under uncertainty. Springer, 2018.
Wentao Bao, Qi Yu, and Yu Kong. Evidential deep learning for open set action recognition. In
Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 13349-13358,
2021.
Umang Bhatt, Javier Antoran, Yunfeng Zhang, Q. VeraLiao, Prasanna Sattigeri, Riccardo Fogliato,
Gabrielle Gauthier Melancon, Ranganath Krishnan, Jason Stanley, Omesh Tickoo, Lama Nach-
man, Rumi Chunara, Madhulika Srikumar, Adrian Weller, and Alice Xiang. Uncertainty as a
form of transparency: Measuring, communicating, and using uncertainty. In Marion Fourcade,
Benjamin Kuipers, Seth Lazar, and Deirdre K. Mulligan (eds.), AIES ’21: AAAI/ACM Conference
on AI, Ethics, and Society, Virtual Event, USA, May 19-21, 2021, pp. 401-413. ACM, 2021.
Marin Bilos, Bertrand Charpentier, and Stephan GUnnemann. Uncertainty on asynchronous time
event prediction. In Advances in Neural Information Processing Systems, pp. 12851-12860, 2019.
Christopher M Bishop. Pattern recognition. Machine learning, 128(9), 2006.
Charles Blundell, Julien Cornebise, Koray Kavukcuoglu, and Daan Wierstra. Weight uncertainty in
neural networks. arXiv preprint arXiv:1505.05424, 2015.
Edouard Capellier, Franck Davoine, VerOniqUe Cherfaoui, and You Li. Evidential deep learning
for arbitrary LIDAR object classification in the context of autonomous driving. In 2019 IEEE
Intelligent Vehicles Symposium, IV 2019, Paris, France, June 9-12, 2019, pp. 1304-1311. IEEE,
2019.
Bertrand Charpentier, Daniel Zugner, and Stephan Gunnemann. Posterior network: Uncertainty
estimation without OOD samples via density-based pseudo-counts. CoRR, abs/2006.09239, 2020.
Bertrand Charpentier, Oliver Borchert, Daniel Zugner, Simon Geisler, and Stephan Gunnemann.
Natural posterior network: Deep bayesian predictive uncertainty for exponential family distribu-
tions. arXiv preprint arXiv:2105.04471, 2021.
Wenhu Chen, Yilin Shen, Hongxia Jin, and William Wang. A variational dirichlet framework for
out-of-distribution detection. arXiv preprint arXiv:1811.07308, 2018.
Erik Daxberger, Agustinus Kristiadi, Alexander Immer, Runa Eschenhagen, Matthias Bauer,
and Philipp Hennig. Laplace redux-effortless bayesian deep learning. arXiv preprint
arXiv:2106.14806, 2021.
Armen Der Kiureghian and Ove Ditlevsen. Aleatory or epistemic? does it matter? Structural safety,
31(2):105-112, 2009.
Sven Elflein, Bertrand Charpentier, Daniel Zugner, and Stephan Gunnemann. On out-of-distribution
detection with energy-based models. arXiv preprint arXiv:2107.08785, 2021.
Stanislav Fort, Huiyi Hu, and Balaji Lakshminarayanan. Deep ensembles: A loss landscape per-
spective. arXiv preprint arXiv:1912.02757, 2019.
Yarin Gal and Zoubin Ghahramani. Dropout as a bayesian approximation: Representing model
uncertainty in deep learning. In International conference on Machine Learning, pp. 1050-1059,
2016.
Andrew Gelman, John B Carlin, Hal S Stern, and Donald B Rubin. Bayesian data analysis. Chap-
man and Hall/CRC, 1995.
10
Under review as a conference paper at ICLR 2022
Florin C. Ghesu, Bogdan Georgescu, Eli Gibson, Sebastian GundeL MannUdeeP K. Kalra, Raman-
deep Singh, Subba R. Digumarthy, Sasa Grbic, and Dorin Comaniciu. Quantifying and lever-
aging classification uncertainty for chest radiograPh assessment. In Dinggang Shen, Tianming
Liu, Terry M. Peters, Lawrence H. Staib, Caroline Essert, Sean Zhou, Pew-Thian YaP, and Ali R.
Khan (eds.), Medical Image Computing and Computer Assisted Intervention - MICCAI 2019 -
22nd International Conference, Shenzhen, China, October 13-17, 2019, Proceedings, Part VI,
Volume 11769 of Lecture Notes in Computer Science, pp. 676-684. Springer, 2019.
Will Grathwohl, Kuan-Chieh Wang, Jorn-Henrik Jacobsen, David Duvenaud, Mohammad Norouzi,
and Kevin Swersky. Your classifier is secretly an energy based model and you should treat it like
one. In 8th International Conference on Learning Representations, ICLR 2020, Addis Ababa,
Ethiopia, April 26-30, 2020.
Ang Nan Gu, Christina Luong, Mohammad H. Jafari, Nathan Van Woudenberg, Hany Girgis, Purang
Abolmaesumi, and Teresa Tsang. Efficient echocardiogram view classification with sampling-free
uncertainty estimation. In J. Alison Noble, Stephen R. Aylward, Alexander Grimwood, Zhe Min,
Su-Lin Lee, and Yipeng Hu (eds.), Simplifying Medical Ultrasound - Second International Work-
shop, ASMUS 2021, Held in Conjunction with MICCAI 2021, Strasbourg, France, September 27,
2021, Proceedings, volume 12967 of Lecture Notes in Computer Science, pp. 139-148. Springer,
2021.
Chuan Guo, Geoff Pleiss, Yu Sun, and Kilian Q. Weinberger. On calibration of modern neural
networks. In Doina Precup and Yee Whye Teh (eds.), Proceedings of the 34th International Con-
ference on Machine Learning, ICML 2017, Sydney, NSW, Australia, 6-11 August 2017, volume 70
of Proceedings of Machine Learning Research, pp. 1321-1330. PMLR, 2017.
Manuel Haussmann, Sebastian Gerwinn, and Melih Kandemir. Bayesian evidential deep learning
with pac regularization. arXiv preprint arXiv:1906.00816, 2019.
Jakob Drachmann Havtorn, Jes Frellsen, S0ren Hauberg, and Lars Maal0e. Hierarchical vaes know
what they don’t know. In Marina Meila and Tong Zhang (eds.), Proceedings of the 38th Inter-
national Conference on Machine Learning, ICML 2021, 18-24 July 2021, Virtual Event, volume
139 of Proceedings of Machine Learning Research, pp. 4117-4128. PMLR, 2021.
Matthias Hein, Maksym Andriushchenko, and Julian Bitterwolf. Why relu networks yield high-
confidence predictions far away from the training data and how to mitigate the problem. In IEEE
Conference on Computer Vision and Pattern Recognition, CVPR 2019, Long Beach, CA, USA,
June 16-20, 2019, pp. 41-50. Computer Vision Foundation / IEEE, 2019.
Dan Hendrycks and Kevin Gimpel. A baseline for detecting misclassified and out-of-distribution
examples in neural networks. In 5th International Conference on Learning Representations, ICLR
2017, Toulon, France, April 24-26, 2017, Conference Track Proceedings.
Eyke Hullermeier and Willem Waegeman. Aleatoric and epistemic uncertainty in machine learning:
an introduction to concepts and methods. Mach. Learn., 110(3):457-506, 2021.
Andrew Ilyas, Shibani Santurkar, Dimitris Tsipras, Logan Engstrom, Brandon Tran, and Aleksander
Madry. Adversarial examples are not bugs, they are features. In Hanna M. Wallach, Hugo
Larochelle, Alina Beygelzimer, Florence d,Alche—Buc, Emily B. Fox, and Roman Garnett (eds.),
Advances in Neural Information Processing Systems 32: Annual Conference on Neural Informa-
tion Processing Systems 2019, NeurIPS 2019, December 8-14, 2019, Vancouver, BC, Canada, pp.
125-136, 2019.
Pavel Izmailov, Patrick Nicholson, Sanae Lotfi, and Andrew Gordon Wilson. Dangers of bayesian
model averaging under covariate shift. arXiv preprint arXiv:2106.11905, 2021a.
Pavel Izmailov, Sharad Vikram, Matthew D. Hoffman, and Andrew Gordon Wilson. What are
bayesian neural network posteriors really like? In Marina Meila and Tong Zhang (eds.), Pro-
ceedings of the 38th International Conference on Machine Learning, ICML 2021, 18-24 July
2021, Virtual Event, volume 139 of Proceedings of Machine Learning Research, pp. 4629-4640.
PMLR, 2021b.
Harold Jeffreys. The theory of probability. OUP Oxford, 1998.
11
Under review as a conference paper at ICLR 2022
Taejong Joo, Uijung Chung, and Min-Gwan Seo. Being bayesian about categorical probability.
CoRR, abs/2002.07965, 2020.
Michael I Jordan, Zoubin Ghahramani, Tommi S Jaakkola, and Lawrence K Saul. An introduction
to variational methods for graphical models. Machine learning, 37(2):183-233, 1999.
Diederik P. Kingma and Max Welling. Auto-encoding variational bayes. In Yoshua Bengio and Yann
LeCun (eds.), 2nd International Conference on Learning Representations, ICLR 2014, Banff, AB,
Canada, April 14-16, 2014, Conference Track Proceedings, 2014.
Anna-Kathrin Kopetzki, Bertrand Charpentier, Daniel Zugner, Sandhya Giri, and StePhan
Gunnemann. Evaluating robustness of predictive uncertainty estimation: Are dirichlet-based
models reliable? In Marina Meila and Tong Zhang (eds.), Proceedings of the 38th International
Conference on Machine Learning, ICML 2021, 18-24 July 2021, Virtual Event, volume 139 of
Proceedings of Machine Learning Research, pp. 5707-5718. PMLR, 2021.
Agustinus Kristiadi, Matthias Hein, and Philipp Hennig. Being bayesian, even just a bit, fixes
overconfidence in relu networks. In Proceedings of the 37th International Conference on Machine
Learning, ICML 2020, 13-18 July 2020, Virtual Event, volume 119 of Proceedings of Machine
Learning Research, pp. 5436-5446. PMLR, 2020.
Morton Kupperman. Probabilities of hypotheses and information-statistics in sampling from
exponential-class populations. Selected Mathematical Papers, 29(2):57, 1964.
Alexey Kurakin, Ian J. Goodfellow, and Samy Bengio. Adversarial examples in the physical world.
In 5th International Conference on Learning Representations, ICLR 2017, Toulon, France, April
24-26, 2017, Workshop Track Proceedings.
Balaji Lakshminarayanan, Alexander Pritzel, and Charles Blundell. Simple and scalable predic-
tive uncertainty estimation using deep ensembles. In Advances in neural information processing
systems, pp. 6402-6413, 2017.
Kimin Lee, Kibok Lee, Honglak Lee, and Jinwoo Shin. A simple unified framework for detecting
out-of-distribution samples and adversarial attacks. In Samy Bengio, Hanna M. Wallach, Hugo
Larochelle, Kristen Grauman, Nicolo Cesa-Bianchi, and Roman Garnett (eds.), Advances in Neu-
ral Information Processing Systems 31: Annual Conference on Neural Information Processing
Systems 2018, NeurIPS 2018, December 3-8, 2018, Montreal, Canada,pp. 7167-7177, 2018.
Shiyu Liang, Yixuan Li, and R. Srikant. Enhancing the reliability of out-of-distribution image
detection in neural networks. In 6th International Conference on Learning Representations, ICLR
2018, Vancouver, BC, Canada, April 30 - May 3, 2018, Conference Track Proceedings, 2018.
Jiayu Lin. On the dirichlet distribution. Mater’s Report, 2016.
Jeremiah Z. Liu, Zi Lin, Shreyas Padhy, Dustin Tran, Tania Bedrax-Weiss, and Balaji Lakshmi-
narayanan. Simple and principled uncertainty estimation with deterministic deep learning via
distance awareness. In Hugo Larochelle, Marc’Aurelio Ranzato, Raia Hadsell, Maria-Florina
Balcan, and Hsuan-Tien Lin (eds.), Advances in Neural Information Processing Systems 33: An-
nual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12,
2020, virtual, 2020.
Zhijian Liu, Alexander Amini, Sibo Zhu, Sertac Karaman, Song Han, and Daniela L. Rus. Efficient
and robust lidar-based end-to-end navigation. In IEEE International Conference on Robotics and
Automation, ICRA 2021, Xi’an, China, May30-June5, 2021, pp. 13247-13254. IEEE, 2021.
Andrey Malinin and Mark J. F. Gales. Predictive uncertainty estimation via prior networks. In
Advances in Neural Information Processing Systems 31: Annual Conference on Neural Infor-
mation Processing Systems 2018, NeurIPS 2018, 3-8 December 2018, Montreal, Canada, pp.
7047-7058, 2018.
Andrey Malinin and Mark J. F. Gales. Reverse kl-divergence training of prior networks: Improved
uncertainty and adversarial robustness. In Advances in Neural Information Processing Systems
32: Annual Conference on Neural Information Processing Systems 2019, NeurIPS 2019, 8-14
December 2019, Vancouver, BC, Canada, pp. 14520-14531, 2019.
12
Under review as a conference paper at ICLR 2022
Andrey Malinin, Sergey Chervontsev, Ivan Provilkov, and Mark Gales. Regression prior networks.
arXiv preprint arXiv:2006.11590, 2020a.
Andrey Malinin, Bruno Mlodozeniec, and Mark J. F. Gales. Ensemble distribution distillation. In
8th International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia,
April 26-30, 2020, 2020b.
Andres R. Masegosa. Learning under model misspecification: Applications to variational and en-
semble methods. In Hugo Larochelle, Marc’Aurelio Ranzato, Raia Hadsell, Maria-Florina Bal-
can, and Hsuan-Tien Lin (eds.), Advances in Neural Information Processing Systems 33: Annual
Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12,
2020, virtual, 2020.
Jeffrey W. Miller. (ml 7.7.a2) expectation of a dirichlet random variable, 2011. URL https:
//www.youtube.com/watch?v=emnfq4txDuI.
Jishnu Mukhoti, Andreas Kirsch, Joost van Amersfoort, Philip HS Torr, and Yarin Gal. Deterministic
neural networks with appropriate inductive biases capture epistemic and aleatoric uncertainty.
arXiv preprint arXiv:2102.11582, 2021.
Kevin P Murphy. Conjugate bayesian analysis of the gaussian distribution. def, 1(2σ2)口6, 2007.
Vaishnavh Nagarajan, Anders Andreassen, and Behnam Neyshabur. Understanding the failure
modes of out-of-distribution generalization. In 9th International Conference on Learning Repre-
sentations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021, 2021.
Eric T. Nalisnick, Akihiro Matsukawa, Yee Whye Teh, Dilan Gorur, and Balaji Lakshminarayanan.
Do deep generative models know what they don’t know? In 7th International Conference on
Learning Representations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019, 2019.
Jay Nandy, Wynne Hsu, and Mong Li Lee. Towards maximizing the representation gap between
in-domain & out-of-distribution examples. Advances in Neural Information Processing Systems,
33, 2020.
Yaniv Ovadia, Emily Fertig, Jie Ren, Zachary Nado, David Sculley, Sebastian Nowozin, Joshua
Dillon, Balaji Lakshminarayanan, and Jasper Snoek. Can you trust your model’s uncertainty?
evaluating predictive uncertainty under dataset shift. In Advances in Neural Information Process-
ing Systems ,pp.13991-14002, 2019.
Tim Pearce, Felix Leibfried, and Alexandra Brintrup. Uncertainty in neural networks: Approxi-
mately bayesian ensembling. In International Conference on Artificial Intelligence and Statistics,
pp. 234-244, 2020.
Tim Pearce, Alexandra Brintrup, and Jun Zhu. Understanding softmax confidence and uncertainty.
arXiv preprint arXiv:2106.04972, 2021.
Kursat Petek, Kshitij Sirohi, Daniel Buscher, and Wolfram Burgard. Robust monocular localization
in sparse hd maps leveraging multi-task uncertainty estimation. arXiv preprint arXiv:2110.10563,
2021.
Danilo Jimenez Rezende and Shakir Mohamed. Variational inference with normalizing flows. In
Francis R. Bach and David M. Blei (eds.), Proceedings of the 32nd International Conference on
Machine Learning, ICML 2015, Lille, France, 6-11 July 2015, volume 37 of JMLR Workshop and
Conference Proceedings, pp. 1530-1538. JMLR.org, 2015.
Murat Sensoy, Lance Kaplan, and Melih Kandemir. Evidential deep learning to quantify classifica-
tion uncertainty. In Advances in Neural Information Processing Systems, pp. 3179-3189, 2018.
Murat Sensoy, Lance M. Kaplan, Federico Cerutti, and Maryam Saleki. Uncertainty-aware deep
classifiers using generative models. In The Thirty-Fourth AAAI Conference on Artificial Intelli-
gence, AAAI 2020, The Thirty-Second Innovative Applications of Artificial Intelligence Confer-
ence, IAAI 2020, The Tenth AAAI Symposium on Educational Advances in Artificial Intelligence,
EAAI 2020, New York, NY, USA, February 7-12, 2020, pp. 5620-5627. AAAI Press, 2020.
13
Under review as a conference paper at ICLR 2022
Yilin Shen, Wenhu Chen, and Hongxia Jin. Modeling token-level uncertainty to learn unknown
concepts in SLU via calibrated dirichlet prior RNN. CoRR, abs/2010.08101, 2020.
Lewis Smith and Yarin Gal. Understanding measures of uncertainty for adversarial example detec-
tion. In Proceedings of the Thirty-Fourth Conference on Uncertainty in Artificial Intelligence,
UAI2018, Monterey, California, USA, August 6-10, 2018, pp. 560-569, 2018.
Ava P Soleimany, Alexander Amini, Samuel Goldman, Daniela Rus, Sangeeta N Bhatia, and Con-
nor W Coley. Evidential deep learning for guided molecular property prediction and discovery.
ACS central science, 7(8):1356-1367, 2021.
Theodoros Tsiligkaridis. Information robust dirichlet networks for predictive uncertainty estimation.
arXiv preprint arXiv:1910.04819, 2019.
Dennis Ulmer and Giovanni Cina. KnoW your limits: Uncertainty estimation with relu classifiers
fails at reliable ood detection. arXiv preprint arXiv:2012.05329, 2020.
Joost van Amersfoort, Lewis Smith, Yee Whye Teh, and Yarin Gal. Uncertainty estimation using
a single deep deterministic neural network. In Proceedings of the 37th International Conference
on Machine Learning, ICML 2020, 13-18 July 2020, Virtual Event, volume 119 of Proceedings
of Machine Learning Research, pp. 9690-9700. PMLR, 2020.
Joost van Amersfoort, Lewis Smith, Andrew Jesson, Oscar Key, and Yarin Gal. On feature collapse
and deep kernel learning for single forward pass uncertainty. arXiv preprint arXiv:2102.11409,
2021.
Tim van Erven and Peter Harremoes. Renyi divergence and kullback-leibler divergence. IEEE
Trans. Inf. Theory, 60(7):3797-3820, 2014.
Xiang Wei, Boqing Gong, Zixia Liu, Wei Lu, and Liqiang Wang. Improving the improved training
of wasserstein gans: A consistency term and its dual effect. In 6th International Conference on
Learning Representations, ICLR 2018, Vancouver, BC, Canada, April 30 - May 3, 2018, Confer-
ence Track Proceedings, 2018.
Florian Wenzel, Kevin Roth, Bastiaan S Veeling, Jakub Swiatkowski, Linh Tran, Stephan Mandt,
Jasper Snoek, Tim Salimans, Rodolphe Jenatton, and Sebastian Nowozin. How good is the bayes
posterior in deep neural networks really? arXiv preprint arXiv:2002.02405, 2020.
Wikipedia.	Exponential family, 2021.	URL https://
en.wikipedia.org/wiki/Exponential_family#Moment-
generating_function_of _the_sufficient_statistic.	Accessed September
2021.
Andrew Gordon Wilson and Pavel Izmailov. Bayesian deep learning and a probabilistic perspective
of generalization. In Hugo Larochelle, Marc’Aurelio Ranzato, Raia Hadsell, Maria-Florina Bal-
can, and Hsuan-Tien Lin (eds.), Advances in Neural Information Processing Systems 33: Annual
Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12,
2020, virtual, 2020.
Ronald R Yager and Liping Liu. Classic works of the Dempster-Shafer theory of belief functions,
volume 219. Springer, 2008.
Fuxun Yu, Zhuwei Qin, Chenchen Liu, Liang Zhao, Yanzhi Wang, and Xiang Chen. Interpreting
and evaluating neural network robustness. In Sarit Kraus (ed.), Proceedings of the Twenty-Eighth
International Joint Conference on Artificial Intelligence, IJCAI 2019, Macao, China, August 10-
16, 2019, pp. 4199-4205. ijcai.org, 2019.
Xujiang Zhao, Feng Chen, Shu Hu, and Jin-Hee Cho. Uncertainty aware semi-supervised learning
on graph data. In Hugo Larochelle, Marc’Aurelio Ranzato, Raia Hadsell, Maria-Florina Balcan,
and Hsuan-Tien Lin (eds.), Advances in Neural Information Processing Systems 33: Annual Con-
ference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020,
virtual, 2020.
14
Under review as a conference paper at ICLR 2022
A	Fundamental Derivations
This appendix section walks the reader through generalized versions of recurring theoretical re-
sults using Dirichlet distributions in a Machine Learning context, such as their expectation in Ap-
pendix A.1, their entropy in Appendix A.2 and the Kullback-Leibler divergence between two Dirich-
lets in Appendix B.3.
A. 1 Expectation of a Dirichlet
Here, We show results for the quantities E[μk] and E[logμk]. For the first, one such derivation is
given by Lin (2016), however we instead adapt the more elegant solution by Miller (2011):
E[μk]
/…/μk"；(；(；) YYμα-1dμι...dμκ
Moving μkαk-1 out of the product:
=Z∙∙∙ Z Γ(α0)	μa-1+1 Π μ^Tdμi …dμκ
J J QK=1 Γ(ak) k 号 k
(8)
For the next step, we define a new set of Dirichlet parameters with βk = αk + 1 and ∀k0 6= k :
βk0 = αk0. Now we can see that for those new parameters, β0is defined as β0 = Pk βk = 1 + α0.
So by virtue of the Gamma function’s property that Γ(α0 + 1) = α0Γ(α0), replacing all terms in
the normalization factor will yield
_ f α αk	r(eo)	π βk-1 力 力 _ ak
=J …/α0∏κ=1rn)Uμ	dμ1...dμK = αo
where in the last step we obtain the final result, since the Dirichlet with new parameters βk must
nevertheless integrate to 1, and the integrals do not regard ak or ao. For the expectation E[log μk],
we first rephrase the Dirichlet distribution in terms of the exponential family (Kupperman, 1964).
The exponential family encompasses many commonly-used distributions, such as the normal, expo-
nential, Beta or Poisson, that all follow the form
p(x; η) = h(x) exp ηT u(x) - A(η)
with natural parameters η, sufficient statistic u(x), and log-partition function A(η). For the Dirich-
let distribution, Wikipedia (2021) provides the sufficient statistic as u(μ) = [log μι,..., μκ]T and
the log-partition function
K
A(α) = XlogΓ(αk) - logΓ(αo)
k=1
(9)
By Wikipedia (2021), we also find that by the moment-generating function that for the sufficient
statistic, its expectation can be derived by
∂A(η)
E[u(x)k ] = F
(10)
Therefore we can evaluate the expected value of log μk (i.e. the sufficient statistic) by inserting the
definition of the log-partition function in Equation (9) into Equation (10):
∂K
E[logμk]=标 Elogr(a®) - logΓ(αo) = Ψ(αk) - Ψ(ao)
k k=1
(11)
which corresponds precisely to the definition of the digamma function as
ψ(χ) = dx logΓ(χ).
15
Under review as a conference paper at ICLR 2022
A.2 Entropy of Dirichlet
The following derivation is adapted from Lin (2016), with the result stated in Charpentier et al.
(2020) as well.
H [μ] = -E [log p(μ∣a)]
=-E[log (Ba YK 靖-1)〕
K
=-E - log B(a) + X(αk - 1)log μk
k=1
K
=log B(a) - X(ak - 1)E[log μk]
Using Equation (11):
K
= log B(a) -	(αk - 1) ψ(αk) - ψ(α0)
KK
= log B(a) +	(αk - 1)ψ(α0) -	(αk - 1)ψ(αk)
K
= log B(a) + (α0 - K)ψ(α0) -	(αk - 1)ψ(αk)
A.3 Kullback-Leibler Divergence between two Dirichlets
The following result is presented using an adapted derivation by Lin (2016) and appears in Chen et al.
(2018) and Joo et al. (2020) as a starting point for their variational objective (see Appendix B.7). In
the following we use Dir(μ; a) to denote the optimized distribution, and Dir(μ; Y) the reference or
target distribution.
KL [p(μ | α)∣[p(μ | Y	)i =E	[log p(〃|a)〕 ]gp(μIy)]	=E	logp(μ I a)	-E	log p(μ I γ)
	=E	K - log B(a) + X(		αk - 1)log μk		
		l	k=1		-j	
		L	K		r	
	-E	-log B(γ) +£(Yk - 1)log μ k=1			k	
Distributing and pulling out B(α) and B(Y) out of the expectation (they don,t depend on μ):
-log Bab E
KX =
(ak - 1)log μk - (Yk - 1)log μk
-log Ba)+E
k
KX( =
-Yk)log μk
Moving the expectation inward and using the identity E[μk] = ψ(αk) - ψ(α0) from Appendix A.1:
B(γ)	K
-log Ba + Σ(αk - Yk)(ψ(ak) - ψ9O))
16
Under review as a conference paper at ICLR 2022
The KL divergence is also used by some works as regularizier by penalizing the distance to a uniform
Dirichlet with γ = 1 (Sensoy et al., 2018). In this case, the result above can be derived to be
KL∣p(μ I α)∣∣p(μ |1)] = log B(O) + X(ak - 1)(ψ(αk) - ψ(α°))
where the log Γ(K ) term can also be omitted for optimization purposes, since it does not depend on
α.
B Additional Derivations
In this appendix we results relevant in a Machine Learning context. These include derivations of
expected entropy (Appendix B.1) and mutual information (Appendix B.2) as uncertainty metrics for
Dirichlet networks. Also, we derive a multitude of loss functions, including the l∞ norm loss of a
Dirichlet w.r.t. a one-hot encoded class label in Appendix B.3, the l2 norm loss in Appendix B.4,
as well as the reverse KL loss by Malinin & Gales (2019), the UCE objective Bilos et al. (2019);
Charpentier et al. (2020) and ELBO Shen et al. (2020); Chen et al. (2018) as training objectives
(Appendices B.5 to B.7).
B.1	Derivation of Expected Entropy
The following derivation is adapted from Malinin & Gales (2018) appendix section C.4.
K
Ep(μ 1χ,什
[P(y| μ)i	= /p(μ |χ, θ)( - X
」	、 L— 1
μk log μk d μ
k=1
H
=-X/p(μ I χ, 0)(〃k logμk)dμ
k=1
Inserting the definition of p(μ ∣ x, D):
K
=-X
k=1
Singling out the factor μk:
α0)
Γ(αk0)
K
μc log μc ɪɪ μα -1 d μ
k0=1
=-X(方―IQr(α0)—fγ―T Zμklogμk	Y	μkk'-1 ∙μαk-1dμ
k=l v(αk) ∏ko ∈κ∖{k} r(ako )J	k0∈⅛}
Adjusting the normalizing constant (this is the same trick used in Appendix A.1):
K
-X
k=1
αc
Γ(α0 + 1)
αo Γ(αk + 1)Hko∈κ∖{k}Γ(αko)
l log μk Y μα -1 ∙ μα d μ
k	k0∈K∖{k}
Using the identity E[log μk] = ψ(αk) - ψ(α0) (Equation (11))
K
X αk (ψ(αk + 1) - ψ(αo + 1)
k=1 α0
—
We can accommodate the extra factor μk by adding 1 to its concentration parameter, adjusting the
whole normalizing constant, and thus obtaining an expectation of log μk w.r.t to a new Dirichlet
distribution which includes this very factor. Therefore, the resulting Digamma functions are also
adjusted to this new distribution.
17
Under review as a conference paper at ICLR 2022
B.2	Derivation of Mutual Information
We start from the expression in Equation (7):
I [y, μ∣x, Di = H Ep(μ | χ,D) [P(y| μ)] - Ep(“ ∣ 乂⑼ H [P(y| μ)]
Given that E[μk] = aak (APPendixA.1) and assuming that point estimate p(μ | x, D) ≈ p(μ | x, 0)
is sufficient (Malinin & Gales, 2018), we can identify the first term as the standard Shannon en-
tropy - PK=ι μk log μk = - PK=I ɑɑk log αθk. Furthermore, the second part We already derived in
APPendix B.1 and thus we obtain:
KK
-X αk log αk + X αk ”即 +1) - ψ(ao +1)
—
log αk - ψ(αk + 1) + Ψ(ao + 1)
α0
B.3	l∞ NORM DERIVATION
In this section We folloW elaborate on the derivation of Tsiligkaridis (2019) deriving a generalized lp
loss, upper-bounding the l∞ loss. This in turn alloWs us to easily derive the l2 loss used by Sensoy
et al. (2018); Zhao et al. (2020). Here We assume the classification target y to be provided in the
form of a one-hot encoded label y = [1y=1, . . . , 1y=K ]T .
Ep(μ | x,θ) [|| y - μ ll∞] ≤ Ep(μ | x,θ) [U y - μ ||p]
1/p
≤ (Ep(μI χ,θ) [|| y - μ ||p])
=(E [(i-μy )p] + X E[μP ])"p
k6=y
In order to compute the expression above, we first realize that all components of μ are distributed
according to a Beta distribution Beta(α, β) (since the Dirichlet is a multivariate generalization of the
beta distribution) for which the moment-generating function is defined as follows:
E[ p] = r(α + p)rCβ)r(a + B) =「(a + p)r(α + β)
Γ Γ(a + P + β)Γ(a)Γ(β) Γ(a + P + β )Γ(a)
Given that the first term in Appendix B.3 is characterized by Beta(α0 - αy, αy) and the second one
by Beta(αk, α0 - αk), we can evaluate it as follows:
Ep(μ I χ,θ)
||
i ≤ γ r(a0 - ay + p)r(a0 - M + 为)
∖r(ao- M+p + joy)r(ao- ay)
+
k6=y
Γ(ak + p)ΓQ⅛kT + ao -*)ʌ P
Γ(a^ + P + ao -M)Γ(ak))
y - μ
r(a0 - ay + p)r(aθ) +	r(ak + p)r(aθ) ∖ P
Γ(ao + p)Γ(ao - a#)	J Γ(p + a0)Γ(ak))
k6=y
∖
(Γ(a0) y
V(a0 + P))
r(Pk=y ak + P
、r(Pk=y °®)
+
k6=y
「(ak + P)
「(a®)
/
1
p
18
Under review as a conference paper at ICLR 2022
B.4	l2 NORM LOSS DERIVATION
Here We present an adapted derivation by Sensoy et al. (2018) for the l2-norm loss to train Dirichlet
netWorks. Here We again use an one-hot vector for a label With y = [1y=1, . . . , 1y=K]T.
ep(“ । χ,θ) [|| y - μ ||2]
EX k=1	(1y=k	-μk)2	(12)
			r
K EX	12 y=k	- 21y=k	μk+ μk	(13)
k=1			」
(14)
K
=X iy=k- 21y=kE[μk ]+ E[μk ]
k=1
Using the identity that E[μk] = E[μk]2 + Var(μk):
K
X12 k
y=k
k=1
K
-21y=kE[μk] + E[μk]2 + Var(μk)
(15)
X 1y=k-E
k=1
[μk ]) + Var(μk)
(16)
Finally, We use the result from Appendix A.1 and the result that Var(μk)
2016):
α0(αo + 1)
(see Lin,
K
X 1y=k
k=1
Ok∖2 + ak(αo - ak)
αo√	α2(ao + 1)
(17)
B.5 Derivation of Reverse KL loss
Here we re-state and annotate the derivation of reverse KL loss by Malinin & Gales (2019) in more
detail, starting form the forward KL loss by Malinin & Gales (2018).
K
Ep(χ,y) X 1y=kKL [p(μ |a) I ∖p(μ |x, θ)]
Ep(x,y)
XX ILkZP* ια)log ppμ⅛⅛ dμ
Writing the expectation explicitly:
=Z X p(y = k, χ) X 1y=k Z p(μ l<α)log ppμμ Xa θ) d μ d X
=Z X P(X)P (y = k|χ) X 1y=k Z p(μ la)log pp；j Xa θ) d μ d X
Ep(x)
k=1
P(y=k|x)	1y=k
k=1
∕p(μiα)ιog ppμ⅛⅜ dμ
K
K
Adding factor in log, collapsing double sum:
Ep(x)
归 p (y=k∣X)∕p(μ∣a)log C: Xa ；='kXX) )d μ
19
Under review as a conference paper at ICLR 2022
Reordering, separating constant factor from log:
Ep(x)
Z X P (y = k| χ)p(μ ∣a)(iog
k=1
K
PK=I P(y = k|X)p(μIa)
p(μ I χ, θ)
-log (X P(y = k| χ)) )dμ
k=1
X---------{---------}
=0
Ep(x)
KL XP(y = k| χ)p(μ ∣α) ∣ b(〃 I χ, θ)
k=1
'-------------{--------------}
Mixture of K Dirichlets
where we can see that this objective actually tries to minimizes the divergence towards a mixture of
K Dirichlet distributions. In the case of high data uncertainty, this is claimed incentivize the model
to distribute mass around each of the corners of the simplex, instead of the desired behavior shown
in Figure 2c. Therefore, Malinin & Gales (2019) propose to swap the order of arguments in the
KL-divergence, resulting in the following:
K
Ep(X)IXP(y = k∣ χ) ∙ KL [p(μ I χ, θ)∣ ∣p(μ ∣a)]
k=1
Ep(X) [X p(y =k| χ) ∙ / p(μ |χ, θ)log pPμμ Xa θ)d μ
Reordering:
Ep(X)
∕p(μlχ, θ) X P(L klχ)log ⅜⅛θ2 d μ
Ep(X)
Ep(X)
KK
Ep(μ । x,θ) XP(y = k| χ) logp(μ I χ, θ) - XP(y = k| χ) logp(μ ∣a)
k=1	k=1
KK
/ p(μ i χ, θ)(log (Y p(〃 I χ, θ)P (y=k| X) - log ∏ p(μ Ia)P(y=k1 X))卜 μ
Ep(X) /p(μ I χ, θ) (log (p(μ I χ, θ)PK=ι P(y=k1 X)
-log (Y (焉 Y *-IriI)d μ
k=1	k0=1
Ep(X)
Zp(μiχ,“log(p(μiχ,θ))-log(γ(B1a) Y即告'—)O ।'dμ
Ep(X)
/p(μ I χ, θ) (log (p(μ I χ, θ)) - log
K
Y μPK=ι P(y=k"-)dμ
k0=1
K
p(y = kI χ)αk0
k=1
Ep(X)
KL [p(μ i χ, θ^p(μIa)]
where a
Therefore, instead of a mixture of Dirichlet distribution, we obtain a single distribution whose pa-
rameters are a mixture of the concentrations of each class.
20
Under review as a conference paper at ICLR 2022
B.6	Uncertainty-aware Cross-Entropy Loss
The uncertainty-aware cross-entropy loss in Bilos et al. (2019); Charpentier et al. (2020) has the
form
LUCE = Ep(μ | x,θ) [log p(y| μ)] = E[log μy ] = ψ(αy) - ψ(a0)
as p(y| μ) is given by the true label in form of a delta distribution, we can apply the result from
Appendix A.1.
B.7	Evidence-Lower Bound For Dirichlet Posterior Estimation
The evidence lower bound is a well-known objective to optimize the KL-divergence between an
approximate proposal and target distribution (Jordan et al., 1999; Kingma & Welling, 2014). We
derive it based on Chen et al. (2018) in the following for the Dirichlet case with a proposal distri-
bution p(μ | x, θ) to the target distribution p(μ |y). For the first part of the derivation, We omit the
dependence on β for clarity.
丫T「/ I nx∣∣ /	I ʌi 1,7	Γ1 p(μ|χ,θ)] IIr	Γ1 p(μ|χ,θ)p(y)
KL h(μ |x, θ)	||p(〃	|y)J	= Ep(“ | χ,θ)	[log p(μ∣y) J	= Ep(“ | χ,θ)	[log —pμy)—
Factorizing p(μ, y) = P(y| μ)p(μ), pulling outp(y) as it doesn,t depend on μ:
Ep(μ । χ,θ) [log 7p(μ | ： θ))] + p(y)
L	P (y| μ)p(μ)J
Ep(μ I χ,θ)
log p(μ,(j)θ) - ep(“ i χ,θ) [logP(y| μ)]+ p(y)
≤ KL[p(μ |χ, θ)∣∣p(μ)] - Ep(μ ∣χ,θ)[logP(y| μ)]
Now note that the second part of the result is the uncertainty-aware cross-entropy loss from Ap-
pendix B.6 and re-adding the dependence of p(μ) on Y, we can re-use our result regarding the
KL-divergence between two Dirichlets in Appendix A.3 and thus obtain:
B(β)	K
LELBO = ψ(βy) - ψ(β0) - log B(Y) + y^(βk - Yk) {ψ(Bk) - ψ(β0))	(18)
which is exactly the solution obtained by both Chen et al. (2018) and Joo et al. (2020).
C Overview over Loss Functions
In Tables 4 and 5, we compare the forms of the loss function used by Evidential Deep Learning
methods for classification, using the consistent notation from the paper. Most of the presented
results can be found in the previous Appendix A and Appendix B. We refer to the original work for
details about the objective of Nandy et al. (2020).
21
	Table 4: Overview over objectives used by prior networks for classification.
Method	Loss function	Regularizer	Comment
Prior networks (Malinin & Gales, 2018)	log B(α) + PK=ι(αk - ak)(ψ(ak) - ψ(a0))	- log BK) + PK=ι(αk - 1) Wak) - ψ(α0))	Target concentration parameters α are created using a label smoothing approach, 1 - (K - 1)ε	ify = k i.e. μk= V	if y = k. Together with setting αο as a hyperparameter, αk = μka0
Prior networks (Malinin & Gales, 2019)	log B∣(α) + PK=ι(ak - ak)(ψ(ak) - ψ(a0))	log Bl) + PK=ι(ak - aQ(ψ(ak) - ψ(a0))	Similar to above, aCk) = 1c=ka^ + 1 for in-distribution and αCk) = 1c=kQ0ut + 1 where we have hyperparameters set to αin = 0.01 and aout = 0. Then finally, α = PK=I p(y = k| x)&k and α = PK=ι p(y = k| x)ak. 1	∖ p
Information Robust Dirichlet Networks (Tsiligkaridis, 2019) Dirichlet via Function Decomposition (Bilos et al.,2019)	(Froo⅛j Plr'P=αk+ρ) + Pk=y rΓ(α+P) I	2 Pk=y(a -I)2(w(i)(Qk) -ψ(I))(Ol))	ψ(I) is the Polygamma function defined as [γ [ Pk= αkk	k J	ψ(I)(X) = dX ψ(x). ψ(ay) - ψ(QO)	λι R0T μk(T)2dτ + λ2 r0t(ν - σ2(τ))2dτ	Factors λ1 and λ2 that are treated as hyperparameters that weigh first term pushing the for logit k to zero, while pushing the variance in the first term to ν.
Prior network with PAC Reg. (Haussmann et al., 2019) Ensemble Distribution Distillation (Malinin et al., 2020b) Prior networks with representation gap (Nandy et al., 2020) Prior RNN (Shen et al., 2020) Graph-based Kernel Dirichlet dist. est. (GKDE) (Zhao et al., 2020)	-log E∣^QK=ι (Ok j	1	ʤp" |α" N" ")_logδ - ι	The expectation in the loss function is evaluated using parameter samples from a weight distribution. δ∈ [0, 1]. ψ(αο) — PK=I ψ(ak) + mM PMM=I PK=ι(ak — 1)	-	The objective uses predictions from a trained ensemble log p(y = k| x, θ(m))	with parameters θ1, . . . ,θM . 一log μy — λK^ PK=I σ(ak)	— Pk=ι K log μk — λKut PK=I σ(αQ	The main objective is being optimized on in-distribution, the regularizer on out-of-distribution data. λin and λout weighing terms and σ denotes the sigmoid function. K Ek=I 1k=y log μk	— log B(α) + (aο — K)ψ(aο) — ɪ2k=ι(ak — 1)ψ(aQ	Here, the entropy regularizer operates on a scaled version of the concentration parameters α = (IK — W) α, where W is learned. PK=I fly=k — O0) + O⅛O0-Ok)	— log Ba) + PlK=ι(ak — Qk)(ψ(αk) — ψ(aο))	α here corresponds to a uniform prior including some 0	information about the local graph structure. The authors also use an additional knowledge distillation objective, which was omitted here since it doesn’t related to the Dirichlet.
Under review as a conference paper at ICLR 2022
K
Table 5: Overview over objectives used by posterior networks for classification.
Method	Loss function	Regularizer	Comment
Evidential Deep Learning (Sensoy et al., 2018) Variational Dirichlet (Chen et al., 2018) Belief Matching (Joo et al., 2020) Posterior networks (Charpentier et al., 2020)	PK=I (1y=k- αo )2+α⅛α-⅛ ψ(βy) - ψ(β0) ψ(βy) - ψ(β0) ψ(βy) - ψ(β0)	-	log B(Kαy+ PK=ι(αk- 1)(ψ(αk)- ψ(αo)) -	log BB(β) + PK=ICek -Yk) (ψCek)- ψCβO)) -	log Be + PK=1(βk - Yk) (ψ(fk) - ψCβ0)) -	log BCα) + Cα0 - K)ψCα0) - PkK=1Cαk - 1)ψCαk)	
Generative Evidential Neural Network (Sensoy et al., 2020)	- PkK=1 Epin(x)log(σ(fθ(x))) + Epout(x) log(1 - σ(fθ(x)))	-log BrOKy) + Pk=y(αk - 1)(ψ(αk) - ψ(αθ))	The main loss is a discriminative loss using ID and OOD samples, generated by a VAE. The regularizer is taken over all classes excluding the true class y (also indicated by α-y).
Under review as a conference paper at ICLR 2022