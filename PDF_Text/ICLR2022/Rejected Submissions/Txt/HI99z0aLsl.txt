Under review as a conference paper at ICLR 2022
Benign Overfitting in Adversarially Robust
Linear Classification
Anonymous authors
Paper under double-blind review
Ab stract
“Benign overfitting”, where classifiers memorize noisy training data yet still
achieve a good generalization performance, has drawn great attention in the ma-
chine learning community. To explain this surprising phenomenon, a series of
works have provided theoretical justification in over-parameterized linear regres-
sion, classification, and kernel methods. However, it is not clear if benign overfit-
ting still occurs in the presence of adversarial examples, i.e., examples with tiny
and intentional perturbations to fool the classifiers. In this paper, we show that
benign overfitting indeed occurs in adversarial training, a principled approach to
defend against adversarial examples. In detail, we prove the risk bounds of the
adversarially trained linear classifier on the mixture of sub-Gaussian data under
`p adversarial perturbations. Our result suggests that under moderate perturba-
tions, adversarially trained linear classifiers can achieve the near-optimal standard
and adversarial risks, despite overfitting the noisy training data. Numerical exper-
iments validate our theoretical findings.
1	Introduction
Modern machine learning methods such as deep learning have made many breakthroughs in a vari-
ety of application domains, including image classification (He et al., 2016; Krizhevsky et al., 2012),
speech recognition (Hinton et al., 2012) and etc. These models are typically over-parameterized:
the number of model parameters far exceeds the size of the training samples. One mystery is that,
these over-parameterized models can memorize noisy training data and yet still achieve quite good
generalization performances on the test data (Zhang et al., 2017). Many efforts have been made
to explain this striking phenomenon, which against what the classical notion of overfitting might
suggest. A line of research works (Soudry et al., 2018; Ji & Telgarsky, 2019b; Nacson et al., 2019;
Gunasekar et al., 2018b;a) shows that there exists the so-called implicit bias (Neyshabur, 2017): the
training algorithms tend to converge to certain kinds of solutions even with no explicit regulariza-
tion. Specifically, Soudry et al. (2018); Ji & Telgarsky (2019b); Nacson et al. (2019) demonstrate
that gradient descent trained linear classifiers on logistic or exponential loss with no regularization
asymptotically converge to the maximum L2 margin classifier. Recent works (Bartlett et al., 2020;
Chatterji & Long, 2020; Cao et al., 2021; Wang & Thrampoulidis, 2021; Tsigler & Bartlett, 2020)
further shows that over-parameterized and implicitly regularized interpolators can indeed achieve
small test error, and formulate this phenomenon as “benign overfitting”. More concretely, suppose
the classification model f is parameterized by θ ∈ Θ and the loss is denoted as '(∙). The population
risk is define as
P(x,y)〜D [fθ (x) = y],
where data pair (x, y ) is generated from certain data generation model. Chatterji & Long (2020)
shows that with sufficient over-parameterization, gradient descent trained maximum L2 margin
classifier can achieve nearly optimal population risk on noisy data for data generated from a
sub-Gaussian mixture model. This suggests that the overfitting can be “benign” in the over-
parameterized setting.
Besides these studies on the benign overfitting phenomenon, another well-known feature of mod-
ern machine learning methods is that they are vulnerable to adversarial examples. Recent studies
(Szegedy et al., 2013; Goodfellow et al., 2015) show that modern machine learning systems are
brittle: slight input perturbation that is imperceptible to human eyes could mislead a well-trained
classifier into wrong classification result. These malicious inputs are also known as the adversarial
1
Under review as a conference paper at ICLR 2022
examples (Szegedy et al., 2013; Goodfellow et al., 2015). Adversarial examples raise severe trust-
worthy issues and security concerns on the current machine learning systems especially in security-
critical applications. Various methods (Kurakin et al., 2016; Madry et al., 2018; Zhang et al., 2019;
Wang et al., 2019; 2020) have been proposed to defend against the threats posed by adversarial ex-
amples. One of the notable approaches is adversarial training (Madry et al., 2018). Specifically,
adversarial training solves the following min-max optimization problem,
1n
min — > max /(fθ (χi),yi),
θ∈θ n i=1 χi∈BP(Xi) θ ’ E J
where {(xi, yi)}in=1 is the training set and Bp(xi) = {x : kx - xikp ≤ } denotes the -ball around
xi in `p norm (p ≥ 1). Many empirical or theoretical studies have been conducted trying to analyze
or further improve adversarial training robustness (Zhang et al., 2019; Rice et al., 2020; Wang et al.,
2020; Carmon et al., 2019; Wang et al., 2019; Raghunathan et al., 2020). A recent work (Sanyal
et al., 2021) also pointed out that normally trained interpolators with the presence of label noise are
unlikely to be adversarially robust, while adversarially robust classifiers cannot overfit noisy labels
under certain conditions. However, it is still not clear whether the benign overfitting phenomenon
occurs for extremely over-parameterized models in the presence of adversarial examples.
In this paper, we show that benign overfitting indeed occurs in adversarial training. In order to
properly characterize the benign overfitting phenomenon on adversarial training, we also define the
population adversarial risk, which is the counterpart for population risk in standard training scenario:
P(x,y)〜DmX0 ∈ Bp(X)S.t., fθ(x0) = y].
The adversarial risk measures the misclassification rate of the target classifier under the presence
of `p -norm adversarial perturbations. It is easy to observe that the adversarial risk is always larger
than standard risk as it requires the classifier to correctly classify the data examples within the entire
local `p norm ball.
We summarize our contributions of this paper in the following
•	We show that the benign overfitting phenomenon can occur in adversarially robust linear classi-
fiers with sufficient over-parameterization. Specifically, under moderate `p norm perturbations,
adversarially trained linear classifiers can achieve the near-optimal standard and adversarial
risks, in spite of overfitting the noisy training data.
•	When the perturbation strength is set to be 0, our adversarial risk bound reduces to the stan-
dard one. The resulting standard risk bound extends Chatterji & Long (2020)’s risk bound to
further characterize the behavior of the linear classifier trained by t-step gradient descent.
•	We show that depending on the value of p (perturbation norm), the adversarial risk bound can
be different. The higher value of p (typically for p ≥ 2 case) actually leads to a larger gap
between the adversarial risk and the standard risk with the same .
Notation. we use lower case letters to denote scalars and lower case bold face letters to denote
vectors. For a vector X ∈ Rd, We denote its 'p norm (p ≥ 1) of X by ∣∣xkp = (Pd=1 ∣Xi∣p)"p,
the '∞ norm of X by ∣∣x∣∞ = maxd=1 |xi|. We denote X^p as the element-wisep-power of x. For
p ≥ 1, We denote Brp(X) as the `p norm ball of radius r centered at X. Given tWo sequences {an}
and {bn}, we write an = O(bn) if there exists a constant 0 < C < +∞ such that an ≤ C bn. We
denote an = Ω(bn) if bn = O(an). We denote an = Θ(bn) if an = O (bn) and an = Ω(bn).
2	Related Work
There exists a large body of works on adversarial training, implicit bias and benign overfitting. In
this section, we review the most relevant works with ours.
Adversarial Training. Adversarial training (Madry et al., 2018) and its variants (Zhang et al., 2019;
Wang et al., 2019; 2020) are currently the most effective type of approaches to empirically defend
against adversarial examples (Szegedy et al., 2013; Goodfellow et al., 2015). And many attempts
have been made to understand its empirical success. Charles et al. (2019); Li et al. (2020) showed
that the adversarially trained linear classifier directionally converges to the maximum margin classi-
fier. Gao et al. (2019); Zhang et al. (2020b) showed that adversarial training with neural networks can
achieve low robust training loss. Yet these conclusions cannot explain the test (population) perfor-
mances. Another line of research focuses on the generalization performance of adversarial training
2
Under review as a conference paper at ICLR 2022
and the number of training samples. Schmidt et al. (2018) showed that adversarial models require
more data than standard models to achieve certain test accuracy. Chen et al. (2020) showed that more
data may actually increase the gap between the generalization error of adversarially-trained models
and standard models. Yin et al. (2019); Cullina et al. (2018) studied the adversarial Rademacher
complexity and VC-dimensions. Some other works focus on the trade-off between robustness and
natural accuracy (Zhang et al., 2019; Tsipras et al., 2019; Wu et al., 2020; Raghunathan et al., 2020;
Yang et al., 2020; Dobriban et al., 2020; Javanmard & Soltanolkotabi, 2020), adversarial model
complexity lower bound (Allen-Zhu & Li, 2020), as well as the provable robustness upper bound
(Fawzi et al., 2018; Zhang et al., 2020a).
Recently, some works also focus on studying the learning of robust halfspaces and linear models.
Montasser et al. (2020) studied the conditions on the adversarial perturbation sets under which halfs-
paces are robustly learnable in the presence of random label noise. Diakonikolas et al. (2020) studied
the computational complexity of adversarially robust halfspaces under `p norm perturbations. Zou
et al. (2021) showed that adversarially trained halfspaces are provably robust with low robust clas-
sification error in the presence of noise. Dan et al. (2020) proposed an adversarial signal to noise
ratio and studied the excess risk lower/upper bounds for learning Gaussian mixture models. Taheri
et al. (2020); Javanmard & Soltanolkotabi (2020) studied adversarial learning of linear models on
Gaussian mixture data where the data dimension and the number of training data points have a fixed
ratio.
Implicit Bias. Several recent works studied the implicit bias of various training algorithms in over-
parameterized models. Soudry et al. (2018) studied the implicit bias of gradient descent trained on
linearly separable data while Ji & Telgarsky (2019b) studied the non-separable case. Gunasekar
et al. (2018a) studied the implicit bias of various optimization methods in linear regression and
classification problems. Ji & Telgarsky (2019a) studied the implicit bias for deep linear networks and
Arora et al. (2019); Gunasekar et al. (2018b) studied the implicit bias for matrix factorization. Lyu
& Li (2020) studied the implicit regularization of homogeneous neural networks with exponential
loss and logistic loss.
Benign Overfitting and Double Descent. A series of recent works have studied the “benign over-
fitting” phenomenon Bartlett et al. (2020) that when training over-parameterized models, classifiers
can still achieve good population risk even when overfitting the noisy training data. Bartlett et al.
(2020); Tsigler & Bartlett (2020) studied the risk bounds for over-parameterized linear (ridge) re-
gression and showed that under certain settings, the interpolating linear model with minimum param-
eter norm can have asymptotically optimal risk. Chatterji & Long (2020); Cao et al. (2021); Wang &
Thrampoulidis (2021) studied the risk bounds in linear logistic regression and linear support vector
machines. Belkin et al. (2018; 2019a;b); Hastie et al. (2019); Wu & Xu (2020) further quantified the
dependency curve between the population risk and the degree of over-parameterization and showed
that the curve has a double-descent shape.
3	Problem Setting and Preliminaries
We consider a sub-Gaussian mixture data generation model in our work. Specifically, the clean data
(e, e)〜D is generated such that, for each data point (e, y) ∈ Rd X {士1}, We have y 〜 Unif({±1})
and e = yμ + ξ where ξ ∈ Rd and ξ1,ξ2 ,...,ξd are i.i.d. zero-mean SUb-GaUssian variables with
sub-Gaussian norm at most 1. The actual data examples are sampled from a noisy distribution D
which is close to the clean distribUtion De. Specifically, D can be any distribUtion over Rd × {±1}
who has the same marginal distribUtion on Rd and the total variation distance dTV(D, De) ≤ η where
η denotes the noise level.
Note that oUr data generation model is standard for stUdying the popUlation risk of over-
parameterized linear classification. In fact, it is exactly the same as the one stUdied in Chatterji
& Long (2020). In this model, following standard coUpling lemma (Lindvall, 2002), there always
exists a joint distribUtion on original data and noisy data ((xe, ye), (x, y)) sUch that the marginal dis-
tribUtion for (xe, ye) is D, the marginal distribUtion for (x, y) is D, P[x = xe] = 1 and P[y 6= ye] ≤ η.
In this paper, we stUdy the problem of robUst binary classification with training data {(xi, yi)}in=1
drawn i.i.d. from the distribUtion D. Let’s denote the “clean” sample index as C := {k : yk = yek}
and the “noisy” sample index as N := {k : yk 6= yek}. We consider the adversarially trained linear
3
Under review as a conference paper at ICLR 2022
Algorithm 1 Gradient Descent Adversarial Training
1:	input: Training data {xi, yi}in=1, number of training iterations T, maximum perturbation
strength , training step sizes αt ;
2:	initialize model parameter θ0 = 0
3:	for t = 1, . . . , T do
4:	for each {xi , yi} do
5:	x0i = argmaxx0i∈Bp(xi) exp(-yiθt>-1x0i)
6:	end for
7:	θt = θt-i - αt ∙VθL(θt-i)
8:	end for
classifier under exponential loss. In such case, the adversarial loss can be explicitly written as
n
L(θ) = X max exp(-yiθ>x0i).	(3.1)
i=1 x0i ∈Bp(xi)
In gradient descent adversarial training algorithm, the adversarial loss L(θ) is minimized by first
solving the inner maximization problem in (3.1) with respect to the current model parameter θt-1
and then update the model parameter θt by performing gradient descent to minimize the adversarial
loss in each iteration. We summarized the training procedure for gradient descent adversarial train-
ing1 in Algorithm 1. Note that in the linear classifier setting, the inner maximization problem in
(3.1) has the following property
argmax exp(-yiθ>x0i) = argmax exp(-yiθ>(xi + ui)) = argmin yiθ>ui.	(3.2)
x0i ∈Bp (xi)	ui∈Bp(0)	kui kp≤
By Holders, inequality it is easy to observe that the optimal adversarial loss and the corresponding
gradient can be written as
nn
L(θ) = Xexp(-yiθ>Xi + d∣θkq), VθL(θ) = — X(yiXi - e ∙ ∂∣∣θkq) exp(-yiθ>Xi + e∣∣θkq),
i=1	i=1
where 1/p+1/q = 1. Also note that in the over-parameterized settings, training examples draw from
our data generation model are linearly separable with high probability (See Lemma 5.1 in Section
5). Linearly separable property ensures that the training samples have a positive margin (with high
probability). Following Li et al. (2020), we also define the standard and adversarial margin as
Y= max min yiθ>Xi, Y := max min min yiθ>χi,	(3.3)
kθkq =1 i∈[n]	kθk2=1 i∈[n] x0i∈Bp(xi)
which are useful in our later analysis. We also define the unique linear classifier θ that achieves
adversarial margin Y defined above as w.
4	Main Results
In this section, we study both the behavior of the population risk and the population adversarial risk
for adversarially trained linear classifiers.
Assumption 4.1. The adversarial perturbation radius is upper bounded by a constant R and is
smaller than the 'p data margin γ, i.e., E ≤ min{R, 7}.
The goal of adversarial training is to obtain high-accuracy classifiers that are also robust to small
input perturbations which can be ignored by human beings (e.g., small '∞-norm perturbations that
are invisible to human eyes). Therefore, Assumption 4.1 is reasonable by constraining the maximum
allowable perturbation magnitude.
Assumption 4.2. The noise ξ in the data generation model satisfies that E[∣ξ∣22] ≥ κd for some
constant κ.
Assumption 4.2 is a common condition that has also been considered in Chatterji & Long (2020). It
ensures that the summation of the variances of the data input increases in the order of Θ(d). Clearly,
this assumption covers the most common setting where the entries of ξ are i.i.d. and have a variance
larger than or equal to κ.
1	Note that in practice people often initialize θ0 by a small random vector (e.g., Xavier initialization (Glorot
& Bengio, 2010)), while we follow Li et al. (2020) and set θ0 = 0 for the ease of theoretical analysis.
4
Under review as a conference paper at ICLR 2022
Assumption 4.3. The gradient descent starts at 0, and the step sizes are set as α0 = 1/(Gdn),
3q-2
at = a ≤ 1/(GdnM) for M = max{[2d + e(q — 1)d2q-2 /γ] exp(-γ2/(Gd) + e/G), 1} and a
constant G.
Assumption 4.3 summarizes our assumptions about the gradient descent algorithm on the adversarial
loss. The learning rate conditions here are to ensure the convergence of adversarial training, and is
inspired by Li et al. (2020).
We first present our theorem for standard risk of adversarial training method (Algorithm 1).
Theorem 4.4 (Standard Risk of Adversarial Training). For any p ∈ [1, +∞), suppose that As-
sumptions 4.1, 4.2 and 4.3 hold with κ ∈ (0, 1] and large enough constants R and G. Moreover,
for any δ ∈ (0,1), suppose the number of training samples n ≥ Clog(1∕δ), the dimension d ≥
C ∙ max{nkμk2, n2 log(n∕δ)}, the noise level η < 1∕C, and ∣∣μk2 ≥ Cmax{log(n∕δ), e∣∣μkq} for
a large enough constant C. Then with probability at least 1 - δ, adversarially trained linear classifier
fθt for sufficiently large t under `p -norm e-perturbation satisfies the following standard risk
P(x,y)〜D [fθt (χ) = y] ≤ η + exp
((kμ∣2- 4e∣μkq)
I	(C00 + e)√d
C000 kμk2 logn
log t
where C0, C00, C000 > 0 are absolute constants, 1/p + 1/q = 1.
Remark 4.5. Theorem 4.4 presents the standard risk of adversarial training under `p norm pertur-
bations. Note that adversarially trained linear classifier enjoys a bounded population risk which
decreases as the number of training iterations t increases. Specifically, when t → ∞, we have
t→ιn P(x,y)〜D [fθt (x) = y] ≤ η + exp
(kμ∣2- 4e∣μkq) Y
(C00 + e)√d	)
(4.1)
Remark 4.6. For (4.1), consider the case when the sample size n is fixed but dimension d and ∣∣μ∣2
are growing, we discuss the conditions to reach minimum standard risk of noise level η. Note that
when 1 ≤ P ≤ 2 We have q ≥ 2 and ∣∣μ∣q ≤ ∣∣μ∣2. In this case, if ∣∣μ∣2 = Ω(d1/4), the standard
risk will come close to the noise level η when dis sufficiently large. When p > 2 and therefore q < 2,
we have ∣∣μ∣q ≤ d1几-1/2^^. In this case, if ∣∣μ∣2 = Ω(d1/4) and e = O(∣4112^1/0-1/2), the
standard risk will come close to the noise level η with sufficiently large d. Note that our theorem
condition also requires that ∣μ∣2 = O(√d). Therefore, in order to reach the standard risk of η, we
need ∣∣μ∣2 = Θ(dr) for some r ∈ (1/4,1∕2].
Remark 4.7. Choosing e = 0 will reduce to the standard training case. Specifically, ifwe set e = 0
in (4.1), it reduces to the same conclusion as Theorem 3.1 in Chatterji & Long (2020). However,
our result is more general, as it covers the setting of adversarial training and gives risk bounds for
the linear model obtained with a finite number of gradient descent iterations.
Theorem 4.8 (Adversarial Risk of Adversarial Training). For any δ ∈ (0, 1), under the same con-
ditions as in Theorem 4.4, with probability at least 1 - δ, the adversarially trained linear classifier
fθt for sufficiently large t under `p-norm e-perturbation satisfies the following adversarial risk if
1≤p≤2
P(x,y)〜D [∃x0 ∈ Bp(X)S.t., fθ(χ0) = y]
≤ η + exp
((∣μ∣2- 4针〃|国
I (C00 + e)√d
C0T及Ig log n
log t
and if p > 2,
P(x,y)〜D [∃x0 ∈Bp(x) s.t., fθ(x0) = y
≤ η + exp
(1^12-461^1^
I (C00 + e)√d
C0011〃||2 log n
log t
where C0, C00, C000 > 0 are absolute constants, 1/p + 1/q = 1.
Remark 4.9. Theorem 4.8 shows the adversarial risk of adversarial training under `p norm per-
turbations. The major difference from the standard risk (Theorem 4.4) lies in the additional e or
—
—
—
—
5
Under review as a conference paper at ICLR 2022
d1/q-1/2 term in the exponential function. This aligns with common sense that adversarial risk
should always be larger than the standard risk. This also suggests that for larger p-norm (p > 2)
perturbation, the same magnitude of perturbation would lead to a larger gap between the adversarial
risk and the standard risk. In terms of the perturbation strength, we can also observe that with a
larger , adversarially trained classifiers obtain worse adversarial risk. This has been verified by
many empirical observations of adversarial training (Madry et al., 2018; Zhang et al., 2019).
Remark 4.10. Note that when t → ∞, if 1 ≤ p ≤ 2, we have the following adversarial risk bound:
t→m∞ P(x,y)〜D [∃x0 ∈Bp(x),fθ(x0) = y] ≤ η + exp(-
and if p > 2, we have
t→ιn P(x,y)〜D [∃x0 ∈ Bp(X), fθ(x0) = y] ≤ η + exp
—
Similar to the standard risk case (Remark 4.6), When 1 ≤ P ≤ 2, if ∣∣μk2 = Θ(dr) for some
r ∈ (1/4, 1/2], the adversarial risk will also come close to the noise level η with sufficiently large
d. When P > 2, if we have ∣∣μ∣2 = Θ(dr) for some r ∈ (1/4,1/2] and e = O(kμk2∕d1∕q), the
adversarial risk Will be close to η With sufficiently large d. Note that compared to the standard risk,
this requirement on e is slightly stronger.
Remark 4.11. Note that our results imply a striking fact that unlike those observed in previous stud-
ies (e.g., Rice et al. (2020) showed that overfitting leads to worse empirical robustness on real image
distributions), overfitting in adversarial training can be benign for certain distributions. Specifically,
Remark 4.10 shows that for linear models with sub-Gaussian mixture data, the overfitting effect
is indeed benign. This is later empirically verified in the experiments for both linear and neural
network models.
5	Proof Outline of the Main Results
In this section, we present the proofs of our main theorems, which consists of three main steps.
Statistical properties of the training data points. We first list some basic properties of the training
data points based on our data model defined in Section 3.
Lemma 5.1 (Lemma 4.7 in Chatterji & Long (2020)). Let Zk = ykxk. There exist absolute con-
stants R, κ and G and C, such that if the assumptions in Theorem 4.4 hold, then with probability at
least 1 - δ,
—≤ ∣∣Zk 112 ≤ cod for all k ∈ [n],
c0
∣z>Zj∣ ≤ co(kμk2 + Pdlog(n∕δ)) for all i = j,
∣μ>Zk - k”∣2l ≤ kμk2∕2 for all k ∈ C,
∣”>Zk - (-kμ∣2)∣ ≤ kμ∣2∕2 for all k ∈N,
(5.1)
(5.2)
(5.3)
(5.4)
the number of noisy samples |N | ≤ (η+c1)n, and all training samples are linearly separable, where
c0 > 1 is an absolute constant.
Lemma 5.1 directly follows Lemma 4.7 in Chatterji & Long (2020). It provides direct high proba-
bility bounds for ∣∣Zk ∣∣2 and μ>Zk and also suggests that Zk vectors are nearly pairwise orthogonal
in over-parameterized settings. It also guarantees that training examples are linearly separable with
high probability.
Landscape properties of the training objective function. Given the properties of the training data
points, we proceed to establish landscape properties of the objective function L(θ1). The following
lemma bound the loss for the adversarially trained classifier in step 1.
Lemma 5.2. [Theorem 3.4 in Li et al. (2020)] Under the same conditions as in Theorem 4.4, with
probability at least 1 - δ, we have L(θ1) ≤ 2n, and
L(θt+1) ≤ L(θt),	(5.5)
θt>w	c3logn
1 - --~_-- ≤----------
l∣θt∣2 — log t
for all t > 0, where c3 is an absolute constant.
(5.6)
6
Under review as a conference paper at ICLR 2022
By Lemma 5.2, one can easily observe that the adversarial training loss is bounded by 2n along
the entire training trajectory. Lemma 5.2 also suggests that when t → ∞, the adversarially trained
classifier θt will converge in direction to the max adversarial margin classifier w defined in (3.3).
Length and direction of the adversarial training iterates θt . We also establish properties of the
adversarial training iterates θt . We have the following lemmas.
Lemma 5.3. Under the same conditions as in Theorem 4.4, for all adversarial training iteration
t > 0, with probability at least 1 - δ, we have kθt+1 k2 ≤ (√C0 + e)√dPtm=O αmL(θm), where
c0 is the absolute constant in Lemma 5.1.
Lemma 5.3 upper bound the L2 norm of adversarially trained classifier θt by the summation of
training losses along the training trajectory.
Lemma 5.4. Let zk = ykxk, under the same conditions as in Theorem 4.4, for all adversar-
ial training iteration t ≥ 0, with probability as least 1 - δ, we have maxkn=1 exp(-θt>zk) ≤
c3 minkn=1 exp(-θt>zk), where c3 > 0 is an absolute constant.
Lemma 5.4 provides us a way to control the loss the noisy examples during the training procedure.
Note that if maxkn=1 exp(-θt>zk) ≤ c3 minkn=1 exp(-θt>zk), we also have maxkn=1 exp(-θt>zk +
kθtkq) ≤ c3 minkn=1 exp(-θt>zk + kθtkq). Therefore, the worst example training loss can be
bounded via the best example training loss and further be bounded by the average training loss
L(θt). In this way, we can guarantee that those noisy examples will not have major influence on
model training even in later training stages.
By using Lemmas 5.1-5.4, we establish the following key lemma for our main theorems.
Lemma 5.5. Under the same condition as in Theorem 4.4, with probability at least 1 - δ, the
adversarially trained linear model parameter θt satisfies
μ>θt > (kμk2	k k ∖	1________c3k"k2 logn
kθtk2≥14	kμkqJ (√C0 + e)√d log t .
where c0 is the absolute constant in Lemma 5.1.
Lemma 5.5 provides the lower bound for the inner product of μ and the direction of θt. This lemma
extends Lemma 4.4 in Li et al. (2020) by considering the training iteration t rather than just the
converged classifier w, and also extends to the adversarial training setting. Notice that this lower
bound actually gets larger with the increase of iteration t.
Finalizing the proof. We now present the proof for Theorems 4.4 and 4.8.
Proof of Theorem 4.4. First, following standard coupling lemma (Lindvall, 2002), there always ex-
ists a joint distribution on original data and noisy data ((xe, ye), (x, y)) such that the marginal distri-
bution for (xe, ye) is D, the marginal distribution for (x, y) is D, P[x = xe] = 1 and P[y 6= ye] ≤ η.
Notice that the standard population risk can be written as
P(x,y)〜D [fθt (X)= y] = P(χ,y)〜D [V ∙ θ>x < 0]
≤ η + P(χ,y)〜D [y ∙ θ>χ < 0,y = e]
=η + P(x,y)〜D [e ∙ θ>χ < 0],	(5.7)
where the inequality holds since P[y 6= ye] ≤ η. Since yeis the clean label for χ, yeχ follows the same
distribution as ξ + μ and E[e ∙ θ>x] = θ>μ. Therefore, (5.7) can be further written as
P(x,y)〜D [fθt (X) = y] ≤ η + P(x,y)〜D [e ∙ θ>x - E[e ∙ θ>x] < -θ> 〃]
=η + P(χ,y)〜D [θ> (eχ - E[eχ]) < -θ>μ]
((θ>μ)2∖
≤ η + exp(-c E),
(5.8)
where the last inequality holds by applying a Hoeffding-type concentration inequality (Theorem C.1)
with t = (θ>μ)2. This bound in (5.8) enables the application of Lemma 5.5 which characterizes
7
Under review as a conference paper at ICLR 2022
how the direction of θt aligns with μ during training. By direct calculation, We have
P(x,y)〜D [fθt (χ) = y] ≤ η + exp
—
c3kμk2 log n
log t
This concludes the proof.
□
Proof of Theorem 4.8. Similar as in the proof of Theorem 4.4, we start with a calculating an upper
bound of the population risk based on the formulation of the label noise. By the definition of the
adversarial risk, we have
P(x,y)〜D [∃x0	∈ Bp(X)S.t.,	fθt	(x0)	=	y]	=	P(x,y)〜D	[∃X0	∈	Bp(X)S.t., y ∙ θ>X0 < 0]
≤ η + P(χ,y)〜D[∃x0 ∈ Bp(X)S.t., y ∙ θ>χ0 < 0,y = e]
η + P(x,y)〜Dl min e ∙
(,y) Lu∈BP(0)
θt>(X + u) <0
η + P(χ,y)〜D [e ∙ θ>X-ekθtkq < 0],	(5.9)
where the inequality holds in the same way as in (5.7). Since yeis the clean label for X, yeX follows
the same distribution as ξ + μ and E[e ∙ Θ>x] = θ>μ. Therefore, (5.9) can be further written as
P(x,y)〜D [∃x0 ∈ Bp(X)S∙t∙, fθt(x0) = y] ≤ η + P(χ,y)〜D[e∙ θ>x - E[e ∙ θ>x] < -θ>μ + d∣θtkq]
1 < -θ>μ + ekθtkq]
2
=η+P(x,y)〜D [θ>(ex - E[ex])
≤ η + exp (-c¾k^
kθtk2
(5.10)
where the second inequality holds by applying the Hoeffding-type concentration inequality (Theo-
rem C.1) with t = (θ>μ - d∣θtkq)2. Based on (5.10) and Lemma 5.5, we can further give bounds
of the adversarial risk. We consider the two settings 1 ≤ p ≤ 2 and 2 < p < ∞ separately.
When 1 ≤ p ≤ 2, we have q ≥ 2 and kθkq ≤ kθk2. In this case, by Lemma 5.5 we obtain
P(x,y)〜D [fθt (x) = y] ≤ η + eχp
c3kμk2 logn
log t
WhenP > 2 and therefore q < 2, we have ∣∣μkq ≤ d1∕qτ∕2kμk2. In this case, by Lemma 5.5 we
obtain
P(x,y)〜D [fθt (x) = y] ≤ η + eχp
c3kμk2 log n
log t
This concludes the proof.
□
6	Experiments
In this section, we experimentally study the behavior of the adversarially trained linear classifier in
the over-parameterized regime on synthetic data. Specifically, we generate 50 training samples and
2000 test samples and set the label noise ratio η = 0.1 for all experiments. Each clean sample (xe, ye)
is drawn from a Gaussian mixture model such that y 〜Unif({±1}) and x = yμ + ξ where ξ ∈ Rd
and ξ1,ξ2,... ,ξd are i.i.d. standard Gaussian variables and μ simply shares the same direction as
an all-one vector but has various different magnitudes. This aligns with our model assumptions in
Section 3. For the adversarial training algorithm, we directly follows Algorithm 1 except using a
more practical Xavier normal initialization (Glorot & Bengio, 2010), i.e., sampling θo i.i.d. from
from N(0,1 /√d). We set the learning rate at = 0.001 and the total number of iterations T = 1000
for all experiments. All results are obtained by averaging over 10 independent runs (both data
sampling and training).
In the first set of experiments, we verify our main conclusions in this paper, that benign overfitting
can occur in adversarial training. Figure 1 (a-d) illustrates the risk and the adversarial risk of ad-
versarially trained linear classifiers versus the dimension d under different scalings of μ for both
8
Under review as a conference paper at ICLR 2022
(a) `2 perturbation
(b) '2 perturbation	(C) '∞ perturbation
*∙a≈WesUWpv
ε=0.001
ε=0.01
e = 0.02
optimal risk
0∙0∙
*H -WeSUWPV
0	500	1000	1500	2000	2500	0	200	400	600	800	1000	0	200	400	600	800	1000
Dimension	iterations	⅛lterations
(d) '∞ perturbation	(e)	'2 perturbation	(f)	'∞ perturbation
Figure 1: (a-d) Risk and adversarial risk of adversarially trained linear Classifiers versus the di-
mension d under different scalings of μ. (a)(b) show the results for '2 perturbation with e = 0.1
and (c)(d) show the results for '∞ perturbation with e = 0.01. (e-f) Adversarial risk of adversarially
trained linear classifiers versus the training iterations t for different e with d = 200 and ∣∣μk2 = d0.3.
The training error reaches 0 for all experiments.
'2-norm and '∞-norm perturbations. We can observe that when ∣∣μ∣2 = d0∙2, the (adversarial) risk
starts to increase as the dimension d increases after an initial dive for both '2-norm and '∞ -norm
perturbations. While for cases where ∣∣μ∣2 = d0.3 and ∣∣μ∣2 = d0.4, we can observe that the (adver-
sarial) risk decreases steadily to the optimal risk η as the dimension d increases. This results backup
our theory in Section 4 that the optimal risk is achievable when ∣∣μ∣2 = Θ(dr) and r ∈ (1/4,1/2].
Note that the training error reaches 0 for all settings in Figure 1.
In Figure 1 (e-f), we present the adversarial risk2 of adversarially trained linear classifiers versus the
training iterations t with different e but fixed dimension d and ∣∣μ∣2 for both '2-norm and '∞-norm
perturbations. We can also observe that in general, a larger e will lead to the worse adversarial risk
of the adversarially trained classifier. This also backs up our theory in Theorem 4.8.
As our ultimate goal is to study the benign overfitting phenomenon in real-world adversarial training
settings, we also conducted experiments on 2-layer neural networks with ReLU activation functions.
In fact, the performances on the 2-layer ReLU network suggest very similar trends as the linear
model. Due to space limit, we display these results in the supplemental materials.
7	Conclusions and Future Work
In this paper, we show that the benign overfitting phenomenon also occurs in adversarial training, a
principled approach to defend against adversarial examples. Specifically, we derive the risk bounds
of the adversarially trained linear classifiers and show that under moderate 'p-norm perturbations,
they can achieve the near-optimal standard and adversarial risks, despite overfitting the noisy training
data. The numerical experimental results also validate our theoretical findings.
Our current analysis is limited to linear classifiers, while in practice, adversarial training is com-
monly used with neural networks. We believe our work is the first step towards analyzing benign
overfitting in adversarially trained neural networks. Yet extending our current analysis to adversari-
ally trained neural networks is highly non-trivial and we leave it as a future work.
References
Zeyuan Allen-Zhu and Yuanzhi Li. Feature purification: How adversarial training performs robust
deep learning. arXiv preprint arXiv:2005.10190, 2020.
2Here we omit the plot for standard risk as the curves are essentially overlapping to each other.
9
Under review as a conference paper at ICLR 2022
Sanjeev Arora, Nadav Cohen, Wei Hu, and Yuping Luo. Implicit regularization in deep matrix
factorization. Advances in Neural Information Processing Systems, 32, 2019.
Peter L Bartlett, Philip M Long, Gabor Lugosi, and Alexander Tsigler. Benign ovefitting in linear
regression. Proceedings of the NationalAcademy of Sciences ,117(48):30063-30070, 2020.
Mikhail Belkin, Siyuan Ma, and Soumik Mandal. To understand deep learning we need to under-
stand kernel learning. In International Conference on Machine Learning, pp. 541-549. PMLR,
2018.
Mikhail Belkin, Daniel Hsu, Siyuan Ma, and Soumik Mandal. Reconciling modern machine-
learning practice and the classical bias-variance trade-off. Proceedings of the National Academy
of Sciences, 116(32):15849-15854, 2019a.
Mikhail Belkin, Daniel Hsu, and Ji Xu. Two models of double descent for weak features. arXiv
preprint arXiv:1903.07571, 2019b.
Yuan Cao, Quanquan Gu, and Mikhail Belkin. Risk bounds for over-parameterized maximum mar-
gin classification on sub-gaussian mixtures. arXiv preprint arXiv:2104.13628, 2021.
Yair Carmon, Aditi Raghunathan, Ludwig Schmidt, John C Duchi, and Percy S Liang. Unlabeled
data improves adversarial robustness. In NeurIPS, pp. 11192-11203, 2019.
Zachary Charles, Shashank Rajput, Stephen Wright, and Dimitris Papailiopoulos. Convergence and
margin of adversarial training on separable data. arXiv preprint arXiv:1905.09209, 2019.
Niladri S Chatterji and Philip M Long. Finite-sample analysis of interpolating linear classifiers in
the overparameterized regime. arXiv preprint arXiv:2004.12019, 2020.
Lin Chen, Yifei Min, Mingrui Zhang, and Amin Karbasi. More data can expand the generalization
gap between adversarially robust and standard models. In International Conference on Machine
Learning, pp. 1670-1680. PMLR, 2020.
Daniel Cullina, Arjun Nitin Bhagoji, and Prateek Mittal. Pac-learning in the presence of evasion
adversaries. Proceedings of the 32nd International Conference on Neural Information Processing
Systems, 2018.
Chen Dan, Yuting Wei, and Pradeep Ravikumar. Sharp statistical guaratees for adversarially ro-
bust gaussian classification. In International Conference on Machine Learning, pp. 2345-2355.
PMLR, 2020.
Ilias Diakonikolas, Daniel M Kane, and Pasin Manurangsi. The complexity of adversarially robust
proper learning of halfspaces with agnostic noise. Advances in Neural Information Processing
Systems, 2020.
Edgar Dobriban, Hamed Hassani, David Hong, and Alexander Robey. Provable tradeoffs in adver-
sarially robust classification. arXiv preprint arXiv:2006.05161, 2020.
Alhussein Fawzi, Hamza Fawzi, and Omar Fawzi. Adversarial vulnerability for any classifier. In
Proceedings of the 32nd International Conference on Neural Information Processing Systems, pp.
1186-1195, 2018.
Ruiqi Gao, Tianle Cai, Haochuan Li, Cho-Jui Hsieh, Liwei Wang, and Jason D Lee. Convergence
of adversarial training in overparametrized neural networks. Advances in Neural Information
Processing Systems, 32:13029-13040, 2019.
Xavier Glorot and Yoshua Bengio. Understanding the difficulty of training deep feedforward neural
networks. In Proceedings of the thirteenth international conference on artificial intelligence and
statistics, pp. 249-256. JMLR Workshop and Conference Proceedings, 2010.
Ian J Goodfellow, Jonathon Shlens, and Christian Szegedy. Explaining and harnessing adversarial
examples. ICLR, 2015.
Suriya Gunasekar, Jason Lee, Daniel Soudry, and Nathan Srebro. Characterizing implicit bias in
terms of optimization geometry. In International Conference on Machine Learning, pp. 1832-
1841. PMLR, 2018a.
10
Under review as a conference paper at ICLR 2022
Suriya Gunasekar, Blake Woodworth, Srinadh Bhojanapalli, Behnam Neyshabur, and Nathan Sre-
bro. Implicit regularization in matrix factorization. In 2018 Information Theory and Applications
Workshop (ITA), pp. 1-10. IEEE, 2018b.
Trevor Hastie, Andrea Montanari, Saharon Rosset, and Ryan J Tibshirani. Surprises in high-
dimensional ridgeless least squares interpolation. arXiv preprint arXiv:1903.08560, 2019.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recog-
nition. In CVPR, pp. 770-778, 2016.
Geoffrey Hinton, Li Deng, Dong Yu, George Dahl, Abdel-rahman Mohamed, Navdeep Jaitly, An-
drew Senior, Vincent Vanhoucke, Patrick Nguyen, Brian Kingsbury, et al. Deep neural networks
for acoustic modeling in speech recognition. IEEE Signal processing magazine, 29, 2012.
Adel Javanmard and Mahdi Soltanolkotabi. Precise statistical analysis of classification accuracies
for adversarial training. arXiv preprint arXiv:2010.11213, 2020.
Ziwei Ji and Matus Telgarsky. Gradient descent aligns the layers of deep linear networks. In
International Conference on Learning Representations, 2019a.
Ziwei Ji and Matus Telgarsky. The implicit bias of gradient descent on nonseparable data. In
Conference on Learning Theory, pp. 1772-1798. PMLR, 2019b.
Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classification with deep convo-
lutional neural networks. In Neurips, pp. 1097-1105, 2012.
Alexey Kurakin, Ian Goodfellow, and Samy Bengio. Adversarial examples in the physical world.
arXiv preprint arXiv:1607.02533, 2016.
Yan Li, Ethan X.Fang, Huan Xu, and Tuo Zhao. Implicit bias of gradient descent based adversarial
training on separable data. In International Conference on Learning Representations, 2020.
Torgny Lindvall. Lectures on the coupling method. Courier Corporation, 2002.
Kaifeng Lyu and Jian Li. Gradient descent maximizes the margin of homogeneous neural networks.
In International Conference on Learning Representations, 2020.
Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and Adrian Vladu.
Towards deep learning models resistant to adversarial attacks. ICML, 2018.
Omar Montasser, Surbhi Goel, Ilias Diakonikolas, and Nathan Srebro. Efficiently learning adver-
sarially robust halfspaces with noise. In International Conference on Machine Learning, pp.
7010-7021. PMLR, 2020.
Mor Shpigel Nacson, Nathan Srebro, and Daniel Soudry. Stochastic gradient descent on separable
data: Exact convergence with a fixed learning rate. In The 22nd International Conference on
Artificial Intelligence and Statistics, pp. 3051-3059. PMLR, 2019.
Behnam Neyshabur. Implicit regularization in deep learning. arXiv preprint arXiv:1709.01953,
2017.
Aditi Raghunathan, Sang Michael Xie, Fanny Yang, John Duchi, and Percy Liang. Understanding
and mitigating the tradeoff between robustness and accuracy. arXiv preprint arXiv:2002.10716,
2020.
Leslie Rice, Eric Wong, and J Zico Kolter. Overfitting in adversarially robust deep learning. ICML,
2020.
Amartya Sanyal, Puneet K. Dokania, Varun Kanade, and Philip Torr. How benign is benign overfit-
ting ? In International Conference on Learning Representations, 2021.
Ludwig Schmidt, Shibani Santurkar, Dimitris Tsipras, Kunal Talwar, and Aleksander Madry. Ad-
versarially robust generalization requires more data. Advances in Neural Information Processing
Systems, 2018.
11
Under review as a conference paper at ICLR 2022
Daniel Soudry, Elad Hoffer, Mor Shpigel Nacson, Suriya Gunasekar, and Nathan Srebro. The im-
plicit bias of gradient descent on separable data. The Journal of Machine Learning Research, 19
(1):2822-2878, 2018.
Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian Goodfellow,
and Rob Fergus. Intriguing properties of neural networks. arXiv preprint arXiv:1312.6199, 2013.
Hossein Taheri, Ramtin Pedarsani, and Christos Thrampoulidis. Asymptotic behavior of adversarial
training in binary classification. arXiv preprint arXiv:2010.13275, 2020.
Alexander Tsigler and Peter L Bartlett. Benign overfitting in ridge regression. arXiv preprint
arXiv:2009.14286, 2020.
Dimitris Tsipras, Shibani Santurkar, Logan Engstrom, Alexander Turner, and Aleksander Madry.
Robustness may be at odds with accuracy. In International Conference on Learning Representa-
tions, 2019.
Roman Vershynin. Introduction to the non-asymptotic analysis of random matrices. arXiv preprint
arXiv:1011.3027, 2010.
Ke Wang and Christos Thrampoulidis. Benign overfitting in binary classification of gaussian mix-
tures. In ICASSP 2021-2021 IEEE International Conference on Acoustics, Speech and Signal
Processing (ICASSP), pp. 4030-4034. IEEE, 2021.
Yisen Wang, Xingjun Ma, James Bailey, Jinfeng Yi, Bowen Zhou, and Quanquan Gu. On the
convergence and robustness of adversarial training. In ICML, pp. 6586-6595, 2019.
Yisen Wang, Difan Zou, Jinfeng Yi, James Bailey, Xingjun Ma, and Quanquan Gu. Improving
adversarial robustness requires revisiting misclassified examples. In International Conference on
Learning Representations, 2020.
Boxi Wu, Jinghui Chen, Deng Cai, Xiaofei He, and Quanquan Gu. Do wider neural networks really
help adversarial robustness? arXiv preprint arXiv:2010.01279, 2020.
Denny Wu and Ji Xu. On the optimal weighted `2 regularization in overparameterized linear regres-
sion. Advances in Neural Information Processing Systems, 33, 2020.
Yao-Yuan Yang, Cyrus Rashtchian, Hongyang Zhang, Russ R Salakhutdinov, and Kamalika Chaud-
huri. A closer look at accuracy vs. robustness. Advances in Neural Information Processing
Systems, 33, 2020.
Dong Yin, Ramchandran Kannan, and Peter Bartlett. Rademacher complexity for adversarially
robust generalization. In International Conference on Machine Learning, pp. 7085-7094. PMLR,
2019.
Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals. Understanding
deep learning requires rethinking generalization. In International Conference on Learning Rep-
resentations, 2017.
Hongyang Zhang, Yaodong Yu, Jiantao Jiao, Eric Xing, Laurent El Ghaoui, and Michael Jordan.
Theoretically principled trade-off between robustness and accuracy. In ICML, pp. 7472-7482,
2019.
Xiao Zhang, Jinghui Chen, Quanquan Gu, and David Evans. Understanding the intrinsic robust-
ness of image distributions using conditional generative models. In International Conference on
Artificial Intelligence and Statistics, pp. 3883-3893. PMLR, 2020a.
Yi Zhang, Orestis Plevrakis, Simon S Du, Xingguo Li, Zhao Song, and Sanjeev Arora. Over-
parameterized adversarial training: An analysis overcoming the curse of dimensionality. Ad-
vances in Neural Information Processing Systems, 2020b.
Difan Zou, Spencer Frei, and Quanquan Gu. Provable robustness of adversarial training for learning
halfspaces with noise. International Conference on Machine Learning, 2021.
12
Under review as a conference paper at ICLR 2022
A Comparison with Dan et al. (2020), Taheri et al. (2020) and
Javanmard Soltanolkotabi (2020)
Dan et al. (2020) proposed an adversarial signal to noise ratio and studied the excess risk lower/upper
bounds for learning Gaussian mixture models. Compared to the setting studied in Dan et al. (2020),
our setting covers additional label flipping noises. More importantly, we study an estimator found
by gradient descent that overfits the training data, while Dan et al. (2020) studied a specific plug-in
estimator which does not overfit the training data. Due to these differences, there is a discrepancy in
the risk bounds derived in both papers.
Taheri et al. (2020); Javanmard & Soltanolkotabi (2020) studied adversarial learning of linear mod-
els in the proportional limit setting, i.e., d/n = O(1). In this setting, the data Gram matrix and
the sample covariance matrix can be studied based on random matrix theory/Gaussian comparison
inequalities/convex Gaussian min-max theorem. In contrast, in our setting where d > O(n2), the
sample covariance matrix is singular but the n × n Gram matrix concentrates around its expecta-
tion. Therefore, our setting is different from the proportional limit setting in Taheri et al. (2020);
Javanmard & Soltanolkotabi (2020), and these results are not directly comparable.
B Proof of Key Technical Lemmas
B.1 Proof of Lemma 5.2
Proof. We first prove that L(θ1) ≤ 2n. To show this, we observe that θ1 = α0 Pkn=1 zk. Therefore
n
L(θ1) = X exp(-θ1>zk + kθ1 kq)
k=1
nn	n
= Xexp - α0 Xzi>zk + α0 Xzi
k=1	i=1	i=1	q
≤ X exp (α°n , o(kμ∣∣2 + P log(n∕δ)) + e√c0d
k=1
n
≤ X exp(1∕16) ≤ 2n,
k=1
where the first equality holds due to Lemma 5.1 and the fact that for any u ∈ Rd , kukq ≤ kuk1 ≤
√dku∣∣2, while the second inequality is by the choice of sufficiently small a° and the assumptions
that d ≥ Cnkμk2 and e ≤ R for some absolute constants C and R.
The rest part of Lemma 5.2 summarizes parts of the results in Li et al. (2020). However, the results
in Li et al. (2020) are derived under the setting that kxi k2 ≤ 1, Therefore to prove lemma 5.2, we
re-scale our data and model parameters and convert our setting to the setting in Li et al. (2020).
By lemma 5.1, with probability at least 1 - δ, kxi k22 ≤ c0d for all i ∈ [n]. We therefore denote
B := √C0d, and then Xi := Xi/B has '2-norm less than or equal to one. Further denote by βt the
linear model parameters in Li et al. (2020)’s algorithm, ezi = yixei , ηt as their step sizes, ee as their
perturbation strength, and
〜
γe :
max min
kθk2 =1 i∈[n]
yiθ>Xei
as the `p margin. Then the adversarial training update rule in Li et al. (2020) is
n
βt+1 = Bt - 2 ^X Vβ eχp(-β>Zk + e∣βtkq).
Note that our update rule is
n
θt+1 = θt - αt X Vθ exp(-θt>zk + ekθt kq).
k=1
13
Under review as a conference paper at ICLR 2022
Now in order to apply the results inLi et al. (2020), we convert our parameters to match their scaling.
Since
Θt+1 = θt- at X Vθ exp(-Bθ>Zk/B + e∣∣Bθt kq/B)
i
=θt- nBa X V(bθ) exp(-Bθ>Zk/B + e∣∣Bθtkq∕B).
i
Therefore
Bθt+1 = Bθt-----n~X X v(BΘ) eχp(-Bθ>Zk/B + EkBθtkq/B).
It is easy to observe that we can now apply Theorem 3.3 and Theorem 3.4 in Li et al. (2020) by
setting βt = Bθt, ηt = nB2αt, e= /B. Moreover, by xei = xi/B, e= /B and the definition of
γ, We have e = 7/B. Based on these relations, it is easy to see that under the conditions of Lemma
5.2, xei, ηt, e, γe satisfy the assumptions of Theorems 3.3 and 3.4 in Li et al. (2020). Now (5.5) is an
intermediate result of the proof of Theorem 3.3 in Li et al. (2020), and (5.6) folloWs by Theorem 3.4
in Li et al. (2020).	□
B.2 Proof of Lemma 5.3
Proof. We have
t
kθt+lk2 = X am ∙VL(θm)
m=0	2
t
≤ X amkVL(θm)k2
m=0
t	n
≤ Eam E(Zk —〜∂∣∣θm∣Iq) ∙ exp ( - Z>θm + E∣∣θmkq)
m=0	k=1	2
Where the first three inequality hold by triangle inequality. By Lemma C.2, We have
tn
kθt+1∣2 ≤ X am X(kZkk2 + e√d) ∙ exp ( — Z>θm + e∣∣θmkq)
m=0	k=1
tn
≤ (√C0 + E)VzdE am £ ∙ exp ( — Z>θm + d∣θmkq)
t
= (√C0 + e) √d]TamL(θm),
m=0
where the second inequality is due to Lemma 5.1.	□
B.3 Proof of Lemma 5.4
Proof. We will prove this lemma by induction.
Let,s denote Ek = exp(-θ>Zk). Without loss of generality, let Et denotes the maximum of
{Ekt }kn=1 and E2t denotes the minimum of {Ekt }kn=1. We also define At := E1t /E2t and the goal is
to show that At ≤ 5c02 .
For the base case (t = 0), we have Ek0 = exp(0) = 1. Therefore we have A0 = 1 ≤ 5c20.
14
Under review as a conference paper at ICLR 2022
For t > 0, notice that
At+1
exP(一θ3zι) _ exp(-θjzι) exp(αRL(θt)>zι)
exp(-θt+1Z2)	exp(-θjz2) exp(αtVL(θt)τZ2)
At ∙
At ∙
exp(-at Pn=I(Zk - E训 θt∣∣q )TZ1 . exp(-θTZk + d∣θt∣Iq ))
exp(-at Pn=I(Zk-回 θt∣∣q )τZ2 • exp(-θTZk + d∣θt∣Iq ))
exp(一αt(zi - E训 θt∣∣q )TZ1 • exp(-θTZk + d∣θt∣Iq ))
exp(-αt(Z2 - E训 θt∣∣q )tZ2 • exp(-θTZk + E∣∣θt∣∣q ))
X----------------------------------------------'
"^^^^^^^^^^^^^^^^^^^^^^^^^^^^^{^^^^^^^^^^^^^^^^^^^^^^^^^^^^^""^
I1
exp(-at Pn=I(Zk-E训 θtkq )τZ1 • exp(-θ)zk + E∣θt Ilq ))
exp(-αt pn=2(zk - E训θt∣∣q)τz2 • exp(-θ>Zk + E∣∣θt ∣∣q)) .
(B.1)
I
,
{z
I^2
For term I1, note that by Lemma 5.1 we have
rco ≤kzk k2 ≤ E
Also since by Lemma C.2, we have Ild∣∣θt∣Iq Ilp
IzT训θt∣∣q∣≤∣∣Zk∣∣q ∙∣∣∂∣∣θt∣∣q∣∣p 二
Therefore, we have
1,
∣∣Zk∣∣q≤∣∣Zk∣∣ι ≤ √d∣∣Zk∣∣2 ≤√C0d.
(B.2)
I1 ≤ exp ( — αt (-c√C0^d) exp(-θ>Zi + e∣θt∣q) + at (c0d + c√C0^d) exp(-θ>Z2 + d∣θt∣∣q)
exp ( — atE2 ( (------E√C0d) At —k。d + E√C0d) ) exp (E∣∣θtIq)).
For term I2, by (5.2) and (B.2) we have
I ≤ exp
k=1
≤ exp ( 2&tL(θt) (co(∣∣μ∣∣2 + VZdlog(n∕δ)) + E√C0d
n
E∣∣θt∣∣q )+ X exp(-θ>Zk + E∣∣θt∣∣q )
k=2
(B.4)
Substitute (B.3) and (B.4) into (B.1), we have
At+1 ≤ At • exp I — atEt ( (---------------E√C0^d) At —k0d + E√C0^d) ) exp (E∣∣θt∣∣q)
• exp f 2αtL(θt) (。0(||4||2 + VZdlog(n∕δ)) + e√c0d) 1.
(B.3)
(B.5)
Let US consider two cases here. If (d∕c0 — e√c0d)Αt — (c°d + e√C0d) > c0d, i.e., At > (2c0 +
e√C0)∕(l∕c0 - e√C0), we further have
At+1 ≤ At • exp ( - at Et c°d exp (E∣∣θt∣∣q)) ∙ exp 2atL(θt) (c° (∣∣μ∣∣2 + VZd log(n∕δ)) + E√C0d
≤ At • exp ( - atEtc°dexp (E∣∣θt∣∣q))
• exp(2atnE2 卜0(口H2 + Pdlog(n∕δ)) + E√C0d) exp (E∣∣θt∣∣q))
=At ∙ exp ( - atEtc°(d - 2n∣∣μ∣∣2 - 2n√dlog(n∕δ) - 2nE√C0) exp (E∣∣θt∣∣q)
≤ At,
15
Under review as a conference paper at ICLR 2022
where the second inequality is due to the fact that L(θt) = PL Ek exp (Ek&kq) and 段
maxk Ett while the last inequality holds since d ≥ C ∙ max{n∣∣μk∣, n2 log(n∕δ)}.
On the other hand, if At ≤ (2co + e√co)∕(1∕co - e√co), we have
At+1 ≤ At ∙ exp QtEt(cod + e√c0d) exp (d∣θtkq))
- exp ^2αtL(θt) ^o(kμk∣ + VZdlog(n∕δ)) + c√⅛d))
≤ At ∙ exp (αtL(θt) (cod + c√⅛d)) ∙ exp(2四刀(仇)卜。(∣∣μ∣∣∣ + Pdlog(n∕δ)) + c√⅛d
≤ At ∙ exp ^2αtn(co(2kμk∣ + 2√dlog(n∕δ) + d) + 3c√od))
≤ (2co + M)∕(1∕CO-M) ∙ exp(1∕8)
≤ 5co,
where the first inequality is due to the fact that At > 0, the third inequality holds by Lemma 5.2,
the fourth inequality is because at ≤ 1∕(COCnd) and d ≥ C ∙ max{nkμk∣,n2 log(n∕δ)} and the
last inequality is because e < C0 and C0 can be chosen such that C0 ≤ 1∕(2co^5) and we have
1∕co - S > 1∕(2co).
This concludes the proof.	□
B.4 Proof of Lemma 5.5
Proof. Note that
n
μ>θt+ι = μ> (θt+ at X (Zk -Edkθtkq) exp(-θ>zk + dlθkI))
k=1
n
=μ>θt - αtE ∙ μτ∂∣∣θtkq ∙ L(θt) + at X (μ>Zk) exp(-θ>Zk + E∣∣θ∣∣q))
k=1
≥ μτθt - αtE∣∣μ∣∣q ∙ L(θt) + at £ (μ>Zk) exp(-θ>Zk + e∣θ∣q))
+ at X (μτZk) exp(-θ>Zk + e∣θ∣q)),	(B.6)
k∈N
where the inequality holds in the same way as in (B.2). By Lemma 5.1 ((5.3) and (5.4)), we further
bound (B.6) by
μ>θt+ι ≥ μ>θt - αtEll从∣∣q ∙L(θt)+ -2t X llμk∣ exp(-θ>Zk+ ElleIlq))
k∈C
----2 X kμk∣ exp(-θ>Zk + EIlθ∣∣q))
k∈N
=μ θt - αtEIl从llq ∙L(θt)+ -kkμk∣〃θt) - 2αtll从ll∣ X exp(-θτZk+ EIleIlq)) ∙
k∈N
(B.7)
Note that we have
X exp(-e>Zk + elimiq) = X exp(-θτZk) ∙ ^pgimiq)
k∈N	k∈N
≤ C3(η + Cι)n∙ (maXEk) ∙ exp(EIθIq)
≤ c3(η + cι)L(θt)
≤ CL(θt),
8
16
Under review as a conference paper at ICLR 2022
where the first inequality is due to Lemma 5.2 and the last inequality is because η < 1/C and c1 can
be chosen arbitrarily small given sufficient large C. Therefore, (B.7) can be further written as
μ>θt+ι ≥ μ>θt- αtekμkq ∙ L(θt) + ^2tkμk2L(Ot) —4kμk2L(Ot)
=μ>θt + α(呼-e∣∣μkq) ∙ L(θt)
=(呼-ek"kq) ∙ XX amL(θm),	(B.8)
m=0
where the last equality is due the fact that O0 = 0. Now we multiply kwk2/kOt+1k2 on both sides
of (B.8) and take t → ∞
t
lim
t→∞
IlWIl2(μ>仇+1)
kθt+1k2
≥ Iim (呼
t→∞	4
-ekμkq
kwk2
kθt+1k2
αmL(Om).
m=0
Since IwI2 = 1, and by Lemma 5.2, it is easy to observe that w = limt→∞ Ot/IOtI2, we have
μ>w ≥
≥
呼-Mq
kμk2
4
lim Pm=Oamj(θm)
t→∞	IOt+1I2
-ekμkq
where the last inequality is due to Lemma 5.3. Note that Lemma 5.2 also suggests that IOt/IOt I2 -
wI2 ≤ c3 log n/ log t, we have
μ>w = μ> ( w —
洗+洗）
≤kμk2 •卜 -彘 ∣2+μ⅛
≤
c3kμk2 log n + μ>θt
log t	ι∣θtk2.
Therefore,
μ>θt
≥ μ>w —
C3∣μ∣2 log n
log t
kμk2	∣∣,,k ∖	1_____c3k"k2 logn
4	k“叼(√0 + e)√d	logt
≥
□
C Auxiliary Lemmas
Theorem C.1 (Proposition 5.10 in Vershynin (2010)). LetX1, X2, . . . , Xn be independent centered
sub-Gaussian random variables, and let K = maxi IXiIψ2. Then for every a = (a1, a2, . . . , an) ∈
Rn and for every t > 0, we have
n	Ct2
P(二aiχi >t)≤eχp(-K2∣μ1),
where C > 0 is a constant.
Lemma C.2. For any O ∈ Rd ,
Ildkθ∣∣q∣∣2 ≤√d, ∣∣∂∣∣θkq∣∣p = L
Proof. Note that we have
θq-1
(dkθkq)i = ⅛T ∙ sign(θ),
IOIq
17
Under review as a conference paper at ICLR 2022
*s 一α--∙JesJ,u>p4
0	200	400	600	800	1000	0
■Alterations
(a) `2 perturbation
Figure 2: Risk and adversarial risk of adversarially trained linear classifiers versus the training
iterations t for different perturbation level . The label noise level is set as η = 0.1, the training set
size n = 50, dimension d = 200 and ∣∣μ∣∣2 = d0.4. The train error reaches 0 for all experiments.
6-5-4-3-2.
Ooooo
~s-eμesj>p4
200	400	600	800	1000
■Alterations
(b) '∞ perturbation

*s 一α-eues∙l,u>p4
---ε = 0.01
ε = O.l
----£=0.2
----optimal risk
----ε = 0.001
200	400	600	800 IOOO
■Alterations
(b) '∞ perturbation
O 200	400	600	800 IOOO O
■Alterations
(a) `2 perturbation
Figure 3: Risk and adversarial risk of adversarially trained linear classifiers versus the training
iterations t for different perturbation level . The label noise level is set as η = 0.1, the training set
size n = 50, dimension d = 1000 and ∣∣μ∣∣2 = d0.3. The train error reaches 0 for all experiments.
and since for any vector U ∈ Rd, kUkq ≥ ku∣∣∞, ku∣∣2 ≤ √d∣∣uk∞, we have
∂kθkq2
WqT)L
kθr1
≤ √⅜θk玄1
-kθkq-1
≤ √d,
where ◦ denotes element-wise power. This concludes the first part of the lemma. For the second
part, by p-norm definition we have
∂kθkqp
W(q I)Ip =	1 (XX(θqT)p[1∕p =	1	((XXθq)1∕qγτ = 1
kθkq-1	kθkq-"i=1 i J	kθkq-1	i=l J )
□
D	Additional Experiments
In this section, we present the additional experiments covering more settings as well as more com-
plex models such as 2-layer neural network.
D. 1 Adversarially Trained Linear Classifier Under Various Settings
In Figures 2,3,4, we plot the adversarial risk of adversarially trained linear classifiers versus the
training iterations t for different perturbation level for various combinations of dimension d and
∣∣μk2. Specifically, in Figure 4, We can observe that with moderate perturbations and sufficient
over-parameterization, adversarially trained linear classifiers can achieve near-optimal adversarial
risks.
18
Under review as a conference paper at ICLR 2022
Figure 4: Risk and adversarial risk of adversarially trained linear classifiers versus the training
iterations t for different perturbation level . The label noise level is set as η = 0.1, the training set
size n = 50, dimension d = 1000 and ∣∣μ∣∣2 = d04. The train error reaches 0 for all experiments.
0.20
0.18
0.16
0.12
0.10
s 0.14
0
500	1000	1500	2000	2500
Dimension
(a) `2 perturbation
M2=d°∙2
∣M∣2 = d°∙3
IM∣2 = d°4
optimal risk
0	500
(c) '∞ perturbation
(b) `2 perturbation
1000	1500	2000	2500
Dimension
(d) '∞ perturbation
Figure 5: Risk and adversarial risk of adversarially trained 2-layer ReLU network versus the di-
mension d under different scalings of μ. (a)(b) show the results for '2 perturbation with e = 0.1
and (c)(d) show the results for '∞ perturbation with e = 0.01. The training error reaches 0 for all
experiments.
D.2 Adversarially Trained 2-layer Neural Networks
We have also conducted extra experiments on 2-layer neural networks with ReLU activation func-
tions (one extra fix-dimension hidden layer). The data generation process are the same as our linear
experiments. Note that in this setting, we no longer have the closed-form solutions to the inner max-
imization problem. Therefore, we following Madry et al. (2018) and use 10-step Projected Gradient
Descent to get the inner maximizer.
As can be seen from Figure 5, the empirical results on 2-layer ReLU network suggest very similar
trends as the linear classifier for both adversarial risks and standard risks. This further backs up our
theoretical conclusions.
19