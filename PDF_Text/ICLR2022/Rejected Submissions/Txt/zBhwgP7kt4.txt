Under review as a conference paper at ICLR 2022
Dynamic Least-S quares Regression
Anonymous authors
Paper under double-blind review
Abstract
In large-scale supervised learning, after a model is trained with an initial dataset,
a common challenge is how to exploit new incremental data without re-training
the model from scratch. Motivated by this problem, we revisit the canonical
problem of dynamic least-squares regression (LSR), where the goal is to learn
a linear model over incremental training data. In this setup, data and labels
(A(t), b(t)) ∈ Rt×d X Rt evolve in an online fashion (t》d), and the goal is to ef-
ficiently maintain an (approximate) solution of minx(t) kA(t)x(t) - b(t) k2 for all
t ∈ [T]. Our main result is a dynamic data structure which maintains an arbitrarily
small constant approximate solution to dynamic LSR with amortized update time
O(d1+o(1)), almost matching the running time of the static (sketching-based) so-
lution. By contrast, for exact (or 1/ poly(n)-accuracy) solutions, we show a sepa-
ration between the models, namely, that dynamic LSR requires Ω(d2-o⑴)amor-
tized update time under the OMv Conjecture (Henzinger et al., STOC’15). Our
data structure is fast, conceptually simple, easy to implement, and our experiments
demonstrate their practicality on both synthetic and real-world datasets.
1	Introduction
The problem of least-squares regression (LSR) dates back to Gauss in 1821 (Stigler, 1981), and
is the backbone of statistical inference (Hastie et al., 2001), signal processing (Rabiner & Gold,
1975), convex optimization (Bubeck, 2015), control theory (Chui, 1990) and network routing (Lee &
Sidford, 2014; Madry, 2013). Given an overdetermined (n》d) linear system A ∈ Rn×d, b ∈ Rn,
the goal is to find the solution vector x that minimizes the mean squared error (MSE)
min kAx - bk2.	(1)
x∈Rn
Among many other loss functions (e.g., 'p) that have been studied for linear regression, '2-regression
has been the most popular choice as it is at the same time robust to outliers, and admits a high-
accuracy efficient solution.
The computational task of least-squares regression arises naturally in high-dimensional statistics
and has been the central of focus. The exact closed-form solution is given by the well-known
Normal equation x? = (A>A)-1A>b, which requires O(nd2) time to compute using naive matrix-
multiplication, or O(ndω-1) ≈ O(nd1.37) time using fast matrix-multiplication (FMM) (Strassen,
1969) for the current FMM exponent of ω ≈ 2.37 (Le Gall, 2014; Alman & Williams, 2021).
Despite the elegance and simplicity of this closed-form solution, in practice the latter runtime is
often too slow, especially in modern data analysis applications where both the dimension of the
feature space (d) and the size of datasets (n) are overwhelmingly large. A more modest objective in
attempt to circumvent this computational overhead, is to seek an e-accurate solution that satisfies
∣∣Ax — b∣∣2 ≤ (1 + e) min ∣∣Ax — b∣∣2∙
x∈Rd
This was the primary motivation behined the development of the sketch-and-solve paradigm, where
the idea is to first compress the matrix into one with fewer (〜 d/e2) rows and then to compute the
standard LSR solution but over the smaller matrix. A long line of developments on this framework
culminates in algorithms that run in close to input-sparsity time (Sarlos, 2006; Clarkson & Woodruff,
2017; Nelson & Nguyen, 2013; Chepurko et al., 2021). In particular, a direct application of sketch-
and-solve yields an algorithm runs in Oe(nnz(A)e-1 + dω) 1, which is near optimal in the “low
1In this paper we use O(∙) to hide polylogarithmic terms, and We use Oe(∙) to hide Polyaogd, e-1) terms.
1
Under review as a conference paper at ICLR 2022
precision” regime (e = 1/ poly(log d)). Interestingly, when combined with more sophisticated
ideas of preconditioning and (conjugate) gradient descent, the runtime of this algorithm in terms
of the error e can be further improved to O(nnz(A) log(1/e) + dω), which yields a high precision
algorithm, i.e., it can efficiently solve the problem to within polynomial accuracy e =1/ poly(d).
Dynamic least-squares In many real-world scenarios of the aforementioned applications, data is
evolving in an online fashion either by nature or by design, and such applications require maintaining
the solution (1) adaptively, where rows of the data matrix and their corresponding labels (A(t) , b(t) )
arrive one-by-one incrementally. This is known as the dynamic least-squares regression problem.
The origins of dynamic least-squares regression was in control theory of the 1950’s (Plackett, 1950),
in the context of dynamical linear systems. In this setup, the data matrix [A(t) , b(t)] corresponds to
the set of measurement and it evolves in an online (incremental) fashion, and the goal is to efficiently
maintain the (exact) solution to a noisy linear system b := A(t)x(t) + ξ(t) without recomputing
the LSR solution from scratch. The recursive least-squares (RLS) framework and the celebrated
Kalman filter (Kalman, 1960) provide a rather simple update rule for maintaining an exact solu-
tion for this problem, by maintaining the sample covariance matrix and using Woodburry’s identity
(which assert that an incremental update to [A(t), b(t)] translates into a rank-1 update to the sample
covariance matrix), and hence each update can be computed in O(d2) time (Kalman, 1960).
Beyond this classic motivation for dynamic LSR, a more timely motivation comes from modern
deep learning applications: Most neural networks need to be frequently re-trained upon arrival on
new training data, in order to improve prediction accuracy, and it is desirable to avoid recomputing
weights from scratch. This problem of efficient incremental training of DNNs has been studied
before in elastic machine learning (Liberty et al., 2020) and in the context of continual learning
(Parisi et al., 2019). Our work sheds light on this question by analyzing the minimal computational
resources required for `2 loss-minimization.
Despite the rich and versatile literature on static LSR, the understanding of the dynamic counterpart
was so far quite limited: The previous best known result requires O(d2) amortized update time (by
a direct application of the Woodbury identity). The basic questions we address in this papers are:
Is it possible to achieve faster update time for maintaining an exact solution? How about a small-
approximate solution - Is it then possible to achieve amortized O(d) or even input-sparsity time?
In this paper, we settle both of these questions and present an essentially complete characterization
of the dynamic complexity of LSR.
1.1	Overview of our results
Our first result is a negative answer to the first question above of maintaining exact (or polynomial-
accuracy) LSR solutions in the dynamic setting - We prove that Kalman,s approach is essentially
optimal, assuming the popular Online Matrix-Vector (OMv) Conjecture (Henzinger et al., 2015) 2:
Theorem 1.1	(Hardness of exact dynamic LSR, informal). Assuming the OMv Conjecture, any
dynamic algorithm that maintains an e =1/ poly(d)-approximate solution for the dynamic LSR
problem over T = poly(d) iterations, must have Ω(d2-o(1)) amortized update time per iteration.
Theorem 1.1 separates the static and the dynamic complexities of the exact LSR problem: As men-
tioned above, the static problem can be solved by batching rows together using FMM in time
O(Tdω-1), whereas the dynamic problem requires Ω(Td2) by Theorem 1.1. Indeed, the impli-
cation Theorem 1.1 is stronger, it also separates the static and dynamic complexity of approximate
LSR problem under the high precision regime, it asserts that a polylogarithmic dependence on the
precision (i.e. d poly(log(1/e))) on update time is impossible (assuming OMv), in sharp contrast to
the static case.
We next focus on an approximate version of this classic online problem, dynamic e-LSR, where
the goal is to efficiently maintain, during all iterations t ∈ [T], an e-approximate solution under
incremental row-updates to A(t) and labels b(t), where efficiency is measured by the amortized
2This conjecture postulates that multiplying a fixed d × d matrix A with an online matrix B, column-by-
column (ABi), requires d3-o(1) time, in sharp contrast to the batch setting where this can be done using FMM
in dω《 d3 time. See Section 4.
2
Under review as a conference paper at ICLR 2022
update time for inserting a new row. A natural complexity benchmark for this dynamic problem is
the aforementioned best static sketch-and-solve solution, which for n = T is O(nnz(A(T))e-1 +
dω) = O(nnz(A)e 1) for T》d. Our mam result is a ProVably efficient and practical dynamic
data structure, whose total running time essentially matches the complexity of the offline problem:
Theorem 1.2 (Main result, informal Version of Theorem 3.1). For any accuracy parameter e>0,
there is a randomized dynamic data structure which, with probability at least 0.9, maintains an e-
approximate solution to the dynamic LSR problem simultaneously for all iterations t ∈ [T], with
total update time
O(e-2 nnz(A(T)) log(T) + e-6d3 log5(T)).
Theorem 1.2	almost matches the fastest static sketching-based solution, up to polylogarithmic terms
and the additive FMM term. When T》d, this theorem shows that amortized update time of our
algorithm is O(d1+o(1)).
1.2 Related work
Sketching and sampling The least squares regression as a fundamental problem has been exten-
sively studied in the literature. A long line of work (Ailon & Chazelle, 2006; Clarkson & Woodruff,
2017; Nelson & Nguyen, 2013; Avron et al., 2017; Cohen et al., 2015; Woodruff, 2014; 2021) have
focused on using dimension reduction technique (sketching or sampling) to speedup the computation
task, culminates into algorithms that run in O (nnz(A) log(1/e) + dω) time (Clarkson & Woodruff,
2017). See Appendix A for detailed discussions.
Regression in online, streaming, and sliding window models Least-squares regressions have
also been studied in various computational models, though the focus of these models are generally
not the (amortized) running time. Our algorithm uses techniques developed by Cohen et al. (2020),
where they study the regression problem in the online model, with the goal of maintaining a spec-
tral approximation of data matrix in the online stream. Their algorithm only needs to store Oe(d)
rows but the amortized running time is still Ω(d2) (see Section 3.1 for detailed discussions). In
the streaming model, the main focus is the space complexity, and a direct application of random
Gaussian sketch or count sketch reduces the space complexity to Oe(d2e-1) and it is shown to be
tight (Clarkson & Woodruff, 2009). Recent work of (Braverman et al., 2020) studies regressions
and other numerical linear algebra tasks in the sliding window model, where data come in an online
stream and only the most recent updates form the underlying data set. The major focus ofa sliding
window model is still the space complexity, and there is no amortized running time guarantee.
Disparity from online learning Our work crucially differs from online learning literature (Hazan,
2019), in that the main bottleneck in online regret-minimization and bandit problems is information-
theoretic, whereas the challenge in our loss-minimization problem is purely computational. See
Appendix A for detailed discussions.
2 Problem formulation
In a dynamic least-squares regression problem, initially, we are given a matrix A(0) ∈ Rn0 ×d
together with a vector b(0) ∈ Rn0. At the t-th step, a new data of form ((a(t))>, β(t)) ∈ Rd × R
arrives, and the goal is to maintain an e-approximate solution. A formal description is provided
below, where we assume n0 = d +1for simplicity (see Remark 2.3).
Definition 2.1 (Dynamic least-squares regression). Let d ∈ N+ and e ∈ [0, 1) be two fixed parame-
ters. We say an algorithm solves e-approximate dynamic least squares regression if
•	The data structure is given a matrix A(0) ∈ R(d+1)×d and a vector b(0) ∈ Rd+1 in the
preprocessing phase.
•	For each iteration t ∈ [T], the algorithm receives updates a(t) ∈ Rd and β(t) ∈ R. Define
A(t) := [(A(t-1))>, a(t)]> ∈ R(d+t+1)×d to be A(t-1) appended with a new row (a(t))>,
and b(t) := [(b(t-1))>, β(t)]> ∈ Rd+t+1 to be b(t-1) appended with a new entry β(t).
After this update, the algorithm outputs an e-approximate solution x(t) ∈ Rd:
kA(t)x(t) -b(t)k2 ≤ (1+e) min kA(t)x-b(t)k2.
x∈Rd
3
Under review as a conference paper at ICLR 2022
We write [0 : T]={0, 1,...,T}, and for any t ∈ [0 : T], we denote M(t) := [A(t) , b(t)] ∈
R(d+t+1)×(d+1). We make the following assumptions.
Assumption 2.2. We assume 1. Each data have bounded `2 norm, i.e., ∀i ∈ [T +d+ 1], the i-th row
of M(T) satisfies ∣∣M(T) ∣∖ ≤ D. 2. The initial matrix M(0) has full rank, and its smallest singular
value is bounded by σd+1 (M(0)) ≥ σfor some polynomially small σ ∈ (0, 1).
Remark 2.3. We remark that these assumptions are essentially w.l.o.g. for the following reasons: 1.
Real world data inherently have bounded `2 norm, and in applications like machine learning, data
(0)
are often normaze. . e can assume te nta matrx as + rows ecause rute-forcey
adding these d + 1 initial rows would only take O(d3 *) time, and this is within our desired total
running time of Oe(nnz(A(τ)) + d3). 3. To satisfy the assumption that σd+ι(M(0)) ≥ σ for some
polynomially small σ, we could let the initial matrix M(0) = σ ∙ Id+ι. This is equivalent to adding
a small regularization term of σ ∙ ∣∣xk2 and this incurs only a polynomially small additive error.
3	Dynamic e-LSR data structure
In this section, we provide an approximation algorithm for the dynamic least squares regression.
Notably, our algorithm maintains an e-approXimate solution in near input sparsity time.
Theorem 3.1 (Data structure for dynamic least squares regression). Let e > 0, d,T ∈ N. There ex-
ists a randomized algorithmfor dynamic least-squares regression (Algorithm 1-4). With probability
at least 0.9, the algorithm maintains an e-approximation solution for all iterations t ∈ [T] and the
total update time over T iterations is at most O e-2 nnz(A(T))log(T) + e-6d3 log5(TD∕σ)). Our
data structure uses at most O (e-2d2 ∙ log2 (TD∕σ)) space.
Notations We use a superscript (t) to denote the matrix/vector/scaler maintained by the data
structure at the end of the t-th iterations. In particular, the superscript (0) represents the variables
after the preprocessing step. For any matrix A ∈ Rn×d, i ∈ [n] we define its leverage score
T(A) ∈ Rn as Ti(A) := a>(A>A)*ai. We define the generalized leverage score (same as Cohen
et al. (2015)) of A ∈ Rn×d with respect to another matrix B∈ Rn0×d as: TB(A)= a>(B>B),ai.
For more properties of the leverage scores see Section B.1.
3.1	Technique overview
Our approach is formally described in Algorithmic We first overview the ideas behind it.
From a high level view, our approach follows the online row sampling framework (cohen et al.,
2015; 2020; Braverman et al., 2020): When a new row arrives, we sample and keep the new row
with probability proportional to the (approximated version of) online leverage score
TdM+(tt+-11) (M(t)) = (m(t))>((M(t-1))>M(t-1))-1m(t).
The sampled matrix is a spectral approximation to the true data matrix. We maintain an approximate
least-squares regression solution using this sampled matrix.
Naively computing the online leverage score takes O(d2) time. In order to accelerate this computa-
tion, we use two approximations:
1.	Similar to cohen et al. (2020), we compute the online leverage scores with respect to the
sampled matrix instead of the true data matrix. However, this idea alone is still not enough
to achieve sub-quadratic time.
2.	We use a JL-embedding3 trick (Spielman & Srivastava, 2011) to compress the size of the
d X d matrix to ≈ e-2 X d. In this way, in each iteration it only takes Oe(d) time to compute
the approximate online leverage score.
We further use an inductive analysis to bound the overall error (Lemma 3.4).
Finally, we adopt a similar strategy as cohen et al. (2020) to prove that sampling according to the
approximate online leverage score still keeps at most Oe (d) sampled rows (Lemma 3.7). Whenever
a row is sampled, it takes O(d2) time to update the maintained matrices using Woodburry identity.
Hence, the amortized update time of the sampled rows is Oe(d3∕T) = o(d) when T》d.
3Johnson-Lindenstrauss (JL) Lemma shows a way to embed high-dimensional vectors to low-dimensional
space while preserving the distances between the vectors. A rigorous statement is shown in Lemma B.4.
4
Under review as a conference paper at ICLR 2022
Remark 3.2 (Difference from sketching-based solutions). Our approach crucially differs from the
sketching-based solutions, which do not provide any speedup over the direct application of Wood-
burry identity (O(d2) time per iteration). A sketching-based solution maintains a sketched matrix
SM ∈ ROetd)×d, where S is a sketching matrix (e.g. SRHT (Ailon & Chazelle, 2006) or Count
Sketch (Clarkson & Woodruff, 2017)) that mixes the rows of M. When a new row of M arrives, at
least one row of the sketched matrix SM needs to be updated, in contrast to our sampling-based
approach where the sampled matrix is not updated in most of the iterations.
Implementation We explain the detailed implementation of our algorithm. At the beginning of the
t-th iteration, a sampling matrix D(t-1) is derived based on the online leverage score, and the sub-
sampled matrix N(t-1) = D(t-1)M(t-1) maintains a spectral approximation on the column space
of M(t-1) = [A(t-1), b(t-1)]. Let s(t-1) denote the number of sampled rows. To obtain spectral
approximation, we maintain the approximate covariance matrices H(t-1) = ((N(t-1))>N(t-1))-1
and B(t-1) = N(t-1)H(t-1). The online leverage score τ(t) of a new row m(t) =((a(t))>,β(t))
can be approximated as kB(t-1)m(t-1) k2. To efficiently compute the leverage score of a new row,
we left-multiply by a JL matrix J(t-1) and maintain a proxy Be (t-1) = J(t-1)B(t-1), with the
guarantee that kBe (t-1) mk2 ≈ kB(t-1)mk2 for any m ∈ Rd+1 with high probability.
When a new row m(t) is inserted at the t-th iteration, we sample it via the approximate online
leverage score (Line 3 of Sample). We only perform update if the new row is sampled. In
that case, we renew the JL matrix and perform a series of careful updates on all the variables
that we maintain (See UPDATEMEMBERS). To obtain the final solution x(t) ∈ Rd, we solve
mi□x∈Rd ∣∣D(t)A(t)x 一 D(t)b(t)k2, which has the closed-form solution of x(t) = G(t) ∙ u(t)
and can be efficiently maintained by taking G(t) = ((A(t))>(D(t))2A(t))-1 ∈ Rd×d and
u(t) = (A(t))>(D(t))2b(t).
Algorithm 1 PREPROCESS (A, b, e, T)
1:	M — [A, b]	. M ∈ R(d+1)×(d+1)
2:	D J Id+ι
# Spectral approximation
3:	s J d +1
4:	N 一 D ∙ M	. N ∈ Rs×(d+1)
5:	H J ((N)>N)-1 .H ∈ R(d+1)×(d+1)
6:	B 一 N ∙ H	. B ∈ Rs×(d+1)
# JL approximation
7:	δJ O(1/T2), k J O(e-2 log(T /δ))
8:	J J JL(s, e, δ, T) . JL matrix J ∈ Rk×s
9:	B 一 J ∙ B	. B ∈ Rk×(d+1)
# Maintain solution
10:	G J (A>D>DA)-1	.G ∈ Rd×d
11:	uJ A>D2b	.u ∈ Rd
12:	X 一 G ∙ u	. X ∈ Rd
Algorithm 3 S AMPLE (m)
1:	τ 一 ∣B ∙ m∣2
2:	P — min{3(1 + e)2e-2τlog(1∕δ), 1}
3:	V J 1/√P With probability p, and V J 0
otherwise
Algorithm 2 UPDATE (a, β)
1: m J [a> ,β]>			. m ∈ Rd+1
2: V J SAMPLE(m)			.V∈R
	D0		
3: D J			
	0V		
4: if V 6= 0 then UPDATEMEMBERS(m)			
5: return X			
Algorithm 4 UPDATEMEMBERS (m)
	# Update spectral	approximation
1	: sJs+1	
2	∆H J - Hmm>H/p 1+m> Hm/p	
3	: HJH+∆H	
4	B J [(B + N ∙ ∆H)>,	H ∙ m∕√P]>
5	N J [N>, m∕√p]>	
	# Update JL approximation	
6	: J J JL(s, e, δ, T)	
7	〜 B J J ∙ B	
	# Update solution	
8	Gaa> G/p G J G - 1+a>Ga∕p	
9	U J U + β ∙ a/p	
10	X J G ∙ u	
We outline the proof of Theorem 3.1, and defer the detailed proof to Appendix C due to space limits.
5
Under review as a conference paper at ICLR 2022
3.2	Correctness
We show the correctness of our algorithm and prove it maintains an e-approXimate solution for all
iterations with high probability. We start with closed-form formulas for all the variables we maintain.
Lemma 3.3 (Closed-form formulas). At the t-th iteration of Algorithml—^, we have
1.	M(t) =[A(t),b(t)] ∈ R(d+t+1)×(d+1).
2.	D(t) ∈ R(d+t+1)×(d+t+1) is a diagonal matrix with s(t) non-zero entries.
3.	N㈤=(D㈤M㈤)S(t),* ∈ Rs(t) ×(d+1), where S⑴ ⊂ [d +1 + 1] is defined as the set of
non-zero entries of D(t).
4.	H㈤=((N(O)>N(t))-1 ∈ R(d+1)×(d+1).
5.	B(t) = N(t)H(t) ∈ Rs(t) ×(d+1).
6.	BB⑴=J㈤∙ B⑴ ∈ Rk×(d+1), where k = O(e-2 log(T∕δ)).
7.	G㈤=((A⑴)>(D㈤)2A⑴)— 1 ∈ Rd×d.
8.	u(t) =(A(t))>(D(t))2b(t) ∈ Rd.
9.	X㈤=((A㈤)>(D⑴)2A㈤)-1 ∙ (A⑴)>(D㈤)2b⑴ ∈ Rd
The following lemma is key for our correctness analysis. It shows that we maintain a good approxi-
mation on online leverage scores and a spectral approximation of M(t) throughout all iterations.
Lemma 3.4 (Spectral approximation via leverage score maintenance). With probability at least
1 - 2Tδ,
(1-e)2τdM+(tt+-11)(M(t)) ≤τ(t) ≤ (1+e)2τdM+(tt+-11)(M(t)), ∀t∈ [T],	(2)
and
(M㈤)>(D⑴)2M㈤ ≈ (M⑴)>M⑴，∀t ∈ [0 : T].	(3)
Proof Sketch. We prove by induction and show that with probability 1 - 2tδ, Eq. (3) holds for all
t0 ∈ [0 : t] and Eq. (2) holds for all t0 ∈ [t]. The base case t = 0 holds trivially, as D(0) = Id+1,
and therefore, (M(0))>(D(0))2M(0) = (M(0))>M(t). Given the induction hypothesis upon t - 1,
we proceed in the following three steps.
• We first use the induction hypothesis to prove that kB(t-1)m(t) k is a good estimate on the
online leverage score, that is
(1 - e)τM二 1) (M⑴)≤ kB(t-1) ∙ m㈤ k2 ≤ (1 + e)τMt-1) (M⑴).
• We then use the JL lemma (Lemma B.4) to show that with probability 1 - δ, the sketched
covariance matrix kBe (t-1) m(t) k returns good estimation kB(t-1)m(t) k. That is
(1 -e)2 ∙ TMt-I) (M⑴)≤ kB(T) ∙ m㈤ k2 ≤ (1 + e)2 ∙ TMi) (M㈤).(4)
• Finally, we wrap up the proof by proving the second part of induction. In particular, we
show that conditioned on Eq. (4) holds, we have
(M(t))>(D(t))2M(t) ≈ (M(t))>M(t).
The proof then follows by an union bound over failure events.
□
It is well known that spectral approximations of (M(t))>M(t) give approximate solutions to least
squares regressions (Woodruff, 2014), so we have proved the correctness of our algorithm.
Lemma 3.5 (Correctness of Algorithm[1-4). With probability at least 1-O(1∕T), in each iteration,
UPDATE of Algorithm 2 outputs a vector X(t) ∈ Rd such that
kA(t)X(t) -b(t)k2 ≤ (1+e) min kA(t)X - b(t) k2.
x∈Rd
6
Under review as a conference paper at ICLR 2022
3.3	Time analysis
Next, we bound the overall update time of our algorithm. We first compute the worst case update
time of Algorithm 2. When ν(t) =0, i.e., the t-th row is not sampled, the UPDATE procedure
only needs to compute the approximate leverage score τ (t) (SAMPLE, Algorithm 3), and it takes
O(k ∙ nnz(m(t))) time. When V(t) = 0, i.e., the t-th row is sampled, the UPDATE procedure makes
a call to UPDATEMEMBERS (Algorithm 4), and it takes O(k ∙ s(t) ∙ d) time. Plugging in the value of
k, we have the following lemma.
Lemma 3.6 (Worst case update time). At the t-th iteration of the UPDATE procedure (Algorithm 2),
•	If Vitt = 0, then UPDATE takes O(e-2 log(T∕δ) • nnz(a(tt)) time.
•	If ν(t) = 0, then UPDATE takes O{e-2s(e)dlog(T∕δ)) time.
To bound the amortized update time, we need to bound the total number of sampled rows, and this
is closely related to the sum of online leverage scores. Such an upper bound was already established
by Cohen et al. (2020), here we present a slightly generalized version of it.
Lemma 3.7 (Sum of online leverage scores, generalization of Theorem 2.2 of (Cohen et al., 2020)).
If the matrix M(T ) satisfy Assumption 2.2, then
T
XTMt-I)(M(t)) ≤ O(dlog(TD∕σ)).
t=1
Now we are ready to bound the amortized update time of our algorithm.
Lemma 3.8 (Amortized update time). With probability at least 0.99, the total running time of
UPDATE over T iterations is at most O(e-2 nnz(AiTt)log(T) + e-6d3 log5(TD∕σ)).
Proof Sketch. In this proof sketch we simplify the second term as d3 • poly(e-1 log(T D∕σ)). The
first term comes from the computation cost of querying leverage score, which takes O (e-2 log(T∕δ) •
nnz(a(t)) time in the t-th iteration even if the t-th row is not sampled. The second term bounds the
total update time for the sampled rows:
•	From Lemma 3.4 and Lemma 3.7, with high probability the sum of the approximate online
leverage scores τ(t) are bounded by O(d log(T D∕σ)).
•	Using Markov inequality, the total number of sampled rows is bounded by
TT
S(T) =O(Xp(tt) = O(e-2 • log(1∕δ) ∙ X τ(t)) ≤ O(d ∙ poly(e-1 log(TD∕σ))).
i=1	t=1
•	Since there are S(T) sampled rows, and for each sampled row we update data structure
members in ≤ O S(T)d • poly(e-1 log(T D∕σ)) time, the total update time for sampled
rows is
S(T) •O(S(T)d • poly(e-1 log(TD∕σ))) = O(d3 • poly(e-1 log(TD∕σ))).	□
4	Hardness result
We prove a Ω(d2-o(I)) amortized time lower bound for dynamic least squares regression with high
precision, assuming the OMv conjecture. The OMv conjecture was originally proposed by Hen-
zinger et al. (2015), and it is widely accepted in the theoretical computer science community.
Conjecture 4.1 (OMv conjecture, (Henzinger et al., 2015)). Let d ∈ N,T = poly(d). Let γ>0
be any constant. B ∈{0, 1}d×d is a Boolean matrix. ∀t ∈ [T], a Boolean vector z(t) ∈{0, 1}d
is revealed at the t-th step. We say an algorithm solves the OMv problem if it returns the Boolean
matrix-vector product Bz(t) ∈ Rd at every time step. The conjectures states that there is no al-
gorithm that solves the OMv problem using poly(d) preprocessing time and O(d2-γ) amortized
running time, and has an error probability ≤ 1∕3.
7
Under review as a conference paper at ICLR 2022
The results in this section are all under the Word RAM model where the word size w = O(log d).
Our main result is formally stated below.
Theorem 4.2 (Hardness of dynamic-least squares regression with high precision). Let d ∈ N, T =
poly(d), e = d8τ2 = 1/poly(d), and let Y > 0 be any constant. Assuming the OMv conjecture is
true, any dynamic algorithm that maintains an e-approximate solution ofthe least squares regression
requires at least Ω(d2-γ) amortized time per update.
Our lower bound is proved by first reducing the standard OMv conjecture for Boolean matrices to
OMv-hardness for well-conditioned positive semidefinite (PSD) matrices over real numbers. Then
we use this new OMv-hardness result to prove our lower bound for dynamic least squares regression.
We only provide a proof sketch here and detailed proof are delayed to Section D.
OMv-hardness for well-conditioned PSD matrix The OMv conjecture asserts the hardness of
solving online Boolean matrix-vector product exactly. We extend it to solving online real-valued
matrix-vector product for well-conditioned PSD matrices, while allowing polynomially small error.
Lemma 4.3 (Hardness of approximate real-valued OMv). Let d ∈ N,T = poly(d). Let γ>0 be
any constant. Let H ∈ Rd×d be a symmetric matrix whose eigenvalues satisfy 1 ≤ λd(H) ≤ •…≤
λ1 (H) ≤ 3. For any t ∈ [T], z(t) ∈ Rd is revealed at the t-th step, and kz(t) k2 ≤ 1. Assuming the
OMv conjecture is true, then there is no algorithm with poly(d) preprocessing time and O(d2-γ)
amortized running time that can return an O(1/d2)-approximate answer to Hz(t) for all t, i.e., a
vector y(t) ∈ Rd s.t. ky(t) - Hz(t) k2 ≤ e, and has an error probability ≤ 1/3.
Proof Sketch. Given a Boolean matrix B ∈{0, 1}d×d in the OMv conjecture, we construct a PSD
ɪ B
d
2Id
2Id
matrix H = I1 g>
Given a binary OMV query vector z(t), We construct z(t) = (0d, z(t)) ∈ R2d. Since H has a constant
∈ R2d×2d. We note that H is symmetric and 1 ≤ λd(H) ≤ λ1 (H) ≤ 3.
condition number, we can prove that rounding an e 〜1 /d2 -approximate answer b ≈e H ∙ z(t) still
gives the correct binary answer to Bz(t).	□
Reducing OMv to dynamic least-squares regression We next wrap up the proof of Theorem 4.2
by reducing OMv to dynamic e-LSR.
Proof Sketch of Theorem 4.2. Given a PSD matrix H and a sequence of query {Hz(t)}tT=1 of the
problem in Lemma 4.3, we reduce it to a dynamic e-LSR, where the initial A is such that A>A =
H-1 (this preprocessing step of the reduction takes 〜 dω time), and the label is 0 for the initial
d data. For each t ∈ [T], the incoming row a(t) is a small scaled version of z(t), i.e, a(t)
d2√T ∙ z(t) ∈ Rd, and the label is 1. Let x?t) be the optimal solution at the t-th step, H(t):
((A(t))>A(t))-2, the reduction is complete via the following three steps:
• Step 1. x(t) and x?t) are close, i.e., x(t) = x?t) ± O(d4√τ).
• Step 2. x(t) — χ(t-D recovers H(t-1)a(t), i.e., x(t) — x(t-1) = H(t-1)a(t) ± O( d√^).
• Step 3. H(IWt) is close to Ha⑶，i.e., H(t-1)a(t) = Ha⑶ ± O(^^).
In particular, let y(t) = d2√T(x(t) — x(t-1)), Step 2 and 3 directly implies ∣∣y — HZ⑶k2 ≤
O(1∕d2). This completes the proof.	□
5	Experiments
Our method is most suitable for data distributions that are non-uniform. Indeed, if the data has
low coherence (they are all similar to each other), then the naive uniform sampling is as good as
leverage score sampling. We perform empirical evaluations on our algorithm over both synthetic
and real-world datasets.
Synthetic dataset We follow the empirical study of (Dobriban & Liu, 2019) and generate data
from the elliptical model. In this model a(t) = w(t)Σz(t), where Zt 〜N(0,Id) is a random
Gaussian vector, Σ ∈ Rd×d is a PSD matrix, and w(t) is a scaler. The label is generated as b(t) =
8
Under review as a conference paper at ICLR 2022
ha(t), x?i + w(t)ξ(t), where x? ∈ Rd is a hidden vector and ξ ~ N(0,1) is standard Gaussian
noise. This model has a long history in multivariate statistics, see e.g. (Martin & Maes, 1979). In
our experiments, We set Σ = Id for simplicity. In order to make the dataset non-trivial, We set w(t)
to be large (= √T) for a few (= d/10) iterations, and small (= 1) for the rest of the iterations. We
setT= 400000 and d = 500.
Real-world dataset We use the VirusShare dataset from the UCI Machine Learning Repository4.
We select this dataset because it has a large number of features and data points, and has low errors
when fitted by a linear model. The dataset is collected from Nov 2010 to Jul 2014 by VirusShare
(an online platform for malware detection). It has T = 107888 data points and d = 482 features.
Baseline algorithms We compare with three baseline methods. 1. Kalman’s approach makes use
of Woodburry identiy and gives an exact solution. 2. The uniform sampling approach samples new
rows uniformly at random. 3. The row sampling approach samples new rows according to the exact
online leverage scores. (Cohen et al., 2020)
Our experiments are executed on an Apple M1 CPU with codes written in MATLAB. We repeat all
experiments for at least 5 times and take the mean. On both datasets, we initiate the model based
on the first 10% of the data. The experiment results are formally presented in Figure 1 and more
details can be found in Appendix E. Our algorithm consistently outperforms baseline methods: Our
algorithm runs faster when achieving comparable error rates.
0	50	100	150	200
Running Time
10	20	30	40
Running Time

(a) Synthetic dataset	(b) VirusShare
Figure 1: Experiment results. The x-axis shows the running time (unit: seconds), and the y-axis
shows the relative error (err/errstd - 1), where err is the error of the particular approach, and errstd
is the error of the static Normal equation. The y-axis is on a log scale. For uniform sampling, we
take sampling probability p = 0.05, 0.1, 0.2, 0.5. For row sampling and our algorithm, we take the
error parameter e = 0.1,0.2,0.5,1. Kalman,s approach has a relative error of 0.
6	Conclusion
We provide the first practical and provably fast data structure for dynamic least-squares regression,
obtaining nearly tight upper and lower bounds for this fundamental problem. On the algorithmic
side, we design an e-approximation dynamic algorithm whose total update time almost matches the
input sparsity of the (online) matrix. On the lower bound side, we prove that it is impossible to
maintain an exact (or even high-accuracy) solution with《d2-o⑴ amortized update time under the
OMv conjecture. As such, this result exhibits the first separation between the static and the dynamic
LSR problems.
Our paper sets forth several interesting future directions. On the theoretical side, a very interest-
ing question is whether it is possible to reduce the additive term d3 of our algorithm to matrix-
multiplication time dω? A second open problem—of interest in both theory and practice—is whether
it is possible to achieve input-sparsity amortized update time in the fully dynamic setting, i.e., when
allowing both addition and deletion of data rows? Finally, it would be interesting to find connections
between dynamic least-squares regression and incremental training of more complicated models,
such as dynamic Kernel-ridge regression and to deep neural networks.
4https://archive.ics.uci.edu/ml/datasets.php
9
Under review as a conference paper at ICLR 2022
References
Thomas D Ahle, Michael Kapralov, Jakob BT Knudsen, Rasmus Pagh, Ameya Velingker, David P
Woodruff, and Amir Zandieh. Oblivious sketching of high-degree polynomial kernels. In Pro-
Ceedings of the Fourteenth Annual ACM-SIAM Symposium on Discrete Algorithms,pp.141-160.
SIAM, 2020.
Nir Ailon and Bernard Chazelle. Approximate nearest neighbors and the fast johnson-lindenstrauss
transform. In Proceedings of the thirty-eighth annual ACM symposium on Theory of computing,
pp. 557-563, 2006.
Josh Alman and Virginia Vassilevska Williams. A refined laser method and faster matrix multipli-
cation. In Proceedings of the 2021 ACM-SIAM Symposium on Discrete Algorithms (SODA), pp.
522-539. SIAM, 2021.
Haim Avron, Kenneth L Clarkson, and David P Woodruff. Faster kernel ridge regression using
sketching and preconditioning. SIAM Journal on Matrix Analysis and Applications, 38(4):1116-
1138, 2017.
Christos Boutsidis, David P Woodruff, and Peilin Zhong. Optimal principal component analysis in
distributed and streaming models. In Proceedings of the forty-eighth annual ACM symposium on
Theory of Computing, pp. 236-249, 2016.
van den Jan Brand, Yin-Tat Lee, Danupon Nanongkai, Richard Peng, Thatchaphol Saranurak, Aaron
Sidford, Zhao Song, and Di Wang. Bipartite matching in nearly-linear time on moderately dense
graphs. In 2020 IEEE 61st Annual Symposium on Foundations of Computer Science (FOCS), pp.
919-930. IEEE, 2020a.
van den Jan Brand, Yin Tat Lee, Aaron Sidford, and Zhao Song. Solving tall dense linear programs
in nearly linear time. In Proceedings of the 52nd Annual ACM SIGACT Symposium on Theory of
Computing (STOC), pp. 775-788, 2020b.
van den Jan Brand, Yin Tat Lee, Yang P Liu, Thatchaphol Saranurak, Aaron Sidford, Zhao Song, and
Di Wang. Minimum cost flows, mdps, and '1-regression in nearly linear time for dense instances.
In Proceedings of the 53rd Annual ACM SIGACT Symposium on Theory of Computing (STOC),
pp. 859-869, 2021a.
van den Jan Brand, Binghui Peng, Zhao Song, and Omri Weinstein. Training (overparametrized)
neural networks in near-linear time. In 12th Innovations in Theoretical Computer Science Con-
ference (ITCS 2021), 2021b.
Vladimir Braverman, Petros Drineas, Cameron Musco, Christopher Musco, Jalaj Upadhyay, David P
Woodruff, and Samson Zhou. Near optimal linear algebra in the online and sliding window
models. In 2020 IEEE 61st Annual Symposium on Foundations of Computer Science (FOCS), pp.
517-528. IEEE, 2020.
SebaStien Bubeck. Convex optimization: Algorithms and complexity. Foundations and Trends in
Machine Learning, 8(3-4):231-357, 2015.
Nadiia Chepurko, Kenneth L Clarkson, Praneeth Kacham, and David P Woodruff. Near-
optimal algorithms for linear algebra in the current matrix multiplication time. arXiv preprint
arXiv:2107.08090, 2021.
Charles K. Chui. Estimation, control, and the discrete kalman filter (donald e. calin). SIAM Re-
view, 32(3):493-494, 1990. doi: 10.1137/1032097. URL https://doi.org/10.1137/
1032097.
Kenneth L Clarkson and David P Woodruff. Numerical linear algebra in the streaming model. In
Proceedings of the forty-first annual ACM symposium on Theory of computing, pp. 205-214,
2009.
Kenneth L Clarkson and David P Woodruff. Low-rank approximation and regression in input spar-
sity time. Journal of the ACM (JACM), 63(6):1-45, 2017.
10
Under review as a conference paper at ICLR 2022
Michael B Cohen, Yin Tat Lee, Cameron Musco, Christopher Musco, Richard Peng, and Aaron
Sidford. Uniform sampling for matrix approximation. In Proceedings of the 2015 Conference on
Innovations in Theoretical Computer Science (ITCS), pp.181-190. ACM, 2015.
Michael B Cohen, Cameron Musco, and Jakub Pachocki. Online row sampling. Theory OF Com-
puting, 16(15):1-25, 2020.
Edgar Dobriban and Sifan Liu. Asymptotics for sketching in least squares. In Proceedings of the
33rd International Conference on Neural Information Processing Systems, pp. 3675-3685, 2019.
Petros Drineas, Michael W. Mahoney, and S. Muthukrishnan. Sampling algorithms forl 2 regression
and applications. In Proceedings of the seventeenth annual ACM-SIAM symposium on Discrete
algorithm (SODA), pp. 1127-1136, 2006a.
Petros Drineas, Michael W. Mahoney, and S. Muthukrishnan. Subspace sampling and relative-
error matrix approximation: Column-based methods. In Approximation, Randomization, and
Combinatorial Optimization. (APPROX-RANDOM), pp. 316-326, 2006b.
Petros Drineas, Michael W. Mahoney, and S. Muthukrishnan. Subspace sampling and relative-
error matrix approximation: Column-row-based methods. In European Symposium on Algorithms
(ESA), pp. 304-314, 2006c.
Trevor Hastie, Jerome H. Friedman, and Robert Tibshirani. The Elements of Statistical Learning:
Data Mining, Inference, and Prediction. Springer Series in Statistics. Springer, 2001. ISBN 978-
1-4899-0519-2. doi: 10.1007/978-0-387-21606-5. URL https://doi.org/10.1007/
978-0-387-21606-5.
Elad Hazan. Introduction to online convex optimization. arXiv preprint arXiv:1909.05207, 2019.
Monika Henzinger, Sebastian Krinninger, Danupon Nanongkai, and Thatchaphol Saranurak. Unify-
ing and strengthening hardness for dynamic problems via the online matrix-vector multiplication
conjecture. In Proceedings of the forty-seventh annual ACM symposium on Theory of computing,
pp. 21-30, 2015.
Shunhua Jiang, Zhao Song, Omri Weinstein, and Hengjie Zhang. A faster algorithm for solving
general lps. In Proceedings of the 53rd Annual ACM SIGACT Symposium on Theory of Computing
(STOC), pp. 823-832, 2021.
William B Johnson and Joram Lindenstrauss. Extensions of lipschitz mappings into a hilbert space.
Contemporary mathematics, 26(189-206):1, 1984.
Rudolph Emil Kalman. A new approach to linear filtering and prediction problems. Journal of Basic
Engineering, 82(1):35-45, 1960.
Frangois Le Gall. Powers of tensors and fast matrix multiplication. In Proceedings of the 39th
international symposium on symbolic and algebraic computation, pp. 296-303, 2014.
Yin Tat Lee and Aaron Sidford. Path finding methods for linear programming: Solving linear
programs in te(√rank) iterations and faster algorithms for maximum flow. In 2014 IEEE 55th
Annual Symposium on Foundations of Computer Science, pp. 424-433. IEEE, 2014.
Yin Tat Lee, Zhao Song, and Qiuyi Zhang. Solving empirical risk minimization in the current matrix
multiplication time. In Annual Conference on Learning Theory (COLT), 2019.
Edo Liberty, Zohar Karnin, Bing Xiang, Laurence Rouesnel, Baris Coskun, Ramesh Nallapati, Julio
Delgado, Amir Sadoughi, Yury Astashonok, Piali Das, et al. Elastic machine learning algorithms
in amazon sagemaker. In Proceedings of the 2020 ACM SIGMOD International Conference on
Management of Data, pp. 731-737, 2020.
Aleksander Madry. Navigating central path with electrical flows: From flows to matchings, and
back. In 2013 IEEE 54th Annual Symposium on Foundations of Computer Science (FOCS), pp.
253-262. IEEE, 2013.
Nick Martin and Hermine Maes. Multivariate analysis. Academic press London, 1979.
11
Under review as a conference paper at ICLR 2022
Xiangrui Meng and Michael W Mahoney. Low-distortion subspace embeddings in input-sparsity
time and applications to robust linear regression. In Proceedings of the forty-fifth annual ACM
symposium on Theory ofcomputing, pp. 91-100, 2013.
Jelani Nelson and HUy L Nguyen. Osnap: Faster numerical linear algebra algorithms via sparser
subspace embeddings. In 2013 ieee 54th annual symposium on foundations of computer science,
pp. 117-126. IEEE, 2013.
Rasmus Pagh. Compressed matrix multiplication. ACM Transactions on Computation Theory
(TOCT), 5(3):1-17, 2013.
German I Parisi, Ronald Kemker, Jose L Part, Christopher Kanan, and Stefan Wermter. Continual
lifelong learning with neural networks: A review. Neural Networks, 113:54-71, 2019.
R. L. Plackett. Some theorems in least squares. Biometrika, 37(1/2):149-157, 1950. ISSN
00063444. URL http://www.jstor.org/stable/2332158.
Lawrence R Rabiner and Bernard Gold. Theory and application of digital signal processing. Engle-
wood Cliffs: Prentice-Hall, 1975.
Ilya Razenshteyn, Zhao Song, and David P Woodruff. Weighted low rank approximations with
provable guarantees. In Proceedings of the forty-eighth annual ACM symposium on Theory of
Computing, pp. 250-263, 2016.
Vladimir Rokhlin and Mark Tygert. A fast randomized algorithm for overdetermined linear least-
squares regression. Proceedings of the National Academy of Sciences, 105(36):13212-13217,
2008.
Tamas Sarlos. Improved approximation algorithms for large matrices via random projections. In
2006 47th Annual IEEE Symposium on Foundations of Computer Science (FOCS’06), pp. 143-
152. IEEE, 2006.
Daniel A Spielman and Nikhil Srivastava. Graph sparsification by effective resistances. SIAM
Journal on Computing, 40(6):1913-1926, 2011.
Daniel A Spielman and Shang-Hua Teng. Nearly-linear time algorithms for graph partitioning,
graph sparsification, and solving linear systems. In Proceedings of the thirty-sixth annual ACM
symposium on Theory of computing, pp. 81-90, 2004.
Stephen M. Stigler. Gauss and the Invention of Least Squares. The Annals of Statistics, 9(3):
465-474, 1981. doi: 10.1214/aos/1176345451. URL https://doi.org/10.1214/aos/
1176345451.
Volker Strassen. Gaussian elimination is not optimal. Numerische mathematik, 13(4):354-356,
1969.
David Woodruff. A very sketchy talk (invited talk). In 48th International Colloquium on Au-
tomata, Languages, and Programming (ICALP 2021). Schloss Dagstuhl-Leibniz-Zentrum fuer
Informatik, 2021.
David P. Woodruff. Sketching as a tool for numerical linear algebra. Foundations and Trends in
Theoretical Computer Science, 10(1-2):1-157, 2014.
12