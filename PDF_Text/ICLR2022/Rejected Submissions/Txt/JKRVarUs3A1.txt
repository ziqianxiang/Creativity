Under review as a conference paper at ICLR 2022
Distributed Optimal Margin Distribution Ma-
CHINE
Anonymous authors
Paper under double-blind review
Ab stract
Optimal margin Distribution Machine (ODM), a newly proposed statistical learn-
ing framework rooting in the novel margin theory, demonstrates better generaliza-
tion performance than the traditional large margin based counterparts. Nonethe-
less, the same with other kernel methods, it suffers from the ubiquitous scalability
problem in terms of both computation time and memory. In this paper, we propose
a Distributed solver for ODM (DiODM), which leads to nearly ten times speedup
for training kernel ODM. It exploits a novel data partition method to make the
local ODM trained on each partition has a solution close to the global one. When
linear kernel used, we extend a communication efficient distributed SVRG method
to further accelerate the training. Extensive empirical studies validate the superi-
ority of our proposed method compared to other off-the-shelf distributed quadratic
programming solvers for kernel methods.
1	Introduction
Recently, the study on margin theory (Gao & Zhou, 2013) demonstrates an upper bound disclos-
ing that maximizing the minimum margin does not necessarily result in a good performance, and
instead, the distribution rather than a single margin is much more important. Later on, the study
on lower bound (Gr0nlund et al., 2019) further proves that the upper bound is almost optimal UP to
a logarithmic factor. Inspired by these insightful works, the Optimal margin Distribution Machine
(ODM) is proposed (Zhang & Zhou, 2019), which explicitly optimizes the margin distribution by
maximizing the mean and minimizing the variance simultaneously, and exhibits much better gen-
eralization performance than the traditional large margin based counterparts.Due to the superiority
shown on both binary and multi-class classification tasks (Zhang & Zhou, 2014; 2017), many works
attempt to extend ODM to more genreal learning settings, just to list a few, cost-sensitive learn-
ing (Zhou & Zhou, 2016; Cheng et al., 2017), weak supervised learning (Zhang & Zhou, 2018a;b;
Luan et al., 2020; Zhang & Jin, 2020), multi-label learning (Tan et al., 2020), online learning (Zhang
et al., 2020) and regression (Rastogi et al., 2020).
Plenty of successes on various learning tasks validate the superiority of this new statistical learning
framework. However, these ODM based extensions suffer from the scalability problem because of
both computation time and memory, the same as other kernel methods. Some works have devoted to
accelerate the training of ODM for large scale data, e.g., CSVRG (Tan et al., 2019) takes advantage
of the coreset method, whose main idea is to adaptively construct the landmark points to sketch the
whole data. Other approximation methods being able to directly speed up ODM include random
Fourier feature (Rahimi & Recht, 2007) and Nystrom (Williams & Seeger, 2001). Notice that the
random Fourier method adopts a data-independent kernel mapping while the NyStrOm method takes
a distribution-unaware sampling, they are both inferior to coreset method due to the insufficient
use of data, which may only lead to a degraded generalization performance. All these methods are
proposed for computing on one cpu core, but with the dramatic progress of digital technologies, the
data generated devices become as diverse as computers, mobile phones, smart watches, cars, etc,
and the amount of data created each day grows tremendously, which one cpu core can hardly afford
and motivates us to seek the distributed training methods.
Up to now many machine learning methods have already owned their distributed versions, among
which the closest one to our problem is the distributed SVMs, whose ideas can be concluded into
two classes. The first one works in distributed data level, i.e., dividing the data into partitions on
1
Under review as a conference paper at ICLR 2022
which local models are trained and then combined to produce the larger local or global models. For
example, in (Graf et al., 2004; Hsieh et al., 2014; Singh et al., 2017), a tree architecture on parti-
tions is designed first, guided by which the solutions of different partitions are aggregated; in (Yu
et al., 2005; Navia-Vazquez et al., 2006; Loosli et al., 2007), some key instances identification and
exchange are further introduced to accelerate the training; in (Si et al., 2017), both low-rank and clus-
tering structure of the kernel matrix are considered to get an approximation of kernel matrix. The
other one works in distributed optimization level, that is directly applying the distributed optimiza-
tion method, such as augmented Lagrangian method (Forero et al., 2010) and alternating direction
method of multipliers (Boyd et al., 2010), or extending existing solver to distributed environment,
e.g. distributed SMO (Cao et al., 2006).
Although many distributed solver for quadratic programming (QP) problems can be directly ap-
plied to ODM, these off-the-shelf solvers all ignore the intrinsic structure of the problem and can
hardly achieve the greatest efficiency. We propose a specially designed Distributed solver for
ODM (DiODM) in this work. To be specific, we put forward a novel data partition method so
that ODM trained on each partition has a solution close to that trained on the whole data. There-
fore, when some partitions are merged to form a larger partition, the solution of ODM on it can be
quickly obtained by concatenating the corresponding local solutions as the initial point. Besides, in
the case of linear kernel being used, we extend a communication efficient distributed SVRG method
to further accelerate the training. To summarize, the remarkable differences of DiODM compared
with existing distributed QP solvers are threefold:
1)	DiODM incorporates a novel partition strategy, which makes the solution of local ODM on each
partition close to the global one so that the training can be accelerated.
2)	When linear kernel is used, DiODM extends a communication efficient distributed SVRG method
to further accelerate the training.
3)	DiODM can maintain the generalization performance of ODM meanwhile achieve nearly ten
times speedup, much more efficient than existing distributed QP solvers.
In the rest of this paper, some preliminaries are first introduced in section 2, followed by the technical
detail of the proposed DiODM in section 3. In section 4, we present the results of experimental and
give empirical observations. Last, in section 5 we conclude the paper with future work.
2	Preliminaries
Before diving into the technical details, we first give some default notation in our paper. Sets are
designated by upper case letters with mathcal font (e.g., S). The input space is X ⊆ RN and
Y = {1, - l} is output space.d•] is the round UP function. For any positive integer M, the set of
integers {1, . . . , M} is denoted by [M]. For the feature mapping φ : X 7→ H associated to some
positive definite kernel κ where H is the corresponding reproducing kernel Hilbert space (RKHS),
κ(x, z) = hφ(x), φ(z)iH holds for any x and z. In this paper we consider the shift-invariant kernel
such as RBF kernel for simplicity, which satisfies κ(x, z) = κ(x - z).
The traditional large margin based methods maximize the minimum margin, and consequently the
obtained decision boundary is only determined by a fraction of instances with the minimum mar-
gin (ScholkoPf & Smola, 2001), which may hurt the generalization performance.
On the other hand, ODM explicitly optimizes the margin distribution. Given a labeled data set
{(xi, yi)}i∈[M] , ODM is initially formalized by maximizing the margin mean and minimizing the
margin variance:
min
w,羽ξi,ei
2Ilwk2 - ηγ + 2M X (ξ2 + e2)
i∈[M]
s.t. Y - ξi ≤ Yi ≤ 7 + ɛi, ∀i ∈ [M],
(1)
where η, λ are the trading-off parameters, Yi = yiw>φ(xi) is the margin of the labeled instance
(xi , yi ) and Y7 is the mean of Yi . Note that the two slack variables ξi and i are deviations from the
margin mean, therefore the third term is exactly the margin variance.
The margin mean can be fixed as 1 since scaling w does not affect the decision boundary. Besides,
instances with margin less than 1 are more likely to be misclassified, therefore we introduce a new
2
Under review as a conference paper at ICLR 2022
hyper-parameter υ ∈ [0, 1] to weight the ξi. Last but not least, notice that instances not lying on
the hyperplane yiw>φ(xi) = 1 all contribute to the decision boundary, to achieve a lightweight
model, we can tolerate the deviation smaller than the given threshold θ. Plugging all these back into
Eqn. (1) yields the primal problem of ODM:
min P(W) = 1 kwk2 + 余 X ξ1 + ；；2
w,ξi,i	2	2M i∈[M] (1-θ)2
s.t. 1 - θ - ξi ≤ yiw>φ(xi) ≤ 1 + θ + i, ∀i ∈ [M].
By introducing the Lagrange multipliers ζ , β ∈ R+M for the 2M inequality constraints respectively,
the dual problem of ODM is
ζ,minM d(Z，例= I(Zf)>Q(Zf) + MMc(UkZk2 + kβk2) + (θ - I)IM Z + (θ +I)IM β
where [Q]j = y%yj κ(xi, Xj) and C = (1 - θ)2∕λυ is a constant. The detailed derivation can be
found in supplementary. By denoting α = [Z; β], the dual ODM can be rewritten as a standard
convex QP problem:
min f (α) = 1 α>Hα + b>α,	(2)
α∈R2+M	2
in which
(θ - 1)1M
(θ + 1)1M .
Q + M cυI	-Q
-Q	Q +M cI
H
Notice that Eqn. (2) only involves 2M decoupled box constraints, thus it can be efficiently solved
by a dual coordinate descent method. To be specific, in each iteration, only one variable is selected
to be updated while other variables are kept as constants, which yields the following univariate QP
problem of t:
min f(α + tei) = 1[H]iit2 + [Vf(α)]it + f(α),	(3)
t2
which has a closed-form solution:
[α]i J max([α]i - [Vf(α)]i∕[H]ii, 0).
3 Proposed Method
DiODM works in distributed data level, that is it works by dividing the data into partitions on which
local models are trained and then used to find the larger local or global models. For simplicity, we
assume there are K = pL partitions at first with the same cardinality m, i.e., m = M∕K. The data
set {(xi, yi)}i∈[M] are ordered so that the first m instances are on the first partition, and the second
m instances are on the second partition, etc. That is for any instance (xi, yi), the index of partition
to which it belongs is P (i) = d(i - 1)∕me.
Suppose {(xi(k), yi(k))}i∈[m] is the data on the k-th partition, then the local ODM on it is
min
ζk,βk∈R+m
dk(Zk,βk) = 2(Zk - βk)>Q(k)(Zk - βk)
+ ^2^ (UkZk k2 + kβkk2) + (θ - 1)1mZk + (θ + 1)1mβk,
where [Q(k)]ij = yi(k)yj(k)κ(xi(k), x(jk)). This problem can be rewritten as a standard convex QP
problem in the manner of Eqn. (2), and efficiently solved by dual coordinate descent method as
Eqn. (3).
Once the parallel training of pL local ODMs are completed, we merge every p partitions to form
pL-1 larger partitions. On each larger partition, a new local ODM is trained again by dual coor-
dinate descent method, but the optimization procedure is not started from the scratch. Instead, the
3
Under review as a conference paper at ICLR 2022
p solutions obtained on the p corresponding smaller partitions making up this larger partition, are
concatenated as the initial point of the optimization. Since by our proposed novel partition strategy,
this concatenated solution is already a good approximation to the optimal solution of ODM on the
larger partition. We will elaborate on this in subsequent sections.
The above procedure is repeated until the solution converges or all the partitions are merged together.
We give the pseudo-code of DiODM in Algorithm 1.
Algorithm 1 DiODM
Input: Data set D = {(xi, yi)}M=ι.
Parameter: Partition control parameter p, number of stratums S, number of iterations L.
Output The solution α.
1:	Get S landmarks by Eqn. (5) on D.
2:	Sample instances without replacement to get partitions D1, . . . , DpL .
3:	Initial the dual solution αL = 0.
4:	for l = L, . . . , 1 do
5:	if α converge then
6:	break
7:	end if
8:	for p = 1, . . . , pl do
9:	Solve the ODM with dual coordinate descent.
10:	end for
11:	Merge every p partitions to form new partitions.
12:	αl-1 = αl.
13:	end for
14:	return αl .
3.1 Convergence
In this subsection, we present a theorem to guarantee the convergence of the proposed method.
Notice that the optimization variables on each partition are decoupled, they can be jointly optimized
by the following problem:
min d(Z, β) = 1(Z - β)>Q(Z- β) + mc(UkZk2 + kβk2) + (θ - 1)1MZ + (θ + 1)1Mβ,
ζ,β∈R+M	2	2
where Qe = diag(Q(1) *, . . . , Q(K)) is a block diagonal matrix. It can be seen that the smaller the
K, the more close the above formula to ODM, and when K = 1, it exactly degenerates to ODM.
Therefore, DiODM deals with ODM by solving a series of problems which approaches to it, and the
solution of former problems can be helpful for the optimization of the latter problems.
Theorem 1. Suppose the optimal solutions of ODM and its approximate problem, i.e., d(Z, β), are
α? = [Z? ; β?] and αe? = [Z? ; β?] respectively, then the gaps between these two optimal solutions
satisfy
0 ≤ d(Z?, β?) - d(Z?, β?) ≤ U2(Q + M(M - m)c),
U2
kα? - α*k2 ≤ — (Q + M(M - m)c),
Mcυ
where Q =	i,j:P (i)6=P (j) |[Q]ij | is the sum of the absolute values of the Q’s entries which turn to
zero in Q, and U = max(∣∣α?k∞, ∣∣α *k∞) upperbounds the infinity norm of solutions.
Due to the page limitations, we only provide the sketch of proof here. The full proof can be found
in the supplementary.
Proof sketch. The left-hand side of the first inequality is due to the optimality of Z? and β? .
By comparing the definition of d(Z, β) with that of d(Z, β), we can easily find that the only differ-
ences are the change of Q to Qe , and M to m. Thus the gap between d(Z?, β?) and de(Z?, β?) can
4
Under review as a conference paper at ICLR 2022
be upper bounded by the infinity norm of α? and the sum of the absolute values of the Q’s entries
which turn to zero in Q. The gap between d(ζ?, β?) and d(ζ?, β?) can be upper bounded in a sim-
ilar way. Combining these together with the optimality of ζ? and β?, i.e., d(ζ?, β?) ≤ d(ζ?, β?),
can yield the right-hand side of the first inequality.
Notice that f(αe?) is a quadratic function, hence besides the gradient g and Hessian matrix H, all its
higher derivatives are all zero, and it can be precisely expanded at α? as
f (α?) + g>(α? - α?) + (α? - α?)>H(α? - α?),
in which g>(αe? - α?) is nonnegative according to the the first order optimality condition. Further-
more, H can be lower bounded by the sum of a positive semidefinite matrix and a scalar matrix:
H	-QQ	-QQ +McυI I .
By putting all these together, We can show that ∣∣<α? 一 α?∣∣2 is upper bounded by f (α?) — f (α?),
i.e., d(ζ?, β?) - d(ζ?, β?), and with the right-hand side of the first inequality we can derive the
second inequality.	口
This theorem indicates that the gap between the optimal solutions of ODM and its approximation,
that is the problem solved in each iteration of DiODM, depends on M - m and Q. As the iteration
going on, the partitions become larger and larger, then the number of instances m on each partition
tends to be the total number of instances M ; Furthermore, the matrix Q approaches to Q which
makes Q decrease. Therefore, the solution obtained in each iteration of DiODM is getting closer
and closer to that of ODM, that is to say, our proposed algorithm converges.
3.2	Partition S trategy
In this section we detail the partition strategy, which plays an important role in our proposed method,
since partition strategy can significantly affect the optimization efficiency. Up to now, most partition
strategies utilize the clustering algorithms to form the partitions. For example, DC-SVM (Hsieh
et al., 2014) adopts the kernel k-means and simply regards each cluster as a partition. However,
ODM heavily depends on the mean and variance of training data. Directly treating clusters as
partitions will lead to significant difference among the distribution of partitions and the whole data,
which makes the local solutions on each partition are far from the global one.
To preserve the original distribution as much as possible, we borrow the idea from stratified sam-
pling, i.e., we first divide the data set into some homogeneous strata, and then apply random sam-
pling within each stratum. To be specific, suppose the goal is to generate K partitions. We first
choose S landmark points {φ(zs)}s∈[S] in RKHS, and then construct one stratum for each land-
mark point by assigning the rest instances to the stratum in which its nearest landmark point lies,
i.e., the index of stratum xi belongs to is
2(i) = arg min ∣∣φ(xi) 一 φ(zs)∣∣.
s∈[S]
For each stratum Cs, we equally divide it into K pieces by random sampling without replacement,
and take one piece from each stratum to make up a partition and totally K partitions are created.
The remaining question is how to select these landmark points. Obviously, they should be represen-
tative enough to sketch the whole data distribution. To this end, we introduce the minimal principal
angle which is defined between different stratum:
T = min Lrccos 名言
i6=j	∣x∣∣z ∣
x ∈ Ci , z ∈ Cj
Apparently, the larger the angle, the higher variation among the strata, and the more representative
each partition is, which is strictly described by the following theorem.
Theorem 2. For shift-invariant kernel κ with κ(0) = r2, that is ∣φ(x)∣ = r for any x. With the
partition strategy described above, for any k ∈ [K], we have
U2M2c
dk(Zk, βk) - d(Z ?, β?) ≤ —2— +2UM + U 2M 2r2 + U 2r2 cos T (2Θ - M2),
where Θ = Ei j∈[M] i=j1^(i)=wj), and U is same with Theorem 1.
5
Under review as a conference paper at ICLR 2022
Proof sketch. Follow the construction process mentioned in appendix, we generate a data set Dk0
where |Dk0 | = |D| = M . According to our settings, the number of instances in |Dk0 | belong to the
s-th stratum equals to |Cs|. Give the definition of dk(ζk, βk) and d0k(ζk0 , βk0 ) in appendix, we have
d0k(ζk0 , βk0 ) = dk(ζk, βk) since Dk and Dk0 share the same margin mean and variance under the
same hyperplane.
Denote α0k = [ζk0 ; βk0 ], γk0 = [ζk0 - βk0 ]. By contrasting the definition of d0k(ζk0 , βk0 ) with that of
d(ζ?, β?), we can find that the differences consists of three parts. The first part is
M (UkZk k2 + kβk k2-υkζ *k2-kβ*k2),
and the second part is
(θ + 1)1M Iek- β?) + (θ - 1)1M (Zk - Z ?).
Note that both of above parts can be upper bounded by the infinity norm of α0k . The third part can
be denoted as
gk(Yk) = 1 YJQlkYk - 1 Y?>QY?.
With the boundness that -U2 ≤ γ^iγj, Ykiγkj ≤ U2, 0 ≤ U ≤ 1, 0 ≤ θ ≤ 1, we have
gk(Yk0) ≤U2 X (κ(x0i(k), x0j (k)) - κ(xi,xj))	(4)
i,j∈[M]
Upper bound κ(x0i(k), x0j(k)) - κ(xi, xj) by minimal principal angle and the law of cosines, we can
easily find that gk (Yk0 ) ≤ U2M 2r2 + U2r2 cos τ (2Θ - M 2). By combining the upper bound of
above three parts, We can complete the proof.	□
As shown in theorem 2, we derive an upper bound on the gap between the optimal object value on
D and on Dk. Note that 2Θ > M 2 holds for any s ∈ [S] when |Cs| < M/2, which can be easily
satisfied. Thus, we can get more approximate solution in each partition by maximizing the minimal
principal angle τ .
However, due to the high computation cost, it is impossible to directly maximize minimal principal
angle. Instead, we implement a greedy iterative process:
z1 = arg max κ(z, z), zs = arg max	SCs-1(z)
z∈D	z∈D / {zi,...,Zs-l}
(5)
where SCs(z) = κ(z, z) - Kz>,sKs-1Kz,s is the Schur complement of a new candidate land-
mark zs+1 based on the landmarks {z1, ..., zs} which have been already selected, Kz,s =
[κ(z, z1), ..., κ(z, zs)], Ks ∈ Rs×s is defined as [Ks]i,j = κ(zi, zj). In the process of solving
zs, we actually seek the instance which maximize the determinant of kernel matrix construct by
{z1, ..., zs}. It is obviously that the maximum determinant is achieved when φ(zs) is orthogonal
to span{φ(z1), ..., φ(zs-1)}. Therefore, we can maximize the minimal principal angle by choosing
the instance with maximum Schur complement.
Itis noteworthy that each partition generated by our proposed strategy extracts proportional instances
from each stratum, thus preserves the distribution. Besides, compared with other partition strategies
based on k-means (Singh et al., 2017), we consider not only in the original feature space, but also in
the situation where data can hardly be linear separated. Last but not the least, our partition strategy
has lower time complexity.
3.3	Acceleration for Linear Kernel
The solving process of dual coordinate descent requires too much storage and computing resources,
mainly caused by the enormous kernel matrix. It is noteworthy that when linear kernel is used, we
can directly solve the primal form of ODM to avoid computing and storing kernel matrix. Denote
the gradient of p(w) on instance (xi, yi) as Vpi(w), we have
Vpi(W) = W +
λ(yiW>Xi + θ - 1)yiXi1i三ι
(1-θ)2
λυ(yiW>Xi - θ - 1)yiXiL∈i
+	(1 - θ)2
6
Under review as a conference paper at ICLR 2022
where I1 = {i | yiw>xi < 1 - θ}, and I2 = {i | yiw>xi > 1 + θ}.
Since the the objective function of ODM is differentiable, distributed SVRG (DSVRG) (Lee et al.,
2017) can be exploited in this scenario. It generates a series of extra auxiliary data sets sampling
from the the original data set without replacement which share the same data distribution as the
whole data set, so that an unbiased estimation of the gradient can be computed. In each iteration, all
nodes are joined together to compute the full gradient first. Then each node performs the iterative
update of SVRG in serial in a “round robin” fashion, i.e., let all nodes stay idle except one node
performing a certain steps of iterative updates using its local auxiliary data and passing the solution
to the next node. We show the process of solving DiODM by DSVRG algorithm in Algorithm 2.
Algorithm 2 Accelerated DiODM for linear kernel
Input: Training data set D = {(xi , yi)}iM=1
Parameters: Number of partitions K, number of stratums S, number of stages St , step size η, the
number of iterations T .
Output: Solution at St stage wSt
1:	Get S landmark points by Eqn. (5) on D.
2:	Sample instances without replacement to get partitions D1 , . . . , DK
3:	Generate the auxiliary array R1, . . . , Rk where Ri = {j|(xj, yj) ∈ Di}
4:	S V— 1.
5:	for l = 0, 1, . . . , St - 1 do
6:	The center node sends wl to each node.
7:	for each node j = 1, 2, . . . , K in parallel do
8:	Compute hj = Pi∈Dj VPi (wl)
9:	send hlj to center.
10:	end for
11:	hl = M Pj=I hj
12:	Initial the solutions w0l+1 = w l
13:	fort = 0, 1,2, . . . ,T - 1 do
14:	Sample instances (xi , yi ) from Ds where i ∈ Rs
15:	wtl++11 = wtl+1 - ηVpi (wtl+1) - Vpi (wl) + hl)
16:	Rs V Rs\i
17:	if Rs = 0 then
18:	send wtl+11 to node s + 1.
19:	sV s +t+11.
20:	end if
21:	end for
22:	wl+1 = wTl+1
23:	end for
24:	return w St
4	Experiments
In this section, we evaluate the performance of our proposed algorithms by comparing with other
QP solvers.
4.1	Setup
We evaluate the performance of our proposed algorithms on seven real-world data sets.1 The statis-
tics of these data sets are summarized in Table 1. All features are normalized into the interval [0, 1].
For each data set, eighty percent of instances are randomly selected as training data, while the rest
are selected as testing data. All the experiments are performed on a Spark (Zaharia et al. (2012))
cluster with one master and five workers. Each machine is equipped with 16 Intel Xeon E5-2670
CPU cores and 64G RAM.
1https://www.csie.ntu.edu.tw/~cjlin/Hbsvmtools/datasets/
7
Under review as a conference paper at ICLR 2022
DiODM is compared with three state-of-the-art distributed QP solvers, that is, Cascade ap-
proach (Ca-ODM) (Graf et al. (2004)), DiP approach (DiP-ODM) (Singh et al. (2017)) and DC
approach (DC-ODM) (Hsieh et al. (2014)). We denote our proposed dual coordinate descent based
algorithm as DiODM and accelerating algorithm as acc-DiODM. Besides, in evaluating the effi-
ciency of acc-DiODM, two state-of-the-art gradient based methods are implementd, that is SVRG
method (ODMsvrg) (Johnson & Zhang (2013)) and CSVRG method (ODMcsvrg) (Tan et al. (2019)).
Data sets	gisette	phishing	a7a	cod-rna	ijcnn1	skin-nonskin	SUSY
#Instances	7,000	11,055	32,561	59,535	141,691	245,057	5,000,000
#Features	5,000	68	123	8	22	3	18
Table 1: Data set statistics.
4.2	Results with Linear kernel
Figure 1 presents the time cost and test accuracy on data sets with linear kernel. In the Figure 1(a)(e),
we show the training speedup ratio with cores increasing from 1 to 32 for DiODM and acc-DiODM.
When 32 cores used, DiODM achieves more than 9 times training speedup while acc-DiODM
achieves over 5 times training speedup. In other six figures, we can see that both DiODM and
acc-DiODM show highly competitive performance compared with other methods. Specifically, acc-
DiODM achieves better test accuracy and faster training speed than Ca-ODM, DPP-ODM and DC-
ODM. DiODM achieves the better test accuracy on 5 data sets and just slightly worse than DC-ODM
on skin-nonskin data set. On time cost, DiODM achieves faster training speed on all 6 data sets.
g ∣se He
phishmg
a7a
acc-DIODM:
⑻..............
acceleration result
⅛peeds
..(?)
SUSY： training result
75726966
+ GaODM
-卡-DPP-ODM
* DiODM
-*- aco∙DiODM
-→- DC-ODM
sldn-nonskl Inlng result
ββ8582797β73
+ Ca-ODM
-iɑ- DPP-ODM
* DiODM
-X- aco∙DiODM
-→- DC-ODM
1	2	3
Treirwig 7ιne(×10⅞)
_ ..电
a7a: training result
....⑴
IJcnnIMraInIng result
S885β279
1
2
-I— Ca-ODM
-I- □ PP-OD M
→- DiODM
-¼- aco∙DiODM
—DC-ODM
32	16	8	4	2	1
Cores
_________ (®).. _ ..
DIODM Acceleration Result
Ol ⅛lw
32	16	8	4	2	1
Cores
012345678
Trwing 1ιnβ(×10⅜)
,(9...
COd∙ma: training result
β1ββ85β279
0	1	2	3	4	5
Trwing Iine(XlO2S)
84β2807β76
Ca-ODM
DPP-OQM
* DioDM
-头-aco∙DiODM
-H- DC-ODM
Treirwig 1ιne(×10⅞)
......⑺，..
phishing: training result
9290β88684
Ca-O□M
□ PP-OD M
* DiODM
-÷÷- aco∙DiODM
-H- DC-ODM
O 20	40	60	80	1∞ O 10	20	30	40
TreringIime(S)	TrartngTine(S)
Figure 1: Comparisons of different methods using linear kernel. Each point of acc-DiODM except
in Figure (a) indicates the result when every one third of stages executed. Other points except in
Figure (e) indicate the result stop at different levels.
4.3	Results with rbf kernel
Figure 2 presents the time cost and test accuracy on six data sets. It can be seen that DiODM shows
highly competitive performance compared with other methods. Specifically, DiODM achieves the
best test accuracy on 4 data sets and just slightly worse than DC-ODM on other 2 data sets. On time
cost, DiODM achieves the fastest training speed on all 6 data sets.
4.4	Comparison with Gradient Based Methods
Figure 3 compares the training speed and generalization performance between our acceleration
method and other gradient based methods. We observe that our method can get competitive test
accuracy. Meanwhile, our method achieves over 5 times faster speed than other methods. This
8
Under review as a conference paper at ICLR 2022
(a)
SUSY： training result
- -
75726966
(％)δ1EJn∞v-SSl
O 1	2	3	4	5	6	7	8
Training Time(×10⅛)
.∕dλ ..
cod-ma: training result
...√b,)....
skιn-nonskιn: training result
-------
94918ββ582797673
(％)δ1EJn∞v-səl
O 1	2	3	4	5
Training Time(×102s)
r.® ..
a7a: training result
....O	..
ιjcnn1: training result
918ββ58279
(％)δ1EJn∞v-səl
93908784
(％)δ1EJn∞v-SSl
~∖	I	I	I	I	I
0	1	2	3	4	5	6
Training Time(×10⅞)
β3817977
(％)δ1EJn∞v-Sel
-l	I	I I I	I
O	20	40	60	βθ	1∞	120
Training Time(s)
9290888684
(％)δ1EJn∞v-səl
0	12	3
Training Time(×102s)
.....(Q ∙..
phishing: training result
O 10	20	30	40	50	60
Training Time(s)
Figure 2: Comparisons of different methods using RBF kernel. Each point indicates the result when
stop at different levels.
indicates that our distributed acceleration method achieves great training speed while hold the gen-
eralization performance.
(¾)δ∙BJn∞v - səj.
cod-ma: training result
928986β3
(％)δ1EJn∞v-SSl
20	40	6OeOloO
Training Time(s)
......(d)...
phishing: training result
..
gιsette: training result
97959391
(％)δ1EJn∞v-Sel
∖	I	I I	I	I-
O	30	60	90	120	150
Training Time(s)
IeO
92
89
86
S3
O 20	40	60 βθ
Training Time(s)
....⑼∙∙..
skιn-nonskιn: training result
9087848178
(％)δ1EJn∞v-səl
O 20	40	6OeOloO
Training Time(s)
Figure 3: Comparisons of different gradient based methods.
K)..
a7a: training result
β3817977
(％)δ1EJn∞v-Sel
10 20 30 40 50 60 70 βθ
Training Time(s)
_________ (O.
SUSY: training result
74716β
(％)δ1EJn∞v-səl
O 200	400	600 β∞ 10∞
Training Time(s)
O
O
5 Conclusion
While lots of works have been proposed to solve QP problems, these off-the-shelf solvers usually
ignore the intrinsic structure of the problem, thus can hardly be implemented in ODM. We propose
a distributed ODM solver called DiODM, in order to retain the first- and second- order statistics
in both linear and nonlinear feature space. Additionally, an accelerating method is implemented
to improve the training speed for linear kernel. According to our experiments, DiODM shows the
superiority to other distributed QP solvers and generates great training acceleration in distributed
environment. In the future, we will consider the circumstance in which data is located on different
devices and can not be gathered together due to limited bandwidth or user privacy.
9
Under review as a conference paper at ICLR 2022
References
Stephen Boyd, Neal Parikh, Eric Chu, Borja Peleato, and Jonathan Eckstein. Distributed optimiza-
tion and statistical learning via the alternating direction method of multipliers. Foundations and
Trends in Machine Learning, 3(1):1-122, 2010.
Lijuan Cao, S. S. Keerthi, Chong Jin Ong, Jianqiu Zhang, Uvaraj Periyathamby, Xiuju Fu, and
Henry P. Lee. Parallel sequential minimal optimization for the training of support vector ma-
chines. IEEE Transactions on Neural Networks, 17(4):1039-1049, 2006.
Fanyong Cheng, Jing Zhang, Cuihong Wen, Zhaohua Liu, and Zuoyong Li. Large cost-sensitive
margin distribution machine for imbalanced data classification. Neurocomputing, 224:45-57,
2017.
Pedro A. Forero, Alfonso Cano, and Georgios B. Giannakis. Consensus-based distributed support
vector machines. Journal of Machine Learning Research, 11:1663-1701, 2010.
Wei Gao and Zhi-Hua Zhou. On the doubt about margin explanation of boosting. Artificial Intelli-
gence, 203:1-18, 2013.
Hans P Graf, Eric Cosatto, Leon Bottou, Igor Dourdanovic, and Vladimir Vapnik. Parallel support
vector machines: The cascade svm. In Advances in Neural Information Processing Systems 17,
pp. 521-528, Vancouver, Canada, 2004. MIT Press.
Allan Gr0nlund, Lior Kamma, KasPer Green Larsen, Alexander Mathiasen, and Jelani Nelson.
Margin-based generalization lower bounds for boosted classifiers. In Advances in Neural In-
formation Processing Systems, PP. 11963-11972, Vancouver, Canada, 2019.
Cho-Jui Hsieh, Si Si, and Inderjit S. Dhillon. A divide-and-conquer solver for kernel suPPort vector
machines. In Proceedings of the 31st International Conference on Machine Learning, PP. 566-
574, Beijing, China, 2014.
R. Johnson and T. Zhang. Accelerating stochastic gradient descent using Predictive variance reduc-
tion. In Advances in Neural Information Processing Systems 26, PP. 315-323, Lake Tahoe, NV,
2013.
Jason D. Lee, Qihang Lin, Tengyu Ma, and Tianbao Yang. Distributed stochastic variance reduced
gradient methods by samPling extra data with rePlacement. Journal of Machine Learning Re-
search, 18(122):1-43, 2017.
Gaelle Loosli, Stephane Canu, and Leon Bottou. Training invariant support vector machines using
selective samPling. In Large-Scale Kernel Machines, PP. 301-320, 2007.
Tianxiang Luan, Tingjin Luo, Wenzhang Zhuge, and Chenping Hou. Optimal representative distri-
bution margin machine for multi-instance learning. IEEE Access, 8:74864-74874, 2020.
Angel Navia-Vazquez, D. Gutierrez-Gonzalez, Emilio Parrado-Hernandez, and J. J. Navarro-
Abellan. Distributed support vector machines. IEEE Transactions on Neural Networks, 17(4):
1091-1097, 2006.
Ali Rahimi and Benjamin Recht. Random features for large-scale kernel machines. In Advances in
Neural Information Processing Systems, pp. 1177-1184, Vancouver, Canada, 2007.
Reshma Rastogi, Pritam Anand, , and Suresh Chandra. Large-margin distribution machine-based
regression. Neural Computing and Applications, 32:3633-3648, 2020.
Bernhard ScholkoPf and Alexander J. Smola. Learning with kernels: Support vector machines,
regularization, optimization, and beyond. MIT Press, Cambridge, MA, 2001.
Si Si, Cho-Jui Hsieh, and Inderjit S. Dhillon. Memory efficient kernel approximation. Journal of
Machine Learning Research, 18:1-32, 2017.
Dinesh Singh, Debaditya Roy, and C Krishna Mohan. Dip-svm : Distribution preserving kernel
support vector machine for big data. IEEE Transactions on Big Data, 3(1):79-90, 2017.
10
Under review as a conference paper at ICLR 2022
Zhi-Hao Tan, Teng Zhang, and Wei Wang. Coreset stochastic variance-reduced gradient with appli-
cation to optimal margin distribution machine. In Proceedings of the 33th AAAI Conference on
ArtficiaIInteIligence,pp. 5083-5090, Honolulu, HL 2019.
Zhi-Hao Tan, Peng Tan, Yuan Jiang, and Zhi-Hua Zhou. Multi-label optimal margin distribution
machine. Machine Learning, 109(3):623-642, 2020.
Christopher Williams and Matthias Seeger. Using the nystrθm method to speed up kernel machines.
In Advances in Neural Information Processing Systems, pp. 682-688, Cambridge, MA, 2001.
Hwanjo Yu, Jiong Yang, Jiawei Han, and Xiaolei Li. Making svms scalable to large data sets using
hierarchical cluster indexing. Data Mining and Knowledge Discovery, 11(3):295-321, 2005.
Matei Zaharia, Mosharaf Chowdhury, Tathagata Das, Ankur Dave, Justin Ma, Murphy McCauly,
Michael J. Franklin, Scott Shenker, and Ion Stoica. Resilient distributed datasets: A fault-tolerant
abstraction for in-memory cluster computing. In 9th USENIX Symposium on Networked Systems
Design and Implementation, pp. 15-28, San Jose, CA, 2012.
Teng Zhang and Hai Jin. Optimal margin distribution machine for multi-instance learning. In
Proceedings of the 29th International Joint Conference on Artificial Intelligence, pp. 2383-2389,
2020.
Teng Zhang and Zhi-Hua Zhou. Large margin distribution machine. In Proceedings of the 20th
ACM SIGKDD Conference on Knowledge Discovery and Data Mining, pp. 313-322, New York,
NY, 2014.
Teng Zhang and Zhi-Hua Zhou. Multi-class optimal distribution machine. In Proceedings of the
34th International Conference on Machine Learning, pp. 4063-4071, Sydney, Australia, 2017.
Teng Zhang and Zhi-Hua Zhou. Optimal margin distribution clustering. In Proceedings of the 32nd
AAAI Conference on Artificial Intelligence, pp. 4474-4481, New Orleans, LA, 2018a.
Teng Zhang and Zhi-Hua Zhou. Semi-supervised optimal margin distribution machines. In Pro-
ceedings of the 27th International Joint Conference on Artificial Intelligence, pp. 3104-3110,
Stockholm, Sweden, 2018b.
Teng Zhang and Zhi-Hua Zhou. Optimal margin distribution machine. IEEE Transactions on Knowl-
edge and Data Engineering, 32(6):1143-1156, 2019.
Teng Zhang, Peng Zhao, and Hai Jin. Optimal margin distribution learning in dynamic environ-
ments. In Proceedings of the 34th AAAI Conference on Artificial Intelligence, pp. 6821-6828,
New York, NY, 2020.
Yu-Hang Zhou and Zhi-Hua Zhou. Large margin distribution learning with cost interval and unla-
beled data. IEEE Transactions on Knowledge and Data Engineering, 28(7):1749-1763, 2016.
11
Under review as a conference paper at ICLR 2022
A Appendix
A.1 Preliminaries
Given a labeled data set {(x1, y1), . . . , (xM, yM)}, the primal problem of ODM is
1	2 λ	ξi2 + υi2
min P(H) = -kwk +— E 7i—亦,
w,ξi,i	2	2M i∈[M] (1 - θ)2
s.t. 1 - θ - ξi ≤ yiH>φ(xi) ≤ 1 + θ + i, ∀i ∈ [M].
Denote X = [φ(xι),... ,Φ(xm )], Y = diag(yι,..., yM), ξ = [ξι;...; £m ], e =鸟；...;EM ],the
above primal form can be rewritten as
1	2	λ(kξk2 + υkk2)
mξ,p(W) = 2 IHl +	2M(1-Θ)2	,
s.t. (1 - θ)1M -ξ ≤ YX>w ≤ (1 + θ)1M +,
where 1M is the M -dimensional all one vector.
(6)
With Lagrange multipliers ζ , β ∈ R+M for the two constraints respectively, the Lagrangian of
Eqn. (6) leads to
L = 2 kwk2 + TM"?” - Z >(YX>H -(1-θ)lM + ξ)
+ β> (YX>H - (1 + θ)1M - ),
and the KKT conditions are
H = XY(Z-β), ξ ="1# Z, e = M1卫 β,
λ	λυ
ζi(yiH>φ(xi) - (1 - θ) + ξi) = 0, βi(yiH> φ(xi) - (1 + θ) - Ei) = 0.
(7)
(8)
(9)
Eqn. (8) is derived by setting the partial derivative of L w.r.t. {H, ξ, e} to zero. Eqn. (9) is the
complementary slackness conditions. Observe that yiH> φ(xi) < 1 - θ and yiH> φ(xi) > 1 +
θ cannot hold simultaneously, therefore at least one of the two slack variables ξi and Ei is zero.
According to Eqn. (8), we have ζiβi = 0 for any i ∈ [M].
The following dual problem of ODM follows by substituting Eqn. (8) back into Eqn. (7):
ζ,minMd(Z，β)=l(ζ - β)>Q(ζ - β)+M(UkZk2+kβk2)+(θ - 1)1M Z+(θ+1)1M e，
where Q = YX>XY and C = (1 一 θ)2∕λυ is a constant. By denoting α = [Z; β], above problem
can be rewritten as a standard convex quadratic programming:
min
α∈R2+M
f S) = 1 α>
Q + MCυI
-Q
Q + MCI α +
(θ - 1)1M
(θ+ 1)1M
>
α.
Suppose the instances on the k-th partition are {(x(1k), y1(k)), . . . , (x(mk), ym(k))}, then the dual prob-
lem of ODM on the k-th partition is
min	dk(Zk, βk) = 1(Zk 一 βk)>Q(k)(Zk 一 βk)
ζk ,βk ∈R+m	2
+ ^2^ (UkZk k2 + kβkk2) + (θ - 1)1mZk + (θ + 1)1mβk,
where Qk = YkXk>XkYk, Xk = [φ(x(1k)),...,φ(x(mk))],andYk = diag(y1(k),..., ym(k)). No-
tice that the optimization variables Zk and βk are decoupled on each partition, by merging all the K
problems together, we can get the formulation of DiODM:
min d(Z, β) = 1(Z - β)>Q(Z- β) + mc(UkZk2 + kβk2) + (θ - 1)1MZ +(θ + 1)1Mβ,
ζ,β∈R+M	2	2
1	ʃʌ	1 ∙	/ z-ʌ	Z-X ∖	∙	Flli♦	1	.	∙	J=	Γ J=	Jo T	IC	「C	Cl
where Q	= diag(Q1, . . . , Qk)	is a block diagonal	matrix, Z	= [Z1;	. . . ; Zk],	and β	= [β1;	. . . ; βk].
12
Under review as a conference paper at ICLR 2022
A.2 Proof of Theorem 1
Theorem 1. Suppose the optimal solutions of ODM and DiODM are α? = [ζ? ; β?] and αe? =
[ζ? ; β? ], respectively. The gaps between the optimal objective values and solutions satisfy
0 ≤ d(Z?, β?) - d(Z?, β*) ≤ U2(Q + M(M - m)c),	(10)
U2
kα? - α*k2 ≤ — (Q + M(M - m)c),	(11)
Mcυ
一	一	.... .._ . ..	.. ~. ..	.. ≈. .. .	一一	- ，一 r .
where U = max(kZ*k∞, kβ*k∞, kZ*k∞, kβ?k∞) ,and Q = Pi,j：p (i)=p (j) ∣[Q]ij L
Proof. The left-hand side of Eqn. (10) is due to the optimality of ζ? and β?.
Without loss of generality, suppose the instances {(x1, y1), . . . , (xM, yM)} are ordered by partition
index, i.e., the first m instances are on the first partition, and the second m instances are on the second
partition, etc. According to the definition of d(ζ, β) and d(ζ, β), and by denoting γ = ζ - β, we
have
d(ζ, β) = e(ζ, β) + 1(ζ - β)> (Q - Q )(ζ - β)+(M - m)c (UkZk2 + kβk2)
=e(Z, β) + 2	X	YiYj [Q]ij + (M - m)c (UkZk2 + kβk2).
i,j:P (i)6=P (j)
In particular, the following holds:
d(Z?, β?)	=	e(Z ?, β?)	+1	X	γ? Y?[Q]ij +	(M - m)c (UkZ ?k2 +	kβ*k2),	(12)
i,j:P (i)6=P (j)
d(Z?, β?)	=	d(Ze, β*)	+ 2	X	e? e?[Q]ij +	(MJ (Uke?k2 +	kβ*k2).	(13)
i,j:P (i)6=P (j)
Notice that at least one of Zi and βi is zero, thus ∣γi∣ ≤ ∣Zi∣ + ∣βi∣ ≤ max(∣Zi∣, ∣βi∣) = U. Subtracting
Eqn. (12) from Eqn. (13) yields the right-hand side of Eqn. (10):
1
d(Z*, β?) - d(Z*, β?) ≤ 2 E	(e?e? - Y?Y?)[Q]ij
i,j:P (i)6=P (j)
+ (M - m)c[υ(ke*k2 -kZ*k2) + (kβ*k2 -kβ*k2)]
≤ 2 X	∖eγj-YiYjH[Q]ijI + (M - TmC(Uke*k2 + kβ*k2)
i,j:P (i)6=P (j)
≤ U2Q + U2M(M - m)c,
where the first inequality follows from the optimality ofZ* and β*, and the third inequality is derived
by the boundness of Z, β, γ and U ≤ 1.
Since f(α) is a quadratic function, it can be expanded at α* as
f(α*) = f (α*) + Vf (α*)>(α* - α*) + (α* - α*)>V2f (α*)(α* - α*)
≥ f (α*) + (α* - α*)>V2f (α*)(α* - α*)
= f(α*) + (αe* - α*)> Q+-MQcUI Q+-QMcI (αe*-α*)
≥ f(α*) + (αe* - α*)> McUI McI (αe*-α*)
≥ f(α*)+McUkαe* - α*k2,
where the first inequality follows from the first order optimality condition, and the third inequality
uses the fact U ≤ 1. Thus kαe* - α* k2 can be upper bounded by
1	U2
kα* - α*k2 ≤ —(d(ζ*, β*) - d(ζ*, β*)) ≤ — (Q + M(M - m)c),
McU	McU
which shows that Eqn. (11) holds and concludes the proof.	□
13
Under review as a conference paper at ICLR 2022
A.3 Proof of Theorem 2
Theorem 2. For shift-invariant kernel κ with κ(0) = r2, that is kφ(x)k = r for any x. With the
partition strategy described above, for any k ∈ [K], we have
U2M2c
dk(Zk,βk) — d(Z?,β?) ≤ -ɪ- +2UM + U2M2r2
+ U2r2 cos τ (2Θ - M2)
where θ = Pi,j∈[M],i=j 1w(xi) = w(xj)
Proof. Construct a data set Dk0 based on partition Dk by repeating each instance K times which
appeared in Dk , so that |Dk0 | = |D| = M . According to our settings, the number of instances in
|Dk0 | belong to the s-th stratum equals to |Cs|. Denote Dk0 = {(x0i(k), yi0(k))}i∈[M] and guarantee
that 夕(Xi)=夕(xi(k)). Denote the dual form of ODM on Dk as
U mm in Mdk(Zk, βk) = 2(Zk- βk )>Qk (Zk - βk) + -2-(UkZk 112 + l∣βk 112)
+(θ-1)1>Mζk0 +(θ+1)1>Mβk0,
where Q0k =Yk0X0k>X0kYk0,X0k = [φ(x01(k)),..., φ(x0m(k))], and Yk0 =diag(y10(k),...,ym0 (k)).
Denote the primal form of ODM on Dk0 as
0	1	2 λ	ξi0 + υ0i
min Pk(W) = 5kwk +丽 E -∩~~M,
w,ξi,i k	2	2M i∈[M] (1 - θ)2
s.t. 1-θ-ξi0 ≤ yi0w>φ(x0i) ≤ 1+θ+0i, ∀i∈ [M].
For convenience, let k = 1 so that Dk = {(xi, yi)}i∈[m] . From the primal problem of ODM on Dk,
we have
1	2 λ	ξi2 + υi2	1	2 λ	ξi2 + υi2
Pk(Wk) = 2 kwkk +2m ∑ D2 = 2 kwkk + 2ME KD2
i∈[m]	i∈[m]
=1 kwkk2 + 2M X ξ(i- υ)i =Pk(Wk),
i∈[M]
We can infer that the optimal solution of ODM on Dk and Dk0 are the same. Therefore, d0k(Zk0 , βk0 )
dk (Zk , βk ). According to the definition,
dk(Zk, βk) - d(Z?,β?) =2(Zk - βk)>Qk(Zk - βk) - 1(Z? - β*)>Q(Z? - β?)
+ M (υkZk k2 + kβk k2-υkZ *k2-kβ*k2)
+ (θ - 1)1M (Zk - Z ?) + (θ + 1)1M (βk - β?)
=I X (Ykiγkjy0(k)yj(k)κ(χi(k), XFkh-Yjyiyjκ(xi, Xj))
i,j∈[M]
+ MU (kZkk2-kZ*k2) + M (kβk k2-kβ*k2)
+ (Θ - 1)1M(Zk - Z?) + (θ + 1)1M(βk - β?)
(14)
and -U2 ≤ Y?Y?, Yk iγk j ≤ —2, 0 ≤ U ≤ 1, 0 ≤ θ ≤ 1. Then dk(Zk, βk) - d(ζ?, β?) can be upper
bounded by
d'k(Zk, βk) - d(Z?, β?) ≤ U2 X (K(X户,xj(k))- κ(xi, Xj)) + -2Mc +2UM (15)
i,j∈[M]
14
Under review as a conference paper at ICLR 2022
Notice that κ(xi, Xj) = 1 (kφ(xi)k2 + ∣∣φ(xj)k2 - kφ(xi) - φ(xi)k2). Sequentially,
X (κ(x0i(k), x0j (k)) - κ(xi, xj))
i,j ∈[M]
=1 X (kφ(χi)- φ(χj)k2 -kφ(χi(k)) - φ(χj(k))k2)
i,j∈[M]
=1 X	(kφ(χi) - φ(χj )k2 -kφ(χi(k)) - φ(χj (k))k2)
W(Xi)=W(Xj )
+1 X	(kφ(χi) - φ(χj )k2 -kφ(χi(k)) - φ(χj (k))k2).
W(Xi)=W(Xj)
For the situation where 夕(Xi)= 夕(Xj), the maximal value of ∣∣φ(xi) - φ(xj)∣2 is 4r2, square
of diameter of the ball. The minimal value of ∣φ(xi) - φ(xj)∣2 is 2r2(1 - cos τ) which can
be computed by the law of cosines since the angle between φ(Xi) and φ(Xj) is greater than τ.
According to the law of cosines, we can also upper bound ∣φ(Xi) - φ(Xj)∣2 by 2r2(1 - cosτ)
when 夕(Xi)=夕(Xj). It is obviously that ∣φ(xi) - φ(xj-)∣2 ≥ 0, thus
X (K(Xi(k), Xj(k))-K(Xi, Xj)) ≤ 2 X (4r2 - 2r2(ι - cosT))
i,j ∈[M]	W(Xi)6=W(Xj)	(16)
+ 2 X	2r2(I - CosT).
W(Xi)=W(Xj)
Substitute Eqn. (16) into Eqn. (15), we can derive the bound
U2M2c
dk(Zk, βk) - d(Z*, β?) ≤ U2 X	((2r2 - r2(1 - cosT)) + —2—
W(Xi)6=W(Xj )
+ X r2 (1 - cos T)) + 2UM
W(Xi)=W(Xj)
U2M2c
≤ U2r2Θ(1 + cosτ) + U2r2(M2 — Θ)(1 — cosT) +---
U2M2c
=U2M2r2 + U2r2 cosT(2Θ - M2) + U^— + 2UM
+ 2UM
□
15