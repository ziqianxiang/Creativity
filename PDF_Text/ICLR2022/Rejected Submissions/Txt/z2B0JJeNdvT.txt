Under review as a conference paper at ICLR 2022
Distributed Zeroth-Order Optimization: Con-
vergence Rates that Match Centralized
Counterpart
Anonymous authors
Paper under double-blind review
Ab stract
Zeroth-order optimization has become increasingly important in complex opti-
mization and machine learning when cost functions are impossible to be described
in closed analytical forms. The key idea of zeroth-order optimization lies in the
ability for a learner to build gradient estimates by queries sent to the cost func-
tion, and then traditional gradient descent algorithms can be executed replacing
gradients by the estimates. For optimization over large-scale multi-agent sys-
tems with decentralized data and costs, zeroth-order optimization can continue
to be utilized to develop scalable and distributed algorithms. In this paper, we
aim at understanding the trend in performance transitioning from centralized to
distributed zeroth-order algorithms in terms of convergence rates, and focus on
multi-agent systems with time-varying communication networks. We establish a
series of convergence rates for distributed zeroth-order subgradient algorithms un-
der both one-point and two-point zeroth-order oracles. Apart from the additional
node-to-node communication cost due to the distributed nature of algorithms, the
established rates in convergence are shown to match their centralized counterpart.
We also propose a multi-stage distributed zeroth-order algorithm that better uti-
lizes the learning rates, reduces the computational complexity, and attains even
faster convergence rates for compact decision set.
1	Introduction
Various machine learning tasks ultimately boil down to solving optimization problems of different
forms, where the cost functions are formed jointly by the data accumulated in experiences and
the model used in representing the learning framework. Gradient descent algorithms have been
playing a foundational role in practically solving such optimization problems. However, for learning
tasks with high-dimensional data and involved learning representations, access to the gradient of the
cost function may turn out not possible: the cost function supporting the learning may not have
a closed analytical form; or it is simply too computationally costly to be properly differentiated.
Zeroth-order optimization provides a systemic way of facilitating gradient descent without direct
access to gradient information, where oracles query the cost function values and generate gradient
estimates. Zeroth-order methods have shown a number of successful applications, e.g., searching
for adversarial attacks in deep learning Chen et al. (2019); Liu et al. (2019) and policy search in
reinforcement learning Vemula et al. (2019).
The literature has also explored the potential in extending the standard (centralized) zeroth-order
optimization to distributed settings over multi-agent systems, where the data and cost functions are
scattered across a network of decentralized agents. With the help of a communication network,
the agents may collaboratively solve the network-level optimization task by iteratively exchanging
decisions obtained from local zeroth-order descent. The rates of convergence of centralized zeroth-
order optimization algorithms are now well understood for several sub-classes of convex functions.
We are interested in systematically investigating these convergence rates scale for the corresponding
distributed algorithms, and focus on the case of time-varying communication networks.
1
Under review as a conference paper at ICLR 2022
1.1	Problem Definition
Consider a network of agents (nodes) V =
following distributed optimization problem
minimize
sub ject to
{1, . . . , N }. The agents aim to collectively solve the
N
f(x):= P fi(x)
i=1
x∈ X.
(1)
Here x ∈ Rd is the decision variable, X ⊆ Rd is a convex decision space, and fi : Rd → R is a
private convex objective function associated with agent i.
The communication network connecting the nodes is described by a time-varying graph G(t) =
(V, E(t)), where E(t) is the set of activated links at time t. Let A(t) be a weight matrix at time t
for the graph G(t): for each link (i, j) ∈ E(t), a weight [A(t)]ij > 0 is assigned, and [A(t)]ij = 0
for (i, j) ∈/ E(t). We impose the following assumption on the communication network E(t) and the
weight matrix A(t).
Assumption 1 (i) There exists a positive integer B such that the union graph (V, E(kB +1) ∪∙∙∙∪
E((k+1)B)) is strongly connected for all k ≥ 0; (ii) A(t) is doubly stochastic, i.e., PiN=1 [A(t)]ij =
1 and PjN=1 [A(t)]ij = 1; (iii) [A(t)]ii ≥ ξ for all i, and [A(t)]ij ≥ ξ if (j, i) ∈ E(t), where ξ > 0.
1.2	Function Classes
Let Fcvx denote the set of all convex functions on Rd . We define the following three classes of
convex functions in Fcvx .
•	The Lipschitz continuous class Flip (Lf, X) contains the functions in Fcvx that admit a finite
Lipschitz constant Lf over X, i.e.,
Flip (Lf, X) := {g ∈ Fcvx : ∀x, x0 ∈ X, |g(x) - g(x0)| ≤ Lfkx - x0k}.
•	The smooth class Fsmo (sf, X) contains the functions that admit a sf -Lipschitz continuous
gradient over X, i.e.,
Fsmo(Sf, X) = {g ∈ Fcvx : ∀x, x0 ∈ X, INg(X)- Vg(x0)k ≤ Sf IlX - x0k}.
•	The strongly convex class Fsc(μf, X) contains the functions that are μf -strongly convex,
i.e.,
Fsc(μf,X) = {g ∈ Fcvx : ∀x, X0 ∈ X,g(x) ≥ g(x0) + hVg(x0), X — x0)+ μ2f ∣∣x — x0∣2}.
1.3	Contributions and Related Work
Contributions. We first present MAZOPA, a multi-agent zeroth-order projection averaging algo-
rithm. In MAZOPA, the agents iteratively carry out local zeroth-order descents for their private
costs to generate intermediate decisions, send these intermediate decisions to their neighbors over
the graph G(t), and then update their decisions by projecting the average neighboring intermediate
decisions onto X. For distributed zeroth-order oracles based on one-point or two-point estimates, a
series of convergence rate results are established for the three basic function classes. Remarkably,
the convergence rates for distributed algorithms are found to be matching their centralized coun-
terpart, and sometimes even tighter rates are obtained, as summarized in Table 1. These results
show that by paying the price of node-to-node communication, distributed zeroth-order optimiza-
tion provides equal performance guarantees as those of centralized approaches. Next, we generalize
the MAZOPA to a multi-stage setting, where the local zeroth-order descents take place for multiple
steps before the projected averaging in a sequence of epochs. Such multi-stage MAZOPA is shown
to be able to reduce the computational complexity, while providing improved convergences rates
compared to MAZOPA when the decision set is compact.
2
Under review as a conference paper at ICLR 2022
Table 1: Convergence rates established for MAZOPA
ZOO	Lipschitz Class [Centralized Counterpart]	Strongly Convex Class [Centralized Counterpart]
One-point	Flip:O(t⅜4)[O(t⅜4) Flaxmanetal.(2005)]	FliP ∩ Fsc:O(T23) [ 0(^2TT^) AgarWal et al.(2010)]
		FliP ∩ Fsmo:OlTd3 j	FliP ∩ Fsmo ∩Fsc:O(√T )[O (djnTL) AgarWal et al.(2010)]
Two-point	Flip: OIqT) B/) ShamH (2017)]	Flip ∩ Fsc：O(djnTT))[O(d2TT)) Agarwal etal. (2010)]
	Zip ∩ Fsmo: O (J T )	FliP ∩ Fsmo ∩ Fsc： O(djnTT)) [O(d2lT(T1) AgarWaletal.(2010)]
Related Work. Recently, many types of centralized zeroth-order optimization algorithms have been
studied, and their convergence rates (and the way they depend on the dimension) have been estab-
lished in different settings. For unconstrained convex optimization, Nesterov & Spokoiny (2017)
develops several types of two-point gradient estimators and achieves convergence rates that scale
with dimension as O(d2). For constrained stochastic optimization, Duchi et al. (2015) establishes
that the convergence rates are sharp up to factors at most logarithmic in the dimension. Zeroth-order
optimization has a natural connection to bandit online optimization, where the latter focuses on dy-
namic environment where the objective functions are varying over time (see, e.g., Flaxman et al.
(2005); Agarwal et al. (2010); Shamir (2013; 2017); Bubeck et al. (2017); Lattimore (2020); Hazan
& Levy (2014)). In particular, the seminal work Flaxman et al. (2005) constructs a one-point gradi-
ent estimator (or one-point bandit feedback model) and achieves an O(d/T 1/4) average regret. For
two-point gradient estimator, Shamir (2017) establishes the tightness of the dimension-dependent
factor O(√d) in the framework of zeroth-order stochastic mirror descent.
It is worth zooming into the literature on distributed zeroth-order/bandit online optimization. Due
to the absence of a central coordinator, the algorithms developed should always rely on local com-
putations and communications (e.g., Yuan & Ho (2015); Yi et al. (2020); Jakovetic et al. (2018);
Hajinezhad et al. (2019); Wang et al. (2019); Pang & Hu (2019); Hajinezhad & Zavlanos (2018);
Wan et al. (2020)). This makes the convergence analysis of the distributed zeroth-order/bandit on-
line optimization algorithms more challenging. In Yuan & Ho (2015), the authors develop a class
of distributed zeroth-order optimization algorithms that require two functional evaluations at each
iteration, and establishes asymptotic convergence of the algorithm. Non-asymptotic convergence
is established in Jakovetic et al. (2018); Hajinezhad et al. (2019); Wang et al. (2019); Pang & Hu
(2019); Hajinezhad & Zavlanos (2018), but the dimension-dependence factors are either O(d2) or
far from optimal. The work Yi et al. (2020) considers distributed online optimization with long-term
constraints and establishes bounds on regret as well as constraint violations. To avoid Euclidean
projection onto the constraint set, Wan et al. (2020) develops a distributed bandit online optimiza-
tion algorithm based on conditional gradient descent and one-point bandit feedback, and achieves a
regret scaling of O(T3/4Sn T).
2	THE MAZOPA ALGORITHM AND ITS CONVERGENCE RATES
In this section, we present the MAZOPA algorithm and establish the convergence rates for the three
function classes.
2.1	Distributed Zeroth-Order Oracles
Let n be a random vector in Rd drawn from some probability distribution. Then
ʌ , - , . - ___________________________________________
fi(x； δ) := En [fi(x + δn)]	(2)
is a smoothed function for fi . Here δ > 0 is a parameter setting the level of the smoothing. We
introduce the following definition on distributed zeroth-order oracles (DistZOO).
Definition 1 (DistZOO) A vector gi(x; δ) ∈ Rd is called a distributed zeroth-order oracle at node
i if the following conditions hold:
(i)	E[gi(x; δ)] = Nfi (x; δ) forall X ∈ Rd;
(ii)	If fi ∈ FliP(Lf) ,then fi ∈ FliP(Lf) as well, and there holds Ifi (x; δ) 一 fi (x) ∣ ≤ PdLf δ,
with pd being some positive constant;
3
Under review as a conference paper at ICLR 2022
(iii)	If fi ∈ Fsmo(Sf), then ∣fi(x; δ) 一 fi(x) ∣ ≤ 1 Pdsf δ2 with Pd being some positive constant.
A number of DistZOO satisfying Definition 1 can be obtained using existing gradient estimators,
see, e.g., Liu et al. (2020). In the paper, we provide two representative gradient estimators that are
commonly adopted in the literature. Let ui be a random vector independently generated from a unit
sphere B1 in Rd. Then (e.g., Flaxman et al. (2005))
gOP(x; δ) := fi(x + δtUi)Uid∕δ	⑶
is a one-point DistZOO satisfying Definition 1. Moreover,
gTP(x; δ) ：= 2δ (fi(x + δu) 一 fi(x - δu))u	⑷
is a two-point DistZOO satisfying Definition 1 (e.g., Shamir (2017)).
2.2	THE MAZOPA ALGORITHM
We present the following Multi-Agent Zeroth-Order Projection Averaging (MAZOPA) algorithm,
which consists of two steps, a local zeroth-order optimization step and a distributed averaging step.
MAZOPA, whose pseudo-code is presented in Algorithm 1, is a variation of the multi-agent sub-
gradient averaging algorithm proposed in Nedic et al. (2008); Nedic & Ozdaglar (2009); Nedic et al.
(2010), where the local optimization step is executed by sub-gradient descent.
Algorithm 1 MAZOPA: Xi(T) = MAZOPA (Xi(1), ηt, δt, X)
Require: step size ηt, DistZOO gi(x; δt) with exploration parameter δt for all i ∈ V
Ensure: xi(1) ∈ X, ∀i ∈ V
1:	for t = 1 to T do
2:	Node i queries the DistZOO at point xi(t) and receives gi(xi(t); δt)
3:	Node i computes
vi(t) = xi(t) 一 ηt ∙ gi(xi(t); δt)
4:	Node i updates its state by using the information received from its instant neighbors
xi(t + 1) =projXX[A(t)]ijvj(t)
j=1
5:	end for
Output： Xi(T) = T PT=IXi(t * (i) (ii) * * * *
2.3 Main Results
Let Xi(T) be the output of Algorithm 1 at agent i. We denote the optimal solution of problem (1)
by x? = argminχ∈χ f (x). Defining X° := {x + U : X ∈ X, U ∈ Bι}, We present the following
results on the convergence rate of the MAZOPA algorithm.
Theorem 1 Let Assumption 1 hold. Let DistZOO take the form of gOP(∙). Further assume that
|fi(xi(t) + δt Ui (t))| ≤ Cfor all i ∈ V. We have the following convergence results for every i ∈ V
and all T ≥ 1.
(i) Consider f ∈ FliP(Lf, X°) for all i ∈ V. Setting η = dT3∕4 and δt = tι14, t = 1,...,T,
it holds that E[f (Xi(T))] — f (x?) = O(TdJ).
(ii) Consider	fi	∈	FliP(Lf,X°) ∩	Fsmo(sf,X°).	Setting η = dT2∕3	and	δt	= 6,t =
1,...,T, it holds that E[f (Xi(T))] — f (x?) = O(Td3).
Theorem 2 Let Assumption 1 hold. Let DistZOO take the form of gτp(∙). Set η = √= and
δt = √t, t = 1,...,T. Consider f ∈ FliP(Lf, X°),i ∈ V. Then, for every i ∈ V and all T ≥ 1, we
have E[f(Xi(T))] — f (x?) = O(∕d).
4
Under review as a conference paper at ICLR 2022
With strong convexity, the convergence rates established above can be further strengthened.
Theorem 3 Let Assumption 1 hold. Let DistZOO take the form of gOP(∙). Further assume that
|fi(xi(t) + δt ui (t))| ≤ Cfor all i ∈ V. We have the following convergence results for every i ∈ V
and all T ≥ 1.
(i)	Consider f ∈ FliP(Lf, X°) ∩ Fsc(μf, X°) for all i ∈ V. Setting ηt = 志 and δt = t1/3
t = 1,...,T ,it holds that E[f (Xi(T))] 一 f (x?) = O( T23).
(ii)	ConSiderfi ∈ FliP(Lf, X°) ∩ Fsmo(Sf ,X°) ∩ Fsc(μf, X°). Setting η =册 and δt = t1^
t = 1,...,T, it holds that E[f (Xi(T))] — f (x?) = O(专).
Theorem 4 LetAssumption 1 hold. Let DistZOO take theform of gτp(∙). Set η =焉 and δt = 1,
t = 1,...,T. Consider f ∈ FliP(Lf, X°)∩Fsc(μ/, X°),i ∈ V. Then,forevery i ∈ V andallT ≥ 1,
we have E[f (Xi(T))] — f (x?) = O(dlnTT)).
3	MULTISTAGE MAZOPA: ADAPTIVE LOCAL DESCENT
We now propose a multi-stage variant of Algorithm 1. We impose the following compactness as-
sumption on the constraint set X.
Assumption 2 There exists 0 < RX < ∞ such that kxk ≤ RX for all x ∈ X.
3.1	The Algorithm
The basic idea is to divide the optimization process into a sequence of epochs, each of which has an
exponentially decreasing step size and an exponentially increasing iteration number. The updates in
the inner loop of each stage are just made according to Algorithm 1 with fixed step size. In each
stage only the average point is maintained and used as the starting point of the next stage. This idea
of setting up multi-stage optimization algorithms was originally explored in Hazan & Kale (2011).
Take positive integers m ≥ 1 and a ≥ 2. Let k = [log。(ɪ + 1)_|, where [x[ represents the largest
inter with value no greater than x ∈ R. We divide the T time steps into k\ epochs by
Epoch 1 : 1, . . . , T(1);
Epoch 2 : T(1)+	1, . . . , T(2);
Epoch k\ : T(k\-1)+	1, .. .,T(k\).
Here T(1) = m, T(2) = am, . . . , T(k\) = ak\ m. For the jth epoch, all agents will run the MAZOPA
algorithm, and denote output of the j-th epoch at agent i by Xj) (T(j)). The pseudo-code of the
resulting multi-stage MAZOPA is presented in Algorithm 2.
Compared to the MAZOPA algorithm, the multi-stage MAZOPA has the following advantages:
(i)	Multistage MAZOPA only requires each node projects its estimates onto the ball BRX,
rather than the constraint set X in each epoch. In particular, multistage MAZOPA algorithm
significantly reduces the number of Euclidean projections onto the constraint set X from T
to k\. This makes the algorithm more computationally efficient.
(ii)	Multistage MAZOPA better utilizes the step size rules, in the sense that at earlier epochs
of the algorithm, larger step sizes are adopted to facilitate convergence, while smaller step
sizes are adopted to achieve better accuracy at later epochs.
3.2	Optimal Convergence Rates
We now modify the definitions of FliP, Fsmo and Fsc by replacing X with BRX , with slight abuse of
notation. As it turns out, the multistage MAZOPA enjoys refined convergence rates.
5
Under review as a conference paper at ICLR 2022
Algorithm 2 Multistage MAZOPA
Require: exploration parameter δ(1), step size η(1), T(1) = m, total number of iterations T, integer
a ≥ 2, and scalar b > 1
Ensure: xi(1) (1) ∈ X for all i ∈ V, and set k = 1
1:	while j = 1, . . . , k\ do
2:	Call Algorithm 1 to obtain
Xij)(Tej)) = MAZOPA (Xj)⑴,η⑺,δ⑶,BRX)
3:	Compute xej+1)(1) = PrOjX(X Iijj(T(j)))
4:	Update ηek+1) = ɪn(k) and δej+1) = bδij)
5:	Update T(j+1) = aT(j)
6:	Update j = j + 1
7:	end while
Output： Xi(T) = PrOjX(Xik"(T(加)))
Theorem 5 Let Assumptions 1 and 2 hold. Let DistZOO take the form of gτp(∙). Set a = b,
T⑴=m = 1, η(D = 34a and δe1) = 1. Consider f∣ ∈ FliP(Lf, BRX) ∩ Fsc(μf, BRX), i ∈ V. We
have Ehmax∣∈v {kXi(T) - x*∣∣2}] = O(τ+ι).
The idea of the analysis leading to Theorem 5 can also be extended to one-point oracles. If DistZOO
takes the form of gop(∙), one needs to impose the following assumption on the objective functions,
that is, ∣fi(Xi(t) + δtui(t))∣ ≤ C forall i ∈ V. FOrfi ∈ FliP(Lf,BRX) ∩ Fsc(μfBRX), set-
ting a = b3, the final estimates enjoy a convergence rate of E[maxi∈v {∣∣Xi(T) - x*∣∣2}] =
O((T +1)1/3). For fi ∈ FliP(Lf,BRX) ∩ FsmO(Sf,BRX) ∩ Fsc(μf, BRX), setting a = b4 * * * * * 10, there holds
E[maχi∈v {kXi(T) - X?k2}] = O(√T2+1).
4 Numerical Examples
In this section, we evaluate the performance of the proposed algorithms on a distributed ridge re-
gression problem.
System setup. The optimization problem has the following form:
N
minimize	f (x) = P (ɪ(aTX - bi)2 + PkXk2)
i=1
sub ject to	kXk1 ≤ k
(5)
where X ∈ Rd is the optimization variable, the data pair (ai , bi ) ∈ Rd × R is only known to node i
with ai and bi being generated uniformly from the unit normal distribution.
Network setup. We implement the proposed algorithms over a randomly generated network that
consists of N = 50 nodes, which is shown in Fig. 1. In the simulations, we set d = 10, k = 3/4,
ρ = 1/2, and RW = 3/4. We evaluate the performance of the algorithms via the average of
10 implementations. The weight matrix associated with the graph is generated according to the
maximum-degree weights:
[A(t)]ij
1
1 + dmax ,,
1 - Trrdi-
1+dm
ax
0,
(j, i) ∈ Et
i=j
(j, i) ∈/ Et
where dmax = maχi∈V{di} is the maximum degree of Gt (di denotes the degree of node i).
Results. The performance of algorithms MAZOPA and multistage MAZOPA is illustrated via plot-
ting the maximum function errors, maxi∈v f (Xi(T)) and maxi∈v f (Xi(T)), as a function of the
number of iterations T in Fig. 2. As a benchmark, the convergence performance of the gradient
6
Under review as a conference paper at ICLR 2022
Fig. 1. A random network of 50 nodes.
algorithm is displayed in Fig. 2 as well. From the numerical results it is clear that the maximum
function errors are vanishing for all zerroth-order algorithms. In fact, the convergence performance
of two-point MAZOPA is even comparable to the gradient method. Moreover, the multistage vari-
ants in general exhibit better convergence performance, and this is more obvious for the case of
two-point MAZOPA. These numerical results are in compliance with the theoretical findings in the
paper.
JOL山 uo⅞UnLLUJnUJXEIΛI
Fig. 2. Convergence performance of the algorithms.
Reproduction of the results. The code used for producing this numerical example is provided in
the suplementary material.
5 Conclusions
We have established a series of convergence rates for distributed zeroth-order subgradient algorithms
that match their centralized counterpart for Lipschitz, smooth, and strongly convex function classes.
These results provided the theoretical benchmarks for zeroth-order approaches over complex dy-
namic networks. We also proposed a multi-stage variant of the algorithm that better utilizes the
learning rates and attains even improved convergence rates. In future work, it is worth exploring
the connection between the convergence rates and the underlying communication complexity for
distributed zeroth-order algorithms.
7
Under review as a conference paper at ICLR 2022
References
Alekh Agarwal, Ofer Dekel, and Lin Xiao. Optimal algorithms for online convex optimization with
multi-point bandit feedback. In COLT,, pp. 28-40. Citeseer, 2010.
Sebastien Bubeck, Yin Tat Lee, and Ronen Eldan. Kernel-based methods for bandit convex opti-
mization. In Proceedings of the 49th Annual ACM SIGACT Symposium on Theory of Computing,
pp. 72-85, 2017.
Xiangyi Chen, Sijia Liu, Kaidi Xu, Xingguo Li, Xue Lin, Mingyi Hong, and David Cox. Zo-
adamm: Zeroth-order adaptive momentum method for black-box optimization. arXiv preprint
arXiv:1910.06513, 2019.
John C Duchi, Michael I Jordan, Martin J Wainwright, and Andre Wibisono. Optimal rates for
zero-order convex optimization: The power of two function evaluations. IEEE Transactions on
Information Theory, 61(5):2788-2806, 2015.
Abraham D Flaxman, Adam Tauman Kalai, Adam Tauman Kalai, and H Brendan McMahan. Online
convex optimization in the bandit setting: gradient descent without a gradient. In Proceedings of
the sixteenth annual ACM-SIAM symposium on Discrete algorithms, pp. 385-394. Society for
Industrial and Applied Mathematics, 2005.
Davood Hajinezhad and Michael M Zavlanos. Gradient-free multi-agent nonconvex nonsmooth
optimization. In 2018 IEEE Conference on Decision and Control (CDC), pp. 4939-4944. IEEE,
2018.
Davood Hajinezhad, Mingyi Hong, and Alfredo Garcia. Zone: Zeroth order nonconvex multi-agent
optimization over networks. IEEE Transactions on Automatic Control, 2019.
Elad Hazan and Satyen Kale. Beyond the regret minimization barrier: an optimal algorithm for
stochastic strongly-convex optimization. In Proceedings of the 24th Annual Conference on Learn-
ing Theory, pp. 421-436. JMLR Workshop and Conference Proceedings, 2011.
Elad Hazan and Kfir Levy. Bandit convex optimization: Towards tight bounds. In Advances in
Neural Information Processing Systems, pp. 784-792, 2014.
Dusan Jakovetic, Dragana Bajovic, Anit Kumar Sahu, and Soummya Kar. Convergence rates for
distributed stochastic optimization over random networks. In 2018 IEEE Conference on Decision
and Control (CDC), pp. 4238-4245. IEEE, 2018.
Tor Lattimore. Improved regret for zeroth-order adversarial bandit convex optimisation. Mathemat-
ical Statistics and Learning, 2(3):311-334, 2020.
Sijia Liu, Pin-Yu Chen, Xiangyi Chen, and Mingyi Hong. signsgd via zeroth-order oracle. In
International Conference on Learning Representations, 2019.
Sijia Liu, Pin-Yu Chen, Bhavya Kailkhura, Gaoyuan Zhang, Alfred O Hero III, and Pramod K
Varshney. A primer on zeroth-order optimization in signal processing and machine learning:
Principals, recent advances, and applications. IEEE Signal Processing Magazine, 37(5):43-54,
2020.
Angelia Nedic and Asuman Ozdaglar. Distributed subgradient methods for multi-agent optimiza-
tion. IEEE Transactions on Automatic Control, 54(1):48, 2009.
Angelia Nedic, Alex Olshevsky, Asuman Ozdaglar, and John N Tsitsiklis. Distributed subgradient
methods and quantization effects. In 2008 47th IEEE Conference on Decision and Control, pp.
4177-4184. IEEE, 2008.
Angelia Nedic, Asuman Ozdaglar, and Pablo A Parrilo. Constrained consensus and optimization in
multi-agent networks. IEEE Transactions on Automatic Control, 55(4):922-938, 2010.
Yurii Nesterov and Vladimir Spokoiny. Random gradient-free minimization of convex functions.
Foundations of Computational Mathematics, 17(2):527-566, 2017.
8
Under review as a conference paper at ICLR 2022
Yipeng Pang and Guoqiang Hu. Randomized gradient-free distributed optimization methods for a
multi-agent system with unknown cost function. IEEE Transactions on Automatic Control, 2019.
Ohad Shamir. On the complexity of bandit and derivative-free stochastic convex optimization. In
Conference on Learning Theory, pp. 3-24. PMLR, 2013.
Ohad Shamir. An optimal algorithm for bandit and zero-order convex optimization with two-point
feedback. Journal of Machine Learning Research, 18(52):1-11, 2017.
Anirudh Vemula, Wen Sun, and J Bagnell. Contrasting exploration in parameter and action space:
A zeroth-order optimization perspective. In The 22nd International Conference on Artificial In-
telligence and Statistics, pp. 2926-2935. PMLR, 2019.
Yuanyu Wan, Wei-Wei Tu, and Lijun Zhang. Projection-free distributed online convex optimization
with o(√T) communication complexity. In International Conference on Machine Learning, pp.
9818-9828. PMLR, 2020.
Yinghui Wang, Wenxiao Zhao, Yiguang Hong, and Mohsen Zamani. Distributed subgradient-free
stochastic optimization algorithm for nonsmooth convex functions over time-varying networks.
SIAM Journal on Control and Optimization, 57(4):2821-2842, 2019.
Xinlei Yi, Xiuxian Li, Tao Yang, Lihua Xie, Tianyou Chai, and Karl H Johansson. Distributed bandit
online convex optimization with time-varying coupled inequality constraints. IEEE Transactions
on Automatic Control, 2020.
Deming Yuan and Daniel WC Ho. Randomized gradient-free method for multiagent optimization
over time-varying networks. IEEE Transactions on Neural Networks and Learning Systems, 26
(6):1342-1347, 2015.
9
Under review as a conference paper at ICLR 2022
A Key lemmas
We first establish the basic convergence result for Algorithm 1 that is based on DistZOO gi(X; δt),
which plays a crucial role in subsequent analyses. We will sometimes use i∙ to denote a node in V
just to highlight the focus on a given node (but i indeed may take any value in V and therefore it is
a generic node).
Lemma 1 LetAssumption 1 hold. Let gi(Xi(t); δt) be a DistZOO that satisfies Definition 1 (i) and
(ii). Then, for any i* ∈ V and all T ≥ 1, there holds
T	TN
X(E[f (Xi∙ (t))] — f(X*)) ≤ XX E[∣fi(X?) - fi(X*; δt)∣]
t=1	t=1 i=1
TN
+ XXE[∣fi(Xi∙(t)) - fi(Xi∙(t); δt)∣]
t=1 i=1
T
+X
t=1
E [Λ*(t)] - E[Λ* (t + 1)]
2nt
+ p1 Lf
TN
+ 2 Xnt XE[ kgi(Xi(t); δt)k2 ]
t=1 i=1
T-1	N
+ P2Lf X nt XE[ kgi(Xi(t); δt)k]
t=1	i=1
where Pi = 2NmaXi∈v{∣∣Xavg(1)-Xi(I)k} + 2N-ββ (PN=IkXi(1)k), P2
and Λ*(t) = PN=I ∣∣Xi(t) - x*k2 With Xavg⑴=N PN=1 Xi(1), α =
β = (1 — 4NN2 Y/)
Before presenting the proof of Lemma 1, we provide the following two supporting lemmas. The
first lemma characterizes the convergence property of the transition matrix induced by weight matrix
A(t) (see Nedic et al. (2008)).
Lemma 2 Define the transition matrix as A(t : ') = A(t)A(t — 1)…A(' + 1)A(') for all
t ≥ ` ≥ 1, and write A(t : t) = A(t). A(t : `) satisfies
[A(t : ')]ij - N ≤ αβt-'+1
where α
1/B
The second lemma establishes the accumulated disagreement for every node in the network.
Lemma 3 (Disagreement) Let Assumption 1 hold. For every node i ∈ V, we have
X kXavg(t) - Xi(t)k ≤ kXavg ⑴-Xi(1)k + γ-ββ kXi(1)k
T-1	N
+ ɪ +4 EntE kgi(Xi(t);δt)k
1 - β	t=1	i=1
where Xavg (t) = N PN=I Xi(t).
Proof. To simplify the presentation, we denote
N
Vi ⑴=X[A⑴]ijVj ⑴
j=1
Si(t) = projχ (Vi(t)) — Vi(t).
10
Under review as a conference paper at ICLR 2022
Step 3 in Algorithm 1 can be rewritten as
Xi(t + 1)= Vi(t) + Si(t).
Our analysis relies on the estimate of ksi(t)k, which can be bounded as follows:
NN
Ilsi⑴k ≤ IIprOjX (Vi(U)- XA⑴]jχj(t)∣∣ + X[A⑴]j kηtgj(Xj⑴；δt)k
j=1	j=1
where the inequality is based on Step 3 in Algorithm 1 and A(t) is double stochastic (cf. As-
sumption 1). Using the non-expansiveness of the Euclidean projection projχ(∙) and the fact that
PjN=1 [A(t)]ij Xj (t) ∈ X, we have
N
kSi(t)k ≤ 2X[A(t)]ij kηtgj(Xj(t); δt)k.	(6)
j=1
We now derive the general expressions for Xavg(t + 1) and Xi(t + 1), respectively. For Xavg(t + 1),
we have
1N	1N
Xavg (t + 1) = xavg(t) - Nfntgi(Xi9； δt) + N ESi (t).
i=1	i=1
Applying the preceding inequality recursively, we get
tN	tN
xavg (t + 1) = xavg (I)- X N X n`gi (xi('); δ') + X N X si(').	G)
Similarly, for Xi(t + 1), we have
N	t N	t-1 N
xi(t +1) = X[A(t: 1)]ijxj(I)- XX[A(t: ')]ijn`gj(Xj('); δ') + XX[A(t:' + 1)]ijsj(`) + si(t).
j=1	'=1j=1	'=1j=1
(8)
Combining (7) and (8), gives
N t N
IlXavg(t + I)- xi(t + 1)k ≤ X	[A(t	:	1)]ij - N	kxj ⑴k+ XX	[A(t	: ')]ij	- N	n'kgj	(Xj (');	δ')k
j=1	'=1j=1
t-1 N
+ XX [A(t : ' + 1)]ij - N
Combining the results in (6), (9) and Lemma 2, leads to
1
ksj (')k + kSi(t)k + N EkSi(t)k.
i=1
(9)
Ixavg(t+ 1) -xi(t+ 1)I ≤ αβt XIxi(1)I
tN	N
+ 3α X βt-'n'X kgi (Xi('); δ')k +4ntX kgi(Xi(t); δt)k
where we used the following relation, based on (6):
N	N	NN	N
kSi(t)k +	N X	kSi(t)k ≤ 2X	kSi(t)k	≤ 4 XX[A(t)]ij	kntgj(Xj(t);	δt)k	≤ 4nt X	kgi(Xi(t);	δt)k.
i=1	i=1	i=1 j=1	i=1
This implies that
T	N	T-1
XIXavg(t) -Xi(t)I ≤ IXavg(1) -Xi(1)I +α XIXi(1)I	Xβt
t=1	i=1	t=1
T-1 t	N	T-1	N
+ 3ɑ XX βt-'n' X kgi (Xi('); δ')k +4 X nt X kgi(Xi (t); δt)k.
t=1 '=1	i=1	t=1	i=1
(10)
11
Under review as a conference paper at ICLR 2022
This, in combination with (10), leads to the final bound.
[Proof of Lemma 1]. Denote
N
Λ(t) = X kxi(t) - xk2,	∀x ∈ X,t ≥ 1.	(11)
i=1
We follow the standard analysis by deriving the general evolution of ∆(t),
N
Λ(t+1)=X
i=1
ProjX (χ[A⑴]ijVj ⑴
- x ≤ X kvi(t) - xk2
i=1
(12)
where the inequality follows from the non-expansiveness of the Euclidean projection and the con-
vexity of norm square function. Expanding the term further gives
NN
Λ(t + 1) = Λ(t) + X kηtgi(xi(t); δt)k2 - 2ηt X hgi(xi(t); δt), Xi(t) - Xi	(13)
i=1	i=1
Taking the expectation on both sides and using the following property of DistZOO (cf. Definition
1(i)):
E [gi(Xi(t); δt)] = Vfi(Xi(t); δt)
we further obtain
NN
E [Λ(t +1)] = E [Λ(t)]+ n XE[ kgi(xi(t); δt)k2 ] - 2% X (E[fi(xi(t);屏)]-f (x;屏))
i=1	i=1
(14)
which implies
TN	T	T N
XXE[fiX(t);δt)] - f(x;δt) ≤ X E[( )] -2 [( +1)] + 2XηtXE[kgi(xi(t);δt)k2].
t=1 i=1	t=1	2ηt	2 t=1	i=1
(15)
We turn our attention to the left-hand Side of (15). By adding and subtracting the term f (x* (t); δt)
and using the Lipschitz continuity of fi (cf. Definition 1(ii)), it follows that
TN	T	TN
XX£(Xi⑴;St) ≥ X/(Xi∙⑴;St)- Lf XX kxi(t)- xi∙⑴k
which, together with (15), yields
X E[∕X∙(t); δt)] - f(x; δt) ≤ X E[A(t)] -2E Mt +1)]
t=1	t=1	2ηt
t=1	t=1	(16)
T N	TN
+ 2 Xηt XE[kgi(Xi(t);δt)k2] + Lf XX kXi(t) - Xi∙(t)k.
t=1	i=1	t=1 i=1
The desired result follows by relating the left-hand side to the original function f, using the dis-
agreement estimate in Lemma 3, and setting x = x? .
B Proofs of Theorems 1 and 2
We first provide the the following lemma that characterizes the properties of the DistZOOs in (3)
and (4). Its proof can be derived by resorting to Flaxman et al. (2005); Shamir (2017), which we
omit here to save space.
Lemma 4 Suppose that fi ∈ Flip (Lf) for all i ∈ V. We have the following.
12
Under review as a conference paper at ICLR 2022
(i)	For gOP(∙) ,there hold Pd = 1 and
E[kgOP(Xi(t);δt)k2] ≤ (Cd J
where C = maxi∈v ∣fi(Xi(t) + δtUi(t))∣ with Xi(t) ∈ X and ui(t) uniformly drawnfrom
Bi. We have Pd = 1 when fi ∈ Fsmo(sf).
(ii)	For gτp(∙), there holdPd = 1 and
E[kgTP(Xi(t)；δt)k2] ≤ cLfd
where C is some universal Constant In addition, Pd = 1 when fi ∈ Fsmo(sf).
Now we are ready to prove Theorems 1 and 2.
(i) First, using the property of the DistZOO gop(∙) (cf. Definition 1(iii)), it follows that
N
X Ifi(Xi∙(t)) - fi(Xi∙ (t); δt)∣ ≤ NLf δt
i=1
N
X∣fi(X?) - fi(X*; δt)∣ ≤ NLf δt
i=1
(17)
We now focus on the case of f ∈ FliP(Lf, X°). Combining with inequality (17), the results in
Lemmas 4(i) and 1, gives
T	T1
X (E[f (Xi∙ (t))] - f (X?)) ≤ p1Lf + 2NLf X δt + 2— E [λ?⑴]
t=1	t=1	2ηt
TT
+ 2 NC 2d2 X δt+ p2NLf Cd X δt
where we used the fact that ηt is a function of T and Jensen’s inequality, i.e.,
E[∣∣gOP(Xi(t); δt)∣∣] ≤ (E[∣∣g0P(Xi(t); δt)∣∣2])1/2 ≤ Cd.
(18)
Substituting the explicit expressions of η = dT1/ and δt
by T, we find that
tl∕4 into (18) and dividing both sides
1T
T E(E[f(xi∙ (t))] - f(x*)) = O
(19)
t=1
where we used the following inequality thet PtT=1 ta = O(T1+a), ∀a 6= -1. The desired result
follows by using the convexity of function f, i.e., T1 PT=I f (xi∙ (t)) ≥ f (Xi∙(T)).
When fi ∈ FliP(Lf, X°) ∩ Fsmo(Sf, X°), it follows from the property of the DistZOO gop(∙) (cf.
Definition 1(iii)) that
N1
Elfi(Xi•⑴)一fi(Xi∙⑴;δt)∣ ≤ 2 Nsf δ2
i=1	2
N1
EIfi(X*) - fi(X?； δt)∣ ≤ 2 Nsf δt.
i=1	2
We then combine the preceding inequality and the results in Lemmas 4(i) and 1 to get
TT
X(E[f(Xi∙(t))] - f(X?)) ≤ PiLf + Nsf Xδ2 + 2-E[Λ*(1)]
t=1	t=1	2ηt
TT
+ 2NC2d2 X ηt + p2NLf CdX ηt.
2	t=1 δt	t=1 δt
(20)
(21)
13
Under review as a conference paper at ICLR 2022
In contrast with the bound in (18), the second term on the right-hand side of (21) now becomes
Nsf PtT=1 δt2, which gives us much more space when choosing δt ; we can show that the choices of
η = dT12∕3 and δt = t16 yield the optimal convergence rate O (Td/3).
(ii) When f ∈ FliP(Lf, X°), we have the following result for Algorithm 1 running with DistZOO
gTP(∙):
TT
X (E[f (Xi∙⑴力-f (X*》≤ p1Lf + 2NLf X δt + 2— E [λ?⑴]
t=1	t=1	2ηt
+ 2Cdcd + P2√c√d) NLfηtT
(22)
where we have used the bounds in (17), Lemma 4(ii) and Lemma 1. Then we can deduce from the
terms nɪ- and ηtT that the optimal choice of η is 力.Hence, substituting the explicit expressions
for ηt
√dτand δt
F , the desired bound can be concluded.
√t into (22), dividing both sides with T, and using the convexity of function
When f ∈ FliP(Lf, X°) ∩ Fsmo(Sf, X°), it can be shown that the term (2NLf PT=I δt) on the
right-hand side of (22) is replaced by (Nsf PtT=1 δt2), because of (20). As we discussed in the case
of f ∈ FliP(Lf, X°) ,the convergence rate is determined by the terms involving . and ηT. Hence,
the convergence rate is the same as that of the case when fi in only Lipschitz continuous. The proof
is complete.
C Proofs of Theorem 3 and 4
First, We claim that for the DistZOOs gOP(∙) and gτP(∙), the strongly convexity of f implies the
strongly convexity of its smoothed variant fi, and its proof is straightforward. We now establish the
basic convergence results for Algorithm 1 running with gOP(∙) and gτP(∙). It follows from (13) that
NN
E [Λ(t + 1)] = E [Λ(t)] + η2 XE[ kgi(Xi(t); δt)k2 ] - 2ηt XEKVfi(Xi(t); δt), Xi(t) - X〉]
i=1	i=1
NN
≤ E[Λ(t)] + η2XE[kgi(χi(t);δt)k2] - 2ηt X(E[fi(xi(t);δt)] - fi(x))
i=1	i=1
-μ∕ηtE [A(t)]
(23)
where in the equality we used the relation E [gi(xi(t); δt)] = Vfi(Xi(t); δt) (cf. Definition 1(i)) and
in the inequality we used the strongly convexity of function fi. Summing the inequalities in (23)
over t = 1 to t = T and regrouping the terms, we obtain
TN	T N
XX (E[fi(Xi(t);δt)] - fi(X)) ≤ 2 Xηt XE[kgi(Xi(t);δt)k2]
t=1 i=1
t=1 i=1
1T 1
+2 X In (E Mt)]
-E[Λ(t + 1)]) - μfE[Λ(t)]
1T	N	1	1
=2 EntΕE[kgi(Xi(t);δt)k2] +2 — -μf E[Λ(1)]
2 t=1	i=1	2 η1
T
+ 5 X  -----------Mf) E [λ⑴]- 5—E [λ(T +I)].
2 t=2 ηt	ηt-1	2ηT
(24)
By substituting the expression for n = 念 into (24) and dropping the negative term, it follows
TN	T N
X X (E[fi(Xi(t); δt)] - fi(X)) ≤ 2 X nt X E[ kgi(Xi(t); δt )k2 ].
t=1 i=1
t=1 i=1
(25)
14
Under review as a conference paper at ICLR 2022
Then, following the same lines as that of the proof ofLemma 1, We have that, for DistZOOs gOP(∙)
and gTP(∙),
T
TT
E(Ef(xi∙ (t))] - f (x*)) ≤ ]TE[∣f (x?) - f(x? δt)∣] + ]TE[∣f (xi∙(t))- f(xi∙ (t); δt)∣]
t=1	t=1	t=1
T	N	T-1	N
+ PiLf + 2 Xηt XE[ kgi(xi(t); δt)k2 ] + P2Lf X ηt XE[ kgi(xi(t); δt)k ].
t=1	i=1	t=1	i=1
(26)
(i)	We now derive the convergence rate results for Algorithm 1 running with DistZOOs g0P(∙).
When fi ∈ FliP(Lf, X°) ∩ Fsc(μf, X°), we combine the results in (17), (26) and Lemma 4(i) to get
T	TT	T
X (E[f(xi∙(t))]	- f(x*))	≤ PILf	+ 2NLf X δt + 2NC2d2	X δt + p2NLfCd X δt	.
t=1	t=1	t=1 t	t=1 t
By substituting η = * and δt = into the preceding inequality and using the convexity of F,
it yields the following optimal bound
T X (E[f(xi∙(t))] - f (x?)) = O (T1/3).
When f ∈ Flip(Lf, X°) ∩ Fsmo(Sf, X°) ∩ Fsc(μf, X°), it follows from an argument similar to that
of (21) that
T	TT	T
X (E[f(xi∙(t))]	- f(x*))	≤ PILf +	Nsf X δ2	+ 2NC2d2 X	ηt	+ p2NLfCdX	ηt.
t=1	t=1	2	t=1	δt	t=1	δt
It can be proven that the choice of δt =备 yields the optimal convergence rate, that is, O (√√T).
(ii)	The proof for Algorithm 1 running with DistZOO gτp(∙) can be obtained in a similar way by
exploiting the properties of the DistZOO gτp(∙).

D	Proof of Theorem 5
We provide the basic convergence result for each stage k, and we start by deriving a similar bound
as that of Lemma 1 as follows:
T (k)	T (k)	T (k)
X (E[f (x(k)(t))] - f (x?)) ≤ X E[∣f (x*) - f(x*； δ(k))∣] + X E[∣f (x(k)(t)) - /(x(k)(t); δ(k))∣]
t=1	t=1	t=1
+ X E[A(k)，?(切-η¾",*(t + 2)] + PIk)Lf + 2 X 产 XE[幅(x(k)(t); δ(k)升2]
T(k)	N
+P2Lf X产 Xe[幅(x(k)(t);淤))||]
t=1	i=1
(27)
where Λ(k),*(t) = PN=I ∣∣xik)(t) - x*k2 and PIk) satisfies the following bound, according to
compactness of the set X,
PIk)=2Nm∈%{kxavg⑴-x(k)⑴∣}+2Nae (X∣xik)⑴∣) ≤ 卜N+汇N2) rx.
i=1	(28)
15
Under review as a conference paper at ICLR 2022
The left-hand side of (27) can be further bounded by using the strongly convexity of F, that is,
T(k)
T1k) X f (Xik)(t)) - f (x?)) ≥ Nf (x*), X(k)(T(k)) - x*〉+ Nμf kχik)(T(k)) - x?k2
t=1
Applying the first-order optimality condition to the preceding inequality, i.e., BF(x*), x-x*) ≥ 0
for any x ∈ X, yields
T(k)
T1k) X ff (x(k)(t)) - f(x*)) ≥ Nμf kxik+I)(I)- x*k2	(29)
t=1
where the second inequality follows from the non-expansiveness of the Euclidean projection
projχ(∙), and the last equality from Step 3 in Algorithm 2. Combining the inequalities (27), (28)
and (29), We have for any i∙ ∈ V,
NFE[kx(k+I)⑴-x*k1 ≤ P3")-x*k2] +(4N +J2) LfRX出
η	-β
T(k)	T
+ T1k) X E[∣f(x*) - f(x*; δ(k))∣] + TIk) XE[∣f(x(k)(t)) - f(x(k)(t); δ(k))∣]
t=1	t=1
T(k)	N	T(k)	N
+ 2Tk) X 产 XE[幅(x(k)(t)；δ(k)升2] + P2Lf Tk) X 产 XE[幅(x(k)(t);δ叼∣].
t=1	i=1	t=1	i=1
(30)
From (30) We find that the convergence depends on the properties of the DistZOOs, and We first
derive the dimension-dependence error bounds for DistZOO gOP(∙) and the bounds for gTP(∙) nat-
urally folloWs from the derivations.
For DistZOO gop(∙), when f ∈ FliP(Lf, BRX) ∩ Fsc(μf, BRX), it follows from (17), (30) and
Lemma 4(i) that
Ehmax {kx(k+1)⑴-x*k2}] ≤ Ehmax {kx(k)⑴-x*k2}] μfη(1)T(k)
+ 4Lδ(k) +4(2+ 产N) LfRx ɪ + 2p2LCd% + ɪC?"?晨2
μf	1	1 — p μ μf	Ts)	μf	δ(k)	μf	(δ(k))2
On the other hand, we have
T(k) = ak-1τ(1),	n(k) = -ɪ η⑴,	δ(k) = ɪ δ(I)
ak-1	bk-1
This, together with inequality (31), gives
Ehmax {kx(k+I)(I)-X*k2}i ≤ Ehmax {kx(k)(1) - x*k2}] μfη(1)T(1)
1
(31)
(32)
+ ------,-------、、k—1 ×
(mrn {a, b, a,法})
4 Lfδ(I)+4(2 + ι⅞N)Lf RX T⅛
(33)
+ 2P2 Lf Cdη(1) + ɪ C 2 d 瑞)
μf	δ(I)	μf	(δ(I))
By substituting T(I) = m = 1, n(1) = mi3；； b2} and δ(1) = 1 into the preceding relation, we
arrive at
Ehmax {∣∣x(k+I)(I) - x*∣∣2}] ≤ Ehmax {||x(k)(1) - x*∣∣2}]-3---7 +-------R——Lr
[i∈v'11 i L " ʃj - L∈v' "i"	" j」4min {b,隹}陋皿{b, a })k-1
(34)
16
Under review as a conference paper at ICLR 2022
where We have used the fact that min {a,b, b, b2} = min {b,普}, due to a > b2 (because of
b2 > 1) and b > 1, and
Ri= 4Lf δ⑴ +4(2+ 产N) Lf RXɪ + 2p2Lf Cdn^ + ɪC2d2.
μf	1	1- β J μf	T(1)	μf	δ(1)	μf	(S(I))2
We next show by induction that
EhmVX {kx(k)(1)-x*k2}i ≤ 4maχ⅛RX}	(35)
where h = min {b, b2 }. For k = 1, we can use the following bound to deduce that inequality (35)
holds, maxi∈V {kxi(1)(1) - x?k2} ≤ 4R2X ≤ 4hmax{R1, R2X}. We then assume that inequality
(35) holds for k and show it holds for k + 1 as well,
Eh max {kχ(k+I)(I)-χ*k2 }i ≤ 4h Eh max {kχ(k)(I)-X*k2}] + hR⅛
≤
3 max{R1 , RX2 }
max{R1 , RX2 }
hk1
hk1
4 max{R1 , R2X}
≤	hk-1
+
which leads to the conclusion in (35). It is easy to verify that the total number of stages in Algorithm
2is k\ = [loga (mT + 1)J, and the final estimates returned by Algorithm 2 are Xi(T), i ∈ V. Hence,
applying k\ + 1 to (35), we have
Ehmax {kXi(T)-x*k2}]≤ 4mhX{R1-RX}
V 4h2 max{Rι, RX}
一	hloga( T + I)
4h2 max{R1, RX2 }
（mT+1）
1
IOghS)
(36)
where we used the inequality that k\ ≥ loga (T + 1)- 1. We are left to find the minimum of
logh(a)=logmin{b,含}⑷
which is achieved when b
b2. This yields the following final conver-
gence rate, that is, E [maxi∈v {∣∣Xi(T) - x?
equality follows from R1 = O d2 .
k2} ≤
4b2m⅞L = O ((T+⅞^) , wherethe
When fi ∈ Flip(Lf, BRX) ∩ Fsmo(Sf, BRX) ∩ Fsc(μf, BRX), the dimension-dependence bound can
be obtained in a similar fashion.
For DistZOO gTP(∙), when f ∈ Flip(Lf,BRX) ∩ Fsc(μf,BRX), it follows from (17), (30) and
Lemma 4(ii) that
Eh maX {kx(k+I)(I)-x*k2}i ≤Eh max {H)(I)-X?k2}i f⅛w
+4 Lf δ(k)+4 (2+⅛n ) LfRX T⅛)+ Lf (Cd+2p2√c√d)产
(37)
By setting b = a and then substituting T(1) = 1, n(1) = 4a- and δ(1) = 1 into the preceding
3μf
inequality it follows that
Ehmax{kx(k+1)(I)-X*k2}] ≤ Ehmax{kxik)(I)-X?k2}iɪ + -⅛	(38)
i∈V	i∈V	4a	a -
where R2 = 4Lf δ(1) + 4(2+ 曰N) Lf RXTIiy + Lf (cd +2p2√C√d) n(1). Then, following
an argument similar to that of part (i), we obtain
E [maX {kxi(T) - X? k2}] ≤
4a2 max{R2, RX}	o ( d )
τ+ι = O (T + 1)
Similarly, when f ∈ Flip(Lf, BRX) ∩ Fsmo(sf, BRX) ∩ Fsc(μf, BRX), we have
EhmaX{kXi(T) -x*k2}i ≤ 4a2m*12,RX} = O (TA1)
where R = 2 f (δ⑴)2 + 4(2 + IaeeN) Lf RXTIIy + L (cd + 2p2√c√d) n(1). The proof is
complete.
17