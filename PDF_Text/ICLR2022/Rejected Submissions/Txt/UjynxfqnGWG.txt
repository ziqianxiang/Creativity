Under review as a conference paper at ICLR 2022
Inductive Biases and Variable Creation
in Self-Attention Mechanisms
Anonymous authors
Paper under double-blind review
Ab stract
Self-attention, an architectural motif designed to model long-range interactions
in sequential data, has driven numerous recent breakthroughs in natural language
processing and beyond. This work provides a theoretical analysis of the inductive
biases of self-attention modules, where our focus is to rigorously establish which
functions and long-range dependencies self-attention blocks prefer to represent.
Our main result shows that bounded-norm Transformer layers create sparse vari-
ables: they can represent sparse functions of the input sequence, with sample
complexity scaling only logarithmically with the context length. Furthermore, we
propose new experimental protocols to support this analysis and to guide the prac-
tice of training Transformers, built around the large body of work on provably
learning sparse Boolean functions.
1	Introduction
Self-attention mechanisms have comprised a dramatic paradigm shift in deep learning in recent
years, appearing ubiquitously in recent empirical breakthroughs in sequence modeling and unsuper-
vised representation learning. Starting with large-scale natural language processing (Vaswani et al.,
2017), self-attention has enjoyed surprising empirical successes in numerous and diverse modali-
ties of data. In many of these settings, self-attention has supplanted traditional recurrent and con-
volutional architectures, which are understood to incorporate inductive biases about temporal and
translational invariances in the data. Self-attention models discard these functional forms, in favor
of directly and globally modeling long-range interactions within the input context.
The proliferation of self-attention raises countless mysteries for theorists and empiricists. One fun-
damental question concerns its statistical properties: How should we think about the inductive biases
of self-attention models? More specifically, we can ask: Which functions do self-attention blocks
prefer to represent? How many (approximately) distinct functions can they represent? To this end,
this work initiates an analysis of the statistical foundations of self-attention, as it is used in today’s
state-of-the-art models.
Our main technical contribution is a classical norm-based generalization bound for a Transformer
network, which can be extended to related and future architectures via a modular abstraction of at-
tention mechanisms. In particular, the capacity (in terms of log covering number) of the function
class of bounded weight self-attention heads (and Transformers) grows only logarithmically in the
context length, which provides theoretical justification for the empirical observation that attention
models can learn long-term dependencies without overfitting. Next, we show that bounded-norm
self-attention heads are capable of representing wide classes of sparse functions. This representa-
tional capacity result, combined with the generalization results, provides a partial theoretical expla-
nation for the observed sparsity bias of attention models, which we term sparse variable creation.
We accompany this analysis with an experimental study of the sample complexity needed by Trans-
formers to learn sparse Boolean conjunctions, and verify the sample complexity scaling law pre-
dicted by the theory in this clean synthetic setting. We discuss how to extend and repurpose this
experimental protocol of benchmarking long-context sequence models on synthetic “cryptographic”
tasks. We find that Transformers trained with gradient descent can learn sparse parities with noise,
which may be of independent interest, exposing the empirical study of Transformers to the rich
theory established around this problem.
1
Under review as a conference paper at ICLR 2022
1.1	Related work
The direct precursors to modern self-attention architectures were recurrent and convolutional net-
works augmented with attention mechanisms (Bahdanau et al., 2014; Luong et al., 2015; Xu et al.,
2015). Landmark work by Vaswani et al. (2017) demonstrated significantly improved machine trans-
lation models via self-attention only; autoregressive language models followed shortly (Liu et al.,
2018; Radford et al., 2018; 2019; Brown et al., 2020), as well as self-supervised representation
learning (Devlin et al., 2018). More recently, self-attention has demonstrated promise in computer
vision (Dosovitskiy et al., 2020), protein folding (Jumper et al., 2021), theorem proving (Polu &
Sutskever, 2020), program synthesis (Chen et al., 2021b), and reinforcement learning (Chen et al.,
2021a; Janner et al., 2021).
Norm-based generalization bounds. There is a vast body of literature dedicated to establishing
statistical guarantees for neural networks, including VC-dimension and shattering bounds (dating
back to Anthony et al. (1999)). In recent years, generalization bounds have been established for
various architectures under norm bounds including (Bartlett et al., 2017; Neyshabur et al., 2015;
2017; Golowich et al., 2018; Long & Sedghi, 2019; Chen et al., 2019) using covering-based argu-
ments; Jiang et al. (2019) provide an extensive empirical study of how well these bounds predict
generalization in practice. Our work complements these bounds by establishing norm-based bounds
for attention models. Our main results rely on a novel reduction to the '∞ covering number bound
for linear function classes given by Zhang (2002).
Theory for attention models. Our work complements various existing theoretical perspectives on
attention-based models. Vuckovic et al. (2020) formulate a dynamical system abstraction of at-
tention layers, arriving at similar Lipschitz constant calculations to ours (which are coarser-grained,
since they focus on contractivity and stability rather than finite-sample statistical guarantees). Zhang
et al. (2019); Snell et al. (2021) study idealizations of optimization problems for self-attention
heads. Wei et al. (2021) propose a definition of statistically meaningful approximation of func-
tion classes that ties statistical learnability with expressivity, and show that Boolean circuits can
be SM-approximated by Transformers with a sample complexity bound that depends mildly on cir-
cuit depth (rather than context size), using a margin amplification procedure.1 See Appendix D for
a broader survey of related work, including universal function approximation-style results (which
ignore statistical considerations).
2	Background and notation
Notation. We use d to denote the embedding dimension for input tokens. T denotes the number
of tokens in an input sequence, a.k.a. the context length or context size of an attention mechanism.
And m refers to the number of samples (input sequences) in a data set. ∣∣ ∙ ∣∣2 denotes the spectral
norm for matrices, and ∣∣ ∙ ∣∣p,q denotes the (p, q) matrix norm where the p-norm is over columns
and q-norm over rows. ∣∣ ∙ ∣∣p denotes the 'p norm for vectors. For '2-norm, We drop the subscript.
B is generally used to quantify bounds on norms of matrices and L for Lipschitz constants. ∆n-1
denotes the simplex of dimension n, that is, ∆n-1 := {x ∈ Rn : x ≥ 0, ∣x∣1 = 1}.
Covering numbers and uniform generalization bounds. Our main technical contribution is a
generalization bound arising from carefully counting the number of functions representable by a
Transformer. This main complexity notion we use is covering number.
We will use the following definition of ∞-norm covering number adapted from Zhang (2002):
Definition 2.1 (Covering number). For a given class of vector-valued functions F, the covering
number N∞(F; ε; {z(i)}m=ι; ∣ ∙ ∣∣) is the smallest size of a collection (a Cover) C ⊂ F such that
∀f ∈ F, ∃f ∈ C satisfying maxi ∣∣f(z(i)) — f(z(i))∣ ≤ ε. Further, define N∞(F,ε,m, ∣ ∙ ∣)=
SuPz⑴…z(m)N∞(F; ε; z(1),..., z(m), ∣ ∙ ∣∣).
If F is real-valued (instead of vector-valued), we drop the norm from the notation. Furthermore for
functions parameterized by a set of parameters Θ, we exploit the notation to replace F by Θ.
1Quoting the discussion following Theorem 4.1 of (Wei et al., 2021): “The correct norm-based Rademacher
complexity bound to use for Transformers is unclear.”
2
Under review as a conference paper at ICLR 2022
Figure 1: Diagrams of attention modules described in Section 3: alignment scores (grey edges) de-
termine normalized attention weights (blue), which are used to mix the inputs xi：T. Left: Attention
with a general context. Center: Stackable self-attention layer, where xi：T is the input as well as
the context. Right: Auxiliary [CLS] token to extract a single scalar from a self-attention layer,
providing a real-valued function class for classification or regression tasks.
Recall from Zhang (2002) that for the class of linear functions, Flin = {x → W ∙ X :
w ∈ Rd, kwks ≤ BW}, we have the covering number bound of N∞(F; ε; {x(i)}im=i) ≤
O (BXBW/ε2 ∙ log(BχBWm∕ε), where ∣∣x(i)∣∣ ≤ BX for i ∈ [m]. Importantly, note that the
covering number has a mild dependence on m, only logarithmic; this logarithmic dependence on m
will be helpful when we turn our analysis to the capacity of attention mechanisms.
Generalization bounds. This work focuses on providing log-covering number bounds, which de-
termine the generalization error via standard Rademacher complexity and chaining arguments. The
following lemma relates these quantities; we refer the reader to Appendix A.1 for more details.
Lemma 2.2 (Generalization bound via covering number). Let D be a distribution over X ×
R and let ` : R × R be a b-bounded loss function that is L-Lipschitz in its first argu-
ment. For a given function class F and f ∈ F, let risk(f; D) := E(χ,y)〜D['(f (x),y)] and
risk (f；(z(i) ,y(i))m=ι) := mm Pm=i '(f(z(ii),y(ii). Suppose F satisfies |f| ≤ A for all f ∈ F
and logN∞(F; ε; x(i), . . . , x(m)) ≤ CF /ε2 for all for all x(i), . . . , x(m) ∈ Xm . Then for any
δ > 0, with probability at least 1 - δ, simultaneously for all f ∈ F and some constant c > 0,
∣risk(f; D)- d ff ;(x(i),y(i))m=i)| ≤ 4cL,CF(1 + log(APm/CF))
+ 2b ∕0g≡
2m
3	Abstractions of attention and self-attention
Attention is not straightforward to define — unlike other architectural components such as convo-
lutions and residual connections, it is a broader model design principle, with numerous variations
possible. In this section, we present an abstraction of attention mechanisms, guided by the different
manifestations discussed in (Luong et al., 2015), with the goal of enabling a more unified and mod-
ular statistical analysis. Subsequently, we show how to represent the Transformer (the predominant
attention-based architecture) as a special case of this formulation. Our goal is not necessarily an
all encompassing formulation of attention mechanisms, but rather an abstraction that helps to guide
general design principles and theoretical analysis.
3.1	Attention
Intuitively, we would like to capture the notion that an output variable selects (“attends to”) a part
of the input context on which it will depend, based on a learned function of global interactions
(see Figure 1, left). To this end, we define an attention head as a function which maps a context
X of T inputs {xt ∈ X}tT=i (e.g. the tokens in a sentence, pixels in an image, or intermediate
activations in a neural network) and a context z ∈ Z to an output y ∈ Y . In this work, we will
exclusively consider X , Y, Z to be Rd; d is the embedding dimension, and we define the matrix of
inputs X = [xix2 . . . xT]> ∈ RT×d. An attention head uses z to select the inputs in X to which the
output y will “attend”, which we formalize below:
Definition 3.1 (Attention head). An attention head is a function f : X → Y, specified by an “align-
ment score” function Score : X × Z → R parameterized by θs ∈ Θs, normalization function
Norm : RT → ∆T-i, and position-wise transformations φin : X → V, φout : V → Y parameter-
3
Under review as a conference paper at ICLR 2022
ized by θin ∈ Θin and θout ∈ Θout. The output of an attention head on input X ∈ XT, z ∈ Z is
y = Φout (X [Norm (Score(xι,z; θs),..., Score(XT,z； θs))]『in(xt； θin); θout)
=φout (φin(X； θin)>Norm (Score(xι,z; θs),..., Score(XT,z； θs)} θout)
where φin(X; θ) = [φin(x1; θ) . . . φin (xT; θ)]> denotes the row-wise application of φin.
The above definition corresponds to leftmost diagram in Figure 1. Here, V is a vector space of input
representations “mixed” by the normalized alignment scores; in this work, we will set V = Rk. A
function class of attention heads is induced by specifying parameter classes for {Θs , Θin, Θout}.
3.2	Self-attention and Transformers
A self-attention head is a special case of an attention head, in which the context z is one of the
inputs Xt themselves: interactions between elements in X are used to select the elements of X on
which f depends. In this case, we will use the term context to denote X. The focus of this work is
to analyze the inductive biases of such a construction. For example, for a self-attention head (see
Figure 1 (center)), we would have that the t-th component is:
Iyt = φοut (Φin(X； θin)>Norm(Score(X, xt； θs)); θ0ut),
We now define the Transformer self-attention architecture as a special case of the above. Since a
Transformer layer has shared parameters between multiple output heads, it will be convenient to
define all T outputs of this layer at once.
Definition 3.2 (Transformer self-attention layer). A Transformer attention layer is a collection of T
attention heads with outputs y1, . . . , yT, specified by the following choices of function classes (with
shared parameters between the heads) where the context for yτ is Xτ .
•	Score(X, Xτ; {WQ, WK}) := Xτ>WQWK>X, WQ, WK ∈ Rd×k (for output yt)
•	φin(x; WV):= W>x,	WV ∈ Rd×k
•	φout(X; WC) := WC>σ(X),	WC ∈ Rk×d, Lσ -Lipschitz function σ : R → R applied position-
wise, with σ(0) = 0.
•	Norm(x) := SoftmaX(x) = ι>XeXpχX)
Defining Y := [y1y2 . . . yT]> ∈ RT×d and [RowSoftmax(M)]t,: := softmax(Mt,:), we have
Y = σ (RowSoftmaX (XWQ(XWK)>) XWV) Wc.
Functions from the above class of Transformer layers map RT ×d to itself, so that instances from
this function class can be composed. Although Definition 3.2 only contains the “self-attention”
component, it is not merely a simplified idealization of the full Transformer architecture used in
practice. We discuss some remaining discrepancies (positional embeddings, layer normalization,
parallel heads, position-wise feedforward networks) in Section 4.3 and the appendix.
Extracting scalar outputs from a Transformer. We introduce one more construction: the canoni-
cal way to extract a scalar prediction from the final layer of a Transformer. This is the setup used by
the classification modules in BERT (Devlin et al., 2018) and all of its derivatives. For a context of
size T, a Transformer layer with T + 1 inputs is constructed, with a special input index [CLS]. The
input at this position is a vector X[CLS] ∈ Rd (which can be considered as a constant, a part of the
input, or a trainable parameter); the output is a linear function w>y[CLS] , for a trainable parameter
w ∈ Rd. This defines a class of functions mapping RT×d → R, parameterized by a Transformer
layer’s parameters and w, which we call the class of scalar-output Transformers.
4	Capacity measures of attention modules
In this section, we present our main technical results, along with overviews of their proofs. Sec-
tion 4.1 bounds the capacity of a general attention head. Section 4.2 instantiates this bound for the
4
Under review as a conference paper at ICLR 2022
case of a single Transformer self-attention head. Section 4.3 generalizes this bound for full depth-L
Transformer networks. Our sample complexity guarantees scale only logarithmically in the context
length T, providing rigorous grounding for the intuition that the architecture’s inductive bias selects
sparse functions of the context. Lastly, in Section 4.4, we complement this capacity analysis by
exhibiting classes of functions expressible using low-norm Transformer architectures. Combining
these representation results and corresponding capacity bounds, we coin the term sparse variable
creation to refer to this inductive bias.
Note: Throughout this section, assume that kxt k2 ≤ BX for all t ∈ [T]. Note that this allows
for the FrobeniUs norm ∣∣X∣∣f to scale with √T. The key challenge throughout our analysis is to
avoid incurring factors of norms which take a sum over the t dimension, by analyzing the attention
parameters in appropriately chosen geometries.
4.1	Capacity of a general attention head
Recall that the attention head architecture can be represented as a function fhead : RT×d × Rd → Rd
parameterized by θs , θin , θout as
fhead(X, z; θs, θin, θout) = φout φin(X; θin)>Norm(Score(X, z; θs)); θout .
Denote the corresponding function class by Fhead := {(X, z) 7→ fhead (X, z; θs , θin, θout) : θs ∈
Θs, θin ∈ Θin, θout ∈ Θout}. To convert the vector-valued function class to a scalar output function
class, we define Fscalar := {(X, z) 7→ w>f (X, z) : f ∈ Fhead, w ∈ Rd, ∣w∣ ≤ Bw}.
For simplicity, we will focus only on the attention part and assume that φout is a fixed function (no
parameters) and w is fixed. It is not hard to handle these even if allowed to be trainable. For the case
of Transformers, we handle this more generally (see Appendix A.6).
Assumption 4.1. We make the following assumptions:
1.	φout is Lout-Lipschitz in the '2-norm, that is, ∀a, b ∈ Rk, ∣φout(α) — φout(b)∣ ≤ LoUtka - b∣∣.
2.	φin is Bin-bounded in '2-norm, thatis, ∣φin(α; θin)k ≤ Binkak for all a ∈ Rd and θin ∈ ㊀E.
3.	Norm is continuously differentiable and its Jacobian satisfies ∀θ ∈ RT , ∣J Norm(θ)∣1,1 ≤
CNorm .
The Jacobian assumption might seem strong. However, softmax (the most commonly used Norm
function) satisfies this with Csoftmax = 2 (see Corollary A.7).
We prove the following bound on the covering number of Fhead for m samples,
Th	eorem 4.2 (Attention head capacity). Under Assumptions 4.1, the covering number of Fhead
satisfies
logN∞ (Fhead； ε{(X⑴，z⑴)}
m
i=1
∞
≤ inf log N
α∈[0,1]
CNormLoutBinBX
,k∙k2)
αε
； {(x(ti), z(i))}i∈[m],t∈[T]
+ log N∞ Fi
n；
(1 — α)ε
Lout
{xt,i}i∈[m],t∈[T]； I ∙ ∣∣2
where FScore = {(x,z) → Score(x,z; θs) : θs ∈ Θs}, and Fin = {x → φin (x； θin) : θin ∈ ㊀E}.
Note that the bound is in terms of the N∞ covering number of functions that dependent on dimen-
sions d or k and not T. The effect of T only shows up in the number of samples to cover. The
N∞ number for many classes scales only logarithmically with the number of samples (for eg, linear
functions Zhang (2002)). This is exactly what allows us to get a logT dependence for Transformers.
Since w is fixed, an ε-covering of Fhead directly gives us an εBw-covering for Fscalar implying,
log N∞ (Fscalar ； ε; { (X ⑴，Z⑺)}1 ) ≤ log N∞ (Fhead； ElBw； { (X ⑴,Z⑴)}[[，k ∙ k 2) ∙
Proof overview. In order to prove the bound, we first show a Lipschitzness property of ftf-head. This
property allows us to construct the cover by using the covers for FScore and Fin .
5
Under review as a conference paper at ICLR 2022
Lemma 4.3 ('∞-Lipschitzness of ftf-head). For any θs, θs ∈ Θs,仇口，仇口 ∈ ㊀山;for all X ∈ RT×d,
such that X> 2 ∞ ≤ BX,
2,∞
fhead(X, z; θs, θin, w) - fhead(X, z; θbs, θbin, w)
^
^
≤ CNorm LoutBinBX ||Score(X,Z； θs) - Score(X,z; θs)∣∣^ + Lout Mn (X; θin) - Φin(X; θin)∣∣2 ɔɔ
The most crucial part of this proof is to ensure that we do not get a spurious T dependence when
accounting for the attention mechanism. The key observation here is that the attention part of the
network is computed using Norm, whose Jacobian norm is bounded. This allows us to use the mean-
value theorem to move to the maximum ('∞) error over T tokens instead of sum ('ι), which could
potentially incur a T factor. Furthermore, this allows us to combine all samples and tokens and
construct a '∞-cover for mT samples.
4.2	Capacity of a Transformer self-attention head
Let us now look at the case of a Transformer self-attention head and instantiate the covering bound.
For ease of presentation and to focus on the self-attention part, we collapse WQWK> to a single
matrix (this does not change the representation), set k = d and remove the linear layer WC2. Then
the Transformer self-attention head (for any fixed τ) can be described as
ftf-head(X； WV, WQK) ：= σ (W>X>softmax (XW>kXτ))
which is obtained from the general formulation by setting the context to be xτ ,
Score(X, xτ; WQK) = XWQ>Kxτ, Norm = softmax and φout = σ.
Let us define the function class of self-attention heads with bounded norm, Ftf-head := {X 7→
ftf-head(X;WV,WQK) : kWVT k2,∞ ≤ BV∞, kWVk ≤ BV, kWQKk2,∞ ≤ BQ∞K}. Since
WV , WQK have dimensions dependent on d and k, bounding their norms does not hide a T de-
pendence. As before, to convert this vector-valued function class to a scalar output function class,
we define Ftf-scalar :={X 7→ w>f(X) : f∈Ftf-head,w ∈Rd,kwk ≤Bw}.
We obtain the following bound on the covering number of Ftf-head as a corollary of Theorem 4.2:
Corollary 4.4. For any ε > 0 and X(1), . . . , X(m) ∈ RT×d such that X (i) >	≤ BX for all
i ∈ [m], the covering number ofFtf-head satisfies
log N∞(Ftf 一 head； ε; X ⑴，...，X (m), k ∙ k2) . (dLσ BX )2 ∙
((B∞)3 + (B∞K BV) 3 )3
ε2
• log(mT)
Here . hides logarithmic dependencies on quantities besides m and T.
Our bounds have a logarithmic dependence on T, highlighting the inductive bias of the transformer
towards selecting sparse functions of the context.
Proof overview. The above result follows from bounding the covering numbers of FQK := {z 7→
x>WqkZ : k WQKk2,∞ ≤ B∞K} and Fv := {z → W>z : k WT∣∣2,∞ ≤ B∞, ∣∣Wvk ≤ Bv}.
NOtethat |x> Wqk x-x>Wqk x| ≤ IlWQK X-WQK x|| since ∣∣Xτ k ≤ 1, so the covering number of
FQK is at most the covering number of the class of functions of the form x 7→ WQK x. Therefore,
a bound on the vector-valued linear function class suffices to handle both covering numbers. We
derive the following covering bound which gives the desired result.
Lemma 4.5. Let W : {W ∈ Rd1 ×d2 : kW k2,∞ ≤ B∞}, and consider the function class F : {x 7→
Wx : W ∈ W}. For any ε > 0 and x(1)， . . . ， x(N) ∈ Rd1 satisfying ∀i ∈ [N]， x(i) ≤ BX,
logN∞(F； ε; X⑴，…，X(NHHI2). (d2B∞BX)2 • log(N).
2See Appendix 4.3 for a general analysis.
6
Under review as a conference paper at ICLR 2022
The proof of Lemma 4.5 actually proves a somewhat stronger bound for the function class given
by {W ∈ Rd1×d2 : kW k2,∞ ≤ B∞, kW k2,1 ≤ B1}, with d2B∞B1 in the numerator instead of
(d2 B∞ )2 , but we have kept the latter formulation for simplicity of presentation.
Finally, we discuss how to account for some important architectural modifications.
Positional embeddings. In practice, the permutation-invariant symmetry of a Transformer network
is broken by adding a positional embedding matrix P ∈ RT ×d to the input X at the first layer. In
practice, the embedding matrix is often fixed and non-trainable. Our results extend to this setting in
a straightforward way; see Appendix A.4. If these matrices are to be trained from a sufficiently large
class (say, P>2,∞ ≤1), the dependence of the log-covering number on T could become linear.
Multi-head self-attention. In almost all applications of Transformers, multiple parallel self-
attention heads are used, and their outputs aggregated, to allow for a richer representation. Our anal-
ysis directly extends to this setting; see Appendix A.5 for details. When a single attention head is re-
placed with the sum of H parallel heads, the log-covering number scales up by a factor of poly(H).
Layer normalization. State-of-the-art Transformer networks are trained with layer normalization
modules (Ba et al., 2016), which is generally understood to aid optimization. We keep a variant of
layer normalization in the covering number analysis- it proves to be useful in the analysis of full
attention blocks (see Appendix A.6), as it keeps the norm of the embedding of each token bounded.
Removing these layers would lead to a worse dependence on the spectral norm of the matrices.
4.3	Capacity of deep Transformer networks
In this section, we will extend our results for L-layer Transformer blocks. Denote the weights of
layer i by W* (i) := WQ(i), WK(i), WV(i), WC(i) . Further denote the set of weights up to layer i by
W1:i = (W(1), . . . , Wi-1). Denote the input representation of layer i by gt(fi-)block(X; W1:i). We
inductively define gt(fi-)block : RT×d → RT×d starting with gt(f1-)block (X; W1:1) = X (the input):
g(f+lOCk (X； W1"+1) ：= Πnorm (。(∏norm (f (g(f)block (X；W1:i)；W(i)WC(i)
with f(Z; {WQ, WK, WV, ∙}) := RowSoftmax (ZWQ (ZWK)>) ZWV,
where Πnorm denotes layer normalization3 applied to each row. We use a slightly modified version
of LayerNorm where instead of normalizing to norm 1, we project it to the unit ball. Let us denote
the class of depth-L transformer blocks by
Ft(fL-b)lock:= X → gt(fL-b+lo1c)k(X； W 1:L+1): ∀ i ∈ [L], WV(i)2, WK(i)WQ(i)>	, WC(i)2 ≤ C2,
WV(i)2,∞,WK(i)WQ(i)>2,∞,WC(i)2,∞≤C∞).
To obtain a final scalar output, we use a linear function of the [CLS] output,
gtf-scalar(X； W 1:L+1,w) = w> [g (X; W 1:L+1)][3$] :. Let the scalar output function class be
FtfLCalar ：= {X → W>f(X)[CLS] ： f ∈ FfLb)lock,W ∈ Rd，1同1 ≤ Bw}∙
Theorem 4.6 (Theorem A.17 (Simplified)). Suppose ∀i ∈ [m], X (i) 2,∞ ≤ BX, then we have
log N∞(Ft‰; ε; X ⑴,...，X(m)) . (C2Lσ )O(L) ∙“ BXBW WC ∙ log(mT).
Note that the dependence onT is only logarithmic even for deeper networks. The dependence on
embedding dimension and (2, ∞) norms of the weight matrices is quadratic. As long as the spectral
norms of the matrices are bounded by 1 and σ is 1-Lipschitz (which holds for sigmoids and ReLUs),
the exponential dependence on L can be avoided.
3Layer normalization allows for the norms of the outputs of each token in each layer to remain bounded by
1. Note that the norm of the entire input can still have a dependence on T . Our results would go through with
a worse dependence on the spectral norms if we were to remove layer norm.
7
Under review as a conference paper at ICLR 2022
4.4	Sparse variable creation: an inductive bias for self-attention
The above analysis shows that function classes bottlenecked by self-attention mechanisms are
“small” in terms of the context size. In this section, we answer the converse question: which func-
tions of interest can they express? To this end, we show in this section that sparse Boolean functions
are realizable by bounded-norm Transformers.
Given a Boolean function f : {0, 1}T → R which only depends on s of its inputs, we represent f
using a self-attention head ftf-head composed with a feedforward network fmlp ; this is the repeated
block in the standard Transformer architecture. Intuitively, ftf-head can select an s-dimensional sub-
set of inputs to “attend to”, while fmlp memorizes an arbitrary function of these s inputs (requiring
up to ≈ 2s parameters). In the regime of s logT (think of ftf-head ◦ fmlp as implementing a single
composable Boolean gate), the corresponding statistical guarantees are meaningful. In order to de-
scribe this combination of sample-efficient sparsification of rich contexts and subsequent restricted
use of universal function approximation, we coin the term sparse variable creation.
Setup. We consider the classes of Boolean functions f : {0, 1}T → R representable by bounded-
norm scalar-output Transformer heads ftf-scalar : RT×d → R. To do this, we must first fix a mapping
from {0, 1}T to RT×d; we discuss several natural choices in Appendix B.1. The simplest of these
uses a sum of token and positional embeddings X(b)t,: := ebt + vt, for a set of approximately
orthogonal unit vectors {e0, e1, v1, . . . , vT}. After choosing a mapping X(b), the setup of the repre-
sentation problem is evident: given f (b), find Transformer weights θtf-head and feedforward network
weights θmlp such that
ftf+mlp(X(b)； θtf-head,θmlp) ：= fmlp (ftf-head(X(b)； %-head)； θmlp) ≈ f ⑸,	∀b ∈ {0, 1}T
Main representational results. We show that Transformer blocks can represent I-sparse Boolean
functions, whose values only depend on some subset of indices I ⊆ [T]. We present informal
statements of these approximation results below, and present the precise statements in Appendix B.2.
Proposition 4.7 (Sparse variable creation via Transformers; informal). Under any of the input map-
pings X (b), we have the following guarantees:
•	ftf-scalar alone can approximate a particular monotone symmetric s-sparse Boolean function, with
weight norms kWQkF ≤ O (log(T s)) , kWK kF , kWV kF , kWC kF ≤ O(s).
•	ftf+mlp can exactly represent symmetric s-sparse functions, with the same Transformer weight
norms as above; the feedforward network weights satisfy kW1 kF , kW2kF , kwkF ≤ O(poly(s)).
•	ftf+mlp can exactly represent general s-sparse functions, with the same Transformer weight norms
as above; the feedforward network weights satisfy IlWIIlF , ∣∣W2∣∣F , ∣∣w∣∣f ≤ O(2s ∙ Poly(S)) ∙
Proof ideas. Each construction uses the same idea: select WQ , WK so that the attention mixture
weights approximate the uniform distribution over the relevant positions, then use the ReLU network
to memorize all distinct values of f. Full proofs are given in Appendix B.4.
5	Empirical scaling laws for learning Boolean gates
Our theoretical analysis has shown that Transformers can represent sparse Boolean functions, with
sample complexity scaling mildly with the context size. In this section, we present an empirical
study of whether Transformer architectures (as they are trained and used in state-of-the-art language
modeling) exhibit these scalings in practice.
We introduce a rigorous benchmark for probing the empirical sample complexity of a Transformer:
attribute-efficient learning of a planted sparse Boolean function. We choose a family of distinct
distributions {D1, . . . , DN} on {0, 1}T × {0, 1}, corresponding to supervised learning problems,
such that the feature distributions are identical, and the uniform mixture -N Pi=1 Di is invariant
under all permutations of the indices 1,...,T. We then select an i* ∈ [N] uniformly at random,
train a Transformer binary classifier on m samples from Di*, then evaluate the generalization error
via cross-validation. Since the architecture, initialization, training algorithm, and training data are
permutation-invariant, at least Ω(log N) samples are required to learn this distribution: one sample
can only reveal a single bit of information about i*, via its binary label. We are interested in the
empirical scaling of the sufficient sample size m to solve this problem, in terms of N.
8
Under review as a conference paper at ICLR 2022
əz"-dujes -∙isu
_ __ __
S3一 5o∣ 6u≡-b~
example training curves fT=400)
____ ___ ___
kbp-=vuo Co⅞-V-D>
0	100	200	300	4a)	500
tmininq iteneticns
Figure 2:	Statistically probing a Transformer by training it on a 3-way AND of a hidden subset
of i.i.d. random bits. Left, center: Sublinear scaling of the empirical sample complexity. Right:
Example training curves in the {overfitting, correct} regimes: T = 400, m = {100, 200}.
3-way parity of T=IO bits
0.7
0.6
0.5
f 0.4
>0,3
0.2
0.l∙
0.0
0	100	200	300	400	500
training Iterations
3-way parity ofT=15 bits
0.7-
0.6
0.5
0.4
0.3
0.2-
0.1-
0.0
0	250 500 750 1000 1250 1500 1750 2000
training Iterations
Figure 3:	A curious empirical finding: Transformers can learn sparse parities. 10 loss curves (with
online batches) are given for this setup with s = 3, T = {10, 15, 20}, showing abrupt phase transi-
tions from random guessing to perfect classification. See Appendix C.2 for details.
Learning sparse AND gates. The simplest instantiation of this experimental framework is a hidden
s-way AND under the uniform distribution (i.e. T i.i.d. random bits). The Transformer must learn
which of the N = (T) combinations of inputs determines the label, which requires m ≥ Ω(s log T)
samples. The theory predicts the sample complexity of learning a bounded-norm Transformer should
match this scaling in T. With hyperparameters typical of Transformer setups used for natural data,
we indeed observe that the empirical sample complexity scales sublinearly with T. Figure 2 sum-
marizes our findings; details are provided in Appendix C.1.
Learning sparse parities. We can also replace the sparse AND operation with XOR: the label is
the parity of a hidden subset of input bits. This variant emphasizes the “cryptographic” nature of
this experimental setup, due to its known computational hardness. Ω(TS) statistical queries (thus,
batch gradient descent steps) are necessary (Kearns, 1998); in the presence of noise, the fastest
known algorithms for learning parities with noise require TQ(S) time (Valiant, 2012). Figure 3
(with details in Appendix C.2) show that Transformer models can fit sparse parities. This raises an
intriguing question: if theory suggests that “exhaustive search-like” methods are computationally
necessary for this problem, why does local search (i.e. gradient-based training) succeed? We leave
this computational (as opposed to statistical) mystery as an open direction for future work.
6 Conclusion and future work
We have presented a theoretical analysis of the inductive biases of self-attention models, finding
that they can learn sparse Boolean functions with sample complexity scaling logarithmically in the
context length. We call this phenomenon sparse variable creation. Our analysis is accompanied
by new empirical probes involving training Transformers on sparse Boolean functions, where we
corroborate this scaling of the sample complexity in practice. We believe our capacity bounds are
improvable (we have only sought to obtain an optimal dependence on T). Incorporating aspects of
computation and depth remains a perennial challenge in this line of inquiry.
Building further upon the principles of attention constitutes a vibrant frontier of empirical research
(Tolstikhin et al., 2021; Lee-Thorp et al., 2021; Jaegle et al., 2021b;a; d’Ascoli et al., 2021). We
hope that the theoretical foundations and experimental probes presented in this paper will assist
in further expanding the breadth of applications of self-attention, and developing more compute-
efficient, data-efficient, controllable, and reliable algorithmic interventions.
9
Under review as a conference paper at ICLR 2022
References
Martin Anthony, Peter L Bartlett, and Peter L Bartlett. Neural network learning: Theoretical foun-
dations, volume 9. cambridge university press Cambridge, 1999.
Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint
arXiv:1607.06450, 2016.
Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly
learning to align and translate. arXiv preprint arXiv:1409.0473, 2014.
Peter Bartlett, Dylan J Foster, and Matus Telgarsky. Spectrally-normalized margin bounds for neural
networks. arXiv preprint arXiv:1706.08498, 2017.
Peter L. Bartlett and Shahar Mendelson. Rademacher and gaussian complexities: Risk bounds
and structural results. Journal of Machine Learning Research ,3:463-482, 2002. URL http:
//www.jmlr.org/papers/v3/bartlett02a.html.
Satwik Bhattamishra, Kabir Ahuja, and Navin Goyal. On the ability and limitations of transformers
to recognize formal languages. arXiv preprint arXiv:2009.11264, 2020a.
Satwik Bhattamishra, Arkil Patel, and Navin Goyal. On the computational power of transformers
and its implications in sequence modeling. arXiv preprint arXiv:2006.09286, 2020b.
Tom B Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal,
Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are
few-shot learners. arXiv preprint arXiv:2005.14165, 2020.
Lili Chen, Kevin Lu, Aravind Rajeswaran, Kimin Lee, Aditya Grover, Michael Laskin, Pieter
Abbeel, Aravind Srinivas, and Igor Mordatch. Decision transformer: Reinforcement learning
via sequence modeling. arXiv preprint arXiv:2106.01345, 2021a.
Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde, Jared Kaplan, Harri Ed-
wards, Yura Burda, Nicholas Joseph, Greg Brockman, et al. Evaluating large language models
trained on code. arXiv preprint arXiv:2107.03374, 2021b.
Minshuo Chen, Xingguo Li, and Tuo Zhao. On generalization bounds of a family of recurrent neural
networks. arXiv preprint arXiv:1910.12947, 2019.
Kevin Clark, Urvashi Khandelwal, Omer Levy, and Christopher D Manning. What does BERT look
at? an analysis of BERT’s attention. arXiv preprint arXiv:1906.04341, 2019.
George Cybenko. Approximation by superpositions ofa sigmoidal function. Mathematics of control,
signals and systems, 2(4):303-314, 1989.
StePhane d'Ascoli, Hugo Touvron, Matthew Leavitt, Ari Morcos, Giulio Biroli, and Levent Sagun.
Convit: Improving vision transformers with soft convolutional inductive biases. arXiv preprint
arXiv:2103.10697, 2021.
Mostafa Dehghani, Stephan Gouws, Oriol Vinyals, Jakob Uszkoreit, and Eukasz Kaiser. Universal
transformers. arXiv preprint arXiv:1807.03819, 2018.
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep
bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018.
Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas
Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An
image is worth 16x16 words: Transformers for image recognition at scale. arXiv preprint
arXiv:2010.11929, 2020.
Richard M Dudley. The sizes of compact subsets of hilbert space and continuity of gaussian pro-
cesses. Journal of Functional Analysis, 1(3):290-330, 1967.
Noah Golowich, Alexander Rakhlin, and Ohad Shamir. Size-independent sample complexity of
neural networks. In Conference On Learning Theory, pp. 297-299. PMLR, 2018.
10
Under review as a conference paper at ICLR 2022
Anirudh Goyal, Alex Lamb, Jordan Hoffmann, Shagun Sodhani, Sergey Levine, Yoshua Bengio,
and Bernhard SchOlkopf. Recurrent independent mechanisms. In International Conference on
Learning Representations, 2020.
Anirudh Goyal, Aniket Didolkar, Nan Rosemary Ke, Charles Blundell, Philippe Beaudoin, Nicolas
Heess, Michael C Mozer, and Yoshua Bengio. Neural production systems. Advances in Neural
Information Processing Systems, 34, 2021.
Kurt Hornik, Maxwell Stinchcombe, and Halbert White. Multilayer feedforward networks are uni-
versal approximators. Neural networks, 2(5):359-366, 1989.
Jiri Hron, Yasaman Bahri, Jascha Sohl-Dickstein, and Roman Novak. Infinite attention: Nngp and
ntk for deep attention networks. In International Conference on Machine Learning, pp. 4376-
4386. PMLR, 2020.
Andrew Jaegle, Sebastian Borgeaud, Jean-Baptiste Alayrac, Carl Doersch, Catalin Ionescu, David
Ding, Skanda Koppula, Daniel Zoran, Andrew Brock, Evan Shelhamer, et al. Perceiver io: A
general architecture for structured inputs & outputs. arXiv preprint arXiv:2107.14795, 2021a.
Andrew Jaegle, Felix Gimeno, Andrew Brock, Andrew Zisserman, Oriol Vinyals, and Joao Carreira.
Perceiver: General perception with iterative attention. arXiv preprint arXiv:2103.03206, 2021b.
Michael Janner, Qiyang Li, and Sergey Levine. Reinforcement learning as one big sequence mod-
eling problem. arXiv preprint arXiv:2106.02039, 2021.
Yiding Jiang, Behnam Neyshabur, Hossein Mobahi, Dilip Krishnan, and Samy Bengio. Fantastic
generalization measures and where to find them. arXiv preprint arXiv:1912.02178, 2019.
William B Johnson, Joram Lindenstrauss, and Gideon Schechtman. Extensions of lipschitz maps
into banach spaces. Israel Journal of Mathematics, 54(2):129-138, 1986.
John Jumper, Richard Evans, Alexander Pritzel, Tim Green, Michael Figurnov, Olaf Ronneberger,
T r . 1	rɪ-ɪ	1 -∣T-1	I ʌ	A	. ∙ r^r / 11 t	I ʌ	1	. 1 T τ∙ 1 ι	.
Kathryn Tunyasuvunakool, RUSS Bates, AUgUStin Zidek, Anna Potapenko, et al. Highly accurate
protein structure prediction with alphafold. Nature, 596(7873):583-589, 2021.
Michael Kearns. Efficient noise-tolerant learning from statistical queries. Journal of the ACM
(JACM), 45(6):983-1006, 1998.
Giancarlo Kerg, Bhargav Kanuparthi, Anirudh Goyal, Kyle Goyette, Yoshua Bengio, and Guillaume
Lajoie. Untangling tradeoffs between recurrence and self-attention in artificial neural networks.
Advances in Neural Information Processing Systems, 33, 2020.
Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint
arXiv:1412.6980, 2014.
James Lee-Thorp, Joshua Ainslie, Ilya Eckstein, and Santiago Ontanon. Fnet: Mixing tokens with
fourier transforms. arXiv preprint arXiv:2105.03824, 2021.
Valerii Likhosherstov, Krzysztof Choromanski, and Adrian Weller. On the expressive power of
self-attention matrices. arXiv preprint arXiv:2106.03764, 2021.
Peter J Liu, Mohammad Saleh, Etienne Pot, Ben Goodrich, Ryan Sepassi, Lukasz Kaiser,
and Noam Shazeer. Generating wikipedia by summarizing long sequences. arXiv preprint
arXiv:1801.10198, 2018.
Philip M Long and Hanie Sedghi. Generalization bounds for deep convolutional neural networks.
arXiv preprint arXiv:1905.12600, 2019.
Kevin Lu, Aditya Grover, Pieter Abbeel, and Igor Mordatch. Pretrained transformers as universal
computation engines. arXiv preprint arXiv:2103.05247, 2021.
Minh-Thang Luong, Hieu Pham, and Christopher D Manning. Effective approaches to attention-
based neural machine translation. arXiv preprint arXiv:1508.04025, 2015.
11
Under review as a conference paper at ICLR 2022
Behnam Neyshabur, Ryota Tomioka, and Nathan Srebro. Norm-based capacity control in neural
networks. In Conference on Learning Theory, pp. 1376-1401. PMLR, 2015.
Behnam Neyshabur, Srinadh Bhojanapalli, and Nathan Srebro. A pac-bayesian approach to
spectrally-normalized margin bounds for neural networks. arXiv preprint arXiv:1707.09564,
2017.
Michael A Nielsen. Neural networks and deep learning, volume 25. Determination press San
Francisco, CA, 2015.
Ryan O’Donnell. Analysis of boolean functions. arXiv preprint arXiv:2105.10386, 2021.
Stanislas Polu and Ilya Sutskever. Generative language modeling for automated theorem proving.
arXiv preprint arXiv:2009.03393, 2020.
Alethea Power, Yuri Burda, Harri Edwards, Igor Babuschkin, and Vedant Misra. Grokking: Gener-
alization beyond overfitting on small algorithmic datasets. In ICLR MATH-AI Workshop, 2021.
Alec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. Improving language under-
standing by generative pre-training. 2018.
Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. Language
models are unsupervised multitask learners. OpenAI blog, 1(8):9, 2019.
Anna Rogers, Olga Kovaleva, and Anna Rumshisky. A primer in bertology: What we know about
how bert works. Transactions ofthe Association for Computational Linguistics, 8:842-866, 2020.
Hava T Siegelmann and Eduardo D Sontag. On the computational power of neural nets. Journal of
computer and system sciences, 50(1):132-150, 1995.
Charlie Snell, Ruiqi Zhong, Dan Klein, and Jacob Steinhardt. Approximating how single head
attention learns. arXiv preprint arXiv:2103.07601, 2021.
Yi Tay, Mostafa Dehghani, Samira Abnar, Yikang Shen, Dara Bahri, Philip Pham, Jinfeng Rao,
Liu Yang, Sebastian Ruder, and Donald Metzler. Long range arena: A benchmark for efficient
transformers. arXiv preprint arXiv:2011.04006, 2020.
Ian Tenney, Dipanjan Das, and Ellie Pavlick. Bert rediscovers the classical nlp pipeline. arXiv
preprint arXiv:1905.05950, 2019.
Ilya Tolstikhin, Neil Houlsby, Alexander Kolesnikov, Lucas Beyer, Xiaohua Zhai, Thomas Un-
terthiner, Jessica Yung, Daniel Keysers, Jakob Uszkoreit, Mario Lucic, et al. Mlp-mixer: An
all-mlp architecture for vision. arXiv preprint arXiv:2105.01601, 2021.
Gregory Valiant. Finding correlations in subquadratic time, with applications to learning parities and
juntas. In 2012 IEEE 53rd Annual Symposium on Foundations of Computer Science, pp. 11-20.
IEEE, 2012.
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,
Eukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Advances in neural information
processing systems, pp. 5998-6008, 2017.
James Vuckovic, Aristide Baratin, and Remi Tachet des Combes. A mathematical theory of atten-
tion. arXiv preprint arXiv:2007.02876, 2020.
Colin Wei, Yining Chen, and Tengyu Ma. Statistically meaningful approximation: a case study on
approximating turing machines with transformers. arXiv preprint arXiv:2107.13163, 2021.
Kelvin Xu, Jimmy Ba, Ryan Kiros, Kyunghyun Cho, Aaron Courville, Ruslan Salakhudinov, Rich
Zemel, and Yoshua Bengio. Show, attend and tell: Neural image caption generation with visual
attention. In International conference on machine learning, pp. 2048-2057. PMLR, 2015.
Greg Yang. Tensor programs ii: Neural tangent kernel for any architecture. arXiv preprint
arXiv:2006.14548, 2020.
12
Under review as a conference paper at ICLR 2022
Chulhee Yun, Srinadh Bhojanapalli, Ankit Singh Rawat, Sashank J Reddi, and Sanjiv Kumar.
Are transformers universal approximators of sequence-to-sequence functions? arXiv preprint
arXiv:1912.10077, 2019.
Jingzhao Zhang, Sai Praneeth Karimireddy, Andreas Veit, Seungyeon Kim, Sashank J Reddi, Sanjiv
Kumar, and Suvrit Sra. Why are adaptive methods good for attention models? arXiv preprint
arXiv:1912.03194, 2019.
Tong Zhang. Covering number bounds of certain regularized linear function classes. Journal of
Machine Learning Research, 2(Mar):527-550, 20θ2.
13
Under review as a conference paper at ICLR 2022
A Proofs of capacity bounds
In this section we present the full proofs (including the omitted proofs) of our capacity bounds. We
also cover relevant background and useful technical lemmas.
A.1 Rademacher complexity and generalization bounds
Here we briefly review Rademacher complexity and its relationship to covering numbers and gener-
alization bounds. We refer the reader to Bartlett & Mendelson (2002) for a more detailed exposition.
Definition A.1 (Empirical Rademacher complexity). For a given class of functions F = {f : X →
R} and {z(i) ∈ X}im=1, the empirical Rademacher complexity Rb(F; z(1), . . . , z(m)) is defined as
1m
R(F; Z ⑴，...，z(m)) = L Eε SUp X εif(z⑺)，
m f∈Fi=1
where ε is a vector of m i.i.d. Rademacher random variables (Pr[εi = 1] = Pr[εi = -1] = 1/2).
In order to relate the Rademacher complexity and '∞-covering numbers, We use a modified version
of Dudley’s metric entropy.
Lemma A.2 (Dudley (1967); modified). Consider a real-valued function class F such that |f | ≤ A
for all f ∈ F. Then
R(F; Z(I),…，z(m)) ≤ C ∙ inf (δ + /A rlogN∞(F £;；(1),…,z(m)Idε)
for some constant c > 0.
Proof sketch. The original statement is for 2-norm covering number, but the ∞-norm case reduces
to the 2-norm case because N2(∙) ≤ N∞(∙). The original statement also fixes δ = 0 rather than
taking an infimum. Also, the standard statement has the integral go from 0 to ∞, but these are easily
replaced with δ and A.	□
For our paper, we will instantiate the above lemma for log covering numbers scaling as 1∕ε2.
Corollary A.3 (Rademacher Complexity via covering number). Consider a real-valued function
class F such that |f | ≤ A forall f ∈ F. Suppose log N∞ (F; ε; Z(I),..., z(m)) ≤ CF/ε2 ,then
RR(F； Z(I),…，Zg) ≤ C ∙ ,CF ∙(1 + log (Apm∕CF'
for some constant c > 0.
Proof. Using Lemma A.2, we have for some constant C > 0,
Rb(F
log(A∕δ)
C inf δ +
δ≥0
C
1 + log (AvzmTCF
□
14
Under review as a conference paper at ICLR 2022
We can now obtain a generalization guarantee from the Rademacher complexity of a function class:
Theorem A.4 (Bartlett & Mendelson (2002)). Let D be a distribution over X × R and let
` : R × R be a b-bounded loss function that is L-Lipschitz in its first argument. For a given func-
tion class F and f ∈ F, let risk(f; D) := E(χ,y)〜D['(f(x),y)] and risk f ;(z(i),y(i))m=i):=
工 Pm=I '(f (z(i)), y(i)). Thenforany δ > 0, with probability at least 1 — δ, simultaneously for all
f∈F,
∣risk(f ； D)- dk f ；(Z⑴,y⑴虞J ∣ ≤ 4L RR(F; Z ⑴,…,z(m)) + 2bj log*.
Combining the above, we get:
Lemma A.5 (Lemma 2.2 (restated)). Consider a function class F such that |f| ≤ Afor all f ∈ F
and log N∞(F; ε; x(1),..., x(m)) ≤ CF/ε2 forall x(1),..., x(m) ∈ Xm. Thenforany δ > 0, with
probability at least 1 - δ, simultaneously for all f ∈ F,
∣risk(f ； D)- dd (f; (X⑴,y ⑴度J ∣ ≤ 4$和(1 + log (A√m∕CF)) + 2bj log：/",
for some constant c > 0.
A.2 Useful lemmas
Lemma A.6. Consider function f : Rd → ∆d-1 such that the Jacobian of the function satisfies
kJ f (θ)k1,1 ≤ cf for all θ ∈ Rd, then for any vectors θ1, θ2 ∈ Rp,
kf(θ1) - f (θ2)k1 ≤ cf kθ1 - θ2k∞.
Proof. By the fundamental theorem of calculus applied to g(t) = f (tθ1 + (1 - t)θ2), followed by
a change of variables:
1J(tθ1+(1 -t)θ2)dt (θ1 -θ2),
f(θ1)-f(θ2)
We have
kf(θ1)-f(θ2)k1
Z J (tθ1 + (1 - t)θ2 ) (θ1 - θ2 )dt
0
1
By Jensen’s inequality:
≤Z 1kJ(tθ1+(1-t)θ2)(θ1-θ2)k1dt
0
Using kAxk1 ≤ kAk1,1 kxk∞:
≤Z 1kJ(tθ1+(1-t)θ2)k1,1kθ1-θ2k∞dt
0
By assumption on the Jacobian:
≤ cf kθ1 - θ2k∞ .
□
Corollary A.7. For vectors θ1, θ2 ∈ Rp, ksoftmax(θ1) - softmax(θ2)k1 ≤ 2kθ1 - θ2 k∞.
Proof. Observe that for softmax, the Jacobian satisfies:
J(θ) = diag(softmax(θ)) - softmax(θ)softmax(θ)>).
We have for all θ, h,
pp
kJ(θ)k1,1 =^X^X ∣softmax(θ)i(l[i = j] — SoftmaX(θj)|
i=1 j=1
15
Under review as a conference paper at ICLR 2022
p
softmax(θ)i 1 - softmax(θ)i +	softmax(θ)j
i=1
p
= 2	softmax(θ)i (1 - softmax(θ)i)
i=1
≤ 2.
j6=i
Combining the above with Lemma A.6 gives the desired result.
Lemma A.8. For αi , βi ≥ 0, the solution to the following optimization
n
αi
mm 〉 F
x1,...,xn	x2
i=1 i
n
subject to	βixi = C
i=1
is C and is achieved at Xi = C (αβi) / where Y = Pn=ι a；/3//.
□
Proof. The proof follows by a standard Lagrangian analysis.	□
Lemma A.9 (Contraction of Πnorm). Let Πnorm be the projection operator onto the unit norm ball.
For any vectors u, v, we have kΠnorm(u) - Πnorm (v)k ≤ ku - vk.
Proof. If u, v are both in the unit ball then this follows trivially. Let us assume that kuk ≥ kv k and
kuk ≥ 1 WLOG. First suppose kvk ≤ 1. Let BV(1) = αu be the projection ofv in the direction ofu,
and let BV2 = v - BV(1) . Then
kΠnorm(u) - Πnorm(v)k2 = ku/kuk - vk2
= ku/kuk - (αu + BV2 )k2
= k(kuk-1 - α)u - BV2 k2
= (kuk-1 - α)2kuk2 + kBV2 k2
≤ (1 - α2)kuk2 + kBV2 k2	since kuk-1 < α < 1
= ku - (αu + BV2 )k2
= ku-vk2
If kvk > 1, then
k∏norm(u) — ∏norm (v) k = k ∏norm (u/k V k ) — ∏norm (v/k V k ) k ≤ k 〃/k V k—v/k V kk < k〃 -V∣∣∙
where the second-to-last inequality follows from the ∣∣vk < 1 case.	□
Lemma A.10 (Zhang (2002) Theorem 4). Let V : {V : V ∈ Rd1 , kVk ≤ B1} and Flinear = {x 7→
V>x : V ∈ V}. For any δ > 0 and x(1), ∙ ∙ ∙ , x(N) satisfying kx(i) k ≤ B2 ∀i,
B2B2
log N∞ (Flinear; ε; x),…，x ≤ 36 —- log(2 d4B1B2∕ε + 2e N + 1) ∙
A.3 Omitted proofs
Proof of Lemma 4.3. Observe that,
fhead(X, z; θs, θin) — fhead(X, z; θbs, θbin))
=∣∣φout (φin(X； θin)>Νorm(Score(X,z; θs))) — φout
(φin(X; θbin)> Norm(Score(X, z; θbs)))
16
Under review as a conference paper at ICLR 2022
By LoUt-Lipschitzness of φoUt and bound on kwk:
≤ LoUt IIIφin(X; θin)> Norm(SCore(X, z; θs)) - φin(X; θbin)> Norm(SCore(X, z; θbs))
By triangle inequality:
≤ LoUt IIIφin(X; θin)> Norm(SCore(X, z; θs)) - Norm(SCore(X, z; θbs))III
>
^
+ LOUt Il Φin(X； θin) - Φin(X; θin)	Norm(SCore(X, z; θs))
Using kPvk ≤ kP k2,∞kvk1 and Bin-boundedness of φin:
≤ LoUt Bin IINorm(SCore(X, z; θs)) - Norm(SCore(X, z; θbs))II
+ LoUtIl (φin(X； θin) — Φin(X; θin)),
IINorm(SCore(X, z; θbs ))II
By Lemma A.6 and the assumption on Norm:
^
>
≤ LoUt CNorm Iφin (X; θin ) I2,∞
By boundedness of φin and IX> I
SCore(X, z; θs) - SCore(X, z; θs)II	+ LoUt II φin(X; θin) - φin(X; θin)
2,∞
2,∞ ≤ BX :
^
≤ LoUtCNormBinBX	IISCore(X, z;	θs)	- SCore(X, z;	θs)II	+	LoUt I	φin(X; θin) -	φin(X; θin)
>
∞
2,∞
□
Proof of Theorem 4.2. Our goal is to show that for every ε > 0, collection of inputs
(X(1), z(1)), . . . , (X(m), z(m)), there is a cover Chead such that for all θs ∈ Θs, θin ∈ Θin, there
is some (bbs, bbin) ∈ Chead such that maxi ∣∣fhead(X(i),z(i); θs,θin) - fhead(X(i),z(i); bbs, θin)∣l ≤ ε.
Observe that for all θs , θs,
max kSCore(X(i), z(i) ; θs)-SCore(X (i), z(i); θbs)k∞ = max	SCore(xt(i) , z(i); θs) - SCore(x(ti) , z(i); θbs) .
i∈[m]	i∈[m],t∈[T]
… .一 一 _ 一一一 ^
Similarly, for all θin , θin,
max
i∈[m]
—
= max	IIφin(xt(i); θin) - φin(xt(i);
2,∞	i∈[m],t∈[T]
This crucially allows us to aggregate over the i and t dimensions together.4 Therefore, we can
consider N∞ covers for the above to bound the overall covering number.
Let CSCore be the εSCore-cover (∞) for FSCore over inputs (xt(i), z(i))	of size
N∞ FSCore; εSCore; {(xt , z )}i∈[m],t∈[T] .
Also, Let Cin be the εin-cover (∞) for Fin over inputs {xt(i)}i∈[m],t∈[T] of size
N∞ (Fin； εin; {xt ^}i∈[m],t∈[T]; k ∙ k2 ) .
^ ^
^
We are ready to construct the cover for Fhead. Set Chead = {fhead(∙; θs,θin))i∈[m] ： θs ∈ CSCore,
θbin ∈
Cin}. Then for any θs ∈ Θs, θin ∈ Θin, there exists θbs, θbin ∈ Chead, such that for all i ∈ [m], using
Lemma 4.3:
IIfhead (X (i), z(i); θs, θin) - fhead (X (i), z(i); θbs, θbin)II ≤ CNorm LoUt Bin BX εSCore + LoUtεin.
4In the case of the Transformer self-attention mechanism, we will obtain ∞-norm covering numbers for
Score and φin that have only logarithmic dependence on the number of examples. Because of this aggregation
trick, the resulting covering number for the whole layer will have merely logarithmic dependence on the context
length T .
17
Under review as a conference paper at ICLR 2022
The size of the cover we have constructed is,
log |Chead | = log |CScore | + log |Cin |
=log n∞ (FSCore； εScoreX(xt' ∖ Z ɔ ) }i∈ [m] ,t∈ [T ]^ + log N∞ (Fin； εin; {xt } i iemr,]∈E[,i; ]； k
•k2
and we are done.
□
Proof of Corollary 4.4. By Theorem 4.2, the covering number of Ftf-head satisfies
logN∞ (Ftf-head； £；{(X(i),Z(i)
+ log N∞
∞
≤ inf log N
α∈[0,1]
(1 - α)ε
; 2LσBvBX;
{(xt( ), z(i))}i∈[m],t∈[T]
{x(ti)}i∈[m],t∈[T]; k • k2
;
;
where we have used the fact that for a scalar-output Transformer layer:
•	softmax satisfies the Jacobian assumption with Csoftmax = 2 using Corollary A.7.
•	Lout is the Lipschitz constant of σ: Lσ .
•	Bin is a bound on the norm of WV>x with respect to norm of x: BV .
By Lemma 4.5, for any εQK, εV > 0:
1 M (τ	(i (i) (i)u	'JdBQKfBX)2 log(mT)
log N∞ (FQK ； εQK ；{(xt ,z( ))}i∈[m],t∈[T ]) . ---至-----------
1 H (下 〃(i) (i)u	ll CJdBVinfBX)2 log(mT)
log N∞ (FV ； εV X(Xt ,z( ))}i∈[m],t∈[T ]； k • k2 ) . ---2--------
εV
since WQK , WV ∈ Rd×d (k = d). We want to choose εQK and εV to minimize the sum of the
above two terms, subject to
2Lσ BV BX εQK + LσεV ≤ ε.
By Lemma A.8, the solution to this optimization leads to an optimal bound of:
log N∞(Ftf-head; ε; X ⑴,...，X (M)) . (dLσBχ )2 ・
(B∞)3 * * * * + (B∞1 K BV BX)2 )3
ε2
• log(mT).
□
Proof of Lemma 4.5. Let B∞ be an upper bound on kW k2,∞ and B1 be an upper bound on kW k2,1.
The approach will be to cover each of the columns of W independently, treating each as specifying
a linear function from Rd1 → R.
By Lemma A.10, letting V(b) : {v : v ∈ Rd1 , kvk ≤ b} and Flinear(b) = {x 7→ v>x : v ∈ V (b)},
for any δ > 0
logN∞(Flinear(b); δ; x(1), • • • , x(N)) ≤
cb2BX2 log((1 + bBX /δ)N)
δ2
given that kx(i) k ≤ BX for all i.
^
^
(b; δ ) = {x 7→ vb> x : vb ∈ Vb }
In fact the cover, which we denote by Flinear(b; δ), is proper: Flinear
for some finite subset V ⊂ V(b).
Let
d2
S = (k1,k2, .. .,kd2) : ki ∈ {0, 1, .. .,d2} foralliand0 ≤	ki ≤ d2
18
Under review as a conference paper at ICLR 2022
Given any W ∈ W, let vi denote the ith column of W. For each i, let
ki = Bd2- kvik
B∞
Let SW = (k"...,kd2). Then SW ∈ S, and
∞k ki ≤ kvik ≤ ∞Γk (ki + I) ∀i.
d2	d2
For every tuple S = (k1, k2, . . . , kd2) ∈ S, let
WcS =	n[vb1vb2	. . .	vbd2]	:	vbi	∈	Flinear	(B∞(k + I)∕d2;	ε√(ki + I)∕(2d2)) for all i}.
Our cover will be F : {x 7→ Wx : W ∈	S∈S WS}. Note that
d2
|F| ≤ XYN∞ (FIinear(B∞(ki + I)∕d2); ε√(ki + I)∕(2d2); X⑴,…，X(N))
S∈S i=1
V X	XX b∞a2(ki + I)2 Iog(N)
.S∈Sexp1i=1 d2ε2(ki + 1)∕(2d2)
exp
S∈S
≤ |S| exp
^B^ X (ki+1)
4B1B∞BX2 log(N)
ε2
where in the last step we used the fact that
d2	d
X ki ≤ 最 X kvik = d2Bι∕B∞
Since |S| ≤ (簧)=exp(O(d2 log d2)), we obtain
log |Fb| .
d2B1B∞BX2 log(N)
ε2
as desired. In particular, we obtain the more concise, but looser, bound from the lemma statement
by using the fact that B1 ≤ d2B∞
For a particular W = [v1 . . . vd2] ∈ W it is guaranteed that there is a matrix W ∈ WSW, W =
[vb1 . . . vbd2], such that for each i ∈ [d2],
|vi>Xn
b>xnl≤ εVk⅛1 ∀n ∈ [N ].
—
where ki is the ith element of SW . We then obtain the desired covering property:
nm∈a[Nx] W Xn - WcXn = nm∈a[Nx]
u d2
t	(vi>Xn
i=1
—
≤t
X ε2 ("
i=1
ε
d2 + P= 1 ki
2d2
≤ε
□
19
Under review as a conference paper at ICLR 2022
A.4 Capacity with positional embeddings
Since the Transformer architecture is permutation invariant for all t 6= τ , positional embeddings
(fixed or trainable) are typically added to the inputs to distinguish the different positions of the
tokens. These positional embeddings are matrices P ∈ RT ×d such that P = [p1 . . . pT]> for
pi ∈ Rd . Accounting for the positional embeddings as input, a single Transformer attention head
can be expressed as:
ftf-pos(X, P; Wv, WQK) := σ (W>(X + P)>softmax ((X + P)W>k① + Pτ))).
For a fixed positional embedding P, let us define
Ftf-POs(P) ：= {X → ftf-pos(X,P; Wv ,Wqk ) ： kW>k2,∞ ≤ B∞, kWv k ≤ BvJWqk k2,∞ ≤ BQK}
. Position embedding just impacts the input into the covering bound argument which effects the
bound in terms of the P> 2,∞ as given below,
Lemma A.11. For all X(1), . . . , X(m) ∈ RT×d such that X (i) >	≤ BX for all i ∈ [m], and
P ∈ RT×d such that kP> k2,∞ ≤ BP, the covering number of Ftf-Pos(P) satisfies
((B∞) 3 + (2B∞k BV(BX + BP))3 )3
log N∞(Ftf 一 Pos(P); ε; X ⑴，...，X(m), k∙k2) . (dL。(BX +Bp ))2∙ ʌ--------------ε2---------------L- ∙ log(mT ).
Proof. Observe that ftf-Pos(X, P; Wv, WQK) = ftf-head(X + P; Wv, WQK). Thus we have,
logN∞ (Ftf-Pos(P); ε; {(X(i))}-ι，k∙ l∣2) = logN∞ (Ftf-head; ε; {χ"' + P} ∙〔，k . l∣2).
For all i ∈ [m],	(X (i)	+ P)>	2,∞	≤	X(i)>	+	P >	2,∞ ≤ BX +	BP.	Therefore, using
Corollary 4.4, We get the desired result.	□
Therefore our bounds go through for fixed positional embeddings. If we were to train the embed-
dings, We Would need a much finer cover on the embeddings Which could incur a T dependence.
A.5 Capacity of multiple parallel heads
In virtually all practical applications of Transformers since their inception, instead of using one set
of Weights for an attention head, there are parallel attention heads, Which have separate identically-
shaped parameters; their outputs are concatenated. For the purposes of this analysis, suppose We
have
HH
ftf-heads X; Wv[h],WQ[hK]	:= X ftf-head X;Wv[h],WQ[hK]	.
h=1	h=1
Let us define the class of multi-head self-attention With H heads as
Ftf-heads := nX 7→ ftf-heads X;nWv[h],WQ[hK] oH	:
∀h∈ [H],lWv[h]>l2,∞ ≤Bv∞[h],lWv[h]l ≤ Bv[h], lWQ[hK] l2,∞≤BQ∞K[h]o.
Lemma A.12. For all X(1), . . . , X(m) ∈ RT×d such that X (i) >	≤ BX for all i ∈ [m], the
covering number of Ftf -heads satisfies
⑴…	2 (PH=ι(B∞[h])2 + (2B∞K[h]BVγ)3)3
logN∞(Ftf - heads； ε; X⑴，...，X(m), k∙k2) . (dLσBX)2∙∖-------------2--------------- ∙log(mT).
ε2
Proof. For all h ∈ [H], let Ch be an εh-covering of Ftf-head With Weight bounds corresponding
to head h. Since ftf-heads X; nW[h], W[h] o = PH ftf-head (X; W[h] , W[h] ), We have
t - eas	v , QK h=1	h=1 t - ea	v , QK ,
20
Under review as a conference paper at ICLR 2022
C := C1 × . . . × CH5 is an PhH=1 εh -covering for Ftf-heads . Using Corollary 4.4 (and optimizing
for εh using Lemma A.8, by breaking them into individual errors for each head), we have
兽	兽	2 (PH=I(B∞[h])2 +(2B∞k [h]B[h])2 )3
log |C| = ElOg |Ch| ≤ £ ≤ (dLσBX)2-----------------^2----------------log(mT).
h=1	h=1	ε
□
To see the dependence on H, consider the setting where the weight bounds are the same for each
head (dropping the [h] subscript), then we get,
logN∞(Ftf-heads;ε;X⑴,...，X(m), k∙k2) . (dLσBX)2∙H3∙
((B∞)2 +(2B∞K BV) 3 )3
ε2
∙log(mT).
A.6 Capacity of deep Transformer networks
We will consider an L-layer transformer. Let us denote the weights of layer i by W (i)
WQ(i),WK(i),WV(i),WC(i) such that
and
2,∞
≤ BQ∞K(i), WV(i)	≤ BV∞(i) and WC(i)	≤ BC∞(i). Let us further
denote the set of weights up to layer i by W1:i = (W(1), . . . , Wi-1). Let the input representation
of layer i be gt(fi-)head(X; W1:i). We inductively define g with gt(f1-)head(X; W1:1) = X
gf+2d (X； W*+1) = ∏norm (。(∏norm f (gfhead (X； W1") ； W⑺)))WCi)) With
f(Z;{WQ,WK,WV,WC}) = RowSoftmax (ZWQ (ZWK)>) ZWV,
Where Πnorm is applied roW-Wise. Our final output is gtf-scalar(X； W1:L+1, w)
w>g(Lhead (X； W 1：L+1) [CLS] for ∣∣wk ≤ Bw.
In order to construct a cover, We Will first bound the distance betWeen the function g With different
Weight parameters W 1:L+1 and Wc1:L+1. This bound Will depend on the closeness of the parameters
Which Will alloW us to construct a cover of the netWork in an iterative fashion by constructing covers
of each layer.
A.6.1 Lipschitzness of the network
To bound the Lipschitzness of the netWork, We Will first bound the distance betWeen f With different
Weights and inputs.
Lemma A.13 (Instantiation of Lemma 4.3). For any WK, WcK, WV, WcV, WQ, WcQ ∈ Rd×k, for all
Z ∈ RT×dsuchthat Z>2,∞ ≤ 1,
∣(∕ (Z； {Wq,Wk, Wv, ∙}) - f (z； {cQ, cκ, CV, ∙}))> ?
≤ 2 ∣WV ∣2 (WQWK> -WcQWcK>)Z>	+(WV-WcV)>Z>
Proof. Consider a fixed roW τ of the output of the functions,
∣∣f (Z； {Wq,Wk, Wv, ∙})[τ] - f(Z； {Wq,Wk, cv, ∙}) [τ] ∣∣
=∣∣W>Z>softmax (ZWκW>zτ) — c>Z>softmax (zcκW>zτ) ∣∣
5Here, × denotes the Cartesian product: the functions obtained by using the every combination of parame-
ters of each individual cover.
21
Under review as a conference paper at ICLR 2022
By triangle inequality:
≤ I W>ZT (softmax (ZWKWTzT) — softmax (ZWKWTzT)) I
+ U (Wv — CV)τZτsoftmax (ZcKWTzT) I
USing ∣∣Pvk ≤ ∣∣P∣∣2,∞ ∣∣vkι:
≤ HWTZτII2, Jsoftmax (ZWKWTzT) — softmax (ZWKWTzT)Hl
+ Il (Wv - CV)τZTH2 Jsoftmax (ZcKCTzT)HI
By Corollary A.7, IIZτH2,∞ ≤ 1, ∣PQ∣2,∞ ≤ ∣Pk2kQk2,∞,and ∣PTk2 = ∣P∣3
≤ 2 ∣Wv ∣2 IIzWk WTzT - ZcK cTZtL + II(WV - cv )τZ TH2 OO
≤ 2 ∣Wv∣2 H (WqWK - cQcκ) ZtH2 OO +∣∣(Wv - cv)τZTH? OO
□
Lemma A.14. For any WK, Wv,Wq ∈ Rd×k, for all Z,Z ∈ RT×d such that ^Zτ H2 ∞ ≤
IjZTk2,∞ ≤ 1,	'
Kf(Z; {Wq,Wk, Wv, ∙}) - f(Z; {Wq, Wk, Wv, ∙}))>
≤ kWv∣∣2 (1 + 4 IlWKw>∣D I I (Z - Z)τ I I 2"
2,∞
Proof. Consider a fixed row T of the output of the functions,
f (Z; {Wq, Wk, Wv, ∙?) [τ] - f (Z; {Wq, WK, Wv, ∙}) [τ][1
= HWTZτsoftmax (ZWkWTzT) - WTZTSOftmaX (ZWKWTzT) H
By triangle inequality:
≤ WT (z - Z)T softmax (ZWkWTzT)
+ HWTZT (softmax (ZWkWTzT) — softmax (ZWKW>b∙)) H
USing ∣∣Pvk ≤ ∣∣P∣∣2,∞kvkι:
≤ HWT (Z - Z) % Jsoftmax (ZWKWTzT)H1
+ H WTZTH	HSOftmaX (ZWk WTzT) — softmax (ZWK WTST) H
By Corollary A.7, HZTH2 g ≤ 1 and ∣PQ∣2,∞ ≤ ∣∣P∣∣2∣Q∣2,∞:
≤ ∣Wv∣2 H(Z - Z)τII2 OO + 2∣Wv∣2 IIzWkwTzT - ZWkwTbTL
By triangle inequality:
≤ ∣Wv∣2 H(Z- Z)τII2 OO + 2∣Wv∣2 (H(Z -Z)WkwTzTHoO + HZWKWT (zT -zT)IIJ
SinCeHZTH2	≤ 1 and ∣∣Pv∣∞ ≤ ∣∣P>||22||引|:
≤ ∣Wv∣2 (1 + 4IIWkwTII2) H(Z-Z)τII2oo.
□
With the above lemmas, we are ready to prove the effect of change of weights on g.
22
Under review as a conference paper at ICLR 2022
Lemma A.15. For any Wi+1,Wi+1 satisfying the norm constraints,
(肃叔Ck(X ； W 1:i+1) - gf-+∣0Ck(x; c1"+1))>
2,∞
≤ (Wq-碎))> σ (nnorm (f((χ; CLi);节⑴)))>
I τ R(i) R(i)	(I-I- AR⑶)	(C⑴	(¥■ I"]"' c(i)	( V∙ Cr1")) T
+ Lσ Bc BV	!J + 4BQK J	(gtf - block	(X ； W )	- gtf - block	(X ； W J J
+ 2LσB(i)BV) KWQi)WK)T-移府K)T)温。ck (X；c1：i)T|2
+ LσB(i (Wv -而)>gfb∣ock (X；W1:i)T	.
2,∞
Proof. Unrolling one layer, we have
Using Lemma A.9 for each row:
≤ W((PTσ (∏norm (f (gfkd (ɪ ； W 1：i) ； W(i)))) T-碎)>σ (∏norm (f (jfLd (X W")；节⑴)))
Bounding the norm per row:
⑷ ≤∣W) ∣∣2
σ (∏norm (f (您head (X ； W^) ； W(C))))T- σ (∏norm (f (gfhead (X ；铲")M(C))))T
2,∞
Since σ is Lσ -Lipschitz and ∣∣ W(i)∣∣ ≤ B() for each row:
≤ Lσ B * ∏norm (f (θfhead (X ； W 1：i) ； W^))' - ∏norm (f (9 (ɪ ； W1：i) ； C(C)))T
2,∞
Using Lemma A.9 for each row:
≤ LσB(i) f (对head (ɪ； W 1：i) ； W(C))T- f (^^ (X W") M(C))T
2,∞
By triangle inequality:
≤ LσB(C) f (对head (ɪ； W1：C) ； W(C))T- f (^^ (X W") ； W(C))T
2,∞
+ LσBC) f (gfhead (X； C1：C) ； W(C))T - f (gfL (X； C1：C) WC))T
2,∞
23
Under review as a conference paper at ICLR 2022
By Lemma A.13 and A.14 and norm bounds on the matrices:
≤ LσBC)B(V)(1+4BQK) g(f)head (X； W1:i)> - g(X; c1：i)>
2,∞
+2LσBC(i)BV(i)	WQ(i)WK(i)> -WcQ(i)WcK(i)>gt(fi-)head X；Wc1:i)>
+LσBC(i)(WV-WcV)>gt(fi-)head X；Wc1:i)>	.
2,∞
Combining the above gives Us the desired result.	□
Lastly, we take account of the last linear weight and observe that,
Lemma A.16. For any W1:L+1, Wc1:L+1 and w, wb,
|gtf 一 scalar (X ； W 1'L+1 ,w) - gtf 一 scalar (X ； C1：L+1,u) |
≤ kwk UO(L+1) (X∙ W 1:L+1)	- o(L+1) (X∙ C1:L+1)	+ (W - w)>o(L+1) (X∙ C1:L+1)
≤ kwk gtf-block X；W [CLS] - gtf-block X； W [CLS] + ||(w - w) gtf-block X；W	[CLS] ||
Proof. Observe that,
|gtf-scalar(X； W 1：L+1 ,w) - gtf-scalar (X；铲/+1,6)|
=w>gtL盘(X； W 1：L+1 )[cls] - w>gtL思k (X； c1L+1) γcls1
[CLS]
By triangle inequality:
≤ w> (o(L+1) (X∙ W 1:L+1)	- o(L+1) (X∙ C1:L+1)	ʌ 1 + (W - W)>o(L+1) (X∙ C1:L+1)
≤ ||w	gtf-block X；W [CLS] - gtf-block X； W [CLS] || + ||(w - w) gtf-block X； W [CLS]
Bounding the inner product by norms:
≤ kwk Ub(L+1) (X∙ W 1:L+1)	-b(L+1) (X∙ C1:L+1)	+ (w-W)>b(L+1) (X∙ C1:L+1)
≤ kWk UUgtf-block X；W [CLS] - gtf-block X； W [CLS] UU + ||(W - W) gtf-block X；W	[CLS] ||
□
A.6.2 Constructing the cover
The cover construction follows the standard recipe of composing covers per layer (as in Bartlett et al.
(2017)).
Theorem A.17. Let Ft(fL-s)calar represent the class of functions of L-layer Transformer blocks satisfy-
ing the norm bounds (specified before) followed by linear layer on the [CLS] token. Then, for all
log N∞(FtfLCaiar； £； X ⑴,...，X(叫||」2).
l0g(εmT) × 卜W + XX αv3 (d2B∞(i)2 + d2 (2LσBC)BV)BQK
where αi = Qj<iLσBC(j)BV(j)(1 + 4BQ(jK) ).
(i)) 3 + k3 (LσBC)B∞(V)) 2)!
Proof. Our goal is to show that for every ε > 0, and collection of inputs X(1), . . . , X(m), there is
a cover C of vectors in R(m) such that for all W 1:L+1 and W satisfying the norm bounds, there is
some v ∈ C such that maxi |gtf-scalar(X(i)； W1:L+1, W) - v| ≤ ε.
In each layer of the transformer, WQ(i) and WK(i) always appear together in the form WK(i)WQ(i) .
Therefore, we will overload notation and define WQ(iK) : WK(i) WQ(i) . Our cover C will be proper,
24
Under review as a conference paper at ICLR 2022
consisting of vectors of the form (gtf-scalar(X(i) ; Wc1:L+1 , wb))i∈[m] . We will build the cover itera-
tively by finding finite collections of matrices Wc1:i for each layer.
First observe that for any collection of Z(1), . . . , Z(m) ∈ RT×d1, and any W, Wc ∈ Rd1 ×d2,
max W>Z(i)> - Wc>Z(i)>	= max	W>zt(i) - Wc>zt(i) .
i∈[m]	2,∞	i∈[m],t∈[T]	t	t
This crucially allows us to aggregate over the samples and context length. In particular, we can apply
Lemma 4.5 with the input vectors (zt(i))i∈[m],t∈[T]; a total of mT input vectors. Specifically, for any
ε and W(d1, d2, α) := {W ∈ Rd1 ×d2 | kW> k2,∞ ≤ α} with fixed Z(i) satisfying Z(i)>	≤
1, Lemma 4.5 gives us such a cover.
First let us build a cover for one Transformer layer with inputs Z(1), . . . , Z(m). We will begin with
creating an εV -cover WcV for the function class of linear transformations given by WV : {W ∈
Rd×k,W>2,∞ ≤ α,kWk2 ≤ s} and εQK-cover WcQK for WQK := {W ∈ Rd×d,kWk2,∞ ≤
β, kW k2 ≤ r} and inputs Z(1), . . . , Z(m). For each pair of WcV ∈ WcV and WcQK ∈ WcQK, we
construct an εC -cover WcC (WcV , WcQK) for WC : {W ∈ Rk×d, kW k2,∞ ≤ γ, kW k2 ≤ c} and
inputs σ Πnorm f Z(i);WcV,WcQK	m . Our final cover is
Wc:=	(WcV,WcQK,WcC)	: WcV	∈WcV,WcV	∈WcV,WcC	∈WcC(WcV,WcQK)	.
Using Lemma A.15, We can show that W is an ε-cover for g(∙; {Wv, WQK, WC}) and inputs
Z(1) , . . . , Z(m) where
ε = εC + 2Lσ csεQK + LσcεV.
Using Lemma 4.5, the size of the cover is,
・ ■—∙.	・■— ・・■—	I—	—	—	・
|Wc| ≤ |WcV||WcQK| max	WcC(WcV,WcQK)
WcV ∈WcV
WQK ∈WQK
.---
=⇒ log |Wc| .
log(mT).
We are now ready to inductively construct a cover for the deeper network. Suppose
We have a ε(i)-cover W1:i for g(∙; W 1:i) on X⑴，…，X(m). We show how to con-
struct an ε(i+1)-cover for g(∙; W 1:i+1). For every element W1:i ∈ W1:i we construct a
ε(Ci) + 2LσBC(i)BV(i)ε(Qi)K + LσBC(i)ε(Vi) -cover
for the transformer layer (as above) on
inputs ng(X(j); WW1:i)o	. Consider the cover
WW1:i+1 := (WW1:i, WW(i)) : WW1:i
---∙-l . .- --- < .- ∖ --- . --- -l - 1
∈ WW1:i,WW(i) ∈ WWi(WW1:i)	.
By Lemma A.15, this gives,
ε(i+1) = LσBC(i)BV(i)(1 + 4BQ(i)K)ε(i) + ε(Ci) + 2LσBC(i)BV(i)ε(Qi)K + LσBC(i)ε(Vi).
The size of the cover is
|WW1:i+1| ≤ |WW1:i|	max	WWi(WW1:i) .
CLi∈Cl"	I
Inductively applying this, we get
ε(L+1) = XL	YLσBC(j)BV(j)(1+4BQ(jK) )	ε(Ci) + 2LσBC(i)BV(i)ε(Qi)K + LσBC(i)ε(Vi)
i=1 j<i
25
Under review as a conference paper at ICLR 2022
+
2LσBC(i)BV(i)ε(Qi)K+LσBC(i)ε(Vi)
where αi = Qj<i LσBC(j)BV(j)(1 + 4BQ(jK) ).
The size of the cover is
log |Wc1:L+1
L
≤X
i=1
k2B∞⑴2	d2B∞(i)2	d2B∞(i)2∖
g(i)2	+ N) 2- + g(i)2	I
εV	εQK	εC
log(mT).
Notice that the layer-norm maintains the norm bound on the inputs. Lastly, we need to cover the
linear layer on the [CLS] token and compose it with the cover of g1:L (as before). Using Lemma
A.10 and A.16, we can get the final ε-cover C with
L
ε=BwXαi ε(Ci)+2LσBC(i)BV(i)ε(Qi)K+LσBC(i)ε(Vi) +εw
i=1
and size
log |C| .
Bw2 log(m)
ε2w
L
+X
i=1
k2B∞(i)2
d2B∞κ ⑴2
+ ；
d2BC∞(i)2
εC)2
log(mT)
+
Using Lemma A.8, the size of the cover for fixed ε gives us the desired result.
□
B Proofs for sparse function representation
B.1	Setup
Reductions from Boolean functions to Transformers. In order to establish our function ap-
proximation results, we must first define a canonical mapping between length-T Boolean strings
b ∈ {0, 1}T and Transformer inputs X ∈ RT×d. The key point (which has also been considered
since the inception of the Transformer (Vaswani et al., 2017), and continues to be a crucial consider-
ation in practice (Dosovitskiy et al., 2020)) is that the network’s permutation-equivariant symmetry
needs to be broken by assigning different embeddings to different indices of b. There are several
possible natural choices here, which are all of practical interest:
•	Deterministic positional embeddings. Fix positional embedding matrices P ∈ RT ×d , E ∈
R{0,1}×d, and a special direction v[CLS] ∈ Rd, such that the T + 3 vectors {Pt,:}tT=1 ∪
{Ej,Jj∈0,1 ∪{v[CLS] } are an approximately orthonormal basis for Rd (see below). The
input to the Transformer is then X = Eb + P, where Eb ∈ RT×d such that [Eb]t,: = Ebt,:
for each t ∈ [T]. In the ftf-scalar formulation, we choose the auxiliary input x[CLS] to
be the constant vector v[CLS] . This closely matches applications of Transformers in NLP
(Vaswani et al., 2017).
•	Trainable positional embeddings. Like the above, but P is a trainable parameter; we still
require approximate orthogonality of {Ej,.}j∈0,1 ∪ {v[cls] }. Itis also possible to consider
the case where E and v[CLS] are trainable (matching the way token embeddings are trained
in practice). This becomes important in the regime of large vocabulary sizes that require
embeddings to capture shared information between tokens; however, this is not necessary
for our constructions, as we limit our consideration to binary tokens. This simplifies our
constructions and improves statistical rates; additionally, it is a popular and well-studied
alternative (Vaswani et al., 2017; Devlin et al., 2018; Radford et al., 2018; 2019; Brown
et al., 2020).
•	Bag of vectors. Fix a matrix V ∈ RT ×d with approximately orthogonal rows (like the
deterministic P), but choose the Transformer input
X := V diag(b).
26
Under review as a conference paper at ICLR 2022
This construction replaces positional embeddings with positional “indicator vectors” which
can be swapped between any of the Transformer’s input positions. It has the advantage of
being symmetric with respect to permutation of the Transformer’s input positions: it turns
out that
ftf-scalar(V diag(b)) = ftf-scalar(V Πdiag(b)),
for any T × T permutation matrix Π. It is also the most natural construction when consider-
ing the composition of sparse Boolean functions across multiple layers: a layer can output
combinations of the basis rows vi for further function composition, like Boolean gates.
Approximately orthonormal basis. Each of the Boolean function approximation constructions
will rely on a basis set of vectors, which will be used as positional embeddings (or the variable
indices in the bag-of-vectors construction). We will fix a set of approximately orthonormal vectors
{vi : kvik = 1}iT=0 1 in Rd: for each i 6= j, we have |vi>vj | ≤ ∆. When ∆ = 0, the maximal T0
for which such a set exists is d; for ∆ ∈ (0, 2), the Johnson-LindenstraUss lemma (Johnson et al.,
1986) implies that the maximal set of is of size exp(Θ(d∆2)). For given choices of d, ∆ and a
maximal {v1, . . . , vT0 }, our construction is valid for contexts of length T ≤ T0. For the special
vectors e0, e1, v[CLS] , we will assume that these are exactly orthogonal to the vi and each other, so
that the Vi must be a basis in dimension d -1 or d - 3. This is for clarity only- it reduces the number
of error terms to propagate through the analysis.
Self-attention block. In each construction (which specifies an input X ∈ RT ×d, we will specify
the parameters WQ, WK, WV , WC, w = e1 ofa scalar-output Transformer ftf-scalar, which takes an
input X ∈ R(T+1)×d; the auxiliary token input will be the constant vector x[CLS] := v[CLS] ∈ Rd.
The internal activation function σ is chosen to be the identity. Summarizing, the functional form of
ftf-scalar ∈ Ftf-scalar in these constructions is
ftf-scaiar(X； Wq, WK, WV, Wc, eι) = SOftmaX (v>Cls] WQWKXT) XWVWCeι.
In the intermediate lemmas, it will also be useful to consider the corresponding attention head output
ftf-head(X； Wq, Wk, Wv, WC) = SOftmaX (v>1s] WQWKXT) XWVWc,
and its projections ftf-head ◦ Πdproj onto the first dproj coordinates.
Feedforward networks. We establish some notation for feedforward networks. An L-layer feed-
forward network, with activation function σ : R → R and dimensions d1 , . . . , dL+1 , is parame-
terized by weight matrices Wi ∈ Rdi+1 ×di, and maps x ∈ Rd1 to y ∈ RdL+1, by the iterative
equations
y1T := σ(xTW1),
yiT+1 := σ(yiTWi),	i = 1, . . . ,L - 1,
fmlp(x; W1, . . . ,WL)T = yT := yLTWi.
When dL+1 = 1, we will use the notation w instead ofWL. It will be convenient to incorporate bias
weights by introducing an extra input coordinate Wi ∈ R(di+1 +1)×di, and augmenting the linear
function accordingly:
yiTWi 7→ [yiT 1]Wi.
Self-attention composed with a feedforward network. The full definition of the Transformer
layer composes a self-attention layer (ftf-layer : RT ×d → RT ×d) with a position-wise feedforward
network (fmlp : Rd → Rd). We will use this combination of modules to establish our function
approximation results: ftf-layer acts as a sparse bottleneck, while fmlp approximates an arbitrary
function of the selected coordinates. For our single-layer constructions, it is most convenient to
establish notation for a scalar-output Transformer with a feedforward network. To this end, define
Ftf+mlp to be the function class with the same ScOre, NOrm, φin functions as in Ftf-scalar (thus, the
same parameters WQ, WK, WV), with identity activation function, buta feedforward neural network
replacing the linear。。玳 and w. Concretely, with L = 3 and the ReLU activation function (•)+，
Ftf+mlp contains functions of the form
ftf+mip(X ； θ)= ((yτW1)+W2)+ w,
y = softmax (VTCLs] WQWKXT) XWVWCw,
with parameters θ := (WQ, WK, WV, WC, W1, W2, w).
27
Under review as a conference paper at ICLR 2022
Multiple self-attention heads. The final component we will need for the function approximation
setup is multi-headed self-attention. We will extend the definition of the single-headed ftf-head to
HH
ftf-heads X;nWQ[h],WK[h],WV[h],WC[h]o	:= X ftf-head X;WQ[h],WK[h],WV[h],WC[h],
and substitute this definition into ftf+mlp when discussing a multi-headed construction.
Classes and properties of Boolean functions. We will call a Boolean function f : {0, 1}T → Y
I-sparse if it only depends on a fixed subset I ⊆ [T] of its inputs:
bi = b0i ∀i ∈ I =⇒ f (b) = f (b0).
Overloading notation, if I = s, we will also call f s-sparse. We will call an I-sparse Boolean
function f symmetric if its value is invariant under permutation of the indices in I:
|{i ∈ I : bi = 1}| = |{i ∈ I : b0i = 1}| =⇒ f(b) = f(b0).
Further, we will call an I-sparse real-valued symmetric Boolean function f : {0, 1}T → Y mono-
tone if f(b) is monotonically increasing in r := |{i ∈ I : bi = 1}|. If, for some γ > 0, it holds
that f(r + 1) ≥ f(r) + γ for each r = 0, . . . , s - 1, we call f γ-strictly monotone. A vector-valued
I-sparse Boolean function f : {0, 1}T → Rdf is γ-injective if
kf(b) - f(b0)k∞ ≥ γ
for each b, b0 that differ at some position i ∈ I; f is called B-bounded if kf (b)k∞ ≤ B for all
b ∈ {0, 1}T.
Uniform approximation. For some ε ≥ 0 and a function f : {0, 1}T → Rd, we say that fb ∈ F
ε-uniformly approximates f under the mapping b 7→ X(b) if
fb(X(b))-f(b)	≤ε,	∀b∈ {0, 1}T.
B.2	Results
We give an overview of the function approximation results under each input mapping b 7→ X (b), as
a multi-part proposition:
Proposition B.1 (Sparse variable creation with Transformers). The function classes
Ftf -scalar, Ftf+mlp contain the following classes of sparse Boolean functions:
•	Deterministic positional embeddings: For any I, Ftf-scalar can approximate a particular
monotone symmetric I-sparse f, with Transformer weight norm bounds from the real-
valued construction in Lemma B.2. Ftf+mlp with 1 head can exactly represent any sym-
metric s-sparse f, with the same bounds on Transformer weight norms, and feedforward
network weight norms scaling as O(poly(s)). Ftf+mlp with s heads can exactly represent
any s-sparse f, with Transformer weight norm bounds from the vector-valued construction
in Lemma B.2, and feedforward network weight norms scaling as O(Poly(S) ∙ 2s).
•	Trainable positional embeddings: For any I, Ftf-scalar can approximate a particular mono-
tone symmetric I-sparse f, with positional embedding and Transformer weight norm
bounds from the real-valued construction in Lemma B.3. Ftf+mlp with 1 head can ex-
actly represent any symmetric s-sparse f, with the same bounds on P and Transformer
weight norms, and feedforward network weight norms scaling as O(Poly(s)). Ftf+mlp with
s heads can exactly represent any sparse f, with P and Transformer weight norm bounds
from the vector-valued construction in Lemma B.3, and feedforward network weight norms
scaling as O(Poly(S) ∙ 2s).
•	Bag of vectors: For any I, Ftf-scalar can approximate a particular monotone symmetric
I-sparse f, with Transformer weight norms from Lemma B.4. Ftf+mlp with 1 head can
represent any symmetric S-sparse f, with the same Transformer weight norm bounds, and
feedforward network weight norms scaling as O(Poly(S)). Ftf+mlp with 1 head can also
exactly represent any S-sparse f, with the same bounds on Transformer weight norms, and
feedforward network weight norms scaling as O(Poly(S) ∙ 2s).
28
Under review as a conference paper at ICLR 2022
The formal statements are obtained by (γ∕4)-uniformly approximating a γ-strictly monotone or
γ-injective function with self-attention alone (Lemmas B.2, B.3, B.4), then applying a robust uni-
versal function representation construction (Lemmas B.5, B.6) appropriately. They are organized as
follows:
Lemma B.2 (Deterministic P, no MLP). Suppose X(b) = P+Eb with deterministic P. LetI ⊆ [T]
such that |I| = s ≤ d, k, and ∆ < 1/s. Then, for all 0 < γ ≤ 1, there exists a 1-bounded, (2/s)-
strictly monotone symmetric I -sparse Boolean function gI : {0, 1}T → R and Transformer head
parameters such that ftf -scalar (X (b); WQ, WK, WV , WC, w = e1) (γ /4)-uniformly approximates
gI . The norms satisfy
log ( 8T )
∣∣Wq∣∣f ≤ -ta⅛,	kWκIIf ≤ s,	kWvIIf ≤ 2,	IlWCIlF ≤ 1.
1 - s∆
Also, there exists a 1-bounded, 2-injective I -sparse Boolean function gI0 : {0, 1}T
s-headed Transformer parameters such that ftf-head X (b); WQ[h] , WK[h] , WV[h] , WC[h]
uniformly approximates gI0 . The norms of each head satisfy
→ Rs and
}h=i)。πs
WK[h]F ≤ 1,
WV[h]F≤2,	WC[h]F ≤ 1.
Lemma B.3 (Trainable P, no MLP). Suppose X(b) = P + Eb with trainable P. Let I ⊆ [T]
such that |I| = s ≤ d, k. Then, for any 0 < γ ≤ 1, and with the same gI as in Lemma B.2, there
exists P and Transformer head parameters such that ftf -scalar (X (b); WQ, WK, WV, WC, w = e1)
(γ∕4) -uniformly approximates gɪ. The norms satisfy
IIP>∣∣2,1 ≤ s,	kWQ∣F≤ log (8T
IWK IF ≤ 1, IWV IF ≤ 2, IWC IF ≤ 1.
Also, for the same gI0 as in Lemma B.2, there exists P and s-headed Transformer parameters such
that ftf-head X (b); WQ[h], WK[h], WV[h] , WC[h]	。 Πs uniformly approximates gI0 . The norms of
each head satisfy
∣∣p>∣∣2,1 ≤ s, II咕IIF ≤log (8T
WK[h]F ≤1,	WV[h]F ≤ 2,	WC[h]F≤1.
Lemma B.4 (Bag of vectors, no MLP). Suppose X(b) = V + diag(b). Let I ⊆ [T] such that
|I| = s ≤ d, k, and ∆ < 1/s. Then, for all s∆ < γ < 1, there exists an s-bounded, (1/s)-
strictly monotone symmetric I -sparse Boolean function gI : {0, 1}T → R and Transformer head
parameters such that ftf -scalar (X (b); WQ, WK, WV, WC, w = e1) (γ /4)-uniformly approximates
gI. The norms satisfy
log(83卢)
kWQkF ≤---1 7 ʌ---,	IIWKIIf ≤ S + 1,	IIWVIIf ≤ 2s,	IIWCIIf ≤ s∙
1 - s∆
Also, there exists a 1-bounded, (1/s)-injective I -sparse Boolean function gI0 : {0, 1}T → Rs and
Transformer head parameters such that ftf -head (X (b); WQ, WK, WV, WC) 。 Πs uniformly approx-
imates gI0 . The norms satisfy the same bounds as above.
Lemma B.5 (Monotone to symmetric functions via MLP). Let f : {0, 1}T → R be any real-valued
symmetric s-sparse Boolean function with index set I. Let WQ, WK, WV, WC be the parameters of
a function
ftf-head(X; WQ, WK, WV, WC ) ：= SOftmaX (v%〕WQWKX >) XWV WC,
and let Π1 : Rd → R be the projection onto the first coordinate. Suppose that under some mapping
b → X(b), ftf-head。∏s (γ∕4)-uniformly approximates a B-bounded Y-StrictIy monotone Sym-
metric I -sparse Boolean function g : {0, 1}T → R, for some γ. Then, there exists a function
ftf+mlp ∈ Ftf+mlp with the same weights WQ , WK, WV, WC, and 3-layer feedforward network
weights W1 , W2 , w, such that
ftf+mlp(X(b)) = f(b),	∀b∈ {0, 1}T,
29
Under review as a conference paper at ICLR 2022
with dimensions (d2, d3) = (4(s + 1), 2(s + 1)) and weight norms satisfying
kWιk∞ ≤ 8maX(1,B),	kW2k∞ ≤ 8S，	kwk∞ ≤ ° mχ TIf (b)∣.
γ	γ	b∈{0,1}
Lemma B.6 (Injective to arbitrary functions via MLP). Let f : {0, 1}T → R be any real-valued
s-sparse Boolean function with index set I such that |I| = s ≤ d. Let WQ , WK , WV , WC be the
parameters of a function
ftf一head(X; Wq, WK, Wv, WC) ：= SOftmaX (入〕WQWKXT) XWVWc,
and let Πs : Rd → Rs be the projection onto the first s coordinates. Suppose that under some
mapping b → X (b), ff-head o∏s (γ∕4)-uniformly approximates a Y-injeCtivefunction g : {0,1}T →
Rs satisfying kg(b)k∞ ≤ B. Then, there exists a function ftf+mlp ∈ Ftf +mlp with the same weights
WQ, WK, WV, WC, and 3-layer feedforward network weights W1, W2, w, such that
ftf+mlp(X(b)) = f(b),	∀b∈ {0, 1}T,
with dimensions (d2, d3) = (4s2s, 2 ∙ 2s) and weight norms satisfying
kW1k∞ ≤ 8maX(1,B),	kW2k∞ ≤ 8S,	kwk∞ ≤ b max T If (b)∣.
γ	γ	b∈{0,1}
B.3 Useful lemmas
We will use a construction which approximates a “hard selection” of s indices using the softmax
mixture; for this, we will need to quantify the approximation error when the inputs to the softmax
function are bounded.
Lemma B.7 (Softmax truncation). Let z ∈ (R ∪ {-∞})T such that zt ≥ Rfor each 1 ≤ t ≤ s,
and zt ≤ 0for each s + 1 ≤ t ≤ T. Define z0 ∈ (R ∪ {-∞})T so that zt0 = zt for 1 ≤ t ≤ s, and
Zt = -∞ for s + 1 ≤ t ≤ T. Then, letting e-∞ = 0 in the definition of softmax(∙), we have
0	T - s	2T
ksoftmaχ(z)- SOftmaX(Z)kι ≤2 sexpR < eχp(R).
Proof. We have
ksoftmaX(ZO)- SOftmaX(Z)kι = XXeχp(Zt) (IT^pFy - IT^M)+ X ιexp(∣⅛.
The first summation is equal to
I	1t exp(Z0) V T - S
1t exp(Z) — s exp(R),
while the same upper bound holds for the second summation, since each term is at most S exp(R). □
Our results on approximating arbitrary sparse Boolean functions will depend on a generic con-
struction for robustly approximating an arbitrary function f : Rd → R with a feedforward neural
network. For simplicity of presentation, we use a standard6 7 3-layer ReLU network construction,
which exactly represents a piecewise constant function in specified regions.
Lemma B.8 (Exact function representation with a 3-layer ReLU net). Let f : Rdf → R, and
let x1 , . . . , xn ∈ Rdf such that kxi k∞ ≤ B for each i ∈ [n], kxi - xj k∞ ≥ 4δ for each i 6=
j ∈ [n]. Then, there is a 3-layer feedforward network with ReLU activations, with parameters
W1 ∈ R(df +1)×d2, W2 ∈ R(d2+1)×d3,w ∈ Rd37, such that
fmlp (xi + Z) = f (xi )
for all i ∈ [n] and kZk∞ ≤ δ, where ReLU(x) := x+ = max(0, x) is applied entrywise, with
d2 = 4ndf ,	d3 = 2n,
kW1k∞ ≤ mαχδιB), kW2k∞ ≤ df,
kwk∞ ≤ maxIf(xi)I.
i∈[n]
6For example, this follows from the discussion in Chapter 4 of (Nielsen, 2015).
7Here, W1 , W2 have bias terms; w does not.
30
Under review as a conference paper at ICLR 2022
Proof. First, we construct a one-dimensional “bump” function basis, and propagate the Lipschitz
constants. A threshold function with a linear “ramp” of width δ can be obtained from a linear
combination of 2 ReLU functions:
(0	X ≤ 一δ
Vδ(x) ：= (x∕δ + 1)+ — (x∕δ)+ = < x∕δ + 1 —δ ≤ X ≤ 0 .
[l	x ≥ 0
Next, we construct the bump function
ψδ (X) ：= νδ (X) — νδ (2δ — X).
By this construction, We have ψδ (x) = 1 for 0 ≤ X ≤ 2δ and ψδ (x) = 0 for X ≤ —δ and X ≥ 3δ,
interpolating linearly on [-δ, 0] and [2δ, 3δ]. Next, define
ψδ(X; X0) ：= ψδ(X — X0 + δ)
7 + 2)+
: + 1)+
—
—
x0 一 X_LQ , f x0 一 x . -l ʌ
-ɪ+2)++l -ɪ+1)+
so that ψδ (X; X0) = 1 for |X — X0 | ≤ δ, ψδ (X; X0) = 0 for |X — X0 | ≥ 2δ.
We construct the first layer W1 ∈ R(d+1)×(4nd) using these bump functions: indexing the 4nd
dimension by (h ∈ [4], i ∈ [n], j ∈ [d]), We construct
r 11	]	r 11 一
[W1]:,(1,i,:) ：= 一Xi + 2 ∙ 1>	,	[W1]:,(2,i,:)：=	一詈δ+ 1>	,
r 一 11	"I	r 一 11 一
[W1]:,(3,i,:) ：= Xi +2 ∙ 1>	,	[W1]:,(4,i,:)	：=	Xi +δ 1> ,
so that
([x 1]> [[W1]j,(l,i,:) [W1]j,(2,i,:) [W1]j,(3,i,:) [W1]j,(4,i,:)])+ [1 一 1 一 1 1]> = ψδ(XIXij).
The second layer is used to construct n activations Which are indicators of Whether X is in the neigh-
borhood of each Xi. For each Xi, We Will simply average the df one-dimensional indicators for each
coordinate, and implement a threshold function νδ∕df (1 一 x). We choose W2 ∈ R(4ndf+1)×(2n),
With the 4ndf + 1 dimension indexed by (h, i,j) and an extra bias dimension ⊥, and the 2n dimen-
sion indexed by (h0 ∈ {1, 2}, i0 ∈ [n]) so that
W2](h,i,:),(h0,i0) ：= [1 一 1 一 1 1]h ∙ l[i = i0] ∙ δ ∙ 1,
[W2]⊥,(1,i0) ：= 1 一 f,	[W2]⊥,(2,i0) ：= 一-δf .
Finally, the third (output) layer w ∈ R2n, With dimensions indexed by (h ∈ {1, 2}, i ∈ [n]),
multiplies the indicators of each Xi by the desired f(Xi):
W(1,i) ：= f(Xi),	W(2,i) ：= 一f(Xi).
For any X0 ∈ Rdf, let Bδ(X0) be the set of X such that kX 一 X0k∞ ≤ δ. By this construction, for
each X ∈ Bδ(Xi), we have f (x) = Xi, as required.	□
Note that We use 3-layer ReLU netWorks for function approximation in order to minimize the intro-
duction of unnecessary notation. Some remarks:
•	It would be routine to replace this construction with any architecture which can represent
an arbitrary function approximately (Hornik et al., 1989; Cybenko, 1989); this includes the
2-layer feedforward networks (and nonlinear activations other than the ReLU) which are
typically used by Transformers in practice.
•	It is possible to embed this construction in ftf+mlp with a 2-layer ReLU network, by using
(WC , W1 , W2 ) and introducing a nonlinearity after WC, without changing the results.
•	When df = 1, W2 is unnecessary (one can represent f directly using the bump function
basis).
31
Under review as a conference paper at ICLR 2022
B.4 Proofs
Throughout the constructions in each case, we will refer to standard coordinate bases in several
spaces:
•	E0, E1 ∈ Rd denote the embeddings of the 0, 1 tokens E0,: , E1,:.
•	e(1k), . . . , e(kk) denotes the standard basis in Rk.
•	ei(d) denotes the standard basis in Rd.
•	e(1T), . . . , e(TT), e([TCL)S] denotes the standard basis in RT+1 with the special [CLS] index.
•	Recall that the vi form a ∆-approximate orthonormal basis forRd, v[CLS] , e0, e1 are exactly
orthogonal to each of them as well as each other, and d is chosen such that these conditions
can be met.
Let n(i) be a unique bijection between I and [s]. Let vI := Pi∈I vi.
Approximate vector equality. We will use u ≈ε v to denote that two vectors u, v ∈ Rdu satisfy
ku-vk∞ ≤ ε.
Proof of Lemma B.2. We construct attention heads such that the softmax mixture always selects the
indices in I.
Single head, deterministic P. We seek to approximate the 1-bounded, (2/s)-strictly monotone
function
1 X Xi,
s i∈I
where χi = +1 if bi = 0 and -1 if bi = 1. Set
WQ :=Rv[CLS]e(1k)>,	WK := vIe(1k)>, WV := (E0 - E1)e(1k)>, WC := e(1k)e(1d)>,
where R will be chosen later. Then, by approximate orthogonality,
v[>CLS] WQWK>X> = v[>CLS] WQWK>(P + Eb)> = v[>CLS] WQWK>P> ≈Rs∆ R X ei(T)>.
i∈I
By Lemma B.7,
Softmax (Es] WQWKXT) -S X e(T)>	≤	(R-T2Rs∆).
i∈I	1
Finally, we have
XWVWC = EbWV WC = X χiei(T) e(1d)>,
i∈[T]
so that by Holder,s inequality,
ftf-head(X) ◦ ∏1 = SoftmaX (VTCLS] WQWKXT) XWVWCe1d) ≈ -2T— 1 XXi
exp(R-2Rs∆) s
i∈I
To get (γ∕4)-uniform approximation, We choose
Multiple heads, deterministic P . For h = 1, . . . , s, and the same R as above:
WQ[h] := Rv[CLS]e(1k)T,	WK[h] := vn-1(h)e(1k)T, WV[h] := (E0-E1)e(2k)T, WC[h] := e(1k)e(hd)T.
32
Under review as a conference paper at ICLR 2022
This is the same construction as above, but each head only selects one of the coordinates in I. Thus,
by the same analysis,
ftf-head(X) ◦ ∏s ≈	2T	yX XieT)、.
exp(R-2Rs∆)	χi n(i) .
i∈I
This function is clearly 1-bounded and 2-injective.	□
Proof of Lemma B.3. The constructions closely follow Lemma B.2, but are simpler.
Single head, trainable P. For each i ∈ I, set the trainable positional embeddings to be
v1 i ∈ I
0 otherwise
Set
WQ :=Rv[CLS]e(1k)>,	WK :=	v1e(1k)>,	WV	:=	(E0	-	E1)e(1k)>,	WC	:= e(1k)e(1d)>.
Now, we have (with equality)
v[>CLS]WQWK>X> =RXei(T)>,
i∈I
so that Lemma B.7 gives
SOftmaX (v>cLs] WQWKXT) - S X e(T)T	≤ eχ2TR).
i∈I	1
Like before, we have
ftf-head(X) ◦ ∏1 = SOftmax (VTCLS] WQWKXT) XWVWCeId) ≈S X Xi
i∈I
To get (γ∕4)-uniform approximation, We choose
R = bg (8T
Pi,: :=
Multiple heads, trainable P. For each i ∈ I, set the trainable positional embeddings to be
e(d)	i ∈ I
en(i) i ∈ I
.
0 otherWise
For h = 1, . . . , S, and the same R as above:
WQ[h]	:= Rv[CLS]e(1k)T,	WK[h]	:=	e(hd)e(1k)T,	WV[h]	:=	(E0-E1)e(1k)T,	WC[h]	:=	e(1k)e(hd)T.
This is the same construction as above, but each head only selects one of the coordinates inI. Thus,
by the same analysis,
ftf-head (X) ◦ πs ≈ eχpτπ7 X Xiendi).
i∈I
□
Proof of Lemma B.4. This input mapping does not use position embeddings, and does not need
multiple heads to implement arbitrary (non-symmetric) functions. The constructed monotone and
injective functions are slightly different, but the proof strategy is very similar. The key difference is
that the softmax mixture is uniform only on the positions i ∈ I Where bi = 1.
Bag of vectors, scalar output. The function We Will approximate is defined as folloWs:
gI(r) :=r-s,	where r=X bi,
i∈I
S = |I|.
33
Under review as a conference paper at ICLR 2022
Note that this function is (1/s)-strictly monotone, and has absolute value bounded by s. Set
WQ := Rv[CLS] e(1k)> ,	WK := (vI +
v[CLS] )e1	,
WV := Xvie(nk()i>) - v[CLS] Xe(nk()i)! , WC := Xe(nk()i)e(1d)>,
i∈I	i∈I	i∈I
where R will be chosen later. Then, by approximate orthogonality,
v[CLS] WQWK>X> ≈Rs∆ R v
[CLS] + X biei(T)> ,
i∈I
so that by Lemma B.7,
SOftmaX (v>l s1 WQWK X >)———(e(T)> + X	T)>' || ≤ —/' r, ʌ`.
、[CLS] Q K - r +ι<[cLS] T i i 力J】一 exp(R - 2Rs∆)
Finally, we have
XWVWce1d = -se[TLs] + X bi ∙ v>vι ∙ el T ≈s∆ -se[TLs] + X bie(T),
i∈[T]	i∈I
so that
|ftf-head (X) ◦ Π1- gI (r)| ≤
softmaχ (vbs] wqWKX>) —+i ee[TL>] + X bie(T)T
r + 1	i∈I
(∣∣XWVWce(1d)∣∣∞+S∆
1
+ Ilsoftmax (v>cLS] wqwKx t)∣∣1 (S∆)
≤	2Ts(1 + ∆)	ι ∆
≤ exp(R - 2Rs∆)+ S .
To get (γ∕4)-uniform approximation, We choose
log (8T^
R _ γ γ γ-$△
=	1 - s∆
Bag of vectors, S-dimensional output. We use the same construction as above, except
W X (k)	(d)T
Wc :=	en(i)en(i) .
i∈I
This Will alloW us to approximate the function
gI (b) = r+彳 X(bi- 1)en(i)，
r + i∈I
Which is (1/S)-injective and has absolute value is bounded by 1. Then, for each i ∈ I, We have
XWVWCe(d = -e[TLs] + VTvI ∙ e(T) ≈s∆ -e[TLs] + bie(T).
Repeating the above analysis for each coordinate, We have
ftf-head (X) ◦ Πs ≈ε gI0 (r),
Where a slightly tighter bound
=2T (1 + s∆) δ
ε	exp(R — 2Rs∆) + S
comes from the fact that ∣∣XWVWCei(d) ∣∣	is noW bounded by 1 instead of S. The previous choice
of R suffices for (γ∕4)-uniform approximation.	□
ProofofLemma B.5. This follows by instantiating Lemma B.8 with δ = γ∕8, df = 1,n = S + 1.
Notice that a (γ/4)-uniform approximation of a γ-strictly monotone function satisfies the conditions
needed for Lemma B.8.	□
ProofofLemma B.6. This follows by instantiating Lemma B.8 with δ = γ∕8,df = s,n = 2s.
Notice that a (γ∕4)-uniform approximation of a γ-injective function satisfies the conditions needed
for LemmaB.8.	□
34
Under review as a conference paper at ICLR 2022
C Experiment details
C.1 EMPIRICAL SCALING LAWS FOR LEARNING SPARSE AND GATES
In this section, we provide details for the empirical sample complexity scaling law experiments,
which are the main empirical verification of the log T dependence of the sample complexity arising
from the analysis.
Experimental setup. Synthetic tasks, parameterized by sample size m and context T , were gen-
erated by the protocol described in the main paper: in each trial, one of the T3 subsets of indices
was selected uniformly at random, under i.i.d. random inputs X 〜 Unif({0, 1}T). A 1-layer
Transformer network (with the scalar output convention) was trained with Adam (Kingma & Ba,
2014) and batch gradients of the cross entropy loss for binary classification. For each choice of
T ∈ {100, 150,. . . , 350, 400} ∪ {500, 600, 700, 800} and B ∈ {50, 60, 70, . . . , 200}, 50 inde-
pendent trials were performed (re-randomizing the dataset generation, random initialization, and
dropout masks). Cross-validation was performed on a holdout sample of size 104 every 10 itera-
tions. At the end of 1000 training iterations, the trial was counted as a success if the maximum
validation accuracy throughout training was greater than 0.99. (In 100% of runs, the training loss
was driven to < 10-4, with 100% training accuracy, within 1000 iterations.)
Architecture. Like (Chen et al., 2021a), our experimental setup is based on a popular PyTorch
implementation (https://github.com/karpathy/minGPT), with some optimizations for
faster 1-layer training and inference. This implementation includes widely-used architectural details
which deviate slightly from the theoretical presentation; refer to the referenced repository for details.
All hyperparameter settings left undiscussed are taken from the defaults in this codebase.
Hyperparameters. A fixed architecture was used (d = 64, 16 parallel heads), with trainable
positional embeddings initialized with Gaussian entries N(0, σ2), σ = 0.02, 3 input token embed-
dings (corresponding to 0, 1, [CLS]), and 2 output embeddings (corresponding to the possible la-
bels 0, 1). For regularization mechanisms, typical choices were used: 0.1 for {attention, embedding,
output} dropout; 10-4 weight decay. The Adam optimizer was instantiated with typical parameters
η = 10-3,β1 = 0.9, β2 = 0.999.
Results and discussion. Our findings are summarized by Figure 4, enlarged from the main paper.
On the left, the fraction of successful trials is plotted for T = {100, 200, 400, 800} with standard
errors derived from the normal approximation. Notice that in this “low-data” regime, Transformer
training is quite sensitive to the stochasticity in the experiments, including the training data sample
(which needs to be sublinear in the context size), so there is no sharp “phase transition”. On the
right, critical sample sizes are shown, defined as the smallest m for which the fraction of successes
was greater than 0.5, with standard errors derived from 50 bootstrap samples. We observe that this
architecture is able to solve this “planted sparse Boolean function” task with a sublinear scaling in
the sample size.
Infrastructure and computational costs. Each training run took at most 20 minutes on an
NVIDIA RTX A6000 GPU (with most of computation time spent on cross-validation). Although
these experiments are not in the same regime as state-of-the-art settings (such as BERT pretraining),
they require optimized GPU implementations to run in a reasonable timeframe (days, as opposed to
months).
C.2 Learning parities
A natural question for further exploration is whether Transformers can learn other Boolean func-
tions. In particular, the statistical probe presented for the s-way AND function is equally valid for
XOR. The latter is arguably more intriguing: parities are the basis elements in the monomial (i.e.
Fourier) expansion ofa Boolean function; (O’Donnell, 2021); furthermore, there are computational
hardness barriers; see the works cited in the main paper for further context.
35
Under review as a conference paper at ICLR 2022
empirical sample complexity
success probabilities
。-8
1 O
576.0 Λ uυra 而 >l」d
sample size
Figure 4: Enlarged plots from Figure 2, the main experimental validation of our theory.
086420864
2 11111
3SdEs -8€u
IO2	2 ×102 3 × 10¾ × IO2 6 ×102
context length T
Experimental setup. These experiments match the setting of the main AND experiments, except
for a few differences, which we enumerate here. Gradient-based training is done on streaming online
losses (so that there is no training/validation split), with batch size 2048, for 10000 iterations (since
parities take significantly longer to fit).
Discussion. The key observation of this exploratory experiment is to point out the phenomenon
that Transformer architectures can fit sparse parities at all (rather than to measure any particular
trend), pointing to computational mysteries discussed in the main paper. We do not present empirical
scaling laws for this set of experiments, as the experiments with small batch sizes are higher in
variance, and we are unable to scale this phenomenon (i.e. learn any parities) at context sizes above
T = 50.
D	Additional related work
In this section, we discuss some additional related work.
Attention-based architectures. Building upon the success of modern attention-based architec-
tures, a large body of work (e.g. Goyal et al. (2020; 2021), and the works cited within) has sought
to explicitly induce model sparsity and modularity in the architecture design. Our analysis is rele-
vant to any architecture that uses a softmax (or similar) bottleneck for statistical capacity, and could
inform design principles for norm-based capacity control of these architectures.
Expressive power of Transformers. Several works establish results on the representational power
of self-attention architectures in regimes where the statistical guarantees are necessarily weak or
vacuous (i.e. there are too many functions in the class). Dehghani et al. (2018); Yun et al. (2019);
Bhattamishra et al. (2020a;b) establish universal function approximation and Turing-completeness,
which have been known for previous architectures (Siegelmann & Sontag, 1995). Our work is a
significantly finer-grained analysis, in which we establish a hierarchy of function classes (indexed
by sparsity s) representable by these architectures, with tight (in terms of T) statistical guarantees.
Hron et al. (2020); Yang (2020) analyze properties of the kernels induced by Transformers at the
infinite-width limit.
Likhosherstov et al. (2021) analyze the sparsity patterns representable by a self-attention head, with
results superficially similar to ours: when the embedding dimension is at least logarithmic in the
context length, all sparse matrices can be approximately realized by an attention head. However,
their analysis is not about the capacity of the function class: it quantifies over the input X , and
holds the parameters (WQ , WK , . . .) to be constant (rather than vice versa). This finding serves
as an interesting complement to our result: even though the attention mixture weights can take on
exponentially many sparsity patterns for distinct inputs, the generalization error scales as log(T).
36
Under review as a conference paper at ICLR 2022
Interpreting attention mixtures. A line of empirical work (“BERTology”) has made progress on
understanding and interpreting state-of-the-art Transformer language models by examining the acti-
vations of their attention mechanisms (Clark et al., 2019; Tenney et al., 2019; Rogers et al., 2020). In
some cases, these works have found instances in which Transformers seem to have learned features
that are reminiscent of (sparse) hand-crafted features used in natural language processing, without
explicit supervision. Our analysis formalizes the intuition that self-attention heads can represent
sparse interactions within the context in a statistically meaningful way.
Other theoretical work on self-attention. Kerg et al. (2020) analyze self-attention and its ben-
efit for learning long-term dependencies by establishing gradient norms bounds and showing how
attention helps address the problem of gradient vanishing in recurrent networks. In contrast to our
results that analyze the statistical and representational properties of attention-based architectures,
this work focuses on the computational aspects of gradient-based methods on recurrent networks
with self-attention.
Synthetic experiments with Transformers. Power et al. (2021) train small Transformer networks
on synthetic algebraic tasks, and discover an abrupt phase transition from overfitting to correct
generalization similar to ours. Tay et al. (2020) propose some synthetic tasks for benchmarking
the ability of Transformer variants to capture long-range dependences. Chen et al. (2021a) present
a synthetic demonstration of extrapolation (inferring a maximum-reward path from random walk
observations) when using Transformers for offline reinforcement learning. Lu et al. (2021) probe the
transfer learning capabilities of pretrained Transformers, and consider some simple Boolean tasks.
Our experimental protocol of learning sparse Boolean functions provides a simple and fundamental
setting for elucidating computational and statistical properties of sequence modeling architectures.
37