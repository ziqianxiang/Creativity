Under review as a conference paper at ICLR 2022
Disentangling one factor at a time
Anonymous authors
Paper under double-blind review
Ab stract
With the overabundance of data for machines to process in the current state of
machine learning, data discovery, organization, and interpretation of the data be-
comes a critical need. Specifically of need are unsupervised methods that do not
require laborious labeling by human observers. One promising approach to this
enedeavour is Disentanglement, which aims at learning the underlying genera-
tive latent factors of the data. The factors should also be as human interpretable
as possible for the purposes of data discovery. Unsupervised disentanglement
is a particularly difficult open subset of the problem, which asks the network to
learn on its own the generative factors without any link to the true labels. This
problem area is currently dominated by two approaches: Variational Autoencoder
and Generative Adversarial Network approaches. While GANs have good perfor-
mance, they suffer from difficulty in training and mode collapse, and while VAEs
are stable to train, they do not perform as well as GANs in terms of interpretabil-
ity. In current state of the art versions of these approaches, the networks require
the user to specify the number of factors that we expect to find in the data. This
limitation prevents “true” disentanglement, in the sense that learning how many
factors is actually one of the tasks we wish the network to solve. In this work
we propose a novel network for unsupervised disentanglement that combines the
stable training of the VAE with the interpretability offered by GANs without the
training instabilities. We aim to disentangle interpretable latent factors “one at a
time”, or OAT factor learning, making no prior assumptions about the number or
distribution of factors, in a completely unsupervised manner. We demonstrate its
quantitative and qualitative effectiveness by evaluating the latent representations
learned on three benchmark datasets; DSprites, 3DShapes and CelebA.
1	Introduction
Deep learning models, which are now widely adopted across multiple A.I. tasks ranging from vision
to music generation to game playing (Krizhevsky et al., 2017; Oord et al., 2016; Mnih et al., 2015),
owe their success to their ability to learn representations from the data rather than requiring hand-
crafted features that older models required. However, this self-learning of abstract representations
comes at the known cost of the resulting representations being cryptic and inscrutable to human
observers. These learned representations might be dramatically affected by noise or spurious corre-
lations between the data and the labels - the representations might encode ’useless’ information from
the input data which is correlated with the corresponding label Geirhos et al. (2020). This makes
them more vulnerable to slight changes in the data distribution. A more comprehensive understand-
ing of the data down to essential indivisible factors would allow us to learn insights, sort and label
data, and facilitate downstream learning more efficiently. This also requires, critically, that these
factors be somehow interpretable as well. This approach, dubbed Disentanglement. requires that we
learn the data from its fundamental building block, so-called “disentangling” the true factors - the
factors of variation - that generate the data. If one were to learn these factors, one would learn all
possible causes of variation in a given dataset, and would in some sense gain complete understand-
ing of the underlying machinery, so to speak. In this work, we propose a new method that attempts
to learn the true disentangled factors one at a time, in a way that maintains interpretability for future
use.
0Code available at https://github.com/OATFactor/OATFactor
1
Under review as a conference paper at ICLR 2022
1.1	What is a disentangled representation?
While it is easy to informally talk of factors of variation, actually pinning down concrete definitions
of disentangled learning has proved a somewhat more difficult task. Though there is no commonly
accepted formalized notion of disentanglement or validation metrics (Higgins et al., 2018), recent
works have characterized disentangled representations, based on natural intuition, as one which
encodes each informative factor of variation of the data in separate latent dimenions (Bengio, 2013),
such that a change in single factor of variation produces a change in only a subset of the learned latent
representation. This is refered to as the separability (Do & Tran, 2020) quality of the representations,
also called disentanglement (Eastwood & Williams, 2018) and modularity (Ridgeway & Mozer,
2018). Separability ensures that the downstream tasks which depend on a certain subset of factors
are not affected by changes in other factors thus facilitating robust models Suter et al. (2019).
Some previous works (Ridgeway & Mozer, 2018; Eastwood & Williams, 2018) have suggested
that a single dimension of the learned disentangled representation should completely describe a
factor. This constraint along with separability entails a bijective mapping between the true factors of
variation and the learned representation. However, enforcing this constraint might not be conducive
to learn complex factors in a single latent variable Esmaeili et al. (2018). Hence in this work we
focus primarily on separability. For an in depth discussion on interpretability and how we enforce it
in the proposed work, we refer the reader to A.1.
1.2	What is missing from current approaches to solving disentanglement?
Due to the monumentous task of learning data without supervision, current methods have attempted
to tackle the subproblems of separability and interpretability, but few have successfully solved both
at once. Learning these disentangled representations in a semi-supervised setting Kulkarni et al.
(2015); Siddharth et al. (2017); Locatello et al. (2019) is a relatively easier task where additional
annotated data is available that give a strong backpropgation gradient to the network to guide it
to cleanly separate factors. However, if the main goal of disentanglement is to discover unknown
factors in large data corpuses, then it is not the right direction to require labels; rather the network
should learn in an unsupervised fashion. For unstructured data, all current state of the art approaches
based on unsupervised disentanglement rely on a deep generative neural network, built either on a
Variational Autoencoder or Generative Adversarial Network structure (See Related Work.) Many
popular VAE based methods are able to do well in separating factors out but make strict assumptions
on the number of factors and their structure and those that do, do not explicitly attempt to also make
the factors interpretable. Other methods based on GANs suffer from common issues from training of
GANs, but also tend to only learn a small subset of factors (See 2). Here we propose a novel hybrid
network that provides the stability and strong gradients of VAE training with the performance of
GAN training, without mode collapse.
Most current VAE-based state of the art methods make the assumption that there are a fixed number
of independent factors for all the data points in the dataset. However in real datasets, in addition
to the independent factors common to all points in the dataset, there might also be some correlated,
nuisance or noisy factors pertinent to specifically only certain data points. Moreover, approaches
rely on a heuristically chosen latent dimension d, sufficiently large to encompass all true factors.
However, this suffers from the same pitfalls for many of the same reasons that algorithms like k-
means cluster do, namely that given a new, unseen dataset, we do not necessarily know how many
independent factors there are. In fact, this is one of the main goals of disentanglement in the first
place, to glean insights about the data. Our method instead assumes that there is a set of independent
factors, and one of entangled nuisance and correlated factors, and separates them out. We then
iteratively learn to disentangle each factor of variation one at a time, such that the network learns on
its own different independently controllable factors thus removing a current hand-tuned roadblock
on the way to true unsupervised disentanglement. Second, in the same training loop, we ensure that
the disentangled latent representation follows intepretable latent code manipulation (Section. 1.1),
which says that a change in a single latent should make a distinct and noticeable change in the output
(Eastwood & Williams, 2018). Together, we demonstrate our proposed model is able to learn both
critical qualities of disentanglement, in a completely unsupervised manner.
2
Under review as a conference paper at ICLR 2022
1.3	Contributions
Our main contributions are as follows:
•	We introduce a new completely unsupervised generative neural model, One at a Time
(OAT) factor learning that combines the stability of VAE training with the accuracy of
GAN learning, which contains both a set of independent separable factors and a set of
entangled factors, that produces separable and interpretable latent factor codes
•	Our proposed model is the first unsupervised method that is capable of learning an arbitrary
number of latent factors via incremental unsupervised interventions in the latent space
•	We test and evaluate our algorithm on three datasets and across multiple metrics. Our em-
pirical results strongly suggest that our proposed method is effective in finding interpretable
factors and competitive with the most recent disentanglement strategies
2	Related Work
Various authors have attempted to learn unsupervised disentangled representations using genera-
tive models in recent years. State-of-the-art approaches for unsupervised disentanglement learning
can be broadly classified into two categories based on the type of generative model used; one via
Variational Autoencoders (VAE) (Kingma & Welling, 2014; Rezende et al., 2014), and another via
Generative Adversarial Networks (GAN) (Goodfellow et al., 2014).
2.1	Via Variational Autoencoders
Variational Autoencoders are a deep generative neural network model which learns an approximate
posterior distribution of the latent representations from the data, while trying to maximize the data
log-likelihood. They can be thought of as an autoencoder with an additional loss term that drives the
reconstructions to be closer together in latent space. VAEs assume that the data x is generated from a
set of latent features Z with a prior P(Z) according to the model pθ (x∣z)p(z). The top-down generator
Pθ(x|z) and a bottom UP inference network qφ(z∣χ) are modeled as multilayer neural networks and
trained jointly to maximize the marginal log-likelihood of the empirical distribution of the training
data. However, since the marginal log-likelihood is intractable, VAEs optimize a a tractable lower
bound L on the data log-likelihood pθ (x) called the Evidence Lower Bound (ELBO):
L = Eq(x)[Eqφ(z∣Xi)[log Pθ (XiIZ)] - KLSφ(ZE)IIp(Z))]	⑴
where q(x) = N PL δ(xi) is the distribution of the training data. The first term measures the
reconstruction error and the second term measures the distance between the approximate posterior
distribution qφ(ZIx) and the assumed prior distribution p(Z). Many state-of-the-art unsupervised
disentanglement methods extend the above objective function to impose additional constraints on
the structure of the latent space to match the independent prior assumption. β-VAE (Higgins et al.,
2017) and AnnealedVAE (Burgess et al., 2018) heavily penalize the KL divergence term thus forcing
the learned posterior distribution qφ(ZIx) tobe independent like the prior. Factor-VAE (Kim & Mnih,
2019) and β-TCVAE (Chen et al., 2019) penalize the total correlation of the aggregated posterior
qφ(Z). TC = KL(q(Z)II QiK=1q(Zi)) where the aggregated posterior is calculated as qφ(Z) =
Ep(χ)[q(Z∣Xi)] = NN PN=I qφ(Z∖xi) using adversarial and statistical techniques respectively. DIP-
VAE (Kumar et al., 2018) forces the covariance matrix of the aggregated posterior q(Z) to be close
to the indentity matrix by method of moment matching. Other works improved the performance
by making a specific design for discrete factors (Dupont, 2018; Jeong & Song, 2019); and use
optimization techniques based on annealing to encode information effectively in the discrete and
continuous factors.
2.2	Via Generative Adversarial Networks
Models based on GANs, explicitly condition the generator network with a set of independent latent
variables c (by concatenation with random noise Z), and train the generator to generate data which
has high mutual information with c. The most prominent work from the GAN family is InfoGAN
3
Under review as a conference paper at ICLR 2022
(Chen et al., 2016) which learns disentangled, semantically meaningful representations by maximiz-
ing a lower bound on the intractable mutual information between the conditioning latent variables c
and the generated samples G(z, c).
min max L(D, G) - λI (c; G(z, c))	(2)
where the adversarial loss is given by;
L(D,G) = Ex〜P, [log(D(x))] + Ez〜p(z),c〜p(c)[log(1 - D(G(z, c)))]	(3)
Assuming a perfect discriminator, the generator tries to minimize the Jensen-Shannon Divergence
between the true data distributing and the generated distribution. By changing the value of the
conditioning variables, the generator is forced to make distinct and noticeable changes in the data,
such that the value of the conditioning variables can be recovered from the generated data alone.
InfoGAN-CR (Lin et al., 2020) add a contrastive regularizer to the InfoGAN model, which is trained
to predict the changes in the latent space given only the pairs of images generated from the respective
latent codes. (Zhu et al., 2020) augment their objective with a similar self supervised learning task
to predict the dimension of the latent variable which is different from the pair of images. Some other
works based on GANs are (Jeon et al., 2019; Liu et al., 2020). (Liu et al., 2020) add orthogonal
regularization to encourage independent representations.
Alternatively, approaches based on the InfoGAN framework find interpretable factors of variation
through the Information Maximization principle (InfoMAX). However, in many cases these ap-
proaches suffer from mode collapse - a phenomenon that causes complete failure in training. Crit-
ically, GANs lack the ability to provide an explicit modeling of the latent space, as VAEs do (by
explicitly learning the parameters of an encoder distribution), as the entire GAN model relies on
implicit sampling. We aim to address this issue by our hybrid approach combining VAEs with GAN
components.
Suter et al. (2019) introduced the concept of interventions to study the robustness of the learned
representations under the Independent Mechanisms (IM) (Schoelkopf et al., 2012) assumption. In
this paper, however, we use the method of interventions while training the model to find disentangle
common factors from their entangled set. (Lee et al., 2020) use a VAE to disentangle and then pass
into a GAN to generate high-fidelity images; our approach differs in that the GAN component is
tightly integrated into our training loop, we perform interventions, and we split the latents into two
spaces (See Sec. 3). Hu et al. (2018) condition the latent space of entangled representations with a
disentangled code, which they learn in a supervised way using specific attribute discriminators. As
far as the authors are aware, we are the first to split the latent dimension into entangled and disentan-
gled in a completely unsupervised way, as well as the first to combine this with interventions, and
with incremental learning.
3	Our Method
3.1	Generative Disentanglement
Most previous works on disentanglement take a generative view of data where they assume that an
unknown generative model has produced the data. The data is assumed to be composed from an
a-priori set of factors ofvariation Gk(k = 1,…,K 0), which contains the different human-defined
atomic features that assume different values for specific instances. Here, K0 is assumed to be the
“true” number of independent factors of variation in the data. The data is then assumed to be gener-
ated by a two-step process. First, the values of the different factors of variation, g, are sampled from
a factorized distribution p(G) = QkK=1 p(Gk), where p can be any probability distribution. A gen-
erator function p(x|G) then maps the specific values, g, sampled from p(G), to a high-dimensional
datapoint x. Thus, p(x|g) describes a causal mechanism (Suter et al., 2019) invariant to changes in
the distributions p(gi). In this generative view the aim of disentangled representation learning then
becomes to uncover these “true” factors of variation from the data alone and re-encode each factor
as an independent latent representation.
We posit, however, that this generative view of disentanglement is too limiting and restrictive when
it comes to real-world data, as described in section 1.1. Even though the factors are independent
concepts, knowing what observation of x we obtained renders the different latent causes dependent
4
Under review as a conference paper at ICLR 2022
as certain factor realizations tend to co-occur more than others as a characteristic of the dataset. In
the proposed work we do not assume any particular factorization of the distribution of the factors
of variation and instead separate out the factors through iterative interventions from their entangled
counterpart as discussed in further sections. These interventions ensure that the dimensions of the
latent representation are independently controllable and have no causal effect on each other. More-
over, independent interventions also ensure that the confounding variables for the different factors
are integrated out. In addition to the generative factors of variation of the dataset, we also model a
second set of entangled, correlated nuisance factors pertinent to that particular data point. We train
our network to systematically discern the meaningful latent factors shared across the dataset from
the nuisance ones, both of which are important to maximize the log-likelihood of the data (See 3.3
for details).
3.2	Disjoint Entangled and Disentangled Latent Sets
In practise training a VAE, guided by the reconstruction loss to maximize the log-likelihood of
the data does not lead to disentangled representations, i.e., do not adequately separate out all of
the factors within one latent space z . While constraints can be imposed on the latent space to
enforce independence between the different dimensions of the learned representations, they can lead
to blurry reconstructions (Kim & Mnih, 2019) where some factors might be ignored altogether.
Because of these practical limitations, we propose a novel method of splitting the latent space z into
two disjoint sets: the disentangled set zι = {z1,z2,…，ZK} where Zk ∈ R and the entangled set
z2 ∈ Rd . Here, K is the number of factors that the network learns, and is not predetermined but
upper-bounded. This factorizes the generative model as: pθ (x, z) = pθ(χ∣z1,z2)p(z2) QK P(Zi).
During training, the factors common to the dataset are disentangled into z1 , while the nuisance
factors that are either correlated with the other factors in the dataset, or are noise factors that are
specific to individual samples, but are not representative of the dataset as a whole are encoded in z2
which has a higher encoding capacity. This ensures that the learning of the disentangled latent set
is not solely driven by the reconstruction cost. Our proposed model is the first VAE-based model
to incorporate this notion of two sets of latent variables, which were used in a somewhat similar
manner in GANs in (Chen et al., 2016; Lin et al., 2020).
3.3	Proposed Architecture
Our proposed model consists of an encoder and a decoder as in a standard VAE, modeled as deep
convolutional neural networks, with an additional discriminator network attached (see Fig. 1). The
encoder and decoder network parameterize the posterior distribution of the latent representation
qφ(z|x) and the generative model distributionpθ(x|z) respectively. In addition to the standard VAE,
we attach a discriminator network Dw to the output of the VAE decoder. This ensures that the distri-
bution of images generated by the intervention procedure (See Sec 3.4.2) is close to the distribution
of the images in the training set and thus changes in the latent space translate only to interpretable
changes. This discriminator distinguishes between the true images in the dataset and generated
images by the decoder by using the “real or fake” paradigm of GAN training.
Per the discussion in Section 3.2, the latent representation layer is divided into two sets, a disentan-
gled set z1 anda correlated entangled set z2. Thus the entire latent space is denoted as z = (z1, z2).
We use the boldface z to represent a vector-valued random variable. The encoder network encodes
the data into the two sets of latent variables qφ1 (z1 |x) and qφ2 (z2|x), with φ1 and φ2 sharing weights
for a set number of initial layers. The two sets are then passed through the decoder pθ (x|z) to yield
the reconstructed images X.
3.4	Training Procedure
Our training procedure consists of a pre-training phase and a main learning procedure: a pre-training
phase where we train aVAE to maximize the data log-likelihood, and the main training phase, where
the OAT factor learning is performed.
5
Under review as a conference paper at ICLR 2022
Figure 1: The complete OAT architecture. First, an input image, x, is passed into the VAE en-
coder, a deep convolutional neural network (CNN), and encoded via two multi-layered components
qφ1 (z1 |x) and qφ2 (z2|x), into two distinct latent spaces, a “factorized” or disentangled space, z1,
and a correlated space, z2, which is then decoded by a deep transpose convolutional neural network,
to produce a reconstructed image X. The insight of OAT training is that it may not be possible to
decorrelate all of the data for various reasons, so we first group the correlated latents into one space,
z2, and then “peel off” each independent factor one at a time. Next, an intervention is made on one
latent variable in the new disentangled space, z11, creating a new latent z1, which is passed through
the decoder to produce a new image X1. This factor-reconstructed X1 is then passed back through
the encoder to ensure the encoder learns how to encode that particular factor change into the same
intervention-altered disentangled latent z1. The factor-reconstructed values X1 are then passed into a
discriminator Dw along with real images X, to ensure that the factor-altered reconstructions remain
realistic.
3.4.1	VAE Pre-training
We first train the encoder and the decoder to maximize the likelihood of the training data under the
generative model pθ(X | z2) as detailed in equation 1. In this step, only the z2 latent set is learned;
which encodes all the informative factors of variation albeit in an entangled way. Since we do not
enforce any extra independence constraints on the latent space, the learning of the latent space is
driven by the reconstruction loss. The posterior distribution qφ2 (z2|X) is regularized to be similar to
the zero mean, unit variance, isotropic Gaussian priorp(z2). Thus, the objective function we aim to
minimize is evidence lower-bound (ELBO) of the data log-likelihood as in a regular VAE.
LELBO = Eq(x)[Eqφ2 (Z2∣x)[Pθ(XIz2)] - KL(qφ2 (Z2 |X)IIp(Z2川	(4)
where p(z2 ) = N(0, Id)
The first term in the above objective function minimizes the reconstruction error of the data points
from the latent representations alone. This ensures that the latent representations encode the impor-
tant information in the data or all the different factors of variation in the dataset.
3.4.2	OAT Factor Disentangling
The main contribution of our work occurs at the this stage, where we perform OAT factor disentan-
glement. We outline the process as a two-step process:
Step 1: Passing Through The Disentangled Latents
Once the pre-training is completed, the reconstruction loss saturates and all the informative factors
are encoded in Z2, however they are highly entangled. In step 1, we perform the same VAE training
pass as in the pre-training phase, but we now pass the data also through Z11:k, where k is the number
of dimensions learned until that point. This will eventually allow the model to encode information in
Z1 from the data in a disentangled way instead of their entangled representations in Z2. For brevity
6
Under review as a conference paper at ICLR 2022
we denote the set (z11:k, z2) as z. Thus objective function for this step is as follows:
K
LI = Eq(χ)[Eqφ(Z∣x)[Pθ(XIZ)] - βKLSφ2(Z2|X)IIp(Z2^ - X YiKL(qφι (Zlx)IIp(Zi))]	⑸
i=1
where p(Z1) = Qik=1 p(Z1i ) = Qik=1 N (0, 1). Here γi is a weighting factor that acts as a mask,
which indicates turning off latent elements of Z1 that are not being currently trained as part of the
OAT iterative procedure. During the pre-training γi {i = 1 : K} are set to zero. At the beginning
of the OAT training procedure, only the first element of Z1 , Z11 , is learned, with γ1 = 1 and for the
latents with γi{i = 2 : k} = 0 the gradients are not calculated and the weights are not updated. This
constitutes a core of the “one at a time” component, and is critical for the network’s functionality:
by focusing on one latent factor at a time, we can iteratively learn and discover each latent factor at
different points during the training. As the training proceeds the value of more γ's are flipped to 1
which allows more latents in Z1 to be seen by the network.
The value of β is increased linearly during training to encourage the model to encode information
in Z1 instead ofZ2. This along with the intervention described in the further sections ensure that the
posterior distribution of the disentangled set does not collapse to the prior.
Step 2: Interventions and Change-Discriminator: In order to ensure that an independent, in-
terpretable factor of variation is encoded in Z1i , we perform interventions on each of the learned
dimensions of Z1. Thus it is important for any particular dimension i in Z1, that changing the value
of another dimension Z1j (j 6= i) should not change the value of Z1i when the corresponding gener-
ated data from the intervention is re-encoded. Interventions: For the intervention procedure, we
start with a learned representation Z = {zι, z2} such that zι 〜q©1(zjx) and z2 〜q@? (z2Iχ),
encoded from an some datapoint X. We then uniformly select a dimension k ∈ [K] from the learned
dimensions ofz1 to change it’s value during intervention. We sample a new value for the dimension
k from the prior distribution p(Z1k) say c to create a new representation zk = ({c, Z1\k}, z2 ). Thus
the two representations are same in all dimensions except k. The new representation is then passed
through the decoder to generate a data point Xk. This generated data is then re-passed back through
the encoder to obtain the representations Zk = ({c, z∖k}, Z2) sampled from the encoder distribu-
tions q©i (zι IXk) and q©2 (z2 IXk) respectively. Both the encoder and the decoder are then trained to
reconstruct zk according to 6. We refer to this procedure of altering a single factor, combined with
data generation and re-passing through the encoder, an intervention, per Suter et al. (2019).
While we want to reconstruct the intervened latent variable, we also want to ensure that the changes,
the intervened latent variable makes in the corresponding generated data, are interpretable. This is
ensured by constraining the distribution of the generated data to lie strictly in the true data man-
ifold. Thus a particular generated datum differs from the training datum in a subset of factors of
variation, ideally in only one factor corresponding to the intervened latent variable. To address this,
we train the discriminator Dw to differentiate between the training data and the generated data. The
decoder weights are updated to generate data which would fool the discriminator. Thus the complete
objective function can be written as follows;
L2 = Ep(z)[Epθ(χk∣zk)[qφ(zkIXk)] - βKL(pθ(XkIzk)IIq(x))]	(6)
The first term in the above objective function minimizes the reconstruction error of the re-encoded,
intervened latent representation from the generated data. Because the intervened representations
are reconstructed from the generated data alone, the decoder is forced to make distinct changes in
the generated images corresponding to the different dimensions of z1. This ensures that the disen-
tangled set z1 is independently controllable and interpretable, a desirable property of disentangled
representations.
Since it is difficult to compute analytically the high-dimensional true data distribution from the sam-
ples alone, the KL divergence in the second term is replaced with the Jensen-Shannon Divergence
(JS-divergence). In Goodfellow et al. (2014) given an optimal discriminator network Dw, described
in Sec. 3.3, training the decoder to fool the discriminator essentially minimizes the JS-divergence
between the generated distribution and the training images.
During the training, step 1 and step 2 are performed one after the other in an alternative fashion,
similar to other two-step learning algorithms such as wake-sleep (Hinton et al., 2006). This process
7
Under review as a conference paper at ICLR 2022
Model	FactorVae	MIG	DCI	BetaVAE
VAE	0.63±.06	-0!0-	0.30±.10	
β-VAE	0.63±.10	0.21	0.41±.11	
FactorVAE (γ=40)	0.82±.01	0.43±.01	0.74±.01	0.84±.01
β -TCVAE	0.62±.07	0.45	0.29±.01	
InfoGAN	0.82±.01	0.22±.01	0.60±.02	0.87±.01
InfoGAN-CR	0.88±.01	0.37±.01	0.71±.01	0.95±.01
OAT (zι=10, z2=10)	0.82±.11	0.36±.13	0.78± .01	0.80±.11
OAT (zι=5, z2=10)	0.84±.12	0.44 ± .03	0.74± .05	0.82±.10
OAr (zι=12, z2=10)	0.78 ± .08	0.33±.11	0.77± .01	0.76 ± .08
Table 1: Comparisons of the popular disentanglement metrics on the dSprites dataset. A perfect
disentanglement corresponds to 1.0 scores. The scores are averages over 10 runs with different
random seeds.
has the effect of minimizing the symmetric KL divergence between the joint generative and inference
distributions pθ (x, z) and qφ (x, z) A.
During training, the first latent variable of the set z11 is trained and thus γ1 ’s value is changed from
0 to 1. This latent is trained using interventions until it’s KL-divergence saturates and it can encode
no more information. Then, we start training the next latent variable from the set z1 i.e. z12 and the
value of γ2 is switched from 0 to 1. The stopping criteria for this process depends on the dataset.
If the dataset has a fixed number of factors which are independent like in the synthetic datasets
dSprites and 3DShapes, the process of adding more latents is stopped when the KL-divergence of
the entangled set, z2, goes to zero and it encodes no more information. That is when we can say
that all the factors of variation have been found. For real datasets like CelebA, we can find continue
finding factors until the desired number of factors have been found.
4	Empirical Evaluation
For quantitative evaluation, we run experiments on two synthetic datasets generated from indepen-
dent ground truth factors of variation; dSprites Matthey et al. (2017) and 3DShapes Burgess &
Kim (2018). For qualitative evaluation we use a real dataset with unknown factors of variation, the
CelebA dataset Liu et al. (2015).
We evaluate the learned representations with one metric from each of the three kinds of metrics as
described in (Zaidi et al., 2021). We use the FactorVAE (Kim & Mnih, 2019), the BetaVAE(Higgins
et al., 2017), Mutual Information Gap (MIG) (Chen et al., 2019) and the Disentanglement-
Completeness-Informativeness (DCI) (Eastwood & Williams, 2018) metric to quantitatively eval-
uate disentanglement on dSprites (1) and 3DShapes (2). We also qualitatively evaluate the models
trained on CelebA 4, dSprites 4, 3D shapes (ref: A.4). More details regarding the implementation
and the metrics and the latent traversals can be found in the appendix ??.
We also perform ablation studies on the dimension of z1, K, and z2, d, on the dSprites dataset.
We find that increasing d beyond a certain range (5-10), does not affect the metrics but make the
reconstructions sharper as shown in A.4. The value of K affects how the factors that are encoded in
the different dimensions of z1 and thus affects the metrics.
4.1	Discussion
From Table 1 we see that OAT beats the previous baselines on the DCI metric while performing
comparably on the other metrics. We noticed that in the DCI metric, OAT consistently got high
scores for informativeness and disentanglement, but lower scores for completeness. The complete-
ness score is high if each factor is completely captured by only a single latent variable. However,
as pointed in Eastwood & Williams (2018) enforcing completeness might be counter-productive to
learning disentangled representation for complicated factors like rotation.
With the latent traversals 4 we see that the factors are not confined to single latent dimensions but
instead are encoded over several dimensions. For example, complicated factors like rotation and
8
Under review as a conference paper at ICLR 2022
Figure 2: (Left): Traversals for the CelebA dataset show that OAT(z1=10,z2=24) is able to disen-
tangle multiple unique factors smoothly on real-world data, with unknown true factors. (Right):
Results showing the OAT (z1=5,z2=10) architecture disentangling various factors for a synthetic
dataset. From top to bottom: oval to heart, oval to square, rotation, and size. The rest of the val-
ues for different factors are encoded in different dimensions as completeness is not enforced during
disentanglement
Model	FactorVae	MIG	DCI
FaCtorVAE (Y =10)	0.33 土 .06	0.14 ±.02	0.75 土 .01
FaCtorVAE(Y=40)	0.42 ± .01	0.32 土 .03	0.73 土 .02
β-TCVAE	0.53 ± .01	0.38 土 .02	0.76 土 .01
InfoGAN-CR	0.68 ± .01	0.26 土 .03	0.53 土 .04
OAT (zι=10, Z2=10)	0.64 ±.18	0・39 土 0・04	0.77± .01
Table 2: Comparisons of the disentanglement metrics on the 3DShapes dataset averaged over 10
runs with different random seeds.
shape are encoded over two latent dimensions. This is a possible explanation for the lower MIG
scores as the MIG score essentially computes compactness along with informativeness while giving
less importance to modularity. As pointed in (Zaidi et al., 2021) MIG has a high score even if one
latent dimension encodes information about multiple factors as long as one factor is only encoded
by one dimension. This also explains why OAT(z1=5, z2=10) has a higher MIG score than the other
OAT models. However simpler factors like size are consistently encoded in a single latent variable.
From the ablation studies we see that when |z1 | is large, some factors are encoded in more dimen-
sions of z1 , whereas when |z1 | is small, more than one factor is forced into single dimension of z1 .
Thus, this model performs best when |z1 | is larger than the true number of factors of variation if
known.
We also notice that for some factors like y-axis, x-axis there is an abrupt change in the traversals. We
believe this is an artifact of sampling interventions from the prior, and would change if a different
intervention method is used. We use a zero mean unit variance Gaussian distribution for sampling.
In future works we plan to experiment with sampling from different distribution like learned priors
and approximate posterior distribution.
5	Conclusion
In this work we present a novel generative neural network framework for unsupervised disentan-
glement, One at a Time (OAT) Factor Learning. We demonstrate that with the use of unsupervised
interventions, the network is able to learn smooth traversals across each latent dimension, and that
the latent dimensions learned are informative, interpretable and separate without the use of labels.
With the addition of the two separate latent spaces, OAT is able to learn an arbitrary number of
factors, whereas before the latent dimension had to be pre-determined. Due to its design, it is able
9
Under review as a conference paper at ICLR 2022
to find a balance between the training stability of VAEs with the generative quality of GANs, while
catering to real datasets with arbitrary number of factors. We hope that this work paves the path
towards learning disentangled representations from real world datasets.
References
Yoshua Bengio. Deep learning of representations: Looking forward, 2013.
Chris Burgess and Hyunjik Kim. 3d shapes dataset. https://github.com/deepmind/3dshapes-dataset/,
2018.
Christopher P. Burgess, Irina Higgins, Arka Pal, Loic Matthey, Nick Watters, Guillaume Desjardins,
and Alexander Lerchner. Understanding disentangling in β-vae, 2018.
Tong Che, Yanran Li, Athul Paul Jacob, Yoshua Bengio, and Wenjie Li. Mode regularized generative
adversarial networks, 2017.
Ricky T. Q. Chen, Xuechen Li, Roger Grosse, and David Duvenaud. Isolating sources of disentan-
glement in variational autoencoders, 2019.
Xi Chen, Yan Duan, Rein Houthooft, John Schulman, Ilya Sutskever, and Pieter Abbeel. Info-
gan: Interpretable representation learning by information maximizing generative adversarial nets,
2016.
Kien Do and Truyen Tran. Theory and evaluation metrics for learning disentangled representations,
2020.
Emilien Dupont. Learning disentangled joint continuous and discrete representations, 2018.
Cian Eastwood and Christopher K. I. Williams. A framework for the quantitative evaluation of
disentangled representations. In International Conference on Learning Representations, 2018.
URL https://openreview.net/forum?id=By-7dz-AZ.
Babak Esmaeili, Hao Wu, Sarthak Jain, Alican Bozkurt, N. Siddharth, Brooks Paige, Dana H.
Brooks, Jennifer Dy, and Jan-Willem van de Meent. Structured disentangled representations,
2018.
Robert Geirhos, Jom-Henrik Jacobsen, ClaUdio Michaelis, Richard ZemeL Wieland Brendel,
Matthias Bethge, and Felix A. Wichmann. Shortcut learning in deep neural networks. Na-
ture Machine Intelligence, 2(11):665-673, Nov 2020. ISSN 2522-5839. doi: 10.1038/
s42256- 020- 00257- z. URL http://dx.doi.org/10.1038/s42256-020-00257- z.
Ian J. Goodfellow, Jean PoUget-Abadie, Mehdi Mirza, Bing XU, David Warde-Farley, Sherjil Ozair,
Aaron CoUrville, and YoshUa Bengio. Generative adversarial networks, 2014.
I. Higgins, Lolc Matthey, A. Pal, C. Burgess, Xavier Glorot, M. Botvinick, S. Mohamed, and
Alexander Lerchner. beta-vae: Learning basic visUal concepts with a constrained variational
framework. In ICLR, 2017.
Irina Higgins, David Amos, David Pfau, Sebastien Racaniere, Loic Matthey, Danilo Rezende, and
Alexander Lerchner. Towards a definition of disentangled representations, 2018.
Geoffrey E. Hinton, Simon Osindero, and Yee Whye Teh. A fast learning algorithm for deep belief
nets. Neural Computation, 18:1527-1554, 2006.
Zhiting Hu, Zichao Yang, Xiaodan Liang, Ruslan Salakhutdinov, and Eric P. Xing. Toward con-
trolled generation of text, 2018.
Insu Jeon, Wonkwang Lee, and Gunhee Kim. IB-GAN: Disentangled representation learning
with information bottleneck GAN, 2019. URL https://openreview.net/forum?id=
ryljV2A5KX.
Yeonwoo Jeong and Hyun Oh Song. Learning discrete and continuous factors of data via alternating
disentanglement, 2019.
10
Under review as a conference paper at ICLR 2022
Hyunjik Kim and Andriy Mnih. Disentangling by factorising, 2019.
Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization, 2017.
Diederik P Kingma and Max Welling. Auto-encoding variational bayes, 2014.
Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classification with deep convo-
Iutional neural networks. Communications ofthe ACM, 60(6):84-90, 2017.
Tejas D. Kulkarni, Will Whitney, Pushmeet Kohli, and Joshua B. Tenenbaum. Deep convolutional
inverse graphics network, 2015.
Abhishek Kumar, Prasanna Sattigeri, and Avinash Balakrishnan. Variational inference of disen-
tangled latent concepts from unlabeled observations. In International Conference on Learning
Representations, 2018. URL https://openreview.net/forum?id=H1kG7GZAW.
Wonkwang Lee, Donggyun Kim, Seunghoon Hong, and Honglak Lee. High-fidelity synthesis with
disentangled representation, 2020.
Zinan Lin, Kiran Koshy Thekumparampil, Giulia Fanti, and Sewoong Oh. Infogan-cr and model-
centrality: Self-supervised model training and selection for disentangling gans, 2020.
Bingchen Liu, Yizhe Zhu, Zuohui Fu, Gerard de Melo, and Ahmed Elgammal. Oogan: Disentan-
gling gan with one-hot sampling and orthogonal regularization, 2020.
Ziwei Liu, Ping Luo, Xiaogang Wang, and Xiaoou Tang. Deep learning face attributes in the wild.
In Proceedings of International Conference on Computer Vision (ICCV), December 2015.
Francesco Locatello, Stefan Bauer, Mario Lucic, Gunnar Raetsch, Sylvain Gelly, Bernhard
Scholkopf, and Olivier Bachem. Challenging common assumptions in the unsupervised learning
of disentangled representations. In Proceedings of the 36th International Conference on Machine
Learning, volume 97 of Proceedings of Machine Learning Research, pp. 4114-4124. PMLR,
09-15 Jun 2019.
Loic Matthey, Irina Higgins, Demis Hassabis, and Alexander Lerchner. dsprites: Disentanglement
testing sprites dataset. https://github.com/deepmind/dsprites-dataset/, 2017.
Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Veness, Marc G Belle-
mare, Alex Graves, Martin Riedmiller, Andreas K Fidjeland, Georg Ostrovski, et al. Human-level
control through deep reinforcement learning. nature, 518(7540):529-533, 2015.
Aaron van den Oord, Sander Dieleman, Heiga Zen, Karen Simonyan, Oriol Vinyals, Alex Graves,
Nal Kalchbrenner, Andrew Senior, and Koray Kavukcuoglu. Wavenet: A generative model for
raw audio. arXiv preprint arXiv:1609.03499, 2016.
Danilo Jimenez Rezende, Shakir Mohamed, and Daan Wierstra. Stochastic backpropagation and
approximate inference in deep generative models, 2014.
Karl Ridgeway and Michael C. Mozer. Learning deep disentangled embeddings with the f-statistic
loss, 2018.
Bernhard Schoelkopf, Dominik Janzing, Jonas Peters, Eleni Sgouritsa, Kun Zhang, and Joris Mooij.
On causal and anticausal learning, 2012.
N. Siddharth, Brooks Paige, Jan-Willem van de Meent, Alban Desmaison, Noah D. Goodman, Push-
meet Kohli, Frank Wood, and Philip H. S. Torr. Learning disentangled representations with semi-
supervised deep generative models, 2017.
Raphael Suter, ore Miladinovic, Bernhard Scholkopf, and Stefan Bauer. Robustly disentangled
causal mechanisms: Validating deep representations for interventional robustness, 2019.
Julian Zaidi, Jonathan Boilard, Ghyslain Gagnon, and MarC-Andre Carbonneau. Measuring disen-
tanglement: A review of metrics, 2021.
Xinqi Zhu, Chang Xu, and Dacheng Tao. Learning disentangled representations with latent variation
predictability, 2020.
11
Under review as a conference paper at ICLR 2022
A Appendix
A.1 Interpretability of disentangled representations
To be of use for downstream tasks, successful transfer learning and domain adaptation (Bengio,
2013) or to glean insights from observers be them machines or humans, the representations must
somehow be interpretable. Interpretability has a simple intuitive understanding: that each factor
represents a human-defined concept regarding the data, which humans could easily understand and
identify. In practice, however, this is only intuitive because these are concepts pre-trained in human
brains, and are not as clearly separable for machines without any imported biases; it is easy for
the network to fuse multiple of what appear to humans as “essential factors” into one latent code
while still keeping the factors independent and orthogonal to one another other; As an example, the
network simply can learn a rotation of the “human” representation, which would appear from an
observer to combine factors together. To address this, we define interpretability here in a new form
not as uniquely human interpretable persay but as interpretable latent representation manipulation:
each individual atomic code should make a unique and noticeable change in the output. Critically
we are not defining noticable as being for humans only, as emphasized by the rotational example
above, but by any observer.
A.2 Proofs
We assume a generative modelpθ (χ,z) = pθ (χ∣z)p(z) whereP(Z) is the prior assumed on the latent
space and pθ(x|z) is modelled by the generator. The corresponding inference model is qφ(χ, Z)=
qφ(z∣χ)q(χ) where q(χ) governs the empirical data distribution of observed data while the inference
model is qφ(z∣x).
The ELBO in [eq:1] can further be written as the KL divergence between the inference model and
the generative model as follows:
LI= Eq(x)[Eqφ(z∣x) [log Pθ (X|z)] - KLSφ(ZIx)IIp(Z))]
=Eqφ(χ,z)[log pθ (x|z)] — Eq(X)KL(qφ(z∣x)l∣p(z))]
q (ZIx)
=Eqφ(x,z)[logpθ (x|z)] — Eq(X)Eqφ(z∣x) log Ip(Z)
= Eqφ(x,z) [logpθ(xIZ) - log qφ(ZIx) + logp(Z)]
= Eqφ(X,z)[log pθ(x, Z) — log qφ(ZIx)]
= Eqφ(X,z)[log pθ(x, Z) — logqφ(ZIx) + log q(x) — log q(x)]
= Eqφ(X,z)[log pθ(x, Z) — logqφ(x, Z)] + Eqφ(X,z) log q(x)
=-KL(qφ(x, z)Hpθ(x,z)) — H(q(X))
≤ -KL(qφ(x, z)∣∣pθ(x, z))
By maximizing the lower bound L we minimize the KL divergence, which is equivalent to the
maximum likelihood objective as follows:
Minimizing the first term gives us the maximum likelihood objective. Given the asymmetric nature
of the KL divergence this ensures that our generative model pθ (x) has non zero probability for every
x 〜q(x). However, this does not ensure that the model gives a low probability to images not in
q(x). Thus we seek to also minimize the reverse KL divergence.
The ELBO in [eq:6] can further be written as the KL divergence between the generative model and
the inference model as follows:
12
Under review as a conference paper at ICLR 2022
L2 = Ep(z)[Epθ(x∣zt)[log qφ(zlxt)] - KL(Pθ(XIzt) IIq(X))]
=epθ(x,z)[logqφ(XIz)] - Ep(Z)KL(Pθ(XIzt)IIq(X))]
=Ep θ (x,z) [log qφ(X〔z)] - Ep(Z)Epθ (x|z) log -j(X)，
= Epθ(x,z)[log qφ(XIz) - logPθ(XIzt) + log q(X)]
= Epθ(x,Z)[log qφ(X, z) - logPθ(XIzt)]
= Epθ(x,Z)[log qφ(X, z) - logPθ(X,zt) + log P(z)]
= -KL(Pθ(X,z)IIqφ(X,z)) - H(P(zt))
≤ -KL(Pθ(X, z)IIqφ(X, z))
By minimizing the first term we ensure that the learned aggregated posterior qφ(z) is similar to the
factorized prior distribution. This is important for disentangling the factors, as we want the learned
representation to be independent like the prior.
Thus we see that by maximizing both L1 + L2 we minimize the symmetric KL divergence.
A.3 Experimental details
For our experiments we use the standard VAE architecture proposed in Kim & Mnih (2019) for the
encoder and the decoder model. We use [0, 1] normalised data as targets for the mean of a Bernoulli
distribution logPθ(XIz). We use the negative cross entropy as the reconstruction loss and optimize
the model using the Adam optimiser (Kingma & Ba, 2017) with a learning rate of 1e - 04 for the
dSprites dataset and 5e - 05 for the 3D shapes dataset and the CelebA dataset.
For the discriminator we use the architecture proposed in Lin et al. (2020) and use the Adam op-
timizer with a learning rate of 1e - 04 and 1e - 05 for dSprites and 3Dshapes respectively with
β1 = 0.9 and β2 = 0.999.
We train the models on the dSpirtes dataset for 30 epochs and on the 3DShapes and the CelebA
dataset. for 50 epochs with a batch size of 128. To calculate the metrics we use 10 different random
seeds and calculate the average of all the runs for the scores.
A.4 Further Ablation Results
We experimented with different dimensions of z2 and noticed that a larger Iz2I produces crisp re-
constructions for datasets with a fixed number of factors of variation as can be seen in A.4 and A.4.
However, the metrics do not change much as the dimension of z2 is increased beyond 10.
For real datasets too we notice that a larger z2 produces reconstructions with more details and more
factors. This facilitates the discovery of more factors in the disentangled set during the OAT proce-
dure as can be seen in ??
A.5 Evaluation metrics
For implementation details and hyperparameter settings of the metrics, we directly follow the set-
tings in Locatello et al. (2019). Our VAE architecture is the one Kim & Mnih (2019) use in their
experiments and the discriminator architecture is based on Lin et al. (2020).
A.5.1 BetaVAE, FactorVAE
We evaluate the learned representations with one metric from each of the three kinds of metrics
as described in (Zaidi et al., 2021). The intervention-based metrics compare representations by
creating subsets of data in which one or more ground-truth factors are kept constant. These metrics
do not make any assumptions on the factor-code relations which is their main advantage. We use
the Factor-VAE metric from the intervention-based metrics kind. In this metric, in a batch a factor
13
Under review as a conference paper at ICLR 2022
Figure 3: Top: The original input, Bottom: Reconstructions after passing through trained full OAT
(z1=10,z2=10) model (as described in Experimental Setup.) Note that the model produces crisp,
non-blurry, reconstructions.
Figure 4: Training images (left) and the corresponding reconstructions (right) for z1=10 and z2=20.
Gi is chosen randomly.Then, a fixed number of pairs from the data are selected where the value of
the factor Gi is the same. The intuition is that representation dimensions associated with the fixed
factor should have the same value, which means a smaller difference than the other representation
dimensions. Finally, a linear classifier is trained on the data set to predict which factor was fixed.
The accuracy of the classifier is the score.
A.5.2 DCI
Predictor-based metrics use regressors or classifiers to predict factors from the representations.
These metrics train models to predict factor realizations from the representations. Then the use-
fulness of each code dimension in predicting a given factor is analyzed. These methods are naturally
suited to measure explicitness. We use the DCI-Lasso and (Eastwood & Williams, 2018) metrics to
measure explicitness and modularity.
A.5.3 MIG
Information-based metrics compute a disentanglement score by estimating the mutual information
(MI) between the factors and the representations. These methods require fewer hyper-parameters
than intervention-based and predictor-based metrics. Moreover, they do not make assumptions on
14
Under review as a conference paper at ICLR 2022
Figure 5: More Traversals for the CelebA dataset, for the same run (no cherry-picking between runs.)
OAT is able to disentangle multiple unique factors smoothly on real-world data, with unknown true
factors.

Figure 6: Traversals for the 3D shapes dataset with |z1 |=6 and |z2|=10. Factors from top to bottom:
shape, floor color, orientation, size, wall color, floor color.
15
Under review as a conference paper at ICLR 2022
the nature of the factor-representation relations. We use the MIG (Chen et al., 2016) to measure all
the three facets of disentanglement.
A.5.4 Latent Traversals
For the CelebA dataset the ground truth values or the factors of variations are not known apriori.
Therefore we use latent traversals 4 as a way to measure disentanglement qualitatively (Che et al.,
2017). Disentanglement is evaluated qualitatively by traversing the latent space, by fixing all the
dimensions of the representations except one and varying the values of that one dimension. For
the varying dimension discrete values are sampled over its distribution and the resulting generated
samples are visualized. A model has better disentanglement if for different values of the varying
dimension, the resulting generated samples have a distinct and noticeable change for an interpretable
factor of variation. We also qualitatively evaluate the models trained on dSprites and 3D shapes (ref:
A.4).
A.6 Downstream CelebA tasks
To show that our method learns more informative factors than current state of the art approaches,
we perform a number of downstream tasks on the learned disentangled representations. The CelebA
dataset has 40 binary attributes for each image in the dataset. We extract the latent representations
from the best disentangling models, namely, β-VAE, Factor-VAE and the β-TCVAE and train 40
SVMs to predict each of the binary attributes. We use 1000 (1k) and 10000 (10k) examples as a
training set for these SVMs and evaluate it’s performance on 2k examples in the test set. The results
of the table 3 are calculated by taking the average of the accuracies of the 40 SVMs.
Model	kernel:linear, 10k	kernel:linear, 1k	kernel:rbf, 10k	kernel:rbf, 1k
β-VAE	0.64	0.62	0.69	066
FactorVAE (γ=40)	0.72	0.71	0.75	0.74
β -TCVAE	0.74	0.74	0.79	0.77
OAT (zι=10, Z2=10)	0.79	0.78	0.81	0.80
Table 3: Average accuracy over 40 SVMs trained for each model representation, using different
number of training examples.
Thus we see that the representations learned by OAT are more informative than the state of the art
VAE-based approaches. We believe that OAT will perform better in the downstream tasks where
certain factors might not be present in all the images but only in a subset of them.
16