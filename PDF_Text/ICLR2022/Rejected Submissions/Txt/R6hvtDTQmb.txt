Under review as a conference paper at ICLR 2022
Adapting Stepsizes by Momentumized Gradi-
ents Improves Optimization and Generaliza-
TION
Anonymous authors
Paper under double-blind review
Ab stract
Adaptive gradient methods, such as Adam, have achieved tremendous success in
machine learning. Scaling gradients by square roots of the running averages of
squared past gradients, such methods are able to attain rapid training of modern
deep neural networks. Nevertheless, they are observed to generalize worse than
stochastic gradient descent (SGD) and tend to be trapped in local minima at an
early stage during training. Intriguingly, we discover that substituting the gra-
dient in the second moment estimation term with the momentumized version in
Adam can well solve the issues. The intuition is that gradient with momentum
contains more accurate directional information and therefore its second moment
estimation is a better choice for scaling than that of the raw gradient. Thereby we
propose AdaMomentum as a new optimizer reaching the goal of training fast
while generalizing better. We further develop a theory to back up the improvement
in optimization and generalization and provide convergence guarantees under both
convex and nonconvex settings. Extensive experiments on a wide range of tasks
and models demonstrate that AdaMomentum exhibits state-of-the-art perfor-
mance consistently. The source code is available at https://anonymous.
4open.science/r/AdaMomentum_experiments-6D9B.
1	Introduction
Prevailing first-order optimization algorithms in modern machine learning can be classified into two
categories. One is stochastic gradient descent (SGD) (Robbins & Monro, 1951), which is widely
adopted due to its low memory cost and outstanding performance. SGDM (Sutskever et al., 2013)
which incorporates the notion of momentum into SGD, has become the best choice for optimizer in
computer vision. The drawback of SGD(M) is that it scales the gradient uniformly in all directions,
making the training slow especially at the begining and fail to optimize complicated models well be-
yond Convolutional Neural Networks (CNN). The other type is adaptive gradient methods. Unlike
SGD, adaptive gradient optimizers adapt the stepsize (a.k.a. learning rate) elementwise according to
the gradient values. Specifically, they scale the gradient by the square roots of some form of the run-
ning average of the squared values of the past gradients. Popular examples include AdaGrad (Duchi
et al., 2011), RMSprop (Tijmen Tieleman, 2012) and Adam (Kingma & Ba, 2015) etc. Adam, in
particular, has become the default choice for many machine learning application areas owing to its
rapid optimizing speed and outstanding ability to handle sophisticated loss curvatures.
Despite their fast speed in the early training phase, adaptive gradient methods are found by stud-
ies (Wilson et al., 2017; Zhou et al., 2020) to be more likely to exhibit poorer generalization ability
than SGD. This is discouraging because the ultimate goal of training in many machine learning tasks
is to exhibit high performance during testing phase. In recent years researchers have put many efforts
to mitigate the deficiencies of adaptive gradient algorithms. AMSGrad (Reddi et al., 2019) corrects
the errors in the convergence analysis of Adam and proposes a faster version. Yogi (Reddi et al.,
2018) takes the effect of batch size into consideration. M-SVAG (Balles & Hennig, 2018) transfers
the variance adaptation mechanism from Adam to SGD. AdamW (Loshchilov & Hutter, 2017b) first-
time decouples weight decay from gradient descent for Adam-alike algorithms. SWATS (Keskar &
Socher, 2017) switches from Adam to SGD throughout the training process via a hard schedule
and AdaBound (Luo et al., 2019) switches with a smooth transation by imposing dynamic bounds
1
Under review as a conference paper at ICLR 2022
on stepsizes. RAdam (Liu et al., 2019) rectifies the variance of the adaptive learning rate through
investigating the theory behind warmup heuristic (Vaswani et al., 2017; Popel & Bojar, 2018). Ad-
aBelief (Zhuang et al., 2020) adapts stepsizes by the belief in the observed gradients. Nevertheless,
most of the above variants can only surpass (as they claim) Adam or SGD in limited tasks or under
specifically and carefully defined scenarios. Till today, SGD and Adam are still the top options in
machine learning, especially deep learning (Schmidt et al., 2021). Conventional rules for choos-
ing optimizers are: from task perspective, choose SGDM for vision, and Adam (or AdamW) for
language and speech; from model perspective, choose SGDM for Fully Connected Networks and
CNNs, and Adam (or AdamW) for Recurrent Neural Networks (RNN) (Cho et al., 2014; Hochreiter
& Schmidhuber, 1997b), Transformers (Vaswani et al., 2017) and Generative Adversarual Networks
(GAN) (Goodfellow et al., 2014). Based on the above observations, a natural question is:
Is there a computationally efficient adaptive gradient algorithm that can converge fast and mean-
while generalize well?
In this work, we are delighted to discover that simply replacing the gradient term in the second
moment estimation term of Adam with its momentumized version can achieve this goal. Our idea
comes from the origin of Adam optimizer, which is a combination of RMSprop and SGDM. RM-
Sprop scales the current gradient by the square root of the exponential moving average (EMA) of
the squared past gradients, and Adam replaces the raw gradient in the numerator of the update term
of RMSprop with its EMA form, i.e., with momentum. Since the momentumized gradient is a more
accurate estimation of the appropriate direction to descent, we consider putting it in the second mo-
ment estimation term as well. We find such operation makes the optimizer more suitable for the
general loss curvature and can theoretically converge to minima that generalize better. Extensive
experiments on a broad range of tasks and models indicate that: without bells and whistles, our pro-
posed optimizer can be as good as SGDM on vision problems and outperforms all the competitors
in other tasks, meanwhile maintaining fast convergence speed. Our algorithm is efficient with no
additional memory cost, and applicable to a wide range of scenarios in machine learning, especially
deep learning. More importantly, AdaMomentum requires little effort in hyperparameter tuning and
the default parameter setting for adaptive gradient method works well consistently in our algorithm.
Notation We use t, T to symbolize the current and total iteration number in the optimization pro-
cess. θ ∈ Rd denotes the model parameter and f (θ) ∈ R denotes the loss function. We further
use θt to denote the parameter at step t and ft to denote the noisy realization of f at time t because
of the mini-batch stochastic gradient mechanism. gt denotes the t-th time gradient and α denotes
stepsize. mt , vt represent the EMA of the gradient and the second moment estimation term at time
t of adaptive gradient methods respectively. is a small constant number added in adaptive gradient
methods to refrain the denominator from being too close to zero. β1 , β2 are the decaying parame-
ter in the EMA formulation of mt and vt correspondingly. For any vectors a, b ∈ Rd, we employ
√a, α2, ∣a∣,a∕b, a ≥ b, a ≤ b for elementwise square root, square, absolute value, division, greater
or equal to, less than or equal to respectively. For any 1 ≤ i ≤ d, θt,i denotes the i-th element of θt.
Given a vector x ∈ Rd, we use kxk2 to denote its l2-norm and kxk∞ to denote its l∞-norm.
2	Algorithm
Preliminaries & Motivation Omitting the
debiasing operation and the damping term ,
the adaptive gradient methods can be generally
written in the following form:
mt
θt+ι = θt - α-^.
√Vt
(1)
Table 1: Comparison of AdaMomentum and clas-
sic adaptive gradient methods in mt and Vt in (1).
Optimizer	mt	vt
SGD	gt	1
Rprop	gt	gt2
RMSprop	gt	(1 - β2) Pti=1 β2t-igi2
Adam	(1-βι) Pi= βt-igi	(1 - β2) Pti=1 β2t-igi2
Ours	(1-βι) Pi= βH	(1 - β2) Pit=1 β2t-imi2
Here mt, vt are called the first and second mo-
ment estimation terms. When mt = gt and
vt = 1, (1) degenerates to the vanilla SGD. Rprop (Duchi et al., 2011) is the pioneering work us-
ing the notion of adaptive learning rate, in which mt = gt and vt = gt2 . Actually it is equivalent
to only using the sign of gradients for different weight parameters. RMSprop (Tijmen Tieleman,
2012) forces the number divided to be similar for adjacent mini-batches by incorporating momen-
tum acceleration into vt. Adam (Kingma & Ba, 2015) is built upon RMSprop in which it turns
2
Under review as a conference paper at ICLR 2022
Figure 1: Illustration of the optimization process of Adam and AdaMomentum. A general loss curve
can be composed to three areas: A) transition from a plateau to a downgrade; B) a steep downgrade;
C) from downgrade to entering the basin containing the optimum. An ideal optimizer ought to sus-
tain large stepsize before reaching the optimum and reduce its stepsize near the optimum. Compared
to Adam, AdaMomentum can adapt the true stepsize more appropriately along the loss curve and
maintain smaller stepsize near convergence. Refer to Section 3.1 for more detailed analysis.
gt into momentumized version. Both RMSprop and Adam boost their performance thanks to the
smoothing property of EMA using momentum. Due to the fact that momentumized gradient is a
more accurate estimation than raw gradient, we deem that there is no reason to use gt in lieu of mt
in second moment estimation term vt . Therefore we propose to replace the gi s in vt of Adam with
their momentumized versions mi s, which further smooths the exponential moving average.
Algorithm 1 AdaMomentum (ours). All mathematical operations are element-wise.
1	Initialization: Parameter initialization θo, step size a, damping term e, m° J 0, v° J 0, t J 0	
2	: while θt not converged do	
3	:	tJt+1	. Updating time step
4	gt J Vθft(θt-ι)	. Acquiring stochastic gradient at time t
5	:	mt J β1 mt-1 + (1 - β1)gt	. EMA of gradients
6	:	vt J β2vt-1 + (1 - β2)mt2 + e	. EMA of squared momentumized gradients
7	:	mct J mt/(1 - β1t )	. Bias correction of first moment estimation
8	V J vt∕(1 — βt)	. Bias correction of second moment estimation
9	θt J Θt-1 — α ∙ mt∕√bt	. Updating parameters
10	: end while	
Detailed Algorithm The detailed procedure of our proposed optimizer is displayed in Algo-
rithm 1. There are two major modifications based on Adam, which are marked in red and blue
respectively. One is that we replace the gt in vt of Adam with mt , which is the momentumized
gradient. Hence We name our proposed optimzier as AdaMomentum. The other is the location of e
(in Adam e is added after √∙ in line 10 of Alg.1). We discover that moving the adding e from outside
the radical symbol to inside can consistently enhance performance. To the best of our knowledge,
our method is the first attempt to put momentumized gradient in the second moment estimation term
of adaptive gradient methods. Note that although the modifications seem simple to some degree,
they can lead to siginificant changes in the performance ofan adaptive gradient optimizer due to the
iterative nature of optimization methods, which will also be elaborated in the following section.
3	Why AdaMomentum over Adam ?
3.1	AdaMomentum is more suitable for general loss curvature
In this section, we show that AdaMomentum can converge to (global) minima faster than Adam
does via a 1-D example. The left part of Figure 1 is the process of optimization from a plateau to a
basin area, where global optimum is assumed to exist. The right part is the zoomed-in version of the
situation near the minimum, where we have some peaks and valleys. This phenomenon frequently
takes place in optimization since there is only one global minimum with probably a great number of
local minima surrounding (Hochreiter & Schmidhuber, 1997a; Keskar et al., 2017).
3
Under review as a conference paper at ICLR 2022
Benefits of substituting gt with mt. We first explain how substituting mt for gt in the precon-
ditioner vt can improve training via decomposing the trajectory of parameter point along the loss
curve. 1) In area A, the parameter point starts to slide down the curve and |gt| begins to enlarge
abruptly. So the actual stepsize a/√vt is small for Adam. However the absolute value of the mo-
mentumized gradient mt is small since it is the EMA of the past gradients, making a/√vt still large
for AdaMomentum. Hence AdaMomentum can maintain higher training speed than Adam in this
changing corner of the loss curve, which is what an optimal optimizer should do. 2) In area B, since
the exponential moving average decays the impact of past gradients exponentially w.r.t. t, the mag-
nitude of the elements of mt will gradually becomes as large as gt. 3) In area C, when the parameter
approaches the basin, the magnitude of gt decreases, making the stepsizes of Adam increase imme-
diately. In contrast, the stepsize of AdaMomentum is still comparatively small as |mt| is still much
larger than |gt|, which is desired for an ideal optimizer. Small stepsize near optimum has benefits for
convergence and stability. A more concrete illustration is given in the right part of Figure 1. If the
(2)
stepsize is too large (e.g. in Adam), the weight parameter θt may rush to θt+1 and miss the global
optimum. In contrast, small stepsize can guarantee the parameter to be close to the global minimum
(see θt(+1)1) even if there may be tiny oscillations within the basin before the final convergence.
Benefits of changing the location of e. Next We elaborate why putting e under the √ is beneficial.
We denote the debiased second moment estimation in AdaMomentum as vbt and the second moment
estimation term without e as vbt0 . By simple calculation, we have
tt
bt = ((1 - β2)∕(1 - β2)) ∙ X βt-im2 + ---, b0 = ((1 - β2)∕(1 - β2)) ∙ X β2-im2.
i=1	1 - β2	i=1
Hence we have b = b0 + e∕(1 一 β2). Then the actual stepsizes are α∕( Jb0 + e∕(1 一 β2)) and
α∕( y∕v't + e) respectively. In the final stage of optimization, Vt is very close to 0 (because the values
of gradients are near 0) and far less than e hence the actual stepsizes can be approximately written as
√1 - β2α∕√e and α∕e.As e usually takes very tiny values ranging from 10-8 to 10-16 and β2 usu-
ally take values that are extremely close to 1 (usually 0.999), we have √1 - β2α∕√e《 α∕e. There-
fore we may reasonably come to the conclusion that after moving e term into the radical symbol,
AdaMomentum further reduces the stepsizes when the training is near minima, which contributes to
enhancing convergence and stability as we have discussed above.
3.2	AdaMomentum converges to minima that generalize better
The outline of Adam and our proposed AdaMomentum can be written in the following unified form:
mt = β1mt-1 + (1 - β1)gt, vt = β2vt-1 + (1 - β2)kt2,
θt+ι=θt—αmt. ((I- βt)q Vt/(I--)).	⑵
where kt = gt in Adam and kt = mt in AdaMomentum. Inspired by a line of work (Pavlyukevich, 2011;
Simsekli et al., 2019; Zhou et al., 2020), we can consider (2) as a discretization of a continuous-time process
and reformulate it as its corresponding Levy-driven stochastic differential equation (SDE). Assuming that the
gradient noise Zt = gt — Vf (θt) is independent and centered symmetric e-stable (SeS) (Levy & Levy, 1954)
distributed with covariance matrix Σt possessing a heavy-tailed signature (αe ∈ (0, 2]), we are able to derive
the Levy-driven SDE of (2) as:
dθt = -qt Rt-1 mt dt + υRt-1ΣtdLt,	dmt	=	β1(Vf(θt)	-	mt),	dvt	=	β2(kt2	- vt),	(3)
where Rt = diag(pvt/(1 — β2)), υ = a1-1/ae,qt = 1/(1 — βt) and Lt is the e-stable LeVy motion with
independent components. We are interested in the local stability of the optimizers and therefore we suppose
process (3) is initialized in a local basin Ω with a minimum θ* (w.l.o.g., we assume θ* = 0). To investigate
the escaping behavior of θt, we first introduce two technical definitions.
Definition 1 (Radon Measure (Simon et al., 1983)). If a measure m(∙) defined on the σ-algebra of Borel sets
of a Hausdorff topological space X is 1) inner regular on open sets, 2) outer regular on all Borel sets, and 3)
finite on all compact sets, then the measure is called a Radon measure.
Definition 2 (Escaping Time & Escaping Set). We define escaping time Γ := inf {t ≥ 0 : θt 6∈
Ω-υ"}, where Ω-υ" = {y ∈ Ω : dis(∂Ω,y) ≥ υγ}. Here Y > 0 is a constant. We define escaping
set Y := {y ∈ Rd : R-1∑θ*y ∈ Ω-υ"}, where ∑θ* = limθt→θ* ∑t,Rθ* = limθt→θ* Rt.
4
Under review as a conference paper at ICLR 2022
We study the relationship between Γ and Υ and impose some standard assumptions before proceeding.
Assumption 1. f is non-negative With an upper bound, and locally μ-strongly convex in Ω.
Assumption 2. There exists some constant L > 0, s.t. kVf (x) — Vf (y)∣b ≤ L ∣∣x — y∣∣2 , ∀x,y.
Assumption 3. We assume that RrhVf (θt)∕(1 + f (θt)),qtR-1mti dt ≥ 0 a.e., and βι ≤ β2 ≤ 2βι. There
exist v-,v+ > 0 s.t. each coordinate of √vt can be uniformly bounded in (V-, v+) and there exist τm,τ > 0
s.t. kmt - mbtk2 ≤ τm R0t-(mx - mbx) dx	and kmbtk2 ≥
by solving (3) with υ = 0.
^
O
τ ∣∣Vf (θt)∣∣ , where mt and θt are calculated
Assumption 1 and 2 impose some standard assumptions of stochastic optimization Ghadimi & Lan (2013);
Johnson & Zhang (2013). Assumption 3 requires momentumized gradient mt and Vf(θt) to have similar
directions for most of the time, which have been empirically justified to be true in Adam (Zhou et al., 2020).
Based on the above assumptions, we can prove that for algorithm of form (2), the expected escaping time is
inversely proportional to the Radon measure of the escaping set:
LemmaL Under Assumptions 1-3, let υα+1 = Θ(e) and ln(2∆∕(μυ1/3)) ≤ 2μτ(βι — β2∕4)∕(β1v+ +μτ),
where ∆ = f (θo) — f (θ*). Then given any θo ∈ Ω-2υ", for (3) we have
E(Γ) =Θ(υ∕m(Υ)),
where m(∙) is a non-zero Radon measure satisfying that m(U) < m(V) if U ⊂ V.
Because larger set has larger volume, i.e., V (U) ≤ V (V) ifU ⊂ V, from Lemma 1 we have the escaping time
is negatively correlated with the volume of the set Υ. Therefore, we can come to the conclusion that for both
Adam and AdaMomentum, if the basin Ω is sharp which is ubiquitous during the early stage of training, Y has
a large Radon measure, which leads to smaller escaping time Γ. This means both Adam and AdaMomentum
prefer relatively flat or asymmetric basin He et al. (2019) through the training process.
On the other hand, upon encountering a comparatively flat basin or asymmetric valley Ω, we are able to prove
that AdaMomentum will stay longer inside. Before we proceed, we need to impose two mild assumptions.
Assumption 4. There exists a constant H > 0 s.t. ∣Vf (θt)∣2 ≤ H, ∣gt ∣2 ≤ H, ∀t ∈ [T].
Assumption 5. For AdaMomentum, there exists To ∈ N s.t., diag(∑t) ≤ βιE(m2-ι)/(2 — βι) when t > To.
Here Assumption 4 is a common assumption in stochastic optimization (Ghadimi & Lan, 2013; Johnson &
Zhang, 2013). As β1 is always set as positive number close to 1, Assumption 5 basically requires that the
gradient noise variance to be smaller than the second moment of m when t is very large. This assumption
is mild as 1) we can select mini-batch size to be large enough to satisfy it as the noise variance is inversely
proportional to batch size (Bubeck, 2014). 2) The magnitudes of the variances of the stochastic gradients are
usually much lower than that of the gradients (Faghri et al., 2020). Then we can come to the following result.
Proposition 1. Under Assumptions 1-5, upon encountering a comparatively flat basin or asymmetric valley Ω,
we have
E Γ(ADAMOMENTUM) ≥ E Γ(ADAM) .
In other words, when falling into a flat/asymmetric basin, AdaMomentum is more stable than Adam and will not
easily escape from it. Combining the aforementioned results and the fact that minima at the flat or asymmetric
basins tend to exhibit better generalization performance (as observed in Keskar et al. (2017); He et al. (2019);
Hochreiter & Schmidhuber (1997a); Izmailov et al. (2018); Li et al. (2018)), we are able to conclude that
AdaMomentum is more likely to converge to minima that generalize better, which may buttress the improve-
ment of AdaMomentum in empirical performance. All the proofs in section 3.2 are provided in Appendix B.
4	Convergence Analysis of AdaMomentum
In this section, we establish the convergence theory for AdaMomentum under both convex and non-convex
object function conditions. We omit the two bias correction steps in the Algorithm 1 for simplicity and the
following analysis can be easily adapted and applied to the de-biased version as well.
4.1	Convergence analysis in convex optimization
We analyze the convergence of AdaMomentum in convex setting utilizing the online learning frame-
work (Zinkevich, 2003). Given a sequence of convex cost functions fι(θ), ∙…,fτ(θ), the regret is defined
as R(T) = PT=ι [ft(θt) — ft(θ*)], where θ* = argmin® PT=Ift (θ) is the optimal parameter and ft can be
interpreted as the loss function at the t-th step. Then we have:
5
Under review as a conference paper at ICLR 2022
Theorem 1. Let {θt} and {vt} be the sequences yielded by AdaMomentum. Let at = α∕y∕t, β1,1 = βι, 0 <
βι,t ≤ βι < 1,vt ≤ vt+ι for all t ∈ [T] and Y = βι/√β2 < L Assume that the distance between any θt
generated by AdaMomentum is bounded, ∣∣θm 一 θn k∞ ≤ D∞ for any m,n ∈ {1,…，T}. Then We have the
following bound on the regret:
R(T) ≤
D∞√T	√----+	D∞	β1,t√vt,i +	a√1 + log T
2α(1- βι) = Wri + 2(1- βι) = = at + (1 - βι)3(1 - γ)√Γ->
d
X kg1:T,ik2 .
i=1
Theorem 1 implies that the regret of AdaMomentum can be bounded by O1(√T), especially when the
data features are sparse as Section 1.3 in Duchi et al. (2011) and then we have Pd=I √VTi 冬 VZd and
Pd=Ikgi：T,ik2 ≪√dT.Whenwe impose additional assumptions that β1,t decays exponentially and that the
gradients of ft are bounded (Kingma & Ba, 2015; Liu et al., 2019), we can obtain the following corollary:
Corollary 1. Further Suppose βι,t = βιλt and the function ft has bounded gradients, kVft(θ)k∞ ≤ G∞ for
all θ ∈ Rd, AdaMomentum achieves the guarantee R(T)/T = 0(1/√T) for all T ≥ 1:
R(T) ≤	dG∞α√1+log T	+	dD∞ G∞	+	dD∞ G∞βι
~Tr~ ~ (1 - βι)3(1 - γ)√(1 - β2)T	2a(1 - βι)√T	2a(1 - βι)(1 - λ)2T.
Clearly observed from Corollary 1, the average regret of AdaMomentum converges to zero as T goes to infinity.
The proofs of Theorem 1 and Corollary 1 are provided in Appendix C.1.
4.2 Convergence analysis in non-convex optimization
When f is non-convex and lower-bounded, we derive the non-asymptotic convergence rate of AdaMomentum.
Theorem 2. We suppose that Assumptions 2 and 4 hold, and β1,t is chosen such that 0 ≤ β1,t+1 ≤ β1,t <
1, 0 < β2,t < 1,∀t ∈ [T]. We further assume at∕√vt ≥ at+ι∕√vt+ι for any t ∈ [T], PT=I at ≤ η(T).
TaT, mint∈[T],j∈[d] vt,j ≥ c ≥ and mint∈[T] at ≤ a, then we have:
・明∣V7"□ Wtr H l^C1H2η(T) , da da2
minEkVf (θt)kt ≤ ToT [—c —+C2 √c+C3 -+c4
Taτ(QI+Q2 η(T ))，
for some positive constants Q1, Q2. Here C1, C2 , C3 are positive constants independent ofd or T, while C4 is
a positive constant independent of T.
Note that the conditions in Theorem 2 can be satisfied in most scenarios (Kingma & Ba, 2015; Chen et al.,
2019; Reddi et al., 2019). For instance, we can simply employ the common setting at = ɑ/ʌ/t, βt,t = 1 /t.
Corollary 2. When at is further chosen to be a/ʌ/t, AdaMomentum satisfies:
min E kVf (θt)kt ≤ -H- [ CIH 2a2(1+log(T))+ Ct da + C3 da2 + C4
t∈[T]	2 一 TTa L	c	√c	c J
=√T(Q" Q曰 log(T)),
for some constants Qi, Qt. Ci, Ct, C3, C4 are similarly defined in Theorem 2.
Corollary 2 manifests the O(log(T)/√T) convergence rate of AdaMomentum in the nonconvex case when we
commonly use at = a/ʌ/t. We refer readers to the detailed proof in Appendix C.2.
5 Experiments
We empirically investigate the performance of AdaMomentum in both optimization and generalization.
5.1	Faster & Better Optimization
5.1.1	2-D Toy examples
We compare the optimization process of AdaMomentum with three prevalent optimziers SGDM, RMSprop,
Adam and recently proposed AdaBelief , on four classic and representative 2-variable objective functions in
1O(∙) denotes O(∙) with hidden logarithmic factors.
6
Under review as a conference paper at ICLR 2022
O	»	4
* ， ∙
O	2	Λ	t
(a) Trajectory for Sphere (b) Trajectory for Three-
Function.
Hump Camel Function.
(c) Trajectory for Beale (d) Trajectory for Ackley
Function.	Function.
Figure 2: 2D Trajectory visualization of SGDM, Adam, RMSprop, AdaBelief and AdaMomentum
on classic functions. AdaMomentum reaches the optimal point (marked as purple cross) the fastest
in all the cases and converges stably to the optimum without big oscillations. Best viewed in color.
Table 2: FID score (1) of Spectral Normalized Generative Adversarial Network on CIFAR-
10 (KrizheVsky & Hinton, 2009) dataset. f is reported in ZhUang et al. (2020).
SGDMt	Adam(W)*	YOgit	AdaBOUndt	RAdamt	AdaBeIieft	Ours
49.70 ± 0.41 13.05 ± 0.19 14.25 ± 0.15 55.65 ± 2.15 12.70 ± 0.12 12.52 ± 0.16 12.06 ± 0.21
nUmerical optimization literatUre: Sphere FUnction (bowl-shaped) (Dixon, 1978), Three-HUmp Camel FUnction
(valley-shaped)2, Beale FUnction (mUltimodal)3 and Ackley FUnction (with nUmeroUs local minima) (Adorio
& Diliman, 2013). To ensUre fair comparison, we Use the same hyperparameters in the foUr adaptive gradi-
ent methods and finetUne the learning rate of SGDM for the best performance. As illUstrated in FigUre 2,
AdaMomentUm achieves the most rapid convergence in all the cases and is stable once reaching the global
mininUm. Meanwhile, SGDM is highly Unstable and inaccUrate in the descending directions, and RMSprop,
Adam, AdaBelief is mUch slower. AlthoUgh these toy examples are simple, they give hints to the behavior of
optimizers in complex deep learning tasks as they can be viewed as the local dynamics which occUr freqUently
in deep learning (ZhUang et al., 2020). The details of the loss fUnctions and hyperparameter configUrations of
the experiment are in Appendix A. The GIFs and the 3D trajectory figUres are inclUded in the sUpplementary.
5.1.2 Generative Adversarial Network
Training of GANs is extremely Unstable. To fUrther stUdy the optimization ability and stability of AdaMomen-
tUm, we experiment on GAN eqUipped with spectal normalization (Miyato et al., 2018). For the generator and
the discriminator network, we adopt ResNets for adeqUate expression ability. We train the model for 100000
iterations on CIFAR-10 with batch size 64, and the two learning rates are set both as 0.0002. For AdaMomen-
tUm all the other hyperparameters are set as defaUlt valUes. Experiments are rUn 5 times independently and we
report the mean and standard deviation of Frechet Inception Distance (FID, the lower the better) HeUsel et al.
(2017) in Table 2. From Table 2 it is reasonable to draw the conclUsion that AdaMomentUm oUtperforms all
the best tUned baseline optimizers by a large margin, reaching mean FID score as low as 12.06 with its defaUlt
hyperparameter valUes. Here Adam eqUals AdamW becaUse the optimal weight decay parameter valUe is 0.
5.2 Superior Generalization
We condUct experiments on varioUs modern network architectUres for different tasks covering both vision
and langUage processing area: 1) image Classification on CIFAR-10 (Krizhevsky & Hinton, 2009) and Ima-
geNet (RUssakovsky et al., 2015) with CNN; 2) langUage modeling on Penn Treebank (MarcUs et al., 1993)
dataset Using Long Short-Term Memory (LSTM) (Hochreiter & SchmidhUber, 1997b); 3) neUral machine trans-
lation on IWSTL’14 DE-EN (Cettolo et al., 2014) dataset employing Transformer. We compare AdaMomen-
tUm with seven state-of-the-art optimizers: SGDM (SUtskever et al., 2013), Adam (Kingma & Ba, 2015),
AdamW (Loshchilov & HUtter, 2017b), Yogi (Reddi et al., 2018), AdaBoUnd (LUo et al., 2019), RAdam (LiU
et al., 2019) and AdaBelief (ZhUang et al., 2020). We perform a carefUl and extensive hyperparameter tUning for
2https://en.wikipedia.org/wiki/Test_functions_for_optimization
3http://www-optima.amp.i.kyoto-u.ac.jp/member/student/hedar/Hedar_
files/TestGO.htm
7
Under review as a conference paper at ICLR 2022
..................................
989694929088
(％) >υro⅛υυ< ura-ll
0	2 5 50 75 1∞ U5 150 175 200
Epoch
(a)	Train Accuracy for VGGNet.
(东)>U2□UU< C-2F
O 25	50	75 IOO 125	150	175	200
Epoch
(b)	Train Accuracy for ResNet.
(东)>U2□UU< C-2F
96
94
92
90
88
O 25	50	75 IOO 125	150	175	200
Epoch
(c)	Train Accuracy for DenseNet.
O 25	50	75	1∞	125	150	175	200
Epoch
Epoch
O 25	50	75 IOO 125	150	175	200
(d) Test Accuracy for VGGNet. (e) Test Accuracy for ResNet. (f) Test Accuracy for DenseNet.
Figure 3:	Train and test accuracy of different optimizers on CIFAR-10 (Krizhevsky & Hinton, 2009).
all the optimizers compared, and the detailed strategy and configuration are given in Appendix D due to space
limit. It is worth mentioning that in experiments we discover that setting α = 0.001, β1 = 0.9, β2 = 0.999
(the default setting for adaptive gradient methods in applied machine learing) works well in most cases. This
elucidates that our optimizer is tuning-friendly, which reduces human labor and time cost and is crucial in
practice. The mean results with standard deviations over 5 seeds are reported in all the following experiments.
5.2.1	CNN for Image Classification
Table 3: Test accuracy (%) of CNNs on CIFAR-10 dataset. The best in Red and second best in blue.
Architecture	Non-adaptive ∣		Adaptive gradient methods					Ours
	SGDM	Adam	AdamW	Yogi	AdaBound	RAdam	AdaBelief	
VGGNet-16	94.73±0.12	93.29±0.10	93.33±0.15	93.44±0.16	93.79±0.17	93.90±0.10	94.57±0.09	94.80±0.10
ResNet-34	96.47±0.09	95.39±0.11	95.48±0.10	95.28±0.19	95.51±0.07	95.67±0.16	96.04±0.07	96.33±0.07
DenseNet-121	95.03±o∙19	93.92±0.20	93.87±0.14	93.72±0.18	93.99±0.08	94.00±0.07	94.74±0.14	95.08±0.19
Table 4: Top-1 test accuracy (%) on ImageNet (Russakovsky et al., 2015) dataset.
SGDM	Adam	AdamW	Yogi	AdaBound	RAdam	AdaBelief	Ours
70.41±0.13	65.36±0.25	68.77±0.14	68.93±0.08	69.32±0.19	69.24±0.12	69.98±0.09	70.45±0.06
CIFAR-10 We experimented with three prevailing deep CNN architectures: VGG-16 (Simonyan & Zisser-
man, 2015), ResNet-34 (He et al., 2016) and DenseNet-121 (Huang et al., 2017). The growth rate of DenseNet-
121 is set as 12 to match CIFAR-10 dataset. In each experiment we train the model for 200 epochs with batch
size 128 and decay the learning rate by 0.2 at the 60-th, 120-th and 160-th epoch. We employ label smoothing
technique (Szegedy et al., 2016) and the smoothing factor is choosen as 0.1. Figure 3 displays the training
and testing results of all the compared optimizers . As indicated, both the training accuracy and the testing
accuracy using AdaMomentum can be improved as fast as with other adaptive gradient methods, being much
faster than SGDM, especially before the third learning rate annealing. In testing phase, AdaMomentum can
exhibit performance as good as SGDM and far exceeds other baseline adaptive gradient methods, including the
recently proposed AdaBelief (Zhuang et al., 2020) optimizer. This contradicts the result reported in Zhuang
et al. (2020), where they claim AdaBelief can be better than SGDM. This largely stems from the fact that
Zhuang et al. (2020) did not take an appropriate stepsize annealing strategy or tune the hyperparameters well.
Training 200 epochs with ResNet-34 on CIFAR-10, our experiments show that AdaMomentum and SGDM can
reach over 96% accuracy, while in Zhuang et al. (2020) the accuracy of SGDM is only around 94% .
8
Under review as a conference paper at ICLR 2022
口 一 xa-d-adttωl
80 j—>-1----1------------------------L-
0	2 5	50	75	1∞ U5 150	175	200
Epoch
(a)	I-Layer LSTM.
0	25	50	75	100	125	150	175	200
Epoch
(b)	2-Layer LSTM.
口 一 xa-d-adttωl
90
85
80
75
70
65
60
0	25	50	75	100	125	150	175	200
Epoch
(c)	3-Layer LSTM.
Figure 4:	Test perplexity curve on Penn Treebank (Marcus et al., 1993) dataset.
Table 5: Test perplexity (1) results of LSTMs on Penn Treebank (Marcus et al., 1993) dataset.
Layer # ∣ SGDM Adam AdamW Yogi AdaBound RAdam AdaBelief Ours
1	85.31±0.09	84.55±0.10	88.18±0.14	86.87±0.14	85.10±0.22	88.60±0.22	84.30±0.23	80.82±0∙19
2	67.25±0.20	67.11±0.20	73.61±0.15	71.54±0.14	67.69±0.24	73.80±0.25	66.66±0.11	64.85±0.09
3	63.52±0.16	64.10±0.25	69.91±0.20	67.58 ±0.08	63.52±0.11	70.10±0.16	61.33±0.19	60.08±0.11
ImageNet To further corroborate the effectiveness of our algorithm on more comprehensive dataset, we
perform experiments on ImageNet utilizing ResNet-18 as backbone network. We execute each optimizer for
90 epochs utilizing cosine annealing strategy, which can exhibit better performance results than step-based
decay strategy on ImageNet (Loshchilov & Hutter, 2017a; Ma, 2021). As indicated in Table 4, AdaMomentum
far exceeds Adam in Top-1 test accuracy and outperforms all the competitors including SGD with momentum.
5.2.2	LSTM for Language Modeling
We implement LSTMs with layer number from 1 to 3 on Penn Treebank dataset, where adaptive gradient
methods are the main-stream choices (much better than SGD). In each experiment we train the model for 200
epochs with batch size of 20 and decay the learning rate by 0.1 at 100-th and 145-th epoch. Test perplexity
(the lower the better) against training epochs is plotted in Figure 4 and the best perplexity value is summarized
in Table 5. Clealy observed from Figure 4 and Table 5, AdaMomentum achieves the lowest perplexity in all
the settings and consistently outperform other competitors by a considerable margin. The training curve is
given in Figure 5 in Appendix D due to space limit. Particularly on 2-layer and 3-layer LSTM, AdaMomentum
maintains both the fastest convergence and the best performance, which substantiates its superiority.
5.2.3	Transformer for Neural Machine Translation
Transformers have been the dominating architecture
in NLP and adaptive gradient methods are usually
employed to train transformers due to their stronger
Table 6: BLEU score (↑) on IWSTL’14 DE-
EN (Cettolo et al., 2014) dataset.
ability to handle attention-models (Zhang et al., SGDM Adam AdamW AdaBeIief Ours
2019). To test AdaMomentum on transformer, We	28.22±0.24 30.14±1.56 35.62±0.13 35.60±0.12 35.66±0.11
experiment on IWSTL'14 German-to-English with
the Transformer small model adapting the code from fairseq package.4 We set the length penalty as 1.0, the
beam size as 5, warmup initial stepsize as 10-7 and the warmup updates iteration number to be 8000. We train
the models for 55 epochs and the results are reported according to the average of the last 5 checkpoints. As
shown in Table 12, our optimizer achieves the highest average BLEU score with the lowest variance.
6	Conclusion
In this work, we proposed AdaMomentum as a new optimizer for machine learning. We theoretically demon-
strate why AdaMomentum outperforms Adam in optimization and generalization. We further validates the
superiority of AdaMomentum through both toy examples and large-scale experiments on real-world datasets.
Our algorithm is simple and effective with four key advantages: 1) maintaining fast convergence rate; 2) clos-
ing the generalization gap between adaptive gradient methods and SGD; 3) applicable to various tasks and
models; 4) introducing no additional parameters and easy to tune. Combination of AdaMomentum with other
techniques such as Nesterov’s accelerated gradient (Dozat, 2016) may be of independent interest in the future.
4https://github.com/pytorch/fairseq
9
Under review as a conference paper at ICLR 2022
7	Ethics Statement
Our work follows all ethical standards and laws. All the experiments were conducted on publically available
datasets, with no new data concerning human or animal subjects generated.
8	Reproducibility Statement
We adhere to ICLR reproducibility standards and provide all necessary information to reproduce our experi-
mental and theoretical results. We ensure the reproducibility of our work through several ways, namely
•	All the source code and presented figures are available at anonymous link https://anonymous.
4open.science/r/AdaMomentum_experiments-6D9B.
•	The detailed descriptions of the classic loss functions used in Toy exmpales in Section 5.1.1 are given
in Appendix A.
•	All the technical details and proofs in Section 3.2 are included in Appendix B. All the proofs in
Section 4 are provided in Appendix C.
•	The detailed hyperparameter tuning rule and configurations of the experiments in Section 5 are given
in Appendix D.
References
E.P. Adorio and U.P. Diliman. MVF: Multivariate test functions library in c for unconstrained global optimiza-
tion. Retrieved June 2013, from http://www.geocities.ws/eadorio/mvf.pdf, 2013.
Lukas Balles and Philipp Hennig. Dissecting adam: The sign, magnitude and variance of stochastic gradients.
In ICML, 2018.
Sebastien Bubeck. Convex optimization: Algorithms and complexity. arXiv preprint arXiv:1405.4980, 2014.
Mauro Cettolo, Jan Niehues, Sebastian Stuker, Luisa Bentivogli, and Marcello Federico. Report on the 11th
iwslt evaluation campaign, iwslt 2014. In Proceedings of the International Workshop on Spoken Language
Translation, Hanoi, Vietnam, 2014.
Xiangyi Chen, Sijia Liu, Ruoyu Sun, and Mingyi Hong. On the convergence ofa class of adam-type algorithms
for non-convex optimization. In ICLR, 2019.
KyUnghyUn Cho, Bart van Merrienboer, Dzmitry Bahdanau, and Yoshua Bengio. On the properties of neural
machine translation: Encoder-decoder approaches. In Proceedings of SSST-8, Eighth Workshop on Syntax,
Semantics and Structure in Statistical Translation, 2014.
Laurence Charles Ward Dixon. The global optimization problem. an introduction. Toward global optimization,
1978.
Timothy Dozat. Incorporating nesterov momentum into adam. ICLR Workshop, 2016.
John Duchi, Elad Hazan, and Yoram Singer. Adaptive subgradient methods for online learning and stochastic
optimization. JMLR, 2011.
Fartash Faghri, David Duvenaud, David J Fleet, and Jimmy Ba. A study of gradient variance in deep learning.
arXiv preprint arXiv:2007.04532, 2020.
Saeed Ghadimi and Guanghui Lan. Stochastic first-and zeroth-order methods for nonconvex stochastic pro-
gramming. SIAM Journal on Optimization, 2013.
Ian J Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron
Courville, and Yoshua Bengio. Generative adversarial networks. NeurIPS, 2014.
Haowei He, Gao Huang, and Yang Yuan. Asymmetric valleys: Beyond sharp and flat local minima. NeurIPS,
2019.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In
CVPR, 2016.
Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. Gans trained
by a two time-scale update rule converge to a local nash equilibrium. NeurIPS, 2017.
10
Under review as a conference paper at ICLR 2022
SePP HoChreiter and Jurgen SChmidhuber. Flat minima. Neural computation, 1997a.
SePP Hochreiter and Jurgen SChmidhuber. Long short-term memory. Neural Computation, 1997b.
Gao Huang, Zhuang Liu, Laurens Van Der Maaten, and Kilian Q Weinberger. Densely ConneCted Convolutional
networks. In CVPR, 2017.
Pavel Izmailov, Dmitrii PodoPrikhin, Timur GariPov, Dmitry Vetrov, and Andrew Gordon Wilson. Averaging
weights leads to wider oPtima and better generalization. UAI, 2018.
Rie Johnson and Tong Zhang. ACCelerating stoChastiC gradient desCent using PrediCtive varianCe reduCtion.
NeurIPS, 2013.
Nitish Shirish Keskar and RiChard SoCher. ImProving generalization PerformanCe by switChing from adam to
sgd. arXiv preprint arXiv:1712.07628, 2017.
Nitish Shirish Keskar, Dheevatsa Mudigere, Jorge NoCedal, Mikhail Smelyanskiy, and Ping Tak Peter Tang.
On large-batCh training for deeP learning: Generalization gaP and sharP minima. ICLR, 2017.
Diederik P. Kingma and Jimmy Ba. Adam: A method for stoChastiC oPtimization. In ICLR, 2015.
A. Krizhevsky and G. Hinton. Learning multiPle layers of features from tiny images. Master’s thesis, Depart-
ment of Computer Science, University of Toronto, 2009.
Paul Levy and Paul Levy. Theorie de ('addition des variables aleatoires. Gauthier-Villars, 1954.
Hao Li, Zheng Xu, Gavin Taylor, ChristoPh Studer, and Tom Goldstein. Visualizing the loss landsCaPe of
neural nets. NeurIPS, 2018.
Liyuan Liu, Haoming Jiang, PengCheng He, Weizhu Chen, Xiaodong Liu, Jianfeng Gao, and Jiawei Han. On
the varianCe of the adaPtive learning rate and beyond. ICLR, 2019.
Ilya LoshChilov and Frank Hutter. Sgdr: StoChastiC gradient desCent with warm restarts. ICLR, 2017a.
Ilya LoshChilov and Frank Hutter. DeCouPled weight deCay regularization. ICLR, 2017b.
LiangChen Luo, Yuanhao Xiong, Yan Liu, and Xu Sun. AdaPtive gradient methods with dynamiC bound of
learning rate. ICLR, 2019.
Xuezhe Ma. APollo: An adaPtive Parameter-wised diagonal quasi-newton method for nonConvex stoChastiC
oPtimization, 2021. URL https://openreview.net/forum?id=5B8YAz6W3eX.
MitChell P. MarCus, BeatriCe Santorini, and Mary Ann MarCinkiewiCz. Building a large annotated CorPus of
English: The Penn Treebank. Computational Linguistics, 1993.
Takeru Miyato, Toshiki Kataoka, Masanori Koyama, and YuiChi Yoshida. SPeCtral normalization for generative
adversarial networks. ICLR, 2018.
Ilya Pavlyukevich. First exit times of solutions of stochastic differential equations driven by multiplicative levy
noise with heavy tails. Stochastics and Dynamics, 2011.
Martin Popel and Ondrej Bojar. Training tips for the transformer model. PBML, 2018.
S Reddi, Manzil Zaheer, Devendra Sachan, Satyen Kale, and Sanjiv Kumar. AdaPtive methods for nonconvex
oPtimization. In NeurIPS, 2018.
Sashank J Reddi, Satyen Kale, and Sanjiv Kumar. On the convergence of adam and beyond. ICLR, 2019.
Herbert Robbins and Sutton Monro. A stochastic aPProximation method. The annals of mathematical statistics,
1951.
Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang, Andrej
KarPathy, Aditya Khosla, Michael Bernstein, et al. Imagenet large scale visual recognition challenge. IJCV,
2015.
Robin M Schmidt, Frank Schneider, and Philipp Hennig. Descending through a crowded valley-benchmarking
deeP learning oPtimizers. ICML, 2021.
Leon Simon et al. Lectures on geometric measure theory. The Australian National University, Mathematical
Sciences Institute, Centre for Mathematics & its Applications, 1983.
11
Under review as a conference paper at ICLR 2022
Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image recognition.
ICLR, 2015.
Umut Simsekli, Levent Sagun, and Mert Gurbuzbalaban. A tail-index analysis of stochastic gradient noise in
deep neural networks. In ICML, 2019.
Ilya Sutskever, James Martens, George Dahl, and Geoffrey Hinton. On the importance of initialization and
momentum in deep learning. In ICML, 2013.
Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jon Shlens, and Zbigniew Wojna. Rethinking the incep-
tion architecture for computer vision. In CVPR, 2016.
Geoffrey Hinton Tijmen Tieleman. Lecture 6.5-rmsprop: Divide the gradient by a running average of its recent
magnitude. Coursera: Neural networks for machine learning, 2012.
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser,
and Illia Polosukhin. Attention is all you need. NeurIPS, 2017.
Ashia C Wilson, Rebecca Roelofs, Mitchell Stern, Nathan Srebro, and Benjamin Recht. The marginal value of
adaptive gradient methods in machine learning. NeurIPS, 2017.
Jingzhao Zhang, Sai Praneeth Karimireddy, Andreas Veit, Seungyeon Kim, Sashank J Reddi, Sanjiv Kumar,
and Suvrit Sra. Why are adaptive methods good for attention models? arXiv preprint arXiv:1912.03194,
2019.
Pan Zhou, Jiashi Feng, Chao Ma, Caiming Xiong, Steven Hoi, and Weinan E. Towards theoretically under-
standing why sgd generalizes better than adam in deep learning. In NeurIPS, 2020.
Juntang Zhuang, Tommy Tang, Yifan Ding, Sekhar Tatikonda, Nicha Dvornek, Xenophon Papademetris, and
James Duncan. Adabelief optimizer: Adapting stepsizes by the belief in observed gradients. NeurIPS, 2020.
Martin Zinkevich. Online convex programming and generalized infinitesimal gradient ascent. In ICML, 2003.
12
Under review as a conference paper at ICLR 2022
A Details of Toy Examples
The detailed description of the four 2-parameter loss functions are given as below:
The Sphere Function is
f (x) = x12 + x22 ,
whose global minimum is f (x*) = 0 at x* = (0, •…,0).
The Three Hump Camel Function is
x6
f (x) = 2x1 — 1.05x1 +—+ + X1X2 + x2,
6
whose global minimum is f (x*) = 0 at x* = (0, •…,0).
The Beale Function is
f(x) = (1.5 - x1 + x1x2)2 + (2.2 - x1 + x1 x22 )2 + (2.625 - x1 + x1x32)2 ,
whose global minimum is f(x*) = 0 at x* = (3, 0.5).
The Ackley Function is
f (x) = -20exp (—0.2小^ (x2 + x2)) — exp ɑ (cos(2πxι) + cos(2∏x2))) + 20 + exp(1),
whose global minimum is f (x*) = 0 at x* = (0, •…,0).
Hyperparameter configuration For all the four toy experiments, we employ the same stepsize for the
three adaptive gradient methods: RMSProp, Adam and AdaMomentum. For SGDM We set the momentum
parameter as 0.9 and finetune the learning rate for each toy experiment in set {0.0001, 0.001, 0.01, 0.1, 1, 10}
as the optimal learning rate varies for different loss functions. After careful parameter tuning, the configuration
is: for Sphere Function, the learning rate for adaptive gradient methods and SGDM are both 0.1; for Three-
Hump Camel Function, the learning rate for adaptive gradient methods is 0.1 and the learning rate for SGDM
is 0.001; for Beale Function, the leanring rate for adaptive, gradient methods is 0.1 and the learning rate for
SGDM is 0.1; for Ackley Function, the leanring rate for adaptive, gradient methods is 0.1 and the learning rate
for SGDM is 0.1. Combining this with Figure 2, we can see that AdaMomentum with the stepsize 0.1 manifests
universally faster and more stable convergence to the optimum than discreetly tuned SGDM and other adaptive
gradient approaches.
B	Technical details of Subsection 3.2
Here We provide more construction details and technical proofs for the Levy-driven SDE in Adam-alike adap-
tive gradient algorithm (2). In the beginning we introduce a detailed derivation of the process (3) as well as
its corresponding escaping set Υ in definition 2. Then we give some auxiliary theorems and lemmas, and
summarize the proof of Lemma 1. Finally we prove the proposition 1 and give a more detailed analysis of the
conclusion that the expected escaping time of AdaMomentum is longer than that of Adam in a comparatively
flat basin.
B.1	DERIVATION OF THE LEVY-DRIVEN SDE (3)
To derive the SDE of Adam-alike algorithms (2), We firstly define mt = β1mt-1 + (1 — βι)Vf (θt) with
m00 = 0. Then by the definition it holds that
t
m0t — mt = (β1 — 1) Xβ1t-iζt.
i=0
Following Simsekli et al. (2019), the gradient noise ζt has heavy tails in reality and hence we assume that
1—βι (mt — mt) obeys SeS distribution with time-dependent covariance matrix Σt. Since we can formulate
(2) a1s
θt+ι = θt — am + α(mt - mt) where Zt = (1 — βt)/门 VtHt；,	(4)
zt	zt	1	(1 — β2t )
and we can replace the term (rm - mt) by a-α (1 - βt)ΣtS where each coordinate of S is independent
and identically distributed as SαeS(1) based on the property of centered symmetric αe-stable distribution. Let
13
Under review as a conference paper at ICLR 2022
Rt = diag(q
Vt
(i-β2)
), and we further assume that the step size
α is small, then the continuous-time version of
the process (4) becomes the following SDE:
dθt = -R-1 Ttddtt∖ + α1-αe R-1∑t dLt
(1 - β1t)
dmt = βι(Vf (θt) - mt), dvt =e2(姆一vt).
After replacing m0t with mt for brevity, we get the SDE (3) consequently.
B.2	Proof of Lemma 1
To prove Lemma 1, we first introduce Theorem 3.
Theorem 3. Suppose Assumptions 1-3 hold. We define κι = V ∣T1L-ι∣ and κ =e]不+目『(βι — β42) With
a constant ci. Let υα+1 = Θ(α), po = i6(i+c2) and ln (l2∆//3 ) ≤ κ2υ-1/3 where ∆ = f (θo) — f (θ*) and
a constant c2. Then for any θo ∈ Ω-2υ", U > —1, U ∈ (0, υo], Y ∈ (0, γo] and P ∈ (0, po] satisfying υγ ≤ po
and limυ→0 ρ = 0, the Adam-alike algorithm in (2) obey
1 - P
1 + u + P
≤ E [exp (—um(Υ)Θ(υ-1)Γ)] ≤
1 + P
1 + U - ρ
From Theorem 3, by setting υ small, it holds that for any adaptive gradient algorithm the upper and lower
bounds of its expected escaping time Γ is at the order of (盛丫)), which directly implies Lemma 1 conclusively.
Therefore, it suffices to validate Theorem 3.
Theorem 3 is adapted from Theorem 1 in Zhou et al. (2020) and the proof is given in Section B.2.3. Before
we proceeed, we first provide some prerequisite notations in Section B.2.1 and list some useful theorems and
lemmas in Section B.2.2.
B.2.1	Preliminaries
For analyzing the uniform Levy-driven SDES in (3), we first introduce the Levy process Lt into two components
ξt and εi, namely
Lt = ξt + εt ,	(5)
whose characteristic functions are respectively defined as
E heihλ,ξtii
E heihλ,εtii
=etR Rd∖{o}εl{kyk2≤ υδ }ν(dy)
=etR Rd∖{o}εl{kyk2≤ υδ }ν(dy)
where ε = eihλ,yi — 1 — i〈X,y〉I {∣∣y∣∣2 ≤ 1} with U defined in (3) and a constant δ s.t. υ-δ < 1. Accordingly,
the Levy measure V of the stochastic processes ξ and ε are
νξ = V(A	∩{kyk2	≤	Uδ}),	νε	= V(A	∩{kyk2	≥	Uδ}), where A ∈B(Rd).
Besides, for analysis, we should consider affects of the Levy motion Lt to the Levy-driven SDE of Adam
variants. Here we define the Levy-free SDE accordingly:
(dθt =	—μtQ-1m t,
dmb t = β1 (Vf (θbt) — mb t),	(6)
Idbt =	β2(V(fθt)2 — bt).
where Qt = diag(√Vt).
B.2.2	Auxiliary theorems and lemmas
Theorem 4 (Zhou et al. (2020)). Suppose Assumptions 1-3 hold. Assume the sequence {(θt , mb t , vbt)} are
produced by (6). Let St = hqt (√ωtbt) with ht = βι, qt = (1 — βt)-i and ωt = (1 — βt)-i. We define
kx∣∣y = Pi yix2. Then for Levy-driven Adam SDEs in (6), its Lyapunov function L(t) = f (θt) — f (θ*) +
1 kmt kb-i with the optimum solution θ* in the current local basin Ω obeys
Lt ≤ AexP (- βιv+"+μτ (β1 - β卜)，
14
Under review as a conference paper at ICLR 2022
1	A	" G ∖	c∕^Γ⅛∖	1	∙^∙	C El	( ri' 1	1	11 ，八 1
where ∆ =	f (θo) —	f (θ ) due to m0	= 0. The sequence	{θt}	produced by (6) obeys
θbt — θ*∣∣2 ≤ 2∆exp
2μτ
βιv+ + μτ
(β1 -力).
—
Lemma 2 (Zhou et al. (2020)). ⑴ The process ξ in the Levy process decomposition can be decomposed into
two processes ξ and linear drift, namely,
ξt = ξt + μυt,	(7)
where ξ is a zero mean Levymartingale With boundedjumps.
⑵ Let δ ∈ (0,1),μυ = E(ξι) and TU = υ-θ for some θ > 0,po = po(δ) = 1-δ > 0 and θo = θo(δ)=
1-δ > 0. Suppose U is sufficiently small such that Θ(1) ≤ U- 1-δ and υ-ρ — 2(C + Θ(1))υ6(1-δ)+2 ≥ 1
with a constant C = ∣R0<u≤1 u2dΘ(u)∣ ∈ (0, +∞). Then for all δ ∈ (0,δ0),θ ∈ (0,θo) there are po =
po (δ) = 2 and υo = υo(δ, ρ) such that the estimates
kUξTυ k2 = U kμυk2Tυ < U2ρ and P([Uξ]dTυ ≥ Uρ) ≤ exp(—U-p),
hold for all p ∈ (0, p0] and U ∈ (0, U0]
Lemma 3 (Zhou et al. (2020)). Let δ ∈ (0,1) and gt≥° be a bounded adapted Cadlag stochastic process with
values in Rd, TU = υ-θ, θ > 0. Suppose supt≥o Ilgtk is well bounded. Assume po = po(δ) = 1-δ > 0,
θo = θo(δ) = 1-δ > 0, po = P. For ξt in (7), there is δo = δo(δ) > 0 such that for all P ∈ (0,po) and
θ ∈ (0, θ0 ), it holds
P sup U
o≤t≤Tυ
dt
X	gsi-dξbsi
i=1 o
≥ UP) ≤ 2exp (—υ-p),
i
for all p ∈ (0, po] and 0 < U ≤ Uo with Uo = U(p), where ξsi represents the i-th entry in ξs .
Lemma 4 (Zhou et al. (2020)). Under Assumptions 1-3 hold, assume δ ∈ (0,1),po = po(δ)=3]+-：^)>
0,θo = θo(δ) = ⅛δ > 0,po = min(b(1+c1 κι) ,p), c2 ln (券)≤ υ-θ0 where κι = ν-∣τm-ι∣ and
C2 = β]Vμ+μτ (βι — β2) in Adam-alike adaptive gradient algorithms. For all pb ∈ (0, po), P ∈ (0,po],
0 < U ≤ Uo with Uo = Uo(pb), and θo = θo, we have
SuP P ( SuP ∣∣θt — θt∣∣ ≥ 2υb ) ≤ 2exp(-υ-P),
θ0 ∈Ω ∖O<t<σ1 Il	∣∣2 - ，一
(8)
1	.1	/ʌ	1 ri'	，•	1	1	1 1	∕c∖	1 ，八■	1	ι ■	.ι ι
where the sequences θt and θt are respectively produced by (3) and (6) in adaptive gradient method .
B.2.3	Proof of Theorem 3
r∖ C mi - ι	ɛ ,ι ■	r-	r-	∕c∖	ι	ι ■ τ	» ι	，i	/1	ι ri' .	. ɛ
Proof. The idea of this proof comes from (8) we showed in Lemma 4 where the sequence θt and θt start from
the same initialization. Based on Theorem 4, we know that the sequence {θt} from (6) exponentially converges
to the minimum θ* of the local basin Ω. To escape the local basin Ω, we can either take small steps in the
process ζ or large jumps Jk in the process ε. However, (8) suggests that these small jumps might not be helpful
for escaping the basin. And for big jumps, the escaping time Γ of the sequence {θt } most likely occurs at the
time σ1 if the big jump UJ1 in the process ε is large.
The verification of our desired results can be divided into two separate parts, namely establishing upper bound
and lower bound of E [exp (—um(Υ)Θ(υ-1)Γ)] for any U > —1. Both of them can be established based on
the following facts:
IP(R-1∑θUJk ∈ Ω±uy, ∣υJkkp ≤ R) — P (R-1∑θ*UJk ∈ Ω土υγ, kυJkkp ≤ R)∣ ≤ δ ∙ θ(^-ə,
∣P (R-1∑θUJk ∈ Ω, ∣UJk∣2 ≤ R) — P (R-*1∑θ*UJk ∈	Ω,	∣UJkkp	≤ R)∣ ≤	| ∙ I(U-^,
P (R-*1∑θ*UJk ∈ Ω) — P(R-*1∑θ*UJk ∈ Ω, ∣UJk∣2 ≤	R)	≤ δj0 ∙	|(：二).	(9)
Specifically, for the upper bound of E [exp (—um(Y)I(U-1)Γ)], we consider both the big jumps in the
process ε and small jumps in the process ζ which may escape the local minimum. Instead of estimating the
escaping time Γ from Ω, we first estimate the escaping time e from Ω-ρ. Here we define the inner part of Ω
as Ω-ρ := {y ∈ Ω : dis(∂Ω,y) ≥ p}. Then by setting P → 0, we can use e for a decent estimation of Γ.
15
Under review as a conference paper at ICLR 2022
We denote ρ = υγ where Y is a constant SuCh that the results of Lemma 2-4 hold. So for the upper bound We
mainly focus on Ξe in the beginning and then transfer the results to Γ. In the beginning, we can show that for
any u > -1 it holds that,
+∞
E exp -um(Υ)Θ(υ-1)Ξe ≤ XE e-um(Υ)Θ(υ-1)tk I Ξe = tk + Resk ,
k=1
where
E
Resk ≤
E
e-um(Υ)Θ(υ-1)tkI nΞe ∈ (tk-1,
e-um(Υ)Θ(υ-1)tk-1InΞe ∈ (tk-
ifu ∈ (-1, 0]
, if u ∈ (0, +∞).
Then using the strong Markov property we can bound the first term E e-um(Υ)Θ(υ-1)tk I Ξe = tk	as
Ri = +∞ E he-um(γ)θ(υ-1)tkI {Γ = tk}i ≤ αυ(I+ P/3) X (
k=1	1+uαυ	k=1
1 - αυ(1 - ρ)	-
1 + uαυ
≤ αυ (1 + ρ∕3)
1 + uαυ
+∞
X(
k=0
1 — ɑυ (1 — ρ)
1 + uαυ
k-1
1 + P/3
1 + U - ρ
On the other hand, for the lower bound of E exp -um(Υ)Θ(υ-1)Γ , we only consider the big jumps in
the process ε which could escape from the basin, and ignore the probability that the small jumps in the process
Z which may also lead to an escape from the local minimum θ*. Specifically, we can find a lower bound by
discretization:
+∞
E [exp (-um(Υ)Θ(υ-1)Γ)] ≥ XE [exp (-um(Υ)Θ(υ-1)tk)I{Γ = tk}].
Then we can lower bound each term by three equations (9) we just listed here, which implies that for any
θo ∈ Ω-:
E he-um(Υ)Θυ-1 Γi ≥ αυ (I - P)	( 1 - αυ (I + P) ) fc 1 =	1 - P
1	_l - 1 + uαυ ~[ ∖	1 + uαυ	J 1 + U + p ,
where P → 0 as U → 0. The proof is completed.	口
B.3 Proof of Proposition 1
Proof. Since we assumed the minimizer θ* = 0 in the basin Ω which is usually small,we can employ second-
order Taylor expansion to approximate Ω as a quadratic basin whose center is θ*. In other words, we can
write
Ω = {y ∈ Rd f(θ*) + 1 y>H(θ*)y ≤ h(θ*)
where H(θ*) is the Hessian matrix at θ* of function f and h(θ*) is the basin height. Then according to
Definition 2, we have
Υ = {y ∈ Rd I y>∑θ* R-1H(θ*)R-*1∑θ*y ≥ hf}.
Here Re* = limθt→θ* diag(,vt∕(1 — β2)) is a matrix depending on the algorithm, hf = 2(h(θ*) — f (θ*))
and ∑θ* is independent of the alogorithm, i.e. the same for Adam and AdaMomentum. Firstly, we will prove
that vt(ADAMOMENTUM) ≥ vt(ADAM) when t → ∞. To clarify the notation, we use θt , mt , vt , gt to denote the
symbols for Adam and θt , me t , vet , get for AdaMomentum, and ζt is the gradient noise. By using Lemma 1 and
above results, we have θt ≈ θt ≈ θ* before escaping when t is large, and thus Vt = limθt→e* [Vf (θt) + Zt]2
and e = limθt→θ* [βιmt-i + (1 — βι)(Vf (θt) + Zt)[2. We will firstly show that E(et) ≥ E(Vt) when t is
large.
E(Vt) =E( lim*[Vf(θt)+Zt]2)=(i) lim* E([Vf(θt) + Zt]2)
θt→θ*	θt→θ*
elim* (E(Vf(θt)2) + E(2Vf(θt)Zt) + E(Z2))
(=ii)E(θtl→imθ*Vf(θt)2)+θtl→imθ*E(2Vf(θt)Zt)+θtl→imθ*E(Zt2)
(=iii)	lim
θt→θ*
E(Zt2),
16
Under review as a conference paper at ICLR 2022
where (i) and (ii) are due to the dominated convergence theorem (DCT) since we have that we know both
∣∣Vf (θt)k2 and l∣Vf(θt) + Z2k2 could be bounded by H in Assumption 4. And (iii) is due to the fact that
Vf (θ*) = 0 since function f attains its minimum point at θ*, and Zt has zero mean, i.e.
lim* E(Vf (θt)Zt )= lim* E(Vf (θt))E(Zt) = 0.
θt→θ*	θt→θ*
And similarly we can prove that,
E(vet) =E	lim [β1met-1 +(1 -β1)(Vf(θet)+ζt)]2
∖θt-θ*	J
= lim	E(β12me t2-1) + E((1 - β1)2(Vf(θet) + ζt)2) + E(2β1(1 - β1)me t-1V(f(θet) + ζt))
θt→θ*
=(i) β12 lim* E(met2-1) + (1 - β1)2 lim*E(ζt2),
where we can get the equality (i) simply by the same argument with dominated convergence theorem we just
used:
lim E(V(f(θet)2) = E( lim V(f(θet)2)=(i) 0,
θt→θ*	θt→θ*
lim E(V(f(θet)ζt) =E( lim V(f (θet)ζt) (=ii) 0,
θet →θ*	θet→θ*
lim E(met-1(Vf(θet) + ζt)) =E( lim met-1Vf(θet))+ lim E(me t-1)E(ζt) (=iii) 0,
θet→θ*	θet→θ*	θet→θ*
2
where we get the equality (i) and (ii) since the function f(θt)2 and f (θt)ζt could be absolutely bounded by
H2. And the first term in equality (iii) is 0 since we have ∣∣τmt-ι1卜 ≤ H by its definition and Vf (θ*) = 0,
and the second term vanishes since the noise ζt has zero mean. Based on the Assumption 5, we have
2 - β1
E(m2-1)≥ --βλE(Z2),
β1
which implies that E(vet) ≥ E(vt) when t is large. It further indicates that R(θA*DAMOMENTUM) ≥ Rθ(A*DAM).
We consider the volume of the complementary set
Yc = {y ∈ Rdl y>∑θ*R-*1H(θ*)R-*1∑θ*y < hf },
which can be viewed as a d-dimensional ellipsoid. We can further decompose the symmetric matrix M :=
Σθ* R-IH(θ*)R-*1∑θ* by SVD decomposition
M = U>AU,
where U is an orthogonal matrix and A is a diagonal matrix with nonnegative elements. Hence the transforma-
tion y → Uy is an orthogonal transformation which means the volume of Υc equals the volume of set
{y0 ∈ Rd∣y0>Ay0 <hf}.
Considering the fact that the volume of a d-dimensional ellipsoid centered at 0 Ed(r) = {(xι,x2, ∙∙∙ , Xn):
2
Pd=I R ≤ 1} is
i
n
∏ 2	-
V(Ed(r)) = AnJ	∏n=1Ri,
r( 2 + 1)
and the fact we just proved that Rθ(A* DAMOMENTUM) ≥ R(θA*DAM). Therefore we deduce the volume of
Y(ADAMOMENTUM) is smaller than that of Y(ADAM), which indicates that for Radon measure m(∙) We have
m(Υ(ADAMOMENTUM)) ≥ m(Υ(ADAM)). Based on Lemma 1, we consequently have E(Γ(ADAMOMENTUM)) ≥
E(P(ADAM)).	□
C Proofs in Section 4
C.1 Proof of the convergence results for the convex case
17
Under review as a conference paper at ICLR 2022
C.1.1 Proof of Theorem 1
Proof. Firstly, according to the definition of AdaMomentum in Algorithm 1, by algebraic shrinking we have
2	T-1
Xmti
√⅛ = X
T
m2 +
√tvt^
(PT=I(I- %)πk-jβιτ-k+ιgj,i^
{T PT=ι(1- β2)βT-j mj,i
T -1	2
≤ T 粤i= +
士 √vt,* i
(Pj=1 ∏T-j βl,T-k+l)(Pj=ι ∏T-j β1,T-k+lgj,i}
Jt PT=1(l-β2)βT-j mj,i
≤ X & + (PT=ι βT-j)(PT=1 βT-jgj,i)
_ t=ι E	Jt(1- β2 PT=1 βT-jmj,i
T-1	2
≤X 等=+
t=1 √tvti
T-1	2
Xmti
=√tv⅛ +
1	X______________________βT-jg2,i____________________
(I- β1)PTQf j=1 JpT=I βT-j (Pj=1(1 - βι,ι)∏k=1βι,j-k+ιgι,i)2
X & +	1	X	βT-jg2,i
L G (I —I)PT(T-β) 之 JpT=ι β厂j((1-βι,j)gj,ι)2
≤
T-1	2
Xmti
√⅛ +
1	X	βT-jg2,i
(1-βl)pT(T-β) & qβT-j(1- β1,j)2g2,i
T -1	2
≤)X型三+
— t=1 √v,
1
(1 - βι)2C(f-β)
T
XγT-jgj,i,
j=1
where (i) arises from βι,t ≤ βι, and (ii) comes from the definition that Y = √1^. Then by induction, We have
X 工 ≤ X------------------1	X γt-jgj i
t=1 √tvti	t=1 (I- βι)2Pt(1 -场)j=1
(i)
≤
(1 - β1)2√Γ-W
(1 - βι )2 λ∕1 - β2
(1 - βι)2VΓ-β2
1
(1 - βι)2√1≡J2
γj-t
√jf
TT
Xgt,iX
TT
Xgt,iX
T
∑gt,i ∙
t=1
γj-t
(1 - γ)√t
T
≤
≤
≤
1
1
1
1
1
≤
(1 - β1)2(1 - γ)√1 - β2
≤ (1 - β1)2(1- γ)√1≡12 kg1zik2 tX t
(≤)	√1+log T
≤ (1 - βι)2(1 - γ)√r-W
kg1:T,ik2,
where (i) exchangings the indices of summing, (ii) employs Cauchy-Schwarz Inequality and (iii) comes from
the following bound on harmonic sum:
T1
Et ≤ 1+log T.
t=1
18
Under review as a conference paper at ICLR 2022
Due to convexity of ft , we get
ft(θt) - ft(θ*) ≤ g>(θt- θ*)
d
=X gt,i(θt,i - θ*i).	(10)
i=1
According to the updating rule, we have
mt
θt+ι = θt — at—=
√vt
θt - αt
mt-1 +
(11)
SubstraCting θ*, squaring both sides and considering only the i-th element in vectors, We obtain
(θt+1,i - θi)2 = (θt,i - θ*i)2 - 2αt
mt-1,i +
By rearranging the terms, We have
2at 1√^ gt,i(θt,i - θ3 = (θt,i-θi )2 - (θt+1,i - θ:i)2-2at∙√⅛ ∙ mt-1,i(θt,i - θ*i)+a2
Further We have
gt,i(θt,i - θp =2a(lʒɪ t) [(θt,i - θ:i)2 - (θt+ι,i - θ;i)2] +
+ ι⅛ ai- θt,i)mj,i
2at(1 -,βι,t)[(θt,i - θ∙i)2 - (θt+1,i - θ-i)2] +
at√VtJ
2(1 - βι,t)
at √vt,i
2(1- βι,t)
+ β1,t
+ 1 - βι,t
1
v 4 -
詈∙(θi-
√at
θt,i) ∙√at ∙ m⅛^
v4i
≤ 河
—2at(1 - βI)
[(θt,i -
-(θt+1,i - G)2] + 2(1 一 βι)
2
mt,i
βι,t
2at (1 — βι,t)
(θ; - etNk +
βι a	m2-ι,i
2(1 - βι) ∙√tVtt^
(12)
(13)
+
Where (13) bounds the last term of (12) by Cauchy-SchWarz Inequality and plugs in the value of at. Plugging
(13) into (11) and summing from t = 1 to T , We obtain
Td
R(T )= XX gt,i (θt,i - θ*i)
T d	——-
≤ XX 2θtττ⅛ [(θt,i-味2 - (θi,i
Td
-θ^i)2] + XX
t=1 i=1
a
2(1 - βι)
m2,i
√tv^
Td
+XX
t=1 i=1
d
β1,t
2at(1 - β1,t)
Td
(θ*,i - θt,i)2√V77 + XX
≤X	VvIi
-i=1 2aι(1 — βι)
(θ1,i - %)2 + 2(1 -仇)
t=1 i=1
Td
βιa	m2-ι,i
2(1 - βι) ∙√tVtt^
∑∑(θt,i - θ,*i)2
√vt-17
—
2
mt,i
√tVt,i
at-1
Td	Td
+ XX 2ai⅛) WL θt,i)2 历+XX
t=1 i=1	t	1	t=1 i=1
a
1- βι
(14)
(15)
19
Under review as a conference paper at ICLR 2022
where (15) rearranges the first term of (14). Finally utilizing the assumptions in Theorem 1, we get
d	--	τ d
RT) ≤X 20筌而 d∞ + 2(⅛ X X d∞
√vt-1,i
—
αt-1
,D∞ X X
+ 而f = ⅛
βι,W
αt
d
+X
i=1
α √1 + log T
(1 - βι)3(1 - γ)√1 - β2
kg1:T ,i k2
=X √vτi d2 + D∞ X X
==2ɑτ(1 - βι)	∞ + 2(1 - βι)==
β1,tv 吉
αt
d
+X
i=1
α √1 + log T
(1 - β1)3(1- γ)√1-W
kg1:T,ik2 ,
(16)
which is our desired result.
□
C.1.2 Proof of Corollary 1
Proof. Plugging αt = √ and βι,t = βιλt into (16), we get
Next, we employ
m02,i = 0 ≤ G2∞
R(T) ≤ 2⅛‰ X √t +
d
+X
i=1
D∞2
2α(1 - β1)
α √1 + log T
(1 - β1)3(1 - γ)√i-W
Td
XX βιλtptv^
kg1:T ,i k2 .
(17)
Mathematical Induction to prove that vt, i ≤ G∞ for any 0 ≤ t ≤ T, 1 ≤ i ≤ d. ∀i, we have
. Suppose mt-1,i ≤ G
2
mt,i =
∞ , We have
(β1,tmt-1,i + (1 - β1,t)gt,i)2
(i)	2	2
≤ β1,tmt-1,i + (1 - β1,t)gt,i
≤ β1,tG2∞ + (1 - β1,t)G2∞ = G2∞ ,
Where (i) comes from the convexity of function f = x2 . Hence by induction, We have mt2,i ≤ G2∞ for all
0 ≤ t ≤ T . Furthermore, ∀i, We have v0,i
vt,i =
≤
= 0 ≤ G2∞. Suppose vt-1,i ≤ G2∞, We have
β2vt-1,i + (1 - β2)mt2,i
β2G2∞ + (1 - β2)G2∞ = G2∞ .
Therefore, by induction, we have vt,i ≤
dG∞ TT and (17), we obtain
G2∞, ∀i, t. Combining this with the fact that Pid=1 kg1:T ,ik2 ≤
R(T) ≤
dG∞D∞ √∕T
2α(1 - β1)
+ dG∞D∞ β1	^t√t +	dG∞α√1 + log T
2α(1 - β1) t=1	(1 - βι)3(1 - γ)P(1 - β2)T.
(18)
For PT=I λt√t, We apply arithmetic geometric series upper bound:
T
T
∑λt√t ≤ ftλ ≤
1
(1-λ2.
(19)
t=1
t=1
Plugging (19) into (18) and dividing both sides by T, We obtain
dD∞2 G∞β1
dD∞2 G∞
R(T) /	~j∞~v- I -O-	~^∞j∞
-----≤ -------------------+ -----------------------+ ------：-----：~~：---- .
T — (1 - βι)3(1 - γ)P(1 - β2)T	2α(1 - βι)√T	2α(1 - βι)(1 - λ)2T
dG∞α "+log T
Which concludes the proof.
□
C.2 Proof of the convergence results for the non-convex case
20
Under review as a conference paper at ICLR 2022
C.2.1 Useful Theorem
Theorem 5. (Chen et al. (2019)) Suppose Assumptions 2 and 4 are satisfied, β1,t is chosen such that 0 ≤
β1,t+1 ≤ βι,t < 1, 0 < β2 < 1,∀t > 0. There exists some constant G SuCh that ∣∣αt ∙ -√∣t-∣∣ ≤ G,∀t. Then
Adam-type algorithms yield
T
E X at hVf (θt), Vf (θt)∕√Vti
t=1
TT
≤ E Ci X katgt∕√Vtk2 +C2 X
t=1	t=1
αt
αt-1 ∣
√vt-i∣∣ι
T
+ C3 X
t=1
(20)
where C1 , C2 and C3 are constants independent of d and T, C4 is a constant independent of T , the expectation
is taken w.r.t all randomness corresponding to {gt }.
Furthermore, let Yt := min7∙∈[d] min^}t ɪ √== denotes the minimum possible value of effective stepsize at
time t over all possible coordinate and past gradients {gi}it=1. The convergence rate of Adam-type algorithm
is given by
min E[kVfe)k2] = O(W),
where si(T) is defined through the upper bound of RHS of (20), and PT=I Yt = Ω(s2(T)).
We present the proof of this Theorem in subsection C.2.4 and C.2.5 for completeness and reader’s convenience.
C.2.2 Proof of Theorem 2
Proof. We will first bound each term on RHS of Equation (20). Given all conditions in Theorem 2 hold, we
have that
T
Xkαtgtk22
t=1
1T
≤ 1E Xα2kgtk2
c t=1
(ii)
≤
(21)
where (i) arises from the fact that 0 < c ≤ vt , ∀t ∈ [T] and inequality (ii) is based on kgt k2 ≤ H, ∀t ∈ [T].
Then,
T
EX
t=1
αt
αt1
vt1 ∣1
dT
Xi=1Xt=1
=(i) E
≤√c,
(22)
and here (i)	holds since We assume that ∙O=	≥	αt+1	,∀t	∈ [T],	and (ii)	comes from the fact that 0 < c ≤
v v	√v⅛	—	√vt+1	,	l j, v 7	一
vt , 0 < αt ≤ α, ∀t ∈ [T]. Next,
E
≤ 丁
(23)
21
Under review as a conference paper at ICLR 2022
where we have (i) because Il 意 - √⅛∣∣2 = √⅛ - √V⅛ ≤ √c
a lower bound of the LHS of Equation (20):
On the other hand, we can obtain
X α S，f〉# ≥ H E
T
Xa kvf(θt)k2
t=1
≥ ToTminE kvf(仇)k2，
(24)
E
where the last equality comes from the fact that vt is weighted average of mt2 and kmt k2 ≤ H since mt is also
an exponential moving average of gt .
By combining the results in (21), (22), (23) and (24) to (20), we obatain
TaT
H
min E kvf(θt )k22
t∈[T]
≤
E Xat (vf (θt), v√vt))#
T
≤ E C1X
t=1
2T
+ C2 X
2	t=1
αt	αt-1
------------
√vt-√vt-1
T
+ C3 X
1	t=1
αt	αt-1 II2
√t -√V=1 ll2 +C4
≤ CH X α2 + C√α + q + C4 = CHnT) + C2√ + C3 吐 + C4.
c t=1	c c	c	c c
After rearrangement, we can easily deduce that
min EkVfe)k2 ≤ THT
C1H2η(T)
+ C2 √O + C3 dα2 + C4
cc
c
TOT(QI + Q2 n(T)),
(25)
where
Q1 = H
+ C3
Q2 = C1H3
c
□
C.2.3 Proof of Corollary 2
By choosing at = ɑ/ʌ/t ≤ a, ∀t ∈ [T], we have
T	T 1
TaT = a√T,	η(T) =X a2 = a2 X t ≤ a2(l + log(T)).
t=1	t=1 t
Combining this with (25) and making some rearrangement, we have:
min E kVf (θt)k2 ≤ -H= [Ci CaH2(1+log(T)) + C2da + C3da2 + C4
t∈[τ]	11 j ' "2 - a√T [	C	≠	C
=√T(Q" Qa log(T)).
where
+ C3
a	Ci H3a
Q2 =------
C
□
C.2.4 Technical lemmas for proof of Theorem 5
In this section, we introduce seven useful lemmas.
Lemma 5 (Chen et al. (2019)). Let θ0 , θi in the Algorithm, consider the sequence
Zt = θt +	,1,jl- (θt - θt-1), ∀t ≥ 2.
1 - βi,t
22
Under review as a conference paper at ICLR 2022
The following holds true:
zt+1 - zt
β1,t+1
βι,t
1 - β1,t+1	1 - β1,t
αtmt
√vt
and
z2 - z1
βι,t
1 — βι,
βl,2
言)mt-1- √gt, ∀t>1,
β1,1
1 - β1,2	1 - β1,1
α1m1
√vι
αιgι
√Vι
Lemma 6 (Chen et al. (2019)). Suppose that the conditions in Theorem 5 hold, then we have
6
E [f(zt+1 - f(zt))] ≤ XTi,
where
T1
T2
mi-1
—
β1,i+1
1 - β1,i+1	1 - βi
XX (vf (Zi),
i=1
(26)
(27)
T3 = -E
T4 = E
βi
2
12
(28)
(29)
—
β1,i
β1,i+1
1 - β1,i+1	1 - β1,i
T6 = E
T5 = E
(30)
(31)
—
—
—
—
—
—
Lemma 7 (Chen et al. (2019)). Suppose that the condition in Theorem 5 hold, then for T1 in (26) it holds that
T1 = -E
X 0R ⅛
i-1- ) mi-1
≤ H2占
td
EXX
Lemma 8 (Chen et al. (2019)). Suppose the conditions in Theorem 5 are satisfied, then T3 in (28) can be
bounded as:
T3 = -E
XX (vf (Zi),
i=1
β1,i+1
βi
1 - β1,i+1	1 - βi
—
β1
≤
—
β1,t+1
1 - β1	1 - β1,t+1
Lemma 9 (Chen et al. (2019)). Suppose assumptions in Theorem 5 are satisfied, then for T4 in (29), it holds
that
T4 = E
≤ 3L

Lemma 10 (Chen et al. (2019)).
have
β1
β1,i+1
1 - β1,i+1
—
β1,t+1
—
β1,i
I2
αimi I
1 - βι,i)	√vi Il2
2	1 - β1	1 - β1,t+1	.
Suppose the assumptions in Theorem 5 are satisfied, then for T5 in (30), we
T5 = E
t 3L
工^2^
i=1
%
1 - β1,i
3L
≤——

2
2
言)mTL
(⅛ 了 H 2E "XX XX
23
Under review as a conference paper at ICLR 2022
Lemma 11 (Chen et al. (2019)). Suppose the assumptions in Theorem 5 are satisfied, then T2 in (27) can be
bounded as:
T2 = - E
XX ai ∕vf (zi),
i=1
+ L2
+ L2H2
+ 2H2E
+ 2H2E
C.2.5 Proof of Theorem 5
Proof. We can prove Theorem 5 after combining Lemma 5, 6, 7, 8, 9, 10 and 11. Specifically, firstly based on
Lemma 5 it holds that
E [f(zt+1) - f(z1)]
6
≤ X	Ti
i=1
=E X 2LKΓ⅛1 - 1⅛)…≠fl + E IX 3L k"H2
+E
-E
-E
XX 2 L
i=1
X (Vf(Zi), 1⅛
t
Eai hVf(z∕gi∕√7i
i=1
Xt	vf(zi),
i=1
β1,i + 1
1 一 β1,i+1
Then we can combine Lemma 6, 7, 8, 9, 10 and 11 and further merge similar terms. We have that
24
Under review as a conference paper at ICLR 2022
6
E [f(zt+1) - f(zt)] ≤XTi
i=1
≤H2 -β1- E
一 1 - βι
td
Xi=2Xj=1
(β1	_	βl,t+1	ʌ (H2 + g2)+3L (	β1	_ βl,t Y G2
1— — βι	1 - βι,t+J (H +	)+ 2(1-仇 1-βι,J
+3L (ι⅛ )2 H 2E "X X (√⅛-√αv⅛ )2#
d t1
+ L2H2
(1⅛)4 (1⅛)2E XX
j1	2
(2-M-1 )21
v√vi	√vi-1√ j∖
4
2
+2H 2E " XK 除
+ 2H2E X
j=1
1
j
t
— E X ai Ef(X以 Vf (xi)∕√Vii
i=1
Finally, after rearrangement and some calculation, it can be verified that
t
E X a〈Vf(xi), Vf(Xi)/≠7>
i=1
T
T
≤ E	CIX Ilatgt/√vtk2 + C2X	√α= -
I I )4-
αt-1
t1
T
+ C3 X
t=1
αt αt-1
------------
√vt	√vt-1
t1
2# +C4,
√vt	√vt-ι
1
where we have
CI, 2 L+1+L2
β1
12
1 - β1	1 - β1
C2，H2 rβ⅛r +2H2,
1 - β1
C3, h1 + L2 (1⅛ )2( τ⅛
C4 , (1^-1β∙) (H2 + G2) +
!(1⅛ )2,
2
G2 + 2H2E[ ∣αι∕√V1 k1 ] + E[f(zι) - f(z*)],
1 - β1
and z* is an optimal where f (∙) takes its minimum. This result directly implies Theorem 5.
□
D	Additional Experimental Details
Hyperparameter tuning rule For hyperparameter tuning, we perform extensive and careful grid search
to choose the best hyperparameters for all the baseline algorithms.
For SGDM, we set the momentum parameter as 0.9 and search the optimal learning rate α among set
{30.0, 1.0, 0.1, 0.01, 0.0015, 0.001, 0.0005, 0.0002} for all the experiments.
For adaptive gradient method baselines (Adam, AdamW, Yogi, Adabound, RAdam and AdaBelief),
we search for β1 among {0.5, 0.6, 0.7, 0.8, 0.9}, β2 among {0.9, 0.98, 0.99, 0.999, 0.9999}, α among
{0.1, 0.01, 0.0015, 0.001, 0.0005}, weight decay parameter among {1.2×10-6, 10-4, 5× 10-4} and among
{10-8, 10-12, 10-16} for all the experiments except ImageNet. All the additional method-specific parameters
25
Under review as a conference paper at ICLR 2022
are carefully chosen according to the original paper setting. Due to extremely large computing workload on
ImageNet dataset, we set β1 = 0.9, β2 = 0.999, α = 0.001 and = 1 × 10-8 for all the adaptive gradient
methods, and finetune weight decay parameter from set {0, 1 × 10-4, 5 × 10-4, 10-2, 5 × 10-2} for the values
we reported as we run.
For our AdaMomentum, we employ the default parameters as β1 = 0.9 and choose learning rate and weight
decay parameter using same parameter searching scheme as the adaptive gradient baseline methods. β2 is set
as 0.999 for all the tasks. We Choose = 10-8 for image classification tasks and = 1 × 10-16 for other
tasks. We find this setting is universally ample for satisfactory performance, which further demonstrates the
superiority of AdaMomentum that little tuning effort is needed.
All the experiments reported are performed on NVIDIA GeForce RTX 2080Ti GPUs with Intel Core i7-8700K
3.70GHz CPUs. We provide some additional information concerning the empirical experiments for complete-
ness.
D. 1 Image classification
Table 7: Well tuned hyperparameter configuration of the adaptive gradient methods for CNNs on
CIFAR-10._______________________________________________________________________
Algorithm ∣ Adam AdamW Yogi AdaBound RAdam AdaBelief AdaMomentum
Stepsize α	0.001	0.001	0.001	0.001	0.001	0.001	0.001
β1	~09	0.9	0.9	09	0.9	0.9	0.9
β2	0.999-	0.999	0.999	-0999-	0.999	0.999	0.999
Weight decay	5 × 10-4	5 × 10-4	5 × 10-4	5 × 10-4	5 × 10-4	5 × 10-4	5 × 10-4
	-10-8-	10-8	10-8	-10-8-	10-8	10-8	10-8
CIFAR datasets The values of the hyperparameters after careful tuning of the reported results of the adap-
tive gradient methods on CIFAR-10 in the main paper is summarized in Table 7. For SGDM, the optimal
hyperparameter setting is: the learning rate is 0.1, the momentum parameter is 0.9, the weight decay param-
eter is 5 × 10-4. For Adabound, the final learning rate is set as 0.1 (matching SGDM) and the value of the
hyperparameter gamma is 10-3.
Table 8: Well tuned hyperparameter configuration of the adaptive gradient methods for CNNs on
ImageNet.
Algorithm ∣ Adam AdamW Yogi AdaBound RAdam AdaBelief AdaMomentum
Stepsize α	0.001	0.001	0.001	0.001	0.001	0.001	0.001
β1	~09	0.9	0.9	09	0.9	0.9	0.9
β2	0.999-	0.999	0.999	-0999-	0.999	0.999	0.999
Weight decay	1 × 10-4	1 × 10-4	1 × 10-4	1 × 10-4	1 × 10-4	1 × 10-2	5 × 10-2
	-10-8-	10-8	10-8	-10-8-	10-8	10-8	10-8
ImageNet The values of the hyperparameters after careful tuning of the reported results of the adaptive
gradient methods on CIFAR-10 in the main paper is summarized in Table 8. For SGDM, the stepsize is 0.1, the
momentum parameter is 0.9 and the weight decay is 5 × 10-4.
D.2 LSTM on language modeling
Table 9: Well tuned hyperparameter configuration of adaptive gradient methods for 1-layer-LSTM
on Penn Treebank dataset.____________________________________________________________
Algorithm ∣ Adam AdamW Yogi AdaBound RAdam AdaBelief AdaMomentum
Stepsize α	0.001	0.001	0.01	0.01	0.001	0.001	0.001
β1	09	0.9	0.9	09	0.9	0.9	0.9
β2	-0999-	0.999	0.999	-0999-	0.999	0.999	0.999
Weight decay	1.2 × 10-4	1.2 × 10-4	1.2 × 10-4	1.2 × 10-4	1.2 × 10-4	1.2× 10-4	1.2 × 10-4
	-10-12-	10-12	10-8	10-8-	10-12	10-16	10-16
26
Under review as a conference paper at ICLR 2022
120
no
too
90
80
70
60
口一 XWdsd u≡∙ll
O 25	50	75	1∞ U5 150	175	200
Epoch
(a)	I-Layer LSTM.
O 25	50	75	1∞ U5 150	175	200
Epoch
(b)	2-Layer LSTM.
口一 XWdsd u≡∙ll
IlO
100
90
80
O
)	75	1∞	125	150	175	200
Epoch
(C) 3-Layer LSTM.
Figure 5: Train perplexity Curve on Penn Treebank dataset.
Table 10: Well tuned hyperparameter Configuration of adaptive gradient methods for 2-layer-LSTM
on Penn Treebank dataset.____________________________________________________________
Algorithm ∣ Adam AdamW Yogi AdaBOund RAdam AdaBelief AdaMomentum
Stepsize α	0.01	0.001	0.01	0.01	0.001	0.01	0.001
β1	O	0.9	0.9	0.9	0.9	0.9	0.9
β2	-0.999-	0.999	0.999	-0.999-	0.999	0.999	0.999
Weight deCay	1.2 × 10-4	1.2 × 10-4	1.2× 10-4	1.2 × 10-4	1.2 × 10-4	1.2× 10-4	1.2 × 10-4
	-10-12	10-12	10-8	-10-8-	10-12	10-12	10-16
Table 11: Well tuned hyperparameter Configuration of adaptive gradient methods for 3-layer-LSTM
on Penn Treebank dataset.____________________________________________________________
Algorithm ∣ Adam AdamW Yogi AdaBound RAdam AdaBelief AdaMomentum
Stepsize α	0.01	0.001	0.01	0.01	0.001	0.01	0.001
β1	0.9	0.9	0.9	0.9	0.9	0.9	0.9
β2	-0.999-	0.999	0.999	-0.999-	0.999	0.999	0.999
Weight deCay	1.2 × 10-4	1.2 × 10-4	1.2× 10-4	1.2 × 10-4	1.2 × 10-4	1.2× 10-4	1.2 × 10-4
	-10-12-	10-12	10-8	-10-8-	10-12	10-12	10-16
27
Under review as a conference paper at ICLR 2022
The training perplexity curve is illustrated in Figure 5. We can clearly see that AdaMomentum is able to make
the perplexity descent faster than SGDM and most other adaptive gradient methods. In experimental settings,
the size of the word embeddings is 400 and the number of hidden units per layer is 1150. We employ dropout
in training and the dropout rate for RNN layers is 0.25 and the dropout rate for input embedding layers is 0.4.
The optimal hyperparameters of adaptive gradient methods for 1-layer, 2-layer and 3-layer LSTM are listed in
Tables 9, 10 and 11 respectively. For SGDM, the Well tuned stepsize is 30.0 and the momentum parameter is
0.9. For Adabound, the final learning rate is set as 30.0 (matching SGDM) and the value of the hyperparameter
gamma is 10-3.
D.3 Transformer on neural machine translation
Table 12: Well tuned hyperparameter configuration of adaptive gradient methods for transformer on
IWSTL’14 DE-EN dataset.
Algorithm	Adam	AdamW	AdaBelief	AdaMomentum
Stepsize ɑ	0.0015	0.0015	0.0015	0.0005
βι	0.9	^^09^^	0.9	0.9
β2	0.98	^^098^^	0.999	0.999
Weight decay	10-4	10-4	10-4	10-4
€	10-8	10-8	10-16	10-16
For transformer on NMT task, the well tuned hyperparameter values are summarized in Table 12. The stepsize
of SGDM is 0.1 and the momentum parameter of SGDM is 0.9. Initial learning rate is 10-7 and the minimum
learning rate threshold is set as 10-9 in the warm-up process for all the optimizers.
28