Under review as a conference paper at ICLR 2022
Tight lower bounds for Differentially Pri-
vate ERM
Anonymous authors
Paper under double-blind review
Ab stract
We consider the lower bounds of differentially private ERM for general con-
vex functions. For approximate-DP, the well-known upper bound of DP-ERM
is O( Vzp lθn(1∕δ)), which is believed to be tight. However, current lower bounds
are off by some logarithmic terms, in particular Ω(兴)for constrained case and
Ω( √ ) for unconstrained case. We achieve tight Ω( Vzplog(1∕δ)) lower bounds
n log p	n
for both cases by introducing a novel biased mean property for fingerprinting
codes.
As for pure-DP, we utilize a novel `2 loss function instead of linear functions
considered by previous papers, and achieve the first (tight) Ω(系)lower bound.
We also introduce an auxiliary dimension to simplify the computation brought by
`2 loss.
Our results close a gap in our understanding of DP-ERM by presenting the funda-
mental limits. Our techniques may be of independent interest, which help enrich
the tools so that it readily applies to problems that are not (easily) reducible from
one-way marginals.
1 Introduction
Since the seminal work of Dwork et al. (2006), differential privacy (DP) has become the standard
and rigorous notion of privacy guarantee for machine learning algorithms, among which many fun-
damental ones are based on empirical risk minimization (ERM). Motivated by this, private ERM
becomes one of the most well-studied problem in the DP literature, e.g. Chaudhuri and Monteleoni
(2008); Rubinstein et al. (2009); Chaudhuri et al. (2011); Kifer et al. (2012); Song et al. (2013); Jain
and Thakurta (2014); Bassily et al. (2014); Talwar et al. (2015); Kasiviswanathan and Jin (2016);
Fukuchi et al. (2017); Wu et al. (2017); Zhang et al. (2017); Wang et al. (2017); Iyengar et al.
(2019); Bassily et al. (2020); Kulkarni et al. (2021); Asi et al. (2021); Bassily et al. (2021); Wang
et al. (2021).
Roughly speaking, in the ERM setting, we are given a convex function family defined on a convex
set C ⊆ Rp and a sample set D = {dι, ∙∙∙ , dn} drawn i.i.d from some unknown distribution P with
the objective to minimize the loss function
1n
L(0； D) = - ∑'(θ; di),
n i=1
and the value L(θ; D) - minθo∈c L(θ0; D) is called the excess empirical loss with respect to solution
θ, measuring how it compares with the best solution in C .
Private ERM in the constrained case was studied first and most of the previous literature belongs
to this case. More specifically, the constrained case considers convex loss functions defined on a
bounded convex set C ( Rp. Assuming the functions are 1-Lipschitz over the convex set of diameter
1, the Ω(誓)lower bound of private ERM is given by Bassily et al. (2014), even for (special and
simpler) generalized linear model (GLM).
However, there are still several aspects that existing works don,t cover. First, existing upper bounds
are off by at least a logarithmic term ,log(l∕δ). For example in Wang et al. (2017); Bassily et al.
1
Under review as a conference paper at ICLR 2022
(2019) they give upper bounds like O(Lllθ0-θ 口；'，log(1/6), which are believed to be tight. In
Bassily et al. (2014), they present a lower bound Ω(筌)by reducing linear loss to one-way marginal
results in Hardt and Talwar (2010); Bun et al. (2018). In Steinke and Ullman (2015) they achieve the
tight lower bound for answering one-way marginals with respect to `1 norm, but it does not imply
tight lower bounds for general loss functions by the methods in Bassily et al. (2014) directly.
Another aspect is DP-ERM in the unconstrained case which was neglected before and gathered
people’s attention recently, see Jain and Thakurta (2014); Song et al. (2021). The unconstrained
case is interesting in that we can’t use linear loss any more which lies at the heart of the construction
in the constrained case. Moreover, previous algorithms in the constrained case suffer from the curse
of dimensionality when P is large. For example, when P = Ω(n2),the lower bound is Ω⑴ and any
private algorithm can not get meaningful bounds on the excess empirical loss. Song et al. (2021)
proves an dimension-independent O(，；y)upper bound in the unconstrained setting for the special
case of GLMs, where rank denotes the rank of the feature matrix of GLM. However, it’s unknown
if similar results can be achieved for general loss functions. As for the lower bound, Asi et al.
(2021) give Ω( n^P) lower bound by considering 'ι loss as the objective functions and reducing
the results also from one-way marginals.
1.1	Our contributions
In this paper, we fill up the two gaps together by proving an Ω(min(1, Vzpln("δ))) tight lower
bound for the excess risk of unconstrained 1-Lipschitz convex loss functions for approximate dif-
ferentially private algorithms. This bound is automatically applicable in the constrained case, which
improves previous results and achieve a tight lower bound for both constrained and unconstrained
case. We summarize our main results as follows:
•	We prove an Ω(min(1, Vzp Iog(I/δ))) tight lower bound for the excess risk of unconstrained
1-Lipschitz convex loss functions for approximate differentially private algorithm. This
bound improves Asi et al. (2021) by a log(p),log(1∕δ) factor in the unconstrained case
and matches the upper bound in Kairouz et al. (2020).
•	We also prove an Ω(min(1,盍))lower bound for the excess risk of unconstrained 1-
Lipschitz convex loss functions for any pure differential privacy algorithm.
Note that our main results for unconstrained case can be extended to constrained case directly, thus
our lower bound for approximate private algorithm is ,log(1∕δ) multiplicative better than the well-
known bound in Bassily et al. (2014) with the help of group privacy technique in Steinke and Ullman
(2015).
A key contribution of this paper is novel tools for private lower bound techniques. For most
problems, accuracy lower bound in the private setting is established via reduction from one-way
marginals. Hence the tools for lower bounds is quite limited. We contribute to refinement of such
tools - in particular, we propose modifications such as the additional ”biased means” property to
fingerprinting codes, which is the key lower bound technique. Such modifications help enrich the
tools so that it readily applies to problems which are not (easily) reducible from one-way marginals.
1.2	Our techniques
In general, the direct technical challenge of the unconstrained case lies in the choice of loss function
and the difficulties caused by the new loss function. The loss function is required to be convex and
Lipschitz-continuous at the same time. In the constrained case, the linear loss function is obviously
a good choice for constructing lower bounds, because it is easy to analyze and easily reducible from
one-way marginals. However, in the unconstrained case any non-trivial linear loss function can
take '-∞' value thus not applicable anymore. We further observe that convexity plus Lipschitz-
continuity means ‘asymptotically linear’: the sub-gradient along any direction must converge. This
observation guides us in choosing new loss functions for unconstrained DP-ERM (which can be
extended to constrained case directly). We briefly introduce the new problems caused by the new
loss functions and our method to overcome them for approximate-DP and pure-DP separately.
2
Under review as a conference paper at ICLR 2022
1.2.1	Approximate-DP
The construction of our lower bound for approximate-DP is based on the Fingerprinting Codes,
which was first studied by Wagner (1983) and developed by Boneh and Shaw (1998); Tardos (2008).
From a technical perspective, we change the previously used linear loss and use an `1 norm function
instead where '(θ; d) = ∣∣θ - d∣∣ι. '1 loss has been used in a concurrent work Asi et al. (2021)
which proves an Ω(陋^og P) lower bound of approximate DP in the constrained case by reducing the
results from one-way marginals, and can be extended to unconstrained case directly. We improve
this bound by logarithmic terms and achieve optimality by utilizing the group privacy technique
from Steinke and Ullman (2015). We observe a novel biased mean property in the fingerprinting
code to successfully combine and adjust these techniques to fit the `1 loss.
We briefly describe the proof based on group privacy technique first. In the group privacy we need
to copy some hard instances of data-set Dk of size nk := bn/kc according to the construction of
fingerprinting codes by k times, and append n - knk data points to get a final data-set D of size n.
Fix any (e, δ)-differentially private algorithm A for D, if We remove one element i* from Dk, we
can get DkLi^ and D-i* where D—i* and D can have at most k elements different. Running A on D
and D—i* respectively, we get an (ke, δ0)-differential privacy algorithm for Dk and D—i*. Setting k
appropriately, if A can lead to small error on the DP-ERM, it can be an adversary which contradicts
the properties of the fingerprinting codes. Intuitively, the differential privacy means it is hard to find
the removed element i*, but fingerprinting codes suggest the removed element is traceable as long
as Dk satisfies the required properties and A leads to small excess empirical loss with respect to
L(θ; Dk).
The direct use of the biased mean property is in appending the n - knk points. As nearly all of
previous lower bounds in DP convex optimization are based on the results from one-way marginals,
we try to demonstrate the proof in the language of one-way marginals. Because for linear functions,
large one-way marginal errors lead to large excess empirical loss directly, which means the lower
bound of private one-way marginals can apply to DP-ERM. But it is obvious that without additional
assumption, large one-way marginal errors can not mean large excess empirical loss of the `1 loss
functions anymore. Consider the toy case when p = 1 and Dk = {di}ib=n/1kc where di ∈ {0, 1}.
Denote the mean of these [n/k[ by Dk. Similarly let D be the mean values of D constructed from
Dk by method above. For example, if Dk = 1/2 and we only append points 1/2, then D = 1/2 and
whatever the one-way marginals are, the excess empirical loss (of the n 'ι loss functions) can be 0, as
L(θ; D) = 1 Pn=IIlθ-di∣∣ι is a constant function over [0,1]. So we need the mean Dk to be biased.
More specifically, we need |Dk - 1/2| should be larger than some value depending on k, then we
can append n - knk dummy points safely. For general p, we need the unbiased mean property holds
for a large fraction of coordinates. For any single dimension, the biased mean property serves to
ensure the prediction of the column (dimension) is unchanged during the group privacy mechanism,
in which some number of dummy points are appended that may potentially change the prediction of
unbiased column. This novel property sets more stringent conditions. Fortunately, we observe that
the previous construction of fingerprinting codes in Bun et al. (2018) satisfies it.
1.2.2	Pure-DP
Although the square loss seems tempting in the constrained case, which intuitively reduces the
unconstrained case to constrained because the loss value grows fast outside a bounded region and
also makes computation simple. It’s unfortunately non-Lipschitz in the unconstrained case thus not
applicable directly.
We use the novel '2-norm loss as a natural substitute, which is both convex and Lipschitz-
continuous. Unlike the constrained case, the '2-norm loss brings the drawback that the minimizer
of the ERM problem no longer has a closed form solution or any nice property for computation.
Roughly speaking, in the analysis of Bassily et al. (2014) they have an ’adding dummy points’ pro-
cedure which will perturb the minimizer. Linear loss has this nice property that after adding these
dummy points the minimizer can only move along its direction, while for '2 loss the minimizer
might be intractable. To overcome this problem, we define the Fermat point and introduce an aux-
iliary dimension to simplify the messy calculation brought by '2-norm loss in our analysis. The
3
Under review as a conference paper at ICLR 2022
dummy points have support only in this auxiliary dimension and guarantee that the perturbation
of the minimizer is still along its own direction, reducing computation in any high dimension to a
two-dimension subspace spanned by the minimizer and the auxiliary dimension.
1.3	Constrained and Unconstrained
In this subsection, we briefly discuss the relationships and differences between constrained case and
unconstrained case, and compare our bounds with previous bounds.
Previous studies on DP-ERM mostly focus on the constrained setting, and the unconstrained case
recently attract people's interest because Jain and Thakurta (2014); Song et al. (2021) found that an
O( '；产)upper bound Can be achieved for minimizing the excess risk of GLMs, which evades the
curse of dimensionality.
It has been known that the unconstrained condition is necessary for dimension independence, as
pointed out by Bassily et al. (2014) in which they prove an Ω( Jnp) lower bound even for minimizing
constrained GLMs for the case when “rank ≤ n p”.
We are interested in the necessity of the unconstrained condition to get rank-dependent bound. The
unconstrained GLM can be viewed as a rank-dimensional problem, as the noise added in the null
space of the feature matrix will not affect the excess empirical loss. However, this does not hold
in the constrained case. Take the dimension-independent algorithm in Song et al. (2021) which is
based on SGD as an example. The pitfall for the dimension-independent algorithm lies in projection
if SGD is modified to projected-SGD for constrained case, that running SGD in the constrained
setting requires projection which might “increase rank”. We can see there is some fundamental
difference between constrained and unconstrained case, and analyzing unconstrained case is also an
interesting and important direction.
Classic methods, like Bassily et al. (2014) usually connect linear loss to one-way marginals Bun
et al. (2018), and then use lower bounds for one-way marginals to imply lower bounds for linear
loss. As Bassily et al. (2014) are using results from Hardt and Talwar (2010); Bun et al. (2018) in
one-way marginals and Steinke and Ullman (2015) achieves tight Ω( Vzplα("δ)) bound by using
the novel group privacy technique, one may ask whether combining Steinke and Ullman (2015) and
Bassily et al. (2014) can achieve the tight lower bound in the constrained case trivially. Because
Steinke and Ullman (2015) considers `1 distance but Bassily et al. (2014) considers `2 distance,
there is a √p gap between them and one can,t directly combine them. Though with some effort,
one may get tighter bounds in the constrained case by modifying the results in Steinke and Ullman
(2015) from `1 norm to `2 norm, then applying the analysis in Bassily et al. (2014).
As shown in Asi et al. (2021), proving a nearly tight lower bound in the unconstrained case is direct
by utilizing one-way marginals and choosing the right objective functions, but getting rid off those
extra logarithmic terms in the unconstrained case is nontrivial as the one-way marginals can not
work directly in group privacy. To the best of our knowledge, our result is the first time that achieves
this improved tight lower bound for general loss function class in both cases. See Table 1.4 for
detailed comparisons between previous bounds and ours.
1.4	Related work
The existing lower bounds of excess empirical loss, i.e. the constrained case in Bassily et al. (2014)
and the unconstrained case in Song et al. (2021), are all using GLM functions. The objective func-
tion used in Bassily et al. (2014) is `(θ; d) = hθ, di which can’t be applied in the unconstrained
case, otherwise the loss value would be infinite. Considering this limitation, Song et al. (2021)
adopts '(θ; d) = ∣hθ,xi - y|. They transfer the problem of minimizing GLM to estimating one-way
marginals, and then get the lower bound by properties in the definition of the Fingerprinting Codes.
As mentioned before, our lower bound are based on `1 norms, thus we can not transfer to one-
way marginals directly. Merely using the properties in the definition of Fingerprinting Codes is not
enough for a good lower bound. Instead, we need to make full use of the concrete structure of the
codes.
4
Under review as a conference paper at ICLR 2022
Article	Constrained?	Loss Function	PUre DP	Approximate DP
BaSSily et al.(2014)	constrained	GLM	ω( n)	Ω( √) 	∖ns)	
Song etal.(2021)	unconstrained	GLM	N/A	ω( ^nank)
Asi etal. (2021)	both	general	N/A	Ω( -√~)
Ours	both	general	ω( n)	n log p Ω( √p Iog(I/6)
Table 1: Comparison on lower bounds for private convex ERM. Our lower bounds can be extended
to constrained case easily. The lower bound of Song et al. (2021) is weaker than ours in the important
p n setting.
As for the upper bounds, the private ERM Wang et al. (2017) and private Stochastic Convex Op-
timization (SCO) Feldman et al. (2020) for convex and smooth functions are extensively studied,
where the objective is to minimize the function Ed〜P ['(θ; d)] in the SCO and people only need
(nearly) linear gradient queries to get optimal excess loss. But for convex functions without any
smoothness assumption, the current best algorithms Kulkarni et al. (2021); Asi et al. (2021) will
need more queries (n1.375 in the worst case). Besides, most of the previous works are consider-
ing problems in `2 norm, and there are some recent results Bassily et al. (2021); Asi et al. (2021)
studying the general `p norm.
1.5	Roadmap
In section 2 We introduce background knowledge needed in the rest of the paper. In section 3
We prove the main result of this paper, an Ω(min(1, Vzp lθg("δ))) lower bound for approximate DP-
ERM in the unconstrained case. In section 4 we discuss an Ω(min(1, %)) lower bound for the excess
risk of pure DP algorithms for minimizing any unconstrained 1-Lipschitz convex loss function.
Section 5 concludes this paper. All missing (technical) proofs can be found in the appendix.
2	Preliminary
We consider minimizing the excess risk of unconstrained Lipschiz convex function with DP algo-
rithms in this paper, where we let n denote the sample size and p be the dimension of a sample.
In this section, we will introduce main background knowledge required in the rest of the paper.
Additional background knowledge such as the definition of GLM can be found in appendix.
Definition 2.1 (Differential privacy). A randomized mechanism M is (, δ)-differentially private if
for any event O ∈ Range(M) and for any neighboring databases D and D0 that differ in a single
data element, one has
Pr[M(D) ∈ O] ≤ exp() Pr[M(D0) ∈ O] +δ.
When δ > 0, we refer to the above condition as approximate differential privacy. The special case
when δ = 0 is called pure differential privacy.
Definition 2.2 (Empirical Risk Minimization). Given a family of convex loss functions
{'(θ, d)}d∈D of θ over K ⊆ Rp and a set of samples D = {dι,…，dn} over the universe D,
the objective of Empirical Risk Minimization (ERM) is to minimize
1n
LW D) = - E'(θ; di).
n i=1
The excess empirical loss with respect to a solution θ is defined by
L(θ; D) - L(θ*; D)
where θ* ∈ arg minθ∈κ L(θ; D), measuring the performance of the solution θ compared with the
best solution in K.
Definition 2.3 (G-Lipschitz Continuity). A function f : Rp → R is G-Lipschitz continuous with
respect to `2 norm if the following holds for all θ, θ0 ∈ Rp :
If(θ) - f(θ0)l ≤ Gkθ - θ0k2	(1)
5
Under review as a conference paper at ICLR 2022
The Chernoff Bound will serve to prove the fingerprinting code constructed in Bun et al. (2018)
satisfies our modified definition of fingerprinting code as well.
Proposition 2.4 (The Chernoff Bound). Let X = Pin=1 Xi where Xi = 1 with probability pi and
Xi = 0 with probability 1 -pi. Assume all Xi are independent random variables. Letu = in=1 pi.
Then
P(|X - u| ≥ δu) ≤ 2exp(-uδ2∕2).	(2)
3	Approximate DP
In this section, we consider the lower bound for approximate differential privacy where 2-O(n) <
δ < o(1/n). Such assumption on δ is common in literature, for example in Steinke and Ullman
(2015). We briefly introduce the (classic) fingerprinting codes first:
3.1	Fingerprinting codes
Definition 3.1 (Fingerprinting codes). We are given n, p ∈ N, ξ ∈ (0, 1]. A pair of (random)
algorithms (Gen, Trace) is called an (n, p)-fingerprinting code with security ξ ∈ (0, 1] if Gen
outputs a code-book C ∈ {0, 1}n×p and for any (possibly randomized) adversary AFP and any
subset S ⊆ [n], if We set C ~r AFP(CS), then
•	Pr[c ∈ F (CS) V Trace(C, c) =⊥] ≤ξ
•	Pr [Trace (C, c) ∈ [n]\S] ≤ ξ
Where F (CS)	= c ∈	{0, 1}d	|	∀j	∈	[d], ∃i ∈ S, cj	=	cij	, and the probability is taken over the
coins of Gen, Trace and AFP.
There is a very good motivation behind the fingerprinting codes. For example, a softWare distributor
adds a fingerprint to each copy of her softWare to protect the IP. A coalition of malicious users
can compare their copies and find the digits that differ Which belong to the fingerprint. For other
locations they can’t decide and Won’t change them, Which is called the marking condition. This is
the reason that We requires c ∈ F(CS).
The tWo properties of fingerprinting codes demonstrate that one can identify at least one malicious
user among all With high probability. Bun et al. (2018) extends the definition that the codes can tol-
erate a small fraction of errors in the marking condition. We further modify this definition, requiring
the codes to have biased means, see beloW.
3.2	Our Result
We modify the definition of fingerprinting code instead for our analysis.
Definition 3.2 (Error Robust Biased Mean Fingerprinting Codes). Given n, p ∈
N, ξ, β, α1 , α2, α3 ∈ (0, 1]. We say a pair of (random) algorithms (Gen, Trace) is an (n, p)-
fingerprinting code With security ξ and (α1, α2, α3)-biased mean, robust to a β fraction of errors if
Gen outputs a code-book C ∈ {0, 1}n×p and for any (possibly randomized) adversary AFP and
any coalition S ⊆ [n], if we set C —r AFP(CS), then
•	Pr[c ∈ Fβ (CS) V Trace(C, c) =⊥] ≤ξ
•	Pr [Trace (C, C) ∈ [n]\S] ≤ ξ
•	Pr[Gα1 (C) ≥ (1 - α2)] ≤ α3
where Fe (CS) = {c ∈{0,1}p | Prj^R[p]∖∃i ∈ S,Cj = Cij ] ≥ 1 - β}, Ga (CS) = |{j ：
| Pi∈S cij/∖S| - 1/2| ≤ α}∣ is the number of slightly biased columns in CS and the probability is
taken over the coins of Gen, Trace and AFP.
We use the fingerprinting code in Bun et al. (2018) for the construction of our lower bound, see
Algorithm 1 in the appendix. We utilize an `1 loss and use the fingerprinting code in Bun et al.
6
Under review as a conference paper at ICLR 2022
(2018) as our ’hard case’. To proceed, we first introduce a few lemmas which would be of use later.
Similar to Bun et al. (2018), we have the following standard lemma which allows us to reduce any
< 1 to = 1 case without loss of generality, using the well-known ’secrecy of the sample’ lemma
from Kasiviswanathan et al. (2011).
Lemma 3.1. A condition Q has Sample complexity n* for algorithms with (1, o(1 /n))-dferential
privacy (n* is the smallest sample size that there exists an (1, o(1 /n))-differentially private algo-
rithm A which satisfies Q), ifand only ifit also has sample complexity Θ(n*∕e) for algorithms with
(, o(1/n))-differential privacy.
Notice that Lemma 3.1 discusses the sample complexity of the algorithm, therefore is independent of
the (α1, α2, α3)-biased mean appeared in the above definition which only concerns the construction
of the fingerprinting code. The following lemma verifies that the fingerprinting code Algorithm 1
indeed has biased mean as in definition 3.2. The proof is straightforward by using the Chernoff
bound multiple times.
Lemma 3.2. Algorithm 1 (the fingerprinting code) has (1/100,999/1000, exp(—Ω(p))-biased
mean.
Directly combining Lemma 3.2 and Theorem 3.4 from Bun et al. (2018), we have the following
lemma, which states that for the fingerprinting code Algorithm 1 which we will use in proving our
main theorem to satisfy the error robust biased mean property in definition 3.2, one needs roughly
Ω(√p) samples.
Lemma 3.3. For every p ∈ N and ξ ∈ (0, 1], there exists an (n, p)-fingerprinting code (Algorithm
1) with security ξ and (1/100,999/1000, exp(-Ω(p))-biased mean, robust to a 1/75 fraction of
error for
n = n(p, ξ) = Ω(Pp/ log(1/()).
We are ready to prove the main result of this section by using Lemma 3.3 to reach a contradiction.
Consider the following `1 norm loss function. Define
'(θ;d) = ∣∣θ — d∣∣ι,θ,d ∈ Rp	(3)
For any data-set D = {dι,…,dn}, We define L(θ; D) = ɪ PZi '(θ; di).
Theorem 3.4 (Lower bound for (, δ)-differentially private algorithms). Let n, p be large enough
and 1 ≥ > 0, 2-O(n)< δ < o(1/n). For every (, δ)-differentially private algorithm with output
θpriv ∈ Rp, there is a data-set D = {di,…，dn} ⊂ {0,1}p ∪ {2}p such that
E[L(θpriv； D) — l(θ?; d)] = Ω(min(1, ^plOg(M)GC)	(4)
where ` is G-Lipschitz, θ? is a minimizer of L(θ; D), and C is the diameter of the set
{arg minθ L(θ; D)|D ⊂ {0, 1}n×p}, which contains all possible true minimizers.
Due to the space limit, We leave the proof of the Theorem 3.4 in the appendix.
The dependence on the diameter C makes sense as one can minimize a substitute loss function
'0(x) = '(ax) where a ∈ (0,1) is a constant instead, which decreases LiPschitz constant G but
increases the diameter C. Note also that C > 0 Whenever all possible D don’t share the same
minimizer of L, which is often the case. This bound improves a log factor over Bassily et al.
(2014) by combining the the group privacy technique in Steinke and Ullman (2015) and our modified
definition of fingerprinting code.
We leave several remarks discussing slight generalizations of Theorem 3.4.
Remark 3.5. Our lower bound can be directly extended to the constrained setting, by setting
the constrained domain to be [0, 1]n×p which contains the convex hull of all possible minimizers
{arg minθ L(θ; D)|D ⊂ {0, 1}n×p}.
Remark 3.6. Similarly, we can derive an Ω(min(1, VZranknog(I/δ)) lower bound when we addition-
ally assume the rank of gradient subspace. The analysis remains the same except we first apply
orthogonal transformation then set the complement of the gradient subspace to be all 0’s in D.
7
Under review as a conference paper at ICLR 2022
Remark 3.7. The third property of definition 3.2 serves the group privacy analysis to further im-
prove a log(1∕δ) term over Bassily et al. (2014). One can simplify the proof by setting k = 1
and borrow the lower bound for 1-way marginals from Bun et al. (2018), at the cost of losing this
log(1∕δ) term. See appendix for details.
4	Pure DP
In this section, we give a lower bound for -(pure) differentially private algorithms for minimizing
unconstrained convex Lipschitz loss function L(θ; D). In the construction of lower bounds for
constrained DP-ERM (Bassily et al. (2014)), they chose linear function `(θ; d) = hθ, di as their
objective function which isn’t applicable in the unconstrained setting because it could decrease to
negative infinity. Instead, we use a novel `2 norm loss function to over come this problem:
'(θ;d) = ∣∣θ - d∣∣2,θ,d ∈ Rp	(5)
For any dataset D = {dι,…,dn}, We define L(θ; D) = 1 P2ι '(θ; di). Clearly, both ' and L are
convex and 1-Lipschitz. The structure of the proof is similar to that in Bassily et al. (2014), while
technical details are quite different as We need to handle a non-linear objective function. Different
from the simple average of points in Bassily et al. (2014), We need to consider the Fermat point
instead, Which is the minimizer of the `2 norm loss function.
4.1	Fermat point
Definition 4.1 (Fermat point). The set of Fermat points P(D) ofa dataset D = {d1, ..., dn} contains
points minimizing its `2 distance to all points in D:
n
P(D) = {arg min X||x-di||2}	(6)
x∈Rp
i=1
One obstacle of using `2 norm as our loss is that Fermat points aren’t unique in the Worst case.
Given a (finite) dataset D, We can easily see that P(D) is a compact subset of the convex hull of D,
Which encourages us to define a unique “maximum” element in P (D). To do so, We introduce the
folloWing Well-order on Rp .
Definition 4.2 (Coordinate dictionary order). A point x is said to be larger than y in coordinate
dictionary order if and only if there exists an index i ∈ [n] such that xi > yi, and for any j < i we
have that xj = yj.
It’s straightforWard to verify that CDO (coordinate dictionary order) is a Well-order. Next We use
CDO to select a unique member from the set P(D) of all Fermat points.
Definition 4.3 (Ordered Fermat point). The Ordered Fermat point q(D) of a dataset D =
{d1 , ..., dn } is defined as:
q(D) = arg max CDO(x)	(7)
x∈P (D)
Such q(D) must exist for a finite dataset as long as P(D) is compact and non-empty, because there
can’t be an ordered infinite sequence With its limit outside of P (D) Which contradicts compactness.
The technical proof of the folloWing proposition is deferred to appendix.
Proposition 4.1. q(D) always exists for a finite dataset D.
Note that q(D) is unique by definition and is alWays a minimizer of L(θ; D) over Rp. In the fol-
loWing subsection We are going to shoW that any pure DP algorithm can’t estimate q(D) With good
accuracy, then prove that a large error in estimating q(D) Will lead to large error in the excess risk
of `2 norm loss as Well, establishing the main loWer bound of this section.
4.2	Lower bound
In this subsection, We prove a loWer bound on the excess risk incurred by any -differentially private
algorithm Whose output is denoted by θpriv ∈ Rp . We first introduce the folloWing lemma shoWing
8
Under review as a conference paper at ICLR 2022
that it’s impossible to find the location of the ordered Fermat point q(D) with good accuracy using
a pure DP algorithm.
The proof follows the spirit of Bassily et al. (2014), constructing datasets ’far away’ from each other
such that the events of estimating the Fermat point of each dataset accurately are mutually disjoint.
Then by differential privacy as long as one can estimate one dataset accurately, one can estimate any
other one with certain probability as well. The sum of all these probabilities is no more than 1 due
to the disjointness, which leads to the desired bound.
We denote eι，(1,0,…,0)> and let ㊉ denote the direct sum of vectors, i.e. α ㊉ β = (α, β) where
a ∈ Ra ,β ∈ Rb are both vectors. For a vector a and a set S, We denote α ㊉ S = {(α,β) : β ∈ S}.
Lemma 4.4. Let n,p ≥ 2 and e > 0. There is a number M = Ω(min(n, P)) such
that for any -differentially private algorithm A, there is a dataset D = {d1, ..., dn} ⊂
(0 ㊉{ √p-, — √p-}p-1) ∪ {eι, —eι, 0} with || Pn=ι di∣∣2 ≤ M Such that, with probability at
least 1/2 (taken over the algorithm random coins), we have
||A(D) — q(D)∣∣2 = Ω(min(1,2))	⑻
n
The classic analysis of Bassily et al. (2014) contains an ’adding dummy points’ which will perturb
the location of the minimizer. In the constrained case, such perturbation won’t change the direction
of the minimizer (seen as a vector), but in the unconstrained case non-linear loss functions no longer
enjoy such good properties. To oversome this issue, we introduce the auxiliary dimension and the
dummy points we add have support only in this dimension. The benefit of doing so is that the Fermat
point q(D) will also only change along its direction after we add dummy points, which simplifies
the computation.
Lemma 4.4 implies that it’s impossible to estimate the ordered Fermat point with good accuracy
using a pure DP algorithm. In the following theorem we are going to show that a bad estimate on
the ordered Fermat point leads to higher `2 norm loss. As the fermat point is a minimizer of`2 norm
loss, we can naturally translate the discrepancy in estimating q(D) to the excess risk.
Theorem 4.5 (Lower bound for -differentially private algorithms). Let n, p ≥ 2 and > 0. For ev-
ery -differentially private algorithm with output θpriv ∈ Rp, there is a dataset D = {d1, ..., dn } ⊂
(0 ㊉{ √p-, — √p-}p-1) ∪ {eι, 一eι, 0} Such that, with probability at least 1/2 (over the algo-
rithm random coins), we must have that
L(θpriv； D) — mm 乙僚 D) = Ω(min(1, ɪ))	(9)
θ	n
The proof is based on calculation in the two-dimensional subspace spanned by q(D) and the auxil-
iary dimension. By observing that q(D) is perpendicular to the auxiliary dimension, we can parame-
terize θpriv by these two unit vectors and write down the expression of L(θpriv; D) — minθ L(θ; D)
explicitly. Then by elementary inequality scaling we get the desired result.
Remark 4.6. In fact the lower bound in Theorem 4.5 also holds for the case p = 1. The only
difference is that the case p = 1 doesn’t need the auxiliary dimension because the perturbation
of the minimizer is always along its direction. We can simply use dummy points {1, —1, 0} and a
similar analysis to Bassily et al. (2014) to achieve this result.
5	Conclusion
In this paper, we study differentially private convex ERM in the unconstrained case and give the first
tight lower bounds for approximate-DP ERM for general loss functions. Our results also directly
imply a same lower bound for the constrained case, improving the classic lower bound in Bassily
et al. (2014) by log(1∕δ). We also give an Ω(*)lower bound for unconstrained PUre-DP ERM
which recovers the result in the constrained case. Our techniques enrich the quite limited tools in
constructing lower bounds in the Private setting and we hoPe they can find future use, esPecially
for those Problems which are not (easily) reducible from one-way marginals. Designing better
algorithms for general (un)constrained DP-ERM based on our insights would also be an interesting
and meaningful direction, which we leave as future work.
9
Under review as a conference paper at ICLR 2022
References
Hilal Asi, Vitaly Feldman, Tomer Koren, and Kunal Talwar. Private stochastic convex optimization:
Optimal rates in `1 geometry. arXiv preprint arXiv:2103.01516, 2021.
Raef Bassily, Adam Smith, and Abhradeep Thakurta. Private empirical risk minimization: Efficient
algorithms and tight error bounds. In 2014 IEEE 55th Annual Symposium on Foundations of
Computer Science, pages 464-473. IEEE, 2014.
Raef Bassily, Vitaly Feldman, Kunal Talwar, and Abhradeep Guha Thakurta. Private stochastic
convex optimization with optimal rates. In Advances in Neural Information Processing Systems,
pages 11282-11291, 2019.
RaefBassily, Vitaly Feldman, Cristobal Guzman, and KUnal Talwar. Stability of stochastic gradient
descent on nonsmooth convex losses. arXiv preprint arXiv:2006.06914, 2020.
Raef Bassily, Cristobal Guzman, and Anupama Nandi. Non-euclidean differentially private StochaS-
tic convex optimization. arXiv preprint arXiv:2103.01278, 2021.
Dan Boneh and James Shaw. Collusion-secure fingerprinting for digital data. IEEE Transactions on
Information Theory, 44(5):1897-1905, 1998.
Mark Bun, Jonathan Ullman, and Salil Vadhan. Fingerprinting codes and the price of approximate
differential privacy. SIAM Journal on Computing, 47(5):1888-1938, 2018.
Kamalika Chaudhuri and Claire Monteleoni. Privacy-preserving logistic regression. In NIPS, vol-
ume 8, pages 289-296. Citeseer, 2008.
Kamalika Chaudhuri, Claire Monteleoni, and Anand D Sarwate. Differentially private empirical
risk minimization. Journal of Machine Learning Research, 12(3), 2011.
Cynthia Dwork, Frank McSherry, Kobbi Nissim, and Adam Smith. Calibrating noise to sensitivity
in private data analysis. In Theory of cryptography conference, pages 265-284. Springer, 2006.
Cynthia Dwork, Aaron Roth, et al. The algorithmic foundations of differential privacy. Foundations
and Trends in Theoretical Computer Science, 9(3-4):211-407, 2014.
Vitaly Feldman, Tomer Koren, and Kunal Talwar. Private stochastic convex optimization: optimal
rates in linear time. In Proceedings of the 52nd Annual ACM SIGACT Symposium on Theory of
Computing, pages 439-449, 2020.
Kazuto Fukuchi, Quang Khai Tran, and Jun Sakuma. Differentially private empirical risk minimiza-
tion with input perturbation. In International Conference on Discovery Science, pages 82-90.
Springer, 2017.
Moritz Hardt and Kunal Talwar. On the geometry of differential privacy. In Proceedings of the
forty-second ACM symposium on Theory of computing, pages 705-714, 2010.
Roger Iyengar, Joseph P Near, Dawn Song, Om Thakkar, Abhradeep Thakurta, and Lun Wang. To-
wards practical differentially private convex optimization. In 2019 IEEE Symposium on Security
and Privacy (SP), pages 299-316. IEEE, 2019.
Prateek Jain and Abhradeep Guha Thakurta. (near) dimension independent risk bounds for differen-
tially private learning. In International Conference on Machine Learning, pages 476-484. PMLR,
2014.
Peter Kairouz, Monica Ribero, Keith Rush, and Abhradeep Thakurta. Dimension independence in
unconstrained private erm via adaptive preconditioning. arXiv preprint arXiv:2008.06570, 2020.
Shiva Prasad Kasiviswanathan and Hongxia Jin. Efficient private empirical risk minimization for
high-dimensional learning. In International Conference on Machine Learning, pages 488-497.
PMLR, 2016.
Shiva Prasad Kasiviswanathan, Homin K Lee, Kobbi Nissim, Sofya Raskhodnikova, and Adam
Smith. What can we learn privately? SIAM Journal on Computing, 40(3):793-826, 2011.
10
Under review as a conference paper at ICLR 2022
Daniel Kifer, Adam Smith, and Abhradeep Thakurta. Private convex empirical risk minimization
and high-dimensional regression. In Conference on Learning Theory, pages 25-1. JMLR Work-
shop and Conference Proceedings, 2012.
Janardhan Kulkarni, Yin Tat Lee, and Daogao Liu. Private non-smooth empirical risk minimization
and stochastic convex optimization in subquadratic steps. arXiv preprint arXiv:2103.15352, 2021.
Benjamin IP Rubinstein, Peter L Bartlett, Ling Huang, and Nina Taft. Learning in a large function
space: Privacy-preserving mechanisms for svm learning. arXiv preprint arXiv:0911.5708, 2009.
Shuang Song, Kamalika Chaudhuri, and Anand D Sarwate. Stochastic gradient descent with differ-
entially private updates. In 2013 IEEE Global Conference on Signal and Information Processing,
pages 245-248. IEEE, 2013.
Shuang Song, Thomas Steinke, Om Thakkar, and Abhradeep Thakurta. Evading the curse of dimen-
sionality in unconstrained private glms. In International Conference on Artificial Intelligence and
Statistics, pages 2638-2646. PMLR, 2021.
Thomas Steinke and Jonathan Ullman. Between pure and approximate differential privacy. arXiv
preprint arXiv:1501.06095, 2015.
Kunal Talwar, Abhradeep Thakurta, and Li Zhang. Nearly-optimal private lasso. In Proceedings
of the 28th International Conference on Neural Information Processing Systems-Volume 2, pages
3025-3033, 2015.
Gabor Tardos. Optimal probabilistic fingerprint codes. Journal of the ACM (JACM), 55(2):1-24,
2008.
Neal R Wagner. Fingerprinting. In 1983 IEEE Symposium on Security and Privacy, pages 18-18.
IEEE, 1983.
Di Wang, Minwei Ye, and Jinhui Xu. Differentially private empirical risk minimization revisited:
Faster and more general. In Advances in Neural Information Processing Systems, pages 2722-
2731, 2017.
Puyu Wang, Yunwen Lei, Yiming Ying, and Hai Zhang. Differentially private sgd with non-smooth
loss. arXiv preprint arXiv:2101.08925, 2021.
Xi Wu, Fengan Li, Arun Kumar, Kamalika Chaudhuri, Somesh Jha, and Jeffrey Naughton. Bolt-on
differential privacy for scalable stochastic gradient descent-based analytics. In Proceedings of the
2017 ACM International Conference on Management of Data, pages 1307-1322, 2017.
Jiaqi Zhang, Kai Zheng, Wenlong Mou, and Liwei Wang. Efficient private erm for smooth objec-
tives. arXiv preprint arXiv:1703.09947, 2017.
11
Under review as a conference paper at ICLR 2022
A	Additional background knowledge
A.1 Generalized Linear Model (GLM)
The generalized linear model (GLM) is a flexible generalization of ordinary linear regression that
allows for response variables that have error distribution models other than a normal distribution. To
be specific,
Definition A.1 (Generalized linear model (GLM)). The generalized linear model (GLM) is a special
class of ERM problems where the loss function `(θ, d) takes the following inner-product form:
`(θ; d) = `(hθ, xi; y)	(10)
for d = (x, y). Here, x ∈ Rp is usually called the feature vector and y ∈ R is called the response.
A.2 Properties of differential privacy
In this subsection we introduce several very basic properties of differential privacy without proving
them (refer Dwork et al. (2014) for details). Readers familiar with the field of differential privacy
can feel free to skip this section.
Proposition A.1 (Group privacy). If M : Xn → Y is (, δ)-differentially private mechanism, then
for all pairs of datasets x, x0 ∈ Xn, then M(x), M(x0) are (k, kδek)-indistinguishable when
x, x0 differs on exact k locations.
Proposition A.2 (Post processing). If M : Xn → Y is (, δ)-differentially private and A : Y → Z
is any randomized function, then A ◦ M : Xn → Z is also (, δ)-differentially private.
Proposition A.3 (Composition). Let Mi be an (i, δi)-differentially private mechanism for all i ∈
[k]. If M[k] is defined to be
M[k] (x) = (M1(x), ..., Mk(x))	(11)
then M[k] is ( ik=1 i , ik=1 δi)-differentially private.
B Fingerprinting code
In this section we briefly introduce the mechanism of the fingerprinting code Algorithm 1. The
sub-procedure part is the original fingerprinting code in Tardos (2008), with a pair of randomized
algorithms (Gen, Trace). The code generator Gen outputs a codebook C ∈ {0, 1}n×p. The ith row
of C is the codeword of user i. The parameter p is called the length of the fingerprinting code.
The security property of fingerprinting codes asserts that any codeword can be “traced” to a user i.
Moreover, we require that the fingerprinting code can find one of the malicious users even when they
get together and combine their codewords in any way that respects the marking condition. That is,
there is a tracing algorithm Trace that takes as inputs the codebook C and the combined codeword
c0 and outputs one of the malicious users with high probability.
The sub-procedure Gen0 first uses a sin2 x like distribution to generate a parameter pj (the mean)
for each column j independently, then generates C randomly by setting each element to be 1 with
probability pj according to its location. The sub-procedure Trace0 computes a threshold value Z
and a ’score function’ Si(c0) for each user i, then report i when its score is higher than the threshold.
The main-procedure was introduced in Bun et al. (2018), where Gen adds dummy columns to the
original fingerprinting code and applies a random permutation. Trace can first ’undo’ the permu-
tation and remove the dummy columns, then use Trace0 as a black box. This procedure makes the
fingerprinting code more robust in that it tolerates a small fraction of errors to the marking condition.
C Omitted proofs
C.1 Proof of Lemma 3.1
Proof. The proof uses a black-box reduction, therefore doesn’t depend on Q. The direction
that O(n*∕e) samples are sufficient is equal to proving the assertion that given a (1,o(1∕n))-
12
Under review as a conference paper at ICLR 2022
Algorithm 1 The Fingerprinting Code (Gen, Trace)
1:	Sub-procedure Gen0 :
2:	Let d = 100n2 log(n∕ξ) be the length of the code.
3:	Let t = 1/300n be a parameter and let t0 be such that sin2 t0 = t.
4:	for j = 1, ..., d: do
5:	Choose random r uniformly from [t0, π∕2 — t0] and let Pj = sin2r7∙. Note that Pj ∈ [t, 1 一 t].
6:	For each i = 1, ..., n, set Cij = 1 with probability pj independently.
7:	end for
8:	return C
9:	Sub-procedure Trace0(C, c0):
10:	Let Z = 20n log(n∕ξ) be a parameter.
11:	For each j = 1,…,d, let qj∙ =，(1 一 Pj "pj∙.
12:	For each j = 1, ..., d, and each i = 1, ..., n, let Uij = qj if Cij = 1 and Uij = —1/qj else wise.
13:	for each i = 1, ..., n: do
14:	LetSi(c0) =Pjd=1c0jUij
15:	Output i if Si(c0) ≥ Z/2.
16:	Output ⊥ if Si(c0) < Z/2 for every i = 1, ..., n.
17:	end for
18:	Main-procedure Gen:
19:	Let C be the (random) output of Gen0, C ∈ {0, 1}n×d
20:	Append 2d 0-marked columns and 2d 1-marked columns to C .
21:	Apply a random permutation π to the columns of the augmented codebook.
22:	Let the new codebook be C0 ∈ {0, 1}n×5d.
23:	return C0
24:	Main-procedure Trace(C, c0):
25:	Obtain C0 from the shared state with Gen.
26:	Obtain C by applying π-1 to the columns of C0 and removing the dummy columns.
27:	Obtain c by applying π-1 to c0 and removing the symbols corresponding to fake columns.
28:	return i randomly from Trace0 (C, c).
differentially private algorithm A, we can get a new algorithm A0 with (, o(1/n))-differential pri-
vacy at the cost of shrinking the size of the dataset by a factor of .
Given input and a dataset X, we construct A0 to first generate a new dataset T by selecting each
element of X with probability independently, then feed T to A. Fix an event S and two neighboring
datasets X1, X2 that differs by a single element i. Consider running A on X1. If i is not included
in the sample T, then the output is distributed the same as a run on X2 . On the other hand, if i is
included in the sample T , then the behavior of A on T is only a factor of e off from the behavior
of A on T \ {i}. Again, because of independence, the distribution of T \ {i} is the same as the
distribution ofT conditioned on the omission of i.
For a set X, let PX denote the distribution of A(X), we have that for any event S,
PXi (S) = (1 — e)pχι (S|i ∈ T)+ epχι (S|i ∈ T)
≤ (1 — e)pχ2 (S)+ e(e ∙ PX2 (S) + δ)
≤ exp(2)PX2 (S) + δ
A lower bound of px、(S) ≥ exp(—e)pχ2 (S) — eδ∕e can be obtained similarly. To conclude,
since δ = o(1/n) as the sample size n decreases by a factor of , A0 has (2, o(1/n))-differential
privacy. The size ofX is roughly 1/ times larger than T, combined with the fact that A has sample
complexity n* and T is fed to A, A0 has sample complexity at least Θ(n*∕e).
For the other direction, simply using the composability of differential privacy yields the desired
result. In particular, by the k-fold adaptive composition theorem in Dwork et al. (2006), we can
combine 1/ independent copies of (, δ)-differentially private algorithms to get an (1, δ/) one and
notice that if δ = o(1/n), then δ/ = o(1/n) as well because the sample size n is scaled by a factor
of E at the same time, offsetting the increase in δ.	□
13
Under review as a conference paper at ICLR 2022
C.2 Proof of Lemma 3.2
Proof. In line 5 of algorithm 1, every column j is assigned a probability pj independently where
Pr[∣Pj - 21< 0.002] < 400	(12)
by straightforward calculation. By the Chernoff bound (with u < p/400, δ = 1), with probability at
least
1 - 2 exp(-p/800)	(13)
,at least 1 -焉 fraction of the columns have |pj - 11 ≥ 0.002. Denote mj to be the mean of
entries of column j, then by using the Chernoff bound again (with δ = 0.001), we have that with
probability at least
1 - 2 exp(-n/8000000)	(14)
a column j actually satisfies |mj - 21 ≥ 0.001. Again by the Chernoff bound (with U ≤
2 exp(-n/8000000)p and uδ = 0.01p) together with the union bound, at least 0.99 fraction of
all columns have |mj - 21 ≥ 0.001 with probability at least
1 - 2exp(-p∕800) - 2exp(-pen/8000000/40000) = 1 - O(e-Q(p))	(15)
□
C.3 Proof of Theorem 3.4
Proof. Let (α1,α2,a3) = (1/100,999/1000, exp(-Ω(p)) be the parameters in the statement of
Lemma 3.3. Let k = Θ(log(1∕δ)) be a parameter to be determined later and nk = bn∕kc.
Consider the case when P ≥ Pnk first, where Pnk = Odnk log(1∕δ)). Without loss of generality,
We assume E = 1 first, and Pnk = O(nk log(1∕δ)) corresponds to the number in Lemma 3.3 where
we set ξ = δ. We will use contradiction to prove that for any (, δ)-differentially private mechanism
M, there exists some D ∈ {0,1}n×p with Gα1-1∕k(D) ≤ 1 - α2 such that
E[L(M(D); D)- L(θ?; D)] ≥ Ω(p)
(16)
Assume for contradiction that M : {0, 1}n×p → [0, 1]n×p is a (randomized) (E, δ)-differentially
private mechanism such that
E[L(M(D); D) - L(θ?; D)] < ^^
for all D ∈ {0,1}n×p with Gɑ1-1∕k(D) ≤ (1 - α2). We then construct a mechanism Mk =
{0, 1}nk×p with respect to M as follows: with input Dk ∈ {0, 1}nk×p, Mk will copy Dk for k
times and append enough 0’s to get a dataset D ∈ {0, 1}n×p. The output is Mk(Dk) = M(D).
ek 1
Mk is (k, ee--1 δ)-differentially private by the group privacy. According to the construction above,
we know that if Gα1 (Dk) < 1 -α2, then Gɑ1-1∕k(D) < 1 - α2 as well.
We consider algorithm AFP tobe the adversarial algorithm in the fingerprinting codes, which rounds
the the output Mk(Dk) to the binary vector, i.e. rounding those coordinates with values no less than
1/2 to 1 and the remaining 0, and let c = AFP (M(D)) be the vector after rounding. As Mk is
(k, ek-1 δ)-differentially private, AFP is also (k, ek-1 δ)-differentially private.
If for some Dk ∈ {0, 1}nk×p with Gα1 (Dk) ≤ 1 - α2, D (constructed from Dk as above) further
satisfies
E[L(M(D); D) - L(θ?; D)] < ^^
As we are considering the `1 loss, we can consider the loss caused by each coordinate indepen-
dently. Recall that Mk(Dk) = M(D). The fraction of those nearly unbiased columns (the mean
is close to 1/2) is at most 1 - α2, and we treat the worst-case error for them. For other 'ɑι-biased'
columns (coordinates) which take at least α2 fraction of all, if M(D) is right for the prediction, then
appending 0’s can’t change the prediction and Mk(Dk) is also right. Thus we have that
E[L(Mk(Dk); Dk) - L(θ?; Dk)] <E[L(M(D);D) - L(θ?; D)] + (1 - α2)p < ɪ.
900
14
Under review as a conference paper at ICLR 2022
By Markov Inequality we know that
Pr[L(Mk (Dk); Dk) - L(θ?; Dk)] ≥ ɪ] ≤ 1/5.
180
c ∈/ Fβ (Dk) means that there is at least βp all-one or all-zero columns in Dk, but c is inconsistent
in those coordinates. Thus if c ∈ Fe(Dk), We have that L(Mk(Dk); Dk) - L(θ?; Dk) ≥ βp∕2 =
p/150 > p/180 for the Dk by Lemma 3.3, implying
Pr[c ∈ Fβ(Dk)] ≥ 4/5.	(17)
By the first property of the codes, one also has
Pr[L(Mk(Dk);Dk) - L(θ?; Dk) ≤ p/180 ^ Trace(Dk, c) =⊥]
≤ Pr[c ∈ Fβ(Dk) ^ Trace(Dk, c) =⊥] ≤ δ.
Recall that the arguments above are for those Dk ∈ {0, 1}nk×p With Gα1 (Dk) ≤ 1 - α2, Which
happens With probability at least 1 - α3 by the third property of fingerprinting codes. By union
bound, We can upper bound the probability Pr[Trace(Dk, c) =⊥] ≤ 1/5 + δ + α3 ≤ 1/2. As a
result, there exists i* ∈ [nk] such that
Pr[i* ∈ Trace(Dk, c)] ≥ 1∕(2nk).	(18)
Consider the database with i* removed, denoted by D-%*. Let c = AFP(M(D-犷)) denote the
vector after rounding. By the second property of fingerprinting codes, We have that
Pr[i* ∈ Trace(D-i* ,c0)] ≤ δ.
By the differential privacy and post-processing property ofM,
ek	1
Pr[i* ∈ Trace(Dk, c)] ≤ ek Pr[i* ∈ Trace(D-i* ,c0)] + ——-δ.
which implies that
ɪ ≤ ek+1δ.	(19)
2nk
Recall that 2-O(n) < δ < o(1∕n), and Equation (19) suggests k/n ≤ 2ek∕δ for all valid k, but it
is easy to see there exists k = Θ(log(1∕δ)) to make this inequality false, which is contraction. As a
result, there exists some D ∈ {0,1}n×p with Ga、— i/k (D) ≥ (1 - α2) subject to
E[L(M(D); D) - L(θ*; D)] ≥ O1000p = Ω(p).
For the (, δ)-differential privacy case, setting Q to be the condition
E[L(M(D); D) - L(θ*; D)] = O(p).
in Lemma 3.1, we have that any (, δ)-differentially private mechanism M which satisfies Q for all
D ∈ {0,1}n×p with Gɑι-ι∕k(D) ≥ 1 - α2 must have n ≥ Ω(,plog(1∕n)∕c).
Now we consider the case when P < Pnk, i.e. when n > n*，Ω(ʌ/plog(1∕δ)∕e). Given any
dataset D ∈ {0, i}n*×p with Gα1-1/k(D) ≥ 1 - α2, we will construct a new dataset D0 based on
D by appending dummy points to D like in Lemma 4.4. Specifically, if n - n* is even, we append
n - n* rows among which half are 0 and half are {1}p. If n - n* is odd, we append n-nj -1 points
0, n-n*-1 points {1}p and one point {1∕2}p.
Denote the new dataset after appending by D0, we will draw contradiction if there is an (, δ)-
differentially private algorithm M0 such that E[L(M(D0); D0) - L(θ*; D0)] = o(n*p∕n) for all D0,
by reducing M0 to an (, δ)-differentially private algorithm M which satisfies E[L(M(D); D) -
L(θ*; D)] = o(p) for all D with Gɑι-ι∕k(D) ≥ 1 - ɑ2.
15
Under review as a conference paper at ICLR 2022
We construct M by first constructing D0, and then use M0 as a black box to get M(D) = M0 (D0).
It’s clear that such algorithm for D preserves (, δ)-differential privacy. It suffices to show that if
E[L(M0(D0); D0) - L(θ?; D0)] = o(n?p/n),	(20)
then L(M(D); D) - L(θ?; D) = o(p), which contradicts the previous conclusion for the case
n ≤ n?. Specifically, if n - n? is even, we have that
n?E[L(M(D); D) - L(θ?; D)] = nE[L(M0(D0); D0) - L(θ?; D0)].
and if n - n? is odd we have that
n?E[L(M(D);)D - L(θ?; D)] ≤ nE[L(M0(D0); D0) - L(θ?; D0)] + p/2,
both leading to the desired reduction. We try to explain the above two cases in more detail. If
n 一 n* is even, then the minimizer of L(; D) and L(θ*; D) are the same. And the distributions of
the M(D) and M0(D0) are the same and indistinguishable. Multiplying n* or n depends on the
number of rows (recall that we normalize the objective function in ERM). The second inequality is
because we append one point {1/2}p, which can only increase the loss (k1/2p 一 θ* k1) by p/2 in
the worst case.
Combining results for both cases we have the following:
E[L(θpriv； D) 一 l(θ?; D)] = Ω(min(p, pn*))	(21)
n
To conclude, observe that G = √p and C = √p. In particular, let D = (d,…,d)> ∈ {0,1}n×p
contain n identical copies of rows d ∈ {0, 1}p, θ* = d. Going over all such D, we find that the
set {argminθ L(θ; D)|D ⊂ {0,1}n×p} contains {0,1}p, with diameter at least √p. Meanwhile, its
diameter can,t exceed √p obviously.	□
C.4 Details of Remark 3.7
We give a sketch of Remark 3.7. In Bun et al. (2018) they prove the following lower bound for
1-way marginals:
Proposition C.1 (Corollary 3.6 in Bun et al. (2018)). The family of 1-way marginals on {0, 1}d
has sample complexity at least Ω(√d) for (1/3,1/75)-accuracy and (O(1'), o(1 /n))-dferential
privacy.
Inspecting the proof we find that the constant 1/3 in the above proposition is chosen casually, and
can be replaced by any constant c < 1/2 for free, as the proof only requires 1 一 c is rounded to 1
and c is rounded to 0 respectively.
The third property of the fingerprinting code implies that with high probability, at most 0.01 fraction
of all columns have mean with bias smaller than 0.001. When we assume the opposite for the sake
of contradiction, by union bound, at least 1/75 一 1/100 = 1/300 fraction of columns have both
’large error on 1-way marginal’ and ’large bias on mean’.
For any such column j, the algorithm is forced to predict wrongly on the question ’Is there more 0’s
than 1’s in column j’ as the range of prediction is restricted in [0, 1] and choose c + 0.001 > 1/2,
thus leading to error on `1 norm loss.
C.5 Proof of Proposition 4.1
Proof. We assume P (D) = 0 without loss of generality. To verify P(D) is compact, We first
observe that P(D) is bounded. To prove P(D) is closed, notice that when P(D) = 0, the function
f(x) = Pin=1 ||x 一 di||2 is continuous and non negative, which implies its image is of the form
[a, ∞). Therefore the pre-image of the open set (a, ∞) is also open, whose complement is exactly
P(D).
To find the ordered Fermat point, we reduce the dimension of P(D) one after another. Because
P(D) is compact, the largest value of the first coordinate a1 , argmaxx∈P (D)x1 exists, and the
ordered Fermat point must lie on the restriction of P(D) on {x|x1 = a1} which is also compact
and non-empty. We continue this process until all dimensions are peeled and there is one point left
because the only non-empty set with zero dimension is a single point.	□
16
Under review as a conference paper at ICLR 2022
C.6 Proof of Lemma 4.4
Proof. By using a standard packing argument We can construct K = 2p-ɪ points d(1),..., d(K) in
0 ㊉{ √p-, - √p- }p-1 such that for every distinct pair d(i), d(j) of these points, We have
||d⑺一d(j) ∣∣2 ≥ 1	(22)
8
It is easy to shoW the existence of such set of points using the probabilistic method (for example, the
Gilbert-Varshamov construction of a linear random binary code).
Fix e > 0 and define n? = -∏p-. Let's first consider the case where n ≤ 4n?. We construct K
160
datasets D(1), ..., D(K) Where for each i ∈ [K], D(i) contains n copies of d(i). Note that q(D(i)) =
d(i), We have that for all i 6= j,
∣∣q(D⑺)一q(Dj))∣∣2 ≥ 1
8
(23)
Let Abeany -differentially private algorithm. Suppose that for every D(i), i ∈ [K], With probability
atleast 1/2, ∣∣A(D(i)) - q(D(i) )∣∣2 < 焉,i.e.,Pr[A(D(i)) ∈ B(D(i))] ≥ ɪ where for any dataset D,
B(D) is defined as
B(D) = {x ∈ RP :||X -q(D)∣∣2 < 116}
(24)
Note that for all i 6= j, D(i) and D(j) differs in all their n entries. Since A is -differentially private,
for all i ∈ [K], we have Pr[A(D(v)') ∈ B(D(i))] ≥ 2e-en. Since all B(D(i)) are mutually disjoint,
then
KK
EeYn ≤ Y^Pr[A(D(1)) ∈ B(D(i))] ≤ 1	(25)
which implies that n > 4n? for sufficiently large p, contradicting the fact that n ≤ 4n? . Hence,
there must exist a dataset D(i) on which A makes an '2-error on estimating q(D) which is at least
1/16 with probability at least 1/2. Note also that the `2 norm of the sum of the entries of such D(i)
is n.
p — 1
Next, we consider the case where n > 4n?. As before, we construct K = 2 "ɪ datasets
D⑴，…，D(K) of sizen where for every i ∈ [K], the first n? entries of each dataset D⑶ are
the same as dataset D(i) from before whereas the remaining n - n? entries are constructed as fol-
lows. The first [n-n C of those entries are all copies of eι whereas the following [n-n C are copies
of -e1. The last entry is set to be 0 when n - n? is odd.
Note that any two distinct datasets D⑶,D(j) in this collection differ in exactly n? entries. Let A
be any -differentially private algorithm for answering q. Suppose that for every i ∈ [K], with
probability at least 1/2, we have that
l∣A(D⑴)一q(D⑴)l∣2 < 32n
(26)
Note that for all i ∈ [K], we have that q(D(i)) = λq(D(i)) where λ
and
ʌ	n? - 1
,4[安C2-(n*不
n?
nn2 —2nn?
if n - n? is even
(27)
if n - n? is odd. We notice that n? ≤ λ ≤ 2n?, and is independent of the choice of i. Now, we
define an algorithm A for answering q on datasets D of size n? as follows. First, A computes λ
and appends e1, -e1, 0 as above to get a dataset D of size n. Then, it runs A on D and outputs
Λ
-(D). Hence, by the post-processing propertry of differential privacy, A is ^-differentially private
since A is -differentially private. Thus for every i ∈ [K], with probability at least 1/2, we have
that || A(D(i)) - q(D(i)) ∣∣2 < ι6. However, this contradicts our result in the first part of the proof.
17
Under review as a conference paper at ICLR 2022
Therefore, there must exist a dataset DD(i) in the above collection such that, with probability at least
1/2,
p
l∣A(DD⑴)-q(D⑴)l∣2 ≥ M ≥ ,19.
32n	5120n
(28)
Note that the '2 norm of the sum of entries of such DD (i) is always n?.
□
C.7 Proof of Theorem 4.5
Proof. LetAbean -differentially private algorithm for minimizing L and let θpriv denote its output.
We choose the dataset D (with corresponding di) constructed in Lemma 4.4. When n ≤ 4n?, D
contains only identical elements di so that minθ L(θ; D) = 0, and
L(θpriv； D) — mal(°； D) = L(θpriv； D) = ∣∣θpriv - q(D)∣∣2 = Ω(min(1, p-))	(29)
θ	n
by Lemma 4.4. When n > 4n*, We denote r，∣∣θpriv 一 q(D)∣∣2 = Ω(min(1,卷)).Notice that
di, e1, -e1, 0 all lie in a 2-dimensional subspace and ||di||2 = 1 is perpendicular to e1, we may
assume di = e2 without loss of generality. Because q(D) = λe2, we parameterize θpriv as follows
θpriv = (x1, λ + x2, x3, ..., xp)	(30)
where Pip=1 xi2 = r2 . Now the excess loss satisfies
L(θpriv; D) 一 min L(θ; D)
nn n?
≥ (n* W + (λ — 1)2 +2(λ — 1)x2 + bC" + 1 + λ +2λx2 +2xι
+ bn 2n C pr2 + 1 + λ2 + 2λx2 - 2xι)/n
(opening up the expression)
——n*	,- -------------- — — n	,- ---------------
≥ (b -2-C PTr + (λ — 1)2 + 2xι + b —2-C pr2 + (λ — 1)2 — 2xι)/n
(dropping the first term)
——n*	,- ----------
≥ (bCPr2 + (λ — 1)2)/n
(max{x1 , —x1 } ≥ 0)
≥b T C∙ r = Ω(min(1,匕))
2 n	n
(n > 4n*,r = Ω(min(1,—)))
n
□
18