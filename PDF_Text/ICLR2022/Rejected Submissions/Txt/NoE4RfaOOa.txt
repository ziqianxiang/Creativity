Under review as a conference paper at ICLR 2022
Where Can Quantum Kernel Based Classifica-
tion Methods Make A Big Difference?
Anonymous authors
Paper under double-blind review
Ab stract
The classification problem is a core problem of supervised learning, which is
widely present in our life. As a class of algorithms for pattern analysis, Ker-
nel methods have been widely and effectively applied to classification problems.
However, when very complex patterns are encountered, the existing kernel meth-
ods are powerless. Recent studies have shown that quantum kernel methods can
effectively handle some classification problems of complex patterns that classi-
cal kernel methods cannot handle. However, this does not mean that quantum
kernel methods are better than classical kernel methods in all cases. It is still un-
clear under what circumstances quantum kernel methods can realize their great
potential. In this paper, by exploring and summarizing the essential differences
between quantum kernel functions and classical kernel functions, we propose a
criterion based on inter-class and intra-class distance and geometric properties to
determine under what circumstances quantum kernel methods will be superior.
We validate our method with toy examples and multiple real datasets from Qiskit
and Kaggle. The experiments show that our method can be used as a valid deter-
mination method.
1	Introduction
Since the birth of quantum computing, researchers have been looking for the best place to apply
quantum algorithms. The first two quantum algorithms devised were Grover (1996) and Shor (1994).
They proved the advantages of quantum algorithms in some specific problems such as search and
factorization theoretically. With the rise of artificial intelligence, more and more quantum algorithms
combine quantum computing with machine learning. For example, quantum neural network (QNN)
was first proposed by Ezhov & Ventura (2000) but was only defined in general terms at the physical
level. Ricks & Ventura (2003) defined an approach to train QNNs, but the complexity of its method
is exponential. Subsequently, Lloyd et al. (2013), Blacoe et al. (2013), and Rebentrost et al. (2014)
tried to introduce quantum computing into clustering, distributed semantics, and SVMs, respectively,
but their approaches were too limited to theory. As researchers introduce quantum into various
machine learning algorithms, Schuld et al. (2015), Biamonte et al. (2017), Kopczyk (2018), Ciliberto
et al. (2018) have started to summarize and sort out the concept of quantum machine learning.
The physical implementation of quantum computers has made great strides in recent years. In
2019, Arute et al. (2019) announced the achievement of quantum hegemony, a milestone event
in quantum computing. Also, thanks to the emergence of several quantum computing platforms,
such as those of IBM and Google, it has become possible for ordinary researchers to translate their
research on quantum machine learning algorithms from theory to practice. For example, Farhi
& Neven (2018), Quek et al. (2021), Verdon et al. (2019), Garg et al. (2019), Srivastava et al.
(2020), and Meichanetzidis et al. (2020) have demonstrated the value of quantum machine learning
in machine learning tasks by practice, respectively.
On the other hand, one of the most famous machine learning algorithms is the kernel method. A
detailed description of the kernel method has been given by Burges (1998a), Muller et al. (2001),
Scholkopf (2001) and Hofmann et al. (2008). Inspired by the classical kernel approach, Rebentrost
et al. (2014) proposed a quantum kernel approach based on SVM, but only theoretically feasible.
It was not until SchUld & Killoran (2019), and HavHcek et al. (2019) systematically proposed two
feasible implementations of quantum kernel methods that made quantum kernel methods became
1
Under review as a conference paper at ICLR 2022
one of the most mature and practically valuable quantum machine learning methods. Later, Blank
et al. (2020), Wang et al. (2021), Kusumoto et al. (2021), and Peters et al. (2021) experimentally
demonstrated the superiority of quantum kernel methods on some datasets. However, none of them
have systematically explored the conditions when quantum kernel methods exist to their advantage.
Schuld (2021) summarizes the connection between quantum kernel methods and classical kernel
methods. However, it is still unclear when quantum kernel methods will have advantages over
classical kernel methods. In this paper, we conclude under what circumstances the quantum kernel
method is better or worse than the classical kernel method. Specifically,
•	We propose that the quantum kernel function is probabilistic and classify the existing kernel
functions.
•	We propose a distance-based criterion δ to determine whether the quantum kernel method
has the quantum advantage for a given dataset and demonstrate this experimentally.
•	We explore and find the relationship between the superiority of quantum kernel methods in
two dimensions and (1) the complexity of the data pattern (2) the data based on Mersenne
Twister random distribution.
2	Background And Related Work
Classical Kernel. Kernel methods, summarised by Muller et al. (2001) and Hofmann et al. (2008),
are an important class of machine learning methods that carry out machine learning by defining
which data points are similar to each other and which are not. Mathematically, the similarity is a
distance in the data space, i.e., the distance between digital representations of data points. Specifi-
cally, the kernel method uses a feature mapping function fc to map data from a point in the original
input space O to a higher-dimensional Hilbert feature space Fc, i.e., fc : O → Fc, making sepa-
rability between data classes more explicit. One of the most famous methods is the support vector
machine (SVM) proposed by Burges (1998b).
One important factor that makes the kernel method successful is the kernel track. Scholkopf (2001)
pointed out that instead of explicitly calculating the distance in high-dimensional Hilbert space,
this distance can be calculated implicitly in low-dimensional input space by the kernel function
K , but with the same effect. It can reduce the computational effort significantly and avoid a large
number of calculations. A nonlinear classification problem is one of the classical machine learning
problems, and kernel methods can effectively handle such problems. Recall that in classical kernel
methods, such as support vector machine, a data point xi ∈ Rn is mapped into a potentially much
higher dimensional feature space Fc via a nonlinear mapping function fc , where xi is represented
by φ(xi), i.e., fc : xi → φ(xi). In space Fc, the nonlinear classification problem becomes a linear
classification problem and simplifies the problem. The inner product of φ(xi)T φ(xj) is often seen
as distances between xi and xj in the new space Fc .
Quantum Kernel. The quantum kernel method is a kernel method designed to run on quantum
computers based on quantum computing properties. Its principle is almost identical to the classical
kernel method except that it maps the data point from the original input space O to the quantum
Hilbert space Fq, i.e., fq : O → Fq. The key to the quantum kernel methods is the quantum
mapping function fq . We can view the feature mapping function fq as the key to define the quantum
kernel methods. Thus, if the quantum kernel approach is superior, the superiority lies in the quantum
mapping function. The mechanism of quantum kernel methods is basically the same as that of
classical kernel methods. The data point xi is mapped from the original input space O to the quantum
state space Fq, i.e., fq : Xi → ∣φ(χi)i, where the ∣∙> denotes a vector and physically it represents a
state of some quantum system. The (∙∣ is the Hermitian Conjugate of the vector |). In practice, the
feature map is realized by acting the circuit U(xi) on the initial quantum state |0ni, i.e.,
∣φ(g)i = U(Xi) ∣0ni.	(1)
The quantum kernel can be obtained by running the circuit U*(xj)U(Xi) on the initial quantum
state |0ni, where Ut is the Hermitian conjugate of U. Then estimate |(0n| U*(χj)U(Xi) |0ni |2 by
counting the frequency of the 0n output as a value of k(Xi, Xj). Fig.1A shows the process flow of
the quantum kernel method and classical kernel method.
2
Under review as a conference paper at ICLR 2022
Quantum Kernel Method Based On Pauli Feature Map. Following the IBM quantum computing
platform, we take two qubits as an example. The general expression of a 2-qubit quantum kernel is
k(Xi,χj) = | hΦ(χi)∣Φ(χj)i |2 = |〈01Ut(xj)U(Xi) ∣02〉|2.	(2)
By the definition of HaVHcek et al. (2019), the quantum circuit U is realized by U (~) =
Uφ(~x)H 2Uφ(~x)H 2, where the N is the Kronecker Product of two matrices. For the Second-
order Pauli-Z eVolution circuit, Uφ(~x) = eXp(i(X0Z0 + X1Z1 + (π - X0)(π - X1)Z0Z1)), where
Z0, Z1 are quantum Z-Gate s, and H is the quantum Hadamard-Gate. We denote the corresponding
quantum kernel method as the Z-ZZ quantum kernel method, and the corresponding feature map is
showed in Fig.1B(3). The feature maps of the Z quantum kernel method and the ZZ quantum kernel
method are shown in Fig.1B(1) and Fig.1B(2), respectiVely. In this paper, all references to quantum
kernel methods refer to the Z-ZZ quantum kernel method unless otherwise stated. In section 4.3,
we compare these three quantum kernel methods.
Support Vector Machine. Support Vector Machine is a maximal margin classifier. It is seen as one
of the most successful cases of the kernel approach. SVMs are dedicated to finding a hyper-plane that
separates different classes and makes the margin as large as possible. In general cases, i.e., nonlinear
cases, the data is mapped non-linearly to high dimensional Hilbert space by a mapping function.
Then the distance between two data points can be calculated using the kernel function. Suppose
we have a set of data points D = {(x1, y1), ∙ ∙ ∙ , (Xn,yn)}, where Xi ∈ Rd and yi ∈ {-1, +1}.
According to Burges (1998b), the nonlinear SVM can be modified and expressed by an optimization
problem as maximize:
n 1n
LD ≡Σ>-2∑αiαjyiyjk(Xi , Xj)
i	i,j=0
(3)
s.t. 0 ≤ αi ≤ C and En ɑiyi = 0, where i = 1,… ,n. The decision function is
Ns	Ns
f (X) = sign(	αiyiφ(si)T φ(X) + b) = sign(	αiyik(si , X) + b),	(4)
i=1	i=1
where si are the support vectors and Ns is the number of support vectors. The SVM-based quantum
kernel method is very similar in principle to the traditional SVM, except that the computation of the
kernel is performed on a quantum computer. Havllcek et al. (2019) refer to it as quantum kernel
estimation. We show the specific estimation method in Section 4.2.
B.
A.
Figure 1: A. Working process of classical kernel method and quantum kernel method. B. Based
on the IBM quantum computing platform, (1), (2), and (3) denote the feature maps of Z, ZZ, and
Z-ZZ quantum kernel methods, respectively. Note that we only show one repetition here, and in the
experiments, the number of repetitions of each method is set to two.
3 Methods
3.1	Quantum kernel is a probabilistic kernel
The kernel function is an equation for measuring similarity. In vector space, we estimate the simi-
larity of vectors utilizing vector kernel functions. Similarly, graph kernels describe the similarity of
two graphs, and tree kernels compare the similarity of trees, which are often used in natural language
3
Under review as a conference paper at ICLR 2022
processing. A question arises as to how to define kernels or what kind of kernel functions can be
effectively used or not. There is no answer to this question. Mercer (1909) argues that a valid kernel
function needs to satisfy symmetry and positive definiteness. However, some kernel functions that
do not obey Mercer’s condition still achieve good results in some specific tasks, such as the widely
used sigmoid kernel function proposed by Lin & Lin (2003). Mix kernel function proposed by Smits
& Jordaan (2002) tries to achieve better properties when combining different kernel functions.
The mechanism of the quantum kernel function is similar to some traditional kernel functions. It
follows the Mercer theorem and is a practical kernel function that expands the family of kernel
functions. However, its implementation is based on quantum superposition states and entanglement.
Since the values obtained are based on probabilities in a statistical sense, we call it a probabilistic
kernel function. For correspondence, we call the classical kernel function a deterministic kernel
function. We try to clearly show the relationship and difference between different kernel functions
by a diagram (Fig.2). It is worth noting that there are no guarantees for one kernel to work better
than the other in all cases, according to the No Free Lunch Theorem (Wolpert & Macready (1997)).
Choosing different kernel functions in various subjects will achieve better results. The primary
purpose of this paper is to investigate under what circumstances the quantum kernel method is better
or worse than the classical kernel method.
Vector Kernel
Kernel
。辛S三qeqo」d
Satisfy Mercer⅛ Condition
Linear Kernel	Polynomial Kernel
Laplacian Kernel	RBF Kernel
Cosine Similarity
Kernel	…
Today/s Quantum Kernel
Pauli Feature Map
Based Kernel
Basic Inner
Product Based
Kernel
DLP Feature Map
Based Kernel
Customer Defined
Kernel
Don,t Satisfy Mercer⅛ Condition
Tree Kernel
Sigmoid Kernel
Customer Defined Kernel
Markov Kernel
Customer Defined Kernel
Graph
Kernel
。÷≡sc一UUw3。
Figure 2: Kernels Category
3.2 The proposed patterns and criteria for judging QKM
First, we would like to demonstrate the advantages of quantum kernel methods over classical kernel
methods when dealing with data patterns based on the Mersenne Twister random distribution. The
Mersenne Twister is a pseudo random number generator which was first proposed by Matsumoto
& Nishimura (1998). The Mersenne Twister is used as default pseudo random number generator
by many software, such as Python, R, and PHP. The Mersenne Twister random distribution is a
distribution that be generated by the Mersenne Twister method.
Theorem 3.1 (Advantages of QKM for random distributions) In two dimensions, the Z-ZZ
quantum kernel method has better learning ability than classical kernel methods for randomly dis-
tributed data patterns based on Mersenne Twister.
Proof. We assume that the Z-ZZ feature map can effectively simulate the efficacy of the feature map
proposed by Liu et al. (2021). By Matsumoto & Nishimura (1998), for a k-bits binary number, the
Mersenne Twister algorithm generates discrete uniformly distributed random numbers in the range
[0, 2k - 1]. Solving this problem is a discrete logarithm problem (DLP). For DLP, Liu et al. (2021)
say no efficient classical algorithm can achieve inverse-polynomially better accuracy than random
guessing. Therefore, the Z-ZZ quantum kernel method can demonstrate quantum superiority over
the classical kernel methods for Mersenne Twister-based randomly distributed data patterns.
4
Under review as a conference paper at ICLR 2022
Second, we demonstrate that whether quantum kernel methods have quantum advantages is related
to the δ of the data set, where δ will be defined in Equ.(7). Because any classification problem can
be transformed into a binary situation, we all base our study on the binary classification problem.
If the distance between classes is large enough, i.e., the simple case, the quantum kernel methods
are not as good as the classical kernel methods. Based on fourteen Adhoc-Modify datasets (Section
4.1 Qiskit package datasets (6)), we measure the δ of each dataset and draw a graph with δ as the
horizontal coordinate. Fig.3 shows that when δ increases to a certain level, the advantage of the
quantum kernel method disappears.
To measure the degree of separation of two classes Cl and Cm, we first define the inter-class distance
1	Nl Nm
D(Cl ,Cm) = NTr XX d(xkl),xjm)),
lm
k=1 j=1
(5)
where Nl and Nm are the number of observations belonging to class Cl and Cm , respectively. The
x(nl) is the n-th sample in class Cl and d(, ) is the euclidean distance between two samples, i.e.,
d(~, y) = PPn=1(xi - yi)2.
For the sake of uniformity, we also need to define the intra-class distance for class C, i.e.,
NN
D(C)= N(N - 1) X X d(xk ,xj),
k=1j=1,j6=k
(6)
where the N is the of class C, and xn is the n-th sample in class C. We propose a criterion to
evaluate whether quantum kernel methods will be better than classical kernel methods. In a binary
classification problem, we define the degree of integration of Cl and Cm as δlm.
δ _ D(Cl,Cm	7
δlm = D(Cl) + D(Cm)	⑺
Theorem 3.2 illustrates that the larger the δ, the greater the separation of the two classes. When the
delta is large enough, the quantum kernel function loses its quantum advantage.
Theorem 3.2 (Deficiencies of the QKM) In the case ofa balanced number of the two classes, there
exists δ0 such that the quantum kernel method will not be better than the classical kernel method in
handling classification problems when δ > δ0. In practice, δ0 is usually taken as 0.6.
Proof. Suppose our measurement independent identical distribution M1,M2 •… MR which have
expectation E(M) = μ and variance D(M) = σ2, where M is the random variable, R is the
number of measurement shots. By the Central Limit Theorem, for any m, the distribution function
Fκ(m) = P { Pi=√MRRE ≤ m} SatiSfieS: limR→∞ FR(X) = limR→∞{ "i=√MDRE ≤ x}=
√2∏ Rm∞ e-12 dt. This shows that when R is large enough, the random variable YR = Zi=IJM-RE
obeys normal distribution N(0,1). So, PR=1 Mi = √RDYR + RE = √RσYR + Rμ obeys normal
distribution N(Rμ, Rσ2). However, even fully error corrected, the quantum methods still have the
noise caused by measurement. Two types of data are linearly separable or almost linearly separable
when D(Cl, Cm) D(Cl) + D(Cm), i.e., δ δ0. It is easy for a deterministic kernel method to
find a boundary line with an infinitely small error. So, when δ > δ0, the quantum kernel method is
not better than the classical kernel method in handling classification problems.
In fact, as a probabilistic kernel method, a quantum kernel method has more error than a determinis-
tic kernel method caused by the measurement process. It is especially evident in some simple cases
because we assume that deterministic kernel methods must find a boundary line. However, errors in
probabilistic methods are inevitable.
4	Experiments
4.1	Data preparation
Qiskit package datasets. Five datasets from qiskit.ml.datasets were used in the experiments. (1)
Digits datasets. This dataset consists of 1797 8×8 images, and each image is a handwritten number
5
Under review as a conference paper at ICLR 2022
Figure 3: The relationship between δ and the predicting accuracy in Ad hoc dataset. The top graph
shows the training accuracy, and the bottom graph shows the testing accuracy.
0〜9. We perform binary classification for every two digits. Thus, 45 small datasets were generated.
(2) Breast cancer dataset. (3) IriS dataset. (4) Wine dataset. (5) AdhoC dataset. Note that (1)〜(4) are
copies from UCI ML Hand-written Digits Dataset, UCI ML Breast Cancer Wisconsin (Diagnostic)
Dataset, UCI ML Iris Plants Dataset, and UCI ML Wine Recognition Dataset, respectively. For
(5) Adhoc dataset, we separate the two classes and translate all the data points in one class by the
same distance, leaving the other class unchanged. In this way, we get 14 datasets, denoted as (6)
Adhoc-Modify datasets.
Kaggle datasets. Five datasets from Kaggle were used in the experiments. (7) Email spam dataset
(Balaka Biswas), (8) Heart disease dataset (Zeeshan Mulla). (9) Giants and dwarfs dataset (Vinesm-
suic). (10) Star type dataset (Baris Dincer). (11) Drug dataset (Pratham Tripathi).
Geometric toy datasets. We designed the two-dimensional geometric toy datasets to illustrate the
learning ability of the quantum kernel method for processing classification problems in geometric
patterns. (12) Geometric non-random datasets. We designed 2〜5 layers of concentric circles and
four squares with different mixing degrees. Fig.4A shows the four circular datasets and four square
datasets. (13) Geometric random datasets. Based on the Mersenne Twister random distribution,
we designed datasets with random distributions in three geometric patterns: Circular, square, and
equilateral triangle patterns. The three graphs on the right side of Fig.4B show a sample of these
three datasets, respectively.
4.2	Process of training and testing of quantum kernel methods
We briefly introduce the training process of the quantum kernel method here, following the idea
of Liu et al. (2021). Suppose a training dataset Dtrain = {(x1, y1), ..., (xn, yn)}. Since the only
difference of the quantum kernel method in an SVM from the classical SVM is kernel calculation,
here we only show the kernel calculation process in a quantum computer.
Training: For each pair of data points Xi and Xj (i = j) in Dtrain, we apply U * (Xi )U (Xj) on the
input 0 2 . After R repetitive runs, we record the number of times the output results in 0 2 ,
denoted as R0 and k(xi,xj∙) = RR0, where i = j and k(xi,xi) = 1. In the end, we apply Equ.(3)
directly. Testing: When there comes a new sample Xnew , for each data point Xi in Dtrain , we apply
U* (Xi)U(Xnew) on the input 0 2 . After R repetitive runs, we record the number of times the
output results in ∣0应2), denoted as R1 and k(xnew,xi) = R12R2R. In the end, we apply Equ.(4)
directly.
6
Under review as a conference paper at ICLR 2022
4.3	Experiments Results
The quantum kernel method is good at complex data patterns. In this part, we explore the ability
of quantum kernel methods to solve classification problems under challenging patterns. Firstly, we
increase the learning difficulty by increasing the complexity of the geometric patterns and get four
patterns, i.e., P1~P4 in Fig.4A. Experiments show that from P1 to P4, the classical kernel approach,
which initially performs better, gradually loses its superiority. In contrast, the superiority of the
quantum kernel approach begins to emerge.
To further increase the learning difficulty, we hypothesized the existence of a pattern in which the
data are randomly distributed over the geometry according to the Mersenne Twister. The right three
graphs in Fig.4B show this kind of data pattern. To explore the learning ability of quantum ker-
nel methods for Mersenne Twister randomly distributed data patterns, we plotted the relationship
between the size of the dataset and the prediction accuracy of each method. Considering the ran-
domness of the random distribution, we take the average result of 50 times as the result of each
experiment. The experiments demonstrate that the quantum kernel method has a more robust learn-
ing capability than the classical kernel method in this complex pattern.
O 5
L S
ADe.IrDUV
Pl	P2	P3	P4	6	12	18	30	42	54
Data Pattern	Data Size
Figure 4: A. The horizontal coordinates P1~P4 represent the data patterns of different difficulties of
the geometric non-random dataset, respectively. B. The horizontal coordinates indicate the dataset
size, and the vertical coordinates indicate the prediction accuracy. B illustrates the relationship
between the prediction correctness and the size of the dataset under three patterns in the geometric
random distribution dataset.
The quantum kernel method fails at large values of δ. In this part, we will show that our cri-
teria can effectively determine whether quantum kernel methods can demonstrate advantages over
classical kernel methods or not. First, taking the Digits dataset as an example, we perform binary
classification for every two digits. Thus, we generate 45 small datasets. We then use PCA to re-
duce the dimensionality of all data to 2 dimensions, as shown in Fig.5. We recorded each small
dataset’s training and test accuracy after applying four classical kernel methods and a quantum
kernel method. The bottom left panel in Fig.5 shows the prediction accuracy of each method. Ac-
cording to the experimental results, we attach a corner mark to each data set. The red rounded corner
markers indicate that the quantum kernel method has the potential for quantum dominance on the
corresponding dataset, while the black rounded corner markers indicate that it does not. The blue
corner markers indicate that the quantum kernel method is indistinguishable from the classical ker-
nel method. Intuitively, the quantum kernel approach for linearly divisible data sets does not show
superiority. However, for complex models with relatively high fusion, the quantum kernel approach
has the potential to be superior.
To validate our criteria δ, we prepared 81 datasets: 45 small datasets from Digits, 14 datasets from
Adhoc-Modify, Breast cancer, Iris, Wine, Email spam, Heart disease, Giants and dwarfs, Star Type,
Drug, 4 datasets from Geometric non-random, i.e., the top-left, bottom-left, top-right and bottom-
right four datasets in Fig.4A, 10 datasets from Geometric random including five circles of size thirty
and five squares of size forty. The difference in prediction accuracy between classical kernel methods
7
Under review as a conference paper at ICLR 2022
O
1
Figure 5: The top right panel shows the 45 datasets after visualization, and the bottom left panel
shows the prediction accuracy of the five methods, which are SVM models based on Gaussian kernel,
linear kernel, polynomial kernel, sigmoid kernel, and quantum kernel, respectively. The blue line
represents the training set, and the red represents the test set. Red corners indicate the case where the
training or test scores of the quantum kernel method outperform all classical kernel methods. Black
corners indicate the case where the training or test scores of the quantum kernel method are worse
than any classical kernel method. Blue corners indicate the case where the training or test scores of
the quantum kernel method are equal to the best value of all classical kernel methods.
and quantum kernel methods on each dataset is calculated. It is shown by the relevant vertical lines
in Fig.6. The magnitude of δ on each dataset is shown with an asterisk in Fig.6. According to the
experiments, when the quantum kernel method has an advantage, the asterisks appear below the 0.6
level line all the time. We can get the conclusion that (i) the availability of quantum superiority
correlates with δ and (ii) the quantum kernel method can be better than the classical kernel method
when δ < 0.6, although it is not a sufficient condition.
Simple quantum kernel methods do not offer quantum advantages. When the feature space is so
large that the kernel function is computationally expensive, quantum kernel methods can effectively
estimate their kernel functions, but classical kernel methods cannot. So, quantum kernel methods are
preferred over classical kernel methods for classification problems with complex patterns. However,
a simpler quantum kernel function can be simulated classically. For a simple-kernel-based quantum
kernel method, it no more has the superiority. Its ability to handle complex pattern problems is
significantly reduced and even inferior to classical kernel methods. Interestingly, sometimes this
simple quantum kernel function works well with simple pattern problems.
We illustrate the above opinion experimentally. In this part, we prepared 68 datasets: 45 small
datasets from Digits, Breast cancer, Iris, Wine, Adhoc, Email spam, Heart disease, Giants and
dwarfs, Star Type, Drug, 4 datasets from Geometric non-random, i.e., the top-left, bottom-left, top-
8
Under review as a conference paper at ICLR 2022
Figure 6: The horizontal coordinate is the 81 training datasets, the left vertical coordinates indicate
the method prediction accuracy, and the right vertical coordinates indicate the value of δ . The green
circles represent the best classical kernel methods in RBF, Linear, Polynomial, and Sigmoid kernel-
based kernel methods. The purple circles represent the quantum kernel methods. Under the same
dataset, if the classical kernel method is not worse than the quantum kernel method, we use the green
vertical line to indicate how better the classical kernel method is than the quantum kernel method
and the black asterisk to indicate the magnitude of δ. Otherwise, we use the purple vertical line to
indicate how better the quantum kernel method is than the classical kernel method and use the red
asterisk to indicate the magnitude of δ.
right and bottom-right four datasets in Fig.4A, 10 datasets from Geometric random including five
circles of size thirty and five squares of size forty. By comparing the performance of the best classi-
cal kernel method and the three quantum kernel methods on 68 training datasets, we find that some
kernel methods based on simple quantum kernel functions, such as the Z quantum kernel method,
do outperform the Z-ZZ kernel method on some datasets, but do not outperform the best classical
kernel methods. However, when quantum superiority exists, it is often achieved by Z-ZZ kernel
methods. The experiments show that the quantum kernel method superiority on only one dataset is
achieved by the Z quantum kernel method.
Figure 7: The horizontal coordinate is the 68 training datasets, the left vertical indicates the method’s
prediction accuracy, and the right vertical coordinate indicates the value of δ . The hollow black
circles, red circles, orange circles, and pink circles represent the best classical kernel methods, Z-ZZ
quantum kernel-based, ZZ quantum kernel, and Z quantum kernel-based quantum kernel methods,
respectively. We use blue asterisks to indicate the value of δ when the Z -quantum kernel-based
method is optimal, which has only one case, and orange asterisks to indicate the value of δ when the
ZZ -quantum kernel-based method is the best, although this case does not occur in the experiment.
Correspondingly, red and black asterisks are used to represent the value ofδ when the Z-ZZ quantum
kernel method and the classical kernel method are best, respectively.
5	Conclusion
The classification problem is one of the most common problems in machine learning, and both clas-
sical kernel methods and quantum kernel methods can effectively handle the classification problem.
However, it is difficult to determine what situation each of them is suitable for. This paper explores
when quantum kernel methods can take quantum advantage by comparing different kernel functions.
Moreover, a judgment criterion is proposed to help one decide when quantum kernel methods can
achieve better results than classical kernel methods. Experiments show that our method is effective.
9
Under review as a conference paper at ICLR 2022
References
Frank Arute, Kunal Arya, Ryan Babbush, Dave Bacon, Joseph C Bardin, Rami Barends, Rupak
Biswas, Sergio Boixo, Fernando GSL Brandao, David A Buell, et al. Quantum supremacy using
a programmable superconducting processor. Nature, 574(7779):505-510, 2019.
Balaka Biswas. Email Spam Classification Dataset. https://www.kaggle.com/balaka18/
email-spam-classification-dataset-csv.
Baris Dincer. Star Type Classification Dataset. https://www.kaggle.com/brsdincer/
star-type-classification.
Jacob Biamonte, Peter Wittek, Nicola Pancotti, Patrick Rebentrost, Nathan Wiebe, and Seth Lloyd.
Quantum machine learning. Nature, 549(7671):195-202, 2017.
William Blacoe, Elham Kashefi, and Mirella Lapata. A quantum-theoretic approach to distribu-
tional semantics. In Proceedings of the 2013 Conference of the North American Chapter of the
Association for Computational Linguistics: Human Language Technologies, pp. 847-857, 2013.
Carsten Blank, Daniel K Park, June-Koo Kevin Rhee, and Francesco Petruccione. Quantum classi-
fier with tailored quantum kernel. npj Quantum Information, 6(1):1-7, 2020.
Christopher JC Burges. A tutorial on support vector machines for pattern recognition. Data mining
and knowledge discovery, 2(2):121-167, 1998a.
CJ Burges. Data mining and knowledge discovery 2: 121, 1998b.
Carlo Ciliberto, Mark Herbster, Alessandro Davide Ialongo, Massimiliano Pontil, Andrea Roc-
chetto, Simone Severini, and Leonard Wossnig. Quantum machine learning: a classical per-
spective. Proceedings of the Royal Society A: Mathematical, Physical and Engineering Sciences,
474(2209):20170551, 2018.
Alexandr A Ezhov and Dan Ventura. Quantum neural networks. In Future directions for intelligent
systems and information sciences, pp. 213-235. Springer, 2000.
Edward Farhi and Hartmut Neven. Classification with quantum neural networks on near term pro-
cessors. arXiv preprint arXiv:1802.06002, 2018.
Dinesh Garg, Shajith Ikbal, Santosh K Srivastava, Harit Vishwakarma, Hima Karanam, and
L Venkata Subramaniam. Quantum embedding of knowledge for reasoning. Advances in Neural
Information Processing Systems, 32:5594-5604, 2019.
Lov K Grover. A fast quantum mechanical algorithm for database search. In Proceedings of the
twenty-eighth annual ACM symposium on Theory of computing, pp. 212-219, 1996.
Vejtech HavliCek, Antonio D Corcoles, Kristan Temme, Aram W Harrow, Abhinav Kandala, Jerry M
Chow, and Jay M Gambetta. Supervised learning with quantum-enhanced feature spaces. Nature,
567(7747):209-212, 2019.
Thomas Hofmann, Bernhard Scholkopf, and Alexander J Smola. Kernel methods in machine learn-
ing. The annals of statistics, 36(3):1171-1220, 2008.
Dawid Kopczyk. Quantum machine learning for data scientists. arXiv preprint arXiv:1804.10068,
2018.
Takeru Kusumoto, Kosuke Mitarai, Keisuke Fujii, Masahiro Kitagawa, and Makoto Negoro. Exper-
imental quantum kernel trick with nuclear spins in a solid. npj Quantum Information, 7(1):1-7,
2021.
Hsuan-Tien Lin and Chih-Jen Lin. A study on sigmoid kernels for svm and the training of non-psd
kernels by smo-type methods. submitted to Neural Computation, 3(1-32):16, 2003.
Yunchao Liu, Srinivasan Arunachalam, and Kristan Temme. A rigorous and robust quantum speed-
up in supervised machine learning. Nature Physics, pp. 1-5, 2021.
10
Under review as a conference paper at ICLR 2022
Seth Lloyd, Masoud Mohseni, and Patrick Rebentrost. Quantum algorithms for supervised and
unsupervised machine learning. arXiv preprint arXiv:1307.0411, 2013.
Makoto Matsumoto and Takuji Nishimura. Mersenne twister: a 623-dimensionally equidistributed
uniform pseudo-random number generator. ACM Transactions on Modeling and Computer Sim-
Ulation (TOMACS), 8(1):3-30,1998.
Konstantinos Meichanetzidis, Stefano Gogioso, Giovanni De Felice, Nicolo Chiappori, Alexis
Toumi, and Bob Coecke. Quantum natural language processing on near-term quantum computers.
arXiv preprint arXiv:2005.04147, 2020.
James Mercer. Xvi. functions of positive and negative type, and their connection the theory of
integral equations. Philosophical transactions of the royal society of London. Series A, containing
papers of a mathematical or physical character, 209(441-458):415-446, 1909.
K-R Muller, Sebastian Mika, Gunnar Ratsch, Koji Tsuda, and Bernhard Scholkopf. An introduction
to kernel-based learning algorithms. IEEE transactions on neural networks, 12(2):181-201, 2001.
Evan Peters, Joao Caldeira, Alan Ho, Stefan Leichenauer, Masoud Mohseni, Hartmut Neven, Pana-
giotis Spentzouris, Doug Strain, and Gabriel N Perdue. Machine learning of high dimensional
data on a noisy quantum processor. arXiv preprint arXiv:2101.09581, 2021.
Pratham Tripathi. Drug Classification Dataset. https://www.kaggle.com/
prathamtripathi/drug-classification.
Yihui Quek, Stanislav Fort, and Hui Khoon Ng. Adaptive quantum state tomography with neural
networks. npj Quantum Information, 7(1):1-7, 2021.
Patrick Rebentrost, Masoud Mohseni, and Seth Lloyd. Quantum support vector machine for big
data classification. Physical review letters, 113(13):130503, 2014.
Bob Ricks and Dan Ventura. Training a quantum neural network. Advances in neural information
processing systems, 16:1019-1026, 2003.
Bernhard Scholkopf. The kernel trick for distances. Advances in neural information processing
systems, pp. 301-307, 2001.
Maria Schuld. Supervised quantum machine learning models are kernel methods. arXiv preprint
arXiv:2101.11020, 2021.
Maria Schuld and Nathan Killoran. Quantum machine learning in feature hilbert spaces. Physical
review letters, 122(4):040504, 2019.
Maria Schuld, Ilya Sinayskiy, and Francesco Petruccione. An introduction to quantum machine
learning. Contemporary Physics, 56(2):172-185, 2015.
Peter W Shor. Algorithms for quantum computation: discrete logarithms and factoring. In Proceed-
ings 35th annual symposium on foundations of computer science, pp. 124-134. Ieee, 1994.
Guido F Smits and Elizabeth M Jordaan. Improved svm regression using mixtures of kernels. In
Proceedings of the 2002 International Joint Conference on Neural Networks. IJCNN’02 (Cat. No.
02CH37290), volume 3, pp. 2785-2790. IEEE, 2002.
Santosh Kumar Srivastava, Dinesh Khandelwal, Dhiraj Madan, Dinesh Garg, Hima Karanam, and
L Venkata Subramaniam. Inductive quantum embedding. Advances in Neural Information Pro-
cessing Systems, 33, 2020.
Guillaume Verdon, Trevor McCourt, Enxhell Luzhnica, Vikash Singh, Stefan Leichenauer, and Jack
Hidary. Quantum graph neural networks. arXiv preprint arXiv:1909.12264, 2019.
Vinesmsuic. Star Categorization Giants And Dwarfs Dataset. https://www.kaggle.com/
vinesmsuic/star-categorization-giants-and-dwarfs.
Xinbiao Wang, Yuxuan Du, Yong Luo, and Dacheng Tao. Towards understanding the power of
quantum kernels in the nisq era. arXiv preprint arXiv:2103.16774, 2021.
11
Under review as a conference paper at ICLR 2022
David H Wolpert and William G Macready. No free lunch theorems for optimization. IEEE trans-
actions on evolutionary computation, 1(1):67-82, 1997.
Zeeshan Mulla. Heart Disease Dataset. https://www.kaggle.com/zeeshanmulla/
heart-disease-dataset.
12