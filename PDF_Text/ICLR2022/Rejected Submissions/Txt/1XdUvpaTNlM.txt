Under review as a conference paper at ICLR 2022
BWCP: Probabilistic Learning-to-Prune Chan-
nels for ConvNets via Batch Whitening
Anonymous authors
Paper under double-blind review
Ab stract
This work presents a probabilistic channel pruning method to accelerate Con-
volutional Neural Networks (CNNs). Previous pruning methods often zero out
unimportant channels in training in a deterministic manner, which reduces CNN’s
learning capacity and results in suboptimal performance. To address this problem,
we develop a probability-based pruning algorithm, called batch whitening channel
pruning (BWCP), which can stochastically discard unimportant channels by mod-
eling the probability of a channel being activated. BWCP has several merits. (1) It
simultaneously trains and prunes CNNs from scratch in a probabilistic way, explor-
ing larger network space than deterministic methods. (2) BWCP is empowered by
the proposed batch whitening tool, which is able to empirically and theoretically
increase the activation probability of useful channels while keeping unimportant
channels unchanged without adding any extra parameters and computational cost
in inference. (3) Extensive experiments on CIFAR-10, CIFAR-100, and ImageNet
with various network architectures show that BWCP outperforms its counterparts
by achieving better accuracy given limited computational budgets. For example,
ResNet50 pruned by BWCP has only 0.58% Top-1 accuracy drop on ImageNet,
while reducing 42.9% FLOPs of the plain ResNet50.
1	Introduction
Deep convolutional neural networks (CNNs) have achieved superior performance in a variety of
computer vision tasks such as image recognition (He et al., 2016), object detection (Ren et al.,
2017), and semantic segmentation (Chen et al., 2018). However, despite their great success, deep
CNN models often have massive demand on storage, memory bandwidth, and computational power
(Han & Dally, 2018), making them difficult to be plugged onto resource-limited platforms, such as
portable and mobile devices (Deng et al., 2020). Therefore, proposing efficient and effective model
compression methods has become a hot research topic in the deep learning community.
Model pruning, as one of the vital model compression techniques, has been extensively investigated.
It reduces model size and computational cost by removing unnecessary or unimportant weights or
channels in a CNN (Han et al., 2016). For example, many recent works (Wen et al., 2016; Guo et al.,
2016) prune fine-grained weights of filters. Han et al. (2015) proposes to discard the weights that
have magnitude less than a predefined threshold. Guo et al. (2016) further utilizes a sparse mask on
a weight basis to achieve pruning. Although these unstructured pruning methods achieve optimal
pruning schedule, they do not take the structure of CNNs into account, preventing them from being
accelerated on hardware such as GPU for parallel computations (Liu et al., 2018).
To achieve efficient model storage and computations, we focus on structured channel pruning (Wen
et al., 2016; Yang et al., 2019a; Liu et al., 2017), which removes entire structures in a CNN such as
filter or channel. A typical structured channel pruning approach commonly contains three stages,
including pre-training a full model, pruning unimportant channels by the predefined criteria such as
`p norm, and fine-tuning the pruned model (Liu et al., 2017; Luo et al., 2017), as shown in Fig.1 (a).
However, it is usually hard to find a global pruning threshold to select unimportant channels, because
the norm deviation between channels is often too small (He et al., 2019). More importantly, as some
channels are permanently zeroed out in the pruning stage, such a multi-stage procedure usually not
only relies on hand-crafted heuristics but also limits the learning capacity (He et al., 2018a; 2019).
1
Under review as a conference paper at ICLR 2022
Channels	0-1 Masks	Channels	Channels
Channels
Norm
(a) Norm-based method
I	I Large
I	I Medium
I	I Small
Soft Masks
Soft Masks	Channels
ObabUity
Probability ∣	∣ Large
[	[Medium
I	I Small
(b) Our proposed BWCP
Figure 1: Illustration of our proposed BWCP. (a) Previous channel pruning methods utilize a hard
criterion such as the norm (LiU et al., 2017) of channels to deterministically remove unimportant
channels, which deteriorates performance and needs a extra fine-tuning process(Frankle & Carbin,
2018). (b) Our proposed BWCP is a probability-based pruning framework where unimportant
channels are stochastically pruned with activation probability, thus maintaining the learning capacity
of original CNNs. In particular, our proposed batch whitening (BW) tool can increase the activation
probability of useful channels while keeping the activation probability of unimportant channels
unchanged, enabling BWCP to identify unimportant channels reliably.
To tackle the above issues, we propose a simple but effective probability-based channel pruning
framework, named batch-whitening channel pruning (BWCP), where unimportant channels are
pruned in a stochastic manner, thus preserving the channel space of CNNs in training (i.e. the
diversity of CNN architectures is preserved). To be specific, as shown in Fig.1 (b), we assign each
channel with an activation probability (i.e. the probability of a channel being activated), by exploring
the properties of the batch normalization layer (Ioffe & Szegedy, 2015; Arpit et al., 2016). A larger
activation probability indicates that the corresponding channel is more likely to be preserved.
We also introduce a capable tool, termed batch whitening (BW), which can increase the activation
probability of useful channels, while keeping the unnecessary channels unchanged. By doing so,
the deviation of the activation probability between channels is explicitly enlarged, enabling BWCP
to identify unimportant channels during training easily. Such an appealing property is justified by
theoretical analysis and experiments. Furthermore, we exploit activation probability adjusted by
BW to generate a set of differentiable masks by a soft sampling procedure with Gumbel-Softmax
technique, allowing us to train BWCP in an online “pruning-from-scratch” fashion stably. After
training, we obtain the final compact model by directly discarding the channels with zero masks.
The main contributions of this work are three-fold. (1) We propose a probability-based channel
pruning framework BWCP, which explores a larger network space than deterministic methods. (2)
BWCP can easily identify unimportant channels by adjusting their activation probabilities without
adding any extra model parameters and computational cost in inference. (3) Extensive experiments on
CIFAR-10, CIFAR-100 and ImageNet datasets with various network architectures show that BWCP
can achieve better recognition performance given the comparable amount of resources compared
to existing approaches. For example, BWCP can reduce 68.08% Flops by compressing 93.12%
parameters of VGG-16 with merely accuracy drop and ResNet-50 pruned by BWCP has only 0.58%
top-1 accuracy drop on ImageNet while reducing 42.9% FLOPs.
2	Related Work
Weight Pruning. Early network pruning methods mainly remove the unimportant weights in the
network. For instance, Optimal Brain Damage (LeCun et al., 1990) measures the importance of
weights by evaluating the impact of weight on the loss function and prunes less important ones.
However, it is not applicable in modern network structure due to the heavy computation of the Hessian
matrix. Recent work assesses the importance of the weights through the magnitude of the weights
itself. Specifically, (Guo et al., 2016) prune the network by encouraging weights to become exactly
zero. The computation involves weights with zero can be discarded. However, a major drawback of
weight pruning techniques is that they do not take the structure of CNNs into account, thus failing to
help scale pruned models on commodity hardware such as GPUs (Liu et al., 2018; Wen et al., 2016).
Channel Pruning. Channel pruning approaches directly prune feature maps or filters of CNNs,
making it easy for hardware-friendly implementation. For instance, relaxed `0 regularization (Louizos
2
Under review as a conference paper at ICLR 2022
et al., 2017) and group regularizer (Yang et al., 2019a) impose channel-level sparsity, and filters with
small value are selected to be pruned. Some recent work also propose to rank the importance of
filters by different criteria including `1 norm (Liu et al., 2017; Li et al., 2017), `2 norm (Frankle &
Carbin, 2018) and High Rank channels (Lin et al., 2020). For example, (Liu et al., 2017) explores the
importance of filters through scale parameter γ in batch normalization. Although these approaches
introduce minimum overhead to the training process, they are not trained in an end-to-end manner
and usually either apply on a pre-trained model or require an extra fine-tuning procedure.
Recent works tackle this issue by pruning CNNs from scratch. For example, FPGM (He et al., 2019)
zeros in unimportant channels and continues training them after each training epoch. Furthermore,
both SSS and DSA learn a differentiable binary mask that is generated by channel importance and
does not require any additional fine-tuning. Our proposed BWCP is most related to variational
pruning (Zhao et al., 2019) and SCP (Kang & Han, 2020) as they also employ the property of
normalization layer and associate the importance of channel with probability. The main difference
is that our method adopts the idea of whitening to perform channel pruning. We will show that the
proposed batch whitening (BW) technique can adjusts the activation probability of different channels
according to their importance, making it easy to identify unimportant channels. Although previous
work SPP(Wang et al., 2017) and DynamicCP (Gao et al., 2018) also attempt to boost salient channels
and skip unimportant ones, they fail to consider the natural property inside normalization layer and
deign the activation probability empirically .
3	Preliminary
Notation. We use regular letters, bold letters, and capital letters to denote scalars such as ‘x’, and
vectors (e.g.vector, matrix, and tensor) such as ‘x’ and random variables such as ‘X’, respectively.
We begin with introducing a building layer in recent deep neural nets which typically consists of
a convolution layer, a batch normalization (BN) layer, and a rectified linear unit (ReLU) (Ioffe &
Szegedy, 2015; He et al., 2016). Formally, it can be written by
Xc = Wc * z, Xc = YcXc + βc, Yc = max{0, Xc}	(1)
where C ∈ [C] denotes channel index and C is channel size. In Eqn.(1), ‘*' indicates convolution
operation and wc is filter weight corresponding to the c-th output channel, i.e. xc ∈ RN×H×w. To
perform normalization, Xc is firstly standardized to Xc through Xc = (Xc — E[xc])/,D[xc] where
EH and D[∙] indicate calculating mean and variance over a batch of samples, and then is re-scaled to
Xc by scale parameter γc and bias βc. Moreover, the output feature yc is obtained by ReLU activation
that discards the negative part of Xc.
Criterion-based channel pruning. For channel pruning, previous methods usually employ a ‘small-
norm-less-important’ criterion to measure the importance of channels. For example, BN layer can
be applied in channel pruning (Liu et al., 2017), where a channel with a small value of γc would
be removed. The reason is that the c-th output channel Xc contributes little to the learned feature
representation when γc is small. Hence, the convolution in Eqn.(1) can be discarded safely, and filter
wc can thus be pruned. Unlike these criterion-based methods that deterministically prune unimportant
filters and rely on a heuristic pruning procedure as shown in Fig.1(a), we explore a probability-based
channel pruning framework where less important channels are pruned in a stochastic manner.
Activation probability. To this end, we define an activation probability of a channel by exploring the
property of the BN layer. Those channels with a larger activation probability could be preserved with
a higher probability. To be specific, since Xc is acquired by subtracting the sample mean and being
divided by the sample variance, we can treat each channel feature as a random variable following
standard Normal distribution (Arpit et al., 2016), denoted as Xc. Note that only positive parts can
be activated by ReLU function. Proposition 1 gives the activation probability of the c-th channel,
i.e. P(Xc) > 0.
Proposition 1 Let a random variable Xc 〜N(0,1) and Yc = max{0, YcXc + βc}. Then we have
(1) P(Yc > 0) = P(Xc > 0) = (1 + Erf(βc∕(√2∣γc∣))∕2 where Erf(X) = R0X 2∕√∏ ∙ exp-t2dt,
and (2) P(Xc > 0) = 0 ⇔ βc ≤ 0 and γc → 0.
Note that a pruned channel can be modelled by P (Xc > 0) = 0. With Proposition 1 (see proof
in Appendix A.2), we know that the unnecessary channels satisfy that Yc approaches 0 and βc is
3
Under review as a conference paper at ICLR 2022
Figure 2: A schematic of the proposed Batch Whitening Channel Pruning (BWCP) algorithm that
consists of a BW module and a soft sampling procedure. By modifying BN layer with a whitening
operator, the proposed BW technique adjusts activation probabilities of different channels. These
activation probabilities are then utilized by a soft sampling procedure.
negative. To achieve channel pruning, previous compression techniques (Li et al., 2017; Zhao et al.,
2019) merely impose a regularization on γc, which would deteriorate the representation power of
unpruned channels (Perez et al., 2018; Wang et al., 2020). Instead, we adopt the idea of whitening
to build a probabilistic channel pruning framework where unnecessary channels are stochastically
disgarded with a small activation probability while important channels are preserved with a large
activation probability.
4	Batch Whitening Channel Pruning
This section introduces the proposed batch whitening channel pruning (BWCP) algorithm, which
contains a batch whitening module that can adjust the activation probability of channels, and a soft
sampling module that stochastically prunes channels with the activation probability adjusted by BW.
The whole pipeline of BWCP is illustrated in Fig.2.
By modifying the BN layer in Eqn.(1), we have the formulation of BWCP,
Xcut =	XC	Θ	mc(P(Xc > 0))	(2)
|{z}	'-------{-------}
batch whitening	soft sampling
where xθut, XC ∈ RN×H×W denote the output of proposed BWCP algorithm and BW module,
respectively. ‘’ denotes broadcast multiplication. mc ∈ [0, 1] denotes a soft sampling that takes the
activation probability of output features of BW (i.e. P(Xc > 0)) and returns a soft mask. The closer
the activation probability is to 0 or 1, the more likely the mask is to be hard. To distinguish important
channels from unimportant ones, BW is proposed to increase the activation probability of useful
channels while keeping the probability of unimportant channels unchanged during training. Since
Eqn.(2) always retain all channels in the network, our BWCP can preserve the learning capacity of
the original network during training (He et al., 2018a). The following sections present BW and soft
sampling module in detail.
4.1	Batch Whitening
Unlike previous works (Zhao et al., 2019; Kang & Han, 2020) that simply measure the importance of
channels by parameters in BN layer, we attempt to whiten features after BN layer by the proposed
BW module. We show that BW can change the activation probability of channels according to their
importances without adding additional parameters and computational overhead in inference.
As shown in Fig.2, BW acts after the BN layer. By rewriting Eqn.(1) into a vector form, we have the
formulation of BW,
Xnij = ∑- 1(Y Θ Xnij + β)	(3)
where Xnj ∈ RC ×1 is a vector of C elements that denote the output of BW for the n-th sample at
location (i,j) for all channels. Σ-2 is a whitening operator and Σ ∈ Rc×c is the covariance matrix
of channel features {Xc}C=ι. Moreover, Y ∈ RC×1 and β ∈ RC×1 are two vectors by stacking
Yc and βc of all the channels respectively. Xnij ∈ RC×1 is a vector by stacking elements from all
channels of XnCij into a column vector.
Training and inference. Note that BW in Eqn.(3) requires computing a root inverse of a covariance
matrix of channel features after the BN layer. Towards this end, we calculate the covariance matrix Σ
within a batch of samples during each training step as given by
4
Under review as a conference paper at ICLR 2022
1 N,H,W
ς = NHW X (Y © Xnij)(Y Θ Xnij )T = (YYT) Θ P	⑷
n,i,j=1
where P is a C-by-C correlation matrix of channel features {Xc}C=ι (See details in Appendix A.1).
The Newton Iteration is further employed to calculate its root inverse, Σ-2, as given by the following
iterations
Σk = 2(3∑k-i - Σk-1∑), k = 1, 2,…，T.
(5)
where k and T are the iteration index and iteration number respectively and Σ0 = I is a identity
matrix. Note that when ∣∣I 一 Σ∣b < 1, Eqn.(5) converges to Σ-2 (Bini et al., 2005). To satisfy this
condition, Σ can be normalized by Σ∕tr(Σ) following (Huang et al., 2019), where tr(∙) is the trace
operator. In this way, the normalized covariance matrix can be written as ΣN = YYT © P/ ∣Y∣22 .
C- 1
During inference, we use the moving average to calculate the population estimate of ΣN2 by
following the updating rules, ΣN2 = (1 一 g)ΣN2 + g∑N2. Here ∑n is the covariance calculated
within each mini-batch at each training step, and g denotes the momentum of moving average. Note
ʌ -1
that ∑n2 is fixed during inference, the proposed BW does not introduce extra costs in memory or
八-1
computation since ΣN2 can be viewed as a convolution kernel with size of 1, which can be absorbed
into previous convolutional layer. For completeness, we also analyze the training overhead of BWCP
in Appendix Sec.A.3 where we see BWCP introduces a little extra training overhead.
4.2	Analysis of BWCP
In this section, we show that BWCP can easily identify unimportant channels by increasing the
difference of activation between important and unimportant channels.
-	- —	-	- -1	1/
Proposition 2 Let a random variable	X	〜N(0,1) and	Yc = max{0, [∑n2 (γ © X + β)]c}.
_____ _、 _ , .ʌ	_、	, 一 . , , ʌ	_ , , , !— . . ... , _
Then we have P(Yc > δ) = P(XC >	δ)	=	(1 + Erf((βc	— δ)∕(√2∣γd))∕2, where δ is a small
positive constant, YC and βc are two equivalent scale parameter and bias defined by BW module.
Take T = 1 in Eqn.(5) as an example, we have YC = 1 (3γc — PC=I YdYCPdc/ ∣∣Y∣∣2), and βc =
2 (3βc — PC=I βdYdYcPdc/ IlY k2) where PdC is the PearSon's correlation between channel features
XC and Xd.
By Proposition .2, BWCP can adjust activation probability by changing the values of Yc and βc
in Proposition 1 through BW module (see detail in Appendix A.4). Here we introduce a small
positive constant δ to avoid the small activation feature value. To see how BW changes the activation
probability of different channels, we consider two cases as shown in Proposition 3.
Case 1: βc ≤ 0 and Yc → 0. In this case, the c-th channel of the BN layer would be activated
with a small activation probability as it sufficiently approaches zero. We can see from Proposition
3, the activation probability of c-th channel still approaches zero after BW is applied, showing
that the proposed BW module can keep the unimportant channels unchanged in this case. Case 2:
|Yc | > 0. For this case, the c-th channel of the BN layer would be activated with a high activation
probability. From Proposition 3, the activation probability of c-th channel is enlarged after BW is
applied. Therefore, our proposed BW module can increase the activation probability of important
channels. Detailed proof of Proposition 3 can be found in Appendix A.5. We also empirically verify
Proposition 3 in Sec. 5.3. Notice that we neglect a trivial case in which the channel can be also
activated (i.e. βc > 0 and |Yc | → 0). In fact, the channels can be removed in this case because the
channel feature is always constant which can be deemed as a bias.
4.3	Soft Sampling Module
The soft sampling procedure samples the output of BW through a set of differentiable masks. To be
specific, as shown in Fig.2, we leverage the Gumbel-Softmax sampling (Jang et al., 2017) that takes
the activation probability generated by BW and produces a soft mask as given by
m，c = GumbelSoftmax(P(XC > 0)； T)	(6)
where τ is the temperature. By Eqn.(2) and Eqn.(6), BWCP stochastically prunes unimportant
channels with activation probability. A smaller activation probability makes mc more likely to be
5
Under review as a conference paper at ICLR 2022
close to 0. Hence, our proposed BW can help identify less important channels by enlarging the
activation probability of important channels, as mentioned in Sec.4.2. Note that mc can converge
to 0-1 mask when τ approaches to 0. In the experiment, we find that setting τ = 0.5 is enough for
BWCP to achieve hard pruning at test time.
Proposition 3 Let δ = ∣∣γk2 qPC=1 (Yjβc - Ycβj)2ρ2j/(kYk2 - PC=I γ2ρcj). With γ and jβ
defined in Proposition 2, we have (1) P(Xc > δ) = 0 if ∣γ∕ → 0 and βc ≤ 0, and (2) P(Xc > δ) ≥
P(Xc ≥ δ) if ∣γc∣ > 0.
Solution to residual issue. Note that the number of channels in the last convolution layer must
be the same as previous blocks due to the element-wise summation in the recent advanced CNN
architectures (He et al., 2016; Huang et al., 2017). We solve this problem by letting BW layer in the
last convolution layer and shortcut share the same mask as discussed in Appendix A.6.
4.4	TRAINING OF BWCP
This section introduces a sparsity regularization, which makes the model compact, and then describes
the training algorithm of BWCP.
Sparse Regularization. With Proposition.1, we see a main characteristic of pruned channels in BN
layer is that γc sufficiently approaches 0, and βc is negative. By Proposition 3, we find that it is also
a necessary condition that a channel can be pruned after BW module is applied. Hence, we obtain
unnecessary channels by directly imposing a regularization on γc and βc as given by
LsParse = ^X	λ1∣Yc∣ + λ2βc	⑺
c=1
where the first term makes γc small, and the second term encourages βc to be negative. The
above sparse regularizer is imposed on all BN layers of the network. By changing the strength of
regularization (i.e. λι and λ2), We can achieve different pruning ratios. In fact, βc and ∣γc∣ represent
the mean and standard deviation of a Normal distribution, respectively. Following the empirical rule
of Normal distribution, setting λ1 as triple or double λ2 Would be a good choice to encourage sparse
channels in implementation. Moreover, We observe that 42.2% and 41.3% channels With βc ≤ 0
,while 0.47% and 5.36% channels with ∣γc| < 0.05 on trained plain ReSNet-34 and ReSNet-50.
Hence, changing the strength of regularization on γc Will affect FLOPs more than that of βc . If one
wants to pursue a more compact model, increasing λ1 is more effective than λ2 .
Training Algorithm. BWCP can be easily plugged into a CNN by modifying the traditional BN
operations. Hence, the training of BWCP can be simply implemented in existing software platforms
such as PyTorch and TensorFlow. In other words, the forward propagation of BWCP can be
represented by Eqn.(2-3) and Eqn.(6), all of which define differentiable transformations. Therefore,
our proposed BWCP can train and prune deep models in an end-to-end manner. Appendix A.7 also
provides the explicit gradient back-propagation of BWCP. On the other hand, we do not introduce
extra parameters to learn the pruning mask mc . Instead, mc in Eqn.(6) is totally determined by the
parameters in BN layers including γ, β and Σ. Hence, we can perform joint training of pruning mask
mc and model parameters. The BWCP framework is provided in Algorithm 1 of Appendix Sec A.6
Final architecture. The final architecture is fixed at the end of training. During training, we use the
Gumbel-Softmax procedure by Eqn.(6) to produce a soft mask. At test time, we instead use a hard
0-1 mask achieved by a sign function (i.e. sign(P (Xc > 0) > 0.5)) to obtain the network’s output.
To make the inference stage stable, we use a sigmoid-alike transformation to make the activation
probability approach 0 or 1 in training. By this strategy, we find that both the training and inference
stage are stable and obtain a fixed compact model. After training, we obtain the final compact model
by directly pruning channels with a mask value of 0. Therefore, our proposed BWCP does not need
an extra fine-tuning procedure.
5	Experiments
In this section, we extensively experiment with the proposed BWCP on CIFAR-10/100 and ImageNet.
We show the advantages of BWCP in both recognition performance and FLOPs reduction comparing
with existing channel pruning methods. We also provide an ablation study to analyze the proposed
framework. The details of datasets and training configurations are provided in Appendix B.
6
Under review as a conference paper at ICLR 2022
Table 1: Performance comparison between our proposed approach BWCP and other methods on
CIFAR-10. “Baseline Acc.” and “Acc.” denote the accuracies of the original and pruned models,
respectively. “Acc. Drop” means the accuracy of the base model minus that of pruned models (smaller
is better). “Channels J”, “Model Size J”, and “FLOPs J” denote the relative reductions in individual
metrics compared to the unpruned networks (larger is better). ‘*’ indicates the method needs a extra
fine-tuning to recover performance. The best-performing results are highlighted in bold.
Model	Method	Baseline Acc. (%)	Acc. (%)	Acc. Drop	Channels J (%)	Model Size J (%)	FLOPs J (%)
	DCP* (Zhuang et al., 2018)	93.80	-9349-	0.31	-	49.24	50.25
	AMC* (He et al., 2018b)	92.80	91.90	0.90	-	-	50.00
ResNet-56	SFP (He et al., 2018a)	93.59	92.26	1.33	40	一	52.60
	FPGM (He et al., 2019)	93.59	92.93	0.66	40	一	52.60
	SCP (Kang & Han, 2020)	93.69	93.23	0.46	45	46.47	51.20
	BWCP (Ours)	93.64	93.37	0.27	40	44.42	50.35
	Slimming* (Liu et al., 2017)	94.39	-9239-	1.80	80	73.53	68.95
DenseNet-40	Variational Pruning (Zhao et al., 2019)	94.11	93.16	0.95	60	59.76	44.78
	SCP (Kang & Han, 2020)	94.39	93.77	0.62	81	75.41	70.77
	BWCP (Ours)	94.21	93.82	0.39	82	76.03	71.72
	Slimming* (Liu et al., 2017)	93.85	-929-	0.94	70	87.97	48.12
VGGNet-16	Variational Pruning (Zhao et al., 2019)	93.25	93.18	0.07	62	73.34	39.10
	SCP (Kang & Han, 2020)	93.85	93.79	0.06	75	93.05	66.23
	BWCP (Ours)	93.85	93.82	0.03	76	93.12	68.08
	DCP* (Zhuang et al., 2018)	94.47	-9469-	-0.22	-	23.6	27.0
MobileNet-V2	MDP (Guo et al., 2020)	95.02	95.14	-0.12	-	-	28.7
	BWCP (Ours)	94.56	94.90	-0.36	-	32.3	37.7
5.1 RESULTS ON CIFAR- 1 0
For CIFAR-10 dataset, we evaluate our BWCP on ResNet-56, DenseNet-40 and VGG-16 and compare
our approach with Slimming (Liu et al., 2017), Variational Pruning (Zhao et al., 2019) and SCP (Kang
& Han, 2020). These methods prune redundant channels using BN layers like our algorithm. We
also compare BWCP with previous strong baselines such as AMC (He et al., 2018b) and DCP
(Zhuang et al., 2018). The results of slimming are obtained from SCP (Kang & Han, 2020). As
mentioned in Sec.4.2, our BWCP adjusts their activation probability of different channels. Therefore,
it would present better recognition accuracy with comparable computation consumption by entirely
exploiting important channels. As shown in Table 1, our BWCP achieves the lowest accuracy drops
and comparable FLOPs reduction compared with existing channel pruning methods in all tested base
networks. For example, although our model is not fine-tuned, the accuracy drop of the pruned network
given by BWCP based on DenseNet-40 and VGG-16 outperforms Slimming with fine-tuning by
1.41% and 0.91% points, respectively. And ResNet-56 pruned by BWCP attains better classification
accuracy than previous strong baseline DCP AMC (He et al., 2018b) and DCP (Zhuang et al., 2018)
without an extra fine-tuning stage. Besides, our method achieves superior accuracy compared to the
Variational Pruning even with significantly smaller model sizes on DensNet-40 and VGGNet-16,
demonstrating its effectiveness. We also test BWCP with MobileNet-V2 on the CIFAR10 dataset.
From Table 1, we see that BWCP achieves better classification accuracy while reducing more FLOPs
We also report results of BWCP on CIFAR100 in Appendix B.3.
5.2	Results on ImageNet
For ImageNet dataset, we test our proposed BWCP on two representative base models ResNet-34 and
ResNet-50. The proposed BWCP is compared with SFP (He et al., 2018a)), FPGM (He et al., 2019),
SSS (Huang & Wang, 2018)), SCP (Kang & Han, 2020) HRank (Lin et al., 2020) and DSA (Ning
et al., 2020) since they prune channels without an extra fine-tuning stage. As shown in Table 2, we
see that BWCP consistently outperforms its counterparts in recognition accuracy under comparable
FLOPs. For ResNet-34, FPGM (He et al., 2019) and SFP (He et al., 2018a) without fine-tuning
accelerates ResNet-34 by 41.1% speedup ratio with 2.13% and 2.09% accuracy drop respectively, but
our BWCP without finetuning achieve almost the same speedup ratio with only 1.16% top-1 accuracy
drop. On the other hand, BWCP also significantly outperforms FPGM (He et al., 2019) by 1.07%
top-1 accuracy after going through a fine-tuning stage. For ResNet-50, BWCP still achieves better
performance compared with other approaches. For instance, at the level of 40% FLOPs reduction,
the top-1 accuracy of BWCP exceeds SSS (Huang & Wang, 2018) by 3.72%. Moreover, BWCP
outperforms DSA (Ning et al., 2020) by top-1 accuracy of 0.34% and 0.21% at level of 40% and
50% FLOPs respectively. However, BWCP has slightly lower top-5 accuracy than DSA (Ning et al.,
2020).
Inference Acceleration. We analyze the realistic hardware acceleration in terms of GPU and CPU
running time during inference. The CPU type is Intel Xeon CPU E5-2682 v4, and the GPU is
7
Under review as a conference paper at ICLR 2022
Table 2: Performance of our proposed BWCP and other pruning methods on ImageNet using base
models ResNet-34 and ResNet-50. ‘"‘indicates the pruned model is fine-tuned.
Model	Method	Baseline Top-1 Acc. (%)	Baseline Top-5 Acc. (%)	Top-1 Acc. Drop	Top-5 Acc. Drop	FLOPs，(%)
	FPGM* (He et al., 2019)	73.92	9162	1.38	0.49	41.1
	BWCP* (Ours)	73.72	91.64	0.31	0.34	41.0
ResNet-34	SFP(HeefaI7,2018a)--	73.92	91:62	2.09	1.29	4Γ.Γ
	FPGM (He et al., 2019)	73.92	91.62	2.13	0.92	41.1
	BWCP (Ours)	73.72	91.64	1.16	0.83	41.0
	FPGM* (He et al., 2019)	76.15	92.87	;	1.32	0.55	53.5
	BWCP* (Ours)	76.20	93.15	0.48	0.40	51.2
	-SSS^(HUaπg^Wang,2018)^	76.12	92:86	4.30	2.07	43.0
	DSA (Ning et al., 2020)	一	一	0.92	0.41	40.0
	HRank* (Lin et al., 2020)	76.15	92.87	1.17	0.64	43.7
ResNet-50	ThiNet* (Luo et al., 2017)	72.88	91.14	0.84	0.47	36.8
	BWCP (Ours)	76.20	93.15	0.58	0.40	42.9
	FPGM (He et al., 2019)	76.15	92.87	2.02	0.93	53.5
	SCP (Kang & Han, 2020)	75.89	92.98	1.69	0.98	54.3
	DSA (Ning et al., 2020)	一	一	1.33	0.80	50.0
	BWCP (Ours)	76.20	93.15	1.02	0.60	51.2
Table 3: Effect of BW, Gumbel-Softmax (GS),
and sparse Regularization in BWCP. The results
are obtained by training ResNet-56 on CIFAR-10
dataset. ‘BL' denotes baseline model.
Cases	BW	GS	Reg	Acc. (%)	Model Size J	FLOPs J
BL	~Γ~	X	X	-93.64-	-	-
(1)	✓	X	X	94.12	-	-
(2)	X	X	✓	93.46	-	-
(3)	X	✓	✓	92.84	46.37	51.16
(4)	✓	✓	X	94.10	7.78	6.25
(5)	✓	X	✓	92.70	45.22	51.80
BWCP	✓	✓	✓	93.37	44.42	50.35
Table 4: Effect of regularization strength λ1 and
λ2 with magnitude 1e - 4 for the sparsity loss in
Eqn.(7). The results are obtained using VGG-16
on CIFAR-100 dataset._________________________
λ1	λ2	Acc. (%)	Acc. Drop	FLOPs J (%)
1.2	0.6	73.85	-0134-	33.53
1.2	1.2	73.66	-0.15	35.92
1.2	2.4	73.33	0.18	54.19
0.6	1.2	74.27	-0.76	30.67
2.4	1.2	71.73	1.78	60.75
NVIDIA GTX1080Ti. We evaluate the inference time using ResNet-50 with a mini-batch of32 (1) on
GPU (CPU). GPU inference batch size is larger than CPU to emphasize our method‘s acceleration on
the highly parallel platform as a structured pruning method. We see that BWCP has 29.2% inference
time reduction on GPU, from 48.7ms for base ResNet-50 to 34.5ms for pruned ResNet-50, and
21.2% inference time reduction on CPU, from 127.1ms for base ResNet-50 to 100.2ms for pruned
ResNet-50.
5.3	Ablation Study
Effect of BWCP on activation probability. From the analysis in Sec. 4.2, we have shown that
BWCP can increase the activation probability of useful channels while keeping the activation
probability of unimportant channels unchanged through BW technique. Here we demonstrate this
using Resnet-34 and Resnet-50 trained on ImageNet dataset. We calculate the activation probability
of channels of BN and BW layer. It can be seen from Fig.3 (a-d) that (1) BW increases the activation
probability of important channels when ∣γc∣ > 0; (2) BW keeps the the activation probability of
unimportant channels unchanged when βc ≤ 0 and γc → 0. Therefore, BW indeed works by making
useful channels more important and unnecessary channels less important, respectively. In this way,
BWCP can identify unimportant channels reliably.
Effect of BW, Gumbel-Softmax (GS) and sparse Regularization (Reg). The proposed BWCP
consists of three components including BW module (i.e. Eqn. (3)) and Soft Sampling module with
Gumbel-Softmax (i.e. Eqn. (6)) and a spare regularization (i.e. Eqn. (7)). Here we investigate the
effect of each component. To this end, five variants of BWCP are considered: (1) only BW module
is used; (2) only sparse regularization is imposed; (3)BWCP w/o BW module; (4) BWCP w/o
sparse regularization; and (5) BWCP with Gumbel-Softmax replaced by Straight Through Estimator
(STE) (Bengio et al., 2013). For case (5), we select channels by hard 0-1 mask generated with
mc = sign(P (Xc > 0) - 0.5) 1. The gradient is back-propagated through STE. From results on
Table 3, we can make the following conclusions: (a) BW improves the recognition performance,
implying that it can enhance the representation of channels; (b) sparse regularization on γ and β
slightly harm the classification accuracy of original model but it encourages channels to be sparse as
also shown in Proposition 3; (c) BWCP with Gumbel-Softmax achieves higher accuracy than STE,
showing that a soft sampling technique is better than the deterministic ones as reported in (Jang et al.,
2017).
1y = Sign(X) = 1 if X ≥ 0 and 0 if x < 0.
8
Under review as a conference paper at ICLR 2022
(•) Ra>N∙t-34-lmrl.0.bπl	(H Rultet-34-lmrl.O.bπ 1
O 20 dθ 60
Ch*∏∏*l lndvɪ
(c) RαM∙t-50-l∙y∙ΠΛ>.bπl
(d) Rm Itot-SO-Iwarl Λ>. bπl
Oun co-⅞-ELOU^
(∙) VeGNt-Iayarl
(fl VGGNat Iw∙rl2
0J8S co-⅞-ELOυ
4βM0 MMO 120M0
Tralnlna Iterations
O 20 dβ 60 O 20 dβ 60 O
Ch*ππ*l lndvɪ	Chvnnvl lndvɪ
a dβ 60 O dβM0 MMO 12M00
Chvnnvl lndvɪ
Tralnlna Iterations
Figure 3: ((a) & (b)) and ((c) & (d)) show the effect of BWCP on activation probability with trained
ResNet-34 and ResNet-50 on ImageNet, respectively. The proposed batch whitening (BW) can
increase the activation probability of useful channels when ∣γ∕ > 0 while keeping the unimportant
channels unchanged when when βc ≤ 0 and γc → 0. (e) & (f) show the correlation score for the
output response maps in shallow and deeper BWCP modules during the whole training period. BWCP
has lower correlation score among feature channels than original BN baseline.
Impact of regularization strength λ1 and λ2. We analyze the effect of regularization strength λ1
and λ2 for sparsity loss on CIFAR-100. The trade-off between accuracy and FLOPs reduction is
investigated using VGG-16. Table 4 illustrates that the network becomes more compact as λ1 and λ2
increase, implying that both terms in Eqn.(7) can make channel features sparse. Moreover, the flops
metric is more sensitive to the regularization on γ, which validates our analysis in Sec.4.2). Besides,
we should search for proper values for λ1 and λ2 to trade off between accuracy and FLOPs reduction,
which is a drawback for our method.
Effect of the number of BW. Here the effect of the number of BW modules of BWCP is in-
vestigated trained on CIFAR-10 using Resnet-56 consisting of a series of bottleneck structures.
Note that there are three BN layers in each
(a) we use BW to modify the last BN in each
bottleneck module; hence there are a total of 18
BW layers in Resnet-56; (b) the last two BN
layers are modified by our BW technique (36
BW layers) (c) All BN layers in bottlenecks are
replaced by BW (54 BW layers), which is our
proposed method. The results are reported in
Table 5. We can see that BWCP achieves the
best top-1 accuracy when BW acts on all BN
layers, given the comparable FLOPs and model
bottleneck. We study four variants of BWCP:
Table 5: Effect of the number of BW modules on
CIFAR-10 dataset trained with ResNet-56. ‘# BW’
indicates the number of BW. More BW modules
in the network would lead to a lower recognition
accuracy drop with comparable computation con-
sumption.
# BW	Acc. (%)	Acc. Drop	Model Size J (%)	FLOPs J (%)
18	93.01	0.63	4470	50.77
36	93.14	0.50	45.29	50.45
54	93.37	0.27	44.42	50.35
size. This indicates that the proposed BWCP more benefits from more BW layers in the network.
BWCP selects representative channel features. It is worth noting that BWCP can whiten channel
features after BN through BW as shwon in Eqn.(3). Therfore, BW can learn diverse channel features
by reducing the correlations among channels(Yang et al., 2019b). We investigate this using VGGNet-
16 with BN and the proposed BWCP trained on CIFAR-10. The correlation score can be calculated
by taking the average over the absolute value of the correlation matrix of channel features. A larger
value indicates that there is redundancy in the encoded features. We plot the correlation score
among channels at different depths of the network. As shown in Fig.3 (e & f), channel features after
BW block have significantly smaller correlations, implying that channels selected by BWCP are
representative. This also accounts for the effectiveness of the proposed scheme.
6	Discussion and Conclusion
This paper presented an effective and efficient pruning technique, termed Batch Whitening Channel
Pruning (BWCP). We show BWCP increases the activation probability of useful channels while
keeping unimportant channels unchanged, making it appealing to pursue a compact model. Partic-
ularly, BWCP can be easily applied to prune various CNN architectures by modifying the batch
normalization layer. However, to achieve different levels of FLOPs reduction, the proposed BWCP
needs to search for the strength of sparse regularization. With probabilistic formulation in BWCP,
the expected FLOPs can be modeled. The multiplier method can be used to encourage the model to
attain target FLOPs. For future work, an advanced Pareto optimization algorithm can be designed to
tackle such multi-objective joint minimization. We hope that the analyses of BWCP could bring a
new perspective for future work in channel pruning.
9
Under review as a conference paper at ICLR 2022
Ethics Statement. We aim at compressing neural nets by the proposed BWCP framework. It could
improve the energy efficiency of neural network models and reduce the emission of carbon dioxide.
We notice that deep neural networks trained with BWCP can be plugged into portable or edge devices
such as mobile phones. Hence, our work and AI in edge devices would have the same negative impact
on ethics. Moreover, network pruning may have different effects on different classes, thus producing
unfair models as a result. We will carefully investigate the results of our method on the fairness of the
model output in the future.
Reproducibility Statement. For theoretical results, clear explanations of assumptions and a complete
proof of propostion 1-3 are included in Appendix. To reproduce the experimental results, we provide
training details and hyper-parameters in Appendix Sec.B. Moreover, we will also make our code
available by a link to an anonymous repository during the discussion stage.
References
Devansh Arpit, Yingbo Zhou, Bhargava U Kota, and Venu Govindaraju. Normalization propagation:
A parametric technique for removing internal covariate shift in deep networks. International
Conference in Machine Learning, 2016.
YoshUa Bengio, Nicholas Leonard, and Aaron C. Courville. Estimating or propagating gradients
through stochastic neurons for conditional computation. CoRR, abs/1308.3432, 2013. URL
http://arxiv.org/abs/1308.3432.
Dario A Bini, Nicholas J Higham, and Beatrice Meini. Algorithms for the matrix pth root. Numerical
Algorithms, 39(4):349-378, 2005.
Liang-Chieh Chen, George Papandreou, Iasonas Kokkinos, Kevin Murphy, and Alan L. Yuille.
Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully
connected crfs. IEEE Transactions on Pattern Analysis and Machine Intelligence, 40(4):834-848,
2018.
Lei Deng, Guoqi Li, Song Han, Luping Shi, and Yuan Xie. Model compression and hardware
acceleration for neural networks: A comprehensive survey. Proceedings of the IEEE, 108(4):
485-532, 2020.
Jonathan Frankle and Michael Carbin. The lottery ticket hypothesis: Finding sparse, trainable neural
networks. arXiv preprint arXiv:1803.03635, 2018.
Xitong Gao, Yiren Zhao, Eukasz Dudziak, Robert Mullins, and Cheng-Zhong Xu. Dynamic channel
pruning: Feature boosting and suppression. arXiv preprint arXiv:1810.05331, 2018.
Jinyang Guo, Wanli Ouyang, and Dong Xu. Multi-dimensional pruning: A unified framework for
model compression. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern
Recognition, pp. 1508-1517, 2020.
Yiwen Guo, Anbang Yao, and Yurong Chen. Dynamic network surgery for efficient dnns. In Advances
in neural information processing systems, pp. 1379-1387, 2016.
Song Han and William J. Dally. Bandwidth-efficient deep learning. In Proceedings of the 55th
Annual Design Automation Conference on, pp. 147, 2018.
Song Han, Jeff Pool, John Tran, and William Dally. Learning both weights and connections for
efficient neural network. In Advances in neural information processing systems, pp. 1135-1143,
2015.
Song Han, Huizi Mao, and William J. Dally. Deep compression: Compressing deep neural networks
with pruning, trained quantization and huffman coding. In ICLR 2016 : International Conference
on Learning Representations 2016, 2016.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image
recognition. In 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp.
770-778, 2016.
10
Under review as a conference paper at ICLR 2022
Yang He, Guoliang Kang, Xuanyi Dong, Yanwei Fu, and Yi Yang. Soft filter pruning for accelerating
deep convolutional neural networks. arXiv preprint arXiv:1808.06866, 2018a.
Yang He, Ping Liu, Ziwei Wang, Zhilan Hu, and Yi Yang. Filter pruning via geometric median
for deep convolutional neural networks acceleration. In Proceedings of the IEEE Conference on
Computer Vision and Pattern Recognition, pp. 4340-4349, 2019.
Yihui He, Ji Lin, Zhijian Liu, Hanrui Wang, Li-Jia Li, and Song Han. Amc: Automl for model
compression and acceleration on mobile devices. In Proceedings of the European Conference on
Computer Vision (ECCV),pp. 784-800, 2018b.
Gao Huang, Zhuang Liu, Laurens Van Der Maaten, and Kilian Q Weinberger. Densely connected
convolutional networks. In Proceedings of the IEEE conference on computer vision and pattern
recognition, pp. 4700-4708, 2017.
Lei Huang, Yi Zhou, Fan Zhu, Li Liu, and Ling Shao. Iterative normalization: Beyond standardization
towards efficient whitening. In Proceedings of the IEEE Conference on Computer Vision and
Pattern Recognition, pp. 4874-4883, 2019.
Zehao Huang and Naiyan Wang. Data-Driven Sparse Structure Selection for Deep Neural Networks.
In ECCV, 2018.
Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by
reducing internal covariate shift. arXiv preprint arXiv:1502.03167, 2015.
Eric Jang, Shixiang Gu, and Ben Poole. Categorical reparameterization with gumbel-softmax. 2017.
Minsoo Kang and Bohyung Han. Operation-aware soft channel pruning using differentiable masks.
arXiv preprint arXiv:2007.03938, 2020.
A Krizhevsky. Learning multiple layers of features from tiny images. Technical report, 2009.
Yann LeCun, John S Denker, and Sara A Solla. Optimal Brain Damage. In NIPS, 1990.
Hao Li, Asim Kadav, Igor Durdanovic, Hanan Samet, and Hans Peter Graf. Pruning Filters for
Efficient ConvNets. In ICLR, 2017.
Mingbao Lin, Rongrong Ji, Yan Wang, Yichen Zhang, Baochang Zhang, Yonghong Tian, and Ling
Shao. Hrank: Filter pruning using high-rank feature map. In Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition, pp. 1529-1538, 2020.
Zhuang Liu, Jianguo Li, Zhiqiang Shen, Gao Huang, Shoumeng Yan, and Changshui Zhang. Learn-
ing efficient convolutional networks through network slimming. In Proceedings of the IEEE
International Conference on Computer Vision, pp. 2736-2744, 2017.
Zhuang Liu, Mingjie Sun, Tinghui Zhou, Gao Huang, and Trevor Darrell. Rethinking the value of
network pruning. arXiv preprint arXiv:1810.05270, 2018.
Christos Louizos, Max Welling, and Diederik P Kingma. Learning sparse neural networks through
l_0 regularization. International Conference on Learning Representation, 2017.
Jian-Hao Luo, Jianxin Wu, and Weiyao Lin. Thinet: A filter level pruning method for deep neural
network compression. In Proceedings of the IEEE international conference on computer vision,
pp. 5058-5066, 2017.
Xuefei Ning, Tianchen Zhao, Wenshuo Li, Peng Lei, Yu Wang, and Huazhong Yang. Dsa: More
efficient budgeted pruning via differentiable sparsity allocation. arXiv preprint arXiv:2004.02164,
2020.
Ethan Perez, Florian Strub, Harm De Vries, Vincent Dumoulin, and Aaron Courville. Film: Visual
reasoning with a general conditioning layer. In Proceedings of the AAAI Conference on Artificial
Intelligence, volume 32, 2018.
11
Under review as a conference paper at ICLR 2022
Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun. Faster r-cnn: Towards real-time object
detection with region proposal networks. IEEE Transactions on Pattern Analysis and Machine
Intelligence, 39(6):1137-1149, 2017.
Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang,
Andrej Karpathy, Aditya Khosla, Michael Bernstein, et al. Imagenet large scale visual recognition
challenge. International Journal of Computer Vision, 115(3):211-252, 2015.
Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image
recognition. arXiv preprint arXiv:1409.1556, 2014.
Huan Wang, Qiming Zhang, Yuehai Wang, and Haoji Hu. Structured probabilistic pruning for
convolutional neural network acceleration. arXiv preprint arXiv:1709.06994, 2017.
Yikai Wang, Wenbing Huang, Fuchun Sun, Tingyang Xu, Yu Rong, and Junzhou Huang. Deep
multimodal fusion by channel exchanging. Advances in Neural Information Processing Systems,
33, 2020.
Wei Wen, Chunpeng Wu, Yandan Wang, Yiran Chen, and Hai Li. Learning structured sparsity in
deep neural networks. In Proceedings of the 30th International Conference on Neural Information
Processing Systems, pp. 2074-2082, 2016.
Huanrui Yang, Wei Wen, and Hai Li. Deephoyer: Learning sparser neural network with differentiable
scale-invariant sparsity measures. arXiv preprint arXiv:1908.09979, 2019a.
Jianwei Yang, Zhile Ren, Chuang Gan, Hongyuan Zhu, and Devi Parikh. Cross-channel com-
munication networks. In Advances in Neural Information Processing Systems, pp. 1295-1304,
2019b.
Mao Ye, Chengyue Gong, Lizhen Nie, Denny Zhou, Adam Klivans, and Qiang Liu. Good subnetworks
provably exist: Pruning via greedy forward selection. In International Conference on Machine
Learning, pp. 10820-10830. PMLR, 2020.
Chenglong Zhao, Bingbing Ni, Jian Zhang, Qiwei Zhao, Wenjun Zhang, and Qi Tian. Variational
convolutional neural network pruning. In Proceedings of the IEEE Conference on Computer Vision
and Pattern Recognition, pp. 2780-2789, 2019.
Zhuangwei Zhuang, Mingkui Tan, Bohan Zhuang, Jing Liu, Yong Guo, Qingyao Wu, Junzhou Huang,
and Jinhui Zhu. Discrimination-aware channel pruning for deep neural networks. arXiv preprint
arXiv:1810.11809, 2018.
12
Under review as a conference paper at ICLR 2022
The appendix provides more details about approach and experiments of our proposed batch whitening
channel pruning (BWCP) framework. The broader impact of this work is also discussed.
A More Details ab out Approach
A.1 CALCULATION OF COVARIANCE MATRIX Σ
By Eqn.(1) in main text, the output of BN is Xncij = Ycxncij + βc. Hence, We have E[Xc]=
NHW PNHW(Ycxncij + βc) = βc. Then the entry in c-th row and d-th column of covariance
matrix Σ of Xc is calculated as follows:
1 N,H,W
Ncd = NHW): (Ycxncij + βc - E[Xc])(Ydxndij + 8d - E[Xd]) = YcYdPcd	(8)
n,i,j
where Pcd is the element in c-th row andj-th column of correlation matrix of x. Hence, we have Pcd ∈
[-1,1]. Furthermore, we can write Σ into the vector form: Σ = YYT Θ NHW PNiH,W XnijxnijT =
γγT ρ.
A.2 Proof of Proposition 1
For (1), we notice that we can define Yc = -Yc and Xc =-Xc 〜N(0,1) if Yc < 0. Hence, we can
assume Yc > 0 without loss of generality. Then, we have
P(Yc > 0) = P(Xc > 0) = P(Xc > -β)
∕+∞ 1	_t2 ,
,_exp 2 dt
J-βc √2∏
γc
I'0 -Lexp-哆 dt + ∣`+∞ 3expV dt
J- βc √2∏	Jo	√2∏
γc
(9)
产
广
0
-^exp-12 dt + Z+∞ -^exp-12 dt
√2∏	Jo	√2∏
Ef( √YC) + 1
2
When Yc < 0, we can set Yc = -Yc. Hence, we arrive at
P(Yc > 0) = P(Xc > 0) = Erf( √2J + 1	(10)
τ-τ /C'	1	1	. T>	A L/C T ∖	1	-∖7-	T> .	∏	1 ，广	CC -∖7- 、	1	，广
For (2), let us denote Xc 〜N(0,1), and Xc = YcXc + βc and Yc = max{0,Xc} where Yc
represents a random variables corresponding to output feature yc in Eqn.(1) in main text. Firstly, it is
easy to see that P(Xc > 0) =0 ⇔ Eχχc [Yc] = 0 and Eχχc [Yʃ] = 0. In the following we show that
EχXc [Yc] = 0 and Eχχc ∖Y2 = 0 ⇔ βc ≤ 0 and Yc = 0. Similar with (1), we assume Yc > 0 without
loss of generality.
For the sufficiency, we have
「βc	1
ExXc [Yc] =	0 ∙ √=exp
-∞	2π
_x2 _	∕*+∞	_	1
t 2 dxc +	(Ycxc + βc) ∙ -7= exp
J- βc	√2π
γc
-χ2 7-
, 2 dxc,
Ycexp
√2π
βC
2γ2	β
—+ β (1 + Erf[
2
βc
√2Yc
]),
(11)

where Erf [x] = √ Rx exp-t2 dt is the error function. From Eqn.(11), we have
lim EχcK]
γc →o+
lim
γc→o+
Ycexp
βC
2γ2	βr
：—+ lim βc (1 + Erf[
γc→o+ 2
βc
]) = 0
(12)

√2π
13
Under review as a conference paper at ICLR 2022
Table 6: Runing time comparison during training between BWCP, vanilla BN and SCP. The proposed
BWCP achieves better trade-off between FLOPs reduction and accuracy drop although it introduces
a little extra computational cost during training. ‘F’ denotes forward running time (s) while ‘F+B’
denotes forward and backward running time (s). The results are averaged over 100 iterations. The
GPU is NVIDIA GTX1080Ti. The CPU type is Intel Xeon E5-2682 v4.
Model	Mothod	CPU (F) (s)	CPU (F+B) (s)	GPU (F) (s)	GPU (F+B) (s)	Acc. Drop	FLOPs J (%)
	vanilla BN	0.184	0.478	0.015	0.031	0	0
ResNet-50	SCP	0.193	0.495	0.034	0.067	1.69	54.3
	BWCP (Ours)	0.239	0.610	0.053	0.104	1.02	51.2
In the same way, we can calculate
f- βc	1	χ2	+r∞
ExCK2] =	0 ∙ -7≡=exp 2 dxc +	(YCxC + βC)2
J-∞	2 2∏	J-βc
γC
C
-	----/=T
YCeCeXp 2γc
√2π
十三(1+Erf[ √⅛,
_xC
2 dxC,
(13)
From Eqn.(13), we have
γclimo+ 除 [Y2]
-21
YCeCeXp 2γ2
lim ------j=---
γC→0+	2π
+γclim+ 三 (1+Erf[＞。
(14)
For necessity, We show that if Exc [YC] = 0 and E比c [YC2] = 0, then YC → 0 and β0 ≤ 0. In essence,
It can be acquired by solving Eqn Eqn.(11) and Eqn.(13). To be specific, βC*Eqn.(.11)-Eqn.(13)
gives us YC = 0+ . Substituting it into Eqn.(.11), we can obtain eC ≤ 0. This completes the proof.
A.3 Training overhead of BWCP
The proposed BWCP introduces a little extra computational cost during training. To see this, we
evaluate the computational complexity of SCP and BWCP for ResNet50 on ImageNet with an input
image size of 224 × 224. We can see from the table below that the training BWCP is slightly slower
on both CPU and GPU than the plain ResNet and SCP. In fact, the computational burden mainly
comes from calculating the covariance matrix and its root inverse. In our paper, we calculate the
root inverse of the covariance matrix by Newton’s iteration, which is fast and efficient. Although
BWCP brings extra training overhead, it achieves better top-1 accuracy drop under the same FLOPs
consumption.
A.4 Proof of Proposition 2
一	一 q	―-1 ,	二	__1 ,	二、一1 一	-1 .二 一1 一
First, we can derive that XC = ∑n (YΘX+β) = ∑n2 (YΘX) + ∑n β = (∑n2 Y)ΘX + ∑n β.
-1	ʌ	-1
Hence, the newly defined scale and bias parameters are Y = ∑n2 Y and β = ∑n2 β. When T = 1,
we have ∑n2 = 2 (3I - ∑n) by Eqn.(5) in main text. Hence we obtain,
1	1	YYT
Y=产-ςn-i-⅛∙ θ P)Y
2(3y -⅛^
CC
γγ YiYjPijYj∙,…E YcYjPCjYj
=1 j =1
T
)
(15)
1
2
C2	C2
(3 - X jj )Y1,…，(3-X 当)yc
j⅛ kYk2	j=i kYk2
T
14
Under review as a conference paper at ICLR 2022
Similarly, β can be given by
1	1	γγT
β=5(3I- ∑)β =x(3I - γγ2 © ρ)β
2	2	kγk2
C
C
T
2(3β -
1
kYk2
C
31 - (
j=1
∑YιYjPijβj, …，EYcYjPCjBj	)
j=1
Yjej Pij
kY k2
j=1
)Y1,…,邛 C - XIYF )YC
(16)
1
2
T
-1 1 1 •	1	. /'	. 1 -	.1	♦	∕' ʌ	1 A • 1 ʌ	∙ . ∙ C
Taking each component of vector Eqn.(15-16) gives Us the expression of Yc and βc m Proposition 2.
A.5 Proof of Proposition 3
For(1), through Eqn.(15), we acquire ∣^c∣ = ɪ |3 — PC=I jcj ∣∣γc∣. Therefore, ∣^c∣ → 0 if ∣γc∣ → 0.
1	22
On the other hand, by Eqn.(16), We have βc ≈ 1 (3 — ^^)βc < βc ≤ 0. Here We assume that
Pcd = 1 if c = d and 0 otherwise. Note that the assumption is plausible by Fig.3 (e & f) in main
text from Which We see that the correlation among channel features Will gradually decrease during
training. We also empirically verify these tWo conclusions by Fig.4. From Fig.4 We can see that
∣γc∣ ≥ ∣γc∣ where the equality holds iff ∣γc∣ = 0, and βc is larger than βc if βc is positive, and ViCe
versa. By Proposition 1, We arrive at
,ʌ , ʌ
P(Xc > δ) < P(Xc > 0)=0
(17)
where the first '>' holds since δ is a small positive constant and '=’ follows from ∣γc| → 0 and
βc ≤ 0. For (2), to show P(Xc > δ) > P(Xc > δ), we only need to prove P(Xc > ---^βc-) >
一,二 A ^ ^ 一.一.	.	R ^ RG -	—. 一	一 一
P(Xc > --βc-), which is equivalent to ---^βc- < --β∣c. To this end, we calculate
∣γc∣βc - ι^c∣βc = |Yc| 2 (3及 - (PC=I γ⅜⅜j )γc) - 2(3 - PC=I ⅛⅜j )lγclβc
lγcl-lγcl	2 (3 — PC=I ⅛ )∣γcl-∣γcl
■PC Yj βj YCPCj	PC	YjeCPCj
=乙j=i	IWkj - 乙j=i
1 _ PC	YjPCj
1 - "1 E
=PC=IYj(βjγC-YjβC)PCj ≤ k⅛匚 1 (βjYc-Yjβc)2P2j
P sPC	Yj ρCj	P sPC	Yj ρCj
1 - "1 E	1 - "1 E
_ l∣γl∣2qPC=i (βjYc - Yjβc)2ρcj _ s
ι∣γ ∣∣2 - PC=I Y2pcj	δ
(18)
where the '≤' holds due to the CaUChy—Schwarz inequality. By Eqn.(18), we derive that ∣Yc∣(δ -
βc) ≤ ∣Yc∣(δ - βc) which is exactly what we want. Lastly, we empirically verify that δ defined in
Proposition 3 is a small positive constant. In fact, δ represents the minimal activation feature value
(i.e. Xc = YcXc + βc ≥ δ by definition). We visualize the value of δ in shallow and deep layers
in ResNet-34 during the whole training stage and value of δ of each layer in trained ResNet-34 on
ImageNet dataset in Fig.5. As we can see, δ is always a small positive number in the training process.
We thus empirically set δ as 0.05 in all experiments.
To conclude, by Eqn.(17), BWCP can keep the activation probability of unimportant channel un-
changed; by Eqn.(18), BWCP can increase the activation probability of important channel. In this
way, the proposed BWCP can pursuit a compact deep model with good performance.
15
Under review as a conference paper at ICLR 2022
0	10	20	30	40	50	∞
CMr>n∙∣ M∙x
0	20	40	60	80 100 UO
CMnrMi M∙x
100 UO 200	250	0 IOO 200	300	400	500
Chlnn∙l Index	O>∙nn∙l teκi∙x
Figure 4:	Experimental observation of how our proposed BWCP changes the values of Yc and βc
in BN layers through the proposed BW technique. Results are obtained by tranining ResNet50 on
ImageNet dataset. We investigate γc and βc at different depths of the network including layer1.0.bn1,
layer2.0.bn1,layer3.0.bn1 and layer4.0.bn1. (a-d) show BW enlarges βc when βc > 0 while reducing
βc when βc ≤ 0. (e-h) show that BW consistently increases the magnitude of γc across the network.
(a) ResNet-34-layer 1.0.bn 1
0.022 -I	0.0050
0.020
0.018
0.016
(O 0.014
0.012
0.010
0.∞8
0.006
0.0045
0.0040
0.0035
0.0030
0.0025
0.0020
Λ AniK
(b) ResNet-34-layer4.0.bn 1
0.05
0.04
0.03
0.02
0.01
0.00
(c) ResNet-34-per layer
20	40	60	80
Trelnlng Epochs
1∞
0	20	4 0	60	80	100	0	5	10	15	20	2 5	30
Trelnlng Epochs	Layers
Figure 5:	Experimental observation of the values of δ defined in proposition 3. Results are obtained
by tranining ResNet-34 on ImageNet dataset. (a & b) investigate δ at different depths of the network
including layer1.0.bn1 and layer4.0.bn1 respectively. (c) visualizes δ for each layer of ResNet-34.
We see that δ in proposition 3 is always a small positive constant.
Figure 6: Illustration of BWCP with shortcut in basic block structure of ResNet. For shortcut with
Conv-BN modules, we use a simple strategy that lets BW layer in the last convolution layer and
shortcut share the same mask. For shortcut with identity mappings, we use the mask in previous layer.
16
Under review as a conference paper at ICLR 2022
Algorithm 1 Forward Propagation of the proposed BWCP.
1:	Input: mini-batch inputs X ∈ RNXCXH×W.
2:	Hyperparameters: momentum g for calculating root inverse of covariance matrix, iteration number T .
3:	Output: the activations xout obtained by BWCP.
4:	calculate standardized activation: {XC}C=1 in Eqn.(1).
5:	calculate the output of BN layer: XC = YCXC + βc.
6:	calculate normalized covariance matrix: ΣN = Yk2 Θ NH W PNiHJW X nij X nij
7:	Σ0 = I.	2
8:	for k = 1 to T do
9:	∑k = I (3∑k-ι — ∑k-ι∑N)
10:	end for
-1
11:	calculate whitening matrix for training: ΣN2 = ∑t.
-1	-1	-1
12:	calculate whitening matrix for inference: ΣN2! — (I-g>N2 + g ςν 2.
1
-K ~
13:	calculate whitened output: Xnij = ∑N Xnij.
人	—1	1 C
14:	calculate equivalent scale and bias defined by BW: Y = Σ- 2 Y and β = Σ- 2 β.
15:	calculate the activation probability by Proposition 2 with Y and β, obtain soft masks {mc}C=1 by Eqn.(6).
16:	calculate the output of BWCP: Xout = XC Θ mc.
Figure 7: Illustration of forward propagation of (a) BN and (b) BWCP. The proposed BWCP prunes
CNNs by replacing original BN layer with BWCP module.
A.6 Solution to Residual Issue
The recent advanced CNN architectures usually have residual blocks with shortcut connections (He
et al., 2016; Huang et al., 2017). As shown in Fig.6, the number of channels in the last convolution
layer must be the same as in previous blocks due to the element-wise summation. Basically, there
are two types of residual connections, i.e. shortcut with downsampling layer consisting of Conv-
BN modules, and shortcut with identity. For shortcut with Conv-BN modules, the proposed BW
technique is utilized in downsampling layer to generate pruning mask mS. Furthermore, we use a
simple strategy that lets BW layer in the last convolution layer and shortcut share the same mask
as given by mc = mS ∙ mCast where mCast and mS denote masks of the last convolution layer. For
shortcut with identity mappings, we use the mask in the previous layer. In doing so, their activated
output channels must be the same.
A.7 Back-propagation of BWCP
The forward propagation of BWCP can be represented by Eqn.(3-4) and Eqn.(9) in the main text
(see detail in Table 1), all of which define differentiable transformations. Here we provide the
back-propagation of BWCP. By comparing the forward representation of BN and BWCP in Fig.7,
we need to back-propagate the gradient ddLut to ddL for backward propagation of BWCP. For
∂xnij	∂xnij
simplicity, we neglect the subscript ‘nij’.
By chain rules, we have
∂ L
∂ X
=Y Θ m Θ
∂ L
∂ Xout
∂ L	_1
+/②N 2 )
(19)
17
Under review as a conference paper at ICLR 2022
Table 7: Performance of our BWCP on different base models compared with other approaches on
CIFAR-100 dataset.______________________________________________________________
Model	Mothod	Baseline Acc. (%)	Acc. (%)	Acc. Drop	Channels J (%)	Model Size J (%)	FLOPs J (%)
	Slimming* (Liu et al., 2017)	77.24	-74.52-	2.72	60	29.26	47.92
ResNet-164	SCP (Kang & Han, 2020)	77.24	76.62	0.62	57	28.89	45.36
	BWCP (Ours)	77.24	76.77	0.47	41	21.58	39.84
	Slimming* (Liu et al., 2017)	74.24	-73.53-	0.71	60	54.99	50.32
DenseNet-40	Variational Pruning (Zhao et al., 2019)	74.64	72.19	2.45	37	37.73	22.67
	SCP (Kang & Han, 2020)	74.24	73.84	0.40	60	55.22	46.25
	BWCP (Ours)	74.24	74.18	0.06	54	53.53	40.40
VGGNet-19	Slimming* (Liu et al., 2017)	72.56	-73.01-	-0.45	50	76.47	38.23
	BWCP (Ours)	72.56	73.20	-0.64	23	41.00	22.09
	Slimming* (Liu et al., 2017)	73.51	-73.45-	0.06	40	66.30	27.86
VGGNet-16	Variational Pruning (Zhao et al., 2019)	73.26	73.33	-0.07	32	37.87	18.05
	BWCP (Ours)	73.51	73.60	-0.09	34	58.16	34.46
where ∂L(∑n2) denotes the gradient w.r.t. X back-propagated through ∑n2
_1
obtain the gradient w.r.t. ∑n2 as given by
To calculate it, we first
	∂ L	=Y γ∂^T+β ∂fT	(20)
	_ 1 ∂ ∑N 2		
where ∂L 	= ∂γ	二 X Θ m G	∂ L	∂m .	∂ L ∂Xout + ∂γ X θ ∂Xout	(21)
and	∂L	∂m	∂L = m H(X Θ	)) ∂ β	∂β	∂ Xout		(22)
The remaining thing is to calculate d∂m and ∂m^. Based on the Gumbel-Softmax transformation, we
arrive at
mc(I-mcfC^c,βC)	if d = C
TP(XC>0)(1-P(XC>0)),	=
0, otherwise
-mc(I-mcf6c,βC)	βcYc if d = c
TP(XC>0)(1-P(XC>0)) |Yc|2 1	=
0, otherwise
(23)
(24)
1	P，八 A ∖ ∙ . 1	FF ∙ 1 ∙ . 1	∙ . i'	. ∙	i' 1 ʌ X τ G	∙ . .	♦ 1 ■	∕C∖ i'	∙
where f (γc, βc) is the probability density function of R.V. Xc as written in Eqn.(2) of main text.
_ 1
To proceed, we deliver the gradient w.r.t. ∑n2 in Eqn.(20) to Σ by Newton Iteration in Eqn.(6) of
-1
main text. Note that ∑n2 = ∑t, we have
∂ΣN=- 2 X W-I)T 爸
k=1
(25)
where ∂∑ can be calculated by following iterations:
∂L	3 ∂L	1 ∂L	2 T 1	2 T ∂L	T
∂∑k-1 = 2∂∑k - 2∂∑k(∑k-1∑) - 2(∑k-1) ∂∑kς
1	∂L
-2(Ek-I)Td__(Ek-IE)T k = T,…，1.
Given the gradient w.r.t. ∑n in Eqn.(25), we can calculate the gradient w.r.t. X back-propagated
-1
through ∑n2 in Eqn.(19) as follows
∂ L ι	T^T	∂ L	∂ LT
dL(∑N2) = (Yγ2 ® (行 + 行T))X	(26)
∂X	kγk	∂EN	∂EN
Based on Eqn.(19-26), we obtain the back-propagation of BWCP.
18
Under review as a conference paper at ICLR 2022
B More Details about Experiment
B.1	Dataset and Metrics
We evaluate the performance of our proposed BWCP on various image classification benchmarks,
including CIFAR10/100 (Krizhevsky, 2009) and ImageNet (Russakovsky et al., 2015). The CIFAR-10
and CIFAR-100 datasets have 10 and 100 categories, respectively, while both contain 60k color images
with a size of 32 × 32, in which 50k training images and 10K test images are included. Moreover,
the ImageNet dataset consists of 1.28M training images and 50k validation images. Top-1 accuracy
are used to evaluate the recognition performance of models on CIFAR 10/100. Top-1 and Top-5
accuracies are reported on ImageNet. We utilize the common protocols, i.e. number of parameters
and Float Points Operations (FLOPs) to obtain model size and computational consumption.
For CIFAR-10/100, we use ResNet (He et al., 2016), DenseNet (Huang et al., 2017), and VGGNet (Si-
monyan & Zisserman, 2014) as our base model. For ImageNet, we use ResNet-34 and ResNet-50.
We compare our algorithm with other channel pruning methods without a fine-tuning procedure.
Note that a extra fine-tuning process would lead to remarkable improvement of performace (Ye et al.,
2020). For fair comparison, we also fine-tune our BWCP to compare with those pruning methods.
The training configurations are provided in Appendix B.2. The base networks and BWCP are trained
together from scratch for all of our models.
B.2	Training Configuration
Training Setting on ImageNet. All networks are trained using 8 GPUs with a mini-batch of 32 per
GPU. We train all the architectures from scratch for 120 epochs using stochastic gradient descent
(SGD) with momentum 0.9 and weight decay 1e-4. We perform normal training without sparse
regularization in Eqn.(7) on the original networks for first 20 epochs by following (Ning et al., 2020).
The base learning rate is set to 0.1 and is multiplied by 0.1 after 50, 80 and 110 epochs. During
fine-tuning, we use the standard SGD optimizer with Nesterov momentum 0.9 and weight decay
0.00005 to fine-tune pruned network for 150 epochs. We decay learning rate using cosine schedule
with initial learning rate 0.01. The coefficient of sparse regularization λ1 and λ2 are set to 7e-5 and
3.5e-5 to achieve Flops Reduction at a level of 40%, while λ1 and λ2 are set to 9e-5 and 3.5e-5
respectively to achieve FLOPs reduction at a level of 40%. Besides, the covariance matrix in the
proposed BW technique is calculated within each GPU. Like (Huang et al., 2019), we also use
group-wise decorrelation with group size 16 across the network to improve the efficiency of BW.
Training setting on CIFAR-10 and CIFAR-100. We train all models on CIFAR-10 and CIFAR-100
with a batch size of 64 on a single GPU for 160 epochs with momentum 0.9 and weight decay 1e-4.
The initial learning rate is 0.1 and is decreased by 10 times at 80 and 120 epoch. The coefficient of
sparse regularization λ1 and λ2 are set to 4e-5 and 8e-5 for CIFAR-10 dataset and 7e-6 and 1.4e-5
for CIFAR-100 dataset.
B.3	More Results of BWCP
The results of BWCP on CIFAR-100 dataset is reported in Table 7. As we can see, our approach
BWCP achieves the lowest accuracy drops and comparable FLOPs reduction compared with existing
channel pruning methods in all tested base models.
19