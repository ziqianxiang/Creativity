Under review as a conference paper at ICLR 2022
a General Unified Graph Neural Network
FRAMEWORK AGAINST ADVERSARIAL ATTACKS
Anonymous authors
Paper under double-blind review
Ab stract
Graph Neural Networks (GNNs) are powerful tools in representation learning for
graphs. However, they are reported to be vulnerable to adversarial attacks, rais-
ing numerous concerns for applying it in some risk-sensitive domains. There-
fore, it is essential to develop a robust GNN model to defend against adversarial
attacks. Existing studies address this issue only considering cleaning perturbed
graph structure, and almost none of them simultaneously consider denoising fea-
tures. As the graph and features are interrelated and influence each other, we pro-
pose a General Unified Graph Neural Network (GUGNN) framework to jointly
clean the graph and denoise features of data. On this basis, we further extend
it by introducing two operations and develop a robust GNN model(R-GUGNN)
to defend against adversarial attacks. One operation is reconstructing the graph
with its intrinsic properties, including similarity of two adjacent nodes’ features,
sparsity of real-world graphs and many slight noises having small eigenvalues in
perturbed graphs. The other is the convolution operation for features to find the
optimal solution adopting the Laplacian smoothness and the prior knowledge that
nodes with many neighbors are difficult to attack. Experiments on four real-world
datasets demonstrate that R-GUGNN has greatly improved the overall robustness
over the state-of-the-art baselines.
1	Introduction
Graph Neural Networks(GNNs) have drawn great attention as graphs can represent complex re-
lationships among nodes. Graphs are ubiquitous in different domains, which are usually applied
in recommender systems(Ying et al., 2018a), chemistry(Duvenaud et al., 2015), social media(Qiu
et al., 2018) and so on. Utilizing the strong representation capacity of graphs, we can enhance
performance of down-stream tasks such as node classification(Kipf & Welling, 2017; Velickovic
et al., 2018; Klicpera et al., 2019), link prediction(Grover & Leskovec, 2016; Bojchevski et al.,
2018) and graph classification(Defferrard et al., 2016; Ying et al., 2018b). A GNN model often
consists of several graph convolution layers. A common practice of convolution layers is utilizing
a feed-forward network to transform features and then aggregating transformed features. A series
of convolution layers have been proposed and achieved great success such as GCN(Kipf & Welling,
2017), GAT(Velickovic et al., 2018) and PPNP(Klicpera et al., 2019).
However, GNN models composed of these convolution layers are vulnerable to adversarial attacks.
Attacks can be conducted on either node features or the graph structure, while most existing adver-
sarial attacks on graph data focus on modifying the graph structure(Xu et al., 2020). They always
try to add, delete, or rewire edges to change the graph structure. Although these perturbations are
unnoticeable, they can easily degrade the performance of GNN models, which may cause bad con-
sequences. For example, spammers may create virtual followers to increase the chance of false
messages being recommended and spread. The lack of GNNs’ robustness raises increasing con-
cerns for applying it in some risk-sensitive domains. Therefore, it is necessary to develop graph
defense techniques. Many existing defense methods focus on cleaning perturbed graphs by detect-
ing properties of clean graphs and effects of specific attacks on graphs(Entezari et al., 2020; Jin
et al., 2020b). Prior knowledge according to these researches can help GNN models defend against
adversarial attacks to a certain extent. The study(Jin et al., 2020b) has proved that adversarial at-
tacks could lead perturbed graphs to violate some properties of real graphs. For example, the rank of
attacked graph increases and adversarial attacks often connect nodes with large feature differences.
1
Under review as a conference paper at ICLR 2022
Figure 1: Concrete design of R-GUGNN. We clean the attacked graph and denoise features.
However, existing studies only focus on cleaning the perturbed graph structure, and almost none of
them simultaneously consider denoising features.
As the graph and features are closely tied and contain perturbations and noises, in this paper, we pro-
pose a General Unified Graph Neural Network(GUGNN) framework to jointly clean the graph and
denoise features. Based on the GUGNN framework, we further introduce two kinds of operations.
One operation is reconstructing the graph with its properties that real-world graphs are sparse(Zhou
et al., 2013), the features of two adjacent nodes tend to be similar(McPherson et al., 2001) and per-
turbed graphs have many slight noises with small eigenvalues. Nodes with more neighbors are hard
to attack(ZUgner et al., 2018). Though We cannot change the number of nodes' first-order neigh-
bors, we can adjust the size of nodes’ neighborhood to add some high-order neighbors to nodes.
According to this principle, from the denoising perspective, We design the convolution operation
for features to find the optimal solution. Utilizing the tWo kinds of operations above, We develop a
robust model(R-GUGNN), Which can be applied for defending against different adversarial attacks.
The contributions of this paper are summarized as folloWs:
•	We propose GUGNN frameWork to jointly clean the graph and denoise features for pertur-
bations and noises existing in the graph and features.
•	We introduce tWo kinds of operations to clean attacked graphs and denoise features respec-
tively based on the GUGNN frameWork.
•	For defending against adversarial attacks, We develop a concrete model R-GUGNN to re-
alize the goal of the GUGNN frameWork utilizing the tWo kinds of operations.
•	Experiments shoW that R-GUGNN has a strong capacity for defending against different
adversarial attacks and stably outperforms the state-of-the-art defense models.
2	Related Work
This section has tWo parts, including graph neural netWorks, and adversarial attacks and defenses
for GNNs.
2.1	graph neural networks
In this subsection, We revieW some famous graph neural netWork models, including GCN, GAT,
PPNP, and a unified GNN frameWork UGNN. For more knoWledge about GNNs, you can refer to
some revieWs(Wu et al., 2020; Zhang et al., 2020).
The convolution of GCN(Kipf & Welling, 2017) is defined in the graph spectral domain. Avoiding
computing the full eigenvectors of the graph Laplacian matrix, based on Chebyshev polynomials,
GCN only uses the first-order polynomial to simplify the graph convolution, Which has been an
accepted good graph convolution layer for its performance. The convolution of GAT(Velickovic
et al., 2018) is defined in the spatial domain. The difference betWeen GAT and GCN is that GAT
learns different attention scores for neighbors When aggregating features. PPNP(Klicpera et al.,
2
Under review as a conference paper at ICLR 2022
2019) derives a propagation scheme based on personalized PageRank. It propagates information
from a large and adjustable neighborhood instead of the first-order neighborhood directly. The
neighborhood can be adjusted via a hyper-parameter.
UGNN(Ma et al., 2020) is a unified GNN framework available for different feature propagation
processes from a denoising perspective. It proposes a denoising optimization problem with the
Laplacian regularization term and tries to solve it in different ways utilizing the first derivative or the
optimal solution. Original Laplacian matrix can be also replaced with different normalized forms.
Different solutions to the optimization problem are corresponding to various convolution layers with
different feature aggregation processes such as GCN, GAT, PPNP and so on.
2.2	adversarial attacks and defenses for GNNs
We recommend a repository DeepRobust(Li et al., 2020) for readers. It contains many adversarial
attacks and defenses on the graph, which is quite useful for researchers. For more knowledge about
adversarial attacks and defenses for GNNs, you can refer to the review(Jin et al., 2020a).
Some adversarial attack methods have been proposed to show the vulnerability of GNNs with some
unnoticeable perturbations added to the graph structure or node attributes. In the field of node
classification, the aim of adversarial attacks is fooling GNNs into classifying nodes incorrectly. Poi-
soning attacks change the graph structure before we train GNN models, which is one of the most
common settings of adversarial attacks on graph data. Poisoning attacks have various types, includ-
ing global attack, targeted attack and random attack. Based on the whole graph, the goal of global
attack is to degrade the overall performance of GNNs. One of the state-of-the-art global attacks is
metattaCk(Zugner & Gunnemann, 2019), which generates the poisoning attacks based on meta-
learning. Targeted attack generates attacks on some specific nodes and aims to fool GNNs on these
target nodes. The nettack(Zugner et al., 2018) is one of the state-of-the-art targeted attacks, which
aims to change the graph structure and features of target nodes or nearby nodes with perturbations
remaining unnoticeable. Random attack adds random noises to the clean graph whose concrete
practice is adding, removing or flipping edges randomly.
Methods about preventing GNNs from adversarial attacks are also developed to improve the ro-
bustness of GNNs recently. To mitigate the effects of adversarial attacks on the graph, RGCN(Zhu
et al., 2019) uses Gaussian distributions as hidden representations of nodes instead of plain vectors
in other GNNs. Considering that nettack is a high-rank attack, GCN-SVD(Entezari et al., 2020) is
proposed to reconstruct the perturbed graph with only the top-k largest singular components. Using
such a low-rank approximation, GCN-SVD can reduce the effects of nettack. In fact, the practice
of ensuring low-rank is removing noises with small singular values of the graph. Pro-GNN(Jin et al.,
2020b) jointly optimizes a structural graph and a robust GNN model from the perturbed graph with
some properties of clean graphs. Pro-GNN has big improvement over other defense models with
these properties. However, like other defense models, Pro-GNN doesn’t take denoising features into
account jointly.
3	THE PROPOSED FRAMEWORK
In this section, we first present GUGNN framework, and then we introduce a novel graph recon-
struction operation. At last, we show the convolution operation for features and our concrete design
of R-GUGNN model, which is used to realize the goal of GUGNN framework.
3.1	notions
We denote some notations here. Denote X ∈ RN ×d as the feature matrix, where N, d represent
the number of samples and the dimension of features respectively. Denote G = {V, E} as the
graph, where V represents node sets and E represents edge sets. We also use the adjacency matrix
A ∈ RN ×N to represent G .
3
Under review as a conference paper at ICLR 2022
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
Algorithm 1: R-GUGNN
Input: Adjacency matrix A, Feature matrix X, Labels y, Hyper-parameters m, c,β, λ, Learning
rate η .
Output: GNN parameters θ
~ _ . . .. ~ ~
A = A + I; Initialize S = A;
for i= 1 to m do
Calculating Z in formula(5);
S = S - C Z;
S = prox*β (S);
S = Proxs (S);
Initialize F = X;Randomly initialize θ;
while Stopping condition is not met do
Forward propagation using two convolution layers:
Using feature transformation formula: F = FW,
and feature aggregation formula: F = (I + λL)-1 F;
Getting output y0 ;
Calculating gradient g according to y and y0 ;
Backward propagation: θ = θ - ηg;
return θ;
3.2	the general unified GNN framework
To discard perturbations and noises in the graph and features, considering the tight connection be-
tween them, we propose our general united graph neural network(GUGNN) framework to solve
such problem, which is shown as follows:
argmin L = ∣∣S - AkF + YkF - XkF + C ∙ tr(FTLF) + β ∙ f(S)
S,F
(1)
where S and F are the learned adjacency and feature matrix. L is the Laplacian matrix of S.
L = D - S, where D is a diagonal matrix and Dii = PjN=1 Sij. L can be also replaced with
different normalized forms. tr(FTLF) is Laplacian regularization term for both denoising features
and cleaning the graph. f(S) is a flexible regularization term to enforce some prior over S. γ, c and
β are hyper-parameters to balance different components.
From a united perspective, we view that both features and the graph contain noises and our goal is
jointly optimizing F and S. tr(FTLF) can be rewritten as 11 PNj=I Sij(fi - j)2, where fi is the
i-th row of F. This term represents that features of two adjacent nodes should be similar, which is
the guidance for both learning F and S. Although X and A have some noises, they can represent the
real features and the graph to a large extent. So, the learned F and S should be similar to X and A
respectively, which are the meanings of kF - Xk2F and kS - Ak2F . In addition, we add some prior
to the graph in f(S) to make it more accurate.
3.3	the novel graph reconstruction operation
We focus on cleaning the perturbed graph supposing F=X. Formula(1) of GUGNN can be rewritten
as follows:
argmin L = ∣∣S - AkF + C ∙ tr(XTLX) + β ∙ f (S)	⑵
S
Considering that the graph contains noises, we rewrite formula(2) as follows:
argmin L = ∣∣S — AkF + C ∙ tr(XTLX) + β∣∣Sk*
S	(3)
= L1 + L2 + L3
The adjacency matrix with self-loop A and the normalized Laplacian matrix L are adopted. L =
D-2 LD-2. tr(XT L X) is equal to 1 PNj=I Sij (√xi-√j= )2. Since degrees of the perturbed
4
Under review as a conference paper at ICLR 2022
Table 1: Description of datasets
	NLCC	ELCC	Classes	Features
Cora	2485	5069	7	1433
Citeseer	2110	3668	6	3703
Cora-ML	2810	7981	7	2879
Polblogs	1222	16714	2	/
graph are approximately equal to those of the real graph, for the convenience of calculation, we let
Dii = pN=i Aj. ksk* = Prank(S) σi , where σi is the i-th singular value of S.
To solve formula(3), We let dL∂+ L2 = 0 to get the closed form solution.
∂ L1 + L2
∂S-
= 2(S - Ae) + c
(4)
(5)
S

A
—
c
2
We denote formula(5) as S = A - C Z for convenience. A proximal operator of nuclear norm is
adopted to remove noises and reserve main properties(Entezari et al., 2020).
prox*β(S) = Udiag(max{σi — β, 0})iVT	(6)
where S = Udiag(σι... σw)VT is the singular value decomposition of S. Let S = prox*β(S) to
represent this step. For the constraint Sij∈ [0, 1], We let S = S + I to enhance self-loop, and set
Sij<0 to 0 and Sij>1 to 1. We denote this step as S = proxs(S), which can make the graph sparse
at the same time.
3.4	the convolution operation for features
After getting the cleaned graph through several graph reconstruction operations above, we fix it and
focus on denoising features, formula(1) of GUGNN can be rewritten as follows:
argmin L = ∣∣F - XkF + λ ∙tr(FTLF)	⑺
F
where λ = C. In this case, formula(7) is equal to that of UGNN(Ma et al., 2020). We use the
normalized Laplacian matrix L and let ∂F = 0 to find the optimal solution.
∂L
∂F =2(F - X) + 2λLF = 0	(8)
F = (I + λL)-1X	(9)
Formula(9) is the process of feature aggregation. Before it, we let X = XW to transform features,
where W is the parameter of a single GNN convolution layer. This is our convolution operation for
features, which is proved(Ma et al., 2020) equal to PPNP(Klicpera et al., 2019). So, our convolution
operation for features can also adjust nodes’ neighborhood to enhance the model’s robustness.
3.5	the design of R-GUGNN model
Utilizing the two kinds of operations, we design R-GUGNN model and show it in Figure 1, where
m is the number of graph reconstruction operations we can set. In step ①，we fix features and
clean the graph with m graph reconstruction operations. In step ②，we fix the cleaned graph and
denoise features with two graph convolution layers. We train GNN parameters θ using the two graph
convolution layers and classify nodes finally. Concrete steps of R-GUGNN are shown in Algorithm
1.
5
Under review as a conference paper at ICLR 2022
Table 2: Node classification performance (Accuracy±Std) under metattack
Datasets	Ptb Rate(%)	GCN	GAT	RGCN	GCN-SVD PrO-GNN R-GUGNN
	0	83.06±0.52^^84.09±0.66^^83.83±0.59^^77.69±0.52^^85.49±0.38^^82.97±0.33
	5	77.08±1.05	79.96±1.01	79.21±0.42	77.54±0.91	79.03±1.31	82.25±0.51
	10	70.46±1.14	74.82±1.33	73.11±0.76	72.73±0.90	74.11±0.77	80.91±0.30
Cora	15	65.21±1.64	70.17±1.34	68.68±0.80	69.11±0.67	70.34±0.50	80.60±0∙37
	20	54.69±1.37	58.33±1.49	58.35±0.39	57.46±2.04	67.78±0.48	77.96±1.15
	25	49.53±1.47	52.22±2.76	53.27±0.56	54.46±1.60	66.19±0.85	76.05±1.37
	0	72.22±0.49^^72.87±1.38^^72.98±0.29^^67.89±0.68^^72.68±0.78^^73.59±0.33
	5	69.76±1.91	71.87±1.60	71.66±0.43	68.44±0.44	72.17±1.67	73.15±0.71
	10	67.25±1.30	70.02±1.18	69.17±0.57	69.73±0.88	73.06±0.50	73.96±0.42
Citeseer	15	63.87±1.47	67.30±2.03	65.93±0.37	68.06±0.45	71.24±0.54	73.25±0.80
	20	56.00±1.36	60.08±1.09	56.83±0.54	68.71±0.65	69.22±0.65	71.64±0.59
	25	57.10±2.45	61.00±1.99	58.69±0.47	65.43±1.01	57.23±1.22	71.74±0.99
	0	85.77±0.32^^85.46±0.51 ^^85.97±0.42^^78.78±0.17^^85.30±0.66^^85.29±0.24
	5	80.01±0.42	81.20±0.81	80.68±0.39	77.92±0.22	83.92±0.45	84.70±0.35
「era ΛΛT	10	74.51±0.56	75.97±0.90	74.70±0.76	77.61±0.39	81.69±0.42	84.12±0.16
Cora-ML	15	54.36±0.66	57.80±1.24	55.86±1.06	74.92±0.33	53.88±0.45	82.13±0.46
	20	45.64±0.71	42.02±2.36	48.08±0.29	51.01±0.94	46.99±2.82	70.74±0.90
	25	48.20±1.45	46.68±2.51	50.58±0.42	66.57±0.51	50.82±0.45	74.30±0.47
	0	95.83±0.40^^94.99±0.41 ^^95.30±0.25^^91.93±0.37^^95.25±0.14^^95.68±0.31
	5	72.81±0.91	76.69±0.96	72.04±0.54	89.10±0.35	93.53±0.47	95.30±0.57
Polblogs	10	72.71±0.80	72.56±1.33	71.89±0.51	81.25±0.50	87.53±0.83	94.37±0.85
	15	68.35±0.42	54.73±8.66	68.66±0.66	70.27±2.42	85.88±1.79	91.44±5.74
	20	59.34±2.45	50.07±4.35	62.14±0.80	58.73±3.64	77.05±3.35	84.13±5.08
	25	58.39±1.61	50.91±2.45	59.89±0.96	53.03±2.32	70.34±2.05	70.72±7.27
4	experiments
In this section, we evaluate the effectiveness of R-GUGNN model compared with the state-of-the-
art GNN models against different attacks. We first introduce the experimental settings and then
present results of a series of experiments. At last, we conduct the ablation study and analyze hyper-
parameters of R-GUGNN.
4.1	Experimental settings
4.1.1	datasets
We compare different models on four benchmark datasets, including three citation graphs, i.e.,
Cora(McCanUm et al., 2000), Citeseer(GiIes et al.,1998)and Cora-ML(Bojchevski & Gunnemann,
2017), and one blog graph, i.e., Polblogs(Jin et al., 2020b). Cora-ML is the subset of machine learn-
ing papers from Cora dataset, which is also a well-known dataset in GNN field. Since Polblogs
dataset has no node features, a N × N identity matrix is used to act as the feature matrix. We
only consider the largest connected Component(LCC) in each dataset(Jin et al., 2020b; ZUgner et al.,
2018). Table 1 contains detailed information about the dataset.
4.1.2	Baselines
R-GUGNN model are compared with the state-of-the-art GNN and defense models in repository
DeepRobust(Li et al., 2020), i.e., GCN(Kipf & Welling, 2017), GAT(Velickovic et al., 2018),
RGCN(Zhu et al., 2019), GCN-SVD(Entezari et al., 2020) and Pro-GNN(Jin et al., 2020b). We
adopt the default parameter settings in GCN and GAT. The number of hidden units of RGCN are
tuned from {16, 32, 64, 128}. The reduced rank of the perturbed graph in GCN-SVD is tuned from
{5, 10, 15, 50, 100, 200}. For Pro-GNN, we use the tuned hyper-parameters the author gives online.
6
Under review as a conference paper at ICLR 2022
0	1	2	3	4	5
Number of PertUrbations Per Node
-,-R-GUGNN
GCN
-A-GAT
RGCN
-X-GCN-SVD
Pro-GNN
-∙-RGCN
-X-GCN-SVD
PrO-GNN
12	3	4
Number of Perturbations Per
Number of Perturbations Per Node
0	1	2	3	4	5
Number of Perturbations Per Node
(a) Cora
(b) Citeseer
(c) Cora-ML	(d) Polblogs

Figure 2:	Node classification performance (Accuracy) under nettack

(a) Cora
0.74
0.72
0.7
0.68
0.66
0.64
0.62
0.6
0.58
0	0.2	0.4	0.6	0.8	1
Perturbation Rate
(b) Citeseer
(c) Cora-ML
0	0.2	0.4	0.6	0.8	1
Perturbation Rate
(d) Polblogs


Figure 3:	Node classification performance (Accuracy) under random attack
4.1.3 Parameter Settings
Just as(Jin et al., 2020b), for each dataset, we choose 10% of nodes for training, 10% of nodes
for validation and the remaining 80% of nodes for testing. The average performance of 10 runs is
reported for all experiments below. The hyper-parameters of all the models are tuned based on the
loss and accuracy on the validation set. Note that the same hyper-parameters are used under the same
attack for the same dataset no matter what perturbation rate is. If there are no special instructions, all
models adopt two graph convolution layers with 16 hidden units. Learning rate of Adam optimizer
η is fixed as 0.01 and negative log likelihood loss is adopted for a fair comparison(RGCN has its
own loss function).
4.2 Performance against different attacks
The node classification performance of R-GUGNN is evaluated against three types of poisoning
attacks, i.e., global attack, targeted attack and random attack. Since Ploblogs dataset has no real
node features, hyper-parameter c of R-GUGNN is set to 0 on Ploblogs dataset.
4.2.1	against Global attack
The famous metattaCk(Zugner & Gunnemann, 2019) is Used as the global attack to conduct exper-
iments and all the default parameter settings in the authors’ original implementation are adopted.
Concretely, the strongest variant Meta-Self is applied for all datasets. The perturbation rate of
metattack on the graph is from 0% to 25% with a step of 5%, since too heavy attacks are no-
ticeable and make no sense. We report the average accuracy of node classification with standard
deviation on test set and highlight the optimal results in bold. Concrete results are shown in Table 2
and we draw some conclusions:
• R-GUGNN has great improvement compared to others on four datasets. The average im-
provement of accuracy under different perturbation rates over GCN on four datasets is
about 16%, 10%, 19% and 21% respectively. When the graph is heavily perturbed, the
improvement is larger. For example, when the perturbation rate is 20%, the improvement
over GCN is about 23%, 16%, 25% and 25% on four datasets respectively. When compared
with different second best models, improvement can reach 10%, 6%, 19% and 7% on four
datasets. These results prove that R-GUGNN can defend against metattack very well.
7
Under review as a conference paper at ICLR 2022
S 2 8 8 6 4 2
8j7 7 7 7
1	. - 0 I I I I
Oo Oooo
Aue,ln84 -səl
0	0.2	0.4	0.6	0.8	1
Perturbation Rate
-■-R-GUGNN
-+-PPNP
no-β
-M-GCN
Uo-s->Qa PeJPUss
0 -----1----1----1----1-----1
0	0.2	0.4	0.6	0.8	1
Perturbation Rate
(b) Standard Deviation
0.7
(a) Accuracy
AUeJnyqs31
Hyper-Parameter:m
(a) m
Hyper-Parameter:c
Figure 4: Performance of variants of R-GUGNN
Hyper-ParameterR
(d) λ
(b) c
(c) β
Figure 5: Parameter analysis on Cora-ML dataset under 15% metattack
• Accuracy of R-GUGNN is stably high under different perturbation rates on all datasets.
The gap between accuracy of R-GUGNN on clean and perturbed graphs is small. For
example, gaps of the accuracy of R-GUGNN under 25% and 0% metattack on Cora and
Citeseer datasets are only about 7% and 2%. Besides, the overall standard deviations of
R-GUGNN are small. However, the lack of real node features on Polblogs dataset causes
big standard deviations when the graph is heavily attacked.
4.2.2	against targeted attack
The typical netttaCk(Zugner et al., 2018) is employed as the targeted attack to conduct experiments
and all the default parameter settings in the authors’ original implementation are adopted. We select
nodes with degree >10 as targeted nodes from the test set. The number of perturbations of the
graph on each targeted node is from 0 to 5 with a step of 1. We report accuracy of these targeted
nodes as results, which are shown in Figure 2. R-GUGNN suffers less effects of nettack and
also performs greatly and stably. For example, compared to the second best method Pro-GNN,
R-GUGNN achieves 10% and 5% improvement on Citeseer and Cora-ML datasets. These results
prove that R-GUGNN can defend against netttack very well.
4.2.3	against random attack
Performance of R-GUGNN under random attack is evaluated here. We add random perturbations on
the graph from 0% to 100% with a step of 20%. Concrete results are shown in Figure 3. R-GUGNN
outperforms other models again and the improvement is distinct. For example, compared to different
second best models, R-GUGNN achieves a 2.5% and 3.5% improvement on Citeseer and Cora-ML
datasets. These results prove that R-GUGNN can defend against random attack very well.
From the overall performance, we observe that the advantage of R-GUGNN is obvious compared
with others and its performance is stably great. In conclusion, R-GUGNN is robust enough to defend
against different attacks.
4.3	Ablation S tudy
R-GUGNN contains m graph reconstruction operations. Ifwe discard these operations and only use
two convolution layers for features, R-GUGNN is equal to PPNP(Klicpera et al., 2019). So, in this
8
Under review as a conference paper at ICLR 2022
subsection, we compare PPNP with GCN and R-GUGNN on Cora dataset under random attack as
an example to illustrate. In addition, we set β and c to 0 to understand the impact of each component
in the graph reconstruction operation. Furthermore, we observe standard deviations of R-GUGNN
when c=0.
In Figure(4)(a), we can see that performance of R-GUGNN is better than PPNP and performance of
GCN is the worst. It shows graph reconstruction operations are significant(R-GUGNN vs PPNP),
and adjusting neighborhood is beneficial to defending against attacks(PPNP vs GCN). PPNP curve
and the no-β curve overlap very well, which indicates that removing noises with small singular
values plays a quite important role in cleaning the graph. What’s more, if c=0, not only model’s
performance is poor, but also the standard deviation rises a lot especially when the graph is heavily
attacked in Figure(4)(b). It shows the Laplacian regularization term is significant in improving
stability of R-GUGNN, which explains why the standard deviation is big on Polblogs dataset under
heavy attack.
4.4	Parameter Analysis
In this subsection, we show performance of R-GUGNN with different values of hyper-parameters
i.e., m, c, β, and λ. We use Cora-ML dataset under 15% metattack as an example to illus-
trate. The value range of m is from 1 to 5 with the step of 1. The value of c is selected in
{10-5, 10-4, 10-3, 10-2, 10-1}. We select β from 1 to 3.5 and λ from 0.5 to 3 with the step of
0.5. In the process of tuning one hyper-parameter, other hyper-parameters are fixed as the optimal.
Figure 5 shows effects of different values of hyper-parameters.
m is the number of graph reconstruction operations. Our novel operations of R-GUGNN are impor-
tant for defending against attacks, and even one such operation improves model’s robustness(76.87%
accuracy). However, proper m can boost the accuracy and too many such operations cannot bene-
fit R-GUGNN. β is also a key affecting the performance of R-GUGNN, which controls how many
noises with small singular values to remove. When β is too small, noises cannot be removed entirely.
While when β is too big, the main properties of the graph can be hurt. λ is used to adjust the size of
nodes’ neighborhood when propagating features and choosing proper λ is also important. c is used
to control the Laplacian smoothness of the graph. We find the big value of c hurts the performance
of R-GUGNN, but when c is small, accuracy doesn’t decrease a lot. From a whole performance, all
hyper-parameters have an interval of values where the performance of R-GUGNN is stably great.
5	Conclusion
In this paper, we propose GUGNN, a novel general unified framework to effectively enhance the
robustness of GNNs against adversarial attacks by jointly cleaning the perturbed graph and denoising
the features of data. Furthermore, we extend this framework by reconstructing the graph and making
convolution operations of features with intrinsic properties, and propose a robust GNN model R-
GUGNN. Experiment results show that R-GUGNN stably outperforms the state-of-the-art baselines
under different adversarial attacks. In the future, we aim to extend this framework to other models
on graphs, even more complicated graph structures for mining the rich value underlying graph data
of various domains.
References
Aleksandar BCjchevski and StePhan Gunnemann. Deep gaussian embedding of graphs: UnsUPer-
vised inductive learning via ranking. arXiv preprint arXiv:1707.03815, 2017.
Aleksandar Bojchevski, Oleksandr Shchur, Daniel ZUgner, and Stephan GUnnemann. Netgan: Gen-
erating graphs via random walks. In ICML, 2018.
M. Defferrard, X. Bresson, and P. Vandergheynst. Convolutional neural networks on graphs with
fast localized spectral filtering. In NIPS, 2016.
D. Duvenaud, D. Maclaurin, J. Aguilera-Iparraguirre, R. GCmez-Bombarelli, Timothy D. HirzeL
Alan Aspuru-Guzik, and Ryan P. Adams. Convolutional networks on graphs for learning molec-
ular fingerprints. ArXiv, abs/1509.09292, 2015.
9
Under review as a conference paper at ICLR 2022
Negin Entezari, Saba A Al-Sayouri, Amirali Darvishzadeh, and Evangelos E Papalexakis. All you
need is low (rank) defending against adversarial attacks on graphs. In Proceedings of the 13th
International Conference on Web Search and Data Mining, pp.169-177, 2020.
C Lee Giles, Kurt D Bollacker, and Steve Lawrence. Citeseer: An automatic citation indexing
system. In Proceedings of the third ACM conference on Digital libraries, pp. 89-98, 1998.
Aditya Grover and J. Leskovec. node2vec: Scalable feature learning for networks. Proceedings
of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining,
2016.
Wei Jin, Yaxin Li, Han Xu, Yiqi Wang, Shuiwang Ji, Charu Aggarwal, and Jiliang Tang. Adver-
sarial attacks and defenses on graphs: A review, a tool and empirical studies. arXiv preprint
arXiv:2003.00653, 2020a.
Wei Jin, Yao Ma, Xiaorui Liu, Xianfeng Tang, Suhang Wang, and Jiliang Tang. Graph structure
learning for robust graph neural networks. Proceedings of the 26th ACM SIGKDD International
Conference on Knowledge Discovery & Data Mining, 2020b.
Thomas Kipf and M. Welling. Semi-supervised classification with graph convolutional networks.
ArXiv, abs/1609.02907, 2017.
Johannes Klicpera, Aleksandar Bojchevski, and Stephan Gunnemann. Predict then propagate:
Graph neural networks meet personalized pagerank. In ICLR, 2019.
Yaxin Li, Wei Jin, Han Xu, and Jiliang Tang. Deeprobust: A pytorch library for adversarial attacks
and defenses. arXiv preprint arXiv:2005.06149, 2020.
Yao Ma, Xiaorui Liu, Tong Zhao, Yozen Liu, Jiliang Tang, and Neil Shah. A unified view on graph
neural networks as graph signal denoising. arXiv preprint arXiv:2010.01777, 2020.
Andrew Kachites McCallum, Kamal Nigam, Jason Rennie, and Kristie Seymore. Automating the
construction of internet portals with machine learning. Information Retrieval, 3(2):127-163,
2000.
M. McPherson, L. Smith-Lovin, and J. Cook. Birds of a feather: Homophily in social networks.
Review of Sociology, 27:415-444, 2001.
J. Qiu, Jian Tang, Hao Ma, Yuxiao Dong, Kuansan Wang, and Jie Tang. Deepinf: Social influence
prediction with deep learning. Proceedings of the 24th ACM SIGKDD International Conference
on Knowledge Discovery & Data Mining, 2018.
Petar Velickovic, Guillem Cucurull, A. Casanova, Adriana Romero, P. Lio’, and Yoshua Bengio.
Graph attention networks. ArXiv, abs/1710.10903, 2018.
Zonghan Wu, Shirui Pan, Fengwen Chen, Guodong Long, Chengqi Zhang, and S Yu Philip. A
comprehensive survey on graph neural networks. IEEE transactions on neural networks and
learning systems, 32(1):4-24, 2020.
Han Xu, Yao Ma, Haochen Liu, Debayan Deb, H. Liu, Jiliang Tang, and Anil K. Jain. Adversarial
attacks and defenses in images, graphs and text: A review. International Journal of Automation
and Computing, 17:151-178, 2020.
Rex Ying, Ruining He, Kaifeng Chen, Pong Eksombatchai, William L. Hamilton, and J. Leskovec.
Graph convolutional neural networks for web-scale recommender systems. Proceedings of the
24th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining, 2018a.
Rex Ying, Jiaxuan You, Christopher Morris, Xiang Ren, William L. Hamilton, and J. Leskovec.
Hierarchical graph representation learning with differentiable pooling. ArXiv, abs/1806.08804,
2018b.
Ziwei Zhang, Peng Cui, and Wenwu Zhu. Deep learning on graphs: A survey. IEEE Transactions
on Knowledge and Data Engineering, 2020.
10
Under review as a conference paper at ICLR 2022
Ke Zhou, H. Zha, and Le Song. Learning social infectivity in sparse low-rank networks using multi-
dimensional hawkes processes. In AISTATS, 2013.
Dingyuan Zhu, Ziwei Zhang, Peng Cui, and Wenwu Zhu. Robust graph convolutional networks
against adversarial attacks. In Proceedings of the 25th ACM SIGKDD International Conference
on Knowledge Discovery & Data Mining, pp. 1399-1407, 2019.
Daniel Zugner and StePhan Gunnemann. Adversarial attacks on graph neural networks via meta
learning. arXiv preprint arXiv:1902.08412, 2019.
Daniel Zugner, Amir Akbarnejad, and Stephan Gunnemann. Adversarial attacks on neural networks
for graph data. In Proceedings of the 24th ACM SIGKDD International Conference on Knowledge
Discovery & Data Mining, pp. 2847-2856, 2018.
11