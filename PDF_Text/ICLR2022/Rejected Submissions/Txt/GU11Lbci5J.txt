Under review as a conference paper at ICLR 2022
Understanding AdamW through Proximal
Methods and Scale-Freeness
Anonymous authors
Paper under double-blind review
Ab stract
Adam has been widely adopted for training deep neural networks due to less
hyperparameter tuning and remarkable performance. To improve generalization,
Adam is typically used in tandem with a squared `2 regularizer (referred to as
Adam-'2). However, even better performance can be obtained with AdamW, which
decouples the gradient of the regularize] from the update rule of Adam-'2. Yet,
we are still lacking a complete explanation of the advantages of AdamW. In this
paper, we tackle this question from both an optimization and an empirical point
of view. First, we show how to re-interpret AdamW as an approximation of a
proximal gradient method, which takes advantage of the closed-form proximal
mapping of the regularizer instead of only utilizing its gradient information as in
Adam-'2. Next, we consider the property of “scale-freeness" enjoyed by AdamW
and by its proximal counterpart: their updates are invariant to component-wise
rescaling of the gradients. We provide empirical evidence across a wide range of
deep learning experiments showing a correlation between the problems in which
AdamW exhibits an advantage over Adam-'2 and the degree to which we expect the
gradients of the network to exhibit multiple scales, thus motivating the hypothesis
that the advantage of AdamW could be due to the scale-free updates.
1	Introduction
Recent years have seen a surge of interest in applying deep neural networks (LeCun et al., 2015) to a
myriad of areas. While Stochastic Gradient Descent (SGD) (Robbins & Monro, 1951) remains the
dominant method for optimizing such models, its performance depends crucially on the step size
hyperparameter which controls how far the algorithm proceeds along the negative stochastic gradient
in each step to update the model parameters. To alleviate this problem, people have developed a
fruitful line of research on adaptive gradient methods (e.g. Duchi et al., 2010a; McMahan & Streeter,
2010; Tieleman & Hinton, 2012; Zeiler, 2012; Luo et al., 2018; Zhou et al., 2018). These methods
provide mechanisms to automatically set stepsizes, and have been shown to greatly reduce the tuning
effort while maintaining good performance. Among those adaptive algorithms, one of the most
widely used is Adam (Kingma & Ba, 2015) which achieves good results across a variety of problems
even by simply adopting the default hyperparameter setting.
In practice, to improve the generalization ability, Adam is typically combined with a `2 regularization
which adds the squared `2 norm of the model weights on top of the loss function (which we will call
Adam-'2 hereafter). This technique is usually referred to as weight decay because when using SGD,
the `2 regularization works by first shrinking the model weights by a constant factor in addition to
moving along the negative gradient direction in each step. However, as pointed out in Loshchilov &
Hutter (2019), for Adam, there is no fixed regularization that achieves this same effect. To address
this, they provide a method called AdamW that decouples the gradient of the `2 regularization from
the update of Adam and directly decays the weights. The two algorithms are shown in Algorithm 1.
Although AdamW frequently outperforms Adam-'2, the approach is primarily motivated empirically
without a clear understanding of why it works so well.
Recently, however, Bjorck et al. (2020) applied AdamW in Natural Language Processing (NLP) and
Reinforcement Learning problems and found no improvement of performance over sufficiently tuned
Adam-'2. Considering the huge popularity of AdamW (KUen et al., 2019; Lifchitz et al., 2019; Carion
et al., 2020), we investigate when and why AdamW has a significant improvement over Adam-'2.
1
Under review as a conference paper at ICLR 2022
In this paper, we focus on understanding the training and testing dynamics of the AdamW update
in contrast to Adam-'2. We consider this contrast from the lens of optimization theory rather than
directly investigating generalization over multiple epochs. First, we unveil the surprising connection
between AdamW and proximal updates. In particular, we show that AdamW is an approximation of
the latter and we also confirm such similarity with an empirical study. Noticing that AdamW and
the proximal update are both scale-free while Adam-'2 is not, we derive a theorem showing that
scale-free optimizers enjoy an automatic acceleration with respect to the condition number on certain
cases. This gives AdamW a concrete theoretical advantage in training over Adam-'2.
Next, we empirically identify the scenario of training very deep neural networks with batch normal-
ization (BN) switched off as a case in which AdamW substantially outperforms Adam-'2 in both
testing and training. Note that the setting of removing BN is not our invention: indeed, there is
already active research in this (De & Smith, 2020; Zhang et al., 2019). The reason is that BN has
many disadvantages (Brock et al., 2021) including added memory overhead (Bulb et al., 2018) and
training time (Gitman & Ginsburg, 2017), and a discrepancy between training and inferencing (Singh
& Shrivastava, 2019). BN has also been found to be not suitable for many cases including distributed
computing with a small minibatch per GPU (Wu & He, 2018; Goyal et al., 2017), sequential modeling
tasks (Ba et al., 2016), and contrastive learning algorithms (Chen et al., 2020). Moreover, there are
already SOTA architectures that do not use BN including the Vision transformer (Dosovitskiy et al.,
2021) and the BERT model (Devlin et al., 2019).
For such settings of removing BN, we observe that the magnitudes of the coordinates of the updates
during training are much more concentrated about a fixed value for AdamW than for Adam-'2,
which is an expected property of scale-free algorithms. Further, as depth increases, we expect a
greater diversity of gradient scalings, a scenario that should favor scale-free updates. Our experiments
support this hypothesis: deeper networks have more dramatic differences between the distributions of
update scales between Adam-'2 and AdamW, and larger accuracy advantages for AdamW.
Specifically, the contributions of this paper are:
1.	We show that AdamW can be seen as an approximation of the proximal updates, which
utilize the closed-form proximal mapping of the regularizer instead of only its gradient.
2.	We point out the scale-freeness property enjoyed by AdamW and show the advantage of
such a property on concrete problem classes.
3.	We find a scenario where AdamW is significantly better than Adam-'2 in both training and
testing performance and report an empirical observation of the correlation between such
advantage and the scale-freeness property of AdamW.
The rest of this paper is organized as follows: In Section 2 we discuss the relevant literature. The
connection between AdamW and the proximal updates as well as its scale-freeness are explained
in Section 3. We then report the empirical observations in Section 4. Finally, we conclude with a
discussion of the results and point out some potential future directions.
2	Related Work
By enforcing the magnitude of the model weights to be small, weight decay has long been a standard
technique to improve the generalization ability in machine learning (Krogh & Hertz, 1991; Bos &
Chug, 1996) and is still widely employed in training modern deep neural networks (Devlin et al., 2019;
Tan & Le, 2019). Here, we do not attempt to explain the generalization ability of AdamW. Rather,
we assume that the regularization and the topology of the network guarantee good generalization
performance. Instead, we study algorithms from the perspective of convergence rate.
The use of proximal updates in the batch optimization literature dates back at least to 1965 (Moreau,
1965; Martinet, 1970; Rockafellar, 1976; Parikh & Boyd, 2014), and more recently used even in the
stochastic setting (Toulis & Airoldi, 2017; Asi & Duchi, 2019). We are not aware of any previous
paper pointing out the connection between AdamW and proximal updates.
The scale-free property was first proposed in the online learning field (Orabona & Pdl, 2018). There,
they do not need to know a priori the Lipschitz constant bounding the gradient norms while still able
to achieve the optimal rates. To the best of our knowledge, scale-freeness has not been explored as an
explanation for the efficiency of deep learning optimization algorithms.
2
Under review as a conference paper at ICLR 2022
Algorithm 1 Adam With L2 regularization (Adam-'2) and Adam With decoupled weight decay (AdamW)
LoShChilov & Hutter (2017) (Note that these two are exactly the same when λ = 0, namely no weight decay.)
1:	Given α, β1, β2, , λ ∈ R, {ηt}t≥0. All operations on vectors are element-wise.
2:	Initialize: xo ∈ Rd, first moment vector m° J 0, second moment vector vo J 0
3:	for t = 1, 2, . . . , T do
4:	Compute the stochastic gradient Vft(Xt-1) evaluated on a mini-batch of samples
5:	gt J Vft(Xt-1) +λxt-ι
6:	mt J βιmt-i + (1 — βι)gt, Vt J β2Vt-i + (1 — βz)g2
7:	mt J mt∕(1 — βt), Vt J Vt∕(1 — β2)
8:	Xt J Xt-1 —ηtλxt-i —ηtamt∕(√vt + e)
9:	end for
3 Theoretical Insights on the Merits of AdamW
AdamW and Proximal Updates: Here, We shoW that AdamW approximates a proximal update With
squared `2 regularization. This provides a first theoretical motivation for AdamW.
Consider that We Want to minimize the objective function
F (X) = 2 Ilxk2 + f (x),	⑴
Where λ > 0 and f(X) : Rd → R is a function bounded from beloW. We could use a stochastic
optimization algorithm that updates in the folloWing fashion
Xt = Xt-1 — Mtpt,
(2)
Where Mt is a generic matrix containing the learning rates and pt denotes the update direction.
Specifically, We consider Mt = ηtId Where ηt is a learning rate schedule, e.g., the constant one or
the cosine annealing (Loshchilov & Hutter, 2017). This update covers many cases:
1.	pt = g t gives us the vanilla SGD;
2.
pt
gt
√Pt=ι g2 + e
gives the AdaGrad algorithm (Duchi et al., 2011);
3. Pt = αm^t/(√vt + e) recovers the Adam algorithm (see Algorithm 1).
Note that in the above We use g t to denote the stochastic gradient of the entire objective function:
Vft(Xt-1) + λxt-ι (if the regularizer is not present λ = 0), with mt and Vt both updated using gt.
This update rule (2) is given by the folloWing online mirror descent update (Nemirovsky & Yudin,
1983; Warmuth & Jagota, 1997; Beck & Teboulle, 2003):
Xt = argmin 2 IIXt-1∣∣2 + f(Xt-i) + P>(X — Xt-i) + 2(X — Xt-i)>M-1(X - Xt-i) ∙
x∈Rd
This approximates minimizing a first-order Taylor approximation of F centered in Xt-1 plus a term
that measures the distance between the Xt and Xt-1 according to the matrix Mt-1. The approximation
becomes exact when Pt = Vf (Xt-1) + λXt-1.
However, this is not the only way to construct first-order updates for the objective function (1). An
alternative route is to linearize only f and to keep the squared `2 norm in its function form:
Xt = argmin 2∣∣x∣∣2 + f(Xt-i)+ P>(X — Xt-1) + 1 (X — Xt-i)>M-1(X - Xt-i),
x∈Rd
This update rule is using the proximal operator (Moreau, 1965; Parikh & Boyd, 2014) of 21∣ ∙ ∣2 with
respect to the norm ∣∣ ∙ ∣∣m-1. It is intuitive why this would be a better update: We directly minimize
the squared `2 norm instead of approximating it. From the first-order optimality condition, we have
Xt = (Id + λMt)-1(Xt-1 — MtPt) .	(3)
When λ = 0, the update in (2) and this one coincide. Yet, when λ 6= 0, they are no longer the same.
3
Under review as a conference paper at ICLR 2022
We now show how the update in (3) generalizes the one in AdamW. The update of AdamW is
Xt = (1 — ληt)xt-i — ηtαm^t/(VZvt + e) .	(4)
On the other hand, using Mt = ηtId and Pt = αm^t/(VZvt + e) in (3) gives:
Xt = (1 + ληt)-1(xt-i — ηtατn t/(pvt + e)),	(5)
which we will call AdamProx afterward. Its first-order Taylor approximation around ηt = 0 is
Xt ≈ (1 — ληt)xt-i — ηtθt^t/(√vt + e),
which is exactly the AdamW update (4) as in the original paper.
The careful reader might notice that the approximation of AdamW to AdamProx becomes less
accurate when ηt becomes too large, and so be concerned whether this approximation is practical at
all. Fortunately, in practice, ηt is never large enough for this to be an issue. Most practical learning
rate schedules, e.g., cosine, exponential, polynomial, and step decay, all decrease from η0 = 1, or
some even smaller value. Thus, the remainder term of this approximation is O(ληt2) ≤ O(λη02)
which we should always expect to be small as both λ and ηt are small. Consequently, we can expect
AdamW and AdamProx to perform similarly for learning rate schedules ηt commonly employed in
practice, and we will indeed confirm this empirically in Section 4.3.
While perhaps not widely known, the theory of proximal methods is a deep and beautiful branch of
optimization (see Parikh & Boyd (2014) for a survey). On the other hand, although AdamW is a very
popular practical algorithm (e.g., for training BERT (Devlin et al., 2019) and vision transformer-based
architectures (Dosovitskiy et al., 2021)), it is still unclear how it works so well. So, this connection
opens the door to new ways to design optimization algorithms. For example, the proximal update
gives an immediate theoretical advantage: the convergence rate, at least in the convex case, will
depend on ∣∣Vf (xt)kMt ratherthan on ∣∣Vf (xt) + λxtkMt DUchi et al.(2010b).
AdamW is Scale-Free An optimization algorithm is said to be scale-free if its iterates do not change
when one multiplies any coordinate of all the gradients by a positive constant (Orabona & Pdl, 2018).
It turns out that the update (4) of AdamW and the update (5) of AdamProx are both scale-free when
e = 0. This is evident for AdamW, since the scaling factor for any coordinate of the gradient is kept
in both mt and VZvt and will be canceled out when dividing them. In contrast, for Adam-'2, the
addition of the weight decay vector to the gradient (see Line 5 of Algorithm 1) destroys this property.
Note that in practical applications e is very small but not zero, yet we empirically verify in Section 4.2
that it is small enough to still approximately guarantee the scale-free property.
We want to emphasize the comparison between Adam-'2 and AdamW: once Adam-'2 adopts non-
zero λ, it loses the scale-freeness property; in contrast, AdamW still enjoys this for arbitrary λ. The
same applies to any AdaGrad-type and Adam-type algorithms that employ weight decay strategy in
the same way as Adam-'2 by simply adding the gradient of the '2 regularizer directly to the gradient
of f (as implemented in Tensorflow and Pytorch). Such algorithms are scale-free only when they do
not use weight decay. In fact, this is one of our main observations: scale-freeness helps optimizers in
deep learning and AdamW preserves this scale-freeness property even with an `2 regularizer.
We stress that the scale-freeness is an important but largely overlooked property of an optimization
algorithm. It has already been utilized to explain the success of AdaGrad (Orabona & Pdl, 2018).
Recently, Agarwal et al. (2020) also advocates for setting the e in the denominator of AdaGrad to be
0 thus making the update indeed scale-free, and they also provide an NLP experiment where such
choice yields the best performance.
Now, we show the effect of scale-freeness from a theoretical point of view. Consider a twice contin-
uously differentiable function f . It is well-known that the convergence rate of many optimization
algorithms, like gradient descent, on minimizing f depends on the condition number κ(V2 f (x)), i.e.,
the ratio of its largest eigenvalue to its smallest eigenvalue of the Hessian (see, e.g. Nesterov, 2004).
It turns out that scale-freeness can effectively reduce the effects of the condition number, as detailed
by the following theorem whose proof can be found in the appendix.
Theorem 1. Let f be a twice continuously differentiable function such that x* ∈ argmin f (x). Then,
let fΛ be the family of functions such that x* ∈ argmin fΛ(x), and V2fΛ(x) = ΛV2 f (x), where
Λ = diag(λ1, . . . , λd) 0. Consider a scale-free algorithm whose convergence rate can be bounded
depending on the condition number of the objective function. Then, the convergence of such algorithm
used to minimize f will only depend on the smallest condition number among all functions fΛ.
4
Under review as a conference paper at ICLR 2022
To give an example of when this is advantageous, consider when V2f (x) is a diagonal matrix:
V2f(x) = diag(gι(x),g2(x),∙∙∙,gd(x)).
Assume μ ≤ μi ≤ gi (x) ≤ Mi ≤ M for i ∈ {1,..., d}. Denote j = arg maxi Mi/μi. Choose λi
s.t. μj∙ ≤ λiμi ≤ λigi (x) ≤ λiMi ≤ Mj and we have that ΛV2f (x) has a condition number κ0 =
Mj /μj. This gives scale-free algorithms a big advantage when maxi Mi /μi《M/μ. Specifically:
O0
1
」8ZEU- E 04 8 UUSMP Z-
(a) Condition number 1.
」①ZEU- E 0-l->① uusMP Z_
10^6
IO0
10^3
IO0
IO1	IO2
Iteration
(b) Condition number 100000.
Figure 1: Non-scale-free GD
v.s. scale-free AdamW on
Corollary 1. For quadratic problems f (X) = 1 x> Hx + b>x + c,
with H diagonal and positive definite, any scale-free algorithm
will not differentiate between minimizing f and f(x) = 1 x>X +
H-1 b> x + c. As the condition number of f is 1, the operation, and
most importantly, the convergence, of a scale-free algorithm will not
be affected by the condition number of f at all.
Figure 1 illustrates Corollary 1. Here, we compare GD (non-scale-
free) with AdamW (scale-free) on optimizing two quadratic func-
tions with the same minimizer, but one hessian matrix being a
rescaled version of the other, resulting in different condition num-
bers. Even after tuning the learning rates, the figure clearly shows
that the updates of AdamW (starting from the same point) and thus
its convergence to the minimizer, is completely unaffected by the
condition number, while GD’s updates change drastically and its
performance deteriorates significantly when the condition number is
large. Moreover, note that poor training performance mostly likely
will also lead to poor testing performance.
quadratic functions with differ- This also explains AdaGrad’s improvements over SGD in certain
ent condition numbers.	scenarios. As an example, in Appendix B we analyze a variant
of AdaGrad with restarts and show an improved convergence on
strongly convex functions due to scale-freeness. Note that the folklore justification for such improve-
ments is that the learning rate of AdaGrad approximates the inverse of the Hessian matrix, but this
is incorrect: AdaGrad does not compute Hessians and there is no reason to believe it approximates
them in general.
More importantly, another scenario demonstrating the advantage of scale-freeness is training deep neu-
ral networks. Neural networks are known to suffer from the notorious problem of vanishing/exploding
gradients (Bengio et al., 1994; Glorot & Bengio, 2010; Pascanu et al., 2013). This problem leads to
the gradient scales being very different across layers, especially between the first and the last layers.
The problem is particularly severe when the model is not equipped with normalization mechanisms
like Batch Normalization (BN) (Ioffe & Szegedy, 2015). In such cases, when using a non-scale-free
optimization algorithm (e.g. SGD), the first layers and the last layers will proceed at very different
speeds, whereas a scale-free algorithm ensures that each layer is updated at a similar pace.
4	Empirical Findings of AdamW on Deep Learning Experiments
In this section, We empirically compare Adam-'2 With AdamW. First (Section 4.1), We report
experiments for deep neural networks on image classification tasks (CIFAR10/100). Here, AdamW
enjoys a significant advantage over Adam-'2 when the batch normalization is switched off on deeper
neural netWorks. We also report the correlation betWeen this advantage and the scale-freeness property
of AdamW. Next (Section 4.2), We shoW that AdamW is still almost scale-free even When the used
in practice is not 0, and how, contrary to AdamW, Adam-'2 is not invariant of the loss function being
multiplied by a positive constant. Finally (Section 4.3), We shoW that AdamW performs similarly to
AdamProx in practice, thus supporting the observations in Section 3.
Data Normalization and Augmentation: We consider the image classification task on CIFAR-
10/100 datasets. Images are normalized per channel using the means and standard deviations computed
from all training images. We adopt the data augmentation technique following Lee et al. (2015) (for
training only): 4 pixels are padded on each side of an image and a 32 × 32 crop is randomly sampled
from the padded image or its horizontal flip.
5
Under review as a conference paper at ICLR 2022
(I∞∙0*) 3Z 一 Sdss -e=u_
(IOo∙0*) ©z 一 Sd3sra≡Ξ
(b) 56 Layer Resnet on CIFAR10
(a) 20 Layer Resnet on CIFAR10
O 0.010.050.1 0.5 1 5
Weight decay (*0.001)
Weightdecay (*O.OO1)
(d) 218 Layer Resnet on CIFAR10
(c) 110 Layer Resnet on CIFAR10
(IOQ。*SNSdalSra≡lη
5 15 15 0
0.060.1
Iood*WZ 一 Sd 旧 S -BEU 一
Weight decay (*0.001)

(e) 100 layer DenseNet-BC on CIFAR100
Figure 2: The final Top-1 test error on using AdamW vs. Adam-'2 on training a ReSnet/DenseNet
with Batch Normalization on CIFAR10/100 (where the black circle denotes the best setting).
Models: For the CIFAR-10 dataset, we employ the Residual Network1 model (He et al., 2016) of
20/44/56/110/218 layers; and for CIFAR-100, we utilize the DenseNet-BC2 model (Huang et al.,
2017) with 100 layers and a growth-rate of 12. The loss is the cross-entropy loss.
Hyperparameter tuning: For both Adam-'2 and AdamW, We set β1 = 0.9, β2 = 0.999, e = 10-8
as suggested in the original Adam paper Kingma & Ba (2015). To set the initial step size α and
Weight decay parameter λ, We grid search over {0.00005, 0.0001, 0.0005, 0.001, 0.005} for α and
{0, 0.00001, 0.00005, 0.0001, 0.0005, 0.001} for λ. Whenever the best performing hyperparameters
lie in the boundary of the searching grid, We alWays extend the grid to ensure that the final best-
performing hyperparameters fall into the interior of the grid.
Training: For each experiment configuration (e.g. 110-layer Resnet batch normalization), We
randomly select an initialization of the model to use as a fixed starting point for all optimizers and
hyperparameter settings. We use a mini-batch of 128, and train 300 epochs unless otherWise specified.
4.1	AdamW vs. Adam-'2 ON Image Classification Tasks, the Influence of Batch
Normalization, and the Correlation with Scale-freeness
With BN, Adam-'2 is on par with AdamW: Recently, BjorCk et al. (2020) found that AdamW has
no improvement in absolute performance over sufficiently tuned Adam-'2 in some reinforcement
learning experiments. We also discover the same phenomenon in several image classification tasks,
see Figure 2. indeed, the best Weight decay parameter is 0 for all cases and AdamW coincides With
Adam-'2 in these cases. Nevertheless, AdamW does decouple the optimal choice of the weight decay
parameter from the initial step size much better than Adam-'2 in all cases.
Removing BN: Notice that the models used in Figure 2 all employ Batch Normalization (BN) (ioffe &
szegedy, 2015). BN works by normalizing the input to each layer across the mini-batch to make each
1https://github.com/akamaster/pytorch_resnet_cifar10
2https://github.com/bearpaw/pytorch-classification
6
Under review as a conference paper at ICLR 2022
(a) 20 Layer Resnet on CIFAR10
(b) 44 Layer Resnet on CIFAR10
(c) 56 Layer Resnet on CIFAR10
(f) 100 layer DenseNet-BC on CIFAR100
Figure 3: On using AdamW vs. Adam-'2 on training a ReSnet/DenseNet without Batch Normalization
on CIFAR10/100. (Left two) The final Top-1 test error (where the black circle denotes the best
setting). (Middle two) The training loss and test accuracy curve when employing the initial step size
and the weight decay parameter that gives the smallest test error. (Right two) The histogram of the
magnitude of corresponding updates of all coordinates of the network near the end of the training
when employing the initial step size and the weight decay parameter that gives the smallest test error.
Order of magnitude (power of 2)
coordinate have zero-mean and unit-variance. Without BN, deep neural networks are known to suffer
from gradient explosion and vanishing (Schoenholz et al., 2017). This means each coordinate of the
gradient will have very different scales, especially between the first and last layers. For non-scale-free
algorithms, the update to the network weights will also be affected and each coordinate will proceed
at a different pace. In contrast, scale-free optimizers are robust to such issues as the scaling of any
single coordinate will not affect the update. Also, we already mentioned in the introduction some
other reasons for considering removing BN. Thus, we consider the case where BN is removed as that
is where AdamW and Adam-'2 will show very different patterns due to SCale-freeness.
Without BN, AdamW Outperforms Adam-'2: In fact, without BN, AdamW outperforms Adam-'2
even when both are finely tuned, especially on relatively deep neural networks (see Figure 3). AdamW
not only obtains a much better test accuracy but also trains much faster.
Correlation between AdamW’s Advantage and Scale-freeness: We also observe that the advantage
of AdamW becomes more evident as the network becomes deeper. Recall that as the depth grows,
without BN, the gradient explosion and vanishing problem will become more severe. This means that
for the non-scale-free Adam-'2, the updates of each coordinate will be dispersed on a wider range of
7
Under review as a conference paper at ICLR 2022
(a) Loss multiplied by 10
Figure 4: The final top-1 test error of AdamW vs. Adam-'2 on optimizing a 110-layer ReSnet With
batch normalization removed optimized by AdamW or Adam with the loss function multiplied by 10
(left tWo figures) and 100 (right tWo figures).
Weight decay (*0.001)
(b) Loss multiplied by 100
scales even When the same Weight decay parameter is employed. In contrast, the scales of the updates
of AdamW Will be much more concentrated in a smaller range. This is exactly verified empirically as
illustrated in the 5th & 6th columns of figures in Figure 3. There, We report the histograms of the
absolute value of updates of Adam-'2 vs. AdamW of all coordinates near the end of training (for their
comparison during the Whole training process please refer to the Appendix).
This correlation between the advantage of AdamW over Adam-'2 and the different spread of cor-
responding update scales Which is induced by the scale-freeness property of AdamW provides an
empirical justification on when and why AdamW excels over Adam-'2.
SGD and Scale-freeness: The reader might wonder why SGD is known to provide state-of-the-art
performance on many deep learning architectures (e.g., He et al., 2016; Huang et al., 2017) without
being scale-free. At first blush, this seems to contradict our claims that scale-freeness correlates with
good performance. In reality, the good performance of SGD in very deep models is linked to the use
of BN that normalizes the gradients. Indeed, we verified empirically that SGD fails spectacularly
when BN is not used. For example, on training the 110 layer Resnet without BN using SGD with
momentum and weight decay of 0.0001, even a learning rate of 1e - 10 will lead to divergence.
4.2	Verifying Scale-Freeness: AdamW vs. Adam-'2 under Loss Multiplication
In the previous section, we elaborated on the scale-freeness property of AdamW and its correlation
with AdamW,s advantage over Adam-'2. However, one may notice that in practice, the e factor in
the AdamW update is typically small but not 0, in our case 1e-8, thus preventing it from completely
scale-free. in this section, we verify that the effect of such an on the scale-freeness is negligible.
As a simple empirical verification of the scale-freeness, we consider the scenario where we multiply
the loss function by a positive number. Note that any other method to test scale-freeness would be
equally good. For a feed-forward neural network without BN, this means the gradient would also be
scaled up by that factor. in this case, the updates of a scale-free optimization algorithm would remain
exactly the same, whereas they would change for an optimization algorithm that is not scale-free.
Figure 4 shows results of the loss function being multiplied by 10 and 100 respectively on optimizing
a 110-layer Resnet with BN removed. For results of the original loss see Figure 3d. We can see
that AdamW has almost the same performance across the range of initial learning rates and weight
decay parameters, and most importantly, the best values of these two hyperparameters remain the
same. This verifies that, even when employing a (small) non-zero , AdamW is still approximately
scale-free. In contrast, Adam-'2 is not scale-free and we can see that its behavior varies drastically
with the best initial learning rates and weight decay parameters in each setting totally different.
4.3	AdamW and AdamProx Behave very Similarly in Practice
in section 3, we showed theoretically that AdamW is the first order Taylor approximation of
AdamProx (update rule (5)). Beyond this theoretical argument, we also verified empirically that
the approximation is good. in Figure 5, we consider the case when ηt = 1 for all t - a relatively
large constant learning rate schedule. in such cases, AdamW and AdamProx still behave very
similarly. This suggests that for most learning rate schedules, e.g., cosine, exponential, polynomial,
and step decay, which all monotonously decrease from η0 = 1, AdamProx will remain a very
good approximation to AdamW. Thus, it is reasonable to try to understand AdamW by instead
understanding the more classically-linked AdamProx.
8
Under review as a conference paper at ICLR 2022
(a) ResNet on CIFAR-10
(b) DenseNet-BC on CIFAR-100
Figure 5: The final Top-1 test error of using AdamW vs. AdamProx on training (where the black circle
denotes the best setting). (Top row) a 110-layer ResNet with Batch-normalization removed on CIFAR-
10 (trained for 300 epochs). (Bottom row) a 100-layer DenseNet-BC with Batch-normalization
removed on CIFAR-100 (trained for 100 epochs).
5 Conclusion and Future Works
In this paper, we provide explanations for the merits of AdamW from two points of view. We first
show that AdamW is an approximation of the proximal updates both theoretically and empirically.
We then identify the setting of training very deep neural networks without batch normalization
in which AdamW substantially outperforms Adam-'2 in both training and testing, and provide an
empirical explanation through the scale-freeness property of AdamW. Nevertheless, there are still
many directions worth exploring and we name a few here.
Update for no-square `2 regularization. In-
stead of using the squared `2 regularization, we
might think to use the `2 regularization, that
is without the square. This is known to have
better statistical properties than the squared `2
(Orabona, 2014), but it is not smooth so it is
harder to be optimized. However, with proxi-
mal updates, we don’t have to worry about its
non-smoothness. Hence, we can consider the
objective function
F(x) =λkxk2+f(x) .
Figure 6: The final Top-1 test error of using
AdamW vs. AdamProxL2 to train a 110-layer
ResNet with BN removed on CIFAR10 (where the
black circle denotes the best setting).
The corresponding prox-SGD update was derived in Duchi & Singer (2009) for scalar learning rates
and it is easy to see that it generalizes to our setting for Mt = ηt Id as
xt+1 = max (1 - Ilxt-ηPtIl, 0) (Xt - ηtPt).
Its performance, named AdamProxL2, as shown in Figure 6, can be on a par with AdamW.
Update for `1 regularization. The `1 regularization is widely used for achieving the sparsity of the
solution. Recently, Neyshabur (2020) proposed β-lasso, a variant of the LASSO algorithm or namely
regression with `1 regularization, and showed significant improvements on training fully-connected
networks. Thus, we are curious to see how the variant of AdamW derived from using `1 regularization
instead will behave. Specifically, we consider the objective function:
F(x) =λkxk1+f(x),
for which the proximal update is obtained by solving:
argmin 2 [(x - (Xt - MtPt)]TM- [x - (Xt - Mtpt)] + λ∣∣x∣∣ι .
x
When Mt is a diagonal matrix with positive eigenvalues, the above objective function is decomposable
and we can solve for each dimension. The solution for 1-d case was derived in Duchi & Singer (2009)
and it is easy to see that it generalizes to our setting as
Xt+1,i = sign(Xt - Mtpt)imax(|Xt - Mtpt|i - λMt,i,0) .
Distributed Training As already noted in Section 4.1, batch normalization is not very compatible
with distributed training. Since AdamW outperforms Adam-'2 significantly in settings with BN
switched off, at least in feed-forward neural networks, we can apply AdamW in distributed training
to see if it still enjoys the same merits.
9
Under review as a conference paper at ICLR 2022
References
N. Agarwal, R. Anil, E. Hazan, T. Koren, and C. Zhang. Disentangling adaptive gradient methods
from learning rates. arXiv preprint arXiv:2002.11803, 2020.
H. Asi and J. C. Duchi. Stochastic (approximate) proximal point methods: Convergence, optimality,
and adaptivity. SIAM Journal on Optimization, 29(3):2257-2290, 2019.
Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint
arXiv:1607.06450, 2016.
A. Beck and M. Teboulle. Mirror descent and nonlinear projected subgradient methods for convex
optimization. Operations Research Letters, 31(3):167-175, 2003.
Y. Bengio, P. Simard, and P. Frasconi. Learning long-term dependencies with gradient descent is
difficult. IEEE transactions on neural networks, 5(2):157-166, 1994.
J. Bjorck, K. Q. Weinberger, and C. P. Gomes. Understanding decoupled and early weight decay.
ArXiv, abs/2012.13841, 2020.
S.	Bos and E. Chug. Using weight decay to optimize the generalization ability of a perceptron.
Proceedings of International Conference on Neural Networks (ICNN’96), 1:241-246 vol.1, 1996.
Andrew Brock, Soham De, Samuel L. Smith, and Karen Simonyan. High-performance large-scale
image recognition without normalization. In Proc. of the International Conference on Machine
Learning (ICML), pp. 1059-1071, 2021.
Samuel Rota Bulo, Lorenzo Porzi, and Peter Kontschieder. In-place activated batchnorm for memory-
optimized training of dnns. In Proceedings of the IEEE Conference on Computer Vision and
Pattern Recognition, pp. 5639-5647, 2018.
N. Carion, F. Massa, G. Synnaeve, N. Usunier, A. Kirillov, and S. Zagoruyko. End-to-end object
detection with transformers. In European Conference on Computer Vision, pp. 213-229. Springer,
2020.
T.	Chen, S. Kornblith, M. Norouzi, and G. Hinton. A simple framework for contrastive learning
of visual representations. In Hal DaUma III and Aarti Singh (eds.), Proceedings of the 37th
International Conference on Machine Learning, volume 119 of Proceedings of Machine Learning
Research, pp. 1597-1607. PMLR, 13-18 Jul 2020.
Soham De and Sam Smith. Batch normalization biases residual blocks towards the identity function
in deep networks. Advances in Neural Information Processing Systems, 33, 2020.
J.	Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of deep
bidirectional transformers for language understanding. In NAACL-HLT, 2019.
A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai, T. Unterthiner, M. Dehghani,
M. Minderer, G. Heigold, S. Gelly, J. Uszkoreit, and N. Houlsby. An image is worth 16x16
words: Transformers for image recognition at scale. In 9th International Conference on Learning
Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021. OpenReview.net, 2021.
J. Duchi and Y. Singer. Efficient online and batch learning using forward backward splitting. The
Journal of Machine Learning Research, 10:2899-2934, 2009.
J. Duchi, E. Hazan, and Y. Singer. Adaptive subgradient methods for online learning and stochastic
optimization. In COLT, 2010a.
J. C. Duchi, S. Shalev-Shwartz, Y. Singer, and A. Tewari. Composite objective mirror descent. In
COLT, pp. 14-26, 2010b.
J. C. Duchi, E. Hazan, and Y. Singer. Adaptive subgradient methods for online learning and stochastic
optimization. Journal of Machine Learning Research, 12:2121-2159, 2011.
Igor Gitman and Boris Ginsburg. Comparison of batch normalization and weight normalization
algorithms for the large-scale image classification. arXiv preprint arXiv:1709.08145, 2017.
10
Under review as a conference paper at ICLR 2022
X. Glorot and Y. Bengio. Understanding the difficulty of training deep feedforward neural networks.
In Proc. of the thirteenth international conference on artificial intelligence and statistics, pp.
249-256. JmLR Workshop and Conference Proceedings, 2010.
P. Goyal, P. Doll犯 R. B. Girshick, P. Noordhuis, L. Wesolowski, A. Kyrola, A. Tulloch, Y. Jia, and
K. He. Accurate, large minibatch SGD: Training ImageNet in 1 hour. ArXiv, abs/1706.02677,
2017.
K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning for image recognition. In Proc. of the
IEEE conference on computer vision and pattern recognition, pp. 770-778, 2016.
G. Huang, Z. Liu, L. Van Der Maaten, and K. Q. Weinberger. Densely connected convolutional
networks. In Proceedings of the IEEE conference on Computer Vision and Pattern Recognition, pp.
4700-4708, 2017.
S. Ioffe and C. Szegedy. Batch normalization: Accelerating deep network training by reducing
internal covariate shift. In International conference on machine learning, pp. 448-456. PMLR,
2015.
D. P. Kingma and J. Ba. Adam: A method for stochastic optimization. In International Conference
on Learning Representations (ICLR), 2015.
A. Krogh and J. Hertz. A simple weight decay can improve generalization. In NIPS, 1991.
J. Kuen, F. Perazzi, Z. Lin, J. Zhang, and Y.-P. Tan. Scaling object detection by transferring
classification weights. In Proc. of the IEEE/CVF International Conference on Computer Vision,
pp. 6044-6053, 2019.
Y. LeCun, Y. Bengio, and G. Hinton. Deep learning. Nature, 521(7553):436-444, 2015.
C. Lee, S. Xie, P. Gallagher, Z. Zhang, and Z. Tu. Deeply-supervised nets. In Proc. of the Eighteenth
International Conference on Artificial Intelligence and Statistics, volume 38, pp. 562-570. PMLR,
2015.
Y. Lifchitz, Y. Avrithis, S. Picard, and A. Bursuc. Dense classification and implanting for few-shot
learning. In Proc. of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp.
9258-9267, 2019.
I. Loshchilov and F. Hutter. SGDR: Stochastic gradient descent with warm restarts. In International
Conference on Learning Representations, 2017.
I. Loshchilov and F. Hutter. Decoupled weight decay regularization. In International Conference on
Learning Representations, 2019.
L. Luo, Y. Xiong, Y. Liu, and X. Sun. Adaptive gradient methods with dynamic bound of learning
rate. In International Conference on Learning Representations, 2018.
B. Martinet. RegUIariSatiOn d'in6qUatiOnS Variationnelles par approximations successives. rev.
frangaise informat. Recherche Operationnelle, 4:154-158, 1970.
H. B. McMahan and M. J. Streeter. Adaptive bound optimization for online convex optimization. In
COLT, 2010.
J.-J. Moreau. Proximite et dualite dans un espace hilbertien. Bulletin de la Societe mathematique de
France, 93:273-299, 1965.
A.	S. Nemirovsky and D. Yudin. Problem complexity and method efficiency in optimization. Wiley,
New York, NY, USA, 1983.
Y. Nesterov. Introductory lectures on convex optimization: A basic course, volume 87. Springer,
2004.
B.	Neyshabur. Towards learning convolutions from scratch. In H. Larochelle, M. Ranzato, R. Hadsell,
M. F. Balcan, and H. Lin (eds.), Advances in Neural Information Processing Systems, volume 33,
pp. 8078-8088. Curran Associates, Inc., 2020.
11
Under review as a conference paper at ICLR 2022
F. Orabona. Simultaneous model selection and optimization through parameter-free stochastic
learning. In Advances in Neural Information Processing Systems 27, 2014.
F. Orabona and D. PdL Scale-free online learning. Theoretical Computer Science, 716:50-69, 2018.
Special Issue on ALT 2015.
N. Parikh and S. Boyd. Proximal algorithms. Foundations and Trends in optimization, 1(3):127-239,
2014.
R. Pascanu, T. Mikolov, and Y. Bengio. On the difficulty of training recurrent neural networks. In
International conference on machine learning, pp. 1310-1318. PMLR, 2013.
H. Robbins and S. Monro. A stochastic approximation method. Annals of Mathematical Statistics,
22:400-407, 1951.
R.	T. Rockafellar. Monotone operators and the proximal point algorithm. SIAM journal on control
and optimization, 14(5):877-898, 1976.
S.	Schoenholz, J. Gilmer, S. Ganguli, and Jascha Sohl-Dickstein. Deep information propagation. In
International Conference on Learning Representations (ICLR), 2017.
Saurabh Singh and Abhinav Shrivastava. Evalnorm: Estimating batch normalization statistics for
evaluation. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp.
3633-3641, 2019.
M. Tan and Quoc V. Le. EfficientNet: Rethinking model scaling for convolutional neural networks.
ArXiv, abs/1905.11946, 2019.
T.	Tieleman and G. Hinton. Lecture 6.5-rmsprop: Divide the gradient by a running average of its
recent magnitude. COURSERA: Neural Networks for Machine Learning, 2012.
P. Toulis and E. M. Airoldi. Asymptotic and finite-sample properties of estimators based on stochastic
gradients. The Annals of Statistics, 45(4):1694-1727, 2017.
M. K. Warmuth and A. K. Jagota. Continuous and discrete-time nonlinear gradient descent: Relative
loss bounds and convergence. In Electronic proceedings of the 5th International Symposium on
Artificial Intelligence and Mathematics, 1997.
Yuxin Wu and Kaiming He. Group normalization. In Proceedings of the European conference on
computer vision (ECCV), pp. 3-19, 2018.
M. D. Zeiler. ADADELTA: an adaptive learning rate method. arXiv preprint arXiv:1212.5701, 2012.
H. Zhang, Y. N. Dauphin, and T. Ma. Fixup initialization: Residual learning without normalization.
In International Conference on Learning Representations, 2019.
D. Zhou, Y. Tang, Z. Yang, Y. Cao, and Q. Gu. On the convergence of adaptive gradient methods for
nonconvex optimization, 2018. arXiv:1808.05671.
12
Under review as a conference paper at ICLR 2022
Appendices
A Proof of Theorem 1
Proof. From the Fundamental Theorem of Calculus we have:
▽f(X) = ▽/(/*)+ Z V2f(x* + t(x — x*))(x — x*)dt = Z V2f(x* + t(x — x*))(x — x*)dt .
00
一	_	_	≈	,	.	. _C ʌ ,	.	_ __ ≈	,	_	__ ≈	,	.	. __ -,	.
Thus, for any function 〃(x) whose Hessian is ΛV2f (x) and VfA(X*) = 0, V〃(x) = ΛVf (x).
Now, from the definition of a scale-free algorithm, the iterates of such an algorithm do not change
when one multiplies any coordinate of all the gradients by a positive constant. Thus, a scale-free
algorithm optimizing f behaves the same as if it is optimizing 〃.	□
B A Scale-free Algorithm with Dependency on the Condition
Number
Algorithm 2 AdaGrad (Duchi et al., 2011; McMahan & Streeter, 2010) (All operations on vectors
are element-wise.)
Input: #Iterations T , a set K, x1 ∈ K, stepsize η
for t = 1 . . . T do
Receive: Vf(xt)
Set: ηt = / + η
建	√∑i=ι(vf (Xi))2
Update: xt+1 = ΠK (xt — ηtVf(xt)) where ΠK is the projection onto K.
end for
Output: X = T P= Xt.
Algorithm 3 AdaGrad with Restart
Input: #ROUndS N, xo ∈ Rd, upper bound on ∣∣xo — x* k∞ as D∞, strong convexity μ, smoothness M
Set: Xo = xo
for i = 1... N do	2
Run Algorithm 2 to get X i with T = 32d M, xι = X i-i, K = {x : ∣x-x i-ι∣∞ ≤ D∞ }, η = D∞-12
end for
Output: X N.
Theorem 2.	Let K be a hypercube with kX — y k∞ ≤ D∞ for any X, y ∈ K. For a convex function
f, set n = D√∞∞, then Algorithm 2 guarantees for any X ∈ K:
T
X f(xt) — f(x)
t=1
≤t
2dD∞ X ∣Vf(xt)∣2 .
t=1
(6)
Theorem 3.	For a μ StrOngIy convex and M smooth function f, denote its unique minimizer as
X* ∈ Rd Given Xo ∈ Rd, assume that ∣∣Xo 一 x* k∞ ≤ D∞, then Algorithm 3 guarantees:
D2
∣XN - X*k∞ ≤ D∞ .
Thus, to get a X such that ∣∣x — x* ∣∞ ≤ G we need at most 32dM log4 (D∞/e) gradient calls.
ProofofTheorem 3. Consider round i and assume K passed to Algorithm 2 is bounded w.r.t. '∞
norm by D∞i. When f is μ-strongly convex and M smooth, let X = x*, Equation (6) becomes:
T
X f(xt) — f(x*) ≤
t=1
T
2dD∞2 i X	∣Vf(xt)∣2
t=1
≤t
T
4MdD∞2 i X(f(xt) — f(x*))
t=1
∖
13
Under review as a conference paper at ICLR 2022
where the second inequality is by the M smoothness of f. This gives:
T
X f(xt) - f(x*) ≤ 4MdD∞i .
t=1
Let X i 二 T PT=1 Xt We have by the μ-strong-convexity that:
2	2 1 T	8M dD2
kxi- X ιι∞ ≤ kxi- x k ≤ μCf(X) - f(X )) ≤ μτXf(Xt) - f(X )) ≤ —μT i.	⑺
D2
Put T = 32dMμ in Equation (7) we have that kXi 一 x* k∞ ≤ -ɪ. Thus, after each round, the '∞
distance between the update Xi and x* is shrinked by half, which in turn ensures that x* is still inside
the K passed to Algorithm 2 in the next round with D∞i+1
D∞i. This concludes the proof.	口
Proof of Theorem 2.
T
X f(Xt) - f(X)
t=1
T
≤ XhVf (Xt), Xt - Xi
t=1
T d ∂f
=XX ∂xl (χt)* (Xtj- Xj)
t=1 j =1 Xt,j
Td
=Xt=1Xj=1
Td
≤Xt=1Xj=1
dT
≤Xj=1Xt=1
(Xtj- Xj)2 - (Xtj- ηt,j∂xfj(Xt) - Xj)
2ηt,j
Td
+X X等
(xt)
≤ D∞∞
2η
(Xtj - Xj )2 - (Xt+1,j - Xj)
2ηt,j
t-1
2
i1
T
X
t=1
jjɪ (卷-η⅛)+X X
2	ηt,j	ηt-1,j j =1 t=1
2 T d	2
+X X 等(⅛ (Xt))
—
-
2
dT
X
j=1 t=1
XX X (u⅛M
≤ X D∞u
≤ j=1 的 t
X∖2D∞ X (<(Xt))
2 E=I(悬(Xi))2
(xt)
d
j
j
T
2
η
≤t
Td	2
2dD∞XX (西(Xt))
t=1 j =1 t,j
∖
T
2dD∞ X kV∕(xt))k2 .
t=1
where the first inequality is by convexity, the second one by the projection lemma as the projection
onto a hypercube equals performing the projection independently for each coordinate, the fifth one
by Lemma 5 in (McMahan & Streeter, 2010), and the last one by the concavity of √∙.
□
14
Under review as a conference paper at ICLR 2022
C The Histograms of the Magnitude of each Update Coordinate
during the Entire Training Phase
In this section, We report the histograms of the absolute value of updates of Adam-'2 vs. AdamW of
all coordinates divided by α during the whole training process. From the figures shown below, we
can clearly see that AdamW,s updates remain in a much more concentrated scale range than Adam-'2
during the entire training. Moreover, as the depth of the network grows, Adam-%'s updates become
more and more dispersed, While AdamW’s updates are still concentrated. (Note that the leftmost bin
contains all values equal or less than 2-27 ≈ 10-8.1 and the rightmost bin contains all values equal
to or larger than 1.)
15
Under review as a conference paper at ICLR 2022
Figure 7: The histograms of the magnitudes of all updates (without α) of a 20-layer Resnet with BN
removed trained by AdamW or Adam-'2 on CIFAR10.
16
Under review as a conference paper at ICLR 2022
Figure 8: The histograms of the magnitudes of all updates (without α) of a 44-layer Resnet with BN
removed trained by AdamW or Adam-'2 on CIFAR10.
17
Under review as a conference paper at ICLR 2022
(a) Adam-'2
Figure 9: The histograms of the magnitudes of all updates (without α) of a 56-layer Resnet with BN
removed trained by AdamW or Adam-'2 on CIFAR10.
18
Under review as a conference paper at ICLR 2022
(a) Adam-'2
Figure 10: The histograms of the magnitudes of all updates (without α) of a 110-layer Resnet with
BN removed trained by AdamW or Adam-'2 on CIFAR10.
19
Under review as a conference paper at ICLR 2022
Adam EPOCh 1
0 0 -2 7-2Φ21-18-15-12-9 -6 -3 0
Adam EPOCh IOo
∩ ∩ J~~I-■~~I----1 1 1 n-
--27-2Φ21-18-15-12-9 -6 -3 0
Adam EPOCh 200
0 OLl__^≡≡fl∣V∣l∣∣l*Mwu^,——r-
--27-2Φ21-18-15-12-9 -6 -3 0
Adam EPoCh 300
0 0 L∣≡≡^m<≡≡^m^*≡^≡≡^*≡^≡≡^__
-27-2421-18=l⅛12-9 -6 -3 0
Order of magnitude (power of 2)
(a) Adam-'2
0.0
Figure 11: The histograms of the magnitudes of all updates (without α) of a 218-layer Resnet with
BN removed trained by AdamW or Adam-'2 on CIFAR10.
1.0
0.0
1.0
0.0
1.0
0.0
1.0
0.8
0.6
0.4
0.2
0.8
0.6
0.4
0.2
0.8
0.6
0.4
0.2
0.8
0.6
0.4
0.2
AdamW Epoch 1		
	,I	L
-27-24-21-18-15-12 -9 -6 -3 AdamW Epoch 100		0
	L	
-27-24-21-18-15-12 -9 -6 -3 0 AdamW Epoch 200		
		hb∣iI∣.	
-27-24-21-18-15-12 -9 -6 -3 0 	AddmW EPnCh 300			
		
-27-24-21-18-15-12 -9 -6 -3 0
Order of magnitude (power of 2)
(b) AdamW
20
Under review as a conference paper at ICLR 2022
Figure 12: The histograms of the magnitudes of all updates (without α) of a 100-layer DenseNet-BC
with BN removed trained by AdamW or Adam-'2 on CIFAR100.
21