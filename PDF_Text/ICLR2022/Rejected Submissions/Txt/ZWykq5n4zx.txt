Under review as a conference paper at ICLR 2022
Boosting the Confidence of Near-Tight Gen-
eralization Bounds for Uniformly Stable
Randomized Algorithms
Anonymous authors
Paper under double-blind review
Ab stract
High probability generalization bounds of uniformly stable learning algorithms
have recently been actively studied with a series of near-tight results established
by Feldman & Vondrak (2019); Bousquet et al. (2020). However, for randomized
algorithms with on-average uniform stability, such as stochastic gradient descent
(SGD) with time decaying learning rates, it still remains less well understood if
these deviation bounds still hold with high confidence over the internal random-
ness of algorithm. This paper addresses this open question and makes progress to-
wards answering it inside a classic framework of confidence-boosting. To this end,
we first establish an in-expectation first moment generalization error bound for
randomized learning algorithm with on-average uniform stability, based on which
we then show that a properly designed subbagging process leads to near-tight high
probability generalization bounds over the randomness of data and algorithm. We
further substantialize these generic results to SGD to derive improved high prob-
ability generalization bounds for convex or non-convex optimization with natural
time decaying learning rates, which have not been possible to prove with the ex-
isting uniform stability results. Specially for deterministic uniformly stable algo-
rithms, our confidence-boosting results improve upon the best known generaliza-
tion bounds in terms of a logarithmic factor on sample size, which moves a step
forward towards resolving an open question raised by Bousquet et al. (2020).
1 Introduction
In many statistical machine learning problems, the ultimate goal is to design a suitable algorithm
A : ZN 7→ W that maps a training data set S = {zi}i∈[N] ∈ ZN to a model A(S) in a closed subset
W of an Euclidean space such that the following population risk function evaluated at the model is
as small as possible:
R(A(S)):= EZ〜D['(A(S); Z)],
where ` : W × Z 7→ R+ is a non-negative bounded loss function whose value `(w; z) measures
the loss evaluated at z with parameter w , and D represents a distribution over Z . It is generally the
case that the underlying data distribution is unknown, and in this case the training data is usually
assumed to be an i.i.d. set, i.e., S i±' DN. Then, a natural alternative measurement that mimics the
computationally intractable population risk is the empirical risk defined by
1N
RS(A(S)) := EZ〜Unif(S)['(A(S); Z)] = NN E'(A(S); zi).
N i=1
The bound on the difference between population and empirical risks is of central interest in under-
standing the generalization performance of learning algorithm A. Particularly, we hope to derive
a suitable law of large numbers, i.e., a sample size vanishing rate bN such that the generalization
bound |RS(A(S)) - R(A(S))| . bN holds with high probability over the randomness of S and po-
tentially the randomness of A as well. Provided that A(S) is an almost minimizer of the empirical
risk function RS, say RS(A(S)) . minw∈W RS (w) + bN, the generalization bound immediately
implies an excess risk bound R(A(S)) - mi□w∈w R(W) . bN + √= due to the standard risk de-
composition and Hoeffding’s inequality. Therefore, generalization bounds also play a crucial role in
understanding the stochastic optimization performance of a learning algorithm.
1
Under review as a conference paper at ICLR 2022
The generalization bounds can be naturally implied by uniform bounds on supw∈W |R(w) -
RS (w)| (Bartlett et al., 2006; Bottou & Bousquet, 2008). While broadly applicable (e.g., to non-
convex problems) and leading to tight generalization in some specific regimes (e.g., margin-based
learning (Kakade et al., 2009)), uniform convergence bounds in general cases might suffer from the
polynomial dependence on dimensionality and thus are not suitable for high-dimensional models
which are ubiquitous in modern machine learning. Alternatively, a powerful proxy for analyzing the
generalization bounds is the stability of learning algorithms to changes in the training dataset. Since
the seminal work of Bousquet & Elisseeff (2002), stability has been extensively demonstrated to
beget dimension-independent generalization bounds for deterministic learning algorithms (Mukher-
jee et al., 2006; Shalev-Shwartz et al., 2010), as well as for randomized learning algorithms (such
as bagging and SGD) (Elisseeff et al., 2005; Hardt et al., 2016). So far, the best known results about
generalization bounds are offered by approaches based on the notion of uniform stability (Feldman
& Vondrak, 2018; 2019; Bousquet et al., 2020). Inspired by these recent breakthrough results, we
seek to derive sharper high-probability generalization bounds for randomized learning algorithms
with on-average uniform stability. A concrete working example of our study is the widely used
stochastic gradient descent (SGD) algorithm that carries out the following recursion for all t ≥ 1
with learning rate ηt > 0:
Wt ：= ∏w (wt-ι - ηNw'(wt-ι;zξt)),	(1)
where ξt ∈ [N] is a uniform random index of data under with or without replacement sampling,
and ΠW is the Euclidean projection operator associated with W . Despite its popularity in the study
of stability theory (Hardt et al., 2016; Zhou et al., 2019; Lei & Ying, 2020), the high probability
generalization bounds of SGD are still relatively under explored through the lens of uniform stability.
1.1	Prior Results
Let us now briefly review some state-of-the-art generalization bounds for uniformly stable algo-
rithms and their randomized variants. We denote by S =. S0 if a pair of data sets S and S0 differ
in a single data point. A randomized learning algorithm A is said to have on-average γN -uniform
stability if it satisfies the following uniform bound
sup	∣Ea['(A(S); z)] - Ea['(A(S0); z)]| ≤ YN.
S=. S0,z∈Z
This definition is equivalent to the concept of uniform stability defined over the on-average loss
Ea['(A(S); z)]. Suppose that the loss function is LiPschitz and bounded in the interval (0,1]. Then
essentially it has been shown in Feldman & Vondrak (2019) that for any δ ∈ (0, 1), with probability
at least 1 - δ over S, the on-average generalization error is upper bounded by
|Ea [R(A(S)) - RS(A(S))]I . YN log(N) log (N) + JlogN1δδ.	⑵
Recently, Bousquet et al. (2020) derived a slightly improved uniform stability bound that implies
|Ea [R(A(S)) - RS(A(S))]∣ . YNIOg(N)log G) + JlogN/δ.	⑶
These generalization bounds are near-tight (up to a logarithmic factor log(N)) in the sense of a lower
bound on sum of random functions provided in that paper. While sharp in the dependence on sample
size, one common limitation of the above uniform stability implied generalization bounds lies in
that these high-probability results only hold in expectation with respect to the internal randomness
of algorithm.
Further suppose that A has YN -uniform stability with probability at least 1 - δ0 for some δ0 ∈ (0, 1)
over the randomness of A, i.e.,
Pa《SUp	∣'(A(S); z) - '(A(S0); z)∣≤ YN ≥ ≥ 1 - δ0.
S=. S0,z∈Z
(4)
Suppose that the randomness of A is independent of the training set S. Then with probability at
least 1 - δ - δ0 over S and A, the bound of Bousquet et al. (2020) naturally implies
|R(A(S))-RS(A(S))| . YN log(N) log
(δ)+严粤
(5)
2
Under review as a conference paper at ICLR 2022
Algorithm 1: Confidence-Boosting for Randomized Learning
Input : A randomized learning algorithm A and a training data set S = {zi}i∈[N] i吧.DN.
Output: Ak*(Sk*).
Uniformly divide S into K disjoint subsets such that S = Uk∈[κ] Sk and |Sk | = K, ∀k ∈ [K].
for k = 1, 2, ..., K do
I Estimate Ak(Sk) as an output of the randomized algorithm A over subset Sk.
end
Compute k* = argmink∈[K] ∖Rs∖sk(Ak(Sk)) - RSk (Ak(Sk))∣.
This is by far the best known generalization bound of randomized stable algorithms that hold with
high probability jointly over data and algorithm. The result, however, relies heavily on the high-
probability uniform stability condition expressed in equation 4. For the SGD (see equation 1) with
fixed learning rate ηt ≡ η, it is possible to show that γN .η√T + ηT and δ0 = Nexp(-枭)
in equation 4 (Bassily et al., 2020). For SGD with time decaying learning rate that has been widely
studied in theory Harvey et al. (2019); Rakhlin et al. (2012) and applied in practice for training
popular deep nets such as ResNet and DenseNet Bengio et al. (2017), it is not clear if the condition
in equation 4 is still valid for γN and δ0 of interest. On the other hand, it is possible to show (see
the proofs of Corollary 1 and 2) that SGD with time decaying learning rate has desirable on-average
uniform stability.
More specially for randomized learning methods such as bagging (Breiman, 1996) and SGD equa-
tion 1, the randomness of algorithm can be precisely characterized by a vector of i.i.d. parameters
ξ = {ξ1, ..., ξT} which are independent on data S. In such cases, suppose that A(S; ξ) has uniform
stability with respect to ξ at any given S, i.e., supξ=ξ, ∣'(A(S; ξ)) - '(A(S; ξ0))∣ ≤ PT. Then the
high probability bound established in Elisseeff et al. (2005) shows that with probability at least 1 -δ,
|R(A(S))-RS(A(S))| .γN+
(1 + NYN
V √N
+ √Tρτ
log
G)
(6)
Provided that YN . NN and PT . T1, the above bound shows that the generalization bound scales
as O(√= + 力)with high probability. However, the above bound will show no guarantee on
convergence if YN & √= and/or PT & 力.For example, this is actually the case for SGD with
time decaying learning rate η = O( 1) on non-convex loss functions in which YN . 2NT and PT
can scale as large as O(1).
Open problem and motication. Keeping the merits and deficiencies of above recalled prior results
in mind, it still remains an open issue if the existing deviation bounds can hold with high confidence
for randomized algorithms with on-average uniform stability (such as SGD with decaying learning
rates). The goal of the present study is to derive sharper high-probability generalization bounds for
randomized algorithms that hold jointly over the randomness of data and algorithm, based on the
relatively weaker notion of on-average uniform stability rather than its high probability counterpart.
1.2	Overview of Our Results
The confidence-boosting technique of Schapire (1990) is a classical meta approach that allows us
to boost the dependence of a learning algorithm on the failure probability δ from 1∕δ to log(1∕δ),
at a certain cost of computational complexity. The fundamental contribution of our work is to
reveal that the confidence-boosting trick yields near-tight high probability generalization bounds
for uniformly stable randomized learning algorithms. The novelty lies in a refined analysis of the
in-expectation first moment generalization error bound for a randomized learning algorithm with on-
average uniform stability, which leads to improved high-probability generalization bounds over the
randomness of data and algorithm via confidence-boosting. More specifically, given a randomized
learning algorithm A, we consider the following subbagging process over training set:
Boosting the Confidence via Subbagging. We independently run A over K disjoint and u-
niformly divided training subsets {Sk}k∈[K] to obtain solutions {Ak (Sk)}k∈[K] . Then we e-
3
Under review as a conference paper at ICLR 2022
valuate the validation error of each candidate solution over its complementary training subset,
and output Ak*(Sk*) that has the smallest gap between training error and validation error, i.e.,
k* = arg mink∈[κ] ∣Rs∖Sk (Ak(Sk)) 一 RSk (Ak (Sk ))∣. Specially when A is deterministic, this re-
duces to a standard subbagging process, namely a variation of bagging using without-replacement
sampling for subsets generation (see, e.g., Andonova et al., 2002). In general, this is essentially a
subbagging procedure with greedy model selection for randomized algorithms over multiple dis-
joint subsets. Here we have assumed without loss of generality that N is a multiplier of K. The
considered procedure of confidence-boosting for randomized learning is outlined in Algorithm 1.
Main Results. In what follows, we highlight our main results on the generalization bounds of the
output of Algorithm 1 along with the implications for SGD and deterministic algorithms:
•	General results. Suppose that the loss is Lipschitz and bounded in the range of (0, 1]. Our main
result in Theorem 1 show that for any δ ∈ (0,1), setting K N log( 1) yields the following
generalization bound of the output of Algorithm 1 that holds with probability at least 1 一 δ over
S and {Ak}k∈[K] :
∣R(Ak*(Sk*)) - Rs(Ak*(Sk*))| . ɪ
K
市N + γm, N + JN) + J log'')
where γm,N andγm2,N are respectively mean-uniform stability and mean-square-uniform stability
bounds introduced in Definition 1. In contrast to the bound in equation 6, our bound is not relying
on the uniform stability with respect to the random bits of algorithm.
•	Stronger generalization bounds for SGD via confidence-boosting. We then use our general result-
s to study the benefit of confidence-boosting on the generalization bounds of SGD-w (SGD via
with-replacement sampling as outlined in Algorithm 2). The main results are a series of corollar-
ies of Theorem 1 when substantialized to SGD with smooth (Corollary 1) or non-smooth (Corol-
lary 2) convex loss, and smooth non-convex loss functions (Corollary 3). For an instance, our
result in Corollary 1 showcases that when invoked to SGD-w on smooth convex loss with learn-
ing rates η = O(表),the generalization bound of the output of Algorithm 1 with K N log( δ) is
upper bounded by
∕ιog(τ)	√T + √N ιog(i∕δ)
V N + N
|R(ASGD-w,k*(Sk*))-RS(ASGD-w,k* (Sk* ))|
Compared with the O(空)in-expectation bound of smooth convex SGD (Hardt et al., 2016), our
above bound is competitive in order while it holds with high confidence.
•	Sharper bounds for uniformly stable algorithms via confidence-boosting. Specially for a deter-
ministic learning algorithm A that has γN -uniform stability for each data set size N ≥ 1, we
further show through Corollary 4 that the following bound holds for the output of Algorithm 1
with K = log( 1):
∣R(A(Sk*)) — Rs(A(Sk*))| . YK + Jlog(K0.
In the case of YN . √= which holds in some popular learning paradigms such as regularized
ERM, the above bound implies (recall K N log(1))
∣R(A(Sk*))- RS(A(Sk* ))1 . ylogNδ),
which is sharper than the best known result in equation 5 (under δ0 = 0) in the sense of the
removal of a log(N) factor. This is a side contribution of our work that might be of independent
interest towards answering an open question raised by Bousquet et al. (2020).
In addition to the generalization bounds, we have also derived a high probability excess risk bound
for uniformly stable randomized learning with confidence-boosting. More specifically, with a proper
modification of the output of Algorithm 1, we can show that with probability at least 1 - δ over S
and {Ak}k∈[K] :
R(Ak* (Sk* )) 一 WmiW R(W) . γm, N + δop1 + 4 gN/’,
where ∆opt is the in-expectation empirical risk optimization error as given by equation 7.
4
Under review as a conference paper at ICLR 2022
2 Generalization Analysis with Confidence-Boosting
In this section, we present a set of generic results on the generalization bounds of randomized learn-
ing algorithms with confidence-boosting as described in Algorithm 1.
2.1	Preliminaries and a Key Lemma
We first introduce the following concept of mean-uniform stability that serves as a powerful tool for
analyzing the generalization bounds of randomized learning algorithms (Hardt et al., 2016; Elisseeff
et al., 2005; Bassily et al., 2020).
Definition 1 (Mean and Mean-Square Uniform Stability of Randomized Algorithms). Let A :
ZN 7→ W be a randomized learning algorithm that maps a data set S ∈ ZN to a model A(S) ∈ W.
Then A is said to have γm,N -mean-uniform stability if for every N ≥ 1,
sup EA [kA(S) -A(S0)k] ≤γm,N.
S=. S0
Moreover, A is said to have γm2,N -mean-square-uniform stability if for every N ≥ 1,
sup EA kA(S)-A(S0)k2 ≤γm2,N.
S=. S0
Here the algorithm outputs A(S) and A(S0) share the same random bits associated with the algo-
rithm. The above defined notion of mean-uniform stability is also known as Uniform Argument
Stability (UAS) (Bassily et al., 2020), which was originally introduced by Liu et al. (2017) for non-
parametric hypotheses. The notion of mean-square uniform stability is stronger than mean-uniform
stability in the sense that the former naturally implies the latter such that γm,N ≤ √γm2,N. More-
over, we say a function f is G-Lipschitz continuous over W if |f (w) - f (w0)| ≤ Gkw - w0k for all
w, w0 ∈ W, and it is L-smooth if kVf (w) 一 Vf (w0)k ≤ LkW - w0k for all w, w0 ∈ W.
Inspired by a second moment bound for generalization error of uniformly stable algorithms
from Bousquet et al. (2020, Section 5), we first establish the following lemma which states that if
a randomized learning algorithm has γm,N -mean-uniform stability, then its on-average first moment
generalization error bound will be as small as O(γm,N + √γm2,N + √N) when the loss function is
Lipschitz continuous. This result is an adaptation of the second moment bound derived by Bousquet
et al. (2020) to on-average uniform stable randomized algorithms, and that bound is also the source
ofγm2,N entering into play. For completeness, we provide a proof for this result in Appendix B.1.
Lemma 1. Suppose that a randomized learning algorithm A : ZN 7→ W has γm,N -mean-uniform
stability and γm2 ,N -mean-square-uniform stability. Assume that the loss function ` is G-Lipschitz
with respect to its first argument and is bounded in the range of [0, M]. Then we have
EA,S ||R(A(S)) 一 RS (A(S))|] ≤ G√Ym2,N + GYm,N +
M
Remark 1. In comparison to the on-average bound |EA,S [R(A(S)) 一 RS (A(S))]| ≤ Gγm,N (see,
e.g., Hardt et al., 2016, Theorem 2.2), our on-average first moment bound in Lemma 1 is substantial-
ly stronger and it turns out to play a crucial role in deriving high probability bounds for randomized
algorithms. When A is deterministic, the above bound reduces to the explicitly or implicitly known
first moment bound for uniformly stable algorithms (see, e.g., Feldman & Vondrak, 2018; Bousquet
et al., 2020) which is tighter in logarithmic factors than the one implied by the p-th moment in-
equality of (Bousquet et al., 2020, Theorem 4). In this case, our bound is stronger than the bound
from Bousquet & Elisseejf (2002, Lemma 9) which essentially scales as √N + √γm,N inournotation.
2.2	Main Results on Generalization B ound
Let us recall the subbagging process as described in Algorithm 1: we independently run A over
K even and disjoint training subsets {Sk}k∈[K] to obtain solutions {Ak (Sk)}k∈[K] , and then pick
Ak* (Sk*) that has the smallest difference between training error and validation error (over the com-
plementary training subset S \ Sk*). The following theorem is our main result about the high prob-
ability generalization bound of the output Ak* (Sk* ) evaluated over the entire training set S. See
Appendix B.2 for its proof which builds largely on the first moment bound in Lemma 1 and the fact
that at least one of the solutions generated by subbagging generalizes well with high probability.
5
Under review as a conference paper at ICLR 2022
Theorem 1. Suppose that a randomized learning algorithm A : ZN 7→ W has γm,N -mean-uniform
stability and γm2 ,N -mean-square-uniform stability as well. Assume that the loss function ` is G-
Lipschitz with respect to its first argument and is bounded in the range of [0, M]. Then for any
ɑ,δ ∈ (0,1) and K ≥ ɪ-a log( 4), With probability at least 1 一 δ over the randomness of S and
{Ak}k∈[K], the output of Algorithm 1 satisfies
∣R(Ak* (Sk*)) - RS(Ak* (Sk*))|
OK (Gpγm2, K + Gγm, N + M
rN)+M产粤
Remark 2. To gain some intuition on the superiority of our bound in Theorem 1, let us consider
K N log (1) as allowed in the conditions. If γm,N . √= and γ*N . N, then our high-probability
Jlog(i∕δ)
V N
bound in Theorem 1 roughly scales as
which is sharper than the on-average bounds
in equation 2 and equation 3 with YN . √N. More precise consequences ofthese general results on
SGD and deterministic uniformly stable estimators such as ERMs and full gradient descent method
will be discussed shortly in the sections to follow.
Remark 3. In sharp contrast to the bound in equation 5 that requires high probability uniform
stability and the bound in equation 6 that assumes uniform stability over the random bits of algo-
rithm, our bound in Theorem 1 holds under a substantially milder notion of mean(-square)-uniform
stability over data. In terms of the tightness of bound, note that the confidence term
#gNM is
necessary even for an algorithm with fixed output. The uniform stability terms Ym, N and γm2, N are
also near-tight as the the algorithm output can change arbitrarily with respect to these quantities.
2.3	On Excess Risk Bounds
To understand the optimization performance of a randomized learning algorithm A with confidence-
boosting, we further study here the excess risk bounds of Algorithm 1 which are of special interest
for stochastic convex optimization problems. In the following analysis, the global minimizer of the
population risk and in-expectation empirical risk sub-optimality of the randomized algorithm are
respectively denoted by
w* ：= arg min R(W) and ∆opt := EAS RS(A(S)) — min RS(W) .	(7)
w∈W	w∈W
In order to derive the excess risk guarantees, we first need to slightly modify the output of Algo-
rithm 1 as Ak*(Sk*) where k* = argmink∈[κ] RS∖Sk(Ak(Sk)). The following theorem is our
main result about the high probability excess risk bounds of such a modified output of confidence-
boosting. See Appendix B.3 for its proof.
Theorem 2. Suppose that a randomized learning algorithm A : ZN 7→ W has Ym,N -mean-uniform
stability. Assume that the loss function ` is G-Lipschitz with respect to its first argument and is
bounded in the range of [0, M]. Thenfor any 0,δ ∈ (0,1) and K ≥ 1-不 log( 4), With probability
at least 1 - δ over the randomness ofS and {Ak}k∈[K], the modified output of Algorithm 1 satisfies
R(Ak*(Sk*)) — R(w*) . 1 (GYm,N + ∆opt) + MaK∕δ).
Remark 4. Unlike the generalization error bounds, the excess risk bounds established in Theorem 2
are not relying on the mean-square uniform stability of the algorithm, but with an additional term of
in-expectation optimization error for minimizing the empirical risk. For deterministic optimization
algorithms such as ERMs with ∆opt = 0, similar excess risk bounds can be implied by the generic
results of Shalev-Shwartz et al. (2010, Theorem 26) developed for the confidence-boosting approach.
Remark 5. Consider K N log (I) and YmN . √N. Then the bound in Theorem 2 roughly scales
as。(产/ + ∆opt).
Finally, we comment on the difference between the generalization error and excess risk analysis
inside the considered confidence-boosting framework. Since the excess risk is non-negative and
6
Under review as a conference paper at ICLR 2022
Algorithm 2: SGD via With-Replacement Sampling (ASGD-w)
Input : Data set S ={zi}i∈[N] i吧. DN, step-sizes {ηt}t≥1, #iterations T, initialization w0.
Output： WT = T ∑t∈[τ] Wt.
for t = 1, 2, ..., T do
Uniformly randomly sample an index ξt ∈ [N] with replacement;
Compute Wt = ∏w (wt-i — ηNw'(wt-ι; zξt)).
end
its in-expectation bound is standardly known for uniformly stable learning algorithms, the high-
confidence bound in Theorem 2 can be easily derived via invoking Markov inequality to the inde-
pendent runs of algorithm over K disjoint subsets. The generalization error analysis, however, is
way more challenging in the sense that establishing tight in-expectation first moment generalization
bound (see Lemma 1) for uniformly stable randomized algorithms is by itself highly non-trivial.
3 Implications for Stochastic Gradient Descent
In this section we demonstrate the applications of the generic bound in Theorem 1 to the widely
used SGD algorithm. We focus on a variant of SGD under with-replacement sampling as outlined
in Algorithm 2, which we call ASGD-w. In what follows, we denote by {ASGD-w,k}k∈K the outputs
of ASGD-w over subsets {Sk}k∈K when applied with Algorithm 1. Our results readily extend to the
without-replacement variant of SGD and the corresponding results can be found in Appendix D.
3.1	Convex Optimization with Smooth Loss
For smooth and convex losses such as logistic loss, we can derive the following result as a direct
consequence of Theorem 1 with α = 1/2 to ASGD-w. See Appendix C.1 for its proof.
Corollary 1. Suppose that the loss function is '(•;•) is convex, G-Lipschitz and L-smooth with
respect to its first argument, and is bounded in the range of [0, M]. Consider Algorithm 1 specified
to Asgd-w with learning rate η ≤ 2/L for all t ≥ 1. Thenfor any δ ∈ (0,1) and K N log(4), with
probability at least 1 — δ over the randomness ofS and {ASGD-w,k}k∈[K], the generalization error
is upper bounded as ∣R(AsGD-w,k- (Sk-)) — RS(Asgd-w,r- (Sk- ))| .
G2J (X η2+N (X ηt!)
G2 T
+ N X ηt + M
.bg(10
Remark 6. For the conventional step-size choice of ηt = j√t, the high probability generalization
O("
bound in Corollary 1 is dominated by
+
√T+ VZNlog(Vδ)), which matches the corre-
sponding O(NT-) in-expectation boundfor SGD with smooth convex losses (Hardt et al., 2016).
3.2	Convex Optimization with Non-smooth Loss
Now we turn to study the case where the loss is convex but not necessarily smooth, such as the
hinge loss and absolute loss. The following result as a direct consequence of Theorem 1 for the
specification of Algorithm 1 to ASGD-w with non-smooth convex loss and time varying learning rate
{ηt}t≥i. Its proof is provided in Appendix C.2.
Corollary 2. Suppose that the lossfunction is '(•；•) is convex and G-Lipschitz with respect to its first
argument, and is bounded in the range of [0, M]. Consider Algorithm 1 specified to ASGD-w. Then
for any δ ∈ (0,1) and K N log( 4), with probability at least 1 — δ over the randomness of S and
{ASGD-w,k}k∈[K], the generalization error satisfies |R(ASGD-w,k- (Sk-)) — RS (ASGD-w,k- (Sk-))| .
ηt+M 产*
7
Under review as a conference paper at ICLR 2022
Remark 7. For constant rates ηt ≡ η, Corollary 2 admits a high probability generalization bound
of scale O(η√T + η NT +
JIog(Nyδ)) which matches the near-optimal rate by Bassily et al. (2020,
Theorem 3.3). More importantly, our deviation bound in Corollary 2 still holds for time varying
learning rates.
3.3	Non-convex Optimization with Smooth Loss
We further study the performance of Algorithm 1 for SGD on smooth but not necessarily convex
loss functions, such as normalized sigmoid loss (Mason et al., 1999). The following result is a direct
application of Theorem 1 to ASGD-w with smooth non-convex loss. See Appendix C.3 for its proof.
Corollary 3. Suppose that the loss function is '(•;•) is G-Lipschitz and L-smooth with respect to
its first argument, and is bounded in the range of [0, M]. Consider Algorithm 1 specified to ASGD-w
with ηt ≤ L. Let ut := η2 + 2ηt P；=； exp(L Pt=T+1 小加「 for all t ≥ 1. Thenforany δ ∈ (0,1)
and K N log( 4), with probability at least 1 一 δ over the randomness of S and {AsGD-w,k }k∈[κ], the
generalization error is upper bounded as ∣R(AsGD-w,k* (Sk*)) 一 RS(ASGD-w,k* (Sk* ))| .
T
t=1
eχp (L X ητ) Ut+ (N X eχp (L X ητ) ηt + M Jlog”®
τ =t+1	t=1	τ =t+1
1T
G2/X
Remark 8. For the constant learning rates ηt ≡ LT, Corollary 3 admits high probability gener-
alization bound of scale O( JlogN®). For time decaying learning rates ηt
LVt with arbitrary
ν ≥ 1, it can be verified that the corresponding bound is of scale O(
T1/v Iog(T)
νN
+尸囱）.
4 Implications for Deterministic Uniformly Stable Algorithms
This section is devoted to showing that confidence-boosting is also beneficial for deriving stronger
generalization bounds for uniformly stable deterministic learning algorithms. First we note that
when there is no internal randomness in A, the definition of mean(-square)-uniform stability reduces
to the conventional concept of γN -uniform stability for deterministic algorithms given by
SUP kA(S) - A(S0)k ≤ YN = Ym,N = √Ym2,N∙
S=. S0
Let us now consider a specification of Algorithm 1 to a uniformly stable deterministic algorithm A.
Since there is no randomness contained in A, we have that Ak(Sk) = A(Sk) for all k ∈ [K] in such
a deterministic case. Then the following result is a direct consequence of Theorem 1 when applied
to the considered deterministic learning regime.
Corollary 4. Suppose that a deterministic learning algorithm A : ZN 7→ W has γN -uniform
stability. Assume that the loss function ` is G-Lipschitz with respect to its first argument and is
bounded in [0, M]. Thenfor any α,δ ∈ (0,1) and K ≥ Iog-4/", With probability at least 1 — δ over
the randomness of S, the output of Algorithm 1 satisfies
∣R(A(Sk*)) — RS(A(Sk*))| . -K (GYN + MyK) + M《'弋广δ.
To demonstrate the superiority of our bound over prior ones, let us consider - = 0.5 and K N log( ɪ)
in the above corollary. In the regime YN . √= which is of interest in many popular deterministic
learning paradigms/algorithms such as regularized ERM (Shalev-Shwartz et al., 2009) and full gra-
dient descent (Feldman & Vondrak, 2019), Corollary 4 implies a generalization bound for A(Sk* )
over the data set S that scales as ∣R(A(Sk*)) — RS(A(Sk* ))| . JIogN®.
In comparison, the best
known bound in equation 5 essentially from Bousquet et al. (2020) gives (keep in mind that δ0 = 0
in the deterministic case) ∣R(A(S)) — RS(A(S))I . lo√N) log( 1). AS We can see that inside the
carefully designed framework of confidence-bossting via subbagging, our generalization bound gets
rid of the logarithmic factor log(N ) from the above best known result, though the generalization is
with respect to the estimation over a specific part of the sample. We expect this result will fuel future
research towards fully resolving the corresponding open question raised in Bousquet et al. (2020).
8
Under review as a conference paper at ICLR 2022
5	Other Related Work
The idea of using stability of a learning algorithm, namely the sensitivity of estimated model to the
changes in training data, for generalization performance analysis dates back to the seventies (Vap-
nik & Chervonenkis, 1974; Rogers & Wagner, 1978; Devroye & Wagner, 1979). For deterministic
learning algorithms, algorithmic stability has been extensively studied with a bunch of applications
to establishing strong generalization and excess risk bounds for stable learning models like k-NN
and regularized ERMs (Bousquet & Elisseeff, 2002; Zhang, 2003; Klochkov & Zhivotovskiy, 2021).
The stability theory for randomized learning algorithms was formally introduced and investigated
by Elisseeff et al. (2005). In a recent breakthrough work (Hardt et al., 2016), it was shown in that
the solution obtained via stochastic gradient descent is expected to be stable and generalize well for
smooth convex and non-convex loss functions. For non-smooth convex losses, the stability induced
generalization bounds of SGD have been established in expectation (Lei & Ying, 2020) or devia-
tion (Bassily et al., 2020). In Kuzborskij & Lampert (2018), a set of data-dependent generalization
bounds for SGD were derived based on the stability of algorithm. More broadly, generalization
bounds for stable learning algorithms that converge to global minima were established in Charles
& Papailiopoulos (2018); Lei & Ying (2021). For non-convex sparse learning, algorithmic stabili-
ty theory has been applied to derive the generalization bounds of the popularly used iterative hard
thresholding (IHT) algorithm (Yuan & Li, 2021). The uniform stability bounds on SGD have also
been extensively used for designing differential privacy stochastic optimization algorithms (Bassily
et al., 2019; Feldman et al., 2020).
Bagging (or bootstrap aggregating) is one of the earliest and most popular ensemble methods that
has been widely applied to reduce the variance for unstable learning algorithms such as decision
tree and neural networks (Breiman, 1996; Opitz & Maclin, 1999), and sometimes stable algorithms
such as SVMs (Valentini & Dietterich, 2003). As an important variant of bagging, subbagging has
been proposed to reduce the computational cost of bagging via training base models under without-
replacement sampling (Buhlmann, 2012). The stability and generalization bounds of bagging have
been analyzed for both uniform (Elisseeff et al., 2005) and non-uniform (Foster et al., 2019) av-
eraging schemes. Unlike these prior results for bagging with averaging aggregation, our bounds
are obtained based on a confidence-boosting greedy aggregation scheme which turns out to yield
sharper dependence on the uniform stability parameter.
The confidence-boosting technique has long been applied for obtaining sharp high-probability ex-
cess risk bounds from the corresponding strong in-expectation bounds (Shalev-Shwartz et al., 2010;
Mehta, 2017). For generic statistical learning problems, confidence-boosting has been used to con-
vert any low-confidence learning algorithm with linear dependence on 1 /δ to a high-confidence
algorithm with logarithmic factor log(1∕δ). For learning with exp-concave losses, a relevant ERM
estimator with in-expectation fast rate of convergence was converted to a high-confidence learning
algorithm with an almost identical fast rate of convergence UP to a logarithmic factor on 1∕δ (Mehta,
2017). While sharing a similar spirit of boosting the confidence, our generalization analysis is sub-
stantially more challenging than those prior excess risk analysis in terms of tightly deriving in-
expectation first moment generalization bound for uniformly stable randomized algorithms.
6	Conclusions
In this paper we presented a generic confidence-boosting method for deriving near-optimal high
probability generalization bounds for uniformly stable randomized learning algorithms. At a nut-
shell, our main results in Theorem 1 and Theorem 2 reveal that a carefully designed subbagging
process in Algorithm 1 can yield high-confidence generalization and risk bounds under the notion
of mean(-square)-uniform stability. Our theory has been substantialized to SGD on both convex and
non-convex losses to obtain stronger generalization bounds especially in the case of time decaying
learning rates. When reduced to deterministic algorithms, the proposed method removes a logarith-
mic factor on sample size from the best known bounds. While sharper in the dependence on sample
size, our confidence-boosting results are only applicable to one of the independent runs of algorithm
A over K disjoint training subsets of equal size with K N log (1). It is so far not clear if these
near-optimal bounds can be further extended to the full-batch setting where the generalization is
with respect to the evaluation of algorithm over the entire sample. We leave the full understanding
of such an open issue raised by Bousquet et al. (2020) for future investigation.
9
Under review as a conference paper at ICLR 2022
References
Savina Andonova, Andre Elisseeff, Theodoros Evgeniou, and Massimiliano Pontil. A simple algo-
Iithm for learning stable machines. In ECAI, pp. 513-517, 2002.
Peter L Bartlett, Michael I Jordan, and Jon D McAuliffe. Convexity, classification, and risk bounds.
Journal of the American Statistical Association, 101(473):138-156, 2006.
Raef Bassily, Vitaly Feldman, Kunal Talwar, and Abhradeep Thakurta. Private stochastic convex
optimization with optimal rates. arXiv preprint arXiv:1908.09970, 2019.
Raef Bassily, Vitaly Feldman, Cristobal Guzman, and Kunal Talwar. Stability of stochastic gradient
descent on nonsmooth convex losses. In Advances in Neural Information Processing Systems, pp.
1-10, 2020.
Yoshua Bengio, Ian Goodfellow, and Aaron Courville. Deep learning, volume 1. MIT press Mas-
sachusetts, USA:, 2017.
Leon BottoU and Olivier Bousquet. The tradeoffs of large scale learning. In Advances in Neural
Information Processing Systems, pp. 161-168, 2008.
Olivier Bousquet and Andre Elisseeff. Stability and generalization. Journal of Machine Learning
Research, 2(Mar):499-526, 2002.
Olivier Bousquet, Yegor Klochkov, and Nikita Zhivotovskiy. Sharper bounds for uniformly stable
algorithms. In Conference on Learning Theory, pp. 610-626, 2020.
Leo Breiman. Bagging predictors. Machine learning, 24(2):123-140, 1996.
Peter Buhlmann. Bagging, boosting and ensemble methods. In Handbook of computational Statis-
tics, pp. 985-1022. Springer, 2012.
Zachary Charles and Dimitris Papailiopoulos. Stability and generalization of learning algorithms
that converge to global optima. In International Conference on Machine Learning, pp. 744-753,
2018.
Luc Devroye and Terry Wagner. Distribution-free inequalities for the deleted and holdout error
estimates. IEEE Transactions on Information Theory, 25(2):202-207, 1979.
Andre Elisseeff, Theodoros Evgeniou, and Massimiliano Pontil. Stability of randomized learning
algorithms. Journal of Machine Learning Research, 6(Jan):55-79, 2005.
Vitaly Feldman and Jan Vondrak. Generalization bounds for uniformly stable algorithms. In Ad-
vances in Neural Information Processing Systems, pp. 9747-9757, 2018.
Vitaly Feldman and Jan Vondrak. High probability generalization bounds for uniformly stable algo-
rithms with nearly optimal rate. In Conference on Learning Theory, pp. 1270-1279, 2019.
Vitaly Feldman, Tomer Koren, and Kunal Talwar. Private stochastic convex optimization: optimal
rates in linear time. In Proceedings of the 52nd Annual ACM SIGACT Symposium on Theory of
Computing, pp. 439-449, 2020.
Dylan J Foster, Spencer Greenberg, Satyen Kale, Haipeng Luo, Mehryar Mohri, and Karthik Srid-
haran. Hypothesis set stability and generalization. In Advances in Neural Information Processing
Systems, 2019.
Moritz Hardt, Ben Recht, and Yoram Singer. Train faster, generalize better: Stability of stochastic
gradient descent. In International Conference on Machine Learning, pp. 1225-1234, 2016.
Nicholas JA Harvey, Christopher Liaw, Yaniv Plan, and Sikander Randhawa. Tight analyses for non-
smooth stochastic gradient descent. In Conference on Learning Theory, pp. 1579-1613. PMLR,
2019.
Sham M Kakade, Karthik Sridharan, and Ambuj Tewari. On the complexity of linear prediction:
Risk bounds, margin bounds, and regularization. In Advances in Neural Information Processing
Systems, pp. 793-800, 2009.
10
Under review as a conference paper at ICLR 2022
Yegor Klochkov and Nikita Zhivotovskiy. Stability and deviation optimal risk bounds with conver-
gence rate o(1/n). In Advances in Neural Information Processing Systems, 2021.
Ilja Kuzborskij and Christoph Lampert. Data-dependent stability of stochastic gradient descent. In
International Conference on Machine Learning, pp. 2820-2829, 2018.
Yunwen Lei and Yiming Ying. Fine-grained analysis of stability and generalization for sgd. In
International Conference on Machine Learning, 2020.
Yunwen Lei and Yiming Ying. Sharper generalization bounds for learning with gradient-dominated
objective functions. In International Conference on Learning Representations, 2021.
Tongliang Liu, Gabor Lugosi, Gergely Neu, and Dacheng Tao. Algorithmic stability and hypothesis
complexity. In International Conference on Machine Learning, pp. 2159-2167. PMLR, 2017.
Llew Mason, Jonathan Baxter, Peter Bartlett, and Marcus Frean. Boosting algorithms as gradient
descent in function space. In Advances in Neural Information Processing Systems, pp. 512-518,
1999.
Nishant Mehta. Fast rates with high probability in exp-concave statistical learning. In Artificial
Intelligence and Statistics, pp. 1085-1093. PMLR, 2017.
Sayan Mukherjee, Partha Niyogi, Tomaso Poggio, and Ryan Rifkin. Learning theory: stability is
sufficient for generalization and necessary and sufficient for consistency of empirical risk mini-
mization. Advances in Computational Mathematics, 25(1-3):161-193, 2006.
David Opitz and Richard Maclin. Popular ensemble methods: An empirical study. Journal of
Artificial Intelligence Research, 11:169-198, 1999.
Alexander Rakhlin, Ohad Shamir, and Karthik Sridharan. Making gradient descent optimal for
strongly convex stochastic optimization. In International Conference on Machine Learning, pp.
1571-1578, 2012.
William H Rogers and Terry J Wagner. A finite sample distribution-free performance bound for
local discrimination rules. The Annals of Statistics, pp. 506-514, 1978.
Robert E Schapire. The strength of weak learnability. Machine learning, 5(2):197-227, 1990.
Mark Schmidt, Nicolas L Roux, and Francis R Bach. Convergence rates of inexact proximal-gradient
methods for convex optimization. In Advances in Neural Information Processing Systems, pp.
1458-1466, 2011.
Shai Shalev-Shwartz, Ohad Shamir, Nathan Srebro, and Karthik Sridharan. Stochastic convex opti-
mization. In Conference on Learning Theory, 2009.
Shai Shalev-Shwartz, Ohad Shamir, Nathan Srebro, and Karthik Sridharan. Learnability, stability
and uniform convergence. Journal of Machine Learning Research, 11(Oct):2635-2670, 2010.
Giorgio Valentini and Thomas G Dietterich. Low bias bagged support vector machines. In Interna-
tional Conference on Machine Learning, pp. 752-759, 2003.
V. N. Vapnik and A. Ya. Chervonenkis. Theory of Pattern Recognition [in Russian]. Nauka, 1974.
Xiao-Tong Yuan and Ping Li. Stability and risk bounds of iterative hard thresholding. In Interna-
tional Conference on Artificial Intelligence and Statistics, pp. 1702-1710. PMLR, 2021.
Tong Zhang. Leave-one-out bounds for kernel methods. Neural Computation, 15(6):1397-1437,
2003.
Yi Zhou, Huishuai Zhang, and Yingbin Liang. Understanding generalization error of sgd in non-
convex optimization. In International Conference on Acoustics, Speech, and Signal Processing
(ICASSP), 2019.
11
Under review as a conference paper at ICLR 2022
A Auxiliary Lemmas
We need the following lemma from Hardt et al. (2016) which shows that SGD iteration is non-
expansive on convex and smooth loss.
Lemma 2 (Hardt et al. (2016)). Assume that f is convex and L-smooth. Then for any w, w0 ∈ W
and α ≤ 2/L, we have the following bound holds
∣∣w — aVf(W) — (w0 — aVf(w0))k ≤ ∣∣w — w0∣∣.
The following lemma, which can be proved by induction (see, e.g., Schmidt et al., 2011), will be
used to prove the main results in Section 3.
Lemma 3. Assume that the nonnegative sequence {uτ}τ ≥1 satisfies the following recursion for all
t≥1:
t
ut ≤ St +	ατuτ ,
τ=1
with {Sτ}τ ≥1 an increasing sequence, S0 ≥ u02 and ατ ≥ 0 for all τ. Then, the following inequality
holds for all t ≥ 1:
t
Ut ≤ VZSt + EaT.
τ=1
B Proofs for the Results in Section 2
In this section, we present the technical proofs for the main results stated in Section 2.
B.1 Proof of Lemma 1
We need the following lemma essentially from Bousquet et al. (2020) that provides a first moment
bound for the sum of random functions.
Lemma 4. Let S = {Z1, Z2, ..., ZN} be a set of i.i.d. random variables valued in Z. Letg1, ..., gN
be a set of measurable functions gi : ZN 7→ R that satisfy ES [gi2 (S)] ≤ M2 and EZi [gi (S)] = 0
for all i ∈ [N]. Then we have
ES
N
X gi(S)
i=1
≤X
i6=j
,S(j)
[(gi(S) — gi(Sj ))2] + M √N,
where S(j) = {Z1, ..., Zj-1, Zj0, Zj+1, ..., ZN} and S0 = {Z10 , Z20, ..., ZN0 } is another i.i.d. sample
from the same distribution as that of S.
Proof. We reproduce the proof in view of the argument in Bousquet et al. (2020, Section 5) showing
that {gi} are weakly correlated. For any i 6= j, since EZi [gi(S)] = 0 and EZj [gj (S)] = 0, we can
verify that
ES hgi (S(j))gj (S)i = ES\Zj hEZj hgi(S(j))gj(S) | S\Zjii = ES\Zj hgi(S(j))EZj [gj(S) |S\Zj]i =0,
where we have used the independence of the elements in S ∪ {Zj0}. Similarly, we can show that
ES hgi(S)gj(S(i))i =ES hgi(S(j))gj(S(i))i =0.
Then it follows that for any i 6= j ,
|ES[gi(S)gj(S)]| = ES,S(i),S(j) [gi(S)gj(S)]
= ES,S(i),S(j) h(gi(S)—gi(S(j)))(gj(S)—gj(S(i)))i
≤ES,S(i),S(j) h(gi(S) —gi(S(j)))(gj(S)—gj(S(i)))i .
12
Under review as a conference paper at ICLR 2022
Based on the above bound and Jensen’s inequality we get
N
ES Xgi(S)
i=1
2
≤t
≤s
≤s
=s
Es I (X gi(S)
N
XES [gi(S)gj(S)] +XES[gi2(S)]
i6=j
i=1
EES,s(i),s(j) [∣(gi(S) - gi(S"(gj(S) -gj(S(i)))∣] + M√N
i6=j
1 XEs,s(i),s(j) [(gi(S) - gi(Sj)))2 + (gj(S) - gj(S⑺))2i + M√N
i6=j
XEs,s(j) [(gi(S) - gi(Sj))H + M√N.
i6=j
∖
This proves the desired bound.
□
Now we are ready to prove the result in Lemma 1.
ProofofLemma 1. Let Us consider	hi(S) :=	R(A(S))	-	'(A(S); Zi)	and	gi(S)	=	hi (S)-
EZi [hi(S)] for i ∈ [N]. Then by assumption we have
EZi [gi (S)] = 0, ES [gi2 (S)] ≤ M2 .
For each i ∈ [N], let S(i) denote a random data set that is identical to S except that one of the
Zi is replaced by another random sample Zi0 . For any i 6= j, since the loss is non-negative and
G-Lipschitz, it can be verified that
|gi(S) - gi(S(j))| ≤maxn∣∣∣hi(S) -hi(S(j))∣∣∣ ,∣∣∣EZi[hi(S) -hi(S(j))]∣∣∣o
≤max GkA(S)-A(S(j))k,EZi GkA(S)-A(S(j))k
which readily implies
ES,S(j)	gi(S)-gi(S(j))2 ≤ ES,S(j) hG2kA(S) - A(S(j))k2i .
Then invoking Lemma 4 to {gi } yields
N
ES ∣∣Xgi(S)
∣ i=1
≤	X ES,S(j)
i6=j
[(gi(S)-gi (S ⑶))2]
+ M√N
≤G IXEs,s(j) [kA(S)- A(S j))k2] + M√N.
i6=j
FUrther, it can be verified that
∣N	∣
ES ∣∣XEZi[hi(S)]∣∣
∣ i=1	∣
(A.1)
N
=ES	X EZi [R(A(S)) - '(A(S); Zi)]
∣ i=1	∣
∣N	∣
=ES	X Ez」Ezi ['(A(S); Z0)] - '(A(S); Zi)]
∣ i=1	∣
∣N
=Es	X EZjEZi ['(A(S); Z0)] - EZi ['(A(S(i))； Zi)]]
∣ i=1
(A.2)
≤ES,S(i) GkA(S)-A(S(i))k .
13
Under review as a conference paper at ICLR 2022
By combining equation A.1 and equation A.2 we obtain
EA,S [|R(A(S)) - RS(A(S))|]
=Nn EA,S
G
≤ N EA
N
X(gi(S)+EZi[hi(S)])
i=1
ES,S(j)
i6=j
kA(S) - A(S(j))k2
+ NEA,s,s(i) hkA(S) - A(S(i))ki + √N
G
≤-
≤ N
EA,S,S(j)
i6=j
[kΑ(S) — A(S(j))k2] + GEA,s,s(i) [kA(S) - A(S⑴)k]
M
十 √N
≤G√γm2,N + GYmN +
M
√N
where in the last inequality We have used the stability conditions on A. The proof is completed. □
B.2 Proof of Theorem 1
We first establish the following intermediate result that captures the effects of subbagging on ran-
domized algorithms: it basically tells that with K N log( 1), at least one of the solutions generated
by subbagging generalizes well with high probability.
Lemma 5. Suppose that a randomized learning algorithm A : ZN 7→ Whasγm,N -mean-uniform
stability and γm2 ,N -mean-square-uniform stability as well. Assume that the loss function ` is G-
Lipschitz with respect to its first argument and is bounded in the range of [0, M]. Then for any α, δ ∈
(0,1) and K ≥ Iog-2(δ), with probability at least 1 一 δ over the randomness of {(Ak, Sk)}k∈[κ],
the sequence {Ak(Sk)}k∈[K] generated by Algorithm 1 satisfies
min
k∈[K]
IR(Ak(Sk))- RSk (Ak(Sk)) 1. 1
GpYm2, K + Gγm, K + M
Proof. From Lemma 1 we have that over randomized algorithm A and data S with |S |
N
K ,
EA,S 卜R(A(S))
一 RS(A(S))|] ≤ GpYmΓJ + Gγm, N + M
Since {Ak, Sk}k∈[K] are independent to each other, by Markov inequality we know that
αK ≤ δ,
P{Ak,Sk } (Am∈iK]1R(Ak (Sk )) 一 RSk (Ak (Sk ))| ≥ a (GpYm2, K + GYm, K + M ∖ N
which implies the desired bound.
□
Next we proceed to prove the main result in Theorem 1.
Proof of Theorem 1. Let us consider the following three events:
E = {lR(Ak*(Sk*)) - RS(Ak*(Sk*))I. OKkGK+GYm,K + Mrŋ+ MjKo-(KN)},
E1:= max |R(Ak(Sk)) - RS\Sk (Ak(Sk))| .M
k∈[K]
K log(K∕δ)
(K-1)N
,
E2 := {k∈iKJR(Ak (Sk )) - RSk (Ak(Sk))I . £ (GpYm2, K + GYm, K + M ^^
14
Under review as a conference paper at ICLR 2022
We can show that E ⊇ E1 ∩ E2 . Indeed, suppose that E1 and E2 simultaneously occur. Consequently
the following inequality is valid:
|R(Ak*(Sk*)) -RS(Ak*(Sk*))|
R(Ak* (S k* )) - V7RSk* (Ak*(Sk* )) -
∣K
≤V7 IR(Ak * (Sk* )) - RSk * (Ak* (Sk* )) | +
K
K - 1
K
K - 1
K
RS∖Sk*(Ak*(Sk*))
IR(Ak*(Sk*))- Rs∖Sk* (Ak*(Sk*))∣
≤ K ∣RS∖Sk* (Ak* (Sk* )) - RS k* (Ak* (Sk* ))∣ + IR(Ak* (Sk* )) - RS-∖Sk* (Ak* (Sk* )) I
=1 ɪ min∣RS∖Sk (Ak (Sk)) — RSk (Ak(Sk))I + ∣R(Ak*(Sk*)) - Rs∖sk* (Ak*(Sk* ))∣
K k∈[K]
=评 min IRS∖Sk (Ak (Sk )) - R(Ak (Sk )) + R(Ak (Sk )) - RSk (Ak (Sk )) ∣
K k∈[K]
+ ∣∣R(Ak* (Sk* )) - RS∖Sk* (Ak* (Sk* ))∣∣
≤V7 min IR(Ak (Sk )) - RSk (Ak (Sk ))| + V7 max IRS∖Sk (Ak (Sk )) - R(Ak (Sk )) ∣
K k∈[K]	K k∈[K]
+ ∣∣R(Ak* (Sk* )) - RS∖Sk* (Ak* (Sk* ))∣∣
≤V7 min IR(Ak (Sk )) - RSk (Ak (Sk ))| +-77- max IR(Ak (Sk )) - RS∖Sk (Ak (Sk )) ∣
K k∈[K]	K k∈[K]
ζ2ɪ GG pγ 2 N + GY N + M JK-! + MSK Iog(K/δ)
〜ɑK IGVnm2,K 十 GYm，K 十 MV N)十 My (K - 1)N ,
where in “Zi” We have used the definition of k*, and “Z2" follows from Ei, E2. With leading terms
preserved in the above we can see that E occurs.
Next we can show that PS,{Ak}(E1) ≤ 2. Toward this end, let Us consider the following events for
all k ∈ [K]:
Ek ：= {∣R(Ak(Sk)) - RS∖Sk(Ak(Sk))∣ < MjKlo-(KN) }.
Clearly, it is true that Ei = TK=I Ek. It is sufficient to prove that PS,Ak (Ek) ≤ 2K holds for each
k ∈ [K]. Indeed, consider the random indication function β(S,Ak) := 1讦 associated with the
event Eik . Then we have the following holds for each k ∈ [K]:
=ES,Ak [β(S, Ak)]
=EAk,Sk [ES∖Sk|Ak,Sk [β(S,Ak) I Ak,Sk]]
ζ=1EAk,Sk
ES∖Sk[β(S,Ak) IAk,Sk]
=EAk,Sk
PS∖Sk(IR(Ak(Sk))- RS∖Sk(Ak(Sk))I & MjKlo-(KN)) I Ak,Sk
ζ2
≤EAk,Sk
—I Ak, Sk
2K 1 k,
δ
2K,
where in “Zi” we have used the independence between {Ak, Sk} and S \ Sk, and “Z2" is due to
Hoeffding’s inequality conditioned on {Ak, Sk}, keeping in mind that Ak (Sk) is independent on
the data set S \ Sk of size (1 - 1/K)N. It follows by union probability that
K K _∖	K	_ δ
PS,{Ak}而=PS,{Ak} (U Ek) ≤ X PS,Ak (Ek) ≤ 2
15
Under review as a conference paper at ICLR 2022
Further, from the part(b) of Lemma 5 We have Ps,{Ak} (&) ≤ 2. Combining this and the preceding
bound yields
PS,{Ak} (E) ≥ PS,{Ak} (EI ∩E2) ≥ 1 - PS,{Ak} (EI)- PS,{Ak}(E2) ≥ 1 - ∖ - g = 1 - δ∙
This implies the desired result in part(b) as K/(K - 1) ≤ 2 for K ≥ 2.	□
B.3 Proof of Theorem 2
We first present the folloWing simple lemma about the in-expectation risk bounds of a randomized
algorithm Which Will be used in our analysis.
Lemma 6. Suppose that a randomized learning algorithm A : ZN 7→ W has γm,N -mean-uniform
stability. Assume that the loss function ` is G-Lipschitz with respect to its first argument. Then we
have
Ea,S [R(A(S))- R(w* )] ≤ Gγm,N + ∆opt.
Proof. By risk decomposition We can shoW that
Ea,s [R(A(S)) - R(w*)]
=Ea,s [R(A(S)) - RS(A(S)) + RS(A(S))- RS(w*) + RS(w*) - R(w*)]
≤ |EA,S [R(A(S)) - RS (A(S))]| + ∆opt
≤Gγm,N + ∆opt ,
Where in the last inequality We have used the on-average generalization bound by Hardt et al. (2016,
Theorem 2.2).	□
Lemma 7. Suppose that a randomized learning algorithm A : ZN 7→ W has γm,N -mean-uniform
stability. Assume that the loss function ` is G-Lipschitz with respect to its first argument. Then
for any ɑ,δ ∈ (0,1) and K ≥ Iog-2/"), with probability at least 1 - δ over the randomness of
{(Ak, Sk)}k∈[K], the sequence {Ak (Sk)}k∈[K] generated by Algorithm 1 with the modified output
satisfies
min R(Ak(Sk))- R(w*) . 1 (GYm,K + ∆。“).
k∈[K]	α	K
Proof. Recall the modified output Ak*(Sk*) where k = argmi□k∈[κ] R5\Sk(Ak(Sk)). From
Lemma 6 we have that over randomized algorithm A and data S with |S| = N,
Ea,S hR(A(S)) - R(w*)] ≤ GYm, K + ∆opt.
Since {Ak, Sk}k∈[K] are independent to each other, by Markov inequality we know that
P{Ak,Sk} (mK] R(Ak(Sk)) - R(w*) ≥ 1 (GYm,K + ∆oPt)) ≤ ακ ≤ δ,
which implies the desired bound in part(b).	□
Next we proceed to prove the main result in Theorem 2.
Proof of Theorem 2. Let us consider the following three events:
E := {|R(Ak* (Sk* )) - RS (Ak* (Sk* ))| . α (Gγm, K + δoPt) + M (K F 1)N	,
Ei:=(m” (Ak (Sk))- RS∖Sk (Ak(Sk))1. Mi
K log(K∕δ)
(K-1)N
,
E2 := {mK] R(Ak(Sk))- R(w*) . 1 (GYm,K + ∆opt)卜
16
Under review as a conference paper at ICLR 2022
Similarly, we show that E ⊇ E1 ∩ E2 . Indeed, suppose that E1 and E2 simultaneously occur. Conse-
quently the following inequality is valid:
R(Ak* (Sk))- R(w*)
=R(Ak*(Sk*)) - Rs's%* (Ak*(Sk*)) + Rs's%* (Ak*(Sk*)) - R(w*)
=R(Ak*(Sk*))- Rs's%* (Ak*(Sk*))+ min Rsa (Ak (Sk))- R(w*)
k∈[K]
=R(Ak*(Sk*)) - Rs∖s%* (Ak*(Sk*)) + min {Rs∖s%(Ak(Sk)) - R(Ak(Sk)) + R(Ak(Sk)) - R(w*)}
k∈[K]
≤ min(R(Ak (Sk)) - R(w*)) + 2 段筋限5(Ak (Sk)) - R(Ak(Sk ))∣
.1
α
,λ、— / IK Iog(K外)
/m, K +Apt) + My (K - 1)N ,
where in “Zi” We have used the definition of k*, and “Z2" follows from Ei, E2. With leading terms
preserved in the above we can see that E occurs.
Based on_the same proof argument as that of the part(b) of Theorem 1 we can show that
Ps,{Ak}(Ei) ≤ 2. Further, from the part(b) of Lemma 7 we have Ps,{Ak}(E2) ≤ 2. Combin-
ing this and the preceding bound yields
Ps,{Ak} (E) ≥ Ps,{Ak} (Ei ∩ E2) ≥ 1 - Ps,{Ak} (EI)- Ps,{Ak}(E2)≥ 1 - | - | = 1 - δ.
This implies the desired result in part(b) as K/(K -1) ≤ 2 for K ≥ 2.
□
C Proofs for the Results in Section 3
In this section, we present the technical proofs for the main results stated in Section 3.
C.1 Proof of Corollary 1
We begin with presenting and proving the following lemma that gives the mean(-square)-uniform
stability bounds for ASGD-w on convex and smooth loss functions such as logistic loss.
Lemma 8. Suppose that the loss function is '(•;•) is convex, G-Lipschitz and L-smooth with respect
to its first argument. Assume that ηt ≤ 2/L for all t ≥ 1. Then ASGD-w has mean-uniform stability
such that
2G
sup EASGD-w [kASGD-w(S) - ASGD-w(S )k] ≤ N Σ>,
s=. s0	N t∈[T]
and has mean-square-uniform stability such that
40G2
sup EASGD-w kASGD-w(S) -
ASGD-w(S0)k2] ≤ -ɪ
.0	N
Xb 褚+N
t=i
Proof. The first mean-uniform stability bound can be straightforwardly derived based on the argu-
ment of Hardt et al. (2016, Theorem 3.7). We focus on proving the second mean-square-uniform
stability bound. For any pair of S, S0, let us define the sequences {wt }t∈[T] and {wt0 }t∈[T] that are
respectively generated over S and S0 via ASGD-w via sample path ξ = {ξt}t∈[T]. Note by assumption
that w0 = w00 . We distinguish the following two complementary cases.
Case I:	zξt = zξ0 . In this case, by invoking Lemma 2 we immediately get
kwt - Wtk2 =kπW(Wt-I- Mw'(Wt-1； zξt)) - πw(WO-I- Mw'(W0-1； zξt))k2
≤kwt-i - ηtVw'(wt-1； Zξt) - (wt-1 - ηtVw'(w0-ι; zξt))k2	(A.3)
≤kWt-1 - Wt0-1k2.
17
Under review as a conference paper at ICLR 2022
Case II:	zξt 6= zξ0 . In this case, we have
∣∣wt - wtk2 =k∏W(wt-1 - ηtVf (w)) - ∏W(w0 - αVf (w0))k2
≤kwt-ι - ηvw'(Wt-1 zξj - (Wt-I- ηtvw'(W0-i； zξJ)k2
≤ (kwt-1 - Wt-Ik + ηt(kvw '(wt-1； zξt )k + kvw'(wt-i; zξ Jk))2	(A.4)
≤ (kwt-1 - Wt-Ik + 2Gηt)
=kWt-1 - Wtt-1k2 + 4GηtkWt-1 - Wtt-1k + 4G2ηt2,
where in the last but inequality We have used '(•;•) is G-LiPschitz with respect to its first argument.
Let βt = βt(S, St, ξ) := 1{z 6=z0 } be the random indication function associated with event zξt 6=
zξt . Based on the recursion forms equation A.3 and equation A.4 and the condition W0 = W0t we
can show that for all t ≥ 1,
tt
kWt - Wtt k2 ≤ X4GβτητkWτ-1 - Wτt-1k + X4G2βτητ2.
τ=1	τ=1
Then applying Lemma 3 with simple algebraic manipulation yields
kWt-Wttk2	≤8G2	IXt	βτητ2+4Xtβτ ητ!	j .
Since by assumption S and St differ only in a single element, under the scheme of uniform sampling
without replacement, we can see that βt(S, S0,ξ)〜 Bernoulli(1∕N) and {βt(S, S0,ξ)} is an
i.i.d. sequence of Bernoulli random variables. It follows that
Eξ[t] kWt - Wttk2
=8G2 IXb
≤8G2 (X
2
=8G2 IN5 XX ηT
τ=1
[βτβτ0 ] ητητ0
τ6=τ0
+N (XX ητ!j≤ 40G2 (事 XX ητ+N2 (XX ητ!
where we have used Eξt [βt] = Eξt [β2] = N. The convexity of squared Euclidean norm leads to
Eξ [∣wt - WTk2] ≤ P二I Eξ[t] TWt-Wtk2] ≤ 40G2 (N XX褚 + N
t=1
Note that the above holds for any S =. St, i.e.,
t 2	40G2	T 2	1
SUPoEξ [∣wt - WTk] ≤ ɪ (Xηt + N
The proof is concluded.
□
With Lemma 8 in place, we are ready to prove Corollary 1.
Proof of Corollary 1. From Lemma 8 we know that ASGD-w has mean-uniform stability with param-
eter
γm,N
2G
~Ν工
ηt,
t∈[T]
18
Under review as a conference paper at ICLR 2022
and mean-square-uniform stability with parameter
γm2,N
40G2
N
XX η22 + N
t=1
The desired results then follow immediately via invoking Theorem 1 with α = 1/2.
□
C.2 Proof of Corollary 2
We first establish the following lemma on the mean(-square)-uniform stability of ASGD-w in the case
of non-smooth convex loss.
Lemma 9. Suppose that the loss function is '(•;•) is convex and G-Lipschitz with respect to its first
argument. Then ASGD-w has mean-uniform stability such that
sup EASGD-w
S=. S0
[kASGD-w(S) -
ASGD-w(S0)k] ≤ 2G
ηt,
and has mean-square-uniform stability such that
sup EASGD-w
S=. S0
T	32G2
kA
SGD-w(S) -
ASGD-w(S0)k2] ≤ 40G2Eη2 + -NNr
t=1
Proof. Let us define the sequences {wt}t∈[T] and {wt0}t∈[T] that are respectively generated over S
and S0 via ASGD-w via sample path ξ = {ξt}t∈[T] . Suppose that S =. S0 and consider a hitting time
variable t0 = inf {t : zξt 6= zξ0 }. Let βt
function associated with event zξt 6= zξ0 .
(2020, Lemma 3.1) that
βt(S,S0,ξ)
1{zξt 6=zξ0 t }
be the random indication
Conditioned on t0 , it has been shown by Bassily et al.
kwt - wt0 k ≤ 2G
ητ2 + 4G	βτητ ≤ 2G
τ=t0	τ=t0 +1
u~^t	t
tuX ητ2+4GX βτητ.
τ=1	τ=1
(A.5)
t
t
Then we can show the following for all t ≤ T
Eξ[t] [kwt-wt0k] ≤2G
where we have used the fact that {βt} is an i.i.d. sequence of Bernoulli(1/N) random variables.
The convexity of Euclidean norm leads to
Eξ[τ] [kwT - WTk] ≤
PtT=1 Eξ[t] [kwt-wt0k]
≤ 2GtXηt + 4G Xηt,
t=1
t=1
T
which is the first desired bound. Similarly, based on the square of the bound equation A.5 we can
show that
t
Eξ[t] kwt-wt0k2] ≤Eξ[t] 8G2	ητ2 + -2G2
τ=1
2
βτ ητ
tt
=8G2 X ητ2 + -2G2Eξ[t] X βτ2ητ2 + Xβτβτ0ητητ0
τ=1	τ =1	τ 6=τ0
=8G2 X ητ2 + -2G2 (N X η2+N x % *
τ=1	τ=1	τ 6=τ0
t
≤40G2 Xητ2+
τ=1
-2G2 (XX
中哈
19
Under review as a conference paper at ICLR 2022
where We have used Eξt [βt] = Eξt [β2] = N. It follows directly from the convexity of loss that
T	32G2
Eξ[τ] [kwT - WTk ] ≤ 40G X ηt + N2
2
t=1
The proof is concluded.
Equipped with Lemma 9, we are now in the position to prove Corollary 2.
Proof of Corollary 2. From Lemma 9 we know that ASGD-w with non-smooth convex loss has
mean(-square)-uniform stability with parameters
TT
Ym,N = 2GuJX η2 + 4G X ηt,
t=1
t=1
Ym2,N=40G2E η2 + 3N
t=1
2
The desired results then follow immediately via invoking Theorem 1 with α = 1/2.
C.3 Proof of Corollary 3
We first establish the following lemma on the mean(-square)-uniform stability of ASGD-w in the
considered non-convex regime.
Lemma 10. Suppose that the loss function is '(•;•) is G -Lipschitz and L-smooth with respect to its
first argument. Consider ηt ≤ 1/L. Let
t-1	t-1
ut := ηt2 + 2ηt	exp L	ηi ητ
τ=1	i=τ +1
for all t ≥ 1. Then ASGD-w has mean-uniform stability such that
2G T	T
sup EASGD-w [kASGD-w(S) -
ASGD-w(SO)k] ≤ -N-EeXp LEnT ηt,
S=. S0	N t=1	τ=t+1
and has mean-square-uniform stability such that
4G2 T	T
sup EASGD-w kASGD-w (S) - ASGD-w(S0)k2] ≤ -N∑exp 3L	ητ ut.
S=S0	t=1	τ =t+1
Proof. Let us define the sequences {wt}t∈[T] and {wt0}t∈[T] that are respectively generated over
S and S0 via ASGD-w via sample path ξ = {ξt}t∈[T] . Suppose that S =. S0. Let us consider
∆t := Eξ[t] [kwt - wt0 k]. Then based on the arguments of Hardt et al. (2016, Theorem 3.8) we
know that with probability 1 - N over ξt, kwt - w0k ≤ (1 + ntL)∣∣wt-ι - w0-1k, and ∣∣wt - w0k ≤
∣∣wt-1 - w0-∕∣ + 2Gnt with probability NN. Therefore we have
≤ (1-	(1 + ntL)4tT + N
=((1- N)(1+ntL)+N) δ
-1 + 2Gnt )
ι + 2Gnt
1+ 1
≤ exp 1
N
NN
2Gnt
N
2Gnt
N
2Gnt
≤ exp (ntL) Δt-1 +
20
Under review as a conference paper at ICLR 2022
where we have used 1 + x ≤ exp(x). Then we can unwind the above recursion form to obtain that
for all t ≥ 1,
∆t ≤ XX [ YY exp(ηiL)[2Nτ = 2G XX exp (L XX η) %,
τ=1 i=τ +1	τ=1	i=τ +1
(A.6)
where we have used ∆0 = 0. The convexity of Euclidean norm leads to
Eξ[τ] [kwT - WT k] ≤
PT=IEξ["∣wt- wtk]
≤ 2G X eχp (L X ητ) ηt,
t=1	τ =t+1
T
which immediately implies the first desired bound as it holds for all S =. S0 .
To show the mean-square-uniform stability bound, let Us consider ∆t := Eξ^ ]∣∣wt - wt∣∣2]. Then
we can verify that with probability 1 - N over ξt, kwt - w0k2 ≤ (1 + ηtL)2kwt-ι - wt-ιk2,and
with probability N,
kwt - wt0 k2 ≤ (kwt-1 - wt0-1k +2Gηt)2 = kwt-1 - wt0-1k2 + 4Gηtkwt-1 - wt0-1 k + 4G2ηt2 .
Therefore we have
δt ≤ (1 - N)(I + r∏tLf δt-i + N (δt-i + 4GntA-I + 4G2η2)
≤
2	1	4G2
1 + ntL) + N) δt-1 + ɪ
/ ∖
t-1	/	t-1	∖
n2 + 2nt E eχp I L E ni I nτ I
τ=1	i=τ +1
V	{	}/
1+ (1 - N) (2ntL +
4G2ut
δ t-1 + 丁
≤eχp ((1 - N) (2ntL + n2L2)) δt-i +—N-t
≤ exp (2ntL + n2L2) ∆t-1 + 4GNut,
where in the second inequality we have used the bound equation A.6 on ∆t. Recall that ∆0 = 0.
Then we can unwind the above recursion form to obtain
∆t ≤ 4N2XXI YY exp(2n,L + n2L2)]UT ≤
τ =1 i=τ +1
4G2 t	t
NT TeXp 3L 工 ni uτ
τ=1	i=τ +1
where we have used nt ≤ 1/L. It follows immediately from the convexity that
Ilr 「11_	_0 ∣∣2] / Pt=1 Eξ[t] [kwt - w0k2] /4G2 XX (QT XX	ʌ
Eξ[τ] U∣ιυτ - WTIl J ≤	t	≤ n~ TeXp I 3L	nτ J ut
t=1	τ =t+1
which is the second desired bound. The proof is completed.
□
With Lemma 10 in place, we proceed to prove the main result in Corollary 3.
Proof of Corollary 3. From Lemma 10 we know that ASGD-w with smooth non-convex loss has
mean(-square)-uniform stability with parameters
γm,N
2G T
N 3 exp
L	nτ	nt ,
τ =t+1
4G2 T	T
Ym2,N = -N ^eXP (3L ∑ nτ) Ut.
The desired results then follow immediately via invoking Theorem 1 with α = 1/2.	□
21
Under review as a conference paper at ICLR 2022
Algorithm 3: SGD via Without-Replacement Sampling (ASGD-wo)
Input : Data set S ={zi}i∈[N] i吧. DN, step-sizes {ηt}t≥1, #iterations T, initialization w0.
Output： WT = T ∑t∈[τ] Wt.
for t = 1, 2, ..., T do
Uniformly randomly sample an index ξt ∈ [N] with or without replacement;
Compute Wt = ∏w (wt-i - ηNw'(wt-ι; zξt)).
end
D Augmented Results for SGD under Without-Replacement
Sampling
In this section, we further consider applying our main results in Theorem 1 to the variant of SGD
under without-replacement sampling (ASGD-wo), as is outlined in Algorithm 3. For the sake of sim-
plicity and readability, we only consider single-epoch processing with T ≤ N, and we focus on the
case where the loss is convex but non-smooth. The extensions of our analysis to multi-epoch pro-
cessing, i.e., T ≤ rN for some integer r ≥ 1, and to convex or non-convex smooth loss functions
are more or less straightforward and thus are omit.
We start by establishing the following lemma on the mean(-square)-uniform stability of ASGD-wo
which can be easily proved based on the result from Bassily et al. (2020, Lemma 3.1).
Lemma 11. Suppose that the lossfunction is '(•;•) is convex and G -Lipschitz with respect to its first
argument. Consider T ≤ N. Then ASGD-wo has mean-uniform stability such that
2G T
sup EASGD-wo [kAsGD-wo(S) - AsGD-wo(S 0)k] ≤ N £ ∖ £〃2，
S=. S0	N t0=i	t=t0
and has mean-square-uniform stability such that
4G2 T T
sup EASGD-wo kASGD-wo(S) -
ASGD-wo(S0)k2] ≤ -N ∑ ∑η2∙
S=S0	t0=i t=t0
Proof. Let WT(S,ξ) and WT(S0,ξ) respectively be the output generated over S = {zi}i∈[N] and
S0 = {zi0}i∈[N] by ASGD-wo via sample path ξ = {ξt}t∈[T] . Recall that T ≤ N. Let us define a
hitting time variable t0 = inf{t : zξt 6= zξ0 }. Since S =∙ S0, the uniform randomness of ξt implies
that
P(t0=j) = N, j ∈ [N]∙
Given t ∈ [T], it follows from Bassily et al. (2020, Lemma 3.1) that
t
kWt - Wt0k2 ≤ 4G2 X ητ2∙
τ=t0
Then we have
Eξ[t][kWt-W0 k2] ≤ 4G2 XX ητ≤ 4N2 XX ηT ∙
t0=i τ=t0	t0=i τ=t0
The convexity of squared Euclidean norm leads to
The proof is completed.
□
22
Under review as a conference paper at ICLR 2022
The following result is a direct consequence of Theorem 1 when invoking Algorithm 1 to ASGD-wo
with non-smooth convex loss.
Corollary 5. Suppose that the loss function is '(•;•) is convex and G-Lipschitz with respect to its
first argument, and is bounded in the range of [0, M]. Consider Algorithm 1 specified to ASGD-wo
with T ≤ N. Thenfor any δ ∈ (0,1) and K ≥ 2log(4), with probability at least 1 一 δ over the
randomness ofS and {ASGD-wo,k}k∈[K], the output of ASGD-wo satisfies
lR(ASGD-wo,k* (Sk* )) 一 RS (ASGD-wo,k*(Sk ))|
.G2u N X X 噬 + N X JX2 + M产KM.
t0=1 t=t0	t0=1	t=t0
Proof. From Lemma 11 we know that ASGD-wo has mean(-square)-uniform stability with parameters
2G T uu T 2	4G2 T T 2
γm,N = N X t Xη2, γm2,N =下工工％.
t0=1	t=t0	t0=1 t=t0
The results then follow immediately via invoking Theorem 1 with α = 1/2.
□
Remark 9. Specially for constant learning rates ηt ≡ η, Corollary 5 admits a high probability
(爆+ η字+
generalization bound of scale O
Cl弋δ
. For general time varying learning
rates, our bound in Corollary 5 still holds with high probability. For example, when ηt H 1, the
generalization bound scales as O
("
+√+
qwδr)
with high probability.
23