Under review as a conference paper at ICLR 2022
Soft Actor-Critic with Inhibitory Networks
for Faster Retraining
Anonymous authors
Paper under double-blind review
Ab stract
Reusing previously trained models is critical in deep reinforcement learning to
speed up training of new agents. However, it is unclear how to acquire new
skills when objectives and constraints are in conflict with previously learned skills.
Moreover, when retraining, there is an intrinsic conflict between exploiting what
has already been learned and exploring new skills. In soft actor-critic (SAC) meth-
ods, a temperature parameter can be dynamically adjusted to weight the action en-
tropy and balance the explore × exploit trade-off. However, controlling a single
coefficient can be challenging within the context of retraining, even more so when
goals are contradictory. In this work, inspired by neuroscience research, we pro-
pose a novel approach using inhibitory networks to allow separate and adaptive
state value evaluations, as well as distinct automatic entropy tuning. Ultimately,
our approach allows for controlling inhibition to handle conflict between exploit-
ing less risky, acquired behaviors and exploring novel ones to overcome more
challenging tasks. We validate our method through experiments in OpenAI Gym
environments.
1	Introduction
In reinforcement learning (RL), for all but the simplest tasks, agents behavior must be optimized
with respect to multiple goals and constraints (Sutton & Barto, 2018). It is common in practice for
new objectives and constraints to arise after already having trained an agent on existing ones. In
order for the behavior of the agent to account for these additional constraints, retraining is required.
Within the context of deep RL, retraining an agent to new constraints gives problems in balancing
exploitation of previously learned skills with learning new skills. In this paper, we present a method
for efficiently retraining an agent to acquire new skills in a similar environment.
Existing approaches to solving the problem of learning new skills include the use of hierarchical
RL structures (Dayan & Hinton, 1993; Barto & Mahadevan, 2003), such as the options framework
(Sutton et al., 1999; Comanici & Precup, 2010), universal value functions (Schaul et al., 2015),
option-critic (Bacon et al., 2017), FeUdal networks (Vezhnevets et al., 2017), and data-efficient
hierarchical RL (Nachum et al., 2018). Other methods use multiple policies and value functions
each of which are optimized for simple objectives that can then be composed to achieve complex
objectives (Van Seijen et al., 2017; Sahni et al., 2017; Haarnoja et al., 2017; Hansen et al., 2020;
Barreto et al., 2020). In this paper we propose to address the problem through the use of multiple
value functions to provide a complex evaluative input to a single policy network. By applying
different value functions in a state dependent fashion, the reward provided to the policy network
during training can remain the same as in prior training when appropriate, and can switch to a
different reward when the situation indicates new constraints or goals.
The mechanism we propose to govern the output of the composite value function is based upon
neuroscientific research on inhibitory control (Diamond, 2013). The brain uses inhibition to interrupt
ongoing goal-directed action when unexpected events or conflicts arise. The horse-race model of
Logan et al. (2015), which describes the behavioral response to a goal and its inhibition in terms
of dual processes, is supported by many behavioral and neurobiological studies (Verbruggen &
Logan, 2009; Shenoy et al., 2011; Ide et al., 2013; Schall et al., 2017). We implement this inhibitory
concept in soft actor-critic (SAC) algorithm (Haarnoja et al., 2018a) by using an additional value
network (inhibitory), as opposed to retraining the previously learned value network (ongoing), to
1
Under review as a conference paper at ICLR 2022
learn the new skill evaluation. Additionally, we propose the use of an inhibitory policy network to
control inhibition. We call this SAC with inhibitory networks approach SAC-I. SAC-I is distinct
from previous value composition work (Haarnoja et al., 2017; Van Niekerk et al., 2019), since it
proposes a specific mechanism to train and compose value networks and generate a single policy
network focused on fast and improved retraining. We hypothesize that retraining of RL agents can
be accelerated by creating independent and mutually inhibitory evaluative processes that will change
the reward function used during learning in a state dependent manner. SAC provides two important
features for the SAC-I: a replay buffer that can be partitioned into episodic memories related to each
evaluative-learning process (Botvinick et al., 2019) and an automated entropy estimation (Haarnoja
et al., 2018b), which allows computing two separate temperature parameters and exploring actions
differently.
There are two main contributions in this work. First, we develop the SAC-I architecture for
accelerated retraining, that encompasses the use of inhibitory networks for the control of multi-
ple evaluative networks. This approach modifies SAC methods by separating the learning pro-
cess, which includes training multiple value functions, storing episodic replay buffers, estimat-
ing distinct temperature parameters, and learning an inhibition policy when necessary (Section
3). Second, we provide a detailed validation showing the different components of SAC-I and
its improvements over SAC in two modified environments from OpenAI Gym (Section 4). The
LunarLanderContinuous-v2 with a bomb appearing randomly resembles the classic stop-signal
paradigm (Logan et al., 2015; Verbruggen & Logan, 2009) in inhibitory control studies. A mixed
version of the BipedalWalkerHardcore-v3, highlights the out-performance of SAC-I over SAC,
which is not able to successfully solve the task.
2	Background and Related Work
2.1	Maximum Entropy Reinforcement Learning
In RL, knowing the best way to explore while exploiting is non-trivial, environment-dependent,
and still an active area of research (Hong et al., 2018). Maximum entropy RL theory provides a
principled way to address this particular challenge, and has been a key element in many of the recent
RL advancements, providing improved exploration and faster learning (Thomas, 2014; Schulman
et al., 2017a; Haarnoja et al., 2017; 2018a;b; Ziebart, 2010). Given a Markov decision process
(MDP) with a set of states S, a set of actions A, a transition function T and a reward function R,
forming a tuple < S, A, T, R > (Puterman, 1994), a stochastic policy π : S → A is a mapping from
states to probabilities of selecting each possible action, where ∏(a∣s) represents the probability of
choosing action a given state s. In maximum entropy RL, as in an MDP, the goal is to find the
optimal policy ∏ that provides the highest expected sum of rewards, while additionally maximizing
the entropy of each visited state, leading to the expression (Ziebart, 2010):
π* = arg max 岁旧⑶小)〜ρ∏ [rt + αH(π( ∙ |st))],	(1)
where α is the temperature parameter that controls the stochasticity of the optimal policy, ρπ is
the state-action marginal of the trajectory distribution induced by the policy, rt is a shorthand
for the environmental reward r(st, at) at time t, and H(π) represents the entropy of the policy,
E[-log(∏(a∣s))]. This approach allows a state-wise balance between exploitation and exploration.
For states with high reward, a low entropy policy is permitted while, for states with low reward, high
entropy policies are preferred, leading to greater exploration. The discount factor γ is omitted in
the equation for simplicity since it leads to a more complex expression for the maximum entropy
case (Thomas, 2014). But it is required for the convergence of infinite-horizon problems, and it is
included in our final algorithm.
2.2	Soft Actor-Critic
Soft actor-critic (SAC) (Haarnoja et al., 2018a) is one of the most successful maximum entropy RL
methods and has become a common baseline algorithm in most of the RL libraries, outperform-
ing state-of-the-art methods Haarnoja et al. (2018a;b). Like the deep deterministic policy gradient
(DDPG) approach (Lillicrap et al., 2016), SAC is a model-free and off-policy method, using a replay
buffer, where the policy and value functions are approximated using neural networks. In addition,
2
Under review as a conference paper at ICLR 2022
it incorporates a policy entropy term into the objective function facilitating exploration, similar to
soft Q-learning (Haarnoja et al., 2017). Similar to trust region policy optimization (TRPO) (Schul-
man et al., 2015) and proximal policy optimization (PPO) Schulman et al. (2017b), SAC uses a
stochastic policy and is known to be more stable than DDPG. In short, SAC combines the best
of DDPG (sample efficiency) and TRPO/PPO (stability through stochastic policies). As expressed
in Equation 1, the SAC policy/actor is trained with the objective of maximizing the expected cu-
mulative reward and the action entropy at a particular state. The critic is the soft Q-function and,
following the Bellman equation, is expressed by: Q(st, at) = r + Y旧§叶1 〜p[V(st+ι)], where P
represents the state transition probability, and the soft value function is parameterized by the Q-
function: V(st+ι) = Eat〜∏[Q(st+ι,at+ι) 一 αlog π(at+ι ∣st+ι)]. The SoftQ-function is trained to
minimize the following objective function given by the mean squared error between predicted and
observed state-action values:
JQ = E(st,at)〜D 2 (Q(St,at) - (r + Y%+ι〜p[V(St+I)D)2 ,	⑵
where D denotes the replay buffer, and V is the target value function (Mnih et al., 2015). Finally, the
policy is updated to minimize the KL-divergence between the policy and the exponentiated state-
action value function (Haarnoja et al., 2018b), and can be expressed by:
Jn = Est〜D [Eat〜∏[αlog ∏(at∣st) ― Q(st,at)]] .	(3)
2.3	Inhibitory Control
Inhibitory control, also known as response inhibition, is a critical compo-
nent of the executive functions and refers to the ability to modify ongo-
ing actions in response to unexpected and dynamically changing task de-
mands (Aron, 2007; Diamond, 2013). In Shenoy et al. (2011), inhibitory
control is formalized as a rational decision-making problem, and a com-
putational model using Bayesian inference and stochastic control tools
is proposed and validated by behavioral data from humans and animals.
Using a widely adopted paradigm known as the stop-signal task (Logan
et al., 2015), authors show that the optimal policy, whether to go or stop,
systematically depends on accumulating sensory evidence, which sup-
ports the hypothesis that the brain is implementing statistically optimal
decision-making (Shenoy et al., 2011). Figure 1 depicts the two pro-
Figure 1: Dual process
model in the stop-signal
task (SST).
cesses involved in the stop-signal task. The Go process starts with a go signal followed by a stop
signal which triggers the stop process. The stop process will dominate when its activation is larger
than the Go process activation. The key assumption is that both processes are stochastically inde-
pendent, as supported by behavioral studies (Verbruggen & Logan, 2009). Stop-signal reaction time
(SSRT) is defined as the time necessary to respond to the stop stimulus. In further work using func-
tional MRI, the anterior cingulate cortex, a region in the brain implicated in a variety of cognitive
control functions, is shown to activate proportionally to a Bayesian prediction error between pre-
dicted and observed events resembling the temporal-difference methods in reinforcement learning
(Ide et al., 2013). These previous computational models indicate that when dealing with unexpected
events such as the stop signal the brain implements a dual-process model driven by prediction error
and responds to the conflict in an optimal way.
2.4	Related Work
Value function composition. In value function composition (Haarnoja et al., 2017; Van Niekerk
et al., 2019), the goal is to model a new task by composing value functions previously trained
on sub-tasks. In Todorov (2009), they show that for linearly solvable MDPs (Todorov, 2007), pre-
trained value function estimators can be optimally composed and solved. The composition is a union
of tasks, an “OR” composition, and is defined by taking the softmax over the component reward
signals. Van Niekerk et al. (2019) extends this result to the standard and entropy regularized RL
settings. Haarnoja et al. (2017) defines a composition rule that approximately solves the intersection
of tasks in the entropy regularized setting, an “AND” composition, with the composed signal as
an average over the constituent rewards. Despite the similarities, our approach is fundamentally
different since it composes a previously learned value function with one which is newly trained, and
moreover does not involve combining their value estimates.
3
Under review as a conference paper at ICLR 2022
Figure 2: SAC-I policy update.
Hybrid reward architecture. In Hybrid Reward Architecture (HRA) (Van Seijen et al., 2017), the
goal is to learn a complex task by decomposing its reward, and training separate value functions for
each component. The total reward is replaced by an equivalent representation as the sum, an “AND”
composition, of decomposed constituent rewards. They show that HRA learns more efficiently than
the deep Q-network algorithm (DQN) (Mnih et al., 2015) when both algorithms have otherwise
identical network architectures. SAC-I is similar to HRA in the training of multiple Q networks
using different rewards, however essentially different since it does not aim to combine the reward
values. In our approach, Q networks are trained independently and used to provide specialized
action values depending on the state, which is defined by the inhibition rule.
Transfer learning in RL. Broadly speaking, transfer learning in RL consists of transferring the
knowledge gained in one task to improve the learning performance in a related, but different task
(Taylor & Stone, 2009; Lazaric, 2012). This knowledge can be some type of learned representation
(Rusu et al., 2016b), reward shaping (Brys et al., 2015), demonstration (Schaal, 1996), model dy-
namics (Ammar et al., 2012), or policy (Rusu et al., 2016a). Our work is within the large field of
transfer learning, however we are primarily focused on transferring learned value and policy func-
tions among identical aspects of a task, while learning new skills (value functions) and retraining
the previously learned policy within the similar environment.
3	Methods
3.1	SAC-I: SAC with Inhibitory Networks
Inhibitory control is traditionally defined as the
ability to stop ongoing or planned cognitive or
motor processes, overriding impulsive or ha-
bitual responses (Aron, 2007). One possible
RL implementation is at the executive level by
switching between two policies using a hierar-
chical architecture. However, this approach re-
quires having to train multiple policies, and a
more complex hierarchical model. We propose
a computationally less expensive approach and
do the switching at the Q network level, since it will ultimately drive the change of the habitual re-
sponse or action. We implement inhibitory control in this broader sense at a higher cognitive level,
where the action execution (policy) is updated by the evaluative processes that precede execution.
We apply inhibitory control to the SAC algorithm by having multiple, competing value functions
(critics) that take turns depending on the current task demand. Rather than having a Q network that
learns the Go task and later the Stop task, we keep the previous Q network that knows to evaluate
the Go task and train a new Q-I network that learns the Stop task. Thus, both Q networks are trained
independently. Here, the term “inhibitory” refers to a new evaluative process replacing an ongoing
one. In Figure 2, we depict the SAC policy update with inhibitory networks. Go networks estimate
ongoing evaluation (which led to pre-trained skills), while the Stop networks estimate the value of
the new event, leading the policy to learn new skills. Additionally, we introduce the use of an in-
hibitory policy network to learn the inhibition policy responsible for deciding how and when to use
the outcomes from the multiple Q networks. Alternatively, to avoid having to train an additional
policy, a parametric state-based inhibition rule can be implemented using knowledge about the en-
vironment or updated constraints. We use Q networks to estimate all the value functions, following
the implementation in Haarnoja et al. (2018b), and the clipped double-Q learning trick, introduced
in twin delayed DDPG (TD3) (Fujimoto et al., 2018) to avoid their overestimation.
3.2	Episodic Memory through Selective Replay Buffer
In neuroscience research, episodic memory refers to the brain’s ability to recollect past experiences,
and is an important component of learning (Tulving, 2002). In recent years, it has been applied
to DRL as a non-parametric framework to retrieve past successful experiences to improve sample
efficiency (Blundell et al., 2016; Lin et al., 2018; Botvinick et al., 2019) or to avoid catastrophic
forgetting (Isele & Cosgun, 2018). In this work, we implement episodic memory in its simplest
form; a partition of the state space according to an inhibition rule or policy that yields a partition
4
Under review as a conference paper at ICLR 2022
of the replay buffer. Let S = {SR, SI } be a partition of the state space. The corresponding replay
buffer partition is D = {DR, DI} where set membership ofa tuple (sk, a, rk, s0) is parameterized by
sk for k ∈ {R, I} where sR ∈ SR and sI ∈ SI. For ease of notation, s0 = st+1, and the next state
can belong to any of the partitions, s0 ∈ S. Note that inhibitory states have an associated inhibitory
reward rI and regular states have the associated reward rR. These corresponding rewards are stored
in the replay buffers but the replay buffers are not parameterized by them. The loss function of each
Q network is computed using memories from the corresponding replay buffer as expressed by:
JQk = E(s,a)〜Dk 2 (Qk (s,a)-(rk+ Y Es,〜p[V(s0)]))2	for k ∈{R,I}.	(4)
Notice that if we use a single replay buffer that contains both the regular and the inhibitory re-
wards (s, a, rR, rI, s0), and sample a tuple containing the regular (s, a, rR, 0, s0) or the inhibitory
(s, a, 0, rI, s0) rewards depending on the updated Q network, we would have sparser rewards partic-
ularly for the Q-I network. Therefore, having separate memories promotes faster learning (as shown
in Figure 4) and, importantly, also allows the estimation of separate entropy parameters.
3.3	Automated Dual Entropy Estimation
The SAC algorithm is sensitive to the α temperature changes depending on the environment, reward
scale and training stage, as shown in the initial paper (Haarnoja et al., 2018a). To address this
issue, the same authors propose to automatically adjust the temperature parameter by formulating
the problem with a dual objective; maximize entropy while satisfying a minimum entropy constraint
(Haarnoja et al., 2018b). The goal is still maximizing the cumulative expected reward (Equation 1),
but the average entropy of the policy is now constrained by a minimum value. The full derivation
of the dual optimization problem is given in Haarnoja et al. (2018b). In practice, the optimization
is performed recursively as follows: at time t, given the current estimate at, the optimal policy ∏t
is estimated as described in Equation 1. Subsequently, given πt, the αt is approximated using a
neural network. While in the standard SAC, there is a single α parameter, in SAC-I, we propose
estimating two separate α temperature parameters, αR and αI, to allow distinct action entropy for
the previously learned and the new skills, respectively. This is implemented by training two separate
α networks, with regular states sR ∈ SR distinguished from inhibitory states sI ∈ SI, with losses
given by:
Jak = Ea〜∏[-ak logπ(a | Sk) - αkHo],	for k ∈ {R,I},	(5)
where H0 is the minimum expected entropy. The policy loss function is composed of two terms:
Jn = E Esk〜Sk [Ea〜∏[αk log∏(a∣Sk) - Qk(Sk,a)]] .	(6)
k∈{R,I}
3.4	Inhib itory Policy Network
In our proposed SAC-I algorithm (Figure 2), for the cases in which a state-dependent inhibition rule
is not defined, we propose training an inhibitory policy network πI . This network can be trained
as an automated hard switch or a soft modulator between the regular and inhibitory Q networks.
The inhibitory policy network is a stand-alone agent with its own Q networks (not shown in Figure
2), however it shares the same replay buffers. The loss functions are the standard ones as defined
in Equations 2 and 3. Ultimately, the goal of the inhibitory policy network is to maximize the
environment’s reward by learning to choose between the Go and Stop networks and/or by modulating
the Stop network. We show the implementation of both cases next in Section 4.
SAC-I is summarized in Algorithm 1.
4	Experiments and Results
In order to show two different use-cases of SAC-I algorithm, as well as to evaluate it as a way
to speed up training during transfer learning, we use continuous tasks from the Box2D simulator,
OpenAI Gym (Brockman et al., 2016), LunarLanderContinuous-v2 and BipedalWalkerHardcore-v3.
Importantly, we include custom modifications to them to emulate the scenarios in which retraining
is necessary. By adding a random bomb in the LunarLander task, we show the advantages of using
5
Under review as a conference paper at ICLR 2022
Algorithm 1: Soft Actor Critic with Inhibitory Networks (SAC-I)
Initialize QR1, QR2, QI1, QI2, policy π, policy πI, αR and αI networks parameters;
Initialize the target Qr` , Qr2 , Q八 and Q4 networks weights;
Initialize the replay buffers DR and DI ;
for each episode do
for each environment step do
Given st, sample at from π(st) and (st+1, rt) from the environment;
Use an inhibition rule or inhibitory policy πI to classify st ;
ifst ∈ SR, push (st,at,rRt, st+1) to DR;
else if st ∈ SI push (st,at,rIt,st+1) toDI;
end
for each gradient step do
Sample a batch of memories from from {DR , DI};
for k ∈ {R, I} do
I Update Qk1 and Qk2 (Equation 4), ak (Equation 5), and Qk1 and Qk2 (soft-update);
end
Update the policy network π (Equation 6);
If an inhibitory policy network is used, update πI and the associated Q networks;
end
end
SAC-I, compared to standard SAC, when retraining with conflicting goals, similar to what happens
in a stop-signal task (Logan et al., 2015), i.e. stopping an ongoing action (to land) whenever an
unexpected event occurs (to avoid bomb). In the experiments with BipedalWalkerHardcore-v3,
we train agents in a simpler version of the task (BipedalWalker-v3), and retrain them in the more
complex task. We show how SAC-I can help transfer learning and adjust inhibitory control. In all
the experiments provided in this section, we used five random seeds to account for the variability
during training and compare agents trained across a fixed number of steps. All the hyperparameters
are available in Table 1 (Appendix).
4.1	LunarLanderContinuous with Bomb
Environment. The original version of LunarLanderContinuous-v2 is modified in order to include
a bomb that appears randomly within a region above the landing pad (Figure 3). Like the original
version, it includes the environment reward (moving from the top of the frame to the landing pad:
100-140 points, each leg contact: +10, crashing: -100, successful landing: +100, firing engine: -0.3
per frame), but additionally it includes a penalty for hitting the bomb (-150) and a time penalty
(-0.1 per frame) to motivate landing as quickly as possible (like in SST). Importantly, the bomb
coordinates are included in the observation state only after the bomb appears so the agent does not
know about its existence beforehand. Further details can be found in Appendix.
Figure 3: LunarLanderContinuous-v2 with Bomb. (a) Episode starts with the lander placed ran-
domly on top of the frame around the center. (b) Go trial: refers to an episode without a bomb, and
it is exactly as the original environment. (c) Stop trial: refers to episodes in which a bomb randomly
appears close to the landing pad (between the two flags). (d) It shows a successful landing during a
stop trial after avoiding hitting the bomb. By default, the bomb appears in 50% of the episodes.
Experimental design. Initially, a standard SAC agent is trained in the original environment
LunarLanderContinuous-v2. This agent, which we will call “baseline agent”, takes about 250K
steps to reach an average cumulative reward of 200 and is able to successfully land in most of
the episodes. All the network weights are transferred to the retrained agent. The Q-I network is
trained from scratch. In this experiment, the inhibition rule/policy works as a switch between the
6
Under review as a conference paper at ICLR 2022
regular and inhibitory Q networks. We show both cases, user-defined inhibition rule and learned
inhibitory policy network. Experiments are performed in a single machine with Intel Core i7-9850H
CPU@2.60Hz x 12, Quadro RTX3000, RAM 16GB. The average rewards reported in the figures
represent the average of the episode reward, including the bomb penalty, over the last 100 episodes.
Agents are trained for a fixed number of 2K episodes, approximately 500K steps.
Advantage of retraining. In this experiment, we show how the performance of a standard SAC
agent is boosted by retraining it using weights from the baseline agent, which already knows how to
safely land. In order to learn bomb avoidance skills, we shaped the original reward with an additional
penalty given by the expression rbomb.proxy = -1e4 × (db - 0.3)4, where db is defined as the agent’s
distance to the center of the bomb (xb, yb). The idea is to have a field-type avoidance mechanism
and give a penalty proportional to bomb proximity. Average reward results are depicted in Figure 4,
orange and blue lines. For retraining, all the network weights are loaded from the baseline agent and
updated in the new task with bomb. The retrained agent (blue line) clearly learns to complete the task
(land and avoid the bomb) faster than the agent trained from scratch (orange line). In about 300K
steps, it starts to avoid the bomb and at 500K steps is mostly able to complete the task successfully.
The agent from scratch takes > 1.25M steps to learn the task.
Effect of using episodic memory and dual alpha in SAC-I.
We parse out the distinct contributions of different components
of SAC-I by training agents with and without episodic mem-
ory and the dual α temperature parameters. In these versions,
we use a user defined inhibition rule given by y > yb and
db < 0.3. If these conditions are met, the inhibitory Q-I net-
work is used with the same shaping used for the SAC agent,
rI = rbomb.proxy. Results are shown in Figure 4. The SAC-
I vanilla agent (green line) is trained without episodic mem-
ory and with a single α parameter. The SAC-I with episodic
memory (purple line) is trained with separate experience re-
play buffer for the inhibitory states, but uses only a single en-
Figure 4: Comparison of different
SAC and SAC-I agents.
tropy parameter α. The SAC-I agent with episodic memory and dual α (red line) has faster learning,
reaching an average reward of 200 after 300K steps, and highlighting the importance of these com-
ponents1.
SAC-I performance for different conflict levels. The brain’s inhibitory control is known to be
modulated by levels of conflict between ongoing processes (Braver et al., 2001). For instance in
the SST, changing the frequency of stop signals, which is associated with expectation, alters the
response time to go signals. In a similar way, we investigate whether and how the frequency of
stop trials (episodes with bomb) impacts the agent’s learning, other than the overall performance
which is expected to decrease for increased occurrence of bombs. We hypothesize that the bomb
frequency will impact SAC-I less than SAC because it trains separate Q networks for different skills
involved in the task (i.e. landing and bomb avoidance). Both SAC and SAC-I agents are trained with
varying bomb frequencies, and results are shown in Figure 5. As expected, the overall performance
Figure 5: Performance of SAC-I and SAC agents for different bomb frequencies.
decreases for higher frequency. SAC-I agent reaches an average reward of 200 in about 200K, 300K
and 600K for frequencies 25%, 50% and 75%, respectively. For the SAC agent, it is clear that bomb
frequency not only affects performance but also the training progression. For example, a significant
drop in performance is observed around step 50K for Bomb 75% as well as for 50%, although less.
This likely happens because learning to avoid the bomb interferes with its initial ability to land.
1Further SAC-I agents are all trained with episodic memory and dual α parameters estimation.
7
Under review as a conference paper at ICLR 2022
Interestingly, although both agent’s performances converge asymptotically, the difference is larger
for Bomb 50% case, in which the uncertainty is the highest. This is likely because SAC and SAC-I
agents learn different strategies to avoid the bomb. While the SAC agent adopts an optimal global
strategy accordingly to different bomb frequencies (i.e, if bomb is frequent, slow down when task
starts), the SAC-I agent keeps the same landing strategy independent of bomb frequency and learns
to avoid the bomb when it appears.
Training without reward shaping. Further, we examined the
training of SAC and SAC-I agents without using any reward
shaping for the bomb avoidance. Both agents use the same
reward r = r0 + rbomb , where r0 is the original reward and
rbomb is a sparse penalty for hitting the bomb. Results2 are
shown in Figure 6. Surprisingly, the SAC* agent without shap-
ing (yellow line) performed better than the one with, mean-
ing that shaping is negatively impacting its training. In con-
trast, the SAC-I* agent without shaping (gray line) success-
fully learns the task with similar speed to the one with shaping
(red line), likely because it keeps separate critic networks to
evaluate landing and bomb avoidance. In further experiments,
Figure 6: SAC and SAC-I agents
with and without shaping.
we observed that different reward shaping can boost SAC-I’s performance even more, but not SAC
(see results with conservative shaping in Appendix, Figure 10).
Learning when to inhibit. In this experiment, we show an agent trained with an inhibitory policy
network SAC-I** and compare it to an agent trained with an inhibition rule SAC-I*. The SAC-I*
agent (gray line) uses a simple inhibition rule (“whenever bomb appears”) to switch between the
regular and inhibitory Q networks (Figure 2). While, the SAC-I** agent (brown line) learns an
inhibition policy, i.e. the best timing to inhibit. This SAC-I** agent (brown line) performs as good
as the SAC-I* and outperforms the SAC agents (Figure 6).
4.2	B ipedalWalkerHardcore-v3
In this evaluation, we show a different use of inhibitory networks, to control the magnitude of inhi-
bition. To create the retraining scenario, first, we train a standard SAC agent in an easier environ-
ment BipedalWalker-v3 (Figure 7a), in which the goal is to walk through a plain terrain, and use
its weights to retrain SAC and SAC-I agents in BipedalWalkerHardcore-v3 (Figure 7b-d). In the
BipedalWalker task, agents naturally get a negative “inhibitory” reward whenever they are stuck.
We use SAC-I to learn as well as to weight that negative reward.
Figure 7: BipedalWalker environment. (a) Plain terrain: agent has to learn to walk forward; (b)
Stuck position: agent starts to receive negative reward since it is spending energy without moving
forward; (c) Fall: agent can fall because it loses balance, stumbles itself, or fall into a hole; (d)
Overcoming obstacles: the agent learns avoiding the holes or going over the blocks.
The BipedalWalkerHardcore-v3 is a challenging task, known to be unsolvable for many of the sim-
pler non-recurrent DRL architectures or model-free RL methods (Wei & Ying, 2021). We solve the
task using a standard two-layer dense-network architecture, and adopt two strategies: removing the
fall-penalty and creating a cumulative version of the task reward. Otherwise, the task is unsolvable
with SAC algorithm (see Figure 11 in Appendix).
Learning how to inhibit. In Figure 8, we show the training performance of SAC and two
versions of SAC-I agents learning BipedalWalkerHardcore-v3. All agents are retrained from
baseline. The SAC agent reward consists of the raw environment reward r0 , but with a
cumulative reward and without the fall penalty3, expressed by r = r0 + rstuck - rfall.
2SAC and SAC-I agents from Figure 4 are also shown to facilitate comparison, using the same color coding.
3Mimicking life, harsh fall penalty seems to block learning. See Figure 11 in Appendix.
8
Under review as a conference paper at ICLR 2022
The cumulative reward is defined by rstuck = Pi5=0 rt-i, ac-
counted if it is negative, i.e. the agent is at a “stuck” state.
The cumulative reward provides a less noisy feedback. The pri-
mary goal of SAC-I (standard) is to separately train a Q-I critic
network specialized on modeling the new reward structure, i.e.
rI = r0 + rstuck - rfall . The SAC-I* agent (adaptive) learns
an inhibitory policy network to estimate the weight w so that
rɪ* = ro + W X『stuck - rfaiι. ThiS SAC-1* agent With adap-
tive inhibition (green line) outperforms the other agents, and the
Figure 8: Performance of
SAC and SAC-I agentS.
difference iS highlighted in the mixed verSion of the taSk (See further Figure 9).
Mixed version of BipedalWalkerHardcore-v3. To replicate the Scenario in Which there are go
and Stop epiSodeS, We retrain the baSeline agent on a modified taSk mixing both the BipedalWalker-
v3 (Go trial) and BipedalWalkerHardcore-v3 (Stop trial). For each epiSode, We randomly chooSe
in Which environment the agent Should perform. IntereStingly, thiS makeS the learning even more
challenging, becauSe the obStacleS are SparSer in time and the agent haS to explore and exploit While
learning the eaSy and hard verSionS of the taSk SimultaneouSly. In Figure 9, We ShoW that uSing
inhibitory netWorkS iS critical to SucceSSfully learn the mixed taSk. Both agentS are retrained uSing
the Same reWard Structure aS preSented in the previouS Section. Although the taSk getS eaSier aS the
percent of go trialS iS increaSed, from 10% to 30%, We obServe that it iS more difficult to learn the
mixed verSion of the taSk Since there are leSS Stop trialS to learn from (for inStance, compare the tWo
plotS in the middle column). AlSo, We obServe Some training inStability for 70% Stop trial, likely
becauSe the interference betWeen learning the Stop and Go trialS (3rd column). IntereStingly, We
obServe that the SAC-I* agent With adaptive inhibition haS more Stability acroSS the Stop training
SeSSionS. For Stop 90%, the averageS of the reWard Standard deviation are 47.5 and 27.5 for SAC-I
and SAC-I* agentS, reSpectively. For Stop 70%, the averageS are 56.3 and 31.7, reSpectively. Finally,
We obServe that, unlike the SAC agent, only the SAC-I agentS are able to SucceSSfully learn the Stop
trialS (2nd column). Note, they are trained uSing the Same reWard Structure (previouS Section).
Figure 9: SAC and SAC-I agentS performance in the mixed verSion of the BipedalWalkerHardcore-
v3. FirSt roW ShoWS reSultS for the taSk With 90% Stop trialS (hardcore), While the Second, With 70%.
From the left to the right, reWardS are averaged acroSS all, only Stop and only Go trialS.
5	Conclusions
In thiS Work, We draW a parallel betWeen neuroScience reSearch in inhibitory control and reinforce-
ment learning of competing value functionS. We propoSe an SAC algorithm uSing inhibitory net-
WorkS With a particular focuS on retraining the agent to acquire a neW Skill, While Still exploiting
previouSly learned abilitieS. Through our experimentS, We ShoW that SAC-I agentS are able to main-
tain higher reWardS from the beginning of retraining Since they keep a previouS ability compared to
retrained SAC agentS (Figure 6). With SAC-I, We advance the uSe of SAC methodS by introduc-
ing the uSe of multiple value netWorkS With reSpective epiSodic replay bufferS, aS Well aS diStinct
entropy parameter eStimation for SkillS at different training StageS. In Figure 9, We ShoW SAC-I iS
able to SucceSSfully learn the mixed verSion of the BipedalWalkerHardcore-v3, otherWiSe unSolvable
With the Standard SAC. We believe that the experimentS and reSultS preSented in thiS paper offer a
proof of concept of the advantageS uSing SAC methodS With inhibitory netWorkS (SAC-I) for faSter
retraining.
9
Under review as a conference paper at ICLR 2022
References
Haitham B. Ammar, Karl Tuyls, Matthew E. Taylor, Kurt Driessens, and Gerhard Weiss. Reinforce-
ment learning transfer via sparse coding. In Proceedings of the 11th International Conference
on Autonomous Agents and Multiagent Systems - Volume 1, AAMAS ’12, pp. 383390, Rich-
land, SC, 2012. International Foundation for Autonomous Agents and Multiagent Systems. ISBN
0981738117.
Adam R. Aron. The neural basis of inhibition in cognitive control. The Neuroscientist, 13(3):
214-228,2007. doi: 10.1177/1073858407299288. PMID: 17519365.
Pierre-Luc Bacon, Jean Harb, and Doina Precup. The option-critic architecture. In Proceedings
of the Thirty-First AAAI Conference on Artificial Intelligence, AAAI’17, pp. 17261734. AAAI
Press, 2017.
Andre Barreto, Shaobo Hou, Diana Borsa, David Silver, and Doina Precup. Fast reinforcement
learning with generalized policy updates. Proceedings of the National Academy of Sciences, 117
(48):30079-30087, 2020. ISSN 0027-8424. doi: 10.1073/pnas.1907370117. URL https:
//www.pnas.org/content/117/48/30079.
Andrew G. Barto and Sridhar Mahadevan. Recent advances in hierarchical reinforcement learn-
ing. Discrete Event Dynamic Systems, 13(12):4177, 2003. ISSN 0924-6703. doi: 10.1023/A:
1022140919877.
Charles Blundell, Benigno Uria, Alexander Pritzel, Yazhe Li, Avraham Ruderman, Joel Z Leibo,
Jack Rae, Daan Wierstra, and Demis Hassabis. Model-free episodic control, 2016.
Matthew Botvinick, Sam Ritter, Jane X Wang, Zeb Kurth-Nelson, Charles Blundell, and Demis
Hassabis. Reinforcement learning, fast and slow. Trends in cognitive sciences, 23(5):408422,
May 2019. ISSN 1364-6613. doi: 10.1016/j.tics.2019.02.006. URL https://doi.org/10.
1016/j.tics.2019.02.006.
Todd S. Braver, Deanna M. Barch, Jeremy R. Gray, David L. Molfese, and Avraham Snyder. Ante-
rior Cingulate Cortex and Response Conflict: Effects of Frequency, Inhibition and Errors. Cere-
bral Cortex, 11(9):825-836, 09 2001. ISSN 1047-3211. doi: 10.1093/cercor/11.9.825. URL
https://doi.org/10.1093/cercor/11.9.825.
Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman, Jie Tang, and
Wojciech Zaremba. Openai gym. arXiv preprint arXiv:1606.01540, 2016.
Tim Brys, Anna Harutyunyan, Matthew E. Taylor, and Ann Nowe. Policy transfer using reward
shaping. In Proceedings of the 2015 International Conference on Autonomous Agents and Multi-
agent Systems, AAMAS ’15, pp. 181188. International Foundation for Autonomous Agents and
Multiagent Systems, 2015. ISBN 9781450334136.
G. Comanici and D Precup. Optimal policy switching algorithms for reinforcement learning. In
Proceedings of the 9th International Conference on Autonomous Agents and Multiagent Systems
(AAMAS 2010), pp. 709-714, 2010.
Peter Dayan and Geoffrey E Hinton. Feudal reinforcement learning. In S. Hanson, J. Cowan,
and C. Giles (eds.), Advances in Neural Information Processing Systems, volume 5. Morgan-
Kaufmann, 1993.
Adele Diamond. Executive functions. Annual Review of Psychology, 64:135-68, 2013.
S. Fujimoto, H. van Hoof, and D. Meger. Addressing function approximation error in actor-critic
methods. In Proceedings of the 35th International Conference on Machine Learning, volume 80,
pp. 1582-1591. PMLR, 2018.
Tuomas Haarnoja, Haoran Tang, Pieter Abbeel, and Sergey Levine. Reinforcement learning with
deep energy-based policies. In Doina Precup and Yee Whye Teh (eds.), Proceedings of the 34th
International Conference on Machine Learning, volume 70 of Proceedings of Machine Learning
Research, pp. 1352-1361, International Convention Centre, Sydney, Australia, 06-11 Aug 2017.
PMLR.
10
Under review as a conference paper at ICLR 2022
Tuomas Haarnoja, Aurick Zhou, Pieter Abbeel, and Sergey Levine. Soft actor-critic: Off-policy
maximum entropy deep reinforcement learning with a stochastic actor. In Jennifer Dy and An-
dreas Krause (eds.), Proceedings of the 35th International Conference on Machine Learning,
volume 80 of Proceedings of Machine Learning Research, pp. 1861-1870, Stockholmsmssan,
Stockholm Sweden, 10-15 JUl 2018a. PMLR. URL http://Proceedings .mlr.press/
v80/haarnoja18b.html.
Tuomas Haarnoja, Aurick Zhou, Kristian Hartikainen, George Tucker, Sehoon Ha, Jie Tan, Vikash
Kumar, Henry Zhu, Abhishek Gupta, Pieter Abbeel, and Sergey Levine. Soft actor-critic algo-
rithms and applications. CoRR, abs/1812.05905, 2018b. URL http://arxiv.org/abs/
1812.05905.
Steven Hansen, Will Dabney, Andre Barreto, David Warde-Farley, Tom Van de Wiele, and
Volodymyr Mnih. Fast task inference with variational intrinsic successor features. In Interna-
tional Conference on Learning Representations, 2020. URL https://openreview.net/
forum?id=BJeAHkrYDS.
Zhang-Wei Hong, Tzu-Yun Shann, Shih-Yang Su, Yi-Hsiang Chang, Tsu-Jui Fu, and Chun-Yi Lee.
Diversity-driven exploration strategy for deep reinforcement learning. In S. Bengio, H. Wallach,
H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Garnett (eds.), Advances in Neural Infor-
mation Processing Systems, volume 31. Curran Associates, Inc., 2018.
Yujing Hu, Weixun Wang, Hangtian Jia, Yixiang Wang, Yingfeng Chen, Jianye Hao, Feng
Wu, and Changjie Fan. Learning to utilize shaping rewards: A new approach of reward
shaping. In H. Larochelle, M. Ranzato, R. Hadsell, M. F. Balcan, and H. Lin (eds.), Ad-
vances in Neural Information Processing Systems, volume 33, pp. 15931-15941. Curran As-
sociates, Inc., 2020. URL https://proceedings.neurips.cc/paper/2020/file/
b710915795b9e9c02cf10d6d2bdb688c- Paper.pdf.
J.S. Ide, P. Shenoy, A.J. Yu, and C.S. Li. Bayesian prediction and evaluation in the anterior cingulate
cortex. J Neurosci, 33(5):2039-47, 2013.
David Isele and Akansel Cosgun. Selective experience replay for lifelong learning. In Sheila A.
McIlraith and Kilian Q. Weinberger (eds.), Proceedings ofthe Thirty-Second AAAI Conference on
Artificial Intelligence, (AAAI-18), the 30th innovative Applications of Artificial Intelligence (IAAI-
18), and the 8th AAAI Symposium on Educational Advances in Artificial Intelligence (EAAI-18),
New Orleans, Louisiana, USA, February 2-7, 2018, pp. 3302-3309. AAAI Press, 2018. URL
https://www.aaai.org/ocs/index.php/AAAI/AAAI18/paper/view/16054.
Alessandro Lazaric. Transfer in Reinforcement Learning: A Framework and a Survey, pp. 143-173.
Springer Berlin Heidelberg, Berlin, Heidelberg, 2012. ISBN 978-3-642-27645-3. doi: 10.1007/
978-3-642-27645-3.5. URL https://doi.org/10.10 07/97 8-3-64 2-27 64 5-3_5.
Timothy P. Lillicrap, Jonathan J. Hunt, Alexander Pritzel, Nicolas Heess, Tom Erez, Yuval Tassa,
David Silver, and Daan Wierstra. Continuous control with deep reinforcement learning. In Yoshua
Bengio and Yann LeCun (eds.), 4th International Conference on Learning Representations, (ICLR
2016), 2016. URL http://arxiv.org/abs/1509.02971.
Zichuan Lin, Tianqi Zhao, Guangwen Yang, and Lintao Zhang. Episodic memory deep q-networks.
In Proceedings ofthe 27th International Joint Conference on Artificial Intelligence, IJCAI’18, pp.
24332439. AAAI Press, 2018. ISBN 9780999241127.
G. D. Logan, M. Yamaguchi, J. D. Schall, and T. J. Palmeri. Inhibitory control in mind and brain
2.0: Blocked-input models of saccadic countermanding. Psychological Review, 122(2):115-147,
2015. doi: https://doi.org/10.1037/a0038893.
Andrew J. Martin. Fear of Failure in Learning, pp. 1276-1278. Springer US, Boston, MA, 2012.
ISBN 978-1-4419-1428-6. doi: 10.1007/978-1-4419-1428-6.266. URL https://doi.org/
10.1007/978-1-4419-1428-6_266.
Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A. Rusu, Joel Veness, Marc G. Belle-
mare, Alex Graves, Martin Riedmiller, Andreas K. Fidjeland, Georg Ostrovski, Stig Petersen,
11
Under review as a conference paper at ICLR 2022
Charles Beattie, Amir Sadik, Ioannis Antonoglou, Helen King, Dharshan Kumaran, Daan Wier-
stra, Shane Legg, and Demis Hassabis. Human-level control through deep reinforcement learn-
ing. Nature,518:529-533, 2015. ISSN00280836. URL http://dx.doi.org/10.1038/
nature14236.
O. Nachum, S. Gu, H. Lee, and S. Levine. Data-efficient hierarchical reinforcement learning. Ad-
vances in Neural Information Processing Systems, pp. 3303-3313, 2018.
Martin L Puterman. Markov decision processes: discrete stochastic dynamic programming. John
Wiley & Sons, 1994.
Andrei A. Rusu, Sergio Gomez Colmenarejo, Caglar Gulcehre, Guillaume Desjardins, James Kirk-
patrick, Razvan Pascanu, Volodymyr Mnih, Koray Kavukcuoglu, and Raia Hadsell. Policy distil-
lation, 2016a.
Andrei A. Rusu, Neil C. Rabinowitz, Guillaume Desjardins, Hubert Soyer, James Kirkpatrick,
Koray Kavukcuoglu, Razvan Pascanu, and Raia Hadsell. Progressive neural networks. CoRR,
abs/1606.04671, 2016b. URL http://arxiv.org/abs/1606.04671.
Himanshu Sahni, Saurabh Kumar, Farhan Tejani, and Charles L. Isbell Jr. Learning to compose
skills. CoRR, abs/1711.11289, 2017. URL http://arxiv.org/abs/1711.11289.
Stefan Schaal. Learning from demonstration. In Proceedings of the 9th International Conference on
Neural Information Processing Systems, NIPS’96, pp. 10401046, Cambridge, MA, USA, 1996.
MIT Press.
J.D. Schall, T.J. Palmeri, and G.D. Logan. Models of inhibitory control. Philos Trans R Soc Lond B
Biol Sci., 372(1718), 04 2017. doi: 10.1098/rstb.2016.0193.
Tom Schaul, Daniel Horgan, Karol Gregor, and David Silver. Universal value function approxima-
tors. In Francis Bach and David Blei (eds.), Proceedings of the 32nd International Conference
on Machine Learning, volume 37 of Proceedings of Machine Learning Research, pp. 1312-1320,
Lille, France, 07-09 Jul 2015. PMLR.
John Schulman, Sergey Levine, Pieter Abbeel, Michael Jordan, and Philipp Moritz. Trust region
policy optimization. In Francis Bach and David Blei (eds.), Proceedings of the 32nd International
Conference on Machine Learning, volume 37 of Proceedings of Machine Learning Research, pp.
1889-1897, Lille, France, 07-09 Jul 2015. PMLR.
John Schulman, Pieter Abbeel, and Xi Chen. Equivalence between policy gradients and soft q-
learning. CoRR, abs/1704.06440, 2017a. URL http://arxiv.org/abs/1704.06440.
John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy
optimization algorithms. CoRR, abs/1707.06347, 2017b. URL http://arxiv.org/abs/
1707.06347.
B. Seymour, T. Singer, and R Dolan. The neurobiology of punishment. Nat Rev Neurosci, 8:300-
311, 2007. URL https://doi.org/10.1038/nrn2119.
Pradeep Shenoy, RP Rao, and AJ Yu. A rational decision making framework for inhibitory control.
In Advances in Neural Information Processing Systems (NIPS), volume 23, pp. 2146-2154. MIT
Press, 2011.
Richard S. Sutton and Andrew G. Barto. Reinforcement Learning: An Introduction. A Bradford
Book, Cambridge, MA, USA, 2018. ISBN 0262039249.
Richard S. Sutton, Doina Precup, and Satinder Singh. Between mdps and semi-mdps: A framework
for temporal abstraction in reinforcement learning. Artif. Intell., 112(12):181211, 1999. ISSN
0004-3702. doi: 10.1016/S0004-3702(99)00052-1.
Matthew E. Taylor and Peter Stone. Transfer learning for reinforcement learning domains: A survey.
Journal of Machine Learning Research, 10(1):1633-1685, 2009.
12
Under review as a conference paper at ICLR 2022
Philip Thomas. Bias in natural actor-critic algorithms. In Eric P. Xing and Tony Jebara (eds.), Pro-
ceedings of the 31st International Conference on Machine Learning, volume 32 of Proceedings
ofMachine Learning Research, pp. 441-448, Bejing, China, 22-24 JUn 2014. PMLR.
EmanUel Todorov. Linearly-Solvable markov decision problems. In B. Scholkopf, J. Platt,
and T. Hoffman (eds.), Advances in Neural Information Processing Systems, volUme 19.
MIT Press, 2007. URL https://proceedings.neurips.cc/paper/2006/file/
d806ca13ca3449af72a1ea5aedbed26a- Paper.pdf.
EmanUel Todorov. Compositionality of optimal control laws. In Y. Bengio, D. SchUUrmans, J. Laf-
ferty, C. Williams, and A. CUlotta (eds.), Advances in Neural Information Processing Systems,
volUme 22. CUrran Associates, Inc., 2009. URL https://proceedings.neurips.cc/
paper/2009/file/3eb71f6293a2a31f3569e10af6552658-Paper.pdf.
Endel TUlving. Episodic memory: from mind to brain. Annual Review of Psychology, 53:1-25,
2002.
Benjamin Van Niekerk, Steven James, Adam Earle, and Benjamin Rosman. Composing valUe
fUnctions in reinforcement learning. In Kamalika ChaUdhUri and RUslan SalakhUtdinov (eds.),
Proceedings of the 36th International Conference on Machine Learning, volUme 97 of Proceed-
ings of Machine Learning Research, pp. 6401-6409. PMLR, 09-15 JUn 2019. URL http:
//proceedings.mlr.press/v97/van-niekerk19a.html.
Harm Van Seijen, Mehdi Fatemi, JoshUa Romoff, Romain Laroche, Tavian Barnes, and
Jeffrey Tsang. Hybrid reward architectUre for reinforcement learning. In I. GUyon,
U. V. LUxbUrg, S. Bengio, H. Wallach, R. FergUs, S. Vishwanathan, and R. Garnett
(eds.), Advances in Neural Information Processing Systems, volUme 30. CUrran Asso-
ciates, Inc., 2017. URL https://proceedings.neurips.cc/paper/2017/file/
1264a061d82a2edae1574b07249800d6-Paper.pdf.
JUan Vargas, Lazar Andjelic, and Amir Barati Farimani. Effects of sparse rewards of different
magnitUdes in the speed of learning of model-based actor critic methods, 2020.
F. VerbrUggen and G.D. Logan. Models of response inhibition in the stop-signal and stop-change
paradigms. Neurosci Biobehav Rev., 33:647-61, 2009. doi: 10.1016/j.neUbiorev.2008.08.014.
Alexander Sasha Vezhnevets, Simon Osindero, Tom SchaUl, Nicolas Heess, Max Jaderberg, David
Silver, and Koray KavUkcUoglU. FeUdal networks for hierarchical reinforcement learning. In
Proceedings of the 34th International Conference on Machine Learning - Volume 70, ICML’17,
pp. 35403549. JMLR.org, 2017.
Honghao Wei and Lei Ying. Fork: A forward-looking actor for model-free reinforcement learning,
2021.
B. D. Ziebart. Modeling Purposeful Adaptive Behavior with the Principle of Maximum Causal
Entropy. PhD thesis, Machine Learning Dpt., Carnegie Mellon University, PittsbUrgh, PA, 2010.
13
Under review as a conference paper at ICLR 2022
A	Appendix
B roader Impacts
Efficiency in retraining is useful in situations where training is computationally expensive or can
cause damage and wear to instruments like robotic arms. Our work proposes a method of efficient
retraining of agents by reusing trained policies and value function estimators. Ultimately, we con-
tribute to the body of work that prioritizes creative reuse of models.
Table 1: List of common hyper-parameters used in the SAC training.
Parameter	Value
Optimizer Replay buffer size Number of hidden layers Number of hidden neurons Batch size Learning rate Discount factor γ Soft τ Entropy target τ Activation function Target update interval Number of episodes before training πI	Adam 1.0e6 1 (LunarLander), 2 (BipedalWalker) 256 64 and 128 5.0e-4 0.99 1.0e-3 -3 ReLU 1 0 (LunarLander),100 (BiPedalWalker)
Supplementary Experiments and Results
LunarLanderContinuous-v2 with bomb. The observation space consists of the original 8 states
(related to position, angle, velocities and leg contact) plus 4 values indicating the upper left and
bottom right coordinates of the bomb zone (a box with a width of ≈ 10% of the screen size).
The agent assumes horizontal coordinates x ∈ [-1, 1] and vertical coordinates y ∈ [0, 1.4], where
y = 0 corresponds to the ground. The bomb is randomly placed with center coordinates xc ∈
[-0.2, 0.2] and yc ∈ [0.1, 0.5], and appears with a default frequency of 50% (stop trials), which
creates an environment with the highest uncertainty. In the stop trials, the bomb appears randomly
after the agent achieves an altitude between the range [0.9, 1.1]. Importantly, the bomb coordinates
are included in the observation state only after the bomb appears so that the agent does not know
about its existence beforehand. If the bomb is absent, four dummy negative values are provided
in the state. The episode ends if the agent hits the bomb (i.e. agent inside the bomb zone) or the
ground, successfully lands or reaches the maximum length of 1000 steps.
Conservative reward shaping. In LunarLanderContinuous-
v2 with bomb, it is expected that how the inhibitory reward
rI is shaped, as described in the main text, influences the
agents’ training. Indeed, we tested a few shaping variants
and observed differences. However, results comparing SAC-
I vs. SAC were qualitatively similar to the ones presented so
far. One potential issue with the current reward shaping to
avoid the bomb is that it does not consider any of the aspects
involved in landing. Therefore, we present another variant
of the rI which includes some of the aspects of the raw re-
ward such as rewarding getting close to the landing pad, slow-
Figure 10: Comparison of SAC
ing down, and penalizing “aggressive” maneuvers (large ve- and SAC-I agents with conserva-
locities and vertical angles). The more conservative reward tive shaping.
shaping is given by the sum of the following components
rI = rx + ry + rangle + rvel, where rx = -1/(6 × dx + 0.1) + 0.77, ry = -3 × (dy - 0.05)2,
rangle = -angle2, rvel = -2 × (vx2 + vy2 + va2ngle), and (dx, dy) are the horizontal and vertical
distances to the bomb, respectively. In short, the inhibitory reward has maximum value 0 and com-
ponents with quadratic forms. Coefficients are adjusted to have rI < 0 and a balance between the
different rewards. If the inhibition rule given by conditions dx < 0.2 and y > yb is met, the Q-I
14
Under review as a conference paper at ICLR 2022
network is used along with the inhibitory reward rI . In this version, the bomb avoidance is driven
by the horizontal distance dx, independent of dy, making the agent more conservative. Results are
shown in Figure 10. Unlike SAC, the SAC-I is benefited by the shaping and it offers a mechanism
to include additional shaping without interfering previously learned Q networks.
BipedalWalker environments. In both BipedalWalker-v3 and BipedalWalkerHardcore-v3 tasks,
the primary goal is to reach the end of the track using the least amount of energy as possible.
The agent gets a dense reward for moving forward (total 300+ points up to the end) and -100 for
falling. There is a small cost for applying motor torque and moving the joints, so agents with
optimal movements score more. The observation state has a total of 24 variables split between
14 related to the agent’s body (including the angles and velocities of hull, hip joints, knee joints,
vertical and horizontal speeds) and 10 LIDAR sensor readings. Everything the agent knows about
the environment is through the sensors, so it does not know about obstacles or holes until they
are scanned by the sensor, neither about its position in the environment since no coordinates are
given. Note that sensor readings are relative to the agent’s position and angle. These characteristics
make the hardcore version significantly more challenging than the plain terrain version. The agent
has 2 legs, each leg has 2 joints, and therefore the action space has size 4 with continuous values
within the range of [-1, 1]. Each episode ends whenever the agent reaches the end of the track,
falls or reaches the maximum of 2000 steps. Some informal discussion regarding the difficulty
in solving the task can be found in https://ai.stackexchange.com/questions/13848/has-anyone-been-
able-to-solve-openais-hardcore-bipedal-walker-with-their-implem .
BipedalWalker experimental design. The standard SAC agent is trained in the BipedalWalker-v3
to be used as a baseline agent in further training in the hardcore version. Agents are trained with three
different seeds for 3K episodes, and the one with the best average reward is selected as the baseline.
This agent reaches an average over the last 100 episodes above 300+ in about 0.5M steps similar to
the results reported in Wei & Ying (2021). All the network weights are transferred to the retrained
agent. The Q-I network is also retrained from previous weights. For the BipedalWalkerHardcore-v3,
we train agents with five seeds for 5K episodes, and compute the average reward across runs.
Effect of removing the fall penalty. In neuroscience, punishments (negative rewards) are known
to produce aversive conditioning or prediction of aversive events that leads to defensive or ag-
gressive Pavlonian actions (Seymour et al., 2007), what in turn can cause fear of failure and
procrastination in learning (Martin, 2012). Negative rewards are often successfully used in RL.
However, there are studies showing that intermixing positive and sparse negative rewards might
have adverse effects in learning, as reported in a recent work using DDPG in robotic environ-
ment Vargas et al. (2020). Moreover, optimally shaping reward is still an open area of re-
search (Hu et al., 2020), and it is likely to depend on the particular DRL method being used.
Interestingly, in our experiments in BipedalWalkerHardcore-
v3, we observe that the SAC agents are greatly benefited from
removing the fall penalty from the raw environment reward.
We observe that, with the fall penalty, often the agent learns
to simply stop in front of the block by fear of falling (which
is likely a local minimum), never exploring enough to over-
come the large obstacles. In Figure 11, we illustrate the ef-
fects of removing the fall penalty and including the stuck
penalty. Removing the fall penalty (-100) included in origi-
nal reward/penalty provided by the environment allowed the
agent to explore riskier actions to overcome obstacles. The
SAC agent without stuck penalty is also unable to learn the
task successfully (purple line). It should be noted that retrain-
ing from a baseline agent is also critical to speed-up training
and make possible solving the task. Average reward represents
the average over the last 100 episodes of the raw environmental
Figure 11: Agent performance
in BipedalWalkerHardcore-v3, re-
trained using SAC algorithm with
(brown) and without (blue line) the
fall penalty.
reward.
Training from scratch in the mixed version of the BipedalWalkerHardcore-v3. In Figure 12, we
show the average reward during training for the SAC and SAC-I agents, in the environment with 90%
stop trials. Training the SAC-I agent from scratch is slower than retraining from the baseline agent
(Figure 10, Stop trial 90%). However, the SAC-I agent is still able to solve the hardcore task (Stop
trials). Meanwhile, the SAC agent is only able to solve the simple task (Go trials). The advantage
15
Under review as a conference paper at ICLR 2022
of retraining is evident during the initial training steps. The retrained SAC-I agent completes the
simple task from the beginning of the training (compare with Figure 10, Stop trial 90%). The SAC-I
agent trained from scratch learns to complete the Go trials only after 1M steps.
Figure 12: SAC and SAC-I agents performance in the mixed version of BipedalWalkerHardcore-v3.
Agents are trained from scratch. Stop trials are 90% of the total number of episodes.
16