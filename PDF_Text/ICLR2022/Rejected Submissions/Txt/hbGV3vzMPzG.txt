Under review as a conference paper at ICLR 2022
On the Impact of Hard Adversarial Instances
on Overfitting in Adversarial Training
Anonymous authors
Paper under double-blind review
Ab stract
Adversarial training is a popular method to robustify models against adversarial
attacks. However, it exhibits much more severe overfitting than training on clean
inputs. In this work, we investigate this phenomenon from the perspective of
training instances, i.e., training input-target pairs. We provide a quantitative metric
measuring the difficulty of an instance in the training set and analyze the model’s
behavior on instances of different difficulty levels. This lets us show that the decay
in generalization performance of adversarial training is a result of the model’s
attempt to fit hard adversarial instances. We theoretically verify our observations
for both linear and general nonlinear models, proving that models trained on
hard instances have worse generalization performance than ones trained on easy
instances, and that this generalization gap is larger in adversarial training. Finally,
we investigate solutions to mitigate adversarial overfitting in several scenarios,
including when relying on fast adversarial training and in the context of fine-tuning
a pretrained model with additional data. Our results demonstrate that using training
data adaptively improves the model’s robustness.
1	Introduction
The existence of adversarial examples (Szegedy et al., 2014) causes serious safety concerns when
deploying modern deep learning models. For example, for classification tasks, imperceptible pertur-
bations of the input instance can fool state-of-the-art classifiers. Many strategies to obtain models that
are robust against adversarial attacks have been proposed (Buckman et al., 2018; Dhillon et al., 2018;
Ma et al., 2018; Pang et al., 2020; 2019; Samangouei et al., 2018; Xiao et al., 2020), but most of them
have been found to be ineffective in the presence of adaptive attacks (Athalye et al., 2018; Croce &
Hein, 2020b; Tramer et al., 2020). Ultimately, this leaves adversarial training (Madry et al., 2018)
and its variants (Alayrac et al., 2019; Carmon et al., 2019; Gowal et al., 2020; Hendrycks et al., 2019;
Kumari et al., 2019; Wu et al., 2020; Zhang et al., 2019a) as the most effective and popular approach
to construct robust models. Unfortunately, adversarial training yields much worse performance on
the test data than vanilla training. In particular, it strongly suffers from overfitting (Rice et al., 2020),
with the model’s performance decaying significantly on the test set in the later phase of adversarial
training. While this can be mitigated by early stopping (Rice et al., 2020) or model smoothing (Chen
et al., 2021b), the reason behind the overfitting of adversarial training remains poorly understood.
In this paper, we study this phenomenon from the perspective of training instances, i.e., training
input-target pairs. We introduce a quantitative metric to measure the difficulty of the instances and
analyze the model’s behavior, such as its loss and intermediate activations, on training instances of
different difficulty levels. This lets us discover that the model’s generalization performance decays
significantly when it fits the hard adversarial instances in the later training phase.
To more rigorously study this phenomenon, we then perform theoretical analyses on both linear and
nonlinear models. For linear models, we study logistic regression on a Gaussian mixture model, in
which we can calculate the analytical expression of the model parameters upon convergence and thus
the robust test accuracy. Our theorem demonstrates that adversarial training on harder instances leads
to larger generalization gaps. We further prove that the gap in robust test accuracy between models
trained by hard instances and ones trained by easy instances increases with the size of the adversarial
budget. In the case of nonlinear models, we derive the lower bound of the model’s Lipschitz constant
when the model is well fit to the adversarial training examples. This bound increases with the
1
Under review as a conference paper at ICLR 2022
difficulty level of the training instances and the size of the adversarial budget. Since a larger Lipschitz
constant indicates a higher adversarial vulnerability (Ruan et al., 2018; Weng et al., 2018a;b), our
theoretical analysis confirms our empirical observations.
Our findings can be broadly applied to obtain robust models. To evidence this, in addition to the
standard adversarial training settings of (Madry et al., 2018), we study the following two scenarios:
fast adversarial training and adversarial fine-tuning with additional training data. Our proposed
method assigns adaptive targets or adaptive weights on training instances to avoid fitting hard input-
target pairs. We show it can mitigate adversarial overfitting and improve models’ performance. For
fast adversarial training, our results are better than other accelerated adversarial training methods
available on RobustBench (Croce et al., 2020). For adversarial fine-tuning with additional training
data, we show improved performance over the methods in (Alayrac et al., 2019; Carmon et al., 2019).
Contributions. In summary, our contributions are as follows: 1) Based on a quantitative metric of
instance difficulty, we show that fitting hard adversarial instances leads to degraded generalization
performance in adversarial training. 2) We conduct a rigorous theoretical analysis on both linear and
nonlinear models. For linear models, we show analytically that models trained on harder instances
have larger robust test error than the ones trained on easy instances; the gap increases with the size
of the adversarial budget. For nonlinear models, we derive a lower bound of the model’s Lipschitz
constant. The lower bound increases with the difficulty of the training instances and the size of the
adversarial budget, indicating both factors make adversarial overfitting more severe. 3) We show
that the adaptive use of the easy and hard training instances can improve the performance in fast
adversarial training and adversarial fine-tuning with additional training data.
Notation and terminology. In this paper, x and x0 are the clean input and its adversarial counterpart.
We use fw to represent a model parameterized by w and omit the subscript w unless ambiguous.
o = fw(x) and o0 = fw(x0) are the model’s output of the clean input and the adversarial input.
Lw (x, y) and Lw (x0 , y) represent the loss of the clean and adversarial instances, receptively, in
which we sometimes omit w and y for notation simplicity. We use kwk and kXk to represent the
l2 norm of the vector w and the largest singular value of the matrix X, respectively. sign is an
elementwise function which returns +1 for positive elements, -1 for negative elements and 0 for
0. 1y is the one-hot vector with only the y-th dimension being 1. The term adversarial budget
refers to the allowable perturbations applied to the input instance. It is characterized by lp norm
and the size E as a set S(P)(E) = {∆∣k∆∣∣p ≤ e}, with E defining the budget size. Therefore, given
the training set D, the robust learning problem can be formulated as the min-max optimization
minw Ex〜D max∆∈s(p)⑹ Lw(X + ∆). A notation table is provided in Appendix A.
In this paper, vanilla training refers to training on the clean inputs, and vanilla adversarial training to
the adversarial training method in (Madry et al., 2018). RN18 and WRN34 are the 18-layer ResNet (He
et al., 2016) and the 34-layer WideResNet (Zagoruyko & Komodakis, 2016) used in (Madry et al.,
2018) and (Wong et al., 2020), respectively. To avoid confusion with the general term overfitting,
which denotes the gap between the training and test accuracy, we employ the term adversarial
overfitting to indicate the phenomenon that robust accuracy on the test set decreases significantly in
the later phase of vanilla adversarial training. This phenomenon was pointed out in (Rice et al., 2020)
and does not occur in vanilla training. Our code is submitted on GoogleDrive anonymously1.
2	Related Work
We concentrate on white-box attacks, where the attacker has access to the model parame-
ters. Such attacks are usually based on first-order information and stronger than black-box at-
tacks (Andriushchenko et al., 2020; Dong et al., 2018). For example, the fast gradient sign
method (FGSM) (Goodfellow et al., 2014) perturbs the input based on its gradient’s sign, i.e.,
∆ = E sig n(Ox Lw (x)). The iterative fast gradient sign method (IFGSM) (Kurakin et al., 2016)
iteratively runs FGSM using a smaller step size and projects the perturbation back to the adversarial
budget after each iteration. On top of IFGSM, projected gradient descent (PGD) (Madry et al., 2018)
use random initial perturbations and restarts to boost the strength of the attack.
Many methods have been proposed to defend a model against adversarial attacks (Buckman et al.,
2018; Dhillon et al., 2018; Ma et al., 2018; Pang et al., 2020; 2019; Samangouei et al., 2018; Xiao
IhttPS://drive.google.com/file/d/1vb6ehNMkBeNIM3dLr_igKMUcCgBd9ZmK/view?USP=Sharing
2
Under review as a conference paper at ICLR 2022
et al., 2020). However, most of them were shown to utilize obfuscated gradients (Athalye et al., 2018;
Croce & Hein, 2020b; Tramer et al., 2020), that is, training the model to tackle some specific types
of attacks instead of achieving true robustness. This makes these falsely robust models vulnerable
to stronger adaptive attacks. By contrast, several works have designed training algorithms to obtain
provably robust models (Cohen et al., 2019; Gowal et al., 2019; Raghunathan et al., 2018; Salman
et al., 2019; Wong & Kolter, 2018). Unfortunately, these methods either do not generalize to modern
network architectures or have a prohibitively large computational complexity. As a consequence,
adversarial training (Madry et al., 2018) and its variants (Alayrac et al., 2019; Carmon et al., 2019;
Hendrycks et al., 2019; Kumari et al., 2019; Wu et al., 2020; Zhang et al., 2019a) have become the de
facto approach to obtain robust models in practice. In essence, these methods generate adversarial
examples, usually using PGD, and use them to optimize the model parameters.
While effective, adversarial training is more challenging than vanilla training. It was shown to require
larger models (Xie & Yuille, 2020) and to exhibit a poor convergence behavior (Liu et al., 2020).
Furthermore, as observed in (Rice et al., 2020), it suffers from adversarial overfitting: the robust
accuracy on the test set significantly decreases in the late adversarial training phase. (Rice et al.,
2020) thus proposed to perform early stopping based on a separate validation set to improve the
generalization performance in adversarial training. Furthermore, (Chen et al., 2021b) introduced
logit smoothing and weight smoothing strategies to reduce adversarial overfitting. In parallel to this,
several techniques to improve the model’s robust test accuracy were proposed (Wang et al., 2020; Wu
et al., 2020; Zhang et al., 2021), but without solving the adversarial overfitting issue. By contrast,
other works (Balaji et al., 2019; Huang et al., 2020) were empirically shown to mitigate adversarial
overfitting but without providing any explanations as to how this phenomenon was addressed. In this
paper, we study the causes of adversarial overfitting from both an empirical and a theoretical point of
view. We also identify the reasons why prior attempts (Balaji et al., 2019; Chen et al., 2021a; Huang
et al., 2020) successfully mitigate it.
3	A Metric for Instance Difficulty
Parametric models are trained to minimize a loss objective based on several input-target pairs called
training set, and are then evaluated on a held-out set called test set. By comparing the loss value of
each instance, we can understand which ones, in either the training or the test set, are more difficult
for the model to fit. In this section, we introduce a metric for instance difficulty, which mainly
depends on the data and on the perturbations applied to the instances.
Let L(X) denote the average loss of x's corresponding perturbed input across all training epochs. We
define the difficulty of an instance x within a finite set D as
d(x) =P(L(X) < L(e)∣e 〜u(D)) +1 P(L(X) = L(e)∣e 〜U(D)),	(i)
where e 〜U(D) indicates e is uniformly sampled from the finite set D. d(x) is a bounded function,
close to 0 for the hardest instances, and 1 for the easiest ones. We discuss the motivation and
properties of d(X) in Appendix D.1, and show that it mainly depends on the data and the perturbation
applied, the model architecture or the training duration can hardly affect the difficulty function. That
is, d(X) can represent the difficulty of X within a set under a specific type of perturbation.
4	Instance Difficulty and Adversarial Overfitting
The model-agnostic difficulty metric of Section 3 allows us to select training instances based on
their difficulty. In Figures 20 and 21 of Appendix D.2, we show some samples of the easiest and the
hardest instances of each class in CIFAR10 (Krizhevsky et al., 2009) and SVHN (Netzer et al., 2011),
respectively. In both cases, the easiest instances are visually highly similar, whereas the hardest ones
are much more diverse, some of them being ambiguous or even incorrectly labeled. Below, we study
how easy and hard instances impact the performance of adversarial training, with a focus on the
adversarial overfitting phenomenon. The detailed experimental settings are deferred to Appendix C.1.
4.1	Using a Subset of Training Data
We start by training RN18 models for 200 epochs using either the 10000 easiest, random or hardest
instances of the CIFAR10 training set via either vanilla training, FGSM adversarial training or PGD
3
Under review as a conference paper at ICLR 2022
adversarial training. The adversarial budget is based on the l∞ norm and = 8/255. Note that
the instance’s difficulty is defined under the corresponding perturbation type, and we enforce these
subsets to be class-balanced. For example, the easiest 10000 instances consist of the easiest 1000
instances in each class. We provide the learning curves under different settings in Figure 1.
(a) PGD Adversarial Training
(b) FGSM Adversarial Training
(c) Vanilla Training
Figure 1: Learning curves obtained by training on the 10000 easiest, random and hardest instances
of CIFAR10 under different scenarios. The training error (dashed lines) is the error on the selected
instances, and the test error (solid lines) is the error on the whole test set.
For PGD adversarial training, in Figure 1(a), while we observe adversarial overfitting when using
the random instances, as in (Rice et al., 2020), no such phenomenon occurs when using the easiest
instances: the performance on the test set does not degrade during training. However, PGD adversarial
training fails and suffers more severe overfitting when using the hardest instances. Note that, in
Figure 5 (Appendix D.3), we show that this failure is not due to improper optimization.
In contrast to PGD, FGSM adversarial training and vanilla training (Figure 1(b), 1(c)), through which
the model does not achieve true robustness (Madry et al., 2018), do not suffer from severe adversarial
overfitting. In these cases, the models trained with the hardest instances also achieve non-trivial test
accuracy. Furthermore, the gaps in robust test accuracy between the models trained by easy instances
and by hard ones are much smaller.
In Appendix D.3, we perform additional and comprehensive experiments, evidencing that our
conclusions hold for different datasets and values of , and for an adversarial budget based on the
l2 norm. We show that more severe adversarial overfitting happens when the size of the adversarial
budget increases. Furthermore, we experiment with training models using increasingly many
training instances, start with the easiest ones. Our results in Figure 10 show that the models can
benefit from using more data, but only using early stopping as done in (Rice et al., 2020). This
indicates that the hard instances can still benefit adversarial training, but need to be utilized in an
adaptive manner.
4.2	Hard Instances Lead to Overfitting
Let us now turn to the more standard setting where we train the model with the entire training set. To
nonetheless analyze the influence of instance difficulty in this scenario, we divide the training set
D into 10 non-overlapping groups {Gi}i9=0, with Gi = {x ∈ D|0.1 × i ≤ d(x) < 0.1 × (i + 1)}.
That is, G0 is the hardest group, whereas G9 is the easiest one. We then train an RN18 model on the
entire CIFAR10 training set using PGD adversarial training and monitor the training behavior of the
different groups. In particular, in Figure 2(a), we plot the average loss of the instances in the groups
G0 , G3 , G6 and G9 . The resulting curves show that, in the early training stages, the model first fits the
easy instances, as evidenced by the average loss of group G9 decreasing much faster than that of the
other groups. By contrast, in the late training phase, the model tries to fit the more difficult instances,
with the average loss of groups G0 and G3 decreasing much faster than that of the other groups. In
this period, however, the robust test error (solid grey line) increases, which indicates that adversarial
overfitting arises from the model’s attempt to fit the hard adversarial instances.
In addition to average losses, inspired by (Ilyas et al., 2019), which showed that the penultimate
layer’s activations of a robust model correspond to its robust features that cannot be misaligned
by adversarial attacks, we monitor the group-wise average magnitudes of the penultimate layer’s
activations. As shown in Figure 2(b), the model first focuses on extracting robust features for the
easy instances, as evidenced by the comparatively large activations of the instances in G9 . In the late
4
Under review as a conference paper at ICLR 2022
phase of training, the norm of the activations of the hard instances increases significantly, bridging
the gap between easy and hard instances. This further indicates that the model focuses more on the
hard instances in the later training phase, at which point it starts overfitting.
(a) Average loss.
Figure 2: Analysis on the groups G0, G3, G6 and G9 in the training set. The right vertical axis
corresponds to the training (dashed grey line) and test (solid grey line) error under adversarial attacks
for both plots. Left plot: The left vertical axis represents the average loss of different groups. Right
plot: The left vertical axis represents the average l2 norm of features extracted during training for
different groups.
φpn--u6ew
(b) Average l2 norm of extracted features.
5	Theoretical Analysis
We now study the relationship between adversarial overfitting and instance difficulty from a theoretical
viewpoint. We start with the linear model, where the loss function has an analytical expression, and
then generalize our analysis to the nonlinear cases. We use {xi , yi}in=1 to represent the training
data, and (X, y) as its matrix form. {x0i, yi}in=1 and (X0, y) are their adversarial counterparts. Here,
xi ∈ Rm, yi ∈ {-1, +1}, X ∈ Rn×m and y ∈ {-1, +1}n. The notation used for our theoretical
analysis is summarized in Table 3 of Appendix A.
5.1	Logistic Regression
We study the logistic regression model under an l2 norm based adversarial budget. In this case, the
model is parameterized by w ∈ Rm and outputs sign(wT x0i) given the adversarial example x0i of
the input xkThe loss function for this instance is ɪ-.1wτX . We assume over-parameterization,
which means n < m.
The following theorem shows that, under mild assumptions, the parameters of the adversarially
trained logistic regression model converge to the l2 max-margin direction of the training data.
Theorem 1. Fora dataset {xi, yi}in=1 that is linearly separable under the adversarial budget S (2) (),
any initial point w0 and step size α ≤ 2kXk-2, the gradient descent wu+1 = wu - αOwLwu(X0)
converges asymptotically to the l2 max-margin vector of the training data. That is,
W
两,
lim
u→∞
Wu
kwuk
where Wb = arg min kWk
w
s.t. ∀i ∈ {1, 2, ..., n}, WTxi ≥ 1 .
(2)
The proof is deferred to Appendix B.1. Theorem 1 extends the conclusion in (Soudry et al., 2018),
which only studies the non-adversarial case. It also indicates that the optimal parameters are only
determined by the support vectors of the training data, which are the ones with the smallest margin.
According to the loss function, the smallest margin means the largest loss values and thus the hardest
training instances based on our definition in Section 3.
To further study how the training instances’ difficulty influences the model’s generalization perfor-
mance, we assume that the data points are drawn from a K-mode Gaussian mixture model (GMM).
Specifically, the k-th component has a probability pk of being sampled and is formulated as follows:
if y = +1, Xi 〜N(rkη, I); if y = -1, Xi 〜N(-rkη,I).
(3)
5
Under review as a conference paper at ICLR 2022
Here, η ∈ Rm is the unit vector indicating the mean of the positive instances, and rk ∈ R+ controls
the average distance between the positive and negative instances. The mean values of all modes in
this GMM are colinear, so rk indicates the difficulty of instances sampled from the k-th component.
Without the loss of generality, we assume r1 < r2 < ... < rK-1 < rK. As in Section 4, we consider
models trained with the subsets of the training data, e.g., n instances from the l-th component. l = 1
then indicates training on the hardest examples, while l = K means using the easiest. In matrix form,
we have X = rlyηT + Q for the instances sampled from the l-th component, where the rows of
noise matrix Q are sampled from N (0, I).
Although the max-margin direction in Equation (2), where the parameters converge based on Theo-
rem 1, does not have an analytical expression, the results in (Wang & Thrampoulidis, 2020) indicate
that, in the over-parameterization regime and when the training data is sampled from a GMM, the
max-margin direction is the min-norm interpolation of the data with high probability. Since the
latter has an analytical form given by XT (XXT)-1y, we can then calculate the exact generalization
performance of the trained model as stated in the following theorem.
Theorem 2. If a logistic regression model is adversarially trained on n separable training instances
sampledfrom the l-th component ofthe GMM model described in(3). If nmn is sufficiently large2,
then with probability 1 一 O( 1), the expected adversarial test error R under the adversarial budget
S(2) (), which is a function of rl and , on the whole GMM model described in (3) is given by
K
R(rl, ) = XpkΦ (rkg(rl) 一 ) , g(rl) = (C1 一
k=1
cr2⅛2y)1 …≥ 0.
(4)
Ci, C are independent of E and r. Thefunction Φ is defined as Φ(x) = P(Z > x), Z 〜N(0,1).
We defer the proof of Theorem 2 to Appendix B.2, in which we calculate the exact expression of
R(rl, E), C1, C2, and show that C1, C2 are positive numbers almost surely. Since C1 and C2 are
independent of rl, and Φ(x) is a monotonically decreasing function, we conclude that the robust test
error R(rl , E) becomes smaller when rl increases. That is, when the training instances become easier,
the corresponding generalization error under the adversarial attack becomes smaller.
Theorem 2 holds for all E only if the training data is separable under the corresponding adversarial
budget. The following corollary shows that the difference in the robust test error between models
trained with easy instances and the ones with hard ones increases when E becomes larger.
Corollary 1. Under the conditions of Theorem 2 and the definition of R in Equation (4), if E1 < E2,
then we have ∀ 0 ≤ i < j ≤ K, R(ri, E1) 一 R(rj , E1) < R(ri, E2) 一 R(rj, E2).
The proof is in Appendix B.3. This indicates that, compared with training on clean inputs, i.e., E = 0,
the generalization performance of adversarial training with E > 0 is more sensitive to the difficulty of
the training instances. This is consistent with our empirical observations in Figure 1.
5.2	General Nonlinear Models
In this section, we study the binary classification problem using a general nonlinear model. We
consider a model with b parameters, i.e., w ∈ Rb. Without loss of generality, we assume the output
of the function fw to lie in [一1, +1]. Furthermore, we assume isoperimetry of the data distribution:
Assumption 1. The data distribution μ is a composition of K C-isoperimetric distributions on
Rm, each of which has a positive conditional variance. That is, μ = PK=I αk μk, where ak > 0
and PK=I ɑk = 1. We define σ2 = Eμk [Var[y∣x]], and without loss of generality assume that
σ1 ≥ σ2 ≥ ... ≥ σK > 0. Furthermore, given any L-Lipschitz function fw, i.e., ∀x1, x2, kfw (x1) 一
fw(x2)k ≤ Lkx1 一 x2k, we have
mt2
∀k ∈{1, 2,…，K} P(X 〜μk,kfw(X)- Eμk(fw)k ≥ t) ≤ 2e-2cL2 .	⑸
This is a benign assumption; the data distribution is a mixture of K components and each of
them contains samples from a sub-Gaussian distribution. These components correspond to training
2Specifically, m and n need to satisfy m > 10nlogn + n — 1 and m > Cnrl√log 2n∣∣ηk∙ The constant
C is derived in the proof of Theorem 1 in (Wang & Thrampoulidis, 2020).
6
Under review as a conference paper at ICLR 2022
instances of different difficulty levels measured by the conditional variance. We then study the
property of the model fw under adversarial attacks.
Definition 1. Given the dataset {xi, yi}in=1, the model fw, the adversarial budget S(p) () and a
positive constant C, we define the function h(C, ) as
h(C, ) = min min hi,w ()
s.t. T (C, ) =
1n
W w| n Efw (xi) - yi)
i=1
≤C
(6)
where hi,w()
max ζ, s.t. [fw (xi) - Z, fw (Xi) + Z] ⊂ {fw (xi + ∆) ]δ
Here, x0i is the adversarial example of x. We omit the superscript (p) for notation simplicity.
By definition, hi,w () ≥ 0 depicts the bandwidth ζ of the model’s output range in the domain
of the adversarial budget on a training instance. h(C, ) is the minimum bandwidth among the
models whose mean squared error on the adversarial training set is smaller than C. Based on the
definitions of T and hi,w, and for a fixed value of C, we have ∀1 < 2, hi,w (1) ≤ hi,w (2 ) and
T(C, 2) ⊂ T(C, 1). As a result, ∀1 < 2, h(C, 1) ≤ h(C, 2). In addition, since ∀C1 < C2,
T(C1, ) ⊂ T (C2, ) for a fixed value of , we have ∀C1 < C2, h(C1, ) ≥ h(C2, ). That is to say,
h(C, ) is a monotonically non-decreasing function on and a monotonically non-increasing function
on C. In practice, when fw represents a deep neural network, h(C, ) increases with almost surely,
because the attack algorithm usually generates adversarial examples at the boundary of the adversarial
budget. We then state our main theorem below.
Theorem 3. Given n training pairs {xi, yi}n=ι Sampledfrom the l -th component μι ofthe distribution
in Assumption 1, the parametric model fw, the adversarial budget S(p) () and the corresponding
function h defined in Definition 1, we assume that the model fw is in the function space F =
{fw,w ∈ W} with W ⊂ Rb having a finite diameter diam(W) ≤ W and, ∀w1,w2 ∈ W, kfw1 -
fw2 k∞ ≤ J kw1 -w2 k∞. We train the model fw adversarially using these n data points. Let x0 be
the adversarial example ofthe data point x and δ ∈ (0,1). Ifwe have n En=ι(fw (Xi) — yi)2 = C
andγ := σl2 + h2(C, ) - C ≥ 0, then with probability at least 1 - δ, the Lipschitz constant offw is
lower bounded as
Lip(fw) ≥ 2
where Lip(fw) is the Lipschitz constant offw: ∀x1, x2, kfw (x1) - fw (x2)k ≤ Lip(fw)kx1 -x2 k.
nm
c(b log(4WJγ-1) - log(δ∕2 - 2e-2-11nY2))
(7)
The proof is deferred to Appendix B.4. Theorem 3 extends the results in (Bubeck & Sellke, 2021)
to the case of adversarial training. Note that modern deep neural network models typically have
millions of parameters, so b max{c, m, n}. In this case, we can approximate the lower bound
⑺ by Lip(fw ) & 27 J bc log(nWJYT),
and the right hand side increases with γ.
Since γ :
σl2 + h2(C, ) - C, the lower bound increases with both σl and but decreases as C increases.
The Lipschitz constant is widely used to bound a model’s adversarial vulnerability (Ruan et al., 2018;
Weng et al., 2018a;b): larger Lipschitz constants indicate higher adversarial vulnerability. Recall
that γ needs to be non-negative, so C is upper bounded, which means that the model is well fit to
the adversarial training set and the adversarial vulnerability is approximately the generalization gap.
Based on this, the adversarial vulnerability of a model increases with the size of the adversarial budget
and the difficulty level of the training instances; it also increases as the training mean squared error
decreases. That is, under the same adversarial budget, the adversarial vulnerability increases with the
instances’ difficulty, measured by σl in our theorem; using the same training instances, the adversarial
vulnerability increases with the adversarial budget measured by . In addition, as adversarial training
progresses, the mean squared error C on the adversarial training instances becomes smaller, which
makes the lower bound of the Lipschitz constant larger. This indicates that adversarial vulnerability
becomes larger in the later phase of adversarial training.
We provide empirical evidence to confirm the conclusions of Theorem 3 in Appendix D.4. Since
calculating the Lipschitz constant of a deep neural network is NP-hard (Scaman & Virmaux, 2018),
exactly calculating the Lipschitz constant (Jordan & Dimakis, 2020) can only be achieved for simple
multi-layer perceptron (MLP) models, not for modern deep networks. Instead, we estimate the upper
bound of the Lipschitz constant numerically, as in (Scaman & Virmaux, 2018).
V
7
Under review as a conference paper at ICLR 2022
6	Mitigating Adversarial Overfitting
Existing methods mitigating adversarial overfitting study the standard adversarial training scenario:
PGD adversarial training from scratch. In Appendix D.5, we show that they use either adaptive
inputs (Balaji et al., 2019) or adaptive targets (Huang et al., 2020; Chen et al., 2021b) to implicitly
avoid fitting hard input-target pairs, thus providing an explanation for their success.
Below, we study two settings other than standard adversarial training: fast adversarial training
and adversarial fine-tuning with additional data, to validate our findings. We show that methods
inspired by our findings can avoid adversarial overfitting and improve the performance. The detailed
experimental settings for this section are in Appendix C.2.
6.1	Fast Adversarial Training
Adversarial training in (Madry et al., 2018) introduces a significant computational overhead. Thus
it is desirable to accelerate this method. Our experiments in this section are based on adversarial
training with transferable adversarial examples (ATTA in (Zheng et al., 2020)), which stores the
adversarial perturbation for each training instance as an initial point for the next epoch. We show that
adaptively utilizing the easy and hard training instances not only mitigates adversarial overfitting, but
also significantly improves the performance of the final model.
First, we introduce a reweighting scheme to assign lower weights to hard instances when calculating
the loss objective. Specifically, each training instance is assigned a weight equal to the adversarial
output probability of the true label. Then this weight is normalized to ensure that the weights in a
mini-batch sum to 1. Note that our reweighting scheme is based on the adversarial output instead
of the clean output, because the adversarial output probability will also be used to calculate the loss
objective. As a result, the computational overhead of the reweighting scheme is negligible.
In addition to reweighting, we also use adaptive targets to improve the performance. For each training
instance (x, y), we maintain an adaptive moving average target t. t is updated in an exponential
average manner in each epoch t J ρt + (1 - ρ)o0 where P is the momentum factor. This is similar
to the target in (Huang et al., 2020), but, similarly to the reweighting scheme, we use the adversarial
output o0 instead of the clean output o to avoid an increase in computational complexity. The final
adaptive target we use is t = β1y + (1 - β)t and thus the loss objective is Lw (x0, t). The factor
β controls how “adaptive” our target is: β = 0 yields a fully adaptive moving average target t and
β = 1 yields a one-hot target 1y. We provide the pseudocode as Algorithm 1 in Appendix C.2.
We run experiments on CIFAR10 using WRN34 models under the l∞ adversarial budget of size =
8/255, the standard setting where most fast adversarial training algorithms are benchmarked (Croce
et al., 2020). We evaluate the model’s robust accuracy on the test set by AutoAttack (Croce & Hein,
2020b), the popular and reliable attack for evaluation. The results are provided in Table 2, where the
results of the baseline methods are taken from RobustBench (Croce et al., 2020). We also report the
number of epochs and the number of forward and backward passes in a mini-batch update of each
method. The product of these two values indicates the training complexity.
We can clearly see that both reweighting and adaptive targets improve the performance on top
of ATTA (Zheng et al., 2020). Note that our method based on adaptive targets achieve the best
performance while needing only 1/4 of the training time of (Chen et al., 2021a), the strongest
baseline. (Wong et al., 2020) is the only baseline consuming less training time than ours, but its
performance is much worse than ours; it suffers from catastrophic overfitting when using a WRN34
model. In Appendix D.6, we provide the learning curves of our methods under different settings and
show that both reweighting and adaptive targets mitigate adversarial overfitting. We also conduct an
ablation study on the value of β and find that a decrease in β decreases the generalization gap. This
indicates that the more adaptive the targets, the smaller the generalization gap.
6.2	Adversarial Finetuning with Additional Data
We observe that adversarial overfitting occurs in the small learning rate regime in Section 4. To
further study this, we propose to fine-tune an adversarially pretrained model using additional training
data, because we also use small learning rate to fine-tune a model. While additional training data was
8
Under review as a conference paper at ICLR 2022
Method	Model	Epochs Complexity AA		
(Shafahi et al., 2019)-	WRN34	200	2	41.17
(Wong et al., 2020)	RN18	15	4	43.21
(Zheng et al., 2020)	WRN34	38	4	44.48
(Zhang et al., 2019a)	WRN34	105	3	44.83
(Chenetal., 2021a)_ _	WRN34	J00_ _	_ 7 _	_ _51.12_
Reweighting (Ours)	^WRN34	^38^ 一	- - 4 -	一 ^46.15 -
Adaptive Target (Ours)	WRN34	38	4	51.17
Table 1: Comparison between different accelerated adver-
sarial training methods in robust test accuracy against Au-
toAttack (AA). The baseline results are from RobustBench.
Complexity shows the number of forward passes and back-
ward passes in one mini-batch update.
Duration	Method	AA
WRN34 on CIFAR10, e		=8/255
No Fine Tuning		52.01-
	Vanilla AT	'54.11 —-
1 Epoch	Ours	54.69
	vanilla AT ^	'55749 —-
5 Epoch	Ours	56.99
RN18 on SVHN, e =		-002
No Fine Tuning		67.77-
	Vanilla AT	'70.81 — 一
1 Epoch	Ours	72.53
	vanilla AT ^	'72718 —-
5 Epoch	Ours	73.35
Table 2: Robust accuracy of fine-tuned
models against AutoAttack(AA).
shown to be beneficial in (Alayrac et al., 2019; Carmon et al., 2019), we demonstrate that letting the
model adaptively fit the easy and hard instances can further improve the performance.
We conduct experiments on both CIFAR10 and SVHN, using WRN34 and RN18 models, respectively.
The experimental settings are the same as (Carmon et al., 2019) except the learning rate. We tune the
learning rate and find that fixing it to 10-3 is the best choice. The model is fine-tuned for either 1
epoch or 5 epochs, which means that each additional training instance is used either 5 times or only
once. This is because we observed the performance of vanilla adversarial training to start decaying
after 5 epochs. As such, methods requiring many epochs such as (Balaji et al., 2019) and (Huang
et al., 2020) are not applicable here.
Our first technique, reweighting, is the same as in Section 6.1. In addition to reweighting, we can
also add a KL regularization term measuring the KL divergence between the output probability of
the clean instance and of the adversarial instance. The KL term encourages the adversarial output
to be close to the clean one. In other words, the clean output probability serves as the adaptive
target. For hard instances, the clean and adversarial inputs are usually both misclassified. Therefore,
the clean outputs of these instances constitute simpler targets compared with the ground-truth
labels. Ultimately, the loss objective of a mini-batch {xi}iB=1 used for fine-tuning is expressed as
LFT({xi}iB=1)=PiB=1Wi [Lw(Xi) + λKL(θi∣∣oi)] where Wi is the adaptive weight when We use
re-weighting, or 1/B otherwise. λ is 6 when using the regularization term and 0 otherwise.
We use both reweighting and KL regularization to fine-tune the model. Our results are shown in
Table 2, where the robust test accuracy is also evaluated by AutoAttack. Itis clear that our methods can
improve the performance under all settings. We also conduct an ablation study in Appendix D.7. All
these results show that avoiding fitting hard adversarial examples helps to improve the generalization
performance in adversarial fine-tuning with additional training data.
7	Conclusion
We have investigated adversarial overfitting from the perspective of the easy and hard training
instances. By introducing a quantitative, model-agnostic and normalized metric to measure the
instance difficulty, we have shown that a model’s generalization performance under adversarial
attacks degrades during the later phase of training as the model fits the hard adversarial instances. We
have conducted theoretical analyses on both linear and nonlinear models. On an over-parameterized
logistic regression model, we have shown that training on harder adversarial instances leads to poorer
generalization performance. On general nonlinear models, we have proven that the lower bound
of a well-trained model’s Lipschitz constant increases with the difficulty of the training instances.
Finally, our experiments on fast adversarial training and adversarial fine-tuning with additional data
have demonstrated that adaptively using the training instances mitigates adversarial overfitting and
improves the model’s robustness.
9
Under review as a conference paper at ICLR 2022
Ethics S tatement
Adversarial attacks may cause unexpected failure of modern deep learning models, especially in
some safety-critical applications such as self-driving and medical imaging. Our work studies the
defense methods and focuses on how to improve training robust models to prevent such failure from
happening. We are not aware of any ethics issues about our work, as we are using publicly available
data and models.
Reproducibility S tatement
The experimental settings of our empirical study are demonstrated in detail in Appendix C. Especially,
we also provide the pseudocode of our algorithm there. Our code is submitted anonymously on
GoogleDrive3. It will be made publicly available when our work is published.
References
Jean-Baptiste Alayrac, Jonathan Uesato, Po-Sen Huang, Alhussein Fawzi, Robert Stanforth, and
Pushmeet Kohli. Are labels required for improving adversarial robustness? In Advances in Neural
Information Processing Systems, pp.12192-12202, 2019.
Maksym Andriushchenko, Francesco Croce, Nicolas Flammarion, and Matthias Hein. Square attack:
a query-efficient black-box adversarial attack via random search. In European Conference on
Computer Vision, pp. 484-501. Springer, 2020.
Anish Athalye, Nicholas Carlini, and David Wagner. Obfuscated gradients give a false sense of
security: Circumventing defenses to adversarial examples. arXiv preprint arXiv:1802.00420, 2018.
Yogesh Balaji, Tom Goldstein, and Judy Hoffman. Instance adaptive adversarial training: Improved
accuracy tradeoffs in neural nets. arXiv preprint arXiv:1910.08051, 2019.
George Boole. The mathematical analysis of logic. Philosophical Library, 1847.
Sebastien BUbeck and Mark Sellke. A universal law of robustness via isoperimetry. arXiv preprint
arXiv:2105.12806, 2021.
Jacob Buckman, Aurko Roy, Colin Raffel, and Ian Goodfellow. Thermometer encoding: One hot way
to resist adversarial examples. In International Conference on Learning Representations, 2018.
URL https://openreview.net/forum?id=S18Su--CW.
Yair Carmon, Aditi Raghunathan, Ludwig Schmidt, John C Duchi, and Percy S Liang. Unlabeled
data improves adversarial robustness. In Advances in Neural Information Processing Systems, pp.
11190-11201, 2019.
Jinghui Chen, Yu Cheng, Zhe Gan, Quanquan Gu, and Jingjing Liu. Efficient robust training via back-
ward smoothing, 2021a. URL https://openreview.net/forum?id=49V11oUejQ.
Tianlong Chen, Zhenyu Zhang, Sijia Liu, Shiyu Chang, and Zhangyang Wang. Robust overfitting
may be mitigated by properly learned smoothening. In International Conference on Learning
Representations, 2021b. URL https://openreview.net/forum?id=qZzy5urZw9.
Jeremy M Cohen, Elan Rosenfeld, and J Zico Kolter. Certified adversarial robustness via randomized
smoothing. arXiv preprint arXiv:1902.02918, 2019.
Francesco Croce and Matthias Hein. Minimally distorted adversarial examples with a fast adaptive
boundary attack. In International Conference on Machine Learning, pp. 2196-2205. PMLR,
2020a.
Francesco Croce and Matthias Hein. Reliable evaluation of adversarial robustness with an ensemble
of diverse parameter-free attacks. In International Conference on Machine Learning, 2020b.
3https://drive.google.com/file/d/1vb6ehNMkBeNIM3dLJigKMUCCgBd9ZmK∕view?USP=Sharing
10
Under review as a conference paper at ICLR 2022
Francesco Croce, Maksym Andriushchenko, Vikash Sehwag, Edoardo Debenedetti, Nicolas Flam-
marion, Mung Chiang, Prateek Mittal, and Matthias Hein. Robustbench: a standardized adversarial
robustness benchmark. arXiv preprint arXiv:2010.09670, 2020.
Guneet S. Dhillon, Kamyar Azizzadenesheli, Jeremy D. Bernstein, Jean Kossaifi, Aran Khanna,
Zachary C. Lipton, and Animashree Anandkumar. Stochastic activation pruning for robust
adversarial defense. In International Conference on Learning Representations, 2018. URL
https://openreview.net/forum?id=H1uR4GZRZ.
Yinpeng Dong, Fangzhou Liao, Tianyu Pang, Hang Su, Jun Zhu, Xiaolin Hu, and Jianguo Li. Boosting
adversarial attacks with momentum. In Proceedings of the IEEE conference on computer vision
and pattern recognition, pp. 9185-9193, 2018.
Ian J Goodfellow, Jonathon Shlens, and Christian Szegedy. Explaining and harnessing adversarial
examples. arXiv preprint arXiv:1412.6572, 2014.
Sven Gowal, Krishnamurthy Dj Dvijotham, Robert Stanforth, Rudy Bunel, Chongli Qin, Jonathan
Uesato, Relja Arandjelovic, Timothy Mann, and Pushmeet Kohli. Scalable verified training for
provably robust image classification. In Proceedings of the IEEE International Conference on
Computer Vision, pp. 4842-4851, 2019.
Sven Gowal, Chongli Qin, Jonathan Uesato, Timothy Mann, and Pushmeet Kohli. Uncovering
the limits of adversarial training against norm-bounded adversarial examples. arXiv preprint
arXiv:2010.03593, 2020.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image
recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition,
pp. 770-778, 2016.
Dan Hendrycks and Thomas Dietterich. Benchmarking neural network robustness to common
corruptions and perturbations. In International Conference on Learning Representations, 2019.
URL https://openreview.net/forum?id=HJz6tiCqYm.
Dan Hendrycks, Kimin Lee, and Mantas Mazeika. Using pre-training can improve model robustness
and uncertainty. In International Conference on Machine Learning, pp. 2712-2721, 2019.
Wassily Hoeffding. Probability inequalities for sums of bounded random variables. In The collected
works of Wassily Hoeffding, pp. 409-426. Springer, 1994.
Roger A Horn and Charles R Johnson. Matrix analysis. Cambridge university press, 2012.
Lang Huang, Chao Zhang, and Hongyang Zhang. Self-adaptive training: beyond empirical risk
minimization. Advances in Neural Information Processing Systems, 33, 2020.
Andrew Ilyas, Shibani Santurkar, Dimitris Tsipras, Logan Engstrom, Brandon Tran, and Aleksander
Madry. Adversarial examples are not bugs, they are features. In Advances in Neural Information
Processing Systems, pp. 125-136, 2019.
Ziwei Ji and Matus Telgarsky. The implicit bias of gradient descent on nonseparable data. In
Conference on Learning Theory, pp. 1772-1798. PMLR, 2019.
Matt Jordan and Alexandros G Dimakis. Exactly computing the local lipschitz constant of relu
networks. arXiv preprint arXiv:2003.01219, 2020.
Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint
arXiv:1412.6980, 2014.
Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images. 2009.
Nupur Kumari, Mayank Singh, Abhishek Sinha, Harshitha Machiraju, Balaji Krishnamurthy, and
Vineeth N Balasubramanian. Harnessing the vulnerability of latent layers in adversarially trained
models. In Proceedings of the Twenty-Eighth International Joint Conference on Artificial In-
telligence, IJCAI-19, pp. 2779-2785. International Joint Conferences on Artificial Intelligence
Organization, 7 2019. doi: 10.24963/ijcai.2019/385. URL https://doi.org/10.24963/
ijcai.2019/385.
11
Under review as a conference paper at ICLR 2022
Alexey Kurakin, Ian Goodfellow, and Samy Bengio. Adversarial examples in the physical world.
arXiv preprint arXiv:1607.02533, 2016.
Chen Liu, Mathieu Salzmann, Tao Lin, Ryota Tomioka, and Sabine Susstrunk. On the loss landscape
of adversarial training: Identifying challenges and how to overcome them. Advances in Neural
Information Processing Systems, 33, 2020.
Xingjun Ma, Bo Li, Yisen Wang, Sarah M. Erfani, Sudanthi Wijewickrema, Grant Schoenebeck,
Michael E. Houle, Dawn Song, and James Bailey. Characterizing adversarial subspaces using local
intrinsic dimensionality. In International Conference on Learning Representations, 2018. URL
https://openreview.net/forum?id=B1gJ1L2aW.
Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and Adrian Vladu. To-
wards deep learning models resistant to adversarial attacks. In International Conference on Learn-
ing Representations, 2018. URL https://openreview.net/forum?id=rJzIBfZAb.
Yuval Netzer, Tao Wang, Adam Coates, Alessandro Bissacco, Bo Wu, and Andrew Y Ng. Reading
digits in natural images with unsupervised feature learning. 2011.
Tianyu Pang, Kun Xu, Chao Du, Ning Chen, and Jun Zhu. Improving adversarial robustness via
promoting ensemble diversity. In International Conference on Machine Learning, pp. 4970-4979,
2019.
Tianyu Pang, Kun Xu, Yinpeng Dong, Chao Du, Ning Chen, and Jun Zhu. Rethinking softmax cross-
entropy loss for adversarial robustness. In International Conference on Learning Representations,
2020. URL https://openreview.net/forum?id=Byg9A24tvB.
Aditi Raghunathan, Jacob Steinhardt, and Percy Liang. Certified defenses against adversarial
examples. arXiv preprint arXiv:1801.09344, 2018.
Leslie Rice, Eric Wong, and Zico Kolter. Overfitting in adversarially robust deep learning. In
International Conference on Machine Learning, pp. 8093-8104. PMLR, 2020.
Wenjie Ruan, Xiaowei Huang, and Marta Kwiatkowska. Reachability analysis of deep neural
networks with provable guarantees. In IJCAI, pp. 2651-2659, 2018. URL https://doi.org/
10.24963/ijcai.2018/368.
Hadi Salman, Greg Yang, Jerry Li, Pengchuan Zhang, Huan Zhang, Ilya Razenshteyn, and Sebastien
Bubeck. Provably robust deep learning via adversarially trained smoothed classifiers. arXiv
preprint arXiv:1906.04584, 2019.
Pouya Samangouei, Maya Kabkab, and Rama Chellappa. Defense-GAN: Protecting classifiers
against adversarial attacks using generative models. In International Conference on Learning
Representations, 2018. URL https://openreview.net/forum?id=BkJ3ibb0-.
Kevin Scaman and Aladin Virmaux. Lipschitz regularity of deep neural networks: analysis and
efficient estimation. In Proceedings of the 32nd International Conference on Neural Information
Processing Systems, pp. 3839-3848, 2018.
Ali Shafahi, Mahyar Najibi, Amin Ghiasi, Zheng Xu, John Dickerson, Christoph Studer, Larry S
Davis, Gavin Taylor, and Tom Goldstein. Adversarial training for free! arXiv preprint
arXiv:1904.12843, 2019.
Daniel Soudry, Elad Hoffer, Mor Shpigel Nacson, Suriya Gunasekar, and Nathan Srebro. The implicit
bias of gradient descent on separable data. The Journal of Machine Learning Research, 19(1):
2822-2878, 2018.
Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian Goodfellow,
and Rob Fergus. Intriguing properties of neural networks. In International Conference on Learning
Representations, 2014. URL http://arxiv.org/abs/1312.6199.
Antonio Torralba, Rob Fergus, and William T Freeman. 80 million tiny images: A large data set for
nonparametric object and scene recognition. IEEE transactions on pattern analysis and machine
intelligence, 30(11):1958-1970, 2008.
12
Under review as a conference paper at ICLR 2022
Florian Tramer, Nicholas Carlini, Wieland Brendel, and Aleksander Madry. On adaptive attacks to
adversarial example defenses. Advances in Neural Information Processing Systems, 33, 2020.
Ramon Van Handel. Probability in high dimension. Technical report, PRINCETON UNIV NJ, 2014.
Roman Vershynin. High-dimensional probability: An introduction with applications in data science,
volume 47. Cambridge university press, 2018.
Ke Wang and Christos Thrampoulidis. Benign overfitting in binary classification of gaussian mixtures.
arXiv preprint arXiv:2011.09148, 2020.
Yisen Wang, Difan Zou, Jinfeng Yi, James Bailey, Xingjun Ma, and Quanquan Gu. Improving adver-
sarial robustness requires revisiting misclassified examples. In International Conference on Learn-
ing Representations, 2020. URL https://openreview.net/forum?id=rklOg6EFwS.
Lily Weng, Huan Zhang, Hongge Chen, Zhao Song, Cho-Jui Hsieh, Luca Daniel, Duane Boning,
and Inderjit Dhillon. Towards fast computation of certified robustness for relu networks. In
International Conference on Machine Learning,pp. 5276-5285. PMLR, 2018a.
Tsui-Wei Weng, Huan Zhang, Pin-Yu Chen, Jinfeng Yi, Dong Su, Yupeng Gao, Cho-Jui Hsieh, and
Luca Daniel. Evaluating the robustness of neural networks: An extreme value theory approach. In
International Conference on Learning Representations, 2018b. URL https://openreview.
net/forum?id=BkUHlMZ0b.
Eric Wong and Zico Kolter. Provable defenses against adversarial examples via the convex outer
adversarial polytope. In International Conference on Machine Learning, pp. 5286-5295. PMLR,
2018.
Eric Wong, Leslie Rice, and J. Zico Kolter. Fast is better than free: Revisiting adversarial training. In
International Conference on Learning Representations, 2020. URL https://openreview.
net/forum?id=BJx040EFvH.
Dongxian Wu, Shu-Tao Xia, and Yisen Wang. Adversarial weight perturbation helps robust general-
ization. Advances in Neural Information Processing Systems, 33, 2020.
Chang Xiao, Peilin Zhong, and Changxi Zheng. Enhancing adversarial defense by k-winners-take-all.
In International Conference on Learning Representations, 2020. URL https://openreview.
net/forum?id=Skgvy64tvr.
Cihang Xie and Alan Yuille. Intriguing properties of adversarial training at scale. In International
Conference on Learning Representations, 2020. URL https://openreview.net/forum?
id=HyxJhCEFDS.
Sergey Zagoruyko and Nikos Komodakis. Wide residual networks. arXiv preprint arXiv:1605.07146,
2016.
Dinghuai Zhang, Tianyuan Zhang, Yiping Lu, Zhanxing Zhu, and Bin Dong. You only propagate
once: Accelerating adversarial training via maximal principle. In Advances in Neural Information
Processing Systems, pp. 227-238, 2019a.
Hongyang Zhang, Yaodong Yu, Jiantao Jiao, Eric Xing, Laurent El Ghaoui, and Michael Jordan.
Theoretically principled trade-off between robustness and accuracy. In International Conference
on Machine Learning, pp. 7472-7482, 2019b.
Jingfeng Zhang, Jianing Zhu, Gang Niu, Bo Han, Masashi Sugiyama, and Mohan Kankanhalli.
Geometry-aware instance-reweighted adversarial training. In International Conference on Learning
Representations, 2021. URL https://openreview.net/forum?id=iAX0l6Cz8ub.
Haizhong Zheng, Ziqi Zhang, Juncheng Gu, Honglak Lee, and Atul Prakash. Efficient adversarial
training with transferable adversarial examples. In Proceedings of the IEEE/CVF Conference on
Computer Vision and Pattern Recognition, pp. 1181-1190, 2020.
13
Under review as a conference paper at ICLR 2022
A Notation
b	Section 5.2	The number of parameters in a general nonlinear model.
c	Assumption 1, Section 5.2	The coefficient in isoperimetry.
C	Section 5.2	The mean squared error on the adversarial training set.
d	Equation 1, Section 3	The function introduced by the proposed difficulty metric.
D	Section 3	The data set.
fw	Section 1	The model parameterized by w.
F	Theorem 3, Section 5.2	The function space of the model.
G	Section 4.2	Groups of the training set divided by instance difficulty.
h	Definition 1, Section 5.2	The bandwidth of the model’s output range.
J	Theorem 3, Section 5.2	The Lipschitz constant of fw w.r.t w.
K	Section 5	The number of components in the data distribution.
l	Section 5	The component index where the training data is sampled.
L	Assumption 1, Section 5.2	The Lipschitz constant of fw w.r.t the input.
L	Section 1	The loss function.
m	Section 5	Dimension of the input data.
n	Section 5	The number of training instances.
o, o0	Section 6	Model’s output of the clean and the adversarial input.
p	Section 1	Shape of the adversarial budget.
r	Equation 3, Section 5.1	The coefficient in the GMM model.
R	Theorem 2, Section 5.1	The robust test error.
t, te	Section 6.1	The adaptive target and the moving average target.
w	Section 5	Model parameters.
W	Theorem 3, Section 5.2	The diameter upper bound of the parameter space.
W	Theorem 3, Section 5.2	The space of model parameters.
x, x0, X	Section 1 & Section 5	Clean input, adversarial input and its matrix form.
y, y	Section 1 & Section 5	Label and its vector form.
α	Algorithm 1	The step size of the adversarial attacks.
β	Section 6.1	The coefficient controlling how adaptive the target is.
γ	Theorem 3, Section 5.2	The non-negative variable introduced in Theorem 3.
δ	Theorem 3, Section 5.2	The probability introduced in Theorem 3.
	Section 1	The size of the adversarial budget.
η	Equation 3, Section 5.1	The direction of the mean of each GMM’s component.
ρ	Section 6.1	The momentum calculating the moving average target.
μι, μι	Assumption 1, Section 5.2	Data distribution and its l-th component.
σ	Assumption 1, Section5.2	The conditional variance of the data distribution.
Table 3: The notation in this paper. We provide the section of their definition or first appearance.
B	Proofs in Theoretical Analysis
B.1 Proof of Theorem 1
Similar to (Soudry et al., 2018), we can assume all instances are positive without the loss of generality,
this is because we can always redefine yi xi as the input. In this regard, the loss to optimize in a
logistic regression model under the adversarial budget S(2) () is:
n
Lw(X) = X l(wT xi - kwk)
i=1
(8)
Here l(∙) is the logistic function: l(χ) = ι+1-x. We use X ∈ Rn×m to represent the training set as
said in Section 5, then the loss function L(w) is kXk2-smooth, where kXk2 is the maximal singular
value of X. Since function Lw is convex on w, so gradient descent of step size smaller than 2kXk-2
will asymptotically converge to the global infimum of the function Lw on w .
14
Under review as a conference paper at ICLR 2022
Before proving Theorem 1, we first introduce the following lemma:
Lemma 1. Consider the max-margin vector wb of the vanilla case defined in Equation (2), we then
introduce the max margin vector wc0 defined under the adversarial attack of budgetS(2) () as follows:
T
w0 = arg min kwk s.t. ∀i ∈ {1, 2, ..., n}, wTxi - kwk ≥ 1
w
(9)
wc0	w
Then we have w0 is collinear With W, i.e., W^- = ∣W
,	, kwc0 k	kwb k
1
Proof. We show that W =	Cl∣ w0 and prove it by contraction.
c0
Let s assume ∃v, s.t. kvk < k “且U and ∀i ∈ {1,2, ...,n}, VTXi ≥ 1, then we can consider
1+kwc0 k	i
v0 = (1 + kW0k)v. The l2 norm of v0 is smaller than that of W0, and we have
∀i ∈ {1, 2,	..., n}, v0T xi	-	kv0k	= (1 +	kWc0k)vTxi	-	kv0k	> (1 +	kWc0k) -	kWc0k	= 1
(10)
Inequality 10 shows we can construct a vector v 0 whose l2 norm is smaller than W0 and satisfying the
condition (9), this contracts with the optimality of W0 . Therefore, there is no solution of condition (2)
c0
whose norm is smaller than k ” 2 U.
1+kwc0k
c0
On the other hand,——1^w0 satisfies the condition (2) and its l2 norm is k “ L . As a result, we
1+kw0 k	1+kw0 k
CIC
have W =——¼τ w0. That means W and w0 are collinear.	□
1+ekw0k
With Lemma 1, Theorem 1 is more straightforward, whose proof is shown below. Regarding the
convergence analysis of the logistic regression model in non-adversarial cases, we encourage the
readers to find more details in (Ji & Telgarsky, 2019; Soudry et al., 2018).
Proof. Theorem 1 in (Ji & Telgarsky, 2019) and Theorem 3 in (Soudry et al., 2018) proves the
convergence of the direction of the logistic regression parameters in different cases. In this regard, we
can let w∞ = limu→∞ |熹：卜.That is to say, for sufficiently large u, the direction of the parameter
W(u) can be considered fixed. As a result, the adversarial perturbations of each data instance xi is
fixed, i.e., W∞ .
We can then apply the conclusion of Theorem 3 in (Soudry et al., 2018) here, the only difference
is the data points are {xi - W∞}in=1. Therefore, the parameter W(u) will converge to the l2
max margin of the dataset {xi - W∞}in=1. When t → ∞, we have W(u)T (xi - W∞) =
W(u)T xi - kW(u)k. This is exactly the adversarial max margin condition in (9). Based on
Lemma 1, we have limu→∞ IIw(U)Il = W^- = ∣p‰	□
,	u→∞ kw(U)k kw0k kwk
B.2 Proof of Theorem 2
Given the parameter W of the logistic regression model, we can first calculate the robust error for the
k-th component of the GMM model defined in (3).
Lemma 2. The 0-1 classification error of a linear classifier W under the adversarial attack of the
budget S(2) () for the k-th component of the GMM model defined in (3) is:
T
Rk 9 = φ( -⅛Γ Y)	(II)
where Φ(x) = P(Z > x), Z 〜N(0,1).
15
Under review as a conference paper at ICLR 2022
Proof. For a random drawn data instance (x,y), the adversarial perturbation is -yek^. Let's
decompose X as τ%yη + z, where Z 〜N(0, I). Then, We have
Rk(e) = P(ywτ(X - yeIiwr) < 0)= P(ywτ(τkyη + Z - ye-w) < 0)
kwk	kwk	(12)
= P(-ywT Z > τkwTη - ekwk)
Since Z 〜N(0, I), we have -ywτZ 〜N(0, (-ywτ)T(-ywT)) = N(0, WTw). Furthermore
T
-ywk	〜N(0,1), and we can further simplify Rk (e) as follows:
TT	T
b	-ywT Z	τkwTη	τkwTη
Rk(e) = P(-H~~H- >	~~H-e) = φ(-∏~~H-e)	(13)
kwk	kwk	kwk
□
With Lemma 2, we can straightforwardly calculate the robust error for all components of the GMM
model defined in (3):
KT
R(e) = XPkΦ(Tkwn - e)	W
k=1	w
On the other hand, Theorem 1 indicates the parameter w will converge to the l2 max margin.
However, for arbitrary training set, we do not have the closed form of w, which is a barrier for
the further analysis. Nevertheless, results from (Wang & Thrampoulidis, 2020) indicates in the
over-parameterization regime, the parameter w will converge to min-norm interpolation of the data
with high probability.
Lemma 3. (Directly from Theorem 1 in (Wang & Thrampoulidis, 2020)) Assume n training instances
drawn from the l-th mode of the described distribution in (3) and each of them is a m-dimensional
vector. If nmn is sufficiently large4, then the l2 max margin vector in Equation (2) will be the same
as the solution of the min-norm interpolation described below with probability at least (1 — O(1)).
W = argmin ∣∣w∣∣ s.t. ∀i ∈ {1, 2,…,n}, yi = WTXi	(15)
w
Since the min-norm interpolation has a closed solution W = XT(XXT)-1y, Lemma 3 will greatly
facilitate the calculation of R(W) in Theorem 2. To simplify the notation, we first define the following
variables.
U = QQT, d= Qη, s = yT U-1y, t = dU-1d, v = yTU-1d	(16)
The proof of Theorem 2 is then presented below.
T
Proof. Based on (14), the key is to simplify the term Wwwn, let's denote it by A, then we have:
2	nT WWT n	(yT(xxT )-1Xn)2
=^Tw- =	yT (XXT )-1y
(17)
The key challenge here is to calculate the term (XXT)-1 where X = τlyηT + Q. Here we utilize
Lemma 3 of (Wang & Thrampoulidis, 2020) and Woodbury identity (Horn & Johnson, 2012), we
have:
yT (XX)-1 = yTU-1 -
(τl2 skηk2 + τl2v2 + τlv - τl2st)yT + τlsdT
τ2s(knk2 -1) + (ITIv +1)2
U-1	(18)
4Specifically, m and n need to satisfy m > 10nlogn + n — 1 and m > Cnrl√log 2n∣∣ηk∙ The constant
C is derived in the proof of Theorem 1 in (Wang & Thrampoulidis, 2020).
16
Under review as a conference paper at ICLR 2022
Here, s, t, v, U and d are defined in Equation (16). The scalar divisor comes from the matrix inverse
calculation. Base of Equation (18), we can then calculate yT(XXT)-1y and yT (XXT)-1Xη.
yT(XXT)-1y=s-
(rl2 skηk2 + rl2v2 + rlv - rl2st)s + rlsv
r2s(knk2 — t) + (rιv + 1)2
s
(19)
r2S(IlnlI2 - t) + (rιv + 1)2
yT(XXT)-1Xη = yT (XXT)-1(rιyηT + Q)η
= rιIηI2yT (XXT)-1y + yT (XXT)-1d
rιs(IηI2 - t) + rιv2 + v
r2s(knk2 — t) + (TTv + 1)2
(20)
Plug Equation (19) and (20) into (17), we have:
(ris(knk2 - t) + rιv2 + v)2
s (r2s(knk2 — t) + (rιv + 1)2)
S(Ink2 -1) + v2__________knk2 -1________
S	r2s(knk2 - t) + (rιv + 1)2	()
S(Ink2 -1) + v2___________________1________________
S	(SUnk2-t)+v2ʌ r2 I	2v	I 1
i一仿产—-r rι+而尸mrι +而尸』
Plug (21) into (14), we then obtain the robust error on all components of the GMM defined in (3):
K	1 ι
R(rι, e)=与Pkφ (Irkg(rι) - e), g(rι) = (CI - Cr2 + C )2
s(knk2 -1)+ v2 C = s(knk2 -1)+ v2 C =	2v	+	1
S , 2 = knk2 -1	, 3 = knk2 -tι+knk2-1.
(22)
We study the sign of C1 and C2 . Consider U = QQT is a positive semidefinite matrix, so S =
yU-1yT ≥ 0. In addition, We have knk2 — t = nT(I 一(QQT)-1) n. Since I - (QQT)-1 =
(I - (QQT)-1)T(I - (QQT)-1) is a positive semidefinite matrix, we can obtain I - (QQT)-1 is
also a positive semidefinite matrix. As a result, C1 and C2 are both non-negative.
□
B.3 Proof of Corollary 1
To prove Corollary 1, We first prove the folloWing lemma:
Lemma 4. Under the condition of Theorem 2 and R in Equation (4),
monotonically decreases with .
ARrIlQ is negative and
Proof. Based on Equation (22), We have:
∂ R(rι,c)
∂rι
XPkΦ0(rkg(rι) - E)号")
∂rι
-1
(23)
Since the training data is separable, We have ∀k, rkwTn - Ekwk > 0, Which is equivalent to the
folloWing:
∀k, rkg(rι) - E > 0
(24)
17
Under review as a conference paper at ICLR 2022
First, pk is a positive number by definition. Consider function Φ(x) monotonically decrease with
x and is convex when x > 0, so ∀k, Φ0(rkg(rl) - ) is negative and decreases with . In addition,
g(rι) increases with r and is independent on e, so 飞；? Can be considered as a positive constant.
Therefore, 六丁) is negative and monotonically decreases with e.
□
Now, we are ready to prove Corollary 1:
Proof. We subtract the left hand side from the right hand side in the inequality of Corollary 1:
烟 、仅 ʌi 必 、饭 、「广∂R(rι,"	广∂Rgg)
[R(rj, e1) - R(ri, EI)] - [R(rj, ∈2) - R(ri, ∈2)] = J -∂r-dr - J -∂r-dr
_ ∕rj ∣^∂R(rι,Eι)	∂R(rι,s)]d
=Jri [ ^r	而-J rl
>0
(25)
The last inequality is based on r7- > r^ ⑦ > s, Lemma 4, which indicates [ dRrj1)
∂R(rι 簿2)
∂rι
is always positive. We reorganize (25) and obtain R(ri, E1) - R(rj, E1) < R(ri, E2) - R(rj, E2).
□
B.4 Proof of Theorem 3
We start with the following lemma.
Lemma 5. Given the assumptions of Theorem 3, we define g(x) = E(y|x), z(x) = y - g(x) and
consider γ = σl2 + h2(C, E) - C, then the following inequality holds.
1n
∀a ∈ (0,1),P(∃fw ∈F : n ]ζ(yi - fw (χi))2 ≤ C)
na2γ2	1 n	1
≤ 2e 8	+ P(∃fw ∈ F : n ɪ2 fw (Xi)Z(Xi) ≥ 2(1 - 3a)γ)
(26)
Proof. Given the definition of h(C, E), we have:
(yi - fw (X0i))2 = [(yi - fw(Xi)) + (fw (Xi) - fw(X0i))]2
≥ (yi - fw(Xi))2 + (fw (Xi) - fw(X0i))2	(27)
≥ (yi - fw(Xi))2 + h2(C, E)
For the first inequality, X0i is the adversarial example which tries to maximize the loss objective,
yi ∈ {-1, +1} and the range of fw is [-1, +1], so hyi - fw(Xi),fw(Xi) - fw(X0i)i ≥ 0. The
second inequality is based on the definition of h2(C, E) in Definition 1. As a result, we can simplify
the left hand side of (26) as follows:
nn
P(∃fw ∈ F : 1 X(yi- fw(Xi))2 ≤ C) ≤ P(∃fw ∈ F : 1 X(yi- fw(Xi))2 ≤ C - h2(C, e))
n i=1	n i=1
(28)
18
Under review as a conference paper at ICLR 2022
We consider the sequence {z(xi)}n=ι, it is i.i.d with Eμι (z(x)2) = E*% [Var(y∣x)] = σp. Since the
range of the prediction is [-1, +1], so z2 (x) ∈ [0, 4]. Then, we have the following inequality by
Hoeffding’s inequality (Hoeffding, 1994).
1n
∀a ∈ (0,1),P(— J2z2(xi) ≤ σ2 - aγ) ≤ e-
n
i=1
na2γ2
8
(29)
Similarly, we consider the sequence {z(xi)g(xi)}in=1, the following inequality holds based on the
Hoeffding’s inequality and the fact E(z(x)g(x)) = 0, z(x)g(x) ∈ [-2, +2].
1n
∀a ∈ (0,1),P(— V"z(xi)g(xi) ≤ aγ) ≤ e-
n
i=1
na2γ2
8
(30)
Now we study the right hand side of (28):
1n	1n
—X(yi - fw(xi))2 =	—	X	(z2(xi)	+ (g(xi)	-	fw(Xi))2 +	2z(xi)(g(xi) - fw(Xi)))
n i=1	n	i=1
i=1	1	i=n1	(31)
≥	—	£	(z2(xi)	+ 2z(xi)g(xi) - 2z(xi)fw(Xi))
i=1
Consider the following reasoning:
1n
—y^(yi - fw (Xi)) ≤ C - h (C, E)= σl - Y
—i=1
ι A o, _ o	ι 工，、.，、ι,. 一、
——〉:Z (Xi) ≥ σl - aγ	⇒ —〉: z(xi) fw(xi) ≥ 2(I - 3a)γ
i=1	i=1
1	ʌ z 、, 、
一旌 Z(Xi)g(xi) ≥ -aγ
—
i=1
(32)
As a result, we have:
1n
P(∃fw ∈ F : —	(yi- fw(Xi) ≤ C - h2(C, E)))
1n	1n
≤P(∃fw ∈ F : — XZ2(Xi) ≤ σ2 - aγ) + P(∃fw ∈ F : — Xz(Xi)g(Xi) ≥ -aγ)+
—	i=1	— i=1
1n	1
P(∃fw ∈ F : - EZ(Xi)fw(Xi) ≥ 2(1 - 3a)γ)
—	i=1
na2 2	1 n	1
≤2e--L + P(∃fw ∈ F : - Vz(Xi)fw(Xi) ≥ -(1 - 3a)γ)
—2
i=1
(33)
The first inequality is based on the reasoning of (32). The second inequality is based on (29) and (30).
Based on the inequality (28) and (33), we conclude the proof.
□
19
Under review as a conference paper at ICLR 2022
To further simplify the right hand side of (26), P(∃fw ∈ F : ^ Pn=Iz(Xi)fw(Xi) ≥ 1 (1 - 3a)γ)
needs to be bounded, and this is solved by the following lemma.
Lemma 6. Given the assumptions of Theorem 3 and the definition of g(X), z(X) in Lemma 5, then
the following inequality holds.
∀a ∈ (0, 1), aɪ > 0, a2 > 0 and aι + a2 = ~(1 — 3a),
1 n	1	nm 2 2	2 2	(34)
P(∃fw ∈F ： - Ez(Xi )fw (Xi) ≥ 2(1 - 3a)γ) ≤ 2|F|e-l44mL2 alY + 2e-8 a2γ
n i=1
Proof. We recall that the data points {xi,yi}n=1 are sampled from the distribution μι, which is
c-isoperimetric. For any L-Lipschitz function f, we have:
mt2
∀t,P[∣fw(x) - Eμι (fw)| ≥ t] ≤ 2e-2cL2	(35)
Since z(x) = y - g(x) ∈ [-2, +2], we can then bound P[z(x)(fw(x) - E*% (fw)) ≥ t]:
∀t, P[z(x)(fw(x) - Eμι (fw)) ≥ t] ≤ P[∣z(x)(fw(x) - Eμι (fw))| ≥ t]
≤ P[∣(fw(x) - Eμι(fw))| ≥ 2] ≤ 2e-8mL2	(36)
Here we utilize the proposition in (Vershynin, 2018; Van Handel, 2014)5, which claims if {Xi}in=1
are independent variables and all C-Subgaussian, then 表 ENi Xi is 18C-Subgaussian. Therefore,
we have:
n	mt2
∀t,	Ez(Xi)(fw(Xi) - Eμι(fw)) ≥ t] ≤ 2e- 144cL2
i=1
Let t = aιγ√n, then we have:
n
1	nm 2 2
P[— Ez(Xi)(fw (Xi)- Eμι (f)) ≥ aιγ] ≤ 2e- 144cL2 1γ
n i=1
In addition, we can bound P[n1 Pn=1 z(Xi)Eμι (fw) ≥ a2γ] by:
(37)
(38)
1n	1n
P[∃fw ∈ F : - Xz(Xi)Eμι(fw) ≥ a2γ] ≤ P[- X Iz(Xi)I ≥ a2γ] ≤ 2e-8a2γ2	(39)
n i=1	n i=1
The first inequality is based on the fact E*1 (fw) ∈ [-1, +1]; the second inequality is based on
Hoeffding’s inequality.
Now, we are ready to bound the probability P(∃fw ∈ F : ɪ Pn=Iz(Xi)fw(Xi) ≥ 11 (1 一 3a)γ).
1n	1
P(∃fw ∈ F ： - Ez(Xi)fw(Xi) ≥ 2(1 - 3a)γ)
n i=1
1n	1n
≤P[∃fw ∈ F ： n X z(Xi)(fw(Xi) - Eμι (f)) ≥ alγ] + P[∃fw ∈ F ： n X z(Xi)Eμι (fw) ≥ a2γ]
≤2∣F∣e- 144mL2 a1γ2 + 2e-8 OalY
(40)
The first inequality is based on the fact aι + a，2 = 2 (1 - 3a); the second inequality is based on the
Boole’s inequality (Boole, 1847), inequality (38) and (39).
□
5Proposition 2.6.1 in (Vershynin, 2018) and Exercise 3.1 in (Van Handel, 2014)
20
Under review as a conference paper at ICLR 2022
To simplify the constant notation, We let a = 1, aι = 16 and a2 = 1. We plug this into the inequality
(26) and (34), then:
1 n	nγ2	nmγ2
P(∃fw ∈ F : — V(yi - fw(xi))2 ≤ C) ≤ 4e- 29 + 2∣F∣e-212cL2
n
i=1
(41)
NoW We turn to the proof of Theorem 3.
Proof. We let FL = {fw W ∈ W, Lip(fw) ≤ L}, FY = {fw |w ∈ W, W = J Θ z, Z ∈ Zb} and
Fγ,L = Fγ ∩ FL. Correspondingly, We let WL = {w|w ∈ W, Lip(fw) ≤ L}, Wγ = {w|w ∈
W, w = J Θ z, z ∈ Zb} and Wγ,L = WY ∩ Wl. Because the diameter of W is W, we have
∣FY,L∣ ≤ |FY| ≤ (4WJ) . Here, Θ means the element-wise multiplication.
Note that the inequality (41) is valid for any values of C as long as it satisfies Y ≥ 0. Based on this,
IC J C + 2 Y
We apply the substitution 1	2
IY j2 Y
, then:
1 n	1	nγ2	nmγ2
P(∃fw ∈ Fγ,L : - V(yi - fw(XiX2 ≤ C + 5Y) ≤ 4e-21 + 2∣F∣e-E2
n i=1	i	2	(42)
≤ 4e— nγ2 +2eb l°g(4WJ)—2≡C⅛
Based on the definition of WY,L, we can conclude that ∀W1 ∈ WL, ∃W2 ∈ WY,L s.t.kW1 - W2 k∞ ≤
8J . Therefore, ∀fWι ∈ FL, ∃fW2 ∈ Fγ,Ls.~t. k fWι - fW2 k ∞ ≤ 8. Letchoose such fW2 ∈ Fγ,L
given an arbitrary fw1 ∈ FL, then:
(y - fw1 (x))2 = (y - fw2 (x))2 + (2y - fw1 (x) - fw2 (x))(fw2 (x) - fw1 (x))
≥ (y - fW2 (x))2 - Y l(2y - fwι (X)- fW2 (x))l	(43)
≥ (y - fW2 (x))2 - Y
The first inequality in (43) is based on Holder,s inequality; the second inequality is based on
y ∈ {--, +-} and the range of ∀fw ∈ F is [--, +-].
We combine (41) with (43), then:
-n	-n	-
P(∃fw ∈ Fl : -£(yi - fw(xi))2 ≤ C) ≤ P(∃fw ∈Fγ,L :—£(yi- fw(xi))2 ≤ C + 2Y)
n i=1	n i=1	2
22
≤ 4e—nY- +2eb l°g(4WJ)—2¾
(44)
Note that FL is the set of functions in F whose Lipschitz constant is no larger than L. We set the
right hand side of(44)to be δ and then get L = 27 q c(b log(4WJγ-i)-nmδ∕2-2e-2Tinγ2 )) ∙ ThiS
concludes the proof.
□
21
Under review as a conference paper at ICLR 2022
C Experimental Settings
C.1 General Settings
The ResNet-18 (RN18) architecture is same as the one in (Wong et al., 2020); the WideResNet-34
(WRN34) architecture is same as the one in (Madry et al., 2018). Unless specified, the l∞ adversarial
budget used for CIFAR10 dataset (Krizhevsky et al., 2009) 6 is 8/255 and for SVHN dataset (Netzer
et al., 2011) 7 is 0.02. In PGD adversarial training, the step size is 2/255 for CIFAR10 and 0.005
for SVHN; PGD is run for 10 iterations for both datasets. For adversarial attacks using a different
adversarial budget, the step size is always 1/4 of the adversarial budget’s size, and we always run
it for 10 iterations. To comprehensively and reliably evaluate the robustness of the model, we use
AutoAttack (Croce & Hein, 2020b), which is an ensemble of 4 different attacks: AutoPGD on cross
entropy, AutoPGD on difference of logits ratio, fast adaptive boundary (FAB) attack (Croce & Hein,
2020a) and square attack (Andriushchenko et al., 2020). Unless specified, we use stochastic gradient
descent (SGD) with a momentum to optimize the model parameters, we also use weight decay whose
factor is 0.0005. Unless specified, the momentum factor is 0.9, the learning rate starts with 0.1 and is
divided by 10 in the 1/2 and 3/4 of the whole training duration. The size of the mini-batch is always
128.
We run the experiments on a machine with 4 NVIDIA TITAN XP GPUs. It takes about 6 hours to
adversarially train a RN18 model for 200 epochs, and a whole day to adversarially train a WRN34
model for 200 epochs.
C.2 Settings of Experiments in Section 6
Algorithm 1: One epoch of the accelerated adversarial training We use in Section 6.1.
Input: training data D, model f, batch size B, PGD step size α, adversarial budget S(p) (),
coefficient ρ, β .
for Sample a mini-batch {xi, yi}B=ι 〜 D do
∀i, obtain the initial perturbation ∆i as in (Zheng et al., 2020).
∀i, one step PGD update: ∆i — ∏s(p() [∆i + αsign(θ∆iLθ(Xi + ∆i, yi)]).
∀i, update the cached adversarial perturbation ∆i as in (Zheng et al., 2020).
if use reWeight then
∀i, Weight wi = sof tmax[f (xi + ∆i)]yi
else
∀i, Weight wi = 1
end if
∀i, query the adaptive target K and update: t J Pti + (1 - P)Softmax[f (Xi + ∆i)].
∀i, the final adaptive target t =廿13 + (1 - β)ti
Calculate the loss pj1 W PB WiLθ(Xi + ∆i, ti) and update the parameters.
end for
Fast Adversarial Training Our experiment on fast adversarial training is on CIFAR10 and =
8/255. The pseudocode of our method is demonstrated as Algorithm 1. We use ATTA (Zheng et al.,
2020) to initialize the perturbation of each training instance. The step size α of the perturbation
update is 4/255, same as (Zheng et al., 2020). The average coefficient P and β is 0.9 and 0.1 unless
explicitly stated. Our learning rate scheduler also folloWs (Zheng et al., 2020): We train the model for
38 epochs, the learning rate is 0.1 on the first 30 epochs, it decays to 0.01 in the next 6 epochs and
further decays to 0.001 in the last 2 epochs. When We use adaptive targets, the first 5 epochs are the
Warmup period in Which We use fixed targets. Since the goal here is to accelerate adversarial training,
We do not use a validation set to do model selection as in (Rice et al., 2020). We use the standard data
augmentation on CIFAR10: random crop and random horizontal flip.
6Data available for doWnload on https://WWW.cs.toronto.edu/ kriz/cifar.html. MIT license. Free to use.
7Data available for doWnload on http://ufldl.stanford.edu/housenumbers/. Free for non-commercial use.
22
Under review as a conference paper at ICLR 2022
Adversarial Finetuning with Additional Data For CIFAR10, we use 500000 images from 80
Million Tiny Images dataset (Torralba et al., 2008) with pseudo labels in (Carmon et al., 2019) 8. For
SVHN, we use the extra held-out set provided by SVHN itself, which contains 531131 somewhat
less difficult samples. When we construct a mini-batch, half of its instances are sampled from the
original training set and the other half are sampled from the additional data. The learning rate in the
fine-tuning phase is always 10-3 and is always fixed. Since we only fine-tune the model for only 1 or
5 epochs, we do not use a validation set for model selection.
D	Additional Experiments and Discussion
D. 1 Properties of the Difficulty Metric
To study the factors affecting the difficulty function defined in (1), let us denote by d1, d2 the difficulty
functions obtained under two different training settings, such as different network architectures or
adversarial attack strategies. We then define the difficulty distance (D-distance) between two such
functions in the following equation. In this regard, the expected D-distance between two random
difficulty functions is 0.375.
D(dι,d2 = Ex〜U(D)|di(x) - d2(x)∣ ∙	(45)
We then study the properties of the difficulty metric of Equation (1) by performing experiments on
the CIFAR10 and CIFAR10-C (Hendrycks & Dietterich, 2019) dataset, varying factors of interest. In
particular, we first study the influence of the network by using either a RN18 model, trained for either
100 or 200 epochs (RN18-100 or RN18-200), or a WRN34 model trained for 200 epochs (WRN34).
To generate adversarial attacks, we make use of PGD with an adversarial budget based on the l∞
norm with = 8/255. This corresponds to the settings used in other works (Hendrycks & Dietterich,
2019; Madry et al., 2018). The other hyper-parameters follow the general settings in Appendix C
In the left part of Table 4, we report the D-distance for all pairs of settings. Each result is averaged
over 4 runs, and the variances are all below 0.012. The D-distances in all scenarios are all very small
and close to 0, indicating the architecture and the training duration have little influence on instance
difficulty based on our definition.
d1∖d2	RN18-100	RN18-200	WRN34	d1∖d2	Clean	FGSM	PGD
RN18-100	^^0.0189^^	0.0232	0.0355	Clean	0.0189	0.0607	0.1713
RN18-200	0.0232	0.0159	0.0299	FGSM	0.0607	0.0843	0.1677
WRN34	0.0355	0.0299	0.0178	PGD	0.1713	0.1677	0.0857
Table 4: D-distances between difficulty functions in different settings, including different model
architectures and training duration (left table), and different types of perturbations (right table).
We then perform experiments by varying the attack strategy using a RN18 network. As shown by
the D-distances reported in the right portion of Table 4, the discrepancy between values obtained
with clean, FGSM-perturbed and PGD-perturbed inputs is much larger, thus indicating that our
difficulty function correctly reflects the influence of an attack on an instance. In addition, Table 5
demonstrates the D-distance between the difficulty functions based on clean instances, FGSM-
perturbed instance, PGD-perturbed instances and different common corruptions from CIFAR10-
C (Hendrycks & Dietterich, 2019)9. Note that (Hendrycks & Dietterich, 2019) only provides corrupted
instances on the test set, so we train models on the clean training set and test model on corrupted
test set in these cases. We use RN18 architecture and train it for 100 epochs in all cases, results are
reported on the test set. Compared with the results in the left half of Table 4, the D-distance is much
larger here. This indicates the difficulty function depends on the perturbation type applied to the
input, including the common corruptions.
The results in Table 4 and 5 demonstrate that our difficulty metric mainly depends on the data and on
the perturbation type; not the model architecture or the training duration.
8Data available for download on https://github.com/yguooo/semisup-adv. MIT license. Free to use.
9Data available for download on https://github.com/hendrycks/robustness. Apache License 2.0. Free to use.
23
Under review as a conference paper at ICLR 2022
dl∖d2	brightness	contrast	defocus	elastic	fog	gaussian_blur
Clean	0.1279	0.3219	0.2646	0.2115	0.2324	0.3069
FGSM	0.1303	0.3128	0.2642	0.2098	0.2289	0.3064
PGD	0.1873	0.3082	0.2616	0.2319	0.2414	0.2959
						
d1∖d2	glass_blur	jpeg	motion_blur	pixelate	gaussian_noise impulse_noise	
Clean	0.2809	0.1838	0.2520	0.2365	0.2999	0.2869
FGSM	0.2760	0.1853	0.2520	0.2417	0.2918	0.2807
PGD	0.2825	0.2026	0.2605	0.2551	0.2980	0.2866
d1∖d2	saturate	shot_noise	snow	spatter	zoom_blur	SPeCkIe_noise
Clean	0.1335	0.2832	0.2033^^	0.1930	0.2654	0.2829
FGSM	0.1329	0.2754	0.2003	0.1946	0.2657	0.2759
PGD	0.1932	0.2841	0.2148	0.2297	0.2711	0.2901
Table 5: D-distances between difficulty functions of vanilla / FGSM / PGD training and training
based on 18 different corruptions on CIFAR10-C. We run each experiment for 4 times and report the
average value.
Figure 3: The relationship between the difficulty function based on the average loss values and the
one based on the average 0-1 errors. The left figure is based on the RN18-200 model; the right figure
is based on the WRN34 model. The correlation between these two metrics are 0.9466 (left) and
0.9545 (right), respectively.
1.0-
0.8-
0.6-
0.4-
0.2-
o.o-
0.0	0.2
0.4	0.6	0.8	1.0
Difficulty Measured by Average Loss
In the definition of our difficulty metric in Equation (1), the difficulty of one instance is based on
its average loss values during the training procedure. It is intuitive, because the values of the loss
objective represents the cost that model needs to fit the corresponding data point. The bigger this
cost is, the more difficulty this instance will be. To make the metric stable and prevent the metric
from being sensitive to the stochasticity in the training dynamics, we use the average value of the
loss objective for each instance to define its difficulty. In addition to the average loss objectives,
we can also use the average 0-1 error to define the difficulty function. In Figure 3, we plot the
relationship between the difficulty metric based on the average loss values and the one based on the
average 0-1 error for instances in the CIFAR10 training set when we train a RN18-100 model and
a WRN34 model. We can see a strong correlation between them for both models. The correlation
of the difficulty measured by two metrics for the same instance is 0.9466 in the RN18-100 case and
0.9545 in the WRN34 case. The high correlation indicates we can use either metric to measure the
difficulty. Since the loss objective values are continuous and finer-grained, we choose it as the basis
of the difficulty function we use in this paper.
D.2 Examples of Easy and Hard Instances of CIFAR 1 0 and SVHN
Figure 20 and 21 demonstrate the easiest and the hardest examples of each class from CIFAR10 and
SVHN, respectively. The difficulty of these instances is calculated based on PGD attacks. We can
see most easy examples are visually consistent, while most hard examples are ambiguous or even
incorrectly labeled.
24
Under review as a conference paper at ICLR 2022
D.3 Training on a Subset
Different Optimization Method on Hard Instances We find the failure of PGD adversarial training
on the hardest 10000 instances of CIFAR10 training set does not arise from the optimizers. In
Figure 5, we use SGD with different initial learning rates (“SGD, lr=1e-2” and “SGD, lr=1e-3”) and
the adaptive optimizer such as Adam (Kingma & Ba, 2014) (“Adam, lr=1e-4”). The learning rate of
SGD optimizer decrease to its 1/10 in the 100th and 150th epoch, while the learning rate of Adam
optimizer is fixed during training. Although optimizers like Adam can make the model fit the training
subset better, none of these methods can make the robust test accuracy significantly above the chance
of random guesses, i.e., 10%.
Longer Training Duration In Figures 7 and 8, we conduct adversarial training for a longer duration
until the loss on the hard training instances converges. Specifically, the model is trained for 600
epochs, with a learning rate is initialized to 0.1 and divided by 10 after every 100 epochs. In Figure 7,
we adversarially train a RN18 model on the hardest 10000 training instances. Our conclusions from
Section 4.1 still hold: Adversarial training on the hard instances leads to much more severe overfitting,
greatly widening the generalization gap. In Figure 8, we adversarially train a RN18 model on the
whole training set and calculate the average loss on the groups G0, G3, G6 and G9. Similarly to
training for 200 epochs, the model first fits the easy training instances and then the hard ones. This
can be seen by the fact that the average loss of G9 decreases much faster in the beginning and quickly
saturates. In other words, the harder the group, the later we see a significant decrease in its average
loss value. This observation is also consistent with our findings in Section 4.2.
Results on SVHN dataset Figure 6 demonstrates the learning curves of PGD adversarial training
based on a subset of the easiest, the random and the hardest instances of SVHN dataset. We let the
size of each subset be 20000, because the training set of SVHN is larger than that of CIFAR10. The
model architecture is RN18 in these cases. We have the same observations here: training on the
hardest subset yields trivial performance, training on the random subset has significant generalization
decay in the late phase of training while training on the easiest subset does not.
Different Values of and l2-based Adversarial Budget Figure 4 and Figure 9 demonstrate the
learning curves of RN18 models under different adversarial budgets on CIFAR10, in both l∞ and l2
cases. In l∞ cases, the adversarial budgets are 2/255, 4/255 and 6/255; in l2 cases, the adversarial
budgets are 0.5, 0.75 and 1. With the increase in the size of the adversarial budget, we can see a clear
transition from the vanilla training: more and more severe generalization decay when training on the
random or the hardest subset.
Training with Different Amount of Data In Figure 10, we compare the learning curves of PGD
adversarial training on increasing more training data, with the easiest ones coming first. If we do
model selection on a validation set as in (Rice et al., 2020), the selected models are still better on
both CIFAR10 and SVHN when they are trained with more data, although the final models in these
cases are not necessarily better. The results indicate the hard instances are still useful to improve the
model’s performance, but we need to utilize them in a different way.
(a) = 2/255
08
0.6-
04
0.2 ∙
0.0-
0	25	50	75	100 125 150 175 200
Epoch
(b) = 4/255
(c) = 6/255
Figure 4: Learning curves of training on PGD-perturbed inputs against different sizes of l∞ norm
based adversarial budgets using the easiest, the random and the hardest 10000 training instances. The
instance difficulty is determined by the corresponding adversarial budget and is thus different under
different adversarial budgets. The dashed lines are robust training error on the selected training set,
the solid lines are robust test error on the entire test set.
25
Under review as a conference paper at ICLR 2022
Epoch
Figure 5: Learning curves of PGD adversari-
ally trained models on the hardest 10000 in-
stances in the CIFAR10 training set by dif-
ferent optimizers. The dashed lines are ro-
bust training error on the selected training
instances, the solid lines are robust test error
on the entire test set.
0.8
£ 0.6
ro
c≤
R 0.4
LU
0.2
0.0
---EaSy 20k
—Random 20k
---Hard 20k
0	25	50	75	100 125 150 175 200
Epoch
Q 8≡4 2
Ioooo
①eH」0」」山
O
O
4
O "5
O O
3 P
E
O
1
∞
5
Figure 6: Learning curves obtained by train-
ing using the easiest, the random and the hard-
est 20000 instances of the SVHN training set.
The training error (dashed lines) is the robust
error on the selected instances, and the robust
test error (solid lines) is always the error on
the entire test set.
Figure 7: Learning curves of PGD adversar-
ial training on the hardest 10000 training in-
stances. The model is trained for 600 epochs.
The training error (dashed lines) is the robust
error on the selected instances, and the robust
test error (solid lines) is always the error on
the entire test set.
0	25	50	75	100 125 150 175 200
Epoch
(a) e = 0.50
0	25	50	75	100 125 150 175 200
Epoch
(b) e = 0.75
Epoch
Figure 8: The left vertical axis represents the
average loss of the training instances in the
groups G0, G3, G6 and G9 . The right vertical
axis represents the robust error for the whole
training (dashed grey line) and test (solid grey
line) set. The model is trained for 600 epochs.
0	25	50	75	100 125 150 175 200
Epoch
(c) € = 1.00
Figure 9: Learning curves of training on PGD-perturbed inputs against different size of l2 norm
based adversarial budgets using the easiest, the random and the hardest 10000 training instances. The
instance difficulty is determined by the corresponding adversarial budget and is thus different under
different adversarial budgets. The dashed lines are robust training error on the selected training set,
the solid lines are robust test error on the entire test set.
26
Under review as a conference paper at ICLR 2022
Figure 10: Learning curves of PGD adversarial training using increasing more training data in
CIFAR10 and SVHN. The dashed lines represent the robust training error on the selected training
instances; the solid lines represent the robust test error on the entire test set. Left: we use the easiest
10000, 20000, 30000, 40000 and the whole training set of CIFAR10. Right: we use the easiest 20000,
30000, 40000 and the whole training set of SVHN.
D.4 Lipschitz Analysis
Training Set	LiPsChitz in l∞ Cases (× 104)	LiPsChitz in l2 Cases (× 104) e = 2/255 e = 4/255 e = 8/255	e = 0.50	e = 0.75	e = 1.00
Easy10K Random10K Hard10K	591	606	1454	334	367	291 28.98	79.96	93.63	30.01	31.28	39.34 72.42	117.60	567.24	60.62	80.06	77.55
Table 6: Upper bound of the Lipschitz constant under different settings of and training instances.
We conduct numerical analysis to validate Theorem 2 of Section 5.2. Calculating the exact Lipschitz
constant of a neural network models is NP-hard, so we utilize the method introduced in (Scaman &
Virmaux, 2018) to derive the upper bound of Lipschitz constants instead. We use CIFAR10 as the
dataset and RN18 as the network architecture.
Table 6 demonstrates the upper bound of the Lipschitz constant of models trained in different settings.
In the l∞ cases, we set to be 2/255, 4/255 and 8/255; in the l2 cases, we set to be 0.5, 0.75 and 1.
Due to the stochasticity introduced by power method, we run the algorithm in (Scaman & Virmaux,
2018) for 20 times and report the average, we find the variance is negligible. Based on the results in
Table 6, it is clear that models adversarially trained by the hard training instances have much larger
Lipschitz constant than the ones by the easy training instances in all cases.
Figure 11 demonstrates the curves of the Lipschitz upper bound when the model is adversarially
trained by the easiest, the random and the hardest 10000 instances. The adversarial budget is l∞
norm based and = 8/255. We can clearly see that as the training goes, the Lipschitz upper bound
increases in all cases, which is consistent with our analysis in Section 5.2. In addition, compared
with training on easy instances, the Lipschitz upper bound of the models adversarially trained on hard
instances increases much faster.
D.5 Revisiting Existing Methods Mitigating Adversarial Overfitting
Existing methods mitigating adversarial overfitting can be generally divided into two categories:
one is to use adaptive inputs, such as (Balaji et al., 2019); the other is to use adaptive targets, such
as (Chen et al., 2021b; Huang et al., 2020). Both categories aim to prevent the model from fitting
hard input-target pairs. In this section, we pick one example from each category for investigation. We
provide the learning curves of the methods we study in Figure 12. We use the same hyper-parameters
as in these methods’ original paper, except for the training duration and learning rate scheduler, which
follow our settings. These methods clearly mitigate adversarial overfitting: The robust test error does
not increase much in the late phase of training, and the generalization gap is much smaller that that of
PGD adversarial training.
27
Under review as a conference paper at ICLR 2022
8867432」
Oooooooo
①H-0」」山
6 5 4 3 2
Ooooo
Illll
Iusu8 zu。Sd-IPUnO」①dd∩
5
17
O
5
2
1
O E
O O
1 P
E
5
2
Figure 11: The curves of the Lipschitz upper bound when the model is adversarially trained by the
easiest, the random and the hardest 10000 instances. The y-axis is log-scale.
O
Io
2
5
I 7
1
O
5
-
*
'O O
1 P
E
5
2
Figure 12: Learning curves of PGD adversarial training (PGD AT), instance-adaptive training (IAT)
and self-adaptive training (SAT). Dashed lines and solid lines represent the robust training error and
the robust test error, respectively.
Instance-Adaptive Training Using an instance-adaptive adversarial budget has been shown to miti-
gate adversarial overfitting and yield a better trade-off between the clean and robust accuracy (Balaji
et al., 2019). In instance-adaptive adversarial training (IAT), each training instance xi maintains its
own adversarial budget’s size i during training. In each epoch, i increases to i + ∆ if the instance
is robust under this enlarged adversarial budget. By contrast, i decreases to i - ∆ if the instance is
not robust under the original adversarial budget. Here, ∆ is the step size of the adjustment.
We use the same settings as in (Balaji et al., 2019) except that we use the same number of training
epochs and learning rate scheduling as the one in other experiments for fair comparison. Specially,
we set the value of and ∆ to be 8/255 and 1.9/255, respectively, same as in (Balaji et al., 2019).
The first 5 epochs are warmup, when we use vanilla adversarial training (Madry et al., 2018).
Figure 13 demonstrate the relationship between the instancewise adversarial budget i and the
corresponding instance’s difficulty d(xi). It is obvious that they are highly correlated: the correlation
is 0.844. Therefore, instance-adaptive training adaptively uses smaller adversarial budgets for hard
training instances, which prevents the model from fitting hard input-target pairs.
Self-Adaptive Training Self-adaptive training (SAT) (Huang et al., 2020) solves the adversarial
overfitting issue by adapting the target. By contrast with common practice consisting of using
a fixed target, usually the ground-truth, SAT adapts the target of each instance to the model’s
output. Specifically, after a warm-up period, the target ti for an instance xi is initialized as a
one-hot vector by its ground-truth label yi and updated in an iterative manner after each epoch as
ti J Pti + (1 - ρ)θi. Here, P is a predefined momentum factor and o% is the output probability of
the current model on the corresponding clean instance. SAT uses the loss of TRADES (Zhang et al.,
2019b) but replaces the ground-truth label y with the adaptive target ti: LSAT (xi) = L(xi, ti) +
λmax∆i∈s(e) KL(θi∖∖oi), where KL refers to the Kullback-Leibler divergence and λ is the weight
for the regularizer. Furthermore, SAT uses a weighted average to calculate the loss of a mini-batch;
the weight assigned to each instance xi is proportional to the maximum element of its target ti but
28
Under review as a conference paper at ICLR 2022
d(xi)
Figure 13: The instance adaptive adversarial
budget size i for the training set of CIFAR10
as a function of the instance difficulty d(xi).
The model architecture is RN18 and the value
of is 8/255.
Epoch
Figure 14: Average weights of different
groups in the training set of CIFAR10 dur-
ing training. During the warmup period (first
90 epochs), the weight for every training in-
stance is 1. The model architecture is RN18.
normalized to ensure that all instances’ weights sum up to 1. By weighted averaging, the instances
with confident predictions are strengthened, whereas the ambiguous instances are downplayed.
Similarly, we use the same settings as in (Huang et al., 2020) except we use the same number of
training epochs and learning rate scheduling: we train the model for 200 epochs and the first 90
epochs are the warmup period.
Figure 14 demonstrates the average weight assigned to instances belonging to the group G0, G3, G6
and G9. It is clear that the hard instances are assigned much lower weights than the easy instances.
For example, the weight assigned to G0, the easiest 10% training instances, is close to 1, while the
weight assigned to G9, the hardest 10% training instances, is only around 0.4.
Furthermore, Figure 15 shows the accuracy of the group G0 , G3 , G6 and G9 during training using the
original ground-truth label 1y and the adaptive target t, respectively. For the adaptive target, one
adversarial instance x0 is considered to be correctly classified if and only if arg maxi fw(x0)i =
arg maxi ti. For easy instances, 1y is mostly close to t, so accuracy in both cases is high and the gap
between them is small. For hard instances, 1y is usually not consistent with t, while accuracy under
the adaptive target t is much higher than the group-truth label y. This indicates self-adaptive training
makes adaptive target easier to fit for the originally hard instances.
Figure 15: Robust training accuracy during training when we use the original groundtruth label (left)
or use the adaptive target calculated during training (right).
D.6 Extra Results and Discussion on Fast Adversarial Training
We also conduct ablation study in the context of fast adversarial training. In Figure 16, we change
the value of β in our algorithm (pseudocode in Algorithm 1) and plot the learning curves. Lower
the value of β is, more weights assigned to the adaptive part of the target: β = 0 means we directly
utilize the moving average target as the final target, β = 1 means we use the one-hot groundtruth
29
Under review as a conference paper at ICLR 2022
≈eHLU
0	5	10	15	20	25	30	35
Epoch
Figure 16: The learning curves with different
values of β . The solid curve and the dashed
curve represent the robust test error and the
robust training error, respectively.
≈e≈一。」」LU
0	5	10	15	20	25	30	35
Epoch
Figure 17: The learning curves of ATTA with
and without reweighting. The solid curve and
the dashed curve represent the robust test error
and the robust training error, respectively.
P9u∙s,sSV£M 96saΛ4
0.0
0.0	0.2	0.4	0.6	0.8	1.0
Difficulty Level
Figure 18: The relationship between the dif-
ficulty value and the weight assigned to each
instances when using reweighting. We use
the average weight across epochs. The corre-
lation between them is 0.8900.
0.2
一 9qn%t8
0.0	0.2	0.4	0.6	0.8	1.0
Difficulty Level
Figure 19: The relationship between the dif-
ficulty value and the average value of the
true label’s probability when using the adap-
tive targets. The correlation between them is
0.9604.
label. Figure 16 clearly shows us the generalization gap decreases with the decrease in β. That is to
say, the adaptive target can indeed improve the generalization performance.
Figure 17 compare the learning curves of ATTA (Zheng et al., 2020) with and without reweighting.
The first 5 epochs are the warmup period. The results confirm that the reweighting scheme can
prevent adversarial overfitting and decrease the generalization gap.
To confirm that the algorithm we use is consistent with our theoretical and empirical analysis in
Section 4 and 5, we study the relationship between the instance difficulty and the weight assigned
to them when using reweighting, as well as the soft target when using adaptive targets. Since the
evaluation of model robustness is based on the PGD attack, the difficulty value here is based on the
PGD perturbation. In Figure 18, we demonstrate the relationship between the difficulty value and
the average assigned weight for each instance when using reweighting. We calculate the correlation
between these two values on the training set, it is 0.8900. This indicates we indeed assign smaller
weights for hard training instances and assign bigger weights for easy training instances. In Figure 19,
we show the relationship between the difficulty value and the average value of the true label’s
probability in the soft target when we use the adaptive targets. Similarly, we calculate the correlation
between these two values on the training set, it is 0.9604. This indicates the adaptive target is similar
to the ground-truth one-hot target for the easy training instances, while the adaptive target is very
different from the ground-truth one-hot target for the hard training instances. This means, adaptive
targets prevent the model from fitting hard training instances while encourage the model to fit the
easy training instances.
30
Under review as a conference paper at ICLR 2022
D.7 Extra Results and Discussion on Adversarial Finetuning
We conduct ablation study and the results are demonstrated in Table 7. It is clear that both reweighting
and the KL regularization term benefit the performance of the finetuned model.
Duration Method	AUtoAttaCkl Duration Method	AUtoAttaCk
-WRN34 on CIFAR10, C 二	二 8/255	I	RN18 on SVHN, C 二	^o-02
No Fine Tuning	52.01	1 No Fine Tuning	-~6777-
PanmaAT	^54.1f	一 ；	^VanlllaAT	一—70.81 ―一
RW	54.69	「「 LRW	70.83
1 EpoCh KL	54.73	I 1 Epoch KL	72.29
RW + KL	54.69	；	RW + KL	72.53
YlnmaAT	^55.49"	-T	^VanιllaAT	—^72.18 ―一
RW	56.41	1 RW	72.72
5 EpoCh KL	56.55	I 5 Epoch TrT I	KL	73.17
	RW + KL	56.99	I	RW + KL	73.35
Table 7: Ablation study on the influence of reweighting (RW) and the KL regularization term (KL) in
the performanCe of adversarial finetuning with additional data.
31
Under review as a conference paper at ICLR 2022
plane, 0999 plane plane, 0.999 plane plane, 0998 plane
SSBBΘ3
plane, 0.996 plane plane, 0.995 plane plane, 0.995 plane
PtS，，囱甲。W图
BBSHSS
plane, 098 plane plane, 0.98 plane plane, 0986 plane
SSSSΘΘ
plane, 0.000	bird	plane, 0.002	frog	plane, 0.002	frog
EIEI 1隔屈信
plane, 0.003	bird	plane, 0.003	ship	plane, 0.005	truck
Iillpii 日 Iiii
均啕海法工W
plane, 0.006	truck	plane, 0.007	frog	plane, 0.007	ship
屋算埃比覆日
plane, 0.007	car	plane, 0.007	cat	plane, 0.008	bird
Ξ□EEES
plane, 0.008	deer	plane, 0.008	horse	plane, 0.009	frog
国厨E≡E≡匿⑥
car, 1.000 car car, 0.999 car car, 0.999 car
圆国园园园国
car, 0.999 car car, 0.998 car car, 0.998 car
H H Ξ Ξ
rar " 440	rar	rar " ”,	rar	rar " ”,	rar
car, 0.998
car
car, 0.997
car
car, 0.997
car
rar " ”,	rar	rar " ”,	rar	rar " ”,	rar
car, 0.997
car
car, 0.997
car
car, 0.997
car
SBSSBH
ca∣∙, 0.996 car car, 0.996 car car, 0.996 car
(a) Plane, Easy.
(b) Plane, Hard.
(c) Car, Easy.
bird, 0.997 bird bird, 0.994 bird bird, 0.988 bird
bird, 0.001 horse bird, 0.001 Ship bird, 0.004 dog
cat, 0.896
cat
cat, 0.866
cat
cat, 0.845
cat
L：- J A QΛΛ	L：- J	A QA <	L：	<S。，IS	L：- J
bird, 0.988
bird
bird, 0.984
bird
bird, 0.978
bird
L： A Λ"Λ	，--	WlS <S«。	，--	L： A Λ"S	"
bird, 0.008
dog
bird, 0.009
dog
bird, 0.009
truck
L:_J λ Off L:_J L:_J narr L:-J L:-J a ar< L:-J
bird, 0.976
bird
bird, 0.966
bird
bird, 0.961
bird
L： A Λ"S	——	WlS <S«。	，--	L： Λ Λl I	"
∖国4Ma 4
L:_J λ	L:_J L:_J λ err L:_J L:_J n orn L:_J
bird, 0.009
cat
bird, 0.009
dog
bird, 0.011
truck
巴日目的0鬣
cat, 0.841 cat cat, 0.837 cat cat, 0.830 cat
ππ□□ss
cat, 0.830
cat
cat, 0.827
cat
cat, 0.824
cat
bird, 0.959
bird
bird, 0.956
bird
bird, 0.950
bird
Ll-J l-L I-Ll 1	L — - -	L ； , 1 I-LI-LlT	L - - - -	Ll-J I-LI-LlT	-k；_
bird, 0.011
horse
bird, 0.012
horse
bird, 0.012
shp

cat, 0.795
cat
cat, 0.786
cat
cat, 0.779
cat
L：-J A Al -ɪ .______________I. L：-J AAl-I .....I. L：-J A Λ> I I_____________________________________________
bird, 0.012
truck
bird, 0.013
truck
bird, 0.014
horse
“一 —一 —一 —— .____________________________________________>
l:-j a qγλ l:-j l:-j λ Oi f l:-j l:-j a o<λ l:-j
bird, 0.950
bird
bird, 0.945
bird
bird, 0.940
bird
aaæaɛs
IHiiiiiibi	画

cat, 0.777
cat, 0.771
cat
cat
cat, 0.774
cat, 0.766
cat
cat
cat, 0.773
cat, 0.757
cat
cat

deer, 0990
(e) Bird, Easy.
(f) Bird, Hard.
(g) Cat, Easy.
deer
deer, 0.968
deer
deer, 0961
deer
HΘΞΞHS
( ■ Λ OlO	>	> ■ Λ Ol Λ	>	> ■ A QΛΛ	> ■
deer, 0949
deer
deer, 0.910
deer
deer, 0908
deer
H0□B0S
(	■ A QΛΛ	>	>	■ A	>	>	■ A 1S”	>	■
deer, 0900
deer
deer, 0.882
deer
deer, 0.876
deer

deer, 0.876
deer
deer, 0.867
deer
deer, 0.855
deer
B	H	E	E	H	H
deer, 0.840 deer	deer, 0.836 deer	deer, 0.836 deer
HH0SEa
deer, 0.829
deer
deer, 0.818
deer
deer, 0.817
deer

(i) Deer, Easy.
frog, 0.976 frog frog, 0.970 frog frog, 0.970 frog
EBBBHH
frog, 0.962 frog frog, 0.961 frog frog, 0.958 frog

frog
frog, 0.948
frog
frog, 0.948
frog
rog, 0.933 frog frog, 0.933 frog
0≡E3E3
(m) Frog, Easy.
ship, 0996 ship ship, 0.993 ship ship, 0990 ship
BBEEas
ship, 0.985	ship	ship, 0.981	ship	ship, 0.981	ship
ship, 0980	ship	ship, 0.978	ship	ship, 0978	ship
冽冽国国H
ship, 0977	ship	ship, 0.977	ship	ship, 0977	ship
巴四日四口口
ship, 0976	ship	ship, 0.973	ship	ship, 0971	ship
ΞΞ33ΞΞ
ship, 0970	ship	ship, 0.970	ship	ship, 0969	ship
(q) Ship, Easy.
deer, 0.001
dog
deer, 0.005 truck
deer, 0.005 tιuck
HHBaaa
J--- A AAA 1-_ _	J ___ Λ Λ> Λ	I . .	Λ Λ> >	L：-J
deer, 0.008
frog
deer, 0.010
frog
deer, 0.011
bird
dog, 0987
dog
dog, 0.987
dog
dog, 0985
dog
□□□□ME
dog, 0984
dog
dog, 0.984
dog
dog, 0984
dog
deer, 0.013 plane deer, 0.015
ship
deer, 0.017
dog

ααoΞ0θ
(	■ Λ Λl O	-W	- 1S1S、、	>	■ ΛΛɔ<	. ■ ■
deer, 0.019
匹
deer, 0.023 horse deer, 0.024
car
HSRiRiaa
deer, 0.025 truck deer, 0.025
frog
deer, 0.026
Shp
■■■昭日日
J--- A AT A	L.__	J--- A I	> ■ Λ Λ^C -W
deer, 0.028 horse deer, 0.034 truck
deer, 0.035
Shp
㈤㈤园淘㈣骂
(j) Deer, Hard.
dog, 0983
dog
dog, 0.970
dog
dog, 0961
dog

dog, 0960
dog
dog, 0.953
dog
dog, 0953
dog

dog, 0952
dog
dog, 0.948
dog
dog, 0947
dog
OOΞΞ00
dog, 0946
dog
dog, 0.945
dog
dog, 0944
dog

(k) Dog, Easy.
frog, 0.002
cat
frog, 0.003 horse frog, 0.004
horse
horse, 1.000 horse horse, 1.000 horse horse, 0.999 horse
frog, 0.007	cat	frog, 0.007	cat	frog, 0.008	cat
EfR附典QQ
，--A Al A	，--	L. - A Λ> >	J---	1-. - A Λ> >	L：-J
frog, 0.010
deer
frog, 0.011
deer
frog, 0.011
bird
■■■■园园
frog, 0.017
frog, 0.023
plane frog, 0.017
Shp
frog, 0.025
dog
bird
frog, 0.018
frog, 0.033
Pane
bird
1-. _ Λ Λ^>^)	J--- A. - C C、，	. --	1-. _ Λ Λ^∣Λ	L：-J
frog, 0.033
deer
frog, 0.037
car
frog, 0.038
bird
UU袈晶网网
horse, 0.998	horse	horse, 0.998	horse	horse, 0.998	horse
第富盟盛届H
horse, 0.997	horse	horse, 0.997	horse	horse, 0.997	horse
□ΞHHBB
horse, 0.996	horse	horse, 0.995	horse	horse, 0.995	horse
出M■■*宣
horse, 0.994	horse	horse, 0.994	horse	horse, 0.994	horse
父过■■■■
(n) Frog, Hard.
(o) Horse, Easy.
car, 0.001 truck car, 0.002 Cat car, 0.002 cat
car, 0.006
，r, 0.012
car, 0.013 plane c<r 0.014 frog car, 0.015 cat
(d) Car, Hard.
cat, 0.001
frog
cat, 0.001
car
cat, 0.002
Pane

cat, 0.004
Plane
cat, 0.004
car
cat, 0.007
ship
■00骡◎留
— 一““，、	L：_J	_ _ Λ Λl I	，―一	L._.
cat, 0.013
bird
cat, 0.014
deer
cat, 0.016
horse
A Al T	.____I.	. _ Λ ΛI O	L	Λ Λ->Λ	J. -
cat, 0.017
truck
cat, 0.019
bird
cat, 0.020
dog
A Λ-t1	.____I.	■一 Λ All	L ：-J	CC'，	J____
cat, 0.023
truck
cat, 0.024
bird
cat, 0.027
deer
aa≡sss
cat, 0.031 ship cat, 0.032 truck cat, 0.032 frog
HHSSQQ
(h) Cat, Hard.
dog, 0.004 truck
dog, 0.004
truck
dog, 0.005 truck
5
J. - A C L.__ J. - A ΛΛD	.	J. - A Λ> A	L.__
dog, 0.007
horse
dog, 0.009
truck
dog, 0.010
horse
昼昼■记噩刷
J. - A Λ> I -U- J. - A Λ> C J--- J. - A Λ> A 1-. _
dog, 0.014
Ship
dog, 0.015
deer
dog, 0.018
frog
回应E*黛随
dog, 0.018
frog
dog, 0.018
frog
dog, 0.019
plane
KS KS窃窃厅N
J. - a CrC J--- J. - a CrC	.	—
dog, 0.020
deer
dog, 0.020
truck
dog, 0.021
plane
J. - a	L:-J J. - a cr、 L._ -	，.- CCr、	L._-
dog, 0.021
bird
dog, 0.022
horse dog, 0.022
horse

(l) Dog, Hard.
horse, 0.001
horse, 0.005
deer horse, 0.002 frog
horse, 0.003 ship
dog
horse, 0.005 plee horse, 0.006
bird
□□SΘEE
U___ Λ ΛΛΛ	L_ _ n ΛΛΛ	_ -	L___ Λ ΛΛΛ I .
horse, 0.008 truck horse, 0.008
Cat
horse, 0.008 frog
U___ Λ ΛΛS L：_J L__________ n ΛΛS	_ -	L___ ΛΛIΛ L：-J
horse, 0.009
bird horse, 0.009
ca
horse, 0.010
bird
DHHHSH
L：_J	A Λ> A 1-. _	L.__ AAIH J--.
horse, 0.010
bird horse, 0.010 frog
horse, 0.012 deer

(p) Horse, Hard.
ship, 0.000 frog ship, 0.000 plane ship, 0.000 dog
ship, 0.006 frog ship, 0.007 deer ship, 0.007 cat
ship, 0.011 truck ship, 0.012 bird ship, 0.012 deer
(r) Ship, Hard.
truck, 1.000
truck, 0.992
truck
truck
truck, 1.000
truck, 0.991
truck
truck
truck, 0999
truck, 0991
truck
truck

(s) Truck, Easy.
tιuιck, 0.001	plane truck, 0.002 deer	tιuιck, 0.002
bird
口口 口 口 口口
,0.011
k, 0.011
cat
,0.013
tack, 0.013 plane truck, 0.015 plee tack, 0.016 hoιse
(t) Truck, Hard.
Figure 20: Easy and hard examples in each category of CIFAR10 dataset. In each subfigure, odd
columns present the original images, and even columns present the PGD-perturbed images. Above
each image, we provide the normalized difficulty defined in Equation (1) as well as the labels: true
labels for the original images and the predicted labels for the perturbed images.
32
Under review as a conference paper at ICLR 2022
0, 0998	0	0, 0.997	0	0, 0997	0
0, 0.993
^^0∣^^
0, 0.993
0, 0992
^^0∣^^
0, 0993
虬。ia□o□。口。□
0, 0.000	3	0, 0.000	5	0, 0.000	7
0, 0.001	5	0, 0.001	2	0, 0.002	1
SlSl 烟更Gnm
0, 0.004	7	0, 0.004	3	0, 0.004	1
■■■■区四
0, 0.005	1	0, 0.005	1	0, 0.006	5
0, 0.007	1	0, 0.007	6	0, 0.009	5
切国■■■■
0, 0.010	1	0, 0.010	5	0, 0.012	7
1, 0999	1	1, 0.998	1	1, 0998	1
KQKsα□□□
1, 0.998	1	1, 0.998	1	1, 0.998	1
CGCQKEKEniE
1, 0998	1	1, 0.998	1	1, 0998	1
翻 KlllllEEEE
1, 0.997	1	1, 0.997	1	1, 0.996	1
nιiffl□ιmm
1, 0.996 1 1, 0.995 1 1, 0.995 1
1, 0.995 1 1, 0.994 1 1, 0.994 1
Ddieieigiq
1, 0.000	3	1, 0.000	6	1, 0.000	3
1, 0.001	8	1, 0.001	2	1, 0.001	3
间罔““
1, 0.001	6	1, 0.001	3	1, 0.001	2
■■国画22
1, 0.001	5	1, 0.002	5	1, 0.002	3
圆圆■■国闰
1, 0.002	2	1, 0.002	5	1, 0.002	5
■■5日回回
1, 0.003	6	1, 0.004	2	1, 0.004	2
国@11■回回
(a) 0, Easy
(b) 0, Hard
(c) 1, Easy
(d) 1, Hard
2, 0.001
1
2, 0.001
9
2, 0.001
1
■■■isnn
2, 0.002	3	2, 0.002	7	2, 0.002	1
回回口口 SESB
3
3
3, 1.000
3, 0.999
3
3
3, 1.000
2, 0.002
1
2, 0.002
1
2, 0.003
6
3
3, 0.998
3
^3^
3, 0.996
T
3, 0.999
3, 0.997
3, 0.996
6	3, 0.000	0	3, 0.001	9

3, 0.005
^Γ^
3, 0.005
^^9^^
4, 1.000
(e) 2, Easy
(f) 2, Hard
(g) 3, Easy
(h) 3, Hard
4
4, 1.000
4
4, 0999
4
EIEl田丑闻4
4, 0999	4	4, 0.999	4	4, 0999	4
4, 0999	4	4, 0.999	4	4, 0999	4
4, 0.999	4	4, 0.999	4	4, 0.999	4
EIEI⑷⑷目醒
(i) 4, Easy
4, 0.000
4, 0.002
3
0
4, 0.002
7
4, 0.002
8
5
5, 0.998
5
5, 0.994
5
8
5, 0.000
9
5, 0.001
7
6, 0999
6
6, 0.997
6
6, 0.997
6
蹲⑪⑹⑹用阳
6, 0994

6, 0.990

6, 0990

6, 0987
6
6, 0.987
6
6, 0986
6
E [SEES®
6, 0983
6
6, 0.983
6
6, 0981
6
匠匠旧⑹⑹⑹
6, 0980
6
6, 0.979
6
6, 0.979
6

(m) 6, Easy
8, 1.000
8
8, 0.999
8
8, 0.998
8
8, 0.994
^^S^^
8, 0.992
^^S^^
8, 0.992
^^S^^
⑻⑻困阻恻恻
。。 。 " *。。 。 。 。
8, 0.992
^^S^^
8, 0.988
^^S^^
8, 0.987
^^S^^
8, 0.987
^^S^^
8, 0.984
^^S^^
8, 0.981
^^S^^
8, 0.981
^^S^^
8, 0.980
^^S^^
8, 0.978
^^S^^

8, 0.977
8
8, 0.976
8
8, 0.974
8
⑻⑻KK整凝
(q) 8, Easy
应的
4, 0.002
7
4, 0.002
4, 0.003
7
1
4, 0.002
4, 0.003
2
0
5, 0.001
0
5, 0.001
1
1
■■口口对就
4, 0.003	1	4, 0.004	0	4, 0.007	1
□□■■EE
4, 0.008
1
4, 0.008
8
4, 0.010
1

4, 0.010
6, 0.003
2
4, 0.010
0
4, 0.010
9
5
5
5, 0.986
5, 0.984
5
5
5, 0.984
5, 0.983
5
5
1	5, 0.003	1	5, 0.003	9
raæænn
1	5, 0.004	1	5, 0.004	4
(j) 4, Hard
(k) 5, Easy
(l) 5, Hard
1
6, 0.003
5
6, 0.003
0
ΠΠBH□□
6, 0.003
^Γ^
6, 0005
^^7^^
6, 0.005


6, 0.005
3
6, 0006
7
6, 0.006
0
日内・■血山
6, 0.006	5	6, 0007	1	6, 0.007	1
6, 0.009
3
6, 0009
0
6, 0.009
7
aE3■曜■*
(n) 6, Hard
7
7, 0.998
7
7, 0.997
7
0
7, 0.001
4
7, 0.002
4
8, 0.001
4
8, 0.001
1
8, 0.001
1
H0田里ISll
。“ nm » 。 “ ∩m > 。 " >
8, 0.001
8, 0.004
^Γ^
^Γ^
8, 0001
8, 0004
^^3^^
^^5^^
8, 0.003
8, 0.004
^^3^^
^^0∣^^
8, 0.005	1	8, 0005	0	8, 0.005	9
3SBHES5]
。“ nne 。 " nne ι 。 " nne *
8, 0.006
^2
8, 0006
^Γ^
8, 0.006
^^9^^
8, 0.007	4	8, 0007	1	8, 0.007	1
(r) 8, Hard
(o) 7, Easy
(p) 7, Hard
9



9
9, 0.997
9, 0.995
9, 0.990
9, 0.988
9, 0.981
9



9
9, 0.996
9, 0.993
9, 0.988
9, 0.988
9, 0.978
(s) 9, Easy
9



9
5
9, 0.001
8
9, 0.001
1
(t) 9, Hard
Figure 21: Easy and hard examples in each category of SVHN dataset. In each subfigure, odd
columns present the original images, and even columns present the PGD-perturbed images. Above
each image, we provide the normalized difficulty defined in Equation (1) as well as the labels: true
labels for the original images and the predicted labels for the perturbed images.
33