Under review as a conference paper at ICLR 2022
A Free Lunch from the Noise:
Provab le and Practical Exploration for Representation Learning
Anonymous authors
Paper under double-blind review
Ab stract
Representation learning lies at the heart of the empirical success of deep learning
for dealing with the curse of dimensionality. However, the power of representa-
tion learning has not been fully exploited yet in reinforcement learning (RL), due
to i), the trade-off between expressiveness and tractability; and ii), the coupling
between exploration and representation learning. In this paper, we first reveal the
fact that under some noise assumption in the stochastic control model, we can ob-
tain the linear spectral feature of its corresponding Markov transition operator in
closed-form for free. Based on this observation, we propose Spectral Dynamics
Embedding (SPEDE), which breaks the trade-off and completes optimistic explo-
ration for representation learning by exploiting the structure of the noise. We pro-
vide rigorous theoretical analysis of SPEDE, and demonstrate the practical supe-
rior performance over the existing state-of-the-art empirical algorithms on several
benchmarks.
1	Introduction
Reinforcement learning (RL) dedicates to solve the sequential decision making problem, where an
agent is interacting with an unknown environment to find the best policy that maximizes the expected
cumulative rewards (Sutton & Barto, 2018). Itis known that the tabular algorithms direct controlling
over the original state and action achieve the minimax-optimal regret depending on the cardinality of
the state and action space (Jaksch et al., 2010; Osband & Van Roy, 2016; Azar et al., 2017; Jin et al.,
2018). However, these algorithms become intractable for the real-world problems with an enormous
number of states, due to the curse of dimensionality. Learning with function approximation upon
good representation is a natural idea to tackle the curse and serving as the key for the success of
deep learning (Bengio et al., 2013). In fact, representation learning lies at the heart of the empirical
successes of deep RL in video games (Mnih et al., 2013), robotics (Levine et al., 2016), Go (Silver
et al., 2017), dialogue systems (Jiang et al., 2021) to name a few. Meanwhile, the importance and
benefits of the representation in RL is rigorously justified (Jin et al., 2020; Yang & Wang, 2020),
which quantifies the regret in terms of the dimension of the known representation based on a subclass
in Markov decision processes (MDPs) (Puterman, 2014). A natural question raises:
How to design provably efficient and practical algorithm for representation learning in RL?
Here, by “provably efficient” we mean the sample complexity of the algorithm can be rigorously
characterized only in terms of the complexity of representation class, without explicit dependency
on the number of states and actions, while by “practical” we mean the algorithm can be imple-
mented and deployed for the real-world applications. Therefore, we not only require the representa-
tion learned is expressive enough for handling complex practical environments, but also require the
operations in the algorithm tractable and computation/memory efficient. The major difficulty of this
question lies in two-fold:
i)	The trade-off between the expressiveness and the tractability in the design of the represen-
tations;
ii)	The learning of representation is intimately coupled with exploration.
Specifically, a desired representation should be sufficiently expressive1 to capture the dynamic sys-
tem, while still tractable in practice. However, in general, expressive representation leads to com-
plicated optimization in learning. For example, the representation in the linear MDP is exponential
1For a formal definition of expressiveness, see (Agarwal et al., 2020a).
1
Under review as a conference paper at ICLR 2022
stronger than the latent variable MDPs in terms of expressiveness (Agarwal et al., 2020a). However,
its representation learning is either depending on a MLE oracle that is computational intractable due
to the constraint on the regularity of conditional density (Agarwal et al., 2020a), or a complicated
constrained min-max-min-max optimization (Modi et al., 2021). On the other hand, Misra et al.
(2020) considers the representation introduced by an encoder in block MDP (Du et al., 2019), in
which the learning problem can be completed by a regression, but with the payoff that the represen-
tations in block MDP is even weaker than latent variable MDP (Agarwal et al., 2020a).
Meanwhile, the coupling of the learning of representation and exploration induces the difficulty in
practical algorithm design and analysis. One cannot learn a precise representation without enough
experiences from a comprehensive exploration, while the exploration depends on an reliable es-
timation of the representation. Most of the known results depends on a policy-cover-based explo-
ration (Du et al., 2019; Misra et al., 2020; Agarwal et al., 2020a; Modi et al., 2021), which maintains
and samples a set of policies during training for systematic exploration, that significant increases the
computation and memory cost in implementation.
In this work, we propose Spectral Dynamics Embedding (SPEDE), bypassing the aforementioned
difficulties and answering the question affirmatively. SPEDE is established on an observation that
connects the stochastic control dynamics (Osband & Van Roy, 2014; Kakade et al., 2020) with
linear MDPs in Section 3. Specifically, by exploiting the property of the noise in the stochastic
control dynamics, we can recover the factorization of its corresponding Markov transition operator in
closed-form without extra computation. This equivalency immediately overcome the computational
intractability in the model estimation in Agarwal et al. (2020a), which breaks the trade-off between
expressiveness and tractability.
More importantly, the connection unifies two highly-related but commonly-known different mod-
els, i.e., stochastic nonlinear control models (Kakade et al., 2020) and linear MDPs (Jin et al.,
2020), therefore, provides the opportunity to share benefits from both sides: i), it sheds the light
on exploiting optimistic control for exploratory representation learning, instead of expensive policy-
cover-based exploration; and ii), it introduces the linear sufficient feature from the spectral space of
Markov operator, in which the planning can be completed efficiently.
We rigorously characterize the statistical property of SPEDE in terms of regret w.r.t. the complexity
of representation class in Section 4, without explicit dependence on the raw feature of state and
action. With the established unified view, our results generalize online control (Kakade et al., 2020)
and linear MDP (Jin et al., 2020) beyond known features. We finally demonstrate the superior
of SPEDE on the MuJoCo benchmarks in Section 5. It significantly outperforms the empirical state-
of-the-art RL algorithms. To our knowledge, SPEDE is the first representation learning algorithm
achieving statistical, computational, and memory efficiency with sufficient expressiveness.
1.1	Related Work
There have been many great attempts to learn a variety of algorithmic representation learning in
RL designed for different purposes, e.g., bisimulation (Ferns et al., 2004; Gelada et al., 2019; Castro,
2020), reconstruction (Watter et al., 2015; Hafner et al., 2019). Recently, there are also several works
considering the spectral features based on decomposing different variants of the transition operator,
including successor features (Dayan, 1993; Kulkarni et al., 2016), proto-value functions (Mahadevan
& Maggioni, 2007; Wu et al., 2018), spectral state-aggregation (Duan et al., 2018; Zhang & Wang,
2019), and contrastive fourier features (Nachum & Yang, 2021). These works are highly-related to
the proposed SPEDE. Besides these features focus on state-only representation, the major difference
between SPEDE and these spectral features lies in i), the target operators in existing spectral features
are state-state transition, which cancel the effect of action; ii), the target operators are estimated
based on empirical data from a fixed behavior policy under the implicit assumption that the estimated
operator is uniformly accurate, ignoring the major difficulty in exploration, while SPEDE carefully
designed the systematic exploration with theoretical guarantee; iii), most of the existing spectral
features rely on explicitly decomposion of the operators, while SPEDE obtains the spectral for free.
Turning to the rigorously-justified representation learning, a large body of effort focuses on
policy-cover-based methods in an “explore-then-commit” strategy (Du et al., 2019; Misra et al.,
2020; Agarwal et al., 2020a; Modi et al., 2021). These algorithms learn a uniformly accurate
model/representation through a reward-free exploration, upon which decouple the learning from the
exploration. Then, the representation from the learned model can be used for optimal policy seeking
for the particular reward. The major difficulty impedes their practical application is the computa-
2
Under review as a conference paper at ICLR 2022
tion and memory cost: the policy-cover-based exploration requires a set of exploratory polices to be
maintained and sampled from during training, which can be extremely expensive.
Another two related lines of research are model-based RL and online control, which are commonly
known overlapped but separate communities considering different formulations of the dynamics.
Our finding establishes the equivalency among the model, so that bridges these communities. Osband
& Van Roy (2014) and Kakade et al. (2020) are the most related to our work in each community.
These models generalize their corresponded linear models, i.e., Jin et al. (2020) and Cohen et al.
(2019), with general nonlinear model and kernel function within a known RKHS, respectively. The
regret of the optimstic (pessimistic) algorithm has been carefully characterized for these models.
However, both of the proposed algorithms in Osband & Van Roy (2014) and Kakade et al. (2020)
require an planning oracle to seek the optimal policy, which might be computational intractable.
In SPEDE, this is easily handled in the equivalent linear MDP.
2	Preliminaries
Markov Decision Process (MDP) is one of the most standard models studied in the reinforcement
learning that can be denoted by the tuple M = (S, A, r, T, μ, H), where S is the state space, A
is the action space, r : S × A → R+ is the reward function2 (where R+ denotes the set of non-
negative real numbers), T : S × A → ∆(S) is the transition and μ is an initial state distribution
and H is the horizon3 (i.e. the length of each episode). A (potentially non-stationary) policy π
can be defined as {πh }h∈[H] where πh : S → A, ∀h ∈ [H]. Following the standard notation, we
define the value function Vhπ (sh ) := ET,π PtH=-h1 r(st, at)|sh = s and the action-value function
(i.e. the Q function) Qhπ (sh , ah ) = ET,π PtH=-h1 r(st, at)|sh = s, ah = a , which are the expected
cumulative rewards under transition T when executing policy π starting from sh and (sh , ah ). With
these two definitions at hand, it is straightforward to show the following Bellman equation:
Qh (Sh, ah) = r(sh, ah) + ESh+1 〜T (∙∣Sh,ah)Vhπ+1(sh+1).
Reinforcement learning aims at finding the optimal policy π* = argmax∏ Es〜μV0∏(s). It is well
known that in the tabular setting when the state space and action space are finite, we can provably
identify the optimal policy with both sample-efficient and computational-efficient optimism-based
methods (e.g. Azar et al., 2017; Jin et al., 2018; Zhang et al., 2021) with the complexity proportion
to |S||A|. However, in practice, the cardinality of state and action space can be large or even infinite.
Hence, we need to incorporate function approximation into the learning algorithm when we deal with
such cases. The linear MDP (Jin et al., 2020) or low-rank MDP (Agarwal et al., 2020a; Modi et al.,
2021) is the most well-known reinforcement learning model that can incorporate linear function
approximation with theoretical guarantee, thanks to the following assumption on the transition:
T (SOBa) = hφ(s,a),μ(Sy)H,
(1)
where φ : S×A→H, μ : S → H are two feature maps and H is a Hilbert space. The most
essential observation for them is that, Qhπ(s, a) for any policy π is linear w.r.t φ(sh, ah), due to the
following observation (Jin et al., 2020):
Vhπ+1(sh+1)T(sh+1 |sh, ah) dsh+1 = φ(sh, ah),/ vπ+l(sh+l)μ(sh+l)dsh+).	⑵
Therefore, φ serves as a sufficient representation for the estimation of Qhπ , that can provide uncer-
tainty estimation with standard linear model analysis and eventually lead to sample-efficient learning
when φ is fixed and known to the agent (see Theorem 3.1 in Jin et al., 2020). However, we in general
do not have such representation in advance4 and we need to learn the representation from the data,
which constraints the applicability of the algorithms derived with fixed and known representation.
3	Spectral Dynamics Embedding
It is naturally to consider how to perform sample-efficient representation learning (and hence
sample-efficient reinforcement learning) that satisfies (1) in an online manner. The most straight-
2In general, the reward can be stochastic. Here for simplicity we assume the reward is deterministic and
known throughout the paper, which is a common assumption in the literature (e.g., Jin et al., 2018; 2020;
Kakade et al., 2020).
3Our method can be generalized to infinite horizon case, see Section 3.2 for the detail.
4One exception is the tabular MDP, where we can choose φ : S × A → R|S|2 |A| that each state-action pair
has exclusive |S| non-zero element and μ : S → R|S| |A| correspondingly defined to make (1) hold.
3
Under review as a conference paper at ICLR 2022
forward idea is performing the maximum likelihood estimation (MLE) in the representation space
(e.g., Agarwal et al., 2020a). Unfortunately, for general cases, such MLE is intractable, due to
the constraints on the regularity of marginal distribution (i.e., hφ(s, a), £0 μ(s0) ds0)= 1) for all
(s, a) ∈ S × A. Moreover, even we can perform MLE for certain cases (for example, the block
MDP), as the representation is estimated from the data, which can be inaccurate, all of the existing
work apply the policy cover technique (Du et al., 2019; Misra et al., 2020; Agarwal et al., 2020a;
Modi et al., 2021) to enforce exploration. However, such procedure can be both computational and
memory expansive when we need amounts of exploratory policy to guarantee the coverage of whole
state space, which makes it not a good practical choice.
To overcome these issues, we introduce Spectral Dynamics Embedding (SPEDE), which leverages
the noise structure to provide a simple but provable efficient and practical algorithm for represen-
tation learning in RL. We first introduce our key observation, which provides us a practical way to
implement the optimism in the face of uncertainty (OFU) principle.
3.1	Key Observation
Our most important observation is that, the density of isotropic Gaussian distribution can be ex-
pressed as the inner product of two feature maps, thanks to the reproducing property and the random
Fourier transform of the Gaussian kernel5 (Rahimi & Recht, 2007):
,/ I 2r、	k l∣χ一μk2∖
φ(x∣μ, σ2I) (X exp (---寻-)
(Reproducing Property) =(k(x, ∙), k(μ, ∙))h
(Random Fourier Transform) = hφ(x, ω, b),夕(μ, ω, b))p(ω b),
(3)
(4)
where k(∙, ∙) is the Gaussian kernel with bandwidth σ: k(x,y) = exp (一 kx-yk2), H is the
Reproducing Kernel Hilbert Space (RKHS) associated with k,夕(x,ω,b) = √2cos(ω>x + b),
hf, g) p = Ep(χ)[f (x)g(x)] and p(ω,b) = N Q;0,1∕σ2I) ∙ U (b; [0, 2π]) with N and U denoting
Gaussian and Uniform distribution, respectively.
Consider the general transition dynamics,
s0 = f*(s, a) + e, e 〜 N(0, σ2), equivalently T(s0∣s, a) X exp (-ks„81 ) ,	(5)
which is the basic setup in the empirical model-based reinforcement learning (e.g., Chua et al., 2018;
Kurutach et al., 2018; Clavera et al., 2018; Wang et al., 2019), and the online (non)-linear control
(e.g., Abbasi-Yadkori & Szepesvari, 2011; Mania et al., 2019; 2020; Simchowitz & Foster, 2020;
Kakade et al., 2020). Here S ∈ Rd, a ∈ A that can be continuous and f * is a dynamic function.
By applying the reproducing property (3) or random Fourier transform (4) for the transition dynam-
ics (5), we can obtain the feature φ and μ satisfies (1) for free. Specifically, taking the reproducing
property as an example, we have that
T (s0∣s, a) = hk(f * (s, a), ∙), (2πσ2)-2k(s0, ∙)))h,	(6)
which means the problem (5) is indeed a linear MDP with φ(s, a) = k(f *(s, a), ∙) and μ(s0)=
(2πσ2)-42k(s0, ∙). Following (2), we know Q(s, a) is in the linear span of the φ(s, a) that is
transformed from f* (s, a). Therefore, finding a good representation of (s, a) is equivalent to finding
a good estimation of f*. In the next section, we will show that, with the well-known optimism in
the face of uncertainty (OFU) principle, we can estimate f* in an online manner with a both sample-
efficient and computational-efficient algorithm.
Remark (Computation-free Factorizable Noise Model): We want to remark that, similar obser-
vations also hold for large amounts of distribution, e.g., the Laplace and Cauchy distribution. We
refer the interested reader to Table 1 in Dai et al. (2014) for the known transformation of kernels
and features. Here we focus on the Gaussian noise, as Gaussian distribution is the most natural
continuous distribution we should work on.
3.2	Practical Algorithm Description
Here, we introduce a generic Thompson Sampling (TS) type algorithm in Algorithm 1 based on the
OFU principle that leverage our observation at the previous section. At the beginning, we provide a
5We provide a brief review on the related definitions in Appendix A.
4
Under review as a conference paper at ICLR 2022
Algorithm 1 Thompson Sampling (TS) Algorithm
Require: Number of Episodes K, Prior Distribution P(f), Reward Function r(s, a).
1:	Initialize the history set H0 = 0.
2:	for episodes k = 1, 2,•一do
3:	Sample fk 〜 P(f∣Hk).	. Draw the Representation.
4:	Find the optimal policy πk on fk with Algorithm 2.	. Planning with fk .
5:	for steps h = 0,1,…，H 一 1 do	.Executing ∏k.
6:	Execute ah 〜∏h(sh).
7:	Observe sh+1.
8:	end for
9:	Set Hk = Hk-1 ∪ {(skh, akh, skh+1)}hH=-01.	. Update the History.
10:	end for
Algorithm 2 Planning with Dynamic Programming
Require: Transition Model f, Reward Function r(s, a).
1:	Initialize φ(s, a), μ(s0) with ⑶ or (4). VH(S) = 0, ∀s.
2:	for steps h = H _ 1, H 一 2,…，0 do
3:	Calculate Qh(S,a) = r(s,a) + hφ(s, a), ʃ Vh+ι(s0)μ(s0) ds0iH.	. Bellman Update.
4:	Set Vh(s) = maxa Qh(s, a), πh(s) = arg maxa Qh(s, a). . Choose the Optimal Policy.
5:	end for
6:	return {πh}hH=-01.
prior distribution P(f) that reflects our prior knowledge on f *. Then for each episode, We draw a f
from the posterior, find the optimal policy with f using the planning algorithm, execute this policy
and eventually inference the posterior with the new observation. Notice that, we choose the policy
optimistically with an sampled f, which enforces the exploration following the principle of OFU.
Meanwhile, we only learn the dynamic with posterior inference and directly obtain the representa-
tion with Equation (3) or (4), which avoids additional error from the representation learning step. As
all of our data is collected with f *, our posterior will shrink to a point mass of f *, which guarantees
we can identify good representation and good policy with sufficient number of data.
One significant part of SPEDE is the computational-efficient planning with fk, thanks to the linear
MDP formulation (6). Prior work assumes an oracle (e.g., Kakade et al., 2020) for such planning
problem, but little is known on how to provably perform such planning efficiently. Notce that, with
the feature φ(S, a) defined via (3) and (4), we know that Qhπ(S, a) is exactly linear in φ(S, a), ∀h, π.
Hence, we can perform a dynamic programming style algorithm that calculate Qh (S, a) with the
given feature φ(S, a), and then greedily select the action at each level h, which is simple yet efficient.
It is straightforward to show that the policy obtained with this dynamic programming algorithm is
optimal with the mathematical induction. We illustrate the detailed algorithm in Algorithm 2.
In such planning algorithm, we need to calculate the term / Vh+ι(s0)μ(s0) ds0 and take the maxi-
mum of Qh(S, a) over a, which can be problematic when the number of states and actions can be
large or even infinite. We will provide more discussion on this issue later.
We then address several practical issues in implementing the proposed SPEDE:
Posterior Sampling The exact posterior inference can be hard if f * doesn,t lie in simple function
class (e.g., linear function class) or has some derived property (e.g., conjugacy), so in practice we
apply the existing mature approximate inference methods like Markov Chain Monte Carlo (MCMC)
(e.g., Neal et al., 2011) and variational inference (e.g., Blei et al., 2017; Kingma & Welling, 2013).
In our implementation, we used Stochastic Gradient Langevin Dynamics (Welling & Teh, 2011;
Cheng & Bartlett, 2018) to train an ensemble of models for achieving the purpose.
Large State and Action Space In general, we need to handle the case when the number of states
and actions can be large, or even infinite. Notice that, when the state space is large, we can estimate
the term / Vh+ι(s0)μ(s0) ds0 with regression based method using the samples from f (Ernst et al.,
2005; Antos et al., 2008). For the continuous action space, we can apply principled policy opti-
mization methods (e.g., Agarwal et al., 2020b) with an energy-based model (EBM) parametrized
policy (Nachum et al., 2017; Dai et al., 2018), treat the linear Qπ(S, a) as the gradient and perform
mirror descent and eventually obtain the optimal policy. However, this is at the cost of an additional
sampling step from the EBM policy. In practice, we introduce a Gaussian policy and perform soft
actor-critic (Haarnoja et al., 2018) policy update, which already provides good empirical perfor-
5
Under review as a conference paper at ICLR 2022
mance. To sum up, for large state and action cases, we learn the critic in the learned representation
space by regression, and obtain the Gaussian parametrized actor with SAC policy update step, in
Line 3 and 4 in Algorithm 2, respectively.
Infinite Horizon Case Our algorithm can be provably extended to the infinite horizon case with
specific termination condition for each episode (e.g., see Jaksch et al., 2010). And in practice, for the
planning part we can solve the linear fixed-point equation with the feature φ(s, a) using the popular
algorithms like Fitted Q-iteration (FQI) (Ernst et al., 2005; Antos et al., 2008). that still guarantees
to find the optimal policy.
4 Theoretical Guarantees
In this section, we provide theoretical results for SPEDE, showing that SPEDE can identify informa-
tive representation and as a result, near-optimal policy in a sample-efficient way. We first define the
notation of regret. Assume at episode k, the learner chooses the policy πk and observes a sequence
{(skh, akh)}hH=-01. We define the regret of the first K episodes (and define T := KH) as:
一	Regret(K) := £在口 [V0*(s£) - V0πk(s£)]	⑺
We want to provide a regret upper bound that is sublinear in T , as when T increases, we collect
more data that can help us build a much more accurate estimation on the representation, which
should decrease the per-step regret and make the overall regret scale sublinear in T . As we consider
the Thompson Sampling algorithm, we would like to study the expected regret EP(f) [Regret(K)],
which takes the prior P(f) into account.
4.1	Assumptions
Before we start, we first state the assumptions we use to derive our theoretical results.
Assumptions on the environment We make the following assumption on the environment, which
is common in the literature (e.g. Azar et al., 2017; Jin et al., 2018; 2020).
Assumption 1 (Bounded Reward). r(s, a) ∈ [0, 1], ∀(s, a) ∈ S × A.
Assumptions on the function class F In practice, We generally approximate f * with some com-
plicated function approximators, so we focus on the setting where we want to find f * from a general
function class F that need not to be linear with certain feature map. This can be helpful for Mu-
JoCo benchmarks that have angle, angular velocity and torque of the agent in the raw state, which
we don’t know how to construct the feature map to make f linear. We first state some necessary
definitions and assumptions on F.
Definition 1 ('2-norm of functions). Define kf ∣∣2 := max(s,a)∈s×A Ilf (s, a)∣∣2∙ Notice that it is not
the commonly used `2 norm for the function, but it suits our purpose well.
Assumption 2 (Bounded Output). We assume that If I2 ≤ C, ∀f ∈ F.
Assumption 3 (Realizability). We assume the ground truth dynamic function f * ∈ F.
We then define the notion of covering number, which will be helpful in our algorithm derivation.
Definition 2 (Covering Number (Wainwright, 2019)). An -cover of F with respect to a metric ρ is
a set {fi}i∈[n] ⊆ F, such that ∀f ∈ F, there exists i ∈ [n], ρ(f, fi) ≤ . The -covering number is
the cardinality of the smallest -cover, denoted as N(F, , ρ).
Assumption 4 (Bounded Covering Number). We assume that N(F, e, k∙∣∣2) < ∞, ∀e > 0.
Remark Basically, Assumption 2 means the the transition dynamic never pushes the state far from
the origin, which holds widely in practice. Assumption 3 guarantees that we can find the exact f * in
F, or we will always suffer from the error induced by model mismatch. Assumption 4 ensures that
we can estimate f* with small error when we have sufficient number of observations.
Besides the bounded covering number, we also need an additional assumption on bounded eluder
dimension, which is defined in the following:
Definition 3 (e-dependency (Osband & Van Roy, 2014)). A state-action pair (s, a) ∈ S ×
A is e-dependent on {(si, ai)}i∈[n] ⊆ S × A with respect to F, if ∀f, f ∈ F satisfying
q
i∈[n] If(si, ai) - f(si, ai)I22 ≤ e satisfies that If (s, a)-f(s, a)I2 ≤ e. Furthermore, (s, a) is
said to be e-independent of {(si, ai)}i∈[n] with respect to Fifitisnot e-dependent on {(si, ai)}i∈[n].
Definition 4 (Eluder Dimension (Osband & Van Roy, 2014)). We define the eluder dimension
dimE (F, e) as the length d of the longest sequence of elements in S × A, such that ∃e0 ≥ e,
every element is e0-independent of its predecessors.
6
Under review as a conference paper at ICLR 2022
Remark Intuitively, eluder dimension illustrates the number of samples we need to make our
prediction on unseen data accurate. If the eluder dimension is unbounded, then we cannot make any
meaningful prediction on unseen data, even we have large amounts of samples. Hence, to make the
learning possible, we need the following bounded eluder dimension assumption.
Assumption 5 (Bounded Eluder Dimension). We assume dimE (F, ) < ∞, ∀ > 0.
4.2	Main Result
Theorem 5 (Regret Bound). Assume Assumption 2 to 5 holds. We have that
Epf)[Regret(K)] ≤ O(PH2T ∙ logN(F,TT/2, ∣H∣2) ∙ dimE(F,TT/2)).
where O represents the order up to logarithm factors.
For finite dimensional function class, logN(F,T-1/2, k ∙ ∣∣2) and dimE(F, T-1/2)) should be
scaled like polylog(T), hence our upper bound is sublinear in T. The proof is in Appendix C.
Here we briefly sketch the proof idea.
Proof Sketch. We first construct an equivalent UCB algorithm (see Appendix B) and bound
Regret(K) for it. Then by the conclusion from Russo & Van Roy (2013; 2014); Osband & Van Roy
(2014), we can directly translate the upper bound on Regret(K) from UCB algorithm to an upper
bound on EP(f) [Regret(K)] ofTS algorithm.
With the optimism, We know for episode k, V0* (Sk) ≤ VVnk (sk), where Vhrk is the value function of
policy πk under the model fk introduced in the UCB algorithm. Hence, the regret at episode k can
be bounded by Vnk (Sk) - V0∏k (Sk), which is the value difference of the policy πk under the two
models fk and f *, that can be bounded by «E [PH-1 ∣If *(sh, aQ - fk(s(,。1)1同 (See Lemma
14 for the details), which means when the estimated model f is close to the real model f* , the policy
obtained by planning on f will only suffer from a small regret. With Cauchy-Schwartz inequality,
we only need to bound E [Pk∈[κ] PH=-Illf *(Sh, ah) - fk(sjζ, ah)∣∣2]. This term can be handled
via Lemma 18. With some additional technical steps, we can obtain the upper bound on Regret(K)
for the UCB algorithm, and hence the upper bound on Ep(f)[Regret(K)] for the TS algorithm. □
Kernelized Non-linear Regulator Notice that, for the linear function class F = {θ>夕(s, a):
θ ∈ R%×d} where 夕：S × A → R% is a fixed and known feature map of certain RKHS6,
when the feature and the parameters are bounded, the logarithm covering number can be bounded
by logN(F, e, k ∙ ∣∣2) . dφ log(1∕e), and the eluder dimension can be bounded by dimE(F, e).
dψ log(1∕e) (see Appendix D for the detail, notice that we provide a tighter bound of the eluder
dimension compared with the one derived in Osband & Van Roy (2014)). Hence, for linear function
class, Theorem 5 can be translated into a regret upper bound of O(HdψT1/2) for sufficient large
T, that matches the results of Kakade et al. (2020)7. Moreover, for the case of linear bandits when
H = 1, our bound can be translated into a regret upper bound of O(dψT1/2), that matches the lower
bound (Dani et al., 2008) up to logarithmic terms.
Compared with Kakade et al. (2020) and Osband & Van Roy (2014) Our results have some
connections with the results from Kakade et al. (2020) and Osband & Van Roy (2014). However,
in Kakade et al. (2020), the authors only considers the case when F only contains linear functions
w.r.t some known feature map, which constrains its application in practice. We instead, consider
the general function approximation, which makes our algorithm applicable for more complicated
models like deep neural networks. Meanwhile, the regret bound from Osband & Van Roy (2014)
depends on a global Lipschitz constant for the value function, which can be hard to quantify with
either theoretical or empirical method. Instead, our regret bound gets rid of such dependency on the
Lipschitz constant with the simulation lemma that carefully exploit the noise structure.
6 Note that, the RKHS here is the Hilbert space that contains f (s, a) with the feature from some fixed and
known kernel, It is different from the RKHS we introduced in Section 3, that contains Q(s, a) with the feature
k(f (s, a), ∙) where k is the Gaussian kernel.
7 Note that T in (Kakade et al., 2020) is the number of episodes, and Vmax in (Kakade et al., 2020) can be
viewed as H2 when the per-step reward is bounded.
7
Under review as a conference paper at ICLR 2022
5	Experiments
In this section, we study the empirical performance of SPEDE in the OpenAI MuJoCo control
suite (Brockman et al., 2016). We use the environments from MBBL (Wang et al., 2019), which
varies slightly from the original environments in terms of modifying the reward function so its gra-
dient w.r.t. the states exists and introducing early termination (ET). Note that the set of environments
contains various control and manipulation tasks, which are commonly used for benchmarking both
model-free and model-based RL algorithms (Yu et al., 2020; Kakade et al., 2020; Luo et al., 2018;
Haarnoja et al., 2018). As aforementioned, for practical implementation, our critic network consists
of a representation network φ(∙) and a linear layer on the top. We follow the same procedure of
Algorithm 1: (1) For finding the optimal policy, we run an actor-critic algorithm (SAC). (2) We
update the representation network of the critic function φ(∙) with a momentum from the model dy-
namics network f (∙). We found this prevent updating the representation of too fast. We provide
the full set of experiments in Appendix E.2, an ablation on the momentum in Appendix E.3 and the
hyperparameter we use in Appendix E.6. 8
Baselines We compare our method with various model-based RL baselines: PETS (Chua et al.,
2018) with random shooting (RS) optimizer, PETS with cross entropy method (CEM) optimizer and
ME-TRPO (Kurutach et al., 2018). Note that these are strong empirical baselines with many hand-
tuned hyperparameters and engineering features (e.g., ensemble of models). Itis usually hard for any
theoretically guaranteed model-based RL algorithm to match or surpass their performance (Kakade
et al., 2020). Another natural baseline is the successor feature (Dayan, 1993), which is one of the
representative spectral features. We compare with the deep successor feature (DeepSF) (Kulkarni
et al., 2016), and for a fair comparison, we only swap the representation objective of SPEDE with
DeepSF and keep the other parts of the algorithm exactly the same.
SPEDE: Performance with the Learned Representation Following Algorithm 1, we are inter-
ested in how SPEDE performs when we conduct planning on top of the representation induced by the
dynamics model in each episode. As most of the rigorously-justified representation learning algo-
rithms are computationally intractable/inefficient, to demonstrate the effectiveness of representation
used in SPEDE, we compare SPEDE with the deep successor features, which is one representative
empirical representation learning algorithm. Moreover, as our method essentially models transi-
tion dynamics, we compare our methods with the state-of-the-art model-based RL algorithms. We
summarize the results of our method in Table 2. We see that our method achieves impressive per-
formance comparing to model-based RL methods. Even in some hard environments that baselines
fail to reach positive reward (e.g., MountainCar, Walker-ET), SPEDE manage to achieve a reward of
52.6 and 501.6 respectively. We also evaluate our representation by comparing SPEDE to the usage
of deep successor feature (DeepSF). Results show that on hard tasks like Humanoid and Walker,
SPEDE manages to achieve 452.6 and 336.0 higher reward respectively.
SPEDE-REG: Policy Optimization with SPEDE Representation Regularizer In order to eval-
uate whether our assumption on linear MDP is valid in empirical settings and study whether such as-
sumption can help improve the performance, we add our model dynamics representation objective as
a regularizer in addition to the original SAC algorithm for learning the Q-function. Specifically, the
algorithm SPEDE-REG consists of vanilla SAC objective with an additional loss putting constraints
on the representation learned by the critic function, due to the intuition that the representation should
satisfy the linear MDP dynamics. We compare its performance with the vanilla SAC algorithm to
show the benefits of dynamic representation. Results in Table 2 show that adding such constraint
significantly improve the performance of SAC: on hard tasks like Hopper-ET, S-Humanoid-ET and
Humanoid-ET, SPEDE-REG improves the performance of SAC by 694.8, 1875.7 and 2000.4.
Discussion of the Results We observe that in the environments with relatively simple dynamics
(top row of Table 2), the proposed SPEDE achieves the SoTA, even comparing with SAC. However,
when the model dynamics of the environment become harder (bottom row of Table 2), the difference
of the performance between the two approaches begin to enlarge. Interestingly, our SPEDE achieves
strong results comparing to model-based approaches, while the joint learning SPEDE-REG outper-
forms model-free algorithm by a huge margin. The performance promotion of SPEDE indicates
the importance on learning a good representation based on model dynamics and again shows the
effectiveness of our approach in both settings.
In fact, the differences in the SoTA usage of SPEDE in easy environments and difficult environments
also reveals the important direction for our future work. The current rigorous representation learning
8Our code is available at https://sites.google.com/view/spede.
8
Under review as a conference paper at ICLR 2022
Table 1: Performance of SPEDE on various MuJoCo control tasks. All the results are averaged across 4
random seeds and a window size of 10K. Results marked With * is directly adopted from MBBL (Wang et al.,
2019). Our method achieves strong performance compared to pure empirical baselines (e.g., PETS). We also
compare SPEDE-REG which regularizes the critic using the model dynamics loss with several model-free RL
method. SPEDE-REG significantly improves the performance of the SoTA method SAC.
	Swimmer	Reacher	MountainCar	Pendulum	I-Pendulum
ME-TRPO*	30.1±9.7	-13.4±5.2	-42.5±26.6	177.3±1.9	-126.2±86.6
PETS-RS*	42.1±20.2	-40.1±6.9	-78.5±2.1	167.9±35.8	-12.1±25.1
PETS-CEM*	22.1±25.2	-12.3±5.2	-57.9±3.6	167.4±53.0	-20.5±28.9
DeepSF	25.5±13.5	-16.8±3.6	-17.0±23.4	168.6±5.1	-0.2±0.3
SPEDE	42.6±4.2	-7.2±1.1	50.3±1.1	169.5±0.6	0.0±0.0
PPO*	38.0±1.5	-17.2±0.9	27.1±13.1	163.4±8.0	-40.8±21.0
TRPO*	37.9±2.0	-10.1±0.6	-37.2±16.4	166.7±7.3	-27.6±15.8
TD3*	40.4±8.3	-14.0±0.9	-60.0±1.2	161.4±14.4	-224.5±0.4
SAC*	41.2±4.6	-6.4±0.5	52.6±0.6	168.2±9.5	-0.2±0.1
SPEDE-REG	40.0±3.8	-5.8±0.6	40.0±3.8	168.5±4.3	0.0±0.1
	Ant-ET	Hopper-ET	S-Humanoid-ET	Humanoid-ET	Walker-ET
ME-TRPO*	42.6±21.1	4.9±4.0	76.1±8.8	72.9±8.9	-9.5±4.6
PETS-RS*	130.0±148.1	205.8±36.5	320.9±182.2	106.9±106.9	-0.8±3.2
PETS-CEM*	81.6±145.8	129.3±36.0	355.1±157.1	110.8±91.0	-2.5±6.8
DeepSF	768.1±44.1	548.9±253.3	533.8±154.9	168.6±5.1	165.6±127.9
SPEDE	806.2±60.2	732.2±263.9	986.4±154.7	886.9±95.2	501.6±204.0
PPO*	80.1±17.3	758.0±62.0	454.3±36.7	451.4±39.1	306.1±17.2
TRPO*	116.8±47.3	237.4±33.5	281.3±10.9	289.8±5.2	229.5±27.1
TD3*	259.7±1.0	1057.1±29.5	1070.0±168.3	147.7±0.7	3299.7±1951.5
SAC*	2012.7±571.3	1815.5±655.1	834.6±313.1	1794.4±458.3	2216.4±678.7
SPEDE-REG	2073.1±119.7	2510.3±550.8	2710.3±277.5	3747.8±1078.1	2170.3±810.9
——SPEDE ——DeepSF-SAC ——PETS-CEM ——PETS-RS
Figure 1: Experiments on MuJoCo: We show curves of the return versus the training steps for SPEDE and
model-based RL baselines. Results show that in these tasks, our method enjoys better sample efficiency even
compared to SoTA empirical model-based RL baselines.
methods, e.g., Du et al. (2019); Misra et al. (2020); Agarwal et al. (2020a) and the proposed SPEDE,
all rely on some model assumption. When the assumptions are satisfied, e.g., Pendulum, Reacher,
and others, our theoretically derived SPEDE variant works extremely well, even better than current
SoTA. However, when the assumption is not fully satisfied, although the decoupled SPEDE achieves
best performance among existing model-based RL and representation learning under fair compari-
son, thejoint learned variant of SPEDE is more robust and promote the current SoTA with significant
margin. An interesting question is whether we can rigorouslyjustify the regularized SPEDE, which
we leave as our future work.
Performance Curves To better understand how the sample complexity of our algorithm compar-
ing to the prior model-based RL baselines, we plot the return versus environment steps in Figure 1.
We see that comparing to prior model-based baselines, SPEDE enjoys great sample efficiency in
these tasks. We want to emphasize that from MBBL (Wang et al., 2019), model-based methods al-
ready show significantly better sample efficiency compared to model-free methods (e.g.PPO/TRPO).
We provide additional results in Appendix E.2.
6	Conclusion
In this paper, we introduce SPEDE, which, to the best of our knowledge, is the first provable and
efficient representation learning algorithm for RL, by exploiting the benefits from noise. We provide
thorough theoretical analysis for SPEDE and strong empirical results demonstrate the effectiveness
of our algorithm. There are still several open problems, e.g., how to rigorously justify the regularized
SPEDE. We leave these problems as interesting future work.
9
Under review as a conference paper at ICLR 2022
Ethnics Statement Our paper focus on one of the core questions in RL, i.e., developing prov-
able and practical algorithms for representation learning in online scenarios under general settings.
Although there can be several potential application of our work in the future, currently we cannot
foresee any positive/negative social impact of our work other than the original positive/negative so-
cial impact of RL. As we conduct our experiments on standard MuJoCo benchmark, our experiments
should not suffer from any discrimination/bias/fairness concerns.
Reproducibility Statement Our code is directly adapted from the open-sourced code base
from (Wang et al., 2019) and we have introduced our implementation details in Section 5 and
Appendix E. We believe our results can be easily reproduced, and our code is available at
https://sites.google.com/view/spede.
References
Yasin Abbasi-Yadkori and Csaba Szepesvari. Regret bounds for the adaptive control of linear
quadratic systems. In Proceedings of the 24th Annual Conference on Learning Theory, pp. 1-
26. JMLR Workshop and Conference Proceedings, 2011.
Alekh Agarwal, Sham Kakade, Akshay Krishnamurthy, and Wen Sun. Flambe: Structural complex-
ity and representation learning of low rank mdps. arXiv preprint arXiv:2006.10814, 2020a.
Alekh Agarwal, Sham M Kakade, Jason D Lee, and Gaurav Mahajan. Optimality and approximation
with policy gradient methods in markov decision processes. In Conference on Learning Theory,
pp. 64-66. PMLR, 2020b.
Mauricio A Alvarez, Lorenzo Rosasco, and Neil D Lawrence. Kernels for vector-valued functions:
A review. Foundations and Trends® in Machine Learning, 4(3):195-266, 2012.
AndraS Antos, Csaba Szepesvari, and Remi Munos. Fitted q-iteration in continuous action-space
mdps. In J. Platt, D. Koller, Y. Singer, and S. Roweis (eds.), Advances in Neural Information
Processing Systems, volume 20. Curran Associates, Inc., 2008.
Nachman Aronszajn. Theory of reproducing kernels. Transactions of the American mathematical
society, 68(3):337-404, 1950.
Mohammad Gheshlaghi Azar, Ian Osband, and Remi Munos. Minimax regret bounds for reinforce-
ment learning. In International Conference on Machine Learning, pp. 263-272. PMLR, 2017.
Yoshua Bengio, Aaron Courville, and Pascal Vincent. Representation learning: A review and new
perspectives. IEEE transactions on pattern analysis and machine intelligence, 35(8):1798-1828,
2013.
David M Blei, Alp Kucukelbir, and Jon D McAuliffe. Variational inference: A review for statisti-
cians. Journal of the American statistical Association, 112(518):859-877, 2017.
Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman, Jie Tang, and
Wojciech Zaremba. Openai gym, 2016.
Pablo Samuel Castro. Scalable methods for computing state similarity in deterministic markov
decision processes. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 34,
pp. 10069-10076, 2020.
Xiang Cheng and Peter Bartlett. Convergence of langevin mcmc in kl-divergence. In Algorithmic
Learning Theory, pp. 186-211. PMLR, 2018.
Kurtland Chua, Roberto Calandra, Rowan McAllister, and Sergey Levine. Deep reinforcement learn-
ing in a handful of trials using probabilistic dynamics models. arXiv preprint arXiv:1805.12114,
2018.
Ignasi Clavera, Jonas Rothfuss, John Schulman, Yasuhiro Fujita, Tamim Asfour, and Pieter Abbeel.
Model-based reinforcement learning via meta-policy optimization. In Conference on Robot
Learning, pp. 617-629. PMLR, 2018.
10
Under review as a conference paper at ICLR 2022
Alon Cohen, Tomer Koren, and Yishay Mansour. Learning linear-quadratic regulators efficiently
with only ʌ/t regret. In International Conference on Machine Learning, pp. 1300-1309. PMLR,
2019.
Bo Dai, Bo Xie, Niao He, Yingyu Liang, Anant Raj, Maria-Florina F Balcan, and Le Song. Scal-
able kernel methods via doubly stochastic gradients. Advances in Neural Information Processing
Systems, 27:3041-3049, 2014.
Bo Dai, Albert Shaw, Lihong Li, Lin Xiao, Niao He, Zhen Liu, Jianshu Chen, and Le Song. Sbeed:
Convergent reinforcement learning with nonlinear function approximation. In International Con-
ference on Machine Learning, pp. 1125-1134. PMLR, 2018.
Varsha Dani, Thomas P Hayes, and Sham M Kakade. Stochastic linear optimization under bandit
feedback. 2008.
Peter Dayan. Improving generalization for temporal difference learning: The successor representa-
tion. Neural Computation, 5(4):613-624, 1993.
Simon Du, Akshay Krishnamurthy, Nan Jiang, Alekh Agarwal, Miroslav Dudik, and John Langford.
Provably efficient rl with rich observations via latent state decoding. In International Conference
on Machine Learning, pp. 1665-1674. PMLR, 2019.
Yaqi Duan, Zheng Tracy Ke, and Mengdi Wang. State aggregation learning from markov transition
data. arXiv preprint arXiv:1811.02619, 2018.
Damien Ernst, Pierre Geurts, and Louis Wehenkel. Tree-based batch mode reinforcement learning.
Journal of Machine Learning Research, 6:503-556, 2005.
Norm Ferns, Prakash Panangaden, and Doina Precup. Metrics for finite markov decision processes.
In UAI, volume 4, pp. 162-169, 2004.
Carles Gelada, Saurabh Kumar, Jacob Buckman, Ofir Nachum, and Marc G Bellemare. Deepmdp:
Learning continuous latent space models for representation learning. In International Conference
on Machine Learning, pp. 2170-2179. PMLR, 2019.
Geoffrey Grimmett and David Stirzaker. Probability and random processes. Oxford university
press, 2020.
Tuomas Haarnoja, Aurick Zhou, Pieter Abbeel, and Sergey Levine. Soft actor-critic: Off-policy
maximum entropy deep reinforcement learning with a stochastic actor. In International confer-
ence on machine learning, pp. 1861-1870. PMLR, 2018.
Danijar Hafner, Timothy Lillicrap, Ian Fischer, Ruben Villegas, David Ha, Honglak Lee, and James
Davidson. Learning latent dynamics for planning from pixels. In International Conference on
Machine Learning, pp. 2555-2565. PMLR, 2019.
Thomas Jaksch, Ronald Ortner, and Peter Auer. Near-optimal regret bounds for reinforcement
learning. Journal of Machine Learning Research, 11(4), 2010.
Haoming Jiang, Bo Dai, Mengjiao Yang, Tuo Zhao, and Wei Wei. Towards automatic evaluation of
dialog systems: A model-free off-policy evaluation approach. arXiv preprint arXiv:2102.10242,
2021.
Chi Jin, Zeyuan Allen-Zhu, Sebastien Bubeck, and Michael I Jordan. Is q-learning provably effi-
cient? In Proceedings of the 32nd International Conference on Neural Information Processing
Systems, pp. 4868-4878, 2018.
Chi Jin, Praneeth Netrapalli, Rong Ge, Sham M Kakade, and Michael I Jordan. A short
note on concentration inequalities for random vectors with subgaussian norm. arXiv preprint
arXiv:1902.03736, 2019.
Chi Jin, Zhuoran Yang, Zhaoran Wang, and Michael I Jordan. Provably efficient reinforcement
learning with linear function approximation. In Conference on Learning Theory, pp. 2137-2143.
PMLR, 2020.
11
Under review as a conference paper at ICLR 2022
Sham Kakade, Akshay Krishnamurthy, Kendall Lowrey, Motoya Ohnishi, and Wen Sun. Infor-
mation theoretic regret bounds for online nonlinear control. arXiv preprint arXiv:2006.12466,
2020.
Diederik P Kingma and Max Welling. Auto-encoding variational bayes. arXiv preprint
arXiv:1312.6114, 2013.
Tejas D Kulkarni, Ardavan Saeedi, Simanta Gautam, and Samuel J Gershman. Deep successor
reinforcement learning. arXiv preprint arXiv:1606.02396, 2016.
Thanard Kurutach, Ignasi Clavera, Yan Duan, Aviv Tamar, and Pieter Abbeel. Model-ensemble
trust-region policy optimization. arXiv preprint arXiv:1802.10592, 2018.
Sergey Levine, Chelsea Finn, Trevor Darrell, and Pieter Abbeel. End-to-end training of deep visuo-
motor policies. The Journal ofMachine Learning Research,17(1):1334-1373, 2016.
Yuping Luo, Huazhe Xu, Yuanzhi Li, Yuandong Tian, Trevor Darrell, and Tengyu Ma. Algorith-
mic framework for model-based deep reinforcement learning with theoretical guarantees. arXiv
preprint arXiv:1807.03858, 2018.
Sridhar Mahadevan and Mauro Maggioni. Proto-value functions: A laplacian framework for learn-
ing representation and control in markov decision processes. Journal of Machine Learning Re-
search, 8(10), 2007.
Horia Mania, Stephen Tu, and Benjamin Recht. Certainty equivalence is efficient for linear quadratic
control. In Proceedings of the 33rd International Conference on Neural Information Processing
Systems, pp. 10154-10164, 2019.
Horia Mania, Michael I Jordan, and Benjamin Recht. Active learning for nonlinear system identifi-
cation with guarantees. arXiv preprint arXiv:2006.10277, 2020.
Dipendra Misra, Mikael Henaff, Akshay Krishnamurthy, and John Langford. Kinematic state ab-
straction and provably efficient rich-observation reinforcement learning. In International confer-
ence on machine learning, pp. 6961-6971. PMLR, 2020.
Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Alex Graves, Ioannis Antonoglou, Daan Wier-
stra, and Martin Riedmiller. Playing atari with deep reinforcement learning. arXiv preprint
arXiv:1312.5602, 2013.
Aditya Modi, Jinglin Chen, Akshay Krishnamurthy, Nan Jiang, and Alekh Agarwal. Model-free
representation learning and exploration in low-rank mdps. arXiv preprint arXiv:2102.07035,
2021.
Ofir Nachum and Mengjiao Yang. Provable representation learning for imitation with contrastive
fourier features. arXiv preprint arXiv:2105.12272, 2021.
Ofir Nachum, Mohammad Norouzi, Kelvin Xu, and Dale Schuurmans. Bridging the gap between
value and policy based reinforcement learning. arXiv preprint arXiv:1702.08892, 2017.
Radford M Neal et al. Mcmc using hamiltonian dynamics. Handbook of markov chain monte carlo,
2(11):2, 2011.
Ian Osband and Benjamin Van Roy. Model-based reinforcement learning and the eluder dimension.
Advances in Neural Information Processing Systems, 27:1466-1474, 2014.
Ian Osband and Benjamin Van Roy. On lower bounds for regret in reinforcement learning. arXiv
preprint arXiv:1608.02732, 2016.
Martin L Puterman. Markov decision processes: discrete stochastic dynamic programming. John
Wiley & Sons, 2014.
Ali Rahimi and Benjamin Recht. Random features for large-scale kernel machines. In Proceedings
of the 20th International Conference on Neural Information Processing Systems, pp. 1177-1184,
2007.
12
Under review as a conference paper at ICLR 2022
Walter Rudin. Fourier analysis on groups. Courier Dover Publications, 2017.
Daniel Russo and Benjamin Van Roy. Eluder dimension and the sample complexity of optimistic
exploration. Advances in Neural Information Processing Systems, 2013.
Daniel Russo and Benjamin Van Roy. Learning to optimize via posterior sampling. Mathematics of
Operations Research, 39(4):1221-1243, 2014.
David Silver, Julian Schrittwieser, Karen Simonyan, Ioannis Antonoglou, Aja Huang, Arthur Guez,
Thomas Hubert, Lucas Baker, Matthew Lai, Adrian Bolton, et al. Mastering the game of go
without human knowledge. nature, 550(7676):354-359, 2017.
Max Simchowitz and Dylan Foster. Naive exploration is optimal for online lqr. In International
Conference on Machine Learning, pp. 8937-8948. PMLR, 2020.
Elias M Stein and Rami Shakarchi. Real analysis. Princeton University Press, 2009.
Richard S Sutton and Andrew G Barto. Reinforcement learning: An introduction. MIT press, 2018.
Martin J Wainwright. High-dimensional statistics: A non-asymptotic viewpoint, volume 48. Cam-
bridge University Press, 2019.
Ruosong Wang, Russ R Salakhutdinov, and Lin Yang. Reinforcement learning with general value
function approximation: Provably efficient approach via bounded eluder dimension. Advances in
Neural Information Processing Systems, 33, 2020.
Tingwu Wang, Xuchan Bao, Ignasi Clavera, Jerrick Hoang, Yeming Wen, Eric Langlois, Shunshi
Zhang, Guodong Zhang, Pieter Abbeel, and Jimmy Ba. Benchmarking model-based reinforce-
ment learning. arXiv preprint arXiv:1907.02057, 2019.
Manuel Watter, Jost Tobias Springenberg, Joschka Boedecker, and Martin Riedmiller. Embed to
control: A locally linear latent dynamics model for control from raw images. arXiv preprint
arXiv:1506.07365, 2015.
Max Welling and Yee W Teh. Bayesian learning via stochastic gradient langevin dynamics. In
Proceedings of the 28th international conference on machine learning (ICML-11), pp. 681-688.
Citeseer, 2011.
Yifan Wu, George Tucker, and Ofir Nachum. The laplacian in rl: Learning representations with
efficient approximations. arXiv preprint arXiv:1810.04586, 2018.
Lin Yang and Mengdi Wang. Reinforcement learning in feature space: Matrix bandit, kernels, and
regret bound. In International Conference on Machine Learning, pp. 10746-10756. PMLR, 2020.
Tianhe Yu, Garrett Thomas, Lantao Yu, Stefano Ermon, James Zou, Sergey Levine, Chelsea
Finn, and Tengyu Ma. Mopo: Model-based offline policy optimization. arXiv preprint
arXiv:2005.13239, 2020.
Anru Zhang and Mengdi Wang. Spectral state compression of markov processes. IEEE transactions
on information theory, 66(5):3202-3231, 2019.
Zihan Zhang, Xiangyang Ji, and Simon Du. Is reinforcement learning more difficult than bandits?
a near-optimal algorithm escaping the curse of horizon. In Conference on Learning Theory, pp.
4528-4531. PMLR, 2021.
13
Under review as a conference paper at ICLR 2022
A	Backgrounds on Reproducing Kernel Hilb ert Space
We briefly introduce the basic concepts of the Reproducing Kernel Hilbert Space, which is helpful
on understanding our paper. To start with, we first define the inner product (Stein & Shakarchi,
2009).
Definition 6 (Inner Product). Afunction〈•，)H : H×H → R is said to be an inner product on H
if it satisfies the following conditions:
1.	Positive Definiteness: ∀u ∈ H,〈u,u)≥ 0, and〈u, Ui = 0 ^⇒ U = 0.
2.	Symmetry: ∀u, v ∈ H, hu, vi ∈ hv, ui.
3.	Bilinearity: ∀α, β ∈ R, u, v, w ∈ H, hαu + βv, wi = αhu, wi + βhv, wi.
Additionally, we can define a norm with the innerproduct: kuk =，(u, Ui.
A Hilbert space is a space equipped with an inner product and satisfies an additional technical
condition of completeness. The finite-dimension vector space with the canonical inner product is an
example of the Hilbert space. We remark that H can also be a function space, for example, the space
contains all square integrable functions (i.e. R f (x)2 dx < ∞, generally denoted as L2) is also a
Hilbert space with inner product hf, gi = R f (x)g(x) dx.
We then define the kernel, and introduce the notion of positive-definite kernel (Alvarez et al., 2012).
Definition 7 ((Positive-Definite) Kernel). A function k : X → X → R is said to be a kernel on non-
empty set X if there exists a Hilbert space H and a feature map φ : X → H such that ∀x, x0 ∈ X,
we have
k(x, x0) = hφ(x), φ(x0)iH.
Moreover, the kernel is said to be positive definite if∀n ≥ 1, ∀{ai}i∈[n] ⊂ R and mutually distinct
set {xi}i∈[n] ⊂ X, we have that
ΣΣaiajk(xi, xj) > 0.
i∈[n] j∈[n]
Some well-known kernels include:
•	Linear Kernel: k(x, y) = hx, yi, with the canonical feature map φ(x) = x.
•	Polynomial Kernel: k(x, y) = (hx, yi + c)m, where m ∈ N+ and c ∈ R+.
•	Gaussian (a.k.aradial basis function, RBF) Kernel: k(x,y) = exp (kx-yk2). It,s known
that such kernel is positive definite.
Now we can define the Reproducing Kernel Hilbert space (RKHS) (Aronszajn, 1950).
Definition 8 (Reproducing Kernel Hilbert Space (RKHS)). The Hilbert space H of R-valued func-
tion defined on a non-emptry set X is said to be a reproducing kernel Hilbert space (RKHS) is there
is a kernel k : X × X → R, such that
1.	∀x ∈ X, k(x, ∙) ∈ H.
2.	∀x ∈ X, f ∈ H, hf, k(x, ∙))h = f (x) (a.k.a the reproducing property), which also implies
that (k(x, ∙), k(y, •))= k(x, y).
Here k is called a reproducing kernel of H.
We provide an intuitive interpretation on the definition of RKHS when H is the space of linear
function. Consider X = Rd and k(x, y) = hx, yi. With the definition of the kernel k, we can see
that k(χ, ∙) : X → R is a linear function, and thus lies in H. Meanwhile, ∀f ∈ H, there exists
θf such that f(x) = θf>x. We define the inner product on H via hf, giH = hθf, θgi, and thus
hf(,k(x,∙))iH = θ>x = f (x), which demonstrates the reproducing property, and shows that the
space of linear function on any finite-dimensional vector space is an RKHS with linear kernel as the
corresponding reproducing kernel.
14
Under review as a conference paper at ICLR 2022
We state the following theorems without the proof.
Theorem 9	(Moore-Aronszajn (Aronszajn, 1950)). Every positive definite kernel k is associated
with a unique RKHS H.
Notice that, Moore-Aronszajn theorem guarantees that all of the positive kernel can be represented as
the inner product in certain Hilbert space, hence we can have a linear representation of the Gaussian
distribution induced by the reproducing property of Gaussian kernel, as we illustrated in the main
text.
Theorem 10	(Bochner (Rudin, 2017)). A continuous, shift-invariant kernel (i.e. k(x, y) = k(x-y))
is positive definite if and only if k(x - y) is the Fourier transform of a non-negative measure ω, i.e.
k(x - y)
exp(iω> (x - y)) dP(ω) =
2 cos(ω>x + b) cos(ω>y + b) d(P(ω) × P(b)),
where P(b) is a uniform distribution on [0, 2π].
Bochner’s theorem shows that any continuous positive definite shift-invariant kernel (e.g. Gaussian
kernel, Laplacian kernel) can be represented as the inner product of random Fourier feature, which
provides an additional way to provide a representation for certain distribution (see Rahimi & Recht,
2007; Dai et al., 2014).
B An Equivalent Upper Confidence B ound Algorithm
In this section, we provide a generic Upper Confidence Bound (UCB) algorithm with the OFU prin-
ciple, and show the connections and differences between the UCB algorithm and the TS algorithm.
The prototype for our UCB algorithm is illustrated in Algorithm 3.
Algorithm 3 Upper Confidence Bound (UCB) Algorithm
Require: Number of Episodes K, Failure Probability δ ∈ (0, 1), Reward Function r(s, a).
1:	Initialize the history set Ho = 0.
2:	for episodes k = 1, 2,•一do
3:	Compute πk via	. Optimistic Planning.
π
(πk, fk) = arg max V0π(s0).
一：一
π∈Π,f∈Fk
where Fk is defined in (9).
4:	for steps h = 0,1,…，H 一 1 do
5:	Execute ah 〜∏h(sh).
6:	Observe sh+1.
7:	end for
8:	Set Hk =Hk-1∪{(skh,akh,skh+1)}hH=-0* 1 2.
9:	end for
. Execute πk .
. Update the History.
Notice that, the only difference between UCB algorithm and TS algorithm is the mechanism of
finding f we use to plan for each episode (highlighted in blue). For UCB algorithm, we perform an
optimistic planning, which finds the fk that potentially has the largest cumulative reward. However,
such constrained optimization problem is NP-hard even for the simplest linear bandits (Dani et al.,
2008). Instead, for TS algorithm, we only sample the fk from the posterior distribution, which gets
rid of the complicated constraint optimization. We are interested in the UCB algorithm, as the worst
case regret bound of the UCB algorithm can be directly translated to the expected regret bound of
the TS algorithm without the need of explicit manipulation of the prior and the posterior(Russo &
Van Roy, 2013; 2014; Osband & Van Roy, 2014).
Confidence Set Construction Perhaps the most important part in OFU-style algorithm is the con-
struction of confidence set Fk . To enable sample-efficient learning, the confidence set should
1. contain f * with high probability, so that We can identify f * eventually;
2. shrink as fast as possible, so that we can identify f * efficiently.
In the tabular setting, Fk is constructed via the concentration of sub-Gaussian/sub-Gamma random
variable (e.g. Azar et al., 2017; Jin et al., 2018; Zhang et al., 2021), and in the linear MDP set-
ting, Fk is constructed via the concentration on the linear parameters. As we don’t assume any
15
Under review as a conference paper at ICLR 2022
specific structures, we instead constructed Fk via the concentration on the `2 error, following the
idea of (Russo & Van Roy, 2013; Osband & Van Roy, 2014). Specifically, consider the least-square
estimates defined by
H-1
fκ = argminL2,κ(f) := X X kf(sh,ah) - sh+ιk2.	⑻
f∈F	k∈[K] h=0
As s∖+ι = f *(sh, ah) + eh where 用 is the GaUssian noise added to the step h at the k-th episode,
We know fκ Will not deviate from f * a lot. Meanwhile, as K increases, the estimation fκ should
become closer to f *. Specifically, define the empirical 2-norm ∣∣ ∙ ∣∣2,Et as
H-1
∣g∣22,EK := XX∣g(skh,akh)∣22.
k∈[κ] h=0
We can construct the confidence set based on the following lemma:
Lemma 11 (Confidence Set Construction (Russo & Van Roy, 2013; Osband & Van Roy, 2014)).
Define
Fκ = {f ∈F : kf - fκ∣2,Ek ≤ JβK(F,δ,α)} ,	(9)
then
Pf* (f * ∈ \ Fk) ≥ 1 - 2δ,	(10)
where
βκ(F, δ, α) = 8σ2 log(N(F, ɑ, ∣∣∙ ∣∣2)∕δ) + 2Ha(12C + √8dσ2 log(4K2H∕δ)).	(11)
The proof can be found in Appendix C.1. Notice that, the empirical 2-norm ∣∣f 一 fκ ∣∣2,Ek scales
linearly with K, and βκ* (F, δ, α) only scales as log K, so the confidence set shrinks. Meanwhile,
Equation 10 guarantees that f* ∈ Fk, ∀k with high probability. Hence, it satisfies our requirement
for the confidence set.
Regret Upper Bound We have the following upper bound of the regret for the UCB algorithm:
Theorem 12 (Regret Bound). Assume Assumption 2to5 holds. We have that
Regret(K) ≤ O(PH2T ∙ logN(F,TT/2, ∣ ∙ ∣2) ∙ dimE(F,TT/2)).
where O represents the order UP to logarithm factors.
C Technical Proof
C.1 Proof for Lemma 11
Proof. We first show the following concentration on the `2 error:
Lemma 13 (Concentration of `2 error (Russo & Van Roy, 2013; Osband & Van Roy, 2014; Wang
et al., 2020)). ∀δ > 0, f : S × A → R, we have
Pf* (L2,κ (f) ≥ L2,κ (f *) + 2 kf 一 f *∣2,Eκ 一 4σ2 Iog(I∕δ), ∀K ∈ N) ≥ 1 一 δ
PYoof Define the filtration Hk,h = {(Sh,αh)}i∈[k-i],h=0,…,H-1 ∪{(Sk,ak)}h=1,and the random
variable Zk,h adapted to the filtration Hk,h via:
Zk,h =kf*(sh,ah) - sh+ιk2 -kf(sh,ah) - sh+ιk2
=kf *(sh, ah) - sh+ιk2 -kf (sh, ah) - f *(sh, ah) + f *(sh, ah) 一 sh+ιk2
=- kf (sh, ah) - f*(sh, ah)k2 + 2hf (sh, ah) 一 f *(sh,, ah.), eh,i,
where eh. = sh+1 — f*(sh.,ah). Thus, E(ZhHk,h) = -∣f(sh,ah) - f*(sh., ah)∣2, and Zh +
∣f (Skh, akh) 一 f* (Skh, akh)∣22 is a martingale w.r.t Hk,h. Notice that we assume e is an isotropic
Gaussian noise with variance σ2 on each of the dimension, thus the conditional moment generating
function of Zkh + ∣f (Skh, akh) - f* (Skh, akh)∣22 satisfies:
Mk,h(λ)=logE[exp(λ(Zh + Ilf(Sh,aQ- f*(sh,ah)k2))∣Hk,h]
16
Under review as a conference paper at ICLR 2022
= logE[exp(h2λf (sh, ah) - f *&, a皆,eQ∖Hk,h]
≤2σ2λ2kf(sh)- f*(sh,αQk2∙
Applying Lemma 4 in (Russo & Van Roy, 2013), we have that, ∀x, λ ≥ 0,
(H-I	H-1	∖
XX
λZk,h ≤ X - λ(1 - 2λσ2) X X kf (sh, ah) - f *(sh αh)k2,	Vk ∈ N) ≤ 1 - exp(-x).
k∈[K] h=0	k∈[K] h=0
Take λ = 412，x = log 1∕δ, and notice that Pk∈[K] Ph=O Zk,h = L2,κ(f *) — L2,κ(f), We have
the desired result.	□
We construct an α-cover Fa in F with respect to ∣∣T∣2. With a standard union bound, we know that
condition on f *, with probability at least 1 一 δ, we have that
L2,K(fα) - L2,K(f *) ≥ 2kfa - f *k2,Eκ - 4σ2 log(∣Fa∣∕δ),	VK ∈ N, fa ∈ Fa.
Thus, we have that
L2,K(f) - L2,K(f*) ≥ 1 kf - f*k2,Eκ - 4σ2 log(∣Fa∣∕δ)
+ fmiFɑ { 2 kfa - f *k2,Eκ - 2 kf - f *k2,Eκ + L2,κ (f) - L2,κ (fa)}.
、--------------------------------------{z-----------------------------------}
Discretization Error
We then deal with the discretization error. Assume α ≤ 2C (or otherwise we only have a trivial
cover) and kfα(s, a) - f(s, a)k2 ≤ α, we have that
kf a(s,a) - f *(s,a)∣∣2 -kf (s,a) - f *(s,a)∣∣2
=kf a(s, a)k2 -kf (s, a)k2 + 2hf *(s, a),f (s, a) - f a(s, a))
≤ max {kf (s, a) + yk22 - kf(s,a)k22} + 2Cα
kyk2 ≤α
= max {2hf (s, a), yi + kyk22} +2Cα
kyk2 ≤α
≤4Cα + α2 ≤ 6Cα,
where the inequality is by Cauchy-Schwartz inequality and α ≤ 2C . Meanwhile,
ks0 - f (s, a)k22 - ks0-fα(s,a)k22
=2hs0,fα(s,a)-f(s,a)i+kf(s,a)k22 - kfα(s,a)k22
≤2(e, fa(s, a) - f (s, a)) + 2(f *(s, a),fa(s, a) - f (s, a)) + 2Cα + α2
≤2kk2α + 6Cα.
We now consider the concentration property of kek2. Here we simply follow (Jin et al., 2019) and
notice that e is √dσ-norm-sub-Gaussian, we have that
P(kek2 > √2dσ2 log(2∕δ)) ≤ δ.
By a union bound, we have that
∞ H-1
P(∃k, ke∣∣2 > √2dσ2 log(4k2H∕δ)) ≤2 xx 击 ≤ δ.
k=1 h=0
Sum all these up, we can see with probability 1 - δ, VK ∈ N, the discretization error is upper
bounded by:
Ha(12C + √8dσ2 log(4K2H∕δ)).
As we consider the least square estimate fκ, we have that L2,κ(fκ) - L2,κ(f *) ≤ 0. Substitute
back, we have the desired results.	□
C.2 Simulation Lemma
Lemma 14 (Simulation Lemma (adapted from Lemma 3.9 in (Kakade et al., 2020))). Given f,
∖ / 一 C ,1	1 r . ∙ CTr 1 τ τrττ	ι ■	,,1	ι ι c ι r ⅛	, ∙ 「
Vs ∈ S, the value function V and V Corresponding to the model f and f * satisfies
Vn(s) - Vn (s) ≤ H3/2 t E
H-1
min
h=0
2kf*(sh,ah) - f(sh,ah)k2
σ2
1
17
Under review as a conference paper at ICLR 2022
Proof. We first show the following difference lemma:
Lemma 15 (Difference Lemma). Assume the trajectory {(sh, ah)}hH=-01 is generated via policy π
and ground truth f *, define
H-1
Vh =	r(sτ, aτ)
τ=h
then ∀τ ∈ {1,…，H 一 1}, we have:
V^π(so) - V0 =E 0 S^	、ShVn(s0)i 一 Vτ
0 ∖ 0，	0	sT 〜N (f (sτ-ι ,aτ-ι),σ2I)	τ τ τ	T
τ-1
+ X IEsh 〜N (f(sh-ι ,ah-ι),σ21) M(sh)] - Vn(sh)].
h=1
Proof. When τ = 1, we can obtain the result with a0 = π(s0) and
n	n0
V0 (SO)= r(S0,π(SO))+ EsI〜N(f(s0,α0),σ2I)vi (SI).
We only need to show the case when τ = 2, and the case when τ > 2 can be derived via recursion.
Notice that
Vn (SO)- v0 =EsI 〜N (f(so ,ao),σ2I) [VT(SI)] 一 VI
=Vn(SI)- V1 + Esl 〜N (f(so,ao),σ21)[*⑸)]一 Vn (s1)
=Es2 〜N (f(sι ,aι),σ2I) hVn(S2)] - V2 + EsI 〜N (f (so,ao),σ2I) [VIn (S1)] - VIn (S1),
where the last equality is due to the fact that aι = ∏(si).	□
We then follow the idea of “optional stopping” used in (Kakade et al., 2020) and show the following
“optional stopping” simulation lemma.
Lemma 16 (“Optional Stopping” Simulation Lemma). Consider the stochastic process over the
trajectories {(Sh, ah)}hH=-O1 generated via policy π and ground truth f*, where the randomness is
from the Gaussian noise in the dynamics. Define a stopping time τ w.r.t this stochastic process and
a given model f via:
T := min{h ≥ 0 : Vh(Sh) ≤ Vh(sh)}.
Furthermore, define a random variable:
Vn(Sh) = max{VT(Sh), V(Sh)},
we have that
H-1
VVn (so) - S (so) ≤ E X 1h<τ (Esh+ι 〜N (f*(sh …I )C (Sh+ι) - Esh+1 〜N fh ,ah ),σ2 I)Vn (Sh+1)
h=O
where the expectation is w.r.t the stochastic process over the trajectories.
Proof. Define the filtration Fh := {i}ih=-O1, where i is the noise that add to the dynamics at step i.
Define
,ʌ _ . . 一
Mh = EV (so) — V0∣Fh],
which is a Doob martingale with respect to Fi (Grimmett & Stirzaker, 2020). As τ ≤ H, by Doob’s
optional stopping theorem, we have that
,ʌ _. 一 一 一 , ʌ _. . 一一
E[Vn (so) — V0] = E[Mτ] = E[E[Vn (so) — V0∣Fτ ]].
We then provide a bound for Mτ . By Lemma 15, we have that
Mτ =E[Vn (so) — V0∣Fτ ]
=EsT 〜N (f(sτ-ι,aτ-l),σ2I) hVn (Sτ )] - Vn (Sτ )
τ-1
+ Esh〜N(/(sh-ι,ah-ι),σ2I) [Vn(Sh)] - X Vn(Sh)
h=1
τ
=X (Esh 〜N (/(sh,ah),σ2) M (Sh)] 一 Vn (Sh))
h=1
18
Under review as a conference paper at ICLR 2022
τ
≤ X (Esh 〜N (f^(sh,ah),σ2I) [VΠ (Sh)] - V (Sh))
h=1
H
=X 1h≤τ (Esh〜N(f(sh,ah),σ2Γ) [Vhπ(sh)] - Vn(Sh))，
h=1
where the third inequality follows the definition of τ (and thus Vτπ (Sτ) = Vτπ (Sτ) and Vhπ (Sh) =
Vhr (Sh) for h < T.)
The proof is then concluded via the following observation:
E h1h≤τ (Esh 〜N (Ksh ,ah),σ2I) hVhr(sh)i - Vn(Sh))]
=E E 1h≤τ (Esh〜N(/(sh,ah),σ2I) hVn(Sh)i - Vn(Sh))[h-]
=E E 1h-1<τ (Esh〜N(/(sh,ah),σ2I) hVn(Sh)] - Vn(Sh))[h-1]]
=E 1h-1<τE ](Esh〜N(/(sh,ah),σ2I) hVn(Sh)] - Vn(Sh))卜h-1]]
=E [1h-1<τ (Esh 〜N (/(sh,ah),σ2) [Vn (Sh)] — Esh 〜N (f*(sh,ah),σ2) [Vn(Sh))]],
where the third equality is due to the fact that 1h-ι<τ is measurable under Fh-ι.	□
Before we finally provide the proof of Lemma 14, we state the following lemma that bound the
expectation under two isotropic Gaussian distribution with different mean:
Lemma 17 (Difference of Expectation under Different Mean Isotropic Gaussian). ∀ (approximately
measurable) positive function g, we have that
Ez〜N(μι,σ2I)[g(z)] — Ez〜N(μ2,σ2I)[g(z)] ≤ min
,1 } JEz~N(μι ,σ2I)[g(Z)F
√2kμ1 — μ2k
σ
Proof.
Ez~N(μι ,σ2I)[g(Z)] — Ez~N(μ2,σ2I)[g(Z)]
=Ez~N(μι ,σ2I)
g(z) 1 - exp
2(μ1 — μ2)>z + kμ2k2 -kμ1k2
2σ2
≤jEz〜N("I)[g(ζ)2]jEz〜N(μι,σ2i) (1 — exp (2(〃2 —22Jμ2k2 + kμ1止))2
We then calculate
Ez〜N(μι,σ21) I 1 - exp
2
=1-----7-----
√2πσd/'
1
+ √2πσd/2
1
2 exp
exp
2(μ 一 μι)>z - kμ2k2 + kμι∣l2
ʌ	2σ2	))
-kz — μ1k2 + 2(μ2 — μl)>z — kμ2k2 + kμ1k2
2σ2
— kz — μιk2 + 4(μ2 — μι)>z — 2kμ2k2 + 2kμ1k2
dz
— 1 +	--
√2πσd/2
exp
2σ2
-kz — (2μ2 — μ1)k2 + 2kμ2 — μ1k2
2σ2
dz
dz
=- 1 + exp ( kμ2 - μ1k2 )
—— ɪ + exp [	. I -
σ2
Also notice that, as g is positive, a simple bound is that
Ez~N(μι ,σ2I)[g(z)] - Ez~N(μ2,σ2I)[g(z)] ≤ Ez~N(μι ,σ2I)[g(z)] ≤ JEz~N(μι ,σ2I) [g(z)?] ∙
Thus,
Ez~N(μι ,σ2I) [g(z)] - Ez~N(μ2,σ2I) [g(z)] ≤ JEz~N(μι ,σ2I)[g(Z)F jmin {exp (
kμ2 — μ1k2
σ2
-1,1 .
19
Under review as a conference paper at ICLR 2022
Notice that, if ∣∣μ2 - μιk ≥ σ, then exp (kμ2σμιk2) - 1 ≥ 1. Meanwhile, When X ∈ [0,1],
exp(x) ≤ 1 + 2x. Thus,
"in 卜XP (⅛μl) - 1,l} ≤ Smm {l + 2⅛μ1-Tiy = min {2⅛<, l}
which finishes the proof.	□
With Lemma 16, we have that
Vn (S0)- Vπ (S0)
≤E h1h-1<τ Es0h〜N (f(sh,ah),σ2) V (Sh)] - Esh 〜N (f*(sh,ah),σ2) [Vh (Sh))ii
≤ X E F+mnV * * * *^l min (√2k∙πsh,ahσ-& h,ah)2k」)#
≤ X r [Esh…(―鹏(sh+ι)2ii t E "min ( 2f *'ah'ah'2",1)]
≤t E ^X Esh+1~P(If*(Sh,Oh)) [Vπ(Sh+ι)2i] tE "X min ( 2f *(Sh, ah)σ- f(sh, ah)2 k H
h=0	h=0
≤H3/2 t E X min ( 2f*(sh,ah：- f(sh,ah)2k, 1)#
where the second inequality is due to Lemma 17, and the last inequality is due to the fact that
~ .
VΠ(sh+ι) ≤ H，Vh.	□
C.3 Sum of Width S quare
Lemma 18 (Bound on the Sum of Width Square). Define
WF(s,a) := sup k∕(s, a) - f (s,a)k2.
f,f ∈F	—
If {βk*}k∈[K] is a non-decreasing sequence, and kfk2 < C, ∀f ∈ F, then:
H-1
X XwF2t(Skh,akh ) ≤ 1+4C2HdimE F,T-1/2 + 4βKdimE F,T-1/2 (1+logT)
k∈[K] h=0
Proof. We first show the following lemma, which will be helpful in our proof.
Lemma 19 (Lemma 1 in (Osband & Van Roy, 2014)). If {βk}k∈[K] is a non-decreasing sequence,
we have
X X IwFk(sh,ah)> ≤ (βKΓ + H) dimE(F,e).
k∈[K ] h=0
Proof. We first consider when wFk (Skh , akh ) > e and is e-dependent on n disjoint sub-sequences of
{(sh, ah)}i∈[k-1]. By the definition of e-dependent, we know ∣∣f - f k2,Ek > ne2. On the other
hand, by triangle inequality, we know ∣∣∕ - f ∣∣2,Ek ≤ 2√βk ≤ 2√βK, thus n < 4βKK. Hence we
know when WFk (sh, ah) > e, then (sh, ah) is at most e-dependent on 4∣K disjoint sub-sequences
of {(Sih ,aih )}i∈[k-1].
We then show that, for any sequence {(si, ai)}i∈[N], there is some element (sj, aj) that is e-
dependent on at least dim/Fe) - H disjoint sub-sequences of {(si, ai)}i∈j-i]. Let n satisfies
that ndimE (F, e) + 1 ≤ N ≤ (n + 1)dimE (F, e), and we will construct n disjoint sub-sequences
20
Under review as a conference paper at ICLR 2022
{Bi}i∈[n]. We first let Bi = {(si, ai)}, ∀i ∈ [n]. If (sk+1, ak+1) is -dependent on each Bi, i ∈ [n],
we have the desired results. Otherwise, we append (sk+1, ak+1) to the sub-sequence that it is -
independent with. Repeat this process until some j > n + 1 is -dependent on each sub-sequence
or we have reached N. In the latter case we have Pi∈[n] |Bi| ≥ ndimE (F, ) (here we can add
at most H - 1 data to avoid the case we need a new episode of data), and since each element of
a sub-sequence is -independent with its predecessors, |Bi| ≤ dimE (F, ), ∀i by the definition of
eluder dimension. Thus |Bi| = dimE (F, ), ∀i. And in this case, (sN, aN) must be -dependent on
each sub-sequence by the definition of eluder dimension. Notice that, as our data is collected in an
episodic pattern, there are at most H - 1 sub-sequences that contains ”imaginary” final episode data
introduced to the construction. In this case, We know that there are at least dim；(Fe) - H disjoint
sub-sequences that (sN, aN) is -dependent, which finishes our claim.
We finally consider the sub-sequence B = {(skh, akh)} with wFk (skh, akh) > . We know that each
element in B is e-dependent on at most 4βK disjoint sub-sequence of B, but at least e-dependent on
dim∣BF e) -H SUb-SeqUenCeOf B. Thus we know |B| ≤ (4∣K + H) dimE(F, e), which concludes
the proof.	□
For notation simplicity, we define wt,h := wFt (sth, ath).	We first reorder the sequence
{wt,h}k∈[κ],0≤h≤H-1 → {wi}i∈[KH], such that wι ≥ …WTH. Then we have
H-1
wF2t(skh,akh)=	wi2≤	wi21wi<T-1/2 +
w21Wi≥T-1/2 ≤ 1 + Σ wi2 1wi ≥T -1/2.
k∈[K] h=0	i∈[KH]	i∈[KH]	i∈[KH]	i∈[KH]
As we order the sequence, wj ≥ e means
H-1
Hence we know
k∈[K] h=0
1wFt(skh,akh)>e≥j.
e ≤1 /	邻 K	=
V dimE(F,e) - H
which means if wi ≥ T -1/2 , then wi < min 2C,

4βκdimE (F, e)
一HdimE (F, e)
∕4βKdimE(F,T-1/2) O H
V k-HdimE(F,T-1/2) ʃ. HenCe,
X 21	≤4C 2 H di	( F T-1/2、+	X	4βK dimE(F，T-1/2)
i∈⅛ i wi≥T-1/2 ≤	Ej ) + j = HdimEL-1/2) + 1 j-H dimE (F，T T2)
≤4C2HdimE F,T-1/2 +4βKdimE F,T-1/2 (1+logT),
which finishes the proof.
□
C.4 Proof for Theorem 5 and Theorem 12
Proof. Define Ek = Pf* (f * ∈ Fk). When constructing the confidence set, take α = T -1/2 and
δ = 0.25 in Lemma 11, which leads to
βk =8σ2 log(4N (F ,T-1/2, ∣∣∙∣∣2)) + HT-1/2(12C + √8dσ2 log(16k2H)).
With our confidence set construction, we know that Pk∈[κ] P(Ek) ≤ 0.5. Notice that
Regret(K) = X V0*(s0k)-V0πk(s0k)
k∈[K]
≤E X E [P(Ek)[V*(sk) - V∏k(Sk)]] + H X P(Ek)
k∈[K]	k∈[K]
≤E X E [V∏k (Sk) - V∏k (Sk)i +0.5H
k∈[K]
21
Under review as a conference paper at ICLR 2022
(
≤H3/2 E t
k∈K
≤uH2TE
t
H-1
h=0
H-1
X X min
k∈[K] h=0
2fk(Shah)- f*(sh,αh)k2
σ2
2fk(sh,αh)- f*(sh,αh)k2
σ2
+ 0.5H
+ 0.5H
E
1
2H2T
≤V k
(1 + 4C2HdimE (F, TT/2) + 4βKdimE (F, TT/2) (1+log T)) + 0.5H,
where the first equality is due to the fact that the total reward for each episode is bounded in [0, H],
the second inequality is due to the optimism and our confidence set construction, the third inequal-
ity is due to Lemma 14, the fourth inequality is due to Cauchy-Schwartz inequality and the final
inequality is due to Lemma 18 , which concludes the proof of Theorem 12. Following the idea of
(Russo & Van Roy, 2013; 2014; Osband & Van Roy, 2014), we can translate the worst-case regret
bound for UCB algorithm into the expected regret bound for TS algorithm, that conclude the proof
of Theorem 5.	□
Remark It can be undesirable that our regret bound scale with σ-1, which means our algorithm
can perform pretty bad when the noise level is extremely low. Itis also more or less counter-intuitive.
We want to remark that, such phenomenon is only an artifact introduced by our proof strategy. The
simulation lemma (Lemma 14) works well whenf(s, a) -f(s, a) is small. However, we need to
tolerate some bad episodes to collect sufficient samples, that can eventually make the error small.
Fortunately, the regret of such bad episode is at most H. Hence, we can use the following strategy
to get rid of the dependency on σ-1.
Definition 20 (Bad and Good Episodes). Define episode k asa bad episode, if ∃h ∈ {0,1,…，H -
1}, SUCh that wkh := WFk (Sh, ah) is the largest HdimE(F, σ2T1/2) elements in the set
{wk,h}k∈[K],0≤h≤H-1. Define episode k as a good episode, ifit is not a bad episode.
By the definition, We know there are at most HdimE(F, σ2T-1/2) bad episodes. We then show
the following lemma, that can be directly generalized from Lemma 18, by setting = σ2T -1/2 and
remove the terms from bad episodes.
Lemma 21. If {βk}k∈[κ] is a non-decreasing sequence, and kf∣∣2 < C, ∀f ∈ F, then:
H-1
X	X WFt (sh,ah) ≤ σ2 +4βκdimE (F, σ2TT/2) (1 + log T)
k∈[K],k is good h=0
Eventually, we can obtain the following regret bound, by setting the regret of bad episodes as H,
and bounding the regret of good episodes with Lemma 21.
Theorem 22 (Improved Regret Bound). Assume Assumption 2 to 5 holds. Take α = σ2T-1/2 and
δ = 0.25 in Lemma 11, which leads to	_______________
βk := 8σ2 log(4N (F ,σ2T-1/2, k∙k2)) + Hσ2T T/2(12C + /8dσ2 log(16k2H)).
We have that ____________________________________________
Regret(K) ≤ JH2T(攀 + 1)dimE(F, σ2TT/2)(1 + logT) + 0.5H + H2dimE(F, σ2TT/2)
We would like to remark, that the definition of bad and good episodes is only used for the proof. We
don,t need to make any modification on the algorithm. Notice that, as βk a σ2, our upper bound
in 22 can only scale with σ-1 through the logarithm covering number log(4N (F, σ2T -1/2 and
eluder dimension dimE (F, σ2T -1/2 ). When F is a linear function class, both term should scale
with polylog(σ), that matches the result from (Kakade et al., 2020).
D Bounds on the Complexity Term under Linear Realizab ility
We provide the upper bound on the covering number and the eluder dimension of F when F :=
{θ>夕:θ ∈ Rd^×d, ∣∣θ∣∣2 ≤ W} where φ : S × A → Rd^ is some known feature map. We first
make the following standard assumption:
22
Under review as a conference paper at ICLR 2022
Assumption 6 (Bounded Feature).
k2(s, a)∣∣2 ≤ B,∀(s, a) ∈ S ×A.
D.1 Covering Number
Theorem 23	(Covering Number Bound). We have that
2BW ∖dφ
N (F ,e,k∙k2) ≤ {++-)
Proof. Notice that, by Cauchy-Schwartz inequality, we have that
max	∣∣ε>P(s,a)∣∣2 ≤ 右|吗|12,	'ε ∈ Rd^.
(s,a)∈S×A
Thus, denote ε = [εi]i∈[d] , we have that
.max ztkε>3(s,a)k2 = , max ZtX kε>ψ(s,a)k2 ≤ B2 X I∣εik2 = B2kεk2.
(s,a)∈S×A	(s,a)∈S×Ai∈[d]	i∈[d]
Hence, to find an e-cover for F, WejUstneed to find an e/B-cover of {θ : θ ∈ Rd^×d, ∣θ∣2 ≤ W}.
By standard argument on the covering number of Euclidean space (e.g. Lemma 5.7 in (Wainwright,
2019)), we can conclude the desired result.	□
D.2 Eluder Dimension
Theorem 24	(Eluder Dimension Bound). We have that
dimE(F，e)≤ "log 卜 +
12W 2B2 ʌ
+1.
Proof. Our proof follows the idea in (Russo & Van Roy, 2013). Define
Wk := Sup 卜θι - θ2)τ^(s, a): S X ((θι- θ2)>夕i(si,ai))2 ≤ J。1,。2 ∈ RdQd, k&k ≤ W, |四|| ≤ W
[	V i∈[k-1]
For notation simplicity, define Wk :=夕(si, a) θ := θι-θ2, and Φk := Pi∈[k-i] ψiψ>. Obviously,
we have that IθI ≤ 2W . Moreover, by straightforward calculation, we know
X ((θι - θ2)τWi(si, ai))2 = Trace(θτWkθ)∙
i∈[k-1]
(0)2
Define Vk := Φk + (4W21, we start from considering the problem
max Trace(θτWkWkτθ), subject to Trace(θτVkθ) ≤ 2e2.
θ
The Lagrangian can be formed as
L(θ, γ) = -Trace(θτWkWkτθ) + λ(Trace(θτVkθ) - 2e2), λ ≥ 0.
The optimality condition of θ is
(λVk - WkWkτ)θ = 0.
As Vk is of full rank, λVk - Wk WT has rank at least dψ — 1 (as Wk WT is of rank 1). So the equation
(λVk - WkwT)θi = 0, θi ∈ Rdy	______
only has one non-zero solution. Substitute back, we know that (define ∣∣x∣A := √xτAx):
sup{Trace(θTWkWTθ): Trace(θτVkθ) ≤ e2} = √2e0∣Wkkv-
1.
With the conclusion above, we have that
Wk ≤ sup{θTWk : Trace(θτΦkθ) ≤ e2, ∣θ∣ ≤ 2W} ≤ sup{θτWk : Trace(θ>Vkθ) ≤ 2e2} = √2e0∣WkIlV-
k
Hence, if Wk ≥ e0, then Wk Vk-1 Wk ≥ 0.5. Moreover, with Matrix Determinant Lemma, if Wi ≥
∀i < k, we have
det(Vk) = det(Vk-1)(1 + WkτVk-1Wk) ≥ det(Vk-1)
Meanwhile,
1.
e0,
Trace(Vk) d
det(Vk) ≤ —d(^	≤
(e0)2d (3「1
4W2d 2 )
23
Under review as a conference paper at ICLR 2022
Hence, we know
(3 YkT)/d	4W2 B2
⑸	≤	(C2
k - 1
d
+ 1.
Now we only need to find the largest k that can make this inequality hold. For notation simplicity,
define a := 4,2)，, n = k-1. As log(1 + x) ≥ ι+xχ and log X ≤ x/e, We have
nn
-≤ nlog 3/2 ≤ log(α + 1) + logn ≤ log(α + 1) + log3 + log(n∕3) ≤ log(α + 1) +log3+ —.
3	3e
Substitute back, we can obtain the desired result.	□
24
Under review as a conference paper at ICLR 2022
E	Experimental Details
E.1 Algorithm Summary
Our algorithm is easily built on SAC. The only difference we make is we decouple the critic network
into a representation network φ(∙) and a linear layer l(∙) on top of the representation. The represen-
tation network is governed by the model dynamics loss in SPEDE, and we train a linear layer to
predict the Q-value as it lies in the linear space of the representation guaranteed by our analysis. We
update the representation by a momentum factor and keep the policy update the same procedure as
SAC.
E.2 Full Experiments
Table 2: Performance of SPEDE on various MuJoCo control suite tasks. Our method achieve
strong performance even comparing to pure empirical baselines. To be specific, in hard tasks like
HUmanoid-ET and Ant-ET, SPEDE outperforms the baselines significantly. Results with * are di-
rectly adopted from MBBL (Wang et al., 2019). We also provide the SoTA model-free RL method
SAC as a reference.
	Swimmer	Ant-ET	Hopper-ET	Pendulum
ME-TRPO*	30.1±9.7	42.6±21.1	4.9±4.0	177.3±1.9
PETS-RS*	42.1±20.2	130.0±148.1	205.8±36.5	167.9±35.8
PETS-CEM*	22.1±25.2	81.6±145.8	129.3±36.0	167.4±53.0
DeepSF	25.5±13.5	768.1±44.1	548.9±253.3	168.6±5.1
SPEDE	42.6±4.2	806.2±60.2	732.2±263.9	169.5±0.6
SAC*	41.2±4.6	2012.7±571.3	1815.5±655.1	168.2±9.5
	Reacher	Cartpole	I-pendulum	Walker-ET
ME-TRPO*	-13.4±5.2	160.1±69.1	-126.2±86.6	-9.5±4.6
PETS-RS*	-40.1±6.9	195.0±28.0	-12.1±25.1	-0.8±3.2
PETS-CEM*	-12.3±5.2	199.5±3.0	-20.5±28.9	-2.5±6.8
DeepSF	-16.8±3.6	194.5±5.8	-0.2±0.3	165.6±127.9
SPEDE	-7.2±1.1	138.2±39.5	0.0±0.0	501.58±204.0
SAC*	-6.4±0.5	199.4±0.4	-0.2±0.1	2216.4±678.7
	MountainCar	Acrobot	SlimHumanoid-ET	Humanoid-ET
ME-TRPO*	-42.5±26.6	68.1±6.7	76.1±8.8	776.8±62.9
PETS-RS*	-78.5±2.1	-71.5±44.6	320.7±182.2	106.9±102.6
PETS-CEM*	-57.9±3.6	12.5±29.0	355.1±157.1	110.8±91.0
DeepSF	-17.0±23.4	-74.4±3.2	533.8±154.9	241.1±116.6
SPEDE	50.3±1.1	-69.0±3.3	986.4±154.7	886.9±95.2
SAC*	52.6±0.6	-52.9±2.0	843.6±313.1	1794.4±458.3
E.3 Ablations
Our ablation experiments are trying to study an important design choice of the practical algorithm:
the momentum used to update the critic function. We summarize the results in Table 3. We can see
that using a small large momentum factor such as 0.999 shows better performance. This is intuitively
understandable: large momentum factor slows down the update speed of the representation of the
critic function and thus stabilize the training. Such phenomenon illustrates the importance of slowly
update the representation.
Table 3: Ablation Suty of SPEDE on MuJoCo tasks. We see that a small momentum factor help
stabilize the performance, especially in environments like Huamoid and Hopper-ET.
	Hopper-ET	Ant-ET	S-Humanoid-ET	Humanoid-ET
SPEDE-0.9	593.2±37.4	877.7±45.9	881.6±385.2	232.9±63.4
SPEDE-0.99	305.9±13.4	707.9±51.1	629.3±106.9	818.1±130.6
SPEDE-0.999	732.2±263.9	806.2±60.2	986.4±154.7	886.9±95.2
25
Under review as a conference paper at ICLR 2022
E.4 Comparison to LC3
We provide a comparison of empirical results with LC3 (Kakade et al., 2020), which is also an
algorithm with rigorous theoretical guarantees. Despite the major difference that we are learning the
representation while LC3 assumes a given feature, the performance of SPEDE is much better than
LC3 in tasks like Mountain Car and Hopper.
Table 4: Comparison of SPEDE with LC3 on MuJoCo tasks. LC3 only achieves good performance
on relatively easy tasks like Reacher. However, their performance on Hopper and Mountain-Car is
much Worse than SPEDE._____________________________________________________
	Reacher	MountainCar	Hopper
SPEDE	-7.2±1.1	50.3±1.1	732.2±263.9
LC3	-4.1±1.6	27.3±8.1	-1016.5±607.4
E.5 Performance Curves
We provide an additional performance curve including ME-TRPO in Figure 2 for a reference.
——SPEDE ——DeepSF-SAC ——PETS-CEM ——PETS-RS ——ME-TRPO
Figure 2: Experiments on MuJoCo: We show curves of the return versus the training steps for SPEDE and
model-based RL baselines. We also include the final performance of ME-TRPO from (Wang et al., 2019) for
reference.
E.6 Hyperparameters
We conclude the hyperparameter we use in our experiments in the following.
Table 5: Hyperparameters used for SPEDE in all the environments in MuJoCo.
Hyperparameter Value
Actor lr	0.0003
Model lr	0.0001
Actor Network Size	(1024,1024,1024)
Fourier Feature Size	1024
Discount	0.99
Target Update Tau	0.005
Model Update Tau	0.001
Batch Size	256
26