Under review as a conference paper at ICLR 2022
Intervention Adversarial Auto-encoder
Anonymous authors
Paper under double-blind review
Ab stract
In this paper we propose anew method to stabilize the training process of the latent
variables of adversarial auto-encoders, which we name Intervention Adversarial
auto-encoder (IVAAE). The main idea is to introduce a sequence of distributions
that bridge the distribution of the learned latent variable and its prior distribution.
We theoretically and heuristically demonstrate that such bridge-like distributions,
realized by a multi-output discriminator, have an effect on guiding the initial la-
tent distribution towards the target one and hence stabilizing the training process.
Several different types of the bridge distributions are proposed. We also apply
a novel use of Stein variational gradient descent (SVGD) (Liu & Wang, 2016),
by which point assemble develops in a smooth and gradual fashion. We conduct
experiments on multiple real-world datasets. It shows that IVAAE enjoys a more
stable training process and achieves a better generating performance compared to
the vanilla Adversarial auto-encoder (AAE) (Makhzani & Shlens, 2015).
1	Introduction
Generative models are widely used for image and texture production. Among them there are two
base models which are most appealing to scholars for their elegant theoretical foundation and close
combination with neural network, namely Variational Auto-encoders (VAE)(Kingma & Welling,
2013) and Generative adversarial network (GAN)(Goodfellow et al., 2014). VAE maximizes a
lower bound of the log-likelihood called ELBO which decomposes to a reconstruction loss term
and a regularization term within an auto-encoder structure. GAN, however, goes beyond the like-
lihood concern, trying to generate data directly using a learned map from the latent space that is
trained in an adversarial manner. Both models have some weakness. For example, GAN’s training
is quite unstable and sometimes may have mode collapse problem. VAE usually produces less sharp
pictures. The regularity of the latent space is also an important concern for VAE that affects the
quality of the generated images.
There’re many attempts to alleviate the above drawbacks. Adversarial Auto-encoders (AAE) com-
bines the techniques of both VAE and GAN network. It imposes a discriminator on the latent
space of an auto-encoder to classify the latent distribution p(z) and q(z), where the adversar-
ial term serves as the regularizer. Such choice improves the regularization effect of the latent
variable and alleviates the mode collapse problem due to an auto-encoder structure. Wasserstein
Auto-Encoders(WAE)(Tolstikhin et al., 2017) generalize different regularization approaches on la-
tent space, proposing adversarial method (WAE-GAN) and kernel based method (WAE-MMD). It
provides AAE with theoretical support as well. Intervention Generative Adversarial Network (IV-
GAN)(Liang et al., 2020), another attempt to combine GAN with the encoder structure, stabilizes
GAN’s training process by intervening on the latent distribution so that the reconstructed data dis-
tribution could approach the target more robustly.
In this work, we will propose this novel regularization method on AAE models. As basically an auto-
encoder structure, the key question is how to let the original distribution in latent space approach
the prior distribution. We intervened on the encoded data, which inherently brings intervened dis-
tribution. We will show that a loss term in our model can be transformed to a certain measure of
the overlap among multiple distributions. When minimizing the loss in terms of these constructed
variables, we expect not only the original distribution to approach the target one, but the distribution
of the intervened data (we call bridge distribution) to serve as a guidance to have such approaching
process more fast and robust as well.
1
Under review as a conference paper at ICLR 2022
2	Methodology
Training a GAN model has always been a challenge (e.g., the process is known to be unstable and
prone to mode collapse). AAE’s auto-encoder structure alleviates the problem with a reconstruc-
tion term. It first updates the auto-encoder upon the reconstruction loss, then use the discriminator
to distinguish the encoded latent variables and generated ones following Gaussian distribution, up-
dating the discriminator and encoder in an adversarial manner. However, such architecture doesn’t
completely solve GAN’s problem. Decreasing the JS divergence has a relatively low approaching
efficiency between two distributions. Adversarial training on latent space doesn’t avoid this problem
at all. Therefore there’s a desire to apply a more powerful regularization term to have the approach-
ing process more efficiently and stable. We consider to construct a series of “bridge distributions”
by certain transformations so that they in a sense lie between the encoded distribution and target one.
We let the discriminator to classify these multiple distributions, and update the encoder to let them
approach each other. Intuitively such bridge distributions would serve as a guidance to help the la-
tent variables follow the target distribution more quickly and robustly, but we need more theoretical
arguments first.
Before we expound on further details, we want to turn our attention to how to construct the
transformations mentioned above. We first explain the meaning of intervention. It is similar to what
is exhibited in IVGAN(Liang et al., 2020). For completeness we show it here in our context.
Definition 1 Let X be the set of all random variables in Rd on the probability space (Ω, X, P).
T is the set of all mappings from Rd to Rd. For a distribution function with support in Rd, we
call T ∈ T “P-intervention” if X 〜P ⇒ T(X)〜P, for all X ∈ X. If S ∈ T satisfies that
T(X)〜P, ∀T ∈ S ⇒ X 〜P, we denote S “Complete Intervention Group”.
A rough understanding of the above definition is that complete intervention group let the meet of
subsequent multiple distributions embed their final distributing pattern, i.e. P(x). Combining the
idea in our model, the intervention can be performed on our encoded data. We hope when these
originally different distributions approach each other, they all approach the prior distribution, which
is exactly the feature of the transformation proposed above. Therefore the complete intervention
group can serve to ensure the identifiability of our method.
To be more concrete, we set the P to be standard Gaussian and propose several intervention patterns
that possess the above feature.
A simplest example of P-intervention is to replace some of the dimension with normal distributed
one. Therefore we have our Blockwise substitution: Rd space is divided equally by t parts, where
t|d. , For 0 ≤ k ≤ t, let Tk be the transformation that substitute the first k blocks with random
variables following standard normal distribution independent with the original variable. Then we
get a complete intervention group {Tk}.
There’s other ways to create intervention groups. For example, we let x be replaced by xcos(空)+
zsin(k∏), for k = 0,1,..., t, where Z follows standard normal distribution independent with x. We
call such construction pattern Radial substitution.
Except for the two substitution method, we also apply Stein variational gradient descent (SVGD)
(Liu & Wang, 2016) to construct the bridge distribution by iteratively updating points. By setting
the target distribution P the updated particles will approximately approach P. More details will be
specified in the next section.
Now we propose our model called Intervention Adversarial Auto-Encoder (IVAAE). [Figure 1]
shows its specific structure, which contains an encoder E, decoder G and a discriminator D .
Images x is fed into the E to get the encoded latent code z, which is decoded by G to obtain the
reconstructed images x. The encoded z is transformed by a complete intervention group Tk (T0
represent the identical mapping) to get zk which is discriminated by D with output of size t + 1.
Through maximizing and minimizing alternatively a cross entropy loss, D and E is both updated.
Intervention Loss
2
Under review as a conference paper at ICLR 2022
x~pdata
Intervention
----A Intervention loss
χ'—
_______A Reconstruction
Loss

Figure 1: The structure of our model
The key distinction of our model from AAE is the intervention loss. More specifically, when encoded
latent z = E(x) is obtained, for every k, 0 ≤ k ≤ t, we intervene z by Tk to get zk . Then we feed
zk into the discriminator to obtain a t + 1 size output d, and compute the cross entropy between d
and labels ek, averaging on k, finally to update discriminator by gradient flow. The encoder update
in the same manner.
Therefore the intervention loss is an unbiased estimator of its theoretical form as follow:
LIV (E,D) = -Ek 〜u[t]Eχ 〜pdata[log(Dk (Tk (E(X))))]
=-Ek 〜u[“ 〜pk [log(Dk (z))]	(1)
where Dk, pk represent the (k + 1)th digit of D output, the distribution of zk = Tk(E(x)), respec-
tively.
Then we give theoretical results that evidence the rationality of our proposed models.
Theorem 2 Given t + 1 latent variables {zk}tk=0 with density function support in Rd, and their
probabilistic density function {pk}tk=0, the classifier D is trained to minimize the cross entropy loss
as equation 1 under the constraint that it=0 Dj(z) = 1. Then the optimal classifier satisfies:
Di(Z) = Ptpi(Z[)
j=0 pj (z)
And the corresponding optimal intervention loss can be expressed as follows:
LIV(E, D) = -JS(p0, p2, ..., pt) + log(t + 1)
where JSlq1,q2,…,qk) = 1 Pk=I(KL(qjl∣q)), q = 1 Pk=I qj∙
Proof : See Appendix.
Theorem 2 tells us that when the discriminator reaches optimal, the intervention loss can be re-
garded as a certain distance among multiple distributions. We know when t = 1, the loss becomes
the original JS divergence which conforms with GAN. However, it is known that JS divergence
isn’t a perfect measure because the gradient would be zero if two variables have distributions with
disjoint support. According to the theorem, situation for multiple distributions will suffer less from
such problem, for bridge distributions provide greater chances for intersection which consequently
produces gradient. Therefore it would be more stable to update the encoder.
We now further illustrate the advantage of the multi-object adversarial training. Although GAN’s
convergence has been theoretically proved(Goodfellow et al., 2014), the training process, is well-
known delicate and unstable. One important reason is that the optimal discriminator can always
achieve too perfect no matter how close it is between the generated data manifold and the real data
manifold (Arjovsky & Bottou, 2017). One cause for this is that two manifolds is disjoint with
probability 1 due to their low dimension(Arjovsky & Bottou, 2017) on the data space.
3
Under review as a conference paper at ICLR 2022
Algorithm 1 Intervention Adversarial Auto-Encoder (IVAAE)
Input: learning rate: α, dimension of the space: d, number of bridge distributions: t Interventions:
Tk, 0 ≤ k ≤ t, hyperparameters: λ, μι, μ2, minibatch size: m.
Output: OD, OE, OG
1:
2:
3:
4:
5:
6:
7:
8:
9:
10:
11:
12:
13:
14:
15:
16:
17:
18:
for number of training iterations do
Sample {xj }jm=1 from the training set
Sample {zj}jm=1 from the prior P(z)
Compute Zj = E(xj), j = 1,…，m
Compute Xj = G(Zj), j = 1,..., m
for k = 0, ..., t do
Compute the intervened latent variables
Zjk = Tk(Zj), j = 1,...,m
end for
Update the parameters of D by:
OD 一 OD + mμιVθD pm=ι Pk=O IogDk (Zjk )
Calculate Lrecon, LIV
Update the parameters of G by:
Θg J Θg + m Vθg λLrecon
Update the parameters of E by:
Θe j Θe + mVθG {λLrecon + μ2Lιv }
end for
return OD, OE, OG
Different from GAN, AAE applies discriminator on the latent distribution, therefore what it learns
isn’t a low-rank manifold, but a continuous distribution with full dimension. But the problem doesn’t
get solved in AAE setting. We address it in a heuristic way. We run AAE on real-world dataset for
50 epochs, then fix the encoder and decoder and update the discriminator alone. We observe how
perfect the discriminator will develop. Figure 2 records the changing curve of adversarial loss every
iteration. According to Figrue 2, the adversarial loss still decreases remarkably toward 0, especially
when latent dimension goes high. That’s because of the limit batch size. IVAAE could alleviate
this problem by boosting the difficulty for discriminator to classify more than two distributions and
equivalently introducing more particles. We apply our models IVAAE. We choose t = 4, block-
wise substitution, with the rest of structure all the same as the AAE model. The loss is recorded
with fixed encoder and decoder after 50 epochs, too. We note that although the adversarial loss is
decreasing as well, the drop is much more moderate than AAE. We also examine the consequence of
such loss decline. We choose the adversarial loss for encoder: -Epdata [log(D(E(x)))], and record
the gradient norm of the encoder after the encoder and decoder fixed. According to Figure 3, AAE
is less stable compared to IVAAE.
AAE	IVAAE
12 3 4
- - - -
Oooo
Illl
SSo 二.les-l>pe
10-5
O IOOO 2000	3000	4000	5000	6000
training iterations
Figure 2: We train the model for 50 epochs. Left: AAE; Right: IVAAE (t = 4, block-wise). Then
we fix the encoder and decoder to run the discriminator alone. We record the loss change every
iteration. The dimension of latent space is chosen at 16, 32, 64.
O 1
O -
1 W
SSo 二eμes-l>pe
O IOOO 2000	3000	4000	5000	6000
training iterations
4
Under review as a conference paper at ICLR 2022
Sφ⅛w>
training iterations
Figure 3: We choose the latent dimension at 16 and run models for 50 epochs. Then we record
the change of encoder’s gradient with the encoder and decoder fixed. Note that Ladv (E, D) =
-E[log(D(E(x)))] is used as the encoder loss.
3	Related Work
3.1	Generative Models
Since the appearance of VAE and GAN model, large amount of attempts have been conducted
to improve their performance on generating data. Many of them put their attention on the latent
space. 2-stage VAE(Dai & Wipf, 2019) diagnoses the poor generate quality of VAE model and
attributes it to the mismatch of the distribution between the encoded q(z) and the prior p(z). They
handles the problem by imposing another VAE structure to learn the latent distribution, which
improves the model’s sample quality. RAE(Ghosh et al., 2019) view VAE from a deterministic
angle. It abandons the stochastic component and replace it with new proposed regularization term,
which provide an alternative to encode smooth and meaningful latent variables. Info VAE(Zhao
et al., 2019) enhance VAE model by introducing to ELBO a mutual information term between x
and z , which is a generalization of both VAE and AAE. It improves the quality of the variational
posterior and strengthen the bond between data space and latent features. Our method, different
from the above, dedicates to improve regularity of latent space by enhancing and stabilizing the
training process. It applies solid schemes and techniques while not deviating from the original target.
3.2	SVGD
Stein variational gradient descent (SVGD) is proposed as a variational inference algorithm (Liu &
Wang, 2016). The method iteratively transports a set of particles to match the target distribution by
applying a form of functional gradient descent that minimized KL divergence among a reproducing
kernel Hilbert space (RKHS). By using the assignment as follow:
m
zi+1 J Zi + m ^χ[k(zj, sllog P(Zlj)+Vzl k(Zj∙, Z)],
m j=1
(2)
where the k represents the kernel of RKHS, m batch size, the particles Zi gradually approximate
the target distribution p(Z). We apply the method in the construction of bridge distribution of the
IVAAE model. We choose step size e = 1, kernel function RBF k(z, z0) = exp(-1 ∣∣z 一 z0∣∣2)
with bandwidth h , and set the target distribution to the prior distribution p(Z), which is standard
5
Under review as a conference paper at ICLR 2022
normal in our context. To calculate the intervention loss, we obtain the batched encoded data zi =
E(xi), i = 1. . . m. We plug it in the iterative function above to update it t times to get {zl}lt=1.
Then we use {zl } as the input of discriminator to obtain the loss term and do the back propagation
all the way back. This is computationally practical because of the trivial form of Vlog p(z).
4	Experiments
In this section we conduct experiments on multiple datasets including MNIST (LeCun, 1998), CI-
FAR10 (Krizhevsky & Hinton, 2010), CelebA (Liu et al., 2018). We use different patterns of bridge
distribution in IVAAE and compare the performance to those of the related baseline method with
similar architecture. By exhibiting multiple measures during the train and showing the generated
pictures, we empirically demonstrate that the performance of our methods is superior on training
stability and generating images quality. The measures we use include FID (Heusel et al., 2017), a
popular index to measure the generating variety and quality, and Frechet Latent Distance (FLD),
a similar form of distance we propose to judge the latent distribution. The bridge distributions we
employ include blockwise and radial patterns, corresponding method denoted by IVAAEbw and
IVAAErad, respectively. We choose t = 4 in all of our experiments. The competing approaches we
apply include WAE-GAN, for it generalizes the AAE algorithm, and WAE-MMD as well.
We use Pytorch to implement our models. In IVAAE and other baseline models we adopt similar
architecture in order to give persuasive conclusion. We use deterministic models only. For all
datasets and models, we set hyperparameters as follows: learning rate α = 0.001 for encoder
and decoder, α = 0.0005 for discriminator, t = 4, λ = 1, μι =*,μ?=*,learning rate
decays by multiplying 0.5 after 30 epochs, further multiplying 0.2 after 50 epochs, and 0.1 after
100 epochs. We run all the codes for 300 epochs. Note that the intervention term in IVAAE
is down weighted by & because the derivatives of logrithmatic function becomes 冷 greater
when striking balance at ±, instead of 2 in binary conditions. There,s no noise added in any layers.
We calculate FID by generating 20k latent variables from the prior and feeding them to the trained
decoder. We randomly choose 60k images, get the encoded z, and use the Frechet distance to
measure its difference with the standard Gaussian:
FLD(Z) = kμzk2 + Tr(∑z - 2p∑Z + I)
where μz and ∑z is the means and covariance matrix of z, respectively. We choose the model with
the lowest FID score and calculate its FLD score to measure the regularization performance of
latent variables.
MNIST
The MNIST dataset contains 60K images of 28 × 28 handwritten digits. Before we use them, we
resize the images to 64 × 64 pixels. Instead of going for the best model performance, we adopt the
same architecture as WAE (Tolstikhin et al., 2017) do to compare the results among those models
with same structure. We choose the latent size 16, batch size 64. The network structure is two
fully connected linear layers for encoder, decoder and discriminator. The only difference in WAE
and IVAAE’s model structure is the output size of discriminator. See Table 3 in appendix. We use
learning rate distinct with the ground settings: 0.0001 for decoder and discriminator, 0.00005 for
the encoder. We run each model for 300 epochs and learning rate is divided by 2 every 100 epochs.
CelebA
CelebA dataset contains over 200k images of humans faces. We apply transform to crop 140 × 140
pixels from the central of images and then resize to 64 × 64 digits before we use them. The
architecture we adopt is similar to what is recommended in WAE. The dimension of latent space is
64. We use convolutional network for encoder and decoder with 5 × 5 or 6 × 6 size of filters, and a
network composed of four fully connected linear layers of 512 elements for the discriminator. See
Table 4 in appendix. Batch size is set at 100.
6
Under review as a conference paper at ICLR 2022
CIFAR10
To implement methods on CIFAR10 we set dimension of latent space 128. We reshape the
CIFAR10 images to 64 × 64 size. The network structure is completely the same as the network for
CelebA, except for the output size of encoder and input size for decoder and discriminator. The
latent dimension is relatively large considering batch size we choose as 128. We will note that
WAE-GAN fails to converge after 30-40 epochs while IVAAE is stable during the whole training 4.
Except for block-wise replacement and weighted Gaussian, we also use the SVGD(Liu & Wang,
2016) technique as a third method to construct bridge distribution. We obtain particle assembles
zl from encoded batched data z by iterative assignment (equation 2). A notable question is, once
the batch size is limited, the particle assemble zl is not approaching standard normal but with a
shrunk variance lower than 1. Therefore we need a high width h which could enlarge the repulsive
force between points. The discrepancy is also amplified when the data’s dimension goes higher and
batch size goes smaller. Therefore we implement the method only on MNIST dataset which needs
relatively low latent dimension.
We let t = 4 and choose RBF kernel. The step size is set to 1. We obtain {zl}lt=-11 for every two
iterations, while zt is sampled from standard Gaussian. SVGD degrades to a gradient descent of
MAP when width h = 0, while high h would cause the convergence to slowing down, so finally we
med2
choose h = 100h0, ho = m∣gn as recommended (LiU & Wang, 2016), where med is the median
of the pairwise distance between the current points zl . Besides, we add 0.1 Gaussian noise on the
particles after every iteration.
Note that in SVGD process points update in a relative smooth way by adding deviation on former
points. It is similar to the radial substitution. Moreover, when using equation 2 to update a latent
point, all points in current batch are involved, therefore the tendency of one point towards Gaussian
distribution is directly affected by force from other points. Such interaction become stronger under a
great bandwidth h. Intuitively we hope this feature would improve the hidden layer’s regularity. To
measure such regularity to a certain extent, we utilize the trained encoder to do classification work.
We fix the parameters of encoder after training WAE-GAN, IVAAErad, IVAAEsvgd models, respec-
tively. Then a two-layer MLP with 1000 nodes each is connected to the encoder’s last hidden layer
to do supervised learning. We set batch size 60, learning rate 0.0001, and evaluate the classification
error rate every 100 training iterations. The result is favorable to our argument as shown in Figure
5:
MNlST
C∣FAR10
FigUre 4: FID cUrves for different methods calcUlated dUring the training. Note that WAE-GAN’s
training process is not stable on CIFAR10.
7
Under review as a conference paper at ICLR 2022
Figure 5: MNIST classification error rate for methods WAE-GAN, IVAAErad, IVAAEsvgd. Param-
eters of the encoder is fixed after pretrained.
Table 1: FID score for different methods on multiple datasets. FID results are calculated every 10
epochs and we choose the minimum. Lower score means generated images are better. * means we
don’t go for the best performance across different structure, but for comparison purpose only.
Methods	MNIST	CelebA	CIFAR10
WAE-MMD	^^讦^^	36	87
WAE-GAN	53*	33	109
IVAAEbw	49*	34	117
IVAAErad	50*	33	80
IVAAEsvgd	49*	-	-
Table 2: FLD score for different methods on multiple datasets. We choose FLD when FID reaches
the best. Lower score means the encoded latent distribution is closer to the Guassian prior.
Methods	MNIST	CelebA	CIFAR10
WAE-MMD	-^0.05^^	0.34	10.9
WAE-GAN	5.5	1.5	15.1
IVAAEbw	0.2	0.22	4.0
IVAAErad	0.07	0.18	11.8
IVAAEsvgd	5.7	-	-
From the result we could notice that WAE-GAN’s training stability and speed is lower than IVAAE
on dataset we study. It conforms to what have been discussed in previous section. When the latent
size reach higher as 128 (with batch size 128), WAE-GAN suffers from intense oscillation. Every
time the model converges to a certain degree, the adversarial loss decreases and reconstruction loss
increases sharply. IVAAE performs well under the situation we inspect, and the FID score and the
FLD [Figure 6] when FID reaches the lowest is overall better [Table 1, 2]. Curiously, in terms of
the performance, among patterns we choose there seems to be no pattern that are superior to the
other on every datasets. On CIFAR10 we notice that IVAAErad attain far lower FID score than
IVAAEbw , while the FLD score goes to the opposite. There seems to be a trade-off relation between
the generate quality and the regularity (how close to the standard normal) of latent variables. It is an
interesting phenomenon to explore in the future work.
8
Under review as a conference paper at ICLR 2022
WAE-GAN
O
S
O
S
S
08
0.6
-0.4
0.2
-ι.o
0	10	20	30	40	50	60
-0.0
IVAAErad
0	10	20	30	40	50	60
-1.0
f:
0.4
-0.2
-0.0
Figure 6:	Heat maps for covariance matrix of the encoded z for different models when converging.
We choose CelebA with the latent dimension as 64. The brightness represent absolute value of the
corresponding entry. The brighter the closer to 1. The darker the closer to 0.


SC所mo ［灰ʧʌnrarg
n ■ ■
(a) WAE-GAN
(b) IVAAEbw
(c) IVAAErad
Figure 7:	CelebA images randomly generated. Left: WAE-GAN; Mid: IVAAEbw ; Right:
IVAAErad.
5 Conclusion
In this paper we propose a generative model which possess robust and stable training property by
leveraging interventions on latent variables and classifying them. It creatively applies the multiple-
output structure to discriminate over two distributions. It is theoretically proved that the intervened
latent distributions can be regarded as certain bridge distributions which work as a guidance to pull
the encoded variables to the target distribution. By conducting a heuristic loss trace for discrimina-
tor, we demonstrate that multiple classifying have a significant stabilizing effect on the adversarial
training in practice.
To structure different bridge distribution, we also apply the SVGD method to smoothly update the
latent variables. We successfully combine the method in our adversarial training, and to some extent
demonstrate the hidden space enjoys better regularity as well.
Besides, multiple experiments are conducted on real-world datasets. By using measures on both
images generating quality and latent regularity, we conclude a better performance of our models
compared to the baseline models. The data results coincide with our inference on the stability of
model training.
References
Martin Arjovsky and Leon Bottou. Towards principled methods for training generative adversarial
networks. arXiv preprint arXiv:1701.04862, 2017.
Bin Dai and David Wipf. Diagnosing and enhancing vae models. arXiv preprint arXiv:1903.05789,
2019.
9
Under review as a conference paper at ICLR 2022
Partha Ghosh, Mehdi SM Sajjadi, Antonio Vergari, Michael Black, and Bernhard SchOlkopf. From
variational to deterministic autoencoders. arXiv preprint arXiv:1903.12436, 2019.
Ian J Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil
Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial networks. arXiv preprint
arXiv:1406.2661, 2014.
Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter.
Gans trained by a two time-scale update rule converge to a local nash equilibrium. arXiv preprint
arXiv:1706.08500, 2017.
Diederik P Kingma and Max Welling. Auto-encoding variational bayes. arXiv preprint
arXiv:1312.6114, 2013.
Alex Krizhevsky and Geoff Hinton. Convolutional deep belief networks on cifar-10. Unpublished
manuscript, 40(7):1-9, 2010.
Yann LeCun. The mnist database of handwritten digits. http://yann. lecun. com/exdb/mnist/, 1998.
Jiadong Liang, Liangyu Zhang, Cheng Zhang, and Zhihua Zhang. Intervention generative adversar-
ial networks. arXiv preprint arXiv:2008.03712, 2020.
Qiang Liu and Dilin Wang. Stein variational gradient descent: A general purpose bayesian inference
algorithm. arXiv preprint arXiv:1608.04471, 2016.
Ziwei Liu, Ping Luo, Xiaogang Wang, and Xiaoou Tang. Large-scale celebfaces attributes (celeba)
dataset. Retrieved August, 15(2018):11, 2018.
Alireza Makhzani and Jonathon Shlens. Adversarial autoencoders. arXiv preprint
arXiv:1511.05644, 2015.
Ilya Tolstikhin, Olivier Bousquet, Sylvain Gelly, and Bernhard Schoelkopf. Wasserstein auto-
encoders. arXiv preprint arXiv:1711.01558, 2017.
Shengjia Zhao, Jiaming Song, and Stefano Ermon. Infovae: Balancing learning and inference in
variational autoencoders. In Proceedings of the aaai conference on artificial intelligence, vol-
ume 33, pp. 5885-5892, 2019.
10
Under review as a conference paper at ICLR 2022
A Appendix
A.1 Proof of Theorem 2
LIV (E,D) = -Ek 〜U [t]Ez〜Pk [log(Dk(z))]
—
X / Pk (Z)IogDk (z)dz
t+ 1 k=0 z
-p(z)	p(ek |z)logDk (z)dz
z	k=0
Last equation holds because p(ek |z) = P(Ep(P)(Z)
Notice that
Pk(Z)
(t+1)p(z).
t
-	p(ek |z)logDk (z)
k=0
-Xp(eklz)log Dk(Z) + H(P("z))
k=0	p(ek|z)
≥ - log Xp(ekIZ) ∙ DD(Z) +H(p(.|z))
k=0	p(ek|Z)
=H(P(IZ))
The equality holds when Dk(Z) H p(ek|z) H pk(∕), i.e. Dk(Z) = Ptpk(Z)(、
j=0 Pj (Z)
Plugging Dk(Z) into the loss term, we obtain:
LIV(E,D) = — —17 X ppk(Z)IogDk(Z)dZ
t+ 1k=0 Z
=-木 X Z Pk (Z)Iog PtPk (Z)()dZ
t+ 1k=0 Z	j=0 pj (Z)
=—7τ1γ X Z Pk(Z) [logpk⅛ — log(t + 1)1dZ
t+ 1k=0 Z	p(Z)
1t
=-1+1 EKL(PkkP)+ log(t +1)
k=0
So we have our theorem proved.

A.2 Network Structure
Table 3: The network architecture we use for MNIST. FC represents fully connected layer.
E	G	D
INPUT 64 × 64	INPUT Z	INPUT Z
FC(1024)	FC(16, 1024)	FC(16, 1024)
ReLU	ReLU	ReLU
FC(1024, 1024)	FC(1024, 1024)	FC(1024, 1024)
ReLU	ReLU	ReLU
FC(1024, 16)	FC(64 X 64), Tanh	FC(t + 1)
11
Under review as a conference paper at ICLR 2022
Table 4: The network architecture we use for CelebA and CIFAR10. CONV(C, K, S) represents
convolutional layer with C channels, K×K-size kernels, and S strides. DCONV represents decon-
volutional layer, parameter meaning similar with CONV. BN represents batch normalization layer.
E	G	D
INPUT 64 × 64 × 3	INPUT Z	INPUT z
CONV(C128, K5, S2)	FC(1024 × 8 × 8)	FC(512)
BN, ReLU	DCONV(C512, K5, S1)	ReLU
CONV(C256, K5, S2)	BN, ReLU	FC(512, 512)
BN, ReLU	DCONV(C256, K5, S1)	ReLU
CONV(C512, K5, S2)	BN, ReLU	FC(512, 512)
BN, ReLU	DCONV(C128, K5, S2)	ReLU
CONV(C1024, K5, S2)	BN, ReLU	FC(512, 512)
BN, ReLU	DCONV(C1, K4, S0)	ReLU
FC(C1)	Tanh	FC(t + 1)
12