Under review as a conference paper at ICLR 2022
A Sampling-Free Approximation of
Gaussian Variational Auto-Encoders
Anonymous authors
Paper under double-blind review
Ab stract
We propose a sampling-free approximate formulation of Gaussian variational auto-
encoders. Instead of computing the loss via stochastic sampling, we propagate the
Gaussian distributions from the latent space into the output space. As computing
the exact probability is intractable, we propose to locally approximate the decoder
network by its Taylor series. We demonstrate that this approximation allows us
to approximate the Gaussian variational auto-encoder training objective in closed
form. We evaluate the proposed method on the MNIST, the Omniglot, the CelebA,
and the 3D Chairs data sets. We find that our sampling-free approximation performs
better than its sampling counterpart on the Frechet inception distance and on par
on the estimated marginal likelihood.
1	Introduction
Variational Auto-Encoders (VAE) allow generating data, mapping data into a latent space, and
modifying data by perturbing it in a controlled manner in the latent space [1, 2]. This enables them
to generate and modify images [3, 4], speech [5], and molecular structures [6], or detect outliers in
distributions [7]. VAEs extend the concept of regular auto-encoders by modeling the latent space as a
distribution and with a distributional constraint that allows for a distributional latent space. Thus,
they enforce all samples drawn from the prior to be representative for the data set. For this, VAEs
predict a distribution through the encoder, sample from this distribution, and propagate the sample
through the decoder. During training, they constrain the predicted distribution with respect to the
prior, and require that the reconstructions produced by the decoder are correct. To allow for random
sampling in the context of gradient descent, Gaussian VAEs employ the sampling reparameterization
trick [8], which enables backpropagation through the sampling operation. In fact, the sampling
reparameterization trick and VAEs have received a lot of attention in research [3, 9-11].
Recently, research has also started to look into techniques to compute the posterior and marginal
distribution in an exact analytical solution without the need for sampling. In a sampling-free formula-
tion, during training, the loss is not computed via stochastic sampling but derived by propagating the
distributions from the latent space to the output space. However, computing the exact likelihood is
in many cases intractable. Two approaches have recently been proposed to address this challenging
problem. Balestriero et al. [12] propose to compute the likelihood for small neural networks by inte-
gration over piecewise continuous polytopes. Lucas et al. [13] formulate an analytical sampling-free
linear VAE, which can be efficiently computed in closed form.
In this context, the following work proposes a sampling-free approximate formulation for VAEs that
allows sampling-free training of VAEs. To this end, we approach the problem of intractability by
approximating the probability distributions instead of computing them exactly. This approximation
is done by local Taylor series expansion of locally affine (ReLU-activated) neural networks. This
corresponds to the local linearization of the respective neural network and allows for a local lineariza-
tion of the decoder of the Gaussian VAE. We propose to use the local linearization to estimate the
(co-)variances for each data point, while the median of the output distribution is computed exactly.
Locally approximating neural networks by their Taylor series around each individual data point
enables us to express the covariance matrix of the probability distribution in terms of the Jacobian of
the neural network. Furthermore, as computing the covariance matrix explicitly is very expensive, the
proposed formulation allows computing the likelihood without explicitly computing the covariance
matrix nor its inverse. This allows us to compute the training objective without sampling at a reason-
1
Under review as a conference paper at ICLR 2022
able additional cost for standard architectures, specifically, at the cost of evaluating the Jacobian of
the decoder. Note that the local Taylor series expansion or local linearization of the neural network
decoder function does neither correspond to a global linearization of the neural network decoder nor
a global approximation via its Maclaurin series.
We evaluate the proposed method on the CelebA [14], the 3D Chairs [15], and the MNIST [16]
data sets. When considering the number of samples necessary to estimate a covariance matrix of
equivalent quality wrt. the Frobenius norm via stochastic sampling, our approximation is (on average)
equivalent to more than 235 samples for a deep network on the CelebA data set. Overall, the proposed
approach achieves competitive results compared to sampling Gaussian VAEs. Especially on the
Frechet Inception Distance (FID) [17], the sampling-free VAE performs better than the sampling
VAE, while the estimated marginal likelihoods between both methods are on par. This demonstrates
that local linearization is a good approximation and does not damage performance in comparison to
a regular sampling VAE. Following the setup by Lucas et al. [13], the sampling-free VAE leads to
smaller (e, δ)-posterior collapse than its sampling counterpart.
We summarize the contributions of this work as follows:
•	We present a novel sampling-free approximation for training Gaussian VAEs. In contrast to exact
computation, it requires only a small overhead compared to its sampling counterpart, and reaches a
less noisy approximation of the training objective than sampling.
•	We show that the sampling-free Gaussian VAE approximation achieves competitive performance
on the MNIST, Omniglot, CelebA, and 3D Chairs data sets compared to sampling VAE methods.
Also, the sampling-free formulation also achieves a better FID on CelebA when provided with the
same computational budget as its sampling counterpart.
•	We find that sampling-free VAEs are more robust against posterior collapse than sampling VAEs.
The full implementation as well as experiments will be publicly available upon publication.
2	Related Work
Balestriero et al. [12] present an approach to compute the exact likelihood for small yet deep neural
networks using the continuous piecewise affinity of respective neural networks to compute the exact
likelihood by partitioning the network and integrating over the polytopes. Based on this, an analytical
Expectation-Maximization algorithm is derived that enables gradient-free learning. The method is
empirically validated by training a three layer deep generative network with 8 and 16 hidden units and
with one latent dimension on the number 4 from the MNIST data set, where they demonstrate that an
exact sampling-free VAE is superior to the stochastic sampling VAE. However, as the exact analytical
formulation of deep neural networks has a computational complexity that is exponential in the size of
the neural network, it is intractable for larger networks. Lucas et al. [13] examine sampling-free linear
VAEs, which by the nature of their linearity allow for efficient exact computation of the likelihood
and are able to show that the linear VAE recovers probabilistic principal component analysis (pPCA)
[18]. They employ the sampling-free linear VAE for understanding posterior collapse in VAEs and
apply it to the MNIST data set.
Ghosh et al. [19] propose regularized deterministic auto-encoders, to learn a smooth latent space
without sampling via different regularization schemes. They regularize their auto-encoders through
weight decay, the gradient norm of the decoder and/or spectral normalization, which all bound
the Lipschitz continuity of the decoder. Our work (implicitly) also includes a kind of gradient
regularization of the decoder. However, in our work, this arises from approximating the marginal
log-likelihood via local Taylor approximations of the decoder.
Tolstikhin et al. [20] propose Wasserstein Auto-Encoders (WAEs), which minimize the Wasserstein
distance between the model distribution and the target distribution. Kolouri et al. [21] propose Sliced
Wasserstein Auto-Encoders (SWAEs), which regularize the auto-encoders using the sliced / marginal
Wasserstein distance between the distribution of the encoded training samples and a predefined
sampleable distribution. Note that both WAEs and SWAEs require sampling [20-22]. Knop et
al. [22] propose Cramer-Wold Auto-Encoders (CWAEs), a sampling-free distributionally regularized
Auto-Encoder. They introduce the Cramer-Wold distance, which has a simple analytical formula
for computing normality of high-dimensional samples. By regularizing their auto-encoders via the
2
Under review as a conference paper at ICLR 2022
Cramer-Wold distance, they can evaluate their training objective in closed form. Note that CWAEs
are not VAEs as, for VAEs, the distance to the true distribution is measured using KL divergence
under the latent variable model [22]. Instead, the CWAE is a different kind of generative model that
was designed to have a tractable sampling-free objective.
Burda et al. [23] introduce importance weighted sampling for VAEs by using multiple samples to
have a tighter variational lower bound. Huang et al. [24] propose hierarchical importance weighted
auto-encoders reducing the number of redundant samples and introducing a hierarchical structure to
induce correlation. Dieng et al. [25] use importance weighted VAEs using an EM-based algorithm
employing moment matching. Roeder et al. [26] propose a low-variance gradient estimator for VAEs.
Park et al. [27] enrich the posterior by applying the Laplace approximation to VAEs, which enables
modeling a more expressive full-covariance Gaussian posterior. Tomczak et al. [28] also produce an
improved posterior distribution through a Householder transformation-based volume-preserving flow.
Nielsen et al. [29] bridge the gap between normalizing flows [30] and VAEs by preseting SurVAE
Flows, which use surjective transformation which are deterministic in one direction and stochastic /
sampling in the reverse direction. Morrow et al. [31] present VAEs with normalizing flow decoders.
Taylor series approximations [32] are a ubiquitous tool in variational inference (VI), e.g., in the delta
or the Laplace method [33, 34]. VI methods, such as the delta method or the Laplace method require
solving an optimization problem to obtain an approximation of an output distribution. In contrast,
our method uses the Taylor approximation to produce a closed-form estimate of the training objective
and the only optimization is the training of the VAE.
A phenomenon frequently appearing in VAEs is posterior collapse, i.e., the posterior distribution
(produced by an encoder) is close to the prior distribution. This leads to a reduced expressivity of
the VAE’s latent space. To tackle this, Kingma et al. [8] proposed to constrain the KL divergence
such that it is only active if the KL divergence is above a threshold. Other works used KL annealing
[35, 36] or constrained the posterior to have a minimum distance to the prior [37]. Lucas et al. [13]
defined the (e, δ)-posterior collapse in order to reliably quantify posterior collapse in VAEs.
3	Method
We begin this section by introducing the conventional data generating process of VAEs. After that,
we briefly motivate and derive the evidence lower bound (ELBO) and present our sampling-free
local approximation of the ELBO including an efficient way of computing it. Following this, we
discuss a method for stabilizing training and investigate the balance between reconstruction quality
and variance of reconstructions.
3.1	Gaussian Variational Auto-Encoders
We begin by stating the assumptions about the data generating process. Note that this process does
not differ from the conventional data generating process of VAEs [1]. For easier reference, we follow
the notation of Kingma and Welling [9]. The goal of VAEs is to approximate the true distribution of
the datap*(x) with a distribution parameterized via a modelpθ(x) ≈ p*(x). Here,
p(z) = N(z; 0, I)
Pθ(x|z) = N(x; fθ(z), CI) for some c > 0
Pθ(x) = /Pθ(z)Pθ(x|z) dz .
(1)
(2)
(3)
z denotes an m-dimensional vector referring to the hidden / latent space, x denotes an n-dimensional
vector referring to the input / output space, fθ is the decoder / generator network, and c captures the
observation uncertainty. N denotes a multivariate Gaussian distribution and Im the m × m identity
matrix. To approximate the intractable pθ (z|x), VAEs include an encoder network gφ and a network
that computes the encoder’s uncertainty hφ (in the form ofa diagonal covariance matrix) such that:
qφ(z|x) ≈ pθ(z|x)
where qΦ (z|x) = N (z; gφ(x),hφ(x)).
(4)
3
Under review as a conference paper at ICLR 2022
The training objective is to maximize the log-likelihood of our data log pθ (x) by optimizing θ and φ:
log PP (X)=Eqφ3x) (log pJ∣z∣X())+Eqφ(ZIx) (log pφ⅛))	(cf.[9])
=Eqφ(z∣x) (log PP (XIZ))- Eqφ(z∣x) (log qpZx)) + Eqφ(z∣x) (log Pl^
=Eqφ(z|x) (logPp(x∣z)) - DκL(qφ(z∣x),p(z)) + Dkl(⅛φ(z∣x),Pp(ZIX)).
、---------------------V----------------------/ 、-----------------------'
ELBO	not computable but non-negative
(5)
(6)
(7)
The last term is not computable, because PP (z∣x) is intractable, and hence the log-likelihood log PP (x)
cannot be computed exactly. However, since a KL-divergence is non-negative, the first two terms
yield a lower bound for logPP(x), the so-called evidence lower bound (ELBO) [1, 9, 38]. The ELBO
is the substitute objective that allows training VAEs.
ELBO = log PP (x) - DKL(qφ(∣x),PP(∣x)) ≤ log PP (x)
(8)
The KL-Divergence DKL(qφ(z∣x), P(z)) can be computed in closed form. However, the computation
of Eqφ(z∣x)(logpp(x∣z)) is generally intractable and is therefore usually approximated via stochastic
sampling from the latent distribution qφ(z∣x). In the stochastic sampling approach, a data point x is
propagated through gφ and hφ, an element z is sampled from N (z; gφ(x), hφ(x)), and propagated
through fp, which approximates the output distribution (including the observation uncertainty)
N(x; fp(z),cI) where Z ~ N (z; gφ(x), hφ(x)). Since the observation uncertainty is an isotropic
normal distribution, the sample log-likelihood logPp (x∣z) reduces to
logpp(x∣z) = -2c (IIfP(z) - x∣∣2) - n log(2πc)	(9)
which is, apart from the normalization constant, a mean squared error [1].
3.2	Sampling-Free Approximation
At this point, we introduce the proposed sampling-free approach. In the sampling-free approach, a
data point x is propagated through gφ and hφ, and then the obtained distribution N(z; gφ(x), hφ(x))
is (without sampling) approximately propagated through fp , which yields the output distribution
N (x; fp(z), CI) where Z ~ N (z; gφ(x), hφ(x)). The problem is that computing the exact transfor-
mation of the distribution N (z; gφ(x), hφ(x)) by fp is intractable (for non-trivial fp) because it can
have an exponential complexity. The largest analytical sampling-free approach so far was able to
operate on 24 hidden neurons [12]. This is because, for the exact computation, every linear region of
fp has to be computed separately. To simplify notation, in the following, we drop subscripts θ and φ
for f, g, h.
To make this problem tractable, we approximate the neural network decoder via a local Taylor series
expansion, which simplifies computing the output distribution. For that, we define the Taylor series
of the decoder f around g(x) for each data point x as fg(x). We display the Taylor series of an
example neural network function in Figure A.1 in the supplementary material. Notably, the function
and its Taylor series are equal near this point g(x), which is also the point of largest density of
the distribution N(z; g(x), h(x)). Due to the continuous piece-wise affinity of f, we only need to
compute the first two terms of the Taylor expansion, as the remaining terms are zero. Thus,
fg(x)(g(x) + e) = f(g(x)) + df(g(x)) ∙ e = f(g(x)) + Jf(g(x))∙ e	(10)
dg (x)
where Jf(g(x)) is the Jacobian of f at point g(x), in the following abbreviated as J. This approxima-
tion is exact if e is small enough, such that it stays inside the linear region of f in which g(x) is located.
In Section 4.1, we empirically demonstrate that the Taylor series delivers a good approximation for
the neural network decoders in VAEs. For neural networks that are not piece-wise affine, like the
tanh networks used in the experiments in Section 4.3, we also use Equation 10, which in this case is
the first-order Taylor expansion. Using the Taylor series for each point fg(x) we can approximate the
output distribution as follows:
fg(x) (Z) ≈ f (z)	where Z ~N(z; g(x),h(x))	(11)
4
Under review as a conference paper at ICLR 2022
As the family of Gaussian distributions is closed under affine transformations and summation [39]:
fg(x) (Z)〜N (x′; f (g(x)), J h(x) JT)	where Z 〜N(z; g(x), h(x))
and including the observation uncertainty
fg(x) (z) + o 〜N(x′； f (g(x)), J h(x) JT + CI)	where o 〜N(o; 0, CI).
(12)
`----------V----------'
=: Σx
(13)
This leads to a sampling-free approximation of the log-likelihood
Eqφ(z∣χ) logPθ(x|z) ≈ log ((2∏)-2 det(Σχ)-2 exp (-2(f (g(x)) - x)τΣ-1(f (g(x)) - x)))
=-n log(2π) - 1 logdet(Σχ) - 2(f (g(x)) - X)TΣχ1(f(g(x)) - x) (14)
At this point, it is necessary to resolve Σx-1 and det(Σx).
Lemma 1 (Woodbury matrix identity). [40]. The inverse of a rank-m correction of a matrix can be
computed in terms of the inverse of the original matrix as
(A+ UV)-1 = A-1 - A-1U(Im + VA-1U)-1VA-1.	(15)
(17)
(18)
Since Σχ = CI + J h(x) Jτ, and h(x) is a diagonal matrix, We can set U = J ,h(x) such that
UUT = UV = J h(x) JT. As the computation of the covariance via the Jacobian directly produces
a low-rank decomposition of the covariance matrix and because m ≪ n, this allows for a fast and
space-efficient computation. Thus,
Σx1 = (cI + Jh(X)JT)-1	= (cI + UU T)T
=C I - C IU(Im + UT1 IU)TUT C I = C I - C2 U(Im + C U TU)TUT	(16)
=C (I - C U(Im + C UT U )-1U t)	= C (I - U (cIm + U TU)TU t)
which only requires inverting an m × m matrix (innermost parentheses).
Lemma 2 (Matrix determinant lemma). [41]. The determinant of a rank-m correction of a matrix
can be computed in terms of the inverse and determinant of the original matrix as
det(A+UV) = det(Im + V A-1U) det(A)
With U defined as above, we can see that
det(Σx) = det (cI + Jh(x)Jτ)	= det (cI + UUτ)
=det (Im + UT1IU) det(cI) = det (Im + CUτU) cn
As we can now compute Σx-1 and det(Σx) very efficiently, we can extend the log-likelihood as
Eqφ(z∣χ) logPθ(x|z) ≈ -n log(2π) - 1 logdet(Σχ) - 2(f (g(x)) - x)τ∑χ1(f (g(x)) - x)
=-n log (2∏) - 2 logdet(Im + 1UτU) - n log(c)
-1 (f (g(x)) - x)T C (I - U(cIm + UTU)-1Uτ) ∙ (f (g(x)) - x) (19)
`-----------------------------V------------------------------'
=-2c(f(g(X))-X)τ(f(g(X))-X) + 2c((f(g(X))-X)TU)(CIm+UTU)-I ∙(Uτ(f(g(X))-X))
Here, as the inverse of the covariance matrix (Σx-1 of size n × n) does not need to be computed
explicitly and the factorization of the covariance can be multiplied with the error, only operations on
small matrices need to be performed. Specifically, it requires one multiplication of a vector of size
n with a matrix of size m × n, the inversion of an m × m matrix, and the multiplication of vectors
from both sides with an m × m matrix. This significantly reduces the computation cost.
Based on these insights, we can write the ELBO as
ELBO=Eqφ(z∣X) (logpθ(x∣z)) - DκL(qφ(z∣x),p(z))	(20)
≈ -n log (2πc) - 2 logdet(Im + CUτU) - ɪ(f (g(x)) - x)τ(f (g(x)) - x)
+ 2c ((f (g(x)) - x)τU) (cIm + UTU)T (Uτ(f (g(x)) - x)) - DκL(qφ(z∣x),p(z))
Since the training objective is to maximize the ELBO, the corresponding loss is L = - ELBO.
5
Under review as a conference paper at ICLR 2022
3.3 Stabilizing Training and Reducing Bias by Regularization
In practice, for covariance matrices with large variances, the ratio between the largest and smallest
eigenvalue can become extremely large even with Tikhonov regularization (e.g. 108). Thus, training
is not feasible because the gradients in some or most directions are vanishingly small. To account for
this, we propose a regularization that works independently of the ratio between eigenvalues. For this,
we use a mixture between the approximated log-likelihood as described above and the log-likelihood
of a conventional auto-encoder (i.e., only considering observation-noise). Specifically, we replace
∑x1 by Y ∙ ∑x1 + (1 - Y) ∙ (cI)-1. This regularization allows for an efficient and stable training,
even if training with Σx-1 alone would be vanishingly slow or diverges. This means, we calculate
Σx,γ ≈ 11 - γCU(cI + UTU)-1UT insteadof Σx1 = CI - CU(cI + UTU)-1UT (21)
where Y quantifies the degree to which the vari-
ation produced by the decoder is considered.
In our experiments, we found that Y ≈ 0.99
avoids vanishingly slow training and divergence
due to numerical instabilities. Empirically, we
found that reducing Y can reduce the bias of our
sampling-free approximation. In Figure 1, we
train a model using the proposed sampling-free
objective with Y and evaluate it using the unbi-
ased sampling ELBO. We find that with Y=0.95,
the model converges to an ELBO of -8000. By
reducing Y, i.e., increasing the regularization,
we can increase the ELBO that the model con-
verges to, which means that we improve the
model. Setting Y=0.67, the ELBO already con-
verges to -7500, and after setting Y=0.5 it con-
verges to -7400. We find that Y = 0.5 works
well overall and, thus, we use it as a canonical
choice in our experiments.
Figure 1: By scheduling Y , we can observe that
increasing the regularization (i.e., reducing Y) re-
duces the approximation bias and increases the
point of convergence, which improves the model.
The model is trained with the biased sampling-free
VAE objective on the CelebA data set and evalu-
ated using the unbiased sampling ELBO estimate.
4 Experiments
4.1 Quality of the Taylor Series Approximation
To evaluate the quality of the
Taylor series approximation in
VAEs, we consider the precision
to which variances and covari-
ances are estimated and compare
this to different numbers of sam-
ples. For this, we use sampling
VAEs trained on the CelebA and
3D Chairs data sets as described
in more detail in the next section
and with a latent dimensionality
of 16. We consider two metrics:
The Pearson correlation coeffi-
cient of the (pixel-wise) output
Table 1: Quality of the Taylor series approximation. For both
data sets, we trained a (sampling) VAE and evaluate the Pearson
correlation between the true and estimated variances. For the
CelebA data set, we also evaluate the Frobenius norm between
the true and estimated covariance matrices. We evaluate our
approximation and compare it to different numbers of samples.
Method	Taylor Sampling
#Points	1		5	10	25	50	70	100	235
CelebA	Var. Corr.	.967	.680	.816	.912	.955	.966	.976	.988
	Cov. Frob.	297	17 446	7 779	2951	1 456	1 033	737	302
3D Chairs	Var. Corr.	.975	.686	.819	.914	.953	.966	.976	.988
variances and the Frobenius norms of output covariance matrices. Here, we compute the true output
(co-)variances via a large number of samples (15 000) and compare it to the (co-)variances produced
by our approximation or the (co-)variances produced by a certain number of samples. Results are
shown in Table 1. We find that, even in large deep neural networks, we achieve Pearson correlations
of 96.7 - 97.5% for estimating the variances, which would require 70 - 100 samples in the latent
space when relying on sampling. On the Frobenius norm of covariance matrices, our approximation
is (on average) equivalent to more than 235 stochastic samples for the CelebA data set. Because
computing the true covariance matrix is intractable, we estimate it to a precision of 1% of the reported
Frobenius norms. For the 3D Chairs data set, we omit estimating the true covariance matrices due to
6
Under review as a conference paper at ICLR 2022
computational feasibility. In addition, we analyzed the ratio between the variances produced by our
sampling-free propagation and the sampling approach and found that across data sets this ratio is
around 1.2. This means that the variances are over-estimated by a factor of 1.2 by our approximation.
We attribute this to the fact that non-linearities such as ReLU can become constant which is not
anticipated by the Taylor approximation.
4.2 MNIST, CelebA, and 3D Chairs
We evaluate the sampling-free VAE on the CelebA
data set [14], on the 3D Chairs data set [15], as
well as on the MNIST data set [16]. For the
CelebA and 3D Chairs experiments, we use the
same network architecture as Higgins et al. [3] and
train it with the Adam optimizer [42] at a learning
rate of 10-4 for 106 iterations. We use 12 - 64
latent dimensions for CelebA and 8 - 32 latent
dimensions for 3D Chairs. As in previous works
on VAEs, we set c = 0.5 which corresponds to a
standard deviation of the observation uncertainty
of σ = ʌ/θɪð for all methods. We use a model
for the encoder and decoder with 5 convolutional
and 1 fully connected layers each. This is also the
architecture that was used in many recent works
on sampling VAEs [3, 11] as well as similar to
most public implementations. Further details can
be found in the supplementary material.
Qualitative Evaluation In Figure 2, we show
the manifold of a sampling-free VAE in compar-
ison to a sampling VAE. Figure 3 displays qual-
itative results for CelebA and 3D Chairs. Here,
we display reconstruction results as well as traver-
sals in the latent space for 3 images and 2 latent
dimensions. In the supplementary material, we
qualitatively compare the sampling-free VAE to
the sampling VAE, the (sampling) β-VAE, as
well as to a conventional auto-encoder.
Figure 3 shows that the sampling-free VAE can
successfully reconstruct and that latent traversals
produce meaningful images. For the CelebA data
set, we show the traversal of the latent dimensions
corresponding to, ‘smiling‘, ‘color temperature’,
‘brightness of background’, and ‘hair color’. For
the 3D Chairs data set, the latent dimensions corre-
spond to 'chair∙→arm chair, and 'color’, ‘wheels’,
and ‘thickness of legs and back’. For the auto-
encoder on the 3D Chairs data set, even among a
variety of hyper-parameters, training always con-
verges to the plain white image as can be seen in
the supplementary material.
OQOOO 333S>
OoOoo6∙33S
oqoooo335
OOQQoo338
ooooo^33S
OOOOOOd 3 3
000000^3 8
l□bt>C>SG6 / ʃ
SSGGQqq 夕，
U⅛MHH^<17 7
34Wq-I 77
HHHM∙1Π777
HMM*TT7*77∙7
气 RT 3 *1 *7 φ7 *7 7
*1,1 *1 *3 *ɜ w7 w? *7 *7
7
7
7
7
7
7
? ? 2 Z Z / /
∙⅛3 ? ? ZZ / /
%%?»eɔɪz / /
¾5¾33a2,
7
7
7
Figure 2: Learned MNIST manifolds. Left:
sampling-free VAE. Right: sampling VAE. The
latent space is 2 dimensional and the displayed
range is [-5, 5]. Training details are in the sup-
plementary material.
Figure 3: Reconstructions and traversals for the
CelebA (left) and 3D Chairs (right) data sets.
The traversals are in the range of [-3, 3]. An
exhaustive display of all latent dimensions as
well as a comparison to sampling VAEs and AEs
is provided in the supplementary material.
Quantitative Evaluation To quantitatively evaluate the sampling-free approximate VAE, we com-
pare the Frechet Inception Distance (FID) as well as the estimated marginal likelihood (ML) between
a conventional sampling VAE and a sampling-free VAE as shown in Figure 4. The FID quantifies
how close the images produced by the VAE’s data generating process are to the original data set as
well as the quality of the produced images. For the FID, the distribution of images sampled from
the VAE is compared to the distribution of images in the data set by utilizing embeddings produced
by an Inception neural network. The ML quantifies the quality of a VAE by quantifying how close
7
Under review as a conference paper at ICLR 2022
Figure 4: Quantitative evaluation of FID (left) and log marginal likelihood (right) on the CelebA
data set for different sizes of the latent space. On FID, the sampling-free Gaussian VAE significantly
outperforms the sampling Gaussian VAE and performs on par regarding the log marginal likelihood.
Results average over 3 runs and minimum / maximum are marked.
posteriors are to the priors as well as considering the reconstruction error under sampling. For the
ML, for each image, the marginal likelihood is estimated via many samples (50 000). We found that
for CelebA, for each size of the latent space, the FID of the sampling-free VAE is significantly better
than the FID of the sampling VAE. Furthermore, we found that the ML of the sampling-free VAEs
is very close to the ML of the sampling VAEs, for some latent dimensionalities, the sampling-free
VAE is better, while for others the sampling VAE is better. Plots for the 3D Chairs data set can be
found in the supplementary material. We emphasize that we evaluated the sampling-free VAE also
via sampling such that there is no bias in the estimates.
To evaluate the models based on compu-
tational budget, we additionally report re-
sults for the sampling-free VAE to match
the computational budget of the sampling
VAE’s full training. Specifically, we evalu-
ate the sampling-free variant after 650 000
training steps, while the sampling counter-
part trains for 1 000 000 steps. The results
are shown in Table 2. Even in this setting,
the sampling-free formulation achieves a
better on FID than the sampling VAE on
the CelebA data set, however, the esti-
mated log marginal likelihood suffers.
Table 2: Quantitative evaluation for the sampling-free
VAE, where we include the performance of the sampling-
free VAE with shorter training such that the total training
budget matches the total full training time of the sam-
pling VAE. Results are reported for a 16 dimensional
latent space and averaged over 3 runs.
Method	CelebA 3D Chairs
FID log ML FID log ML
Sampling-Free Gaussian VAE	95.79 -7244.9 51.96 -7078.8
Sampling Gaussian VAE	99.66 -7245.3 53.53 -7078.9
Sampling-Free (matching budget) 98.00 -7247.1 54.21 -7079.6
4.3	Comparison to other Gradient Estimate Variance Reduction Methods
We compare our method to the Sticking-the-Landing (Stick) method by Roeder et al. [26] and to the
importance weighted auto-encoders (IWAE) by Burda et al. [23]. For this, we train a VAE on the
MNIST [16] and Omniglot [43] data sets and for the sampling methods (vanilla sampling VAE, Stick,
IWAE) we vary the number of samples. The encoder and decoder of the architecture each have two
hidden layers with tanh activation functions and 200 neurons per layer. We train the model at a batch
size of 20 using the Adam optimizer [42]. This follows the setup by Roeder et al. [26]. However, a
difference is that we model the image space with Gaussians in contrast to the Bernoulli outputs used
by Roeder et al. [26]; as the method ([26]) readily supports Gaussian outputs, we use it with Gaussian
outputs. As tanh is not piece-wise linear, we use the first-order Taylor approximation in this case.
Table 3: Evaluation on MNIST (left) and Omniglot (right) with different methods and numbers of
samples. The metric is the NLL on the test set (smaller is better) averaged over 5 runs.
Samples	Ours	vanilla VAE	Stick	IWAE	Samples	Ours	vanilla VAE	Stick	IWAE
1	26.18	27.61	26.79	27.62	1	30.14	31.49	30.99	31.49
5	—	26.98	26.50	26.44	5	—	31.15	30.75	30.91
20	—	26.62	26.48	26.11	20	—	30.88	30.73	30.51
50	—	26.64	26.59	26.08	50	—	30.83	30.67	30.36
100	—	26.58	26.79	25.99	100	—	30.76	30.73	30.34
200	—	26.70	26.70	25.95	200	—	30.72	30.70	30.30
8
Under review as a conference paper at ICLR 2022
Table 4: Posterior collapse experiment: VAEs trained on MNIST with a 200-dimensional latent
space. Full batch training on a subset of 1 000 MNIST images for a controlled setting where for
the sampling-free VAE no stochastic effects are present, and for the sampling approach only the
stochasticity of the reparameterization is present. Results are averaged over 10 runs.
Method / Metric Epochs	(e, δ)-posterior collapse		Reconstruction Error		KL-Divergence	
	100	1000	100	1 000	100	1 000
Sampling VAE	99.0% (±1.5%)	60.0% (±15.7%)	172.5 (±3.0)	46.2 (±12.4)	0.1 (±0.1)	4.8 (±1.3)
Sampling-Free VAE	28.4% (±4.2%)	0.6% (±0.6%)	38.7 (±0.5)	9.8 (±0.4)	2.9 (±0.1)	4.2 (±0.1)
We report the results in Table 3. We can see that on both data sets, the sampling-free method achieves
competitive performance. The sampling VAE and the Sticking-the-Landing method do not achieve
the same NLL even with 200 samples. Only the IWAE can outperform the sampling-free method on
MNIST with at least 20 samples. On Omniglot, our method also performs better than IWAE with 200
samples. Overall, the results show that the sampling-free approximation performs very well, even
when compared to training with large numbers of samples.
4.4	Posterior Collapse
We analyze the effect of a sampling-free approximate formulation of deep VAEs on the problem of
posterior collapse. For this, We use the notion of an (e, δ)-collapse of a dimension introduced by
Lucas et al. [13]. It is defined as follows: A latent dimension i has (e, δ)-collapsed if
Ex〜P (DκL(qχ(zi),p(zi)) < e) ≥ 1 - δ.	(22)
We follow Lucas et al. [13] in setting δ = 0.01. We replicate their setting of training a VAE
with 2 hidden layers and 200 latent dimensions for 1 000 steps with full batch training on a subset
of 1 000 MNIST images. We report the fraction of dimensions that have (e, δ)-collapsed where
e = 0.05, δ = 0.01, the reconstruction error, as well as the KL divergence after 100 and 1 000 training
steps. We report these results averaged over 10 runs in Table 4.
We find that, for the sampling-free VAE, the reconstruction error is smaller than the reconstruction
error for the sampling VAE. While the sampling VAE has a posterior collapse in 60% of the latent
dimensions, the sampling-free VAE suffers from posterior collapse in only 0.6% of the latent
dimensions. Simultaneously, the KL-Divergence is lower for the sampling-free VAE. These results
indicate that the sampling-free VAE is more robust against (e, δ)-collapse than the sampling VAE.
4.5	Runtime Analysis
Finally, we discuss the runtime of the proposed approach and compare it to its sampling counterpart.
For this analysis, we compare the training times of both methods on the CelebA data set for 106
steps at a batch size of 64 and 16 latent dimensions on a single Quadro RTX 6000 GPU. For the
sampling VAE we measure an average training time of 9.7 hours, while the proposed sampling-free
formulation takes on average 14.5 hours on the same GPU. Note that, in case of the sampling-free
VAE, the computationally most expensive component is computing the Jacobian. Thus, if the hidden
dimension would be very large, the computational overhead would scale linearly (O(m)) with the
dimensionality of the hidden space m. Note that the overhead of our method is small in comparison
to the (in the number of neurons) exponential overhead of computing the exact sampling-free ELBO.
5 Discussion & Conclusion
We proposed a novel approximate sampling-free approximation of Gaussian VAEs. Our formulation is
reasonably fast to compute, i.e., it can be computed in linear time in the number of latent dimensions,
contrasting prior art which had an exponential complexity for computing the exact objective. We found
that the proposed formulation achieves a significantly lower variance in estimating the objectives
than the sampling approach. However, while the sampling approach is unbiased, our lower-variance
approximation is not unbiased. Nevertheless, our approximation learns better Gaussian VAEs than its
sampling counterpart and can even compete when the computational budget is fixed. Furthermore, we
observe that the sampling-free VAE is more robust against posterior collapse than the sampling VAE.
We hope to have paved the way for future research to analyze and understand the behavior of VAEs.
9
Under review as a conference paper at ICLR 2022
Reproducibility
In this work, we only use public data sets. We implemented all experiments in PyTorch [44] and our
implementation will be made publicly available upon publication. Network architectures and training
hyperparameters are specified in the supplementary material.
References
[1]	D. P. Kingma and M. Welling, “Auto-encoding variational bayes,” in International Conference on
Learning Representations (ICLR), 2014.
[2]	D. J. Rezende and F. Viola, “Taming vaes,” arXiv preprint arXiv:1810.00597, 2018.
[3]	I. Higgins, L. Matthey, A. Pal, C. Burgess, X. Glorot, M. Botvinick, S. Mohamed, and A. Lerchner,
“Beta-vae: Learning basic visual concepts with a constrained variational framework,” in International
Conference on Learning Representations (ICLR), 2017.
[4]	T. Inoue, S. Choudhury, G. De Magistris, and S. Dasgupta, “Transfer learning from synthetic to real
images using variational autoencoders for precise position detection,” in 2018 25th IEEE International
Conference on Image Processing (ICIP), IEEE, 2018, pp. 2725-2729.
[5]	M. Blaauw and J. Bonada, “Modeling and transforming speech using variational autoencoders,” Inter-
speech 2016; 2016 Sep 8-12; San Francisco, CA., 2016.
[6]	Q. Liu, M. Allamanis, M. Brockschmidt, and A. L. Gaunt, “Constrained graph variational autoencoders
for molecule design,” in Proc. Neural Information Processing Systems (NIPS), 2018.
[7]	S. Eduardo, A. Nazdbal, C. K. Williams, and C. Sutton, “Robust variational autoencoders for outlier
detection and repair of mixed-type data,” in International Conference on Artificial Intelligence and
Statistics, PMLR, 2020, pp. 4056-4066.
[8]	D. P. Kingma, T. Salimans, R. Jozefowicz, X. Chen, I. Sutskever, and M. Welling, “Improving variational
inference with inverse autoregressive flow,” in Proc. Neural Information Processing Systems (NIPS),
2016.
[9]	D. P. Kingma and M. Welling, “An introduction to variational autoencoders,” arXiv preprint
arXiv:1906.02691, 2019.
[10]	B. Dai and D. Wipf, “Diagnosing and enhancing vae models,” in International Conference on Learning
Representations (ICLR), 2019.
[11]	C. P. Burgess, I. Higgins, A. Pal, L. Matthey, N. Watters, G. Desjardins, and A. Lerchner, “Understanding
disentangling in beta-vae,” in Proc. Neural Information Processing Systems (NIPS), 2017.
[12]	R. Balestriero, S. Paris, and R. G. Baraniuk, “Analytical Probability Distributions and EM-Learning for
Deep Generative Networks,” in Proc. Neural Information Processing Systems (NIPS), 2020.
[13]	J. Lucas, G. Tucker, R. B. Grosse, and M. Norouzi, “Don’t Blame the ELBO! A Linear VAE Perspective
on Posterior Collapse,” in Proc. Neural Information Processing Systems (NIPS), 2019, pp. 9408-9418.
[14]	Z. Liu, P. Luo, X. Wang, and X. Tang, “Deep learning face attributes in the wild,” in Proc. International
Conference on Computer Vision (ICCV), 2015.
[15]	M. Aubry, D. Maturana, A. A. Efros, B. C. Russell, and J. Sivic, “Seeing 3d chairs: Exemplar part-based
2d-3d alignment using a large dataset of cad models,” in Proc. International Conference on Computer
Vision and Pattern Recognition (CVPR), 2014.
[16]	Y. LeCun, C. Cortes, and C. Burges, “Mnist handwritten digit database,” ATT Labs, vol. 2, 2010. [Online].
Available: http://yann.lecun.com/exdb/mnist.
[17]	M. Heusel, H. Ramsauer, T. Unterthiner, B. Nessler, and S. Hochreiter, “Gans trained by a two time-scale
update rule converge to a local nash equilibrium,” arXiv preprint arXiv:1706.08500, 2017.
[18]	M. E. Tipping and C. M. Bishop, “Probabilistic principal component analysis,” Journal of the Royal
Statistical Society. Series B (Statistical Methodology), vol. 61, no. 3, pp. 611-622, 1999.
[19]	P. Ghosh, M. S. M. Sajjadi, A. Vergari, M. Black, and B. Scholkopf, “From variational to deterministic
autoencoders,” in International Conference on Learning Representations (ICLR), 2020.
[20]	I. Tolstikhin, O. Bousquet, S. Gelly, and B. Schoelkopf, “Wasserstein auto-encoders,” in International
Conference on Learning Representations (ICLR), 2018.
[21]	S. Kolouri, P. E. Pope, C. E. Martin, and G. K. Rohde, “Sliced-wasserstein autoencoder: An embar-
rassingly simple generative model,” in International Conference on Learning Representations (ICLR),
2019.
[22]	S. Knop, P. SPUrek, J. Tabor, I. Podolak, M. Mazur, and S. Jastrzebski, “Cramer-wold auto-encoder,”
Journal of Machine Learning Research, vol. 21, 2020.
[23]	Y. Burda, R. Grosse, and R. Salakhutdinov, “Importance weighted autoencoders,” in International
Conference on Learning Representations (ICLR), 2016.
10
Under review as a conference paper at ICLR 2022
[24]	C.-W. Huang, K. Sankaran, E. Dhekane, A. Lacoste, and A. Courville, “Hierarchical importance weighted
autoencoders,” in International Conference on Machine Learning (ICML), PMLR,2019,pp. 2869-2878.
[25]	A. B. Dieng and J. Paisley, “Reweighted expectation maximization,” arXiv preprint arXiv:1906.05850,
2019.
[26]	G. Roeder, Y. Wu, and D. Duvenaud, “Sticking the landing: Simple, lower-variance gradient estimators
for variational inference,” in Proc. Neural Information Processing Systems (NIPS), 2017.
[27]	Y. Park, C. Kim, and G. Kim, “Variational laplace autoencoders,” in International Conference on Machine
Learning (ICML), PMLR, 2019, pp. 5032-5041.
[28]	J. M. Tomczak and M. Welling, “Improving variational auto-encoders using householder flow,” arXiv
preprint arXiv:1611.09630, 2016.
[29]	D. Nielsen, P. Jaini, E. Hoogeboom, O. Winther, and M. Welling, “Survae flows: Surjections to bridge
the gap between vaes and flows,” in Proc. Neural Information Processing Systems (NIPS), 2020.
[30]	G. Papamakarios, E. Nalisnick, D. J. Rezende, S. Mohamed, and B. Lakshminarayanan, “Normalizing
flows for probabilistic modeling and inference,” Journal of Machine Learning Research, vol. 22, no. 57,
pp. 1-64, 2021.
[31]	R. Morrow and W.-C. Chiu, “Variational autoencoders with normalizing flow decoders,” arXiv preprint
arXiv:2004.05617, 2020.
[32]	B. Taylor, Methodus incrementorum directa et inversa. Londini: Typis Pearsonianis prostant apud Gul.
Innys ad Insignia Principis in Coemeterio Paulino, 1715.
[33]	C. Wang and D. M. Blei, “Variational inference in nonconjugate models.,” Journal of Machine Learning
Research, vol. 14, no. 4, 2013.
[34]	D. M. Blei, A. Kucukelbir, and J. D. McAuliffe, “Variational inference: A review for statisticians,”
Journal of the American statistical Association, vol. 112, no. 518, pp. 859-877, 2017.
[35]	S. R. Bowman, L. Vilnis, O. Vinyals, A. Dai, R. Jozefowicz, and S. Bengio, “Generating sentences from a
continuous space,” in Proceedings of The 20th SIGNLL Conference on Computational Natural Language
Learning, Association for Computational Linguistics, 2016.
[36]	C. K. S0nderby, T. Raiko, L. Maal0e, S. K. S0nderby, and O. Winther, “Ladder variational autoencoders,”
in Proc. Neural Information Processing Systems (NIPS), 2016.
[37]	A. Razavi, A. v. d. Oord, B. Poole, and O. Vinyals, “Preventing posterior collapse with delta-vaes,” in
International Conference on Learning Representations (ICLR), 2019.
[38]	M. I. Jordan, Z. Ghahramani, T. S. Jaakkola, and L. K. Saul, “An introduction to variational methods for
graphical models,” Machine learning, vol. 37, no. 2, pp. 183-233, 1999.
[39]	M. P. Deisenroth, A. A. Faisal, and C. S. Ong, Mathematics for machine learning. Cambridge University
Press, 2020.
[40]	M. A. Woodbury, “Inverting modified matrices,” Memorandum report, vol. 42, no. 106, p. 336, 1950.
[41]	D. Harville, “Matrix algebra from a statistician’s perspective springer,” New York, 1997.
[42]	D. Kingma and J. Ba, “Adam: A method for stochastic optimization,” in International Conference on
Learning Representations (ICLR), 2015.
[43]	B. M. Lake, R. Salakhutdinov, and J. B. Tenenbaum, “Human-level concept learning through probabilistic
program induction,” Science, vol. 350, no. 6266, pp. 1332-1338, 2015.
[44]	A. Paszke, S. Gross, F. Massa, A. Lerer, J. Bradbury, G. Chanan, T. Killeen, Z. Lin, N. Gimelshein,
L. Antiga, A. Desmaison, A. Kopf, E. Yang, Z. DeVito, M. Raison, A. Tejani, S. Chilamkurthy, B. Steiner,
L. Fang, J. Bai, and S. Chintala, “Pytorch: An imperative style, high-performance deep learning library,”
in Proc. Neural Information Processing Systems (NIPS), 2019, pp. 8024-8035.
11
Under review as a conference paper at ICLR 2022
A Taylor Series Visualization
Figure A.1: Example of a Taylor series expansion of a locally affine neural network at point g(x). f
(gray) is the original function / neural network, and fg(x) (orange) is the Taylor series of f at point
g(x). As g(x) is the mean of the latent normal distribution that is transformed by f, the Taylor series
at point g(x) gives a good approximation in the vicinity of g(x).
B	Implementation Details
In the following, we discuss implementation details including network architectures.
Throughout the experiments in this work, we use c = 0.5, which coincides with many public
implementations (which omit the prefactor of 蚩 / set this to 1).
B.1	CelebA and 3D Chairs
Network Architecture For the CelebA and 3D Chairs experiments, we use the architecture by
Higgins et al. [3]. The encoder consists of five convolutional layers with kernel size 4 × 4, a stride of 2,
and (32, 32, 64, 64, 256) latent dimensions with ReLU activations. Following that, a fully-connected
layer mapping to the latent space. The explicit architecture is: Conv 32x4x4 (stride 2), ReLU,
Conv 32x4x4 (stride 2), ReLU, Conv 64x4x4 (stride 2), ReLU, Conv 64x4x4
(stride 2), ReLU, Conv 256x4x4 (stride 1), ReLU, FC 256Tlatent_dim.
The decoder is the deconvolutional reverse of the encoder. In the qualitative evaluation, we use a
latent dimension of 32 for CelebA, and a latent dimension of 16 for 3D Chairs. In the quantitative
evaluation, we use the range of 12 - 64 for CelebA, and the range of 8 - 16 for 3D Chairs.
Training We trained the CelebA and 3D Chairs models for 106 iterations with the Adam opti-
mizer [42] at a learning rate of 10-4.
B.2	MNIST
Network Architecture Here, we use a fully connected network architecture. The encoder consists
of three fully connected layers with hidden dimensions (512, 256) and with ReLU activations. The
decoder is the reverse of the encoder. For the 2D traversal experiment, we use a latent dimension of 2
and for the experiments examining posterior collapse a latent dimension of 200.
Training We trained the MNIST model for the 2D traversal for 5 ∙ 104 iterations with the Adam
optimizer [42] at a learning rate of 10-4. For the experiments examining posterior collapse, we
trained for 103 iterations / epochs at full batch training on a reduced data set of 1 000 random samples
and with a learning rate of 10-3.
C Additional Results
C.1 Quantitative Evaluation for 3D Chairs
See Figure C.1 for the quantitative performance of the sampling-free VAE compared to a sampling
VAE for different latent dimensionalities.
12
Under review as a conference paper at ICLR 2022
Figure C.1: Quantitative evaluation of FID (left) and log marginal likelihood (right) on the 3D Chairs
data set for different sizes of the latent space. On FID (lower is better), the sampling-free VAE
significantly outperforms the sampling VAE and performs on par / better regarding the log marginal
likelihood (larger is better). Results average over 3 runs and minimum / maximum are marked.
C.2 Standard Deviations
Table 5: Table 2 but with standard deviations.
Method	CelebA	3D Chairs FID	log ML	FID	log ML
Sampling-Free Gaussian VAE Sampling Gaussian VAE Sampling-Free (matching budget)	95.79 ± 0.33	-7244.9 ±	0.6	51.96 ± 1.75	-7078.8 ± 0.1 99.66 ± 0.65	-7245.3 ±	0.1	53.53 ± 1.05	-7078.9 ± 0.0 98.00 ± 1.79	-7247.1 ±	0.4	54.21 ± 1.63	-7079.6 ± 0.1
C.3 Gap between unbiased ELBO and biased ELBO
Number of Iterations
Figure C.2: Gap between unbiased ELBO (Sampling ELBO) and biased ELBO (Sampling-Free
ELBO) for different γs. Both lines are of the same model, which is trained with the biased ELBO
and evaluated with both methods. Same training setting as in Figure 1.
C.4 Further Examples, Full Latent Traversals, and Comparison to Other
Methods
In Figures C.3-C.4, We show reconstructions and full latent traversals for 3D Chairs for the sampling-
free VAE, the sampling VAE, the sampling β-VAE, and an auto-encodere. In Figures C.5-C.8, we
show the same for CelebA. For the auto-encoder on the 3D Chairs data set, even among a variety of
hyper-parameters, training always converges to the plain white image.
13
Under review as a conference paper at ICLR 2022
AnaIytiCal VAE	SdmPling 0 — VAE	SdmPIing VAE	AUtO-EnCOder
	j⅛	W	⅜	⅜	■
*	.	*		.	
	rfl	ħ	›		ħ
	岛	h			h
⅜	J_	■		1	■
	Λ	*	V	Λ	,
	fl	ħ	>	肃	隔
	岛	h			h
⅜	J_	■	⅜	⅜	9
	Λ		»	.	J
	fl	ħ	>	帚	A
	岛	h	⅜		
	i	■			
	Λ	⅛			
		ħ			
	岛	h			
		,		J	ɪ
*	R	⅜	ɪ	T	T
局	4	4	T	T	T
.，		,	rR	.	.
			ɪ	T	ɪ
Jr	得			T	T
市		而	f.	fl	
⅜	,	,	A	ɪ	ɪ
	fl	禺	T	T	T
					
					
					
					,f
ɪ	⅜	⅜	,	(	⅛
T	T	T	T	T	T
.	,	.	,	.	.
	⅜	⅜			⅝
T	T	W	4	4	Ijl
	f!	Λ		4	卡
T	T	*	⅜	*	*
T	彳	T	T	#	
					
					
					
工	JL			,	■
$		⅜	⅜	、	%
了	T	T	T	JV	月
J	/	4	J	.	.
V	T	A			.
T	^τ		*	总	a
			而	∣jr	
⅜	$	.	⅜	I	J_
ħ	R	R	V	T	T
					
					
					
			■	I,	
	卜	,	⅜	*	⅝
J	J	4	T	T	T
,	.	.	.	.	.
、	⅜	⅜		A	
*	β		T	T	
	而				N
硝	ɪ	⅜	⅜		I
*	彳	T	T	R	
					
					
					
if				,	9
		⅜	⅜	⅜	‰
4	T		T	彳	.
一	,	.		⅜.	⅜1
		J		A	⅛
T	T	W	T	T	T
fl	科	国	,	用	比
H	I	B	A	⅜	
4	4	T	4	*	T
					
					
					
得	■	而		工	上
A	I	A	⅝		⅛
fl	fl	.	T		¥
,	.	,	,	.	,
⅜				A	
W	4	4	4	4	4
J	府			.	■
R	$	A	⅝	⅝	‰
f	>	T	T	■	.
					
					
					
	fl	Λ		JL	ɪ
		$	上	T	V
.	*	9	T	T	T
而	fl	高	.		
		⅜		Ir	⅜
	T	T	f	T	Y
fl		♦	.	累	¼
ft	⅜		⅜	⅛	,
fl	fl	T	T	T	T
					
					
					
f		■	fl	fl	用
	*	A	⅜	⅛	&
J		W	T	T	T
.	■	,	,	,	.
⅜	A	⅜	A	⅜	⅜
4	4	4	4	4	4
fl		Λ	fl		M
⅜	⅜		⅜	⅝	⅜
T	T	T	f	T	
					
					
					
Figure C.3: Additional reconstructions and all traversals for the 3D Chairs data set (part 1).
14
Under review as a conference paper at ICLR 2022
AnaIytiCal VAE	SdmPling 0 — VAE	SdmPIing VAE	AUtO-EnCOder
⅜	阳		*		*
M	帝	ħ		琳	h
1	料	土	⅜	R	T
.			R	.	卜
	阳		⅜		*
周	⅜	ħ			
1	料	土		R	土
		卜	R		
⅜	阳		.		*
周	⅜	ħ			h
1	料	土	⅞	R	土
		卜	R		
	阳				
	不	ħ			
1	料	土			
.		卜			
	f				
⅞	⅜	A	$	⅜	务
T	T	T	T	4	4
.	.	.	,	,	,
	⅝	⅝	A	⅜	I
4	W	4	W	W	l∙
				fl	fl
*		⅛	⅝	⅜	⅜
*	*	T	T	T	TT
					
					
					
R	R	离			工
；	⅜	⅝	⅜	⅜	⅜
m	T	T		T	
.	.	.	,	,	.
⅜	A	⅜		⅜	⅜
4	W	4	W	W	
酒			K		H
⅜	A	⅜	*	⅝	⅛
4	T	T	T		F
					
					
					
	布	M	M	用	
ɪ	T		,	_L	*
V	T	V		⅜	⅜
.	.	.			
.	⅝			、	、
.	身		I	1	I
	Y		fl		ɪ
¼	⅜	⅜	A	⅛	⅝
>	>	彳	T	T	T
					
					
					
fl			f.	同	rff
*		*	_L	V	T
fl	N	I		T	T
.	›	.	,	.	一
T	⅜	、		V	V
I		4	.	T	
■	罔	而		JL	凹
	⅜		ɪ	T	τ
9			T	y	T
					
					
					
			宿	JV	督
ɪ	T	*	A		⅛
T	T	彳	H	3	O
.	.	.	.	.	,
A		⅜	A	⅜	⅜
	4	4	4	W	
N					
V	T	ɪ	⅜		
T	T	T	4		*
					
					
					
	用		f		内
⅝	A	⅜	、		卜
~τ	<		T	T	T
.	.	,	,	,	,
	A	.			A
W	4		W	W	W
>	K		产	fl	而
⅜		.	$	⅝	⅞
9	T	T	T	T	T
					
					
					
ff			*	♦	
V	*	⅜	$	⅛	⅜
V	ɪ	T	T		rf
.		.	.	.	.
⅜		、	⅜	A	
fl	7	4	T	4	
ɪ	M	机	K		
ɪ	、		⅛		
T	T	T	4	«	
					
					
					
_fl_	fl	fl	lM	if	F
ɪ	⅜	*	*	*	W
T	T	T	彳		.
,	.	.	,	.	,
	A	⅜	A	⅝	
W	4	4	W	W	
	5	而	fl	fl	再
T	T	V	⅜	A	⅜
T	T	T	4	fl	fl
					
					
					
Figure C.4: Additional reconstructions and all traversals for the 3D Chairs data set (part 2).
15
Under review as a conference paper at ICLR 2022
Figure C.5: Reconstructions and traversals for the CelebA data set with 32 latent dimensions (part 1).
16
Under review as a conference paper at ICLR 2022
∏2∣2∣2∣
P际网依网网I • • ♦
SamPIing VAE
，旄中1S传有
■011闹周国
学 7?T?m M7 倒 GrI
-上工后吗；
Figure C.6: Reconstructions and traversals for the CelebA data set with 32 latent dimensions (part 2).
17
Under review as a conference paper at ICLR 2022
Auto-Encoder
网回网网网网
Analytical VAE
ge∏
国圈闹1―㈤脏!置网同国0
南剧剧倒Ira
SQQQBo


E阳隔003
rFGGFKIEFIFImrI 可
JlfrlF
qlrl令浦
言W，
& Sr惠

网⑸闷网网网HiPI网网网网
Q RRG 0 /;
Figure C.7: Reconstructions and traversals for the CelebA data set with 32 latent dimensions (part 3).
18
Under review as a conference paper at ICLR 2022
AQdlytlCal ∖∕Ag
,arŋpling β - V步
SgmPlIng YAE
Auto-Encoder
IB第超0』∙
㈤第耳。刃,
> ■ ■ ■ ■
m@riQ∕	雀0®FSririr¢001 直固直 ririri∣
网网阿科―浦
I可可MrI r71I I i i rI"IN,U<D<I层
Figure C.8: Reconstructions and traversals for the CelebA data set with 32 latent dimensions (part 4).
19