Under review as a conference paper at ICLR 2022
A Scaling Law for Syn2real Transfer:
How Much Is Your Pre-training Effective
Anonymous authors
Paper under double-blind review
Ab stract
Synthetic-to-real transfer learning is a framework in which a synthetically generated
dataset is used to pre-train a model to improve its performance on real vision tasks.
The most significant advantage of using synthetic images is that the ground-truth
labels are automatically available, enabling unlimited expansion of the data size
without human cost. However, synthetic data may have a huge domain gap, in
which case increasing the data size does not improve the performance. How can
we know that? In this study, we derive a simple scaling law that predicts the
performance from the amount of pre-training data. By estimating the parameters
of the law, we can judge whether we should increase the data or change the
setting of image synthesis. Further, we analyze the theory of transfer learning by
considering learning dynamics and confirm that the derived generalization bound is
consistent with our empirical findings. We empirically validated our scaling law on
various experimental settings of benchmark tasks, model sizes, and complexities of
synthetic images.
1 Introduction
The success of deep learning relies on the availability of large data. If the target task provides limited
data, the framework of transfer learning is preferably employed. A typical scenario of transfer
learning is to pre-train a model for a similar or even different task and fine-tune the model for the
target task. However, the limitation of labeled data has been the main bottleneck of supervised
pre-training. While there have been significant advances in the representation capability of the models
and computational capabilities of the hardware, the size and the diversity of the baseline dataset have
not been growing as fast (Sun et al., 2017). This is partially because of the sheer physical difficulty of
collecting large datasets from real environments (e.g., the cost of human annotation).
In computer vision, synthetic-to-real (syn2real) transfer is a promising strategy that has been attracting
attention (Su et al., 2015; Movshovitz-Attias et al., 2016; Georgakis et al., 2017; Tremblay et al.,
2018; Hinterstoisser et al., 2019; Borrego et al., 2018; Chen et al., 2021). In syn2real, images used for
pre-training are synthesized to improve the performance on real vision tasks. By combining various
conditions, such as 3D models, textures, light conditions, and camera poses, we can synthesize an
infinite number of images with ground-truth annotations. Syn2real transfer has already been applied
in some real-world applications. Teed & Deng (2021) proposed a simultaneous localization and
mapping (SLAM) system that was trained only with synthetic data and demonstrated state-of-the-art
performance. The object detection networks for autonomous driving developed by Tesla was trained
with 370 million images generated by simulation (Karpathy, 2021).
The performance of syn2real transfer depends on the similarity between synthetic and real data. In
general, the more similar they are, the stronger the effect of pre-training will be. On the contrary,
if there is a significant gap, increasing the number of synthetic data may be completely useless, in
which case we waste time and computational resources. A distinctive feature of syn2real is that we
can control the process of generating data by ourselves. If a considerable gap exists, we can try
to regenerate the data with a different setting. But how do we know that? More specifically, in a
standard learning setting without transfer, a “power law”-like relationship called a scaling law often
holds between data size and generalization errors (Rosenfeld et al., 2019; Kaplan et al., 2020). Is
there such a rule for pre-training?
1
Under review as a conference paper at ICLR 2022
pretrain task
.J0t① Is-
objdet	semseg
0.98 Illlllll IlllF
0.97	.
0.96 •	J-∙
0.95	*■-.
0.94	'	，P
0.96
0.95
0.94 •
•
0.93
0.90
0.88
0.86
0.84
0.82
103	103.5	104	104.5	103	103.5	104	104.5	103	103.5	104	104.5	103	103.5	104	104.5
# of pretrain images
coco ade20k imagenet
semseg
sinclass
finetune task
Figure 1: Empirical results of syn2real transfer for different tasks. We conducted four pre-training
tasks: object detection (objdet), semantic segmentation (semseg), multi-label classification
(mulclass), surface normal estimation (normal), and three fine-tuning tasks for benchmark
datasets: object detection for MS-COCO, semantic segmentation for ADE20K, and single-label
classification (sinclass) for ImageNet. The y-axis indicates the test error for each fine-tuning task.
Dots indicate empirical results and dashed lines indicate the fitted curves of scaling law (1). For more
details, see Section 4.2.
In this study, we find that the generalization error on fine-tuning is explained by a simple scaling law,
test error ' Dn-α + C,	(1)
where coefficient D > 0 and exponent α > 0 describe the convergence speed of pre-training, and
constant C ≥ 0 determines the lower limit of the error. We refer to α as pre-training rate and C
as transfer gap. We can predict how large the pre-training data should be to achieve the desired
accuracy by estimating the parameters α, C from the empirical results. Additionally, we analyze the
dynamics of transfer learning using the recent theoretical results based on the neural tangent kernel
(Nitanda & Suzuki, 2021) and confirm that the above law agrees with the theoretical analysis. We
empirically validated our scaling law on various experimental settings of benchmark tasks, model
sizes, and complexities of synthetic images.
Our contributions are summarized as follows.
•	From empirical results and theoretical analysis, we elicit a law that describes how generalization
scales in terms of data sizes on pre-training and fine-tuning.
•	We confirm that the derived law explains the empirical results for various settings in terms of
pre-training/fine-tuning tasks, model size, and data complexity (e.g., Figure 1). Furthermore, we
demonstrate that we can use the estimated parameters in our scaling law to assess how much
improvement we can expect from the pre-training procedure based on synthetic data.
•	We theoretically derive a generalization bound for a general transfer learning setting and confirm
its agreement with our empirical findings.
2	Related Work
Supervised pre-training for visual tasks Many empirical studies show that the performance at
a fine-tuning task scales with pre-training data (and model) size. For example, Huh et al. (2016)
studied the scaling behavior on ImageNet pre-trained models. Beyond ImageNet, Sun et al. (2017)
studied the effect of pre-training with pseudo-labeled large-scale data and found a logarithmic scaling
behavior. Similar results were observed by Kolesnikov et al. (2019).
2
Under review as a conference paper at ICLR 2022
Syn2real transfer The utility of synthetic images as supervised data for computer vision tasks
has been continuously studied by many researchers (Su et al., 2015; Movshovitz-Attias et al., 2016;
Georgakis et al., 2017; Tremblay et al., 2018; Hinterstoisser et al., 2019; Borrego et al., 2018; Chen
et al., 2021; NeWell & Deng, 2θ2θ; DeVaranjan et al., 2020; Mousavi et al., 2020; Hodan et al., 2019).
These studies found positive evidence that using synthetic images is helpful to the fine-tuning task. In
addition, they demonstrated hoW data complexity, induced by e.g., light randomization, affects the
final performance. For example, NeWell & Deng (2020) investigated hoW the recent self-supervised
methods perform Well as a pre-training task to improve the performance of doWnstream tasks. In this
paper, folloWing this line of research, We quantify the effects under the lens of the scaling laW (1).
Neural scaling laws The scaling behavior of generalization error, including some theoretical Works
(e.g., Amari et al., 1992), has been studied extensively. For modern neural netWorks, Hestness et al.
(2017) empirically observed the poWer-laW behavior of generalization for language, image, and
speech domains With respect to the training size. Rosenfeld et al. (2019) constructed a predictive form
for the poWer-laW in terms of data and model sizes. Kaplan et al. (2020) pushed forWard this direction
in the language domain, describing that the generalization of transformers obeys the poWer laW in
terms of a compute budget in addition to data and model sizes. Since then, similar scaling laWs have
been discovered in other data domains (Henighan et al., 2020). Several authors have also attempted
theoretical analysis. Hutter (2021) analyzed a simple class of models that exhibits a poWer-laW n-β
in terms of data size n With arbitrary β > 0. Bahri et al. (2021) addressed poWer laWs under four
regimes for model and data size. Note that these theoretical studies, unlike ours, are concerned With
scaling laWs in a non-transfer setting.
Hernandez et al. (2021) studied the scaling laWs for general transfer learning, Which is the most
relevant to this study. A key difference is that they focused on fine-tuning data size as a scaling
factor, While We focus on pre-training data size. Further, they found scaling laWs in terms of the
transferred effective data, Which is converted data amount necessary to achieve the same performance
gain by pre-training. In contrast, Eq. (1) explains the test error With respect to the pre-training data
size directly at a fine-tuning task. Other differences include task domains (language vs. vision) and
architectures (transformer vs. CNN).
Theory of transfer learning Theoretical analysis of transfer learning has been dated back to
decades ago (Baxter, 2000) and has been pursued extensively. Among others, some recent studies
(Maurer et al., 2016; Du et al., 2020; Tripuraneni et al., 2020) derived an error bound of a fine-tuning
task in the multi-task scenario based on complexity analysis; the bound takes an additive form
O(An-1/2 + Bs-1/2), Where n and s are the data size of pre-training and fine-tuning, respectively,
With coefficients A and B . Neural netWork regression has been also discussed With this bound
(Tripuraneni et al., 2020). In the field of domain adaptation, error bounds have been derived in
relation to the mismatch betWeen source and target input distributions (Ganin et al., 2016; Acuna
et al., 2021). They also proposed algorithms to adopt a neW data domain. HoWever, unlike in this
study, no specific learning dynamics has been taken into account. In the area of hypothesis transfer
learning (Fei-Fei et al., 2006; Yang et al., 2007), among many theoretical Works, Du et al. (2017) has
derived a risk bound for kernel ridge regression With transfer realized as the Weights on the training
samples. The obtained bound takes a similar form to our scaling laW. HoWever, the learning dynamics
of neural netWorks initialized With a pre-trained model has never been explored in this context.
3	S caling Laws for Pre-training and Fine-tuning
The main obstacle in analyzing the test error is that We have to consider interplay betWeen the effects
of pre-training and fine-tuning. Let L(n, s) ≥ 0 be the test error ofa fine-tuning task With pre-training
data size n and fine-tuning data size s. As the simplest case, consider a fine-tuning task Without
pre-training (n = 0), Which boils the transfer learning doWn to a standard learning setting. In this
case, the prior studies of both classical learning theory and neural scaling laWs tell us that the test
error decreases polynomially1 With the fine-tuning data size s, that is, L(0, s) = Bs-β + E With
decay rate β > 0 and irreducible loss E ≥ 0. The irreducible loss E is the inevitable error given by * &
1For classification With strong loW-noise condition, it is knoWn that the decay rate can be exponential (Nitanda
& Suzuki, 2019). HoWever, We focus only on the polynomial decay Without such strong condition in this paper.
3
Under review as a conference paper at ICLR 2022
the best possible mapping; it is caused by noise in continuous outputs or labels. Hereafter we assume
E = 0 for brevity.
3.1	Induction of scaling law with small empirical results
To speculate a scaling law, we conducted
preliminary experiments.2 We pre-trained
ResNet-50 by a synthetic classification task
and fine-tuned by ImageNet. Figure 2 (a)
presents the log-log plot of error curves
with respect to pre-training data size n,
where each shape and color indicates a dif-
ferent fine-tuning size s. It shows that the
pre-training effect diminishes for large n.
In contrast, Figure 2 (b) presents the rela-
tions between the error and the fine-tuning
size s with different n. It indicates the error
(a)
0.90 -・
•.
0.85 -	・
O
t
①
& 0.80 -
A
▲ ▲
0.75 -	▲
A
103	103.5	104	104.5
# of pretrain images
104∙1	104∙4	104∙7
105
105.5
106
# of finetune images
Figure 2: Scaling curves with different (a) pre-training
size and (b) fine-tuning size.
drops straight down regardless of n, confirming the power-law scaling with respect to s. The above
observations and the fact that L(0, s) decays polynomially are summarized as follows.
Requirement 1. lims→∞ L(n, s) = 0.
Requirement 2. limn→∞ L(n, s) = const.
Requirement 3. L(0, s) = Bs-β.
Requirements 1 and 3 suggest the dependency of n is embedded in the coefficient B = g(n), i.e., the
pre-training and fine-tuning effects interact multiplicatively. To satisfy Requirement 2, a reasonable
choice for the pre-training effect is g(n) = n-α + γ; the error decays polynomially with respect to n
but has a plateau at γ . By combining these, we obtain
L(n, s) =δ(γ+n-α)s-β,	(2)
where α, β > 0 are decay rates for pre-training and fine-tuning, respectively, γ ≥ 0 is a constant, and
δ > 0 is a coefficient. The exponent β determines the convergence rate with respect to fine-tuning
data size. From this viewpoint, δ(γ + n-α) is the coefficient factor to the power law. The influence
of the pre-training appears in this coefficient, where the constant term δγ comes from the irreducible
loss of the pre-training task and n-α expresses the effect of pre-training data size. The theoretical
consideration in Section E.5 suggests that the rates α and β can depend on both the target functions
of pre-training and fine-tuning as well as the learning rate.
3.2	Theoretical deduction of scaling law
Next, we analyze the fine-tuning error from a purely theoretical point of view. To incorporate the
effect of pre-training that is given as an initialization, we need to analyze the test error during the
training with a given learning algorithm such as SGD. We apply the recent development by Nitanda
& Suzuki (2021) to transfer learning. The study successfully analyzes the generalization of neural
networks in the dynamics of learning, showing it achieves minmax optimum rate. The analysis uses
the framework of the reproducing kernel Hilbert space given by the neural tangent kernel (Jacot et al.,
2018).
For theoretical analysis of transfer, it is important to formulate a task similarity between pre-training
and fine-tuning. If the tasks were totally irrelevant (e.g., learning MNIST to forecast tomorrow’s
weather), pre-training would have no benefit. Following Nitanda & Suzuki (2021), for simplicity of
analysis, we discuss only a regression problem with square loss. We assume that a vector input x and
scalar output y follow y = φ0(x) for pre-training and y = φ0(x) + φ1 (x) for fine-tuning, where we
omit the output noise for brevity; the task types are identical sharing the same input-output form, and
task similarity is controlled by φ1.
We analyze the situation where the effect of pre-training remains in the fine-tuning even for large
data size (s → ∞). More specifically, the theoretical analysis assumes a regularization term as the
2The results are replicated from Appendix C.2; see the subsection for more details.
4
Under review as a conference paper at ICLR 2022
'2-distance between the weights and the initial values, and a smaller learning rate than constant in
the fine-tuning. Hence we control how the pre-training effect is preserved through the regularization
and learning rate. Other assumptions made for theoretical analysis concern the model and learning
algorithm; a two-layer neural network having M hidden units with continuous nonlinear activation3
is adopted; for optimization, the averaged SGD (Polyak & Juditsky, 1992), an online algorithm, is
used for a technical reason.
The following is an informal statement of the theoretical result. See Appendix E for details. We
emphasize that our result holds not only for syn2real transfer but also for transfer learning in general.
Theorem 1 (Informal). Let fn,s (x) be a model of width M pre-trained by n samples
(xι,yι),..., (xn, yn) and fine-tuned by S SamPles (x； ,y1),..., (Xs,yS) where inputs x, x0 〜p(x)
are i.i.d. with the input distribution p(x) and y = φo(x) and y0 二 夕(x0) = φo(x0) + φι(x0). Then
the generalization error ofthe squared loss L(n, S) = ∣fn,s (x)一 2(x) ∣2 is boundedfrom above with
high probability as
ExL(n, S) ≤ A1 (cM + A0n-α)S-β + εM .	(3)
εM and cM can be arbitrary small for large M; A0 and A1 are constants; the exponents α and β
depend on φ0, φ1, p(x), and the learning rate of fine-tuning.
The above bound (3) shows the correspondence with the empirical derivation of the full scaling
law (2). Note that the approximation error εM is omitted in (2).
We note that the derived bound takes a multiplicative form in terms of the pre-training and fine-tuning
effects, which contrasts with the additive bounds such as An-1/2 + BS-1/2 (Tripuraneni et al., 2020).
The existing studies consider the situation where a part of a network (e.g., backbone) is frozen during
fine-tuning. Therefore, the error of pre-training is completely preserved after fine-tuning, and both
errors appear in an additive way. This means that the effect of pre-training is irreducible by the effect
of fine-tuning, and vice versa. In contrast, our analysis deals with the case of re-optimizing the entire
network in fine-tuning. In that case, the pre-trained model is used as initial values. As a result, even
if the error in pre-training is large, the final error can be reduced to zero by increasing the amount of
fine-tuning data.
3.3	Insights and Practical Values
The form of the full scaling law (2) suggests that there are two scenarios depending on whether
fine-tuning data is big or small. In “big fine-tune” regime, pre-training contributes relatively little.
By taking logarithm, we can separate the full scaling law (2) into the pre-training part u(n) =
log(n-α + γ) and the fine-tuning part v(S) = -β log S. Consider to increase n by squaring it.
Since the pre-training part cannot be reduced below log(γ) as u(n) > u(n2) > log(γ), the relative
improvement (u(n2 ) - u(n))/v(S) becomes infinitesimal for large S. Figure 2 (b) confirms this
situation. Indeed, prior studies provide the same conclusion that the gain from pre-training can easily
vanish (He et al., 2018; Newell & Deng, 2020) or a target task accuracy even degrade (Zoph et al.,
2020) if we have large enough fine-tuning data.
The above observation, however, does not mean pre-
training is futile. Dense prediction tasks such as depth
estimation require pixel-level annotations, which critically
limits the number of labeled data. Pre-training is indispens-
able in such “small fine-tune” regime. Based on this, we
hereafter analyze the case where the fine-tuning size S is
fixed. By eliminating S-dependent terms in (2), we obtain
a simplified law (1) by setting D = δS-β and C = δγS-β.
After several evaluations, these parameters including α
can be estimated by the nonlinear least squares method
(see also Section 4.1).
O
由
6
Log Pre-training Size
Target Error
As a practical benefit, the estimated parameters of the Figure 3: Pre-training scenarios.
simplified law (1) bring a way to assess syn2real transfer.
Suppose we want to solve a classification task that requires at least 90% accuracy with limited labels.
3 ReLU is not included in this class, but we can generalize this condition; see (Nitanda & Suzuki, 2021).
5
Under review as a conference paper at ICLR 2022
We generate some number of synthetic images and pre-train with them, and we obtain 70% accuracy
as Figure 3 (a). How can we achieve the required accuracy? It depends on the parameters of the
scaling law. The best scenario is (b) — transfer gap C is low and pre-training rate α is high. In
this case, increasing synthetic images eventually leads the required accuracy. In contrast, when
transfer gap C is larger than the required accuracy (c), increasing synthetic images does not help to
solve the problem. Similarly, for low pre-training rate α (d), we may have to generate tremendous
amount of synthetic images that are computationally infeasible. In the last two cases, we have to
change the rendering settings such as 3D models and light conditions to improve C and/or α, rather
than increasing the data size. The estimation of α and C requires to compute multiple fine-tuning
processes. However, the estimated parameters tell us whether we should increase data or change the
data generation process, which can reduce the total number of trials and errors.
4	Experiments
4.1	Settings
For experiments, we employed the following transfer learning protocol. First, we pre-train a model
that consists of backbone and head networks from random initialization until convergence, and we
select the best model in terms of the validation error of the pre-training task. Then, we extract the
backbone and add a new head to fine-tune all the model parameters. For notations, the task names
of object detection, semantic segmentation, multi-label classification, single-label classification,
and surface normal estimation are abbreviated as objdet, semseg, mulclass, sinclass,
and normal, respectively. The settings for transfer learning are denoted by arrows. For example,
objdet→semseg indicates that a model is pre-trained by object detection, and fine-tuned by
semantic segmentation. The experiments were conducted on an in-house cluster containing NVIDIA
V100 GPUs. The total amount of computation was approximately 1700 GPU days (200 for image
rendering, 1300 for pre-training, and 200 for fine-tuning). All the results including Figure 1 are
shown as log-log plots.
Pre-training: We prepared four tasks: mulclass, objdet, semseg, and normal. We used
ResNet-based models, where backbones were ResNet-50, unless otherwise specified, and the head
networks were customized for each task. Synthetic images for pre-training were generated by
BlenderProc (Denninger et al., 2019), an image renderer that can handle several domain randomization
methods. For rendering, We used the setting of the BOP challenge 2020 (Hodan et al., 2020) as our
default setting. We used 172 3D models, where ten objects appeared on average for each image.
We applied texture randomization for Walls and a floor, randomization for area and point lights,
and randomization for the camera. In most cases, the models Were pre-trained With 64,000 images.
We trained all models for the same fixed number of iterations depending on pre-training tasks and
selected the best models for fine-tuning, Which Were validated by another 1000 synthetic images
generated in the same Way.
Fine-tuning: We evaluated sinclass by ImageNet (Russakovsky et al., 2015), objdet by
MS-COCO (Lin et al., 2014), and semseg by ADE20K (Zhou et al., 2016). The number of images
used Was 1% of each data set (roughly, 12,000 for ImageNet, 1000 for COCO, and 200 for ADE20K).
We fine-tuned the pre-trained models With these subsets of data for a fixed number of iterations and
reported the error metrics for validation sets at the last iteration. The metrics Were top-1 accuracy
for classification, mean mAP for MS-COCO, and mean IoU for ADE20K. These metrics take their
values from 0 to 1, and We converted them into errors such as 1 - accuracy.4
Curve fitting: After obtaining the empirical errors L, We estimated the parameters of (1) by non-
linear least squares in the log-log space. We solved the minimization problem of i | log L(ni, s) -
log(Dni-α + C)|2 With a fixed fine-tuning data size s and pre-training data sizes ni = 2i × 1000
for data point index i = 0, . . . , 6. In the experiments, We empirically encountered some instability
betWeen D and α. We fixed D = 0.48 by the median values of D’s for all the settings and estimated
α and C independently for each case. We explain this procedure With more details in Appendix D.
4Although the cross-entropy loss is commonly used, several studies (Sharma & Kaplan, 2020; Bahri et al.,
2021) shoW that the scaling laWs also hold for 1 - accuracy.
6
Under review as a conference paper at ICLR 2022
4.2	Scaling law universally explains downstream performance for various
TASK COMBINATIONS
Figure 1 shows the test errors of each fine-tuning task and fitted learning curves with Eq. (1), which
describes the effect of pre-training data size n for all combinations of pre-training and fine-tuning
tasks. The scaling law fits with the empirical fine-tuning test errors with high accuracy in most cases.
4.3	Bigger models reduce the transfer gap
model -∙- ri8 r34 *
r50 + r101 -B- r152
objdet → objdet
mulclass → sinclass
0.96
0.90
0.88
0.86
0.84
0
φ
ω
φ
0.95
0.94
0.93
# of pretrain images
104 104.5
103.5	104	104.5
103 103.5
mulclass → sinclass	objdet → objdet
••
0.84	'、	∖
'、	0.935 -	■
、、▲ ∖
O 0.82	、'、	、、、
0.930 一
\ ▲ \
0.80	■
区 0.925 -
X
`	因
20	30	50	20	30	50
# of parameters (Million)
Figure 4: Effect of model size. Best viewed in color. Left: The scaling curves for
mulclass→sinclass and objdet→objdet cases. The meanings of dots and lines are the
same as those in Figure 1. Right: The estimated transfer gap C (y-axis) versus the model size (x-axis)
in log-log scale. The dots are estimated values, and the lines are linear fittings of them.
0.82
103
We compared several ResNet models as backbones in mulclass→sinclass and
objdet→objdet to observe the effects of model size. Figure 4 (left) shows the curves of
scaling laws for the pre-training data size n for different sizes of backbone ResNet-x, where
x ∈ {18, 34, 50, 101, 152}. The bigger models attain smaller test errors. Figure 4 (right) shows
the values of the estimated transfer gap C. The results suggest that there is a roughly power-law
relationship between the transfer gap and model size. This agrees with the scaling law with respect to
the model size shown by Hernandez et al. (2021).
4.4	Scaling law can extrapolate for more pre-training images
fitting
---powerlaw
---- proposed
model
T- r50
r101
r152
Figure 5: Ability to extrapolate. Left: The solid lines represent the fitted power law and the dashed
curves represent the fitted scaling law (1), in which the laws were fitted using the empirical errors
where the pre-training size n was less than 64,000 (the first five dots). The vertical dashed line
indicates where n = 64,000. Right: The root-mean-square errors between the laws and the actual
test errors in the area of extrapolation (the last five dots).
model	powerlaw	proposed
r50	0.0099	0.0024
r101	0.0074	0.0016
r152	0.0091	0.0017
7
Under review as a conference paper at ICLR 2022
We also evaluated the extrapolation ability of the scaling law. We increased the number of synthetic
images from the original size (n = 64,000) to 1.28 million, and see how the fitted scaling law predicts
the unseen test errors where n > 64,000. As a baseline, we compared the power-law model, which
is equivalent to the derived scaling law (1) with C = 0. Figure 5 (left) shows the extrapolation
results in objdet→objdet setting, which indicates the scaling law follows the saturating trend
in regions with large pre-training sizes for all models, while the power-law model fails to capture it.
The prediction errors is numerically shown in Figure 5 (right), which again shows our scaling law
achieves better prediction performance.
4.5	Data complexity affects both pre-training rate and transfer gap
objdet → objdet
1000	3000	10000	30000
# of pretrain images
appearance
→- multiple
-single
light
—fix
---random
background
→- fix
→- random
object
5000
4500
4000
3500
0.8
0.7
0.6
0.5
0.96
0.95
0.94
+
obj texture	w/	w/o	w/	w/o	w/	w/o	w/	w/o
appear single	single	multiple	multiple	multiple	multiple	multiple	multiple
light	fix	fix	fix	fix	random	random	random	random
background fix	fix	fix	fix	fix	fix	random	random
• w/ texture
+ w/o texture
Figure 6: Effect of synthetic image complexity. Best viewed in color. Left: Scaling curves of
different data complexities. Right: Estimated parameters. The error bars represent the standard error
of the estimate in least squares.
We examined how the complexity of synthetic images affects fine-tuning performance. We controlled
the following four rendering parameters: Appearance: Number of objects in each image; single
or multiple (max 10 objects). Light: Either an area and point light is randomized or fixed in
terms of height, color, and intensity. Background: Either the textures of floor/wall are randomized
or fixed. Object texture: Either the 3D objects used for rendering contain texture (w/) or not (w/o).
Indeed, the data complexity satisfies the following ordered relationships: single < multiple in
appearance, fix < random in light and background, and w/o < w/ in object texture5. To quantify
the complexity, we computed the negative entropy of the Gaussian distribution fitted to the last
activation values of the backbone network. For this purpose, we pre-trained ResNet-50 as a backbone
with MS-COCO for 48 epochs and computed the empirical covariance of the last activations for all
the synthetic data sets.
The estimated parameters are shown in Figure 6, which indicates the following (we discuss the
implications of these results further in Section 5.1).
•	Data complexity controlled by the rendering settings correlates with the negative entropy, implying
the negative entropy expresses the actual complexity of pre-training data.
•	Pre-training rate α correlates with data complexity. The larger complexity causes slower rates of
convergence with respect to the pre-training data size.
•	Transfer gap C mostly correlates negatively with data complexity, but not for object texture.
As discussed in Section 4.1, we have fixed the value of D to avoid numerical instability, which might
cause some bias to the estimates of α. We postulate, however, the value of D depends mainly on
the fine-tuning task and thus has a fixed value for different pre-training data complexities. This can
be inferred from the theoretical analysis in Appendix E.5: the exponent β in the main factor s-β
of D does not depend on the pre-training data distribution but only on the fine-tuning task or the
pre-training true mapping. Thus, the values of D should be similar over the different complexities,
and the correlation of α preserves.
5The object category of w/o is a subset of w/, and w/ has a strictly higher complexity than w/o.
8
Under review as a conference paper at ICLR 2022
5 Conclusion and Discussion
In this paper, we studied how the performance on syn2real transfer depends on pre-training and
fine-tuning data sizes. Based on the experimental results, we found a scaling law (1) and its
generalization (2) that explain the scaling behavior in various settings in terms of pre-training/fine-
tuning tasks, model sizes, and data complexities. Further, we present the theoretical error bound for
transfer learning and found our theoretical bound has a good agreement with the scaling law.
5.1	Implication of complexity results in Section 4.5
The results of Section 4.5 has two implications. First, data complexity (i.e., the diversity of images)
largely affects the pre-training rate α. This is reasonable because if we want a network to recognize
more diverse images, we need to train it with more examples. Indeed, prior studies (Sharma &
Kaplan, 2020; Bahri et al., 2021) observed that α is inversely proportional to the intrinsic dimension
of the data (e.g., dimension of the data manifold), which is an equivalent concept of data complexity.
Second, the estimated values of the transfer gap C suggest that increasing the complexity of data is
generally beneficial to decrease C, but not always. Figure 6 (right) shows that increasing complexities
in terms of appearance, light, and background reduces the transfer gap, which implies that these
rendering operations are most effective to cover the fine-tuning task that uses real images. However,
the additional complexity in object texture works negatively. We suspect that this occurred because of
shortcut learning (Geirhos et al., 2020). Namely, adding textures to objects makes the recognition
problem falsely easier because we can identify objects by textures rather than shapes. Because CNNs
prefer to recognize objects by textures (Geirhos et al., 2018; Hermann et al., 2019), the pre-trained
models may overfit to learn the texture features. Without object textures, pre-trained models have
to learn the shape features because there is no other clue to distinguish the objects, and the learned
features will be useful for real tasks.
5.2	Lessons to transfer learning and synthetic-to-real generalization
Our results suggest the transfer gap C is the most crucial factor for successful transfer learning
because C determines the maximum utility of pre-training. Large-scale pre-training data can be
useless when C is large. In contrast, if C is negligibly small, the law is reduced essentially to n-α,
which tells that the volume of pre-training data is directly exchanged to the performance of fine-tuning
tasks. Our empirical results suggest two strategies for reducing C: 1) Use bigger models and 2) fill
the domain gap in terms of the decision rule and image distribution. For the latter, existing techniques
such as domain randomization (Tobin et al., 2017) would be helpful.
5.3	Limitations of this study
•	In the experiments, the scale of data is relatively limited (million-scale, not billion).
•	We only examined ResNet as a network architecture (no Transformers).
•	Although there are various visual tasks, our study only covers a few of them. Extending our
observations to other visual tasks such as depth estimation, instance segmentation, and keypoint
detection, as well as to other data domains such as language is future work.
•	The theoretical results assume several conditions that may contradict the actual setting in the
experiments. For example, our theory relies on ASGD instead of vanilla SGD. Also, the task types
are assumed to be identical for the pre-training and fine-tuning tasks.
•	In this study, we focus on finding a general rule of transfer learning, rather than improving absolute
performance on specific tasks. We used popular vision tasks such as classification and ready-made
rendering settings that is not designed to pre-train for the tasks. We expect to observe more
performance gain with other syn2real-friendly tasks such as optical flow and elaborate rendering
settings in future work.
References
David Acuna, Guojun Zhang, Marc T Law, and Sanja Fidler. f-domain-adversarial learning: Theory
and algorithms. arXiv preprint arXiv:2106.11344, 2021.
9
Under review as a conference paper at ICLR 2022
Zeyuan Allen-Zhu, Yuanzhi Li, and Yingyu Liang. Learning and generalization in overparameterized
neural networks, going beyond two layers. CoRR, abs/1811.04918, 2018. URL http://arxiv.
org/abs/1811.04918.
Shun-ichi Amari, Naotake Fujita, and Shigeru Shinomoto. Four types of learning curves. Neural
Computation, 4(4):605-618,1992.
Sanjeev Arora, Simon Du, Wei Hu, Zhiyuan Li, and Ruosong Wang. Fine-grained analysis of
optimization and generalization for overparameterized two-layer neural networks. In Proceedings
of the 36th International Conference on Machine Learning, volume 97, pp. 322-332, 2019.
Yasaman Bahri, Ethan Dyer, Jared Kaplan, Jaehoon Lee, and Utkarsh Sharma. Explaining neural
scaling laws. arXiv preprint arXiv:2102.06701, 2021.
Peter L Bartlett, Dylan J Foster, and Matus J Telgarsky. Spectrally-normalized margin bounds for
neural networks. In Advances in Neural Information Processing Systems, volume 30. Curran
Associates, Inc., 2017.
Jonathan Baxter. A model of inductive bias learning. Journal of artificial intelligence research, 12:
149-198, 2000.
Daniel Bolya, Chong Zhou, Fanyi Xiao, and Yong Jae Lee. Yolact: Real-time instance segmentation.
In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 9157-9166,
2019.
Joao Borrego, Atabak Dehban, RUi Figueiredo, Plinio Moreno, Alexandre Bernardino, and Jose
Santos-Victor. Applying domain randomization to synthetic data for object category detection.
arXiv preprint arXiv:1807.09834, 2018.
A. Caponnetto and E. De Vito. Optimal rates for regularized least-squares algorithm. Foundations of
Computational Mathematics, 7(3):331-368, 2007.
Liang-Chieh Chen, George Papandreou, Florian Schroff, and Hartwig Adam. Rethinking atrous
convolution for semantic image segmentation. arXiv preprint arXiv:1706.05587, 2017.
Wuyang Chen, Zhiding Yu, Shalini De Mello, Sifei Liu, Jose M. Alvarez, Zhangyang Wang, and
Anima Anandkumar. Contrastive syn-to-real generalization. arXiv preprint arXiv:2104.02290,
2021.
Maximilian Denninger, Martin Sundermeyer, Dominik Winkelbauer, Youssef Zidan, Dmitry Ole-
fir, Mohamad Elbadrawy, Ahsan Lodhi, and Harinandan Katam. Blenderproc. arXiv preprint
arXiv:1911.01911, 2019.
Jeevan Devaranjan, Amlan Kar, and Sanja Fidler. Meta-sim2: Unsupervised learning of scene
structure for synthetic data generation. In European Conference on Computer Vision, pp. 715-733.
Springer, 2020.
Simon Du, Jason Lee, Haochuan Li, Liwei Wang, and Xiyu Zhai. Gradient Descent Finds Global
Minima of Deep Neural Networks. In Proceedings of the 36th International Conference on
Machine Learning, volume 97, pp. 1675-1685, 2019.
Simon S Du, Jayanth Koushik, Aarti Singh, and Barnabas Poczos. Hypothesis Transfer Learning
via Transformation Functions. In I Guyon, U V Luxburg, S Bengio, H Wallach, R Fergus,
S Vishwanathan, and R Garnett (eds.), Advances in Neural Information Processing Systems,
volume 30. Curran Associates, Inc., 2017. URL https://proceedings.neurips.cc/
paper/2017/file/352fe25daf686bdb4edca223c921acea-Paper.pdf.
Simon S. Du, Wei Hu, Sham M. Kakade, Jason D. Lee, and Qi Lei. Few-shot learning via learning
the representation, provably. arXiv preprint arXiv:2002.09434, 2020.
Li Fei-Fei, R Fergus, and P Perona. One-shot learning of object categories. IEEE Transactions on
Pattern Analysis and Machine Intelligence, 28(4):594-611, 2006. doi: 10.1109/TPAMI.2006.79.
10
Under review as a conference paper at ICLR 2022
Yaroslav Ganin, Evgeniya Ustinova, Hana Ajakan, Pascal Germain, Hugo Larochelle, Frangois
Laviolette, Mario Marchand, and Victor Lempitsky. Domain-adversarial training of neural networks.
The journaIofmachine learning research,17(1):2096-2030, 2016.
Robert Geirhos, Patricia Rubisch, Claudio Michaelis, Matthias Bethge, Felix A. Wichmann, and
Wieland Brendel. Imagenet-trained cnns are biased towards texture; increasing shape bias improves
accuracy and robustness. arXiv preprint arXiv:1811.12231, 2018.
Robert Geirhos, Jorn-Henrik Jacobsen, Claudio Michaelis, Richard Zemel, Wieland Brendel, Matthias
Bethge, and Felix A Wichmann. Shortcut learning in deep neural networks. Nature Machine
Intelligence, 2(11):665-673, 2020.
Georgios Georgakis, Arsalan Mousavian, Alexander C. Berg, and Jana Kosecka. Synthesizing
training data for object detection in indoor scenes. arXiv preprint arXiv:1702.07836, 2017.
Priya Goyal, Piotr Dolldr, Ross Girshick, Pieter Noordhuis, Lukasz Wesolowski, Aapo Kyrola,
Andrew Tulloch, Yangqing Jia, and Kaiming He. Accurate, large minibatch SGD: Training
ImageNet in 1 hour. arXiv preprint arXiv:1706.02677, 2017.
Kaiming He, Ross Girshick, and Piotr Dolldr. Rethinking imagenet pre-training. arXiv preprint
arXiv:1811.08883, 2018.
Tom Henighan, Jared Kaplan, Mor Katz, Mark Chen, Christopher Hesse, Jacob Jackson, Heewoo
Jun, Tom B. Brown, Prafulla Dhariwal, Scott Gray, Chris Hallacy, Benjamin Mann, Alec Radford,
Aditya Ramesh, Nick Ryder, Daniel M. Ziegler, John Schulman, Dario Amodei, and Sam Mc-
Candlish. Scaling laws for autoregressive generative modeling. arXiv preprint arXiv:2010.14701,
2020.
Katherine L. Hermann, Ting Chen, and Simon Kornblith. The origins and prevalence of texture bias
in convolutional neural networks. arXiv preprint arXiv:1911.09071, 2019.
Danny Hernandez, Jared Kaplan, Tom Henighan, and Sam McCandlish. Scaling laws for transfer.
arXiv preprint arXiv:2102.01293, 2021.
Joel Hestness, Sharan Narang, Newsha Ardalani, Gregory Diamos, Heewoo Jun, Hassan Kianinejad,
Md. Mostofa Ali Patwary, Yang Yang, and Yanqi Zhou. Deep learning scaling is predictable,
empirically. arXiv preprint arXiv:1712.00409, 2017.
Stefan Hinterstoisser, Olivier Pauly, Hauke Heibel, Martina Marek, and Martin Bokeloh. An
annotation saved is an annotation earned: Using fully synthetic training for object instance
detection. arXiv preprint arXiv:1902.09967, 2019.
Tomds Hodan, Vibhav Vineet, Ran Gal, Emanuel Shalev, Jon Hanzelka, Treb Connell, Pedro Urbina,
Sudipta N Sinha, and Brian Guenter. Photorealistic image synthesis for object instance detection.
In 2019 IEEE International Conference on Image Processing (ICIP), pp. 66-70. IEEE, 2019.
Tomds Hodan, Martin Sundermeyer, Bertram Drost, Yann Labb6, Eric Brachmann, Frank Michel,
Carsten Rother, and Jifi Matas. BOP challenge 2020 on 6D object localization. European
Conference on Computer Vision Workshops (ECCVW), 2020.
Minyoung Huh, Pulkit Agrawal, and Alexei A. Efros. What makes imagenet good for transfer
learning? arXiv preprint arXiv:1608.08614, 2016.
Marcus Hutter. Learning curve theory. arXiv preprint arXiv:2102.04074, 2021.
Arthur Jacot, Franck Gabriel, and Clement Hongler. Neural tangent kernel: Convergence and
generalization in neural networks. In Advances in Neural Information Processing Systems 31, pp.
8571-8580. Curran Associates, Inc., 2018.
Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B. Brown, Benjamin Chess, Rewon Child,
Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. Scaling laws for neural language models.
arXiv preprint arXiv:2001.08361, 2020.
11
Under review as a conference paper at ICLR 2022
Andrej Karpathy. Tesla ai day. https://www.youtube.com/watch?v=j0z4FweCy4M,
2021.
James Kirkpatrick, Razvan Pascanu, Neil Rabinowitz, Joel Veness, Guillaume Desjardins, An-
drei A Rusu, Kieran Milan, John Quan, Tiago Ramalho, Agnieszka Grabska-Barwinska, Demis
Hassabis, Claudia Clopath, Dharshan Kumaran, and Raia Hadsell. Overcoming catastrophic
forgetting in neural networks. Proceedings of the National Academy of Sciences of the
United States of America, 114(13):3521-3526, mar 2017. ISSN 1091-6490. doi: 10.1073/
pnas.1611835114. URL http://www.ncbi.nlm.nih.gov/pubmed/28292907http:
//www.pubmedcentral.nih.gov/articlerender.fcgi?artid=PMC5380101.
Alexander Kolesnikov, Lucas Beyer, Xiaohua Zhai, Joan Puigcerver, Jessica Yung, Sylvain Gelly,
and Neil Houlsby. Big transfer (bit): General visual representation learning. arXiv preprint
arXiv:1912.11370, 2019.
Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr
Dolldr, and C LaWrence Zitnick. Microsoft coco: Common objects in context. In European
conference on computer vision, pp. 740-755. Springer, 2014.
Tsung-Yi Lin, Piotr Dolldr, Ross Girshick, Kaiming He, Bharath Hariharan, and Serge Belongie.
Feature pyramid netWorks for object detection. In Proceedings of the IEEE conference on computer
vision and pattern recognition, pp. 2117-2125, 2017.
Jonathan Long, Evan Shelhamer, and Trevor Darrell. Fully convolutional netWorks for semantic
segmentation. In Proceedings of the IEEE conference on computer vision and pattern recognition,
pp. 3431-3440, 2015.
Andreas Maurer, Massimiliano Pontil, and Bernardino Romera-Paredes. The benefit of multitask
representation learning. Journal of Machine Learning Research, 17(81):1-32, 2016. URL
http://jmlr.org/papers/v17/15-242.html.
Mehdi Mousavi, Aashis Khanal, and Rolando Estrada. Ai playground: Unreal engine-based data
ablation tool for deep learning. In International Symposium on Visual Computing, pp. 518-532.
Springer, 2020.
Yair Movshovitz-Attias, Takeo Kanade, and Yaser Sheikh. HoW useful is photo-realistic rendering
for visual learning? arXiv preprint arXiv:1603.08152, 2016.
Alejandro NeWell and Jia Deng. HoW useful is self-supervised pretraining for visual tasks? arXiv
preprint arXiv:2003.14323, 2020.
Behnam Neyshabur, Ryota Tomioka, and Nathan Srebro. Norm-based capacity control in neural
netWorks. In Proceedings of The 28th Conference on Learning Theory, pp. 1376-1401, 2015.
Behnam Neyshabur, Srinadh Bhojanapalli, David McAllester, and Nathan Srebro. Exploring Gen-
eralization in Deep Learning. In Advances in Neural Information Processing Systems 30, pp.
5947-5956, 2017.
Atsushi Nitanda and Taiji Suzuki. Stochastic gradient descent With exponential convergence rates of
expected classification errors. In Proceedings of the Twenty-Second International Conference on
Artificial Intelligence and Statistics, volume 89 of Proceedings of Machine Learning Research, pp.
1417-1426, 2019.
Atsushi Nitanda and Taiji Suzuki. Optimal rates for averaged stochastic gradient descent under neural
tangent kernel regime. In International Conference on Learning Representations, 2021.
Atsushi Nitanda, Geoffrey Chinot, and Taiji Suzuki. Gradient descent can learn less over-
parameterized tWo-layer neural netWorks on classification problems, 2020.
Boris T Polyak and Anatoli B Juditsky. Acceleration of stochastic approximation by averaging. SIAM
journal on control and optimization, 30(4):838-855, 1992.
12
Under review as a conference paper at ICLR 2022
Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun. Faster r-cnn: Towards real-time object
detection with region proposal networks. IEEE transactions on pattern analysis and machine
intelligence, 39(6):1137-1149, 2016.
Jonathan S Rosenfeld, Amir Rosenfeld, Yonatan Belinkov, and Nir Shavit. A constructive prediction
of the generalization error across scales. arXiv preprint arXiv:1909.12673, 2019.
Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang,
Andrej Karpathy, Aditya Khosla, Michael Bernstein, Alexander C. Berg, and Li Fei-Fei. ImageNet
Large Scale Visual Recognition Challenge. International Journal of Computer Vision (IJCV), 115
(3):211-252, 2015. doi: 10.1007/s11263-015-0816-y.
Utkarsh Sharma and Jared Kaplan. A neural scaling law from the dimension of the data manifold.
arXiv preprint arXiv:2004.10802, 2020.
Hao Su, Charles R Qi, Yangyan Li, and Leonidas J Guibas. Render for cnn: Viewpoint estimation in
images using cnns trained with rendered 3d model views. In Proceedings of the IEEE International
Conference on Computer Vision, pp. 2686-2694, 2015.
Chen Sun, Abhinav Shrivastava, Saurabh Singh, and Abhinav Gupta. Revisiting unreasonable
effectiveness of data in deep learning era. In Proceedings of the IEEE international conference on
computer vision, pp. 843-852, 2017.
Taiji Suzuki. Fast generalization error bound of deep learning from a kernel perspective. In
Proceedings of the Twenty-First International Conference on Artificial Intelligence and Statistics,
volume 84, pp. 1397-1406, 2018.
Zachary Teed and Jia Deng. Droid-slam: Deep visual slam for monocular, stereo, and rgb-d cameras.
arXiv preprint arXiv:2108.10869, 2021.
Josh Tobin, Rachel Fong, Alex Ray, Jonas Schneider, Wojciech Zaremba, and Pieter Abbeel. Domain
randomization for transferring deep neural networks from simulation to the real world. arXiv
preprint arXiv:1703.06907, 2017.
Jonathan Tremblay, Aayush Prakash, David Acuna, Mark Brophy, Varun Jampani, Cem Anil, Thang
To, Eric Cameracci, Shaad Boochoon, and Stan Birchfield. Training deep networks with synthetic
data: Bridging the reality gap by domain randomization. arXiv preprint arXiv:1804.06516, 2018.
Nilesh Tripuraneni, Michael I. Jordan, and Chi Jin. On the theory of transfer learning: The importance
of task diversity. arXiv preprint arXiv:2006.11650, 2020.
Colin Wei and Tengyu Ma. Improved Sample Complexities for Deep Neural Networks and Robust
Classification via an All-Layer Margin. In International Conference on Learning Representations,
2020. URL https://openreview.net/forum?id=HJe_yR4Fwr.
Jun Yang, Rong Yan, and Alexander G Hauptmann. Cross-Domain Video Concept Detection
Using Adaptive Svms. In Proceedings of the 15th ACM International Conference on Multimedia,
MM ’07, pp. 188-197, New York, NY, USA, 2007. Association for Computing Machinery.
ISBN 9781595937025. doi: 10.1145/1291233.1291276. URL https://doi.org/10.1145/
1291233.1291276.
Bolei Zhou, Hang Zhao, Xavier Puig, Tete Xiao, Sanja Fidler, Adela Barriuso, and Antonio Torralba.
Semantic understanding of scenes through the ade20k dataset. arXiv preprint arXiv:1608.05442,
2016.
Bolei Zhou, Hang Zhao, Xavier Puig, Sanja Fidler, Adela Barriuso, and Antonio Torralba. Scene
parsing through ade20k dataset. In Proceedings of the IEEE conference on computer vision and
pattern recognition, pp. 633-641, 2017.
Barret Zoph, Golnaz Ghiasi, Tsung-Yi Lin, Yin Cui, Hanxiao Liu, Ekin D. Cubuk, and Quoc V. Le.
Rethinking pre-training and self-training. arXiv preprint arXiv:2006.06882, 2020.
13
Under review as a conference paper at ICLR 2022
Appendix
A	Training Details
A.1 Object detection
We used Faster-RCNN (Ren et al., 2016) with FPN (Lin et al., 2017) as object detection models and
ResNet (Goyal et al., 2017) as a backbone network of Faster-RCNN.
We used the following training procedure: We trained the model using momentum SGD of momentum
0.9 with weight decay of 10-4. The global batch size was set to 64 when training ResNet18, ResNet34,
ResNet50, and ResNet101. The batch size was set to 32 when training ResNet152 to avoid out-of-
memory errors. The batch statistics in batch normalization layers were computed across all GPUs.
We used a base image size of 640 × 640 in the same way as YOLACT training (Bolya et al., 2019).
We used mixed16 training to reduce the memory footprint. We also adopted random horizontal
flipping as data augmentation to images. The learning rate was set to 0.02, and we used the cosine
decay with a warmup scheme. The warmup length is 120,000 images (3,750 iterations for ResNet152
and 1,875 iterations for other models). As for evaluation, we followed the standard settings in COCO
dataset (Ren et al., 2016).
We pre-trained the model with 14,400,000 images (450,000 iterations for ResNet152 and 225,000
iterations for other models). We used the models that achieved the best mmAP as the initial value of
fine-tuning.
We used COCO (Lin et al., 2014) as the fine-tuning dataset. We trained the model with 1,440,000
images (45,000 iterations for ResNet152 and 22,500 iterations for other models) during fine-tuning.
A.2 Semantic segmentation
We used DeepLabV3 (Chen et al., 2017) with the softmax cross-entropy loss as the semantic
segmentation model and ResNet50 (Goyal et al., 2017) as its backbone. The model configuration
follows the implementation in torchvision6. It should be noted that DeepLabV3 requires dilated
ResNet as the backbone, which is not the case in object detection and classification tasks. Even though,
the shapes of weight tensors of dilated ResNet50 exactly match those of non-dilated ResNet50; thus
we can use the pre-trained weights of dilated and non-dilated ResNet50 interchangeably.
The learning procedure is based on the reference implementation7 of torchvision. We added an
auxiliary branch based on FCN (Long et al., 2015) which takes conv4 of the backbone as the input.
In the computation of loss function, the loss for the auxiliary branch is computed in the same way as
for the main branch and is added to the overall loss after multiplying by the factor 0.5. The model
was trained using momentum SGD of momentum 0.9 with weight decay of 10-4. The global batch
size was set to 32. The batch statistics in batch normalization layers were computed across all GPUs.
During training, images were first resized so that the length of the shorter edge becomes an integer
uniformly chosen from [520 × 0.5, 520 × 2], then horizontally flipped with probability 0.5, finally
randomly cropped to 480 × 480. The learning rate (LR) was decayed according to the polynomial
LR schedule of rate 0.9 and initial LR of 0.02. For the parameters of the auxiliary classifier, the LR
was multiplied by 10. The evaluation was performed once every 3,125 iterations (almost equivalent
to 5 epochs in full ADE20K). In the evaluation, images were resized so that the length of the shorter
edge becomes 520.
In pre-training, we trained 125,000 iterations which roughly equals 200 epochs in full ADE20K
(Zhou et al., 2017). We used the model that achieved the best mIoU as the initial value of fine-tuning.
We pre-trained models using our synthetic datasets. When training with them, backgrounds (points at
which no foreground objects were present) were also considered to be a separate class in semantic
segmentation.
6https://github.com/pytorch/vision
7https://github.com/pytorch/vision/tree/master/references/segmentation
14
Under review as a conference paper at ICLR 2022
We used the ADE20K (Zhou et al., 2017) datasets as the fine-tuning target. In fine-tuning, we trained
the model for 18,750 iterations, which correspond to 30 epochs of full ADE20K. The metric was
mIoU score.
A.3 Multi-label classification
We used ResNet (Goyal et al., 2017) with binary cross-entropy used as the loss function in multi-label
classification. We used the following training procedure: We trained the model using momentum
SGD of momentum 0.9 with weight decay of 10-4. The batch size was set as 32 per GPU, thus
256 in total. We trained the models for 112,500 iterations. The input size of the images was simply
resized to 640 × 640. We adopted random horizontal flipping as data augmentation to images. The
learning rate was set to 0.1. The cosine decay with a warmup scheme was used. The warmup length
was 120,000 images. The evaluation was performed once every 120,000 images. In the evaluation,
the image size was the same as that used during training, and data augmentation was not used. We
used the mAP score as the metric.
A.4 Single-label classification
As in multi-label classification, we used ResNet. The softmax cross-entropy was used as the loss
function of the single-label classification. The learning procedure is based on (Goyal et al., 2017).
However, we used cosine decay for the learning rate scheduling.
A.5 Surface normal estimation
As in semantic segmentation, we used DeepLabV3 (Chen et al., 2017) as the model for surface
normal estimation. The model configuration and the training procedure were exactly the same as in
semantic segmentation, except for the following changes:
•	Dimension of output channels was changed to 3, each of which corresponds to the 3 axes of the
normal vector,
•	Initial LR was changed to 0.04,
•	Length of pre-training was 200,000 iterations, which corresponds to 100 epochs in our synthetic
dataset,
•	Random flipping was not performed during the data augmentation, and
•	Loss function was the average of the value, 1.0 - n ∙ n, which was computed for each valid pixel
where n is the ground-truth normal vector and n is the model output (after L2 normalization).
B Synthetic Data Details
The data generation strategy is based on the “on surface sampling” setting in the BoP challenge
dataset8. In this setting, the sampled objects will be spawned in a cube-shaped room with one point
light and one surface light. As the objects to be spawn, we used all the BoP object sets, i.e., LM,
T-LESS, ITODD, HB, YCB-V, RU-APC, IC-BIN, IC-MI, TUD-L, and TYO-L.9. There are 173
objects in total. After generated a random scene (position of objects, lights, etc), we took 10 pictures
by 10 different camera poses. This means that, if we have 10K images in total, there are 1K unique
scenes, and 9K images are inflated by just changing the camera angle and position.
To control the data complexity, we selected four attributes in the generation strategy and prepared
two options for each.
Appearance controls how many objects are generated in a room in a single scene. For each scene,
we randomly select ten objects for the multiple setting and one object for the single setting.
Light controls the light sources. In the random setting, the color, height, and strength of lights are
randomized. In contrast, in the fix setting, they are all fixed.
8https://github.com/DLR-RM/BlenderProc/tree/main/examples/bop_challenge
9https://bop.felk.cvut.cz/datasets/
15
Under review as a conference paper at ICLR 2022
Background controls the texture of the room, i.e., floor and walls. In the random setting, we assign
a random PBR material from the CC0 Textures10 library, and we selected one carpet texture
for the fix setting.
Object texture controls the object set to be used. The BoP object set is consists of several types
of object sets as described above. Among them, T-LESS and ITODD consist of industry-
relevant objects and they do not have textures and colors.11 For the w/o setting, we only
used such texture-less objects to be sampled. In contrast, we sample all the 173 objects
include T-LESS and ITODD in the w/ setting.
We generated eight variations of datasets by changing these attributes, which were used in the
experiments of Section 4.5. Figure 7 shows the example of generated images with the value of each
attribute.
10https://ambientcg.com
11T-Less and ITODD contain 30 and 28 objects, respectively.
16
Under review as a conference paper at ICLR 2022
object texture:
appearance:
light:
background:
w/
multiple
random
random
object texture:
appearance:
light:
background:
w/o
multiple
random
random
object texture:
appearance:
light:
background:
w/
multiple
random
fix
object texture:
appearance:
light:
background:
w/
multiple
fix
fix
object texture:
appearance:
light:
background:
w/o
multiple
random
fix
object texture:
appearance:
light:
background:
w/o
multiple
fix
fix
object texture:
appearance:
light:
background:
w/
single
fix
fix
object texture:
appearance:
light:
background:
w/o
single
fix
fix
Figure 7: Example of generated datasets
17
Under review as a conference paper at ICLR 2022
0.7
rorre tse
0.3
pretrain task
semseg	mulclass	normal
Figure 8: The estimated values of the pre-training rate α and the transfer gap C in the cross-task
setting (as the same as Figure 1). The error bars present the standard error of the estimates in the
least squares.
→- 2 -∙- 7 →- 32
finetune ratio (%)
-∙- 4 -∙- 16 →- 64
」0」」①ISs
∙- 1 -∙- 4 →- 16 -∙- 64
pretrain size (K)
-∙- 2 -∙- 8 →- 32
0.7
0.5
0.3
104.5	105	105.5
# of finetune images
103	103.5	104	104.5
# of pretrain images
Figure 9: Empirical and fitting results for various pre-train and fine-tune data sizes in
mulclass→sinclass. All curves are fitted using the full law (2). Best viewed in color. Left:
Effect of pre-training data size (x-axis) for fixed fine-tuning data sizes. Right: Effect of fine-tuning
data size (x-axis) for fixed pre-training data sizes.
C Additional Experiments
C.1 Estimated parameters in the cross-task setting
Figure 8 shows the estimated parameters (α, C) at the experiments described in Section 4.2. Note
that the result of α at normal→semseg is omitted because its estimated value is highly unstable
(the standard deviation is larger than 1).
C.2 Full scaling law collectively relates pre-training and fine-tuning data
SIZE
Next, we verify the validity of the full scaling law (2). In the mulclass→sinclass setting
with ResNet-50, we changed the fine-tuning data size from 2% to 64% of the ImageNet.12 We then
fitted all results by a single equation (2) to estimate the parameters except the irreducible loss E
12ImageNet contains a class imbalance problem. If we use 100% of the ImageNet, we cannot provide the
same sample size per class. To eliminate the effect of class imbalance, we made the sampling ratio to keep the
balance up to 64% and excluded the case of 100%.
18
Under review as a conference paper at ICLR 2022
(we assumed E = 0 from the preliminary results in Figure 2). The results in Figure 9 show that all
empirical test errors are explained remarkably well by Eq. (2), which has only four parameters to fit
in this case. The estimated parameters are α = 0.544, β = 0.322, γ = 0.478, and δ = 41.8.
C.3 Linearized results
The transfer gap C in (1) causes a plateau of the scaling law. Conversely, if we subtract the estimated
C from the results, we must be able to recover the power-law scaling. To confirm this, we subtracted
the estimated C from the empirical errors L of the previous results. Figures 10-12 show the modified
version of scaling law fittings. Overall, the empirical errors behave linearly along with the estimated
power-law term Dn-α. Note that, in mulclass→semseg and normal→semseg, a few points
of L become negative after subtracting C, and these points are not depicted.
pretrain task
objdet	semseg	mulclass	normal
0.030 ♦一
•	•	(
0.010	.〜∙~.	•	•	a -
JTl
0.003	∙	、•∙一、•
0.001
objdet
coco
」ou① Is-
1e-02 •	•	•
1e-05
1e-08
semseg
ade20k
finetune task
0.100 -I-I-I-I-I-I-I-I-----1-I-I-I-I-I-I-I- - e V .
0.010	、0-•
*∙ ʌ J	，0 .
0.001
103	103.5	104	104.5	103	103.5	104	104.5	103	103.5	104	104.5
# of pretrain images
sinclass
imagenet
Figure 10: The linearized version of Figure 1.
objdet → objdet
mulclass → sinclass
0.10
0.030
r18
005
r34
0.010
r50
0.03
r101
r152
0.003
103
103.5
103
# of pretrain images
Figure 11: The linearized version of Figure 4.
model

D EMPIRICAL DEPENDENCY BETWEEN α AND D
When C is non-zero, the joint estimation of D and α in (1) have an issue of numerical stability due
to the small number of observations and noise, which can cause high dependence on each other.
Figure 13 (left) shows the curves of (1) with C = 0.5, where the solid red curve is D = 0.5, α = 0.4
and the dashed blue line is D = 1, α = 0.5. We see that both curves are almost indistinguishable for
19
Under review as a conference paper at ICLR 2022
3e-02
1e-02
3e-03
φ
ω
1e-03
3e-04
objdet → objdet
appearance
→- multiple
-single
light
—fix
—random
background
→- fix
→- random
object
1000	3000	10000	30000
# of pretrain images
• w/ texture
+ w/o texture
Figure 12: The linearized version of Figure 6.
Figure 13: Examples of parameter dependency.
2.0
1.5
1.0
0.5
0.0
0.00 0.25 0.50 0.75 1.0
α
a large n. Figure 13 (right) shows the actual landscape in terms of α and D of the nonlinear least-
squares at objdet→objdet, the bright areas indicate the fitting loss is small. We see that there
is a quadratic-like trajectory in the landscape, which implies the solutions are somehow redundant.
Similar landscapes were observed for other tasks (Figure 14).
0.0	0.5
pretrain task
objdet
210
0.5
0.0
∕0^H
0.5
0.0
2.0
0.5
0.0 二
semseg	mulclass	normal
.0 0.0	0.5
.0 0.0	0.5
.0 0.0	0.5	1.0
α
Figure 14: Loss landscapes of curve fittings.
finetune task
semseg sinclass
20
Under review as a conference paper at ICLR 2022
To avoid this issue, we fixed a common D for all the cases and estimated α for each. To determine
D, We used the following procedure. First, We prepared two global parameters α, D and set 0.5 as
their initial values. Then, we fitted the curves by two equations, Dn-α + C and Dn-α + C, and
estimated a and D. Next, we computed the median of α and substituted them into a. We did the
C∙ 7-Λ	1	7^∖	* i-.	C∙	.	1	1	c∙ -f∖	八，CT . 1	♦	,
same for D and D. After a few iterations, we got a converged value of D = 0.48. In the experiments,
we used the value for D and fixed it.
E Details of Theoretical Analysis
This section gives details of the theoretical discussions given in Section 3.2. For the analysis of
learning and generalization bound, we use the techniques developed recently by Nitanda & Suzuki
(2021). There are many works on the generalization of neural networks. To list a few, Neyshabur
et al. (2015), Neyshabur et al. (2017), Bartlett et al. (2017), Wei & Ma (2020), and Suzuki (2018)
analyze the generalization of neural networks based on complexity bounds. These generalization
bounds, however, do not consider an algorithm of learning, such as stochastic gradient descent (SGD).
Recently, learning dynamics of neural networks has been analyzed based on Neural Tangent Kernel
(NTK) Jacot et al. (2018) and global convergence of wide neural networks has been revealed Allen-
Zhu et al. (2018); Du et al. (2019). Based on the NTK framework, Arora et al. (2019) and Nitanda
et al. (2020) showed a generalization bound of the gradient descent learning of neural networks. More
recently, Nitanda & Suzuki (2021) focused the functional space given by NTK and showed that the
two-layer neural network with averaged SGD achieves the minimax optimal rate with respect to the
function class used in the standard theory of function estimation with kernels. We employ the method
of Nitanda & Suzuki (2021), which is the most suitable for our analysis of transfer learning: it enables
to examine the dependence on the initial parameter in the learning, and avoids the assumption of a
positive margin of eigenvalues used in Arora et al. (2019) and Nitanda et al. (2020).
E.1 Problem Setting
In the pre-training, the task is to learn the target function φo with T0 training data (Xi, yi)T「where
yi = φ0(xi), while in the fine-tuning phase, the network is initialized by the final parameter learned
by the pre-training, and the whole parameter is updated in the training. We assume that the target
function in the fine-tuning is given by
夕(x) = φo(x) + Φι(x),	(4)
and Ti training data is given by (xj,yj)T= 1 with yj =夕(Xj). In this setting, the goal of the fine-
tuning phase will be to learn the additional function φ1 mainly. Note that, for simplicity of analysis,
we assume noiseless training data, i.e., we assume the supervised signal yj is given by a deterministic
function of Xj , but extension to more general cases is not difficult as discussed in Nitanda & Suzuki
(2021). In the analysis, the data are assumed to satisfy X ∈ Rd, kXk2 = 1 and y ∈ [0, 1]. The
distribution of the input data X is denoted by ρX , and the same for the pre-training and fine-tuning.
For tractable theoretical analysis, we consider a simple scalar-valued two-layer neural network model
with M hidden units:
1M
gΘ(X)= √M ɪ^arσ(bTX).	⑸
r=1
We omit the bias term, but with obvious modification, it is not difficult to include it (see Nitanda &
Suzuki (2021)).
As in Nitanda & Suzuki (2021), we consider the averaged stochastic gradient descent (ASGD), where
one training sample is given at every time step for the stochastic gradient descent as in online learning,
and all the parameters in the time course are averaged after the final time step for the inference, that
is, after proceeding up to prescribed T (T = T0 or T1) time steps, the parameter to be used in the
inference is given by
T
Θ(T) := τ+1 XΘ(t).	(6)
+ t=0
The final network uses this averaged parameter, i.e., the final network is given by gθ(τ)(x).
21
Under review as a conference paper at ICLR 2022
The parameter is initialized as Θ(0) = (a(10) , b(10) , . . . , a(M0) , b(M0)). For the pre-trainig, each b(0) is
independently given by the uniform distribution on the unit sphere. As in Nitanda & Suzuki (2021),
a(0) are initilized as 1 or -1 so that gΘ(0) = 0. As explained before, the initial parameter of the
fine-tuning is the same as the averaged parameter of the pre-training θ^. The objective function to
minimize for the pre-training and fine-tuning is given by the following regularized empirical risk:
L(Θ) := 2 X(yi - gθ(Xi))2 + 2{ka - a⑼k2 + X kbr - brO)k2},	⑺
where λ is the regularization coefficient, which is a hyperparameter. The values of λ in the pre-training
and fine-tuning can be different, and denoted by λ0 and λ1, respectively. Note that the regularization
in Eq. (7) is not the most common '2-regularization, where ∣∣Θk2 is used for the regularization.
When applied in fine-tuning, however, the above regularization can be interpreted as elastic weight
consolidation (Kirkpatrick et al., 2017), which prevents forgetting the pre-trained parameters.
We consider online learning, in which at every step t (t ≤ T - 1), one datum xi is sampled from ρX
independently, and (xi, yi) (or (Xi, yi) in pre-training) is used to update the parameter according to
the gradient descent:
Θ(t+1) = θ㈤-ηdL(θ(t)),	(8)
∂Θ
where η is a learning rate. More explicitly,
art+1) - ar0) = (I - ηλ)(art) - arO)) - √m (gθ(t) - yt)σ(btt^τ Xt),
brt+1)—br0) = (I- ηλ)(b(t) - brO))- √=①㊀⑴一Iyt) ar/(亚,Xt)Xt.	⑼
E.2 Neural Tangent Kernel
In the theoretical analysis, the neural tangent kernel (NTK, Jacot et al., 2018) is used for approximat-
ing the dynamics of ASGD by a linear functional recursion on the corresponding function space. The
NTK of this model is given by
k∞(X, X0) = Eb(0) [σ(b(O)τ X)σ(b(O)τ X0)] + Xτ X0 Eb(0) [σ0(b(O)τ X)σ0(b(O)τ X0)].	(10)
The positive definite kernel k∞ naturally defines a reproducing kernel Hilbert space (RKHS), which
is denoted by H∞ .
The integral operator Σ∞ on L2 (ρX) is defined by
Σ∞f ：= / k∞(∙,x)f (x)dρx(x).	(11)
It is known that Σ∞ admits eigendecomposition
Σ∞ψs = γsψs,	(12)
where ψs is an eigenvector with ∣ψs ∣L2(ρX ) = 1 and γ1 ≥ γ2 ≥ . . . > 0 are eigenvalues in
descending order. Mercer’s theorem tells that k∞ has an expansion:
∞
k∞(X, X0) =	γsψs(X)ψs(X0),
s=1
where the convergence is understood as in L2(ρX) for general, and absolutely and uniformly if ρX is
a uniform distribution on a compact set.
E.3 Assumptions
For theoretical analysis, we make the following assumptions. For an operator Σ, the range of Σ is
denoted by R(Σ).
22
Under review as a conference paper at ICLR 2022
(A1) The activation function σ is differentiable up to the second order, and there exists C > 0
such that kσ00k∞ ≤ C, ∣∣σ0k∞ ≤ 2, and ∣σ(u)∣ ≤ 1 + |u| for ∀u ∈ R.
(A2) supp(ρX) ⊂ {x	∈	Rd | kxk ≤ 1} and y ∈	[-1, 1].
(A3) There exist 1/2	≤	r0, r1 ≤ 1 such that φ0	∈ R(Σr∞0 )	and φ1	∈ R(Σr∞1 ).
(A4) There exists ξ > 1 such that γ' = Θ('-ξ).
As in Nitanda & Suzuki (2021), Assumption (A1) assumes that the activation σ is differentiable in
this paper. In Nitanda & Suzuki (2021), however, they have developed a theory on how to extend
the results to the case of ReLU by approximating it with a smooth function. It is well known that
Assumption (A4) specifies the complexity of the hypothesis class H∞ (Caponnetto & De Vito,
2007); a faster eigen-decay (large ξ) implies the small complexity of the class. The assumption
(A3) controls the smoothness of the target functions φ0, φ1. In fact, the functions are included
in H∞, since φi ∈ R(Σ∞2) ⊂ H∞. When a function f has the expansion f = p` agψ', the
assumption f ∈ R(Σr∞) means a` = o('-ξr-1/2). A function with a larger r is smoother, which
is easier to learn. It is known (Caponnetto & De Vito, 2007; Nitanda & Suzuki, 2021) that ξ and
r are the two basic parameters to control the convergence rate of generalization attained by kernel
regression for a large sample size. Under the assumptions (A3) and (A4), given N i.i.d. training
data (Xi, Yi) with Xi 〜PX and 匕=4o(Xi) + ε with additive noise ε 〜N(0, σ2), the kernel
ridge regression Qχ with the regularization parameter λ = N-ξ∕(2rξ+1) achieves the generalization
E[kθλ -夕0kL2(PX)]=O(N-2rξ/(2rξ+1)) for any function 夕0 with 夕0 ∈ R(Σr), and it is known
this rate is optimal.
In the sequel, when φ ∈ R(Σr) and φ = Σrψ, we write kΣ-rφk := kψk.
E.4 Generalization bound
The dynamical behavior of pre-training can be discussed exactly in the setting of Nitanda & Suzuki
(2021). Let φo be the result of pre-training, i.e., φo := gdTo), where θ^ is the averaged parameter
pre
by ASGD. By optimizing the regularization parameter λ0, Corollary 1 in Nitanda & Suzuki (2021)
shows that for sufficiently large T0, with a choice of λ0 = T0-ξ∕(2r0ξ+1),
ʌ	__2r0ξ
Ekφ0 - φ0kL2(ρχ) ≤ εM + cT0 0r°ζ + ɪ (1 + 1加衰0φ0kL2(ρχ))	(13)
with high probability, where c is a universal constant and εM can be arbitrarily small for a large
-2r0ξ
M. As discussed in Section E.3, it is known (Caponnetto & De Vito, 2007) that the rate T0 2r0ξ+1
achieves the minimax optimal rate with respect to T0 over the class specified by r0 and ξ.
In fine-tuning, the initial parameter is given by Θ(0) = Θ^, and ASGD is applied with (xt, yt) for
t = 1, . . . , T1.
By extending Theorem 1 in Nitanda & Suzuki (2021), we can derive a generalization bound in the
following theorem. Recall that the regularization coefficient and learning rate of fine-tuning are
denoted by λ1 and η1 , respectively.
Theorem 2. Suppose Assumptions (A1)-(A3) hold. After pre-training that gives Eq. (13), fine-
tune the network by Eq. (9) with a learning rate η1 and regularization coefficient λ1 that satisfy
kΣ∞kop ≥ λ1 > 0 and 4(6 + λ1)η1 ≤ 1. Then, for any ε > 0, δ ∈ (0, 1), and T1 ∈ N, there exists
M0 ∈ N such that for any M ≥ M0, the following bound holds with probability at least 1 - δ over
the random initiailzation of pre-training:
EkgΘ(T1) 一2kL2(ρx)
≤ε+c0λ21r0kΣ-∞r0φ0k2L2(ρX)+c1λ12r1kΣ-∞r1φ1k2L2(ρX)
+ T + 1 {λ-1Ekφ0 - φ0kL2 (PX) (1 + 11夕衰0 φ0kL2 (PX)) + λ2riTkφ0kL2(ρχ) + kφ1kH∞ }
+ (T +c31)2 η2 {λ-2Ekφ0 - φ0kL2(ρχ )(1 + 口夕衰0 φ0kL2(ρχ)) + λ2ri-2kφ0kL2(ρχ) + λ-1kφ1kH∞ }
+ Tc+1 (1 + kTH∞ + 24k∑∞0H∣L2(ρχ))Tr[∑∞(∑∞ + λ1I)-1],	(14)
23
Under review as a conference paper at ICLR 2022
where φb0 is the result of pre-training and ci (i = 0, 1, 2, 3, 4) are universal constants.
The term ε is arbitrarily small for a large value of M, i.e., wide network. The proof of Theorem 2
will be given in Section E.6.
E.5 Analysis of convergence rates
We consider the rates of the generalization bound for E||g©(Ti)一 夕∣∣L2(ρχ)With respect to Ti. As
typical cases, we assume λ1 → 0 and η1 = O(1) as T1 → ∞. The dominant terms in Eq. (14) may
vary according to the configurations of λ1 and η1 With respect to T1 . We Will shoW the rates in some
settings that are relevant to transfer learning.
First, note that under Assumption (A4), the factor tr[Σ∞(Σ∞ + λ1I)-1] is given by (Caponnetto &
De Vito, 2007)
Tr[∑∞(∑∞ + λιI )-1 ] = O(λ-16).	(15)
By neglecting ε, the terms in Eq. (14) thus have the folloWing rates:
(a0)λ21r0,	(a1)λ21r1,	(b) T1-1λ1-1R0,	(c)T1-1λ21r1-1,	(d) T1-1,
(e) τ-2η-2λ-2R0,	(f) τ-2η-2λ2r1-2	(g) τ-2η-2λ-1,	(h) τ-1λ-1/ξ.	(16)
Here R0 := Ekφb0 一 φ0k2L2(ρ ) is of constant rate With respect to T1, but explicitly shoWn for the
later use.
Since λ1 → 0 and r1 ≥ 1/2, the terms (c) and (d) are of smaller rate than (b). LikeWise, (f) and (g)
are smaller than (e). The candidates of dominant terms are thus (a0), (a1), (b), (e), (g), and (h).
E.5. 1 Large regularization coefficient
In transfer learning, it is reasonable to use strong regularization in fine-tuning, Which encourages the
parameters to stay close to the initial value that is obtained in the pre-training. In this subsection, We
- ξ
consider the case where λ1 is larger than T1 2r1ξ+1, which would be the optimal rate if the network
- ξ
was trained with random initialization (see Nitanda & Suzuki (2021)), If λ1 = T1 2r1ξ+1 was taken,
it is easy to see that the influence of pre-training would not appear explicitly in the convergence rate.
In the sequel, we write F《G(T1) if there are a > 0 and T * such that F ≤ aG(T1) for all T1 ≥ T *.
In this notation, we assume
- ξ
λ1》T1 2r1ξ+1.	(17)
- ξ
The rate λ1 = T1 2r1ξ+1 is given by equating the rates of (a1) and (h). Therefore, under the
assumption of Eq. (17), the rate (a1) is larger than (h), and thus it suffices to consider (a0), (a1), (b)
and (e) as the candidates of dominant terms. Note that, if λ1 → 0, the terms (a0) and (a1) decrease,
while (b) and (e) increase to infinity.
We will discuss below the possible cases of dominant terms under the assumption Eq. (17). In
the analysis, although the error of the pre-training R0 is regarded as a constant, we yet wish to
consider the dependence of the fine-tuning result on R0. We thus set the regularization coefficient λ1
dependent on R0, and show that in all the cases, the generalization bound takes the form
EkgΘ(Tι) - PkL2(ρχ) ≤ ε + CRVTIe,	(18)
where ν > 0 is a constant. As R0 ≤ c0 + A0T0-α0 from Eq. (13), the factor R0α can be bounded from
above as	R0ν ≤ c + AT0-α0ν
for large T0 . As a result, we obtain
Ekgθ(τι) - 月隹(PX) ≤ ε + C(C + AT-α)T-β,	(19)
24
Under review as a conference paper at ICLR 2022
where C, c, A are constants. As we will see, the exponents α and β depend on r0, r1, ξ and η1.
Eq. (19) accords with the bound in Theorem 1.
In the sequel, we use ζ ≥ 0 for the learning rate such that η1 = T1-ζ.
Case I: Small learning rate T1λ1η12	1. In this case, (e)	(b). We equate (a0) or (a1) with (e)
to obtain λ1 for achieving the best possible upper bound of the two terms.
(I-A) ro ≥ r1. Since λ1 → 0 for T1 → ∞, (a1) is larger than (a0). By equating (a1) and (e), we find
that the best choice of λι is
-ι-Z
λι =O(TI r1+1).
We further consider
-ι-Z
λι = Ti r1+1 RV
for dependence on R0 . To determine ν, we assume that R0 is a small value, and consider the
rate of (a1) and (e) with respect to R0 after plugging the above λ1 to them. By equating the
-2ri(1-Z)	- 2ri(1-Z)
rates of (a1) T∖ r1+1 R2r1ν and (e) T∖ r1+1 R1-2ν, the best possible rate of Ro is attained by
ν = 1/(2r1 + 2). The dominant rate of Eq. (16) is thus
-2rι(1-Z)	ri
Ti-^r1+1 R 产	(20)
attained by
-ι-Z	i
λi = T1 r1+1 RF1)	(21)
We need to identify the conditions on ζi to meet the requirements. The condition λi → 0 is equivalent
- ξ
to Z < 1. There are two other conditions: λι》T 2r1ξ+1 and T1λ1η2《 1. Given Ro is of constant
rate, the former is equivalent to 一 2rξ+i ≤ 一 ⅛⅛, which results in
Z ≥ rιξ +1 - ξ
Z ≥ 2rιξ +1 .
The latter condition is equivalent to 1 一 2Z 一 r-^ ≤ 0, which is
ri
Z ≥ ——1—
Z ≥ 2ri + 1
It is not difficult to see
ri > rιξ +1 — ξ
2ri + 1)2rιξ + 1
for ξ > 1. As a result, the condition on Z is
ri
2ri + 1
≤ Z < 1.
(22)
If ηi = Ti-ζ is taken to satisfy this condition, the optimal rate of λi is given by Eq. (21). Finally, the
resulting generalization bound is given by
-2ri (1-Z)	ri
Ekgθ(Ti) — TL2(px) ≤ ε + cTi ʒɪɪR(FT.
(23)
(I-B) ri > ro： In this case, (a0) is of larger rate than (a1). By a similar argument to (I-A), with the
rate	ι-ζ	i 	i-^；-	7=；	i—T=T λi = Ti r0+1 Ror0+2,	(24)
The generalization bound is given by
	-2r0 (1-ζ)	r0 Ekgθ(τι) — TL2(px) ≤ ε + cTi	r0+1 R00+1.	(25)
The condition on Z is	r ro	(2ri 一 rO)ξ + 1 一 ξl “ / / i	CG maxI 2r0∏, -2r;ξ+!—} ≤ζ < 1.	(26)
25
Under review as a conference paper at ICLR 2022
Table 1: Generalization bounds in various conditions.
(Case II): large learning rate T1λ1η12	1. Next, we consider the case where the learning rate η1
is large so that T1λ1η12	1, which includes the constant η1. Under this condition, (b) is of larger
rate than (e).
(II-A) ro ≥ ri. In this case, (a1) is of larger rate than (a0). A similar argument to (I-A) provides
-	1	1
λι = Ti 2r1+1 Rjr1+1,	(27)
and the generalization bound is given by
2r1	2r1
Ekgθ(T1) - Φ0kL2(ρχ) ≤ ε + CT-ERT.	(28)
The conditions are λι》T1 2r1 ξ+1 and Tiλ1η2》1. The former condition always holds for ξ > 1,
and the latter is equivalent to Z ≤ ^r+. The resulting condition on Z is
ri
0<ζ≤ 2r7+i.	(29)
(II-B) ri > ro： In this case, (a0) is of larger rate. With
1	1
λi = Ti 2r0+τR02r0+ ,	(30)
the generalization bound is given by
-2r0	2r0
EkgθG — Φ0kL2(ρx) ≤ ε + cTi 2r0+1 R0r0+1.	(31)
The conditions λi》T 2r1ξ+1 and Tiλiη2》1 are respectively ri ≤ ro + ξ-ξi and Z ≤ 2r+i.
Thus, we require
0 <Z ≤	,	ro <ri ≤ ro + ξ-1,	(32)
2ro + 1	2ξ
In summary, the generalization bounds in various conditions are summarized in Table 1.
E.6 Proof of Theorem 2
The proof of Theorem 2 is based on the application of the theory in Nitanda & Suzuki (2021) to the
fine-tuning phase, adapting the initialization given
by the result of pre-training φbo .
In the sequel, we focus on the fine-tuning with Ti samples with yt =夕(xt). Recall that
夕(x) = φo(x) + φi(x).
26
Under review as a conference paper at ICLR 2022
E.6.1 REFERENCE ASGD ON RKHS
We use a surrogate sequence of functions in an RKHS for the proof. Let kM be the random feature
approximation of the TNK k∞, i.e.,
1 M	xTx0 M
kM(x,x ) = M X σ(bTx)σ(bTX ) + -ɪ X σ (bTX)σ (bTX ),	(33)
r=1	r=1
where (br)rM=1 is i.i.d. random sample from the uniform distribution on the unit sphere Sd-1. The
associated RKHS is denoted by HM .
A reference ASGD is defined by the following update rule of functions in the RKHS HM :
g(t+1) = (1 - ηλ)g(t) - η(g(t)(χt) - yt)kM(∙,χt),	(t = 0,...,T - 1)
with the initialization given by g(0) := φb0. The average is taken at the final step:
1 T1
户:=T⅛1 Xg(t).
1 t=0
(34)
(35)
By considering continual learning of pre-training and fine-tune, a slight modification of (Nitanda &
Suzuki, 2021, Propososion A) derives the following proposition.
Proposition 3. Assume (A1) and (A2). Suppose that η1λ1 < 1. Then for any T1 ∈ N and ε > 0,
there is M* = M*(Tι, ε) ∈ N such that during the fine-tuning learning
kg(t)- gΘ(t) l∣L∞(ρx) ≤ ε,	(36)
holdsfor any M ≥ M* and 0 ≤ t ≤ T.
This proposition shows that, if we use a very wide network, the learning of ASGD in the parameter
space can be approximated by the reference ASGD on the RKHS with negligible error.
The generalization bound will be given by the following decomposition:
kgθ(Tι)-2 kL2(ρχ)≤ 2kgθ(Tι)- U(TI)kL2 (PX)+ 2ku(TI)-2 kL2 (PX),	(37)
in which the first term of the right hand side is bounded by Proposition 3 with an arbitrary small value
ε for large M . The second term will be discussed in the next subsection.
E.6.2 Convergence rates of reference ASGD
In this section, we write λ and η for λ1 and η1 for simplicity. The covariance operators Σ∞ and ΣM
for H∞ and HM, respectively, are defined by
∑∞ := EPX [k∞(∙,X) 0 k∞(∙,X)*],
ςM := Eρχ [kM (∙, X) 0 kM (∙, X )*],
where * denotes the adjoint; equivalently,
Σ∞f = / k∞(∙,x)f(x)dρx(x),	∑Mh = / kM(∙,x)h(x)dρχ(x),
for f ∈ H∞ , h ∈ HM. The regularized target functions φ(Mi),λ (i = 0, 1) are defined by
φ(Mi),λ := (ΣM + λI)-1ΣMφi	(i =0,1).
φ∞,λ is defined similarly with Σ∞. Note that ψM,λ := (∑m + λI)-1 ∑m夕=φM,λ + φM)λ∙
First, We decompose kU(TI) - ^kL2(ρx)by
(38)
(39)
kU(TI)-夕kL2(ρχ) = kU(TI)-2M,λ +
≤ 3IIg(TI)一2M,λ∣[
φ(M0),λ + φ(M1),λ - φ0 - φ1k2L2(PX)
2L2(PX) + 3kφ(M0),λ
φ0k2L2(PX) + 3kφ(M1),λ - φ1k2L2(PX).
—
27
Under review as a conference paper at ICLR 2022
The second and third terms are known to have a bound, with high probability, (Nitanda & Suzuki,
2021, Propositions C and D)
kφ(Mi),λ-φik2L2(ρX) ≤ε+λ2rikΣ-∞riφik2L2(ρX),	(i=0,1),	(40)
where ε is arbitrarily small for large M . We have thus, with high probability,
kg(τ1')-φkL2(ρχ)≤ε+3∣Ig(TI)一2M,λkL2(ρχ)+3λ2r0 ||夕衰0 φo kL2(ρχ)+3λ2r1 INJ φιkL2(ρχ).
(41)
As shown in Nitanda & Suzuki (2021), the term Ilg(TI) - 夕M,λ ∣∣L2(ρχ)can be analyzed by the bias
and noise terms of the stochastic recursion on RKHS Eq. (34), which is rewritten as
g(t+1) = (I - ηHt- ηλI)g㈤ + ηytkM(∙, xt),	(42)
where
Ht= kM(∙, Xt) 0 kM(∙, xt)*,
is a one-sample estimate of ∑m . By subtracting 夕M,λ from both hand sides of Eq. (42), We have
g(t+1) -2M,λ = (I ― ηHt - ηλi)(g(t) -2M,λ) + βt,	(43)
where
β = ηytkM(∙, Xt)- η(Ht + λI)(∑M + λI) 1ςMPM,λ
is the zero mean noise term. Using this recursive formula, Nitanda & Suzuki (2021) derives a bound:
IIU(T)-2 M,λkL2(ρχ)≤ τ +1 ikςm + λI)-1/2(g(0)-2 M,λ)∣L2(ρχ)
+ (TI +C21)2η2 k(*M + λI)-1(g⑼-°M,λ)kL2(ρχ)
+ Tc+1 (1 + kTL2(ρχ) + 24k∑∞r1 TL2(ρχ ))Tr[∑M (∑m + λI )-1].
(44)
To bound this expression further, we use Proposition B in Nitanda & Suzuki (2021)
k(∑M + λI )-1/2φ(M,λkL2(Pχ) ≤ 2∣ΦikH∞.	(45)
Then, using the decomposition g(0) 一 夕M,λ = (φo — φo) + (φo — φM)λ) 一 φM∖ We obtain that,
with high probability,
∣∣(ςM + λI)-1∕2(g(O)-2M,λ)kL2(ρχ)
≤ 3∣I(ςM + λI)-"2(φ0 - φ0)kL2(ρχ) + 3∣∣(ςM + λI)-1∕2(φ0 - φM,λ)kL2(ρχ)
+ 3∣∣(ςm + λI 厂"2φM∣λkL2(ρχ)
33
≤ λ kφ0 - φ0kL2(ρχ) + λλ 0 llφ0kL2(ρχ) + εM + 6kφ1kH∞ ,	(46)
Where We use Eq. (40) for the second and third terms and Eq. (45) for the fourth term in the last
inequality. Similarly, With high probability, We have
∣∣(ςm + λI)-1(g(O)-2M,λ)∣∣L2(ρχ)
≤ 3∣I(ςM + λI)-1(φ0 - φ0)kL2(ρχ) + 4∣∣(ςM + λI)-1(φ0 - φM),λ)kL2(ρχ)
+ 3I(ΣM + λI)-1φ(M1),λ I2L2(ρX)
33	6
≤ λ kφ0 - φ0kL2(ρχ) + λλ 0 kφ0kL2(ρx) + εM + λ kφ1kH∞ .	(47)
It is also knoWn (Nitanda & Suzuki, 2021, Proposition B) that, for λ ≤ IΣ∞ I,
Tr[ΣM (ΣM + λI)-1] ≤ 3Tr[Σ∞(Σ∞ + λI)-1].	(48)
Combining Eqs.(41), (44), (46), (47), and (48), We obtain the assertion of the theorem.
28