Figure 1: AI-SARAH vs. SARAH: (a) evolution of the optimality gap P(W) — P and (b) the squarednorm of stochastic recursive gradient kvtk2; AI-SARAH: (c) evolution of the step-size, upper-bound,local Lipschitz smoothness and (d) distribution of Si of stochastic functions. Note: in (a), P is alower bound of P (w*); in (c), the white spaces suggestfull gradient computations at outer iterations;in (d), bars represent medians of si ’s.
Figure 2: Evolution of ∣∣VP(w)k2 for Y ∈ {焉,击,门,8, 4, 2}: regularized (top row) and non-regularized (bottom row) logistic regression on ijcnn1, rcv1, real-sim, news20 and covtype.
Figure 3: Average (top row) and total (bottom row) running time of AI-SARAH and other algorithmsfor the regularized case.
Figure 4: Running minimum per effective pass (top row) and wall clock time (bottom row) of∣∣VP(w)k2 between other algorithms with all hyper-parameters configurations and AI-SARAH for theregularized case. Note: the horizontal dashes in blue represent the minimum ∣∣VP(w)∣2 achievedby AI-SARAH at certain effective pass or time point.
Figure 5: Evolution of ∣∣VP(w)∣∣2 for the regularized case by effective pass (top row) and wallclock time (bottom row).
Figure 6: Evolution of P(w) for the regularized case by effective pass (top row) and wall clock time(bottom row).
Figure 7: Running maximum of testing accuracy for the regularized case by effective pass (top row)and wall clock time (bottom row).
Figure 8: Evolution of AI-SARAH’s step-size α and upper-bound αmax for the regularized case.
Figure 9: '2-regularized case ijcnn1, rcv1, real-sim, news20 and covtype withY∈{ 64, 32,16, 8, 4, 1 }: evolution of P (W) (top row) and ∣∣VP (w)∣2 (middle row) and running maxi-mum of testing accuracy (bottom row).
Figure 10: '2-regularized case of a1a, gisette, w1a, w8a and mushrooms with Y ∈{614, 312,16, 8, 4,1}: evolution of P (W) (top row) and ∣∣VP (w)k2 (middle row) and running maxi-mum of testing accuracy (bottom row).
Figure 11:	Non-regularized case ijcnn1, rcv1, real-sim, news20 and covtype with γ ∈{ 64, 32,16, 8, 4,1}: evolution of P (W) (top row) and ∣∣VP (w)∣2 (middle row) and running maxi-mum of testing accuracy (bottom row).
Figure 12:	Non-regularized case a1a, gisette, w1a, w8a and mushrooms withγ∈{ 64 , 32, ιi6,8, 4,1}: evolution of P (W) (top row) and ∣∣VP (w)k2 (middle row) and running maxi-mum of testing accuracy (bottom row).
Figure 13:	Ending loss (top row), ending squared norm of full gradient (middle row), maximumtesting accuracy (bottom row) of different hyper-paramters and algorithms for the '2-regularizedcase on ijcnn1, rcv1, real-sim, news20 and covtype datasets.
Figure 14:	Ending loss (top row), ending squared norm of full gradient (middle row), maximumtesting accuracy (bottom row) of different hyper-paramters and algorithms for the '2-regularizedcase on a1a, gisette, w1a, w8a and mushrooms datasets.
Figure 15:	Ending loss (top row), ending squared norm of full gradient (middle row), maximumtesting accuracy (bottom row) of different hyper-paramters and algorithms for the non-regularizedcase on ijcnn1, rcv1, real-sim, news20 and covtype datasets.
Figure 16:	Ending loss (top row), ending squared norm of full gradient (middle row), maximumtesting accuracy (bottom row) of different hyper-paramters and algorithms for the non-regularizedcase on a1a, gisette, w1a, w8a and mushrooms datasets.
Figure 17: Average ending ∣∣VP(w)∣∣2 for '2-regularized case - AI-SARAH vs. Other Algorithms:AI-SARAH is shown as the horizontal lines; for each of the other algorithms, the average ending∣VP (w)∣2 from different configurations of hyper-parameters are indexed from 0 percentile (theworst choice) to 100 percentile (the best choice); see Section A.2.2 for details of the selection criteria.
Figure 18: Average ending ∣∣VP(w)k2 for non-regulariZedcase- AI-SARAH vs. Other Algorithms.
Figure 19: Non-regularized case: evolution ofP(W) (top row), ∣VP (W)∣2 (middle row), and runningmaximum of testing accuracy (bottom row).
Figure 20: '2-regularized case: evolution of P (W) (top row), kVP (W) ∣∣2 (middle row), and runningmaximum of testing accuracy (bottom row).
Figure 21: Non-regularized case: evolution of P(W) (top row), kVP(W) ∣∣2 (middle row), and runningmaximum of testing accuracy (bottom row).
Figure 22: '2-regularized case: evolution of AI-SARAH's step-size α and upper-bound amaχ.
Figure 23: Non-regularized case: evolution of AI-SARAH’s step-size α and upper-bound αmax.
