Figure 1: Our approach. Using a number of safe trajectories demonstrated by humans in the realenvironment, we train a dynamics model that functions as a learned simulator. Using this simulator,we train a reward model by asking humans for feedback on hypothetical trajectories. Once thereward model is robust, we deploy an agent in the real environment using model predictive control.
Figure 2: Our training procedure for an apple collection task. Light blue: steps performed usinglearned dynamics model. First, a human demonstrates a number of trajectories, exploring thoroughlywhile avoiding the unsafe (red) parts of the state space. From these trajectories, we train a dynamicsmodel, and use it to generate a number of random trajectory videos. We ask humans to providefeedback on these videos in the form of reward sketches, then use these sketches to train an initialreward model. Since this reward model may not be sufficiently robust, we generate another setof trajectory videos by optimising for maximum and minimum reward as predicted by the currentreward model, exposing for example instances where an agent would otherwise exploit the rewardmodel. Using sketches on these trajectories, we train an improved reward model. This cycle canbe repeated a number of times until a human deems the reward model to be good enough, at whichpoint we deploy the agent using model predictive control.
Figure 3: 3D environments used for our experiments. Environments consist of green apples, redblocks, and a gate opened by a switch. Wall and floor colours are randomised. Also shown are theagentâ€™s body in the top corner of the environment, and two timer columns, indicating how much timeis left in the episode by how much of the column is green. Left: cliff edge environment. The agentmust avoid falling off the edge of the environment. Right: dangerous blocks environment. Theagent should avoid interfering with the red blocks, the distance between which encodes the rewardsent to the agent.
Figure 4: Example rollout from dynamics model using test set initial state and action sequence. Werequire high-quality rollouts for humans to be able to give meaningful feedback. Top row: ground-truth frames. Bottom row: frames predicted by dynamics model, conditioned on ground-truth frameat step 0 and (for second column onwards) action sequence. Note that the rollout remains coherentover many timesteps, even modeling the opening of the gate when the agent steps on the button.
Figure 5:	Cliff edge environment results. Left: safety, measured by the number of times the agentfell off the edge of the environment. Note that ReQueST train-time safety violations consists entirelyof mistakes by human contractors while providing demonstration trajectories. Right: performance,measured by the average number of apples eaten per episode. Each difficulty represents a differentarena size, with larger arenas being easier. Runs are repeated over 10 seeds, with bar heights/labelsshowing medians and error bars showing 25th/75th percentiles.
Figure 6:	Dangerous blocks environment results. Left: safety, measured by the total number oftimes the agent touched the blocks. Note that ReQueST train-time safety violations consist entirelyof mistakes by human contractors while providing demonstration trajectories. Right: performance,measured by the average number of apples eaten per episode.
Figure 7:	Scaling results for dynamics model and reward model. Top: representative predictionsfrom dynamics models trained on 1k, 3k, 9k, 30k, and 100k trajectories. Left: final smoothed testlosses from training dynamics models on 19 dataset sizes. See Appendix D for details. Right:performance of ReQueST agent using reward models trained on different fractions of full sketchesdataset (comprised of sketches on both random and optimised trajectories). Bar height shows medianapples eaten in 100 evaluation episodes over 10 seeds; error bars show 25th and 75th percentiles.
Figure 8: All six of our environments variants, showing the three possible arena sizes. First column:small. Second column: medium. Third column: large.
Figure 9: Dynamics model training curves.
