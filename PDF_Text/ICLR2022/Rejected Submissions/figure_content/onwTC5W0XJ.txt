Figure 1: CNNs need not learn causal features. The examples are taken from the Pascal VOC 2012dataset (first row) for aeroplane-cow classification in a biased dataset. In the second row we see thefeatures learnt by the CNN highlighted using the Grad-Cam method. It is evident that the model haslearnt features such as the sky and grassland for classifying aeroplane and cow respectively.
Figure 2: Activation mask creation: In the case of lack of annotated data, one can create activationmask for guidance very easily (this can be implemented in Python). The order is from left to right.
Figure 3: Illustration of the methodology: Along with images and labels, activation masks are alsoinput for guidance. Dotted lines denote the flow present only during the training phase.
Figure 4: Demonstrating the effectiveness of proposed CFCN: This figure shows the counterpartheatmaps from CFCN for the images in Fig. 1. We see that unlike CNN, CFCN is able to focus onthe causal features belonging to the aeroplanes and the cows present in the images.
Figure 5: Comparison of models on qualitative results: CFCN variants learn causal featureswhereas the features learnt by baselines are not causal.
Figure 6: Convergence: Causally-focus loss does not affect the convergence in training CFCNs.
Figure 7: Illustration of effect of the trade-off parameter (α) in the Loss: We see that, withincrease in the value of α, the model is able to focus more on the class-discriminatory causal featurein the input images.
Figure 8: Illustration of the effect of human guidance in the form of activation mask: As weincrease the amount of human guidance in the form of activation masks accuracy increases (left),and loss reduces at a greater rate with the increase in the training dataset size (in x-axis, 1 unitequals to 1000 samples). One observation is that, with additional guidance the models are able tolearn efficiently with less data.
Figure 9: Robust feature learning: The above figures show the robustness of feature learning inCFCNs against changes in the background and the brightness of the input images.
Figure 10: Obtaining the activation masks: The folloWing table shoWs some examples of ob-taining activation masks from different annotations available for some aligned tasks such as imagesegmentation and object localization. First roW in the figure shoWs the input images, second roWshoWs their corresponding available annotations and third roW shoWs the activation masks generatedfrom these annotations. As in the case of Brain MRI dataset, binary masks are readily available,Which can be directly used for our purpose.
Figure 11: Enhanced visual explanations: This figure demonstrates the benefits of all convolu-tional nets in terms of visual explanations. The feature map outputs for the input digits (a) 6 and (b)9 are given. The images represent the classes 0 to 9 in row major order from left to right.
Figure 12: Convergence: Causally-focus loss does not affect the convergence in training CNNs.
Figure 13: Comparing the causally-focus loss: In the above plot, we compare the values of thecausally-focus loss for different models on the benchmark datasets. We observe that, our modelsperform better than baselines in all the dataset cases.
Figure 14: Additional qualitative results presenting the Grad-Cam heatmaps generated for the Bike-Helmets dataset provided by Make-ML.
Figure 15: Additional visual results comparing the robustness of features learnt by CNN and CFCNfor the vary-background experiment. The results show that CFCN consistently outperforms CNN interms of robust feature learning.
