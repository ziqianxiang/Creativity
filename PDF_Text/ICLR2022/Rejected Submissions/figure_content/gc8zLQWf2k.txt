Figure 1: Clean Acc. (left) & Adv. Acc (right) on Atypical Set ofCIFAR100 under WRN28D0 : infl(xi, x0j) > 0.15, for ∀xi ∈ Datyp}. In Figure. 1, we report the algorithm’s performance (clean& adv. acc.) on these atypical sets along with the training process. From the results, we observethat the WRN28 model is capable to memorize almost all atypical samples and their adversarialcounterparts, since it can achieve ≈ 100% clean & adversarial accuracy on the training atypical set.
Figure 2: Adversarial Training vs. Traditional ERM on Typical Set of CIFAR100 under WRN28impact of atypical samples on the typical samples. From the results, we find that the existence ofatypical samples makes a significant influence on the typical samples. For example, an adversariallytrained model without atypical samples has 94.7% clean accuracy and 55.0% adversarial accuracyon the test typical samples (on the last epochs). While, the model trained with all atypical samplesonly has 90.2% and 50.4% clean & adv. accuracy, respectively. These results suggest: the moreatypical samples exist in the training set, the poorer performance the model will have on Dt0yp. Inother words, these atypical samples act like “poisoning” samples (Biggio et al., 2012; Xu et al., 2019)which can deteriorate the model’s performance on typical samples. It is also worth mentioning thatthis poisoning phenomenon does not appear in the traditional ERM algorithm. From Figure 2c, wecan see that the models have equal accuracy on the typical set, when different numbers of atypicalsamples appear in the training set.
Figure 3: Poisoning Atypical SamplesPoisoning Atypical Samples A natural question is what kinds of atypical samples are likely to“poison” model robustness and why? Different from previous literature about poisoning samples,which assume that poisoning samples are most mis-labeled samples (Li et al., 2020), CIFAR100 is aclean dataset with no or very few wrong labels. However, we hypothesize that the atypical sampleswhich poison the model performance might pertain to some features of a “wrong” class. Recall thatatypical samples are always distinct from the main data distribution in their labeled class, it is likelythat they are closer to the distribution of a “wrong” class. As shown in Fig. 3, in CIFAR100 dataset,we identify an atypical “plate” which visually resembles images in “apple” (the examples are foundvia the method in Section 4.1). If the model memorizes this atypical “plate” and predicts any sampleswith similar features to be “plate”, the model cannot distinguish between “apple” and “plate”.
Figure 4: Clean Acc.(left) & Adv. Acc.(right) of BAT6	Related WorksMemorization and atypical sam-ples. The memorization effectof overparameterized DNNs havebeen extensively studied both empir-ically (Zhang et al., 2016; Nakkiranet al., 2019) and theoretically (Bartlett& Mendelson, 2002). From traditionalviews, the memorization can be harm-ful to the model generalization, be-cause it makes DNN models easily fitthose outliers and noisy labels. How-ever, recent studies point out the concept of “benign overfitting” (Bartlett et al., 2020; Feldman,2020; Feldman & Zhang, 2020), which suggests the memorization effect necessary for DNNs to haveextraordinary performance on modern machine learning tasks. Especially, the recent work (Feld-man & Zhang, 2020) empirically figures out those atypical/rare samples in benchmark datasetsand show the contribution from memorizing atypical samples to the DNN’s performance. Besidesthe work (Feldman & Zhang, 2020), there are also other strategies (Carlini et al., 2019) to findatypical samples in training dataset. Notably, our work is not the first effort to study the influence of
Figure 5: Examples of Images with Different Memorization Values13Under review as a conference paper at ICLR 2022In Fig. 6, we provide histograms to show the distribution of the estimated memorization values ofall training samples from CIFAR10, CIFAR100 and Tiny ImageNet. From Fig. 6, we can observethat atypical samples (with high memorization value > 0.15) consist of a significant fraction (over40% & 50% respectively) in CIFAR100 and Tiny ImageNet. In CIFAR10, they also consist of anon-ignorable fraction which is over 10%.
Figure 6: Frequencies of Training samples with Different Memorization Values in Various DatasetsIn Fig. 7, we provide several pairs of images with high influence value (as defined in Section 2.1)which is over 0.15. In each pair, the training sample also has a high memorization value over 0.15.
Figure 7: High Influence Pairs with Influence Value > 0.15B	Additional Results for Preliminary StudyIn this section, we provide the full results of the preliminary study in Section 3 on CIFAR10,CIFAR100 and Tiny ImageNet, to illustrate the distinct behaviors of the memorization effect betweentraditional ERMs and adversarial training. In both ERM and adversarial training, we train the modelsunder ResNet18 and WideResNet28-10 (WRN28) architectures. In the experiments, we train themodels for 200 epochs with learning rate 0.1, momentum 0.9, weight decay 5e-4, and decay thelearning rate by 0.1 at the epoch 150 and 200. For adversarial training, we conduct experiments usingPGD adversarial training Madry et al. (2017) by default to defense against l∞-8/255 adversarialattack, With the exception on Tiny ImageNet, which is against l∞-4/255 attack. For robustnessevaluation, we conduct a 20-step PGD attack.
Figure 8: Clean Accuracy on Atypical Set of CIFAR100(a) ResNet18.	(b) WRN28.
Figure 9: Clean Accuracy on Atypical Set of CIFAR10(a) ResNet32.
Figure 10: Clean Accuracy on Atypical Set of Tiny ImageNet15Under review as a conference paper at ICLR 2022(ii) Additional Results for Preliminary Study - Section 3.1 In Adversarial TrainingFig. 11, Fig. 12 and Fig. 13 report the performance of adversarially trained models. We evaluate theclean accuracy and adversarial accuracy on the training atypical set Datyp and test atypical set Da0typ.
Figure 14: Clean Accuracy on Typical Set of CIFAR100(a) ResNet18.
Figure 16: Clean Accuracy on Typical Set of Tiny ImageNetFigure 15: Clean Accuracy on Typical Set of CIFAR10(b) WRN28.
Figure 15: Clean Accuracy on Typical Set of CIFAR10(b) WRN28.
Figure 17: Clean Accuracy and Adversarial Accuracy on Typical Set of CIFAR100Figure 18: Clean Accuracy and Adversarial Accuracy on Typical Set of CIFAR10Figure 19: Clean Accuracy and Adversarial Accuracy on Typical Set of CIFAR100B.3	An ablation study for “Poisoning Atypical samples”In this subsection, we conduct additional experiments to verify the claims about "poisoning atypicalsamples". In particular, we aim to demonstrate that: the atypical images with high "poisoning scores"are likely to obtain features which visually resemble the images from a (wrong) different class, sofitting them are likely to degrade the model performance. To validate our claim, we print out 200images of atypical samples from CIFAR100 training set, with highest / lowest poisoning scores asdefined in Eq.(6) of the paper. Then, we let two individual human annotators to label each image, bychoosing one of 4 options including: (a.) This image belongs to class y1. (b.) This image belongsto class y2. (c.) Both class y1 and y2 are likely. (d.) Neither class y1 or y2. Here, y1 is the groundtruth label of the sample; y2 is the class other than y1 which the model has the maximal confidence:y2 = arg maxt6=y1 Ft(x), and the model F is obtained via PGD adversarial training, with all atypicalsamples removed from the training set. We report the percentage of the answers of the humanannotators. We report the percentage of the answers of the human annotators.
Figure 18: Clean Accuracy and Adversarial Accuracy on Typical Set of CIFAR10Figure 19: Clean Accuracy and Adversarial Accuracy on Typical Set of CIFAR100B.3	An ablation study for “Poisoning Atypical samples”In this subsection, we conduct additional experiments to verify the claims about "poisoning atypicalsamples". In particular, we aim to demonstrate that: the atypical images with high "poisoning scores"are likely to obtain features which visually resemble the images from a (wrong) different class, sofitting them are likely to degrade the model performance. To validate our claim, we print out 200images of atypical samples from CIFAR100 training set, with highest / lowest poisoning scores asdefined in Eq.(6) of the paper. Then, we let two individual human annotators to label each image, bychoosing one of 4 options including: (a.) This image belongs to class y1. (b.) This image belongsto class y2. (c.) Both class y1 and y2 are likely. (d.) Neither class y1 or y2. Here, y1 is the groundtruth label of the sample; y2 is the class other than y1 which the model has the maximal confidence:y2 = arg maxt6=y1 Ft(x), and the model F is obtained via PGD adversarial training, with all atypicalsamples removed from the training set. We report the percentage of the answers of the humanannotators. We report the percentage of the answers of the human annotators.
Figure 19: Clean Accuracy and Adversarial Accuracy on Typical Set of CIFAR100B.3	An ablation study for “Poisoning Atypical samples”In this subsection, we conduct additional experiments to verify the claims about "poisoning atypicalsamples". In particular, we aim to demonstrate that: the atypical images with high "poisoning scores"are likely to obtain features which visually resemble the images from a (wrong) different class, sofitting them are likely to degrade the model performance. To validate our claim, we print out 200images of atypical samples from CIFAR100 training set, with highest / lowest poisoning scores asdefined in Eq.(6) of the paper. Then, we let two individual human annotators to label each image, bychoosing one of 4 options including: (a.) This image belongs to class y1. (b.) This image belongsto class y2. (c.) Both class y1 and y2 are likely. (d.) Neither class y1 or y2. Here, y1 is the groundtruth label of the sample; y2 is the class other than y1 which the model has the maximal confidence:y2 = arg maxt6=y1 Ft(x), and the model F is obtained via PGD adversarial training, with all atypicalsamples removed from the training set. We report the percentage of the answers of the humanannotators. We report the percentage of the answers of the human annotators.
