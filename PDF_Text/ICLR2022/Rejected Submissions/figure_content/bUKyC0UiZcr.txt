Figure 1: Our representation is trained to encodethe area that the agent has learned to cover. Skillsare continuously trained on the representation todiscover new areas where novel data is collectedto refine the representation, progressively extend-ing its coverage. Similar incremental discovery isat the core of several works (Ecoffet et al., 2021;Pong et al., 2019; Machado, 2019).
Figure 2: The gridworld domains with the fixed initial state s0 highlighted in red.
Figure 3: TATC representations learned throughout the training. Axes scales were equalized. Toprow (R1): U-MAZE. Middle row (R2): T-MAZE. Bottom row (R3): 4-ROOMS. The colors reflectthe distances in terms of the dynamics. They can be seen as quantities proportional to the length ofthe shortest path from s0 (marked in red) to the represented state.
Figure 4:	Learned representation’s ability to approximate the value function. Lap-rep was learnedin the same non-uniform prior setting (non-μ) with d = 2 and d = 3 (no improvement was observedfor higher values). The dashed line gives the performance of Lap-rep in the uniform prior setting(μ). TATC outperforms LAP-REP in non-μ setting, and succeeds in recovering its expressive powerwhen learned from the uniform prior. Performances were averaged over 5 different runs.
Figure 5:	Control performance (episode reward) in the fixed initial state setting (non-uniform prior).
Figure 6: TATC learned representation visu-alized on a grid of positional states. Colorsreflect the distance in the representation spacefrom the initial state, highlighted in red. Axesscales were equalized. We can visually appreci-ate how the U-shaped continuous state domainis mapped to a flatter manifold reflecting thepresence of the wall.
Figure 7: Results of reward shaping usinglearned representations: Performances were av-eraged over 5 different runs and then exponen-tially smoothed (0.9) for better visualization.
Figure 8: Skills Evaluation: performance gath-ered from 5 independent runs and then exponen-tially smoothed (0.9) for better visualization.
Figure 9: Learned representations in the gridworld domains with the non-augmented objective.
Figure 10: Learned representations when uniformly sampling over the state space. Without theboredom term, the representation does not reflect temporally-extended dynamics. The colors reflectthe distances in terms of the dynamics. They can be seen as quantities proportional to the length ofthe shortest path from the s0 (marked in red) to the represented state.
