Figure 1: The left half of the figure shows key frames and their associated instructions for a task inthe BabyAI environment. The right half depicts a single state from the Crafting environment and itsassociated goal and instruction.
Figure 2: The left diagram depicts the general model architecture used for our approach. Notice howthe policy and encoder can be completed separated from the instruction component for mixed-datatraining or inference. The diagram on the right depicts its implementation for the partial observedenvironments using a GPT-like transformer encoder. The diagram shows our masking scheme atepisode step t: latent vectors from beyond time t are masked from the language decoder.
Figure 3: Snapshots of a rollouts from an oracle agent and our trained agents on the same unseentask in BabyAI. Our agent is able to predict instructions, given below each image, with high fi-delity. Additionally, we see that our learned agent employs a different exploration strategy, but stillcompletes the task exhibiting strong generalization.
Figure 4: Data scaling with and without instrUcUtions.
Figure 5: Architecture figure for the fully observed case. Observations are tokenized and then inputto a Vision-Transformer like encoder.
