Figure 1: TempoRL: A temporal prior is trained on task-agnostic trajectories. Actions are then sampled froma dynamic mixture between a state-independent temporal prior and the policy in downstream learning of morecomplex tasks. Our method works with both vector-based and image-based state inputs.
Figure 2: Graphical models representing different priors: from left to right, a behavioral prior, a generaltemporal prior, a state-independent temporal prior and a single-step state-independent temporal prior. Solidarrows represent the environment’s transition function T, while dashed arrow indicate conditional modeling.
Figure 3: Overview of environments used in our experimental validation.
Figure 4: Comparison of downstream performance of behavioral and temporal priors when conditioned onactions, states and combinations thereof.
Figure 5: A qualitative comparison of sampled explorationtrajectories in a 2D room. Our method achieves directedbehavior while covering most of the state space. SAC andSAC+AR(2) fail to cover the full state space, while SAC-PolyRL fails to reach distant areas consistently.
Figure 6: Accelerating downstream RL. While other methods are mostly competitive on training tasks(reach and room-maze), TempoRL is able to accelerate RL for unseen tasks (door-close, door-open,window-close, window-open, u-maze).
Figure 7: Transfer of a one-stepstate-independent temporal priorto Visual RL. TempoRL (red)compared with SAC (brown) andPARROT (orange).
Figure 8: Comparison between the full method and its simplified version. The simplified version does notmodify SAC’s update rules.
Figure 9:	Left: performance on additional downstream tasks from MT10 (Yu et al., 2020). Right: meanperformance over 9 out of 10 tasks in MT10 (peg-insert-side does not allow hindsight relabeling).
Figure 10:	Performance on an additional maze structure (complex-maze). Settings are identical to thosepresented in Section 5.3.
Figure 11:	Performance on tasks derived from swimmer-v2. Temporal priors are capable of guiding explo-ration even when states are corrupted by noise.
Figure 12:	Performance on downstream tasks when modeling a one-step state-independent temporal priorthrough various generative models.
Figure 13:	Performance on downstream tasks when disabling or enabling HER, with different horizons forn-step returns.
Figure 14:	Performance on SAC+AR(n) for different values of n.
Figure 15: A qualitative comparison of sampled exploration trajectories in a robot manipulation environment(reach). Our method achieves directed behavior while covering most of the state space, outperforming SAC-PolyRL. On the other hand, uniform sampling (SAC) and action-repeat (SAC+AR(2)) fail to cover the full statespace.
Figure 16: Comparison of methods on window-open with image-based states: TempoRL (red), SAC(brown) and PARROT (orange).
