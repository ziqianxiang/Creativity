Figure 1: Accuracy (y-axis) of existing meta-learners on the hardest and the easiest episode acrossstandard few-shot datasets and meta-learners (x-axis). Note that there is a wide gap of â‰ˆ 50%between the prediction performance on the easiest and hardest episode.
Figure 2: Semantic Properties of Hard and Easy Episodes: (a) Hard episode, Class: electric-guitar;(b) Easy episode, Class: mixing-bowl; The images marked in red borders are misclassified queryexamples.
Figure 3: Accuracy of easy and hard episodes (y-axis) during the course of meta-training acrossdifferent epochs (x-axis); Hard episodes often have a final accuracy less than the maximum accu-racy reached during meta-training. The meta-learner used is prototypical networks with a Conv-4backbone trained using episodes with 5-shot, 5-way.
Figure 4: Hard episodes have a wider gap between the final accuracy (y-axis) and the maximumaccuracy reached during meta-training (x-axis), in comparison to easy episodes. This behaviour ismore pronounced for mini-ImageNet and tieredImageNet.
Figure 5: (a) Total number of local forgetting events (y-axis) across different thresholds (x-axis)during the course of meta-training; (b) Total number of local forgetting events during the first 20epochs of meta-training; (c) Total number of local forgetting events during the last 20 epochs ofmeta-training. The number of local forgetting events is higher for hard episodes in comparison tothe easy episodes across different thresholds.
Figure 6: Loss (x-axis) vs. Accuracy (y-axis) plot for miniImageNet.
Figure 7: Loss (x-axis) vs. Accuracy (y-axis) plot for CIFAR-FS.
Figure 8: Loss (x-axis) vs. Accuracy (y-axis) plot for tieredImageNet.
Figure 9: Distribution of episode hardness (difficulty) for mini-ImageNet across different meta-learners. We find that the distribution of hardness approximately follows a gamma distribution.
Figure 10: Accuracy of hard and easy episodes (y-axis) during the course of meta-training acrossdifferent epochs (x-axis). Different colors signify different episodes.
Figure 11: For a set of 25 hard and easy episodes, we report the mean of the maximum accuracyreached during the course of meta-training and the accuracy at the end of meta-training. For hardepisodes, we observe a substantial gap denoting global forgetting.
Figure 12: Transferability of episode hardness for tieredImageNet.
Figure 13: Transferability of episode hardness for mini-ImageNet.
Figure 14: Transferability of episode hardness for CIFAR-FS.
Figure 15: Visual semantic properties of hard episodes: We find that if there are closely relatedfine-grained categories in an episode (e.g, Malamute vs. Dalmatian), the meta-learner (prototypicalnetworks + ResNet-12) often gives a wrong prediction.
Figure 16: Distribution of query losses for tieredImageNet and CIFAR-FS across Prototypicalnetworks, R2D2 and MetaOptNet (SVM).
Figure 17: Forgetting events for R2D2 and MetaOptNet across mini-ImageNet and tieredImageNetdatasets.For R2D2 we use the Conv-4 backbone, while for MetaOptNet we use ResNet-12.
Figure 18: Distribution of query losses for in-distribution episodes (Red) and OOD episodes (Blue)for Prototypical Networks with a Conv-4 backbone.
