Figure 1: Qualitative and quantitative illustrations of spatiotemporal mode collapse on the large-scale, real-world RoboNet dataset. (Left) RoboNet contains complicated dynamics modes in videoscollected in various environments. The prediction results collapse to blurry motions due to theincompatibility of learning various dynamics modes. (Right) Unlike the prior art (GUen & Thome,2020) that performs better when separately trained in the subset of each environment (denoted asâ€œSP"), the proposed ModeRNN manages to benefit from large-scale learning in all environments.
Figure 2: ModeCell tackles spatiotemporal mode collapse via a decoupling-aggregation frameworkbased on mode slots. Multiple ModeCells are stacked to form the complete architecture of ModeRNN.
Figure 3: (a) Demonstration of STMC on RoboNet using an existing video prediction model fordisentangling visual dynamics (Guen & Thome, 2020). (b) The slot bus in ModeRNN learns distinctrepresentations for samples from different environments. (c) The four slots in ModeRNN learndecoupled features for various spatiotemporal modes. (d) The importance weights of mode slotsrespond differently to different data environments in RoboNet, i.e., families of similar video sequences.
Figure 4: Showcases of future prediction on RoboNet in (Top) action-free and (Bottom) action-conditioned setups. These examples are randomly sampled from the Stanford environment. Weprovide examples for other data collection environments in Appendix A.
Figure 5: Examples of (Top) action-free and (Bottom) action-conditioned video prediction from theGoogle environment.
Figure 6: Examples of (Top) action-free and (Bottom) action-conditioned video prediction from theBerkeley environment.
Figure 7: Examples of (Top) action-free and (Bottom) action-conditioned video prediction from thePenn environment.
Figure 8: Examples of predicted future frames on the Human3.6M dataset.
Figure 9: Examples of predicted future frames on the Mixed Moving MNIST dataset.
Figure 10: t-SNE visualization on the Mixed Moving MNIST dataset. (a) Illustration of STMC onexisting approach. (b) Different mode slots learn different components of visual dynamics. (c-d) Theimportance weights and slot bus show discriminative representations, which implies the ability tolearn less blurry motions and thus alleviates STMC.
Figure 12: Demonstration of spatiotemporal mode collapse on the KTH dataset using A-distance. (a)A-distance based on the memory states, i.e., Bt for MOdeRNN. (b) A-distance based on the outputstates, i.e., Ht for ModeRNN).
Figure 13: (a, c) Illustration of STMC on the existing ConvLSTM model on KTH and radar echodataset of Guangzhou (GZ). (b, d) The slot bus of ModeRNN shows discriminative representations ondifferent groups of video dynamics. The two groups in KTH respectively correspond to subtle handmotion (e.g., hand-waving, hand-clapping, and boxing) and more global body motion (e.g., running,walking, and jogging). The two groups in Radar are divided by different seasons.
Figure 14: Examples of predicted future frames on the radar echo dataset.
Figure 15: t-SNE visualization of the SA-ConvLSTM hidden states on the RoboNet dataset.
