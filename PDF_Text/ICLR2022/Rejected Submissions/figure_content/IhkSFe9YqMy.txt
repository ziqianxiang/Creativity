Figure 1: In the HalfCheetah-v2 environment, the state of the agent in different trainingstages is compared. The abscissa means the different positions on the agent, and the ordinatemeans the collected attitude information at the specific position, the difference is obvious atposition s1, s8, s12 and s14. Different colors of legend indicate different Q values.(a) Startcollecting at 2e4.(b) Start collecting at 5.6e5.
Figure 2: Compare the sampling results by using the linear distribution sampling functionand the uniform distribution sampling function. The left picture shows the sampling resultby linear distribution, and the right picture shows another. We sampled 100 samples in[0, 20].
Figure 3: Samples of Mujoco tasks. In order from the left: HalfCheetah-v2, Swimmer-v2,Walker2d-v2, Hopper-v2.
Figure 4: Performance curves of the Agents using DDPG and DDPG with ERM: DDPG(red), DDPG with ERM (green).
Figure 5: Performance curves for a selection of domains using TD3 and TD3 with ERM:TD3 (cyan), TD3 with ERM (yellow).
Figure 6: Performance curves for a selection of domains using SAC and SAC with ERM:SAC (purple), SAC with ERM (blue).
Figure 7: Performance curves for HalfCheetah-v2 task using different algorithms: (a)DDPG(red), DDPG with ERM (blue), TD3 (cyan), TD3 with ERM (yellow), SAC (purple), SACwith ERM (green) (b) SAC (green), DDPG with ERM (red).
Figure 8: The overall structure diagram of the Experience Replay More (ERM) algorithm.
