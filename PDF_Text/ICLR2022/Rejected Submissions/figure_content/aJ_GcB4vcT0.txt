Figure 1: Sketch of Algorithm 1 (Section 3.1). The symbolic encoder is initially fully neural. We alternatebetween VAE training with the program architecture fixed (Step 1), and supervised program learning to increasethe depth of the program by 1 (Step 2). Once we reach a symbolic program, we train the model one last time tolearn all the parameters. The color (in terms of lightness) of the symbolic encoder corresponds to the encoderbecoming more symbolic over time.
Figure 2: Our DSL for sequential domains, similar to the one used in Shah et al. (2020). x,㊉，and ㊉θ representinputs, basic algebraic operations, and parameterized library functions, respectively. fun x.e(x) represents afunction that evaluates an expression e(x) over the input x. selS selects a subset S of the dimensions of theinput x. mapaverage g x applies the function g to every element of the sequence x and returns the averageof the results. We employ a different approximation of the if -then-else construct.
Figure 3: (a) Trajectories in synthetic training set. Initial/final positions are indicated in green/blue. Redlines delineate ground-truth classes, based on final positions. (b) k = 2 learned binary programs using ouralgorithm. The first program (top) thresholds the final x-position while the second program (bottom) thresholdsthe final y-position. (c, d, e) Neural latent variables reduced to 2 dimensions. Top/bottom rows are coloredby final x/y-positions respectively (green/yellow is positive/negative). (c) Clusters in TVAE neural latent spacecorrespond to 4 ground-truth classes. (d) After learning the first program, the neural latent space containsclusters only based on the final y-position. (e) After learning the second program, all 4 ground-truth classeshave been extracted as programs and the remaining neural latent space contains no clear clustering.
Figure 4:	Learned programs on CalMS21. The subscripts represents the learned weights and biases, in partic-ular, the brackets contain the weights for the affine transformation followed by the bias.
Figure 5:	Applying symbolic encodersfor self-supervision. “Features” is baselinew/o self-supervision. “TREBA” is a self-supervised approach, using either expert-crafted programs or our symbolic encodersas the weak-supervision rules. The shadedregion is std dev over 9 repeats. The std devfor our approach (not shown) is comparable.
