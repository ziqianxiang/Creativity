Figure 1: Examples of novel skills learnt by ROEL. Images are generated through policy rolloutunder a fixed skill z specified at the beginning of each episode.
Figure 2: ROEL and DADS solving the same out-of-sample EE. ROEL is able to leverage its knowl-edge of previous tasks to rapidly cross a gap and climb the hill, whilst DADS meanders around thegap.
Figure 3: Dispersion of skill returns using the original bipedal extrinsic reward function. Each skillsample has been fitted to a normal distribution and assigned a unique color.
Figure 4: Comparison of the in and out-of-sample performance of the skill-dynamics network, sam-pled from 5 random discrete priors. The average error between the predicted and actual observationis plotted and enclosed by a shaded region out to 1-SD. Note the different y-axis scale in each graph.
Figure 5: Components of the bipedal walkerkept straight. The episode terminates if it reaches the finish position (xf inish) or falls over andcollides with the ground. All terms are normalized by a scale variable to ensure reward behavior isgenerated correctly regardless of the rendering viewport size.
