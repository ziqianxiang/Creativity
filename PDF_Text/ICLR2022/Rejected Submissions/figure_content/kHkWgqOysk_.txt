Figure 1: Realistic Semi-SupervisedLearning may simultaneously containunlabeled ID and OOD data. ID datacome from the same classes as labeleddata while OOD data come from classesthat are not seen in labeled data.
Figure 2:	Analysis of pre-trained model. (a) Histogram of the computed confidence scores c(x) overID and OOD data. (b) On ID data, pseudo-label distribution is balanced since they share the samedistribution with the labeled data. (c) On OOD data, the pseudo-label distribution is imbalanced.
Figure 3:	(a) PL model degrades as the mismatch ratio increases. (b) Confusion matrix of PL modelwhen the mismatch ratio = 100%. It demonstrates that the imbalance of pseudo labels on OOD dataaffects PL’s performance. A lot of ID samples with class 1-5 are misclassified into class 0. Also, asthe PL process goes on, the imbalance pseudo-labels on OOD data get even worse.
Figure 4: Four strategies of how to label OOD data. Different shapes represent different groundtruths. Data with the same color are labeled as the same classes. A shape with a solid outline meansit is an ID sample while OOD data are represented with dashed line. (a) No Labeling acts as abaseline where OOD data are omitted. (b) Re-Assigned Labeling re-labels OOD data to certain IDclasses. (c) Open-Set Labeling labels all the OOD data as a unified class. (d) Oracle Labeling usesthe ground truths of OOD data. Test accuracy under 100% class mismatch ratio is reported.
Figure 5: Illustration of Y-Model and its two main branches. (b) is the main structure of Y-Modelwhere wejudge by the ID confidence if certain Unlabeled data belongs to ID classes or not. The datawith high confidence will perform Re-balanced Pseudo-Labeling, while those with low confidencewill get their pseudo-labels by Semantic Exploration Clustering. (a) Re-balanced Pseudo-Labelingtruncates the number of pseudo-labeled data to the minimum, making the pseudo-labels balancedand filtering out OOD data. (c) Semantic Exploration Clustering simulates the process of learningfrom ground truth labels of OOD data, creating pseudo-labels on extra K classes by clustering.
Figure 6: Comparison with existing methods on CIFAR10 and SVHN dataset with Wide-ResNet-28-2 network. Class mismatch ratios are varied. The shaded regions with the curves indicate thestandard deviations of the accuracies over five runs. (a) (b) Comparison with traditional SSL meth-ods. These methods suffer from performance degradation as the mismatch ratio increases. (c) (d)Comparison to two existing class-mismatched SSL methods - UASD and DS3L. Our methods per-form better in almost all the experimental setups.
Figure 7: Three experiments on CIFAR10 benchmark to validate the functionality of RPL and SEC.
Figure 8: t-SNE visualization of supervised baseline, vanilla PL and Υ-Model on CIFAR10’s test set.
