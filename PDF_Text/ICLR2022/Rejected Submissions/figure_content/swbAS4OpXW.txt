Figure 1: Diagram of one-shot generative domain adaptation. Left: The overall framework,where a GAN model pre-trained on the large-scale source data is transferred to the target domainwith only one training sample. A lightweight attribute adaptor and attribute classifier are introducedto the frozen generator and discriminator respectively. Right: Realistic and highly diverse synthesisresults after adapting the pre-trained model with different targets. Here, our efficient design managesto transfer both semantic attributes and artistic styles within a few minutes.
Figure 2: Qualitative comparison on one-shot adaptation between FreezeD (Mo et al., 2020),Cross-Domain (Ojha et al., 2021), and our proposed GenDA. The first column shows the referenceimages. GenDA significantly outperforms the other competitors from the diversity perspective.
Figure 3: One-shot adaptation with different target samples. GenDA manages to capture therepresentative characters from the given reference, such as sunglasses, artistic style for faces, andvegetation, pyramid material for churches.
Figure 4: Transferring the “common” semantic of more than one reference image. On theleft are the training samples, while on the top are the samples synthesized before adaptation. In theremaining rows, images from the same column are produced with the same latent code. Under thesettings of two-shot adaptation (i.e., the last two rows), we can tell that (1) in the second last row,all people are wearing eyeglasses (common attribute of the two references) but they have the samegender (divergent attribute) as the original synthesis in the top row. (2) similarly, in the bottom row,all people are female yet the “eyeglasses” attribute is preserved from the original synthesis.
Figure 5: Cross-domain adaptation, where GenDA manages to transfer the character of an out-of-domain target (first column) to the source domain.
Figure A1: Analysis on the transformation vector for domain adaptation, which is noted as a inEq. (3). We launch 15 one-shot adaptation experiments with 5 kids, 5 sunglasses, and 5 sketches asthe target, respectively. After the model converges, we collect the transformation vector learned bythe attribute adaptor, and then perform PCA on all 15 vectors. We can tell that our GenDA tends tolearn similar transformation regarding the reference with similar characters. The 15 training samplesare visualized together with the PCA results.
Figure A2: Visualization of transformed latent spaces after domain adaptation. Each clustercorresponds to a particular training set. Left: The latent spaces learned with different referenceimages are clearly separable. Right: When we use two reference images, which have commonattributes, as a combined training set, the transformed representations tend to locate at the overlappedregion between the representations learned from each reference image independently.
Figure A3: Latent representation comparison before and after adaptation. The latent space isclearly pushed from the red cluster to the blue cluster when there exists an obvious gap between thesource and the target domains.
Figure A4: Comparison with the style transfer task. GenDA could transfer the characteristic ofthe target image to the source model in more harmonious way, while SWAG (Wang et al., 2021)transfers the color inconsistently, leading to obvious artifacts.
Figure A5: Visual relation of one-shot adaptation with different target samples. Here, for eachmodel (i.e., faces and churches), each column is generated with the same latent code. Our GenDA isable to transfer the most distinguishable attributes of the target domain, but almost maintains othersemantics (e.g., the face pose and the church shape).
Figure A6: Visual relation of cross-domain adaptation. Here, for each model (i.e., faces andchurches), each column is generated with the same latent code. We can tell that, under the cross-domain setting, our GenDA can still inherit some characteristics from the source model (e.g., theface pose and the church shape).
Figure A7: Semantic interpolation with the model pre-trained on FFHQ human faces (Karraset al., 2019). The first and last columns are the synthesized images with two different latentcodes after one-shot domain adaption, while the remaining columns are the outputs by linearlyinterpolating the two codes. All intermediate results are still high-quality faces, with graduallyvarying semantics (e.g., the gender and the face pose).
Figure A8: Semantic interpolation with the model pre-trained on LSUN Churches (Yu et al.,2015). The first and last columns are the synthesized images with two different latent codes afterone-shot domain adaption, while the remaining columns are the outputs by linearly interpolating thetwo codes. All intermediate results are still high-quality churches, with gradually varying semantics(e.g., the church shape and the cloud in the sky).
