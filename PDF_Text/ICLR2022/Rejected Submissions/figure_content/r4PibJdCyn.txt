Figure 1: Model architecture of TotalRecall. Our framework will make to dot product φθ (u, i)converge to the logarithm of the joint probability p(u, i), and thus can be directly applied in RS toemulate the conditional probability p(i|u), and also in PUMS to replace p(u|i).
Figure 2:	Comparison of results ofMF given by Bernoulli distribution and Multinomial distribution.
Figure 3:	Comparison of the converging speed and diversity of MF modeled with Bernoulli distri-bution (MF) and Multinomial distribution (TR). The x-axis is the data throughput, i.e., the numberof records consumed by the model, and the y-axis is the metric measured on the test data. ForMovie-lens, TR is about 16× times faster, and for Pinterest it is about 10× times.
Figure 4: Comparison of accuracy and diversity metrics of different methods on Amazon booksand Taobao data. HitRate@50 and Recall@50 measure the accuracy, and Distinct items@50 andDistinct Cates@50 measure the diversity. ‘* Jemp' means the users and items representations U andi are l2-normalized, and then their dot product is rescaled with temperature τ = 0.067. By thisway, the model converges much faster to higher accuracy. As for diversity, Bi-InfoNCE surpassUni-InfoNCE and SSM by a large margin.
Figure 5: Results of Next-n-prediction for Amazon data. The improvement of accuracy is substantialwhen n <= 5.
Figure 6: KL Divergence between two marginal distributions of items: PteSt(i) of test data andPdata(i) of next-n training data. Although they are not the full distributions of u and i, to a certainextent they can reveal the discrepancy between the distributions of the training data and the test data.
Figure 7: Comparison of the converging speed between MF and TR. The x-axis is in log scale. Wecan see that TR can obtain better results much faster.
