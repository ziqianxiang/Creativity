Figure 1: The proposed causal diagram of DNN model. (a) Examples of DNNs with convolutionlayers. (b) A graphical abstraction of the DNNs architectures to a simple DAG graph. Each node uirepresents one channel in a hidden layer. (c) Illustrates the proposed causal diagram on an exampleof a CNN with 2 hidden layers, each composed of one convolution unit.
Figure 2: Causal effect of interventions on LeNet parameters over each layer for 3 and 8 digits usingone test inputFigure 3: Distribution of effects of intervention variables over LeNet layers.
Figure 3: Distribution of effects of intervention variables over LeNet layers.
Figure 4: Global explanations and comparison between digits 3 and 8 using 100 samples. (a) and(b) show the frequency of causal filters causal filters in conv1 and conv2 layers, respectively. (c)and (d) show the frequency of causal parameters in fully connected layers. The x-axis indicate theindices of causal parameters.
Figure 5: Causal parameters corresponding to the true class for LeNet when tested on negative sam-ples. The figure on the right shows the original predictions of the model (before) and the correctedone& Fergus (2014)), IG (Sundararajan et al. (2017)) and DeepLift (??). We also wanted to includethe popular LRP (Binder et al. (2016)) method, however, this method doesn’t work on complexarchitectures such as ResNet because the method doesn’t handle the skip connections. Figure 6shows results on MNIST digits using LeNet model. Comparing the explanations generated fordigits 3 and 8, we can see that other methods don’t provide informative explanations to recognizethe digit 3 from 8. Our method shows the effects of the causal filters which explain the differencebetween digits through their shapes. In Fig. 7 we show causal explanations of ResNet18 model onarbitrary classes of ImageNet. Our method captures visual explanations from all layers. The causalfilters capture different meaningful concepts (semantics) at multiple levels of hierarchy. We showhere results from some layers and illustrate further details in Appendix C.
Figure 6: Visual attributions from different methods. Comparison of visual explanations between 3and 8 digits. Our method generates visual explanations by integrating the response of the mediatorscorresponding to the causal filters in each conv layer.
Figure 7: Visualizations and qualitative comparisons. (a) Examples of ImageNet classes. (b) Visualeffects (explanations) of causal parameters of some hidden layers of ResNet18. (c) Saliency mapsobtained by attribution-based methods.
Figure 8: A toy example. MLP with one hidden layer, one input and a linear outputB AppendixOur method applied to ResNet18 trained on MNIST dataset. The model achieved almost perfectperformance on all digits of the MNIST test set. The average performance was 99%, so it wasrare to find negative samples on which the model wrongly predicted the class. We found only onenegative sample corresponding to digit 8, and no negative samples for digit 3. Fig. 9 illustratesexplanations computed from two selected convolution layers. We show in a human interpretableway what are the causes for a wrong prediction and what treatment can fix it.
Figure 9:	Using causal explanations to correct wrong predictions of ResNet18 for MNIST exmaple.
Figure 10:	Visualization of causal effects in all convolution layers for the class object Dalmatien.
Figure 11: Visualization of causal effects in all convolution layers for the class object Merle.
Figure 12: Visualization of causal effects in all convolution layers for the class object Roslein.
Figure 13: Visualization of causal effects in all convolution layers for the class object Butterfly.
