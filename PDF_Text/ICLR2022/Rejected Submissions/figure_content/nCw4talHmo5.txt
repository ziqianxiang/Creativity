Figure 1: Different ways of distributing a neural network (A) over two devices: sequential distribu-tion (B), parallel distribution with communication (C), and parallel distribution without communi-cation (D). Bold red arrows indicate communication over the transmission network.
Figure 2: Illustration of ParaDiS model consisting of4 switches. While white circles denote the neu-rons, gray circles denote either network inputs (e.g., image pixels) or its outputs. All the parametersare shared between switches (except for batch normalization statistics) and trained jointly.
Figure 3: Visualization of the ParaDiS network training scheme, also described by Alg. 1.
Figure 4: Complexity in MFLOPs vs accuracy of MobileNet v1 trained on Imagenet for differ-ent switches (US-Nets and ParaDiS). The vertical dotted lines indicate the complexity of specificswitches. The diamond points are individual models trained for a particular switch. The square bluepoints are switches explicitly trained, while the empty blue circle is never trained, only tested.
Figure 5: Complexity in MFLOPs vs accuracy of ResNet-50 trained on Imagenet for differentswitches (US-Nets, Slimmable and ParaDiS). The vertical dotted lines indicate the complexity ofspecific switches. The diamond points are individual models trained for a particular switch. Thesquare blue points are switches explicitly trained, while the empty blue circle is never trained, onlytested.
Figure 6: A graphical representation of Slimmable and ParaDiS switches. For each switch, thesurface of the blue area is roughly equal to the relative number of parameters, as compared to thefull 1.0× model, and the relative number of FLOPs.
Figure 7:	Complexity in MFLOPs vs PSNR of different WDSR networks (US-Nets, Slimmable andParaDiS) trained on DIV2K on the task of bicubic ×2 image super-resolution. The vertical dottedlines indicate the complexity of specific switches. The diamond points are individual models trainedfor a particular switch.
Figure 8:	Impact of of IPKD and distilling from a wide network for MobileNet v1. These resultscorrespond to 3 rows from Table 1.
Figure 9:	Impact of distilling the activation for MobileNet v1. These results correspond to 3 rowsfrom Table 1.
Figure 10: Impact of distilling the activation for ResNet-50. These results correspond to those fromTable 2.
Figure 11: Complexity in Millions of FLOPs vs accuracy of ResNet-50 trained on Imagenet with 5switches (orange curve): [1.2]×, [1.0]×, [0.5, 0.5]×, [4 × 0.25]×, and [8 × 0.125]×, versus ResNet-50 trained on 4 switches (blue curve). The vertical dotted lines indicate the complexity of specificswitches. The diamond points are individual models trained for a particular switch. The squarepoints are switches explicitly trained, while the empty circles are never trained, only tested.
Figure 12: Complexity in Millions of FLOPs vs accuracy of MobileNetv1 trained on Imagenetwith 8 switches. The vertical dotted lines indicate the complexity of specific switches. The diamondpoints are individual models trained for a particular switch. The square points are switches explicitlytrained, while the empty circles are never trained, only tested.
