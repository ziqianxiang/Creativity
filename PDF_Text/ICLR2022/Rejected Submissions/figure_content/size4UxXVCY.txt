Figure 1: Comparison of the human brain and graph tree neural networklobe, and the auditory nerve begins with the auditory cortex of the temporal lobe; this means thatthe two paths have different starting points. (C-ii) A graph can describe the relationship dependingon whether all nodes are connected, but it is not easy to define the start and end points. A tree isa data structure that can define direction with the leaf and root nodes. On the other hand, a treestructure cannot express the relationship between sibling nodes. (C-iii) Information from differentsensory organs is gathered in the association area. Typically, the posterior parietal cortex(Malachet al., 1995) is located at the top of the head as part of the parietal lobe, and sensory informationsuch as vision and hearing is fused and interpreted; Then, information is coordinated in the frontallobe and humans act. (C-iv) There is also an over-fitting problem in DNN(Widrow et al., 1960), andvarious attempts have been made to solve this in (Srivastava et al., 2014). In DNN, simply deeperlayers lead to an over-fitting problem(Dai et al., 2017). In particular, NasNet(Zoph et al., 2018) hasattempted to design a layer automatically through reinforcement learning and RNN. This means thatthere is an appropriate network depth depending on the complexity of the dataset; we want to designa network that could adjust the depth of the layers according to the complexity of each data, not thedata set. (C-v) Also, the number of human neurons is about 85 billion(Herculano-Houzel, 2009),which is difficult to express in a network. Therefore, GTNNs are designed to solve these problems.
Figure 2: Graph Tree Architecture2.1	Graph Tree Data StructureThe graph tree (GT) is a data structure that can express relational and hierarchical information.
Figure 3: Depth First Convolution & DeconvolutionThis section introduces two propagation methods of depth-first convolution (DFC) (a convolutionmethod for traversing all nodes of GT) and depth-first deconvolution (DFD). Depth-first search (DFS)is a search algorithm for the tree data structure. Unlike general recursive convolution, DFC and DFDare methodologies for learning children’s relationships, feature extraction networks together.
Figure 4: Graph tree datasets of the experimentsimportant point is not limited to any specific architecture; later, this GT could be modified accordingto the task or complexity.
Figure 5: Test Accuracy (The Experiment 1)(c) Fine-Tuningneeds to be more trained, and CNN has an overfitting problem. The reason is that each network hasdifferent epochs to have optimal performance, and setting learning parameters is more complex thanlearning individually.
Figure 6: Used ModelsTable 4: Feature Extraction Networks for Image datasetModel	LeNet-5				LeNet-5 for GTNN					In	Out	kernel	stride	activation	In	Out	kernel	stride	activationConv2D	1	6	(5,5)	1	tanh	1	6	(5,5)	1	tanhAvgPool2D	-	-	(2,2)	1	-	-	-	(2,2)	1	-Conv2D	6	16	(5,5)	1	tanh	6	16	(5,5)	1	tanhAvgPool2D	-	-	(2,2)	1	-	-	-	(2,2)	1	-Conv2D	16	120	(5,5)	1	tanh	16	120	(5,5)	1	tanhFC layer 1	120	84	--	tanh	-	-	-	-	-FC layer 2	84	10	--	softmax	-	-	-	-	-Zero padding	-	-	--	-	120	128	-	-	-Final	-	10	--	-	-	128	-	-	-LeNet-5(LeCun et al., 1989) was used as the image feature extraction network. We create a dimensionof 128 by applying zero padding to the extracted features without using the affine-layer(FC layers)and then forward it to GTNN.
Figure 7: Model structure & Data structure 1(c) RNN(recurrent) & GTThese architectures are the models underlying existing networks. First, Fig.7(a) means a multi-layerperceptron model. If we express this architecture as GT and learn with GTC, which has input only inleaf node, it becomes the same operation process.
Figure 8: Model structure & Data structure 2(b) RNN(recursive) & GTThese models are architectures that learn relational and hierarchical information. First, Fig.8(a) meansa GNN model. The same delivery process is obtained if the relationship and inputs are expressed asrelationship and inputs among sibling nodes of the graph tree with GTC.
Figure 9: Model structure & Data structure 3(b) GT contained various dataThese networks are structures in which each feature extraction process is combined. If we designfeature extraction networks for each type and construct a graph tree with two or three childrenFig.9(a,b), it will be the same delivery process.
Figure 10: Ideal GTFinally, this structure is an ideal graph tree structure (Fig.10). The feature extraction network cor-responds to the sensory organ. And the feature extraction process(Ψ) is selected according to thetype(τ ) of data x(x), and x becomes vector -→x .
Figure 11: Graph tree datasets of the experiment 316Under review as a conference paper at ICLR 2022(a) Feature Extraction Networks(b) Learning plot (49class-Test acc)Figure 12: Feature Extraction Networks & Learning plotAs a third experiment, we learned datasets of various structures with GTNN. Data in the form ofimages, sound data, language data(Socher et al., 2013) in the tree structure, and data in the graphstructure(Dou et al., 2021) were learned at the same time, and the results are as follows. All of theperformances at this time were similar to those of existing networks.
Figure 12: Feature Extraction Networks & Learning plotAs a third experiment, we learned datasets of various structures with GTNN. Data in the form ofimages, sound data, language data(Socher et al., 2013) in the tree structure, and data in the graphstructure(Dou et al., 2021) were learned at the same time, and the results are as follows. All of theperformances at this time were similar to those of existing networks.
