Figure 1: Theoretical predictions closely match the true learnabilities of arbitrary eigen-functions on diverse input spaces. Each plot shows learnability L(φk) (Equation 11) of severaleigenfunctions as a function of training set size n. Theoretical curves show excellent agreement withresults from exact NTK regression (triangles) and finite nets trained via gradient descent (circles).
Figure 3: Eigenfunction learnability always sums to the size of the training set. Stacked barcharts show D-learnability for a particular random D for each of the 10 eigenfunctions over a simple10-point domain. For all architectures, the total height of each bar is approximately n. (A) As perTheorem 1, the summed D-learnabilities for exact NTK regression (left bars) are all exactly n, andthose for trained finite nets (right bars) are remarkably close. Stacked bars show D-learnability forthe 10 eigenfunctions, all from the same training set D of n = 3 data points, stacked from top tobottom in descending order of eigenvalue. A different network architecture was used in each of thefour pairs of columns. As per Lemma 1d, the height of each eigenmode contribution falls in [0, 1].
Figure A1: Our theory correctly predicts that, for low-eigenvalue eigenfunctions, MSE Coun-terintuively increases as points are added to a small training set. (A-C) Generalization MSE ofexact NTK regression (triangles) and finite networks (circles) when learning four different eigen-modes on each of three different domains given n training points. Theoretical curves closely matchexperimental data. Eigenmodes with higher k have lower eigenvalues and thus higher mean MSEs,and for k ∈ {2, 4, 6}, MSE even increases as n increases from zero. Dashed lines show dE/dn|n=0as predicted by Equation 16.
Figure B2:	Theoretical learnability predictions remain accurate even for quite narrow net-works of moderate depth. Plots show learnability vs. training set size for four eigenmodes of the8d hypercube, learned with a 4L ReLU net with various widths. Except for changing width, theseexperiments are identical to those of Figure 1b. Theoretical predictions (solid curves) are the samein all plots. Dashed lines show learnability from a naive, nongeneralizing model; points below theline imply worse-than-chance generalization (see Corollary 1). (A) Infinite-width results using ex-act NTK regression. (B-F) Results for successively narrower finite networks. As width decreases,mean learnability increases slightly, and 1σ error grows. Despite this, mean learnabilities remainremarkably close to our theoretical predictions even at width 20.
Figure B3:	Theoretical MSE predictions remain accurate even for quite narrow networks.
Figure F4: For NTK eigenvalues on the 7-sphere, Var [C(φ,φ)]《C for large n. The statisticVar [C(φ,φ)] /C → 0 is calculated for various n using 100 samples of Φ, φ with the 7-sphereeigenvalues up to k = 6.
Figure I5: Eigenvalues decrease with increasing k for all three domains, the manifestation ofan inductive bias towards smoother functions. All eigenvalues are calculated with the NTK ofa 4L ReLU network as described in the text. (A) NTK eigenvalues for k for the discretized unitcircle (M = 256). Eigenvalues decrease as k increases except for a few near exceptions at highk. (B) NTK eigenvalues for the 8d hypercube. Eigenvalues decrease monotonically with k. (C)NTK eigenvalues for the 7-sphere up to k = 70. Eigenvalues decrease monotonically with k . (D)Eigenvalue multiplicity for the discretized unit circle. All eigenvalues are doubly degenerate (dueto cos and sin modes) except for k = 0 and k = 128. (E) Eigenvalue multiplicity for the 8dhypercube. (F) Eigenvalue multiplicity for the 7-sphere.
