Figure 1: Context encoder as a stack of attention blocks.
Figure 2: Attention modules for task inference.
Figure 3: Inter-task matrix-form momentum contrast.
Figure 4: Left: Test-task performance vs. transition steps sampled for meta-training. Right: t-SNEvisualization of the learned task embeddings zq on Point-Robot-Wind and Sparse-Ant-Dir. Eachpoint represents a query vector which is color-coded according to its task label.
Figure 5: Result on the relabeled Sparse-Point-Robot dataset. (a) State distributions of the expertdatasets for 20 distinct tasks, with goals uniformly distributed on a semicircle. (b) On mixed dataset,FOCAL completely fails in this scenario whereas FOCAL++ variants with batch-wise attention areable to learn. (c) Probability distribution of the batch-wise attention weight of samples with absolutelyzero and non-zero reward. Binary classification AUC = 0.969.
Figure 6: The variance-sparsity relation for FOCAL++/FOCAL on the relabeled Sparse-Point-Robotdataset. The y-axis measures the variance of the bounded task embeddings z âˆˆ (-1, 1)l averagedover all l latent dimensions. See more details in D.2.
Figure 7: Generating process of the relabeled Sparse-Point-Robot dataset.
