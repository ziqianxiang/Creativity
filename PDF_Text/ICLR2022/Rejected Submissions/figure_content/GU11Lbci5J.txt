Figure 1: Non-scale-free GDv.s. scale-free AdamW onCorollary 1. For quadratic problems f (X) = 1 x> Hx + b>x + c,with H diagonal and positive definite, any scale-free algorithmwill not differentiate between minimizing f and f(x) = 1 x>X +H-1 b> x + c. As the condition number of f is 1, the operation, andmost importantly, the convergence, of a scale-free algorithm will notbe affected by the condition number of f at all.
Figure 2: The final Top-1 test error on using AdamW vs. Adam-'2 on training a ReSnet/DenseNetwith Batch Normalization on CIFAR10/100 (where the black circle denotes the best setting).
Figure 3: On using AdamW vs. Adam-'2 on training a ReSnet/DenseNet without Batch Normalizationon CIFAR10/100. (Left two) The final Top-1 test error (where the black circle denotes the bestsetting). (Middle two) The training loss and test accuracy curve when employing the initial step sizeand the weight decay parameter that gives the smallest test error. (Right two) The histogram of themagnitude of corresponding updates of all coordinates of the network near the end of the trainingwhen employing the initial step size and the weight decay parameter that gives the smallest test error.
Figure 4: The final top-1 test error of AdamW vs. Adam-'2 on optimizing a 110-layer ReSnet Withbatch normalization removed optimized by AdamW or Adam with the loss function multiplied by 10(left tWo figures) and 100 (right tWo figures).
Figure 5: The final Top-1 test error of using AdamW vs. AdamProx on training (where the black circledenotes the best setting). (Top row) a 110-layer ResNet with Batch-normalization removed on CIFAR-10 (trained for 300 epochs). (Bottom row) a 100-layer DenseNet-BC with Batch-normalizationremoved on CIFAR-100 (trained for 100 epochs).
Figure 6: The final Top-1 test error of usingAdamW vs. AdamProxL2 to train a 110-layerResNet with BN removed on CIFAR10 (where theblack circle denotes the best setting).
Figure 7: The histograms of the magnitudes of all updates (without α) of a 20-layer Resnet with BNremoved trained by AdamW or Adam-'2 on CIFAR10.
Figure 8: The histograms of the magnitudes of all updates (without α) of a 44-layer Resnet with BNremoved trained by AdamW or Adam-'2 on CIFAR10.
Figure 9: The histograms of the magnitudes of all updates (without α) of a 56-layer Resnet with BNremoved trained by AdamW or Adam-'2 on CIFAR10.
Figure 10: The histograms of the magnitudes of all updates (without α) of a 110-layer Resnet withBN removed trained by AdamW or Adam-'2 on CIFAR10.
Figure 11: The histograms of the magnitudes of all updates (without α) of a 218-layer Resnet withBN removed trained by AdamW or Adam-'2 on CIFAR10.
Figure 12: The histograms of the magnitudes of all updates (without α) of a 100-layer DenseNet-BCwith BN removed trained by AdamW or Adam-'2 on CIFAR100.
