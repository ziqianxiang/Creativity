Figure 1: Performance of OC and PU models on synthetic data. Subfigures (a) and (b) show theperformance of the algorithms in the standard case when negative distribution does not change fromtrain to test. Subfigures (c) and (d) show the effects of negative distribution shift. The OC and PUmodels are OC-SVM and PU-SVM, respectively (section 3.2.1). The OC model is trained on labeledpositive examples. The PU model is trained on labeled positive as well as unlabeled examples, i.e. amixture of positive and negative examples. Black dashed lines represent the decision boundaries ofthe optimal Bayesian classifier. Overdependence on unreliable unlabeled data causes the PU methodto misclassify all anomalies when a shift of negative distribution occurs.
Figure 2: Examples of abnormal car (left)and normal car (right) from Abnormal1001.
Figure 3: ROC AUC with respect to the proportion of positive examples in unlabeled data (a) andthe size of unlabeled data (b).
Figure 4: P-ValUes of Mann-Whitney U-test for identification of a) high Î± or b) negative shift. Weshow results for OC-SVM and DROCC as OC models and PU-SVM and PU-DROCC as PU models.
Figure 6: Separating lines of OC and PU algorithms on multimodal synthetic data. Positive distribu-tion is a mixture of Gaussians with four different centers. The OC and PU models are OC-SVM andPU-SVM, respectively. OC-SVM tries to enclose all modes in a single envelope, whereas PU-SVMcorrectly identifies the location of negative data.
Figure 7: ROC AUC with respect to proportion of positive examples in unlabeled data.
Figure 8: ROC AUC for different sizes of available unlabeled data.
