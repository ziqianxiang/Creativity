Figure 1: Text generation processes of Non-STG and STG are described. In the Non-STG model,every token is sampled from the task-specific policy πa (Left). On the other hand, in the proposedSTG model, each token is selectively sampled from either the PLM policy πLM or the test-specificpolicy πa where the selection is performed by the selection policy πs (Right). Symbols with dashedline represent learnable models.
Figure 2: A simple schematic illustration of Non-STG and STG. Non-STG(RL): the whole se-quence of target is generated from the task-specific policy πa so the right sub-sequence AB is alsopenalized from the delayed feedback. STG: the third token is sampled from πa and the model letsthe other tokens (highlighted with cyan) generated from the PLM’s policy πLM which generates anext letter of the previous alphabet input. Here, πa will be penalized at the third token.
Figure 3: Dynamic selection vs Fixed selection.
Figure 4: The learning curve. The Perplexity (PPL) and Score of each task (e.g. Rouge-L forSummarization) are measured on the 0.5% few-shot train set and the valid set.
