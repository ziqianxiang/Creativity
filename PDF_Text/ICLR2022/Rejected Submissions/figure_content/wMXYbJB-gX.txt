Figure 1: Loss on ResNet-18 over CUB-2011label could be “easier” than training smoothed label 1 . Taking the cross entropy loss in (2) for anexample, one need to optimize a single loss function - log exp(fk(w； x))/ PjK=i exp(fj (w； x))when one-hot label (e.g, yk = 1 and yi = 0 for all i 6= k) is used, but need to optimize all Kloss functions - PiK=i yiLS log exp(fi(w； x))/ PjK=i exp(fj(w； x)) when smoothed label (e.g.,yLS = (1 一 θ)y + θK so that yL = 1 一 (K 一 1)θ∕K and yL = θ∕K for all i = k) is used. NeV-ertheless, training deep neural networks is gradually focusing on hard examples with the increaseof training epochs. It seems that training with smoothed label in the late epochs makes the learningprogress more difficult. In addition, after LSR, we focus on optimizing the oVerall distribution thatcontains the minor classes, which are probably not important at the end of training progress. Pleasesee the blue dashed line (trained by SGD with LSR) in Figure 12, and it shows that there is almostno change in loss after 30 epochs. One question is whether LSR helps at the early training epochsbut it has less (eVen negatiVe) effect during the later training epochs? This question encourages us topropose and analyze a simple strategy with LSR dropping that switches a stochastic algorithm withLSR to the algorithm without LSR.
Figure 2: Testing Top-1, Top-5 Accuracy and Loss on ResNet-18 over Stanford Dogs and CUB-2011. TSLA(s) means TSLA drops off LSR after epoch s.
Figure 3: Testing Top-1, Top-5 Accuracy and Loss on ResNet-18 over CIFAR-100. TSLA(s)/TSLA-pre(s) meansTSLA/TSLA-pre drops off LSR/LSR-pre after epoch s.
Figure 4: The estimated values of δ for different data sets.
