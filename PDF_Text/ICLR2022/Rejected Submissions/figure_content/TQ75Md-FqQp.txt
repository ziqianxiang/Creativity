Figure 1: Adding implicit differentiation on top of a ridge regression solver. The function f (x, θ)defines the objective function and the mapping F , here simply equation (4), captures the optimalityconditions. Our decorator @custom_root automatically adds implicit differentiation to the solverfor the user, overriding JAX’s default behavior. The last line evaluates the Jacobian at θ = 10.
Figure 2: Jacobian estimate errors. Em-pirical error of implicit differentiation fol-lows closely the theoretical upper bound.
Figure 3: CPU runtime comparison of implicit differentiation and unrolling for hyperparameteroptimization of multiclass SVMs for multiple problem sizes. Error bars represent 90% confidenceintervals. (a) Mirror descent solver, with mirror descent fixed point for implicit differentiation.
Figure 4: Distilled dataset θ ∈Rk×p obtained by solving (12).
Figure 5: Particle positionsand position sensitivity vectors,with respect to increasing thediameter of the blue particles.
Figure 6:	Proximal gradient fixed point T(x, θ)We recall that when the proximity operator is a projection, we recover the projected gradient fixedpoint as a special case. Therefore, this fixed point can also be used for constrained optimization. Weprovide numerous proximal and projection operators in the library.
Figure 7:	KKT conditions F (x, θ)Similar mappings F can be written if the optimization problem contains only equality constraints oronly inequality constraints.
Figure 8:	Mirror descent fixed point T(X, θ)Although not considered in this example, the mapping V夕 could also depend on θ if necessary.
Figure 9:	Code example for the multiclass SVM experiment.
Figure 10:	Code example for the task-driven dictionary learning experiment.
Figure 11:	Code example for the dataset distillation experiment.
Figure 12:	Code for the molecular dynamics experiment.
Figure 13: GPU runtime comparison of implicit differentiation and unrolling for hyperparameteroptimization of multiclass SVMs for multiple problem sizes (same setting as Figure 3). Error barsrepresent 90% confidence intervals. Absent data points were due to out-of-memory errors (16 GBmaximum).
Figure 14: Value of the outer problem objective function (validation loss) for hyperparameteroptimization of multiclass SVMs for multiple problem sizes (same setting as Figure 3). As canbe seen, all methods performed similarly in terms of validation loss. This confirms that the fasterruntimes for implicit differentiation compared to unrolling shown in Figure 3 (CPU) and Figure 13(GPU) are not at the cost of worse validation loss.
Figure 15: Jacobian error ∣∣∂x*(θ) - J(X, θ)∣∣2 (see also Definition 1) evaluated with a regularizationparameter of θ = 1, as a function of solution error ∣∣χ*(θ) - X∣∣2 when varying the number of features,on the multiclass SVM task (see Appendix E.1 for a detailed description of the experimental setup).
Figure 16: Distilled MNIST dataset θ ∈ Rk×p obtained by solving (12) through unrolled differen-tiation. Although there is no qualitative difference, the implicit differentiation approach is 4 timesfaster.
Figure 17: L1 norm of position sensitivities in the molecular dynamics simulations, for 40 dif-ferent random initial conditions (different colored lines). Gradients through the unrolled FIREoptimizer (Bitzek et al., 2006) for many initial conditions do not converge, in contrast to implicitdifferentiation.
