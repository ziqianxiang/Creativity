Figure 1: Dual processmodel in the stop-signaltask (SST).
Figure 2: SAC-I policy update.
Figure 3: LunarLanderContinuous-v2 with Bomb. (a) Episode starts with the lander placed ran-domly on top of the frame around the center. (b) Go trial: refers to an episode without a bomb, andit is exactly as the original environment. (c) Stop trial: refers to episodes in which a bomb randomlyappears close to the landing pad (between the two flags). (d) It shows a successful landing during astop trial after avoiding hitting the bomb. By default, the bomb appears in 50% of the episodes.
Figure 4: Comparison of differentSAC and SAC-I agents.
Figure 5: Performance of SAC-I and SAC agents for different bomb frequencies.
Figure 6: SAC and SAC-I agentswith and without shaping.
Figure 7: BipedalWalker environment. (a) Plain terrain: agent has to learn to walk forward; (b)Stuck position: agent starts to receive negative reward since it is spending energy without movingforward; (c) Fall: agent can fall because it loses balance, stumbles itself, or fall into a hole; (d)Overcoming obstacles: the agent learns avoiding the holes or going over the blocks.
Figure 8: Performance ofSAC and SAC-I agentS.
Figure 9: SAC and SAC-I agentS performance in the mixed verSion of the BipedalWalkerHardcore-v3. FirSt roW ShoWS reSultS for the taSk With 90% Stop trialS (hardcore), While the Second, With 70%.
Figure 10: Comparison of SACing down, and penalizing “aggressive” maneuvers (large ve- and SAC-I agents with conserva-locities and vertical angles). The more conservative reward tive shaping.
Figure 11: Agent performancein BipedalWalkerHardcore-v3, re-trained using SAC algorithm with(brown) and without (blue line) thefall penalty.
Figure 12: SAC and SAC-I agents performance in the mixed version of BipedalWalkerHardcore-v3.
