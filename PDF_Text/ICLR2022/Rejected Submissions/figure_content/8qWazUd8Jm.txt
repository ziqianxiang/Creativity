Figure 1: Pictorial depiction for the pro-posed evaluation metrics. The blue and redspheres correspond to the α- and β-supportsof real and generative distributions, respec-tively. Blue and red points correspond to realand synthetic data. (a) Synthetic data fallingoutside the blue sphere will look unrealisticor noisy. (b) Overfitted models can generateostensibly high-quality data samples that are“unauthentic” because they are copied fromthe training data. (c) High-quality data sam-ples should reside inside the blue sphere. (d)Outliers do not count in the β -Recall met-ric. (Here, α=β=0.9, α-Precision=8∕9, β-Recall = 4/9, and Authenticity = 9/10.)Support of realdistribution Fv.
Figure 2: The evaluation and auditing pipelines.
Figure 3: Interpretation of the Pα and Rβ curves. Real distribution is colored in blue, generative distributionis in red. Distributions are collapsed into 1 dimension for simplicity. Here, Pr is a multimodal distribution ofcat images, with one mode representing orange tabby cats and another mode for Calico cats; outliers compriseexotic Caracal cats. Shaded areas represent the probability mass covered by a- and β-supports—these supportsconcentrate around the modes, but need not be contiguous for multimodal distributions, i.e., we have Srα =Srα,1 ∪ Srα,2, and Sgβ = Sgβ,1 ∪ Sgβ,2. (a) Here, the model Pg exhibits mode collapse where it over-representsorange tabbies. Such model would achieve a precision score of P1 = 1 but a suboptimal (concave) Pα curve(panel (d)). Because it does not cover all modes, the model will have both a suboptimal R1 score and Rβcurve. (b) This model perfectly nails the support of Pr, hence it scores optimal standard metrics P1 = R1 = 1.
Figure 4: Predictive modeling with synthetic data. (a) Here, we rank the 4 generative models (ADS-GAN: ×,WGAN-GP: •, VAE: N, GAN: ) with respect to each evaluation metric (leftmost is best). For each metric, wetrain a predictive model on the synthetic data set with highest score, and test its AUC on real data. Ground-truthranking of synthetic data is the ranking of the AUC of predictive models trained on them. (b) Hyper-parametertuning for ADS-GAN. (Dashed lines are linear regression lines.) (c) Post-hoc auditing of ADS-GAN.
Figure 5: (a) Diagnosing mode collapse in MNIST data. (b) Results for the hide-and-seek competition.
Figure 1: Architecture of LSTM Autoencoder, sourced from Srivastava et al. (2015).
Figure 2: Toy experiment I: outlier robustnesAs can be seen, the precision and recall metrics are not robust to outliers, as just a single outlier hasdramatic effects. The IPα and IRβ are not affected, as the outlier does not belong to the α-support(or β-support) unless α (or β) is large.
Figure 3: Toy experiment II: mode resolutionAs can be seen, neither P&R nor D&C notice that the synthetic data only consists of a single mode,whereas the original data consisted of two. The α-precision metric is able to capture this metric: forsmall α the α-support of the original distribution is centred around the two separated, and does notcontain the space that separates the modes (i.e. the mode of the synthetic data).
