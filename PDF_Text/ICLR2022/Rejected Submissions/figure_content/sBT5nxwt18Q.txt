Figure 1: Different Methods: A misclassification of “Leopard” as “Cheetah” on ImageNet is ex-plained by: (A) ExPlanation-by-example saying “I think the test image is a Cheetah because itlooks similar to a training image I also think is a Cheetah”. This is then enhanced by (B) FAMSby Kenny & Keane (2019), and our proposed methods of (C) Latent-based CCRs, and (D) pixel-based CCRs. The FAMs highlight multiple regions in both images, and it is difficult to know wherea feature begins and ends, in contrast, CCRs highlight a very specific part of both images. Note thatthe CCR explanation methods naturally retrieve a different neighbor due to searching a “pool” ofcandidates for the closest feature match, whilst FAMs always use the closest NN.
Figure 2:	Expt. 1 CCR Occlusion (Occ.) and Inclusion (Inc.) Results: The first row of lineplotsshow a comparison of the four different CCR methods proposed in Section 2. The second row showa comparison of the three latent-based CCR methods against LIME, to see the difference betweenLIME and SP-CCRs. Overall, results show the CCR methods do significantly better than randomocclusion/inclusion, and that LIME performs similarly to SP-CCRs. Standard Error bars are shown.
Figure 3:	Expt. 2 Results Retraining: (A) Exploring the hyperparameter choice for alpha, (B) doingsimilar tests for beta. (C) A direct comparison between methods by varying alpha. Results showthat all methods are better than the random baseline in nearly every test, and that CAM seems thebest method to isolate important critical regions in an image.
Figure 4: Expt. 3 results: Correctness ratings for two material-sets (A and B) broken out by“Right*" and Wrong classifications for the two explanation-types, NoBox (example-only) and Box(example-plus-CCR). In the B-Set, the Right*-Box ratings (m=3.04) are reliably different to theRight*-NoBox ratings (NoBox, M=2.83), reflecting people,s performance on ambiguous items inthat material-set.
Figure 5: Expt. 3 Material Examples: (a) A "Pot Pie” is misclassified as “Lemon”. The explanationshows the CCR in the image identified by the CNN. The explanation represents the training data andfeature “used” in the CNN,s classification. Glossed, the explanation says “I think this is a lemon,because it has a similar part to an image I saw before which I learned should be a lemon”. (b)Another misclassification of a “Barbell” as a “Prison” in which the CNN picks up on the bars inthe background reflecting the jail bars in a previous training image. Although anecdotal, an analysisof these two items shows that both have significantly higher helpfulness ratings when the CCR isgiven; t-tests show (a) NoBox 2.37 V Box 3.09, P < 0.001 and (b) NoBox 4.19 V Box 4.48, P < 0.05.
Figure 6: Experimental Results: (A) The agreement metric shows twins get a perfect score of 1.0,with DkNN performing second best. (B/C) The different layers of CNNs trained on CIFAR-10 andFashionMNIST are randomized to determine the sensitivity of the method to model parameters.
Figure 7: Correct examples: Starting from the top left (and going in reading order), We see correctclassifications and a NN explanation showing the CCR used in the classification. Namely, the imagesshow correct classifications of a “Castle”, “Traffic Light”, “iPod”, “Cello”, “Park Bench”, and “JackO Lantern”Test ImageEXPIanation-by-ExampleLabel: HammerClassification: ShovelLabel: ShovelClassification: ShovelFigure 8: Incorrect examples: (a) A “Kimono” is misclassified as a “Violin”, the CCR shows theCNN confused the pipe in the test image as the violinist,s bow. (b) A “Hammer” is misclassifiedas a “Shovel”, the CCR shows the CNN learned to focus on the wooden handle of shovels whenclassifying them, which it partly learned from the training image shown.
Figure 8: Incorrect examples: (a) A “Kimono” is misclassified as a “Violin”, the CCR shows theCNN confused the pipe in the test image as the violinist,s bow. (b) A “Hammer” is misclassifiedas a “Shovel”, the CCR shows the CNN learned to focus on the wooden handle of shovels whenclassifying them, which it partly learned from the training image shown.
Figure 9: More Incorrect examples: (a) An “Acoustic Guitar” is misclassified as an “Electric Guitar”,the CCR shows the CNN has learned to associate the guitar's fretboard With electric guitars, andneglected the rest of the image when classifying the test image. (b) A “Flute” is misclassified asa “Horizontal Bar”, the CCR shows the CNN seems to have focused on the qualitative similaritybetween the horizontal bamboo bar in the training image, and the bamboo flute in the test image.
Figure 10: Multiple CCRs: An incorrect classification of a “Leopard” as a “Cheetah”. Three of themost salient CCRS are shown for the test image, alongside their three closest representations in thepool ofNNs.
Figure 11:	Multiple CCRs: A correct classification of “Covid-19”. Three of the most salient CCRsare shown for the test image, alongside their three closest representations in the pool of NNs.
Figure 12:	Incorrect ImageNet Classifications: SP-CCRs are used to explain two misclassification(A) A “Knot” is misclassified as a “Padlock”, were the CNN has learned to associate a “woodenbackground” feature with the predicted class, causing a bias. (B) A “Rifle” is misclassified as a“Shovel”, where a bias in the CNN has learned to associate a “snowy background” feature with theclass “Shovel”.
