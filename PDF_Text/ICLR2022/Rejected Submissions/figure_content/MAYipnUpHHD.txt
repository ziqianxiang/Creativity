Figure 1: AMR viewed as a Markov decision process.
Figure 2: Individual samples from each true so-lution function class. Each function sampled inbumps and circles is a superposition of a randomnumber of such features in general. Refinementsshown here are produced by IPN.
Figure 3: In-distribution and Static→advection. Performance of IPN, Graphnet and Hypernet-work policies versus baselines. Higher values are better. (a,b) RL policies were trained and testedon the same function class, for static and advection cases independently. (c) Static-trained policieson a function class are tested on advection of the same function class.
Figure 4: Burgers equation and surrogate reward. Solid bars/ER denote exact reward, stripedbars/SR denote surrogate reward. (a) IPN trained and tested on a fixed IC. (b) IPN tested on randomICs using policies trained on a fixed IC (from Figure 4a), policies pretrained on fixed IC and fine-tuned on random ICs, and policies only trained on random ICs. (c/d) Visualization of resultingmeshes for Burgers equation with a fixed bump IC.
Figure 5: Budget↑. (a-b) Policies trained with budget B = 10 (static) and B = 20 (advection) aretested with B = 50. (c) IPN trained with B = 10 generalizes to B = 100. (d) Graphnet trained onadvecting bump with B = 20 generalizes to B = 50.
Figure 6: Size↑. (a-b) Policies trained with initial 8 × 8 mesh were tested on initial 16 × 16 mesh.
Figure 7:	Advection of a bump function. RL policy trained with budget B = 20 generalizes toB= 100.
Figure 9: Example test case on64 × 64 meshFigure 8:	Generalization of policies trained with refinement bud-get B = 10 to test case with B = 100.
Figure 8:	Generalization of policies trained with refinement bud-get B = 10 to test case with B = 100.
Figure 10: IPN trained on 8 × 8 initial meshunderperformed ZZ when tested on 16 × 16initial mesh but makes qualitatively correctrefinements.
Figure 11: OOD. (a) IPN (OOD) was trained onBurgers with multi-bumps IC and tested on Burgerswith a 1-bump IC. (b) IPN (OOD) was trained on1-bump IC and tested with multi-bumps IC.
Figure 12: Static→advection and Budget↑: IPN trained on static bumps (B = 10) transfers toadvection (B = 50).
Figure 13: All train-test combinations. Normalized error reduction of IPN, Graphnet and Hyper-network policies on (a-c) Static AMR and (d-f) Advection PDE. Higher values are better. Legend(colors) shows test classes. RL policies were trained and tested on each combination of true solu-tions. Mean and standard error over four RNG seeds of mean final error over 100 test episodes permethod.
