Figure 1: Left: In the standard setup, therepresentation consists of a single vector, re-quiring a simple vector-to-vector mapping todo operations in the vector space. Right:In BoV-AE, the representation consists of avariable-size bag of vectors, requiring a morecomplex mapping from one bag to anotherbag.
Figure 2: High-level view of the Emb2Embframework. Text Autoencoder Pretraining:An autoencoder is trained on an unlabeledcorpus, i.e., the encoder enc transforms aninput text x into a continuous embedding zx ,which is in turn used by the decoder dec topredict a reconstruction X of the input Sen-tence. Task Training: The encoder is frozen(grey), and a mapping Φ is trained (green) oninput embeddings Zx to output predictions Zysuch that it minimize some loss L(Zy). In-ference :To obtain textual predictions y, theencoder is composed with Φ and the decoder.
Figure 3: Style transfer on Yelp-Reviews.
Figure 4: Reconstruction loss on the valida-tion set for different AEs. fixed: The bagconsists of a single vector obtained by aver-aging the embeddings at the last layer of theTransformer encoder. L0-r: BoV-AEs withL0Drop target ratio r.
Figure 5: Style transfer performance onYelp-Sentences of BoV models compared toa fixed-size AE for varying λsty . As bothcontent retention and style transfer are im-portant for style transfer, the further a graphis to the top right, the better the model.
Figure 6:	Style transfer score depending on the window size.
Figure 7:	Reconstruction loss on the validation set of Yelp-Reviewsfor different autoencoders. fixed: The bag consists of a single vectorobtained by averaging the embeddings at the last layer of the Trans-former encoder. L0-r: BoV-AE with L0Drop target ratio r.
Figure 8: Reconstruction loss on the validation set of Gigaword fordifferent autoencoders. fixed: The bag consists of a single vectorobtained by averaging the embeddings at the last layer of the Trans-former encoder. L0-r: BoV-AE with L0Drop target ratio r.
