Figure 1: Distribution of features extracted using a neural network with two dimensional final layer trained onCIFAR-10 with (a): ProtoNet loss (b): cross-entropy loss with linear projection layer. The ProtoNet featuresdistribute closer to its class center than the features extracted using the model trained on cross-entropy loss withlinear projection layer.
Figure 2: Left: We plotted the values shown in equations 6,7,8 divided by Tr(Σ)2 . The values are computedfrom test-split of each dataset with ResNet12 and ResNet18. We scaled the values so that each dataset’sVWit 3τ ,ς, μQTr(Σ)2to be 1 for simplicity. Right: We showed the ratio of the within-class variance to the between-classvariance computed from the test-split’s features extracted using each backbone trained on each correspondingdataset. We scaled the values so that the maximium value to be 1 for simplicity.
Figure 3: Histogram of cosine similarities With class mean vectors and largest eigen vectors of LefeΣ, Right:∑cwithin-class to between-class variance among all dataset. Thus the method reducing the ratio worksbetter With FC100 features than any other dataset’s features.
Figure 4: We plotted the ratio of the between-Class variance to the Within-Class variance (T)before andafter applying L2-norm averaged over the test classes of each dataset.
