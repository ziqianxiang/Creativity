Figure 1: The schematic diagram for a typical reinforcement learning algorithmtraining action space in many cases leads to lower cumulative reward than a well-designed trainingaction space. To the best of authorsâ€™ knowledge, there is hardly any data-driven tool for the evaluationof training action space for RL. Such a tool leads to superior RL agent performance, lower modeltraining and maintenance cost, and strong multi-disciplinary collaboration [35].
Figure 2: The proposed training action selection framework for reinforcement learningthe transition probability, an acceptable performance margin, and an acceptable number of iterations.
Figure 3: CPU utilization (%) responses on five different EC2 instances (Table 2) from 2PM-UTC9/1/2021-2PM-UTC 9/2/2021 at 1 minute granularity. This training data was generated internally [4].
Figure 4: Examples of dispensable actions. (a) RL loop with all training actions, <small t3a, mediumt3a, large t3a, xlarge t3a, 2xlarge t3a>. The reward is noted to be -21. (b) RL loop with a trainingaction set of <medium t3a, large t3a, xlarge t3a, 2xlarge t3a>. The reward is noted to be -13. (c)RL loop with a training action set of <small t3a, large t3a, xlarge t3a, 2xlarge t3a>. The reward isnoted to be -16. Both small t3a and medium t3a are noted to be dispensable actions.
Figure 5: RL loop with <small t3a, medium t3a, xlarge t3a, 2xlarge t3a> action space. The agentcould never accomplish the goal, therefore, large t3a is an indispensable element in the trainingaction space. Similarly, xlarge t3a and 2xlarge t3a are two indispensable elements in the trainingaction space.
