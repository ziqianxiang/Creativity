Figure 1: An illustration of the properties of local alignment and congregation. Feature representa-tions on the left are more congregated than those on the right. The first and third feature representa-tions on the top are more aligned locally than the other two. Classical error bounds depend only onthe norm of the feature vectors and so cannot distinguish between these. We show theoretically andempirically that the distinctions shown here impact the error of a downstream classifier.
Figure 2: Intuition behind our analysis for the lower bound on error (Theorem 1, Left) and upperbound on generalization (Theorem 2, Right). In each case we look at pairs of data points that fallwithin a distance α of each other (shown as red balls). The lower bound notes that examples withdifferent labels in these balls yield high error. The upper bound notes that outside the red ballscentered on training points (large dots) generalization is not guaranteed. Also shown are the optimal(red) and obtained (black) classifiers.
Figure 3: pa (left axis, solid line) and pc (rightaxis, dotted line) as a function of α for three fea-ture representations on CIFAR-10However, the upper bound suggests that goodgeneralization from small datasets actually re-quires a highly congregated feature space. As such, these highly spread out feature representationscan actually yield large generalization errors in few-shot settings, as demonstrated in Table 1, evenif they yield much lower errors with large training sets. Thus, finding a good representation for atask is a nuanced decision, requiring one to balance between these conflicting requirements.
Figure 4: Correlations of our proposed bounds with actual margin loss. Clockwise from top left:Lower bound vs train loss, Theorem 2 vs excess risk, the classical textbook bound vs excess risk,and Theorem 3 (right) vs actual excess risk.
Figure 5: Our characterization of the feature space allows us to choose the best pre-trained repre-sentation for a given few-shot task. Lower numbers are better. Left: results on CUB-200. Right:results on CIFAR-10.
Figure 6: Scatter plots showing the correlations described in the main paper. For these scatter plots,data across all 45 tasks was pooled together. Clockwise from top left: Lower bound vs trainingloss, bound from theorem 2 vs excess risk, classical Rademacher-based bound vs excess risk, andTheorem 3 vs excess risk.
