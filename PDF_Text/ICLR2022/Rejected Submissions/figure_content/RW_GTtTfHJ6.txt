Figure 1: Standard POMDP setting.
Figure 2: Privileged POMDP setting.
Figure 3: Augmented POMDP setting, with a policy regime indicator I taking values in {0, 1}(1=interventional regime, no confounding, 0=observational regime, potential confounding), suchthat π(a∕ht,st, i = 1) = π(at∣ht, i = 1). This additional constraint introduces a contextualindependence At ⊥⊥ St | Ht, I = 1.
Figure 4: Performance of each RL agent on our three toy problems (the higher the better). We reportthe average cumulative reward (mean ± std) obtained on the real environment. Little markers indicatethe significance of a two-sided Wilcoxon signed-rank test [7] with α < 5%, between our method,augmented, and the baselines no obs (down triangles), naive (squares) and Kallus et al. (up triangles).
Figure 5: Robustness to different degrees of confounding in the door problem (the lower the better).
Figure 6: The gridworld experiment. Left: the JS divergence and cumulative reward obtained byeach method. Right: the initial grid, and a heatmap of the tiles visited by the RL agents at test timeat the |Dint | = 27 mark. At this point, only the augmented method has learned how to pass the wall.
Figure 7: Noisy good expert setting. Heatmaps correspond respectively to the expected reward (toprow, higher is better) and the JS divergence (bottom row, lower is better).
Figure 8:	Random expert setting. Heatmaps correspond respectively to the expected reward (top row,higher is better) and the JS divergence (bottom row, lower is better).
Figure 9:	Perfectly good expert setting. Heatmaps correspond respectively to the expected reward(top row, higher is better) and the JS divergence (bottom row, lower is better).
Figure 10: Perfectly bad expert setting. Heatmaps correspond respectively to the expected reward(top row, higher is better) and the JS divergence (bottom row, lower is better).
Figure 11:	Positively biased expert setting. Heatmaps correspond respectively to the expected reward(top row, higher is better) and the JS divergence (bottom row, lower is better).
Figure 12:	Pessimistic bias expert setting. Heatmaps correspond respectively to the expected reward(top row, higher is better) and the JS divergence (bottom row, lower is better).
Figure 13: Noisy good agent.
Figure 14: Random agent.
Figure 15: Very good agent.
Figure 16: Very bad agent.
Figure 18: Perfect agent.
Figure 19: Average heat-maps over 100 episodes × 10 seeds, of the tiles visited by each trainedagent (no obs, naive, augmented) for different interventional data sizes (22, 23, 24, 25 , 26, 27). Theaugmented approach is the fastest (in terms of interventional data) to learn how to properly escapethe top part of the maze through tile (4, 2), and then move towards the treasure on tile (1, 3).
Figure 20: Average heat-maps over 100 episodes × 10 seeds, of the tiles visited by each trained agent(no obs, naive, augmented) for different interventional data sizes (28, 29, 210, 211, 212, 213). Theaugmented approach is the fastest (in terms of interventional data) to learn how to properly escapethe top part of the maze through tile (4, 2), and then move towards the treasure on tile (1, 3).
