Figure 1: Manifold intrusion caused by deleting a nodeor an edge (gray dot lines) from the left graph. The twosynthetic graphs (middle) haVe the same structures as thetwo original graphs on the right but with different labels.
Figure 2: The proposed mixingschema. Top subfigure depicts thesource graph pair, and the bottom isthe resulting mixed graph for train-ing with mixing ratio λ ∈ [0, 1].
Figure 3: probability densityfunction of Beta distribution.
Figure 4: MixupGraph and ifMixup with mixing ratios fromBeta(1, 1), Beta(2, 2), Beta(5, 1), Beta(10, 1) and Beta(20, 1).
Figure 5: Varying the depth for GCN, ifMixup, and MixupGraph.
Figure 6: Training loss (left) and validation accuracybeing over-fitted by limited number of graph samples in the original training set. As a result, asshown in the right subfigure, even training for a long time, the ifMixup models were not overfitting.
Figure 7: Manifold intrusion caused by connecting a graph pair. The synthetic graph in the middle iscreated by connecting the two graphs from the left, but assigning a soft label. This synthetic graph(with soft label) has the same structure as the right graph with one-hot label.
Figure 8: 2D visualization of the learned representations of the training graphs in NCI109 and NCI1.
Figure 9: Illustration of mixing and recovering in ifMixup.
