Figure 1: Adaptive curricula can result in covariate shifts in environment parameters with respect to a fixedground-truth distribution P(θ) (See top path), e.g. whether apple or banana is the correct fruit to choose perlevel. Here, the policy π , (π ,) always chooses apple (banana). Our method, SAMPLR (bottom path) matchesthe advantages in sampled levels to the advantages observed if sampling levels from P(θ) (blue triangles), thusconstraining the optimal policy under the curriculum distribution P (θ) to match that under P (θ).
Figure 2: A standard RL transition (left) and the fictitious transition used by SAMPLR (right).
Figure 4: Episodic returns (left) and number of rooms in solved levels (middle) during training (dotted lines)and test on the ground-truth distribution (solid lines), for q = 0.7. Normalized test returns and proportion ofapple goals during training for various q are shown on the right. Plots show mean and standard error of 10 runs.
Figure 3: Levels from thestochastic fruit-choice environment.
Figure 5: Left: Proportion of training episodes in which the agent fails to eat any fruit; eats the apple; or eatsthe banana. Right: Number of rooms in levels during training. Plots show mean and standard error of 10 runs.
Figure 6: Left: Levels from FireDungeon. Middle: Test return on the ground-truth distribution of FireDungeon.
