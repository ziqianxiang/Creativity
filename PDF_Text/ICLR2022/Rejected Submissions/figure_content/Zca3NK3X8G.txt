Figure 1: Portfolio evolution through timeWe note that in Jiang et al. (2017), the authors suggest to approximate Vt using an iterative procedure.
Figure 2: The architecture of the WaveCorr policy networka tensor of dimension m × h × d, where m : the number of assets, h : the size of look-back timewindow, d : the number of channels (number of features for each asset), and generates as output anm-dimensional wealth allocation vector. The WaveCorr blocks, which play the key role for extractingcross time/asset dependencies, form the body of the architecture. In order to provide more flexibilityfor the choice of h, we define a causal convolution after the sequence of WaveCorr blocks to adjustthe receptive field so that it includes the whole length of the input time series. Also, similar to theWaveNet structure, we use skip connections in our architecture.
Figure 3: WaveCorr residual blockdimensions m × h × d, the convolutions output tensors of dimension m × h × d0 where each slice ofthe output tensor, i.e. an m × 1 × d matrix, contains the dependency information of each asset overtime. By applying different dilation rates in each WaveCorr block, the model is able of extracting thedependency information for a longer time horizon. A dropout layer with a rate of 50% is considered toprevent over-fitting, whereas for the gradient explosion/vanishing prevention mechanism of residualconnection we use a 1 × 1 convolution (presented on the top of Figure 3), which inherently ensuresthat the summation operation is over tensors of the same shape. The Corr layer generates an outputtensor of dimensions m × h × 1 from an m × h × d input, where each slice of the output tensor, i.e.
Figure 4: An example of the Corr layer over 5 assetsTable 1: The structure of the networkLayer	Input shape	Output shape	Kernel	Activation	Dilation rateDilated conv	(m × h × d)	(m × h × 8)	(1 × 3)	Relu	1Dilated conv	(m × h × 8)	(m × h × 8)	(1 × 3)	Relu	1Corr layer	(m × h × 8)	(m × h × 1)	([m+1] × 1)	Relu	-Dilated conv	(m × h × 9)	(m × h × 16)	(1 × 3)	Relu	2Dilated conv	(m × h × 16)	(m × h × 16)	(1 × 3)	Relu	2Corr layer	(m × h × 16)	(m × h × 1)	([m+1] × 1)	Relu	-Dilated conv	(m × h × 17)	(m × h × 16)	(1 × 3)	Relu	4Dilated conv	(m × h × 16)	(m × h × 16)	(1 × 3)	Relu	4Corr layer	(m × h × 16)	(m × h × 1)	([m+1] × 1)	Relu	-Causal conv	(m × h × 17)	(m × 1 × 16)	(1 × [h - 28])	Relu	-1 × 1 conv	(m × 1 × 17)	(m × 1 × 1)	(1 × 1)	Softmax	-Finally, it is necessary to discuss some connections with the recent work of Zhang et al. (2020), wherethe authors propose an architecture that also takes both sequential and cross-asset dependency intoconsideration. Their proposed architecture, from a high level perspective, is more complex than oursin that theirs involves two sub-networks, one LSTM and one CNN, whereas ours is built solely onCNN. Our architecture is thus simpler to implement, less susceptible to overfitting, and allows formore efficient computation. The most noticeable difference between their design and ours is at the
Figure 5: Comparison of the wealth accumulated by WaveCorr and CS-PPN under random initialpermutation of assets on Can-data’s test set.
