Figure 1: PUUPL is a pseudo-labeling framework for PU learning that uses the epistemic uncertainty ofan ensemble to select confident examples to pseudo-label. The ensemble can be trained with any PU loss forPU data while minimizing the cross-entropy loss on the previously assigned pseudo-labels. On a toy example,a single network is not very confident on most of the unlabeled data (a), resulting in many high-confidenceincorrect predictions and many low-confidence correct ones (c). The epistemic uncertainty of an ensembleis, on the other hand, very low on most of the the unlabeled data (b), resulting in most correct predictionshaving low uncertainty and most incorrect predictions having high uncertainty (d). Thus, the uncertainty of anensemble can be used more reliably to rank predictions and select correct ones (e).
Figure 2: Validation accuracy (left, blue) and expected calibration error (ECE, right, green) for a run onCIFAR-10. The ensemble is trained for a fixed number of 100 epochs before pseudo-labeling, visible as theperiodic spikes in both curves. Note the substantial reduction in ECE in the second and third pseudo-labelingiterations, when the ensemble is trained on soft labels. The best validation accuracy of 90.76% is indicated bythe orange dot and corresponds to a test accuracy of 90.35%.
Figure 3: Mean and standard deviation of the test accuracy obtained over five runs by different variationsof our PUUPL algorithm: (a) different weight initialization at each iteration, (b) balanced or imbalanced PLselection, (c) type of uncertainty, (d) whether to use positive-negative or positive-unlabeled validation set. Notethe different scales on the y-axes.
Figure 4: Mean and standard deviation of the test accuracy obtained over five runs by different hyperparametervalues. Refer to the main text for a discussion. Note the different scales on the y-axes.
