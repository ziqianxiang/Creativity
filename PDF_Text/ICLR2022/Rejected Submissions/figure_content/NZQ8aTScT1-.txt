Figure 1: Architectural Inductive Biases. An demonstration of learnable functions vs archi-tectures for four families of architectures. Each shaded box indicates the maximum learnableeigenspaces within a given compute budget (Dashed Line.) From left to right: (a) MLPs canmodel Long-Range-Low-Frequency interactions; (b) S-CNNs can model Short-Range-High-Frequency interactions; (c) Additionally, D-CNNs can also model interactions between LRLFand SRHF, a.k.a., Median-Range-Median-Frequency interactions. (d) Finally, HS-CNNs can ad-ditionally model interactions of Ultra-Short-Range-Ultra-High-Frequency, Ultra-Long-Range-Ultra-Low-Frequency, and finer interpolations in-between. The Green Arrow indicates the di-rection of expansion of learnable functions when increasing the compute budget.
Figure 2: Architectures/DAGs. vs Eigenfunctions vs Learning Indices. Left to right: DAGs as-sociated to (a) a four-layer MLP; (b) CNN(p2)02, a"D”-CNN that has two ConVolUtionallayer(C)CNN(p)04, a “HR”-CNN that has four convolutional layers; and (d) MSE (Y-axis) vs training setsize (X-axis) for Y2 obtained by NTK-regression for 4 architectures. Here Y2 is a linear combina-tion of eigenfunctions of Short-Range-Low-Frequency interactions (deg(Y2) = 2); see Sec. D forthe expression. The DAGs are generated with p = 4. In each DAG, the Dashed Lines represent theedges with zero weights. The Solid Lines have weights 0, 2 and 4 in (a), (b) and (c), resp. The Col-ored path represents the minimum spanning tree used to compute the spatial indices ofY2. Underarchitectures (a), (b) and (c), the spatial indices are 0, 1 and 4, resp. Each input node represents aninput patch of dimension p4 = d, p2 = d 1 and P = d4 and the frequency indices are 2, 2 X 2 and2 × 4 in (a), (b) and (c), resp.
Figure 3: Learning Dynamics vs Architectures vs Learning Indices. We plot the learning/trainingdynamics of each eigenfunction Yi. From top to bottom: a 4-layer MLP, a 2-layer CNN and a 4-layerCNN. From left to right: residual MSE (per eigenfunction) of NNGP/NTK regression, test/trainingMSE of SGD. The learning indices of Yi in each architecture is shown in the legends.
Figure 4: Learning Dynamics: GAP vs Flatten. We plot the validation MSE of the residual ofeach Yi (left → right: i = 1 → 7) for GAP (Solid lines) and Flatten (Dashed lines). The mean/stdin each curve is obtained by 5 random initializations. Clearly, the residual dynamics of GAP andFlatten are almost indistinguishable for Yi with i ≤ 6. However, GAP outperforms Flatten whenlearning Y7.
Figure 5:	ResNet50-GAP vs ResNet50-Flatten. As the training set size increases the performance(accuracy and loss) gap between the two shrinks.
Figure 6:	Eigenfunction vs Learning Index vs Architecture/DAG. Rows: eigenfunctions Yi withvarious spaCe-frequenCy Combinations. Columns: DAGs assoCiated to (1) a four-layer MLP; (b)CNN(p2产2, a"D”-CNN; (c) CNN(P产4 a “HR”-CNN. Column (d) is the MSE, as a function oftraining set size (X-axis), of the residual of the Corresponding eigenfunCtion Yi obtained by NTK-regression. The colored path in each DAG corresponds to the minimum spanning tree that containsall interacting terms in the k = 0 component of Yi .
Figure 7: MLPs do not benefits from having more layers. We plot the learning dynamics vstraining set size / SGD steps for each eigenfunction Yi . Top: NTK regression and bottom: SGD+ Momentum. Left: MLP; right: CNN. Dashed lines / Solid lines correspond to one-hidden/four-hidden layer networks. For both finite-width SGD training and infinite-width kernel regression,having more layers does not essentially improve performance of a MLP. This is in stark contrast toCNNs (right). By having more layers, the eigenstuctures of the kernels are refined.
Figure 8: ResNet-GAP vs ResNet-Flatten. As the training set size increases the performance(accuracy and loss) gap between the two shrinks substantially. Top/bottom ResNet34/ResNet101F ARCHITECTURE SPECIFICATIONSIn this section, we provide the details of the architectures used in the experiments.
