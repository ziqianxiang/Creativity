Figure 1: FLOPs (left) and wall-clock time (right) of computing the NTK for a 10-layer ReLUfCn. As predicted by Table 1, our methods almost always outperform Jacobian contraction, allow-ing orders of magnitude speed-ups and memory improvements for realistic problem sizes. FLOPsper NTK entry: We confirm several specific predictions: (1) NTK-vector products are the best per-forming method for N = 1, and have cost equivalent to Jacobian for any width W or output size O(top row); (2) NTK-vector products offer an O-fold improvement over Jacobian contraction (left toright columns); (3) NTK-vector products are equivalent to Jacobian contraction for O = 1 (leftmostcolumn); (4) Structured derivatives outperform NTK-vector products iff O < W (O = W are plot-ted as pale vertical lines, which is where Structured derivatives and NTK-vector products intersect);(5) Structured derivatives approach the cost of Jacobian in the limit of large width W (left to right).
Figure 2: Wall-clock time cost of computing an NTK for several ResNet sizes on a pair ofImageNet inputs. Structured derivatives allow the NTK to be computed faster and for larger models(See bottom row - missing points indicate out-of-memory). NTK-vector products, as predicted in§3.6 and Table 2, are advantageous for large O (bottom row), but are suboptimal when the cost ofthe forward pass is large relative to the number of parameters, e.g. when there is a lot of weightsharing (see Table 7 and Table 2), which is the case for convolutions. See Fig. 4 for more ImageNetmodels, §F for analysis of CNN NTK computational complexity, and §H for experimental details.
