Figure 1: Overview of the Customization Approaches - Transformer models during fine-tuning,where the frozen parts of the model (not trainable) are displayed in gray: (a) Custom fine-tuningmodifies all parameters during training; (b) L-EO trains only the embedding and output layers; (c)L-LDB allows to train only the parameters of the last decoder block; (d) Prefix tuning adds a trainableprefix to the encoder and decoder blocks.
Figure 2: This figure shows the total average parameter change after fine-tuning to a new projectdomain, showing that the largest parameter changes occur in deeper parts of the model. This motivatesour choice to try only fine-tuning the later layers of the model.
Figure 3: Task-specific metrics (a) custom models outperform the baseline in terms of perfect matches(solid line) and abstract matches (dotted line); (b) custom models generate code that uses identifiers(i.e., variable and function names) that are more similar to the project codebase.
Figure 4: Validation Loss vs Compute (PF-seconds) - Light lines represent the validation loss curvefor each individual project and fold, while the bold line represents the average for each customstrategy. Custom is the most efficient, lightweight approaches require slightly more compute to reacha comparable validation loss, while prefix is the least efficient, suffering from poor initialization.
