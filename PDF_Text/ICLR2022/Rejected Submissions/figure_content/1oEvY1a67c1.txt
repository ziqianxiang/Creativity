Figure 1: Robustness and adaptation to new datasets has traditionally been achieved by robust pre-training (withhand-selected/data-driven augmentation strategies, or additional data), unsupervised domain adaptation (withaccess to unlabeled samples from the test set), or, more recently, self-supervised learning methods. We showthat on top of these different pre-training tasks, it is always possible (irrespective of architecture, model size orpre-training algorithm) to further adapt models to the target domain with simple self-learning techniques.
Figure 2: For the two point model, we showerror and for the CIFAR10-C simulation, we showimprovement (yellow) vs. degradation (purple)over the non-adapted baseline (BAS). An importantconvergence criterion for pseudo-labeling (toprow) and entropy minimization (bottom row) is theratio of student and teacher temperatures; it lies atτs = τt for PL, and 2τs = τt for ENT. Despitethe simplicity of the two-point model, the generalconvergence regions transfer to CIFAR10-C.
Figure 3:	Entropy minimization (top) Training two point model with momentum 0.9 and different learning rateswith initialization ws = wt = [0.5, 0.5]>.
Figure 4:	Training a two point model without momentum and different learning rates with initialization ws =wt = [0.5, 0.5]>. Note that especially for lower learning rates, longer training would increase the size of thecollapsed region.
Figure 5:	Training a two point model with momentum 0.9 and different learning rates with initialization wswt = [0.6, 0.3]>.
Figure 6: Severity-wise mean corruption error (normalized using the average AlexNet baseline error for eachcorruption) for ResNet50 (RN50), ResNext101 (RNx101) variants and the Noisy Student L2 model. Especiallyfor more robust models (DeepAugment+Augmix and Noisy Student L2), most gains are obtained across higherseverities 4 and 5. For weaker models, the baseline variant (Base) is additionally substantially improved forsmaller corruptions.
Figure 7: Evolution of error during online adaptation for EfficientNet-L2.
Figure 8: Top-1 error for the different IN-D domains for a ResNet50 and training with RPLq=0.8 and ENT. Weindicate the epochs at which we extract the test errors by the dashed black lines (epoch 1 for entand epoch 5for RPLq=0.8).
Figure 9: Systematic predictions of a vanilla ResNet50 on IN-D for different domains.
