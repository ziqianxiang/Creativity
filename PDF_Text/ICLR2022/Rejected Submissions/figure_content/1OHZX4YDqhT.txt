Figure 1: Illustration of Federated Neural Architecture Search (step 1: search locally; step 2: send thegradients of α and w to the server; step 3: merge gradients to get global α and w; step 4: synchronizethe updated α and w to each client.)their design is only in optimization level and does not consider the efficacy of model selection andneural architecture design, leading to a suboptimal solution when using a pre-defined model.
Figure 2: Search SpaceNormally, NAS includes three consecutive components: the search space definition, the searchalgorithm, and the performance estimation strategy Hutter et al. (2019). Our search space follows themixed-operation search space defined in DARTS Liu et al. (2018) and MiLeNAS He et al. (2020c),where we search in two shared convolutional cells and then build it up as an entire model architecture(as shown in Figure 2). Inside the cell, to relax the categorical candidate operations between twonodes (e.g., convolution, max pooling, skip connection, zero) to a continuous search space, mixedoperation using softmax over all possible operations is proposed:o(i,j)(x) = X	exp(aki，j))() = k= Pk0=1 exp(αkij))I------------------'ok (X)(3)'∙^^^^^^^^^{^^^^^^^^^^pkwhere the weight Pk of the mixed operation o(i,j) (x) for a pair of nodes (i,j) is parameterized by avectorαi,j. Thus, all architecture operation options inside a network (model) can be parameterizedasα. More details are introduced in Appendix A.1.1.
Figure 3: Abstract System Architecture of AutoFLWe design an AutoFL system using FedNAS based on FedML He et al. (2020b), an open-sourceresearch library for federated learning. The system architecture is shown in Figure 3. This design sep-arates the communication and the model training into two core components shared by the server andclients. The first is the communication protocol component responsible for low-level communicationamong the server and clients. The second is the on-device deep learning component, which is builtbased on the popular deep learning framework PyTorch. These two components are encapsulated asComManager, Trainer, and Aggregator, providing high-level APIs for the above layers. With the helpof these APIs, in ClientManager, the client can train or search for better architectures and then sendits results to the server-side. In contrast, in ServerManager, the server can aggregate and synchronizethe model architecture and the model parameters with the client-side. More details of the systemdesign can be found in the Appendix.
Figure 4: CIFAR10: Label Skew PartitionClient number(b) Label Allocation per ClientFor personalized model experiments, we generate non-IID data by label skewed partition. In thispartition scheme, we assign images of only five classes to each client and keep the number of imagesper client the same, namely 3000, as shown in Figure 4. For each client, we further split these 3000images into the training and testing datasets by using 75% data, i.e., 2250 images, for training, andthe other 25% as testing data. We perform this split to test personalization as it requires each client tohave their own local test dataset. We also explore latent Dirichlet distribution (LDA) based non-IIDdata distribution for the personalized model setup, and details of this distribution for this experimentcan be found in the appendix A.1.2. Since the model performance is sensitive to the data distribution,we fix the non-IID dataset in all experiments for a fair comparison.
Figure 5: Visualization of validation accuracy of each client and accuracy improvement distributionfor personalized model search.
Figure 6: Test Accuracy on Non-IID Dataset (multiple runs) FedAvg on DenseNet vs. FedNASFigure 6 demonstrates the performance of FedNAS vs. FedAvg. We use a specific non-IID datadistribution given in appendix A.1.2 and keep it fixed for both experiments. For a fair comparison,results are obtained by fine-tuning hyperparameters of each method, and each method is run threetimes. Details of hyperparameter tunning can be found in the appendix A.1.4.
