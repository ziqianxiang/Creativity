Figure 1: Illustrating the achieved fairness of normal training, traditional fair training and ourproposed robust fair training algorithms. Horizontal and vertical axes represent input x and corre-sponding loss value L(x), respectively. Solid blue curves show the loss landscapes. Circles denotemajority data points (xa and x0a), while triangles denote minority data points (xi and x0i). Greenpoints (xa and xi) are in-distribution data while red ones (x0a and x0i) are sampled from test sets withdistribution shifts. (a) Normal training results in unfair models: minority group has worse perfor-mance (i.e., larger loss values). (b) Traditional fair training algorithms can achieve in-distributionfairness but not in a robust way: a small distribution shift can break the fairness due to loss cur-vature biases across different groups. In fact, such learned fair models can have almost the samelarge bias as the normally trained models when facing distribution shifts. (c) Our robust fair trainingalgorithm can simultaneously achieve fairness both on in-distribution data and at distribution shifts,by matching both loss values and loss curvatures across different groups.
Figure 2: The overall framework of CUMA. x isthe input sample. ht is the utility head for the tar-get task. ha is the adversarial head to predict sen-SitiVe attributes. fs is the shared backbone. C(∙)is the curvature estimation function, as defined inEq. (4). Q1 and Q2 are local curVature distri-butions of majority and minority groups, respec-tiVely. Lcm , Lclf and Ladv are three loss terms asdefined in Eq. (9) and (11).
Figure 3: Trade-off curves between fairness and accuracy of different methods. Results are reportedon C&C dataset with “RacePctBlack” as the sensitive attribute.
