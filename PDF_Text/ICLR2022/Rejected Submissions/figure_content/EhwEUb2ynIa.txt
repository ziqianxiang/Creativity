Figure 1: Illustration of multiple methods for adapting CLIP to downstream image classification tasks. Eachlabeled approach can be used separately for fine-tuning the CLIP model. We analyze a variety of fine-tuningmethods such as prompt tuning by prepending a learnable prompt, tuning Layer Normalization parameters,inserting adapter and compacter modules in-between the Transformer layers, and using a linear probe on top ofvisual features. The CLIP model can be also used for inference on a downstream task in a zero-shot manner.
Figure 2: Parameter count and architectures of fine-tuning methods. All of the fine-tuning methods We considertune only a small fraction of the total number of parameters and act in different ways on the model. LayerNorm-tuning only trains existing Layer Normalization parameters across all Transformer layers. The remainingapproaches inject additional parameters which act on different parts of the model: the input, intermediateactivations, and output. Prompt tuning prepends a learnable prompt to the input text embeddings of classes.
Figure 3: Comparison of fine-tuning methods across different regimes of training data and CLIP zero shotperformance. Within each quadrant, results are averaged over all corresponding datasets. LayerNorm tuning isthe strongest baseline and performs the best in all regimes. All fine-tuning methods generally provide a largebenefit over zero-shot CLIP.
Figure 4: Accuracy of baseline fine-tuningmethods across datasets in the low and high-data regimes. Datasets are ordered along thex-axis by average performance of fine-tuningmethods. Flowers and Food101 datasets areommited from the high data regime due tolack of data. We observe that LayerNormtuning is a strong baseline across all regimesand datasets and provides substantial gainsover zero-shot CLIP.
Figure 5: Comparison of fine-tuning methods initialized with fine-tuned LayerNorm across different regimesof training data and CLIP zero shot performance. Although there is no clear best combination across all fourregimes, we recommend general recipes of using Linear Probe with fine-tuned initialization in the high dataregimes and Compacter with fine-tuned initialization in the low data regimes.
