Figure 1: Retrieval-augmented agent (R2A) architecture: (A) R2A augments the agent with a retrieval process. The retrieval process andthe agent maintain separate internal states, mt and st, respectively. The retrieval process retrieves information relevant to the agent’s currentinternal state st from the retrieval batch, which is a pre-processed sample from the retrieval dataset B. The retrieved information ut is usedby the agent process to inform its output (e.g., a policy or value function). (B) A batch of raw trajectories is sampled from the retrieval datasetB and encoded (using the same encoder as the agent). Each encoded trajectory is then summarized via forward and a backward summarizationfunctions (section 2.2) and sent to the retrieval process. (C) The retrieval process is parameterized as a recurrent model and the internalstate mt is partitioned into slots. Each slot independently retrieves information from the retrieval batch, which is used to update the slot’srepresentation and sent to the agent process in ut. Slots also interact with each other via self-attention. See section 2.3 for more details.
Figure 2: Relative improvement of retrieval-augmented R2D2 vs vanilla R2D2 on different Atari games, measured by human normalized score.
Figure 3: Relative performance of ablatedRA-R2D2 versus baseline R2D2 for 5 ab-lations on 10 Atari games. black lines rep-resent Standard deviation.
Figure 4: Gridroboman: Multi-task offline RL with a task-specific retrieval dataset. Average episode return vs. learner steps for the multi-task gridroboman environment when training and evaluating on 10, 20, and 30 tasks. With fewer tasks (a), the baseline DQN agent (blue) andthe retrieval-augmented DQN agent (orange) perform identically; however, when the number of tasks increases (b, c), the retrieval-augmentedagent learns much more effectively than the baseline DQN agent. Results are the average of 3 seeds for each method. Curves for individualtasks are shown in the appendix (Figure 12).
Figure 5: Retrieval of most relevant trajectories and states. A) The retrieval process selects the top-ktraj most relevant trajectories as specifiedin step 2, section 2.3. In the figure scalars represent attention weights. B) Then the retrieval process retrieves relevant information from theselected trajectories by selecting the most relevant states from the top-k trajectories as detailed in step 3, section 2.3Identification of most relevant trajectories and states for each slot. The retrieval mechanismprocess uses an attention mechanism to match a query produced by the the retrieval state associatedwith each slot to keys computed on each time step of each trajectory of the retrieval batch. Thisprocess assigns a single scalar to each state within a trajectory. These scores are used to assign ascalar to each trajectory and then normalized across trajectories. These normalized scores are thenused by the retrieval process to select the top-ktraj most relevant trajectories (see Figure. 5). Theprocess is then again repeated to select most relevant states with the selected trajectories. Here, asingle scalar is assigned to each state with in selected trajectories and then normalized across all thestates. These scores are then used to select most relevant states within the most relevant trajectories.
Figure 6: Each trajectory in the retrieval batch is summarized via a forward and a backward running parameteric structured model (Transformersor slot based recurrent network). Each state in the trajectory is represented by a (ht , bt) tuple where ht represents the summary of the pastand bt represents the summary of the future with that trajectory.
Figure 10: Relative performance of ablated RA-R2D2 versus baseline R2D2 for 6 ablations on 5 Atari games on which RA-R2D2 performsworse than baseline R2D2.
Figure 7: Comparing R2Awith R2D2: Learning curves for retrieval-augmented R2D2 (RA-R2D2) and the R2D2 baseline across differentAtari games. Green is RA-R2D2 and orange is baseline R2D2.
Figure 8: Comparing R2Awith R2D2: Learning curves for retrieval-augmented R2D2 (RA-R2D2) and the R2D2 baseline across differentAtari games. Green is RA-R2D2 and orange is baseline R2D2.
Figure 9: Comparing R2Awith R2D2: Learning curves for retrieval-augmented R2D2 (RA-R2D2) and the R2D2 baseline across differentAtari games. Green is RA-R2D2 and orange is baseline R2D2.
Figure 11: Gridroboman environment illustration. On the board there are three colored objects and the robot is represented by a black block.
Figure 12: Multi-task offline RL with a task-specific retrieval dataset. Evaluation performance for RA-DQN (orange) and baseline DQN(blue) when training on all 30 gridroboman tasks with a single agent. Curves show the performance of each agent (averaged over 3 seeds)when running that agent online in the environment on the specified task.
