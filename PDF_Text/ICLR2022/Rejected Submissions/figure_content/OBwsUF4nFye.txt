Figure 1: An MTL problem consists of m different tasks and a learning algorithm that jointly produces onemodel for each task (Left). For example, in cross-device federated learning, each ‘task’ may represent datafrom a mobile phone client (as depicted), and MTL can be used to learn shared, yet personalized models foreach client (Smith et al., 2017). In traditional differential privacy, if the private data of task k (e.g., Task 1)changes, the models produced by the MTL algorithm should be indistinguishable from the models derivedwithout changing data from task k (Middle). In contrast, JDP allows the model of task k to be dependent on taskk’s data while still protecting other tasks from leaking information about their private data (Right).
Figure 2:	Loss and privacy parameter vs. communication rounds for PMTL. The blue line shows the change ofprivacy parameter in terms of number of communication rounds during training. The orange line shows theaverage training loss across all tasks.
Figure 3:	Comparison of PMTL and training a private global model.
