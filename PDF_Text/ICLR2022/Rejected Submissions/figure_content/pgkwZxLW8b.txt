Figure 1: An FedSS training round: The client sends a set of obfuscated class labels Sk to the FLserver and receives the feature extractor å¤• and a few columns WSk, corresponding to classes inSk, from the weight matrix of the classification layer. The client optimizes this sub network withthe sampled softmax loss and then communicates back the model update to the server. The serveraggregates the model updates from all the selected clients to construct a new global model for thenext round.
Figure 2: The set of classes providing pushing forces for the local training under different sampledSoftmax loss formulations. (a) Input-dependent negative classes (depicted by the red squares) aresampled wrt to the inputs and current model, not feasible in the FL setting. (b) Only using thesampled negatives reduces the problem to a binary classification. (c) Using only the local positiveslets the local objectives diverge from the global one. (d) FedSS approximates the global objectivewith sampled negative classes together with local positives.
Figure 3: Learning curve for different methods for an average value of number of classes |Sk | on theclients. The PosOnly, FedAwS and FullSoftmax methods have |Pk|, |Pk| and n classes respectively,on the clients.
Figure 4: Convergence curves for the proposed FedSS method at different cardinalities of Sk . Giventhat Pk is fixed for a client, the increase in |Sk| is caused by increase in |Nk|. The estimate ofsoftmax probability via sampled softmax improves with the increase in |Sk|, and therefore improvingthe efficacy of the method.
Figure 5: Performance vs number of parameters in the classification layer transmitted to and opti-mized by the clients for Landmarks-Users-160k (a) and the SOP (b) datasets, respectively.
Figure 6: Performance of the FedSS (Ours) and NegOnly methods with different compositionsof the negative classes used for computing the sampled softmax loss. Utilizing on-client classesas additional negatives i.e, FedSS method, has superior performance to the NegOnly method withequivalent number of negatives.
Figure 7: Confusion matrices for Pk of the same client from Landmarks-User-160k dataset. In boththe FedSS and NegOnly formulations we used |Sk | = 95. In the former, the class representationsare learned and well-separated, but are collapsed in the latter.
Figure 8: The number of parameters in the classification layer dominates the model as the numberof classes n grows. We show the percentage of parameters in the last layer using the MobileNetV3architecture (Howard et al., 2019) while varying the number of classes n and dimension d of thefeature (d = 1280 is the default dimensionality of MobileNetV3).
Figure 9: Empirical FedSS gradient noise analysis. As we increase the sample size the differencebetween FedAvg (with FullSoftmax) and FedSS diminishes.
Figure 10: ImageNet-21k: Top-1 accuracy vs number of parameters in the classification layer trans-mitted to and optimized by the clients.
