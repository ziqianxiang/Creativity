Figure 1: In classical Dyna-Style methods, the world model generates episodes starting from a realenvironment state. Then, our robot can practice in the world model and learn how to manipulate theoriginal object. To improve the generalization of the learned policy, we further modified the objectproperty in the state. So the robot has the chance to play with objects with more diverse properties.
Figure 2: The structure causal model of a robot environment. The time-invariant property is modeledas a node m across the temporal dimension that affects all the causal mechanisms. s-m,t and atdenotes the time-variant state and the action at the step t, respectively.
Figure 3: Experimental results of counterfactual property generation in the out-of-distribution ex-periment. The vertical black line shows the boundary between the seen and unseen property valuesduring training. The left part is the seen region. Counterfactually generating the simulated episodeswith unseen property value helps alleviate the performance drop when evaluating unseen propertyduring training. Numbers in the legend denote the average performance in the unseen value range.
Figure 4:	The learning curve of the evaluated models on the training property range in the out-of-distribution experiments. CausalDyna converges as fast as MBPO, although it generates 20%less simulated episodes in the training property range. Numbers in the legend denote the averageAUCRatio.
Figure 5:	Experimental results of counterfactual property generation in the unbalanced distributionexperiment. When counterfactually generating episodes where the object is less encountered duringtraining, CausalDyna helps improve the policy performance on both the objects with head valuesand tail values. For each property, the median value occurs 90% of the time in the environment,and the rest two values share the remaining 10% equally. Numbers in the legend denote the averageperformance over the tail values. Each model has 6 training cases.
Figure 6: Average policy performance at different environment steps. Our method CausalDyna,which counterfactually generating episodes where the object is less frequently encountered duringtraining, reduces the required amount of environment steps and shows the best sample efficiency inthe unbalanced training distribution experiment. Numbers in the legend denote the average AUCRa-tio. Each model has 6 training cases.
Figure 7: CausalDyna with the heavy tail object. Pushing Mass, Unbalanced Training Distribution.
Figure 8: MBPO with the heavy tail object. Pushing Mass, Unbalanced Training Distribution.
