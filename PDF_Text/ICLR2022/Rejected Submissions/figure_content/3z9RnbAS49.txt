Figure 1: Experimental results of the proposed model for both (a, b, c) constant and (d, e, f) time-varyingcases. (a, b) For the constant case, we plotted points with the ratio of the learning rate to the batchsize on the horizontal axis of a log scale and the generalization error on the vertical axis. (d, e)For the time-varying case, we plotted points with the ratio of the initial learning rate to the batchsize and the ratio of the final learning rate to the batch size on the horizontal axis of the log scale,and the generalization error on the vertical axis. (a, b, d, e) For all cases, we also show the resultsfitted by the proposed model as green lines and surfaces, respectively. (c, f) The true generalizationerror and estimation error are plotted, and the mean and standard deviation of the relative error δ arerepresented by μ and σ ,respectively.
Figure 2: Experimental results on the stability of the generalization error model for (a, b) CIFAR10 and (c, d)CIFAR100, for the (a, c) constant case and (b, d) time-varying case. The horizontal axis shows thenumber of networks randomly selected to estimate the generalization error, whereas the vertical axisshows the mean value μ and standard deviation σ of the relative error δ. The shaded areas representone standard deviation from the mean in each direction.
Figure 3: Hyperparameter optimization results of several methods. The number of trained models is plottedon the horizontal axis, and the maximum accuracy achieved among the trained models is plotted onthe vertical axis for the (a, b, c, d) constant and (e, f, g, h) time-varying cases. The 95% confi-dence intervals are shaded. The proposed method always searches for a uniform range, whereas thecomparison method searches the range written in the caption (uniform or log-uniform).
