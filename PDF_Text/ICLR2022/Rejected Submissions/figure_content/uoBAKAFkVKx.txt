Figure 1: Training performance for HDCA compared to ARS and ES for theLunarLanderContinuous-v2 environment. The estimated expected cumulative reward is plot-ted against the training iteration, where training continued until the expected reward minus itsstandard deviation exceeded the threshold of 200. HDCA and ES were trained on a neural networkwith one hidden layer consisting of 100 neurons, while ARS trained a linear policy. 5,000 directionswere evaluated at each training iteration for all three methods, and the number of coordinatesperturbed in each subplot refers to the block size b for HDCA.
Figure 2: Training performance for HDCA-10 compared to ARS and ES for theLunarLanderContinuous-v2 environment. The estimated expected cumulative reward is plottedagainst the training iteration, where training continued until the expected reward minus its stan-dard deviation exceeded the threshold of 200. The left figure shows the expected number of trainingiterations when 250 samples are taken at each iteration, and the right shows the same relationshipfor 5000 samples.
