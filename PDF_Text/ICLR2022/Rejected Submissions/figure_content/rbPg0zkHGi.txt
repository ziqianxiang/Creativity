Figure 1: Classification performance due to different active learning methods on Cifar10 (top left),Cifar100 (top right), SVHN (bottom left), and Caltech101 (bottom right).
Figure 2: Classification performance due to different active learning methods on the semantic seg-mentation task Cityscapes.
Figure 3: Evaluations of different hyper-parameters on Cifar100: noise magnitude Î» (left), samplingtimes K (middle) and noise distribution (right). In the left and middle plots, we use dots withdifferent colors to denote different cycles, e.g. the blue dots at the bottom refer to cycle 1 and thepink dots at the top refer to cycle 7.
Figure 4: Evaluation of using larger selection size, denoted by non-iterative as it only performs anone-off selection. Results with the standard setting (denoted as iterative), copied from Figure 1, arealso shown for comparison. Experiments are conducted on Cifar10 (left) and Cifar100 (right).
Figure 5: Classification performance due to different Bayesian active learning methods on MNIST(left) and Cifar10 (right).
Figure 6: Classification performance due to different active learning methods on the cryo-ET task:50c-snr003.
Figure 7: Regression performance due to different active learning methods on the Ames Housingdataset. The mean absolute error is compared. Lower is better.
Figure 8: Classification performance due to different Bayesian active learning methods on MNISTwith a selection size of 10 (left) and 1 (right), respectively.
Figure 9: Classification performance due to different active learning methods on the ImageNetdataset. Top-1 Accuracy is reported.
Figure 10: Classification performance due to different active learning methods on Cifar10 (left) andCifar100 (right), using MobileNet-v2 as the backbone.
Figure 11: Ablation study of our NoiseStability method on Cifar10. We present the effect of adaptivenoise magnitude (left) and the position to inject noise (right).
