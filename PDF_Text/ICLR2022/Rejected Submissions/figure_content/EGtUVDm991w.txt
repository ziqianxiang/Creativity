Figure 1: (a) We propose Token Pooling, a novel token downsampling method, for visual trans-formers. (b) The proposed method achieves a state-of-the-art trade-off between accuracy and com-putation. (c) Token Pooling uses cluster analysis to aggregate information from individual tokensautomatically. We show the input images and the token clusters at the 6-th layer of DeiT-S.
Figure 2: Score-based downsampling methods (Goyal et al., 2020; Rao et al., 2021) vs. our method.
Figure 3: Main results. (a) shows the accuracy when we apply different downsampling methodsto DeiT-S. More is in Appendix G. (b) shows a comparison between the proposed method with thestate-of-the-art downsampling methods. The results of our method and PoWER-BERT are acquiredby varying K and the base architecture among DeiT-Ti, DeiT-e252, DeiT-e318, and DeiT-S.
Figure 5: Ablation studies of (a) downsampling methods using significance score, and (b) proposedToken Pooling using different clustering algorithms. The base model is DeiT-S for all methods.
Figure 4: The figure shows the results when weapply Token Pooling to various DeiT architec-tures. Token Pooling consistently improves thecomputation-accuracy trade-off for all evaluatedarchitectures. By utilizing both Token Poolingand architecture search, we can further improvethe accuracy at a given flops budget. For exam-ple, at 1 Gflop, we should use Token Pooling onDeiT-e252 instead of DeiT-S.
Figure 6: Default initialization vs. random initialization. Our Token Pooling is robust to initializationof clustering algorithms. The default initialization is top-K w.r.t. significance score, see Algorithm 1.
Figure 7:	This figure shows the results of the pretrained DeiT models provided by Touvron et al.
Figure 8:	Results of convolution downsamplingE	Results without finetuningSince Token Pooling minimizes the reconstruction error due to token downsampling, in this section,we evaluate the performance when we insert Token Pooling layers into a pre-trained model withoutfinetuning. Figure 9 shows the results when we directly insert Token Pooling layers (using thesame downsampling ratios in Figure 5b) into a pretrained DeiT-S. As can be seen, minimizing thereconstruction error, Token Pooling preserves information that enables the model to retain accuracyduring token downsampling.
Figure 9: The figure shows the performance results when we insert Token Pooling layers into aPretrained DeiT-S without finetuning the model (i.e., skiP steP 3 of the training Protocol in Section 5).
Figure 10: Results of models using normalized key and query vectors with (a) α = 1 and (b) learnedα in (6). The base model architecture is DeiT-S.
Figure 11: This figure compares the cost-accuracy curves of Token Pooling with PoWER-BERTusing (a) DeiT-e252 and (b) DeiT-e318 as the base architectures.
Figure 12: Throughput of our PyTorch implementation. Note that our implementation has not beenoptimized for the throughput. The throughput is measured in frames (images) per second (fps).
