Figure 1: The network structure of Bootstrapped HER.
Figure 2: Variance of the Q-values estimated from multiple heads in BHER on Reacher. The leftside denotes the variances of the hindsight transitions (with modified goals) in the replay buffer,while the right side plots the variances of the original transitions. In this illustration experiment, theratio of sampled original data is set to 50%, and another half of data is composed by the hindsightsamples (in the original work of HER, this ratio is set to 20% by default). This is to eliminate theimpact of data imbalance on evaluating the variances in the data. In our formal experiments, thisratio keeps the value as the same in HER.
Figure 3: Environment.
Figure 4:	Performance for allenvironments: success rate (line) with standard deviation range (shadedarea) on all environments acorss 10 random seeds.
Figure 5:	Ablation experiments: success rate (line) with standard deviation range (shaded area) onall environments acorss 10 random seeds.
Figure 6: Performance of BHER with different priority temperature coefficient.
Figure 6:	Ablation experiments results for original data.
Figure 7:	Ablation experiments results for original data and higher variance first.
Figure 7: Distribution of the goals in Reacher. (BHER w/o prioritization only uses bootstrap princi-ple on the basis of HER, but does not use any priority replay)can explore more goals in a shorter period of time. This shows that bootstrap principle has improvedthe agentâ€™s exploration. Fig. 8(b) and 8(c) illustrates that we can make the agent preferentially selectmore valuable goals to learn by using counterintuitive prioritization. This effectively improves theexploitation of agent, which thus learns the optimal strategy faster.
