Figure 1: Illustration of the optimization process of Adam and AdaMomentum. A general loss curvecan be composed to three areas: A) transition from a plateau to a downgrade; B) a steep downgrade;C) from downgrade to entering the basin containing the optimum. An ideal optimizer ought to sus-tain large stepsize before reaching the optimum and reduce its stepsize near the optimum. Comparedto Adam, AdaMomentum can adapt the true stepsize more appropriately along the loss curve andmaintain smaller stepsize near convergence. Refer to Section 3.1 for more detailed analysis.
Figure 2: 2D Trajectory visualization of SGDM, Adam, RMSprop, AdaBelief and AdaMomentumon classic functions. AdaMomentum reaches the optimal point (marked as purple cross) the fastestin all the cases and converges stably to the optimum without big oscillations. Best viewed in color.
Figure 3:	Train and test accuracy of different optimizers on CIFAR-10 (Krizhevsky & Hinton, 2009).
Figure 4:	Test perplexity curve on Penn Treebank (Marcus et al., 1993) dataset.
Figure 5: Train perplexity Curve on Penn Treebank dataset.
