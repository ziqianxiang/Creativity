Figure 1: Our training pipeline. Following best practice we take advantage of extensively pretrainedtransformers, in this case using the same DeepDev-py sequence-to-sequence model used to finetunePyMT5 (Clement et al., 2020). We first use commit data to train both a baseline bugpatcher modelalong with a bug-creator model. Our bug-creator models feeds twenty times as much data toDeepDebug (backtrans). Finally we finetune this model on neural bugs injected into functions thathave executable tests and produce traces, thus obtaining its final evolution DeepDebug (traces).
Figure 2:	Our bug-creator model replaces kwargs.pop with kwargs.get, as used elsewhere in thefunction.
Figure 3:	Our bug-creator model replaces a check for the bucket name starting with self.name with astricter equality check.
Figure 4:	Our bug-creator model deletes the break, leading sax to take the last small value in sections,instead of the first. Our model also introduces a trivial indentation error on the penultimate line.
Figure 5: Example output for a sin-gle frame of the stack trace producedby Pytest . In general there are sev-eral frames for a given stack trace.
Figure 6: Example skeleton using just400 tokens. Note the inclusion ofthe entire buggy focal function (withits name highlighted for the reader)including the control tokens ‘# tar-get edit’ and ‘# end’, as well as theimported functions. Due to limitedspace, the skeleton can only fit allsignatures from the focal class, alongwith several more signatures from out-side the focal class. The skeletonswe use in practice contain more to-kens and thus tend to be fleshier i.e.
