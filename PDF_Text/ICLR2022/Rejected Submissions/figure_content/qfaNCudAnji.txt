Figure 1: Final performance gain for DQNPro over vanilla DQN. Results are averaged over fourrandom seeds, and the y-axis is scaled logarithmically.
Figure 2:	Learning curves for DQN (blue) and DQNPro (red) on four Atari games. X-axisindicates the number of steps from the environment used in training and Y-axis shows undiscountedreturn (sum of rewards). See also Figure S4 for learning curves for all experimental environments.
Figure 3:	A comparison betweenDQNPro and various deep RL base-lines. To compare the agents acrossall games, we first normalize the per-formance of all agents so we can av-erage over all games meaningfully. Tothis end, for each combination of al-gorithm, game, and number of train-ing iterations, we compute the perfor-mance relative to the best policy learnedby Rainbow (Hessel et al., 2018). Wethen average out this quantity across allgames.
Figure 4: The effect of the proximal term on the magnitude of updates to the target network.
Figure 5: A comparison between DQNPro and DQN with periodic (Top) and Polyak (Bottom)updates for target network.
Figure S1:	Learning curves for DQN with prioritized experience replay (PER) (gary) andDQNPro (red) on 15 Atari games. X-axis indicates the number of steps from the environmentused in training and Y-axis shows average undiscounted return.
Figure S2:	Performance gain for DDQNPro over the original DDQN in term of the final per-formance. Results are averaged over four random seeds.The y-axis is scaled logarithmically.
Figure S3:	Learning curves for DDQN (green) and DDQNPro (orange) on 40 Atari games. X-axis indicates the number of steps from the environment used in training and Y-axis shows averageundiscounted return.
Figure S4:	Learning curves for DQN (blue) and DQNPro (red) on 40 Atari games. X-axis indi-cates the number of steps from the environment used in training and Y-axis shows average undis-counted return17Under review as a conference paper at ICLR 2022▽h(W)J Vw 丽Figure S5:	A comparison between DQN, DQNPro in parameter space, and DQN in function space.
Figure S5:	A comparison between DQN, DQNPro in parameter space, and DQN in function space.
