Figure 1: We compare the offline-online setting to batch RL. We see that while training only offlinedoes not perform well in any environment alone, allowing learning afterwards can significantlyimprove the performance of the policy.
Figure 2: We compare the offline-online setting to purely online learning. We plot learning curves forstandard online DQN/TTN, online DQN/TTN initialized with the offline dataset and offline-onlineDQN/TTN. We find that initializing with the offline dataset is better than the standard procedure butadditionally training on it is still best.
Figure 3: We present learning curves for two dataset sizes in the offline-online setting, 10 thousandand 50 thousand transitions for DQN and TTN. The larger dataset provides a noticeable boost.
Figure 4: We compare the offline-online setting to batch RL when a dataset of 50 thousand transitionsis used. We observe qualitatively similar behaviour as with 10 thousand transitions.
Figure 5: We compare the offline-online setting to batch RL. A dataset of 50 thousand transitions isused.
Figure 6: We compare the offline-online setting to purely online learning. We plot learning curves forstandard online DQN/TTN, online DQN/TTN initialized with the offline dataset and offline-onlineDQN/TTN. We use a dataset of 50 thousand transitions here and find qualitatively similar results tousing 10 thousand transitions, although performance is usually increased.
