Figure 1: Overview of training an MoE with DTS-Gate. At the beginning, the experts are not trainedand tokens would be distributed to all experts to enable sufficient updates for experts. As the trainingcontinues, the output weights of gate begin to diverge because experts tend to have their own domain.
Figure 2: The 16 experts’ weights and loads distribution of GPT-MoE-Dense (all experts are ac-tivated for each token) among 12 MoE-layers (totally 24 layer). Figure 2(a) shows that the gate’sweights to each expert is uneven. And in Figure 2(b), we route tokens to its Top-1 expert based onthe DenseMoE gate, and the loads between experts is naturally imbalanced.
Figure 3: End-to-end performance comparison between three models. Figure 3(a) and Figure 3(b)represent the curve of PPL over iterations and FLOPs, where DTS-Gate can obtain 2.0x speed-up toreach the same validation perplexity, as well as higher FLOPs-efficiency of a 1.42x speed-up.
Figure 4: Scalability for MoE-DTS models. It shows that More experts or more MoE-layers (largermodels with constant FLOPs), will lead to better FLOPs-efficiency.
Figure 5: Different Temperature Scheduler. We vary the temperature interval values and experimentsshow that larger beginning temperature will cost more computation at early stage, while provide littleextra gain (1 to 0.1 V.S. 2 to 0.1). Small temperature at end will lead to the model degradation, showby 1 to 0.1 and 2 to 0.1 at 15000 iteration in Figure 5(a)Increase the Expert Number Based on the 12-layer GPT-Small model(117M), we replace the7th FFN layer by one MoE layer and vary its experts number within {1, 4, 8, 16}. As shown byFigure 4(a), MoE-DTS keeps consistent improvements when increasing expert numbers.
Figure 6: Effect of Balanced LoadsTokens Percentage of Each Expert0123456789101112131415Expert ID0.60.50.4-0.3-0.2-0.1-0.0(b) Expert Loads(40960 tokens)Max/Min Temperature Under small temperatures, the weight distribution of experts is close toone-hot, which leads to the one-token-one-expert distribution and high computation efficiency, butthe variance of gradients is large. In the contrast, large temperatures result in nearly uniform dis-tribution gate weights, which evolves more experts into training but the variance of gradients issmall. We schedule the temperature τ to start from a large value and anneal it to a small τ gradually.
Figure 7: The Loads of each expert in 7th MoE-Layer, with 16 experts in total, accumulated of 16sentence.
Figure 8: The Loads of each expert in 7th MoE-Layer, with 16 experts in total, in the sentence level.
Figure 9: Average expert per token during training, with 16 experts in each MoE layer.
