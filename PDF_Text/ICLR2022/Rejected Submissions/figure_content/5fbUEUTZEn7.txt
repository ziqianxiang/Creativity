Figure 1: The proposed GKNN architecture. The input graph is fed into one or more GKC layers,where sub-graphs centered at each node are compared to a series of structural masks through akernel function. The output is a new set of real-valued feature vectors associated to the graph nodes,which goes through a vector quantization operation. We obtain a graph-level feature vector throughpooling on the nodes features, which is then fed to an MLP to output the final classification label.
Figure 2: Ablation study: bar plots of classification accuracy with standard error. Left to right:number of nodes (i.e., structural mask size), number of structural masks, kernel functions (Wl,WL with Optimal Assignment, Graphlet, Propagation, and Pyramid match kernel), subgraph radius,number of GKC layers, and weight of the JSD loss.
Figure 3: Interpretability analysis results. Each column refers to a graph motif. Upper row: originalmotif. Mid row: structural mask with the strongest response. Bottom row: a sample graph includingthe motif. Colors indicate the node response to the filter, with lighter colours (yellow) indicating ahigh response and darker colors (blue) indicating a low response.
Figure 4: Top 4 significant structural masks (top) learned by our model on the MUTAG dataset andtheir response over input graph nodes (bottom). The left bar plot shows the impact of each structuralmask on the classification loss.
Figure 5: Top 4 significant structural masks learned by our model on the MUTAG (left) and PTC(right) datasets starting from two different random initializations of the model weights (rows).
Figure 6: Left: Evolution of the learned structural masks during the training epochs showing twodifferent initializations (belonging to two different models) that led to the same structural mask onthe MUTAG dataset. Right: Sample of training curves of a model trained on MUTAG.
