Figure 1: Visualization of SSL checkpoints for ImageNet (first row) and STL-10 (second row) ondifferent frameworks. Color is used to show the KNN Top1 accuracies. Red indicates high accuracy,while blue indicates low accuracy. Left Column is our proposed CL-ID framework while RightColumn is Align-Unif framework (Wang and Isola, 2020). Our framework shows qualitativelyconsistent results: Good representations tend have high CL as well as high ID (in the upper-rightcorner).
Figure 2: Correlation Plots between Top1 accuracy and its various predictors. The first row isImageNet and the second row is STL-10. First Column: Vanilla CL-ID Predictor. Second Column:Linear CL-ID Predictor. Third Column: Linear Align-Unif Predictor (Wang and Isola, 2020).
Figure 3: Robustness Analysis for ImageNet (First Row) and STL-10 (Second Row)). Left Col-umn: The heat map of Pearson Coefficient between the Top-1 accuracy and linear CL-ID predictor.
Figure 4: Trajectory of DeepCluster and KNN-DeepCluster for ImageNet (first row) and STL-10 (second row) for 400 epochs. Left: ID vs CL. Middle: Top-1 accuracy vs. ID. Right: Top-1accuracy vs. Linear CL-ID predictor (from Section 4.3). Our proposed modification makes therepresentation more learnable at convergence without a significant compromise of ID, leading to abetter solution. There is still a positive correlation between predictors obtained in Section 4.2 andthe performance, especially after models are warmed up (after epoch 8 in ImageNet and epoch 49 inSTL-10).
Figure 5: Other attempts on measuring expressiveness and learnability. In all cases, MI is estimatedby training a MINE network, where learnability is estimated by training student network that takesthe images and predicts the representations with cosine loss (Left) or L2 loss (Middle) or the K-means labels of the representations (Right).
Figure 6: Cluster Analysis with Davies-BoUldin (DB) index and the effect of replacing Kmeanslabels with KNN predictions on STL10 with 512 clusters. Left: After Kmeans, each cluster hasunequal size, and the larger cluster are also well separated (low DB index). Middle and Right:The originally well separated clusters gain size while the opposite ones lose size after KNN re-assignment. The smallest DB index clusters are least effected by the re-assignment.
