Figure 1: Softmax attention outPuts can onlylie within the convex hull sPanned by thevalue vectors vi (blue region). Removingthis constraint with our normalized attentionPooling (NAP) leads to an increased robust-ness to hyPerParameters (See Section 5.1.1.)Hyperparamter Settingnot both, of the following arguments: (1) Attention helPs with the information flow by Providingmore direct, dynamic links between inPuts and outPuts. (2) Attention is directly interPretable as onecan investigate the Percentages to which different inPuts are “attended” to. However, this secondargument has been challenged recently, as several works show that attention weights do not directlycorrelate with Predictions (Jain & Wallace, 2019; Wiegreffe & Pinter, 2019; Pruthi et al., 2019;Brunner et al., 2020; Pascual et al., 2020). With interPretability already in disPute, we focus on theother argument. SPecifically, we question whether attention is the best way to route information andargue that aPart from the Proven success, softmax attention might not always be the best oPtion.
Figure 2: The standard deviation (σ) and norm of apooling output depends on the sequence length N (x-axis) if the output is not normalized. Softmax atten-tion outputs (blue) scale similar to mean pooling atinitialization, i.e., Transformers focus more on localinformation in longer sequences. Experiment detailsare given in Appendix C.
Figure 3: Difference in 1 Transformer layer.
Figure 4: Left: Pseudo code of the data generation. The case distinction points 64 and 50 are chosenarbitrarily. Right: Task setup for outputs across all tokens (cf. Section 5.1) and outputs from the firsttoken (cf. Section 5.2). Green boxes represent the trainable network layers (shared across tokens)while red boxes represent the pooling across tokens. The targets of the displayed examples wouldbe [0, 0, 0, 1] and [1, 0, 0, 0], respectively.
Figure 5: Learning rate (y-axis) vs. model dimension d (x-axis) on the argmin-first-argmax casedistinction task (with output across all tokens). The pixels’ R (red), G (green) and B (blue) valuescorrespond to min-, mean- and max-accuracy, respectively, of the corresponding hyperparametercombination - see main text for details. The plot shows the validation accuracy when validating onsequences of length N = 64. Crosses indicate the combination for best validation accuracy, whichwe report with standard deviation behind the model name.
Figure 6: Biased data results on the case distinction task (with output across all tokens). RGBpixel values correspond to argmin-, first- and argmax-mean-case-accuracies, respectively. Top row:Learning rate (y-axis) vs. sequence length N (x-axis). Bottom row: Learning rate (y-axis) vs.
Figure 7: Learning rate (y-axis) vs. model dimension d (x-axis) on the case distinction task withoutput from the first token. RGB pixel values correspond to the case accuracies. Crosses indicatethe best accuracy, reported behind the model name.
Figure 8: Learning rate (y-axis) vs. model dimension d (x-axis) on the mode finding task. RGBpixel values correspond to min, mean and max accuracy. Crosses indicate the combination for bestaccuracy, reported behind the model name.
Figure 9: Learning rate (y-axis) vs. attention headsM (x-axis) on the case distinction task (first tokenoutput). RGB pixel values correspond to min, meanand max accuracy. Crosses indicate the combinationfor best accuracy, reported behind the model name.
Figure 10: Learning rate (y-axis) vs. model dimension d (x-axis). RGB pixel values correspondto min, mean and max validation performance. Crosses indicate the best combination (validation),from which we report the test performance. For NON, sum and max see Appendices I and J. Left:Protein-protein-interaction task. Shown is the node classification F1-score. Right: Altered workingmemory graph agent in the Baby-AI level 3 reinforcement learning task. Shown is the success rate.
Figure 11: Visual definitionBWa(g)1 — 2α + √1 — 4αln -------------/1 — 2α — ʌ/l — 4αProof. Substituting X = exp(li) and y = Ej= exp(lj) we can write g as g(x) = χ+y ∙(1 — χ+yj.
Figure 12: Schematics of 1 Transformer-layer block of the different architectures investigated.
Figure 13: Learning rate (y-axis) vs. model dimension d (x-axis) on the argmin-first-argmax casedistinction task (with output across all tokens) - architecture modification ablation study. In the firsttwo rows, RGB pixel values correspond to min-, mean- and max-accuracy. In the last two rows,RGB pixel values correspondto argmin-, first- and argmax-mean-case-accuracies. 1. row: Trainingaccuracy (sequence length N = 128). 2. row: Validation accuracy when validating on sequencesof half the length (N = 64). 3. row: Training case accuracy (sequence length N = 128). 4.
Figure 14: Learning rate (y-axis) vs. model dimension d (x-axis) in the different task setups whenjust replacing the softmax in BERT with normalization. The plots from left to right are groupedaccording to the tasks: The case task with outputs across all tokens (cf. Section 5.1), the case taskwith outputs from the first token (cf. Section 5.2) and the mode task (cf. Section 5.3). For the casetasks, the left sub-plot reports min, mean and max accuracies while the right sub-plot reports meancase accuracies as RGB pixel values. The mode plot reports min, mean and max accuracies. Theplots show the accuracies for N = 128.
Figure 15: Case accuracies over the course of training on the argmin-first-argmax case distinctiontask with output across all tokens, cf. Section 5.1. Each small sub-plot shows the case accuracies(y-axis, bottom is set to 0%, top to 100%) over the course of training (x-axis). Solid lines representthe mean accuracy over the 5 random seeds while shaded areas fill the spread between min- andmax-accuracy achieved. Models BERT and MTE are shown here, cf. Figures 16 and 17.
Figure 16: Case accuracies over the course of training on the argmin-first-argmax case distinctiontask with output across all tokens, cf. Section 5.1. Each small sub-plot shows the case accuracies(y-axis, bottom is set to 0%, top to 100%) over the course of training (x-axis). Solid lines representthe mean accuracy over the 5 random seeds while shaded areas fill the spread between min- andmax-accuracy achieved. Models NAP and NON are shown here, cf. Figures 15 and 17.
Figure 17: Case accuracies over the course of training on the argmin-first-argmax case distinctiontask with output across all tokens, cf. Section 5.1. Each small sub-plot shows the case accuracies(y-axis, bottom is set to 0%, top to 100%) over the course of training (x-axis). Solid lines representthe mean accuracy over the 5 random seeds while shaded areas fill the spread between min- andmax-accuracy achieved. Models sum and max are shown here, cf. Figures 15 and 16.
Figure 18: Learning rate (y-axis) vs. model dimension d (x-axis) on the argmin-first-argmax casedistinction task (with output across all tokens). The pixels’ R (red), G (green) and B (blue) valuescorrespond to min-, mean- and max-accuracy, respectively, of the corresponding hyperparametercombination. The plot shows the accuracy when evaluating on sequences of length N = 128.
Figure 19: Rerun of the experiments of Figure 5 with L = 4 instead of L = 2. Crosses indicate thecombination for best validation accuracy, which we report behind the model name.
Figure 20: Rerun of the experiments of Figure 5 with L = 6 instead of L = 2. Crosses indicate thecombination for best validation accuracy, which we report behind the model name.
Figure 21: Learning rate (y-axis) vs. batch size (x-axis) on the argmin-first-argmax case distinctiontask (with output across all tokens). RGB pixel values correspond to argmin-, first- and argmax-case-accuracies, respectively. Top row: Training accuracy (sequence length N = 128). Bottomrow: Validation accuracy on sequences of length N = 64.
Figure 22: Learning rate (y-axis) vs. initialization scale (x-axis) on the case distinction task (outputacross tokens). RGB pixel values correspond to the case-accuracies. Top row: Training accuracy(N = 128). Bottom row: Validation (N = 64).
Figure 23: Learning rate (y-axis) vs. percentage of identity-case (x-axis). R = identity accuracy, Gand B = argmin accuracy.
Figure 24: Learning rate (y-axis) vs. model dimension d (x-axis) on the case distinction task withoutput from the first token. RGB pixel values correspond to min, mean and max accuracy. Top row:Training accuracy (sequence length N = 128). Bottom row: Validation accuracy when validatingon sequences of half the length (N = 64). Crosses indicate the combination for best validationaccuracy, which we report with standard deviation behind the model name.
Figure 25: Learning rate (y-axis) vs. Transformer-layers L (x-axis) on the case distinction task(output from the first token). RGB pixel values correspond to min, mean and max accuracy. Toprow: Training accuracy (sequence length N = 128). Bottom row: Validation accuracy whenvalidating on sequences of half the length (N = 64). Crosses indicate the combination for bestvalidation accuracy, which we report with standard deviation behind the model name.
Figure 27: Learning rate (y-axis) vs. model dimension d (x-axis) in the different task setups withlearned aggregation weights. The plots from left to right are grouped according to the tasks: Thecase task with outputs across all tokens (cf. Section 5.1), the case task with outputs from the firsttoken (cf. Section 5.2) and the mode task (cf. Section 5.3). For the case tasks, the left sub-plotreports min, mean and max accuracies while the right sub-plot reports mean case accuracies as RGBpixel values. The mode plot reports min, mean and max accuracies. The plots show the accuraciesfor N = 128.
Figure 28: Full results of the GNN experiments on the protein-protein-interaction task. The plotshows a variation of learning rate (y-axis) vs. model dimension d (x-axis). RGB pixel valuescorrespond to min, mean and max validation node classification F1-score. Crosses indicate thebest combination (validation), from which we report the test performance behind the model name.
Figure 29: Full results of the RL experiments. The plot shows a variation of learning rate (y-axis) vs.
Figure 30: Replica of Figure 5: Learning rate (y-axis) vs. model dimension d (x-axis) on theargmin-first-argmax case distinction task (with output across all tokens). The rows from top tobottom correspond to min-, mean- and max-accuracy, respectively. The plots show the validationaccuracy when validating on sequences of length N = 64. Crosses indicate the combination for bestvalidation accuracy, which we report with standard deviation behind the model name.
Figure 31: Replica of Figure 6 (top row): Biased data results on the case distinction task (with outputacross all tokens). The rows from top to bottom correspond to argmin, first and argmax-mean-case-accuracies, respectively. Shown is the learning rate (y-axis) vs. sequence length N (x-axis).
Figure 32:	Replica of Figure 6 (bottom row): Biased data results on the case distinction task (withoutput across all tokens). The rows from top to bottom correspond to argmin, first and argmax-mean-case-accuracies, respectively. Shown is the learning rate (y-axis) vs. percentage of argmin-case inthe data (x-axis) with fixed N = 128.
Figure 33:	Replica of Figure 7: Learning rate (y-axis) vs. model dimension d (x-axis) on the casedistinction task with output from the first token. The rows from top to bottom correspond to argmin,first and argmax-mean-case-accuracies. Crosses indicate the best accuracy, reported behind themodel name.
Figure 34:	Replica of Figure 8: Learning rate (y-axis) vs. model dimension d (x-axis) on the modefinding task. The rows from top to bottom correspond to min, mean and max accuracy. Crossesindicate the combination for best accuracy, reported behind the model name.
Figure 35:	Replica of Figure 9: Learning rate (y-axis) vs. attention heads M (x-axis) on the casedistinction task (first token output). The rows from top to bottom correspond to min, mean and maxaccuracy. Crosses indicate the combination for best accuracy, reported behind the model name.
Figure 36: Replica of Figure 10: Learning rate (y-axis) vs. model dimension d (x-axis). The rowsfrom top to bottom correspond to min, mean and max validation performance. Crosses indicate thebest combination (validation), from which we report the test performance. Left: Protein-protein-interaction task. Shown is the node classification F1-score. Right: Altered working memory graphagent in the Baby-AI level 3 reinforcement learning task. Shown is the success rate.
Figure 37: Replica of Figure 13 (top two rows): Learning rate (y-axis) vs. model dimension d (x-axis) on the case distinction task (with output across all tokens) - ablation study. The rows fromtop to bottom correspond to min, mean and max accuracy for N = 128 (top 3 rows) and N = 64(bottom 3 rows). Crosses indicate the combination for best accuracy, reported in Table 1.
Figure 38: Replica of Figure 13 (bottom two rows): Learning rate (y-axis) vs. model dimensiond (x-axis) on the case distinction task (with output across all tokens) - ablation study. The rowsfrom top to bottom correspond to argmin, first and argmax accuracy for N = 128 (top 3 rows) andN = 64 (bottom 3 rows). Crosses indicate the combination for best accuracy, reported in Table 1.
Figure 39: Replica of Figure 14: Learning rate (y-axis) vs. model dimension d (x-axis) in thedifferent task setups when just replacing the softmax in BERT with normalization. The plots fromleft to right are grouped according to the tasks: The case task with outputs across all tokens (cf.
Figure 40: Replica of Figure 18: Learning rate (y-axis) vs. model dimension d (x-axis) on theargmin-first-argmax case distinction task (with output across all tokens). The rows from top tobottom show min, mean and max accuracy for N = 128. Crosses indicate the combination for bestaccuracy, reported behind the model name.
Figure 41: Replica of Figures 19 and 20: Learning rate (y-axis) vs. model dimension d (x-axis) onthe argmin-first-argmax case distinction task (with output across all tokens) with L = 4 (top 3 rows)and L = 6 (bottom 3 rows). The rows from top to bottom show min, mean and max accuracy forN = 64. Crosses indicate the combination for best accuracy, reported behind the model name.
Figure 42: Replica of Figure 21: Learning rate (y-axis) vs. batch size (x-axis) on the case distinctiontask (with output across all tokens). The rows from top to bottom correspond to argmin, first andargmax-mean-case-accuracies for N = 128 (top 3 rows) and N = 64 (bottom 3 rows). Crossesindicate the combination for best accuracy, reported behind the model name.
Figure 43: Replica of Figure 22: Learning rate (y-axis) vs. initialization scale (x-axis) on the casedistinction task (output across tokens). The rows from top to bottom correspond to argmin, first andargmax-mean-case-accuracies for N = 128 (top 3 rows) and N = 64 (bottom 3 rows).
Figure 44: Replica of Figure 23: Learning rate (y-axis) vs. percentage of identity-case (x-axis). Toprow: identity accuracy. Bottom row: argmin accuracy.
Figure 45: Replica of Figure 24: Learning rate (y-axis) vs. model dimension d (x-axis) on the casedistinction task (first token output). The rows from top to bottom show min, mean and max accuracyfor N = 128 and then N = 64. Crosses indicate the combination for best accuracy, reported behindthe model name.
Figure 46: Replica of Figure 25: Learning rate (y-axis) vs. Transformer-layers L (x-axis) on thecase distinction task (output from the first token). The rows from top to bottom correspond to min,mean and max accuracy for N = 128 (top 3 rows) and N = 64 (bottom 3 rows). Crosses indicatethe combination for best mean accuracy, reported behind the model name.
Figure 47: Replica of Figure 26: Learning rate (y-axis) vs. vocabulary size S (x-axis) on the modefinding task. The rows correspond to min, mean and max accuracy for N = 128 (top 3 rows) andN = 256 (bottom 3 rows). Crosses indicate the learning rate for best mean accuracy, reported inTable 5.
Figure 48: Replica of Figure 27: Learning rate (y-axis) vs. model dimension d (x-axis) in thedifferent task setups with learned aggregation weights. The plots from left to right are groupedaccording to the tasks: The case task with outputs across all tokens (cf. Section 5.1), the case taskwith outputs from the first token (cf. Section 5.2) and the mode task (cf. Section 5.3). For the casetasks, the left sub-plots report min, mean and max accuracies (from top to bottom) while the rightsub-plots report mean case accuracies (argmin, first and argmax from top to bottom). The modeplots report min, mean and max accuracies. The plots show the accuracies for N = 128.
Figure 50: Replica of Figure 29: Full results of the RL experiments. The plot shows a variationof learning rate (y-axis) vs. model dimension d (x-axis). The rows from top to bottom correspondto min, mean and max success rate when evaluating on 10,000 newly generated games. Crossesindicate the combination for best success rate, which we report with standard deviation behind themodel name. Note here that training is stopped early if a success rate of 99% is reached.
