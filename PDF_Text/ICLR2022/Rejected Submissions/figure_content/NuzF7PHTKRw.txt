Figure 1: Main structure of EAT-C: The path-planner recursivelygenerates a sub-task tree for a given task (g0 , g), while the envi-ronment generator (EG) adversarially modifies the environment ofeach sub-task. RL agent is trained on a bottom-up curriculum andits collected data are used to train the path-planner and EG.
Figure 2: (a) report the success rate (mean±std averaged over 6random seeds) of EAT-C and baselines on test tasks in 2D Pusherenvironments. (b) Ablation study of EAT-C on 2D Pusher tasks.
Figure 3: (a)-(c) illustrate the 2-3 key steps for completing eachtask. In Scavenging, the agent will have 2 points when it col-lects food each time. (d)-(f) report different methods’ performance(mean±std over 10 random seeds) on multiple test tasks.
Figure 4: Visualization of EAT-C. A 2D robot with a 4-joint arm starts from the initial state (pink), navigates tothe object (green) location, and then pushes the object to the goal state (black). The histograms in (a) and (b)represents the expected return of EG taking action bt, and the costs of sub-tasks predicted by the path planner inlayer k = 2, respectively.
Figure 6: (a)-(c) illustrate the 2-3 key steps for completing each task. In Scavenging, the agentwill have 2 points when it collects food each time. (d)-(f) report different methods’ performance(mean±std over 10 random seeds) on multiple test tasks.
Figure 7: Average time-cost of the RL agent to complete a sub-task from layer-3 of the sub-tasktree. As the training proceed, time-cost that the agent needs to complete each sub-task decreasessignificantly, indicating that πp does not propose infeasible goals, and EG does not make the rewardmore sparse.
Figure 8: 7DoF Robotic Arm in a training environment with randomly sampled obstacles (those cyancubes).
