Figure 1: Overview of the proposed algorithm. In order to reduce the additional communicationbetween the clients and the server, we propose to cluster similar client activations and send the clustercentroids to the server instead. This is equivalent to adding a vector quantization layer in split neuralnetwork training. The success of our method relies on a novel variant of product quantizer and agradient correction technique. We refer the reader to Section 4 for further details.
Figure 2: Illustration of our proposed quantizer. Given a mini-batch of activations Z ∈ Rd×B, thereare three steps: (i) divide each activation vector into q subvectors; (ii) stack subvectors into R groupsbased on their indices; (iii) perform K-means clustering to get L centroids for each group. Eachsubvector can be represented by the index of the closest centroid in its corresponding group. On theserver, by simply rearranging centroids, we can get the quantized activations Z.
Figure 3: Quantization error (lower is better) versus compression ratio (larger is better). Our proposedquantizer achieves a better quantization error vs. compression ratio trade-off as compared to vanillaproduct quantization (PQ) and K-means. In this example, the activation size is d = 9216, and themini-batch size is B = 20. The activations are trained via a two-layer CNN on Federated EMNIST. Ineach curve, we vary the number of clusters L. For green curves, the number of subvectors q takes valuein {288, 1152, 4608}; for red curves, the number of groups R takes value in {2304, 1152, 384, 1},and q is fixed as 4608.
Figure 4:	Trade-off between the accuracy and compression ratio. Our proposed method can achieveup to 490×, 247×, and 51× communication reduction with less than 5% accuracy drop on FEMNSIT,SO Tag, and SO NWP, respectively. Each curve in the figures corresponds to one value of q (numberof subvectors); and each point on a curve corresponds to a specific value of L (number of clusters).
Figure 5:	Ablation studies on FEMNIST. (a) and (b): Validation accuracy on FEMNIST when fixingλ and varying q and L. Setting a small positive value for λ improves accuracy for almost all (q, L)pairs; (c) Thanks to subvector grouping, our proposed quantizer can achieve an order of magnitudelarger compression ratio as compared to vanilla product quantization scheme.
Figure 6: Training curves for different algorithms on FEMNIST. Although split learning-basedapproaches (SplitFed and ours FedLite) communicate at every iteration, they communicate much lessthan FedAvg.
