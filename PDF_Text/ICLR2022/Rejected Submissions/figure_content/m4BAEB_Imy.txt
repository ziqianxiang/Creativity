Figure 1: Schemes of pruning described in Hoefler et al. (2021)3.2	Magnitude PruningIt is a simple and effective selection scheme where the magnitude of the weights is used as a metricto determine which weights to drop and which weights to keep. After sparsifying the network thisway, retraining is done to get high accuracies again.
Figure 2: Weight distribution before and after applying global magnitude pruning on custom VGGmodel for CIFAR-103Under review as a conference paper at ICLR 20223.3	Lottery Ticket HypothesisThe Lottery Ticket Hypothesis initially introduced in Frankle & Carbin (2018), and extended toquantized networks in Diffenderfer & Kailkhura (2021) theorizes that there exists optimal sub-networks in the initialization of an overparameterized network. The authors prove theoreticallyand validate practically this claim in their papers. They introduce a learnable parameter ”score”which decide the weights to retain and the ones to drop. Diffenderfer & Kailkhura (2021) claimsto reduce network sizes by 50% with negligible reduction or sometimes even increase in accuracycompared to the fully connected counterparts.
Figure 3: iPrune Masking procedure5	Experimental ResultsWe performed tests on MNIST (LeCun et al. (1998)) and CIFAR-10 (Krizhevsky & Hin-ton (2009)) datasets. For all the results given below, the network architecture used forMNIST was (1024D - 1024D - 10D) and for CIFAR-10 a modified version of VGG-Small, specifically: (128C3 - 128C3 - MP2 - 256C3 - 256C3 - MP2 - 512C3 -512C3 - MP2 - 1024D - 1024D - 10D)For MNIST, no preprocessing was performed. We skipped the last layer while pruning. For CIFAR-10 we applied ZCA preprocessing as was suggested in Courbariaux et al. (2015) for gains of around2-3% in accuracy. We applied weight-decay (5e-2) on the last layer (dense) of the network and usedthe square hinge loss as the loss function. This together behaves like an L2-SVM block. We use theAdam optimizer with initial learning rate of 3e-3 with no learning rate scheduler. While pruning,we skipped the first two convolution layers and the last dense layer.
Figure 4: Weight distribution before, after pruning and after retraining a custom VGG modelTable 1 gives the results after applying iPrune on MNIST. There is less than 2.3% difference betweenthe fully connected, full precision network and pruned BinaryConnect deterministic model with 7/8inputs. The memory gain is around 2200x and computation gain is around 70x.
