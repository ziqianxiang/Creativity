Figure 1: Humanoid agents trainedwith a linear reward shift drasticallygain asymptotic performance.
Figure 2: Illustrative figure for conservative exploitation, with a positive constant bias added to thereward function.
Figure 3: Illustrative figure for curiosity-driven exploration with a negative shifted reward4.3	(Curiosity-Driven) Optimistic ExplorationOn the other hand, if we shift the reward function to the negative side, it is equivalent to optimisticinitialization. Figure 3 (a-b) illustrate how adding a negative bias leads to curiosity-driven explo-ration: while adding a negative constant value b- on the reward function lead to negatively shiftedoptimal Q-value function Qb- (Figure 3 (a)), minimizing the difference between a Q-value approx-imator and the optimal Q-VaIUe will enable calculating an upper-bound estimation for Qb, as shownin Figure 3(b). With sufficiently large b- (so that b- larger than the maximal value of any s, a-pair),such an upper bound ofQb can be used to conduct curiosity-driven exploration. Intuitively, initializ-ing a value network that predicts value larger or equal than the true value will lead to curiosity-drivenexploration, as any visited state will be assigned a low value and the policy that learns to performthe action with a higher value will tend to choose novel actions.
Figure 4: Results on offline RL settings. We verify our key insight that a positive reward shift equalsto conservative exploitation thus helps offline value estimation, while a negative reward shift leadsto worse performance.
Figure 5: Results on continuous control tasks, the method of Random Reward Shift (RRS) outper-forms its value-based baselines in most environments.
Figure 6: Value-based RND with shifted prior: Plugging the vanilla RND into DQN is not well-motivated according to our analysis in Section 4.3.2. The insight of equivalence between negativereward shifting and curiosity-driven exploration motivates us to shift the vanilla RND with a con-stant, which drastically improves the performance of RND when working with DQN.
Figure 7: Performance with different reward shift constants.
Figure 8: Performance with different reward shift constants and different number of Q-networks.
Figure 9:	Examples of environments used in Section 5.3. The first figure shows the MountainCar-v0environment where a car needs to accumulate potential energy to reach the flag, to receive a positivereward. The second figure shows the maze of the Empty-Random task with size of 6, the third oneshows the MultiRoom of level S2-N4, where there are 2 rooms with size 4, the last figure showsexample of FourRoom task with size 17. In our experiments, as we use the vanilla DQN as thebaseline, which is not suitable for partial observable tasks, we use a smaller maze of size 7 and 9 toavoid further dependency on memories. In all tasks of the MiniGrid domain, the triangular red agentneed to navigate to the green goal square, and the observable region is only a 7x7 square the agentis facing to (i.e., the regions with shallower color in the last three figures).
Figure 10:	Performance with different reward shift constants in RND.
