Figure 1: Policy gradient vari-ance averaged over the trainingprocess. All bars show the meanand one standard deviation of 4seeds.
Figure 2: (a): Performance of different implementations of HC-decomposition framework (Q-HC),Q-RNN and vanilla SAC. The task is the continuous sum-form PI-DRMDP task and qn is a uniformdistribution between 15 and 20. The results show the mean and standard deviation of 7 seeds each.
Figure 3: Comparisons of Q-HC-Pairwise-1, Q-HC-RNN and baselines on 5 sum-form PI-DRMDPtasks. The dashed line is the value of Oracle SAC. qn is fixed as 20 (Zheng et al., 2018; Oh et al.,2018). The results show the mean and the standard deviation of 7 runs. All curves are furthersmoothed equally for clarity. The last picture shows the relative performance w.r.t the Oracle SAC onsum-form PI DRMDP tasks with general reward function (Appendix B). Each data point shows theaverage of 5 tasks of each algorithm. X-axis refers to the overlapping steps of the reward function.
Figure 4: Left: 100 × 100 Point-Reach task with additional positional reward (indicated by areacolor). Reward is given every 20 steps. Middle: Learning curves of Q-HC-Singleton and severalbaselines. Dashed line represents Q-HC-Singleton without regularization. Y-axis shows the numberof steps needed for the point agent to reach the target area (cut after 500 steps). All curves representthe mean of 10 seeds and are smoothed equally for clarity. Right: Heatmap visualization of bφ inQ-HC-Singleton.
Figure 5: Experiments of non sum-form PI-DRMDP tasks. The first line shows the result of Maxform and the second line shows the result of Square form. The learning curves are mean and onestandard deviation of 6-7 seeds.
Figure 6: Learning curves of TD3-based Q-HC-Singleton, Q-HC-Pairwise-1, Q-RNN and vanillaTD3. All curves show the mean and one standard deviation of 7 seeds. The result is consistent withthat of SAC-based methods.
Figure 7: More ablations on Lreg term. The task is a sum-form PI-DRMDP with n uniformly drawnfrom 15 to 20. Dashed lines are the no regularization version of the algorithm Q-HC with the samecolor. All curves show the mean and a standard deviation of 6-7 seeds.
Figure 8: Ablation of HC-decomposition in Q-HC-Singleton and Q-HC-Pairwise-1. The task isa sum-form PI-DRMDP with n fixed as 20. Dashed lines are two ablations Q-Singleton and Q-Pairwise-1. All curves show the mean and a standard deviation of 6 seeds.
Figure 9: Relative Average Performance of our algorithms and the baselines on PI-DRMDP-c taskswith general reward functions. Each dot represents the relative performance of the algorithm overthe PI-DRMDP-c task (average over Reach-v2, Hopper-v2, HalfCheetah-v2, Walker2d-v2, Ant-v2).
Figure 10: Learning curves of our algorithms Q-HC and several previous baselines. The first linerefers to the task with c = 0. The second line refers to the task with c = 5. The third line refers tothe task with c = 10. All curves show the mean and one standard deviation of 6-7 seeds.
Figure 11: Visualization of bφ(st, at) of Q-HC-Singleton without regulation term.
