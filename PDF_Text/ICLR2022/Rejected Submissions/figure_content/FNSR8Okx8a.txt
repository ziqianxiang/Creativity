Figure 1: Comparing L2 (black), PrioritizedL2 (red), and Full-PrioritizedL2 (blue) in terms of testingRMSE v.s. number of mini-batch updates. (a)(b) show the results trained on a large and small training set,respectively. (c) shows the result of a corresponding RL experiment on mountain car domain. We compareepisodic return v.s. environment time steps for ER (black), PrioritizedER (red), and Full-PrioritizedER(blue). Results are averaged over 50 random seeds on (a), (b) and 30 on (c). The shade indicates standard error.
Figure 2: (a) shows the GridWorld (Pan et al., 2019). It has S = [0, 1]2, A = {up, down, right, lef t}. Theagent starts from the left bottom and learn to reach the right top within as few steps as possible. (b) and (c)respectively show the state distributions with uniform and prioritized sampling methods from the ER buffer ofprioritized ER. (d) shows the SC queue state distribution of our Dyna-TD.
Figure 3: (a)(b) show the distance change as a function of environment time steps for Dyna-TD (black),PrioritizedER (forest green), and Dyna-TD-Long (orange), with different weighting schemes. The dashedline corresponds to our algorithm with an online learned model. The corresponding evaluation learning curve isin the Figure 4(c). (d) shows the policy evaluation performance as a function of running time (in seconds) withER(magenta). All results are averaged over 20 random seeds. The shade indicates standard error.
Figure 4: Episodic return v.s. environment time steps. We show evaluation learning curves of Dyna-TD(black), Dyna-Frequency (red), Dyna-Value (blue), PrioritizedER (forest green), and ER(magenta) withplanning updates n = 10, 30. The dashed line denotes Dyna-TD with an online learned model. All results areaveraged over 20 random seeds after smoothing over a window of size 30. The shade indicates standard error.
Figure 5: (a) (b) show episodicreturns v.s. environment time stepsof Dyna-TD (black) with an on-line learned model, and other com-petitors on Hopper and Walker2drespectively. Results are averagedover 5 random seeds after smooth-ing over a window of size 30. Theshade indicates standard error.
Figure 6: (a) shows the roundabout domain with S ⊂ R90 . (b) shows crashes v.s. total driving time steps duringpolicy evaluation. (c) shows the average speed per evaluation episode v.s. environment time steps. (d) shows theepisodic return v.s. trained environment time steps. We show Dyna-TD (black) with an online learned model,PrioritizedER (forest green), and ER (magenta). Results are averaged over 50 random seeds after smoothingover a window of size 30. The shade indicates standard error.
Figure 7:	The function f (x) = ln ɪ — ɪ, x > 0. The function reaches maximum at X = 1.
Figure 8:	(a) show cubic v.s. square function. (b) shows their absolute derivatives. (c) shows the hitting timeratio v.s. initial value x0 under different target value xt . (d) shows the ratio v.s. the target xt to reach underdifferent x0 . Note that a ratio larger than 1 indicates a longer time to reach the given xt for the square loss.
Figure 9:	Figure(a)(b) show the testing RMSE as a function of number of mini-batch updates withincreasing noise standard deviation σ added to the training targets. We compare the performancesof Power4(magenta), L2 (black), Cubic (forest green). The results are averaged over 50 randomseeds. The shade indicates standard error. Note that the testing set is not noise-contaminated.
Figure 10: Figure (a)(b) show the training RMSE as a function of number of mini-batch updateswith a training set containing 4k examples and another containing 400 examples respectively. Wecompare the performances of Full-PrioritizedL2 (blue), L2 (black), and PrioritizedL2 (red). Theresults are averaged over 50 random seeds. The shade indicates standard error.
Figure 11: Figure(a)(b) show the training RMSE as a function of number of mini-batch updates withincreasing mini-batch size b. Figure (c)(d) show the testing RMSE. We compare the performancesof Full-PrioritizedL2 (blue), Cubic (forest green). As we increase the mini-batch size, the twoperforms more similar to each other. The results are averaged over 50 random seeds. The shadeindicates standard error.
Figure 12: Figure(a) shows MazeGridWorld(GW) taken from Pan et al. (2020) and the learningcurves are in (b). We show evaluation learning curves of Dyna-TD (black), Dyna-Frequency (red),and Dyna-Value (blue). The dashed line indicates Dyna-TD trained with an online learned model.
