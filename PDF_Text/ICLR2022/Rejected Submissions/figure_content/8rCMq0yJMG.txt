Figure 1: Illustration of source-target unified knowledge distillation.
Figure 2: The pipeline of lite residual hypothesis transfer.
Figure 3:	The peak memoryfootprint of STU-KD and thecomparison methods.
Figure 4:	The inference accu-racy curves of STU-KD andits variants on W → A task.
Figure 5: The communicationcost of STU-KD and its vari-ants on W → A task.
Figure 6:	SourceAs the convergence criterion is reached. Co-KD stops. The compact model with w0 (t) is thenbroadcast to the M edge devices as their new compact models.
Figure 7:	Visualization results of (a) ResNet-18, (b) SHOT, (c) TO-KD, and (d) STU-KD on A →D task.
