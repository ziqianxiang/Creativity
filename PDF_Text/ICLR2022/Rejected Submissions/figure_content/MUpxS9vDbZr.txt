Figure 1: A basic MDP. If (s0 , a0) is contained in thedataset but (s1, a1) is not, by carefully selecting thevalues Q(s0, a0) and Q(s1, a1), we can construct ex-amples where the Bellman error of the dataset is 0 butthe value error is arbitrarily large.
Figure 2: Comparing the Bellman error (top row) with value error (bottom row) of value functions trained witha dataset of 1m on-policy transitions. Error terms are evaluated over a held-out test set of on-policy rollouts.
Figure 3: The final Bellman error and value error of functions trained with datasets gathered by increasinglynoisy versions of the target policy. 0.0 is an on-policy dataset and the remainder are off-policy. Error barscapture the standard deviation over 10 seeds. Bellman error was clipped to 10k on the HalfCheetah task forFQE for visual clarity, as the value estimate diverged for 0.1 and 0.2 noise levels (see the learning curves inAppendix C.2). ■ FQE consistently outperforms ■ BRM while having significantly higher Bellman error.
Figure 4: Visualizing the final value estimated by BRM after training on different datasets corresponding tovarying noise levels. The true value of the target policy and the behavior policy are displayed to providereference, as well as BRM when trained to evaluate a suboptimal policy (corresponding to TD3 trained for300k time steps rather than 3m). Error bars capture the standard deviation over 10 seeds (but are visually hardto see as the deviation is low). We can see that ∙ BRM typically converges to a value which is closer to the 上behavior policy rather than the ▼ target policy, and typically prefers values which are close to 0. Interestingly,the BRM trained to evaluate the suboptimal target policy converges to the same value on the noisiest datasets,suggesting that the influence of the target policy on what value BRM converges to is reduced with increaseddistribution shift.
Figure 5:	Visualizing the learning curves of the Bellman error and value error of value functions trained byBRM. The shaded area captures the standard deviation over 10 seeds. We can observe that Bellman error eval-uated with the on-policy dataset is roughly representative of the value error, while the Bellman error evaluatedwith their training datasets is not.
Figure 6:	Visualizing the learning curves of the Bellman error and value error of value functions trained byFQE. The shaded area captures the standard deviation over 10 seeds. Bellman error of individual trials isclipped to 10k for visual clarity.
Figure 7: Comparing the absolute Bellman error with the absolute value error. The shaded area captures thestandard deviation over 10 seeds. Both algorithms are trained using on-policy data collected by the targetpolicy.
Figure 8: Comparing the mean squared Bellman error with the mean squared value error. The shaded areacaptures the standard deviation over 10 seeds. Both algorithms are trained using on-policy data collected bythe target policy.
Figure 9: Comparing the mean squared Bellman error with the absolute value error, using a dataset of 50k,rather than 1m as in Figure 2. The shaded area captures the standard deviation over 10 seeds. Both algorithmsare trained using on-policy data collected by the target policy.
Figure 10: Comparing the Bellman error with value error. The shaded area captures the standard deviation over10 seeds. Both algorithms are trained using on-policy data collected by the target policy. Figure 2 shows theBellman error and value error, evaluated on a held-out test set (repeated in Figure 10b). In this figure we alsodisplay the error terms over the training set (Figure 10a). We see near identical figures. This shows that thedataset is sufficiently large that overfitting to individual transitions does not occur.
Figure 11: Comparing the mean squared Bellman error with the absolute value error using a dataset of 1m buttrained for 10m time steps total, rather than 1m as in Figure 2. The shaded area captures the standard deviationover 10 seeds. Both algorithms are trained using on-policy data collected by the target policy.
