Figure 1: Overview of our approach. The method consists of a minimax game theoretic objective. It firstapplies a clustering approach to generate n group of clusters based on the different visual features in the ob-servation. A generator is then used to style-translate observation from one cluster to another while maximizingthe change in action probabilities of the policy. In contrast, the policy network updates its policy parametersto minimize the translation effect while optimizing for the RL objective that maximizes the expected futurereward.
Figure 2: Overview of style transfer generator module. The experience trajectory observation data for thetask environment are separated into different classes based on visual features using Gaussian Mixture Model(GMM) clustering algorithm. The task of the generator is then to translate the image from one class image toanother classes images. In this case, the “style” is defined as the commonality among images in a single class.
Figure 5: Generalization results. Test performance comparison on Procgen environments with image observa-tion. ARPO performs better in many environments where it outperforms the baselines. The results are averagedover 3 seeds.
Figure 6: Sample efficiency (Train) and generalization (Test) results on Distracting control Walker walk envi-ronments. The results are averaged over 3 seeds.
Figure 7: Sample efficiency results. Train performance (learning curve) comparison on Procgen environ-ments. Despite using perturbation on the observation, our agent ARPO outperforms the baselines in manyenvironments, thus achieving better sample efficiency during training. The results are averaged over 3 seeds.
Figure 8:	Probabilistic measure (Agarwal et al., 2021b) of how likely our agent ARPO is to improve over thebaseline PPO and RAD. We observe that ARPO is up to 54% likely to improve over PPO and up to 72% likelyto improve over baseline RAD in the Procgen generalization setup.
Figure 9: Distracting control Cheetah run environment results. The results are averaged over 3 seeds. (a, c)Sample efficiency (train), and (b, d) Generalization (test).
Figure 10: Distracting control Walker walk environment results (including SAC). The results are averagedover 3 seeds. (a, c) Sample efficiency (train) (b, d) Generalization (test).
Figure 11: Distracting control Cheetah run environment results (including SAC). The results are averagedover 3 seeds. (a, c) Sample efficiency (train), and (b, d) Generalization (test).
Figure 12: Ablation results. The results are averaged over 3 seeds. (a) Our ARPO agent’s results on differentcluster numbers. It improves generalization (test) performance with the increase in cluster number in theClimber environment. (b) Our ARPO agent’s results on different β values which determine participation onadversarial optimization.
Figure 13: Sample translation of generator on background distraction for Walker-walk environment fromDistracted Control Suite benchmark. We see that the translated observations retain the semantic that is therobot pose while changing non-essential parts. Interestingly, we observe that the adversarial generator blackoutsome non-essential parts of the translated images. This scenario might indicate that the min-max objectivebetween policy network and generator tries to recover the actual state from the observation by removing partsirrelevant to the reward.
Figure 14: Sample translation of generator on color distraction for Walker-walk environment from DistractedControl Suite benchmark.
Figure 15: Sample translation of generator on four Procgen environments. We see that the generator retainsmost of the game semantic while changing the background color and the texture of various essential objects.
Figure 16: Sample translation of generator during various training phases on background distraction forWalker-walk environment from Distracted Control Suite benchmark. [Second row] We see that when translatedback to the same cluster (in the second row), the generated images get some distortion (e.g., blackout someparts), while the essential parts remain mostly intact. This scenario might be happening due to the regularizationof the cycle consistency loss by the KL component in equation 5. [Third row] We see that the translation to adifferent cluster generates images that changes the irrelevant parts while mostly keeping the semantic intact. Inboth the second and third row, the translation gets better as time progresses, suggesting the adaptation of bothpolicy and generator networks due to the min-max objective.
Figure 17: KL regularization during RL training for four Procgen environments. At the beginning of thetraining, both the policy and generator behave randomly; thus, the KL values become nearly zero. Gradually, theKL started to increase as the policy learning progressed with the adversarial generated observation. Eventually,the policy tries to adjust to the changes in the observation, and the KL values become stable (bigfish) or goingdownward (jumper, and climber). We see that in fruitbot environment, the values started to increase at theend, which suggests a possible divergence. This scenario might be an explanation of why the generalizationperformance started to drop at the end in Figure 5, possible overfitting. Thus this KL measure might be a tool todetect possible divergence and overfitting. However, each environment’s stability cutoff time (policy iteration)is different, suggesting that the KL regularization behavior might be different depending on the environmentcomplexity. Ideally, with enough training, the KL values should converge to zero (again) if the policy reachesoptimal (for true MDP state) and the generator fully recovers the true state from the observation and changesall the irrelevant information from the observation. Note that We limit the policy to train until a cutoff time (22Million RL timesteps).
