Figure 1: Algorithm flowcharts for DQN (left) and DQN with our proposed Concurrent Training technique(right). By computing the agent’s policy from the target network parameters θ- instead of the main networkparameters θ, our method breaks the sequential dependency between training and environment samplingthat is inherent to the standard DQN algorithm. This allows training and sampling to occur in parallel viamultithreading, greatly reducing the overall runtime.
Figure 2: Abstract timing diagrams illustrating the theoretical speedup of our optimizations in DQN. Byoverlapping sampling and training, the training cost can be effectively masked (concurrency). To mitigatethe remaining sampling bottleneck, multiple environments can be simulated in parallel (synchronization).
Figure 3: Attempting to run asynchronous environments for deep reinforcement learning leads to significantresource competition between threads (left), introducing a performance bottleneck. This can be alleviatedby first synchronizing the threads and then sharing computation in a single minibatch (right).
