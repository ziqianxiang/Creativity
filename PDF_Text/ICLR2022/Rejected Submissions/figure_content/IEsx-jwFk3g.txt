Figure 1: The proposed ReBraiD model for integrating brain structure and dynamics (the architec-ture shown is for classification). For each batch with batch size B, input X has a dimension of(B, 1, N, T)1, and A, Aadp both have the dimension (B, N, N). The encoder (green part) encodestemporal and spatial information alternatively, producing a latent representation in (B, dlatent, N, 1).
Figure 2: Comparison of strided non-causal TCN (left) and dilated causal TCN (right). The causalaspect is achieved through padding (kerneLsize — 1) X dilation zeros to the layer's input. Theresulting y always has the same length as input x, in which yτ only depends on inputs xt≤τ. Wecan view strided non-causal TCN as the rightmost node of a dilated causal TCN.
Figure 3: 3a: Ablation studies on different input length (please see table 3 in appendix for numericalvalues of weighted F1 under each setting); 3b: Choosing number of GNN to TCN layer ratio.
Figure 4: 4a: Temporal importance sanity check of IG results on two pieces of inputs with a largeoverlap period. Attribution maps are offset aligned; 4b: AttrX distributions across brain regions forVWM task, where the upper one separates left and right hemispheres and the lower one combinesthem (e.g. LH-VisCent and RH-VisCent are combined to region VisCent). Refer to table 4 inappendix to see brain region names that the x-axis numbers represent.
Figure 5: Column averages of task-averaged AttrA (mapped into 34 subnetworks defined by the17-network parcellation with left, right hemispheres). Top row is obtained from real SC induced Aand bottom rows is obtained from random SC induced Arand. Attributions are normalized to [0, 1].
Figure 6: ROI attributions from ATTRA and ATTRX . Tasks are: Resting, VWM, DYN, DOT, MOD,PVT from left to right. Edge color and width are based on task-averaged ATTRA ∈ R200×200, andnodes color and size are based on task and temporal-averaged ATTRX ∈ R200. For the visualizationpurpose, only edges with highest attributions are kept to ensure sparsity being 0.009 (down fromaround 0.196). For ROI attributions based only on AttrA where important sender ROIs are reflectedby node sizes, please refer to fig. 13 in appendix.
Figure 7: AttrX distributions of 3 subjects doing VWM task (left) and MOD task (right). Outliersthat go beyond [Q1 - 1.5 IQR, Q3 + 1.5 IQR] are omitted. VWM has a much smaller averageattribution variance than MOD.
Figure 8: Dynamic functional connectivities.
Figure 9: Inner-cluster smoothing toy example.
Figure 10: Learned latent adaptive adjacency matrices Aadp.
Figure 11: 11a: adding inner cluster smoothing or input-dependent adaptive adjacency matrix makesthe model more stable across various learning rates. 11b: Validation loss v.s. training epochs. Inputlength is 256 and four smoothing modules are used. Legends are the soft-assignment cluster numbersof the four smoothing modules. Our other experiments are using decreasing cluster numbers thathalved each module, corresponding to the 100-50-25-12 here.
Figure 12: Confusion matrices of: (12a) ReBraiD (our proposed model), (12b) model with coarsenedgraph (setting (viii)), (12c) Graph Transformer (best-performing baseline). Tasks are 1-Resting, 2-VWM, 3-DYN, 4-DOT, 5-MOD, 6-PVT.
Figure 13: Important ROIs based on AttrA . Tasks are: Resting, VWM, DYN, DOT, MOD, PVTfrom left to right. Node sizes are based on column sums of AttrA ∈ R200×200 and edge width aredirecly based on AttrA . For the visualization purpose, only edges with highest attributions are keptto ensure sparsity being 0.009 (down from around 0.196).
Figure 14: Task-averaged ATTRA ∈ R200×200 . The top row is obtained from real SC induced A,and the bottom row is obtained from random SC induced Arand .
Figure 15: Network parcellation of Yeo’s 17-networks. Figure is from Kahali et al. (2021)Table 5: AttrA: Top 10 brain subnetworks initiate important connections during different tasks(fig. 13 provides a ROI-based visualization).
Figure 16: Brain subnetwork attribution distributions from AttrX .
Figure 17: Reproducibility validation: attributions obtained from two models trained on differentdata split and initialized randomly. Attribution values are normalized to [0, 1] and the larger theattribution value, the stronger indicating power that ROI has for a certain task. Tasks are Resting,VWM, DYN, DOT, MOD, and PVT.
Figure 18: 18a: A typical adjacency matrix for simulated graph signals. 18b: Task averaged AttrXof simulation (I). Attribution values are normalized. 18c: Task averaged Aadp of simulation (I) andits entry averages per column. 18d: Task averaged Aadp of simulation (II). 18e: Task averaged AttrAof simulation (II). Attribution values are normalized.
