Figure 1: PSEUDσ illustration. In every episode k, PSEUDσ assigns pseudo-labels along with theirevidential uncertainty using trained neural network f (k-1) from previous episode. The uncertainty isused as weight to adaptively adjust the loss in this episode’s neural network f(k) ’s training to reducethe effect of bad pseudo-labels in an inner-loop training with N epochs.
Figure 2: (a) Dependence of the entropy (Eq. 9) on epistemic uncertainty and virtual observationparameter α for a fixed aleatoric uncertainty E[σ2] = 1. As the the epistemic uncertainty increases,the entropy is also increases for all values of parameter α. For example, figure (b) demonstrate thetrend for a fixed α = 2. Figure (c) demonstrates the dependence of empirical weights (Eq. 14) onepistemic uncertainty. The empirical weights tend to decrease as the epistemic uncertainty increases.
Figure 3: Uncertainty highlycorrelates to label quality.
