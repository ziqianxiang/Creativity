Figure 1: The underlying concept of DT-GAN is to transfer and generate foreground contents witha variety of artistic styles (e.g., light / heavy strokes) across different background domains.
Figure 2: Overview of all modules in DT-GAN.
Figure 4: Qualitative comparison of reference-guided image synthesis results on the SDIdataset. Each method transforms the given source images into target foreground domains (e.g.,Scratches) with the styles and contents extracted from the reference images.
Figure 5: Ablation study. (a) The baseline StarGAN v2. (b) + Style-Content branches. (c) + Fore-ground classifier. (d) + Background classifier. (e) + Separately decoding foreground and backgroundin G. (f) + Anchor foreground domain (e.g. Normal). (g) + Noise injection in Mapping Network.
Figure 6: Visual effect of randomly sampled style codes on fixed pairs of reference background(Source) and foreground (Content) images.
Figure 7: Overview of the SDI dataset.
Figure 9: EfficientNet-b4 with GRL.
Figure 8: ResNet-50 with GRL.
Figure 10: Reference-guided image synthesis results on the SDI dataset. The first row and the firstcolumn are the real images sampled from the dataset, while the rest are synthetic images generatedby the proposed DT-GAN. Our model provides translations between different foreground domains(Normal, Scratches and Spots) with styles and contents extracted from reference imageswhile the backgrounds from source images are well preserved.
Figure 11: Latent-guided image synthesis results of StyleGAN v2 on the SDI dataset and the MVTecAD dataset. We train a model for each product and generate 16 Scratches images from randomlysampled latent codes.
Figure 12: Latent-guided image synthesis results of StyleGAN v2 on the SDI dataset and the MVTecAD dataset. We train a model for each product and generate 16 Spots images from randomlysampled latent codes.
Figure 13: Latent-guided image synthesis results of BigGAN with DiffAug on the SDI dataset andthe MVTec AD dataset. We train a model for each product and generate 16 Scratches imagesfrom randomly sampled latent codes.
Figure 14: Latent-guided image synthesis results of BigGAN with DiffAug on the SDI dataset andthe MVTec AD dataset. We train a model for each product and generate 16 Spots images fromrandomly sampled latent codes.
Figure 15: Latent-guided image synthesis results of StarGAN v2 on the SDI dataset and the MVTecAD dataset. The model is trained on a joint set of aforementioned datasets and performs translationfrom Normal to Scratches. Note that without the style-content separation and the FG/BGdisentanglement, StarGAN v2 not only fails to preserve the background from the given Sourceimage but also fail to generates legit defects.
Figure 16: Latent-guided image synthesis results of StarGAN v2 on the SDI dataset and the MVTecAD dataset. The model is trained on a joint set of aforementioned datasets and performs translationfrom Normal to Spots. Note that without the style-content separation and the FG/BG disentan-glement, StarGAN v2 not only fails to preserve the background from the given Source image butalso fail to generates legit defects.
Figure 17: Latent-guided image synthesis results of DT-GAN on the SDI dataset and the MVTecAD dataset. The model is trained on a joint set of aforementioned datasets and performs translationfrom Normal to Scratches. Note the our model takes input Source images as background andonly synthesizes the foreground defects from randomly sampled latent code compared to StyleGANv2 and BigGAN.
Figure 18: Latent-guided image synthesis results of DT-GAN on the SDI dataset and the MVTecAD dataset. The model is trained on a joint set of aforementioned datasets and performs translationfrom Normal to Spots. Note the our model takes input Source images as background and onlysynthesizes the foreground defects from randomly sampled latent code compared to StyleGAN v2and BigGAN.
Figure 19: Reference-guided image synthesis results of Mokady et al. (2020) on the SDI dataset andthe MVTec AD dataset. We train a model for each product and each defect type. Then we translateNormal images to Scratches by taking the Source as background and applying the foregrounddefect from Reference to it.
Figure 21: Reference-guided image synthesis results StarGAN v2 on the SDI dataset and the MVTecAD dataset. The model is trained on a joint set of aforementioned datasets and performs translationfrom Normal to Scratches by taking the Source as background and applying the foregrounddefect from Reference to it. Note that without the style-content separation and the FG/BG disen-tanglement, StarGAN v2 not only fails to preserve the background from the given Source image butalso fail to generates legit defects.
Figure 22: Reference-guided image synthesis results of StarGAN v2 on the SDI dataset and theMVTec AD dataset. The model is trained on a joint set of aforementioned datasets and performstranslation from Normal to Spots by taking the Source as background and applying the fore-ground defect from Reference to it. Note that without the style-content separation and the FG/BGdisentanglement, StarGAN v2 not only fails to preserve the background from the given Sourceimage but also fail to generates legit defects.
Figure 23: Reference-guided image synthesis results DT-GAN on the SDI dataset and the MVTecAD dataset. The model is trained on a joint set of aforementioned datasets and performs translationfrom Normal to Scratches by taking the Source as background and applying the foregrounddefect from Reference to it.
