Figure 1: Two types of risk in MARL: (a) agent-wise risk and (b) environment-wise risk. Note thatfriendly agents learn a policy over time, while enemies in the environment act through a stationarydistribution. Current value-based CTDE methods do not consider risks explicitly or tackle them in anentangled way which may lead to a suboptimal solution.
Figure 2: The median test win rate for DRIMAand eight value-based CTDE baselines, averagedacross all 12 scenarios in our experiments. Re-sults for each scenario are reported in Figure 4.
Figure 3: Architecture of DRIMA. Each agent network and true action-value estimator are built basedon IQN and input a quantile of agent-wise risk wagt and environment-wise risk wenv , respectively.
Figure 4: Median test win rate with 25%-75% percentile over four random seeds, comparing DRIMAwith eight baselines.
Figure 5: Agent-wise utility function. φ : [0, 1] → Rd denotes a quantile embedding function(Dabney et al., 2018a){Z'(τ" % 仅agt)}MSellV ―►-→ Zjt(s,T,U, Wθnv)Figure 6: True action-value estimator. φ : [0,1] → Rd denotes a quantile embedding function(Dabney et al., 2018a).
Figure 6: True action-value estimator. φ : [0,1] → Rd denotes a quantile embedding function(Dabney et al., 2018a).
Figure 7:	Transformed action-value estimator. | ∙ | is employed to enforce monotonicity constraint(Rashid et al., 2018).
Figure 8:	Ablation studies on varying risk-levels (agent-wise risk Figure 9: Comparison of DRIMAwagt, environment-wise risk wenv) of DRIMA in MMM2-v1, oneof SUPER HARD tasks.
Figure 10:	Robustness of DRIMA in relatively easy tasks. Differing risk-levels (agent-wise risk wagtenvironment-wise risk wenv) in DRIMA do not degrade below the second-best baseline.
Figure 11:	Median test win rate with 25%-75% percentile over four random seeds, comparing DRIMAwith five baselines.
