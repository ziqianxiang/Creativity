Figure 1: Task and candidate networks. a) Task. Left: learning an extendable library of motifswithout interference. Right: without additional training, stringing the motifs into chains with arbi-trary orders. b) Additive and multiplicative architectures, who may succeed in the task in a becausethey segregate parameters into motif-specific sets (schematized in colors) while benefiting from fixedshared recurrent and readout weights (schematized in black). c) Minimum root mean square errorover training with overcomplete training set, depending on the gain hyperparameters gad, gmu, andgoptb. Dots are individual networks, the line is the average. In red, we show the mean minimum rootmean square error in the control architecture (averaged over five individually trained networks foreach g0cn). d) Examples easy oscillatory motif and hard step motif; fitting the latter requires a largernumber of basis functions (complex exponentials) of varied oscillation frequencies. e) ‘Classical’strategy to promote zero-shot transfer in order to produce a chain of motifs: train ANNs to producethe motifs starting from random initial network states xinit that emulate the variability of networkstates at the end of motifs Xend (where μ indexes the motif). f) Increased inaccuracy during zero-shot transfer to chains of motifs compared to when starting from random xinit drawn from the samedistribution as during training (additive architecture). g) As in f but for oscillatory motifs. Note thatthe target trajectory (black) is buried below the output of the network (colored lines), because thelatter is almost perfectly accurate with random Xinit.
Figure 2:	Motif sequencing by RNNs trained on individual motifs from random initializations.
Figure 3:	Improvisation of motif sequences by RNNs with thalamocortical insights.a) Traininga preparatory module consisting of P loops weights forming a perturbation UprepVp|rep of the con-nectivity, for a strongly nonlinear network. b) Training the motif-specific parameters in the additive,multiplicative and two-input architectures. c) Average root mean square error over motifs duringtraining conditions vs. during transitioning, in the additive and multiplicative architectures; and withor without preparatory module. Ten step motifs are used. d) Example network performance wheninitializing the network state with the same random distribution as during training, as opposed towhen transitioning from another motif. e) Same as c but comparing additive and two-inputs archi-tectures using more motif transitions. Stars indicate a significant error increase (signed rank test).
Figure 4:	Oscillatory motifs used in this paper.
Figure 5:	Step motifs used in this paper.
Figure 6: Performance of an additive network, N=300, step motifs. Left: the network outputs foreach motif when starting from 9 different random x values. The saturations (light to dark) indicatedifferent trials. Right: network outputs for each motif when starting from the x values taken at theend of the other 9 motifs. Colors indicate the identity of the prior motif (as labeled in the left panel).
Figure 7: Performance of a multiplicative network, N=100, step motifs. Conventions as inFigure 6.
Figure 8: Performance of an additive network, N=1000, step motifs. Conventions as in Figure6.
Figure 9:	Performance of an additive network, N=300, oscillatory motifs. Conventions as inFigure 6.
Figure 10:	Performance of a control network, N=50, step motifs. Conventions as in Figure 6.
Figure 11:	Robust transitions in thalamocortical model. a) Adjusting a motif-specific loopthrough the thalamic unit t2 (i.e. motor preparation, left), leading to the control of both eigenvalues(middle) and eigenvectors of the dynamics x(t) such that the readout robustly follows motif 2 (right).
Figure 12: Using a thalamic transition module rescues transitioning for nonlinear RNNstrained with SGD. a,b,c Additive model. a. EigensPeCtrum of gadJ + UPreP VpreP after trainingof UPreP and VPreP (orange crosses) and when replacing J with a random matrix not used duringtraining (blue dots). Black circle has radius gad. b. |r| versus time before (dotted lines) and after(solid line) training of UPreP and Vprep. c. The grey bars indicate the time during which the transitionmodule was active. Other conventions as in Figure 6. d,e,f. As in a,b,c for the multiplicative model.
Figure 13:	Using a thalamic transition module rescues transitioning between oscillatory motifsin an additive ANN. Conventions as in Figure 6.
Figure 14:	Comparing a ‘two-inputs’ vs. additive network. Left half: ‘two-inputs’ network; righthalf: additive network, adjusted for the number of parameters tuned per motif. Top half: comparingrandom initialization as experienced during training, and transitioning from 70% of the duration ofother motifs. As a reminder, in the panels ’when starting from other motifs’, colors indicate theidentity of the prior motif (as labeled in the panel ’when starting from random’ - where the motifsin this panel are shown in full, even though here during transitioning they were interrupted at 70%of the duration shown). Notice that, during sequencing, the networks smoothly interpolate betweenthe various output values at the moment when the first motif is interrupted, and the readout valueat the start of the next motif, without having to make a detour through a fixed readout value (aswould occur if the networks were reset to a fixed - i.e., non motif-specific - state before each motif).
Figure 15:	Representative example learning curves for the additive and multiplicative net-works (without a preparatory module). These are chosen examples from the networks shown inthe main text Fig.1c. In this case, because we were trying to compare the small control network(which was harder to train) with the additive and multiplicative networks, we used the non-defaultADAM parameters described in the previous section with a small learning rate (learning rate =10-4) Note that all transitions are included in the training set here.
Figure 16:	Adding noise in the initial conditions during training improves noise robustness.
Figure 17:	Using a preparatory module along with a random initialization during trainingspeeds up learning. Evolution of the error (the mean square error - our objective function - inblue, and its square root in orange) over training epochs (each of these consists of several trainingbatches), when training networks on single step motifs. Standard ADAM parameters with a learningrate of 10-3 are used, and a small Gaussian iid noise (std = 0.001) is added to the dynamics ateach timepoint. a) Additive networks, without a preparatory module with either noiseless initializa-tion (left) of standard Gaussian initialization (middle); or with a preparatory module and standardGaussian iid initialization of motifs (right). b) Multiplicative networks, with standard Gaussian ini-tialization, either without (left) or with (right) a preparatory module. The preparatory module speedsup learning in both additive and multiplicative networks.
