Figure 1: We devise a method that learns tasks usingonly goal text from user and images from the envi-ronment at training time. We train several agents onseveral tasks and distill their policies into a multi-taskpolicy that is conditioned on goal text and current ob-servation. The distilled policy is able to generalize tonew unseen tasks at test time. We assume no accessto environment reward, state, demonstrations, or goalimages.
Figure 2: We compute reward of an image observation conditioned on goal language description, with noaccess to state, demonstrations, or goal images. We first parse the goal description into object noun phrasesand desired object spatial relationships. We then leverage CLIP’s image and language encoders EI and EL tolocate the object noun phrases in the image observation. Using the object states and the desired object spatialrelationships, reward is generated to train the agent for the desired task in its environment.
Figure 3: a) Mask R-CNN object detection results b) Mask R-CNN detection results on far view c) MaskR-CNN instance segmentation results on far view d) Grad-CAM result with ’red block’ text input (white beinghighest intensity) e) Grad-CAM result with ’yellow block’ text input (white being highest intensity)which is towards the front in the first camera view and y2 pointing upward similar to the first cameraview. The second camera is needed to know if the object is placed in the front or behind anotherobject correctly. The third set (close to & inside of) require access to a front camera view with a 45degree downward tilt towards the ground. This camera view is needed to see if the object is gettingcloser to another object in two orthogonal directions at once where as a left only or front view onlyallows you to determine one dimension of closeness. For “inside of” a 45 degree camera helps theagent see if the object is going inside another object without occlusion. The threshold for “inside of”is much smaller than for “close to” since the centroid can come much closer when an object goesinside a container object.
Figure 4: We showcase the BaseReward Model in the Double In-verted Pendulum doing slightlybetter than oracle reward. We av-erage results over 3 seeds per task.
Figure 5: We visualize the results of the base reward model which is a trivial dot product between the goallanguage description and image observation. The top row (green box) displays a successful utilization of thebase reward model and the bottom row (red box) shows a failure case. The x-axis represents image index.
Figure 6: We showcase our Full Reward Model performing almost as well as oracle reward on SawyerSimRobot-Pushing (pushing a blue puck close to a yellow puck), FetchSimRobot-Stacking (stacking a yellow block on topof a red block), and FetchSimRobot-Placing (placing a yellow block on the right of a red block). We averageresults over 3 seeds per task.
