Figure 1: Robust Deep Reinforcement Learning via MUlti-VieW InfOmratiOn Bottleneck (DRIBO)incorporates the inherent temporal structure of RL and unsupervised multi-view settings into robustrepresentation learning in RL. We consider sequential multi-view observations, o?T and o12T, oforiginal observations 01：T sharing the same task-relevant information while any information notshared by them are task-irrelevant. For example, natural video backgrounds of sequential observationsare task-irrelevant and can be drastically different between training and testing environments. DRIBOuses a multi-view information bottleneck loss to ensure that s[：T and s12T, the representations ofmulti-view observations, shares maximal task-relevant information while reducing the task-irrelevantinformation. DRIBO trains the RL policy and (or) value function on top of the encoder.
Figure 2: Left: DMC observationswithout visual distractors. Middle: ob-servations with natural videos as back-grounds. Right: spatial attention mapsof encoders for the middle images.
Figure 3: Results for DMC over 5 seeds with one standard error shaded in the natural video setting.
Figure 4: t-SNE of latent spaces learned with DRIBO (left t-SNE) and CURL (right t-SNE). Wecolor-code the embedded points with reward values (higher value yellow, lower value green). Eachpair of solid lines indicates the corresponding embedded points for observations with an identicalforeground but different backgrounds. DRIBO learns representations that are neighboring in theembedding space with similar reward values. This property holds even if the backgrounds aredrastically different (see middle images). By contrast, CURL maps the same image pairs to points faraway from each other in the embedding space.
Figure 5: DRIBO achieves better performance by capturing the temporal structure of RL from longertraining sequences. Compressing away task-irrelevant information using the SKL term in DRIBOloss improves performance when the architecture choice and training configurations are the same. Weperform 5 runs for each method under the natural video setting. More results are in Appendix C.2.
Figure 6: Average returns on DMC tasks over 5 seeds with mean and one standard error shaded inthe clean setting.
Figure 7: Average returns on DMC tasks over 5 seeds with mean and one standard error shaded inthe natural video setting.
Figure 8: Average SKL values during training in DMC environments with natural videos as back-ground.
Figure 9: Average returns of DRIBO and DRIBO-no-skl on DMC tasks over 5 seeds with mean andone standard error shaded in the clean setting.
Figure 10: Average SKL values during training in DMC environments in the clean setting.
Figure 11: Training and testing performance of RAD and DRIBO.
Figure 12: Training and testing performance of CURL and DRIBO.
Figure 13: Training and testing performance of PI-SAC and DRIBO.
Figure 14: Training environments of DMC under the natural video setting: The backgroundvideos are sampled from arranging flower class in Kinetics dataset.
Figure 15: Test environments of DMC under the natural video setting: We show the sequentialobservations with natural videos as background in DMC and their corresponding spatial attentionmaps of the DRiBo trained encoder.
Figure 16: Spatial attention maps for each convolutional layers of the DRIBO trained encoders. Theobservations are the same as ones in Figure 2 from the snapshots during testing.
Figure 17: t-SNE of latent spaces learned with DRIBO (left) and CURL (right). They are the same ast-SNE in Figure 4. But we color-code the embedded points corresponding to their backgrounds. Theobservations are from the same trajectory but with different background natural videos (the same asin Figure Figure 4). Points from different backgrounds are close to each other in the embedding spacelearned by DRIBO, whereas no such structure is seen in the embedding space learned by CURL.
Figure 18: Screenshots of the three Procgen games where our approach DRIBO does not improvegeneralization performance compared to the other methods. From left to right, they are Plunder,Chaser and DodgeBall.
