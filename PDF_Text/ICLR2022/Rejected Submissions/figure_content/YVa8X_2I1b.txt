Figure 1: Model Overview: We propose INFERNO, a model that infers and renders object-centric3D scene representations. 1 Our model first decomposes an input observation into multiple objectslots. 2 For each slot we infer a structured 3D representation. 3 The shape and appearance determinecanonical objects rendered through NeRFs. 4 Objects are transformed and located in the overallscene according to their pose. 5 We combine objects and background and render a low-resolutionscene given a camera location. 6 The input is reconstructed by upscaling the low resolution scene.
Figure 2: Manipulations on CLEVR: we show some examples of the manipulations we performto CLEVR2345 images, including object removal and addition, changing the scale of an object, andobject translation. Our model can perform these transformations because it disentangles object poseand appearance.
Figure 3: Object Discovery on CLEVR6: INFERNO identifies the different objects in a scenewithout supervision. For each input image, we show which regions of the input are attended by eachobject as well as the background. We include an example of a failed segmentation in the last row,where one object slot (4) is trying to represent multiple objects at the same time.
Figure 4: Additional reconstructions on CLEVR2345.
Figure 5: Additional object additions and removals on CLEVR2345. For each scene, we showimages with one randomly added object, and with 1-3 random objects removed. Some of theseimages show out-of-distribution samples with a number of objects not seen during training (2-5objects).
Figure 6: Novel view synthesis on CLEVR2345. For each scene, we move the camera ±15°on the azimuth axis. Additionally, we zoom in the scene twice. While our model is trained witha fixed default camera pose and single scene views, it is able to generalize to small camera posemodifications and render novel views of a scene.
Figure 7: NeRF outputs on CLEVR2345. For each input scene we show the reconstructed imageas well as each the low resolution output of the NeRF function. This output is then upscaled with aneural network to obtain the reconstructed scene. While NeRF output lack full detail, they correctlydepict each individual object and their position in the scene.
Figure 8: Shape and appearance disentanglement on CLEVR2345. For each input scene we alterthe appearance vector of an object in the scene. First, we show the reconstructed scene. Then, forone of the objects in the scene, we replace its appearance vector by that of another object in thescene, while keeping its shape vector fixed. We perform this operation two times. We observe that,while the shading and color of the object changes as we change the appearance vector, its shaperemains constant.
Figure 9: Object occupancy maps on CLEVR2345. For each input scene we show the recon-structed image as well as each individual object occupancy map. Occupancy maps are obtained byintegrating the density outputs of the NeRF function along rays going from the image plane to thescene. We observe that each object is rendering a part of the scene corresponding to a single objectinstance. Outputs of the NeRF function have low resolution and are upscaled by a neural network.
