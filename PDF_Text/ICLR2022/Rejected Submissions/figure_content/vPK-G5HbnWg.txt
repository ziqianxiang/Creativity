Figure 1: Illustration of PACE. The input DAG is first injectively represented as a sequence throughthe dag2seq framework, and then the sequence is fed into multiple stacked masked Transformerencoder blocks. The operations of nodes (i.e. node types) are visualized as colors, and nodes in thesequence is sorted according to the canonical label generated in the dag2seq framework.
Figure 2: Illustration of the ambiguitywhen applying sequence models (suchas Transformer) to the DAG encodingproblem.
Figure 4: The illustration of PACE in the VAE architecture (PACE-VAE)Similar to PACE, the decoder is constructed upon the Transformer decoder block. Each Transformerdecoder block consists of a masked multi-head self-attention layer (i.e. Euqtion 7), a multi-headattention layer (i.e. Equation 5 except that the key matrix and value matrix are computed frompoints z in the latent space), and a feed-forward layer (i.e. Equation 6). The decoder takes aMLP as the embedding layer to generate node type embeddings as PACE. In analogous to thedag2seq framework in PACE, the decoder also uses a GNN to generate the positional encoding basedon the learnt canonical order of nodes. Then the node embeddings and positional encodings areconcatenated and then fed into multiple consecutive Transformer decoder blocks to predict the noderepresentations, which is used to predict the node types and the existence of edges. In analogous tothe standard Transformer decoder, the decoder performs the shift right trick (i.e. the ith outputednode representation corresponds to the i + 1th node in the sequence) and adds a start symbol node (i.e.
Figure 5: Best architectures on NA and BN detected by PACE.
