Figure 1: (a) The infinite-data ambiguity of reward learning data sources, and the ambiguity toler-ance of downstream applications of a learnt reward function, are both invariances of objects derivedfrom reward functions (sections 1.2 and 3). These invariances have a partial order (section 4): here,X → Y means that Y can be derived from X, or equivalently that Y is at least as ambiguous asX . The objects are: the reward function itself (R); Q-functions (Q); Maximum Entropy (β) andsupportive optimal policies (<) and their induced trajectory distributions (∏β, ∆g and ∏<, △*); thereturn function restricted to partial and full trajectories (Gζ, Gξ); Boltzmann-distributed (β) andnoiseless (<) comparisons between these trajectories (We, We and W<, W<). (b) Several basic fami-lies of reward transformations form the basis for our main results (section 2). These transformationsexist in a related hierarchy, within (shown here) and across tasks (section 4).
