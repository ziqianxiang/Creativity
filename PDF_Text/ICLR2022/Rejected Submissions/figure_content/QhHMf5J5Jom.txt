Figure 1: Empirical results of syn2real transfer for different tasks. We conducted four pre-trainingtasks: object detection (objdet), semantic segmentation (semseg), multi-label classification(mulclass), surface normal estimation (normal), and three fine-tuning tasks for benchmarkdatasets: object detection for MS-COCO, semantic segmentation for ADE20K, and single-labelclassification (sinclass) for ImageNet. The y-axis indicates the test error for each fine-tuning task.
Figure 2: Scaling curves with different (a) pre-trainingsize and (b) fine-tuning size.
Figure 4: Effect of model size. Best viewed in color. Left: The scaling curves formulclass→sinclass and objdet→objdet cases. The meanings of dots and lines are thesame as those in Figure 1. Right: The estimated transfer gap C (y-axis) versus the model size (x-axis)in log-log scale. The dots are estimated values, and the lines are linear fittings of them.
Figure 5: Ability to extrapolate. Left: The solid lines represent the fitted power law and the dashedcurves represent the fitted scaling law (1), in which the laws were fitted using the empirical errorswhere the pre-training size n was less than 64,000 (the first five dots). The vertical dashed lineindicates where n = 64,000. Right: The root-mean-square errors between the laws and the actualtest errors in the area of extrapolation (the last five dots).
Figure 6: Effect of synthetic image complexity. Best viewed in color. Left: Scaling curves ofdifferent data complexities. Right: Estimated parameters. The error bars represent the standard errorof the estimate in least squares.
Figure 7: Example of generated datasets17Under review as a conference paper at ICLR 20220.7rorre tse0.3pretrain tasksemseg	mulclass	normalFigure 8: The estimated values of the pre-training rate α and the transfer gap C in the cross-tasksetting (as the same as Figure 1). The error bars present the standard error of the estimates in theleast squares.
Figure 8: The estimated values of the pre-training rate α and the transfer gap C in the cross-tasksetting (as the same as Figure 1). The error bars present the standard error of the estimates in theleast squares.
Figure 9: Empirical and fitting results for various pre-train and fine-tune data sizes inmulclass→sinclass. All curves are fitted using the full law (2). Best viewed in color. Left:Effect of pre-training data size (x-axis) for fixed fine-tuning data sizes. Right: Effect of fine-tuningdata size (x-axis) for fixed pre-training data sizes.
Figure 10: The linearized version of Figure 1.
Figure 11: The linearized version of Figure 4.
Figure 12: The linearized version of Figure 6.
Figure 13: Examples of parameter dependency.
Figure 14: Loss landscapes of curve fittings.
