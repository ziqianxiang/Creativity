Figure 1: An illustration of GAL for NLP. We use open-domain data once for self-supervised pretraining(e.g., BERT) and once for training a large LM (e.g., GPT-2). BERT is fine-tuned on labeled data to yield aclassifier for the task of interest. GPT-2 is fine-tuned on the same data without labels to obtain an unconditionaltask-specific LM, which is used to generate lots of synthetic in-domain unlabeled data for self-training and KD.
