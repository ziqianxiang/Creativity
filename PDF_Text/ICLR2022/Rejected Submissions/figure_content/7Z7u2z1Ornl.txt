Figure 1: Increasing complexity.
Figure 2: Training time of backprop withskips. Relative training time and perfor-mance for different T ∈{4, 16, 32}. All runsrequire the same memory, except standardbackprop T ∈{16, 32}, which require more.
Figure 3: Recurrent vs. stacked. (a) Performance for different numbers of iterations. (b) Extrap-olation performance on n∈[11 . . 20] for models trained with set size n=10. We stop training therecurrent model early, to match the validation performance of the stacked on n=10. The recurrentmodel derives greater benefits from adding iterations and generalizes better.
Figure 4: Adding gradient skips tobackprop (a) Standard backprop (b)TBPTT, applying backprop on 4 itera-tions every 2nd iteration (c) Backpropwith skips at iterations 1, 6, 7, 8, whicheffectively reduces the training time,while retaining the same number of re-finement steps.
