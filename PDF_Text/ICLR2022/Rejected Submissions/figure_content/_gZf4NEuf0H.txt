Figure 1: The test accuracy in (a) and condensation in (b, c) of networks on CIFAR10. Each networkconsists of the convolution part of resnet18 and fully-connected (FC) layers with size 1024-1024-10and softmax. The color in (b, c) indicates the inner product of normalized input weights of twoneurons in the first FC layer, whose indexes are indicated by the abscissa and the ordinate. Wediscard about 55% of the hidden neurons, in which the L2-norm of each input weight is smallerthan 0.001, while remaining ones bigger than 0.05 in (b). The convolution part is equipped withReLU activation and initialized by Glorot normal distribution (Glorot & Bengio, 2010). For FClayers in (a), the activation is ReLU and they are initialized by three common methods (red) andthree condensed ones (green) as indicated in Table 1. The learning rate is 10-3 for epoch 1-60 and10-4 for epoch 61-100. For (b, c), the learning rate is 5 × 10-6 for visualization and FC layersare initialized by N(0, -4—) and equipped with ReLU in (b) and X tanh(x) in (C) as activationmoutfunctions. Adam optimizer with cross-entropy loss and batch size 128 are used for all experiments.
Figure 2: Condensation of two-layer NNs. The color indicates D(u, v) of two hidden neurons’ inputweights at epoch 100, whose indexes are indicated by the abscissa and the ordinate, respectively.
Figure 3: Condensation of six-layer NNs with residual connections. The activation functions forhidden layer 1 to hidden layer 5 are x2 tanh(x), x tanh(x), sigmoid(x), tanh(x) and softplus(x),respectively. The numbers of steps selected in the sub-pictures are epoch 1000, epoch 900, epoch900, epoch 1400 and epoch 1400, respectively, while the NN is only trained once. The color in-dicates D(u, v) of two hidden neurons’ input weights, whose indexes are indicated by the abscissaand the ordinate, respectively. The training data is 80 points sampled from a 3-dimensional functionP3k=1 4 sin(12xk + 1), where each xk is uniformly sampled from [-4, 2]. n = 80, d = 3, m = 18,dout = 1, var = 0.012, lr = 4 × 10-5.
Figure 4: Condensation of low-frequency functions with two-layer NNs in (a,b) and condensation ofthe first FC layer of the Resnet18-like network on CIFAR100 in (c). The color indicates D(u, v) oftwo hidden neurons’ input weights, whose indexes are indicated by the abscissa and the ordinate. For(a,b), two-layer NN at epoch: 100 with activation function: x2 tanh(x). For (a), we discard about15% of hidden neurons, in which the L2-norm of each input weight is smaller than 0.04, whileremaining those bigger than 0.4. The mean magnitude here for each parameter is (0.42/785)0.5〜0.01, which should also be quite small. All settings in (a) are the same as Fig. 2, except for thelower frequency target function. Parameters for (b) are n = 60000, d = 784, m = 30, dout = 10,var = 0.0012. lr = 5 × 10-5. The structure and parameters of the Resnet18-like neural networkfor (c) is the same as Fig. 1(b), except for the data set CIFAR100 and learning rate lr = 1 × 10-6.
Figure 5: The outputs of two-layer NNs at epoch 1000 with activation functions tanh(x), x tanh(x),x2 tanh(x), and ReLU(x) are displayed, respectively. The training data is 40 points uniformlysampled from sin(3x) + sin(6x)/2 with x ∈ [-1, 1.5], illustrated by green dots. The blue solidlines are the NN outputs at test points, while the red dashed auxiliary lines are the first, second,third and first order polynomial fittings of the test points for (a, b, c, d), respectively. Parameters aren = 40, d = 1, m = 100, dout = 1, var = 0.0052, lr = 5 × 10-4.
Figure 6: The direction field for input weight w := (w, b) of the dynamics in (4.3) at epoch 200.
Figure 7: Losses from Fig. 1 to Fig.4. The original pictures and the numbers of steps correspondingto each sub-picture are written in the sub-captions.
Figure 8: Losses from Fig. 5 to Fig.6. The original pictures and the numbers of steps correspondingto each sub-picture are written in the sub-captions.
Figure 9: Evolution of condensation from Fig. 1(b) to Fig. 2(b) and Fig. 1(c). The evolution fromthe first row to the fifth row are corresponding to the Fig. 1(b), Fig. 1(c), Fig. 4(c), Fig. 2(a), Fig.
Figure 10: Evolution of condensation from Fig. 2(c) to 2(f) and Fig. 4(a). The evolution from thefirst row to the fifth row are corresponding to the Fig. 2(c), Fig. 2(d), Fig. 2(e), Fig. 2(f), Fig. 4(a).
Figure 11: Evolution of condensation from Fig. 3(a) to 3(e). The evolution from the first row to thefifth row are corresponding to the Fig. 3(a), Fig. 3(b), Fig. 3(c), Fig. 3(d), Fig. 3(e). The numbersof evolutionary steps are shown in the sub-captions, where sub-figures in the last row are the epochsin the article.
Figure 12: Evolution of condensation from Fig. 6(a) to 6(d) and 4(b). The evolution from the firstrow to the fifth row are corresponding to the Fig. 4(b), Fig. 6(a), Fig. 6(b), Fig. 6(c), Fig. 6(d). Thenumbers of evolutionary steps are shown in the sub-captions, where sub-figures in the last row arethe epochs in the article.
Figure 13: Condensation of Resnet18-like neural networks on CIFAR10. Each network consistsof the convolution part of resnet18 and fully-connected (FC) layers with size 1024-1024-10 andsoftmax. The color in figures indicates the inner product of normalized input weights of two neuronsin the first FC layer, whose indexes are indicated by the abscissa and the ordinate, respectively. Wediscard the hidden neurons, in which the L2-norm of each input weight is smaller than 0.001, whileremaining ones bigger than 0.05 in (b). The convolution part is equipped with ReLU activationand initialized by Glorot normal distribution (Glorot & Bengio, 2010). The activation functions aretanh(x), ReLU(x), sigmoid(x), softplus(x) and x tanh(x) for FC layers in (a), (b), (c), (d) and(e), separately. The numbers of steps selected in the sub-pictures are epoch 20, epoch 8, epoch 30,epoch 30 and epoch 61, respectively. The learning rate is 3 × 10-8, 5 × 10-6, 1 × 10-8, 1 × 10-8and 5 X 10-6, separately .The FC layers are initialized by N(0, --3-), and Adam optimizer withmoutcross-entropy loss and batch size 128 are used for all experiments.
Figure 14: Condensation of Resnet18-like neural networks on CIFAR100. Each network consistsof the convolution part of resnet18 and fully-connected (FC) layers with size 1024-1024-10 andsoftmax. The color in figures indicates the inner product of normalized input weights of two neuronsin the first FC layer, whose indexes are indicated by the abscissa and the ordinate, respectively. Wediscard about 15% of the hidden neurons, in which the L2-norm of each input weight is smallerthan 0.0001, while remaining ones bigger than 0.0025 in (b). The convolution part is equippedwith ReLU activation and initialized by Glorot normal distribution (Glorot & Bengio, 2010). Theactivation functions are tanh(x), ReLU(x), sigmoid(x), softplus(x) and x tanh(x) for FC layersin (a), (b), (c), (d) and (e), separately. The numbers of steps selected in the sub-pictures are epoch 10,epoch 10, epoch 10, epoch 10 and epoch 250, respectively. The learning rate is 1 × 10-7, 1 × 10-7,3 × 10-8, 3 × 10-8 and 1 × 10-6, separately. The FC layers are initialized by N(0, -4—), andmoutAdam optimizer with cross-entropy loss and batch size 128 are used for all experiments.
Figure 15: Condensation of six-layer NNs without residual connections. The activation functions forhidden layer 1 to hidden layer 5 are x2 tanh(x), x tanh(x), sigmoid(x), tanh(x) and softplus(x),respectively.The numbers of steps selected in the sub-pictures are epoch 6800, epoch 6800, epoch6800, epoch 6800 and epoch 6300, respectively, while the NN is only trained once. The colorindicates D(u, v) of two hidden neurons’ input weights, whose indexes are indicated by the abscissaand the ordinate, respectively. The training data is 80 points sampled from a 3-dimensional functionP3k=1 4 sin(12xk + 1), where each xk is uniformly sampled from [-4, 2]. n = 80, d = 3, m = 18,dout = 1, var = 0.0082, lr = 5 × 10-5.
Figure 16:	Three-layer NN at epoch 700. (a-f) are for the input weights of the first hidden layer and(g-l) are for the input weights of the second hidden layer. The color indicates D(u, v) of two hiddenneurons’ input weights, whose indexes are indicated by the abscissa and the ordinate, respectively.
Figure 17:	Five-layer NN. The first to fourth columns of each row are for the input weights ofneurons from the first to the fourth hidden layers, respectively. The color indicates D(u, v) of twohidden neurons’ input weights, whose indexes are indicated by the abscissa and the ordinate, respec-tively. The training data is 80 points sampled from a 5-dimensional function P3k=1 3 sin(10xk + 1),where each xk is uniformly sampled from [-4, 2]. n = 80, d = 5, m = 18, dout = 1,var = 0.0082. lr = 1.5 × 10-5, 1.5 × 10-5, 1.5 × 10-5, 1.5 × 10-5, 1.5 × 10-6 andepoch is 10000, 10000, 26000, 10000, 20000 for tanh(x), xtanh(x), x2tanh(x), sigmoid(x),softplus(x), respectively.
Figure 18: Five-layer NN. The first to fourth columns of each row are for the input weights ofneurons from the first to the fourth hidden layers, respectively. The color indicates D(u, v) of twohidden neurons’ input weights, whose indexes are indicated by the abscissa and the ordinate, respec-tively. The training data is 80 points sampled from a 5-dimensional function P3k=1 3 sin(10xk + 1),where each xk is uniformly sampled from [-4, 2]. n = 80, d = 5, m = 18, dout = 1, var = 0.0082.
