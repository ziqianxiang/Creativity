Figure 1: Recognition performance of w2v-BERTwith ATM and random masking on IHM-eval andtest-other sets by varying the masking percent-age during pre-training. The FT is performedon LS-100 for evaluating test-other, while IHM-eval is evaluated with model FT with AMI. Ran-dom masking shows a substantial shift in perfor-mance when varying the masking from 30% to40%, while the ATM remains robust to changesin masking percentage.
Figure 2: Performance comparison of different MSM architectures with and without applying ATMon IHM-eval and SDM-eval in AMI. All these models are FT using AMI. Here “cfr” refers toconformer.
Figure 3: 2D plot of the %WER on IHM-eval across PT and FT ckpts. using baseline (left) andATM (right) models. The FT is done using AMI.
Figure 4: Comparing the behavior of ATM using LS-scorer and AMI-scorer having different %WERon 1 hour of Libri-light (LL-1hr) with AMI evaluation sets)challenging scenario by testing the ability of ATM using scorers from different domains with varyingrange of performances. A LS-scorer with 51.8%WER on LL-1hr performs well on both IHM-evaland SDM-eval, while the AMI-scorer with 46.8% WER slightly improves over the above mentionedLS-scorer. The key aspect in selecting better scorer is to choose ckpt. either from the category whichachieves %WER within the range of 50-60 on the small subset of pre-training data.
Figure 5: Validating baseline, ATM and ATM+S using contrastive loss, number of unique codesused from the codebook and the MSM accuracy on dev-other during pre-training. A small bump isobserved at around 0.1 million training iteration due to the change in learning rate and is ignoredduring analysis.
Figure 6: Working procedure of HuBERT-conformer model as described in section 2.3. The k-means cluster ids act as labels and they are refined using the bottleneck features extracted from thecontext network itself.
Figure 7: Working procedure of Wav2vec2-conformer model as described in section 2.3. The en-coded representations are masked and passed to context network Ω and the resulting output Cj islearnt to be closer to quantized outputA.8 Statistical Significance analysis on %WER performance for multipleEVALUATION SETSThe table 4 results are on Librispeech and obtaining 0.1% improvement in Librispeech testsets isstatistically signficant. For instance, the dev-clean test set contains 54402 words and 0.1% gainsdenotes a recovery of 54 words. Also, the recent works on self-supervised training such as HuBERTshows improvement between wav2vec2-Large and HuBERT-Large only on dev-clean with 0.1%gain in Table 3.
Figure 8: Working procedure of W2V-BERT model as described in section 2.3. Cross-entropy lossis computed between predictions of context network Λ and the quantized labels yj . Contrastive lossis computed in parallel as in wav2vec2.
