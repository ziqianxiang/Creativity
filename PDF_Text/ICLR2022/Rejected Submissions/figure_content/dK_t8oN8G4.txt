Figure 1: Top: For each training example x, our algorithm uses program synthesis to infer therelational constraints cx = A(x) present in x. Then, it uses cx and x to train pφ(cx) and pθ(x | cx).
Figure 2: Left: Poetry generated using relational constraints C 〜pφ (∙). Right: user modified variantof c where the last two lines share a prototype with the two lines before them.
Figure 3: A song generated using our approach, and a nearly identical song generated where part ofthe sampled relational constraints c were manually modified. These pieces were generated using A3,and the same reference measures W were used, but Φc was slightly perturbed (the similarity relationswere changed).
Figure 4: Left: Poetry generated using relational constraints C 〜pφ(∙). Right: user modified variantof c where the last two lines share a prototype with the two lines before them.
Figure 5: An example of a song generated using our approach (A1). Measures that have the sameprototype are shown in the same color. Note the existence of repeating four-bar phrases, foundcommonly in folk songs.
Figure 6: An example of a song generated using our approach (A2). Measures that have the sameprototype are shown in the same color. Note the existence of clear phrase endings marked by longnotes or rests, particularly the recurring pattern of fast notes resolving into long notes.
Figure 7: An example of a song generated using our approach (A3). Measures that have the sameprototype are shown in the same color. The existence of two-bar and three-bar phrases is apparent,but the close note and rhythm similarities among different prototypes weaken the overall clarity ofthe song’s melody.
Figure 8: An example of a song generated using Magenta’s hierarchical MusicVAE model finetunedon our dataset. While the local structure is extremely coherent, it does not seem to possess theexpected internal repetition/development.
Figure 9: An example of a song generated using AttentionRNN trained on our dataset. Note theexistence of erratic rhythms and unclear structure, which are common traits of custom-trainedAttentionRNN models.
Figure 10: An example of a song generated using MusicAutoBot. Note the repetitive nature andstark contrast between the first half and second half of the song, which are common problems withtransformer models.
Figure 11: An example of a song generated using StructureNet. While some degree of internalstructure is apparent, and the local coherence is high, the pattern of internal repetition seems fairlyarbitrary.
Figure 12:	Left: Poetry generated using relational constraints C 〜pφ (∙). Right: Poetry generated byGPT2-Opt. Notice the lack of characteristic structure in GPT2-Opt, despite its coherence.
Figure 13:	An example of poetry generated using our approach (A1). Lines that have the sameprototype are shown in the same color.
Figure 14:	An example of poetry generated using our approach (A3).
Figure 15:	A poem generated using GPT2-Opt. It is more plausible than BERT in terms of globalstructure, which may be due to the fact that GPT2 is a better text generation tool than BERT, but it isstill somewhat repetitive and its structure is not very human-like.
Figure 16:	A poem generated using BERT. It is clearly overly repetitive and not very semanticallycoherent, and lacks high-level structure.
Figure 17:	A poem generated using RichLyrics. While it is less repetitive than non-conditionedBERT, it is still not very semantically coherent, and lacks high-level structure.
Figure 18:	A poem generated using our ablation. While it is much more coherent, it lacks theidiomatic rhyme and meter structure of our approach.
