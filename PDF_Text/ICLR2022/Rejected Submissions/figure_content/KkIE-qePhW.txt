Figure 1: Illustration of consistent pruning versus non-consistent pruning. On the left, the inputconsists of a graph with two highlighted environments whose topology is similar because the cen-tral nodes have the same set of neighbors. In this case, consistent pruning results with a graphthat preserves the similarity between these environments while non-consistent pruning, e.g., ran-dom removal of edges, results with dissimilar environments. On the right, the two highlightedenvironments have dissimilar topologies because the central nodes have different sets of neighbors.
Figure 2: Example scenario in which we are tasked to classify nodes into 2 classes. Each of the nodesA, B, C, D belong to class 2, while each of the nodes E, F, G, H belong to class 1. Moreover, Foreach node among {E, F, H, G} there exists a node among {A, B, C, D} that the intersection of thesets of their neighbors has size of at least 2.
Figure 3: Test results for node classification problems. For each dataset, we report the results interms of the metriC identified with that dataset (in solid lines), and the running time per iteration (indensely dotted lines) under the speCified pruning Configurations.
Figure 4: Mean Absolute Error (MAE) and normalized running times for graph regression problems.
Figure 5: Accuracy and running time comparisonfor the synthetic graph classification dataset.
Figure 6:	Variance of neighborhood sizes as a function of edge percentage preserved from the orig-inal graph (Log scale).
Figure 7:	Additional reSultS for node claSSification problemS. We report the reSultS in termS of themetric identified with the dataset (in solid lines), and the running time per iteration (in densely dottedlines) under the specified pruning configurations.
Figure 8:	MAE and normalized running times for additional QM9 targets. Errors are presented assolid lines (Lower is better). Normalized running times are presented as dotted lines.
