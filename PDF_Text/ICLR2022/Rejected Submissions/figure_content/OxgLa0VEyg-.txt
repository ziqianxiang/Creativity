Figure 1: Algorithm schematic. Loss LÏ‰ is trained to optimize held-out domain performance onR-MNIST and then deployed on novel datasets.
Figure 2:	The evolution of posterior entropy for target domain test samples during training. Left:Correctly classified samples. Right: Misclassified samples.
Figure 3:	Perturbation analysis on OfficeHome: ITL-Net vs ERM. Multiplicative Gaussian noisewith mean 1 and std: 0.01, 0.05, 0.08 is added to network weights.
Figure 4: 1D Loss Landscape: ITL-Net vs ERM on OfficeHome. Left two: Source domain losslandscape. Right two: Target domain loss landscape.
Figure 5:	Comparison of loss functions (from left to right): CE, SCE, FOCAL and ITL. The range ofITL is normalised between 0 and 1.
Figure 6: The learning curve for ITL meta-training stage.
Figure 7: 1D Loss Landscape: ITL-Net vs ERM on OfficeHome. Top row: Source domain losslandscape. Bottom row: Target domain loss landscape.
