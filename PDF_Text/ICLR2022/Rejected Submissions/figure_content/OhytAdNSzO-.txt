Figure 1: Our scaled ViT models achieve comparableor better accuracy-efficiency trade-off as compared tosome recent dedicatedly designed ViT variants.
Figure 2: Illustrating the effect of scaling fac-tors on a ViT architecture (class/distillationtoken is omitted for better visual clarity).
Figure 3: The accuracy of ViT is sensitive to the aspectratio. Note that the vertically aligned points are modelswith the same scaling factors except image resolution (I).
Figure 4: Random permutation on top of theDeiT-Scaled, where those on the Pareto fron-tier are marked as DeiT-Scaled-RP.
Figure 5: Resulting models from our iterativegreedy search, where models achieving the bestaccuracy-FLOPs trade-offs are marked as DeiT-Scaled-Tiny/Small/Base. The architecture configurations (i.e.,sets of d, h, e, r, I, and p) leading to these best modelsare extracted as our scaling strategies dedicated to ViT.
Figure 6: Comparing the optimaldevices. FLOPs/V100/TX2/Pixel3O Baseline■* FLOPs Scaling△ VlOO ScalingO TX2 Scaling*⅜ Pixel3 Scaling50	100	150	200	250	300	350Latency on PiXel3 (ms)(d)models resulting from scaling for different hardwareScaling represents the scaling strategies obtained onFLOPs/V100/TX2/Pixel3, and DeiT models are marked as the comparison Baseline.
Figure 7: Cost breakdown of DeiT-Tiny on different devices in terms of (1) the number of FLOPs,(2) the 1/FPS on V100, (3) the latency on TX2, and (4) the latency on Pixel3, where MLP rep-resents the cost of all the Linear layers of ViT, MSA-SA-MatMul represents the cost of ma-trix multiplication among Q(uery), K(ey), and V(alue) in ViT’s multi-head attention, MSA-SA-Reshape&Transpose&Gather represents the cost of merely the data movement in ViT’s multi-headattention, and the cost of all other operators are denoted as Others.
Figure 8: The rank correlation co-efficient between the hardware-cost on different devices.
