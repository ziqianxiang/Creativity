Figure 1: Corpus BLEU scores of LAMIN1and LAMIN2 under different Boltzmanntemperature β . Greedy policy is used in bothcases. The gray dashed line indicates thebaseline performance of 27.3 achieved by abeam-search policy in Vaswani et al. (2017).
Figure 2: Learning curves of some LAMINvariants. Beam search with a beam size of 4is used for the orange line, which matches thesame setting with the baseline result of 27.3(gray dashed line). Other lines use greedypolicy which is about 2x faster.
Figure 3: An ELP examplecause Qmin(1, a) > BQmin(1, a) = 1 for all a, the λcorresponding to Qmin needs to have λ(1, a) = 0 for all a. Such a λ cannot encode any policy.
Figure 4: An μ-admissible trajectory that goes through s* but never returns.
Figure 5: A copy of Figure 3C.3 AN COUNTER-EXAMPLE FOR THE Q-FORM MINIMAX VALUES IN ELPSIn this subsection we elaborate more about the counter-example as illustrated by Figure 3 in Sec-tion 5 (which is copied above). In this ELP, S = {0, 1, 2, 3, 4, 5}, A = {1, 2, 3}. State 4 and 5are terminal states, from which any action leads to state 0. Choosing action 1, 2, 3 under state 0deterministically transits to state 1, 2, 3, respectively. All actions under state 1 lead to state 4, andall actions under state 2 and 3 lead to state 5. The agent only receives non-zero rewards at terminalstates, with R(4) = 1, R(5) = 2. The initial state is set to state 4 (i.e. ρ0(S) > 0 only if S = 4).
Figure 6: An example of discounted-MDP for studying saddle points of the V -form Lagrangian.
