Figure 1: Random unlabeled samples considered to be task-relevant by the SSL method (confidenceabove in-distribution thresholds for ODST, above 95% confidence for Fixmatch and above the Otsuthreshold for MTCF) are shown together with their confidence and predicted label (mistakes aremarked red). MTCF and Fixmatch show severe distribution shift and only our ODST is able toselect the correct samples. All methods are trained on CIFAR10 with 4k labeled images and anunlabeled set consisting of 41k CIFAR10 training images and 1M images from 80MTI.
Figure 2: CIFAR10-50k: Plot of randomly chosen, exclusively selected samples from 80MTI forNSST (top) and ODST (bottom) over all three iterations. False positives are marked red.
Figure 3: CIFAR100-45k: Random selection of 14 samples in the third iteration of self-training forthe ResNet50 architecture. False positives are marked in red.
Figure 4: Logarithmic histogram of nearest neighbour distances between CIFAR test set and 80MTIfor image pairs with an l2 distance below 2000/255.
Figure 5: Visualization of various exclusion thresholds for CIFAR10 (above line) and CIFAR100(below line). The top image shows the sample from 80MTI and the lower one the nearest neighbourin the test set.
Figure 6: CIFAR10-4k + 1M TI: Visualisation of 42 randomly drawn samples considered to betask-relevant for various methods. For ODST and NSST , those are all samples above the in-distribution threshold in the 5th training iteration and above both the in- and out-distribution thresh-old for ODST+ and NSST+. FixMatch uses a fixed confidence threshold (95%) and MTCF Otsuthresholding to determine which samples are from in- and out-of-distribution. Both variants ofODST are the only methods where almost all selected samples are indeed task relevant and ODST+achieves slightly higher precision due to the additional OD-threshold. FixMatch and MTCF are ableto select task-relevant samples, however both show a high false-positive rate and make systematicerrors, for example FixMatch and to some extent MTCF as well learns to label Humans as Dog.
Figure 7: CIFAR10-4k + 1M LSUN: Similar to Figure 6, we plot accepted samples for variousmethods. In this scenario, FixMatch, NSST and NSST+ do once again not show the desired be-haviour. On the other hand, ODST, ODST+ and MTCF show nearly perfect sample selection. WhileMTCF has a great precision (99.70%), we note that it suffers from poor recall (18.47%) and thusdoes not make use of a majority of available in-distribution samples. ODST and ODST+ on the otherhand have a comparable precision (99.26%,98.67%) with a much higher recall (92.81%,92.50%).
Figure 8: CIFAR10-4k+1M TI: Number of samples per class from the unlabeled data selectedas task-relevant for ODST, ODST+, FixMatch and MTCF with a logarithmic y-axis scaling. Theblue bar corresponds to the absolute number of added samples and the orange bar only shows thesamples which come from the unlabeled train data (remember that 41k CIFAR10 samples are inthe unlabeled dataset). We note the very uneven selection of MTCF with almost no cars but a lotof frogs (of which most are wrong, see Figure 6), while the selection of ODST+ and FixMatchis more evenly distributed even though FixMatch selects far too many as task-relevant (428k out of1041k possible). As 80MTI contains some task relevant samples, a perfection selection will have theblue bar slightly higher than the orange one, i.e. it will add the relevant samples out of the 1M tinyimages. However, the disparity between the two should not be too large (FixMatch) as this impliesthat unrelated rubbish images are added. Also, the orange bars should not be too low (MTCF) asthis implies that a lot of available in-distribution samples are not used.
Figure 9: CIFAR10-4k+1M TI - Analysis ofMTCF: We show the histogram of confidence valuesof the CIFAR10-classifier of MTCF for all unlabeled samples (left) and of the ones which are forthe ID-OD-classifier of MTCF above the Otsu-threshold and thus considered task-relevant by MTCF(right). Note that MTCF considers a lot of samples task-relevant for which the classifier is quitelow-confident and discards the majority of samples for which the classifier is highly confident. Thisin strong contrast to ODST+ where only unlabeled samples are selected for which the classifier ishighly confident (above the in-and out-distribution threshold).
Figure 10: CIFAR10-50k: We visualize the selected samples that are over the in-distribution thresh-old for ODST and NSST and above both the in- and out-distribution threshold for ODST+ in NSST+in the third iteration of self-training. Note that both NSST variants suffer from a strong distributionshift and start making systematic mistakes, like labeling humans as ”dog”. Both ODST variants onthe other hand are able to select mostly task-relevant samples27Under review as a conference paper at ICLR 2022C.3 CIFAR100-45k:Similar to the CIFAR10-50k setting, we use the full CIFAR100 training set in this setting, howeveras there is no validation set available we split it into 45k train and 5k validation images. We againuse all 80 million tiny images as unlabeled data. The OOD-validation set for the Plus variants are 2kCIFAR10 samples with classes ”car” and ”truck” removed. In Table 8, we again report corruptionrobustness measured on CIFAR100-C as well as test set accuracy and OOD-FPR95. Again we alsogive the chosen self-training iteration based on validation accuracy.
Figure 11: CIFAR100-45k: We visualize the selected samples that are over the in-distributionthreshold for ODST and NSST and above both the in- and out-distribution threshold for ODST+AND NSST+ by the first iteration model (Top 50k).
