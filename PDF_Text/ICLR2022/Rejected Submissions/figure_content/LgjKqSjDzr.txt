Figure 1: The Architecture of SALT that has two blocks, Transformers and Linear layers. SALTtrain with [CLS] token for fine-tuning and with contextual embedding values for Semi-supervisedlearning and Pre-training.
Figure 2: The blocks of SALT. The left (a) is a sub block of Linear layer block. The right (b) is asub block of Transformer block.
Figure 3: The improved embedding layer introduced by SALT.
Figure 4: The variants of SALT. (a) SALT (b) SALT-linear (c) SALT-former (d) SALT-onewayAs shown in Figure 4, there are four variants of SALT. The first is two-way sharing between twoblocks. The second and third variants are designed only with Transformer and Linear layer blocks,6Under review as a conference paper at ICLR 2022SALT variantsshare mode	SALT	SALT-OneWay	SALT-former	SALT-Linearshare	0.9094	0.9075	0.9069	0.9066Non share	0.9089	0.9071	0.9065	0.9055Table 3: The results of the comparison between share mode and Non share mode with SALT variants.
Figure 5: Visualization of the (feature-wise) attention matrices. For each stack in the model we plotthe row. (a) and (b) show the effect of share mode and non-share mode, respectively. (c) is fromSALT-foremr and (d) is from SALT-linear.
