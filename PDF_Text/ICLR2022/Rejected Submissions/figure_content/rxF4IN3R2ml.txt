Figure 1: Example of a demand forecasting task where periods T - 3 and T + 2 both have promotions.
Figure 2: The decoder attends over past forecasts for the same target horizon - the context ht containsfeedback information (demand and other encoded signals), allowing the model to adjust forecaststhat are either too volatile or not volatile enough as the target date approaches.
Figure 3: Martingale diagnostic process {Vt} for P50 (left) and P90 (right) forecasts. Trajectoriesare demand-weighted and the results are averaged over all items, target weeks in the test period(2018-2019). Closer to zero is better.
Figure 4: Components of the MQTransformer architectureand weights are shared across all time points and horizons. The output layer has one output perhorizon, quantile pair. Figure 4a shows the structure of both our attention blocks, where the sub-layer“Attention Cell” is replaced with either the decoder-encoder or decoder-self attention.
Figure 5: MQTransformer architecture with learned global/local positional encoding, horizon-specificdecoder-encoder attention, and decoder self-attention16Under review as a conference paper at ICLR 2022C Large S cale Demand Forecasting ExperimentsC.1 Ablation StudyIn the ablation study, the model MQT-All uses a standard multi-head attention applied to the concate-nated contexts for all horizons at each period t. A separate head is used for each output horizon (52heads on the private dataset).
Figure 6: Forecast evolution analysis on the retail dataset. Left: Martingale Diagnostic Process{Vt}. Right: QL by lead time, averaged over target dates from 2016-03-01 through 2016-05-01; QLtrajectories are centered around 0.
