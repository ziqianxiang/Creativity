Figure 1: Sampled posterior distributions between w1 & w2 (top row) and w1 & w3 (bottom row) fora linear regression model. From left to right, (a) the first column has posterior distributions sampledwith SGLD, (b) the second and third columns are sampled from S-SGLD, and (c) the fourth and fifthare sampled from Sd-SGLD. The imposed parameter structure for each are shown above the plots.
Figure 2: Comparison between pSGLD, S-pSGLD, and Sd-pSGLD of various numbers of parametergroups M for (a) accuracy, (b) IAC, and (c) ESS. Right-most points on the plots represent modelsthat have every parameter belonging to its own parameter group. S-pSGLD methods are were notable to be evaluated at such extreme values of M due to computational scaling.
Figure 3: Comparison for (a) CIFAR-10, (b) SVHN and (c) FMNIST using ResNet-20 with (i)pSGLD and (ii) SGHMC sampling algorithms and their proposed variational variants for modelaveraged accuracy. Grid search was used to determine optimal hyperparameters for each method —more details can be found in the Appendix.
Figure 4:	Model comparison with different hyperparameters for the pSGLD versions of our baselinesand proposed algorithms. We evaluated expected accuracy over iterations on CIFAR-10. Subscript"d" denotes the dropout version of our approach. LR refers to the learning rate, and ρ denotes thedropout rate.
Figure 5:	Model comparison with different hyperparameters for the SGHMC versions of our baselinesand proposed algorithms. We evaluated expected accuracy over iterations on CIFAR-10. Subscript"d" denotes the dropout version of our approach. LR refers to the learning rate, and ρ denotes thedropout rate.
Figure 6:	Model comparison with different hyperparameters for the pSGLD versions of our baselinesand proposed algorithms. We evaluated expected accuracy over iterations on SVHN. Subscript "d"denotes the dropout version of our approach. LR refers to the learning rate, and ρ denotes the dropoutrate.
Figure 7:	Model comparison with different hyperparameters for the SGHMC versions of our baselinesand proposed algorithms. We evaluated expected accuracy over iterations on SVHN. Subscript "d"denotes the dropout version of our approach. LR refers to the learning rate, and ρ denotes the dropoutrate.
