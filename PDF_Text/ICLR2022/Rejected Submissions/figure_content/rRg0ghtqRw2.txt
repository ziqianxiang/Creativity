Figure 1: The evolution of a level: At first, the editor places blocks outside the trajectory of the optimal policy,which acts as an augmentation as the agent has a fully observable view. Then, the editor moves the agent furtherfrom the goal, before placing challenging obstacles in its path. Note that since the agent can move diagonally inthis environment, the final level is solvable. Each level is a high Positive Value Loss at the time it is included inthe level store, thus the level co-evolves with the agent over time.
Figure 2: An overview of ACCEL. Levels are (randomly)sampled from a generator, and evaluated, with high regretlevels added to the level buffer. The curator selects levelsto replay, which are used to train the student agent. Aftertraining, the levels are passed to the editor and the editedlevels are added to the level store if they are high regret.
Figure 3: Levels generated by ACCEL. Though alllevels are evolved from the same DR level, they re-quire different behaviors to solve. Left: the agent cango up or left and reach the goal. Middle: the goal ison the left, while on the right the left path is blocked.
Figure 5: MiniGrid training data. a): the performance of all agents on the the shortest path length and numberof blocks on training levels, where ACCEL quickly develops highly challenging levels. Plots show the meanand standard error across five runs. b) Example levels generated by DR, PLR and ACCEL.
Figure 6: Zero-shot transfer results. Agents are evaluated for 100 episodes on a series of human designedmazes, plots show mean and standard error for each environment, across five runs.
Figure 7: a) Zero-shot performance on a large procedurally-generated maze environment. Agents are evaluatedfor 100 episodes, bars show mean and standard error. ACCEL achieves over double the success rate of the nextbest method, despite beginning with empty rooms. b) Example levels produced by each UED algorithm.
Figure 8: Performance of ACCEL whenusing the Uniform DR generator, com-pared to DR and PLR. We test all agentsduring training on the same DR distribu-tion. Curves show mean with sem shaded.
Figure 9: Levels generated by ACCEL. Note that in all cases, each individual level along the evolutionary pathis at the frontier for the student agent at that stage of training. As we can see, the edits compound to produce aseries of challenges: in the first level the lava gradually surrounds the agent, such that they can initially explorein multiple directions but at the end the task can only be solved by going down and to the right. In the middlerow we see a level where the agent always has a direct run at the goal, but a corridor is evolved over time tobecome increasingly narrow, before being filled in so the agent has to go around. Finally in the bottom row thelevel begins with simple augmentations before moving the agent behind a barrier, which results in a challengingtask where the agent has to move in a diagonal direction to escape the lava.
Figure 10: A single level evolved in the MiniGrid environment, starting from top left, ending bottom right.
Figure 11:	Maze evolution with the DR generator. Top row shows starting DR levels, originally included inthe replay buffer due to having high positive value loss. After many edits (up to 40), they produce the bottomrow, which were all chosen to be in the highest 50 replay scores after 10k gradient steps. As we see, the sameDR level can produce distinct future levels, in some cases multiple high regret levels.
Figure 12:	The Evolving Frontier. The top row shows four levels from the same lineage, at generations 27, 45,53 and 63. Underneath each is a bar plot showing the Return and Positive Value Loss (PVL) for four differentACCEL policies, saved after 5k, 10k, 15k and 20k updates. At generation 27, all four checkpoints can solve thelevel, but the 5k checkpoint has the highest learning potential (PVL). On the right we see that by generation 63,only the 15k and 20k checkpoints are able to achieve a high return on the level.
Figure 13: Aggregate metrics for each band of generations. For example, “30-45” refers to all the levelsbetween generation 30-45. The later generation levels are harder for the early agents to solve, while the earlyagents have higher return and PVL for the earlier levels.
Figure 14: How do complexity metrics relate to difficulty? The plot shows the block count and shortest pathlength. From left to right we evaluate the agents at four checkpoints: 5k, 10k, 15k, 20k PPO updates. The colorrepresents the solved rate. As we see, the 5k agent is unable to solve the levels with higher block count andlonger paths to the goal, while the 20k agent is able to solve almost all levels.
Figure 15: Test performance, both in distribution (Empty, 10 and 20 Tiles) and out of distribution (CrossingS9N1). Each evaluation is conducted for 100 trials. Plots show the mean and standard error across five runs.
Figure 16:	MiniGrid Zero-Shot Environments. Those with an asterisk are procedurally generated: For Smalland Large Corridor, the position of the goal can be in any of the corridors, for SimpleCrossing and Four Roomssee Chevalier-Boisvert et al. (2018) and for PerfectMaze see Jiang et al. (2021a).
Figure 17:	Probability of improvement of ACCEL vs. PLR across all the benchmark environments in Figure 16,using the open source notebook fromm Agarwal et al. (2021b). The probability of improvement represents theprobability that Algorithm X outperforms Algorithm Y on a new task from the same distribution.
Figure 18: PerfectMazesXL. A 101x101 procedurally-generated MiniGrid environment. The agents have totransfer zero-shot from training in a 15x15 grid. This environment is challenging even for humans, since theagent only has a partially observable view, it requires memorizing the current location at all times to ensureexploring all corners of the grid.
Figure 19: Small grid results: a) On the left, we compare training with DR, randomizing obstacles that canbe walls or lava, on the right we see that when the obstacles are lava the choice of DR parameterization has asignificant impact on performance. In b) we see the test performance of these agents after 1k updates, PLRapplied to the Binomial (Bin) levels is able to produce a strong agent, why? In c) we see a density plot of DRand PLR levels, showing frequency by Return and Positive Value Loss, which shows PLR curates solvablelevels where the agent receives a learning signal.
Figure 20: Performance of agents trained on different level distributions when tested on each of the distributions.
Figure 21: Replay buffer diversity vs. return in the lava environment. On the left we show the concentrationof the replay buffer, measured as the percentage of the top 100 high-regret levels that can be produced by justten parents. On the right we compare the average return on ten-tile test environments. “Small” corresponds to abuffer of size 4k, with no generator, while “Large” indicates a buffer of size 10k, using a generator 10% of thetime.
