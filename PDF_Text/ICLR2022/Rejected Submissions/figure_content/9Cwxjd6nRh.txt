Figure 1: a) In distribution conditional image generation. An image from ImageNet validation set(first column) is used to compute the representation output by a trained SSL model (Dino). Therepresentation is used as conditioning for the diffusion model. Resulting samples are shown in thesubsequent columns (see Fig. 12). We observe that our conditional diffusion model produces samplesthat are very close to the original image. b) Out of distribution (OOD) conditional. How well doesRCDM generalize when conditioned on representations given by images from a different distribution?(here a WikiMedia Commons image, see Fig. 13 for more). Even with OOD an conditionning,the images produced by RCDM are very close visually to the original image. c) Interpolationbetween two images from ImageNet validation data. We apply a linear interpolation between theSSL representation of the images on the first column and the representation of the images on the lastcolumn. We use the interpolated vector as conditioning for our model that produce the samples thatare showed in column 2 to column 6. Fig. 16 in appendix shows more sampled interpolation paths.
Figure 2: a) Table of results on ImageNet. We compute the FID (Heusel et al., 2017) and IS (Salimanset al., 2016) on 10 000 samples generated by each models with 10 000 images from the validationset of ImageNet as reference. KDE* means that we used our unconditional representation samplingscheme based on KDE (Kernel Density Estimation) for conditioning IC-GAN instead of the methodbased on K-means introduces by Casanova et al. (2021). b) Table of ranks and mean reciprocal ranksfor different encoders. This table show that RCDM is faithful to the conditioning by generatingimages which have their representations close to the original one.
Figure 3: On the first and second row, RCDM samples conditioned on the usual backbone representa-tion (size 2048) and projector representation (size 256) with Dino. Same on the third and forth rowwith SimCLR representations (2048 and 128). For comparison, we also added samples from RCDMtrained on representation given by a supervised trained model. We clearly observe that the projectoronly keeps global information and not its context, as opposed to the backbone. This indicates thatinvariances in SSL models are mostly achieved in the projector representation, not the backbone.
Figure 4: We use our conditional generative model to get insight on the invariance (or covariance) ofrepresentations with respect to several data augmentations. On an original image (top left) we applyspecific transformations (visible in the first column). For each transformed image, we compute theSSL representation with Dino, SimCLR and a supervised network and condition the correspondingRCDM with that representation to sample 3 images. We see that despite their invariant trainingcriteria, the 2048 dimensional SSL representations appear to retain information on object scale,grayscale vs color, and color palette of the background, much like the supervised representation.
Figure 5: Visualization of adversarial attacks with RCDM. We use Fast Gradient Sign to attacka given image (top-left corner) with various values for the attack coefficient epsilon. On the firstrow, we only show the adversarial images obtained from a supervised encoder: refer to Fig. 30 inthe Appendix to see the (similar looking) adversarial examples for each model. On the followingrows, we show the reconstruction of the attacked images with RCDM for various models. For asupervised representation, RCDM reconstructs an animal that belong to another class, a lion in thiscase. However, we observe that if we use SimCLR or Swav as encoder (third and forth row), theimages generated by RCDM are still dogs even with higher values for epsilon.
Figure 6: Visualization of direct manipulations over the backbone representation space. In thisexperiment, we find the most common non zero dimension across the neighborhood of the imageused as conditioning (top-left dog). On the second row, we set these dimensions to zero and useRCDM to decode the truncated representation. We observe that RCDM produces examples with avariety of clothes meaning that all information about the background and the dog is removed. In thethird and forth row, instead of setting the most common dimensions to zero, we set them to the valueof the corresponding dimension in the representation associated to the image on the left. As we cansee, the corresponding dog get various clothes which were not present in the original image.
Figure 7: Gradient based samples from S (h). Leftmost image x is used to obtain target SSLrepresentation h (2048 dimensions) with either a standard DINO-ResNet50 (Dino) or one trainedwith additive noise as extra augmentation (Dino+n). A random initialized input is moved so that itsrepresentation will match h, by minimizing either L2 or cosine distance using Adam or L-BFGSrespectively (indicated in column headers). We display the samples x(T) obtained after T = 10, 000iterations obtained from the respective optimizers and distances, in all cases starting from a randomGaussian image as x(0). These x(T) have the same SSL representation as x or very close (relativedistance 0.4%, 0.1%, 3.6%, 3.3% respectively, see details in appendix Tab. 1,2,3), but do not resemblenatural images. Samples obtained from Dino+n look slightly more natural: we distinguish faint edgessimilarly shaped to the original. Similar experiments but applied on the lower-dimensional projectionhead embeding are reported in Figure 9 in appendix.
Figure 8: Depiction of four rows of the Jacobianmatrix Jf (x) for the input x given in Figure 7,with f being a Resnet50 trained with standardDINO (left) and additive noise (DINO+n) (right).
Figure 9: Reprise of Fig. 8 but now when considering the mapping to be the resnet50 backbone andthe projection head of DINO. Top row is when using DINO+noise and the bottom row is when usingstandard DINO. As was the case when using the resnet50 backbone only, we do not obtain realisticinputs from S(x) when following gradient directions.
Figure 10: Depiction of 36 rows of the Jacobian matrix of a trained DINO model that either employedGaussian noise on the images during training (top) or did not (bottom). Clearly the use of noiseduring training produce Jacobian matrices with more natural patterns.
Figure 11: (a) Illustration of considered image generation methods. A real input x yields representa-tion h. All methods start from a random noise image x(0) . Gradient-based representation matching(light blue arrows) will move it towards S(h) i.e. until its representation matches h, but won’t landon the data-manifold M. Unconditional reverse diffusion (ADM model, green arrows) will move ittowards the data manifold. Our representation-conditioned diffusion model (RCDM, red arrows)will move it towards M ∩ S (h), yielding a different natural-looking image with the same givenrepresentation. (b) Representation-Conditionned Diffusion Model (RCDM). From a diffusion processthat progressively corrupts an image, the model learns the reverse process by predicting the noise thatit should remove at each step. We also add as conditioning a vector h, which is the representationgiven by a SSL or supervised model for a given image c. Thus, the network is trained explicitly todenoise towards a specific example given the corresponding conditioning. The diffusion model usedis the same as the one presented by Dhariwal & Nichol (2021) with the exception of the conditioningon the representations.
Figure 12: Generated samples from RCDM on 256x256 images trained with representations pro-duced by Dino. We put on the first column the images that are used to compute the representationconditioning. On the following column, we can see the samples generated by RCDM. It is worth todenote our generated samples are qualitatively close to the original image.
Figure 13: Generated samples from RCDM model on 256x256 images trained with representationsproduced by Dino on Out of Distribution data. We put on the first column the images that are used tocompute the representation. On the following column, we can see the samples generated by RCDM.
Figure 14: High resolution samples from our conditional diffusion generative model using the superresolution model of Dhariwal & Nichol (2021). We use the small images on the top of each biggerimage as conditioning for a diffusion model trained with Dino representation on 128x128 images.
Figure 15: Each vectors that result from the linear interpolation is feed to a RCDM trained withBarlow Twins representation.
Figure 16: Diversity of the samples generated by RCDM on interpolated representations. Each rowcorresponds to different random noise for the same conditioning. On the first and last column are thereal images used for the interpolation. All of the images in-between those rows are samples fromRCDM.
Figure 17: We find the nearest neighbors in the representation space of samples generated by RCDM.
Figure 18: After generating samples with respect to a specific conditioning, we compute back therepresentation of the generated samples and find which are the closest neighbors in the validationset. Then, we compute the rank of the original image that was used as conditioning within the setof neighbors. When the rank is one, it implies that the nearest neighbors of the generated samplesis the conditioning itself, meaning that the generated samples have their representation that is veryclose in the representation space to the one used as conditioning. We can see that for SimCLR,the generated samples are much closer in the representation space to their conditioning than thesupervised representation. This is easily explain by the fact that supervised model learn to mapimages from a same class toward a similar representation whereas SSL models try to push furtheraway different examples.
Figure 19: Visual analysis of the variance of the generated samples for a specific image when using asupervised encoder. The first image (in red) in the one used as conditioning.
Figure 20: Visual analysis of the variance of the generated samples for a specific image when using aSimCLR encoder. The first image (in red) in the one used as conditioning.
Figure 21: Unconditional generation following the protocol of section C. Our simple generativemodel of representations consists in applying a small Gaussian noise over representation computedfrom random training images of ImageNet. We use these noisy vector as conditioning for our 256x256RCDM trained with Dino representations. We note that the generated images looks realistic despitesome generative artefact like a two-headed dog and an elephant-horse.
Figure 22: Squared Euclidean distances in the Dino representation space. We show the squaredeuclidean distance between the conditioning image on the leftmost column on first row and differentimages to get an insight about how close the samples generated by the diffusion model stay close tothe representation used as conditioning. The distances with the conditioning is printed below eachimages. On the first row, we show random images from the ImageNet validation data. On the secondrow, we take random validation examples belonging to the same class as the conditioning. On thethird row, we find the closest training neighbors of the conditioning in the representation space. Onforth row, D.A. means Data Augmentation which consist in horizontal flip, CenterCrop, ColorJitter,GrayScale and solarization. On fifth row (D.A. 2), we use the random data Augmentation used inthe paper of (Caron et al., 2020; 2021). On the last row, we show the generated samples from ourconditional diffusion model that use Dino representation. The samples produces by our model aremuch closer to the conditioning than other images.
Figure 23: Squared Euclidean distances in the SimCLR projector head representation space.Weshow the squared euclidean distance between the conditioning image on the leftmost column on firstrow and different images to get an insight about how close the samples generated by the diffusionmodel stay close to the representation used as conditioning. The distances with the conditioning isprinted below each images. On the first row, we show random images from the ImageNet validationdata. On the second row, we take random validation example belonging to the same class asthe conditioning. On third row, we find the closest training neigbords of the conditioning in therepresentation space. On forth row, D.A. means Data Augmentation which consist in horizontal flip,CenterCrop, ColorJitter, GrayScale and solarization. On fifth row (D.A. 2), we use the random dataAugmentation used in the paper of (Caron et al., 2020; 2021). On the last row, we show the generatedsamples from our conditional diffusion model that use SimCLR projector head representation.
Figure 24: Comparison of the euclidean distance between IC-GAN and RCDM. We use the sameself-supervised representation as conditioning (Swav encoder) for RCDM and IC-GAN. We computethe euclidean distance between the representation of the generated images versus the representationused as conditioning. We observe that samples of RCDM are much closer in the representation space(and also visually) to the conditioning. Samples of IC-GAN show a higher variability, thus fartheraway in the representation space.
Figure 25: Generated samples from RCDM trained with representation from various self-supervisedmodels. The image used for conditioning is a baby kangaroo on the top row, left-most column. Then,we generate 9 samples for each model with different random seed. We observe that the representationgiven by dino isn’t very invariant while the one given by SimCLR or VicReg show much betterinvariance. We also show the samples of RCDM trained on the representation given by the projector(The embedding on which is usually applied the SSL criterion). There is a much higher variability inthe generated samples. Maybe too much to be used for a classification task since we can observeclass crossing.
Figure 26: Generated samples from RCDM trained with representation from various self-supervisedmodels. We generate 9 samples for each model with different random seeds. We observe that therepresentation given by dino isn’t very invariant while the one given by SimCLR or VicReg showmuch better invariance. We also show the samples of RCDM trained on the representation given bythe projector (The embedding on which is usually applied the SSL criterion). There is a much highervariability in the generated samples. Maybe too much to be used for a classification task since we canobserve class crossing.
Figure 27: We compare how much the samples generated by RCDM change depending on differenttransformations of a given image and the model and layer used to produces the representation. Tophalf uses 2048 representation. Bottom half uses the lower dimensional projector head embedding. Weobserve that using the projector head representation leads to a much larger variance in the generatedsamples whereas using the traditional backbone (2048) representation leads to samples that are veryclose to the original image. We also observe that the projector representation seems to encode objectscale, but contrary to the 2048 representation, it seems to have gotten rid of grayscale-status andbackground color information.
Figure 28: Same setup as Figure 27 except with other imagesconditioning.
Figure 29: Generated samples from RCDM using the mean representation for a specific class (goldenretriever) in ImageNet for various SSL models.
Figure 30: Visualization of adversarial examples We use RCDM to visualize adversarial examplesfor different models. For each model, we trained a linear classifier on top of their representations topredict class labels for the ImageNet dataset. Then, we use FGSM attack over the trained model usinga NLL loss to generate adversarial examples towards the class lion. For each model, we visualizeadversarial examples for different values of which is the coefficient used in front of the gradientsign. In the supervised scenario, even for small values of epsilon which doesn’t seem to change theoriginal image, the decoded image as well as the predicted label by the linear classifier becomes alion. However it’s not the case in the self-supervised setting where the dog still get the same class orget another breed of dog as label until the adversarial attack becomes more visible to the human eye(For value superior to 0.5).
Figure 31: Background suppression and addition Visualization of direct manipulations over therepresentation space. On the first row, we used the full representation of the dog’s image on thetop-left as conditioning for RCDM. Then, we find the most common non zero dimension acrossthe neighborhood of the image used as conditioning. On the second row, we set these dimensionsto zero and use RCDM to decode the truncated representation. We observe that RCDM producesexamples of the dog with a high variety of unnatural background meaning that all information aboutthe background is removed. In the third and forth row, instead of setting the most common nonzero dimension to zero, we set them to the value of corresponding dimension of the representationassociated to the image on the left. As we can see, the original dog get a new background and a newpose.
Figure 32: Same setup as Figure 31 except that instead of using the most common non zero-dimensions as mask, we used the least common non-zero dimensions as mask. On the second row,we observe that some information about the original dog is removed such that in each column, we geta slightly different breed of dog while the background stay fixed. On the third and forth row, we sawthat the information about the background (grass) is propagated through the samples (which was notthe case in Figure 31).
Figure 33: Algebraic manipulation of representations from real images (left-hand side of =) allowsRCDM to generate new images with novel combination of factors. Here we use this technique withImageNet images, to attempt background substitutions.
Figure 34: Different samples of RCDM conditioned on a satellite image of the earth (source: NASA).
