Figure 1: Split MNIST experiment. (a). Average test accuracy on the split MNIST experiment. Allthe approaches are tested in Class-IL scenario except XdG which performs Task-IL scenario. (b).
Figure 2: (a). Average test accuracy on the split CIFAR100 experiment. All the approaches are testedin Class-IL scenario except XdG which perfroms Task-IL scenario. (b). Forgetting matrix of themodels in the CIFAR100 experiments. In the forgetting matrix, the value at ith column and jth rowis the forgetting rate of task i after learning task j .
Figure 3: MNIST experiment with and without multi-views augmentation. TCNN learns five differenttasks sequentially without having the task information during both training and testing state. Theblocks at the bottom represents different state. Blue blocks: The model is tested on the learnedtasks. Green blocks: The model is learning the current task. Red blocks: The model is tested onthe unseen tasks. The red dashed lines are task boundaries. The blue shadows represent the area ofmean(tlh) Â± std(tlh), where tlh is the highest task likelihood generated by the model for the giveninput. When a change is detected, TCNN adapts to the new task automatically without forgetting theprevious ones.
Figure 4: Difference between the extracted samples distribution and the kernels in the probabilisticlayers. We use the hidden layers of the first expert to extract the features of the samples from task 1and 2 respectively. The task likelihood is high if the two distributions are overlapped (e.g. Figure 4a,where samples are from Task 1, kernels are from: expert 1). The task likelihood will be decreased ifthey are different (e.g. Figure 4b, where samples are from Task 2, kernels are from: expert 1).
Figure 5: (a). Number of parameters used in CIFAR100 . (b). Supplementary split mnist experimentsfor reducing complexity of the TCNN. The TCNN_progressive and TCNN_share are trained bythe Equation 7 and 8 respectively. Overall, by using progressive training and parameter sharingtechniques, the performance of the model does not degrade. The accuracy fluctuates around thebaseline TCNN model.
