Figure 1: Domains of the DM Control Suite (Tassa et al., 2018) used in this paper. (a) cart-pole, (b) finger, (c)ball in cup, (d) walker, and (e) humanoid. These domains include various challenges like high-dimensionality,non-rigid strings and multiple contacts.
Figure 2: The learning curves comparing deterministic and stochastic models for five seeds on the five testdomains. The shaded area marks the 20/80 percentile. For all systems except the humanoid, the stochasticand deterministic ensembles learn a good model that enables successful planning. The obtained reward ismarginally below the reward obtained by planning with the true model. For the humanoid both approaches fail.
Figure 3:	The learning and loss curves averaged across five seeds for different horizons of the multi-step loss.
Figure 4:	The learning curves averaged over five seeds for different number of ensembles. The shaded areahighlights the 20/80 percentile. The obtained reward increases when increasing the number of ensembles. Forstochastic ensembles the increase in performance is higher compared to deterministic ensembles. Without anensemble both models do not fail at the task and only obtain a lower reward.
Figure 5: The learning curves averaged over 5 seeds for different levels of input noise. The shaded areahighlights the 20/80 percentile. For deterministic models the input noise is not required to achieve a highreward. For stochastic models the input noise significantly improves the reward for walker and ball in cup.
Figure 6: The reward discrepancy measuring the difference between the expected and observed reward. Ifthe discrepancy is positive the planner is too optimistic. If the value is negative the planner is too pessimistic.
Figure 7: The obtained reward for the four different DM control suite environments and 5 differentdatasets with different average reward. In most cases, the model-based planning approach obtains ahigher reward than the average dataset reward. Even when the dataset contains only random actions,i.e., dataset 0, the planner obtains a much higher reward than random actions.
Figure 8: The obtained reward when decreasing the replan frequency for the deterministic, stochasticand true model. The shaded area highlights the 20/80 percentile. For the different replan frequencies,the deterministic model obtains a better or comparable reward compared to the stochastic model.
Figure 9: The learning curves and loss curves for the multi-step loss using a schedule to linearly in-crease the horizon. The schedule cause the loss curve to start identical and spread once the scheduleincreases the horizon. Compared to training on the multi-step loss without a schedule, the trainingwith the schedule performs comparable or worse.
Figure 10:	The learning and loss curves for different time discretization averaged over 5 seeds.
Figure 11:	The learning and loss curves for the stochastic model with different time discretizationaveraged over 5 seeds. The shaded area highlights the 20/80 percentile. Similar to the deterministicmodel, the optimal time step depends on the domain. The optimal time steps are identical to theoptimal time steps of the deterministic model.
