Figure 1: Idealized model of the behav-ior of entropy and similarity for a phys-ical system for different changes ∆.
Figure 2: Iteration schemes to create data sequences of decreasing similarity. Variation from thereference state can be introduced via the initial conditions of a numerical PDE simulation (method[A], left), or via spatio-temporal data changes on data from a repository (method [B], right).
Figure 3: Example sequences from training sets(top, column-wise: Adv, 3×Smo, Bur, 3×Liq)and test sets (bottom, column-wise: 2×Iso,Cha, Mhd, Tra, AdvD, LiqN, SF, Sha, Wav).3random seeds. Furthermore, we use adjusted versions of the noise integration for two test sets, byadding noise to the density instead of the velocity field in the Advection-Diffusion model (AdvD),and overlayed noise to the background area of the liquid simulation (LiqN).
Figure 4:	MSE correlation histograms of training data (orange) and validation data (blue) with a binsize of 0.05. Each histogram roughly follows a truncated normal distribution.
Figure 5:	Standard Conv+ReLU blocks (left) are interwoven with input and resolution connections(blue dotted and red dashed), to form the combined network architecture (right) with about 350kweights. The output of each scale block is concatenated with the downsampled input for lowerscales, leading to features that are spread across multiple resolutions for a stable metric computation.
Figure 6: Combined validation and testperformance for different batch sizes band slicing values v (markers), and theusage of a running sample mean RM andcorrelation aggregation AG (colors).
Figure 7: Distance deviation from the mean prediction over different rotation angles (left) and scal-ing factors of the inputs (right) for a simple CNN and the proposed multiscale model.
Figure 8: Top: Analysis of forced isotropic turbulence across three time spans. The high SRCCvalues indicate strong agreement between Pearson’s distance and the similarity model (SRCCa), andbetween the similarity model and VolSiM (SRCCb). Bottom: Examples from the overall sequence,visualized via a mean projection along the x-axis and color-coded channels.
Figure 9:	Loss curves from the loss function analysis in Fig. 6 for different models. Shown are theraw losses per batch over all training iterations (lighter line), and an exponential weighted movingaverage with α = 0.05 (darker line) for better visual clarity.
Figure 10:	Example sequences of simulated training data, where each row features a full sequencefrom a different random seed.
Figure 11: Example sequences of simulated (top two data sets), collected (middle data set), andgenerated (bottom two data sets) test data. Each row contains a full sequence from a differentrandom seed.
Figure 12: Example sequences of collected test data from JHTDB, where each row shows a fullsequence from a different random seed. Notice the smaller cutout scale factor s for the middleexample in each case. The predominant x-component in Cha and is separately normalized for amore clear visualization.
