Figure 1: Examples ofKandinsky patterns. (a) fol-lows the pattern: ”the figurehas 2 pairs of objects with thesame shape”, but (b) isn’tobject-centric reasoning tasks. The key idea is to combine neural-based object-centric learningmodels with the differentiable implementation of first-order logic. It has three main components: (i)object-centric perception module, (ii) facts converter, and (iii) differentiable reasoning module. Theobject-centric perception module extracts information for each object and has been widely addressedin the computer vision community (Locatello et al., 2020a; Redmon et al., 2016). Facts converterconverts the output of the visual perception module into the form of probabilistic logical atoms,which can be fed into the reasoning module. Finally, differentiable reasoning module performsthe differentiable forward-chaining inference from a given input. It computes the set of groundatoms that can be deduced from the given set of ground atoms and weighted logical rules (Evans &Grefenstette, 2018; Shindo et al., 2021). The final prediction can be made based on the result of theforward-chaining inference.
Figure 2: An overview of NSFR. The object-centric model produces outputs in terms of objects. Thefacts converter obtains probabilistic facts from the object-centric representation. The differentiableforward-chaining inference computes the logical entailment softly from the probabilistic facts andweighted rules. The final prediction is computed based on the entailed facts.
Figure 3: An overview of the facts-converting process. NSFR decomposes the raw-input imagesinto the object-centric representations (left). The valuation functions are called to compute theprobability of ground atoms (middle). The result is converted into the form of vector representationsof the probabilistic ground atoms (right).
Figure 4: The inference timewith different batch sizes.
Figure 5: Training examples in each Kandinsky data set. The left three images are positive examples,and the right three images are negative examples.
Figure 6: Examples in the CLEVR-Hans data set. CLEVR-Hans3 has three classes and CLEVR-Hans7 has seven classes, respectively. Each example represents each class in the data set.
Figure 7: The statistics of the pre-training data set in Kandinsky Patterns tasks. The distribution ofthe class label (left) and the distribution of the position of the objects (right). The class labels andthe positions are generated randomly.
Figure 8: The confusion matrix of the YOLOv5 model in the test split after the pre-trainingtrained YOLOv5 model classifies the objects in Kandinsky figures correctly.
Figure 9: Curriculum learning andreasoning in NSFR.
Figure 10:	The visualization of various or functions. The maximum and minimum values for eachimage is shown on top. The softor dγ function with a sufficiently small smooth parameter approxi-mates the logical or function for probabilistic values.
Figure 11:	The visualization of the difference between the original or function and other or func-tions, i.e., probabilistic sum and softor γd . The maximum and minimum values in each image isshown on top. The softor dγ function with a sufficiently small smooth parameter approximates thelogical or function for probabilistic values.
