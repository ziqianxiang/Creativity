Figure 1: Our stability CertifiCate implies a modularity prinCiple. It may be used to reCursivelyConstruCt CompliCated ’networks of networks’ while automatiCally maintaining stability.
Figure 2: Summary of network architectures. Nonlinear subnetworks are constrained to meet eitherTheorem 1 via sparse initialization (A) or Theorem 5 via direct parameterization (B). Linear negativefeedback connections are trained between the subnetworks according to 6. See Section A5 foradditional details on model definitions.
Figure 3: Example 3 × 16 Sparse Combo Net. The hidden-to-hidden nonlinear weights (W inTheorem 6) are initialized based on a set sparsity, and do not changed over training (A). The linearinter-subnetwork connections (L in Theorem 6) are constrained to be antisymmetric (with respectto the overall network metric), and are updated in the training process (B).
Figure 4: Permuted seqMNIST performance over the course of training for two 11 × 32 SparseCombo Nets with different sparsity levels.
Figure 5: Permuted seqMNIST performance plotted against the number of subnetworks. Each sub-network has 32 neurons. Results are shown for both Sparse Combo Net and SVD Combo Net.
Figure S1:	Weight matricesfor each of the 32 unit non-linear component RNNs thatwere used in the best perform-ing 16 × 32 network on per-muted sequential MNIST.
Figure S2:	Performance of Sparse Combo Nets on the Permuted seqMNIST task by combinationnetwork size. We test the effects on final and first epoch test accuracy of both total network sizeand network modularity. The former is assessed by varying the number of subnetworks while eachsubnetwork is fixed at 16 units (A), and the latter by varying the distribution of units across differentnumbers of subnetworks with the total sum of units in the network fixed at 352 (B). Note that theseexperiments were run prior to optimizing the sparsity initialization settings. Experiments on totalnetwork size were later repeated with the final sparsity settings (Figure 5). The results of both thesize experiments are consistent.
Figure S3:	Permuted seqMNIST performance by component RNN initialization settings. Test ac-curacy is plotted over the course of training for four 16 × 32 networks with different density levelsand entry magnitudes (A), highlighting the role of sparsity in network performance. Test accuracyis then plotted over the course of training for two 3.3% density 16 × 32 networks with differententry magnitudes (B), to demonstrate the role of the scalar. When the magnitude becomes too highhowever, performance is out of view of the current axis limits.
Figure S4: Permuted seqM-NIST performance on re-peated trials. Four differ-ent 16 × 32 networks with3.3% density and entries be-tween -6 and 6 were trained.
Figure S5: seqCIFAR10 performance on repeated trials. Three different 16 × 32 networks with3.3% density and entries between -6 and 6 were trained for 200 epochs, with learning rate dividedby 10 after epochs 140 and 190. (A) depicts test accuracy for each of the networks over the courseof training. (B) depicts the training loss for the same networks.
Figure S6: seqCIFAR10 performance on repeated trials with shorter training (done to complete moretrials). Nine different 16 × 32 networks with 3.3% density and entries between -6 and 6 were setup to train for 150 epochs, with learning rate divided by 10 after epochs 90 and 140. Most of thesenetworks hit runtime limit before completing, however they all got through at least 100 epochs andall had test accuracy exceed 61%. This figure depicts test accuracy for each of the networks over thecourse of training.
Figure S7:	Pytorch Lightning code for Sparse Combo Net Cell30Under review as a conference paper at ICLR 2022ΛtJ¢fine nβtwor*class E>⅜ss⅛⅜blv SV(Lightnin⅜⅛todule∙):Pytorch nodule +ortau∙dx∕dt * -MWh^re Lau > 0t p*)ltraining tte folioMlng SySte■:+ Wphi(x) ♦ L∙m+ u(t)IS a nonllneðrɪt/, H H block diagonal, L 1& icme 'contracting' CCabinatlOh ∣utrix ⅞nd u is ⅛onβ input.
Figure S8:	Pytorch Lightning code for SVD Combo Net cell.
Figure S9: Detailed architecture diagrams for Sparse Combo Net (A) and SVD Combo Net (B).
