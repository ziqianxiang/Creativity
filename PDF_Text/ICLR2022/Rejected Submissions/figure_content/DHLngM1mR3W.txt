Figure 1: AAVAE: The input to the model is an augmented view of x+ 〜A(x), the target is theoriginal input x. The loss is the reconstruction term of the ELBO (Eq. 3) without the KL-divergence.
Figure 2: The AAVAE uses a Gaussian likelihood on pixels for the reconstruction loss with a speci-fied width of the distribution (logscale). In (a), we let the decoder learn the logscale and observe theillusion of overfitting as mentioned in Mattei & Frellsen (2018). In (b), we fix the logscale parameterto an arbitrary scalar by sampling uniformly between [-5, 2]. In both cases, we fail to observe anycorrelation between the quality of density estimation and learned representation. Plots shown forCIFAR-10 (Krizhevsky & Hinton, 2009) dataset.
Figure 3: Downstream classification accuracy on CIFAR-10 (Krizhevsky & Hinton, 2009) whenwe add back KL divergence based regularization with a β-coefficient (Higgins et al., 2016) to theloss function of AAVAE defined in Eq. 3. We observe a negligible change in the quality of rep-resentations, as measured by the classification task, when the KL-term is weighted with a β 1.
Figure 4: On CIFAR-10 (Krizhevsky & Hinton, 2009), we demonstrate AAVAEs insensitivity to hy-perparameters: (a) batch size, (b) latent space dimension, (c) decoder architecture, and (d) logscaleparameter (width of the Gaussian likelihood). We vary one specific hyperparameter while keep-ing the rest fixed for these insensitivity ablations. We select the minibatch size between 128-1024,the dimensionality of the latent space between 64-512, the decoder architecture from decoders thatmirror {resnet18, resnet34 or resnet50} encoders, and sample the logscale values from a uniformdistribution between [-5, 2].
Figure 5: Part (a) shows cosine similarity matrices between pairs of vectors produced by views of thesame example and between pairs of vectors produced by views of different examples. We observea posterior collapse in the case of VAEs in (a)(i). For AAVAEs in (a)(ii), we see strong alignmentbetween views of the same example while the views of different examples are far apart from eachother in the representation space. In (b), we show images from the STL-10 dataset (Coates et al.,2011) and their corresponding perturbed versions that generate the cosine similarity matrices in (a).
