Figure 1: Left. ProPortion of oPtimal and suboPtimal goalarrivals. Our method has a marked inflection (arrow) wherearrivals at the sub-oPtimal goal decrease and arrivals at the oP-timal goal increase. Shaper has learned to guide Controllerto forgo the suboPtimal goal in favour of the oPtimal one.
Figure 2: Discovering subgoals on SubgoalMaze. Left. Learning curves. Right. HeatmaPof shaPing rewards guiding Controller to gate.
Figure 3: Red-Herring Maze. Ignoring non-beneficial shap-ing reward. Left. Learning curves. Right. Heatmap of addedshaping rewards. ROSA ignores the RHS of the maze, whileRND incorrectly adds unuseful shaping rewards there.
Figure 4: Benchmark performance.
Figure 5: Ablation ExperimentsOriginOptimal0123456Suboptimal-Standard Policy P1(b) Responsiveness to Controller policies24612 Ablation StudiesOur reward-shaping method features a mechanism to selectively pick states to which intrinsic rewardsare added. It also adapts its shaping rewards according to Controller’s learning process. In thissection, we present the results of experiments in which we ablated each of these components. In
Figure 6: Performance of ROSA compared with the exploration bonus replaced by count-basedmethod.
Figure 7: Performance of ROSA compared with different values of φ parameters.
Figure 8: ROSA is robust to the component used to generate exploration bonus L. ROSA worksequally well when we RND or Count-based method forL.
Figure 9: ROSA is robust to the component used to generate exploration bonus L. ROSA worksequally well when we RND or Count-based method forL.
