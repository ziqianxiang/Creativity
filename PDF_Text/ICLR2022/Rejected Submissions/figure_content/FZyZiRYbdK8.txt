Figure 1: (a) Architecture for end-to-end training of the proposed DRL framework (training detailsin Sec. 2.3). (b) Example images for category ‘Train’ in VisDA. The estimated density ratios for theeasy and hard target images are shown, respectively. The DRL framework gives higher uncertainpredictions for the harder example (x2) that is more cluttered and hence not well-represented in thesource domain.
Figure 2: Formulation of the pseudo labelbased UDA or SSL methods with DRL. Theunsupervised losses represent the loss functionuser can impose on the unlabeled target data.
Figure 3: Density ratios vsHSF on ImageNet V2.
Figure 4: Brier score (upper) and reliability diagrams (lower) on Office31, Office-Home and VisDA.
Figure 5: (a)-(b) Main results on VisDA-17 (5 random seeds) with the test accuracy and Brierscore. The results show that DRST outperforms the baselines significantly. (c) Distribution gapPs(φ(x))∕P(φ(x)) - PS(φ(x),y)/P(Φ(x),y) as a proxy of covariate shift. DRST helps tofurther reduce this gap over DRL with self-training. (d) Accuracy vs. estimated weights. Theimprovement from DRST increases on harder examples (lower weights).
Figure 6: Model attention visualized using Grad-Cam (Selvaraju et al., 2017). We also show thepredicted labels by different methods. Our method captures the shape features of the image better.
