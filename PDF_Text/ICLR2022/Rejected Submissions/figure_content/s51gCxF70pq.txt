Figure 1: Depiction of KSL’s architecture. Online and momentum paths are highlighted in orangeand green, respectively. Recurrent operation shown with red arrow. EMA updates shown withdashed arrows. Gradient flow shown with dotted, purple arrows. Stop gradient (sg) shown with //.
Figure 2: Episodic evaluation returns for agents trained in DMControl. Mean as bold line and ± onestandard deviation as shaded area.
Figure 3: PCA projections of latent representations learned by KSL (top row) and DrQ (bottomrow) at 5k (left column), 10k (middle column), and 15k (right column) environment steps. Colorsindicate the reward received in the original state.
Figure 4: MSE of a linear regression fitted to estimate reward from latent representations. The rightplot re-scales the y-axis to show the difference between non-SAC-Pixel methods. Lower is better.
Figure 5: `2 distance between latent representations of augmented states. From left-to-right: randomtranslate, random erase, Gaussian noise.
Figure 6: `2 distance between consecutive latent states that are produced by the evaluation encoders.
Figure 7: Episodic evaluation returns for KSL, PI-SAC, and PRL agents trained in PlaNet bench-mark suite. Mean as bold line and ± one standard deviation as shaded area. Purple and green dottedlines indicate when 50k/50k PRL and 250k/250k PRL, respectively, begin to receive task-specificrewards.
Figure 8: Depiction of options for knowledge sharing. Critic’s Q-functions shown on far left. KSL’stransition module T is shown on far right with its two components: knowlege sharing (KS) andtransition predictor (TP). Data-flow is shown with dashed lines.
Figure 9:	Grid search performance for KSL for k ∈ {1, 3, 5} (left to right) and l ∈ {0, 1, 2}. Meanperformance shown in bold and ± one standard deviation with shaded area.
Figure 10: Depiction of how KSL fits within the greater SAC algorithm.
Figure 11: Mean ± one standard deviation of evaluation returns for convolutional (conv) and fully-dense (no conv) transition model.
Figure 12: Evaluation curves for the encoder-generalization experiment.
Figure 13: Performance of agents on Cheetah, Run (left) and Walker, Walk (right) for scenario (i)(red) and (ii) (blue). Plots depict averages (bold line) and one standard deviation (shaded area) overfive runs.
Figure 14: φij for for all tasks in the PlaNet benchmark suite throughout training.
Figure 15: Depiction of the six tasks tasks in the PlaNet benchmark suite.
Figure 16: First two principal components of SAC Pixel.
Figure 17: First two principal components of RAD.
Figure 18: First two principal components of CURL.
Figure 19: First two principal components of SAC-AE.
Figure 20: Random erase augmentation.
Figure 21: Gaussian noise augmentation.
