Figure 1: Multi-decoder Manifold Regularized AutoencoderSingle Encoder - Multiple Decoders To achieve the properties outlined in the previous section,we propose a manifold-regularized common-encoder, multiple-decoder model (Figure 1). Thecommon encoder f (Yj,k) = Enc(Yj,k) takes any set of neuronal activities from any subject and mapsthem in a many-to-one fashion to the same latent space L, with the same stimulus prediction. Ourmodel features several decoders gj(f (Y∙,k)) = Dec[j](f (Y∙,k)) ≈ Yj,k, to decode back to subjectspecific responses. The rationale for using a common encoder is for the model to learn a commonfMRI language from which subject specific responses can be derived. While the fMRI data exhibitsstrong batch effect between subjects, the common encoder builds invariance in the latent spaceto individual noise in the encoded space. The individual decoders enable incorporating subject-dependent interpretation of the latent information from the common encoder. Thus this frameworkpushes individual noise into the decoding space. In particular, the decoders can learn to reconstructthe subject-specific signals and recreate batch effect to reconstruct data. The flexibility providedby individual decoders allow the single common encoder to only retain input-dependent variabilitybetween subject responses that is sufficient for the decoders to reconstruct the measurements. Thisbase model is trained with a reconstruction error penalty for each subject seen at training time.
Figure 2: Comparison of extended manifold by MRMD-AE, MR-AE and PHATE Landmark on theSherlock early-visual ROI. The MSE between extended manifold coordinates and ground truth werecomputed (lower is better). MRMD-AE extended manifold embeddings achieve highest classificationaccuracy in predicting movie stimulus labels. Error bars are standard deviation across subjects.
Figure 3: PHATE visualization of raw data and latent representation f (Yj), 1 ≤ j ≤ 15, coloredby subject. The raw data exhibits clear batch effect while the latent space is better aligned betweensubjects. The incorporation of higher levels of latent space alignment penalty by increasing λ in Eq. 1better removes batch effect in the shared space, demonstrated by decreasing EMD.
Figure 4: Cross subject translation MSE comparison. Combinations of latent space alignmentpenalty and translation penalty were used. MRMD-AE with better aligned latent space achieve bettertranslation MSE. We also trained the model explicitly for translation by incorporating the translationloss, achieving the best translation performance with the lowest translation MSE.
