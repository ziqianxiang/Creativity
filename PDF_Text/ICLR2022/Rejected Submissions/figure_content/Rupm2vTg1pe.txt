Figure 1: Differences between layer ',s graphical model of the original CGMM and the proposediCGMM. Observable variables are blue circles, latent ones are empty circles, and white boxesdenote prior knowledge. Each ICGMM is an HDP mixture model where the group j for each nodeobservation Xu is pre-determined by the set of states of neighboring nodes qN-1 computed at layer` - 1. Contrarily to CGMM, the number of values that the latent indicator variable qu can assumeis automatically adjusted to fit the underlying data distribution. Dashed arrows denote the flow ofcontextual information from previous layers through the neighbors of each node u.
Figure 2: Figures 2a and 2b analyze the relation between depth, performances, and the number ofchosen states on NCI1.
Figure 3: We show comparative results on the size and quality of graph embeddings between CGMMand ICGMM. Overall, ICGMM generates â‰ˆ 0 unused latent states, with consequent savings in termsof memory and compute time of the classifier with respect to CGMM. See the text for more details.
