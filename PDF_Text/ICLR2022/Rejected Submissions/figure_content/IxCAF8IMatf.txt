Figure 1: Illustration of DGM in four different Figure 2: Toy example of accumulated errorforms. Diamonds are deterministic variables and (KL divergence) between the teacher and stu-circles are random variables. (a) Original form; (b) dent for local distillation and our method. Ex-Auxiliary form; (c) Our semi-auxiliary form; (d) perimental settings are presented in the last para-Compact semi-auxiliary form.	graph in Section 3.2.
Figure 3: Illustration of different distillation methods. Each pair of conditional distributions markedwith same color represents an independent distillation component. (a) Distillation on vanilla neuralnetwork. (b) GAN distillation. (c) Distillation on a 2-layer fully-visible auto-regressive DGM. (d)Local distillation on the original DGM. (e) Our distillation method with semi-auxiliary form of DGM.
Figure 4: Comparison of FID for different methods in data-free VAE compression on three datasets:CelebA, CIFAR10, and SVHN. We compute the FID between the generated samples by student andground truth averaged over 3 random seeds. Teacher model size is fixed to 5.4M parameters.
Figure 5: Strokes generated by VRNNs. Different strokes are represented with different colors. (a)Ground truth dataset. (b) Teacher VRNN (12M parameters) trained from scratch. (c) Student VRNN(2.4M parameters, 5.0 times smaller) distilled by our method. (d) Student VRNN distilled using localdistillation. (e) Student VRNN trained from scratch.
Figure 6: DGMs used in our experiments. (a) 5-layer DGM without input variables. (b) 6-layer DGMwith one binary input variable indicating which domain the sample comes from. (c) DGM with asmany latent variables and target variables as the sequence length and no input variable. (d) 5-layerDGM with two target variables and no input variable. (e) 2-layer DGM with one input variable15Under review as a conference paper at ICLR 2022E DatasetsTo evaluate the performance of the proposed distillation method, we conduct experiments on thefollowing benchmark datasets.
Figure 7: FID between samples generated by the student and teacher averaged over 3 random seeds.
Figure 8: Images generated by VAE trained on CelebA dataset. (a) Ground truth dataset. (b) TeacherVAE (5.4M parameters) trained from scratch. (c) Student VAE (1.4M parameters, 3.9 times smaller)distilled by our method. (d) Student VAE distilled using local distillation. (e) Student VAE trainedfrom scratch.
Figure 9: Effect of Î» in Eq. (7) on the performance of our method.
Figure 10:	Generation distribution of HMs. We performed and plot kernel density estimation using272 generated samples for each HM. (a) Ground truth. (b) Teacher HM (354 parameters) trainedfrom scratch. (c) Student HM (102 parameters, 3.5 times smaller) distilled using our method. (d)Student HM distilled using local distillation. (e) Student HM trained from scratch.
Figure 11:	A toy example of accumulated error (KL divergence) between the teacher and student forlocal distillation and our method in a DMG with discrete random variables.
