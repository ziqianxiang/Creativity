Figure 1: (left) In offline meta-RL, an agent uses offline data from multiple tasks T1, T2, . . . , each with rewardlabels that must only be provided once. (middle) In online meta-RL, new reward supervision must be providedwith every environment interaction. (right) In semi-supervised meta-RL, an agent uses an offline dataset collectedonce to learn to generate its own reward labels for new, online interactions. Similar to offline meta-RL, rewardlabels must only be provided once for the offline training, and unlike online meta-RL, the additional environmentinteractions require neither external reward supervision nor additional task sampling.
Figure 2: Left: The distribution of the KL-divergence between the posterior qφe (z | h) and the prior p(z) overthe course of meta-training, when conditioned on data from the offline dataset (blue) or on online data from thelearned policy (orange). Data from the learned policy results in posteriors that are substantially farther from theprior, suggesting a significant difference in distribution over z. Note that online data from the learned policyis not available for meta-training, but only used for measurement. Right: The performance of the policy afteradaptation when adapted using data from the offline dataset (blue) or the learned policy (orange). Althoughthe meta-RL policy adapts well when conditioned on z sampled from the offline dataset, the performance doesnot increase when z is sampled from the learned policy. Since the same policy is evaluated, the change inz-distribution is likely the cause for the drop in performance.
Figure 3: (Left) In the offline phase, we sample a history h0 to compute the posterior qφe (z | h0). We thenuse a sample from this encoder and another history batch h to train the networks. In red, we then update thenetworks with h and the Z sample. (Right) During the self-supervised phase, We explore by sampling Z 〜p(z)and conditioning our policy on these observations. We label rewards using our learned reward decoder, andappend the resulting data to the training data. The training procedure is equivalent to the offline phase, exceptthat we do not train the reward decoder or encoder since no additional ground-truth rewards are observed.
Figure 4: We propose a new meta-learning evaluation domain based on the environment from Khazatsky et al.
Figure 5: We report the final return of meta-test adaptation on unseen test tasks versus the amount of self-supervised meta-training following offline meta-training. Our method SMAC, shown in red, consistently trainsto a reasonable performance from offline meta-RL (shown at step 0) and then steadily improves with onlineself-supervised experience. The offline meta-RL methods, MACAW Mitchell et al. (2021) and BOReL arecompetitive with the offline performance of SMAC but have no mechanism to improve via self-supervision. Wealso compare to SMAC (SAC ablation) which uses SAC instead of AWAC as the underlying RL algorithm. Thisablation struggles to train a value function offline, and so struggles to improve on more difficult tasks.
Figure 6: Example XY-coordinates visited by a learned policy on the Ant Direction task. Left: Immediatelyafter offline training, the post adaptation policy moves in many different directions when conditioned on hoffline(blue). However, when conditioned on honline (orange), the policy only moves up and to the left, suggesting thatthe post-adaptation policy is sensitive to data distribution used to collect h. Right: After the self-supervisedphase, the post-adaptation policy moves in many directions regardless of the data source, suggesting that theself-supervised phase mitigates the distribution shift between conditioning on offline and online data.
Figure 7: We duplicate Figure 6 but include the exploration trajectories (green) and example trajectories from theoffline dataset (red). We see that the exploration policy both before and after self-supervised training primarilymoves up and to the left, whereas the offline data moves in all direction. Before the self-supervised phase, wesee that conditioning the encoder on online data (orange) rather than offline data (blue) results in very differentpolicies, with the online data resulting in the post-adaptation policy only moving up and to the left. However, theself-supervised phase of SMAC mitigates the impact of this distribution shift and results in qualitatively similarpost-adaptation trajectories, despite the large difference between the exploration trajectories and offline datasettrajectories.
Figure 8: Learning curves when performing self-supervised training on the test environments (red) or themeta-training environments (blue). We also compare to an oracle that trains on test environments in combinationwith ground-truth rewards (black). We see that interacting with the test environment without rewards allows forsteady improvement in post-adaptation test performance and obtains a similar performance to meta-training onthose environments with ground-truth rewards.
Figure 9: Illustrations of two evaluation domains, eachof which has a set of meta-train tasks (examples shownin blue) and held out test tasks (orange). The domainsinclude (left) a half cheetah tasked with running at dif-ferent speeds and (right) a quadruped ant locomoting todifferent points on a circle.
