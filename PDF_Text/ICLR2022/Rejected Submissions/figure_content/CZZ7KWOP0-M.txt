Figure 1: Unit energy comparisons.
Figure 2: Supernets for NLP and CV tasks: (a) For NLP, we adopt a multi-branch structure for eachblock of the supernet, where Attn+Conv represents the channel-wise concatenation of these twoblocks, and (b) for CV tasks, we consider a multi-resolution pipeline for each block of the supernet.
Figure 4: (a) Illustration of the proposed heterogeneous weight sharing strategy, where weights ofshift blocks are quantized to powers of two; (b) visualization of the adopted learnable transformationkernel T(âˆ™) for mapping the shared weights of Gaussian distribution to a Laplacian distribution.
Figure 3: Heterogeneous weight distributions.
Figure 5: BLEU scores vs. FLOPs of ShiftAddNAS over SOTA baselines on NLP tasks.
Figure 6: Accuracy vs. energy costs of Shif-tAddNAS over baselines.
Figure 7: Visualization of the heterogeneous weight distributions in Conv/Add/Shift layers.
Figure 8: Visualization of the searched architecture with 82.8% top-1 test accuracy on ImageNet.
