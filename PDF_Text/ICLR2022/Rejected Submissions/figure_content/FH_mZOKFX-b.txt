Figure 1: Approximation comparison experiments in small-scale setting. (a) shows equality ofTr(H) and Tr(F). (b), (c), and (d) respectively compare how different the approximation methodof TIC estimation is from the exact case. All full results are shown in Appendix D.2.2.
Figure 2:	Correlation between the generalization gap and the TIC estimates. (a) is a problemsetting outside the NTK regime, where the correlation between TIC and the generalization gap isweak; (b) (c) are a problem setting close to the NTK regime, where the correlation is stronger. Allfull results are shown in Appendix D.
Figure 3:	Relationship between Pearson's Correlation (generalization gap and TIC estimates),and d/n. It should be noted that the correlation between the TIC estimates and the generalizationgap is high in regions with large d/n, which are considered to be close to the NTK regime. Allfull results of its value and other metric’s Spearman’s Correlation and Kendall’s τ Coefficient resultincluding are shown in Appendix D.
Figure 4:	Correlation between the generalization gap and the TIC estimates in training process.
Figure 5: A comparative experiment using TIC as an evaluation value for pruning with SHAin HPO for training of CIFAR10 on ResNet-8: (a) shows the case where all hyperparametercandidates are trained to the end without pruning. (b) shows the case where pruning is performedbased on validation loss as a baseline. (c) shows the pruning method using TIC. In the figure, all thelegends on the right side show the trials with different hyperparameters, and the final generalizationperformance (validation loss) to be reached is in descending order. The 1st place trial is shown indark purple and the 3rd place in light purple.
Figure 6: TIC takes into account the bias of the estimateB Asymptotic equivalence of TIC to Cross-ValidationSince deep learning usually requires a lot of data and a huge amount of time for training, the Hold-Out method is commonly used to divide the training data into train data for training, validationdata for model selection, especially for hyperparameter optimization, and test data to verify theperformance of the model. This method is relatively fast, but its evaluation varies depending on howthe data is divided, and it is not used when the number of data is small. In K-fold cross-validation,the entire train data is divided into K pieces. Then, one of them is used as the validation data, and theremaining K-1 groups are decomposed into training data. The validation data and training data areswapped and repeated, and the verification is repeated so that all cases become validation cases. Theleave-one-out cross-validation (LOOCV) is a method that uses only one piece of the entire train dataas validation data. LOOCV is empirically known to have high performance and is often used whenthe overall data is small. If the number of data is n, the bias of the estimation error is O(1/√n) forthe Hold-Out method and O(1/n) for LOOCV (Stone, 1977). However, LOOCV requires n timesthe computational cost. In a case such as ImageNet-1K, 1.2 million images can be used as trainingdata, and the current trend is to use the Hold-Out method, which allows the estimation error to beO(1∕y∕n) with small computational cost, instead of reducing the estimation error to O(1∕n) at theexpense of huge computational cost.
Figure 7: Distribution of training loss and generalization gap on the trained modelsGeneralization. Gap18Under review as a conference paper at ICLR 2022D	Additional Experimental ResultsD. 1 Full Results of Correlation between Generalization Gap and TIC LowerB ound EstimatesWe summarize these results, evaluated with three different correlation coefficients, in Table 7. Theseresults are the calculated 3 types of correlation coefficients for the plot shown in figure 4a, 4b and 2c. Furthermore, the relationship between these correlation coefficients and the values of the ratios ofthe parameters to the number of data is shown in Figure 8. The result of plotting these results alongwith d/n is shown in Figure 8.
Figure 8: Relationship between correlation coefficient and d/n. It should be noted that the correla-tion between the TIC estimates and the generalization gap is high in regions with large d/n, whichare considered to be close to the NTK regime.
Figure 9: Small-scale experiments: comparison of the TIC estimate w/ H(θ) and F (θ) respec-tively.
Figure 10: Small-scale experiments: comparison of the TIC estimate.
Figure 11: Small-scale experiments: comparison of the TIC estimate and Generalization Gap.
Figure 12: Practical-scale MNIST experiments: comparison of the TIC estimates.
Figure 13: Practical-scale MNIST experiments: elements of the TIC estimates.
Figure 14: Practical-scale CIFAR experiments: comparison of the TIC estimates.
Figure 15: Practical-scale CIFAR experiments: elements of the TIC estimates.
Figure 16:	Correlation between generalization gap and TIC estimates in MNIST experiments,through training process. The color map shows the epoch.
Figure 17:	Correlation between generalization gap and TIC estimates in CIFAR experiments,through training process.The color map shows the epoch.
Figure 18: Distribution of training loss and generalization gap on the trained modelsTable 13: Correlation: TIC estimates Tr(C(θ))/Tr(F(θ)) and generalization gapModel	Dataset	Spearman's	Kendall’s τ	Pearson’s Correlation3-LNN	Tiny MNIST	^^0.277^^	0.238	0.2563-LNN	Small Tiny MNIST	0.806	0.622	0.8113-NN w/ SC	Tiny MNIST	-0.19	-0.137	-0.3473-Wide NN w/ SC	Tiny MNIST	0.834	0.656	0.96733Under review as a conference paper at ICLR 20220.0400.035Generalization Gap vs TIC Estimate0∙0∙0∙dp0UOIJPZWəuə00.0000.005SmllTinyMNIST on 3-LNNTinyMNIST on 3-LNNTinyMNIST on 3-NN w/ SCTinyMNIST on Wide 3-NN w/ SC
