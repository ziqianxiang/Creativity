Figure 2: T-SNEs of the first three classes of clean CIFAR-10 data and the perturbations generated viaDeepConfuse (Feng et al., 2019) and error-minimizing noises (Huang et al., 2021). The perturbationsare flattened and normalized into unit norms.
Figure 3: An illustration of the shortcut learn-ing phenomenon in this paper.
Figure 4: Visualization of perturbed images and normalized perturbations. Columns a) and b) usesynthetic perturbations. Columns c) and d) use the attack in Huang et al. (2021).
Figure 5: Training curves of ResNet-18 models on perturbed and clean data. The word ‘poisoned’denotes the model is trained on perturbed data. The test performance is evaluated on clean data. Thetest accuracy is low throughout training when synthetic perturbations are added.
Figure 6: T-SNEs of targeted adversarial examples (Fowl et al., 2021b) and NTGA (Yuan & Wu,2021). Perturbations from the same class are well clustered. Notably, many embeddings of NTGAare overlapped, suggesting that it uses very similar perturbations for some examples.
Figure 7: Training curves of ResNet18 models trained on SVHN, CIFAR-10, and CIFAR-100 datasets.
