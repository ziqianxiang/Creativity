Figure 1: Examples of the en-fr code switched synthetic sentences. The words replaced with trans-lations or aliases are marked with bold font and underline, respectively.
Figure 2: Examples of the cycles extracted from knowledge graph.
Figure 3: MLM on the code switched synthetic sentence “motor car [mask] designed to carry[mask] passager.”. The cross entropy loss LCS is only computed over the randomly masked entityand relation tokens highlighted in lime green. For simplicity, the sub-word tokens are not shown inthis example.
Figure 4: Examples of the masked training samples for logic reasoning. The relations are highlightedin orange. The masked entity and relation tokens are highlighted in lime green.
Figure 5: An example (English) extracted from our cross-lingual logic reasoning (XLR) dataset.
