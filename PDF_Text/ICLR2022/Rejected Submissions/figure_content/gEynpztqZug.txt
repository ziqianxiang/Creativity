Figure 1: Mako Framework4.1	Automatic HYPERPARAMETER SearchBefore producing weak labeling functions, two questions must be answered: (1) HoW do We choosethe model architecture for our weak labeling functions? (2) How do We train these models afterarchitecture is decided? Formally, these questions lead to the decision of architectural and traininghyperparameters of labelers, which Mako accomplishes automatically.
Figure 2: Searched Architectures ofWeak Labeling FunctionsFor example, to re-create our binary MNIST LML tasks, one needs to first arrange the 45 tasks as0 vs 1, 0 VS 2, ..., 8 VS 9. Then, split the labeled training data, unlabeled training data and testing13Under review as a conference paper at ICLR 2022data into 120/11880/2000. All the data splits have balanced classes. Finally, perform our proceduredescribed in Section 4, starting from the hyperparameter search in the shrunken search space.
Figure 3: Labeling aCCuraCies of the task sequenCes after ConfidenCe Calibration and thresholding,with threshold = %/num-ClaSSeS + 0.01 for all tasks, i.e. labeling accuracies of (Xi,u,c,Yiπu c)desCribed in 4.314Under review as a conference paper at ICLR 2022Appendix C Additional Supervised LML Experiment AnalysisWe visualize peak per-task accuracy and final accuracy of supervised LML experiments (Figure 4)and learning curve of final accuracies in these experiments (Figure 5), summarized in Table 1 and2. In these plots, we used the same notation L and U of the main text to specify the training datasetting, except TrueU denoting the case trained on the same instances of U but using the true labelsinstead.
Figure 4: Supervised LML experiments, showing mean and 95% confidence interval. Peak per-taskaccuracy increases as more mako-labeled data is provided as training, showing the benefit of thegenerated labels.
Figure 5: Learning Curve Comparisons(d) DF-CNN on Binary CIFAR-1015Under review as a conference paper at ICLR 2022Appendix D	Additional Semi-Supervised LML ExperimentAnalysisWe include learning curves for each of the instance-incremental semi-supervised LML experimentsin Figure 6 and each of the class-incremental semi-supervised LML experiments in Figure 7. Alltraining curves show final accuracy up to task i as defined in Equation 3, where task i is the currenttask being trained. For easier tasks with high labeling accuracy, such as CIFAR-10, the Mako-enabled semi-supervised LML methods perform similarly to the equivalent supervised fully-labeledtask sequence. Otherwise, the Mako-enabled semi-supervised methods approach fully supervisedperformance.
Figure 6: Instance-incremental semi-supervised vs fully supervised learning curve comparisonsMako-Iabeled tfFully-labeled tf----Mako-Iabeled dfcnn—Fully-labeled dfcnnΓ½ Γ¼ ∏-*Mako-Iabeled denFully-labeled den0	1	2	3	4	5Task0	1	2	3	4	5Task----Mako-Iabeled dfcnnFully-labeled dfcnn(a) Binary CIFAR-10 Experiment0	5	10	15	20Task(b) 5-way CIFAR-100 Experiment0	1	2	3	4	5Task
Figure 7: Class-incremental semi-supervised vs fully supervised learning curve comparisons.
