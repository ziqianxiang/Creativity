Figure 1: Online RL collect the data by interacting with the environment and they donâ€™t utilize theexisting logged dataset while offline RL only exploit logged dataset without any future performanceimprovement. By contrast, we focus on obtaining the best of both worlds.
Figure 2: Learning curves on the D4RL (Fu et al., 2020) task Hopper-medium-replay-v0. The re-ported results are the averaged performance across five random seeds and the shaded areas representthe standard deviation across different seeds. The normalized score of 100 is the average returns ofa domain-specific expert while normalized score of 0 corresponds to the average returns of an agenttaking actions uniformly at random across the action space.
Figure 3: Ablation study on Walker2d task.
Figure 4: Training curves on D4RL continuous control benchmark across five random seeds. Theshaded areas represent the standard deviation across different seeds.
Figure 5: Different offline update steps on walker2d task across three random seeds.GCQL-i2e5:initial steps is 2e5, GCQL-i5e4: initial stesp is 5e4, GCQL-o2e4: offline update steps is 2e4,GCQL-o5e3: offline update steps is 5e3.
Figure 6: Different possibility setting on walker2d task across three random seeds.GCQL-X meansthe sample possibility from online buffer is X.
Figure 7: Extra ablation study on Halfcheetah.
Figure 8: Training curves for TD3+BC on D4RL continuous control benchmark across three ran-dom seeds on tasks: random-v0, medium-v0 and medium-replay-v0. TD3BC means the algorithmintroduced by (Fujimoto & Gu, 2021) while TD3BCGC is the variant with our greedy-conservativeframework and online-buffer replay buffer.
Figure 9: Training curves for OFF2ON (Lee et al., 2021) on D4RL continuous control benchmarkacross four random seeds on tasks: random-v0, medium-v0,medium-expert-v0 and medium-replay-v0.
Figure 10: REDQ-ONLINE learn from scratch without the offline pre-training while GCQL andCQL learn from both offline dataset and online interaction.
