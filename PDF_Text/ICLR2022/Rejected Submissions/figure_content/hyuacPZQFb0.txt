Figure 1: Our benchmarking methodology AdaTime consists of three main steps: Data Preparation,Domain Alignment, and Model Selection. We first prepare the train and test data in both source andtarget domains (i.e., XtSrain , XtSest , XtTrain , XtTest). Then the processed source and target data arepassed through the backbone network to extract the corresponding features. The domain alignmentalgorithm being evaluated is then used to address the distribution shift between the source and targetfeatures. Last, we pass the source and target data to select the best hyper-parameters for the domainalignment algorithm. We used three different model selection approaches (Best viewed in color).
Figure 2: Results of best models according to target risk for different methods in terms of accuracyand macro average F1-score.
Figure 3: Comparison between 1D-CNN and 1D-ResNet-18 backbones applied on the both UCI-HAR and HHAR dataset. Results are in terms of macro F1-score.
Figure 4: Class distribution of selected subjects among different datasets16Under review as a conference paper at ICLR 2022C Hyper-parameter importanceThe importance of each hyper-parameter can be valuable when you have low budget for hyper-parameter tuning. As such you can tune the most important hyper-parameter while fixing othersto specific value. In this work, We also study how different hyper-parameters can affect the modelperformance. We test the learning rate against other model specific performance for 5 different do-main adaptation algorithms. To do so, we leverage random forest model and feed the correspondinghyper-parameters as input and the target metric as output (Probst et al., 2019). In our case, we av-eraged all the model selection risks and use them as a metric to calculate the importance of eachparameter.
Figure 5: Parameters importance for some selected UDA methods through the three datasets.
Figure 6: Backbone network of our ADATIME, where K is the kernel size and s is the stride.
