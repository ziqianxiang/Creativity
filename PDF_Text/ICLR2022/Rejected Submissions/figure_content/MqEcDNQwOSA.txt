Figure 1: Assuming the language model has 8 vocabularies(embedding vectors), and an embeddingvector is ready for separation into three sub-embedding vectors. The sub-embedding blocks denotedin the same letter share learning parameters across the embeddings. As a result, 8 embedding vectorscan be reconstructed with only 6 sub-embedding vectors. We suggest two allocating sub-embeddingmethods, 1) sequentially allocate sub-embeddings(Algorithm 1); 2) rearrange the sub-embeddingsusing their contextual information from a pretrained network(Algorithm 2).
Figure 2: 3-d scatter plots of each k-sub-embedding. The embedding vectors are pointed indifferent colors depending on the last sub-embedding vector.
Figure 3: Inter-similarity and intra-similarity of each hidden state. Sinter and Sintra of eachk-sub-embedding goes almost 1.0 according to highly correlated embeddings.
Figure 4:	3-d scatter plot of each layer in 3-sub-embedding network. The hidden states of eachtoken are applied PCA to draw in 3-d space. The tokens gather around to narrow cone at the lastlayer.
Figure 5:	3-d scatter plot of each layer in RoBERTaMEDIUM (ours).
Figure 6:	3-d scatter plot of each layer in 2-sub-embedding network.
Figure 7:	3-d scatter plot of each layer in 3-sub-embedding network.
Figure 8:	3-d scatter plot of each layer in 4-sub-embedding network.
Figure 9:	3-d scatter plot of each layer in 6-sub-embedding network.
Figure 10:	3-d scatter plot of each layer in 8-sub-embedding network.
