Figure 1: Representation of our method, in which aDARTS supernet is simply inserted into the networkcomponents of a standard RL training pipeline, whichmay potentially be highly distributed.
Figure 2: Illustration of our supernet via stacking nor-mal and reduction cells. Solid lines correspond to se-3.2 RL-DARTSFor notation, we use standard conventions in thelected ops after discretizing the supernet, which containsall possible ops weighted using α. If R > 0, we add aninital Conv3x3 for preprocessing.
Figure 3: Left: Softmax op weights over all edges in the cell when training the supernet with PPO + “Classic”search space on Dodgeball. Zero op weight is not shown to improve clarity. Note that by 25M steps, the opchoices have already converged towards a sparser solution. Right: Sanity check to verify that the supernetachieves a regular training curve, using vanilla IMPALA-CNN as a rough gauge. Both use depths 16 × 3.
Figure 4: Analogous settings with Figure 3 using Rainbow + “Micro” search space. Left: Softmax weightswhen training Rainbow with infinite levels on Bigfish, also converging towards a sparser solution. Right: Sanitycheck for supernet when using Rainbow.
Figure 5: Evolution of discovered cells over a DARTS optimization process. Left: A cell discovered in theearly stage which is dominated by skip connections and only linear ops. Right: A cell discovered in the endwhich possesses several reasonable local structures similar to Conv + ReLU residual connections.
Figure 6: Evaluation of 9 distinctdiscrete cells in order from the tra-One reason why a discretized cell may underperform is if its cor- jectory of α on the Starpilot envi-responding supernet fails to learn. One simple reason is due to ronment when using Rainbow.
Figure 7: (a) Two different supernets trained on the Dodgeball environment using Rainbow, with correspondingdiscretized cells evaluated using 3 random seeds. (b) Discretized cell from Supernet 1. Note the similarity toregular Conv3x3 + ReLU designs. (c) Discretized cell from Supernet 2, which uses too many Tanh nonlinearities,known to cause vanishing gradient effects.
Figure 8: Left 2x2 Plot: Examples of discrete cell evaluations using the “Classic” search space with PPO, withdepths 64 × 3. Right: Normal (Top) and Reduction (Bottom) cells found for “Plunder” which achieves fastertraining than IMPALA-CNN. Note the interesting use of 5x5 convolutional kernel sizes later in the cell.
Figure 9: Histogram of 100 random cells’ rewards on Dodgeballwith Rainbow + “Micro” search space (depths 64 × 3), with 95%substantially worse and 55% unable to even train. More results inAppendix E.
Figure 10: Left 2x2 Plot: Evaluation of the discrete cell joint-trained over all environments using depths 64 × 5to emphasize comparison differences. Right (Top): Discrete cell found. Right (Bottom): Average Normalizedrewards over all 16 games during supernet + baseline training.
Figure R1: Attempts to predict random cell performances on the Dodgeball environment by using the Jacobiancovariance predictor. Jacobian covariance scores are computed as in (Mellor et al., 2020) by evaluating thenetworks (using the cells with depths 64 × 3) on mini-batches of B = 16 random images sampled from theDodgeball environment using a pretrained policy’s trajectories. For each cell, Jacobian Covariance scores acrossmini-batches are averaged, and the performance on the Dodgeball environment is normalized by the maximumpossible return.
Figure R2: Results with RL-DARTS discrete cells on DM-Control when using the SAC algorithm.
Figure 11: Supernet training using PPO on Dodgeball with infinite levels, when using the ”Classic” searchspace with/without ReLU nonlinearities, under the same hyperparameters.
Figure 12: Supernet training using Rainbow on Dodgeball with infinite levels, when using the ”Micro” searchspace with/without trainable architecture variables α, under the same hyperparameters.
Figure 13: Large IMPALA-CNNs evaluated on Dodgeball using either Infinite or 200 levels with PPO. For the200 level setting, lighter colors correspond to test performance.
Figure 14: Comparison of discretized cells before and after supernet training, on Starpilot using PPO withI = 6 nodes.
Figure 15: Comparison of discretized cells before and after supernet training, on Plunder using PPO with I = 6nodes.
Figure 16: Evaluated discretized cells discovered throughout training the supernet with Rainbow.
Figure 17: Supernet and their corresponding discrete cell rewards across all environments in Procgen usingRainbow, after normalizing using IMPALA-CNN’s performances. Thus, the black dashed line at 1.0 correspondsto IMPALA-CNN.
Figure 18: Histogram of the normalized rewards of 100 random cells evaluated on Dodgeball and Maze, usingthe “Micro” search space.
