Figure 1: Superimposed trajectories for neurons 82 - 84 across one thousand trials of VDCM, froma perfectly trained GRU. Neurons 82 and 83 demonstrate slow manifold dynamics during the delayperiod of each trial. These three neurons represent the qualitative behavior across all 250 neurons.
Figure 2: Left: the system state is on a slow manifold (blue). The output projection yt(h), canensure that the a specified dimension in the output space has the largest value (green circle), Due tothe argmax function, this dimension will be chosen as the class. Right: we can express each elementof yt as a linear combination of the values of each neuron (defined by the learned output weights),which can be interpreted as a hyperplane, where our position on this hyperplane is determined byht . The hyperplanes can be arranged such that regions in the hidden-state space exists, where eachpossible readout takes on a higher value than the others. The white points indicate the class decisionboundaries, and the blue points indicate each element of yt from an example ht .
Figure 3: Results of our method to map out thestructure of the slow-manifold in our trainedRNN. Top: probability of a successful per-turbation at the beginning of the delay period(t = 11) for each possible element, indicatedby color, across each encoding time-step. Suc-cess indicates that the neurons perturbed suc-cessfully altered the appropriate decoding time-step readout to the desired element without al-tering the readouts at any other time-step. Bot-tom: number of neurons used for each per-turbation. All pairs of time-steps and elementsrequired at least 5 neurons.
Figure 4: Left: the 73 neurons exhibiting slow manifold dynamics during the delay period.
Figure 5: Left: mean activity of our defined metric κ = (1 - [zt]ρ)(1 - [rt]ρ) for each neuronwhen we line up the decoding phases across trials. We reordered the neurons such that those which”reset” at time-step 1 of decoding are on top, followed by those which ”reset” for the first time attime-step 2, etc. Doing so reveals the demonstrated staircase pattern, indicating that there exists adistinct set of neurons which are reset at each step of the decoding phase. Right: a recoloring of Fig.
Figure 6: Left: Schematic representation of the synthetic solution to VDCM for arbitrary S andK = 2, where elements are notated as either A or B . The system acts in one of two modes, eitherencoding or decoding. During the encoding phase of each trial, elements to be remembered arepresented to the network, setting the values of the first S neurons (a slow manifold) sequentially, asdictated by an internal clock. After the first S time-steps, the system switches out of the encodingmode into decoding. The clock is not reactivated until the readout signal is given, which will beginreseting the values of the S neurons allocated to memory to zero, again sequentially. Done properly,the network will readout the encoded elements in their original order. Right: An example of thememory structure for the S = K = 2 case of our solution. The decision boundary separates theh1 - h2 plane into regions associated with each element. Four subregions exist, where informationcan be encoded such that each of the four possible sequences can be readout.
Figure 7: Nonzero parameter matricesand bias vectors used in our syntheticsolution to VDCM. Each is brokenUP into subcomponents, where nonzerosubcomponents are denoted in gray.
Figure 8: Hidden-state trajectories for 1000 superimposed trials of VDCM using our syntheticsolution, for both the S = 2 case (left) and the S = 10 case (right). Neurons fall into one of threefunctional categories. S neurons are allocated to holding memory. S + 1 neurons create the clock,which indicates what time-step of encoding or decoding is present, and one neuron is allocated to aswitch. When low, the switch indicates the trial is in its encoding phase. When high, the trial hasfinished encoding. A high switch in conjunction with the readout signal begins the decoding phase.
Figure 9: Schematic representation of themechanistic connectivity of the processes andstructures used to accurately perform VDCM,modified parity bit, and a binary charactertranslation task. Left: input is encoded inmemory, which is self sustaining, and is laterdecoded and outputted. Middle: nearly iden-tical to VDCM, but what is encoded is the out-put of an XOR operation between the input andthe previous encoded value in memory. Right:same as VDCM, but memory must undergo anonlinear computation prior to decoding.
Figure 10:	Equivalent plots to Fig. 4 (Right), for each set of neurons used to encode each additionaltime-step during the encoding phase of VDCM.
Figure 11:	Plot indicating how we chose a threshold ζ on κ, where values under this threshold areconsidered ”not-reset”, and those above are considered ”reset.” Top: histogram showing the samplemean values of κ each neuron takes across all steps of the decoding phase, over one thousand trials.
Figure 12: Test performance for trained RNNs on VDCM. Colors indicate mean activity of eacharchitecture and initialization strategy used (10 networks each). The shaded region about each curvedepicts the variance across networks. Left: cross-entropy loss across epochs. Middle: traditionalnetwork accuracy. Right: depicts what fraction of test trials the network outputted the sequencewith no mistakes. Notice that no network other than GRU got close to learning VDCM.
Figure 13: Test performance for trained RNNs on a modified parity bit task. Colors indicate meanactivity of each architecture and initialization strategy used (10 networks each). The shaded regionabout each curve depicts the variance across networks. Left: cross-entropy loss across epochs.
Figure 14: Test performance for trained RNNs on a binary character translation task. Colorsindicate mean activity of each architecture and initialization strategy used (10 networks each). Theshaded region about each curve depicts the variance across networks. Left: cross-entropy lossacross epochs. Middle: traditional network accuracy. Right: depicts what fraction of test trialsthe network outputted the sequence with no mistakes. LSTM learns this task the best, but is stillunable to consistently output entire the desired sequences with no mistakes.
Figure 15: Superimposed trajectories one thousand trials of VDCM, from a perfectly trained GRU.
Figure 16: Superimposed trajectories one thousand trials of VDCM, from a perfectly trained GRU.
Figure 17: Superimposed trajectories one thousand trials of VDCM, from a perfectly trained GRU.
Figure 18: Superimposed trajectories one thousand trials of VDCM, from a perfectly trained GRU.
Figure 19: Superimposed trajectories one thousand trials of VDCM, from a perfectly trained GRU.
Figure 20: Superimposed trajectories one thousand trials of VDCM, from a perfectly trained GRU.
Figure 21: Superimposed trajectories one thousand trials of VDCM, from a perfectly trained GRU.
Figure 22: Superimposed trajectories one thousand trials of VDCM, from a perfectly trained GRU.
Figure 23: Superimposed trajectories one thousand trials of VDCM, from a perfectly trained GRU.
Figure 24: Superimposed trajectories one thousand trials of VDCM, from a perfectly trained GRU.
Figure 25: Superimposed trajectories one thousand trials of VDCM, from a perfectly trained GRU.
Figure 26: Superimposed trajectories one thousand trials of VDCM, from a perfectly trained GRU.
Figure 27: Superimposed trajectories one thousand trials of VDCM, from a perfectly trained GRU.
Figure 28: Superimposed trajectories one thousand trials of VDCM, from a perfectly trained GRU.
Figure 29: Superimposed trajectories one thousand trials of VDCM, from a perfectly trained GRU.
Figure 30: Superimposed trajectories one thousand trials of VDCM, from a perfectly trained GRU.
Figure 31: Superimposed trajectories one thousand trials of VDCM, from a perfectly trained GRU.
Figure 32: Superimposed trajectories one thousand trials of VDCM, from a perfectly trained GRU.
Figure 33: Superimposed trajectories one thousand trials of VDCM, from a perfectly trained GRU.
Figure 34: Superimposed trajectories one thousand trials of VDCM, from a perfectly trained GRU.
Figure 35: Superimposed trajectories one thousand trials of VDCM, from a perfectly trained GRU.
Figure 36: Superimposed trajectories one thousand trials of VDCM, from a perfectly trained GRU.
Figure 37: Superimposed trajectories one thousand trials of VDCM, from a perfectly trained GRU.
Figure 38: Superimposed trajectories one thousand trials of VDCM, from a perfectly trained GRU.
Figure 39: Superimposed trajectories one thousand trials of VDCM, from a perfectly trained GRU.
