Figure 1: (left) Example observations to the OBM system. At each time-step, the OBM obtains a segmenteddepth map of a single object. (right) Example domain layout. Sample layout with a robot trajectory, field ofview (in yellow) and tables that can contain objects. Objects in the domain can move both locally on the tablethey are on as well as to different tables (simulating perturbations induced by the inhabitants). The robot movesthrough the environment acquiring local, partial observations of objects and must predict the number, location,and shape of objects it has seen.
Figure 2: Architecture and pseudocode of OBM-Net. Observations are fed sequentially to OBM-Net, andencoded with respect to each hypothesis. A subset of the hypotheses are updated at each time-step, withcorresponding slot counts incremented according to attention weight. Slots are then decoded, with the confidenceof an output proportional to underlying slot count.
Figure 3: Qualitative Visualization of OBM-Net. Illustration of OBM-Net execution on the Normal distribu-tion setting. Decoded value of hypothesis (with size corresponding to confidence) shown in red, with groundtruth clusters in black. Observations are shown in blue.
Figure 4:	(left) Generalization with Increased Observations. Plot of LSTM, Set Transformer and OBM-Neterrors when executed at test time on different number of observations from the Normal distribution. Withincreased observations, OBM-Net error continues to decrease while other approaches obtain higher error.
Figure 5:	Qualitative Visualization of OBM-Net Execution on Images. Qualitative visualization of twoimage-based association tasks (left: MNIST, right: airplanes). At the top of each is an example training problem,illustrated by the true objects and an observation sequence. Each of the next rows shows an example test problem,with the ground truth objects and decoded slot values. The three highest-confidence hypotheses for each problemare highlighted in red, and correspond to ground-truth objects.
Figure 6:	(left) Object Recovery over Time. Percentage of objects correctly recovered as a function oftimesteps since seeing the object last. OBM-Net performs similarly to an oracle with ground truth dynamics.
Figure A1: Generalization to Increased Cluster Number. Plots of inferred number of components using aconfidence threshold in OBM-Net compared to the ground truth number of clusters (OBM-Net is trained on only3 clusters). We consider two scenarios, a noisy scenario where cluster centers are randomly drawn from -1 to 1(left) and a scenario where all added cluster components are well seperated from each other (right). OBM-Net isable to infer more clusters in both scenarios, with better performance when cluster centers are more distinct fromeach other.
Figure A2: Visualization of Attention Weights. Plot of decodedvalues of slots (in red) with confidence shown by the size of dot(left), and what slot each input assigns the highest attention towards(right) (each slot is colored differently, with assigned inputs coloredin the same way). Note alignment of regions on the right with thedecoded value of a slot on the left.
Figure A3: Visualization of RelevanceWeights. Plots of the magnitude of rele-vance weights with increased observationnumber on different distributions with dif-fering standard deviation (noise).
Figure A4: 3D Reconstructions of OBM-Net. Illustration of predicted 3D shapes from OBM-Net whenOBM-Net is executed on a test trajectory. We further visualize ground truth meshes seen in the trajectory andfind that predicted shapes coarsely match ground truth meshes.
Figure A5: Illustration of iGibson Environment. (left) Illustration of example RGB input in our iGibsonenvironment. (right) Example configuration of tables in our iGibson environment (tables drawn in blue).
Figure A6: Qualitative Visualization of Domains. Visualizations of the Normal Gaussian, Dynamic domainsand Robotic domains. Observations are transparent while ground truth states are bolded for gaussian anddynamic domains. Four sample image observations shown for robotic domain.
Figure A7: Object Dynamics in Household Domain. Illustration of object dynamics in our simulatedhousehold environment. Objects exhibit either vertical, horizontal, or teleportation motion, dependent on theobject class across configurations A, B, C. We illustrate the motion of each object class, with start and endimages corresponding to 20 timesteps of motion of a object, except for objects that teleport - these change ateach timestep.
Figure A8: Architecture of baseline models.
Figure A9: Architecture of OBM-Net and the shape decoder.
