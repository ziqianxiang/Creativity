Figure 1: Comparing pixel-based RL methods across finer categorizations of evaluation benchmarks. Eachcategory ‘Cx’ denotes different data-centric properties of the evaluation benchmark (e.g., C1 refers to discreteaction, dense reward, without distractors, and with data randomly cropped [33, 35]). Exact descriptions of eachcategory and the algorithms are provided in Table 4 and Table 5. Baseline-v0 refers to applying the standard deepRL agent (e.g., Rainbow DQN [53] and SAC [23]); Baseline refers to adding reward and transition prediction tobaseline-v0, as described in Section 3; Contrastive includes algorithms such as PI-SAC [38] and CURL [36];Metric denotes the state metric losses such as DBC [62]; SSL w/o Contrastive includes algorithms such asSPR [46]; Reconstruction2 includes Dreamer [25] and TIA [19]. For a given method, we always considerthe best performing algorithm. Every method leads to varied performance across data categories, making acomparison which is an average across all categories highly uninformative.
Figure 2: (Left) Baseline for control over pixels. We employ two losses besides the standard actor and criticlosses, one being a reward prediction loss and the other a latent transition prediction loss. The encoded statest is the learnt representation. Gradients from both the transition/reward prediction and the critic are used tolearn the representation, whereas the actor gradients are stopped. In the ALE setting, the actor and critic lossesare replaced by a Rainbow DQN loss [53]. (Right) Natural Distractor in the background for standard DMCsetting (left column) and custom off-center setting (right column). More details about the distractors can befound in Appendix 2.
Figure 3: Baseline Ablations. Average normalized performance across six standard domains from DMC. Meanand std err. for 5 runs. Left plot: Baseline with vs without reward prediction Middle plot: Baseline withnon-linear vs linear reward predictor/decoder. Right plot: Baseline with vs without transition prediction.
Figure 4: Baseline Ablations. Average normalized performance across six standard domains from DMC. Meanand std err. for 5 runs. Left plot: Baseline vs state metric losses (DBC [62]). The performance of baseline iscompared with bisimulation metrics employed by DBC 3. Middle plot: Data Augmentations. Cropping removesirrelevant segments while flip and rotate do not, performing similar to the baseline. Baseline with random cropperforms equally as good as RAD. Right plot: Contrastive and SSL w/o Contrastive. We replace the transitionloss of the baseline with a contrastive version (Contrastive + Rew). Further, we consider simple contrastive(PI-SAC [38]) and SSL w/o contrastive (variant of SPR [46] for DMC) losses as well.
Figure 5: Pixel reconstruction. Averagenormalized performance across six DMC do-mains with distractors. Baseline achievesbetter performance than SOTA methods likeDreamer and TIA 4.
Figure 6: Reconstruction and augmenta-tions for sparse settings. Normalized per-formance for ball-in-cup catch domain fromDMC.
Figure 7: Atari 100K. Human normalized performance (mean/median) across 25 games from the Atari 100Kbenchmark. Mean and 95% confidence interval for 5 runs. Left plot: Comparison for all 25 games. Right plot:Comparison for only dense reward games (7 games from Table 7).
Figure 8: Distractor on Robot Centered configuration We show the sequence of observations with distractorbackgrounds on Cheetah Run, Hopper Hop and Walker Walk tasks. It is to be noted that there can be more thanone video streams playing sequentially for an environment.
Figure 9: Distractor on Robot Off-centered configuration The background distractor configuration is sameas the robot-centered setting, but the robot’s size and position are changed. This setting increases difficulty formethods dependent on well positioning of the robots.
Figure 10: Baseline w/ distractor vs Baseline w/o distractor vs SAC w/ and w/o distractor Performance ofthe proposed baseline on six DMC tasks in the presence and absence of distractors along with the performnceof SAC (Baseline-v0) on the same settings. The minor difference in the performance of baseline in the twotask conditions clearly illustrates the learning of background/distractor invariant representations. This is instark contrast with several experimental analysis showing the overfitting by other algorithms. Additionally, theperformance gain as compared to SAC is visible on both with and without distractor cases.
Figure 11: Baseline w/ augmentations vs Baseline-v0 w/ augmentations Performance of the baseline withcrop and flip augmentations on the task data. The comparison is conducted based on the results of RAD [35]with similar augmentations. Performance curves for each of the six DMC tasks is shown. With cropped data,baseline performs equally well as RAD due to removal of task irrelevant features by cropping. Whereas forflipped data, where none of the task irrelevant features are excluded, baseline surpasses RAD.
Figure 12: Baseline w/ multi-step training Performance of baseline with recurrent latent state space model andreward prediction network (MLP) is shown for a fixed horizon for each of the six DMC tasks. Increasing horizondegrades performance across all the tasks.
Figure 13: Baseline w/ multi-step training and reconstruction Performance of the proposed baseline on sixDMC tasks with multi-step training and added reconstruction loss for each prediction. This resembles thearchitecture of Dreamer [25] corresponding to baseline. Full reconstruction tries to capture task-irrelevantinformation leading to decrease in performance. Further, the performance degrades irrespective of the choice ofthe horizon.
Figure 14:	Baseline component ablations. The importance of the two components of the proposed baseline:Transition and Reward, were analyzed based on the effect of their absence on the performance achieved for eachof the six DMC tasks. The comparison shows that both are necessary to achieve the best performance overall.
Figure 15:	Baseline w/ non-linear vs w/ linear reward decoder. Performance across six DMC tasks with linearand non-linear reward decoders with the baseline architecture. Reward decoder formulation plays a significantrole in the representation learning objective. As a linear reward decoder discards task-irrelevant features tooquickly, the left features are not sufficient to act optimally. On the other hand, a non-linear reward decoder iscomparatively sophisticated in recognizing task-irrelevant features and features required to act optimally.
Figure 16:	Arrangement of Reward and Transition in baseline architecture. Performance across six DMCtasks with reward prediction through transition and independent reward + transition prediction modules. Theaverage performance is similar for both the cases. Having identified the importance of the reward and transitionmodels, the remaining question is how to actually assemble these in the overall architecture. There are twoworkable possibilities for this, one being attaching both independently to the encoded state, i.e., R(s, a, s0), andthe other being attaching the transition model to the encoded state and then attaching the reward model to theoutput of the transition model, i.e., learning reward through transition R(s, a, ^). We test both of these andconclude that both perform equally well. We choose reward through transition as it is marginally better.
Figure 17:	Baseline vs Augmentations. Performance of proposed baseline and RAD [35] with crop/flip/rotateaugmentations on the data from the task. Cropping removes the distractor data and excels in getting the bestperformance in all the six DMC tasks. Baseline performs close to RAD flip and rotate which consider thecomplete pixel data. This indicates that RAD crop takes advantage of the centered position of the task-relevantobjects and well curated structure of the data. This eventually introduces a considerable inductive bias.
Figure 18:	RAD w/ standard DMC setting vs w/ zoomed out DMC setting. We conducted an ablation onperformance of RAD with crop and flip data augmentations for two diverse camera positions for six DMC tasks.
Figure 19:	Baseline vs RAD vs PI-SAC vs CURL. We conducted an ablation on performance of SAC withcontrastive losses on DMC task with data augmentations. Baseline and RAD [35] with random crop werecompared with PI-SAC [38] and CURL [36]. While CURL uses the contrastive loss of CPC [42], PI-SAC uses aConditional Entropy Bottleneck (CEB) objective. PI-SAC performance is significantly better as compared toCURL.
Figure 20:	Baseline vs contrastive vs SSL w/o contrastive. Performance of the baseline is compared with theaddition of contrastive loss instead of the transition loss of the baseline for each of the six DMC tasks. Further,we also compare simple contrastive losses like PI-SAC [38] (without augmentations), CURL [36] and SSL w/ocontrastive losses like an extension of SPR [46] (which used cosine similarity loss) for DMC. Contrastive andSSL w/o Contrastive approaches do not seem to be very effective in the absence of data augmentations.
Figure 21:	Baseline vs Methods with Reconstruction Losses. Performance of the proposed baseline with thestate of the art algorithms employing reconstruction losses, Dreamer [25] and TIA [19]. Dreamer withoutdistractors marks the upper limit for the maximum achievable performance. The performance of baseline, havinga relatively simple architecture, is considerably better than Dreamer and TIA.
Figure 22:	Baseline vs Baseline with Reconstruction losses. The performance of the variants of baselinewith additional reconstruction losses in analyzed for each of the six DMC tasks. The variants include fullreconstruction and partial/relevant reconstruction losses. While the former is same as the approach employedby Dreamer [25], the later only tries to reconstruct the task-relevant features, which is provided explicitly.
Figure 23:	Baseline vs Augmentations vs Reconstruction. Based on the ablation studies with full and relevantreconstruction losses added to the baseline, the relevant reconstruction variant is contrasted with RAD [35],Dreamer [25] and TIA [19]. RAD with cropping removes most of the distractor data and the performanceillustrates that the representations learned by RAD with cropping is equivalent to reconstructing only the relevantpixels. Full reconstruction by Dreamer constructs the lower margin of the plots and TIA improves moderatelyon Dreamer by adding a decoupling between task relevant and irrelevant features. However, the efficacyof these methods are significantly compromised as compared to performance achievable by actually learningtask-relevant features.
Figure 24:	Augmentations with Baseline vs Reconstruction losses. The role of augmentations in the finalperformance of reconstruction loss on the distractor observations is analyzed. While augmentations tend to boostthe performance for all algorithms including Baseline, RAD (Baseline-v0) and PI-SAC (Contrastive), they donot show much improvement with reconstruction losses.
Figure 25:	Baseline vs Value Aware Learning. Performance of the proposed baseline and truly value awarelearning on each of the six DMC tasks. We correlate the relation between the baseline and value aware learning.
