Figure 1: Task accuracies on Split MNIST (Zenke et al., 2017) when replaying only 10 samples of classes 0/1at a single time step. The black vertical line indicates when replay is used. ACC denotes the average accuracyover all tasks after learning Task 5. Results are averaged over 5 seeds. These results show that the time toreplay the previous task is critical for the final performance.
Figure 2: An exemplar tree of replay memory compositions from the proposed discretization method describedin Section 3.2 for Split MNIST. The replay memories from one replay schedule are found by traversing fromtask 1-5 through the tree on the right hand side. The replay memory compositions have been structured accord-ing to the task where they can be used for replay. Note that the replay memory at task 1 is the empty set, i.e.,M = 0. Example images for each task are shown on the left.
Figure 3: Average test accuracies Over tasks after learning the final task (ACC) Over the MCTS simulatiOnsfOr all datasets, where ’S’ and ’P’ are used as shOrt fOr ’Split’ and ’Permuted’. We cOmpare perfOrmancefOr RS-MCTS (Ours) against randOm replay schedules (RandOm), Equal Task Schedule (ETS), and HeuristicScheduling (Heuristic) baselines. FOr the first three datasets, we shOw the best ACC fOund frOm a breadth-firstsearch (BFS) as an upper bOund. All results have been averaged Over 5 seeds. These results shOw that replayscheduling can imprOve significantly Over ETS and OutperfOrm Or perfOrm On par with Heuristic acrOss differentdatasets and netwOrk architectures.
Figure 4: VisualizatiOn using a bubble plOt Of a re-play schedule learned frOm Split CIFAR-100. Thetask prOpOrtiOns vary dynamically Over time whichwOuld be hard tO replace by a heuristic methOd.
Figure 5: Average test accuracies over tasks after learning the final task (ACC) over different replay memorysizes M for the RS-MCTS (Ours) and the Random, ETS, and Heuristic baselines on all datasets. All results havebeen averaged over 5 seeds. The results show that replay scheduling can outperform replaying with randomand equal task proportions, especially for small M, on both small and large datasets across different backbonechoices. Furthermore, our method requires less careful tuning than the Heuristic baseline as M increases.
Figure 6: Number of replayed sam-ples per task for the 5-task datasetsin our tiny memory setting. Ourmethod uses a fixed M = 2 sam-ples for replay, while the baselinesincrement their memory per task.
Figure 7: Comparison of test classification accuracies for Task 1-5 on Split MNIST from a network trainedwithout replay (Fine-tuning), ETS, and RS-MCTS. The ACC metric for each method is shown on top of eachfigure. We also visualize the replay schedule found by RS-MCTS as a bubble plot to the right. The memorysize is set to M = 10 with uniform memory selection for ETS and RS-MCTS. Results are shown for 1 seed.
Figure 8: Number of replayed samples per task for 10-task Permuted MNIST (top) and the 20-task datasetsin the experiment in Section 4.5. The fixed memory size M = 50 for our method is reached after learningtask 6 and task 11 on the Permuted MNIST and the 20-task datasets respectively, while the baselines continueincrementing their number of replay samples per task.
