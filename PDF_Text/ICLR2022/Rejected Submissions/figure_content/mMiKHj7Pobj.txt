Figure 1: Distributions of users over time. Left: A distribution which remains constant over time,following the i.i.d assumption. Right: Auto-induced Distributional Shift (ADS) results in a changein the distribution of users in our content recommendation environment. (see Section 5.2 for details).
Figure 2: In the widely studied problems of (a) reinforcement learning (RL) with state s, actions, reward S tuples, and (b) i.i.d. supervised learning (SL) with inputs x, predictions y and loss l,there are no issues of undesirable incentives for ADS. We focus on cases where there are incentivespresent which the learner is not meant to pursue (c,d). Lines show paths of influence. The learnermay have incentives to influence any nodes descending from its action, A, or prediction, y. Whichincentives are undesirable (orange) or desirable (cyan) for the learner to pursue is context-dependent.
Figure 3: (a) No context swapping (b) Context swapping rotates learners through different environ-ments. This removes the incentive for a learner to “invest” in a given environment, since it will beswapped out of that context later and not be able to reap the benefits of its investment.
Figure 4: Average level of non-myopic cooperate behavior observed in the RL unit test, with twometa-learning algorithms (A) PBT and (B) REINFORCE. Lower is better, since the goal is for (non-myopic) incentives for ADS to remain hidden. Despite the inner loop being fully myopic (γ = 0),outer-loop (OL) optimizers reveal incentives for ADS (top rows). Context swapping effectivelyhides this incentive, reducing ADS (bottom rows).
Figure 5: Left: Offline Q-learning can reveal incentives for ADS when pooling data from differentpolicies. Yellow regions represent policy pairs (θ1, θ2) for which Q(C) > Q(D) in the MyopicRL unit test, resulting in non-myopic behavior. Right: Even online, Q-learning fails the unit testfor some random seeds; empirical p(cooperate) stays around 80-90% in 3 of 5 experiments(bottom row). Each column represents an independent experiment. Q-values for the cooperateand defect actions stay tightly coupled in the failure cases (col. 1,2,5), while in the cases passingthe unit test (col. 3,4) the Q-value of cooperate decreases over time.
Figure 6: Content recommendation experiments. Left: using Population Based Training (PBT)increases accuracy of predications faster, leads to a faster and larger drift in users’ interests, P(y |x),(Center); as well as the distribution of users, P (x), (Right). Shading shows std error over 20 runs.
Figure 7: Average level of non-myopic (i.e. cooperate) behavior learned by agents in the unittest for incentives for ADS. Despite making the inner loop fully myopic (γ = 0), population-basedtraining (PBT) can reveal incentives for ADS, leading agents to choose the cooperate action (toprow). context swapping successfully prevents this (bottom row). Columns (from left to right) showresults for populations of 10, 100, and 1000 learners. In the legend, “interval” refers to the interval(T) of PBT (see Sec. 2.2). Sufficiently large populations and short intervals are necessary for PBTto induce nonmyopic behavior.
Figure 8: Results on the Supervised Learning ADS unit test mirror those on the RL unit test.
Figure 9: The same experiments as Figures 5, 10, run for 50,000 time-steps instead of 3000, toillustrate the persistence of non-myopic behavior.
Figure 10: More independent experiments with Q-learning, exactly following Figure 5. Q-learningfails the unit test in a total of 10/30 experiments (including those from Figure 5).
Figure 11:	More independent experiments with Q-learning, exactly following Figure 5,using context swapping. This leads to a 100% success rate on the unit test.
Figure 12:	Context swapping doesn’t have the desired effect in the content recommendation envi-ronment.
Figure 13: Content recommendation results for different values of α1, α2.
