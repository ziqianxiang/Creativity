Figure 1: Static quantization vs. client-adaptive quantization when accumulating parameters pA andpB. (a): Static quantization uses the same quantization level for pA and pB . (b) Client-adaptivequantization uses a slightly higher quantization level for pB because pB is weighted more heavily.
Figure 2: Time-adaptive quantization. A smallquantization level (q) decreases the loss with lesscommunication than a large q, but converges toa higher loss. This motivates an adaptive quanti-zation strategy that uses a small q as long as it isbeneficial and then switches over to a large q. Wegeneralize this idea into an algorithm that mono-tonically increases q based on the training loss.
Figure 3: Exemplary quantization level assignment for 4 FL clients that train over 5 rounds. Eachround, two clients get sampled for training.
Figure 4: Communication-accuracy trade-off curves for training on FEMNIST with FederatedQSGD and DAdaQuant. We plot the average highest accuracies achieved up to any givenamount of clientâ†’server communication. Appendix A.3 shows curves for all datasets, as well asDAdaQuanttime and DAdaQuantclients, with similar results.
Figure 7:	Communication-accuracy trade-off curves for Federated QSGD and DAdaQuantclients .
Figure 8:	Scalability of AdaQuantFL vs. DAdaQuant.
Figure 9: Illustration of the Bernoulli random variable Qqi (pi). si is the length of the quantizationinterval. pi is rounded up to (c + 1)si with a probability proportional to its distance from csi.
