Figure 1: Closed-loop continual learning framework: an additional test memory is constructed toapproximate validation data and used as feedback signal to guide online hyperparameter adaptation.
Figure 2: Reinforcement learning-based adaptive replay framework: the RL agent selects replay hy-perparameters and the CL agent executes the replay update and evaluates the model on test memoryto output reward and state for the RL agent.
Figure 3: Approximation of test loss using test memory and training memory.
Figure 4: RL-based replay hyperparameter adaptation (CIFAR100): a) replay ratio selection per-centage in each task; b) replay iteration selection percentage in each task.
Figure 5: Quality of feedback signal with respect to test buffer size in terms of correlation with realtest loss.
Figure 6: Adaptive memory iteration.
