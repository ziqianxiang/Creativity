Figure 1: Learning curves obtained by training on the 10000 easiest, random and hardest instancesof CIFAR10 under different scenarios. The training error (dashed lines) is the error on the selectedinstances, and the test error (solid lines) is the error on the whole test set.
Figure 2: Analysis on the groups G0, G3, G6 and G9 in the training set. The right vertical axiscorresponds to the training (dashed grey line) and test (solid grey line) error under adversarial attacksfor both plots. Left plot: The left vertical axis represents the average loss of different groups. Rightplot: The left vertical axis represents the average l2 norm of features extracted during training fordifferent groups.
Figure 3: The relationship between the difficulty function based on the average loss values and theone based on the average 0-1 errors. The left figure is based on the RN18-200 model; the right figureis based on the WRN34 model. The correlation between these two metrics are 0.9466 (left) and0.9545 (right), respectively.
Figure 4: Learning curves of training on PGD-perturbed inputs against different sizes of l∞ normbased adversarial budgets using the easiest, the random and the hardest 10000 training instances. Theinstance difficulty is determined by the corresponding adversarial budget and is thus different underdifferent adversarial budgets. The dashed lines are robust training error on the selected training set,the solid lines are robust test error on the entire test set.
Figure 5: Learning curves of PGD adversari-ally trained models on the hardest 10000 in-stances in the CIFAR10 training set by dif-ferent optimizers. The dashed lines are ro-bust training error on the selected traininginstances, the solid lines are robust test erroron the entire test set.
Figure 6: Learning curves obtained by train-ing using the easiest, the random and the hard-est 20000 instances of the SVHN training set.
Figure 7: Learning curves of PGD adversar-ial training on the hardest 10000 training in-stances. The model is trained for 600 epochs.
Figure 8: The left vertical axis represents theaverage loss of the training instances in thegroups G0, G3, G6 and G9 . The right verticalaxis represents the robust error for the wholetraining (dashed grey line) and test (solid greyline) set. The model is trained for 600 epochs.
Figure 9: Learning curves of training on PGD-perturbed inputs against different size of l2 normbased adversarial budgets using the easiest, the random and the hardest 10000 training instances. Theinstance difficulty is determined by the corresponding adversarial budget and is thus different underdifferent adversarial budgets. The dashed lines are robust training error on the selected training set,the solid lines are robust test error on the entire test set.
Figure 10: Learning curves of PGD adversarial training using increasing more training data inCIFAR10 and SVHN. The dashed lines represent the robust training error on the selected traininginstances; the solid lines represent the robust test error on the entire test set. Left: we use the easiest10000, 20000, 30000, 40000 and the whole training set of CIFAR10. Right: we use the easiest 20000,30000, 40000 and the whole training set of SVHN.
Figure 11: The curves of the Lipschitz upper bound when the model is adversarially trained by theeasiest, the random and the hardest 10000 instances. The y-axis is log-scale.
Figure 12: Learning curves of PGD adversarial training (PGD AT), instance-adaptive training (IAT)and self-adaptive training (SAT). Dashed lines and solid lines represent the robust training error andthe robust test error, respectively.
Figure 13: The instance adaptive adversarialbudget size i for the training set of CIFAR10as a function of the instance difficulty d(xi).
Figure 14: Average weights of differentgroups in the training set of CIFAR10 dur-ing training. During the warmup period (first90 epochs), the weight for every training in-stance is 1. The model architecture is RN18.
Figure 15: Robust training accuracy during training when we use the original groundtruth label (left)or use the adaptive target calculated during training (right).
Figure 16: The learning curves with differentvalues of β . The solid curve and the dashedcurve represent the robust test error and therobust training error, respectively.
Figure 17: The learning curves of ATTA withand without reweighting. The solid curve andthe dashed curve represent the robust test errorand the robust training error, respectively.
Figure 18: The relationship between the dif-ficulty value and the weight assigned to eachinstances when using reweighting. We usethe average weight across epochs. The corre-lation between them is 0.8900.
Figure 19: The relationship between the dif-ficulty value and the average value of thetrue label’s probability when using the adap-tive targets. The correlation between them is0.9604.
Figure 20: Easy and hard examples in each category of CIFAR10 dataset. In each subfigure, oddcolumns present the original images, and even columns present the PGD-perturbed images. Aboveeach image, we provide the normalized difficulty defined in Equation (1) as well as the labels: truelabels for the original images and the predicted labels for the perturbed images.
Figure 21: Easy and hard examples in each category of SVHN dataset. In each subfigure, oddcolumns present the original images, and even columns present the PGD-perturbed images. Aboveeach image, we provide the normalized difficulty defined in Equation (1) as well as the labels: truelabels for the original images and the predicted labels for the perturbed images.
