Figure 1: Illustration of our bootstrap procedure, which we call few-shot distillation. We use few-shot prompts sampled from GPT-3 to generate an initial dataset of synthetic translations from ageneratively pretrained language model (left). The few-shot examples are then discarded and thesynthetic bitext reformatted for finetuning on the autoregressive language modeling objective (right).
