Figure 1: (a) Four scaling regimes Here we exhibit the four regimes we focus on in this work. (top-left,bottom-right) Variance-limited scaling of under-parameterized models with dataset size and over-parameterizedmodels with number of parameters (width) exhibit universal scaling (αD = αW = 1) independent of thearchitecture or underlying dataset. (top-right, bottom-left) Resolution-limited over-parameterized models withdataset or under-parameterized models with model size exhibit scaling with exponents that depend on the detailsof the data distribution. These four regimes are also found in random feature (Figure 2a) and pretrained models(see supplement). (b) Resolution-limited models interpolate the data manifold Linear interpolation betweentwo training points in a four-dimensional input space (top). We show a teacher model and four student models,each trained on different sized datasets. In all cases teacher and student approximately agree on the trainingendpoints, but as the training set size increases they increasingly match everywhere. (bottom) We show 4∕α0versus the data manifold dimension (input dimension for teacher-student models, intrinsic dimension for standarddatasets). We find that the teacher-student models follow the 4∕α0 (dark dashed line), while the relationship fora four layer CNN (solid) and WRN (hollow) on standard datasets is less clear.
Figure 2: (a) Random feature models exhibit all four scaling regimes Here we consider linear teacher-student models with random features trained with MSE loss to convergence. We see both variance-limitedscaling (top-left, bottom-right) and resolution-limited scaling (top-right, bottom-left). Data is varied bydownsampling MNIST by the specified pool size. (b) Duality and spectra in random feature models Herewe show the relation between the decay of the kernel spectra, αK, and the scaling of the loss with number ofdata points, αD, and with number of parameters, αP (top) The spectra of random FC kernels on pooled MNIST(bottom) appear well described by a power law decay. The theoretical relation αD = αP = αK is given by theblack dashed line.
Figure 3: Effect of data distribution on scaling exponents For CIFAR-100 superclassed to N classes (left),we find that the number of target classes does not have a visible effect on the scaling exponent. (right) ForCIFAR-10 with the addition of Gaussian noise to inputs, we find the strength of the noise has a strong effect onperformance scaling with dataset size. All models are WRN-28-10.
Figure S1: Alternate metrics and stopping conditions We find similar scaling behavior for both the loss anderror, and for final and best (early stopped) metrics.
Figure S2: This figure shows scaling trends of MSE loss with dataset size for teacher/student models. Theexponents extracted from these fits and their associated input-space dimensionalities are shown in Figure 1.
Figure S3:	This figure shows scaling trends of CE loss with dataset size for various image datasets. Theexponents extracted from these fits and their associated input-space dimensionalities are shown in Figure 1.
Figure S4:	This figure shows the variation of αP with the input-space dimension. The exponent αP is thescaling exponent of loss with model size for teacher-student setup.
Figure S5:	Effect of aspect ratio on dataset scaling We find that for WRN-d-k trained on CIFAR-10, varyingdepth from 10 to 40 has a relatively mild effect on scaling behavior, while varying the width multiplier, k, from 1to 12 has a more noticeable effect, up until a saturating width.
Figure S6:	Duality between dataset size vs feature number in pretrained features Using pre-trained embedding features of EfficientNet-B5 (Tan & Le, 2019) for different levels of regularization,we see that loss as function of dataset size or loss as a function of the feature dimension track eachother both for small regularization (left) and for tuned regularization (right). Note that regularizationstrength with trained-feature kernels can be mapped to inverse training time (Ali et al., 2019; Leeet al., 2020). Thus (left) corresponds to long training time and exhibits double descent behavior,while (right) corresponds to optimal early stopping.
Figure S7:	Four scaling regimes exhibited by pretrained embedding features Using pretrainedembedding features of EfficientNet-B5 (Tan & Le, 2019) for fixed low regularization (left) and tunedregularization (right), we can identify four regimes of scaling using real CIFAR-10 labels.
Figure S8: Loss on the teacher targets scale better than real targets for both untrained andtrained features The first three columns are infinite width kernels while the last column is a kernelbuilt out of features from the penultimate layer of pretrained WRN 28-10 models on CIFAR-10. Thefirst row is the loss as a function of dataset size D for teacher-student targets vs real targets. Theobserved dataset scaling exponent is denoted in the legend. The second row is the normalized partialsum of kernel eigenvalues. The partial sum’s scaling exponent is measured to capture the effect of thefinite dataset size When empirical ακ is close to zero. The third row shows ω% for teacher-studentand real target compared against the kernel eigenvalue decay. We see the teacher-student 6⅛ areapproximately constant.
