Figure 1: Working mechanism ofNMT systemsremove the unimportant neurons. After pruning, the smaller size DNNs could achieve competi-tive accuracy with the original DNNs but require less computational costs. Another type of tech-niques Wang et al. (2018); Graves (2016); Figurnov et al. (2017), called input-adaptive techniques,this type of technique dynamically skip a certain part of the DNNs to reduce the number of com-putations in the inference time. By skipping some parts of the DNNs, the input-adaptive DNNscan balance the accuracy and computational costs. However, recent studies (Haque et al., 2020;Hong et al., 2020) show input-adaptive DNNs are not robustness against the adversary attack, whichimplies the input-adaptive will not save computational costs under attacks.
Figure 2: The distribution of FLOPs and latency before and after token-level attacksmore computational resources. The higher the response latency, the worse the real-time translationquality. We measure the latency on two hardware platforms: Intel Xeon E5-2660v3 CPU and Nvidia1080Ti GPU.
Figure 3: The distribution of FLOPs and latency before and after character-level attacksadversarial examples to slow down NMT systems. Thus, TranSlowDown is effective in evaluatingthe efficiency robustness of NMT systems.
Figure 4: Remaining battery power of themobile device after T5 translating benign andfor IoT and mobile devices. Recall that the adversar-ial example used in our experiment only inserts onecharacter in the benign sentence. This minimal per-turbation can result from benign user typos instead adversarial sentenceof from the adversary. Thus, the results suggest the criticality and the necessity of increasing NMTsystems, efficiency robustness.
Figure 5: The efficiency metric of T5 before and after token-level attacksFigure 6: The efficiency metric of FairSeq before and after token-level attacks14Under review as a conference paper at ICLR 2022■ Benign ■ Baseline ■ OursFigure 7: The efficiency metric of T5 before and after character-level attacks■ Benign ■ Baseline ■ OursFLOPs	IntelV5 CPU Latency 1080 Ti GPU LatencyFigure 8: The efficiency metric of FairSeq before and after character-level attacksTable 7: The maximum I-FLOPS ofMackbox CharaCter-level attackSource	Target	1	2	3	4	5H-NLP	FairSeq	900.00	273.08	2400.00	573.08	2400.00	T5	1700.00	1700.00	1700.00	1700.00	1700.00FairSeq	H-NLP	250.00	325.00	400.00	400.00	433.33	T5	1400.00	1400.00	1400.00	1400.00	1400.00T5	H-NLP	3733.33	3733.33	3733.33	3733.33	3733.33	FairSeq	1177.78	1337.50	1542.86	1337.50	1337.5015σ∖A.4 Case Study Results
Figure 6: The efficiency metric of FairSeq before and after token-level attacks14Under review as a conference paper at ICLR 2022■ Benign ■ Baseline ■ OursFigure 7: The efficiency metric of T5 before and after character-level attacks■ Benign ■ Baseline ■ OursFLOPs	IntelV5 CPU Latency 1080 Ti GPU LatencyFigure 8: The efficiency metric of FairSeq before and after character-level attacksTable 7: The maximum I-FLOPS ofMackbox CharaCter-level attackSource	Target	1	2	3	4	5H-NLP	FairSeq	900.00	273.08	2400.00	573.08	2400.00	T5	1700.00	1700.00	1700.00	1700.00	1700.00FairSeq	H-NLP	250.00	325.00	400.00	400.00	433.33	T5	1400.00	1400.00	1400.00	1400.00	1400.00T5	H-NLP	3733.33	3733.33	3733.33	3733.33	3733.33	FairSeq	1177.78	1337.50	1542.86	1337.50	1337.5015σ∖A.4 Case Study ResultsIn this section, we put more generated efficiency adversarial examples for all our victim NMT systems.
Figure 7: The efficiency metric of T5 before and after character-level attacks■ Benign ■ Baseline ■ OursFLOPs	IntelV5 CPU Latency 1080 Ti GPU LatencyFigure 8: The efficiency metric of FairSeq before and after character-level attacksTable 7: The maximum I-FLOPS ofMackbox CharaCter-level attackSource	Target	1	2	3	4	5H-NLP	FairSeq	900.00	273.08	2400.00	573.08	2400.00	T5	1700.00	1700.00	1700.00	1700.00	1700.00FairSeq	H-NLP	250.00	325.00	400.00	400.00	433.33	T5	1400.00	1400.00	1400.00	1400.00	1400.00T5	H-NLP	3733.33	3733.33	3733.33	3733.33	3733.33	FairSeq	1177.78	1337.50	1542.86	1337.50	1337.5015σ∖A.4 Case Study ResultsIn this section, we put more generated efficiency adversarial examples for all our victim NMT systems.
Figure 8: The efficiency metric of FairSeq before and after character-level attacksTable 7: The maximum I-FLOPS ofMackbox CharaCter-level attackSource	Target	1	2	3	4	5H-NLP	FairSeq	900.00	273.08	2400.00	573.08	2400.00	T5	1700.00	1700.00	1700.00	1700.00	1700.00FairSeq	H-NLP	250.00	325.00	400.00	400.00	433.33	T5	1400.00	1400.00	1400.00	1400.00	1400.00T5	H-NLP	3733.33	3733.33	3733.33	3733.33	3733.33	FairSeq	1177.78	1337.50	1542.86	1337.50	1337.5015σ∖A.4 Case Study ResultsIn this section, we put more generated efficiency adversarial examples for all our victim NMT systems.
