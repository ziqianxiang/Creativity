Figure 1: Comparison of training schedules for OFA, CompOFA, and fOFA. Length on the horizon-tal axis is proportional to the number of epochs in each phase. For OFA, “elastic kernel,” “elasticwidth,” and “elastic depth” are the phases of training specified in Cai et al. (2020) that are not usedin CompOFA and fOFA.
Figure 2: The search space used for fOFA, based on the architecture space of MobileNetV3 (Howardet al., 2019). The dimension K refers to the size of the convolutional kernel, W to the channelexpansion ratio, and D to the number of repetitions of the block.
Figure 3: In upper attentive sampling, the largest possible model (in our case with D = 4 andW = 6 for all blocks, shown in red) is selected during each batch of the training process. The othermodels selected at each batch are randomly chosen from all possible sub-networks. Upper attentivesampling differs from the “sandwich model” of Yu et al. (2020) in that the smallest possible model(shown in blue) need not be selected at each batch.
Figure 4: Comparison of floating point operation vs. accuracy for each of the 243 models in thecompOFA searchspace, for compOFA (orange) and fOFA with n = 4 (blue).
