Figure 2: A set of histograms showing redundancy in learned group convolution kernels. On thex-axis is the ratio of variance explained by the first principal component when applying PCA on theset of spatial kernels along the group axis of a group convolution kernel. Y-axis shows the proportionof kernels with this explained variance ratio, where all bins sum to 1. The number of spatial kernelsis listed in the title of each subfigure. Throughout the training process, redundancy in the groupconvolution kernels increases. Left to right: subsequent layers in the network.
Figure 3: Test error vs.
Figure 4: Test error ver-sus sampling resolution ofSO(2) on MNIST-rot forseparable and non-separableSE(2)-CNNs.
Figure 5: Test error ver-sus sampled extent of thedilation group for separableand non-separable R2 o R+-CNNs.
Figure 6: Test error ver-sus resolution over rotationand scale group for separableSim(2)-CNN.
Figure 8: Process time perepoch (one pass over trainingand test sets) in seconds fordifferent resolutions on H .
Figure 7: Test error versusresolution over rotation andscale group for H-separableSim(2)-CNN.
Figure 9: The convolution operation in a CNN. In upper section of the figure above, a spatialkernel k is transformed by group element x(2,2) ∈ R2 to yield Lx(2,2) (k). In the lower section, k istransformed under the action of each element of (a discretisation of) the group R2, to yield a set K oftranslated copies of k . Convolving over fin with each of these copies yields a new feature map foutdefined over G = R2.
Figure 10: An example of the lifting convolution for G = SE(2) = R2 o SO(2). In the uppersection of the above figure, a spatial kernel k is transformed by group element (x, θ) ∈ SE(2) withX = (2, 2), θ = 90° to yield Lx(2 2) Lθ90 (k). In the lower section, k is transformed under the actionof each element of (a discretisation of) the group SE(2), to yield a set K of translated and rotatedcopies of k . Convolving fin with each of these copies yields a new feature map fout defined overG = SE(2).
Figure 13: An example of the separable group convolution for (a discretisation of) G = SE(2) =R2 o SO(2). In the upper section of the above figure, a kernel kH (defined over H) and a kernelkR2 (defined over R2) are transformed by group element (x, θ) ∈ SE(2) with X = (2,2),θ = 90° toyield Lx(2,2) Lθ90 (kH) and Lx(2,2) Lθ90 (kR2). In the lower section, kH and kR2 are transformed underthe action of each element of (a discretisation of) the group SE(2), to yield sets KH of transformedcopies of kH and a set kR2 of translated and rotated copies of k. Convolving fin with KH and KR2sequentially yields a feature map fout defined over G = SE(2).
Figure 15: An example of a local kernel gridH on the quotient group SO(3)/SO(2). Weobtain a volumetrically constant grid on G bysampling a set of equidistant points in g andmapping them to G via the exponential map.
Figure 14: An example of a localised kernelk on the quotient group SO(3)/SO(2). Al-though the SIREN parameterising the kernelis defined on g, we can associate the kernelvalues sampled on g with the relevant ele-ments g0 ∈ G through use of the grid H de-fined on G, which we map to g via the loga-rithmic map.
Figure 16: An example of a configuration of features fout a single separable group convolution kernelis unable to represent.
Figure 17: Example of aliasing effects in analytical kernel parameterisations. Assume we areperforming a lifting convolution to the SE(2) group. We evaluate two variants of our kernel function;kf exhibiting low frequencies over the kernel domain, and k, exhibiting high frequencies over kerneldomain. We evaluate these functions for a spatial grid Hge to obtain a kernel under the identityrotation, and for a slightly rotated version of this grid Hge. We see the kernel sampled from kfchanges smoothly, whereas the high frequency components in k% lead to considerably differentresults for the two grids.
Figure 18: Architecture used over all experiments.
Figure 20: Test error vs rotation angle ofMNIST test set, when trained on uprightMNIST, for non-separable SE(2)-CNN.
Figure 19: Test error vs rotation angle ofMNIST test set, when trained on uprightMNIST, for separable SE(2)-CNN.
Figure 21: Results for depthwise group sep-arable and group separable SE(2)-CNN onrotated MNIST for different resolutions onthe group.
Figure 22: Results for depthwise group sep-arable and group separable R2 o R+ -CNNon CIFAR10 for different resolutions on thegroup.
Figure 23: Test accuracy of SE(2)-CNNs on rotated MNIST for different sizes hiddenlayers in the SIREN parameterisation.
Figure 24: Test accuracy of R2 o R+-CNNs on CIFAR10 for different sizes hidden layersin the SIREN parameterisation.
Figure 25: Test error (%) of R2 o R+-CNNs on scaled MNIST for random sampling versus discreti-sation of the scale group.
Figure 26: A set of histograms showing variance along the subgroup axis in separable groupconvolutions. In contrast to Fig. 2, here we show the variability of the convolution kernel along thesubgroup axis. This may be seen as an inverse measure of redundancy. Here we can see that contraryto in non-separable group convolution kernels, variability along the group axis increases over thetraining process, indicating that redundancy decreases. Separable group convolutions thus solve theredundancy issues in group convolution kernels.
