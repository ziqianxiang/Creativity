Figure 1: Overview of VIMPAC. Sampled video frames are discretized by VQ-VAE encoder intodiscrete tokens, which are then block-masked (in light yellow blocks). The model is self-supervisedby two tasks: 1) mask-then-predict task predicts the masked tokens from their visible context; 2)contrastive learning task differentiates between positive and negative clips (details in Fig. 2) with[CLS] token feature. For brevity, we only show 2 frames with small token maps.
Figure 2: Illustration of pre-training tasks. (a): block masking constructs the 3D-contiguous mask-ing cube while i.i.d masking independently samples masked tokens. (b): given the reference clip, thepositive clip is uniformly sampled from the same video (video 1) while negative clips are sampledfrom other videos (video 2, 3). No spatial augmentations are applied to the raw video clips.
Figure 3: Data samples from temporally-heavy and spatially-heavy datasets. While temporally-heavy datasets need the temporal information to make decisions, most actions in spatially-heavydatasets could be inferred from just a single frame.
Figure 4: Nearest-neighbour reconstruction of block masking and i.i.d masking. We mask tokens atdifferent ratios and reconstruct them by simply copying their spatial or spatio-temporal neighbours.
Figure 5: Masked-token model reconstruction for SSV2 and Kinetics-400 datasets. Comparing 1.
