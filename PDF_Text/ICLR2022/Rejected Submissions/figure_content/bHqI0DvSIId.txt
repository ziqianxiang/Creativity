Figure 1: Neural SA pipeline for the TSP. Starting with a solution (tour) xk , we sample an actiona = (i, j) from our learnable policy/proposal distribution, defining start i and endj points ofa 2-optmove (replacing two old with two new edges). Each pane shows both the linear and graph-basedrepresentations for a tour. From xk and a we form a proposal x0 which is either accepted or rejectedaccording to the traditional MH step used in SA. Accepted moves assign xk+1 = x0 ; whereas,rejected moves assign xk+1 = xk .
Figure 2: (a) Policy network: The same MLP isapplied to all inputs pointwise. We use this net-work in all CO problems.
Figure 3: Results on Rosenbrockâ€™s function: (a) Example trajectory, moving from red to blue, show-ing convergence around the minimiser at (1,1) (b) Neural SA has higher acceptance ratio than thebaseline, a trend observed in all experiments, (c) Standard deviation of the learned policy as a func-tion of iteration. Large initial steps offer great gains followed by small exploitative steps, (d) Anon-adaptive vanilla SA baseline cannot match an adaptive one, no matter the standard deviation.
Figure 4: (a) Exemplar knapsack rollout on Knap50: Each row is an item and each column is asubsequent iteration. Items absent from the knapsack are in black and those present are in white.
Figure 5: (a) Plot of the BIN50 primal objective for vanilla, Neural, and Greedy Neural SA. 25th,50th, and 75th percentiles shown: Greedy and Neural SA have faster convergence than vanilla SA.
Figure 6: Policy for the Travelling Salesperson Problem. At each step, an action consists of selectinga pair of cities (i,j), one after the other. The figure depicts aTSP problem layed out in the 2D plane,with the learnt proposal distribution over the first city i in the left, and in the right, the distributionover the second city j, given i = 12. We mask out and exclude the neighbours of i (0 and 14) ascandidates for j because selecting those would lead to no changes in the tour. It is clear the modelhas a strong preference towards a few cities, but otherwise the probability mass is spread almostuniformly among the other nodes. However, once i is fixed, Neural SA strongly favours nodes jthat are close to i. That is a desirable behaviour and even features in popular algorithms like LKH-3(Helsgaun, 2000). That is because a 2-opt move (i,j) actually adds edge (i, j) to the tour, so leaningtowards pairs of cities that are close to each other is more likely to lead to shorter tours.
