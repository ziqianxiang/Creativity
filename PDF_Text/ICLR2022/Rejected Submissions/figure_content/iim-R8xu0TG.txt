Figure 1: FitVid predicts high qual-ity images of the future given the firstfew frames. Here, the visual detailon the pushed object is preserved ef-fectively, and even the shadow of therobot’s arm is predicted correctly. Inthis example, FitVid is trained on 15Mvideo frames of 7 diffedrent robots fromRoboNet (Dasari et al., 2019).
Figure 3: Effect of augmentation meth-ods on overfitting on KITTI. Both Ran-dAugment and Random Crop improvethe overfitting but the combination oftwo gets the best results.
Figure 4: FitVid on action-conditioned RoboNet (Dasari et al., 2019). The model is conditionedon the first two frames and is predicting the next ten frames given the future actions of the roboticarm. These figures demonstrate how the predicted movements of the arm closely follows the groundtruth given that the future actions is known. The model also predicts detailed movements of thepushed objects (visible in the left example) as well as filling in the previously unseen backgroundwith some random objects (look at the object that appear behind the robotic arm in the right). Also,to demonstrate a failure case, notice the wrong predictions of robot’s fingers in the right example.
Figure 6: FitVid on Human3.6M (Ionescu et al., 2014). This figures demonstrates extremely detailedand human-like motions predicted by FitVid, conditioned on the given context frames. However,on closer inspection, it can be seen that the human subject in the video is changing, from the testsubject to a training subject. This is particularly evident from the cloths. This phenomena indicatesthat, although FitVid is capable of generalizing to the frames out of training distribution, however, itmorphs the human subject into a familiar one from the training set and then plays the video fromthe memory. In fact, we can find similar videos in the training set as visualized in the last row. Thehighlighted frame is the one used for finding the closest training video. Check Figure 13 for morepredicted frames.
Figure 7: Overfitting of FitVid without augmentation. This figure visualizes the training andevaluation metrics on (a) Human3.6M (Ionescu et al., 2014), (b) KITTI (Geiger et al., 2013b) and(c) Robonet (Dasari et al., 2019), without augmentation. As it can be seen, in all cases, FitVid overfitson the training data except for Robonet. This is evident from the evaluation measurement going upwhile the training keeps decreasing. In case of Robonet, FitVid with 302M parameters did not overfitbut a scaled version of the model with 600M parameters did, as can be seen in (d). y-axis is LPIPS.
Figure 8: FitVid on BAIR robot pushing dataset (Ebert et al., 2017) with no actions. The model isconditioned only on the first frame and is predicting the next 16 frames. Given that the future actionsof the robotic arm is unknown, the prediction can diverge substantially from the ground truth video.
Figure 9: Zero-shot robot domain. We evaluate FitVid on planning tasks on a Franka Emika Pandainvolving kitchen, cleaning, and office items. We did not collect any training data for this task. Insteadthe model is trained on RoboNet and fine-tuned on augmented data from Wu et al. (2021a).
Figure 10: Example tasks for zero-shot object pushing using a robotic arm. The goal in each trial isto push the a specific object to a predetermined goal location. The trial is considered successful, ifthe robot pushes at least half of the object overlaps with its goal location at any point in the episode.
Figure 11: More detailed video from Figure 4 which illustrates FitVid on action-conditioned RoboNet (Dasari et al., 2019). The model is conditioned on the first twoframes and is predicting the next ten frames given the future actions of the robotic arm. These figures demonstrate how the predicted movements of the arm closelyfollows the ground truth given that the future actions is known. The model also predicts detailed movements of the pushed objects (visible in the top example) as wellas filling in the previously unseen background with some random objects (look at the object that appear behind the robotic arm in the bottom example). Also noticethe wrong predictions of robots fingers in the bottom example.
Figure 12: More detailed video from Figure 5 which illustrates FitVid on KITTI dataset (Geiger et al., 2013b). As it can be seen in this figure, the model generateshigh quality prediction of the future in a dynamic scene. Note how in the top example FitVid keeps predicting the movement of the shadow on the ground till it movesout of the frame. After that, the model keeps pushing the background closer in each frame, implying driving forward. We noticed that the quality of predictions dropsubstantially faster when there are more objects in the scene e.g. the driving scenes inside a city as can be seen in the bottom example. This indicates the model stillfails to generalize to more complex scenes with more moving subjects.
Figure 13: More detailed video from Figure 6 which illustrates FitVid on Human3.6M (Ionescu et al., 2014). This figure demonstrates extremely detailed andhuman-like motions predicted by FitVid, conditioned on the given context frames. However, on closer inspection, it can be seen that the human subject in the videois changing, from the test subject to a training subject. This is particularly evident from the cloths. This phenomena indicates that, although FitVid is capable ofgeneralizing to the frames out of training distribution, however, it moɪphs the human subject into a familiar one from the training set and then plays the video fromthe memory.
Figure 14: More detailed video from Figure 8 which illustrates FitVid on BAIR robot pushing dataset (Ebert et al., 2017) with no actions. The model is conditionedonly on the first frame and is predicting the next 16 frames. Given that the future actions of the robotic arm is unknown, the prediction can diverge substantiallyfrom the ground truth video. However, the model predicts movements for the objects whenever the arm pushes the object in an imaginary scenario. It also fills thebackground with random objects.
Figure 15: These are copies of Figure 6 from Franceschi et al. (2020), Figure 7 from Villegas et al.
