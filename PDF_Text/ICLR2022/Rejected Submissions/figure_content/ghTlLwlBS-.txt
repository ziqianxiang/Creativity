Figure 1: The overall pipeline of the FRL model. Given a document about the environment dy-namics, a goal description, and the observation of the initial environment, the manager module willgenerate a plan to reach the goal in a backwards manner. The worker will fulfill the sub-goals of theplan one by one according to the observation of the environment and the sub-goal. The red arrowsand the black arrows represent the data flow with and without gradient propagation respectively.
Figure 2: The structure of the worker agent.
Figure 3: The structure of the manager agent.
Figure 4: Common failurecase in FRL3.2	Results and comparisonOverall results The performance of our model with other models is show in Table 1. In thetable, worker (random) denotes a worker with a random manager, and FRL (backwards) denotesour framework with a manager generating sub-goals in a backwards manner, i.e., with the multi-hopmanager model in 2.4. FRL (forwards) is an ablation of our solution, with the manager generatingsub-goals in a forward manner. We run 5 randomly initialized worker training on 6 × 6 and 10 ×10 grid-sized RTFM games respectively. Upperbound is the performance of our worker with thegroundtruth sub-goals provided.
Figure 5: Average win rate of 5 worker trainingruns.
Figure 6: Worker trained with reward for reaching an object w/o alive requirement tends to be short-sighted. While the one trained with alive requirement tries to avoid risk.
