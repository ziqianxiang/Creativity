Figure 1: A toy demonstration of catastrophic concentration from least confidence acquisition functions.
Figure 2: Core-set radii (δ) differs between optimal, greedy and Closest-K-means core-sets (K = 4). Crossesand dots represent the labelled and unlabelled set, respectively, and translucent circles show the δ-cover. Sener& Savarese (2018) found a δ-linear upper bound on core-set loss convergence.
Figure 3: Quadrant classification of X ∈ [一1,1]2, represented as translucent dots. The first column shows theinitial dataset and model confidence. Columns progress from left to right at a rate of 20 label acquisitions perstep. Solid squares represent current acquisitions, while translucent squares represent previous acquisitions.
Figure 4: A sample acquisition from the toy quadrant dataset in Figure 3. Beam search for the top greedycore-sets ranks the configurations by increasing overall confidence (i.e., decreasing negative log confidence).
Figure 5: Ablation results on CIFAR10 showing the mean and 1 standard deviation derived from 5 randominitializations. Left: using beam search (beam=20) to greedily find the core-set configuration with the lowestlog confidence has no effect. Middle: weighing perceived distances by uncertainty (pweighted) results in betterperformance and beam search reduces variance, resulting in the highest final scores. Note that using a singlebeam in “pweighted” resulted in occasionally deteriorating performance, which was avoided using 20 beams.
Figure 6: Means and 1 standard deviation derived from 5 random initializations on image classification Us-ing the same acquisition budgets as Sener & Savarese (2018). Weighing perceived distances by uncertainty(pweighted) and using beam search (beam=10) to find the core-set configuration with the lowest log confidencecauses significant active learning improvement over vanilla core-sets.
Figure 7: We fix λc = λe = 1 and observe the probability β of our algorithm performing better than theoriginal greedy core-set algorithm across a range of distances δ from the nearest training point at differentslices of doubt z. All colored regions represent possible values of β. Orange regions occur when δ < 1∕λc,which represent the probability of quadratic convergence of core-set loss in respect to δ (see Equation 8). Allblue regions indicate values of β in which our algorithm performs at least as well as greedy core-set search.
Figure 8: Left: collected data With one feature being the value along the real axis. Middle: the true distributionof the input features. Right: Gaussian mixture with 32 components fit to the collected data. We purposefullyoverfit the GMM because the distribution of features is not knoWn a priori and We Would like high resolutionfor computing joint information later.
Figure 9: Given the trained Gaussian mixture model from Figure 8, We estimate the probability per componentfor each element. We use the probabilities to compute a modified batch-BALD joint information score (M-bBALD), Which is positively correlated With entropy across the components. The batches With the highestscores occur at dense regions but are spread out across the feature distribution. We Want to avoid redundantlabelling of elements in the left column. Top row: each figure contains eight dotted lines that represent thelocations of elements in three different batches. Bottom row: corresponding probabilities per GMM componentfor each element.
Figure 10: The distribution of modified batch-BALD scores for randomly sampled batches have a long righttail from Which We mine for likely core-set elements. Range of scores depends on the number of componentsin the Gaussian mixture model.
Figure 11: In the toy experiment With 0.5 standard deviation, clusters were mostly separable and core-setvariants dominated all baselines. Shaded area represents one standard deviation.
Figure 13: Batch-BALD effectively maximized the distance between elements of selected batches.
Figure 12: In the toy experiment With 1 standard deviation, clusters overlapped substantially and probabilisticcore-set methods formed a modest upper bound in accuracy over all baselines. Shaded area represents onestandard deviation.
Figure 14: Maximizing entropy resulted in concentrated sampling in the most uncertain regions.
Figure 15: Least confidence also concentrates sampling along uncertain regions.
