Figure 1: (Left) Comparison of predictions given context points (black dot) by the Attentive NP(ANP) and DKNP (Ours) after learning samples generated from a periodic kernel with fixed hyper-parameters. The mean (blue curve) and sigma (shaded blue area) predicted from the DKNP betterrepresent the data, including target points (red dot) compared to the ANP. (Right) Visualization ofkernel functions learned by the DKNP. As a result of kernel learning in the DKNP, the underlyingprior distribution of data can be inferred (periodic kernel in this case).
Figure 2: Model architecture of DKNPs. DKNPs estimate the predictive distribution N (y; μy, Σy)by using two attention-based deterministic paths using Multihead Cross-Attention (MCA) to modelthe mean vector (upper path) and the full covariance matrix (lower path).
Figure 3: Comparison of predictions by ANP and DKNP. The dark blue line indicates the mean, andthe light blue shade indicates one standard deviation range.
Figure 4: Visualization of the correlation matrices learned from the DKNP compared to the ground truths. Wetake a Monte Carlo estimate (averaging 1,000 kernel functions of the same type) to visualize the ground truthkernels for multiple hyperparameters at once.
Figure 5: Comparison of predictions by ANP and DKNP. The dark blue line indicates the mean, andthe light blue shade indicates one standard deviation range.
Figure 6: Visualization of the learned correlation matrices and the sampled data for Periodic-Periodic andRBF-Periodic compared to the ground truths.
Figure 7: (Left) The prior learned by DKNP. (Right) Predicted means of various models given single contextpoint shown in first column.
Figure 8: Examples of the image completion task. Based on the original image, roughly 10% of pixels (100pixels) were given as the context. The filled images generated from DKNP (Ours) are much more diverse andclearer, compared to other baselines.
