Figure 2: Algorithm 1 with different I : Training loss and validation perplexity v.s. (Left) epoch and(right) wall clock time on training an AWD-LSTM to do language modeling on Penn Treebank.
Figure 1: Algorithm 1 with different I : Training loss and test accuracy v.s. (Left) epoch and (right)wall clock time on training a 56 layer Resnet to do image classification on CIFAR10.
Figure 3: Algorithm 1 with different I : Training loss and validation perplexity v.s. (Left) epoch and(right) wall clock time on training an AWD-LSTM to do language modeling on Wikitext-2.
Figure 4: Performance v.s. # of iterations eachGPU runs on training ResNet-56 on CIFAR-10showing the parallel speedup.
Figure 5: Proportions of iterations in each epochin which clipping is triggered v.s. epochs show-ing clipping is very frequent.
Figure 6: Training loss and test accuracy v.s. (Left) epoch and (right) wall clock time on training a32 layer Resnet to do image classification on CIFAR10.
Figure 7:	Performance v.s. # of iterations each GPU runs on training AWD-LSTMs to do languagemodeling on Penn Treebank (left) and Wikitext-2 (right) showing the parallel speedup.
Figure 8:	`2 norm of gradients over epoch on training ResNet-56 on CIFAR-10.
Figure 9: Training an AWD-LSTM to do language modeling on Penn Treebank in a heterogeneoussetting where each node accesses only a different subset of the whole dataset.
Figure 10: Performance v.s. # of communication rounds each GPU conducts on training (a) Resnet-56 on CIFAR10 (b) AWD-LSTM on Penn Treebank (c) AWD-LSTM on Wikitext-2.
Figure 11: Train an AWD-LSTM to do language modeling on Penn Treebank where only a subsetof nodes participate in each communication.
Figure 12: Algorithm 1 with different I: Training loss and test accuracy v.s. (a) epoch, (b) wall clocktime, and (c) # of iterations each GPU runs on training a 56 layer Resnet to do image classificationon CIFAR10.
