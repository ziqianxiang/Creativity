Figure 1: Conceptual illustrations of two possible odd-one-out tasks, and corresponding possibleexplanations. This figure depicts odd-one-out tasks with feature dimensions of color, texture, shape,and size, and the two types of explanations we consider. Property explanations identify relevant ob-ject features, while reward explanations specify which feature(s) make the choice correct or incor-rect. (a) The second object is the odd one out, because it is a unique color. (b) The first object is theodd one out, because it is uniquely large. Explanations of incorrect choices identify all features.
Figure 2: RL agent with auxil-iary prediction of explanations.
Figure 3: Explanations help agents learn the perceptual odd-one-out tasks in both RL environments.
Figure 4: Explanations can deconfound perfectly correlated features. (a) Schematic depictions andenvironment screenshots from train and test. The agent is trained in confounded settings, where thetarget object is unique in color, shape, and texture. The agent is tested in deconfounded settings,where one object is unique along each dimension (and an additional distractor object has no uniqueattributes). (b) When trained without explanations, the agent is biased towards using color (thesimplest feature) in evaluation. (c) However, if the agent is trained with explanations that target anyparticular feature, the agent tends to use that feature to choose in the deconfounded evaluation. (3seeds per condition, chance is random choice among valid objects.)For humans, explanations help identify which specific aspects of a situation are generalizable (Lom-brozo and Carey, 2006). Could explanations also help RL agents to disentangle confounded features,and generalize to out-of-distribution tests? We explore this with a different training and testing setup(Fig. 4a). In training, one object is the odd-one-out along three feature dimensions (color, shape, and4Under review as a conference paper at ICLR 2022texture). Thus, any or all of these features could be used to solve the task—the dimensions are per-fectly confounded. In test, however, the features are deconfounded: there is a different odd-one-outalong each dimension. We explore the effect of explanations that consistently refer to a single fea-ture dimension (without mentioning others) on the agent’s behavior in deconfounded evaluation. Wetrain agents in four conditions: no explanations, color-only explanations, shape-only explanations, ortexture-only explanations. Although color, shape, and texture are confounded within each episode,
Figure 5: Explanations allow agents to meta-learn to perform experiments. (a) Each episode consistsof 3 trials where the agent gets to experiment with a magic wand in order to discover which featuredimension is relevant, followed by a final trial where it must choose the unique object along thatdimension. In this case the relevant dimension is color. In the first trials the agent transformsthe shape and texture of the objects, but is not rewarded for picking them up. In the third trial, ittransforms the color and is rewarded for picking the object up. The agent can then infer that it shouldchoose the different-colored object in the final trial. (b) In some episodes, the experiments are easy,because all the object attributes are the same, and the agent only needs to transform an object andselect that object. Agents trained with explanations learn these tasks, while agents trained withoutexplanations do not. (c) In other episodes, the experiments are harder, because the object attributesare all paired—the agent must transform one object, and then pick up another which has been madeunique. With explanations, agents learn these difficult levels as well. (4 seeds per condition.)Explanations help humans to understand causal structure (Lombrozo, 2006; Lombrozo and Carey,2006). The ability of deep learning to learn causal structure is sometimes questioned (e.g. Pearl,2019), but while theoretical limitations hold for passive learners, RL agents can intervene and cantherefore learn causal structure. Indeed, agents can meta-learn causal reasoning in simple settings(Dasgupta et al., 2019). We therefore investigate whether explanations could help agents meta-learnto identify causal structure in more challenging odd-one-out tasks in richer environments.
Figure 6: Different explanation types offer complementary, separable benefits. We compare agentstrained with all explanations or none (as above) to those trained with only property explanations(red), or only reward explanations (blue). (a) In the basic 2D tasks, either kind of explanations issufficient for learning, but having both types together is substantially faster. (b) In the 3D tasks,property explanations result in comparable learning, while reward explanations are not as effective,but still better than none. This is likely due to the memory challenge of these tasks—property ex-planations can help the agent discover what to encode to make its choice, while reward explanationscannot. (c) In the meta-learning to experiment setting, by contrast, only reward explanations (orboth together) result in any learning on the hard levels. (2 seeds per sub-type condition.)In order to better understand the benefits of explanations, we explored our results further in a varietyof analyses, ablations, and control and experiments. We outline our findings here; see Appx. A forfull results. We found that explanations as input are not helpful (Appx. A.3), and can interfere withthe benefits of explanations as targets. We also found that a curriculum of tasks that teaches objectproperties is not as effective as explanations (Appx. A.6), and that explanations are more beneficialin more complex tasks (Appx. A.5). We highlight three particularly interesting findings here.
Figure 7:	In the 2D setting, agents trained with explanations learn all dimensions, but agents trainedwithout explanations learn to fully solve the tasks only if the relevant dimension is position (theeasiest dimension), and only partly learn to solve the tasks with color (the next easiest dimension).
Figure 8:	Explanations must be behaviourally (as well as contextually) relevant to be useful in chal-lenging settings; explanations that are contextually-irrelevant are useless in every experiment. (a) Inthe basic 2D odd-one-out tasks, behavior-irrelevant explanations eventually result in relatively com-parable performance compared to full explanations, but produce much slower learning. Context ir-relevant explanations are not substantially different than no explanations. (b-c) In both easy and hardlearning-to-experiment levels, only an agent with full, behavior and context-relevant explanations isable to learn the tasks at all. Thus, more challenging task settings require more specific, behavior-releavnt explanations. (3 seeds for behavior-irrelevant/context-irrelevant conditions.)Our results (Fig. 8) show that explanations that are relevant to both situation and behavior are mostuseful, situation-relevant but behavior-irrelevant explanations can be better than nothing in somecases, and totally irrelevant explanations are not beneficial at all. Specifically, for the basic tasksbehavior-irrelevant explanations still result in some learning, but are much slower than full behavior-relevant explanations.
Figure 9: Providing explanations as agent inputs is not beneficial (performs better than no explana-tions), and is actively detrimental if explanations are also used as targets. (3 seeds per as-input con-dition, 5 seeds for main conditions.)15Under review as a conference paper at ICLR 2022A.4 Different kinds of explanations have complementary, sometimesSEPARABLE BENEFITSWe generally provided agents with both property explanations and reward explanations. Is one ofthese explanations more useful than the other? Are they redundant? To answer these questions, weconsidered providing the agent with each kind of explanation independently. We generally find thathaving both types of explanations is best, and the benefits of different types depend on the setting.
Figure 10: The benefits of explanations depend on task difficulty. We train agents in the 2D environ-ment on easier odd-one-out tasks where only two of the four dimensions are ever relevant. (a) Whenonly the easy dimensions of position and color are ever relevant, the agents trained without explana-tions learn just as rapidly as the agents trained with explanations. (b) When the agents are trained onlevels where only the harder dimensions of shape and texture are relevant, explanations still accel-erate learning substantially. However, the agents trained without explanations achieve some learn-ing in this condition, while they do not achieve any learning on these dimensions in the harder tasksused for the main experiments (see Fig. 7c-d). (2 seeds per condition.)While we generally considered tasks with many feature dimensions that might be relevant, here weshow that simpler tasks in which only two dimensions vary do not always require explanations forlearning. However, task complexity depends on both the number of possibly relevant dimensionsand the base difficulty of those dimensions. In Fig. 10 we show that explanations are not beneficialcompared to no-explanations when only the easy features of position and color are relevant. Ex-planations are still beneficial when the features are more difficult (shape and texture). But even inthis condition, the agent without explanations exhibits some learning, while it does not learn thesedimensions at all in the main experiments, where the easier dimensions are also included (see Fig.
Figure 11: Agents trained with a curriculum of tasks that teach the properties (a) do not learn the odd-one-out tasks (b) any better than agents trained with no explanations. Results are similar whetherthe agent uses a shared policy (light green) for both the curriculum and odd-one-out tasks, or usesseparate policies for each (dark green). (2 seeds per condition for curriculum conditions, 5 seeds formain conditions.)Because predicting property explanations alone can be beneficial, we next consider whether theagent could benefit from learning properties through auxiliary tasks which teach those properties,rather than through explanations. Specifically, we provide the agent with a simpler property-learningtask in 50% of episodes, where it receives a property like “red” as an input instruction, and has tochoose the corresponding object (all objects are different along each feature dimension). These tasksprovide a different way to force the agent to learn the properties of the objects. On the odd-one-outtasks, the agent receives the instruction “find the odd one out” to distinguish its goal.
Figure 12: Ablating the auxiliary reconstruction losses does not alter the pattern of results in the 2Denvironment—thus reconstruction is not necessary for learning these tasks. Comparing to the resultsin Fig. 3b, which does include reconstruction losses, shows that reconstruction losses are also notsufficient without explanations. (2 seeds per condition.)Table 1: Numerical results from main experiments/figures in each domain—mean ± standard devi-ation across seeds. Results are average performance (% correct) across evaluations during the last1% of training.
