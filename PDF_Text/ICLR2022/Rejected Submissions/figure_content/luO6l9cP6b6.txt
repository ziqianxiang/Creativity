Figure 1: An overview of our experiment paradigm. Starting with a model (e.g., pretrained BERT,GloVe-initialized LSTM, etc.), we copy it and fine-tune it on the regular and scrambled train setusing a scrambling function F. The model is then evaluated on regular and scrambled test sets. Ourpaper explores different options for F and a number of variants of our models to try to quantity theamount of transfer and identify its sources.
Figure 2: Zero-shot evaluation with the Bag-of-Word(BoW) model on scrambled datasets and the dummymodel. Numbers are the differences between the cur-rent points and the first points in percentages.
Figure 3: Performance results when fine-tuning end-to-end for different number of Transformer lay-ers. Annotated numbers are the differences between the red lines and the green lines in percentages.
Figure 4: Performance results when fine-tuning only the classifier head by freezing all proceedinglayers in BERT (red line) vs. fine-tuning end-to-end, which includes the classifier head and all pro-ceeding layers in BERT (green line). Numbers are scores for the red lines. Scoring for each task isdefined in Section 5.
Figure 5: Correlations between cosine similarities of word embeddings before fine-tuning v.s. fine-tuning with scrambled datasets. Measurements of correlations are defined in Section 7.5.
Figure 6: Accuracy of word identity probes when applied to hidden states of each layer comparingto the control task introduced by Hewitt & Liang (2019). Measurements of accuracies are defined inSection 7.5.
Figure 7: Distributions of difference in word frequency for each dataset.
