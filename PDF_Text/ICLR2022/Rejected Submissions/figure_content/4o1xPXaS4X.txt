Figure 1: The training loss and natural test accuracy of models with 1) standard training on cleandata (a) and error-minimizing poisoned data (b); 2) adversarial training on clean data (c) and error-minimizing poisoned data (d). All experiments are conducted with ResNet-18 on CIFAR-10 dataset.
Figure 2: The training loss and robust test accuracy of adversarial training with poisoned data gen-erated with (a) robust (AT) and non-robust (ST) pre-trained models; and (b) different target assign-ments. All experiments are conducted with ResNet-18 on CIFAR-10 dataset.
Figure 3: The training loss (a), the natural test accuracy (b), and the robust test accuracy (c) ofadversarial training under clean datasets, AT-pre-trained poisons, and IAT poisons, respectively.
Figure 4: Comparison among baselines (clean data, Huang et al. (2021), Fowl et al. (2021b)) andADVIN (ours) of robust training accuracy (a), natural test accuracy (b) and robust test accuracy (c)with adversarial training. All experiments are conducted with ResNet-18 on the CIFAR-10 dataset.
Figure 5: Comparison between ADVersarial Inducing Noise (ADVIN) and STanDard InducingNoise (STDIN) of robust training accuracy (a), natural test accuracy (b) and robust test accuracy (c)with adversarial training. All experiments are conducted with ResNet-18 on the CIFAR-10 dataset.
Figure 6: Confusion matrix for clean CIFAR-10 test datasets of models trained with different poi-sons(ADAIN with NextCycle label assignment, Clean training data, and ADAIN with NearSwaplabel assignment, respectively.) where rows correspond to the ground truth labels and cols corre-spond to the predicted labels.
Figure 8: The poisoned images generated from PGD200 attack with a pre-trained robust model. Tenimages are randomly selected from each of the class in CIFAR-10. Each row shows ten pairs ofclean and poisoned images. Here the poison budget Îµp is set to 32/255.
