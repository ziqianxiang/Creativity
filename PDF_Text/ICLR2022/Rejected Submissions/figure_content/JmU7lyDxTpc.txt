Figure 2: A qualitative comparison between a ResNet-18 and our analytical results. (a): Heat-map of empirical generalization error (0-1 classification error) for the ResNet-18 trained on CIFAR-10 with 15% label noise. X-axis denotes the inverse of weight-decay regularization strength and Y-axis represents the training time. (c): Heat-map of the analytical generalization error (mean squarederror) for the linear teacher-student setup with K = 100, the condition number of the modulationmatrix. (b, d): Three slices of the heat-maps for large, intermediate, and small amounts of regu-larization. Analysis: As predicted by Eqs. 13 and 15, κ = 100 implies that a subset of featuresare learned 100 times faster that the rest. Intuitively, large amounts of regularization allow for thefast-learning features to be learned by not to overfit. Intermediate levels of regularization result in aclassical U-shaped generalization curve but prevent slow features from learning. Small amounts ofregularization allow for both fast and slow features to be learned, leading to double descent.
Figure 3: Left: Phase diagram of the generalization error as a function of R(t) and Q(t) (Eqs. 13and 15). The generalization error for all pairs of (R, Q) ∈ [0.0, 0.8] × [0.0, 1.6] is contour-plottedin the background in shades of beige, with the best generalization performance being attained onthe lower right part of the plot. The trajectories describe the evolution of R(t) and Q(t) as trainingproceeds. Each trajectory correspond to a different κ, the condition number of the modulation matrixF in Eq. 2. κ describes the ratio of the rates at which two sets of features are learned. Right: Thecorresponding generalization curves for different plotted over the training time axis. Analysis: Thetrajectory with κ = 1e5 (bright yellow) starts at the origin and advances towards point A (a descentin generalization error). Then by over-training, it converges to point B (an ascent in generalizationerror). For the other trajectories with smaller κ, a first descent in generalization error occurs up tothe point A, then an ascent happens, but they no longer converge to point B . Instead, by furthertraining, these trajectories converge to point C implying a second descent.
Figure 4: The teacher-student set-up in Sec. equation 2.1. We compare the analytical solutions tosimulations performed on our teacher-student setup with d = 100, P = 50, n = 150 and We plot theerror bars over 100 random seeds. The solutions and the simulations match closely and we observedouble descent over the generalization error.
