Figure 1: Training via gradient descent of linear G-CNNs (with linearization /3) implicitly biasestowards sparse singular values for Fourier matrix coefficients of β (see Theorem 1). Figure la tracesthe D8 Fourier space sparsity over the course of training three architectures in an overparameterizedclassification task. The G-CNN converges to a Fourier-sparse linearization in contrast with fully-connected and convolutional networks. Figure Ib illustrates the uncertainty principles discussed inSection 6 which show that sparseness in (group) Fourier space necessitates being “dense” in realspace, and vice versa. See Appendix D for more visualizations of the implicit bias.
Figure 2: Cross-correlation of two functions over a group is equivalent to matrix multiplication overirreps (shown as blocks of a larger matrix here) in Fourier space.
Figure 3: Norms of the linearizations of three different linear architectures for the non-abelian groupG = (C5 × C5) × Q8 trained on a binary classification task with 10 isotropic Gaussian data points.
Figure 4: Norms of the linearizations of three different linear architectures for the non-abelian groupG = (Cr28 × Cf2s) × D8 trained using the digits 1 and 5 from the MNIST dataset.
Figure 5: Group Fourier norms for nonlinear architectures with ReLU activations show that non-linear G-CNNs seem to implicitly regularize locally. Both figures track the mean norm of the per-sample local linearizations of each network. The networks in Figure 5a use a pooling layer after theconvolutional layers to maintain invariance. Figure 5a evaluates a binary classification task usingthe MNIST digits 0 and 5. Figure 5b is obtained on networks with a final linear layer, and evaluatesa binary classification task with 10 isotropic Gaussian data points.
