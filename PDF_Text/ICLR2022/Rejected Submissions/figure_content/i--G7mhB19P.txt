Figure 1: Illustration of parametrization-dependence of EGD and independence ofNGD. Consider two parameter spaces (W1 ,W2) and two optimization trajectories ineach: one EGD, one NGD. If we map theseinto the hypothesis space (H) then EGD findsdifferent optima, but NGD finds the same.
Figure 2: Implicit regularization of EGD and NGD on logistic loss in separable classification.
Figure 3: Illustration of the neural tangentkernel in EGF, EGD, NGF and NGD (leftto right) in matrix factorization models ofdifferent depth (top to bottom). The al-gorithms take gradient steps to minimisethe squared error on a single observationat the middle of the matrix. Each panelshows how entries of the full 11 × 11matrix move from a random initial state.
Figure 4: NGD and EGD in a 1000 dimensional sparse classification task, where the ground truthclassifier has 20 non-zero components. Left: Test accuracy of EGD depends on parametrizaion.
Figure 5: Performance of unregularized EGD and NGD in rank-5 matrix completion tasks usingdifferent architectures. Left and Middle: Using deep matrix product parametrizations with L ≥ 2layers, EGD can reach low training error and identify low-rank solutions even when the number ofobservations is small. By contrast, NGD in the same problem works similarly to EGD in the naiveparametrization and fails to generalize completely. Right: 2 (orange) and 3 (green) layer modelswere initialized by collapsing randomly initialized deeper models to test the effect of initializationseparately from the effect of EGD dynamics. Initialization plays a negligible role in the inductivebias of EGD in deep matrix factorization.
Figure 6: During peer review, reviewers requested a lower dimensional variant of the experimentreported in Figure 4. Instead of 1000 dimensions, in this experiment we used D = 50, and insteadof S = 20 non-zero components, the real β had S = 5 non-zero entries. The experimental setup andhyperparameters were otherwise not changed from Figure 4. The 5-layer diagonal network performspoorly, which is likely a result of sensitivity to hyperparameters, we expect that with additional fine-tuning of the hyperparameters for this experiment, L = 4 would do at least as well as the shallowL = 1 model.
