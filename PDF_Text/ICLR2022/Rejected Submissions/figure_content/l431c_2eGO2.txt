Figure 1: DNN (Left), Ours (Right). Interpola-tion experiment on CIFAR10 to show embeddingsof the linear interpolation of two randomly pickedinput samples from class 1 (purple) and class 2(yellow). Red and green samples are classified asclass 1, and orange and blue samples as class 2. Asthe color changes from red to green, the predictiveentropy increases. Same for the color change fromblue to orange. Note, DNN classifies interpolatedpoints with very high confidence (low entropy)even if the samples shift drastically from the data.
Figure 2: Histograms of the Euclidean distanceof the embeddings (model trained on CIFAR-10).
Figure 3: First figure: Fisher criterion (α) on the embedding space for various degrees of elasticcorruption intensity levels for C10 (domain-shift). Higher α indicates more compact and separatedclusters in the feature space which is desirable. Right figures: heatmaps of the profile of entropiesas the interpolation factor between samples of two classes varies; left-most heatmap: DNN WRNon C10, the entropy is mostly zero; right-most heatmap: Mix-MaxEnt WRN on C10, there is a highentropy barrier separating the two classes, while the entropy is low close to the class clusters.
Figure 4: Accuracy, ECE and AdaECE for some corruptions that show the remarkable improvementsof using SRN for CIFAR-10-C over not using it, architecture WideResNet28-10.
Figure 5: Fisher criterion (and the spectral norm of its factors) for some corruptions that show the re-markable improvements of using SRN for CIFAR-10-C over not using it, architecture WideResNet28-10.
Figure 6: DNN (Left), Ours (Center), Mixup (Right). Interpolation experiment on CIFAR10 toshow embeddings of the linear interpolation of two randomly picked input samples from class 1(purple) and class 2 (yellow). Red and green samples are classified as class 1, and orange and bluesamples as class 2. As the color changes from red to green, the predictive entropy increases. Samefor the color change from blue to orange. Note, DNN classifies interpolated points with very highconfidence (low entropy) even if the samples shift drastically from the data. However, Mix-MaxEnt’sentropy increases as we go away from the data. Details of this experiment is provided in Section 5.2.
Figure 7: Left figure: (λ,H)-heat map for Mixup (α = 0.3) trained using WideResNet28-10 onCIFAR-10.
Figure 8: Embedding Space Reliability Plots: a simple way to visualize the confidence/overconfidenceregions in the latent space for a specific test set. Red points represent histogram bins on whichthe network is overconfident. Green points represent histogram bins on which the network isunderconfident. Yellow histogram bins can either represent well-calibrated histogram bins or areasof the projection space that contain no embedding. It is clearly visible that Mix-MaxEnt producesbetter-calibrated predictions by reducing the overconfidence far away from the centers of the clustersand by increasing the confidence close to the cluster centers. On the other hand, DNN shows largeand sharp overconfidence/underconfidence regions.
Figure 9: (Part 1 of 3) Accuracy, ECE and AdaECE for all corruptions and intensity values ofCIFAR-10-C, architecture WideResNet28-10. A similar pattern can be observed in all other cases.
Figure 10: (Part 2 of 3) Accuracy, ECE and AdaECE for all corruptions and intensity values ofCIFAR-10-C, architecture WideResNet28-10.
Figure 11: (Part 3 of 3) Accuracy, ECE and AdaECE for all corruptions and intensity values ofCIFAR-10-C, architecture WideResNet28-10.
Figure 12: (Part 1 of 3) Fisher criterion, ||SW ||F and ||SB ||F for all corruptions and intensity valuesof CIFAR-10-C, architecture WideResNet28-10. A similar pattern can be observed in all other cases.
Figure 13: (Part 2 of 3) Fisher criterion, ||SW ||F and ||SB ||F for all corruptions and intensity valuesof CIFAR-10-C, architecture WideResNet28-10.
Figure 14: (Part 3 of 3) Fisher criterion, ||SW ||F and ||SB ||F for all corruptions and intensity valuesof CIFAR-10-C, architecture WideResNet28-10.
