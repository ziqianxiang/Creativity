Figure 1: Architecture for the Object Perception by Predictive LEarning (OPPLE) networkImages at each time point are processed by the Object Extraction and Depth Perception Networksindependently. All networks use U-Net structure. The dotted boxes indicate the entire OPPLEnetwork acting at each time step (details omitted for t - 1). Motion information of each object isestimated from the spatial information extracted for each object between t-1 and t. Objects betweenframes are soft-matched by a score depending on the distance between their latent codes. Self- andobject motion information are used together with object segmentation and depth map to predict thenext image by warping the current image. The segmented object images and depth, together withtheir motion information and the observer’s motion, are used by the imagination network to imaginethe next scene and fill the gap not predictable by warping. The error between the final combinedprediction and the ground truth of the next image provides teaching signals for all three networks.
Figure 2: Example of object segmentation by different modelslate there may be fundamental limitation in the approach that learns purely from static discrete im-ages. Patches in the background with coherent color offer room to compress information similarlyas objects with coherent colors do, and their shapes re-occur across images just as other objects.
Figure 3: A-D: distribution of IoU. All models have IoU < 0.01 for about 1/4 of objects. OnlyOPPLE shows a bi-modal distribution while other models’ IoU are more skewed towards 0. E-F:object localization accuracy of OPPLE for object’s polar angle and distance relative to the camera.
Figure 4: Comparison between ground truth depth and inferred depthGround truth depth12Under review as a conference paper at ICLR 2022A.3 Network training and datasetWe trained the three networks jointly using ADAM optimization Kingma & Ba (2014) with a learn-ing rate of 3e-4, = 1e- 6 and other default setting in PyTorch, with a batch size of 24. 40 epochswere trained on the dataset. We set λspatial = 1.0, λdepth = 0.1 and Lmap = 0.005 Images in datasetsare rendered in Unity environment and downsampled to 128 × 128 resolution for training. Imagesare rendered in sequence of 7 steps each time in a room with newly selected texture and object.
