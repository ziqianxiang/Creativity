Figure 1: The VAE-SD model (Left). From an å…«-dimensional phase space input, a o-dimensionalprediction of future time-steps is derived. The loss function has three parts: the reconstruction lossis replaced by a prediction loss, the KL-divergence enforces the prior on to the latent space, andthe extra loss term enforces the supervised disentanglement of domain parameters in latent space.
Figure 2: Parameter distribution for the video pendulum test-sets. The initial angle and angularvelocity are drawn from the same uniform distribution for all test-sets. For the in-distribution test-setwe draw the pendulum length and gravity from the same distribution as during training. The OODtest-sets represent distribution shifts of increasing magnitude, where parameters are drawn fromtotally different space which has zero overlap with the training and in-distribution test-set.
Figure 3:	Disentanglement scaling methods. MAE for the first 200 time-steps. Boxplots show thetop 5 best performing sets of hyperparameters of each architecture. Both VAE-SD & VAE-SSDoutpeform the VAE in all 3 systems. VAE-SSD better captures the parameter space of the originaltest-set but in most cases VAE-SD generalizes better OOD.
Figure 4:	Disentanglement in VAE Vs MLP. MAE for the first 200 time-steps. Boxplots showthe top 5 best performing sets of hyperparameters of each architecture. While disentanglement inVAE-SD consistently improves results, disentanglement in MLP-SD does not always generalize wellOOD, producing unstable predictions for the OOD Lotka-Volterra datasets.
Figure 5:	Model predictions in phase space. Trajectories are taken from the OOD Test-Set Hard ofeach system. The model input is noisy. The circle and bold 'x' markers denote the start and end ofthe ground truth trajectories respectively.
Figure 7: Prediction quality on the video pendulum. SSIM(top) and PSNR(bottom) as a functionof the distance predicted into the future (x axis)4.3	ResultsFor each dynamical system we focus on the performance on the three test-sets, the in-distribution testset, which shares the same parameter distribution with the training set, and the two OOD test-sets(Easy and Hard), which represent an increasing parameter shift from the training data. Models arecompared on the cumulative Mean Absolute Error (MAE) between prediction and ground truth forthe first 200 time-steps. We consider this to be sufficiently long-term as it is at least 20 times longerthan the prediction horizon used during training. Long predictions are obtained by re-feeding themodel predictions back as input. This approach has been shown to work well in systems where thedynamics are locally deterministic (Fotiadis et al., 2020). A summary of the quantitative results canbe found in Figures 3 & 4 and Table 8. To account for the variability in the results, we present asummary of the best 5 runs of each model, selected by validation MAE. We generally observe thatmodel performance is correlated to the distribution shift of test-sets, and this is consistent for allsystems and models. The MAE is increasing as we move from the in-distribution test-set to the OODEasy and Hard test-sets. Nevertheless, not all models suffer equally from the OOD performance drop.
Figure 8: Absolute difference between ground truth and predictions on the test-set of the pendulumdata set.
Figure 9: Example illustration of the parameter distribution for the LV test sets. The regions do notoverlap, colors represent regions not boundaries. The OOD Easy test (orange) set does not include anyof the parameter configurations of the training and original test set (green). Respectively, the OODHard dataset (blue) does not include none of the OOD Easy or the original test set configurations.
Figure 10: Example image sequence from the video pendulum data set.
Figure 11:	Mean Absolute Error for first 200 time-steps. The bars represent the 5 experiments ofeach model with lower MAE. In all three systems disentangled VAEs provide an advantage over theother baseline. The disentanglement in MLP does not increase performance as consistently. Thescaling in VAE-SSD allows it to better capture the parameter space of the original test-set but in mostcases VAE-SD generalizes better OOD. Similarly, LSTM does not generalize well OOD.
Figure 12:	Model predictions in phase space. Trajectories are taken from the OOD Test-Set Hardof each system. The model input is noisy. The circle and bold 'X' markers denote the start and end ofthe ground truth trajectories respectively.
Figure 13:	Prediction quality on the video pendulum as a function of the distance predicted intothe future (x axis). For LPIPS lower is better.
Figure 14: Absolute difference between ground truth and predictions on the OOD Test-set Hard ofthe video pendulum data set.
