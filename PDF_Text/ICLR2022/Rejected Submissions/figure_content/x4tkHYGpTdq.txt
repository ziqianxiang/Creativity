Figure 1: The overview of our proposals. The sparse masks can have unstructured or structured patterns,which leads to training and inference efficiency. During the fine-tuning, we only train decomposed matrices U ,V and non-zero elements in S2 .
Figure 2: Left: Different method for generating S2 inBERTBase . Right: Different number of non-zero elements inS2 . Empty: no non-zero element in S2 . Decompose: matrixdecomposition method aforementioned. Magnitude: pickingelements with highest magnitude. Random: random matrix.
Figure 3: Performance comparison of two decomposition methods under different ranks. We add quadratictrend lines for better visualization quality.
Figure 4: Weight change distributions.
Figure A5: DSEE performance compared to vanilla magnitude pruning at different sparsity. MagnitudePruning: vanilla magnitude pruning which tunes W directly.
Figure A6: Convergence speed of two methods (LoRA and DSEE). The x-axis represents evaluationsteps and y-axis represents the test accuracy.
