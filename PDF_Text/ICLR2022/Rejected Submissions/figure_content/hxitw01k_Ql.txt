Figure 1: A Update scheme from Cover & Hellman (1970) vs B our update scheme. Using thesame notation as Cover & Hellman (1970), yn is the reward obtained by playing the arm en and Tnis the memory state (not to confuse with the transition probability T of Sec 2.1). To make the linkwith Sec 2.1 in our setup the state sn correspond to the tuple (en , yn , Tn , kA , kB), the observationon to the triplet (en , yn , Tn) and the action an is represented by the purple and green arrows. Thered arrow can be seen as part of the conditional probability T (sn+1 |sn, an). C The column ofconfidence policy, that can be used in the RAM case, exemplified for M = 8. The memory statesare organized into two columns. The distribution p0 initializes the agent’s memory into states 1Aor 1B with equal chance. The agent keeps playing the same arm, moving up and down a columndepending on the reward, unless it is in the memory state 1 (it then switches arm after a negativereward). Once the agent reaches a state at the top of a column, it can only step down with smallprobability e if a negative reward is obtained. This two-column arrangment effectively double thenumber of memory states, by creating 2M = 16 distinct states. In this policy, the value of thememory can be viewed as a measure of the confidence in the arm being played.
Figure 2: Left Probability to play the worst arm q vs. the reset probability r for different memorysizes M and two values of μ (the difference between the mean outcome of the two arms). Right qvs. memory size M for different r and μ indicated in legend. The solid lines show the analyticalresult corresponding to column of confidence policy (CCP), whereas symbols show the results of thelearning algorithm (see Sec 2.2) for a RAM memory with a policy initialized close to the predictedoptimal column of confidence policy (π0 ≈ πCCP + 10-4). Learning does not find strategies thatperform better than the optimal column of confidence policy, even for large values of r, indicatingthat it is a local optimum. In this figure, error bars would be smaller than the symbols.
Figure 3: Necklace policy with memory of the m = 4 last arms played and rewards obtained (Me-mento memory cf. 2.6). Memory states are organized into 3 (5 if we consider the two end states)cycles of arms played, called necklaces in combinatorics. The memory state also contains the re-wards obtained, but these are not explicitly shown here for the sake of readability. Most of time,the agent stays in the same necklace by playing the oldest action he remembers. Each necklacehas 2 inputs and 2 outputs states (some states can be both input and output). The agent has a finiteprobability to leave its current necklace only when the output state is maximally informative: all 4rewards are + for A and - for B to move to the left, or the opposite to move to the right. Thickerarrows represent high probability transition (deterministic in some situations); dashed arrows repre-sent input/output transitions between necklaces; and thin arrows represent the small probabilities toleave the two end states.
Figure 4: Dynamics of the optimization algorithm for different initialization methods. Probabilityto play the worse arm q vs. algorithm time t (time defined in (3)) for different seeds. 20 initializationseeds are shown in light color and the median is shown in solid color. The wall time is caped to 1 hourper optimization. A-B RAM with M = 8 and M = 20, we compare the random initialization, thelinear initialization where the jumps in memory states are initialized to be contiguous, the columnsinitialization corresponding to linear initialization with the extra constraint that the last action isrepeated except for the memory state 1 and the CCP initialization, very close to the column ofconfidence policy (i.e. π0 ≈ πCCP + 10-4, a difference that explains why the red curves candecrease). C-D q vs. t for the Memento memory m = 3 and m = 4. We compare the randominitialization with the cycles initialization that repeats the oldest action, except if all the rememberedplays correspond to a maximally informative event (during training a path between the cycles has tobe learned) and the necklace where 0 and 1 has to be learned. E-F q vs. t for randomly initializedpolicies. The values of m (3 and 4) and M (16 and 64) are chosen such that the total memoryneeded to perform these strategies Meff is identical in each panel. The dashed line corresponds tocalculation of q for the CCP and for the necklace policy (for which we only have a prediction forr = 0). In this figure μ = 0.1.
Figure 5: Markov chainConsider a random walk on a chain of sites with probabilities ri to move from site i to i + 1 and lito move from i to i — 1 (Figure 5). First, we want to compute the probability Pi→j (with i ≤ j) that,starting at site i, the walker visits the site j + 1 before the site i — 1. Note that Pi→j only dependson {lk }1=2 and {rk }k=j Note also that according to this definition Pi→ = iɪ-. We also definePu as the probability to reach i 一 1 before j + 1 by starting in j, then we have PiJi = r⅛-.
