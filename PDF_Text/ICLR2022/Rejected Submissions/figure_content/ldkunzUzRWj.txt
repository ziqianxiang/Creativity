Figure 1: Illustration of finding useful negative items for pairwise loss optimization: (a) is the initialstage of optimization when it’s easy to get one negative item; (b) shows that useful negative itemsare more difficult to get as the learning process moves forwards; (c) sampling negative items fromuniform distribution equals to do unbiased random walk on fully connected item-item graph; (d)presents an alternative solution depending on a bias random walk.
Figure 2: Maximum (solid line)and minimum (dash line) imbal-ance value along with different de-cay parameter β on Yelp and Ama-zon Movies&Tv datasets.
Figure 3: Visualizing the projection of learned embeddings with the classical uniform sampling andthe proposed sampler VINS by the T-SNE algorithm into two-dimensional space (colored by vertexdegree levels, red-top 25%, blue-bottom 25%, green-the rest).
Figure 4: Illustration to show the connection be-tween class-imbalance and gradient vanishment.
Figure 5: Evolution of maximum/minimum imbalance value of different sampling methods.
Figure 8: The evolution of estimated rank variable ψ(ri).
Figure 9:	Evolution of maximum/minimum imbalance value of different sampling methods.
Figure 10:	Ranking performance on F1/NDCG metric of shalloW and deep models.
