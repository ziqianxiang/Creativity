Figure 1: The optimization process of thevariational parameters Φ(n) using evolution-ary search. A. Some states are selected asparents. B. Each child undergoes mutation.
Figure 2: Graphical Representation ofthe Model Architecture Used in Nu-merical Experiments.
Figure 3: Denoising results for House. D comparesPSNRs (in dB) obtained with different ‘zero-shot’ mod-els using a fixed patch size and number of latents (see textfor details). E lists PSNRs for different algorithms withdifferent optimized hyper-parameters. The top categoryonly requires the noisy image. The middle requires addi-tional information such as noise level (KSVD, WNNM,BM3D) or additional noisy images with matched noiselevel(n2v1). The bottom three algorithms use large cleandatasets. C depicts the denoised image obtained withTVAE for σ = 50 in the best run (PSNR=30.03 dB).
Figure 4: Comparison of data representations learned by denoising image patches (cf. App. B.6).
Figure 5: Inpainting results for House (50% missing pixels) and Castle (50% and 80% missing).
Figure 6: Generic and general VAE decoding model (left), VAE model with standard continuouslatents (center), and the VAE model with binary latents (right) of Eqn. (1) (right).
Figure 7: Typical collection of methods used to optimize the encoding model of VAEs. Left: Meth-ods of the standard procedure to optimize VAEs. Middle: Examples of additional methods appliedto maintain the standard VAE procedure also for VAEs with discrete latent variables. Right: Alter-native direct discrete optimization of VAE encoding models.
Figure 8: TVAE Training on Simple Bars Data:Noiseless Output of the TVAE’s DNN for the8 Possible One-hot Input Vectors Over SeveralTraining Epochs. Generating parameters are inthe last row.
Figure 9:	Correlated Bars Test. The plot showsthe ratio between inferred and ground-truth log-likelihoods log pΘ (~x) of data points with inter-esting bar combinations. The inferred values arereported below the data points themselves.
Figure 10:	Generative Parameters for the Correlated Bars Test (left); ELBO Values over Epochs for10 runs (center); Example Datapoints and Samples from the Generative Model (right).
Figure 11: Reliability of different evolutionary operator combinations (cf. App. A.1) on bars test(see App. B.2 for details). Reliability is quantified in terms of the normalized difference betweenthe lower bound at the last training epoch and the log-likelihood computed using the ground-truthgenerating parameters. For the strategy labeled as ‘random’, we randomly sampled states from aBernoulli distribution with p(zh = 1) = -1.
Figure 12: ELBO Gain of TVAE Compared to Linear VAE with Binary Latents (on 16 × 16 imagepatches).
Figure 13: Inpainting of the ‘House‘ Image with TVAE.
Figure 14: Inpainting of the ‘Castle‘ Image with TVAE. Left Original image. Center Trainingimage (top, 50% missing pixels, bottom, 80% missing pixels). Right Inpainting result with TVAE(top, 50% missing pixels, bottom, 80% missing pixels).
Figure 15: Log-likelihood estimates and sparsity obtained for CIFAR-10. For comparison, weconsidered Gumbel-softmax VAE (GSVAE; Jang et al., 2016)) and VLAE (Park et al., 2019a).
Figure 16: Results for ‘Zero-Shot’ Audio Denoising. Panel A depicts the dB-scaled amplitudespectrogram of an example from the LJ-Speech dataset. Panel B was obtained after adding Gaussiannoise (σ = 0.1). Panels C-F show the denoising results of GA (Lu & Loizou, 2008), MMSE-STSA(Ephraim & Malah, 1984), DCDAP (Narayanaswamy et al., 2021b) and TVAE. Panel G reports SNRand PESQ improvements averaged over five examples per dataset (the improvement is computed bycomparing the denoised waveform with the target waveform; see App. B.7 for details).
