Figure 1: Comparison of LMSA and MSA operation process. Up: Standard multi-head self-attention mechanism, Down: Low-relation multi-head self-attention mechanism. qkdim representsthe dimension of the Q or K variable after being compressed.
Figure 2: Overall structure of Swin and Cswin Transformer.
Figure 3: Comparison of Q, K, V generated by different self attention mechanisms.(a): Q, K, Vgenerated by standard Self-Attention.(b): Q, K, V generated by CvT Self-Attention3 methodologyUnlike text sequence features, visual features often contain more nonobjective information. Thedevelopment of Transfomer in the computer vision field shows that the self-attention mechanismcan effectively extract image features, and there is still much room for improvement. However, thestandard self-attention mechanism has a computational complexity of O(N 2). As the image size4Under review as a conference paper at ICLR 2022increases, the self-attention mechanism consumes a lot more computing time and storage space,which limits the efficiency of the self-attention mechanism in visual tasks. To reduce the complexityof the self-attention mechanism and improve the computational efficiency, in this paper, we proposeLow-Relation Mutil-Head Self-Attention, which aims to reduce the low-quality data that has a smalleffect on the calculation result in the self-attention.
Figure 4: Data effcient analysis ofMSA and LMSA at different thresholds6Under review as a conference paper at ICLR 2022Table 2: Comparison of complexity between different efficient attentionEFFCIENTATTENTION	Complexity_________Memory Compressed Attention O(N2 ∙ D)Sparse Attention	O(N√N ∙ D)Longformer	O(N ∙ (k + g) ∙ D)Axial Attention	O(N ∙ (H + W) ∙ D)Refomer	o(n ∙ log(N) ∙ D)Synthesizer	O(N2 ∙ D)LMSA	O(N2 ∙ Dq,k3.4 Comparison with other Efficient AttentionWhen using Transformer for image feature extraction tasks, in order to reintegrate feature infor-mation and reduce feature scales, it is often used in conjunction with downsampling, which makesHigh-Level features have more dimensions, this also shows that it is necessary to optimize the num-ber of dimensions for visual tasks. Table 2 lists the comparison of the computational complexityof the main Efficient Transformer, the work of dimensional optimization is very rare among them.
Figure 5: Effect of different compression ratio on results.Cifar-100 is used to experiment onSWin transformer without pre-training.Left:InflUenCe of different compression ratio on accu-racy.Right:Data effective rate under different compression rates5	ConclusionThis paper proposes a novel variant of Efficient Attention: LMSA, which limits the number ofsignals that generate relational matrices, improves the quality of Query and Key signals and greatlyreduces the computational consumption of MSA. Experiments have verified that LMSA can beequal to MSA in accuracy and even has advantages. This brings the confidence to LMSA insteadof MSA to embed various types of Transformers. We demonstrate that dimensional consistency isunnecessary in self-attention calculations, choosing an appropriate LMSA can achieve competitiveresults while saving computing resources.
