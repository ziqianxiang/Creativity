Figure 1: Left: An overview of the proposed SQL algorithm for text generation. Text generationis challenging due to sparse reward (i.e., the rewards of all intermediate steps are 0) and large ac-tion space (i.e., large vocabulary). Our SQL formulation enables several key algorithmic featuresas highlighted with yellow color, including (1) the combined on- and off-policy updates for the bestof both, (2) bridging the final non-zero reward to directly supervise the Q-value estimation at inter-mediate steps for learning stability, and (3) simultaneously updating the Q-values of all candidateactions for efficiency. Right: We explore diverse applications of the text-generation RL algorithm.
Figure 2: Soft Q-Learning with path consistency learning (PCL) objectives, where we illustrate witha vocabulary of size 3. Left: Single-step objective (Eq.9), where for each (st , at), the computationinvolves step t and t+ 1. Dashed boxes in dark green and gray indicate the regression target, wherethe intermediate reward rt is often 0 due to sparsity. The gradient is applied to parameters θ at stept (indicated by orange color). Right: Multi-step objective (Eq.11) which aggregates from step t allthe way to T . In this way, the final-step non-zero reward rT is used as the regression target.
Figure 3: Left: entailment generation performance plotted against diversity (average of H1 andH2). Circles represent results of top-p sample outputs, and triangles represent results of beam-search outputs. Right: entailment attack performance against diversity (average of H1 and H2).
Figure 4: Conditioning on a topic (e.g., ‘‘science’’), the prompt generator automatically pro-duces a short piece of text (i.e., prompt) such that, by prepending the prompt to the input text, thepretrained LM will generate continuation sentences of the particular topic. The subsequent compo-nents serve as the reward functions to train the prompt generator. The discrete steps (highlighted inred) make previous gradient-based prompt tuning approaches not applicable here.
Figure 5: Average topic accuracy.
Figure 6: Entailment generation performance plotted against diversity (average of H1 and H2).
Figure 7: Training curves on validation sets. Left: Training curves on E2E with best hyperparameterconfigurations. Middle: Training curves on E2E with varying reward scale. Right: Training curveson CommonGen with varying reward scale.
