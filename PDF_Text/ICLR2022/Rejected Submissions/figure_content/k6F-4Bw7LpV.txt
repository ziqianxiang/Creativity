Figure 1: The setup and result of Experiment 1. The CIFAR-10 train set is labeled as either Animalsor Objects, with label noise affecting only cats. A WideResNet-28-10 is then trained to 0 train erroron this train set, and evaluated on the test set. Full experimental details in Appendix C.2conditioned on a subgroup (cat), the distribution of the true labels is close to that of the classifieroutputs. Third, this is not the behavior of the Bayes-optimal classifier, which would always outputthe maximum-likelihood label instead of reproducing the noise in the distribution. The networkis thus behaving poorly from the perspective of Bayes-optimality, but behaving well in a certaindistributional sense (which we will formalize soon).
Figure 2: Feature Calibration. (A) Random confusion matrix on CIFAR-10, with a WideResNet28-10 trained to interpolation. Left: Joint density of labels y and original class L on the train set. Right:Joint density of classifier predictions f (x) and original class L on the test set. These two jointdensities are close, as predicted by Conjecture 1. (B) Constant partition: The CIFAR-10 train set isclass-rebalanced according to the left panel distribution. The center and right panels show that bothResNets and MLPs have the correct marginal distribution of outputs, even though the MLP has hightest error.
Figure 3: Feature Calibration. (A) CIFAR-10 with p fraction of class 0 → 1 mislabeled on thetrain set. Plotting observed noise on classifier outputs vs. applied noise on the train set. (B) Multiplefeature calibration on CelebA. (C) TV-distance between (L(x), f (x)) and (L(x), y) for a variant ofExperiment 1 with error on the distinguishable partitions (ε). The error was changed by changing thenumber of samples n.
Figure 4: Distributional Generalization for WideResNet on CIFAR-10. The confusion matriceson the train set (top row) and test set (bottom row) remain close throughout training.
Figure 5: Distributional Generalization in Experiment 2. Joint densities of the distributionsinvolved in Experiment 2. The top panel shows the joint density of labels on the train set:(CIFAR_Class (x), y). The bottom panels shows the joint density of classifier predictions onthe test set: (CIFAR_Class (x), f (x)). Distributional Generalization claims that these two jointdensities are close.
Figure 6: Joint density of (y, Class(x)), top, and (f (x), Class(x)), bottom, for test samples (x, y)from Experiment 2 for an MLP.
Figure 7: Feature Calibration with original classes on CIFAR-10: We train a WRN-28-10 onthe CIFAR-10 dataset where we mislabel class 0 → 1 with probability p. (A): Joint density of thedistinguishable features L (the original CIFAR-10 class) and the classification task labels y on thetrain set for noise probability p = 0.4. (B): Joint density of the original CIFAR-10 classes L and thenetwork outputs f(x) on the test set. (C): Observed noise probability in the network outputs on thetest set (the (1, 0) entry of the matrix in B) for varying noise probabilities pTo show that this is not dependent on the particular class used, we also show that the same holds for arandom confusion matrix. We generate a sparse confusion matrix as follows. We set the diagonal to0.5. Then, for every class j, we pick any two random classes for and set them to 0.2 and 0.3. Wetrain a WRN-28-10 on it and report the test confusion matrix. The resulting train and test densitiesare shown in Figure 2A. As expected, the train and test confusion matrices are close, and share thesame sparsity pattern.
Figure 8: Feature Calibration for Decision trees on UCI (molecular biology). We add label noisethat takes class 2 to class 1 with probability p ∈ [0, 0.5]. The top row shows the confusion matrixof the true class L(x) vs. the label y on the train set, for varying levels of noise p. The bottom rowshows the corresponding confusion matrices of the classifier predictions f (x) on the test set, whichclosely matches the train set, as predicted by Conjecture 1.
Figure 9: Decision trees on UCI (wine). We add label noise that takes class 1 to class 2 withprobability p ∈ [0, 0.5]. Each column shows the test and train confusion matrices for a given p. Notethat this decision trees achieve high accuracy on this task with no label noise (leftmost column).
Figure 10: Decision trees on UCI (mushroom). We add label noise that takes class 0 to class 1 withprobability p ∈ [0, 0.5]. Each column shows the test and train confusion matrices for a given p. Notethat this decision trees achieve high accuracy on this task with no label noise (leftmost column).
Figure 11: Feature Calibration for multiple features on CelebA: We train a ResNet-50 to performbinary classification task on the CelebA dataset. The top row shows the joint distribution of this tasklabel with various other attributes in the dataset. The bottom row shows the same joint distributionfor the ResNet-50 outputs on the test set. Note that the network was not given any explicit inputsabout these attributes during training.
Figure 12: Coarse partitions as distinguishable features: We consider a setting where the originalclasses are not distinguishable, but a superset of the classes are distinguishable.
Figure 13: Distributional Generalization. Train (left) and test (right) confusion matrices for kernelSVM on MNIST with random sparse label noise. Each row corrosponds to one value of inverse-regularization parameter C . All rows are trained on the same (noisy) train set.
Figure 14: Distributional Generalization for WideResNet on CIFAR-10. We apply label noisefrom a random sparse confusion to the CIFAR-10 train set. We then train a single WideResNet28-10,and measure its predictions on the train and test sets over increasing train time (SGD steps). Thetop row shows the confusion matrix of predictions f(x) vs true labels L(x) on the train set, and thebottom row shows the corresponding confusion matrix on the test set. As the network is trained forlonger, it fits more of the noise on the train set, and this behavior is mirrored almost identically on thetest set.
