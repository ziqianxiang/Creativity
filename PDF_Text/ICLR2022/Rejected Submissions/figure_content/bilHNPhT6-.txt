Figure 1: Each point is a solution found for a different tradeoff α. (a) DiME can find solutions on a concavePareto front (right) whereas linear scalarization (LS) cannot. (b,c) For humanoid, DiME finds better solutionsand obtains higher hypervolume. Above (i.e., higher task reward) and to the right (i.e., lower cost) is better.
Figure 2: Per-tradeoff performance for a representative subset of tasks; Appendix D contains plots for allfifteen tasks. On some tasks, the best solution found by DiME (any variation) obtains significantly higher taskperformance than the best found by LS. In addition, DiME multi can train a single policy for a range of trade-offs,that performs comparably to learning a separate policy for each tradeoff, after the same number of learning steps.
Figure 3: Finetuning learning curves, for DiME (left three) and LS (right three). (Appendix D contains plots forall five tasks.) For both DiME and LS, learning the tradeoff (orange) converges to better performance than fullyimitating the teacher (dashed purple), while learning as quickly. Error bars show standard deviation for ten seeds.
Figure 4: (a) This plot shows the per-episode task reward and action norm cost, rather than the average acrossepisodes. Note that the linear scalarization (LS) cannot find solutions along the Pareto front for walk, because itis likely concave. In contrast, DiME can find solutions that compromise between the two objectives. (b) DiMEfinds similar Pareto fronts as MO-MPO, a state-of-the-art multi-objective RL algorithm.
Figure 5: DiME multi trains a single trade-off-conditioned policy, that finds a similar Pareto front as DiMEdoes for humanoid (a) walk and (b) run. Each plot for DiME multi shows five trained policies, for differentrandom seeds; these seeds vary somewhat in their performance because the policy learns different strategies(e.g., spinning, running sideways, etc.). Each policy is conditioned on trade-offs α linearly spaced between 0and 1. The color denotes the trade-off. For DiME, αtask = 1; DiME multi uses a convex combination (i.e.,αtask = 1 - αcost).
Figure 6:	With the original objective scales for humanoid run, both linear scalarization (left plot) and DiME(right) obtain similar performance. When the action norm cost is scaled up by ten times, DiME’s performance isunaffected because its trade-off setting is invariant to reward scales—note the two clusters of differently-coloredpoints. The color denotes the tradeoff α and the marker symbol denotes whether the action norm cost is scaledby ten times. Note that in the plots, the y-axis is the unscaled value of action norm cost.
Figure 7:	Per-objective learning curves for humanoid run. During training, DiME (below) is able to improve onboth objectives simultaneously. In contrast, LS (above) can typically only improve on one objective at a time.
Figure 8: Per-tradeoff performance for all nine Control Suite tasks from RL Unplugged. Across all nine tasks,the best solution found by DiME (any variation) obtains either higher or on-par task performance than the bestfound by linear scalarization (LS). In addition, DiME can train a single policy for a range of tradeoffs (DiMEmulti, dark orange and dark blue), that performs comparably to learning a separate policy for each tradeoff(orange and blue), after the same number of learning steps.
Figure 9: Across nine tasks from D4RL, DiME and LS (CRR) perform on-par.
Figure 10: Learning curves for finetuning, for DiME (top row) and linear scalarization (LS, bottom row). α = 1corresponds to fully imitating the behavioral prior, while α = 0 corresponds to learning from scratch. Theoptimal fixed tradeoff α depends on the task. For both DiME and LS, learning the tradeoff (orange) converges tobetter performance than fully imitating the behavioral prior (dashed purple), while learning as quickly. The errorbars show standard deviation across ten seeds.
