Figure 1: GrASP using its model to perform lookaheadplanning with actions/options selected via the affordancemodule. The agent plans by either constructing a K-arydepth-D search tree or a partial search tree through UCT.
Figure 2: Top: Learning performance of GrASP agents and the TD3 baseline on 3 hierarchical taskswith pretrained continuous options. Bottom: Visualizations of object-centric options discovered bythe GrASP SA-3 agent in the Collect task. Left panel shows the trajectories generated by the optionchosen by each of 3 affordance heads, in 3 different colors, from three different starting states for theagent. Right panel shows the same for a single start state but varying positions of objects.
Figure 3: Learning performance of four GrASP agents and the model-free baseline TD3 on nineDM Control Suite tasks. UCT-50 is the best performing UCT-based GrASP agent; GA-4 is thebest performing of the Goal-conditioned Affordance agents; A-8 is the best performing of theunconditioned-Actions agents; GA-1 is of special interest because it collapses the tree to a singletrajectory; see text. Learning curves for all agents are in the Appendix.
Figure 5: Top: Finger-Spin; Bottom: Walker-Walk. Distributions of differences in returns betweenthe GA-4 agent planning with discovered affordances and agents following the policies of singleaffordance-mapping heads.
Figure 4: A visualization of the GA-4 agent switching between actionsoutput by the K = 4 distinct affor-dance heads. Partial episodes arefrom Fish-Swim.
Figure 6:	Learning performance of GrASP and Dreamer agents on four DM Control Suite tasks. Wepresent GA=4 version as it was one of the best performing GrASP agents in Section. 3.2.
Figure 7:	First Row: Fish-Swim; Second: Ball-in-Cup-Catch; Third: Cheetah-Run; Fourth: Fish-Upright; Fifth: Reacher-Easy. Each row corresponds to a DMControl Suite Task and summarizes thedistributions of differences in returns between the GA-4 agent planning with discovered affordancesand agents following the policies of single affordance-mapping heads.
Figure 8:	Visualizations of object-centric options discovered by GrASP GA-3 in the Collect task.
Figure 9:	Learning performance of different variants of GrASP agent and the model-free baselineTD3 on nine DM Control Suite tasks. Recall that GA-1 is of special interest because it collapses thetree to a single trajectory. We presented a subset of the learning curves from this Figure as the mainresult in our main text.
Figure 10:	Learning performance of different variants of GrASP agents and the model-free TD3baseline on 2 hierarchical tasks with pretrained continuous options. All the agents shown here learnto select from the same pretrained space of option policies, thus making a fair comparison over thehierarchical agents. A subset of the learning curves from this Figure was presented in our main text.
Figure 11: Left: Learning performance of GrASP agents with different planning depths D = 1, 2, 3.
