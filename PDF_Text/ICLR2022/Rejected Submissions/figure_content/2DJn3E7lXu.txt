Figure 1: How well the different predictors rank the test architectures, depending on the trainingset size and averaged over the five selected datasets. Left plots: absolute Kendall’s Tau rankingcorrelation, higher is better. Right plots: same as left, but centered on the predictor-average.
Figure 2: A trained XGBoost prediction model on normalized ImageNet16-120 raspi4-latency testdata. Left: The latency prediction (y-axis) for every architecture (blue dot) is approximately correct(red line). Center: The same data as on the left, the distribution of deviations made by the predictor(blue) and a normal distribution fit to them (orange). Right: A mixed distribution can simulatetypical deviation distributions as that in the center plot.
Figure 3: An example of predictor-guided architecture selection, std=0.5. Left: The simulated pre-dictor makes an inaccurate latency prediction for each architecture (blue), resulting in the selectionof the supposedly-best architectures (orange dots). Even though the predicted Pareto front (orangeline) may differ significantly from the ground-truth Pareto front (red line), most selected architec-tures are close to optimal. Right: Same data as left. The true Pareto front (red) and that of theselected architectures (orange). Simply accepting all selected architectures results in a Mean Re-duction of Accuracy (MRA) of 1.06%, while verifying the predictions and discarding inferior resultsimproves that to 0.43%. The hypervolume (HV, area under the Pareto-fronts) is reduced by 0.07.
Figure 4: Simulation results, with the standard deviation of the predictor deviations and the resultingKT correlation on the x-axis. Left: Verifying the hardware predictions can significantly improve theresults, even more so for better predictors. Center: The drops in average accuracy are dependant onthe dataset and hardware metric. Right: Considering more candidate architectures and using betterprediction models improves the results; larger values are better.
Figure 5: Basic NAS-Bench-201 / HW-NAS cell design. Each of the six orange paths is finalizedwith exactly one out of five candidate operations {Zero, Skip, Convolution 1× 1, Convolution 3×3,Average Pooling 3×3}.
Figure 6: How the data of each selected HW-NAS-Bench dataset is distributed (not normalized).
Figure 7:	How the data of each selected TransNAS-Bench-101 dataset is distributed (not normal-ized). Since all architectures are measured for latency on the same hardware, the resulting datasetsare much less diverse than the HW-NAS-Bench ones.
Figure 8:	Fit time (in seconds) of predictors to data, depending on the training set size. By far themost expensive methods are network-based. However, a significant portion of this time is spent onthe hyper-parameter optimization prior to the actual fitting.
Figure 9:	Further examples of predictor deviation distributions, as visualized in the center of Fig-ure 2. Left: Linear Regression on CIFAR100, edgegpu, energy consumption. Center: SupportVector Machine on Jigsaw. Right: small MLP on ImageNet16-120, raspi4, latency.
Figure 10: The sampled values of gaussian+uniform fit the measured predictor mistakes better thana single distribution, as they are roughly normally distributed, but include outliers.
Figure 11: Similar to Figure 3. When the discovered Pareto set is considerably worse than the truePareto set, it is possible for the Mean Reduction of Accuracy of the Pareto subset (MRApareto) to beworse than the average over all architectures (MRAall). This naturally happens more frequently forworse predictors with a high sampling std. and low KT correlation. Consequentially, the differencebetween MRAall and MRApareto is wider for better predictors (see Figure 4). Additionally, all ofthe selected non-Pareto-front members are clustered in a high-latency area and redundant with eachother. This emphasizes the limitations of just considering drops in accuracy, as the hardware metricaspect is ignored. In this case, the predictor-guided selection failed to find a low-latency solution.
Figure 12:	Examples to explain measurement methods.
Figure 13:	Standard deviation over the predictor deviations (x axis) and Kendall’s Tau correlation(y axis), for the trained predictors on HW-NAS-Bench (left) and in simulation (right). The simulatedpredictor inaccuracies are slightly pessimistic (low KT), but still match the true values.
