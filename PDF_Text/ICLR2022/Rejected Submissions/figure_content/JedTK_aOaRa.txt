Figure 1: With sufficient consensus, the best query-utility tradeoff obtained lies in a regime ofσG where the data-dependent bound is used. In the 1st row, we maximize the σG of T PATE whilemaintaining sufficiently high values of the performance metrics. The chosen values for (σG, T) are9, 1.8 for Pascal VOC, 10, 3 for MIMIC-CXR, and 7, 2.8 for CheXPert and 7, 2.7 PadChest. Whenthere is a lack of consensus (on PadChest), we see that the data-independent `2 (L2-DI) mechanismbound outperforms all others. For a well chosen T, there is little-to-no impact on the consensus ofthe data-dependent regime (c.f. Binary PATE and T PATE which leverage the data-dependent boundwhen it reduces privacy loss). Because of this, T PATE achieves a competitive query-utility tradeoff.
Figure 2: Using CaPC to improve model performance. Dashed lines represent mean balancedaccuracy (BAC). We retrain a given model using additional data labelled by all other models from thesame ensemble. We observe a mean increase of BAC by 2.0 percentage points on CheXpert.
Figure 3:	Powerset PATE outperforms Binary PATE as more labels are generated randomly,i.e., p=0.5. We use 50 teachers, privacy noise σGNMax = 7, and the privacy budget ε is set to 20. Ifall votes are random, then both Binary and Powerset PATE fall back on the data independent bound.
Figure 4:	Binary PATE outperforms Powerset PATE when they both have a similarly high gap.
Figure 5: Examplehierarchical struc-ture of labels in Chestradiography setting.
Figure 6: The label distribution for each multi-label dataset (Pascal VOC, CheXpert, MIMIC-CXR,and PadChest).
Figure 7: Binary vs Powerset PATE: number of answered queries. We compare the number ofanswered queries vs number of k first labels selected from the Pascal VOC and CheXpert datasets.
Figure 8: Binary vs Powerset PATE: performance. We compare the drop in performance in termsof accuracy (ACC), area under the curve (AUC), and mean average precision (MAP) between BinaryPATE and Powerset PATE as we increase the scale of the Gaussian noise σGNMax . We select the first11 labels in Pascal VOC and all 11 labels from CheXpert. We keep the privacy budget ε = 20. Theprivate multi-label classification with PATE performs better (preserves higher values of the metrics)using the Binary approach.
Figure 9: Binary vs Powerset PATE: CDF of gaps (differences between vote counts). We use 50teacher models trained on the Pascal VOC and CheXpert datasets. These are raw gaps without addingany noise to the vote histograms. Most of the gaps are relatively large (> 40) for the Binary PATEper label, which shows that teachers are confident about the answers to queries. The average gap forthe Powerset method is relatively low (only 18 for Pascal VOC and 13 for CheXpert).
Figure 10: Binary vs Powerset PATE: gaps and metrics for Pascal VOC. We use 50 teachermodels. These are raw gaps without adding any noise to the vote histograms.
Figure 11: Binary vs Powerset PATE: gaps and metrics for CheXpert. We use 50 teacher models.
Figure 12: Value of the metric vs τ -clipping of votes in `2 norm. For a given τ, we plot accuracy(ACC), balanced accuracy (BAC), and mean average precision (mAP).
Figure 13: Value of the metric vs τ -clipping of votes in `1 norm. For a given τ, we plot accuracy(ACC), balanced accuracy (BAC), and mean average precision (mAP).
Figure 14: Value of the metric versus noise standard deviation σ. For a given σ, we plot accuracy(ACC), balanced accuracy (BAC), and mean average precision (mAP).
Figure 15: Compare methods: number of answered queries vs σ - using the Pascal VOC dataset,with τι = 3.4 (set for '1-norm clipping, and τ2 = 1.8 set for T-PATE and '2-norm clipping. Thered vertical line denotes the selected value of σ.
Figure 16: Retraining with privacy budget ε = 10 for the Pascal VOC dataset.
Figure 17: Cross-domain retraining With CaPC. We train 10 models on PadChest (PC) andcompare their performance on PadChest test set against the ensemble of these models (PC Ensemble),ensemble of 50 CheXpert models (CX Ensemble), and finally retrain the 10 PadChest models viabinary multi-label PATE using the CheXpert ensemble (after CaPC on CX).
Figure 18: Using CaPC to improve the weakest models. Dashed lines represent mean values ofthemetrics: Balanced Accuracy (BAC) and AUC. We retrain a given model using additional CheXpertdata labelled by the ensemble of all the other models trained on CheXpert. All metrics are improvedafter retraining by around 0.05 on average.
Figure 19: The analysis of the balanced accuracy (y-axis) of votes from the ensemble as We increasethe (sigma of the Gaussian) noise (x-axis) from the binary multi-label PATE. We search for varianceof the noise qg that can preserve high BAC for the ensemble. Left: 50 CheXpert models with trainand test on CheXpert, qg ≤ 9.0 preserves more than 0.69 of BAC. Middle: 50 CheXpert modelswith train on CheXpert and test on PadChest test set, qg ≤ 9.0 preserves more than 0.75 of BAC.
