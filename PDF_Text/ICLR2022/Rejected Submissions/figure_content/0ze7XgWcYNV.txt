Figure 1: An illustration of the Hari framework in a human-assisted navigation task. An agentcan only observe part of an environment and is asked to find a mug in the kitchen. An assistantcommunicates with the agent and can provide it with information about the environment and thetask. Initially (A) it may request more information about the goal, but may not know enough aboutwhere it currently is. For example, at location B, due to limited perception, it does not recognize thatit is in a living room and stands next to a couch. It can obtain such information from the assistant.
Figure 2:	Analyzing the behavior of the RL-learned interaction policy (on validation en-vironments).
Figure 3:	Analyzing the effect of simultane-ously varying the cost of the Cur, Goal,Sub, Do actions (on validation environ-ments), thus trading off success rate versusnumber of actions taken.
