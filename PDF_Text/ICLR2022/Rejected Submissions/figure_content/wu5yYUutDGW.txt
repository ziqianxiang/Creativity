Figure 1: Examples of the video scene segmentation. In each row, we visualize the shots includingsimilar visual cues (e.g., characters, places, etc.) with the same colored border.
Figure 2: Overall pipeline of our proposed framework, BaSSL.
Figure 3: Illustration of four pre-training pretext tasks.
Figure 4: An example in each row shows a sequence of shots sampled from the same scene wherethere exists no ground-truth scene-level boundary. Our method finds a pseudo-boundary shot (high-lighted in red) that divides a sequence into two pseudo-scenes (represented by green and orangebars, respectively) so that semantics (e.g., places, characters) maximally changes.
Figure 5: Comparison between existing approaches and ours for video scene segmentation. Theexisting approach focuses only on learning shot-level representation given by shot encoder (fENC).
Figure 6: Visualization of similarity (below) between shot representations in randomly sampledconsecutive shots (above). We observe that the shot representations are clearly clustered as addingpretext tasks one by one.
Figure 7: Comparison between the ground truth scene boundaries and the discovered pseudo-boundaries based on the DTW algorithm. The examples are sampled from the MovieNet-SSegdataset. All boundary shots are highlighted in red.
