Figure 1: LatentKeypointGAN generates images (first column) with associated keypoints (secondcolumn), which enables local editing operations, such as moving, switching, and adding parts.
Figure 2: Overview. Starting from noise z, LatentKeypointGAN generates keypoint coordinates, kand their embeddings w. These are turned into feature maps that are localized around the keypoints,forming conditional maps for the image generation via SPADE blocks. The images are generatedby toRGB blocks. At test time, the position and embedding of keypoints can be edited separately.
Figure 3: Location and scale editing. The first column is the source and the last the target. Theimages in-between are the result of the following operations. First row: pushing the eye keypointdistance from 0.8x to 1.2x. Note that the marked eye keypoints in this row are slightly shiftedupward for better visualization. Second row: interpolating the hair keypoint to move the fringefrom right to left. Third row: scaling the keypoint location and, therefore, the face from 1.15x to0.85x. Fourth row: interpolating all keypoint locations, to rotate the head to the target orientation.
Figure 5: Disentangled keypoint embeddings on FFHQ. The leftmost images are the source andthe rightmost images are the target. The cross landmarks on the first column denote the parts tobe changed. The second column shows the difference between the original image and the changedimage. The third to the second to last columns show the interpolation between the original imageand the target image.
Figure 4: Disentangled Background.
Figure 6: Editing on Bedroom. Left first row: interpolating the keypoint embedding on the curtain.
Figure 7: Editing on BBC Pose. The first row shows the source image and the second row theediting results. First three columns: the human appearance is swapped with the small target im-age. Middle three columns: changing the position to the one in the overlayed target. Last threecolumn: changing the background (the bottom right corner shows the difference).
Figure 8: Keypoints. We show the keypoints on each dataset.
Figure 9: Image editing quality comparison. We compare the image editing quality with both,supervised (left) and unsupervised (right). LatentKeypointGAN improves on the methods in bothclasses.
Figure 10: Lorenz et al. (2019) on LSUN Bedroom. (Left) Detected keypoints. The keypointsare static and do not have semantic meaning. (Right) Reconstructed images. The reconstructioncompletely fails.
Figure 11: (Left) Spatially Adaptive Normalization. SPADE takes two inputs, feature map andstyle map. It first uses batch normalization to normalize the feature map. Then it uses the stylemap to calculate the new mean map and new standard deviation map for the feature map. (Right)Keypoint-based ConvBlock.
Figure 12: Detailed architecture. (Left) LatentKeypointGAN generator. The numbers in theparenthesis is the output dimension of the Keypoint-based ConvBlock. For example, (512, 4, 4)means the output feature map has a resolution of 4 × 4 and the channel size is 512. The toRGBblocks are 1 × 1 convolutions to generate the RGB images with the same resolution as correspondingfeature maps. (Middle) LatentKeypointGAN discriminator. The number in the last parenthesisis the output dimension. For example, (512, 4, 4) means the output feature map has a resolution of4 × 4 and the channel size is 512. At each resolution, we apply two convolutions, one with stride 2 todownsample feature maps and one with stride 1 to extract features. Leaky ReLU (Maas et al., 2013)is used after all convolutions except the linear layer in the last.. (Right) Progressive Training.. Theadapting period is the same as PGGAN (Karras et al., 2018) and StyleGAN (Karras et al., 2019). Inthe non-adapting period, we do not use the linear combination.
Figure 13: Ablation study on multiplicative embedding. We show the T-SNE and PCA visualiza-tion of embeddings learned on FFHQ. The first two column shows keypoint embeddings and the lasttwo column shows keypoint embeddings and background embedding.
Figure 14: Ablation study on architecture. We show the keypoints for different architectures.
Figure 15: Ablation study on hyperparameters. (Left) Face generation with on FFHQ with τ =0.002. We use the red circle to mark the artifacts in the images. (Right) Face generation on FFHQwith number of keypoints 10 (top) and 6 (bottom). More keypoints lead to a stronger influence ofthe keypoint embedding. However, the 6-keypoint version still provides control, e.g., glasses, nosetype, and pose. From left to right: original image, replaced background (difference map overlayed),replaced keypoint embeddings (target image overlayed), exchanged eye embeddings, and keypointposition exchanged.
Figure 20: Ablation Test onBackground loss.
Figure 16: Visualization for different combinations of number of keypoints and keypoint sizeτ . If both, the number of keypoints and the keypoint size τ , are small (top left), the keypoint istrivial. If both of them are large (bottom right), the keypoints distribute uniformly over the imagesinstead of focusing on parts.
Figure 17: Editing on different combinations of number of keypoints K and keypoint sizeτ. K=1, 6. Column 1: original image; column 2: part appearance source image used to swapappearance; column 3: the combined image with shape from the original images and the appearancefrom the part appearance source image; column4: we randomly swap a single keypoint close to themouth; column 5: resulting difference map when changing the keypoint in the 4th column; column6: move the face to the left and add another set of keypoints on the right; column 7: removingall keypoints. If τ = 0.0002, the keypoints are trivial, and cannot be used to change appearance.
Figure 18: Editing on different combinations of number of keypoints K and keypoint sizeτ. K=8,12. For a small τ = 0.0002, the keypoint is trivial. When τ is large the background isentangled (K = 8, τ = 0.02) in some cases. We found the combinations of (K = 12, τ = 0.0005)and (K = 12, τ = 0.01) both give the best editing controllability.
Figure 19: Editing on different combination of number of keypoints K and keypoint size τ .
Figure 21: (Left) GAN Loss Importance. Without gradient penality + logistic loss, as in SPADE,keypoint coordinates remain static. (Right) Scheduling the keypoint generator learning rate.
Figure 22: (a) Training process. We visualize the image generated during the training. (b) Fail-ure cases. The left top two images show asymmetric faces: different eye colors for the man anddifferent blusher for the woman. The middle top two images show the entanglement of hair andbackground. The right top two images show that the pose of head is hidden in the relative positionsof other keypoints than the keypoints on the head. We visualize the process of removing parts at thebottom. We sequentially remove the left eye, right eye, mouth, nose, and the entire face. Due to theentanglement of hair and background, the hair remains even if we remove the whole face.
