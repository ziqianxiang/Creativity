Figure 1: Deep Explore Option framework. During interactions, the state is passed through both theExplorer and Exploiter Networks and one of them acts according to the Explore Option. Transitionswith intrinsic reward are stored in the replay buffer, the corresponding PER priorities are assignedby both Exploiter and Explorer according to their respective loss. The mini-batch is then createdusing transitions according to each agent’s priorities following 2-PER sampling of both trees, andused for training of all components.
Figure 2: Deep Explore Option architectures against Rainbow baselines in Atari. Shaded areas rep-resent ± standard deviation over 5 runs (3 for hyperparameter studies). Top row: easy-explorationgames; middle row: hard-exploration games. Bottom row, left: hyperparameter studies for ws andcswitch in Gravitar.
Figure 3: Explorer interaction frequency and Q-value estimates in all games for the Multi archi-tecture.
