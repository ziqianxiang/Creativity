Figure 1: An illustrative example of our algorithm. The x-axis is the time step (for the onlineprediction setup), and the y-axis is the prediction. We look at the 75% and the 25% quantileof the prediction. The purple markers are the true labels. The true label is initially drawn fromUniform[-2, 2] but drifts to Uniform[-5, -1]. Upper Left: The original predictions that is notcalibrated because distribution drifted at time step 5. Upper Right: The “basic” algorithm: we movethe quantiles up or down to achieve calibration. However, the prediction is sometimes infeasible (e.g.
Figure 2: Left: An illustration for Section 3.2 where we draw analogy to a mechanical system. Weattach a “spring” between neighboring quantiles that resists being compressed or stretched. Thefinal prediction Zkt is the equilibrium state of this mechanical system. This is the natural way tocompute the compromise of several conflicting requirements. Right: Illustration of the effect of PIDin Section 3.3. Here we plot the error Fkt - αkt when the initial prediction systematically under-predicts. P corresponds to the basic algorithm, and the error oscillates around 0. PI corresponds toadding an integral term, and the error converges to 0. PID adds an additional derivative term, whichreduces overshoot and improves convergence.
Figure 3:	Example predictions on the stock prediction dataset (for Amazon). Left is conformalcalibration only, and right is conformal calibration + our method. We plot both the sequence of pre-dictions, and the reliability diagram (i.e. the relationship between target frequency αk and empiricalfrequency FkT /T). Amazon growth has been faster than expected by analysts at times (such as steps21-26, or 30-33), conformal calibration failed to capture this, while our method rapidly adapted tothis distribution shift within a few time steps.
Figure 4:	Reliability diagram for different methods on stock prediction. Each blue line is the reli-ability diagram for a single stock. The yellow shaded area is the 99% confidence interval for thereliability diagram of an oracle predictor (i.e. it is αk ± b,(T) for δ = 0.01). From left to rightwe use original predictions, conformal calibration, our method, and an regret minimization baseline.
Figure 5: The decision loss reduction if we use conformal / ours instead of the original expertpredictions for a COVID response decision task. The shaded area is 1 standard deviation computedby bootstrap sampling geographical locations, and the dashed line is the total case number. Bothconformal / ours reduce decision loss (so all curves are below 0), but our method reduces loss morecompared to conformal when distribution changes drastically, i.e. when case numbers spike.
Figure 6: Comparison of loss values between synthetic data and other datasets, where each pointcorresponds to a hyper-parameter candidate which is obtained during hyper-parameter search. Thex-axis and y-axis show loss values on synthetic dataset and other datasets, respectively. The perfor-mance on simple synthetic data is highly correlated with the performance on real data.
Figure 7: Evaluating the robustness to hyper-parameter choice. The x-axis and y-axis shows thepinball loss and ece, respectively. The star is the hyper-parameter we fix throughout the experiments.
Figure 8: Example predictions on the COVID prediction dataset. Left is conformal calibration only,and right is conformal calibration + our method. We plot both the sequence of predictions, andthe reliability diagram (i.e. the relationship between target frequency αk and empirical frequencyFkT /T).
Figure 9: Example predictions on regression benchmark datasets with distribution drifts. From topto bottom, we apply ’linear shift’, ’scale shift’, ’jump’, and ’cycle’, respectively. We plot both thesequence of predictions, and the reliability diagram (i.e. the relationship between target frequencyαk and empirical frequency FkT /T).
