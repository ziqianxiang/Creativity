Figure 1: (a) GP regression with an RBF ker-nel illustrates uncertainty-based OOD detection.
Figure 2: Standard deviation σ(f*) of the predictive posterior of BNNs. We perform Bayesianinference on a mixture of two Gaussians dataset considering different priors in function space inducedby different architectural choices. The problem is treated as regression task to allow exact inferencein combination with GPs (a, b, c). Predictive uncertainties for finite-width networks are obtainedusing HMC (d, e, f).
Figure 3: Standard deviation σ(f*) of the pre-dictive posterior using GPs with common ker-nel functions. GP regression is performed on thesame dataset as in Fig. 2.
Figure 4: The importance of the choice ofweight prior p(w). Here, we perform 1d regres-sion using HMC with either (a) a width-awareprior N(0, H) or (b) a standard prior N(0, σW).
Figure 5: OOD challenges whenencoding function space proper-ties into weight space prior. (a)Samples from a GP prior p(f |X).
Figure 6: Generalization and OOD detection. Mean andthe first three standard deviations of the predictive posteriorfor different function space priors.
Figure 7: Sampling from regions of low uncer-tainty. Here, we treat epistemic uncertainty (mea-sured as σ(f*)) as an energy function and performrejection sampling (black dots). If uncertaintyfaithfully captures p(x), these samples should re-semble in-distribution data.
Figure S1: NNGP kernel values y∕k(x*, x*) for various architectural choices. Note, that theNNGP kernel value k(x*, x*) represents the prior variance of function values under the induced GPprior at the location x* (cf. Eq. 1). As emphasized in Sec. 4, k(x*, x*) is constant for an RBF kernel,which has important implications for OOD detection, such as that a priori (before seeing any data) allpoints are treated equally. This is not the case for the ReLU kernel, which has an angular dependenceand depends on the input norm (cf. Eq. 5 and its dependence on k0(x, x0)). The kernel induced bynetworks using an error function (Erf) or a cosine as nonlinearity seems to be more desirable in thisrespect (note the scale of the colorbars).
Figure S2: Mean f* and standard deviation σ (f*) of the posterior p(f* | X* ,X, y) for GPregression with NNGP kernels for various architectural choices, such as number of layers or non-linearity. Note, that Tanh and Erf nonlinearities are quite similar in shape, which is reflected inthe similar predictive posterior that is induced by these networks. We use the analytically knownkernel expression for the Erf kernel (Williams, 1997), and use MC sampling for the Tanh network(cf. Fig. S3).
Figure S3:	Monte Carlo error when estimating NNGP kernel values. Eq. 4 requires estimationwhenever no analytic kernel expression is available (for instance, when using a hyberbolic tangent non-linearity as in Fig. S2). Here, we visualize the error caused by this approximation for ReLU and errorfunction (erf) networks when computing kernel values k(x*, x*). Eq. 4 requires a recursive estima-tion of expected values, where we estimate each of them using N samples (N ∈ [102, 103, 104, 105]).
Figure S4:	NNGP kernel values do not generally reflect Euclidean distances. This figure showskernel values k(xq, xp) plotted as a function of the Euclidean distance kxq - xpk2 (using pairsof training points from the two Gaussian mixtures dataset). As outlined in Sec. 4 and SM B, theinterpretation of an RBF kernel as Gaussian kernel that can be used in a KDE of p(x) is important tojustify implied OOD, at least for low-dimensional problems. Unfortunately, the kernels induced bycommon architectures do not seem to be distance-aware and are thus not useful for KDE.
Figure S5: Standard deviation σ (f*) of the predictive posterior. We perform Bayesian inferenceon a dataset composed by two concentric rings (SM D) comparing posterior uncertainties of GPswith NNGP kernels and an RBF kernel with those obtained by finite-width neural networks. Onlythe function space prior induced by an RBF kernel or RBF network causes epistemic uncertaintiesthat allow outlier detection in the center or in between the two circles. However, the RBF network’suncertainties decrease with increasing input norm, which can be counteracted by further increasingσμ (Sec. 3).
Figure S6: Standard deviation over the logits f* and mean and standard deviation over the sig-moid outputs s(f*) for a 2D classification problem. We perform approximate Bayesian inferencewith HMC for finite-width networks on a mixture of two Gaussians dataset considering differentpriors in function space induced by different architectural choices. The problem is now treated asclassification task using HMC to sample from the posterior. We show the standard deviation σ(f*) oflogits f* in (a, b, c), the standard deviation σ(s(f*)) of the predicted probability for the input beingin the positive class in (d, e, f), and the corresponding mean s(f*) (g, h, i). The ReLU network has 5hidden units, the cosine has 100 hidden units, and the RBF network has 500 hidden units.
Figure S7: 2D visualizations of the SplitMNIST posterior for a GP with RBF kernel (l = 5).
Figure S8: Continual learning with uncertainty-based replay. As mentioned in Sec. 7, if uncer-tainty should only be low on in-distribution data, a generative model can be obtained by samplingfrom regions of low uncertainty. Here, we use this idea to realize a replay-based continual learningalgorithm (Lin, 1992) without the need of storing data or maintaining a separate generative model. Inthis case, we split a polynomial regression dataset into two tasks (denoted by the black vertical bar).
