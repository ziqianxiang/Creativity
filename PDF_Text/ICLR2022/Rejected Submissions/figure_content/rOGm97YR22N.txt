Figure 1: Magnitude of the statesâ€™ er-ror propagation in time-continuous re-current neural networks gives rise to thevanishing or exploding of the gradient(first two models). mmRNNs are a so-lution to keep a constant gradient flowto avoid these phenomena in modelingirregularly sampled data.
Figure 2: The adjoint method makes numericalerror when computing the gradients. a) Continu-ous vector field implied by an ODE. b) NumericalODE-solvers realize a discrete trajectory on thevector field. c) The adjoint ODE creates a time-reversed vector field. d) Discrete trajectory of theadjoint ODE-solver diverges from the trajectory ofthe forward simulation due to discretization andnumerical imprecision.
Figure 3: Left: Illustration of how vanishing gradients make RNN training difficult when the dataexpress long-term dependencies. The prediction error can be thought of as a teaching signal indicatinghow the dynamics should be changed to minimize the loss. The vanishing gradient of the ODE-RNNmakes the teaching signal weaker when propagating it back in time. Conversely, the teaching signalstays near-constant in mmRNNs. Right: The resulting loss surfaces of the ODE-RNN is much steeperthan mmRNN, making the training difficult.
Figure 4:	Dense codingaA = 4	aA =...
Figure 5:	Event-based codingFigure 6:	Dense and event-based coding of the same time-series. An event-based coding is moreefficient than a dense coding at encoding sequences where the transmitted symbol changes onlysparsely.
Figure 6:	Dense and event-based coding of the same time-series. An event-based coding is moreefficient than a dense coding at encoding sequences where the transmitted symbol changes onlysparsely.
