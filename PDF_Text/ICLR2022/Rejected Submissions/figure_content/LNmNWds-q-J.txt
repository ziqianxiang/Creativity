Figure 1: The considered problem setting and the motivation for our 3D Infomax pre-training.
Figure 2: We first pre-train a 2D network fa by maximizing the mutual information (MI) between itsrepresentation za of a molecular graph G and a 3D representation zb produced from the molecules’conformers Rj. In step 2, the weights of fa are transferred and fine-tuned to predict properties.
Figure 3: The single conformer example shows a batch of three molecular graphs as input to the2D network with the corresponding three conformers as input to the 3D model. During 3D pre-training, the contrastive loss L enforces high similarity between latent representations that comefrom the same molecule (green arrows) while encouraging dissimilarity otherwise (red arrows).
Figure 4: The left plot shows the MAE for the QM9’s homo and GEOM-Drugs’ Gibbs propertywhen varying the number of GEOM-Drugs’ conformers used during pre-training. The right plotdepicts the MAE when using different numbers of molecules of GEOM-Drugs during pre-training.
Figure 5: Depiction of two groups of molecules where all the molecules in the top row have the sameBemis-Murcko scaffold (Bemis & Murcko, 1996) which is different from the scaffold to which thetwo molecules in the bottom row belong. We can easily obtain the scaffold of a molecule usingRDKit (Landrum, 2016).
Figure 6: a) The average number of conformers necessary to cover a certain amount of Boltzmannweight in GEOM-Drugs. For a given amount of cumulative Boltzmann weight on the horizontalaxis, the vertical axis shows the average number of conformers necessary to pass that threshold. b)Histogram of how many molecules there are in GEOM-Drugs with a certain amount of conformers.
