Figure 1: Accuracy vs. parameters,trained and evaluated on ImageNet-1k.
Figure 2: ConvMixer uses â€œtensor layout" patch embeddings to preserve locality, and then appliesd copies of a simple fully-convolutional block consisting of large-kernel depthwise convolutionfollowed by pointwise convolution, before finishing with global pooling and a simple linear classifier.
Figure 3: Implementation of ConvMixer in PyTorch; see Appendix D for more implementations.
Figure 4: Patch embedding weights for a ConvMixer-1024/20 with patch size 14 (see Table 2).
Figure 5: Patch embedding weights for a ConvMixer-768/32 with patch size 7 (see Table 2).
Figure 6:	Random subsets of 64 depthwise convolutional kernels from progressively deeper layersof ConvMixer-1536/20 (see Table 1).
Figure 7:	A more readable PyTorch (Paszke et al., 2019) implementation of ConvMixer, where h =dim, d = depth, p = patch_size, k = kernel_size.
Figure 8:	An implementation of our model in exactly 280 characters, in case you happen to know ofany means of disseminating information that could benefit from such a length.
