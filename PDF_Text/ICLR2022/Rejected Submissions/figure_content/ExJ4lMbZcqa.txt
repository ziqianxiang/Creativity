Figure 1: Visual cues reveal key factors influencing reverb effects on human speech audio. Forexample, these audio speech samples (depicted as waveforms and spectrograms) are identical lexically,but have very different reverberation properties owing to their differing environments. In the church,reverb is strong, in the classroom it is less, and when the speaker is distant from the camera it is againmore evident. We propose audio-visual dereverberation to learn from both modalities how to stripaway reverb effects, thereby enhancing speech quality, recognition, and speaker identification.
Figure 2: Audio-visual rendering for a Matterport environment. Left: bird's-eye view of the 3D environment.
Figure 3: VIDA model architecture. We convert the input speech to a spectrogram and use overlappingsliding windows to obtain 2.56 second segments. For visual inputs, We use separate ReSNet18networks to extract features er and ed, which are fused to obtain ec. We feed the spectrogram segmentSr to a UNet encoder, tile and concatenate ec with the encoder,s output, then use the UNet decoderto predict the clean spectrogram Ss. During inference, we stitch the predicted spectrogams into a fullspectrogram and use Griffin-Lim (Griffin & Lim, 1984) to reconstruct the dereverberated waveform.
Figure 4: Left: Cumulative WER against PESQ of the input speech (a) and the speaker distance.
Figure 5: Example input images, anechoic spectrograms, reverberant spectograms and spectrogramsdereverberated by VIDA (top is from a scan, bottom is a real pano). The speaker is out of view in thefirst case and distant in the second case (back of the classroom). Though both received audio inputsare quite reverberant, our model successfully removes the reverb and restores the source speech.
Figure 6: Cumulative WER against PESQ of the input speech (a) and the speaker distance.
