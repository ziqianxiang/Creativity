Figure 1: Network architecture used for PRT task, With YO=φr (ψ∣nc(O)). The figure shows howrelational learning is performed on the source MNIST data set (to learn e.g. that digit 5 is greater than3). As discussed in Section 6, moving to the target domain (to learn that a stack of blocks is greaterthan another) involves training a new ^^nc/dec together with a subset of the φr relation-decoders(with fixed parameters), where the rest are held out as zero-shot transferred relation-decoders.
Figure 2: [Top] Relation-decoder prediction accuracy per relation and model, in the source (left) andtarget domains. Relations are abbreviated on the x-axis by {S: isSuccessor, P: isPredecessor,E: isEqual, G: isGreater, L: isLess}, with a red highlight identifying a relation included as aguide for ψYenc . [Bottom] Impact of different values of β for each relation-decoder (mean acrossall relations in the source domain (left) and mean for held-out relations only in the target domain(right). Notably, it can be seen that our model (DC) is not impacted while all other models show adecrease of accuracy in the target domain.
Figure 3: Modified [Top] Con-A values for each relation-decoder model, referenced to source (left)and target (right) domains (lower values better). [Bottom] Con-I values (lower values better) foreach relation-decoder model referenced to source (left) and target (right) domains, where stackedbars are for formula: transitivity (white), asymmetry (magenta) and reflexivity (black). In all plots,darker color shades denote higher values of β, corresponding to greater disentanglement pressurefrom the β-VAE. In top-left and bottom plots, blue, green and red groups show results for data-embeddings, interpolation and extrapolation embeddings respectively (see main text for details).
Figure 4: Example of two BlockStacks data set images.
Figure 5: Depiction of a set of DC relation-decoders for binary relations isGreater, isLess, isE-qual, isSuccessor and isPredecessor. Each DC relation-decoder (for each relation) has a one-hotmask, ur (that is in this example the same across relations), which ensures only the zeroth dimen-sions of the embedding arguments are compared, giving zi,0 and zj,0 .
Figure 6: GC values (higher values better), for each relation-decoder model referenced to sourcedomain. Darker color shades denote higher values of β, corresponding to greater disentanglementpressure from the β-VAE. Blue, green and red groups show results for data-embeddings, interpola-tion and extrapolation embeddings respectively (see main text for further details regarding the datasplits).
Figure 7: Analysis of domain-specific information retention by the β-VAE when using differentrelation-decoders for ordinality relation decoding. We attempt to predict the overall BlockStacksstack height on the final fixed embeddings obtained after isSuccessor relation-decoder alignment.
