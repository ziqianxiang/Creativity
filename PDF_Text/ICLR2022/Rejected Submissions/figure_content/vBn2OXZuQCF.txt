Figure 1: (Left) Edges between (class, domain) pairs indicate how “connected” they are byaugmentations (e.g., cropping, colorization). Small crop sizes in contrastive learning can increase theconnectivity between disparate domains. For example, α denotes the probability that augmentationsconnect examples of the same class across different domains. (Right) When α and β, which measuresthe connectivity between images of the same class or same domain respectively, are larger than γ(different domain and different class connecivity), contrastive pre-training produces a feature spacewhere training on labeled data from the source domain (Real — green, filled) achieves high accuracyon the target domain (Sketch — gray, hollow).
Figure 2: (Left) Illustrative toy example with 2 domains and 2 classes,where each class-domain pair is a single node in the graph. Edge weightsdenote connectivities (probability of two class-domain pairs augmentinginto the same point). (Middle) When α (same-class-different-domainconnectivity) is greater than γ (connectivity across different domainsand classes), the domain representation are oriented so that linear clas-sification on the source classifies the target accurately. (Right) Whenα <γ, no linear classifier can label both the source and target accurately.
Figure 5: Ablation of connectivity by removingunlabeled data points near the margin of adomain classifier. Removing these points fromthe pre-training dataset reduces target accuracymore than removing a random subset of thesame size.
Figure 4: Plot of our measure of connectivityratios against target accuracy of contrastivepre-training on DomainNet. Our quantity highlycorrelates r=0.84 with target accuracy.
