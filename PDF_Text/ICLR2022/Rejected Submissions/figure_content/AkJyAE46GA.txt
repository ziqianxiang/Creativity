Figure 1: Active learning can resolve task ambiguity, which is especially salient in few-shotsettings. Here, the provided training data leaves the model unsure of the task: is it to predictthe shape or the color of the object? Pretraining enables models to disentangle and weigh variouscompeting features, making them good active learners that can choose disambiguating examples(e.g. the blue square), resolving this task ambiguity.
Figure 2: Uncertainty sampling outperforms random sampling on all datasets, especially onminority classes. Shaded regions represent 95% CIs (Gaussian approx.).
Figure 3: All types of uncertainty sampling outperform random sampling on iWildCam.
Figure 4: Uncertainty sampling identifies and upsamples disambiguating examples. For bothWaterbirds and Treeperson, uncertainty sampling selectively acquires examples where the spuriousand core features disagree. Y-axis: percentage oversampling of uncertainty over random sampling.
Figure 5: Uncertainty sampling upsamples both visible and latent minority subgroups. Fractionof Amazon examples acquired by random and uncertainty sampling, stratified by star rating andproduct category. Uncertainty sampling preferentially acquires examples with lower star ratings andrarer product categories, despite the latter attribute not being visible to the model.
Figure 6: Both causal and spurious features are more linearly separable in pretrained models.
Figure 7: Uncertainty sampling only provides gains when using pretrained models. S-R50, M-R50, and M-R101 correspond to the BiT-S-R50x1, BiT-M-R50x1, and BiT-M-R101x1 pretrainedmodels, respectively, while R50-NP and R101NP correspond to ResNet models which are not pre-trained. Shaded regions represent 95% CIs (Gaussian approx.).
Figure 8: Active learning improves upon random sampling for Treeperson when using ViT-B/16. However, the gains are not as large as for BiT, perhaps because ViT was pretrained for almost8x fewer epochs.
Figure 9: Acquisitions for Treeperson using ViT. The ViT model requested labels for minorityclasses at a significantly lower rate than did the BiT model.
Figure 10:	Waterbirds background mismatch dose-response experiment. As the train and testdistribution diverge, the benefit that active learning provides increases.
Figure 11:	All pretrained models acquire disambiguating subgroups much more efficientlythan their non-pretrained counterparts. The pretrained models do not simply oversample basedon the the bird or the background; instead they oversample disambiguating examples which havemismatched backgrounds. By contrast, the non-pretrained model only oversamples images with awater background, and accordingly is less able to perform well on the balanced validation set.
Figure 12: Pretrained models ask for labels of images with mismatched backgrounds, whilenon-pretrained models do not.
Figure 13: Without any finetuning, pretrained models already embed images into a usefulfeature space.
Figure 14: Accuracy on CIFAR10 as more samples are acquired with uncertainty vs random acqui-sition. Shaded regions represent 95% CIs (Gaussian approx.). All runs are with the pretrained modelBiT-M-R50x1. (a): The usual CIFAR10. (b) and (c): CIFAR10 where the training set has 5 out of10 classes from which 90% samples are removed. (b) is accuracy on validation split with the samedistribution as train. (c) is on balanced validation split.
