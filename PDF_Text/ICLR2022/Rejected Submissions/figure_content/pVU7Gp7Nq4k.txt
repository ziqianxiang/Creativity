Figure 1: The mechanism of representation mitosis. (a) We analyse the final representations ofdeep neural networks (DNN), namely the activities of the last hidden layer of neurons (light blue) Wefocus on the performance and the statistical properties of randomly chosen subsets of wc neuronswhich we call “chunks”. In the chunked network shown here, wc = 5 out of 9 neurons are held, andused to predict the output. (b) As we increase the size of the chunk wc in a state-of-the-art DNN, herea DenseNet40, the test error of the chunked network (orange line) becomes similar to the test error ofa full network of width W = wc (blue line). In this regime, which is reached when wc is larger thana threshold w* (shaded area) the error approaches its asymptotic value error∞ as a power-law w-1/2(dashed line). (c) Illustration of three final representations for networks of increasing width. As thewidth of the network increases, additional neurons fit to new features of the data (red neuron). As thenetwork width goes beyond a critical width W * , additional neurons instead copy features alreadylearnt from data, and form what we call clones. This mechanism, which we call representationmitosis, is suggested by the wc-1/2 decay of the chunk error, and by the statistical analysis we presentin this paper.
Figure 2: Scaling of the test errorwith width for various DNN The av-erage test error of neural networks withvarious architectures approaches the testerror of an ensemble of such networksas the network width increases. Thenetwork size shown here is the widthof the final representation. For largewidth, we find a power-law behaviourerror - error∞ 8 W-1/2 across datasets and architectures. Full experimentaldetails in Sec. 2.1ιo-0516 32 64 128 256 512 1024network size-∙- pMNIST-FC5∣-∙- CIFAR10-WR28-∙- CIFAR100-WR28—CIFAR10-DenseNet40same width (Fig. 2). This implies that a model obtained by selecting a random chunk of wc > wc*
Figure 3: Scaling of the test error of chunks of neurons extracted from the final representationof wide NNs We plot how the test error of chunked networks approaches the error of an ensemble ofchunked networks as the chunk size wc increases. Chunks are formed by selection a number of wcneurons at random from the final representation of the widest networks: a FC5 on pMNIST (widthW = 512), and Wide-ResNet-28 for CIFAR10 (W = 512) and CIFAR100 (W = 1024). The shaded-1/2regions indicate regions where the error of the chunks with wc neurons decays as wc .
Figure 4: The three signatures of representation mitosis (i) The training errors of the full networks(blue) and of the chunks taken from the widest network (orange) approach zero beyond a criticalwidth / chunk size, resp. (panels a-c). (ii) The final representation of the widest network can bereconstructed from a chunk using linear regression (1) with an explained variance R2 close to 1 (bluelines in panels d-f). (iii) The residuals of the linear map can be modeled as independent noise: weshow this by plotting the mean correlation of these residuals (green line, panels d-f), averaged over100 reconstructions starting from different chunks. A low correlation at high R2 indicates that thechunk contains the information of the full representation with some statistically independent noise.
Figure 5: The onset of mitosis during training. a: As in Fig. 4, we show the mean correlation ofthe residuals of the linear reconstruction of the final representation from chunks, but this time as afunction of training epochs. A small correlation indicates that the reconstruction error in going fromchunks to final representation can be modelled as independent noise. Data obtained from the sameWR28_8 trained on CIFAR10 as in Fig. 4. b: Training error during training for chunks of differentsizes. After the network has reached zero training error at 〜160 epochs, continuing to train improvesthe training accuracy of the chunks. c: Test and training error during training for the full network.
Figure 6: A network trained without regularization on CIFAR10. a: the test error of chunksof a Wide-ResNet28-8 trained without data augmentation and weight decay (blue) and for a well-regularized network (orange, taken from Figure 3-b). b: Mean correlation between residuals of thelinear reconstruction of the full representation from chunks of different sizes for two networks: onetrained without data augmentation and weight decay (thick lines), and one using state-of-the-arttechniques (thin lines, same data as in Fig. 5-a).
Figure 7: Dynamics of mitosis on CIFAR100 a:As in Fig. 4, we show the mean correlation of theresiduals of the linear reconstruction of the final representation of a Wide-Resnet28_8 from chunks,but this time as a function of training epochs. A small correlation indicates that the reconstructionerror in going from chunks to final representation can be modelled as independent noise. b: Trainingerror of chunks of a Wide-Resnet28_8 and its full layer representation. From epoch 150 to epoch 185the training error of the chunks with size 128/256 decreases below 0.5%, while for smaller chunkssizes it remains above 5%. Random chunks with size larger than 128/256 can fit the training set, thushaving the same representational power as the whole network on the training data. For W > 128/256the test accuracy is decaying approximately with the same law as that of independent networks withthe same width (see Fig. 3). This picture suggests that for CIFAR100 the size of a clone is 128/256,slightly larger than the size of the clones in CIFAR10. c: Training and test error dynamics for thesame Wide-ResNet28_8. After epoch 150 the training error of the full network remains consistentlysmaller than 0.1% (orange profile) while the test error continues to decrease until epoch 185 from0.194 to 0.1765 (blue profile). In the same range of epochs (150-185) the training error of smallerchunks decreases sensibly (see panel b).
Figure 8: No representation mitosis in ResNet50 on ImageNet a: Decay of the test error as afunction of the network width (blue) and for chunks of the widest ResNet50 (orange) to the errorof an ensemble of ResNet50_4. The ensemble consists of four networks. b: Mean correlation (seeSec. 2.2) of the residuals of the linear map of a chunk of the last hidden representation to the fullrepresentation. The network examined is ResNet50_4. c: R2 coefficient of the ridge regression fit ofa chunk of the last hidden representation of a ResNet50_4 to its full layer representation.
Figure 9: The representation mitosis for a DenseNet40 architecture. a: Decay of the test errorof independent networks (blue) and chunks of the widest network (orange) to the error of an ensembleaverage of ten of the widest networks (DenseNet40-BC, k=128) b: Blue profile: R2 coefficient ofthe ridge regression of a chunk of wc neurons (x-axis) to the full layer representation. Green profile:mean correlation of the residuals of the mapping as described in Sec. 2.2. c: Training error ofvarious DenseNet40 of increasing width (blue) and of chunks of the widest architecture (orange). d:The mean correlation of the residuals from the linear reconstruction of the final representation fromchunks of a given size for a DenseNet40-BC (k=128) during training. e: Training error dynamicsof chunks of a DenseNet40-BC (k=128). f: Training and test error dynamics for a DenseNet40-BC(k=128).
Figure 10: A Densenet40 without mitosis A DenseNet40-BC (k=128) trained on CIFAR10 withoutweight decay and data augmentation. This experiment reproduces on a DenseNet the analysis shownon a Wide-ResNet28 in Sec. 3.1. It shows that a: also in a DenseNet architecture not well regularized-1/2error -error∞ decays faster than wc	and b: the mean correlation of the residuals do not decreaseduring training. The thin profiles of panel b are the same as those shown in Fig. 9-d.
Figure 11: Wide-RN28_1 with a wide output layer trained on CIFAR10 We tested whether it isenough to increase the width of the final representation to see mitosis, or if instead one has to increasethe width of the full network. We trained a ResNet28_1 increasing only the number of channels inthe last layer. We modified the number of output channels of the last block of conv4 and analysed therepresentation after average pooling, as we did in the other experiments. The network was trainedfor 200 epochs using the same hyperparameters and protocol described in Sec. 2. The figure showsthat the test error of the modified ResNet28_1 for is approximately constant (blue profile). On thecontrary when we increase the width of the whole network the test error decays to the asymptotictest error with an approximate scaling of 1/√w (orange profile). Therefore mitosis occurs when thewidth of the whole architecture is increased, rather than just the width of the final layer.
