Figure 1: The Pioneer Push task and the Push and Reach task.
Figure 2: Evaluated average return of value target lower bounding with diSCounted return (lb-DR)vS SAC or DDPG on Atari gameS and epiSodiC FetChEnv taSkS. Solid CurveS are the mean aCroSSfive (for Atari) or three (otherS) SeedS, and Shaded areaS are +/- one Standard deviation.
Figure 3: Value target lower bounding with goal distance return (lb-GD) and lb-DR+GD vs HERon episodic FetchEnv and Pioneer tasks. Solid curves are the mean across three seeds, and shadedareas are +/- one standard deviation.
Figure 4: Illustration of value target lower bounding speeding up value learning as training pro-gresses from stages 0 to 3. The task is to navigate in the state space from start state S to end stateT, with sparse reward 1 at T and 0 elsewhere. The curve from S to T denotes a training experiencethat reaches the target. The shaded areas denote roughly states whose value has been significantlyimproved during training up to that stage.
Figure 5:	Fraction of training experience where lb-DR value target is greater than the Bellman target,on Atari games and episodic FetchEnv tasks, plotted against the number of training iterations. Solidcurves are the mean across five (for Atari) or three (others) seeds, and shaded areas are +/- onestandard deviation.
Figure 6:	Learned values of lb-DR and SAC (for Atari games) and DDPG (for FetchEnv tasks),evaluated on the training experience and plotted against the number of training iterations. Solidcurves are the mean across five (for Atari) or three (others) seeds, and shaded areas are +/- onestandard deviation.
Figure 7:	Fraction of training experience where lb-GD or lb-DR+GD value target is greater thanthe Bellman target, on episodic FetchEnv and Pioneer tasks, plotted against the number of trainingiterations. Solid curves are the mean across three seeds, and shaded areas are +/- one standarddeviation.
Figure 8:	Learned values of lb-DR, lb-DR+GD and HER on episodic FetchEnv and Pioneer tasks,evaluated on the training experience and plotted against the number of training iterations. Solidcurves are the mean across three seeds, and shaded areas are +/- one standard deviation.
Figure 9:	Evaluated average return of value target lower bounding with discounted return (lb-DR) vsSAC or DDPG, td-lambda and RetraCe on Atari Breakout and epiSodiC FetChEnv taSkS. Solid CurveSare the mean aCroSS five (for Atari) or three (otherS) SeedS, and Shaded areaS are +/- one Standarddeviation.
Figure 10:	Learned values of lb-DR and SAC (for Atari games), DDPG (for FetchEnv tasks), td-lambda and Retrace, evaluated on the training experience and plotted against the number of trainingiterations. Solid curves are the mean across five (for Atari) or three (others) seeds, and shaded areasare +/- one standard deviation.
