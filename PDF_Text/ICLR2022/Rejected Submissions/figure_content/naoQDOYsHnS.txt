Figure 1: BMA framework. (a) The overall policy scheme. (b) The architecture of pseudometric-based actionrepresentation learning. The action encoder aims to generate action representations, whose `1 distances betweeneach other equals the expected pseudometric distance between their corresponding original actions. The generatedaction embeddings eis and ejs are both conditioned on the same state s. During training, since it is hard to find twotransitions that have the same state but different actions from the dataset, (s, ai, Rsai) is sampled from the dataset,and aj is uniformly sampled from the finite action set A. The reward of (s, aj) and the transitions of (s, aj) and(s,ai) are estimated by corresponding models. Ie is a function to estimate whether o7âˆ™ is an in-distribution actionand thus determine whether there is an o.o.d. penalty on the pseudometric distance. The objective J is the meansquare error between keis - ejsk1 and the summation of the reward distance, the discounted transition distributiondistance, and the o.o.d. penalty.
Figure 2: Experimental results. (a) Comparing BMA equipped with BCQ and CQL against directly trainingoffline RL algorithms (Discrete BCQ, Discrete CQL, and BC) on the original action spaces in 4 environments withlarge action spaces. (b) Comparing the performance of BMA against other widely used action representations(transition-based representation, reconstruction-based representation, two kinds of random representations, andexternal representation ) in 4 environments with large action spaces. (c) Ablations. We perform two offlinealgorithms (CQL and BCQ) with BMA in two environments (Multi-step maze and Recommendation system).
Figure 3: Visualization of different representation spaces. Since we set the dimension of representations as2 in the multi-step maze environment, we directly plot the action representations. The position of each dot inthese plots equals the corresponding action embeddings, and the color corresponds to the ground-truth actionvalue predicted by a model (Chandak et al., 2019) with online training. Two rows are in terms of two differentstates. Each column from left to right corresponds to the representations learned by the online model, the rand-1representations, the reconstruction representations, and ours.
Figure 4:	Details of the network architectures used in our experiments.
Figure 5:	An example of the influence of different action spaces on the behavioral regularizations of offlineRL algorithms.
Figure 6:	Online performance of BMA, the transition-based action representations, and thereconstruction-based representations in the Multi-step maze environment.
