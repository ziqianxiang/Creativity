Figure 1:	Heatmap comparing the outputs for the exact logit-space probabilistic-and function forindependent logits, ANDIL(x, y); our constructed approximation, ANDAIL(x, y); and max(x, y).
Figure 2:	Comparison of the exact logit-space probabilistic-or function for independent logits,ORIL(x, y); our constructed approximation, ORAIL(x, y); and max(x, y).
Figure 3:	Comparison of the exact logit-space probabilistic-xnor function for independent logits,XNORIL(x, y)； our constructed approximation, XNORAIl(x, y); and Signed_geomean(x, y).
Figure 4: Visualisation of weight matrices learntby two-layer MLPs on a binary classification task,where the target output is the parity of the inputs.
Figure 5: We trained CNN on MNIST, MLP on flattened-MNIST, using Adam (1-cycle, 10 ep),hyperparams determined by random search. Mean (bars: std dev) of n=40 weight inits.
Figure 6: ResNet50 on CIFAR-10/100, varying the activation function. The same activation function(or ensemble) was used through the network. The width was varied to explore a range of networksizes (see text). Trained for 100 ep. with Adam, using hyperparams determined by random search onCIFAR-100, w = 2, only.
Figure 7: ReLU unit, and ReLU units followed by a linear layer to try to approximate ORIL, leavinga dead space where the negative logits should be.
Figure 8: Solving XOR with a single hidden layer of 2 units, using either ReLU or XNORAILactivation. Circles indicate negative (blue) and positive (red) training samples. The heatmaps indicatethe output probabilities of the two networks.
Figure 9: Heatmaps showing ANDIL, ANDAIL, their difference, and their relative difference.
Figure 10: Heatmaps showing ORIL, ORAIL, their difference, and their relative difference.
Figure 11: Heatmaps showing XNORIL, XNORAIL, their difference, and their relative difference.
Figure 12: Heatmaps showing the gradient with respect to x and y of ANDIL and ANDAIL .
Figure 13: Heatmaps showing the gradient with respect to x and y of ORIL or ORAIL .
Figure 14: Heatmaps showing the gradient with respect to x and y of XNORIL xnor XNORAIL .
Figure 15: Training results, regression experiment on second synthetic datasetWe found that a simple model with three hidden layers, each with eight neurons, utilizing XNORAILwas able to go from a validation RMSE of 0.287 at the beginning of training to a validation RMSE of0.016 after 100 epochs. Comparatively, an identical model utilizing the ReLU activation function wasonly able to achieve a validation RMSE of 0.271 after 100 epochs. In order for our ReLU networkto match the validation RMSE of our 8-neuron-per-layer XNORAIL model, we had to increase themodel size by 32 times to 256 neurons at each hidden layer.
Figure 16: We trained MLPs on the Covertype dataset, with a fixed 80:20 random split. Trained withAdam, 50 ep., 1-cycle, using LRs determined automatically with LR-finder. Mean (bars: std dev) ofn= 10 weight inits.
Figure 17:	We trained 2-layer MLPs discriminators on JSB Chorales using Adam (constant LR1 × 10-3, 150 ep.), Mean (bars: std dev) of n =10 weight inits.
Figure 18:	Cosine similarities between pre-activation weights of two activation functions in the firstlayer of an MLP trained on JSB Chorales.
