Figure 1: Illustrations of the relationship between optimization path lengths and generalization. (Left)Linear regression model. (Right) Random feature regression model with the feature extracted by aneural network. We train both models by gradient descent under GauSSian initialization N(μ, σ2) Withvaried μ and σ, and record the trajectory lengths len(w(0), ∞) from initialization until convergence.
Figure 2: learning rate = 0.0113Under review as a conference paper at ICLR 2022linear regression modelO0-2110①• training error▲ test errorrandom feature regression model• training error▲ test error	▲Ien(VV⑼,8)	Ien(I√8,8)Figure 3: learning rate = 0.1linear regression model	random feature regression modelJoiJ ①Ien(I/1/(°), 8)	len(∣ψω∖ ∞)Figure 4: learning rate = 0.5A.2 MNIST classificationIn this subsection, we calculate the trajectory length on the MNIST classification problem to show
Figure 3: learning rate = 0.1linear regression model	random feature regression modelJoiJ ①Ien(I/1/(°), 8)	len(∣ψω∖ ∞)Figure 4: learning rate = 0.5A.2 MNIST classificationIn this subsection, we calculate the trajectory length on the MNIST classification problem to showthe relation between optimization and generalization under different initializations.
Figure 4: learning rate = 0.5A.2 MNIST classificationIn this subsection, we calculate the trajectory length on the MNIST classification problem to showthe relation between optimization and generalization under different initializations.
Figure 5: MNIST classification14Under review as a conference paper at ICLR 2022From this figure, we can see that for different random initializations, short optimization paths isassociated with good generalization gap. This numerical result exhibits the generality of our theoryand suggests that the path length plays an important role to connect optimization and generalization.
