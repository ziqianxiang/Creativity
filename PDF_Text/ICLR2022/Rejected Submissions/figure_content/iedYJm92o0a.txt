Figure 1: Overview of our scratchpad approach applied to predicting code execution and comparisonto direct execution prediction. Top: Previous work has shown that large pre-trained models achievepoor performance when asked to directly predict the result of executing given computer code (Austinet al., 2021). Bottom: In this work, we show that training models to use a scratchpad and predictthe program execution trace line-by-line can lead to large improvements in execution predictionperformance. N.B. Although the example above only has one loop iteration for each loop, all loopsare unrolled across time.
Figure 2: Example of input and target for additionwith a scratchpad. The carry is recorded in thedigit following “C:”. Comments (marked by #)are added for clarity and are not part of the target.
Figure 3: Using a scratchpad significantly improves the performance of pre-trained Transformer-based models on addition, including their ability to generalize out of the training distribution tonumbers with more digits. Models were trained on 1-8 digit addition. The baseline models weretrained without intermediate scratchpad steps.
Figure 4: Example of polynomial evaluationwith a scratchpad. Each term in the polyno-mial is computed separately and then added.
Figure 5: Example synthetic Python program.
Figure 6: Top: examples of single line data. Bottom: example CodeNet submission.
Figure 7: Long addition ablation results. Here, we comparing the baseline and scratchpad results toa model that is first fine-tuned on the scratchpad and then subsequently fine-tuned to perform directexecution (the baseline). The intermediate scratchpad training seem to not have any significanteffect on the overall performance, indicating that the extra training-time information seen by thescratchpad model does not seem solely responsible for the scratchpad model’s performance.
