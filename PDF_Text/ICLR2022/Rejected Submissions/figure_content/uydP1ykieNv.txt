Figure 1: Normal ensemble training of multiple sub-models (Left) and the proposed ensemble-in-one training within a random gated network (Right). By selecting the paths along augmented layers,the ensemble-in-one network can instantiate nL sub-models, where n represents the augmentationfactor of the multi-gated block for each augmented layer and L represents the number of augmentedlayers in the network.
Figure 2: The construction of random gated network based on random gated blocks. The forwardpropagation will select one path to allow the input pass. Correspondingly, the gradients will alsopropagate backward along the same path.
Figure 3: Investigation on the hyper-parameters involved in the Ensemble-in-One construction andtraining. All these experiments are implemented on ResNet-20 over CIFAR-10 dataset. Left: Theblack-box adversarial accuracy under different sample count p per iteration; Middle: The black-boxadversarial accuracy under different distillation perturbation d ; and Right: the adversarial accuracyunder different augmentation factor n.
Figure 4: Contrasting the robustness of Ensemble-in-One with previous ensemble training meth-ods. Left: adversarial accuracy under black-box transfer attack; and right: adversarial accuracyunder white-box attack. The number after the slash stands for the number of sub-models withinthe ensemble. The evaluations include ResNet-20 and VGG-16 over the CIFAR-10 dataset. Thedistillation perturbation strength of VGG-16-based EIO is set as d = 0.03.
Figure 5: Contrasting the robustness of Ensemble-in-One and AdvT with different adversarial per-turbation settings. The experiments are implemented on ResNet-20 over CIFAR-10. The “ft-epoch”means the fine-tuning epoch of the derived model. When aligning the clean accuracy, EIO achievesbetter robustness than AdvT.
Figure 6: Contrasting the robustness of Ensemble-in-One with other methods on CIFAR-100 dataset.
Figure 7: The training curves and time cost of Ensemble-in-One. The experiments are made onResNet-20-based EIO training, with default hyper-parameters. Left: The clean validation accuracythroughout the training process. The fine-tuning epoch is extended to 40 epoch. As is observed, 20epochs are adequate to help the derived model to fully converge. Right: The training time cost ofBaseline, AdvT, DVERGE, and EIO, which are evaluated on an NVIDIA GeForce RTX 2080 Ti.
Figure 8: Investigation on the stability of derived models from the RGN. The experiments are madeon CIFAR-10 dataset based on ResNet-20 and VGG-16 respectively. Black-box adversarial accuracyis evaluated.
