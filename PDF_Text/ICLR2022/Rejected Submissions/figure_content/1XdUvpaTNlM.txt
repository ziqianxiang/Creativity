Figure 1: Illustration of our proposed BWCP. (a) Previous channel pruning methods utilize a hardcriterion such as the norm (LiU et al., 2017) of channels to deterministically remove unimportantchannels, which deteriorates performance and needs a extra fine-tuning process(Frankle & Carbin,2018). (b) Our proposed BWCP is a probability-based pruning framework where unimportantchannels are stochastically pruned with activation probability, thus maintaining the learning capacityof original CNNs. In particular, our proposed batch whitening (BW) tool can increase the activationprobability of useful channels while keeping the activation probability of unimportant channelsunchanged, enabling BWCP to identify unimportant channels reliably.
Figure 2: A schematic of the proposed Batch Whitening Channel Pruning (BWCP) algorithm thatconsists of a BW module and a soft sampling procedure. By modifying BN layer with a whiteningoperator, the proposed BW technique adjusts activation probabilities of different channels. Theseactivation probabilities are then utilized by a soft sampling procedure.
Figure 3: ((a) & (b)) and ((c) & (d)) show the effect of BWCP on activation probability with trainedResNet-34 and ResNet-50 on ImageNet, respectively. The proposed batch whitening (BW) canincrease the activation probability of useful channels when ∣γ∕ > 0 while keeping the unimportantchannels unchanged when when βc ≤ 0 and γc → 0. (e) & (f) show the correlation score for theoutput response maps in shallow and deeper BWCP modules during the whole training period. BWCPhas lower correlation score among feature channels than original BN baseline.
Figure 4:	Experimental observation of how our proposed BWCP changes the values of Yc and βcin BN layers through the proposed BW technique. Results are obtained by tranining ResNet50 onImageNet dataset. We investigate γc and βc at different depths of the network including layer1.0.bn1,layer2.0.bn1,layer3.0.bn1 and layer4.0.bn1. (a-d) show BW enlarges βc when βc > 0 while reducingβc when βc ≤ 0. (e-h) show that BW consistently increases the magnitude of γc across the network.
Figure 5:	Experimental observation of the values of δ defined in proposition 3. Results are obtainedby tranining ResNet-34 on ImageNet dataset. (a & b) investigate δ at different depths of the networkincluding layer1.0.bn1 and layer4.0.bn1 respectively. (c) visualizes δ for each layer of ResNet-34.
Figure 6: Illustration of BWCP with shortcut in basic block structure of ResNet. For shortcut withConv-BN modules, we use a simple strategy that lets BW layer in the last convolution layer andshortcut share the same mask. For shortcut with identity mappings, we use the mask in previous layer.
Figure 7: Illustration of forward propagation of (a) BN and (b) BWCP. The proposed BWCP prunesCNNs by replacing original BN layer with BWCP module.
