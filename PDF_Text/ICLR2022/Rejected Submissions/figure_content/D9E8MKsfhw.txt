Figure 1: Pre-trained and randomly ini-tialized DistilBERT on Split YahooQAdataset. Performance of the first task vi-sualized over sequential learning of tasks(averaged over 5 runs). Both modelsstart with approximately equal averagetask accuracy, but pre-trained initializa-tion leads to significantly less forgetting.
Figure 2: Loss contours for Task 1 on each dataset. Each contour shows the location of the modelparameters after training on each of the first three tasks. The top row shows contours for randomlyinitialized models and the bottom row shows contours for pre-trained initialized models.
Figure 3: Evolution of task accuracy during sequential training on 5-dataset-NLP. We comparethe performance of pre-trained and randomly initialized models, for first three tasks in a sequence,across five different random task orderings (Seq1,...,Seq5). We see that both models start withapproximately equal task accuracy, but pre-trained initialized models undergo lesser forgetting thanrandomly initialized models.
Figure 4: Evolution of task accuracy during sequential training on 5-dataset. We compare theperformance of pre-trained and randomly initialized models, for first three tasks in a sequence,across five different random task orderings (Seq1,...,Seq5). We see that both models start withapproximately equal task accuracy (except for CIFAR-10), but pre-trained initialized models undergolesser forgetting than randomly initialized models.
Figure 5: Loss interpolation plots for each dataset. Blue is pre-trained models, red is randomlyinitialized models. We interpolate between the checkpoint after task 1 (T1)/ task 2 (T2) to thecheckpoint after every other task, tracking the loss in the process. In general, the loss landscape isflatter along these paths for pre-trained initialized models compared to randomly initialized models.
Figure 6: Loss contours for task 1 (T1) and task 2 (T2) of Split CIFAR-50. The top row visualizesloss contours for task 1 where w1, w2, w3 are minima obtained after sequential training on tasks 1,2, and 3, respectively. Similarly, the bottom row visualizes loss contours for task 2 after sequentialtraining on tasks 2, 3, and 4. All of the above models start with random weights. SAM (Finetune +SAM, ER + SAM) leads to wide task minima compared to Finetune and ER methods.
Figure 7: Loss contours for SVHN (T1) and MNIST (T2) of 5-dataset. The top row visualizesloss contours for SVHN where w1, w2, w3 are minima obtained after sequential training on SVHN,MNIST, and nonMNIST, respectively. Similarly, the bottom row visualizes loss contours for MNISTwhere w2, w3, w4 are minima obtained after sequential training on tasks MNIST, nonMNIST, andFashion-MNIST. All of the above models start with random weights. SAM (Finetune + SAM, ER +SAM) leads to wide task minima compared to Finetune and ER methods.
Figure 8: Loss contours for Task 1 on 5 task sequences of 5-dataset-NLP. Each contour shows thelocation of the model parameters after training sequentially on Task 1 (w1), Task 2 (w2), Task 3(w3). The top row shows contours for randomly initialized models (w/o PT) and the bottom rowshows contours for pre-trained initialized models (w/ PT).
Figure 9: Loss contours for Task 2 on 5 task sequences of 5-dataset-NLP. Each contour shows thelocation of the model parameters after training sequentially on Task 2 (w2), Task 3 (w3), Task 4(w4). The top row shows contours for randomly initialized models (w/o PT) and the bottom rowshows contours for pre-trained initialized models (w/ PT).
Figure 10: Loss contours for Task 1 on 5 task sequences of Split YahooQA. Each contour showsthe location of the model parameters after training sequentially on Task 1 (w1), Task 2 (w2), Task3 (w3). The top row shows contours for randomly initialized models (w/o PT) and the bottom rowshows contours for pre-trained initialized models (w/ PT).
Figure 11: Loss contours for Task 2 on 5 task sequences of Split YahooQA. Each contour showsthe location of the model parameters after training sequentially on Task 2 (w2), Task 3 (w3), Task4 (w4). The top row shows contours for randomly initialized models (w/o PT) and the bottom rowshows contours for pre-trained initialized models (w/ PT).
Figure 12: Loss contours for Task 1 on 5 task sequences of Split CIFAR-50 with 5 epochs of trainingon each task. Each contour shows the location of the model parameters after training sequentially onTask 1 (w1), Task 2 (w2), and Task 3 (w3). The top row shows contours for randomly initializedmodels (w/o PT) and the bottom row shows contours for pre-trained initialized models (w/ PT).
Figure 13: Loss contours for Task 2 on 5 task sequences of Split CIFAR-50 with 5 epochs of trainingon each task. Each contour shows the location of the model parameters after training sequentially onTask 2 (w2), Task 3 (w3), and Task 4 (w4). The top row shows contours for randomly initializedmodels (w/o PT) and the bottom row shows contours for pre-trained initialized models (w/ PT).
Figure 14: Loss contours for Task 1 on 5 task sequences of 5-dataset with 5 epochs of training oneach task. Each contour shows the location of the model parameters after training sequentially onTask 1 (w1), Task 2 (w2), and Task 3 (w3). The top row shows contours for randomly initializedmodels (w/o PT) and the bottom row shows contours for pre-trained initialized models (w/ PT).
Figure 15: Loss contours for Task 2 on 5 task sequences of 5-dataset with 5 epochs of training oneach task. Each contour shows the location of the model parameters after training sequentially onTask 2 (w2), Task 3 (w3), and Task 4 (w4). The top row shows contours for randomly initializedmodels (w/o PT) and the bottom row shows contours for pre-trained initialized models (w/ PT).
