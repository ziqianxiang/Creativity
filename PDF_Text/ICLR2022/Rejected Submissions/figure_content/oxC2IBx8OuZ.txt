Figure 1: Information loss (left y-axis) and accuracy(right y-axis) for training ResNet18 on split-CIFAR10with different core set sizes (x-axis) and α = 0.2.
Figure 2: Performance of different algorithms on noisy quadratic model. The curves all evaluate the loss onglobal test datasets.
Figure 3: Process of constructing local datasets Withoverlap. Blue lines bound the current local datasets. Red—I - -4- ，、 L - I - ɪ - -4- -fck C ,-⅛ -4-	.⅛ _ — 一 ~一~ . ⅛ 一 一- 4 1 1 4O ，26Under review as a conference paper at ICLR 2022M × S subsets for S different clients: eachclient has M disjoint local subsets, and we putthese M subsets in sequence. For each client,we use a pointer to point to the start positionof the current local dataset. Figure 3 show thecase when M = 5: D1 - D5 are 5 disjoint localsubsets of client i. The pointer point to the startpoint of a local dataset of the current round and the blue lines bound current subsets. At the beginningof training, pointers belong to each client will point to the beginning of the sequence. At the end ofeach round, the pointer moves s steps, and if it moves to the end of all the sequence, it will comeback to the start point.
Figure 4:	Models are trained on various datasets with α = 0.1. CFL Without Core Set method use regularizationmethods, and CFL With Core Set methods use core set methods. All these two CFL algorithms use FedAvg asbackbone.
Figure 5:	Evaluation on global test data and past appeared training data. We trained ResNet18 on split-Cifar10dataset with α = 0.2 for 85 rounds.
Figure 6: Models are trained on split-Fashion-MNIST and split-Cifar10 datasets with α = 0.1. The loss isevaluated on global test datasets. Left column is the full curve of 300 rounds, and figures in right column arepartially enlarged curves.
