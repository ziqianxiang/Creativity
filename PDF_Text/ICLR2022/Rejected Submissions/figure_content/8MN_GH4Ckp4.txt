Figure 1: (a) Left: the neural quiver. Each vertex in the top row represents a layer of the neuralnetwork and each arrow indicates a linear mapping. The bottom vertex is the “bias” vertex. (b)Right: a quiver with skip connections.
Figure 2: Different radial activations.
Figure 3: Parameter reduction in 3 steps. Since the activation function is radial, it commutes withorthogonal transformations. This example has no bias and thus has nred = (1,1,1,1) instead of(1, 2, 3, 1). (See Footnote 3.) The number of trainable parameters reduces from 24 to 3.
Figure 4: If the loss is invariant with respect to an orthogonal transformation Q of the parameterspace, then optimization of the network by gradient descent is also invariant with respect to Q. Inthis example, projected and usual gradient descent match; this is not the case in higher dimensions,as explained through examples in the appendix (see F.4 and H.1).
Figure 5: Parameter reduction in 3 steps. Since the activation function is radial, it commutes withorthogonal transformations. This example has L = 3, n = (1, 4, 4, 1), and no bias. The reduceddimension vector is nred = (1,1,1,1). The number of trainable parameters reduces from 24 to 3.
