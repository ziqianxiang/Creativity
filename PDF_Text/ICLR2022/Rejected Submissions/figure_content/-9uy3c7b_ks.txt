Figure 1: The PlaNet Benchmarks. The performance of StateSAC can be seen as an upper boundbecause it learns directly from internal physical states rather than pixels. Our algorithm LCERconsistently achieves better or comparable performance than other methods(except StateSAC) onall environments5.1	Evaluation on Sample EfficiencyIn this section, We evaluate LCER on the PlaNet benchmark, which consists of six challenging con-trol tasks from the DMControl Suite (Tassa et al., 2018). The PlaNet benchmark is first introducedin (Hafner et al., 2019) and later widely used to benchmark sample efficiency in (Kostrikov et al.,2020b; Laskin et al.; Lee et al., 2020b;a).
Figure 2: The learning curves when data augmentation is not available. LCER can still continuouslyimprove the performance to a great extent without the help of data augmentation, showing LCER’sability to handle different data formats (beyond pixels) of observations.
Figure 3: The Learning curves on Distracting Control Suite, a challenging benchmark that containsseveral kinds of visual distractions. Top row: the observations on each task. Bottom row: thelearning curves on each task. The relatively poor performance of PI-SAC indicates that encodingpredictive information does not ensure representations of high quality. Although these tasks are sodifficult that SAC fails to solve, LCER can still improve the agent’s performance gradually, showingLCER’s robustness to distractions.
Figure 4: The effect of K and β . Figure 4(a) shows that larger K generally leads to better perfor-mance. The effect of β is more complicated as shown in Figure 4(b). Larger β does lead to betterperformance within a certain range, however when β goes beyond this range(i.e.β = 1), it mayhinder the performance. In practice, we can gradually increase the value of β to search for optimalperformance.
Figure 5: The learning curves of transferring obtained representations to target tasks. From left toright, the source tasks are cartpole-swingup, hopper-stand, walker-walk respectively. “LCER” and“SAC transferred” refer to using the representations obtained by LCER and vanilla SAC respec-tively, and “SAC” refers to training from scratch using vanilla SAC on target tasks. The represen-tation obtained by LCER can encode more useful information than vanilla SAC, leading to betterperformance on target tasks.
Figure 6: Additional aggregate metrics on DMControl 100K benchmark.
