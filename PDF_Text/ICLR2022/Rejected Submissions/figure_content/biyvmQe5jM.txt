Figure 1: Evolution of the weight norm when training with step-wise decay (decay times marked byblack dashed lines). The learning rate is decayed when the weight norm converges after bouncing.
Figure 2: Training curves of two experiments from table 1.
Figure 3: ResNet-50 trained on ImageNet for different learning rates and decay factors. (a) ABELbeats others schedules when using non-optimal learning rates. At learning rate 40, only ABELconverges. (b) ABEL is robust with respect to changes in the decay factor, its performance does notdepend too much on the decay factor because it adjusts the number of decays accordingly.
Figure 4: Wide Resnet on CIFAR-10 without data augmentation evolved for 200 epochs (left) and 15epochs (right). In this setup, the weight norm bounces at around 15 epochs. a) Both schedules reachtraining error 0 and their performance is the same (error of 7.3). b) If we evolve the model for only15 epochs, both schedules can still get training error 0 without a weight norm bounce, we think this isthe reason why there is no performance difference in a).
