Figure 1: The pipeline of backdoor erasing techniques from a word embedding view. (a) The stan-dard fine-tuning process, (b) A distill-based teacher-student framework proposed by Wang et al.
Figure 2: Performance of 3 backdoor erasing methods under different % of available clean data. Theplots show the average ASR (left) and ACC (right) over all four attacks. GBA significantly reducethe ASR to nearly 0% with only 1% clean data.
Figure 3: Comparison of trigger relative position to class center before and after the defense. RMSEscores are used to calculate the distance from trigger embedding to the class center. Four differentrare tokens (cf, mn, tq, bb) are used as triggers. We use the average score overall five datasets.
Figure 4: Parameter analysis: performance of our GBA approach under different λreveals that λ can certainly be tuned more to improve the performance of GBA. In short, the processof finding the right scaling factor λ is to find a balance between the ASR and the ACC. A practicalstrategy is to select λ until the clean accuracy drops below an acceptable threshold. This can reliablyfind an optimal λ, as increasing λ can always improve the robustness.
