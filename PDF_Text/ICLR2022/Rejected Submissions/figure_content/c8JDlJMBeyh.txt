Figure 1: HUman-NN Interface helps human locate and correct NN's errors by modifying NN's logic.
Figure 2: Pipeline for HUman-AI Interface. Red arrow represents NN-to-human path, which showsthe reasoning logic of original NN to human with structural concept graph (SCG). It consists ofVisual Concept Extractor (discover visual concepts) and Graph reasoning Network (GRN). Greenarrow represents the Human-to-NN path, which changes NN,s decision making with human,s priorknowledge. It consists of three steps: (1) human change SCG, (2) train GRN with human logic, and(3) transfer human knowledge to NN by partial knowledge distillation.
Figure 3: Examples of human modification of class-specific Structural Concept Graph (C-SCG).Eachclass has only one c-SCG, We use one instance to visualize the states before and after human usersmodifying c-SCG of the corresponding class. (a) Node (concept) modification: human user find thatthe NN,s original concept 2 (in green, building window) to be non-causal inference and concept 3 (inred, wheel) is not discriminative and representative for this particular type of vehicle. So the humanuser chooses two new concepts from concept pool and replace them on the c-SCG. (b) Edge (conceptrelationship) modification: human user finds that the spatial relationship between concept 2 (in green,legs) and 4 (in pink, tail) to be unstable and the two concepts have minimal dependency on each other,hence deleted the edge between them on the c-SCG.
Figure 4: Pipeline of training Graph Reasoning Network with Human modified c-SCG. Given inputI, we conduct multi-resolution segmentation and concept match step. In concept matching step,yellow circle represent the matched concepts for each class-of-interest (black dummy nodes denotesundetected concepts).(More details in Sec. 3.2.2)structure relationships and dependency between concepts. After understanding the meaning ofc-SCG, human users can then intuitively make modifications to c-SCGs (e.g., removing the incorrectnodes/edges) based on their knowledge or other priors, in order to improve the NN’s performance.
Figure 5: The pipeline of PartialKnowledge Distillation. Differentfrom traditional knowledge distil-lation, partial knowledge distilla-tion adopts two teachers with dif-ferent expertise: GRN (teacher1) focuses on class of interest (6classes in this example), and fixedoriginal NN (teacher 2) will fo-cuses on the rest of classes (theclasses we don,t want to change,14 classes in this example). Afterdistillation with different temper-atures and concatenation, we canuse both soft labels and hard labelsto train the student model.
Figure 6: Example of edges (concepts relationship) modification. (a) biased iLab dataset. (b)Human user can remove incorrect edges to guide the model to ignore irrelevant concept relationshipsintroduced by dataset bias. (c) iLab-20M three class classifier performance.
Figure 7: Zero-shot learning: Human users teach NN to learn to encode new objects with HNLSec. 4.2 provide explanation for each step.
Figure 8: Image source: (Ge et al., 2021) Pipeline for Visual Reasoning Explanation framework. (a)The Visual Concept Extractor (VCE) discovers the class-specific important visual concepts. (b) Inoriginal NN, the representation of the top N concepts is distributed throughout the network (coloreddiscs and rectangles). (c) Using Visual Concept Graphs that are specific to each image class, ourVRX learns the respective contributions from visual concepts and from their spatial relationships,through distillation, to explain the network’s decision. (d) In this example, the concept graphs coloredaccording to contributions from concepts and relations towards each class explain why the networkdecides that this input is a Jeep and not others.
Figure 9: Image source: (Ge et al., 2021) An example result with the proposed VRX. To explainthe prediction (i.e., fire engine and not alternatives like ambulance), VRx provides both visual andstructural clues. Colors of visual concepts (numbered circles) and structural relationships (arrows)represent the positive or negative contribution computed by VRX to the final decision (see colorscale inset). (a): The four detected concepts (1-engine grill, 2-bumper, 3-wheel, 4-ladder) and theirrelationships provide a positive contribution (blue) for fire engine prediction. (b, c): Unlike (a), thetop 4 concepts, and their relationships, for ambulance/school bus are not well matched and contributenegatively to the decision (green/yellow/red colors).
Figure 10: Image source: (Ge et al., 2021) Class-specific importance weights eji highlight theimportant concept relationships for different classes, eji (shown as tables for each class) revealsthe information transformation between concepts, which shows the dependency between concepts:concept 1 and 2 contribute most information to other concepts, which makes them the 2 mostdiscriminating concepts for a fire engine.
Figure 11: Results of node (concepts) modification: for the ambulance structural concept graph(c-SCG), human find NN's original concept 3 (in green frame, ground) and concept 4 (in pink frame,building roof) is anti-causal inference, due to the fact that many ambulance training images were incity backgrounds, so we choose two new concepts, side of the car (in orange frame) and car headlight(in purple frame) from discovered concept pool and substitute them. The rest examples are similar.
Figure 13: Examples of human modify both nodes and edges. (a) For the bird “white stork”，Wedelete the edges between head and feathers because they don't have a very stable relationship, i.e.
Figure 14: Zero-shot learning: Human users teach NN to learn to encode new objects with HNLGiven images of objects A, B, C, and D (object EiSa new class with no images). Step 1: train aResNet-18 with images of objects A, B, C, and D; then use VCE to discover the shared concepts;we can use concept match to obtain image-level SCGs (I-SCG) for learned objects. Step 2: humansfirst analyze the relationship of new object E with shared concepts and learned objects (ABCD), thenbuild custom I-SCG for object E by recombining the nodes and edges. Then train a GRN that canclassify both ABCD and E. Step 3, we use knowledge distillation to transfer the knowledge fromSCG back to the original NN. Step 4: The original NN obtain the ability to classify the new object ETable 7: Performance (Confusion matrix) of Zero-shot learning with HNI. (left) GRN with customI-SCGs of new object E. (middle) Original NN ResNet-18 trained with images of objects A, B, C, Dcan not identify object E in the test set. (right) ResNet-18 learned to encode and recognize object Eafter knowledge distillation through proposed HNI.
Figure 15: Configuration of general setting of Zero-shot-learningF	Human-in-the-Loop User StudyWe conducted a user study with responses from 43 ML practitioners and students giving us thefollowing data.
