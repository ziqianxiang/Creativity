Figure 1: Diagrams of attention modules described in Section 3: alignment scores (grey edges) de-termine normalized attention weights (blue), which are used to mix the inputs xi：T. Left: Attentionwith a general context. Center: Stackable self-attention layer, where xi：T is the input as well asthe context. Right: Auxiliary [CLS] token to extract a single scalar from a self-attention layer,providing a real-valued function class for classification or regression tasks.
Figure 2:	Statistically probing a Transformer by training it on a 3-way AND of a hidden subsetof i.i.d. random bits. Left, center: Sublinear scaling of the empirical sample complexity. Right:Example training curves in the {overfitting, correct} regimes: T = 400, m = {100, 200}.
Figure 3:	A curious empirical finding: Transformers can learn sparse parities. 10 loss curves (withonline batches) are given for this setup with s = 3, T = {10, 15, 20}, showing abrupt phase transi-tions from random guessing to perfect classification. See Appendix C.2 for details.
Figure 4: Enlarged plots from Figure 2, the main experimental validation of our theory.
