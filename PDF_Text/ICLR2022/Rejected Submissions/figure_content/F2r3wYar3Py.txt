Figure 1: Canvas transformations (A), transformation flows (B), and distortions (C).
Figure 2: Canvas grid and its corresponding lattice. Local distortions are computed at every pair ofneighboring edges. One example of neighboring edges is highlighted in red.
Figure 3: An anchor system (A) and its transformation (B). (C) exemplifies a configuration of(G, PC)-solution path consisting of a chain of anchor grids/IattiCeS and a chain of blurring.
Figure 4: MNIST (10 classes) in the tiny-data regime: first 1-20 training images per class and fulltest set (examples shown on top). For each model listed in the legend, We plot its test accuracy versusthe training size N (bottom left) and also the smallest N needed to reach a threshold of 90% accuracy(bottom right). Our model outperforms all other models for all N ∈ {1,..., 20}, requiring the fewesttraining examples (first four or six per class) to reach 90% accuracy.
Figure 5: EMNIST-letters (26 classes) in the tiny-data regime: first 1-20 training images per classand full test set (examples shown on top). Results are shown in the same way as in Figure 4. Ourmodel outperforms all other models for all N ∈ {1, . . . , 20}, requiring the fewest training examples(first eleven or twelve per class) to reach 75% accuracy (due to increased difficulty in this dataset).
Figure 6: One-shot classification in the Omniglot challenge (A) and its error-rate leaderboard (B).
Figure 7: Archetype generation Via k-means-Style clustering in our learned similarity space.
Figure 8: Generalization of our distortable canvas model for two major future directions (two ends).
