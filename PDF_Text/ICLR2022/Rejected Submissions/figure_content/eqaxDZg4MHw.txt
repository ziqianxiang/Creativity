Figure 1: Visualization of different levels of (a) Jumper and (b) Climber. In both the games, the firstand second level differ in theme whereas the first and third level differ in both theme and layout.
Figure 2: UCB-DrAC with theme randomization (DrAC-TR) gets close to the test performance ofDrAC-FT on games with fixed themes indicating that it learned theme invariant features. Thus, itoutperforms UCB-DrAC implying the need for better domain randomization and data augmentation.
Figure 3: PPO with inverse model regularization (PPO inv m) is competitive (in terms of testperformance) against UCB-DrAC, DAAC and iDAAC in the ProcGen domain despite being a muchsimpler method. Black horizontal lines are the test performance of a PPO baseline. Dashed line istraining performance.
Figure 4: We compare PPO, PPO with inverse model regularization (PPO inv m), UCB-DrAC, DAAC,and iDAAC on test levels (a) when using different hyperparameters for each game and (b) when usinga single set of hyperparameters across all gamestuple, then this regularization minimizes CE(at, fÏ†(zt, zt+1)) where CE is the cross-entropy loss.
Figure 6: We compare fine-tuning the policy head (Head-only) versus the entire policy network (Full).
Figure 5: Transfer: We compare fine-tuning the policy head (Head-only) versus the entire policynetwork (Full) on new levels (201 - 400). The policy is trained on 200 levels. Fine tuning just thepolicy head allows us to recover this performance on the training levels ([1 - 200]), showing thatthe visual features only require a small number of training levels to generalize. We additionallyinclude the learning curve of a policy trained from scratch (From Scratch), a policy head trained ona randomly initialized visual encoder (Head (Random)), and a policy head trained on a imagenetpretrained visual encoder (Head (ImageNet)) to show the importance of using learned visual features.
Figure 7:	Visualization of games placed next to the RL algorithm on which they perform the best.
Figure 8:	Transfer: We compare fine-tuning the policy head (Head-only) versus the entire policynetwork (Full) on 1k new levels. The policy is trained on 200 levels. Fine tuning just the policy headallows us to recover this performance on the training levels ([1 - 200]). We additionally include thelearning curve of a policy trained from scratch (From Scratch), a policy head trained on a randomlyinitialized visual encoder (Head (Random)), and a policy head trained on a imagenet pretrained visualencoder (Head (ImageNet)) to show the importance of using learned visual features.
Figure 9:	Transfer: We compare fine-tuning the policy head (Head-only) versus the entire policynetwork (Full) on 10k new levels. The policy is trained on 200 levels. Fine tuning only the policyhead fails to recover the performance on the training levels. We additionally include the learningcurve of a policy trained from scratch (From Scratch), a policy head trained on a randomly initializedvisual encoder (Head (Random)), and a policy head trained on a imagenet pretrained visual encoder(Head (ImageNet)) to show the importance of using learned visual features.
Figure 10:	Transfer: We compare fine-tuning the policy head (Head-only) versus the entire policynetwork (Full) on 100k new levels. The policy is trained on 200 levels. Fine tuning only the policyhead fails to recover the performance on the training levels. We additionally include the learningcurve of a policy trained from scratch (From Scratch), a policy head trained on a randomly initializedvisual encoder (Head (Random)), and a policy head trained on a imagenet pretrained visual encoder(Head (ImageNet)) to show the importance of using learned visual features.
