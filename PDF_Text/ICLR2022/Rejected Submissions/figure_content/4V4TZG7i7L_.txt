Figure 1: Hierarchical decomposition. (a) The image background in the two pictures depends onthe bird species. (b) We propose to capture such dependencies via hierarchies. (c) A closer look atthe image background reveals that it is modality-exclusive, i.e., not described by the caption.
Figure 2: Related work. (a) MVAE, MMVAE, (b) MDVAE, (c) MDVAE, MHVAE. Note that wediscuss an alternative inference network for the MDVAE in App. D.
Figure 3: Proposed generation and inference.
Figure 4: Hierarchical models can factorize Unimodal variations along the hierarchy This figurevisualizes possible latent representations for the multimodal data from Fig. 1: two modalities xι andx2, where the brown color indicates shared structure and the yellow/green colors modality-exclusivevariations.
Figure 5: CUB/Oxford Flower: Generating image feature vectors from caption feature vectors.
Figure 6: CUB/Oxford Flower: Generating caption feature vectors from image feature vectors.
Figure 7: Oxford Flower: generating images from captions. Top: The penultimate column rep-resents the HMVAE with two hierarchical levels for the images (L(1) = 2). The last column corre-sponds to the HMVAE with L(1) = 5, which is the best model considering all measures across allmodalities. Bottom: as in the plot above, we generate p(xi |g) from q(g|xj6=i). However, for the la-tent distributions starting at the latent variables indicated below the plot, we use the mean and not aconventional sample. We can thereby evaluate generated samples given varying degrees of hierar-chical expressivity.
Figure 8: CUB/Oxford Flower: Generating image feature vectors from caption feature vectors.
Figure 9:	CUB/Oxford Flower: Generating caption feature vectors from image feature vectors.
Figure 10:	Oxford Flower: Unconditional generation. We generate p(xi |g) from g 〜p(g) fori ∈ {1, 2}. In (b), we generate caption features, look up the nearest-neighbor feature from the testset, and visualize the respective caption.
Figure 11: Unimodal hierarchicalVAEs (S0nderby et al., 2016)(a) Inference network. The red edgesvisualize the merging procedure ofbottom-up and top-down information.
Figure 12:	Feature genereation on CUB/Oxford Flower - HMVAE. (a) Image ft. VAE.(b) CaP-tion ft. VAE. Both decoders parameterizes normal distributions. We do not display the LeakyReLUactivation functions.
Figure 13:	Image generation on Oxford Flower - HMVAE. (a) Image VAE. The decoder generatesscalars, which are used to compute the binary cross-entropy with the ground truth. We do not displaythe Gelu activation functions (Hendrycks and Gimpel, 2016). (b) Caption ft. VAE. The decoderparameterizes a normal distribution. We do not display the LeakyReLU activation functions.
Figure 14: MDVAE: Alternative inference networkD MDVAE: alternative inference networkThere are several inference network choices for multimodal disentanglement VAEs (MDVAEs):Fig. 2c depicts the natural inference network. Figure 14 modifies this network to include dependen-cies between g and z1:M . This modified architecture recovers our proposed hierarchical inferencenetwork for the special case of two hierarchical layers (Fig. 3b). Note that the edges between g andz1:2 are still absent in the generative model (Fig. 2b): the model must learn to represent independentvariation in zι. For example, the model must generate pθ(xi∣Zi, g) from Zi 〜pθ(zι) (not neces-sarily Zi 〜qφ(zι∖xi, g)) and g 〜qφ(g∣Xj=i).
