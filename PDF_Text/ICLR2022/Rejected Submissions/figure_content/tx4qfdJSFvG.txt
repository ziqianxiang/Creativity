Figure 1: A diagram illustrating our Feature Propagation framework. On the left, a graph with miss-ing node features. In the initial reconstruction step, Feature Propagation reconstructs the missingfeatures by iteratively diffusing the known features in the graph. Subsequently, the graph and the re-constructed node features are fed into a downstream GNN model, which then produces a prediction.
Figure 2: Graph Fourier transform magni-tudes of the original Cora features (red) andthose reconstructed by FP for varying ratesof missing rates (we take the average overfeature channels). Since FP minimizes theDirichlet energy, it can be interpreted as alow-pass filter, which is stronger for a higherrate of missing features.
Figure 3: Test accuracy for varying rate of missing features on six common node-classificationbenchmarks. For methods that require a downstream GNNs, a 2-layer GCN (Kipf & Welling, 2017)is used. On OGBN-Arxiv, GCNMF goes out-of-memory and is not reported.
Figure 5: Test accuracy on the synthetic datasets from Abu-El-Haija et al. (2019) with differentlevels of homophily. We use GraphSage as downstream model as it is preferable to GCN on lowhomophily data (Zhu et al., 2020).
Figure 4: Run-time (in seconds) of FP, PaGNN and GCNMF.
Figure 6: Test accuracy for varying rate of missing features on six common node-classificationbenchmarks. For methods that require a downstream GNNs, a 2-layer GraphSAGE (Hamilton et al.,2017) is used. On OGBN-Arxiv, GCNMF goes out-of-memory and is not reported.
