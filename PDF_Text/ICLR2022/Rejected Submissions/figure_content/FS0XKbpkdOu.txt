Figure 1: Applying Sphere2Vec to geo-aware image classification. The supervised image classification module(green) is a InceptionV3 network similar to that of Mac Aodha et al. (2019); the supervised location classificationmodule (blue) can be Sphere2Vec or any other inductive location encoders (Chu et al., 2019; Mac Aodha et al.,2019; Mai et al., 2020b); the self-supervised training stage (orange) pre-trains the location encoder based onunlabeled image data. The dotted lines indicates that there is no back-propergation through these lines.
Figure 2: Applying location encoders to differentiate two visually similar species: Arctic foX and bat-eared foX.
Figure 3: Spectral representations of different encoders. Each point pλm , φnq represent an interaction terms oftrigonometric functions in the encoder. Points on the axis correspond to single terms with no interactions.
Figure 4: iNat2017 dataset and model performance comparison: (a) Sample locations for validation set wherethe dashed and solid lines indicates latitudes; (b) The number of training and validation samples in differentlatitude intervals. (c) MRR difference between a model and baseline grid on the validation dataset. (d)∆MRR “ MRRpsphereC') — MRRpgrid) for each latitude-longitude cell. Red and blue color indicatespositive and negative ∆M RR while darker color means high absolute value. The number on each cell indicatesthe number of validation data points while "1K+" means there are more than 1K points in a cell. (e) The numberof validation samples v.s. ∆MRR “ M RRpsphereC ') — M RRpgrid) per latitude-longitude cell. Theorange dots represent moving averages. (b) The number of validation samples v.s. ∆MRR per latitude band.
Figure 5: Comparing different self-supervised learning objectives L and different encoders epxq in a labellimited setting on iNat2018. epxq is first trained on Xunlabeled with different self-supervised objectives, and thensupervised trained with different amount of training labels. Different curves indicates different self-supervisedobjectives L and different epxq (denoted as “[]”) indicated by the legend. The X axis indicates differenttraining label ratio Γ and the Y axis indicates the Top 1 accuracy from the joint predict of image classifierand epxq. No Prior indicates an standalone image classifier with no location encoder. Sup. indicates traininglocation encoders with supervised learning loss. The shaded area along each curve indicates the standarddeviations we get after running each setting for 5 times. (a) Comparison among five models with identical modelarchitecture but different training objectives L on iNat2018 dataset. MSE, BI ´ LbBaItch ` LnBeIgloc ` LsBiImcse,and MC ´ LbMaCtch ` LnMeCgloc ` LsMimC cse indicate training sphereM ` with three different self-supervisedobjectives before supervised training. (b) Comparing the performance of sphereM ` and theory (the 2nd bestmodel on iNat2018 dataset) in both supervised only (Sup.) and self-supervised plus supervised (Sup.&MC)settings. We can see that in both settings, sphereM ` is able to outperform theory under different ratio Γ.
Figure 6: An illustration for map projection distortion: (a)-(d): Tissot indicatrices for four projections. The equalarea circles are putted in different locations to show how the map distortion affect its shape.
Figure 7: Ablation study on different unsupervised learning setting on iNat2018 dataset. (a) Alblation study onBI loss. BI ´ LbBaItch ` LnBeIgloc ` LsBiImcse indicates the full BI loss while BI ´ LbBaItch ` LnBeIgloc deletes theSimCSE component. BI ´ LbBaItch additionally deletes the negative location loss component. Comparing amongthose three model settings can help us understand the effect of different loss components. Similarly, we do thesame ablation study on MC loss and show in (b).
Figure 8: Comparison among two models with identical model architecture but different training objectiveson fMoW dataset. Each point on each curve indicates a specific training process. We repeat each of them forfive times and show the standard deviations as shaded areas along the line. We can see that with statisticallysignificance, MC unsupervised loss is also effective in the few shot learning setting on the fMoW dataset.
Figure 9: Impact of MRR by The Number of Samples at Different Latitude Bands.
Figure 10: Compare the predicted distributions of example species from different models. The first figure ofeach row marks the data points from iNat2018 training data.
Figure 11: Embedding clusterings of iNat2017 models. (a) wrap* with 4 hidden ReLU layers of 256 neurons;(d) rbf with the best kernel size σ “ 1 and number of anchor points m “ 200; (b)(c)(e)(f) are Space2Vecmodels Mai et al. (2020b) with different min scale Tm%n = {10-6, lθ´2}.a (g)-(l) are different Sphere2Vecmodels.ba They share the same best hyperparameters: S “ 64, rmax “ 1, and 1 hidden ReLU layers of 512 neurons.
Figure 12: Embedding clusterings of iNat2018 models. (a) wrap* with 4 hidden ReLU layers of 256 neurons;(d) rbf with the best kernel size σ “ 1 and number of anchor points m “ 200; (b)(c)(e)(f) are Space2Vecmodels (Mai et al., 2020b) with different min scale rmin “ {10”, lθ´3 }.a (g)-(l) are Sphere2Vec models withdifferent min scale.ba They share the same best hyperparameters: S “ 64, rmax “ 1, and 1 hidden ReLU layers of 512 neurons.
Figure 13: The data distributions of four synthetic datasets (U1, U2, U3, and U4) generated from the uniformsampling method. (e) shows the U4 dataset in a 3D Euclidean space. We can see that if we treat these datasets as2D data points as grid and theory, the vMF distributions in the polar areas will be stretched and look like 2Daniostropic multivariate Gaussian distributions. However, this kind of systematic bias can be avoided if we use aspherical location encoder as Sphere2Vec.
Figure 14: The data distributions of four synthetic datasets (S1.3, S2.3, S3.3, and S4.3) generated from thestratified sampling method With Kmax = 64. We can see that when Nμ increases, a more fine-grain stratifiedsampling is carried out. The resulting dataset has a larger data bias toward the polar areas.
