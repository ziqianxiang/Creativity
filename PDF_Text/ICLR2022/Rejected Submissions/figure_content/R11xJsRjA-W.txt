Figure 1: OOD accuracy (left), Membership Inference attack accuracy (center) and Mean Rank metric (right)for the MNIST datasets. The line shapes denote different number of training domains. The error bars representstandard deviation over three runs.
Figure 2: OOD and MI attack accuracy of differentially-private ERM and Perfect-Match algorithmsâ€¢	The stable feature metric (Mean Rank) can explain the negative result for RQ1. Random-Matchhas a higher OOD accuracy than ERM or IRM but has a higher mean rank, indicating that it doesnot learn stable features. Thus, we obtain a positive answer to RQ2: methods that learn morestable features provide better membership privacy empirically, on both datasets. Our empiricalresults apply to methods and datasets in the setting where true stable features are not known, andhence extend the purely theoretical results from Tople et al. (2020).
Figure 3: OOD accuracy (left), MI attack accuracy (middle) and Train-test accuracy gap (right) for ChestXRaydataset. The legend mentions the test domain.
Figure 4: Loss distribution for theclass y=1 for two method A and BWith a lower generalization gap but higher MI attack accuracy, method B emulates the behavior ofRandom-Match algorithm on MNIST datasets. Similarly, Method A emulates the CSD results forChestXray dataset with ChexPert as the test domain, where the generalization gap is higher but theMI attack accuracy is low. We thus present our first result,Result 1: Out-of-Distribution generalization error cannot always explain the membershipinference risk of a model.
Figure 5: Results for syntheticdataset where Linear-AUC measuresthe learnt stable features and explainsthe membership attack accuracy ofthe models.
Figure 6: AI Attack Accuracy for Rotated-MNIST with color as an attribute.
Figure 7: Results for synthetic slab dataset With generalization gap included.
