Figure 1: Explaining with Conceptual Counterfactuals: (a) In generating conceptual explanations,the first step is to define a concept library. After defining a concept library, We then learn a conceptactivation vector (CAV) (Kim et al., 2018) for each concept. (b) Given a misclassified sample, suchas the Zebra image shown here as an African hunting dog, we would like to generate a conceptualcounterfactual. Meaning that we would like to generate a perturbation in the embedding space thatwould correct the model prediction, using a weighted sum of our concept bank. (c) Our methodassigns a score to the set of concepts. A large positive score means that adding that concept to theimage (e.g. stripes) will increase the probability of correctly classifying the image, as will removingor reducing a concept with a large negative score (e.g. polka dots or dogness).
Figure 2: Validating CCE by Identifying Spurious Correlations: (a) First, We train 5-class animalclassification models on skewed datasets that contain spurious correlations that occur in practice.
Figure 3: Validating CCE: (a) Here, we take an arbitrary test sample of a Granny Smith apple thatis originally correctly classified and perturb the image by turning it gray. We compute CCE scoresat each perturbed image and observe that the score for greenness increases as the image is grayed.
Figure 4: CCE explains model mistakes using learned biases and image quality conditions. (a)CCE identifies dark skin type correlation with the allergic contact dermatitis condition that exists inthe training dataset. (b, c, d) CCE identifies image artifacts that degrade the model performance.
Figure 5: Pseudocode for CES in Python-like syntax concept learning procedure. Learning theconcepts (lines 1-6) just needs to be done once and can be carried out offline.
Figure 6: Different reasons of model mistakes for the same class..
Figure 7: Validity constraint improves explanations. .
Figure 8: Learning image quality concepts. Here We have the positive samples for different imagequality concepts. For each corruption type, we provide examples for different levels of severity. Theoriginal image is obtained from the Fitzpatrick dataset.
Figure 9: CAM based methods fail to communicate model biases. Here We show two exampleswhere CCE is able to identify the bias in the data that drives the model mistake. However, one of themethods that are very commonly used to understand model predictions, GradCam++, fails to identifyand communicate the underlying reason.
Figure 10: Chest X-Ray images from different views. Here we provide several lateral view andfrontal view images from the SHC dataset. On the left, we see images from the lateral view and onthe right we have images from the frontal view. NIH dataset does not have any lateral view images inthe dataset, which can explain why a model trained on the NIH dataset performs poorly when testedon lateral view images.
Figure 12: In an analogous manner to Fig. 3, We take images that were originally correctly classifiedas robin and strawberry, and perturb them by removing red colors until they are misclassified bythe model (x-axis). The CCE scores correctly identify the concept of redness as the most importantconcept for correcting the model,s mistakes.
Figure 11: Validating CCE Through Low-Level Image Perturbations: (a) Here, we take anarbitrary test sample of a Granny Smith apple that is originally correctly classified and perturb theimage by turning it gray until it is eventually misclassified (in this case, as mortar). We computethe CCE scores at each perturbed input image and observe that the score for greenness increases,corresponding to the degree to which we remove the green color from the image. (b) We repeat thisfor 25 different images of Granny Smith apples (some of which are shown under the plot) and findthat the same trends generally hold true (each image is a gray line). Although a few images do notfollow this trend, the mean CCE score (bolded green line) does.
Figure 13: (a, b) We report the performance of CCE as we vary the severity level of the correla-tion.(K% severity means K% of the images in the training dataset has the concept). The bolded lineshows the median performance across 20 scenarios, and the confidence levels show the first andthird quantiles. We observe that starting from relatively low levels of severity(â‰ˆ 20%), CCE is ableto identify the spuriously correlated concept in a majority of the scenarios. (c) We evaluate BatchMode CCE over controlled experiments with varying severity levels. The y-axis corresponds to therank of the target concepts, the x-axis gives the severity of the spurious correlation, the bold line isthe median performance across 20 scenarios and the confidence intervals give the lower and upperquartiles. We observe that Batch Mode CCE is able to provide an automated analysis of the modelbiases using a batch of samples, given that it can identify the spuriously correlated concept as one ofthe major causes over all model mistakes for the given class.
Figure 14: (a) We observe how the performance changes with respect to K for the metric PreCiSion@K.
