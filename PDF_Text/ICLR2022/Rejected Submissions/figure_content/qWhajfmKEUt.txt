Figure 1: The architecture of DNN with Feature Spectral Regularization (FSR). FSR alleviates thedominance of top eigenvalues and enhances the role of relatively smaller eigenvalues in classifica-tion. When combined with AT, FSR further help learn more diverse features with higher dimensions.
Figure 2: Spectral analysis of models with fea-tures extracted from (a) natural examples and (b)adversarial examples on CIFAR-10. We scale allthe eigenvalues that the largest one is 1. “PC-ID"(AnSUini et al., 2019) denotes the estimatedintrinsic dimension (ID) of features. The sharpdistribution of eigenvalues in standard trainedmodel leads to a lower ID, while ID becomeshigher by imposing adversarial training and FSR.
Figure 3: Atoy model that demonstratesthe validity of the defined variation.
Figure 4: Variation on different eigenvectors in feature space under various attacks. The resultsreveal that the adversary adds more components along the eigenvectors with smaller eigenvalues.
Figure 5: Spectral analysis of FSR. (a) normalized eigenvalues (max-normalized) on CIFAR-10;(b) normalized eigenvalues on CIFAR-100; (c) normalized eigenvalues on SVHN.
Figure 6: Variation of all eigenvectors in feature space to adversarial attacks on CIFAR-10. ‘STD’means the model is trained on natural examples. ‘AT’ means the model is trained on adversarialexamples. It should be noticed that the range of ordinate values for ‘STD’ is different from ‘AT’.
Figure 7: Variation of all eigenveCtors in feature spaCe to adversarial attaCks on CIFAR-100.
Figure 8: Variation of all eigenveCtors in feature spaCe to adversarial attaCks on SVHN.
Figure 9: (a) Comparison of test ECE (lower is better) curves when training ResNet-18 on CIFAR-10. (b) FSR can improve the confidence calibration.
