Figure 1: Comparison of cross-attention (CA) and dot-product based dual-encoder (DE) models on theMSMARCO-Passage re-ranking task. Using 6-layer BERT models with varying embedding size (Turcet al., 2019), we report the train and dev set MRR@10 averaged over three independent trials. Forsufficiently large embedding dimension, the DE model closely matches the performance of the CA onthe training set; however, there is a sizable gap in the test set performance. This points to the poorerDE model performance being largely an issue of generalisation, rather than model capacity. Suitabledistillation (DIST) from the CA model manages to prevent such overfitting — potentially by worseningthe training performance — and largely bridges the gap between the two models. See §3.2 for detailson the experimental setup, and §4 for details on our distillation strategy.
Figure 2: Comparison of CA (left) and DE (middle) model predictions on the MSMARCO-passage test set.
Figure 3: Comparison of BERT-based cross-attention (CA) and dual-encoder (DE) models on the MSMARCO-Passage re-ranking task. For varying embedding dimension of an underlying 2-layer BERT model, wereport the train and dev set MRR@10. We see the same trend as in Figure 1: for sufficiently largeembedding dimension, the DE model closely matches the performance of the CA on the training set;however, there is a sizable gap in the test set performance.
Figure 4: Learning curve for CA, DE, and DIST models on the MSMARCO-Passage train and test set. Here, weuse the small-bert-6-768 architecture. Distillation is seen to saturate training performance beyond acertain point, while still resulting in a solution with better generalisation on the test set.
Figure 5: Comparison of CA and DE model predictions on the MSMARCO-Passage train set. We observe thesame general trends as the test set (Figure 2). For visual clarity, the scores are translated to have meanzero.
Figure 6: Comparison of CA and DE model predictions on the MSMARCO-Passage test set. Here, we do notcenter the scores to have mean zero. We see that the DE model scores possess a strong baseline shiftover the model scores.
Figure 7: Comparison of CA, DE and distilled DE (DIST) model scores on the MSMARCO-Passage dev set atinitialisation (top row), after 10, 000 steps of training (middle row), and at the completion of 300, 000steps of training (bottom row). At initialisation, all models have overlapping score distributions, butthe CA model operates at a much tighter range. After a few steps of training, the CA model alreadysees a peaky score distribution on the negative, while the DE model has a diffuse distribution. Thedistilled model however manages to overcome this, and produce a sharper distribution than even CA.
Figure 8: M3 SE distillation model margins on the MSMARCO-Passage test set. Compared to the DE modeltrained with one-hot labels, the distilled model is seen to more confidently distinguish positives fromnegatives, evidenced by the margin distribution having more mass on larger values.
