Figure 1: The intuition underpinning MV: A better learner is less affected by the mutated labels.
Figure 2: Performance of MV in model selection. MV, CV, Test denote MV score, CV accuracy, andTest Accuracy. Red and blue points are the original training data without noise (top-three rows) andwith 0.2 noise (bottom-three rows). Areas with different colours show the decision boundaries. Weobserve that MV captures well the match between decision boundaries and data patterns.
Figure 3: Changes in MV, CV accuracy, and test accuracy when increasing the maximum depth forDecision Trees. The x-axis ticks for car and connect differ to capture MV’s inflection point. We canobserve that, while CV and test accuracy agree with MV on the key influence point in most cases,they are less responsive to large depths (which lead to overfitting).
Figure 4: Influence of SVM parameters on CV and MV. The horizontal/vertical axis is gamma/C.
Figure 5: Influence of CNN’s dropout rate and learning rate on MV and validation/test accuracy.
Figure 6: Influence of training data size on MVwhen increasing maximum depths (horizontalaxis) of Decision Trees. For a given depth, largerdatasets tend to yield larger MV values.
Figure 7: Influence of mutation degree η on MV. The results show that different η lead to similar MVvalues and identical model validation conclusions.
Figure 8:	MV (first row) and test accuracy (second row) on training data of increasing size withdifferent maximum depths (horizontal axis). MV is responsive to data size changes; 2) MV no longerdecreases for large depths when the training data size is sufficiently large.
Figure 9:	Model selection results with UCI datasets (extended analysis for RQ1).
Figure 10:	Performance of MV in model selection with label swapping (MV-SL) and random labelreplacement (ML-RL).
Figure 11: Performance of MV in suggesting hyperparameters that follow Occam’s Razor on datasetCancer. The training data has only 300 samples. The low values of MV on large depths and numberof trees provide warnings to developers that the hyperparameters are over complex and violate therule of Occam’s Razor. The unnecessary complexity in the complex learner affects the interpretabilityof the learner, also making it vulnerable to training label attacks.
Figure 12: Correlation between MV training accuracy changes, and the new training accuracy basedon the original labels.
