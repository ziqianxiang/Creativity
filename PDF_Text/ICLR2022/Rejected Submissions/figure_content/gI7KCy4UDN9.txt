Figure 1: The cross-platform inconsistency caused by non-deterministic model inference. Thisinconsistency is a catastrophe to establish general-purpose image compression systems, and it isalmost inevitable when practicing learned image compression with floating-point arithmetic.
Figure 2: Diagram of joint autoregressive and hyperprior architecture for learned image compres-sion (Minnen et al., 2018; Cheng et al., 2020). AE/AD denote arithmetic en/de-coder. θ is the setof predicted entropy parameters (i.e. π, μ,σ), modeling the distribution of y element-wisely. Theblue, red and orange arrows denote encoding, decoding and shared data flows, respectively. Thehighlighted orange networks are shared by both encoding and decoding to estimate code probabilityfor AE/AD, demanding cross-platform consistency.
Figure 3: The integer-arithmetic-only inference. An offline-constrained integer-arithmetic-only re-quantization will be adopted during inference, which we will discuss in Section 3.2.
Figure 4: Visualization of the proposed 65-level STD parameter discretization. The orange dotsdenote binary-logarithmically distributed major levels and the blue ones are linearly interpolatedminor levels.
Figure 5: RD curves evaluated on Kodak. (a) RD performance of different models w/ or w/o quan-tization. The solid lines with cross markers denote original model inference with floating pointnumbers. The dots represent quantized 8-bit integer-arithmetic-only models, sharing the same coloras its floating-point version. (b) We test Sun et al. (2021) by quantizing the activations in bothPer-Channel and per-tensor manner. We also test Bane et al. (2019).
Figure 6: Comparison between scaling-clipping requantization and clipping-scaling requantization.
Figure 7: Comparison of our binary logarithm discretization and natural logarithm discretizationproposed in Bane et al. (2019). The Y-axis is the percent of discretized standard deviation introducedfor meaningful comparison: σ% = σσ~ X 100% With σmax = 256 for the natural logarithm oneand σmax = 32 for ours.
Figure 8: Comparison of quantized Minnen et al. (2018) w/ and w/o Brecq weight rounding recon-struction. The results are evaluated on Kodak.
Figure 9:BNSdFigure 10: The same models as Figure 5(a) evaluated on Tecnick.
Figure 10: The same models as Figure 5(a) evaluated on Tecnick.
Figure 11:18Under review as a conference paper at ICLR 2022C.6 Orthogonality with parallel context modelAn issue of learned image compression with context models is the inefficiency of serial decoding.
Figure 12: Operation diagrams of different learned image compression architectures. (a) an auto-encoder like model (Bane et al., 2017) (b) Scale hyperprior model. ha and hs represent hyperanalysis and synthesis while Z denotes the hyperprior (Bane et al., 2018). (c) Joint autoregressiveand hyperprior model. C denotes the context model (Minnen et al., 2018).
Figure 13: When the cross-platform inconsistency occurs, the discretized σ (and also μ) may becomedifferent between the sender Alice and the receiver Bob. Thus, the corresponding CDF index i andi + 1 differ, which ends up with failed decoding.
