Figure 1: Methods such as data aug-mentation try to make the training dis-tribution large at the expense of com-plexity and performance. In spite ofthese efforts, these methods still oftenfail to adequately cover the target distri-bution.
Figure 2: Our proposed Invariance through Inference architecture. The encoder F takes an obser-vation of target domain, and learns to fool the discriminator, while the discriminator D predictswhether the input is an encoded target observation or a latent sample from source buffer. This adver-sarial training encourages the distribution of encoder outputs to be similar to the latent embeddingsampled from the source buffer. Cdyn is the pretrained forward and inverse dynamics networks usedonly to guide the encoder during adaptation.
Figure 3: Samples from the modified DistractingCS with intensities increasing from (left-most col-umn) zero to (right-most column) one for the (top row) color, (middle row) camera pose, and (bottomrow) background distractions.
Figure 4: The gain of applying Invariance through Inference to various distracted baselines. Dashedlines denote the performance of the baseline agent in the target environment (i.e., zero-shot), whilesolid lines represent the performance gains of the base agent with ITI (our method).
Figure 5: Relative improvement (compared to zero-shot) as a function of adaptation steps whenapplying ITI to different baseline policies. As in Figure 4, each point represents the mean over ninedomains and five random seeds. The results correspond to an intensity value of 1.0.
