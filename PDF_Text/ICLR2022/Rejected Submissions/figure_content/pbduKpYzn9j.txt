Figure 1: (a) Student-1 has similar outputs with teacher and Student-2 has different outputs. Theleft column is the generated images of the original student. The image in the top right corner isthe teacher output. We demonstrate the intermediate results to show how RGB/LPIPS loss influ-ences the image generation of student. (b)(c) Cosine distance between the gradient of GAN lossand RGB/LPIPS loss. The x-axis denotes training steps. For similar student, RGB/LPIPS loss iscooperating with GAN loss. For dissimilar student, RGB/LPIPS loss is competing with GAN loss.
Figure 2: The style module S(∙) determines whether student can learn from the teacher's output. Ifstudent inherits teacher’s S(z), it can learn the teacher’s output well no matter how the convolutionC(∙) is initialized. If student uses random S(z), it cannot learn the teacher's output no matter howthe convolution C(∙) is initialized.
Figure 3: StyleGAN2 shows good factorization in the w space. It is possible to control a singlesemantic factor such as pose, lighting condition, glasses and hair color by moving the style vector wof a certain layer along a specific direction.
Figure 4: Image editing results.
Figure 5: Image projection results. In each pair, the left image is from real world (not from trainingset) and the right image is the projected result by our model. Our method can model the real facedistribution well.
Figure 6: Synthesized results on resolution 256×256. Random Ss (z) denotes the style moduleSs(z) is randomly initialized. For Two-Stage, we compress the original 8-layer style module into 5layers. The images of each row are generated using the same input noise z. Note that all the studentsare trained with mimicking loss. Random Ss(z) cannot generate images consistent with teacher dueto the different style module. Two-Stage mitigate output discrepancy issue by directly mimickingstyle module, but there are still semantics differences from teacher. Compared to CAGAN, ourgenerated images have fewer artifacts and are more similar to teacher.
Figure 7: Generation results on resolution 1024×1024. The synthesized images of Our method areof better quality than CAGAN. In several semantic factors such as beard, haircut and glasses, ourresults are more similar to the full-size model even though we do not inherit convolution weights.
