Figure 1: Results from the ImageNet-ILSVRC-2012 dataset (validation set). Left: The single imageidentified by the hand surveyed image selection of Birhane & Prabhu (2021). Right: Range ofsamples from our CLIP pre-selection. In summary, CLIP is detecting over 1,5k out of 50,000 imagesfrom ImageNet’s validation set as possible offending and over 30k out of 1,281,167 from the trainingset. Like our classifier, the pornographic classifier used in (Birhane & Prabhu, 2021) identifies the 5thimage in the first row as inappropriate. However, our classifier finds additional images along otherdimensions of inappropriate content. This provides an extension to private obfuscation of the facesand pornographic content classifiers provided by Birhane & Prabhu (2021). We blurred the images tonot offend the reader and to not violate privacy.
Figure 2: The SMID dataset. a) rating < 2.5 are samples with possible offensive content and> 3.5 images with positive content. b-c) PCA visualization of SMID feature space using differentpre-trained models. Coloring of data samples indicates the moral rating of the image’s content. Arating of four and five are immoral content and one and two moral content.
Figure 3: Performances of pre-trained models ResNet50 and ViT-B. The ResNet50 is pre-trained onImageNet1k, ImageNet21k (Deng et al., 2009) and the WebTextImage dataset (Radford et al., 2021).
Figure 4: Soft-prompt tuning on vision-language representation space. The squared data samplesvisualize the locations of the initial prompt and the crosses the final prompts. On the right, the nearestimage samples for each prompt are displayed.
