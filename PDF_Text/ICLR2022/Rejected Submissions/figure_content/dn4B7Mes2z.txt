Figure 1: Deep networks are biased toward low-rank: The approximated probability density function (PDF)of the effective rank ρ over the Gram matrix is computed from features of the networks. The Gram matrix iscomputed with 256 random inputs, and we use 4096 network parameter samples to approximate the cumulativedistribution function. The CDF is used to compute the PDF via the finite difference method. We apply Savitzky& Golay (1964) filter to smoothen out the approximation. There exists more probability mass for lower-rank rankembeddings when adding more layers. The experiment is repeated for both normal and uniform distributions.
Figure 2: Distribution at convergence:Rank distribution after training the networkwith gradient descent. The dotted line indi-cates the initial distribution, the solid lineindicates the converged distribution, and thegreen line indicates the task rank.
Figure 3: Low-rank bias & optimizers:Least-squares trained on linear neural net-works using various optimization methods.
Figure 4: Bias of parameterization: The effective rank of the Gram matrix from initialization to convergenceon various depth. For each depth, we train a linear network using gradient descent on least squares regression.
Figure 5: Training dynamics: Singular values of the Gram matrix for both original (left) and linearlyover-parameterized (right) model throughout training. The models are trained on CIFAR100 using SGD. Sincethe first few singular values dominate the distribution, we plot the negative log magnitude of the normalizedsingular values to better visualize how the intermediate singular values change. The singular values are sortedfrom largest to smallest σi < σi+1 (top to bottom in the figure) where blue means large and red means small.
Figure 6: Theoretical and empirical singular-value distributions: We show that even on finite matrices, thesingular-value distribution matches that of the theoretical distribution. This implies that deeper finite-widthlinear neural networks should have lower effective rank in practice.
Figure 7: Linear reparameterization: Fora model F, We can reparameterize any linearlayer to another functionally equivalent layer(shown in the box below). In this work Wemainly explore reparameterization of depth.
Figure 8: Comparing rank-measures: Comparison between various pseudo-metrics of rank whenvarying the number of layers. The threshold is set to T = 0.01 for threshold rank.
Figure 9: Least-sqaures ablation: Least-squares experiment using both effective-rank and thresholded rankmeasure. We run the experiments on various task-ranks 30, 16, 4. For thresholded rank, we use various thresholdvalues of τ = {0.001, 0.005, 0.01} and show that it correlates well with effective rank. The thresholded rankhas a downside of being sensitive to the threshold values, and one has to subjectively tune the suitable threshold,making it a suboptimal choice. The figure shows that depending on the rank of the task, the generalizationperformance depends on the depth. When the task rank is high, shallower models perform better, and when thetask rank is low, deeper models perform better. This aligns with our observation that the model parameterizationbiases the hypothesis search space in neural networks even if the models are effectively the same and span thesame set of functions.
Figure 10: Kernel ablation: We ablate our least-squares experiments by using various kernel distance functions.
Figure 11: Dynamics per layer: The singular values of the individual weights during CIFAR10 0training.On the left We have the unnormalized singular values, and on the right the distributions arescaled by the largest singular values. We uniformly subsample 24 singular values for the visualization.
Figure 12: Dynamics overlay: We overlay the singular values of the Conv4 weights. We observedthat the effective rank first rapidly decreases early on in the training and then bounces back UP slowlythroughout the rest.
Figure 13: Residual connections: The effectiverank of linear models trained with and withoutresidual connection on a low-rank least-squaresproblem. Contrary to feed-forward networks, resid-ual networks maintains the effective rank of theweights even when adding more layers. Residualnetworks without batch-normalization suffer fromunstable output variance after 16 layers.
Figure 14: Rank landscape: The landscape of the effective rank P of a linear function We parameterized eitherby a single-layer network (We = W) or a two-layer linear network (We = W2 Wi). The visualization illustratesa simplicity bias of depth, where the two-layer model has relatively more parameter volume mapping to lowerrank We. Both models are initialized to the same end-to-end weights We at the origin. Motivated by Goodfellowet al. (2015), the landscapes are generated using 2 random parameter directions u, v to compute f (α, β) =P(W + α ∙ U + β ∙ V) for the single-layer model and f (α, β) = ρ((W2 + ɑ ∙ u2 + β ∙ v2) ∙ (Wi + α ∙ ui + β ∙ vi))for the two-layer model (u = [u1 , u2], v = [v1 , v2]).
Figure 15: Kernel rank landscape: The landscape of the effective rank P computed on the kernelsconstructed from random features.
Figure 16: Gram matrices of networks: Gram matrices of neural networks trained with various non-linearitiesand depth. The Gram matrix is computed using the cosine-distance on the features of the test-set near zero-training loss. Increasing the number of layers decreases the effective rank of the Gram matrix on a variety ofnon-linear activation functions. The Gram matrix is hierarchically clustered (Rokach & Maimon (2005)) forvisualization. We observe the emergence of block structures in the Gram matrix as we increase the number oflayers, indicating that the embeddings become lower rank with depth.
