Figure 1: An overview of ourapproach. We tackle a feder-ated learning setup with clientsthat have different architecturesby using a shared Graph Hyper-network (GHN) that can adaptto any network architecture. Ateach communication round, a lo-cal copy of the GHN is trainedby each client using their owndata and network architecture (il-lustrated as graphs within eachclient; nodes represent layersand edges represent computa-tional flow), before returning tothe server for aggregation.
Figure 2: The GHN architecture. GHN is composed of two sub-networks: a Graph neural network(GNN) G and a set of MLPs {Hl}lâˆˆL, one for each layer type. We input a graph representation of anarchitecture to the GHN, with the colored node features corresponding to different layer types. Wethen apply the GNN to produce node features. Finally, the node features for the parametric layers arefurther processed by an MLP Hl to attain the final layer weights.
Figure 3: Generalization to unseen archi-tectures: our method (light blue) quicklyramps up to high performance and maintainsa considerable gap compared to training fromscratch (dark blue).
Figure 4: FLHA-GHN on CIFAR-10 Withdifferent communication rates.
Figure 5: The four client architectures used in CIFAR-10/100 experiments.
Figure 6: Hypernetwork weight initialization. We compare (blue:) a direct Kaiming weight initializa-tion He et al. (2015b) of a convolutional layer with (orange:) the resulting weight initialization by thehypernetwork, without (left) and with (right) our initialization scheme.
Figure 7: Generalization to a smaller 4 layer CNN architecture. Our method (light blue) quicklyramps up to high performance and maintains a considerable gap compared to training from scratchuntil convergence(dark blue).
