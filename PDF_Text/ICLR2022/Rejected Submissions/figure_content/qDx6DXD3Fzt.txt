Figure 1: ProoD's Architecture: Our method produces high confidence on the in-distribution sample(R.ImgNet) using Eq. (2). It also provides an upper bound on the worst-case confidence over allimages within a neighborhood of the shown OOD sample (OpenImages) using Eq. (3). Bounding theconfidence on this infinite set of OOD samples is possible without any loss in accuracy.
Figure 2: (Provable) OOD performance depends on the bias: Using CIFAR10 as the in-distribution and the test set of OpenImages as OOD we plot the test accuracy, AUC and GAUC (seeSection 4.2) as a function of the bias shift ∆ (see Eq. (8)). For small ∆ the AUCs tend to be worsethan for the OE model, but small bias shifts also provide stronger guarantees so some trade-off exists.
Figure 3: Left, Asymptotic confidence: We plot the mean confidence in the predicted in-distributionclass for different models as one moves away from CIFAR100 samples along the trajectories x + αn,where n ∈ [-0.5, 0.5]d and α ≥ 0. Only GOOD and ProoD converge to uniform confidence. Right,Adversarial asymptotic confidence: We try to find adversarial directions in which ProoD remainsat a constant high confidence, as opposed to converging to low confidence. We plot the maximum ofp^(i∣χ) across 100 adversarially chosen directions as one moves further in these directions by factorsof a. Note that p(i∣x) → 0 implies p(y∣x) → KK.
Figure 4: Bias selection for CIFAR100 and RImgNet: Using CIFAR100 (top) and R.ImgNet(bottom) as the in-distribution and the test set of OpenImages (or NotR.ImgNet respectively) as OODwe plot the test accuracy, AUC and GAUC as a function of the bias shift ∆ (see Eq. (8)).
Figure 5: Bigger models do not yield better guarantees: We show scatter plots of the GAUC ofdifferent architectures against the log of the number of trainable parameters in the model. The orangecross indicates the architecture that is used in the main paper. There is no clear dependence ofperformance on model size, so it is preferable to use fairly small models.
