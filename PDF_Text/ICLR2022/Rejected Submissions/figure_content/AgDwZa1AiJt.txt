Figure 1: Left: A schematic of the proposed two-stage inference framework where after distillationwe retain both the lightweight student model and the large teacher model. At inference time ifthe student finds an instance hard, we fall back to the teacher for a prediction. Right: The two-stage framework on CIFAR-100 image classification task using ResNets allows us to aggressivelytrade-off size of the student, thereby reducing the overall computation in expectation and achievingbetter accuracy compared to performing inference based on only the student. Note that the numbersannotated at each (student size, total compute cost) point on the plots denote the overall accuracy ofthe corresponding setup. For the two-stage inference, as we increase the size of the student, we canalways achieve an accuracy of 0.75 by delegating an appropriate fraction of instances to the teacher.
Figure 2: Comparison of various class-specific distillation methods on CIFAR-100. Baseline denotesthe standard distillation from (3). CD-I, CD-II, and CD-III denote the class-specific distillation ap-proaches defined in Sec. 4.1, Sec. A.1, and Sec. A.2, respectively, with |Lin| = L0 = 30. Accord-ingly, we compute in-domain accuracy on test instances from the 30 classes in Lin . Here, each litestudent (ResNet-32) employs margin-based delegation to the teacher. The right-most plot depictsthe (inference) latency vs. in-domain accuracy trade-off for the two-stage inference procedure.
Figure 3: Comparison of various class-specific distillation methods on ImageNet-1k. Baseline de-notes the standard distillation from (3). CD-I and CD-III denote the class-specific distillation ap-proaches defined in Sec. 4.1 and Sec. A.2, respectively, with |Lin| = L0 = 300. Accordingly, wecompute in-domain accuracy on the test instances from the 300 classes in Lin. Here, each lite student(MobileNetV3-0.75) employs margin-based delegation. The right-most plot depicts the (inference)latency vs. in-domain accuracy trade-off for the two-stage inference procedure.
Figure 4: Performance of the proposed two-stage inference on NLP tasks. Left half: Overall and in-domain accuracy of the margin-based distillation coupled with margin-based delegation on MNLI.
Figure 5: Comparison of various class-specific distillation methods on (subset of) ImageNet-21k.
Figure 6: Performance of two-stage inference procedure enabled by the margin-based distillationapproach from Sec. 4.2 on CIFAR-100. Again, Baseline denotes the standard distillation from (3)with a = 0 and b = 1. MD represent the margin-based distillation approach from (17). Here, eachlite student model (a ResNet-32 model) employs margin-based delegation. In-domain accuracy iscomputed on the test instances where student achieves a margin ρ of at least 0.4.
Figure 7: Performance of two-stage inference procedure enabled by the margin-based distillationapproach from Sec. 4.2 on ImageNet. Here, each lite student model (a MobileNetV3-0.75 model)employs margin-based delegation. In-domain accuracy is computed on the test instances wherestudent achieves a margin ρ of at least 0.4.
Figure 8: The inference-latency vs. in-domain performance trade-off realized by the two-stage in-ference procedure on the NLP tasks. MD denotes the margin-based distillation with label smoothingparameter α. Left: On MNLI dataset, RoBERTa-Large and MobileBERT are used as the teacherand student models, respectively. Here, Baseline corresponds to the two-stage inference enabled bythe standard distillation (cf. (3)). Right: On SQuAD dataset, T5 and MobileBERT are used as theteacher and student models, respectively. Here, Baseline denotes a normally trained MobileBERTused in combination with T5 model.
