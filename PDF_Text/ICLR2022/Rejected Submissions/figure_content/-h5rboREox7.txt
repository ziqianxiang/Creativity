Figure 1: Robust overfitting can be viewedas an early part of the epoch-wise double de-scent. Here we employ PGD training (Madryet al., 2018) on CIFAR-10 (Krizhevsky, 2009)with Wide ResNet (WRN) (Zagoruyko & Ko-modakis, 2016) and a fixed learning rate.
Figure 2: The illustration of the origin of implicit label noise in adversarial training. The traditionaladversarial label introduces the implicit label noise by inducing a distribution mismatch between theassigned label distribution and true label distribution of the adversarial example. Our rectified modelprobability as an alternative labeling can provably reduce this distribution match.
Figure 3: Without explicitly injecting labelnoise, standard training can also produce dou-ble descent if conducted on a dataset aug-mented by fixed and small adversarial per-turbation, which can be properly explainedby our implicit label noise perspective. De-tailed experiment settings can be found inAppendix F.3.
Figure 4: (Upper) NLL loss obtained on the validation set for different T and λ. (Bottom) Robust testaccuracy at the best and last checkpoint by adversarial training with the rectified model probabilitywith different T and λ. λ = 0.8 for grid search on T (Left) and T = 2 for grid search on λ (Right).
Figure 5: Our method can effectively mitigate robust overfitting for different datasets.
Figure 6: Varying sample size will shrink the area under the epoch-wise double descent curve, butwill not significantly distort its shape.
Figure 7: Effect of model on the epoch-wise double descent curveModel architecture. We also experiment on model architectures other than Wide ResNet, includingpre-activation ResNet-18 (He et al., 2016) and VGG-11 (Simonyan & Zisserman, 2015). We selectthese configurations to ensure approximately comparable model capacities2. As shown in Figure7, different model architectures may produce slightly different double descent curves. The seconddescent of VGG-11 in particular will be delayed due to its inferior performance compared to residualarchitectures.
Figure 8: The effect of the learning rate scheduler on the epoch-wise double descent curve inadversarial training. Modulating the model capacity can produce training curves with diversebehaviors. Different model architectures may produce slightly different double descent curves. Thetraining curve is smoothed by moving average with a window of 5.
Figure 9: (Left) Dependence of double descent on the perturbation radius. ε = 0/255 indicates thestandard training where no double descent occurs. (Right) Dependence of double descent on the dataquality. The curves are smoothed by a window of 5 epochs to reduce overlapping.
Figure 10: Dependence of double descent on the number of attack iterations. As more iterations areemployed in the inner maximization, neither epoch-wise nor model-wise double descent changessignificantly except for an extremely small model (WRN-28-1/8). For the model-wise double descent,only the test error at the best checkpoint is shown to avoid overlapped curves since the last checkpointachieves similar error as the best checkpoint in this training setting.
Figure 11: Even with extremely high Gaussian noise corrupting the training set, no significant doubledescent can be observed. This shows the input perturbation is not essential to produce double descent.
Figure 12: Examplemixup augmentation.
Figure 13: The histograms of optimal temperature (left) and interpolation ratio (right) of individualexamples.
