Figure 1: Comparison of the eigenvalue distributions of the left-hand side (exact expression) and the right-hand side (decoupled one) of Eq. (12) in the main text. They agree with each other except for very smalleigenvalues during the entire training dynamics.
Figure 2: Training dynamics of the loss and the SGD noise strength N . In the figure, we multiplied N by anumerical factor to emphasize that N is actually proportional to the loss in a later stage of the training. (Left)Fully-connected network trained by the Fashion-MNIST dataset. (Middle) Convolutional network trained bythe CIFAR-10 dataset. (Right) the loss vs N in the training of the convolutional network. The dashed line is astraight line of slope 1, which implies N(X L(θ).
Figure 3: (a) Exponent φ for the stationary distribution PSs(θ) (X L(θ)-φ for d = 1 in the linear regression.
Figure 4: Schematic illustration of the escape from a potential barrier.
Figure 5: Training dynamics of the loss function and the SGD noise strength N for (a) a fullyconnected network trained by the Fashion-MNIST dataset and (b) a convolutional network trainedby the CIFAR-10 dataset. In the figure, we multiplied N by a numerical factor to emphasize thatN is actually proportional to the loss in an intermediate stage of the training. Loss vs N for (c) afully connected network trained by the Fashion-MNIST and (d) a convolutional network trained byCIFAR-10. Dashed lines in (C) and (d) are straight lines of slope 1, which imply N α L(θ).
Figure 6: Hessian eigenvalues for a pre-trained neural network studied in Sec. 5.3. (a) The histogramof the Hessian eigenvalues. Most eigenvalues are close to zero, but there are some large eigenvalues,which correspond to outliers.
