Figure 1: Pruning 11 edges from afully-connected 21-edge network.
Figure 2: LeNet-300-100 trained on MNIST after pruning with SNIP, SynFlow and layerwise uni-form random pruning. Left: gaps between direct and effective compression. Right: SynFlow has abetter sparsity-accuracy trade-off than SNIP when plotted against direct compression (dashed), butnot against effective compression (solid curves fitted to dots that represent inidvidual experiments).
Figure 3: Effective versus direct compression across different pruning methods and architectures(curves and bands represent min/average/max across 3 seeds where subnetworks disconnect lastamong a total of 5 seeds).
Figure 4: Test accuracy (min/average/max) of subnetworks trained from scratch after being prunedby different algorithms plotted against direct (dashed) and effective (solid) compression. Dashedand solid curves overlap for SynFlow and SNIP-iterative. Solid curves are fitted to scatter data (notshown for clarity of the presentation) as in Figure 2.
Figure 5: Original methods for pruning at initialization (solid) and random pruning with correspond-ing layerwise sparsity quotas (dashdot). Test accuracy of the unpruned network is shown in grey.
Figure 6: Test performance of trained subnetworks after random pruning with different layerwisesparsity distributions. Original SynFlow (black) is shown for reference.
Figure 7: Effective compression pro-duced by regular (dashdot) and our ef-fective (solid) pruning on ResNet-18 ac-cording to ranking-based (left) and ran-dom (right) algorithms. Our procedureshelp pruning reach target effective spar-sity, falling short only when the subnet-work is on the brink of disconnection.
Figure 8: Left: effective versus direct compression of VGG-16 When pruned by different algorithms.
Figure 9: Layerwise direct compression quotas of LeNet-5 (top) and VGG-16 (bottom) associatedwith SynFlow (left), our IGQ (middle), and LAMP (right). Percentages indicate layer sizes relativeto the total number of parameters; colors are assigned accordingly from blue (smaller layers) to red(larger layers). Curves of LAMP and SynFlow end when the underlying network disconnects.
Figure 10: Test performance of retrained subnetworks after pruning With different magnitude-basedmethods. Uniform+ is not shown for LeNet-300-100 since it is designed for convolutional networks.
