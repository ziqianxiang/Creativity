Figure 1: Above: Inference of latent MNIST digit classes with negative label supervision using asmall CNN trained on the RQ criterion (§2.1). Below: Joint inference of latent pixel classes in animage (a). The prior beliefs Pi (I) over three classes - Sky (red), boat (green), water (blue) - aremanually set (b). A small CNN using no data except (a,b) infers the posterior classes (c).
Figure 2: Cross-entropy and implicit QR / RQlosses in PyTorch. Note that normalization in (2)is done within a batch, rather than across the en-tire dataset. In practice, this may be sufficient ifbatches are large and representative of the diver-sity in the data. Otherwise, the denominator in (2)may need to be updated in an online fashion.
Figure 3: Above: Accuracies of MNIST and CIFAR-10 classifiers trained with varying numbers ofnegative labels per example; the lighter variant of each color and marker shows the peak accuracyover 300 training epochs. (Average of 10 runs with standard error region.) Below: Confusion ma-trices of MNIST classifiers in the course of training on batches of 128 pairs of digits. The trajectoryof convergence to the diagonal shows that uncertainty is first resolved for the digits 0 and 9, then 1and 8, etc.
Figure 4: Predictions of models trained with QR loss on the NLCD-only prior in the Chesapeakeregion, shown on regions of 1000×1000 pixels in Pennsylvania and 500×500 pixels in New York.
Figure 5: Prior generation process for land cover mapping: “NLCD only prior” as used in §4.3 and“hand-coded prior” and ‘learned prior” as used in §4.4.
