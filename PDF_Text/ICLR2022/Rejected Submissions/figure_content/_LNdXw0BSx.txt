Figure 1: Task formulation: Entity-driven generation for increased coherence and consistency.
Figure 2: D-MNEMELM: The entity memory is initialized based on the (contextualised) embeddingrepresentations of the prompt tokens (a). Next, the narrative is processed ChUnk-by-chunk. Ateach model layer, there is a (pre-trained) self-attention block that considers all previous context,and a new, randomly initialized cross-attention block for attending to the entity memory. The twocomponents are combined via a gating mechanism (b). Finally, the representation of the currentchunk is used for updating the dynamic values of the entity memory (c). The cross-attention scoresare regularized during training based on gold token-level entity mentions (d).
Figure 3: Perfomance of D-MNEMELM versus VANILLALMthe sequence length is equal to 512 tokens.
Figure 4: Entity negative log-likelihood and percentage degradation in NLL caused by shortening the Trans-formerXL memory length from 500 to 100 tokens for VanillaLM and D-MnemeLM on both datasets.
Figure 5: Questions asked during human evaluation on the generated stories. After reading allstories and answering all questions, human judges also select the best and worst story from the threeprovided stories.
