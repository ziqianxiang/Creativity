Figure 1: Calibration regularized training using MSE loss and CE2(a) Effect of the bandwidth bFigure 1b shows the effect of the regularization parameter γ on the performance of a ResNet-110model. The orange point represents a model trained with MSE loss, and the blue points (KDE-MSE)correspond to models trained with regularized MSE loss by an L2 calibration error for differentvalues of γ . As expected, the calibration regularized training decreases the L2 calibration error atthe cost of slightly increased error.
Figure 2: Extension of the binned estimator to the probability simplex, compared with the KDE-ECE. The KDE-ECE achieves a better approximation to the finite sample, and accurately modelsthe fact that samples tend to be concentrated near low dimensional faces of the simplex.
Figure 3: Relationship between the KDE-ECE estimates and their corresponding binned approxima-tions on the three types of calibration. Each point represents a ResNet-56 model trained on a subsetof three classes from CIFAR-10. The 3000 probability scores of the test set are assigned in 25 binswith adaptive width for the binned estimate. A bandwidth of 0.001 is used for KDE-ECE.
Figure 4: Canonical calibration on CIFAR-10J MMCE•	£.1KDE-CRE•	Li KDE-CRE•	MMCE•	L1KDE-CRE*	L3 KDE-CRE1e-5(a) ResNet-110• MMCE• L1KDE-CRE・ L3 KDE-CRE(c) Wide-ResNet-28-10(d) DenseNet-40Figure 5: Marginal calibration on CIFAR-100MMCELi KDE-CREL, KDE-CRE• CREACC
Figure 5: Marginal calibration on CIFAR-100MMCELi KDE-CREL, KDE-CRE• CREACC(b) ReSNet-110(SD)• CRE•	MMCE•	Li KDE-CRE•	L3 KDE-CRE9Under review as a conference paper at ICLR 2022ReferencesA. Appice, P. Rodrigues, V. S. Costa, C. Soares, Joao Gama, and A. Jorge. Novel decompositionsof proper scoring rules for classification : Score adjustment as precursor to calibration. 2015.
Figure 6: Top-label and marginal calibration on CIFAR-10.
Figure 7: Top-label and marginal calibration on CIFAR-100Figure 8: An example of a simplex binned estimator and kernel-density estimator for CIFAR-10(c) Corresponding Dirichlet KDEaveraged over 120 ResNet-56 models trained on a subset of three classes from CIFAR-10. Bothestimators are biased and have some variance, and the plot shows that the combination of the two isin the same order of magnitude. The empirical convergence rates (slope of the log-log plot) is givenin the legend and is shown to be close to the theoretically expected value of -0.5.
Figure 8: An example of a simplex binned estimator and kernel-density estimator for CIFAR-10(c) Corresponding Dirichlet KDEaveraged over 120 ResNet-56 models trained on a subset of three classes from CIFAR-10. Bothestimators are biased and have some variance, and the plot shows that the combination of the two isin the same order of magnitude. The empirical convergence rates (slope of the log-log plot) is givenin the legend and is shown to be close to the theoretically expected value of -0.5.
Figure 9: Relationship between the ECE metric based on binning and kernel density estimation(KDE-ECE) for the three types of calibration: canonical, marginal and top-label. In every row, adifferent number of points are used to approximate the KDE-ECE.
Figure 10: Canonical calibration on Kather using a Resnet-50 modelbatch size for the regularization varies. The orange point is our normal experimental set-up with justone dataloader (i.e. the same points are used for loss and KDE-ECE computation) as a comparison.
Figure 11:	KDE-ECE estimates and their corresponding binned approximations on the three typesof calibration for varying number of points used for the estimation. The ground truth is calculatedusing 3000 probability scores of the test set. For the binned estimate, the points are assigned in 25bins with adaptive width. A bandwidth of 0.001 is used for KDE-ECE.
Figure 12:	Absolute difference between ground truth and estimated ECE for varying number ofpoints used for the estimation. The ground truth is calculated using 3000 probability scores of thetest set. For the binned estimate, the points are assigned in 25 bins with adaptive width. A bandwidthof 0.001 is used for KDE-ECE. Note that the axes are on a log scale.
Figure 13:	Training with different batches for loss and regularization (2 KDE-CRE), where the batchsize for the loss is fixed and the batch size for the regularization varies. The orange point shows ourusual experimental set-up where we train with only one batch (KDE-CRE). Upper row: marginal,lower row: top-label.
