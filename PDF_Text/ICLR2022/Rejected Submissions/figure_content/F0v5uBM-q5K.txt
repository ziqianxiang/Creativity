Figure 1: Power-accuracy trade-off at post training. For each pre-trained full-precision model,we used (a) ZeroQ (Cai et al., 2020) and (b) BRECQ (Li et al., 2021) to quantize the weights andactivations at post-training. In (a) we quantize to 4 bits and in (b) to 2 bits. Then, we convertthe quantized models to work with unsigned arithmetic (—), which already cuts down 33% ofthe power consumption in (a) and 58% in (b) (assuming a 32 bit accumulator). Using our PANNapproach to quantize the weights (at post-training) and remove the multiplier, further decreases powerconsumption and allows achieving higher accuracy for the same power level (↑). See more examplesin Appendix A.5.1to obtain even improved results. Our approach is based on careful analysis of the power consumed byadditions and multiplications, as functions of several factors. We rely on bit toggling activity, whichis known to be the main factor affecting dynamic power consumption, and support our theoreticalanalysis with accurate gate-level simulations on a 5nm process.
Figure 2: Multiply-accumulate. The multiplieraccepts two b-bit inputs. The product is thensummed with the previous B bit sum, whichawaits in the flip-flop (FF) register.
Figure 3: (a) Each color represents the power of an unsigned bx-bit MAC for some value of bx . InPANN, we can move on a constant power curve by modifying the number of additions per element R(vertical axis) on the expense of the activation bit width bx (horizontal axis). (b) We plot the ratiobetween the quantization errors of a regular quantizer (RUQ) and a PANN tuned to work at the samepower budget. As can be seen, PANN outperforms RUQ at the low bit widths (where the MSE ratiois above 1). It should be noted that at the high bit widths, both approaches achieve low errors inabsolute terms, but RUQ is relatively better. In the Gaussian setting, which is closer to the distributionof DNN weights and activations, the range over which PANN outperforms RUQ is a bit larger.
Figure 4: (a) In red we plot the power consumed by a multiplication operation as measured in ourPython simulation. This curve agrees with the theoretical power model in Eq. (3) in the paper, i.e.
Figure 5: (a) Using our Python simulation, We measured the average number of toggles betweenunsigned multiplication and signed multiplication for bit widths of 4-8. As can be seen, we obtainedan average ratio of 92% (red curve). This observation is aligned with the power measured in our5nm silicon process (blue curve). (b) Unlike the uniform distribution, we can see that the majority ofvalues occupies roughly half of the allowed interval and therefore on average, we observe a bit lesstoggles than with the uniform distribution (here the bit width is b = 8).
Figure 6: Counting bit toggles in the multiplier’s internal adders. We depict a snapshot of themultiplier’s internal components at two consecutive addition instructions. At state i - 1 we sum 1111and 0001 and at state i we sum 1111 and 0100. In our python simulation, we compare between thebit status of consecutive operations therefore in this example, we will count four toggles (two in theinput words and two in the internal carry outputs).
Figure 7: Python simulation for signed integers. On the left, we plot the power consumed by themultiplier. We counted the toggles at the inputs of the multiplier (row 1 in Table 1 of the paper) aswell as the toggles inside its internal units (row 2 in Table 1). We can see that the power measured inour simulations closely agrees with power model in Eq. (1), 0.5b2 +b. On the right, we plot the powerconsumed by the accumulator, where the label “acc inputs” refers to the power due to the bit flips atits input (row 3 in Table 1). In this case B = 32 and therefore we observe a constant power of 16.
Figure 8: Python simulation for unsigned integers. Here we repeat the experiment of Fig. 7,but with numbers drawn only from the interval [0, 2b-1) for both the uniform and the Gaussiandistributions. On the left, we can see that the overall power of the multiplier has not changed muchand is aligned with Eq. (3) in the paper. However, on the right we can see that due to the use ofunsigned values, the power consumed by the toggling at the accumulator inputs is dramaticallyreduced (0.5bacc instead of 0.5B). The rest of the power contributors did not change much.
Figure 9: Working with bw < bx in a Booth encoder multiplier. On the right, we uniformlydrew bx -bit numbers from [-2bx-1, 2bx-1) and bw -bit numbers from [-2bw-1, 2bw -1), for variousbw ≤ bx . We can see that the power is affected only by the larger bit width (bx), and remainsnearly constant when reducing only bw. On the left, we repeat the same experiment, however withunsigned values, where the bx -bit input is uniformly drawn from [0, 2bx-1) and the bw -bit input from[0, 2bw-1). Here, there is a slight benefit in decreasing only bw. The black dashed curve connects thepower measurements for the cases where bw = bx , and follows the parabolic behaviour.
Figure 10: Working with bw < bx in a simple serial multiplier. On the right, we uniformly drewbx-bit numbers from [-2bx-1, 2bx-1) and bw -bit numbers from [-2bw-1, 2bw-1) such that bw ≤ bx.
Figure 11:	(a) Based on our power model, we show that a significant amount of power can besaved with minimal effort, by switching to work with unsigned numbers. Here we assume a 32 bitaccumulator. (b) Any weight matrix W can be split into its positive and negative parts (See Sec. 4).
Figure 12:	Power-accuracy trade-off at post training with different bit width accumulators. Werepeat the experiment of Fig. 1, however this time we assume a 21 bits accumulator in (a) and a 17bit accumulator in (b). Theretofore, when converting the quantized models to work with unsignedarithmetic (—), it cuts down 21% of the power consumption in (a) and 39% in (b).
Figure 13: Power-accuracy trade-off at post training. For each pre-trained full-precision model,we used (a) ACIQ (Banner et al., 2019) and (b) GDFQ (Shoukai et al., 2020) to quantize the weightsand activations to 4 bits at post-training. Converting the quantized models to work with unsignedarithmetic (—), already cuts down 33% of the power consumption (assuming a 32 bit accumulator).
Figure 14: Power-accuracy trade-off at post training. For each pre-trained full-precision model,we used (a) ZeroQ (Cai et al., 2020) and (b) GDFQ (Shoukai et al., 2020) to quantize the weightsand activations to 2 bits at post-training. Converting the quantized models to work with unsignedarithmetic (—), already cuts down 58% of the power consumption (assuming a 32 bit accumulator).
Figure 15: Optimal bit width analysis for PANN. The first two rows depict the MSE as a functionof the activation bit width bχ for the cases where the weights and the activations are uniformlydistributed and for the setting in which they are Gaussian (the activations are further subjected to aReLU function in this case). The third row shows the classification error of a ResNet18 model onImageNet. Although the precise value of the optimal bx is a bit different than the first two rows, thequalitative behavior is similar. For both for the Gaussian simulation and the ImageNet results, weused ACIQ Banner et al. (2019) to quantize the activations.
