Figure 1: Reducing costs with curvature sub-sampling, MC-sampling, and parameter groups.
Figure 2: Gradient, curvature and noise during training. Columns show the 3c3d architecture atinitialization (left), an early (epoch 5, center), and advanced (epoch 68, right) stage of training onCIFAR- 1 0 with SGD (hyperparameters from Schneider et al. (2019); Dangel et al. (2020), details inAppendix B.2). For each direction k, characterized by its curvature Î»k, we monitor (a) the directionalgradient magnitude; (b) gradient-eigenvector alignment; (c,d) SNRs of curvatures and gradients.
Figure 3: Comparison of optimizers on noisy quadratic. Statistics for different optimizers during(left) and at the end (center) of training over 100 runs. For the loss values, we present mean (dashedline), median (solid line) and a confidence interval (shaded area) between the lower and upper quartile.
