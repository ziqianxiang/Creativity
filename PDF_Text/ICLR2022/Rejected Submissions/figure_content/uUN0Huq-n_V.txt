Figure 1: System overviewdefine an action as a new chord being played, and the state is represented by the previous chord. Themidi fragments can be transformed into pianorolls, with each pianoroll representing one trajectory.
Figure 2: Reinforcement learning tuning architecture5	ExperimentsThe code of the deep learning models for experiments was implemented in python 3.6 under thetensorflow framework version 1.15. The code is extended from three major open sources. The codefor Daniel Johnsonâ€™s Bi-axial LSTM is based on the implementation by Nikhil Kotecha (Kotecha,2018b). The code for the AIRL is based on the implementation by Justin Fu (Fu, 2018). The codefor the deep RL training is based on the implementation done by Google Magenta Team (Team,2020).
Figure 3: User study overall resultsmusic theory all the time. On the other hand, a system based on music theory alone cannot generatea versatile and creative music piece. The proposed model, the model tuned with a combination ofmusic theory and AIRL rewards (MT+AIRL Model), seems to be a compromise that can generatecompositions that comply with music theory rules, keeping uniformity through the composition andnot showing a fragmented style, while still showing some degree of creativity and variety. Thismodel also has consistently the best results in both objective and subjective evaluation.
