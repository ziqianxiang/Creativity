Figure 1: Basic insight into reward redistribution. Left panel, Row 1: An agent has to take a keyto unlock a door. Both events increase the probability of receiving the treasure, which the agentalways gets as delayed reward, when the door is unlocked at sequence end. Row 2: The Q-functionapproximation typically predicts the expected return at every state-action pair (red arrows). Row 3:However, the Q-function approximation requires only to predict the steps (red arrows). Right panel,Row 1: The Q-function is the future expected return (blue curve). Green arrows indicate Q-functionsteps and the big red arrow the delayed reward at sequence end. Row 2 and 3: The redistributedrewards correspond to steps in the Q-function (small red arrows). Row 4: After redistributing thereward, only the redistributed immediate reward remains (red arrows). Reward is no longer delayed.
Figure 2: The function of a protein is largely determined by its structure. The relevant regions ofthis structure are even conserved across organisms, as shown in the left panel. Similarly, solving atask can often be decomposed into sub-tasks which are conserved across multiple demonstrations.
Figure 3: The five steps of Align-RUDDERâ€™s reward redistribution. (I) Define events and turndemonstrations into sequences of events. Each block represent an event to which the originalstate is mapped. (II) Construct a scoring matrix using event probabilities from demonstrationsfor diagonal elements and setting off-diagonal to a constant value. (III) Perform an MSA of thedemonstrations. (IV) Compute a PSSM. Events with highest column scores are indicated at the toprow. (V) Redistribute reward as the difference of scores of sub-sequences aligned to the profile.
Figure 4: Example of alignment and reward redistribution for demonstrations of ObtainDiamond.
Figure 5: Comparison of Align-RUDDER and other methods on Task (I) (left) and Task (II) (right)with respect to the number of episodes required for learning on different numbers of demonstrations.
Figure 6: Comparing the consensus frequencies between behavioral cloning (BC, orange), wherefine-tuning starts, the fine-tuned model (blue), and human demonstrations (green). The plot is insymmetric log scale (symlog in matplotlib). See Appendix Fig. A.19 for mapping of letters to items.
