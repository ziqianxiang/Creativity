Figure 1: Diagram presenting the approach to infer weights for the decision layer for new classes4 Results4.1	Experimental setupIn this section we present the experimental setup. All experiments presented for the FASHIONMNIST and CIFAR datasets were performed using google collaboratory. Experiments using thePl@ntNet dataset were performed using a Dell PowerEdge R730 server, with 2 CPUs Intel (R) Xeon(R) CPU E5-2690 v3 @ 2.60GHz; 768 GB of RAM; and running on a Linux CentOS 7.7.1908 kernelversion 3.10.0-1062.4.3.e17.x86_64. The machine is equipped with a single NVIDIA Pascal P100GPU, with 16GB RAM. Implementations were performed using Python 3.7 along with the Kerasdeep learning library.
Figure 2: Embedding obtained on the cifar10 dataset when using a latent space with two dimensionsusing NSL, each color represents a different class. Inner points are the class weights while outerpoints come from the training set, notice how classes from the outer circle are aligned with the innercircleK *	NSL	Triplet	Constrative-2-	0.852	0.663	^07003	0.725	0.570	0.5514	0.629	0.406	0.4225	0.545	0.296	0.328Table 1: Model results for the cifar 10 dataset. K * refers to the amount of unseen classes, whileother columns refer to the method and accuracy obtained on the test set.
Figure 3: Distribution of the training dataset. Note the long tail distribution presenting how thereare many classes with small amounts of data and few with a large amount.
Figure 4: Comparing models sharing the same architecture and optimized upon the same data andnumber of epochs, but with different amount of seen classes. We evaluate their ability to classifynovel classes.
Figure 5:	Analysis for the joint scenario showing the results for different models on seen and unseenclasses.
Figure 6:	Analysis for the joint scenario showing the results for different models on the unseenclasses.
Figure 7:	Accuracy obtained in different few shot scenarios for the cifar10 dataset when comparingusing inferred weights versus further optimizing them.
Figure 8:	F1 score on the test set for three different datasets where we show results when weightsare: (a) Trained: the ones found during optimization; (b) Single anchor: weights are inferred usinga single random anchor example; and (c) weights are inferred using the training set to build a classprototype.
Figure 9:	Cosine similarity between inferred weights, obtained as in Eq. 3, and optimized weights,obtained through cross-entropy optimization using the incremental learning constraint, for differentnumbers of novel classes on the fashion MNIST datasetFigure 11 first shows the cosine similarity between the inferred and the optimized weights for differ-ent numbers of unseen class. As we can observe, when the number of novel classes is small, the two12Under review as a conference paper at ICLR 2022sets of weights are almost identical, which means that the inferred weights are as good as the onesoptimized through incremental learning (while being much faster and simpler to compute). Withlarger numbers of novel classes, we can observe that the mean cosine similarity is still very high.
Figure 10:	Ratio of accuracy after optimization versus inferred weights on unseen = 10 - seenclasses when optimization occurs on seen classes for the fashion mnist dataset.
Figure 11: Cosine similarity between inferred weights, obtained as in Eq. 3, and optimized weights,obtained through cross-entropy optimization using the incremental learning constraint, for differentnumbers of novel classes on the fashion MNIST dataset when using only two samples to generatethe novel classB	ExperimentsAll performed experiments are available via the following google drive folder: https://drive.
