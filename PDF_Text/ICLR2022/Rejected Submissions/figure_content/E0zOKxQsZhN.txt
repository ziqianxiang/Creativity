Figure 1: Implementation Matters for RecurrentModel-Free RL. This paper identifies critical designdecisions for recurrent model-free RL that outperformsnot only prior implementations (e.g. PPO-GRU andA2C-GRU from Kostrikov (2018)), but also purpose-designed methods (e.g. VRM from Han et al. (2020)).
Figure 2: Learning curves on two meta RL environments. The single best variant of our implementationon recurrent model-free RL can surpass the specialized meta RL method off-policy VariBAD (Dorfman et al.,2020), and match the performance of an “Oracle” policy that gets to observe the hidden state.
Figure 3: Learning curves on one robust RL environment, Cheetah-Robust. We show the average returns(left figure) and worst returns (right figure) of each method. The single best variant of our implementation onrecurrent model-free RL can greatly outperform the specialized robust RL method MRPO (Jiang et al., 2021),and surpass the Oracle that are trained with same simulation and gradient steps.
Figure 4: Learning curves on generalization in one RL environment, Hopper-Generalize. We show theinterpolation success rates (left figures) and extrapolation success rates (right figures) of each method. Thesingle best variant of our implementation on recurrent model-free RL can be par with the specialized methodEPOpt-PPO-FF (Rajeswaran et al., 2017a) in interpolation and outperform it in extrapolation. The data ofEPOpt-PPO-FF and A2C-RC (a recurrent model-free on-policy RL method) are copied from the Table 7 & 8in Packer et al. (2018).
Figure 5: Comparison between shared and separate re-current actor-critic architecture with all the other hyper-parameters same, on Semi-Circle, a toy meta RL environ-ment. We show the performance metric (left) and also gra-dient norm of the RNN encoder(s) (right, in log-scale). Forthe separate architecture, :critic and :actor refer to theseparate RNN in critic and actor networks, respectively.
Figure 6: The network architecture of our implementation on recurrent model-free RL. The leftpart shows the actor network, and the right shows the critic network. Each block shows a trainablemodule, with independent weights. We italicize the previous action and reward encoders as they areoptional. By default, each encoder has one hidden layer, each RNN is one-layer LSTM or GRU,each MLP has two hidden layers.
Figure 7: Learning curves on “standard” POMDP environments that preserve positions &angles but occlude velocities in the states in PyBullet (Coumans & Bai, 2016) (namely “-P”).
Figure 8: Learning curves on “standard” POMDP environments that preserve velocities butocclude positions & angles in the states in PyBullet (Coumans & Bai, 2016) (namely “-V”). Weshow the results from the single best variant of our implementation on recurrent model-free RL, thepopular recurrent model-free on-policy implementation (PPO-GRU, A2C-GRU) (Kostrikov, 2018),and also model-based method VRM (Han et al., 2020). Note that VRM is around 5x slower thanours, so we have to run 0.5M environment steps for it.
Figure 9: Learning curves on meta RL environments. We show the results from the singlebest variant of our implementation on recurrent model-free RL, the specialized meta RL methodVariBAD (Dorfman et al., 2020)Semi-Circlevarιbadarkovianandomenv_steps21Under review as a conference paper at ICLR 2022Oo52OoooOooo0 5 0 52 116>(DIEn4 ①」2000--- Ours: recurrent model-free RL
Figure 10: Learning curves on robust RL environments. We show the average returns (left fig-ures) and worst returns (right figures) from the single best variant of our imPlementation on re-current model-free RL, the sPecialized robust RL method MRPO (Jiang et al., 2021). Note that ourmethod is much slower than MRPO, so we have to run our method within 3M environment stePs.
Figure 11: Learning curves on generalization in RL environments. We show the interPolationsuccess rates (left figures) and extraPolation success rates (right figures) from the single best vari-ant of our imPlementation on recurrent model-free RL. We also show the final Performance of thesPecialized method EPOPt-PPO-FF (Rajeswaran et al., 2017a) and another recurrent model-free(on-Policy) RL method (A2C-RC) coPied from the Table 7 & 8 in Packer et al. (2018).
Figure 12: Ablation study of our implementation on “standard” POMDP environments thatpreserve positions & angles but occlude velocities in the states in pybullet (Coumans & Bai,2016) (namely “-P”). We show the single factor analysis on the 4 decision factors including RL,Encoder, Len, and InPuts for each environment.
Figure 13: Ablation study of our implementation on “standard” POMDP environments thatpreserve velocities but occlude positions & angles in the states in pybullet (Coumans & Bai,2016) (namely “-V”). We show the single factor analysis on the 4 decision factors including RL,Encoder, Len, and Inputs for each environment.
Figure 14:	Ablation study of our implementation on meta RL environments. We show the singlefactor analysis on covering all the decision factors25Under review as a conference paper at ICLR 20221	2env_stepsRLO OO OO O2 I 13	0	12	3le6	env_steps le6HoPPer-RobustEncoder> 2000%⅛ 100000	12	3
Figure 15:	Ablation study of our implementation on robust RL environments. We show thesingle factor analysis on the 4 decision factors including RL, Encoder, Len, and InPuts for eachenvironment in both average returns and worst returns.
Figure 16:	Ablation study of our implementation on generalization in RL environments. Weshow the single factor analysis on the 3 decision factors including RL, Len, and Inputs for eachenvironment in both interpolation and extraploation success rates.
Figure 17: Comparison between shared and separate recurrent actor-critic architecture with allthe other hyperparameters same, on Pendulum-V, a toy “standard” POMDP environment. We showthe performance metric (left) and also the gradient norm of the RNN encoder(s) (right, in log-scale).
Figure 18: Final normalized returns of our implemented recurrent model-free RL algorithm withthe same hyperparameters, and the prior method VRM (Han et al., 2020) across the eight environ-ments in “standard” POMDPs, each of which trained in 0.5M simulation steps.
Figure 19: Ablation study on RNN architecture in “standard” POMDPs (“-P”). We show twomodels that are only different from our best single variant in the RNN architectures, namely using2-layer stacked GRU (td3-2gru) and PF-GRU (Ma et al., 2020a;b) (td3-pfgru), instead of 1-layerGRU.
Figure 20: Ablation study on RNN architecture in “standard” POMDPs (“-V”). We show twomodels that are only different from our best single variant in the RNN architectures, namely using2-layer stacked GRU (td3-2gru) and PF-GRU (Ma et al., 2020a;b) (td3-pfgru), instead of 1-layerGRU.
Figure 21: Ablation study on RNN architecture in Meta RL. We show one model that are onlydifferent from our best single variant in the RNN architecture, namely using 2-layer stacked LSTM(td3-2lstm), instead of 1-layer LSTM.
Figure 22: Ablation study on RNN architecture in robust RL. We show one model that are onlydifferent from our best single variant in the RNN architecture, namely using 2-layer stacked LSTM(td3-2lstm), instead of 1-layer LSTM.
Figure 23: Ablation study on RNN architecture in generalization in RL. We show one modelthat are only different from our best single variant in the RNN architecture, namely using 2-layerstacked LSTM (td3-2lstm), instead of 1-layer LSTM.
Figure 24: Additional learning curves on meta RL environments. We show the results from thesingle variant of our implementation on recurrent model-free RL (sac-gru-oar-400), and the spe-cialized meta RL method off-policy VariBAD (Dorfman et al., 2020) (their official implementationVariBAD-BOReL and our re-implementation VariBAD).
