Figure 1: The proposed architecture. The red-colored components in the diagram indicates beingboth pre-trained, and frozen during training the summarizer model. The green-colored units arelearned from scratch for summarization task.
Figure 2: The linear autoencoder architecture with 3 encoder (L1, L2, L3), and decoder (L4, L5, L6)layers. Tensors X , X0 , and Z are representing the input, output, and the compressed latent repre-sentation respectively. The autoencoder maintain the same sequence length (S) during compressionand only reduce the embedding size (D).
