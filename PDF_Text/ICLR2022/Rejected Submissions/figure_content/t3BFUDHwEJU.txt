Figure 1: (a) Hard exploration problem example, (b) optimal value function of geometrically discounted rlwith a discount factor γ = 0.99 and (c) non-geometrically discounted RL.
Figure 2: The normalized coefficients ΦD (t) over time for different values of the delay parameter DIntuitively, the proposed criterion (LD) describes the goal of an agent that discards short term gainsin favor of long term discounted returns. In the following, we consider yet a more diverse problemformulation by using a linear combination of the delayed losses. Let Lη be the following objectivefunction defined as:DLη (π, r) := Eπ,p0 Xη(t)rt	such that η(t) = XwdΦd(t)	(2)t	d=0where the depth D ∈ N and the coefficients wd ∈ R for any d ∈ [0, D] are known.
Figure 3: Hard exploration environmentsStationary solutions: We start by evaluating the performances of the learned policies using Gen-eralised Soft Actor Critic (GSAC) (Algorithm 1). As discussed earlier, unlike the geometricallydiscounted setting, the policy update is not guaranteed to improve the performances. For this rea-son, depending on the initialisation, the algorithm can either converge to the optimal stationarypolicy, or get stuck in a sequence of sub-optimal policies.
Figure 4: Learned stationary policy (GSAC) performances as the depth parameter varieshorizon H = 0. We also reported the performances of the best stationary policy learned using GSACfor a depth parameter D = 5.
Figure 5: Learned H-close optimal control4.2	Simulated Continuous Control BenchmarksIn this section, we propose to evaluate our methods on several continuous robotics domains. To thisend, we consider 5 different environments within the mujoco physics engine (Todorov et al., 2012):Walker2d, Hopper, HalfCheetah, Ant and Humanoid. As results in the tabular settings showed thatthere exists a threshold beyond which increasing the value of the delay D hurts the performance, wefix D = 1, where only two discount factors γ0 and γ1 are used. We compare our proposed methodsto the classic SAC algorithm to investigate the implications of using both critics Qθ0 and Qθ1 .
Figure 6: Training curves on continuous control benchmarks. Generalized Soft-Actor-Critic (GSAC) showsbetter sample efficiency across all tasks5 ConclusionDesigning human-like autonomous agents requires (among other things) the ability to discard shortterm returns in favor of long term outcomes. Unfortunately, existing formulations of the reinforce-ment learning problem stand on the premise of either discounted or average returns: both providinga monotonic weighting of the rewards over time.
