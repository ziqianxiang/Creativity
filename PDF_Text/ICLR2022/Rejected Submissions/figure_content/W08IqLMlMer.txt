Figure 1: Overview of the pipeline for pre-training the general policy and fine-tuning it online.
Figure 2: The detailed model structure for offline and online MADT.
Figure 3: Performance of offline MADT comparing with baselines on four easy or (super-)hard SMACmaps. The dotted lines represent the mean values in the training set. Columns (a-d) are averagereturns from (poor, medium, good) datasets from top to the bottom.
Figure 4: The average returns with and without the pre-trained model..
Figure 5: Few-shot and Zero-shot validation results. (a) shows the average returns of the universal MADTpre-trained from all five tasks data and the policy trained from scratch, individually. We limit the environmentinteraction to 2.5M steps. (b) shows the average returns of a held-out map (3s_vs_4z), where the universalMADT is trained from data on (2m_vs_1z, 2s_vs_1sc, 3m, 3s_vs_3z).
Figure 6: Ablation results on a hard map, 5m_vs_6m, for validating the necessity of (a) MAPPO in MADT-Online, (b) input formulation, (c) online version of MADT.
