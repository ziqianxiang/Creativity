Figure 1: Illustration of EASEE on RotationGrid environment. The input is information about thedynamics of the environment known in advance under the form of action sequence equivalences (Λdenotes the empty action sequence). This is used to construct a representation of all the unique statesthat can be visited in 3 steps. The probabilities of sampling each action are then determined to exploreas uniformly as possible. The probabilities of visiting each unique state are displayed on the right.
Figure 2: Example of iterative graph construction with Ω = {a1a1 〜Λ, a2a1 〜a1a2} and amaximum depth of 2. The 8th construction step corresponds to the pruning of the edge (1, 0).
Figure 3: (a, b): Ratio of the number of unique visited states during 100 episodes followingEASEE over standard -greedy policy, for different equivalence sets and depths in the environmentsCardinalGrid and RotationGrid respectively. (c, d): Number of unique visited states according to thenumber of episodes for EASEE with a fixed depth of 4 compared to standard -greedy policy.
Figure 4: Mean reward over training with 95% confidence intervals.
Figure 5: Example of initial state of DoorKey environment.
Figure 6: The Freeway environment from Atari 2600.
Figure 7: Performances of DQN and DQN + EASEE on the Atari 2600 games Boxing, Carnival. A95% confidence interval over 10 random seeds is shown.
