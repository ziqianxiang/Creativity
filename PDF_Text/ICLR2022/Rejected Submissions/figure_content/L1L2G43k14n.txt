Figure 1: Schematic loss landscape for three functions that have zero-error on the training set.
Figure 2: The correlation between flatness and the Bayesian prior for the n = 7 Booleansystem. The functions are defined on the full space of 128 possible inputs. The priors P (f) areshown for the 1000 most frequently found functions by SGD from random initialization for a twohidden layer FCN, and correlate well with log(flatness). The function with the largest prior, which isthe most "flat" is the trivial one of all 0s or all 1s. An additional feature is two offset bands caused bya discontinuity of Boolean functions. Most functions shown are mainly 0s or mainly 1s, and the twobands correspond to different number of outliers (e.g. 1s when the majority is 0s).
Figure 3: The correlation between logP(f), sharpness and generalization accuracy on MNISTand CIFAR-10. For MNIST |S|=500, |E|=1000; for CIFAR-10 |S|=5000, |E|=2000. The attack setsize |A| varies from 0 to |S | and generates functions with different generalization performance. (a)-(c)depicts the correlation between generalization and log P (f) for FCN on MNIST, FCN on CIFAR-10and Resnet-50 on CIFAR-10, respectively. (d)-(f) show the correlation between generalization andflatness for FCN on MNIST, FCN on CIFAR-10, and Resnet50 on CIFAR-10, respectively. In thisexperiment, all DNNs are trained with vanilla SGD.
Figure 4: SGD-variants can break the flatness-generalization correlation, but not the log P (f)-generalization correlation. The figures show generalization v.s. log P(f) or flatness for the FCNtrained on (a) and (d) - MNIST With EntroPy-SGD; (b) and (e) - MNIST With Adam; (C) and ⑴-CIFAR-10 with Adam. for the same S and E as in fig. 3. Note that the correlation with the prior isvirtually identical to vanilla SGD, but that the correlation With flatness measures changes significantly.
Figure 5: How flatness evolves with epochs. At each epoch we calculate the sharpness measurefrom Definition 2.1 (sharpness is the inverse of flatness) and the prior for our FCN on MNIST with|S | = 500. The green dashed line denotes epoch 140 where zero-training error is reached and post-training starts. The red dashed line denotes epoch 639 where parameter-rescaling takes place withα = 5.9. Upon parameter-rescaling, the sharpness increases markedly, but then quickly decreasesagain. The inset shows that the prior is initially unchanged after parameter-rescaling. However, largegradients mean that in subsequent SGD steps, the function (and its prior) changes, before recoveringto (nearly) the same function and log P (f).
Figure S6: The diagram of different definitions for functions represented by DNNs.
Figure S7:	The direct correlation between sharpness and spectral norm of Hessian for the 1000 mostfrequently found functions found after SGD runs for a two hidden layer FCN, in the n = 7 Booleansystem (Same system as in fig. 2) .
Figure S8:	The correlation between prior and flatness in Boolean system where the flatness ismeasured by spectral norm of Hessian, for the 1000 most frequently occurring functions found bySGD runs with a two hidden layer FCN. The system is the same n = 7 Boolean system as in fig. 2except that we use a different metric of flatness.
Figure S9: Two Hessian-based flatness metrics show analogous behavior to sharpness defined in(definition 2.1). The architecture and dataset are FCN/MNIST, with training set size |S| = 500, andtest set size |E| = 1000; which are the same settings as fig. 3 (d) and fig. 4 (e). Optimizer: SGD (a)- (b): The correlation between Hessian-based flatness metrics and generalization. (c) - (d): Sharpnessand Hessian-based flatness metrics correlate well with one another. Optimizer: Adam (e) - (f): Thecorrelation between Hessian-based flatness metrics and generalization breaks down, just as it does forsharpness in fig. 4. (g) - (h): Sharpness and Hessian-based flatness metrics correlate well with oneanother, even though they don’t correlate well with generalization.
