Figure 1:(b) recurrent network dynamicsyt = qtf(wt, xt) - ɪ LijyJ - λyf(c) steady state network outputyt = ^f(wi,χt)qj[(L + λiy1]ijj/(d) Hebbian update rules for Gaussian kernel∖f(u,v) = e"F:Wia « [yifi]xa - [yifi]^ia依oc ytft - %Lij« ytyj - Lij√Neural network implementation of the optimization in Eq. 9 (a) network architecture(b) recurrent netWork dynamics (c) steady state netWork response (d) Hebbian update rules for thespecial case of Gaussian kernels (the precise form of these updates Will be depend on the kernel)At this point We Can simply replace all dot products With the equivalent nonlinear similarities f (∙, ∙)□in Equation 20 and rearrange the terms to yield our key inequality (Eq. 3).
Figure 2: Overview of our Hebbian radial basis function network on the half moons dataset (a)dataset, (b) features {wi}i1=6 1 (c,d,e) response profiles of 3 neurons.
Figure 3: Approximation error vs. dimensionality for the half moons dataset (a) learned features forn = 4, 16, 64 (b) input-output similarities for 16 dimensional networks (c) normalized root-mean-square error between true kernel matrix and various approximation methods. The neural method(dashed blue) that we derive in Section 3 performs well for n <= 16, but the approximation actuallygets worse as we increase the dimensionality.
Figure 4: Utility of kernel similarity matching for downstream tasks. (a) principle components ofthe input vectors x (b) principle components of the 16 dimensional neural representations y (c)clustering generated by kmeans on x (d) clustering generated by kmeans on y. For (a,b) the thecolors are given by ground truth labels while in (c,d) the colors are given by the KMeans clustering.
Figure 5: Approximation error vs. dimensionality for the MNIST dataset. (a) f (u, V) = U ∙ V(linear kernel) (b) f (u, V) = ku∣∣k v∣∣ (U ∙ V)3 (a nonlinear kernel). For the linear kernel all methodsgive relatively small approximation error once n > 100. Although yet again we see that the neuralmethod does not continue to decrease as the dimensionality increases beyond 200, even in the linearsetting.
Figure 6: (a) response distribution (after the procedure we describe in the text for removing the signdegeneracy). The nonlinear kernels (α = 2, 3, 4) naturally give rise to sparse distributions. (b) testset accuracy of a linear classifier classification for MNIST (c) train set accuracy of the correspondinglinear classifier. Interestingly all nonlinear kernels give nearly identical train and test set results. Thelinear kernel gives nearly identical results to simply training the classifier directly on the pixels.
Figure 7:	Training loop to perform the GDA-based optimazation of Eq. 9 written using PyTorchNmin max minWLY1TT xt=1N-X ytf (wi, xt) - 2f (Wi, wi)=1+2 x [Li?- hyiyj i—2 L2J+2 X(Jt)22 i,j=1	2	2 i=1(26)This more clearly shows the relationship between the linear similarity matching objectives and themore general kernel similarity matching objective. When f (wi, X)= w『x, We are in fact left withthe exact objective studied in previous works on linear similarity matching Pehlevan et al. (2018).
Figure 8:	Feedforward weights (W) learned by the network for α = 1, 2, 3, 4. When α = 1, theweights appear to be complicated linear combinations of input vectors. As α increases, the weightsbegin to resemble “templates”, i.e. whole digits. In the main text, we argue this behavior resultsfrom the increasing sharpness of neural tuning as α increases.
Figure 9:	“Linearized responses” for a subset of neurons from networks with α = {1, 2, 3, 4}.
Figure 10: Eigenvalue spectrum of the input similarity matrix f(xt, xt0) and learned output similar-ity matrix yt ∙ yt，. If similarity matching were optimal, (i.e. we just performed uncentered kernelpca on the input similarity matrix) the largest 800 eigenvalues would be exactly matched and sub-sequent eigenvalues would be zero. We see that increasing α brings up the tails of the spectrum,approximately ”whitening” the responses. For α = 1, because the inputs are 400 dimensional, thespectrum only has at most 400 nonzero eigenvalues.
