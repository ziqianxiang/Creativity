Figure 1: Top row: Learning curves observed during the search. The darker the learning curve, thelater it was evaluated during the search. Bottom row: y-axis shows a sequence of learning curveevaluations (bottom to top). The color indicates accuracy. The darker red the higher the accuracy.
Figure 2: The feature extractor φ used in our deep kernel.
Figure 3:	LCBench: Aggregated results over 35 different datasets. The normalized wallclock timerepresents the actual runtime divided by the total wallclock time of DyHPO including the overheadof fitting the deep GP. DyHPO achieves the best performance among all methods for both metrics.
Figure 4:	Critical difference diagram for LCBench for results corresponding to the time DyHPOtook to complete 200, 600 and 1,000 epochs. DyHPO’s improvement is statistically significant.
Figure 5:	TaskSet: Aggregated results over 12 different NLP tasks. Again, DyHPO shows the bestperformance among all methods for both evaluation metrics.
Figure 6:	Critical difference diagram for TaskSet for results after 200, 600 and 1,000 epochs, re-spectively. DyHPO’s improvement is statistically significant.
Figure 8: DyHPO quickly finds well-performingconfigurations. Given enough time, most methodsfind equally good architectures.
Figure 7: Left: Aggregated results for LCBench. Right: Results on ImageNet from NAS-Bench201. Using the learning curve gives only little advantage on average for the LCBench problems.
Figure 9: The learning curve as an explicit input is very important for each task of NAS-Bench 201.
Figure 10: NAS-Bench-201 Regret Results.
Figure 11: Detailed results on a per dataset level for TaskSet.
Figure 12: Detailed results on a per dataset level for LCBench.
Figure 13: Detailed results on a per dataset level for LCBench (cont.)18Under review as a conference paper at ICLR 2022Random	BOHB	DragonflyHyperband	DEHB	DyHPOSeM-SOd WJO-IBqEnN φrosφ><Taskset10	20	30	40	50NumberofEpochsRandom	BOHB	DragonflyHyperband	DEHB	DyHPORandom	BOHB	DragonflyHyperband	DEHB	DyHPOLCBenChO 10	20	30	40	50Number of EpochsRandom	BOHB	DragonflyHyperband	DEHB	DyHPOFigure 14: DyHPO efficiently selects top-performing candidates and keeps training them, avoidingtraining poor configurations for a long time.
Figure 14: DyHPO efficiently selects top-performing candidates and keeps training them, avoidingtraining poor configurations for a long time.
Figure 15: DyHPO spends most its budget on top-performing candidates.
Figure 16: Percentage of configuration i) belonging to the top 1/3 configurations at a given budget,and ii) that were in the bottom 2/3 of configurations at one of the previous budgets. Here the budgetis represented by the number of steps or epochs.
