Figure 1: We synthesized 20 clients with the Fashion-MNIST training data, where every two clientsreceived only the same class images to simulate a concept shift, and separately trained the same CNNmodel for 1 epoch. We present all filter parameters update magnitudes for the first 8 channels (row)in the first convolutional layer for all clients (column) after 1 round of training. The pairs of clientsgrouped together with same colors (one lighter and one darker) signify that they shared the same datadistribution. Each polar plot contains the update magnitudes of all parameters in the same channel.
Figure 2: A high-level overview of FedDrop using FedAvg as the base algorithm. Here, we use 4training clients for illustration. During each round of server averaging, FedDrop carries out dropoutprobability optimization to encourage collaboration between sparse clients for the next training round.
Figure 3: A high-level overview of the SyncDrop layer. (a) For each channel across all clients ina forward pass, We draw U 〜U(0,1) from the uniform distribution. If the keep probability (1 -dropout probability) of a channel in a client is less than u, then the channel is dropped from the model.
Figure 4:	Comparing FL methods with the same local training epochs per round E used for CIFAR-10.
Figure 5:	Comparing the FLOPs vs. communication trade-off across different FL methods reaching atarget accuracy for a given dataset. We highlight the Pareto optimal points with dotted lines.
Figure 6:	Varying α for the dataset splits D|C| (α) of CIFAR-10 with E = 8.
Figure 7:	Ablation of design choices. (a, b) SyncDrop with and without synchronization. (c, d) withand without dropout scaling and fixed initialization under different FLOPs ratio r for CIFAR-10.
Figure 8:	Sensitivity analyses of (a) optimization iterations I, (b) FLOPs constraint regularize] μ,and (c, d) the FLOPs budget ratio r.
Figure 9: We present the evolution of dropout keep probabilities (horizontal axis) for multiple roundsof training (vertical axis) for VGG-9 training with FLOPS budget ratio r = 0.5 and the numberof local epochs per round E is 4. After a few rounds of training, FedDrop can learn to distinguishchannel importance.
Figure 10: The FLOPs vs. communication trade-off curves of different FL methods reaching a targetaccuracy for CIFAR-10. We highlight the Pareto optimal points with dotted lines.
Figure 11: The FLOPs vs. communication trade-off curves for Fashion-MNIST.
Figure 12:	The FLOPs vs. communication trade-off curves for SVHN.
Figure 13:	Varying α for the dataset splits D|C| (α) of CIFAR-10.
Figure 14:	Varying α for the dataset splits D|C| (α) of Fashion-MNIST. Log-scale is used for clarity.
Figure 15:	Varying α for the dataset splits D|C| (α) of SVHN. Log-scale is used for clarity.
Figure 16:	Comparing FL methods with the same local training epochs per round E used for SVHN.
Figure 17:	Comparing FL methods with the same local training epochs per round E used forFashion-MNIST.
Figure 18: Reducing the FLOPs budget ratio r for CIFAR-10.
Figure 19: Reducing the FLOPs budget ratio r for Fashion-MNIST.
Figure 20: Reducing the FLOPs budget ratio r for SVHN.
Figure 21:	Comparing the FLOPs vs. communication trade-off across different FL methods reachinga target accuracy for CIFAR-10. We highlight the Pareto optimal points with dotted lines.
Figure 22:	Comparing the FLOPs vs. communication trade-off across different FL methods reachinga target accuracy for Fashion-MNIST. We highlight the Pareto optimal points with dotted lines.
Figure 23:	Comparing the FLOPs vs. communication trade-off across different FL methods reachinga target accuracy for SVHN. We highlight the Pareto optimal points with dotted lines.
Figure 24:	Reducing device participation to 1%. Some plots use log-scale for clarity.
Figure 25: All parameter update magnitudes for each channel (row) in the first convolutional layer forall clients (column) after 1 round of training. The pairs of clients with same colors (one lighter andone darker) signify that they shared the same image class. The length of each ray is the magnitude ofthe parameter update, and the parameters are ordered by the angles of the rays. The results are takenfrom the motivating example in Figure 1.
Figure 26:	Comparing the FLOPs vs. communication trade-off across different FL methods reachinga 50% accuracy for the Shakespeare dataset. We highlight the Pareto optimal points with dotted lines.
Figure 27:	Comparing FL methods with the same local training epochs per round E used forShakespeare.
Figure 28:	Comparing FL methods on the number of communication rounds vs. accuracy with thesame local training epochs on Shakespeare.
Figure 29:	Comparing the FLOPs vs. communication trade-off across different FL methods reachinga target accuracy for a given dataset. We highlight the Pareto optimal points with dotted lines.
Figure 30:	Comparing FL methods with the same local training epochs per round E used forShakespeare.
Figure 31:	Comparing FL methods on the number of communication rounds vs. accuracy with thesame local training epochs on Shakespeare.
