Figure 2: 1-Goal Reacher Task. Comparison between learning a Successor Feature based SAC algorithm withpre-defined φ and w vs regressing either φ and w. These results are sanity checks that given appropriaterepresentations φ and w, it is possible to incorporate SF within our Actor-Critic continuous action framework.
Figure 3: Architecture component of each representation module.
Figure 4: Continuous control tasks we are considering in this work, starting with simple 2D link reacher taskto more complex metaworld reacher and door close tasks.
Figure 5: Multi-Goal reacher task regression φ and W comparison.
Figure 6: Multi-Goal Reacher Task comparison between SAC and ACSF. This plot shows the training for thetrain and evaluation setup for the 2Dlink reacher task. The training and evaluation on the same target goalsseen at train time show similar performance between a SAC policy and a ACSF policy. However we areinterested in the generalization result in Figure 6b where ACSF outperforms SAC on unseen target goals.
Figure 7: Metaworld reacher task comparison of SAC and ACSF on Reacher and Door close tasksindividually, and trained jointly. These results show our method performs in par with learning without anexplicit SF representation.
Figure 8: Agent learning various forms of reaching skills.
