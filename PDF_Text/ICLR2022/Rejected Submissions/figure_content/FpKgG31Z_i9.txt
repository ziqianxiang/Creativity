Figure 1: The optimizer grafting operation introduced and studied in this work: meta-optimizerM#D uses the update magnitude of optimizer M, and the direction of D.
Figure 2: BERT experiments for implicit hyperparameter transfer, comparing hyperparameter searchfor SGD (with momentum) vs. grafting with M = Adam. Adam#SGD outperformed all pure SGDruns significantly; most hyperparameter settings for SGD caused training to diverge. Trials on batchsizes 8192 and 32768 are shown.
Figure 3: ResNet experiments for implicit hyperparameter transfer, comparing hyperparametersearch for AdaGrad vs. grafting with a well-tuned baseline M = SGD. SGD#AdaGrad outper-formed all pure AdaGrad runs. The batch size for the ImageNet run was 8192 and the CIFAR-10run was 128. Further details can be found in the appendix.
Figure 4: A learning rate schedule for AdaGrad discovered using grafting. Before the first learningrate decay (imposed by the tuned schedule for M=SGD), grafting discovers an implicit polynomialwarmup when transferring the performance to D=AdaGrad.
Figure 5: Per-layer learning rate corrections during the first 2000 iterations of BERT pretraining withAdam#SGD, enabling the discovery of the learning rate correction to SGD. More visualizations areprovided in the supplementary material.
Figure 6: Per-layer learning rate corrections during the first 2000 (resp. 500) iterations of BERTpretraining with Adam#SGD with batch size 8K (resp. 32K), enabling the discovery of the learningrate correction to SGD. A median filter of width 51 is applied to each sequence, for clarity.
Figure 7:	Per-layer learning rate corrections for all training iterations, in the BERT pretraining setupswith Adam#SGD. Some of these corrections increase exponentially; note that despite performingthese large corrections, the grafted optimizer converges to nearly state-of-the-art accuracy.
Figure 8:	Per-layer learning rate schedule corrections from Figure 6, displayed on separate plots bytype of parameter group.
