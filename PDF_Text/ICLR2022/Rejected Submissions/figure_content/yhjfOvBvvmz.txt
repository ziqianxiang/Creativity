Figure 1: Computation graph for the WET-VAE model. For weak supervision, we add aset of weak labels {y1, ..., yM} to the model.
Figure 2:	Illustration of WEDIS algorithm. Two networks are trained separately. After the trainingprocedure of the two networks, we check the embedded factors by the latent traversal and find therelation functions by heuristically testing the WET-VAE decoder and policy.
Figure 3:	Illustration of trajectory scaling. This trajectory scaling provides large scale planning thatis beneficial to solving long-horizon problems.
Figure 4: The latent traversals for each skill. (a)〜(C) show the results of the WET-VAE model,and (d) shows the results the trajectory VAE that does not use any weak supervision, where bothmodels are trained on trajectories with T = 5. Each row represents the trajectories generated bylatent variables changing values of corresponding dimensions of the latent variable from -1.5 to 1.5,while keeping the others to zeros.
Figure 5: The latent traversals of the decoder for each skill. (Top) Predicted trajectories by thepredictive model. (Bottom) Actual trajectories of the agent by the policy network. The colors of(red, green, blue, cyan, yellow) correspond to the values of (-1, -0.5, 0, 0.5, 1) of latents, respectively.
Figure 6: Two navigation tasks. Thegoals are denoted as the red circles.
Figure 7: The evaluation results of the MPC on the navigation problems. The dashed gray linesdenote goal-distances from the origin. WEDIS outperforms the unsupervised skill learning methods.
Figure 8: Examples of the MPC results on sparse rewards (Top) Maze 1. (Bottom) Maze 2. Theblue and red lines represent the predictive paths and the actual paths, respectively, and the crossmarks indicate the goal positions. The dashed gray lines denote error between the last positions ofpredictive and actual trajectories at each planning step.
Figure 9: The labeling criteria and the trajectories in the dataset.
Figure 10: The latent traversals from -1.5 to 1.5. (a)〜(e) show the results from the models trainedwith all the same hyper-parameters except for β and γ. The trajectories in all figures are plotted withT = 5.
Figure 11: The latent traversals from -1.5 to 1.5 (Left) polar coordinate representation (Middle)Cartesian coordinate representation (Right) representation learned without weak supervision.
Figure 12: The latent traversals from -1 to 1. WET-VAE can learn the disentangled representationsof the acceleration and speed in 1D. For visibility, we represent the values of the latents at the y axisand use different colors at each time-step for T = 5. Since the speed is zero for z 1 = 0, the firsttwo figures of the second rows are plotted with z 1 = -1.
Figure 13: The latent traversals from -1.5 to 1.5. WET-VAE can learn the disentangled represen-tations of the curvature, acceleration, direction and speed. For visibility, we plot the accelerationfigure with the points with different colors for each time-step, instead of the arrowed lines.
Figure 14: Mujoco environments. (Left) Ant (Right) Half-CheetahWe present the additional experimental results for the Ant and the Half-Cheetah of Mujoco in Figure14. Figure 15 shows that the Ant learns the disentangled skills of the speed and direction of the fullcircle angles in 2D. In Figure 16, it is shown that the Half-Cheetah learns the disentangled speed-acceleration skills in 1D.
Figure 15: The latent traversals in Ant trained with speed and the direction ranging the full circleangles.
Figure 16: The latent traversals from -1 to 1 in Half-Cheetah. (Top) predicted trajectories by thepredictive model. (Bottom) actual trajectories by the Half-Cheetah. For visibility, we represent thevalues of the latents at the y axis and use different colors at each time-step for T = 6.
Figure 17: Additional MPC results. (Top) Maze1. (Bottom) Maze2.
Figure 18: Additional MPC results. (Top) S-shaped maze. We use T = 6 and Hz = 50 for asequence of two skills. (Bottom) Obstacles. T = 5 and Hz = 50 for single skill.
