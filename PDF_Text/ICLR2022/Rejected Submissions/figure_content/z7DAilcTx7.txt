Figure 1:	Comparison of Langevin and PGD adversarialtraining on MNIST. We plot the error rate against FGSMas a function of the number of backprops. We observe thatLangevin converges much faster as it only needs to compute1 gradient per update of the classifier by being able to re-usethe samples from the previous iterations. After 8 iterations,Langevin achieves a score that is not even reached by PGDafter 1600 iterations.
Figure 2:	Comparison of Langevin and PGD adversarialtraining on CIFAR. We plot the error rate against FGSMas a function of the number of backprops. We observe thatLangevin converges much faster as it only needs to compute1 gradient per update of the classifier by being able to re-usethe samples from the previous iterations.
Figure 3: Influence of σ (noise_scale) on the performance of the classifier when training withAdversarial Transport framework with architecture A. These results are for T = 1 and ηl (λ) = 0.2.
Figure 4: Influence of ηl(λ) (eta_langevin) on the performance of the classifier when training withAdversarial Transport framework with architecture A. These results are for T = 1 and σ = 0.2. Wedid the same experiment with other values of σ but whatever the value of ηl (λ), σ = 0.2 gives thebest result.
