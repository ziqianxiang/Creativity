Figure 1: (a) Value uncertainty exploration algo-rithms like UCB and Thompson Sampling rely onan estimate of the uncertainty of the current valuefunction, a quantity that is intractable already formoderately complex value functions and environ-ments. (b) δ2 -exploration on the other hand onlyneeds access to the reward prediction error δ tocompute how to select an action that trades offexploration and exploitation in a principled way.
Figure 2: Cliff-walking RL task. Both δ2-exploration algorithms achieve higher cumulative rewardat a faster rate than -greedy action selection for both grid sizes, even attaining the optimal reward(indicated by the black dotted line). Moreover, as the bar plots show, δ2 -exploration reaches a lowervariability of received rewards, which indicates that it is more stable. e-greedy exploration withe = 1/，n(s), where n(s) is the number of times state S has been visited. Learning rate is set toαn(s, a) = 0.1(100 + 1)/(100+n(s, a)), where n(s, a) is the number of updates of each state-action.
Figure 3: δ2-exploration DQN policies consistently outperform -greedy exploration and noisy netsand are on par and sometimes better than the strong but computationally costly Bootstrapped DQNin the Atari 2600 domain. DQN training is done for 20M steps for all games, except VideoPinballwhich is trained for 80M steps (see Appendix A). Performance is then computed as the average scoreover the last 1M frames of training. Bars indicate score normalized to human performance (see Mnihet al. (2015)) across 6 random seeds, error bars are SEM. δ2-UCB and δ2-Sampling explorationare compared to -greedy exploration with linearly decreasing over 1M steps starting from 1.0to F = 0.1, as standard (Mnih et al., 2015). Both δ2 -exploration strategies reach consistentlycompetitive policies at the end of training.
Figure 4: Learning curves of -greedy exploration, bootstrapDQN, noisy nets, δ2 -UCB and δ2-Sampling on 4 “easy” and 4 “hard” (i.e., performance of DQN is respectively higher and lowerthan that of human players) additional Atari 2600 games. Scores are averaged in intervals of 500kframes, plots indicate mean score over 6 independent random seeds, and shaded areas indicatemin and max scores over the 6 seeds. δ2 -exploration algorithms consistently have scores amongthe highest throughout learning compared to the -greedy strategy, noisy nets and bootstrapDQN.
