Figure 1:	CSQ quantizer and gradient.
Figure 2:	Zero-point for CLQ and CSQ at 2-bit precision. Affine quantizer allows integer zero-pointonly (shown in green circles). In relaxed affine quantizer, zero-point can take any real value.
Figure 3:	Percentage error reduction using CSQ. The results are shown for ResNet-18 trained onImageNet.
Figure 4:	Distribution of optimal zero-point values of relaxed affine quantizer, across layers. Thegraphs also show the zero-points for CSQ and CLQ, represented as vertical lines.
Figure 5: Distribution of quantized product values when using CSQ vs. CLQs for weight, and CLQufor unsigned activation.
Figure 6: Representational capacity of quantized product with signed activations, using differentquantization methods for 2, 3 and 4-bit precision.
Figure 7: With 2-bit CSQ, quantization levels (shown on the x-axis) can resemble Gaussian distri-bution while, at the same time, real-valued weight data are also uniformly mapped to them (scaleparameter S is omitted for brevity).
Figure 8: How real-valued weight data are mapped to CLQ vs. CSQ after training ResNet-18 onImageNet.
