Figure 1: Illustration of the potential deleterious effects of distillation on data subgroups. We train aResNet-56 teacher on CIFAR-100-LT, a long-tailed version of CIFAR-100 (Cui et al., 2019; Caoet al., 2019) where some labels have only a few associated samples, and a ResNet-50 teacher onImageNet. For each dataset, we self-distill to a student ResNet of the same depth. On CIFAR-100-LT, as is often observed, distillation helps the Overall accuracy over one-hot student training (〜2%absolute). However, such gains come at significant cost on subgroups defined by the individualclasses: on the ten rarest classes, distillation harms performance by 〜1%. Similarly, on ImageNet,distillation harms the average accuracy of the worst-10 classes by 〜3%. Our proposed techniques(§4) can roughly preserve the overall accuracy, while boosting subgroup performance.
Figure 2: Cumulative gain of ResNet-34 self-distillation on ImageNet. For index k, wecompute the gain in average accuracy overthe k worst classes. While average accuracy(k = 1000) improves by +0.4%, for k ≤ 40,distillation harms over the one-hot model (evi-denced by the negative gain).
Figure 3: Logit statistics on CIFAR-100 LT, for the teacher and distilled student under a self-distillation setup (ResNet-56 → ResNet-56). We show the statistics on 10 class buckets: these arecreated by sorting the 100 classes according to the teacher accuracy, and then creating 10 groupsof classes. As expected, the student follows the general trend of the teacher model. Strikingly, weobserve that the teacher model tends to systematically confidently mispredict samples in the higherbuckets, thus incurring a negative margin; such misplaced confidence is largely transferred to thestudent, whose accuracy suffers on such buckets. Note that we consider statistics on the test set.
Figure 4: Logit statistics for ResNet-50 self-distillation on ImageNet, for the (early-stopped) teacher,self-distilled student, and one-hot (non-distilled) student. Per Figure 3, we first create 10 class buckets.
Figure 5: Logit statistics for the teacher, student with one-hot labels, and student with distilled labelsacross: datasets and architectures.
Figure 6: Per-class accuracies for one-hot and self-distilled ResNet-18 (left) and ResNet-34 (right) onImageNet. The diagonal denotes classes where both models achieve the same accuracy. Distillationtends to worsen performance on “hard” classes for the one-hot model, i.e., those with low accuracy(red rectangle).
Figure 7: Logit statistics for ResNet-56 self-distillation on CIFAR-100 LT, for the teacher, self-distilled student, and our adaptive methods. Per Figure 3, we create 10 class buckets. AdaMarginflattens both the margin and log-loss distributions. AdaAlpha increases log loss across classes, whileimproving margins on few buckets, including flipping the bucket 5 to have positive margin.
Figure 8: Study of regularisation samples on training set, CIFAR-100 LT.
Figure 9: Study of regularisation samples on test set, CIFAR-100 LT.
