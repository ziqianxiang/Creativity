title,year,conference
 code2seq: Generating sequences fromstructured representations of code,2018, arXiv preprint arXiv:1808
 Program synthesis with large languagemodels,2021, arXiv preprint arXiv:2108
 Generativecode modeling with graphs,2018, In International Conference on Learning Representations
 Learning from examples to improve codecompletion systems,2009, In Proceedings of the 7th joint meeting of the European software engineeringconference and the ACM SIGSOFT symposium on the foundations of software engineering
 Evaluating large language models trained oncode,2021, arXiv preprint arXiv:2107
 Towards synthesizing complex programs from input-output examples,2018, In International Conference on Learning Representations
 Pymt5:Multi-mode translation of natural language and python code with transformers,2020, In Proceedingsof the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)
 Generating bug-fixes usingpretrained transformers,2021, arXiv preprint arXiv:2104
 Codebert: A pre-trained model for programming and naturallanguages,2020, In Proceedings of the 2020 Conference on Empirical Methods in Natural LanguageProcessing: Findings
 Code-searchnet challenge: Evaluating the state of semantic code search,2019, arXiv preprint arXiv:1909
 Personalized language model for query auto-completion,2018, InProceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume2: Short Papers)
 Prefix-tuning: Optimizing continuous prompts for generation,2021, arXivpreprint arXiv:2101
 Pretrained transformers as universalcomputation engines,2021, CoRR
 Automatic generation of natural language summaries for java classes,2013, In 2013 21stInternational Conference on Program Comprehension (ICPC)
 Automatic generation of release notes,2014, In Proceedings of the 22nd ACMSIGSOFT International Symposium on Foundations of Software Engineering
 Dis-tributed fine-tuning of language models on private data,2018, In International Conference on LearningRepresentations
 Personalizing asr for dysarthricand accented speech with limited data,2019, arXiv preprint arXiv:1907
 Intellicode compose: codegeneration using transformer,2020, In Prem Devanbu
 Unit testcase generation with transformers and focal context,2021, arXiv preprint arXiv:2009
 Attention is all you need,2017, CoRR
 Attention is all you need,2017, In Advances in neural informationprocessing systems
