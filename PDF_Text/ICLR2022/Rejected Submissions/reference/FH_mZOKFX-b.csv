title,year,conference
 Information theory and an extension of the maximum likelihood principle,1998, InSelected papers of hirotugu akaike
 Onexact computation with an infinitely wide neural net,2019, arXiv preprint arXiv:1904
 Spectrally-normalized margin bounds forneural networks,2017, In Proceedings of the 31st International Conference on Neural InformationProcessing Systems
 On empirical comparisons of optimizers for deep learning,2019, arXiv preprintarXiv:1910
 Sharp minima can generalizefor deep nets,2017, In International Conference on Machine Learning
 Hyperparameter Optimization,2019, In Automated Machine Learning
 Neural tangent kernel: convergence and gen-eralization in neural networks,2018, In Proceedings of the 32nd International Conference on NeuralInformation Processing Systems
 Non-stochastic best arm identification and hyperparameteroptimization,2016, In Artificial Intelligence and Statistics
 Fantas-tic generalization measures and where to find them,2019, In International Conference on LearningRepresentations
 On large-batch training for deep learning: Generalization gap and sharp minima,2016, arXivpreprint arXiv:1609
 Limitations of the empirical fisher approxi-mation for natural gradient descent,2019, arXiv preprint arXiv:1905
 Topmoumoute online naturalgradient algorithm,2007, In NIPS
 Deep neural networks as gaussian processes,2018, In International Conference onLearning Representations
 Wide neural networks of any depth evolve as linear modelsunder gradient descent,2019, arXiv preprint arXiv:1902
 Optimizing neural networks with kronecker-factored approximatecurvature,2015, In International conference on machine learning
 Optimizing neural networks with kronecker-factored approximatecurvature,2015, In International conference on machine learning
 The effective number of parameters: An analysis of generalization and regularization innonlinear learning systemsâ€™,1992, in je moody
 In search of the real inductive bias: On therole of implicit regularization in deep learning,2014, arXiv preprint arXiv:1412
 Norm-based capacity control in neuralnetworks,2015, In Conference on Learning Theory
 Bayesian deep convolutional networks withmany channels are gaussian processes,2018, In International Conference on Learning Representations
 Fast exact multiplication by the hessian,1994, Neural computation
 Very deep convolutional networks for large-scale imagerecognition,2015, In International Conference on Learning representations
 Woodfisher: Efficient second-order approximations for modelcompression,2020, arXiv preprint arXiv:2004
 On the Uniform convergence of relative freqUencies ofevents to their probabilities,2015, In Measures of complexity
 Data-dependent sample complexity of deep neUral networks via lipschitzaUgmentation,2019, arXiv preprint arXiv:1905
 Understandingdeep learning reqUires rethinking generalization,2016, arXiv preprint arXiv:1611
 Noisy natUral gradient asvariational inference,2018, In International Conference on Machine Learning
