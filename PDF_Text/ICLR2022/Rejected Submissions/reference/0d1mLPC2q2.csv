title,year,conference
 Variationalinformation distillation for knowledge transfer,2019, In CVPR
 Learning efficientobject detection models with knowledge distillation,2017, In NeurIPS
 An empirical analysisof the impact of data augmentation on knowledge distillation,2020, arXiv preprint arXiv:2006
 Imagenet: A large-scalehierarchical image database,2009, In CVPR
 Improved regularization of convolutional neural networkswith cutout,2016, arXiv preprint arXiv:1708
 Knowledge transfer via distillationof activation boundaries formed by hidden neurons,2019, In AAAI
 Distilling the knowledge in a neural network,2014, InNeurIPS Workshop
 Training products of experts by minimizing contrastive divergence,2002, NeuralComputation
 Densely connectedconvolutional networks,2017, In CVPR
 Batch normalization: Accelerating deep network training byreducing internal covariate shift,2015, In ICML
 Deep learning,2015, Nature
 Large-margin softmax loss for convolu-tional neural networks,2016, In ICML
 Shufflenet v2: Practical guidelines forefficient cnn architecture design,2018, In ECCV
 Relational knowledge distillation,2019, In CVPR
 Learning deep representations with probabilistic knowledgetransfer,2018, In ECCV
 Correlation congruence for knowledge distillation,2019, In ICCV
 Fitnets: Hints for thin deep nets,2015, In ICLR
 Mo-bilenetv2: Inverted residuals and linear bottlenecks,2018, In CVPR
 Deep learning in neural networks: An overview,2015, Neural networks
 From theories to queries: Active learning in practice,2011, In AISTATS Workshop on ActiveLearning and Experimental Design
 Very deep convolutional networks for large-scale imagerecognition,2015, In ICLR
 Rethinkingthe inception architecture for computer vision,2016, In CVPR
 Contrastive representation distillation,2020, In ICLR
 Similarity-preserving knowledge distillation,2019, In CVPR
 Collaborative distillationfor ultra-resolution universal style transfer,2020, In CVPR
 Knowledge distillation and student-teacher learning for visual intelli-gence: A review and new outlooks,2021, TPAMI
 Unsupervised feature learning via non-parametric instance discrimination,2018, In CVPR
 Revisiting knowledge distillationvia label smoothing regularization,2020, In CVPR
 Paying more attention to attention: Improving the perfor-mance of convolutional neural networks via attention transfer,2017, In ICLR
 mixup: Beyond empiricalrisk minimization,2018, In ICLR
