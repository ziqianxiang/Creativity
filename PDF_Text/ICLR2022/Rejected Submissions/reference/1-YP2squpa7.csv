title,year,conference
 Sparsely-connected neural networks: Towards ef-ficient VLSI implementation of deep neural networks,2017, In 5th International Conference on LearningRepresentations
 Efficient supervisedlearning in networks with binary synapses,0027, Proceedings of the National Academy of Sciences
 Unreasonable effectiveness of learning neural networks: Fromaccessible states and robust ensembles to basic algorithmic schemes,0027, Proceedings of the NationalAcademy of Sciences
 Learningmay need only a few bits of synaptic precision,2016, Phys
 Shaping the learning landscape in neuralnetworks around wide flat minima,0027, Proceedings of the National Academy of Sciences
 Statistical theory of superlattices,1935, Proc
 Learning by message passing in networks of discretesynapses,2006, Phys
 Entropy-sgd: Biasing gradient descentinto wide valleys,2017, In 5th International Conference on Learning Representations
 The inverse variance-flatness relation in stochastic gradient descent is criticalfor finding flat minima,2021, Proceedings of the National Academy of Sciences
 Inference in deep networks in highdimensions,1884, In 2018 IEEE International Symposium on Information Theory (ISIT)
 Low-density parity-check codes,1962, IRE Transactions on information theory
 Understanding the difficulty of training deep feedforwardneural networks,2010, In Yee Whye Teh and Mike Titterington (eds
 An empirical investi-gation of catastrophic forgetting in gradient-based neural networks,2013, arXiv preprint arXiv:1312
 Bi-narized neural networks,2016, In D
 Fantas-tic generalization measures and where to find them,2020, In International Conference on LearningRepresentations
 Phasetransitions and sample complexity in bayes-optimal matrix factorization,2016, IEEE Transactions onInformation Theory
 Overcomingcatastrophic forgetting in neural networks,2017, Proceedings of the national academy of sciences
 Belief propagation neural networks,2020, In H
 Synaptic metaplas-ticity in binarized neural networks,2021, Nature Communications
 Visualizing the loss land-scape of neural nets,2018, In S
 Multi-layer generalized linearestimation,2017, In 2017 IEEE International Symposium on Information Theory (ISIT)
 Expectation propagation for approximate bayesian inference,1558, In Proceedings ofthe Seventeenth Conference on Uncertainty in Artificial Intelligence
 Bilinear generalized approximate messagepassing—part i: Derivation,2014, IEEE Transactions on Signal Processing
 Reverend Bayes on inference engines: A distributed hierarchical approach,1982, CognitiveSystems Laboratory
 On ising’s model of ferromagnetism,1936, Mathematical Proceedings of the CambridgePhilosophical Society
 Vector approximate message passing,2019, IEEETransactions on Information Theory
 Neural models ofBayesian belief propagation,0262,
 Neural enhanced belief propagation on factor graphs,2021, InInternational Conference on Artificial Intelligence and Statistics
 Criticalinitialisation in continuous approximations of binary neural networks,2020, 2020
 Multi-layer bilinear generalized approximatemessage passing,2021, IEEE Transactions on Signal Processing
 See Sec,1024, B
