title,year,conference
 Second-order stochastic optimization for machinelearning in linear time,2017, 2017
 A convergence theory for deep learning via over-parameterization,2019, In ICML
 On the convergence rate of training recurrent neuralnetworks,2019, In NeurIPS
 Subspace embeddings for the polynomialkernel,2014, In NeurIPS
 Exact natural gradient in deep linearnetworks and its application to the nonlinear case,2018, In S
 Numerical MethodSfor Least Squares Problems,1996, Society for Industrial and AppliedMathematics
 Generalization bounds of stochastic gradient descent for wide anddeep neural networks,2019, In NeurIPS
 A measure of asymptotic efficiency for tests of a hypothesis based on the sum ofobservations,1952, The Annals of Mathematical Statistics
 Low rank approximation and regression in inputsparsity time,2013, In SymPosium on Theory of ComPuting Conference (STOC)
 Solving linear programs in the current matrixmultiplication time,2019, In STOC
 Faster approximate lossy generalized flow via interiorpoint algorithms,2008, In Proceedings of the fortieth annual ACM symposium on Theory of computing(STOC)
 Bert: Pre-training of deepbidirectional transformers for language understanding,2018, arXiv preprint arXiv:1810
 Sketching for kronecker product regres-sion and p-splines,2017, In AISTATS
 Optimal sketching forkronecker product regression and low rank approximation,2019, In Advances in Neural InformationProcessing Systems (NeurIPS)
 Gradient descent finds globalminima of deep neural networks,2019, In International Conference on Machine Learning (ICML)
 Gradient descent provably optimizesover-parameterized neural networks,2019, In ICLR
 Deep residual learning for imagerecognition,2016, In Proceedings of the IEEE conference on computer vision and pattern recognition(CVPR)
 Neural tangent kernel: convergence and gen-eralization in neural networks,2018, In Proceedings of the 32nd International Conference on NeuralInformation Processing Systems (NeurIPS)
 Polylogarithmic width suffices for gradient descent to achieve arbi-trarily small test error with shallow relu networks,2020, In ICLR
 A faster interiorpoint method for semidefinite programming,2020, In FOCS
 Faster dynamic matrix inverse forfaster lps,2021, In STOC
 Suprema of chaos processes and the re-stricted isometry property,2014, Communications on Pure and Applied Mathematics
 Imagenet classification with deep con-volutional neural networks,2012, Advances in neural information processing systems
 Gradient-based learning applied todocument recognition,1998, Proceedings of the IEEE
 Wide Neural Networks of Any Depth Evolve as Linear Modelsunder Gradient Descent,2019, 2019a
 A faster cutting plane method and its impli-cations for combinatorial and convex optimization,2015, In Foundations of Computer Science (FOCS)
 Solving empirical risk minimization in the current matrixmultiplication time,2019, In Conference on Learning Theory (COLT)
 Learning overparameterized neural networks via stochastic gradientdescent on structured data,2018, In NeurIPS
 On the linearity of large non-linear models: when andwhy the tangent kernel is constant,1595, In H
 Faster ridge regression via the sub-sampled randomized hadamard transform,2013, In Advances in neural information processing systems(NIPS)
 Newton sketch: A near linear-time optimization algorithmwith linear-quadratic convergence,2017, SIAM J
 Restricted isometries for partial random circulantmatrices,2012, Applied and Computational Harmonic Analysis
 Non-asymptotic theory of random matrices: extreme singu-lar values,1576, In Proceedings of the International Congress of Mathematicians 2010 (ICM 2010) (In4 Volumes) Vol
 Masteringthe game of go with deep neural networks and tree search,2016, nature
 Mastering the game of gowithout human knowledge,2017, nature
 Fast sketching of polynomial kernelsof polynomial degree,2021, In ICML
 Going deeper with convolutions,2015, InProceedings ofthe IEEE conference on computer vision and pattern recognition
 Speeding-up linear programming using fast matrix multiplication,1989, In 30th AnnualSymposium on Foundations of Computer Science
 Sketching as a tool for numerical linear algebra,2014, Foundations and Trends inTheoretical Computer Science
 Near input sparsity time kernel embeddings via adaptivesampling,2020, In ICML
 Fast convergence of natural gradient descentfor over-parameterized neural networks,2019, In Advances in Neural Information Processing Systems(NeurIPS)
 An improved analysis of training over-parameterized deep neuralnetworks,2019, In NeurIPS
 The correctness follows directly from Lemma D,2022,8
