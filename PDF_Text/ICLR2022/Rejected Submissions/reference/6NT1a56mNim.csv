title,year,conference
 Weakly supervised learning of semantic parsers for mappinginstructions to actions,2013, Transactions of the Association for Computational Linguistics
 Beyond the imitation game: Measuring and extrapolating the capabil-ities of language models,2021, In preparation
 Language models arefew-shot learners,2020, arXiv preprint arXiv:2005
 Evaluating large language modelstrained on code,2021, arXiv preprint arXiv:2107
 Bert: Pre-training of deepbidirectional transformers for language understanding,2018, arXiv preprint arXiv:1810
 Human instruction-following with deepreinforcement learning via transfer-learning from text,2020, arXiv preprint arXiv:2005
 Long short-term memory,1997, Neural computation
 The curious case of neural textdegeneration,2019, arXiv preprint arXiv:1904
 Ai2-thor: An interactive 3d environment forvisual ai,2017, arXiv preprint arXiv:1712
 Implicit representations of meaning in neurallanguage models,2021, arXiv preprint arXiv:2106
 Roberta: A robustly optimized bert pretrainingapproach,2019, arXiv preprint arXiv:1907
 Pretrained transformers as universalcomputation engines,2021, arXiv preprint arXiv:2103
 Grounding language in play,2020, arXiv preprint arXiv:2005
 Environment-driven lexicon induc-tion for high-level instructions,2015, In Proceedings of the 53rd Annual Meeting of the Associationfor Computational Linguistics and the 7th International Joint Conference on Natural LanguageProcessing (Volume 1: Long Papers)
 Virtualhome: Simulating household activities via programs,2018, In Proceedings of the IEEEConference on Computer Vision and Pattern Recognition
 Watch-and-help: A challenge for social perception and human-ai collaboration,2020, arXivpreprint arXiv:2010
 Languagemodels are unsupervised multitask learners,2019, OpenAI blog
 Exploring the limits of transfer learning with a unified text-to-texttransformer,2019, arXiv preprint arXiv:1910
 Sentence-bert: Sentence embeddings using siamese bert-networks,2019, arXiv preprint arXiv:1908
 Alfworld: Aligning text and embodied environments for interactive learning,2020, arXivpreprint arXiv:2010
 olmpics-on what language modelpre-training captures,2020, Transactions ofthe Association for Computational Linguistics
 Multi-modal few-shot learning with frozen language models,2021, arXiv preprint arXiv:2106
 Attention is all you need,2017, In Advances in neural informationprocessing systems
 HUggingface's transformers:State-of-the-art natural language processing,2019, arXiv preprint arXiv:1910
5	Details of Human EvaluationsHuman evaluations are conducted on Amazon Mechanical Turk,2018, For each method
