title,year,conference
 Fast convergence of inertialdynamics and algorithms with asymPtotic vanishing viscosity,2018, Mathematical Programming
 First-order oPtimization algorithms viainertial systems with hessian driven damPing,2020, Mathematical Programming
 The zig-zag Process and suPer-efficient sam-Pling for bayesian analysis of big data,2019, Annals of Statistics
 On explicit l2-convergence rate estimate for underdampedlangevin dynamics,2019, arXiv preprint arXiv:1908
 On the convergence of stochastic gradient MCMCalgorithms with high-order integrators,2015, NIPS
 A unified particle-optimization framework for scalable bayesian sampling,2018, In The Conference on Uncertainty inArtificial Intelligence
 Convergence of langevin mcmc in kl-divergence,2018, PMLR 83
 Underdamped langevinmcmc: A non-asymptotic analysis,2018, Proceedings of the 31st Conference On Learning Theory
 On the global convergence of gradient descent for over-parameterized models using optimal transport,2018, In Advances in neural information processingsystems
 On sampling from a log-concave density using kineticLangevin diffusions,2020, Bernoulli
 Non-convex learning via replicaexchange stochastic gradient mcmc,2020,In International Conference on Machine Learning
 Hypocoercivity for kinetic equationswith linear relaxation terms,2009, Comptes Rendus Mathematique
 Hypocoercivity for linear kinetic equa-tions conserving mass,2015, Transactions of the American Mathematical Society
 Sampling from strongly log-concave distributions with the unad-justed langevin algorithm,2016, arXiv preprint arXiv:1605
 High-dimensional bayesian inference via the unadjustedlangevin algorithm,2019, Bernoulli
 Couplings and quantitative contractionrates for langevin dynamics,2019, The Annals of Probability
 Spectral properties of hypoelliptic operators,2003, Communications inmathematical physics
 Approximate inference with wasserstein gradient flows,2020, InInternational Conference on Artificial Intelligence and Statistics
 Accelerating diffusions,2005, Annals ofApplied Probability
 Global rates of convergence in log-concave densityestimation,2016, The Annals of Statistics
 Effective diffusion in the fokker-planck equation,1989, Mathematical notes of the Academyof Sciences of the USSR
 Ensemble preconditioning formarkov chain monte carlo simulation,2018, Statistics and Computing
 Stochastic Runge-Kutta acceleratesLangevin Monte Carlo and beyond,2019, NeurIPS
 Stein variational gradient descent: A general purpose bayesian inferencealgorithm,2016, In Advances in neural information processing Systems
 Sampling can be fasterthan optimization,2019, Proceedings of the National Academy of Sciences
 Ergodicity for sdes and ap-proximations: locally lipschitz vector fields and degenerate noise,2002, Stochastic processes and theirapplications
 Splitting methods,2002, Acta Numerica
 Computable bounds for geometric convergence rates ofmarkov chains,1994, The Annals ofApplied Probability
 Irreversible langevin samplers and variance reduc-tion: a large deviations approach,2015, Nonlinearity
 Exponential convergence of langevin distributions andtheir discrete approximations,1996, Bernoulli
 Spectral methods for langevin dynamics and associated errorestimates,2018, ESAIM: Mathematical Modelling and Numerical Analysis
 The randomized midpoint method for log-concave sampling,2019, InAdvances in Neural Information Processing Systems
 Understanding the acceleration phe-nomenon via high-resolution differential equations,2021, Mathematical Programming
 A differential equation for modeling nesterov’saccelerated gradient method: Theory and insights,2014, In Advances in Neural Information ProcessingSystems
 Rapid convergence of the unadjusted langevin algorithm:Isoperimetry suffices,2019, In Advances in Neural Information Processing Systems
 Accelerated information gradient flow,2019, arXiv preprintarXiv:1909
 Sampling as optimization in the space of measures: The langevin dynamics as acomposite optimization problem,2018, In Conference On Learning Theory
 A variational perspective on acceleratedmethods in optimization,2016, Proceedings of the National Academy of Sciences
 The case for bayesian deep learning,2020, arXiv preprint arXiv:2001
 Policy optimization as wasser-stein gradient flows,2018, In International Conference on Machine Learning
 This result holds Uniformlyfor all k ≥ 0 and k cango to ∞,2022, In particular
 Recall from Lemma D,2022,8
