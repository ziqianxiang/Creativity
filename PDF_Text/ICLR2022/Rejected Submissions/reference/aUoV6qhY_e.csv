title,year,conference
 Structured pruning of deep convolutionalneural networks,2017, ACM J
 The lottery ticket hypothesis for pre-trained BERT networks,2020, InHugo Larochelle
 BERT: pre-training of deepbidirectional transformers for language understanding,2019, In Jill Burstein
 Depth-adaptive transformer,2020, In8th International Conference on Learning Representations
 Reducing transformer depth on demand with struc-tured dropout,2020, In 8th International Conference on Learning Representations
 Dynabert: DynamicBERT with adaptive width and depth,2020, In Hugo Larochelle
 schubert: Optimizing elements of BERT,2020, In Dan Jurafsky
 ALBERT: A lite BERT for self-supervised learning of language representations,2020, In8th International Conference on Learning Representations
 Super tickets in pre-trained language models: From model compres-sion to improving generalization,2021, In Chengqing Zong
 Are sixteen heads really better than one? InHanna M,2019, Wallach
 Languagemodels are unsupervised multitask learners,2019, 2019
 Exploring the limits of transfer learning with a unified text-to-texttransformer,2019, CoRR
 Minilm:Deep self-attention distillation for task-agnostic compression of pre-trained transformers,2020, InHugo Larochelle
 HUggingface'stransformers: State-of-the-art natural language processing,2019, CoRR
 Lite transformer with long-shortrange attention,2020, In 8th International Conference on Learning Representations
 Deebert: Dynamic early exiting foraccelerating BERT inference,2020, In Dan Jurafsky
 Bert-of-theseus: CompressingBERT by progressive module replacing,2020, CoRR
 Know what you donâ€™tneed: Single-shot meta-pruning for attention heads,2020, CoRR
 BERT loses patience: Fast and robust inference with early exit,2020, In HugoLarochelle
 Discrimination-aware channel pruning for deep neural networks,2018, InSamy Bengio
