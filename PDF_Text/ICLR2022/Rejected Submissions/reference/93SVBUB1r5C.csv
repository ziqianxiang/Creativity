title,year,conference
 On the convergence rate of training recurrent neuralnetworks,2019, In Advances in Neural Information Processing Systems
 On exactcomputation with an infinitely wide neural net,2019, In Advances in Neural Information ProcessingSystems
 Learning Theory from First Principles,2021, 2021
 On the sample complexity of learning with geometricstability,2021, arXiv preprint arXiv:2106
 Theory of classification: A survey ofsome recent advances,2005, ESAIM: probability and statistics
 Convolutional rectifier networks as generalized tensor decom-positions,2016, In International Conference on Machine Learning
 Inductive bias of deep convolutional networks through poolinggeometry,2016, arXiv preprint arXiv:1605
 Generalization error ratesin kernel regression: The crossover from the noiseless to noisy regime,2021, arXiv preprintarXiv:2105
 Gradient descent finds globalminima of deep neural networks,2019, In Proceedings of the 36th International Conference on MachineLearning
 Gradient descent provably optimizesover-parameterized neural networks,2019, In International Conference on Learning Representations
 The spectrum of kernel random matrices,2010, The Annals of Statistics
 Locality defeats the curse of dimen-sionality in convolutional teacher-student scenarios,2021, arXiv preprint arXiv:2106
 Linearized two-layersneural networks in high dimension,2021, The Annals of Statistics
 Testing for homogeneity with kernel fisherdiscriminant analysis,2007, In NIPS
 Neural tangent kernel: Convergence and gen-eralization in neural networks,2018, In Advances in Neural Information Processing Systems
 Kernelalignment risk estimator: Risk prediction from training data,2020, arXiv preprint arXiv:2006
 Imagenet classification with deep convo-lutional neural networks,2012, In Advances in Neural Information Processing Systems
 Deep learning,2015, Nature
 Enhanced convolutional neural tangent kernels,2019, arXiv preprint arXiv:1911
 Just interpolate: Kernel “ridgeless” regression can gen-eralize,2020, Annals of Statistics
 End-to-end kernel learning with supervised convolutional kernel networks,2016, arXivpreprint arXiv:1605
 Generalization error of random fea-tures and kernel methods: hypercontractivity and kernel matrix concentration,2021, arXiv preprintarXiv:2101
 Learning with invariances in randomfeatures and kernel models,2021, arXiv preprint arXiv:2102
 Deep vs,2016, shallow networks: An approximation theoryperspective
 Analysis of boolean functions,2014, Cambridge University Press
 Understanding machine learning: From theory to algo-rithms,2014, Cambridge University Press
 Neural kernels without tangents,2020, In International Conference onMachine Learning
 The unreasonable effective-ness of patches in deep convolutional kernels methods,2021, arXiv PrePrint arXiv:2101
 Stochastic gradient descent optimizesover-parameterized deep relu networks,2018, arXiv:1811
 Consider the inner-product kernel function h : R → R defined on the hy-percube Qq,2022, By rotational symmetry (see Section 2
 From Assumption 3,2022,(a) (forexample by adapting the proof of Lemma C
