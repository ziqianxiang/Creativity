title,year,conference
 High-dimensional dynamics of general-ization error in neural networks,2020, Neural Networks
 Why bigger is not always better: on finite and infinite neural networks,2020, InInternational Conference on Machine Learning
 Deep kernel processes,2021, In InternationalConference on Machine Learning
 Products of rectangular random matrices:singular values and progressive scattering,2013, Physical Review E
 On the optimization of deep networks: Implicitacceleration by overparameterization,2018, In ICML
 Implicit regularization in deep matrixfactorization,2019, Advances in Neural Information Processing Systems
 Fine-grained analysis ofoptimization and generalization for overparameterized two-layer neural networks,2019, In InternationalConference on Machine Learning
 Implicit regularization via neural feature alignment,2021, In InternationalConference on Artificial Intelligence and Statistics
 Benign overfitting in linearregression,2020, Proceedings of the National Academy of Sciences
 Reconciling modern machine learningpractice and the bias-variance trade-off,2018, arXiv preprint arXiv:1812
 Reconciling modern machine-learningpractice and the classical bias-variance trade-off,2019, Proceedings of the National Academy of Sciences
 Blind super-resolution kernel estimation using aninternal-gan,2019, In Advances in Neural Information Processing Systems
 Animage is worth 16x16 words: Transformers for image recognition at scale,2020, arXiv preprintarXiv:2010
 Gradient descent finds globalminima of deep neural networks,1675, In International Conference on Machine Learning
 Gradient descent provably optimizesover-parameterized neural networks,2018, arXiv preprint arXiv:1810
 Qualitatively characterizing neural networkoptimization problems,2015, In International Conference on Learning Representations
 Expandnets: Linear over-parameterization totrain compact convolutional networks,2020, In Advances in Neural Information Processing Systems
 Deep residual learning for imagerecognition,2016, In Proceedings of the IEEE conference on computer vision and pattern recognition
 Neural reparameterization improvesstructural optimization,2019, arXiv preprint arXiv:1909
 Implicit rank-minimizing autoencoder,2020, In Advances in Neural InformationProcessing Systems
 Adam: A method for stochastic optimization,2015, In YoshuaBengio and Yann LeCun (eds
 Skip-thought vectors,2015, In Advances in Neural Information Processing Systems
 Learning multiple layers of features from tiny images,2009, 2009
 Imagenet classification with deep con-VolUtional neural networks,2012, Advances in neural information processing systems
 Algorithmic regularization in oVer-parameterizedmatrix sensing and neural networks with quadratic actiVations,2018, In Conference On Learning Theory
 Towards resolVing the implicit bias of gradient descentfor matrix factorization: Greedy low-rank learning,2020, arXiv preprint arXiv:2012
 ConVergence of gradient descent on separable data,2019, In The 22ndInternational Conference on Artificial Intelligence and Statistics
 Deepdouble descent: Where bigger models and more data hurt,2019, arXiv preprint arXiv:1912
 Sgd on neural networks learns functions of increasing complexity,2019, arXiv preprintarXiv:1905
 Plancherel-rotach formulae for aVerage characteristic polynomials of productsof ginibre random matrices and the fuss-catalan distribution,2014, Random Matrices: Theory andApplications
 In search of the real inductiVe bias: On therole of implicit regularization in deep learning,2015, In International conference on machine learning
 Resurrecting the sigmoid in deeplearning through dynamical isometry: theory and practice,2017, In Advances in neural informationprocessing systems
 The emergence of spectral universality indeep networks,1924, In International Conference on Artificial Intelligence and Statistics
 Gradient starvation: A learning proclivity in neural networks,2020, arXiv preprintarXiv:2011
 Implicit regularization in deep learning may not be explainable bynorms,2020, In Advances in neural information processing systems
 Clustering methods,2005, In Data mining and knowledge discoveryhandbook
 The effective rank: A measure of effective dimensionality,2007, In 200715th European Signal Processing Conference
 Smoothing and differentiation of data by simplified leastsquares procedures,1964, Analytical chemistry
 Exact solutions to the nonlinear dy-namics of learning in deep linear neural network,2014, In In International Conference on LearningRepresentations
 Thepitfalls of simplicity bias in neural networks,2020, arXiv preprint arXiv:2006
 Im-plicit neural representations with periodic activation functions,2020, Advances in Neural InformationProcessing Systems
 A formal theory of inductive inference,1964, part i
 Going deeper with convolutions,2015, InProceedings of the IEEE conference on computer vision and pattern recognition
 Low-ranksolutions of linear matrix equations via procrustes flow,2016, In International Conference on MachineLearning
 Deep learning generalizes becausethe parameter-function map is biased towards simple functions,2019, In International Conference onLearning Representations
 Global convergence of adaptive gradient methods for anover-parameterized neural network,2019, arXiv preprint arXiv:1902
 Aggregated residualtransformations for deep neural networks,2017, In Proceedings of the IEEE conference on computervision and pattern recognition
 A fine-grained spectral perspective on neural netWorks,2019, arXiv preprintarXiv:1907
 Understand-ing deep learning requires rethinking generalization,2017, In International Conference on LearningRepresentations
 Fast convergence of natural gradient descentfor over-parameterized neural netWorks,2019, In Advances in Neural Information Processing Systems
 The unreasonableeffectiveness of deep features as a perceptual metric,2018, In Proceedings of the IEEE conference oncomputer vision and pattern recognition
 Neural architecture search With reinforcement learning,2017, In InternationalConference on Learning Representations
