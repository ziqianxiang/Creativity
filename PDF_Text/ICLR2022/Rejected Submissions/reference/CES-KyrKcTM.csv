title,year,conference
 Unsupervised labelnoise modeling and loss correction,2019, In ICML
 Webly supervised learning of convolutional networks,2015, In ICCV
 Can cross entropy loss be robustto label noise,2020, In IJCAI
 Robust loss functions under label noise for deepneural networks,2017, In AAAI
 Training deep neural-networks using a noise adaptation layer,2017, InICLR
 Deep residual learning for imagerecognition,2016, In Proceedings of the IEEE conference on computer vision and pattern recognition
 Conditional variance penalties and domain shiftrobustness,2021, Machine Learning
 Using trusted data to traindeep networks on labels corrupted by severe noise,2018, In NeurIPS
 Using pre-training can improve model robustnessand uncertainty,2019, In ICML
 Improvingneural networks by preventing co-adaptation of feature detectors,2012, NeurIPS
 Mentornet: Learning data-driven curriculum for very deep neural networks on corrupted labels,2018, In ICML
 Generalization bounds in thepresence of outliers: a median-of-means study,2021, In International Conference on Machine Learning
 Recovering best statistical guarantees via the emPirical divergence-based distributionallyrobust oPtimization,2019, Operations Research
 Robust classification via mom mini-mization,2020, Machine Learning
 Robust sparse estimation tasks in high dimensions,2017, arXiv preprint arXiv:1702
 Learning fromnoisy labels with distillation,2017, In ICCV
 Early-learningregularization prevents memorization of noisy labels,2020, In NeurIPS
 Self-error-correcting convolutionalneural network for learning with noisy labels,2017, In FG
 Robust multivariate mean estimation: the optimality oftrimmed mean,2021, TheAnnals of Statistics
 Nor-malized loss functions for deep learning with noisy labels,2020, In ICML
 Empirical bernstein bounds and sample variance penaliza-tion,2009, Proc
 Coresets for robust training of deep neuralnetworks against noisy labels,2020, In NeurIPS
 Learning to label aerial images from noisy data,2012, In ICML
 Variance-based regularization with convex objectives,2017, InAdvances in Neural Information Processing Systems
 Learning with noisylabels,2013, In NIPS
 Makingdeep neural networks robust to label noise: A loss correction approach,2017, In CVPR
 Learning with symmetriclabel noise: the importance of being unhinged,2015, In NeurIPS
 Trainingconvolutional networks with noisy labels,2015, In ICLR
 Convergence analysis for distributionally robust optimization and equilib-rium problems,2016, Mathematics of Operations Research
 Rethinkingthe inception architecture for computer vision,2016, In CVPR
 Pac-bayes-empirical-bernstein inequality,2013, Advances in NeuralInformation Processing Systems 26 (NIPS 2013)
 Learningfrom noisy large-scale datasets with minimal supervision,2017, In CVPR
 Improved mean absolute error for learningmeaningful patterns from abnormal training data,2019, Technical report
 Symmetric crossentropy for robust learning with noisy labels,2019, In ICCV
 Training noise-robust deep neural networks viameta-learning,2020, In CVPR
 A topologicalfilter for learning with label noise,2020, In NeurIPS
 Part-dependent label noise: Towards instance-dependentlabel noise,2020, In NeurIPS
 L_dmi: A novel information-theoretic lossfunction for training deep nets robust to label noise,2019, In NeurIPS
 Safeguarded dynamic label regressionfor noisy supervision,2019, In AAAI
 Iterative cross learningon noisy labels,2018, In WACV
 Learning withfeature-dependent label noise: A progressive approach,2021, In ICLR
 Generalized cross entropy loss for training deep neural networkswith noisy labels,2018, In NeurIPS
 Distilling effectivesupervision from severe label noise,2020, In CVPR
 Error-bounded correction of noisy labels,2020, In ICML
