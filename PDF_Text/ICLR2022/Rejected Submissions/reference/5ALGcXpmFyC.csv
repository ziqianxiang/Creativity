title,year,conference
 The neural tangent kernel in high dimensions: Triple descentand a multi-scale theory of generalization,2020, In ICML
 High-dimensional dynamics of generalization error in neuralnetworks,2020, Neural Networks
 Convergence rates of spectral distributions of large sample covariancematrices,2003, SIAM J
 Reconciling modern machine-learning practice and the classical bias-variance trade-off,2019, Proceedings of the National Academyof Sciences
 Data Sci,2020,
 Exact expressions for doubledescent and implicit regularization via surrogate random design,2020, ArXiv
 Bert: Pre-training of deepbidirectional transformers for language understanding,2019, In NAACL
 High-dimensional asymptotics of prediction: Ridge regression andclassification,2015, arXiv: Statistics Theory
 Double descent in adversarial training: An implicitlabel noise perspective,2021, ArXiv
 Scaling description of generalization with number of parametersin deep learning,2019, ArXiv
 Linearized two-layers neuralnetworks in high dimension,2019, ArXiv
 Rate of convergence to the semi-circular law,2003, Probability Theory andRelated Fields
 On the rate of convergence to the marchenko-pastur distribution,2011, arXiv:Probability
 Surprises in high-dimensional ridgeless leastsquares interpolation,2019, ArXiv
 Improvingneural networks by preventing co-adaptation of feature detectors,2012, ArXiv
 Implicitregularization of random feature models,2020, In Hal DaUme In and Aarti Singh (eds
 Imagenet classification with deep convolu-tional neural networks,2012, Communications of the ACM
 An analytic theory of generalization dynamics and transfer learning indeep linear networks,2019, ArXiv
 The generalization error of random features regression: Preciseasymptotics and double descent curve,2019, arXiv: Statistics Theory
 Optimal regularization canmitigate double descent,2020, In International Conference on Learning Representations
 Regularization of neuralnetworks using dropconnect,2013, In Sanjoy Dasgupta and David McAllester (eds
