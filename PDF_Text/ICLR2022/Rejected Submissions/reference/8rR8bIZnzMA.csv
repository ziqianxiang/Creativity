title,year,conference
 Local graph partitioning using pagerank vectors,2006, InFoundations of Computer Science
 Language models are few-shot learners,2020, In Advances in NeuralInformation Processing Systems
 Exploring simple siamese representation learning,2021, In Conference onAdvances in Neural Information Processing Systems
 Elements of Information Theory,2006, 2006
 Traffic graph convolutionalrecurrent neural network: A deep learning framework for network-scale traffic learning andforecasting,2019, IEEE Transactions on Intelligent Transportation Systems
 Convolutional neural networks ongraphs with fast localized spectral filtering,2016, In Advances in Neural Information Processing Systems
 BERT: Pre-training of deepbidirectional transformers for language understanding,2019, In Conference of the North AmericanChapter of the Association for Computational Linguistics: Human Language Technologies
 Animage is worth 16x16 words: Transformers for image recognition at scale,2020, arXiv preprintarXiv:2010
 Convolutional networks on graphs forlearning molecular fingerprints,2015, In Advances in Neural Information Processing Systems
 Dyngem: Deep embedding method for dynamicgraphs,2018, CoRR
 node2vec: Scalable feature learning for networks,2016, In InternationalConference on Knowledge Discovery and Data Mining
 Inductive representation learning on largegraphs,2017, In Advances in Neural Information Processing Systems
 Heterogeneous graph transformer,2020, InWWW â€™20: The Web Conference 2020
 Swin transformer: Hierarchical vision transformer using shifted windows,2021, arXiv preprintarXiv:2103
 Probabilistic Machine Learning: An introduction,2022, 2022
 Evolvegcn: Evolving graph convolutional networksfor dynamic graphs,2020, In Conference on Artificial Intelligence
 Semi-supervised user geolocation via graphconvolutional networks,2018, In Proceedings of the Association for Computational Linguistics
 Temporal graph networks for deep learning on dynamic graphs,2020, arXiv preprintarXiv:2006
 Dynamic graph representationlearning via self-attention networks,2018, arXiv preprint arXiv:1812
 Structured sequencemodeling with graph convolutional recurrent networks,2018, In International Conference on NeuralInformation Processing
 Understanding self-supervised learning dynamicswithout contrastive pairs,2021, arXiv preprint arXiv:2102
 Self-supervisedlearning from a multi-view perspective,2020, arXiv preprint arXiv:2006
 Attention is all you need,2017, In Advances in Neural InformationProcessing Systems
 Linformer: Self-attention withlinear complexity,2020, arXiv preprint arXiv:2006
 Inductive repre-sentation learning on temporal graphs,2020, In International Conference on Learning Representations
 Two sides of thesame coin: Heterophily and oversmoothing in graph convolutional neural networks,2021, arXiv preprintarXiv:2102
 Graph transformernetworks,2019, In Advances in Neural Information Processing Systems
 Big bird: Transformers forlonger sequences,2020, In Advances in Neural Information Processing Systems
 Graph-bert: Only attention is needed forlearning graph representations,2020, arXiv preprint arXiv:2001
 Pairnorm: Tackling oversmoothing in gnns,2020, In InternationalConference on Learning Representations
 Dynamic Network Embedding by ModellingTriadic Closure Process,2018, In Conference on Artificial Intelligence
