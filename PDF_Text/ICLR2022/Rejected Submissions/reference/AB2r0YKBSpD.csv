title,year,conference
 Explaining neuralscaling laws,2021, arXiv preprint arXiv:2102
 ParaCrawl: Web-scale acqUisition of parallel cor-pora,2020, In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics
 Language models arefew-shot learners,2020, arXiv preprint arXiv:2005
 BLEU might be guilty but references are notinnocent,2020, In Proceedings of the 2020 Conference on Empirical Methods in Natural LanguageProcessing (EMNLP)
 Learning curvesfor analysis of deep networks,2021, In Marina Meila and Tong Zhang (eds
 Dual conditional cross-entropy filtering of noisy parallel corpora,2018, arXivpreprint arXiv:1809
 Scaling laws for neural languagemodels,2020, arXiv preprint arXiv:2001
 Imagenet classification with deep con-VolUtional neural networks,2012, Advances in neural information processing systems
 Intelligent selection of language model training data,2010, InProceedings of the ACL 2010 Conference Short Papers
 Learning transferable visualmodels from natural language supervision,2021, arXiv preprint arXiv:2103
 COMET: A neural framework for MTevaluation,2020, In Proceedings of the 2020 Conference on Empirical Methods in Natural LanguageProcessing (EMNLP)
 A constructive predictionof the generalization error across scales,2019, arXiv preprint arXiv:1909
 BLEURT: Learning robust metrics for textgeneration,2020, In Proceedings of the 58th Annual Meeting of the Association for ComputationalLinguistics
