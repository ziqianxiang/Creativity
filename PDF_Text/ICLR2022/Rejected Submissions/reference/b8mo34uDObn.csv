title,year,conference
 Transferring inductive biases throughknowledge distillation,2020, arXiv preprint arXiv:2006
 Adamatch:A unified approach to semi-supervised learning and domain adaptation,2021, CoRR
 Model compression,2006, In InternationalConference on Knowledge Discovery and Data Mining (KDD)
 Unlabeleddata improves adversarial robustness,2019, In Advances in Neural Information Processing Systems(NeurIPS)
 Distilling the knowledge in a neural network,2015, InNIPS Deep Learning and Representation Learning Workshop
 Parameter-efficient transfer learningfor NLP,2019, In Kamalika Chaudhuri and Ruslan Salakhutdinov (eds
 Universal language model fine-tuning for text classification,2018, InProceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume1: Long Papers)
 Text-to-text pre-training for data-to-text tasks,2020, In Proceedings ofthe 13th International Conference on Natural Language Generation
 Finetuning pretrained transformers into rnns,2021, arXiv preprintarXiv:2103
 Removing spurious features can hurt accuracy and affect groupsdisproportionately,2021, In ACM Conference on Fairness
 Prefix-tuning: Optimizing continuous prompts for generation,2021, InProceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)
 ROUGE: A package for automatic evaluation of summaries,2004, In Text SummarizationBranches Out
 Robustness to adversarialperturbations in learning from incomplete data,2019, In Advances in Neural Information ProcessingSystems (NeurIPS)
 Accelerating DeepLearning Workloads through Efficient Multi-Model Execution,2018, In NeurIPS Workshop on Systemsfor Machine Learning
 Bleu: A method for automaticevaluation of machine translation,2002, In Proceedings of the 40th Annual Meeting on Association forComputational Linguistics
 BLEU: A method for automaticevaluation of machine translation,2002, In Association for Computational Linguistics (ACL)
 Language modelsare unsupervised multitask learners,2019, 2019
 Learning multiple visual domains withresidual adapters,2017, In I
 Robust fine-tuning of zero-shot models,2021, arXivpreprint arXiv:2109
 Bitfit: Simple parameter-efficient fine-tuning for transformer-based masked language-models,2021, CoRR
 Human behavior and the principle of least effort,1949, 1949
