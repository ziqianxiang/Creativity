title,year,conference
 Synthetic qa cor-pora generation with roundtrip consistency,2019, In Proceedings of the 57th Annual Meeting of theAssociation for Computational Linguistics
 Data augmentation generative adversarialnetworks,2017, arXiv:1711
 Remixmatch: Semi-supervised learning with distribution alignment and augmenta-tion anchoring,2019, arXiv:1911
 Piqa: Reasoning about physical com-monsense in natural language,2020, In Proceedings of the AAAI Conference on Artificial Intelligence
 Large scale GAN training for high fidelitynatUral image synthesis,2019, In International Conference on Learning Representations
 LangUage models arefew-shot learners,2020, arXiv:2005
 Unlabeled dataimproves adversarial robustness,2019, arXiv:1905
 Semi-Supervised Learning,2009, MIT
 Xgboost: A scalable tree boosting system,2016, Proceedings of the22nd acm sigkdd international conference on knowledge discovery and data mining
 Big self-supervised models are strong semi-supervised learners,2020, NeurIPS
 Self-training avoids using spuriousfeatures under domain shift,2020, In Hugo Larochelle
 Electra: Pre-trainingtext encoders as discriminators rather than generators,2020, International Conference on LearningRepresentations
 On the asymptotic improvement in the out-come of super-vised learning provided by additional nonsupervised learning,1970, IEEE Transactions on Computers
 Good semi-supervised learning that requires a bad gan,2017, arXiv preprint arXiv:1705
 Bert: Pre-training of deepbidirectional transformers for language understanding,2019, Proceedings of the 2019 Conference ofthe North American Chapter of the Association for Computational Linguistics: Human LanguageTechnologies
 Self-training improves pre-training for natural language understand-ing,2020, arXiv:2010
 Learning to recognize patterns without a teacher,1967, IEEE Transactions on InformationTheory
 Making pre-trained language models better few-shotlearners,2020, arXiv:2012
 Aspect-based sentimentanalysis of drug reviews applying cross-domain and cross-data learning,2018, Proceedings of the 2018International Conference on Digital Health
 Deberta: Decoding-enhanced bertwith disentangled attention,2020, arXiv:2006
 A baseline for detecting misclassified and out-of-distributionexamples in neural networks,2016, ICLR
 A style-based generator architecture for generative adver-sarial networks,2019, Proceedings of the IEEE conference on computer vision and pattern recognition
 Contextual augmentation: Data augmentation by words with paradigmatic rela-tions,2018, In Proceedings of the 2018 Conference of the North American Chapter of the Associationfor Computational Linguistics: Human Language Technologies
 Multiclass classification of dry beans using computer visionand machine learning techniques,2020, Computers and Electronics in Agriculture
 Learning multiple layers of features from tiny images,2009, 2009
 Data augmentation using pre-trained trans-former models,2020, arXiv:2003
 Pseudo-label: The simple and efficient semi-supervised learning method fordeep neural networks,2013, Workshop on challenges in representation learning
 Cor-pora generation for grammatical error correction,2019, arXiv:1904
 Improving multi-task deep neu-ral networks via knowledge distillation for natural language understanding,2019, arXiv:1904
 Roberta: A robustly optimized bert pretrainingapproach,2019, arXiv:1907
 Self-distillation amplifies regularizationin hilbert space,2020, In Hugo Larochelle
 Exemplar vaes for exemplar based genera-tion and data augmentation,2020, arXiv:2004
 Semi-supervised learning with generative adversarial networks,2016, arXiv preprintarXiv:1606
 Realistic evalu-ation of deep semi-supervised learning algorithms,2018, NeurIPS
 Statistical and algorithmic insights for semi-supervisedlearning with self-training,2020, CoRR
 Languagemodels are unsupervised multitask learners,2019, 2019
 Automatically generating extraction patterns from untagged text,1996, Proceedings of thenational conference on artificial intelligence
 Semi-supervised self-training of objectdetection models,2005, Applications of Computer Vision and the IEEE Workshop on Motion and VideoComputing
 Probability of error of some adaptive pattern-recognition machines,1965, IEEE Transactionson Information Theory
 Neural machine translation of rare words withsubword units,2016, In Proceedings of the 54th Annual Meeting of the Association for ComputationalLinguistics (Volume 1: Long Papers)
 A simple but tough-to-beat data augmentation approach for natural language understanding and generation,2020, arXivpreprint arXiv:2009
 Very deep convolutional networks for large-scale imagerecognition,2014, arXiv:1409
 Fixmatch: Simplifying semi-supervised learningwith consistency and confidence,2020, arXiv:2001
 Ernie: Enhanced representation through knowledge integration,2019, arXivpreprint arXiv:1904
 SuperGLUE: A stickier benchmark for general-purpose languageunderstanding systems,2019, arXiv:1905
 Kdgan: Knowledge distillation with genera-tive adversarial networks,2018, NeurIPS
 Theoretical analysis of self-trainingwith deep networks on unlabeled data,2021, In International Conference on Learning Representations
 Transformers: State-of-the-art naturallanguage processing,2020, Proceedings of the 2020 Conference on Empirical Methods in NaturalLanguage Processing: System Demonstrations
 Conditional bert contextualaugmentation,2019, International Conference on Computational Science
 Fashion-mnist: a novel image dataset for benchmark-ing machine learning algorithms,2017, arXiv:1708
 Unsupervised dataaugmentation for consistency training,2019, arXiv preprint arXiv:1904
 Bert-of-theseus: Compressingbert by progressive module replacing,2020, Proceedings of the 2020 Conference on Empirical Methodsin Natural Language Processing (EMNLP)
 Billion-scale Semi-supervised learning for image classification,2019, arXiv:1905
 G-daug: Generative data augmen-tation for commonsense reasoning,2020, arXiv:2004
 Unsupervised word sense disambiguation rivaling supervised methods,1995, 33rdannual meeting of the association for computational linguistics
 QANet: Combining local convolution with global self-attention for readingcomprehension,2018, ICLR
 Wide residual networks,2016, In Edwin R
 Defending against neural fake news,2019, Advances in Neural Information ProcessingSystems
 mixup: Beyond empiricalrisk minimization,2018, ICLR
