title,year,conference
 Neural machine translation by jointlylearning to align and translate,2014, arXiv preprint arXiv:1409
 Lambdanetworks: Modeling long-range interactions without attention,2021, arXiv preprintarXiv:2102
 Attention augmentedconvolutional networks,2019, In Proceedings of the IEEE International Conference on Computer Vision
 Revisiting resnets: Improved training and scaling strategies,2021, arXivpreprint arXiv:2103
 Understanding robustness of transformers for image classification,2021, arXiv preprintarXiv:2103
 End-to-end object detection with transformers,2020, arXiv preprint arXiv:2005
 Emerging properties in self-supervised vision transformers,2021, arXiv preprintarXiv:2104
 Uniter: Universal image-text representation learning,2020, In European Conference onComputer Vision
 A2-nets: Doubleattention networks,2018, arXiv preprint arXiv:1810
 Rethinking attentionwith performers,2020, arXiv preprint arXiv:2009
 Finding the needle in the haystackwith convolutions: on the benefits of architectural bias,2019, In Advances in Neural InformationProcessing Systems
 Bert: Pre-training of deepbidirectional transformers for language understanding,2018, arXiv preprint arXiv:1810
 Animage is worth 16x16 words: Transformers for image recognition at scale,2020, arXiv preprintarXiv:2010
 Deep Learning,2016, MIT Press
 Explaining and harnessing adversarialexamples,2014, arXiv preprint arXiv:1412
 Levit: a vision transformer in convnetâ€™s clothing for faster inference,2021, arXivpreprint arXiv:2104
 Deep residual learning for imagerecognition,2016, In Proceedings of the IEEE conference on computer vision and pattern recognition
 Benchmarking neural network robustness to common corrup-tions and perturbations,2019, Proceedings of the International Conference on Learning Representations
 Natural adversarialexamples,2021, CVPR
 Distilling the knowledge in a neural network,2015, arXivpreprint arXiv:1503
 Relation networks for objectdetection,2018, In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition
 Transformers are rnns:Fast autoregressive transformers with linear attention,2020, In International Conference on MachineLearning
 Imagenet classification with deep convolu-tional neural networks,2017, Communications of the ACM
 Gradient-based learning applied todocument recognition,1998, Proceedings of the IEEE
 The largelearning rate phase of deep learning: the catapult mechanism,2020, arXiv preprint arXiv:2003
 Understanding the difficultyof training transformers,2020, arXiv preprint arXiv:2004
 Swin transformer: Hierarchical vision transformer using shifted windows,2021, arXiv preprintarXiv:2103
 On the robustness of vision transformersto adversarial examples,2021, arXiv preprint arXiv:2104
 Rethinkingthe design principles of robust vision transformer,2021, arXiv preprint arXiv:2105
 Stand-alone self-attention in vision models,2019, arXiv preprint arXiv:1906
 Evaluation of Pooling Operations in Con-volutional Architectures for Object Recognition,2010, In Konstantinos Diamantaras
 Deep learning in neural networks: An overview,0893, Neural Networks
 On the adversarial robustnessof visual transformers,2021, arXiv preprint arXiv:2103
 Videobert: A jointmodel for video and language representation learning,2019, In Proceedings of the IEEE InternationalConference on Computer Vision
 Fixing the train-test resolutiondiscrepancy,2019, arXiv preprint arXiv:1906
 Goingdeeper with image transformers,2021, arXiv preprint arXiv:2103
 Attention is all you need,2017, In Advances in neural informationprocessing systems
 Self-attention withlinear complexity,2020, arXiv preprint arXiv:2006
 Visual Transformers: Token-based Image Representation and Processingfor Computer Vision,2006, arXiv:2006
 Cvt:Introducing convolutions to vision transformers,2021, arXiv preprint arXiv:2103
 Incorporatingconvolution designs into visual transformers,2021, arXiv preprint arXiv:2103
