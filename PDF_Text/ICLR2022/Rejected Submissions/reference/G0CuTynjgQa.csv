title,year,conference
 A convergence theory for deep learning via over-parameterization,2019, In International Conference on Machine Learning
 Towards principled methods for training generative adversarialnetworks,2017, In International Conference on Learning Representations
 Dropout: Explicit forms andcapacity control,2021, In International Conference on Machine Learning
 Understanding over-parameterization in generative ad-versarial networks,2021, In International Conference on Learning Representations
 Spectrally-normalized margin bounds forneural networks,2017, Advances in Neural Information Processing Systems
 Benign Ovefitting in linearregression,2020, Proceedings of the National Academy of Sciences
 Reconciling modern machine-learning practice and the classical bias-variance trade-off,2019, Proceedings of the National Academyof Sciences
 Introduction to statistical learning the-ory,2004, In Machine Learning 2003
 Large scale gan training for high fidelity naturalimage synthesis,2019, In International Conference on Learning Representations
 Gradient descent finds globalminima of deep neural networks,2019, In International Conference on Machine Learning
 Mathematical models of overparameterized neuralnetworks,2021, Proceedings of the IEEE
 Many paths to equilibrium: Gans do not need to decrease a divergence at everystep,2018, In International Conference on Learning Representations
 Regularisation of neural net-works by enforcing lipschitz continuity,2021, Machine Learning
 How generative adversarial net-works and their variants work: An overview,2019, ACM Computing Surveys (CSUR)
 Regularization matters: A nonparamet-ric perspective on overparametrized neural network,2021, In International Conference on ArtificialIntelligence and Statistics
 Neural tangent kernel: convergence and gener-alization in neural networks,2018, In Advances in Neural Information Processing Systems
 Early-stopped neural networks are consistent,2021, arXivpreprint arXiv:2106
 Traininggenerative adversarial networks with limited data,2020, In Advances in Neural Information ProcessingSystems
 Ana-lyzing and improving the image quality of stylegan,2020, In Proceedings of the IEEE Conference onComputer Vision and Pattern Recognition
 A style-based generator architecture for generativeadversarial networks,2021, IEEE Transactions on Pattern Analysis and Machine Intelligence
 A large-scale studyon regularization and normalization in gans,2019, In International Conference on Machine Learning
 Nonparametric regression with shallow overparameterizedneural networks trained by gd with early stopping,2021, In Conference on Learning Theory
 Onthe effectiveness of least squares generative adversarial networks,2019, IEEE Transactions on PatternAnalysis and Machine Intelligence
 The numerics of gans,2017, In Advances inNeural Information Processing Systems
 Spectral normalizationfor generative adversarial networks,2018, In International Conference on Learning Representations
 Gradient descent gan optimization is locally stable,2017, InAdvances in Neural Information Processing Systems
 Uniform convergence may be unable to explain generaliza-tion in deep learning,2019, In Advances in Neural Information Processing Systems
 On the proof of global convergence of gradient descent for deep relu networks withlinear widths,2021, In International Conference on Machine Learning
 Towards a better understanding and regularization of gan training dy-namics,2019, In Conference on Uncertainty in Artificial Intelligence (UAI)
 f-gan: Training generative neural sam-plers using variational divergence minimization,2016, In Advances in Neural Information ProcessingSystems
 Training robustneural networks using lipschitz bounds,2021, IEEE Control Systems Letters
 On the convergence and ro-bustness of training gans with regularized optimal transport,2018, In Advances in Neural InformationProcessing Systems
 Minimizing finite sums with the stochasticaverage gradient,2017, Mathematical Programming
 Learning disconnected man-ifolds: a no gan’s land,2020, In International Conference on Machine Learning
 Improving generalization and stability ofgenerative adversarial networks,2019, In International Conference on Learning Representations
 Wasserstein auto-encoders,2018, In International Conference on Learning Representations
 On data augmentation for gan training,2021, IEEE Transactions on Image Processing
 Generalization in generative adversarial networks: A novel perspective from pri-vacy protection,2019, In Advances in Neural Information Processing Systems
 Robustness and generalization,2012, Machine learning
 Understanding and stabilizing gans’training dynamics with control theory,2020, In Proceedings of the 37th International Conference onMachine Learning
 Self-attention generativeadversarial networks,2019, In International Conference on Machine Learning
 Consistency regularization forgenerative adversarial networks,2020, In International Conference on Learning Representations
 On the discrimination-generalization tradeoff in gans,2018, In International Conference on Learning Representations
 Energy-based generative adversarial networks,2017, InInternational Conference on Learning Representations
 Differentiable augmentation fordata-efficient gan training,2020, In Advances in Neural Information Processing Systems
 Image augmentations forgan training,2020, arXiv preprint arXiv:2006
 Lipschitz generative adversarial nets,2019, In International Conference on MachineLearning
 Gradient descent optimizes over-parameterized deep relu networks,2020, Machine Learning
