title,year,conference
 Gradient based sample selectionfor online continual learning,2019, In Advances in Neural Information Processing Systems
 Efficientlifelong learning with A-GEM,2019, In 7th International Conference on Learning Representations
 On tiny episodic memories in continuallearning,2019, arXiv preprint arXiv:1902
 Continual learning in low-rankorthogonal subspaces,2020, Advances in Neural Information Processing Systems
 Uncertainty-guidedcontinual learning with bayesian neural networks,2020, In 8th International Conference on LearningRepresentations
 Using noise to compute error surfaces in connectionist networks:A novel means of reducing catastrophic forgetting,2002, Neural Computation
 Accelerated gradient methods for nonconvex nonlinear andstochastic programming,2016, Mathematical Programming
 Deep residual learning for imagerecognition,2016, In Proceedings of the IEEE conference on computer vision and pattern recognition
 Overcomingcatastrophic forgetting in neural networks,2017, Proceedings of the national academy of sciences
 Optimal continual learning has perfectmemory and is np-hard,2020, arXiv preprint arXiv:2006
 Learning multiple layers of features from tiny images,2009, 2009
 Gradient-based learning applied todocument recognition,1998, Proceedings of the IEEE
 Overcoming catastrophic forgetting withunlabeled data in the wild,2019, In Proceedings of the IEEE International Conference on ComputerVision
 A neural dirichlet process mixture modelfor task-free continual learning,2020, In 8th International Conference on Learning Representations
 Non-convex finite-sum optimization viascsg methods,2017, In Advances in Neural Information Processing Systems
 Gradient episodic memory for continual learning,2017, InAdvances in Neural Information Processing Systems
 Under-standing the role of training regimes in continual learning,2020, In Advances in Neural InformationProcessing Systems 33: Annual Conference on Neural Information Processing Systems 2020
 Continual learning in reinforcement environments,1995, PhD thesis
 A lifelong learning perspective for mobile robot control,1994, In Intelligent Robotsand Systems
 Lifelong learning with dynamicallyexpandable networks,2018, In 6th International Conference on Learning Representations
 Adaptive methodsfor nonconvex optimization,2018, In Advances in neural information processing systems
 It implies that the cumulativesum of Γt does not increase monotonically,2022, Therefore
 The practical continual learning tasks have the restric-tion on full access to the entire data points of previously learned tasks,2022, Unlike taking expectation21Under review as a conference paper at ICLR 2022over It 〜M and M 〜P ∪ C
