title,year,conference
 Natural gradient works efficiently in learning,2000, Neural Computation
 JAX: composable transformations of Python + NumPy programs,2020, 2020
 BackPACK: Packing more into backprop,2020, In InternationalConference on Learning Representations
 Bootstrap methods: Another look at the jackknife,1979, The Annals of Statistics
 The early phase of neural network training,2020, InInternational Conference on Learning Representations
 Limitations of the empirical Fisher approximation fornatural gradient descent,2019, In Advances in Neural Information Processing Systems
 Deep learning via Hessian-free optimization,2010, In Proceedings of the 27th International Conferenceon Machine Learning
 Kronecker-factored curvature approximations for recurrent neuralnetworks,2018, In International Conference on Learning Representations
 Measurements of three-level hierarchical structure in the outliers in the spectrum of deepnetHessians,2019, In Proceedings of the 36th International Conference on Machine Learning
 Fast exact multiplication by the Hessian,1994, Neural Computation
 Online structured Laplace approximations for overcomingcatastrophic forgetting,2018, In Advances in Neural Information Processing Systems
 A scalable Laplace approximation for neural networks,2018, InInternational Conference on Learning Representations
 DeePOBS: A deeP learning oPtimizer benchmark suite,2019, In 7thInternational Conference on Learning Representations
 Fast curvature matrix-vector Products for second-order gradient descent,2002, Neuralcomputation
 WoodFisher: Efficient second-order aPProximation for neural networkcomPression,2020, In Advances in Neural Information Processing Systems
