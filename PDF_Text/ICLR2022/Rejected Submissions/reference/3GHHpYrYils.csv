title,year,conference
 Estimating or propagating gradientsthrough stochastic neurons for conditional computation,2013, CoRR
 Online algorithms and stochastic approximations,1998, In David Saad (ed
 Analyzing bagging,2002, The annals OfStatistics
 Deep sequential neural networks,2015, EWRL
 Learning factored representations in a deepmixture of experts,2014, ICLR
 Switch transformers: Scaling to trillion parametermodels with simple and efficient sparsity,2021, arXiv preprint arXiv:2101
 Pathnet: Evolution channels gradient descent in super neuralnetworks,2017, CoRR
 Approach to anytime learning,1992, In Proceedings ofthe Ninth International Conference on Machine Learning
 Adam: A method for stochastic optimization,2014, arXiv preprintarXiv:1412
 Adam: A method for stochastic optimization,2015, In YoshuaBengio and Yann LeCun (eds
 Learning multiple layers of features from tiny images,2009, University of Toronto
 Gradient-based learning appliedto document recognition,1998, Proceedings of the IEEE
 Gshard: Scaling giant models with conditionalcomputation and automatic sharding,2020, CoRR
 Roberta: A robUstly optimized bert pretrainingapproach,2019, arXiv preprint arXiv:1907
 Learning Under conceptdrift: A review,2018, IEEE Transactions on Knowledge and Data Engineering
 A sUrvey on transfer learning,2010, TKDE
 LangUagemodels are UnsUpervised mUltitask learners,2019, OpenAI blog
 Continual Learning in Reinforcement Environments,1994, PhD thesis
 NeUral machine translation of rare words withsUbword Units,2016, In Proceedings of the 54th Annual Meeting of the Association for ComputationalLinguistics (Volume 1: Long Papers)
 Very deep convolUtional networks for large-scale imagerecognition,2015, In International Conference on Learning Representations
 Lifelong learning algorithms,1998, In Learning to learn
 Statistical learning theory,1998, Wiley New York
 Efficient continUal learning with modUlarnetworks and task-driven priors,2021, In International Conference on Learning Representations
 Growing a brain: Fine-tuning by increasingmodel capacity,2017, In IEEE Conference on Computer Vision and Pattern Recognition
 Constructive algorithms for hierarchical mixtures ofexperts,1995, In Advances in Neural Information Processing Systems
 Firefly neural architecture descent: a generalapproach for growing neural networks,2020, In Advances in Neural Information Processing Systems
 Lifelong learning with dynamicallyexpandable networks,2018, In 6th International Conference on Learning Representations
 Adadelta: an adaptive learning rate method,2012, arXiv preprint arXiv:1212
