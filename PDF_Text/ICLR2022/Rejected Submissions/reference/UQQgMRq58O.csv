title,year,conference
 Robust bi-tempered logistic loss basedon bregman divergences,2019, In Advances in Neural Information Processing Systems
 An investigation of how label smoothing affectsgeneralization,2020, arXiv preprint arXiv:2010
 Learning with instance-dependentlabel noise: A sample sieve approach,2020, In International Conference on Learning Representations
 Towards better decoding and language model integration in sequence tosequence models,2017, Proc
 Label distribution learning,2016, IEEE Transactions on Knowledge and Data Engineering
 Mixture proportion estimation via kernel embeddingsof distributions,2016, In International conference on machine learning
 Deep residual learning for image recognition,2016, InProceedings of the IEEE conference on computer vision and pattern recognition
 Distilling the knowledge in a neural network,2015, arXiv preprintarXiv:1503
 Complementary-label learning for arbitrarylosses and models,2019, In International Conference on Machine Learning
 Mentornet: Learning data-drivencurriculum for very deep neural networks on corrupted labels,2018, In International Conference on MachineLearning
 Adam: A method for stochastic optimization,2014, arXiv preprintarXiv:1412
 Learning multiple layers of features from tiny images,2009, Technicalreport
 Early-learning regular-ization prevents memorization of noisy labels,2020, Advances in Neural Information Processing Systems
 The importance of understanding instance-level noisy labels,2021, arXiv preprint arXiv:2102
 Peer loss functions: Learning from noisy labels without knowing noise rates,2020, InInternational Conference on Machine Learning
 Learning from corrupted binarylabels via class-probability estimation,2015, In International Conference on Machine Learning
 Learning with noisylabels,2013, In Advances in neural information processing systems
 Making deepneural networks robust to label noise: A loss correction approach,2017, In Proceedings of the IEEE Conferenceon Computer Vision and Pattern Recognition
 Regularizing neuralnetworks by penalizing confident output distributions,2017, arXiv preprint arXiv:1701
 Identifying mislabeled data usingthe area under the margin ranking,2020, arXiv preprint arXiv:2001
 A stochastic approximation method,1951, The annals of mathematicalstatistics
 Classification with asymmet-ric label noise: Consistency and maximal denoising,2013, In COLT
 Rethinking theinception architecture for computer vision,2016, In Proceedings of the IEEE conference on computer vision andpattern recognition
 Attention is all you need,2017, In Proceedings of the 31st International Conference onNeural Information Processing Systems
 When optimizing f -divergence is robust with label noise,2020, In InternationalConference on Learning Representations
 Towards understanding label smoothing,2020, arXiv preprintarXiv:2006
 L_dmi: An information-theoretic noise-robust lossfunction,2019, NeurIPS
 Rethinking bias-variance trade-off forgeneralization of neural networks,1076, In International Conference on Machine Learning
 Searching to exploit memorizationeffect in learning with noisy labels,1078, In International Conference on Machine Learning
 Dual t:Reducing estimation error for transition matrix in label-noise learning,2020, arXiv preprint arXiv:2006
 Re-thinking soft labels for knowledge distillation: A bias-variance tradeoff perspective,2021, arXiv preprintarXiv:2102
 Clusterability as an alternative to anchor points when learningwith noisy labels,2021, arXiv preprint arXiv:2102
" Making deep neural nets perform robust training on â€œnoisily"" labeleddatasets remains a challenge",2021, Classical approaches of learning with noisy labels assume the noisy labels areindependent to features
 Our work does not intend to particularly focus on thenoise rate estimation,2021, For readers interested in the noise rate estimation
