title,year,conference
 The implicit regularization of stochastic gradientflow for least squares,2020, In International Conference on Machine Learning
 On the convergence of a class of adam-typealgorithms for non-convex optimization,2018, In International Conference on Learning Representations
 Global convergence ofthe heavy-ball method for convex optimization,2015, In 2015 European control conference (ECC)
 Gradient descent aligns the layers of deep linear networks,2018, arXivpreprint arXiv:1810
 Directional convergence and alignment in deep learning,2020, arXivpreprint arXiv:2006
 Characterizing the implicit bias via a primal-dual analysis,2021, InAlgorithmic Learning Theory
 Fast margin maximization via dual acceleration,2021, InInternational Conference on Machine Learning
 Fantasticgeneralization measures and where to find them,2019, arXiv preprint arXiv:1912
 On large-batch training for deep learning: Generalization gap and sharp minima,2017, InInternational Conference on Learning Representations
 Adam: A method for stochastic optimization,2014, arXiv preprintarXiv:1412
 An improved analysis of stochastic gradient descent withmomentum,2020, Advances in Neural Information Processing Systems
 Stochastic gradient descent on separabledata: Exact convergence with a fixed learning rate,2019, In The 22nd International Conference onArtificial Intelligence and Statistics
 In search of the real inductive bias: On therole of implicit regularization in deep learning,2015, In ICLR
 The implicit bias of adagrad on separable data,2019, In Advances in NeuralInformation Processing Systems
 On the convergence of adam and beyond,2019, arXivpreprint arXiv:1904
 Analytical study of momentum-based accelerationmethods in paradigmatic high-dimensional non-convex problems,2021, Advances in Neural InformationProcessing Systems
 Boosting: Foundations and algorithms,2013, Kybernetes
 Non-ergodicconvergence analysis of heavy-ball algorithms,2019, In Proceedings of the AAAI Conference onArtificial Intelligence
 On the importance of initializationand momentum in deep learning,1139, In International conference on machine learning
 The role of momentum parameters in the optimalconvergence of adaptive polyakâ€™s heavy-ball methods,2021, arXiv preprint arXiv:2102
 Attention is all you need,2017, In Advances in neural informationprocessing systems
 The implicit bias for adaptive optimizationalgorithms on homogeneous neural networks,2021, In International Conference on Machine Learning
 The marginalvalue of adaptive gradient methods in machine learning,2017, In Advances in neural informationprocessing systems
 Aggregated residualtransformations for deep neural networks,2017, In Proceedings of the IEEE conference on computervision and pattern recognition
 A unified analysis of stochastic momentum methods fordeep learning,2018, In IJCAI International Joint Conference on Artificial Intelligence
 On the linear speedup analysis of communication efficientmomentum sgd for distributed non-convex optimization,2019, In International Conference on MachineLearning
