title,year,conference
 Pseudo-labelingand confirmation bias in deep semi-supervised learning,2020, In IJCNN
 Self-labelling via simultaneousclustering and representation learning,2020, In ICLR
 There are many con-sistent explanations of unlabeled data: Why you should average,2019, In ICLR
 Towards open set deep networks,2016, In CVPR
 Mixmatch: A holistic approach to semi-supervised learning,2019, In NeurIPS
 Deep clustering for unsu-pervised learning of visual features,2018, In ECCV
 Semi-supervised learning under classdistribution mismatch,2020, In AAAI
 Imagenet: A large-scalehierarchical image database,2009, In CVPR
 SCAN: learning to classify images without labels,2020, In ECCV
 Semi-supervised learning by entropy minimization,2004, In NIPS
 Safe deep semi-supervised learning for unseen-class unlabeled data,2020, In ICML
 Revisiting self-training for neuralsequence generation,2020, In ICLR
 A baseline for detecting misclassified and out-of-distributionexamples in neural networks,2017, In ICLR
 Trash to treasure: Harvesting OOD data with cross-modal matching for open-setsemi-supervised learning,2021, CoRR
 Label propagation for deep semi-supervised learning,2019, In CVPR
 Semi-supervised learning with deep generative models,2014, In NIPS
 Temporal ensembling for semi-supervised learning,2017, In ICLR
 Pseudo-label: The simple and efficient semi-supervised learning method fordeep neural networks,2013, In Workshop on challenges in representation learning
 Focal loss for denseobject detection,2017, In ICCV
 On the consistency training for open-set semi-sUpervised learning,2021, CoRR
 VirtUal adversarial training:A regUlarization method for sUpervised and semi-sUpervised learning,2019, TPAMI
 RealisticevalUation of deep semi-sUpervised learning algorithms,2018, In NeurIPS
 Semi-sUpervised self-training of objectdetection models,2005, In WACV
 Towardopen set recognition,2013, TPAMI
 TransdUctivesemi-sUpervised deep learning Using min-max featUres,2018, In ECCV
 Fixmatch: Simplifying semi-sUpervised learningwith consistency and confidence,2020, In NeurIPS
 Mean teachers are better role models: Weight-averaged consis-tency targets improve semi-sUpervised deep learning resUlts,2017, In NIPS
 Self-training with noisy stUdentimproves imagenet classification,2020, In CVPR
 Towards k-means-friendly spaces:SimUltaneoUs deep learning and clUstering,2017, In ICML
 MUlti-task cUrricUlUm framework for open-setsemi-sUpervised learning,2020, In ECCV
 Wide residUal networks,2016, In BMVC
