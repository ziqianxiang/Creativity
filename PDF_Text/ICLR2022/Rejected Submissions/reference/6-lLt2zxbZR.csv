title,year,conference
 Pondernet: Learning to ponder,2021, CoRR
 Language models arefeW-shot learners,2020, arXiv preprint arXiv:2005
 What does BERT lookat? an analysis of bertâ€™s attention,2019, CoRR
 A structural probe for finding syntax in Word representa-tions,2019, In Proceedings of the 2019 Conference of the North American Chapter of the Associationfor Computational Linguistics: Human Language Technologies
 Can transformer models mea-sure coherence in text: Re-thinking the shuffle test,1058, In Chengqing Zong
 Albert: A lite bert for self-supervised learning of language representations,2019, arXiv preprintarXiv:1909
 Assessing the ability of lstms to learn syntax-sensitive dependencies,2016, Transactions of the Association for Computational Linguistics
 Efficient estimation of word represen-tations in vector space,2013, arXiv preprint arXiv:1301
 On knowing a word,1999, Annual review ofpsychology
 Making transformers solvecompositional tasks,2021, arXiv preprint arXiv:2108
 Time-dial: Temporal commonsense reasoning in dialog,2021, arXiv preprint arXiv:2106
 Improving language under-standing by generative pre-training,2018, 2018
 Languagemodels are unsupervised multitask learners,2019, 2019
 Choice of plausible alternatives:An evaluation of commonsense causal reasoning,2011, In 2011 AAAI Spring Symposium Series
 Winogrande: An adver-sarial winograd schema challenge at scale,2020, In Proceedings of the AAAI Conference on ArtificialIntelligence
 Time-scale dynamics and the development of an embodied cognition,1995, Mind asmotion: Explorations in the dynamics of cognition
 Attention is all you need,2017, In Advances in neural informationprocessing systems
 Superglue: a stickier benchmark for general-purpose language un-derstanding systems,2019, In Proceedings of the 33rd International Conference on Neural InformationProcessing Systems
 Evaluating commonsense in pre-trainedlanguage models,2020, In Proceedings of the AAAI Conference on Artificial Intelligence
