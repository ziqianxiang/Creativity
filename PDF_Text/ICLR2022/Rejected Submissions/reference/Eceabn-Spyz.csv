title,year,conference
  Learning to learn by gradient descent by gradientdescent,2016,   In D
  Learning to learn by gradient descent by gradientdescent,2016, In Advances in neural information processing systems (NeurIPS)
  Stronger generalization bounds fordeep nets via a compression approach,2018, In International Conference on Machine Learning (ICML)
  Fine-grained analysis of op-timization and generalization for overparameterized two-layer neural networks,2019,  In InternationalConference on Machine Learning (ICML)
  Spectrally-normalized margin bounds forneural networks,2017,  In Advances in Neural Information Processing Systems (NeurIPS)
  Learning to optimize in swarms,2019,  InAdvances in Neural Information Processing Systems (NeurIPS)
   Entropy-SGD: Biasing gradi-ent descent into wide valleys,2017,  In International Conference on Learning Representations (ICLR)
  Training stronger baselines for learning to optimize,2020,  arXiv preprint arXiv:2010
 Automated synthetic-to-real generalization,2020, In International Conference on Machine Learning (ICML)
  Learning to learn without gradient descent by gradientdescent,2017, In International Conference on Machine Learning (ICML)
  Label noise SGD provably prefers flat global mini-mizers,2021, CoRR
  Sharp minima can generalizefor deep nets,2017, In International Conference on Machine Learning (ICML)
  Gradient descent provably optimizesover-parameterized neural networks,2019,  In International Conference on Learning Representations(ICLR)
   Size-independent sample complexity ofneural networks,2018, In Conference On Learning Theory (COLT)
  Simplifying neural nets by discovering flat minima,1994,  InAdvances in neural information processing systems (NeurIPS)
  Flat Minima,1997,  Neural Computation
   Minimax estimation of neural net distance,2018,   In Advances in NeuralInformation Processing Systems (NeurIPS)
  Understanding estimation and generalization error of gener-ative adversarial networks,2021, IEEE Transactions on Information Theory
 Learning to defense by learningto attack,2018, arXiv preprint arXiv:1811
   Fantas-tic generalization measures and where to find them,2020,   In International Conference on LearningRepresentations (ICLR)
 Improving generalization performance by switching fromAdam to SGD,2017, arXiv preprint arXiv:1712
  Adam:  A method for stochastic optimization,2014,  arXiv preprintarXiv:1412
   Learning multiple layers of features from tiny images,2009,   Master’sthesis
 Gradient-based learning applied todocument recognition,1998, Proceedings of the IEEE
  Visualizing the loss land-scape of neural nets,2018, In Advances in Neural Information Processing Systems (NeurIPS)
 Learning to optimize,2016, arXiv preprint arXiv:1606
  Convergence analysis of two-layer neural networks with ReLU activa-tion,2017, Advances in Neural Information Processing Systems (NeurIPS)
 Learning gradient descent: Better generalization and longerhorizons,2017, In International Conference on Machine Learning (ICML)
  Piecewise strong convexity of neural networks,2019,  In Advances in Neural InformationProcessing Systems (NeurIPS)
 A direct adaptive method for faster backpropagation learning: the rpropalgorithm,1993, In IEEE International Conference on Neural Networks
     An  overview  of  gradient  descent  optimization  algorithms,2016,     arXiv  preprintarXiv:1609
  Learning a minimax optimizer:  A pilot study,2021,  In International Conference on LearningRepresentations (ICLR)
 Lecture 6,2012,5—RmsProp: Divide the gradient by a running average of itsrecent magnitude
   Learned optimizers that scale andgeneralize,2017, In International Conference on Machine Learning (ICML)
  Pyhessian: Neural networksthrough the lens of the Hessian,2020, In 2020 IEEE International Conference on Big Data (Big Data)
 Spectral norm regularization for improving the generalizabilityof deep learning,2017, arXiv preprint arXiv:1705
 Towards theoreticallyunderstanding why SGD generalizes better than Adam in deep learning,2020,  In Advances in NeuralInformation Processing Systems (NeurIPS)
  Generalization error bounds with probabilistic guar-antee for SGD in nonconvex optimization,2018, arXiv preprint arXiv:1802
 Understanding the generalization of Adam inlearning neural networks with proper regularization,2021, arXiv preprint arXiv:2108
