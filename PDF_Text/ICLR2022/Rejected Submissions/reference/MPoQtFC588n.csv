title,year,conference
 Neural ordinarydifferential equations,2018, In Proceedings of the Advances in Neural Information Processing Systems
 Diverse branch block: Buildinga convolution as an inception-like unit,2021, arXiv preprint arXiv:2103
 Repvgg:Making vgg-style convnets great again,2021, In Proceedings of the IEEE Conference on ComputerVision and Pattern Recognition
 Understanding the difficulty of training deep feedforward neuralnetworks,2010, In Proceedings of the International Conference on Artificial Intelligence and Statistics
 Delving deep into rectifiers: Surpassinghuman-level performance on imagenet classification,2015, In Proceedings of the IEEE InternationalConference on Computer Vision
 Identity mappings in deep residualnetworks,2016, In Proceedings of the European Conference on Computer Vision
 Mobilenets: Efficient convolutional neural networks formobile vision applications,2017, arXiv preprint arXiv:1704
 Network trimming: A data-drivenneuron pruning approach towards efficient deep architectures,2016, arXiv
 Squeeze-and-excitation networks,2018, In Proceedings of the IEEEConference on Computer Vision and Pattern Recognition
 Batch normalization: Accelerating deep network training byreducing internal covariate shift,2015, In Proceedings of the International Conference on MachineLearning
 Imagenet classification with deep con-volutional neural networks,2012, In Proceedings of the Advances in Neural Information ProcessingSystems
 Efficient backprop,2012, InNeural networks: Tricks of the trade
 Residual distillation: Towards portable deep neural networks withoutshortcuts,2020, In Proceedings of the Advances in Neural Information Processing Systems
 Pruning filters forefficient convnets,2017, In International Conference on Learning Representations (ICLR)
 Learn-ing efficient convolutional networks through network slimming,2017, In Proceedings of the IEEEinternational conference on computer vision
 Going deeper with neural networkswithout skip connections,2020, In Proceedings of the International Conference on Image Processing
 Mo-bilenetv2: Inverted residuals and linear bottlenecks,2018, In Proceedings of the IEEE Conference onComputer Vision and Pattern Recognition
 Exact solutions to the nonlinear dynam-ics of learning in deep linear neural networks,2013, arXiv preprint arXiv:1312
 Very deep convolutional networks for large-scale imagerecognition,2015, In Proceedings of the International Conference on Learning Representations
 Going deeper with convolutions,2015, InProceedings ofthe IEEE Conference on Computer Vision and Pattern Recognition
 Rethink-ing the inception architecture for computer vision,2016, In Proceedings of the IEEE Conference onComputer Vision and Pattern Recognition
 Efficientnet: Rethinking model scaling for convolutional neural net-works,2019, In International Conference on Machine Learning
 Efficientnetv2: Smaller models and faster training,2021, arXiv preprintarXiv:2104
 Residual networks behave like ensembles ofrelatively shallow networks,2016, arXiv preprint arXiv:1605
 A proposal on machine learning via dynamical systems,2017, Communications in Mathematicsand Statistics
 Diracnets: Training very deep neural networks withoutskip-connections,2017, arXiv preprint arXiv:1706
