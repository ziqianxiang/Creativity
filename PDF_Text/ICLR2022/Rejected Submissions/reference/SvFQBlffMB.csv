title,year,conference
 On the opportu-nities and risks of foundation models,2021, arXiv preprint arXiv:2108
 Language models arefew-shot learners,2020, arXiv preprint arXiv:2005
 Robust overfittingmay be mitigated by properly learned smoothening,2021, In 9th International Conference on LearningRepresentations
 An overview of bilevel optimization,2007, Annals ofOperations Research
 Imagenet: A large-scale hi-erarchical image database,2009, In 2009 IEEE conference on computer vision and pattern recognition
 Switch transformers: Scaling to trillion parametermodels with simple and efficient sparsity,2021, arXiv preprint arXiv:2101
 Born-again neural networks,2018, In Jennifer G
 Deep residual learning for imagerecognition,2016, In 2016 IEEE Conference on Computer Vision and Pattern Recognition
 Dark knowledge,2014, Presented as the keynote inBayLearn
 Distilling the knowledge in a neural network,2015, arXivpreprint arXiv:1503
 Annealing knowledge distilla-tion,2021, arXiv preprint arXiv:2104
 Tinybert: Distilling bert for natural language understanding,2020, In Proceedings of the 2020Conference on Empirical Methods in Natural Language Processing: Findings
 Roberta: A robustly optimized BERT pretrainingapproach,2019, CoRR
 Shufflenet V2: practical guidelinesfor efficient CNN architecture design,2018, In Vittorio Ferrari
	When does label Smooth-ing help? In Hanna M,2019, Wallach
 Rethink-ing the inception architecture for computer vision,2016, In Proceedings of the IEEE conference oncomputer vision and pattern recognition
 Superglue: a stickier benchmark for general-purpose language un-derstanding systems,2019, In Proceedings of the 33rd International Conference on Neural InformationProcessing Systems
 Pangu-Î±: Large-scale autoregressive pretrained chineselanguage models With auto-parallel computation,2021, arXiv preprint arXiv:2104
 Self-distillation as instance-specific label smoothing,2020, InHugo Larochelle
