title,year,conference
 Differentiable convex optimization layers,2019, In Advances in Neural Information ProcessingSystems 32: Annual Conference on Neural Information Processing Systems 2019
 Learning to learn by gradient descent by gradientdescent,2016, In Advances in neural information processing systems
 Toward a unified theory of sparse dimensionalityreduction in Euclidean space,2015, Geometric and Functional Analysis
 Convex Optimization,2004, Cambridge University Press
 Low-rank approximation and regression in inputsparsity time,0004, J
 Iterative hessian sketch in input sparsity time,2019, In Proceedingsof 33rd Conference on Neural Information Processing Systems (NeurIPS)
 Randomized block cubic NeWton method,2018, In Proceedings of the35th International Conference on Machine Learning
 Learning space partitions for nearestneighbor search,2020, In 8th International Conference on Learning Representations
 Stochastic block BFGS: squeezing morecurvature out of data,2016, In Proceedings of the 33nd International Conference on Machine Learning
 Accelerated stochasticmatrix inversion: General theory and speeding up BFGS rules for faster second-order optimiza-tion,2018, In Advances in Neural Information Processing Systems 31: Annual Conference on NeuralInformation Processing Systems 2018
 RSN: randomized subspaceNeWton,2019, In Advances in Neural Information Processing Systems 32: Annual Conference onNeural Information Processing Systems 2019
 Learning-based frequency estimationalgorithms,2019, In 7th International Conference on Learning Representations
 Learning-based loW-rank approximations,2019, In Advances inNeural Information Processing Systems 32: Annual Conference on Neural Information ProcessingSystems 2019
 GPU acceleratedsub-sampled NeWton’s method for convex classification problems,2019, In Proceedings of the 2019SIAM International Conference on Data Mining
 On learned sketches forrandomized numerical linear algebra,2020, arXiv:2007
 Osnap: Faster numerical linear algebra algorithms via sparser subspaceembeddings,2013, In 2013 IEEE 54th Annual Symposium on Foundations of Computer Science
 Iterative Hessian sketch: Fast and accurate solution approxi-mation for constrained least-squares,2016, J
 Newton sketch: A near linear-time optimization algorithmwith linear-quadratic convergence,2017, SIAM J
 Sub-sampled Newton methods,2019, Math
 Improved approximation algorithms for large matrices via random projections,2006, In 200647th Annual IEEE Symposium on Foundations of Computer Science (FOCS’06)
 Second-order optimization for non-convex machinelearning: an empirical study,2020, In Proceedings of the 2020 SIAM International Conference on DataMining
