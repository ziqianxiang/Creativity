title,year,conference
 Controlling computation versus quality forneural sequence models,2020, arXiv preprint arXiv:2002
 Language models are few-shotlearners,2020, In H
 Learning imbalanceddatasets with label-distribution-aware margin loss,2019, In Advances in Neural Information ProcessingSystems
 Hierarchical multiscale recurrent neural net-works,2016, arXiv preprint arXiv:1609
 Boosting with abstention,2016, In D
 Class-balanced loss based oneffective number of samples,2019, In CVPR
 Imagenet: A large-scale hier-archical image database,2009, In 2009 IEEE Conference on Computer Vision and Pattern Recognition
 Reducing transformer depth on demand withstructured dropout,2019, arXiv preprint arXiv:1909
 Born-again neural networks,2018, In Proceedings of the 35th International Conference onMachine Learning
 Compressing large-scale transformer-based models:A case study on bert,2020, arXiv preprint arXiv:2002
 Support vector ma-chines with a reject option,2009, In D
 Adaptive computation time for recurrent neural networks,2016, arXiv preprintarXiv:1603
 Second order derivatives for network pruning: Optimal brainsurgeon,1993, Morgan Kaufmann
 Deep residual learning for image recog-nition,2016, In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition(CVPR)
 Identity mappings in deep residualnetworks,2016, In Bastian Leibe
 Gpipe: Efficient training of giantneural networks using pipeline parallelism,2018, arXiv preprint arXiv:1811
 Decoupling representation and classifier for long-tailed recognition,2020, In EighthInternational Conference on Learning Representations (ICLR)
 Revealing the dark secretsof bert,2019, arXiv preprint arXiv:1908
 Learning multiple layers of features from tiny images,2009, Technical report
 Optimalbrain damage,1989, In NIPs
 Pruning and quantization for deep neuralnetwork acceleration: A survey,2021, arXiv preprint arXiv:2101
 FastBERT: a self-distilling BERT with adaptive inference time,2020, In Proceedings of the 58th Annual Meeting ofthe Association for Computational Linguistics
 RoBERTa: A robustly optimized bert pretrainingapproach,2019, arXiv preprint arXiv:1907
 Long-tail learning via logit adjustment,2021, In International Conference on LearningRepresentations
 Improved knowledge distillation via teacher assistant,2020, In Proceedings of the AAAIConference on Artificial Intelligence
 Skeletonization: A technique for trimming the fat from anetwork via relevance assessment,1988, In Proceedings of the 1st International Conference on NeuralInformation Processing Systems
 When does label smoothing help?In H,2019, Wallach
 On the calibration ofmulticlass classification with rejection,2019, In H
 Exploring the limits of transfer learning with a unified text-to-texttransformer,2019, arXiv preprint arXiv:1910
 Fixed encoder self-attention patterns intransformer-based machine translation,2020, arXiv preprint arXiv:2002
 Consistent algorithms for multiclassclassification with an abstain option,2018, Electron
 Fitnets: Hints for thin deep nets,2014, arXiv preprint arXiv:1412
 Neural network-based face detection,1998, IEEE Transactionson Pattern Analysis and Machine Intelligence
 Policy distil-lation,2016, In 4th International Conference on Learning Representations
 Revisiting unreasonable effectiveness of data in deeplearning era,2017, In 2017 IEEE International Conference on Computer Vision (ICCV)
 Rethink-ing the inception architecture for computer vision,2016, In Proceedings of the IEEE Conference onComputer Vision and Pattern Recognition (CVPR)
 EfficientNet: Rethinking model scaling for convolutional neural net-works,2019, In Kamalika Chaudhuri and Ruslan Salakhutdinov (eds
 Rapid object detection using a boosted cascade of simple features,2001, InProceedings of the 2001 IEEE Computer Society Conference on Computer Vision and PatternRecognition
 Skipnet: Learning dynamicrouting in convolutional networks,2018, In Proceedings of the European Conference on ComputerVision (ECCV)
 Learning to model the tail,2017, In Proceedingsof the 31st International Conference on Neural Information Processing Systems
 Deep learning in the era of edge computing: Challenges and opportunities,2020, Fog Computing:Theory and Practice
 CD-I denote the class-specific distillation approaches defined in Sec,1000, 4
