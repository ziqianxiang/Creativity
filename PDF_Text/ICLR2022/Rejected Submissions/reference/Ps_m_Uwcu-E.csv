title,year,conference
 Optimal gradient Com-pression for distributed and federated learning,2020, arXiv preprint arXiv:2010
 The convergence of sparsified gradient methods,2018, arXiv preprint arXiv:1809
 Freezeout: Accelerate trainingby progressively freezing layers,2017, arXiv preprint arXiv:1706
 Leaf: A benchmark for federated settings,2018, arXivpreprint arXiv:1812
 Emnist: Extending mnistto handwritten letters,2017, In 2017 International Joint Conference on Neural Networks (IJCNN)
 Heterofl: Computation and communication efficientfederated learning for heterogeneous clients,2020, arXiv preprint arXiv:2010
 Layerout: Freezinglayers in deep neural networks,2020, SN Computer Science
 Measuring statistical de-pendence with hilbert-schmidt norms,2005, In International conference on algorithmic learning theory
 Lo-cal sgd with periodic averaging: Tighter analysis and adaptive synchronization,2019, arXiv preprintarXiv:1910
 Fedml: A research library and benchmarkfor federated machine learning,2020, arXiv preprint arXiv:2007
 Scaffold: Stochastic controlled averaging for federated learning,2020, InInternational Conference on Machine Learning
 Federated learning: Strategies for improving communication efficiency,2016, arXivpreprint arXiv:1610
 Similarity of neuralnetwork representations revisited,2019, In International Conference on Machine Learning
 Insights on representational similarity in neuralnetworks with canonical correlation,2018, arXiv preprint arXiv:1806
 Svcca: Singular vectorcanonical correlation analysis for deep learning dynamics and interpretability,2017, arXiv preprintarXiv:1706
 Practical low-rank communication com-pression in decentralized deep learning,2020, In NeurIPS
 Atomo: Communication-efficient learning via atomic sparsification,2018, arXiv preprintarXiv:1806
 Pufferfish: Communication-efficientmodels at no extra cost,2021, arXiv preprint arXiv:2103
 Adaptive communication strategies to achieve the best error-runtimetrade-off in local-update sgd,2018, arXiv preprint arXiv:1810
 Tackling the objective in-consistency problem in heterogeneous federated optimization,2020, arXiv preprint arXiv:2007
 Gradient sparsification for communication-efficient distributed optimization,2017, arXiv preprint arXiv:1710
 Tern-grad: Ternary gradients to reduce communication in distributed deep learning,2017, arXiv preprintarXiv:1705
 Large batch optimization for deeplearning: Training bert in 76 minutes,2019, arXiv preprint arXiv:1904
 Wide residual networks,2016, arXiv preprintarXiv:1605
 Accelerating training of transformer-based language models withprogressive layer dropping,2020, arXiv preprint arXiv:2010
3 Proof of Other LemmasLemma A,2022,1
