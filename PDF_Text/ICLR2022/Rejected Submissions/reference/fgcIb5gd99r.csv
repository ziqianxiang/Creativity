title,year,conference
 Improving relation extraction by pre-trainedlanguage representations,2019, In Proceedings of AKBC 2019
 Multi-level structured self-attentions fordistantly supervised relation extraction,2018, In Proceedings of the 2018 Conference on EmpiricalMethods in Natural Language Processing(EMNLP)
 Fewrel: Alarge-scale supervised few-shot relation classification dataset with state-of-the-art evaluation,2018, InProceedings of EMNLP
 Generalizing natural language analysisthrough span-relation representations,2019, In Proceedings of ACL2020
 Self attention mechanism of bidirectional information en-hancement,2021, Applied Intelligence
 Neural relation extrac-tion with selective attention over instances,2016, In Proceedings of the 54th Annual Meeting of theAssociation for Computational Linguistics (Volume 1: Long Papers)
 Matching theblanks: Distributional similarity for relation learning,2019, In Proceedings of ACL2019
 Charformer: Fast character transformers viagradient-based subword tokenization,2021, arXiv preprint arXiv:2106
 Attention is all you need,2017, Advances in Neural Information Processing Systems 30
 Nystromformer: A nystrom-based algorithm for approximating self-attention,2021, 2021
 Docred: Alarge-scale document-level relation extraction dataset,2019, ACL2019
 Relation classification viaconvolutional deep neural network,2014, In Proceedings of COLING 2014
 Attention-basedbidirectional long short-term memory networks for relation classification,2016, In Proceedings of the54th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)
 Autorc: Improving bert based relation classifica-tion models via architecture search,2020, arXiv preprint arXiv:2009
