title,year,conference
 A generalized lot-tery ticket hypothesis,2021, arXiv preprint arXiv:2107
 The generalization-stability tradeoff in neural network pruning,2019, arXiv preprint arXiv:1906
 Language models arefew-shot learners,2020, arXiv preprint arXiv:2005
 Sc-conv: Sparse-complementaryconvolution for efficient model utilization on cnns,2018, In 2018 IEEE International Symposium onMultimedia (ISM)
 The lottery tickets hypothesis for supervised and self-supervised pre-trainingin computer vision models,2020, arXiv preprint arXiv:2012
 The lottery ticket hypothesis for pre-trained bert networks,2020, arXiv preprintarXiv:2007
 Long live the lottery:The existence of winning tickets in lifelong learning,2021, In International Conference on LearningRepresentations
 Earlybert:Efficient bert training via early-bird lottery tickets,2020, arXiv preprint arXiv:2101
 Escort: Efficient sparse convolutional neural networks on gpus,2018, arXiv preprintarXiv:1802
 cudnn: Efficient primitives for deep learning,2014, arXiv preprintarXiv:1410
 Imagenet: A large-scale hi-erarchical image database,2009, In 2009 IEEE conference on computer vision and pattern recognition
 Gradient descent provably optimizesover-parameterized neural networks,2018, arXiv preprint arXiv:1810
 The difficulty of training sparseneural networks,2019, arXiv preprint arXiv:1906
 The early phase of neural networktraining,2020, In International Conference on Learning Representations
 The state of sparsity in deep neural networks,2019, arXivpreprint arXiv:1902
 Eie: efficient inference engine on compressed deep neural network,2016, In ISCA
 Comparing biases for minimal network construction with back-propagation,1988, Advances in neural information processing systems
 Sparsity indeep learning: Pruning and growth for efficient inference and training in neural networks,2021, arXivpreprint arXiv:2102
 Efficient sparse-matrix multi-vector product on gpus,2018, Association for Computing Machinery
 Adaptivesparse tiling for sparse matrix multiplication,2019, In Proceedings of the 24th Symposium on Principlesand Practice of Parallel Programming
 Mobilenets: Efficient convolutional neural netWorks formobile vision applications,2017, arXiv preprint arXiv:1704
 NetWork trimming: A data-drivenneuron pruning approach toWards efficient deep architectures,2016, arXiv preprint arXiv:1607
 A novel data transformation and execution strat-egy for accelerating sparse matrix multiplication on gpus,2020, PPoPP â€™20
 Winning lottery tickets in deep generativemodels,2020, In Proceedings of the AAAI Conference on Artificial Intelligence
 Scaling laws for neural languagemodels,2020, arXiv preprint arXiv:2001
 Tiny imagenet visual recognition challenge,2015, CS 231N
 Optimal brain damage,1990, In Advances in neuralinformation processing Systems
 Snip: Single-shot network pruning basedon connection sensitivity,2019, In International Conference on Learning Representations (ICLR)
 Snip: Single-shot network pruning basedon connection sensitivity,2019, In International Conference on Learning Representations
 Pruning filters forefficient convnets,2016, arXiv preprint arXiv:1608
 Learn-ing efficient convolutional networks through network slimming,2017, In Proceedings of the IEEEInternational Conference on Computer Vision
 Rethinking the value ofnetwork pruning,2019, In 7th International Conference on Learning Representations
 Learning sparse neural networks throughl_0 regularization,2017, arXiv preprint arXiv:1712
 Goodstudents play big lottery better,2021, arXiv preprint arXiv:2101
 Importance estimationfor neural network pruning,2019, In Proceedings of the IEEE Conference on Computer Vision andPattern Recognition
 Adaptive runtime exploiting spar-sity in tensor of deep learning neural network on heterogeneous systems,2017, In 2017 InternationalConference on Embedded Computer Systems: Architectures
 Comparing rewinding and fine-tuning in neuralnetwork pruning,2020, In 8th International Conference on Learning Representations
 An overview of gradient descent optimization algorithms,2016, arXiv preprintarXiv:1609
 Very deep convolutional networks for large-scale imagerecognition,2014, arXiv preprint arXiv:1409
 Picking winning tickets before training bypreserving gradient flow,2020, In International Conference on Learning Representations
 Drawing early-bird tickets: Toward more efficient training ofdeep networks,2020, In International Conference on Learning Representations
 Playing the lottery with rewardsand multiple languages: lottery tickets in rl and nlp,2020, In 8th International Conference on LearningRepresentations
 Wide residual networks,2016, arXiv preprintarXiv:1605
