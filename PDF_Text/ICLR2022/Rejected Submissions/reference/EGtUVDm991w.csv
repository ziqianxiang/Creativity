title,year,conference
 Parametric correspon-dence and chamfer matching: Two new techniques for image matching,1977, Technical report
 Spatially adaptivesubsampling of image sequences,1994, Transactions on Image Processing
 Generating long sequences with sparsetransformers,2019, arXiv preprint arXiv:1904
 Rethinking attentionwith performers,2021, International Conference on Learning Representations (ICLR)
 ImageNet: A large-scalehierarchical image database,2009, In IEEE Conference on Computer Vision and Pattern Recognition(CVPR)
 BERT: Pre-training of deepbidirectional transformers for language understanding,2019, In Proceedings of the 2019 Conference ofthe North American Chapter of the Association for Computational Linguistics: Human LanguageTechnologies
 The state of sparsity in deep neural networks,2019, arXivpreprint arXiv:1902
 PoWER-BERT: Accelerating BERT inference via progressive word-vector elimination,2020, In International Conference on Machine Learning (ICML)
 Rethinking spatial dimensions of vision transformers,2021, In IEEE International Conference onComputer Vision (ICCV)
 MobileNets: Efficient convolutional neural networks formobile vision applications,2017, arXiv preprint arXiv:1704
 Highperformance natural language processing,2020, In Proceedings of the 2020 Conference on EmpiricalMethods in Natural Language Processing: Tutorial Abstracts
 Quantization and training of neural networks forefficient integer-arithmetic-only inference,2018, In IEEE Conference on Computer Vision and PatternRecognition (CVPR)
 Transformers areRNNs: Fast autoregressive transformers with linear attention,2020, In International Conference onMachine Learning (ICML)
 Reformer: The efficient transformer,2020, InInternational Conference on Learning Representations (ICLR)
 Optimal brain damage,1990, In Advances in NeuralInformation Processing Systems (NeurIPS)
 Generating wikipedia by summarizing long sequences,2018, In International Conference onLearning Representations (ICLR)
 Maintaining natural imagestatistics with the contextual loss,2018, In Asian Conference on Computer Vision (ACCV)
 Scalable visual transformers withhierarchical pooling,2021, IEEE International Conference on Computer Vision (ICCV)
 ENet: A deep neu-ral network architecture for real-time semantic segmentation,2016, arXiv preprint arXiv:1606
 Blockwise self-attention for long document understanding,2020, In Proceedings of the 2020 Conference on EmpiricalMethods in Natural Language Processing: Findings
 Improv-ing language understanding by generative pre-training,2018, 2018
 DynamicViT:Efficient vision transformers with dynamic token sparsification,2021, ArXiv
 Learn-ing to zoom: a saliency-based sampling layer for neural networks,2018, In European Conference onComputer Vision (ECCV)
 Communication in the presence of noise,1949, Proceedings of the IRE
 Normalized cuts and image segmentation,2000, IEEE Transactions onPattern Analysis and Machine Intelligence (PAMI)
 EfficientNet: Rethinking model scaling for convolutional neural net-works,2019, In International Conference on Machine Learning (ICML)
 MnasNet: Platform-aware neural architecture search for mobile,2019, In IEEE Conferenceon Computer Vision and Pattern Recognition (CVPR)
 Sparse sinkhorn attention,2020, InInternational Conference on Machine Learning (ICML)
 Efficient transformers: A survey,2020, arXivpreprint arXiv:2009
 Synthesizer:Rethinking self-attention in transformer models,2021, International Conference on Machine Learning(ICML)
 An adaptive sub-sampling method for in-memory compression of scientific data,2009, In Data Compression Conference
 Quick shift and kernel methods for mode seeking,2008, In EuropeanConference on Computer Vision (ECCV)
 Fast transformers with clustered atten-tion,2020, Advances in Neural Information Processing Systems (NeurIPS)
 Linformer: Self-attention withlinear complexity,2020, arXiv preprint arXiv:2006
 Pyramid vision transformer: A versatile backbone for dense prediction with-out convolutions,2021, In IEEE International Conference on Computer Vision (ICCV)
 FBNet: Hardware-aware efficient ConvNet designvia differentiable neural architecture search,2019, In IEEE Conference on Computer Vision and PatternRecognition (CVPR)
 Visual transformers: Token-basedimage representation and processing for computer vision,2020, arXiv preprint arXiv:2006
 Centroid transformers: Learning to abstract with atten-tion,2021, arXiv preprint arXiv:2102
 Big bird: Transformers for longersequences,2020, Advances in Neural Information Processing Systems (NeurIPS)
 ICNet for real-timesemantic segmentation on high-resolution images,2018, In European Conference on Computer Vision(ECCV)
 Rethinking semantic segmentation froma sequence-to-sequence perspective with transformers,2021, In IEEE Conference on Computer Visionand Pattern Recognition (CVPR)
 Our training protocal usesthe same hyper-parameters provided by Touvron et al,2021, (2021)
