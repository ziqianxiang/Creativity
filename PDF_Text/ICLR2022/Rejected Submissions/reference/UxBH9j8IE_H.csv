title,year,conference
 Successfully applying the stabilized lotteryticket hypothesis to the transformer architecture,2020, arXiv preprint arXiv:2005
 Laplacians of graphs and Cheeger's inequalities,1996, In Combinatorics
 Learning to prune deep neural networks via layer-wise optimal brain surgeon,2017, arXiv preprint arXiv:1705
 Gradient flow in sparse neural net-works and how lottery tickets win,2020, arXiv preprint arXiv:2010
 The lottery ticket hypothesis: Training pruned neural net-works,2019, In International Conference on Learning Representations (ICLR)
 Stabilizing thelottery ticket hypothesis,2019, arXiv preprint arXiv:1903
 The lotteryticket hypothesis at scale,2019, CoRR
 Optimal brain surgeon and general networkpruning,1993, In IEEE international conference on neural networks
 Expander graphs and their ap-plications,0273, Bull
 Optimal brain damage,1990, In Advances in neuralinformation processing systems
 Snip: Single-shot network pruningbased on connection sensitivity,2018, arXiv preprint arXiv:1810
 A signal propagationperspective for pruning neural networks at initialization,2019, arXiv preprint arXiv:1906
 ARPACK usersâ€™ guide: solution of large-scale eigenvalue problems with implicitly restarted Arnoldi methods,1998, SIAM
 Analyzing the noise robustnessof deep neural networks,2018, CoRR
 Proving the lottery tickethypothesis: Pruning is all you need,2020, arXiv preprint arXiv:2002
 Interlacing families I: BipartiteRamanujan graphs of all degrees,0003, Ann
 Scalable training of artificial neural networks with adaptive sparse connec-tivity inspired by network science,2018, Nature communications
 To-wards understanding the role of over-parametrization in generalization of neural networks,2018, arXivpreprint arXiv:1805
 On the second eigenvalue of a graph,1991, Discrete Math
 Comparing rewinding and fine-tuning in neuralnetwork pruning,2020, arXiv preprint arXiv:2003
 Pruning neural networkswithout any data by iteratively conserving synaptic flow,2020, arXiv preprint arXiv:2006
 Picking winning tickets before training bypreserving gradient flow,2020, arXiv preprint arXiv:2002
 Drawing early-bird tickets: Towards more efficient trainingof deep networks,2019, arXiv preprint arXiv:1909
