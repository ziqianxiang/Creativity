title,year,conference
 Large scale distributed neural network training through online distillation,2018, In InternationalConference on Learning Representations
 A systematic study of the class imbalanceproblem in convolutional neural networks,2017, arXiv:1710
 Learning imbalanceddatasets with label-distribution-aware margin loss,2019, In Advances in Neural Information ProcessingSystems 32
 Leverag-ing labeled and unlabeled data for consistent fair binary classification,2019, In H
 Class-balanced loss based oneffective number of samples,2019, In CVPR
 Knowledge distillation assemiparametric inference,2021, In International Conference on Learning Representations
 Fairness throughawareness,2012, In Innovations in Theoretical Computer Science Conference (ITCS)
 Born-again neural networks,2018, In Proceedings of the 35th International Conference on MachineLearning
 On calibration of modern neuralnetworks,2017, In Proceedings of the 34th International Conference on Machine Learning
 Equality of opportunity in supervised learning,2016, InAdvances in Neural Information Processing Systems (NIPS)
 Deep residual learning for imagerecognition,2016, In Proceedings of the IEEE conference on computer vision and pattern recognition
 Selective brain damage:Measuring the disparate impact of model pruning,2019, CoRR
 Selective classificationcan magnify disparities across groups,2021, In International Conference on Learning Representations
 Decoupling representation and classifier for long-tailed recognition,2020, In EighthInternational Conference on Learning Representations (ICLR)
 Empirical margin distributions and bounding the generalizationerror of combined classifiers,2002, Ann
 Large-scalelong-tailed recognition in an open world,2019, In IEEE Conference on Computer Vision and PatternRecognition
 Unifying distillation and privileged informa-tion,2016, In International Conference on Learning Representations (ICLR)
 On thestatistical consistency of algorithms for binary classification under class imbalance,2013, In Proceedingsof the 30th International Conference on Machine Learning
 Deepdouble descent: Where bigger models and more data hurt,2020, In International Conference on LearningRepresentations
 What is being transferred in trans-fer learning? In H,2020, Larochelle
 Data distillation:Towards omni-supervised learning,2018, In 2018 IEEE Conference on Computer Vision and PatternRecognition
 Distributionally robust neural networks forgroup shifts: On the importance of regularization for worst-case generalization,2020, In InternationalConference on Learning Representations (ICLR)
 An investigation of why overparameterizationexacerbates spurious correlations,2020, In International Conference on Machine Learning (ICML)
 In S,2018, Bengio
 No subclass left behind: Fine-grainedrobustness in coarse-grained classification problems,2020, In To appear in Conference on NeuralInformation Processing Systems (NeurIPS)
 Seesaw loss for long-tailed instance segmentation,2021, InProceedings of the IEEE Conference on Computer Vision and Pattern Recognition
 Fairness risk measures,2019, In Proceedings of the36th International Conference on Machine Learning
 Understandingdeep learning requires rethinking generalization,2017, In 5th International Conference on LearningRepresentations
 Prime-aware adaptive distillation,2020, In Andrea Vedaldi
 Self-distillation as instance-specific label smoothing,2020, In HugoLarochelle
 Channel distillation: Channel-wiseattention for knowledge distillation,2020, CoRR
