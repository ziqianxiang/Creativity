title,year,conference
 Generating natural language adversarial examples,2018, arXiv preprint arXiv:1804
 Blind backdoors in deep learning models,2020, arXivpreprint arXiv:2005
 On the opportu-nities and risks of foundation models,2021, arXiv preprint arXiv:2108
 Large-scale machine learning with stochastic gradient descent,2010, In Proceedings ofCOMPSTAT
 Language models arefew-shot learners,2020, arXiv preprint arXiv:2005
 Detecting backdoor attacks on deep neural networks byactivation clustering,2018, arXiv preprint arXiv:1811
 Deepinspect: A black-box trojandetection and mitigation framework for deep neural networks,2019, In IJCAI
 Sentinet: Detecting physicalattacks against deep learning systems,2018, 2018
 Bert: Pre-training of deepbidirectional transformers for language understanding,2019, In Proceedings of the 2019 Conference ofthe North American Chapter of the Association for Computational Linguistics: Human LanguageTechnologies
 Bae: Bert-based adversarial examples for text classifi-cation,2020, arXiv preprint arXiv:2004
 Badnets: Identifying vulnerabilities in themachine learning model supply chain,2017, arXiv preprint arXiv:1708
 Ptr: Prompt tuning with rulesfor text classification,2021, arXiv preprint arXiv:2105
 Knowledge-able prompt-tuning: Incorporating knowledge into prompt verbalizer for text classification,2021, arXivpreprint arXiv:2108
 Adversarial example generationwith syntactically controlled paraphrase networks,2018, arXiv preprint arXiv:1804
 Is bert really robust? a strong baselinefor natural language attack on text classification and entailment,2020, In Proceedings of the AAAIconference on artificial intelligence
 Bert-attack: Adversarialattack against bert using bert,2020, arXiv preprint arXiv:2004
 Neural attention distil-lation: Erasing backdoor triggers from deep neural networks,2021, arXiv preprint arXiv:2101
 Trojaning attack on neural networks,2017, 2017
 Collective opinion spam detection: Bridging review networksand metadata,2015, In Proceedings of the 21th acm sigkdd international conference on knowledgediscovery and data mining
 Semantically equivalent adversarial rulesfor debugging nlp models,2018, In Proceedings of the 56th Annual Meeting of the Association forComputational Linguistics (Volume 1: Long Papers)
 Spectral signatures in backdoor attacks,2018, arXivpreprint arXiv:1811
 Neural cleanse: Identifying and mitigating backdoor attacks in neural networks,2019, In 2019IEEE Symposium on Security and Privacy (SP)
 Hateful symbols or hateful people? predictive features for hatespeech detection on twitter,2016, In Proceedings of the NAACL student research workshop
 Transformers: State-of-the-artnatural language processing,2020, In Proceedings of the 2020 Conference on Empirical Methods inNatural Language Processing: System Demonstrations
 Word-level textual adversarial attacking as combinatorial optimization,2019, arXiv preprintarXiv:1910
 Red alarm for pre-trained models: Universal vulnerabilities byneuron-level backdoor attacks,2021, arXiv preprint arXiv:2101
 Generating natural adversarial examples,2017, arXivpreprint arXiv:1710
 Aligning books and movies: Towards story-like visual explanations by watchingmovies and reading books,2015, In Proceedings of the IEEE international conference on computervision
