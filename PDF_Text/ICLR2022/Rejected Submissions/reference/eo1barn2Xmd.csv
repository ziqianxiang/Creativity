title,year,conference
 Distributed second-order optimization using kronecker-factored approximations,2016, 2016
 An accelerated linearly convergent stochasticl-bfgs algorithm,2019, IEEE transactions on neural networks and learning systems
 Imagenet: A large-scalehierarchical image database,2009, In 2009 IEEE conference on computer vision and pattern recognition
 Bert: Pre-training of deepbidirectional transformers for language understanding,2018, arXiv preprint arXiv:1810
 Animage is worth 16x16 words: Transformers for image recognition at scale,2020, arXiv preprintarXiv:2010
 Animage is worth 16x16 words: Transformers for image recognition at scale,2020, arXiv preprintarXiv:2010
 Quasi-newton methods: superlinear convergence without linesearches for self-concordant functions,2019, Optimization Methods and Software
 Practical quasi-newton methods for training deepneural networks,2020, arXiv preprint arXiv:2006
 Stochastic block bfgs: Squeezing morecurvature out of data,2016, In International Conference on Machine Learning
 Deep residual learning for imagerecognition,2016, In Proceedings of the IEEE conference on computer vision and pattern recognition
 Adam: A method for stochastic optimization,2014, arXiv preprintarXiv:1412
 Learning multiple layers of features from tiny images,2009, 2009
 Optimizing neural networks with kronecker-factored approximatecurvature,2015, In International conference on machine learning
 A linearly-convergent stochastic l-bfgsalgorithm,2016, In Artificial Intelligence and Statistics
 Updating quasi-newton matrices with limited storage,1980, Mathematics of computation
 Convolutional NeuralNetwork Training with Distributed K-FAC,2020, In Proceedings of the International Conference forHigh Performance Computing
 Gradient methods for minimizing functionals,1963, Zhurnal vychislitelâ€™noimatematiki i matematicheskoi fiziki
 Improving language under-standing by generative pre-training,2018, 2018
 Zero: Memory optimizationstoward training trillion parameter models,2020, In SC20: International Conference for High PerformanceComputing
 Online algorithms and stochastic approximations,1998, Online Learning
 On the origin of implicitregularization in stochastic gradient descent,2021, arXiv preprint arXiv:2101
