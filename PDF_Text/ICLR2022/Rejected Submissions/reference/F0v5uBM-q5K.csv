title,year,conference
 Variational networkquantization,2018, In International Conference on Learning Representations
 Deepshift: Towardsmultiplication-less neural networks,2019, arXiv preprint arXiv:1905
 Learned step size quantization,2019, In International Conference on LearningRepresentations
 Post-training piecewise linear quantization for deep neural networks,2020, In EuropeanConference on Computer Vision
 Deep learning withlimited numerical precision,2015, In International Conference on Machine Learning
 Deep residual learning for imagerecognition,2016, In Proceedings of the IEEE conference on computer vision and pattern recognition
 Energy table for 45nm process,2014, In Stanford VLSI wiki
 Efficient systolic array based on decompos-able mac for quantized deep neural networks,2019, 2019
 Improving posttraining neural quantization: Layer-wise calibration and integer programming,2020, arXiv preprintarXiv:2006
 Quantization and training of neural networks for efficientinteger-arithmetic-only inference,2018, In Proceedings of the IEEE Conference on Computer Vision andPattern Recognition
 A study of bfloat16 for deep learning training,2019, arXiv preprint arXiv:1905
 Fast algorithms for convolutional neural networks,2016, In Proceedings ofthe IEEE Conference on Computer Vision and Pattern Recognition
 Brecq: Pushing the limit of post-training quantization by block reconstruction,2021, In InternationalConference on Learning Representations
 Neural networks withfew multiplications,2015, arXiv preprint arXiv:1510
 Post-training quantization with multiplepoints: Mixed precision without mixed precision,2021, In Proceedings of the AAAI Conference onArtificial Intelligence
 Wrpn: Wide reduced-precisionnetworks,2017, arXiv preprint arXiv:1709
 Up ordown? adaptive rounding for post-training quantization,2020, arXiv preprint arXiv:2004
 Loss aware post-training quantization,2019, arXiv preprint arXiv:1911
 Wrapnet: Neural net inference with ultra-low-precision arithmetic,2021, In 9th InternationalConference on Learning Representations (ICLR 2021)
 Mo-bilenetv2: Inverted residuals and linear bottlenecks,2018, In Proceedings of the IEEE conference oncomputer vision and pattern recognition
 Strassennets: Deep learning with amultiplication budget,2018, In International Conference on Machine Learning
 Shiftaddnet: A hardware-inspired deep network,2020, Advances in Neural InformationProcessing Systems
