title,year,conference
 A convergence theory for deep learning via over-parameterization,2019, In Kamalika Chaudhuri and Ruslan Salakhutdinov (eds
 On the optimization of deep networks: Implicitacceleration by overparameterization,2018, In Jennifer Dy and Andreas KraUse (eds
 SGD learns over-parameterized networks that provably generalize on linearly separable data,2018, In InternationalConference on Learning Representations
 Saga: A fast incremental gradient methodwith support for non-strongly convex composite objectives,2014, In Advances in Neural InformationProcessing Systems
 Gradient descent finds globalminima of deep neural networks,2019, In Kamalika Chaudhuri and Ruslan Salakhutdinov (eds
 Gradient descent provably optimizesover-parameterized neural networks,2019, In International Conference on Learning Representations
 Accelerating stochastic gradient descent using predictive variancereduction,2013, In Advances in Neural Information Processing Systems
 Linear convergence of gradient and proximal-gradient methods under the PoIyakÂ±OjasieWicz condition,2016, In Paolo Frasconi
 A stochastic gradient method with an exponen-tial convergence rate for finite training sets,2012, In NIPS
 Robust stochastic approximation approach tostochastic programming,2009, SIAM J
 Stochastic variancereduction for nonconvex optimization,2016, In Maria Florina Balcan and Kilian Q
 On the convergence of adam and beyond,2018, InInternational Conference on Learning Representations
 Pegasos: Primal estimated sub-gradientsolver for svm,2007, Association for Computing Machinery
 The im-plicit bias of gradient descent on separable data,1532, J
 Smg: A shuffling gradient-based methodwith momentum,2021, In Marina Meila and Tong Zhang (eds
 Stochastic Gauss-Newton algorithms for noncon-vex compositional optimization,2020, In Hal DaUme In and Aarti Singh (eds
 A stochastic composite gradient method with incremental vari-ance reduction,2019, In H
 An improved analysis of training over-parameterized deepneural networks,2019, In Hanna M
