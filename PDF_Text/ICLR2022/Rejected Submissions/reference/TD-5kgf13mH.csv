title,year,conference
 Depth uncertaintyin neural networks,2020, In Advances in Neural Information Processing Systems
 Estimating or propagating gradientsthroUgh stochastic neUrons for conditional compUtation,2013, arxiv preprint arxiv:1308
 Language models arefew-shot learners,2020, arxiv preprint arxiv:2005
 Improved learning algorithms for mixture of experts in multi-class classification,1999, Neural Networks
 Describing textures in the wild,2014, InProceedings of the IEEE Conf
 ImageNet: A large-scalehierarchical image database,2009, In CVPR
 Bert: Pre-training of deepbidirectional transformers for language understanding,2018, arxiv preprint arxiv:1810
 An image is worth 16x16 words: Transformers for image recognition atscale,2021, In ICLR
 Efficient and scalable bayesian neural nets with rank-1 factors,2020, In International conference on machine learning
 Learning factored representations in a deepmixture of experts,2014, In ICLR (Workshop Poster)
 Switch transformers: Scaling to trillion parametermodels with simple and efficient sparsity,2021, arxiv preprint arxiv:2101
 Exploring the limits of out-of-distributiondetection,2021, arxiv
 Dropout as a bayesian approximation: Representing modeluncertainty in deep learning,2016, In International conference on machine learning
 On calibration of modern neuralnetworks,2017, In International Conference on Machine Learning
 Neural network ensembles,1990, IEEE transactions on patternanalysis and machine intelligence
 Training independent subnetworks for robustprediction,2020, In ICLR
 Benchmarking neural network robustness to common cor-ruptions and perturbations,2019, arXiv preprint arXiv:1903
 Natural adversarialexamples,2019, arXiv preprint arXiv:1907
 Distilling the knowledge in a neural network,2015, NIPSDeep Learning and Representation Learning Workshop
 Multi-class texture analysis in colorectalcancer histology,2016, Scientific reports
 Learning multiple layers of features from tiny images,2009, Technical report
 Simple and scalable predictiveuncertainty estimation using deep ensembles,2017, In Advances in Neural Information ProcessingSystems
 Whym heads are better than one: Training a diverse ensemble of deep networks,2015, arxiv preprintarxiv:1511
 GShard: Scaling giant models with conditionalcomputation and automatic sharding,2021, In ICLR
 Towards fully autonomous driving: Sys-tems and algorithms,2011, In 2011 IEEE Intelligent Vehicles Symposium (IV)
 Sparse-mlp: A fully-mlp architecturewith conditional computation,2021, arxiv
 Revisiting the calibration of modern neural networks,2021, arxiv
 Deep ensembles for low-data transfer learning,2020, arXiv preprint arXiv:2010
 Readingdigits in natural images with unsupervised feature learning,2011, 2011
 Cats and dogs,2012, In 2012IEEE conference on computer vision and pattern recognition
 Carbon emissions and large neural network training,2021, arxivpreprint arxiv:2104
 Vision transformers are robust learners,2021, arxiv
 Learning transferable visualmodels from natural language supervision,2021, arxiv preprint arxiv:2103
 Mixmo: Mixing multiple inputs for multipleoutputs via deep subnetworks,2021, arXiv preprint arXiv:2103
 Outrageously large neural networks: The sparsely-gated mixture-of-experts layer,2017, InICLR
 Aggregatedlearning: A vector-quantization approach to learning neural network classifiers,2020, In Proceedingsofthe AAAI Conference on Artificial Intelligence
 Energy and policy considerations for deeplearning in NLP,2019, arxiv preprint arxiv:1906
 Revisiting unreasonable ef-fectiveness of data in deep learning era,2017, In Proceedings of the IEEE international conference oncomputer vision
 Attention is all you need,2017, InNeurIPS
 Batchensemble: an alternative approach to efficientensemble and lifelong learning,2019, In ICLR
 Hyperparameter ensembles forrobustness and uncertainty quantification,2020, In Neural Information Processing Systems)
 Horizontal and vertical ensemble with deep representa-tion for classification,2013, arxiv
 Exploring sparse expert models and beyond,2021, arxiv preprintarxiv:2105
 Cyclicalstochastic gradient mcmc for bayesian deep learning,2019, In ICLR
 Places: A 10 mil-lion image database for scene recognition,2017, IEEE Transactions on Pattern Analysis and MachineIntelligence
 CIFAR 100			CIFAR10 vs,2022, DTD			CIFAR10 vs
 CIFAR10			CIFAR100 vs,2022, DTD			CIFAR100 VS
04 Â± 0,2022,32	19
