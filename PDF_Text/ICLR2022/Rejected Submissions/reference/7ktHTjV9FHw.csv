title,year,conference
 Cormorant: Covariant molecular neuralnetworks,2019, arXiv preprint arXiv:1906
 On identifiability in transformers,2019, arXiv preprint arXiv:1908
 Advancingdrug discovery via artificial intelligence,0165, Trends in Pharmacological Sciences
 Electra: Pre-training textencoders as discriminators rather than generators,2020, arXiv preprint arXiv:2003
 Convolutional netWorks on graphs for learning molecularfingerprints,2015, In Advances in Neural Information Processing Systems 28
 Generalizing convolutionalneural netWorks for equivariance to lie groups on arbitrary continuous data,2020, In InternationalConference on Machine Learning
 Neuralmessage passing for quantum chemistry,2017, In Proceedings of the 34th International Conference onMachine Learning
 Fine-tuned language models for text classification,2018, 2018
 Improve transformer models with betterrelative position embeddings,2020, 2020
 Generative models forgraph-based protein design,2021, 2021
 Rethinking positional encoding in language pre-training,2021, InInternational Conference on Learning Representations
 Directional message passing for moleculargraphs,2020, In International Conference on Learning Representations
 Comparison of deeplearning With multiple machine learning methods and metrics using diverse drug discovery datasets,2017, Molecular Pharmaceutics
 A structured self-attentive sentence embedding,2017, 2017
 Experimentaland computational approaches to estimate solubility and permeability in drug discovery anddevelopment settings,1997, Advanced drug delivery reviews
 Roberta: A robustly optimized bert pretrainingapproach,2019, 2019
 Large-scale comparison of machine learningmethods for drug target prediction on chembl,2018, Chem
 Molecule attention transformer,2020, 2020
 Relevance of rotationallyequivariant convolutions for predicting molecular properties,2020, arXiv preprint arXiv:2008
 Unsupervised universal self-attention netWorkfor graph classification,2019, CoRR
 Comparison of atom representations ingraph neural netWorks for molecular property prediction,2020, 2020
 Quantumchemistry structures and properties of 134 kilo molecules,2014, Scientific data
 Group equivariant stand-alone self-attentionfor vision,2021, In International Conference on Learning Representations
 Molecular transformer: A model for uncertainty-calibrated chemicalreaction prediction,2019, ACS central science
 Horovod: fast and easy distributed deep learning intensorfloW,2018, arXiv preprint arXiv:1802
 EdgeAttention-based Multi-Relational Graph Convolutional NetWorks,2018, 2018
 Attention is all you need,2017, 2017
 Superglue: A stickier benchmark for general-purpose languageunderstanding systems,2019, In Advances in Neural Information Processing Systems
 SMILES-BERT: Largescale unsupervised pre-training for molecular property prediction,2019, In Proceedings of the 10th ACMInternational Conference on Bioinformatics
 The resulting splits are different thanin the MAT benchmark,2020, For every dataset
