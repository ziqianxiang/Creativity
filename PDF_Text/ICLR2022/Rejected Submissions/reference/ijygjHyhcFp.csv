title,year,conference
 Federated learning based on dynamic regularization,2021, In InternationalConference on Learning Representations
 Distributed delayed stochastic optimization,2012, In 2012 IEEE 51stIEEE Conference on Decision and Control(CDC)
 Federated learning under arbitrary communicationpatterns,2021, In International Conference on Machine Learning
 Lag: Lazily aggregated gradient forcommunication-efficient distributed learning,2018, In NeurIPS
 Lasg: Lazily aggregated stochastic gradients forcommunication-efficient distributed learning,2020, arXiv preprint arXiv:2002
 On the ineffectiveness of variance reduced optimization for deeplearning,2018, arXiv preprint arXiv:1812
 Advancesand open problems in federated learning,2019, arXiv preprint arXiv:1912
 Mime: Mimicking centralized stochastic algorithms infederated learning,2020, arXiv preprint arXiv:2008
 Scaffold: Stochastic controlled averaging for federated learning,2020, InInternational Conference on Machine Learning
 First analysis of local gd on heteroge-neous data,2019, arXiv preprint arXiv:1909
 Federated optimization:Distributed machine learning for on-device intelligence,2016, arXiv preprint arXiv:1610
 Learning multiple layers of features from tiny images,2009, 2009
 A stochastic gradient method with anexponential convergence rate for finite training sets,2012, In NIPS
 Gradient-based learning applied todocument recognition,1998, Proceedings of the IEEE
 On the convergence offedavg on non-iid data,2019, arXiv preprint arXiv:1907
 Asynchronous decentralized parallel stochasticgradient descent,2018, In International Conference on Machine Learning
 Communication-efficientlearning of deep networks from decentralized data,2016, arXiv preprint arXiv:1602
 Gpu asynchronous stochasticgradient descent to speed up neural network training,2013, arXiv preprint arXiv:1312
 Adaptive federated optimization,2020, arXiv preprintarXiv:2003
 Towards flexible device partici-pation in federated learning,2021, In International Conference on Artificial Intelligence and Statistics
 Minimizing finite sums with the stochasticaverage gradient,2017, Mathematical Programming
 Local sgd converges fast and communicates little,2018, arXiv preprint arXiv:1805
 Cooperative sgd: A unified framework for the design and analysis ofcommunication-efficient sgd algorithms,2018, arXiv preprint arXiv:1808
 Tackling the objectiveinconsistency problem in heterogeneous federated optimization,2020, arXiv preprint arXiv:2007
 A field guide to federatedoptimization,2021, arXiv preprint arXiv:2107
 Asynchronous federated optimization,2019, arXiv preprintarXiv:1903
 Federated machine learning: Concept andapplications,2019, ACM Transactions on Intelligent Systems and Technology (TIST)
 On the linear speedup analysis of communication efficientmomentum sgd for distributed non-convex optimization,2019, In International Conference on MachineLearning
 Taming convergence for asynchronous stochastic gradientdescent with unbounded delay in non-convex learning,2020, In 2020 59th IEEE Conference on Decisionand Control (CDC)
 Fedpd: A federated learningframework with optimal rates and adaptivity to non-iid data,2020, arXiv preprint arXiv:2005
