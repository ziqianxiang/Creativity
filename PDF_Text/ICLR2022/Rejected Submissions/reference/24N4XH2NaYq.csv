title,year,conference
 Tabnet: Attentive interpretable tabular learning,2020, arXiv
 Xgboost: A scalable tree boosting system,2016, In Proceedings of the22nd acm Sigkdd international conference on knowledge discovery and data mining
 Simple modifications to improve tabular neural networks,2021, arXiv preprintarXiv:2108
 Greedy function approximation: a gradient boosting machine,2001, Annals ofstatistics
 Deep learning,2016, MIT press
 Revisiting deep learningmodels for tabular data,2021, arXiv preprint arXiv:2106
 The treeensemble layer: Differentiability meets conditional computation,2020, In International Conference onMachine Learning
 Regularization is all you need:Simple neural nets can excel on tabular data,2021, arXiv preprint arXiv:2106
 Net-dnf: Effective deep modeling of tabular data,2020, InInternational Conference on Learning Representations
 Lightgbm: A highly efficient gradient boosting decision tree,2017, Advances in neuralinformation processing systems
 Adam: A method for stochastic optimization,2014, arXiv preprintarXiv:1412
 From softmax to sparsemax: A sparse model of attentionand multi-label classification,1614, In International conference on machine learning
 Forwardthinking: Building deep random forests,2017, arXiv preprint arXiv:1705
 Sparse SeqUence-to-sequence models,2019, arXivpreprint arXiv:1905
 Neural oblivious decision ensembles fordeep learning on tabular data,2019, arXiv preprint arXiv:1909
 Catboost: unbiased boosting with categorical features,2017, arXiv preprint arXiv:1706
 Deep neural decision trees,2018, arXivpreprint arXiv:1806
 mixup: Beyond empiricalrisk minimization,2017, arXiv preprint arXiv:1710
 Deep forest,2017, arXiv preprint arXiv:1702
