title,year,conference
 Podnet: Pooledoutputs distillation for small-tasks incremental learning,2020, In ECCV
 Censoring representations with an adversary,2015, arXiv preprintarXiv:1511
 Making ai forget you: Datadeletion in machine learning,2019, arXiv preprint arXiv:1907
 Dynamic network surgery for efficient dnns,2016, Advancesin Neural Information Processing Systems
 Deep residual learning for imagerecognition,2016, In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition
 Soft filter pruning for accelerating deep convolutionalneural networks,2018, In IJCAI International Joint Conference on Artificial Intelligence
 Asymptotic softfilter pruning for deep convolutional neural networks,2019, IEEE transactions on cybernetics
 Meta filter pruning to accelerate deep convolutionalneural networks,2019, arXiv preprint arXiv:1904
 Distilling the knowledge in aneural network,2014, In NIPS Deep Learning and Representation Learning Workshop
 Learning a unified classifierincrementally via rebalancing,2019, In CVPR
 Fearnet: Brain-inspired model for incremental learning,2018, InInternational Conference on Learning Representations
 Learning multiple layers of features from tiny images,2009, 2009
 Pruning filters forefficient convnets,2016, arXiv preprint arXiv:1608
 The variational fairautoencoder,2015, arXiv preprint arXiv:1511
 icarl:Incremental classifier and representation learning,2017, In Proceedings of the IEEE Conference onComputer Vision and Pattern Recognition
 Grad-cam: Visual explanations from deep networks via gradient-based local-ization,2017, In Proceedings of the IEEE International Conference on Computer Vision
 Topology-preservingclass-incremental learning,2020, In ECCV
 Clip-q: Deep network compression learning by in-parallel pruning-quantization,2018, In Proceedings of the IEEE conference on computer vision and pattern recognition
