title,year,conference
 Fine-grained analysis ofoptimization and generalization for overparameterized two-layer neural networks,2019, In InternationalConference on Machine Learning
 Thelogical expressiveness of graph neural networks,2020, In ICLR
 Be more with less: Hypergraphattention networks for inductive text classification,2020, In EMNLP
 Weisfeiler-lehman refinement requires at least a linear number of iterations,2001, In ICALP
 Neuralmessage passing for quantum chemistry,2017, In ICML
 Language identification in the limit,1967, Inf
 On the computational complexity of algorithms,1965, TransactiOnsOf the American Mathematical SOciety
 Monotone circuits for connectivity require super-logarithmicdepth,1990, SIAM J
 Towards a complexity theory for the congested clique,2018, InSPAA
 Hyperbolic graph neural networks,2017, IEEE Transactionson Knowledge and Data Engineering
 What graph neural networks cannot learn: depth vs width,2020, In InternationalConference on Learning Representations
 Average-case complexity of detecting cliques,2010, PhD thesis
 Thegraph neural network model,2008, IEEE transactions on neural networks
 Modeling relational data with graph convolutional networks,2018, In ESWC
 Attention is all you need,2017, In NeurIPS
 Neural executionof graph algorithms,2020, In ICLR
 This is a direct conclusion by combining Lemma B,2021,2 and Lemma B
