title,year,conference
 An actor-critic algorithm for sequence prediction,2017, InternationalConference on Learning Representations
 Language models are few-shot learners,2020, arXiv preprintarXiv:2005
 Learn-ing action representations for reinforcement learning,2019, In International Conference on MachineLearning
 The selectgen challenge: Finding thebest training samples for few-shot neural text generation,2021, arXiv preprint arXiv:2108
 Few-shot NLG withpre-trained language model,2020, In Proceedings of the 58th Annual Meeting of the Association forComputational Linguistics
 Electra: Pre-trainingtext encoders as discriminators rather than generators,2020, International Conference on LearningRepresentations
 Cold-start reinforcement learning with softmax policy gradient,2017, arXivpreprint arXiv:1709
 Deep rein-forcement learning in large discrete action spaces,2015, arXiv preprint arXiv:1512
 Zero-shot question generation fromknowledge graphs for unseen predicates and entity types,2018, arXiv preprint arXiv:1802
 Maskgan: better text generation via filling inthe_,2018, International Conference on Learning Representations
 Incorporating copying mechanism insequence-to-sequence learning,1631, In Proceedings of the 54th Annual Meeting of the Association forComputational Linguistics (Volume 1: Long Papers)
 Text-to-text pre-training for data-to-text tasks,2020, arXiv preprint arXiv:2005
 Deep reinforcementlearning for sequence-to-sequence models,2019, IEEE transactions on neural networks and learningsystems
 Sample efficient text summa-rization using a single pre-trained transformer,2019, arXiv preprint arXiv:1905
 Actor-critic algorithms,2000, In Advances in neural informationprocessing systems
 Albert: A lite bert for self-supervised learning of language representations,2019, InternationalConference on Learning Representations
 Prefix-tuning: Optimizing continuous prompts for generation,2021, arXivpreprint arXiv:2101
 ROUGE: A package for automatic evaluation of summaries,2004, In Text SummarizationBranches Out
 Roberta: A robustly optimized bert pretrainingapproach,2019, arXiv preprint arXiv:1907
 A deep reinforced model for abstractivesummarization,2017, arXiv preprint arXiv:1705
 Improving language understanding bygenerative pre-training,2018, 2018
 Languagemodels are unsupervised multitask learners,2019, OpenAI blog
 Exploring the limits of transfer learning with a unified text-to-texttransformer,2019, arXiv preprint arXiv:1910
 Sequence level train-ing with recurrent neural networks,2015, arXiv preprint arXiv:1511
 Few-shot text generation with pattern-exploiting training,2020, arXivpreprint arXiv:2012
 Get to the point: Summarization with pointer-generator networks,2017, arXiv preprint arXiv:1704
 Neural abstractive textsummarization with sequence-to-sequence models,2021, ACM Transactions on Data Science
 Ammus : Asurvey of transformer-based pretrained models in natural language processing,2021, arXiv preprintarXiv:2108
 A study of reinforcement learningfor neural machine translation,2018, arXiv preprint arXiv:1808
 Augnlg: Few-shot natural languagegeneration using self-trained data augmentation,2021, arXiv preprint arXiv:2106
 Meta-learning for few-shot natural language processing: A survey,2020, arXiv preprintarXiv:2007
 Learn what notto learn: Action elimination with deep reinforcement learning,2018, arXiv preprint arXiv:1809
 Technical report: Auxiliary tuning and itsapplication to conditional text generation,2020, arXiv preprint arXiv:2006
 Exploring prompt-based few-shot learning for grounded dialoggeneration,2021, arXiv preprint arXiv:2109
 Fine-tuning language models from human preferences,2019, arXivpreprint arXiv:1909
