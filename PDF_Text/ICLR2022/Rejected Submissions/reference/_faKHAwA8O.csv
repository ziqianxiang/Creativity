title,year,conference
 Degan: Data-enriching gan for retrieving representative samples from a trained classifier,2020, InProceedings of the AAAI Conference on Artificial Intelligence
 Knowledgedistillation from internal representations,2020, In AAAI
 Food-101 - mining discriminative com-ponents with random forests,2014, In European Conference on Computer Vision
 Multitask learning,1998, In Encyclopedia of Machine Learning and Data Mining
 Data-free knowledge distillationfor object detection,2021, In WACV
 Dafl: Data-free learning of stUdent networks,2019, In ICCV
 Describing textUres in thewild,2014, 2014 IEEE Conference on Computer Vision and Pattern Recognition
 A linearized framework and a newbenchmark for model selection for fine-tUning,2021, arXiv preprint arXiv:2102
 Data-free adversarialdistillation,2019, ArXiv
 Knowledge concentration: Learning 100k object classifiersin a single cnn,2017, ArXiv
 Transfer learning by adaptive merging of multi-ple models,2019, In MIDL
 Caltech-256 object category dataset,2007, Technical report
 One-shot federated learning,2019, Machine Learningon Devices Workshop at NeurIPS
 Distilling the knowledge in a neural network,2015, ArXiv
 Overcoming catastrophic forgetting for continual learning via model adaptation,2019, In ICLR
 Foodx-251: Adataset for fine-grained food classification,2019, arXiv preprint arXiv:1907
 Overcoming catastrophic forgetting in neural networks,2017, Proceedingsof the National Academy of Sciences
 Lit: Learned intermediate represen-tation training for model compression,2019, In ICML
 Learning without forgetting,2018, IEEE Transactions on Pattern Analysisand Machine Intelligence
 Data-free knowledge distillation for deepneural networks,2017, ArXiv
 Large-scale generativedata-free distillation,2020, ArXiv
 Fine-grainedvisual classification of aircraft,2013, ArXiv
 Zero-shot knowledge transfer via adversarial belief matching,2019, InNeurIPS
 Zero-shot knowledge dis-tillation in deep networks,2019, In International Conference on Machine Learning
 Feature-level ensemble knowledge distillation for aggregatingknowledge from multiple networks,2020, In ECAI
 Scalable transfer learning with expert models,2020, arXivpreprint arXiv:2009
 icarl: Incre-mental classifier and representation learning,2017, 2017 IEEE Conference on Computer Vision andPattern Recognition (CVPR)
 Knowledge distillation meets self-supervision,2020, In ECCV
 Generative low-bitwidth data free quantization,2020, In ECCV
 Billion-scale semi-supervisedlearning for image classification,2019, ArXiv
 Objectdetection with a unified label space from multiple datasets,2020, In ECCV
 Places: A 10 million image databasefor scene recognition,2018, IEEE Transactions on Pattern Analysis and Machine Intelligence
