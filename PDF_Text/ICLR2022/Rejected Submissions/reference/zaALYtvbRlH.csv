title,year,conference
 A deep learning framework for financial time series using stackedautoencoders and long-short term memory,2017, PloS one
 MixText: Linguistically-informed interpolation of hiddenspace for semi-supervised text classification,2020, In Proceedings of the 58th Annual Meeting of theAssociation for Computational Linguistics
 Rethinking attentionwith Performers,2020, In International Conference on Learning Representations
 ELECTRA: Pre-trainingtext encoders as discriminators rather than generators,2019, In International Conference on LearningRepresentations
 A survey of data augmentation approaches for nlp,2021, Findings of ACL
 Nonlinear mixup: Out-of-manifold data augmentation for text classification,2020, InProceedings of the AAAI Conference on Artificial Intelligence
 Augmenting data with mixup for sentence classifi-cation: An empirical study,2019, arXiv preprint arXiv:1905
 Entity and evidence guidedrelation extraction for docred,2020, arXiv preprint arXiv:2008
 Efficient attentions forlong document summarization,2021, In Proceedings of the 2021 Conference of the North AmericanChapter of the Association for Computational Linguistics: Human Language Technologies
 REPT: Bridg-ing language models and machine reading comprehensionvia retrieval-based pre-training,2021, arXivpreprint arXiv:2105
 Highly accurateprotein structure prediction with alphafold,2021, Nature
 Transformers areRNNs: Fast aUtoregressive transformers with linear attention,2020, In International Conference onMachine Learning
 Look-ing beyond the sUrface: A challenge set for reading comprehension over mUltiple sentences,2018, InProceedings of the North American Chapter of the Association for Computational Linguistics:Human Language Technologies
 Reformer: The efficient transformer,2019, InInternational Conference on Learning Representations
 Adap-tive network sparsification with dependent variational beta-bernoulli dropout,2018, arXiv preprintarXiv:1805
 Î²-dropout: A unified dropout,2019, IEEEAccess
 RoBERTa: A robustly optimized BERT pre-training approach,2019, arXiv preprint arXiv:1907
 Deepromoter: robust promoterpredictor using deep learning,2019, Frontiers in genetics
 Languagemodels are unsupervised multitask learners,2019, OpenAI blog
 Improving neural machine translation modelswith monolingual data,2016, In 54th Annual Meeting of the Association for Computational Linguistics
 Long range arena: A benchmark for efficienttransformers,2020, In International Conference on Learning Representations
 HUggingface's transformers:State-of-the-art natural language processing,2019, arXiv preprint arXiv:1910
 Unsupervised data augmentationfor consistency training,2020, In Advances in Neural Information Processing Systems
 Big Bird: Transformers forlonger sequences,2020, In Advances in Neural Information Processing Systems
 mixup: Beyond empiricalrisk minimization,2018, In International Conference on Learning Representations
 Character-level convolutional networks for text clas-sification,2015, In Advances in Neural Information Processing Systems
 Predicting effects of noncoding variants with deep learning-based sequence model,2015, Nature methods
 Document-level relation extractionwith adaptive thresholding and localized context pooling,2021, In Proceedings of the AAAI Conferenceon Artificial Intelligence
