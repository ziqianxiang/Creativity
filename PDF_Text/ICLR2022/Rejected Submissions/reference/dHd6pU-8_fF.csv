title,year,conference
 Cubic regularization in symmetric rank-1 quasi-newton methods,2018, Mathematical Programming Computation
 On efficiently combining limited-memory and trust-region techniques,2017, Mathematical Programming Computation
 Adaptive cubic regularisation methods forunconstrained optimization,2011, part i: motivation
 Practical quasi-newton methods for training deepneural networks,2020, arXiv preprint arXiv:2006
 Deep Learning,2016, MIT Press
 Exploiting negativecurvature directions in linesearch methods for unconstrained optimization,2000, Optimization Methodsand Software
 Stochastic block bfgs: Squeezing more cur-vature out of data,2016, In Maria Florina Balcan and Kilian Q
 The modification of Newton?s method for unconstrained optimization by bound-ing cubic terms,1981, Technical report
 Ar-ray programming with NumPy,2020, Nature
 Neural networks for machine learninglecture 6a overview of mini-batch gradient descent,2012, Cited on
 Adam: A method for stochastic optimization,2014, arXiv preprintarXiv:1412
 On the limited memory bfgs method for large scale opti-mization,1436, Mathematical Programming
 A linearly-convergent stochastic L-BFGSalgorithm,2016, In Arthur Gretton and Christian C
 Numerical Optimization,2006, Springer
 Fashion-mnist: a novel image dataset for benchmark-ing machine learning algorithms,2017, 2017
 Adadelta: an adaptive learning rate method,2012, arXiv preprint arXiv:1212
