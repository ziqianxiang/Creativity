title,year,conference
 Variancereduction in sgd by distributed importance sampling,2015, arXiv preprint arXiv:1511
 Large scale distributed neural network training through online distillation,2018, arXiv preprintarXiv:1804
 Curriculum learning,2009, InProceedings ofthe 26th annual international conference on machine learning
 Coresets via bilevel optimization for continuallearning and streaming,2020, arXiv preprint arXiv:2006
 Language models arefew-shot learners,2020, arXiv preprint arXiv:2005
 Selection via proxy: Efficient data selection for deeplearning,2020, International Conference on Learning Representations
 Cinic-10 is not imagenetor cifar-10,2018, arXiv preprint arXiv:1810
 On statistical bias in active learning: How andwhen to fix it,2021, arXiv preprint arXiv:2101
 Dropout as a bayesian approximation: Representing modeluncertainty in deep learning,1050, In international conference on machine learning
 Deep residual learning for imagerecognition,2016, In Proceedings of the IEEE conference on computer vision and pattern recognition
 Deberta: Decoding-enhanced bertwith disentangled attention,2020, arXiv preprint arXiv:2006
 Bayesian active learning forclassification and preference learning,2011, arXiv preprint arXiv:1112
 Gpipe: Efficient training of giant neuralnetworks using pipeline parallelism,2019, Advances in neural information processing systems
 Accelerating deeplearning by focusing on the biggest losers,2019, arXiv preprint arXiv:1910
 In-datacenter performance analysis ofa tensor processing unit,2017, In Proceedings of the 44th annual international symposium on computerarchitecture
 Biased importance sampling for deep neural networktraining,2017, arXiv preprint arXiv:1706
 Not all samples are created equal: Deep learning withimportance sampling,2018, In International conference on machine learning
 Ordered sgd: A new stochastic optimization framework forempirical risk minimization,2020, In International Conference on Artificial Intelligence and Statistics
 On large-batch training for deep learning: Generalization gap and sharp minima,2016, arXivpreprint arXiv:1609
 Batchbald: Efficient and diverse batchacquisition for deep bayesian active learning,2019, arXiv preprint arXiv:1906
 Active learning under pool set distribution shift andnoisy data,2021, CoRR
 Learning multiple layers of features from tiny images,2009, Masterâ€™s thesis
 Gradient-based learning applied to documentrecognition,1998, Proceedings of the IEEE
 Efficient backprop,2012, InNeural networks: Tricks of the trade
 Confidence-based active learning,2006, IEEE transactions on patternanalysis and machine intelligence
 Training invariant support vector machines usingselective sampling,2007, In Leon Bottou
 Online batch selection for faster training of neural networks,2015, arXivpreprint arXiv:1511
 A surveyon bias and fairness in machine learning,2021, ACM Computing Surveys (CSUR)
 Practical deep learning with bayesian principles,2019, In Proceedingsof the 33rd International Conference on Neural Information Processing Systems
 Deep learning is robust to massivelabel noise,2017, arXiv preprint arXiv:1705
 Prioritized experience replay,2015, arXivpreprint arXiv:1511
 Active learning literature survey,2009, 2009
 Divide and contrast: Self-supervisedlearning from uncurated data,2021, arXiv preprint arXiv:2105
 Learning from massive noisylabeled data for image classification,2015, In CVPR
 Cold case: The lost mnist digits,2019, In Advances in Neural InformationProcessing Systems 32
 Autoassist: A framework to accelerate trainingof deep neural networks,2019, arXiv preprint arXiv:1905
