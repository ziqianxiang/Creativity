title,year,conference
 Bert: Pre-training of deepbidirectional transformers for language understanding,2018, arXiv preprint arXiv:1810
 The pile: An 800gb dataset of diverse textfor language modeling,2020, arXiv preprint arXiv:2101
 Conformer: Convolution-augmented transformerfor speech recognition,2020, arXiv preprint arXiv:2005
 Implementing FFTs in Practice,2012, In C
 Scaling laws for neural languagemodels,2020, arXiv preprint arXiv:2001
 Improving language under-standing by generative pre-training,2018, 2018
 Languagemodels are unsupervised multitask learners,2019, OpenAI blog
 Exploring the limits of transfer learning with a unified text-to-texttransformer,2019, arXiv preprint arXiv:1910
 Efficient transformers: A survey,2020, arXivpreprint arXiv:2009
 Attention is all you need,2017, In Advances in neural informationprocessing systems
 Legendre memory units: Continuous-time repre-sentation in recurrent neural networks,2019, In Advances in Neural Information Processing Systems
 Linformer: Self-attentionwith linear complexity,2020, arXiv preprint arXiv:2006
 Lite transformer with long-shortrange attention,2020, In International Conference on Learning Representations
 Convolutional self-attention networks,2019, arXiv preprint arXiv:1904
 Big bird: Transformers forlonger sequences,2020, In NeurIPS
