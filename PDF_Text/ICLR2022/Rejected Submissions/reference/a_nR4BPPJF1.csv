title,year,conference
 Convex Optimization,2004, Cambridge University Press
 Multitask Learning,1997, Machine learning
 ELECTRA: Pre-trainingText Encoders as Discriminators Rather Than Generators,2020, In International Conference on LearningRepresentations
 Reducing Over-fitting in Deep Networks by Decorrelating Representations,2015, arXiv preprint arXiv:1511
 BERT: Pre-training ofDeep Bidirectional Transformers for Language Understanding,2019, In Proceedings of the 2019Conference of the North American Chapter of the Association for Computational Linguistics:Human Language Technologies
 Size-independent Sample Complexity ofNeural Networks,2018, In Conference On Learning Theory
 ALBERT: A Lite BERT for Self-supervised Learning of Language Representations,2020, InInternational Conference on Learning Representations
 Predicting what you already know helps:Provable self-supervised learning,2020, arXiv preprint arXiv:2008
 Diversity Networks: Neural Network Compression Using DeterminantalPoint Processes,2015, arXiv preprint arXiv:1511
 About the Constants in Talagrandâ€™s Concentration Inequalities for Empirical Pro-cesses,2000, Annals of Probability
 A vector-contraction inequality for Rademacher complexities,2016, In InternationalConference on Algorithmic Learning Theory
 Improving Language Under-standing by Generative Pre-Training,2018, 2018
 Sample EfficientLinear Meta-Learning by Alternating Minimization,2021, arXiv preprint arXiv:2105
 On the Theory of Transfer Learning: TheImportance of Task Diversity,2020, In Hugo Larochelle
 Attention is All you Need,2017, In I
 Uncorrelation and Evenness: a New Diversity-PromotingRegularizer,2017, In International Conference on Machine Learning
 Large batch optimization for deeplearning: Training BERT in 76 minutes,2019, arXiv preprint arXiv:1904
