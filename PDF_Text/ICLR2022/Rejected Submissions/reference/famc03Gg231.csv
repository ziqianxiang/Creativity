title,year,conference
 Search for neutrinolessdouble-Î² decay in ge 76 with the majorana demonstrator,2018, Physical review letters
 Numerical methods for partial differential equations,2014, Academic press
 Layer normalization,2016, In InternationalConference on Learning Representations (ICLR)
 Estimation and inference innonlinear structural models,1974, In Annals of Economic and Social Measurement
 Convergence of quasi-newton matricesgenerated by the symmetric rank one update,1991, Mathematical programming
 Introduction to robotics: mechanics and control,2005, Prentice Hall
 The method of steepest descent for non-linear minimization problems,1944, Quarterlyof Applied Mathematics
 Approximation by superpositions ofa sigmoidal function,1989, Mathematics of control
 Finite volume schemes for diffusion equations: introduction to and review ofmodern methods,2014, Mathematical Models and Methods in Applied Sciences
 Gradient descent provably optimizesover-parameterized neural networks,2018, arXiv preprint arXiv:1810
 Deep neural networks to enable real-time multimessenger astro-physics,2018, Physical Review D
 Deep learning for real-time gravitational wave detection and param-eter estimation: Results with advanced ligo data,2018, Physics Letters B
 Deep learning,2016, MIT pressCambridge
 The marker-and-cell method,1972, Fluid Dyn
 Numerical calculation of time-dependent viscous incompress-ible flow of fluid with free surface,1965, The physics of fluids
 Deep networks withstochastic depth,2016, In European conference on computer vision
 Batch normalization: Accelerating deep network training byreducing internal covariate shift,2015, In arXiv:1502
 On large-batch training for deep learning: Generalization gap and sharp minima,2017, InInternational Conference on Learning Representations (ICLR)
 Deep fluids: A generative network for parameterized fluid simulations,2019, In ComputerGraphics Forum
 Adam: A method for stochastic optimization,2015, In InternationalConference on Learning Representations (ICLR)
 Summary of the fesac transformativeenabling capabilities panel report,2019, Fusion Science and Technology
 Optimizing neural networks with kronecker-factored approximatecurvature,2015, In International conference on machine learning
 Energy-preserving integrators for fluid animation,2009, ACM Transactions on Graphics (TOG)
 Automatic differentiation inpytorch,2017, In Advances in Neural Information Processing Systems
 A hybrid method for nonlinear equations,1970, In Numerical methods for nonlinearalgebraic equations
 Universal differential equations for scientific machinelearning,2020, In arXiv:2001
 A stochastic quasi-newton method for onlineconvex optimization,2007, In Artificial intelligence and statistics
 Inverse problem theory and methods for model parameter estimation,2005, SIAM
001 and batch size of 100,2022, Despite thesimpler
 This adds up to 461 totaltrainable parameters,1000, A bias is applied at each layer and the ReLU activation function is appliedafter each hidden layer
