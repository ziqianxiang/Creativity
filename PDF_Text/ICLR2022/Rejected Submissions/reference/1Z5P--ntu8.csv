title,year,conference
 A convergence theory for deep learning via over-parameterization,2019, InProceedings of the 36th International Conference on Machine Learning
 Gradient flows: in metric spaces and in the space ofprobabilitymeasures,2008, Springer Science and Business Media
 On exact computation with aninfinitely wide neural net,2019, In NeurIPS
 Generalization of two-layer neural networks:An asymptotic viewpoint,2020, In ICLR
 On the global convergence of gradient descent for over-parameterized modelsusing optimal transport,2018, In Advances in Neural Information Processing Systems
 Gradient descent can take exponential timeto escape saddle points,2017, In Advances in Neural Information Processing Systems
 Gradient descent finds global minima of deep neuralnetworks,2019, In Proceedings of the 36th International Conference on Machine Learning
 Gradient descent provably optimizes over-parameterizedneural networks,2019, In International Conference on Learning Representations
 Algorithm-dependent generalization bounds for overparameterized deepresidual networks,2019, In NeurIPS
 Escaping from saddle points â€” online stochastic gradient fortensor decomposition,2015, In Proceedings of The 28th Conference on Learning Theory
 Learning one-hidden-layer neural networks with landscape design,2018, InInternational Conference on Learning Representations
 Neural tangent kernel: Convergence and generalization in neuralnetworks,2018, In Advances in Neural Information Processing Systems
 How to escape saddle points efficiently,2017, InProceedings of the 34th International Conference on Machine Learning
 Deep learning without poor local minima,2016, In Advances in Neural InformationProcessing Systems
 Learning overparameterized neural networks via stochastic gradient descenton structured data,2018, In Proceedings of the 32nd International Conference on Neural InformationProcessing Systems
 A mean field analysis of deep ResNet and beyond:Towards provably optimization via overparameterization from depth,2020, In Proceedings of the 37thInternational Conference on Machine Learning
 A mean field view of the landscape of two-layer neuralnetworks,2018, Proceedings of the National Academy of Sciences
 The loss surface of deep and wide neural networks,2017, In Proceedings ofthe 34th International Conference on Machine Learning
 Optimization landscape and expressivity of deep cnns,2018, In ICML
 Mean field analysis of deep neural networks,2021, Mathematics ofOperations Research
 Theoretical insights into the optimization landscape ofover-parameterized shallow neural networks,0018, IEEE Trans
 Regularization matters: Generalization and optimization of neuralnets v,2019,s
 Global optimality conditions for deep neural networks,2018, InInternational Conference on Learning Representations
 Gradient descent optimizes over-parameterized deep relunetworks,2019, Machine Learning
1 and AssUmption 6,2022,2
