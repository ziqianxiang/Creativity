title,year,conference
 On lazy training in differen-tiable programming,2019, In Advances in Neural Information Processing Systems
 Finite depth and width corrections to the neUral tangent kernel,2020, InInternational Conference on Learning Representations
 How to start training: The effect of initialization and ar-Chitecture,2018, In NeurIPS
 Delving deep into rectifiers: SUrpassinghUman-level performance on imagenet classification,2015, In Proceedings of the IEEE InternationalConference on Computer Vision (ICCV)
 Deep residUal learning for image recog-nition,2016, In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition(CVPR)
 Batch normalization: Accelerating deep network training byredUcing internal covariate shift,2015, In ICML
 Self-normalizingneural networks,2017, In NIPS
 On the number of lin-ear regions of deep neural networks,2014, In Advances in Neural Information Processing Systems
 Analysis of boolean functions,2021, CoRR
 Resurrecting the sigmoid in deep learn-ing through dynamical isometry: theory and practice,2017, Advances in neural information processingsystems
 Deep informationpropagation,2017, In International Conference on Learning Representations
 Disentangling trainability and gen-eralization in deep learning,2019, CoRR
 Mean field residual networks: On the edge ofchaos,2017, In Advances in Neural Information Processing Systems
