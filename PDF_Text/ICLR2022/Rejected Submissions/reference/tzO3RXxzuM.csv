title,year,conference
 Qsgd:Communication-efficient sgd via gradient quantization and encoding,2017, In I
 Private adaptive gradientmethods for convex optimization,2021, In International Conference on Machine Learning
 Private empirical risk minimization: Efficientalgorithms and tight error bounds,2014, In 2014 IEEE 55th Annual Symposium on Foundations ofComputer Science
 Private stochasticconvex optimization with optimal rates,2019, Advances in neural information processing systems
 Concentration inequalities: A nonaSymp-totic theory of independence,2013, Oxford university press
 Sharper bounds for uniformly stablealgorithms,2020, In Conference on Learning Theory
 Fundamentals of statistical exponential families: With applications in statisticaldecision theory,1986, Ims
 Tightening mutual information basedbounds on generalization error,2019, In 2019 IEEE International Symposium on Information Theory(ISIT)
 Composable and versatileprivacy via truncated cdp,2018, In Proceedings ofthe 50th Annual ACM SIGACT Symposium on Theoryof Computing
 The discrete gaussian for differentialprivacy,2020, In NeurIPS
 Distributedtraining With heterogeneous data: Bridging median-and mean-based algorithms,2019, arXiv preprintarXiv:1906
 Distribution-free inequalities for the deleted and holdout errorestimates,1979, IEEE Transactions on Information Theory
 High probability generalization bounds for uniformly stable al-gorithms with nearly optimal rate,2019, In Conference on Learning Theory
 Fast-rate loss bounds via conditional information measureswith applications to neural networks,2021, In 2021 IEEE International Symposium on InformationTheory (ISIT)
 A linear speedup analysis of distributed deep learning with sparseand quantized communication,2018, In S
 How to escape saddlepoints efficiently,2017, In International Conference on Machine Learning
 Stochastic-sign sgd for feder-ated learning with theoretical guarantees,2020, arXiv preprint arXiv:2002
 Dimension independence inunconstrained private erm via adaptive preconditioning,2020, arXiv preprint arXiv:2008
 Gradient-based learning applied todocument recognition,1998, Proceedings of the IEEE
 On generalization error bounds of noisy gradient methodsfor non-convex learning,2020, In International Conference on Learning Representations
 Information-theoretic generalization bounds for stochastic gradient descent,2021, arXivpreprint arXiv:2102
 A userâ€™s guide to measure theoretic probability,2002, Number 8
 On randomsubset generalization error bounds and the stochastic gradient langevin dynamics algorithm,2021, In2020 IEEE Information Theory Workshop (ITW)
 A finite sample distribution-free performance bound forlocal discrimination rules,1978, The Annals of Statistics
 f -divergence inequalities,2016, IEEE Transactions on Information Theory
 Stochastic gradient descent for non-smooth optimization: Conver-gence results and optimal averaging schemes,2013, In International conference on machine learning
 Reasoning about generalization via conditional mutualinformation,2020, In Conference on Learning Theory
 Bayesian learning via stochastic gradient langevin dynamics,2011, InInternational Conference on Machine Learning
 Information-theoretic analysis of generalization capability of learn-ing algorithms,2017, Advances in Neural Information Processing Systems
 Understandingdeep learning requires rethinking generalization,2017, In 5th International Conference on LearningRepresentations
 Wide network learning with differential pri-vacy,2021, arXiv preprint arXiv:2103
 Individually conditional individual mutual information boundon generalization error,2021, In 2021 IEEE International Symposium on Information Theory (ISIT)
 Bypassing the ambient dimension: Private sgdwith gradient subspace identification,2020, In International Conference on Learning Representations
