title,year,conference
 Revisiting the generaliza-tion of adaptive gradient methods,2019, 2019
 Backward feature correction: How deep learning performs deeplearning,2020, arXiv preprint arXiv:2001
 Feature purification: How adversarial training performs robustdeep learning,2020, arXiv preprint arXiv:2005
 A convergence theory for deep learning via over-parameterization,2019, In International Conference on Machine Learning
 On the convergence rate of training recurrent neuralnetworks,2019, In Advances in Neural Information Processing Systems
 Fine-grained analysis ofoptimization and generalization for overparameterized two-layer neural networks,2019, In InternationalConference on Machine Learning
 Onexact computation with an infinitely wide neural net,2019, In Advances in Neural Information ProcessingSystems
 Beyond linearization: On quadratic and higher-order approximation of wideneural networks,2019, In International Conference on Learning Representations
 signsgd:Compressed optimisation for non-convex problems,2018, In International Conference on MachineLearning
 Generalization bounds of stochastic gradient descent for wide and deepneural networks,2019, In Advances in Neural Information Processing Systems
 Closing thegeneralization gap of adaptive gradient methods in training deep neural networks,2020, In InternationalJoint Conferences on Artificial Intelligence
 Gradient descent finds globalminima of deep neural networks,2019, In International Conference on Machine Learning
 Gradient descent provably optimizesover-parameterized neural networks,2019, In International Conference on Learning Representations
 Polylogarithmic width suffices for gradient descent to achievearbitrarily small test error with shallow relu networks,2020, In International Conference on LearningRepresentations
 Improving generalization performance by switching fromadam to sgd,2017, arXiv preprint arXiv:1712
 Adam: A method for stochastic optimization,2015, InternationalConference on Learning Representations
 Learning multiple layers of features from tiny images,2009, 2009
 Learning overparameterized neural networks via stochastic gradientdescent on structured data,2018, In Advances in Neural Information Processing Systems
 Learning over-parametrized two-layer neuralnetworks beyond ntk,2020, In Conference on Learning Theory
 Adaptive gradient methods with dynamicbound of learning rate,2019, arXiv preprint arXiv:1902
 Regularizing and optimizing lstmlanguage models,2018, In International Conference on Learning Representations
 On the convergence of adam and beyond,2018, InInternational Conference on Learning Representations
 On the convergence of adam and beyond,2019, arXivpreprint arXiv:1904
 The marginalvalue of adaptive gradient methods in machine learning,2017, In Advances in Neural InformationProcessing Systems
 Towards theoreticallyunderstanding why sgd generalizes better than adam in deep learning,2020, Advances in NeuralInformation Processing Systems
 Gradient descent optimizes over-parameterized deep ReLU networks,2019, Machine Learning
