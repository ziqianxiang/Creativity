title,year,conference
 A comprehensive survey on transfer learning,2019, arXiv preprint arXiv:1911
 Representation learning: A review and newperspectives,2013, IEEE transactions on pattern analysis and machine intelligence
 Recent advances in autoencoder-basedrepresentation learning,2018, arXiv preprint arXiv:1812
 Memorizationin overparameterized autoencoders,2018, arXiv preprint arXiv:1810
 Modular learning in neural networks,1987, In AAAI
 Challenging common assumptions in the unsupervised learning of disentan-gled representations,2018, arXiv preprint arXiv:1811
 Disentangled repre-sentations from non-disentangled models,2021, arXiv preprint arXiv:2102
 Weakly superviseddisentanglement with guarantees,2019, arXiv preprint arXiv:1910
 Deep representation decom-position for feature disentanglement,2020, arXiv preprint arXiv:2011
 Semi-supervised stylegan for disentanglement learning,2020, In International Conferenceon Machine Learning
 Attention is all you need,2017, In Advances in neural informationprocessing systems
 Diagonal attention and style-based gan for content-style disentan-glement in image generation and translation,2021, arXiv preprint arXiv:2103
 Elements OfCausal Inference - Foundations and LearningAlgorithms,2017, MIT Press
 Causaleffect inference with deep latent-variable models,2017, arXiv preprint arXiv:1705
 Representa-tion learning via invariant causal mechanisms,2020, arXiv preprint arXiv:2010
 Disentangledgenerative causal representation learning,2020, arXiv preprint arXiv:2010
 beta-VAE: Learning basic visual conceptswith a constrained variational framework,2017, ICLR
 Evaluating the disentan-glement of deep generative models through manifold topology,2020, arXiv preprint arXiv:2006
 Understanding disentangling in Î²-VAE,2018, arXiv preprint arXiv:1804
 Disentangling by factorising,2018, arXiv preprint arXiv:1802
 Independent subspace analysis for unsupervisedlearning of disentangled representations,2020, In International Conference on Artificial Intelligence andStatistics
 Wasserstein auto-encoders,2018, In 6th InternationalConference on Learning Representations (ICLR)
 On the latent space of Wassersteinauto-encoders,2018, arXiv preprint arXiv:1802
 Disentangled recurrentwasserstein autoencoder,2021, arXiv preprint arXiv:2101
 Counterfactuals uncover themodular structure of deep generative models,2018, arXiv preprint arXiv:1812
 A style-based generator architecture for generativeadversarial networks,2019, In Proceedings of the IEEE Conference on Computer Vision andPattern Recognition
 Discovering causal signals inimages,2017, In IEEE Conference on Computer Vision and Pattern Recognition (CVPR)
 Robustly disentangledcausal mechanisms: Validating deep representations for interventional robustness,2018, arXiv preprintarXiv:1811
 Weakly-supervised disentanglement without compromises,2020, In International Conferenceon Machine Learning
 Diagnosing and enhancing VAE models,2019, arXiv preprint arXiv:1903
 Understanding posteriorcollapse in generative latent variable models,2019, 2019
 Deep learning face attributes in the wild,2015, InProceedings of International Conference on Computer Vision (ICCV)
 On the transfer of disentangled representations inrealistic settings,2020, arXiv preprint arXiv:2010
 Ganstrained by a two time-scale update rule converge to a local nash equilibrium,2017, In Advances in neuralinformation processing systems
 Hierar-chical quantized autoencoders,2020, arXiv preprint arXiv:2002
 Mish: A self regularized non-monotonic neural activation function,2019, arXiv preprintarXiv:1908
 Auto-encoding variational bayes,2013, arXiv preprintarXiv:1312
 A framework for the quantitative evaluation ofdisentangled representations,2018, 2018
 Interventional robustnessof deep latent variable models,2018, stat
 Learning deep disentangled embeddings with the f-statisticloss,2018, arXiv preprint arXiv:1802
 Visual representation learningdoes not generalize strongly within the same domain,2021, arXiv preprint arXiv:2107
 Toward causal representation learning,2021, Proceedings of theIEEE
 Variational inference of disentangledlatent concepts from unlabeled observations,2017, arXiv preprint arXiv:1711
