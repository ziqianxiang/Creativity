title,year,conference
 A large anno-tated corpus for learning natural language inference,2015, In Proceedings of the 2015 Conference onEmpirical Methods in Natural Language Processing
 ELECTRA: Pre-trainingtext encoders as discriminators rather than generators,2019, In International Conference on LearningRepresentations
 Cracking the contextual commonsense code: Understanding commonsensereasoning aptitude of deep contextual representations,2019, EMNLP 2019
 BERT: Pre-training of deepbidirectional transformers for language understanding,2019, In Proceedings of the 2019 Conference ofthe North American Chapter of the Association for Computational Linguistics: Human LanguageTechnologies
 What bert is not: Lessons from a new suite of psycholinguistic diagnostics forlanguage models,2020, Transactions of the Association for Computational Linguistics
 Neural natural language inference mod-els partially embed theories of lexical entailment and negation,2020, In Proceedings of the ThirdBlackboxNLP Workshop on Analyzing and Interpreting Neural Networks for NLP
 A structural probe for finding syntax in word representa-tions,2019, In Proceedings of the 2019 Conference of the North American Chapter of the Associationfor Computational Linguistics: Human Language Technologies
 Long short-term memory,1997, Neural Computation
 Cross-lingual ability of multilingualbert: An empirical study,2019, In International Conference on Learning Representations
 Adam: A method for stochastic optimization,2014, In Proceedingsof the 3rd International Conference on Learning Representations
 Conditional random fields:Probabilistic models for segmenting and labeling sequence data,1558, In Proceedings of the EighteenthInternational Conference on Machine Learning
 Roberta: A robustly optimized bert pretrainingapproach,2019, arXiv preprint arXiv:1907
 Effective approaches to attention-based neural machine translation,2015, In Proceedings of the 2015 Conference on Empirical Methodsin Natural Language Processing
 Emergentlinguistic structure in artificial neural networks trained by self-supervision,2020, Proceedings of theNational Academy of Sciences
 RNNs implicitly implementtensor product representations,2019, In In Proceedings of the 7th International Conference on LearningRepresentations
 Learning music helps you read: Using transfer to studylinguistic structure in language models,2020, In Proceedings of the 2020 Conference on EmpiricalMethods in Natural Language Processing (EMNLP)
 Learning important features throughpropagating activation differences,2017, In Doina Precup and Yee Whye Teh (eds
 Recursive deep models for semantic compositionality over a sentimenttreebank,2013, In Proceedings of the 2013 Conference on Empirical Methods in Natural LanguageProcessing
 Striving for simplicity: Theall convolutional net,2015, In ICLR (workshop track)
 Axiomatic attribution for deep networks,2017, InDoina Precup and Yee Whye Teh (eds
 Investigating transferabilityin pretrained language models,2020, In Findings of the Association for Computational Linguistics:EMNLP 2020
 From english to foreign languages: Transferring pre-trained language models,2020, arXivpreprint arXiv:2002
 Glue:A multi-task benchmark and analysis platform for natural language understanding,2018, In Proceedingsof the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks forNLP
 Transformers: State-of-the-art naturallanguage processing,2020, In Proceedings of the 2020 Conference on Empirical Methods in NaturalLanguage Processing: System Demonstrations
 Kermit: Complementing transformer architectures with en-coders of explicit syntactic interpretations,2020, In Proceedings of the 2020 Conference on EmpiricalMethods in Natural Language Processing (EMNLP)
 Visualizing and understanding convolutional networks,2014, In DavidFleet
