title,year,conference
 The evolutionof out-of-distribution robustness throughout fine-tuning,2021, arXiv preprint
 Phd thesis: Out of distribution generalization in machine learning,2019, New YorkUniversity
 A simple framework forcontrastive learning of visual representations,2020, In ICML
 On the measure of intelligence,2019, arXiv preprint
 Randaugment: Practical automateddata augmentation with a reduced search space,2020, In NeurIPS
 Efficientlyidentifying task groupings for multi-task learning,2021, arXiv preprint
 Donâ€™t stop pretraining: Adapt language models to domains and tasks,2020, InACL
 Identity mappings in deep residualnetworks,2016, In ECCV
 Benchmarking neural network robustness to common cor-ruptions and perturbations,2019, In ICLR
 Clevr: A diagnostic dataset for compositional language and elementary visualreasoning,2016, arXiv preprint
 Wilds: A benchmarkof in-the-wild distribution shifts,2020, arXiV preprint
 Mnist-c: A robustness benchmark for computer vision,2019, arXivpreprint
 Zero-shot text-to-image generation,2021, arXiv preprint
 Robust fine-tuning of zero-shot models,2021, arXivpreprint
 Unsupervised dataaugmentation for consistency training,2020, In NeurIPS
 Unpaired image-to-image translationusing cycle-consistent adversarial networks,2017, In ICCV
