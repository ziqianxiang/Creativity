title,year,conference
 High-dimensional dynamics of gener-alization error in neural networks,2020, Neural Networks
 Deep convolutionalnetworks do not classify based on global object shape,2018, PLoS computational biology
 Benign ovefitting in linearregression,2020, Proceedings of the National Academy of Sciences
 Shake-shake regularization,2017, arXiv preprint arXiv:1705
 Shortcut learning in deep neural networks,2020, NatureMachine Intelligence
 Surprises in high-dimensional ridgeless least squares interpolation,2019, arXiv preprint arXiv:1903
 The origins and prevalence of texture biasin convolutional neural networks,2019, arXiv preprint arXiv:1911
 Semantic adversarial examples,2018, In Proceedings of theIEEE Conference on Computer Vision and Pattern Recognition Workshops
 Some improvements on deep convolutional neural network based image classi-fication,2013, arXiv preprint arXiv:1312
 Measuring the tendency of CNNs to learn surface statistical regulari-ties,2017, arXiv preprint arXiv:1711
 Similarity of neuralnetwork representations revisited,2019, In ICML
 Just interpolate: Kernel “ridgeless” regression can gen-eralize,2020, AnnalsofStatistics
 Uni-versality and individuality in neural dynamics across large populations of recurrent networks,2019, InAdvances in neural information processing systems
 Right for the wrong reasons: Diagnosing syntacticheuristics in natural language inference,2019, arXiv preprint arXiv:1902
 Spectral normalizationfor generative adversarial networks,2018, In International Conference on Learning Representations
 Insights on representational similarity in neuralnetworks with canonical correlation,2018, arXiv preprint arXiv:1806
 Sgd on neural networks learns functions of increasing complexity,2019, arXivpreprint arXiv:1905
 Svcca: Singular vectorcanonical correlation analysis for deep learning dynamics and interpretability,2017, In Advances inNeural Information Processing Systems
 Probing the state of the art: A critical look at visualrepresentation evaluation,2019, arXiv preprint arXiv:1912
 ”Why should i trust you?” explaining thepredictions of any classifier,2016, In Proceedings of the 22nd ACM SIGKDD international conferenceon knowledge discovery and data mining
 Going deeper with convolutions,2015, InProceedings of the IEEE conference on computer vision and pattern recognition
 Efficientnet: Rethinking model scaling for convolutional neural net-works,2019, In International Conference on Machine Learning
 The effect of task and training onintermediate representations in convolutional neural networks revealed with modified rv similarityanalysis,2019, arXiv preprint arXiv:1912
 Deep learning generalizes because theparameter-function map is biased towards simple functions,2018, arXiv preprint arXiv:1805
 Rotation equiv-ariant cnns for digital pathology,2018, In Alejandro F
 Deeplearning for identifying metastatic breast cancer,2016, arXiv preprint arXiv:1606
 Wide residUal networks,2016, In British Machine VisionConference (BMVC)
 Understandingdeep learning reqUires rethinking generalization,2016, arXiv preprint arXiv:1611
