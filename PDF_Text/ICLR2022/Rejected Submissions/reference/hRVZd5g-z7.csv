title,year,conference
 Structured convolutions for efficient neuralnetwork design,2020, arXiv preprint arXiv:2008
 A closerlook at few-shot classification,2019, arXiv preprint arXiv:1904
 Dual pathnetworks,2017, In Advances in Neural Information Processing Systems
 A survey of model compression and accelerationfor deep neural networks,2017, arXiv preprint arXiv:1710
 Xception: Deep learning with depthwise separable convolutions,2017, In Proceedingsof the IEEE Conference on Computer Vision and Pattern Recognition
 Imagenet: A large-scalehierarchical image database,2009, In Proceedings of the IEEE Conference on Computer Vision andPattern Recognition
 Neural architecture search: A survey,2018, arXivpreprint arXiv:1808
 Model-agnostic meta-learning for fast adaptationof deep networks,2017, In International Conference on Machine Learning
 Channel pruning for accelerating very deep neural net-works,2017, In Proceedings of the IEEE International Conference on Computer Vision
 Mobilenets: Efficient convolutional neural networks formobile vision applications,2017, arXiv preprint arXiv:1704
 Densely connectedconvolutional networks,2017, In Proceedings of the IEEE Conference on Computer Vision and PatternRecognition
 Adam: A method for stochastic optimization,2014, arXiv preprintarXiv:1412
 Similarity of neuralnetwork representations revisited,2019, arXiv preprint arXiv:1905
 Snip: Single-shot network pruningbased on connection sensitivity,2018, International Conference on Learning Representations
 Pruning filters forefficient convnets,2016, arXiv preprint arXiv:1608
 Learning filter basis for convolutionalneural network compression,2019, In Proceedings of the IEEE International Conference on ComputerVision
 Learning filter basis for convolutionalneural network compression,2019, In Proceedings of the IEEE International Conference on ComputerVision
 Gradient episodic memory for continual learning,2017, InAdvances in Neural Information Processing Systems
 Thinet: A filter level pruning method for deep neuralnetwork compression,2017, In Proceedings of the IEEE International Conference on Computer Vision
 Shufflenet v2: Practical guidelinesfor efficient cnn architecture design,2018, In Proceedings of the European Conference on ComputerVision
 T-basis: a compact representation for neural networks,2020, International Conference onMachine Learning
 Efficient neural architecturesearch via parameter sharing,2018, arXiv preprint arXiv:1802
 Stable low-rank tensor de-composition for compression of convolutional neural network,2020, In Proceedings of the EuropeanConference on Computer Vision
 DCFNet: Deep neuralnetwork with decomposed convolutional filters,2018, International Conference on Machine Learning
 Svcca: Singular vectorcanonical correlation analysis for deep learning dynamics and interpretability,2017, In Advances inNeural Information Processing Systems
 Mo-bilenetv2: Inverted residuals and linear bottlenecks,2018, In Proceedings of the IEEE Conference onComputer Vision and Pattern Recognition
 Grad-cam: Visual explanations from deep networks via gradient-based local-ization,2017, In Proceedings of the IEEE International Conference on Computer Vision
 Continual learning with deep generativereplay,2017, In Advances in Neural Information Processing Systems
 Very deep convolutional networks for large-scale imagerecognition,2014, arXiv preprint arXiv:1409
 Clustering convolutional kernels to compressdeep neural networks,2018, In Proceedings of the European Conference on Computer Vision
 Scale-equivariant steerable networks,2019, arXivpreprint arXiv:1910
 And the bitgoes down: Revisiting the quantization of neural networks,2020, International Conference on LearningRepresentations
 Efficientnet: Rethinking model scaling for convolutional neuralnetworks,2019, arXiv preprint arXiv:1905
 Picking winning tickets before training bypreserving gradient flow,2020, In International Conference on Learning Representations
 Wide compres-sion: Tensor ring nets,2018, In Proceedings of the IEEE Conference on Computer Vision and PatternRecognition (CVPR)
 Learning versatile filters forefficient convolutional neural networks,2018, In Advances in Neural Information Processing Systems
 Deepk-means: Re-training and parameter sharing with harder cluster assignments for compressingdeep convolutions,2018, International Conference on Machine Learning
 Empirical evaluation of rectified activations inconvolutional network,2015, arXiv preprint arXiv:1505
 LegoNet: Efficient convolutional neural networks with lego filters,2019, In InternationalConference on Machine Learning
 Wide residual networks,2016, arXiv preprintarXiv:1605
 Shufflenet: An extremely efficientconvolutional neural network for mobile devices,2018, In Proceedings of the IEEE Conference onComputer Vision and Pattern Recognition
 Learning deepfeatures for discriminative localization,2016, In Proceedings of the IEEE Conference on ComputerVision and Pattern Recognition
 Neural architecture search with reinforcement learning,2017, InternationalConference on Learning Representations
 Learning transferable architecturesfor scalable image recognition,2018, In Proceedings of the IEEE Conference on Computer Vision andPattern Recognition
