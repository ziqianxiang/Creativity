title,year,conference
 Approximate nearest neighbors and the fast johnson-lindenstrausstransform,2006, In STOC
 Subspace embeddingand linear regression with orlicz norm,2018, In International Conference on Machine Learning (ICML)
 Optimal cur matrix decompositions,2014, In Proceedings ofthe 46th Annual ACM Symposium on Theory of Computing (STOC)
 A measure of asymptotic efficiency for tests of a hypothesis based on the sum ofobservations,1952, The Annals of Mathematical Statistics
 Low rank approximation and regression in inputsparsity time,2013, In Symposium on Theory of Computing Conference
 Large scale distributed deep networks,2012, InAdvances in neural information processing systems
 Calibrating noise to sensitivityin private data analysis,2006, In Theory Ofcryptography conference
 Almost linear time density level set estima-tion via dbscan,2021, In AAAI
 Speed measurements of residential internet access,2012, In Nina Taftand Fabio Ricciato (eds
 The best constants in the khintchine inequality,1981, Studia Mathematica
 On the convergence of local descent methods in feder-ated learning,2019, arXiv preprint arXiv:1910
 A bound on tail probabilities for quadratic forms inindependent random variables,1971, The Annals of Mathematical Statistics
 A linear speedup analysis of distributed deep learning with sparseand quantized communication,2018, In Proceedings of the 32nd International Conference on NeuralInformation Processing Systems
 Faster dynamic matrix inverse forfaster lps,2021, In STOC
 Advancesand open problems in federated learning,2019, arXiv preprint arXiv:1912
 Scaffold: Stochastic controlled averaging for federated learning,2020, InInternational Conference on Machine Learning
 First analysis of local gd on hetero-geneous data,2019, arXiv preprint arXiv:1909
 Tighter theory for local sgd on identi-cal and heterogeneous data,2020, In International Conference on Artificial Intelligence and Statistics
 Uber dyadische bruche,1923, Mathematische Zeitschrift
 Federated learning: Strategies for improving communication efficiency,2016, arXivpreprint arXiv:1610
 Adaptive estimation of a quadratic functional by model selec-tion,2000, Annals of Statistics 
 Solving empirical risk minimization in the current matrixmultiplication time,2019, In COLT
 Multi-site fmri analysis using privacy-preserving federated learning and domain adaptation:Abide results,2020, Medical Image Analysis
 FedBN: Federated learn-ing on non-IID features via local batch normalization,2021, In International Conference on LearningRepresentations (ICLR)
 Faster ridge regression via the sub-sampled randomized hadamard transform,2013, In Advances in neural information processing systems
 Federated learning of deepnetworks using model averaging,2016, 02 2016
 Improved approximation algorithms for large matrices via random projections,2006, InProceedings of 47th Annual IEEE Symposium on Foundations of Computer Science (FOCS)
 Privacy-preserving deep learning,2015, In Proceedings of the 22ndACM SIGSAC conference on computer and communications security
 Oblivious sketching-based central path method for solving linear pro-gramming problems,2021, In 38th International Conference on Machine Learning (ICML)
 LoW rank approximation with entrywise '1 -normerror,2017, In Proceedings of the 49th Annual Symposium on the Theory of Computing (STOC)
 Relative error tensor low rank approximation,2019, InSODA
 Local sgd converges fast and communicates little,2018, arXiv preprintarXiv:1805
 The error-feedback framework: Better rates forsgd with delayed gradients and compressed communication,2019, arXiv preprint arXiv:1909
 Improved analysis of the subsampled randomized hadamard transform,2011, Advances inAdaptive Data AnaIySiS
 Planningwith general objective functions: Going beyond total rewards,2020, In Annual Conference on NeuralInformation ProceSSing SyStemS (NeurIPS)
 Sketching as a tool for numerical linear algebra,2014, FoundationS and TrendS inTheoretical Computer Science
 Distributed low rank approximation of implicit functions of amatrix,2016, In 2016 IEEE 32nd International Conference on Data Engineering (ICDE)
 On the linear speedup analysis of communication efficient mo-mentum sgd for distributed non-convex optimization,2019, In International Conference on MachineLearning
 Parallel restarted sgd with faster convergence and lesscommunication: Demystifying why model averaging works for deep learning,2019, In ProceedingS ofthe AAAI Conference on Artificial Intelligence
 Federatedlearning with non-iid data,2018, arXiv preprint arXiv:1806
 LetX 〜Xk be a chi-squared distributed random variable with k degrees of freedom,2022, Each one haszero mean and σ2 variance
 Let R ∈ Rb×n denote a random Gaussian matrix as in Definition D,2022,2
 Suppose each f satisfies Assumption 3,2022,1 with μ ≥ 0
 Summing up Lemma F,2022,6 as t varies from 1 to T and k varies from 0 to K — 1
