title,year,conference
 Ms marco: A human generatedmachine reading comprehension dataset,2016, arXiv preprint arXiv:1611
 Emerging properties in self-supervised vision transformers,2021, arXiv preprintarXiv:2104
 Pre-training tasksfor embedding-based large-scale retrieval,2020, arXiv preprint arXiv:2002
 BERT: Pre-training of deepbidirectional transformers for language understanding,2019, In Proc
 Cert: Contrastiveself-supervised learning for language understanding,2020, arXiv preprint arXiv:2005
 Unsupervised corpus aware language model pre-training for densepassage retrieval,2021, arXiv preprint arXiv:2108
 Declutr: Deep contrastive learning forunsupervised textual representations,2020, arXiv preprint arXiv:2006
 Realm: Retrieval-augmented language model pre-training,2020, arXiv preprint arXiv:2002
 Effi-ciently teaching an effective dense retriever with balanced topic aware sampling,2021, arXiv preprintarXiv:2104
 Poly-encoders: Transformerarchitectures and pre-training strategies for fast and accurate multi-sentence scoring,2019, arXiv preprintarXiv:1905
 Billion-scale similarity search with gpus,2019, IEEETransactions on Big Data
 Triviaqa: A large scale distantlysupervised challenge dataset for reading comprehension,2017, In Proc
 ASAM: adaptive sharpness-aware minimization for scale-invariant learning of deep neural networks,2021, arXiv preprintarXiv:2102
 Albert: A lite bert for self-supervised learning of language representations,2019, arXiv preprintarXiv:1909
 Pre-training via paraphrasing,2020, arXiv preprint arXiv:2006
 Pretrained transformers for text ranking: Bert andbeyond,2020, arXiv preprint arXiv:2010
 Introduction to informationretrieval,2008, Cambridge university press
 Distributed representationsof words and phrases and their compositionality,2013, In Advances in neural information processingsystems
 An introduction to neural information retrieval,2018, Foundationsand TrendsÂ® in Information Retrieval
 Passage re-ranking with bert,2019, arXiv preprintarXiv:1901
 From doc2query to doctttttquery,2019, Online preprint
 Exploring the limits of transfer learning with a unified text-to-texttransformer,2019, arXiv preprint arXiv:1910
 Okapi at TREC-3,1995, NIST Special Publication Sp
 Beir: Aheterogenous benchmark for zero-shot evaluation of information retrieval models,2021, arXiv preprintarXiv:2104
 Fever: a large-scaledataset for fact extraction and verification,2018, arXiv preprint arXiv:1803
 Unsupervised feature learning via non-parametric instance discrimination,2018, In Proceedings of the IEEE conference on computer visionand pattern recognition
 Clear: Contrastivelearning for sentence representation,2020, arXiv preprint arXiv:2012
 Approximate nearest neighbor negative contrastive learning for dense textretrieval,2020, arXiv preprint arXiv:2007
