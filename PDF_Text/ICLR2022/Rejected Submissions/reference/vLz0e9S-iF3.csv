title,year,conference
 On the stable equilibrium points of gradient systems,2006, Syst
 High-dimensional dynamics of gener-alization error in neural networks,2020, Neural Netw
 A continuous-time view of early stopping for leastsquares regression,2019, In The 22nd International Conference on Artificial Intelligence and Statistics
 Matrix Analysis,1997, Springer
 Training a 3-node neural network is np-complete,1992, NeuralNetworks
 Sgd learns over-parameterized networks that provably generalize on linearly separable data,2017, arXiv preprintarXiv:1710
 Escaping saddles withstochastic gradients,2018, In Proceedings of the 35th International Conference on Machine Learning
 Amethod for scribe distinction in medieval manuscripts using page layout features,2011, In InternationalConference on Image Analysis and Processing
 Large Deviations Techniques and Applications,2010, Springer BerlinHeidelberg
 Sharp minima can generalizefor deep nets,2017, In International Conference on Machine Learning
 Essentially no barriersin neural network energy landscape,2018, In International conference on machine learning
 Learning one-hidden-layer neural networks with landscapedesign,2018, In International Conference on Learning Representations
 Stopped diffusion processes: Boundary corrections andovershoot,2010, Stochastic Process
 Simplifying neural nets by discovering flat minima,1995, InAdvances in neural information processing systems
 Flat minima,1997, Neural computation
 On the diffusion approximation of noncon-vex stochastic gradient descent,2017, arXiv preprint arXiv:1705
 Quasi-potential as an implicit regularizerfor the loss function in the stochastic gradient descent,2019, arXiv preprint arXiv:1901
 Three factors influencing minima in sgd,2017, arXiv preprint arXiv:1711
 Fantasticgeneralization measures and where to find them,2019, arXiv preprint arXiv:1912
 On large-batch training for deep learning: Generalization gap and sharp minima,2016, arXivpreprint arXiv:1609
 Adam: A method for stochastic optimization,2014, arXiv preprintarXiv:1412
 Imagenet classification with deep convo-lutional neural networks,2012, Adv
 Visualizing the loss land-scape of neural nets,2018, Advances in Neural Information Processing Systems
 A variational analysis of stochastic gradientalgorithms,2016, In Maria Florina Balcan and Kilian Q Weinberger (eds
 Revisiting small batch training for deep neural networks,2018, arXivpreprint arXiv:1804
 Strategies fortraining large scale neural network language models,2011, In 2011 IEEE Workshop on AutomaticSpeech Recognition Understanding
 First exit time analysisof stochastic gradient descent under heavy-tailed gradient noise,2019, arXiv preprint arXiv:1906
 Non-gaussianity ofstochastic gradient noise,2019, arXiv preprint arXiv:1910
 Non-convex learning via stochasticgradient langevin dynamics: a nonasymptotic analysis,2017, In Satyen Kale and Ohad Shamir (eds
 Empirical analysis ofthe hessian of over-parametrized neural networks,2017, arXiv preprint arXiv:1706
 Onthe heavy-tailed theory of stochastic gradient descent for deep neural networks,2019, arXiv preprintarXiv:1912
 A Tail-Index analysis of stochastic gra-dient noise in deep neural networks,2019, In Kamalika Chaudhuri and Ruslan Salakhutdinov (eds
 A bayesian perspective on generalization and stochastic gradientdescent,2017, arXiv preprint arXiv:1710
 Ordinary differential equations and dynamical systems,2000, Grad
 Normalized flat minima: Exploring scale in-variant definition of flat minima for neural networks using pac-bayesian analysis,2020, In InternationalConference on Machine Learning
 Towards understanding generalization of deep learning: Perspectiveof loss landscapes,2017, arXiv preprint arXiv:1706
 How sgd selects the global minima in over-parameterized learning: Adynamical stability perspective,2018, Advances in Neural Information Processing Systems
 A diffusion theory for deep learning dynamics:Stochastic gradient descent exponentially favors flat minima,2020, In International Conference onLearning Representations
 Global convergence of langevin dynamicsbased algorithms for nonconvex optimization,2017, arXiv preprint arXiv:1707
 The anisotropic noise in stochas-tic gradient descent: Its behavior of escaping from sharp minima and regularization effects,2019, InInternational Conference on Machine Learning
