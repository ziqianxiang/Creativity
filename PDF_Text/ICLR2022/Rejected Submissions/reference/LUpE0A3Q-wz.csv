title,year,conference
 Bayesian Posterior Sampling via StochasticGradient Fisher Scoring,2012, In Proc
 Distributed Stochastic Gradient MCMC,2014, InInternational Conference on Machine Learning (ICML)
 Federated Learningvia Posterior Averaging: A New Perspective and Practical Algorithms,2021, In ICLR
 On the Outsized Importance of Learning Rates in Local UpdateMethods,2020, arXiv:2007
 On the Convergence of Stochastic Gradient MCMCAlgorithms with High-order Integrators,2015, In Advances in Neural Information Processing Systems(NeurIPS)
 Stochastic GradientMCMC with Stale Gradients,2016, In Advances in Neural Information Processing Systems (NeurIPS)
 Stochastic Gradient Hamiltonian Monte Carlo,2014, InProc
 Parallel and Distributed MCMC via ShepherdingDistributions,2018, In AISTAT
 User-friendly Guarantees for the Langevin Monte Carlowith Inaccurate Gradient,2019, Stochastic Processes and their Applications
 Large Scale Distributed Deep Networks,2012, InAdvances in neural information processing systems (NeurIPS)
 Non-Convex Learning via ReplicaExchange Stochastic Gradient MCMC,2020, In Proc
 Sampling from a Strongly Log-concave Distribution with theUnadjusted Langevin Algorithm,2016, arXiv:1605
 Decentralized StochasticGradient Langevin Dynamics and Hamiltonian Monte Carlo,2021, arXiv:2007
 FL-NTK: A Neural Tangent Kernel-basedFramework for Federated Learning Convergence Analysis,2021, In International Conference on MachineLearning (ICML)
 TextHide: Tackling DataPrivacy in Language Understanding Tasks,2020, In EMNLP
 Neural Tangent Kernel: Convergence and Gener-alization in Neural Networks,2018, In Advances in neural information processing systems (NeurIPS)
 Scaffold: Stochastic Controlled Averaging for Federated Learning,2020, InInternational Conference on Machine Learning
 First Analysis of Local GD onHeterogeneous Data,2019, arXiv:1909
 Decentralized Stochastic Optimizationand Gossip Algorithms with Compressed Communication,2019, In Proc
 Communication-Efficient Stochastic Gradient MCMC for Neural Networks,2019, In Proc
 On the Convergence ofFedAvg on Non-IID Data,2020, In Proc
 Stochastic Runge-Kutta AcceleratesLangevin Monte Carlo and Beyond,2019, In Advances in Neural Information Processing Systems(NeurIPS)
 Irreversible samplers from jump and continuous Markovprocesses,2018, Stat
 A Complete Recipe for Stochastic Gradient MCMC,2015, InNeural Information Processing Systems (NeurIPS)
 Sampling Can BeFaster Than Optimization,2019, PNAS
 Convex Optimization with Unbounded Nonconvex Oraclesusing Simulated Annealing,2018, In Proc
 Federated Learning of DeepNetworks using Model Averaging,2016, 2016
 Scalable and Robust Bayesian Inference via theMedian Posterior,2014, In International Conference on Machine Learning (ICML)
 Fedsplit: An Algorithmic Framework for Fast FederatedOptimization,2020, arXiv:2005
 Non-convex Learning via StochasticGradient Langevin Dynamics: a Nonasymptotic Analysis,2017, In Conference on Learning Theory
 The randomized midpoint method for log-concave sampling,2019, In Advances inNeural Information Processing Systems
 Privacy-preserving Deep Learning,2015, In SIGSAC conference oncomputer and communications security (CCS)
 Local SGD Converges Fast and Communicates Little,2019, arXiv:1805
 Privacy for Free: Posterior Sampling andStochastic Gradient Monte Carlo,2015, In ICML
 Bayesian Learning via Stochastic Gradient Langevin Dynamics,2011, InInternational Conference on Machine Learning
 Minibatch vs Local SGD for Heteroge-neous Distributed Learning,2020, arXiv:2006
 Parallel Restarted SGD with Faster Convergence and LessCommunication: Demystifying Why Model Averaging Works for Deep Learning,2019, In In Proc
 A Hitting Time Analysis of Stochastic GradientLangevin Dynamics,2017, In Proc
 By assumption A,2022,3
