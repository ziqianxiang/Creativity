title,year,conference
 The benefits of over-parameterization at initialization in deeprelu networks,2019, arXiv PrePrint arXiv:1901
 k-means++: The advantages of careful seeding,2006, TechnicalrePort
 From hard to soft: Understanding deep network nonlin-earities via vector quantization and statistical inference,2019, In International COnference on LearningRePreSentations
 Greedy layerwise learning can scaleto imagenet,2019, In International conference on machine Iearning
 A comparative study of efficient initial-ization methods for the k-means clustering algorithm,2013, ExPert SyStemS With applications
 Towards efficient model com-pression via learned global ranking,2020, In PrOceedingS of the IEEE COnference on COmPUter ViSiOnand Pattern Recognition
 A guide to deeplearning in healthcare,2019, NatUre medicine
 Rigging the lottery:Making all tickets winners,2019, arXiv PrePrint arXiv:1911
 Understanding the difficulty of training deep feedforward neuralnetworks,2010, In PrOceedingS of the thirteenth international conference on artificial intelligence andStatiStics
 Alternatives to the k-means algorithm that find better cluster-ings,2002, In PrOceedingS of the eleventh international conference on InfOrmatiOn and knowledgemanagement
 Complexity of linear regions in deep networks,2019, arXiv PreprintarXiv:1901
 Delving deep into rectifiers: Surpassinghuman-level performance on imagenet classification,2015, In Proceedings of the IEEE internationalconference on COmPUter ViSion
 Soft filter pruning for acceleratingdeep convolutional neural networks,2018, arXiv PrePrint arXiv:1808
 Densely connectedconvolutional networks,2017, In PrOCeedingS of the IEEE COnference on COmPUter ViSiOn and Patternrecognition
 An efficient k-means clustering algorithm: Analysis and implementation,2002, IEEEtransactions on Pattern analysis and machine intelligence
 Effect of depth and width on localminima in deep learning,2019, Neural computation
 Optimal brain damage,1990, In AdVanCeS in neuralinformation PrOCeSSing systems
 SNIP: SINGLE-SHOT NETWORKPRUNING BASED ON CONNECTION SENSITIVITY,2019, In International COnference on LearningRePreSentations
 Halo: Hardware-aware learning to optimize,2020, In Proceedings of the EUrOPean COnference on COmPUter ViSiOn(ECCV)
 Bad global minima exist and sgdcan reach them,2019, arXiv PrePrint arXiv:1906
 Learn-ing efficient convolutional networks through network slimming,2017, In Proceedings of the IEEEInternational Conference on CompUter Vision
 Rethinking the valueof network pruning,2019, In International COnference on Learning RePresentations
 Thinet: A filter level pruning method for deep neuralnetwork compression,2017, In Proceedings of the IEEE international conference on computer vision
 Rectifier nonlinearities improve neural net-work acoustic models,2013, In Proc
 All you need is a good init,2015, arXiv PrePrint arXiv:1511
 Sharp bounds for the number of regions of maxoutnetworks and vertices of minkowski sums,2021, arXiv PrePrint arXiv:2104
 Gaussian mixture models,2009, Encyclopedia of biometrics
 An overview of gradient descent optimization algorithms,2016, arXiv PrePrintarXiv:1609
 Nonlinear principal component analy-sis: neural network models and applications,2008, In PrinciPaI manifolds for data ViSUaIizatiOn anddimension reduction
 Mastering the game of gowithout human knowledge,2017, nature
 Sanity-checking pruning methods: Random tickets can win the jackpot,2020, arXiv PrePrintarXiv:2009
 Pruning neural networkswithout any data by iteratively conserving synaptic flow,2020, arXiv PrePrint arXiv:2006
 Picking winning tickets before training bypreserving gradient flow,2020, arXiv PrePrint arXiv:2002
 Drawing early-bird tickets: Toward more efficienttraining of deep networks,2020, In International Conference on Learning Representations
 Residual dense network forimage super-resolution,2018, In Proceedings of the IEEE conference on computer Vision and Patternrecognition
 An improved analysis of training over-parameterized deep neuralnetworks,2019, In AdvanceS in NeuraI InfOrmatiOn PrOceSSing Systems
