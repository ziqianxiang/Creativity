title,year,conference
 Stochastic variance reduction for variational inequality methods,2021, arXivpreprint arXiv:2102
 Theconvergence of sparsified gradient methods,2018, In Advances in Neural Information Processing Systems
 On the optimization of deep networks: Implicit acceleration byoverparameterization,2018, In Proceedings of the 35th International Conference on Machine Learning (ICML)
 Convex Analysis and Monotone Operator Theory in HilbertSpaces,2017, Springer
 On biased compression fordistributed learning,2020, arXiv preprint arXiv:2002
 Decentralized local stochastic extra-gradient for variational inequalities,2021, arXiv preprintarXiv:2106
 Distributed saddle-pointproblems under similarity,2021, arXiv preprint arXiv:2107
 8-bit approximations for parallelism in deep learning,2015, ICLR
 Distributed deep learning in opencollaborations,2021, CoRR
 Improved complexity bounds in wasserstein barycenter problem,2021, InInternational Conference on Artificial Intelligence and Statistics
 Finite-Dimensional Variational Inequalities and ComplementarityProblems,2003, Springer Series in Operations Research
 Switch transformers: Scaling to trillion parameter models withsimple and efficient sparsity,2021, arXiv preprint arXiv:2101
 Communication efficientdistributed approximate Newton method,2020, In IEEE International Symposium on Information Theory (ISIT)
 A variational inequal-ity perspective on generative adversarial networks,2019, In International Conference on Learning Representations
 Generative adversarial networks,2014, In Neural Information Processing Systems
 MARINA: Faster non-convexdistributed learning with compression,2021, In 38th International Conference on Machine Learning
 Naturalcompression for distributed deep learning,2019, arXiv preprint arXiv:1905
 Stochasticdistributed learning with gradient quantization and variance reduction,2019, arXiv preprint arXiv:1904
 Efficient algorithms for federated saddlepoint optimization,2021, arXiv preprint arXiv:2102
 SCAF-FOLD: Stochastic controlled averaging for on-device federated learning,2020, In International Conference onMachine Learning (ICML)
 The extragradient method for finding saddle points and other problems,1976, Matecon
 Albert:A lite bert for self-supervised learning of language representations,2020, In International Conference on LearningRepresentations
 Deep gradient compression: Reducing thecommunication bandwidth for distributed training,2018, In International Conference on Learning Representations
 Adversarialtraining for large neural language models,2020, arXiv preprint arXiv:2004
 Roberta: A robustly optimized bert pretraining approach,2019, ArXiv
 Towards deeplearning models resistant to adversarial attacks,2017, arXiv preprint arXiv:1706
 Towards deeplearning models resistant to adversarial attacks,2018, In International Conference on Learning Representations
 Federated learning of deepnetworks using model averaging,2016, arXiv preprint arXiv:1602
 Distributed learning withcompressed gradient differences,2019, arXiv preprint arXiv:1901
 1-bit stochastic gradient descent and its applicationto data-parallel distributed training of speech dnns,2014, In Fifteenth Annual Conference of the InternationalSpeech Communication Association
 Fast convergence of regularizedlearning in games,2015, In Neural Information Processing Systems
 Attention is all you need,2017, In I
 Glue: A multi-task benchmark and analysis platform for natural language understanding,2018, arXiv preprint arXiv:1804
 Dual averaging method for solving multi-agent saddle-point problemswith quantized information,2014, Transactions of the Institute of Measurement and Control
 Freelb: Enhanced adversarialtraining for natural language understanding,2019, arXiv preprint arXiv:1909
