title,year,conference
 An introduction tomcmc for machine learning,2003, Machine learning
 Empirical supremum rejection sampling,2002, Biometrika
 Understanding the metropolis-hastings algorithm,1995, TheAmerican Statistician
 Residual energy-based models for text generation,2020, In 8th International Conference on Learning Representations
 Sampling-based minimum bayes risk decoding for neural machinetranslation,2021, CoRR
 Hierarchical neural story generation,2018, In Proceedingsof the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: LongPapers)
 Exposing the implicit energy networksbehind masked language models via metropolis-hastings,2021, CoRR
 Oops itook a gradient: Scalable sampling for discrete distributions,2021, In Marina Meila and Tong Zhang(eds
 Noise-contrastive estimation: A new estimation principlefor Unnormalized statistical models,2010, In Yee Whye Teh and Mike Titterington (eds
 Joint energy-based modeltraining for better calibrated natural language understanding models,2021, In Proceedings of the 16thConference of the European Chapter of the Association for Computational Linguistics: MainVolume
 Training products of experts by minimizing contrastive divergence,2002, NeuralComput
 The curious case of neural textdegeneration,2020, In International Conference on Learning Representations
 A distributional approach to controlledtext generation,2021, In International Conference on Learning Representations
 A Tutorial onEnergy-Based Learning,2006, In Predicting Structured Data
 Limitations ofautoregressive models and their alternatives,2021, In Kristina Toutanova
 Monte Carlo Strategies in Scientific Computing,2004, Springer Series in Statistics
 AccePt-reject methods,2018, In Independent RandomSampling Methods
 Do you have the right scissors? tailoring Pre-trained language models via monte-carlo methods,2020, In Dan Jurafsky
 Energy-based reranking: ImProving neural machine translation using energy-based models,2020, ArXiv
 ImPortance SamPling,2013, In Monte Carlo theory
 Correlation Functions and ComPuter Simulations,1981, Nucl
 Distributional ReinforcementLearning For Energy-Based Sequential Models,2019, ArXiv
 Languagemodels are unsuPervised multitask learners,2019, OpenAI Blog
 Convergence diagnostics for markov chain monte carlo,2020, Annual Review of StatisticsandItsApplication
 The woman workedas a babysitter: On biases in language generation,2019, In Kentaro Inui
 Exploring posterior distributions using markov chains,1992, Technical report
 Various techniques used in connection with random digits,1951, National Bureauof Standards Applied Math Series
 Informed proposals for local MCMC in discrete spaces,2017, arXiv:1711
 In the special case that a global Î² s,2004,t
