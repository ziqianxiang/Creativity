title,year,conference
 On theopportunities and risks of foundation models,2021, arXiv Preprint arXiv:2108
 Language models are few-shot learners,2020, In HugoLarochelle
 Evaluating large languagemodels trained on code,2021, arXiv Preprint arXiv:2107
 Generating long sequences withsparse transformers,2019, arXiv Preprint arXiv:1904
 BERT: Pre-trainingof deep bidirectional transformers for language understanding,2019, In PrOCeedingS of the 2019COnferenCe of the North AmeriCan ChaPter of the ASSOCiatiOn for COmPUtatiOnal LingUiStics:HUman LangUage Technologies
 EFLOPS: algorithm and system co-design fora high performance distributed training platform,2020, In IEEE International SymPOsiUm on HighPerfOrmanCe COmPUter Architecture
 Unified langUage model pre-training for natUral langUage Un-derstanding and generation,2019, In Hanna M
 Switch transformers: Scaling to trillionparameter models with simple and efficient sparsity,2021, arXiv PrePrint arXiv:2101
 Whale: Scaling deep learning model training to the trillions,2020, arXivPrePrint arXiv:2011
 Scaling laws for neUral langUagemodels,2020, arXiv PrePrint arXiv:2001
 Gshard: Scaling giant models with condi-tional COmPUtatiOn and aUtomatic sharding,2020, arXiv PrePrint arXiv:2006
 M6: A Chinese mUltimodal pretrainer,2021, arXiv PrePrintarXiv:2103
 Roberta: A robUstly optimized BERTpretraining approach,2019, arXiv PrePrint arXiv:1907
 Pointer sentinel mixtUremodels,2017, In 5th InternatiOnal COnferenCe on Learning RePreSentations
 Efficient large-scale IangUage model training on gpU clUSters,2021, arXiv PrePrintarXiv:2104
 The LAMBADAdataset: Word prediction reqUiring a broad discoUrse context,2016, In PrOCeedingS of the 54thAnnUal Meeting of the ASSOCiatiOn for COmPUtatiOnal LingUiStiCS (VolUme 1: Long Papers)
 Carbon emissions and large neural networktraining,2021, arXiv Preprint arXiv:2104
 Learning transferable visual models from natural language supervision,2021, InProceedings of the 38th International Conference on Machine Learning
 Zero: Memory optimiza-tions toward training trillion parameter models,2020, In SC20: InternatiOnal Conference for HighPerfOrmanCe Computing
 Zero-infinity: Breaking the gpu memory wall for extreme scale deep learning,2021, arXiv preprintarXiv:2104
 Zero-shot text-to-image generation,2021, arXiv preprint arXiv:2102
 Zero-offload: Democratizing billion-scale modeltraining,2021, arXiv preprint arXiv:2101
 Hash layers for largesparse models,2021, arXiv preprint arXiv:2106
 Turing-nlg: A 17-billion parameter language model by microsoft,2020, MiCrOSOftReSearCh Blog
 Outrageously large neural networks: The sparsely-gated mixture-of-experts layer,2017, In 5th International COnferenCe on Learning RepreSentations
 Mesh-tensorflow: Deep learning for supercomputers,2018, In AdVanceSin NeuraI InfOrmatiOn Processing SyStemS 31: AnnUal COnferenCe on NeUral InfOrmatiOnProcessing SyStemS 2018
 Megatron-lm: Training multi-billion parameter language models using modelparallelism,2019, arXiv Preprint arXiv:1909
 On layer normalization in the transformer architec-ture,2020, In Proceedings of the 37th International Conference on Machine Learning
 mT5: A massively multilingual pre-trained text-to-texttransformer,2021, In Proceedings of the 2021 Conference of the NOrth AmeriCan Chapter of theASSOCiatiOn for Computational LingUiStics: HUman LangUage Technologies
 M6-t: Exploring sparse expert models and beyond,2021, arXivpreprint arXiv:2105
 Large batch optimizationfor deep learning: Training BERT in 76 minutes,2020, In 8th International COnference on LearningRepreSentations
 Pangu-Î±: Large-scale autoregressive pretrainedchinese language models with auto-parallel computation,2021, arXiv preprint arXiv:2104
 Accelerating training of transformer-based language modelswith progressive layer dropping,2020, In Hugo Larochelle
 Cpm-2: Large-scale cost-effective pre-trained languagemodels,2021, arXiv preprint arXiv:2106
 Aligning books and movies: Towards story-like visual explanationsby watching movies and reading books,2015, In 2015 IEEE InternatiOnal COnferenCe on COmpUterViSiOn
