title,year,conference
 Spatio-temporal models for estimatingclick-through rate,2009, In WWW
 Neural machine translation by jointlylearning to align and translate,2016, In arXiv:1409
 An attentive survey ofattention models,2021, In arXiv:1904
 Generating long sequences with sparsetransformers,2019, In arXiv:1904
 Attention-over-attention neural networks for reading comprehension,2016, In arXiv:1607
 Neural turing machines,2014, In arXiv:1410
 Session-based rec-ommendations with recurrent neural networks,2016, In ICLR
 Long short-term memory,1997, In Neural computation
 Self-attentive sequential recommendation,2018, In CIKM
 Adam: A method for stochastic oPtimization,2015, In YoshuaBengio and Yann LeCun (eds
 Reformer: The efficient transformer,2020, In ICLR
 Collaborative filtering with temPoral dynamics,2009, In KDD
 Ask me anything: Dynamic memory networks for naturallanguage Processing,2016, In ICML
 Sac: Acceleratingand structuring self-attention via sParse adaPtive connection,2020, In NeurIPS
 A structured self-attentive sentence embedding,2017, In arXiv:1703
 Image-based rec-ommendations on styles and substitutes,2015, In Proceedings of the 38th International ACM SIGIRConference on Research and Development in Information Retrieval
 Practice on long sequential userbehavior modeling for click-through rate Prediction,2019, In Proceedings of the 25th ACM SIGKDD
 User behavior retrievalfor click-through rate Prediction,2020, In SIGIR
 Ask me even more: Dynamicmemory tensor networks,2017, In arXiv:1703
 Linformer: Self- attention withlinear complexity,2020, In arXiv:2006
 Memory networks,2014, In arXiv:1410
 Personal rec-ommendation using deep recurrent neural networks in netease,2016, In IEEE International Conferenceon Data Engineering
