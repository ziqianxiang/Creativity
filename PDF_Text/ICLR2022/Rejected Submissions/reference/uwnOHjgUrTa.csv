title,year,conference
 Neural architecture search: Asurvey,2019, J
 Learned step size quantization,2019, arXiv preprint arXiv:1902
 Neuflow: A runtime reconfigurable dataflow processor for vision,2011, In Cvpr2011 Workshops
 Fractional max-pooling,2014, CoRR
 Deep learn-ing with limited numerical precision,2015, In International Conference on Machine Learning
 Quantized guided pruning for efficient hardware implementations of convolutionalneural networks,2018, arXiv preprint arXiv:1812
 Attention based pruning for shift networks,2019, arXiv preprintarXiv:1905
 Deep residual learning forimage recognition,2016, In Proceedings of the IEEE conference on computer vision and patternrecognition
 Lit: Block-wise inter-mediate representation training for model compression,2018, arXiv preprint arXiv:1810
 Optimal brain damage,1990, In Advances inneural information processing systems
 Gradient-based learningapplied to document recognition,1998, Proceedings of the IEEE
 Visualizing theloss landscape of neural nets,2017, arXiv preprint arXiv:1712
 Bitprun-ing: Learning bitlengths for aggressive and accurate quantization,2020, arXiv preprintarXiv:2002
 Differentiablemask for pruning convolutional and recurrent networks,2020, In 2020 17th Conference onComputer and Robot Vision (CRV)
 Xnor-net: Im-agenet classification using binary convolutional neural networks,2016, In European conferenceon computer vision
 Fitnets: Hints for thin deep nets,2014, arXiv preprint arXiv:1412
 Very deep convolutional networks for large-scaleimage recognition,2014, CoRR
 Rethinking the inception architecture for computer vision,2015, arXiv preprintarXiv:1512
 Differentiable quantization ofdeep neural networks,2019, arXiv preprint arXiv:1905
 Attention is all you need,2017, In Advances in NeuralInformation Processing Systems
