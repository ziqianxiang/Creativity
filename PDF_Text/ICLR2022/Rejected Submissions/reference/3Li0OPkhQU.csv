title,year,conference
 Fine-grained analysis ofoptimization and generalization for overparameterized two-layer neural networks,2019, In InternationalConference on Machine Learning
 Efficient algorithms for learning depth-2neural networks with general relu activations,2021, arXiv preprint arXiv:2107
 Theory of classification: A survey ofsome recent advances,2005, ESAIM: probability and statistics
 Globally optimal gradient descent for a convnet with gaussianinputs,2017, In International Conference on Machine Learning
 An optimization and generalization analysis for max-poolingnetworks,2020, arXiv preprint arXiv:2002
 Generalization bounds of stochastic gradient descent for wide and deepneural networks,2019, In Advances in Neural Information Processing Systems
 Learning parities with neural networks,2020, Advances in NeuralInformation Processing Systems
 Gradient descent learnsone-hidden-layer cnn: Donâ€™t be afraid of spurious local minima,2018, In International Conference onMachine Learning
 Gradient descent finds globalminima of deep neural networks,2019, In International Conference on Machine Learning
 Improved learning of one-hidden-layer convolutional neural networkswith overlaps,2018, arXiv preprint arXiv:1805
 Gradient descent provably optimizesover-parameterized neural networks,2018, International Conference on Learning Representations
 Learning two-layer neural networks withsymmetric inputs,2018, arXiv preprint arXiv:1810
 Clustering to minimize the maximum intercluster distance,1985, Theoretical computerscience
 Adaptive learning rates for support vector machines working ondata with low intrinsic dimension,2020, arXiv preprint arXiv:2003
 Polylogarithmic width suffices for gradient descent to achievearbitrarily small test error with shallow relu networks,2019, In International Conference on LearningRepresentations
 Exactly computing the local lipschitz constant of relunetworks,2020, arXiv preprint arXiv:2003
 On the computational efficiency of training neuralnetworks,2014, In Advances in Neural Information Processing Systems
 A provably correct algorithm for deep learning that actuallyworks,2018, arXiv preprint arXiv:1803
 Quantifying the benefit of usingdifferentiable learning over tangent kernels,2021, arXiv preprint arXiv:2103
 End-to-end learning of a convolutional neural network viadeep tensor decomposition,2018, arXiv preprint arXiv:1805
 Weighted sums of random kitchen sinks: replacing minimizationwith randomization in learning,2008, In Nips
 Random features for large-scale kernel machines,2007, In NIPS
 Understanding machine learning: From theory toalgorithms,2014, Cambridge university press
 Kernel methods for pattern analysis,2004, Cambridgeuniversity press
 The unreasonable effectivenessof patches in deep convolutional kernels methods,2021, arXiv preprint arXiv:2101
 The design of approximation algorithms,2011, Cambridgeuniversity press
 Fashion-mnist: a novel image dataset for benchmarkingmachine learning algorithms,2017, arXiv preprint arXiv:1708
 Adadelta: an adaptive learning rate method,2012, arXiv preprint arXiv:1212
