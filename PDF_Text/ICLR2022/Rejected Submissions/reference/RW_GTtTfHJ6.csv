title,year,conference
 Pomdps in continuous time and discretespaces,2020, In H
 Bandits with unobserved confounders: Acausal approach,2015, In NIPS
 Off-policy evaluation in infinite-horizon reinforcement learning with latent confounders,2021, In AISTATS
 Acting optimally inpartially observable stochastic domains,1994, In AAAI
 Causal confusion in imitation learning,2019, InNeurIPS
 Statistical comparisons of classifiers over multiple data sets,2006, J
 Combining online and offline knowledge in uct,2007, In ICML
 Reinforcement learning and causal models,2017, In Michael R
 Mastering atari withdiscrete world models,2021, In ICLR
 Pearlâ€™s calculus of intervention is complete,2006, In UAI
 Removing hidden confounding by experi-mental grounding,2018, In NeurIPS
 Adam: A method for stochastic optimization,2015, In ICLR
 Stabilizing off-policy q-learningvia bootstrapping error reduction,2019, In NeurIPS
 Deconfoundingreinforcement learning in observational settings,2018, arXiv preprint
 Nonparametric bounds on treatment effects,1990, The American Economic Review
 Causal inference in statistics: An overview,2009, Statistics Surveys
 The do-calculus revisited,2012, In UAI
 Zero-shot text-to-image generation,2021, arXiv preprint
 Identification of joint interventional distributions in recursivesemi-markovian causal models,2006, In AAAI
 Probabilistic Conditional Independence Structures,2005, Springer
 Off-policy evaluation in partially observableenvironments,2020, In AAAI
 Transfer learning in multi-armed bandits: A causalapproach,2017, In IJCAI
 Near-optimal reinforcement learning in dynamic treatmentregimes,2019, In NeurIPS
 Designing optimal dynamic treatment regimes: A causalreinforcement learning approach,2020, In Hal DaUme In and Aarti Singh
 Bounding causal effects on continuous outcomes,2021, InAAAI
