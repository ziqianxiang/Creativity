title,year,conference
 Language models arefew-shot learners,2020, arXiv preprint arXiv:2005
 Bert: Pre-training of deepbidirectional transformers for language understanding,2019, In Proceedings of the 2019 Conference ofthe North American Chapter of the Association for Computational Linguistics: Human LanguageTechnologies
 Making pre-trained language models better few-shotlearners,2021, In Association for Computational Linguistics (ACL)
 Bertese: Learning to speak to bert,2021, In Proceedingsof the 16th Conference of the European Chapter of the Association for Computational Linguistics:Main Volume
 Knowledge-able prompt-tuning: Incorporating knowledge into prompt verbalizer for text classification,2021, arXivpreprint arXiv:2108
 Albert: A lite bert for self-supervised learning of language representations,2019, In InternationalConference on Learning Representations
 Prefix-tuning: Optimizing continuous prompts for generation,2021, InProceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)
 Roberta: A robustly optimized bert pretrainingapproach,2019, arXiv preprint arXiv:1907
 Cutting down on prompts and parameters: Simple few-shot learning with language mod-els,2021, arXiv preprint arXiv:2106
 Textclassification using label names only: A language model self-training approach,2020, In Proceedings ofthe 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)
 Multitask prompted trainingenables zero-shot task generalization,2021, arXiv preprint arXiv:2110
 Exploiting cloze-questions for few-shot text classification andnatural language inference,2021, In Proceedings of the 16th Conference of the European Chapter ofthe Association for Computational Linguistics: Main Volume
 It's not just size that matters: Small language models are alsofew-shot learners,2021, In Proceedings of the 2021 Conference of the North American Chapter ofthe Association for Computational Linguistics: Human Language Technologies
 Automatically identifying words that can serveas labels for few-shot text classification,2020, In Proceedings of the 28th International Conference onComputational Linguistics
 Elicitingknowledge from language models using automatically generated prompts,2020, In Proceedings of the2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)
 Nsp-bert: A prompt-based zero-shot learnerthrough an original pre-training task-next sentence prediction,2021, arXiv preprint arXiv:2109
 Transformers: State-of-the-artnatural language processing,2020, In Proceedings of the 2020 Conference on Empirical Methods inNatural Language Processing: System Demonstrations
 Character-level convolutional networks for text clas-sification,2015, Advances in neural information processing systems
