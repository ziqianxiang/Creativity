title,year,conference
 Mine: mutual information neural estimation,2018, arXiv preprintarXiv:1801
 Deep clustering for unsu-pervised learning of visual features,2018, In Proceedings of the European Conference on ComputerVision (ECCV)
 A simple frameWork forcontrastive learning of visual representations,2020, In International conference on machine learning
 Speech recognition With deep recur-rent neural netWorks,2013, In 2013 IEEE international conference on acoustics
 Co-teaching: Robust training of deep neural netWorks With extremely noisy labels,2018, InAdvances in neural information processing systems
 Distilling the knoWledge in a neural netWork,2015, arXivpreprint arXiv:1503
 Learning deep representations by mutual information estimationand maximization,2018, arXiv preprint arXiv:1808
 Mentornet: Learning data-driven curriculum for very deep neural networks on corrupted labels,2018, In International Conferenceon Machine Learning
 Imagenet classification with deep convo-lutional neural networks,2012, In Advances in neural information processing systems
 Dividemix: Learning with noisy labels as semi-supervised learning,2020, In International Conference on Learning Representations
 Peer loss functions: Learning from noisy labels without knowing noiserates,2020, In International Conference on Machine Learning
 Deep predictive coding networks for video pre-diction and unsupervised learning,2016, arXiv preprint arXiv:1605
 Improved knowledge distillation via teacher assistant,2020, In Proceedings of the AAAIConference on Artificial Intelligence
 Learning withnoisy labels,2013, In Advances in neural information processing systems
 Self: Learning to filter noisy labels with self-ensembling,2020, In International Conference on Learning Representations
 Contrastive representationsfor label noise require fine-tuning,2021, arXiv preprint arXiv:2108
 Unsupervised learning of visual representations by solving jigsawpuzzles,2016, In European conference on computer vision
 Representation learning with contrastive predic-tive coding,2018, arXiv preprint arXiv:1807
 Makingdeep neural networks robust to label noise: A loss correction approach,2017, In Proceedings of theIEEE Conference on Computer Vision and Pattern Recognition
 Co-learning: Learning from noisy labels withself-supervision,2021, arXiv preprint arXiv:2108
 Symmetric cross en-tropy for robust learning with noisy labels,2019, In Proceedings of the IEEE International Conferenceon Computer Vision
 Learn-ing with noisy labels revisited: A study using real-world human annotations,2021, arXiv preprintarXiv:2110
 Searching to exploit memo-rization effect in learning with noisy labels,2020, In Proceedings of the 37th International Conferenceon Machine Learning
 Understandingdeep learning requires rethinking generalization,2016, arXiv preprint arXiv:1611
 Text understanding from scratch,2015, arXiv preprint arXiv:1502
 Self-paced robust learn-ing for leveraging clean labels in noisy data,2020, In Proceedings of the AAAI Conference on ArtificialIntelligence
 Deep mutual learning,2018, InProceedings of the IEEE Conference on Computer Vision and Pattern Recognition
 Generalized cross entropy loss for training deep neural networkswith noisy labels,2018, In Advances in neural information processing systems
 Con-trast to divide: Self-supervised pre-training for learning with noisy labels,2021, arXiv preprintarXiv:2103
 Clusterability as an alternative to anchor points whenlearning with noisy labels,2021, arXiv preprint arXiv:2102
