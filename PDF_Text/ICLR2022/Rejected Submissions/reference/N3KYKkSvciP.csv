title,year,conference
 Theory of reproducing kernels,1950, Trans
 Fine-grained analysis ofoptimization and generalization for overparameterized two-layer neural networks,2019, arXiv preprintarXiv:1901
 Fast learning rates for plug-in classifiers,2007, TheAnnals of statistics
 Interpolation of Operators,1988, Academic press
 On the inductive bias of neural tangent kernels,2019, arXiv preprintarXiv:1905
 Generalization bounds of stochastic gradient descent for wide anddeep neural networks,2019, Advances in Neural Information Processing Systems
 Reliable evaluation of adversarial robustness with an ensembleof diverse parameter-free attacks,2020, In International Conference on Machine Learning
 Exploring the role of loss functions in multiclassclassification,2020, In 2020 54th Annual Conference on Information Sciences and Systems (CISS)
 Mma training: Directinput space margin maximization through adversarial training,2018, arXiv preprint arXiv:1812
 Gradient descent provably optimizesover-parameterized neural networks,2018, arXiv preprint arXiv:1810
 Largemargin deep networks for classification,2018, arXiv preprint arXiv:1803
 Exploring deep neural networks via layer-peeled model: Minority collapse in imbalanced training,2021, Proceedings of the National Academyof Sciences
 Deep neural networks for estimation and infer-ence,2021, Econometrica
 Dissecting supervised con-Strastive learning,2021, In International Conference on Machine Learning
 On calibration of modern neuralnetworks,2017, In International Conference on Machine Learning
 Neural collapse under mse loss: Proximity to anddynamics on the central path,2021, arXiv preprint arXiv:2106
 Distilling the knowledge in a neural network,2015, arXivpreprint arXiv:1503
 Regularization matters: A nonparamet-ric perspective on overparametrized neural network,2021, In International Conference on ArtificialIntelligence and Statistics
 Evaluation of neural architectures trained with square loss vs cross-entropy in classification tasks,2020, arXiv preprint arXiv:2006
 Neural tangent kernel: Convergence and gen-eralization in neural networks,2018, In Advances in Neural Information Processing Systems
 Polylogarithmic width suffices for gradient descent to achieve arbi-trarily small test error with shallow ReLU networks,2019, arXiv preprint arXiv:1909
 Supervised contrastive learning,2020, arXiv preprintarXiv:2004
 Fast convergence rates of deep neural networks forclassification,2018, arXiv preprint arXiv:1812
 On the rate of convergence of local averaging plug-in classifica-tion rules under a margin condition,2007, IEEE Transactions on Information Theory
 Demystifying loss functionsfor classification,2020, 2020
 Learning overparameterized neural networks via stochastic gradientdescent on structured data,2018, In Advances in Neural Information Processing Systems
 Multiclass learningwith simplex coding,2012, arXiv preprint arXiv:1209
 Optimal rates for averaged stochastic gradient descent underneural tangent kernel regime,2020, arXiv preprint arXiv:2006
 Gradient descent can learn lessover-parameterized two-layer neural networks on classification problems,2019, arXiv preprintarXiv:1905
 Rethinking softmaxcross-entropy loss for adversarial robustness,2019, arXiv preprint arXiv:1905
 Prevalence of neural collapse during the terminalphase of deep learning training,2020, Proceedings of the National Academy of Sciences
 Generalization in deep network classifiers trained with the squareloss,2019, Technical report
 Nonparametric regression using deep neural networks with relu activationfunction,2020, The Annals of Statistics
 Evaluating model calibration in classification,2019, In The 22nd International Conference onArtificial Intelligence and Statistics
 Scattered Data Approximation,2004, Cambridge University Press
 Learning diverse anddiscriminative representations via the principle of maximal coding rate reduction,2020, Advances inNeural Information Processing Systems
 Wide residual networks,2016, arXiv preprintarXiv:1605
 mixup: Beyond empiricalrisk minimization,2017, arXiv preprint arXiv:1710
 Statistical behavior and consistency of classification methods based on convex riskminimization,2004, The Annals of Statistics
