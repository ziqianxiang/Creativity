title,year,conference
 Data dependent randomizedsmoothing,2020, arXiv preprint arXiv:2012
 Obfuscated gradients give a false sense of se-curity: Circumventing defenses to adversarial examples,2018, In International conference on machinelearning
 Evasion attacks against machine learning at test time,2013, In HendrikBlockeel
 Adversarial examples are not easily detected: Bypassing ten de-tection methods,2017, In Proceedings of the 10th ACM workshop on artificial intelligence and security
 Insta-rs: Instance-wise randomized smoothing for improved robustness and accuracy,2021, arXiv preprintarXiv:2103
 Certified adversarial robustness via randomizedsmoothing,2019, In International Conference on Machine Learning
 Robust physical-world attacks on deep learningvisual classification,2018, In Proceedings of the IEEE conference on computer vision and patternrecognition
 Analyzing accuracyloss in randomized smoothing defenses,2020, arXiv preprint arXiv:2003
 Explaining and harnessing adversarialexamples,2014, arXiv preprint arXiv:1412
 Expressivity of deep neural networks,2020, arXivpreprint arXiv:2007
 Formal guarantees on the robustness of a classifieragainst adversarial manipulation,2017, In I
 Adversarial machine learning at scale,2016, arXivpreprint arXiv:1611
 Certifiedrobustness to adversarial examples with differential privacy,2019, In 2019 IEEE Symposium on Securityand Privacy (SP)
 Certified adversarial robustness withadditive noise,2019, Advances in Neural Information Processing Systems
 Differentiable abstract interpretation for prov-ably robust neural networks,2018, In Jennifer Dy and Andreas Krause (eds
 Rethinking randomizedsmoothing for adversarial robustness,2020, arXiv preprint arXiv:2003
 Certified defenses against adversarial exam-ples,2018, In International Conference on Learning Representations
 Provably robust deep learning via adversarially trained smoothed classifiers,2019, InNeurIPS
 Intriguing properties of neural networks,2013, arXiv preprint arXiv:1312
 Lipschitz-margin training: Scal-able certification of perturbation invariance for deep neural networks,2018, In S
 Renyi divergence and kullback-leibler divergence,2014, IEEETransactions on Information Theory
 Pretrain-to-finetune adversarial trainingvia sample-wise randomized smoothing,2021, 2021
 Towards fast computation of certified robustness for ReLU networks,2018, In JenniferDy and Andreas Krause (eds
 Provable defenses against adversarial examples via the convex outeradversarial polytope,2018, In International Conference on Machine Learning
 Randomizedsmoothing of all shapes and sizes,2020, In International Conference on Machine Learning
 But how serious under-certification it is? Assume the case with a linear base classifier,2022, Imagine
 Is provided in Zhai et al,2020, (2020)
