title,year,conference
 Disentangling adaptive gradient methodsfrom learning rates,2020, arXiv preprint arXiv:2002
 Layer normalization,2016, arXiv preprintarXiv:1607
 Mirror descent and nonlinear projected subgradient methods for convexoptimization,2003, Operations Research Letters
 Learning long-term dependencies with gradient descent isdifficult,1994, IEEE transactions on neural networks
 High-performance large-scaleimage recognition without normalization,2021, In Proc
 In-place activated batchnorm for memory-optimized training of dnns,2018, In Proceedings of the IEEE Conference on Computer Vision andPattern Recognition
 End-to-end objectdetection with transformers,2020, In European Conference on Computer Vision
 A simple framework for contrastive learningof visual representations,2020, In Hal DaUma III and Aarti Singh (eds
 Batch normalization biases residual blocks towards the identity functionin deep networks,2020, Advances in Neural Information Processing Systems
 An image is worth 16x16words: Transformers for image recognition at scale,2021, In 9th International Conference on LearningRepresentations
 Adaptive subgradient methods for online learning and stochasticoptimization,2010, In COLT
 Composite objective mirror descent,2010, InCOLT
 Deep residual learning for image recognition,2016, In Proc
 Batch normalization: Accelerating deep network training by reducinginternal covariate shift,2015, In International conference on machine learning
 Adam: A method for stochastic optimization,2015, In International Conferenceon Learning Representations (ICLR)
 Deep learning,2015, Nature
 Deeply-supervised nets,2015, In Proc
 SGDR: Stochastic gradient descent with warm restarts,2017, In InternationalConference on Learning Representations
 Adaptive gradient methods with dynamic bound of learningrate,2018, In International Conference on Learning Representations
 Adaptive bound optimization for online convex optimization,2010, InCOLT
 Proximite et dualite dans un espace hilbertien,1965, Bulletin de la Societe mathematique deFrance
 Problem complexity and method efficiency in optimization,1983, Wiley
 Towards learning convolutions from scratch,2020, In H
 Simultaneous model selection and optimization through parameter-free stochasticlearning,2014, In Advances in Neural Information Processing Systems 27
 Proximal algorithms,2014, Foundations and Trends in optimization
 On the difficulty of training recurrent neural networks,2013, InInternational conference on machine learning
 A stochastic approximation method,1951, Annals of Mathematical Statistics
 Deep information propagation,2017, InInternational Conference on Learning Representations (ICLR)
 Asymptotic and finite-sample properties of estimators based on stochasticgradients,2017, The Annals of Statistics
 Continuous and discrete-time nonlinear gradient descent: Relativeloss bounds and convergence,1997, In Electronic proceedings of the 5th International Symposium onArtificial Intelligence and Mathematics
 Group normalization,2018, In Proceedings of the European conference oncomputer vision (ECCV)
 ADADELTA: an adaptive learning rate method,2012, arXiv preprint arXiv:1212
 Consider round i and assume K passed to Algorithm 2 is bounded w,2022,r
