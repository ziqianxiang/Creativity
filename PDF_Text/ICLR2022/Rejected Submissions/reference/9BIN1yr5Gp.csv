title,year,conference
 XcePtion: Deep learning with depthwise separable convolutions,2017, In Proceedingsof the IEEE conference on computer vision and pattern recognition
 Convex duality of deep neural networks,2020, arXiv preprintarXiv:2002
 Implicit convex regularizers of cnn architectures: Convex opti-mization of two-and three-layer networks in polynomial time,2020, arXiv preprint arXiv:2006
 Global optimality beyond two layers: Training deep relu networksvia convex programs,2021, In International Conference on Machine Learning
 A convex duality framework for gans,2018, In S
 Global optimality in neural network training,2017, In Proceedingsof the IEEE Conference on Computer Vision and Pattern Recognition
 Identity mappings in deep residualnetworks,2016, In European conference on computer vision
 Imagenet classification with deep con-volutional neural networks,2012, Advances in neural information processing systems
 Constrainedreinforcement learning has zero duality gap,2019, arXiv preprint arXiv:1910
 Neural networks are convex regularizers: Exact polynomial-time con-vex optimization formulations for two-layer networks,2020, arXiv preprint arXiv:2002
 On the margin theory of feedforward neuralnetworks,2018, arXiv preprint arXiv:1810
 Aggregated residual trans-formations for deep neural networks,2017, In Proceedings of the IEEE conference on computer visionand pattern recognition
 Wide residual netWorks,2016, arXiv preprintarXiv:1605
 Deep neural netWorks With multi-brancharchitectures are intrinsically less non-convex,2019, In The 22nd International Conference on ArtificialIntelligence and Statistics
 The rest of the proof isanalogous to the proof of Proposition 4,2022, For the problem (29)
