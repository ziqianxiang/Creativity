title,year,conference
 Ensemble knowledge distillation for learning improvedand efficient networks,2019, arXiv preprint arXiv:1909
 Memory-andcommunication-aware model compression for distributed deep learning inference on iot,2019, ACMTransactions on Embedded Computing Systems (TECS)
 Once-for-all: Train onenetwork and specialize it for efficient deployment,2019, In International Conference on LearningRepresentations
 Imagenet: A large-scale hi-erarchical image database,2009, In 2009 IEEE conference on computer vision and pattern recognition
 Collaborative execution of deepneural networks on internet of things devices,2019, arXiv preprint arXiv:1901
 Distilling the knowledge in a neural network,2015, arXivpreprint arXiv:1503
 Mobilenets: Efficient convolutional neural networks formobile vision applications,2017, arXiv preprint arXiv:1704
 Batch normalization: Accelerating deep network training byreducing internal covariate shift,2015, arXiv preprint arXiv:1502
 Optimal brain damage,1990, In Advances in neuralinformation processing systems
 Split computing and early exitingfor deep learning applications: Survey and research challenges,2021, arXiv preprint arXiv:2103
 Inplace knowledge distillation with teacher assistant for improvedtraining of flexible deep neural networks,2021, In 29th European Signal Processing Conference
 Compofa: Compound once-for-all networks for faster multi-platform deployment,2021, arXiv preprint arXiv:2104
 Weight normalization: A simple reparameterization to acceleratetraining of deep neural networks,2016, Advances in neural information processing systems
 On the surprising efficiency of committee-based models,2020, arXiv preprintarXiv:2012
 Wide activation for efficient image and video super-resolution,2020, In 30th British Machine Vision Conference
 Shufflenet: An extremely efficientconvolutional neural network for mobile devices,2018, In Proceedings of the IEEE conference oncomputer vision and pattern recognition
 An important observation is that in most cases slimming a CNNfrom 1,2020,0× (full model) to W × leads to roughly W2 times more parameters in switch 1
