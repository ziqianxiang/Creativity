title,year,conference
 Exploring the un-certainty properties of neural networksâ€™ implicit priors in the infinite-width limit,2020, In InternationalConference on Learning Representations
 Fine-grained analysis ofoptimization and generalization for overparameterized two-layer neural networks,2019, arXiv preprintarXiv:1901
 Explaining neuralscaling laws,2021, arXiv preprint arXiv:2102
 Reconciling modern machine-learning practice and the classical bias-variance trade-off,2019, Proceedings of the National Academyof Sciences
 Characterizing signal propagation to close theperformance gap in unnormalized resnets,2021, arXiv preprint arXiv:2101
 High-performance large-scaleimage recognition without normalization,2021, arXiv preprint arXiv:2102
 Imagenet: A large-scale hi-erarchical image database,2009, In 2009 IEEE conference on computer vision and pattern recognition
 An image is worth 16x16 words: Transformers for image recogni-tion at scale,2021, In International Conference on Learning Representations
 Graph neural tangent kernel: Fusing graph neural networks with graph kernels,2019, In Advancesin Neural Information Processing Systems
 A neural tangent kernel perspective of gans,2021, arXiv preprint arXiv:2106
 Decompos-ing reverse-mode automatic differentiation,2021, arXiv preprint arXiv:2105
 Deep convolutional net-works as shallow gaussian processes,2019, In International Conference on Learning Representations
 Evaluating Derivatives,2008, Society for Industrial and AppliedMathematics
 Finite depth and width corrections to the neural tangent kernel,2020, InInternational Conference on Learning Representations
 Bayesian deep ensembles via the neu-ral tangent kernel,2020, In Hugo Larochelle
 Infinite attention: NNGP andNTK for deep attention networks,2020, In International Conference on Machine Learning
 Neural tangent kernel: Convergence and gen-eralization in neural networks,2018, In Advances in Neural Information Processing Systems
 Deep neural networks as gaussian processes,2018, In International Conference on LearningRepresentations
 Wide neural networks of any depth evolve as linear modelsunder gradient descent,2019, In Advances in Neural Information Processing Systems
 Finite versus infinite neural networks: an empirical study,2020, 2020
 Optimizing neural networks with kronecker-factored approximatecurvature,2015, In International conference on machine learning
 Solution directe de l'equation SecUlaire et de quelques Problemes analogues tran-scendants,1913, C
 Optimal accumulation of jacobian matrices by elimination methods on the dualcomputational graph,2004, Mathematical Programming
 Optimal jacobian accumulation is np-complete,2008, Mathematical Programming
 Dataset meta-learning from kernel ridge-regression,2020, arXiv preprint arXiv:2011
 Dataset distillation with infinitelywide convolutional networks,2021, arXiv preprint arXiv:2107
 Bayesian deep convolutional networks withmany channels are gaussian processes,2019, In International Conference on Learning Representations
 Tadam: Task dependent adaptivemetric for improved few-shot learning,2018, In NeurIPS
 Towards nngp-guided neural architecture search,2020, arXiv preprint arXiv:2011
 Geometry of neural network loss surfaces via randommatrix theory,2017, In Doina Precup and Yee Whye Teh (eds
 Deep informationpropagation,2017, International Conference on Learning Representations
 Disentangling trainability and gener-alization in deep learning,2020, In International Conference on Machine Learning
 Non-Gaussian processes and neural networks at finite widths,2020, In Mathematical andScientific Machine Learning Conference
 Tensor programs ii: Neural tangent kernel for any architecture,2020, arXiv preprintarXiv:2006
 Wide residual networks,2016, In British Machine VisionConference
 Fixup initialization: Residual learning withoutnormalization,2019, arXiv preprint arXiv:1901
 Meta-learning with neuraltangent kernels,2021, In International Conference on Learning Representations
 Neural architecture search with reinforcement learning,2017, 2017
