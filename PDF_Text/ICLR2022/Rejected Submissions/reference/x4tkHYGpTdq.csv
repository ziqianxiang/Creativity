title,year,conference
 Language models arefew-shot learners,2020, arXiv preprint arXiv:2005
 The lottery ticket hypothesis for pre-trained bert networks,2020, arXiv preprintarXiv:2007
 Earlybert:Efficient bert training via early-bird lottery tickets,2021, In Proceedings of the Joint Conference of the59th Annual Meeting of the Association for Computational Linguistics and the 11th InternationalJoint Conference on Natural Language Processing
 Electra: Pre-trainingtext encoders as discriminators rather than generators,2019, In International Conference on LearningRepresentations
 Bert: Pre-training of deepbidirectional transformers for language understanding,2019, In Proceedings of the 2019 Conference ofthe North American Chapter of the Association for Computational Linguistics: Human LanguageTechnologies
 Automatic evaluation of machine translation quality using n-gram co-occurrence statistics,2002, In Proceedings of the second international conference on Human LanguageTechnology Research
 Parameter-efficient transfer learning with diff prun-ing,2020, arXiv preprint arXiv:2012
 Second order derivatives for network pruning: Optimal brainsurgeon,1993, Morgan Kaufmann
 Deberta: Decoding-enhanced bertwith disentangled attention,2020, arXiv preprint arXiv:2006
 Lora: Low-rank adaptation of large language models,2021, arXiv preprint arXiv:2106
 Speeding up convolutional neural networkswith low rank expansions,2014, In Proceedings of the British Machine Vision Conference
 Albert: A lite bert for self-supervised learning of language representations,2019, arXiv preprintarXiv:1909
 Optimal brain damage,1990, In Advances in neuralinformation processing systems
 Snip: Single-shot network pruning basedon connection sensitivity,2018, In International Conference on Learning Representations
 Prefix-tuning: Optimizing continuous prompts for generation,2021, arXivpreprint arXiv:2101
 Roberta: A robustly optimized bert pretrainingapproach,2019, arXiv preprint arXiv:1907
 Learn-ing efficient convolutional networks through network slimming,2017, In Proceedings of the IEEEinternational conference on computer vision
 Generalization guaran-tees for neural networks via harnessing the low-rank structure of the jacobian,2019, arXiv preprintarXiv:1906
 Bleu: a method for automaticevaluation of machine translation,2002, In Proceedings of the 40th annual meeting of the Associationfor Computational Linguistics
 Semi-orthogonal low-rank matrix factorization for deep neural networks,2018, InInterspeech
 Languagemodels are unsupervised multitask learners,2019, OpenAI blog
 Learning multiple visual domains withresidual adapters,2017, In Proceedings of the 31st International Conference on Neural InformationProcessing Systems
 Movement pruning: Adaptive sparsity by fine-tuning,2020, In NeurIPS
 A study oftranslation edit rate with targeted human annotation,2006, In Proceedings of the 7th Conference of theAssociation for Machine Translation in the Americas: Technical Papers
 Attention is all you need,2017, In Advances in neural informationprocessing systems
 Picking winning tickets before training bypreserving gradient flow,2019, In International Conference on Learning Representations
 Structured pruning of large language models,2020, InProceedings of the 2020 Conference on Empirical Methods in Natural Language Processing(EMNLP)
 Huggingface's transformers:State-of-the-art natural language processing,2019, arXiv preprint arXiv:1910
 Generalized low rank approximations of matrices,2005, Machine Learning
 On compressing deep models by lowrank and sparse decomposition,2017, In Proceedings of the IEEE Conference on Computer Vision andPattern Recognition
 Extracting deep neural network bottleneckfeatures using low-rank matrix factorization,2014, In 2014 IEEE international conference on acoustics
