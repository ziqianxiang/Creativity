title,year,conference
 Understanding double descent requires a fine-grained bias-variance decomposition,2020, Advances in Neural Information Processing Systems
 High-dimensional dynamics of generalization error in neuralnetworks,2017, arXiv preprint arXiv:1710
 High-dimensional dynamics of general-ization error in neural networks,2020, Neural Networks
 Scaling and generalization in neural networks: a case study,1989, InAdvances in neural information processing systems
 A continuous-time view of early stopping for leastsquares regression,2019, In The 22nd International Conference on Artificial Intelligence and Statistics
 Asymptotics of wide convolutional neural networks,2020, arxivpreprint arXiv:2008
 Local polynomial regression on unknown manifolds,2007, In Complex datasetsand inverse problems
 A theoretical-empirical approachto estimating sample complexity of dnns,2021, arXiv preprint arXiv:2105
 Spectrum dependent learning curves inkernel regression and wide neural networks,2020, In International Conference on Machine Learning
 Language models arefew-shot learners,2020, arXiv preprint arXiv:2005
 Spectral bias and task-model align-ment explain generalization in kernel regression and infinitely wide neural networks,2021, Naturecommunications
 Learning curves for deep neural networks: a gaussianfield theory perspective,2019, arXiv preprint arXiv:1906
 Asymptotics of wide networks from feynman diagrams,2020, In InternationalConference on Learning Representations
 Eigenvalues of integral operators defined by smooth positive definitekernels,2009, Integral Equations and Operator Theory
 Deep convolutionalnetworks as shallow gaussian processes,2019, In International Conference on Learning Representations
 General-isation error in learning with random features and the hidden manifold model,2020, In InternationalConference on Machine Learning
 Why momentum really works,2017, Distill
 Finite depth and width corrections to the neural tangent kernel,2020, InInternational Conference on Learning Representations
 Surprises in high-dimensional ridgeless least squares interpolation,2019, arXiv preprint arXiv:1903
 Scaling laws for autoregressive generative modeling,2020, arXiv preprint arXiv:2010
 Implicit bias of deep linear networks inthe large learning rate phase,2020, arXiv preprint arXiv:2011
 Learning curve theory,2021, arXiv preprint arXiv:2102
 Neural Tangent Kernel: Convergence andgeneralization in neural networks,2018, In Advances in Neural Information Processing Systems
 Eigenvalues of integral operators with smooth positive definite kernels,1987, Archiv derMathematik
 Deep neural networks as Gaussian processes,2018, In International Conference onLearning Representations
 Wide neural networks of any depth evolve as linear modelsunder gradient descent,2019, In Advances in Neural Information Processing Systems
 Finite versus infinite neural networks: an empirical study,2020, Advances inNeural Information Processing Systems
 Maximum likelihood estimation of intrinsic dimension,2005, InAdvances in neural information processing systems
 The largelearning rate phase of deep learning: the catapult mechanism,2020, arXiv preprint arXiv:2003
 Sgdr: Stochastic gradient descent with warm restarts,2016, arXivpreprint arXiv:1608
 Learning curves for gaussian processes regression: A frameworkfor good approximations,2001, Advances in neural information processing systems
 A variational approach to learning curves,2002, In T
 Learning curves and bootstrap estimates for inference withgaussian processes: A statistical mechanics study,2003, Complexity
 The generalization error of random features regression: Preciseasymptotics and double descent curve,2019, arXiv preprint arXiv:1908
 More data can hurt for linear regression: Sample-wise double descent,2019, arXivpreprint arXiv:1912
 Bayesian deep convolutional networks with manychannels are gaussian processes,2019, In International Conference on Learning Representations
 Neural Tangents: Fast and easy infinite neural networks in python,2020, InInternational Conference on Learning Representations
 Weighted sums of random kitchen sinks: replacing minimizationwith randomization in learning,2008, In Nips
 On the predictability ofpruning across scales,2020, arXiv preprint arXiv:2006
 A constructive predictionof the generalization error across scales,2020, In International Conference on Learning Representations
 Deep informationpropagation,2017, International Conference on Learning Representations
 Neural kernels without tangents,2020, In InternationalConference on Machine Learning
 Learning curves for gaussian processes,1998, In Proceedings of the 11th InternationalConference on Neural Information Processing Systems
 Learning curves for gaussian process regression: Approximationsand bounds,2002, Neural computation
 Non-Gaussian processes and neural networks at finite widths,2020, In Mathematical andScientific Machine Learning Conference
 Wide residual networks,2016, In British Machine VisionConference
 This almost works,2022, Inparticular
 The key observation is to make use of thefact that nearest neighbor distances for D points sampled i,2005,i
