title,year,conference
 Priors for infinite networks,1996, In Bayesian Learning for Neural Networks
 Deep infor-mation propagation,2016, arXiv preprint arXiv:1611
 Deep neural networks as gaussian processes,2017, arXiv preprintarXiv:1711
 Neural tangent kernel: Convergence andgeneralization in neural networks,2018, arXiv preprint arXiv:1806
 Wide neural networks of any depth evolve as linearmodels under gradient descent,2019, arXiv preprint arXiv:1902
 A mean field view of the landscape of two-layersneural networks,2018, Proceedings of the National Academy of Sciences
 Parameters as interacting particles: long timeconvergence and asymptotic error scaling of neural networks,2018, In Proceedings of the 32ndInternational Conference on Neural Information Processing Systems
 On the global convergence of gradient descent for over-parameterized models using optimal transport,2018, arXiv preprint arXiv:1805
 Three factors influencing minima in sgd,2017, arXiv preprintarXiv:1711
 Spherical motion dynamics of deepneural netWorks With batch normalization and Weight decay,2020, arXiv preprint arXiv:2006
 Comparing dynamics: Deepneural netWorks versus glassy systems,2018, In International Conference on Machine Learning
 Anomalous diffusion dynamics of learningin deep neural netWorks,2020, arXiv preprint arXiv:2009
 Automatic differentiation inpytorch,2017, Neural Information Processing Systems Workshop
 Reconciling modern deep learning With tradi-tional optimization analyses: The intrinsic learning rate,2020, In Advances in Neural InformationProcessing Systems
 On the momentum term in gradient descent learning algorithms,1999, Neural networks
 Neural mechanics: Symmetry and broken conservation laws in deep learning dy-namics,2020, arXiv preprint arXiv:2012
 Implicit gradient regularization,2020, arXiv preprintarXiv:2009
 On the origin of implicitregularization in stochastic gradient descent,2021, arXiv preprint arXiv:2101
 Dynam-ical mean-field theory for stochastic gradient descent in gaussian mixture classification,2020, arXivpreprint arXiv:2006
 Optimization and gen-eralization of shallow neural networks with quadratic activation functions,2020, arXiv preprintarXiv:2006
 Stochasticity helps to nav-igate rough landscapes: comparing gradient-descent-based algorithms in the phase retrievalproblem,2021, Machine Learning: Science and Technology
 Just a momentum: Analytical study ofmomentum-based acceleration methods in paradigmatic high-dimensional non-convex prob-lems,2021, arXiv preprint arXiv:2102
 Fluctuation-dissipation relations for stochastic gradient descent,2018, arXiv preprintarXiv:1810
 A tail-index analysis of stochasticgradient noise in deep neural networks,2019, In International Conference on Machine Learning
 A variational analysis of stochastic gra-dient algorithms,2016, In International conference on machine learning
 The implicit regularization of stochasticgradient flow for least squares,2020, In International Conference on Machine Learning
 Which algorithmic choices matter at which batch sizes?insights from a noisy quadratic model,2019, In Advances in Neural Information Processing Systems
 Noise and fluctuation of finite learning ratestochastic gradient descent,2021, In International Conference on Machine Learning
 On large-batch training for deep learning: Generalization gap and sharp min-ima,2016, arXiv preprint arXiv:1609
 Gra-dient descent on neural netWorks typically occurs at the edge of stability,2021, arXiv preprintarXiv:2103
 Empirical analysisof the hessian of over-parametrized neural netWorks,2017, arXiv preprint arXiv:1706
 The full spectrum of deepnet hessians at scale: Dynamics With sgd trainingand sample size,2018, arXiv preprint arXiv:1811
 An investigation into neural net opti-mization via hessian eigenvalue density,2019, In International Conference on Machine Learning
 Analysis of momentum methods,2019, arXiv preprintarXiv:1906
 Structure of stochastic dynamics near fixedpoints,2005, Proceedings of the National Academy of Sciences
 Theory of deep learning iii: explaining the non-overfitting puzzle,2017, arXiv preprint arXiv:1801
 Eigenvalues of the hessian in deep learning:Singularity and beyond,2016, arXiv preprint arXiv:1611
 On the interplay betWeen noise and curvature and its effecton optimization and generalization,2020, In International Conference on Artificial Intelligence andStatistics
 Free energy and the fokker-planck equa-tion,1997, Physica D: Nonlinear Phenomena
