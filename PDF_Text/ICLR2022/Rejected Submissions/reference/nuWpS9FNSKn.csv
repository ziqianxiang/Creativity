title,year,conference
 A spectralalgorithm for latent dirichlet allocation,2015, Algorithmica
 A method of moments for mixturemodels and hidden markov models,2012, In Conference on Learning Theory
 Learning topic models-going beyond svd,2012, In 2012IEEE 53rd annual symposium on foundations of computer science
 A theoretical analysis of contrastive unsupervised representation learning,2019, arXiv preprintarXiv:1902
 Learning representations by maximizingmutual information across views,2019, arXiv preprint arXiv:1906
 Factoring nonnegative matriceswith linear programs,2012, arXiv preprint arXiv:1206
 A correlated topic model of science,2007, The annals of appliedstatistics
 A simple framework forcontrastive learning of visual representations,2020, In International conference on machine learning
 Big self-supervised models are strong semi-supervised learners,2020, arXiv preprint arXiv:2006
 Improved baselines with momentumcontrastive learning,2020, arXiv preprint arXiv:2003
 Bert: Pre-training of deepbidirectional transformers for language understanding,2018, arXiv preprint arXiv:1810
 Fast and robust recursive algorithmsfor separable nonnegativematrix factorization,2013, IEEE transactions on pattern analysis and machine intelligence
 Provable guarantees for self-superviseddeep learning with spectral contrastive loss,2021, arXiv preprint arXiv:2106
 A survey on contrastive self-supervised learning,2021, Technologies
 Predicting what you already know helps:Provable self-supervised learning,2020, arXiv preprint arXiv:2008
 Pachinko allocation: Dag-structured mixture models of topic corre-lations,2006, In Proceedings of the 23rd international conference on Machine learning
 Contextencoders: Feature learning by inpainting,2016, In Proceedings of the IEEE conference on computervision and pattern recognition
 Improving language under-standing by generative pre-training,2018, 2018
 Random features for large-scale kernel machines,2007, In NIPS
 On the convergence of adam and beyond,2019, arXivpreprint arXiv:1904
 Gensim-Python framework for vector space modelling,2011, NLP Centre
 Probabilistic programming inpython using pymc3,2016, PeerJ Computer Science
 A mathematical exploration of why languagemodels help solve downstream tasks,2020, arXiv preprint arXiv:2010
 Complexity of inference in latent dirichlet allocation,2011, Advances inneural information processing systems
 Can pretext-based self-supervised learning be boosted by down-stream data? a theoretical analysis,2021, arXiv preprint arXiv:2103
 Understanding self-supervised learningwith dual deep networks,2020, arXiv preprint arXiv:2010
 Understanding self-supervised learning dynamicswithout contrastive pairs,2021, arXiv preprint arXiv:2102
 Contrastive estimation reveals topicposterior information to linear models,2020, arXiv preprint arXiv:2003
 Self-supervised learning from a multi-view perspective,2020, arXiv preprint arXiv:2006
 Attention is all you need,2017, In Advances in neural informationprocessing systems
 Theoretical analysis of self-training withdeep networks on unlabeled data,2020, arXiv preprint arXiv:2010
 Toward understanding the feature learning process of self-supervisedcontrastive learning,2021, arXiv preprint arXiv:2105
 Character-level convolutional networks for text clas-sification,2015, Advances in neural information processing systems
