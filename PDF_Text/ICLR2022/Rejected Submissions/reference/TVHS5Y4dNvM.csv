title,year,conference
 Lambdanetworks: Modeling long-range interactions without attention,2021, arXiv preprintarXiv:2102
 Cyclemlp: A mlp-like architecturefor dense prediction,2021, arXiv preprint arXiv:2107
 Coatnet: Marrying convolution andattention for all data sizes,2021, arXiv preprint arXiv:2106
 Animage is worth 16x16 words: Transformers for image recognition at scale,2020, arXiv preprintarXiv:2010
 Cmt:Convolutional neural networks meet vision transformers,2021, arXiv preprint arXiv:2107
 Pay attention to mlps,2021, arXiv preprintarXiv:2105
 Stand-alone self-attention in vision models,2019, arXiv preprint arXiv:1906
 Mlp-mixer: Anall-mlp architecture for vision,2021, arXiv preprint arXiv:2105
 Resmlp: Feedforwardnetworks for image classification with data-efficient training,2021, arXiv preprint arXiv:2105
 Goingdeeper with image transformers,2021, arXiv preprint arXiv:2103
 Attention is all you need,2017, In Advances in neural informationprocessing systems
 Pyramid vision transformer: A versatile backbone for dense prediction withoutconvolutions,2021, arXiv preprint arXiv:2102
 Earlyconvolutions help transformers see better,2021, arXiv preprint arXiv:2106
 Incorporatingconvolution designs into visual transformers,2021, arXiv preprint arXiv:2103
 Tokens-to-token vit: Training vision transformers from scratch onimagenet,2021, arXiv preprint arXiv:2101
 mixup: Beyond empiricalrisk minimization,2017, arXiv preprint arXiv:1710
