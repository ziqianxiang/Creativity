title,year,conference
 On the benefits ofmodels with perceptually-aligned gradients,2020, ArXiv
 Towards better understandingof gradient-based attribution methods for deep neural networks,2018, In International Conference onLearning Representations
 Towards evaluating the robustness of neural networks,2017, 2017 IEEESymposium on Security and Privacy (SP)
 Concise expla-nations of neural networks using adversarial training,2020, In International Conference on MachineLearning
 Explaining image classifiersby counterfactual generation,2019, In ICLR
 Grad-cam++: Generalized gradient-based visual explanations for deep convolutional networks,2017, arXivpreprint arXiv:1710
 Robust attribution regular-ization,2019, In Advances in Neural Information Processing Systems
 Certified adversarial robustness via randomizedsmoothing,2019, In ICML
 Reliable evaluation of adversarial robustness with an ensembleof diverse parameter-free attacks,2020, In ICML
 Provable robustness of relu net-works via maximization of linear regions,2019, AISTATS 2019
 Explanations based on the missing: Towards contrastive explanations with pertinentnegatives,2018, In NeurIPS
 Explanations can be manipulated and geometry is to blame,2019, In NeurIPS
 Interpretable explanations of black boxes by meaningful pertur-bation,2017, In Proceedings of the IEEE international conference on computer vision
 Fast ge-ometric projections for local robustness certification,2021, In International Conference on LearningRepresentations (ICLR)
 On locality oflocal explanation models,2021, arxiv
 Explaining and harnessing adversarialexamples,2015, In ICLR
 Provable certificates for adversarial examples: Fitting a ballin the union of polytopes,2019, In NeurIPS
 Provable defenses against adversarial examples via the convex outeradversarial polytope,2018, In ICML
 Lipschitz-certifiable training with a tight outerbound,2020, Advances in Neural Information Processing Systems
 Influence-directed expla-nations for deep convolutional networks,2018, In 2018 IEEE International Test Conference (ITC)
 Feature visualization,2017, Distill
 Explaining deep neural network models with adversarialgradient integration,2021, In Thirtieth International Joint Conference on Artificial Intelligence (IJCAI)
 Foolbox: A python toolbox to benchmarkthe robustness of machine learning models,2017, In Reliable Machine Learning in the Wild Workshop
 Evaluating the visualization of what a deep neural network has learned,2016, IEEE transactionson neural networks and learning systems
 Grad-cam: Visual explanations from deep networks via gradient-based local-ization,2017, In Proceedings of the IEEE international conference on computer vision
 Learning important features throughpropagating activation differences,2017, In International Conference on Machine Learning
 Visualizing the impact of feature attributionbaselines,2020, Distill
 Evaluating robustness of neural networks with mixedinteger programming,2019, In International Conference on Learning Representations
 6 shows an example where there are two dogs in the image,2022, IG with blackbaseline shows that the body of the white dog is also useful to the model to predict its label and theblack dog is a mix: part of the black dog has positive attributions and the rest is negatively contributeto the prediction
