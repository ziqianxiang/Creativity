title,year,conference
 Working memory: looking back and looking forward,1471, Nature Reviews Neuroscience
 Neural machine translation by jointlylearning to align and translate,2014, CoRR
 Short-term memory for serial order: A recurrent neuralnetwork model,0033, Psychological Review
 Language models are few-shot learners,2005, arXiv:2005
 BERT: Pre-training of deepbidirectional transformers for language understanding,1810, arXiv:1810
 RNNs as psycholinguistic subjects:Syntactic state and grammatical dependency,1809, arXiv:1809
 Learning totransduce with unbounded memory,1506, arXiv:1506
 Incorporating copying mechanism insequence-to-sequence learning,2016, arXiv:1603
 Colorlessgreen recurrent networks dream hierarchically,1803, arXiv:1803
 Long short-term memory,1530, Neural Computation
 The curious case of neural textdegeneration,2020, arXiv:1904
 Syntactic structure from deeP learning,2004, arXiv:2004
 Assessing the ability of LSTMs to learnsyntax-sensitive dePendencies,2016, arXiv:1611
 Targeted syntactic evaluation of language models,1808, arXiv:1808
 Pointer sentinel mixturemodels,2016, arXiv:1609
 Regularizing and oPtimizing LSTMlanguage models,1708, arXiv:1708
 Lan-guage models are unsupervised multitask learners,2019, 2019
 Multi-scaletransformer language models,2005, arXiv:2005
 Attention is all you need,1706, arXiv:1706
 Transformers: State-of-the-Art naturallanguage processing,2020, In Proceedings of the 2020 Conference on Empirical Methods in NaturalLanguage Processing: System Demonstrations
