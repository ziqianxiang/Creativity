title,year,conference
 Learning-compression algorithms for neural netpruning,2018, In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition(CVPR)
 Sparse networks from scratch: Faster training without losingperformance,2019, arXiv preprint arXiv:1907
 Rigging the lottery:Making all tickets winners,2020, In Hal DaUme In and Aarti Singh (eds
 Stabilizing thelottery ticket hypothesis,2019, arXiv preprint arXiv:1903
 The state of sparsity in deep neural networks,2019, arXivpreprint arXiv:1902
 Second order derivatives for network pruning: Optimal brainsurgeon,1993, In S
 Delving deep into rectifiers: Surpassinghuman-level performance on imagenet classification,2015, In Proceedings of the IEEE InternationalConference on Computer Vision (ICCV)
 Sparsity indeep learning: Pruning and growth for efficient inference and training in neural networks,2021, arXivpreprint arXiv:2102
 Three factors influencing minima in sgd,2017, arXiv preprintarXiv:1711
 Network pruning that matters: A case study on retrainingvariants,2021, In International Conference on Learning Representations
 The two regimes of deep network training,2020, arXiv preprintarXiv:2002
 Optimal brain damage,1989, In David S
 Towards explaining the regularization effect of initiallarge learning rate in training neural networks,2019, In H
 Dynamic model pruningwith feedback,2020, In International Conference on Learning Representations
 Scalable training of artificial neural networks with adaptive sparse connec-tivity inspired by network science,2018, Nature Communications
 Norm-based capacity control in neuralnetworks,2015, In Conference on Learning Theory
 Comparing rewinding and fine-tuning in neuralnetwork pruning,2020, In International Conference on Learning Representations
 Winning the lottery with continuous sparsi-fication,2020, In H
 Training sparse neural networks using com-pressed sensing,2020, arXiv preprint arXiv:2008
 Very deep convolutional networks for large-scale imagerecognition,2014, arXiv preprint arXiv:1409
 Wide residual networks,2016, arXiv preprintarXiv:1605
