title,year,conference
 Neural network learning: Theoretical foundations,2009, cambridgeuniversity press
 Fine-grained analysis of op-timization and generalization for overparameterized two-layer neural networks,2019, In InternationalConference on Machine Learning
 Deep equals shallow for ReLU networks in kernel regimes,2020, arXivpreprint arXiv:2009
 Spectrum dependent learning curves inkernel regression and wide neural networks,2020, In International Conference on Machine Learning
 Matem Gaus-sian processes on Riemannian manifolds,2020, In Advances in Neural Information Processing Systems
 Jax: composable transformations of python+ numpyprograms,2018, Version 0
 Language models arefeW-shot learners,2020, arXiv preprint arXiv:2005
 Deep neural tangent kernel and Laplace kernel have the same RKHS,2021, InInternational Conference on Learning Representations
 Multivariate normal approximation,2011, In NormalApproximation by Stein’s Method
 Kernel methods for deep learning,2009, In Advances in NeuralInformation Processing Systems
 On kernelized multi-armed bandits,2017, In InternationalConference on Machine Learning
 ToWard deeper understanding of neural netWorks:The poWer of initialization and a dual vieW on expressivity,2016, Advances In Neural InformationProcessing Systems
 Sparse gaussian processes with sphericalharmonic features,2020, In International Conference on Machine Learning
 Generative adversarial nets,2014, Advances in neural informationprocessing systems
 Batchedneural bandits,2021, arXiv preprint arXiv:2102
 Applied Analysis,2011, World Scientific
 Neural tangent kernel: Convergence and gener-alization in neural networks,2018, In Advances in Neural Information Processing Systems
 Gaussianprocesses and kernel methods: A review on connections and equivalences,2018, Available at Arxiv
 Neural contextual bandits without regret,2021, arXiv preprintarXiv:2107
 Imagenet classification with deep con-volutional neural networks,2012, Advances in neural information processing systems
 Deep learning,2015, nature
 Deep neural networks as Gaussian processes,2018, In International Conference onLearning Representations
 On the linearity of large non-linear models: whenand why the tangent kernel is constant,2020, Advances in Neural Information Processing Systems
 On the inductive proof of Legendre addition theorem,2001, StudiaGeophysica et Geodaetica
 Functions of positive and negative type and their commection with the theory of integralequations,1909, Philos
 Nerf: Representing scenes as neural radiance fields for view synthesis,2020, In Europeanconference on computer vision
 Norm-based capacity control in neuralnetworks,2015, In Conference on Learning Theory
 Stein’s method on wiener chaos,2009, Probability Theory andRelatedFields
 Neural tangents: Fast and easy infinite neural networks inpython,2019, arXiv preprint arXiv:1912
 Random features for large-scale kernel machines,2007, In NIPS
 The convergence rate of neural net-works for learned functions of different frequencies,2019, Advances in Neural Information ProcessingSystems
 Im-plicit neural representations with periodic activation functions,2020, Advances in Neural InformationProcessing Systems
 Gaussian process op-timization in the bandit setting: No regret and experimental design,2010, In ICML
 Support vector machines,2008, Springer
 Optimal ordersimple regret for Gaussian process bandits,2021, arXiv preprint arXiv:2108
 Open problem: Tight online confidence intervalsfor RKHS elements,2021, In Conference on Learning Theory
 Regularization matters: A nonparametricperspective on overparametrized neural network,2020, 2020
 On function approx-imation in reinforcement learning: Optimism in the face of large state spaces,2020, In Advances inNeural Information Processing Systems
 Learning bounds for kernel regression using effective data dimensionality,2005, NeuralComputation
 Neural Thompson sampling,2021, InInternational Conference on Learning Representations
 Neural contextual bandits with UCB-based explo-ration,2020, In International Conference on Machine Learning
