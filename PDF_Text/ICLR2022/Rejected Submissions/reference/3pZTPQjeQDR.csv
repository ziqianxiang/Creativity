title,year,conference
 Towards a human-like open-domain chatbot,2020, arXiv preprint arXiv:2001
 Nncp v2: Lossless data compression with transformer,2021, 2021
 A large annotatedcorpus for learning natural language inference,2015, In Proceedings of the 2015 Conference on EmpiricalMethods in Natural Language Processing (EMNLP)
 Language models arefew-shot learners,2020, arXiv preprint arXiv:2005
 Extracting training datafrom large language models,2020, arXiv preprint arXiv:2012
 Can transformers jump around right innatural language? assessing performance transfer from scan,2021, arXiv preprint arXiv:2107
 Lexical entries and rules of language: A multidisciplinary study of Germaninflection,1469, Behavioral and Brain Sciences
 What does bert look at?an analysis of bertâ€™s attention,2019, arXiv preprint arXiv:1906
 The paradox of the compositionality of naturallanguage: a neural machine translation case study,2021, CoRR
 Bert: Pre-training of deepbidirectional transformers for language understanding,2018, arXiv preprint arXiv:1810
 Location Attention for Extrapo-lation to Longer Sequences,2020, In Proceedings of the 58th Annual Meeting of the Association forComputational Linguistics
 Improving text-to-SQL evaluation methodology,2018, In Proceedingsof the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: LongPapers)
 Theoretical limitations of self-attention in neural sequence models,2020, Transactions ofthe Association for Computational Linguistics
 Efficient nearest neighbor languagemodels,2021, arXiv preprint arXiv:2109
 A parallel architecture perspective on language processing,2007, Brain research
 Nearestneighbor machine translation,2021, In International Conference on Learning Representations
 What they do when in doubt: a study of inductive biasesin seq2seq learners,2020, arXiv preprint arXiv:2006
 COGS: a compositional generalization challenge based on semanticinterpretation,2020, In Proceedings of the 2020 Conference on Empirical Methods in Natural LanguageProcessing (EMNLP)
 Adam: A method for stochastic optimization,2014, arXiv preprintarXiv:1412
 Transcoding compositionally: Us-ing attention to find more generalizable solutions,2019, In Proceedings of the 2019 ACL WorkshopBlackboxNLP: Analyzing and Interpreting Neural Networks for NLP
 The case for learned indexstructures,2017, arXiv preprint arXiv:1712
 SentencePiece: A simple and language independent subwordtokenizer and detokenizer for neural text processing,2018, In Proceedings of the 2018 Conference onEmpirical Methods in Natural Language Processing: System Demonstrations
 Generalization without systematicity: On the compositional skillsof sequence-to-sequence recurrent networks,2018, In proceedings of the 35th International Conferenceon Machine Learning (ICML)
 PAQ: 65 million probably-asked questions and what youcan do with them,2021, arXiv preprint arXiv:2102
 Roberta: A robustly optimized bert pretrainingapproach,2019, arXiv preprint arXiv:1907
 Dissecting lottery ticket transformers: Structural and behavioralstudy of sparse neural machine translation,2020, arXiv preprint arXiv:2009
 Words and rules: The ingredients of language,1999, 1999
 On language and connectionism: Analysis of a parallel distributedprocessing model of language acquisition,0010, Cognition
 Bpe-dropout: Simple and effective subwordregularization,2019, arXiv preprint arXiv:1910
 Exploring the limits of transfer learning with a unified text-to-texttransformer,2019, arXiv preprint arXiv:1910
 On Learning the Past Tenses of English Verbs,1986, In Paralleldistributed processing: Explorations in the microstructure of cognition
 Deja vu: an empiricalevaluation of the memorization properties of convnets,2018, arXiv preprint arXiv:1809
 Analysing mathematicalreasoning abilities of neural models,2019, In Proceedings of the 7th International Conference onLearning Representations (ICLR)
 Neural machine translation of rare words withsubword units,2015, arXiv preprint arXiv:1508
 Membership inference attacksagainst machine learning models,2017, In 2017 IEEE Symposium on Security and Privacy (SP)
 Under-standing unintended memorization in language models under federated learning,2021, In Proceedingsof the Third Workshop on Privacy in Natural Language Processing
 Attention is all you need,2017, arXiv preprint arXiv:1706
 Privacy risk in machine learning:Analyzing the connection to overfitting,2018, In 2018 IEEE 31st Computer Security FoundationsSymposium (CSF)
 Understandingdeep learning requires rethinking generalization,2016, arXiv preprint arXiv:1611
