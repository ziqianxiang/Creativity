title,year,conference
 Obfuscated gradients give a false sense ofsecurity: Circumventing defenses to adversarial examples,2018, In ICML
 Poisoning attacks against support vector ma-chines,2012, In ICML
 Adversarial attacks and defences: A survey,2018, arXiv preprint arXiv:1810
 Disrupting model training withadversarial shortcuts,2021, arXiv preprint arXiv:2106
 Learning to confuse: generating training time adversarialdata with auto-encoder,2019, 2019
 Preventing unauthorized use of proprietary data: Poisoning for secure datasetrelease,2021, arXiv preprint arXiv:2103
 Explaining and harnessing adversarialexamples,2015, In ICLR
 Unlearnableexamples: Making personal data unexploitable,2021, In ICLR
 Neural tangent kernel: Convergence and gen-eralization in neural networks,2018, In NeurIPS
 Understanding black-box predictions via influence functions,2017, InICML
 Adversarial machine learning at scale,2017, In ICLR
 Under-standing adversarial attacks on deep learning based medical image analysis systems,2020, PatternRecognition
 Towards poisoning of deep learning algorithms with back-gradientoptimization,2017, In Proceedings of the 10th ACM Workshop on Artificial Intelligence and Security
 Intriguing properties of neural networks,2014, In ICLR
 Better safe than sorry: Pre-venting delusive adversaries with adversarial training,2021, 2021
 Analysis and applications of class-wiserobustness in adversarial training,2021, 2021
 Bilateral adversarial training: Towards fast training of more robustmodels against adversarial attacks,2019, In ICCV
 Improvingadversarial robustness requires revisiting misclassified examples,2020, In ICLR
 Intriguing properties of adversarial training at scale,2020, In ICLR
 Neural tangent generalization attacks,2021, In ICML
 The standardly pre-trained ResNet-18 is trainedfor 60 epochs with an initial learning rate of 0,2017,1
