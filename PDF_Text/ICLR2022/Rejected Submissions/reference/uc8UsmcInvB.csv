title,year,conference
 Understanding deep neuralnetworks with rectified linear units,2016, arXiv preprint arXiv:1611
 Approximationanalysis of convolutional neural networks,2014, work
 Universal approximation bounds for superpositions of a sigmoidal function,1993, IEEETransactions on Information theory
 Spectrally-normalized margin bounds for neuralnetworks,2017, arXiv preprint arXiv:1706
 Depth-width trade-offsfor relu networks via sharkovskyâ€™s theorem,2019, arXiv preprint arXiv:1912
 Better depth-width trade-offs forneural networks through the lens of dynamical systems,2020, In International Conference on MachineLearning
 Approximation by superpositions of a sigmoidal function,1989, Mathematics of control
 Depth separation for neural networks,2017, In Satyen Kale and Ohad Shamir(eds
 Size-independent sample complexity of neuralnetworks,2018, In Conference On Learning Theory
 Identity matters in deep learning,2016, arXiv preprint arXiv:1611
 Nearly-tight vc-dimension bounds for piecewiselinear neural networks,2017, In Conference on Learning Theory
 Rnns can generatebounded hierarchical languages with optimal memory,2020, arXiv preprint arXiv:2010
 Multilayer feedforward networks are universalapproximators,1989, Neural networks
 On the ability of neural netsto express distributions,2017, In Satyen Kale and Ohad Shamir (eds
 Multilayer feedforward networkswith a nonpolynomial activation function can approximate any function,1993, Neural networks
 A mean field view of the landscape of two-layersneural networks,2018, Proceedings of the National Academy of Sciences
 Deterministic pac-bayesian generalization bounds for deepnetworks via generalizing noise-resilience,2019, arXiv preprint arXiv:1905
 A pac-bayesian approach tospectrally-normalized margin bounds for neural networks,2017, arXiv preprint arXiv:1707
 Optimization landscape and expressivity of deep cnns,2018, In Internationalconference on machine learning
 Provable memorization via deep neural networksusing sub-linear parameters,2020, arXiv preprint arXiv:2010
 On the turing completeness of modern neural networkarchitectures,2019, arXiv preprint arXiv:1901
 benefits of depth in neural networks,2016, In Vitaly Feldman
 Universal approximationproperty of neural ordinary differential equations,2020, arXiv preprint arXiv:2012
 Attention is all you need,2017, arXiv preprint arXiv:1706
 Memory capacity of neural networks with threshold and relu activations,2020, arXiv preprintarXiv:2001
 Data-dependent sample complexity of deep neural networks via lipschitzaugmentation,2019, arXiv preprint arXiv:1905
 Improved sample complexities for deep networks and robust classificationvia an all-layer margin,2019, arXiv preprint arXiv:1910
 Universal approximations of invariant maps by neural networks,2021, ConstructiveApproximation
 Understanding deeplearning requires rethinking generalization,2016, arXiv preprint arXiv:1611
 Approximation capabilities of neural odes andinvertible residual networks,1108, In International Conference on Machine Learning
 Universality of deep convolutional neural networks,2020, Applied and computationalharmonic analysis
