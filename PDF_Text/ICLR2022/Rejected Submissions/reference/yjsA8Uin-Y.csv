title,year,conference
 Mixmatch: A holistic approach to semi-supervised learning,2019, arXiv preprintarXiv:1905
 A simple framework forcontrastive learning of visual representations,2020, In International conference on machine learning
 ImageNet: A Large-Scale HierarchicalImage Database,2009, In CVPR09
 Bert: Pre-training of deepbidirectional transformers for language understanding,2018, arXiv preprint arXiv:1810
 On the resistance of nearest neighbor to random noisylabels,2016, arXiv preprint arXiv:1607
 Co-teaching: Robust training of deeP neural networks with extremely noisy labels,2018, InAdvances in neural information processing systems
 A survey on contrastive self-supervised learning,2021, Technologies
 Mentornet: Learning data-driven curriculum for very deep neural networks on corrupted labels,2018, In International Conferenceon Machine Learning
 Beyond synthetic noise: Deep learning on con-trolled noisy labels,2020, In International Conference on Machine Learning
 Imagenet classification with deep con-volutional neural networks,2012, Advances in neural information processing systems
 Noisy labels can inducegood representations,2020, arXiv preprint arXiv:2012
 Dividemix: Learning with noisy labels as semi-supervised learning,2020, In International Conference on Learning Representations
 Provably end-to-end label-noise learning without anchor points,2021, arXiv preprint arXiv:2102
 Early-learningregularization prevents memorization of noisy labels,2020, arXiv preprint arXiv:2007
 Peer loss functions: Learning from noisy labels without knowing noiserates,2020, In Proceedings of the 37th International Conference on Machine Learning
 Learning withnoisy labels,2013, In Advances in neural information processing Systems
 Self: Learning to filter noisy labels with self-ensembling,2019, arXiv preprint arXiv:1910
 Pervasive label errors in test sets destabilizemachine learning benchmarks,2021, arXiv preprint arXiv:2103
 Makingdeep neural networks robust to label noise: A loss correction approach,2017, In Proceedings of theIEEE Conference on Computer Vision and Pattern Recognition
 Estimating training datainfluence by tracing gradient descent,2020, In Advances in Neural Information Processing Systems
 Learning transferable visualmodels from natural language supervision,2021, arXiv preprint arXiv:2103
 Learning adaptive loss for robustlearning with noisy labels,2020, arXiv preprint arXiv:2002
 Fair classification with group-dependent label noise,2021, FAccT
 Symmetric cross en-tropy for robust learning with noisy labels,2019, In Proceedings of the IEEE International Conferenceon Computer Vision
 When optimizing f-divergence is robust with label noise,2021, In InternationalConference on Learning Representations
 Extended T: Learning with mixed closed-set and open-set noisy labels,2020, arXiv preprintarXiv:2012
 Part-dependent label noise: Towards instance-dependentlabel noise,2020, In Advances in Neural Information Processing Systems
 Learning from massive noisylabeled data for image classification,2015, In Proceedings of the IEEE Conference on Computer Visionand Pattern Recognition
 Unsupervised dataaugmentation,2019, arXiv preprint arXiv:1904
 Searching to exploit memo-rization effect in learning with noisy labels,2020, In Proceedings of the 37th International Conferenceon Machine Learning
 Visualizing and understanding convolutional networks,2014, InEuropean conference on computer vision
 mixup: Beyond em-pirical risk minimization,2018, In International Conference on Learning Representations
 Learning withfeature-dependent label noise: A progressive approach,2021, arXiv preprint arXiv:2103
 Generalized cross entropy loss for training deep neural networkswith noisy labels,2018, In Advances in neural information processing systems
 A second-order approach to learning with instance-dependent label noise,2021, In The IEEE Conference on Computer Vision and Pattern Recognition(CVPR)
 Clusterability as an alternative to anchor points whenlearning with noisy labels,2021, In Proceedings of the 38th International Conference on MachineLearning
