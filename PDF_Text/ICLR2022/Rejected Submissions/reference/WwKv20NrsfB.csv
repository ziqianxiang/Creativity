title,year,conference
 Improving the convergence of back-propagation learning with secondorder methods,1988, In Proceedings of the 1988 connectionist models summer school
 The tradeoffs of large scale learning,2008, In Advances in neuralinformation processing systems
 On the convergence of a class of adam-type algorithms fornon-convex optimization,2019, In 7th International Conference on Learning Representations
 Extreme tensoring forlow-memory preconditioning,2020, In International Conference on Learning Representations
 Identifying and attacking the saddle point problem in high-dimensional non-convexoptimization,2014, In Advances in neural information processing systems
 Imagenet: A large-scalehierarchical image database,2009, In 2009 IEEE conference on computer vision and pattern recognition
 A family of variable metric updates derived by variational means,1970, Mathematics ofComputation
 Generating sequences with recurrent neural networks,2013, arXiv preprint:1308
 Shampoo: Preconditioned stochastic tensor optimiza-tion,2018, arXiv preprint arXiv:1802
 Deep residual learning for imagerecognition,2016, In Proceedings of the IEEE conference on computer vision and pattern recognition
 Reducing the dimensionality of data with neuralnetworks,2006, science
 Long short-term memory,1997, NeUral computation
 AdaQN: An adaPtive quasi-newton algorithm for trainingrnns,2016, In Joint European Conference on Machine Learning and Knowledge Discovery in Databases
 Adam: A method for stochastic oPtimization,2015, In InternationalConference on Learning Representations
 Gradient-based learning applied todocument recognition,1998, Proceedings of the IEEE
 Implementation of stochastic quasi-newtonâ€™s method in pytorch,2018, arXivpreprint arXiv:1805
 On the variance of the adaptive learning rate and beyond,2020, In International Conference onLearning Representations
 SGDR: Stochastic gradient descent with warm restarts,2017, InInternational Conference on Learning Representations
 Adaptive gradient methods with dynamicbound of learning rate,2019, In International Conference on Learning Representations
 End-to-end sequence labeling via bi-directional LSTM-CNNs-CRF,2016, InProceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume1: Long Papers) 
 Flowseq: Non-autoregressive conditional sequence generation with generative flow,2019, In Proceedings of the2019 Conference on Empirical Methods in Natural Language Processing
 Deep learning via hessian-free optimization,2010, In Proceedings of the 27th InternationalConference on International Conference on Machine Learning
 Optimizing neural networks with kronecker-factored approximatecurvature,2015, In International conference on machine learning
 Rectified linear units improve restricted boltzmann machines,2010, InICML
 On the difficulty of training recurrent neuralnetworks,2013, In International conference on machine learning
 On the momentum term in gradient descent learning algorithms,1999, Neural networks
 On the convergence of adam and beyond,2018, InInternational Conference on Learning Representations
 A stochastic approximation method,1951, The annals of mathematicalstatistics
 An overview of gradient descent optimization algorithms,2016, arXiv preprintarXiv:1609
 Learning internal representations byerror propagation,1985, Technical report
 A stochastic quasi-newton method for online convexoptimization,2007, In Artificial intelligence and statistics
 Conditioning of quasi-newton methods for function minimization,1970, Mathematics ofcomputation
 Fast large-scale optimization by unifyingstochastic gradient and quasi-newton methods,2014, In International Conference on Machine Learning
 Rethinkingthe inception architecture for computer vision,2016, In Proceedings of the IEEE conference on computervision and pattern recognition
 Attention is all you need,2017, In Advances in neural informationprocessing systems
 The marginalvalue of adaptive gradient methods in machine learning,2017, In Advances in neural informationprocessing systems
 The secant method for simultaneous nonlinear equations,1959, Communications of the ACM
 Aggregated residualtransformations for deep neural networks,2017, In Proceedings of the IEEE conference on computervision and pattern recognition
 ADAHESSIAN:An adaptive second order optimizer for machine learning,2020, arXiv preprint arXiv:2006
 EAdam optimizer: How epsilon impact adam,2020, arXiv preprintarXiv:2011
 Adadelta: an adaptive learning rate method,2012, arXiv preprint:1212
 Adabelief optimizer: Adapting stepsizes by the belief in observedgradients,2020, Advances in Neural Information Processing Systems
