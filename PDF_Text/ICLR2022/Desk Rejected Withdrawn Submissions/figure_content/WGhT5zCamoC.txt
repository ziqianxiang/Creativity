Figure 1: Tokenization of 2 utterances of the keyword ’oily’50751001251501750	25	50	75	0	25	50(a)	(b)Figure 2: Tokenization of 2 utterances of the keyword ’agency12Under review as a conference paper at ICLR 2022(a)Figure 3: Tokenization of 2 utterances of the keyword ’carry(b)(a)	(b)Figure 4: Tokenization of 2 songs with id ’00024’B Gumble S oftmax based Vector QuantizerThe Gumble Softmax based Vector Quantizer product quantizes input latent representation zt ∈ Rmwith C codebooks each containing K quantizers e ∈ RK× C. Given zt, one of the K quantizers
Figure 2: Tokenization of 2 utterances of the keyword ’agency12Under review as a conference paper at ICLR 2022(a)Figure 3: Tokenization of 2 utterances of the keyword ’carry(b)(a)	(b)Figure 4: Tokenization of 2 songs with id ’00024’B Gumble S oftmax based Vector QuantizerThe Gumble Softmax based Vector Quantizer product quantizes input latent representation zt ∈ Rmwith C codebooks each containing K quantizers e ∈ RK× C. Given zt, one of the K quantizersfrom each of the C codebooks are chosen resulting in vectors e1, ..., eC which are concatenated andlinearly transformed from Rm to Rd to output qt ∈ Rd . zt is mapped to l ∈ RC ×K logits to giveprobability scores for the choice of codeword. The probability pc,k of choosing kth quantizer in cthcodebook is given as,13Under review as a conference paper at ICLR 2022(b)(a)Figure 6: Tokenization of 2 songs with id ’00042’
Figure 3: Tokenization of 2 utterances of the keyword ’carry(b)(a)	(b)Figure 4: Tokenization of 2 songs with id ’00024’B Gumble S oftmax based Vector QuantizerThe Gumble Softmax based Vector Quantizer product quantizes input latent representation zt ∈ Rmwith C codebooks each containing K quantizers e ∈ RK× C. Given zt, one of the K quantizersfrom each of the C codebooks are chosen resulting in vectors e1, ..., eC which are concatenated andlinearly transformed from Rm to Rd to output qt ∈ Rd . zt is mapped to l ∈ RC ×K logits to giveprobability scores for the choice of codeword. The probability pc,k of choosing kth quantizer in cthcodebook is given as,13Under review as a conference paper at ICLR 2022(b)(a)Figure 6: Tokenization of 2 songs with id ’00042’(a)(b)Figure 5: Tokenization of 2 songs with id ’00027’exp(lc,k + nk )∕τ
Figure 4: Tokenization of 2 songs with id ’00024’B Gumble S oftmax based Vector QuantizerThe Gumble Softmax based Vector Quantizer product quantizes input latent representation zt ∈ Rmwith C codebooks each containing K quantizers e ∈ RK× C. Given zt, one of the K quantizersfrom each of the C codebooks are chosen resulting in vectors e1, ..., eC which are concatenated andlinearly transformed from Rm to Rd to output qt ∈ Rd . zt is mapped to l ∈ RC ×K logits to giveprobability scores for the choice of codeword. The probability pc,k of choosing kth quantizer in cthcodebook is given as,13Under review as a conference paper at ICLR 2022(b)(a)Figure 6: Tokenization of 2 songs with id ’00042’(a)(b)Figure 5: Tokenization of 2 songs with id ’00027’exp(lc,k + nk )∕τ(20)where τ is a non-negative temperature, n = -log(-log(u)) and u are samples from the uniformdistribution Unif(0, 1). During forward pass, the codeword is chosen as κ = arg maxj pc,j. The
Figure 6: Tokenization of 2 songs with id ’00042’(a)(b)Figure 5: Tokenization of 2 songs with id ’00027’exp(lc,k + nk )∕τ(20)where τ is a non-negative temperature, n = -log(-log(u)) and u are samples from the uniformdistribution Unif(0, 1). During forward pass, the codeword is chosen as κ = arg maxj pc,j. Thestraight-through gradient estimator [Yin et al. (2019)] is utilized to estimate the gradient duringbackward pass.
Figure 5: Tokenization of 2 songs with id ’00027’exp(lc,k + nk )∕τ(20)where τ is a non-negative temperature, n = -log(-log(u)) and u are samples from the uniformdistribution Unif(0, 1). During forward pass, the codeword is chosen as κ = arg maxj pc,j. Thestraight-through gradient estimator [Yin et al. (2019)] is utilized to estimate the gradient duringbackward pass.
