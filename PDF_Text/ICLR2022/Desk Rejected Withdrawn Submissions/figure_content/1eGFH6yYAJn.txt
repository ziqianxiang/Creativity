Figure 1: Overview of Modality Laziness. We divide the features of multi-modal data into 1) self-standing features, which can be learned in both uni-modal and multi-modal learning, and 2) pairedfeatures, which can only be learned in multi-modal joint learning. Although joint training providesthe opportunity to learn paired features, multi-modal models are easier to see more powerful featuresfrom different modalities and quickly saturate and ignore the features that hard-to-learn but stillimportant. As a result, “everybody’s business becomes nobody’s business.”In practice, we demonstrate that UMT can effectively improve multi-modal late-fusion learning ondatasets like VGG-Sound (Chen et al., 2020b) and UCF101 (Soomro et al., 2012), and it also im-proves middle-fusion learning in segmentation tasks on NYU Depth V2 dataset (Silberman et al.,2012). We also compare the multi-modal model’s performance with that of uni-modal model at theclass level, aiming to measure the importance of paired features in different multi-modal datasets(see Table 6). We demonstrate that joint training is important for datasets with more paired fea-tures (e.g., VGG-Sound); as for datasets that paired features are rare (e.g., UCF101), combining theindividual trained uni-modal models can get competitive results (see Table 7).
Figure 2: Linear evaluation on encoders from different multi-modal late-fusion methods. Specifi-cally, we build a linear layer on top of each encoder to receive its detached features. After optimizingthe linear layer towards labels, we use the output accuracy of the linear layer as a metric of the corre-sponding encoder. Here we show the training dynamics of the video encoder in VGG-Sound and theoptical flow encoder in UCF101. Other encoder evaluation results can be found in Appendix A.3.
Figure 3: Illustrate for multi-modal training approaches (Point A) and uni-modal pre-training ap-proaches (Point B) under training procedure and test procedure, where the x-axis represents thefeature set learned by xm2, and the y-axis represents the feature set learned by xm1 . The feature setbecomes larger along the positive direction of the x-axis, and the training error in the blue regionis zero. For uni-modal approaches, xm1 modality learns feature set F1 while xm2 modality learnsfeature set F2 (the intersection between blue region and axis.). In Figure 3(a), multi-modal trainingapproaches learns less features in each modality (F10 instead of F1, F20 instead of F2). In Fig-ure 3(b) where the test error decreases from bottom left to top right, point B (uni-modal pre-trainingapproaches) outperforms point A (multi-modal training approaches).
Figure 4: The evaluation of audio encoder on VGG-Sound and RGB encoder on UCF101 in thewhole training process.
Figure 5: We first select the top 20 test accuracy classes of uni-audio model on VGG-Sound, andthen evaluate different video encoders on these classes. It can be seen that the video encoder inmulti-modal training setting (linear-head and Gradient Blending) is worse than that in uni-videosetting over about 15 classes, indicating that modality laziness occurs in multi-modal training.
Figure 6: Model architecture of naive late fusion (left) and Uni-Modal Teacher (UMT) (right).
Figure 7: Distillation details of UMT for RGB (left) and depth (right) modalities in multi-modalsemantic segmentation (based on ESANet).
Figure 8: Finetuning ProcessEncoder LR	Top-1 Acc	Encoder Eval Audio Video	1e-3	50.98^^	43.98	21.861e-4	49.37	44.71	21.971e-5	50.45	45.28	23.131e-6	50.86	45.29	23.270	50.95	45.15	23.17Table 9: Finetuning Results on VGG-SoundTable 10: Middle Fusion vs Late Fusion (VGG-Sound)Method	Late Fusion	Middle Fusion	UMT (based on Late Fusion)Top-1 Accuracy	49.46	49.87 —	53.46A.8 Can Dropout Tackle Modality Lazines s ?Here we consider the common regularizer, dropout (Srivastava et al., 2014), and a variant of it,namely modality-wise dropout, which randomly drops (with probability 1/3) the feature from onemodality in every iteration. Modality dropout is akin to the ModDrop in Neverova et al. (2015).
