Figure 1: Architecture of transformer driven by the disentangled mask attention.
Figure 2: Implementation procedure of the disentangled mask attention, where the procedure detailsof key k is omitted in figure, nq and nk denotes the number of query and key tokens, respectively.
Figure 3: Head redundancy in encoder of transformer (left) and DMA transformer (right), whereIWSLT’14 De-En test set is used for evaluation. The head index in x-axis and y-axis are accumulatedfrom first head in first layer to last head in last layer.
Figure 4: Top 15 tokens captured by the second cluster in the first head of the last decoder layer byusing DMA transformer, where the text in x-axis shows the BPE tokens, and IWSLT’14 De-En testset is used for evaluation.
Figure 5: Clustering probability distribution (left) and the corresponding semantic mask (right) inthe 1st head of 6th DMA transformer encoder layer, where the BPE tokens in two axes of (right) aresame as the tokens in x-axis of (left), and IWSLT’14 De-En test set is used for evaluation.
Figure 6: Query distribution of 104 sentences (13,312 tokens) in the last decoder layer of transformer(left) and DMA transformer (right), where the point with different colors indicates the query ofdifferent heads, and IWSLT’14 De-En test set is used for evaluation.
Figure 7: Query distribution of 88 sentences (30,272 tokens) in last decoder layer of transformer(left) and DMA transformer (right), where WMT’14 En-De test set is used for evaluation.
Figure 8: Query distribution of 72 sentences (15,552 tokens) in last decoder layer of transformer(left) and DMA transformer (right), where WMT’17 Zh-En test set is used for evaluation.
Figure 9: Head redundancy in encoder of transformer (left) and DMA transformer (right), whereWMT’14 En-De test set was used for evaluation.
Figure 10: Head redundancy in encoder of transformer (left) and DMA transformer (right), whereWMT’17 En-De test set is used for evaluation.
Figure 11: Top 15 tokens captured by the second cluster in the first head of the 5th encoder layer byusing DMA transformer, where WMT’14 En-De test set is used for evaluation.
Figure 12: Clustering probability distribution (left) and the semantic mask (right) in the 1st head of2nd DMA transformer encoder layer, where WMT’14 En-De test set is used for evaluation.
Figure 13: Clustering probability distribution (left) and the semantic mask (right) in the 1st head of6th DMA transformer encoder layer, where WMT’14 En-De test set is used for evaluation.
