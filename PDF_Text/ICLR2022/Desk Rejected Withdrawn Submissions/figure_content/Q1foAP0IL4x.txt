Figure 1: Difference between traditional (1a) and noisy (1b) adversarial training. Given the outputvector of a clean sample (orange circle) and the previous decision boundary (solid line), adversarialinputs (blue cross) lying on the wrong side of the decision boundary are created. When addingthe adversarial samples to the training process, the decision boundary adapts accordingly (dashedline). During Noisy Adversarial Training, the initial adversarial output vector is perturbed randomlywithin a given radius to create a set of additional pseudo adversarial inputs (smaller blue crosses).
Figure 2: Percentages of pseudo adversarial inputs being classified as indicated, depending on theperturbation scaling factor αs. The bars of each group show the results based on the followingmodels: Left bar: Standard trained model; Middle bar: Adversarial trained model; Right bar:Noisy adversarial trained model. C(noisy) = C(adversarial) indicates the noisy adversarial inputis classified the same, as the initial adversarial input. C(noisy) = C(clean) gives the percentageof pseudo adversarial inputs, which return to the original true classification area, while C(noisy)!= C(adversarial) != C(clean) gives the percentage of pseudo adversarial inputs moving to somedifferent, third class when perturbed randomly.
Figure 3: Distribution of perturbation size, measured regarding Coo, for attacks governed by ⅛ (blue), Cl (orange), and CoO (green) on the indicated trained model.
Figure 4:	Perturbation for each pixel governed by '2 (top), '1 (middle), and '∞ (bottom), measuredregarding '∞ on a LCE trained model.
Figure 5:	Perturbation for each pixel governed by '2 (top), '1 (middle), and '∞ (bottom), measuredregarding '∞ on a PGD adversarial trained model.
Figure 6:	Perturbation for each pixel governed by '2 (top), '1 (middle), and '∞ (bottom), measuredregarding '∞ on our proposed noisy adversarial trained model.
