Figure 1: Comparison of fine-tuning methods. (a) is the conventional pre-training and fine-tuningmethod. This paper provides theoretical analysis explaining the ineffectiveness of pre-trained mod-els compared to training from scratch. (b) shows the proposed pre-training data reusing method,which is motivated by our generalization analysis of the effect of pre-training data on fine-tuning.
Figure 2: Accuracy and performance gap when sub-sampling training datausing the supervised pre-trained ResNet18. (a) and (b) show a decreasing trend of performance gain when more training dataare added. The advantage of pre-training data reusing is larger when training data are not sufficient.
Figure 3: Accuracy of fine-tuning using UOT data selection with different numbers of selectedclasses using the supervised pre-trained ResNet18. (a) shows the performance on CUB and the blueline is fine-tuning with all birds classes from ImageNet. The UOT selection achieves a compara-ble performance to the label-based data selection. (b) shows the increased performance of UOTselection on Caltech as more data are reused in UOT, while the performance of random selection isconsistently worse than the UOTâ€™s .
