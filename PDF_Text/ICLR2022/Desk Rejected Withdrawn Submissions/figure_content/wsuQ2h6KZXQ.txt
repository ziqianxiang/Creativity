Figure 1: An illustration of the image-to-image MLP-mixer: First, the image is divided into patcheswhich are transformed with a linear layer to a volume of size W/P × H/P × C. This volumeis transformed with 3D mixer-blocks (16 in our standard architecture), and finally projected backwith a linear transformation to an image. There are many potential choices for the mixer block, ourdefault one mixes across channels, width, and height separately. For example, mixing in channeldimension means applying the same MLP to each of the H/P ∙ W/P-many vectors in channeldirection. The mixer block we propose retains the relative order of the patches.
Figure 2: Left: Gaussian denoising performance for image-to-image MLP-mixer networks of dif-ferent sizes trained on 4000 noisy images of 19 dB PSNR. The mixer is more parameter efficientin that it outperforms the U-net and ViT for almost all sizes considered. Middle: Gaussian denois-ing performance as a function of the number of training images for a network of size 3.4 millionparameters. For a very small number of training images, the U-net performs on paar with the image-to-image MLP-mixer, but in the regime where a moderate number of training examples is available,the mixer slightly outperforms the U-net. Right: Same as the middle plot, but with a larger networksize. It can be seen, that the Unet starts reaching a plateau as the training images increase, but theimage-to-image mixer continues to exhibit an improvement in performance.
Figure 3: Left: Compressed sensing performance for different models sizes when trained on theentire fastMRI knee dataset (35k data). Right: Compressed sensing performance as a function ofthe number of training images for networks of size about 8 million parameters. For a small numberof training images, the U-net reaches slightly higher SSIM than the image-to-image MLP-mixer, butthis performance gap quickly diminishes as training data grows.
Figure 5: Measuring the inductive bias of architectures. We fitted the {Img2Img-Mixer,U-net,ViT,Original-Mixer} to map a random input to (a) an image, (b) Gaussian noise, and (c) the image plusnoise. MSE denotes Mean Square Error of fitting the network output with respect to the clean imagefor img and img + noise, and with respect to the noise for noise. All networks have significantly moreparameters than pixels and can fit both noise and a natural image perfectly, but have an inductivebias, in that they fit the image faster than the noise.
Figure 6: Visualizing the images obtained by fitting a single noisy image with different architec-tures. The U-net fits the low-frequency components of the image before the high-frequency one,while for the image-to-image mixer, we can see the structure imposed by mixing horizontally andvertically.
