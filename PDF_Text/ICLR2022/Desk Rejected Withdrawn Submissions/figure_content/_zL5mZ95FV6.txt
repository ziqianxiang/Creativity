Figure 1:	Two different ways of summing 6 inputs: (a) 6-input LUT and (b) 32-bit FP adder for a6-input neuron with binarized input and output.
Figure 2:	Computation of a simple 4 input neuron implemented by LUT-2 with timing informationto show the timing relationquantization function to quantize the result to a two-bit integer. For example:b1 ∙ 21 + b0 ∙ 20 = fq (woa0 + wɪ a?)	⑶where fq is the quantization function that is integrated into the LUT. In the example, we have b(1 ×1.13) + (0 × 0.92)c = b1.13c = 1 = 012.
Figure 3: Computation of a n-input neuron implemented by LUT-6 without timing information toshow the structure and connection of LUTsFigure 4: DNN model design with BLU neuronpooling’ layer in a pipelined manner to finally generate a quantized integer output for this neuron.
Figure 4: DNN model design with BLU neuronpooling’ layer in a pipelined manner to finally generate a quantized integer output for this neuron.
Figure 5: Accuracy, energy improvement, latency improvement and area improvement of BLUnet,for VGG-16 on CIFAR-10 datasetfour and five inputs, respectively, are also common. Theoretically, implementing BLUnet usingLUT-4 and LUT-5 will increase the total number of LUTs and lower the accuracy because of theincrease in accumulated quantization errors. However, LUT-4 and LUT-5 have higher area efficiencythan LUT-6. Therefore, it is worth exploring especially for resource-constrained scenarios.
Figure 6: Comparison between the traditional method and our implementation.
