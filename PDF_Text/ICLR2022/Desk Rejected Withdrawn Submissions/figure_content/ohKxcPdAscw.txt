Figure 1: 2D experiment—performance of loss-based PGD (top) Vs. UTA-based PGD (bottom)using ε which violates the AT-Asm. Columns a-b. Attacks to a fixed trained classifier, with 1and 20 steps (columns), resp. ◦ and + depict the clean and perturbed samples, resp., and theircolor blue/red depicts their class. Top row: background depicts the assigned probability to eachclass of the attacked model, including its decision boundary. Bottom row: background depicts theuncertainty estimates of a 10 model ensemble, where darker green is higher. Column c. Decisionboundaries after adversarial training with large ε (and k = 15, α = 0.05). Column d. Summary ofthe boundary oscillations over the updates, for a fixed mini-batch size for both PGD and UTA. Foreach point x ∈ R2 we count how many times the classifier changed its prediction, and depict withdarker to lighter color zero to many changes, resp. See § 4.1 and § 4.3 for discussion.
Figure 2: Clean accuracy comparison between PGD and single-model UTA on MNIST, Fashion-MNIST and SVHN, for varying ε used for training (x-axis). The results are averaged over multipleruns.
Figure 3: Catastrophic overfitting (CO)on CIFAR-10 using ResNet 18, averagedover 3 runs. PGD robustness (50 itera-tions, 10 restarts) evaluated for multiple εtestfoor FGSM, Random-FGSM, PGD with twosteps, UTA with 1 step and an ensemble of5 models, and UTA with 2 steps and and en-semble of 5 models.
Figure 4: Catastrophic Overfitting (CO) on the FashionMNIST dataset using a LeNet model; resultsare averaged over 3 runs. Left: training with FGSM using a step size α = 0.2, ε = α; and testingagainst PGD-20 with ε = 0.2 and α = 0.01. Soon after the beginning of the training the FGSMaccuracy suddenly jumps to very high values while the PGD-20 accuracy approaches 0. Note alsohow the clean accuracy decreases and becomes lower than FGSM after CO. Right: training withUTA-1 (single step) using a step size α = 0.2, ε = α; and testing against PGD-20 with ε = 0.2 andα = 0.01. While the PGD robustness for UTA decreases to some extent, the drop is not as large asfor FGSM AT. Note also how UTA-1 AT lead to higher clean accuracy.
Figure 5: PGD and UTA attaCks on a fixed DUE (van Amersfoort et al., 2021) model trained solelywith Clean data on the toy example from § 4.1. Light and dark blue Crosses denote Clean and per-turbed samples of Class 1, and similarly, pink and red Crosses denote Clean and perturbed samples ofClass 0. Left: the baCkground depiCts the loss landsCape of the model (and its deCision boundary),Clean dataset and perturbed samples using the PGD-50 attaCk. Note how the PGD samples Cross theboundary and go to the other Class’ region. Right: entropy of the model, Clean dataset and UTA-50perturbed samples. UTA samples do not Cross the boundary and are able to go towards unexploredregions of the dataset.
Figure 6: PGD and UTA AT on a DUE model trained on a non-isotropic toy example dataset. Left:decision boundary obtained by training adversarially with PGD-50 perturbed samples. The model isnot able to correctly classify the dataset. Right: decision boundary obtained by training adversariallywith UTA-50 perturbed samples. The model is not is now able to classify correctly the dataset andthe decision boudnary is much more similar to the one obtained by training with clean samples only(shown in Figure 5).
Figure 7: TRADES applied on our 2D toy example with λ = 1 and for a large ε similar to theone used in 1c. Red samples in the top left corner end up being wrongly classified as the TRADESobjective tries to align predictions of both red and blue datapoints in this area.
Figure 8: Illustration of the difference between PGD and UTA attacks to classifier trained on CIFAR-10: (i) top row: clean samples, (ii) middle row: UTA perturbations, (iii) bottom row: PGD attacks,where for UTA and PGD we use same setup (1000 steps, step size of 0.001, ε = ∞). We use largenumber of steps to verify empirically if the difference between UTA and PGD depicted in Fig. 1holds on real-world datasets as well. Contrary to the PGD-perturbed samples, the correct class ofthe UTA-perturbed ones remains perceptible. See § C.2.
Figure 9: Catastrophic overfitting (CO) on CIFAR-10 using ResNet 18, averaged over 3 runs. (a):training with FGSM-With step size α = 8/255, ε = α; and testing against PGD-10 with ε = 8/255and ɑ = ε∕4. CO occurs at around iteration 4700. (b): training with UTA with 1 step (fast version),1 sampled model and α = ε = 8/255; testing against PGD-10 (ε = 8/255, α = ε∕4) and FGSM(ε = α = 8/255). We observe that UTA is more robust to CO relative to 9a. (c): Clean accuracycomparison from different AT and UTA methods with different values of the perturbation radius ε.
Figure 10: Full comparison (in addition to the results in Table 1) between PGD and single-modelUTA on Fashion-MNIST, results are averaged over multiple runs. Left: accuracy on the unper-turbed test dataset, for varying εtrain. Right: PGD robustness on the test data points, for varying εtest(x-axis), where dashed-dotted curves are UTA, and solid curves are PGD.
Figure 11: Comparison between PGD and UTA 5-model ensemble on Fashion-MNIST, results areaveraged over multiple runs. Left: accuracy on the unperturbed test dataset, for varying ε. Right:PGD-10 robustness, for varying ε, where dashed-dotted curves are UTA, and solid curves are PGD.
Figure 12: Comparison between PGD and five-model ensemble UTA on MNIST, results are av-eraged over multiple runs. Left: accuracy on the unperturbed test dataset, for varying ε. Right:PGD-10 robustness, for varying ε, where dashed-dotted curves are UTA, and solid curves are PGD.
Figure 13: Comparison between latent-space PGD and latent-space UTA attacks on MNIST, wherefor the latter we use ensemble of five models. See § 5.2 for setup summary and discussion. Left:accuracy on the unperturbed test dataset for varying ε. Training with UTA perturbations in latentspace is not affecting the clean accuracy, unlike PGD. Right: PGD robustness for varying ε, dashed-dotted curves are UTA and solid curves are PGD. 10 random restarts are used for the PGD testing.
