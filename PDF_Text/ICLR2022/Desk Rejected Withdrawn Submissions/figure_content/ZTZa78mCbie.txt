Figure 1: An illustration of ourdata generating process. Eachclass is comprised of points ona manifold; each point is charac-terized by two latent parameters:γ - determines the manifold thepoint belongs to, and θ definesits location on the manifold.
Figure 2: An illustration of a DNNas a GSH function. All points onthe same manifold map to (approx-imately) the same representation,while any two points from differ-ent manifolds go to distant repre-sentations.
Figure 3: A confusion ma-trix of intra (same manifold)vs inter (different manifolds) `2-distances of representations foran MLP trained on synthetic data(Section 5). The intra distancesare close to zero, suggesting thismodel is a GSH function.
Figure 4: A comparison of intra vs inter class distances. Left, we train an MLP on synthetic data (seeSection 5) that satisfies Assumption 1. On the Middle and Right we train a CNN on MNIST andCIFAR-10 respectively. For synthetic data, the GSH property strongly holds. The intra-distances ofthe representation layers for networks trained on MNIST and CIFAR-10 are also significantly smallerthan their inter distances. While a case could be made that the ratio for MNIST implies GSH, forCIFAR-10 GSH does not hold. However, we do observe that a similar mechanism is partially at play.
Figure 5: A comparison of intra vs inter class distances for UnseenManifold for an MLP trained on Mixed synthetic data. Remarkably,even on unseen manifolds the GSH property holds, that is the represen-tation is invariant to the “noisy feature" θ while being sensitive to thesemantically meaningful feature γ .
Figure 6: We show the ability to recover γ with linear regressionover the representation with varying the norm of θ/γ. We see that.6,54.32 」60.0.0.0.0.
