Figure 1: The normalized embedding distribution on the unit hypersphere (here we simplify it asthe two-dimensional case for a better demonstration). For each node, we use the shape and color torepresent its label and embedding, respectively. (a) is the initial random state; (b), (c), and (d) corre-spond to the embedding distribution when temperature τ is high (fixed), low (fixed), and dynamicallyadjusted, respectively. When τ is relatively high, the same penalties on all negative examples makethe embedding too compact in the local scope. When τ is relatively low, the high penalty on the hardnegative examples (highly similar to the anchor example) will lead to the misplacement of embed-dings. By dynamically changing τ ’s value, the final result accords with the global uniformity andlocal separation.
Figure 2: The rate curves under different τ . In (a) and (b), lower τ means more penalties on hardnegative examples. In (c) and (d), low τ deepens the imbalance update between different views.
Figure 3: The curves of loss under different training strategies on Cora, Citeseer, and Pubmed. Theloss value of GLATE w/o Tlower eventually becomes “NaN”.
Figure 4: The results of global uniformity and local separation on Cora, Citeseer, and Pubmed.
Figure 5: GLATE’s performance with different values of hyperparameters p1 and p2.
