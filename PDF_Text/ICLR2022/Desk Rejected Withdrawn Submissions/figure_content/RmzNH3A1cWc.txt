Figure 1: HANT consists of two phases: a candidate pretraining phase (a) and an architecture search phase (b).
Figure 2: Analyzing ILP performance on EfficientNetV2-B3. ILP results in significantly higher modelaccuracy before finetuning than 1k randomly sampled architectures in (b). Accuracy monotonicallyincreases with ILP objective (a). Model accuracy before finetuning correctly ranks models afterfinetuning (c). Train top-1 is measured before finetuning, while Validation top-1 is after.
Figure 3: The histogram of selected operations for top-100 models of EfficientNetV1 derivatives.
Figure 4: Final architectures selected by HANT as EfficientNet-B2/B6 derivatives.
Figure 5: Final architectures selected by HANT as EfficientNetV1-B4/V2-B3 derivatives.
Figure 6: Final architectures selected by HANT as 0.7xResNeST50d_1s4x24d>us3uu<0.995 -0.990-0.985 -0.980-Layer 71.000Layer 14Layer 21•	efn	∙	cb bottlecb res	∙	conv cs3•	cb stack	∙	conv cslefnv2■ identity• repvggdbb Ixl■ vitb。之trans10°	IO1	100	IO1
Figure 7: Result of the pretraining stage for EfficientNetB2, showing three layers equally spacedthroughout the network: 7, 14 and 21. Speedup is measured as the ratio between the latency of theteacher and the latency of the student operation (higher is better). We measure latency using PytorchFP16. Accuracy is the ratio of the operation’s accuracy and the teacher’s (higher is better). Thedashed black lines correspond to the teacher.
Figure 8: Comparison with other models from TIMM package.
