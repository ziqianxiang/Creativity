Figure 1: Example t-SNE embeddings for SupCon - baseline (left) and ExConB - ours (right) onthe CIFAR-100 dataset with a batch size of 256. There are five different classes on the graph, whereeach color represents a different class label. The cross (X) points represent the embeddings fororiginal input instances, while the dots represent the embeddings for input instances obtained usingaugmentations. The number below an input image indicate in terms of percentage the softmax scorecorrsponding to the predicted class. We take note of 4 observations when comparing ExConB toSupCon: 1. The embeddings for instances associated with different classes are farther apart fromeach other; 2. For instances within the same class, the embeddings between original images and theiraugmentations are much closer. 3. The visual quality of the masked images is better. 4. activationscores are either maintained or increased for the correct classes while it is largely decreased whenusing the SupCon baseline. This illustrates the capability of ExConB to take into account task-relevantfeatures.
Figure 2: Explanation-driven supervised contrastive learning framework with background maskdimages (ExConB). We produce explanation-driven masked images as well as randomly modified(transformed) images and then decide whether to add the masked image into positive examples ornegative examples based on its prediction. The gray circle in the graph is the anchor. The orangecircles are positive examples of that anchor. The red circles correspond to the negative samples. Theleft part of the figure corresponds to the scenario where the masked image yields a correct prediction.
Figure 3: Average validation accuracies and average validation losses along the training epochs onthe Tiny ImageNet dataset. We can see that the proposed methods, ExConB and ExCon consistentlyproduce higher validation accuracies and lower losses compared to the baseline method SupCon(Khosla et al., 2020). The training related to our proposed methods is more stable than the baseline.
Figure 4: Augmentation pipeline for ExCon. The orange blocks represent the original image in thebatch (with no augmentation). The pink regions represent the random modifications of the originalimage (e.g., random cropping and color jitting, etc.). The light blue blocks represent the explanation-driven masked images through masking out the unimportant regions (low-saliency regions) andreserving the important regions. The parenthesis contains their labels, where label ‘x’, ‘y’, ‘z’ area subset of example labels from the dataset. The above example shows a simple procedure foraugmenting a data batch with three images. For each original image, we produce two randommodifications of the original image following the work of (Khosla et al., 2020). If the masked imagegives the correct prediction, we adopt the masked image with the original label as well as one of therandom modifications of the original image in the data batch. If the masked image does not give thecorrect prediction, we adopt two random modifications of this image in the data batch.
Figure 5: Augmentation pipeline for ExConB. The notations here are the same as 4, except the factthat when the masked image gives an incorrect prediction, we assign it a background label ‘b‘ andappend two of the same masked images (in order to make up a pair) to the end of the batch.
Figure 6: t-SNE embeddings for SupConOri (left) and ExCon (right) on the CIFAR-100 dataset.
Figure 7: t-SNE embeddings for SupCon (top left), SupConOri (top right), ExCon (bottom left),ExConB (bottom right) on the Tiny ImageNet dataset.
Figure 8: t-SNE embeddings for SupCon (top left), SupConOri (top right), ExCon (bottom left),ExConB (bottom right) on the Tiny ImageNet dataset.
