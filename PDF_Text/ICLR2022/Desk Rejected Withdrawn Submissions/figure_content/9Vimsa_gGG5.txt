Figure 1: Phase diagram for ReLU networks with uncorrelated and anti-correlated Gaussian dis-tributed weights. (a) ReLU networks with uncorrelated weights have two phases. First, a boundedphase where qh is finite and second in which it diverges. The two phases are separated by σW =2.
Figure 2: The above plots show the signal’s length and correlation coefficient after propagatingthrough l layers in a ReLU network with uncorrelated weights. We estimate the length and corre-lation coefficient averaged over M = 1024 input signals, and 40 networks with a constant widthN = 2048. The shaded regions denotes the standard deviation. In the first panel, the vertical dashedline indicates the theoretical phase boundary σw2 = 2, and the solid black line denotes the theoreticalprediction for the length’s fixed point. As the critical boundaries do not depend on the variance ofthe bias, we show results for σb2 = 0.1 only. We find that clh → 1 for all values of σw2 and σb2.
Figure 3:	The above plots show the signal’s length and correlation coefficient after propagatingthrough l layers in a ReLU network with anti-correlated weights with a correlation strength k = 100.
Figure 4:	The above plots show the signal’s length and correlation coefficient after propagatingthrough l layers in a ReLU network with RAI. We estimate the length and correlation coefficientaveraged over M = 1024 input signals, and 40 networks with a constant width N = 2048. Theshaded regions denotes the standard deviation. Similar to Fig. 2, the chaotic region is absent.
Figure 5:	The above plots show the signal’s length and correlation coefficient after propagatingthrough l layers in a ReLU network with RAAI. We estimate the length and correlation coefficientaveraged over M = 1024 input signals, and 40 networks with a constant width N = 2048. Theshaded regions denotes the standard deviation. Similar to ACI, we find a chaotic region. However,the correlations do not converge to zero even for large σw2 .
Figure 6: Average validation loss for ReLU networks trained on the standard teacher task with SGD(left) and Adam optimizer (right) for different initialization schemes. The shaded region shows thestandard deviation around the average loss.
Figure 7: Average validation loss for ReLU networks trained on the simple teacher task with SGD(left) and Adam optimizer (right) for different initialization schemes. The shaded region shows thestandard deviation around the average loss.
Figure 8: Average validation loss for ReLU networks trained on the complex teacher task with SGD(left) and Adam optimizer (right) for different initialization schemes. The shaded region shows thestandard deviation around the average loss.
Figure 9: Average validation loss for ReLU networks trained on the standard teacher task with SGD(left) and Adam optimizer (right) for different weight correlations strengths.
Figure 10: Average validation loss for ReLU networks trained on the simple teacher task with SGD(left) and Adam (right) optimizer for different weight correlations strengths.
Figure 11: Average validation loss for ReLU networks trained on the complex teacher task withSGD (left) and Adam (right) optimizer for different weight correlations strengths.
Figure 12: Average training accuracy for ReLU networks trained on the MNIST task with SGDoptimizer for different initialization schemes.
Figure 13: Average training accuracy for ReLU networks trained on the Fashion-MNIST task withSGD optimizer for different initialization schemes.
Figure 14: Average training accuracy for ReLU networks trained on the CIFAR-10 task with SGDoptimizer for different initialization schemes.
