Figure 1: The minimum required task T* as a function of per task batch-size k, shown for input andfeature dimensions of dx = 36, df = 4. Dots indicate the values used in the experiments.
Figure 2: Training loss (A) and evaluation loss (B) profiles of narrow 2-layer models with initialweight scale 0.1. Each plot shows the result of 20 learning results of different initializations.
Figure 3: Evaluation loss of wide 2-layer model with initial weight scale 0.1 (A), and evaluationloss of wide 4-layer model with initial weight scale 0.001 (B). Note the difference in scale.
Figure 4: Summary figure showing the evaluation loss of wide linear network models across alltraining conditions (averged over 20 runs). Deeper network with smaller initial weights indeedhelps with generalization performance.
Figure 5: Trajectories of singular values (A) and singular vectors (B) of learning dynamics of awide 3-layer network (initial weight scale 0.001). The right column T = 1.25T* shows the case ofsuccessful learning.
