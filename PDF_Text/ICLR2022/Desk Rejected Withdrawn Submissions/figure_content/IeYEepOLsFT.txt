Figure 1: Comparison of Bayesian-PD and existing methods on a 1-D imbalanced linear regressionsynthetic benchmark. Different label distribution types (Normal & Exponential) and extents ofdistribution skewness are studied. We show the visualization of the regression results on the left andthe marginal label distribution on a balanced test set on the right. Although reweighting (in green) iscloser to the oracle (in blue) compared with least square (in yellow), it suffers larger error when thelabel distribution gets more skewed. In comparison, our method (in red), Bayesian-PD, makes theestimation closest to the oracle and has a uniform marginal label distribution on the test set. Mostimportantly, Bayesian-PD’s performance is invariant to the skewness of the training distribution.
Figure 2: Graphical model illustration for train-time and test-time debiasing, respectively. x, y arefrom train set and x, y are from test set. φ and φ are parameters of generative distributions thatgenerate X and X respectively. θ is a learnable regressor. In the imbalanced regression setting,the label distribution p(y∣φ) and p(y∣φ) are different and known. For train-time debiasing, thegenerative parameters are taken into account when estimating regressor θ. For test-time debiasing,the generative paramters are considered when predicting y.
Figure 3: Comparison of marginal label distributions on 2D linear regression. Least square andreweighting show visible bias towards the high-frequency area around the center. In comparison,Bayesian-PD achieves the closest marginal label distribution to the uniform test distribution.
Figure 4: Qualitative comparison for nonlinear regression. Four nonlinear functions are studied.
Figure 5: Bayesian-PD's bMAE gain over the baseline. The light blue area in the background showsthe training label histogram of IMDB-WIKI-DIR. Bayesian-PD improves the performance on taillabels (age < 20 and > 70) substantially.
Figure 6: Synthetic benchmark on random seeds. Although the noise scale keeps the same,reweighting’s performance varies drastically when different random seeds are used. In compari-son, Bayesian-PD is robust to different sampled noises.
Figure 7: IMDB-WIKI-DIR test set visualization. We observe tail labels on both edges of the testdistribution. Overall metrics will not sufficiently assess a model’s performance on the senior group(age <〜75) and the youth group (age >〜15).
Figure 8: Visualization of the training label distribution of IHMR. The horizontal axis is 100 regionsuniformly divided on the pose space according to their geodesic distance to the mean pose.
Figure 9: Qualitative comparison of Bayesian-PD and the baseline, SPIN-RT. Left: SPIN-RT. Right:Bayesian-PD. We observe that the baseline’s predictions are less stretched out. They bias towardsthe mean pose, particularly for poses like raising arms and bending legs. In comparison, our methodeffectively eliminates the bias and recovers rare poses.
