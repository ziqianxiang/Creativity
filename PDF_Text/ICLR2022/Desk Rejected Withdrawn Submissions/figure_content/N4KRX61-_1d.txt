Figure 1: a) A 8x8 DoorKey task; b) The diagram of an SRM designed for the DoorKey task.
Figure 2: a) The probabilistic graphical model of our framework; b) The flow chart of Algorithm 1GAIL objective Jadv as defined in Eq.2 is embedded but with D(s, a) :=exp(f (s,a)+6)exp(f (s,a)+e)+∏A(a∣s)in place of D. We also abbreviate p(∙∣τ; l) and p(∙∣τ; f) as pι(τ) and Pf (T) in the KL-divergenceDKL (pf (τ)∣∣pι(τ)), which can be viewed as a regularization term and turns out to be proportionalto the squared error PtT=-01( [l]](τ [: t]) - f (τ [t]))2. With Eq.5 we decouple the optimization of theGAIL objective Jadv from l such that a standard stochastic gradient descent with batched data canbe conducted on the GAIL objective Jadv . We prove in the Appendix that the stochastic versionEe〜N(0,ι) [Jadv(De)] has the same optimal condition as that of Jadv(D) in Eq.2.
Figure 3: Algo1+AGAC/PPO indicates using AGAC or PPO as the policy learning algorithm in line4 of Algorithm 1. AGAC/PPO+SRM indicates training an AGAC or PPO agent with the concretizedSRM. CNN and LSTM in the parentheses indicate the versions of the actor-critic networks. S4 andS6 in (k) indicate respectively the results for KeyCorridorS4R3 and KeyCorridorS6R3.
Figure 4: Algo1+PPO(CNN) indicates using PPO as the policy learning algorithm in line 4 ofAlgorithm 1 with symbolic constraints including relational predicates; Algo1(w/o c)+PPO(CNN)indicates running Algorithm 1 without symbolic constraint; Algo1(signonly)+PPO(CNN) indicatesrunning Algorithm 1 with symbolic constraint that only concerns the signs of the holessignificantly fewer frames than by training AGAC with the default reward as shown in Fig.3e. GAILand GAN-GCL fail again in the KeyCorridor task. Hence we omit their results in the plot. As shownin Fig.3h, reducing the number of examples (to 1) does not affect performance of Algorithm 1 forproducing a policy with average return of at least 0.8. In Fig.3k, we use the concretized SRM totrain RL agents in the 10 × 10 KeyCorridorS4R3 and 16 × 16 KeyCorridorS6R3 environments, andachieve higher performances with fewer frames than training with the default rewards. We omit theresults from other baselines in this task since AGAC(CNN) is the current SOTA for this task.
Figure 5: AGAC/PPO+SRM indicates that the hole assignments are learned via Algorithm 1;AGAC/PPO_rand# with an index # indicates that the holes are randomly assigned with some Val-ues that satisfy the symbolic constraint for that task. CNN and LSTM indicate the versions of theactor-critic networks.
Figure 7: Algo1+PPO(CNN) indicates using PPO as the policy learning algorithm in line 4 of Algo-rithm 1; Algo1(W/o c)+PPO(CNN) indicates that running Algorithm 1 Without symbolic constraintWhile using PPO(CNN) in line 4; Algo1(signonly)+PPO(CNN) indicates that running Algorithm 1Without symbolic constraint While using PPO(CNN) in line 4;CNN indicates CNN version of theactor-critic netWorks.
Figure 6: AlgoI(SRM#)+AGAC(LSTM) With an index # = 1 〜3 indicates running Algorithm 1with those three designed SRMs and by using AGAC in line 4 of Algorithm 1. PPO/AGAC+SRM#indicates training RL agents With SRM# by using PPO or AGAC algorithm. CNN and LSTMindicate the versions of the actor-critic netWorks.
Figure 8: The diagram of the SRM designed for the DoorKey task.
Figure 9:	The diagram of the SRM designed for the KeyCorridor task.
Figure 10:	The diagram of the SRM designed for the ObstructedMaze task.
