Figure 1: Our automatic portrait video matting method does not require extra inputs. Most state-of-the-art matting methods rely on semantic segmentation methods to automatically generate thetrimap. Their performance is compromised due to the lack of temporal information. Our methodexploits semantic information as well as temporal information from optical flow and produces high-quality results.
Figure 2: The architecture of our network. Our network takes a sequence of frames as inputs andestimate the alpha maps and foreground image for each frame.
Figure 3: Visual example of backwarp-ing. Backwarping can help construct amore complete background. Please notethat our method backwarps the featureWe fuse the correlation Ct and optical flow feature Flotto get the motion featuresMt= gmotion([Ct, Flot]),	â‘ºwhere gfiow_encode(.) indicates the operator for motion maps instead of images.
Figure 4: Examples from our background video dataset. It contains various indoor scenarios.
Figure 5: Visual comparison on the Video240K SD dataset. Our method generates more plausibleresults. We adopt the trimaps Tgt generated from the ground truth alpha maps for the trimap-basedmethods, including Index Matting Lu et al. (2019), Context Matting Hou & Liu (2019), GCAMatting Li & Lu (2020) and FBA Matting Forte & Pitie (2020). For the BGMV2 Lin et al. (2020),we adopt the ground truth background Bgt for it.
