Figure 1: Illustration of the FAIAS model. FAIAS consists of a selector gθ and a predictor fφ . The selector gθtakes the feature vector as an input and predict the probability for each feature to be selected, based on whichwe randomly sample the features. The predictor f φ gets two inputs, one (shown in the upper dot product) isthe reformulated input using the sampled features, the other (shown in the bottom dot product) is by adding thesensitive feature to the sampled features. The difference between the output of fφ w.r.t. the two inputs is thesensitivity loss lsen , which shows the marginal contribution of the sensitive feature to the input. The prediction1	7	1	「I	「「	r	ι	∙	ι	ιιrloss lpred shows the prediction performance by using only sampled features.
Figure 2: Comparison of classification performance (top two rows) and fairness (bottom three rows) on threebenchmark datasets (with sensitive feature shown in the parenthesis). We use three pre-trained models (VGG19and ResNet50) to extract 1,000 latent features for images in CelebA dataset (sensitive feature is sex). Higheraccuracy and true positive rate indicates better classification performance. Lower values for all three fairnessmetrics shows better fairness.
Figure 3: Visualization of reconstructed image based on different sets of feature. The first row shows theoriginal image. The second row shows the reconstructed image from all 1,000 features. The third row shows thereconstructed image after zeroing out the 200 (female), 400 (male) sensitive-relevant features learned by FAIAS.
Figure 4: Heatmap of GradCAM (Selvaraju et al., 2017) visualization of features with high and low p value.
