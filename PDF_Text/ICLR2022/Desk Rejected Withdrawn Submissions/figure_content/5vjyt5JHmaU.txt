Figure 1: ADR is used to mod-ify the action output by policyin the process of training andapplication. s denotes state, adenotes action. And ^ denotesmodified action by ADR.
Figure 2: The original velocity coordinate system re-linearly combines tWo neW dimensions y andW according to the constraints.
Figure 3:	When the constraints in t = 0,1,... ,T in the future becomes effective, c^t Win be cor-rected by action decomposition regular,otherwise a^t = at.
Figure 4:	Two tasks we consider, including: (a)Keep it straight; (b)Passing the intermediate station6.1	Keep it straight6.1.1	Experiment DescriptionThe agent starts from a random starting point to a random final landmark. But we require the agentto maintain a straight line movement as accurately as possible in a certain direction during the firstperiod of time. Although this task seems simple, it is not easy to satisfy the accuracy requirementsfor RL. That is because the larger learning rate of the algorithm leads the faster convergence and thepoorer stability, and the smaller learning rate of the algorithm leads to slow convergence and wasteof time(Smith, 2017).
Figure 5: Reward refers to the mean of accumulated return in a certain number of episodes. Violatedconstraints refers to the accumulation of the agentâ€™s violation of the constraint during the time whenthe constraint occurs. (a)The reward curve from starting to train until it converges. (b)The part of (a)after 4000 episodes. (c)The violated constraint curve. (d)The part of (c) after 4000 episodes. Thegreen dotted line equal to 0 indicates the accumulation of 0 violated constraint. Plotted are medianswith upper and lower quantiles of 5 seeds.
Figure 6: (a)The learning curve of the reward in this task. (b)The part of (a) after 10000 episodes.
