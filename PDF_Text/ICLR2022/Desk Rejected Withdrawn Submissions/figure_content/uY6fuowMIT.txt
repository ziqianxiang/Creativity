Figure 1: Approximate bijective correspondence (ABC). Leveraging weak set supervision,ABC isolates factors of variation which actively vary across sets. Establishing one-to-one corre-spondence between sets of inputs requires isolating factors that commonly vary within each set andsuppressing the factors which do not. For example, the images in set A (left) actively vary by onlythe orientation of the rendered car. We claim that if one-to-one correspondence can be found betweenA and B, for all such set A and B pairs, it must leverage orientation. We find this to be true evenwhen only one of the sets in each mini-batch is set-supervised, as above. Importantly, this allows theincorporation of out-of-domain data with no supervision at all, such as the images of real cars in B.
Figure 2: Set supervision scenarios amenable to ABC. (a) In the simple case with fiveinactive factors for each set, there is only one factor to isolate: the object hue. (b) The sets can bemuch less constrained, here defined by only a single inactive factor. (c) One set may be entirelyunconstrained, with no inactive factors at all. In all three scenarios, ABC isolates factors whichactively vary in both sets.
Figure 3: Isolation of active factors, Shapes3D. (a) Trained with wall hue as the onlyinactive factor, information about object and floor hue is visually apparent in the first two principalcomponents (> 0.98 of total variance) of the R64 embeddings. Each scatter plot displays the same256 embeddings, colored according to each generative factor. (b) With all hue factors inactive, therepresentations become informative about the geometric factors. (c,d) For the networks in (a,b),respectively, we estimate the mutual information I(U; G) between the representations and each of thegenerative factors using MINE (Belghazi et al., 2018). We add Gaussian noise to the representationsto probe information content over different length scales in representation space. When σ equals thelength scale of the loss (vertical dotted line), information about inactive factors (dashed) disappears.
Figure 4: Influence of different aspects of set supervision. We measure the informationcontent of the learned representations U as in Figure 3, with added noise of magnitude σ = τ . Errorbars display the standard deviation across ten random seeds. The inactive factors during training areindicated by shading. (a-c) We find the isolation of active factors to be unchanged when trainingwith one of the two sets unsupervised (ABC-X). (d) Increasing the set size isolates more of theactive factors of variation because finding correspondence requires more discerning power.
Figure 5: Fast style isolation on MNIST. After training ABC with set supervisionwhere digit class is the inactive factor, we evaluate the isolation of the factors of variationrelating to style. (a) We display embeddings of the digit 9, held out during training to testthe isolation of class-independent style information. The embeddings fan out by thicknessand slant. (b) We perform retrieval on the test set using the boxed images along the diagonalas queries; the other images in each row are the nearest embeddings for each digit class.
Figure 6: Retrieval results from ABC-X and ResNet embeddings. Given a query imagefrom the Pascal3D+ test split, we display the four nearest neighbors in embedding space, from thePascal3D+ train split and from 1800 ShapeNet images. The accuracy and visual diversity of theABC-X retrieval results illustrate effective isolation of pose information generalized across both thecategory and the synthetic-to-real domain gap.
Figure 7: Corroborating IMINE with classification task. As a proxy for the mutualinformation, we use the test set classification accuracy of networks trained to predict the sixgenerative factors, one network per factor. As before, the shaded columns indicate whichof the generative factors were inactive while training ABC. Gaussian-distributed randomnoise with σ = ʌ/r was added to the embeddings to effectively remove information on lengthscales less than the characteristic length scale of the ABC loss. The dashed lines show theclassification accuracy that would result from random guessing.
Figure 8: Temperature sets the length scale of the cutoff between active andinactive factors. We train with negative squared Euclidean distance between embeddingsas the similarity measure, which makes √τ a natural length scale for embedding space. Byvarying the temperature used during training (varying vertically), we mark the length scale√τ with a dotted vertical line in each subplot. Predictably, the magnitude of the noise σ atwhich inactive factors are suppressed scales with √τ. Had negative Euclidean distance beenused instead, we would expect the scaling to follow τ . The bottom right subplot shows one ofthe limits of varying the temperature of the ABC loss: when it is too large compared to thespread of the initialized embeddings, training is often unsuccessful.
Figure 9: The case for finding more than one factor of variation, through a simpleexample. We model the embeddings that would be learned from randomly distributed factorsof variation as points sampled uniformly over the unit interval in one to six dimensions.
Figure 10: Ablative studies on Pascal3D+ pose lookup with ABC embeddings.
Figure 11: Ablative studies on Pascal3D+ with spherical regression + ABC net-work. Error bars are the standard error of the mean over 10 random seeds for eachconfiguration, with less than 1% of the runs discarded for lack of convergence. We showresults on the Pascal3D+ test split for the car and chair categories. For each row, the trainingconfiguration is the same as described in Appendix G with only the listed aspect of trainingbeing changed. In the first row, no titration means to the fraction of real images in set B arepresent from the beginning of training. The three similarity measures in the second row arecosine similarity, L2 (Euclidean) distance, and squared L2 distance.
Figure 12: Augmentations used in the pose estimation experiments. We show sampleaugmentations applied to both real and synthetic cars. These include adjusting brightnessand hue, adding normal ly distributed noise to each pixel, random translations of the crop(bounding box), and replacing the background of synthetic images with random crops fromreal images.
Figure 13: Retrieval results over the course of training, comparison. We compareretrieval on the test set of MNIST at various stages of training ABC and the two VAE-basedapproaches mentioned in the main text. As in Figure 5, the query images are the boxed imagesalong the diagonal, and each row is the nearest representative for each class in embeddingspace. Also as before, in all cases the digit 9 was withheld during training.
