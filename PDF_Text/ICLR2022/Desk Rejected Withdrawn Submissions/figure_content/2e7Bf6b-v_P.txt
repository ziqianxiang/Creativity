Figure 1: Figure representing our aggregator-workerpipeline when using a NAS controller with the vanillaES method, where the aggregator proposes models miin addition to a perturbed input θ + σgi, and the workerthe computes the objective f (mi , θ + σgi), which issent back to the aggregator. Both the training of theweights θ and of the model-proposing controller pφ relyon the number of worker samples to improve perfor-mance.
Figure 2: Plot of average normalized optimality gap across all BBOB functions (3 seeds each), when rangingdcat and dcon . As dcon increases, over all combinatorial algorithms (different colors), each ES-ENASvariant (solid line) begins to outperform its corresponding vanilla combinatorial algorithm (dashed line).
Figure 3: (a) Example of sparsifying a neural network setup, where solid edges are those learned by thealgorithm. (b) Example of quantization using a Toeplitz pattern (Choromanski et al., 2018), for the first layerin Fig. 3a. Entries in each of the diagonals are colored the same, thus sharing the same weight value. Thetrainable weights θs = w(1) , ..., w(9) are denoted at the very bottom in the vectorized form with 9 entries,which effectively encodes the larger T with 24 entries.
Figure 4: Comparisons across different environments when using different controllers, on the edge pruningand quantization tasks, when using a linear layer (L) or hidden layer of size 32 (H32).
Figure 5: Regular ES-ENAS experiments with 150 full controller objective value usage plotted in darker colors.
Figure 6: Edge pruning convergence over time, with samples aggregated over 3 seeds from PG runs on Swim-mer. EaCh edge is colored according to a spectrum, with its color value equal to 2 |p — 11 where P is theedge frequency. We see that initially, each edge has uniform (P = 2) probability of being selected, but as thecontroller trains, the samples converge toward a single pruning.
Figure 7: Environment reward plotted alongside the average number of edges used for proposed models.
Figure 8:	Comparison when regular ES/ARS is used as the continuous algorithm in ES-ENAS, vs when CMA-ES is used as the continuous algorithm (which we name “CMA-ENAS”). We use the exact same setting asFigure 2 in the main body of the paper. We use Regularized Evolution (Reg-Evo) as the default combinatorialalgorithm due its strong performance found from Figure 2. We find that ES-ENAS usually converges fasterinitially, while CMA-ENAS achieves a better asymptotic performance. This is aligned with the results (in thefirst row) when comparing vanilla ES with vanilla CMA-ES. For generally faster convergence to a sufficientthreshold however, ES/ES-ENAS usually suffices.
Figure 9:	The results from training both a mask m and weights θ of a neural network with two hidden layers.
Figure 10: (a): Partitioning of edges into distinct weight classes obtained for the linear policy for HalfCheetahenvironment from OpenAI Gym. (b): Partitioning of edges for a policy with one hidden layer encoded by twomatrices. State and action dimensionalities are: s = 17 and a = 6 respectively and hidden layer for thearchitecture from (b) is of size 41. Thus the size of the matrices are: 17 × 6 for the linear policy from (a) and:17 × 41, 41 × 6 for the nonlinear one from (b).
Figure 11: (Left): Final architectures that PG and Reg-Evo converged to on Swimmer with a linear (L) policy,as specified in Subsection 3.3.3. Note that the controller does not select all edges even if it is allowed in theboolean search space, but also ignores some state values. (Right): Convergence result for Reg-Evo, similar toFig. 6 in Subsection 3.3.3.
