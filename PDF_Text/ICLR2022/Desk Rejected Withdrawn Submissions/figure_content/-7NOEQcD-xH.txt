Figure 1: Framework of deep en-semble policy learning.
Figure 2: The evaluation results on Minigrid environments, all the results are conducted with fiverandom seeds. The top and bottom rows show the information about distributional-shift and multi-room, respectively. First column: A snapshot of the environment where the red triangle and greensquare represent the position and the goal of the agent respectively. Second column: Learningcurves of all the compared methods. Third column: Learning curves of DEPL and its variants.
Figure 3: Learning curves about diversity and performance of DEPL and DEPL-Div0	100	200	300	400	500Epochs(c) Multi-room0.6 、+-Iω0.4 IQ0.2C Additional Baseline on MinigridConsidering that DEPL is a special case under the paradigm of mixture of experts (Jacobs et al.,1991), thus we take PMOE (Ren et al., 2021), where a routing function is learned to weight theoutput of each expert, as a baseline. We compare PMOE with DEPL in Minigrid and the learningcurves of both the methods are shown in Figure 4.
Figure 4: Learning curves of DEPL and PMOEFrom the results, we observe that DEPL consistently achieves better sample efficiency than PMOE,which can be attributed to two reasons. Firstly, the mean aggregation can promote the explorationwhile the weight generated by the routing function hurt the attribute. Secondly, to achieve diversityamong sub-policies, a sample is only assigned to a single policy for policy optimization in PMOE,which can lead to a lower sample efficiency compared with DEPL where a diversity strengthenregularization is used to realize diversity.
