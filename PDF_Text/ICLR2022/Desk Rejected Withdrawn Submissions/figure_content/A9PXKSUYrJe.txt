Figure 1: Training on MNIST with 50% symmetric noise. (a) Compared with noisy samples, clean samplesyield relatively smaller loss value and more consistent prediction. (b) Empirical pdf of loss values and (c)their stand deviation justify above conclusion. That is, clean and noisy samples possess distinctive statisticalproperties. However, noisy samples can not be completely identified via a simple threshold filter strategy (bluedotted line in (b) and (c)) with these statistical metrics. The existence of easy and hard noisy samples requiringdifferent ways to handle them accordingly.
Figure 2: The pipeline of CREMA. CREMA trains two parallel networks simultaneously. Cleansamples (mostly clean) Xc and noisy samples (mostly noisy) Xu are separated via estimating thecredibility of each training data. A selective label distribution learning scheme is applied for easilydistinguishable noisy samples in Xu. As for the clean set Xc, likelihood estimation of historicalcredibility sequence is proposed to handle the hard noisy samples via adaptively modulate their lossterm during training.
Figure 3: Training on MNIST with 50% symmetric noise, warm up (i.e., training on all samples with originalnoisy labels) for T epochs. (a) Global learning procedure requires updating all training samplesâ€™ label, causesrelatively large gradient values even on clean samples (areas within blue dotted lines), making it hard to focus oncorrecting noisy labels. (b) Training on CREMA can effectively identify noisy samples and focus on correctingnoisy labels with reletively large gradient values.
