Figure 1: PCDNet decomposition framework. First, the Phase Correlation (PC) Cell estimates theO translation parameters that best align each learned prototype to the objects in the image, and usesthem to obtain (P Ã—O) object and mask candidates. Second, the color module assigns a color to eachof the transformed prototypes. Finally, a greedy selection algorithm reconstructs the input image byiteratively combining the colorized object candidates that minimize the reconstruction error.
Figure 2: (a): Inner structure of the PC Cell. First, the translation parameters are estimated byfinding the correlation peaks between the object prototype and the input image. Second, the proto-type is shifted by phase shifting in the frequency domain. (b): The Color Module estimates colorparameters from the input image and aligns the color channels of a translated object prototype.
Figure 3: Object prototypes (top) and segmentation alpha masks (bottom) learned on the Tetromi-noes dataset. Our model is able to discover in an unsupervised manner all 19 pieces.
Figure 4: Qualitative decomposition and segmentation results on the Tetrominoes dataset. Last rowshows a failure case. (a): Original image. (b): PCDNet Reconstruction. (c)-(e): Colorized trans-formed object prototypes. (f): Semantic segmentation masks. Colors correspond to the prototypeframes in Figure 3. (g): Instance segmentation masks.
Figure 5: Object prototypes learned on the Space Invaders dataset. PCDNet discovers prototypescorresponding to the different elements from the game, including aliens, lasers and ships.
Figure 6:	Comparison of different object-centric models on the Space Invaders dataset. PCDNetis the only one among the compared methods which successfully decomposes the input image intoaccurate object components, and that has semantic knowledge of the object representations. Thecolor of each object corresponds to the frame of the corresponding prototype from Figure 5.
Figure 7:	Object discovery on the NGSIM dataset. PCDNet learns prototypes and masks for differ-ent types of vehicles in an unsupervised manner.
Figure 8: Object prototypes learned on the Tetrominoes dataset. Our model is able to discover in anunsupervised manner all 19 pieces.
Figure 9: Segmentation masks for the learned Tetrominoes object prototypes.
Figure 10: Qualitative results on the Tetrominoes dataset. (a): Original image. (b): PCDNet Re-construction. (c)-(e): Colorized and translated object prototypes selected by PCDNet. (f): Semanticsegmentation masks. Colors correspond to the prototype frames in Figure 8. (g): Instance segmen-tation masks.
Figure 11: Learned prototypes and object masks on the Space Invaders dataset.
Figure 12: Additional qualitative comparison on the Space Invaders dataset.
Figure 13: Additional qualitative comparison on the Space Invaders dataset.
Figure 14: Additional PCDNet unsupervised segmentation qualitative results on the Space Invadersdataset.
Figure 15: Learned vehicle prototypes and masks on th NGSIM dataset.
