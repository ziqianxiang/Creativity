Figure 1: The overall pipeline of the proposed framework. OUr framework includes a proxy depth recon-struction (PDR) model to determine the depth of a virtual viewing ray, a differentiable ray tracer to retrievecorresponding colors from real input images, and a color blending network (CBNet) to recover the RGB colorinformation.
Figure 2: For a virtual camera position x and viewing direction v, we estimate a depth d between the scenepoint w and the camera location x. By reprojecting the scene point to real cameras, we retrieve the color ciand high-level feature fi from the observed images. The cosine distance (angle) si between the virtual viewingdirection and real viewing direction determine the influence of corresponding real cameras when calculatingthe photometric consistency.
Figure 3: (a) An illustration of an omni-directional camera and its captured light-field and a sample image. (b)An illustration of our camera arrangement for dataset generation. For each scene, we capture 125 omnidirec-tional images at different locations for evaluation. The cameras are positioned in a 50 × 50 × 50 centimetervolume (roughly) at the center of each scene.
Figure 4: Qualitative comparison with NeRF and NeRF++ on our generated scenes “Lounge”. Our methodgenerates sharper results than the comparison algorithms.
Figure 5: Qualitative comparisons of our method W or w/o Imaginary Eye Sampling (IES). Without using IES,the image synthesized at a position far from any real camera (top right) suffers much lower quality comparedto the one closer to a real camera (top left) (2.74dB drop). When the IES is applied, the quality of both images(bottom left and bottom right) improves, and the PSNR gap decreases (1.32dB).
Figure 6: Qualitative comparison with NeRF and NeRF++ on the scene “Livingroom”. Our method generatessharper results than the comparison algorithms.
Figure 7: Qualitative comparison of our method with or without features in proxy depth estimation.
Figure 8: Qualitative visualization on different input view number. The three images are from our generatedscenes “Bar”, “Livingroom” and “Diningroom” respectively. Here we compare the reconstruction results byonly 2 input views and 8 input views.
Figure 9: Qualitative comparison of our algorithm with 360SD-Net (Wang et al., 2020) and MatryODShka (At-tal et al., 2020).
Figure 10: Qualitative comparison of our algorithm with 360SD-Net (Wang et al., 2020) and MatryOD-Shka (Attal et al., 2020).
