Figure 1: Top-1 accuracy of various structuredpruning methods (by compressing the ResNet-50 or ResNet-100 architecture) on the ImageNetdataset plotted against the number of parameters inthe model. Our methods, Adjoined Networks (AN),and Differentiable Adjoined Networks (DANs)achieve similar accuracy as compared against cur-rent SOTA pruning methods but with fewer (inmany cases 2x fewer) parameters.
Figure 2: Training paradigm based on adjoined networks. The original and the compressed versionof the network are trained together with the parameters of the smaller network shared across both.
Figure 3: (Left) Standard layer of CNN. (Center) Layer in Adjoined Network. (Right) Layer in DAN.
Figure 4: Top-1% accuracy of various pruning methods (by compressing ResNet-56 architecture)on CIFAR-10 dataset plotted against number of parameters (Left) and FLOPs (Right). Pruningmethods - Gal(Lin et al. (2019)), Hrank(Lin et al. (2020a)) ,He at al(Lin et al. (2019)), ENC(Kimet al. (2019)),NISP(Yu et al. (2018b)), L1(Li et al. (2017)), ABC-Prunner(Lin et al. (2020b)),CaP(Minnehan & Savakis (2019)), KSE(Li et al. (2019)), FPGM(He et al. (2019)), GBN(You et al.
