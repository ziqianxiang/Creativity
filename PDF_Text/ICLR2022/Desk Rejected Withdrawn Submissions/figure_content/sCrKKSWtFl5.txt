Figure 1: We propose an inverse rendering framework that estimates the surface normal, diffusealbedo, specularity, and shadow ofan object. Our method learns the specular basis to fit the observedspecularities accurately and gives clues for normal estimation. We also explicitly parameterize theshadows based on the estimated depth, alleviating artifacts on these shadows.
Figure 2: The four modules of our MLP-based deep photometric stereo framework: (a) specularitymodeling SΦ (see Sec. 3.2) fits a suitable set of suitable BRDF bases to the target specularities;(b) surface modeling MΘ (see Sec. 3.3) estimates the surface normal, as well as parameters of theBRDF given the image coordinates as input; (c) ZΨ estimates a dense depth map, which enables theshadow rendering (see Sec. 3.4) by checking the visibility of the light source at each surface point;and (d) the rendering equation (see Sec. 3.1). All MLPs are optimized in a self-supervised mannerby minimizing the reconstruction error between reconstructed and observed images.
Figure 3: Visualization on the estimated svBRDFs. We select four different surface points on theobject “Harvest” and showcase our estimated BRDF spheres on the right. The results demonstratethat our model can recover the metallic and diffuse materials. We scale up the observed images andnormalize the BRDF spheres for better visualization.
Figure 4:	Shadow parameterization and rendering. As shown in the left figure, shadows are causedby self-occlusion. To determine whether a surface point x falls into the shadow region, we trace thepoint to the light source and sample multiple points x(t) along this ray. Given the light direction land the estimated depth map ZΨ (x), we can query the depth and compare the values to effectivelyparameterize and render the shadow by Eq. (7).
Figure 5:	Qualitative results on “Cow” and “Pot2”. For each object, the odd numbered rows showthe observed image and estimated normal by different methods; the even numbered rows show theangular (normal) error in degrees by different methods.
Figure 6: We select four different light directions. Their distribution is labeled as red points in thelight distributions image in the second row. The first row shows the observed images under thesefour different light sources. The second row presents the results of our rendered shadow region underthe corresponding illuminations. In the third row, we showcase the estimated depth, ground truthsurface normal, estimated surface normal (with and without the shadow factor). In the right-mostimage on the third row, we also compare our estimated normal “w/ shadow” and “w/o shadow”. Theblue color in the comparison corresponds to the area where “w/ shadow” outperforms “w/o shadow”.
Figure 7:	Network architecture of our three MLPs: SΦ , MΘ , ZΨ . In this figure, inputs of thenetwork are shown in the green blocks; outputs are shown in the red blocks. The blue blocksrepresent the fully-connected layers with its size of the hidden channels stated on the top. All fully-connected layers are followed by a ReLU activation layer, except the output layers. The “㊉” in themiddle of the MΘ, ZΨ network denotes the vector concatenation: we add a skip connection after thefourth layer of MΘ , ZΨ, and concatenate its output features with the input.
Figure 8:	Normal estimation on specular objects: “Goblet”, “Reading” and “Harvest”. Asshown in the observed image of these three objects, the “Goblet” is mostly made of metallic materi-als; “Reading” and “Harvest” present many specular effects over the clothes. Our method achievesthe best performance in all these three objects, especially in those regions with high specularities.
Figure 9: Comparison on different choices on the specular model. The above shows the estimatedmaterial of our model with different choices on the specular basis. We can see that, from the object“Goblet” on the left side of the figure, the MLP basis is advantageous in representing metallic effectwith high-peak and long-tail effects than the SG basis. For objects with soft and broad speculareffects, e.g. the ceramic cat on the right of the figure, both models achieve comparable results.
Figure 10: Visualization on the estimated svBRDFs. This figure shows the estimated svBRDFsof the surface points on the objects. On the left is the observed image of “Reading”. We select fourdifferent surface points on the object and showcase our estimated BRDF spheres of those pointson the right side. The results demonstrate that our learning model on the BRDF can accuratelyrecover the materials from metallic to diffuse. Noted that, for better visualization, we normalize theBRDF spheres to have the maximum intensity to be 1. The observed images are also scaled up forvisualization.
Figure 11: Visualization on each terms of the rendering equation Eq. (1). In the above images, the first column displays the observed images of the objects. Thesecond and third column are the estimated diffuse albedo pd, and specular components ps. The fourth column is the shading map, which is computed by the dotproduct between light direction and surface normal (lτn). The last column is the estimated shadows, corresponding to S in the equation. As seen from the diffusealbedo map in the cloth of the “Harvest“ and the small patterns on the “Cow”，the estimated diffuse albedo map retains the objects, fine details. These resultsdemonstrate that our method can recover the fine details of the svBRDF map in the object. Note that, for better visualization, the images we selected here are allilluminated by a front light source. Hence, as can be seen in the observed images, there is little shadows.
Figure 12: Estimated specularities and shadows under different illuminations. The leftmostcolumn presents the ground truth normal and our estimated normal and depth as a reference of theobject’s geometry. We shoW the estimated specular components ρs and estimated shadoWs underthree different extreme lighting directions in the right-three columns. In “CoW”, the object is gener-ally smooth, and our estimation of the shadoWs also visually match the observed images. “Harvest”has a complex geometry and consists of many depth discontinuities over the surface. As discussedin the Sec. 6 of the main paper, our method is influenced by these regions and Will generate a“shalloWer” depth map. Hence, the estimated shadoWs are generally under-estimated.
Figure 13: Results on Gourd&Apple dataset (Alldrin et al., 2008). The columns from left to right are the observed images, our estimated diffuse albedo,Specularities, shading, and surface normal of the objects.
Figure 14: Results on Light Stage Data Gallery (Chabert et al., 2006). The columns from left toright are the observed images, our estimated normal, diffuse albedo, and specularities of the objects.
