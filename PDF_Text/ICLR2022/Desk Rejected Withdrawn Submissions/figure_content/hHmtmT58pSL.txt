Figure 1: A) Protein sequences exhibit functional values that form a complex fitness landscape asillustrated by Romero & Arnold (2009). Predicting the fitness of a given protein sequence is animportant challenge in biology. B) Protein fitness prediction tasks can be further subdivided basedon their problem setting. The number of homologous sequences, level of epistasis, extrapolation tohigher edit distances, and amount of labeled data in few-shot scenarios are major factors to considerfor fitness modeling. C) We introduce gf-tuning, a procedure to tailor a generative LM towardoptimal fitness prediction.
Figure 2:	After evolutionary finetuning, gf-tuning achieves the best overall performance in fitnessprediction on higher homology datasets, which contain a large number of evolutionarily relatedsequences. These results suggest that gf-tuning is the most robust way to adapt the representationlearned from related sequences to assay labeled data.
Figure 3:	ProGen with gf-tuning performs significantly better than baseline models on low homol-ogy domains, which contain a smaller number of evolutionary sequences. Performance is strongeston the lowest homology setting (GB1). In a medium-low homology setting on GFP, augmenteddensity modeling is competitive with gf-tuning.
Figure 4:	ProGen with gf-tuning is competitive with linear regression-augmented ProGen on fitnessprediction of multiple mutations on non-epistatic datasets, when trained on single mutations alone.
Figure 5:	gf-tuning is significantly better than baselines on epistatic protein datasets, when trainingon single mutations and testing on multiple mutations. Since adding the effects of single mutationsis a weak predictor of protein fitness for multiple mutations in these proteins, models that predictthese effects additively (e.g., augmented density modeling) or models that may treat the effect ofeach mutation independently (e.g., masked language modeling) do not perform as well.
