Figure 1: We consider the novel task of unsupervised data selection for semi-supervised learning(SSL). a) Existing SSL methods focus on training the best model given labeled and unlabeled data,whereas we focus on optimizing labeled data selection. b) Existing AL methods learn a classifierbased on random initial selection and alternate between training and instance selection, making itsannotation pipeline both difficult to implement and inefficient under low shot settings.
Figure 2: We quantitatively and qualitatively compare our methods against previous works on fourproperties, i.e., (a) classification accuracies on downstream tasks, (b) class coverage, (c) semanticclass distribution and (d) informativeness of queries, and drastically improve on all of them. Fix-Match* and FixMatch in (a) denotes sampling with “supervised” stratified sampler, random sampler,respectively. Different dot colors in (d) denote different classes. 20 instances are sampled from these3 classes with various sampling strategies. Our selections consistently cover informative samplesacross the space. (better viewed in color and zoomed in)2021). We shift from past model-centric SSL to data-centric SSL, focusing on improving SSLperformance by optimizing data selection from large-scale unlabeled data for annotation (Fig. 1a).
Figure 3: Two-hop propagation considers the in-direct connection between two vertices, and moreaccurately estimate the information disseminationability of each vertex. The overall distance, i.e.,cost(∙), of propagating semantic information fromvertex A to vertices B, C and vertex D to ver-tices E, F are illustrated, where the purple andmagenta lines denote one-hop and two-hop propa-gation, respectively. Numbers shown here are dis-tances between two vertices.
Figure 4: A case where local density alonegives a bad querying results. If we only sam-ple three instances to be annotated from theregion with the highest estimated local den-sity, only the three samples in cluster A willbe queried due to their highest density. How-ever, this may impair the ability of queries tocapture multiple data distribution patterns andis less efficient in propagating semantic infor-mation to all unlabeled data.
Figure 5: Sampling with regularization (right) can produce moreuniform and diversified queries. On the contrary, the queries ac-quired without regularization (left) are usually located in a smalldense region and have high correlations, losing the overall repre-sentativeness. (better viewed in color)clusters’ selected samples and gradually adjust their selections for better diversity and coverage.
Figure 7: Illustration of the behavior of differentutility estimations. K NNMax refers to vanilla K -NN density estimation that takes the max distance ofK nearest-neighbors. Ball Counts counts neighborswithin a radius. Our methods (with *): 1-hop is ourmethod without regularization or second order term.
Figure 6: Illustration of our methods withdifferent ablations on CIFAR-10 with 40samples. In both SSL methods, applying 1-hop propagation gives a large improvements(11.9% and 2.9%, respectively), and regular-ization as well as 2-hop propagation togethergive an additional improvement of 3.6% onTransfer Learning and 5.3% on FixMatch.
Figure 8: Effect of different hyperparameters, λ (Fig.a,b,c) and K (Fig.d). λ balances representativeand uniformity across the feature space. Larger λ indicate more uniform choices but potentiallyless representative samples, or vice versa. Larger K indicates that we are taking more neighborsinto account when estimating the representativeness, which reduces variance but may consider non-relevant samples as being represented. Due to this trade-off, considering more neighbors usuallyleads to better representativeness estimates, until K is greater than its optimal choice, 400.
