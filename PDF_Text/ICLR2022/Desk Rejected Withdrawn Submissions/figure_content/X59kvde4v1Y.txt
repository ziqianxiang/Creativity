Figure 1: Proposed DSDF approach. The individual target Q-values are multiplied with predicted discountedvalues and combined with reward obtained from environment. Finally we use this value to update the Nutility networks and mixing network (In case of QMIX approach)The θh, θu (utility networks) and θm (mixing hypernetwork) are interlinked with each other. The general lossfunction of QMIX isBL(θ) = X (yt - Qtot(τ, u,s : θ))	⑴t=1where θ is the set of parameters of N utility agents (θu,i, i = 1,…，N) and mixing hypernetwork θm,computed over B episodes. Now, if we expand ytyt = r + γ max Qtot (τ0, u0, s0 : θ-)	(2)u0Now, instead of using single γ value we will take the γ inside the equation and we use the mixing networkfunction to calculate the value of Qtotyt = r + max g(u0, s0, γiQu,i, θtot)	(3)u0Here g(.) is the mixing network architecture which is parametrized by θtot, Qu,i is the individual utilityfunction of agent i. Now, we will replace the γi with output of the network θγ . The replaced equation isyt = r + max g (u0,s0,fγ (。1 ,∙∙∙ ,oN, θγ )Qu,i ,θtot)u0
Figure 2: Degradation values of agents considered in each experiment for SMAC4.1.2 lbForaging environmentThis environment contains agents and food resources randomly placed in a grid world. The agents navigate inthe grid world and collect food resources by cooperating with other agents. The environment was modified byadding a couple of additions to the environment as mentioned in the appendix A.
Figure 3:	Returns obtained for StarCraft environment for all the experiments for different map scenariosalong with confidence intervals. The dots in the figures 3b, 3c and 3d are connected for intuitive comparison.
Figure 4:	Returns obtained for lbForaging environment along with discounted factor prediction plot. Theadvantage obtained using DSDF approach when compared with existing methods is not significant here sincethe environment requires lesser collaboration between agents when compared with complex SMACenvironments. However, the results for this environment is depicted as credit assignment for individual agentsis possible here.
Figure 5:	Comparison of individual agents performance using proposed DSDF approach vs existing ap-proaches for lbForaging environment. One can observe the deterministic agents achieved targets in almostall the test episodes whereas the stochastic agents consumed more resources when compared with existingmethods. This confirmed our look-ahead strategy works better and results in good collaborationall the execution episodes. On the other hand, the accordant agents trained with vanilla QMIX and IQLhave also reached their respective targets. The performance of the noisy agents trained using the proposedDSDF method as well as iterative penalization method outperforms the vanilla QMIX and IQL. All the noisyagents with smaller targets reached their respective targets while the agent with higher target settled closer tothe target value. For the case of accordant agents, the agents trained using proposed DSDF method eitherperformed better (Agent 3 and 4) or shown comparable performance (Agent 1) when compared with QMIXand IQL. This could be attributed to the DSDF learning representation which helps accordant agents realizethe limitation of noisy agents and thereby assume additional work-load to compensate for underperformanceof noisy agents. On the other hand, the agents trained with iterative penalization method demonstrates at-parperformance with QMIX and IQl.
