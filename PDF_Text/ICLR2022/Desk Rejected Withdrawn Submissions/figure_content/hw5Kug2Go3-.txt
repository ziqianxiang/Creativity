Figure 1: Variational autoencoder with standard and prototypical regularization schemas. While thestandard regularization considers a unique anchor distribution targeting N(0, 1), our approach con-siders regularization using K distributed anchors in the latent space targeting N (vk,1) distributions.
Figure 2: Exploring interpretability using synthesis from prototypes coordinates. For MNIST it ispossible to observe that prototype coordinates usually reconstruct to known digits, especially withprototypes larger than 10, as we have 10 digits represented in the dataset. For CIFAR-10, one cannotice that the learned clusters are gathered around role models that depict associations betweenforeground and background patterns, which become more detailed with the increase of the number ofprototypes available for anchoring clusters.
Figure 3: t-SNE plots considering a non-regularized autoencoder, a variational autoencoder with KLdivergence loss, and our models with 1, 10, 30 and 50 prototypes for the MNIST dataset. One canobserve that increasing the number of prototypes implement disambiguation of digits in the latentspace.
Figure 4: Stochastic synthesis examples considering 10 prototypes manually chosen correspondingto the 10 digits from our 30-prototype trained model.
Figure 5: Reconstructed CIFAR-10 samples for different models.
Figure 6: Reconstruction mean-squared error evolution for training and testing sets during training.
