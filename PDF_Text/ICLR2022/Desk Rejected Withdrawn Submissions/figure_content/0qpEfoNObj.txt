Figure 1: Visualization of weight volume and features of the last layer in a CNN on MNIST, withand without dropout during training. Plots (b) and (d) present feature visualization for normal anddropout CNN, respectively. We can see that the dropout network has clearer separation betweenpoints of different classes - a clear Sign of a better generalization ability. As We Win argue in thispaper, this better generalization is related to the “weight expansion” phenomenon, as exhibited inplots (a) and (c). For (a) and (c), the Weight volume of the normal CNN and the dropout CNN is0.1592 and 0.3239, respectively. That is, the application of dropout during training “expands” theweight volume. For visualization purpose, We randomly select three dimensions in the Weight co-variance matrix and display their associated 3-dimensional parallelotope. Both (a) and (c) contains3 × 3 = 9 such parallelotopes. We can see that those parallelotopes have a visibly larger volume inthe dropout netWork. Details of the netWorks are provided in Appendix A.
Figure 2: We train two small NNs (64-32-16-10 network with/without dropout) on MNIST. (b)shows the changes in weight volume along with epochs. There are four sampling points (①②③④) totrack the absolute normalized covariance matrix (correlation matrix) of normal gradient updates (a)and dropout gradient updates (c). The results for VGG16 on CIFAR-10 are provided in Appendix C.
Figure 3: Weight volume w.r.t. dropout rate.
Figure 4: Disentanglement noise vs. dropout. We have trained 2 VGG16 networks (Left) and2 AlexNet (Right) on ImageNet-32 and CIFAR-100 respectively, with normal training, dropouttraining, disentanglement noise training (volume expansion) and stochastic noise training (volumecontraction). For each data set, we present their test losses in different plots. More experiments onthe CIFAR-10, SVHN are given in Appendix N.
Figure 5: We have trained two VGG16 networks (with/without dropout) on CIFAR-10. (b) shows thechanges in weight volume along with epochs. There are four sampling points (①②③④) to track theabsolute normalized covariance matrix of normal gradient updates (a) and dropout gradient updates(c).
Figure 6: (a) We sample 10000 3-dimensional correlation matrices and show their determinant andaverage absolute correlation. (b) We sample 10000 4-dimensional correlation matrices and showtheir determinant and average absolute correlation. (c) We train four small NNs (64-32-16-10 withnormal, dropout, weight decay, and batch normalization, respectively) on MNIST. The figure showsthe changes in weight volume along with epochs for these four networks.
Figure 7: Disentanglement noise v.s. dropout. We train 2 VGG16 networks (Left) and 2 AlexNet(Right) on CIFAR-10 and SVHN respectively, with normal training, dropout training, disentangle-ment noise training (volume expansion) and stochastic noise training (volume contraction). For eachdataset, we present their test losses in different plots.
