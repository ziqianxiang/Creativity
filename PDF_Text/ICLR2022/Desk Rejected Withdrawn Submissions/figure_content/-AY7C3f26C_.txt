Figure 1: Problems of state-of-the-art face restoration models. GPEN and GFPGAN are biasedtoward face generation and may alter facial details (e.g. eye color) that are highly correlated withthe identity. DFDNet is biased toward reconstruction and does not remove all degradations. Ourapproach achieves the best balance and restore a high quality face while preserving the identity.
Figure 2: The proposed model with one skip connection. (1-NN: 1-nearest neighbor search. Modu-lation: feature modulation as in StyleGAN2 (Karras et al., 2020). LGF: linear gated feature fusion.)enhances the identity information. Although they have shown inspiring performances, the priorknowledge still dominates throughout the finetuning process, leading to unfaithful restoration, e.g.,color shift and excessive image completion. In other words, when the generator has enoughcapacity, high-fidelity generation is easy, however, faithful generation is hard. Notably, ourapproach can be trained from scratch and reaches a good balance between content preservation andgeneralization ability after tracing the causes of the above issues.
Figure 3: Qualitative comparison by varying the number of skip connections. We count from thelayer with feature resolution 8 × 8, i.e., there exist possible skip connections at resolution nodes{2n+2 × 2n+2}6n=1 when we set the maximum input resolution at 512 × 512.
Figure 4: Illustration of iPrecision. (a) Precision measures the portion (overlapped area) of restoredimages (blue region) that fall into real images (red region). (b) For each restored image, we deter-mine whether it falls into real image manifold by calculating its vectorized feature distance to everyreal image. (c)-(e) show the decision of one restored image e. We consider four neighbors of eachreal image and identities satisfy Ie = Ie0 , Ie 6= Ie0 .. (c) e is the nearest neighbor of e01 and bothhave the same ID. (d) show e is not inside the k-nearest neighborhood. (e) e and e02 have differentIDs though e is the nearest one. Among (c)-(e), only (c) counts as a correct match with iPred = 1.
Figure 5: The advantage of using the pro-posed metric iPred.
Figure 6: Qualitative comparison. (Top) BFR. Note the eyelash and skin tone difference. (Bottom)×16 : 322 → 5122 SR. Note the expression and wrinkle differences.
Figure 7: (a) iPrecision and iRecall with different neighborhood sizes on BFR. DEG* indicates thecreated degraded images. (b) Precision versus recall from various approaches and different tasks. (denotes BFR task,	for ×8 SR and 4 for ×16 SR.)5.3	PROPOSED iPrecision AND iRecall METRICSTo validate whether the proposed metric is more effective than others in face restoration, we start byablating neighborhood size as in Figure 7(a), (i) Increasing the neighborhood sizes leads to higherprecision and recall by allowing more misses. (ii) Our approach consistently gives the best restora-tion quality even when we set the neighborhood size k = 1, meaning that restored faces with ourapproach are the closest ones among all 3K testing images to the sources. (iii) Varying neighbor-hood size would not change ranking order of different methods, demonstrating the robustness ofproposed metrics, therefore in our experiments, we set k = 4. Moreover, as is shown in Figure 7(b),including hard-coded identity information indeed produces more discriminative numbers than cal-culating distances only. In a word, low precision and recall tell us that the model is very likely togenerate a “fake face” of some different person even if the appearance is sharp. By default, we useFaceNet as the feature extractor and find Inception V3 gives similar result in Appendix B.
