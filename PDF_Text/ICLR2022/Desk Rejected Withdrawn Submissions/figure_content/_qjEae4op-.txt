Figure 1: Schematic view of steps for building the MoFE model. In the first step, it uses automatedfactual consistency metrics to filter out training samples with the desirable factual quality. Then inthe second step, it trains reference- and model-based expert models on the filtered and whole trainingset respectively. Finally, in the third step, it combines the best-performing experts through weightsand logits ensembling.
Figure 2: Percentage of overlapped n-grams in XSUM and CNN/DM summaries.
Figure 3:	Examples where MoFE generates fewer novel entities (highlighted in red) that are absentfrom the source article.
Figure 4:	In this example, BART hallucinates percentage amount (50%). MoFE replaces percentageamount to a generic word sharply. Both BART and MoFE hallucinates DIY.
Figure 5:	In this example, BART hallucinates rush hour. In contrast, MoFE generates factuallycorrect summary.
Figure 6:	Both BART and MoFE generate different factual errors, BART hallucinates more than aweek and MoFE hallucinates Aberdeenshire.
Figure 7:	In this example, BART incorporates world knowledge “end of apartheid” and is factuallyconsistent otherwise. MoFE adds factual error “two years”.
Figure 8:	Both BART and MoFE are factually correct, though BART generates US rocket companywhich can not be inferred from the source document (hallucinations vs world knowledge).
Figure 9:	Both BART and MoFE are factually correct, though MoFE replaces EU with EuropeanUnion (world knowledge).
