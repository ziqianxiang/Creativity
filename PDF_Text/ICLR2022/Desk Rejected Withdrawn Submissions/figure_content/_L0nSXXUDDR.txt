Figure 1: To address the problem of noisy labels in the training set, we propose Neighbor ConsistenyRegularization. This regularizer encourages examples with similar feature representations to havesimilar outputs, thus mitigating the impact of training examples with incorrect labels.
Figure 2: Ablation study. Impact of hyperparameters Î±, k and e are evaluated on the CIFAR-10validation set. The ResNet-18 architecture is used.
Figure 3: Predicted confidence of training examples. Whereas the baseline assigns similarconfidence scores to clean and mislabelled examples (usually all high), NCR more often assigns lowerconfidence to mislabelled examples and higher confidence to correct examples.
Figure 4: Similarity distributions. We compare the distribution of cosine similarities for trainingexamples in mini-ImageNet that are correctly and incorrectly labelled as the same class or differentclasses. For mini-ImageNet-Blue, the features learned using NCR achieve significantly better classseparation with 40% noise (or less, not pictured). For the more realistic mini-ImageNet-Red, NCRstill achieves better separation of the clean examples but fails to separate examples that are incorrectlylabelled as the same class.
