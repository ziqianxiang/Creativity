Figure 1: An example illustrating data augmentation. 1a shows the original code that is adapted fromthe CVE-2021-38094 patch. 1b shows functionality equivalent code of 1a where the original codeis transformed by renaming and statements permutation. 1c shows a small variation from 1a wherea potential integer overflow bug is injected.
Figure 2: An illustration of Boost pre-training with a minibatch of three. The original code and itsnode types will be randomly masked with [MASK], and the final representation of masked tokenswill be used to recover their source tokens and node types. The original code, say x, will also betransformed to build (x, x+, x-). Then the pair will be fed into the same transformer encoder andget the embedding of each sequence with [CLS] tokens for contrastive learning.
Figure 3: The evaluation perplexity of lastfive epochs for different Boost variations.
Figure 4: An example in REVEAL dataset. The patched code happens to be in the train split andthe buggy code is in the test split. During inference, Boost MLM+CLR_HN model can correctlypredict the buggy code as vulnerable, while MLM-CLR predicts it as benign.
