Figure 1: Comparison of error flows among MentorNet (the self-teaching version), Co-teaching,Decoupling, Co-teaching+, JoCor, and Co-variance (ours). Assume that error flows comes from thebiased selection of training examples, and the error flow from network A or B is denoted by redarrows or blue arrows, respectively. The 1st panel: MentorNet maintains only one network A. The2nd panel: Co-teaching maintains two networks (A & B). In each mini-batch data, each networkselects its small-loss data to teach its peer network for robust training. The 3rd panel: Decouplingupdates the two networks with prediction-disagreed (!=) examples from a mini-batch. The 4th panel:In Co-teaching+, each network selects its small-loss instances within prediction disagreement (!=)to teach its peer network. The 5th panel: JoCor trains two networks as a whole with a joint loss,which makes predictions of each network closer to peer networkâ€™s. The 6th panel: Co-variance alsomaintains two networks (A & B). In each mini-batch data, each network selects its small-loss datathat meanwhile have high variances among two networks, to teach its peer network.
Figure 2: Illustrations for two types oflong-tailed datasets.
Figure 4: Illustrations of the hy-perparameter sensitivity for the pro-posed method on four imbalancednoisy datasets. The error bar forstandard deviation in each figurehas been shaded.
Figure 3: Mean and standard deviations of test accuracy (%)on two closed-set noisy datasets with different noise levels.
Figure 5: Test accuracy vs. the number of epochs on four long-tailed noisy datasets. The error bar forstandard deviation in each figure has been shaded.
Figure 6: Illustrations of the hyperparameter sensitivity for the proposed method. The error bar forstandard deviation in each figure has been shaded.
