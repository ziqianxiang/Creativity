Figure 1: Our PR-Net avoids solving integral problems by learning a regression model that conformswith a learned governing equation.
Figure 3: The general architecture and the training algorithm of PR-NetGiven a Solution function h and itS partial derivative termS, We train αi,j by minimizing the objectiveloSS. Note that We knoW h in thiS caSe. Therefore, the objective loSS iS defined With h rather thanWith f, unlike Eq. 5. The optimal Solution of αi,j iS not unique SometimeS. HoWever, We note that notrivial SolutionS, e.g., αi,j = 0 for all i,j, exiSt for the inverSe problem.
Figure 4: The curves of log loss valuesdecrease as training goes on for MNIST100	200	300	400Iters5	Discussions & ConclusionsIt recently became popular to design neural networks basedon differential equations. In most cases, ODEs are usedto approximate neural networks. In this work, on the otherhand, we presented a PDE-based approach to design neuralnetworks. Our method simultaneously learns a regressionmodel and a governing equation that conform with eachother. Therefore, the internal processing mechanism of thelearned regression model should follow the learned gov-erning equation. One can consider that this mechanism isa sort of implanting domain knowledge into the regressionmodel. The main challenge in our problem definition isthat we need to discover a governing equation from datawhile training a regression model. Thus, we adopt a jointtraining method of the regression model and the governingequation.
Figure 5: Representation inversion with Tiny ImageNet14Under review as a conference paper at ICLR 2022(b) ODE-Net(a) ResNet(c) PR-NetFigure 6: The visualization of feature maps for MNIST. We use t-SNE to project the feature mapsonto a two-dimensional space.
Figure 6: The visualization of feature maps for MNIST. We use t-SNE to project the feature mapsonto a two-dimensional space.
Figure 7: The visualization of feature maps for SVHN. We use t-SNE to project the feature mapsonto a two-dimensional space.
Figure 8: Extrapolation results by two neural networks (with identical architectures) that are aware orignorant of the governing equation of the Allen-Cahn equation. The blue solid lines are referencesolutions and the red dotted lines are extrapolation predictions. In all cases, better results are obtainedwhen a neural network is aware of the governing equation, i.e., trained with LG .
Figure 9: Out-of-distribution examples+J*;;."%:+(a) FGSM(b) PGDFigure 10: Adversarial attack examples. Goldfish with a confidence of 0.8931 is perturbed to torchwith a confidence of 0.3546 in (a) and to candle with a confidence of 0.4810 in (b).
Figure 10: Adversarial attack examples. Goldfish with a confidence of 0.8931 is perturbed to torchwith a confidence of 0.3546 in (a) and to candle with a confidence of 0.4810 in (b).
Figure 11: An illustration for MNIST/SVHN on how to increase the processing efficiency bydiscretizing the last dimension and share (d, t) pairs.
