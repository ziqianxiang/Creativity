Figure 1: Illustrations of motivation (a) and overview (b) of G2RL. (a) The basic idea of G2RL is to leveragethe state-transition graph to build a differentiable goal generator and an efficient policy learner, where the firstone can be learned by supervised learning with hindsight supervisions, and the second one can be updated byreinforcement learning with relevance sampling. (b) G2RL first constructs the state-transition graph upon thereplay buffer and then uses the graph to generate goal appropriate ge and select relevant trajectories Drel. Whenencountering the continuous environments, we need to apply a space discretization technique mapping a bin ofstates into a node for graph structure.
Figure 2: Illustrations of inference and training steps of our goal generator. During inference, we first constraintthe candidate set of goal generation within the boundary of graph ∂Ge with theoretical guarantee (a), and thendivide the nodes into several groups (b). A differentiable attention network is applied to select one group CATT(c), where the node with the highest value is assigned as the goal ge (d). Our goal generator is trained in ahindsight fashion. Specifically, we first find the optimal path by planning on Ge+1 (e), and then obtain theoptimal goal node g* as the most valuable and reachable node in the path Pe+ι (f). The supervision signal isthe optimal group C* containing g* (g). Note that Lφ with group supervisions instead goal supervisions cansignificantly eliminate instability brought from potentially inaccurate value estimation (h).
Figure 3: Comparison of learning curves of our model G2RL against baselines algorithms on on variousenvironments average across 10 different random seeds. The solid curves depict the mean, the shaded areasindicate the standard deviation, and dashed horizontal lines show the asymptotic performance.
Figure 4: Comparison of learning curves of G2RL in different settings mainly on Ant Maze average across 10different random seeds. These ablation studies verify our architecture design and evaluate the generalizability.
Figure A1: An illustrated example of notations in G2RL. We use purple circle to denote the start node vstartIn this paper, we construct a state-transition graph G on top of the replay buffer D. Specifically, asFigure A1(a) shows, we use Ge to denote the graph built based on historical explored trajectories atthe beginning in episode e. We use vstart and vtarget to denote the start and target nodes. Exceptthe whole state-transition graph containing all possible transitions, there exist many poorly-exploredstates. We measure the exploration (defined as certainty in Section 2.3) of these states according tothe number of their untaken candidate actions. As an untaken action may lead to certain unvisitedstate, we further introduce the definition of the boundary of graph as the set of all the states with atleast one untaken action. We use ∂Ge as the boundary of Ge, as Figure A1(b) shows. As shown inFigure A1(c), after the explorations of one episode, the agent encounters several new states, the setof which is defined as the graph increment. Formally, We use ∆Ge+1 to denote the graph incrementfrom Ge to Ge+1, namely Ge+1 = Ge ∪ ∆Ge+1.
Figure A2: An illustrated example for thestate-transition graph.
Figure A3: An illustrated example of the expansion of the state-transition graph and intuition of the proof. Thenatural expansion of the graph is by adding new trajectories, as shown in (a)-(c). In the proof of Proposition 1,we consider the expansion by involving the nodes within the neighborhood, namely adding nodes column-wisely instead of row-wisely in (d).
Figure A4: An illustrated example for connection between certainty and number of visited states.
Figure A5: An illustrated example for comparison of using the certainty of states and using the visitation countsof states to define the graph boundary. The number at the center of each state denote its visitation number.
Figure A6: Environments used in our experiments. (a) Maze, where the ant agent learns to reach the specifiedtarget position (-ball depicted in red) located at the other end of the U-turn. (b) Ant Maze constructed based onMaze but high-dimensional, where the state space is of 8 dimensions and the action space is of 30 dimensions,and Low-Stochastic and High-Stochastic Ant Maze built based on Ant Maze where the connectivity betweeneach pair of neighbor cells will change at the probability of 5% and 20% in each episode respectively. (c)FetchPush, where the agent is trained to fetch an object from the initial position (rectangle in green) to a distantposition (rectangle in red). (d) FetchPush with Obstacle constructed based on FetchPush but harder, where thebrown block is a static wall that cannot be moved. (e) VizDoom built based on maze but image data as input,where the agent is required to navigate towards the goal in a maze environment with a image as the state. (f)Matterport3D built based on maze but scenes, which are 3D reconstructions of real-world environments, asinput, where the agent is required to reach the target scenes. Some of these illustrations are adopted from theprevious literature (Florensa et al., 2017; Savinov et al., 2018; Ren et al., 2019; Chaplot et al., 2020a).
Figure A7: Visualization of three different mazes designed for Ant Maze environment including TRAIN (a),TEST1(b), TEST2(c). We train G2RL in TRAIN and test it in TEST1 and TEST2 to evaluate the generalizability.
Figure A8: Comparison of learning curves of our models G2RL in different settings mainly on Ant Mazeenvironment average across 10 different random seeds. The solid curves depict the mean, the shaded areasindicate the standard deviation, and dashed horizontal lines show the asymptotic performance.
Figure A9: Visualization of generated goals by G2RL, SoRB and HER in AntMaze environment. The startand the target states are illustrated as purple and red circles respectively. The blue circles indicate ten recentlygenerated goals. The subfigures in the top show the results of HER and the subfigures in the middle show theresults of SoRB, while the subfigures in the bottom show the results of G2RL.
