Figure 1: Model update process of FL, SplitFed and our idea. FL suffers from large time delay and communica-tion/computation burdens for exchanging/UPdating the full model. Although SplitFed can reduce the client-sidecomputation burden, it still requires large time delay since the clients should wait for the backpropagated signalsfrom the server in order to update their model. The communication burden can be also large for transmittingboth the forward activations and backward gradients at every global round. The proposed idea enables to save allthree resources (computation, communication, latency) simultaneously via model splitting and local-loss-basedtraining highly tailored to split learning. Our approach has significant advantage especially when clients havinglow computing powers and transmission rates (e.g., mobile/IoT devices) collaborate to train a large-scale model.
Figure 2: Test aCCuraCy versus CommuniCation load.
Figure 3: Test accuracy versus training time in an IID setup.
Figure 4:	Effect of client-side computing power PC and transmission rate R. FMNIST is utilized for trainingCNN in a non-IID setup. Our scheme is beneficial especially when the clients have relatively small computingpowers and small transmission rates (e.g., mobile/IoT devices).
Figure 5:	Effect of sequential update process at the server. CIFAR-10 is utilized for training VGG-11.
Figure 7:	Test accuracy versus training time. Here, we split the model in a different way compared to the setupin the main manuscript: more layers are allocated to the clients compared to the plots the main manuscript.
Figure 8:	Test accuracy with varying system parameters. Here, we split the model in a different way comparedto the setup in the main manuscript: more layers are allocated to the clients compared to the plots the mainmanuscript. MNIST is utilized for training CNN in an IID setup. Our scheme is beneficial especially when thecomputing powers and and the transmission rates of the clients are low (e.g., mobile/IoT devices).
