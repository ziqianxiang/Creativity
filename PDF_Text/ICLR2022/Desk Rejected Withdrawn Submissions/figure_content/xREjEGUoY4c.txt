Figure 1: Text-CNN model architecturepooling	feeFigure 2: RNN’s core structureods, Transformer completely abandons traditional feature extractors, such as CNN and RNN, anduses a self-attention mechanism. The specific structure diagram is shown in figure 3.
Figure 2: RNN’s core structureods, Transformer completely abandons traditional feature extractors, such as CNN and RNN, anduses a self-attention mechanism. The specific structure diagram is shown in figure 3.
Figure 3: Transformer network structureP E(pos,2i+1) = cos(pos/100002i/dmodel )(2)Self-attention mechanism and multi-headed attention machine are the main ”heritors” for featureextraction in Transformer. They can obtain long-distance dependence information between words inthe text. The multi-head attention mechanism is the integration of multiple self-attention structures,using multiple ”heads” to learn different characteristics. The calculation process of the attentionmechanism is expressed as mapping query and key and value pairs to the output. In actual use,there are multiple queries. In order to perform attention operations on these queries at the same time,these queries are assembled into a matrix Q. In the same way, we can get K and V from keys andvalues. Because it is self-attention, here Q, K, and V are all vectors in the same input, and the finaloutput is the weighted sum of all values. The specific calculation process is shown in equation (3).
Figure 4: Attention Mechanismnlog p(wk | wi, . . . , wk-1 ; ΘBERT).	(4)k=1Where ΘBERT represents the parameters of the BERT model, and w1 , w2, w3, w4, ..., wn representa sequence of words with a number of n. The derivation process of Equation (4) is as follows.
Figure 5: NLI Training Framework4	Knowledge distillation based on multiple teachersConsidering that human teachers teach students, it is impossible for a teacher to be proficient ineverything, so it is often necessary for different teachers to teach different subjects, so that every stu-dent can learn the specialties of each teacher. This article is inspired by the teaching of students bymultiple human teachers, and proposes a knowledge distillation based on multiple teachers. There-fore, in the knowledge distillation, the knowledge ofa single teacher model is often limited. Throughmultiple teacher models to ”teach” students together, the student model can learn the strengths ofthe teacher model, and the generalization effect of the student model is better.
Figure 6: Multi-teacher knowledge distillation flowchart5	Experimental results5.1	Dataset introduction5.1.1	Data SourcesThis paper collects data under real scenarios in a project cooperating with an electric power businessoffice. The members of the project team used the voice recorder to record the dialogue in the realscene offline. Therefore, this data set has very high authenticity. At the same time, the originalrecorded voice data contains a lot of noise, such as car whistle outside the business hall, printernoise in the business hall, and the original voice data also contains a lot of meaningless modalparticles, excessively long pauses, etc. This requires further processing of the original data.
