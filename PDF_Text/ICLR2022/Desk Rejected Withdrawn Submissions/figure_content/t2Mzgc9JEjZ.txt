Figure 1: Examples of column ablations for the left-most image with column width 19px.
Figure 2: Illustration of the smoothed vision transformer. For a given image, we first generate a setof ablations. We encode each ablation into tokens, and drop fully masked tokens. The remainingtokens for each ablation are then fed into a vision transformer, which predicts a class label for eachablation. We predict the class with the most predictions over all the ablations, and use the margin tothe second-place class for robustness certification.
Figure 4: Certified accuracies for ViT and ResNet models on CIFAR-10 and ImageNet for variousadversarial patch sizes. Certification was performed using a fixed ablation size b = 4 for CIFAR-10and b = 19 for ImageNet following Levine & Feizi (2020a).
Figure 3: Ablation accuracies for models on CIFAR-10 and ImageNet column-ablated images. Themodels were trained on column ablations of width b = 19 for ImageNet and b = 4 for CIFAR-10, and evaluated on a range of ablation sizes. ViTs outperform ResNets on image ablations by asizeable margin.
Figure 5: Certified (left) and standard (right) accuracies for a collection of smoothed models trainedwith a fixed ablation size b = 19 on ImageNet, and evaluated with varying ablation sizes. Certi-fied accuracy remains stable across a range of ablation sizes, while standard accuracy substantiallyimproves with larger ablations.
Figure 6: The average time to compute a forward pass for ViTs on 1024 column ablated imageswith varying ablation sizes, with and without dropping masked tokens. The cost of processing a fullimage without dropping masked tokens corresponds to the maximum ablation size b = 224.
Figure 7: Example ablations that we use in our paper.
Figure 8: Certified and standard accuracy for a smoothed model trained and evaluated on ImageNetcolumn ablations with varying widths. The ResNet-50 requires a substantially larger ablation sizefor certification, whereas the ViT-S is more flexible.
Figure 9: Certified and standard accuracy for a smoothed model on CIFAR-10 trained with a fixedablation size (b = 5), and evaluated with varying ablation sizes.
Figure 10: (a) Average time for computing a forward pass on a batch of 1024 image ablations onImageNet (b) Average time for computing a full training step (forward and backward pass) on abatch of 128 image ablations on ImageNetC.3 Effect of dropping tokens on performanceSince the tokens are individually positionally encoded, dropping tokens that are fully masked doesnot remove any information from the input. In Figure 11, we confirm that dropping masked tokensdoes not significantly change the accuracy of the ViT base classifier on ablations.
Figure 11: We compare the ablation accuracies of dropping masked tokens versus processing alltokens for a collection of vision transformers on CIFAR-10 and ImageNet. Dropping masked tokensdoes not substantially degrade accuracy.
Figure 12: Certified and standard accuracy of various ViTs for ImageNet when using strided columnablations with varying stride lengths.
Figure 13: Average time to compute a forward pass for ViTs on 1024 block ablated images withvarying ablation sizes with and without dropping masked tokens.
Figure 14:	Strided block smoothing on ImageNet for a collection of ViT models trained with blockablations of size b = 75.
Figure 15:	Strided block smoothing on ImageNet for ViT-B with a fixed ablation size b = 75.
