Figure 1: Structure of Composite Q-learning. Qi denote the Truncated and Qi：∞ the Shifted Q-functions at step i. Q is the complete Composite Q-function. Directed incoming edges yield thetargets for the corresponding value-function, evaluated at the maximizing actions of the full Q-function. Edges denoted by γ are discounted.
Figure 2: Visualization of Walker2d-v2 (left), Hopper-v2 (middle) and Humanoid-v2 (right).
Figure 3: (a) Mean performance and half a SD over 8 training runs for the Walker2d-v2 environmentwith the default reward. (b) Results over 8 training runs of Composite Q-learning on the vanillareward function and multiple update steps per collected sample for the Walker2d-v2 and Hopper-v2environment.
Figure 4: Normalized area under the learning curve for Composite TD3 in the Walker2d-v2 envi-ronment with different truncation horizons n. The plot shows median and interquartile ranges over8 training runs, each representing mean evaluation performance over 100 initial states.
Figure 5: Mean performance and half a SD over 8 training runs for (left) Walker2d-v2, (middle)Hopper-v2 and (right) Humanoid-v2 with uniform noise on the reward function as in (Romoff et al.,2019).
Figure 6: Performance and TD-errors on Humanoid-v2 of Truncated and Shifted Q-functions atn = 4 w.r.t. the (a) vanilla and (b) noisy reward. Please note that TD-error here means the deviationfrom the associated target.
