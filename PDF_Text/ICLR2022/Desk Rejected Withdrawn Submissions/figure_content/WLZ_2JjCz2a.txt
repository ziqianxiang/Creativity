Figure 1: STU-GAN directly trains sparsity-unbalanced GANs from scratch with an extremely sparsegenerator and a much denser discriminator. The training instability is mitigated by periodicallyexplore parameters for better sparse connectivities with a prune-and-grow scheme during training.
Figure 2: Effect of sparsity unbalance on pruning and fine-tuning. Experiments are conducted withBigGAN on CIFAR-10. Higher sG refers to fewer parameters remaining in generators. Globalpruning refers to pruning weights across layers and uniform pruning refers to pruning layer-wisely.
Figure 3: Effect of sparsity unbalance on GAN training. Higher sG and sD refer to fewer parametersremaining in the corresponding networks. Global pruning refers to pruning weights across layers anduniform pruning refers to pruning layer-wisely.
Figure 4: Comparisons between static sparse GAN and STU-GAN with various combinations betweensG and sD. The sparsity balanced setting with sD = sG is indicated with dashed red lines.
Figure 5: Effect of parameter exploration on different components. “Explore G”, “Explore D”, and“Explore G & D” refers to applying parameter exploration only to generators, discriminators, andboth, respectively. Experiments are conducted with 50% sparse discriminators, i.e., sD = 50%.
Figure 6: FID of sparse unbalanced BigGAN and SNGAN on CIFAR-10 with various ∆T. Theresults of BigGAN in trained with 95% sparse D(x, θsD) and 95% sparse G(z, θsG). The results ofSNGAN in trained with 50% sparse D(x, θsD) and 95% sparse G(z, θsG).
Figure 7: FID of sparse SNGAN trained with extremely sparse discriminator, i.e., sD = 90%.
Figure 8: Example Images Generated by STU-GAN on ImageNet 128 X 128. Each row of imagesare generated via STU-GAN models with various SG and SD = 50%.
