Figure 1: WaveMix Architecture3Under review as a conference paper at ICLR 2022long distances without using self-attention and thereby escaping the quadratic complexity bottleneckof self-attention.
Figure 2: Comparison of classification accuracy and GPU usage by various models on CIFAR-10dataset for a batch size of 64Accuracy vs Depth8684ÈÄÅ82Ao8 80n9978 78TO.
Figure 3: Variation of accuracy with depth for various WaveMix modelsThere is also a higher flexibility with the WaveMix architecture in controlling the number of pa-rameters, as we can separately change the embedding dimensions of the feature maps and output ofthe feed-forward sub-layers, since the embedding dimension of output of the WaveMix layer is onlydependent on how the concatenation of transposed convolutions are performed.
Figure 4: Impact of Dropout in various WaveMix ModelsFigure 3 shows the variation of accuracy in image classification with depth for different WaveMixmodels. We observe the general trend where accuracy of the model increases with addition of eachWaveMix layer across different model embedding sizes.
