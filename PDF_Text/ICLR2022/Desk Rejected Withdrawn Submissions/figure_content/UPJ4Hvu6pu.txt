Figure 1: A prevailing pipeline for training WSSS.
Figure 2: We visualize the effect of early learning (IoUei, green curves) and memorization (IoUm,red curves) on segmentation models trained with (solid lines) and without (dashed lines) ADELE foreach foreground category of a medical dataset SegThor Lambert et al. (2020). The model is a UNettrained with noisy annotations that mimic human errors. IoUel is the IOU between the model outputand the ground truth computed over the incorrectly-labeled pixels. IoUm is the IOU between themodel output and the incorrect annotations. For all classes, IoUm increases substantially as trainingproceeds because the model gradually memorizes the incorrect annotations. This occurs at differentspeeds for different categories. In contrast, IoUel first increases during an early-learning stage wherethe model learns to correctly segment the incorrectly-labeled pixels, but eventually decreases asmemorization occurs. Like memorization, early-learning also happens at varying speeds for thedifferent semantic categories. See Figure 8 in Appendix for the plot on PASCAL VOC.
Figure 3: Visual examples illustrating the early-learning and memorization phenomena. For severalimages in a medical dataset Segthor (Lambert et al., 2020) (top tow rows) and the WSSS dataset VOC2012 (Everingham et al., 2015) (bottom four rows), we show the ground-truth annotations (secondcolumn), noisy annotations (third column) obtained by a synthetic corruption process for the medicaldata and by the classification-based SEAM (Wang et al., 2020b) model for WSSS, the output of amodel segmentation model trained on the noisy annotations after early learning (fourth column), andthe output of the same model after memorization (fifth column). The model for the medical dataset isa UNet. The WSSS model is a standard DeepLab-v1 network trained with the SEAM annotations.
Figure 4: Illustration of the proposed curve fitting method to decide when to begin label correction inADELE (Results on SegThor). On the left, we plot the IoU between the model predictions and theinitial noisy annotations for the same model used in Figures 2 and 3 and the corresponding fit withthe parametric model in Equation 1. The label correction beginning iteration is based on the relativeslope change of the fitted curve. The center image shows the label correction times for differentsemantic categories, showing that they are quite different. On the right graph, the green line showsthe IoUel for a given category Heart. The IoUel equals the IoU between the model output and theground truth computed over the incorrectly-labeled pixels, and therefore quantifies early-learning.
Figure 5: Left: In the proposed multiscale-consistency regularization, rescaled copies of the sameinput (here upscaled ×1.5 and downscaled ×0.7) are fed into the segmentation model. The outputs(p1, p2 and p3) are rescaled to have the same dimensionality (p1, p2 and p3). Regularization promotesconsistency between these rescaled outputs and their elementwise average q. Right: Multi-scaleconsistency regularization leads to more accurate corrected annotations (results on SegThor, resultsfor VOC 2012 can be found in Figure 12).
Figure 6: The performance comparison ofthe baseline and ADELE on the test set ofSegTHOR (Lambert et al., 2020). The modelis trained on noisy annotations with variouslevels of corruption (measured in mIoU withthe clean ground truth annotations). ADELE isable to improve the model performance acrossa wide range of corruption levels.
Figure 7: Left: Category-wise comparison of the IoU (%) of SEAM (Wang et al., 2020b) and SEAMcombined with the proposed method ADELE on the validation set of PASCAL VOC 2012. Right:Visualization of the segmentation results of both methods for several examples. ADELE successfullyimproves segmentation for the first four examples, but not for the last two. We set the backgroundcolor to gray for ease of visualization. More examples can be found in Appendix A.1Figure 7 compares the performance of SEAM and the performance of ADELE combined with SEAMon the validation set separately for each semantic category. ADELE improves performance for mostcategories, with the exception of a few categories where the baseline model does not perform well(e.g. chair, bike). On the right of Figure 7, we show some qualitative segmentation results from thevalidation set. The first four rows show examples where ADELE successfully improves the SEAMsegmentation. The last two rows show examples where it does not. In both the output of SEAM hashighly structured segmentation errors: the prediction encompasses the bike but completely fails tocapture its inner structure, and the chair is missclassified as a sofa. This supports the conclusion thatADELE provides less improvement when the baseline method performs poorly.
Figure 8: We visualize the effect of early learning (IoUel, green curves) and memorization (IoUm,red curves) on segmentation models trained with (solid lines) and without (dashed lines) ADELEfor each category of the WSSS dataset VOC 2012 (Everingham et al., 2015) The WSSS model is astandard DeepLab-v1 network trained with annotations obtained from SEAM (Wang et al., 2020b).
Figure 9: Zoomed-in illustration of early learning for the different semantic categories on PASCALVOC 2012. IoUel first increases during an early-learning stage where the model learns to correctlysegment the incorrectly-labeled pixels, but eventually decreases as memorization occurs. Earlylearning happens at varying speeds for the different semantic categories. The experimental setting isthe same as Figure 2.
Figure 10: Illustration of the proposed curve fitting method to decide when to begin label correctionin ADELE (Results on Pascal VOC). On the left, we plot the IoU between the model predictionsand the initial noisy annotations for the same model used in Figures 9 and 3 and the correspondingfit with the parametric model in Equation 1. The label correction beginning iteration is based onthe relative slope change of the fitted curve. The center image shows the label correction times fordifferent semantic categories, showing that they are quite different. On the right graph, the green lineshows the IoUel for a given category. The IoUel equals the IoU between the model output and theground truth computed over the incorrectly-labeled pixels, and therefore quantifies early-learning.
Figure 11: Illustration of the proposed curve fitting method to decide when to begin label correctionin ADELE. The blue line shows the parametric model in Equation 1 fit to the IoU between the modelpredictions and the initial noisy annotations for the same WSSS model used in Figures 2 and 3.
Figure 12: Multi-scaleconsistency regularization leads to more accurate corrected annotations (resulton Pascal VOC).
Figure 13: Additional segmentation results of SEAM and SEAM+ADELE for several exampleson the validation set of PASCAL VOC 2012. We set the background color to gray for ease ofvisualization. Supplementary for Figure 7.
Figure 15: Additional segmentation results of SEAM and SEAM+ADELE for several exampleson the validation set of PASCAL VOC 2012. We set the background color to gray for ease ofvisualization. Supplementary materials for Figure 7.
Figure 16: Illustration of a possible limitation of the proposed label-correction approach on PASCALVOC 2012. The initial annotations coarsely segment the bike and misclassify the chair as a sofaconsistently in several training examples. This highly structured annotation noise could potentiallyprevent early learning from happening, and therefore from being exploited for label correction. Weset the background color to light gray for ease of visualization.
Figure 17: Visualization of the segmentation results of baseline and baseline+ADELE for severalexamples on the validation set of SegTHOR.
Figure 18: Ablation study for r on SegTHOR. We fixed the other hyperparameters to the defaultsettings in Table 7. We report the test mIoU (%) at the best validation epoch on the Left, the fullresult is shown on the Right.
Figure 19: Ablation study for τ on SegTHOR. We fixed the other hyperparameters to the defaultsettings in Table 7. We report the test mIoU (%) at the best validation epoch on the Left, the fullresult is shown on the Right. The result shows that ADELE is not very sensitive to the value of τ .
Figure 20: Ablation study for r and τ on PASCAL VOC. We fixed the other hyperparameters to thedefault settings in Table 7. We report the validation mIoU (%) at the last training epoch.
