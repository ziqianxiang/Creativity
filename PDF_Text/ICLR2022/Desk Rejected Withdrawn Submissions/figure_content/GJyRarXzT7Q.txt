Figure 1:	Balanced accuracy versus equalized odds for several fine-tuned LMs on the Jigsaw andHateXplain datasets.
Figure 2:	Balanced accuracy versus equalized odds for several fine-tuned LMs when varying onlythe random seed used in fine-tuning.
Figure 3:	Accuracy, balanced accuracy and equalized odds (religion) for fine-tuned LMs whenvarying the amount of data used in training and the random seeds. Error bars denote the Â±1 SE(standard error) of the mean.
Figure 4: BERT: Balanced accuracy versus equalized odds for several fine-tuned LMs on the Jig-saw dataset when varying epsilon and the threshold for binary classification after applying the FSTmethod for group bias mitigation.
Figure 5: BERT: Balanced accuracy versus equalized odds on the Jigsaw dataset when applying theFST and HPS methods for group bias mitigation and threshold post-processing (TPP) alone.
Figure 6: Balanced accuracy versus equalized odds for xseveral fine-tuned LMs (religion group)on the HateXplain dataset when applying the FST and HPS methods for group bias mitigation andthreshold post-processing (TPP) alone.
Figure 7: Accuracy versus equalized odds for several fine-tuned LMs when varying only the randomseed used in fine-tuning.
