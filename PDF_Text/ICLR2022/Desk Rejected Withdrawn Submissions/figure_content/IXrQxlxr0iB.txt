Figure 1: The comparison between Sparse Transformer (ST) and Hierarchical Sparse Trans-former (HST). (a) Sparse Transformer mainly consists of global attention and local attention .
Figure 2: The overall framework of our proposed SAOR. We take local sparse pattern for illustra-tion. The picture shows that one input x will go through the model twice and obtain two distributions,while the left sparse pattern is default and right one shows a rolled version sparse pattern.
Figure 3: Phenomenon of bottleneck and the effectiveness of HST: Accuracy across global token sizein the LRA benchmark. Performance degradation of ST are observed on all datasets when bottlenecksize decrease (blue line). With our proposed HST method, the performance is better (red line). Errorbar denotes standard deviation.
Figure 4: Runtime and memory of full self-attention, ERNIE-Sparse and BigBird (Zaheer et al.,2020).
