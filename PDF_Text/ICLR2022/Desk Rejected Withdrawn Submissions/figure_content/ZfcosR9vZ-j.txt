Figure 1: Unlike baseline optimal transport with randomly sampled mini-batches, our approach firstjointly encodes the entire source and target datasets according to our Pyramid Matching structure andsubsequently extracts mini-batches according to this structure. Exact Optimal Transport solvers canthen be used on these mini-batches to either create a global transport plan or minimize an OT loss.
Figure 2: Visualization of how a batch of various sizes (2, 5, 8) may be extracted with our PyramidMini-Batching (PMB) approach. The source distribution is mapped on the lefthand side of the figure,and the target distribution is mapped on the righthand side of the figure. The center panel illustratesthe order in which nodes are visited when building a batch according to this structure, up until 8 datapoints are selected. Points selected at the same step of tree traversal share the same color and nodeswhich did not contribute any pairs are represented as black. We first select all available pairings froma leaf node, then all of its sibling nodes, and subsequently the parent node. After exhausting theparent node, we then transition to a leaf node of one its siblings and repeat the procedure until weâ€™veconstructed a batch of the desired size.
Figure 3: We display the resulting couplings and Optimal Transport maps from 10 full runs ofInt-PMB-OT and Rand-OT for batch sizes 2, 8, and 10 over a 2D dataset with 10 points. For smallerbatch sizes Int-PMB-OT does a much better job at approximating Wasserstein distance and learningOT maps that adhere to the underlying cluster structure. Both the accuracy of the estimate and sparsityof the learned maps indicate that the smoothing caused by mini-batches is greatly mitigated by PMBapproaches.
Figure 4: Pyramid Mini-Batching approaches allow us to substantially (> %70) improve the align-ments found via gradient flows. As we align distributions by intergrating over the gradient flows,we evaluate how well the aligned distribution has minimized Wasserstein distance at different steps,t = 1, 5, 50. We see that across all batch sizes and all stages of optimization our approach sig-nificantly reduces the residual Wasserstein distance and better aligns the two distributions. Theseexperiments are carried out over our synthetic dataset where each distribution has 1000 cluster centers,100-dimensional features, and 10k data points.
Figure 5: This figure illustrates the marginal impact of the added computational time of constructingmini-batches to the quality of the learned alignment. Using the same experimental setting as Figure4, we show that the dramatic improvement in alignment quality caused by our approach add littleto the overall run-time. We measure the residual Wasserstein distance after t = 1, 5, 50 steps on asynthetic dataset where each distribution has 1000 cluster centers, 100-dimensional features, and 10kdata points.
