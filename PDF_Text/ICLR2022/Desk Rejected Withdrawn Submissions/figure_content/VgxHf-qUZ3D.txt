Figure 1: Illustration of preference conflict in PFL (left) and correlation of preferences (right). Ijdenotes the number of iterations in which rj is selected.
Figure 2: Comparison of the Pareto fronts on differ-ent hyper-parameters. For both methods, the Paretofront and hence the HV are sensitive to the hyper-parameters.
Figure 3: Comparison of the Pareto fronts on Image classification (top) and Fairness (bottom) tasks.
Figure 4: Trends in α and λ during training forthe two Image Classification datasets.
Figure 5: Various generated Pareto fronts by con-ditioning the network inputs α and λ. We see thatby SEO, the network is able to generate variousPareto fronts dependent on the hyper-parameters.
Figure 6: Qualitative results of ablation study. The first row illustrates the effect of sparse sampling,while the second row shows that of SEO.
Figure 7: The Pareto fronts on CelebA--COSMOS-κ- SEPNet0.45 0.50 0.55 0.60 0.65 0.70Loss Oval FaceTable 3: Hypervolume on CelebA Easy andHard tasksMethod	CelebA Easy	CelebA HardSingle Task	3.719	2.222COSMOS	3.706	2.221SEPNet	3.713	2.235Figure 4 shows the trend of α and λ while training for Image Classification. Note that the differentoptimal values are found in each dataset, which implies that to find the optimal values for othermethods, these should be tuned as hyper-parameters for each dataset by grid-search. After training,the two parameters can be conditioned to different values for inference to generate multiple Paretofronts as shown by Figure 5. Hence by following the protocol explained in Section 4.1, we can findthe optimal parameters in the validation set through self-evolutionary optimization.
Figure A1: LeNet (Method & Multi-MNIST& Multi-Fashion & Multi-F+MNIST)	Figure A2: MLP (Adult & Compass & De-fault)Input X	--------------------------------------------- Input xConvolution	--------------------------------------------- Fully ConnectedConditioning Module	--------------------------------------------- Conditioning ModuleConvolution	--------------------------------------------- Fully ConnectedConditioning Module	--------------------------------------------- Conditioning ModuleShared Fully Connected	--------------------------------------------- Head Fully ConnectedHead Fully Connected	----------------------Table A1: EfficientNet-B4 (CelebA)Input xStem ConvolutionMBConvBlocks (0-10)
Figure A3: SEPNet’s convergence graph of the Pareto fronts according to model updates SEP-Net approximates well-spread Pareto fronts from the beginning of training and converges very fast.
Figure A4: Trends in α and λ during training for the three Image Classfication datasets Eachparameter is optimized with different optimal points for different datasets.
Figure A5: Illustration of correlation of preferences. First row uses cosine similarity while the sec-ond row uses JS divergence.
