Figure 1: Illustration of our proposal and previous works. A robust meta-learning should eschewboth meta overfitting, where the meta-model overfits to meta-training tasks due to the limited numberof tasks, and adaption overfitting, where the adapted model overfits to few-shot samples from meta-testing due to the few-shot number. While previous works only consider the meta-generalizationwith meta-regularization to alleviate the meta overfitting and ignore the potential adaption overfit-ting. Our Minimax-Meta Regularization enables the meta-learning algorithm to be generalized atboth meta-level and adaptation-level, which can significantly enhance the generality.
Figure 2: Training accuracy curve. Since 40%of training samples are with the wrong label, themodel maintains train-acc around 60% would beconsidered resistant to overfitting on the training.
Figure 3: Test accuracy curve. Since the testdataset is clean, models that can maintain hightest-acc show better learning robustness (less af-fected by training noise).
