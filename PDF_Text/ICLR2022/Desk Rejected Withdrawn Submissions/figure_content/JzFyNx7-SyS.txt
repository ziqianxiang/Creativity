Figure 1: Illustration of OrphicX. We instantiate our explainer with a variational graph auto-encoder(VGAE), which consists of an inference network and a generative network. The causal features alongwith the spurious features can be used to reconstruct the graph structure within the data distribution,while the causal features are mapped to a graph-structured mask for the causal explanation. Thetarget GNN is pre-trained and the parameters would not be changed during the training of OrphicX.
Figure 2: Illustration of the causal graph. Thecausal features are a set of factors in the latentspace. The causal features and the spurious fea-tures together form the representation of the inputgraph. The graph structure is reconstructed basedon the latent representation; it forms the input ofthe target GNN, along with the feature matrix. ydenotes the predicted label of the GNN target.
Figure 3:	Explanation Performance with Log-Odds Difference. OrphicX consistently achieves thebest performance overall (denser distribution around 0 is better).
Figure 4:	Explanation Visualization (MUTAG): p is the corresponding probability of being classifiedas Mutagenic class by the pre-trained GNN. The graphs in the first column are the target instancesto be explained. The solid edges in other columns are identified as ‘important’ by correspondingmethods. The closer the probability to that of the target instance, the better the explanation is.
Figure 5: Model architectures.
Figure 6: Explanation Performance with Log-Odds Difference. OrphicX consistently achieves thebest performance overall (denser distribution around 0 is better).
Figure 7: Explanation comparisons on BA-shapes. The “house” in green is the ground-truth motifthat determines the node labels. The red node is the target node to be explained (better seen in color).
Figure 8:	Explanation Visualization (MUTAG): p is the corresponding probability of being classifiedas Mutagenic class by the pre-trained GNN. The graphs in the first column are the target instancesto be explained. The solid edges in other columns are identified as ‘important’ by correspondingmethods. The closer the probability to that of the target instance, the better the explanation is.
Figure 9:	Information Flow Measurements. Figure 9a reports the information flow measurementsin the hidden space, where i denotes the ith dimension. Figure 9b reports the ones while the causalinfluence term was removed from the loss function.
