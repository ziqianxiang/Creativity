Figure 1: Illustration of our approach. At each forward step, (i) we measure the mean and the runningvariance of each individual output neuron over batches and time steps. After τ optimization steps,(ii) we select the neurons whose variance was the lowest, and (iii) prune the weight rows (columnsin W T ) that correspond to the selected lowest variance indices. Then, (iv) we squeeze the weightmatrix and perform the weight multiplication with a dense weight matrix. Finally, (v) we expand theoutput product and place sampled mean values in the corresponding indices of the bias term to beadded to the output.
Figure 2: BERTBASE fine-tuned on four downstream tasks. We compare random pruning and variancepruning without any additional fine-tuning. The plots show validation accuracy on MRPC and QNLI,Spearman’s correlation on STSB, and accuracy (continuous) and F1 (dotted) scores on SQuAD1.1,as a function of the sparsity level. As the sparsity level increases, variance pruning (green lines) leadsto much slower performance decrease compared to pruning random rows.
Figure 3: Sparsity level within each Attention black in a pre-trained BERTLARGE model after applyingVariance Pruning in the pre-training phase and reaching 50% sparsity across all layers. Each blockconsists of six linear projection layers: query, key, value, self attention output, intermediate and finaloutput projections.
