Figure 1: (a) is the network architecture that stacks the feature maps output by different layersusing an aggregation operation represented by OM. (b) compares the conventional operation sum-up(used in a classical feature pyramid method FPN (Lin et al., 2017a)) with the two attention-basedoperations proposed in this paper: MSSA and MSCA. Please note that the input feature maps Fi-1and Pi are included in the illustration of OM in (b), while they are independently shown in (a) for abetter visualization of the structure.
Figure 2: The framework of computing MS-Attention (MS-Att). Two kinds of inputs respectivelycorrespond to the computations of MSSA and MSCA. The attention computation module is thesame as in the Vision Transformer (Dosovitskiy et al., 2021). Specifically, two input features aretransformed into Q, K and V first and then fed into a multi-head attention module followed by anMLP. Notably, we adopt a set of average pooling layers (each with a different stride) to generatedmulti-scale features before computing attention. This helps to reduce the computational costs andcapture multi-scale contexts from each single feature map.
Figure 3: The qualitative comparison between the proposed feature aggregation modules (MSCA andMSSA) and baselines (sum-up and concatenation). All models are based on Cascade Mask-RCNNuniformly trained on the MSCOCO train set and evaluated on the val set. We use ResNet-101 as thebackbone. We set the confidence threshold for visualization as 0.6 (zoom-in for a better view).
