Figure 1: Illustration of object aware cropping. Top-Left: We show the original image with randomcrops overlaid. Bottom (red panel): Overlap between random crops tend to miss the object of interest.
Figure 2: Our object-aware cropping approach can be easily plugged into self-supervised learningpipelines and achieves excellent results for classification on OpenImages (left), COCO object de-tection (middle) and COCO semantic segmentation (right). Using object-aware cropping instead ofscene-level cropping provides a consistent boost on BYOL (Richemond et al., 2020) and MoCo-v2(Chen et al., 2020b), two of the top SSL methods. In the case of COCO object detection and semanticsegmentation, this boost allows us to beat pre-training on supervised ImageNet (denoted “Sup. IN").
Figure 3: Analysis of OPenImages data distribution. Left: Performance of supervised and MoCo-V2pre-training as a function of the scale of the objects; we plot the log of average of pixels against thesum of AP for each class. We see no discernible pattern of performance of MoCo-v2 or supervisedlearning as a function of object scale. Right: Performance of supervised learning and MoCo-v2 as afunction of the number of instances in a class; we plot the log number of instances in a class againstthe sum of AP for that class. We do not see any discernible pattern of performance difference as afunction of class size.
Figure 4: Left: distribution of number (log) of images in each class for each of 208 selected classes.
Figure 5:HeightScatter plot of the height and width of the OpenImages Dataset..
Figure 6: Results on COCO then transferred to VOC. Left: Varying the temperature parameter.
Figure 7: Left: We vary the amount of overlap between object and scene crops, as shown. OptimalVOC detection performance is achieved around 58% overlap, decreasing on either side. The purpledot indicates overlap for scene-scene crops (66%). Right: We vary the radius of the object-objectcrops and observe a similar phenomenon of a “sweet spot" distance between the object crops at whichdownstream performance is maximized.
Figure 8: Visualization of BING and random crops.
Figure 9: Visualization of BING and random crops.
Figure 10: Visualization of BING and random crops.
Figure 11: Visualization of BING and random crops.
Figure 12: Visualization of BING and random crops.
Figure 13: First column: Two samples from OpenImages dataset. Second and Third Columns:Random scene crops; Fourth and fifth columns: Random object crops generated by the BING (Chenget al., 2014) algorithm. We see that the object crop tends to center on objects in the image that areoften missed by the scene crops.
