Figure 1: Google speech-to-command dataset used in measurements.
Figure 2: Time, computation, and communication overheads when a different number of participantsand a different number of training passes are used. The lower the better.
Figure 3: Time, computation, and communication overheads versus model complexity. The lowerthe better.
Figure 4: Heuristic explanation of our measurement results. (a) The number of participants. Thebottom scheme is better regarding computation and communication efficiency. (b) The number oftraining passes. The bottom scheme is better regarding time and computation efficiency. (c) Modelcomplexity. The bottom scheme is better for time, computation, and communication efficiency.
Figure 5: Decisions of FedTuning (a) Trajectories for different application preferences. The initialM and E are 20. (b) Trajectories for different initial M and E , where the time, computation, andcommunication preferences are the same (α = β = λ = 0.33).
Figure 6: Illustration of different training aspects.
