Figure 1: Comparing band selectivity and feature map activity across supervised and contrastiverepresentations by visualising input-sonification pairs as log-scaled spectrograms. Note the greateractivity in the higher frequency bins of the contrastive representations for layer 8 and 9.
Figure 2: Select supervised input-sonification pairs. For supervised models, most remarkable soni-fications were found in the middle layers (layers 3-7).
Figure 3: Select contrastive input-sonification pairs. These samples are from layers 8 and 11, andsimilarly remarkable sonifications can be found throughout all the layers.
Figure 4: Layerwise FSD50K (left) and Speech Commands v2 (right) performance for contrastiveand supervised pretraining. mAP and classification accuracy scores on the validation set are re-ported, respectively. Dashed lines represent performance without removing any layers. For Speech-Commands, the red dashed line overlaps the green. 95% confidence intervals shown across severalruns. We conducted this study to rule out suspiciously low feature map activity that we observedwhen evaluating sonifications for the deep layers of the supervised model and ascertain that theselayers were indeed contributing to recognition performance. More information in Bâ€¢	We suspect that max-unpooling might also contribute to this phenomena: it simply replacesmaximum values at the correct indices and leaves other elements zero, possibly inducingdiscontinuities and contributing to noise.
Figure 5: Contrastive feature evolution in the select feature maps from several layers. Note howcertain frequency bins are being increasingly emphasised in the layer 5 feature map, while deadfeature map in layer 10 starts with a full spectrogram, but fades away later.
Figure 6: Effect of removing sonifications on a layer-by-layer basis from the input on FSD50Keval performance. Contrastive model pretrained on AudioSet was finetuned. Dashed lines representbaseline performance. Removing sonifications from the input signal drastically reduces model per-formance, indicating that they represent acoustic elements essential for recognition. 95% confidenceintervals are computed over several runs.
Figure 7: Comparing inputs and sonifications of the best contrastive and supervised models trainedon AudioSet using magnitude squared coherence, with 95% confidence intervals over three runs.
