Figure 1: Left: Comparison of DisTop and Skew-Fit on their ability to reach diverse states. In theVisual Pusher environment, we compare the final distance of the position puck with its desired posi-tion; in the door environment, we compare the angle of the door with the desired angle. DisTop(min)is the minimal distance reached through evaluation episode. At each evaluation iteration, the dis-tances are averaged over fifty goals. Right: Average rewards gathered throughout episodes of 300steps while training on Halfcheetah-v2 and Ant-v2 environments.
Figure 2: Left: Average rewards gathered throughout training episodes. Right: Top view of AntMaze environment with its goal position and an example of OEGN network learnt by DisTop.
Figure 3: Same environments and evaluation protocols than in figure 1. The agent aims at mini-mizing the evaluated distance and increasing the average reward. a- comparison of performance inVisual Pusher between using no constraints in equation 2 and several values of kc ; b- comparison inVisual Pusher of DisTop with OEGN and using a fix number of nodes as in a SOM; c- performanceof DisTop in Visual Pusher for several skew sampling coefficients 1 +α0 ; d- evaluation of DisTop inAnt for several values of ratio.
Figure 4: Visualization of the representations learnt by a VAE and DisTop on the environment dis-played at the far left. From left to right, we respectively see a- the rendering of the maze; b- thecontinuous representation learnt by DisTop with 900-dimensional binary inputs; c- a VAE represen-tation with true (x,y) coordinates; d- a VAE representation with 900-dimensional binary inputs; e-OEGN network learnt from binary inputs.
Figure 5: Different Topologies learnt on the gridworld displayed in figure 4. Each line correspondsto a hyper-parameter and we display the corresponding value on each square. All hyper-parametersare equals to k = 3, kc = 10, δ = 0.1, deltanew = 0.6 except for the changing one in each line, thesecond line for which we change the temperature k = 0.5 and the third line for which we changethe dilatation coefficient kc = 100.
Figure 6: Progressively learnt topology throughout training. We selected six intermediary OEGNnetwork with an equal intermediate number of timesteps. The topology is extracted from one of ourexperiments in section 4curves the topology of the environment and the L2 rewards may admit several local optimas, whichis not desirable. However one can see that a low temperature can also distort the representation,states in rooms (group of inter-connected nodes) tend to be closer with each other than borderingstates, such that an action covers a larger distance in the border than in a room. In fact, in the borders,the agent often hurts the wall and stays in the same position, so, in comparison with large rooms,the bring together is less important than the move away part. We can prevent this by increasingthe dilatation coefficient (first line of figure 5) such that all consecutive states get limited by thedistortion threshold δ, which prevents consecutive embeddings to be equal in equation 2. Overall,relying on δ also becomes interesting when the number of states increases due to the exploration ofthe agent: the agent progressively compresses its representation since interesting negative samplesare less frequent. In the third line of figure 5, we see that we can control the the minimal alloweddistance by playing with δ.
Figure 8: Distance between the final puck position and the wanted puck position in Visual Pusher.
Figure 9: Illustration of a learning step of our growing network and contrastive loss (cf. text).
Figure 10: Illustration of how an agent samples interactions from a buffer.
Figure 11: Examples of 8 skills learnt in Visual Door.
Figure 12: Examples of 8 skills learnt in Visual Pusher.
Figure 13: Examples of 8 skills learnt in Ant Maze.
