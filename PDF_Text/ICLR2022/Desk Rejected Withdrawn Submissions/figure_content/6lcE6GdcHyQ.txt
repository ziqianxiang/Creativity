Figure 1: Left: Audio-visually trained brains can adapt to single-modal tasks while neural networksfailed. Right: It’s not hard for us to distinguish ”Ah” from ”Oh” with pure audio or visual signals,but for artificial neural networks, missing modality(ies) may result in the corruption of the wholesystem.
Figure 2: Structure of our multi-modalcomputational baseline model using twostreaming primary processing combinedwith high-level joint processing.
Figure 3: Left: Samples conducted with Modality Mix where part of the training data have randommodality being zeroed out, and all modal’s data keep the same size. Right: Gated fusion Operationwhere each feature in the two modalities are added with computed gating weight from the averagedvector.
Figure 4: Analysis of negative samples for baseline model in modal missing scenarios. Every posi-tion in the x-axis refers to a class, with red bars indicates the number of samples that are mistakenlyclassified to other classes and green bars means samples from other classes are mistakenly classifiedto this class.
Figure 5: The left and right groups of modules are the two blocks used in the training of single-modal features and our CNN baseline. The LRW dataset has 29 frames for each sample. If moreframes are needed such as samples in the OUlUVS _pad dataset, the output of the first block will beaveraged in the last dimension.
Figure 6: Training curves of the different models, all of which have clear descendent of loss in thetraining.
