Figure 1: (a) An example task that startswith an initial policy shown in blue, andaims to imitate the expert trajectories shownin red. (b)-(d) show the reward landscapesof the ideal reward function, the learned dis-criminator in GAIL, and our method (SS-MAIL), respectively.
Figure 2: Vector field in-duced by negative α.
Figure 3: We plot the expected length of thegenerated trajectory during training in be-tween pedagogical intervention for teacherforcing, scheduled sampling and trajectoryforcing. We show that our curriculum pro-vides a gradual increase in expected trajec-tory lengths for better policy training.
Figure 4: Training error over time inthe Y-Junction environment shows that SS-AIL successfully imitates the expert. Thelow standard deviation of error for SS-AILdespite unfavorable initialization demon-strates the increased robustness of its train-ing dynamics.
Figure 5: A visualization of the multi-modaltrajectories learned by SS-AIL and DNRIon the Y-Junction task. We observe thatDNRI averages over the different modes,while SS-AIL successfully differentiates be-tween them. The inability to distinguish be-tween multi-modal expert trajectories canprove disastrous in continuous state set-tings, as this may lead to visiting states thatdiffer considerably from the expert state-distribution.
Figure 6: Running an ablation, over β, forthe mean and standard deviation of test-ing error in the Y-Junction environment il-lustrates the existence of β values that re-sult in both low mean and standard devi-ation. This highlights Trajectory Forcing’sability to improve the robustness to unfavor-able weight initializations, increase the sam-ple efficiency, as well as eliminate the issueof compounding errors.
Figure 7: We plot the Compounding Errorsover time for the Noisy Mocap environmentshows that SS-AIL successfully compensatesfor noisy inputs during test time, zero-shot.
Figure 8: We plot the training loss overepochs for the Noisy Mocap environmentand show that sampling negative α valuesimproves the final loss and speeds up train-ing. This can be attributed to the richer re-ward gradient in the local neighborhood ofthe current trajectory.
Figure 9: Sample snapshot of the Y-Junction ExperimentB.2	Y-Junction Experimental SetupWe expand upon the simulated synthetic experiments of (Graber & Schwing, 2020), and use thesame numberB.3	NOisy Mocap SetupFor noisy mocap, we add gaussian noise of 0.05 standar deviation.
