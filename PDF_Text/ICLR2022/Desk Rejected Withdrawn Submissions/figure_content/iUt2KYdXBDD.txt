Figure 1: Exemplary mazelayout generation; potentialstarts and goals shown inblue; reaching the goal tile re-sults in a sparse reward of oneacross the maze, which are shown in blue in Fig. 1. The time horizon for navigating the maze is setto T = 100.
Figure 2: VRN architecture and input representation example (agent navigation): The first inputchannel is a kc × kc (with kc = 7) crop Vpz of the value “prior” Vp , centered on the current abstract(high-level) state z of the agent. Given a discretized (x, y) map Φ of the environment, indicating(static) obstacles, a kc × kc crop Φz, centered on z, forms the second input channel. Additional inputchannels are formed by selecting components si of the full (continuous) agent state s and insertingthe value into every cell of the corresponding input channel (exemplarily implied for s3 and s4).
Figure 3: Plots for maze navigation tasks: mean (solid line) ± standard error (shaded area)(c) Stick Robot Examplethe success factors of our VRN: The importance of the “prior” value information is investigated bytraining “VRN NO PRIOR” without this additional information. Furthermore, by training a “DQN(PRIOR)” baseline that globally receives maze and optionally also value “prior” information, wewant to study the influence of the specific VRN architecture.
Figure 4: Exemplary visualization of the vehicle parking tasksCompared to the results presented for the NEXT planner in Chen et al. (2019), of 84 % and 94 %for their variants, respectively, VRN-RL performs a bit worse. Still, VRN-RL achieves up to 81 %success rate for the best seed in the full test set evaluation 3 . When looking at these numbers, itshould be taken into account that our VRN-RL can be used without access to the simulation (apartfrom the static map), which NEXT needs to globally sample states or re-wire the tree (as part ofthe underlying RRT). Furthermore, our VRN-RL is able to learn the navigation from environmentinteraction without NEXT’s safety net of a fallback RRT planner that guarantees to eventually solvethe task and acquire successful training episodes.
Figure 5: Plots for vehicle parking tasks: mean (solid line) ± standard error (shaded area)Parking with Moving Vehicle: In order to showcase the ability of our VRN to consider lateststate information (sensor measurements) during refinement to handle dynamically changing envi-ronments, we modify the parking task. As shown in Fig. 4b, we add a second vehicle, which consid-erably increases the difficulty of successful parking. The other vehicle drives horizontally in frontof the parking slot row from which the goal is selected. The constant velocity is randomly sampledfrom the interval [1.5, 3.5] m/s. Upon a collision of the two vehicles, the velocity of the controlled“ego” vehicle is set to zero until the collision is resolved.
Figure 6: Empty Room Discrete Maze NavigationA AppendixA. 1 Additional Environment DetailsA.1.1 Discrete Maze NavigationWide Corridor Maze Generation: We first generate 9 × 9 random mazes. Then, we expand everyother row and column (the corridors) so that one tile is replaced by five tiles of the same value,resulting in the wide corridors.
