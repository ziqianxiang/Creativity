Figure 1: Online blurry continual learning with noisy labels setup and overview of proposedChameleon Sampling (ChamS). The task share classes similar to blurry setup (Prabhu et al., 2020),examples might be falsely labeled, and the class distribution changes over tasks. The proposedmethod of updating memory balances between diversity and purity, so it avoids both the disturbancefrom noisy labels by low purity and overfitting by training with similar data. We further employ anovel robust learning method with a memory so that the model cannot be interfered by noisy labelsin the memory, and the robust and well-trained model helps select examples at the next task.
Figure 2: Last test accuracy and memory purity changes as the task number increases on CIFAR-10/100 with SYM-40%. The figures with various noise ratio are described in Appendix C.
Figure 3:	Accuracy by different α on CIFAR-10 with SYM-{20%,40%,60%}; (a) Compari-son of the last test accuracy w. and w.o. theadaptive strategy, (b) Comparison of the meanof α for each task with the adaptive strategy.
Figure 4:	Distribution of examples in theepisodic memory training on CIFAR-10 withSYM-20%. GMMs effectively distinguish be-tween (a) clean and noisy labels by loss, and (b)correct and incorrect predictions by uncertainty.
Figure 5: Illustration of last accuracy and memory purity changes as the task number increases onCIFAR-10 with SYM-{20%, 60%}.
Figure 6: Illustration of last accuracy and memory purity changes as the task number increases onCIFAR-100 with SYM-{20%, 60%}.
Figure 7: Illustration of diversity and purity in the memory as the coefficient α changes on CIFAR-10 with (a) SYM-20%, (b) SYM-40%, and (c) SYM-60%.
Figure 8: Illustration of diversity and purity in the memory as the coefficient α changes on CIFAR-100 with (a) SYM-20%, (b) SYM-40%, and (c) SYM-60%.
