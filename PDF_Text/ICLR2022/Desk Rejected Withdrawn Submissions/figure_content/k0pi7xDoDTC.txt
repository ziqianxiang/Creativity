Figure 1: Main idea and technical logic. The left shows the motivation and ideal goal in Sec. 1. Whilethe right part refers to our approach mainly in Sec. 4.
Figure 2: Mapping from source totarget: (a) hard mode drop (left) andsoft mode deficiency (right). Modedropping can be treated as a spe-cial case of the soft mode deficiencywhen the green part is sparse to thelimit. (b) Assume the existence ofthe inverse of mapping G(∙). Givenany set S, the probability measuresof the set G-1(S) and S are equal.
Figure 3: IID-GAN: generator G maps random samples from original standard M-D Gaussian totarget ones and F inverts the target sample back to a source sample which obeys M-D Gaussian.
Figure 4: Example for 2-D source (z) to 2-D target (x) generation and inverse with 8 modes Ringdataset as the real training set. Left 4 columns are the results of VAE, BiGAN, VEEGAN, whileright 4 columns are the results of IID-GAN under different Gaussian consistency losses as detailedin Sec. 4.3 after training 24K batches. For each half part, column 1 and column 5 show the inverseof the real target data, column 2 and column 6 show the sampled z from Gaussian in source space.
Figure 6: QQ-Plot for IID test for standard Gaussian. The closer to diagonal, the closer to Gaussian.
Figure 5: Generations of grid data given poorinitialization. Compared with 1-D Gaussian con-sistency loss, the M-D loss outperforms afterenough batch iterations (i.e. training steps). Sim-ilar results are shown on Ring in Appendix F.
Figure 7: Generating quality and KL divergence(for diversity) from the inverse source to the stan-dard Gaussian on MNIST.
Figure 8: Conditional results on CIFAR-10 for CGAN, MSGAN and conditional HD-GAN. The----IlD-GAN(I-D)----IID-GAN(M-D)c≡∕rf*⅛ ■二一一n跑射囱福由团修《CH/ 昭 r<⅛∕ & Et*。■留■篇略史的・三更斜眼曲惠方喈F白迂加・徐昭丁以FM堡e也热器9悭百口尸13 1而■曳色明generation results of CGAN and MSGAN can easily deteriorate as training proceeds, while IID-GANmaintains good performance consistently.
Figure 10: Comparison among different methods for Ring and Grid.
Figure 9: Comparison of Gaussian consistency loss on Ring data.
Figure 11: Uniformly Sampling in different Co-ordinate SyStemS for MNIST With IID-GAN andVAE model.
Figure 12: UnSuperviSed diSenChantment Withuniform Sampling from the polar Coordinate SyS-tem by IID-GAN. By varying polar angle (y-axiS), We Can get unSuperviSed diSentanglementreSultS With a large polar radiuS (x-axiS). Seemore detailS in appendix.
Figure 13: Results of conditional IID-GAN for MNIST dataset with latent dimension equal to 2.
Figure 15:	Results of conditional IID-GAN for CIFAR-10 dataset with random sampling on labelsand z.
Figure 16:	The generated distribution of images in the two-dimensional z-plane, with both dimensionsof z taking values from -2 to 2. The configuration is the same as Figure 13. It can be seen thatIID-GAN have a better diversity performance compared to CGAN.
Figure 18: Comparison on latent dimension of 5 on CIFAR10 dataset with one dimension value ofthe input latent code z increasing by gradient. The configuration is the same as Figure 14.
Figure 17: Comparison on latent dimension of 10 on CIFAR10 dataset with one dimension value ofthe input latent code z increasing by gradient. The configuration is the same as Figure 14.
Figure 19: Generation results on StackedMNIST for unconditional IID-GAN.
Figure 20: Generation results on CIFAR-10 for unconditional IID-GAN.
Figure 21: Generation results on CIFAR-100 for unconditional IID-GAN.
