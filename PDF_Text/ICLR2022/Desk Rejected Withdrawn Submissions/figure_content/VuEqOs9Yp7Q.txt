Figure 1: Conventional TAL methods are either (a) anchor-based or (b) anchor-free all needs togenerate action proposals. Instead, (c) our global segmentation mask transformer (GSMT) model isproposal-free.
Figure 2:	Overview of our proposal-free Global Segmentation Mask Transformer (GSMT)learning architecture. Given an untrimmed video, GSMT first extracts a sequence of T snippetfeatures with a pre-trained video encoder (e.g., I3D (Carreira & Zisserman, 2017)), and conductsself-attentive learning to obtain snippet embedding with global context. Subsequently, with eachsnippet embedding, GSMT classifies action class (output P ∈ R(K+1)×T with K the action classnumber) and predicts full-video-long foreground mask (output M ∈ RT ×T) concurrently in atwo-branch design. During training, GSMT is optimized by minimizing the difference betweenclass/mask prediction and ground-truth annotations. In inference, GSMT selects top scoring snippetsfrom the classification output P , and then thresholds the corresponding foreground masks in M toyield action instance candidates. Finally, softNMS is applied to remove redundant candidates.
Figure 3:	An illustration of label assignment (see text for more details).
Figure 4: Inter-snippet cosine similarity in the embedding space for a random ActivityNet val video.
Figure 5: False positive profile of GSMT, BMN and R-C3D on ActivityNet. We use top up-to 10Gpredictions per video, where G is the number of ground truth action instances.
Figure 6: The effect of our bIOU loss an a random ActivitNet val video.
Figure 7: A failure case from THUMOS14Ablation of component design in GSMT Our GSMT primarily consists of a Snippet EmbeddingTransformer and 1-D Convolution heads for classification and localization branch. We ablate thenumber of 1-D CNN layers for both the branch heads in Table 8. As the results suggest, only 1 layeris enough for classification branch. A plausible reason for this is that for classification it needs globalinformation and stacking multiple 1-D CNN may affect the global information. For localizationbranch, it is observed that 3 layers give best performance. This is probably because for predicting themasks the network needs to process local information captured by 1-D CNNs. Additionally, we alsoablate the performance of transformer design in head size. Table 9 demonstrates that the performanceof GSMT improves significantly with the increase of heads in the Transformer. However, excessiveheads will lead to overfitting. The performance peaks at four heads.
Figure 8: Fine-grained performance on video subsets with different temporal lengths on THUMOS.
Figure 9: Translated FPS based on Titan XM.
Figure 10:	Qualitative TAL result comparison on videos from (a) ActivityNet-v1.3 and (b) Thu-mos14. We compare our GSMT (first 3 rows) with G-TAD Xu et al. (2020) (last 3 rows). For eachmethod, we show a number of top action detection candidates, with the confidence score given insideeach detection box. It can be seen that for both cases, our GSMT produces more accurate actioninstance detection with much less candidates compared to G-TAD.
Figure 11:	Qualitative TAL result comparison on single-instance videos from (a) ActivityNet-v1.3and (b) Thumos14.
Figure 12:	Qualitative TAL result comparison on multi-instance videos from (a) ActivityNet-v1.3and (b) Thumos14.
