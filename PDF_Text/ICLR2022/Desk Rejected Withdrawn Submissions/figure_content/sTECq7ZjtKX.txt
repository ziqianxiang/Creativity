Figure 1: Test aCCuraCies of various two layer pruned networks at different sparsity from ran-domly initialized full networks (FC-1000, FC-1000-1000, and FC-1000-1000-1000) on MNIST andCIFAR-10 datasets. EaCh experiment is run for five independent trials and the mean aCCuraCiesare reported. The error bands show the standard deviations over five trials. The aCCuraCies of fullnetworks (when 0 neurons are pruned from the hidden layer) at initialization is 〜10%.
Figure 2: Dashed lines (with marker +) show the test accuracies of two layer networks pruned fromtrained full networks (FC-1000, FC-10000, and FC-100000) at different sparsity. Solid lines (withmarker ×) show the test accuracies of the pruned networks after fine-tuning. The mean accuraciesover three independent trials are reported. Error bands show the standard deviations of accuraciesover three trials.
Figure 3: (FC-1000-1000-1000 + CIFAR-10). 3(a) shows the accuracies of sparse networks obtainedby pruning the randomly initialized full network at various sparsity. The sparse networks are thentrained on CIFAR-10 and compared in 3(b). Mean and standard deviation of accuracies over threeindependent trials are reported.
Figure 4: (FC-1000-1000-1000 + CIFAR-10). 4(a) shows the accuracies of sparse networks obtainedby pruning the trained full network at various sparsity. The sparse networks are then fine-tuned andcompared in 4(b). Mean and standard deviation of accuracies over three independent trials arereported.
Figure 5: Ablation studies. In Figures 5(a)-5(c) We compare the performance of FC-10000 onMNIST dataset with various activation functions, network parameter initializations, and uniformweight distributions, respectively. In Figure 5(d) we compare 730 network architectures searchedwith our OSSuM algorithms over FC-1000-1000-1000 with MNIST dataset.
Figure 6: In Figure 6(a), the learning curve of a pruned subnetwork from FC-1000-1000-1000 (128×compressed network) on MNIST is shown. In Figure 6(b), the learning curve of a pruned subnetworkfrom FC-1000-1000-1000 (64× compressed network) on CIFAR-10 is shown.
Figure 7: FC-1000-1000-1000 full networ. Pruning the full network and training the subnetworks atvarious sparsity over MNIST and CIFAR-10 data. Mean and standard deviation of accuracies overthree independent trials are reported.
Figure 8: FC-10000 + MNIST. Figure 8(a) shows the maximum, minimum, and the ratio of thenumber of positive to negative weight parameters in the hidden layer of pruned networks at varioussparsity obtained from OSSuM. Figure 8(b) shows the mean and variance of the weight parametersin the hidden layer of pruned networks at various sparsity obtained from OSSuM.
Figure 9: Full network FC-1000-1000-1000 + CIFAR-10. Figure 9(a) shows the performance of allthe OSSuM variants when the randomly initialized full network is pruned at various sparsity. Figure9(b) compares the performance of pruned subnetworks post-training.
