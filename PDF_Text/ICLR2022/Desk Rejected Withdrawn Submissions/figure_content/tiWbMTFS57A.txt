Figure 1: The first plot reports the mean parameter conditioning for ReLU FCNs at initialization asexamples flow through the various layers; dotted lines represent renormalized networks. The valuesare reported as the natural logarithm of the conditioning of the Hx matrix defined in Section 5; linecolors represent different examples-width-ratios of the tested networks. The second plot reports meanconditioning for 500 examples at varying widths; shaded regions denote 95% confidence intervals.
Figure 2: The first plot reports the ratio between the predicted and the measured loss decrease15atsingle epochs for 6-layers ReLU networks among different learning rates. The second plot reports inblue the progression of the lowest eigenvalue in the gram matrix Hx in the last layer, and in orangethe lower bounds obtained by plugging the measured distance from initialization into Lemma 9.
Figure 3: The first plot shows the lowest eigenvalue of the input conditioning matrix for the variouslayers. The right plot shows the generalization bounds obtained using the estimates in Lemma 4.
Figure 4:	Conditioning plot with respect to the conditioning factor for Tanh activation networks.
Figure 5:	The first plot shows the maximum rowsum of the off diagonal entries of the matrix FxFxT ;the second plot shows the minimum value of the on-diagonal entry and the maximum value of theoff-diagonal entries.
Figure 6: Plot of the normalized highest eigenvalue (top) and lowest eigenvalue (bottom) for thetested widths and for the theoretical prediction for a FCN with ReLU activation.
Figure 7: Plots of conditioning progression for ReLU (top) and Tanh (bottom) networks.
Figure 8: Plots of loss prediction ratio for ReLU and Tanh networks trained with the cross-entropyloss. The plots show the ratio between real loss and predicted loss at each epoch.
Figure 9: Cumulated loss prediction ratio for networks trained under Mean Squared Error, obtainedas the cumulative product of the single epoch ratios. The first plot is for ReLU networks; the secondplot details Tanh networks.
Figure 10: Cumulated loss prediction ratio for networks trained under Cross-Entropy, obtained as thecumulative product of the single epoch ratios. The first plot is for ReLU networks; the second plotdetails Tanh networks.
