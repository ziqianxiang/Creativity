Figure 1: Search space via the one-shot modelinternal structures of each block.
Figure 2: (Left) The IQNAS scheme constructs a quadratic accuracy estimator by measuringthe accuracy contribution of individual design choices and then maximizing this objectiveunder quadratic latency constraints. (Right) Subnetworks’ accuracy predictions by theproposed estimator vs their measured accuracy. High ranking correlations are achieved.
Figure 3:	Performance of predictors vssamPles. Ours is comParable to comPlexalternatives and more samPle efficient.
Figure 4:	ComParing oPtimizers for solvingthe IQCQP over 5 random seeds. All surPassoPtimizing the suPernetwork directly.
Figure 5: Imagenet Top-1 accuracy vs latency.
Figure 6: Design choices insights deduced from the accuracy estimator: The contribution of(Left) depth for different stages, (Middle) expansion ration, kernel size and S&E for differentstages and (Right) for different blocks within a stage.
Figure 7: An Overview scheme of the IQNAS method with computational costsThe search space, latency measurements and formula, supernetwork training and fine-tuningblocks (1,2,3,5,6,7,10,11,12) are identical to those introduced in HardCoRe-NAS:We first train for 250 epochs a one-shot model wh using the heaviest possible configuration,i.e., a depth of 4 for all stages, with er = 6, k = 5 × 5, se = on for all the blocks. Next, toobtain W*, for additional 100 epochs of fine-tuning Wh over 80% of a 80-20 random split ofthe ImageNet train set Deng et al. (2009). The training settings are specified in appendix C.
Figure 8: Kendall-Tau correlation coefficients and MSE of different predictors vs number ofprincipal componentsF.1 Convergence Guarantees for a General BCFW over a Product DomainThe proof is heavily based on the convergence guarantees provided by Lacoste-Julien et al.
