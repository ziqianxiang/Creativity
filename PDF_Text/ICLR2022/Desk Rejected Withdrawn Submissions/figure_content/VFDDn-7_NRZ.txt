Figure 1: Atomic Recursive Operation.
Figure 2: Approximating global self-attention via multiple sliced group-attentions with permutation.
Figure 3: Recursive designs.
Figure 4: A comprehensive ablation study on different design factors.
Figure 5: Comparison of BLEU, training loss and val loss on WMT14 En-De and IWSLT14 De-En(in Appendix Fig. 13) datasets. The red dashed box indicates that LRC makes training more stable.
Figure 6: Illustration of activation distributions on shallow, middle and deep layers of DeiT-Tiny andour SReT-T networks. The input image is in the upper left corner of the first subfigure. Under eachsubfigure, 14 X 14, 28 X 28, 7 X 7 are the resolutions of feature maps. “R1&2” indicates the locationof a number of recursive operations in each block. Interestingly, we observe that properties in the twomodels are fairly different. For DeiT, the information is from poor to rich along with the depth, whilein our model, it is with hierarchies. Shallow layers focus on details and deep layers contain moresemantics. Recursive operation also promotes the hierarchies. Zoom in for better view.
Figure 7: (1) ImageNet-1k results on An-MLP Architecture. (2) Evolution of coefficients.
Figure 8: Illustration of recursive transformer with different designs. “NLL” indicates the non-linearprojection layer within each recursive loop.
Figure 9: The actual optimization landscape from DeiT-108, our SReT-10 8 and SReT-10 8mixed-depth models.
Figure 10: A comprehensive ablation study on different design factors.
Figure 11: Differentpermu. designs.
Figure 12: Our modifications by removingclass token and distillation token.
Figure 13: Comparison of BLEU, training loss and val loss on WMT14 En-De (top) and IWSLT14De-En datasets (bottom). The red dashed box indicates that LRC makes training more stable.
Figure 14: Ablation study on different LRC designs.
Figure 15: Evolution of coefficients at different recursive blocks and layers.
Figure 16: Evolution of coefficients on language OfWMT14 En-De dataset.
