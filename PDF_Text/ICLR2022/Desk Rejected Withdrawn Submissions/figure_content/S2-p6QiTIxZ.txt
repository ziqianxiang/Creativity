Figure 1: Examples of LPDR and empirical LPDR. (a) LPDR ofx for given h in binary classificationwith the linear classifier. Here x is uniformly distributed in R1 2 * * . The h2 and h3 disagree with h inprediction on x, and ∣θ2|/n is the infimum for {ρ(h, h) : h ∈ H(h, x)}. Thus, Lh(x) = ∣θ21∕∏. (b)Empirical LPDR of xi for h on MNIST dataset. The set of x-axis values of the blue dots for xi are{ρS (hc, h) : hc ∈ HC(h, Xi)}, and Lh(Xi) = mm{ρs (hc, h) : hc ∈ HC(h, xi)} (blue arrow).
Figure 2: The need for regulating σ (MNIST). (a) The disagree ratios of the samples goes to 0 as σ1	1	, C C	♦	∕ι ∖ El τπ Γ /7八1。	7 一 c/ 1	,ι	ιdecreases and goes to 0.9 as σ increases. (b) The E[ρS (h, h)] for h ∈ HB decreases as the numberof labeled samples increases when HB is constructed with a static σ for all acquisition steps.
Figure 3:	The performance comparison of DRAL+ with the baseline active learning algorithms onMNIST (a), CIFAR10 (b) and SVHN (c) datasets. Overall, DRAL+ consistently either performsbest or comparable with all other algorithms regardless of dataset.
Figure 4:	The performance comparison of DRAL+ with the baseline active learning algorithms onon CIFAR100 (a) and Tiny ImageNet (b) datasets with WRN-16-8 networks. These datasets aremore difficult. DRAL+ outperforms all other algorithms on more difficult tasks.
Figure 5: The performance comparison on imbalanced HAM10000 dataset with WRN-16-8 in termsof the test accuracy (a) and AUC (b). DRAL+ performs better than or comparable with all otheralgorithms.
Figure 6: The verification of Conjecture 1. (a)-(b) Theoretical verification in binary classificationwith the linear classifier h(x) = sgn(xTw) on uniformly distributed x ∈ X = R2. Let h bea sampled hypothesis With W 〜 N(W,Iσ), then ρ(h,h) = ∣θ - θ∣∕π where -π + θ ≤ θ ≤π + θ. Here, ρ(h, h) is continuous and strictly increasing with σ, thus E[ρ(h, h)] is continuousand strictly increasing with σ . (c) Empirical verification for various datasets with deep networks.
Figure 7: The verification of Conjecture 2. (a)-(b) Theoretical verification in binary classificationwith the linear classifier h(x) = sgn(xTw) on uniformly distributed x ∈ X = R2 . Let HB bethe set of h sampled with W 〜 N(W,Iσ2). The D(h, x) → P[w ∈ W(W, x)] decreases as dincreases with probability tending to 1 as IHB | → ∞. While, Lh(x) = ∣θx - θ∣∕π increases asd increases. Thus, D(h, x1) > D(h, x2) ⇔ Lh(x1) < Lh(x2). (c) Empirical verification forvarious datasets with deep networks the strong negative rank correlation coefficients of from -0.95to -0.94 between the empirical LPDR and the disagree ratio are observed for log σ ∈ (-6, -2).
Figure 8: The performance comparison between using empirical LPDR and using the disagree ratioon MNIST with S-CNN (a), CIFAR10 with K-CNN (b), and CIFAR100 with WRN-16-8 (c). Thereis no significant difference in the performance between the two methods. Thus, LPDR-based activelearning can be performed by using the disagree ratio.
Figure 9: The E[ρS (hn, h)] and log σ with respect to the labeling proceeds for all experimentalsettings. The proposed algorithm reliably guides the E[ρS (hn, h)] tobe the target value by increasingthe variance of sampling as the number of labeled samples increases.
Figure 10: The final accuracy With respect to the ρ* on MNIST (a), CIFAR10 (b), and CIFAR100(C) datasets. The proposed algorithm shows high performance when ρ* = q/m.
Figure 11: The empirical errors of the learned and the sampled hypotheses with respect to thenumber of labeled samples for MNIST (a), CIFAR10 (b), SVHN (c), CIFAR100 (d), Tiny ImageNet(e), and HAM10000 (f) datasets. It is observed that the empirical errors of the sampled hypotheseshave various values.
Figure 12: The performance comparison between DRAL+ and DRAL on MNIST (a), CIFAR10(b), SVHN (c), CIFAR100 (d), Tiny ImageNet (e), and HAM10000 (f) datasets. Overall, DRAL+consistently either performs better than or comparable with DRAL regardless of the experimentalsettings.
Figure 13:	The performance comparison with respect to the hyperparameters of DRAL+ on MNIST,CIFAR10, and CIFAR100 datasets. (a) - (C) β. (d)-⑴ N. (g) - (i) sampling layer. DRAL+ isrobust against β, N, and sampling layer.
Figure 14: The test accuracy with respect to the number of labeled samples from initial to final stepfor all experimental settings.
Figure 15:	The performance comparison with respect to the number of initial labeled samples onMNIST (a), CIFAR10 (b), and CIFAR100 (c) datasets. The proposed algorithm is robust against tothe number of initial labeled samples and performs well even when the initial size is much smaller.
Figure 16: The framework of the proposed algorithm.
