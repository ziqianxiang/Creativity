Figure 1: High-level summary showing how MIKE is trained and how the resulting encoder canbe used to provide representations for downstream components/tasks. Note that the parameters ofthe source/teacher network are frozen during training of the autoencoder, and that the two teacherinstances shown are in-fact the same network being evaluated on different inputs.
Figure 2: Various exemplar images from the 11 segmentation datasets that we derived from COCO.
Figure 3: Radar charts comparing the downstream performance across the 11 coco segmentationtasks. Each of the 11 radar charts is for a single encoder that absorbed the knowledge from onlyone of the segmenter source networks, with the left-to-right/top-to-bottom order corresponding tothe clockwise ordering of the upper left radar chart. These encoders were all trained using the all-layers uniform “knowledge-transfer algorithm”. They all outperform their corresponding baselineautoencoder on every downstream task, indicating that generally useful segmentation features werelearned. The average task performance (measured in IOU) for the baseline and MIKE encoders areshown under each radar chart.
Figure 4: Radar chart comparing the downstream performances of three autoencoders trained ondifferent numbers of source networks (i.e. tasks). The single task encoder was trained against thecar-truck-bus source network. The three task encoder was trained against the three tasks whoseexemplar images have a green border. The final encoder was trained against all 11 source networks.
Figure 5: Layerwise training loss during knowledge transfer from the bench-trafficlight-stopsignsource network. Loss weights were equal for all source network layers throughout training.
Figure 6: Layerwise training loss during knowledge transfer from the bench-trafficlight-stopsignsource network. Loss weight was placed on only one layer at a time, moving sequentially throughthe network at regular intervals.
