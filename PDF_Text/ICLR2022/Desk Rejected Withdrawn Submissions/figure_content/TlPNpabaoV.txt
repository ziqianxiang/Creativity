Figure 1:	Effects of output representation and weight regularization on the distribution of weightsusing (left) LLR representation and (right) weight decaying on the MNIST dataset.
Figure 2:	Test errors vs. total parameters trade-offs on the MNIST dataset using softmax normal-ization and LLR representation: (left) LeNet-300-100 and (right) LeNet5We found that the AdamW optimizer with weight decaying may increase training accuracy byincreasing overfitting and yield less efficient networks, as demonstrated in Figure 3. Comparedwith previous results, the optimal pruned model sizes are dramatically increased and usingweight decaying does not improve the efficiency of trained networks.
Figure 3:	Test errors vs. total parameters trade-offs on the MNIST dataset using AdamW optimiza-tion with (left) LeNet-300-100 and (right) LeNet5 architectures3.3 CIFAR- 1 0 ClassificationSeveral ResNet (He et al., 2016) and DenseNet (Huang et al., 2017) architectures are used for ex-periments on the CIFAR-10 dataset. We compare ResNet with 20/32/56 layers to DenseNet with40/60/100 layers and a growth rate k = 12. To reduce the total number of parameters, bottlenecklayers are enabled for DenseNet. Further comparison and analysis of the overfitting issues areprovided in Appendix C.2.
Figure 4: Test errors vs. total parameters trade-offs on the CIFAR-10 dataset using softmax normal-ization and LLR representation: (left) ResNet and (right) DenseNet3.4 CIFAR- 1 00 ClassificationFigure 5 summarizes the experiment results using different ResNet and DenseNet architectures onthe CIFAR-100 dataset. For all experiments using either softmax normalization or LLR repre-sentation, the same weight-decaying settings are used. In terms of efficiency trade-offs, we see asimilar trend as before: linear increase in accuracy tends to require exponential increase in networkcapacity. We notice that the difference between softmax normalization and LLR representa-tion is less prominent for the DenseNet architecture. One possible reason is that the BSP lossfunction is not yet fully optimized for the DenseNet architecture. Another possible reason isthat the effects of weight decaying are more prominent than softmax normalization for theDenseNet architecture with larger initial model sizes.
Figure 5: Test errors vs. total parameters trade-offs on the CIFAR-100 dataset using softmax nor-malization and LLR representation: (left) ResNet and (right) DenseNetParameters (k)4	Related WorkHistorically, deep neural networks using sigmoid or hyperbolic tangent activation functions weredifficult to train using backpropagation (Rumerlhar, 1986) due to the vanishing gradient problem(Glorot & Bengio, 2010). The introduction of ReLU activation function (Nair & Hinton, 2010)greatly improves training speed for deep learning, yielding improved prediction accuracy in manynew applications. However, using the ReLU activation function also tends to introduce overfittingissues as shown in this paper.
Figure 6: Training and test errors versus epochs using Softmax normalization and LLR representa-tion: (left) LetNet-300-100 and (right) LeNet-5EpochTable 2 summarizes the top-1 error rates, total number of parameters after pruning, and compressionrate for different methods with the LetNet-5 architecture. Fully-connected layers use a fixed pruningsetting of 1.25. For convolutional layers, the pruning settings are set as θk = 0.5+0.1×k, where k =0,1,…，9. The total numbers of pruned parameters and compression ratio in Table 2 are obtainedusing the largest pruning threshold. Weight sharing and inherent structural regularization of CNNfurther mitigate the overfitting issues in training. Using LLR representation and weight decaying,the accuracy of the pruned network is even better than the accuracy in training and testing. Thesnapshot-based method generates efficient networks with 31K parameters and the state-of-the-art14Under review as a conference paper at ICLR 2022performance, better than the ones using the iterative method from Han et al. (2015). Figure 6 (right)also compares the effects of overfitting between softmax normalization and LLR representation.
Figure 7: Test errors vs. total parameters trade-offs on the CIFAR-10 dataset using Softmax normal-ization and LLR representation: (left) ResNet and (right) DenseNetlayers use a fixed pruning setting of 0.75. For convolutional layers, the pruning settings are setas θk = 0.5 + 0.05 X k, where k = 0,1,…,9. The total numbers of pruned parameters andcompression ratio in Table 7 and 8 are obtained using the median pruning threshold.
