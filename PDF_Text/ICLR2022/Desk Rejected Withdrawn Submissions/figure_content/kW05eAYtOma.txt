Figure 1: An illustration of Paretodirection ∆(ω).
Figure 2: An illustration of how various methods find Pareto-optimal policies. (a): Linear scalar-ization can find Pareto-optimal policy but need may not be Pareto-feasible; (b): Lagrangian meth-ods can find feasible policy, while they may fails for too conservative to explore the environment;(c):CPO can find Pareto-optimal policy but cannot find Pareto-feasible policy if initial policy is toofat to reach the feasibility region; (d) Our method can find Pareto-feasible policy, while not eachstep is a Pareto direction.
Figure 3: Illustration of tasks in SafetyGym7Under review as a conference paper at ICLR 2022Goal-LVl 1əposɪdwJOd SPJEMOxButton-Lvl 1	PUsh-LVl 1əposɪdwJωd SISOoGoal-LVl 2	Button-Lvl 2	PUsh-LVl 2əposɪdwEd SPjeMWX1000əposɪdwjəd SlSOoEpisodes (X 10000 steps) 10001000Episodes (× 10000 steps) 1000• TRPO ∙PPO ∙ TRPO-Largrangian ∙ PPO-Largrangian ∙ CPO ∙MGDA ∙ CONTROLFigure 4: Reward and cost curves in all 6 tasks. A dashed line in cost curves represents the threshold.
Figure 4: Reward and cost curves in all 6 tasks. A dashed line in cost curves represents the threshold.
