Figure 1: An illustration of our label shift definition. We employ a transformation L : L#PT = PSto align two domains. The white/square points L xT on the source domain correspond to thewhite/square points xT on the target domain. We measure dY dY fT xT , fS L xT and definethe label shift w.r.t. L as LS(S, T;L) := EPT [dγ (fτ (xτ), fs (L (xτ)))]. Finally, we take infimumover all valid L to define the label shift between two domains.
Figure 2: Label shift estimation for various settings of DA when the source data set SVHN and thetarget data set is MNIST. Here, we denote [C] := {0, 1, ...,C} for a positive integer number C.
Figure 3: Illustration on target performance to show that forcing learning domain-invariant repre-sentations can hurt the target performance. Left: A→W (Office-31). Right: P→I (ImageCLEF-DA).
Figure 4: (a) 1D visualization of finding an appropriate μi. It is able to split same-label pairs(orange points) and different-label pairs (blue points) if μi is set to 号-1-perCentiIe of this array. (b)To observe the Weight values wij of those pairs, We randomly picked a target example to computesimilarity scores With 20 representative source points in a batch, and then sort them in ascendingorder. After computing the Weights, the figures for the same-label pairs tend to be much higherthan that for different-label pairs. A heat-map color is used to represent the Weights magnitude (thebrighter means higher value).
Figure 5: The t-SNE visualization of A→D (Figure a, b) tasks With label and domain information.
Figure 6: The t-SNE visualization of and P→C (Figure a, b) tasks with label and domain informa-tion. Each color denotes a class while the circle and cross markers represent the source and targetdata respectively.
Figure 7: Comparision of convergence performance between LDROT and other approaches on thetransfer task A→D.
