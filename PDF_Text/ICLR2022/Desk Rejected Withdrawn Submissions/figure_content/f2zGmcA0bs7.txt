Figure 1: Overview of our proposed approach. Given a video, audio, and text triplet, the networkextracts modality specific features and converts them into a set of primary capsules. Then, thesecapsules are routed using self-attention to obtain a higher-level activations, which are used to weightcapsule features. The weighted capsule features are projected into a final joint multimodal featurerepresentation. This joint representation space is enforced by a pair-wise contrastive loss.
Figure 2: Proposed Routing by Self-Attention. The input is a set of C capsules. The activation-weighted capsule features are projected into query, key, and value matrices which are used in amulti-head self-attention block to generate higher-level capsule poses. A linear transformation withsoftmax activation then generates the activations for these higher-level capsules.
Figure 3: Qualitative retrieval examples: top-3 zero-shot text-to-video retrieval results on theYouCook2 dataset for the proposed approach with self-attention based routing, the same one butwithout routing mechanism, and MIL-NCE* (* indicates that we used the same backbone as in ourmodel). Correct video colored in green.
Figure 4: Top-4 videos with the highest activation for the particular capsule on the MSR-VTT dataset.
Figure 5: Qualitative evaluation: examples of top-3 zero-shot text-to-video retrieval results on theYouCook2 dataset. Correct video colored in green.
Figure 6: More qualitative examples: top-3 zero-shot text-to-video retrieval results on the YouCook2dataset for the proposed approach with self-attention based routing, the same one but without routingmechanism, and MIL-NCE* (* indicates that we used the same backbone as in our model). Correctvideo colored in green.
Figure 7: Extended figure with examples of capsule highest activations: top-4 videos with the highestactivation for the particular capsule for the HowTo100M, MSR-VTT, and YouCook2 datasets. Labels:#number of capsule: assumed learned “concept”.
