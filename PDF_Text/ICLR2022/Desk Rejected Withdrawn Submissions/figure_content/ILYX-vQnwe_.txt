Figure 1: Examples of general and knowledge-based (KB) visual questions. The question and answersegments that focus on visual content within the image are highlighted in red, and the segments thatrequires external knowledge are highlighted in blue.
Figure 2: Model overview of the BreakDown VQA approach. The question is segmented intosemantic chunks (left top). These chunks are used to retrieve external information from Wikipediaand ConceptNet. Each retrieved piece of knowledge is then encoded as a vector (right top), and fedto a graph neural net (left middle) to predict an answer for each knowledge source. The individualresults are then max-pooled to get the final prediction (left bottom).
Figure 3: Qualitative results from our Break DoWn VQA and a ViLBERT baseline. Q1-Q4 showsuccess cases and Q5 and Q6 illustrate a couple failure cases. Red and green denote wrong and rightanswers, respectively.
