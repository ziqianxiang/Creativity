Figure 1: Depiction of pruning Withgreedy forWard selection as expressedin equations 8 through 10.
Figure 2: Accuracies of pruned, two-layer models on MNIST. Sub-plots depict different dense networksizes, while the x and y axis depict number of pre-training iterations and sub-dataset size, respectively.
Figure 3:	Validation accuracy of two-layer networks on binarized versions of MNIST as described inthe Two-layer Network Experiments Section in the main text. Different subplots display results forseveral different learning rates for models trained with different settings of m and N (i.e., datasetsize and number of hidden neurons, respectively). Shaded regions represent standard deviations ofresults, recorded across three separate trials.
Figure 4:	The performance of two-layer networks with different hidden dimensions over the entire,binarized MNIST dataset that have been pruned to various different hidden dimensions.
Figure 5:	Subnetwork validation accuracy on the CIFAR10 dataset for different settings of andinitial learning rate for fine-tuning. All models are pre-trained identically for 200 epochs. Fine-tuningis performed for 80 epochs, and we report validation accuracy for each subnetwork at the end offine-tuning.
