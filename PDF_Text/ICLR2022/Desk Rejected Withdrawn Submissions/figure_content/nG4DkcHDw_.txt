Figure 1: Atoy example to illustrate the security threats When encountered with backdoor attacksand adversarial attacks. An adversary performs backdoor attacks by planting a predefined trigger (acheckerboard pattern at the bottom right) in a small portion of training samples in bird class. Thetest samples will be wrongly classified as birds whenever the presence of the trigger. Meanwhile, theadversary performs adversarial attacks by adding imperceptible perturbations to mislead the modelto classify the ship picture as the airplane class.
Figure 2:	Examples of poisoned images in bird class. (a) is the original image. (b)-(j) are thepoisoned ones with different types of triggers.
Figure 3:	Attack success rates of adversarially and normally trained models. = 0 means standardtraining. (a) is the results of basic settings. (b) and (c) are the results with varying poison rates. (d)and (e) are the results with different triggers. (f) is the result with poisoning samples not restrictedto the target class. (g) is the result when replacing the ResNet-18 with VGG16 network (Simonyan& Zisserman, 2014). (h) is the result when replacing the target bird class with ship class.
Figure 4:	(a) and (b) are the comparisons with larger or smaller budget in basic settings. (c) and(d) are the results of blended attacks and label-consistent attacks.
Figure 5: The density statistics of PGD steps for clean and poisoned samples.
Figure 6: The pipeline of adversarial training against backdoor attacks.
Figure 7:	Experimental results of standard training, adversarial training and our pipeline.
Figure 8:	Clean accuracies of adversarially and normally trained models.
Figure 9:	More experiments with our proposed pipeline.
Figure 10:	The density statistics of PGD steps for clean and poisoned samples.
