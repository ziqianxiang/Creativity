Figure 1: Fishr principle. Fishr tacklesthe individual (per-sample) gradients of theloss in the network weights θ . Indeed, Fishrmatches the domain-level gradient variancesof the distributions across the two trainingdomains: A ({gAi }in=A1 in orange) and B({gBi }in=B1 in blue). We will show how thisregularization during the learning of θ im-proves the out-of-distribution generalizationproperties by aligning the domain-level losslandscapes at convergence.
Figure 2: Loss landscapes aroundan inconsistent θ * at convergence.
Figure 3: Colored MNIST dynamics. Atepoch 190, λ strongly steps up: then theFishrω regularization matches the domain-level gradient variances (red) and Hessians(purple) across E = {90%, 80%}. This re-duces train accuracy (orange) but increasestest accuracy (blue) as the network learns topredict the digit’s shape.
Figure 4: Risks dynamics on Col-ored MNIST with Fishrω . At epoch190, λ steps us and then empiricalrisks R90% and R80% get closer.
Figure 5: Dynamics on Colored MNIST with Fishrω strategy: at epoch 190, λ steps up. Thendomain-level Hessians are matched across domains (purple). More precisely, they take similar di-rections — high cosine similarity (red) — and similar norms (blue). The Hessians’ norms (orange)remain quite high thus the domain-level loss landscapes are rather sharp.
Figure 6: Dynamics on Colored MNIST with ERM strategy: λ = 0 along training. The Frobeniusdistance between domain-level Hessians (purple) keeps increasing: so does the distance betweentheir norms (blue). Their cosine similarity (red) steadily increases, but slower than with Fishr. Thedomain-level loss landscapes are flat at convergence (low Hessian norms in orange).
