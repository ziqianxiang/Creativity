Figure 1: Instead of retrospectively discovering individual failure cases for AV perception, We actively searchfor causal interventions (edits) to existing scenes that consistently result in perception failures. Shown in themiddle is an example of a single intervention causing perception failure, Which We attribute to the intervention,as opposed to the left side Where a combinatorial set of factors could explain the error. Consistent failuresthrough this type of intervention constitute a challenging group for the perception model as seen on the right.
Figure 2: A complete diagram of our approach: We intervene on the scene by performing transformationslike the pictured yellow truck becoming a white car and then evaluating the delta change in the object detectionmodelâ€™s efficacy. The interventions are guided by a trained MLM. Repeat 10000 times and group the scores toattain an ordered list of challenging groups drawn from vehicle type, weather, and rotation.
Figure 3: Test results with config 18C4 when train-ing on disjoint IID subsets. Results are consistent,suggesting that the harder groups - bikes, rotations,and cybertruck - are ubiquitously hard.
Figure 4: Independently increasing the model capacity(left) and increasing the data size (right). No modeldistinguished themselves and we quickly taper in howeffectively the model utilizes the data. We consider thedip in the capacity chart to be an artifact of the trainingprocedure and using the same settings for all models.
Figure 5: Results of training 18C4 on the base IID 10000 training set plus additional group data. The fivegroups in the top left (Cybertruck, Cola Car, Diamondback, Gazelle, and Crossbike) were added equally. Forall charts, adding any one group improved all of the other evaluation scores, and at no point did we lose efficacyon the IID data as a whole. Figure 10 (Appendix) zooms in on the initial jump.
Figure 6: How much IID data is required to match asmall amount of extra hard group data. Top left shows20000 more IID data was required to reach par on IIDwith 250 group data. Bottom left shows that we neverreached the same level on Diamondbacks with IID dataas with adding Cybertrucks, let alone actual bikes.
Figure 7: Charts showing increasing both data and model capacity at the same time. The left side ranges overmodel capacity with maximum IID data size (85000), while the right side ranges over IID data size with abigger model - 34C4.
Figure 8: Performance of 18C4 on select test sets when adding mode data from the three bikes, the ColaCar,and the Cybertruck on top of either 10000 or 85000 base IID data. Towards improving the results, these twocharts show that itis not the absolute count of the mode data that is important but rather the percent of it relativeto the IID data. We see that in how the trendlines for the two bases are only consistent in the percent chart. Theother modes are not shown for clarity but it holds in general.
Figure 9: We can see that the model size does matter in that for every group the 34C4 model improves overthe 18C4 model. However, the increase is quite small and the data quality and quantity appear to matter muchmore.
Figure 10: Results adding mode data to the base IID 10000 training set. This is the same as Figure 5 butzoomed into just [0, 1000]. The five modes in the top left are the Cybertruck, Cola Car, Diamondback, Gazelle,and Crossbike, each added in equal proportion.
Figure 11: Comparing rotation and weather results for MLM and Random intervention strategies. We see thatMLM fits with Original much better than Random does. Further, Random has a much wider berth of possibleproblematic modes, which is a concern given practical limits to model capacity and data budgets.
