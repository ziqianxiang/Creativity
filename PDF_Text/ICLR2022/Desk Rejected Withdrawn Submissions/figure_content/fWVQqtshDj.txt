Figure 1: The architecture of our proposed method.
Figure 2: Learning curves of our method versus state-of-the-art model-free algorithms. Each learn-ing curve is computed in three runs with different random seeds. The dash line depicts the desiredbest reward. MOBA (“Ours”) achieves faster convergence rate and achieves better performance thanmodel-free methods.
Figure 3: Learning curves of our method versus state-of-the-art model-based algorithms. Eachlearning curve is computed in three runs with different random seeds. MOBA (“Ours”) achievesfaster convergence rate than other model-based methods.
Figure 4: Learning curves of our methods versus SOTA model-based algorithms using three differentbias ranged dynamic models in the half cheetah environment.
Figure 5: Learning curves of MOBA andVanilla-MOBA.
Figure 6: Learning curves of MOBA, Vanilla-MOBA, MBPO, and GrBALA.4 Evaluation Results on Other EnvironmentsIn this part, we evaluate the performance of the proposed MOBA method on different teacherdatasets. In the RL problem, different datasets correspond to different tasks, where each task isdrawn from the same task distribution. To evaluate, we adopt the environment CausalWorld (Ahmedet al., 2021) that contains various features including task generation.
Figure 7: Learning curves of MOBA and PPO on CausalWorld EnvironmentThe results are shown in Figure 7.
