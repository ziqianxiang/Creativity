Figure 1: An overview of our proposed method for continual learning via low-rank network updates. We firstrepresent (and learn) the weight matrix (or tensor) for each layer as a product of low-rank matrices. To train anetwork for new tasks without forgetting the earlier tasks, we reuse the factors from the earlier tasks and adda new set of factors for the new task. Our experiments suggest that a rank-one update is often sufficient forsuccessful continual learning.
Figure 2: Average test accuracy for different datasets (Permuted MNIST, Rotated MNIST, Split CIFAR100,Split miniImageNet) along different tasks using different algorithms (AGEM,EWC, Orthog. Subspace, ICARLand our approach). Parallel full-rank results corresponds to the case when we train every task on separate fullrank networks independently. We showed the average of 20 tasks. It serves as an upper limit for continuallearning approaches.
Figure 3: Evolution of task-wise test accuracy on P-MNIST (i,ii,iii) and R-MNIST (iv,v,vi) datasets for EWC(i,iv), Orthogonal Subspace (ii,v), abd Our approach (iii,vi). We can observe from the decrease in the testaccuracies that EWC and Orthogonal Subspace forget the previous tasks as they learn new tasks. Our approachdoes not show any forgetting as we learn new tasks.
