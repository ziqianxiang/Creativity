Figure 1: Top: illustration of a nerve cell highlighting each studied synaptic diversity mechanism.
Figure 2: Course of the number of rejuvenated synaptic weights during 500 training steps of a shallowMLP.
Figure 3: Plots showing results of the hyper parameter tuning experiment.
Figure 4: Images with the lowest reconstruction error of all batches reconstructed from the AlexNetarchitecture trained on 100 epochs. Left side with our methods combined; on the right side withoutour methods.
Figure 5: We conducted an experiment on ResNet20 to evaluate how weight splitting is influencing aneuronâ€™s synaptic weights with the aim to receive an observation on the balance of the synapses thatconnect two neurons. The blue curve denotes the case with enabled weight splitting; two neuronsare connected by two corresponding synapses in this case and their absolute weight difference iscalculated and averaged over the whole network. In case of no weight splitting, denoted by the orangecurve, there are no corresponding synapses, but we still calculated the difference to another synapsefor comparison. We observe that weight splitting leads to a constant difference in the weights andthat in the case without weight splitting the differences shrink. These observation could cause theobserved difference in convergence behavior and generalization properties that we observed in ourexperiments. We also observe that while the absolute values shrink, the variances of the differencesare constant throughout the training; this means that all weights are affected in the same way, nomatter if they are larger or smaller at the beginning of the training.
Figure 6: Plots showing results of the learning rate variation experiment.
