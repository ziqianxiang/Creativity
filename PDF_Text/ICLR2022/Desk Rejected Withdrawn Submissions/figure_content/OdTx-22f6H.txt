Figure 1: View of the coronal, sagittal and axial planes of the brainThe Attention Residual UNET 3D or AR-UNET 3D is a modification upon the existing ResidualU-Net (Zhang et al., 2017) and Attention U-Net (Oktay et al., 2018), both of which operate in2D. AR-UNET 3D makes use of the residual blocks from ResNet (He et al., 2016) which help inmaintaining skip connections using identity mappings while the proposed soft attention mechanismprovides an added advantage by weighing the more important features, heavily.
Figure 2: AR-UNET 3DThe AR-UNET 3D, as depicted in Figure 2 and described in Table 1 is a 3D advancement of theResidual U-Net (Zhang et al., 2017) and Attention U-Net (Oktay et al., 2018) models that segmentin 2D. The AR-UNET 3D makes use of residual blocks based on the ResNet (He et al., 2016) archi-tecture and possesses the added advantage of soft attention mechanism. This attention mechanismhelps the model learn more important features from previous layers by assigning weights that cancelout less relevant ones while maintaining the importance of features that can help the model learnbetter.
Figure 3: Input, residual and attention blocks in AR-UNET 3DResidual block: The second block in Figure 3 consists of (3x3x3) convolutional layers with skipconnections that aid in retaining important features by using identity mappings and learning theresiduals similar to the ResNet (He et al., 2016) architecture which uses such residual blocks. Eachblock consists of two modules as can be seen Figure 3 and a final skip connection which is summedto the former combination.
Figure 4: LinkNet 3DEncoder: The encoder network comprises an initial block and 4 residual blocks (He et al., 2016)for feature extraction purposes.
Figure 5: Each Encoder(left) and Decoder(right) block (convolutional modules)4Under review as a conference paper at ICLR 2022Linked Architecture: The linking of each encoder to each decoder has been performed exactly asmentioned in (Chaurasia & Culurciello, 2017), in order to recover the lost spatial information whichis lost due to multiple downsampling operations in the encoder. To enable the linking operation,strided convolutions are used in the encoder.
Figure 6: Residual blocks 1, 2, 3 and 45Under review as a conference paper at ICLR 2022Decoder: The decoder network comprises of the 3D Pyramid Pooling Module and the FinalSegmentation Block3D Pyramid Pooling Module: The 3D pyramid pooling module is at the core of the PSPNet-3Darchitecture. The feature map from the final residual block of the encoder is passed into the 3Dpyramid pooling network which then uses four 3D adaptive average pooling layers for featurepooling at four different resolutions (1, 2, 3 and 6). This allows the model to learn the context of theoverall image at four different levels while retaining the spatial information of the image. The 3Dpyramid pooling blocks are demonstrated in Figure 7.
Figure 7: 3D Pyramid Pooling ModuleThe pooled feature maps are then upsampled through a convolution layer and are concatenated withthe original feature map before passing it to the final segmentation block.
Figure 8: Flipped and affine transformed imagesPre-processing techniques like resize, normalization and standardization have been performed alongwith augmentations like 3D flip and affine transformations. All images with 64 slices were processedto contain zero mean and unit standard deviation as a part of the normalization procedure.
Figure 9: Training results from AR-UNET 3D, LinkNet 3D and PSPNet 3D (left to right)7Under review as a conference paper at ICLR 2022Table 4: Training detailsModels	Parameters	Learning Rate	Mean Dice Loss	Best Dice ScoreAR-UNET 3D	35838349	0.0001	0.3270	0.8500LinkNet 3D	32925860	0.00001	0.3431	0.8041PSPNet 3D	74352202	0.0001	0.3334	0.8087Test results for all 3 models are given below.
Figure 10: Test results from AR-UNET 3D, LinkNet 3D and PSPNet 3D (top to bottom)5	Conclusion and future worksDue to computational shortcomings, each model was trained on a restricted amount of data.
