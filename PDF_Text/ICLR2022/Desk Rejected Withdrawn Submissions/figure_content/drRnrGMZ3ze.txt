Figure 1: The comparison of the standard policy gradient method without novelty seeking (left),multi-objective optimization method (middle), and our constrained optimization approach (right) forgenerating novel policies. The standard policy gradient method does not try actively to find novelsolutions. The multi-objective optimization method may impede the learning procedure when thenovelty gradient is being applied all the time, e.g., a random initialized policy will be penalizedfrom getting closer to the previous policy due to the conflict of gradients, which limits the learningefficiency and the final performance. On the contrary, the novelty gradient of our constrainedoptimization approach will only be considered within a certain region to keep the policy beingoptimized away from highly similar solutions. Such an approach is more flexible and makes themulti-objective optimization method as its special case.
Figure 2: Experimental results on the Four Reward Maze Problem. We generate 5 policies withdifferent novel policy generation methods. In each figure, 5 trajectories start from the +1 rewardpoint. Thresholds in CTNB and IPD are set as the averaged novelty of PPO policies.
Figure 3: The performance and novelty comparison of different methods in Hopper-v3, Walker2d-v3and HalfCheetah-v3 environments. The value of novelty is normalized to relative novelty by regardingthe averaged novelty of PPO policies as the baseline. The results are from 10 policies of each method,with the points showing their mean and lines showing their standard deviation.
Figure 4: The performance under different novelty thresholds in the Hopper, Walker and HalfCheetahenvironments. The results are collected from 10 learned policies based on PPO. The box extendsfrom the lower to upper quartile values of the data, with a line at the median. The whiskers extendfrom the box to show the range of the data. Flier points are those past the end of the whiskers.
Figure 5: The visualization of policy behaviors of agents trained by our method in Hopper-v3environment. Agents learn to jump with different strides.
Figure 6: The visualization of policy behaviors of agents trained by PPO in Hopper-v3 environment.
Figure 7: The visualization of policy behaviors of agents trained by our method in Walker2d-v3environment. Instead of bouncing at the ground using both legs, our agents learns to use both legs tostep forward.
Figure 8: The visualization of policy behaviors of agents trained by PPO in Walker2d-v3 environment.
Figure 9: The visualization of policy behaviors of agents trained by our method in HalfCheetah-v3environment. Our agents run much faster compared to PPO agents and at the mean time severalpatterns of motion have emerged.
Figure 10: The visualization of policy behaviors of agents trained by PPO in HalfCheetah-v3environment. Since we only draw fixed number of frames in each line, in the limited time steps thePPO agents can not run enough distance to leave the range of our drawing, which shows that ouragents run much faster.
