Figure 1: Overall workflow. Upper Figure: Our attention-based multi-head token selector to ob-tain token scores for keep/drop decisions. Lower Figure: Token selector is inserted multiple timesthroughout the model, along with the token packaging technique to generate a package token fromthe less informative tokens. The package token is concatenated with the informative tokens to befed in the following transformer blocks.
Figure 2: HeatmaPs showing the informative region detected by each head in DeiT-S. Each attentionhead focuses on encoding different image features and visual receptive fields. Tool refers to (Caronet al.,2021).
Figure 3: Illustration of the first attention ma-trix at the final block. Upper figure is theoriginal PiT-S, lower one is with HFSP.
