Figure 1: Flow of computation in a two-layer network. The constraint function g ensures that h1 ,is within a designed number of standard deviations from the mean ofa learned Gaussian distributionassigned to the class y(i). Subsequent layers propagate samples from this distribution rather thanthe transformed examples directlyThe fact that the constrained formulation in equation 1 has a direct dependency on the number of ex-amples in the dataset is easy to overlook. This stems from the fact that the formulation in equation 1is inherently deterministic: the stochastic counterpart or Sample Average Approximation (SAA)Rubinstein (1981) arising from the empirical risk minimization principle. Hence, the dataset D isbound to the equality constrained program itself via the auxiliary variables {h1, . . . , hL }. Moreprecisely, given a dataset D We need |D| ∙ L ∙ H constraints, for a network of L layers and hiddenactivations of uniform size H. This problem is further exacerbated by the need to also maintain La-grange multipliers when solving programs using primal-dual methods, which doubles this number.
Figure 2: Log-scale mean absolute defect and it's standard deviation across 20 runs Vs gradientsteps on the MNIST dataset, GDA starts with zero defect but the optimization dynamics are ofteneventually destabilized stopping any progress in terms of objective function.
Figure 3: Here we show mean and standard deviation of the of generalization scores across 10random seeds, OAM and DTP are shown for reference. On MNIST both ExtraGradient based meth-ods (ExtraProp and DistProp) have stable optimization dynamics and good performance, GDA lacksboth performance and stability, while a regularization approach suffers from high variance.the Dist-Prop formulation achieves a lower generalization loss, possibly due optimizing a wider supportrather than just the training distribution.
Figure 4: Robustness to hyper-parameters, for DistProp on MNIST. EaCh line represents an exper-iment with values defined by the point of interseCtion with the parameter axis, shown is the rangeof hyper-parameters for whiCh performanCe does not drop more than 10%, i.e. where test aCCuraCyis above 89%. The table reports a possible interpretation of the ranges for eaCh hyperparametersfor whiCh the performanCe does not degrade more than 10% from the best performer, due to theComplex interaCtion between hyperparameters, these values should be taken as an heuristiC at best.
