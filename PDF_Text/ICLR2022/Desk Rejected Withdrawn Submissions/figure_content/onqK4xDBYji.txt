Figure 1: ELMo model Architecture Peters et al. (2018)一 o⅜lut -¾2COmb∙eE一 Lay百⅛2ConCatConCa-2 Background2.1 ELMOLCombine = γ	αj × hj	(1)j=0ELMo (Peters et al. (2018)) stud-ies the features at different depthsof a Bi-LSTM architecture. Theauthors analyze a two-layered Bi-where, hj is the activation, αj is a learnt parameter associatedwith layer j and γ is the scaling factor learnt during training.
Figure 2: BERT model architecture Devlin et al. (2019)EnCOderEnCOder一semi-supervised training phase. MLM objective trains the model to predict the masked, replaced, orretained words. Such a scheme enables bidirectional information flow for the prediction of a partic-ular word. The NSP task is oriented for the model to learn the relationships between the sentences.
Figure 3: Proposed model architecture BERMoBERMo, shown in Figure 3, mod-ifies the BERT architecture by lin-early combining the features usingthe scheme proposed in ELMo inorder to obtain a rich feature repre-sentation. Learnable scalar param-eters αi is associated with the ac-tivations of ith layer with α0 be-ing the scalar parameter associatedwith embedding of the input. Thesescalar parameters are softmax nor-	Figure 4: Combine Blockmalized to ensure Pi1=2 0 αi = 1.
Figure 5: Accuracy/F1-Score comparison with varying epochs for pruning BERTBASE andBERMoBASE to retain 10% of the weights. BERT Baseline represents the performance of the modeltrained for 3 epochs without any pruning (Table 2).
