Figure 1: Modified SEnet Block: The additional blocks in SEnet block i.e. Conv2D + MaxPoolingwith kernel size 3,5 that helps our model to learn better low level structural features along with highlevel features in each layer. Then squeeze and excitation layer, extracted the stacked channel wiseinformation with global receptive field.
Figure 2:	Input MNIST dataset for our model with a) Input MNIST digit with grayscale form b)Input MNIST digit with added Gaussian noise of zero mean and unit variance c) Input MNISTFGSM adversarial digit image for 0.1 epsilon.
Figure 3:	In-class and OOD class samples density vs confidence with probability score for a) InputMNIST digit with grayscale form b) Input MNIST digit with added Gaussian noise of zero mean andunit variance c) Input MNIST FGSM adversarial digit image for 0.1 epsilon.
Figure 4: Modified SEnet Model: Trained for MNIST and Fashion-MNIST dataset with 10 classesas In-distribution and one for OOD sample distribution with overall Alexnet.
Figure 5: Alex-Net model with IN-Class(MNIST, colored samples) and OOD class(EMNIST, blackcolor samples) a) Data distribution from model b) AUROC evaluation and c) plot of data densitywith probability confidenceIn Figure 6a is the output samples from our model with In-class and OOD class samples, there itis quite clear that the OOD samples are having quite less probability score for OOD samples thanIn-class while Figure 6c shows the histogram for softmax score for In-class and OOD samples.
Figure 6: Our Modified SEnet model with IN-Class(MNIST, colored samples) and OODclass(EMNIST, black color samples) a) Data distribution from model b) AUROC evaluation andc) plot of data density with probability confidenceApart from the confidence score for each class, we had visualized the second last layer output dis-tribution of our model that helps us to understand how our loss function can maximize the marginbetween In-class and OOD samples and how the OOD samples normalization affects the magnitudeof distribution.
Figure 7: Data distribution from 2D features from our model a)For entire batch b) overlay view6	Conclusion and Future WorkWe presented a robust approach for OOD dataset, classification method without much drop in In-class classification accuracy. Here our model learns the low-level contextual features along withhigh level features at each layer with modified SEnet block. The loss functions had played thecrucial roles for maximizing the margin between the In-class and OOD class data distribution. Wealso achieved the state-of-the-art (S.O.T.A.) results for few evaluation metrics as in Table 1. Ourapproach can give better and more reliable classification, AI system for real word application.
