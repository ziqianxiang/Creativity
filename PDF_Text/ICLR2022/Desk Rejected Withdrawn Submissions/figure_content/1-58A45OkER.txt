Figure 1: Channel-wise bit allocation plus hardware-aware constraints (HA) achieves the best per-formance on Eyeriss and TPU. Channel-wise bit allocation outperforms layer-wise bit allocationbecause of higher quantization precision. Inference Rate: number of images per second.
Figure 2: Examples of finding optimal bit allocation w/ and w/o hardware-aware constraints.
Figure 3: Number of weights and activations of layers and on-chip capacity on hardware platforms.
Figure 4: A breakdown for the compute time and memory time on Google TPU and MIT Eyeriss.
Figure 5: Examples of a neural network F and a modified neural network F.
Figure 6: Examples of the optimization method. In the left figure, the red point has minimal intercepton Y-Axis and it is selected. The middle and right figures show the selected points on three curveswhen λ = -0.5 and λ = 2.0, respectively.
Figure 7: Distributions of bit rate across layers given different on-chip memory constraints.
Figure 8:	Number of variables that the on-chip memory of specific hardware platforms can accom-modate, with different bit width per variable. The minimum and maximum numbers of activationsthat a single layer can have are highlighted.
Figure 9:	Top-1 accuracy and output distortion under different network size on ResNet-18, ResNet-50, and MobileNet-v2.
