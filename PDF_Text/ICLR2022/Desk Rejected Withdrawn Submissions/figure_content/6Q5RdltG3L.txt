Figure 1: Left: The two big circles represent the attacks or small perturbations that can cause changes visually inmachines perception and human perception respectively. Commonly used adversarial attacks (the red circle), forexample Lp based adversarial attacks, are visually perceived by both machines and humans. We design our attacks(the green circle) to be imperceptible to humans and only affect machine perception. Then we start adversarial trainingwith our attack method and move the relationship as displayed in the left image to the right image. Right: Adversarialtraining with our attacks discourages the model perceiving the attacks in the green circle, so it pushes the machineperception circle to the direction of human perception. In the end of the adversarial training, the perturbations thatmachines perceive and humans perceive overlap more, so that the two perception systems align more closely.
Figure 2: Mailbox fromUnited Kingdom (Left) andfrom Cambodia (Right).
Figure 3: Left: Original, Middle: Our one-step method, Right: PGD (L2). We compare our one-step method andone-step PGD attack using L2 cost function, both with = 10. Here are the first two 1’s in MNIST test split that aresuccessfully attacked by both methods. Our attack does not change the structure nor the true class of the numbers, butL2 attacks make the digits unrecognizable and more similar to the mis-classified label. More examples can be foundin Section B.
Figure 4: The comparison between the original image and adversarial attacks. Our method and PGD method generateimages of similar visual quality, so We do not put PGD images here. PerC-AL has marble effects (in the background ofthe first image and around the beak in the second image). The image quality of LPA degrades, as there are noticeablesand effects in the second image, compared With other images. PPGD has an area of noises in the first image. Theimages in full resolution are available in the supplementary file. More results can be found in Figure 10.
Figure 5: DISTS distances comparison between the attacks and original images. Left: boxplots of empirical DISTSdistance distribution of all methods. The box’s 3 bars each represents the distribution’s 3rd quantile, median, and1st quantile respectively. Our method with a = 0’s distribution has the smallest quantiles. Right: the cumulativedistribution function (CDF) of empirical DISTS distance distribution of all attack methods. At any DIST distance d,our method with a = 0 has the largest IP(xadv ≤ d). Our method with a = 1 has smaller perceptual distances thanPerC_AL, NPTM (LPA), and NPTM (PPGD). The Same plots for LPIPS distance can be found in Figure 12.
Figure 6: Comparison between the human saliency maps of the original image and attacked images. Compared withthe original saliency map, our method generates the map with the least distinction. In the second image, NPTM(PPGD) shifts the attention from the mushrooms in the middle to the right. More results can be found in Figure 11.
Figure 7: After the attacked being processed with jpeg compression defense and bit depth compression defense, ourmethod with confidence 5 has the highest attack success rate in both defense methods and the images are still lessperceptible than NPTM (PPGD)’s images (Figure 5). Our method with confidence 1 generates less perceptible attackimages than PerC_AL and has a better attack success rate in jpeg compression defense.
Figure 9: Left: original, Middle: one-step our method, Right: one-step PGD (L2). Here are additional results ofFigure 3. Each image is the first example of each digit that is successfully attacked by our method and PGD method inthe MNIST test split. We shoW that using 1 - SSIM as the cost function successfully penalize any stuctural changes.
Figure 10: More qualitative results on image comparison. PerC-AL has marble effects in the water in the first imageand above the car in the second image. The image quality of LPA degrades, as there are noticeable sand effects (in thewater in the first image). PPGD has an area of noticeable noises in the first image.
Figure 11: The comparison between the human saliency maps of the original image and adversarial attacked images.
Figure 12: LPIPS distances comparison between all the attacks and the original images. Similar to Figure 5, ourmethod with a = 0 has the lowest boxplot and the CDF curve enclosing the largest area. Both of the plots show thatour method generates attack images with the smallest perceptual distances.
Figure 13: This pie chart shows the percentage of images coming from each country, which has a similar distributionas ColleCted in Shankar et al. (2017). For a Clearer visualization, we unfortunately Cannot fit the Country’s names andperCentage if it has < 1000 images.
