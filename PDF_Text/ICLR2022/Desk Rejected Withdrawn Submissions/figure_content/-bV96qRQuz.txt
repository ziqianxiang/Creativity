Figure 1: Overview of FEAR which first trains a fast but shallow learner to get a reasonable train-ing/validation error threshold and then trains the architecture in a two-stage procedure. In the firststage the architecture is regularly trained until it achieves threshold accuracy. In the second stagemost of the partially trained architecture from the first stage is frozen and training continues for a fewmore epochs. All candidate architectures can then be ranked by final training or validation accuracyobtained via this two stage procedure.
Figure 2: Time to reach threshold training accuracy (x-axis) vs. final test accuracy on 1000 uni-formly sampled architectures on Natsbench. They show a clear relationship where ultimately worseperforming architectures take longer time to reach threshold accuracy than stronger ones.
Figure 3: [Left] Average duration per architecture vs. Spearman,s correlation and [Right] averageduration per architecture vs. common ratio over the top x% of the 1000 architectures sampled fromNatsbench topological search space on CIFAR100. We also show the various zero-cost measuresfrom Abdelfattah et al. (2021) in green. Recall that in Appendix A.6 these measures will be shown tobe non-robust to change of task as well as degradation when the training progresses.
Figure 4: [Left] Time to reach threshold training accuracy (x-axis) vs. final test accuracy on 1000uniformly sampled architectures on Natsbench SSS on CIFAR10. [Right] 21580 architectures on theDARTS search space from Nasbench301 on CIFAR10.
Figure 5: In this illustration on 1000 architectures from Natsbench topology search space on CIFAR10we show how one can use FEAR in the inner loop of any discrete search algorithm to early-rejectmany architectures thus saving time without losing final accuracy. For simplicity we considerrandom search here. A simple modification to random search is made where the fastest time forany architecture to reach threshold accuracy is tracked. If any subsequent architecture improves onthis time then it is updated. If any subsequent architecture during evaluation takes more than x ×fastest_till_now (where x is usually 4.0 in our experiments as that leads to retaining mostof the top ones) then evaluation is terminated and control moves on to the next randomly sampledarchitecture. Such architectures which are early rejected from evaluation are represented by theregion to the right of the green vertical dotted line in the figure. As more and more architecturesare evaluated the estimate of the fastest_till_now improves and so does early terminationof weaker architectures. And this benefit of early stopping weaker architectures is evident in ourexperiments where we see that one can reduce the search time by ≈ 2.4 times on CIFAR100 andImageNet16-120 without losing accuracy as detailed in Section 4.
Figure 6:	[Left] Average duration vs. Spearman’s correlation and [Right] averge duration vs. commonratio over the top x% of the 1000 architectures sampled from Natsbench topological search space onCIFAR10. We also show the various zero-cost measures from Abdelfattah et al. (2021) in green.
Figure 7:	[Left] Average duration vs. Spearman’s correlation and [Right] average duration vs.
Figure 8:	[Left] Average duration vs. Spearman’s correlation and [Right] average duration vs.
Figure 9:	Natsbench CIFAR10 1000 architectures. The stage 1 training threshold was varied.
Figure 10: Natsbench CIFAR100 1000 architectures. The stage 1 training threshold was varied.
Figure 11: Natsbench ImageNet16-120 1000 architectures. The stage 1 training threshold was varied.
Figure 12: Natsbench CIFAR10 1000 architectures. The number of epochs of stage 2 is varied.
Figure 13: Natsbench CIFAR100 1000 architectures. Number of epochs of stage 2 training wasvaried.
Figure 14: Natsbench ImageNet16-120 1000 architectures. Number of epochs of stage 2 training wasvaried.
Figure 15: Natsbench CIFAR10varied.
Figure 16: Natsbench CIFAR100received gradients was varied.
Figure 17: Natsbench ImageNet16-120which received gradients was varied.
Figure 18: Evaluating the ranking performance of zero-cost measures after each epoch of training.
