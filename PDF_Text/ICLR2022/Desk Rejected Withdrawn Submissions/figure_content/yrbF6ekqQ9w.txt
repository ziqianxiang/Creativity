Figure 1: Compared to standard fine-tuning, weight-space ensembles for fine-tuning (WiSE-FT)improve out-of-distribution (OOD) accuracy without decreasing in-distribution (ID) performance.
Figure 2: Samples of the class lemon, from ImageNet (in-distribution) and the derived out-of-distribution datasets considered in our main experiments.
Figure 3: When fine-tuning end-to-end, WiSE-FT outperforms output-space ensembles, intermediatecheckpoints, and quadratic regularization (results shown for ViT-B/16 CLIP).
Figure 4: (Left) Zero-shot and fine-tuned models exhibit diversity in their predictions. (Middle) Onmost OOD datasets, the zero-shot model overrides the linear classifier more than it is overridden.
Figure 5: The zero-shot and fine-tuned models exhibit linear mode connectivity (Frankle et al., 2020)on ImageNet and the main distribution shifts we consider (Observation 1). Moreover, there exists anα for which WiSE-FT outperforms both the zero-shot and fine-tuned models (Observation 2).
Figure 6: A per-dataset breakdown of the key experimental results (Figure 1). WiSE-FT improvesID and OOD accuracy on ImageNet and five derived distribution shifts. Standard ImageNet models,models trained with more data, and existing robustness interventions are from Taori et al. (2020).
Figure 7: A zoomed-out version of Figure 6. WiSE-FT improves ID and OOD accuracy on ImageNetand five derived distribution shifts. Standard ImageNet models, models trained with more data, andexisting robustness interventions are from Taori et al. (2020).
Figure 8: WiSE-FT improves OOD accuracy while maintaining or improving ID accuracy onImageNet-Vid-Robust, YTBB-Robust (Shankar et al., 2019), CIFAR-10.1 (Recht et al., 2019),CIFAR-10.2 (Lu et al., 2020), WILDS-FMoW (Koh et al., 2021; Christie et al., 2018), and WILDS-iWildCam (Koh et al., 2021; Beery et al., 2021). Numbers reported are percentage point improvementsOOD without any loss in ID accuracy compared to standard fine-tuning.
Figure 9: Comparing the relative ID and OOD accuracy of weight-space ensembling with thealternatives described in Section B.3. Many methods follow a concave trend, though weight-spaceensembling provides the best performance overall.
Figure 10: Comparing WiSE-FT with CoOP (Zhou et al., 2021). Both methods fine-tune theViT-B/16 CLIP model on 16 examples per class of ImageNet.
Figure 11: The ID accuracy of WiSE-FT (end-to-end) with mixing coefficient α on ImageNet and anumber of datasets considered by Kornblith et al. (2019b): CIFAR-10, CIFAR-100 (Krizhevsky et al.,2009), Describable Textures (Cimpoi et al., 2014), Food-101 (Bossard et al., 2014), SUN397 (Xiaoet al., 2016), and Stanford Cars (Krause et al., 2013).
Figure 12: WiSE-FT can improve in-distribution accuracy over the linear classifier and zero-shotmodel in the low data regime. On the x-axis we consider k = {1, 5, 10, 25, 50} examples perclass for fine-tuning. On the y-axis we display in-distribution accuracy improvements of WiSE-FTaveraged over seven datasets (Deng et al., 2009; Krizhevsky et al., 2009; Cimpoi et al., 2014; Bossardet al., 2014; Xiao et al., 2016; Krause et al., 2013). For k = 1, the zero-shot model outperformsthe fine-tuned linear classifier, and ensembles closer to the zero-shot model (small α) yield highperformance. When more data is available, the reverse is true, and higher values of α improvein-distribution performance. Appendix F, displays a breakdown for all datasets.
Figure 13: WiSE-FT improves in-distribution accuracy over the linear classifier and zero-shot modelin the low data regime. On the x-axis we consider k = {1, 5, 10, 25, 50} examples per class and thefull training set. On the y-axis we consider the in-distribution accuracy improvement of WiSE-FT overthe (top) zero-shot model, (middle) fine-tuned linear classifier, and (bottom) best of the zero-shotand fine-tuned linear classifier.
Figure 14: WiSE-FT improves in-distribution accuracy over the linear classifier and zero-shot modelin the low data regime. On the x-axis we consider k = {1, 5, 10, 25, 50} examples per class and thefull training set. On the y-axis we consider the in-distribution accuracy improvement of WiSE-FT overthe (top) zero-shot model, (middle) fine-tuned linear classifier, and (bottom) best of the zero-shotand fine-tuned linear classifier.
Figure 15: WiSE-FT provides benefits for all CLIP models. Accuracy can be improved out-of-distribution relative to the linear classifier with less than ∈ {0, 0.1, 1} percentage points (pp) lossin-distribution across orders of magnitude of training compute. The CLIP model RN50x64 requiresthe most GPU hours to train.
Figure 16:	WiSE-FT improves ID and OOD accuracy across a number of distribution shifts with asmaller CLIP ViT-B/16 model.
Figure 17:	Ensembling with a zero-shot model improves the out-of-distribution performanceof an independently trained model. (Left) Output-space ensembling with an independently trainedmodel (NoisyStudent EffiCientNet-B6) with comparable in-distribution performance to the end-to-endfine-tuned model. (Right) Output-space ensembling with an independently trained model withstrong in-distribution performance (NoisyStudent EfficientNet-L2). Results averaged over the fivedistribution shifts as in Figure 1.
Figure 18: Various regularizers trace similar trends when fine-tuning a linear classifier. Com-paring the relative in- and out-of-distribution performance of fine-tuning a linear classifier with thevarious methods of regularization discussed in Section C.3.
Figure 19: Comparing the relative in- and out-of-distribution performance of fine-tuning a linearclassifier with various learning rates and no explicit regularization. As discussed in Section C.3,batch size is chosen randomly from {64, 128, 256} for each experiment. As learning rate increasesthe linear classifiers follow a parabolic trend similar to the trend followed by explicit regularization(see Figure 18).
Figure 20: Effective robustness scatter plots for ObjectNet, with and without adapting to class shift.
Figure 21: Prediction Diversity (PD) for multiple datasets and CLIP models (Equation 3).
Figure 24: Central Kernel Alignment Complement (CKAC) for multiple datasets and CLIPmodels (Equation 8).
Figure 25:	Comparing the relative in- and out-of-distribution performance of WiSE-FT with thealternatives described in Appendix B.3 on ImageNetV2.
Figure 26:	Comparing the relative in- and out-of-distribution performance of WiSE-FT with thealternatives described in Appendix B.3 on ImageNet-R.
Figure 27:	Comparing the relative in- and out-of-distribution performance of WiSE-FT with thealternatives described in Appendix B.3 on ImageNet Sketch.
Figure 28:	Comparing the relative in- and out-of-distribution performance of WiSE-FT with thealternatives described in Appendix B.3 on ObjectNet.
Figure 29:	Comparing the relative in- and out-of-distribution performance of WiSE-FT with thealternatives described in Appendix B.3 on ImageNet-A.
Figure 30: Schematic of the average error landscape. Li et al. (2018) observe that solution spacesfor a given task are high dimensional, while D, Amour et al. (2020); Wortsman et al. (2021) observethat movement within the solution space can change model performance on other data distributions.
