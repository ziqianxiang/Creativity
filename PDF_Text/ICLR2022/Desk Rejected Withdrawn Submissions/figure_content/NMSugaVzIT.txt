Figure 1: Linear predictors learned by two layer linear convolutionalnetwork for the task of classifying digits 0 and 1 in MNIST. The sub-figuresdepict predictors learned by using gradient descent on the exponential lossfor overparameterized networks with kernel size K = (3, 3) and numberof output channels C ∈ {1, 2, 4, 8}(left to right).
Figure 2: RbK,C(fGD) = kUk2 + kVk2 of thepredictor learned by gradient descent on ReLUconvolutional networks with bias on both layers,with different number of output channels C andkernel sizes K, on the MNIST task. The valuesshown are the medians taken over 5 trials.
Figure 3:	Linear predictors learned by two layer linear convolutional network for the task of classifying digits 0 and 1 in MNIST.
Figure 4:	Explicit RK,C margin predictor on sampled MNIST dataset for kernel sizes K ∈ {(1, 1), (28, 28)} (left to right). Thetop row in each sub-figure is the signal domain representation w(U, V), and the bottom row is the Fourier domain representationwb(U, V).
Figure 5:	Linear predictors learned by two layer linear convolutional network on CIFAR-10 task. The sub-figures depict predictorslearned by using gradient descent on the exponential loss for overparameterized networks with C ∈ {1, 2, 3, 4, 8} and kernel sizeK = (1,1). The top row in each sub-figure is the signal domain representation w(U, V), and the bottom row is the Fourier domainrepresentation wb(U, V).
Figure 6: Linear predictors learned by two layer linear convolutional network on CIFAR-10 task. The sub-figures depict predictorslearned by using gradient descent on the exponential loss for overparameterized networks with C ∈ {1, 2, 3, 4, 8} and kernel sizeK = (3, 3). The top row in each sub-figure is the signal domain representation w(U, V), and the bottom row is the Fourier domainrepresentation wb(U, V).
Figure 7: Linear predictors learned by two layer linear convolutional network on CIFAR-10 task. The sub-figures depict predictorslearned by using gradient descent on the exponential loss for overparameterized networks with C ∈ {1, 2, 3, 4, 8} and kernel sizeK = (8, 8). The top row in each sub-figure is the signal domain representation w(U, V), and the bottom row is the Fourier domainrepresentation wb(U, V).
Figure 8: Linear predictors learned by gradient descent on single output channel networks over an augmented input space withkernel sizes K ∈ {(1, 1), (3, 3), (27, 27), (45, 45), (65, 65), (84, 84)} (left to right). The input images from the MNIST dataset areaugmented by padding with zeros to obtain an image of size 112 × 112 with the signal present only in the top-left 28 × 28 block.
Figure 9: Explicit '2,1, '1 and '2 margin predictors on sampled CIFAR-10 dataset. The top row in each sub-figure is the signaldomain representation w(U, V), and the bottom row is the Fourier domain representation wb(U, V)the induced regularizer in these special cases. In Figure 9, we show resulting optimal solutions for K = D (a minimum`2,1 solution) along with the optimal `1,1 solution. We visually see that the `2,1 solution favors similarity across inputchannels at the expense of greater sparsity in the frequency domain.
Figure 10: Linear predictors learned by two layer linear convolutional network on CIFAR-10 task. The sub-figures depictpredictors learned by using gradient descent on the exponential loss for overparameterized networks with C = 3 and kernel sizesK ∈ {(1,1), (3, 3), (8, 8), (20, 20), (32, 32)}. The top row in each sub-figure is the signal domain representation w(U, V), andthe bottom row is the Fourier domain representation wb(U, V).
