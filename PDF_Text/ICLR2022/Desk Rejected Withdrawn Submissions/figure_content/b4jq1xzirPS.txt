Figure 1: An example of TSP-D with state st := (Vt, cttr, ctdr, rttr, rtdr) and action at = (attr, atdr) onsix nodes: black, grey and white nodes represent a depot, customers served by drone and customersserved by truck respectively. Solid and dashed lines correspond to drive and fly arcs respectively. Thedrone and the truck traverse together from the depot to the first customer. Then drone is launched toserve the second customer, while the truck traverses to serve the third customer. At this node, thedrone reunites with the truck to be launched to serve the next customer. Drone and truck reunite afterat the last customer location and traverse back together to the depot.
Figure 2: An Encoder-Decoder structure of Attention-LSTM Hybrid model (HM).
Figure 3: Overview of distributed RL training for HM: each of multiple workers executes rolloutwith different training instances, updates its parameters, and evaluates costs. The learner replaces itsnetwork parameters with the ones from the best worker.
Figure 4: [(a), (b), (c)] AM (greedy) vs. HM (greedy). The percentage difference to TSP-ep-all(N = 20) or DCH/25 (N = 50, 100) is reported for each instance. The dashed and solid linesrepresent the mean values of AM and HM, respectively; [(d)] DCH/g vs. AM (s) vs. HM (s) forvarious g and s values. The average cost to the solution time is reported.
