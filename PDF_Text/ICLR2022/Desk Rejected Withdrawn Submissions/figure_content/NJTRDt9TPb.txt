Figure 1: Discrete latent variable: locomotion towards multiple directions. Trajectories of the imitatedpolicies for four tasks. Different latent codes at inference time are color coded. From top to bottom: numberof modes are 3, 2, 6, 6; number of trajectories per latent code are 1, 3, 3, 3. Both SOG-BC and SOG-GAILprecisely separate and imitate the modes in all the experiments. However, VAE-GAIL and InfoGAIL fail toseparate the modes and to imitate the expert.
Figure 2: Discrete latent variable: locomotion at six velocities. Velocities generated by the policies vsrollout time steps. Three rollouts per mode are visualized. Each mode is marked with a distinct color. BothSOG-BC and SOG-GAIL separate and imitate most of the modes, producing the desired velocities. VAE-GAILand InfoGAIL exhibit an inferior performance.
Figure 3: Continuous latent variable: FetchReach. Sample trajectories of the robotic arm. In (b), (d) experttrajectories are embedded and fed to the learned model for reconstruction. In (c), since InfoGAIL lacks anymodule for encoding expert trajectories, no target point is considered, and random codes are used to generatetrajectories. Blue circles mark different targets. Colors of the trajectories are chosen at random for better visualdistinction. Compared to the baselines, SOG-BC better reaches different targets.
Figure 4: Self-organization in the latent space: FetchReach. In each plot, two dimensions among the threedimensions of the latent space are fixed, whereas the third is varied. Each line corresponds to the location ofthe robotic arm at the final state of different trajectories. In each line, two latent dimensions take a fixed value,while the third varies. Lines are randomly colored for better visual distinction.
Figure 5: Continuous latent variable: HalfCheetah-Vel. In (a), since VAE-GAIL uses a high latent di-mension, in the horizontal axis we consider the desired velocity of the expert trajectory corresponding to thelatent embedding. However, in (b), (c) the horizontal axis corresponds to (the CDF of) the 1-D Gaussian latentvariable. The CDF is applied for better visualization. In SOG-BC, different velocities in range [1.5, 3] arereplicated with a higher certainty.
Figure 7: Optimal choice of λS. Parameter λS is the coef-ficient of the SOG loss in Algorithm 3. Performance of Al-gorithm 3 is illustrated as λS varies. We consider two casesof with/without perturbations. These perturbations are im-posed by taking random actions with a chance of 20%. Thevertical axis is normalized such that the best performancein unperturbed settings achieves a score of 1, and a randompolicy achieves 0. In Figure (b), we observe that the policytrained with higher values of λS shows less robustness toperturbations in several experiments .
Figure 6: Robustness of SOG-BC vs SOG-GAIL: Ant-Dir-6. To create unseen situa-tions, we switch the latent codes to those cor-responding to the directions shown in (a). In(b), (c) we show three trajectories generated indifferent colors. We observe that the ant agenttrained with SOG-GAIL performs the desiredtask flawlessly, while the one trained with SOG-BC often topples over and fails the task.
Figure 8: Neural network architectures. (a) the policy network, (b) the discriminator in GAIL.
Figure 9: Self-organization and Mode Assignment: In the experiment described in Appendix C, the assign-ment of the same winning latent code to data points belonging to the same mode in the ground truth data happensvery quickly. We create additional one dataset with less separation between ground truth modes compared tothe one we used in Appendix C. By the end of epoch 1 for data where the modes are very well separated, andby the end of epoch 3 in data with modes that are closer together, the mode separation is almost complete.
Figure 10:	Three-mode clustering toy example. Ground truth and model predictions at convergence. Pointsare colored-coded with the latent code assignments from the EM algorithm.
Figure 11:	Posterior distribution of soft EM along trainingnever attain purely one-hot posterior, which is shown as follows. In particular, the EM approachmaximizes the expected likelihood of the data (where expectation is taken over all the latent codes),whereas the SOG maximizes the likelihood of the data conditioned at the code where it is maximum.
Figure 12: The training curve of the EM algorithm vs that of SOG.
Figure 13: A contour plot demonstration of h(μ, Λ) for two-dimensional case. The axes correspond to thediagonal elements of Λ = diag(σ2, σ2). In this example, kμk = 3.
Figure 14: MNIST dataset. Output results of Algorithm 4 for the latent dimension of 2. (a) Synthesizedimages (b) Embedding of MNIST test data in 2 dimensions by the search mechanism of the SOG algorithm.
Figure 15: Fashion MNIST. Output results of Algorithm 4 for the latent dimension of 8. Each 6 × 6 block(or block of blocks, etc.) sweeps over one dimension of the latent space. Due to limited space, only part of theentire latent space is plotted. Full results are provided in the link (See the SummPlementary material zip file inthe review stage). Different latent dimensions have smoothly captured semantically meaningful aspects of thedata, i.e. type of clothing, color intensity, thickness, etc.
Figure 16: CelebA. Output results of Algorithm 4 for the latent dimension of 16. (a) Ground truth samples.
