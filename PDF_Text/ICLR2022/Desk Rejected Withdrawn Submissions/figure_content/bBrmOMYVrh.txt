Figure 1: LiST leveragesPromPt-based fine-tuning(FN) with unlabeled data forlabel-efficiency and adaPtersfor reducing tunable Param-eters .	(a) shows classictuning, PromPt-based FN andLiST using RoBERTa-largeas backbone on MNLI taskfor a comParison. The reddash line dePicts the ceilingPerformance with full suPer-vision (400K training labels)with RoBERTa-large. (b)shows the number of tunableParameters for each method.
Figure 3: LiST explores sev-eral adapter placement choices(numbered positions in left) instandard Transformer architecture,with adapter design shown in right.
Figure 4: The underlinedtext depicts task promptto transform classificationinto Fill-in-MASK task.
Figure 5: Performance comparison of Classic-tuning (denoted as “C”) and prompt-based fine-tuning(denoted as “P”) with LiST on MNLI and RTE using language model encoders of different sizes.
