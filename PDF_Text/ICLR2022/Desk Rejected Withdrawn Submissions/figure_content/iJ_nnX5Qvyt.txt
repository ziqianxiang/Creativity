Figure 1: Overview of the proposed RL framework, including the DNNs computational graph em-bedding, RL agent neural architecture, and inference and deployment illustration.
Figure 2: Explorations of training setups and neural architecture configurations by measuring thetesting performance using mismatch and loss values, specifically covering aspects of traininggraphs complexity, scheduling types, and encoder-decoder architectures.
Figure 3: Multi-stage pipelined Edge TPUs inference runtime comparisons between the proposedRL methods and commercial Edge TPU compiler (baseline scale=1). The runtime performance hasbeen consistently improved over commercial compiler with 4, 5, and 6-stage pipelined Edge TPUsystem e.g., ResNet101v2 and ResNet152 execute 〜2.5× faster than Edge TPU compiler.
