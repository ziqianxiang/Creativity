Figure 2: Schematic description. Our Variational Perturbations (VP) method is based on training an explainerqθ of which the output is a distribution of feature attribution. The reconstruction loss Lrecon forces the explainerto provide a faithful feature attirbution while the regularization loss Lreg regularizes the explainer to follow aprior distribution. Since our goal is to explain a classifier’s prediction, we freeze the classifier in training.
Figure 3: Consistency over different resolution. We evaluate feature attribution results with different up-sampling size. Red (blue) colors represent higher (lower) attribution. We observe that our method highlightssimilar regions of the bird across different size of upsampling. However, for instance, nVP with upsample size8 captures a bird object while upsample size 4 highlights background as well as the object. This indicates thatthe variational training leads to more robust explanations.
Figure 4: Perturbation evaluation.
Figure 5: Qualitative results on CUB and ImageNet (top) and Sanity Check of VP on CUB (bottom). Red(blue) represents higher (lower) attribution. The first and the second rows are example attribution maps fromCUB dataset, the third and the fourth rows are from ImageNet dataset. In the fourth row (left) we show visualinspection for sanity check and (right) each x-tick label indicates a layer to which the classifier’s (ResNet50)weight is initialized from the top layer.
Figure 6: Time complexity. Time to infer afeature attribution in Quadro RTX 6000.
Figure 7: Rank matrix over all metrics. The scorein each cell of the rank matrix indicates the number oftimes the explanation method had a specific ranking.
Figure 8: Explanation discrepancy between trainingphase and test phase for L2X when k = 20%.
Figure 9: Sanity Check on CUB dataset.
Figure 10: Examples of feature attribution on MNIST dataset.
Figure 11: Examples of feature attribution on CUB dataset.
Figure 12: Examples of feature attribution on ImageNet dataset.
