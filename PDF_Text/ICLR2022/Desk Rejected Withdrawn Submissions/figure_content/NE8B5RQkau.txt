Figure 1: Self-Distilled Pruning w/ a Cross-Correlation Knowledge Distillation Loss.
Figure 2:	Iterative Pruning Test Performance on GLUE tasks.
Figure 3:	Zero-Shot Results After Iteratively Fine-Pruning XLM-RBase on XGLUE tasks.
Figure 4: Mutual Information Between Unpruned and Pruned Representations (left) andSignal-To-Noise Ratio (right) from PAWS-X Development Set Representations.
Figure 5: PAWS-X: Self-Distilled PruningLeads to Better Performance Recovery.
