Figure 1: Unfair representation leads to differ-ent optimization path and non-invariant opti-mal predictors on the latent space Z .
Figure 2: Explicit and Implicit path alignment. (a) The considered fair representation learningcriteria lies in ensuring the invariant optimal predictor w.r.t. different subgroups on Z (h?0 = h1?).
Figure 3: Sufficiency gap (∆SufR) inregression6Under review as a conference paper at ICLR 2022Method	Accuracy (↑)	△Sufc Q)ERM (I)	0.768 ± 0.004	0.173 ± 0.008Adv_debias (II)	0.760 ± 0.008	0.291 ± 0.006Mixup (III)	0.758 ± 0.003	0.343 ± 0.022IRM_v1 (IV)	0.753 ± 0.004	0.057 ± 0.015One_step (V)	0.755 ± 0.007	0.048 ± 0.008Implicit	0.760 ± 0.007	0.051 ± 0.012Table 1: Toxic comments dataset. Accuracy and∆SufC in different approaches.
Figure 4: Toxic. Accuracy-Fair Trade-offConcretely, for a given ti in each group, we compute the percentile (Y0) at point t: D0(Y0 ≤ ti),then we compute the corresponding ground truth cumulative distribution (Y ) at the same point ti :D(Y ≤ ti|Y ≤ ti). Through the aforementioned approximation, We can compute ∣Do(Y ≤ ti|Y ≤Ii)-DI(Y ≤ ti|Y ≤ ti)|.
Figure 6: Law. MSE-Fair Trade-off8Under review as a conference paper at ICLR 2022(a) ERM(b) Fair Mix-up(c) ImplicitFigure 7: Illustration of the sufficiency gap (∆SufR) in Law dataset (regression). The ERM and Fairmix-up suffer a high ∆SufR , while the proposed implicit alignment can significantly mitigate thesufficiency gap.
Figure 7: Illustration of the sufficiency gap (∆SufR) in Law dataset (regression). The ERM and Fairmix-up suffer a high ∆SufR , while the proposed implicit alignment can significantly mitigate thesufficiency gap.
Figure 8: NLSY. MSE-Fair Trade-off6.2.4	NLSY DatasetThe National Longitudinal Survey of Youth (NLSY, 2021) dataset is a regression task with around7K dataset, which involves the survey results of the U.S. Bureau of Labor Statistics. It is intended togather information on the labor market activities and other life events of several groups for predictingthe income y of each person. We treat the gender as the sensitive attribute. We also normalize theoutput y by diving the 10, 000, then the final output y ranges around [0, 8]. The prediction lossis also the square loss. We adopt representation λ as the two fully connected layers with hiddendimension 200 and Relu activation and predictor h as a linear predictor. We report the test-setsub-group average MSE (Mean Square Error) and Sufficiency Gap (∆SufR) in Tab. 4 and Fig. 8.
Figure 9: Computational time between T -step explicit and implicit approach in CelebA. Specifically,solver = 2 indicates the the conjugate gradient is executed 2 iterations. The results reveals thebenefits of implicit approach: avoiding the back-propagation through the inner-optimization path.
Figure 10: Gradient Norm evolution w.r.t. representation λ in Toxic comments dataset. We visualizeδthe norm of grad (λ) at each training epoch, which suggests a convergence behavior and the gradientfinally tends to zero.
Figure 11: Illustration of the sufficiency gap in NLSY dataset. The ERM and mix-up suffer thehigh predictive sufficiency-gap, while the proposed implicit alignment can significantly mitigate thesufficiency gap. In contrast, the probability calibration is not improved. This results also verifies theinequivalence between the sufficiency gap and calibration gap (Liu et al., 2019).
