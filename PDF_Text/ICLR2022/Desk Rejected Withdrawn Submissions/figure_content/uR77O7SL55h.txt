Figure 1: Overview of a typical workflow with an embedded Sinkhorn layer. We consider a neu-ral network whose input are e.g. images, 3D point clouds, voxel grids, surface meshes, etc. TheSinkhorn layer maps the cost matrix C and marginals a, b to the transportation plan P via itera-tive matrix scaling. In general, the whole network potentially contains learnable weights before andafter the Sinkhorn layer. To compute the gradients EC', Va', Vb') during training, instead of Us-ing naive automatic differentiation, we propose a novel customised backward pass which computesgradients efficiently and in closed form, see Algorithm 1.
Figure 2: Computational complexity. We compare the runtime per iteration (top row) and GPUmemory requirements (bottom row) of our approach (blue) and automatic differentiation (orange).
Figure 3: Wasserstein barycenter. A comparison between our method (top row) and automaticdifferentiation (bottom row) on the application of image barycenter computation. In each cell, weshow 5 centroids of 4 input images (corners) with bilinear interpolation weights. The predictionsof our approach are more stable, even for very few Sinkhorn iterations τ . Moreover, AD is out ofmemory for τ ≥ 200. Here, the input images have a resolution of n = 642 and we set λ = 0.002.
Figure 4: Number sorting. We show that we can improve the Gumbel-Sinkhorn method (Menaet al., 2018) directly with our Sinkhorn module. Specifically, we consider the task of permutationlearning to sort random number sequences of length n ∈ {200, 500, 1000}, see Mena et al. (2018,Sec 5.1) for more details. We replace AD in the GS network with our module (blue curves) andcompare the obtained results to the vanilla GS architecture (orange curves). Our approach yieldsmore accurate permutations while using much less computational resources - GS is out of memoryfor τ > 200, 100, 50 forward iterations, respectively. For all settings, we show the mean proportionof correct test set predictions (solid lines), as well as the 10 and 90 percentiles (filled areas).
Figure 5: Gradient accuracy. We empirically assess the accuracy of the error bound discussed inTheorem 5. Specifically, We show the accuracy of the gradients Va' for the image barycenter exper-iment in Sec. 5.2 and VC' for the number sorting experiment in Sec. 5.3. While both distributionshave a large overlap, the gradients from our approach are noticeably more accurate on average. Notethat both comparisons show histograms on a log scale.
Figure 6: Image barycenter gradients. A qualitative comparison of our gradients (3rd row), the ADgradients (4th row), and the ground truth gradients (last row) for the image barycenter experimentfrom Sec. 5.2. Specifically, we consider the task of interpolating between two input images (1st row)with uniform interpolation weights w1 = w2 = 0.5. We show intermediate snapshots of the obtainedbarycenter image (2nd row) for different numbers of gradient descent iterations t ∈ {0, . . . , 70} thatresult from minimizing the energy in Equation 15.
Figure 7: Manifold barycenter. We compute barycenters of two circular input distributions on thesurface of a sphere (first row). Specifically, we compare the results of minimizing Equation 15 withAD (second row) and our gradients (third row). The sphere is discretized as a triangular mesh with5000 vertices. On this resolution, AD is out of memory for τ ≥ 200 Sinkhorn iterations whereasours is still feasible for τ = 1000. The obtained interpolations produce the slightly elongated shapeof an ellipse since the surface of the sphere has a constant positive Gaussian curvature.
Figure 8: MNIST k-means clustering. A comparison of our approach and automatic differentiationon the task of k-means image clustering. For both approaches, we show the predicted k = 25 clustersfor τ ∈ {5, 10, 50, 100} Sinkhorn iterations. We choose more than 10 clusters to capture severaldifferent appearances and styles for each digit. To make individual results more comparable, weuse an identical random initialization of the cluster centroids for all settings. For AD, the maximumpermissible batch sizes for the 4 settings are 512, 256, 64, 32, whereas ours consistently allows for abatch size of 1024.
Figure 9: Point cloud registration. A qualitative comparison of RPM-Net (Yew & Lee, 2020)and the augmented version with our custom backward pass, see Alg. 1. The increased stability ofthe gradients predicted by our algorithm directly translates into more robust generalization results:Both methods are trained and tested on separate subsets of the 40 object categories in ModelNet40(Wu et al., 2015), see Yew & Lee (2020, Section 6) for more details. Both methods yield accuratepredictions for the clean test data, as indicated by the corresponding quantitative results in Table 1.
