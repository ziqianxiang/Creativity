Figure 1: Projective Manifold Gradient Layer. In the forward pass, the network predicts a rawoutput x, which is then transformed into a valid rotation R = φ(π(x)). We leave this forwardpass unchanged and only modify the backward pass. In the backward pass, we first use Riemannianoptimization to get a goal rotation Rg and map it back to Xg on the representation manifold M.
Figure 2: Projection point Xgp inthe case of quaternion.
Figure 3: Average L2 norm of the network raw output x during training. Left: PMG-6D (w/oreg. λ = 0). Right: RPMG-6D (w/ reg. λ = 0.01)et al., 2015). In the Table 1 and Table 2, we report our results on sofa and bicycle categories, giventhat (Levinson et al., 2020) only reported the detailed numbers for these two categories.
Figure 4: Illustration for regularized projective manifold gradient. First We project X to X byπ , and compute a Riemannian gradient, which is shown as the green arrow. After getting a nextgoal Xg ∈ M by Riemannian gradient, We find the inverse projection Xgp of Xg, which leads to ourprojective manifold gradient, shown as the blue arrow. With a regularization term, we can get ourfinal regularized projective manifold gradient, as the purple arroW.
