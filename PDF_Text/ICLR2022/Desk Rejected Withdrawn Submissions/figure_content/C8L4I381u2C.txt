Figure 1: The visually similar MsPacman and Bank Heist games.
Figure 2: The highly differentCrazy Climber and Pong games.
Figure 3: Image on the left: the four different versions of the Catch environment. In clockwise order:Catch-v0, Catch-v1, Catch-v3 and Catch-v2. Image on the right: learning curves obtained bya DQN agent that is trained from scratch on the aforementioned Catch versions. Shaded areascorrespond to Â±1 std. obtained over 5 different random seeds.
Figure 4: The results obtained after using a pre-trained Catch agent and fine-tuning it on a differentCatch version. We can observe that despite all Catch versions being very similar no positive trans-fer is ever observed, as a model trained from scratch always outperforms a pre-trained, fine-tunednetwork.
Figure 5: The results of our self-transfer experiments. From left to right the performance obtainedon CatchV-0, Catch-V1, Catch-V2 and Catch-V3 after either training only the last linear layer ofa pre-trained Deep-Q Network (dotted lines), or after wholly fine-tuning the model (dashed lines).
Figure 6: Image on the left: a visualization of differently initialized Deep-Q Networks before andafter training. Image on the right: a successful example of positive transfer.
Figure 7: The performance (in cyan) of a fine-tuned pre-trained network whose last layer is initial-ized with parameters that yielded positive transfer, whereas its convolutional and fully conntectedlayers are initialized with parameters that yielded negative transfer.
