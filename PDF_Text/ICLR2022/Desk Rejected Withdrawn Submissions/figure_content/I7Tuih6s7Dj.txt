Figure 1: We transform the logic of a model into an three-layer And-Or graph (AOG), and extractcommon coalitions of variables to build a deeper AOG, so as to further simplify the explanation.
Figure 2: (left) The output score of the model v(S) can be decomposed into the sum of marginalutilities of interactions between input variables in S, {I(L)|L ⊆ S}. (right) The interaction utilityI(S), |S| = m, is fairly assigned to the m variables in S in the computation of Shapley values.
Figure 3: Knowledge distillation cannot ensure the objectiveness of the student model, because thestudent model and the trained teacher model use different image regions to compute features. Thefirst row shows the Grad-CAM attention (Selvaraju et al., 2017) of the teacher model. The secondrow shows the Grad-CAM attention of the student model.
Figure 4: Examples of the AOG extracted from the CNN trained on (a) the CoLA dataset and (b) theSST-2 dataset, respectively. The red color of nodes in the second layer indicates interaction patternswith positive utilities, while the blue color represents patterns with negative utilities. Red edgesindicate the parse graph of an interaction pattern.
Figure 6: The histogramof the re-scaled interac-tion strength. The learnedbaseline values boostedthe sparsity of salient in-teraction patterns.
Figure 5: (a) The change of Rk (the ratio of the explained utilities)along with the number of salient patterns k in the AOG. (b, c) Thechange of the node number and the edge number in the AOG alongwith Rk . AOGs corresponding to adversarially trained models wereless complex than AOGs corresponding to normally trained models.
Figure 7:	The architecture of the ResMLP-5.
Figure 8:	The cosine similarity between the accurate Shapley value φ and the estimated Shapleyvalue φ, when we used different ratios of salient patterns for estimation.
Figure 9: The relationship betWeen Rk and the non-objectiveness ρnon-obj of the AOG explainer.
Figure 10: (a) The relationship between the number of salient patterns k in the AOG and the ratioof the explained utilities Rk, based on the census dataset. The relationship between Rk and (b) thenumber of nodes, and (c) the number of edges in the AOG, based on the census dataset.
Figure 11: (a) The relationship between the number of salient patterns k in the AOG and the ratioof the explained utilities Rk, based on the bike dataset. The relationship between Rk and (b) thenumber of nodes, and (c) the number of edges in the AOG, based on the bike dataset.
Figure 14:	Examples of AOGs extracted from models trained on the census dataset. Red edgesindicate the parse graph of a specific interaction pattern.
Figure 15:	Examples of AOGs extracted from models trained on the bike dataset. Red edges indicatethe parse graph of a specific interaction pattern.
Figure 16:	Examples of AOGs extracted from models trained on the commercial dataset. Red edgesindicate the parse graph of a specific interaction pattern.
Figure 17:	Examples of AOGs extracted from models trained on the SST-2 dataset. Red edgesindicate the parse graph of the most salient interaction pattern.
Figure 18:	Examples of AOGs extracted from models trained on the CoLA dataset. Red edgesindicate the parse graph of the most salient interaction pattern.
Figure 19: (1) The first column shows the relationship between the number of salient patterns k in theAOG and the ratio of the explained utilities Qk, based on different datasets. (2) The second columnshows the relationship between Qk and the non-objectiveness ρnon-obj of the AOG explainer, based ondifferent datasets. (3) The third column and the fourth column show the relationship between Qkand the node number, and the edge number in the AOG, respectively.
Figure 20: The number of patterns (the first column), nodes (the second column), and edges (thethird column) in the AOG, based on baseline values of different learning epochs. The learned base-line value significantly enhanced the conciseness of explanations.
