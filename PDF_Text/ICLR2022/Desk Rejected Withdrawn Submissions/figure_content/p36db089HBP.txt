Figure 1: Training stages of SONG that uses gradient descent to modify the graph structure andtransition probabilities. Based on an input x, the backbone neural network (NN) extracts a vectorrepresentation, which is passed to SONG to obtain a prediction for each class (y1 and y2). At thebeginning of training, a graph has root r, nodes v1 and v2, leaves l1 and l2 , and randomly initializededges (a). In the successive training iterations, the entropy of edge weights grows (b), finally resultingin a sparse binary graph, with two strong edges outgoing from each node (c). Notice that SONGcontains two alternative sets of edges between the nodes (dashed blue arrows and solid red arrows,respectively) that are combined based on the input (see Figure 2 for details).
Figure 2: SONG contains two alternative transition vectors m0i and m1i that aggregate the probabilityof moving from a particular node vi to all other nodes. In (a), they are represented as dashed blueand solid red arrows, respectively. Each node obtains input data x and makes a binary decision withprobabilities σi0 and σi1 of using one transition or another. As σi0 +σi1 = 1, SONG can be transformedto a standard directed graph by combining m0i and m∖, as presented in (b). During training, both σiand m∙i are trained to obtain the optimal decision graph as presented in Figure 1 of the paper.
Figure 3: Construction of the transition matrix and successive steps of our Markov process. On theleft, a graph with its matrices M0 and M1 is presented, followed by an exemplary decision vector σxand the resulting matrix Px. On the right, the flow in a graph is depicted for 3 consecutive steps. Atfirst, the probability is entirely placed in the root. However, in the next steps, the distribution splitsbetween nodes according to the transition probabilities, reaching leaves in step 3. The probabilitiesin the leaves after all steps are class probabilities inferred by the model (the number of steps isconsidered as a method hyperparameter).
Figure 4: Total probability in leaves in successive train-ing epochs for SONG trained on MNIST dataset. Eachcolor represents a different number of internal nodes (9,16, 32, and 64), and each line corresponds to mean andstandard deviation over multiple training repetitions.
Figure 5: Examples of the graph structures obtained by training SONG on the MNIST dataset. Theroot is the top-most node in each graph, and double node borders denote the leaves with numbers of theMNIST classes. For each node vi, we present two edges corresponding to the highest probability fromtwo transition vectors m0 and mʌ (represented as dashed blue and solid red arrows, respectively).
Figure 6: Visualisation of the graphs with edges corresponding to the transition matrices M0 (brownedges) and M 1 (green edges) of the SONG before and after training on the MNIST dataset with 9internal nodes. One can observe that SONG models binarize the connections during gradient training.
Figure 7: BCE loss as well as the number of backand cross edges in the successive training epochs ofSONG with 256 internal nodes and 10 steps trainedfor CIFAR100. One can observe that number of backedges decrease together with decreasing BCE loss.
Figure 8: Visualization of a shallow SONG (SONG-S) trained on MNIST where the nodes arerepresented by the learned filters and the “average” image passing through those nodes (correspondingto the right and left side of each node, respectively). Notice that SONGs contain filters only in theinner nodes, as it is impossible to move out from the leaves.
Figure 9: BCE loss as well as the number of back and cross edges in the successive training epochsof SONG with 64 internal nodes and 10 steps trained for MNIST. One can observe that number ofback edges decrease together with decreasing BCE loss.
Figure 10: Examples of the graph structures obtained by training SONG on the MNIST dataset. Theroot is the top-most node in each graph, and the leaves are denoted by double node borders. Thenumbers on the leaves are the MNIST classes. For each node vi , we present two edges correspondingto the highest probability from two transition vectors m0 and mʌ (represented as dashed blue andsolid red arrows, respectively).
Figure 11: Examples of the graph structures obtained by training SONG on the CIFAR10 dataset.
Figure 12: An input image passing through a SONG trained on CIFAR10. High saturation of the greencolor denotes high probability in the node. Each graph represent a consecutive step of the inference(from left to right, then top to bottom). For each node vi , we present two edges corresponding to thehighest probability from two transition vectors m0 and m1 (represented as dashed blue and solid redarrows, respectively).
Figure 13: An input image passing through a SONG trained on CIFAR10. High saturation of the greencolor denotes high probability in the node. Each graph represent a consecutive step of the inference(from left to right, then top to bottom). For each node vi, we present two edges corresponding to thehighest probability from two transition vectors m0 and m'i (represented as dashed blue and solid redarrows, respectively).
Figure 14: An input image passing through a SONG trained on CIFAR10. High saturation of the greencolor denotes high probability in the node. Each graph represent a consecutive step of the inference(from left to right, then top to bottom). For each node vi , we present two edges corresponding to thehighest probability from two transition vectors m0 and mʌ (represented as dashed blue and solid redarrows, respectively).
Figure 15: An input image passing through a SONG trained on CIFAR10. High saturation of the greencolor denotes high probability in the node. Each graph represent a consecutive step of the inference(from left to right, then top to bottom). For each node vi, we present two edges corresponding to thehighest probability from two transition vectors m0 and mʌ (represented as dashed blue and solid redarrows, respectively).
Figure 16: Simplified example of a SONG graph introduced to draw an intuition about graphbinarization property.
Figure 17: Accuracy, BCE loss, and Lleave in the successive training epochs of SONG trained on theMNIST dataset. Each color represents a different number of internal nodes (64, 128, 255), and eachline corresponds to mean and standard deviation over multiple training repetitions.
Figure 18: Nodes and edges statistics calculated for SONGs trained on the MNIST dataset. For eachcombination of the number of internal nodes and steps, 20 graphs are trained and used to plot thedistributions of four statistics. One can observe a significant difference in SONG structure dependingon those hyperparameters.
Figure 19: Nodes and edges statistics calculated for SONGs trained on the CIFAR10 dataset. Foreach combination of the number of internal nodes and steps, 20 graphs are trained and used to plotthe distributions of four statistics.
Figure 20: Sample matrices M0 and M1 of the SONG before and after training on the MNIST datasetwith 9 internal nodes. One can observe that SONG models binarize the connections during gradienttraining. Graphs corresponding to presented transition matrices are visualized in Figure 6.
Figure 21: Sample matrices M0 and M1 of the SONG before and after training on the MNIST datasetwith 16 internal nodes.
Figure 22: Sample matrices M0 and M1 of the SONG before and after training on the MNIST datasetwith 32 internal nodes.
Figure 23: Sample matrices M0 and M1 of the SONG before and after training on the MNIST datasetwith 64 internal nodes.
Figure 24: Sample matrices M0 and M1 of the SONG before and after training on the MNIST datasetwith 128 internal nodes.
Figure 25: Sample matrices M0 and M1 of the SONG before and after training on the MNIST datasetwith 256 internal nodes.
Figure 26: Mean distances between transition matrices Px for pairs of MNIST input samplesrepresented by a distance matrix (the larger distance, the brighter color). The rows and columnscorrespond to 0-9 digits.
