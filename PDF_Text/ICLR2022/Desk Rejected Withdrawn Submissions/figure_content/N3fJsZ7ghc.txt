Figure 1: Overview of the proposed encryption framework. A set of confusion neurons are addedinto the original network to obtain a larger network, which is referred to as the encrypted network.
Figure 2: Visualization of the attention maps (plotted by the CAM algorithm (Zhou et al., 2016))of ResNet50 after different amounts of confusion neurons are inserted. The original ResNet50 wastrained on ImageNet. One can see that adding confusion neurons destroys the pre-trained model’sability to focus on the discriminative region of the image - with the amount of confusion neuronsincrease, the attention maps become diverged and thus the classification accuracy drops dramatically.
Figure 3:	The impact of adding confusion neurons to different blocks of ResNet18/50. The top roWshoWs the setting that the same (relative) proportion of neurons are added to in each block, Whilethe bottom roW shoWs that the same (absolute) number of neurons are added. Please mind the slightdifference betWeen the ‘samp’ and ‘init’ strategies. All tests are made on ImageNet.
Figure 4:	The impact of adding different amounts of confusion neurons to the ResNet series. Alltests are made on ImageNet. The left two plots show the difference between the ‘samp’ and ‘init’strategies where the latter is more effective, and the right two plots show the additional overheadsrequired, which is linear to the confusion proportion.
Figure 5: The impact of adding different amounts of confusion neurons to the ResNet series. Thetests are made on MS-COCO object detection and instance segmentation tasks.
