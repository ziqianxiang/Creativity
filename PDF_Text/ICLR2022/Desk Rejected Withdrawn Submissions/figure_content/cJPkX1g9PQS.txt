Figure 1: Our coherence model with the auxiliary momentum encoder. φ is our base encoder similar to oursetup in §3.3, while φ0 is our momentum encoder. u+ = fθ(D+) and u- = fθ(D-) are the coherence scoresof the positive and negative documents respectively. Note that only the parameters of φ and the linear layer areupdated through backpropagation.
Figure 2: (a) A plot of the development accuracy during training our contrastive model with and without hardnegative mining, and our complete model with hard negative mining. The accuracies are evaluated after every1000 gradient steps. (b) Results on the various test sets for our model trained with hard negative mining bysampling different number of negatives (h) for ranking. (c) Results on the various test sets for our completemodel trained With different momentum coefficient (μ) values. (d) Results on the various test sets for our modeltrained with different global queue Q sizes. Please note that the agreement values for LMVLM test set havebeen scaled by a factor of 100 to facilitate visualization in figures (b), (c) and (d).
Figure 3: Instructions and study interface for the user study conducted on language model outputs.
