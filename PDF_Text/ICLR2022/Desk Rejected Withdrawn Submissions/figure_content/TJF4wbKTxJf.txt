Figure 1: Comparison with 256 input channels and 128 output channels among (a) depth-wiseseparable convolution, (b) group convolution, and (c) our channel-split recurrent convolution (CSR-Conv) using vanilla RNNs. The linear convolutional operation is denoted as (#input channels, filtersize, #output channels) and vertical small rectangles in (c) denote ReLU activation functions.
Figure 2: General architecture.
Figure 3: Comparison of error vs. compression rate on (a-e) CIFAR-10 and (f) ImageNet.
Figure 4: VGG-16 error (dot size) on CIFAR-10. Each curve indicates similar œÅM values.
Figure 5: Training loss comparison using theVGG-16 backbone on CIFAR-10.
