Figure 1: Overview of generative kernel continual learning. The variational auto-encoder isadopted as a generative model, which learns the data distribution of task t while doing replay over thedata R<t from previous tasks to avoid catastrophic forgetting. The trained decoder of the generativemodel is deployed to generate the coreset Ct. The kernel Kt for the current task t is constructed basedon the generated Ct and Dt . The classifier for task t is constructed based on kernel ridge regressionusing Kt . The encoder network qφt , parameterized by φ, is shared and updated when training onthe task sequence. The decoder network pθ , parameterized with θ, adopts a gating mechanism thatavoids task interference and minimizes forgetting. The kernel network fγt, parameterized by γ, infersa task-specific kernel, which is shared among tasks and trained end-to-end. The shaded network qφtis the trained encoder network from the generative model, which functions as a feature extractor toproduce internal representations for kernel learning.
Figure 2: Generative learning by a variational auto-encoder with supervised contrastive reg-ularization. The internal representations from the intermediate layers of the encoder are used tokernel learning. A projection network fp(∙) is applied to the internal representations, which are fedinto the supervised contrastive loss function for discriminative sample generation. In the decodernetwork, as in (van de Ven et al., 2020), We adopt a gating function G(∙) in red blocks to avoid taskinterference. Black and red arrows show the forward and backward path respectively.
Figure 3: Generative core-set vs. uniform coresetin kernel continual learningon SplitCIFAR100. Gener-ative kernel continual learn-ing needs about 10 timessmaller coreset size for sameaccuracy, highlighting the ex-pressiveness of the generatedcoreset.
Figure 4: Varying coresetsize at inference time. Aver-age accuracy over 20 sequen-tial tasks on SplitCIFAR100when the coreset size differsbetween training (legend) andinference time. Increasingthe coreset size during infer-ence increases average accu-racy considerably.
Figure 5: Comparison with state-of-the-art over 20 consecutive tasks, in terms of average accu-racy. Our model consistently outperforms alternatives on all three benchmarks, especially on themore challenging SplitCFIAR100.
Figure 6:	Coreset internal representation. In this figure, we visualize the internal representationof the encoder network of conditional variational auto-encoder of the first task when we observe 20tasks with (w/) and without (w/o) the contrastive regularization term. As shown, the regularizationterm allows our proposed method to obtain better concentrated features.
Figure 7:	Test dataset internal representation. In this figure, we visualize the internal repre-sentation of the encoder network of conditional variational auto-encoder of the first task when weobserve 20 tasks with (w/) and without (w/o) the contrastive regularization term. As shown, theregularization term allows our proposed method to obtain better concentrated features.
