Figure 1: GCM flowchart for an incoming observation ot . GCM places ot as a node in a graph, and computesits neighborhood N (ot), and then updates the edge set. Task-specific topological priors are defined via theneighborhood. A convolutional GNN queries the graph for belief state bt . A policy π uses the belief fordecision making. Compared to transformers or DNCs, GCM is architecturally simple.
Figure 2: The two-layer 1-GNN used in all our experiments. Colors denote mixing of vertex informationand dashed lines denote directed edges, forming neighborhoods N (ot), N (oi). The current observation otand aggregated neighboring observations oi, oj pass through fully-connected layers (W11, b1), (W21) beforesummation and nonlinearity σ, resulting in the first hidden state zt1 (Eq. 3). We repeat this process at oj , ok , olto form hidden states zj1 , zk1 , zl1 . The second layer combines embeddings of the first layer and the second-layerhidden state zt2 is output as the belief state bt . Additional layers increase the GNN receptive field.
Figure 3: Generating one to K edges in a non-deterministic manner, using our learned topological prior. Wepresent an example of the learned prior with K = 3 at t = 5. An MLP computes logits over previous verticesto produce three distributions Xk; k ∈ {1, 2, 3}, perturbed with Gumbel noise gik. We compute the MAP foreach Xk, resulting in three samples which form a two-edge neighborhood N(o5) containing vertices o1, o3.
Figure 4: (a) The number of trainable pa-rameters per memory model, based on thehidden size |z |. GCM uses much fewer pa-rameters than other memory models. (b) Themeaning of |z | with respect to each memorymodel, as used in all our experiments.
Figure 5: Visualizations of our experiments. (a) The classic cartpole control problem, but where r, θ are hidden.
Figure 6: Stateless cartpole, where the agent must derive velocity from past observations. OpenAI considersfully-observable cartpole solved at a reward of 195 (dashed red line), which only GCM can reliably reach inpartially-observable cartpole. Results represent the mean and 90% confidence interval over three trials.
Figure 7: Results from concentration with hidden size |z| = 32, where n is the number of cards. This teststhe agent’s long-term non-sequential memory. The agent receives a small reward for matching a pair of cards,receiving a cumulative reward of one for matching all pairs. Episode lengths are 50, 75, and 100 respectively.
Figure 8: We compare various GCM priors across hidden sizes |z | for the navigation problem. Since navigationis a spatial problem, the spatial prior performs best. This shows the importance of selecting good priors. Resultsare averaged over three trials and the shaded area represents the 90% confidence interval.
Figure 9: We compare GCM to other memory baselines for the navigation problem. |z | denotes the hidden sizeused across all models. Results are averaged over three trials and the shaded area represents the 90% confidenceinterval.
Figure 10: What do GCML neighborhoods look like for stateless cartpole? At each episodic timestep t > 20,we record the neighborhood N (ot) GCML produces relative to t. We plot the accumulation of all neighbor-hoods over an episode, using |z| = 32, K = 5. We see that GCML learns a temporal prior, where each timestepuses observations from 7 to 10 timesteps ago. Surprisingly, GCML does not use the preceding observation, sug-gesting that the simulation contains high-frequency variations which prevent an accurate estimation of velocitybetween consecutive timesteps. Perhaps a human-defined prior with {t - 7} would produce a smoothed signal,leading to better performance than our {t - 1, t - 2} prior. With K = 5, the maximum possible neighborhoodsize is 5. The mean neighborhood size is 1.54, demonstrating that GCML can learn a peaked distribution,resulting in sparse graphs.
Figure 11: What do GCML navigation neighborhoods look like? At each episodic timestep, we record theneighborhood N (ot) GCML produces. We plot the accumulation of all neighborhoods over an episode, using|z| = 32, K = 5. Unlike the Fig. 10, vertices here are labeled in an absolute fashion (when they occurred).
Figure 12: What do GCML navigation neighborhoods look like in the spatial domain? (a) At each episodictimestep, we record the neighborhood N(ot) GCML produces. We plot the accumulation of all neighborhoodsover an episode, using |z| = 32, K = 5. We bin each vertex in the neighborhood N (ot) by its distance to thevertex ot . We see GCML looks to be spatially-biased, heavily prioritizing nearby vertices over further vertices.
Figure 13: (a) A visual representation of the memory graph from the episode used in Fig. 11 and Fig. 12.
