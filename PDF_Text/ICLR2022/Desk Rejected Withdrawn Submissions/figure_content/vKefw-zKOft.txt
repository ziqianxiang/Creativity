Figure 1: In this work, quantum neural network training and inference are both performed on realquantum machines, making the whole pipeline scalable and practical.
Figure 2: (a) Classical simulation has unscalable computational and memory costs. (b) Noisescreate significant accuracy gaps between QNN classical simulation and on-chip training. (c) Smallgradients suffer from larger relative errors, thus being less reliable.
Figure 3: Quantum neural network architecture.
Figure 5: Efficient on-chip quantum gradient calculation with probabilistic gradient pruning. Gra-dient magnitudes are accumulated within the accumulation window and used as the sampling distri-bution. Based on the distribution, gradients are probabilistically pruned with a ratio r in the pruningwindow to mitigate noises and stabilize training.
Figure 6:	Real QC validation accuracy curves on different datasets and different quantum devices.
Figure 7:	Ablation on pruning ratio, accumulation window width, and pruning window width.
Figure 8: Runtime (s) and memory cost comparison be-tween classical simulation and quantum on-chip run.
