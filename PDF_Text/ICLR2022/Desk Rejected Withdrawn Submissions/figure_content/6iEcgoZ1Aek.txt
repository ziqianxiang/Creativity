Figure 1: (a) is the standard Transformer block at t-th layer. (b) is the proposed T layers Crossformer’sself-attention (left) block and feed-forward block (right).
Figure 2: Comparison of different cross-layer guidance schemes. Crossformer combines (c) and (d) butdecomposes parameters in each layer into private and public parts so only the latter are shared across Crossformer.
Figure 3: Scaling up Crossformer. The performance of Crossformer improves with an increase in (1) thenumber of encoder layers and (2) the number of parameters on the WMT’14 En-De corpus. The Crossformerconsistently outperform the Transformer indicating the generability and scalibility of Crossformer and thecomplementary of Crossformer to different model designs.
