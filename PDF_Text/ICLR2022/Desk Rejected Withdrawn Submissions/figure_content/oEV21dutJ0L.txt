Figure 1: Our Framework Overview: we use a shared encoder for RL and joint self-supervisedlearning. An observation is augmented in two ways; Augw uses Random-Shift only, and Augs usesRandom-Shift and other randomly chosen augmentation method. The encoded latent state zw isused to train an RL algorithm, and both zw and zs are passed to joint self-supervised learning whichconsists of Forward Dynamics, Inverse Dynamics, and Adversarial Representation.
Figure 3: (a) Adversarial Representation learns invariant features between weak and strong aug-mented views of the same observation. (b) Inverse Dynamics predicts an action at given the currentlatent state Zt and the next latent state zt+1. With two-way augmentations, We can predict ^W and^S respectively. Forward Dynamics predicts the next latent state Zt+1 given the current latent statezt and the action at predicted by Inverse Dynamics. We cross-enter the predicted actions ^W and ^Sinto Forward Dynamics.
Figure 4: We use three different backgroundtypes. There are examples on a Cheetahtask in the Deepmind Control suite; Default(left), Simple Distractor (center), and Natu-ral Video (right)eralization. Each RL method is trained for 500K en-vironment steps, and every 5,000 steps, we tested thecurrently trained model by calculating the average return for 10 episodes. We trained each RLmethod over three different seeds. 1 As shown in Table 1, JS2RL shows performance similar to thebest performance of the prior works in data efficiency experiments, and significantly outperformsthe prior works in generalization experiments. More experiment details are in Appendix F.
Figure 5: Data efficiency comparisons on three tasks in the DeepMind Control suite; Walker (leftcolumns), Cheetah (center columns), and Hopper (right column). All RL methods are trained andevaluated on the same Simple Distractor backgrounds. We show the learning curves for each RLmethod on three different seeds with 1.0 standard error shaded.
Figure 6: Generalization comparisons on three tasks in the DeepMind Control suite; Walker (left),Cheetah (center), and Hopper (right). Each task is trained on Simple Distractor backgrounds, andthen is evaluated on Natural Video backgrounds. We show a comparison of our algorithm andbaselines on three different seeds with 1.0 standard error shaded.
Figure 7: t-SNE of representations learned by JS2RL, DBC, SODA and PAD. Even if the back-ground is dramatically different, JS2RL can encode behaviorally-equivalent observations (blue,brown, pink, olive, sky blue) to be most closely located.
Figure 8: (left) Ablation studies for JS2RL where 2W stands for our proposed 2-way data aug-mentations, and 1W stands for 1-way data augmentation using a single encoder and Random-shiftonly. (center) Using multiple strong augmentation techniques together shows better performancethan using a single strong augmentation. (right) Our framework can replace Adversarial Represen-tation with other self-supervised learning method such as Reconstruction or Contrastive methods.
Figure 9: (a) Scenes in CARLA simulations classified as Highway (left column), Town (centercolumn) and Bridge (right column). Each column is captured in the same spot but contains differenttask-irrelevant information such as the Sun, rain, shadows, clouds, etc. (b) Performance comparisonin the autonomous driving environment CARLA. JS2RL outperforms all other RL methods.
Figure 10: Results of data efficiency evaluation for JS2RL and baselines on Default background.
Figure 11: Results of data efficiency evaluation for JS2RL and baselines on Simple Distractor back-ground. We show the learning curves of each tasks on three different seeds with 1.0 standard errorshaded.
Figure 12: Results of data efficiency evaluation for JS2RL and baselines on Natural Video back-ground. We show the learning curves of each tasks on three different seeds with 1.0 standard errorshaded.
Figure 13: Results of generalization evaluation for JS2RL and baselines. We show the learningcurves of each tasks on three different seeds with 1.0 standard error shaded.
Figure 14: Ablation studies for JS2RL, where 2W stands for our two-way data augmentations and1W stands for one-way augmentation (Random-shift only). We show generalization performancecomparison of each ablation case on three different seeds with 1.0 standard error shaded.
Figure 15: Data efficiency comparisons for each background on Walker Walk in the DeepMindControl suite; Default (top left), Simple Distractor (top right) and Natural Video (bottom left). Gen-eralization performance (bottom right) on Walker Walk. We show the learning curves of each ex-periments on three different seeds with 1.0 standard error shaded.
Figure 16: Comparison of cross-entered version (blue) and uncross-entered version (brown) onWalker Walk (left), Cheetah Run (center) and Hopper Hop (right). Cross-entering the predictedactions into Forward Dynamics has a positive effect on performance. We show results on threedifferent seeds with 1.0 standard error shaded.
Figure 17: (left) Data Efficiency evaluation, (right) Generalization evaluation on the DeepMindControl suite (Walker Walk). TD3 with JS2RL (blue), standard TD3 (brown), PPO with JS2RL(red), standard PPO (orange). We show the learning curves of each experiments on three differentseeds with 1.0 standard error shaded.
Figure 18: Hyper-parameter sensitivity of JS2RLâ€™s objective weights. The value corresponding toeach point is the average return over three seeds.
Figure 19: Learning curves with 1M environment steps (brown - Data Efficiency, blue - Generaliza-tion). We show the learning curves of each experiments on three different seeds with 1.0 standarderror shaded.
