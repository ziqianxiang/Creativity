Figure 1: Example of a 3-layers MLP fed with preprocessed inputs. The state s is preprocessedby a feature function ζ(.) (ζ (s) = FF(s) for Fourier Features and ζ(s) = s for raw inputs) beforebeing passed into the MLP with the action a. Learned features φ(s, a) are used for the prediction ofthe Q-value Q(s, a).
Figure 2: The use of Fourier features improves performance and sample efficiency of DQNon discrete control tasks. Evaluation learning curves of NN (blue), FF-NN (orange), and FLF-NN (green), reporting episodic return versus environment timesteps. Results are averaged over 30trainings (different seeds), with shading indicating standard deviation.
Figure 3: The use of Fourier Light features improves the performance and sample efficiency ofPPO on a continuous control task. Evaluation learning curves of NN (blue) and FLF-NN (green),reporting episodic return versus environment timesteps. Results are averaged over 10 trainings withshading indicating the standard deviation.
Figure 4: Fourier Features are more robust to learning rate, buffer size and target updatefrequency. Cumulative reward over different hyperparameter variations, for NN (blue) and FF-NN(orange) on MountainCar-v0 and CartPole-v1. Results are averaged over n trainings and shadingindicating the 95% confidence interval (CI).
Figure 5: The use of Fourier features, and Fourier Light features enhance the expressivenessof the learned features on discrete control tasks. Learning curves of the normalized effective ranksrankδ(Φ) for NN fed with raw inputs (blue), Fourier features (orange), and Fourier Light features(green), averaged over 30 trainings with shading indicating the 95% CI.
Figure 6: Preprocessing inputs with Fourier features or Fourier Light features improve thesmoothness of the Neural Network. Lower and upper bounds on the Lipschitz constant of NNover training timesteps, for NN fed with raw inputs (blue), FF (orange), and FLF (green). Boundsare averaged over 30 trainings. A lower score is better.
Figure 7: Fourier Features/Fourier Light features outperform other standard input prepro-cessings on discrete control tasks. Evaluation learning curves of NN (blue), FF-NN (orange),FLF-NN (green), PF-NN (red), RFF-NN (purple) and TC-NN (brown) reporting episodic returnversus environment timesteps. Results are averaged over 30 trainings with shading indicating thestandard deviation.
Figure 8: Learning curves of the normalized effective rank srankδ (Φ) for NN fed with raw inputs(blue), Fourier features (orange), Fourier Light features (green), Polynomial features (red), RandomFourier Features (purple) and Tile Coding features (brown) on discrete control tasks. Results areaveraged over 30 training with the shade that indicates the 95% CI.
Figure 9: Lower and upper bounds on the Lipschitz constant of NN over training timesteps, of NN(blue), FF-NN (orange), FLF-NN (green), PF-NN (red), RFF-NN (purple), and TC-NN (brown).
Figure 10: Evaluation learning curves of NN (blue) and FLF-NN (green) with DQN on discretecontrol tasks, reporting episodic return versus environment timesteps. Results are averaged over 10trainings (different seeds), with shading indicating standard deviation.
Figure 11: Learning curves of NN (blue) and FLF-NN (green) with DQN on Catcher-v1, reportingepisodic return versus environment timesteps. Results are averaged over 30 trainings (differentseeds), with shading indicating standard deviation.
Figure 12: Fourier Features are more robust to buffer size and target update frequency. Cu-mulative reward over different buffer size values (Figure 12a) and target update frequencies (Figure12b) for NN (blue) and FF-NN (orange) on MountainCar-v0 and CartPole-v1. Results are averagedover 5 trainings with shading indicating the 95% CI.
Figure 13: Learning curves of the normalized overlap for NN (blue), FF-NN (orange), and FLF-NN (green) with DQN on discrete control tasks, reporting normalized overlap versus environmenttimesteps. Results are averaged over 30 trainings (different seeds), with shading indicating the 95%CI.
Figure 14: Learning curves of the normalized effective rank srankδ (Φ) for NN fed with raw inputs(blue), Fourier features (orange), and Fourier Light features (green), averaged over 30 trainings withshading indicating the 95% CI.
Figure 15: Learning curves of the Average Stiffness (AS) for NN fed with raw inputs (blue), Fourierfeatures (orange), and Fourier Light features (green). Results are averaged over 30 trainings withshading indicating the 95% CI.
Figure 16: Learning curves of the Average Interference (AI) for NN fed with raw inputs (blue),Fourier features (orange), and Fourier Light features (green). Results are averaged over 30 trainingswith shading indicating the 95% CI.
Figure 17: Learning curves of the Interference Risk (IR) for NN fed with raw inputs (blue), Fourierfeatures (orange), and Fourier Light features (green). Results are averaged over 30 trainings withshading indicating the 95% CI.
Figure 18: Learning curves of the Percentage of Interference (PercI) for NN fed with raw inputs(blue), Fourier features (orange), and Fourier Light features (green). results are averaged over 30trainings with shading indicating the 95% CI.
Figure 19: Learning curves of the lower and upper bounds of the Lipschitz constant of NN for NNfed with raw inputs (blue), Fourier features (orange), and Fourier Light features (green), averagedover 30 trainings with shading indicating the 95% CI.
Figure 20: Learning curves of the l2 weight norm for NN fed with raw inputs (blue), Fourier features(orange), and Fourier Light features (green), averaged over 30 trainings with shading indicating the95% CI.
Figure 21: Learning curves of the l1 weight norm for NN fed with raw inputs (blue), Fourier features(orange), and Fourier Light features (green), averaged over 30 trainings with shading indicating the95% CI.
Figure 22: Learning curves of the linf weight norm for NN fed with raw inputs (blue), Fourier fea-tures (orange), and Fourier Light features (green), averaged over 30 trainings with shading indicatingthe 95% CI.
