title,year,conference
 An optimistic perspective on offlinereinforcement learning,2020, In International Conference on Machine Learning
 Layer normalization,2016, arXiv preprintarXiv:1607
 The arcade learning environ-ment: An evaluation platform for general agents,2013, Journal of Artificial Intelligence Research
 Generative pretraining from pixels,2020, 2020
 Long short-term memory-networks for machinereading,2016, arXiv preprint arXiv:1601
 Bert: Pre-training of deepbidirectional transformers for language understanding,2018, arXiv preprint arXiv:1810
 An im-age is worth 16x16 words: Transformers for image recognition at scale,2020, In International Confer-ence on Learning Representations
 D4rl: Datasets for deepdata-driven reinforcement learning,2020, arXiv preprint arXiv:2004
 Reformer: The efficient transformer,2019, InInternational Conference on Learning Representations
 Actor-critic algorithms,2000, In Advances in neural informationprocessing systems
 A structured self-attentive sentence embedding,2017, arXiv preprintarXiv:1703
 Human-levelcontrol through deep reinforcement learning,2015, nature
 A decomposable attentionmodel for natural language inference,2016, arXiv preprint arXiv:1606
 Improving language under-standing by generative pre-training,2018, 2018
 Languagemodels are unsupervised multitask learners,2019, 2019
 Temporal difference learning and td-gammon,1995, Communications of the ACM
 Training data-efficient image transformers & distillation through attention,2021, InInternational Conference on Machine Learning
 Goingdeeper with image transformers,2021, arXiv preprint arXiv:2103
 Attention is all you need,2017, In Advances in neural informationprocessing systems
 Linformer: Self-attentionwith linear complexity,2020, arXiv preprint arXiv:2006
 Focal self-attention for local-global interactions in vision transformers,2021, arXiv preprintarXiv:2107
