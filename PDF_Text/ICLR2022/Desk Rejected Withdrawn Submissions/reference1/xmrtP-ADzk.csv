title,year,conference
 Compress: Self-supervised learning by compressing representations,2020, In NeurIPS
 Bats: Binary architecture search,2020, InECCV
 Exponential moving average normalization for self-supervised and semi-supervised learn-ing,2021, In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition(CVPR)
 A simple framework forcontrastive learning of visual representations,2020, In ICML
 Big self-supervised models are strong semi-supervised learners,2020, In NeurIPS
 Understanding gradient clipping in private sgd: ageometric perspective,2020, Advances in Neural Information Processing Systems
 De-biased contrastive learning,2020, In NeurIPS
 Whitening for self-supervised representation learning,2021, In International Conference on Machine Learning
 LIBLINEAR: A library for largelinear classification,2008, JMLR
 Scaling and benchmarking self-supervised visual representation learning,2019, In ICCV
 Self-supervised pretraining ofvisual features in the wild,2021, arXiv preprint arXiv:2103
 Bootstrap your ownlatent: A new approach to self-supervised learning,2020, In NeurIPS
 Training binary neuralnetworks through learning with noisy supervision,2020, In ICML
 Momentum contrast forunsupervised visual representation learning,2020, In CVPR
 Hardnegative mixing for contrastive learning,2020, In NeurIPS
 Binaryduo: Reducing gradientmismatch in binary activation network by coupling binary activations,2020, In ICLR
 Imagenet classification with deep convo-lutional neural networks,2012, In NeurIPS
 i-mix: Astrategy for regularizing contrastive representation learning,2021, In ICLR
 Dual-stream multiple instance learning network forwhole slide image classification with self-supervised contrastive learning,2021, In Proceedings of theIEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)
 Prototypical contrastive learning of unsuper-vised representations,2021, In ICLR
 Rotated binary neural network,2020, In NeurIPS
 Towards accurate binary convolutional neural network,2017, InNeurIPS
 Bi-real net:Enhancing the performance of 1-bit cnns with improved representational capability and advancedtraining algorithm,2018, In ECCV
 Reactnet: Towards precisebinary neural network with generalized activation functions,2020, In ECCV
 Training binary neuralnetworks with real-to-binary convolutions,2020, In ICLR
 Training binary neural net-works using the bayesian learning rule,2020, In ICML
 Representation learning with contrastive predic-tive coding,2018, arXiv preprint arXiv:1807
 Forward and backward information retention for accurate binary neural networks,2020, In CVPR
 Xnor-net: Imagenetclassification using binary convolutional neural networks,2016, In ECCV
 S2-bnn:Bridging the gap between self-supervised real and 1-bit neural networks via guided distributioncalibration,2021, In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recog-nition
 Contrastive multiview coding,2020, In ECCV
 Divide and contrast: Self-supervisedlearning from uncurated data,2021, arXiv preprint arXiv:2105
 Understanding self-supervised learning dynamicswithout contrastive pairs,2021, ICML
 Understanding contrastive representation learning through align-ment and uniformity on the hypersphere,2020, In ICML
 Bidet: An efficient binarized object detector,2020, InCVPR
 Unsupervised feature learning via non-parametric instance discrimination,2018, In CVPR
 What should not be contrastive incontrastive learning,2021, In ICLR
 Large batch training of convolutional networks,2017, arXivpreprint arXiv:1708
 Barlow twins: Self-supervisedlearning via redundancy reduction,2021,2021
 Why gradient clipping acceleratestraining: A theoretical justification for adaptivity,2019, arXiv preprint arXiv:1905
