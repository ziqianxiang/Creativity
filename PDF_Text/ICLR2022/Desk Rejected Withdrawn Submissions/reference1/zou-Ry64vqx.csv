title,year,conference
 Tensorflow: A system for large-scale machine learning,2016, In 12th {USENIX} symposium on operating systems design and imple-mentation ({OSDI} 16)
 A theory of learning from different domains,2010, Machine learning
 Is pruning compression?: Investigating pruning vianetwork layer similarity,2020, In Proceedings of the IEEE/CVF Winter Conference on Applications ofComputer Vision
 Towards federated learning at scale: System design,2019, arXivpreprint arXiv:1902
 Expanding the reachof federated learning by reducing client resource requirements,2018, arXiv preprint arXiv:1812
 Fine-grained stochastic architecture search,2020, arXiv preprint arXiv:2006
 Cross-layer distillation with semantic calibration,2021, In Proceedings of the AAAI Conference on ArtificialIntelligence
 Emnist: Extending mnistto handwritten letters,2017, In 2017 International Joint Conference on Neural Networks (IJCNN)
 Group equivariant convolutional networks,2016, In International confer-ence on machine learning
 Binarizedneural networks: Training deep neural networks with weights and activations constrained to+ 1or-1,2016, arXiv preprint arXiv:1602
 Exploiting linearstructure within convolutional networks for efficient evaluation,2014, In Proceedings of the 27th In-ternational Conference on Neural Information Processing Systems - Volume 1
 Neural architecture search: A survey,2019, TheJournal of Machine Learning Research
 Residual error based knowledge distillation,2021, Neuro-computing
 Distilling the knowledge in a neural network,2015, arXivpreprint arXiv:1503
 Improving neural networks by preventing co-adaptation of feature detectors,2012, arXiv preprintarXiv:1207
 Quantizedneural networks: Training neural networks with low precision weights and activations,2017, The Jour-nal of Machine Learning Research
 Categorical reparameterization with gumbel-softmax,2016, arXivpreprint arXiv:1611
 Advances and open problems in federated learning,2019, arXivpreprint arXiv:1912
 Convolutional deep belief networks on cifar-10,2010, Unpublishedmanuscript
 Ditto: Fair and robust federatedlearning through personalization,2021, In International Conference on Machine Learning
 Deep gradient compression: Re-ducing the communication bandwidth for distributed training,2017, arXiv preprint arXiv:1712
 Neural architecture search withouttraining,2021, In International Conference on Machine Learning
 Improved knowledge distillation via teacher assistant,2020, Proceedings of the AAAIConference on Artificial Intelligence
 When a good fit can be bad,2002, Trends in cognitive sciences
 Very deep convolutional networks for large-scale imagerecognition,2014, arXiv preprint arXiv:1409
 Attentivenas: Improving neural archi-tecture search via attentive sampling,2021, In Proceedings of the IEEE/CVF Conference on ComputerVision and Pattern Recognition (CVPR)
 Gradient sparsification for communication-efficient distributed optimization,2017, arXiv preprint arXiv:1710
 Effective sparsification of neural networkswith global sparsity constraint,2021, In Proceedings of the IEEE/CVF Conference on Computer Visionand Pattern Recognition
 From federated learning to federated neural architec-ture search: a survey,2021, Complex & Intelligent Systems
 Neural architecture search with reinforcement learning,2016, arXiv preprintarXiv:1611
 The compressed model PARAMs and FLOPs are normal-ized according to FedAvg,3000, The maximum communication rounds are 1500 and 3000
 FedMorph has a significantlysmallest generalization gap for MNIST among the competitors,2022, Comparing with FedMorph
