Table 1: Relationship between batch size s and the number of steps K needed for nonconvexoptimization in the sense of (1) of optimizers with constant learning rates	Constant Learning Rate Ru 	(a® (S) = I, βk = β ∈ [0,b] ⊂		e [0, 1))	Rational Function	Optimal Batch Size s?	Minimum Steps Ke (S?)SGD N-Momentum Adam-type	K = 4s 二 「2 S1 - B ɑ K =	4s		dDL 2 n 2 a -e- dDL2n2a	(dDLn )2 e 4 (dDLn )2	e = (e2 - Ce ) S - Ba K = ʒ_Aas-		7be2 - dDLnβ dDL2n2a	(7be2 - dDLnβ)2 (dDLn )2 H	e	( e2 - Ce ) S- Ba	72(be2 - dDLnβ[ h]	Y2(be2 - dDLnβ)2h]Table 2: Relationship between batch size s and the number of steps K needed for nonconvexoptimization in the sense of (1) of optimizers with diminishing learning rates	Diminishing Learning Rate Rule (ak(S) = ^⅛, βk = lβ ∈ [0, b] ⊂ [0, I)) Ik						Rational Function				Optimal Batch Size s?	MinimUmStePS Ke(s?)SGD	Ke	AaS2	+ Ba	r	2L Lna	2( dDLn )2		=e	2S-	ʃ		-e4-N-Momentum	Ke	AaS2	+ Ba	2	√2 Lna	2( dDLn )2		=	(e2-	CF	ʃ 2		(7be2 - dDLnβ)2Adam-type	Ke	AaS2	+ Ba		2L Lna	2( dDLn )2 H		=	(e2-	CF	ʃ	YpHh	γ2 (be2 - dDLnβ) h0The main contribution of this paper is to clarify that the number of steps K = K needed fornonconvex optimization in the sense of an -approximation, 1min E kNf (xk)k2 ≤ 2,	(1)
Table 2: Relationship between batch size s and the number of steps K needed for nonconvexoptimization in the sense of (1) of optimizers with diminishing learning rates	Diminishing Learning Rate Rule (ak(S) = ^⅛, βk = lβ ∈ [0, b] ⊂ [0, I)) Ik						Rational Function				Optimal Batch Size s?	MinimUmStePS Ke(s?)SGD	Ke	AaS2	+ Ba	r	2L Lna	2( dDLn )2		=e	2S-	ʃ		-e4-N-Momentum	Ke	AaS2	+ Ba	2	√2 Lna	2( dDLn )2		=	(e2-	CF	ʃ 2		(7be2 - dDLnβ)2Adam-type	Ke	AaS2	+ Ba		2L Lna	2( dDLn )2 H		=	(e2-	CF	ʃ	YpHh	γ2 (be2 - dDLnβ) h0The main contribution of this paper is to clarify that the number of steps K = K needed fornonconvex optimization in the sense of an -approximation, 1min E kNf (xk)k2 ≤ 2,	(1)k∈[K]of one of SGD, N-Momentum, and Adam-type optimizers can be expressed as a rational function ofbatch size s (see the “Rational Function” columns of Tables 1 and 2).
Table 3: Number of steps, elapsed time, and training accuracy of optimizers when f(xK) ≤ 10-1to train ResNet-20 on CIFAR-10Batch Size	26	27	28	SGD 29	210	211	212	213	214	215Steps	537500	287500	142500	146875^^―^^	—	—	—	—	—Time (m)	34.2	20.8	15.4	14.5	—	—	—	—	—	—Acc. (%)	96.6	96.8	96.6	96.7	—	—	—	—	—	—Batch Size	N-Momentum 26	27	28	29	210 211 212 213 214 215Steps Time (m) Acc. (%)	392187^^27734^^12402^^19531^^—^^=~~=~~—^^=~~ 38.2	21.7	14.3	12.6	—	—	—	—	—	— 96.5	96.7	96.7	96.7	—	—	—	—	—	—Batch Size	M-Adam										26	27	28	29	210	211	212	213	214	215Steps	33593	15234	7226	3125	1367	659	378	320	323	337Time (m)	14.2	11.3	7.2	6.7	6.4	6.4	6.6	6.6	6.4	6.6Acc. (%)	96.4	96.6	96.5	96.7	96.7	97.2	97.5	97.7	97.5	99.0We evaluated the performances of SGD, N-Momentum, and M-Adam with different batch sizes totrain ResNet-20 on the CIFAR-10 dataset with n = 50000. We set α = 10—3, β =10-2, Y = 0.9,hQ = 10-2, and L = 10. H = 10 were set so as to satisfy Y JHh. < 1, i.e., SD SGD = SDNM <s?D,A. Table 2 confirms that the optimal batch sizes of the optimizers are such that s?D,SGD =SD NM = V2Lna ≈ 28 ‹ 213 ≈ √2Lna/(Yy∕HhQ) = SD χ Table 3 shows that the optimizerswith S? (indicated by bold type) could reduce the number of steps more than the ones with otherbatch sizes, (9) was satisfied, and Adam with S? performed better than other optimizers. We also
Table 4: Examples of Hk ∈ S++ ∩ Dd (step 5) in Algorithm 1 (δ, Z ∈ [0,1))	H k	SGD (βk = γ = 0)	H k is the identity matrix.
Table 5: Stochastic gradient complexity (SGC) for e-approximation of optimizers (SGD, N-Momentum, Adam-type, and SPIDER (Fang et al., 2018)) with constant and diminishing learningA theoretical investigation of Stochastic Path-Integrated Differential EstimatoR (SPIDER) for e-approximation in nonconvex optimization was reported in (Fang et al., 2018). In particular, The-orem 2 in (Fang et al., 2018) clarified that SPIDER, which has a constant learning rate, for e-approximation must use the full-batch gradient with the number of samples n or the stochasticgradient with batch size √n. Meanwhile, our results show the optimal batch size of SGD, N-Momentum, and Adam-type optimizers (see Tables 1 and 2). Table 5 indicates that the SGCs ofSGD, N-Momentum, and Adam-type optimizers depend on a positive parameter α. For example,let us set α = e2 and focus on N-MomentUm using diminishing learning rate. Then, the SGC ofN-MomentUm is O(n3/e2). The SGC of SPIDER is O(n + √n∕e2) (see (Fang et al., 2018, Table1) for the SGCs of the variance-reduction type of optimizer).
