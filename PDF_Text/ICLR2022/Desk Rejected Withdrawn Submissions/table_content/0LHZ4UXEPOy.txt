Table 1: Influence of supervised contrastive regularization on generative kernel continual learn-ing over PermutedMNIST, RotatedMNIST and SplitCIFAR100. Left: the contrastive regularizerfurther enhances continual learning average accuracy. Right: optimizing temperature parameter τin Eq. (4) results in a small gain on SplitCIFAR100.
Table 2: Ability to generate multiple kernel types. Re- sults are promising, indepen-	RotatedMNIST		PermutedMNIST	SplitCIFAR100	Linear Polynomial	82.48 82.08	89.23 89.23	72.79 75.33dent of kernel type.	Radial Basis Function	81.42	89.59	72.68Table 3: Comparison with state-of-the-art. Results for other methods are adopted from Der-akhshani et al. (2021). Column unit indicates whether methods exploit a memory unit M or agenerative model G. Generative kernel continual learning yields competitive results on Permuted-MNIST and RotatedMNIST and outperforms alternatives on SplitCIFAR100 by a large margin.
Table 3: Comparison with state-of-the-art. Results for other methods are adopted from Der-akhshani et al. (2021). Column unit indicates whether methods exploit a memory unit M or agenerative model G. Generative kernel continual learning yields competitive results on Permuted-MNIST and RotatedMNIST and outperforms alternatives on SplitCIFAR100 by a large margin.
Table 4: Hyperparameters used to train the kernel network fγ .
Table 5: Hyperparameters used to train the conditioanl variational auto-encoder.
Table 6: Influence of kernel network fγ . Average accuracy of our proposed method with andwithout kernel network over 20 sequential tasks for five different random seeds on SplitCIFAR100.
