Table 1: Compositional and Few-Shot Action Recognition on the “SomethingElse” dataset. All results arereported using ground-truth box annotations.
Table 2: Comparison to state-of-the-art on video action recognition. We report top-1 (%) and top-5 (%)accuracy on SSv2. On Epic-Kitchens100 (EK100), we report top-1 (%) action (A), verb (V), and noun (N)accuracy. On Diving48 we report top-1 (%). For EK100 and Diving48 we used only detected boxes sinceground-truth does not exist. Difference between baselines and ORViT is denoted by (+X). IN refers to IN-21K.
Table 3: Albations. We report top-1 and top-5 action accuracy on the SomethingElse split. We show (a)Contribution of ORViT components (with parameters number in 106 and GFLOPS in 109). (b) Other Object-centric baselines , and (c) ORViT with box input not provided by a detector.
Table 4: Albations. Evaluation of different model ablations and baselines on the “SomethingElse” split (Tables(a-d) see text). We report pretrain, param (106), GFLOPS (109), and top-1 and top-5 video action recognitionaccuracy. Table (e) reports ablations on the Diving dataset.
Table 5: A light-weighted version of ORViT.
Table 6: Comparison to the state-of-the-art on video action recognition. We report pretrain, param (106),GFLOPS (109) and top-1 (%) and top-5 (%) video action recognition accuracy on SSv2. We also report whenour model uses ground-truth (GT) and detected boxes. On Epic-Kitchens100 (EK100), we report top-1 (%)action (A), verb (V), and noun (N) accuracy. On Diving48 we report pretrain, number of frames and top-1 (%)video action recognition accuracy. For EK100 and Diving48 we used only detected boxes since ground-truthdoes not exist.
