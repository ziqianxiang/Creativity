Table 1: p2(μn) values across epoch 73 to 76 for ResNet18 on CIFAR100 dataset (p2(μn)) valuesare from Figure 5d)Layer	P2(μn=73)	p2 (Mn)	δn	p2 (Mn)	δn	P2(μn=76)	δnLayer 1	0.14	0.14	0	0.14	0	0.14	0Layer 5	0.19	0.19	0	0.19	0	0.19	0Layer 9	0.14	0.14	0	0.14	0	0.14	0Layer 13	0.09	0.09	0	0.09	0	0.09	0Layer 18	0.44	0.44	0	0.44	0	0.44	04	ExperimentsIn this section, we empirically evaluate the effectiveness of our hypothesis on six different CNNarchitectures such as ResNet18 (He et al., 2016a), ResNet18+CBS (Sinha et al., 2020), CNN (Le-Cun et al., 1998), CNN+CBS (Sinha et al., 2020), VGG (Simonyan & Zisserman, 2014a), andVGG+CBS (Sinha et al., 2020). We test these CNN architectures on three different datasets(CIFIR10, CIFIR100 (Krizhevsky et al.), and SVHN (Netzer et al., 2011)) and analyze the Compu-tational Time Saving (CTS) and Top-1 classification accuracy by using our hypothesis. We furtherprovide an ablation study to analyze the influence of our strategy.
Table 2: DatasetDataset	Batch Size N	Training Data	Training Iteration t	Validation Data	Validation IterationCIFIR10	64	50000	782	10000	157CIFIR100	64	50000	782	10000	157SVHN	64	73257	1145	26032	4074.2	Computational time saving (CTS)We report the Computational Time Saving (CTS) based on epoch and iteration numbers. We con-sider 200 epochs as the benchmark number to compare with Sinha et al. (2020) use 200 epochs asdefault parameter in CBS (Curriculum by Smoothing). Li et al. (2020) use 200 epochs on CIFAR10and CIFAR100 datasets. Kim et al. (2020) use 200 epochs on VGG and ResNet architecture. Aswe use CBS, VGG, and ResNet architectures on CIFAR10, CIFAR100 datasets, we compare thecomplexity based on 200 epochs for all of our experiments (i.e., six CNN architectures, and threedatasets). The iteration number is different based on the dataset size but we use fixed batch sizenumber to obtain iteration number for a dataset (Table 2). In Table 2, we show the batch size (N),iteration number (t), training, and validation data. We keep the batch size constant (i.e., 64) for alldatasets. That is, in one iteration, the model uses 64 samples.
Table 3: Computational Time Saving (CTS) in percentage and Top-1 classification accuracy (Acc.)on CIFAR10, CIFAR100, SVHN datasets. The bold numbers represent better scores.
