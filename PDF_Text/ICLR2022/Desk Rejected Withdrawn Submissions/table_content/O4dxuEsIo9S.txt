Table A1: Activation sparsity of different layers of ResNet-18 trained on CIFAR-100 using ReLUand Swish at various sparsity levels.
Table A2: Results of testing accuracy for ResNet-18 on CIFAR-100 for our architecture tweakingand training recipe tweaking techniques. Results are mean across 3 runs with different seeds (42,1099, 5469). All the networks are trained with exactly same settings (Section 4.1) for 180 epoch.
Table A3: Results of testing accuracy for ResNet-18 on CIFAR-100 for the composition of archi-tecture tweaking (AT) and training recipe tweaking (TRT) techniques. All the networks are trainedwith exactly same settings (Section 4.1) for 180 epoch. The last column indicates the performancedifference of our method with respect to weight rewinding.
Table A4: Comparison of results of testing accuracy for ResNet-34 on CIFAR-10/100 for variouspruning algorithms with our “Tweaks” incorporated LTH.
