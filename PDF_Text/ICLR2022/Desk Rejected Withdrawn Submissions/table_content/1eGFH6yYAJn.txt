Table 1: Top 1 test accuracy (in %) of linear classifiers trained on frozen encoders from variousmulti-modal late-fusion training methods with uni-modal training on VGG-Sound and UCF101.
Table 2: Depth encoder evaluation on RGB-Depth se-mantic segmentation setting. “Initialization” indicateshow weights are initialized for the network, “Uni-Depth”represents end-to-end training with a depth-only segmen-tation network, and “from RGB+depth” refers to freezingthe depth encoder (ResNet-34) from ESANet (Seichteret al., 2020) then fine-tuning with a new decoder.
Table 3: Results of different multi-modal train- Table 4: Results of Self-Distillation and UMT.
Table 5: Model performance comparison under UMT andESANet on NYU-DepthV2 RGB-Depth semantic seg-mentation task.
Table 6: Top-1 test accuracy of different models on selected classes of VGG-Sound. It appears thatthe multi-modal naive fusion model outperforms other uni-modal models in these classes, and evenexceeding the sum of the accuracy of the uni-audio model and uni-video model. However, we do notfind any classes like those in UCF101, meaning VGG-Sound contains more paired features. Moredetails can be found in Appendix A.10Class ID	164	303	33	255	91	4	152	127	68	155Uni-Audio	30%	7%	34%	10%	43%	50%	18%	0	53%	32%Uni-Video	3%	2%	4%	3%	4%	12%	2%	0	15%	5%Naive Fusion	43%	18%	48%	22%	55%	67%	26%	4%	72%	40%Table 3, UMT outperforms other methods and does improve multi-modal learning in VGG-Soundand UCF101.
Table 8: The Hyperparameters used in our experiments. Noting that NYU Depth V2’s hyperparam-eters can be found in Seichter et al. (2020) and we use ResNet34 as the backbone.
Table 9: Finetuning Results on VGG-SoundTable 10: Middle Fusion vs Late Fusion (VGG-Sound)Method	Late Fusion	Middle Fusion	UMT (based on Late Fusion)Top-1 Accuracy	49.46	49.87 —	53.46A.8 Can Dropout Tackle Modality Lazines s ?Here we consider the common regularizer, dropout (Srivastava et al., 2014), and a variant of it,namely modality-wise dropout, which randomly drops (with probability 1/3) the feature from onemodality in every iteration. Modality dropout is akin to the ModDrop in Neverova et al. (2015).
Table 10: Middle Fusion vs Late Fusion (VGG-Sound)Method	Late Fusion	Middle Fusion	UMT (based on Late Fusion)Top-1 Accuracy	49.46	49.87 —	53.46A.8 Can Dropout Tackle Modality Lazines s ?Here we consider the common regularizer, dropout (Srivastava et al., 2014), and a variant of it,namely modality-wise dropout, which randomly drops (with probability 1/3) the feature from onemodality in every iteration. Modality dropout is akin to the ModDrop in Neverova et al. (2015).
Table 11:	Dropout in multi-modal training (VGG-Sound)Method Naive Fusion Dropout Modality Dropout UMTTop-1 Accuracy 49.46	49.83	51.37	53.46A.9 Sensitivity Analysis on Distillation WeightsHere we test different distillation weights on VGG-Sound. As shown in Table 12, if the weight is toosmall, the model will lack of strength to fight against Modality Laziness; on the other hand, if theweight is too large, the weight of multi-modal cross entropy loss would be relatively small, whichhinders joint multi-modal feature learning.
Table 12:	Different distillation weights of UMT on VGG-Sound-^Weights 0	1	10	20	50	100^Top-IAccuracy	49.46	49.51	51.31	51.51	53.46	53.11Table 13: Top-1 test accuracy of different models on selected classes of UCF101. We select thetop-10 classes according to the gap of accuracy between the multi-modal and uni-modal models.
Table 13: Top-1 test accuracy of different models on selected classes of UCF101. We select thetop-10 classes according to the gap of accuracy between the multi-modal and uni-modal models.
Table 14: Dataset used in Example 1. + means the feature is larger than zero and - means thefeature is less than zero. We denote the predicting probability by p and the rectified probability (dueto pushing force) by p0 .
