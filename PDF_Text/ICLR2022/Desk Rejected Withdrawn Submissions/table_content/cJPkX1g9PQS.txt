Table 1: Results on the WSJ permuted document test set and the various independent test sets of the previousSOTA UNC model and our XLNet based models. Except for the LMvLM results which are reported in termsof Krippendorffâ€™s alpha agreement with human annotators, all other results are reported in terms of accuracy ofthe models in scoring the positive document higher than the negative document. All results are averaged over 5runs with different seeds.
Table 2: Results on the WSJ permuted document test set and other independent test sets on the pairwise andcontrastive models trained on different datasets. All results are averaged over 5 runs with different seeds.
Table 3: Accuracies of the best performing UNC and our full model on the hand-crafted linguistic probedataset constructed by Shen et al. (2021). Examples (abridged for brevity) shown indicate the manual changesmade to make the text incoherent; the original words are shown in blue while the modified/added words areshown in red. Checks C) indicate our model correctly scored the coherent text higher for that example, whilecrosses (X) indicate that our model failed to do so.
Table 4: Examples showing the original coherent document and the incoherent document created bypermuting the sentences of the original. Text taken from WSJ-1778.
Table 5: Configuration parameters for trainingParameters	ValuesMargin-based Pairwise Ranking	- margin	0.1- optimizer	AdamW- scheduler	SWALR- learning rate	5e-6- annealed to	1e-6- anneal rate	5000 steps- batch-size	1- XLNet model	base- dimension size	768Contrastive Learning	- margin	0.1- optimizer	AdamW- scheduler	SWALR- learning rate	5e-6- annealed to	1e-6- anneal rate	5000 steps- batch-size	1
Table 6: Results reported by Mohiuddin et al. (2021) and Pishdad et al. (2020) on various tasks anddatasets that compare the UNC model to two other SOTA neural coherence models proposed by Xuet al. (2019) and Mesgar & Strube (2018). Except those marked by (Agr.) which report agreementwith humans, all other tasks report accuracies. We only include tasks that directly test discoursecoherence phenomena.
