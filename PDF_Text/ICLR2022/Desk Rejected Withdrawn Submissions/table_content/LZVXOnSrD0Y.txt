Table 1: Training Details2-Obj	3-Obj	5-ObjPA-2 DRL-MOA PA-3 DRL-MOA PA-5 DRL-MOABatch size	60	200	60	200	60	200Epochs	1	5	1	5	1	5Input Graph size	120 X 4	40 X 4	120 X 4	40 X 4	120 X 6	40 X 6Steps (per epoch)	20000	2000	25000	2000	20000	2000Training time (hrs)	〜14	〜100 X 0.70	〜23	〜91 X 0.70	〜18	〜40 X 1The details of training of PA-Net and DRL-MOA are given in Table-1. The times are reported basedon training from NVIDIA V100 Volta GPU. Note that in case of DRL-MOA, average training timefor each preference network is 〜1 hr (for {2,3,5} objective). So 100 networks are trained with thetotal training time 〜70hrs.
Table 2: Quantitative comparison of Pareto front for 2-Objective MOTSP40-City			200-City		500-City		1000-City	Algo.	HV (%)	Time (s)	HV (%)	Time (s)	HV (%)	Time (s)	HV (%)	Time (s)NSGA-II (20K)	67.3	5.64	45.4	8.04	38.4	14.7	33.8	27.1NSGA-II (80K)	72.5	21.7	53.9	30.8	46.36	58.7	41.48	107.3MOEA/D (20K)	66.7	9.357	47.5	12.7	40.4	20.7	35.7	33.5MOEA/D (80K)	70.7	34.65	55.98	48.2	48.8	79.53	43.89	130.75DRL-MOA	73.6	5.87	80.63	29.3	84.5	72.9	85.9	145.5PA-2 (ours)	75.4	1.59	83.2	6.03	86.92	15.14	88.45	30.38OR-tools (LS)	78.14	2.16	86.5	86.8	91.07	732	93.39	37308Under review as a conference paper at ICLR 2022Table 3: Quantitative comparison of the Pareto front, for 3,5-Objective MOTSP	200-City		500-CityProblem	Algo.	HV (%) Time ⑸	HV	Time (%)	(s)	NSGA-II (80K)	44.24	35.3	36.15	78.63-obj	DRL-MOA	86.3	26.6	89.8	66.3	PA-3 (K = 91)	84.72	5.2	88.05	12.6	PA-3 (K = 500)	85.41	29.27	88.51	70.8	OR Tools (LS)	89.71	77.8	93.4	672.9
Table 3: Quantitative comparison of the Pareto front, for 3,5-Objective MOTSP	200-City		500-CityProblem	Algo.	HV (%) Time ⑸	HV	Time (%)	(s)	NSGA-II (80K)	44.24	35.3	36.15	78.63-obj	DRL-MOA	86.3	26.6	89.8	66.3	PA-3 (K = 91)	84.72	5.2	88.05	12.6	PA-3 (K = 500)	85.41	29.27	88.51	70.8	OR Tools (LS)	89.71	77.8	93.4	672.9	NSGA-II (80K)	-216-	55.2	16.6	133.2	MOEA/D (80K)	30.1	103.54	22.8	180.45-obj	DRL-MOA	64.6	12.1	69.3	30.1	PA-5 (K = 40)	62.3	2.6	67,4	6.35	PA-5 (K = 500)	67.3	29.9	71.8	72.6	OR Tools (LS)	69.8	34.5	76.32	299.2Application for Coverage Planning: We test our network for coverage planning. We assume ascenario where the robot has to visit all the cells while ensuring the maximum adherence to a pre-computed priority order. Such a scenario is representative of real-world applications. For instance,a cleaning robot has to clean a large area where different regions have varied priorities based on thenumber of people visiting those areas. So the goal is to visit all the cells while minimizing the totaldistance travelled and maximizing the adherence to pre-computed priority order. This task can be
Table 4: Quantitative comparison of the Pareto front for coverage planning.
Table 5: Hypervolume comparison for PA-Net and TSP-Net+LSCities	Network type	No. of Parameters	HV (%)	HV (%)40	748	53.6200	82.7	47.12500	86.8	47.510000	88.2	147.7Training time (hrs)	〜14	〜39Ablation StudiesIn order to understand the contribution of different parts of PA-Net a few ablation studies wereperformed on 2- objective MOTSP instances. Description for various studies performed are asfollows:•	Ablation 1: For this study, PA-Net is trained without the feed forward network of preferenceencoder.
Table 6: Quantitative comparison of Pareto front for 2-Objective MOTSP	40-City		200-City		500-City		1000-City		Training time (hrS)Algo.	HV	Time	HV	Time	HV	Time	HV	Time		(%)	(s)	(%)	(S)	(%)	(S)	(%)	(S)	PA-2 (baseline)	75.7	1.66	83	6.66	86.7	16.5	88.3	34.4	〜14Ablation 1	39.1	1.48	25.6	5.5	25.0	14.5	24.6	29.8	〜13Ablation 2	75.4	1.53	82.6	5.7	86.3	14.3	87.8	29.6	〜13Ablation 3	75.74	1.29	82.8	5.5	86.6	14.15	88.3	28.9	〜12Table 7: Number of trainable parameters for each network in Ablation studiesNetwork type	No. of ParametersAblation 1	1454337Ablation 2	1465617Ablation 3	464129PA-2 (baseline)	1554459Ablation 1 has the worst performance. This indicates that encoder to learn representation of pref-erences plays a critical role in the performance of PA-Net. Interestingly enough, it seems like thechoice of encoder for both preferences and the input TSP graph does not have much impact on theperformance. This indicates that a relatively faster network can be obtained by using a relativelysimpler choice of encoder. Although, the network in Ablation 3 has significantly lower number oftrainable parameters, yet its training time is not significantly lower. This is primarily due to the fact
Table 7: Number of trainable parameters for each network in Ablation studiesNetwork type	No. of ParametersAblation 1	1454337Ablation 2	1465617Ablation 3	464129PA-2 (baseline)	1554459Ablation 1 has the worst performance. This indicates that encoder to learn representation of pref-erences plays a critical role in the performance of PA-Net. Interestingly enough, it seems like thechoice of encoder for both preferences and the input TSP graph does not have much impact on theperformance. This indicates that a relatively faster network can be obtained by using a relativelysimpler choice of encoder. Although, the network in Ablation 3 has significantly lower number oftrainable parameters, yet its training time is not significantly lower. This is primarily due to the factthat during the training, K × B TSP tours are generated in all the networks. This serves as the com-putation bottleneck for the network during the training. Hence, that’s why no significant differencein training times are observed.
Table 8: Comparison of HV for different hyperparameter values for 2-objective MOSTP		40-City	200-City	500-City	1000-CityParameter	Value	HV (%)	HV (%)	HV (%)	HV (%)	20 (baseline)	75.7	83	86.7	88.3K	10	73.5	82.2	85.4	85.4	5	74.8	78.1	81.1	82.3	20 (baseline)	-^757^^	83	86.7	88.3λmax	10	75.6	82.6	86.3	88.0	5	75.9	81.9	85.4	87.3	2.5 × 10-5 (baseline)	-^757^^	83	86.7	88.3α	2.5 × 10-3	73.5	82	85.8	87.4	1.25 × 10-5	76.1	83	86.6	88.1	5.0 × 10-5	75.8	82.5	86.4	87.9	2.5 × 10-8	75	81.7	85.5	87.0These networks trained on different values of the above-mentioned hyperparameters are comparedon the basis of obtained HV for 2-objective MOTSP where 100 preference vectors were used at theinference time. These results are presented in Tab. 8. The following conclusions can be made:•	Higher values of K leads to better results. Intuitively, it makes sense because a highernumber of preference vectors during training can help network learn better and generalizebetter. However, large K leads to longer training time. For example, training time of
