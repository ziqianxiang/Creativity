Table 1: Comparison of our EA method with other EA methods. Note that k = d when embeddingsare real-valued. Overall, TR-GAT is most comprehensive.
Table 2: Statistics of time-aware EA datasets. |P | denotes the total number of reference entity pairs.
Table 3: TKG completion results on ICEWS14, ICEWS05-15 and YAGO15K. Dashes: results arenot reported in the respective literature. The best results among all models are written boldtransformer, have better performance on ICEWS05-15 than TR-GAT regarding MRR, Hits@3 andHits@10 but also suffer from longer training time. Although TeMP models use lower-dimensioanlembeddings than TComplEx, TNTComplEx, ChronoR and ours, the training processes of TeMP-GRU and TeMP-SA on ICEWS05-15 take about 49 and 52 minutes per epoch using a single GTXTitan X GPU, while it averagely takes about 17 minutes for TR-GAT to complete a training epochon ICEWS05-15 with the same device.
Table 4: Entity alignment results on ICEWS and YAGO-WIKI50K datasets. Improv. indicatesthe improvement achieved by TR-GAT against its time-unaware variant TU-GAT. The best resultsamong all models are written bold.
Table 5: Entity alignment results on different test sets of YAGO-WIKI20K.
Table 6: Statistics of TKG completion datasets.
Table 7: Optimal hyperparameters of TR-GAT model for TKG completion.
Table 8: Optimal hyperparameters of target models for DICEWS-1K.
Table 9: Optimal hyperparameters of target models for DICEWS-200.
Table 10: Optimal hyperparameters of target models for YAGO-WIKI20K.
Table 11: Optimal hyperparameters of target models for YAGO-WIKI50K-5K.
Table 12: Optimal hyperparameters of target models for YAGO-WIKI50K-1K.
