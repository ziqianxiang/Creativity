Table 1: GLUE test results, where f refers to continual pre-training. Due to space limitation, Weonly demonstrate the overall confidence level as in Table 2.
Table 2: Fine-tuning results of all tasks.
Table 3: Effect of regularization. The models are trained without attention dropout and weight decay.
Table 4: Effect of Loss features in AttendOut.
Table 5: Comparison of AttendOut and LayerDrop.
Table 6: GLUE development results when intermittently using AttendOut and Vanilla Dropout.
Table 7: Comparison of LSTM-based and MLP-based modeling.
Table 8: Hyperparameters for BERT and RoBERTa in all downstream tasks. LR: learning rate; BSZ:batch size; EP: training epochs; WP: warmup proportion; MSL: maximum sequence length; MLR:meta learning rate.
Table 9: Time consumption using AttendOut on selected tasks compared to using Vanilla Dropout.
