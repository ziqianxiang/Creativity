Table 1: 20 Questions, higher the better		Table 2: Jester Joke, lower the better	Method	ACCUracy(%)	Method	Regretπ* (Ours)	T79	π* (Ours)	3.209SGBS	{26.5, 26.2, 27.2,	SGBS	{3.180, 3.224, 3.278,	26.5, 21.4, 12.8}		3.263, 3.153, 3.090}Uncertainty	14.3	Uncertainty	3.027Bayes-LAL	4.1	Bayes-LAL	3.610Uniform	6.9		Uniform	3.877	While heuristic based algorithms (such as SGBS, uncertainty sampling and Bayes-LAL) can per-form catastrophically for worst-case instances, they can perform very well with respect to a benigndistribution over instances. Here we demonstrate that our policy not only performs optimally underthe instance-dependent worst-case metric but also remain comparable even when evaluated underthe average case metric. To measure the average performance, we construct prior distributions Pbased on the individual datasets:•	For the 20 questions dataset, to draw a θ 〜 P, we uniformly at random select a J ∈ [1000] andsets θi = 2p(j)- 1 for all i ∈ [d].
Table 3: Number of Iterations and Learning RatesProcedure	Hyper-parameter	Experiment				1D Threshold |X| =25	20 Questions |X| = 100	Jester Joke |X| = 100	N	1000 X |Z|	300 × |Z|	2000 × |Z|	M	1000	500	500	L	10	30	30Init + Train +	λbinary	7.5	30	30Fine-tune	λPol-reg(regret)	.2	.8	.8	λPol-reg(fine-tune)	.3	.8	.8	λGen-reg	.05	.1	.05	λbarrier		103 (all)	Table 4: Parallel Sizes and Regularization coefficients•	Fine-tune b This procedure optimizes equation 3, with baselines mink '(∏k, Θ(rk)) evaluatedbased on each πi learned from the previous procedure. Similar to fine-tuning each individual πi ,We warm start a policy π∖κ∕2c and reinitialize W and Θ by running the initialization procedureagain.
Table 4: Parallel Sizes and Regularization coefficients•	Fine-tune b This procedure optimizes equation 3, with baselines mink '(∏k, Θ(rk)) evaluatedbased on each πi learned from the previous procedure. Similar to fine-tuning each individual πi ,We warm start a policy π∖κ∕2c and reinitialize W and Θ by running the initialization procedureagain.
