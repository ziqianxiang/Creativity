Table 1: Winning ratio (%) of MARL algorithms on single-step grouping environment.
Table 2: Winning ratio (%) and the percentage of the landmarks occupied (%) on cooperative nav-igation. In the table, the left column corresponds to the winning ratio, and the right column cor-responds to the landmark occupied ratio for each algorithm. The mean and standard deviation of1,000 repeated experiments is reported.
Table 3: Winning ratio (%) on Starcraft Micromanagement environment. The mean and standarddeviation of 1,000 repeated experiments is reported.
Table 4: Hyperparemeters of LPMARLHyperparemter	ValuesMLP units for GNN, f (∙; θN),f (∙; θM), f (∙; θv)	[32,32]MLP units for coefficient matrix, f (∙; θc)	[64,64]MLP units for policy network, f (∙; θh),f (∙; θι)	[64,64]MLP units for critic network f (∙; φh),f (∙; φι)	[64,64]Nonlinear activation	LeakyReLU, negative slope=0.01learning rate	10-3Discount rate, γ	0.99λ	40Optimizer	AdamA.5 Details about ExperimentsA.5.1 Single-step grouping environmentWe run 10,000 episodes (1 step per episode) for training each algorithm. For every algorithms, weset the size of the replay buffer as 500, and set batch size as 100. Agents receive R = 10 When K =∣{ai}i∈N| (agents are exactly divided into K groups), and get reward R = 10 - 2 ∙ |K - ∣{ai}i∈N||.
