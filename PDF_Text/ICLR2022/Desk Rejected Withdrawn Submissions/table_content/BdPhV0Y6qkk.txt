Table 2: Cifar10Network	Training Strategy	Top-1 Error	Speed- UP_ResNet18	Baseline SGD InterTrain -RandPatch InterTrain-LinAvg	^65%- 5.4% 5.7%	1× 1.74 X 1.69 ×ResNet34	Baseline SGD InterTrain -RandPatch InterTrain-LinAvg	-3.2%- 4.2% 4.6%	F 1.78 X 1.71 XTo underscore the wide applicability of InterTrain, we present our runtime and accuracy trade-offachieved on the Cifar10 benchmarks in Table 2. Across our benchmarks, LinAvg achieves upto1.7 × improvement in runtime, while RandPatch achieves a 1.8× runtime improvement. Bothtechniques provide upto a 0.8-1% boost in accuracy, due to the imrpoved regularization providedvia interpolating samples.
Table 3: Analyzing efficacy of our amenability metricBenchmark	Amenability Metric	Top-1 err	Speed-Up	Our Effort (Region 1 only)	24.56%	1.38	Our Effort (Regions 1 and 3)	24.45%	1.56ResNet50	Threshold = Accuracy	24.80%	1.36	Threshold = Average Loss	24.50%	1.33	Region 1 and threshold = Lincorr for Region 3	25.14%	1.74	7.8	Interpolating versus skipping difficult samplesIn Sec. 3.2, we proposed the concept of interpolating difficult samples that consistently occur inRegion 3 of the loss distribution. These samples are likely to stay incorrect even at the final epochsof training, and it is hence practical to reduce training effort on such samples. In this subsection,we analyze the impact on accuracy and runtime if training effort was completely eliminated on suchsamples, i.e., these samples are skipped for a few epochs, instead of interpolated.
Table 4: Comparing interpolation versus skipping Region 3 samplesBenchmark	k	Top-1 err (SGD)	Speed-Up (SGD)	Top-1 err (Our Effort)	Speed-Up (Our Effort)ResNet50-ImageNet	1 2 3		24.05% 25.41% 25.74%	1:12 1.14 1.15	24.45% 24.4% 24.36%	T.08 1.1 1.11	7.9	Comparative analysis against instance-skipping techniquesIn this sub-section, we further compare selective interpolation against selective skipping strategies.
Table 5: Comparing oracle-based skipping against our effortBenchmark	Training strategy	Top-1 err (SGD)	Speed-Up (SGD)ReSNet50-ImageNet	Selective Interpolation Instance Skipping	24.45% 25.12%	T.56 1.44This can be chalked up to the fact that in InterTrain, some form of training is conducted acrossall the samples in the dataset, resulting in better accuracy. in turn, this allows us to approximatetraining effort on a larger fraction of samples (the green columns in Fig. 14 always remain shorterthan the yellow columns across all epochs). Furthermore, as discussed in the previous sub-section,instance-skipping techniques are unable to leverage the classification performance and runtime bene-fits accrued by skipping extremely difficult samples , i.e., Region 3 approximations. This experimentunderscores the importance of our effort towards accelerating DNN training. interpolating a subsetof the training dataset every epoch thus provides a cheaper training alternative, without sacrificingaccuracy.
