Table 1: Explanation Accuracy on Synthetic Datasets (%).
Table 2: Explanation Accuracy on Real-World Datasets (%).
Table 3: Explanation Accuracy with Edge AUC (* means the rounded estimate of 0.9995 ± 0.0006).
Table 4: Prediction Accuracy of the Pre-trained GNN on Mutag with Various Perturbation (mean isset as 0).
Table 5: Data Statistics of Four Datasets.
Table 6: Motif Statistics on Mutag (Number of Instances Containing Carbon Rings with AssociatedMotifs).
Table 7: Model Accuracy of Four Datasets (%).
Table 8: Data Splitting for Four Datasets.
Table 9: Hyperparameters and RangesHyperparameterRangeCAUSAL DIMENSION DcNEGATIVE ELBO COEFFICIENT λ1SPARSITY COEFFICIENT λ2FIDELITY COEFFICIENT λ3{1, 2, 3,…，8}{0, 0.01, 0.02, 0.05, 0.1, 0.2, 0.5, 1}{0, 0.01, 0.02, 0.05, 0.1, 0.2, 0.5, 1}{0, 0.01, 0.02, 0.05, 0.1, 0.2, 0.5, 1}LVGAE = Eq(Z|X,A)[log p(A|Z)] - KL[q(Z|X, A) k p(Z)],	(6)where KL[q(∙) ∣∣ p(∙)] is the Kullback-Leibler divergence between q(∙) and p(∙). The Guassian prioris p(Z) = Qi p(zi) = QiN(zi|0, 1). We follow the reparameterization trick in [7] for training.
Table 10: Causal Evaluation (%).
Table 11: Ablation Studies for Different Regularization Terms (%).
Table 12: Explanation Time of Different Methods (Per Instance (ms)).
