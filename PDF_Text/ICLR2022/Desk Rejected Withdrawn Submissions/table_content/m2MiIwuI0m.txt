Table 1: Overall performance on all 15 datasets (65 tasks). We report (# tasks with top-1 performance,# tasks with top-3 performance). Models with no top-3 performance on a dataset are left blank.
Table 2: Performance on several tasks from both molecular and social network domains. The top-3and best performing models for each task are highlighted in gray and blue , respectively.
Table 3: Change in performance on removing weighting compo-nents of AWARE. “+" / “-" indicate relative improvement/decreasein performance with respect to the full AWARE model respectively.
Table S1: Details on the benchmark datasets used in our experimentsDataset	# of Tasks	Type	DomainIMDB-BINARY (Yanardag & Vishwanathan,2015)	1	Classification	Social NetworkIMDB-MULTI (Yanardag & Vishwanathan, 2015)	1	Classification	Social NetworkREDDIT-BINARY (Yanardag & Vishwanathan, 2015)	1	Classification	Social NetworkCOLLAB (Yanardag & Vishwanathan, 2015)	1	Classification	Social NetworkMutagenicity (Kazius et al., 2005)	1	Classification	ChemistryTOX2 1 (Tox21 Data Challenge, 2014)	12	Classification	ChemistryClinTox (Artemov et al., 2016; Gayvert et al., 2016)	2	Classification	ChemistryHIV (AIDS Antiviral Screen Data, 2017)	1	Classification	ChemistryMUV (Rohrer & Baumann, 2009)	17	Classification	ChemistryDelaney (Delaney, 2004)	1	Regression	ChemistryMALARIA (Gamo et al., 2010)	1	Regression	ChemistryCEP (Hachmann et al., 2011)	1	Regression	ChemistryQM7 (Blum & Reymond, 2009)	1	Regression	ChemistryQM8 (Ramakrishnan et al., 2015)	12	Regression	ChemistryQM9 (Ruddigkeit et al., 2012)	12	Regression	ChemistryDataset Licenses. The Delaney (Delaney, 2004), CEP (Hachmann et al., 2011), QM7 (Blum &Reymond, 2009), QM9 (Ruddigkeit et al., 2012), MUV (Rohrer & Baumann, 2009), and MUTA-genicity (Kazius et al., 2005) datasets are all licensed under the Copyright © of the American
Table S2: Hyperparameter sweeping for AWAREHyperparameters	Candidate valuesLearning rate	1e-3, 1e-4# of linear layers in the predictor: L	1,2,3Maximum Walk length: T	3, 6, 9, 12Vertex embedding dimension: r	100, 300, 500Random dimension: r0	100, 300, 500Optimizer	AdamBaseline Methods. For all the molecular baseline methods other than GAT, D-MPNN, Attentive FP,and PNA, the hyperparameter search strategy outlined in (Liu et al., 2019) has been adopted. ForGAT and D-MPNN, We use their reported optimal hyperparameters (VelickoVic et al., 2017; Yanget al., 2019). For Attentive FP and PNA, we performed a hyperparameter tuning that included theirreported optimal hyperparameters. For social netWork experiments, We perform hyperparametertuning on PNA and AttentiVe FP, and use the optimal hyperparameters reported for other baselinemethods. In addition, for some of the social netWork datasets, We only consider graphs Whose totalnumber of Vertices is less than a certain threshold (REDDIT-BINARY: 200, COLLAB: 100).
Table S3:	Ablation study I: Change in performance on removing/modifying components of AWARE.
Table S4:	Ablation study II: Change in AWARE’s performance when the vertex embedding matrix Wis randomly initialized and non-trainable, with linear σ. Underline indicates better performance.
Table S5:	Ablation study III: Change in AWARE’s performance when the final predictor is changedfrom a multiple layer NN to a linear predictor. Underline indicates better performance.
Table S6: In this table, we present the performance of 8 models on 4 classification tasks in the domainof social networks (Morgan FP is excluded as it works on molecular graphs). Experiments are run on5 different random seeds, and the average of the 5 reported for each task along with their standarddeviation in the subscript. The top-3 models in each task are highlighted in gray and the best one ishighlighted in blue . Higher is better.
Table S7: In this table, we present the performance of 9 models on 33 classification tasks from thedomain of molecular property prediction. Experiments are run on 5 different random seeds, andthe average of the 5 reported for each task along with their standard deviation in the subscript. Thetop-3 models in each task are highlighted in gray and the best one is highlighted in blue . We markincompatible task/model pairs with a “一”. Higher is better.
Table S8: In this table, we present the performance of 9 models on 28 regression tasks from thedomain of molecular property prediction. Experiments are run on 5 different random seeds, and theaverage of the 5 reported for each task along with their standard deviation in the subscript. The top-3models in each task are highlighted in gray and the best one is highlighted in blue . Models thatare too slow are left blank. Lower is better.
Table S9: Performance of models that use extra edge/3D information on classification tasks. Theexperiments are run on 5 different random seeds, and the average of the 5 reported for each task alongwith their standard deviation in the subscript. The best model in each task is highlighted in blue .
Table S10: Performance of models that use extra edge/3D information on regression tasks. Theexperiments are run on 5 different random seeds, and the average of the 5 reported for each taskalong with their standard deviation in the subscript. Since only QM8 and QM9 tasks include 3Dinformation, DTNN and MPNN are evaluated only on those. The top-3 models in each task arehighlighted in gray and the best one is highlighted in blue . RMSE is used as the evaluation metricfor the first three tasks and MAE for the rest. Lower is better.
