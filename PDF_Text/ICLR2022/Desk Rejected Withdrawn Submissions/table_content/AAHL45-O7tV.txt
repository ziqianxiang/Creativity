Table 1: Tasks and datasetsSequence Modeling Task	DatasetAdding Problem	Conditional random generation (Hochreiter & Schmidhuber, 1997) (Zhang etal.,2016)	Word-level language modeling	PennTreebank(MarcUs etal.,1993), Wikitext-103(Merity et al., 2016) , LAMBADA(PaPemo et al., 2016)	Character-level language modeling	PennTreebank, text8(Mikolov et al., 2012)Polyphonic music prediction	JSB Chorales(Allan & Wihiams, 2005), Nottingham(Greff et al., 2016b), Piano(Bernd, 1998), MUse(Stanford,1984)Digit classification	MNIST(LeCUn et al.,1998)Copy memory	Conditional random generation (Jing et al., 2017)4 ExperimentsThe experiments were all the classical tasks and the datasets used in vanilla TCN were employed todeal with timing problems. TransTCN and vanilla TCN were compared in terms of prediction ac-curacy and the convergence state of training. The intention was to conduct a baseline for TransTCNand further discuss the improvement of TCN with two branches under different structures.
Table 2: The number of parameters for different tasks and modules.
Table 3: The testing performance of TransTCN compared with TCN.
Table 4: The performance of TransTCN compared with cdrc-TCN and gab-TCN.
Table 5: The performance of TransTCN compared with attnconv-TCN.
Table 6: Performance of TransTCN compared with spc-TCN.
