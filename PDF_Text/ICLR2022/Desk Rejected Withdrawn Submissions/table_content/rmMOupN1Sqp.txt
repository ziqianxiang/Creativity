Table 1: Top: automatic evaluations on the Yelp review datas et. The BLEU (human) is calculatedusing the 1000 human annotated sentences as ground truth from Li et al. (2018). The first four resultsare from the original papers. Bottom: human evaluation statistics of base model vs. with EISL. Theresults denotes the percentages of inputs for which the model has better transferred sentences thanother model.
Table 2: The results (test set BLEU) of EISL loss and CE loss applied to non-autoregressive models.
Table 3: Examples of the generated sentences.
Table 4: The results on the political dataset. The first two results are reported by Tian et al. (2018).
Table 5: The results (test set BLEU) of CMLM trained with EISL, compared to other fully non-autoregressive methods. The results are reported by Ghazvininejad et al. (2020) except Transformerbase. Since AXE uses l = 5 length candidates to evaluate, for fairness, we adopt the similar settingfor CMLM with EISL (the results in Table 2 are under l = 1 length candidates, which is defaultsetting in NAT models).
Table 6: Examples of the generated sentences.
Table 7: Convergence time of pretraining and finetuning stages.
Table 8: The results (test set BLEURT) of EISL loss and CE loss applied to non-autoregressivemodels.
Table 9: Example 1.
Table 10: Example 2.
Table 11: Example 3.
Table 12: Example 4.
Table 13: Example 5.
