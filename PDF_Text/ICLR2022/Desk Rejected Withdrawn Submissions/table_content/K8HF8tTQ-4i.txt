Table 1: Illustration of different texts generated from the original GPT-2 and our CTG method, respectively,which demonstrates that our method can successfully steer GPT-2 toward distinct desired attributes.
Table 2: Word control and topic control experiment results. Our method guarantees that the single control wordor any keyword in a topic appears in the generation, and also achieves high language quality and diversity.
Table 3: Sentiment control experiment results. Our method achieves highly competitive results, with thesecond best control satisfaction and the third highest language quality for both positive and negative sentiments.
Table 4: Language detoxification results evaluated on samples generated from toxic prompts. Our methodachieves the second lowest text toxicity, third best perplexity, and competitive diversity scores, indicating itbest balances the effective detoxification and high-quality language generation.
Table 5: Compositional control experiment results. Our method achieves the best balance of good controlsatisfaction and high language quality and diversity.
Table 6: Computational efficiency comparing several baselines to ours with different base LM sizes where “S”and “L” denote small and large GPT-2 models, respectively. Our method requires no training/fine-tuning, usesthe least model parameters, and achieves fastest generation among all baselines, regardless of base LM sizes.
Table A1: Applicability of baselines to various CTG scenariosBaselines	Compatible Experiment Settings					single word	topic	sentiment	Detoxification	multiple controlREINFORCE	✓	✓	✓	✓	REINFORCE-PX	✓	✓	✓	✓	PPO (Ziegler et al., 2019)	✓	✓	✓		GDC	✓	✓	✓	✓	✓CTRL		✓	✓		DAPT			✓	✓	PPLM	✓	✓	✓	✓	✓GeDi			✓	✓	FUDGE	✓	✓			DExperts			✓	✓	PnB					✓Ours	✓	✓	✓	✓	✓These methods require either extensive training/fine-tuning for each single attribute or expensivesampling procedure, making them less useful in practice for flexible, efficient, plug-and-play CTGuse cases. For example, GDC (Khalifa et al., 2021) takes more than 100 hours to train on QuadroRTX 8000 GPUs. In contrast, our method requires no training/fine-tuning.
Table A2: Language detoxification experiment results evaluated on continuation generations fromnon-toxic prompts.
Table A3: Detailed single word control results for each control word. Our results, not necessarilythe best among all baselines, are marked in bold.
Table A4: Detailed topic control results for each topic. Our results, not necessarily the best amongall baselines, are marked in bold.
Table A5: samples of generations in the word control scenario. The control word here is amazing.
Table A6: samples of generations in the word control scenario. The control word here isrestaurant.
Table A7: samples of generations in the word control scenario. The control word here is amusing.
Table A8: samples of generations in the word control scenario. The control word here is Vampire.
Table A9: samples of generations in the topic control scenario. The control topic here is science.
Table A10: samples of generations in the topic control scenario. The control topic here is space.
Table A11: samples of generations in the topic control scenario. The control topic here is fantasy.
Table A12: samples of generations in the topic control scenario. The control topic here is legal.
Table A13: samples of generations in the topic control scenario. The control topic here iscomputers.
Table A14: samples of generations in the sentiment control scenario. The target sentiment here ispositive.
Table A15: samples of generations in the sentiment control scenario. The target sentiment here isnegative.
