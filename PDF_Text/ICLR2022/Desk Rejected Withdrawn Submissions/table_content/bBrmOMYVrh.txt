Table 1: Dataset summary and task descriptions. For each task, we sample K âˆˆ {10, 20, 30} labeledexamples to form five different splits with different random seeds from the original training set, andadd the remaining to the unlabeled set while ignoring their labels.
Table 2: Performance comparison of different model tuning strategies on different tasks withRoBERTa-large as the encoder with standard deviation in parantheses. UST, MetaST, PromptSTand iPET are semi-supervised methods using unlabeled data, whereas Classic and Prompt FN onlyuse labeled data. The best performance is shown in bold.
Table 3: Average accuracy ofprompt FN with different en-coders using |K| = 30 labels onsix tasks.
Table 4: Average accuracy on tun-ing different modules of RoBERTa-large with |K| = 30 labels onsix tasks. Diff shows performancechange relative to Full tuning.
Table 5: Average accuracy of sev-eral lightweight parameter-efficientstrategies with |K| = 30 labelson six tasks along with the num-ber (#) of tunable parameters. Weshow LiST performance with dif-ferent bottleneck dimension d of itsadapters in parantheses. The bestperformance is shown in bold.
Table 6: Ablation analysis of LiST with 30 labels onMNLI and RTE with tunable parameters in parantheses.
Table 7: Task prompt and label words summary. <S1> and <S2> indicate input sentences.
Table 8: Average performance and standard deviation of RoBERTa-large with Classic and Prompt-tuning strategies with varying training labels |K|.
Table 9: Average performance and standard deviation of different encoders with Classic and Prompt-tuning strategies with various training labels |K|.
Table 10: Average performance and standard deviation on tuning different modules of RoBERTa-large with varying amount of training labels |K|.
Table 11: Average performance and standard deviation of several lightweight parameter-efficientprompt-tuning strategies with |K| = 30 training labels. The best performance is shown in boldalong with the number (#) of adapter parameters of total encoder parameters.
Table 12: Average performance over various backbones with with training labels |K | = 30 (withunlabeled data). MetaST, PromptST and LiST are semi-supervised approaches.
Table 13: Average performance over various backbones with with training labels |K| = 20 (withunlabeled data). MetaST, PromptST and LiST are semi-supervised approaches.
Table 14: Average performance over various backbones with with training labels |K| = 10 (withunlabeled data). MetaST, PromptST and LiST are semi-supervised approaches.
Table 15: Ablation analysis of LiST with # of training data = 30.
Table 16: Ablation analysis of LiST with # of training data = 20.
Table 17: Ablation analysis of LiST with # of training data = 10.
Table 18: Performance comparison of classic FN with RoBERTa-large as the encoder with standarddeviation in parantheses. The best performance is shown in bold.
Table 19: Average Accuracy of Adapter w/ various number of training labels (No Semi-supervisedSetting).
