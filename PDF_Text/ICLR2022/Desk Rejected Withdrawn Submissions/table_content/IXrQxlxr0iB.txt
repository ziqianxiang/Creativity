Table 1: Experimental results on the long range arena (LRA) benchmark. The highest score for eachdataset is highlighted in bold and the second place is underlined.
Table 2: Performance of various models on development set of benchmark natural language under-					standing tasks.			—		Setting	BPC	Models	WikiHop	TriviaQA				Acc	F1	EMRoBERTa (Liu et al., 2019)	1.846	Longformer	75.0	75.2		1.705	BigBird	75.9	79.5	-Longformer (Beltagy et al., 2020)							Reproduce			BigBird (Zaheer et al., 2020)	1.678	Longformer	75.2	75.0	67.0ERNIE-Sparse	1.674	BigBird	72.3	73.4	68.6		ERNIE-Sparse	75.8	76.5	71.8										Table 3: MLM BPC for ERNIE-Sparse and					other models.		Table 4: Model comparison for WikiHop and					TriviaQA.			4.2.2 Text ClassificationTo test ERNIE-Sparse on downstream tasks, we first select three text classification tasks: Arxivpaper categories classification (He et al., 2019), IMDB reviews classification (Maas et al., 2011)and Hyperpartisan news detection (Kiesel et al., 2019). The experiment was repeated 5 times for
Table 3: MLM BPC for ERNIE-Sparse and					other models.		Table 4: Model comparison for WikiHop and					TriviaQA.			4.2.2 Text ClassificationTo test ERNIE-Sparse on downstream tasks, we first select three text classification tasks: Arxivpaper categories classification (He et al., 2019), IMDB reviews classification (Maas et al., 2011)and Hyperpartisan news detection (Kiesel et al., 2019). The experiment was repeated 5 times forboth datasets, and the mean and standard deviation were listed in the table. ERNIE-Sparse’shyperparameters are recorded in the appendix A.2.2. Note that all linear transformation weights ofhierarchical attention are shared with the weights of the previous Sparse Transformer attention. Table 2summarizes the results of ERNIE-Sparse. From this table, it shows that ERNIE-Sparse surpassesall baselines on the text classification datasets. For Arxiv, ERNIE-Sparse surpasses baseline bya large margin. For IMDB and Hyperpartisan, the performance gain continues demonstrating thatERNIE-Sparse is capable of utilizing information from long document input.
Table 5: Performance of ERNIE-Sparse by ablating each proposed components. ws means thelinear weights for hierarchical and sparse attention are shared.
Table 6: Ablation for the pooling method ofrepresentative tokens in ERNIE-Sparse fordownstream tasks.
Table 7: Performance of w/ and w/o SAOR: We construct an adversarial dataset by shift word to testthe robustness of ERNIE-Sparse and the sensitivity for sparse pattern shift.
Table 8: Pretraining data statisticsParameter	ERNIE-SPARSEα of LSAOR	0learning rate	3e-5batch size	256weight decay	0.1warmup steps	10ktotal steps	1mmax seq length	4096embedding dim	768#head	12#layer	12activation layer	geludropout	0.1attn dropout	0.1Table 9: Hyperparameters for the ERNIE-Sparse for PretrainingA.1.2 Pretraining HyperparametersWe split any document longer than 4096 into multiple documents and we joined multiple documentsthat were much smaller than 4096. During the pre-training phase, we only use mask language
Table 9: Hyperparameters for the ERNIE-Sparse for PretrainingA.1.2 Pretraining HyperparametersWe split any document longer than 4096 into multiple documents and we joined multiple documentsthat were much smaller than 4096. During the pre-training phase, we only use mask languagemodel for training tasks. Specifically, we mask 15% of tokens in these four datasets, and trainERNIE-Sparse to predict the mask. We warm start ERNIE-Sparse from RoBERTa’s checkpoint.
Table 10: Downstream tasks statistics. Samples of tasks Image and PathFinder are all 32 × 32 images.
Table 11: The upper part is the hyperparameter related to ERNIE-Sparse, while the lower part isthe fixed hyperparameter provided by LRA and cannot be changed.
Table 12: Hyperparameters of classification and question answering tasks.
