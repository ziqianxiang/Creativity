Table 2: Eventual performance against different methods on 6 easy-to-hard continuous control benchmarks. Themeans and the standard deviations are evaluated over more than 10 random seeds.
Table 3: Performance on NGSIM I-80 driv-As shown in Tab. 3, DPO outperforms baseline methodsin all three metrics with higher stability. The decoupledpolicy allows the state predictor to focus on matching thedistribution of expert trajectories, thus achieving smallerdeviations from the expert position distribution. Further-more, since the policy gradient can be computed withnon-differentiable inverse dynamics, we can generate sta-ing task over 5 random seeds.
Table 4: Hyperparameters of DPO.
Table 5: KL divergence between policy-sampled and the expert state transitions distribution.
Table 6: Additional ablation studies on the inverse dynamics regularization.
