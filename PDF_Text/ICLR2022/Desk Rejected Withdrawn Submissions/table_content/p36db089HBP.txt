Table 1: Comparison of models with deep architecture in terms of model features and accuracyon MNIST, CIFAR10 (CIF10), CIFAR100 (CIF100), and TinyImageNet (TinyIN). ResNet18 wasused to extract the vector representation of input images for DNDF (Kontschieder et al., 2015), DT,NBDT (Wan et al., 2020), RDT (Alaniz et al., 2021), and SONG. For DDN (Murthy et al., 2016),DCDJ (Baek et al., 2017), and ANT-A (Tanno et al., 2019), the backbone models are provided inthe brackets. “Ex?” indicates if the method retains interpretable properties such as pure leaves,sequential decisions, and non-ensemble. “SO?” indicates if the model is self-organized (does notrequire a predefined structure). “EE?” indicates if the structure and weights of model are trained inan end-to-end continuous manner.
Table 2: Comparison of SDT (Frosst & Hinton, 2017) and shallow SONG (SONG-S) on three datasets,where shallow corresponds to direct flattened inputs (no backbone network used). The accuracyof each model is reported along with the number of internal nodes specified in the parentheses.
Table 3: Results of SONG in a deep learning setup. One can observe that for the MNIST dataset (a),the performance increases with the increasing number of nodes and steps. In contrast to CIFAR10 (b),where the performance is relatively similar for all combinations of the parameters. It can be causedby the smaller dimension of the representation vector in MNIST (50) than in CIFAR10 (512).
Table 4: Results obtained for selected models from Table 3 (“base”) and their finetuned versionsWe analyze two types of finetuning, either by using basis weights and finetune all the parameters ofthe network (“finetune”) or by taking the graph structure from the base model, reset other networkparameters, and train the network from scratch (“reset”). One can observe that there is no obviouswinning strategy, and it should be considered a hyperparameter. Notice also that we bold the(b) CIFAR10.
Table 5: SONG as a shallow model (SONG-S). One can observe that the performance increases withthe increasing number of nodes and steps for all datasets. We bold the performance reported in themain paper.
