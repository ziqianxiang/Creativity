Table 1: CIFAR10 test accuracy for subnetworks derived from dense networks with varying pre-training amounts (i.e., number of training iterations listed in top row) and sub-dataset sizes.
Table 2: Test accuracy on ImageNet of subnetworks with different FLOP levels derived from densemodels with varying amounts of pre-training (i.e., training epochs listed in top row). We report theFLOP/parameter ratio after pruning with respect to the FLOPS/parameters of the dense model.
Table 3:	Results of empirically analyzing whether y ∈ MN. For each of the possible datasets andsizes, we report the number of training epochs required before the assumption was satisfied for thebest possible learning rate setting.
Table 4:	Displays the hidden dimension of the two-layer neural network used to test the y ∈ MNassumption for different dataset sizes. Hidden dimensions are the same between tests with MNISTand CIFAR10.
