Table 1: Throughput vs. FLOPs. We prune LV-ViT-M in different orthogonal dimensions. Itshows that token slimming achieves the highest throughput at the same FLOPs.
Table 2: Main results on ImageNet. We apply our self-slimming learning on the state-of-the-artvanilla vision transformer LV-ViT (Jiang et al., 2021). Υ means we adopt an extra CNN teacher.
Table 3: Comparison to the state-of-the-art on ImageNet. * denotes the models are trained for 300epochs for a fair comparison. Our SiT achieves the best balance between throughput and accuracy.
Table 4: Ablation studies. If not otherwise specified, all experiments for ablations are conductedon SiT-Ti and run with only 125 training epochs under the supervision of our DKD.
Table 5: More results on DeiT. “DeiTP” indicates the original DeiT and “DeiTC” refers to thevariant with lightweight convolutional patch embedding stacked by four 3×3 convolutions (2×2stride) and one point-wise convolution.
Table 6: Comparisons between DynamicViT and our SiT on DeiT.
