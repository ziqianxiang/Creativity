Table 1: Accuracy (%) (higher is better). Results for GEN-MIR and ER-MIR are taken from AljUndiet al. (2019a). Results for CN-DPM are taken from Lee et al. (2020). Results for GDumb are takenfrom Prabhu et al. (2020). For our models, means and standard deviations for 20 runs are shown.
Table 2: Forgetting (lower is better). Results for GEN-MIR and ER-MIR are taken from AljUndiet al. (2019a). Means and standard deviations for 20 runs are shown.
Table A3: HyperparametersParameter	ValueLearning rate	0.0001Weight decay	0.0001Ensemble size	1024k (top-k selection)	32Ï„ (tanh scaling factor)	250Runs per experiment	20ablations, these and all other hyperparameters, were used for all experiments described in the paper(Table A3). We found performance to be robust to hyperparameter variation, and we did not have totune them for different datasets or experimental settings.
Table A4: Ablations. Left: decreasing the ensemble size results in worse performance. Right:increasing k in top-k classifier selection results in worse performance (ensemble size is 1024). Allresults are final accuracy (%) over 20 runs for all ten classes.
Table A5: BYOL encoder results.
