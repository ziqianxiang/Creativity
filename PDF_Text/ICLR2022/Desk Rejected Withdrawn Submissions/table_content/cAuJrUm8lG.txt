Table 1: Performance scores for ResNet18 andits equivariant versions on Rotation (Rot-TIM)and Rotation+Reflection (R2-TIM) versions ofTinyImageNet (TIM) dataset. Here, CNN4-augand CNN8-aug denote regular CNN models sim-ilar in architecture to those of R4 and R8, respec-tively, at inference phase.
Table 2: Performance scores for VGG and itsequivariant versions on Rotation (Rot-TIM) andRotation+Reflection (R2-TIM) versions of Tiny-ImageNet (TIM) dataset. Here, CNN4-aug andCNN8-aug denote regular CNN models similarin architecture to those of R4 and R8, respec-tively, at inference phase.
Table 3: Results for Object Tracking. Scores arepresented for IEN implementations and two base-lines: SiamFC (Bertinetto et al., 2016) and RE-SiamFC (Gupta et al., 2021)Model	Channels%	Succ.	Pr.
Table 4: Performance scores for Heterogeneous IEN implementations on Rot-TIM dataset, createdthrough reducing the size of fraction of feature groups per layer of the network. For example, R4-2-1implies that R4 is modified such that 50% feature groups per layer are R4, 25% are R2, and rest areR1, thus reducing channels per layer to 69%.
Table 5: Performance scores of E2CNN(Weiler & Cesa, 2019), standard CNN and IEN on Rot-MNIST validation dataset along with extent of equivariance achieved in each conv layer as mea-sured by the equivariance loss for that layer.
Table 6: Details on the composition of E2CNN architectures based on ResNet18 model experi-mented in this paper. Here R4 and R8 denote equivariance to 4 and 8 equidistant orientations,respectively, and R4R denotes equivariance to 4 equidistant rotations and reflections. Numbers forconv layers denote channels per orientation. Channels per layer in all variant are chosen such thatthe total parameters in the model are same as the base ResNet18 modelLayer	kernel size	Resnet-18	E2CNN R4	E2CNN R8	E2CNN R4Rconv-1	3×3	64	34	24	24block-1	3×3	64	34	24	24	3×3	64	34	24	24block-2	3×3	128	68	48	48	3×3	128	68	48	48block-3	3×3	256	136	96	96	3×3	256	136	96	96block-4	3×3	512	272	192	192	3×3	512	272	192	192avg-pool	-	512	272	192	192fc-layer	-	100	100	100	100Table 7: Details on the composition of E2CNN architectures based on VGG model experimentedin this paper. Here R4 and R8 denote equivariance to 4 and 8 equidistant orientations, respectively,and R4R denotes equivariance to 4 equidistant rotations and reflections. Numbers for conv layers
Table 7: Details on the composition of E2CNN architectures based on VGG model experimentedin this paper. Here R4 and R8 denote equivariance to 4 and 8 equidistant orientations, respectively,and R4R denotes equivariance to 4 equidistant rotations and reflections. Numbers for conv layersdenote channels per orientation. Channels per layer in all variant are chosen such that the totalparameters in the model are same as the base VGG model.
Table 8: Details on the composition of E2CNN architectures based on ResNet18 model experi-mented in this paper. Here R4 and R8 denote equivariance to 4 and 8 equidistant orientations,respectively, and R4R denotes equivariance to 4 equidistant rotations and reflections. Numbers forconv layers denote channels per orientation. Channels per layer in all variant are chosen such thatthe total parameters in the model are same as the base ResNet18 modelLayer	kernel size	Resnet-18	E2CNN R4	E2CNN R8	E2CNN R4Rconv-1	3×3	64	34	24	24block-1	3×3	64	34	24	24	3×3	64	34	24	24block-2	3×3	128	68	48	48	3×3	128	68	48	48block-3	3×3	256	136	96	96	3×3	256	136	96	96block-4	3×3	512	272	192	192	3×3	512	272	192	192avg-pool	-	512	272	192	192fc-layer	-	100	100	100	100layers in case of VGG (conv1, conv3, conv5, conv7) are also reported in Table 11. Since theextent of equivariance is computed using the equivariance loss, lower values are better. The extentof equivariance observed in case of R8 in ResNet18 based E2CNN and standard CNN models is
Table 9: Details on the composition of E2CNN architectures based on VGG model experimentedin this paper. Here R4 and R8 denote equivariance to 4 and 8 equidistant orientations, respectively,and R4R denotes equivariance to 4 equidistant rotations and reflections. Numbers for conv layersdenote channels per orientation. Channels per layer in all variant are chosen such that the totalparameters in the model are same as the base VGG model.
Table 10: Classification error on Scale-MNIST and STL-10 datasets for our IEN and three baselinemethods, namely SS-CNN (Ghosh & Gupta, 2019), DSS (Worrall & Welling, 2019) and SESN(Sosnovik et al., 2020b). For Scale-MNIST, we use two variants: image sizes of 28 × 28 and56 × 56. All baseline implementations are based on the description provided in Sosnovik et al.
Table 11: Performance scores and equivariance loss achieved for ResNet18 and its equivariant ver-sions as well as VGG and its equivariant versions on Rotation (Rot-TIM) and Rotation+Reflection(R2-TIM) are shown. Here R4 and R8 denote equivariance to 4 and 8 equidistant rotations and R4Rdenotes equivariance to 4 equidistant rotations and reflections. The extent of equivariance achievedin the 4 conv blocks of ResNet18 and in 4 alternate conv layers in case of VGG are reported for eachmodel, lower values are better.
