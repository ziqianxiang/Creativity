Table 1: Example utterances from an instance of each grammar, for 4 different objects. Best viewedin color.
Table 2: Human evaluation results. Values are accholdoutWe measured subjects’ accuracy on the 3 held out examples. The results are shown in Table 2. ForSYNTH, accholdout was poor for all grammars: humans were unable to spot compositional formusing unfamiliar words. In ENG, accholdout was high for both CONCAT and SHUFDET grammars,7Under review as a conference paper at ICLR 2022Table 3: Compositional inductive bias for some standard Sender models, for natt = 5, nval = 10.
Table 3: Compositional inductive bias for some standard Sender models, for natt = 5, nval = 10.
Table 4: Effect of number of parameter on compositional inductive bias. Results are each averagedover 5 seeds. CI95 is in Table 13Model	Emb size	Params	PERM	PROJ	ROT	SHUFDET	HOL1-layer LSTM	128	139909	1	2.2	7	1.6	>201-layer LSTM	1280	13195525	0.94	1.6	2.7	1.4	>202-layer MLP	128	19428	1	2.1	>20	7	>202-layer MLP	1280	193380	1.02	1.86	>20	10	>20as expected, and low for all other grammars. This shows that the composition functions in perm,proj and rot were not clearly apparent to human subjects, even though, as we shall see next, neuralmodels can acquire these grammars easily.
Table 5: Zero-RNN improves bias against perm. Results are mean over 5 runs. CI95 Table 14.
Table 6: Selected results for low shufdet bias, mean over 10 runs. Full results Table 15.
Table 7: General hyper-parameters, for sender and receiver network experimentsSetting	ValueEmbedding size, demb	128Vocab size, V	4Utterance length, clen	4*nattDropout	0Gradient clipping	5.0Optimizer	AdamBatch size	128General hyper-parameters are shown in Table 7.
Table 9: Comparison of metric results for resent_resnick and resent_relax, using V = 2, natt = 2,nval = 5, clen = 10. No normalization. resent_relax= 1 - HCEGrammar	resent_ours	resent_resnick	resent_relaxCOMP	0.0000	0.0000	0.0000PERM	0.0000	0.0000	0.0000PROJ	0.0000	0.8468	0.8333SHUFDET	0.0000	0.4796	0.6452SHUF	0.0000	0.5600	0.5389ROT	0.0000	0.5510	0.5510HOL	0.0000	0.5616	0.5728Table 10: Comparison of metric results for resent_ours and resent_relax, using V = 4, natt = 3,nval = 4, clen = 6. Normalization enabled.
Table 10: Comparison of metric results for resent_ours and resent_relax, using V = 4, natt = 3,nval = 4, clen = 6. Normalization enabled.
Table 11: Example utterances for 4 objects, using n&tt = 5 and Clen = 4 ∙ n°tt∙	Objects (0, 0, 0, 0, 0)	(0, 0, 0, 0,1)	(0,0,0,1,0)	(0,0,1,0,0)concat	cdbacadcbcacbddacadb	cdbacadcbcacbddacadb	cdbacadcbcacbddacadb	cdbacadcbcacbddacadbperm	dabdccbdcacabddacabc	dabdccbdcacabddacabc	dabdccbdcacabddacabc	dabdccbdcacabddacabcrot	cbccaadbcaacdcbbddcd	cbccaadbcaacdcbbddcd	cbccaadbcaacdcbbddcd	cbccaadbcaacdcbbddcdproj	bbdaacdaadbbabbcbbcb	bbdaacdaadbbabbcbbcb	bbdaacdaadbbabbcbbcb	bbdaacdaadbbabbcbbcbshufdet	bcaccadcbddacdbacadb	bcaccadcbddacdbacadb	bcaccadcbddacdbacadb	bcaccadcbddacdbacadbhol	bdabaabbacacdbaabdcd	bdabaabbacacdbaabdcd	bdabaabbacacdbaabdcd	bdabaabbacacdbaabdcdTable 13 shows the CI95 ranges for the additional results shown in Table 4 (the additional rows inTable 4 were copied from Table 3).
Table 12: CI95 ranges for Sender model results.
Table 13: CI95 ranges for effect of number of parameters.
Table 14: CI95 ranges for performance of ZeroRNN.
Table 15: Full table for shufdet, including CI95 ranges.
Table 16: Relative compositional inductive biases for sender models, for natt = 5, nval = 10. ↑and J denotes columns We want to maximize or minimize respectively.
Table 17: Relative compositional inductive biases for receiver models, for natt = 5, nval = 10. ↑and J denotes columns we want to maximize or minimize respectively.
Table 18: Example results for measuring compositional inductive bias for end to end architectures,for natt = 5, nvai = 10. ↑ and J denotes columns We want to maximize or minimize respectively.
Table 19: Comparison of various RNN models, searching for low shufdet. Results are mean over10 runs.
Table 20: Compositional inductive bias estimated by accuracy after training for fixed number ofsteps. In red, results below 0.5, and in green, results over 0.8.
Table 21: Human evaluation results, synth datasetGrammar	N	t (mins)	score	accholdoutCOMP	15	14 ± 3	200 ± 100	0.6 ± 0.2PERM	18	30 ± 10	300 ± 100	0.2 ± 0.2PROJ	17	19 ±4	160±40	0.04 ± 0.08ROT	21	24 ± 5	170±30	0.02 ± 0.03SHUFDET	17	20 ± 8	240 ± 80	0.7 ± 0.2Table 22: Human evaluation results, eng datasetself-service training table, than to simply be measuring who had decided to write down a translationtable between object and code, e.g. on a piece of paper. However, both our provision of a trainingtable, and the possibility that subjects write down a translation table, means that there is a negligibledifference in scores across grammars, on the training data.
Table 22: Human evaluation results, eng datasetself-service training table, than to simply be measuring who had decided to write down a translationtable between object and code, e.g. on a piece of paper. However, both our provision of a trainingtable, and the possibility that subjects write down a translation table, means that there is a negligibledifference in scores across grammars, on the training data.
