Table 1: A comparison of memory-organization strategies of different Abc models. N denotes thesequence length, and n the memory size. φt denotes the memory control vector for kt and vt , andunif is the discrete uniform distribution.
Table 2: WikiText-103 language modeling perplexity (lower is better). n denotes the memory size.
Table 3: Machine translation test SacreBLEU. Left: sentence-level translation with WMT14 EN-DE;right: document-level translation with IWSLT14 ES-EN.
Table 4: Text classification development set accuracy. RoBERTa is the base-sized RoBERTa model,and its numbers are due to Liu et al. (2019). The second block continues pretraining RoBERTabased on our data with the MLM objective. Bold numbers perform the best among Abc models, andunderlined ones perform on par with or better than RoBERTa-Ours. Inference speed (higher is better)and memory consumption (lower is better) are relative to RoBERTa’s.
Table 5: ABCMLP's SacreBLEU onWMT14 EN-DE development data withvarying memory sizes of cross and causalattention. All models apply greedy decod-ing, without checkpoint averaging.
