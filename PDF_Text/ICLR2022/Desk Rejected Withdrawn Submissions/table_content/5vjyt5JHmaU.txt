Table 1: Comparison between various method	Satisfies constraints during training	Combines with any learning-based continuous-control algorithm	Never limit the exploration region	No limitation in dimensionADR(this paper)	X	X	X	XAchiam et al. (2017)	X	X	X	XDalal et al. (2018)	X	X	X	XReward shaping	X	X	X	Xstate space that the expert trajectory has not visited before, policy learned only from expert data mayperform poorly in these areas. And Fisac et al. (2019) propose a general safety framework basedon Hamilton-Jacobi reachability methods. This safety framework also can work in conjunctionwith any efficient learning algorithm. But this method is computationally intensive and limited indimension. Aswani et al. (2013) use the method about the robust model-predictive control approachand achieve good results in some problems such as quadrotor flight. But it limits the explorationability of the system. And Berkenkamp et al. (2016; 2017) both limit the exploration region ofthe method. The method in Sadraddini & Belta (2016) is conservative since it does not update themodel.
Table 2: Hyperparameters for experimentsHyperparameter	DDPG+ADRmaximum episode length	26learning rate for Adam optimizer	10-2discount factor	0.95batch size	1024number of units in the MLP	64number of hidden layers	2size of the replay buffer	106C Hyperparameters for ExperimentsThe hyperparameter settings of DDPG and DDPG+ADR are exactly the same, and there are noadditional parameters introduced. And in fact, there is no need to adjust the parameters in ourexperiment. Activation function for MLP is ReLU. Table 2 shows the hyperparameters used in theexperiment.
