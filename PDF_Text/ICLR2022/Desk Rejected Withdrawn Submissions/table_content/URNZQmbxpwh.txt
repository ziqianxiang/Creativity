Table 1: Invariance analysis at con-vergence in Colored MNIST acrossthe two training domains E ={90%, 80%}. Details in AppendixB.1 and B.2.2. Compared to ERM,Fishr enforces invariance in Hes-sians (Diag(H90%) ≈ Diag(H80%))by matching the gradient variance(Diag(C90% ) ≈ Diag(C80%)).
Table 2: Colored MNIST results. All meth-ods use hyperparameters optimized for IRM.
Table 3: Test-domain model selection. We format first, Second and worse than ERM results.
Table 4: Training-domain model selection. We format first, Second and worse than ERM results.
Table 5: Performances comparison on the linear dataset from (Shi et al., 2021)17Under review as a conference paper at ICLR 2022A.3.2 Theoretical analysis in the linear classifierSecond, we delve into the theoretical analysis of the Fishr regularization in a linear binary classifier,that leverages p features: these features are either fixed through learning (the linear setting), orpredicted from a trainable features extractor φ. We note zei ∈ Rp the features for the i-th examplefrom the domain e, ^ ∈ [0,1] the predictions after sigmoid and yle ∈ {0,1} the one-hot encodedtarget. The linear layer W is parametrized by weights {wk}pk=1 and bias b.
Table 6: Colored MNIST experimentswithout label flipping.
Table 7: Colored MNIST experiments with different statistics matched. All methods use hyperpa-rameters optimized for IRM.
Table 8: Hyperparameters, their default values and distributions for random search.
Table 9: Summary of the architectures used in DomainBed.
Table 10: Importance of the exponential moving average (ema) on DomainBed’s ColoredMNIST.
Table 11: Importance of the exponential moving average (ema) on DomainBed’s OfficeHome.
Table 12: Fishr (gradient covariance) vs. IGA (gradient mean) on DomainBed’s Colored MNIST.
Table 13: Fishr (gradient covariance) vs. IGA (gradient mean) on DomainBed’s OfficeHome.
Table 14: Impact of the λ distribution from Table 8.
