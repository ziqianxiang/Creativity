Table 2: Evaluation of delayed quantization on more tasks. Experiment settings and notation are explained in Section 4.
Table 1: Quantization-aware training withoutspecial initialization. Delayed Quantitation(Qd) vs Uniform Quantization (Qu).
Table 3:	Empirical analysis of "prune-then-quantize" paradigm and independence assumption. Pruning and quantizationare clearly not independent in (a), while "prune-then-quantize" may not be strictly required in (b), (c).
Table 4:	Switching the optimal order of pruning and quantization results in large performance degradation, whichempirically follows the non-commutativity hypothesis. The optimal order for each task is in bold text.
Table 5: Comparisons with previous pruning and quantization methods on image classification with Cifar10.
Table 6: Fr6chet Inception Distance (FID) of generated samples from Pix2Pix and CycleGAN to real images (smaller isbetter). The reciprocal of FID is used as domain-specific metric to calculate performance density.
