Table 1: Dataset statisticsDATASET	NODES	EDGES	CLASSES	FEATURES	E-DEN	N-DEGCora	2708	5429	7	1433	0.148%	4.01Citeseer	3327	4732	6	3703	0.086%	2.844	Related workThe efforts devoted to improving message propagation in GCNs generally fall into two categories:i)	Efforts towards building deep GCNs. A straightforward solution to realize long-range messagepropagation is to deepen GCNs. However, a challenge in deep GCNs is over-smoothing, whichwas first discussed in Li et al. (2018). To combat this problem, Xu et al. (2018) employed skipconnections for multi-hop message passing to build deep JK-network. Li et al. (2019) used residualconnections and dilated convolutions to deepen GCNs. Klicpera et al. (2019a;b) proposed a propa-gation scheme based on personalized Pagerank to aggregate information from a larger and adjustableneighbors. DAGNN (Liu et al., 2020) incorporates information from large receptive fields throughthe entanglement of representation transformation and propagation. GCNII (Chen et al., 2020) pre-vents over-smoothing by residual connections and identity mapping. Zhao & Akoglu (2020) addeda normalization layer into GCNs, by which they could stack more convolutional layers. Sun et al.
Table 2: Summary of results in terms of classification accuracy (in percent)DATASET	DeepWalk	GCN	SGC	GAT	AGNN	TAGCN	DGCN	OURSCora	67.2	81.5	81.0	83.0	83.1	83.3	83.5	83.5Citeseer	43.2	70.3	71.9	72.5	71.7	71.4	72.6	73.3F F F0123456789	012345&789	0	1	2	3	4	5	6	7	8	9end	end	end"TQjF"V∣aW⅞∣Fl Fl Fiend	end	endFigure 5: Accuracy for varying start and end on Cora (UP) and Citeseer (down) with training labelrate 1% (left), 2% (middle) and 5% (right).
