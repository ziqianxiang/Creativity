Table 1: Main findings regarding LS and KD compatibility in recent works and our work.
Table 2: Knowledge distillation results from ResNet-50 Teacher to ResNet-18 student (A) andResNet-50 student (B) following similar procedure as Shen et al. (2021b) on ImageNet-1K. We showthe top1/ top5 test accuracies. Configurations where LS and KD are compatible are in bold. As onecan clearly observe, with LS-trained teacher, there is a consistent degrade in student performanceas T increases. This can be observed in all our 28 experiments. These results comprehensivelysupport our claim: in the presence of a LS-trained teacher, KD at higher temperatures is renderedineffective. On the other hand, we observe that higher T can improve the performance when usinga teacher trained without LS in fine-grained classification and compact student network distillationexperiments (See Supplementary Tables 5 and 9). All these results are averaged over 3 independentruns. Standard deviations are reported in Supplementary Tables 10, 11 respectively.
Table 3: η analysis for ResNet-18 student for 10 target classes (We show in 2 sets). We use ImageNet hierarchyderived from WordNet (Fellbaum, 1998) to select 4 semantically similar classes and 20 semantically dissimilarclasses (random) to compute the diffusion index η. |S1 | = 4 and |S2| = 20 for each target class. Wedemonstrate that when increasing T = 1 to T = 3, the diffusion index η between target class and S1 reducessubstantially and vice versa for S2 shown for both training and validation set.
Table 4: η analysis for ResNet-50 student for 10 target classes (We show in 2 sets identical to ResNet-18student shown in Table 3). We use ImageNet hierarchy derived from WordNet (Fellbaum, 1998) to select 4semantically similar classes and 20 semantically dissimilar classes (random) to compute the diffusion indexη. |S1 | = 4 and |S2 | = 20 for each target class. We demonstrate that when increasing T = 1 to T = 64,the diffusion index η between target class and S1 reduces substantially and vice versa for S2 shown for bothtraining and validation set (for most target classes). More η analysis is included in Supplementary.
Table 5: Top1/ Top5 Accuracy with Standard deviations for Knowledge distillation results fromResNet-50 Teacher to ResNet-18 student on CUB200-2011, following the exact procedure asShen et al. (2021b). Configurations where LS and KD are compatible are in bold. As one canclearly observe, with LS-trained teacher, there is a consistent degrade in student performance as Tincreases. This can be observed in all our 28 experiments. These results comprehensively supportour claim: in the presence of a LS-trained teacher, KD at higher temperatures is rendered ineffective.
Table 6: Top1/ Top5 Accuracy with Standard deviations for Knowledge distillation results fromResNet-50 Teacher to ResNet-50 student on CUB200-2011, following the exact procedure asShen et al. (2021b). Configurations where LS and KD are compatible are in bold. As one canclearly observe, with LS-trained teacher, there is a consistent degrade in student performance as Tincreases. This can be observed in all our 28 experiments. These results comprehensively supportour claim: in the presence of a LS-trained teacher, KD at higher temperatures is rendered inef-fective. On the other hand, we observe that higher T can improve the performance when using ateacher trained without LS in fine-grained classification and compact student network distillationexperiments (See Supplementary Tables 5 and 9) These experiments are repeated for 3 independentruns and as you can observe the standard deviations are within acceptable range.
Table 7: BLEU scores with Standard deviations for Knowledge distillation results from Trans-former Teacher to Transformer student on IWSLT dataset using English-German translationtask, following the similar procedure as Shen et al. (2021b). Configurations where LS and KD arecompatible are in bold. As one can clearly observe, with LS-trained teacher, there is a consistentdegrade in student performance as T increases. This can be observed in all our 28 experiments.
Table 8: BLEU scores with Standard deviations for Knowledge distillation results from Trans-former Teacher to Transformer student on IWSLT dataset using English-Russian translationtask, following the similar procedure as Shen et al. (2021b). Configurations where LS and KD arecompatible are in bold. As one can clearly observe, with LS-trained teacher, there is a consistentdegrade in student performance as T increases. This can be observed in all our 28 experiments.
Table 9: Top1/ Top5 Accuracy with Standard deviations for Knowledge distillation results fromResNet-50 Teacher to MobileNetV2 student on CUB200-2011. Configurations where LS and KDare compatible are in bold. As one can clearly observe, with LS-trained teacher, there is a consistentdegrade in student performance as T increases. This can be observed in all our 28 experiments.
Table 10: Knowledge distillation results from ResNet-50 Teacher to ResNet-18 student with stan-dard deviations, following similar procedure as Shen et al. (2021b) on ImageNet-1K (Deng et al.,2009). We show the top1/ top5 test accuracies. Configurations where LS and KD are compatible arein bold. As one can clearly observe, with LS-trained teacher, there is a consistent degrade in studentperformance as T increases. This can be observed in all our 28 experiments. These results com-prehensively support our claim: in the presence of a LS-trained teacher, KD at higher temperaturesis rendered ineffective. On the other hand, we observe that higher T can improve the performancewhen using a teacher trained without LS in fine-grained classification and compact student networkdistillation experiments (See Supplementary Tables 5 and 9) All these results are averaged over 3independent runs. The standard deviations are reported in Supplementary Tables 10 and 11 respec-tively. These experiments are repeated for 3 independent runs and as you can observe the standarddeviations are within acceptable range.
Table 11: Knowledge distillation results from ResNet-50 Teacher to ResNet-50 student with stan-dard deviations, following similar procedure as Shen et al. (2021b) on ImageNet-1K (Deng et al.,2009). We show the top1/ top5 test accuracies. Configurations where LS and KD are compatible arein bold. As one can clearly observe, with LS-trained teacher, there is a consistent degrade in studentperformance as T increases. This can be observed in all our 28 experiments. These results com-prehensively support our claim: in the presence of a LS-trained teacher, KD at higher temperaturesis rendered ineffective. On the other hand, we observe that higher T can improve the performancewhen using a teacher trained without LS in fine-grained classification and compact student networkdistillation experiments (See Supplementary Tables 5 and 9). These experiments are repeated for 3independent runs and as you can observe the standard deviations are within acceptable range.
Table 12: Consistency measurements between using pre-defined knowledge graph for ImageNet-1Kas prior vs. feature space distance method for identifying semantically similar / dissimilar classes.
Table 13: This table shows the degree of smoothness as measured by average entropy using thetraining set of CUB200-2011 at different temperatures for normally trained ResNet-50 teacher andLS-trained ResNet-50 teacher. Do note that this analysis is done using CUB200-2011. We makeimportant observations regarding the smoothness of the targets produced by LS-trained teachersand teachers training without LS. (1) As one can observe, at T = 1, LS-trained teacher producessmoother targets compared to the normal teacher. (2) As T increases, the targets become smoother.
Table 14: Results of case study at loWer T With same degree of smoothness. In Counterexample #1,Teacher is ResNet-50, Student is ResNet-50. Two a∕T configurations have been identified such thataverage entropy of the teachers’ output are the same (0.888). We clearly observe different perfor-mances for Student. Similarly, in Counterexample #2, Teacher is ResNet-50, Student is ResNet-18and we clearly observe different performances for Student. For each counterexample, the higher KDperformance is in bold. Through these 2 counterexamples, we show that even at the same degreeof smoothness, distilling from LS-trained teachers produces better students compared to distillingfrom normally-trained teachers at lower T due to lower degree of systematic diffusion (LS and KDare compatible).
Table 15: Results of case study at moderately higher T with same degree of smoothness. InCounterexample #3, Teacher is ResNet-50, Student is ResNet-18. TWo a∕T configurations havebeen identified such that average entropy of the teachers’ output are the same (5.188). We clearlyobserve different performances for Student. Similarly, in Counterexample #4, Teacher is ResNet-50, Student is MobileNetV2 and We clearly observe different performances for Student. For eachcounterexample, the higher KD performance is in bold. Through these 2 counterexamples, We shoWthat even at the same degree of smoothness, distilling from LS-trained teachers produces poorerstudents compared to distilling from normally-trained teachers. This is due to increased degree ofsystematic diffusion as T increases in the presence of LS-trained teachers, thereby producing poorstudents (LS and KD are incompatible).
Table 16: Results of case study at extremely high T with same degree of smoothness. In Coun-terexample #5, Teacher is ResNet-50, Student is ResNet-18. TWo a∕T configurations have beenidentified such that average entropy of the teachers’ output are the same (5.298). We clearly observedifferent performances for Student. Similarly, in Counterexample #6, Teacher is ResNet-50, Stu-dent is ResNet-50 and We clearly observe different performances for Student. In Counterexample#7, Teacher is ResNet-50, Student is MobileNetV2 and We clearly observe different performancesfor Student. For each counterexample, the higher KD performance is in bold. Through these 3counterexamples, We shoW that even at the same degree of smoothness, distilling from LS-trainedteachers produces extremely poorer students compared to distilling from normally-trained teachers.
Table 17: The table shows the class-wise accuracies for the 3 classes used in Fig 1 (penultimatelayer visualization). As one can observe, in the presence of an LS-trained teacher, KD at highertemperatures causes systematic diffusion thereby rendering KD ineffective. We can see this formost classes at increased temperatures shown below. That is, in the presence of a LS-trained teacheras we increase the temperature from T = 1, the accuracies for most of these classes drop dueto systematic diffusion. This can be seen in both training and validation sets. Do note that sincethe validation set contains only 50 samples per class, class wise validation accuracies may not bestatistically reliable and contain outlier points, and we suggest observing the general trend as shownby the average for the set.
Table 18: The table shows the class-wise accuracies for the 5 targets classes used in our systematicdiffusion analysis (η calculation as shown in 3). As one can observe, in the presence ofan LS-trainedteacher, KD at higher temperatures causes systematic diffusion thereby rendering KD ineffective.
Table 19: The table shows the class-wise accuracies for the 5 targets classes used in our systematicdiffusion analysis (η calculation as shown in 3). As one can observe, in the presence ofan LS-trainedteacher, KD at higher temperatures causes systematic diffusion thereby rendering KD ineffective.
Table 20: The table shows results of additional exploration of α and T. CUB200-2011 dataset isused for these experiments.
Table 21: Results of using alternative distance, i.e., pairwise distance, to define the diffusion indexηpairwise . The findings are consistent with using alternative distance.
