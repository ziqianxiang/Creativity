Table 1: A comparison of our method to baselines on key elements. We assume an input resolu-tion of 224 × 224, as used for ImageNet (see Tab. 2 for more details). We consider the Patch Sizeused for the division of the image, type of patch Embedding, number of tokens (#Tokens) result-ing from the patch embedding (for the first level of the hierarchy), and number of levels (#Levels)used in the hierarchy. We consider the Complexity of the attention step in the number of tokensas well as the Region on which attention is applied - either over a local region or over the entireimage (global). B stands for the number of tokens. In our method, we also consider T variantsfor each patch, which results in a total of B × T tokens. CvT* considers overlapping patches (astride of 4 is used) and so the resulting number of tokens is large with respect to patch size. InNeSTt and Swin* attention is applied only to a local number of patches, so the number of tokensis small with respect to patch size.
Table 2: Architectures used for images with input resolution 224 × 224 (ImageNet) and for inputresolution 32 × 32 (CIFAR10 and CIFAR100) for three model types: tiny, small and base. In step 1,we begin by applying a convolutional embedding (Conv emb.) to the input image. A kernel sizeK , stride S and padding P are used and the output has E1 channels. A separate convolutionalembedding is performed for each of the T image shifting variants (indicated beside the curlybracket) and positional encoding is added for each patch and variant embedding. Subsequently,we project the image (Proj.) into a three dimensional tensor, which is then used as input for ourlocal attention block (Local Att.). From step 2 onward, we begin by applying a global attentionblock (Glob Att.). We subsequently un-project the input and apply a downsampling operation(Down), as described in Sec. 3.2, where the resulting dimension is Ei. Hi denotes the number ofheads used. A projection layer is then applied, converting the input into a two-dimensional inputthat can be used for subsequent global attention blocks. Each such step (Global Att. - Down. -Proj.) may be applied a number of times, as indicated by the number beside the curly bracket.
Table 3: Classification accuracy on CIFAR10 and CIFAR100. The number of parameters (in mil-lions), and inference throughput (images per second) on a single GPU are shown.
Table 4: Classification accuracy on the ImageNet validation set. The number of parameters (inmillions), GFLOPs, and inference throughput (images per second) on single GPU are shown.
Table 5: Ablation analysis for examining the contribution of: (1) shifting variants, (2) numberof shifting variants, (3) shifting variants generated using learned convolutional filters, and, (4)employing different positional embeddings for each shifting variant.
