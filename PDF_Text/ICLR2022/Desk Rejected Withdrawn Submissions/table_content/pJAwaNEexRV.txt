Table 1: Results on the UCI datasets (M = 8). The Diabetes and Boston Housing (regression) areevaluated with MAD, and the rest (classification) are evaluated with Accuracy (in percentage).
Table 2: Results on the MNIST (M = 8), CIFAR10 (M = 8), MIMIC3, and ModelNet40 datasets.
Table 3: Comparison between GAL and state-of-the-art methods. In the table, M represents the num- ber of organizations. The performance metrics are MAD for MIMIC3 and Accuracy for ModelNet40. AL requires M times more computation time and communication rounds because it sequentially trains each organization. VAFL requires far more communication rounds because it requires batch-wise		updates from the server. The	communication cost	is O(d ∙ Ei) for VAFL and O(K ∙ T) for GAL,where d, Em, K, T represent the size of feature embeddings, the total number of training epochs of organization m, the size of target labels, and the number of assistance rounds, respectively. Here, d is typically greater than K, and Em is greater than T because GAL allows more local training. Training Epochs Computation Computation Communication Communication DataSet	MethOd	MT	EmP	Time	SpaCe	Round	Cost	ReSUIt Benchmark (Harutyunyan et al., 2019)	1	N/A	100	1 ×	1	×	0	0	×	94.7 MIMIC3	AL (Xian et al., 2020)	4	10	100	4	×	10	×	40	1	×	104.1 GAL	4	10	100	1	×	10	×	10	1	×	96.4 MVCNN (Suetal., 2015)	1	N/A	100	1 ×	1	×	0	0	×	88.1 ModelNet40	VAFL (Chen et al., 2020)	4	N/A	100	1	×	1	×	10000	128	×	81.0 AL (Xian et al., 2020)	12	10	100	12 ×	10	×	120	1	×	77.3 GAL	12	10	100	1	×	10	×	10	1	×	83.0		8Under review as a conference paper at ICLR 2022GAL vs. VAFL Vertical federated learning method such as VAFL (Chen et al., 2020) andSplitFed (Thapa et al., 2020) can be viewed as federated learning with intermediate data fusion. Theytypically require frequent synchronized communications of hidden representations and gradientsto optimize a global model in hindsight. In particular, VAFL allocates separate convolution layersfor each organization and transmits hidden representations to the server. Compared with vAfl,our method GAL outperforms VAFL in the following aspects. VAFL computes and transmits the(gradients of) hidden representations for every batch-wise update between server and clients. Thebatch size of VAFL is 0.01 of the whole dataset. Consequently, 100 training epochs would result in10000 communication rounds for each organization. On the contrary, GAL trains one local model formultiple epochs at each communication round. Each model solves a small part of the overarchingloss function (namely, the pseudo-residual multiplied by the gradient assisted learning rate). 1) GALallows each participant to use its own local model architecture autonomously, while VAFL requiresall participants to fit with deep learning model architecture. 2) Under the constraint of the samenumber of local training epochs, GAL requires a much fewer number of communication rounds thanVAFL to achieve satisfactory performance. 3) GAL avoids sharing true labels and objective functions,
Table 4: Detailed statistics used in each data experiment. The variables d and K respectivelydenote the number of features (or the shape of the image) and the length of the prediction vector (orequivalently, the number of classes in the classification task).
Table 5: The model architecture of Convolutional Neural Networks (CNN) used in our experimentsof the MNIST, CIFAR10, and ModelNet40 datasets. The nc, H, W represent the shape of images,namely the number of image channels, height, and width, respectively. K is the number of classes inthe classification task. The BatchNorm and ReLU layers follow Conv(output channel size, kernelsize, stride, padding) layers. The MaxPool(output channel size, kernel size) layer reduces the heightand width by half.	Image X ∈ Rnc×H×W Conv(64, 3,1,1) MaxPool(64, 2) Conv(128, 3,1,1) MaxPool(128, 2) Conv(256, 3, 1, 1) MaxPool(256, 2) Conv(512, 3, 1, 1) MaxPool(512, 2) Global Average Pooling Linear(512, K)Table 6: Hyperparameters used in our experiments for training local models, gradient assisted learningrates, and gradient assistance weights.
Table 6: Hyperparameters used in our experiments for training local models, gradient assisted learningrates, and gradient assistance weights.
Table 7: Results from the UCI datasets (M = 2). Diabetes and Boston Housing (regression) areevaluated with MAD and the rest (classification) are evaluated with Accuracy.
Table 8: Results from the MNIST (M = 2) and CIFAR10 (M = 2) datasets.
Table 9: Results from the UCI datasets (M = 4). Diabetes and Boston Housing (regression) areevaluated with MAD and the rest (classification) are evaluated with Accuracy.
Table 10: Results from the MNIST (M = 4) and CIFAR10 (M = 4) datasets.
Table 11: Ablation study (M = 2) of gradient assistance weights by adding noises to the transmittedpseudo-residuals to half of the organizations. The evaluation metrics are the same as Tables 1 and 2.
Table 12: Ablation study (M = 4) of gradient assistance weights by adding noises to the transmittedpseudo-residuals to half of the organizations. The evaluation metrics are the same as Tables 1 and 2.
Table 13: Ablation study (M = 8) of gradient assistance weights by adding noises to the transmittedpseudo-residuals to half of the organizations. The evaluation metrics are the same as Tables 1 and 2.
