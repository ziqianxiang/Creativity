Table 1: Average returns of halfcheetah-jump and ant-angleand average success rate of sawyer-door-close that require out-of-distribution generalization. All results are averaged over 6 random seeds.
Table 2: Results for vision experiments. For the Walker task eachnumber is the normalized score proposed in Fu et al. (2020) of the policyat the last iteration of training, averaged over 3 random seeds. Forthe Sawyer task, we report success rates over the last 100 evaluationruns of training. For the dataset, M refers to medium, M-R refers tomedium-replay, and M-E refers to medium expert.
Table 3: Results for D4RL datasets. Each number is the normalized score proposed in Fu et al. (2020) of thepolicy at the last iteration of training, averaged over 3 random seeds. We take results of MOPO, MOReL andCQL from their original papers and results of other model-free methods from (Fu et al., 2020). We include theperformance of behavior cloning (BC) for comparison. We bold the highest score across all methods.
Table 4: We include our automatic hyperparameter selection rule of β on a set of representative D4RLenvironments. We show the policy performance (bold with the higher number) and the regularizer value (boldwith the lower number). Lower regularizer value consistently corresponds to the higher policy return, suggestingthe effectiveness of our automatic selection rule.
Table 5: We include our automatic hyperparameter selection rule of μ(a∣s) on the medium datasets in thehopper and walker2d environments from D4RL. We follow the same convention defined in Table 4 and find thatour automatic selection rule can effectively select μ offline.
Table 6: We include our automatic hyperparameter selection rule of ρ(s) on the medium datasets in the hopperand walker2d environments from D4RL. We follow the same convention defined in Table 4 and find that ourautomatic selection rule can effectively select ρ offline.
Table 7: We include our automatic hyperparameter selection rule of f on the medium datasets in the hopperand walker2d environments from D4RL. We follow the same convention defined in Table 4 and find that ourautomatic selection rule can effectively select f offline.
Table 8: Comparison between COMBO and CQL+MBPO on tasks that require out-of-distribution generaliza-tion. Results are in average returns of halfcheetah-jump and ant-angle and average success rate ofsawyer-door-close. All results are averaged over 6 random seeds, ± the 95%-confidence interval.
