Table 1: Structural pruning results on ImageNet-1K. Our pruned models are reported in Novel ViT Pruning(NVP) lines. We compare the parameters and FLOPs count (and compression ratio), run time speedup andaccuracy of different models. NVP models are compared with DEIT (Touvron et al., 2021), SWIN (Liu et al.,2021), T2T-ViT (Yuan et al., 2021), AutoFormer (Chen et al., 2021a) and SViTE (Chen et al., 2021b). Allcompression ratios and speedups are computed with respect to that of DEIT-Base model. Latency of NVP areestimated on a single GPU with batch size 256. “ASP” stands for post-training 2:4 Ampere sparsity pruning,enabling 2× parameter reduction and 2× throughput on linear operations with TensorRT (Mishra et al., 2021).
Table 2: NViT block dimensions. For comparison the dimensions of a DEIT block are also listed.
Table 3: Comparing NViT models with DEIT model family. All compression ratios and speedups arecomputed with respected to that of the DEIT-Base model. Latency was estimated on an RTX 2080 GPU. DEITaccuracy marked with * indicates the train-from-scratch accuracy we achieve from the DEIT GitHub repo1 usingdefault hyperparameters2, which is slightly lower than the reported accuracy in DEIT paper. All pairs of modelsare trained from scratch with the same hyperparameters from the official DEIT-B script.
Table 4: Transfer learning tasks performance with ImageNet pretraining. We report the Top-1 accuracy offinetuning the ImageNet trained models on other datasets. DEIT-B (Touvron et al., 2021) and ViT-B (Dosovitskiyet al., 2020) results as reported by Touvron et al. (2021) are provided for reference. Speedup evaluated on RTX2080 GPU with respect to DEIT-B.
Table 5: Pruning configurations and remained dimensions for models reported in Table 1. Thereported dimensions are averaged across all the blocks.
Table 6: Datasets used for downstream task experiments.
Table 7: NVP-T model finetuning accuracy with different objectives.
Table 8: Iterative pruning single component to targeted percentage.
Table 9: Pruned model with different head Taylor importance manipulation. The dimension remainedis averaged across all attention blocks. The latency is estimated on a single RTX 2080 GPU withbatch size 256. We report the accuracy of the pruned model without any further finetuning.
