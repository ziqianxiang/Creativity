Table 1: Main results. Comparison with competitive post hoc out-of-distribution detection methods. Allmethods are based on a discriminative model trained on ImageNet, without using any auxiliary outlier data. ↑indicates larger values are better and ] indicates smaller values are better. All values are percentages. Boldnumbers are superior results. § indicates model retraining using a different loss function is required. ? indicates aseparate binary classifier needs to be trained for OOD detection. We report standard deviations estimated across5 independent runs.
Table 3: Comparison With generative-based modelsfor OOD detection. Models are trained on CIFAR-10.
Table 2: Ablation results. Comparison of different OOD detection baselines and sparsification method. Allsparsification methods are based on the same OOD scoring function (Liu et al., 2020), With sparsity parameterP = 0.9. ↑ indicates larger values are better and ] indicates smaller values are better. All values are percentagesand are averaged over multiple OOD test datasets. Bold numbers are superior results. We report standarddeviations estimated across 5 independent runs.
Table 4: Effect of varying sparsity parameter p duringinference time. Model is trained on CIFAR-100 usingDenseNet101 (Huang et al., 2017). A similar trend isobserved for other ID datasets as shown in Appendix A.
Table 5: Ablation on different strategies of choosing a subset ofunits. Values are FPR95 (averaged over multiple test datasets).
Table 6: Difference between the mean of ID’s output and OOD’s output. Here we use CIFAR-100 as ID dataand Ein [fcDICE] - Eout [fcDICE] is averaged over six common OOD benchmark datasets described in Section 4.
Table 7: Effect of varying sparsity parameter p on CIFAR-10 and ImageNet. Results are averaged on the testdatasets described in Section 4.
Table 8: Main comparison results with ResNet-101. Comparison with competitive post hoc out-of-distribution detection methods. All methods are based on a discriminative model trained on ID data only,without using any auxiliary outlier data. ↑ indicates larger values are better and ] indicates smaller values arebetter. All values are percentages and are averaged over six OOD test datasets.
Table 9: Effect of applying DICE with MSP on DenseNet101 pretrained on CIFAR-10. The number is reportedin FPR95.
Table 10: Comparison of different post hoc sparsification method. All sparsification methods are based onthe same OOD scoring function (Liu et al., 2020), with optimal sparsity parameters tuned for each methodindividually. ↑ indicates larger values are better and J indicates smaller values are better. An values arepercentages and are averaged over multiple OOD test datasets.
