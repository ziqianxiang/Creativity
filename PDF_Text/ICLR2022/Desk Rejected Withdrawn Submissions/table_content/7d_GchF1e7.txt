Table 1: Results of full dense BERTLARGE baselines and our variance-pruned 50% sparse model(VP), on the GLUE and SQuAD 1.1 tasks. Baseline results are reported both from the original paperDevlin et al. (2018) and when running the HuggingFace code. Our results are based on the samepre-trained sparse model of total 50% zeroed weights on all tasks. Scores that have parenthesisedcounterparts inside the cell are achieved with further distillation; the ones inside the parenthesis arewithout distillation.
Table 2: GPU memory usage (GB) and computa-tional efficiency (iterations per second) when fine-tuning BERTLARGE on SQuAD 1.1. We comparethe full dense model with the densified 50%-sparsemodel, with batch sizes ranging from one to fourand a common fixed sequence length of 384. Fora batch size of four, the full dense model goesout-of-memory.
