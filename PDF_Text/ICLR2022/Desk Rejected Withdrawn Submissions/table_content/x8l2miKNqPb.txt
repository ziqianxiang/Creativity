Table 1: Indicator updating process.
Table 2: Comparison with joint models between Our Framework and BaselinesDataset	Baseline	P	R	F	(Eberts & Ulges, 2020)	48.89	39.96	43.23	(BekoUlis et al., 2018)	45.30	34.45	38.30	(Zhong & Chen, 2021)	-	-	50.10Qr'i(2*-r∙r' Scierc	(Luan etal., 2018)	47.60	33.50	39.30	(Shenetal., 2021)	52.63	52.32	52.44	(Luan etal., 2019)		-	41.6	0-1 EMPN(our framework; e=3)	56.84	49.77	52.49	0-1 EMPN (our framework; e=2; order loss)	51.50	54.51	52.51	(Eberts & Ulges, 2020)	74.11	68.69	71.16	(BekoUliS et al., 2018)	72.99	63.67	67.01	(Shen et al., 2021)	73.01	71.63	72.35Conll2004	(Adel & SchUtze, 2017)	-	-	62.50	0-1 EMPN(our framework; e=3)	59.56	73.41	64.59	0-1 EMPN(our framework; e=2; order loss)	60.20	65.34	62.39	0-1 EMPN(our framework; e=7; excluded)	83.65	81.28	81.72	(EbertS & Ulges, 2020)	33.03	41.86	36.79Wiki80	(BekoUliS et al., 2018)	71.34	31.28	43.49	0-1 EMPN(our framework; e=7)	74.79	74.38	74.02
Table 3: Comparison with graph models and sequence models between Our Framework and Base-linesDataset	Baseline TyPe	Baseline	P		R		FScierc	Graph models	(Zhang etal., 2018) (Guo et al.,2019)	69.60	- 72.60	65.3 70.70	Sequence models	(Veyseh et al., 2020) (Zhang et al., 2017) (Zhou et al., 2016) EMPN(our framework; e=5) EMPN (our framework; e=5; order loss)	67.58 63.96 71.19 75.00	- 63.9 61.93 72.27 71.48	69.92 65.06 61.03 71.49 72.80	Graph models	(GUo et al.,2019)	92.42	92.18	92.30Conll2004		(ZhoU et al., 2016)	71.26	71.70	70.96	Sequence models	EMPN(our framework; e=2)	93.97	93.81	93.85		EMPN(our framework; e=3; order loss)	92.98	93.10	93.00	Graph models	(GUo et al.,2019)	61.00	52.70	56.60		(Zhang et al., 2017)	71.26	71.70	70.96Wiki80	Sequence models	(Zhou et al., 2016)	75.07	74.85	74.48		EMPN(our framework; e=2)	75.96	75.48	75.35		EMPN(our framework; e=10; order loss)	75.85	75.14	74.83the encoder contain key factors in prediction. They represent the information gathered fromother words to entity mentions.
Table 4: Ablation study on the test set of SciERCExperiment Group	Modified mode	Macro-F	Micro-F(i)	Hi Hj L, U	■68:09	■78:95(ii)	fs, fo, L, U	一	^49^	■64:68(iii)	fs, fo, H_i, Hj		■70:48	ɪɪɪ(iv)	f-s, fo, H-i, Hj L	^6941	78855(v)	fs, fo, Hi, Hj, U	一	^6973	-80.29(vi)	fs, f_o, H_i, Hj L, U With output layer attention	70.22	80.704.5	BERT MODEL STUDYWe replace all encoders in our framework with a BERT model (Devlin et al., 2019) to verify per-formance. The BERT model encodes entity vectors, origin text tokens, and selected trigger tokens.
Table 5: BERT Model study on the test set of SciERCExperiment Group	Modified mode	Macro-F	Micro-F⑴	BERT model with our framework(e=5)	77.65(e=5)	85.43(e=5)(ii)	BERT model with our framework(e=4)	78.82(e=4)	85.22(e=4)(Veyseh et al., 2020)	BERT model with Veysehs work	78.24	-4.6	Case StudyWe output the relation triggers to observe the change of them in different type of relation. The wordsin trigger are generated step by step so that they are in order. EMPN selects the maximal probabilityword vectors with head and tail in order to bridge them by word paths. We conduct the analysiswith a number of right and wrong cases and show 4 of them in figure 3 and figure 4. We choose thetriggers generated in SciERC dataset with e=5 with order loss and exhibit them in the grids.
