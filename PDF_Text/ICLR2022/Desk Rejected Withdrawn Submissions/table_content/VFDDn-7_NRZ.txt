Table 1: Results under different numbers N of naiverecursive operation on ImageNet-IK dataset.
Table 3: Effectiveness of various designs on ImageNet-1K			The overview of our ablation studies isval set. Please refer to Sec. 5.3 and Appendix E for more			shown in Table 3. The first group is thedetails. In this ablation study, the backbone is SReT-TL			baseline and different structures, whichmodel using spatial pyramid architecture.			are pointed by the used factors. We also	#Params (M)	Top-1 (%)	verify the following designs.
Table 4: Comparison of the Top-1 accuracy on ImageNet-1K with state-of-the-art methods. > denotesthat the model is trained without the proposed group self-attention approximation.
Table 5: More ablation results on different group designs.
Table 7: Details of soft distillation training.
Table 6: Details of conventional training.
Table 8: Details of higher-resolution finetuning.
Table 9: Training details of our language models. The architectures we used are in Fairseq [FAIR].
Table 10: SReT architectures (Input size is 3×224 × 224, sliced group is not included for simplicity.)Layers		Output Size	SReT-T	SReT-TL							Stem	Conv-BN-ReLU	32×112×112	3×3 conv, stride 2	3×3 conv, stride 2								Conv-BN-ReLU	64x56x56	3×3 conv, stride 2	3×3 conv, stride 2								Conv-BN-ReLU	64×28×28	3x3 conv, stride 2	3x3 conv, stride 2							Recursive T Block (1)		64x28x28	64-dim MHSA 3.6xFFN/1.0xNLL	x2	x2		64-dim MHSA 4.0xFFN/1.0xNLL	x2	x2Conv-Pooling Layer (1)		128x14x14	3x3 conv, stride 2, group 64	3x3 conv, stride 2, group 64							Recursive T Block (2)		128x14x14	128-dim MHSA 3.6xFFN/1.0xNLL	x5	x2		128-dim MHSA 4.0xFFN/1.0xNLL	x5	x2Conv-Pooling Layer (2)		256x7x7	3x3 conv, stride 2, group 128	3x3 conv, stride 2, group 128							Recursive T Block (3)		256x7x7	256-dim MHSA 3.6xFFN/1.0xNLL	x3	x2		256-dim MHSA 4.0xFFN/1.0xNLL	x3	x2Global Average Pooling		256 x 1 x 1	AdaptiveAvgPool	AdaptiveAvgPool							Linear Layer		1000							#Params (M)		4.8M				5.0M			Accuracy (%)		761				768			Distilled Accuracy (%)		777				779			Finetuning Accuracy ↑384 (%)		797				800			Layers		Output Size	SReT-S	Output Size	SReT-B							Stem	Conv-BN-ReLU	63x 112x 112	3x3 conv, stride 2	96x112x112	3x3 conv, stride 2								COnV-BN-ReLU	126x56x56	3x3 conv, stride 2	168x56x56	3x3 conv, stride 2								COnV-BN-ReLU	126x28x28	3x3 conv, stride 2	336x28x28	3x3 conv, stride 2							
Table 11: Ablation study on different LRC designs.
Table 12: More ablation results on directly expanding depth of baseline DeiT model. * the totalnumber layers of our network is 20 (recursive transformer blocks) + 10 (NLL) + 3 (image patchembeddings). Permutation and inverse permutation layers are not included.
