Table 1: Experimental results on IWSLT’14 De-En translation task. ‘BPE’ denotes the number oftokens in the dictionary of encoder and decoder (encoder/decoder). ‘Ours’ denotes the model istrained by ourselves. ‘Params’ denotes the number of parameters in model.
Table 2: Experimental results on WMT’14 En-De translation task. DMA with nc = 4 is used.
Table 3: Experimental results on WMT’17 Zh-En translation task with very different languages.
Table 4: Number of sentence pairs in different translation tasks.
Table 5: Model configurations of DMA transformer in different tasks. Embed and FFN denotes thedimension of hidden layer in attention and feed-forward layers, respectively. Iterations denotes thenumber of parameters updates.
Table 6: Ablation study of dictionary size on WMT’14 En-De translation task.
Table 7: Ablation of cluster numbers nc on IWSLT’14 De-En translations task.
