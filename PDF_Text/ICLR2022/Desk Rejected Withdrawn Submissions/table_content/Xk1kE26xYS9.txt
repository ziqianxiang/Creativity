Table 1: Results summary for the pixel observations experiments on the DeepMind Control SuiteEvaluation milestone	1.5M environment frames	3M environment framesAlgorithm / Task	DrQv2	GPL-DrQ	GPL-DrQ-Expl+	DrQv2	GPL-DrQ GPL-DrQ-Expl+	Acrobot swingup	277 ± 39	246 ± 31	283 ± 49	414±34	393 ± 29	446 ± 33Cartpole swingup sparse	475 ± 388	740 ± 123	780 ± 25	503 ± 411	837± 15	824 ± 33Cheetah run	771 ± 24	837 ± 29	830 ± 25	873 ± 53	903 ± 1	902 ± 2Finger turn easy	794 ± 159	860 ± 74	810±84	946 ± 16	945 ± 18	952 ± 15Finger turn hard	484 ± 156	587 ± 238	615 ± 233	923 ± 17	893 ± 59	918±24Hopper hop	198 ± 101	242 ± 56	252 ± 76	240 ± 123	296 ± 52	349 ± 90Quadruped run	385 ± 214	564 ± 54	589 ± 71	504 ± 279	712±51	725 ± 112Quadruped walk	591 ± 270	834 ± 46	826 ± 35	897 ± 46	918±16	912±16Reach duplo	219 ± 6	217±6	219 ± 5	227 ± 2	226 ± 1	225 ± 3Reacher easy	961 ± 13	951 ± 21	968 ± 8	952 ± 17	957± 12	962 ± 7Reacher hard	813 ± 122	790 ± 103	798 ± 114	957± 13	946 ± 38	934 ± 18Walker run	569 ± 273	574 ± 275	713 ± 5	617 ± 296 618 ± 298		782 ± 10Top performance count	3/12	8/12	11/12	4/12	7/12	10/126.2	Continuous control from pixelsGPL-DrQ. We also integrate GPL with a recent version of Data-regularized Q (DrQv2) (Yaratset al., 2021a), an off-policy, model-free algorithm achieving state-of-the-art performance for pixel-based control problems. DrQv2 combines image augmentation from DrQ (Yarats et al., 2021b) with
Table 2: Hyper-parameters used for GPL-SACSAC hyper-parametersReplay data buffer size	1000000Batch size	256Minimum data before training	5000Random exploration steps	5000Optimizer	Adam (Kingma & Ba, 2014)Policy/critic learning rate	0.0003Policy/critic β1	0.9Critic UTD ratio	20Policy UTD ratio	1Discount γ	0.99Polyak coefficient ρ	0.995Hidden dimensionality	256Nonlinearity	ReLUInitial entropy coefficient α	1Entropy coefficient learning rate	0.0001Entropy coefficient β1	0.5Policy target entropy HH	Hopper : -1, HalfCheetah: -3, Walker2d: -3, Ant: -4, Humanoid: -2GPL hyper-parameters
Table 3: Hyper-parameters used for GPL-DrQDrQv2 hyper-parameters	Replay data buffer size	1000000 (100000 for Quadruped run)Batch size	256 (512 for Walker run)Minimum data before training	4000Random exploration steps	2000Optimizer	Adam (Kingma & Ba, 2014)Policy/critic learning rate	0.0001Policy/critic β1	0.9Critic UTD ratio	0.5Policy UTD ratio	0.5Discount γ	0.99Polyak coefficient ρ	0.99N-step returns	3 (1 for Walker run)Hidden dimensionality	1024Feature dimensionality	50Nonlinearity	ReLUInitial entropy coefficient α	1Exploration stddev. clip	0.3Exploration stddev. schedule	linear: 1 → 0.1 in 500000 steps
Table 4: Results for the proprioceptive observations experiments on the OpenAI Gym suite aftercollecting 100K experience stepsAlgorithm / Task	SAC-20	REDQ	MBPO	GPL-SAC (Ours)Hopper-v2	2694 ± 902	3007 ± 471	3262 ± 197	3386 ± 92Halfcheetah-v2	5822 ± 728	5625 ± 431	9501 ± 331	9685 ± 658Walker2d-v2	2101 ± 876	1937 ± 968	3377 ± 529	3662 ± 360Ant-v2	482 ± 101	3160 ± 1213	1624 ± 447	4933 ± 333Humanoid-v2	493 ± 94	1460 ± 689	555 ± 61	5155 ± 191Normalized average	0.35 ± 0.28	0.48 ± 0.24	0.52 ± 0.30	0.81 ± 0.12induce overly conservative policies that prefer low-uncertainty behavior, hindering exploration instochastic environments (Ciosek et al., 2019).
Table 5: Average training times for the tested algorithms and ablationsProprioceptive observations tasks	Training time (seconds/1000 env. steps)GPL-SAC (N = 10, UTD = 20)	76.1No pessimism learning	74.2No modern action-value model	51.6GPL-SAC-Expl+	77.0GPL-QR-SAC (W1)	86.6GPL-QR-SAC (W2)	88.1N = 20	85.0N = 15	81.2N=5	62.5N=2	38.8UTD = 10	41.4UTD = 5	20.1UTD = 1	6.3REDQ (Original implementation)	183.8Pixel observations tasks	Training time (seconds/10000 env. steps)GPL-DrQ-Expl+	112.2GPL-DrQ	111.3DrQv2	111.2
