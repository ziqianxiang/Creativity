Table 1: Performance on CIFAR-10-LT.				Table 2: Performance on CIFAR-100-LT.			Method	100	50	10	Method	100	50	10CE	70.4	74.8	86.4	CE	38.4	43.9	55.8Mixup	73.1	77.8	87.1	Mixup	39.6	45.0	58.2LDAM-DRW	77.1	81.1	88.4	LDAM-DRW	42.1	46.7	58.8BBN	79.9	82.2	88.4	BBN	42.6	47.1	59.2Remix-DRW	79.8	-	89.1	Remix-DRW	46.8	-	61.3MetaSAug	80.7	84.3	89.7	MetaSAug	48.0	52.3	61.3cRT+mixup	79.1	84.2	89.8	cRT+mixup	45.1	50.9	62.1LWS+mixup	76.3	82.6	89.6	LWS+mixup	44.2	50.7	62.3MiSLAS	82.1	85.7	90.0	MiSLAS	47.0	52.3	63.2Ours	83.1	85.9	90.4	Ours	48.6	53.9	63.34.2	Comparison with state-of-the-artsResults on CIFAR-LT. To demonstratethe effectiveness of our method, wecompare our method with mixup (Zhanget al., 2017), LDAM-DRW (Cao et al.,2019), BBN (Zhou et al., 2020),Remix-DRW (Chou et al., 2020),MetaSAug (Li et al., 2021), cRT,
Table 3: Performance on ImageNet-LT.		Table 4: Performance on PlaCes365-Lr				Method		Method	all	many	medium	fewCE	44.6	CE	27.2	45.9	22.4	0.36Mixup	45.5	Mixup	29.2	-	-	-LDAM-DRW	48.8	OLTR	35.9	44.7	37	25.3CE-DRW	48.5	OLTR-LMFE	36.2	-	-	-Focal-DRW	47.9	FeatureAug	36.4	42.8	37.5	22.7MetaSAug	52.6	cRT+mixup	38.3	44.1	38.5	27.1cRT+mixup	51.7	LWS+mixup	39.7	41.7	41.3	33.1LWS+mixup	52.0	MiSLAS	40.4	39.6	43.3	36.1MiSLAS	52.7	Ours	40.4	433~~	41.9	31.6Ours	53.2					Table 5: The KL divergence between semantic transformations. The numerical values show thedifferences of transformations for different samples. The KL divergence of the 4-th and 5-th samplesare the most similar since their most similar head classes are the same.
Table 5: The KL divergence between semantic transformations. The numerical values show thedifferences of transformations for different samples. The KL divergence of the 4-th and 5-th samplesare the most similar since their most similar head classes are the same.
Table 6: Ablation study about the memory size of the dictionaryon ImageNet-LT and CIFAR-100-LT with IM=100.
Table 7: Ablation study about the hyper-parameter Î» on CIFAR-10-LT and CIFAR-100-LT with IM=100. Comparisons withother methods against overfitting like dropout and adding Gaus-sian noises.
