Table 1: Comparison of the last test accuracy over benchmarks on CIFAR-10 and CIFAR-100 withSYM-{20%, 40%, 60%} and ASYM-{20%. 40%}. Kisthe size of episodic memory.
Table 2: Comparison of the last test accuracy over various static Î± on CIFAR-10 and CIFAR-100with SYM-{20%, 40%, 60%} and ASYM-{20%, 40%}. K is the size of episodic memory.
Table 3: Ablation study for ChamS on CIFAR-10 and CIFAR-100 with various noise ratios andtypes. SYM and ASYM refer to the symmetric and asymmetric label noise, respectively.
Table 4: Last validation cccuracy on WebVi-sion and Clothing1M with real-world noiseMethods	WebVision (K=1000)	Clothing1M (K=700)RSV+ SELFIE	19.08	28.69RSV+ Co-teaching	16.45	26.27RSV+ DivideMix	11.73	18.99GBS + SELFIE	20.01	29.92GBS + Co-teaching	17.77	28.83GBS + DivideMix	13.98	22.48ChamS (ours)	25.76	30.515 ConclusionWe address online and blurry continual learning with noisy labels, which can be occurred frequentlyin practical AI deployment in real-world. To handle this challenging setup, we propose a methodof balancing diversity and purity in episodic memory management, followed by a complementaryrobust learning approach against noisy labels. Specifically, we define the score function that consid-ers the training loss (purity) and features similarity (diversity) simultaneously. In addition, a data-and noise-agnostic approach is proposed to automatically adjust the balancing coefficient duringtraining. Last, a robust learning method is presented to handle several false labeled examples possi-bly included in the memory. We verify that ChamS not only improves model performance but alsohelps select more informative examples when updating memory. ChamS outperforms other robust
