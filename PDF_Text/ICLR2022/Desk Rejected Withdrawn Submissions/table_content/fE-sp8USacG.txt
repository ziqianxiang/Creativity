Table 1: Overview of the datasets used in this paper.
Table 2: Comparing the effects of DropAttack with various well-known regularization and otherstate-of-the-art adversarial training methods. The reported results are calculated from 5 runs withthe same hyper-parameters except for the random seeds. The “Baseline” used in the five datasetsare LSTM, BiLSTM, BiGRU, LeNet and VGGNet, which are common neural-based deep learningmodels. The best method and the best competitor are highlighted by bold and underline, respec-tively. Additional experimental details and results are provided in the Appendix A.
Table 3: The ability of our method DropAttack to prevent overfitting under different sizes of trainingdata. The hyperparameters are set to k = 3, x = θ = 5, px = pθ = 0.7.
Table 4: Comparing the performance of DropAttack under different number of forward-backwardpropagation K. The reported results are calculated from 5 runs with the same hyper-parameters.
Table 5: Experimental details on the IMDB dataset.
Table 6: Experimental details on the PHEME dataset.
Table 7: Experimental details on the AGnews dataset.
Table 8: Experimental details on the MNIST dataset.
Table 9: Experimental details on the CIFAR-10 dataset.
Table 10: The influence of hyperparameters perturbation coefficient and attack probability p onmodel performance. The tested dataset is IMDB, and the model structure is the same as that in Table2. The top ten performance is emphasized in bold.
