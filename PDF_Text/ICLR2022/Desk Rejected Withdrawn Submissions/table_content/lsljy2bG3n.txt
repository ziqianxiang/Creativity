Table 1: Mean accuracy for supervised and unsupervised learning on node classification. X: nodefeatures; A: adjacency matrix; Y: labels; D: diffusion matrix in (Tian et al., 2019); S: affinity matrixin this paper; f: our reproduce otherwise the numbers are quoted from original papers by default.
Table 2: Mean 10-fold cross validation accuracy on supervised and unsupervised learning. RANDOM:Random walk; N2VEC, S2VEC, G2VEC: Node to vector, sub-graph to vector, graph to vector; w/odiv: our method without diversity loss in Eq. 6	"'''''J'''datasets methods⅛''"'''''``	MUTAG	PTCWR	IMDB-BIN	IMDB-MULTI	Reddit-BIN	SP ( orgwardt & Kriegel, 20 )	85.2 ± 2.4	58.2 ± 2.4	55.6 ± 0.2	38.0 ± 0.3	64.1 ± 0.1-	GK( LerVaShidzeetaL,2009)	81.7 ± 2.1	57.3 ± 1.4	65.9 ± 1.0	43.9 ± 0.4	77.3 ± 0.2	WL ( LerVaShidze et al., 2011)	80.7 ± 3.0	58.0 ± 0.5	72.3 ± 3.4	47.0 ± 0.5	68.8 ± 0.4	DGK (Yanardag & Vishwanat , 2015)	87.4 ± 2.7	60.1 ± 2.6	67.0 ± 0.6	44.6 ± 0.5	78.0 ± 0.4	MLG (Kondor &	,20 )	87.9 ± 1.6	63.3 ± 1.5	66.6 ± 0.3	41.2 ± 0.0	-	GRAPHSAGE ( Hamilton et al., 20 )	85.1 ± 7.6	63.9 ± 7.7	72.3 ± 5.3	-50.9 ± 2.2-	-	GCN( ipf&Wellin , 2016)	85.6 ± 5.8	64.2 ± 4.3	74.0 ± 3.4	51.9 ± 3.8	50.0 ± 0.0	GIN-0 ( Xu et al., 2018a)	89.4 ± 5.6	64.6 ± 7.0	75.1 ± 5.1	52.3 ± 2.8	92.4 ± 2.5	GIN-E ( Xu et al., 2018a)	89.0 ± 6.0	63.7 ± 8.2	74.3 ± 5.1	52.1 ± 3.6	92.2 ± 2.3	GAT-E ( elickovic et al., 2018)	89.4 ± 6.1	66.7 ± 5.1	70.5 ± 2.3	47.8 ± 3.1	85.2 ± 3.3	PATCHY( iepertetal.,20 )	92.6 ± 4.2	60.0 ± 4.8	71.0 ± 2.2	45.2 ± 2.8	86.3 ± 1.6	RANDOM( artneretal.,20 )	83.7 ± 1.5	57.9 ± 1.3	50.7 ± 0.3	-34.7 ± 0.2-	-	N2VEC ( Grover & Leskovec, 2016)	72.6 ± 10.2	58.6 ± 8.0	-	-	-	S2VEC (Adhikari et al., 2018)	61.1 ± 15.8	60.0 ± 6.4	55.3 ± 1.5	36.7 ± 0.8	71.5 ± 0.4	G2VEC ( arayanan et al.,2017)	83.2 ± 9.6	60.2 ± 6.9	71.1 ± 0.5	50.4 ± 0.9	75.8 ± 1.0	INFOGRAPH (SUn et al., 2019)	89.0 ± 1.1	61.7 ± 1.4	73.0 ± 0.9	49.7 ± 0.5	82.5 ± 1.4
Table 3: Top-1 accuracy on graph and vision datasets with different mixing strategies.
Table 4: Mean accuracy on DBLP and PPI datasets. GRACEt means our reproduce result. It ismodified to GRACE (GAT) by replacing the encoder with GAT.
Table 5: Accuracy with different mixing methods across seven graph datasets. The first line MVGRLmeans taking normalized adjacency and diffusion matrix as input to perform contrasting and theobjective function is the same as (Hassani & Khasahmadi, 2020). The remaining lines use objectivesof this paper Jnode in Eq. 8 and Jgraph in Eq. 9. MVGRL+(random mix) means the designedbaseline. Here We report the results with best-tuned hyper-parameters under different methods.
Table 6: Accuracy (top) and AUC (bottom) score on COIL-DEL. HIST-KERNEL means vertex edgehist kernel; n means number of nodes in sampled sub-graph. We use the implementation of the twokernel methods, i.e., HIST-KERNEL (Hido & Kashima, 2009) and WL-KERNEL (Shervashidzeet al., 2011) provided by (Sugiyama et al., 2018) to produce the reported results.
Table 7: Statistics of classification datasets. For graph classification dataset, # NODES, # EDGESimply average number of nodes and edges in each graph.
Table 8: Normalized MI (NMI) and adjusted rand index (ARI) score on node clustering. The score isdirectly calculated by Scikit-learn.
Table 9: Performance on graph classification datasets with different objective functions.
Table 10: Performance on graph and node classification datasets of different branches. za , ha meansonly use the attentive branch output as the final output. Average is the average ensemble, whichmeans z = (za + za)/2 and h = (hb + hb)/2. Same with the ensemble property, the aggregatedoutput is more stable than individual output.
