Table 1: Robustness-generalization summary: comparison between PGD and single-model UTAon Fashion-MNIST, MNIST and SVHN. For a given εtrain and εtest, the listed accuracies are theaverage between the clean and robust accuracy, the latter given by AutoAttack (Croce & Hein, 2020),results are averaged over multiple runs. The best score for each εtest is highlighted in yellow.
Table 2: Latent-space robustness/ general-ization trade-offs reached when training withperturbations in latent space and testing withinput-space perturbations, on MNIST. Meth-ods compared are UTA-AT in latent space(LS-UTA-AT), and PGD-AT in latent space(LS-PGD-AT). The scores given are the aver-age between clean accuracy and robust accu-racy obtained using AutoAttack. Results areaveraged over multiple runs, the best scorefor each εtest is highlighted in yellow..
Table 3:	LeNet architecture used for experiments on FashionMNIST and for latent-space MNISTexperiments. With h×w we denote the kernel size. With cin → yout we denote the number ofchannels of the input and output, for the convolution layers, and the number of input and outputunits for fully connected layers.
Table 4:	LeNet architecture used for experiments on SVHN. With h×w we denote the kernel size.
Table 5:	ResNet architectures for the experiments on CIFAR-10. Each ResNet block contains skipconnection (bypass), and a sequence of convolutional layers, normalization, and the ReLU non-linearity. For clarity we list the layers sequentially, however, note that the bypass layers operate inparallel with the layers denoted as “feedforward” (He et al., 2016). The ResNet block for the model(right) differs if it is the first block in the network (following the input to the model).
