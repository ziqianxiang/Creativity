Table 1: Validation accuracy (%) on CIFAR-10after training a seven-layer MLP.
Table 2: Validation loss on CIFAR-10 aftertraining an eight-layer AEOriginal activation	Dropout or ZeroLiers	Validation loss			Epochs		Top-1	Top-5	Top-10		Baseline	0.485"	0.494	0.500	500ReLU	Dropout (q = 0.05)	0.425	0.428	0.431	500	ZeroLiers-L-k (k0 = 3)	0.311	0.313	0.315	500	Baseline	0.524	0.537	0.543	500LeakyReLU	Dropout (q = 0.05)	0.427	0.439	0.442	500	ZeroLiers-L-k (k0 = 3)	0.302	0.305	0.307	500Table 3: Validation loss on CIFAR-10 aftertraining an eight-layer DAEOriginal activation	Dropout or ZeroLiers	Validation loss			Epochs		Top-1	Top-5	Top-10		Baseline	0.509"	0.515	0.521	500ReLU	Dropout (q = 0.05)	0.423	0.433	0.436	500	ZeroLiers-L-k (k0 = 3)	0.302	0.303	0.305	500	Baseline	0.559"	0.563	0.568	500LeakyReLU	Dropout (q = 0.05)	0.412	0.419	0.424	500	ZeroLiers-L-k (k0 = 3)	0.302	0.303	0.305	500
Table 3: Validation loss on CIFAR-10 aftertraining an eight-layer DAEOriginal activation	Dropout or ZeroLiers	Validation loss			Epochs		Top-1	Top-5	Top-10		Baseline	0.509"	0.515	0.521	500ReLU	Dropout (q = 0.05)	0.423	0.433	0.436	500	ZeroLiers-L-k (k0 = 3)	0.302	0.303	0.305	500	Baseline	0.559"	0.563	0.568	500LeakyReLU	Dropout (q = 0.05)	0.412	0.419	0.424	500	ZeroLiers-L-k (k0 = 3)	0.302	0.303	0.305	5004.1	MLPsWe first verify that ZeroLiers is more effective than Dropout when training an MLP that uses thevariants of ReLU. We train MLP that contains 7 fully connected layers with ReLU, LeakyReLUwith α = 0.01, ELU with α = 1.0, GELU, SiLU, and Mish on CIFAR-10 (Krizhevsky et al., 2009)with a batch size of 64 for 500 epochs. Each hidden layer of the MLP contains 1024-hidden units.
Table 4: Model architecturesModel	Layers	# of parameters (millions)	Batch sizeMultilayer Perceptron	7 FC layers	84	64Autoencoder	8 FC layers	7.7	-64Denoising Autoencoder	8 FC layers	77	-64BERT	3 Transformer encoder layers	457	-128-GPT-2	1 Transformer decoder layers	464	24ResNet	50 weighted layers	458	-128-VGG	19 weighted layers	1437	-128-Transformer	6 encoder layers & 6 decoder layers	56	256Table 5: DatasetsDataset	Source	Task	Training Set Size (MB)CIFAR-10	KriZhevSky et al. (2009)	Image classification	147.5：CIFAR-100	Krizhevsky et al. (2009)	Image classification	148.1Wikitext-2	Merity et al. (2016)	Language modeling	10.4Wikitext-103v2	Merity et al. (2016)	Language modeling	102.2Table 6: Baseline settings of BERT and GPT-2Parameter	BERT	GPT-2Activation function	GELU	GELUVocabulary size	30,522	50,257
Table 5: DatasetsDataset	Source	Task	Training Set Size (MB)CIFAR-10	KriZhevSky et al. (2009)	Image classification	147.5：CIFAR-100	Krizhevsky et al. (2009)	Image classification	148.1Wikitext-2	Merity et al. (2016)	Language modeling	10.4Wikitext-103v2	Merity et al. (2016)	Language modeling	102.2Table 6: Baseline settings of BERT and GPT-2Parameter	BERT	GPT-2Activation function	GELU	GELUVocabulary size	30,522	50,257FC layer dimension	3,072	3,072# of attention heads	12	12Epsilon in layer normalization	10-12	10-5Dimension of encoder and pooling layers	768	768Dropout in self-attention layers	q=0.1	q = 0.1Original Dropout in FC layers	None	q = 0.1Dropout right after GELU	None	NoneDropout in other layers	None	q = 0.1B.2 Fully Connected Layer ConstructionFigure 9 shows the way how to modify a given fully connected layers for the experiments in Sec-
Table 6: Baseline settings of BERT and GPT-2Parameter	BERT	GPT-2Activation function	GELU	GELUVocabulary size	30,522	50,257FC layer dimension	3,072	3,072# of attention heads	12	12Epsilon in layer normalization	10-12	10-5Dimension of encoder and pooling layers	768	768Dropout in self-attention layers	q=0.1	q = 0.1Original Dropout in FC layers	None	q = 0.1Dropout right after GELU	None	NoneDropout in other layers	None	q = 0.1B.2 Fully Connected Layer ConstructionFigure 9 shows the way how to modify a given fully connected layers for the experiments in Sec-tion 4. Figure 9(a) shows the baseline. Figure 9(b) is for Dropout. We insert Dropout for the outputof the activation function in the baseline. The activation function and the Dropout mechanism to-gether are replaced by ZeroLiers in Figure 9(c) to evaluate ZeroLiers.
