Table 1: The results obtained when fine-tuning ten different pre-trained agents (rows) on nine otherAtari games (columns), with DQV (top table) and DDQN (bottom table). Positive values (in cyan)represent positive transfer, while negative values (in orange) represent negative transfer. The darkerthe color, the higher the absolute value of the area ratio score.
Table 2: The area ratio obtained after fine-tuning a pre-trained DQN agent on the different Catch environments. Wecan see that no matter which source game is used for pre-training, transfer learning surprisingly never results in posi-tive transfer.
Table 3: The area ratio scores obtained after performing self-transfer. We can see that if only the last linear layer is trained,then positive transfer is obtained on all Catch environments,whereas if the network is fine-tuned, positive transfer is (in part)only obtained on Catch-v2.
Table 4: Hyper-parameters used when training a DDQN agent from scratch. We follow the experi-mental setup introduced in the original paper Van Hasselt et al. (2016).
Table 5: Hyper-parameters used when training a DQV agent from scratch. We can see that theymostly correspond to the ones presented in Table 4 with the main difference being the epsilon greedyparameter that is set to 0.5 instead of 1.0, and the additional information given for the V networkwhich is trained for learning an approximation of the state-value function.
