Table 1: The identity groups for Jigsaw dataset with their corresponding fine-grained annotations.
Table 2: Jigsaw dataset samplesComment text	Toxicity	GroupThe Atwood fable is Donald, is it? My impression of this noise (over Atwood) is that it’s a gimmick by Atwood and her publisher to cash in on the Donald effect. As if we needed slaves in bonnets to remind us that Donald is a jerk (and where was Atwood’s novel when Monica was being pawed over?). A word to defenders of women: don’t spend your political capital on stupid analogies.	Toxic	GenderI got a question for you, dear, and it is a fair question: We all know what is happening in Syria; where are all the women’s marches over the slaughter in that country?. And, why has Trudeau been silent, like his pal Barry Obama, on taking effective military action against Syria? All you lefties are the same: you have no side vision.	Normal	GenderA.1.2 HateXplain: Toxic text in Twitter and Twitter-like textHateXplain (Mathew et al., 2021) was recently introduced with the intent of studying explanations inoffensive and hate speech in Twitter and Twitter like data (i.e., gab.com). For the purposes of ourstudy, we collapse the annotations for offensive and hate speech into one class of toxic text. Similarto the Jigsaw dataset, HateXplain samples have fine-grained annotations for protected groups. Weuse as groups the coarse-level annotations, as we did for the Jigsaw dataset. The groups that weconsider are presented in Table 4 and a few examples from the dataset are shown in Table 5. Notethe text in each sample is represented in the dataset as a list of tokens; in the table, we concatenatedthem with spaces and this is the way we use them as inputs for the classifiers as well. We used thesplits as provided in the dataset; dataset statistics are shown in Table 6.
Table 3: Jigsaw dataset statistics: sample counts per dataset split and protected groupDataset split	Total	Religion	Race	GenderTrain	1443899	50748	31022	70703Dev	360975	12769	7999	17869Test	97320	3316	1911	436716Under review as a conference paper at ICLR 2022Table 4: The identity groups for HateXplain dataset with their corresponding fine-grained annota-tions.
Table 4: The identity groups for HateXplain dataset with their corresponding fine-grained annota-tions.
Table 5: HateXplain dataset samplesText	Toxicity	Groupwhy is anna acting like she black	Normal	Racelike brazil many are descendants of west africa social constructs labels classifications have ruined a many people who falsely self identify as white or just do not even know abt their native land all b c of them bum ass caucazoids aka caucasians	Toxic	Racesults, we focus on balanced accuracy as the Jigsaw dataset is imbalanced and looking only at accu-racy results may be misleading. In general, higher accuracy leads to higher balanced accuracy, withthe exception of two models - GPT2 and SqUeezeBERT. For these two, the best balanced accuracy isless than 2 percentage points higher than the balanced accuracy corresponding to the highest overallaccuracy across the various hyper-parameter runs. We also experiment with two learning rates andobserve that the large models tend to prefer smaller learning rates, degenerating for lower learningrates. For large models with Jigsaw we fine-tune only for one epoch to keep the compute time under24 hours. The model accuracy we obtained are in line with state-of-the-art results for these types oftasks.
Table 6: HateXplain dataset statistics: sample counts per dataset split and protected groupDataset split	Total	Religion	Race	GenderTrain	15383	-^3924^^	5418	3102Dev	1922	481	672	396Test	1924	468	685	37517Under review as a conference paper at ICLR 2022Table 7: The size (number of parameters, size on disk) for the language models considered in thisstudy.
Table 7: The size (number of parameters, size on disk) for the language models considered in thisstudy.
