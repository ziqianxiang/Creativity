Table 1: Average classification accuracy after the final task. ‘-XT’ means X tasks. We report theaverage incremental accuracy in Appendix A. The column Average indicates the average of results ofeach method over the datasets and memory budgets. The row Diff. gives the difference in accuracybetween our method and the best baseline in each column. The results on the rightmost column arethe accuracy without fixed pre-trained feature extractor. We highlight the best result in each columnin bold. EECt indicates that We follow the original paper for ImageNet experiment, where 50 classesare split into 5 tasks because we are unable to run the official code for 1000 classes on our system.
Table 2: Comparison of proposed covariances. Σc is the covariance used in Table 1. Σt is the sharedcovariance across classes within a task. Σ is the shared covariance across all classes. Each accuracyis the average classification accuracy after the final task is learned. The accuracy of Σ in ImageNetwith |M| = 11000 is copied from |M| = 6000 as its actual memory usage (1512) is the same.
Table 3: Average incremental learning. The best result in each column is highlighted in bold.
Table 4: Zero-shot accuracy of CLIP when the feature extractor, ViT-B/32, is used and the text labelsof training data are available.
