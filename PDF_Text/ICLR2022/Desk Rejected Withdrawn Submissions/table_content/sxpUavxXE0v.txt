Table 1: Comparisons with/without DCL under different numbers of batch sizes from 32 to 512.
Table 2: Comparisons between SimCLR baseline, DCL, and DCLW. The linear and kNN top-1 (%)results indicate that DCL improves the performance of baseline, and DCLW further provides an extraboost. Note that results are under the batch size 256 and epoch 200. All of models are both trainedand evaluated with same experimental settings.
Table 3: kNN top-1 accuracy (%) comparison of SSL approaches on small-scale benchmarks:CIFAR10, CIFAR100, and STL10. Results show that DCL consistently improves its SimCLRbaseline. With multi-cropping (Caron et al., 2020), our DCLW reaches competitive performancewithin other contrastive learning approaches (Chen et al., 2020a; He et al., 2020; Wu et al., 2018; Yeet al., 2019; Dosovitskiy et al., 2015).
Table 4: Improve the DCL model performance on ImageNet-1K with better hyperparameters, temper-ature and learning rate and stronger augmentation.
Table 5: ImageNet-1K top-1 accuracy (%) on SimCLR and MoCo v2 with/without DCL under fewtraining epochs. We further list results under 200 epochs for clear comparison. With DCL, theperformance of SimCLR trained under 100 epochs nearly reaches its performance under 200 epochs.
Table 6: The ablation study of various temperature Ï„ on the CIFAR10.
Table 7: ImageNet-1K top-1 accuracies (%) of linear classifiers trained on representations of differentSSL methods with ResNet-50 backbone. The results in the lower section are the same methods witha larger experiment setting.
Table 8: STL10 comparisons Hypersphere and DCL under the same experiment setting.
Table 9: ImageNet-100 comparisons of Hypersphere and DCL under the same setting (MoCo).
Table 10: ImageNet-100 comparisons of Hypersphere and DCL under the same setting (MoCo v2)except for memory queue size.
Table 11: ImageNet-1K comparisons of and DCL under the best setting. In this experiment both ofthe methods used their optimized hyperparameters.
Table 12: STL10 comparisons of Hypersphere and DCL under different batch sizes.
Table 13: Results of DCL with large batch size and learning epochs.
Table 14: Results of DCL on wav2vec 2.0 be evaluated on two downstream tasks.
Table 15: CIFAR-10 as an example to show that DCL achieves competitive results compared toBYOL, SimSiam, and Barlow Twins.
