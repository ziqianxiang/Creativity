Table 1: Differences between PoseConv3D and GCN.
Table 2: Evalution of PoseConv3D variants. ‘s’indicates shallow (fewer layers); ‘HR’ indicateshigh-resolution (double height & width); ‘wd’ in-dicates wider network with double channel size.
Table 3: PoseConv3D is better or comparable to previous state-of-the-arts. With estimated high-quality 2Dskeletons and the great capacity of 3D-CNN to learn spatiotemporal features, PoseConv3D achieves superiorperformance across 5 out of 6 benchmarks. J , L means using joint- and limb-based heatmap respectively. ++denotes using the same pose estimation result as ours. * means the number is reported by Shao et al. (2020).
Table 4: Comparison to the state-of-the-art of Multi-Modality Action Recognition. Perfect recognitionperformance is achieved on multiple benchmarks with multi-modality fusion. R, F, P indicate RGB, Flow, Pose.
Table 5: An apple-to-apple comparison between 3D heatmapvolumes and 2D heatmap aggregations.
Table 6: 3D-CNN v.s. GCN. We compare the performance of Table 8: Train/Test w. different pose anno-3D-CNN and GCN on several datasets. For 3D-CNN, we re- tations. 3D-CNN shows great generaliza-port the results of 1/10-clip testing. We exclude parameters and tion capability in the cross-PoseAnno settingFLOPs of the FC layer, since it depends on the number of classes. (LQ for low-quality; HQ for high-quality).
Table 7: Recognition performance w. different dropping KPprobabilities. 3D-CNN is more robust to input perturbations.
Table 10: The universality of RGBPose-Conv3D. Theearly+late fusion strategy works both on RGB-dominantNTU-60 and Pose-dominant FineGYM.
Table 9: The design of RGBPose-Conv3D.
Table 11: The architecture of PoseConv3D instantiated with three backbones: C3D, X3D, SlowOnly. Thedimensions of kernels are denoted by T X S2, C for temporal, spatial, channel sizes. Strides are denoted withT, S2 for temporal and spatial strides. GAP denotes global average pooling.
Table 12: RGBPose-Conv3D instantiated with the SlowOnly backbone. The dimensions of kernels aredenoted by T × S2 , C for temporal, spatial, channel sizes. Strides are denoted with T, S2 for temporal andspatial strides. The backbone we use is ResNet50. GAP denotes global average pooling.
Table 13: Ablation study on Pose Extraction.
Table 14: Transferring Ability. Skeleton representations learned on the large-scale Kinetics400 can transferto downstream datasets well. Backbone parameters are frozen for the ‘Linear’ setting.
Table 15: Comparison with state-of-the-art multi-modality action recognition approaches.
Table 16: PoseConv3D with projected 2D poses. We report the recognition performance of the joint model.
Table 17: Uniform sampling also works for RGB-based action recognition. Alls results are for 10-cliptesting, except the ‘uniform-16 (1c)’, which uses 1-clip testing.
Table 18: Top 5 confusion pairs of skeleton-based action recognition on NTU-60 X-Sub. Multi-modalityfusion with RGBPose-Conv3D improves the recognition performance on confusion pairs by a lot.
Table 19: Mean class accuracy on theKinetics-Motion subset.
