Table 1: Comparison for DAFT with standard Transformer and linear attention Transformers: LinearTransformer (Katharopoulos et al., 2020), Performer (Choromanski et al., 2020). Here T, d denotethe sequence length and feature dimension, respectively. DAFT has linear space complexity wrtboth T and d, while not performing query&key dot product, keeping the exp nonlinearity, and notcomputing the explicit attention. Both Linear Transformer and Performer have at least quadraticcomplexity in d, perform query&key dot product but without the exp nonlinearity, and do not needto compute explicit attention.
Table 2: Imagenet 1K classification results with the Transformer architecture from DeiT (Touvronet al., 2020), cropsize is 224. Speed and memory consumption are measured in inference mode on aV100 GPU, batch size is 256.
Table 3: Finetuning DAFT-conv for 100 epochs from a pretrained “DeiT base” on 384 × 384 crops.
Table 4: NLL results on CIFAR10, evaluated by bits/dim, the lower the better. Speed and memoryare measured during training time, with a batch size of 32 across 8 V100 GPUs.
Table 5: Enwik8 results, measured in bits per character (bpc), the lower the better. Baselines com-pared are Reformer (Kitaev et al., 2020), Synthesizer (Tay et al., 2020a) (its best performing denseversion), Linear Transformer (Katharopoulos et al., 2020) and Performer (Choromanski et al., 2020).
Table 6: The effect of factorized parameteriza-tion of DAFT-full.
Table 7: The effect of reprameterization ofDAFT-conv (kernel size 7 × 7).
Table 8: Top 1 accuracy of DAFT-conv without the query term (w/o q). This results in significantperformance drops.
