Table 1: Several typical weighting schemes.
Table 2: Statistics of measurement criteriaCriterion	Method	Number	ScenarioLoss (pred)	SPLJBinary (2010), SPL_Log (2014), Cost-sensitive SPL (2016), Focal Loss (2017), QFL (2020), ASL (2020), SuperLoss (2020), FOCI (2020), Truncated Loss (2021), JTT (2021)	11	Standard, Noise, ImbalanceGradient	GHM (2019),LOW (2021)	2	ImbalanceCategory proportion	ClaSS-balance (2019), G-RW (2021)	2	ImbalancePrior knowledge	CL (2009)	1	Standard, Noise, ImbalanceUncertainty	FOCI (2020)	1	NoiseMargin	GAIRAT(2021)	1	StandardExisting studies only (explicitly or implicitly) give partial answers to the “easy-or-hard” questionon a specific view or scenario. Few studies have attempted to thoroughly discuss the applica-ble/inapplicable scenarios for a given weighting scheme. Meanwhile, several studies have proposedsimilar concerns. Wang et al. (2021b) raised a similar question about “easy-first versus hard-first”under the context of curriculum learning. This paper explores this question from a global perspec-tive, obtaining reasonable findings.
Table 3: Accuracies (%) under flip noises. The best and the second best results are bold and under-lined, respectively.
Table 4: Test accuracies (%) on imbalanced CIFAR10 and CIFAR100 with different imbalancefactors (“-" means there is no record of the results in the original paper.)Data set		Long-tailed CIFAR10				Long-tailed CIFAR100				Imbalance factor	200	100	50	20	10	200	100	50	20	10CE (Baseline)	65.68	70.36	74.81	82.23	86.39	34.84	38.32	43.85	51.14	55.71Focal loss.γ=1	65.29	70.38	76.71	82.76	86.66	35.62	38.41	44.32	51.95	55.78Focal loss_)=0.5	64.00	70.33	76.72	82.89	86.81	35.00	38.69	44.12	51.10	55.70SPL-Binary	65.64	70.94	76.82	82.41	87.09	35.56	38.16	42.77	50.91	56.70SPL_Log	62.05	70.46	75.64	82.66	86.62	33.08	38.51	41.71	49.71	54.79L2RW	66.25	72.23	76.45	81.35	82.12	33.00	38.90	43.17	50.75	52.12Class-balance CE loss	68.77	72.68	78.13	84.56	87.90	35.56	38.77	44.79	51.94	57.57Class-balance Fine-tuning	66.24	71.34	77.44	83.22	83.17	38.66	41.50	46.12	52.30	57.57Class-balance Focal loss	68.15	74.57	79.22	83.78	87.48	36.23	39.60	45.21	52.59	57.99Equalised	-	73.98	-	-	-	-	42.74	-	-	-Mixup	-	73.06	77.82	-	87.10	-	39.54	44.99	-	58.02Meta-weight net	67.20	73.57	79.10	84.45	87.55	36.62	41.61	45.66	53.04	58.91LDAM	66.75	73.55	78.83	83.89	87.32	36.53	40.60	46.16	51.59	57.29FlexW (easy-first)	66.20	73.79	79.11	84.51	88.07	37.21	39.23	44.80	52.11	57.73FlexW (hard-first)	69.40	75.33	80.05	85.46	88.50	37.54	41.69	47.18	53.10	58.98To study the performances of the hard-first and easy-first modes on each category, the accuracy for
Table 5: Accuracies (%) of the competing methods on five graph data sets.
Table 6: mAPs (%) of the four learning schemes on the three VOC data sets.
Table 7: AccUracies (%) of different methods on CIFAR10 and CIFAR100.
Table A-1: Hyper-parameter value intervals in which the performance is stable.
Table A-2: Test accuracies (%) of the competing methods under uniform noise.
Table A-3: The performances of different parameter settings on CIFAR10 under 40% flip noise.
Table A-4: Details of the five graph data sets.
Table A-5: mAPs (%) of the four weighting schemes for 20 categories in VOC-e.
Table A-6: mAPs (%) of the four weighting schemes for 20 categories in VOC-h.
Table A-7: mAPs (%) of the four weighting schemes on 20 categories in VOC-m.
Table A-8: AccUracies (%) of differentmethods on CIFAR10 and CIFAR100.
Table A-9: Accuracies (%) of the three methods under different models on CIFAR10.
