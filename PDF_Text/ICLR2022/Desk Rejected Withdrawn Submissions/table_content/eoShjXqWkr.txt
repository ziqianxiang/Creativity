Table 1: Comparison between concurrent work and ours.
Table 2: (a, b) present the comparison of FID with popular unconditional methods.2 In conditionalgeneration, (c, d) show the comparison with CNN-based conditional models.3 STrans-G in FFHQ2562 only contains 20M parameters, while StyleGAN2 and HiT have 30M and 46M parameters inthe generator. J indicates lower the better, and ↑ indicates higher the better.
Table 3: (a) We study the effects of adopting different attention mechanisms in STtrans-G. ‘Trans’indicates the standard self-attention module (Dosovitskiy et al., 2020), while ‘Grid’ (Jiang et al.,2021) and ‘MultiAxis’ (Zhao et al., 2021) are recent studies applying the localized idea in attentionmodules. For a fair comparison, we also add relative positional encoding (Liu et al., 2021) forthe standard Transformer blocks, denoted by ‘+Rel. Pos.’. (b) We compare different architecturalchoices and normalization layers in AdaNorm-T. The architectures are depicted in Fig. 5(a-c), while‘LN’ and ‘IN’ denote layer normalization and instance normalization, respectively.
Table 4: (a) We report the FID from STrans-G with different window sizes in the attention module.
Table 5: We present the influence of differentiable data augmentation on the final FID. With STrans-G as the generator, we also compare the behavior of StyleGAN2 discriminator (‘CNN’) and ourSTrans-D (‘Trans’).
