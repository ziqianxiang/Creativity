Table 1: CIE count, CEV, SDE, and Accuracy for pruning with and without KD on biased CIFAR100Method	#ofCIEs	CEV	SDE	AccuracyAT + KD	742	0.00187	0.13173	77.100PKT + KD	748	0.00199	0.13098	77.335SP + KD	768	0.00331	0.16162	76.927FSP + KD	742	0.00333	0.16002	75.285KD (Hinton et al., 2015)	770	0.00338	0.16065	78.142AT (Zagoruyko & Komodakis, 2017)	909	0.00430	0.19306	78.097PKT (Passalis & Tefas, 2018)	881	0.00481	0.19891	78.963SP (Tung & Mori, 2019)	838	0.00583	0.21591	78.520FSP (Yim et al., 2017)	877	0.00638	0.22525	78.413Struct Pruning	887	0.00931	0.26687	77.242requiring multiple models to be trained. (2) Accuracy alone is a poor indicator of model quality. InTable 1 Structured pruning has accuracy comparable to AT + KD and PKT + KD but in Figure 2we see that the change in accuracy in structured pruning is not distributed equitably with someclasses having a 300% change in FNR. SDE also neatly captures that the skew in FP/FN resultingfor most models. These experiments show the great potential of our proposed CEV/SDE metrics indistinguishing desirable models from biased models that appear to be equal at a surface level.
Table 2: Low resource ImageNet models from TIMM github Wightman (2021). Top1/Top5, inputimage sizes, and parameter count listed. Index corresponds to axis labels in Figure 3index	model	top1	top5	img size	params x1060	efficientnet-b2 Tan & Le (2019)	80.608	95.310	288	9.111	efficientnet_b1 Tan & Le (2019)	78.792	94.342	256	7.792	efficientnet-b1 -pruned Aflalo et al. (2020)	78.242	93.832	240	6.333	mobilenetv3_large_100_miil Howard et al. (2019)	77.914	92.914	224	5.484	mobilenetv2-120d Sandler et al. (2018)	77.294	93.502	224	5.835	mobilenetv3_large_100 Howard et al. (2019)	75.768	92.540	224	5.486	mobilenetv3-rw	75.628	92.708	224	5.487	mobilenetv2_110d Sandler et al. (2018)	75.052	92.180	224	4.528	pit_ti_distilledN24Heo et al. (2021)	74.536	92.096	224	5.109	deit_tiny_distilled_patch16N24 Touvron et al. (2021)	74.504	91.890	224	5.9110	mobilenetv2」00	72.978	91.016	224	3.5011	resnet18 He et al. (2016)	69.758	89.078	224	11.69model from one of the dozens which are available in many problem spaces. Unfortunately, CIE countis not applicable in the case where you have models already trained and simply want to understandthe trade-offs you will be making. Here we see how one might use CEV/SDE to detect and avoida model more biased than models of similar accuracy. For this example, we have selected a setof models from the TIMM model repository (Wightman, 2021) that have between 3.5 × 106 and
Table 3: Comparison of Error Rate Equality Difference(ERED) Dixon et al. (2018) metrics, andDifference in Expected Value(DEV) Hinnefeld et al. (2018) Metrics with our proposed CEV andSDE on measuring bias. We make our comparison to measure fairness traits of models classifyingthe Titanic dataset Frank E. Harrell Jr. (2017). CEV/SDE are calculated w.r.t the whole dataseterrors, and given protected group. All values are averaged over 5 runs of train/test.
Table 4: Top-1, CEV, SDE, and change in FPR/FNR for selected protected class from ResNet modeltrained on CelebA datasetProtected Attribute	Top-1	CEV	SDE	Change in FPR	Change in FNRFull Test Set	0.9212				Attractive	0.9222	0.0015	0.0331	-31.0809	80.4380Male	0.9225	0.1413	0.2205	12.8440	77.8003Pale Skin	0.9224	0.0035	0.0465	-43.8572	-33.9335Young	0.9215	0.0002	0.0082	-27.6765	150.5065Not Attractive	0.9208	0.0034	0.0493	45.6423	6.8297Not Male	0.9207	0.0053	0.0562	1.2762	47.2981Not Pale-Skin	0.9207	0.0000	0.0021	1.9565	1.4648Not Young	0.9213	0.0035	0.0313	146.3057	0.2381and more likely to incorrectly guess they have Gray Hair. Meanwhile, “Not Pale Skin” has loweraccuracy, but the accuracy and FPR/FNRs are much closer to the average of the model as a whole.
