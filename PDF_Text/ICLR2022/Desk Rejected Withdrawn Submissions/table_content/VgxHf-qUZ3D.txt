Table 1: Hypervolume on Multi-MNIST de-pending on hyper-parameter α and λMethod	α = 0.5	α = 1.0	α = 1.5PHN-LS	2.84	2.84	2.86PHN-EPO	2.88	2.87	2.86COSMOS (λ = 1)	2.99	2.95	2.97COSMOS (λ = 3)	2.94	2.95	2.95COSMOS (λ = 5)	2.96	2.97	2.95where g() is a function that weights each objective Li by the corresponding weight ri which issampled from an n-dimensional Dirichlet distribution. The last equality comes from using LS asg().
Table 2: Quantitative Evaluation on Image Classification and Fairness.
Table 3: Hypervolume on CelebA Easy andHard tasksMethod	CelebA Easy	CelebA HardSingle Task	3.719	2.222COSMOS	3.706	2.221SEPNet	3.713	2.235Figure 4 shows the trend of α and λ while training for Image Classification. Note that the differentoptimal values are found in each dataset, which implies that to find the optimal values for othermethods, these should be tuned as hyper-parameters for each dataset by grid-search. After training,the two parameters can be conditioned to different values for inference to generate multiple Paretofronts as shown by Figure 5. Hence by following the protocol explained in Section 4.1, we can findthe optimal parameters in the validation set through self-evolutionary optimization.
Table A1: EfficientNet-B4 (CelebA)Input xStem ConvolutionMBConvBlocks (0-10)Conditioning ModuleMBCOnvBlocks(11-20)Conditioning ModuleMBConvBlocks (21-30)Conditioning ModuleMBConvBlocks (31)Head ConvolutionHead Fully Connected12Under review as a conference paper at ICLR 2022A.2 Additional ExperimentsTable A2: Hypervolume with or without Sparse Sampling SEPNet Cont’ denotes the SEPNetwithout Sparse Sampling.
Table A2: Hypervolume with or without Sparse Sampling SEPNet Cont’ denotes the SEPNetwithout Sparse Sampling.
Table A3: Hypervolume according to a selection of hyper-parameters for SEOMethod	Multi-MNIST	Multi-Fashion	Multi-F+MNISTSEPNet a&X	3.04	2.39	2.93SEPNet α	3.02	2.37	2.92SEPNet λ	3.03	2.37	2.92Multi-MNlST		Multi-Fashion	Multi-Fashion+MNIST0 5 0 5 0 5 0∙∙∙4 4 3 3OooooooSSo-I S N YSeITask 1 CE LossTask 1 CE LossTask 1 CE LossFigure A3: SEPNet’s convergence graph of the Pareto fronts according to model updates SEP-Net approximates well-spread Pareto fronts from the beginning of training and converges very fast.
Table A4: Model size of each method The separate model requires n models for n preference rays.
