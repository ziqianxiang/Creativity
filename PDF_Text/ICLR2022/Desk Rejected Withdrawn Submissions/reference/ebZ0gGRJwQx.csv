title,year,conference
 A convergence theory for deep learning via over-parameterization,2019, In Kamalika Chaudhuri and Ruslan Salakhutdinov (eds
 Spectrally-normalized margin bounds forneural netWorks,2017, Advances in Neural Information Processing Systems
 Fit Without fear: remarkable mathematical phenomena of deep learning through theprism of interpolation,2021, arXiv preprint arXiv:2105
 Overfitting or perfect fitting? risk bounds forclassification and regression rules that interpolate,2018, Advances in Neural Information ProcessingSystems
 Reconciling modern machine-learning practice and the classical bias-variance trade-off,2019, Proceedings of the National Academyof Sciences
 Do-conv: DepthWise over-parameterized convolutional layer,2020, arXivpreprint arXiv:2006
 Binaryconnect: Training deep neuralnetWorks With binary Weights during propagations,2015, In Advances in neural information processingsystems
 Local sgd optimizes overparameterized neural networks inpolynomial time,2021, arXiv preprint arXiv:2107
 Gradient descent finds globalminima of deep neural networks,2019, In Kamalika Chaudhuri and Ruslan Salakhutdinov (eds
 Gradient descent provably optimizesover-parameterized neUral networks,2018, In International Conference on Learning Representations
 Resist: Layer-wise decomposition of resnets fordistribUtedtraining,2019, Preprint
 Switch transformers: Scaling to trillion parametermodels with simple and efficient sparsity,2021, arXiv preprint arXiv:2101
 Does learning reqUire memorization? a short tale aboUt a long tail,2020, In Proceedingsof the 52nd Annual ACM SIGACT Symposium on Theory of Computing
 DropoUt as a bayesian approximation: Representing modelUncertainty in deep learning,1050, In international conference on machine learning
 Size-independent sample complexity ofneUral networks,2018, In Conference On Learning Theory
 How to characterize thelandscape of overparameterized convolUtional neUral networks,2020, 2020
 Why do deep residUal networks generalizebetter than deep feedforward networks?â€”a neUral tangent kernel perspective,2020, Advances in NeuralInformation Processing Systems
 MUlti-sample dropoUt for accelerated training and better generalization,2019, arXivpreprint arXiv:1905
 Neural tangent kernel: convergence and gen-eralization in neUral networks,2018, In Proceedings of the 32nd International Conference on NeuralInformation Processing Systems
 Survey of dropout methods for deep neuralnetworks,2019, arXiv preprint arXiv:1904
 The implicit regularization of ordinary leastsquares ensembles,2020, In International Conference on Artificial Intelligence and Statistics
 Gshard: Scaling giant models with conditionalcomputation and automatic sharding,2020, arXiv preprint arXiv:2006
 Gradient descent with early stopping isprovably robust to label noise for overparameterized neural networks,2020, In International conferenceon artificial intelligence and statistics
 A mean field analysis of deepResNet and beyond: Towards provably optimization via overparameterization from depth,2020, InInternational Conference on Machine Learning
 The power of interpolation: Understanding theeffectiveness of SGD in modern over-parametrized learning,2018, In International Conference onMachine Learning
 Efficientlarge-scale distributed training of conditional maximum entropy models,2009, In Advances in NeuralInformation Processing Systems
 Simultaneous training of partiallymasked neural networks,2021, arXiv preprint arXiv:2106
 Exploring general-ization in deep learning,2017, Advances in Neural Information Processing Systems
 A PAC-Bayesian approach tospectrally-normalized margin bounds for neural networks,2018, In International Conference on Learn-ing Representations
 Toward moderate overparameterization: Global con-vergence guarantees for training shallow neural networks,2020, IEEE Journal on Selected Areas inInformation Theory
 A data-driven software tool for enabling coop-erative information sharing among police departments,0377, European Journal of Opera-tional Research
 On generalization of adaptivemethods for over-parameterized linear regression,2020, arXiv preprint arXiv:2011
 Theoretical insights into the optimizationlandscape of over-parameterized shallow neural networks,2018, IEEE Transactions on InformationTheory
 An empirical study of example forgetting during deep neural networklearning,2018, In International Conference on Learning Representations
 PowerSGD: Practical low-rank gradientcompression for distributed optimization,2019, Advances In Neural Information Processing Systems32 (Nips 2019)
 Regularization of neuralnetworks using dropconnect,1058, In International conference on machine learning
 Pufferfish: Communication-efficientmodels at no extra cost,2021, Proceedings of Machine Learning and Systems
 Fast dropout training,2013, In international conference on machinelearning
 GIST: Distributed training for large-scale graph convolutional net-works,2021, arXiv preprint arXiv:2102
 Minipatchlearning as implicit ridge-like regularization,2021, In 2021 IEEE International Conference on Big Dataand Smart Computing (BigComp)
 Distributed learning of deepneural networks using independent subnet training,2019, arXiv preprint arXiv:1910
 Dimmwitted: A study of main-memory statistical analytics,2014, Pro-ceedings of the VLDB Endowment
 Non-vacuousgeneralization bounds at the ImageNet scale: a PAC-Bayesian compression approach,2018, In Interna-tional Conference on Learning Representations
 Parallelized stochastic gradientdescent,2010, In Advances in neural information processing systems
 Gradient descent optimizes over-parameterized deep ReLU networks,2020, Machine Learning
