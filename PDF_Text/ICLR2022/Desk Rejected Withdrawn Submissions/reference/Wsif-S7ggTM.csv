title,year,conference
 Layer normalization,2016, arXiv preprintarXiv:1607
 End-to-end object detection with transformers,2020, In European Conferenceon Computer Vision
 A shortnote about kinetics-600,2018, arXiv preprint arXiv:1808
 Spatiotemporal residual networks for video action recog-nition,2016, Advances in Neural Information Processing Systems
 Transformer-xl: Attentive language models beyond a fixed-length context,2019, arXiv preprintarXiv:1901
 Imagenet: A large-scale hi-erarchical image database,2009, In 2009 IEEE conference on computer vision and pattern recognition
 Bert: Pre-training of deepbidirectional transformers for language understanding,2018, arXiv preprint arXiv:1810
 Animage is worth 16x16 words: Transformers for image recognition at scale,2020, arXiv preprintarXiv:2010
 Multiscale vision transformers,2021, arXiv preprint arXiv:2104
 More is less: Learningefficient video representations by big-little network and depthwise temporal aggregation,2019, arXivpreprint arXiv:1912
 X3d: Expanding architectures for efficient video recognition,2020, In Proceed-ings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition
 Fast r-cnn,2015, In Proceedings of the IEEE international conference on computer vision
 Ssan: Separable self-attention network for video represen-tation learning,2021, In Proceedings of the IEEE/CVF Conference on Computer Vision and PatternRecognition
 Mask r-cnn,2017, In Proceedings oftheIEEE international conference on computer vision
 Realformer: Transformer likesresidual attention,2020, arXiv preprint arXiv:2012
 3d convolutional neural networks for human actionrecognition,2012, IEEE transactions on pattern analysis and machine intelligence
 Stm: Spatiotemporal andmotion encoding for action recognition,2019, In Proceedings of the IEEE/CVF International Confer-ence on Computer Vision
 The kinetics human actionvideo dataset,2017, arXiv preprint arXiv:1705
 Imagenet classification with deep con-volutional neural networks,2012, Advances in neural information processing systems
 Tea: Temporal excitationand aggregation for action recognition,2020, In Proceedings of the IEEE/CVF Conference on ComputerVision and Pattern Recognition
 Localvit: Bringing localityto vision transformers,2021, arXiv preprint arXiv:2104
 Swin transformer: Hierarchical vision transformer using shifted windows,2021, arXiv preprintarXiv:2103
 Video transformer network,2021, arXivpreprint arXiv:2102
 Learning spatio-temporalrepresentation with local and global diffusion,2019, In Proceedings of the IEEE/CVF Conference onComputer Vision and Pattern Recognition
 Yolov3: An incremental improvement,2018, arXiv preprintarXiv:1804
 Faster r-cnn: Towards real-time objectdetection with region proposal networks,2015, arXiv preprint arXiv:1506
 Two-stream convolutional networks for action recognitionin videos,2014, arXiv preprint arXiv:1406
 Training data-effiCient image transformers & distillation through attention,2020, arXivpreprint arXiv:2012
 Learning spa-tiotemporal features with 3d convolutional networks,2015, In Proceedings of the IEEE internationalconference on computer vision
 A closerlook at spatiotemporal convolutions for action recognition,2018, In Proceedings of the IEEE conferenceon Computer Vision and Pattern Recognition
 Video classification with channel-separated convolutional networks,2019, In Proceedings of the IEEE/CVF International Conference onComputer Vision
 Attention is all you need,2017, arXiv preprint arXiv:1706
 Video modeling with correlation net-works,2020, In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition
 Attentionnas: Spatiotemporal attention cell search for videoclassification,2020, In European Conference on Computer Vision
 Non-local neural networks,2018, InProceedings of the IEEE conference on computer vision and pattern recognition
 Evolving attention with residual convolutions,2021, arXiv preprintarXiv:2102
 Cvt:Introducing convolutions to vision transformers,2021, arXiv preprint arXiv:2103
 Xlnet: Generalized autoregressive pretraining for language understanding,2019, arXiv preprintarXiv:1906
 Rethinking semantic segmentation froma sequence-to-sequence perspective with transformers,2020, arXiv preprint arXiv:2012
