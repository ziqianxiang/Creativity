title,year,conference
 Gradient based sample selectionfor online continual learning,2019, NeurIPS
 Unsupervisedlabel noise modeling and loss correction,2019, In ICML
 A closer lookat memorization in deep netWorks,2017, In ICML
 RainboW memory:Continual learning With a memory of diverse samples,2021, In CVPR
 Mixmatch: A holistic approach to semi-supervised learning,2019, NeurIPS
 RiemannianWalk for incremental learning: Understanding forgetting and intransigence,2018, In ECCV
 Beyond class-conditional assumption: A primary attempt to combat instance-dependent label noise,2021, In AAAI
 Autoaugment:Learning augmentation strategies from data,2019, In CVPR
 An empiri-cal investigation of catastrophic forgetting in gradient-based neural netWorks,2013, arXiv preprintarXiv:1312
 Co-teaching: robust training of deep neural netWorks With extremely noisy labels,2018, InNeurIPS
 Deep residual learning for image recog-nition,2016, In CVPR
 Mentornet: Learning data-driven curriculum for very deep neural networks on corrupted labels,2304, In ICML
 Overcom-ing catastrophic forgetting in neural networks,2017, PNAS
 Overcomingcatastrophic forgetting by incremental moment matching,2017, In NeurIPS
 Dividemix: Learning with noisy labels as semi-supervised learning,2019, In ICLR
 Webvision database: Visuallearning and understanding from web data,2017, arXiv preprint arXiv:1708
 Self: Learning to filter noisy labels with self-ensembling,2019, In ICLR
 Gdumb: A simple approach that questionsour progress in continual learning,2020, In ECCV
 Training deep neural networks on noisy labels with bootstrapping,2015, In ICLR (Work-shop)
 Learning to learn without forgetting by maximizing transfer and minimizing interfer-ence,2018, In ICLR
 Active learning for convolutional neural networks: A core-setapproach,2018, In ICLR
 Selfie: Refurbishing unclean samples for robustdeep learning,2019, In ICML
 Learning from noisylabels with deep neural networks: A survey,2020, arXiv preprint arXiv:2007
 Robust learning byself-transition for handling noisy labels,2021, In KDD
 Mean teachers are better role models: Weight-averaged consis-tency targets improve semi-supervised deep learning results,2017, In NeuriPs
 Learning from massive noisylabeled data for image classification,2015, In CVPR
 Improving data and model quality in crowdsourc-ing using cross-entropy-based noise correction,2021, Information Sciences
 Robust curriculum learning: From clean label detec-tion to noisy label self-correction,2020, In ICLR
