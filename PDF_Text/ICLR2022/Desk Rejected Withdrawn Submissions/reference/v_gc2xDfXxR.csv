title,year,conference
 Language models are few-shot learners,2020, In Advances in Neural InformationProcessing Systems
 End-to-end object detection with transformers,2020, In European Conferenceon Computer Vision
 Emerging properties in self-supervised vision transformers,2021, arXiv preprintarXiv:2104
 MMDetection: Open mmlab detection toolbox andbenchmark,2019, arXiv preprint arXiv:1906
 A simple frameWork forcontrastive learning of visual representations,2020, In ICML
 Exploring simple siamese representation learning,2021, In Proceedings ofthe IEEE/CVF Conference on Computer Vision and Pattern Recognition
 Improved baselines With momentumcontrastive learning,2020, arXiv preprint arXiv:2003
 An empirical study of training self-supervised visiontransformers,2021, arXiv preprint arXiv:2104
 MMSegmentation: Openmmlab semantic segmentation toolboxand benchmark,2020, https://github
 Imagenet: A large-scalehierarchical image database,2009, In CVPR
 Bert: Pre-training of deepbidirectional transformers for language understanding,2018, arXiv preprint arXiv:1810
 Animage is worth 16x16 words: Transformers for image recognition at scale,2020, arXiv preprintarXiv:2010
 Xcit: Cross-covariance image transformers,2021, arXiv preprint arXiv:2106
 Levit: a vision transformer in convnetâ€™s clothing for faster inference,2021, arXivpreprint arXiv:2104
 Bootstrap your own latent: A new approach to self-supervised learning,2020, In NeurIPS
 Deep residual learning for image recog-nition,2016, In CVPR
 Mask r-cnn,2017, In ICCV
 Momentum contrast forunsupervised visual representation learning,2020, In CVPR
 Space-time correspondence as a contrastive ran-dom walk,2020, arXiv preprint arXiv:2006
 Panoptic feature pyramid net-works,2019, In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition
 Microsoft coco: Common objects in context,2014, In ECCV
 Swin transformer: Hierarchical vision transformer using shifted windows,2021, arXiv preprintarXiv:2103
 Scalable visual transformers withhierarchical pooling,2021, arXiv preprint arXiv:2103
 Learning transferable visualmodels from natural language supervision,2021, arXiv preprint arXiv:2103
 Revisiting unreasonable ef-fectiveness of data in deep learning era,2017, In Proceedings of the IEEE international conference oncomputer vision
 Efficientnet: Rethinking model scaling for convolutional neural net-works,2019, In International Conference on Machine Learning
 Training data-effiCient image transformers & distillation through attention,2020, arXivpreprint arXiv:2012
 Goingdeeper with image transformers,2021, arXiv preprint arXiv:2103
 Attention is all you need,2017, arXiv preprint arXiv:1706
 Pyramid vision transformer: A versatile backbone for dense prediction withoutconvolutions,2021, arXiv preprint arXiv:2102
 Dense contrastive learningfor self-supervised visual pre-training,2021, In Proceedings of the IEEE/CVF Conference on ComputerVision and Pattern Recognition
 Cvt:Introducing convolutions to vision transformers,2021, arXiv preprint arXiv:2103
 Unsupervised feature learning via non-parametric instance-level discrimination,2018, In CVPR
 Region similarityrepresentation learning,2021, In ICCV
 Detco:Unsupervised contrastive learning for object detection,2021, In ICCV
 Segmentingtransparent object in the wild with transformer,2021, arXiv preprint arXiv:2101
 Self-supervised learning with swin transformers,2021, arXiv preprint arXiv:2105
 Sceneparsing through ade20k dataset,2017, In Proceedings of the IEEE conference on computer vision andpattern recognition
