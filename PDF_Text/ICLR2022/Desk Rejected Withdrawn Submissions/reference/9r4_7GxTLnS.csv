title,year,conference
 Self-supervised mul-timodal versatile networks,2020, In NeurIPS
 Layer normalization,2016, arXiv preprintarXiv:1607
 Language models arefew-shot learners,2020, In arXiv preprint arXiv:2005
 End-to-end object detection with transformers,2020, In ECCV
 Multitask learning,1997, Machine learning
 A simple framework forcontrastive learning of visual representations,2020, In ICML
 Gradnorm: Gradientnormalization for adaptive loss balancing in deep multitask networks,2018, In ICML
 A unified architecture for natural language processing: Deepneural networks with multitask learning,2008, In ICML
 Imagenet: A large-scalehierarchical image database,2009, In CVPR
 BERT: Pre-training of deepbidirectional transformers for language understanding,2019, In NAACL
 An image is worth 16x16 words: Transformers for image recognition atscale,2021, In ICLR
 Catastrophic forgetting in connectionist networks,1999, Trends in cognitive sciences
 Rich feature hierarchies for accu-rate object detection and semantic segmentation,2014, In CVPR
 Bootstrap your own latent: A new approach to self-supervised learning,2020, In NeurIPS
 Mask r-cnn,2017, In ICCV
 Bridging nonlinearities and stochastic regularizers with gaussianerror linear units,2016, arXiv preprint arXiv:1606
 Unit: Multimodal multitask learning with a unified trans-former,2021, In ICCV
 Deep networks withstochastic depth,2016, In ECCV
 One model to learn them all,2017, In arXiv preprint arXiv:1706
 The kinetics human action video dataset,2017, arXiv preprint arXiv:1705
 Multi-task learning using uncertainty to weigh lossesfor scene geometry and semantics,2018, In CVPR
 Unifiedqa: Crossing format boundaries with a single qa system,2020, arXivpreprint arXiv:2005
 Self-normalizingneural networks,2017, In NeurIPS
 Unicoder-vl: A universal encoderfor vision and language by cross-modal pre-training,2020, In AAAI
 Vilbert: Pretraining task-agnostic visiolin-guistic representations for vision-and-language tasks,2019, In NeurIPS
 12-in-1: Multi-taskvision and language representation learning,2020, In CVPR
 Attentive single-tasking of multi-ple tasks,2019, In CVPR
 The natural languagedecathlon: Multitask learning as question answering,2018, arXiv preprint arXiv:1806
 End-to-end learning of visual representations from uncurated instructional videos,2020, InCVPR
 Cross-stitch networks formulti-task learning,2016, In CVPR
 Attentionbottlenecks for multimodal fusion,2021, arXiv preprint arXiv:2107
 Exploring the limits of transfer learning with a unified text-to-texttransformer,2019, arXiv preprint arXiv:1910
 Learning multiple visual domains withresidual adapters,2017, In NeurIPS
 Multi-task learning as multi-objective optimization,2018, In NeurIPS
 Revisiting unreasonable ef-fectiveness of data in deep learning era,2017, ICCV
 Hypergrid transformers: To-wards a single model for multiple tasks,2020, In ICLR
 Attention is all you need,2017, In NeurIPS
 Taskonomy: Disentangling task transfer learning,2018, In CVPR
 Alarge-scale study of representation learning with the visual task adaptation benchmark,2019, arXivpreprint arXiv:1910
 Mixup: Beyond empir-ical risk minimization,2017, arXiv preprint arXiv:1710
 Facial landmark detection bydeep multi-task learning,2014, In ECCV
