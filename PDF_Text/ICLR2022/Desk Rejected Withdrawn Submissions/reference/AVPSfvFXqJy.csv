title,year,conference
 Layer normalization,2016, arXiv preprintarXiv:1607
 Emerging properties in self-supervised vision transformers,2021, arXiv preprintarXiv:2104
 A simple frameWork forcontrastive learning of visual representations,2020, In International conference on machine learning
 Big self-supervised models are strong semi-supervised learners,2020, arXiv preprint arXiv:2006
 Exploring simple siamese representation learning,2021, In Proceedings ofthe IEEE/CVF Conference on Computer Vision and Pattern Recognition
 Improved baselines With momentumcontrastive learning,2020, arXiv preprint arXiv:2003
 An empirical study of training self-supervised visualtransformers,2021, arXiv e-prints
 Animage is Worth 16x16 Words: Transformers for image recognition at scale,2020, arXiv preprintarXiv:2010
 Xcit: Cross-covariance image transformers,2021, arXiv preprint arXiv:2106
 Seed:Self-supervised distillation for visual representation,2021, arXiv preprint arXiv:2101
 Disco: Remedy self-supervised learning on lightWeight models With distilled con-trastive learning,2021, arXiv preprint arXiv:2104
 Imagenet-trained cnns are biased toWards texture; increasing shape bias im-proves accuracy and robustness,2018, arXiv preprint arXiv:1811
 Levit: a vision transformer in convnetâ€™s clothing for faster inference,2021, arXivpreprint arXiv:2104
 Bootstrap your oWn latent: A neW approach to self-supervised learning,2020, arXiv preprintarXiv:2006
 Momentum contrast forunsupervised visual representation learning,2020, In Proceedings of the IEEE/CVF Conference onComputer Vision and Pattern Recognition
 Benchmarking neural netWork robustness to common cor-ruptions and perturbations,2019, arXiv preprint arXiv:1903
 Distilling the knoWledge in a neural netWork,2015, arXivpreprint arXiv:1503
 Intriguing properties of vision transformers,2021, arXiv preprintarXiv:2105
 Relational knowledge distillation,2019, In Proceed-ings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition
 Designingnetwork design spaces,2020, In Proceedings of the IEEE/CVF Conference on Computer Vision andPattern Recognition
 Fitnets: Hints for thin deep nets,2014, arXiv preprint arXiv:1412
 Imagenet large scale visualrecognition challenge,2015, International journal of computer vision
 Contrastive representation distillation,2019, arXivpreprint arXiv:1910
 Training data-effiCient image transformers & distillation through attention,2021, InInternational Conference on Machine Learning
 Knowledge distillation meets self-supervision,2020, In European Conference on Computer Vision
 Paying more attention to attention: Improving the perfor-mance of convolutional neural networks via attention transfer,2016, arXiv preprint arXiv:1612
