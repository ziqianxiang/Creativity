title,year,conference
 High-dimensional dynamics of generalization error in neuralnetworks,2017, arXiv preprint arXiv:1710
 Multi-task feature learning,2006, InNIPS
 Convex multi-task feature learn-ing,2008, Machine learning
 A convergence analysis of gradientdescent for deep linear neural networks,2018, arXiv preprint arXiv:1810
 On the optimization of deep networks: Implicitacceleration by overparameterization,2018, arXiv preprint arXiv:1802
 Implicit regularization in deep matrixfactorization,2019, arXiv preprint arXiv:1905
 Learning deep lin-ear neural networks: Riemannian gradient flows and convergence to global minimizers,2019, arXivpreprint arXiv:1910
 Model-agnostic meta-learning for fast adaptationof deep networks,2017, In International Conference on Machine Learning
 Low-rank regularization and solution unique-ness in over-parameterized matrix sensing,2020, In International Conference on Artificial Intelligenceand Statistics
 Deep learning without poor local minima,2016, In Advances in neural informationprocessing Systems
 An analytic theory of generalization dynamics and transferlearning in deep linear networks,2018, arXiv preprint arXiv:1809
 Reptile: a scalable metalearning algorithm,2018, arXiv preprintarXiv:1803
 Meta-learning with memory-augmented neural networks,2016, In International conference on machine learn-ing
 Exact solutions to the nonlinear dynam-ics of learning in deep linear neural networks,2013, arXiv preprint arXiv:1312
 Prototypical networks for few-shot learning,2017, arXivpreprint arXiv:1703
 Matching networks for oneshot learning,2016, Advances in neural information processing systems
