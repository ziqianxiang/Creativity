title,year,conference
 High-performance large-scaleimage recognition without normalization,2021, ArXiv
 End-to-end object detection with transformers,2020, ArXiv
 Crossvit: Cross-attention multi-scale visiontransformer for image classification,2021, ArXiv
 Pre-trained image processing transformer,2021, In Proceedings of theIEEE/CVF Conference on Computer Vision and Pattern Recognition
 Chasing sparsityin vision transformers: An end-to-end exploration,2021, ArXiv
 Twins: Revisiting the design of spatial attention in vision transformers,2021, 2021
 Convit: Improving vision transformers with soft convolutional inductive biases,2021, In ICML
 Imagenet: A large-scale hi-erarchical image database,2009, In 2009 IEEE conference on computer vision and pattern recognition
 Cswin transformer: A general vision transformer backbone with cross-shapedwindows,2021, ArXiv
 An image is worth 16x16 words: Transformers for image recognition atscale,2021, ArXiv
 Xcit: Cross-covarianceimage transformers,2021, ArXiv
 Cmt:Convolutional neural networks meet vision transformers,2021, ArXiv
 Transformer intransformer,2021, ArXiv
 Channel pruning for accelerating very deep neural net-works,2017, In Proceedings of the IEEE international conference on computer vision
 All tokens matter: Token labeling for training better vision transformers,2021, 2021
 Similarity of neuralnetwork representations revisited,2019, ArXiv
 Localvit: Bringing locality to visiontransformers,2021, ArXiv
 Accel-erating convolutional networks via global & dynamic filter pruning,2018, In IJCAI
 Swintransformer: Hierarchical vision transformer using shifted windows,2021, ArXiv
 Designingnetwork design spaces,2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition(CVPR)
 Dynamicvit:Efficient vision transformers with dynamic token sparsification,2021, arXiv preprint arXiv:2106
 Efficientnet: Rethinking model scaling for convolutional neuralnetworks,2019, ArXiv
 Efficientnetv2: Smaller models and faster training,2021, ArXiv
 Patchslimming for efficient vision transformers,2021, arXiv preprint arXiv:2106
 Training data-efficient image transformers & distillation through attention,2021, ArXiv
 Goingdeeper with image transformers,2021, ArXiv
 Attention is all you need,2017, ArXiv
 Attention is all you need,2017, ArXiv
 Kvt:k-nn attention for boosting vision transformers,2021, ArXiv
 Pyramid vision transformer: A versatile backbone for dense prediction withoutconvolutions,2021, ArXiv
 Seg-former: Simple and efficient design for semantic segmentation with transformers,2021, arXiv preprintarXiv:2105
 Evo-vit: Slow-fast token evolution for dynamic vision trans-former,2021, arXiv preprint arXiv:2108
 Tokens-to-token vit: Training vision transformers from scratch on imagenet,2021, ArXiv
 Volo: Vision outlooker forvisual recognition,2021, ArXiv
