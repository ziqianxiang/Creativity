title,year,conference
 Harmonising chorales by probabilistic inference,2005, Ad-Vances in neural information processing Systems
 An empirical evaluation of generic convolutionaland recurrent networks for sequence modeling,2018, arXiv preprint arXiv:1803
 Classical piano midi page,1998, [EB/OL]
 Modeling temporal depen-dencies in high-dimensional sequences: Application to polyphonic music generation and tran-scription,2012, arXiv preprint arXiv:1206
 End-to-end object detection with transformers,2020, In European Conferenceon Computer Vision
 On the propertiesof neural machine translation: Encoder-decoder approaches,2014, arXiv preprint arXiv:1409
 Rethinking attentionwith performers,2020, arXiv preprint arXiv:2009
 Semi-supervised sequence learning,2015, Advances in neural informationprocessing systems
 Transformer-xl: Attentive language models beyond a fixed-length context,2019, arXiv preprintarXiv:1901
 Bert: Pre-training of deepbidirectional transformers for language understanding,2018, arXiv preprint arXiv:1810
 Animage is worth 16x16 words: Transformers for image recognition at scale,2020, arXiv preprintarXiv:2010
 Ms-tcn: Multi-stage temporal convolutional network for actionsegmentation,2019, In Proceedings of the IEEE/CVF Conference on Computer Vision and PatternRecognition
 Lstm:A search space odyssey,2016, IEEE transactions on neural networks and learning systems
 Lstm:A search space odyssey,2016, IEEE transactions on neural networks and learning systems
 Long short-term memory,1997, Neural computation
 Transformers in vision: A survey,2021, arXiv preprint arXiv:2101
 Gradient-based learning applied todocument recognition,1998, Proceedings of the IEEE
 Rethinking resnets: Improvedstacking strategies with high order schemes,2021, arXiv preprint arXiv:2103
 Rethinking resnets: Improvedstacking strategies with high order schemes,2021, arXiv preprint arXiv:2103
 Building a large annotatedcorpus of english: The penn treebank,1993, 1993
 Pointer sentinel mixturemodels,2016, arXiv preprint arXiv:1609
 Nvidia a100 tensor core gpu datasheet,1758, [EB/OL]
 3d object detection with point-former,2021, In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recogni-tion
 The lambada dataset:Word prediction requiring a broad discourse context,2016, arXiv preprint arXiv:1606
 On the difficulty of training recurrent neuralnetworks,2013, In International conference on machine learning
 Deep contextualized word representations,2018, arXiv preprint arXiv:1802
 Improving language under-standing by generative pre-training,2018, 2018
 The principles of deep learning theory,2021, arXivpreprint arXiv:2106
 Linear transformers are secretly fast weightmemory systems,2021, arXiv preprint arXiv:2102
 Attention is all you need,2017, In Advances in neural informationprocessing systems
 Wider or deeper: Revisiting the resnetmodel for visual recognition,2019, Pattern Recognition
 Go wider instead of deeper,2021, arXivpreprint arXiv:2107
 Architectural complexity measures of recurrent neural networks,2016, Advancesin neural information processing systems
