title,year,conference
 Eigenanalysis of self-attention and its reconstruction from partial computation,2021, arXiv preprint arXiv:2106
 Rethinking attention with performers,2020, arXiv preprintarXiv:2009
 What does BERT look at? an analysis ofBERT¡¯s attention,2019, arXiv preprint arXiv:1906
 Universal Transformers,2018, arXivpreprint arXiv:1807
 Imagenet: A large-scale hierarchical imagedatabase,2009, In 2009 IEEE conference on computer vision and pattern recognition
 BERT: Pre-training of deep bidirectionalTransformers for language understanding,2018, arXiv preprint arXiv:1810
 An image is worth 16x16 words:Transformers for image recognition at scale,2021, In International Conference on Learning Representations
 A structural probe for finding syntax in word representations,2019, In Proceedingsof the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: HumanLanguage Technologies
 Attention is not explanation,2019, In Proceedings of the 2019 Conference of the NorthAmerican Chapter of the Association for Computational Linguistics: Human Language Technologies
 Reformer: The efficient Transformer,2020, arXiv preprintarXiv:2001
 Albert: A lite bertfor self-supervised learning of language representations,2019, In International Conference on Learning Representations
 Limits to depth efficiencies of self-attention,2020, InH
 Are sixteen heads really better than one? In H,2019, Wallach
 Random featureattention,2020, In International Conference on Learning Representations
 A call for clarity in reporting bleu scores,2018, In Proceedings of the Third Conference on Machine Translation:Research Papers
 Exploring the limits of transfer learning with a unified text-to-text transformer,2020, Journal of MachineLearning Research
 Fixed encoder self-attention patterns in transformer-basedmachine translation,2020, In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing:Findings
 Revisiting unreasonable effectiveness of data indeep learning era,2017, In Proceedings of the IEEE international conference on computer vision
 Efficient transformers: A survey,2020, arXiv preprintarXiv:2009
 Long range arena : A benchmark for efficient transformers,2021, In International Conferenceon Learning Representations
 Attention is all you need,2017, In Advances in Neural Information Processing Systems
 Tensor2tensor for neural machine translation,2018, arXiv preprintarXiv:1803
 Superglue: a stickier benchmark for general-purpose language understanding systems,2019, InProceedings of the 33rd International Conference on Neural Information Processing Systems
 Glue: A multi-taskbenchmark and analysis platform for natural language understanding,2019, In 7th International Conference on LearningRepresentations
 Attention is not not explanation,2019, In Proceedings of the 2019 Conference onEmpirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural LanguageProcessing (EMNLP-IJCNLP)
 A broad-coverage challenge corpus for sentence understandingthrough inference,1101, In Proceedings of the 2018 Conference of the North American Chapter of the Association forComputational Linguistics: Human Language Technologies
 Big bird: Transformers for longer sequences,2020, arXiv preprintarXiv:2007
