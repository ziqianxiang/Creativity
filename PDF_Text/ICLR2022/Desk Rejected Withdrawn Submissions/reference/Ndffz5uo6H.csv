title,year,conference
 Rethinking attentionwith performers,2020, arXiv preprint arXiv:2009
 Transformer-xl: Attentive language models beyond a fixed-length context,2019, arXiv preprintarXiv:1901
 Bert: Pre-training of deepbidirectional transformers for language understanding,2018, arXiv preprint arXiv:1810
 Shaping belief states with generative environment models for rl,2019, arXiv preprintarXiv:1906
 Tracking the worldstate with recurrent entity networks,2016, arXiv preprint arXiv:1612
 Long short-term memory,1997, Neural computation
 Parallel and serial grouping of image elements in visualperception,2010, Journal of experimental psychology
 Transformers in vision: A survey,2021, arXiv preprint arXiv:2101
 Gradient-based learning applied todocument recognition,1998, Proceedings of the IEEE
 Reviving and improving recurrent back-propagation,2018, In InternationalConference on Machine Learning
 Decoupled weight decay regularization,2017, arXiv preprintarXiv:1711
 Recurrentneural network based language model,2010, In Eleventh annual conference of the international speechcommunication association
 A three-way model for collective learningon multi-relational data,2011, In Icml
 Languagemodels are unsupervised multitask learners,2019, OpenAI blog
 Unbiasing truncated backpropagation through time,2017, arXivpreprint arXiv:1705
 Teach-ing pre-trained models to systematically reason over implicit knowledge,2020, arXiv preprintarXiv:2006
 Long range arena: A benchmark for efficienttransformers,2020, arXiv preprint arXiv:2011
 Attention is all you need,2017, arXiv preprint arXiv:1706
 Linformer: Self-attention withlinear complexity,2020, arXiv preprint arXiv:2006
 Backpropagation through time: what it does and how to do it,1990, Proceedings of theIEEE 
 Towards ai-complete question answering: A set of prerequisite toytasks,2015, arXiv preprint arXiv:1502
 This task was re-introduced as a machine learning benchmarkby Linsley et al,2020, (2019)
