title,year,conference
 Knowledge enhanced fine-tuning for better han-dling unseen entities in dialogue generation,2021, Proceedings of the 2021 Conference on EmpiricalMethods in Natural Language Processing (EMNLP)
 Bert: Pre-training of deepbidirectional transformers for language understanding,2019, In Conference of the North AmericanChapter of the Association for Computational Linguistics (NAACL)
 Representation degenerationproblem in training natural language generation models,2019, International Conference for LearningRepresentation (ICLR)
 Frage: frequency-agnosticword representation,2018, In Conference on Neural Information Processing Systems (Neruips)
 A knowledge-enhancedpretraining model for commonsense story generation,2020, In Transactions of the Association forComputational Linguistics (TACL)
 Mind the facts: Knowledge-boosted coherent abstractive text summarization,2020, arXiv preprint arXiv:2006
 Donâ€™t stop pretraining: Adapt language models to domains and tasks,2020, InProceedings of Annual Meeting of the Association for Computational Linguistics (ACL)
 Adam: A method for stochastic optimization,2015, InternationalConference for Learning Representation (ICLR)
 Biobert: a pre-trained biomedical language representation model for biomedical textmining,2020, Bioinformatics
 K-bert:Enabling language representation with knowledge graph,2020, In AAAI Conference on Artificial Intel-ligence (AAAI)
 Kg-bart: Knowledge graph-augmented bartfor generative commonsense reasoning,2021, In Conference on Artificial Intelligence (AAAI)
 Roberta: A robustly optimized bert pretrainingapproach,2019, arXiv preprint arXiv:1907
 Catastrophic interference in connectionist networks: Thesequential learning problem,1989, In Psychology of learning and motivation
 Representation learning with contrastive predic-tive coding,2018, arXiv preprint arXiv:1807
 On variationalbounds of mutual information,2019, In International Conference on Machine Learning (ICML)
 Pre-trainedmodels for natural language processing: A survey,2020, Science China Technological Sciences
 Exploring the limits of transfer learning with a unified text-to-texttransformer,2020, In Journal of Machine Learning Research (JMLR)
 Lacking the embedding of a word? look it up into a traditional dictio-nary,2021, arXiv preprint arXiv:2109
 Rare words: A major problem for contextualized embeddingsand how to fix it by attentive mimicking,2020, In Proceedings of the AAAI Conference on ArtificialIntelligence (AAAI)
 On mutualinformation maximization for representation learning,2020, International Conference for LearningRepresentation(ICLR)
 Attention is all you need,2017, In Advances in neural informationprocessing systems (Neruips)
 Glue:A multi-task benchmark and analysis platform for natural language understanding,2019, InternationalConference for Learning Representation (ICLR)
 K-adapter: Infusing knowledge into pre-trained models with adapters,2021, Pro-ceedings of Annual Meeting of the Association for Computational Linguistics (ACL)
 Transformers: State-of-the-art natural language processing,2020, In Proceedings of the 2020 Conference on Empirical Methods inNatural Language Processing: System Demonstrations (EMNLP)
 Taking notes on the fly helpsbert pre-training,2021, International Conference for Learning Representation (ICLR)
 Pretrained encyclopedia:Weakly supervised knowledge-pretrained language model,2020, In International Conference of Learn-ing Representation (ICLR)
 Does knowledge help general nlu?an empirical study,2021, arXiv preprint arXiv:2109
 Jaket: Joint pre-training of knowl-edge graph and language understanding,2020, arXiv preprint arXiv:2010
 Asurvey of knowledge-enhanced text generation,2020, arXiv preprint arXiv:2010
 Ernie: Enhancedlanguage representation with informative entities,2019, In Annual Meeting of the Association for Com-putational Linguistics (ACL)
 Pre-training text-to-text transformers for concept-centric common sense,2021, In InternationalConference for Learning Representation (ICLR)
