title,year,conference
 Knowledgedistillation from internal representations,2020, In Proceedings of the AAAI Conference on ArtificialIntelligence
 High-performance large-scaleimage recognition without normalization,2021, arXiv preprint arXiv:2102
 On the efficacy of knowledge distillation,2019, In Proceedingsof the IEEE/CVF International Conference on Computer Vision
 Imagenet: A large-scale hi-erarchical image database,2009, In 2009 IEEE conference on computer vision and pattern recognition
 Pairwiseconfusion for fine-grained visual classification,2018, In Proceedings of the European conference oncomputer vision (ECCV)
 Low-resolution face recognition in the wildvia selective knowledge distillation,2018, IEEE Transactions on Image Processing
 Distilling channels forefficient deep tracking,2019, IEEE Transactions on Image Processing
 Distilling the knowledge in a neural network,2015, arXivpreprint arXiv:1503
 Squeeze-and-excitation networks,2018, In Proceedings of the IEEEconference on computer vision and pattern recognition
 Reconstruction regularizeddeep metric learning for multi-label image classification,2019, IEEE transactions on neural networksand learning systems
 Improving the interpretability of deep neural net-works with knowledge distillation,2018, In 2018 IEEE International Conference on Data Mining Work-Shops (ICDMW)
 Sgdr: Stochastic gradient descent with warm restarts,2016, arXivpreprint arXiv:1608
 Mixed precisiontraining,2017, arXiv preprint arXiv:1710
 When does label smoothing help? InHanna M,2019, Wallach
 Unsupervised knowledge transfer using similarity embed-dings,2018, IEEE transactions on neural networks and learning systems
 Few-shot imagerecognition with knowledge transfer,2019, In Proceedings of the IEEE/CVF International Conferenceon Computer Vision
 Regulariz-ing neural networks by penalizing confident output distributions,2017, In 5th International Conferenceon Learning Representations
 Weight standardization,2019, 2019
 Training deep neural networks on noisy labels with bootstrapping,2014, arXiv preprintarXiv:1412
 Training deep neural networks on noisy labels with bootstrapping,2015, In Yoshua Bengioand Yann LeCun (eds
 Rethink-ing the inception architecture for computer vision,2016, In Proceedings of the IEEE conference oncomputer vision and pattern recognition
 Memory-replay knowledge distillation,2021, Sensors
 Real-time correlation track-ing via joint model compression and transfer,2020, IEEE Transactions on Image Processing
 Pytorch image models,2019, https://github
 Group normalization,2018, In Proceedings of the European conference oncomputer vision (ECCV)
 Disturblabel: Regularizingcnn on the loss layer,2016, In Proceedings of the IEEE Conference on Computer Vision and PatternRecognition
 Revisiting knowledge distillationvia label smoothing regularization,2020, In Proceedings of the IEEE/CVF Conference on ComputerVision and Pattern Recognition
 Regularizing class-wise predictions viaself-knowledge distillation,2020, In Proceedings of the IEEE/CVF conference on computer vision andpattern recognition
 Delving deep into label smoothing,2021, IEEE Transactions on Image Processing
 mixup: Beyond empiricalrisk minimization,2017, arXiv preprint arXiv:1710
 Be yourown teacher: Improve the performance of convolutional neural networks via self distillation,2019, InProceedings of the IEEE/CVF International Conference on Computer Vision
 Heated-up softmaxembedding,2018, arXiv preprint arXiv:1809
 Deep mutual learning,2018, InProceedings of the IEEE Conference on Computer Vision and Pattern Recognition
