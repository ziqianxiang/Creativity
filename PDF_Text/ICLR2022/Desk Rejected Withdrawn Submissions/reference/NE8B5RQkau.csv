title,year,conference
 Variationalinformation distillation for knowledge transfer,2019, In Proceedings of the IEEE/CVF Conference onComputer Vision and Pattern Recognition
 Structured pruning of deep convolutional neuralnetworks,2017, ACM Journal on Emerging Technologies in Computing Systems (JETC)
 The generalization-stabilitytradeoff in neural network pruning,2019, arXiv preprint arXiv:1906
 UnsuPervisedcross-lingual representation learning at scale,2019, arXiv preprint arXiv:1911
 Bert: Pre-training of deePbidirectional transformers for language understanding,2018, arXiv preprint arXiv:1810
 Learning to Prune deeP neural networks vialayer-wise oPtimal brain surgeon,2017, arXiv preprint arXiv:1705
 A comPutationally efficient estimator for mutual information,2008, Proceedings of theRoyal Society A: Mathematical
 Learning both weights and connections forefficient neural networks,2015, arXiv preprint arXiv:1506
 Second order derivatives for network pruning: Optimal brainsurgeon,1993, Morgan Kaufmann
 Distilling the knowledge in a neural network,2015, arXivpreprint arXiv:1503
 A simPle Procedure for Pruning back-ProPagation trained neural networks,1990, IEEEtransactions on neural networks
 Fast convnets using grouP-wise brain damage,2016, In Proceedingsof the IEEE Conference on Computer Vision and Pattern Recognition
 OPtimal brain damage,1990, In Advances in neuralinformation processing systems
 Layer-adaptive sparsity forthe magnitude-based pruning,2020, In International Conference on Learning Representations
 Pruning filters forefficient convnets,2016, arXiv preprint arXiv:1608
 Learn-ing efficient convolutional networks through network slimming,2017, In Proceedings of the IEEEInternational Conference on Computer Vision
 Learning sparse neural networks throughl_0 regularization,2017, arXiv preprint arXiv:1712
 Piggyback: Adapting a single network to multipletasks by learning to mask weights,2018, In Proceedings of the European Conference on ComputerVision (ECCV)
 Optimizing neural networks with kronecker-factored approximatecurvature,2015, In International conference on machine learning
 Self-distillation amplifies regularizationin hilbert space,2020, arXiv preprint arXiv:2002
 Variational dropout sparsifies deep neuralnetworks,2017, In International Conference on Machine Learning
 Semantically-conditioned negative samples for efficientcontrastive learning,2021, arXiv preprint arXiv:2102
 Lookahead: A far-sighted alternative ofmagnitude-based pruning,2020, arXiv preprint arXiv:2002
 Relational knowledge distillation,2019, In Proceed-ings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition
 Pruning algorithms-a survey,1993, IEEE transactions on Neural Networks
 Fitnets: Hints for thin deep nets,2014, arXiv preprint arXiv:1412
 Movement pruning: Adaptive sparsity byfine-tuning,2020, arXiv preprint arXiv:2005
 Woodfisher: Efficient second-order approximations for modelcompression,2020, arXiv preprint arXiv:2004
 Contrastive representation distillation,2019, arXivpreprint arXiv:1910
 Well-read students learn better:On the importance of pre-training compact models,2019, arXiv preprint arXiv:1908
 Information-theoretic measures of influence based on contentdynamics,2013, In Proceedings of the sixth ACM international conference on Web search and datamining
 Glue:A multi-task benchmark and analysis platform for natural language understanding,2018, arXiv preprintarXiv:1804
 Learning structured sparsity indeep neural networks,2016, arXiv preprint arXiv:1608
 Training deep neural networks ingenerations: A more tolerant teacher educates better students,2019, In Proceedings of the AAAIConference on Artificial Intelligence
 Rethinking the smaller-norm-less-informativeassumption in channel pruning of convolution layers,2018, arXiv preprint arXiv:1802
 Model selection and estimation in regression with grouped variables,2006, Journalof the Royal Statistical Society: Series B (Statistical Methodology)
 Barlow twins: Self-supervisedlearning via redundancy reduction,2021, arXiv preprint arXiv:2103
