title,year,conference
 Neural machine translation by jointlylearning to align and translate,2014, arXiv preprint arXiv:1409
 Multigrain: aunified image embedding for classes and instances,2019, arXiv preprint arXiv:1902
 End-to-end object detection with transformers,2020, In European Conference on ComputerVision
 Pre-trained image processing transformer,2020, arXiv preprintarXiv:2012
 Permuteformer: Efficient relative position encoding for long sequences,2021, arXiv preprintarXiv:2109
 Generating long sequences with sparsetransformers,2019, arXiv preprint arXiv:1904
 Rethinking attentionwith performers,2020, arXiv preprint arXiv:2009
 Summed-area tables for texture mapping,1984, In Proceedings of the 11th annualconference on Computer graphics and interactive techniques
 Randaugment: Practical automateddata augmentation with a reduced search space,2020, In Proceedings of the IEEE/CVF Conference onComputer Vision and Pattern Recognition Workshops
 Imagenet: A large-scalehierarchical image database,2009, In 2009 IEEE conference on computer vision and pattern recognition
 Bert: Pre-training of deepbidirectional transformers for language understanding,2018, arXiv preprint arXiv:1810
 Cswin transformer: A general vision transformer backbone with cross-shapedwindows,2021, arXiv preprint arXiv:2107
 Animage is worth 16x16 words: Transformers for image recognition at scale,2020, arXiv preprintarXiv:2010
 Fast convergence ofdetr with sPatially modulated co-attention,2021, arXiv preprint arXiv:2101
 Understanding the difficulty of training deeP feedforward neuralnetworks,2010, In Proceedings of the thirteenth international conference on artificial intelligence andstatistics
 DeeP residual learning for imagerecognition,2016, In Proceedings of the IEEE conference on computer vision and pattern recognition
 Axial attention in multidi-mensional transformers,2019, arXiv preprint arXiv:1912
 Augmentyour batch: ImProving generalization through instance rePetition,2020, In Proceedings of the IEEE/CVFConference on Computer Vision and Pattern Recognition
 DeeP networks withstochastic dePth,2016, In European conference on computer vision
 Finetuning Pretrained transformers into rnns,2021, arXiv preprintarXiv:2103
 Transformers are rnns:Fast autoregressive transformers with linear attention,2020, In International Conference on MachineLearning
 Reformer: The efficient transformer,2020, InInternational Conference on Learning Representations
 Learning multiple layers of features from tiny images,2009, 2009
 Imagenet classification with deep con-volutional neural networks,2012, Advances in neural information processing systems
 Localvit: Bringing locality tovision transformers,2021, arXiv preprint arXiv:2104
 Microsoft coco: Common objects in context,2014, In Europeanconference on computer vision
 Swin transformer: Hierarchical vision transformer using shifted windows,2021, arXiv preprintarXiv:2103
 Video swintransformer,2021, arXiv preprint arXiv:2106
 Sgdr: Stochastic gradient descent with warm restarts,2016, arXivpreprint arXiv:1608
 Decoupled weight decay regularization,2019, In International Confer-ence on Learning Representations
 Conditional detr for fast training convergence,2021, arXiv preprint arXiv:2108
 Image transformer,4055, In International Conference on Machine Learning
 Linear transformers are secretly fast weightprogrammers,2021, In International Conference on Machine Learning
 Natural image statistics and neural representation,2001, Annualreview of neuroscience
 Segmenter: Transformer forsemantic segmentation,2021, arXiv preprint arXiv:2105
 Roformer: Enhanced transformer withrotary position embedding,2021, arXiv preprint arXiv:2104
 Long range arena: A benchmark for efficienttransformers,2020, arXiv preprint arXiv:2011
 Efficient transformers: A survey,2020, arXivpreprint arXiv:2009
 Training data-efficient image transformers & distillation through attention,2020, arXiv preprintarXiv:2012
 Attention is all you need,2017, arXiv preprint arXiv:1706
 Robust real-time object detection,2001, International journal of computervision
 Linformer: Self-attention withlinear complexity,2020, arXiv preprint arXiv:2006
 Pyramid vision transformer: A versatile backbone for dense prediction withoutconvolutions,2021, arXiv preprint arXiv:2102
 Pvtv2: Improved baselines with pyramid vision transformer,2021, arXiv preprintarXiv:2106
 End-to-end video instance segmentation with transformers,2020, arXiv preprint arXiv:2011
 All of nonparametric statistics,2006, Springer Science & Business Media
 Pytorch image models,2019, https://github
 Cvt:Introducing convolutions to vision transformers,2021, arXiv preprint arXiv:2103
 Earlyconvolutions help transformers see better,2021, arXiv preprint arXiv:2106
 Vitae: Vision transformer advanced byexploring intrinsic inductive bias,2021, arXiv preprint arXiv:2106
 Focal self-attention for local-global interactions in vision transformers,2021, arXiv preprintarXiv:2107
 Incorporatingconvolution designs into visual transformers,2021, arXiv preprint arXiv:2103
 mixup: Beyond empiricalrisk minimization,2017, arXiv preprint arXiv:1710
 RiPPle-softmax aims to inject radial bias through riPPling into thevanilla softmax attention,2018, Here We Present tWo Possible Ways to achieve this goal
