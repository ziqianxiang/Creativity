title,year,conference
 Layer normalization,2016, arXiv preprintarXiv:1607
 On invariance in hierarchicalmodels,2009, In Y
 Imagenet: A large-scale hier-archical image database,2009, In 2009 IEEE conference on computer vision and pattern recognition
 Animage is worth 16x16 words: Transformers for image recognition at scale,2020, arXiv preprintarXiv:2010
 Deep pyramidal residual networks,2017, In Proceedingsofthe IEEE conference on computer vision and pattern recognition
 Aug-ment your batch: Improving generalization through instance repetition,2020, In Proceedings of theIEEE/CVF Conference on Computer Vision and Pattern Recognition
 Adam: A method for stochastic optimization,2014, arXiv preprintarXiv:1412
 Learning multiple layers of features from tiny images,2009, pp
 Imagenet classification with deep con-volutional neural networks,2012, In Proceedings of the Advances in Neural Information ProcessingSystems
 Fully convolutional networks for semanticsegmentation,2015, In Proceedings of the IEEE conference on computer vision and pattern recogni-tion
 Designingnetwork design spaces,2020, In Proceedings of the IEEE/CVF Conference on Computer Vision andPattern Recognition
 Very deep convolutional networks for large-scale imagerecognition,2014, arXiv preprint arXiv:1409
 Training data-effiCient image transformers & distillation through attention,2020, arXivpreprint arXiv:2012
 Training data-efficient image transformers & distillation through attention,2021, InInternational Conference on Machine Learning
 Attention is all you need,2017, In Advances in neural infor-mation processing systems
 Wide residual networks,2016, arXiv preprintarXiv:1605
 Aggregating nested trans-formers,2021, In arXiv preprint arXiv:2105
