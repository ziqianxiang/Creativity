title,year,conference
 Lsq+: Improvinglow-bit quantization through learnable offsets and better initialization,2020, In Proceedings of theIEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops
 Yolov4: Optimal speed andaccuracy of object detection,2020, arXiv preprint arXiv:2004
 Deep clustering for unsuper-vised learning of visual features,2018, In Proceedings of the European Conference on Computer Vision(ECCV)
 Unsupervised pre-training ofimage features on non-curated data,2019, In Proceedings of the IEEE/CVF International Conference onComputer Vision
 The lottery tickets hypothesis for supervised and self-supervised pre-training incomputer vision models,2020, arXiv preprint arXiv:2012
 Adversarialrobustness: From self-supervised pre-training to fine-tuning,2020, In Proceedings of the IEEE/CVFConference on Computer Vision and Pattern Recognition
 Exploring simple siamese representation learning,2020, arXiv preprintarXiv:2011
 Improved baselines with momentumcontrastive learning,2020, arXiv preprint arXiv:2003
 Imagenet: A large-scalehierarchical image database,2009, In 2009 IEEE conference on computer vision and pattern recognition
 Bert: Pre-training of deepbidirectional transformers for language understanding,2018, arXiv preprint arXiv:1810
 Nice: Non-linear independent componentsestimation,2014, arXiv preprint arXiv:1410
 Releq: A reinforcement learning approach for automatic deep quantization ofneural networks,2020, IEEE Micro
 Learned step size quantization,2019, arXiv preprint arXiv:1902
 The pascal visual object classes challenge: A retrospective,2015, International journal ofcomputer vision
 Fractrain: Fractionally squeezing bit savings both temporally and spatiallyfor efficient dnn training,2020, arXiv preprint arXiv:2012
 Cpt:Efficient deep neural network training via cyclic precision,2021, arXiv preprint arXiv:2101
 Unsupervised representation learning bypredicting image rotations,2018, arXiv preprint arXiv:1803
 Bootstrap your own latent: A new approach to self-supervised learning,2020, arXiv preprintarXiv:2006
 Momentum contrast forunsupervised visual representation learning,2020, In Proceedings of the IEEE/CVF Conference onComputer Vision and Pattern Recognition
 Quantization and training of neural networks for efficientinteger-arithmetic-only inference,2018, In Proceedings of the IEEE Conference on Computer Vision andPattern Recognition
 A survey on contrastive self-supervised learning,2021, Technologies
 Robust pre-training by adversarialcontrastive learning,2020, arXiv preprint arXiv:2010
 Learning to quantize deep networks by optimizing quantizationintervals with task loss,2019, In Proceedings of the IEEE Conference on Computer Vision and PatternRecognition
 On translation invariance in cnns: Convolutional layerscan exploit absolute spatial location,2020, In Proceedings of the IEEE/CVF Conference on ComputerVision and Pattern Recognition
 Variational graph auto-encoders,2016, arXiv preprint arXiv:1611
 Learning multiple layers of features from tiny images,2009,2009
 Learning representations for automaticcolorization,2016, In European conference on computer vision
 Ternary weight networks,2016, arXiv preprint arXiv:1605
 Self-supervised learning: Generative or contrastive,2020, arXiv preprint arXiv:2006
 Efficient estimation of word representa-tions in vector space,2013, arXiv preprint arXiv:1301
 Distributed repre-sentations of words and phrases and their compositionality,2013, arXiv preprint arXiv:1310
 Apprentice: Using knowledge distillation techniques to improvelow-precision network accuracy,2017, arXiv preprint arXiv:1711
 Wrpn: wide reduced-precisionnetworks,2017, arXiv preprint arXiv:1709
 Unsupervised learning of visual representations by solving jigsawpuzzles,2016, In European conference on computer vision
 Weighted-entropy-based quantization for deep neu-ral networks,2017, In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition
 Improving language under-standing by generative pre-training,2018,2018
 Languagemodels are unsupervised multitask learners,2019, OpenAI blog
 Generating diverse high-fidelity images withvq-vae-2,2019, arXiv preprint arXiv:1906
 Contrastive multiview coding,2019, arXiv preprintarXiv:1906
 Visualizing data using t-sne,2008, Journal of machinelearning research
 Pixel recurrent neural networks,2016, InInternational Conference on Machine Learning
 Haq: Hardware-aware automatedquantization with mixed precision,2019, In Proceedings of the IEEE conference on computer vision andpattern recognition
 Trainingdeep neural networks with 8-bit floating point numbers,2018, In Advances in neural informationprocessing systems
 Contrastive learning with stronger augmentations,2021, arXiv preprintarXiv:2104
 Adversarial weight perturbation helps robust general-ization,2020, Advances in Neural Information Processing Systems
 Unsupervised feature learning via non-parametric instance discrimination,2018, In Proceedings of the IEEE Conference on Computer Visionand Pattern Recognition
 Dnq: Dynamicnetwork quantization,2018, arXiv preprint arXiv:1812
 Graph convolutional policynetwork for goal-directed molecular graph generation,2018, arXiv preprint arXiv:1806
 Making convolutional networks shift-invariant again,2019, In International Conference onMachine Learning
 Colorful image colorization,2016, In Europeanconference on computer vision
 Dorefa-net: Train-ing low bitwidth convolutional neural networks with low bitwidth gradients,2016, arXiv preprintarXiv:1606
 Adaptivequantization for deep neural network,2017, arXiv preprint arXiv:1712
 Trained ternary quantization,2016, arXivpreprint arXiv:1612
