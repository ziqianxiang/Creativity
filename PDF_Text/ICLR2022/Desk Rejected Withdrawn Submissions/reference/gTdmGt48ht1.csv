title,year,conference
 A convergence theory for deep learning via over-parameterization,2019, InInternational Conference on Machine Learning
 Fine-grained analysis of optimization andgeneralization for overparameterized two-layer neural networks,2019, In International Conference on Machine Learning
 Generalization of two-layer neuralnetworks: an asymptotic viewpoint,2020, In International Conference on Learning Representations
 Benign overfitting in linear regression,2020, theNational Academy of Sciences
 Two models of double descent for weak features,2020, SIAM J
 Reconciling modern machine-learning practice and theclassical bias-variance trade-off,2019, the National Academy of Sciences
 Learning with SGD and random features,2018, In Advances inNeural Information Processing Systems
 Dimension independent generalization error by stochastic gradient descent,2020, arXivpreprint arXiv:2003
 Implicit bias of gradient descent for wide two-layer neural networks trained with thelogistic loss,2020, In Conference on Learning Theory
 On lazy training in differentiable programming,2019, In Advances inNeural Information Processing Systems
 Kernel methods for deep learning,2009, In Advances in Neural Information ProcessingSystems
 A precise performance analysis of learning with random features,2020, arXiv preprintarXiv:2008
 Nonparametric stochastic approximation with large step-sizes,2016, Annals ofStatistics
 The spectrum of kernel random matrices,2010, Annals of Statistics
 Generalisation error inlearning with random features and the hidden manifold model,2020, In International Conference on Machine Learning
 Linearized two-layers neural networks inhigh dimension,2021, The Annals of Statistics
 Surprises in high-dimensional ridgelessleast squares interpolation,2019, arXiv preprint arXiv:1903
 Universality laws for high-dimensional learning with random features,2020, arXiv preprintarXiv:2009
 The surprising simplicity of the early-time learningdynamics of neural networks,2020, In Advances in Neural Information Processing Systems
 Neural tangent kernel: Convergence and generalization in neuralnetworks,2018, In Advances in Neural Information Processing Systems
 On the generalization power of overfitted two-layer neural tangent kernelmodels,2020, In International Conference on Machine Learning
 Gradient descent finds global minima for generalizable deep neural networks ofpractical sizes,2019, In IEEE Conference on Communication
 Gradient-based learning applied to documentrecognition,1998, Proceedings of the IEEE
 Towards a unified analysis of random Fourier features,2019, Inthe 36th International Conference on Machine Learning
 Towards an understanding of benign overfitting in neural networks,2021, arXivpreprint arXiv:2106
 On the multiple descent of minimum-norm interpolants andrestricted lower isometry of kernels,2019, In Annual Conference on Learning Theory
 What causes the test error? going beyond bias-variance via anova,2021, Journal of MachineLearning Research
 Kernel regression in high dimensions: Refined analysis beyonddouble descent,2021, In International Conference on Artificial Intelligence and Statistics
 A random matrix approach to neural networks,2018, The Annals ofApplied Probability
 The generalization error of random features regression: Precise asymptotics anddouble descent curve,2019, arXiv preprint arXiv:1908
 Mean-field theory of two-layers neural networks: dimension-free bounds and kernel limit,2019, In Conference on Learning Theory
 Deep double descent:Where bigger models and more data hurt,2019, In International Conference on Learning Representations
 Geometry of optimization and implicitregularization in deep learning,2017, arXiv preprint arXiv:1705
 Optimal rates for averaged stochastic gradient descent under neural tangent kernelregime,2020, In International Conference on Learning Representations
 Random features for large-scale kernel machines,2007, In Advances in Neural InformationProcessing Systems
 Generalization properties of learning with random features,2017, In Advances inNeural Information Processing Systems
 On the origin of implicit regularization in stochasticgradient descent,2020, In International Conference on Learning Representations
 Last iterate convergence of sgd for least-squares in theinterpolation regime,2021, arXiv preprint arXiv:2102
 Computation with infinite neural networks,1998, Neural Computation
 On the optimal weighted `2 regularization in overparameterized linear regression,2020, In Advances inNeural Information Processing Systems
 Rethinking bias-variance trade-off for generaliza-tion of neural networks,2020, In International Conference on Machine Learning
 Understanding deep learning requiresrethinking generalization,2016, arXiv preprint arXiv:1611
 An improved analysis of training over-parameterized deep neural networks,2019, Advances inNeural Information Processing Systems
 Benign overfitting of constant-stepsize sgd for linear regression,2021, In Conference on Learning Theory
