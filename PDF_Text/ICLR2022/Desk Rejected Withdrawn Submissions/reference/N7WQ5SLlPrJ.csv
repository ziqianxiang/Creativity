title,year,conference
 Un-covering and mitigating algorithmic bias through learned latent structure,2019, In Proceedings of the2019AAAI/ACM Conference on AI
 On the opportu-nities and risks of foundation models,2021, arXiv preprint arXiv:2108
 Nuancedmetrics for measuring unintended bias with real data for text classification,2019, In Companion pro-ceedings of the 2019 world wide web conference
 Language models arefew-shot learners,2020, arXiv preprint arXiv:2005
 Fairness in machine learning: A survey,2020, arXiv preprintarXiv:2010
 Measuring and miti-gating unintended bias in text classification,2018, In Proceedings of the 2018 AAAI/ACM Conferenceon AI
 Datasheets for datasets,2018, arXiv preprint arXiv:1803
 Evaluating fairness metrics inthe presence of dataset bias,2018, arXiv preprint arXiv:1809
 Distilling the knowledge in a neural network,2015, arXivpreprint arXiv:1503
 Characterisingbias in compressed models,2020, arXiv preprint arXiv:2010
 Searching for mobilenetv3,2019, In Pro-ceedings of the IEEE/CVF International Conference on Computer Vision
 Learning multiple layers of features from tiny images,2009, Technical report
 A surveyon bias and fairness in machine learning,2019, arXiv preprint arXiv:1908
 Quan-titative analysis of culture using millions of digitized books,2011, science
 Model cards for model reporting,2019, InProceedings of the conference on fairness
 Comparative experiments on disambiguating word senses: An illustration ofthe role of bias in machine learning,1996, arXiv preprint cmp-lg/9612001
 Learning deep representations with probabilistic knowledgetransfer,2018, In Proceedings of the European Conference on Computer Vision (ECCV)
 Mo-bilenetv2: Inverted residuals and linear bottlenecks,2018, In Proceedings of the IEEE conference oncomputer vision and pattern recognition
 Efficientnet: Rethinking model scaling for convolutional neural net-works,2019, In International Conference on Machine Learning
 Contrastive representation distillation,2019, arXivpreprint arXiv:1910
 Training data-effiCient image transformers & distillation through attention,2021, InInternational Conference on Machine Learning
 Similarity-preserving knowledge distillation,2019, In Proceedings of theIEEE/CVF International Conference on Computer Vision
 Paying more attention to attention: Improving theperformance of convolutional neural networks via attention transfer,2017, In ICLR
