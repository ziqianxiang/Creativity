title,year,conference
 Obfuscated gradients give a false sense ofsecurity: Circumventing defenses to adversarial examples,2018, In ICML
 Revisiting batch normalization forimproving corruption robustness,2021, In WACV
 A unified vieWof pieceWise linear neural netWork verification,2018, NeurIPS
 Mitigating evasion attacks to deep neural netWorks viaregion-based classification,2017, In ACSAC
 Autodial:Automatic domain alignment layers,2017, In ICCV
 Unlabeled dataimproves adversarial robustness,2019, arXiv
 Certified adversarial robustness via randomizedsmoothing,2019, ICML
 Imagenet: A large-scalehierarchical image database,2009, In CVPR
 A frameWork for robustness certification ofsmoothed classifiers using f-divergences,2020, In ICLR
 On the connectionbetWeen adversarial robustness and saliency map interpretability,2019, In ICML
 Self-ensembling for visual domain adapta-tion,2017, arXiv
 Ai2: Safety and robustness certification of neural netWorks With abstract interpreta-tion,2018, In IEEE SP
 Explaining and harnessing adversarialexamples,2015, In ICLR
 On the effectiveness of intervalbound propagation for training verifiably robust models,2018, arXiv
 Unering the limitsof adversarial training against norm-bounded adversarial examples,2020, arXiv
 Deep residual learning for image recog-nition,2016, In IEEE CVPR
 Identity mappings in deep residualnetworks,2016, In ECCV
 Benchmarking neural network robustness to common cor-ruptions and perturbations,2019, In ICLR
 Decorrelated batch normalization,2018, In CVPR
 Batch normalization: Accelerating deep network training byreducing internal covariate shift,2015, In ICML
 The robust manifolddefense: Adversarial training using generative models,2019, arXiv
 Consistency regularization for certified robustness of smoothedclassifiers,2020, NeurIPS
 Adversarial machine learning at scale,2016, arXivpreprint arXiv:1611
 Certifiedrobustness to adversarial examples with differential privacy,2019, In IEEE SP
 Tight certificates of adversarialrobustness for randomly smoothed classifiers,2019, arXiv preprint arXiv:1906
 Certified adversarial robustness withadditive noise,2019, NeurIPS
 Revisiting batch normaliza-tion for practical domain adaptation,2016, arXiv
 Towards robust neural networksvia random self-ensemble,2018, In ECCV
 Adversarial attacks arereversible with natural supervision,2021, arXiv preprint arXiv:2103
 Differentiable abstract interpretation for prov-ably robust neural networks,2018, In ICML
 Certify or predict: Boosting certifiedrobustness with compositional architectures,2021, In ICLR
 Evaluating prediction-time batch normalization for robustness under covariateshift,2020, arXiv
 Approximate manifold defense against multiple adver-sarial perturbations,2020, In IJCNN
 Bag of tricks for adversarialtraining,2021, In ICLR
 Adversarial robustness through local linearization,2019, In NeurIPS
 Certified defenses against adversarial exam-ples,2018, ICLR
 Overfitting in adversarially robust deep learning,2020, In ICML
 Unsupervised domain adaptation using feature-whitening and consensus loss,2019, In CVPR
 Provably robust deep learning via adversarially trained smoothed classifiers,2019, InNeurIPS
 A convex relaxationbarrier to tight robustness verification of neural networks,2019, In NeurIPS
 Denoised smoothing: Aprovable defense for pretrained classifiers,2020, NeurIPS
 Towards the first adversariallyrobust neural network model on MNIST,2019, In ICLR
 Provably robust classification of adversarialexamples with detection,2021, In ICLR
 Machine learning in non-stationary environments:Introduction to covariate shift adaptation,2012, MIT press
 Correlation alignment for unsupervised domain adap-tation,2017, In Domain Adaptation in Computer Vision Applications
 Test-timetraining with self-supervision for generalization under distribution shifts,2020, In ICML
 Intriguing properties of neural networks,2014, In ICLR
 Evaluating robustness of neural networks with mixedinteger programming,2017, ICLR
 Adversarial training and robustness for multiple perturbations,2019, InNeurIPS
 Adversarial riskand the dangers of evaluating against weak attacks,2018, ICML
 Tent: Fullytest-time adaptation by entropy minimization,2020, arXiv
 Efficient formal safetyanalysis of neural networks,2018, In NeurIPS
 Towards fast computation of certified robustness for relu networks,2018, In ICML
 Provable defenses against adversarial examples via the convex outeradversarial polytope,2018, In ICML
 Scaling provable adversarialdefenses,2018, NeurIPS
 Self-training with noisy studentimproves imagenet classification,2020, In CVPR
 Randomizedsmoothing of all shapes and sizes,2020, arXiv
 Theoretically principled trade-off between robustness and accuracy,2019, InICML
 Attacks which do not kill training make adversarial learning stronger,2020, In ICML
 We applythe PGD attack With the same hyper-parameters for our validation during training,2020, We save the bestmodel using the early-stopping criteria (Rice et al
