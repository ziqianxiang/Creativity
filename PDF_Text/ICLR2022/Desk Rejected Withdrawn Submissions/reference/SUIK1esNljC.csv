title,year,conference
 Online learning rateadaptation with hypergradient descent,2018, In ICLR
 Online algorithms and stochastic approximations,1998, In Online Learning and NeuralNetworks
 ImageNet: A Large-Scale HierarchicalImage Database,2009, In CVPR
 Marthe: Scheduling the learningrate via online hypergradients,2020, In IJCAI
 Adaptive subgradient methods for online learning and stochasticoptimization,2011, Journal of Machine Learning Research
 Forward and reverse gradient-based hyperpa-rameter optimization,2017, In ICML
 Deep Learning,2016, MIT Press
 Sgd: General analysisand improved rates,2019, In ICML
 Deep residual learning for image recognition,2016, In CVPR
 Sequential model-based optimization for generalalgorithm configuration,2011, In LION
 Population based trainingof neural networks,2017, CoRR
 Autolrs: Automaticlearning-rate schedule by bayesian optimization on the fly,2020, In ICLR
 Large-scale videoclassification with convolutional neural networks,2014, In CVPR
 Adam: A method for stochastic optimization,2015, In ICLR (Poster)
 Imagenet classification with deep convolutional neuralnetworks,2012, In NIPS
 A stochastic gradient method with an exponential convergencerate for finite training sets,2012, In NeurIPS
 Ageneralized framework for population based training,2019, In ACM SIGKDD
 Hyperband: A novel bandit-based approach to hyperparameter optimization,2018, Journal of Machine Learning Research
 SGDR: stochastic gradient descent with warm restarts,2017, In ICLR
 Provably efficient online hyperparameter optimizationwith population-based bandits,2020, In NeurIPS
 Acceleration of stochastic approximation by averaging,1992, Siam Journalon Control and Optimization
 Some methods of speeding up the convergence of iteration methods,1964, ComputationalMathematics and Mathematical Physics
 On the stability and convergence of stochastic gradientdescent with momentum,2018, CoRR
 No more pesky learning rates,2013, In ICML
 Megatron-lm: Trainingmulti-billion parameter language models using model parallelism,2019, CoRR
 Cyclical learning rates for training neural networks,2017, In WACV
 Super-convergence: Very fast training of residual networks using largelearning rates,2017, CoRR
 Dropout: a simple wayto prevent neural networks from overfitting,2014, JMLR
 Lecture 6,2012,5-rmsprop: Divide the gradient by a running average of itsrecent magnitude
 Wngrad: Learn the learning rate in gradient descent,2018, CoRR
 Global convergence of adaptive gradient methods for an over-parameterized neural network,2019, CoRR
 Unified convergence analysis of stochastic momentum methods for convexand non-convex optimization,2016, CoRR
 Mini-batch algorithms with online step size,2019, Knowledge-BasedSystems
 Wide residual networks,2016, In BMVC
 Adadelta: an adaptive learning rate method,2012, CoRR
 Towards automated deep learning: Efficient joint neuralarchitecture and hyperparameter search,2018, In ICML AutoML Workshop
 Fast convergence of natural gradient descent for over-parameterized neural networks,2019, In NeurIPS
