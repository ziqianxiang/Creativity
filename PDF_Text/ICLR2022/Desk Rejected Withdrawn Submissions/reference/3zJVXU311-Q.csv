title,year,conference
 TabNet: Attentive interpretable tabular learning,2021, Proceedings of the AAAIConference on Artificial Intelligence
 Making a science of model search: Hyperparameteroptimization in hundreds of dimensions for vision architectures,2013, In International conference onmachine learning
 Random forests,2001, Machine Learning
 XGBoost: A scalable tree boosting system,2016, In Proceedings of the22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining
 Support-vector networks,1995, Machine learning
 Contrastive Mixup: self- andsemi-supervised learning for tabular domain,2021, ArXiv
 On a model of associative memorywith huge storage capacity,2017, Journal of Statistical Physics
 BERT: pre-training of deep bidirectional trans-formers for language understanding,2019, In Proceedings of the 2019 Conference of the North AmericanChapter of the Association for Computational Linguistics: Human Language Technologies
 CatBoost:unbiased boosting with categorical features,2017, ArXiv
 TabularNet: A neural networkarchitecture for understanding semantic structures of tabular data,2021, In Proceedings of the 27th ACMSIGKDD Conference on KnoWIedge Discovery & Data Mining
 AutoGluon-Tabular:Robust and accurate AutoML for structured data,2020, ArXiv
 Simple modifications to improve tabular neural networks,2021, ArXiv
 Greedy function approximation: A gradient boosting machine,2001, The Annals ofStatistics
 Revisiting deep learning models for tabulardata,2021, ArXiv
 TabGNN: Multiplex graph neural network fortabular data prediction,2021, ArXiv
 Random decision forests,1995, In Proceedings of 3rd International Conference on DocumentAnalysis and Recognition
 Neurons with graded response have collective computational properties like those oftwo-state neurons,1984, Proceedings of the National Academy of Sciences
 TabTransformer: Tabular data modeling usingcontextual embeddings,2020, ArXiv
 Regularization is all you need: Simple neuralnets can excel on tabular data,2021, ArXiv
 LightGBM: A highlyefficient gradient boosting decision tree,2017, In I
 Self-normalizing neural networks,2017, InAdvances in Neural Information Processing Systems
 Self-attention between datapoints:Going beyond individual input-output pairs in deep learning,2021, ArXiv
 Dense associative memory for pattern recognition,2016, In D
 Deep learning,2015, Nature
 NIST handbook of mathematicalfunctions,9780, Cambridge University Press
 Neural oblivious decision ensembles for deep learningon tabular data,2019, ArXiv
 CatBoost: unbiased boostingwith categorical features,2018, In S
 Hopfieldnetworks is all you need,2020, ArXiv
 Hopfieldnetworks is all you need,2021, In 9th International Conference on Learning Representations (ICLR)
 Deep learning in neural networks: An overview,2015, Neural Networks
 Regularization learning networks: Deep learning for tabular datasets,2018, InS
 Tabular Data: Deep learning is not all you need,2021, ArXiv
 SAINT: Improvedneural networks for tabular data via row attention and contrastive pre-training,2021, ArXiv
 AutoInt: Automatic feature inter-action learning via self-attentive neural networks,2019, In Proceedings of the 28th ACM InternationalConference on Information and Knowledge Management
 Attention is all you need,2017, In I
 Modern Hopfield networks and attention forimmune repertoire classification,2020, In Advances in Neural Information Processing Systems
 Large batch optimization for deep learning: Training bert in 76 minutes,1904, In InternationalConference on Learning Representations
