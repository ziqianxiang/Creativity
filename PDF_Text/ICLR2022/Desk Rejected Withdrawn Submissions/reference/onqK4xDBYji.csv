title,year,conference
 Binarybert: Pushing the limit of bert quantization,2020, arXiv preprint arXiv:2012
 Estimating or propagating gradientsthrough stochastic neurons for conditional computation,2013, arXiv preprint arXiv:1308
 Language models arefew-shot learners,2020, arXiv preprint arXiv:2005
 End-to-end object detection with transformers,2020, In Andrea Vedaldi
 Whatyou can cram into a single vector: Probing sentence embeddings for linguistic properties,2018, arXivpreprint arXiv:1805
 BERT: Pre-training of deepbidirectional transformers for language understanding,4171, In Proceedings of the 2019 Conference ofthe North American Chapter of the Association for Computational Linguistics: Human LanguageTechnologies
 Understanding the difficulty of training deep feedforward neuralnetworks,2010, In Proceedings of the thirteenth international conference on artificial intelligence andstatistics
 Compressing bert: Studying the effects ofweight pruning on transfer learning,2020, arXiv preprint arXiv:2002
 Knowledge distillation: Asurvey,2021, International Journal of Computer Vision
 Distilling the knowledge in a neural network,2015, arXivpreprint arXiv:1503
 Pruning and quantization for deep neuralnetwork acceleration: A survey,2021, arXiv preprint arXiv:2101
 Ladabert: Lightweight adaptation of bert through hybrid modelcompression,2020, arXiv preprint arXiv:2004
 Low-rank matrix completion: A contem-porary survey,2019, IEEE Access
 Deep contextualized word representations,2018, In Proceedings of the 2018Conference of the North American Chapter of the Association for Computational Linguistics: Hu-man Language Technologies
 Exploring the limits of transfer learning with a unified text-to-texttransformer,2019, arXiv preprint arXiv:1910
 Pruning algorithms-a survey,1993, IEEE transactions on Neural Networks
 Poor manâ€™s bert: Smaller and fastertransformer models,2020, arXiv e-prints
 Movement pruning: Adaptive sparsity byfine-tuning,2020, In H
 Q-bert: Hessian based ultra low precision quantization of bert,2020, In Proceedingsof the AAAI Conference on Artificial Intelligence
 Very deep convolutional networks for large-scale imagerecognition,2014, arXiv preprint arXiv:1409
 Sesamebert: Attention for anywhere,2020, In 2020 IEEE 7thInternational Conference on Data Science and Advanced Analytics (DSAA)
 Bert rediscovers the classical nlp pipeline,2019, arXivpreprint arXiv:1905
 Q8bert: Quantized 8bit bert,2019, arXivpreprint arXiv:1910
