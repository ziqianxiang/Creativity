title,year,conference
 Losing heads in the lottery: Pruning transformer attention in neuralmachine translation,2020, In Proceedings of the 2020 Conference on Empirical Methods in Natural LanguageProcessing (EMNLP)
 Autoformer: Searching transformers for visualrecognition,2021, arXiv preprint arXiv:2107
 Chasing sparsity in visiontransformers: An end-to-end exploration,2021, arXiv preprint arXiv:2106
 Conditionalpositional encodings for vision transformers,2021, arXiv preprint arXiv:2102
 Bert: Pre-training of deep bidirectionaltransformers for language understanding,2018, arXiv preprint arXiv:1810
 Global sparse momentumsgd for pruning very deep neural networks,2019, arXiv preprint arXiv:1909
 An image is worth 16x16 words:Transformers for image recognition at scale,2020, arXiv preprint arXiv:2010
 Levit: a vision transformer in convnetâ€™s clothing for faster inference,2021, arXiv preprint arXiv:2104
 Dynamic network surgery for efficient dnns,2016, In Advances In NeuralInformation Processing Systems
 Transformer in transformer,2021, arXivpreprint arXiv:2103
 Channel pruning for accelerating very deep neural networks,2017, InProceedings of the IEEE International Conference on Computer Vision
 Transgan: Two transformers can make one strong gan,2021, arXivpreprint arXiv:2102
 Tinybert:Distilling bert for natural language understanding,2019, arXiv preprint arXiv:1909
 Vilt: Vision-and-language transformer without convolution orregion supervision,2021, arXiv preprint arXiv:2102
 Learning multiple layers of features from tiny images,2009, Technical report
 Albert:A lite bert for self-supervised learning of language representations,2019, arXiv preprint arXiv:1909
 Swintransformer: Hierarchical vision transformer using shifted windows,2021, arXiv preprint arXiv:2103
 Rethinking the value of networkpruning,2018, arXiv preprint arXiv:1810
 Learning sparse neural networks through l_0 regular-ization,2017, arXiv preprint arXiv:1712
 Tprune: Efficient transformer pruning for mobiledevices,2021, ACM Transactions on Cyber-Physical Systems
 Accelerating sparse deep neural networks,2021, arXiv preprint arXiv:2104
 Importance estimation for neuralnetwork pruning,2019, In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition
 Improving language understanding bygenerative pre-training,2018, 2018
 ImageNet Large ScaleVisual Recognition Challenge,2015, International Journal of Computer Vision (IJCV)
 The right tool forthe job: Matching model and instance complexities,2020, arXiv preprint arXiv:2004
 Edgebert: Sentence-level energy optimizations forlatency-aware multi-task nlp inference,2020, arXiv preprint arXiv:2011
 The inaturalist species classification and detection dataset,2018, In Proceedings of the IEEEconference on computer vision and pattern recognition
 Pyramid vision transformer: A versatile backbone for dense prediction without convolutions,2021, arXivpreprint arXiv:2102
 A note on piecewise linear and multilinear table interpolation in manydimensions,1988, Mathematics ofComputation
	Pytorch image models,2019,	https://github
 Dreaming to distill: Data-free knowledge transfer via deepinversion,2020, In Proceedings of the IEEE/CVFConference on Computer Vision and Pattern Recognition
 Tokens-to-token vit: Training vision transformers from scratch on imagenet,2021, arXiv preprintarXiv:2101
 Rethinking semantic segmentation from a sequence-to-sequence perspectivewith transformers,2021, In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition
 Bert loses patience: Fast androbust inference with early exit,2020, arXiv preprint arXiv:2006
 Visual transformer pruning,2021, arXiv preprintarXiv:2104
