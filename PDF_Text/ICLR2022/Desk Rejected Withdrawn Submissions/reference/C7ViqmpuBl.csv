title,year,conference
 Quasi-monte carlo featuremaps for shift-invariant kernels,2016, Journal ofMachine Learning Research
 Language models are few-shot learners,2020, In H
 Not-so-random features,2018, In International Conference onLearning Representations
 Recycling randomness with structure for sublineartime kernel expansions,2016, In Proceedings of the 33rd International Conference on InternationalConference on Machine Learning - Volume 48
 The unreasonable effectiveness ofstructured random orthogonal embeddings,2017, In I
 Rethinking attention with per-formers,2021, In International Conference on Learning Representations
 Support-vector networks,885, Mach
 Gaussian quadrature for kernel features,2017, InI
 BERT: Pre-training of deepbidirectional transformers for language understanding,4171, In Proceedings of the 2019 Conference ofthe North American Chapter of the Association for Computational Linguistics: Human LanguageTechnologies
 Random feature mapping with signed circulantmatrix projection,9781, In Proceedings of the 24th International Conference on Artificial Intelligence
 Generative adversarial nets,2014, In Z
 Kernel methods in machine learn-ing,2008, TheAnnalsofStatistics
 A divide-and-conquer solver for kernel support vectormachines,2014, In Proceedings of the 31st International Conference on International Conference onMachine Learning - Volume 32
 Generative models for graph-based protein design,2019, In H
 Transformers areRNNs: Fast autoregressive transformers with linear attention,2020, In Hal DaUme In and Aarti Singh(eds
 Reformer: The efficient transformer,2020, InInternational Conference on Learning Representations
 Learning multiple layers of features from tiny images,2009, Technical report
 Fastfood - computing hilbert space expansionsin loglinear time,2013, In Sanjoy Dasgupta and David McAllester (eds
 Implicitkernel learning,2007, In Kamalika Chaudhuri and Masashi Sugiyama (eds
 Linearized gmm kernels and normalized random fourier features,2017, In Proceedings of the23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining
 Towards a unified analysis of ran-dom Fourier features,2019, In Kamalika Chaudhuri and Ruslan Salakhutdinov (eds
 Learning data-adaptive non-parametric kernels,2020, Journal of Machine Learning Research
 Generating wikipedia by summarizing long sequences,2018, In International Conference onLearning Representations
 Vilbert: Pretraining task-agnostic visiolinguis-tic representations for vision-and-language tasks,2019, In H
 Spherical structured feature maps for kernel approximation,2256, In Doina Precup andYee Whye Teh (eds
 Learning word vectors for sentiment analysis,2011, In Proceedings of the 49th Annual Meetingof the Association for Computational Linguistics: Human Language Technologies
 ListOps: A diagnostic dataset for latent tree learning,2018, InProceedings of the 2018 Conference of the North American Chapter of the Association for Com-putational Linguistics: Student Research Workshop
 Bayesian nonparametric kernel-learning,1078, In Arthur Gretton and Christian C
 Image transformer,2018, In Jennifer Dy and Andreas Krause (eds
 On the turing completeness of modern neu-ral network architectures,2019, CoRR
 On the turing completeness of modern neuralnetwork architectures,2019, In International Conference on Learning Representations
 Exploring the limits of transfer learning with a unified text-to-texttransformer,2020, Journal of Machine Learning Research
 Curran Associates Inc,9781, ISBN 9781605603520
 On gans and gmms,2018, In S
 Biological structure and function emerge fromscaling unsupervised learning to 250 million protein sequences,2020, bioRxiv
 Fourier Analysis on Groups,1990, John Wiley & Sons
 Nonlinear component analysisas a kernel eigenvalue problem,1998, Neural Computation
 Harmonizable mixture kernels with varia-tional fourier features,2019, In Kamalika Chaudhuri and Masashi Sugiyama (eds
 Learning kernels with random features,2016, In D
 Morgan Kaufmann Publishers Inc,1558, ISBN 1558607072
 But how does it work in theory? linear svm withrandom features,2018, In S
 On the error of random fourier features,9780, In Proceedingsof the Thirty-First Conference on Uncertainty in Artificial Intelligence
 Long range arena : A benchmark for efficienttransformers,2021, In International Conference on Learning Representations
 Transformer dissection: An Unified Understanding for transformer¡¯s attention viathe lens of kernel,2019, In Proceedings of the 2019 Conference on Empirical Methods in Natural Lan-guage Processing and the 9th International Joint Conference on Natural Language Processing(EMNLP-IJCNLP)
 Attention is all you need,2017, In I
 A la Carte - Learning Fast Kernels,2015, InGuy Lebanon and S
 Orthogonal random features,2016, In D
 Big bird: Trans-formers for longer sequences,2020, In H
