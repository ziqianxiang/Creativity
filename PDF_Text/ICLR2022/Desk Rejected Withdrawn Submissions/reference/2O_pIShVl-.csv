title,year,conference
 A statistical theory of cold posteriors in deep neural networks,2021, International Confer-ence on Learning Representations
 Learning long-term dependencies with gradient descent isdifficult,1994, IEEE transactions on neural networks
 The promises and pitfalls of stochastic gradient langevindynamics,2018, Advances in Neural Information Processing Systems
 On stochastic gradientLangevin dynamics with dependent data streams: the fully non-convex case,2019, arXiv preprintarXiv:1905
 On the convergence of a class of adam-type algorithms fornon-convex optimization,2019, International Conference on Learning Representations
 Theoretical guarantees for approximate sampling from smooth and log-concavedensities,2017, Journal of the Royal Statistical Society: Series B (Statistical Methodology)
 Non-convex learning via replica exchange stochastic gradientMCMC,2020, International Conference on Machine Learning
 A contour stochastic gradient Langevin dynamics algorithm forsimulations of multi-modal distributions,2020, Conference on Neural Information Processing Systems
 Adaptive subgradient methods for online learning and stochasticoptimization,2011, Journal of Machine Learning Research
 Nonasymptotic convergence analysis for the unadjusted Langevinalgorithm,2017, The Annals of Applied Probability
 Couplings and quantitative contraction rates for Langevindynamics,2019, Annals of Probability
 Global non-convex optimization with discretized diffu-sions,2018, Conference on Neural Information Processing Systems
 Adamp: slowing downthe slodown for momentum optimizers on scale-invariant weights,2021, International Conference onLearning Representations
 Densely connected convolutional networks,2017, IEEEconference on computer vision and pattern recognition
 Laplaceâ€™s method revisited: weak convergence of probability measures,1980, The Annals ofProbability
 Batch normalization: Accelerating deep network training by reducinginternal covariate shift,2015, International Conference on Machine Learning
 Deep residual learning for image recognition,2016, IEEE conference on computervision and pattern recognition
 ADAM: A method for stochastic optimization,2015, International Conference onLearning Representations
 Learning multiple layers of features from tiny images,2009, 2009
 Extremal properties of the solutions of stochastic equations,1985, Theory of Probabilityand its Applications
 A simple proof of the existence of a solution to the ItO's equation with monotonecoefficients,1990, Theory of Probability and its Applications
 On the variance of the adaptive learningrate and beyond,2020, International Conference on Learning Representations
 Decoupled weight decay regularization,2019, International Conference onLearning Representations
 Taming neural networks with tusla: Non-convexlearning via adaptive stochastic gradient langevin algorithms,2020, arXiv preprint arXiv:2006
 Adaptive gradient methods with dynamic bound of learningrate,2019, International Conference on Learning Representations
 Regularizing and optimizing lstm language models,2018, Inter-national Conference on Learning Representations
 Acceleration of stochastic approximation by averaging,1992, SIAM journaloon control and optimization
 Non-convex learning via stochastic gradient Langevindynamics: a nonasymptotic analysis,2017, Conference on Learning Theory
 On the convergence of ADAM and beyond,2018, International Confer-ence on Learning Representations
 Exponential convergence of Langevin distributions and their discreteapproximations,1996, Bernoulli
 Lecture 6,2012,5-rmsprop: Divide the gradient by a running average of itsrecent magnitude
 Bayesian learning via stochastic gradient Langevin dynamics,2011, In Pro-Ceedings of the 28th International Conference on Machine Learning
 The marginal value of adaptive gradientmethods in machine learning,2017, Advances in Neural Information Processing Systems
 Global convergence of Langevin dynamics based algorithms fornonconvex optimization,2018, Conference on Neural Information Processing Systems
 Adaptive methods for nonconvex optimiza-tion,2018, Advances in Neural Information Processing Systems
 Cyclical stochastic gradient MCMC for bayesiandeep learning,2020, International Conference on Learning Representations
 Nonasymptotic estimates for StochasticGradient Langevin Dynamics under local conditions in nonconvex optimization,2019, arXiv preprintarXiv:1910
 Adabeliefoptimizer: adapting stepsizes by the belief in observed gradients,2020, Advances in Neural InformationProcessing Systems
