title,year,conference
 Bayesian reasoning and machine learning,2012, Cambridge University Press
 Deep Frank-Wolfe for neuralnetwork optimization,2018, arXiv preprint arXiv:1811
 Distributions of angles in random packing onspheres,2013, The Journal of Machine Learning Research
 Entropy-SGD: Biasinggradient descent into wide valleys,2016, arXiv preprint arXiv:1611
 Elements of information theory,1999, John Wiley & Sons
 Sharp minima cangeneralize for deep nets,2017, In Proceedings of the 34th International Conference on MachineLearning-Volume 70
 Pac-bayesiantheory meets bayesian inference,2016, arXiv preprint arXiv:1605
 An investigation into neural netoptimization via Hessian eigenvalue density,2019, arXiv preprint arXiv:1901
 Vbald-variational bayesian approximation of log determinants,2018, arXiv preprint arXiv:1802
 MLRGdeep curvature,2019, arXiv preprint arXiv:1912
 Large-scale log-determinant computationthrough stochastic Chebyshev expansions,2015, In International Conference on MachineLearning
 Flat minima,1997, Neural Computation
 A stochastic estimator of the trace of the influence matrix forLaplacian smoothing splines,1990, Communications in Statistics-Simulation and Computation
 Averaging weights leads to wider optima and better generalization,2018, arXiv preprintarXiv:1803
 Three factors influencing minima in sgd,2017, arXiv preprintarXiv:1711
 Three factors influencing minima in SGD,2017, arXiv preprintarXiv:1711
 The break-even point on the optimization tra jec-tories of deep neural netWorks,2020, In International Conference on Learning Representations
 Information theory and statistical mechanics,1957, Phys
 On large-batch training for deep learning: Generalization gap and sharpminima,2016, arXiv preprint arXiv:1609
 Adam: A method for stochastic optimization,2014, arXivpreprint arXiv:1412
 A simple Weight decay can improve generalization,1992, InAdvances in neural information processing systems
 Decoupled Weight decay regularization,2018, 2018
 The Lanczos and conjugate gradient algorithms infinite precision arithmetic,2006, Acta Numerica
 PieceWise strong convexity of neural netWorks,2019, In Advances in NeuralInformation Processing Systems
 The full spectrum of deepnet hessians at scale: Dynamics With sgd trainingand sample size,2018, arXiv preprint arXiv:1811
 Fast exact multiplication by the Hessian,1994, Neural computation
 Geometry of neural network loss surfaces viarandom matrix theory,2017, In Proceedings of the 34th International Conference on MachineLearning-Volume 70
 A scale invariant flatness measure for deep network minima,2019, arXiv preprintarXiv:1902
 Improved bounds on sample size for implicitmatrix trace estimators,2015, Foundations of Computational Mathematics
 Themarginal value of adaptive gradient methods in machine learning,2017, In Advances in NeuralInformation Processing Systems
 Towards understanding generalization of deep learning:Perspective of loss landscapes,2017, arXiv preprint arXiv:1706
 How sgd selects the global minima in over-parameterizedlearning: A dynamical stability perspective,2018, In Advances in Neural Information ProcessingSystems
 Hessian-basedanalysis of large batch training and robustness to adversaries,2018, In Advances in NeuralInformation Processing Systems
 Wide residual networks,2016, arXiv preprintarXiv:1605
 Under-standing deep learning requires rethinking generalization,2016, arXiv preprint arXiv:1611
 mixup: Beyondempirical risk minimization,2017, arXiv preprint arXiv:1710
