title,year,conference
 AuXiliary image regularization fordeep cnns with noisy labels,2015, arXiv preprint arXiv:1511
 Deep k-nn for noisy labels,2020, In International Conferenceon Machine Learning
 Spectrally-normalized margin bounds for neuralnetworks,2017, arXiv preprint arXiv:1706
 Rademacher and gaussian compleXities: Risk bounds andstructural results,2002, Journal of Machine Learning Research
 Confidence scoresmake instance-dependent label-noise learning possible,2020, arXiv preprint arXiv:2001
 Concentration inequalities: A nonaSymptotictheory of independence,2013, OXford university press
 Heteroskedas-tic and imbalanced deep learning with adaptive regularization,2020, arXiv preprint arXiv:2006
 Active bias: Training moreaccurate neural networks by emphasizing high variance samples,2017, arXiv preprint arXiv:1704
 Learning from untrusted data,2017, In Proceedingsof the 49th Annual ACM SIGACT Symposium on Theory of Computing
 Smote: syntheticminority over-sampling technique,2002, Journal of artificial intelligence research
 Understanding and utilizingdeep neural networks trained with noisy labels,2019, In International Conference on Machine Learning
 Robustness ofaccuracy metric and its inspirations in learning with noisy labels,2020, arXiv preprint arXiv:2012
 Imagenet: A large-scalehierarchical image database,2009, In 2009 IEEE conference on computer vision and pattern recognition
 How do neural networks overcome label noise,2018, arXivPreprint
 Model-agnostic meta-learning for fast adaptation ofdeep networks,2017, In International Conference on Machine Learning
 Robust loss functions under label noise for deepneural networks,2017, In Proceedings of the AAAI Conference on Artificial Intelligence
 Rich feature hierarchies for accurateobject detection and semantic segmentation,2014, In Proceedings of the IEEE conference on computervision and pattern recognition
 Training deep neural-networks using a noise adaptationlayer,2016,2016
 Size-independent sample complexity ofneural networks,2018, In Conference On Learning Theory
 Deep Learning,2016, MIT Press
 Recasting gradient-based meta-learning as hierarchical bayes,2018, arXiv preprint arXiv:1801
 Who said what: Modelingindividual labelers improves classification,2018, In Proceedings of the AAAI Conference on ArtificialIntelligence
 On calibration of modern neuralnetworks,2017, In International Conference on Machine Learning
 Sigua:Forgetting may make learning with noisy labels more robust,2020, In International Conference onMachine Learning
 Deep self-learning from noisy labels,2019, In Proceedingsof the IEEE/CVF International Conference on Computer Vision
 Training binary neuralnetworks through learning with noisy supervision,2020, In International Conference on MachineLearning
 Improving generalization bycontrolling label-noise information in neural network weights,2020, In International Conference onMachine Learning
 Deep residual learning for imagerecognition,2016, In Proceedings of the IEEE conference on computer vision and pattern recognition
 Using trusted data to traindeep networks on labels corrupted by severe noise,2018, arXiv preprint arXiv:1802
 Using pre-training can improve model robustnessand uncertainty,2019, In International Conference on Machine Learning
 Using self-supervised learningcan improve model robustness and uncertainty,2019, arXiv preprint arXiv:1906
 Simple and effective regularization methods for training onnoisily labeled data with generalization guarantee,2019, arXiv preprint arXiv:1905
 Self-adaptive training: beyond empirical riskminimization,2020, Advances in Neural Information Processing Systems
 Mentornet: Learning data-driven curriculum for very deep neural networks on corrupted labels,2018, In International Conferenceon Machine Learning
 Learning deep networks from noisy labels withdropout regularization,2016, In 2016 IEEE 16th International Conference on Data Mining (ICDM)
 Adam: A method for stochastic optimization,2014, arXiv preprintarXiv:1412
 Learning multiple layers of features from tiny images,2009,2009
 Cleannet: Transfer learning for scalableimage classifier training with label noise,2018, In Proceedings of the IEEE Conference on ComputerVision and Pattern Recognition
 Gradient-based meta-learning with learned layerwise metric andsubspace,2018, In International Conference on Machine Learning
 Mixture-rank matrixapproximation for collaborative filtering,2017, In Proceedings of the 31st International Conference onNeural Information Processing Systems
 Learning to learn from noisy labeleddata,2019, In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition
 Dividemix: Learning with noisy labels as semi-supervised learning,2020, arXiv preprint arXiv:2002
 Gradient descent with early stopping isprovably robust to label noise for overparameterized neural networks,2020, In International Conferenceon Artificial Intelligence and Statistics
 From label smoothing to label relaxation,2021, In Proceedings ofthe35th AAAI Conference on Artificial Intelligence
 Focal loss for dense objectdetection,2017, In Proceedings ofthe IEEE international conference on computer vision
 Early-learningregularization prevents memorization of noisy labels,2020, arXiv preprint arXiv:2007
 Classification with noisy labels by importance reweighting,2015, IEEETransactions on pattern analysis and machine intelligence
 Peer loss functions: Learning from noisy labels without knowing noiserates,2020, In International Conference on Machine Learning
 Dimensionality-driven learning with noisy labels,2018, In InternationalConference on Machine Learning
 Normal-ized loss functions for deep learning with noisy labels,2020, In International Conference on MachineLearning
 Learning from binary labelswith instance-dependent corruption,2016, arXiv preprint arXiv:1605
 Coresets for robust training of deep neuralnetworks against noisy labels,2020, Advances in Neural Information Processing Systems
 Learning to label aerial images from noisy data,2012, InProceedings of the 29th International conference on machine learning (ICML-12)
 Foundations of machine learning,2018, MITpress
 The distribution of rademacher sums,1990, Proceedings of the AmericanMathematical Society
 Learning with noisylabels,2013, In NIPS
 A pac-bayesian approach to spectrally-normalized margin bounds for neural networks,2017, arXiv preprint arXiv:1707
 Augmentation strategies for learning withnoisy labels,2021, arXiv preprint arXiv:2103
 Multi-objectiveinterpolation training for robustness to label noise,2020, arXiv preprint arXiv:2012
 Makingdeep neural networks robust to label noise: A loss correction approach,2017, In Proceedings of the IEEEConference on Computer Vision and Pattern Recognition
 Identifying mislabeled datausing the area under the margin ranking,2020, arXiv preprint arXiv:2001
 Training deep neural networks on noisy labels with bootstrapping,2014, arXiv preprintarXiv:1412
 Learning to reweight examples forrobust deep learning,2018, In International Conference on Machine Learning
 Deep learning from crowds,2018, In Proceedings of the AAAIConference on Artificial Intelligence
 Calibrated asymmetric surrogate losses,2012, Electronic Journal of Statistics
 Training region-based object detectorswith online hard example mining,2016, In Proceedings of the IEEE conference on computer vision andpattern recognition
 Meta-weight-net: Learning an explicit mapping for sample weighting,2019, arXiv preprint arXiv:1902
 Prototypical networks for few-shot learning,2017, arXivpreprint arXiv:1703
 Selfie: Refurbishing unclean samples for robust deeplearning,2019, In International Conference on Machine Learning
 Trainingconvolutional networks with noisy labels,2014, arXiv preprint arXiv:1406
 Deepface: Closing the gap tohuman-level performance in face verification,2014, In Proceedings of the IEEE conference on computervision and pattern recognition
 Joint optimization frameworkfor learning with noisy labels,2018, In Proceedings of the IEEE Conference on Computer Vision andPattern Recognition
 Learning from noisy labels by regularized estimation of annotator confusion,2019, In Proceedingsof the IEEE/CVF Conference on Computer Vision and Pattern Recognition
 Robustness of condi-tional gans to noisy labels,2018, arXiv preprint arXiv:1811
 Learning with symmetriclabel noise: The importance of being unhinged,2015, arXiv preprint arXiv:1505
 The nature of statistical learning theory,2013, Springer science & business media
 An overview of statistical learning theory,1999, IEEE transactions on neural networks
 Learningfrom noisy large-scale datasets with minimal supervision,2017, In Proceedings of the IEEE conferenceon computer vision and pattern recognition
 Matchingnetworks for one shot learning,2016, arXiv preprint arXiv:1606
 Symmetric crossentropy for robust learning with noisy labels,2019, In Proceedings of the IEEE/CVF InternationalConference on Computer Vision
 Robust probabilistic modeling with bayesian datareweighting,2017, In International Conference on Machine Learning
 Training noise-robust deep neural networks viameta-learning,2020, In Proceedings of the IEEE/CVF Conference on Computer Vision and PatternRecognition
 Less is better:Unweighted data subsampling via influence function,2020, In Proceedings of the AAAI Conference onArtificial Intelligence
 A topologicalfilter for learning with label noise,2020, arXiv preprint arXiv:2012
 Learning to purify noisy labels via metasoft label corrector,2020, arXiv preprint arXiv:2008
 Part-dependent label noise: Towards instance-dependentlabel noise,2020, Advances in Neural Information Processing Systems
 Learning from massive noisylabeled data for image classification,2015, In Proceedings of the IEEE conference on computer visionand pattern recognition
 On the consistency of top-k surrogate losses,2020, In InternationalConference on Machine Learning
 Safeguarded dynamic label regressionfor noisy supervision,2019, In Proceedings of the AAAI Conference on Artificial Intelligence
 Learning with biased complementarylabels,2018, In Proceedings of the European conference on computer vision (ECCV)
 Understandingdeep learning requires rethinking generalization,2016, arXiv preprint arXiv:1611
 mixup: Beyond empiricalrisk minimization,2017, arXiv preprint arXiv:1710
 Self-paced robust learningfor leveraging clean labels in noisy data,2020, In Proceedings of the AAAI Conference on ArtificialIntelligence
 Generalized cross entropy loss for training deep neural networkswith noisy labels,2018, arXiv preprint arXiv:1805
 Meta label correction for noisy labellearning,2021, In Proceedings of the 35th AAAI Conference on Artificial Intelligence
 Error-bounded correction of noisy labels,2020, In International Conference on Machine Learning
