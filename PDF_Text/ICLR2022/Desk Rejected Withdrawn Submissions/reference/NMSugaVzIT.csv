title,year,conference
 For valid generalization the size of the weights is more important than the size of the network,1996, InAdvances in Neural Information Processing Systems
 Spectrally-normalized margin bounds for neural networks,2017, InAdvances in Neural Information Processing Systems
 Can implicit bias explain generalization? stochastic convexoptimization as a case study,2020, In Advances in Neural Information Processing Systems 33: Annual Conference onNeural Information Processing Systems 2020
 Convex duality of deep neural networks,2020, CoRR
 Implicit convex regularizers of cnn architectures: Convex optimization of two- andthree-layer networks in polynomial time,2020, In Proceedings of the 37th International Conference on Machine Learning
 Characterizing implicit bias in terms of optimizationgeometry,2018, In Proceedings of the International Conference on Machine Learning (ICML)
 Implicit bias of gradient descent on linear convolutionalnetworks,2018, In Advances in Neural Information Processing Systems
 Risk and parameter convergence of logistic regression,2018, CoRR
 Gradient descent aligns the layers of deep linear networks,2019, In International Conferenceon Learning Representations (ICLR)
 Directional convergence and alignment in deep learning,2020, In Advances in NeuralInformation Processing Systems
 Learning multiple layers of features from tiny images,2009, Technical report
 A simple weight decay can improve generalization,1991, In Advances in Neural InformationProcessing Systems
 MNIST handwritten digit database,2010, 2010
 Towards resolving the implicit bias of gradient descent for matrixfactorization: Greedy low-rank learning,2021, In 9th International Conference on Learning Representations
 Gradient descent maximizes the margin of homogeneous neural networks,2020, In InternationalConference on Learning Representations (ICLR)
 Lexicographic anddepth-sensitive margins in homogeneous and non-homogeneous deep models,2019, In Proceedings of the InternationalConference on Machine Learning (ICML)
 In search of the real inductive bias: On the role of implicitregularization in deep learning,2015, In International Conference on Learning Representations (ICLR)
 A function space view of bounded norm infinite widthrelu nets: The multivariate case,2020, In International Conference on Learning Representations (ICLR)
 Neural networks are convex regularizers: Exact polynomial-time convex optimizationformulations for two-layer networks,2020, In Proceedings of the 37th International Conference on Machine Learning
 Implicit regularization in deep learning may not be explainable by norms,2020, In Advancesin Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020
 Fast maximum margin matrix factorization for collaborative prediction,2005, InProceedings ofthe International Conference on Machine Learning (ICML)
 Vector-output relu neural network problems are copositiveprograms: Convex analysis of two layer networks and polynomial-time algorithms,2020, CoRR
 Regularization matters: Generalization and optimization of neuralnets v,2019,s
 A unifying view on implicit bias in training linear neuralnetworks,2021, In International Conference on Learning Representations (ICLR)
 Understanding deep learning requiresrethinking generalization,2017, In International Conference on Learning Representations (ICLR)
 Identity crisis: Memorizationand generalization under extreme overparameterization,2020, In International Conference on Learning Representations(ICLR)
