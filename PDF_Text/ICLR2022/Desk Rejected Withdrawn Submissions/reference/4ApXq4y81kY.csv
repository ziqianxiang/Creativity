title,year,conference
 Learning from noisy examples,1988, Machine Learning
 A closer look atmemorization in deep netWorks,2017, In ICML
 Combining labeled and unlabeled data With co-training,1998, In ACCLT
 Active bias: Training moreaccurate neural netWorks by emphasizing high variance samples,2017, In NeurIPS
 Understanding and utilizingdeep neural netWorks trained With noisy labels,2019, In ICML
 Noise against noise: stochasticlabel noise helps combat inherent label noise,2021, In ICLR
 Rethinking importance Weighting fordeep learning under distribution shift,2020, In NeurIPS
 Active sampler: Light-Weight accelerator for complexdata analytics at scale,2015, arXiv preprint arXiv:1512
 Deep learning,2016, MIT press
 Co-teaching: Robust training of deep neural netWorks With extremely noisy labels,2018, InNeurIPS
 Sigua:Forgetting may make learning With noisy labels more robust,2020, In ICML
 Using trusted data to traindeep netWorks on labels corrupted by severe noise,2018, In NeurIPS
 Simple and effective regularization methods for training onnoisily labeled data With generalization guarantee,2020, In ICLR
 O2u-net: A simple noisy label detectionapproach for deep neural netWorks,2019, In ICCV
 MentorNet: Learning data-driven curriculum for very deep neural netWorks on corrupted labels,2018, In ICML
 Learning multiple layers of features from tiny images,2009, Technical report
 The MNIST database of handwrittendigits,1998, http://yann
 Robust inference viagenerative classifiers for handling noisy labels,2019, In ICML
 Dividemix: Learning with noisy labels as semi-supervised learning,2020, In ICLR
 Learning fromnoisy labels with distillation,2017, In ICCV
 Early-learningregularization prevents memorization of noisy labels,2020, In NeurIPS
 Classification with noisy labels by importance reweighting,2016, IEEETransactions on pattern analysis and machine intelligence
 Noise resistant graph ranking for improvedweb image search,2011, In CVPR
 Curriculum loss: Robust learning and generalization against labelcorruption,2020, In ICLR
 Nor-malized loss functions for deep learning with noisy labels,2020, In ICML
" Decoupling"" when to update"" from"" how to update""",2017, InNeurIPS
 Coresets for robust training of neuralnetworks against noisy labels,2020, In NeurIPS
 Readingdigits in natural images with unsupervised feature learning,2011, In NIPS Workshop on Deep Learningand Unsupervised Feature Learning
 Self: Learning to filter noisy labels with self-ensembling,2020, InICLR
 Pervasive label errors in test sets destabilizemachine learning benchmarks,2021, arXiv preprint arXiv:2103
 Makingdeep neural networks robust to label noise: A loss correction approach,2017, In CVPR
 Glove: Global vectors for wordrepresentation,2014, In EMNLP
 A study of gaussian mixture models of color andtexture features for image classification and segmentation,2006, Pattern recognition
 Identifying mislabeled datausing the area under the margin ranking,2020, In NeurIPS
 Learning to reweight examples forrobust deep learning,2018, In ICML
 Meta-weight-net: Learning an explicit mapping for sample weighting,2019, In NeurIPS
 Meta transition adaptation for robust deeplearning with noisy labels,2020, arXiv preprint arXiv:2006
 Learning from noisylabels with deep neural networks: A survey,2020, arXiv preprint arXiv:2007
 Joint optimization frameworkfor learning with noisy labels,2018, In CVPR
 Combating noisy labels by agreement: A jointtraining method with co-regularization,2020, In CVPR
 When optimizing f -divergence is robust with label noise,2021, In ICLR
 Online crowdsourcing: rating annotators and obtaining cost-effective labels,2010, In CVPR-Workshop
 Fashion-mnist: a novel image dataset for benchmarkingmachine learning algorithms,2017, arXiv preprint arXiv:1708
 Learning from massive noisylabeled data for image classification,2015, In CVPR
 LDMI : A novel information-theoretic lossfunction for training deep nets robust to label noise,2019, In NeurIPS
 Rethinking the value of labels for improving class-imbalanced learning,2020, InNeurIPS
 Searching to exploitmemorization effect in learning with noisy labels,2020, In ICML
 Understandingdeep learning requires rethinking generalization,2017, In ICLR
 mixup: Beyond empiricalrisk minimization,2018, In ICLR
 Learning withfeature-dependent label noise: A progressive approach,2021, In ICLR
 Learning noise transition matrix from only noisylabels via total variation regularization,2021, In ICML
 Generalized cross entropy loss for training deep neural networkswith noisy labels,2018, In NeurIPS
 Error-bounded correction of noisy labels,2020, In ICML
 Robust curriculum learning: from clean label detectionto noisy label self-correction,2021, In ICLR
 A second-order approach to learning with instance-dependent label noise,2021, In CVPR
