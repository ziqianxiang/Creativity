title,year,conference
 On the model-basedstochastic value gradient for continuous reinforcement learning,2020, CoRR
 Openai gym,2016, arXiv preprint arXiv:1606
 Exploration by random networkdistillation,2019, In International Conference on Learning Representations
	Deep rein-forcement learning in a handful of trials using probabilistic dynamics models,2018, InS
 Two-timescale networks for nonlinearvalue function approximation,2019, In International Conference on Learning Representations
 Model-based value estimation for efficient model-free reinforcement learning,2018, ArXiv
 Statistical methods for research workers,1925, Edinburgh Oliver & Boyd
 Addressing function approximation error in actor-critic methods,2018, In International Conference on Machine Learning
 Soft actor-critic: Off-policymaximum entropy deep reinforcement learning with a stochastic actor,2018, In Jennifer G
 Double q-learning,2010, In J
 Learn-ing continuous control policies by stochastic value gradients,2015, CoRR
 When to trust your model: Model-based policy optimization,2019, In Advances in Neural Information Processing Systems
 Deepmellow: Removingthe need for a target network in deep q-learning,2019, In Proceedings of the Twenty-Eighth Interna-tional Joint Conference on Artificial Intelligence
 Model-ensembletrust-region policy optimization,2018, In International Conference on Learning Representations
 Sunrise: A simple unified frame-work for ensemble learning in deep reinforcement learning,2021, In International Conference on Ma-chine Learning
 Designing experiments and analyzing data: amodel comparison perspective,2017, 2017
 Human-level control through deep rein-forcement learning,0028, Nature
 Mbrl-lib: Amodular library for model-based reinforcement learning,2021, Arxiv
 Model-based policy optimization with unsu-pervised model adaptation,2020, In H
 Mujoco: A physics engine for model-basedcontrol,2012, In IROS
 Exploring model-based planning with policy networks,2020, In Interna-tional Conference on Learning Representations
 Learning from Delayed Rewards,1989, PhD thesis
