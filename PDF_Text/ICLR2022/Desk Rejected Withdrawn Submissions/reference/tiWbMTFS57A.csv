title,year,conference
 On the convergence rate of training recurrent neuralnetworks,2018, arXiv preprint arXiv:1810
 A convergence theory for deep learning via over-parameterization,2019, In International Conference on Machine Learning
 Benign overfitting in linearregression,2020, Proceedings of the National Academy of Sciences
 To understand deep learning we need to understandkernel learning,2018, In International Conference on Machine Learning
 Global convergence of arbitrary-block gradient methods forgeneralized polyak-lcjasiewicz functions,2017, arXivPrePrintarXiv:1709
 Toward deeper understanding of neural networks: Thepower of initialization and a dual view on expressivity,2016, arXiv PrePrint arXiv:1602
 Gradient descent finds globalminima of deep neural networks,1675, In International Conference on Machine Learning
 Gradient descent provably optimizesover-parameterized neural networks,2018, arXiv PrePrint arXiv:1810
 Stability ofrandomized learning algorithms,2005, Journal of Machine Learning Research
 Proxy convexity: A unified framework for the analysis of neuralnetworks trained by gradient descent,2021, arXiv PrePrint arXiv:2106
 Generative adversarial nets,2014, Advances in neural informationProcessing systems
 A study of con-dition numbers for first-order optimization,2021, In International Conference on Artificial Intelligenceand Statistics
 Delving deep into rectifiers: Surpassinghuman-level performance on imagenet classification,2015, In Proceedings of the IEEE internationalconference on comPuter vision
 Linear convergence of gradient and proximal-gradient methods under the Polyak-IojasieWicz condition,2016, In Joint European Conference onMachine Learning and Knowledge Discovery in Databases
 Learning multiPle layers of features from tiny images,2009, 2009
 Wide neural netWorks of any dePth evolve as linear modelsunder gradient descent,2019, arXiv preprint arXiv:1902
 Finite versus infinite neural netWorks: an emPirical study,2020, arXivpreprint arXiv:2007
 On the benefit of Width for neural netWorks: DisaPPearance ofbad basins,2018, arXiv
 ToWard a theory of oPtimization for over-Parameterizedsystems of non-linear equations: the lessons of deeP learning,2020, arXiv preprint arXiv:2003
 Perturbations of moore-Penrosemetric generalized inverses of linear oPerators in banach sPaces,2014, Acta Mathematica Sinica
 On the method of bounded differences,1989, Surveys in combinatorics
 ToWard moderate overParameterization: Global con-vergence guarantees for training shalloW neural netWorks,2020, IEEE Journal on Selected Areas inInformation Theory
 Gradient methods for minimizing functionals,1963, Zhurnal Vychislitelâ€™noiMatematiki i Matematicheskoi Fiziki
 DeeP informationProPagation,2017, 2017
 Benign overfitting in ridge regression,2020, arXiv preprintarXiv:2009
 Global convergence and variance-reduced oPtimizationfor a class of nonconvex-nonconcave minimax Problems,2020, arXiv preprint arXiv:2002
 Rethinking bias-variancetrade-off for generalization of neural netWorks,2020, In International Conference on Machine Learning
 Gradient descent oPtimizes over-Parameterized deeP relu netWorks,2020, Machine Learning
