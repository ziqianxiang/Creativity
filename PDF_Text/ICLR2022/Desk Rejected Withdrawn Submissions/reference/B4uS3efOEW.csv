title,year,conference
 Unsupervisedlabel noise modeling and loss correction,2019, In Proceedings of the 36th International Conference onMachine Learning
 A closer look atmemorization in deep netWorks,2017, In Proceedings of the 34th International Conference on MachineLearning-Volume 70
 MiXmatch: A holistic approach to semi-supervised learning,2019, In Advances in NeuralInformation Processing Systems
 Understanding and utilizingdeep neural netWorks trained With noisy labels,2019, In International Conference on Machine Learning
 Imagenet: A large-scalehierarchical image database,2009, In 2009 IEEE conference on computer vision and pattern recognition
 Can cross entropy loss be robustto label noise,2020, In Proceedings of the 29th International Joint Conferences on Artificial Intelligence
 Robust loss functions under label noise for deepneural netWorks,2017, In Thirty-First AAAI Conference on Artificial Intelligence
 Training deep neural-netWorks using a noise adaptationlayer,2016, 2016
 Co-teaching: Robust training of deep neural netWorks With eXtremely noisy labels,2018, InAdvances in neural information processing systems
 Deep residual learning for imagerecognition,2016, In Proceedings of the IEEE conference on computer vision and pattern recognition
 Using trusted data to traindeep netWorks on labels corrupted by severe noise,2018, In NeurIPS
 Simple and effective regularization methods for training onnoisily labeled data With generalization guarantee,2019, In International Conference on LearningRepresentations
 Self-adaptive training: beyond empirical riskminimization,2020, Advances in Neural Information Processing Systems
 Neural tangent kernel: convergence andgeneralization in neural netWorks,2018, In Proceedings of the 32nd International Conference on NeuralInformation Processing Systems
 Mentornet: Learning data-driven curriculum for very deep neural netWorks on corrupted labels,2018, In International Conferenceon Machine Learning
 Learning multiple layers of features from tiny images,2009, 2009
 Temporal ensembling for semi-supervised learning,2017, ICLR
 Dividemix: Learning with noisy labels as semi-supervised learning,2020, International Conference on Learning Representation
 Gradient descent with early stopping isprovably robust to label noise for overparameterized neural networks,2020, In International Conferenceon Artificial Intelligence and Statistics
 Webvision database: Visuallearning and understanding from web data,2017, CoRR
 Learning fromnoisy labels with distillation,2017, In Proceedings of the IEEE International Conference on ComputerVision
 Early-learningregularization prevents memorization of noisy labels,2020, Advances in Neural Information ProcessingSystems
 Sgdr: Stochastic gradient descent with warm restarts,2017, ICLR
 Co-matching: Combating noisy labels by augmentationanchoring,2021, arXiv preprint arXiv:2103
 Dimensionality-driven learning with noisy labels,2018, arXiv preprintarXiv:1806
 Normal-ized loss functions for deep learning with noisy labels,2020, In International Conference on MachineLearning
 Self: learning to filter noisy labelswith self-ensembling,2020, In International Conference on Learning Representations (ICLR)
 Makingdeep neural networks robust to label noise: A loss correction approach,2017, In Proceedings of theIEEE Conference on Computer Vision and Pattern Recognition
 Identifying mislabeled datausing the area under the margin ranking,2020, arXiv preprint arXiv:2001
 Training deep neural networks on noisy labels with bootstrapping,2015, In ICLR (Workshop)
 Rethinkingthe inception architecture for computer vision,2016, In Proceedings of the IEEE conference on computervision and pattern recognition
 Joint optimization frameworkfor learning with noisy labels,2018, In Proceedings of the IEEE Conference on Computer Vision andPattern Recognition
 Learning from noisy labels by regularized estimation of annotator confusion,2019, In Proceedingsof the IEEE/CVF Conference on Computer Vision and Pattern Recognition
 Mean teachers are better role models: Weight-averaged consistencytargets improve semi-supervised deep learning results,2017, In Proceedings of the 31st InternationalConference on Neural Information Processing Systems
 Combating label noise in deep learning using abstention,2019, In International Conference onMachine Learning
 Symmetric crossentropy for robust learning with noisy labels,2019, In Proceedings of the IEEE/CVF InternationalConference on Computer Vision
 Combating noisy labels by agreement: A jointtraining method with co-regularization,2020, In Proceedings of the IEEE/CVF Conference on ComputerVision and Pattern Recognition
 Parts-dependent label noise: Towards instance-dependentlabel noise,2020, arXiv preprint arXiv:2006
 Learning from massive noisylabeled data for image classification,2015, In Proceedings of the IEEE conference on computer visionand pattern recognition
 L_dmi: A novel information-theoretic lossfunction for training deep nets robust to label noise,2019, In NeurIPS
 Learning with biased complementarylabels,2018, In Proceedings of the European Conference on Computer Vision (ECCV)
 mixUp: Beyond empiricalrisk minimization,2018, In International Conference on Learning Representations
