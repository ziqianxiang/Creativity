title,year,conference
 T6d-direct: Transformers for multi-object6d pose direct regression,2021, arXiv preprint arXiv:2109
 Beit: Bert pre-training of image transformers,2021, arXiv preprintarXiv:2106
 End-to-end object detection with transformers,2020, In European Conferenceon Computer Vision
 Emerging properties in self-supervised vision transformers,2021, arXiv preprintarXiv:2104
 Psvit: Better vision transformer via token pooling and attention sharing,2021, arXiv preprintarXiv:2108
 Crossvit: Cross-attention multi-scale visiontransformer for image classification,2021, arXiv preprint arXiv:2103
 Pre-trained image processing transformer,2021, In Proceedings of theIEEE/CVF Conference on Computer Vision and Pattern Recognition
 Autoformer: Searching transformersfor visual recognition,2021, arXiv preprint arXiv:2107
 Exploring and improvingmobile level vision transformers,2021, arXiv preprint arXiv:2108
 Chasing sparsityin vision transformers: An end-to-end exploration,2021, arXiv preprint arXiv:2106
 Pix2seq: A languagemodeling frameWork for object detection,2021, arXiv preprint arXiv:2109
 When vision transformers outperform resnetsWithout pretraining or strong data augmentations,2021, arXiv preprint arXiv:2106
 Transformer track-ing,2021, In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition
 Per-pixel classification is not all youneed for semantic segmentation,2021, arXiv preprint arXiv:2107
 Up-detr: Unsupervised pre-training forobject detection With transformers,2021, In Proceedings of the IEEE/CVF Conference on ComputerVision and Pattern Recognition
 Imagenet: A large-scale hier-archical image database,2009, In 2009 IEEE Conference on Computer Vision and Pattern Recognition
 Transvg: End-to-end visual grounding With transformers,2021, arXiv preprint arXiv:2104
 Animage is worth 16x16 words: Transformers for image recognition at scale,2020, arXiv preprintarXiv:2010
 Training vision transformersfor image retrieval,2021, arXiv preprint arXiv:2102
 Xcit: Cross-covariance image transformers,2021, arXiv preprint arXiv:2106
 Container:Context aggregation network,2021, arXiv preprint arXiv:2106
 Levit: a vision transformer in convnetâ€™s clothing for faster inference,2021, arXivpreprint arXiv:2104
 Accelerating sparse dnn models without hardware-support via tile-wise sparsity,2020, In SC20: International Conference for High Performance Comput-ing
 Transformer intransformer,2021, arXiv preprint arXiv:2103
 Learning both weights and connectionsfor efficient neural networks,2015, In Proceedings of the 28th International Conference on NeuralInformation Processing Systems - Volume 1
 Squeeze-and-excitation networks,2018, In Proceedings of the IEEEconference on computer vision and pattern recognition
 Generative adversarial transformers,2021, arXiv preprintarXiv:2103
 All tokens matter: Token labeling for training better vision transformers,2021, arXiv preprintarXiv:2104
 Hotr: End-to-endhuman-object interaction detection with transformers,2021, In Proceedings of the IEEE/CVF Confer-ence on Computer Vision and Pattern Recognition
 Revisiting stereo depth estimation from a sequence-to-sequence perspec-tive with transformers,2020, arXiv preprint arXiv:2011
 Efficienttraining of visual transformers with small-size datasets,2021, arXiv preprint arXiv:2106
 Swin transformer: Hierarchical vision transformer using shifted windows,2021, arXiv preprintarXiv:2103
 Learn-ing efficient convolutional networks through network slimming,2017, In Proceedings of the IEEEinternational conference on computer vision
 Efficient transformer for single imagesuper-resolution,2021, arXiv preprint arXiv:2108
 Dual-stream network for visual recognition,2021, arXiv preprint arXiv:2105
 Trackformer:Multi-object tracking with transformers,2021, arXiv preprint arXiv:2101
 An end-to-end transformer model for 3d objectdetection,2021, arXiv preprint arXiv:2109
 Dynamicvit:Efficient vision transformers with dynamic token sparsification,2021, arXiv preprint arXiv:2106
 Training data-efficient image transformers & distillation through attention,2021, InICML
 Attention is all you need,2017, In Advances in neural informationprocessing systems
 Visual transformers: Token-basedimage representation and processing for computer vision,2020, arXiv preprint arXiv:2006
 Learning texture transformernetwork for image super-resolution,2020, In Proceedings of the IEEE/CVF Conference on ComputerVision and Pattern Recognition
 Transformer-based attentionnetworks for continuous pixel-wise prediction,2021, In ICCV
 Glance-and-gaze visiontransformer,2021, arXiv preprint arXiv:2106
 Tokens-to-token vit: Training vision transformers from scratch onimagenet,2021, arXiv preprint arXiv:2101
 Rethinking semantic segmentation froma sequence-to-sequence perspective with transformers,2021, In Proceedings of the IEEE/CVF Confer-ence on Computer Vision and Pattern Recognition
 Visual transformer pruning,2021, arXiv preprintarXiv:2104
