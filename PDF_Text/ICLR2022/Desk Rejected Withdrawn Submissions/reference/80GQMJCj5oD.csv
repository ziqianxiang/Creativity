title,year,conference
 Information theory and an extension of the maximum likelihood principle,1973, In-ternational Symposium on Information Theory
 Natural gradient works efficiently in learning,1998, Neural Computation
 A closer look at memorization in deep networks,2017, In ICML
 Randomized algorithms for estimating the trace of an implicit sym-metric positive semi-definite matrix,2011, Journal of the ACM
 Gradient-based optimization of hyperparameters,2000, Neural Computation
 Random search for hyper-parameter optimization,2012, JMLR
 Bayesian theory,1994, John Wiley & Sons
 Pattern Recognition and Machine Learning,2006, Springer
 Practical gauss-newton optimisation for deeplearning,2017, In ICML
 A randomized algorithm for approximating the log determinant of a symmetricpositive definite matrix,2017, Linear Algebra and its Applications
 Bayesian back-propagation,1991, Complex Systems
 Semi-supervised learning,2006, MIT Press
 Analysis of stochastic lanczos quadrature forspectrum approximation,2021, In ICML
 Efficient multiple hyperparameter learningfor log-linear models,2009, In NIPS
 Generic methods for optimization-based modeling,2012, JMLR
 Scalable logdeterminants for gaussian process kernel learning,2017, In NeurIPS
 Adaptive subgradient methods for online learning andstochastic optimization,2011, JMLR
 Bilevel Programming forHyperparameter Optimization and Meta-Learning,2018, In ICML
 An investigation into neural net optimizationvia hessian eigenvalue density,2019, In ICML
 A kronecker-factored approximate fisher matrix for convolutionlayers,2016, In ICML
 Large-scale log-determinant computation throughstochastic chebyshev expansions,2015, In ICML
 Approximating spectral sums of large-scale matrices using stochastic chebyshev approximations,2017, SIAM Journal on Scientific Comput-ing 
 A stochastic estimator of the trace of the influence matrix for laplacian smoothingsplines,1990, Communications in Statistics - Simulation and Computation
 Batch Normalization: Accelerating Deep Network Training byReducing Internal Covariate Shift,2015, In ICML
 Universal statistics of fisher information indeep neural networks: Mean field approach,2019, In AISTATS
 Approximateinference turns deep networks into gaussian processes,2019, In NeurIPS
 Adam: A method for stochastic optimization,2015, In ICLR
 Information Criteria and Statistical Modeling,2007, SpringerPublishing Company
 Reviving and improving recurrent back-propagation,2018, In ICML
 DARTS: Differentiable Architecture Search,2018, InICLR
 Optimizing millions of hyperparameters byimplicit differentiation,2020, In AISTATS
 A bayesian perspective ontraining speed and model selection,2020, In NeurIPS
 Probable networks and plausible predictions - a review of practical bayesian meth-ods for supervised neural networks,1995, Network: Computation In Neural Systems
 Introduction to gaussian processes,1998, 1998
 Gradient-based Hyperparameter Opti-mization through Reversible Learning,2015, In ICML
 New insights and perspectives on the natural gradient method,2020, JMLR
 Optimizing neural networks with kronecker-factored approximatecurvature,2015, In ICML
 BAYESIAN LEARNING FOR NEURAL NETWORKS,1994, PhD thesis
 Realisticevaluation of deep semi-supervised learning algorithms,2018, In NeurIPS
 Hyperparameter optimization with approximate gradient,2016, In ICML
 A scalable laplace approximation for neuralnetworks,2018, In ICLR
 Topmoumoute online natural gradientalgorithm,2008, In NIPS
 Estimating the Dimension of a Model,1978, The Annals of Statistics
 Truncated Back-propagationfor Bilevel Optimization,2019, In AISTATS
 Fixmatch: Simplifying semi-supervisedlearning with consistency and confidence,2020, In NeurIPS
 Introduction to Statistical Machine Learning,2015, Morgan Kaufmann PublishersInc
 Distribution of information statistics and validity criteria of models,1976, MathematicalScience
 On the interplay between noise and curvature and its effect on opti-mization and generalization,2020, In AISTATS
 Learning invariances using themarginal likelihood,2018, In NeurIPS
 A widely applicable bayesian information criterion,1532, JMLR
 Wide Residual Networks,2016, In BMVC
 Generalized cross entropy loss for training deep neural networkswith noisy labels,2018, In Advances in Neural Information Processing Systems
