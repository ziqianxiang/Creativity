title,year,conference
 Extremely large minibatch sgd: Training resnet-50 on imagenet in 15 minutes,2017, arXiv preprint arXiv:1711
 Entropy-sgd: Biasing gradientdescent into wide valleys,2019, Journal of Statistical Mechanics: Theory and Experiment
 When vision transformers outperform resnetswithout pretraining or strong data augmentations,2021, arXiv preprint arXiv:2106
 Randaugment: Practical automateddata augmentation with a reduced search space,2020, In Proceedings of the IEEE/CVF Conference onComputer Vision and Pattern Recognition Workshops
 Imagenet: A large-scale hi-erarchical image database,2009, In 2009 IEEE conference on computer vision and pattern recognition
 Adabatch: Adaptive batch sizes fortraining deep neural networks,2017, arXiv preprint arXiv:1712
 Sharp minima can generalizefor deep nets,2017, In International Conference on Machine Learning
 Animage is worth 16x16 words: Transformers for image recognition at scale,2020, arXiv preprintarXiv:2010
 Sharpness-aware minimiza-tion for efficiently improving generalization,2020, arXiv preprint arXiv:2010
 DeeP residual learning for image recog-nition,2016, In IEEE Conference on Computer Vision and Pattern Recognition
 Firecaffe: near-linear acceleration of deeP neural network training on comPute clusters,2016, In IEEE Conference onComputer Vision and Pattern Recognition
 Averaging weights leads to wider oPtima and better generalization,2018, arXiv preprintarXiv:1803
 Three factors influencing minima in sgd,2017, arXiv preprintarXiv:1711
 CatastroPhic fisher exPlosion:Early Phase fisher matrix imPacts generalization,2021, In International Conference on Machine Learn-ing
 Highly scalable deep learning training system withmixed-precision: Training imagenet in four minutes,2018, arXiv preprint arXiv:1807
 On large-batch training for deep learning: Generalization gap and sharp minima,2016, arXivpreprint arXiv:1609
 Exploring the limits of concurrency in ml training on google tpus,2021, Machine Learning andSystems
 Asam: Adaptive sharpness-aware minimization for scale-invariant learning of deep neural networks,2021, arXiv preprintarXiv:2102
 Adaptive estimation of a quadratic functional by model selec-tion,2000, Annals of Statistics
 Visualizing the loss land-scape of neural nets,2017, arXiv preprint arXiv:1712
 Scaling distributed machine learning with system and algorithm co-design,2017, PhD thesis
 Concurrent adversariallearning for large-batch training,2021, arXiv preprint arXiv:2106
 Decoupled weight decay regularization,2017, arXiv preprintarXiv:1711
 Optimizing neural networks with kronecker-factored approximatecurvature,2015, In International conference on machine learning
 Adaptive methods fornonconvex optimization,2018, In Proceeding of 32nd Conference on Neural Information ProcessingSystems (NIPS 2018)
 On the convergence of adam and beyond,2019, arXivpreprint arXiv:1904
 Measuring the effects of data parallelism on neural network training,2018, arXivpreprint arXiv:1811
 A bayesian perspective on generalization and stochastic gradientdescent,2017, arXiv preprint arXiv:1710
 Normalized flat minima: Exploring scale in-variant definition of flat minima for neural networks using pac-bayesian analysis,2020, In InternationalConference on Machine Learning
 Smoothout:Smoothing out sharp minima to improve generalization in deep learning,2018, arXiv preprintarXiv:1805
 Yet another accelerated sgd:Resnet-50 training on imagenet in 74,2019,7 seconds
 Positively scale-invariant flat-ness of relu neural networks,2019, arXiv preprint arXiv:1903
 Image classification atsupercomputer scale,2018, arXiv preprint arXiv:1811
 Imagenet training inminutes,2018, In International Conference on Parallel Processing
 Large batch optimization for deeplearning: Training bert in 76 minutes,2019, arXiv preprint arXiv:1904
 mixup: Beyond empiricalrisk minimization,2017, arXiv preprint arXiv:1710
 Why adam beats sgd for attention models,2019, 2019
