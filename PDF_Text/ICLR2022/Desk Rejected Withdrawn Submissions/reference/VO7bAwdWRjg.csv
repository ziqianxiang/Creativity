title,year,conference
 Optuna:A next-generation hyperparameter optimization framework,2019, In Proceedings of the 25rd ACMSIGKDD International Conference on Knowledge Discovery and Data Mining
 A theory of cerebellar function,1971, Mathematical biosciences
 Spectrally-normalized margin bounds forneural networks,2017, Advances in Neural Information Processing Systems
 Interference and generalization in temporaldifference learning,2020, In International Conference on Machine Learning
 Neural networks for pattern recognition,1995, Oxford university press
 API design for machine learning software:experiences from the scikit-learn project,2013, In ECML PKDD Workshop: Languages for Data Min-ing and Machine Learning
 Quantifying generaliza-tion in reinforcement learning,1282, In International Conference on Machine Learning
 Generalization and regularization indqn,2018, arXiv preprint arXiv:1810
 Using semi-distributed representations to overcome catastrophic forgetting in connec-tionist networks,1991, Proceedings of the AAAI Conference on Artificial Intelligence
 Two geometric input trans-formation methods for fast online reinforcement learning with neural nets,2018, arXiv
 International Foundation for Autonomous Agents and MultiagentSystems,9781, ISBN 9781450375184
 Understanding the difficulty of training deep feedforward neuralnetworks,2010, In Proceedings of the thirteenth international conference on artificial intelligence andstatistics
 Spectral normalisation for deep reinforcement learning: an optimisation perspective,2021, arXivpreprint arXiv:2105
 Regularisation of neural net-works by enforcing lipschitz continuity,2021, Machine Learning
 Soft actor-critic al-gorithms and applications,2018, CoRR
 In Sheila A,2018, McIlraith and Kilian Q
 Learning sparse representations incrementallyin deep reinforcement learning,2019, arXiv
 Reproducibility ofbenchmarked deep reinforcement learning tasks for continuous control,2017, arXiv preprintarXiv:1708
 Adam: A method for stochastic optimization,2014, arXiv preprintarXiv:1412
 Value function approximation in reinforce-ment learning using the fourier basis,2011, In Twenty-fifth AAAI conference on artificial intelligence
 Implicit under-parameterizationinhibits data-efficient deep reinforcement learning,2020, In International Conference on Learning Rep-resentations
 Least-squares policy iteration,2003, The Journal of MachineLearning Research
 Continuous control with deep reinforcement learning,2015, arXivpreprint arXiv:1509
 The utility of sparse represen-tations for control in reinforcement learning,2019, Proceedings of the AAAI Conference on Arti-ficial Intelligence
 Regularization matters in policyoptimization-an empirical study on continuous control,2020, In International Conference on Learn-ing Representations
 I4r: Promoting deep reinforcementlearning by the indicator for expressive representations,2020, In IJCAI
 Deep hybrid neural-kernel networks using randomfourier features,2018, Neurocomputing
 Random fourier feature based deep learning for wirelesscommunications,2021, arXiv preprint arXiv:2101
 Human-level control through deep rein-forcement learning,0028, Nature
 Implicit regularization in deep learning,2017, arXiv preprint arXiv:1709
 Norm-based capacity control in neuralnetworks,2015, In Conference on Learning Theory
 Exploring general-ization in deep learning,2017, Advances in Neural Information Processing Systems
 Fuzzy tiling activations: A simple approach tolearning sparse representations online,2020, In International Conference on Learning Representations
 Multi-goal reinforce-ment learning: Challenging robotics environments and request for research,2018, arXiv preprintarXiv:1802
	Rl baselines3 zoo,2020,	https://github
 Stable baselines3,2019, https://github
 Random features for large-scale kernel machines,2007, In Proceedingsof the 20th International Conference on Neural Information Processing Systems
 Towards generalizationand simplicity in continuous control,2017, In Proceedings of the 31st International Conference onNeural Information Processing Systems
 Learning to learn without forgetting by maximizing transfer and minimizing interfer-ence,2018, arXiv preprint arXiv:1810
 A case for new neuralnetwork smoothness constraints,2020, In Jessica Zosa Forde
 Weight normalization: A simple reparameterization to acceleratetraining of deep neural networks,2016, Advances in neural information processing systems
 Lipschitz regularity of deep neural networks: analysis andefficient estimation,2018, In Proceedings of the 32nd International Conference on Neural InformationProcessing Systems
 Ray interference: a source ofplateaus in deep reinforcement learning,2019, arXiv preprint arXiv:1904
 Proximal policyoptimization algorithms,2017, arXiv preprint arXiv:1707
 Reinforcement Learning: An Introduction,2018, The MIT Press
 Fourier featureslet networks learn high frequency functions in low dimensional domains,2020, arXiv preprintarXiv:2006
 Pygame learning environment,2016, https://github
 On the eigenvector bias of fourier feature net-works: From regression to solving multi-scale pdes with physics-informed neural networks,2021, Com-puter Methods in Applied Mechanics and Engineering
 A deeper look at experience replay,2017, arXiv preprintarXiv:1712
g with bootstrapping,2017, In addition
