title,year,conference
 Tensorflow: A system for large-scalemachine learning,2016, In Proc
 Universal approximation bounds for superpositions of a sigmoidal function,1993, IEEETrans
 Approximation and estimation bounds for artificial neural networks,1994, Mach
 On deep learning as a remedy for the curse of dimensionality innonparametric regression,2019, Ann
 Simultaneous analysis of lasso anddantzig selector,2009, Ann
 Large-scale machine learning with stochastic gradient descent,2010, In Proc
 Decoding by linear programming,2005, IEEE Trans
 Approximations by superpositions of a sigmoidal function,1989, Math
 Consistent feature selection for analytic deep neural networks,2020, arXivpreprint arXiv:2010
 Uncertainty principles and ideal atomic decomposition,2001, IEEETrans
 Learning one-hidden-layer neural networks with landscapedesign,2017, arXiv preprint arXiv:1711
 Size-independent sample complexity ofneural networks,2017, arXiv preprint arXiv:1712
 Beating the perils of non-convexity:Guaranteed training of neural networks using tensor methods,2015, arXiv preprint arXiv:1506
 Adam: A method for stochastic optimization,2014, arXiv preprintarXiv:1412
 On the representation of continuous functions of many variables bysuperposition of continuous functions of one variable and addition,1957, In Doklady Akademii Nauk
 On the connection between learning two-layers neuralnetworks and tensor decomposition,2018, arXiv preprint arXiv:1802
 Norm-based capacity control in neuralnetworks,2015, Conf
 Nonparametric regression using deep neural networks with relu activationfunction,2017, arXiv preprint arXiv:1708
 Regression shrinkage and selection via the lasso,1996, J
 The lasso method for variable selection in the cox model,1997, Stat
 High-dimensional generalized linear models and the LASSO,2008, Ann
 On model selection consistency of Lasso,2006, J
