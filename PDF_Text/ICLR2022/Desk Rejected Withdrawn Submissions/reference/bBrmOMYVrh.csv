title,year,conference
 Intrinsic dimensionality explains the ef-fectiveness of language model fine-tuning,2020, arXiv preprint arXiv:2012
 The second PASCAL recognising textual entailment challenge,2006, 2006
 Adapterhub playground: Simple and flex-ible few-shot learning with adapters,2021, arXiv preprint arXiv:2108
 The fifth PASCAL recognizingtextual entailment challenge,2009, In TAC
 Continual lifelong learningin natural language processing: A survey,2020, In Proceedings of the 28th International Conferenceon Computational Linguistics
 Language models arefew-shot learners,2020, arXiv preprint arXiv:2005
 Once-for-all: Train onenetwork and specialize it for efficient deployment,2020, In International Conference on LearningRepresentations
 ELECTRA: pre-training text encoders as discriminators rather than generators,2020, In 8th International Conferenceon Learning Representations
 The PASCAL recognising textual entailmentchallenge,2005, In the First International Conference on Machine Learning Challenges: EvaluatingPredictive Uncertainty Visual Object Classification
 BERT: pre-training of deepbidirectional transformers for language understanding,2019, In Proceedings of the 2019 Conference ofthe North American Chapter of the Association for Computational Linguistics: Human LanguageTechnologies
 Self-training improves pre-training for natural language understand-ing,2020, arXiv preprint arXiv:2010
 Model-agnostic meta-learning for fast adaptationof deep networks,2017, In International Conference on Machine Learning
 Making pre-trained language models better few-shotlearners,2021, In Association for Computational Linguistics (ACL)
 The third PASCAL recog-nizing textual entailment challenge,2007, In the ACL-PASCAL Workshop on Textual Entailment andParaphrasing
 Supervised contrastive learningfor pre-trained language model fine-tuning,2020, In International Conference on Learning Represen-tations
 Learning both weights and connections forefficient neural network,2015, In NIPS
 Deberta: decoding-enhancedbert with disentangled attention,2021, In 9th International Conference on Learning Representations
 Distilling the knowledge in a neural network,2015, arXivpreprint arXiv:1503
 Optimal brain damage,1990, In Advances in neuralinformation processing systems
 The power of scale for parameter-efficient prompttuning,2021, CoRR
 Roberta: A robustly optimized BERT pretrainingapproach,2019, CoRR
 Roberta: A robustly optimized BERT pretrainingapproach,2019, CoRR
 Decoupled weight decay regularization,2017, arXiv preprintarXiv:1711
 Uncertainty-aware self-training for few-shot textclassification,2020, Advances in Neural Information Processing Systems
 A sentimental education: Sentiment analysis using subjectivity summa-rization based on minimum cuts,2004, 2004
 Adapterhub: A framework for adapting transform-ers,2020, In Proceedings of the 2020 Conference on Empirical Methods in Natural LanguageProcessing (EMNLP 2020): Systems Demonstrations
 Exploring the limits of transfer learning with a unified text-to-texttransformer,2020, Journal of Machine Learning Research
 Learning to reweight examples forrobust deep learning,2018, In International Conference on Machine Learning
It's notjust size that matters: Small language models are also few-shot learners,2021, In Proceedings of the 2021 Conference of the North American Chapter of the Asso-ciation for Computational Linguistics: Human Language Technologies
 Exploiting cloze-questions for few-shot text classification andnatural language inference,2021, In Proceedings of the 16th Conference of the European Chapter ofthe Association for Computational Linguistics: Main Volume
 Strata: Self-training withtask augmentation for better few-shot learning,2021, arXiv preprint arXiv:2109
 Annotating expressions of opinions and emotionsin language,2005, Language resources and evaluation
 A broad-coverage challenge corpus for sen-tence understanding through inference,2018, In Proceedings of the 2018 Conference of the North Amer-ican Chapter of the Association for Computational Linguistics: Human Language Technologies
 A broad-coverage challenge corpus for sen-tence understanding through inference,2018, 2018b
 Unsupervised dataaugmentation for consistency training,2019, arXiv preprint arXiv:1904
 Revisiting few-sample BERT fine-tuning,2021, 2021
 Useradapter: Few-shot userlearning in sentiment analysis,2021, In ACL/IJCNLP (Findings)
