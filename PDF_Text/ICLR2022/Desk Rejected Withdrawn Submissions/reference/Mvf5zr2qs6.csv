title,year,conference
 Natural gradient works efficiently in learning,1998, Neural computation
 Theoretical analysis of auto rate-tuning by batchnormalization,2019, In International Conference on Learning Representations
 Language models arefew-shot learners,2020, arXiv preprint arXiv:2005
 Gradient descent onneural networks typically occurs at the edge of stability,2021, In International Conference on LearningRepresentations
 Sharp minima can generalizefor deep nets,2017, In International Conference on Machine Learning
 Sharpness-aware minimiza-tion for efficiently improving generalization,2020, arXiv preprint arXiv:2010
 Bag of tricks forimage classification with convolutional neural networks,2019, In Proceedings of the IEEE/CVF Con-ference on Computer Vision and Pattern Recognition
 Flat minima,1997, Neural computation
 Norm matters: efficient and accuratenormalization schemes in deep networks,2018, In Proceedings of the 32nd International Conferenceon Neural Information Processing Systems
 The break-even point on optimization trajectories of deep neuralnetworks,2020, In International Conference on Learning Representations
 Catastrophic fisher explosion:Early phase fisher matrix impacts generalization,2021, In International Conference on Machine Learn-ing
 Adascale sgd: A user-friendlyalgorithm for distributed training,2020, In International Conference on Machine Learning
 On large-batch training for deep learning: Generalization gap and sharp minima,2017, In5th International Conference on Learning Representations
 Learning multiple layers of features from tiny images,2009, Masterâ€™s thesis
 Asam: Adaptive sharpness-aware minimization for scale-invariant learning of deep neural networks,2021, arXiv preprintarXiv:2102
 Wide neural networks of any depth evolve as linear mod-els under gradient descent,2019, Advances in neural information processing systems
 SNIP: SINGLE-SHOT NET-WORK PRUNING BASED ON CONNECTION SENSITIVITY,2019, In International Confer-ence on Learning Representations
 A signal prop-agation perspective for pruning neural networks at initialization,2020, In International Confer-ence on Learning Representations
 The largelearning rate phase of deep learning: the catapult mechanism,2020, arXiv preprint arXiv:2003
 Visualizing the loss land-scape of neural nets,2018, Advances in Neural Information Processing Systems
 Decoupled weight decay regularization,2019, In International Confer-ence on Learning Representations
 Optimizing neural networks with kronecker-factored approximatecurvature,2015, In International conference on machine learning
 Norm-based capacity control in neuralnetworks,2015, In Conference on Learning Theory
 The effect of network width onstochastic gradient descent and generalization: an empirical study,2019, In International Conferenceon Machine Learning
 Learning transferable visualmodels from natural language supervision,2021, arXiv preprint arXiv:2103
 Very deep convolutional networks for large-scale imagerecognition,2015, In International Conference on Learning Representations
 On the generalization benefit of noise in stochasticgradient descent,2020, In International Conference on Machine Learning
 A bayesian perspective on generalization and stochastic gradientdescent,2018, In International Conference on Learning Representations
 Pruning neural networkswithout any data by iteratively conserving synaptic flow,2020, Advances in Neural Information Pro-cessing Systems
 On the interplay between noise and curvature and its effect on optimizationand generalization,2020, In International Conference on Artificial Intelligence and Statistics
 Attention is all you need,2017, In Advances in neural informationprocessing systems
 Picking winning tickets before training by pre-serving gradient flow,2020, In International Conference on Learning Representations
 Large batch training of convolutional networks,2017, arXivpreprint arXiv:1708
 Large batch optimization for deep learn-ing: Training bert in 76 minutes,2020, In International Conference on Learning Representations
 Three mechanisms of weight decayregularization,2019, In International Conference on Learning Representations
