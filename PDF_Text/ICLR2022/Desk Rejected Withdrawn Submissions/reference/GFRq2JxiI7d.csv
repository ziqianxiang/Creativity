title,year,conference
 Fine-grained analysis ofoptimization and generalization for overparameterized two-layer neural networks,2019, In InternationalConference on Machine Learning
 Breaking the Curse of Dimensionality with Convex Neural Networks,2014, arXiv e-prints
 Greedy layerwise learning can scaleto imagenet,2019, In International conference on machine learning
 The lottery tickets hypothesis for supervised and self-supervised pre-training incomputer vision models,2020, arXiv preprint arXiv:2012
 The Lottery Ticket Hypothesis for Pre-trained BERT Networks,2020, arXiv e-prints
 EarlyBERT:Efficient BERT Training via Early-bird Lottery Tickets,2020, arXiv e-prints
 Imagenet: A large-scalehierarchical image database,2009, In 2009 IEEE conference on computer vision and pattern recognition
 The mnist database of handwritten digit images for machine learning research,2012, IEEE SignalProcessing Magazine
 Gradient descent finds globalminima of deep neural networks,1675, In International Conference on Machine Learning
 Rigging the Lottery:Making All Tickets Winners,2019, arXiv e-prints
 Gradient Flow in Sparse NeuralNetworks and How Lottery Tickets Win,2020, arXiv e-prints
 An algorithm for quadratic programming,1956, Naval researchlogistics quarterly
 Stabilizing theLottery Ticket Hypothesis,2019, arXiv e-prints
 The State of Sparsity in Deep Neural Networks,2019, arXive-prints
 Learning One Convolutional Layer with OverlappingPatches,2018, arXiv e-prints
 Finite depth and width corrections to the neural tangent kernel,2019, arXivpreprint arXiv:1909
 Deep Residual Learning for ImageRecognition,2015, arXiv e-prints
 Channel Pruning for Accelerating Very Deep NeuralNetworks,2017, arXiv e-prints
 Neural tangent kernel: Convergence andgeneralization in neural networks,2018, arXiv preprint arXiv:1806
 Learning relu networks via alternating minimization,2018, arXivpreprint arXiv:1806
 Revisiting Frank-Wolfe: Projection-free sparse convex optimization,2013, In SanjoyDasgupta and David McAllester (eds
 Learning multiple layers of features from tiny images,2009, 2009
 Pruning Filters forEfficient ConvNets,2016, arXiv e-prints
 Learning overparameterized neural networks via stochastic gradientdescent on structured data,2018, arXiv preprint arXiv:1808
 Convergence Analysis of Two-layer Neural Networks with ReLUActivation,2017, arXiv e-prints
 Rethinking the Value ofNetwork Pruning,2018, arXiv e-prints
 A Mean-field Analysis of DeepResNet and Beyond: Towards Provable Optimization Via Overparameterization From Depth,2020, arXive-prints
 ThiNet: A Filter Level Pruning Method for Deep NeuralNetwork Compression,2017, arXiv e-prints
 Proving the Lottery TicketHypothesis: Pruning is All You Need,2020, arXiv e-prints
 A Mean Field View of the Landscape ofTwo-Layers Neural Networks,2018, arXiv e-prints
 One ticket to win them all: general-izing lottery ticket initializations across datasets and optimizers,2019, arXiv preprint arXiv:1906
 An analysis of approximations formaximizing SUbmodUlar set functionsâ€”i,1978, Mathematical programming
 Training neural networks with local error signals,2019, InInternational Conference on Machine Learning
 Logarithmic Pruning is All You Need,2020, arXive-prints
 Towards moderate overparameterization: global conver-gence guarantees for training shallow neural networks,2019, arXiv e-prints
 Deep Neural Network Training withFrank-Wolfe,2020, arXiv e-prints
 Comparing Rewinding and Fine-tuning in NeuralNetwork Pruning,2020, arXiv e-prints
 Mo-bilenetv2: Inverted residuals and linear bottlenecks,2018, In Proceedings of the IEEE conference oncomputer vision and pattern recognition
 Picking winning tickets before training bypreserving gradient flow,2020, arXiv preprint arXiv:2002
 Picking Winning Tickets Before Training byPreserving Gradient Flow,2020, arXiv e-prints
 On Layer Normalization in the Transformer Architec-ture,2020, arXiv e-prints
 Good subnetworksprovably exist: Pruning via greedy forward selection,2020, In International Conference on MachineLearning
 Greedy Optimization Provably Wins the Lottery: LogarithmicNumber of Winning Tickets is Enough,2020, arXiv e-prints
 Drawing early-bird tickets: Towards more efficient trainingof deep networks,2019, arXiv preprint arXiv:1909
 NISP: Pruning Networks using Neuron Importance ScorePropagation,2017, arXiv e-prints
 Learning one-hidden-layer relunetworks via gradient descent,2019, In The 22nd International Conference on Artificial Intelligence andStatistics
 Discrimination-aware Channel Pruning for Deep Neural Networks,2018, arXiv e-prints
