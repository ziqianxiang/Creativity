title,year,conference
 Layer normalization,2016, CoRR
 Statistical mechanics of deep learning,2020, Annual Review of Condensed Mat-ter Physics
 Gradient descent finds globalminima of deep neural networks,2019, In Kamalika Chaudhuri and Ruslan Salakhutdinov (eds
 Adaptive subgradient methods for online learning andstochastic optimization,2011, In J
 Neocognitron: A self-organizing neural network model fora mechanism of visual pattern recognition,1982, In Shun-ichi Amari and Michael A
 Deep sparse rectifier neural networks,2011, InGeoffrey Gordon
 Deep sparse rectifier neural networks,2011, InGeoffrey Gordon
 Modeling the influence ofdata structure on learning in neural networks: The hidden manifold model,2020, Physical Review X
 Deep relu networks have surprisingly few activation patterns,2019, In NeurIPS
 Which neural net architectures give rise to exploding and vanishing gradients? InS,2018, Bengio
 On the impact of the activation functionon deep neural networks training,2019, In Kamalika Chaudhuri and Ruslan Salakhutdinov (eds
 Relu deep neural networks and linear finiteelements,1991, Journal of Computational Mathematics
 Delving deep into rectifiers: Surpassinghuman-level performance on imagenet classification,2015, IEEE International Conference on Com-puter Vision (ICCV 2015)
 Deep neural networks for acoustic modeling in speech recogni-tion: The shared views of four research groups,2012, IEEE Signal Processing Magazine
 Batch normalization: Accelerating deep network training byreducing internal covariate shift,2015, In Francis Bach and David Blei (eds
 Adam: A method for stochastic optimization,2015, CoRR
 Self-normalizingneural networks,2017, In I
 Imagenet classification with deep convo-lutional neural networks,2012, In F
 Deep learning,2015, Nature
 Deep neural networks as gaussian processes,2018, In International Confer-ence on Learning Representations
 Exploring the function space of deep-learning machines,2018, Physical ReviewLetters
 Large deviation analysis of function sensitivity in random deep neuralnetworks,2020, Journal of Physics A: Mathematical and Theoretical
 Dying relu and initialization: Theoryand numerical examples,2020, Communications in Computational Physics
 Rectifier nonlinearities improve neural net-work acoustic models,2013, In in ICML Workshop on Deep Learning for Audio
 All you need is a good init,2016, CoRR
 Introductory Lectures on Convex Optimization: A Basic Course,1461, Springer PublishingCompany
 Exponential expressivity in deep neural networks through transient chaos,2016, InD
 On theexpressive power of deep neural networks,2017, In Proceedings of the 34th International Conferenceon Machine Learning - Volume 70
 Weight normalization: A simple reparameterization toaccelerate training of deep neural networks,2016, In NIPS
 Exact solutions to the nonlinear dy-namics of learning in deep linear neural networks,2014, In Yoshua Bengio and Yann LeCun (eds
 Deep informationpropagation,2017, In 5th International Conference on Learning Representations
 Understanding and improv-ing convolutional neural networks via concatenated rectified linear units,2016, In Proceedings of the33rd International Conference on International Conference on Machine Learning - Volume 48
 Mastering the game of go withdeep neural networks and tree search,2016, Nature
 Training very deep networks,2015, 2015Neural Information Processing Systems (NIPS 2015 Spotlight)
 Phone recognition with deep sparse rectifier neural networks,2013, 2013 IEEE InternationalConference on Acoustics
 Parametric exponential linear unit for deep con-volutional neural networks,2017, 2017 16th IEEE International Conference on Machine Learning andApplications (ICMLA)
 Instance normalization: The missing ingredient for faststylization,2016, ArXiv
 Deep learning generalizes be-cause the parameter-function map is biased towards simple functions,2019, In International Confer-ence on Learning Representations
 Group normalization,2018, In ECCV
 Adadelta: An adaptive learning rate method,2012, ArXiv
