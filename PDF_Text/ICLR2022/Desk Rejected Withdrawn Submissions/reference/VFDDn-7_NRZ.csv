title,year,conference
 Label refinery:Improving imagenet classification through label progression,2018, arXiv preprint arXiv:1805
 Trellis networks for sequence modeling,2019, In ICLR
 Deep equilibrium models,2019, In Proceedings of theInternational Conference on Neural Information Processing Systems
 Multiscale deep equilibrium models,2020, In Proceedingsof the International Conference on Neural Information Processing Systems
 Language models arefew-shot learners,2020, arXiv preprint arXiv:2005
 Autoformer: Searching transformersfor visual recognition,2021, In Proceedings of the IEEE/CVF International Conference on ComputerVision (ICCV)
 Modeling hierarchical structures with continuousrecursive neural networks,2021, In Proceedings of the 38th International Conference on MachineLearning 
 Imagenet: A large-scalehierarchical image database,2009, In 2009 IEEE conference on computer vision and pattern recognition
 Bert: Pre-training of deepbidirectional transformers for language understanding,2019, In Proceedings of the 2019 Conference ofthe North American Chapter of the Association for Computational Linguistics: Human LanguageTechnologies
 Speech-transformer: A no-recurrence sequence-to-sequencemodel for speech recognition,2018, In 2018 IEEE International Conference on Acoustics
 Dynamic recursiveneural network,2019, In Proceedings of the IEEE/CVF Conference on Computer Vision and PatternRecognition
 Transformer intransformer,2021, arXiv preprint arXiv:2103
 Spatial pyramid pooling in deepconvolutional networks for visual recognition,2016, IEEE transactions on pattern analysis and machineintelligence
 Distilling the knowledge in a neural network,2016, arXivpreprint arXiv:1503
 Albert: A lite bert for self-supervised learning of language representations,2019, arXiv preprintarXiv:1909
 Beyond bags of features: Spatial pyramid matching forrecognizing natural scene categories,2021, In 2006 IEEE Computer Society Conference on ComputerVision and Pattern Recognition (CVPRâ€™06)
 Recurrent convolutional neural network for object recognition,2015, InProceedings of the IEEE conference on computer vision and pattern recognition
 Self-adaptive scaling for learnable residualstructure,2019, In Proceedings of the 23rd Conference on Computational Natural Language Learning(CoNLL)
 A recursive recurrent neural network for statistical ma-chine translation,2014, In Proceedings of the 52nd Annual Meeting of the Association for ComputationalLinguistics (Volume 1: Long Papers)
 Roberta: A robustly optimized bert pretrainingapproach,2019, arXiv preprint arXiv:1907
 Swin transformer: Hierarchical vision transformer using shifted windows,2021, arXiv preprintarXiv:2103
 Effective approaches to attention-basedneural machine translation,2015, In Proceedings of the 2015 Conference on Empirical Methods inNatural Language Processing
 Bleu: a method for automaticevaluation of machine translation,2002, In Proceedings of the 40th Annual Meeting of the Associationfor Computational Linguistics
 Meta pseudo labels,2019, arXivpreprint arXiv:2003
 Designingnetwork design spaces,2014, In Proceedings of the IEEE/CVF Conference on Computer Vision andPattern Recognition
 Meal v2: Boosting vanilla resnet-50 to 80%+ top-1 accuracy onimagenet without tricks,2021, In NeurIPS Workshop
 Supervised neural networks for the classification ofstructures,1997, IEEE Transactions on Neural Networks
 Training data-efficient image transformers & distillation through attention,2020, arXiv preprintarXiv:2012
 Goingdeeper with image transformers,2021, arXiv preprint arXiv:2103
 Attention is all you need,2017, In NIPS
 Pyramid vision transformer: A versatile backbone for dense prediction withoutconvolutions,2021, arXiv preprint arXiv:2102
 Transformer in action: a comparative study of transformer-based acoustic models forlarge scale speech recognition applications,2021, In ICASSP 2021-2021 IEEE International Conferenceon Acoustics
 Cvt:Introducing convolutions to vision transformers,2021, arXiv preprint arXiv:2103
 Self-training with noisy studentimproves imagenet classification,2020, In Proceedings of the IEEE/CVF Conference on Computer Visionand Pattern Recognition
 Tokens-to-token vit: Training vision transformers from scratch on imagenet,2021, arXivpreprint arXiv:2101
 Deepvit: Towards deeper vision transformer,2021, arXiv preprint arXiv:2103
