title,year,conference
 A convergence theory for deep learning via over-parameterization,2019, In International Conference on Machine Learning
 Parameter adaptation instochastic optimization,1999, In Publications of the Newton Institute
 Complexity guarantees for Polyak stepswith momentum,2020, In Conference on Learning Theory
 Two-point step size gradient methods,1988, IMA journal ofnumerical analysis
 On the ineffectiveness of variance reduced optimization for deeplearning,2019, In Advances in Neural Information Processing Systems (NeurIPS)
 Adaptive subgradient methods for online learning andstochastic optimization,2011, Journal of machine learning research
 SGD: General analysis and improved rates,2019, In International Conference on MachineLearning
 Deep residual learning for imagerecognition,2016, In Proceedings of the IEEE conference on computer vision and pattern recognition
 Densely connectedconvolutional networks,2017, In Proceedings of the IEEE conference on computer vision and patternrecognition
 Increased rates of convergence through learning rate adaptation,1988, Neural networks
 Fundamentals of Digital Image Processing,1989, Prentice-Hall
 Improving generalization performance by switching fromAdam to SGD,2017, arXiv preprint arXiv:1712
 Adam: A method for stochastic optimization,2015, In Proceedings ofthe International Conference on Learning Representations
 SSD: Single shot multibox detector,2016, In Proceedings of the EuropeanConference on Computer Vision
 Stochastic Polyakstep-size for SGD: An adaptive learning rate for fast convergence,2021, In International Conference onArtificial Intelligence and Statistics
 SGDR: Stochastic gradient descent with warm restarts,2017, In Proceedingsof the International Conference on Learning Representations
 Adaptive Gradient Descent without descent,2020, In Interna-tional Conference on Machine Learning
 Back-propagation heuristics: a study of the extended Delta-Bar-Delta algorithm,1990, In 1990 IJCNN International Joint Conference on Neural Networks
 Numerical optimization,2006, Springer Science & Business Media
 On convergence-diagnostic based stepsizes for stochastic gradient descent,2020, In Proceedings of the 37th International Conference onMachine Learning
 On the determination of the step size in stochastic quasigradient methods,1983,1983
 Some methods of speeding up the convergence of iteration methods,1964, USSRcomputational mathematics and mathematical physics
 The conjugate gradient method in extremal problems,1969, USSR ComputationalMathematics and Mathematical Physics
 AutoLR: Layer-wise pruning and auto-tuning of learning rates infine-tuning of deep networks,2021, In In proceedings of the AAAI Conference on Artificial Intelligence
 Complexity analysis of second-order line-search algorithmsfor smooth nonconvex optimization,2018, SIAM Journal on Optimization
 Learning applied to successive approximation algorithms,1970, IEEE Transactions onsystems science and cybernetics
 No more pesky learning rates,2013, In InternationalConference on Machine Learning
 Local gain adaptation in stochastic gradient descent,1999, Technical ReportIDSIA-09-99
 The singular values of convolutional layers,2019, InProceedings of the International Conference on Learning Representations
 Two problems with back propagation and other steepest descent learning proceduresfor networks,1986, In Proceedings of the Eighth Annual Conference of the Cognitive Science Society
 Lecture 6,2012,5-RMSProp
 Understanding short-horizon biasin stochastic meta-optimization,2018, In Proceedings of the International Conference on LearningRepresentations
 Large batch training of convolutional networks,2017, arXivpreprint arXiv:1708
 Large batch optimization for deeplearning: Training BERT in 76 minutes,2020, In Proceedings of the International Conference onLearning Representations
 Wide residual networks,2016, In Edwin R
