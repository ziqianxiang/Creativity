Under review as a conference paper at ICLR 2022
Active Deep Multiple Instance Learning
Anonymous authors
Paper under double-blind review
Ab stract
State-of-the-art multiple instance learning (MIL) models achieve competitive per-
formance at the bag level. However, instance-level prediction, which is essential
for many important applications, remains largely unsatisfactory. We propose a
novel active deep multiple instance learning (ADMIL) model that samples a small
subset of informative instances for annotation, aiming to significantly boost the
instance-level prediction. A variance regularized loss function is designed to
properly balance the bias and variance of instance-level predictions, aiming to
effectively accommodate the highly imbalanced instance distribution in MIL and
other fundamental challenges. Instead of directly minimizing the variance reg-
ularized loss that is non-convex, we optimize a distributionally robust bag level
likelihood as its convex surrogate. The robust bag likelihood provides a good
approximation of the variance based MIL loss with a strong theoretical guarantee.
It also automatically balances bias and variance, making it effective to identify the
potentially positive instances to support active sampling. The robust bag likelihood
can be naturally integrated with a deep architecture to support deep model training
using mini-batches of positive-negative bag pairs. Finally, a novel P-F sampling
function is developed that combines a probability vector and predicted instance
scores, obtained by optimizing the robust bag likelihood. By leveraging the key
MIL assumption, the sampling function can explore the most challenging bags
and effectively detect their positive instances for annotation, which significantly
improves the instance-level prediction. Experiments conducted over multiple real-
world datasets clearly demonstrate the state-of-the-art instance-level prediction
achieved by the proposed ADMIL model.
1	Introduction
Multiple Instance Learning (MIL) offers an attractive weakly supervised learning paradigm, where
instances are naturally organized into bags and training labels are assigned at the bag level to reduce
the annotation cost (Dietterich et al., 1997; Settles et al., 2008; Li & Vasconcelos, 2015b; Sultani
et al., 2018). State-of-the-art MIL models achieve competitive performance at the bag level. However,
instance-level prediction, which is essential for many important applications (e.g., anomaly detection
from surveillance videos (Sultani et al., 2018) and medical image segmentation (Ilse et al., 2018))
remains largely unsatisfactory. In MIL, a bag is considered to be positive if at least one of the
instances is positive otherwise negative (Dietterich et al., 1997; HauBmann et al., 2017). To achieve a
high bag level prediction, most existing MIL models effectively leverage this key MIL assumption by
focusing on the most positive instance from a positive bag that is mainly responsible for determining
the bag label (Andrews et al., 2002; Kim & Torre, 2010; Sultani et al., 2θ18; HauBmann et al., 2017).
However, they suffer from two major limitations, which lead to poor instance-level predictions. First,
solely focusing on the most positive instance is sensitive to outliers, which are negative instances that
look very different from other negative ones (Carbonneau et al., 2018). As a result, these instances
may be wrongly assigned a high score indicating they are positive. Second, there may be multiple
types (i.e., multimodal) of positive instances in a single bag (e.g., different types of anomalies in
a surveillance video or different types of skin lesions in a dermatology image). Thus, focusing
on a single most positive instance will miss other positive ones. Both cases will result in a low
instance-level prediction performance. A possible solution to improve the detection of positive
instances is to consider the top-k most positive instances. However, the number of positive instances
may vary significantly across different bags and applying the same k to all bags may be inappropriate.
Furthermore, finding an optimal k for each bag is highly challenging as it takes discrete values.
1
Under review as a conference paper at ICLR 2022
Figure 1: (a) Example of a challenging bag; (b) MI-AL performance on instance-level predictions;
(c)-(e) Prediction scores of instances in the bag in different MI-AL steps.
The underlying reason for the less accurate instance-level prediction is due to the lack of instance
labels. For positive instances that are relatively rare across bags, detecting them by only relying on
bag labels is inherently challenging as the weakly supervised signal (i.e., the bag label) cannot be
propagated to the instance level without sufficient statistical evidence. One promising direction to
tackle this fundamental challenge is to augment MIL with active learning (AL). Multiple instance
AL (or MI-AL) aims to select a small number of informative instances to improve the instance level
prediction in MIL. In most MIL problems, the data is highly imbalanced at the instance level, where
the positive ones are much more sparse. Since the positive instances usually carry more important
information, a primary goal of MI-AL is to effectively sample the positive instances from a candidate
pool dominated by the negative ones. If a true positive instance can be sampled and labeled, it can
help to improve the identification of other similar positive instances in the same and different bags,
which will significantly improve the instance-level predictions.
However, existing MIL models may easily miss some rare positive instances (Sultani et al., 2018).
They may also focus on the wrongly identified negative instances due to their sensitivity to outliers
or incapability of handling multimodal bags. Thus, the true positive instances may be assigned a
low prediction score, indicating that they are predicted as negative with a high confidence. As a
result, commonly used uncertainty based sampling will miss these important instances. Figure 1
(a) shows a challenging bag, which is an image that contains the shadow of a bird (as the positive
class). The positive instances are patches that cover (part of) the bird shadow. Figure 1 (b) shows that
combining uncertainty sampling with a maximum score based MIL model (the green curve) is not
able to sample effectively so that instance-level prediction remains very low over the AL process.
Figure 1 (c) further confirms that the initial prediction score (F-score) of the positive instance is close
to 0, making it hard to be sampled.
We propose a novel MI-AL model for effective instance sampling to significantly boost the instance-
level prediction in MIL. We design an unique variance regularized MIL loss that encourages a high
variance of the prediction scores to address bags with a highly imbalanced instance distribution
and/or those with outliers and multimodal scenarios. Since the variance regularizer is non-convex, we
propose to optimize a distributionally robust bag likelihood (DRBL), which provides a good convex
approximation of the variance based loss with a strong theoretical guarantee. The DRBL automatically
adjusts the impact of the bag-level variance, making it more effective to detect potentially positive
instances to support active sampling. It can also be naturally integrated with a deep architecture
to support deep MIL model training using mini-batches of positive-negative bag pairs. Finally, a
novel P-F sampling function is developed that combines a probability vector (i.e., p) and predicted
instance scores (i.e., f), obtained by optimizing the robust bag likelihood. By leveraging the key MIL
assumption, the sampling function can explore the most challenging bags and effectively detect their
positive instances for annotation, which significantly improves the instance-level prediction. Novel
batch-mode sampling is developed to work seamlessly with the deep MIL, leading to a powerful
active deep MIL (ADMIL) model to support sampling of high-dimensional data used in most MIL
applications. Figure 1 (b) shows the proposed model (purple curve) that significantly improves
instance predictions. Figures 1 (c)-(e) shows P-F sampling dynamically updates the probability p
and score f values to effectively sample the positive instance from the highly challenging bag in a
few steps.
Our main contribution includes: (i) a unique variance regularized MIL loss and its convex surrogate
that address inherent MIL challenges to best support active sampling, (ii) a novel P-F sampling
function to effectively explore most challenging bags with rare positive instances, (iii) mini-batch
training and batch-mode active sampling to support ADMIL in broader MIL applications, and (iv)
state-of-the-art instance prediction performance in MIL while maintaining low instance annotations.
2
Under review as a conference paper at ICLR 2022
2	Related Work
Multiple Instance Learning (MIL). Existing supervised learning models have been leveraged to
tackle MIL problems, including SVM (Andrews et al., 2002), boosting (Xu & Frank, 2004), graph-
based models (Zhou et al., 2009), conditional random field (Deselaers & Ferrari, 2010) and Gaussian
Processes (Hauβmann et al., 2017; Kim & Torre, 2010). Other approaches try to relax the MIL
assumption, which allows positive instances in a negative bag to handle noisy bags (Li & Vasconcelos,
2015a). As MIL is commonly applied to problems, such as video anomaly detection and image
segmentation that involve high dimensional data, deep neural networks have become a popular choice
for training MIL models (Ilse et al., 2018; Sultani et al., 2018). Despite the significant progress made
so far, most existing models focus on improving the bag-level predictions. As a result, instance-level
performance still falls short in meeting the high standard in critical applications (Sultani et al., 2018;
Ilse et al., 2018; HauBmann et al., 2017). The proposed ADMIL model aims to fill out this critical gap
by augmenting MIL with novel active sampling strategies to significantly boost instance predictions
using limited labeled instances to maintain a low annotation cost.
Active Learning (AL). Uncertainty and margin based measures are commonly leveraged in existing
AL models to achieve efficient data sampling (Tong & Koller, 2001; Roy & McCallum, 2001; Holub
et al., 2008; Culotta & McCallum, 2005; Rajan et al., 2008; Joshi et al., 2009). Distributionally robust
optimization has also been adopted in multi-class AL to address sampling bias and imbalanced data
distribution (Zhu et al., 2019). Deep learning (DL) models are good candidates for AL because of their
high-dimensional data processing and automatic feature extraction capability. Existing models mainly
target at improving uncertainty quantification of the network for reliable sampling (Wang et al., 2016;
Gal & Ghahramani, 2015; 2016; Kendall et al., 2015; Leibig et al., 2017). Batch-mode sampling
is commonly used in active DL to avoid frequent model re-training. It focuses on constructing
representative batches to avoid redundant information given by similar instances (Kirsch et al., 2019;
Sener & Savarese, 2017; Ash et al., 2019). AL in the MIL setting has been rarely investigated. One
exception is the MI logistic model and its three uncertainty measures to simultaneously consider
both instance and bag level uncertainty (Settles et al., 2007). However, uncertainty sampling is
ineffective to explore challenging bags, where all instances are confidently predicted as negative.
In addition, the original model is a simple linear model, which does not provide sufficient capacity
for high-dimensional data. There is no systematic way to support batch-mode sampling, either. An
AL framework is developed for MIL tasks in (Yuan et al., 2021), however, sampling is conducted
at the bag level (i.e., choosing bags instead of instances). Thus, it is essentially a multi-label AL
model, aiming to improve the bag-level predictions with fewer annotated bags. This is fundamentally
different from the design goal of ADMIL.
3	Active Deep Multiple Instance Learning
Let {x1, ..., xn} denote a set of instances associated with each bag B, where each xi ∈ RD is a
D-dimensional feature vector. Let tB ∈ {+1, -1} indicate the bag type. All symbols are summarized
by Table 1 in Appendix A. Following the standard MIL assumption discussed earlier, active sampling
will focus on instances in the positive bags as all instances in a negative bag are negative. We also
allow the number of instances to vary from one bag to another.
3.1	Variance Regularization for Effective MI Active Sampling
Let xi+ (or xj-) be the ith (or jth) instance in a positive bag Bpos (or a negative bag Bneg). Following
the MIL assumption, a commonly used loss function for training deep MIL models is to make the
maximum prediction score of instances from a positive bag to be higher than a negative bag (Sultani
et al., 2018):
LMS(Bpos,Bneg) = 1 - i∈mBax [f(xi+;w)] + j∈mBax [f(xj-;w)]	(1)
where f(x; w) ∈ [0, 1] is the prediction score of instance x provided by a deep neural network
parameterized by w and [a]+ = max{0, a}. We omit w from f (x; w) in its future reference to
keep the notation uncluttered. The above objective function aims to maximize the gap between the
maximum prediction score of instances from a positive bag and maximum score from a negative bag.
Model training can be performed by sampling pairs of positive and negative bags (Bpos , Bneg), using
their bag-level labels to evaluate the loss, and performing back-propagation. The maximum score
based MIL (referred to as MS-MIL) models are designed primarily for bag label prediction as it aims
3
Under review as a conference paper at ICLR 2022
to identify a single most positive instance from a positive bag and maximizes its prediction score. In
this way, it fully leverages the MIL assumption (i.e., at least one positive instance in Bpos) and the
weakly supervised signal (i.e., bag-level label).
As discussed earlier, MS-MIL and it top-k extensions suffers from key limitations that impact their
instance-level prediction performance. Meanwhile, they provide inadequate support to sample the
most informative instances to enhance the instance predictions. Inspired by the recent advances in
learning theory to automatically balance bias and variance in risk minimization (Duchi & Namkoong,
2019), we propose a novel variance regularized MIL loss function to capture the inherent character-
istics of MIL, aiming to collectively address highly imbalanced instance distribution, existence of
outliers, and multimodal scenarios. As a result, minimizing the new MIL loss can effectively improve
the prediction scores of the positive instances, making them easier to be sampled for annotation by
the proposed sampling function. In particular, the variance regularized loss introduces two novel
changes to (1), which are formalized below:
LVAR(Bpos, Bneg ) = [l - I1 XL f (x+) + C {Varnf(X +)]] + max [f(x-)]]
n	n	j ∈Bneg
i=1	+
(2)
where ∀i ∈ [1, n], xi+ ∈ Bpos, n is the size of Bpos, Varn is the empirical variance of f(X+) with
X + being a random variable representing an instance from a positive bag, and C is a hyperparameter
to balance the mean score and the variance.
The first key change is to use the mean score to replace the maximum score in (1), which avoids the
model to only focus on the most positive instance in a bag to make it robust to outliers and multimodal
scenarios. Since positive bags are guaranteed to include positive instances and instances in a negative
bag are all negative, it is desirable that the mean score for a positive bag should be high. Maximizing
the mean score in a positive bag using a complex model (e.g., a deep neural network) could effectively
reduce the training loss (by reducing the bias) in estimating the bag-level labels. However, using
the mean score alone is problematic as most instances in a positive bag are usually negative in a
typical MIL setting. As a result, such a low bias model will lead to a very high false positive rate,
which negatively impacts the overall instance-level prediction. The proposed loss function addresses
this issue through the novel variance term, which effectively handles the highly imbalanced instance
distribution. With only a small number of instances being truly positive, the empirical variance Varn
for the bag should be high due to the large deviation of a small number of high scores from the
majority of low scores. It is worth to note that the variance term in (2) plays a distinct role than risk
minimization in standard supervised learning, where it is minimized to control the estimation error.
In contrast, the variance in (2) is encouraged to be large to allow a small set of instances in a bag to be
positive, aiming to precisely capture the imbalanced distribution. To our best knowledge, this is the
first bias-variance formulation in the MIL setting. Conducting MI-AL using variance regularization
still faces two remaining challenges. First, its effectiveness hinges on an optimal balance between the
mean score and the empirical variance, which is controlled by the hyperparameter C. Similar to the
standard supervised learning, there lacks a systematic way of setting such a hyperparameter to achieve
an optimal trade-off. Second, the variance term is non-convex with multiple local minima (Duchi &
Namkoong, 2019), which makes model training much more difficult and time-consuming. Thus, it is
not suitable for real-time interactions to support active sampling.
3.2	Distributionally Robust Bag Likelihood
To address the challenges as outlined above, we propose to formulate a distributionally robust
bag level likelihood (DRBL) as a convex surrogate of the variance regularized loss in (2). By
extending the distributionally robust optimization framework developed for risk minimization in
supervised learning (Namkoong & Duchi, 2017; Duchi & Namkoong, 2019), we theoretically prove
the equivalence between DRBL and variance regularization with high probability. Being convex,
DRBL is easier to optimize that facilitates MIL model training to support fast active sampling.
Furthermore, by setting a proper uncertainty set as introduced next, we show that the parameter C is
directly obtained when optimizing the robust bag likelihood, where the instance distribution in the
bag is constrained by the uncertainty set. As a result, it achieves automatic trade-off between the
mean prediction score and the variance.
We first introduce a probability vector p = (p1, ...,pn)>, where Pi pi = 1, pi ≥ 0, ∀i ∈ {1, ..., n}
and let pi denote the probability that instance xi+ ∈ Bpos can represent the bag. We further introduce
4
Under review as a conference paper at ICLR 2022
a binary indicator vector z = (z1, ..., zn)>, where p(zi = 1) = pi. Let Y be a binary random
variable that denotes the bag label. Conditioning on all the instances in the bag, the (conditional) bag
likelihood for bag Bpos is given by p(Y = 1|z,f) = Qi f(xi+)zi, where f = (f(x1+), ...,f(xn+))>.
By integrating out the indicator variables, we have the marginal bag likelihood as p(Y = 1|p, f) =
Pi pif (xi+). Instead of letting a single most positive instance to determine the bag label, where
p(y = 1|p, f) = f(xk+) with k = arg maxi f(xi+), which is equivalent to MS-MIL, or assigning
equal probability to each instance (i.e., pi = 1/n), which is equivalent to the mean score, we introduce
an uncertainty set Pn that allows p to deviate from a uniform distribution to some extent:
Pn = {p ∈ Rn, p>l = 1,0 ≤ p,Df (p|| ；) ≤ λj	(3)
where Df (p||q) is the f -divergence between two distributions P and q, 1 is a n-dimensional
unit vector, and λ controls the extent that p can deviate from a uniform vector, which essentially
corresponds to the imbalanced instance distribution in the bag. Note that Pn only specifies a
neighborhood that p may deviate from a uniform distribution. Since Pn is a convex set, an optimal
p can be easily computed for each specific bag by optimizing the robust bag likelihood according
to its specific imbalanced instance distribution. This is fundamentally more advantageous than a
top-k approach, where k is discrete and hard to optimize. Next, we show that the optimal robust
bag likelihood is equivalent to the variance regularized mean prediction score with high probability,
which allows us to define a new MIL loss based on DRBL.
Theorem 1. Let X+ be a random variable representing an instance from a positive bag, f(X+) ∈
[0, 1] is the score assigned to an instance, σ2 = Var[f (X +)] and Varn[f(X+)] denote the population
and sample variance of f(X+), respectively, and Df takes the form of χ2-divergence. For a fixed λ
and with n ≥ max(2,福 max(8σ, 44)), we have
n
pm∈aPxn	pif(xi+)
i=1
n XX f (χ+) + J
i=1
λVarn [f(X+)]
n
(4)
with probability at least 1 一 exp (— 7n2§2-), where Pn is an uncertainty set defined by (3).
It is worth to note that given the highly imbalanced positive instances in a typical MIL setting, the
true variance σ 2 should be high. For a bag with a decent size, it guarantees the equivalence in (4)
with high probability. Furthermore, maximizing the robust bag likelihood given on the l.h.s. of
(4) assigns C = √λ, which automatically adjusts the impact of variance based on the uncertainty
set. In Theorem 1, Df takes the form of χ2-divergence between two distributions. Theorem 2
in Appendix C further generalizes this result to the KL-divergence. Detailed proofs are given in
Appendix C. Leveraging the key result from Theorem 1, we formulate a DRBL-based MIL loss,
LDRBL (Bpos, Bneg) =	1 一 max
p∈Pn
n
X	pif(xi+ )
i=1
+ j∈mBanxeg f(xj-)
(5)
The DRBL loss offers a very intuitive interpretation on the newly introduced probability vector p.
Since it can deviate from the uniform distribution as specified by the uncertainty set Pn , each entry
pi essentially corresponds to the contribution (or weight) of xi+ to the bag likelihood (being positive).
As a result, to maximize the robust bag likelihood, instances with a higher prediction score should
receive a higher weight. Meanwhile, constrained by Pn , multiple instances will contribute to the
bag likelihood with a sizable weight as p cannot deviate too much from being uniform. Hence, their
prediction scores will simultaneously be brought up by the model. This makes DRBL robust to the
outlier and multimodal cases as it increases the chance for the true positive instances or multiple
types of true positive instances to be assigned a high prediction score. This provides fundamental
support to the proposed P-F active sampling function that combines the probability vector p and the
prediction score f in a novel way to choose the most informative instances in a bag for annotation.
3.3	P-F Active Sampling
Since we have the prediction score f(xi+) ∈ [0, 1], it can be naturally interpreted as the probability of
instance xi+ being positive. As a result, a straightforward way to perform uncertainty based instance
sampling is to compute the f -score based entropy of the instances, referred to F-Entropy:
χ* = arg max Hf(X+)],H[f (X+)] = -[f (x+)logf (X+) + (1—f (X+))lOg(I -f (X+))]⑹
i∈Bpos
5
Under review as a conference paper at ICLR 2022
Since the sampled instance has the largest prediction uncertainty (according to F-Entropy), labeling
such an instance can effectively improve the model’s instance-level performance. Active sampling
using (6) is straightforward, which involves evaluating H[f(x+)] for all the instances from positive
training bags (note that all the instances in a negative bag are negative). Since we consider a deep
learning model to better accommodate high-dimensional data, sampling one instance at a time
requires frequent model training, which is computationally expensive. Instead, we sample a small
batch of instances in each step based on their predicted F-Entropy. It is worth to note that, due to
the highly imbalanced instance distribution, the majority of the prediction scores, including many
positive instances, may be very low. The goal is to assign a relatively higher score to the potentially
positive instances so that their entropy is not too low, indicating a confident negative prediction,
which will be missed by the sampling function.
As discussed earlier, using the robust bag likelihood as the MIL loss can directly benefit instance
sampling by increasing the chance to assign a higher prediction score to a positive instance so that it
is more likely to be sampled. However, F-Entropy sampling still suffers from two major limitations.
First, for some very difficult bags, such as the sample image shown in Figure 1 (a), identifying
the positive instances (e.g., the patch in the image containing the shadow of a bird) can be highly
challenging. As a result, they may be assigned a very low f score. In fact, as shown in Figure 1
(c), all the instances in this bag receive a very low score with the highest less than 0.01, leading to
a very low entropy. Some additional examples of challenging bags from the 20NewsGroup dataset
are shown in Figure 6 of Appendix B, where all the instances are predicted with a very low score.
Hence, all these instances are predicted as negative with low uncertainty, making them less likely to
be chosen by entropy based sampling. Second, since batch-mode sampling is adopted to reduce the
training cost of a deep network, it is essential to diversify the selected instances in the same batch
to minimize the annotation cost. However, choosing data instances solely based on their predicted
entropy may lead to the annotation of similar instances, which is not cost-effective.
The proposed P-F active sampling overcomes the above two limitations simultaneously through
effective bag exploration by combining the probability vector p and the prediction score f through
a min max function according to their distinct roles in a bag. The key design rationale of P-F
sampling is rooted in the standard MIL assumption that ensures at least one positive instance in each
positive bag to guide effective bag exploration. Both p’s and f ’s along with the bag structure are
dynamically updated during bag exploration to increase the chance of sampling the positive instances
in an under-explored bag. A hybrid loss function further utilizes labels of sampled negative instances
in the same bag to boost the prediction scores of the positive instances. More specifically, let B be
the total number of positive training bags, P-F sampling will choose the following data instance:
XPF = arg	min	f(x+,),
b∈{1,...,B}
and b* = arg max Pb
(7)
where Pb is the probability vector of bag b. For each bag, the sampling function first identifies the
instance xp+ with the largest p value in each bag. Such an instance can be regarded as the most
representative instance in the bag as it makes the largest contribution (according to Pb) to the bag
likelihood. According to the prediction score of x+ , We can categorize bags into three groups: (1)
easy bags, where f (xb+ ) takes a large value, indicating that the model makes confidently correct
predictions, (2) confusing bags, Where f (xb+ ) is reasonably large but uncertain, indicating the model
is still confusing about its prediction, and (3) difficult bags, Where f (xb+ ) is very loW, indicating
the model makes confidently Wrong predictions. It is desirable to sample from both confusing and
difficult bags as the model already makes accurate instance predictions for easy bags. Sampling
instances from the confusing bags can be achieved through the proposed F-Entropy as the model
makes uncertain predictions, Which leads to a high entropy. Finally, sampling from the difficult
bags are fundamentally more challenging due to loW prediction scores for the entire bag. HoWever,
the MIL assumption provides a general direction for bag-level exploration of positive instances as
there must be at least one positive instance in each positive bag. The P-F sampling function in (7)
chooses the representative instance from the bag With the loWest prediction score. Such an instance is
guaranteed to be sampled from an under-explored (i.e., difficult) bag as it has the loWest prediction
score despite being predicted as the most positive instance in the bag.
Extension to the batch-mode sampling is conducted in tWo directions, Within bag and across bags, for
more effective exploration While ensuring diversity of the sampled instances. First, instead of only
sampling the most positive instance from the identified under-explored bag, We propose to sample
6
Under review as a conference paper at ICLR 2022
k > 1 instances as the positive instances may be ranked lower than multiple negative instances in the
bag according to the current prediction scores (see Figure 1 (c) for an example). This helps to more
effectively explore very difficult bags. To ensure diversity among the sampled instances, we keep k
small but sample across multiple bags simultaneously. Only bags with a max prediction score f (xb+ )
less than a threshold (0.3 is used in our experiments) will be explored as these represent the difficult
bags as discussed above. For bags with a larger f (xb+ ), they are either easy bags or confusing bags
that can be effectively sampled using F-Entropy. Our overall P-F sampling function integrates bag
exploration and F-Entropy and gives priority to the former to perform diversity-aware bag exploration
first. As more bags are successfully explored along with MI-AL, less instances will be sampled by
exploration and the focus will be naturally shifted to F-Entropy to perform model fine-tuning. The
detailed sampling process is summarized by Algorithm 1 in Appendix D.
Similar to AL in standard supervised learning, the sampled annotated instances should be used to
improve the model prediction performance. However, the MIL loss primarily focuses on the bag-level
labels due to the lack of instance labels. To this end, we propose a hybrid loss function that integrates
the bag and instance labels. Let Xl = {xl1, xl2, ..., xlm} be the m labeled instances queried by the
proposed active learning function and tl = {tl1, tl2, ..., tlm} with tli ∈ {0, 1} be the corresponding
instance labels. We formulate a supervised binary cross-entropy (BCE) loss as
m
LBCE (χl, tl) = - mm X [ti iog(f (χi)) + (1- ti) iog(i - f (χi))]	⑻
i=1
It is clear that the sampled positive instances provide important supervised signals so that the model
will predict a high score for similar positive instances, which will directly benefit instance-level
prediction. In contrast, the sampled negative instances, especially those chosen from the under-
explored bags, contribute less to improve the prediction performance as their original prediction
scores are already low. However, they play a subtle but essential role to achieve more effective
bag-level exploration. First, if a sampled instance is labeled as negative, it will be removed from the
bag, which does not violate the MIL assumption. Meanwhile, since we have Pipi = 1, the p values
will be redistributed and the chance for each remaining instance to be sampled is therefore increased.
Furthermore, the BCE loss will further bring down the prediction scores of negative instances that are
similar to the sampled one. This may help to improve the score of the positive instance so that it can
have a higher chance to be sampled in the future. Finally, the hybrid loss that combines the MIL loss
and the supervised loss is used to retrain the model after a new batch of instances are queried:
LHybrid(Bpos, Bneg) = LDRBL(Bpos, Bneg) + βLBCE(Xl, tl)	(9)
where β is used to trade-off bag-level and instance-level losses.
4	Experiments
We conduct extensive experimentation over multiple real-world MIL datasets to justify the effec-
tiveness of the proposed ADMIL model. The purpose of our experiments is to demonstrate: (i) the
state-of-the-art instance prediction performance of ADMIL by comparing with existing competitive
baselines, (ii) effectiveness of the proposed P-F active sampling function through comparison with
other sampling mechanisms, (iii) impact of key model parameters through a detailed ablation study,
and (iv) qualitative evaluation through concrete examples to provide deeper and intuitive insights on
the working rationale of the proposed model.
Datasets. Our experiments involve four datasets covering both textual and image data: 20New-
Group (Zhou et al., 2009), Cifar10 (Krizhevsky, 2009), Cifar100 (Krizhevsky, 2009), and Pascal
VOC (Everingham et al., 2015). For 20NewsGroup, the dataset is already available in the MIL setting,
which consists of 20 topics where each topic contains 50 positive and 50 negative bags. For Cifar10
and Cifar100 datasets, bags are constructed by treating each image as an instance. For Cifar10,
images corresponding to digits ‘1’, ‘2’, and ‘5’ are regarded as a positive instance otherwise negative.
In case of Cifar100, images in superclass 2 are treated as positive and the rest as negative. In Pascal
VOC, we perform image segmentation so each image is regarded as a bag and corresponding patches
cropped from the image are treated as instances. In our experiments, images containing birds as a
positive bags and others as negative. Table 2 that summarizes the bag statistics and additional details
for all datasets are provided in Appendix E.
7
Under review as a conference paper at ICLR 2022
(a) 20NeWSGrouP
Figure 2: MI-AL performance comparison
50.0
47.5
⅛ 45.0
Ξ
42.5
40.0
・91
SI
S
I
76.5-
76.0-
H 5
.
■
7,
50
48
< 46
44
42
-∙-' ADMIL-P-F
ADMIL-F-EntroPy
—ADMIL-Random
(a) 20NewsGroup	(b) Cifar10	(c) Cifar100	(d) Pascal VOC
Figure 3: EffectiveneSS of P-F active SamPling
Evaluation metric and model training. To aSSeSS the model Performance, We rePort the inStance-
level mean average PreciSion (mAP) Score, Which SummarizeS a PreciSion-recall curve aS a Weighted
mean of PreciSion achieved at each threShold, With the increaSe in recall from the PreviouS threShold
aS the Weight. mAP exPlicitly PlaceS much Stronger emPhaSiS on the correctneSS of the feW toP
ranked inStanceS than other metricS (e.g., AUC) (Su et al., 2015). ThiS makeS it Particularly Suitable
for inStance Prediction evaluation aS a Small SubSet of inStanceS With the higheSt Prediction ScoreS
Will eventually be identified aS PoSitive for further inSPection (by human exPertS) With the reSt being
ignored. For Cifar10, Cifar100, and PaScal VOC dataSetS, We extract the viSual featureS from the
Second-to-the laSt layer of a VGG16 netWork Pre-trained uSing the imagenet dataSet, yielding a
4,096 dimenSional feature vector for each inStance. For 20NeWSGrouP, We uSe the available 200-
dimenSional feature vector. In termS of netWork architecture, We uSe a 3-layer FC neural netWork.
The firSt layer haS 32 unitS folloWed by 16 unitS and 1 unit FC layerS. We adoPt 60% droPout betWeen
FC layerS. ReLU and Sigmoid activationS are uSed for the firSt and laSt FC layerS. More detailS
regarding the learning rate and hyPerParameter Setting are PreSented in the APPendix.
Performance comparison. To demonStrate the inStance Prediction Performance achieved by the
ProPoSed ADMIL model, We comPare it With comPetitive baSelineS. FirSt, the tWo MI-AL SamPling
StrategieS: MIAL-Uncertainty and MIAL-MIU (SettleS et al., 2007), from the MI logiStic model are
included. Since our dataSetS involve high-dimenSional data, We rePlace the original linear model by the
exact DNN model uSed in our ADMIL So We can focuS on comParing MI active SamPling. The EGL
SamPling technique in (SettleS et al., 2007) WaS not included due to the Prohibitive comPutational coSt
to evaluate the gradient of each inStance outPut With reSPect to the large number of DNN ParameterS.
We alSo imPlement an MS-MIL model and itS toP-k variant With uncertainty SamPling uSing entroPy.
Given the different SizeS of the dataSetS, We query maximum 15 inStanceS Per SteP in 20NeWSGrouP,
30 inStanceS in PaScal VOC, and 150 inStanceS in Cifar10 and Cifar100. Figure 2 ShoWS the MI-AL
curveS for all four dataSetS. ADMIL achieveS the beSt Performance in all caSeS. For moSt dataSetS, it
ShoWS a much better initial Performance, Which reSultS from the ProPoSed DRBL-baSed MIL loSS
that Significantly benefitS MIL Performance in PaSSive learning. Overall the entire MI-AL ProceSS,
ADMIL conSiStently StayS the beSt and convergeS to a higher Point in the end for all dataSetS. For
the PaScal VOC, the toP-k MIL model With entroPy SamPling achieveS cloSer Performance toWardS
the end, Which iS mainly due to the limited PoSitive inStanceS in thiS dataSet. Hence, no teSting bagS
contain Similar PoSitive inStanceS in the challenging bagS that are exPlored by P-F SamPling. While
ADMIL achieveS much better inStance PredictionS in thoSe bagS, the advantage doeS not tranSfer to
the teSting bagS. Our qualitative Study Will Provide a more detailed analySiS on thiS.
Effectiveness of active sampling. To demonStrate the effectiveneSS of the ProPoSed P-F active
SamPling function, We comPare it With tWo other SamPling methodS, F-EntroPy and random SamPling,
While keePing all other PartS of the model the Same. AS ShoWn in Figure 3, P-F SamPling clearly
outPerformS otherS With a large margin in the firSt three dataSetS. It advantage over F-EntroPy iS
Smaller on PaScal VOC due the Same reaSon aS exPlained above. The Performance gain iS mainly
attributed to the effective exPloration of P-F SamPling over the moSt challenging bagS.
8
Under review as a conference paper at ICLR 2022
(b) β = 1
SteP
λ=1，λ = 1.0
λ=0.01
→- λ=10-10
(C) λ = 0.01
(d) β =1
Figure 4: Impact of key model parameters: (a-b) Pascal VOC; (c-d) Cifar100
(a) Sample bag B2
(b) Sample bag B3
Bag	P-F	F-Entropy
BBΓ	~L0O~	0.04
-BF	0.53	0.07
-BF	~0.64~	0.55	一
~BT^	Shadow of a bird	
~B2-	Side view of a bird	
~B3-	Part of a bird	
(c) mAP scores
(d) Percentage of TP bags
Figure 5: (a-b) Poorly explored bags in Pascal VOC; (b) Description of these bags and their mAP
scores; (d) Additional true positive bags successfully explored by P-F sampling
Ablation Study. Figure 4 demonstrates the impact of λ and β . In particular, λ can be set according to
the imbalanced instance distribution within bags, where a larger λ corresponds to a higher imbalanced
distribution. We vary λ in [10-10, 1] and since most bags in the MIL setting are highly imbalanced,
λ ∈ [10-06, 10-02] gives very good performance in general. Figures 4 (b) and (d) show that λ = 0.01
clearly outperforms too large (or small) λ values. As for β, placing less emphasis on an instance
level loss (small β), we may not fully leverage labels of queried instances. Meanwhile, with too
much emphasis on the instance level loss (large β), the model overly focuses on the limited queried
instances with less attention to the bag labels. Therefore, a good balance (with β = 1) results in an
optimal performance, shown in (a) and (c). More ablation study results are provided in Appendix E.
Qualitative analysis. To further justify why the proposed ADMIL model and its P-F sampling
function work better than other competitive baselines, we provide a few illustrative examples to offer
some deeper insights on its good performance. First, we show two additional challenging bags in
addition to the one shown in Figure 1 (a). In particular, as shown in Figure 5 (a-b), B2 presents a
side view of a bird while only a small portion of the bird is visible in B3 . For those difficult cases,
the model originally predicts all instances as a negative with high confidence. However, by coupling
the P-F sampling and the hybrid loss in (9), the positive instances from those bags are successfully
queried. Figure 5 (c) shows a clear advantage in the mAP scores between P-F sampling and F-Entropy.
As a further evidence, we investigate the number of true positive (TP) bags being explored by both
P-F sampling and F-Entropy. TP bags refer to those that the model is being able to query at least one
true positive instance. Instead of reporting the actual number of bags, which is affected by the size of
the dataset, we show the additional percentage TP bags being explored by P-F sampling in Figure 5
(d). It is worth to note that neither method tries to query the easy bags as their positive instances are
correctly predicted with high confidence. The major difference is from the challenging bags and the
percentage of these bags varies among different datasets. Nevertheless, P-F sampling consistently
explores more effectively than F-Entropy across all datasets.
5 Conclusions
To tackle the low instance-level prediction performance of existing MIL models that is essential for
many critical applications, we develop a novel MI-AL model to sample a small number of most
informative instances, especially those from confusing and challenging bags, to enhance the instance-
level prediction while keeping a low annotation cost. We propose to optimize a robust bag likelihood
as a convex surrogate of a variance regularized MIL loss to identify a subset of potentially positive
instances. Active sampling is conducted by properly balancing between exploring the challenging
bags (through P-F sampling) and refining the model by sampling the most confusing instances
(through F-Entropy). The design of the loss function naturally supports mini-batch training, which
coupled with the batch-mode sampling, makes the MI-AL model work seamlessly with a deep neural
network to support broader MIL applications that involve high-dimensional data. Our extensive
experiments conducted on multiple MIL datasets show clear advantage over existing baselines.
9
Under review as a conference paper at ICLR 2022
References
Stuart Andrews, Ioannis Tsochantaridis, and Thomas Hofmann. Support vector machines for multiple-
instance learning. In NIPS, 2002.
Jordan T Ash, Chicheng Zhang, Akshay Krishnamurthy, John Langford, and Alekh Agarwal. Deep
batch active learning by diverse, uncertain gradient lower bounds. arXiv preprint arXiv:1906.03671,
2019.
Marc-Andre Carbonneau, V. Cheplygina, Eric Granger, and Ghyslain Gagnon. Multiple instance
learning: A survey of problem characteristics and applications. Pattern Recognit., 77:329-353,
2018.
Aron Culotta and Andrew McCallum. Reducing labeling effort for structured prediction tasks. In
AAAI, volume 5, pp. 746-751, 2005.
Thomas Deselaers and Vittorio Ferrari. A conditional random field for multiple-instance learning.
In Proceedings of the 27th International Conference on International Conference on Machine
Learning, ICML’10, pp. 287-294, Madison, WI, USA, 2010. Omnipress. ISBN 9781605589077.
Thomas G. Dietterich, R. Lathrop, and Tomas Lozano-Perez. Solving the multiple instance problem
with axis-parallel rectangles. Artif. Intell., 89:31-71, 1997.
John Duchi and Hongseok Namkoong. Variance-based regularization with convex objectives. Journal
of Machine Learning Research, 20(68):1-55, 2019. URL http://jmlr.org/papers/v20/
17-750.html.
M. Everingham, S. M. A. Eslami, L. Van Gool, C. K. I. Williams, J. Winn, and A. Zisserman. The
pascal visual object classes challenge: A retrospective. International Journal of Computer Vision,
111(1):98-136, January 2015.
Yarin Gal and Zoubin Ghahramani. Bayesian convolutional neural networks with bernoulli approxi-
mate variational inference. arXiv preprint arXiv:1506.02158, 2015.
Yarin Gal and Zoubin Ghahramani. Dropout as a bayesian approximation: Representing model
uncertainty in deep learning. In international conference on machine learning, pp. 1050-1059.
PMLR, 2016.
Manuel HauBmann, Fred A. Hamprecht, and Melih Kandemir. Variational bayesian multiple instance
learning with gaussian processes. In 2017 IEEE Conference on Computer Vision and Pattern
Recognition (CVPR), pp. 810-819, 2017. doi: 10.1109/CVPR.2017.93.
Alex Holub, Pietro Perona, and Michael C Burl. Entropy-based active learning for object recogni-
tion. In 2008 IEEE Computer Society Conference on Computer Vision and Pattern Recognition
Workshops, pp. 1-8. IEEE, 2008.
Maximilian Ilse, Jakub M. Tomczak, and Max Welling. Attention-based deep multiple instance
learning. In ICML, 2018.
Ajay J Joshi, Fatih Porikli, and Nikolaos Papanikolopoulos. Multi-class active learning for image
classification. In CVPR, pp. 2372-2379. IEEE, 2009.
Alex Kendall, Vijay Badrinarayanan, and Roberto Cipolla. Bayesian segnet: Model uncertainty
in deep convolutional encoder-decoder architectures for scene understanding. arXiv preprint
arXiv:1511.02680, 2015.
Minyoung Kim and F. Torre. Gaussian processes multiple instance learning. In ICML, 2010.
Andreas Kirsch, Joost Van Amersfoort, and Yarin Gal. Batchbald: Efficient and diverse batch
acquisition for deep bayesian active learning. arXiv preprint arXiv:1906.08158, 2019.
A. Krizhevsky. Learning multiple layers of features from tiny images. University of Toronto, 2009.
Henry Lam. Robust sensitivity analysis for stochastic systems. Math. Oper. Res., 41:1248-1275,
2016.
10
Under review as a conference paper at ICLR 2022
Christian Leibig, Vaneeda Allken, Murat Seckin Ayhan, PhiliPP Berens, and Siegfried Wahl. Lever-
aging uncertainty information from deep neural networks for disease detection. Scientific reports,
7(1):1-14, 2017.
Weixin Li and N. Vasconcelos. MultiPle instance learning for soft bags via toP instances. 2015 IEEE
Conference on Computer Vision and Pattern Recognition (CVPR), PP. 4277-4285, 2015a.
Weixin Li and Nuno Vasconcelos. MultiPle instance learning for soft bags via toP instances. In 2015
IEEE Conference on Computer Vision and Pattern Recognition (CVPR), PP. 4277-4285, 2015b.
doi: 10.1109/CVPR.2015.7299056.
Hongseok Namkoong and John C Duchi. Variance-based regularization with convex objectives. In
I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Gar-
nett (eds.), Advances in Neural Information Processing Systems, volume 30. Curran Asso-
ciates, Inc., 2017. URL https://proceedings.neurips.cc/paper/2017/file/
5a142a55461d5fef016acfb927fee0bd- Paper.pdf.
I.R. Petersen, M.R. James, and P. DuPuis. Minimax oPtimal control of stochastic uncertain systems
with relative entroPy constraints. IEEE Transactions on Automatic Control, 45(3):398-412, 2000.
doi: 10.1109/9.847720.
Suju Rajan, JoydeeP Ghosh, and Melba M Crawford. An active learning aPProach to hyPersPectral
data classification. IEEE Transactions on Geoscience and Remote Sensing, 46(4):1231-1242,
2008.
Nicholas Roy and Andrew McCallum. Toward oPtimal active learning through monte carlo estimation
of error reduction. ICML, Williamstown, PP. 441-448, 2001.
Ozan Sener and Silvio Savarese. Active learning for convolutional neural networks: A core-set
aPProach. arXiv preprint arXiv:1708.00489, 2017.
Burr Settles, Mark Craven, and Soumya Ray. MultiPle-instance active learning. Advances in neural
information processing systems, 20:1289-1296, 2007.
Burr Settles, Mark Craven, and Soumya Ray. MultiPle-instance active learning. In J. Platt,
D. Koller, Y. Singer, and S. Roweis (eds.), Advances in Neural Information Processing Sys-
tems, volume 20. Curran Associates, Inc., 2008. URL https://proceedings.neurips.
cc/paper/2007/file/a1519de5b5d44b31a01de013b9b51a80-Paper.pdf.
Wanhua Su, Yan Yuan, and Mu Zhu. A relationshiP between the average Precision and the area under
the roc curve. In Proceedings of the 2015 International Conference on The Theory of Information
Retrieval, PP. 349-352, 2015.
Waqas Sultani, Chen Chen, and Mubarak Shah. Real-world anomaly detection in surveillance videos.
In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR),
June 2018.
Simon Tong and DaPhne Koller. SuPPort vector machine active learning with aPPlications to text
classification. Journal of machine learning research, 2(Nov):45-66, 2001.
Keze Wang, Dongyu Zhang, Ya Li, Ruimao Zhang, and Liang Lin. Cost-effective active learning for
deeP image classification. IEEE Transactions on Circuits and Systems for Video Technology, 27
(12):2591-2600, 2016.
Xin Xu and Eibe Frank. Logistic regression and boosting for labeled bags of instances. In Advances in
Knowledge Discovery andData Mining, volume 3056,08 2004. doi: 10.1007/978-3-540-24775-3.
35.
Tianning Yuan, Fang Wan, Mengying Fu, Jianzhuang Liu, Songcen Xu, Xiangyang Ji, and Qixiang
Ye. MultiPle instance active learning for object detection. arXiv preprint arXiv:2104.02324, 2021.
Zhi-Hua Zhou, Yu-Yin Sun, and Yu-Feng Li. Multi-instance learning by treating instances as non-i.i.d.
samPles. In Proceedings of the 26th Annual International Conference on Machine Learning, ICML
’09, PP. 1249-1256, New York, NY, USA, 2009. Association for ComPuting Machinery. ISBN
9781605585161.
11
Under review as a conference paper at ICLR 2022
Dixian Zhu, Zhe Li, Xiaoyu Wang, Boqing Gong, and Tianbao Yang. A robust zero-sum game
framework for pool-based active learning. In Kamalika Chaudhuri and Masashi Sugiyama (eds.),
Proceedings of the Twenty-Second International Conference on Artificial Intelligence and Statistics,
volume 89 of Proceedings of Machine Learning Research ,pp. 517-526. PMLR, 16-18 APr 2019.
URL http://proceedings.mlr.press/v89/zhu19a.html.
12
Under review as a conference paper at ICLR 2022
Appendix
Organization. In this appendix, we first summarize the major notations in Table 1. We then provide
more detailed explanation on some challenging bags using the 20NewsGroup dataset as an example
in Appendix B. We extend the main theoretical result given in Theorem 1 to another important type
of f -divergence (i.e., KL divergence) and summarize this additional key theoretical result in Theorem
2 in Appendix C. We then provide the detailed proofs for both Theorems. We present the detailed
P-F sampling algorithm in Appendix D. Additional experimental results are shown in Appendix E.
Finally, we provide the link to the source code in Appendix F.
A Summary of Notations
Table 1: Notations with Descriptions
Notation	Description
X	Set of training bag instances
Y	Binary random variable denoting bag labels
Xi	ith instance present in a bag B
X+	ithinstancepresentiπapositivebag∏Pos
X-	jth instance present in a negative bag Bneg
H	Entropy
t	Binary label of the lth labeled instance
tB	Binary value indicating bag type
n	Total number of instances in a bag B
Bpos	A positive bags in the training set
Bneg	A negative bags in the training set
W	Network Parameters
Vrn	Empirical Variance
σ2	Population Variance
Pn	Uncertainty set
P	n-dimensional vector weights associated with each bag instances
Df	f-divergence indicating the distance between two distributions
f	Functional mapping of the Network parameterized by W
N	Total number of Instances in a training set
i	n-dimensional unit vector
λ	Radius of a ball in DRO framework
Z	n-dimensional binary indicator variable
X+	Random variable corresponding to an instance from a positive bag
B	More Examples of Challenging Bags
Figure 6 shows the p-f plots for three example challenging bags from three different topics in the
20NewsGroup dataset. As shown, the highest f -score from those bags is very low. This implies
that the passive learning model predicts all the instances as negative with a high confidence. Using
F-Entropy, we may not be able to query any instance from those bags because of low uncertainty.
In contrast, by leveraging the standard MIL assumption, the proposed P-F sampling will effectively
explore those bags. Once the positive instances from these bags are queried, they help to accurately
identify similar positive instances in the same and different bags to boost the instance prediction
performance, as evidenced by our experimental results.
C Extension to KL Divergence and Proofs of Theorems
In Theorem 1, we show that the optimal robust bag likelihood is equivalent to the variance regularized
mean prediction score with a high probability by using the χ2-divergence to quantify the deviation
of probability vector p from the empirical (uniform) distribution. In this section, we extend this
13
Under review as a conference paper at ICLR 2022
0.035
0.030
0.025
0.020
f score
Pos Instance
Negative Instance
3 2
O O
■ ■
O O
n-e> d
99I°
gzl°
°8I0
e
£
->

°Z「0
(a) comp.graphics B-8	(b) rec.motorcycles B-2	(c) talk.politics.misc B-13
Figure 6: Example of challenging bags from different topics in 20NewsGroup
theoretical equivalence to KL-divergence and present the result in Theorem 2. We then provide the
detailed proofs for both theorems.
Theorem 2. Let X+ be a random variable representing an instance from a positive bag, f (X+) ∈
[0, 1] is the score assigned to an instance, σ2 = Var[f (X +)] and Varn[f(X+)] denote the population
and sample variance of f (X+), respectively, and Df takes the form of KL-divergence. We have
+ 十、1 ff +	2λVarn[f (X +)]	λ
p∈ρn∑ Pif(X+) = nEf(χ+) + V------------------n-------+ eUJ
i=1	i=1	(10)
s.t. Pn := {p ∈ Rn, p>l = 1, 0 ≤ P,Dkl (p∣∣Po) ≤ λ}
where po = n is the uniform distribution indicating the center of the ball, e (λ) = -λn V 3 ffχX+))] +
O ((n)3/2) with κ3 = Eo[(f (X+) 一 E0[f (X +)])3] and Eo denotes the expectation taken over po.
Remark: Given a bag with a decent size n 1 and since λ is usually set to λ 1 (0.01 is used
in our experiments), We have e (λ) → 0. When the empirical variance Varn[f (X+)] is sufficiently
large (which is true for MIL), the r.h.s. of (10) is dominated by the first two terms, which implies
告	1 G ” 4 ∕2λVarn[f(X+)]
∑Pif (χ+) ≈ n∑f (χ+) + V —nnk-^i
(11)
Next, We present the detailed proofs for both Theorems 1 and 2.
Proof of Theorem 1. Our proof of Theorem 1 is adapted from (Duchi & Namkoong, 2019) by
making extensions that fit the unique design of the distributionally robust bag likelihood (DRBL). We
start by introducing the folloWing lemma, Which Will later be used in our proof.
Lemma 1 (Maurer and Pontil Theorem 10). Let Y be a random variable taking values in [0, L]. Let
σ2 = Var[Y] and Varn[Y] = 1 En=I Yi2 — (1 5Zn=ι Yi )2 be the population and sample variance of
Y, respectively. Then for for n ≥ 2,
(σ — t ≤ VVo[n [Y] ≤ σ + t) ≥ 1 — exp
(12)
The distributionally robust bag likelihood (DRBL), i.e., the l.h.s. of (4), can be formulated as the
folloWing constrained optimization problem:
n
max	pif(χi+)
p∈Pn
i=1
s.t. Pn := {p ∈ Rn, P>l = 1,0 ≤ p,Df (p||n) ≤ n
(13)
Since the Df (p||q) is assumed to be the χ2-divergence and q folloWs the uniform distribu-
tion, Df (p||q) is reduced to the squared Euclidean distance. We first introduce the mean of
14
Under review as a conference paper at ICLR 2022
f (x+)'s, which is denoted as f = 1 Pn=I f (x+). Also, recall We denote the score vector by
f = (f(x1+), ..., f(xn+))> in Section 3.2. Thus, the empirical variance of f(X+) is given by
Varn [f (X+)] = n |f||2 - f2 = n ||f - fl ||2. We further introduce U = P - n, so the objective in
(13) can be transformed as
p>f =(U + -)τf = f + u>f = f + u>(f - fl)	(14)
n
where the last equality holds because UTI = 0. Thus, the optimization problem in (13) can be further
transformed into
max f + uτ(f - /1) s.t. ∣∣u∣∣2 ≤ -ʌ, UTI = 0, U ≥ —-	(15)
where the first constraint is derived by replacing Df with the χ2-divergence. Now, using the Cauchy-
Schwarz inequality, which states that Uτv ≤ ||U||2 ||v||2, gives the following condition
U>(f-f1) ≤√λl∣f-fi∣∣2 = JλVrnf≡
nn
where the equality holds if and only if
(16)
U = √λ(f(x+)- f) = √λ(f(x+)- f)
2 n||f - fl∣∣2	nPnVarniT(X+)J
Since we also have a constraint U ≥ - n, which satisfies if and only if
.√λ(f(x+) - f)、1
mm	i	≥ -1
i∈[n] VZnVarn[f(X+)]
(17)
(18)
Thus, if inequality (18) holds for vector f, we have
maxP>f = f+J
p∈Pn
λVarn[f(X+)]
(19)
which will prove the Theorem given in (4).
What remains is to prove inequality (18) holds with a hi
concentration inequality given by Lemma 1. Since f (
satisfy inequality (18), it is sufficient to have
probability. To show this, we leverage the
+) ∈ [0,1],we have |f (x+)-力
≤ 1. To
nVarn[f(X+)] ≤ 1 or n ≥ Varn[f(X+)]
Let us define the following event
or Varn[f(X+)] ≥ λ
n
(20)
en := {Varn[f (X +)] ≥ 413
σ2}
(21)
n
λ
λ
In Theorem 1, we suppose n ≥ σ2 max{2σ, 11}). Then, on event en, we have n
≥
λ
44λ	、______________
43Varnf(X+)] ≥ Varnf (X+)],
含≥
so that the sufficient condition (20) holds and the (19) becomes true.
Now we find the probability of holding the above event in (21) using Lemma 1. First, L = 1 in our
case, which gives
n----二―：------	n nt 2
P(σ - t ≤ ʌ/Varn [f (X+)] ≤ Q + t) ≥ 1 - exp (------2
The following also holds true:
P (σ - t ≤ PVarn [f(X+)]) ≥ P(σ - t ≤ PVarn [f(X+)] ≤ σ + t) ≥ 1 - eχp (—2
Let t = (1 - ʌ/ɪ) σ, which gives σ - t = ʌ/ɪσ. Substituting this to (12) leads to
P ( ʌ/ɪσ ≤ PVarn [f(X+)]) ≥ 1 - exp (-nt-) ; P(e„) ≥ 1 - exp
15
Under review as a conference paper at ICLR 2022
Further substituting the value of t =(1 - σ gives rise to
P(En) ≥ 1 — exp (—0.359nσ2) ≥ 1 — exp
—
7nσ2
20
This completes the proof of Theorem 1.
Proof of Theorem 2. In order to prove this theorem, we consider two assumptions, which both
hold true for our MIL setting.
Assumption 1: Random variable f (X+) has a finite exponential moment in a neighborhood of 0
under the distribution po i.e.,, Eo [exp(τf (X+))] < ∞ for T ∈ [-τo, τo] for some τo > 0.
Assumption 2: Random variable f(X+) is non-constant under p0.
Assumption 1 is true in our case as f(X+) is bounded in [0, 1]; Assumption 2 also empirically holds
true as there are both positive and negative instances in a positive bag so the output scores are distinct
over different instances in a bag. The second assumption ensures that the uniform distribution p0 is
not a locally optimum, which means there exists an opportunity to upgrade the value by re-balancing
the probability between positive and negative instances in a positive bag.
Consider P that is absolutely continuous with respect to po and therefore the likelihood ratio g =恶
(a.k.a., Radon-Nikodym derivative) exists. Using a change of measure, the optimization problem in
the l.h.s. of (10) can be written as
max Eo[gf(X+)]
g∈L1(p0)
s.t. ∣Eo[glogg] ≤ λ, Eo[g] = 1,g ≥ 0
(22)
where L1(po) is L1-space with respect to the measure po. To solve the optimization problem above,
we formulate its Lagrangian,
g∈m篇 .)E0[gf (X+)]- α (Eo[g log g]- n
(23)
where α is the Lagrange’s multiplier. The solution of the above objective function is given by the
following proposition (Petersen et al., 2000; Lam, 2016):
Proposition 1. Under Assumption 1, when α > 0 is sufficiently large, there exists an unique optimizer
of (23) given by
* (+) = exp( f(α+))
g x _ Eo [exp fX+ i
(24)
Assume that such α* and g* exist and that α* is sufficiently large then
λ
n
Eo[g* logg*] =皿3
α
- log Eo
exp (X
α*
β*Eo[f(X +)exp(β*f(X+))]
Eo [exp(β*f(X+))]
- log Eo[exp β*f(X+)] = β*ψ0(β*) - ψ(β*)
In the above expression, We define β* = OT and ψ(β) = logE0[exp(βf (X+))] is the logarithmic
moment generating function of f(X+ ).
We can write the optimal solution of the objective function (22) as follows
Eo[f(X+)g*]
Eo[f(X+)exp( f⅞+))]
Eo[exp( fX+))]
ψ0(β*)
(25)
16
Under review as a conference paper at ICLR 2022
Now let us perform Taylor expansion of the following
0	∞1	∞1	∞	1	1
βψ ⑼-ψ(β) = X mκm+1β	- X mκmβ = X(m-i)!- m κmβ
m=0	m=0	m=1
∞1	1	1	1
=	-κ---κmβιrl,β =石κ2β + 3κ3∣β + 石κ4β + O(β )
m(m - 2)!	2	3	8
In the above expression, κm = ψ(m) (0) is the m-th derivative of ψ with evaluated at β = 0 and
O(β5) is continuous in β. By Assumption 2, We have κ2 > 0. Therefore, for small enough λ,
above equation reveals that there is a small β* > 0 that is root to the equation λ = βψ0 (β) - ψ(β)
and the root is unique. This is because by Assumption 2, ψ(.) is strictly convex, and therefore,
(ddβ)(βψ0 - ψ(β)) = βψ00(β) > 0 for β > 0, so that βψ0(β) - ψ(β) is strictly increasing.
Since a* =*,this shows that for any sufficiently small λ, We can find a large ɑ* > 0 such that the
corresponding g* in 24 satisfies ʌ = E0[g* logg*]. This means we can write the following
一=χκ2β* + Wκ3β* + oκ4β* + O(β* )	(26)
n2	3	8
We can obtain β * as follow
β*
2
* + O(β* )
In the above expression, first we use the binomial expansion (1 + x) -21 = 1 - 2X + 8x2 ….followed
by substitution of β* in the second term. Now, the corresponding optimal solution becomes following
E0[f(X +)g*] = ψ (β*) = κ1+κ2β*+κ3-^+O(β* ) = κ1 + √2κ2 (一) + —— +O
2	n	3 κ2 n
In the above equation κι = /,κ2 = Varn[f(X +)],κ3 = E0[(f(X+) - E0[f(X+)])3]. This
completes the proof of Theorem 2.
D The P-F S ampling Algorithm
Algorithm 1 shows the detailed description of the proposed P-F active sampling technique. First, we
find the unexplored bags from a pool of positive training bags. To determine the unexplored bags, we
identify the instance with highest p-value from each bag (i.e., b* for bag b according to (7)). Next, we
sort the f -scores of corresponding instances in an non-decreasing order. Based on the sorted scores,
we pick bags whose highest f -scores are less than a given threshold ThPF as defined in Algorithm 1.
From each unexplored bag, we pick k-instances with highest scores. During this process, we avoid
bags, where at least one true positive instance is already queried in previous steps. The maximum
number of instances queried from unexplored bags is bounded by the batch size in each step. During
query, we give higher priority to the least explored bag, whose highest instance score is the smallest.
If the batch size is not reached, we continue to query instances whose uncertainty are the highest
based on their F-Entropy. To ensure that the queried instances are indeed uncertain, we compare the
corresponding F-Entropy with a threshold T hH to avoid querying confident instances.
17
Under review as a conference paper at ICLR 2022
Algorithm 1: P-F Active Sampling
Input： PBPos, Qprev, ThPF, ThH, BSize, k,
Output: Q
Data: B positive training bags // Consists of a feature vector for each
bag
Initialization: UB = {}, count = 0, QP-F = {}, QF = {}
for b ∈ [B] do
Pb J PBpos [b]
b* J arg max Pb \ Qprev [b]
if f (x+J ≤ ThPF then
I UB J b
/* Adding instances from unexplored bags	*/
UB = argsortASCb*∈w f (χ+ )
for b* ∈ UB do
if b* ∈ Qprev then
if positive ins ∈ Qprev [b] then
L continue
else
XPF = argsortDesCb* (f(x+J \ Qprev®])[：k]
for xi ∈ X PF do
if count≥ BSize then
L break
Qp-f [b*] J Xi
count J count+1
Qprev = Qprev ∪ QP-F
/* Adding instances with highest F-Entropy;
H [f (Xi+ )] = - f(Xi+ log f (Xi+ )) + (1 - f (Xi+ )) log(1 - f (Xi+ ))	*/
Cidx = argsortDesCi (H[f(x+)] ≥ ThH)
for i ∈ Cidx do
if count≥ BSize then
L break
if xi+ ∈ Qprev[bi] then
L break
Qf [bi] J x+
count J count+1
Q= Qprev ∪QF
18
Under review as a conference paper at ICLR 2022
Table 2: Bag level distributions on different datasets
Split	20NewsGroup		Cifar10		Cifar100		Pascal VOC	
	Positive	Negative	Positive	Negative	Positive	Negative	Positive	Negative
Train	30	30	-500-	-500-	-500-	500	-124-	-124-
Test	20	20	100	100	100	100	84	84
75
70
⅛65
≡
60
55
(b) β = 1
SteP
Figure 7: Impact of key model parameters: (a-b) Cifar10; (c-d) 20NewsGroup
E Additional Experimental Results
In this section, we first give a detailed description of the datasets. We then present additional ablation
study results that complement the ones presented in the main paper. Finally, we demonstrate some
qualitative examples where our approach is able to sample the true positive instance from the bag
containing outlier but Maximum-Entropy can not.
E.1	Dataset Description
Table 2 summarizes the bag level statistics. We also present the details of each dataset below.
•	20NewsGroup: In this dataset, an instance refers to a post from a particular topic. For each topic,
a bag is considered as positive if it contains at least one instance from that topic and negative
otherwise. This dataset is particularly challenging because of the severe imbalance in terms of
instances where there are very few (≈ 3%) positive instances in each positive bag. While number
of instances per bag may vary, on average there are around 40 instances per bag.
•	Cifar10: In the original dataset, there are 50,000 training and 10,000 testing images with 10 classes
indicating different images. The bags are constructed as follows. First, we pick ‘1’, ‘2’, and ‘5’
related images as positive instances and the rest as negative. To construct a positive bag, we choose
a random number from 1 to 3 and pick the positive instances equal to the randomly generated
number. The rest of the instances are selected from a negative instances pool. For negative bags, all
instances are selected from the negative instance pool. For each bag, we consider 32 instances.
•	Cifar100: In the original dataset, there are 50,000 training and 10,000 testing images with 20
different superclasses indicating different species. Bag construction is similar to Cifar10. In this
case, images from superclass 2 are treated as positive and the rest as negative.
•	Pascal VOC: This dataset consists of 2,913 images, where images are used for segmentation. In
particular, each image is treated as a bag and instances are obtained as follows. We define a grid
size of 60 × 75 and partition the images. Depending on an image size, the number of instances
may very. We treat an instance as positive if at least 5% of the total pixels in a given instance are
related to the object of interest otherwise negative. In our case, we consider bird as the object of
interest. All the images consisting of bird are regarded as positive bags and other as negative.
E.2 Additional Ablation Study
In this section, we present some additional ablation study results to demonstrate the impact of key
model parameters and the stability of the model performance over multiple runs.
Impact of β and λ: Since we have already shown the results on Cifar100 and Pascal VOC, Figure 7
show the impact of λ and β on Cifar10 and 20NewsGroup datasets. Similar to the findings on the
other two datasets, Figures 7 (b) and (d) demonstrate that a λ in the middle range outperforms too
large (or small) λ values. In case of β, placing too less (or too much) emphasis may result in overly
19
Under review as a conference paper at ICLR 2022
(a) 20NeWsGrouP	(b) Cifar10
(C) Cifar100	(d) Pascal VOC
Figure 9: ImPact of hyPerParameter k
Figure 8: MI-AL Performance With standard deviation
(c) Cifar100
(or Poorly) leveraging annotated instances. Therefore, a good balance betWeen the bag-level and
instance-level losses achieves the best result, as shoWn in Figures 7 (a) and (c).
Performance stability. Figure 8 rePorts the Performance comParison With one standard deviation
(comPuted over three runs), Which is rePresented by the vertical black line. As discussed in the
main PaPer, the mean MI-AL curve of ADMIL clearly outPerforms other comPetitive baselines.
MeanWhile, the standard deviation of the ProPosed ADMIL model is also relatively small, Which
indicates its overall stable MI-AL Performance across different datasets in multiPle runs.
Impact of k: Figure 9 shoWs the imPact of the hyPerParameter k, Which is the number of instances
queried in each unexPlored bag, on model Performance. As can be seen, k = 2 achieves a generally
decent Performance across all the datasets. For datasets With a larger size (e.g., Cifar100), a larger k
leads to a slightly better Performance.
F Link to Source Code
For the source code of our exPeriments, Please click here.
20