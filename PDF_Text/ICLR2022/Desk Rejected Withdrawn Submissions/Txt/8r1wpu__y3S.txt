Under review as a conference paper at ICLR 2022
Efficient Regularization for
Robust Deep ReLU Networks
Anonymous authors
Paper under double-blind review
Ab stract
We present a regularization functional for deep neural networks with ReLU activa-
tions, and propose regularizers that encourage networks which are smooth not only
in their predictions but also their decision boundaries. We evaluate the stability of
our networks against the standard set of '2 and '∞ norm-bounded adversaries, as
well as several recently proposed perception-based adversaries, including spatial,
recoloring, JPEG, and a learned neural threat model. Crucially, our models are
simultaneously robust against multiple state-of-the-art adversaries, suggesting that
the robustness generalizes well to unseen adversaries. Furthermore, our techniques
do not rely on adversarial training and are thus very efficient, incurring overhead
on par with two additional parallel passes through the network. On CIFAR-10,
we obtain our results after training for only 4 hours, while the next-best perform-
ing baseline requires nearly 25 hours of training. To the best of our knowledge,
this work presents the first technique to achieve robustness against adversarial
perturbations without adversarial training.
1	Introduction
Recent results in deep learning highlight the remarkable ability of deep neural networks to perform
on real world on tasks ranging from object detection to game playing. However, such models
suffer widely from adversarial examples (Szegedy et al., 2013), revealing that the current generation
of deep models often rely on unexpectedly brittle or spurious features. This behavior not only
presents an attack surface in sensitive applications, but can also have unexpected consequences for
performance-critical settings, e.g., models that are deployed in the physical world.
The standard approach to defending against such adversarial examples has been to formally specify a
set of perturbations, then augment the training procedure with access to the examples generated by
the adversary; such techniques form a broad category called adversarial training (Goodfellow et al.,
2014; Kurakin et al., 2016; Madry et al., 2017). While these methods have been highly successful
in defending against the specific adversaries for which they are tailored, it has been observed that
this strategy confers negligible robustness against other forms of PertUrbations-in many cases even
degrading such robustness (Kang et al., 2020). Assuming access to the adversary during training
is also unrealistic for delivering the tyPe of real-world robustness necessary for either security- or
Performance-critical aPPlications. General robustness against unseen adversaries thus remains a
major oPen Problem.
Our main observation is that achieving such generalizeable robustness requires redefining what it
means to be robust. In Particular, instead of indexing our goal against a sPecific adversary, we ProPose
an aPProach to robustness which eschews the adversarial training Paradigm entirely. Our results
suPPort the conclusion that the models trained using our methods are generally robust. As an added
benefit, because the adversary is often formulated as an inner oPtimization in the training Procedure,
we also avoid the significant comPutational overheads that adversarial training often entails.
Contributions. We develoP regularizers for training locally stable deeP neural networks. Our
goal is to learn a function which does not change much in the neighborhoods of natural inPuts, i.e.,
unPerturbed data drawn from the inPut distribution, indePendently of whether the network classifies
correctly. To that end, we design a regularization functional that exPloits the continuous Piecewise
1
Under review as a conference paper at ICLR 2022
linear nature of ReLU networks to induce a function which varies smooth over the data manifold in
not only its predictions, but also its decision boundaries.
We evaluate our approach by training neural networks with our regularizers for the task of image clas-
sification on CIFAR-10 (Krizhevsky and Hinton, 2009) and MNIST (LeCun et al., 2010). Empirically,
a single network trained using our regularizers exhibits robustness against multiple norm-bounded
adversarial models implementing '2 and '∞-based attacks. We also report state-of-the-art verified
robust accuracy for CIFAR-10 under '∞ perturbations of size E = 8/255. Furthermore, We also
achieve strong robustness against a suite of recently-proposed perception-based adversaries. In both
cases, We outperform adversarial training methods When evaluated on the unseen adversaries, Whereas
our regularizers achieve such performance Without access to any adversaries during training.
Furthermore, our regularizers are cheap: We simply evaluate the netWork at tWo additional random
points for each training sample, so the total computational cost is on par With three parallel passes
through the netWork. Our techniques thus present a novel, regularization-only approach to learning
robust neural netWorks against unseen adversaries, Which achieves performance comparable to
existing tailored defenses While also being an order of magnitude more efficient.
2	Background
ReLU networks Our development focuses on a standard architecture for deep neural netWorks:
fully-connected feedforWard netWorks With ReLU activations. In general, We can Write the function
represented by such a netWork With n layers and parameters θ = {Ai, bi}i=1,...,n-1 as
z0 = x	(1)
Zi	=	Ai ∙ Zi-i	+ bi	for i = 1,...,n - 1	(2)
Zi	=	σ(Zi)	for i = 1,...,n — 2	(3)
f(x; θ) = Zn-1	(4)
Where the Ai are the Weight matrices and the bi are the bias vectors. We call the Zi “hidden
activations”, or more simply, activations, and the ^i “pre-activations”.
In this work, we consider networks in which σ(∙) in (3) is the Rectified Linear Unit (ReLU)
Zi = σ(Zi) := max(0, Zi)	(5)
It is clear from this description that ReLU networks are a family of continuous piecewise linear
functions.
Adversarial robustness One common measure of robustness for neural networks is against a
norm-bounded adversary. In this model, the adversary is given an input budget E over a norm || ∙ ||in,
and asked to produce an output perturbation δ over a norm | ∙ |out. A point χ0 is an E-δ adversarial
example for an input pair (x, y ) if
||x0 - x||in ≤ E	(6)
|f(x0; θ) - y|out ≥ δ	(7)
When the specific norm is either unimportant or clear from context, we also write the first condition
as x0 ∈ N(x), where N (x) refers to the E-ball or neighborhood about x. If such an adversarial
example does	not exist,	we	say	that the network is E-δ robust	at	x.	Standard examples	of ∣∣∙	||in
include the '2	and '∞	norm, defined for vectors as ∣∣χ∣∣∞ :=	maxi	|x/.	For classification tasks,
the adversary is successful if it produces an example in the E-neighborhood of x which causes the
network to misclassify. In this case, we drop δ and say that the network is E-robust at x. Note that if
f(x; θ) is already incorrect, then x suffices as an adversarial example.
3	Related work
Adversarial examples were introduced by Szegedy et al. (2013), who found that naively trained
neural networks suffer almost complete degradation of performance on natural images under slight
perturbations which are imperceptible to humans. A standard class of defenses is adversarial training,
2
Under review as a conference paper at ICLR 2022
which is characterized by training on adversarially generated input points (Goodfellow et al., 2014).
In particular, the Projected Gradient Descent (PGD) attack (Kurakin et al., 2016; Madry et al., 2017)
is widely considered to be an empirically sound algorithm for both training and evaluation of robust
models. However, such training methods rely on solving an inner optimization via an iterative method,
effectively increasing the number of epochs by a multiplicative factor (e.g., an overhead of 5-10x for
standard PGD).
Achieving robustness against multiple adversarial models has also been explored previously (Schott
et al., 2018; Tramer and Boneh, 2019; Maini et al., 2019; Croce and Hein, 2019), though in most cases
these works use weaker variants of the subset of standard adversaries we consider (e.g., a smaller
or the single-step version of PGD), and also require training against specific adversaries. The only
work to study adversarial robustness against multiple unseen adversaries is a recent approach by
Laidlaw et al. (2021), which proposes to first learn a neural perceptual threat model that approximates
a distance between images based on human perception. However, they ultimately employ the threat
model as an adversary during training, incurring overhead on par with standard adversarial training,
and achieve lower robustness against the standard norm-bounded adversaries than our methods.
A related goal is to train models which are provably robust. One method is to use an exact verification
method, such as an MILP solver, to prove that the network is robust on given inputs (Tjeng et al.,
2017). In particular, Xiao et al. (2019) use a similar loss based on ReLU pre-activations to learn
stable ReLUs for efficient verification. However, their loss does not induce a metric and so does not
enjoy the same approximation guarantees as ours; thus they require training against a PGD adversary
to produce a robust model. Certification methods modify models to work directly with neighborhoods
instead of points (Dvijotham et al., 2018; Gowal et al., 2018; Mirman et al., 2018; Wong et al., 2018).
In practice, the inference algorithms must overapproximate the neighborhoods to preserve soundness
while keeping the representation compact as it passes through the network. This strategy can be
interpreted as solving a convex relaxation of the exact verification problem. Though certification thus
far has produced better lower bounds, verification as a technique is fully general and can be applied
to any model (given sufficient time); recent work also suggests that methods using layerwise convex
relaxations may face an inherent barrier to tight verification (Salman et al., 2019).
Finally, several prior works explore regularizing for robustness based on similar loss functionals
(Ross and Doshi-Velez, 2017; Jakubovitz and Giryes, 2018), though they only report results using the
weaker single-step PGD adversary. Hein and Andriushchenko (2017) propose a conceptually similar
regularizer to minimize the difference between logits and show improved `2 certified robustness.
Another recent work by Zhai et al. (2020) uses randomized smoothing for `2 certified robustness, and
claim similar computational advantage due to avoiding adversarial training, but still take 61 hours to
train, compared to only 4 hours in our approach. Finally, we also note that a similar decomposition
between accuracy and stability forms the basis for the TRADES algorithm (Zhang et al., 2019),
though their procedure ultimately still relies on adversarial training.
4	Setting
We begin by reframing the goal of learning functions that are robust using a perspective which
decouples stability from accuracy. The key observation is that we would like to train networks that are
locally stable around natural inputs, even if the network output is incorrect. This approach contrasts
with adversarial training, which attempts to train the network to classify correctly on worst-case
adversarial inputs. In particular, recall that a network is -robust at x if no point in the -neighborhood
of x causes the network to misclassify. We consider the related property of -stability:
Definition 4.1. A function f is -δ stable at an input x if for all x0 ∈ N(x), |f (x) - f0(x)| ≤ δ.
Similarly, a classifier f is -stable at an input x if for all x0 ∈ N(x), f(x) = f(x0).
As -stability does not depend on the correct label for x, we will seek to induce -stability as a
property of the learned function independent of its predictions. For completeness, we state the
following connection between robustness and stability:
Proposition 4.1. A function f is -δ robust at an input x if and only if f is -δ stable at x and
f (x) = y. Similarly, a classifier f is -robust at an input x if and only if f is -stable at x and f
correctly classifies x.
3
Under review as a conference paper at ICLR 2022
We emphasize that this notion of stability is not new, and can be understood as a local Lipschitz
property. For instance, Zheng et al. (2016) propose to regularize deep neural networks for -stability
directly with the following objective:
Lstab(X,x0; θ) = |f (x; θ) - f(x0; θ)∣	(8)
They incorporate this objective as an additional loss term by setting x0 as perturbations of the input
data x, sampled from a Gaussian. While their results demonstrate stability under perturbations
such as JPEG compression, this type of randomized smoothness is well known to deliver negligible
robustness against a more targeted PGD adversary (see, e.g., Jiang et al. (2020)), the reason being that
randomly sampling points to evaluate the stability objective is insufficient to deliver the worst-case
guarantees needed for -robustness.
5	Hamming Regularization
In this section, we develop a novel regularizer based on the Hamming distance between activation
patterns of nearby points. Our main insight is a connection between the stability of a ReLU network
and the distribution of its decision boundaries around natural inputs.
5.1	Hamming distances and ReLU networks
Let f be a ReLU network. Because f is a continuous, piecewise linear function, for any input point
x (minus a set of measure zero), f restricted to a sufficiently small neighborhood of x is a linear
function. We will denote the linear function induced in this way by the input X as fχ(∙; θ), i.e., the
analytic extension of the local linear component about x over the input domain.
Clearly, every local linear component fχ(∙; θ) can be identified with a unique “activation pattern”, i.e.,
the sequence of branches taken at each ReLU by the input X. We will write fH (X; θ) for the map that
takes inputs X to their activation patterns, which live on the unit hypercube {0, 1}N (where N is the
number of ReLUs in the network). If we endow the hypercube with the standard Hamming distance
dH, this induces a pullback (psuedo-)metric in the input space: dH (x, y) = dH(∕h(x; θ), /h (y; θ)).
We note that the metric identification is exactly the set of local linear components fχ(∙; θ).
We are now ready to make the connection between local stability and the Hamming distances so
defined. We will argue that dH yields good bounds on the local stability of f. We show this in
two steps. Consider the objective of reducing the number of distinct local linear components of the
learned function f in the ^-neighborhood of inputs x. Our first result says that minimizing dH over a
neighborhood of points is tractable:
Lemma 5.1. Let X = {xι ,…，Xn} be a set of input points and let X denote the convex polytope
defined by X. If there exists X ∈ X such that dH (xi, X; θ) = 0 for all Xi ∈ X ,then the same also
holdsfor all pairs ofpoints in X, i.e. x, x0 ∈ X =⇒ dH (x, x0) = 0.
The result follows immediately from the previous observation that dH is a pseudo-metric. The
next lemma says that the standard stability objective (Equation 8) enjoys the same property when
combined with previous statement:
Lemma 5.2. Let X = {x1, ..., xn} be a set of input points such that Lemma 5.1 holds. Define the
maximum pairwise difference over X ofthefunction f to be δ := maxχi,χj∈χ |f (xi； θ) — f (Xj; θ)∣.
Thenfor any G X ∈ X such that Ne(X) ⊆ X, we have that f is e-δ stable at X.
Proof. By the assumption that Lemma 5.1 holds, f is a linear function when restricted to X. Fur-
thermore, X is a closed, convex set and thus f achieves its maximum and minimum over X at the
vertices of the polytope. The result then follows from the fact that Ne(X) ⊆ X and the definition of
e-δ stability.	□
The primary significance of these results is that optimizing for stability becomes much more tractable
when viewed in terms of the Hamming regularizer, which provides the primary theoretical justification
for regularizing based on activation patterns. In particular, rather than sampling random perturbations
from, e.g., a Gaussian, we can take them to be the corners of an appropriately chosen polytope; then
we do not depend on any concentration bounds for the quality of our stability guarantees either.
4
Under review as a conference paper at ICLR 2022
5.2	The Hamming regularization functional
Figure 1: Surface and contour plots for Hα (α = 1).
Lemmas 5.1 and 5.2 yield natural objectives for inducing stability. Note that a simple corollary is
that if the set X forms a hypercube, it is both necessary and sufficient to check the assumptions of
either lemma on the opposing vertices to apply their conclusions. We thus propose the following
regularization functionals, which are evaluated pairwise between input points:
LS(Xi,Xj; θ) = |f (Xi； θ) - f (xj; θ)∣2	(9)
LH(Xi,Xj； θ) = dH(xi,Xj； θ)2	(10)
To induce ^-stability at a point x, We can simply minimize LS (X - ρ,x + ρ; θ) and LH (X - ρ,x + ρ; θ)
for ρ ∈ {±}d, where d is the input dimension.
HoWever, the loss term LH is not continuous in the inputs, and furthermore, the gradients vanish
almost everyWhere, so it does not generate good training signals. We thus also propose the folloWing
continuous relaxation of the Hamming distance dH :
Hα(Z, y； θ) := ∣abs(tanh(α * z) — tanh(α * y))	(11)
This form is differentiable everywhere except when z = y, and recovers the Hamming distance when
we take α to infinity. Qualitatively, sensitive activations (i.e., small |z| and |y|) are permitted so
long as they are precise. Figure 1 presents the surface and contour plots of Hα . Note that this idea
can be extended more generally to other activation functions by penalizing differing pre-activations
more when the second derivative of the activation function is large (and so the first-order Taylor
approximation has larger errors).
5.3	Convergence of Hamming functional in the limit
In this section, we develop a theoretical understanding of the proposed Hamming regularizer func-
tional by showing convergence in the infinite sample limit to the Laplacian of a related function using
the theory of manifold regularization.
We begin with some brief background. The manifold assumption states that input data is not drawn
uniformly from the input domain X , also know as the ambient space, but rather is supported on a
submanifold M ⊂ X , called the intrinsic space. There is thus a distinction between regularizing on
the ambient space, where the learned function is smooth with respect to the entire input domain (e.g.,
Tikhonov regularization (Phillips, 1962; Tikhonov et al., 2013)), and regularizing over the intrinsic
space, which uses the geometry of the input submanifold to determine the regularization norm.
Let μ be the probability measure of the input distribution, which has support M. We will define μe
to be the probability measure produced by convolving μ with a uniform distribution over the '∞ ball
of radius e; by definition, one can simulate drawing from the distribution of μe given a sample from μ
simply by adding some random noise from '∞ ball of radius 巳 We write the support of the perturbed
input distribution as M .
5
Under review as a conference paper at ICLR 2022
To apply manifold regularization, we consider the function
g(∙; θ) = (γS∕2f(∙; θ),γTfH(∙; θ))	(12)
which is simply the output of f concatentated with its activation pattern, with some weighting factors
γS,γH. Then clearly,
|g(xi； θ) - g(xj ； θ)∣2 = YSLS (xi,Xj ； θ) + YH LH (xi,Xj ； θ)	(13)
Thus, the regularization functional |g(xi； θ) - g(xj; θ)∣2 is simply the sum of our proposed reg-
ularizers, so characterizing the behavior of g under regularization suffices for understanding the
regularization effects of LS and LH .
The corresponding regularizer is as follows:
1N
N E Ig(Xi) - g(Xj )l2Li,j
i,j=1
(14)
Here, the X1, ..., XN are samples drawn, by assumption, from the perturbed input manifold M
according to μe. The matrix L is known as the discrete graph Laplacian, and is composed of
weights measuring the similarity between samples. A common choice of weights is a heat kernel:
Li,j = L(Xi, Xj) := exp(-||Xi -Xj||2/s). Under these conditions, it can be shown (see, for instance,
Belkin et al. (2006)) that the discrete sum converges to the following integral as the number of sample
grows to infinity:
||g||I2 := M
||VMe g(x)||2d〃e(X)
(15)
Hence, the proposed functionals in Equations 9 and 10 have the effect in the limit of inducing the
change in network outputs and decision boundaries to be smooth where the probability of drawing a
perturbed sample is large, as initially claimed.
5.4	Training with Hamming regularization
In this section, we propose a regularization technique based on the Hamming regularization functional.
As in stability training, our perturbations are randomly sampled, which allows our approach to be
very efficient. However we use the corners of a hypercube for our perturbations in order to take
advantage of the properties developed in the previous sections.
More explicitly, for every sample X, we generate a new random maximal perturbation ρ ∈ {±}d and
produce the pair of perturbed inputs X+ := X + p and X- := X - p. For a batch of points {Xi }iN=1,
we compute the standard stability regularization term as
IIf (∙,Θ)I∣S = X If (x+) - f (x-)∣2	(16)
i
For the Hamming regularization term, we normalize the value for each pair of perturbations to a value
between 0 and 1. More explicitly, let N be the number of samples, n be the number of layers, and
mj be the number of activations at the jth layer. Denote the kth activation in the j th layer of the ith
sample as Zijk. Then we use the following (empirical) regularizer on the activation map fh:
1 N1n 1
IIfH (∙,θ)∣∣H = N X m X mr∣X HaWk ,z-k ； θ)∣2	(17)
n mj	2
The final optimization objective is thus
1N
θ = argminNY^V(f(Xiθ),yi) + γκIIf(∙;θ)“K + γsf(∙;θ)HS + YhIIfH(∙;θ)MH (18)
where V is the loss function, IIf (∙, θ) IIK is the ambient regularizer (e.g., '2 regularization), and the
YK, YS, YH are hyperparameters which control the relative contributions of the different regularizers.
6
Under review as a conference paper at ICLR 2022
Table 1: CIFAR-10 robust accuracy against various adversaries and perturbation models. Results for
the adversarially trained models are taken from Laidlaw et al. (2021). Training times use a single
RTX 1080 GPU.
Defense	clean	'∞	'2 I Spatial			ReColor	JPEG	NPTM	minutes
AT '∞	86.8	49.0	19.2	4.8	54.5	3.2	0.0	1620
AT '2	85.0	39.5	47.8	7.8	53.5	25.5	0.3	1537
AT Spatial	86.2	0.1	0.2	53.9	5.1	0.0	0.0	1607
AT ReColor	93.4	8.5	3.9	0.0	65.0	1.4	0.0	3053
ATNPTM	71.6	28.7	33.3	64.5	67.5	30.9	9.8	1481
Ours	72∙1	36.8	43.4	28.4	63.1	34.2	9.1	252
6	Experimental results
We report results a variety of adversarial settings on CIFAR-10 and MNIST image classification our
regularization techniques. For CIFAR-10, we additionally a smaller CNN for verified robustness,
following the setup in Xiao et al. (2019). We also include several ablation studies in the standard
CIFAR-10 setting to better understand the individual contributions of the proposed method to
robustness. Appendix A gives full experimental details and training hyperparameters.
6.1	Robustness against unseen adversaries
Table 1 reports our main experimental results. All adversaries are given white-box access to the
trained models. We separate the evaluation into two categories. The first category consists of norm-
bounded adversaries, with the '2 and '∞ norm-bounded adversaries being the most common in the
literature. These adversaries are allowed to arbitrarily perturb the input images within an `p ball in
order to cause the network to misclassify. We report results at the standard bounds of '∞ = 8/255
and `2 = 1, using the state-of-the-art AutoAttack ensemble (Croce and Hein, 2020) as the adversary.
The second class consists of perception-based adversaries. These attacks aim to reduce the visual
impact of their perturbations, often relying on heuristic methods to bound the attack. In particular,
the Spatial (Xiao et al., 2018) attack uses a distance similar to optimal flow; ReColor (Laidlaw and
Feizi, 2019) is applied to the color space of the image, rather than pixels; JPEG (Kang et al., 2020)
perturbs the image within the JPEG-compressed space; and NPTM (Laidlaw et al., 2021) uses a
learned perceptual distance which is captured by a separate neural module.
In both cases, the experiments indicate that our Hamming regularizer outperforms adversarial training
on the unseen class. More explicitly, adversarial training against the '∞ and '2 norm-bounded
adversaries imparts negligible robustness against many of the perceptual-based attacks, with the
spatial and NPTM adversaries being particularly challenging; conversely, the three methods which
train against the perceptual attacks all achieve lower robustness against the norm-bounded adversaries
than our defense. Interestingly, the strongest threat model empirically is the NPTM attack, and our
defense achieves nearly the same performance against the NPTM adversary as does the adversarially
trained model (AT NPTM, which uses an approximate version of the NPTM adversary in order to
be efficient enough for training). Note also that, though our method performs worse on clean data
compared with standard AT, the drop is comparable with AT NPTM, which is the only other method
to achieve comparable robust accuracy against all perceptual adversaries.
Finally, as shown in the last column of Table 1, our defense takes drastically less time to train than
every other baseline, being almost 6 times faster than the next best defense.
6.2	Robustness curves against standard PGD
We provide detailed robustness curves for the most common settings for adversarial robustness in the
literature, using the standard 20-step PGD adversary with 10 random restarts. Figure 2 reports the
results on both MNIST and CIFAR-10, for both '2 and '∞ bounds. We also plot for reference the
robustness against the stronger AutoAttack+ adversary used for the results in Table 1 as “ours (AA+)”;
7
Under review as a conference paper at ICLR 2022
0 50 5
07 5 2
1
)%( ycarucca tsuboR
0	10	20	30
'∞ bound (e/255)
(a) Robust accuracy on CIFAR-10 against a 20-step
'∞ PGD adversary With 10 restarts.
)%( ycarucca tsubo
(c) Robust accuracy on MNIST against a 20-step
'∞ PGD adversary with 10 restarts.
Figure 2: Robustness curves against a 20-step PGD adversary with 10 random restarts.
01234
`2 bound (e)
(b) Robust accuracy on CIFAR-10 against a 20-
step `2 PGD adversary With 10 restarts.
100
75
50
25
0
`2 bound (e)
(d) Robust accuracy on MNIST against a 20-step
`2 PGD adversary with 10 restarts.
for CIFAR-10, We additionally ran a 500-step '∞ PGD adversary with 20 restarts at e = 8/255,
yielding 40.4% robust accuracy, labelled “ours (PGD+)”. For comparison, we also plot the robustness
of models trained using vanilla adversarial training against an '∞-bounded PGD adversary at the
standard levels for each dataset (e = 8/255 for CIFAR-10, and e = 0.3 for MNIST), taken from
Madry et al. (2017). For CIFAR-10, our findings mirror those in Schott et al. (2018), namely, that
a model trained using PGD on CIFAR-10 for '∞ performs poorly against '2 attacks. The results
on MNIST shows a much less dramatic difference in performance, which we attribute to the lower
complexity of the task. These results show that, compared with models trained using standard PGD
against '∞ perturbations, the standard setting in the adversarial training literature, our methods
perform comparably (or substantially better, in the case of the `2 adversary on CIFAR-10) in a variety
of settings, despite not using the adversary during training.
6.3	Verified robustness
We also report the verified robustness of our method on CIFAR-10 against '∞ perturbations at
e = 8/255 in Table 2. We provide this metric primarily as a baseline for establishing a provable
lower bound on the robustness achieved by our defense, compared to empirical robustness which is
often a brittle measure of true robustness, particularly for newly proposed defenses (Athalye et al.,
2018; Carlini et al., 2019; Tramer et al., 2020).
8
Under review as a conference paper at ICLR 2022
Table 2: CIFAR-10 verified and robust accuracies for '∞-bounded perturbations at E = 8/255.
Defense	Test Accuracy (%)		
	Verified	Robust	Clean
RS Loss (Xiao et al. (2019))	20.27	26.78	40.45
Hamming Regularization	21.04	25.56	36.66
Table 3: CIFAR-10 ablation studies against an '∞ PGD-adversary at E = 8/255.
Defense	Test Accuracy (%)	
	Robust	Clean
PGD (Madry et al. (2017))	45.8	87.3
FGSM (Madry et al. (2017))	0.0	90.3
Hamming Regularization	40.54	72.1
stability regularizer only	20.11	34.74
Hamming regularizer only	24.87	90.24
We achieve state-of-the-art results in the same setting as in Xiao et al. (2019), using an MILP solver
to directly verify the outputs of the neural network as a program. The main added challenge in this
setting is ensuring that the solver does not time out for a significant fraction of inputs. The RS Loss
proposes to regularize the pre-activations of ReLU networks so as to encourage functions that are
easier to verify by reducing the number of active ReLUs (branches) in the neural network (program).
Our performance suggests that the Hamming regularizer achieves a similar effect. However, we
note a crucial difference between the two approach is that our regularization functional is based on a
pre-metric, and thus can effectively regularize over large perturbation neighborhood (as developed
in Section 5.1). In contrast, Xiao et al. (2019) pair the RS Loss with a standard PGD adversary to
find strong perturbations; thus the approach suffers from the same problems as standard adversarial
training, namely, substantially longer training times as well as likely overfitting to a specific adversary.
6.4	Ablation Studies
Table 3 presents the results of ablation studies using the individual terms of our proposed regularization
scheme on CIFAR-10 against a 20-step, '∞ bound PGD-adversary with 10 random restarts at
E = 8/255. Our results indicate that both terms contribute jointly and individually to our performance.
Intriguingly, applying the Hamming regularizer alone seems to provide a significant amount of
robustness with only minimal degradation in clean accuracy.
7 Conclusion
We design regularizers that encourage piecewise linear neural networks to learn locally stable
functions. We demonstrate this stability by showing that a single model trained using our regularizers
is resilient against both norm-bounded and perception based adversaries, achieving a new state-of-
the-art for robust accuracy against unseen classes of adversaries. We also achieve state-of-the-art
verified robustness of21% against '∞-bounded perturbations of size E = 8/255 on CIFAR-10.
Critically, computing our regularizers relies only on random sampling, and thus does not require
running an inner optimization loop to find strong perturbations at training time. As such, our
techniques exhibit strong scaling, since they increase batch sizes rather than epochs during training,
allowing us to train an order of magnitude faster than standard adversarial training. This work thus
presents the first regularization-only approach to achieve comparable results to standard adversarial
training against a variety of unseen perturbation models.
9
Under review as a conference paper at ICLR 2022
References
Anish Athalye, Nicholas Carlini, and David Wagner. Obfuscated gradients give a false sense of
security: Circumventing defenses to adversarial examples. arXiv preprint arXiv:1802.00420, 2018.
Mikhail Belkin, Partha Niyogi, and Vikas Sindhwani. Manifold regularization: A geometric frame-
work for learning from labeled and unlabeled examples. Journal of machine learning research, 7
(Nov):2399-2434, 2006.
Nicholas Carlini and David Wagner. Towards evaluating the robustness of neural networks. arXiv
preprint arXiv:1608.04644, 2017.
Nicholas Carlini, Anish Athalye, Nicolas Papernot, Wieland Brendel, Jonas Rauber, Dimitris Tsipras,
Ian Goodfellow, Aleksander Madry, and Alexey Kurakin. On evaluating adversarial robustness.
arXiv preprint arXiv:1902.06705, 2019.
Francesco Croce and Matthias Hein. Provable robustness against all adversarial lp-perturbations for
p ≥ 1. arXiv preprint arXiv:1905.11213, 2019.
Francesco Croce and Matthias Hein. Reliable evaluation of adversarial robustness with an ensemble
of diverse parameter-free attacks. arXiv preprint arXiv:2003.01690, 2020.
Krishnamurthy Dvijotham, Sven Gowal, Robert Stanforth, Relja Arandjelovic, Brendan O’Donoghue,
Jonathan Uesato, and Pushmeet Kohli. Training verified learners with learned verifiers. arXiv
preprint arXiv:1805.10265, 2018.
Logan Engstrom, Andrew Ilyas, Shibani Santurkar, and Dimitris Tsipras. Robustness (python library).
2019. URL https://github.com/MadryLab/robustness.
Ian J. Goodfellow, Jonathon Shlens, and Christian Szegedy. Explaining and harnessing adversarial
examples. arXiv preprint arXiv:1412.6572, 2014.
Sven Gowal, Krishnamurthy Dvijotham, Robert Stanforth, Rudy Bunel, Chongli Qin, Jonathan
Uesato, Relja Arandjelovic, Timothy Mann, and Pushmeet Kohli. On the effectiveness of interval
bound propagation for training verifiably robust models. arXiv preprint arXiv:1810.12715, 2018.
Trevor Hastie, Robert Tibshirani, and Jerome Friedman. The elements of statistical learning: data
mining, inference, and prediction. Springer Science & Business Media, 2009.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Identity mappings in deep residual
networks. arXiv preprint arXiv:1603.05027, 2016.
Matthias Hein and Maksym Andriushchenko. Formal guarantees on the robustness of a classifier
against adversarial manipulation. In Advances in Neural Information Processing Systems, pages
2266-2276, 2017.
Daniel Jakubovitz and Raja Giryes. Improving dnn robustness to adversarial attacks using jacobian
regularization. In Proceedings of the European Conference on Computer Vision (ECCV), pages
514-529, 2018.
Haoming Jiang, Zhehui Chen, Yuyang Shi, Bo Dai, and Tuo Zhao. Learning to defense by learning
to attack. arXiv preprint arXiv:1811.01213, 2020.
Daniel Kang, Yi Sun, Dan Hendrycks, Tom Brown, and Jacob Steinhardt. Testing robustness against
unforeseen adversaries. arXiv preprint arXiv:1908.08016, 2020.
Alex Krizhevsky and Geoffrey Hinton. Learning multiple layers of features from tiny images.
Technical Report 0, University of Toronto, Toronto, Ontario, 2009.
Alexey Kurakin, Ian Goodfellow, and Samy Bengio. Adversarial examples in the physical world.
arXiv preprint arXiv:1607.02533, 2016.
Cassidy Laidlaw and Soheil Feizi. Functional adversarial attacks. arXiv preprint arXiv:1906.00001,
2019.
10
Under review as a conference paper at ICLR 2022
Cassidy Laidlaw, Sahil Singla, and Soheil Feizi. Perceptual adversarial robustness: Defense against
unseen threat models. arXiv preprint arXiv:2006.12655, 2021.
Yann LeCun, Corinna Cortes, and CJ Burges. Mnist handwritten digit database. ATT Labs [Online].
Available: http://yann.lecun.com/exdb/mnist, 2, 2010.
Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and Adrian Vladu.
Towards deep learning models resistant to adversarial attacks. arXiv preprint arXiv:1706.06083,
2017.
Pratyush Maini, Eric Wong, and J Zico Kolter. Adversarial robustness against the union of multiple
perturbation models. arXiv preprint arXiv:1909.04068, 2019.
Matthew Mirman, Timon Gehr, and Martin Vechev. Differentiable abstract interpretation for provably
robust neural networks. In International Conference on Machine Learning, pages 3578-3586,
2018.
David L Phillips. A technique for the numerical solution of certain integral equations of the first kind.
Journal of the ACM (JACM), 9(1):84-97, 1962.
Andrew Slavin Ross and Finale Doshi-Velez. Improving the adversarial robustness and interpretability
of deep neural networks by regularizing their input gradients. arXiv preprint arXiv:1711.09404,
2017.
Hadi Salman, Greg Yang, Huan Zhang, Cho-Jui Hsieh, and Pengchuan Zhang. A convex relaxation
barrier to tight robustness verification of neural networks. In Advances in Neural Information
Processing Systems 32, pages 9835-9846. Curran Associates, Inc., 2019.
Lukas Schott, Jonas Rauber, Matthias Bethge, and Wieland Brendel. Towards the first adversarially
robust neural network model on mnist. arXiv preprint arXiv:1805.09190, 2018.
Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian Goodfellow,
and Rob Fergus. Intriguing properties of neural networks. arXiv preprint arXiv:1312.6199, 2013.
Andrei Nikolaevich Tikhonov, AV Goncharsky, VV Stepanov, and Anatoly G Yagola. Numerical
methods for the solution of ill-posed problems, volume 328. Springer Science & Business Media,
2013.
Vincent Tjeng, Kai Xiao, and Russ Tedrake. Evaluating robustness of neural networks with mixed
integer programming. arXiv preprint arXiv:1711.07356, 2017.
Florian Tramer and Dan Boneh. Adversarial training and robustness for multiple pertur-
bations. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alch6-Buc, E. Fox, and
R. Garnett, editors, Advances in Neural Information Processing Systems 32, pages 5866-
5876. Curran Associates, Inc., 2019. URL http://papers.nips.cc/paper/
8821-adversarial-training-and-robustness-for-multiple-perturbations.
pdf.
Florian Tramer, Nicholas Carlini, Wieland Brendel, and Aleksander Madry. On adaptive attacks to
adversarial example defenses. arXiv preprint arXiv:2002.08347, 2020.
Eric Wong, Frank R. Schmidt, Jan Hendrik Metzen, and J. Zico Kolter. Scaling provable adversarial
defenses. arXiv preprint arXiv:1805.12514, 2018.
Chaowei Xiao, Jun-Yan Zhu, Bo Li, Warren He, Mingyan Liu, and Dawn Song. Spatially transformed
adversarial examples. arXiv preprint arXiv:1801.02612, 2018.
Kai Xiao, Vincent Tjeng, Nur Muhammad Shafiullah, and Aleksander Madry. Training for faster
adversarial robustness verification via inducing relu stability. ICLR, 2019.
Runtian Zhai, Chen Dan, Di He, Huan Zhang, Boqing Gong, Pradeep Ravikumar, Cho-Jui Hsieh,
and Liwei Wang. Macer: Attack-free and scalable robust training via maximizing certified radius.
arXiv preprint arXiv:2001.02378, 2020.
11
Under review as a conference paper at ICLR 2022
Hongyang Zhang, Yaodong Yu, Jiantao Jiao, Eric P. Xing, Laurent El Ghaoui, and Michael I.
Jordan. Theoretically principled trade-off between robustness and accuracy. arXiv preprint
arXiv:1901.08573, 2019.
Stephan Zheng, Yang Song, Thomas Leung, and Ian Goodfellow. Improving the robustness of deep
neural networks via stability training. In Proceedings of the ieee conference on computer vision
and pattern recognition, pages 4480-4488, 2016.
12
Under review as a conference paper at ICLR 2022
A	Experimental methods and hyperparameters
We use a PreActResNet18 model (He et al., 2016) for the CIFAR-10 robustness experiments. We
train using SGD with a batch size of 128 and weight decay γK of 5e-4. We follow Maini et al. (2019)
for our learning rate schedule, which is piecewise linear and starts at 0 and goes to 0.1 over first 40
epochs; 0.005 over the next 40 epochs; and 0 over the final 20 epochs. We increase epsilon from 2 to
8 over epochs 10 to 35. We start the weight γS of the manifold regularization at 0.8 and the weight
γH of the Hamming regularization at 2,400; these increase linearly up to a factor of 10 from epochs
20 to 80. We set the hyperparameter α = 8. We use this set of hyperparamaters for all the CIFAR-10
ablation studies (except when setting γS or γH to 0 for the ablation studies involving the individual
regularizers).
We use a two-layer convolutional neural network for the CIFAR-10 verification experiments, consist-
ing of 2x2 strided convolutions with 16 and 32 filters, then a 128 hidden unit fully connected layer.
This is the same model as used in Wong et al. (2018) and Xiao et al. (2019), except those works use a
100 hidden unit fully connected layer. We use the same schedule for the learning rate and as in the
PreActResNet18 model. The weight γS starts at 0.4 and the weight γH starts at 9000; these increase
linearly up to a factor of 10 from epochs 20 to 80. We use the same hyperparameter α as for the
PreActResNet18 model.
We use the CNN with four convolutional layers plus three fully-connected layers from Carlini and
Wagner (2017) for the MNIST robustness experiments. We use the same schedule for the learning
rate, , γS, and γH as in the PreActResNet18 model (except that scales to 0.3). We use the same
hyperparameter α as for the PreActResNet18 model.
To set the hyperparameters γS and γH on the PreActResNet18 model, we ran a grid search for
γS = 0.2, ..., 1.0 and γH /γS = 100, 200, ..., 500 and selected the settings which yielded the best
robust accuracy against a 20-step '∞ PGD adversary with 10 restarts for E = 8/255 on the full
training set; the range of γH /γS was set such that the corresponding losses were roughly equal for a
randomly initialized, untrained network. For reporting results, we train five models using the selected
hyperparameters and report results using the one with the median performance on the test set against
a 20-step '∞ PGD adversary at E = 8/255 on CIFAR-10 or 0.3 on MNIST. For the PreACtReSNet18
model, robust accuracy over the 5 runs ranged from 40.1% to 41.5% with median 40.5%; clean
accuracy ranged from 66.9% to 72.4% with median 70.0%.
For our stability results, we use the full version of AutoAttack+, an ensemble of attacks proposed
by Croce and Hein (2020), for the '2 and '∞ perturbations. We choose the attack because it is
parameter-free, which reduces the possibility of misconfiguration; empirically it is at least as strong
as standard PGD, and has been successful in breaking many proposed defenses. For the Spatial,
Recolor, JPEG, and NPTM adversaries, we used the public implementation by Laidlaw et al. (2021).
For comparing with standard PGD, we use a 20-step PGD adversary with 10 random restarts and a
step size of 2.5 ∙ e/20 as implemented by Engstrom et al. (2019). For verification, We adopt the setup
of Xiao et al. (2019), using the MIP verifier of Tjeng et al. (2017), with solves parallelized over 8
CPU cores and the timeout set to 120 seconds.
B Additional theoretical perspective on the proposed
REGULARIZERS
Section 5.3, and in particular, Equation 14, suggests computing the regularization term between
all pairs of (perturbed) input points. We emphasize that this development is intended to provide a
theoretically rigorous characterization of the behavior of our regularization functional in the ideal
case when the number of samples goes to infinity. These results provide justification to the intuition
that our proposed regularizers indeed penalize functions with respect to both the outputs and decision
boundaries via a precise formulation as the gradient of an auxilliary function over the input manifold.
In contrast, the proposed regularizers (given in Equations 16 and 17) take the sum over pairs of
perturbations. In this section, we present some arguments that, in the setting of our experiments,
our regularizers can be viewed as a sparsified version of the dense regularizer considered in the
previous section. The main observation is that, if the data is sufficiently sparse, the resampled
Laplacian will consist of two types of edges: very short edges between points resampled from the
13
Under review as a conference paper at ICLR 2022
same -neighborhood, and very long edges between points sampled from different -neighborhoods.
For an appropriate choice of the scale factor s, the exponential form of the heat kernel causes the
weights on the long edges to fall off very quickly compared to the short edges. In fact, it is standard
when computing such graph-based quantities to sparsify the Laplacian by truncating the weights
using the k-nearest neighbors, or within some -ball.
The following results gives precise bounds for this approximation of the regularizer in Equation 14
under a certain separation assumption in the data:
Proposition B.1. Consider a function f anda set of points {xi}iN=1 such that the pairwise differences
bounded from above and below by 0 < c1 ≤ ||f (xi) - f (xj)||22 ≤ c2 < ∞ for all i 6= j. Let s be
the parameter of the heat kernel ks = exp(-||xi - xj ||2 /s). Assume further that the points {xi} are
separated in the sense that there exist at least O(n) pairs ofpoints (xii,xi2),i ∈ S whose distances
are boundedfrom above by √S, and the remainder of the points have distances boundedfrom below
by m√s for some constant m > L Writing R(L) := PNj=ι(f (Xi) — f (Xj))2Lij for the dense
regularizer and R(L) for the regularizer with weights truncated to an -ball, we have that
R(L) — c2N2 exp(—m2) ≤ R(L) ≤ R(L) ≤ (1 + b-1)R(L)	(19)
where b = Θ((c1 /c2)(exp(m2)/N)).
Proofof Proposition B.1. We introduce Le := L — Le, which is just the LaPlacian on the complement
of the neighborhood subgraph, i.e., the subgraph whose edges are at least . Clearly R(L) =
R(Le) + R(Le). Then we have the following bounds:
R(Le) = X (f (Xi) — f(xj))2Li,j ≤ c2N2 exp(—m2)	(20)
i,j6∈S
R(Le) = X (f(Xi) — f(Xj))2Li,j ≥ ac1N exp(—1)	(21)
i,j∈S
where the a is a constant introduced for the O(N) edges in the graph of Le.
Take b = (ac1/c2)(exp(m2 — 1)/N). A simple rearrangement gives c2 = (b-1ac1)(exp(m2 —
1)/N), thus we have that
R(L) = R(Le) + R(L e)	(22)
≤ R(Le) + c2N2 exp(—m2)	(23)
≤ R(Le) + (b-1ac1)(exp(m2 — 1)/N)N 2 exp(—m2)	(24)
= R(Le) + b-1ac1Nexp(—1)	(25)
≤ R(Le) + b-1R(Le)	(26)
= (1 + b-1)R(Le)	(27)
Then the first inequality follows from (23), the second inequality is trivial, and the third inequality
follows from (22)-(27).	□
Discussion In this work, we only consider bounded functions f (i.e., the output of a softmax layer,
and the 0-1 activations of a neural network), which allows us to apply this proposition. This result
gives two bounds on the dense regularization R(L) in terms of R(Le). The first inequality says that
the absolute approximation error is bounded by a term which vanishes unless the squared sample
count N2 grows as the exponential of the squared separation m2. The third inequality gives a tighter
bound on the relative error in a term that vanishes unless the linear sample count N grows as the
exponential of the squared separation m2, but requires that we have good control over the ratio c1/c2.
Note that the dependence on c1 /c2 is unavoidable in the sense that a function which is very smooth in
local neighborhoods of size s will necessarily have large relative contribution from the longer edges;
however, in this case we still have a bound on the absolute contribution (though we trade the ratio
c1 /c2 for a factor of N). Crucially, in both cases the error bound depends on an exponential factor
of the squared separation m2, which, under the curse of the dimensionality, grows with the input
dimension. Note that the assumption that there is a linear number of close points is satisfied by our
resampling procedure.
14
Under review as a conference paper at ICLR 2022
Table 4: Dense vs. sparse regularizer, CIFAR-10 against an '∞ PGD-adversary at E = 8/255.
Regularizer	Test Accuracy (%)	
	Robust	Clean
Sparse	40.54	69.95
Dense	35.04	64.08
Next, to provide some empirical support that the separation m can be made nontrivial, we offer the
following statistics from CIFAR-10, the main setting of our experiments. We selected a random
subset of 10,000 (20%) training samples. For each of these points, we found the closest point within
the remaining 49,999 training samples, and computed the distance. Using the `2 metric, we found
a mean distance of 9.597, a median distance of 9.405, and the distance at the 10th percentile to be
5.867. Conversely, in our experiments, We sample points in an '∞ ball of radius E = 8/255, which
is contained in an '2 ball of radius √3 ∙ 32 ∙ 32 ∙ 8/255 = 1.739. In fact, only 21 training samples,
or 0.21% of the random subset, have a partner closer than twice the perturbation bounds. We note
that these statistics also assume training over the entire dataset, rather than the Stochastic Gradient
Descent algorithm commonly used in deep learning, which yields batches of data that are almost
certainly sparse. Furthermore, as the dimension of the inputs grows, we would expect the distance
between pairs of input points in the training distribution to increase exponentially; indeed, this is one
of the manifestations of the oft-cited curse of dimensionality (Hastie et al., 2009).
Finally, we find in our experiments that the dense regularizer of Equation 14 performs worse than the
sparse regularizer in practice, which agrees with the intuition that the pairwise distances between
input points is too large to provide any additional regularization benefit (and in fact, actually slightly
hurts performance due to an over-regularization effect). Table 4 presents a summary of these results.
Our regularizer is also more efficient to compute by a linear factor, and since all the pairwise distances
are constant, we can drop the Laplacian weights entirely.
15