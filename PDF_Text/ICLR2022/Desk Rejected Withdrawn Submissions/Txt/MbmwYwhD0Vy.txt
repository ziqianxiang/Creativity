Under review as a conference paper at ICLR 2022
A Novel Convergence Analysis for the
Stochastic Proximal Point Algorithm
Anonymous authors
Paper under double-blind review
Ab stract
In this paper, we study the stochastic proximal point algorithm (SPPA) for general
empirical risk minimization (ERM) problems as well as deep learning problems.
We present an efficient implementation of SPPA with minor modification for
different problem definitions and we observe that efficiently implemented SPPA
has faster and more stable convergence than the celebrated stochastic gradient
descent (SGD) algorithm, and its many variations, for both convex and non-convex
problems. Due to the fact that per-iteration update of SPPA is defined abstractly
and has long been considered expensive, its convergence proof has not been well-
studied until recently. In this paper, we close the theoretical gap by providing its
convergence for convex problems. Our proof technique is different from some
of the recent attempts. As a result, we present a surprising result that SPPA
for convex problems may converge arbitrarily fast, depending on how the step
sizes are chosen. As a second contribution, we also show that for some of the
canonical ERM problems and deep learning problems, each iteration of SPPA
can be efficiently calculated either in closed form or closed to closed form via
bisection—the resulting complexity is exactly the same as that of SGD. Real data
experiments showcase its effectiveness in terms of convergence compared to SGD
and its variants.
1	Introduction
It has been widely accepted that when training large-scale machine learning models, the training
algorithm should act in a sample-by-sample manner in order to reduce computational and memory
overhead—the size of the data set may be too large that calculating the full gradient information is
too costly. Moreover, most machine learning problems does not have to be solved with very high
accuracy, since the ultimate goal is not to fit the training data but rather to generalize the algorithm
well such that the performance is decent on unseen data.
Most existing stochastic algorithms are based upon the stochastic gradient descent (SGD) framework
(Bottou et al., 2018). SGD is extremely easy to implement and provides asymptotic convergence,
although the convergence rate is generally slow (and subject to careful choice of step sizes). Various
approaches have been proposed to accelerate the plain vanilla SGD. Reducing the variance of the
stochastic gradient and introducing adaptive learning schemes are two main lines of research. SVRG
(Johnson & Zhang, 2013) and SAGA (Defazio et al., 2014) (and their follow-up works such as
(Defazio, 2016)) focus on reducing the variance of the stochastic gradient descent, at the cost of
increased time or memory complexities (to the order of the entire data set). On the other hand,
AdaGrad (Duchi et al., 2011) and Adam (Diederik P. Kingma, 2014) introduce adaptive learning
schemes and effectively keep the algorithm fully stochastic and light-weight. Besides these practical
improvements, theoretical progress has been made by on quantifying the best possible convergence
rate using first-order information (Lei et al., 2017; Allen-Zhu, 2017; 2018a;b).
1.1	Stochastic proximal point algorithm (SPPA)
In this paper, we explore a different type of stochastic algorithm called the stochastic proximal point
algorithm (SPPA), also known as stochastic proximal iterations (Ryu & Boyd, 2014) or incremental
proximal point method (Bertsekas, 2011a;b). Consider the following optimization problem with the
1
Under review as a conference paper at ICLR 2022
objective function in the form of a finite sum of component functions
minimize
w ∈Rj
1 n
一，Ifi (W) = F (W).
n占
(1)
SPPA takes the following simple form:
Algorithm 1 Stochastic proximal point algorithm (SPPA)
1:	repeat
2:	randomly draw it uniformly from {1,...,n}
3:	Wt+ι — argminw 儿 fit (W) + (1∕2)kW - Wt k2 = Prox及龙(Wt)
4:	until convergence
Line 3 of Algorithm 1 calculates the proximal operator of the function λt力 evaluated at Wt, denoted
as ProxAt ft (Wt). This is the stochastic version of the proximal point algorithm, which dates back to
Rockafellar (1976).
SPPA has an abstract per-iteration update rule and it acquires more information from the problem than
solely the first order derivatives, which makes it not as universally applicable as SGD. Yet, thanks to
the more information inquired, it is possible to obtain faster and more robust convergence guarantees.
While some ‘accelerated’ versions of SGD demand additional time or space overhead to go over the
entire data set, SPPA does not have any overhead and it is suitable to be used in a completely online
setting, it performs well even the data samples are not revisited again.
To the best of our knowledge, people started studying the convergence behavior of SPPA only
recently (Bertsekas, 2011a; Ryu & Boyd, 2014; Bianchi, 2016; Patragcu, 2020; Toulis et al., 2021).
Somewhat surprisingly, the convergence analysis of SPPA draws little resemblance to its deterministic
counterpart, the proximal point algorithm. This is unlike the case for SGD, of which the convergence
analysis follows almost line-by-line to that of the subgradient method. Moreover, existing analysis
of SPPA shows no improvement in terms of convergence rate, which seems counter-intuitive due
to the nature of the updates. Most authors also accept the premise that the proximal operator is
sometimes difficult to evaluate, and thus proposed variations to the plain vanilla version to handle
more complicated problem structures (Wang & Bertsekas, 2013; Duchi & Ruan, 2018; Asi & Duchi,
2019; Davis & Drusvyatskiy, 2019).
1.2 Contributions
The main contribution of this paper is to provide a completely novel convergence analysis of SPPA
for general convex problems. This contribution, together with the efficient implementation strategies
discussed in the appendix, results in great practical results in almost all of the classical empirical risk
minimization (ERM) problems in large-scale machine learning. While there exists some convergence
results of SPPA for convex problems, the novel analysis provided in this work requires minimal
assumptions (nothing but the convexity of the loss functions). The new convergence analysis also
shows great resemblance to its deterministic counterpart, which is not the case from other works.
In the appendix we will discuss how to efficiently compute the abstract per-iteration update rule for
SPPA with complexity that is comparable to SGD-type methods. An interesting observation is that in
a lot of ERM examples the resulting update has a similar form like SGD, but with a smartly chosen
step size derived from the proximal update. For a class of smooth risks such as the logistic loss,
we appeal to reformulate the optimization problem such that we get close to closed form solution
via bisection. We also briefly discuss how to modify the algorithm to have the ability to handle
nonsmooth regularization terms such as the L1 norm, using the stochastic Douglas-Rachford splitting
approach.
Finally, we apply SPPA with efficient implementations to a large variety of classification and
regression problems. Numerical experiments are conducted not only for linear classification and
regression (in the appendix) models, but also nonconvex deep neural network problems. Although
the convergence analysis provided in this paper does not cover nonconvex cases, empirical results
suggest that it is still worth treating SPPA as an effective alternative for deep learning.
2
Under review as a conference paper at ICLR 2022
2 Convergence Analysis
In this section we provide convergence analysis of SPPA for general convex loss functions equation 1
to a global minimum in expectation. In recent years there have been some work tackling the same
problem, e.g., Bertsekas (2011a); PatraScU (2020); ToUlis et al. (2021). In this paper, however,
we provide new convergence analysis that is much easier to understand while requiring nearly no
assUmptions other than convexity. For this reason we believe the theoretical contribUtion is significant
enoUgh as it broadens the applicability of SPPA.
There is a well-known resemblance between proximal methods and gradient descent, assUming the loss
function is differentiable: while a full gradient descent step takes the form Wf+1 = Wf - λtNF(Wf),
the definition of a full proximal step guarantees that 也VF(wt+ι) = Wt+1 - Wt, meaning that
Wf+ι = Wf - λtVF(Wf+ι). Therefore, one might expect that a well-established convergence analysis
of SGD for nonconvex problems, for example (Bottou et al., 2018, §4.3), can be seamlessly applied
to SPPA. However, when applied in a stochastic fashion, the situation is a little more complicated.
Consider E[V力(Wf) | Wf], where the expectation is taken over the sampling procedure conditioned
on Wf, for SGD we typically require V力(Wf) to be an unbiased estimator of Vf(Wf), which is easy
to satisfy if if is uniformly sampled from {1,...,n} given Wf. For SPPA then one needs to consider
E[Vfi(Wf+ι) | Wf], again over the sampling procedure conditioned on Wf. This is in fact difficult
to quantify because the update Wf+1 depends on the sample that is drawn from the data set. The
equation E[V力(Wf+ι) | Wf] = VF(Wf+ι) does not make sense because Wf+ι on the right-hand-side is
still random conditioned on Wf. It is for this reason that existing analysis of SPPA differs drastically
from its deterministic counterpart PPA (Bertsekas, 2011a; Ryu & Boyd, 2014; Bianchi, 2016).
What we can show, however, is that the distribution of if is still uniform conditioned on Wf+1 instead
of Wf , as formalized in the following proposition.
Lemma 1. At every iteration of SPPA (Algorithm 1), we have the conditional probability
P (it | Wf+ι) = 1/n.
The proof of Lemma 1 is relegated to the supplementary material. What Lemma 1 suggests is that
given the current iterate Wf+1 , without knowing its predecessors Wf and beyond, every component
function 力 is equally likely to be picked as the proximal update that leads to Wf+ι. In other words,
conditioned on the current iterate without knowing the past, we do not gain additional information
about which component function 力 is more likely to have been selected. As it turns out, Lemma 1
significantly simplifies the following convergence analysis while showing resemblance to that of
deterministic PPA. What is more, the resulting analysis requires no assumptions other than convexity:
typical assumptions such as Lipschitz smoothness or bounded variance are not required.
Remark. Lemma 1 holds for t = 0 if the initialization w° is random, and the distribution from
which it is drawn has the sample space the same as the domain of the objective function equation 1.
In practice W0 is usually drawn from a distribution that is independent from the cost function, say
N(0, 7). In this case, for a given value of w ι, there is no chance of eliminating possible w°'s because
any corresponding W0 is in the sample space of the initialization distribution.
2.1	Generic convex case without strong convexity
Under Lemma 1, we have the following proposition, which serves as the stepping stone for our main
convergence results.
Proposition 1. Suppose each fι,...,fn is convex, then at iteration t ofSPPA (Algorithm 1)we have
2儿(F(Wf+ι) - F(W★)) ≤ E[kWf - w^k2 | Wf+ι]-∣∣Wf+ι - w^∣∣2,	(2)
where w^ denotes an optimal solution of equation 1.
Proof. We start by the equation
kWf - W^k2 = kWf - Wf+1 + Wf+1 - W^k2
=kWf+1 - W^k2 + 2(Wf - Wf+ι)>(Wf+1 - WQ + kWf - Wf+ι∣∣2.	(3)
3
Under review as a conference paper at ICLR 2022
According to the definition Wf+1 = argmin^ λt fit (W) + (1∕2)kW - w∕∣2, We know that Wf - ^t+ι is
a subgradient of 也 fit at Wt+1. Therefore
儿 fit (Wt+1) + (Wt - Wt + ι)>(W - Wt + 1)≤ λi fit (W)
at any w. Let W = w^ and substitute it in equation 3, we have
kWt - W^k2 ≥ kWt+1 - W^k2 + 22t(fit(Wt+1)- fit(W大)) + kWt - Wt+11∣2
≥ kWt+1 - W J∣2 + 22t(fit(Wt+1) - fit(W★)).
Finally, taking condition expectations on both sides given Wt+1 (but not Wt nor it)
E[kWt - w^k2 | Wt+1] ≥ kWt+1 - w^k2 + 22tE[(fi(Wt+1) - fi(WQ | Wt+1].
According to Lemma 1, the conditional distribution over it is uniform, thus
E[fit (wt+1) - fit (WQ | Wt+1] = F(Wt+1) - F(wQ,
and we obtain equation 2.
With the help of Proposition 1, the rest of the results follows straight-forwardly.
Theorem 1. Ifall f1,...,fn are convex, and an initialization w° is Pickedfrom a distribution with the
sample space the same as the domain of F, then the sequence {Wt } generated by SPPA (Algorithm 1)
satisfies
Iim inf E[F(Wt)] - F(w*) ≤，EkWo - w*k2,	(4)
T →∞ t ≤τ	2 工 t=1 ^t
where w* denotes an optimal solution ofequation 1. The term Ek Wo - w*∣∣2 is a COnStantfOr any
approach of initializing W0.
Proof. Taking total expectation over equation 2, we have
2λt (E[F(Wt+1)] - F(w*)) ≤ E[kWt - w*∣2] - E[kWt+1 - w*k2].
Sum over all t = 1,2,...,T, we have
τ
22 λt (E[F(wt+1)] - F(w*)) ≤ EkWO- w*∣∣2 - EkWT - w*∣∣2 ≤ EkWo - w*∣∣2.
t=1
On the left-hand-side we have (by definition) inf E[F(Wt)] ≤ E[F(w丁)] for any τ. Applying that,
dividing both sides by 2 Et λt, and letting T → ∞, we get equation 4.	□
Remark. The proofs here take conditional expectations backwards, i.e., conditioned on Wt+1 and
average over the previous iteration. This may be a little counter-intuitive. However, there is nothing
wrong mathematically—expectation is a linear operator so E[A + B] = E[A] + E[B] under all
circumstances; if an inequality f (ɪ) ≤ g(ɪ) holds for all values of X in its sample space, then
E [ f (x)] ≤ E[g(x)] for any distribution over X since it is just a nonnegative sum on both sides.
Theorem 1 states a generic results on the convergence of SPPA. The left-hand-side of equation 4 is,
by definition, nonnegative; the right-hand-side, however, goes to zero if the infinite sum St At → ∞.
This implies that the infimum if E[F(Wt)] goes to zero for appropriately chosen step sizes. Somewhat
surprisingly, the only assumption we made was that the functions are convex. The celebrated SGD, on
the other hand, requires at least two more assumptions: Lipschitz smoothness and that the stochastic
gradients have bounded variance. What is more, the flexible choice of At means that we can make
the convergence rate arbitrarily fast, by allowing At to be increasing (rather than decreasing in most
gradient-based methods). Of course this may lead to large variance of the sequence, which we may
want to avoid in practice.
Here we provide the convergence rate for two commonly used step size rules.
Corollary 1. Ifall f1,...,fn are convex, then the sequence {wj generated by SPPA (Algorithm 1)
with a constant step size At = A satisfies
inf E[F(Wt)] - F(w*) ≤ ɪ k Wo - w*k2,	(5)
t ≤T	2AT
where W* denotes an optimal solution of equation 1.
4
Under review as a conference paper at ICLR 2022
The proof is straight-forward by substituting λt with λ in equation 4. This shows that a constant step
size (regardless of its value) gives a O(1∕T) sublinear convergence rate.
Corollary 2. Ifall fι,...,fn are convex, then the Sequence {wj generated by SPPA (Algorithm 1)
with an increasing step size rule λt = tλ satisfies
inf E[F(Wt)] — F(w*) ≤ 亚(T + 1)kwo - w*k2,	(6)
where w* denotes an optimal solution of equation 1.
The proof comes from the arithmetic series 1 + 2 + ∙∙∙+ T = T (T + 1)/2. This shows thatthe increasing
step size rule λt = tλ gives a O(1/T2) sublinear convergence rate, the same as Nesterov’s optimal
gradient algorithm (2013).
While the expected convergence looks excellent, we notice that in practice the performance also
depends on the variance of the sequence generated by SPPA, which could be large if the expected
convergence rate is too fast. We can even have linear convergence rate by letting λt increase
exponentially, but the variance would be so large that it makes little practical sense. As we will see in
the experiment section, a constant step size rule still gives the best performance in most cases.
2.2 Strongly convex case
Similar to most other algorithms, convergence can be significantly improved if the loss functions are
strongly convex.
Proposition 2. Suppose each fι,...,fn is Strongly convex with parameter μ, and an initialization
w0 is picked from a distribution with the sample space the same as the domain of F, then at iteration
t of SPPA (Algorithm 1) we have
2λt (F(Wt+ι)- F(w*)) ≤ E[kWt — w*k2 | Wt+ι]-(1 + λt/μ)kWt+ι — w*∣∣2,	(7)
where w* denotes an optimal solution of equation 1.
Proof. Following equation equation 3, We again have that Wt — wt+ι is a subgradient of fit, which
satisfies the following inequality if it is strongly convex
λtfit(Wt+ι) + (Wt — Wt+ι)>(w — Wt+ι) + λt-IlW — wt+ιk2 ≤ λifit(w)
t	2μ	t
at any W. Let W = W* and substitute it in equation 3, we have
k Wt — W*k2 ≥ k Wt+1 — W*k2 + 2λt (fit (Wt+l) — fit (W*)) + : k W — Wt+1 k2 + k Wt — Wt+1 k2
≥ (1 + λt∕μ)kwt+1 - w*k2 + 2λt(fit(Wt+ι) - fit(W*)).
Taking condition expectations on both sides given Wt+ι (but not Wt nor it) and invoking Lemma 1,
we obtain equation 7.
The main convergence of SPPA for strongly convex functions is the following.
Theorem 2. IfaU fι,...,fn are strongly convex with parameter μ, then the sequence {wt} generated
by SPPA (Algorithm 1) with constant step sizes λt = λ satisfies
inf E[F(wt)] - F(w*) ≤	1-μZ)	EkWo - w*k2,	(8)
t≤Z	(1 + λ∕μ)z - 1
where W* denotes an optimal solution of equation 1. The term EkWo - W*k2 is a constant for any
approach of initializing Wo. For large enough T, the denominator on the right-hand-side of equation 7
is almost (1 + λ∕μ)Z , which indicates a linear convergence rate.
Proof. Letting λt = λ, taking total expectation over equation 7 and multiplying both sides by
(1 + λ∕μ)t, we have
2λ(1 +λ∕μ)t (E[F(Wt+1)] - F(W*)) ≤ (1 +λ∕μ)tE[kWt -W*k2] - (1 +λ∕μ)t+1E[kWt+1 - W*k2].
5
Under review as a conference paper at ICLR 2022
Sum over all t = 1,2,...,T, We have
T
2λ 2(1 +λ∕μ)t (E [F (κt+ι)] - F (w*)) ≤ Ek w0-w* k2-(1+，/〃 )t Ek WT-w* k2 ≤ Ek w0-w* k2.
t=1
On the left-hand-side we have (by definition) inf E[F(Wt)] ≤ E[F(WT)] for any τ. Replacing every
E[F(Wt)] With the infimum and applying the geometric series
T
2(I + MW =
t=0
(I + 2/〃)T - 1
入/林
we get equation 8.

Final remark. In this section, we provided convergence of SPPA in expectation, following the
strategies utilized in the tutorial (Bottou et al., 2018) which is easy to understand. In the supplementary
we also provide a stronger almost sure convergence using Martingales theorem.
2.3 Extension to stochastic Douglas-Rachford Splitting
In many cases the ERM formulation includes a regularization term g (W) as
minimize
w ∈Rj
1 Λ
N fi (w) + g(W).
n W
(9)
The most widely adopted regularization include the Tikhonov regularization PkWk2, the lasso
regularization PkW∣∣ι to promote sparsity, and the group-lasso regularization to promote group-
sparsity. The Tikhonov regularization can easily be incorporated into the ERM parts without affecting
the efficiency of the proximal updates. However, for the nonsmooth ones it may not be ideal to spread
it into the stochastic terms as 力(w) + g(w). We propose to modify the Douglas-Rachford splitting in
order to handle situation like this, which keeps track of two sequences {wt} and {Wt}:
Algorithm 2 Stochastic Douglas-Rachford splitting (SDRS)
1:	repeat
2:	Wt+ι — Prox 及 g (W t)
3:	randomly draw it uniformly from {1,...,n}
4:	Wt+ι - Wt + ProxAffit (2wt+ι — Wt) — wt+ι
5:	until convergence
There has been some attempts to study the convergence of SDRS in the name of stochastic ADMM
(Ouyang et al., 2013; Zhong & Kwok, 2014; Huang et al., 2019) or online ADMM (Wang & Banerjee,
2012). However, most of them do not consider a full proximal update but rather a ‘linearized’ update
similar in the form ofa (stochastic) gradient step; Wang & Banerjee (2012) considers the full proximal
update but in an online setting using regret bounds to prove convergence, which resulted in a slower
convergence rate as is typical comparing incremental verses stochastic methods.
3	Experiments
We show some real data experiments to demonstrate effectiveness of the proposed SPPA implementa-
tions in linear regression and binary classification problems. We compare our proposed algorithms
with SGD and its three variants: SGD with momentum (Sutskever et al., 2013), AdaBelief (Zhuang
et al., 2020), and Adam (Diederik P. Kingma, 2014), with the default settings suggested in their
original papers. We used the PyTorch (Paszke et al., 2019) implementation of the algorithms and used
the same platform for implementing our customized optimization algorithm. We ran SPPA with our
proposed implementations with various λ values among {0.001,0.005,0.01,0.05,0.1,0.5,1}. The
performance was consistent with different settings. We ran all the experiments with 100 different
seeds, 0-100, to show the robustness of the algorithms to different stochastic settings. We ran all the
experiments in Google Colab Pro environment. 1
1https://colab.research.google.com/notebooks/intro.ipynb
6
Under review as a conference paper at ICLR 2022
Figure 1: SVM on Bank Note Authentication:
regularized hinge loss per seconds, batch size 1
Figure 2: SVM on Bank Note Authentication:
classification accuracy per seconds, batch size 1
3.1	Linear classification
We perform binary classification using SVM and logistic regression. Our aim is to show the
performance of our proposed SPPA algorithms compared to other state of the art optimization
algorithms using two different datasets, Bank Note Authentication and IMDB. The inner loop in
SPPA algorithm is run at most 20 times for the results presented below.
Bank Note Authentication Dataset. The bank note authentication dataset is a publicly available
dataset in UCI machine learning repository Dua & Graff (2017). Data consists of genuine and
counterfeit banknote images. The dataset has 1372 instances. Each instance has 5 attributes out of
which 4 are the features and one is the target attribute. The target attribute contains two values: 0
and 1, where 0 represents genuine note and 1 represents fake note. The ratio of the two classes are
balanced 55/45 (genuine/counterfeit). We pre-process the data to have target values to be -1 and 1. We
perform classification using SVM and logistic regression. Considering the per iteration complexity
of SPPA, we plotted the loss-time, accuracy-time figures, Figure 1 and Figure 2 respectively. The
Figure 1 demonstrates how the regularized hinge loss value decreases with respect to time (seconds)
using different algorithms. The Figures 1 and 2 show that with our efficient implementation, SPPA
takes less amount of time to reach to a smaller loss, and a greater accuracy despite of its per iteration
complexity. The range shown in the plots clearly shows that SPPA is more robust to change in
stochastic settings.Due to lack of space we present further experiments on the performance of SPPA
using logistic regression on Bank Note Authentication dataset in supplementary.
IMDB Dataset. IMDB is large movie review dataset, used for binary sentiment classification Maas
et al. (2011). There are 25,000 movie reviews for training, and 25,000 movie reviews for testing.
We used the processed bag of words format to run our experiments. The ratio of the two classes are
balanced, 50/50 (positive sentiment/negative sentiment). The target attribute contains two values:
-1 and 1, where -1 represents negative sentiment whereas 1 represents positive sentiment. Similar
to Bank Note Authentication dataset, we perform classification using SVM and logistic regression
with IMDB dataset. SPPA outperforms the other algorithms when we use logistic regression to do
the classification. Surprisingly, SGD Momentum diverged with this particular data set for logistic
regression problem. Even though SGD Momentum performs similar to our proposed SPPA in most
cases, this experiment shows that it is not as stable as SPPA. We tried different parameter settings for
all algorithms and the presented results are the best performance of each algorithm. The Figure 3
shows the logistic loss, and the Figure 4 shows the accuracy per seconds. Focusing on the Figure 3,
we see that the logistic loss follows a decreasing trend with SPPA whereas with other algorithms the
decrease is not too visible. Moreover, Figure 4 shows that SPPA reaches to a higher test accuracy
compared to the other algorithms.The performance of SPPA is similar, but not particularly better
than the state of the art methods when we use SVM. Related experiment results are available in
supplementary.
7
Under review as a conference paper at ICLR 2022
5 0 5 0 5 0
8 8 7 7 6 6
A。SrDX
Figure 3: Logistic Regression on IMDB: loss per Figure 4: Logistic Regression on IMDB: classifi-
seconds	cation accuracy per seconds
O IOOOO 20000 30000 40000 50000 60000
Stochastic Update
Figure 5: CNN on CIFAR10: cross entropy loss
on test data
O IOOOO 20000 30000 40000 50000 60000
Stochastic Update
Figure 6: CNN on CIFAR10: prediction accuracy
on test data
3.2	Deep neural net classification
CIFAR10. CIFAR10 (Krizhevsky et al., 2010) consists of 50,000 training and 10,000 test images
of size 32 × 32 in 10 classes. The network we use for this experiment is based on convolutional deep
neural network. It consists of 5 layers, each of the first 2 being a combination of 5 × 5 convolutional
filters and 2 × 2 max pooling and last 3 being fully connected. The loss function is cross entropy
loss, and the activation function is RELU. Batch size is 4. To show the behaviour of the algorithms
more in detail we recorded the loss values at every 500 stochastic update, the experiment is run for 5
epochs and in total 125 updates are presented (25 × 500 × 4 giving the number of training samples).
The accuracy is calculated on the test data at every 500 stochastic update overall for 5 epochs, using
the accuracy calculation in PyTorch tutorial. Convergence plots are shown in Figures 5 and 6. To
show that SPPA works not only on the CNN architecture, we also tried training a residual neural
network (ResNet) on CIFAR10. For the details about the network, one can refer to 20 layer ResNet
architecture on CIFAR10 (He et al., 2016). In our settings the batch size is 32. The results are shown
in Figures 7 and 8. As we can see, the performance is indeed consistent as in the CNN case.
MNIST. MNIST Handwritten Digits Data set (LeCun et al., 2010) is a common data set used for
showing the effectiveness of many state of art the algorithms. It consists of 60,000 training, 10,000
test images of size 28 × 28 in 10 classes. The network we used for this experiment is based on
convolutional deep neural network. It consists of 5 layers, each of the first 3 being combination of
3 × 3 convolutional filters and 2 × 2 max pooling with stride of 2 and last 2 being fully connected. The
loss function is cross entropy loss, and the activation function is RELU. Batch size is 64. To show the
behaviour of the algorithms more in detail we recorded the loss values at every 25 stochastic update,
the experiment is run for 1 epoch and in total 38 updates are presented (25×38×64 giving the number
of training samples). The accuracy is calculated on the test data at every 500 stochastic update, using
the accuracy calculation in PyTorch tutorial. * 2 As observed in Figures 9 and 10, SPPA-based methods
2
2 https://github.com/pytorch/tutorials/blob/master/beginner_source/
blitz/cifar10_tutorial.py
8
Under review as a conference paper at ICLR 2022
Figure 7: ResNet on CIFAR10: cross entropy loss,
batch size 32
Oooooooo
87654321
Figure 8: ResNet on CIFAR10: prediction accu-
racy, batch size 32
Figure 9: CNN on MNIST: cross entropy loss,
batch size 64
Figure 10: CNN on MNIST: prediction accuracy,
batch size 64
outperform all SGD-based algorithms in terms of both the cross entropy loss and prediction accuracy
on the test set.
4	Conclusion
In this paper we presented efficient implementations of the stochastic proximal point algorithm
(SPPA) for general ERM problems and selected deep learning problems. We showed that for
each of the ERM problem, one iteration of SPPA can be expressed in closed form or even as a
simple optimization algorithm that can be easily solved by bisection method. We also proved
that with our proposed efficient implementation, SPPA has same order of complexity as SGD.
There do exist some convergence results of SPPA for convex problems different than ours, but our
approach require only convexity of loss functions. The new convergence analysis also shows great
resemblance to its deterministic counterpart, which is not the case from other works. The resulting
algorithm is more robust in different stochastic settings and has cheap per-iteration complexity, while
enjoying convergence under milder conditions. We demonstrated the performance of SPPA with our
proposed efficient implementation on some well-known classification and regression data sets for
ERM problems, and showed that they outperform many existing methods.
References
Zeyuan Allen-Zhu. Katyusha: The First Direct Acceleration of Stochastic Gradient Methods. In
STOC, 2017.
Zeyuan Allen-Zhu. Katyusha X: Practical Momentum Method for Stochastic Sum-of-Nonconvex Op-
timization. In Proceedings of the 35th International Conference on Machine Learning, ICML ’18,
2018a.
Zeyuan Allen-Zhu. How To Make the Gradients Small Stochastically. In Proceedings of the 32nd
Conference on Neural Information Processing Systems, NeurIPS ’18, 2018b.
9
Under review as a conference paper at ICLR 2022
Nuno Antonio, Ana de Almeida, and Luis Nunes. Hotel booking demand datasets. Data in brief, 22:
41-49, 2019.
Hilal Asi and John C Duchi. Stochastic (approximate) proximal point methods: Convergence,
optimality, and adaptivity. SIAM Journal on Optimization, 29(3):2257-2290, 2019.
Dimitri P Bertsekas. Incremental proximal methods for large scale convex optimization. Mathematical
programming, 129(2):163, 2011a.
Dimitri P Bertsekas. Incremental gradient, subgradient, and proximal methods for convex optimiza-
tion: A survey. Optimization for Machine Learning, 2010(1-38):3, 2011b.
Pascal Bianchi. Ergodic convergence of a stochastic proximal point algorithm. SIAM Journal on
Optimization, 26(4):2235-2260, 2016.
Leon Bottou, Frank E Curtis, and Jorge NocedaL Optimization methods for large-scale machine
learning. SIAM Review, 60(2):223-311, 2018.
Damek Davis and Dmitriy Drusvyatskiy. Stochastic model-based minimization of weakly convex
functions. SIAM Journal on Optimization, 29(1):207-239, 2019.
Aaron Defazio. A simple practical accelerated method for finite sums. Advances in neural information
processing systems, 29:676-684, 2016.
Aaron Defazio, Francis Bach, and Simon Lacoste-Julien. Saga: A fast incremental gradient method
with support for non-strongly convex composite objectives. In Advances in Neural Information
Processing Systems, pp. 1646-1654, 2014.
Jimmy Ba Diederik P. Kingma. Adam: A method for stochastic optimization. In Proceedings of the
3rd International Conference on Learning Representations, 2014.
Dheeru Dua and Casey Graff. UCI machine learning repository, 2017. URL http://archive.
ics.uci.edu/ml.
John Duchi, Elad Hazan, and Yoram Singer. Adaptive subgradient methods for online learning and
stochastic optimization. Journal of Machine Learning Research, 12(Jul):2121-2159, 2011.
John C Duchi and Feng Ruan. Stochastic methods for composite and weakly convex optimization
problems. SIAM Journal on Optimization, 28(4):3229-3259, 2018.
Hadi Fanaee-T and Joao Gama. Event labeling combining ensemble detectors and background
knowledge. Progress in Artificial Intelligence, pp. 1-15, 2013. ISSN 2192-6352. doi: 10.1007/
s13748-013-0040-3. URL [WebLink].
Simon Haykin. Adaptive Filtering Theory. Prentice Hall, 2002.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image
recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition,
pp. 770-778, 2016.
Feihu Huang, Songcan Chen, and Heng Huang. Faster stochastic alternating direction method
of multipliers for nonconvex optimization. In Kamalika Chaudhuri and Ruslan Salakhutdinov
(eds.), Proceedings of the 36th International Conference on Machine Learning, volume 97 of
Proceedings of Machine Learning Research, pp. 2839-2848. PMLR, 09-15 Jun 2019. URL
https://proceedings.mlr.press/v97/huang19a.html.
Rie Johnson and Tong Zhang. Accelerating stochastic gradient descent using predictive variance
reduction. In Advances in Neural Information Processing Systems, pp. 315-323, 2013.
Frank P Kelly. Reversibility and stochastic networks. Cambridge University Press, 2011.
Alex Krizhevsky, Vinod Nair, and Geoffrey Hinton. Cifar-10 (canadian institute for advanced
research). URL http://www. cs. toronto. edu/kriz/cifar. html, 5, 2010.
10
Under review as a conference paper at ICLR 2022
Yann LeCun, Corinna Cortes, and CJ Burges. Mnist handwritten digit database. 2010. URL
http://yann. lecun. com/exdb/mnist, 7:23, 2010.
Lihua Lei, Cheng Ju, Jianbo Chen, and Michael I Jordan. Non-convex finite-sum optimization via
Scsg methods. In Advances in Neural Information Processing Systems, pp. 2348-2358, 2017.
Andrew L. Maas, Raymond E. Daly, Peter T. Pham, Dan Huang, Andrew Y. Ng, and Christopher
Potts. Learning word vectors for sentiment analysis. In Proceedings of the 49th Annual Meeting
of the Association for Computational Linguistics: Human Language Technologies, pp. 142-150,
Portland, Oregon, USA, June 2011. Association for Computational Linguistics. URL http:
//www.aclweb.org/anthology/P11-1015.
Yurii Nesterov. Introductory lectures on convex optimization: A basic course, volume 87. Springer
Science & Business Media, 2013.
Yurii E Nesterov. A method for solving the convex programming problem with convergence rate
o(1/k2). In DokL akad. nauk Sssr, volume 269, pp. 543-547, 1983.
Jorge Nocedal and Stephen Wright. Numerical optimization. Springer Science & Business Media,
2006.
Hua Ouyang, Niao He, Long Tran, and Alexander Gray. Stochastic alternating direction method of
multipliers. In Sanjoy Dasgupta and David McAllester (eds.), Proceedings of the 30th International
Conference on Machine Learning, volume 28 of Proceedings of Machine Learning Research, pp.
80-88, Atlanta, Georgia, USA, 17-19 Jun 2013. PMLR. URL https://proceedings.mlr.
press/v28/ouyang13.html.
Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor
Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. Pytorch: An imperative style,
high-performance deep learning library. In Advances in neural information processing systems, pp.
8026-8037, 2019.
Andrei Patragcu. New nonasymptotic convergence rates of stochastic proximal point algorithm for
stochastic convex optimization. Optimization, pp. 1-29, 2020.
David Pollard. A user’s guide to measure theoretic probability. Number 8. Cambridge University
Press, 2002.
R Tyrrell Rockafellar. Monotone operators and the proximal point algorithm. SIAM journal on
control and optimization, 14(5):877-898, 1976.
Ernest K Ryu and Stephen Boyd. Stochastic proximal iteration: a non-asymptotic improvement upon
stochastic gradient descent. Preprint, 2014.
Ilya Sutskever, James Martens, George Dahl, and Geoffrey Hinton. On the importance of initialization
and momentum in deep learning. In International conference on machine learning, pp. 1139-1147,
2013.
Panos Toulis, Thibaut Horel, and Edoardo M Airoldi. The proximal Robbins-Monro method. Journal
of the Royal Statistical Society: Series B (Statistical Methodology), 83(1):188-212, 2021.
Huahua Wang and Arindam Banerjee. Online alternating direction method. In Proceedings of the
29th International Coference on International Conference on Machine Learning, pp. 1699-1706,
2012.
Mengdi Wang and Dimitri P Bertsekas. Incremental constraint projection-proximal methods for
nonsmooth convex optimization. SIAM J. Optim.(to appear), 2013.
Wenliang Zhong and James Kwok. Fast stochastic alternating direction method of multipliers. In
Eric P. Xing and Tony Jebara (eds.), Proceedings of the 31st International Conference on Machine
Learning, volume 32 of Proceedings of Machine Learning Research, pp. 46-54, Bejing, China, 22-
24 Jun 2014. PMLR. URL https://proceedings.mlr.press/v32/zhong14.html.
Juntang Zhuang, Tommy Tang, Sekhar Tatikonda, Nicha Dvornek, Yifan Ding, Xenophon Pa-
pademetris, and James S Duncan. Adabelief optimizer: Adapting stepsizes by the belief in
observed gradients. arXiv preprint arXiv:2010.07468, 2020.
11
Under review as a conference paper at ICLR 2022
A	Efficient Implementation
In this section we introduce several efficient methods to calculate the proximal operator update in
Algorithm 1 line 3. At first glance it may seem as hard as solving the batch problem itself, but we
will see that there are some interesting properties when 力 involve the loss evaluation of only one or a
few data points in a data fitting scenario. On the other hand, it is not clear how to parallelize SPPA
with mini-batches, which leaves room for future work.
A.1 Least s quares loss
For the least squares loss 力=(yi - x>w)2, the proximal update is a linear least squares problem with
closed-form solution ^t+ι = (/ + 及Xfx>)-1 (Wf + xiyi). It can be efficiently calculated by invoking
the Sherman-Morrison formula and avoid directly inverting a d × d matrix for W ∈ Rd as
x>i Wt - yi
Wt+1 = Wt -kXik2 + 1/2Xi.
The update rule looks like, but is not exactly the same as, the normalized least mean squares (NLMS)
algorithm (Haykin, 2002). It has appeared in (PatraScu, 2020), but We include it here for completeness.
A.2 Logistic (cross-entropy) loss
In general, the one-sample loss function can be written as %(W) = x>W = I(x>w). For example, the
one-sample loss for logistic regression is 力(W) = log(1 + exp(-yix>w)). Suppose it is differentiable,
then by applying the chain rule, we have V%(w) = Xi 10 (x>w), where 10 is the derivative of the scalar
function I(∙). Plugging this into the optimality condition λtV力(wt+ι) + Wt+ι - Wt = 0, we see that
Wt+1 has the form
Wt+1 = Wt - axi,
where a is some scalar. The proximal update reduces to finding the scalar a that solves the following
convex problem,
>	2	a2	2
minimize I(x>Wt - akXi∣∣2) + --kXi∣∣2.
a	22t
This can be done by setting the derivative equal to zero. Due to convexity, its derivative is a monotonic
function, so the root can be obtained via bisection.
As a concrete example, consider the logistic loss %(w) = log(1 + exp(-yixj w)) with yr∙ = ±1.
According to the aforementioned arguments, we need to solve the nonlinear equation
=	exp(x>Wt - α∣∣Xik2)
y " 11 + exp(x>Wt - a∣Xi∣∣2)*
Notice that the right-hand-side is a number between 0 and ±2t , which gives the initial upper and
lowerbound on a. Furthermore, we see that x>iWt and ∣xi ∣ 2 only need to be calculated once, with
O(d) flops for w ∈ Rd; each bisection step takes constant time and it needs no more than 20 ~ 30
scalar computations to render an accurate-enough solution.
A.3 Hinge loss
Regarding nonsmooth optimization problems, it turns out many of the widely used loss functions
admit closed form updates. The main idea is to consider the subgradient calculus, and find the point
where 0 is in the subdifferential. Take support vector machine (SVM) as an example, in which the
loss function is the hinge loss %(w) = [1 - yr∙x>w]+. Its subdifferential is
'{0}	1 - yix>w < 0,
∂fi(w) = - {-yiXi}	1 - yix>W > 0,
、{-ayiXi|0 ≤ a ≤ 1}	1 - yr∙x>W = 0.
12
Under review as a conference paper at ICLR 2022
When added with a proximal term, the subdifferential set is added with W - Wf. Asa result, the
update rule is simply
Wf	y(x>Wt > 1,
Wt+1 = < Wt + λyixi	yix>Wt < 1 - λ(t)kxf ∣∣2,
.、	1-ytχ>wt	,1	.
Wt + 几yiXi -H t∣∣2	otherwise.
t	Jii ytk*tk2
An interesting observation here is that it looks like the perceptron algorithm with an adaptively chosen
step-size.
A.4 Absolute error loss
Robust regression using absolute error loss 力(w) = |yi - x>W |, is another example to nonsmooth
convex loss function. The derivation is similar to that of SVM: the subdifferential for the one-sample
loss is
'{χi}	yi - x>W < 0,
∂fi(w) = Y-Xi}	yi - χ>W > 0,
、{axi | - 1 ≤ a ≤ 1} yi - x>W = 0.
After adding the proximal term, the update rule is
Wt - Axi	yi - x>Wt < 一川IXi k2,
Wt+1 = < Wt + Ari	yi -x>Wt > AkXik2,
Wt + y-AP Xi otherwise.
k χt k
A.5 Generic losses
Even for generic nonlinear programming problems, it is still possible to efficiently evaluate the
proximal update beyond merely a simple gradient step. The idea is to apply the limited-memory
BFGS algorithm (Nocedal & Wright, 2006), or L-BFGS for short. L-BFGS is a memory-efficient
implementation of the famous quasi-Newton algorithm BFGS. In a nut shell, L-BFGS is an iterative
algorithm that makes use of the second-order information from the optimization loss function, but
does not require solving matrix inverses and only requires explicitly evaluating first-order gradients.
For a prescribed number of iterations, it requires one matrix-vector multiplication and multiple vector
multiplications. As a result, the overall complexity is again O(d) if the initial guess of the Hessian
matrix is diagonal.
While there are certain limitations for applying L-BFGS to general nonlinear programming problems,
we reckon that it fits perfectly in the context of SPPA implementations.
•	L-BFGS has to specify a good initial guess of the Hessian approximation matrix. While
in many cases people simply use the identity matrix to start, it may result in very poor
approximation. Fortunately, thanks to the proximal term, the identity matrix is in fact a very
good initial guess for the Hessian matrix for SPPA updates.
•	In order to save memory consumption, L-BFGS has to prescribe the number of iterations
before running the algorithm. Obviouly, if the prescribed number of iteration is too large,
we incur unnecessary computations, while if it is too small we need to invoke another round
with a new estimated Hessian matrix. However, again in the context of SPPA, the proximal
term naturally provides a good initialization Wt. Our experience show that prescribing 10
iterations of L-BFGS updates is more than enough to obtain accurate solutions.
On the other hand, it perhaps makes more sense to simply use some more advanced first-order
methods such as Nesterov’s accelerated gradient descent (1983; 2013) to calculate the proximal
update. Both L-BFGS and accelerated gradient descent evaluates the gradient of the loss function
with O(d) complexity, and the question is how to leverage convergence rate versus sophistication. B * *
B	Proof of Lemma 1
At every iteration of SPPA, a component function 力 is picked uniformly at random to obtain the
update. Therefore P(it |Wt) = 1/n, obviously. Lemma 1, however, states that the conditional
13
Under review as a conference paper at ICLR 2022
probability of it given the next update Wf+1 is still uniform. Recall that the update rule is
Wf+ι = arg min也 fit(W) + (1∕2)kW - Wt∣∣2.	(10)
What Lemma 1 implies is that if We know Wt+ι is obtained at iteration t, without knowing the previous
iterate Wt, we gain no additional information regarding which component function 力 is more likely
to have rendered the update equation 10.
Consider the sequence of indexes i1, i2,   At every iteration of SPPA, an index is independently
drawn from {1,...,n} with equal probabilities. We may also treat this as a Markov chain with the
transition probability P(it |it-ι) = 1∕n. This seems unnecessary at the moment, since an independent
sequence is a trivial special case of a Markov chain. However, this implies P(it ∣Wt) = P(it ∣it-ι)
and subsequently P(it |Wt+1) = P(it |it+1). The question then boils down to studying the transition
probability of the reversed Markov chain.
Kolmogorov’s criterion gives a necessary and sufficient condition for a Markov chain to be identical
to its time-reversed version (Kelly, 2011).
Theorem 3.	A stationary Markov chain is reversible if and only if its transition probability satisfies
P ( J1 | J2)P ( J2 | J3) …P ( jm-1 | jrn)P (Jm | Ji) = P ( Ji | jrn)P ( J2 | Ji ) …P ( Jm | jm-1)
for any finite Sequence of states jι,..., Jm∙
It is easy to see that in our case the Kolmogorov’s criterion trivially holds since the transition
probability is 1/n in all cases. This proves that P(i∕it+ι) = P(it |it-ι) = 1/八 and thus P(it |Wt+ι)=
1∕n.
C	Almost Sure Convergence
In the main paper we presented the result that the infimum of E[F(Wt)] goes to zero, following the
techniques presented by Bottou et al. (2018). What this implies is that for some iteration S we may
have a very small E[F(WS)], but for t > s the expected loss may go up again. To make a stronger
claim that the random variable F(Wt) converges almost surely, we are going to use martingales
convergence results. The difference in this case is that we are going to use the backwards version of
martingale convergence. Here we list the key results referenced from (Pollard, 2002).
Definition 1. Let {Xt : t ∈ N} be a sequence of integrable random variables, adapted to a decreasing
filtration {Gt : t ∈ N}. We call {(Xt, Gt) : t ∈ N} a reversed supermartingale if E[Xt∣Gt+ι] ≥ Xt+ι
for all t.
In other words, {(Xt, Gt) : t ∈ N} is a reversed supermartingale if and only if {(X-t, G-t) : t ∈
-N} is a supermartingale. In particular, for each fixed t, the finite sequence Xt, Xt-1, . . . , X0 is a
supermartingale with respect to the filtration Gt ⊆ Gt-ι ⊆ ∙∙∙ ⊆ Go.
The theory for reversed positive supermartingale is analogous to the regular supermartingale theory.
Theorem 4.	For every reversed, positive supermartingale {(Xt, Gt) : t ∈ N}, there exists an X∞ for
which Xt → X∞ almost surely.
Equipped with the reversed supermartingale theory, we have the following:
Theorem 5.	Ifall fι,...,fn are convex and λt is Iowerbounded, then the sequence {wJ generated
by SPPA (Algorithm 1) satisfies that F(Wt) → F(W★) almost surely.
Proof. Define the filtration Gt = {Wt, Wt+1, . . .} for all t = 0, 1,   Consider inequality equation 2
from Proposition 1. Notice that
E[kWt - w^k2 | Wt+ι] = E[kWt - w^k2 | Gt+ι].
This means {(k Wt - w^∣∣2, Gt) : t ∈ N} is a reversed supermartingale. Applying Theorem 4, we have
that {∣∣Wt - W J2} converges almost surely; in other words
Ik wt- w^ k2 - k wt+ι- w^ ∣∣2∣ → 0
almost surely.
14
Under review as a conference paper at ICLR 2022
Notice that Proposition 1 also implies
kWL w*k2 - kwt+1 - w*k2 ≥ 2Λt(F(Wt+1) - F(W★)).
Combining this with the previous result shows that
2t(F(Wt+ι) - F(w*)) → 0
almost surely. Since We assume that λt is Iowerbounded for all t, this means F(Wt+ι) → F(w*)
almost surely.
D Additional Experiments
D.1 Clas sification
In the main paper, we showed the performance of our unique implementation of SVM on Bank Note
Authentication dataset and Logistic Regression on IMDB dataset. For the sake of completeness,
we will show the performance of SPPA with our implementation for Logistic Regression on Bank
Note Authentication dataset. Figure 11 shows the logistic loss value per second using Bank Note
Authentication dataset. The performance of SPPA with our implementation is comparable with SGD
with momentum. It still performs better than most of the other algorithms and it is more robust to
change in seeds. Figure 12 shows that SPPA with our implementation reaches to the highest accuracy
value on test data in less than a second, whereas the other algorithms take around 3 seconds to reach
to an even smaller accuracy value. The robustness of SPPA with our implementation is consistent in
this experiment too.
Figure 11: Logistic Regression on Bank Note
Authentication: loss per seconds
Figure 12: Logistic Regression on Bank Note Au-
thentication: classification accuracy per seconds
D.1.1 Clas sification with L1 Regularization
We perform binary classification using SVM and logistic regression with L1 regularization. Our aim
is to show the performance of our proposed SPPA algorithms in presence of L1 regularization term
compared to other state of the art optimization algorithms using two different datasets, Bank Note
Authentication and IMDB. In the results presented below, for each algorithm every stochastic update
is followed by soft-thresholding to impose the effect of L1 regularization term. The inner loop in
SPPA algorithm is run at most 20 times for the results presented below.
Bank Note Authentication Dataset. We perform classification using SVM and logistic regression
with L1 regularization. Considering the per iteration complexity of SPPA, we plotted the loss-time,
accuracy-time figures, Figure 13 and Figure 14 respectively. The Figure 13 demonstrates how the
regularized hinge loss value decreases with respect to time (seconds) using different algorithms. The
Figures 13 and 14 show that with our efficient implementation, SPPA takes less amount of time to
reach to a smaller loss, and a greater accuracy despite of its per iteration complexity. The range
shown in the plots clearly shows that SPPA is more robust to change in stochastic settings.
We present further experiments on the performance of SPPA using logistic regression with L1
regularization on Bank Note Authentication dataset loss-time, accuracy-time figures, Figure 15 and
15
Under review as a conference paper at ICLR 2022
Figure 13: SVM with L1 Regularization on Bank
Note Authentication: loss per seconds, batch
size 1
Figure 14: SVM with L1 Regularization on Bank
Note Authentication: classification accuracy per
seconds, batch size 1
Figure 15: Logistic Regression with L1 Regular-
ization on Bank Note Authentication: loss per
seconds
Figure 16: Logistic Regression with L1 Regular-
ization on Bank Note Authentication: classifica-
tion accuracy per seconds
Figure 16 respectively. In Figure 15 SPPA reaches to a smaller loss value in shorter amount of time
compared to other algorithms, the closest performance is by Stochastic Gradient with Momentum
algorithm. In Figure 16 SPPA reaches to the largest accuracy value in shortest amount of time and it
has smallest variance compared to the other algorithms.
IMDB Dataset. We perform classification using SVM and logistic regression with L1 regularization
on IMDB dataset. SPPA outperforms the other algorithms when we use logistic regression to do
the classification. We aim to see the change in performances of the algorithms in presence of L1
regularization. According to the experiments presented there is not a big difference in the performance
of algorithms with L1 regularization except for the slightly increase on the loss values. The Figure 17
shows the loss, and the Figure 18 shows the accuracy per seconds when logistic regression model is
used with different optimization algorithms to perform classification on IMDB dataset.
D.2 Regression
Bike Sharing. Bike Sharing is a publicly available data set containing hourly or daily count of
rental bikes as well as environmental and seasonal settings. The data set we are using is from UC
Irvine Machine Learning Repository. 3 The core data set is related to the two-year historical log
corresponding to years 2011 and 2012 from Capital Bike share system, Washington D.C., USA
(Fanaee-T & Gama, 2013). It consists of 17379 samples (12512 training, 3476 test, 1391 validation)
3 https://archive.ics.uci.edu/ml/datasets/bike+sharing+dataset
16
Under review as a conference paper at ICLR 2022
Figure 17: Logistic Regression with L1 Regular-
ization on IMDB: loss per seconds
55
50
5 0 5 0 5 0
8 8 7 7 6 6
0	5	10	15	20	25	30	35
Seconds
Figure 18: Logistic Regression with L1 Regu-
larization on IMDB: classification accuracy per
seconds
Figure 19: Linear Regression on Bike Sharing: Figure 20: Linear Regression on Hotel ADR:
mean squared error loss per seconds, batch size 16 mean squared error loss per seconds, batch size 1

with 16 features each to predict hourly count of rental bikes. We perform linear regression with two
different loss functions, MSE loss. The Figure 19 shows mean squared error loss per seconds. We
used 100 seeds to test the robustness of algorithms. SPPA reaches to a smaller loss value in less
number of iterations. Batch size is 16.
Hotel Average Daily Rates (ADR). Hotel ADR is a publicly available data set consisting of hotel
demand data for two different types of hotels; resort hotels and city hotels. Each hotel data set
consists of 40,060 samples with 31 features each to predict average daily rate values (Antonio et al.,
2019). In our experiments, we used resort hotels with 40,060 samples (30045 training, 10015 test)
with 8 features. We perform linear regression with mean squared error (MSE). The Figure 20 shows
mean squared error loss per seconds. SPPA reaches to a smaller loss value in less number of iterations
and in less time. Batch size is 16. In Figure 20, MSE loss goes to very small value only in first few
iterations. Our experiments show that overall performance of SPPA is better in linear regression
problems using Hotel ADR dataset.
D.3 Regression with L1 Regularization
We perform linear regression with L1 regularization and show the results of different algorithms
including our efficient implementation on both Bike Sharing 4 and Hotel Average Daily Rates (ADR)
datasets (Antonio et al., 2019).
Bike Sharing. We perform linear regression with L1 regularization on Bike sharing dataset and
compare the performances of different optimization algorithms. The Figure 21 shows the decrease in
the loss values over time for different algorithms. SPPA outperforms the other algorithms and reaches
to a smaller value in a shorter time. We tried different settings for the algorithms, in the results
presented in Figure 21 the learning rate for SPPA is 1/sqrt(t) where t is the number of iterations.
4 https://archive.ics.uci.edu/ml/datasets/bike+sharing+dataset
17
Under review as a conference paper at ICLR 2022
Seconds
Figure 22: Linear Regression with L1 regulariza-
tion on Hotel ADR: mean squared error loss per
seconds, batch size 1
0	1	2	3	4	5	6
Seconds
Figure 21: Linear Regression with L1 regulariza-
tion on Bike Sharing: mean squared error loss per
seconds, batch size 16
0 5 0 5 0
3 2 2r-JI
0.0.0.0.0.
Sso-J山 SW

O
O
The other advantage, stability of SPPA is also showcased in Figure 21. Compared to the results
without the L1 regularizer, the L1 regularizer helps SPPA to get smaller loss values.
Hotel Average Daily Rates (ADR). We perform linear regression with L1 regularization on Hotel
ADR dataset and compare the performances of different optimization algorithms. The Figrure 22
shows the decrease in loss values over time for different algorithms. SPPA converges to a very small
loss value much faster, in less than a second it converges to the minimum loss. We tried different
settings for the algorithms, in the results presented in Figure 22 the learning rate for SPPA is 1/t
where t is the number of iterations.
D.4 Mean-absolute-error Regression
We showcased the performance of our algorithm with mean squared error loss in regression problem
with two real data-sets in the main paper. Here, we will present our results on the same data sets
using absolute error loss. Figure 23 shows that, SPPA with our unique implementation reaches to a
smaller loss value in less amount of time and significantly outperforms the alternative methods on
Bike Sharing dataset. However, the error range of the algorithms are similar. Figure 24 shows the
performance on Hotel ADR dataset with absolute error loss, our implementation of SPPA reaches to
a smaller loss value in a significantly less amount of time, in the first few iterations. In addition to its
great performance, looking at the error bar one can see that it is much more robust to different seed
settings compared to other algorithms.
Figure 23: Linear Regression on Bike Sharing: Figure 24: Linear Regression on Hotel ADR: ab-
absolute error loss per seconds, batch size 16 solute error loss per seconds, batch size 1
18