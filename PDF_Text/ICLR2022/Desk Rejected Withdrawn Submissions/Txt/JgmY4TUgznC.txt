Under review as a conference paper at ICLR 2022
Few- S hot Multi-task Learning via Implicit
REGULARIZATION
Anonymous authors
Paper under double-blind review
Ab stract
Modern machine learning is highly data-intensive. Few-shot learning (FSL) aims
to resolve this sample efficiency problem by learning from multiple tasks and
quickly adapt to new tasks containing only a few samples. However, FSL prob-
lems proves to be significantly more challenging and require more compute ex-
pensive process to optimize. In this work, we consider multi-task linear regression
(MTLR) as a canonical problem for few-shot learning, and investigate the source
of challenge of FSL. We find that the MTLR exhibits local minimum problems
that are not present in single-task problem, and thus making the learning much
more challenging. We also show that the problem can be resolved by overparam-
eterizing the model by increasing both the width and depth of the linear network
and initializing the weights with small values, exploiting the implicit regulariza-
tion bias of gradient descent-based learning.
1 Intro
Despite the recent success of large deep neural-networks models, training these models typically
require extremely large dataset. In contrast, human cognition exhibits an impressive feature that
enables quickly learning new concept from a small set of experience and to robustly generalize to
unseen tasks. An active area of research to bridge this sample efficiency gap is few-shot learning and
multi-task learning, which aims to aggregate learning from many number of related tasks to extract
the shared feature/information across tasks, and leverages it to quickly learn new tasks (Argyriou
et al., 2006; 2008; Santoro et al., 2016; Vinyals et al., 2016; Snell et al., 2017; Finn et al., 2017;
Nichol & Schulman, 2018).
However, learning to solve these few-shot/multi-task problems proves to be considerably more chal-
lenging than single-task problems. Popular approaches include meta-learning setup that uses sepa-
rate inner-loop and outer-loop optimization that are both compute and memory intensive (Finn et al.,
2017; Nichol & Schulman, 2018) , or require additional unsupervised learning steps that learn the
relatedness metric between objects (Santoro et al., 2016; Vinyals et al., 2016; Snell et al., 2017).
In this work, we investigate the challenge of few-shot learning problems by considering a multi-
task version of linear regression problem as a canonical example, in which the tasks are related
by sharing a small common feature space (Argyriou et al., 2006; 2008). Our analysis reveals that
despite the simplicity of the problem, the few-shot/multi-task learning (MTL) setting makes the
linear regression problem significantly more difficult to solve. We find that multi-task setting makes
the linear regression problem highly non-convex and introduces poor local minima, which hinders
the performance of gradient-descent learning. However, we show that the problem can be resolved
by overparameterizing the linear network model with increased the width and depth and initializing
the weights with small values. This setting exploits the recently investigated implicit regularization
bias of gradient descent-based learning.
Out contribution includes: 1. Characterization and complexity analysis of multi-task linear regres-
sion problem, 2. Derivation of an analytic expression that quantifies the few-shot generalization
performance of the model, and 3. a simple learning method to find the solution.
1
Under review as a conference paper at ICLR 2022
2	Problem Formulation
Here, we describe the few-shot multi-task learning problem considered in this work, which fol-
lows a similar construction as in Argyriou et al. (2006). We are given T supervised learning tasks,
where each task t is identified by a function ft : Rdx → R that describes the input-label rela-
tionship of the corresponding dataset. Each dataset contains k samples of input-label examples
Dt = (xt1,yt1),…,(xtk, ytk) ∈ Rdx X R, which can also be described by using matrix notations
as Xt ∈ Rdx ×k and yt ∈ R1×k.
The multi-task learner trains on T tasks {Dt}tT=1 to uncover the underlying structure shared across
the task distribution that generalizes to unseen tasks. Specifically, we consider that the tasks are
related by sharing a small set of features. Formally, we assume that the task can be represented by
the function
df
ft(x) = Eaithi(X) = at ∙ h(x)
i=1
(1)
where hi : Rdx → R are the features, ait ∈ R are the task-specific regression parameters and
df is the total number of features. For simplicity, we consider the case that the features are linear
homogeneous functions, i.e. they are of the form h(x) = Ux, where U ∈ Rdf ×dx. Without loss of
generality, we further assume that U is an orthogonal matrix, which simplifies the derivations later.
Thus, the final form of the task function is
ft (x) = atUx
(2)
The input data is assumed to be drawn from a standard zero-mean unit Normal distribution.
2.1	Complexity of multi-tas k problem
The simplicity of our setup makes it straightforward to quantify all the degrees of freedom (DOF)
involved in the multi-task learning problem. The parameter count involved in the task description
equation 2 is df for ct per task and df × dx shared across the tasks. However, the orthogonality
condition UU| = Idf introduces df2 constraints, which restricts DOF. Therefore, the total DOF for
characterizing a set ofT structured tasks is (T × df +df × (dx - df)). Note that the shared structure
reduces the DOF by (T - df) × (dx - df ), which is what ultimately allows the few-shot learning
feasible.
From this, we can estimate the minimum number of tasks that is needed to completely characterize
the MTL problem. Since each datapoint (xtm, ytm) provides one constraint, the total data in T tasks
with k datapoints contributes to T × k constraints, which needs to be greater or equal to the above
DOF: i.e. T × k ≥ T × df + dx × (dx - df). Therefore, the minimum required number of tasks
that makes the k-shot multi-task learning feasible is
df × (dx - df )
k - df
(3)
which proves to be critical for controlling the difficulty of solving the MTL problem. Note that T*
can dramatically increase for MTL with small k 〜 df, since then most of the information of each
task’s dataset gets utilized for identifying task-specific parameters, only leaving k - df constraints
per task to characterize the shared parameters (Fig 3).
2.2	Linear network model
The task structure in equation 2 implies that it can be naturally modeled by two-layer linear neural
networks.
gt(x) = ctWx
(4)
where W ∈ Rdh ×dx , ct ∈ Rdh , and dh is the dimension of the hidden layer, which may differ
from the task’s feature dimension df . For deeper versions of the network model, we consider the
weight W being composed of a product of l matrices, W = Wl …W2W1, with Wl ∈ Rdh×dx
and Wi6=l ∈ Rdx×dx .
2
Under review as a conference paper at ICLR 2022
Figure 1: The minimum required task T* as a function of per task batch-size k, shown for input and
feature dimensions of dx = 36, df = 4. Dots indicate the values used in the experiments.
Note that most deep network models used in multi-task learning settings indeed takes the same
architecture as equation 4 that they also use the different top-level layer is used for each specific
task, while the earlier feature-encoding layers are shared across all tasks.
3	Method
For training, we use gradient descent algorithm with momentum to update both the task-specific ct
and the shared parameters Wi to minimize the empirical risk ED [Lt(ct, W)], where D ≡ {Dt}tT=1
and
Lt(ct,W) = E(x,y)^Dt [L(y,gt(x))] + eg ∣∣2	⑸
where L is the square-loss and the second term is a regularization loss on ct . A small regularization
term on W may also be applied, which is suppressed here for notational simplicity.
For evaluating the generalization capacity of the learned weight W to unseen tasks, we derive an
analytic lower-bound of the expected loss on a df -shot learning task. The derivation assumes first
adapting/optimizing c over a df batch of datapoints of an evaluation task, called the support set.
However, due to the small batch-size, the quality of the estimation can critically depend on the
specifics of the sampled batch; For example, the support set can be completely non-informative if
all input data X are from the null space of U, in which all label data are y = 0. Here, we remove
such dependency by considering the most informative batch as the support set, which exactly spans
the range of U, i.e.
XX|/df = U|U	(6)
With c adapted to the support set of a evaluation task, taking expectation of the loss over the task
and input distributions yields the following lower bound
Leval(W) =Et[E(Xt,yt)[Lt(ctopt,W;Xt,yt)]]	(7)
= Tr[UU|] +Tr[(WU|UW| + eI)-1(WW| - 2WU|UW| + eI)]	(8)
which is expressed entirely in terms of W and U. See Appendix for the derivation.
This evaluation loss quantifies how much W aligns with the low-rank feature space, i.e. the the row
space of U. To see this, consider W whose singular vectors are either entirely inside or outside the
feature space. That is, given the singular value decomposition W = OσV |, the singular vectors
projected onto U have norm Pln(Pm UlmVmn)2 that is either zero or one. In this case, the above
loss reduces to
Leval,opt =、、	2e
—乙 σ2 + e
i∈I i
+x L
i∈I
(9)
(10)
3
Under review as a conference paper at ICLR 2022
where I is the indicator function for distinguishing whether the i’th singular mode is within the range
of U. (See Appendix.) Note that the first term promotes large singular values within the range of U,
whereas the second term penalizes singular values outside the subspace. Therefore, this evaluation
loss is minimized when W exhibits the same low-rank structure as U.
4	Results
We investigate the efficacy of solving the few-shot MTL problem using gradient decent learning in
various data regimes. We control the difficulty of the problem by varying the per task batch size
k and the number of tasks T in the training dataset in relation to the given input dimension and
feature dimension of the problem. For each batch size, we use the number of task T to be around
the theoretical minimum required number T* with increment size of 25%.
With this setup, we investigate the effect of width, depth, and initial value of the linear network
model on its learning dynamics. The weights Wl are initialized to be orthogonal matrix with the
singular vectors of adjacent layer being aligned, which yields the fastest learning dynamics, as
shown/introduced in Saxe et al. (2013) and widely used in linear network research (Arora et al.,
2018a;b; 2019; Lampinen & Ganguli, 2018; Advani & Saxe, 2017).
4.1	Low-rank architecture
First, let’s consider the case in which the network’s hidden dimension matches the task’s feature
dimension, i.e. dh = df . This condition may appear to be most suitable for solving the MTL
problem, since the model architecture is explicitly constrained to have the same rank as the task
structure, thus can achieve good generalization performance on unseen tasks.
Fig ?? shows that the learning dynamics can fully fit the training data when the problem is highly un-
derdetermined (T < T* , two left columns), or when the batch-size approaches the input dimension
(k = dx , bottom row), which is no longer a few-shot learning problem. In other conditions, how-
ever, the learning dynamics of the model fail to perfectly fit the training data and instead converges
to the poor local minimum solutions. Moreover, the model shows poor generalization performance
in all conditions (except for the bottom row), and thus has failed to learn the correct task structure.
This result shows that even linear models can exhibit poor local minima when considered in few-
shot multi-task learning problems. This is indeed very surprising, since all previous studies on linear
networks, primarily conducted in single-task settings, have shown that the loss landscapes of linear
models only exhibit global minima and their learning dynamics are guaranteed to converge to those
global minima in almost all initialization (Arora et al., 2018a;b; Bah et al., 2019; Geyer et al., 2020;
Kawaguchi, 2016) . To our best knowledge, this is the first demonstration of local minima problem
in linear models.
4.2	Full-rank model
Next, we consider the over-parameterized condition in which the hidden dimension matches the
input dimension dh = dx . This full rank condition makes learning of the training dataset trivial,
since the task-specific parameters c can fullly fit the data without ever learning W . Consequently,
learning dynamics in this model exhibits smooth decay toward zero training loss. However, it also
means the shared parameter does not learn the generalizable, low rank tasks structure. Consequently,
this model exhibits even worse generalization performance than the narrow model. See Fig 3A. Note
the scale of the y-axis.
Can the overparameterized models be trained to learn the task structure of the MTL problem? Recent
investigations have shown that deep linear networks show implicit bias toward low rank solutions
when trained with gradient descent algorithm, which becomes stronger for deeper networks that are
initialized with small weight values (Arora et al., 2018a;b; 2019; Lampinen & Ganguli, 2018; Advani
& Saxe, 2017). Next, we investigate whether such implicit regularization bias can be exploited to
rescue the generalization performance of the over-parameterized model.
Our simulation result with confirms that indeed deep linear networks that are initialized with small
weights can indeed greatly enhance the generalization performance in the MTL problem (See Fig 3B
4
Under review as a conference paper at ICLR 2022
Figure 2: Training loss (A) and evaluation loss (B) profiles of narrow 2-layer models with initial
weight scale 0.1. Each plot shows the result of 20 learning results of different initializations.
200	«0
T = 1.25×T*
Iterations
Iterations
and Fig 4). The resulting model show smooth decay in the training loss, while also keeping the
evaluation loss small.
The singular value decomposition of the network weight W reveals that the deep linear network
models indeed learns the low rank task structure. The singular values exhibit large gap between df
number of main singular modes (Fig 5A), and the corresponding singular vectors shows increasing
alignment with the row space of U (Fig 5B) during learning. Our result confirms that the implicit
bias for regularization in gradient descent can be exploited to discover low-rank solutions that gen-
eralizes across tasks.
5	Conclusion
Few-shot/multi-task learning exhibits many challenging aspects to solve. In this work, we made a
step toward understanding those challenges by investigating a simplified canonical model of multi-
task regression problem. We found that the multi-task version introduces increased non-convexity of
loss landscape with poor local minima, which hinders learning by simple gradient descent algorithm.
We then showed that this problem can be resolved by considering larger, overparameterized models
that are well initialized to exploit the implicit regularization bias of gradient-descent learning. This
5
Under review as a conference paper at ICLR 2022
A
T整5X产。。T邺∙75X严
-jl0∞lχ7.*20∞
°	T¾×7zp0°
Iterations
Figure 3: Evaluation loss of wide 2-layer model with initial weight scale 0.1 (A), and evaluation
loss of wide 4-layer model with initial weight scale 0.001 (B). Note the difference in scale.
	
	
	
	∖	
S
	
	
Γ		Γ		
V1				『，，
T= 0.5×T
■n	1	Γ, 一		1	Γ, ɪi	1	
	
[— 一 ■	
	IL		—
		π	
200
T= 1.25 ×T*
Iterations
result could be further investigated to yield more efficient approaches for solving few-shot learning
and meta-learning problems.
References
Madhu S Advani and Andrew M Saxe. High-dimensional dynamics of generalization error in neural
networks. arXiv preprint arXiv:1710.03667, 2017.
Andreas Argyriou, Theodoros Evgeniou, and Massimiliano Pontil. Multi-task feature learning. In
NIPS, 2006.
Andreas Argyriou, Theodoros Evgeniou, and Massimiliano Pontil. Convex multi-task feature learn-
ing. Machine learning, 73(3):243-272, 2008.
Sanjeev Arora, Nadav Cohen, Noah Golowich, and Wei Hu. A convergence analysis of gradient
descent for deep linear neural networks. arXiv preprint arXiv:1810.02281, 2018a.
Sanjeev Arora, Nadav Cohen, and Elad Hazan. On the optimization of deep networks: Implicit
acceleration by overparameterization. arXiv preprint arXiv:1802.06509, 2018b.
6
Under review as a conference paper at ICLR 2022
lij≡
Figure 4: Summary figure showing the evaluation loss of wide linear network models across all
training conditions (averged over 20 runs). Deeper network with smaller initial weights indeed
helps with generalization performance.
300	48
T = 0.75×T*
Iterations
Figure 5: Trajectories of singular values (A) and singular vectors (B) of learning dynamics of a
wide 3-layer network (initial weight scale 0.001). The right column T = 1.25T* shows the case of
successful learning.
Sanjeev Arora, Nadav Cohen, Wei Hu, and Yuping Luo. Implicit regularization in deep matrix
factorization. arXiv preprint arXiv:1905.13655, 2019.
Bubacarr Bah, Holger Rauhut, Ulrich Terstiege, and Michael Westdickenberg. Learning deep lin-
ear neural networks: Riemannian gradient flows and convergence to global minimizers. arXiv
preprint arXiv:1910.05505, 2019.
Chelsea Finn, Pieter Abbeel, and Sergey Levine. Model-agnostic meta-learning for fast adaptation
of deep networks. In International Conference on Machine Learning, pp. 1126-1135. PMLR,
2017.
Kelly Geyer, Anastasios Kyrillidis, and Amir Kalev. Low-rank regularization and solution unique-
ness in over-parameterized matrix sensing. In International Conference on Artificial Intelligence
and Statistics, pp. 930-940. PMLR, 2020.
7
Under review as a conference paper at ICLR 2022
Kenji Kawaguchi. Deep learning without poor local minima. In Advances in neural information
processing Systems,pp. 586-594, 2016.
Andrew K Lampinen and Surya Ganguli. An analytic theory of generalization dynamics and transfer
learning in deep linear networks. arXiv preprint arXiv:1809.10374, 2018.
Alex Nichol and John Schulman. Reptile: a scalable metalearning algorithm. arXiv preprint
arXiv:1803.02999, 2(3):4, 2018.
Adam Santoro, Sergey Bartunov, Matthew Botvinick, Daan Wierstra, and Timothy Lillicrap. Meta-
learning with memory-augmented neural networks. In International conference on machine learn-
ing, pp. 1842-1850. PMLR, 2016.
Andrew M Saxe, James L McClelland, and Surya Ganguli. Exact solutions to the nonlinear dynam-
ics of learning in deep linear neural networks. arXiv preprint arXiv:1312.6120, 2013.
Jake Snell, Kevin Swersky, and Richard S Zemel. Prototypical networks for few-shot learning. arXiv
preprint arXiv:1703.05175, 2017.
Oriol Vinyals, Charles Blundell, Timothy Lillicrap, Daan Wierstra, et al. Matching networks for one
shot learning. Advances in neural information processing systems, 29:3630-3638, 2016.
A Appendix
Here, we derive the analytic form of the lower-bound of the generalization loss, defined as
Leval(W) ≡ Et[Eχt,yt [L(W, C0p; Dt))]]	(11)
where
L(W,c;D) = kcWX-yk22+kck2	(12)
XX|
=(cW - aU)-------(WTCT - UTaT)	(13)
k
where y is expanded as yk = aUxk .
The adapted ctopt is defined as the optimum of eq 13 for the given weight W and data ctopt =
argminc Lt. This optimum can be identified by the zero gradient condition wrt ct:
0 = CoPtWXX- WT - atUXX WT + ecθpt,
tk	k	t
which yields
X XT	X XT
CoPt = at(UXX W t)(WU T XX UW T+eI )-1
tk	k
(14)
We consider the case of few-shot adaptation in which the batch-size of the support set matches
the feature dimension k = df . As explained in the main text, the lower bound of the few-shot
performance is achieved by having a maximally informative support set, which spans the row space
of U: i.e.
ɪ XtX- = U tu
K
By denoting P ≡ WUT, the adapted C equation A can be expressed as
CoPt = atPj
where Pt ≡ PT(PPT + EI)-1 is pseudo-inverse of P with E regularization, which reduces to the
true pseudo-inverse in the limit → 0.
8
Under review as a conference paper at ICLR 2022
The lower-bound evaluation loss is then computed as (suppressing the regularization term kct k2
for the simplicity of notation)
Leval=Eat[EXt[kat(PtW-U)Xtk22] = Tr[(PtW - U)(W|Pt| - U|)] ≈ Tr[UU| - 2P|Pt| +Tr[W|(PP| + EI)-1W]	(15) (16) (17)
where the identities of the expectation Eat [at|at] = I and EX [XX|] = I are used at equation 16
and the approximation PtTPt ? ≈ (PP | + I)-1 which holds in the limit → 0, is used at
equation 17.
Expanding Pand rearranging terms in equation 17 yields
Leval,opt = Tr[UU|] + Tr[(WU|UW| + I)-1(W W | - 2WU|UW| + I)]	(18)
A.1 SVD analysis of equation 18
Given the singular value decomposition of W: W = OσV |. we consider the case in which each of
the right singular vectors in V is either completely inside or outside the the row space of W. That is,
the norm of the projected vector Pln (Pm Ulm Vmn)2 is either zero or one, which is described by
a indicator function: Ij = 1 if the ith singular mode is overlaps with W, or 0 otherwise. Expanding
equation 18 with the SVD decomposition W = OσV| simplifies to
Leval,opt =Tr[I+ (σ2I + EI)-1(σ2 -2σ2I+E)]
Xlr	σ2 - 2σ2Ii + E
=u+	σ Ii+E
ii
Σ
i
σi2Ii + EIi + σi2 - 2σi2Ii + E
σ2Ii + E
X σ2 + E + (-σ2 + E)Ii
i
X
i∈I
σ2 Ii + E
2E + X σ2+E
E 2 丁
(19)
(20)
(21)
(22)
(23)
9