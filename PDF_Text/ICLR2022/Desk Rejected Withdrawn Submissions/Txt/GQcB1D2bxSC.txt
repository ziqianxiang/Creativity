Under review as a conference paper at ICLR 2022
SHAQ: Incorporating Shapley Value Theory
into Multi-Agent Q-Learning
Anonymous authors
Paper under double-blind review
Ab stract
Value factorisation proves to be a useful technique in multi-agent reinforcement
learning (MARL), but the underlying mechanism is not yet fully understood. This
paper explores a theoretical framework for value factorisation with interpretability.
We generalise Shapley value in coalitional game theory to Markov convex game
(MCG) and use it as a value factorisation method for MARL. We show that
the generalised Shapley value possesses several features such as (1) efficiency:
the sum of optimal generalised Shapley values is equal to the optimal global
value, (2) fairness in factorisation of the global value, and (3) sensitiveness to
dummy agents. Moreover, we show that MCG with the grand coalition and the
generalised Shapley value is within -core, which means no agents have large
incentives to deviate from the grand coalition. Since MCG with the grand coalition
is equivalent to global reward game, it is the first time that Shapley value is
rigorously proved to be rationally applied as a value factorisation method for
global reward game. Moreover, extending from the Bellman operator we propose
Shapley-Q operator that is proved to converge to the optimal generalised Shapley
value. With stochastic approximation, a new MARL algorithm called Shapley Q-
learning (SHAQ) is yielded. We show the performance of SHAQ on Predator-Prey
for modelling relative overgeneralisation and StarCraft Multi-Agent Challenge
(SMAC). In experiments, we also demonstrate the interpretability of SHAQ that is
lacking in the state-of-the-art baselines.
1	Introduction
Cooperative game is a critical research area in multi-agent reinforcement learning (MARL). Many
real-life tasks can be modeled as cooperative games, e.g. the coordination of autonomous vehicles
(Keviczky et al., 2007), autonomous distributed logistics (Schuldt, 2012) and search-and-rescue
robots (Ramchurn et al., 2010). In this paper, we consider global reward game (also known as team
reward game), i.e. an important subclass of cooperative games, where agents aim to maximize
cumulative global rewards over time. There are mainly two categories of methods to solve this
problem: (1) each agent identically maximizes cumulative global rewards, i.e. learning with a shared
value function (Sukhbaatar et al., 2016; Omidshafiei et al., 2018; Kim et al., 2019); and (2) some
scheme is applied to distribute the cumulative global rewards to each agent so that they are able
to maximize the factorised value function according to their own contributions, i.e. learning with
(implicit) credit assignment (e.g. marginal contribution and value factorisation) (Foerster et al., 2018;
Sunehag et al., 2018; Rashid et al., 2018; Son et al., 2019; Zhou et al., 2020).
By the view of non-cooperative game theory, global reward game can be formed as Markov game
(Shapley, 1953a) with a global reward (a.k.a. team reward). Its aim is learning a stationary joint
policy to reach a Markov equilibrium so that no agent tends to unilaterally change its policy to
maximize cumulative global rewards. Standing by this viewpoint, learning with value factorisation
is inexplicable (Wang et al., 2020c). To clearly interpret the value factorisation, in this paper we
stand by the viewpoint of coalitional game theory (Chalkiadakis et al., 2011), where the objective is
forming coalitions and finding a value assignment scheme to distribute each coalition’s value among
the agents belonging to it so that no agent have large incentives to deviate from its coalition. Such a
tuple involving a coalition structure (i.e., a collection of formed coalitions) and a value assignment
scheme is a stable solution called core, analogous to Markov equilibrium. It is obvious that the
1
Under review as a conference paper at ICLR 2022
formation of the grand coalition (i.e., only one coalition including all agents) within a core in the
context of cooperative game theory naturally interprets the value factorisation in global reward game,
if the value assignment scheme here is equivalently regarded as a value factorisation method.
Wang et al. (2020c) extended convex game (i.e., a type of game in coalitional game theory) (Chalki-
adakis et al., 2011) to dynamic scenarios that is renamed as Markov convex game (MCG) in this
paper for appropriateness. We consider MCG with the grand coalition in this paper and find a value
assignment scheme so that the solution is within the -core (i.e., a relaxation of core). Although Wang
et al. (2020c) gave an analytic form of Shapley value for MCG that mimics the original version (i.e.,
with no actions and states) for the static scenario (Shapley, 1953b), the effect of policy is neglected
so that it is not promising to be within the (-)core (which means that agents are probably irrational
to stay within the grand coalition with the Shapley value as the value assignment scheme). This
motivates us to propose the generalised Shapley value for MCG with explicit theoretical guarantees.
Given the rationality and the other properties of the generalised Shapley value shown in this paper, we
derive Shapley-Q optimality equation that is an extension of Bellman optimality equation (Bellman,
1952; Sutton & Barto, 2018). Besides, we propose Shapley-Q operator and prove its convergence to
Shapley-Q optimality equation and therefore the optimal joint deterministic policy is achieved. Since
Shapley-Q operator belongs to the class of value iteration algorithms, its stochastic approximation
called Shapley Q-learning (SHAQ) is naturally derived. Note that the above contributions were never
mentioned in the prior work.
The analytic form of the generalised Shapley value is impractical to fulfil the decentralised execution.
To address this problem, with a suppose the effect of permutations on the generalised Shapley Q-value
is transferred to the correlated weights ai(s, a。and the generalised Shapley Q-value becomes fully
decentralised. In implementation of SHAQ with deep reinforcement learning, to mitigate the sample
inefficiency caused by permutations of agents in the prior work (Wang et al., 2020c), we force the
function w.r.t. a coalition to be permutation invariant by summing up the agents’ action features (i.e.,
the decentralised generalised Shapley Q-values), which is based on the fact that each coalition is
defined as a set (Chalkiadakis et al., 2011).
To evaluate SHAQ, we run experiments on Predator-Prey for modelling the relative overgeneralisation
(Bohmer et al., 2020) to show that SHAQ possesses the ability to tackle this common game-theoretic
pathology (Wei & Luke, 2016) and the multi-agent StarCraft benchmark tasks (Samvelyan et al., 2019)
to demonstrate the performance on the more general and challenging tasks. From the experimental
results, SHAQ shows not only a generally good performance on solving all tasks, but also the
interpretability that the state-of-the-art baselines lack.
2	Background
We now define Markov convex game (MCG) following the prior work (Wang et al., 2020c) (that was
called extended convex game). We complement the definition with the concept of core (for dynamic
scenarios rather than the original static one) that was neglected from Wang et al. (2020c).
In MCG, the equation of dynamics is defined as P(SIs, a), where s, s' ∈ S and a ∈ A. S is the set of
states and A = ×i∈N Ai , where N is the set of all agents called grand coalition and Ai is an action
set of agent i belonging to N. If considering any coalition C ⊆ N , the joint action set of agents
belonging to C is denoted as AC = ×i∈CAi. vπC (s) = EπC [ ∑τ∞=t γτ-tRCτ ∣ St = s], where γ ∈ (0, 1),
represents the value function of a coalition C controlled by the policies of agents belonging to C ,
i.e., πC(aC∣s) = ×i∈Cπi(ai∣s), shortened as πC; and RCτ is the reward for coalition C at time step τ.
Accordingly, RτN is the global reward at time step τ that might be written as R(s, a) or R in the rest
of paper, where the τ might be ignored for conciseness. The value of cumulative global rewards
(i.e., with the grand coalition) is denoted as vπ (s) ∈ (0, +∞) and the value of empty coalition is
denoted as vπ0 (S) = 0. As for other coalitions C ⊂ N, VnC (S) ∈ (0, +∞). Similarly, the Q-value
for the grand coalition is defined as Qπ(s, a) ∈ (0, +∞), and the Q-value for a coalition C ⊂ N is
defined as QnC (s, ac) ∈ [0, +∞). QnC (s, a。) is defined as the optimal coalitional Q-value of C w.r.t.
πD
the optimaljointpolicy of D ⊆ C, i.e. ∏D. QnC (s, a。) is denoted as QnC (s, a。) for conciseness. The
π C
solution of MCG is a tuple containing a collection of coalitions called coalition structure and a value
assignment scheme to agents under different coalitions. Mathematically, this tuple is represented as
2
Under review as a conference paper at ICLR 2022
(CS, x(s)), where CS = {Cι,…,Cn} is the coalition structure and X(S) = (Xi(S)}i∈N is the value
assignment scheme.
There is a condition for characterizing the convexity (a.k.a. supermodularity) of MCG as follows:
max vπC∪ (S) + max vπC∩ (S) ≥ max vπCm (S) + max vπCk (S),
πC∪	πC∩	πCm	πCk
(1)
In practice, we usually assume that Cm ∩ Ck = 0, ∀Cm, Ck ⊂ N and this simplifies Eq.1 to the
definition in Wang et al.(2020c) where max∏c∩ vπC∩ (s) = 0 (since vπ0 (s) = 0).
In MCG with the grand coalition (i.e., CS = {N}), core is defined as a set of value assignment
schemes by which no agents have large incentives to deviate to gain more profits. Nevertheless,
sometimes the exact core does not exist. To address it, we define an approximate solution called
-core such that
-core = {X(S) ∣ max x(S∣C) ≥ max vπC (S) - , ∀C ⊆ N, S ∈ S },
πC	πC
(2)
where maxπC x(S∣C) = ∑i∈C maxπi xi (S). When = 0, -core becomes core. We aim to find
the smallest value of e that satisfy Eq.2 to obtain the least core denoted by e*-core, where e* =
inf { ∣ that makes Eq.2 hold}. The definition of (-)core here is a direct extension from the original
definition for static convex game (Shapley, 1971; Chalkiadakis et al., 2011). The relationship between
core and e-core is analogous to the relationship between Nash equilibrium and epsilon equilibrium.
The agent with the value assignment scheme lying in the (e-)core is defined as a rational agent.
3	Generalised S hapley Value for Markov Convex Game
In this section, we (1) construct coalitional marginal contribution; (2) use coalitional marginal
contribution to define the generalised Shapley value for MCG; (3) show that the generalised Shapley
value lies in the e*-core of MCG with the grand coalition (i.e., rationality); and (4) show the properties
of the generalised Shapley value, i.e., fairness, sensitiveness to dummy agents, and efficiency. Since
the generalised Shapley value and Shapley Q-value are equivalent, we only show the theoretical
results for the generalised Shapley value. Note that the proof of (3) above is important, since this gives
the reason why the grand coalition exists with the generalised Shapley value as the value assignment
scheme. In other words, if (3) above is invalid, then it is irrational for agents to stay within the grand
coalition, since they might gain more value assignments with other coalition structures.
Coalitional Marginal Contribution. Marginal contribution has been broadly used as a credit
assignment technique in the prior MARL algorithms, however, most of them were constructed based
on the counterfactual regret (Foerster et al., 2018). In this section, we introduce coalitional marginal
contribution by the view of coalitional game theory in Definition 1.
Definition 1. In Markov convex game (MCG), with a Sequence ofagents (j1,j2,…，j∣N∣}, Yjn ∈ N
forming the grand coalition N, where n ∈ {1, ..., ∣N ∣}, ja ≠ jb if a ≠ b, the coalitional marginal
contribution of an agent i is defined as the following equation such that
Φi(S∣Ci) = max vπCi∪{i} (S) - max vπCi (S),
πCi	πCi
(3)
where Ci = {j1, ...,jn-1} for jn = i is an arbitrary intermediate coalition where agent i would join
during the process of grand coalition formation.
Proposition 1. The coalitional marginal contribution w.r.t. the action of each agent can be derived
as follows:
Φi(s,ai∣Ci) = maxQnCi∪{i (s,aci∪{i}) - maxQnCi (s, a%)∙
aC	πCi	i	aC	i
(4)
As Proposition 1 shows, The coalitional marginal contribution w.r.t. the action of each agent
(analogous to Q-value) can be derived according to Eq.3. It is usually more useful in solving MARL
problems. We now clarify meaning of Eq.4. Q*πCi (S, aCi) indicates the optimal coalitional Q-value of
Ci w.r.t. the optimal joint policy of Ci. QππCCi∪{i} (S, aCi∪{i}) indicates the optimal coalitional Q-value
Ci
3
Under review as a conference paper at ICLR 2022
of Ci ∪ {i} w.r.t. the optimal joint policy of Ci . Note that the agents’ policies are assumed to be
independent in this work.
Generalised Shapley Value. It is apparent that coalitional marginal contribution only considers
one permutation to form the grand coalition. By the viewpoint from Shapley (1953b), the fairness is
achieved through considering all permutations of agents. Therefore, we construct the generalised
Shapley value by coalitional marginal contributions as Definition 2 shows.
Definition 2. In MCG with the grand coalition, the generalised Shapley value is represented as
vφ(s) =	∑	≡≡N⅞Ci≡ ∙Φi(s∣Ci).
Ci ⊆N /{i}	∣N∣!
With deterministic policies, the generalised Shapley Q-value is represented as
Qφ(s,ai) =	∑	ICiMNNCiI-1)! ∙ Φi(s,ai∣Ci),
Ci⊆N∖{i}	Nl!
where Φi(s, aiICi) is defined in Eq.4.
(5)
(6)
As Definition 2 shows, the generalised Shapley value is a linear combination of coalitional marginal
contributions with different permutations of agents forming the grand coalition for promising fairness.
∣Ci∣!(NNCi∣-1)! can be Seen as the probability measure over Ci, i.e., P(CiIN/{i}), and the generalised
Shapley value can be interpreted as the expectation of coalitional marginal contributions w.r.t.
Ci ~ P(CiN/{i}).
Proposition 2. In Markov convex game with the grand coalition, the generalised Shapley value
possesses several features as follows: (1) the sensitiveness to dummy agents: viφ(s) = 0; (2) efficiency:
maxπvπ(s) = ∑i∈N maxπi viφ(s).
Proposition 2 shows 2 features of the generalised Shapley value. The most important feature is (2),
which means that the maximum global value is equal to the sum of the maximum generalised Shapley
values. In other words, the maximum global value is able to be reached through each agent’s local
optimization for its generalised Shapley value. With the fairness in Definition 2, we have showed 3
features of the generalised Shapley value.
Theorem 1.	The generalised Shapley value is a solution in the e* -core of Markov convex game
(MCG) with the grand coalition and
*	_
e = SUP
C∈P(N )∖{N ,0}
• max VnC (s)}.
πC
P(∙) denotes the power set. Theorem 1 shows the rationality of the generalised Shapley value, i.e.,
no agents have large incentives to deviate from the grand coalition. In other words, the equivalence
between MCG with the grand coalition and global reward game almost holds, with the generalised
Shapley value proposed in this paper as a value assignment scheme. As a result, the generalised
Shapley value is reasonable to be a value factorisation method in global reward game.
4 Shapley Q-Learning
In this section, we firstly derive the Shapley-Q optimality equation by incorporating the efficiency of
the generalised Shapley Q-value into the Bellman optimality equation for the global Q-value, given
the results in Theorem 1 that no agents have large incentives to deviate from the grand coalition. Then,
we propose the Shapley Q-operator that can converge to the optimal generalised Shapley Q-value
and therefore the optimal joint deterministic policy can be achieved. For easy implementation, we
derive the Shapley Q-learning and its variant for fitting decentralised execution. The construction
of modules for Shapley Q-learning follows the convergence condition in Theorem 2, so that the
convergence of the learning process is possible to be guaranteed.
Shapley-Q Optimality Equation. Based on Bellman optimality equation (Bellman, 1952; Sutton &
Barto, 2018), we derive an equation called Shapley-Q optimality equation for evaluating the optimal
4
Under review as a conference paper at ICLR 2022
generalised Shapley value (see Appendix B.4) such that
Qφ* (s, a) = Iw(s, a) ∑ P(Sls, a)[R + Y ∑ max Qf (s',ai)],	(I)
s'∈S	i∈N Oi
where I is an identity matrix; w(s, a) = [wi(s, ai)]τ ∈ RN∣; Qφ (s, a) = [Qφ (s, ai)]τ ∈ RN∣ and
出*
Qiφ (S, ai) denotes the optimal generalised Shapley Q-value. Given the theoretical results in Theorem
1 that almost no agents have large incentives to deviate from the grand coalition, the derivation of
Eq.7 also depends on two conditions: (1) the efficiency of the generalised Shapley value proved in
Proposition 2; (2) a suppose that Qφ (s, aj = wi(s, aj Qn(s, a), where Qn (s, a) > 0. It reveals
an implication that ∀s ∈ S and a； = argmaXai Qφ (s, a。, we have the solution wi(s, a：) = 1∕∣N∣.
Literally, the credit assignments would be equal and each agent would receive Qn (s, a)∕∣N∣ if
making optimal decisions. It is apparent that the efficiency holds under this situation and this can
be interpreted as an extremely fair credit assignment, i.e., the award to each agent should not be
discriminated if all of them perform optimally, regardless of their roles. The equal credit assignment
was also revealed by Wang et al. (2020a) recently from another perspective of analysis. Note that the
value of Wi(s, ai) for 出 ≠ argmaXai Qφ (s, ai) is needed to find (e.g., during learning).
Shapley-Q Operator. To find an optimal solution given by Eq.7, we now propose an operator
called Shapley-Q operator, i.e., Y : ×i∈NQφ(s, ai) ↦ ×i∈NQφ(s, ai), which is specifically defined
as follows:
Y(Xi∈NQφ(s, ai)) = I w(s,a) ∑ p(s'∣s,a)[R + Y ∑ maxQφ(s',ai)],	⑻
s'∈S	i∈N ai
where wi(S, ai) = 1∕∣N ∣ when ai = arg maxai Qiφ(S, ai). We prove that the optimal joint determinis-
tic policy can be achieved through the Shapley-Q operator in Theorem 2.
Theorem 2.	Shapley-Q operator can converge to the optimal Shapley Q-values and therefore the
optimal joint deterministic policy is achieved when maxs { ∑i∈N maxai Wi (s, ai)} < γ.
Shapley Q-Learning. For easy implementation, we derive the TD error for Shapley Q-learning
(SHAQ) based on Shapley-Q operator (see Appendix B.4) such that
∆(s, a, s') = R + γ ∑ maxQφ(s',ai) - ∑ δi(s,ai) Qφ(s,ai).	(9)
i∈N ai	i∈N
where
δi (s, ai ) =
{1	ai = arg maxai Qiφ(s, ai),
αi(s, ai) ai ≠ arg maxai Qiφ(s, ai).
(10)
Here ai (s, ai) = ∣N ∣ 仪/ @)for ai ≠ argmaXai Qφ (s, aj In implementation, We should guarantee
the condition stated in Theorem 2 for the convergence of SHAQ to the optimality.
Implementation of Shapley Q-Learning. We now describe a practical implementation of SHAQ
for Dec-POMDP (Oliehoek, 2012), i.e. global reward game but with partial observations during
execution. For this reason, the history of each agent is substituted for the global state of each
generalised Shapley Q-value in implementation to guarantee the optimal deterministic joint policy
(Oliehoek, 2012). Generalised Shapley Q-value is now replaced by Qiφ(τi, ai), where τi is a history of
partial observations of agent i. Since centralised training decentralised execution (CTDE) (Oliehoek
et al., 2008) is applied, the global state S for (^i(s, ai) is able to be obtained during training.
Proposition 3. Suppose any coalitional marginal contribution can be factorised to the form such
that Φi(s, ai∣Ci) = m(s, aCi∪{i}) Qi(s, ai), with the condition such that
ECi~p(Ci∣N/{i}[m(S, aCi∪{i})]
we have
{Qi (S, ai) = Qi(S, ai)
[αi(s, ai) Qi(s, ai) = αi(s, ai) Qi(S, ai)
where ai(s,ai) = E,^p^e.Nf/.})[ &i(s,ai； aj)].
{K∈(0,1)
ai = arg maxai Qiφ(s, ai),
ai ≠ arg maxai Qiφ(s, ai),
ai = arg maxai Qi(s,ai),
ai ≠ arg maxai Qi(s,ai),
(11)
5
Under review as a conference paper at ICLR 2022
zɔ	JK 1	∙ ,1 F	, 1 ∙	1	, ∙	1	, ∙ l' , ∙ zA /	∖ , T ,1
Compatible with decentralised execution, we use only one parametric function Qi (τi , ai) to directly
approximate Qiφ(τi, ai). By Proposition 3, the information of coalition can be equivalently transferred
to ai(s, ai； ad As a result, δi(s, aj is equivalently transferred to the form as follows:
δi (s, ai ) =
{1
[αi(s,ai) = ECi~p(Ci∣N∖{i})[ αi(S,ai； aCi) ]
ai = argmaxai Qi(s,ai),
ai ≠ arg maxai Qi(s,ai).
(12)
To solve partial observations, Qi(τi, ai) is represented as recurrent neural network (RNN) with GRUs
(Chung et al., 2014). &i(s, ai； aj) is approximated by a parametric function Fs + 1 such that
1M
αi(s,ai)=而 ∑ Fs(QCk (TCk, ack ), Qi(Ti,ai)) + 1,
k=1
(13)
where QCk (TCk, ack)= 击 ∑j∈ck Qj(τj, aj) and Ck Z P(J∣N/{i}) is sampled M times to approx-
imate Ej~p(Ci∣N\{i})[ &i(s, ai； aCi)]; and Fs is a monotonic function, additionally with an absolute
activation function on the output, whose weights are generated from hyper-networks w.r.t. the global
state, similar to the architecture of QMIX (Rashid et al., 2018). We show that Eq.13 satisfies the
condition in Theorem 2 (see Appendix B.5) such that maxs { ∑i∈N max0i wi(s, ai)} < γ.
By using the framework of fitted Q-learning (Ernst et al., 2005) and inserting the above constructed
variables, the practical loss function derived from Eq.9 is therefore stated as follows:
minEs,τ,a,R,τ‛[ ( R + Y ∑ maxQi(Ti,ai； θ-) - ∑ $i(s,ai； λ) Qi(Ti,ai； θ) )2 ],	(14)
θ,λ	i∈N ai	i∈N
where all agents share the parameters of Qi(s, ai； θ) and ai(s, ai； λ) respectively; and Qi(s/, ai； θ-)
works as the target where θ- is periodically updated. The general training procedure follows the
paradigm of DQN (Mnih et al., 2013), with a replay buffer to store the online collection of agents’
episodes. To depict an overview of the algorithm, the pseudo code is shown in Appendix C.
5	Related Work
Value Factorisation. To deal with the instability during training in global reward game by inde-
pendent learner (Claus & Boutilier, 1998), centralised training and decentralised execution (CTDE)
(Oliehoek et al., 2008) was proposed and became a general paradigm for MARL. Based on CTDE,
MADDPG learned a global Q-value that can be regarded as identically assigning to all agents with
same credits during training (Wang et al., 2020c), which may cause the unfair value assignment and
therefore the difficulty for learning (Wolpert & Tumer, 2002). To avoid this problem, VDN (Sunehag
et al., 2018) was proposed to automatically learn the factorised Q-value, assuming that any global
Q-value is equal to the summation of decentralised Q-values. Nevertheless, this factorisation of
the global Q-value may cause the limitation on representation of the global Q-value. To mitigate
this problem, QMIX (Rashid et al., 2018) and QTRAN (Son et al., 2019) were proposed to repre-
sent the global Q-value with a richer class w.r.t. decentralised Q-values, based on an assumption
called Individual-Global-Max (IGM) that was mainly for showing the convergence to the optimal
joint deterministic policy. Shapley Q-value proposed in this paper belongs to the family of value
factorisation methods, but based on the theoretical framework of MCG that is interpretable in theory
and with the realistic insights from static cooperative games, e.g. network flow games (Kalai &
Zemel, 1982), induced subgraph game (Deng & Papadimitriou, 1994) that can be used for modelling
social networks, and facility location games (Deng et al., 1999). These games show meaningful
representations for coalition and its reward, which verifies the realistic existence of the concepts
introduced in MCG. In real world, these concepts cannot be always explicitly defined, however, it can
be assumed that these concepts inherently exist and only the exposed concept (e.g. the global reward)
needs to be focused on. Compared to IGM, the efficiency of the generalised Shapley Q-value plays a
significant role of proving the convergence to the optimal joint deterministic policy. The generalised
Shapley value can be categorised as a sort of linear value factorisation and the efficiency of linear
value factorisation was also studied by Wang et al. (2020a) from another perspective of analysis.
Relationship to VDN. It is apparent to observe that by setting δi(s, ai) = 1 for any state-action
pairs, SHAQ is degraded to VDN (Sunehag et al., 2018). In other words, VDN is a subclass of SHAQ.
6
Under review as a conference paper at ICLR 2022
As a result, we show the reason why VDN works well in most scenarios by the framework of MCG
proposed in this paper. In this sense, its poor performance on some scenarios is due to the mistakes
on defining δi(s, ai) = 1 in Eq.9 over the sub-optimal actions.
Comparison with SQDDPG. Since this work is an extension from Wang et al. (2020c), we clarify
the difference between these two works to avoid the confusions. (1) As Wang et al. (2020c) claimed,
there exists an efficient value assignment schemes that guarantees a solution in the core of MCG
with the grand coalition. We show that this value assignment scheme is actually coalitional marginal
contribution proposed in this paper (see Proposition 5 in Appendix B.2). The fault led by Wang et al.
(2020c) is that the property of MCG is misunderstood as identical to that of the original convex game
(Chalkiadakis et al., 2011). However, in our results the consideration of policy changes the property
of MCG and lead to the different theoretical results for Shapley value (see Theorem 1). Fortunately,
the correctness of functional approximation for Shapley Q-value by Wang et al. (2020c) is unaffected,
and it can be regarded as an implementation of the generalised Shapley Q-value proposed in this
paper. (2) We propose a learning algorithm called Shapley Q-learning that was never mentioned in
the prior work. (3) In implementation, the sample complexity of SQDDPG for different permutations
of agents is higher than that of SHAQ. For discrete states and actions, SQDDPG attempts to directly
learn the value of different permutations (i.e., ∣S ∣∣A∣∣N ∣! possible Q-values), while SHAQ applies the
fact that the value for the same coalition is identical (since a coalition is a set) and learns an invariant
value function for each coalition (i.e., ∣S∣∣A∣[ ∣N∣! - ∑C⊆N [ ∣C∣!(∣N ∣ - ∣C∣)! - 1 ] ] possible Q-values).
6	Experiments
In this section, we show the experimental results of SHAQ on predator-prey (Bohmer et al., 2020)
and various tasks in StarCraft Multi-Agent Challenge (SMAC) 1. The baselines that we select for
comparison are COMA Foerster et al. (2018), VDN (Sunehag et al., 2018), QMIX (Rashid et al.,
2018), MASAC (Iqbal & Sha, 2019), QTRAN (Son et al., 2019), QPLEX (Wang et al., 2020b)
and W-QMIX (including CW-QMIX and OW-QMIX) (Rashid et al., 2020). The implementation
details of our algorithm are shown in Appendix D.1, whereas the implementation of baselines are
from Rashid et al. (2020) 2. We also compare SHAQ with SQDDPG (Wang et al., 2020c) 3,which
is left to Appendix E.4 due to the limitation of space. For all experiments, we use the -greedy
exploration strategy, where is annealed from 1 to 0.05. The annealing time steps vary among
different experiments. For predator-prey, we apply 1 million time steps for annealing, following the
setup from Wang et al. (2020b). For easy and hard maps in SMAC, we apply 50k time steps for
annealing, the same as that in Samvelyan et al. (2019); while for super-hard maps in SMAC, we
apply 1 million time steps for annealing to obtain more explorations so that more state-action pairs
can be visited. About the replay buffer size, we set 5000 for all algorithms that is the same as Rashid
et al. (2020). To fairly evaluate all algorithms, we run each experiment with 5 random seeds. All
graphs showing experimental results are plotted with the median and 25%-75% quartile shading.
6.1	Predator-Prey for Modelling Relative Overgeneralisation
EnISH B,Lue-Paw
---SHAQ
---QPLEX
——QMIX
---VDN
——CW-QMIX
——OW-QMIX
---QIRAN
---COMA
——MASAC
En5H-js8h∙ue-paw
---SHAQ
---QPLEX
——QMIX
---VDN
——CW-QMIX
——CW-QMlX
---QTRAN
---∞MA
——MASAC
En5H-js8h∙ue-paw
---SHAQ
---QPLEX
——QMK
---VDN
——CW-QMK
——CW-QMlX
--QTRAN
--∞MA
——MASAC
(a) p = - 0.5.	(b) p = - 1.
(c)p = - 2.
Figure 1: Median test return for Predator-Prey with different values of p.
1The version that we use in this paper is SC2.4.6.2.69232 rather than the newer SC2.4.10. As reported from
Rashid et al. (2020), the performance is not comparable across versions.
2The source code of baseline implementation is from https://github.com/oxwhirl/wqmix.
3The code of SQDDPG is implemented based on https://github.com/hsvgbkhgbv/SQDDPG.
7
Under review as a conference paper at ICLR 2022
In this seCtion, we run the experiments on a partially-observable task Called Predator-Prey for
modelling relative overgeneralisation (Bohmer et al., 2θ2θ). Relative overgeneralisation is a common
game theoretiC pathology that the suboptimal aCtions is preferred when matChed with arbitrary aCtions
from the collaborating agents (Wei & Luke, 2016), where 8 predators that we can control aim to
capture 8 preys with random policies in a 10x10 grid world. Each agent’s observation is a 5x5
sub-grid centering around it. If a prey is captured by coordination of 2 agents, predators will be
rewarded by 10. On the other hand, each unsuccessful attempt by only 1 agent will be punished by a
negative reward p. As Rashid et al. (2020) reported, only QTRAN and W-QMIX can solve this task,
while Wang et al. (2020b) found that the failure was primarily due to the lack of explorations. For
this reason, we apply the identical epsilon annealing schedule (i.e. 1 million time steps) with that in
Wang et al. (2020b). As Figure 1 shows, SHAQ can solve the relative overgeneralisation tasks with
different levels (i.e., with different values of p). With the epsilon annealing strategy from Wang et al.
(2020b), W-QMIX does not perform as well as reported in Rashid et al. (2020). The reason could
be its poor robustness to the increased explorations (Rashid et al., 2020) for this environment. The
good performance of VDN validates our analysis in Section 5, whereas the performance of QTRAN
is surprisingly almost invariant to the value of p. The performances of QPLEX and QMIX become
obviously worse when p = - 2. The failures of MASAC and COMA could be due to that relative
obvergeneralisation prevents policy gradient methods from better coordination (Wei et al., 2018).
6.2	StarCraft Multi-Agent Challenge
We now evaluate SHAQ on the more challenging SMAC tasks, the environmental settings of which
are the same as that in Samvelyan et al. (2019). To broadly compare the performance of SHAQ
with baselines, We select 4 easy maps: 8m, 3s5z, 1c3s5z and 10m_vs_11m; 3 hard maps: 5m_vs_6m,
3s_vs_5z and 2c_vs_64zg; and 4 super-hard maps: 3s5z_vs_3s6z, Corridor, MMM2 and 6h_vs_8z. All
training is through online data collection. Due to the limited space, we only show partial results in
the main part of paper and leave the rest of results in Appendix E.2.
Performance Analysis. First, we compare performances between SHAQ and baselines. It shows
in Figure 2 that SHAQ outperforms all baselines on 3 hard maps, meanwhile SHAQ can tackle
the challenging super-hard maps. For 3s5z_vs_3s6z, SHAQ possesses the fastest convergence rate
and the best final performance. For MMM2, SHAQ suffers from the slower convergence rate but
can achieve a good final performance. On 6h_vs_8z, SHAQ can beat other baselines except for
CW-QMIX. Surprisingly, VDN performs well on most of hard maps, which verifies our analysis in
Section 5. QMIX and QPLEX perform well on the most of maps, except for 3z_vs_5z, 2c_vs_64zg and
6h_vs_8z. As for COMA, MADDPG and MASAC, their poor performances could be due to the weak
adaptability in challenging tasks. Although QTRAN can theoretically represent the complete class of
global Q-value (Son et al., 2019), its complicated learning paradigm could impede its generalisation
to challenging tasks which results in its poor performance. W-QMIX can perform better than QMIX
on some maps that verifies that the problem for QMIX on the restricted representation of global
Q-value is fixed to some extent. Nevertheless, due to the inevitable hyperparameter tuning with no
laws Rashid et al. (2020), it is difficult to be generalised to all scenarios (see Appendix E.3).
*EM-j3u5paw
9.Z5 O.∞ 0.7S
ZS 1.50
一SHAQ
---QPLEX
——QMK
--VDN
——CW-QMIX
——OW-QMlX
--QTRAN
∞MA
MASAC
*EM-j3u5paw
i 159 1.75 ZOO
---SHAQ
---QPLEX
——QMIX
---VDN
——CW-QMIX
——OW-QMlX
--QTRAN
--OTMA
——MASAC
一SHAQ
--QPLEX
——QMIX
--VDN
——CW-QMK
——OW-QMlX
--QTRAN
—8MA
——MASAC
025 0.50 0.75 1.00 1.25 1.50 1.75 2.00
T (mil)
(a) 5m_vs_6m.
*EM-j3 u5pθw
2	3	4
T (mil)
—SHAQ
--QPLEX
——QMK
--VDN
——CW-QMIX
——OW-QMlX
--QTRAN
—OTMA
——MASAC
*EM-j3 u5pθw
(b) 3s_vs_5z.
SHAQ
QPLEX
QMlX
VDN
CW-QMIX
OW-QMlX
QTRAN
COMA
MASAC
(C) 2c_vs_64zg.
—SHAQ
--QPLEX
——QMIX
--VDN
——CW-QMK
——OW-QMlX
--QTRAN
—8MA
——MASAC
5
(d) 3s5z_vs_3s6z.	(e) MMM2.	(f) 6h_vs_8z.
Figure 2: Median test win % for hard (a-C), and super-hard (d-f) maps of SMAC.
8
Under review as a conference paper at ICLR 2022
Interpretability of SHAQ. To show the interpretability of SHAQ, we conduct a test on 3m (i.e.
a simple task in SMAC) with both -greedy policy (for obtaining both optimal and sub-optimal
decisions) and greedy policy (for certainly obtaining optimal decisions). As seen from Figure 3a,
Agent 3 faces the direction opposite to enemies, meanwhile, the enemies are apparently out of its
attacking range. It means that Agent 3 does not contribute to the team, i.e., it is almost a dummy
agent, so it only receives 0.8400 (near 0) as its generalised Shapley value. In contrast, Agent 1 and
Agent 2 are attacking enemies. However, Agent 1 additionally suffers from more attacks (with lower
health) than Agent 2. As a result, Agent 1 contributes more than Agent 2 and therefore receives more
credits as its generalised Shapley value. On the other hand, we can see from Figure 3e that with the
optimal policies all agents receive almost identical credits as generalised Shapley values. This is
consistent with the analysis of Shapley-Q optimality equation that agents should receive equal credits
if they execute the optimal joint actions. Note that if any agent executes a sub-optimal action, it will
earn different credits that is deserved. These all together constructs the fairness of credit assignments.
In summary, we demonstrate that (1) when agents perform optimal policies, they receive almost
identical credits as generalised Shapley values; (2) when an agent performs as a dummy agent, it
receives credits near 0 as its generalised Shapley value; and (3) each agent’s generalised Shapley
value is positively correlated to its contribution, so in fairness. To verify that the generalised Shapley
values learned by SHAQ is non-trivial, we also show the results for VDN, QMIX and QPLEX. It
is surprising that the decentralised Q-values of all baselines are also almost identical among agents
for the optimal decisions. Since VDN is a subclass of SHAQ and possesses the same form of loss
function for optimal actions, it is reasonable to obtain the similar results to SHAQ. The explanation
for results of QMIX and QPLEX deserves to be studied in the future work. As for the sub-optimal
decisions, VDN does not possess an explicit interpretation as SHAQ due to incorrectly defining
δi(s, ai) = 1 over sub-optimal actions, which verifies our analysis in Section 5. QMIX and QPLEX
cannot show explicit interpretations over sub-optimal decisions.
(c) QMIX: -greedy.
(d) QPLEX: -greedy.
(a) SHAQ: -greedy. (b) VDN: -greedy.
(e) SHAQ: greedy.
(f) VDN: greedy.
Figure 3: Visualisation of the evaluation for SHAQ and baselines on 3m in SMAC: each colored
circle is the centered attacking range of a controllable agent (in red), and each agent’s factorised
Q-value is reported on the right. We mark the direction that each agent face by an arrow for clearness.
(g) QMIX: greedy.
(h) QPLEX: greedy.
7	Conclusion
This paper generalises Shapley value in coalitional game theory to Markov Convex Game (MCG)
through constructing an appropriate analytic form of coalitional marginal contribution. We also
formally prove the rationality of Shapley value for MCG with the grand coalition (i.e. equivalent to
global reward game), so that it can be used as a value factorisation method for global reward game.
Moreover, we show the property of the generalised Shapley value: (1) efficiency, (2) sensitiveness to
dummy agents, and (3) fairness. By the property of efficiency, we propose Shapley-Q operator with the
proof of convergence to the optimal joint deterministic policy as well as its stochastic approximation
called Shapley Q-learning (SHAQ). We evaluate SHAQ on Predator-Prey for modelling relative
overgeneralisation (Bohmer et al., 2020) and the challenging multi-agent StarCraft benchmark tasks
(Samvelyan et al., 2019). SHAQ shows generally good performance with interpretability, compared
with state-of-the-art multi-agent reinforcement learning algorithms. Specifically, it verifies the
theoretical results such as sensitiveness to dummy agents and fairness via complicated experiments.
9
Under review as a conference paper at ICLR 2022
Ethics S tatement
This work is still standing on the theoretical analysis, so no further social impacts or ethical problems
will be caused at the moment.
Reproducibility S tatement
In this section, we discuss the reproducibility of this work. First, the complete proofs of the theoretical
results are shown in Appendix. Specifically, the assumptions and some preliminary theoretical results
that aids the proofs of main theoretical claims are shown in Appendix B.1 and B.2 respectively. The
proofs of theoretical claims in Section 3 are shown in Appendix B.3. The proofs of theoretical claims
and derivations in Section 4 are shown in Appendix B.4 and B.5. To help readers understand the
theoretical framework defined in Section 2, we also provide specific discussions and examples in
Appendix A.2. The details of experiments are shown in Section 6 and Appendix D. The pseudo code
of Shapley Q-learning (SHAQ) is shown in Appendix C, for helping implementing the algorithm.
Finally, we also include the source code for implementing SHAQ in the supplementary materials.
References
Marco Ancona, Cengiz Oztireli, and Markus Gross. Explaining deep neural networks with a
polynomial time algorithm for shapley value approximation. In International Conference on
Machine Learning, pp. 272-281. pMlR, 2019.
Stefan Banach. SUr les operations dans les ensembles abstraits et leur application aux equations
inte´grales. Fund. math, 3(1):133-181, 1922.
Richard Bellman. On the theory of dynamic programming. Proceedings of the National Academy of
Sciences of the United States of America, 38(8):716, 1952.
Wendelin Bohmer, Vitaly Kurin, and Shimon Whiteson. Deep coordination graphs. In International
Conference on Machine Learning, pp. 980-991. pMLR, 2020.
Richard H. Byrd, Gillian M. Chin, Jorge Nocedal, and Yuchen Wu. Sample size selection in
optimization methods for machine learning. Mathematical Programming, 134(1):127-155, 2012.
doi: 10.1007/s10107-012-0572-5.
Georgios Chalkiadakis, Edith Elkind, and Michael Wooldridge. Computational aspects of cooperative
game theory. Synthesis Lectures on Artificial Intelligence and Machine Learning, 5(6):1-168,
2011.
Junyoung Chung, Caglar Gulcehre, KyungHyun Cho, and Yoshua Bengio. Empirical evaluation of
gated recurrent neural networks on sequence modeling. arXiv preprint arXiv:1412.3555, 2014.
Caroline Claus and Craig Boutilier. The dynamics of reinforcement learning in cooperative multiagent
systems. AAAI/IAAI, 1998(746-752):2, 1998.
Harold Garth Dales, H Garth Dales, Pietro Aiena, Jorg Eschmeier, Kjeld Laursen, and George A
Willis. Introduction to Banach algebras, operators, and harmonic analysis, volume 57. Cambridge
University Press, 2003.
Xiaotie Deng and Christos H Papadimitriou. On the complexity of cooperative solution concepts.
Mathematics of operations research, 19(2):257-266, 1994.
Xiaotie Deng, Toshihide Ibaraki, and Hiroshi Nagamochi. Algorithmic aspects of the core of
combinatorial optimization games. Mathematics of Operations Research, 24(3):751-766, 1999.
Damien Ernst, Pierre Geurts, and Louis Wehenkel. Tree-based batch mode reinforcement learning.
Journal of Machine Learning Research, 6:503-556, 2005.
Jakob N Foerster, Gregory Farquhar, Triantafyllos Afouras, Nantas Nardelli, and Shimon Whiteson.
Counterfactual multi-agent policy gradients. In Thirty-Second AAAI Conference on Artificial
Intelligence, 2018.
10
Under review as a conference paper at ICLR 2022
Ian J Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil
Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial networks. arXiv preprint
arXiv:1406.2661, 2014.
David Ha, Andrew Dai, and Quoc V. Le. Hypernetworks. 2017. URL https://openreview.
net/pdf?id=rkpACe1lx.
Thomas Hofmann, Aurelien Lucchi, Simon Lacoste-Julien, and Brian McWilliams. Variance reduced
stochastic gradient descent with neighbors. In C. Cortes, N. Lawrence, D. Lee, M. Sugiyama, and
R. Garnett (eds.), Advances in Neural Information Processing Systems, volume 28, pp. 2305-2313.
Curran Associates, Inc., 2015.
Shariq Iqbal and Fei Sha. Actor-attention-critic for multi-agent reinforcement learning. In Interna-
tional Conference on Machine Learning, pp. 2961-2970. PMLR, 2019.
Tommi Jaakkola, Michael I Jordan, and Satinder P Singh. On the convergence of stochastic iterative
dynamic programming algorithms. Neural computation, 6(6):1185-1201, 1994.
Ruoxi Jia, David Dao, Boxin Wang, Frances Ann Hubis, Nick Hynes, Nezihe Merve Gurel, Bo Li,
Ce Zhang, Dawn Song, and Costas J Spanos. Towards efficient data valuation based on the
shapley value. In The 22nd International Conference on Artificial Intelligence and Statistics, pp.
1167-1176. PMLR, 2019.
Ehud Kalai and Eitan Zemel. Generalized network problems yielding totally balanced games.
Operations Research, 30(5):998-1008, 1982.
TamaS Keviczky, Francesco Borrelli, Kingsley Fregene, Datta Godbole, and Gary J Balas. De-
centralized receding horizon control and coordination of autonomous vehicle formations. IEEE
Transactions on control systems technology, 16(1):19-33, 2007.
Daewoo Kim, Sangwoo Moon, David Hostallero, Wan Ju Kang, Taeyoung Lee, Kyunghwan Son,
and Yung Yi. Learning to schedule communication in multi-agent reinforcement learning. In
International Conference on Learning Representations, 2019.
I Elizabeth Kumar, Suresh Venkatasubramanian, Carlos Scheidegger, and Sorelle Friedler. Problems
with shapley-value-based explanations as feature importance measures. In International Conference
on Machine Learning, pp. 5491-5500. PMLR, 2020.
Ryan Lowe, Yi Wu, Aviv Tamar, Jean Harb, OpenAI Pieter Abbeel, and Igor Mordatch. Multi-agent
actor-critic for mixed cooperative-competitive environments. In Advances in Neural Information
Processing Systems, pp. 6379-6390, 2017.
Scott Lundberg and Su-In Lee. A unified approach to interpreting model predictions. arXiv preprint
arXiv:1705.07874, 2017.
Anuj Mahajan, Mikayel Samvelyan, Lei Mao, Viktor Makoviychuk, Animesh Garg, Jean Kossaifi,
Shimon Whiteson, Yuke Zhu, and Animashree Anandkumar. Tesseract: Tensorised actors for
multi-agent reinforcement learning. In Marina Meila and Tong Zhang (eds.), Proceedings of the
38th International Conference on Machine Learning, ICML 2021, 18-24 July 2021, Virtual Event,
volume 139 of Proceedings of Machine Learning Research, pp. 7301-7312. PMLR, 2021.
Francisco S Melo. Convergence of q-learning: A simple proof. Institute Of Systems and Robotics,
Tech. Rep, pp. 1-4, 2001.
Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Alex Graves, Ioannis Antonoglou, Daan
Wierstra, and Martin Riedmiller. Playing atari with deep reinforcement learning. arXiv preprint
arXiv:1312.5602, 2013.
Frans A Oliehoek. Decentralized pomdps. In Reinforcement Learning, pp. 471-503. Springer, 2012.
Frans A Oliehoek, Matthijs TJ Spaan, and Nikos Vlassis. Optimal and approximate q-value functions
for decentralized pomdps. Journal of Artificial Intelligence Research, 32:289-353, 2008.
11
Under review as a conference paper at ICLR 2022
Shayegan Omidshafiei, Dong-Ki Kim, Miao Liu, Gerald Tesauro, Matthew Riemer, Christopher
Amato, Murray Campbell, and Jonathan P How. Learning to teach in cooperative multiagent
reinforcement learning. arXiv preprint arXiv:1805.07830, 2018.
Sarvapali D Ramchurn, Alessandro Farinelli, Kathryn S Macarthur, and Nicholas R Jennings. Decen-
tralized coordination in robocup rescue. The ComputerJournal, 53(9):1447-1461, 2010.
Tabish Rashid, Mikayel Samvelyan, Christian Schroder de Witt, Gregory Farquhar, Jakob N. Foerster,
and Shimon Whiteson. QMIX: monotonic value function factorisation for deep multi-agent
reinforcement learning. In Proceedings of the 35th International Conference on Machine Learning,
ICML 2018, StOckhOlmsmassan, Stockholm, Sweden, July 10-15, 2018, volume 80 of Proceedings
of Machine Learning Research, pp. 4292-4301. PMLR, 2018.
Tabish Rashid, Gregory Farquhar, Bei Peng, and Shimon Whiteson. Weighted qmix: Expanding
monotonic value function factorisation for deep multi-agent reinforcement learning. Advances in
Neural Information Processing Systems, 33, 2020.
Mikayel Samvelyan, Tabish Rashid, Christian Schroeder de Witt, Gregory Farquhar, Nantas Nardelli,
Tim GJ Rudner, Chia-Man Hung, Philip HS Torr, Jakob Foerster, and Shimon Whiteson. The
starcraft multi-agent challenge. arXiv preprint arXiv:1902.04043, 2019.
Arne Schuldt. Multiagent coordination enabling autonomous logistics. KI-KunstliChe Intelligenz, 26
(1):91-94, 2012.
Lloyd S Shapley. Stochastic games. Proceedings of the national academy of sciences, 39(10):
1095-1100, 1953a.
Lloyd S Shapley. A value for n-person games. Contributions to the Theory of Games, 2(28):307-317,
1953b.
Lloyd S Shapley. Cores of convex games. International journal of game theory, 1(1):11-26, 1971.
Kyunghwan Son, Daewoo Kim, Wan Ju Kang, David Hostallero, and Yung Yi. QTRAN: learning to
factorize with transformation for cooperative multi-agent reinforcement learning. In Proceedings
of the 36th International Conference on Machine Learning, ICML 2019, 9-15 June 2019, Long
Beach, California, USA, volume 97 of Proceedings of Machine Learning Research, pp. 5887-5896.
PMLR, 2019.
Sainbayar Sukhbaatar, arthur szlam, and Rob Fergus. Learning multiagent communication with
backpropagation. In Advances in Neural Information Processing Systems 29, pp. 2244-2252.
Curran Associates, Inc., 2016.
Peter Sunehag, GUy Lever, Audrunas Gruslys, Wojciech Marian Czarnecki, Vinlcius Flores Zambaldi,
Max Jaderberg, Marc Lanctot, Nicolas Sonnerat, Joel Z. Leibo, Karl Tuyls, and Thore Graepel.
Value-decomposition networks for cooperative multi-agent learning based on team reward. In
Proceedings of the 17th International Conference on Autonomous Agents and MultiAgent Systems,
AAMAS 2018, Stockholm, Sweden, July 10-15, 2018, pp. 2085-2087. International Foundation for
Autonomous Agents and Multiagent Systems Richland, SC, USA / ACM, 2018.
Richard S Sutton and Andrew G Barto. Reinforcement learning: An introduction. MIT press, 2018.
Jianhao Wang, Zhizhou Ren, Beining Han, Jianing Ye, and Chongjie Zhang. Towards understanding
linear value decomposition in cooperative multi-agent q-learning. arXiv preprint arXiv:2006.00587,
2020a.
Jianhao Wang, Zhizhou Ren, Terry Liu, Yang Yu, and Chongjie Zhang. Qplex: Duplex dueling
multi-agent q-learning. arXiv preprint arXiv:2008.01062, 2020b.
Jianhong Wang, Yuan Zhang, Tae-Kyun Kim, and Yunjie Gu. Shapley q-value: A local reward ap-
proach to solve global reward games. Proceedings of the AAAI Conference on Artificial Intelligence,
34(05):7285-7292, Apr 2020c.
12
Under review as a conference paper at ICLR 2022
Tonghan Wang, Tarun Gupta, Anuj Mahajan, Bei Peng, Shimon Whiteson, and Chongjie Zhang.
RODE: learning roles to decompose multi-agent tasks. In 9th International Conference on Learning
Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021. OpenReview.net, 2021.
Ermo Wei and Sean Luke. Lenient learning in independent-learner stochastic cooperative games. The
Journal of Machine Learning Research ,17(1):2914-2955, 2016.
Ermo Wei, Drew Wicke, David Freelan, and Sean Luke. Multiagent soft q-learning. In 2018 AAAI
Spring Symposium Series, 2018.
David H Wolpert and Kagan Tumer. Optimal payoff functions for members of collectives. In
Modeling complexity in economic and social systems, pp. 355-369. World Scientific, 2002.
Meng Zhou, Ziyu Liu, Pengwei Sui, Yixuan Li, and Yuk Ying Chung. Learning implicit credit
assignment for multi-agent actor-critic. arXiv preprint arXiv:2007.02529, 2020.
13
Under review as a conference paper at ICLR 2022
A Additional Background
A. 1 Value Factorisation in MARL
Although there are lots of works on (Q-)value factorisation in MARL, most of them are based on an
assumption called Individual-Global-Max (IGM) (Son et al., 2019) that is defined in Definition 3.
Definition 3. For a joint Q-value Qπ (s, a) with a deterministic policy, if the following equation is
assumed to hold such that
arg maax Qπ (s, a) = (argmaaxQi(s,ai))i=1,2,...,∣N∣,	(15)
then we say that (Qi (s, ai))i=1,2,...,∣N ∣ satisfies Individual-Global-Max (IGM) and Qπ (s, a) can be
factorised by (Qi (s, ai))i=1,2,...,∣N ∣.
There are 3 popular frameworks that are followed by most of works implementing the IGM, called
VDN (Sunehag et al., 2018), QMIX (Rashid et al., 2018) and QTRAN (Son et al., 2019).
VDN. VDN linearly factorises a global value function such that
Qπ (s, a) = ∑ Qi(s,ai),	(16)
i∈N
so that Eq.15 holds.
QMIX. QMIX learns a monotonic mixing function fS : ×i∈NQi(s, ai) × S ↦ R to implement the
factorisation such that
Qπ(s,a)=fs(Q1(s,a1),...,Q∣N∣(s,a∣N∣)),	(17)
so that Eq.15 holds. Although QMIX has a richer functional class of factorisation than that of VDN, it
meets a problem that maxa Qπ(s, a) = ∑i∈N maxai Qi(s, ai) does not necessarily hold, which may
lead to the bias on Q-value estimation (Son et al., 2019) and affect the learning process to achieve the
optimal joint policy. Theoretically, VDN does not possess the problem discussed above, however, the
functional class of the simply additive factorisation is so restrictive (Rashid et al., 2018).
QTRAN. QTRAN gives a sufficient condition for value factorisation that satisfies IGM such that
∑ Qi(s,ai)-Qπ(s, a) + vπ(s) = {0 n a = a,
i∈N	≥ 0 a ≠ a,
(18)
where
vπ(S) = maxQn(s,a) - ∑ Qi(s,ɑi).
a	i∈N
In Eq.18, a = ×i∈N。八 and a = ×i∈Nali where ali = argmaxai Qi(s, ai) because of IGM. Additionally,
Son et al. (2019) showed that the above condition also holds for affine transformation on Qi, ∀i ∈ N
such that WiQi + bi. For this reason, an additional transformed global Q-value such that Qπ'(s, a)=
∑i∈N Qi(S, ai) by setting wi = 1 and ∑i∈N bi = 0 is used to represent the value factorisation. It is
forced to fit the above condition with a learned global Q-value Qπ(S, a) and vπ(S). Son et al. (2019)
argued that finding the factorisation of Qπ (s, a) is equivalent to finding [Qi]i∈N to satisfy IGM.
Therefore, a value factorisation for obtaining decentralised Q-values that satisfies IGM is found.
A.2 Interpretation of Definitions for MCG
Condition of Markov Convex Game. Eq.1 implies a fact existing in most real-life scenarios that a
larger coalition results in the greater optimal global value in cooperation and therefore greater optimal
value (credit) assignments (see Remark 1) that directly increases the agents’ incentives for joining the
grand coalition. This can be regarded as a reason for raising and studying global reward game with
value factorisation. This interpretation for dynamic scenarios in this paper is consistent with the one
given by Shapley (1971) for the static scenarios, known as the snowball effect.
Remark 1. Suppose that C1 = {1, 2} and C2 = {3}, so C∪ = C1 ∪ C2 = {1, 2, 3} and C∩ = C1 ∩
C2 = 0. Therefore, by Eq.1 we Can write that max∏{i ? 3} vπ{1,2,3} (s) ≥ max∏{i 2} vπ{1,2} (s) +
maxπ{3} vπ{3} (S). As a result, we can get that
max vπ{1,2,3} (S) - max vπ{1,2} (S) = max Φ{3} (S∣C{1,2}) ≥ max vπ{3} (S).	(19)
14
Under review as a conference paper at ICLR 2022
Since C2 only includes agent 3, maxπ{3} vπ{3} (s) is naturally agent 3’s optimal value assignment
here. It is obvious that with a larger coalition, agent 3’s optimal value assignment increases.
Insight into e-core. InEq.2, X(S) = (Xi(S) }i∈N indicates the value assignment scheme for the grand
coalition. In the context of cooperative game theory, it is called imputation vector. maxπC x(s∣C) =
∑i∈C maxπi xi(S) indicates the sum of value assignments (for the grand coalition) of the agents who
is in comparison under coalition C. By Remark 2 and 3, it is obvious that Eq.2 indicates that the
optimal global value obtained by the value assignment scheme in the e-core with the grand coalition is
almost no less than that they can achieve with other coalition structures, which is called the maximal
social welfare in the prior work (Wang et al., 2020c). It is an intuitive interpretation of e-core (with
the grand coalition).
Remark 2. Suppose that a coalition structure is CS = {C1, C2, ..., Cn} where ∪kn=1Ck = N
and each Ck is mutually exclusive (i.e., Cm ∩ Cn = 0, if m ≠ n), the optimal global value with respect
to CS is represented as maxπ vπ(S) = ∑kn=1 maxπC vπCk (s).
Remark 3. Suppose that the condition of e-core holds for the grand coalition (i.e. N) with some value
assignment scheme x(s) = (Xi(S)}i∈N. For an arbitrary coalition structure CS = {Cι, C2,..., Cn}
other than {N}, where ∪kn=1Ck = N and each Ck is mutually exclusive, we can write down the
equation such that
max X(S∣Ck ) ≥ max vπCk (S) - e, ∀Ck ∈ CS.	(20)
πCk	πCk
Ifwe sum up Eq.20 for all coalitions in CS, we can get the following equation such that
∑ max X(S∣Ck ) ≥ ∑ max vπCk - ∣CS∣e.	(21)
Ck∈CS πCk	Ck∈CS πCk
Recall that maxπC X(S∣Ck) = ∑j∈Ck maxπi Xi(S). The LHS of Eq.21 can be written as follows:
∑ max X(S∣Ck ) =∑∑maxXi(S) = ∑ maxXi(S) = maxvπ(s),	(22)
Ck ∈CS πCk	Ck∈CSj∈Ck πi	j∈N πi	π
where max∏ vπ (s) is denoted as the optimal global value obtained by the value assignment SCheme
in the e-core. By the result in Remark 2, the RHS of Eq.21 can be written as follows:
∑ max vπCk - ∣CS∣e = maxvπ(S) - ∣CS∣e,	(23)
Ck ∈CS πCk	π
where maxπ vπ (S) is the optimal global value obtained by coalition structures. By substituting Eq.22
and 23 into Eq.21, we can get that
max vπ(s) ≥ max vπ (s) 一 ∣CS∣e.
ππ
If ∣CS∣e ≥ 0 is bounded, we say that the optimal global value obtained by the value assignment scheme
in the e-core is almost no less than the optimal global value obtained by CS. Note that the value of
∣CS ∣e should be as small as possible.
B Complete Mathematical Proofs
B.1	Assumptions
Assumption 1. Any joint policy can be factorised to a permutation of decentralised (i.e. disjoint)
policies based on predecessor coalitions, i.e., πC = ×C ∈Π(C)πi(Ci). Π(C) here is a set of predecessor
coalitions induced by an arbitrary permutation (or a sequence) of agents to form a coalition C and
πi(Ci) is a policy of agent i for the predecessor coalition Ci. This is consistent with the definition of
vπC (S) that is a set-valued function. If only one permutation is considered, πi(Ci) will be denoted as
πi for simplification and conciseness.
Assumption 2. The functional space of each agent i’s policy πi is able to be separable with respect
to the predecessor coalitions, i.e., ∏ = L∣Ci⊆N/{i} ∏i (Ci), Yi ∈ N, where U denotes the disjoint
union. For example, πi(Ci) ∩ πi(Di) = 0, ifCi ≠ Di, ∀Ci, Di ∈ N /{i}. Literally, each agent is
able to learn a generalised policy that is capable of handling sub-policies for different predecessor
coalitions, where each sub-policy for a predecessor coalition is with a disjoint parametric space.
Assumption 3. If an agent i ∈ N is dummy it will not give any contribution to any predecessor
coalition Ci ⊆ N /{i}, meanwhile, no members in the predecessor coalition Ci will react in different
manners after agent i joins.
15
Under review as a conference paper at ICLR 2022
B.2	Preliminary Theoretical Results
Proposition 4. ∀Ci ⊆ N and ∀s ∈ S, Eq.1 is satisfied if and only if maxπi Φi(s∣Ci) ≥ 0.
Proof. ∀Ci ⊆ N and ∀s ∈ S, given that Eq.1 is satisfied, with the fact that Ci ∩ {i} = 0 We can get
the equation such that
max vπCi ∪{i} (s) ≥ max vπCi (s) + max v πi (s).
πCi∪{i}	πCi	πi
(24)
Since maxπi vπi (s) ≥ 0 by the definition in Markov convex game, we can easily get the equation
such that
max v πCi ∪{i} (s) - max vπCi (s) ≥ 0.
πCi∪{i}	πCi
(25)
Therefore, we can get the equation such that
maxΦi(s∣Ci) ≥ 0.	(26)
πi
With the same conditions, the reverse direction of proof apparently holds by going through from
Eq.26 to 24.	□
Proposition 5. In Markov convex game with the grand coalition, coalitional marginal contribution
satisfies the property of efficiency: maxπ vπ (s) = ∑i∈N maxπi Φi(s∣Ci).
Proof. For any Ci ⊆ N /{i} and i ∈ N, according to Eq.3 we can get the equation such that
maxΦi(s∣Ci) = max vπCi∪{i} (s) - max vπCi (s),
πi	πCi∪{i}	πCi
(27)
where maxπC ∪{i} vπCi (s) = maxπC vπCi (s), since the decision of agent i will not affect the value
of Ci (i.e. the coalition excluding agent i). Given the definition that vπ0 (S) = 0 and the result from
Eq.27, by Assumption 1 we can get the equations such that
maxvπ(s)
π
=max vπ{jι} (s) - max vπ0 (s)
π{j1 }	π0
+ max vπ{j1} (s) - max vπ{j1} (s)
π{j1,j2}	π{j1}
+⋮
+ max vπ (s) - max VnN/{jn} (s)
π	πN /{jn}
= ∑ max Φi(s∣Ci).	(28)
i∈N πi
□
Theorem 3. Coalitional marginal contribution is a solution in the core of Markov convex game
(MCG) with the grand coalition.
Proof. The complete proof is as follows.
Firstly, if we would like to prove that coalitional marginal contribution is a value assignment scheme
in the core (with the grand coalition), we just need to prove that for any intermediate coalition C ⊆ N,
the following condition is satisfied such that
max Φ(s∣C) ≥ max vπC (s), ∀s ∈ S,	(29)
πC	πC
where maxπC Φ(s∣C) = ∑i∈C maxπi Φi(s∣Ci).
Suppose for the sake of contradiction that we have maxπC Φ(s∣C) < maxπC vπC (s) for some s ∈ S
and some coalition C = {j1,j2, ..., j∣C∣} ⊆ N, where jn ∈ C and n ∈ {1, 2, ..., ∣C∣}. We can assume
without the loss of generality that the coalition C is generated by the sequence j1,j2,…,j∣c∣}, i.e.,
16
Under review as a conference paper at ICLR 2022
the agents joins in C following the order j1,j2, ...,j∣C∣. Now, for each n ∈ {1, 2, ..., ∣C∣}, we have
{j1,j2, ...,jn-1} ⊆ {1, 2, ..., jn - 1}. Following Eq.1, we can write out the inequality as follows:
max vπC∪n (s) + max vπC∩n (s) ≥ max vπCmn (s) + max vπCkn (s),
πCn	πCn	πCn	πCn
C∪	C∩	m	Ck
Ckn = {1, 2,..., jn -1},
Cmn = {j1,j2, ...,jn},
C∩n =Cmn ∩Ckn = {j1,j2,...,jn-1},
C∪n =Cmn ∪Ckn = {1, 2,...,jn}.
Next, we rearrange Eq.30 and the following inequality is obtained such that
max vπC∪n (s) - max vπCkn (s) ≥ max vπCmn (s) - max vπC∩n (s),
πn	πn	πn	πn
C∪	Ck	Cm	C∩
(30)
(31)
Since we can express maxπC vπC (s) as follows:
max VnC (S) = max vπj1 (S) - max vπ° (S)
∏C	∏jι	∏0
+ m{ ax} vπ{j1,j2} (S) - max vπj1 (S)
+⋮
+ maxVnC(S) - max vnC/{jn} (s),
KC	πC∕{jn}
and due to Definition 1 we can obviously get the following equations such that
Φi(S∣Ci) = Φi(S∣Ckn) = max VπC∪n (S) - max VπCkn (S)
πCn	πCn
kk
U
max Φi (S∣Ci) = max Φi(S∣Ckn)
πi	πi
max VπC∪n (S) - max VπCkn (S),
πCn	πCn
∪k
by adding up these inequalities in Eq.31 for all C ∈ N, we can directly obtain a new inequality such
that
∑ max Φi(S∣Ci) = max Φ(S∣C) ≥ max VπC (S).	(32)
i∈C πi	πC	πC
It is obvious that Eq.32 contradicts the suppose, so we have showed that Eq.29 always holds for any
coalition C ⊆ N. For this reason, we can get the conclusion that coalitional marginal contribution is a
solution in the core of Markov convex game (MCG) with the grand coalition.	□
B.3	Mathematical Proofs for the Generalised Shapley Value
Proposition 1. The coalitional marginal contribution w.r.t. the action of each agent can be derived
as follows:
Φi(s,ai∣Ci) = maxQ∏F∪{i} (s, a1∪{i}) - maxQnCi (s, a。)	(33)
aCi	πCi	aCi
Proof. The complete proof is as follows.
We now rewrite maxπC VπCi∪{i} (S) as follows:
max VπCi∪{i} (S) = max ∑ πCi∪{i}(aCi∪{i}∣S) QπCi∪{i} (S, aCi∪{i})
πCi	πCi aCi∪{i}
(Since πCi∪{i} is a deterministic joint policy, we can have the following equation.)
= max max QπCi∪{i}(S,aCi∪{i})
aCi πCi	i
(WeWrite max QπCi∪{i} (s, a%∪{Q as Q∏"i} (s, aCi∪w∙))
πCi	πCi
=max Q：”i} (s, a%∪{i}).	(34)
aCi	πCi
17
Under review as a conference paper at ICLR 2022
Similarly, we rewrite maxπC vπCi (s) as follows:
max VnCi (S) = max max QnCi (s, a5) = max QπCi (s, a5) = max QnCi (s, ac).	(35)
πCi	aCi πCi	i	aCi	πCi	i	aCi	i
Since maxπC vπCi (s) is irrelevant to ai, by Eq.34 and 35 we can get that
Φi(s,ai∣Ci) = maxQ：?"" (s, aci∪{i}) - maxQnCi (s, a%)∙
aCi	πCi	i	aCi	i
By Eq.36, we can get the following result such that
Φ7(s,ai∣Ci) = max Φi(s,ai∣Ci)
πi
=max { max Q；?”" (s, aCi∪{i}) - max QnCi (s, a4)}
πi	aCi	πCi	i	aCi	i
= max max max QπCi∪{i} (s, aCi∪{i}) - max max QπCi (s, aCi)
πi	aCi πCi	i	aCi πCi	i
= max mCax mCax QπCi∪{i} (s, aCi∪{i}) - mCax mCax QπCi (s, aCi)
= max max QπCi∪{i} (s, aCi ∪{i}) - max max QπCi (s, aCi)
aCi πCi∪{i}	i	aCi πCi	i
=max QnCi∪{i} (s, a4∪{i}) - max QnCi (s, a4) ∙
aCi	aCi
(36)
(37)
The proof is completed.
□
Lemma 1. For any agent i ∈ N, ∀s ∈ S, its optimal generalised Shapley value denoted as
maxπi viφ (s) satisfies the following equation such that
φf、 C	∣Ci∣!(∣N∣ - ∣Ci∣ -1)!	而…^
max vφ (S) =	∑	--------ΓΓT∏-------maxφi(SICi),
：i	Ci⊆N∖{i}	∣N∣！	πi(Ci)
where πi(Ci) is the policy of agent i with respect to its predecessor coalition Ci.
Proof. By convexity of the maximum operator, we can easily derive the equation such that
max vφ(s) ≤ Ci⊆∑,} 一「∙ maxφi(SICi).	(38)
However, if we reasonably assume that the functional space of each agent’s policy is separable with
respect to its predecessor coalition Ci ⊆ N as Assumption 2 claims, we can write Eq.38 as follows:
φ(、 L	lCiM1N1TCi1 - 1)! ɪ / ∣z7 ʌ
max vφ(S) =	∑	-------Mn--------maxφi(SICi),
πi	Ci⊆N∖{i}	INI!	πi(Ci )
(39)
where ∏i(Ci) is a sub-policy of agent i with respect to its predecessor coalition Ci.	口
Remark 4. Usually, it is difficult to make decisions in parallel for all permutations to form the grand
coalition. Instead, it learns one πi (i.e. a policy projection) that can approximate (or take place of)
all πi(Ci) with respect to Ci ⊆N, minimising the distance that is defined as follows:
. I φ( ∖ V	lCiMlNlTCil - 1)! ɪ / ∣Z7 ʌ I
∏i = argmin ∣ maxVi(s) -	∑	--------r-7π-------maxΦi(Sgi) ∣.
πi	πi	Ci⊆N∖{i}	lNl!	πi
As Eq.38 shows, maxπi Viφ(S) is a lower bound of ∑Ci ⊆N ∖{i} ∣Ci∣!(Ν¾Ci∣τ)! ∙max∏i Φi(SlCi), so the
distance above perhaps never converges to 0 in practice, i.e., there exists an error bound. If the error
bound such that
max ∣
πi
maxViφ(S) -	∑
πi	Ci⊆N∖{i}
lCil!(lNl -lCil - 1)!
lNl!
∙ max Φi(SlCi)
πi
∣≤
is extremely small such that → 0, there should be almost no gap between the theoretical results
analysed in this paper and the practical results. Nevertheless, in theory we can directly use Eq.39 for
the convenience of analysis.
18
Under review as a conference paper at ICLR 2022
Proposition 2. In Markov convex game with the grand coalition, the generalised Shapley value
possesses several features as follows: (1) the sensitiveness to dummy agents: viφ(s) = 0; (2) efficiency:
maxπvπ(s) = ∑i∈N maxπi viφ(s).
Proof. The complete proof is as follows. We will firstly prove the (1), then (2). For any agent i ∈ N ,
∀s ∈ S, its generalised Shapley value denoted as viφ (s).
Proof of (1). Let us define Π(N ) as the set of all permutations of agents. For any permutation
m ∈ Π(N ) of agents to form the grand coalition, by the reasonable assumption in Assumption 3, for
any predecessor coalition Cim ⊆ N /{i} we have maxπCm vπCim (s) = maxπCm vπCim∪{i} (s), ∀s ∈ S,
ii
thereby Φi(s∣Cim) = 0. Also, the above analysis fulfills for all permutations of agents to form the
grand coalition. By the definition of the generalised Shapley value in Markov convex game shown in
Definition 2, it is not difficult to see that the generalised Shapley value for the dummy agent will be 0
such that viφ(s) = 0. The proof of (1) completes.
Proof of (2). The objective is proving that the generalised Shapley value satisfies the following
equation such that
maxvπ(s) = ∑ max viφ (s), ∀s ∈ S,	(40)
π	i∈N πi
where viφ(s) denotes the generalised Shapley value. By the result from Proposition 5 and Assumption
1, for an arbitrary sequence m ∈ Π(N) we can get the equation such that
maxvπ(s) = ∑ max Φi(s∣Cim), ∀s ∈ S,	(41)
where Φi(s∣Cim) is a coalitional marginal contribution and πi(Cim) (Cim is the predecessor coalition
that agent i meets in the sequence m) is a sub-policy of agent i ∈ N for the sequence m. If we
consider all possible sequences of agents to form the grand coalition and add all these inequalities,
we can get the following equation such that
∑	maxvπ(s) =∑∑max Φi(s∣Cim), ∀s ∈ S.
m∈Π(N) π	m∈Π(N) i∈N πi(Cim)
U
高 ∑	m∏aχvπ(S)=高 ∑	∑ max φi(SICm), ∀s ∈ S.	(42)
∣N∣!m∈Π(N) π	∣N∣! i∈N m∈Π(N) πi(Ci )
Next, to ease life we start from the left hand side of Eq.42. We can directly get the following equation
such that
∑r7 ∑ maxvπ(s) = -ɪ- ∙ ∖N∣! ∙maxvπ(s∣N) = maxvπ(s∣N).
∣N∣! m∈Π(N) π	∣N∣! π	π
(43)
Now, we start processing the right hand side of Eq.42. By rearranging it, we can get the equations
such that
焉 ∑ ∑	maχφi(s∣cm) = ∑ 焉 ∑	maχφi(s∣cm)
IN I! i∈N m∈Π(N) πi(Ci )	i∈N IN I! m∈Π(N) πi(Ci )
(The identical Cim in different sequences is written as Ci
and we can rearrange the equation as follows.)
=∑ 襦 ∑	∣Ci∣!(∣N∣ TCiI - 1)! ∙ maxΦi(s∣Ci)
i∈C IN ∣∙ Ci⊆N∖{i}	πi(ci)
∑	∑	创％?-1)! mx)Φi(s∣Ci).
i∈NCi⊆N∖{i}	∣N ∣!	πi(Ci)
(44)
19
Under review as a conference paper at ICLR 2022
By Lemma 1, we can get the following equations such that
∑	∑
i∈N Ci⊆N ∖{i}
∣Ci∣!(∣N∣ -∣Ci∣ - 1)!
∣N∣!
max Φi(s∣Ci)
πi(Ci )
=∑ max ∑
i∈N πi Ci⊆N∖{i}
岗!(INNF-1)!…i)
Inserting the results from Eq.43 and 46 to Eq.42, we can get the equation such that
maxvπ(s∣N) = ∑ maxvφ(s), VS ∈ S.
π	ieN Ki
Therefore, the proof for (2) completes.
(45)
(46)
(47)
□
=∑mX vφ (S)∙
Theorem 1. The generalised Shapley value is a solution in the e*-core of Markov convex game
(MCG) with the grand coalition and
e*=c£p(挪n,j(I-3> maX VnC (s)].
Proof. The complete proof is as follows.
Proof sketch. We need to construct the inequality similar to that in the proof of Theorem 3. Different
from the proof in Theorem 3, we should consider all permutations of sequences generated from
{jι, j2,…,j∣C∣} rather than an arbitrary sequence. For the result with an arbitrary sequence from the
proof of Theorem 3, we can directly apply it to the proof here.
Similar to the aim of proof in Theorem 3, wejust need to show that for any intermediate coalition
C ⊆ N the following condition should be satisfied such that
maxX(s∣C) ≥ max VnC (s), Vs ∈ S,	(48)
∏C	∏C
where max∏c X(s∣C) = ∑i€C max* vφ(s) and vφ(s) is denoted as the Shapley value of an agent
belonging to the coalition C. As Eq.5 shows, vφ(s) can be expressed as the equation as follows:
vφ(s) =	∑	∣ Ci ∣1(INN『∣-1)! ∙Φi(s∣α).
Ci⊆N∖{i}	∣N ∣∙
By Lemma 1, we can get the equation such that
ψf、 L	∣Ci∣!(INITCiI-1)!	ɪ / ∣z7 ʌ
max vφ(S) =	∑	---------ΓΓ7∏--------max φi (SICi),
πi	Ci⊆N ∖{i}	IN I!	πi(Ci)
(49)
(50)
where ∏i(Ci) is a sub-policy of agent i for the predecessor coalition Ci.
Suppose for the sake of contradiction that we have max∏c X(s∣C) < max∏c VFC (s) - E(C) for some
S ∈ S and some coalition C = {jι, j2,…,j∣c∣} ⊆ N, where jn ∈ C, n ∈ {1,2,…,∣C∣} and
E(C) = (1 -黑)∙ max VnC (s).
N ∣!	πc
First, we define P(N∖{i}) as the power set of N∖{i} and P(C∖{i}) as the power set of C∖{i}. For
the convenience of proof, we rewrite the Eq.50 to the form such that
maxvφ(s) =	∑	WCi ∙ max Φi(s∣Ci) +	∑	WCi ∙ max Φi(s∣Ci),
πi	Ci⊆c∖{i}	Ki(Ci)	Ci∈p(N ∖{i})∖p(c∖{i})	Ki(Ci)	(51)
`-------------V----------------Z `---------------------V---------------------Z
The part correlated to C.	The part uncorrelated to C.
where WCi is the fraction of occurrence frequency (e.g. ∣'i∣'(IN-1CiI-I)!) for their correlated coalitional
marginal contributions.
20
Under review as a conference paper at ICLR 2022
For each sequence m = j1,j2,…,j∣c∣} generated from C, thanks to the result from Theorem 3 We
have an equation such that
∑ max Φi(s∣Cim) ≥ max vπCm (s), ∀s ∈ S,	(52)
i∈C πi (Cim )	πCm
Where Φi(s∣Cim) denotes the coalitional marginal contribution of agent i belonging to the coalition C
for the predecessor coalition Cim of the agent in this sequence m and πCm = ×i∈Cπi(Cim) denotes the
joint policy of agents for sequence m.
If We add the inequalities of all sequences, We can get the equation as folloWs:
m∈∑Π(C)i∑∈Cπmi(Caimx)Φi(s∣Cim)≥
∑ max vπCm (s), ∀s ∈ S,
m∈Π(C) πCm
(53)
Where Π(C ) indicates the set of all permutations generated from the coalition C .
Dividing ∣N∣! on both sides of Eq.53, We can get the equation such that
1
同
∑∑
m∈Π(C) i∈C
m(Camx) Φi(s∣Cim) ≥
πi (Ci )
7-7:7 ∑ max VnCm (s), ∀s ∈ S.
∣N∣! m∑(C) ∏cm
(54)
To avoid confusion, We firstly process the left hand side of Eq.54 as folloWs:
W me∑3∑ ∏mcm) φi(s∣Cm)=i∑ W me∑(c) ∏mcm) φ"s∣Cm)
(The identical Cim in different sequences is Written as Ci
and We can rearrange the equation as folloWs.)
=∑ 焉 ∑ ∣Ci∣!(∣c∣ - ∣Ci∣ -1)! ∙ mαχΦi(s∣Ci)
i∈C ∣N∣! Ci⊆C∖{i}	πi(Ci)
=∑	∑	≡≡i≡ ∙mαχΦi(s∣Ci)
i∈C Ci⊆C∖{i}	∣N∣!	πi(Ci)
∣Ci∣!(∣C∣-∣Ci∣-1)!
(We use WCi here to represent each  ---——---------.)
∣N∣!
∑∑WCi ∙ max Φi(s∣Ci).
i∈CCi⊆C∖{i}	πi(Ci)
(55)
Next, We process the right hand side of Eq.54 as folloWs. By Assumption 1, We can get that
each vπCm is identical. By Assumption 2, We can use a generalised joint policy πC = ×i∈C πi
to include Um∈∏(c) ∏cm, since ∏ = ∣dj⊆N∖{i} ∏i(Ci) ⊇ Um∈∏(c) ∏i(Cm and Um∈∏(c) ∏cm =
Um∈∏(C) ×i∈c∏i(Cm) = ×i∈c Um∈∏(c) ∏i (Cm). Therefore, we can get the following results such that
∑- ∑ max VnCm (s) = —∑ max VnC (s)
∣N∣! m∑(C) ∏Cm ( ) ∣N∣！ m∈∑(C) ∏C ( )
=号∙ maXVnC (s).
(56)
Combining the results from Eq.55 and 56, we can get the equation as follows:
∑∑WCi ∙ max Φi(s∣Ci) ≥
i∈C Ci⊆C∖{i}	πi(Ci)
∣C∣!	∏C(、
阿∙ maX VC (s).
(57)
By the result from Proposition 4, it is trivial that maxπi(Ci) Φi (s∣Ci) ≥ 0 always holds. For this
reason,
∆=	∑	WCi ∙ max Φi(s∣Ci) ≥ 0
Ci∈P(N∖{i})∖P(C∖{i})	πi (Ci)
21
Under review as a conference paper at ICLR 2022
also always holds. Adding ∆ to the left hand side of Eq.57, by Assumption 2 we can get the equations
such that
∑Ci⊆∑{i} wCi ∙ nm(ax)φi(s∣Ci)"≥N ∙ max Vvn ⑸
U
∑ max vφ(S) = maxX(s∣C) ≥ -ʒ- ∙ max vπC (S)
i∈C πi i πC	∣N∣! πC
max VnC(S)-叫!-y! ∙ max vπC (s)
πC	∣N∣!	πC
max vπC (s) - (C).
πC
(58)
It is apparent that the result from Eq.58 contradicts the suppose and we show that the existence of the
-core, where
(C)=(1-∣∣
• max vπC (s).
πC
(59)
Next, to find the least core We continue to analyse the e(C) to find a e* that can make Eq.58 hold for
any coalition, thereby finding the e*-core.
Since (1 - 揣)• vv0 (s) = 0 and (1 - 辟)• maxv vπ (s) = 0 as well as the fact that vπC (s) ≥ 0 for
any C and S ∈ S, it is obvious that there exists a supremum between the coalition 0 and N.
For any C ⊆ N, we can get the following upper bound for e(C) such that
e(C) ≤ CeP(部N ,」(I-冬) • max VnC(s)}.
(60)
Henceforth, we can get that
inf {e ∣
maxX(s∣C) ≥ max vnC (s) 一 e, ∀s ∈ S, ∀C ⊆
nC
sup
C∈P(" )/{N ,0}
nC
N!) • max VnC(S)}.
(61)
Therefore, we prove that the generalised Shapley value is a solution in the e*-core, where
*
e* =	sup
C∈P(N )/{N ,0}
沫) ∙ max VnC(S)}.
(62)
□
B.4 Mathematical Proofs and Derivations for Shapley Q-Learning
Derivation of Shapley-Q Optimality Equation. First, according to Bellman’s principle of opti-
mality (Bellman, 1952; Sutton & Barto, 2018) we can write out Bellman optimality equation for the
global Q-value such that
Qn (s, a) = ∑ P(S'∣s, a)[R + Y max Qn(S′, a)].	(63)
′a
s′
For convenience, we only consider the finite state space and action space here. By the property of
efficiency (i.e. (2) in Proposition 2), we get that
maxQn(S’,a) = ∑ maxQφ (S',ai).	(64)
a	i∈N	ai
As per the theoretical results in Theorem 1, the grand coalition with the generalised Shapley (Q-)value
as the value assignment scheme always stays in the e*-core. For any subsequence of decisions
22
Under review as a conference paper at ICLR 2022
(subgame), almost no agents would have large incentives to deviate from the grand coalition or
change the value assignment scheme, since the current strategy (i.e., coalition structure and value
assignment scheme) can lead to the maximal individual value assignment and the maximal social
welfare. Standing by the side of solution concept, any subsequence of a sequence of decisions in the
-core is still in the -core. Since the solution in the -core can lead to the optimal social welfare (i.e.,
the said optimal global Q-value), this is consistent with the fact that the optimal global Q-value over
a time step should be built upon the optimal global Q-value over any subsequent time steps, as per
the principle of optimality (Bellman, 1952). Literally, we can simply think of that all agents at the
beginning of a sequence of decisions are rational and remain the grand coalition and the generalised
Shapley value as the value assignment until the end of game. The above discussion implies the
regularity of Eq.64.
By definition in Section 2, Qn (s, a) > 0 holds. Since Proposition 4 and Eq.37, We can get that
Qφ (s, a。≥ 0. By using a extremely small number (i.e. 10-7) to approximate Qφ (s, ai) = 0, We
can get that Qφ (s, aj ≥ 10-7. It is a fact that for all S ∈ S and ai ∈ Ai, there exists a wi(s, a%) > 0
such that	,
Qφ (s,ai) = wi(s,ai) Qn(s,a).	(65)
If we denote w(s, a) = [wi(s, ai)]τ and Qφ (s, a) = [Qφ (s, ai)]τ, given Eq.65 we can write that
Qφ,(s,a) = Iw(s,a) Qn(s,a),	(66)
where I is an identity matrix. Combined with Eq.64 and 66, we can rewrite Eq.63 to the equation as
follows:
Qφ (s, a) = Iw(s, a) ∑P(Sis, a)[R + Y ∑ max Qf (S'一)].	(67)
s′	i∈N ai
From Eq.65, we know that wi(S, ai) > 0. Therefore, we can rewrite Eq.65 to the following equation
such that	,
Wi(s,ai)-1 Qφ (s,ai) = Qn(s,a).	(68)
Ifwe sum up Eq.68 for all agents, we can obtain that
∑ Wi(s,ai)-1 Qφ (s,ai) = ∣N∣ Qf(s, a).	(69)
i∈N
Therefore, we can get the following equation such that
∑	-V ∙ Qφ* (s, ai) = QΠ(s, a).	(70)
i∈N ∣n∣ Wi(S,ai)
Substituting Eq.70 for Qn (s, a) in Eq.64, we can get the following equation such that
max Σ IMl 1——V ∙ QT (S,ai) = Σ max QT (S,ai)∙
a i∈N ∣N∣ wi(s,ai)	i	i∈N ai	i
Since a = ×i∈N ai , we can get that
Σ max∣“∣ 1----------； ∙ Qf (S,ai) = Σ max QT (S,ai)∙
i∈N ai ∣N∣ wi(s,ai)	i	i∈N ai	i
(71)
(72)
It is apparent that ∀s ∈ S and a『=argmaXai Qφ (s, a。we have the solution Wi(s, a：) = 1/\N∣.
Lemma 2 ( Dales et al. (2003) ). A set of real matrices M with a sub-multiplicative norm is a Banach
Algebra and a non-empty complete metric space where the metric is induced by the sub-multiplicative
norm. A sub-multiplicative norm \\ ∙ \\ is a norm satisfying thefoUowing inequality such that
∀A, B ∈ M :\\AB\\ ≤ \\A\\\\B\\.
Lemma 3. For a set of real matrices M, given an arbitrary matrix A = [aij] ∈ Rm×n, \\A\\1 =
max1≤j≤n ∑1≤i≤m \aij \ is a sub-multiplicative norm.
23
Under review as a conference paper at ICLR 2022
Proof. The complete proof is as follows.
First, we select two arbitrary matrices belonging to M, i.e, A =[。汴]∈ Rm×r and B = [bkj] ∈ Rr×n.
Then, we start proving that ∣∣ ∙ ∣∣ι is a sub-multiplicative norm as follows:
∣∣AB∣∣1
∣L∑≤* bkj ]∣∣1
=max ∑ ∣ ∑ aikbkj∣
≤j≤n ι≤i≤m 1≤k≤r
(By triangle inequality, we can obtain the following inequality.)
≤ ImaX ∑	∑ Iaikbkj ∣
≤j≤n 1≤i≤m 1≤k≤r
= ImaXl ∑	∑ ∣ɑikI Ibkj I
≤j≤n 1≤i≤m 1≤k≤r
=max ∑	∑ Iaiki lbkj i
1≤j≤n 1≤k≤r 1≤i≤m
= ImaXl Σ lbkjl ∑ Iaikl
≤j≤n 1≤k≤r	1≤i≤m
≤ llBIIImax Σ Iaikl
1≤k≤r 1≤i≤m
= I∣b∣∣1∣a∣∣i
=I I A∣ I 1 I I B∣∣1.
Therefore, we prove that given an arbitrary matrix A = [aj] ∈ Rm×n, ∣∣A∣∣1 = max1<7-≤n ∑ι≤i≤m IaijI
is a sub-multiplicative norm.	□
Lemma 4. For all S ∈ S and a ∈ A, Shapley-Q operator is a contraction mapping in a non-empty
complete metric space when maxs ( ∑i€N maxαi Wi(S, ai)} < ɪ.
Proof. The complete proof is as follows.
To ease life, we firstly define some variables that will be used for proof such that
Qφ = ×i∈NQφ ∈ R∣N∣×∣s∣∣a∣,
W ∈ r∣n∣×∣s∣∣a∣,
p ∈ r∣s∣∣a∣×∣s∣,
1 = [1,1,...,1]T,
where A = ×i∈NAi. Then, for an arbitrary matrix A ∈ Rm×n, we define the ∣∣ ∙ ∣∣ι for the induced
matrix norm such that
∣∣A∣∣ι = max
1≤j≤n
∑	Iaij ∣,
1≤i≤m
where aj is an arbitrary element in A. By Lemma 3, ∣∣ ∙ ∣∣ι defined here is a sub-multiplicative
norm. By Lemma 2, the set of real matrices RIN冈s∣∣a∣ with the norm ∣∣ ∙ ∣∣1 is a Banach algebra and a
non-empty complete metric space with the metric induced by ∣∣ ∙ ∣∣1.
To show that the operator Y is a contraction mapping in the supremum norm, we just need to
show that for any Qφ = ×i€N(Qf)1 ∈ RIN∣×∣s∣∣a∣ and Qg = ×i€N(Qf)2 ∈ RIN∣×∣s∣∣a∣, We have
24
Under review as a conference paper at ICLR 2022
∣∣ΥQ1φ - ΥQ2φ∣∣1 ≤ δ∣∣Q1φ - Q2φ∣∣1, where δ ∈ (0, 1).
∣∣ΥQ1φ -ΥQ2φ∣∣1
=max 1τ I w(s, a) ∑ P(SIs, a)[R(s, a) + Y ∑ max (Qφ)1(s', ai)]
s,a	s'∈S	i∈N ai
-I w(s, a) ∑ p(s'∣s, a)[R(s, a) + γ ∑ max (Qφ)2(s',ai)]
s'∈S	i∈N ai
=max 1τ I w(s, a) ∑ p(s'∣s, a)[R(s, a) + Y ∑ max(QΦ )ι(s',ai)
s,a	s'∈S	i∈N ai
— R(s,a) - Y ∑ max(Qφ)2(s',ai)]
i∈N ai	2
=Y max 1τ Iw(s, a) ∑ P(SIs, a)[ ∑ max (Qφ)ι(s',ai) — ∑ max (Qφ)2(s',ai)]
s,a	s'∈S	i∈N ai	i∈N ai
≤ γmax 1τ I w(s, a) max ∑ p(s'∣s, a)[ ∑ max (Qφ)](s',ai) — ∑ max(QΦ)2(s',ai)]
s，a	s，a s'∈S	i∈N ai	i∈N ai
(If we write δ = Ymax 1τ∣I w(S, a) ∣, we can have the following equation.)
s,a
=δmax ∑ P(SIs,a)[ ∑ max(Qφ). (s',ai) — ∑ max(Qφ)2(s',ai)]
s，a s'∈S	i∈N ai	i∈N ai
≤ δmax ∑ P(SIs,a) ∑ max(Qφ). (s',ai) - ∑ max(Qφ)2(s',ai)
s，a s'∈S	i∈N ai	i∈N ai
=δ ∑ max(Qφ)ι(s',ai) - ∑ max(Qφb(s',ai)
i∈N ai	i∈N ai
=δ ∑ [max(Qφ)ι(s',ai) - max(Qφ)2(s',ai)]
i∈N ai	ai
(By triangle inequality, we can obtain the following inequality.)
≤ δ ∑ max (Qφ)ι(s',ai) - max (Qφ)2(s',ai)
i∈N ai	ai
≤ δ ∑ max (Qf)I(S',ai)-(Qφb(s',ai)
i∈N ai	1	2
(Since a = ×i∈Nai, we have the following equation.)
=δmax ∑ (Qφ)ι(S',ai)-(Qφ)2(S',ai)
a i∈N	1	2
≤ δmax ∑ ∣(Qiφ)1(z,ai) - (Qiφ)2(z,ai)∣
z,a i∈N	1	2
= δ∣∣Q1φ - Q2φ∣∣1.
Now, we need to discuss the condition for δ ∈ (0, 1). Apparently, δ > 0, so we just need to discuss the
condition to guarantee that δ < 1. We now have the following discussions such that
δ = Y max 1τ∣I w(S, a) ∣ < 1 (Since wi(S, ai) > 0.)
s,a
⇒ Ymax ∑ wi(S, ai) < 1 (When Y ≠ 0, we can have the following inequality.)
s,a i∈N
⇒ max ∑ Wi(S, ai) < 一 (Since a = ×i∈Nai, We have the following equation.)
s,a i∈N	Y
⇒ max { ∑ max Wi(S, ai)} < ―.
s	i∈N ai	Y
25
Under review as a conference paper at ICLR 2022
Therefore, we show that Shapley-Q operator Υ is a contraction mapping in the non-empty
complete metric space generated by RIN∣×∣S∣∣A∣ with the metric induced by ∣∣ ∙ ∣∣ι, When
maxs { ∑i∈N maxai wi(s,ai)} < Y. Finally, it is apparent that wi(s,aj = 1∕∣N∣ when a% =
arg maxai Qφ(s, a。satisfies the above condition.	口
Remark 5. It is not difficult to extend the result from Theorem 4 to other measure spaces (e.g.,
Lebesgue measure space) for the infinite S and A instead of the counting measure for the finite S
and A that we used here. Since the proof sketch is similar, we ignore the proof here.
Corollary 1. According to Banach fixed-point theorem (Banach, 1922), Shapley-Q operator admits a
unique fixed point. Moreover, starting by an arbitrary start point, the sequence recursively generated
by Shapley-Q operator can finally converge to that fixed point.
Proof. Since (RIN冈s∣∣a∣, ∣∣ ∙ ∣∣ι} is a non-empty complete metric space and ShaPley-Q operator Y
is shown as a contraction mapping in Lemma 4, by Banach fixed-point theorem (Banach, 1922) we
can directly conclude that Shapley-Q operator Υ admits a unique fixed point. Furthermore, starting
by an arbitrary start point, the sequence recursively generated by Shapley-Q operator Υ can finally
converge to that fixed point.	□
Theorem 2. Shapley-Q operator can converge to the optimal Shapley Q-values and therefore the
optimal joint deterministic policy is achieved when maxs { ∑i∈N max0i Wi (s, aj} < ɪ.
Proof. By Corollary 1, we get that Shapley-Q operator admits a unique fixed point. Since Eq.7 is
obviously a fixed point for Shapley-Q operator, it is not difficult to conclude that the optimal Shapley
Q-value can be converged to and the optimal joint deterministic policy is achieved.	口
Stochastic Approximation of Shapley-Q Operator. We now derive the stochastic approximation
of Shapley-Q operator, i.e. a form of Q-learning derived from Shapley-Q operator. By sampling from
p(s'∣s, a), the Q-learning algorithm can be expressed as follows:
Qφ+ι(s, a) — Qφ(s, a) + αt(s, a)[Iw(s, a)(Rt + γ ∑ mαx(Qφ)t(s',αi)) - Qφ(s, a)].	(73)
i∈N ai
Lemma 5 (Jaakkola et al. (1994)). The random process {∆t} taking values Rn defined as
∆t+1(x) = (1 - αt(x))∆t(x) + αt(x)Ft(x)
converges to 0 w.p.1 under the following assumptions:
•	0 ≤ αt ≤ 1, ∑t αt(x) = ∞ and ∑t αt2 ≤ ∞;
•	∣∣E[Ft(x)∣Ft]∣∣W ≤ δ∣∣∆t∣∣W, with 0 ≤ δ < 1;
•	var[Ft(x)∣Ft] ≤ C(1 + ∣∣∆t∣∣2W), for C > 0.
Theorem 4. For a finite MCG, the Q-learning algorithm derived by Shapley-Q operator given by the
update rule such that
Qφ+ι(s,a) - Qφ(s,a) + αt(s,a)[lw(s,a)(Rt + Y ∑ max(Qφ)t(s',ai)) — Qφ(s,a)],
i∈N ai
converges w.p.1 to the optimal generalised Shapley Q-values if
∑αt(s,a) = ∞	∑αt2(s,a) ≤ ∞	(74)
tt
for all S ∈ S and a ∈ A as well as maxs { ∑i∈N max0i Wi (s,ai)} < γ.
Proof. The proof follows the sketch of proving the convergence of Q-learning given by Melo (2001).
First, we rewrite Eq.73 to
Qφ(s,a) = (1 — αt(s,a))Qφ(s,a) + αt(s,a)[lw(s,a)(Rt + Y ∑ max(Qφ)t(s',ai))].
i∈N ai
26
Under review as a conference paper at ICLR 2022
By subtracting Qφ (s, a) and letting
∆t(s, a) = Qφ(s, a) - Qφ* (s, a),
we can transform Eq.73 to
∆t+1 (s, a) = (1 - αt(s, a))∆t(s, a) + αt(s, a)Ft(s, a),
where	*
Ft(s, a) = Iw(s, a)(Rt + Y ∑ max(Qφ)t(s',ai)) - Qφ* (s, a).
i∈N ai
Since s′ ∈ S is a random sample from Markov Chain, so we can get that
E[Ft(s, a)∣Ft] = ∑ p(s'∣s, a)[lw(s, a)(Rt + Y ∑ max(Qφ)t(s', ai)) - Qφ* (s, a)]
s'∈S	i∈N ai
=Iw(s, a) ∑ p(s'∣s, a)[Rt + Y ∑ max(Qφ)t(s',ai)] - Qφ* (s, a)
s'∈S	i∈N ai
(Since max { ∑ max Wi(s, ai)} < ɪ.)
s	i∈N ai	Y
= ΥQtφ(s, a) - ΥQφ* (s, a).
By the results from Theorem 4, we can get that
∣∣E[Ft(s,a)∣Ft]∣∣1≤δ∣∣Qtφ(s,a)-Qφ*(s,a)∣∣1 = δ∣∣∆t(s, a)∣∣1,
where δ ∈ (0, 1).
Next, we get that
var[Ft(s, a)∣Ft] = E[(lw(s, a)(Rt + Y ∑ max(Qφ)t(s', ai)) - Qφ* (s, a)
i∈N ai
-ΥQtφ(s,a)+Qφ*(s,a))2]
=E[(lw(s, a)(Rt + γ ∑ max(Qφ)t(s',ai)) - ΥQφ(s, a))2]
i∈N ai
=var[(lw(s,a)(Rt + γ ∑ max(Qφ)t(s',ai))∣Ft].
i∈N ai
Since Rt and Iw(s, a) are bounded, it clearly verifies that
var[Ft(s,a)∣Ft] ≤ C(1 + ∣∣∆t(s,a)∣∣21)
for some constant C .
Finally, by Lemma 5 it is easy to see that ∆t converges to 0 w.p.1, i.e., Qtφ(s, a) converges to
Qφ (s, a) w.p.1, given the condition in Eq.74.	□
Derivation of Shapley Q-Learning. By stochastic approximation, i.e. sampling s' fromp(s'∣s, a),
Shapley-Q operator can be expressed as follows:
Qφ(s,a) = Iw(s, a)(R + Y ∑ maxQiφ(s, ai)),	(75)
i∈N ai
where I is an identity matrix; w(s,a) = [wi(s, ai)]τ ∈ RN∣; and Qφ(s, a) = [Qφ(s, ai)]τ ∈ RN∣.
Eq.75 can be equivalently represented as
(Iw(s, a))-1Qφ(s, a) = 1(R+ Y ∑ maxQiφ(s, ai)),	(76)
i∈N ai
where 1 is a vector of ones. Next, we multiply 1τ on both sides and obtain the following equation
∑ —71~~; ∙ Qφ(s,ai) = ∣N∣(R + Y ∑ maxQφ(s,ai)).	(77)
i∈Nwi(s,ai)	i	i∈N ai i
27
Under review as a conference paper at ICLR 2022
By dividing ∣N∣ on both sides, we can get that
Σ ∣.z-∣ 1-----； ∙ Qφ（s,ai） = R + Y Σ max Qφ（s,ai）.
i∈N ∣N∣wi（s,ai）	i	i∈N ai	i
Since Wi（s, a，） = 1∕∣N ∣ for a，i = arg maXai Qφ（s, aj by defining δ"s, a，） = ∣N∣ w1（s,ai）
that
δ （s a ） = ｛1	a，= argmaXai Qφ（s,a，），
i , i	Iai（S,a，） a，≠ argmaXai Qφ（s,a，），
Where a，（s, a，）= M /,ai）for a，≠ argmaXai Qφ（s, a，）.
（78）
we can get
（79）
By substituting Eq79 to Eq.78, we can get the following equation such that
∑ 6，（s,a，）Q?（s,a，）= R + Y ∑ maxQφ（s',a，）.	（80）
，eN	，eN ai
Therefore, We derive the TD error for Shapley Q-learning （SHAQ） such that
∆（s, a, s'） = R + γ ∑ max Q?（s',a，）- ∑ 6，（s,a，）Q?（s,a，）.
，eN ai	，eN
（81）
The TD error for SHAQ is necessary for the TD error for Eq.73 （i.e. the stochastic learning process
that We proved to converge to the optimal generalised Shapley Q-value in Theorem 4）. For this reason,
the condition maXs ｛ ∑i∈N maxoi w，（s, a，）｝ < Y needs to be satisfied so that the convergence to the
optimality is possible to hold.
B.5 Mathematical Derivation for Implementation of Shapley Q-Learning
Proposition 3. Suppose any coalitional marginal contribution can be factorised to the form such
that 中，（§«，|。，） = m（s, aci∪ri｝） Q，（s, a，）, with the condition such that
ECi-p（Ci∣N/｛，｝ [m（s, aCi∪｛i｝）] = ｛K ∈ （0 1）
we have
{Q， （s, a，） = Q，（s, a，）
Iai（S,a，）Q7（s,a，）= a，（s,a，） Q，（s, a，）
where a，（s,a，）=旧J~?（©火/{，}）［ a，（s,a，； a4）］.
a，= argmaXai Qφ（s,a，）,
a，≠ argmaXai Qφ（s,a，）,
a， = arg maXai Q，（s,a，）,
a， ≠ arg maXai Q，（s, a，），
>Λ	/` ɪɪ T	/`	Cl	A	1	√F,	/	I zɔ ∖	/	∖ ʌ /	∖	1
Proof. We suppose for any S ∈ S and a ∈ A, we have Φg（s, a，|C，） = m（s, aσ,∪｛i｝） Q，（s, a，） and
ECi [m（s, aci∪｛i｝）] = 1 when a，= argmaXai Qφ（s, a，）. By the definition of the generalised Shapley
Q-value, it is not difficult to obtain
Q?（s,a，）= Eci［①，^,。，。，）］
TO Γ /	∖ A /	、-1
= ECi[ m（s, aCi∪｛i｝） Q，（s,a，） ]
= ECi[ m（s, aCi∪｛i｝） ] Q，（s,a，）.
Recall that δ，（s, a，） is defined as follows:
…）={a，（s,a，）
a，= argmaXai Qφ（s,a，），
a，≠ argmaXai Qφ（s,a，）.
（82）
If a，= argmaXai Qφ（s, a，），it is not difficult to get that Qφ（s, a，） = Q，（s, a，）.
If a，≠ argmaXai Qφ（s, a，），we can have the following equation such that
a，（s,a，） Qφ（s,a，） = a，（s,a，） E4[ m（s, a4∪ri｝） Q，（s,a，）]
= ECi[ a，（s,a，） m（s,aCi∪｛i｝） ] Q，（s,a，）
=Eci[ a，（s,a，；a4） ] Q，（s,a，）.
Since under this situation Q，（s, a，） is always a scaled Qφ（s, a，） with the scale of 1/K, the decisions
are consistent.	□
28
Under review as a conference paper at ICLR 2022
Implementation of ai(s, ai).	As introduced in the main part of paper, when ai ≠
argmaxai Qi(s, a。，αi(s, a。is implemented as follows:
1M
a，-=而 ∑ Fs(QCk (TCk, a©% ), Qi(Ti,ai)) + 1,
k=1
where
1
QCk (τCk, aCk) = jCk; ∑ Qj (Tj,aj)
∣Ci ∣ j ∈Cik
and Ck Z P(Ci∣N/{i}) that follows the distribution w.r.t. the occurrence frequency of Ci; and
Fs(∙, ∙) is a monotonic function with an absolute activation function on the output whose weights are
generated from hypernetworks w.r.t. the global state, similar to the architecture of QMIX (Rashid
et al., 2018). Since FS(∙, ∙) ≥ 0 always holds, it is not difficult to obtain that ai(s, aj ≥ 1 always
holds. As Eq.11 shows, it is not difficult to get that ai(s, a。= K-1 αi(s, a，Since K ∈ (0,1), we
get that αi(s, ai) > 1.
As introduced in the main part of paper, the following equation is satisfied such that
1
δi(s,ai)
∣N∣ wi(s,ai).
τ-< . . . 11	. (-> . .. Λ .	入，	∖	/	∖ ,	1 ɛt .	. .... .I - .∙	. i 1. . i
For all s ∈ S and ai ≠ arg maxai Qi(s, ai), δi(s, ai) = αi(s, ai) > 1. So, we can derive that
Wi(S,ai) = NIai'ai)
1
1
1
⇒ maxwi(s, ai) = max
ai
ai
∣N∣αi(s,ai)	∣N∣ minai ai(s,a。< ∣N∣
⇒ 0 < ∑ maxwi(s, ai) < 1.
i∈N ai
二
1
For all s ∈ S and ai = arg maxai Qi(s, ai), δi(s, ai) = δi(s, ai) = 1. So, we can derive that
1
wi(s,ai) = ∣N∣
⇒ ∑ maxwi(s, ai) = 1.
i∈N ai
Therefore, we can directly obtain that for all s ∈ S and a ∈ A,
0 < max { ∑ maxwi(s, ai)} ≤ 1.
s	i∈N ai
Since Y ∈ (0,1), we can get that ɪ > 1. As a result, we show that for all S ∈ S and a ∈ A,
0 < max { ∑ max Wi(s, ai)} <
s i∈N ai	γ
We get that our implementation of ai(s, a。satisfies the condition in Theorem 2.
C Algorithm of S hapley Q-learning
In this section, we present the pseudo code of Shapley Q-learning in Algorithm 1. The general
paradigm can be divided into such parts: (1) collecting samples through -greedy strategy and store
the collected samples to a replay buffer for training; (2) sampling a batch of episodes of samples
from the replay buffer; (3) calculating Qi(Tt+1 ,alt+1; θ-), ai(sk, ak; λ) and Qi(Tlt, at; θ); and (4)
constructing a loss of Shapley Q-learning and updating parameters to minimise the loss.
Implementation of Sampling from p(Ci ∣N /{i}) (Line 4 in Algorithm 2). As introduced before,
the analytic form of P(Ci∣N/{i}) is ICi∣!(∣N-∣∣Ci∣-1)! that is actually the occurrence frequency of
29
Under review as a conference paper at ICLR 2022
Algorithm 1 Shapley Q-learning
1: 2: 3: 4: 5: 6: 7: 8: 9:	Initialise a set of agents N and set N = ∣N ∣ Initialise Qi (τi , ai ; θ) with the shared parameters among agents Initialise 0⅛(s,生；λ) with the shared parameters among agents Initialise Qi(τi, ai; θ-) by copying Qi(τi, ai; θ) with the shared parameters among agents Initialise a replay buffer B repeat Initialise a container E for storing an episode Observe an initial global state s1 and each agent’s partial observation oi1 from an environment for t=1:T do
10:	Get T = Sm)m=Lt for each agent
11:	For each agent i, select an action t a random action	with probability a [argmaxai Q以Tt,&;θ) otherwise
12:	Execute ait of each agent to get the global reward Rt, st+1 and each agent’s oit+1
13:	Store (st (ot)i=1:N, (at)i=LN, Rt, st+1, (ot+1)i=iN) to E
14:	end for
15:	Store E to B
16:	Sample a batch of episodes with batch size B from B
17:	for each sampled episode do
18:	for k=1:T do
19:	Get each transition (sk, (ok	N, (ak)i=i：N ,Rk, sk+1, (ok+1)i=iN)
20:	For each agent i, get Ti = (o7)m=Lk
21:	For each agent i, calculate Qi(Tk, ak; θ)
22:	For each agent i, calculate αi(sk, aik; λ) by Algorithm 2
23:	For each agent i, calculate δi(sk, aik; λ) as follows: ^ ( k k; λ) = {1	ak = argmaxai Qi(Sk,a； θ) i S , "i'	[<^i(sk,αk; λ) αk ≠ argmaxai Qi(Sk ,ai; θ) (viaAlgorithm2)
24:	For each agent i, get τk+1 =(以％=1：k+‹
25:	For each agent i, get ak+1 by argmaXai Qi(Tf+1, ai； θ)
26:	For each agent i, calculate Qi(τk+1, αk+1; θ-)
27:	end for
28:	end for
29:	Construct a loss as follows: 1B	2 min石 ∑ [(Rk + Y ∑ maxQH ,a ;θ-)- ∑ δi(sk,%;λ) QQ(W,akθ)) ] θ,λ B k=1	i∈N aik	i	i	i∈N	i	i i
30:	Update θ and λ through the above loss
31:	Periodically update θ- by copying θ
32:	until Qi(Ti,ai；θ) converges
Algorithm 2 Calculating ai(s, aQ
1: 2: 3: 4: 5: 6: 7: 8:	Input: s, (Qi(Ti, ai； θ))i=1:N, M OUtpUt: (αi(s,ai))i=1N for each agent i do Sample M preceding coalitions Ck Z P(CjN/{i}) for k=1:M do 1 Get QCk (TCk , aCk ) =商 ∑j∈Ck Qj (Tj, aj) end for 1M Get ai(s,ai) = M ∑Fs(QCk(TCk, aCk), QiS”)) + 1
9: end for
correlated coalition Ci . Since each coalition is formed by different permutations, it can be instead
sampled from permutations directly with uniform distribution where 焉 is the probability distribution
over each permutation. It is not difficult to find that these two sampling strategy induce the same
probability distribution for obtaining Ci , so they are equivalent. In practice, we sample multiple
permutations (saying M) from the uniform distribution in parallel. From each sampled permutation,
we extract the the relevant Ci for each agent i. Afterwards, M coalitions for agent i are obtained for
30
Under review as a conference paper at ICLR 2022
calculating the coalitional marginal contributions and therefore the approximate generalised Shapley
value is obtained.
D Experimental Setups
D. 1 Implementation Details of Shapley Q-learning
We now provide the additional implementation details that are omitted from the main part of paper.
First, Fs(∙, ∙) is a 3-layer network (consecutively with two affine transformation and an activation
of absolute), where the hidden-layer dimension is 32. The parameters of each affine transformation
are generated by hyper-networks (Ha et al., 2017) with input as the global state, whose details are
shown in Table 1. The architecture of each agent’s Q-value is a RNN with GRUs cell (Chung et al.,
2014), whose hidden-layer dimension is 64. The input dimension is state dimension and the output
dimension is action dimension.
Table 1: Table of specifications for Fs(∙, ∙).
Network	Structure
1ST WEIGHT MATRIX 1ST BIAS 2ND WEIGHT MATRIX 2ND BIAS	[LINEAR(STATE_DIM, 64), ReLU, LINEAR(64, 32*2), ABSOLUTE ] [LINEAR(STATE_DIM, 64)] [LINEAR(STATE_dim, 64), ReLU, linear(64, 32), ABSOLUTE ] [LINEAR(STATE_DIM, 32), ReLU, LINEAR(32, 1)]
Taking the lessons of training two coupling modules from GANs (Goodfellow et al., 2014), we take
separate learning rates for ai(s, aQ and Qi(s, ai). The learning rate for Qi(s, ai) is fixed at 0.0005
for all tasks. Nevertheless, the learning rate for ai(s, ai) is dependent on the number of controllable
agents. We use RMSProp optimizer for training in all tasks. All models are implemented in PyTorch
1.4.0 and each experiment is run on Nvidia GeForce RTX 2080Ti with periods from 4 to 26 hours.
D.2 Hyperparameters of Baselines
The hyperparameters of all baselines except for SQDDPG (Wang et al., 2020c) are consistent with
Rashid et al. (2020) and Wang et al. (2020b). The hyperparamers of SQDDPG are shown as follows:
(1) The policy network is consistent with the other baselines, while the critic network is with 3 hidden
layers and each layer is with 64 neurons. (2) The policy network is updated every 2 time steps, while
the critic network is updated each time step. (3) The multiplier of the entropy of policy is 0.005. The
rest of settings are identical with other baselines.
D.3 Predator-Prey for Modelling Relative Overgeneralisation
We give the experimental setups of Predator-Prey (Bohmer et al., 2020) in Table 2.
Table 2: Table of experimental setups of Predator-Prey.
Hyperparameters	Value	Description
BATCH SIZE	32	THE NUMBER OF EPISODES FOR EACH UPDATE
DISCOUNT FACTOR γ	0.99	THE IMPORTANCE OF FUTURE REWARDS
REPLAY BUFFER SIZE	5,000	THE MAXIMUM NUMBER OF EPISODES TO STORE IN MEMORY
EPISODE LENGTH	200	MAXIMUM TIME STEPS PER EPISODE
TEST EPISODE	16	THE NUMBER OF EPISODES FOR EVALUATING THE PERFORMANCE
TEST INTERVAL	1 0,000	THE TIME STEP FREQUENCY FOR EVALUATING THE PERFORMANCE
EPSILON START	1.0	THE START EPSILON VALUE FOR EXPLORATION
EPSILON FINISH	0.05	THE FINAL EPSILON VALUE FOR EXPLORATION
EXPLORATION STEP	1,000,000	THE NUMBER OF STEPS FOR LINEARLY ANNEALING
MAX TRAINING STEP	1,000,000	THE NUMBER OF TRAINING STEPS
TARGET UPDATE INTERVAL	200	THE UPDATE FREQUENCY FOR TARGET NETWORK
LEARNING RATE	0.0001	THE LEARNING RATE FOR δi (s, ai)
α FOR W-QMIX VARIANTS	0.1	THE WEIGHT FOR CW-QMIX AND OW-QMIX
SAMPLE SIZE	10	THE SAMPLE SIZE FOR COALITION SAMPLING
31
Under review as a conference paper at ICLR 2022
Table 3: Introduction of maps and characters in SMAC.
Map Name	Ally Units	Enemy Units	Categories
3S5Z	3 STALKERS & 5 zEALOTS	3 STALKERS & 5 zEALOTS	EASY
1C3S5Z	1 COLOSSI & 3 STALKERS & 5 zEALOTS	1 COLOSSI & 3 STALKERS & 5 zEALOTS	EASY
8M	8 MARINES	8 MARINES	EASY
10M_VS_11M	10 MARINES	11 MARINES	EASY
5m_vs_6m	5 MARINES	6 MARINES	HARD
3s_vs_5z	3 STALKERS	5 zEALOTS	HARD
2c_vs_64zg	2 COLOSSI	64 zERGLINGS	HARD
3s5z_vs_3s6z	3 STALKERS & 5 zEALOTS	3 STALKERS & 6 zEALOTS	SUPER-HARD
MMM2	1 MEDIvAC, 2 MARAUDERS & 7 MARINES	1 MEDIvAC, 3 MARAUDERS & 8 MARINES	SUPER-HARD
6h_vs_8z	6 HYDRALISKS	8 zERGLINGS	SUPER-HARD
CORRIDOR	6 zEALOTS	24 zERGLINGS	SUPER-HARD
D.4 S tarCraft Multi-Agent Challenge
The StarCraft Multi-Agent Challenge (SMAC) (Samvelyan et al., 2019) is a popular testbed for
multi-agent reinforcement learning (MARL) algorithms. The main difficulties are (1) challenging
dynamics, (2) partial observability and (3) high-dimensional observation space. During training, both
the global state of the environment and each agent’s local observation are able to be obtained; however,
during execution, only each agent’s local observation can be observed. For this reason, SMAC fits
the centralised training and decentralised execution (CTDE) paradigm. In each micromanagement
task, the ally units are controlled by agents and the enemy units are controlled by the built-in game
AI. The agents need to learn a strategy to solve some challenging combat scenarios and defeat their
opponents with maximum win rate.
In this paper, we evaluate the proposed SHAQ on 11 typical combat scenarios in SMAC that can be
classified into three categories: easy (8m, 3s5z, 1c3s5z and 10m_vs_11m), hard (5m_vs_6m, 3s_vs_5z
and 2c_vs_64zg), and super-hard (3s5z_vs_3s6z, Corridor, MMM2 and 6h_vs_8z). More details of
these tasks are provided in Table 3. The specific experimental setups for SMAC are shown in Table 4
and 5.
Table 4: Table of experimental setups for SMAC.
Hyperparameters	Easy	Hard	Super Hard	Description
BATCH SIzE	32	32	32	The number of episodes for each update
DISCOUNT FACTOR γ	0.99	0.99	0.99	The importance of future rewards
REPLAY BUFFER SIzE	5,000	5,000	5,000	The maximum number of episodes to store in memory
MAX TRAINING STEP	2,000,000	2,000,000	5,000,000	The number of training steps
TEST EPISODE	32	32	32	The number of episodes for evaluation
TEST INTERvAL	10,000	10,000	10,000	The time step frequency for evaluating the performance
EPSILON START	1.0	1.0	1.0	THE START EPSILON vALUE FOR EXPLORATION
EPSILON FINISH	0.05	0.05	0.05	THE FINAL EPSILON vALUE FOR EXPLORATION
EXPLORATION STEP	50,000	50,000	1,000,000	THE NUMBER OF STEPS FOR LINEARLY ANNEALING
TARGET UPDATE INTERvAL	200	200	200	The update frequency for target network
α FOR OW-QMIX	0.5	0.5	0.5	The weight for OW-QMIX
α FOR CW-QMIX	0.75	0.75	0.75	The weight for CW-QMIX
SAMPLE SIzE	10	10	10	The sample size for coalition sampling
E	Extra Experimental Results
E.1 Ablation Study for SHAQ
Sample Size M for Approximating α(s,ai). To study the impact of sample size M on the
performance of SHAQ, we conduct an ablation study as Figure 4a shows. We observe that the
small M is able to achieve fast convergence rate but with high variance, while the large M is with
low variance but comparatively slow convergence rate. The observations are consistent with the
conclusions from stochastic optimisation (Byrd et al., 2012; Hofmann et al., 2015). As a result, we
select M = 10 in practice, to trade off between convergence rate and variance.
An Empirical Law for Selecting the Learning Rate for c^i(s,ai). To provide an empirical law
on selecting the learning rate for ai(s,ai), We statistically fit a curve of the learning rate w.r.t. the
32
Under review as a conference paper at ICLR 2022
Table 5: The learning rate for training c^i (s, a。of SHAQ for various maps in SMAC.
MAP NAME	NUMBER OF AGENTS	Learning Rate for Ci(s, ai)
2c_vs_64zg	2	0.002
3s_vs_5z	3	0.001
5m_vs_6m	5	0.0005
6h_vs_8z	6	0.0005
Corridor	6	0.0005
8m	8	0.0003
3s5z	8	0.0003
3s5z_vs_3s6z	8	0.0003
1 c3s5z	9	0.0002
10m_vs_11m	10	0.0001
MMM2	10	0.0001
(a) Comparison among different
M on 5m_vs_6m. The [∙] indi-
cates the value of M.
—SHAQ ɪlɪ
——SHAQ [10]
—SHAQ [20]
< < <
£2 6=u-eβπ
s=MΠE,L uspaw
NumberofAgents
10
(b) Relationship between learning
rate for training c^i (s, ai) and the
number of controllable agents.
—A-ι.ι
---&1-1.2
---⅛-1.3
---哥-1.05
---4-1.07
⅛-l.09
--- at-leamtng
0.25 Oia 0.75 IM 1.25 Ijo 1.75 2.CO
T (mil)
(c) Comparison between manu-
ally preset and learning c^i (s, ai)
on 5m_vs_6m.
Figure 4: The figures of 3 ablation studies for SHAQ on SMAC.
number of controllable agents by the experimental results on SMAC that is shown in Figure 4b. It
is seen that the learning rate for α^i(s, a。is generally negatively related to the number of agents.
In other words, as the number of agents grows, the learning rate for ai(s, a。is recommended to
be smaller. For example, if the number of agents is more than 10, the learning rate for α^i(s, ai) is
recommended to be 0.0001 as the guidance from Figure 4b.
The Necessity of Learning Oi(s, a) Some readers may be concerned about the necessity of
learning &i(s,a，i). To answer this question, We study the necessity of learning ai(s,ai) on 5m_vs_6m.
Since the learned αi(s, a。finally converges to 1.1029, we grid search the fixed values of (^i(s, a%)
around this number. As Figure 4c shows, αi(s, a。with manually preset fixed value cannot work as
well as the learned &i(s, a) Therefore, we demonstrate the necessity of learning ai(s, a%) here.
E.2 Experimental Results on Extra SMAC Maps
To thoroughly compare the performance of SHAQ with baselines, we also run experiments on 5 extra
maps in SMAC as Figure 5 shows. 8m, 3s5z, 1c3s5z and 10m_vs_11m are an easy maps and corridor
is a super-hard map. The strategy of epsilon annealing is consistent with the previous experiments for
SMAC. It is obvious that SHAQ also performs generally well on these 5 maps.
E.3 EXTRA EXPERIMENTAL RESULTS ON W-QMIX WITH α = 0.1
To show the significance of tuning α for W-QMIX, we also run W-QMIX with α = 0.1 in addition to
the best α reported in Rashid et al. (2020). We can observe from Figure 6 that the performances of
W-QMIX are not comparatively identical for each choice of α. As a result, W-QMIX suffers from
the separate tuning of α for each scenario. Unfortunately, Rashid et al. (2020) did not provide an
empirical law for selecting a, while SHAQ possesses an empirical law to select αi(s, aj as Figure
4c shows.
33
Under review as a conference paper at ICLR 2022
IIX
IIX
*U-M-S8X Ue-Paw
—SHAQ
——QPLEX
——QMIX
--VDN
——CW-QMIX
——OW-QMlX
——QTRAN
∞MA
——MASAC
*UiM-səɪ Ue-Paw
(b) 1c3s5z.
∞ 0.25 OJO 0.75
5 UO 1.75 2.00
一SHAQ
--QPLEX
——QMIX
--VDN
——CW-QMK
——OW-QMK
--QTRAN
—8MA
——MASAC
—SHAQ
——QPLEX
——QMIX
--VDN
——CW-QMK
——OW-QMlX
——QTRAN
一COMA
——MASAC
025 0.50 0.75 1.00 1.25 1.50 1.75 2.00
T (mil)
025 030 0.75 1.00 1.25 1.50 1.75 2.00
T (mil)
*UiM-səɪ Ue-Paw
2	3
T (mil)
(c) 10m_vs_11m.
—SHAQ
——QPLEX
——QMIX
--VDN
——CW-QMIX
——OW-QMlX
——QTRAN
∞MA
—MASAC
(d) 8m.	(e) Corridor.
Figure 5:	Median test win % for 5 extra maps in SMAC.
——SHAq
——QFLDC
——QMK
---VDH
-CW-QMOCta.1]
——OW-QMtXta.1]
—CW-QMtX [a.75]
——OWCMDCtaJJ
---QUlAH
COMA
——MASAC
—SHAQ
——QPLBC
——QUIX
---VDH
—CW-QMOCta.1]
——OMM)MIXg.IJ
-CW-QMIX [a,75]
——0W4UtX [a.5J
——QTIUH
COMA
——MASAC
025 OSe 0.75 1.C0 125 Ise 1.75 200
T (mil)
025 0∙50 0.75
——SHAQ
——QPlfX
——QMK
---VDH
——CW4 MIX[a,l]
——OW-QUK [0.1]
---CW4MIX[a,75]
——OW-QUK ta.5]
——qruH
COMA
——MASAC
li5 1.50 1.75 2M
(a) 3s5z.	(b) 1c3s5z.	(c) 10m_vs_11m.
WUMvnSəi Ue-Paw
D.25 Og 0.7S
(d) 5m_vs_6m.
—SHAQ
--QFLDC
——QMIX
--VDH
-CW-QMOCta.1]
—OW-QMtX[α.l]
—CW-QMtX [β.75J
—OWCMDCtaJJ
--QUlAH
COMA
MASAC
WUMvnSəi Ue-Paw
025 OSe 0.75 1.C0 125 ISO 1.75 2AO
T (mil)
——SHAQ
——QPlfX
——QMIX
---VDH
——CW4 MIX[a,l]
——OW-QUK [0.1]
---CW4MIX[a,75]
——OW-QUK [0.5]
——QTRAH
COMA
——MASAC
025 OSe 0.75 140 125 1.5β 1.75 200
T (mil)
(e) 3s_vs_5z.	(f) 2c_vs_64zg.

Figure 6:	Median test win % for easy (1st row) and hard (2nd row) maps of SMAC for W-QMIX
with different α.
E.4 Comparison with SQDDPG
To emphasize the improvement of SHAQ from SQDDPG (Wang et al., 2020C), we exClusively
Compare these two algorithms on 3 maps. As Figure 7 shows, the performanCe of SHAQ surpasses
that of SQDDPG on all 3 maps, while SQDDPG Can only learn on the simplest map 3m. The most
possible reason for the failure of SQDDPG to CompliCated tasks is its sample Complexity ineffiCienCy
for permutations of agents as disCussed in SeCtion 5 that leads to the diffiCulty in learning. Apparently,
the implementation of Coalition invarianCe of SHAQ mitigates this weakness so that it is able to solve
more Challenging tasks.
(a) 3m.
*U-M-S8X u5pθw
(b) 3s5z.
Figure 7: Median test win % for 3 maps of SMAC to Compare SHAQ with SQDDPG.
*U-M-S8X u5pθw
2.∞
(C) 3s_vs_5z.
34
Under review as a conference paper at ICLR 2022
(c) QMIX: -greedy.
(g) QMIX: greedy.
(d) QPLEX: -greedy.
(h) QPLEX: greedy.
Figure 8:	Visualisation of the evaluation for SHAQ and baselines on 3s5z_vs_3s6z in SMAC: each
colored circle is the centered attacking range of a controllable agent (in red), and each agent’s
factorised Q-value is reported on the right. We mark the direction that each moving agent face by an
arrow.
E.5 More Visualisation
To verify our theoretical results more firmly, we show the Q-values on a more complicated scenario
in SMAC, i.e. 3s5z_vs_3s6z during test in Figure 8. First, We take a look into the optimal decisions
(from greedy decision). SHAQ can still demonstrate the equal credit assignment as we claimed before.
Unfortunately, VDN does not explicitly shoW equal credit assignment. The possible reason is that part
of parameters of Q-value are shared betWeen optimal decisions and sub-optimal decisions. Therefore,
the parametric effects of the mistakes conducted on sub-optimal decisions to the optimal decisions by
VDN during learning may be exaggerated When the number of agents increases. About QMIX and
QPLEX, the Q-values of optimal decisions are difficult to be interpreted in this complicated scenario.
For both strategies, the agent Who is responsible for kiting 4 (i.e. Agent 3 for QMIX and Agent 2 for
QPLEX) receives the loWest credit, hoWever, it is an important role to the team in a combat tactic.
Next, We focus on the demonstration of the mixture of optimal and sub-optimal decisions (from
-greedy decision). As for SHAQ, Agent 1 and Agent 3 are participating into the battle, so deserving
almost the equal credit assignment. HoWever, Agent 2 drops teammates and escapes from the center
of battle, so it contributes almost nothing to the team. As a result, it can be regarded as a dummy
agent and therefore obtains the credit near 0. This is consistent again With our theoretical analysis.
About VDN, it coincidentally receives near 0 for the dummy agent (i.e. Agent 3) in this scenario.
Nevertheless, the loW credit assignments to the other 2 agents Who participate in the battle is difficult
to be interpreted. About QMIX, the agents Who participate in the battle (i.e. Agent 2 and Agent 3)
receive the loWest credits, While the agent (i.e. Agent 1) Who escapes from the battle receives the
highest credit. For QPLEX, the agents’ behaviours are difficult to be interpreted.
E.6 Extra Experimental Results for Predator-Prey
In Figure 9, We shoW the results for W-QMIX With p=-0.5,-1,-2 and the annealing steps as 50k to
support our claims in the experimental analysis that the poor performance of W-QMIX on Predator-
Prey is due to its poor robustness to the increased explorations for this environment. We also shoW
the performance of SQDDPG With p=-2 and the epsilon annealing steps as 1 mil.
F	Extra Related Works
Shapley Value for Machine Learning. Shapley value has been broadly applied in machine learning
research community. Lundberg & Lee (2017), Ancona et al. (2019) and Kumar et al. (2020) applied
Shapley value as a measure of feature importance for statistical models or deep neural netWorks. Jia
et al. (2019) valued annotated data by approximating their contributions to the model. These methods
above are in static scenarios that just directly used the original Shapley value theory for application.
HoWever, our Work extends the Shapley value theory to Markov dynamics With action space. The
4https://en.wikipedia.org/wiki/Glossary_of_video_game_terms.
35
Under review as a conference paper at ICLR 2022
Oooo
2 4 6
- - -
u,lnl upw
——SQDDPG_PREDATOR_PREY
CWQMIX_PREDATOR_PREY_P=-0.5
——CWQMIX_PREDATOR_PREY_P=-1
——CWQMIX_PREDATOR_PREY_P=-2
OWQMIX_PREDATOR_PREY_P=-0.5
——OWQMIX_PREDATOR_PREY_P=-1
OWQMIX_PREDATOR_PREY_P=-2
.0
0.2
0.4	0.6	0.8
T (mil)
ι.o
Figure 9:	Median test return for Predator-Prey evaluated with SQDDPG and W-QMIX (inlcuding
CW-QMIX and OW-QMIX). For SQDDPG, p=-2 and the epsilon annealing steps are 1 mil. For
W-QMIX, we evaluate the performances on p=-0.5,-1,-2 and the epsilon annealing steps are 50k.
theoretical framework of Shapley value for MCG with the grand coalition proposed in this paper is
easy to be applied for designing MARL algorithms with interpretability.
Learning Paradigm for MARL. Considering the algorithmic frameworks among MARL algorithms
with CTDE, it can be classified to two categories such as multi-agent Q-learning algorithms (Sunehag
et al., 2018; Rashid et al., 2018; Son et al., 2019; Wang et al., 2020b; 2021) and multi-agent actor-critic
algorithms (Lowe et al., 2017; Foerster et al., 2018; Iqbal & Sha, 2019; Wang et al., 2020c; Mahajan
et al., 2021). In this paper, we propose Shapley Q-learning (SHAQ) that belongs to the category of
multi-agent Q-learning algorithms.
36