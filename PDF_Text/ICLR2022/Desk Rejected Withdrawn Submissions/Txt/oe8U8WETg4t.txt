Under review as a conference paper at ICLR 2022
Linear Backpropagation Leads to Faster Con-
VERGENCE
Anonymous authors
Paper under double-blind review
Ab stract
Backpropagation is widely used for calculating gradients in deep neural networks
(DNNs). Applied often along with stochastic gradient descent (SGD) or its variants,
backpropagation is considered as a de-facto choice in a variety of machine learning
tasks including DNN training and adversarial attack/defense. Nevertheless, unlike
SGD which has been intensively studied over the past years, backpropagation is
somehow overlooked. In this paper, we study the very recent method called “linear
backpropagation” (LinBP), which modifies the standard backpropagation and can
improve the transferability in black-box adversarial attack. By providing theoretical
analyses on LinBP in neural-network-involved learning tasks including white-
box adversarial attack and model training, we will demonstrate that, somewhat
surprisingly, LinBP can lead to faster convergence in these tasks. We will also
confirm our theoretical results with extensive experiments.
1 Introduction
Over the past decade, the surge of research on deep neural networks (DNNs) has been witnessed.
Powered with large-scale training, DNN-based models have achieved state-of-art performance in a
variety of applications. Tremendous amount of research has been done to improve the architecture (He
et al., 2016; Simonyan & Zisserman, 2015; Huang et al., 2017; Vaswani et al., 2017; Dosovitskiy
et al., 2021) and the optimization method (Kingma & Ba, 2014; Loshchilov & Hutter, 2019; 2017;
Reddi et al., 2019; Sutskever et al., 2013) for DNNs. The optimization involving DNNs usually
utilizes stochastic gradient descent (SGD) (Bottou, 2010) that minimizes some learning loss and
uses backpropagation (BP) (LeCun, 1988) for computing gradients. Unlike SGD which has been
thoroughly studied and innovated (cf. Adam, Adagrad, SGDW, etc), BP seems overlooked in deep
learning over the past decade and normally considered as a de-factor choice in DNN-involved
optimizations.
Very recently, Guo et al. (2020) introduced a slightly different way of computing gradients called
linear BP (LinBP) in which the forward pass of a DNN was left unchanged, while the partial
derivative regarding some of the rectified linear unit (ReLU) activation function was skipped in
the backward pass. It was shown empirically that LinBP leads to improved results in generating
transferable adversarial examples for performing black-box adversarial attacks (Papernot et al.,
2017), in comparison to using the original BP. The results were enlightening from our perspective,
since the superior performance was surprisingly obtained by performing less precise computation of
gradient. The superiority of LinBP was originally conjectured as less overfitting and less gradient
obfuscation (Athalye et al., 2018). Yet, in this paper, we would like to study the optimization
convergence using LinBP, to shed more light on the method. We target two practical applications that
care about convergence, i.e., white-box adversarial attack and model parameter training.
In the remainder of this paper, we shall first revisit some preliminary knowledge regarding model
training, white-box adversarial attack, and LinBP. We will then introduce a teacher-student frame-
work (Tian, 2017) which uses ReLU activation functions and squared l2 loss. Under the framework,
we give rigorous theoretical convergence analyses on LinBP. Our theoretical results show that, in
white-box adversarial attack scenarios, using LinBP can produce more deceptive adversarial ex-
amples, in comparison to using the standard BP in the same hyperparameter settings. Similarly,
we will show theoretically that LinBP can also help to converge faster than BP in model training.
Simulation experiments confirm our theoretical results, and extensive experimental results also verify
1
Under review as a conference paper at ICLR 2022
our findings in more general and practical settings using a variety of different DNNs, including
VGG-16 (Simonyan & Zisserman, 2015), ResNet-50 (He et al., 2016), DenseNet-161 (Huang et al.,
2017), and MobileNetV2 (Sandler et al., 2018).
2 Background and Preliminary Knowledge
2.1	Model Training
Together with SGD or its variant, BP has long been adopted as a default method for computing
gradients in training machine learning models. Consider a typical update rule of SGD, we have
Wt+1 = Wt- Nwt X L(x(fc),y(fc)),	(1)
k
where {x(k),y(k)} is a couple of the k-th training instance and its label, Nwt Pk Lyylk)) is
the partial gradient of the loss function with respect to a learnable weight vector W, and η ∈ R+
represents the learning rate.
2.2	White-box Adversarial Attack
Another popular optimization problem in deep learning is to generate adversarial examples, which is
of particular interest in both the machine learning community and the security community. Instead of
minimizing the prediction loss as in the objective of model training, adversarial attack aims to craft
examples that lead to arbitrary incorrect model predictions (Goodfellow et al., 2015). The adversarial
examples are expected to be perceptually indistinguishable to benign ones. In the white-box setting,
where it is assumed that the architecture and parameters of the victim model are both known to the
adversary, we have the typical learning objectives:
max L(x + r, y) and min krkp, s.t., arg max probj (x + r) 6= arg max probj (x) (2)
where r is a perturbation vector whose lp norm is constrained to guarantee that the generated example
X + r is perceptually similar to x, function PrObj(∙) provides the prediction probability for the j-th
class. For solving the problems in Eq. (2), a series of methods have been proposed. For instance,
with p = ∞, i.e., for l∞ attack, Goodfellow et al. (2015) proposed the fast gradient sign method
(FGSM) which simply calculates the sign of the input gradient, i.e., sign(NxL(x)), and adopted
e ∙ sign(VχL(x)) as the perturbation. To enhance the power of the attack, follow-up work proposed
iterative FGSM (I-FGSM) (Kurakin et al., 2017) and PGD (Madry et al., 2018) that took multiple
steps of update and computed input gradients using BP in each of the steps. The iterative process of
these methods is similar to that of model training, except that the update is performed in the input
space instead of the parameter space and normally some constrains are required for attacks. Other
famous attacks include DeepFool (Moosavi-Dezfooli et al., 2016) and C&W’s attack (Carlini &
Wagner, 2017), just to name a few.
2.3	Linear Backpropagation
For a d-layer neural network model, the forward pass include the computation of
fd(x) = WT σ(Wd-1 ∙∙∙ σ(W1 X)),
(3)
where σ(∙) is the activation function and commonly set as ReLU function, Wι, ∙∙∙ , Wd are learnable
weight matrices in the model. In order to generate a white-box adversarial example on the basis
of x with the obtained fd(x), we shall further compute the prediction loss L(x, y) of x and then
backpropagate the loss to compute the gradient VxL(x, y), the backward propagation for x is
formulated as
Vx L(x, y)
M1
• ∙ Md-IWd
∂L(x, y)
∂fd(x),
(4)
where Mi = Wi∂σ(fi(x))∕∂fi(x). LinBP (Guo et al., 2020) keeps the computation in the
forward pass unchanged while removing the influence of the ReLU activation function (i.e.,
∂σ(fi (x))∕∂fi (x)) in the backward pass. That being said, LinBP computes:
V χL(x, y)
Wl …WlI dL(x，y)
WI Wd ∂fd(x).
(5)
2
Under review as a conference paper at ICLR 2022
Since the partial derivative of the activation function has been removed, it is considered linear in the
backward pass and thus called LinBP. In practice, LinBP may only remove some of the nonlinear
derivatives, e.g., Guo et al. (2020) only modified those starting from the (m + 1)-th layer and used
the following formulation:
V χL(χ, y)
MI …MmWm+1 ,
d L(X, y)
∙∙Wd fxτ,
(6)
The method was shown to be effective in improving the transferability of the generated adversarial
examples on some source model (Guo et al., 2020).
Although LinBP was only proposed for obtaining adversarial examples, we may also adopt it to
compute the gradient with respect to model weights for training. Note that the gradient of loss with
respect to Wi in the standard BP is formulated as
Vw.L(x, y) = σ(fi-ι(x)) ((X)) Mi+ι …Md-IWddLX 2y ) ,	(7)
Wi x , y ui 1' 〃卜 ∂fi(x)	i+1 d 1 d ∂fd(x) J ,
where we somewhat abuse the notations and define σ(f0(x)) = x for simplify. Similar to Eq. (5), a
linearized version of the gradient is
V wiL(x, y)= σ(fi-ι(x))(W，+1 …Wd	：) )：	(8)
∂fd(x)
3 Theoretical Analyses
In this section, we provide convergence analyses of LinBP, and compare it to the standard BP. There
have been attempts for studying the training dynamics of neural networks (Du et al., 2019a; Arora
et al., 2019; Tian, 2017; Du et al., 2019b). We mainly follow Tian (2017)’s work and consider the
teacher-student framework with ReLU activation function and squared l2 loss. We shall start from
the theoretical analyses for white-box adversarial attack and then consider model training.
3.1	Theoretical Analyses for Adversarial Attack
Compare to the two-layer teacher-student frameworks in Tian (2017)’s, we here consider a more
general model, which can be formulated as
g(W, V, x) = Vσ(Wx),	(9)
where X ∈ Rd1 is the input data vector, W ∈ Rd2×d1 and V ∈ Rd3 ×d2 are weight matrices, σ(∙) is
the ReLU function, i.e., σ(∙) = max(∙, 0). In the teacher-student frameworks, we assume the teacher
networks possesses the optimal adversarial example x? , and the student network learn the adversarial
example x from the teacher network, in which the loss function is set as the squared l2 loss between
the output of student and teacher networks, i.e.,
L(X) = 2kg(W, V, x) - g(W, V, x?)k2.	(10)
We further define D(W, x) := diag(Wx > 0). From Eq. (9) and Eq. (10), we can easily obtain the
analytic expression of the gradient with respect to x for standard BP:
VxL(x) = WT D(W, x)VT V(D(W, x)Wx - D(W, x?)Wx?).	(11)
Compared with Eq. (11), the gradient obtained from LinBP removes the derivatives of ReLU function
in the backward pass, i.e.,
VxL(x) = WTVTV(D(W, x)Wx - D(W, x?)Wx?).	(12)
Inspired by Theorem 1 in Tian (2017)’s work, we introduce the following lemma to give the analytic
expression of the gradients in expectations in adversarial settings.
Lemma 1 Denote G(e, x) := WT D(W, e)VT VD(W, x)Wx, where e ∈ Rd1 is a unit vector,
x ∈ Rd1 is the input data vector, W ∈ Rd2 ×d1 and V ∈ Rd3×d2 are weight matrices. If W and V
follow independent standard Gaussian distribution, we have
E (G(e, x)) = d2 [(∏ — Θ)x + 冈 sinΘe],
2π
where Θ ∈ [0, π] is the angle between e and x.
3
Under review as a conference paper at ICLR 2022
The proof can be found in the appendix. With Lemma 1, the expectation of Eq. (11) and Eq. (12) can
be formulated as
E[VχL(x)] = G(X∕∣∣χ∣∣,X)-G(X∕kχk,χ?) = d2(X-χ?) + d2 fθx? - k⅛ sinθχ) , (13)
2	2π	kxk
and
E[VχL(x)] = G(x∕kxk, x) - G(x*∕kx?k, x?) = d2(x - x*),	(14)
respectively, where Θ ∈ [0, π] is the angle between X and X? . We follow prior works (Goodfellow
et al., 2015; Kurakin et al., 2017; Madry et al., 2018) and focus on l∞ attacks. Different iterative
gradient-based l∞ attack methods may have slightly different update rules, here, for ease of unified
analyses, we simplify their update rules as
x(t+1) = Clip(x(t) - ηVx(t)L(x(t))),	(15)
where Clip(∙) = min(x + el, max(x 一 el, ∙)) performs element-wise input clip to guarantee that
the intermediate results always stay in the range fulfilling the constraint of kx(t+1) - xk∞ ≤ . We
use the updates of standard BP and LinBP, i.e., VxL(X) and VχL(x), to obtain {x(t)} and {x(t)},
respectively. Under all the settings above, we propose the following theorem to give convergence
analysis on LinBP.
Theorem 1 For the two-layer teacher-student network formulated as Eq. (9), the adversarial attack
sets Eq. (10) and Eq. (15) as the loss function and the update rule, respectively. Assume that W and
V follow independent standard Gaussian distribution, x?〜 N (μι, σ2), x(0)〜 N (0, σ2), and η is
reasonably small1. Let x(t) and X(t) be the adversarial examples generated in the t-th iteration of
attack using BP and LinBP, respectively, then we have
Ekx? - X⑴kι ≤ Ekx? - X⑴kι.
The proof is shown in the appendix. Theorem 1 shows that LinBP can produce adversarial examples
closer to the optimal adversarial examples x? compared to standard BP in the same settings for any
finite number of iteration steps, which means that LinBP can craft more powerful and destructive
adversarial examples in the white-box attack. It is also straightforward to further derive from our
proof that for a benign example with low prediction loss, LinBP produces a more powerful adversarial
example starting from the benign one. The conclusion should be surprising because the popular
white-box attack methods like FGSM (I-FGSM) and PGD mostly use the gradient obtained by the
standard BP and yet we find LinBP may lead to stronger attacks. From our proof, it can be easily
derived that the norm of Eq. (14) is bigger than the norm of Eq. (13), which means LinBP provide
larger update. Also, the direction of the update obtained from LinBP is closer to the residual x - x?
compared with the standard BP. These may cause LinBP to produce more destructive adversarial
examples and powerful attack even in white-box settings. We have conducted extensive experiments
on deeper networks using common attack methods to verify our conclusions, which are shown in
Section 4.2.
3.2	Theoretical Analyses for Model Training
For model training, we adopt the same idea in adversarial attack to analyze the performance of LinBP.
Yet we mainly consider the shallow one-layer teacher-student framework since the student network
may not converge to the teacher network in two-layer or deeper models. We will show in Section 4.2
that many of our results hold in more complex network architectures. The one-layer network can be
formulated as
h(x, w) = σ(xTw),	(16)
where X ∈ Rd is the input vector, W ∈ Rd is the weight vector, and σ(∙) is the ReLU function. Given
a set of training samples, we obtain h(X, w) = σ(Xw), where X = [x1T; ...; xTN] is the input data
matrix, xk ∈ Rd is the k-th training samples, for k = 1, ..., N. We assume the teacher network have
the optimal weight, and the loss function can be formulated as
L(W) = Il∣h(X, w) - h(X, w?)k2,	(17)
1See Eq. (21) for more details of the constraint.
4
Under review as a conference paper at ICLR 2022
where w and w? are the weight vector for the student network and teacher network, respectively.
Therefore, we can derive the partial gradient with respect to w for standard BP:
VwL(W) = XTD(X, W)(D(X, W)XW - D(X, w*)Xw*).	(18)
Also, the partial gradient to w for LinBP is formulated as
V WL(W) = XT (D(X, w)Xw — D(X, w*)Xw*).	(19)
While training the simple one-layer network with SGD, the update rule can be formulated as
W(t+1) = W(t) - ηVw(t)L(W(t)),	(20)
where We use VWL(W) and VWL(W) shown in Eq. (18) and Eq. (19) to obtain {W(t)} and {W(t)}
for standard BP and LinBP, respectively. Similar to Theorem 1, we have the following theorem to
analyze the training effect of LinBP.
Theorem 2 For the one-layer teacher-student network formulated as Eq. (16), the training task
sets Eq. (17) and Eq. (20) as the loss function and the update rule, respectively. Assume that X
is generated from standard Gaussian distribution, w?〜N (μι,σ2), W(O)〜N (0, σ2, and η is
reasonably Smaltl. Let w(t) and W(t) be the weight vectors obtained in the t-th iteration oftraining
using standard BP and LinBP respectively. Then we have
E∣∣w? — W(t)kι ≤ E∣∣w? — Wmkι.
The proof is also deferred in the appendix. Theorem 2 shows that LinBP may let the weight vector
get closer toward the optimal target weight vector W? relative to standard BP on the same learning
rate and training iterations, which means LinBP can also lead to faster convergence in model training
when the assumptions are satisfied.
Discussions about η and t. In Theorem 1 and Theorem 2, we assume that the update step size in
adversarial attacks and the learning rate in model training are reasonably small. Here we would like
to talk more about such assumptions. Precisely, we mean that η should be small enough to satisfy the
following constraints: for Theorem 1, it is required that
।X ηd2a - ηd2)mτ-Pji∣ < ∣(i - ηd2『X? - χ(0))∣,	(21)
j =0 2π	2	2
for m = 1, . . . , t and i = 1, . . . ,d1 indicating the i-th entry of a d1-dimensional vector, where
Pj = θjx? 一 kXj)k Sin θjx(j). And for Theorem 2,
(1 - ηN)m-1-jqjiI < 1(1 - ηN)m(w? - W(0))l,	(22)
j=0
for m = 1,...,t and i = 1,...,d, where qj = θjw? 一 口嵋/：sin θjw(j). It is nontrivial to obtain
analytic solutions for Eq. (21) and Eq. (22). However, we can know that they are both more likely to
hold when t is small. Since the maximum number of learning iterations for generating adversarial
examples is often set to be small (at most several hundred) in methods like I-FGSM and PGD, the
assumption is easier to be fulfilled than in the setting of model training (which can take tens of
thousands of iterations), indicating that the superiority of LinBP can be more obvious in performing
adversarial attacks. We will give empirical discussions in Section 4.
4	Experiments
In this section, we provide experimental results on synthetic data (see Section 4.1) and real data (see
Section 4.2) to confirm our theorem and compare LinBP against BP in more practical settings for
white-box adversarial attack/defense and model training, respectively. The practical experiments show
that our theoretical results are hold in variety of different model architectures. All the experiments
were performed on NVIDIA GeForce RTX 1080 Ti and the code was implemented on PyTorch (Paszke
et al., 2019). Our code is included in our supplementary materials.
2See Eq. (22) for a precious formulation of the constraint.
5
Under review as a conference paper at ICLR 2022
4.1	Simulation Experiments
Adversarial attack. We constructed a tWo-layer neural netWork and performed adversarial attack
folloWing the teacher-student frameWork described in Section 3.1. To be more specific, the victim
model is formulated in Eq. (9) and the loss function is given in Eq. (10). Eq. (15) is the update rule
to generate adversarial examples. FolloWing the assumption in Theorem 1, the Weight matrices V
and W Were generated from independent standard Gaussian distributions. Similarly, the optimal
adversarial example x? and the initial adversarial example x(0) Were obtained via sampling from
Gaussian distributions, i.e., x?〜N(μ1,σ2) and X(O)〜N(0, σ2), where μ1, σ1, and σ2 can in fact
be arbitrary constants. Here we set μ1 = 1.0, σ1 = 2.0, and σ2 = 1.0. In our experiments, we set
d1 = 100, d2 = 20, d3 = 10, η = 0.001, and = 0.25. And the maximum number of iteration step
was set as 100. We randomly sampled 10 sets of weight matrices, optimal adversarial examples,
and initial adversarial examples for different methods. The l1 distance between the obtained student
adversarial examples and the optimal adversarial examples from the teacher is shown in Figure 1(a)
. The results show that LinBP can make the adversarial examples converge faster to the optimal
ones, indicating that LinBP helps to craft more powerful examples in the same hyperparameter
settings, which clearly confirms our Theorem 1. As we mentioned in Section 3, the norm of the
update obtained from LinBP is larger than that obtained from the standard BP, which may cause faster
convergence for LinBP. For deeper discussions, we have conducted two more experiments to analyze
the effect of LinBP. We used l2 norm and l∞ norm to normalize the update obtained from LinBP
and standard BP. η was set as 0.05 and 0.005, respectively. The results are shown in Figure 1(b) and
Figure 1(c) , where we can find our conclusion holds with gradient normalization.
40	60
Number of Iteration
(b) l2 normalized updates
Figure 1: LinBP leads to more powerful white-box adversarial
optimal ones.
(c) l∞ normalized updates
examples which are closer to the
(a) Original updates
Model training. As described in Section 3.2, we constructed the one-layer neural network formulaed
as Eq. (16) and trained it following the student-teacher framework. Note that Eq. (17) is the loss
function. Similar to the adversarial attack experiments, the input data matrix X was generated from
the standard Gaussian distribution and the optimal weight vector w? in the teacher network and
the initial weight vector w(0) in the student network were obtained via sampling from a Gaussian
distributions With μ1 = 1.0, μ2 = 0.0, σ1 = 3.0, and σ2 = 1.0. We used SGD for optimization and
set N = 100, d = 10, and η = 0.001 in our experiments. The maximal number of optimization
iteration Was as 10000 to guarantee training convergence. We used the l1 distance betWeen the
Weight vector in the teacher model and that in the student model to evaluate their difference. Figure 2
illustrates the experimental results over 10 runs and compares the tWo methods. From the results We
can observe that LinBP leads to more accurate approximation (i.e., smaller l1 distance) to the teacher
as Well as loWer training loss in comparison to BP, Which clearly confirms our Theorem 2, indicating
that LinBP can lead to faster convergence in the same settings.
4.2	More Practical Experiments
We also conducted experiments on more practical settings for adversarial attack and model training
on MNIST (LeCun et al., 1998) and CIFAR-10 (Krizhevsky et al., 2009), using DNNs With a variety
of different architectures, including a simple MLP, LeNet-5 (LeCun et al., 1998), ResNet-50 (He
et al., 2016), DenseNet-161 (Huang et al., 2017), and MobileNetV2 (Sandler et al., 2018). License of
the datasets and models can be found in the official paper or GitHub.
Adversarial attack on DNNs. We performed White-box adversarial attacks on CIFAR-10 using
several different DNNs, including VGG-16, ResNet-50, DenseNet-161, and MobileNetV2. We used
the test set of CIFAR-10 to implement I-FGSM (Kurakin et al., 2017) on pre-trained models collected
6
Under review as a conference paper at ICLR 2022
(a) Faster decrease of training loss
Figure 2: LinBP leads to lower training loss and better approximation to the teacher model, especially
at the early stage of training.
(b) Lower approximation error to the teacher
on GitHub to generate adversarial examples for this experiment. The attack success rate of these
adversarial examples was adopted to evaluate the performance of the attack, and only the correctly
classified benign CIFAR-10 images were chosen for generating adversarial examples. The update step
size of the iterative attack η was fixed to be 2/255. We tested with different settings of the maximum
number of learning step K and the perturbation budget . I-FGSM was performed following these
settings and we summarize the attack performance using LinBP and BP in Table 1. We considered
K ∈ {5, 10} and ∈ {0.05, 0.1}. It is easy to see in Table 1 that LinBP gains consistently higher
attack success rates with I-FGSM, showing that it can help generate more powerful adversarial
examples.
Table 1: Success rate of white-box adversarial attacks using I-FGSM with LinBP and BP in different
settings. Higher success rate (i.e., lower prediction accuracy on the adversarial examples) indicates
more powerful attack.
Method	K		VGG-16	ReSNet-50	DenSeNet-161	MobileNetV2
	5	0.05	71.37%	66.97%	58.25%	91.88%
BP	5	0.1	71.37%	66.97%	58.25%	91.88%
	10	0.05	90.99%	86.25%	82.94%	98.56%
	10	0.1	93.25%	90.76%	87.53%	98.87%
	5	0.05	97.80%	^^90.23%^^	82.56%	99.85%
LinBP	5	0.1	97.80%	90.23%	82.56%	99.85%
	10	0.05	99.90%	98.67%	96.31%	100.0%
	10	0.1	100.00%	99.72%	99.81%	100.0%
As we have analyzed, the superiority of LinBP can be more significant with smaller t. In general,
for performing adversarial attacks, the maximum number of optimization steps is rather limited and
normally at most hundreds. Under such circumstance, the constraint of η in Eq. (21) is easier to be
fulfilled and thus LinBP can be consistently better than BP in most test cases. We have also tested
with K = 500 and using both LinBP and BP gained 100% attack success rate with K = 500.
Model training. We first trained and evaluated the MLP and LeNet-5 on MNIST. The MLP has
four parameterized and learnable layers, and the numbers of its hidden layer units are 400, 200, and
100. We used SGD for optimization and the learning rate was 0.001 and 0.005 for the MLP and
LeNet-5, respectively. The training batch size was set to 64 and the training process lasted for at
most 50 epochs. The training loss and training accuracy are illustrated in Figure 3, and note that we
fix the random seed to eliminate unexpected randomness in training. We can easily observe from
the training curves in Figure 3 that the obtained MLP and LeNet-5 models show lower prediction
loss and higher accuracy when incorporating LinBP, especially in the early dozens of epochs, which
suggests that the incorporation of LinBP can be beneficial to the convergence of SGD. The same
observation can be made on the test set of MNIST and the results are shown in Figure 4.
We further report our experimental results on CIFAR-10. ResNet-50 (He et al., 2016), DenseNet-
161 (Huang et al., 2017), and MobileNetV2 (Sandler et al., 2018) were trained and evaluated on the
dataset. The architecture of these networks and their detailed settings can be found in their papers.
7
Under review as a conference paper at ICLR 2022
----MLP-BP
----MLP-LinBP
LenetS-BP
----LenetS-LinBP
(a) Decrease faster in training loss	(b) Increase faster in training accuracy
Figure 3: Compare the training loss and training accuracy of the MLP and LeNet-5 on MNIST using
LinBP and BP.
(a) Decrease faster in test loss
Figure 4: Compare the test loss and test accuracy
and BP.
——MLP-BP
----MLP-LinBP
LenetS-BP
----LenetS-LinBP
30 35 40 45 50

(b) Increase faster in test accuracy
of the MLP and LeNet-5 on MNIST using LinBP
The optimizer was SGD and the learning rate was set to 0.002. We set the batch size as 128 and
trained for 100 epochs. We evaluated the prediction loss and accuracy of trained models using LinBP
and BP. Similar to the experiment on MNIST, we fixed the random seed. The training results in
Figure 5 and test results in Figure 6 demonstrate that equipped with LinBP, the models achieved
lower prediction loss and higher prediction accuracy in the same training settings, especially when
the training just got started. Nevertheless, as training progressed, the superiority of LinBP became
less obvious and finally the performance of LinBP became just comparable to that of the standard BP,
which is consistent with our theoretical discussions in Section 3.2.
0 8 6 4 2 0
Lo.6o.o.o.
AJUnZV MU∙≡∙C5∙IH
DenseNet-BP
DenseNet-LinBP
ResNetSO-BP
MobileNetVl-BP
ResNetSO-LinBP
IVTobileNetVl-LinBP
(a) Decrease faster in training loss	(b) Increase faster in training accuracy
Figure 5:	Compare the training loss and training accuracy of ResNet-50, DenseNet-161, and Mo-
bileNetV2 on CIFAR-10 using LinBP and BP.
We also noticed that in certain scenarios, optimization with LinBP may fail to converge on very large
DNN models (and large learning rates, e.g., VGG-Net (Simonyan & Zisserman, 2015)), and it is
ascribed to unsatisfactory approximation to the gradient and needs to be further studied in future
work. Decreasing the learning rate appropriately may relieve the problem to some extent.
Adversarial training for DNNs. Since the discovery of adversarial examples, the vulnerability
of DNNs has been intensively discussed. Tremendous effort has been devoted to improve the
8
Under review as a conference paper at ICLR 2022
O
.0
S
----DenseNet-BP
ResNetSO-BP
MobileNetVl-BP
----DenseNet-LinBP
----ResNetSO-LinBP
----MobileNetV2-LinBP
10 20 30 40 50 60 70 80 90 100
Epochs
(a) Decrease faster in test loss	(b) Increase faster in test accuracy
Figure 6:	Compare the test loss and test accuracy of ResNet-50, DenseNet-161, and MobileNetV2 on
CIFAR-10 using LinBP and BP.
robustness of DNNs. Thus far, a variety of methods have been proposed, in which adversarial
training (Goodfellow et al., 2015; Madry et al., 2018) has become an indispensable procedure in many
application scenarios. In this context, we would like to study how LinBP can be adopted to further
enhance the robustness of DNNs. We used PGD to generate adversarial examples to construct our
dataset on which we train our model using SGD. There exist multiple strategies of adopting LinBP in
adversarial training, i.e., 1) computing gradients for updating model parameters using LinBP just
like in the model training experiment and 2) generated adversarial examples using LinBP as in the
adversarial attack experiment. We observed that the first strategy is beneficial, and further applying
the second strategy in combination slightly increases the performance in latter epochs (See Figure 6).
Specifically, we applied the classification accuracy on adversarial examples using the standard BP to
evaluate the model robustness. The experiment was performed on CIFAR-10 using MobileNetV2,
where the attack step size was 2/255, = 8/255, K = 5 and the training learning rate was 0.01.
Ooooooo
6 5 4 3 2 1
Aɔu.Injɔv -SOI IRMseSIaAPV
Figure 7: Compare the robustness of MobileNetV2 models shielded with different adversarial training
methods. We use “LinBP∣” and “LinBP*" to indicate models trained using the first strategy and the
combination of the two strategies as described in the above paragraph, respectively.
5 Conclusions
We have studied the optimization convergence of the standard BP and compare to the very recent
method called LinBP which skips ReLUs during the backward pass. Theoretical analyses have
been carefully performed in two popular application scenarios, i.e., white-box adversarial attack
and model training. In addition to the benefit on black-box transferability which has already been
shown in Guo et al. (2020)’s work, we have proven in this paper that LinBP also leads to generate
more destructive adversarial examples and faster convergence in the same hyperparameter settings.
Experimental results on simulated data confirms our theoretical results. Extensive experiments on
MNIST and CIFAR-10 further validate that the theoretical results hold in practical settings on a
variety of DNN architectures. Our theoretical and empirical discussions in the paper perform thorough
analysis on LinBP, and may become an inspiration for more researches on convergence analysis in
neural-network-involved learning tasks.
9
Under review as a conference paper at ICLR 2022
Ethics S tatement
Our theoretical and empirical discussions in the paper are related to machine learning robustness.
Make more destructive adversarial samples will fool the trained model and hamper the model
robustness. That may cause potential privacy and security concerns. We have read the codes of ethics,
and we guarantee that our work conforms to them.
Reproducibility S tatement
For our theoretical results, we state the full set of assumptions and the complete proof can be found
in the appendix. In our supplemental materials, we submit the entire codes for all the experiments in
the paper. We use the public datasets MNIST and CIFAR-10 which can be directly downloaded in
Pytorch.
References
Sanjeev Arora, Simon Du, Wei Hu, Zhiyuan Li, and Ruosong Wang. Fine-grained analysis of
optimization and generalization for overparameterized two-layer neural networks. In ICML, 2019.
Anish Athalye, Nicholas Carlini, and David Wagner. Obfuscated gradients give a false sense of
security: Circumventing defenses to adversarial examples. In ICML, 2018.
Leon Bottou. Large-scale machine learning with stochastic gradient descent. In Proceedings of
COMPSTAT,2010, pp. 177-186. Springer, 2010.
Nicholas Carlini and David Wagner. Towards evaluating the robustness of neural networks. In IEEE
symposium on security and privacy (SP), 2017.
Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas
Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image
is worth 16x16 words: Transformers for image recognition at scale. In ICLR, 2021.
Simon Du, Jason Lee, Haochuan Li, Liwei Wang, and Xiyu Zhai. Gradient descent finds global
minima of deep neural networks. In ICML, 2019a.
Simon S Du, Xiyu Zhai, Barnabas Poczos, and Aarti Singh. Gradient descent provably optimizes
over-parameterized neural networks. In ICLR, 2019b.
Ian J. Goodfellow, Jonathon Shlens, and Christian Szegedy. Explaining and harnessing adversarial
examples. In ICLR, 2015.
Yiwen Guo, Qizhang Li, and Hao Chen. Backpropagating linearly improves transferability of
adversarial examples. In NeurIPS, 2020.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image
recognition. In CVPR, 2016.
Gao Huang, Zhuang Liu, Laurens Van Der Maaten, and Kilian Q Weinberger. Densely connected
convolutional networks. In CVPR, 2017.
Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint
arXiv:1412.6980, 2014.
Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images. 2009.
Alexey Kurakin, Ian Goodfellow, and Samy Bengio. Adversarial machine learning at scale. In ICLR,
2017.
Yann LeCun. A theoretical framework for back-propagation. In Proceedings of the 1988 connectionist
models summer school, volume 1, pp. 21-28, 1988.
Yann LeCun, Leon Bottou, Yoshua Bengio, and Patrick Haffner. Gradient-based learning applied to
document recognition. Proceedings of the IEEE, 86(11):2278-2324, 1998.
10
Under review as a conference paper at ICLR 2022
Ilya Loshchilov and Frank Hutter. Sgdr: Stochastic gradient descent with warm restarts. In ICLR,
2017.
Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. In ICLR, 2019.
Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and Adrian Vladu.
Towards deep learning models resistant to adversarial attacks. In ICLR, 2018.
Seyed-Mohsen Moosavi-Dezfooli, Alhussein Fawzi, and Pascal Frossard. Deepfool: a simple and
accurate method to fool deep neural networks. In CVPR, 2016.
Nicolas Papernot, Patrick McDaniel, Ian Goodfellow, Somesh Jha, Z Berkay Celik, and Ananthram
Swami. Practical black-box attacks against machine learning. In ACM on Asia conference on
computer and communications security, 2017.
Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor
Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. Pytorch: An imperative style,
high-performance deep learning library. In NeurIPS, 2019.
Sashank J Reddi, Satyen Kale, and Sanjiv Kumar. On the convergence of adam and beyond. arXiv
preprint arXiv:1904.09237, 2019.
Mark Sandler, Andrew Howard, Menglong Zhu, Andrey Zhmoginov, and Liang-Chieh Chen. Mo-
bilenetv2: Inverted residuals and linear bottlenecks. In CVPR, 2018.
Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image
recognition. In ICLR, 2015.
Ilya Sutskever, James Martens, George Dahl, and Geoffrey Hinton. On the importance of initialization
and momentum in deep learning. In ICML, 2013.
Yuandong Tian. An analytical formula of population gradient for two-layered relu network and its
applications in convergence and critical point analysis. In ICML, 2017.
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz
Kaiser, and Illia Polosukhin. Attention is all you need. arXiv preprint arXiv:1706.03762, 2017.
A	Appendix
A.1 THE INFLUENCE OF η
As discussed in the main paper, the superiority of LinBP would be more obvious with reasonably
small η . We tried using LinBP with varying η, and observed that slightly decreasing the learning rate
for model training stabilized model training using LinBP and guaranteed the superiority of LinBP.
Here, for adequate comparison, we report results using two more η values than in the main paper in
Figure 8. The basic learning rate η is set as 0.001 and 0.005 for MLP and LeNet-5 in training MNIST
model just equals to the experiments in the main paper.. We further scaled the basic learning rate by a
factor of 10 and 0.1 in the experiment and report results in Figure 8.
It can be seen that, when scaled the learning rate by 10×, the performance of LinBP and BP are
similar on the MLP but on the LeNet-5 the training failed to converge with LinBP while it still
converged with BP 3. The results conform that a reasonably small η help stabilize training using
LinBP and guarantees its superiority.
3The same observation was also made when training VGG-16 on CIFAR-10 using LinBP and a learning rate
of 0.005.
11
Under review as a conference paper at ICLR 2022
0 5 10 15 20 25 30 35 40 45 50
Epochs
(a) Training loss (1 × learning rate)
0 8 6 4 2 0
Lo.o.o.o.o.
AJEJnZV XU∙≡∙s,lH
0 50 100 150 200 250 300 350 400 450 500
Epochs
(c) Training loss (0.1× learning rate)
(b) Training accuracy (1× learning rate)
Epochs
(d) Training accuracy (0.1× learning rate)
——MLP-BP
----MLP-LinBP
Lenet5-BP
----LenetS-LinBP
0 8 6 4 2
LSS0.0.
0 5 10 15 20 25 30 35 40 45 50
Epochs
(e)	Training loss (10× learning rate)
0.0
----MLP-BP
——MLP-LinBP
Lenet5-BP
----LenetS-LinBP
I 4I--1------1-----1-----------.-----------.---------------
0	5	10 15	20 25	30 35	40 45	50
Epochs
(f)	Training accuracy (10× learning rate)

”UJnJJV MUPIWI
Figure 8: The influence of setting different learning rate to LinBP and BP for training the MLP and
LeNet-5 on MNIST.
A.2 Proofs
In this subsection, we will give theoretical proofs on Lemma 1, Theorem 1 and Theorem 2 in the
main paper. Note that our proofs are mainly based on the following theorem in Tian (2017)’s work.
Theorem 1 in Tian (2017) Denote F(e, w) := XT D(X, e)D(X, w)Xw, where e is a unit vector
and X = [x1, x2, ∙ ∙ ∙ , XN]T ∈ RNXd is the input data matrix. If X is standard Gaussian distributed,
we have
N
E (F(e, W)) =	[(π — θ)w + ∣∣wk Sin θe],
where θ ∈ [0, π] is the angle between e and w.
A.2. 1 Proof of Lemma 1
We first recall the contents of Lemma 1.
Lemma 1 Denote G(e, x) := WT D(W, e)VT VD(W, x)Wx, where e ∈ Rd1 is a unit vector,
x ∈ Rd1 is the input data vector, W ∈ Rd2Xd1 and V ∈ Rd3Xd2 are weight matrices. If W and V
12
Under review as a conference paper at ICLR 2022
follow independent standard Gaussian distribution, we have
E (G(e, X)) = d2[(∏ - Θ)x + ∣∣xk sinΘe],
2π
where Θ ∈ [0, π] is the angle between e and x.
Proof. Assume W = [wι,…，w&?]t and V = [vι,…，7&2], where Wi ∈ Rd1 and Vi ∈ Rd3, for
i = 1, ∙∙∙ ,d2. For G(e, x), we have
d3
G(e, x) =	Σ	Σ	Σ vidvdjwiwjTx.	(23)
i:wiT x≥0,wiT e≥0 j:wjT x≥0,wjT e≥0 d=1
We consider the expectation of G(e, x), there is
d2 d2	d3
E(Ge X)) =E EE(E Videdj )E(WiWT ∙I(WT x≥0,wT e≥0,wT x≥0,wT e≥0)(X, e)x),	(24)
i=1 j=1	d=1
where IA (x) is the indicator function, i.e., IA(x) equals 1 if x ∈ A and equals 0 if x ∈/ A.
As the assumption that W and V follow independent standard Gaussian distribution, we find
E(Pdd3=1 vidvdj) = 0 when i 6= j and E(Pdd3=1 vidvdj) = 1 when i = j. Therefore, Eq. (24) can be
simplified as
d2
E(Ge X)) = X E(WiWT ∙I(WTχ≥0,wTe≥0)(x, e)x).	(25)
i=1
We then introduce a coordinate system, where e = [1,0,…，0]t, X = ∣∣x∣ [cos Θ, Sin Θ, 0,…，0]t
are hold. Thus, Wi = [r cos φ%, r sin φi, w33,…,wi,dι]t. We then can rewrite Eq. (25) as
E(G(e, x)) = E X	WiWiTx
i:WT x≥0,WT e≥0
i i	T	(26)
= E	WiWiTx
i ; φi∈[-∏∕2+θ,π∕2]
Then we compute the following equation,
R(φ0) = E
NN X	WiWiT
i ; φi∈[0,φo]
E [wwt∣φ ∈ [0, φo]] P [φ ∈ [0, φo]]
Z ∞ ...Z φ0Z
-∞	0	0
∞d
WWT p(r)p(φ)	p(wi)rdrdφdw3 . . . dwd1 ,
i=3
(27)
∞
r	r r	—J	t 1 I 1	1	~	-K-KT Cll	♦	t∙ . ∙-l J	.1	CCi♦
where p(r) = e 2 and p(φ) = 2∏. Since W follows gaussian distribution, the off-diagonal and
diagonal elements except the first 2 X 2 block are equal to 0 and φφ∏ respectively. The first 2 X 2
block can be formulated as
R(Φθ)[L2,L2]=广 Z∞
00
r cos φ
r sin φ
r3e
r2
2
2π
[r cos φ r sin φ] p(r)p(φ)rdrdφ
f
dr
0
φ0
cos2 φ
cos φ sin φ
cos φ sin φ
sin2 φ dφ.
(28)
∞
0
1 2 + sin 2φo 1 — cos 2φo
4∏ 1 — cos 2φo 2 — sin 2φo
Further we have
R(φ0)
2∏ Id
1
+ 4∏
sin 2φ0
1 — cos 2φ0
0
1 — cos 2φ0 0
— sin 2φ0	0
00
(29)
13
Under review as a conference paper at ICLR 2022
Then Eq. (26) can be formulated as
E(G(e, x))
∏ π - Θτ I d2 ∕∣^0 2]	「 sin(2Θ - π)
d2 ^^2^Id + 4∏(|_2 0∖ - [l - cos(2Θ - π)
π - Θ	d2kxk 2sinΘ
d2 F-X +	-	0
2π	4π	0
—[(π - Θ)x + ∣∣xk sinΘe]
2π
1 - cos(2Θ -π) cos Θ
-sin(2Θ-π ) )∣x∣ sinΘ
(30)
A.2.2 Lemma for further proof.
Before we delve deep into Theorem 1 and Theorem 2, we first propose a lemma for convenience. The
lemma is described as follows,
Lemma 2 Let αi define as
Oi = (U ∙ x? - V ∙ Xi)Sgn(x? - Xi)
U u ∙ X? — v ∙ Xi,
ɪv ∙ Xi — U ∙ X?,
if Xi? > Xi ,
if Xi? < Xi ,
where U and V are constants, x?〜N(μ1, σ2) and Xi 〜N(μ2, σ2). The expectation of αi can be
formulated as
E(αi) = 2γ(u ∙ σ2 + v ∙ σ2) + (u ∙ μι - V ∙ μ2)(2P(Xi < x?) - 1),
where — —.	1	_ e-(乂1-μ2)2∕2(σ2+σ2)
r ] = √2∏(σ2 + σ2)
and μ2 = 0.
> 0. Further we have E(αi) > 0 when U > 0, v > 0
Proof. From the definition of αi , the expectation of αi can be formulated as
E(θi) = E(U ∙ x? - v ∙ Xi|x? > Xi)P(x? > Xi) + E(v ∙ Xi- U ∙ x?|x? < Xi)P(x? < Xi). (31)
Further, We have x? - Xi 〜N(μι - μ2,σ2 + σ2). Therefore, We can conclude that P(x? > Xi)=
1 - F(0) and P(x? < Xi) = F(0), where F(∙) denotes the cumulative distribution function for
N(μι - μ2,σ2 + σ2).
We then solve the conditional cumulative distribution function for x? when x? > Xi. We define fι(∙),
f2(∙) denote the probability density function of x? and Xi and Fι(∙), F2(∙) denote the cumulative
distribution function of Xi? and Xi . There we have
G1(x) = P(Xi? ≤ x|Xi? > Xi)
=P(Xi < x? ≤ χ)
―	P(Xi < x?)
=P(Xi 1< X?) /∞ P(y< x? ≤ Xidn	(32)
1x
=P(Xi < x?) ∕∞(FI(X)- Fl(y))f2(y)dy.
Therefore, the probability density function can be formulated as
1xx
gι(X) = P(x. < x?) (FI(X)J	f2(y)dy)0 - (J	(FI(y)f2(y)dy)0
=ET―: ?、(fι(X) /	f2(y)dy) + FI(X)f2(X) - FI(X)f2(X)	(33)
P(Xi < Xi)	-∞
=f1(X)F2(X)
—P(Xi < x?).
14
Under review as a conference paper at ICLR 2022
Similarly, we first solve the conditional cumulative distribution function for xi when xi? > xi . There
we have,
G2(x) = 1 - P(xi > x|xi? > xi)
P(x < xi < xi?)
=1 ——— ---------：—
P(Xi < x?)
1 ---------
P(Xi < x?)
1
1 —-------
P(Xi < x?)
f
X
Z
X
P(x < xi < y)f1 (y)dy
(F2 (y) - F2(x))f1 (y)dy.
(34)
1
The probability density function can be formulated as
g2(x)
P(xi < xi?)
1
P(xi < xi?)
∞∞
(F2(x)	f1 (y)dy)0 - (	(F2(y)f1 (y)dy)0
XX
(f2 (x)	f1 (y)dy) - F2(x)f1 (x) + F2(x)f1 (x)
X
(35)
1
f2(x)(1- Fι(x))
P(Xi < X?)
We assume φ(∙) and Φ(∙) represent the probability density function and cumulative distribution
function for standard gaussian distribution. There we have
E(Xi? |Xi? > Xi) =	xg1 (x)dx
-∞
P(xi < xi?)
1
P(xi < xi?)
1
P(xi < xi?)
Z∞
∞
Z∞
∞
σ φ(
x-μι 粒(χ-μ2)dx
σ1
σ2
」(X-μ1 )2/2b2 Φ( x-μ2 )dx
x
∖p2∏σ∖
e-(X-μι)2∕2σ2 φ( x-μl )dχ
σ2
σ2
(36)
1
The integral can be devided into two part. The first part can be formulated as
/∞ x - μι
J-∞ 2∏σσ1
」(X-μι)2∕2σ2 φ( χ-μ2 )dχ
σ2
Φ(X - μ)
σ2
σ1
2σ2π
d(-eTx-μI)32σ2)
e-(X-μι )2/2^2
dx
dΦ(守)
dx
dx
dx
Z∞
e-(X-*I)2 ∕2σ2-设一四厂/^2 dχ	(37)
∞
σι e
2σ2π
-(μι-μ2)2∕2(σ2 + σ2) σ1σ2 A
σ12
√σi^+"σ2
e-(“i-μ2)2∕2S2+σ2)
√2π(σ2+ σ2)
Step 3 equals Step 4 because there is R-∞ e-(aX2+bX+c)dx = e(b2-4aC)∕4apa. The second part can
be formulated as
Z∞
∞ √2∏σιe
-(X-μι)2∕2σ2 Φ( χ-μ2 )dχ = μι
σ2
Zf1 (x)F2 (x)dx
∞
(38)
From Eq. (33) we have
Zg1 (x)dx
∞
R∞∞ fι(χ)F2(χ)dχ = 1
-P(Xi < X?)—-
(39)
15
Under review as a conference paper at ICLR 2022
Therefore we have
Z ∞
-∞
4 e-(x-μι)2∕2σ1 Φ( =2 )dx
√2πσι	σ
μι / f1(x)F2(x)dx = μιP(Xi < x?)
J—oo
(40)
As the result in Eq. (37) and Eq. (40), Eq. (36) can be formulated as

e一(μι—μ2 )2∕2(σ2+σ2)
where Y
E(x?|x? > Xi)
P(Xi < x?)
十 μι
=	Yσ2	+,
-P(Xi < x?)	1,
e—(M1—μ2)2∕2(σ2 +σ2) > 0.
(41)
1
Similarly, we have
E(Xi|x? > Xi) = /	xg2(x)dx
J—oo
P(xi < xi?)
1
P(xi < xi?)
1
Z ∞
-∞
Z ∞
-∞
x-μ2 )(1 - φ( x-μι ))dx
σ2
σι
一(X-“2)2/2登(1 - Φ( x-μ1 ))dx
σι
P(xi < xi?)
-/ —e
√2πσ2
导(
x
e—(x—μ)2∕2σ2(ι - φ(X - μι
σι
The integral can also be devided into two part. The first part can be formulated as
))dx
(42)
/∞ X - μ2
J—z Λ∕2πσ2
e—(x—μ)2∕2σ2 (1 - Φ(x-μ1 ))dx
r∞
√⅛ LR- φ( x-≡)]
σι
d(-e—(x—μ)2∕2σ2)
dx
dx
…)2∕2σ2 d(1- φ(守))
dx
dx
-σ2 j e—(x—*1)2/2说一(x-*2)2∕2σ2 dx
2σι∏ J—z
(43)
=W e—(“1—"2)2∕2(σ2+σ2)w2 师
2σ1π	Pσ2 + σ2
=	,	σ2	e-(*i-μ2)2∕2(σ2+σ2)
√2π(σ2 ÷ σ2)
The second part can be formulated as
Z ∞
∞ √⅛ e
From Eq. (35) we have
L(X- "G2/^2 (1 - φ( x-μ1 ))dx = μ2
σι
f	∕2(x)(1 - Fι(x))dx
J —Z
(44)
Zg2(x)dx
-∞
JZo f2(x)(1 — FI (x))dx
P(xi < xi?)
(45)
Therefore we have
Z ∞
∞ √⅛e
L(X—“G2∕2σ2(1 - Φ(x-μ1 ))dx = μ2
σι
/	f2(x)(1- Fι(x))dx
J —Z
(46)
1
1
μ2P(w < w?)
16
Under review as a conference paper at ICLR 2022
Therefore, Eq. (42) can be formulated as
E(xi|xi? > xi)
P(xi < xi?)
-γσ22
P(Xi < χ? + μ2.
+ μ2
(47)
Due to symmetry, we also have
E(Xi|x? < Xi) = P(X? χ?) + μ2,
and
-γσ2
E(XiIXi < Xi)= P(Xi > X?) + μ1.
Therefore, Eq. (31) can be formulated as
E(ai) =Y(U ∙ σ2 + V ∙ σ2) + (u ∙ μ1 - V ∙ μ2)P(Xi < x?
+ Y(V ∙ σ2 + u ∙ σ2) + (v ∙ μ2 - u ∙ μ1 )P(Xi > x?)	(48)
=2γ(u ∙ σ2 + v ∙ σ2) + (u ∙ μ1 - V ∙ μ2)(2P(Xi < x?) - 1),
where Y = /	1, = e-(μ1-μ2)2/2(σ2 +σ2) > 0. When μ2 = 0, We have
2π(σ12+σ22 )
E(αi) = 2γ(u ∙ σ2 + V ∙ σ2) + uμ1(2P(xi < x?) - 1).
If u > 0 and v > 0, we have Y(U ∙ σ2 + V ∙ σ2) > 0, and
uμι(2P(xi < x?) - 1) = uμι(1 - 2F(0)),
(49)
(50)
where F is the cumulative distribution function for Xi? - Xi , and follows the gaussian distribution
N(μ1,σ2 + σ2). If μι > 0, we have F(0) < 0.5, and Eq. (50) is greater than 0. If μι < 0, we have
F(0) > 0.5, and Eq. (50) is also greater than 0. Therefore, we have uμι(1 - 2F(0)) ≥ 0. In sum,
we have E(ai) > 0 when u > 0, v > 0 and μ2 = 0.
A.2.3 Proof of Theorem 1
We first recall the update rules for BP and LinBP, which are formulated as
x(t+I) = CliP(X㈤-ηVx(t)L(X㈤))，	(51)
and
X(t+1) = Clip(X㈤-nVX)L(X(t))),	(52)
respectively, where Clip(∙) = min(x + el, max(x - el, ∙)) and the expectation of the gradient
obtained by BP and LinBP can be computed from Lemma 1, i.e.,
N	N	kX? k
E[VxL(X)] = G(X∕kxk,X)-G(X∕kxk,x?) = E(X-x*) + 2∏ (θx?-kkXksinθx) , (53)
and
N
E[VXL(x)] = G(x∕kxk, x) - G(x?/kx?k, x?) = E(X - x?),	(54)
, respectively. Note Θ ∈ [0, π] is the angle between X and X?. Then we begin our proof.
Theorem 1 For the two-layer teacher-student network formulated as Eq. (9), the adversarial attack
sets Eq. (10) and Eq. (15) as the loss function and the update rule, respectively. Assume that W and
V follow independent standard Gaussian distribution, x? ~ N(μι, σ2), x(0) ~ N(0, σ2), and n is
reasonably small4. Let x(t) and X(t) be the adversarial examples generated in the t-th iteration of
attack using BP and LinBP, respectively, then we have
Ekx? - X⑴kι ≤ Ekx? - X⑴kι.
4See Eq. (67) for more details of the constraint.
17
Under review as a conference paper at ICLR 2022
Proof We first analyse the property of the Eq. (51). Assume x(v) is the first {x(t) } to satisfy
|xi(v) - xi(0) | = , which also means |xi(v-1) - xi(0) | < . If xi(v) = xi(0) + , from Eq. (51), we have
xi(v) =	(lip(x(v-1)-华(XL)-x?)) Clip((1 - ηd2)x(v-1) + 华X?)	(55) xi(O) + e.
As |xi(v-1) - xi(0) |
< , we have x? > x(0) + . Therefore, for the v + 1-th step, we have
xi(v+1) =	yip(x(v)-竽(X(V)-x?)) Gp((1-ηd⅛(v) + 竽 x?)	(56) Gp((1-竽)(x(°)+ e) + 竽X?) xi(O) + e,
note that the final step is established because (1 -噬)(x(0) + e) + ηd2-x? > x(0) + e. Further We
have xi(t) = xi(0) + for ∀t > v. If xi(v) = xi(0) - , from Eq. (51), we have
xi(v)	=Clip((1 -吟)x(v-1) + 华x?) 2 i	2 i	(57) (O) xi - e.
As |xi(v-1) - xi(0) | < , We have x? < x(0) - . Therefore, for the v + 1-th step, We have
xi(v+1) =	ep((1- ηd2 )x(v) + ηd2 X?) Gp((1- ηd2)(x(°)-e) + ηd2x?)	(58) xi(O) - e.
Therefore we have for ∀t > v, xi(t)	= xi(O) - e. In sum, for ∀t > v, we have |xi(t) - xi(O) | = e. Similar
conclusion can be for made for Eq. (52), therefore We can find that if a x(t) (or x(t)) achieve the bound
of Clip, the following steps will keep the bounded value. We then let Pj = θjx? - 八％)： Sin θjx(j),
the lι distance bewteen x(t) (X⑴)and x? can be formulated as
Ekx? - x(t+1)kι = Ekx? - CIiP(X⑴-nVx(t)L(x(t)))kι
EkH ((1- ηd2)(x? - x(t)) + 华Pt) kι
2	2π
EkH ((1- ηd2)t+1(x? -X(O)) + XX T(1 -竽厂p) kι,
(59)
and
Ekx? - X(t+1)kι = Ekx? - CIiP(X(t) - ηVXt)L(X(t)))ki
EkH
t+1 (x? - X(O))) kι,
(60)
where H(∙) = min(x? - X(O) + el, max(x? - X(O) - el, ∙)). From Eq. (59) and Eq. (60), further
we have
EkX? - X(MkI = EkH ((1- ηd2)t+1(x? -X(O)) + XX ηd2(1 - ηd2)t-jpJ kι
=XE|H ((1 - ηd2)t+1(x? - x(O)) + E *(1 - ηd2尸PJ I,
(61)
18
Under review as a conference paper at ICLR 2022
and
Ekx? - x(t+1)k
k1
) i|.
(62)
Note that x(0) = x(0) inthetheorem. If |x?-x(0)∣ < e, then for ∀t, |x?-x(0)∣ < C and |x?-X(0)| < e,
which the Clip function can be removed in this case. Eq. (61) and Eq. (62) can be formulated as
Ekx? - x(t+1)kι = XXE|(1 - ηd2)t+1(x? -xi0)) + XX ηd2(1 - ηd2尸PjiI
i=0	j=0 π
=XXE ((1- ηd2)t+1(x? - x(0)) + XX η2∏2(1 - ηd2厂Pj) sgn(x?-姗)),
i=0	2	j=0 2π	2
(63)
and
d
Ekx? - x(t+1)k1 = XE
i=0
(X? - X(O))) Sgn(X?-
xi(0)).
(64)
If ∣x? - x(0) ∣ ≥ c, the sign of H(•) is determined by the sign of x? - xi0). Therefore Eq. (61) and
Eq. (62) can be formulated as
and
Ekx? -x(t+1)k1
d
Ekx? - X(t+1)k1 = X EH
i=O
PjiJ sgn(x? - X(O)),
(65)
t+1 (X?- XiO)))Sgn(X? -XiO)).
(66)
Recall the assumption that η is sufficiently small, to be exact, it should satisfy the following con-
straints,
(1 - ηd2)m-1-jPjiI < I(1 - ηd2)m(x? - x(0))I,	(67)
j=O
for m = 1, . . . , t and i = 1, . . . , d. Under the constraints, we find the sign of xi? - xi(O) determine
the sign of ((1 - ηN)t-jpji) . Therefore, when We compare Eq. (63) and Eq. (64), Eq. (65)
and Eq. (66), we actually compare the sign of E(PjiSgn(w? - W(O)). We let Z(j) = (1 - ηdd2 +
ηNXXj)k Sin θj) and β(j) = ηd2(1 - θ). As θj ∈ [0,∏] and 1 - ηdd2 > 0, we have Z(j) > 0 and
β(j ) > 0 for ∀j . There we have
Ex(t+1) = E Z(t)x(t) + β(t)x?
t	tt
Y Z(i) x(O) + X Y Z(j)β(i)
i=O	i=O j=i+1
19
Under review as a conference paper at ICLR 2022
We then solve the following equation,
E (Psisgn(X? -X(O))) =E 0θsx? - jkXsk- Sin θsχ(S))Sgn(X? -X(O)))
=E (θs - jkχη∣ sinθs X Y Z(j)e(i) x*sgn㈤-XiO))- (69)
E (∣Xy∣ sin θs YZ(i)! x(O)Sgn(X? -Xi°).
?	s-1 s-1	?	t-1
We have θs - IkxJk	sin θs	P Q	Z(j)β(i)	> 0 and	,,kXs)1,,	sin θs Q	Z(i)	> 0. Recall that x?〜
x	i=O j=i+1	x	i=O
N(μι,σ2) and x(0)〜 N(0, σ22), using the Lemma 2, We have
E(PjiSgn(wi? -wi(O)) > 0.	(70)
With Eq. (70), We find Eq. (63) and Eq. (65) are greater than Eq. (64) and Eq. (66), reSpectively. That
iS to Say, We have
E-x? - X(t+1)-1 ≤ E-x* - x(t+1)-1-1.
A.2.4 Proof of Theorem 2
Theorem 2 For the one-layer teacher-student network formulated as Eq. (16), the training task
sets Eq. (17) and Eq. (20) as the loss function and the update rule, respectively. Assume that X
is generated from standard Gaussian distribution, w?〜N (μι,σ2), W(O)〜N (0, σ2, and η is
reasonably small5. Let w(t) and W(t) be the weight vectors obtained in the t-th iteration oftraining
using standard BP and LinBP respectively. Then we have
E-w* - W㈤-1 ≤ E-w* - W⑴-1.
Proof. We firSt recall the partial gradient to W for BP and LinBP, Which are formulated aS
VwL(w) = XTD(X, w)(D(X, w)Xw - D(X, w*)Xw*),	(71)
and
VWL(W) = XT(D(X, w)Xw - D(X, w*)Xw*),	(72)
reSpectively. From Theorem 1 in Tian (2017), We can calculate the expectation of Eq. (71) and
Eq. (72) aS
N	N	-w?-
E[VwL(w)] = -(w - w*) + 2π (θw? - Iwk sin θw) ,	(73)
Where θ ∈ [0, π] iS the angle betWeen w and w?, and
N
E[VwL(w)] = y(w - W ).	(74)
The expectation of 11 distance between w(t+1) (W(t+1)) and w? can be formulated as
E-w? - w(t+1)-1 = E-w? - w(t) + nVw(t)L(w(t))-1
=E-(I- nN)(w?-w(t)) + nN∙ (θw? --w⅞sinθw(t)) -1,	(75)
and
E-W? - W(t+1)-1 = E-w* - W⑴ + nVw(t)L(W⑴)-1
=E-(1 - nN)(w* - W⑴)-1.
5See Eq. (79) for a precious formulation of the constraint.
(76)
20
Under review as a conference paper at ICLR 2022
We assume qj = θjw? - 口场)、Sin θjw(j). Therefore, Eq. (75) can be formulated as
Ekw? - w(t+1)kι = ElI(I- η^- )(w?- w(t))+ η^- qtkι
2	2π
=Ek(I- ηN)2(w? - w(t-1)) + (1 - ηN)ηNqt-1 + ηNqtkι	(77)
2	2	2π	2π	(77)
=Ek(1 - ηN)t+1(w? - W(O)) + XX η2∏(1 - ηN)t-jqj kι.
j=0 π
And Eq. (76) can be formulated as
Ekw? - W(t+1)kι = Ek(1 - ηN)(w? - W㈤)kι
2N	(78)
=Ek(1 - ητ)t+1(w? - W⑼)kι.
Note that W(O) = w(0). As mentioned in Theorem 1, We assume η is is sufficiently small, to be exact,
it should satisfy the following constraints,
m
IX nN(1 - ηN)m-jqji∣ < 1(1 - ηN)m+1(w? - W(O)儿	(79)
j =O 2π	2	2
for m = 1, . . . , t and i = 1, . . . , d. Under the constraints, wi? - wi(O) determine the sign of
wi? - wi(t+1) in Eq. (77), then Eq. (77) and Eq. (78) can be calculated as
Ekw? - w(t+1)k1
d
XE
i=O
d
XE
i=O
卜-ηN)
((1 - ηN)
(80)
(81)
and
Ekw? -
Similar to Theorem 2, using Lemma 2, We have
E qjisgn(wi? - wi(O)) > 0.
With Eq. (82), We find Eq. (80) is greater than Eq. (81), i.e.,
Ekw? - W(t+1)k1 ≤ Ekw? - w(t+1)k1.
(82)
21