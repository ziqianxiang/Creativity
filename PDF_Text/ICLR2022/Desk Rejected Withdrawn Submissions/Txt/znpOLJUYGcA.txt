Under review as a conference paper at ICLR 2022
Automatic Integration for Neural
Point Processes
Anonymous authors
Paper under double-blind review
Ab stract
The fundamental bottleneck of learning continuous time point processes is in-
tegration. Due to the intrinsic mathematical difficulty of symbolic integration,
neural point process models either constrain the intensity function to an integrable
functional form or apply certain numerical methods. However, the former has lim-
ited expressive power, and the latter suffers additional numerical errors and high
computational costs. In this paper, we introduce automatic integration for neural
point process models, a new paradigm for exact, efficient, non-parametric infer-
ence of point process. We validate our method on many synthetic temporal point
process datasets and focus on the recovery of the underlying intensity function.
We demonstrate that our method has clear advantages for learning temporal data
governed by complex intensity functions. On real-world datasets with noise and
unknown intensity functions, Our method is also much faster than state-of-the-art
neural point process models with comparable prediction accuracy.
1	Introduction
Social media posts, stock transactions, COVID infections — millions of event sequences are being
generated every day. Unlike regular time series, event sequences are irregularly sampled with miss-
ing values. Many deep sequence models such as RNN, LSTM (Hochreiter & Schmidhuber, 1997)
or Transformer (Vaswani et al., 2017) do not emphasize on the time intervals between events, only
updating the hidden representation whenever events occur. In contrast, neural point process (NPP)
models including Neural Hawkes Mei & Eisner (2016), Neural ODE Chen et al. (2018) allow hidden
representation to vary between events and are better suited for modeling event sequences.
Neural point processes combine deep sequence models with point processes. However, existing
NPPs often fail to validate that the model can indeed recover the true intensity function. Even for
those with synthetic experiments (Du et al., 2016; Shchur et al., 2019), the validation is only limited
to monotone intensity changes, such as Hawkes and self-correcting processes. Hence, it remains an
open question whether NPP can accurately pick up the subtlety in the inter-event intensity change
and infer the true influence function. Furthermore, log-likelihood is a gold standard for evaluating
NPPs, but we found test log-likelihood may not be a good metric. Because learning a wrong intensity
function can have little to no effect on the test likelihood, as we will show in Section 4.
The expressivity of NPP is another limitation. Existing methods assume that the current influence
follows an exponential decay of the intensity function (Du et al., 2016), or of the latent representation
(Mozer et al., 2017), or even a linear interpolation (Zuo et al., 2020). Such an assumption is easily
violated in real-world scenarios. Consider the delayed jump effect in social media posts, the content
of a viral post will not be visible until several hours later. In other cases, the event influence can be
cyclic, e.g. a social media bot posts every day around the same time. Both scenarios turn out to very
challenging for the existing NPP models.
As deep neural networks are universal functional approximators. The question is: can we directly
use deep neural networks to approximate the influence functions in point processes? If successful,
the resulting NPP would significantly relax the assumption imposed by existing NPPs and open
up new venues to model complex real-world event dynamics with “delayed jump” or “cyclic influ-
ence”. Unfortunately, such a strategy has a major bottleneck that has prevented others from pursing
further: it requires the integration of a complicated deep neural network over a large time span,
where numerical integration is both inefficient and prone to numerical errors.
1
Under review as a conference paper at ICLR 2022
In this paper, we introduce the automatic integration technique (Lindell et al., 2021; Li et al., 2019)
to the field of point process to effectively solve this problem. Our method takes a dual network
approach, a gradient network and an integral network. The gradient network is trained............ We
validate on three different temporal point processes using the automatic integration technique and
notice that some constraints (e.g. same influences for events) still need to be enforced in order to let
the model learn correctly. Nonetheless, with the constraint, the model is able to capture complicated
dynamics with high time efficiency.
To summarize, our contributions are the following:
•	We first combine neural temporal point processes and automatic integration, introducing
ways to enforce intensity function’s positivity given that the original technique does not
allow controlling integrant’s functional form.
•	We show the automatic integration efficiently learns smoother intensity function with
higher accuracy, compared to traditional numerical methods.
•	We propose a simple RNN-free model that can recover relatively complicated influence
functions, enjoys high training speed and performs on par with the state-of-the-art methods
on real-world data.
2	Background
Temporal Point Processes. A temporal point process (TPP) is a counting process N (t), repre-
senting the number of events that occurred before time t. It is characterized by a scalar non-negative
intensity function λ* (t). Given a infinitesimal half-open time window [t, t + dt), conditioning on the
history events before time t, Ht := {t1, ..., tn}tn≤t, the intensity function means the event arrival
rate at t, and is formally defined as
λ*(t):
lim E[N(t,t + dt)∣Ht]
∆ t —→ 0	dt
The notation * is from Daley & Vere-Jones (2007) to indicate the intensity is conditional on the
past but not including the present. A example of TPP is Hawkes process, characterized by λ* (t)=
μ + α Pt <t exp(一β(t — ti)), where β is a scalar parameter. Arrival of a new event results in a
sudden increase of α, and the influence of this event will decay exponentially with the rate β over
time. μ is the base intensity meaning the rate of event happening on its own.
Neural Point Processes. Neural Point Process (NPP) models (Mei & Eisner, 2016; Du et al., 2016;
Zuo et al., 2020) combine deep sequential models with TPP. State-of-the-art NPPs first encode the
events into hidden representations using either RNN or Transformer, then model the conditional
intensity function as
λ*(t) = μ + f+(wT h + fcurrent (t,tn)),	λ*(t) = f+(wT fcurrent(h, t, tn ))
Here tn is the time of the last event, wT maps the hidden representation to a scalar intensity, f+
ensures the intensity is positive. The function fcurrent represents the current influence of the intensity
or the hidden states from t to tn . fcurrent is usually assumed to be an exponential decay function,
which means the intensity update between event is monotone or uni-modal.
Given a parametric intensity function λ^(t), the log likelihood of an event sequence {tι,…，tN}
observed in time interval [0, T] generated by the intensity function is
L({t1,..., tN})
N
X iog(λ》(t-)) +
i=1
∕∖(t)
Due to the presence of integration, designing the appropriate form of the intensity function λ^(t)
becomes essential. If the intensity function is simple such as an exponential decay (Du et al., 2016),
then the integration has a closed form. But this also limits the expressive power of the NPP model.
If the intensity function is complex, then numerical integration is required (Mei & Eisner, 2016; Zuo
et al., 2020). We found in our experiments that the numerical errors may prevent the model from
recovering the true underlying intensity function.
2
Under review as a conference paper at ICLR 2022
Automatic Integration Integration is fundamentally more difficult than differentiation. Unlike
differentiation which can always break down composite functions using the chain rule, integra-
tion has no effective way to decompose composite functions. Closed-form antiderivative solutions
only exist for a small set of functions, and when they exist, different algorithms other than back-
propagation are required for finding them, such as the Risch-Norman algorithm (Risch, 1969). This
makes the direct integration of a deep neural network intractable.
Using deep neural networks for automatic integration have been investigated in (Teichert et al., 2019;
Li et al., 2019; Lindell et al., 2021). A common idea is think in reverse: transforming integration
to learning the weights for the gradient of another neural network. Specifically, an integral network
is first constructed, and then a gradient network is formed by reassembling the integral network’s
computational graph. The gradient network is the partial derivative of the integral network w.r.t. a
specific input dimension. Thus, the definite integral can be computed by the fundamental theorem
of calculus, using the integral network’s values at the endpoints. A drawback of this dual network
approach is that we cannot control the form of the integrant, e.g. add an exponential layer to it to
make it positive. In other words, after we know the antiderivative ofa function, itis clear that finding
the antiderivative of the exponential of the function is still intractable.
Numerical integration, including Riemann integral, Monte Carlo sampling, and orthogonal polyno-
mials (Davis & Rabinowitz, 2007) are not “automatic”. Liu (2020) propose Taylor approximation
using derivatives from auto-differentiation and also name it automatic integration; it still requires
partitioning of the integral interval. Such a method requires partitioning of the integral interval,
multiple computational graphs and is computationally expensive.
3	Methodology
3.1	Model Definition
We consider the following neural point process model:
λ*(t)= μ + X f‰(t- ti, Hti)	(1)
i
Where μ is a scalar parameter representing the base intensity. We model the event intensity as the
sum of the influence from all the past events, each represented by a neural network function fN+N
with a positive scalar output. Hti is the entire history before event i.
Note that the NPP in Equation 1 directly models the influence of individual event separately which is
infeasible for existing NPPs. Because for simple influence functions, it usually involves a monotone
decay over time, far-away events would have diminishing influence. For complex influence function
where numerical integration is required, a large amount of samples is required to evaluate the very
first event’s influence on the final event. Most models opt for using deep sequence models to embed
the history and only consider latest event. In contrast, thanks to automatic integration, our model
can efficiently evaluate definite intervals over a long time span without such a constraint.
Our design has two major benefits. First, as neural networks are universal function approximators,
our model is able to approximate any inter-event change in intensity when a new event arrives,
including the aforementioned “delayed jump” and “cyclic influence” scenarios. Second, the model
is more interpretable. We can analyze different past events’ contribution percentages to a new event
because the intensity function is fully decomposable. Whereas for those with deep sequence models,
the past influence is a black-box as it is determined arbitrarily by the hidden representation.
There are many options for representing Hti as an input to the neural network. The first one is to
simply ignore Hti and to assume all events share the same feed-forward neural network function,
hence the model AutoInt Process with same influence:
λ*(t)= μ + X /Nn(t- ti),	fNN : R1 → R1	⑵
i
Such a model is quite limited in its expressivity. Because there are no hidden states in the fNN,
other contextual information of the event can not be easily integrated into the model.
3
Under review as a conference paper at ICLR 2022
An alternative is to still use an RNN or Transformer to encode the history Hti into hidden represen-
tation hi , hence the model AutoInt Process with unconstrained influence:
λ*(t) = μ + X /Nn(t- ti ㊉ hi),	fNN ： Rhidden+1 → R1	⑶
i
This design is highly expressive. However, it may also require strong regularization, since the
model could easily overfit. Recall that the TPP likelihood is P log λ* (ti) - R0T λ* (t). The model in
Equation 3 could drive the likelihood to infinity by having extremely high intensity when an event
is going to happen and 0 intensity elsewhere.
The third option is a balanced one: we assume all events to share the same influence function,
meanwhile, allow the magnitude of influence to vary based on the hidden state, hence the model
AutoInt-RNN Process with same influence:
λ*(t)= μ + X gNN(hi)/Nn(t- ti),	fNN : R1 → R1,	gNN : Rhidden → R1	(4)
i
This setting provides additional regularization compared to the former one .
3.2	Reassembling of the Gradient networks
The composition of the gradient network that shares the parameter with the integral network can be
implemented by recursion according to the chain rule.
Here we give a brief example of how AutoInt’s reassembling works. Consider
FNN (x) = W2 sin(W1x),
the gradient network is
/nn(x):= FNN(x) = W2 cos(W1x) ∙ W1
3.3	Imposing the Non-negativity Constraint
The core idea of automatic integration is to construct the integral network first, and then reassemble
the computational graph to represent the integrant. Since the integrant network comes from the
reassembling, one cannot add a final layer to it. Nonetheless, state-of-the-art models usually enforce
intensity’s non-negativity by having a final exponential or softplus layer (Du et al., 2016; Mei &
Eisner, 2016; Zuo et al., 2020).
However, we have full control over the construction of the integral network. The intensity function
would be non-negative if we constrain the integral network to be monotonically increasing. The
monotonic neural network has been a well-researched topic for improving the model’s robustness to
adversarial attacks. There are two groups of approaches for enforcing network monotonicity (Liu
et al., 2020): 1. guarantee monotonicity by construction and 2. use certain heuristics to detect
non-monotonic part of the network (e.g. negative gradient) and include in the loss function.
Since when there are negative intensities, the temporal point process model is ill-defined, we have
to guarantee monotonicity by construction, which is usually done by constraining all signs of the
linear layers’ weights to be positive and having a monotonic activation function. The weight sign
constraints are usually done by either using an elementwise exponential transformation (Sill, 1998)
or a projected stochastic gradient descent (setting all weights to positive after each step) (Chorowski
& Zurada, 2014). The monotonicity of the activation function means sine function normally used
as AutoInt’s activation function is no longer applicable (Lindell et al., 2021). Since automatic inte-
gration involves the gradient network, we would like the activation function to be infinitely differ-
entiable; sine function is thus a good choice in normal settings.
We empirically determined that for the task of fitting a non-negative integrant network with a func-
tion and obtain the function’s definite integral, the projected stochastic gradient descent converges
to the ground truth stabler than the exponential transformation method. Also, tanh activation has
similar performance with sine activation, as also indicated by Parascandolo et al. (2016).
4
Under review as a conference paper at ICLR 2022
3.4	Loss Function
Given the constructed monotonic intensity integral network FNN, the influence function is fNN =
∂fNN
-obta obtained from reassembling the computational graph of FNN. The log-likelihood of an event
∂t
sequence {t1, ..., tN} observed in time interval [0, T] with respect to the model is
N	i-1	N
L({t1,...,tN}) =Xlog XfNN(ti-tj)	+ X[FNN(T - ti) - FNN(0)]
i=1	j=1	i=1
4 Experiments
Through AutoInt can be directly implemented by Pytorch, it would suffer additional performance
penalties (Lindell et al., 2021), so we implement our version of AutoInt inheriting the Pytorch’s
sequential network.
4.1	Datasets
In the Introduction section, we mentioned the traditional neural point process’s drawback in cap-
turing the dynamics of the point process governed by multimodal or non-smooth current influence
function. Here, we simulate three complicated synthetic datasets using Ogata’s thinning algorithm
(Chen, 2016). The first is the Shaky Hawkes process, characterized by the conditional intensity
function
N
λ*(t) = μ + α ^X cos((t — ti) + 1) exp(-β(t — ti))
i=1
where μ, α, and β are scalar parameters that follow the same definition as in an ordinary Hawkes
process. Compared to a regular Hawkes process, the Shaky Hawkes process assumes each event’s
influence function exhibits a cyclic pattern, making it multimodal when no event arrives in a period
of time. The second is the Shift Hawkes process, characterized by
N
λ*(t) = μ + α X 1(t - ti > γ)exp(-β(t - t - Y))
i=1
where γ is a scalar threshold value. The shift Hawkes process assumes the influence of each event
arrival to be delayed for time γ . The intensity function therefore may have discontinuity between
events. The last is the Delay Peak process, characterized by
N
λ*(t) = μ + α ^XReLu(-(β(t - ti) - 1)2 + 1)
i=1
Delay Peak process assumes events’ influence at arrival is 0. Then the influence gradually emerges
like climbing a hill, and finally decreases like going down a hill. The current influence features no
change when an event arrives and non-smoothness when it fully disappears. We also include a real-
world dataset, earthquake JP, that include the time of all earthquakes in Japan from 1990 to 2020
with magnitude of at least 2.5, gathered by Chen et al. (2020).
4.2	Baselines
For validating our model’s capability of accurately capture functions, in addition to the negative log-
likelihood, we include each model’s learned intensity’s mean absolute percentage error with respect
to the ground truth intensity.
We have two groups of baselines: the first group follows the same model as our setting 1 (that
is, to assume all events’ influence function is of the same form) but applies different numerical
integration techniques. This group of baselines include Liu (2020)’s Taylor approximation method
A.1, the Clenshaw-Curtis quadrature method ?? and the Monte Carlo integration method. The first
two methods approximate the neural network with orthogonal polynomials (Taylor and Chebyshev
5
Under review as a conference paper at ICLR 2022
polynomials) and then rely on the easy integrability of polynomials. The third one is the most
common method used for modeling the temporal point process.
In the second group, we include state-of-the-art methods with outstanding performance, including
RMTPP (Du et al., 2016), Neural-Hawkes (Mei & Eisner, 2016) and Transformer Hawkes (Zuo
et al., 2020). Additionally, Mozer et al. (2017) has proposed a continuous-time GRU that interpolates
hidden states between events. Its logic is pretty much the same as Neural-Hawkes’s continuous-
time LSTM. So given that Neural-Hawkes is the only baseline that interpolates hidden states, we
additionally include a CT-GRU variant of Neural-Hawkes to increase the diversity of our baselines.
RMTPP is proposed quite early; its current influence function is an exponential decay function and
the decay rate is a scalar parameter across all sequences. In order to have a fairer comparison, we
added a revised version of RMTPP that has its decay rate being also dependent on the hidden states.
We also keep the original RMTPP and call it RMTPP original. Most baseline methods originally use
a linear mapping from the hidden state to the intensity function. Given that our inter-event dynamic
is complicated, we added ”deep mapping” variants of CT-GRU, CT-LSTM Neural-Hawkes, and
Transformer Hawkes, which has a three-layer MLP mapping from the hidden states to the intensity
function. By adding all these variants, we believe we have a fair coverage of possible TPP baselines.
In the second group, we include state-of-the-art methods with outstanding performance, including
RMTPP (Du et al., 2016), neural-Hawkes (Mei & Eisner, 2016) and Transformer Hawkes (Zuo et al.,
2020).
6
Under review as a conference paper at ICLR 2022
Comparing the Training Efficiency of Models
5000
■ training time (seconds)
3750 —
-jeəuzjsə*MeH」8E」。JSUeIi=
dsəo S8*Λ∕VeH -IelIUO-SUEJJ.
ωddɪ」。>J.
βs-≡,o ddLIΛIH
ddl!ΛIH
ωddɪOJBO φluow
」eaun sθ*Me工 WIS-1」。
deəo S$/WeH HlSl-Io
3u∑jsə*MEH ⊃tto⅛
dəsd sə*MeH CIHEhLO
ωddɪ>φ⅞⅛δ
ddl ΠB8，王 Oln‹
ω告 ∩Hqluoln<
ωddl luolπv
Model	shakyHawkes shiftHawkes	decayPeak earthquakesJP
	MAPE	LL	MAPE	LL	MAPE	LL	LL
AutoInt TPP SI	0.1843	-35.3762	0.0356	-39.3599	0.0373	-41.9944	8.6187
AutoInt-GRU TPP SI	0.3353	-37.9182	0.4675	-44.0076	0.1107	-42.3124	7.7730
AutoInt-GRU TPP	0.6435	-64.7072	0.4522	-58.4046	0.4100	-65.0632	-2.6385
Chebyshev TPP SI	0.2197	-35.5183	0.0541	-39.4831	17.4032	-451.6057	8.4299
CT-GRU Hawkes Deep	0.2432	-36.3658	0.1235	-39.6273	0.0708	-42.1338	8.6817
CT-GRU Hawkes Linear	0.2243	-35.6063	0.1262	-39.7173	0.1103	-42.1959	5.9148
CT-LSTM Hawkes Deep	0.1938	-35.2895	0.1381	-39.8087	0.1623	-42.6656	9.7533
CT-LSTM Hawkes Linear	0.2168	-35.4043	0.1473	-40.0411	0.1468	-42.5548	7.0620
Monte Carlo TPP SI	0.1935	-35.6090	0.0462	-39.3527	0.0378	-41.9868	8.1906
RMTPP	0.2216	-35.4175	0.1515	-39.8077	0.1660	-42.5876	4.8615
RMTPP Original	0.2562	-35.6549	0.2630	-39.7893	0.2183	-42.7965	7.7663
Taylor TPP SI	0.2004	-35.3771	0.0999	-39.7062	0.4674	-45.4099	7.7142
Transformer Hawkes Deep	0.2639	-36.3195	0.2105	-40.2917	0.1864	-42.9732	3.8893
Transformer Hawkes Linear	0.2812	-36.1831	0.2316	-40.6717	0.2342	-43.3308	6.0588
References
Ricky TQ Chen, Yulia Rubanova, Jesse Bettencourt, and David Duvenaud. Neural ordinary dif-
ferential equations. In Proceedings of the 32nd International Conference on Neural Information
Processing Systems, pp. 6572-6583, 2018.
Ricky TQ Chen, Brandon Amos, and Maximilian Nickel. Neural spatio-temporal point processes.
arXiv preprint arXiv:2011.04583, 2020.
Yuanda Chen. Thinning algorithms for simulating point processes. Florida State University, Talla-
hassee, FL, 2016.
Jan Chorowski and Jacek M Zurada. Learning understandable neural networks with nonnegative
weight constraints. IEEE transactions on neural networks and learning systems, 26(1):62-69,
2014.
Daryl J Daley and David Vere-Jones. An introduction to the theory of point processes: volume II:
general theory and structure. Springer Science & Business Media, 2007.
Philip J Davis and Philip Rabinowitz. Methods of numerical integration. Courier Corporation, 2007.
Nan Du, Hanjun Dai, Rakshit Trivedi, Utkarsh Upadhyay, Manuel Gomez-Rodriguez, and Le Song.
Recurrent marked temporal point processes: Embedding event history to vector. In Proceedings
7
Under review as a conference paper at ICLR 2022
of the 22nd ACM SIGKDD international conference on knowledge discovery and data mining,
pp.1555-1564, 2016.
SePP Hochreiter and Jurgen Schmidhuber. Long short-term memory. Neural computation, 9(8):
1735-1780, 1997.
Haibin Li, Yangtian Li, and Shangjie Li. Dual neural network method for solving multiple definite
integrals. Neural computation, 31(1):208-232, 2019.
David B Lindell, Julien NP Martel, and Gordon Wetzstein. Autoint: Automatic integration for fast
neural volume rendering. In Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition, pp. 14556-14565, 2021.
Keqin Liu. Automatic integration. arXiv e-prints, pp. arXiv-2006, 2020.
Xingchao Liu, Xing Han, Na Zhang, and Qiang Liu. Certified monotonic neural networks. arXiv
preprint arXiv:2011.10219, 2020.
Hongyuan Mei and Jason Eisner. The neural hawkes process: A neurally self-modulating multivari-
ate point process. arXiv preprint arXiv:1612.09328, 2016.
Michael C Mozer, Denis Kazakov, and Robert V Lindsey. Discrete event, continuous time rnns.
arXiv preprint arXiv:1710.04110, 2017.
Giambattista Parascandolo, Heikki Huttunen, and Tuomas Virtanen. Taming the waves: sine as
activation function in deep neural networks. 2016.
Robert H Risch. The problem of integration in finite terms. Transactions of the American Mathe-
matical Society, 139:167-189, 1969.
Oleksandr Shchur, Marin Bilos, and Stephan Gunnemann. Intensity-free learning of temporal point
processes. arXiv preprint arXiv:1909.12127, 2019.
Joseph Sill. Monotonic networks. 1998.
Gregory H Teichert, AR Natarajan, A Van der Ven, and Krishna Garikipati. Machine learning
materials physics: Integrable deep neural networks enable scale bridging by learning free energy
functions. Computer Methods in Applied Mechanics and Engineering, 353:201-216, 2019.
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,
Eukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Advances in neural information
processing systems, pp. 5998-6008, 2017.
Simiao Zuo, Haoming Jiang, Zichong Li, Tuo Zhao, and Hongyuan Zha. Transformer hawkes
process. In International Conference on Machine Learning, pp. 11692-11702. PMLR, 2020.
A Appendix
A.1 Taylor Automatic Integration
Let β1, . . . , β5 be real constants with β1 6= 0. If the real numbers A1, ..., A5 are given by
4β2A5
—--
βι ,
-中-3 停 + β )A5,
-中Y管+ β”4十
β2A2	β3 A3	β4A4	β5A5
F	β1	β1	βΓ
8
Under review as a conference paper at ICLR 2022
and y1(c), ..., y5(c) are given by evaluation of function and its derivatives
f(zc} + βfc} ε + 1 2f 0(c) + 21! β2f ⑵(C)卜2
yo	yι	|	V	}
y2
+ nβ3f0(c) + β1β2f(2) (c) +
'------------{----------}
y3
+ 3!β3f(3) (C) } ε3 + {β4f O(C) + (β1β3 + 2β2) f ⑵(C) + 2β2β2f(3)(C) +
I-------------/ I--------------------------------------------------/
{z
y3
{z
y4
+ 4β4f ⑷(c)} ε4
|------V-----}
y4
+ nβ5f0(C) + (β1 β4 + β2β3) f(2) (C) +
'-----------------------------------}
y5
+ 2 (β2β3 + β1β2) f ⑶(C) + 6β3β2f (4)(c) + 5!β5f (5)(c)卜5
'-------------------------------------------------------}
y5
Then function Rab f(x) can be approximated by Pk Rab yk (x)Ak.
A.2 Chebyshev Automatic Integration
We are going to decompose a function f(x) to the sum of the Chebyshev polynomials Tn(x), which
follows the recurrence relationship
T0(x) = 1, T1(x) = x, Tn+1(x) = 2xTn(x) - Tn-1(x)
or generally,
Tn(x) = cos n cos-1 x
The roots of the Chebyshev polynomials are Chebyshev nodes, where
Xk = Cos 22k-1 ∏
2n
k= 1,...,n
We can use affine transformation to map the integrant from x ∈ [a, b] to x ∈ [-1, 1] and evaluate
(b- a) P R-11 Tn (x) as an estimator of Rab f (x). Consider the average of function evaluated at
nodes, y = f (Xk). The Chebyshev coefficients are
c0 = K^ X f (Xk ),Cn|n=0 = K X Tn(Xk )f(xk ),
where Xk are the Chebyshev nodes. The integral of a Chebyshev polynomial is
Tn(X)dX =	Tn(cos θ)d cos θ
= -	cos(nθ) sin θdθ
=—ɪ J(sin((n + 1)θ) — sin((n — 1)θ))dθ
1
2
cos((n + 1)θ)	cos((n - 1)θ)
n+1
+ const.
n 1
4 T Tn+1 (X) — TnT(X)) + const
2 V n + 1 n — 1
(5)
(6)
(7)
(8)
(9)
9