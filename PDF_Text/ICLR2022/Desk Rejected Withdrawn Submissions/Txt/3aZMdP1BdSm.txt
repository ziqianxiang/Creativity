Under review as a conference paper at ICLR 2022
Identifying Interactions among Categorical
Predictors with Monte-Carlo Tree Search
Anonymous authors
Paper under double-blind review
Ab stract
Identifying interpretable interactions among categorical predictors for predictive
modeling is crucial in various research fields. Recent studies have examined inter-
pretable interactions using decision tree (DT) learning methods, which construct
DTs by greedy rules due to the high memory and time complexity of building
and evaluating DTs, resulting in a local optimal solution. This paper formulates
the selection of quadratic and higher order interactive terms into a LASSO prob-
lem and then relaxes it into multiple DT learning problems. A Monte Carlo Tree
Search-based interaction selection (MCTs-IS) method is proposed to identify the
optimal DT in an online learning manner. A DT pruning strategy is developed
based on LASSO that can easily be applied to MCTs. We prove that MCTs-IS
converges with high probability to the optimal solution of the DT learning prob-
lem. Extensive experiments have been conducted to demonstrate the effectiveness
of the proposed algorithm on real-world datasets.
1	Introduction
n-way interaction among categorical predictors occurs when values of n categorical predictors de-
pend on each other, or n categorical predictors affect the variability of the response variables in a
nonlinear form (Lian et al., 2018). Understanding interactions provides domain knowledge about
how the predictors function together to vary the target. Nevertheless, even a small set of categorical
predictors can generate a large number of candidate interactions because of the combinatorial ex-
plosion, i.e., n binary features have 2n possible n-way interactions. For high-dimensional features,
considering high-way interactions dramatically increases the number of trainable parameters in a
predictive model, causing the curse of dimensionality.
Nonlinearity models such as Factorization Machines (FM) (Rendle, 2010; Sun et al., 2021; Pande,
2020) and Deep Neural Networks (DNNs) (Cheng et al., 2016; Qu et al., 2016; Guo et al., 2017; Lian
et al., 2018; Tao et al., 2020; Liu et al., 2020) have been applied to map predictors and/or weights
to a low-dimensional space to avoid the curse of dimensionality, which can also be used to compute
interactions and predict response variables. The nonlinear dimensionality reduction, however, used
in these approaches does not ensure convergence, and as a consequence, does not generate stable
low-dimensional representations and cannot identify critical interactions from candidates. These are
critical considerations for research areas such as ad-click recommendation (Tsang et al., 2020) and
AI gaming techniques (Silver et al., 2016).
Interactive terms can be automatically selected by taking candidate terms as input features and im-
posing a regularizer to penalize models that use many interactive terms. The regularizer helps elim-
inate redundant interactions from the models (Bien et al., 2013; Lim & Hastie, 2015; Vaughan et al.,
2020). However, these methods can only exclude terms from a pre-given set of interactive terms
rather than discover new interactions. Because DNNs are capable of extracting critical features
from raw data, black-box interaction detection algorithms (Tsang et al., 2017; 2020) identify inter-
active terms based on the trained network parameters with heuristic rules. The high nonlinearity
of DNNs, however, makes it difficult to interpret the identified interactions even though they are
explicitly modeled. Besides the lack of interpretability, random exploration is necessary to find a
better local minimizer for the nonconvex loss function, which then leads to unstable output.
The tree-based interaction selection methods can effectively model categorical variables and their
interactions with decision trees (DTs), and have received increasing attention in recent years.
1
Under review as a conference paper at ICLR 2022
(Sorokina et al., 2008; He et al., 2014; Luo et al., 2019; Bao et al., 2020). Instead of exclud-
ing interactions from pre-given ones, tree based methods explore new interactions by identifying
higher-order interactions based on lower-order ones. Their convergence guarantees ensure stable
solutions. Furthermore, features’ candidate values appearing together in a traversal path of DT are
interacting with one another, since the condition of a child node is predicated on the condition of the
parent node. Therefore, interaction identified by DTs can be easily interpreted (He et al., 2014; Bao
et al., 2020). These methods are NP-complete to learn the optimal DT (Laurent & Rivest, 1976).
Therefore, DT construction algorithms rely on heuristics such as the greedy algorithm and are not
guaranteed to yield the global optimal DT (Kretowski & Grzes, 2005).
Contributions. We propose a Monte-carlo tree search based interaction selection (MCTs-IS)
method to select interactions among categorical predictors. Our main contributions are summarized
as follows.
We formulate the high-way interaction selection problem with the safe-screen Least Absolute
Shrinkage and Selection Operator (safe-screen LASSO) (Xiang et al., 2016), which identifies a
subset of features receiving zero weights in the vanilla LASSO solution using significantly less
computing and memory resources. The safe-screen LASSO problem is then relaxed into several
DT learning problems, and a DT pruning strategy is derived from the screening criteria for the safe-
screen LASSO. As pruning strategies from other methods are usually derived from local optimal DT
construction methods, interactions that occur in the global optimal solution may be discarded. By
contrast, our approach eliminates interactions within pruned DT’s nodes in the optimal solution of
the vanilla LASSO, which does not produce a local optimal solution.
We propose MCTs-IS to learn the optimal DT in each DT learning problem. In MCTs-IS, Monte
Carlo Tree search (MCTs) (Kocsis et al., 2006), a framework approximating the optimal choices
in exponentially large search spaces, is applied to search the optimal DT iteratively in a space-
and computation- efficient manner. Specifically, MCTs-IS is trained iteratively on data instances
presented one-by-one in an online feature selection style, and in each iteration, only a subset of
features in the data instance is used to update the model parameters. In this way, MCTS-IS can
update model by accessing and saving a limited number of features. Finally, we give the proof
showing that with enough iterations, MCTs-IS converges to the optimal DT with high probability.
Extensive experiments are performed on datasets with large samples (from 〜1.9M to 〜40M) to
confirm the convergence of selected interactions of MCTs-IS. We then show that benefiting from
the safe-screen LASSO and MCTs, MCTs-IS can be efficiently trained, and the performance of
predictive models can be improved with the help of the selected interactions.
Related Work. To deal with the large number of possible interactions, penalization methods, such
as Hierarchical LASSO (Bien et al., 2013; Vaughan et al., 2020) and Group-LASSO (Lim & Hastie,
2015), have been proposed for selecting interactions based on hierarchy assumptions, which remove
high-order interactions when the related low-order interactions have been eliminated. FM-based
methods differ from penalization methods in that they fit all candidate interactions with fewer pa-
rameters (Rendle, 2010; Pande, 2020; Sun et al., 2021). In practice, however, these methods usually
consider two or three-way interactions due to the complexity of identifying high-way interactions.
DNNs have shown great success as powerful tools for obtaining feature representations in learn-
ing high-way interactions. Interaction selection methods such as Wide&Deep (Cheng et al., 2016),
Product-Based Neural Networks (PNN) (Qu et al., 2016), Factorization-Machine based neural net-
work (DeepFM) (Guo et al., 2017) and AutoDeepFM(Liu et al., 2020) fit interactions in an implicit
fashion, where the formats of the learned interactions are not clear. As a way of learning explicit
DNN interactions, eXtreme Deep Factorization Machine (xDeepFM) (Lian et al., 2018) models the
interactions based on DNNs with specifically designed architectures. Rather than identify specific
interactions, Neural Interaction Detection (NID) (Tsang et al., 2017) and Global Interaction Detec-
tion and Encoding for Recommendation (GLIDER) (Tsang et al., 2020) detect predictors interacting
with each other by checking for non-additive effects in the non-linear activation functions of DNNs.
In addition to DNNs, tree-based methods are also effective in identifying high-way interactions.
They model interactions using DTs, which are then selected according to various tree construc-
tion methods including gradient boosting decision tree (GBDT) (He et al., 2014), classification and
regression tree (CART) (Sorokina et al., 2008), and the successive mini-batch gradient descent (SM-
2
Under review as a conference paper at ICLR 2022
Figure 1: Examples of (a) DT and (b) FDT.
BGD) (Luo et al., 2019). Recently, the tree-based method has been successfully fused with DNNs
in ResFusion to improve the performance(Bao et al., 2020).
Organization. The remainder of this paper is structured as follows. In Section 2, the interaction
selection problem is formulated with LASSO and then relaxed to several DT construction problems.
MCTs-IS is described in Section 3 to search the optimal DT, followed by a theoretical analysis
section, where the convergence of MCTs-IS is discussed. In Section 5 we present experimental
results and conclude the paper with a brief discussion of the future work in the last section.
2	The Problem Formulation
In this section, we first show how to relax the LASSO-based interaction selection problem into DT
learning problems. Then we derive the divide-and-conquer approach to obtain the optimal DT in
each DT learning problem. All proofs of theorems in this section are provided in Appendix A.
2.1	Selecting interactions with DTs
We assume that each (x, y) in dataset D is sampled from an unknown but fixed distribution, where
y ∈ R+ is the response value, and x = [x1, x2, ..., xq] is a vector of categorical predictors. Without
loss of generality, we denote the candidate values of the ith predictor of x are integers ranging from
1 to Ki . Before introducing the LASSO problem, we first describe how DTs model interactions.
Definition 1 (DT). Fig. 1a describes a DT, whose nodes in odd depths are called data nodes, while
those in even depths are called feature nodes. Each data node nd stores a dataset Snd. The root
node is a data node, and the depth of the root node nr is 1, thus Snr = D. Each data node nd has
at most one feature node nif, whose index i ∈ {1, 2, ..., q} is unique among all feature nodes along
the path from DT’s root node to nd. Each feature node divides its parent node’s dataset according
to the ith predictor of x, and then saves these sets in its Ki child data nodes. The jth child node
nid,j contains data Snd whose ith categorical predictor equals j. In the following section, we omit
writing the subscript of nid,j and nif if no confusion can arise.
For a DT T , the leaf nodes are indexed by 1 to lT , where lT is the total number of the leaf nodes
in T. According to Definition 1, each (x, y) ∈ D can be assigned to one and only one leaf node,
whose index is marked as lxT . In DTs, predictors that appear in a traversal path interact with one
another. Let us take the DT in Fig. 1a as an example. The leaf node marked (1,0,0) stands for
the interaction between age=old and gender=female. Since each leaf node represents an interaction
among predictors, T can be regarded as a model to traverse specific predictor interactions. An
interaction can be expressed by a one-hot encoding vector, consisting of 0s in all lT cells except for
a single leaf node in the lxT cell.
Assume there are totally NT trees that are used to transfer a data point x into NT one-hot
encoding vectors, i.e., xT1 , xT2 , ..., xTNT . To exam the performance of the transformed inter-
actions, we first model the relationship between labels and interactions with a linear model
y = PiN=T1 PljT=i1 wjTi xjTi + (x,y), where xjTi ∈ {0, 1} is the jth element of xTi, wjTi is the
3
Under review as a conference paper at ICLR 2022
trainable weights of xjTi , and (x,y) is the error variable adding noise to the linear relationship.
LASSO is then used to fit the model to select meaningful interactions, since weights of unim-
portant interactions will be set as zero, and thus be discarded in LASSO. Rather than minimizing
EPNT PlTi	Ti Ti 2	PNT PlTi	Ti
(x,y)〜D(y	i=i=i	/	jj-l Wj	xj	) +	i jj - 1)j=1 lwj	| directly,	as	shown in Eq. 1, We present
the LASSO in the safe-screen style (Xiang et al., 2016), which allows a subset of features receiving
zero weights to be identified before minimizing the loss function, as shown in Theorem 1.
W0Ti = WjTi j ∈ WJTi ,	= WjTi
j 0 otherwise,	0
NT lTi
min L(w, T1, T2, ..., TNT) = min Ls(w, T1, T2, ..., TNT) + XX |wj0Ti |,
ww
i-1 j-1
NT lTi
s.t. Ls (w, T1,T2,…,TNτ ) = E(χ,y)〜D (y - XXwj0TixjTi)2
i-1 j-1	(1)
0Ti	00Ti
j ∈ WJ i ∪ WJ i,
otherwise,
W0τi = {i∣ΕxTiy + CF VzExTi > λ,j ∈ [lTi]},
WJ00Ti = {j|pa(nlTi,j0) =pa(nlTi,j),j0 ∈ WJ0Ti, j ∈ [lTi]},
where W is the set of all trainable weights in L, and λ is a positive real number measuring the degree
of regularization, CF is a positive real number, and nT,j denotes the jth leaf node of Ti and ρa(∙)
the parent node. To avoid W = 0, we have λ ∈ [0, λmax], where λmax = maxi,j (|ExjTi y|).
The safe-screen LASSO shown in Eq. 1 can be regarded as a metric to evaluate the quality of in-
teractions transformed by the set of DTs. By solving argminT1,...,TN minwL(W, T1, T2, ..., TNT),
the optimal set of DTs (or interaction transformers) can be identified. In Eq. 1, the index set WJTi is
calculated, and all weights with indices that do not belong to WJTi are set to zeros in L prior to the
minimization of L.
Assumption 1. 0 <	≤ (1----/ CF	D) < L
λ	λmax _ V	√Ε(x,y)〜Dy2/
Theorem 1.	Under assumption 1, let {w*,T1, w*,T2 ,…，w*,TNT } be the solution of the correspond-
ing LASSO problem ofEq. 1, if j ∈ WT, w；Ti = 0.
Remark 1. For each interaction feature xjTi, ify ∈ {0, 1}, the left-hand side of the screen criterion
0T	T	T
in WJTi is a convex combination between xjTi ’s true positive rate (TPR) and frequency. If xjTi has
low TPR and frequency, its weight is more likely to equal zero in Eq. 1.
Remark 2. Let Ti0 be the DT after pruning all Ti ’s leaf nodes whose indices are not included in WJTi,
and removing parent nodes of these leaf nodes. By including WJ00Ti into WJTi, ∀i ∈ [1, 2, ..., NT], xTi0
will always be a one-hot encoding vector, which can simplify the following problem formulation.
Even though the safe-screen LASSO reduce the simplify the optimization problem by discarding a
set of zero-weighted features before minimizing the loss function, the optimization problem in Eq. 1
does not have a closed-form solution, which requires a computationally expensive numerical method
to minimize the loss function. To further simplify the calculation, after setting weights whose indices
are not included in WJTi as zero, we ignore the l1 penalty, and relax the the minimization of L into
minimizing Ls's upper bound function Ls, as shown in Eq. 2.
1 NT	j NT	lTi
mwnLs(W,T1,T2,…,Tnt) = mwn — XLsri(WTi) = mWn — XΕ(χ,y)〜D(y - XWjjriXTi)2
T i-1	T i-1	j-1
(2)
It is possible to easily minimize LsTi, ∀i ∈ {1, 2,…，NT} independently in Eq. 2, as shown in
Theorem 2.
4
Under review as a conference paper at ICLR 2022
Theorem 2.	Let w*,Ti = argmi□wTi Ls Z(WT) and LT = min’wTi Ls i (WTi). With the event
Ex,Ti,j = {x is assigned to Ti ’s jth leaf node}, we have
LTi= X P(Eχ,Ti,j)E[(y-E[y∣Eχ,Ti,j])2IEχ,Ti,j] + X	P(Ex,Ti,j)E[y2l&zj]∙⑶
j∈wTi	j∈[lTi ]∖wTi
While the minimum loss of DT can be calculated with Eq. 3, it would be infeasible to choose the
best one from a large number of candidates. Accordingly, a rule is proposed to reduce the number
of candidate DTs.
Theorem 3.	Let Ti be the candidate DT set of the ith DT Ti. With Ti0 = {T |T ∈ Ti &
{1,2,…，lTi} = wTi}, we build T- = {T|T ∈ Ti0 & T0∖{T} doesn't contain T,s sub-tree}.
∀T ∈ T, ∃T0 ∈ T- with LTo ≤ LT.
Remark 3. The proof of Theorem 3 can be made by considering two factors of Eq. 3: 1) with T ∈ T,
when we prune T ,s nodes with indices that do not belong to WT and remove their parent nodes, the
resultant T0 has a lower loss; 2) If T can be expanded to T00 by splitting one of T ’s leaf node with
a specific feature in X, and all resultant new leaf nodes belong to WT , we have LT〃 ≤ LT.
Remark 4. For ∀T ∈ Ti- and ∀(x, y) ∈ D, based on the analysis in Remark 2, x can be transferred
into a one-hot encoding feature by T.
With Ti- defined in Theorem 3, the optimal DT can be identified from Ti- instead of Ti, which
leaves the second term of Eq. 3 at zero. Therefore, for ∀i ∈ {1, 2, ..., NT}, the optimal DT can be
obtained by solving Eq. 4.
lTi
Ti* = arg min XΡ(Ex,Ti,j)E(x,y)〜D [(y - E[y∣Ex^])2∣Ex,Ti,j]∙	(4)
T∈Ti- j=1
For the sake of simplicity, the subscripts of Ti*, Ti and Ti are omitted.
2.2 Identifying the optimal DT with the divide-and-conquer approach
To efficiently obtain the optimal DT in Eq. 4, we introduce a divide-and-conquer approach to iden-
tify T* , which is more time- and space- efficient compared with separately computing loss for each
of DTs in T- . Before presenting the approach, we define a data structure FDT (Feature Involved
DT) in Definition 2 for storing DTs in T.
Definition 2 (FDT). FDT is a DT (Definition 1) except that each of its data nodes (nd) contains all
candidate feature nodes, whose indices differ from those of all feature nodes within the path from
FDT’s root node to nd.
FDT is a better space-saving alternative than storing DTs separately since only one path is saved in
the FDT if multiple DTs share the same path from the root node to any other node. From FDT, one
can derive DT by selecting at most one feature node for each of FDT’s data nodes, as shown in Fig.
1b. Although the fully expanded FDT is a huge tree, most of its nodes are not associated with DTs
in T- , and thus can be ignored in searching the optimal DT shown in Eq. 4. Theorem 4 proposes a
pruning strategy to reduce the size of the FDT, and we demonstrate that each of the DTs in T- can
be obtained from the pruned FDT.
Theorem 4.	Let TP be the FDT after pruning its feature nodes whose child data nodes all satisfy
the criterion set forth in Eq. 5 , and TP be the set of all candidate DTs obtained from TP by
selecting one and only one feature node for each ofTP ’s internal data nodes. We have TP = T-.
E(x,y)〜D y1(-X,c,y)SSnc^∖ + CF VZP((x, y) ∈ Snd ) < λ,	(5)
where P((χ,y) ∈ Snd) and E(x,y)〜D [y((x,y)∈s d∖ can befurther decomposed into Eq. 6 according
to Bayes’ theorem.
P ((x, y) ∈ Snd) = Y	P(xi =j|(x,y) ∈ Spa(pa(nid,0j))),
nid,0j ∈end \nr	(6)
E(x,y)~D [y1-(^x,yf)∈Snnd ] = E(x,y)~D [y | (x, y) ∈ Snd ]P ((x,y) ∈ Snd ),
5
Under review as a conference paper at ICLR 2022
where end is the node set containing all data nodes along the path from the FDT’s root node to nd,
and pa(pa(nid,0j )) indicates nid,0j ’s grandparent node.
The optimal DT can be identified in T P via the divide-and-conquer approach. First, we denote
TP ’s sub-tree rooted in nd as TnPd. DTs can be obtained from TnPd by selecting one and only one
child feature nodes for each of TnPd ’s data node. Let TnPd be the set of all candidate DTs obtained
from TnPd . By replacing D and T- in Eq. 4 with Snd and TnPd, we create a sub-problem Pnd of
Eq. 4, identifying the DT with the minimum loss from TnPd. It’s easy to prove that the optimal DT
identified in Pnd0 is a sub-tree of that identified in Pnd if TnPd 0 is TnPd ’s sub-tree. Therefore, if nd
is TP 's internal data node, Lnd can be calculated based on the solutions of other sub-problems, as
shown in Eq. 7.
Lnd=nfminnd)Lnf=nfminnd) d∑ P(Xi=j|(x，y)∈SndRnd,.
nid,j ∈c(nf)
(7)
If nd is TP's leaf node, TnP has only one node (i.e. nd). Therefore, We simply have Lnd =
E(x,y)〜D [(y - E(x,y)〜D [j∣(x,y) ∈ Snd])2∣(x,y) ∈ Snd ].
By recursively calculating Lnd for each of the data nodes in TP, we can finally get the minimum loss
in Eq. 4 by calculating Lnr, where n is the root node of TP. For each of the internal data nodes
in TP, only one child feature node is selected, which enables us to determine the corresponding
optimal DT with Lnr.
3 The Proposed MCTs-IS Method
Before pruning TP and obtaining the optimal DT via Eqs 5 and 7, parameters in Eqs. 6 and 7
should be obtained first. Therefore, estimators are built to estimate these parameters on the basis of
(x， y) ∈ D. Since estimating all unknown parameters with a big D is computationally intractable
because of the complexity of FDT, we propose MCTs-IS to calculate these estimators.
MCTs-IS estimates FDT by constructing FDT iteratively from scratch. Let the constructed FDT in
the tth iteration be T Estimators Pnd t, Rn t and σ^nf t are saved in data and feature nodes of Tt,
i,j,	,	,
and used to estimate P(Xi = j∣(χ,y) ∈ Spa(pa(nd.))), E[y|(x, y) ∈ S*] and Lnf in Eqs 6 and 7.
At the beginning of the MCTs-IS, Tt only has a root node n. In each iteration, three steps, selection,
backpropagation and expansion, are conducted in MCTs-IS to select a set of data and feature nodes
from Tt, update estimators of parameters in selected data nodes with the sampled data (Xt, yt), and
adding new feature and data nodes to Tt. By repeatedly adding new nodes to Tt, each of TP's node
can finally be found in Tt. The pipeline of MCTs-IS is shown in Algorihtms 1.
1) Selection. In this step, a set of TtS data nodes will be selected, which starts from its root node.
Each selected data node nd will group its child feature nodes that do not meet the pruning guide-
lines presented in Theorem 4 into a selectable feature set l(nd), and then the feature node with
the (potentially) lowest estimated Lnf will be selected from l(nd) according to the node selection
policy ∏t(∙) as described later. To estimate whether a TtS feature node nf meets the pruning cri-
terion in Theorem 4, we replace unknown parameters of the left-hand side of Eq.5 with estimators
to build Snd = Qnd0. ∈e d\n„ Bn" + CF q/"nd,t "底〔∈e，\” *l,t ∙ FolloWing Theorem 4, if for
i,j nd r	i,j,	i,j nd r	i,j,
∀nd ∈ C(nf), Snd ≤ λ, nf meets the estimated pruning criterion, and thus belongs to l(pa(nf)). For
each selected feature node nf, nf's child data node with (Xt, yt) ∈ Snd is selected. As shown in line
18 of the Algorithm 1, the selection procedure will be conducted repeatedly until the agent selects a
TtS leaf node, or a TtS data node nd with l(nd) = 0. The main challenge in selecting feature node
is to maintain the balance between exploiting fully estimated feature node and exploring those with
limited estimation, i.e. exploitation vs. exploration. Accordingly, a feature node selection policy
∏t(∙) : nd → nf is proposed in Eq. 8.
πtSd)=arg nmind) σnf L CpJ
log Pnf0 ∈l(nd) Vnf0 ,t
Vnf,t
(8)
6
Under review as a conference paper at ICLR 2022
Algorithm 1: MCT-IS
1:	Procedure MCT-IS
2:	Input: Tfs root nr , the total number of rounds tmax .
3:	t = 1
4:	repeat
5:	Sampling a data (xt, yt) from the dataset
6:	Search(nr,{nr},1,1,t,(xt, yt))
7:	t = t + 1
8:	until t > tmax
9:
10:	Function Search
11:	Input: Tt's data node nd, the set SNt, pruning strategy estimator Pyb and pb, number of
iteration t and sampled data (xt, yt).
12:	//Expansion
13:	If nd has no child node, add feature nodes to nd, and add data nodes to all added feature nodes
14:	if c(nd) = 0 or l(nd) = 0 then
15:	Return SNt
16:	else
17:	//Selection
18:	nf = ∏t(nd), nd — nf's child data node nd with (xt, yt) ∈ S那，
19:	SNt J SNt ∪ {nd0}
20:	SNt = Search(nd0, d + 1, pyb, pb, t, (xt, yt))
21:	//Back-propogation
22:	for nf0 ∈ c(nd0)\l(nd0) ∪ {nf} do
23:	nd00 J nf0’s child data node nd00 with (xt, yt) ∈ Snd00
24:	Update Vndoo,t+ι,Bndθ0,t+ι, Pnd，，,t+ι and μndoo,t+ι following section 3.2
25:	snd00 - Pybμnd00,t+1 + CF √Pbpnd00,t+1
26:	end for
27:	Update Gn,t+ι following Eq. 9, Vnf ,t+ι J Vnf ,t + 1
28:	end if
29:	Return SNt
where Vnf ,t measures how many times nf has been selected so far, σn,t is the estimator of Lnf,
whose definition will be given in the backpropagation subsection.
Remark 5. The second term of Eq. 8 considers the uncertainty of estimation as opposed to greedily
selecting the node with the lowest Gn ,t. Specifically, the more data are used to estimate the Param-
eter, the less uncertainty in estimation we may encounter. In this case, a feature node estimated by
few data can be Selected even though it has a higher Gn,t.
2)	Backpropagation. Let SNt be the set of selected data nodes in the selection step, and we further
denote SNt,i as the ith selected data node. In the step of back-propagation, the estimators within
SNt will be updated by the sampled (xt, yt). Note that the memory cost in this step is small since
all estimators can be updated incrementally. For each nid,j ∈ SNt, we have nd ,t+1 = Bnd ,t + yt,
Vndj,t+1 = Vndj,t +1, Pndj,t+1
_______nij,t+1	∙ΛX∖(↑ Γ∣ M	— Bnd,t+1
PKi V ,	and μnd,t+1 = V d
j0=1 Vnd 0,t+1	nd,t+1
i,j0
Given a T's feature
node nf, if nf ∈/ l(Pa(nf)), and estimators calculating l(Pa(nf)) are not updated in the following
iterations of MCTs-IS, nf will no longer be selected even though nf is mistakenly excluded from
l(pa(nf)). Therefore, if nf ∈ l(pa(nf)), we additionally update Vnd,t+ι, B*,t+1, Pnd ∙,t, Bnd ∙,t
and snd in each ofnf’s child node nd, as shown in line 22-26 of the Algorithm 1. By updating these
nodes, nf might be included in l(Pa(nf)) again, and then selected by MCTs-IS.
7
Under review as a conference paper at ICLR 2022
After updating Pnd t and μ^ t, for ∀SNti ∈ SNNnr, &&&加)t will be updated with Eq. 9.
i,j,	i,j,	t,
σpa(SNt,i),t+1
Vpa(SNt,i),tσpa(SNt,i),t + Lpa(SNt,i),t
Vpa(SNt,i ),t + 1
|SNt |
St Lpa(SNt,i ),t = E wSNt,i,j (yt - μSNt,i,t+1)2
j=i
(9)
wSNt,i,j
(YSNt) j
PjSNtl(YSNt)j0
υ + VSNt,|SNt | ,t+1
YSNt =----
tκ
where P|jS=Ni t| wSNt,i,j =1, υ,κ ∈ R+, and K ∈ [0,1].
Remark 6. Lpa(SNt,i),t in Eq. 9 is a weighted (measured by wSNt,i,j) mean square errors of data
nodes in {S Nt,j |j ∈ [i, |SNt|]}. When the visit time of the deepest node in SNt (i.e. SNt,|SNt|) is
small (YSNt < 1), the square errors of shallow nodes carry a larger weight than deep nodes and it
produces a stable heuristic information which guides the following iterations, since shallow nodes
are estimated by more samples than deep ones. As the visit time of the deepest node in SNt is large
enough (YSNt > 1), the deepest node in SNt has the largest weight. In Appendix B, for each of
TP 'sleafnodes, Parentfeaturenodes, we show that limt→∞ E[σnf ,t] = L*, in Lemma 3.
After updating σpa(SNGt, for each parent node of data node SNt,i in SNt, We have Vpa(SNt,外,t =
Vpa(SNt,i),t-1 + 1.
3)	Expansion. At the step of expansion, if the deepest node in SNt has no child nodes, all of its
candidate child feature nodes and the resulted data nodes will be appended to it, as shown in line 13
of Algorithm 1. When a new feature node nf is added to T σn ,t is set to 0; when a new data node
nd is added, the initial values ofVnd,t, Bnd,t and snd are set to 1, 0 and 1, respectively.
4	Theoretical Analysis
To the best of our knowledge, we are the first work to formulate the relationship between the integra-
tion selection and the MCTs, and in Theorem 5, we show that the probability obtaining the optimal
DT in MCTs-IS converges to 1. The proof of Theorem 5 can be found in Appendix A.
Theorem 5. In the tth iteration ofMCTs-IS, let TP be the FDT after pruning all Tt 飞feature nodes
nf with nf ∈ l(pα(nf)). For each of TtP ’s leafnodes，Parentfeature node nf , if Lf is estimated
by 6" t, and for each TtP's internal data node nd, Lnd is calculated on the basis of the estimated
Lnf0, a DT Tt can be obtaιnedfrom TP, and we have limt→∞ P(Tt = T ) = 1.
5	Experiments
In this section, extensive experiments were performed to confirm the performance of interactions
identified by MCTs-IS in boosting the predictive results of machine learning models.
Datasets. We evaluated MCTs-IS and baseline algorithms on Avazu 1 and TYGEM 2. Avazu is
a Click-through rate (CTR) dataset widely used for bench-marking interaction identification algo-
rithms. We processed Avazu dataset following Sun et al. (2021)’s pipeline. After data processing,
each of 〜40M samples in Avazu dataset has 23 categorical fields, which can be further transferred
into 〜1.54M binary features. Although interactions exists in the dataset of Avazu, most of its fea-
tures are anonymized, which makes it hard to interpret the identified interactions. Therefore, we
further evaluated algorithms with records of matches played on the online Go game platform called
TYGEM. High order interactions are important in playing the game of Go, since moves of both real
and AI Go players are heavily relied on interactions among previous moves (also called Shapes and
Connections). Following the data preprocessing pipeline in Appendex B, we extract a binary classi-
fication dataset from TYGEM, which has 〜1.9M records, and each record has 48 ternary features.
1https://www.kaggle.com/c/avazu-ctr-prediction/data
2https://github.com/yenw/computer-go-dataset
8
Under review as a conference paper at ICLR 2022
Dataset	Model	Interaction selection method					
		N/A			+GBDT			+MCTs-IS(ours)	
		Test AUC	Test Loss	Test AUC	Test Loss	Test AUC	Test Loss
Avazu	TR	-0.750	0.396	-0.756	-0.394	^0765	-0.388
	TM	-0.770	0.386	-0.768	-0.387	-0.771	-0.385
	FmFM	-0.778	0.382	-0.773	-0.392	-0.779	-0.383
TYGEM	-LR	-0.658	0.556	^0786	-0.455	-0.794	-0.449
	TM	-0.795	0.468	-0.788	-0.452	-0.797	-0.446
	FmFM	0.8088	0.457	0.786	0.455	0.8087	0.439
Table 1: Performance comparison
Both Avazu and TYGEM datasets were randomly split into 8:1:1 as the training set, validation set,
and test set.
Baseline Algorithms. After transferring features in Avazu and TYGEM into high-order interac-
tions with MCTs-IS and the Gradient Boosting Decision Tree (GBDT) algorithm, a widely used
DT-based interaction identification method, we evaluated the performance of these high-order inter-
actions in boosting the AUC and log loss of shallow models, i.e. Logistic regression (LR), FM and
Field-matrixed Factorization Machines (FmFM), the state-of-the-art shallow method in predicting
the Avazu dataset. Among these baseline algorithms, GBDT is implemented with LightGBM (Ke
et al., 2017), a gradient boosting framework that uses tree based learning algorithms, while LR, FM
and FmFM were trained and tested with the open-source training code of FmFM (Sun et al., 2021)
implemented on the basis of the TensorFlow(Abadi et al., 2016).
Experimental Methods. We randomly sampled 1M records from the training set to train both
the MCTs-IS and the GBDT. With the fitted DT-based methods, all features in the train, valid and
test sets were transferred into interactions. After that, LR, FM and FmFM were used to predict
labels with these interactions under different hyper-parameter setting (more details can be found in
Appendix B) and optimizers (SGD with momentum and Adam). After selecting the hyper-parameter
setting with the highest AUC (Area Under the ROC Curve) on the validation set, the AUC and the
log loss on the test set were used to evaluate performances of baseline algorithms. We implement
all the algorithms on a server equipped with Intel Xeon Gold 6150 2.7GHz CPU, 192GB RAM, and
an NVIDIA Tesla V100 GPU. The result is shown in Table 5.
Result and Discussion. In Table 5, we have four observations. 1) Compared with reported results
in the paper proposing FmFM, we got better results in the Avazu dataset because instead of Adam,
we use SGD with momentum to solve FM and FmFM, which usually have a better performance
than Adam in non-convex optimization. 2) LR+MCTs-IS, FM+MCTs-IS and FmFM+MCTs-IS had
better predictive results than those with GBDT. 3) Although high-order interactions can be identified
by MCTs-IS and GBDT, they failed to boost the performance of FM and FmFM, which considered
all pairwise interactions. It indicates that the second-order feature interactions plays more important
roles in Avazu dataset than high-order interactions since only a subset of two-way interactions can
be identified by DTs. 4) In the Go game dataset TYGEM, high order interactions are critical for
both real and AI players to learn how to play the Go game. Therefore, FmFM+MCTs-IS achieved
much lower loss compared with FmFM, which means that by learning knowledge patterns in Go
games visualized in Appendix B, FmFM got more confidence in the label prediction.
Conclusions and Future Work
In this work, we propose MCTs-IS to identify high-way interactions in an online learning manner
based on the framework of MCTs. A new pruning strategy is derived based on the safe-screen
LASSO modeling the interaction selection problem to reduce the search space. We also provide
the theoretical analysis of the convergence on identifying interactions with the MCTs framework.
In the future, as an extensible framework, the MCTs in the MCTs-IS may be enhanced by various
technologies, including parallel computing, expert knowledge, and DNNs. Additionally, we are
interested in finding a low dimensional representation of the identified sparse high-way interactions,
so that they can be applied to deep learning models without over-fitting.
9
Under review as a conference paper at ICLR 2022
References
Martin Abadi, Ashish Agarwal, Paul Barham, Eugene Brevdo, Zhifeng Chen, Craig Citro, Greg S
Corrado, Andy Davis, Jeffrey Dean, Matthieu Devin, et al. Tensorflow: Large-scale machine
learning on heterogeneous distributed systems. arXiv Preprint arXiv:1603.04467, 2016.
Junmei Bao, Yangguang Ji, Yonghui Yang, Le Wu, and Ruiji Fu. Resfusion: A residual learning
based fusion framework for ctr prediction. In China Conference on Information Retrieval, ρρ.
29-41. Springer, 2020.
Jacob Bien, Jonathan Taylor, and Robert Tibshirani. A lasso for hierarchical interactions. AnnaIs of
statistics,41(3):1111,2013.
Heng-Tze Cheng, Levent Koc, Jeremiah Harmsen, Tal Shaked, Tushar Chandra, Hrishi Aradhye,
Glen Anderson, Greg Corrado, Wei Chai, Mustafa Ispir, et al. Wide & deep learning for recom-
mender systems. In Proceedings of the 1st WOrkshOP on deep Iearning for recommender systems,
pp. 7-10, 2016.
Huifeng Guo, Ruiming Tang, Yunming Ye, Zhenguo Li, and Xiuqiang He. Deepfm: a factorization-
machine based neural network for ctr prediction. arXiv PrePrint arXiv:1703.04247, 2017.
Xinran He, Junfeng Pan, Ou Jin, Tianbing Xu, Bo Liu, Tao Xu, Yanxin Shi, Antoine Atallah, Ralf
Herbrich, Stuart Bowers, et al. Practical lessons from predicting clicks on ads at facebook. In
PrOCeedings of the Eighth International WorkshOP on Data Mining for OnIine Advertising, pp.
1-9, 2014.
Guolin Ke, Qi Meng, Thomas Finley, Taifeng Wang, Wei Chen, Weidong Ma, Qiwei Ye, and Tie-
Yan Liu. Lightgbm: A highly efficient gradient boosting decision tree. In Proceedings of the31st
InternationaI ConferenCe on NeUraI Information Processing Systems, NIPS'17, pp. 3149-3157,
Red Hook, NY, USA, 2017. Curran Associates Inc. ISBN 9781510860964.
Levente Kocsis, Csaba Szepesvari, and Jan Willemson. Improved monte-carlo search. Univ. Tartu,
Estonia, Tech. Rep, 1, 2006.
Marek Kretowski and Marek Grzes. Global learning of decision trees by an evolutionary algorithm.
In Information ProCessing and SeCUrity Systems, pp. 401T10. Springer, 2005.
Hyafil Laurent and Ronald L Rivest. Constructing optimal binary decision trees is np-complete.
Information ProCessing letters, 5(1):15-17, 1976.
Jianxun Lian, Xiaohuan Zhou, Fuzheng Zhang, Zhongxia Chen, Xing Xie, and Guangzhong Sun.
xdeepfm: Combining explicit and implicit feature interactions for recommender systems. In
ProCeedings of the 24th ACM SIGKDD InternationaI Conference on KnoWIedge DisCoVery &
DataMining, pp. 1754-1763, 2018.
Michael Lim and Trevor Hastie. Learning interactions via hierarchical group-lasso regularization.
JoUrnaI of ComPUtationaI and GraPhiCaI Statistics, 24(3):627-654, 2015.
Bin Liu, Chenxu Zhu, Guilin Li, Weinan Zhang, Jincai Lai, Ruiming Tang, Xiuqiang He, Zhenguo
Li, and Yong Yu. Autofis: Automatic feature interaction selection in factorization models for
click-through rate prediction. In Proceedings of the 26th ACM SIGKDD InternationaI ConferenCe
on KnoWIedge DisCoVery & Data Mining, pp. 2636-2645, 2020.
Yuanfei Luo, Mengshuo Wang, Hao Zhou, Quanming Yao, Wei-Wei Tu, Yuqiang Chen, Wenyuan
Dai, and Qiang Yang. Autocross: Automatic feature crossing for tabular data in real-world ap-
plications. In ProCeedings of the 25th ACM SIGKDD InternationaI Conference on KnoWIedge
DisCoVery & Data Mining, pp. 1936-1945, 2019.
Harshit Pande. Field-embedded factorization machines for click-through rate prediction. arXiv
PrePrint arXiv:2009.09931, 2020.
Yanru Qu, Han Cai, Kan Ren, Weinan Zhang, Yong Yu, Ying Wen, and Jun Wang. Product-based
neural networks for user response prediction. In 2016 IEEE 16th InternationaI ConferenCe on
DataMining (ICDM), pp. 1149-1154. IEEE, 2016；
10
Under review as a conference paper at ICLR 2022
Steffen Rendle. Factorization machines. In 2010 IEEE International Conference on data mining, pp.
995-1000. IEEE, 2010.
David Silver, Aja Huang, Chris J Maddison, Arthur Guez, Laurent Sifre, George Van Den Driessche,
Julian Schrittwieser, Ioannis Antonoglou, Veda Panneershelvam, Marc Lanctot, et al. Mastering
the game of go with deep neural networks and tree search. nature, 529(7587):484^89, 2016.
Daria Sorokina, Rich Caruana, Mirek Riedewald, and Daniel Fink. Detecting statistical interactions
with additive groves of trees. In Proceedings of the 25th international Conference on Machine
learning, pp.1000-1007, 2008.
Yang Sun, Junwei Pan, Alex Zhang, and Aaron Flores. Fm2: Field-matrixed factorization machines
for recommender systems. In PrOceedingS of the Web COnference 2021, pp. 2828-2837, 2021.
Zhulin Tao, Xiang Wang, Xiangnan He, Xianglin Huang, and Tat-Seng Chua. Hoafm: a high-order
attentive factorization machine for ctr prediction. InfOrmatiOn PrOceSSing & Management, 57(6):
102076, 2020.
Michael Tsang, Dehua Cheng, and Yan Liu. Detecting statistical interactions from neural network
weights. arXiv PrePrint arXiv:1705.04977, 2017.
Michael Tsang, Dehua Cheng, Hanpeng Liu, Xue Feng, Eric Zhou, and Yan Liu. Feature inter-
action interpretability: A case for explaining ad-recommendation systems via neural interac-
tion detection. In InternatiOnal COnference on Learning RePreSentations, 2020. URL https：
//openreview.net/forum?id=BkgnhTEtDS.
Gregory Vaughan, Robert Aseltine, Kun Chen, and Jun Yan. Efficient interaction selection for
clustered data via stagewise generalized estimating equations. StatiSticS in Medicine, 39(22):
2855-2868, 2020.
Zhen James Xiang, Yun Wang, and Peter J Ramadge. Screening tests for lasso problems. IEEE
transactions on Pattern analysis and machine intelligence, 39(5):1008-1027, 2016.
11
Under review as a conference paper at ICLR 2022
A	Appendix A
Theorem 1.	Under assumption 1, let {w*,T1, w*,T2 ,…，w*,TNT } be the solution of the correspond-
ing LASSO problem ofEq. 1, if j ∈ wT, w；Ti = 0.
Proof. Suppose that D has N samples, and let (xt, yt) be the tth sample of D. We define the loss
function of the LASSO as
N	NT lTi
mwin X(yt - X X wjTi xtT,ij)2 + N λkwk1,	(10)
PtN 1 xtTi yt
where λ ≤ λmaχ = maxi,j (— Nt,j—). Following the proof of Theorem 2 in section 4.2 of the
safe-screen LASSO (Xiang et al., 2016), given i ∈ [NT] and j ∈ [lTi], if
1	XX Ti +(1 λ MsPlN=I y2 PN=I(XTj)2 λ	(11)
N N Xtjyt+(1 - λmχ )V F	N - ≤λ,	(11)
we have wjTi = 0. Then letN → ∞, and based on Assumption 1, Eq. 11 can be rewritten as
E(x,y)〜DXji y + CF JE(χ,y)〜DXTi ≤ λ,
which is the screen criterion in the set of w0Ti. Since weights of features whose indices are not
included in w0Ti equal zero, and w0Ti is a subset of wTi, if j ∈ WTi, w*,Ti = 0, which finish the
proof.	□
Theorem 2.	Let w*,Ti = argmi□wTi LsT(WTi)and LT = minwTi LsT(WT). With the event
Ex,Ti,j = {X is assigned to Ti ’sjth leaf node}, we have
LTi= X P(Eχ,Ti,j)E[(y - E[y∣Eχ,Ti,j])2∣Eχ,Ti,j] + X	P(Eχ,Ti,j加城∣Eχ,Ti,j]∙⑶
j∈wTi	j∈[lTi ]∖wTi
Proof. Since for any j ∈ WTi, WjTi = 0. Therefore, minwT LsT(WTi)can be written as
minwTi LSTi(WTi)
=minwTi X P(Ex,Ti,j)E[(y -WjTi)2|Ex,Ti,j + X	P(Ex,Ti,j)E[y2|Ex,Ti,j.	(12)
j∈wTi	j∈[iT ]∖wTi
The first term of Eq. 12 has a closed-form solution
WjTi = E[y|Ex,Ti,j .	(13)
Substitute Eq. 13 into Eq 12, we finally get
LTi =minwTi LSTi(WTi)
= X P (Ex,Ti,j)E[(y - E[y|Ex,Ti,j)2|Ex,Ti,j + X	P(Ex,Ti,j)E[y2|Ex,Ti,j.
j∈wTi	j∈[lT ]∖wTi
□
Theorem 3.	Let Ti be the candidate DT set of the ith DT Ti. With Ti0 = {T |T ∈ Ti &
{1,2,...,lτi} = WTi}, we build T- = {T|T ∈ Ti0 & T0∖{T} doesn't contain T,s sub-tree}.
∀T ∈ T, ∃T0 ∈ T- with LTo ≤ LT.
12
Under review as a conference paper at ICLR 2022
Proof. For any T ∈ Ti with WT ⊂ {1, 2,…，lτ}, We can repeatedly pruning T's feature nodes all
of whose child nodes' indices are not included in WT until we get T0 with WT0 = {1, 2,..., lτo}.
According to factor 1 in Remark 3, by pruning T's leaf nodes whose indices are not included in WT,
the resultant DT has a lower loss. Therefore, we can prove that for any T ∈ Ti, there exists a DT in
Ti0, whose loss is smaller than that ofT. Then according to factor 2 in Remark 3, for any T ∈ Ti0, if
T is the sub-tree of any DT in Ti0∖{T}, it has larger loss, which concludes the proof.	□
Theorem 4.	Let TP be the FDT after pruning its feature nodes whose child data nodes all satisfy
the criterion set forth in Eq. 5 , and TP be the set of all candidate DTs obtained from TP by
selecting one and only one feature node for each ofTP ’s internal data nodes. We have TP = T-.
E(x,y)〜D[yi(x,y)∈snd] + CFVZP((χ,y) ∈ Sn) < λ,	(5)
where P((x,y) ∈ Snd) andE(χ,y)〜D [y1(χ,y)∈s d∖ CanbefUrtherdecomPosedintoEq. 6 according
to Bayes’ theorem.
P ((x, y) ∈ Snd) = Y	P(xi =j|(x,y) ∈ Spa(pa(nid,0j))),
nid,0j ∈end \nr	(6)
E(x,y)〜D [y1(x,y)∈Snd] = E(x,y)〜D [y|(x,y) ∈ Snd ] P ((x,y) ∈ Snd ),
where end is the node set containing all data nodes along the Path from the FDT’s root node to nd,
andpa(pa(nid,0j)) indicates nid,0j ’s grandParent node.
Proof. The proof is elementary and is hence omitted.	□
The proof of Theorem 5 can be finished by proving Lemmas 1-5 on the basis of Assumption 2-4.
Lemmas 1 and 2 give the lower bound of visit time for each of FDT's nodes. Building on the results
of Lemmas 1 and 2, Lemmas 3-5 show that for each of TP's leaf node's parent feature node nf,
limt→∞ E[3nf,t] = Lnf with high probablity.
Assumption 2. ∀i ∈ [lT], ExiT is bounded by 0 < αb ≤ ExiT ≤ βb < 1.
Assumption 3. y ∈ {0, 1}.
Assumption 4. For each of data node nd in Tt, we have
|E(x,y)〜D [y1(x,y)∈Snd] + CF VZP((X, y) ∈ Snd ) - N ≥ 3,
where s is a fixed Postive real number.
Lemma 1. Let nf be Tt 'sfeature node. Then we assumes that the ith selection of nf happens in the
tthf i iteraions of MCTs-IS. With Gt nf = { nf is selected in the tthf i iteration of MCTs-IS} and
n ,i	nf ,i ,	n ,i
1i,nd,τ = 1(xtpa(nd,τ ,.,P = vlGtpa(nd, ji,pa(nd,, )), When δ1 ∈ (0, 1) and N ≥ C(lθg	), We
N
P(X 1i,ndτ ≥ α2bN) ≥ 1 - δι.
i=1	,
Proof. On the basis of the Hoeffding’s inequality, we have
P(XN 1i,nd	≤	(E1i,nd	-	)N) ≤	e-22N	and P(XN 1i,nd	≥	(E1i,nd	+)N)	≤	e-22N
θ,τ	θ,τ	θ,τ	θ,τ
i=1	i=1
(14)
13
Under review as a conference paper at ICLR 2022
First, we have
N
P (for ∀τ 0 ∈ [Kθ ], X 1i,ndτ ≥ α N)
i=1	,
N
=P(	X X 1i,nθd,τ
T0∈[Kθ]∖{τ} i=1
N
≤ (1- α)N & for ∀τ0 ∈ [KθNT}, X 1i,nd - NEl* , ≥ (与-E%d ,)N)
2	θ,τ	θ,τ0	2	θ,τ0
i=1
N
=P (	X X 1i,nd,τ 0- N X E1i,nd,τ, ≤ (I- X 叫小一中 IN
τ0∈[Kθ]∖{τ} i=1	v0 ∈[kp]	τ0∈[kθ]
N
& for ∀τ 0 ∈ [Kθ ]∖{τ }，X 1i,ndτ - N E1i,nd T 0 ≥ (ɪ - E1i,nd T JN)
i=1	,	,	2	,
≥P(f or ∀τ0 ∈ [Kθ]∖{τ}, (EIZT - 2)N
Kθ - 1
N
≥ X 1i,nθd,T
- NE1i,nθd,T
≥ ( α2b - 叫&
(E1ind -等)N	N
≥P(for ∀τ0 ∈ [Kθ]∖{τ}，	, K - 1	≥ X 1i,nd,τ - N叫崂
(α - E1i,nd 0 )N
θ,T
Kθ - 1
N
≥1- X	P(X 1i,nθd T0 - NE1i,nθd T
τ0∈[Kθ]∖{τ}	i=1	,	,
(E1i,nd.τ0-等)N
Kθ -1	)
N
X P(X 1i,nθd,T0 - NE1i,nθd,T
τ0∈[Kθ]∖{τ}	i=1
(E1i,nd,τ-矍)N
Kθ -1	)
With K = maxθ∈[q] Kθ, by combining Eqs .14 and 15, we have
(15)
N
1- X P(X 1i,nθd T0 - NE1i,nθd T
τ0∈[Kθ]∖{τ}	i=1	,	,
≤-
(E1i,nd,τ0 - eι)N
Kθ -1	)
—
0
0
—
0
0
≥
)
N
τ0∈[KXθ]∖{τ}P(Xi=11i,nθd,T
- NE1i,nθd,T
〉(E1i,nd,τ -eι)N
≥ K - 1	)
0
0
N
≥1- X	P(X 1i,nθd T0 - NE1i,nθd T
τ0∈[Kθ]∖{τ}	i=1	,	,
αb N
2Kθ - 2)
N
τ0∈[KXθ]∖{τ}P(Xi=11i,nθd,T
- NE1i,nθd,
〉αbN
≥ 2Kθ - 2)
≤
-αb2 N
≥1 - 2(Kθ - 1)e "θ-1)2
-αb2 N
>1 — 2Ke 2K
With N > Ω(log 2KK2-), We finally have P(PN=I 13崎 > ObN) > 1 - δι.
□
Lemma 2. Define the event A = {The FDT Tt fitted by MCTs-IS has already been fully expanded,
and for each of Tt ,s data node nd, Snd < 1 and Snd > 1 are correctly estimated.}. When iteration
number is greater than tA, A happens with high probability.
Proof. Based on Theorem 4 in (Kocsis et al., 2006), for each feature node nf in Tt, We have Vnf ,t =
Ω(log Vpa(nf),t), while for each data node nd in Tt, according to Lemma 1, if Vndt is large enough,
we have P (for ∀τ0 ∈ [Kθ], Vnd 伴 > 号 Vpa(nd Jt) > 1 一 δ, where δ is a small positive real
value. Therefore, if the number of iterations of MCTs-IS is large enough, With large probability, the
FDT tree Tt can be fully expanded, and under Assumption 4, for each of TtS data node nd, Snd < 1
and Snd > 1 can be correctly estimated according to the hoeffding inequality.	□
14
Under review as a conference paper at ICLR 2022
Lemma 3. Let TP be the FDT after pruning all Tt 飞 feature nodes nf with nf ∈ l(pa(nf)), and
nf be TP ,s leafnode'sfather node conditioned on A. With Gtf nf = { nf is Selected in the ttf .
t	nf,i,n	nf,i
iteration of MCTs-IS} and 1i,nθd,τ = 1(xtpa(nd ),i,p = v|Gtpa(nd ),i,pa(nθd,τ)), we have
|
1
Vnf t
n ,tmax
Vnf t
n , max
Σ
i=1
E[
nθd,τ ∈c(nf)
1i,nθd,τ (ytnf,i
—
ii0=1 1i0,nθd,τytnf
ii0=1 1i0,nθd,τ
L)2]-Lζf I ≤O(
log Vnf,t
Vnf t
n , max
max )
(16)
Proof. With μnd = E(χ,y)〜D [y∣(x, y) ∈ Snd ]，由‘厄" hand Side of Eq. 16 Can be expanded to
1
Vf^^
n , max
f ,tmax
E[	1i,nθd,τ(ytnf,i -
i=1	nθd,τ ∈c(nf )
Ei0 = 1 1i0,nd,τ ytnf,i0
i0,nθd,τ
)2|A]
=E[
1
Vf
n , max
Vf
n ,tmax
E E	ii,*(ytnf,i-")2∣A]
i=1	nθd,τ ∈c(nf)
+ E[
1
Vf
n , max
Vf
n ,tmax
E E I，,…
i=1 nθd,τ ∈c(nf)
Ei0=1 1i0,nd,τ ytnf,i0 ) 2 A]
ii0=1 1i0,nθd,τ
2 nf,tmax
+ E[v---------- E E	1i,nd τ (ytnf i - μnd J(〃nd
Vf	θ,τ n ,i	θ,τ θ,
n ,tmax	i=1	nθd,τ ∈c(nf)
i0,nθd,τ
)|A]
—
(17)
The first term of Eq. 17 is Lnf, while the second term can be further expanded as
Vnf,
tmax
Vnf,tm
X
i=1
Σ
nθd,τ ∈c(nf)
E[1i,nθd,
Ei0 = 1 1io,nd,τ3nd,τ - ntnf,i)
ii0=1 1i0,nθd,τ
)2|i, θ, τ, A]
Vnft
n , max
Vnft
n ,tmax
X X E[1
i=1	nθd,τ ∈c(nf)
i,nθd,τ (
Ei'∈[i]∖i"1"i'ndJ"nd,τ - ytnf∙") )2∣i00, i,θ,τ, A]
i0=1 1i0,nθd,τ
Vnf,
tmax
Vnft
n ,tm
X
i=1
E[1i,nθd,τ
nθd,τ ∈c(nf)
(1i∕/ dd (μrιd	- yt f 〃))2
∖ i ,nθ,τ'Lnθ,τ	"nf,i0 "
―(Pi0=1 1i0,nd,τ)2-
|i00, i, θ, τ, A]
Vnf,
tmax
Vnft
n ,tm
X
i=1
Σ
nθd,τ ∈c(nf)
E[1i,nθd,
,ndjμnd,τ - ytnf,i")) Ei0∈[i]∖i00 1i0,n;
ii0=1 1i0,nθd,τ
d-τ (μnd-τ■-yf0) )∣i00, i,θ,τ, A]
(18)
+
+
1
1
1
2
(
15
Under review as a conference paper at ICLR 2022
The third term of Eq. 18 equals 0 because yt f 00 is independent with yt f 0 . After fully expanding
the first term of Eq. 18 recursively, we get
1
VnfI^^
n , max
Vnft
n ,tmax
E E	E[ii,nd,τ(μnd,τ
i=1	nθd,τ ∈c(nf)
∑2i0=1 1i0,nd,τ ytnf,i0
i0,nθd,τ
)2|i, θ, τ, A]
1
κfl
n , max
Vnft
n ,tmax
X
i=1
E[1i,nθd,τ
nθd,τ ∈c(nf)
Pio = 1 1i,nθ,τ(μnθ,τ - ytnf,J
(Pio=1 1i0,nd,τ )2
∣i,θ,T, A]
≤
Vnf
,tmax
Vnft
n ,tmax
X
i=1
1ind
X E[ pi ,1θ,τ - ∣i,θ,τ, A]
nθd,τ ∈c(nf)	i0=1 i0 ,nθd,τ
1
κfl
n , max
Vf
n ,tmax
P(1i,nθd,τ
nθd,τ ∈c(nf)	i=1
1|i, θ, τ, A)E[
1 +	i0∈[i-1] 1i0,nθd,τ
lli,nd,τ
Since 1i,nd
is a bernoulli random variable, we have
1,i,θ,τ,A]
(19)
1
—
1
1	1	1 - (1 - E[1i,nθd τ |i, θ, τ, A])i	1
-≤	E[ 1	, p--------i-------l1i,nd	=1,i,θ,τ,	A]	=------而η--------LTj------疝------≤	--
i 1 + i0∈[i-1] 1i0,nθdτ i,nθ,τ	iE[1i,nθd τ |i, θ, τ, A]	iαb
,	,	(20)
Then based on PN=1 i = Θ(log N). Eq. 19 can be further upper bounded by
1
Vf
n , max
Vf
n ,tmax
X X % +P ∖, d
nθd,τ∈c(nf)	i=1	1 + i0∈[i-1] 1i0,nθd,τ
l1i,nd,τ
1,i,θ,τ,A]
Vf
n ,tmax
max nθd,τ ∈c(nf)
≤O -- ≤O(
iαb
log Vnf,tmax
i=1
Vnft
n , max
1
≤--
Ff ,t
Σ
)
The last term of Eq. 17 can be upper-bounded by
Vnft
n ,tmax
E[
Vnft
n , max
i=1
nθd,τ ∈c(nf)
1i,ndτ(ytnf。- μnd )(μndτ
θ,τ n ,i	θ,τ	θ,τ
ii0=1 1i0,nθd,τ ytnf,
ii0=1 1i0,nθd,τ
2
Σ
Σ
—
nf,tmax
=E[ V——	X X	1i,nd,τ (yt
n ,tmax	i=1	nθd,τ ∈c(nf)
“Pi0 = 1 1i0 ,nd,τ (μnd,τ - ytnf,i0)∖S
- μndτ)(	E ；	)lGi,nf
,	i0=1 1i0,nθd,τ
A]
-E[
tmax
Vnft
n ,tm
X
i=1
Σ
nθd,τ ∈c(nf)
1i,nθd,
(ytnf,i - μnd,J2
τ
i0,nθd,τ
|Gi,nf
A]
Vnf t
n ,tmax
+ E[
Vnf t
n ,tmax
i=1
1κnθ,τ(ntnf,i - μnd,
)(Pii0-=111
i0,nd τ (μnd T
θ,τ θ,τ
pi0=11
i0,nθd,τ
- ytnf,i0))|Gi,nf
A]
Eq. 21's second term of equals 0, and its first term is lower bounded by -Ω(
log Vnf ,tmaχ
(21)
Eq. 19 is greater than 0 while Eq. 21 is smaller than 0, we have
匕f +
n ,tmax
). Since
1
Vrt^^
n , max
Vf
n ,tmax
X E[ X
i=1	nθd,τ ∈c(nf)
1i,nθd,τ
log Vnf ,tmaχ
Vnf t
n , max
).
2
2
Σ
Σ
τ
|
(
□
16
Under review as a conference paper at ICLR 2022
Lemma 4. Let X1, ..., XN be independent random variables on {0, 1}. With
N
f(X1,...,XN) =	(Xn
—
n=1
we have :
N	n-1
P(X( 一Xn-X Xi )2
n=1
i=1
N	n-1
- E[X(⅜-Xn - X -)1) ≤
n=1
i=1
2t2
81N
e

Proof. First, function f can be written as
N	n-1
f (X1,…,XN)=X( 片 Xn- X U )2.
n=1
i=1
Then for every i = 1, ..., nand every (x1, ..., xn),(x01, ..., x0n) that differ only in the i-th coordinate
(xj = x0j for all j 6= i), we have
|f(X1,...,XN)-f(X10,...,XN0 )|
N
=|	(
n=i+1
n-1 Xn -	X
n
j∈[n-1]∖{i}
Xj - Xi )2 - ( n-1 Xn-
nn
Σ
j∈[nτ]∖{i}
Xj - X)2
nn
≤∣ ∑ 2(
n=i+1
n-ɪ Xn
n
Σ
j∈[nτ]∖{i}
X-Xi )| +1
N
X(
n=i+1
Xi2 - Xi02
n2
)|
+|2(
i-1
j=1
i-1 (Xi - Xi))I + 1(F)2(X2 - Xi2)l
N
—
—
n
i
i
n
i - 1 2 N n - 1 N 1
≤3(—--) + 2〉	—2----+)	—2
i	n2	n2
n=i+1	n=i+1
≤3+2
n-1
n∈[N]
n2
1
+〉 F
n2
n∈[N]
π
≤3 + 2log(N +1) + -
6
For the sake of simplicity, We assume t that when N ≥ t/, 3 + 2 log(N +1) + 6 ≤ 3 log(N). Then
according to McDiarmid’s inequality, we have
N	n-1	N
P (X(「Xn - X X )2 - E[X( L Xn
n=1
i=1
n=1
n-1 V,	1	i-
- E,)2] ≥ 3 log NJN log( δ~)) ≤ δι,
i=1 n	δ1
N	n-1
P(X(二Xn-X Xi)2
n=1
i=1
JV _	1
- E[X( n-1 Xn
n
n=1
X Xi)2] ≤ -3log N Jnlog(小)≤ δι.
N
—
□
Lemma 5. With
σnb d t
n , max
1
Vdt
n ,tmax
Vnd,t
max	i=1
(yt
nd,i
—
σn f t
n , max
Vnf,t
Vnf t
n ,tmax
XX
max	i=1	nθd,τ ∈c(nf)
ii0=1 1i0,ndyt
pio=ι i
i0 ,nd
1i,nθd,τ (ytnf,i
nd,i0 )2
—
ii0=1 1i0,nθd,τytn
i0=1 1i0,nθd,τ
1
f,i0 )2
17
Under review as a conference paper at ICLR 2022
We have
P(IVnf ,MM f ,tmaχ - Vnf ,tmaxE[* f 小。]1 ≥ 3K	^^，"max JeIVnf "max l°g( ?) ≤ 3δ
Proof. According to Lemma 1, the event CV f = { ∀nd ∈ l(nf), Vnd,tmax ≥ e1 Vnf,tmax }
happens with large probability when Vnf,tmax is large enough. Let P (CV f )≤ δ1, with Lemma
4, we have
P(Vnf ,tmaχσn f ,Mx - Vnf ,"^^ f ,』]≥ 3K lOg elVnf ,小 小1 Vnf ,jθg( W))
≤P (CV f
n ,tmax
)P(Vnf "max* f ,tmax - Vf "axE[M f "mJ	^K lOg 气匕,小。『1%f 小 lOg( ^ )忆,,小)
+ P(CViTf t	)
n ,tmax
≤	''	P(Vnd	t	σ∖d	t —	Vnd	t	E[σ0 d t ] ≥ 3log	eιVnd	t	J∈ιVnd	t	log(-)∣Cnf	t ) + δι
n ,tmax n ,tmax n ,tmax	n ,tmax	n ,tmax	n ,tmax	n ,tmax
nd ∈l(nf)
≤(K + 1)δ1
Similarly, we have
P(Vnf ,tmaxσn f ,tmax - Vnf "max E[M f 小。]≤ -3K lOg	,小 小1 Vnf ,jθg( W)) ≤ (K +1)δ1,
which finishes the proof.	□
Theorem 5. In the tth iteration ofMCTs-IS, let TP be the FDT after pruning all Tt 飞feature nodes
nf with nf ∈ l(pα(nf)). For each of TP ’s leafnodes，Parentfeature node nf , if Lf is estimated
by df t, and for each TP，s internal data node nd, Lnd is calculated on the basis of the estimated
Lnfo, a DTTt can be ObtalnedfrOm TP, and we have limt→∞ P(Tt = T*) = 1.
Proof. According to Lemma 2, when the iteration number approaches to infinity, we first have
TP = TP. In this case, let nf be TP's leaf nodes' parent node. Based on Lemmas 3 and 5, with
large enough t, Lnf0 can be estimated accurately. Besides, with the help of hoeffding inequality,
Pnd can accurately estimate P(Xi = j ∣(x, y) ∈ Snd) in Eq. 7. Therefore, following Eq. 7, for any
ni,j
TP's internal data node nd, Lnd = mi□nf ∈c") Lnf holds for a large probability when the number
of iteration approaches to infinity.	□
B Appendix B
B.1	TYGEM Preprocessing
19×19 Go is a game played on a board that is represented as a 19×19 grid. A position on the
Go board can be filled with either a white or black stone by a white or black player, and thus can
be regarded as a tenary feature ({empty, black, white}). In the TYGEM 9D vs. 9D dataset, we
first extract game records from 2006 to 2016. For each record, we then exclude all 19×19 board
configurations whose centers are not empty and only keep positions within the 7×7 region around
the center of the board (which is called Tengen) from each remaining configuration. Note that the
7×7 region is much larger than the widely used 3×3 region (also called 3×3 pattern) in the training
of AI GO players. In the event the white player plays the next move, we reverse all colors of stones
within each of the remaining 7×7 regions to ensure that the black player plays the next. Following
that, we assign labels for these regions based on whether the player places a stone in the center
during the next move (marked as 1) or not (marked as 0). Our final step is to randomly eliminate
99% records with labels of 0, to achieve a balanced data set with NogativeLabel ≈ 3.
18
Under review as a conference paper at ICLR 2022
B.2	Implementation Details of MCTs-IS and GBDT
Before running MCTs-IS and GBDT, the 48 features of TYGEM were randomly divided into eight
groups for MCTs-IS to be able to select high-way interactions. In this way, TYGEM’s features are
reduced to 8 while the number of possible values for each feature was increased from 3 to 36 .
In MCTs-IS, snd is used for pruning trees in MCTs-IS. Let n3 be the set containing all data nodes
with depth 3 in the FDT. For any MCTs-IS’ data node nd except the root node, ∃nid,0j ∈ n3, the
calculation of Snd requires factors P(Xi = j|(x, y) ∈ Spa(Pa(*)] and E(x,y)〜D [y|(x, y) ∈ SndJ
(see Eq. 6 for details). We estimated the two factors for each nid,0j ∈ n3 before running MCTs-IS
based on the training set. During MCTs-IS, these estimations were fixed and can be directly applied
to the calculation of snd .
B.3	Parameter Setting
The parameter settings of MCTs-IS and GBDT are given as follows.
1.	MCTs-IS. NT was set as 23 and 8 for the Avazu and TYGEM datasets, respectively. In the
ith FDT learning problem, all child feature nodes of the FDT’s root node except the ith one
are pruned. Regarding MCTs-IS’s hyper-parameters, we set Cp to 0.5, and υ and κ to 20
and 40, respectively. For cF and λ, a grid search was conducted over {0.05, 0.005, 0.0005}
and {5e - 3, 5e - 4, 5e - 5} respectively.
2.	GBDT. Consistent with MCTs-IS, 23 and 8 trees were built for Avazu and TYGEM
datasets in GBDT, respectively. In our work, GBDT was implement by the package of
Lightgbm, and Lightgbm limits the complexity of DTs according to the maximum number
of leaves in the DT. Therefore a grid serach was conducted over {100, 500, 1000, 2000} for
the maximum number of leaf nodes.
LR, FM and FmFM were trained after the interaction selection with MCTs-IS and GBDT. For
FmFM, we additionally use Mini-Batch Gradient Descent with Momentum (SGD with momen-
tum) to to minimize the loss function, besides the Adam optimizer implemented in its open-source
code. In SGD with momentum, 20 epochs were scheduled. The learning rate was set as 0.1, and was
halved every five epochs . Additionally, we conduct a grid search on the penalty of the regularizer
over {1e - 5,1e-6,1e- 7,1e-8}.
B.4	MCTS-IS’s Selected Interactions in TYGEM
All interactions with snd > λ (N=577) are collected and correlated to labels with χ2 test. After-
wards, interactions significantly correlated with labels (P < 0.05, Bonferroni corrected) are selected
and ranked according to the χ2 statistic. Figure. 2 displays the top six interactions.
19
Under review as a conference paper at ICLR 2022
(4)	{51	(6)
Figure 2: Interactions selected by MCTs-IS. Positions marked by ”?” mean that these positions can
be empty, or be placed by either white or black stones.
20