Under review as a conference paper at ICLR 2022
Robust and Data-efficient Q-learning by
Composite Value-estimation
Anonymous authors
Paper under double-blind review
Ab stract
In the past few years, off-policy reinforcement learning methods have shown
promising results in their application for robot control. Deep Q-learning, how-
ever, still suffers from poor data-efficiency and is susceptible to stochasticity or
noise in transitions and reward, which is limiting with regard to real-world appli-
cations. We alleviate these problems by proposing two novel off-policy Temporal-
Difference formulations: (1) Truncated Q-functions which represent the return for
the first n steps ofa target-policy rollout w.r.t. the full action-value and (2) Shifted
Q-functions, acting as the farsighted return after this truncated rollout. This de-
composition allows us to optimize both parts with their individual learning rates,
achieving significant learning speedup and robustness to variance in the reward
signal, leading to the Composite Q-learning algorithm. We employ Composite Q-
learning within TD3 and compare Composite TD3 with TD3 and TD3(∆), which
we introduce as an off-policy variant of TD(∆). Moreover, we show that Com-
posite TD3 outperforms TD3 as well as TD3(∆) significantly in terms of data-
efficiency in multiple simulated robot tasks and that Composite Q-learning is ro-
bust to stochastic environments and reward functions.
1	Introduction
In recent years, Q-learning (Watkins & Dayan, 1992) has achieved major successes in a broad range
of areas by employing deep neural networks (Mnih et al., 2015; Silver et al., 2018; Lillicrap et al.,
2016), including environments of higher complexity (Riedmiller et al., 2018) and even in first real-
world applications (Haarnoja et al., 2019). Deep Q-learning methods, however, are still far away
from being easily applicable to solving complex real-world robotic tasks due to their high demand
for data samples and their lack in learning stability caused by the stochastic nature of transitions in
real-world settings. Especially stochasticity in the immediate reward signal itself may cause policies
to converge to suboptimal performance. This stochasticity can be induced by multiple sources of
randomness, e.g. through stochastic next state transitions due to sensor or actuator noise, occlusions,
or due to data-driven reward estimation (Author, s; Ni et al., 2020; Fu et al., 2018). This increases
the complexity of value-estimation tremendously due the long temporal horizon the reward signal
has to propagate through, possibly depending on many decision steps and transitions. To overcome
the complexity of growing task horizon, in this work, we propose to break down the long-term return
into a composition of several short-term predictions over a fixed temporal horizon. We approach this
via a new formulation of consecutive bootstrapping from value-functions corresponding to different
horizons of the target-policy associated to the full return. For its formulation, we define Truncated
Q-functions, representing the return for the first n steps of a target-policy rollout w.r.t. the full action-
value. In addition, we introduce Shifted Q-functions which represent the farsighted return after this
truncated rollout. Both are then combined in a mutual recursive definition of the Q-function for the
final algorithm. Starting from a reformulation of the vanilla Q-learning formalism, our approach
offers the flexibility to separately optimize hyperparameters for different parts of value-estimation,
showing that Shifted Q-functions suffer less from variance compared to the full return thus allowing
for higher learning rates.
Most similar to our approach is TD(∆) (Romoff et al., 2019), composing the full return of value-
functions with increasing discount. Increasing discount factors translate to growing task horizons,
leading to a decomposition of the long-term prediction task into multiple subtasks of smaller com-
plexity. In their work, Romoff et al. (2019) argue that values for smaller discount values can poten-
1
Under review as a conference paper at ICLR 2022
tially stabilize more quickly, leading to faster and more stable learning. Since the values correspond-
ing to larger discount values only represent the residual to the values of smaller discount, however,
TD(∆) does not alleviate the issues arising in stochastic environments. Other techniques for variance
reduction in TD-learning are reward estimation (Romoff et al., 2018) or averaging value-functions
(Anschel et al., 2017). Both techniques are orthogonal to our work and can be combined with Com-
posite Q-learning to possibly further enhance learning stability. However, as we show in this work,
the increase in performance and data-efficiency comes from the possibility of higher learning rates
in the long-term Shifted Q-functions and not the short-sighted Truncated Q-functions. It can thus be
argued that reward estimation and averaging come at a cost of slower convergence. Asis et al. (2020)
propose to learn action-values for a fixed-horizon via consecutive bootstrapping. In their formula-
tion, however, the consecutive value-functions are estimated over different target-policies w.r.t. the
different horizons and ignore the residual of the full return. Thus, they can therefore be non-optimal
w.r.t. the task at hand.
Our contributions are fourfold. First, we introduce the Composite Q-learning algorithm. Second,
we show that the targets of Shifted Q-functions suffer less from variance compared to the full action-
value, thus allowing for higher learning rates. Third, we introduce TD3(∆), an off-policy extension
of TD(∆) to deep Q-learning. And fourth, we employ Composite Q-learning within TD3 and eval-
uate Composite TD3 in various simulated robot tasks under normal and noisy reward functions.
2	Background
We consider tasks modelled as Markov decision processes (MDP), where an agent executes action
at ∈ A in some state st ∈ S following its stochastic policy π : S × A 7→ [0, 1]. According to the
dynamics model M : S × A × S 7→ [0, 1] of the environment, the agent transitions into some state
st+1 ∈ S and receives scalar reward rt. The agent aims at maximizing the expected long-term return
Rn(St) = Eaj≥t~∏,sj>t~m[pT=-t1 γj-trj|st], where T is the (possibly infinite) temporal horizon
of the MDP and Y ∈ [0,1] the discount factor. It therefore tries to find ∏*, s.t. Rπ* ≥ Rπ for all ∏.
If the model of the environment is unknown, model-free methods based on the Bellman Optimality
Equation over the so-called action-value Q以St, at) = Eaj>t~∏,sj>t~M[PT=t1 Yj-trj |st, at], Can
be used:
Q*(st,at) = Yt + Y maxEsj>t-M [Q*(st+ι,a)] .	(1)
a j>
In the following, We abbreviate Eaj>t^∏,Sj>t^M[∙∣st,at] by E∏,m[∙]. One of the most pop-
ular model-free reinforcement learning methods is the sampling-based off-policy Q-learning
algorithm (Watkins & Dayan, 1992). A representative of continuous model-free reinforce-
ment learning with function approximation is the Deep Deterministic Policy Gradient (DDPG)
actor-critic method (Lillicrap et al., 2016). In DDPG, actor μ is a deterministic mapping
from states to actions, μ : S → A, representing the actions that maximize the critic Q, i.e.
μ(st) = argmaXa Q(st, a). Q and μ are estimated by function approximators Q(∙, ∙∣Θq)
and μ(∙∣θμ), parameterized by θQ and θμ. The critic is optimized on the mean squared er-
ror between predictions Q(sj,aj∣Θq) and targets yj = r + γQ0(sj+ι, μ0(sj+ι∣θμ0)∣θQ0),
where Q0 and μ0 are target networks, parameterized by θQ0 and θμ0. The parameters of
μ are optimized following the deterministic policy gradient theorem (Silver et al., 2014),
Vθμ J m1 Pj VaQ(s,a∣θQ)∣s=sj,a=μ(sj∣θμ)Vθμμ(s∣θμ), and the parameters of the target net-
works are updated via Polyak averaging, i.e. θQ0 J (1 一 T)θQ0 + τθQ and θμ0 J (1 一 T)θμ0 + τθμ,
with τ ∈ [0, 1].
The state-of-the-art actor-critic method TD3 (Fujimoto et al., 2018) then adds three adjustments to
vanilla DDPG. First, the minimum prediction of two distinct critics is taken for target calculation to
alleviate overestimation bias, an approach belonging to the family of Double Q-learning algorithms
(van Hasselt et al., 2016). Second, Gaussian smoothing is applied to the target-policy, addressing
the variance in updates. Third, actor and target networks are updated every d-th gradient step of the
critic, to account for the problem of moving targets.
2
Under review as a conference paper at ICLR 2022
Figure 1: Structure of Composite Q-learning. Qi denote the Truncated and Qi：∞ the Shifted Q-
functions at step i. Q is the complete Composite Q-function. Directed incoming edges yield the
targets for the corresponding value-function, evaluated at the maximizing actions of the full Q-
function. Edges denoted by γ are discounted.
3	Combining Long-term and Short-term Predictions in
Q-learning
In the following, we introduce the Composite Q-learning algorithm, along with Off-policy TD(∆)
as an additional baseline. Both approaches aim at decomposing the long-term value into values
of smaller time scales. Composite Q-learning approaches this by dividing the bootstrapping into
a short-term and a long-term component. Off-policy TD(∆) on the other hand formalizes a delta
function to estimate the remainder ofan action-value corresponding to a smaller discount factor.
3.1	Composite Q-learning
We estimate the return of n-step rollouts of the target-policy via Truncated Q-functions which we
then combine to the full return with model-free Shifted Q-functions, an approach we call Composite
Q-learning, while remaining purely off-policy. Since these quantities cannot be estimated directly
from single-step transitions, we introduce a consecutive bootstrapping scheme based on intermediate
predictions. For an overview, see Figure 1. Code based on the implementation of TD31 can be found
in the supplementary.
3.1.1	Truncated Q-functions
In order to decompose the action-value into multiple truncated returns, assume that n	(T - 1)
and that (T - 1 - t) mod n = 0 for task horizon T. We make use of the following observation:
Qπ(st, at) = E∏,M Irt + γrt+l + Y2rt+2 + Y3rt+3 +-+ YTTrT-l]
t+n-1	t+2n-1
= Eπ,M	X γj-trj +γn	X γj-(t+n)rj
j=t	j=t+n	(2)
+ .•. + YT-n ( X Yj-(T-n)rj].
j=T-n
That is, we can define the action-value as a combination of partial sums of length n. We can then
define the Truncated Q-function as Qπn(st, at) = Eπ,M[Ptj+=nt -1 Yj-trj], which we plug into Equa-
tion (2):
Qn (st, at) = En,M[Qn,(st, at) + YnQn(St+n, at+n) + …+ YT-nQn(sT-n, aT-n)].⑶
Please note that the assumption of (T -1-t) mod n = 0 only leads to an easier notation and is nota
general restriction. If (T - 1 -t) mod n 6= 0, then the last partial sum simply has a shorter horizon.
1https://github.com/sfujim/TD3
3
Under review as a conference paper at ICLR 2022
Following Theorem A.1 in the appendix, We approximate Qn(St,at) off-policy via consecutive
bootstrapping. In order to limit the prediction to horizon n, we estimate n different truncated value-
functions belonging to increasing horizons. Let QTr(∙, ∙∣θTr) denote a function approximator with
parameters θTr and n outputs, subsequently called heads, estimating Truncated Q-functions Q*.
Each output QiTr bootstraps from the prediction of the preceding head evaluating the target-policy of
full return Q for a fixed horizon, with the first approximating the immediate reward function. The
targets are therefore given by:
yjT,r1 = rj and
yj>1 = rj + γQT-ι(Sj+l,μ0(Sj+l∣θ“0)∣θTr0),
(4)
where μ0 corresponds to the target-actor maximizing the full Q-value as defined in Section 2 and
QT-I the output of the respective Q-target-network. That is, QTr represents evaluations of μ at
different stages of truncation and yjT,ri<n serve as intermediate predictions to get yjT,rn . We then only
use QTnr, which implements the full n-step return, as the first part of the composition of the Q-target,
but not the QiT<r n values.
Please note that in order to estimate Equation (3), the dynamics model would be needed to get
st+c∙n of a rollout starting in st. In the following, we describe an approach to achieve an estimation
of Equation (3) model-free.
3.1.2	Shifted Q-functions
To get an estimation for the remainder of the rollout Q∏∞ = E∏,m [γnQ(st+n, at+n)] after n steps,
we use a consecutive bootstrapping formulation of the Q-prediction as a means to skip the first n
rewards of a target-policy rollout. Let QSh(∙, ∙∣θSh) denote the function approximator estimating
the Shifted Q-functions Q<∞ by n different outputs, parameterized by θSh. We can shift the Q-
prediction by bootstrapping without taking the immediate reward into account. The Shifted Q-targets
for heads QiSh therefore become:
yShι =γQ'(sj+ι,μ'(sj+ι∣θμ)∣θQ) and
yj>ι =YQSh0ι(Sj+ι ,〃«升1邛0)怛叫.
The variance of the remainder of the rollout decreases with later initial point in time:
var[γiQπ(st+i, at+i)] = var[γi(rt+i + γQπ(st+i+1, at+i+1))]
= var[γirt+i] + var[γi+1Qπ(st+i+1, at+i+1)]	(6)
+ 2cov[γirt+i,γi+1Qπ(st+i+1,at+i+1)].
If cov[γirt+i, γi+1Qπ(st+i+1, at+i+1)] ≥ 0, it holds that:
var[γiQπ(st+i, at+i)] ≥ var[γi+1Qπ(st+i+1, at+i+1)].	(7)
Please note thatγiQπ(st+i, at+i) corresponds to Shifted Q-functions at different stages i of shifting,
with i = 0 for the full Q-function. It hence follows:
var[Qπ(st,at)] ≥ var[Qf：g(st,at)] ≥ var[QS∞(st,at)] ≥ …≥ var[Qn：g(st,at)].	(8)
The covariance of reward and future return is likely to be positive if the immediate reward depends
on the next state (Romoff et al., 2018). Otherwise the covariance is zero. Hence, it can be argued
that this inequality is strict if the variance of the reward is larger than zero. This also holds for the
case of zero variance for the immediate reward, if the immediate reward depends on a stochastic next
state. We therefore argue that in these settings, as very common in real-world experiments, Shifted
Q-functions allow for higher learning rates compared to the full Q-function, since this translates
to lower variance in their gradients. We empirically validate this finding in our experiments in
Section 4.
4
Under review as a conference paper at ICLR 2022
3.1.3	Composition
Following the definitions of Truncated and Shifted Q-functions, we can compose the full return. In
the appendix, we show that the composition is a representation of the complete Q-value and provide
an analysis of Tabular Composite Q-learning. In the function approximation setting, we can define
the Composite Q-target as:
yQ = rj+ Y(Qnr0(Sj + ι,μ0 ⑶+ι∣θμ0)∣θτr0) + Qnh0 ⑶+1,μ0 ⑶+ι∣θμ0)∣θsh0)),	⑼
approximated by Q(∙, ∙IΘQ) With parameters θQ. Wejointly estimate QTr, QSh and Q with function
approximator QC(∙, ∙∣θc).
The incorporation of truncated returns divides the time scale of the long-term prediction by the
Shifted Q-function. We call this algorithm Composite Q-learning. Please note that Composite Q-
learning is equivalent to Q-learning if the learning rates for Truncated, Shifted and full Q-function
are set to the same values. However, a notable advantage is that Composite Q-learning offers an
independent optimization for the different learning rates corresponding to different temporal hori-
zons. A detailed description of Deep Continuous Composite Q-learning in its general form is given
in Algorithm 1. For an exemplary application within TD3 (as in our experiments in the following),
Gaussian policy smoothing has to be added to all targets in Line 8, as well as taking the minimum
prediction of two distinct critics for each target. Furthermore, actor and target networks have to be
updated with delay.
1
2
3
4
5
6
7
8
9
10
Algorithm 1: Deep Continuous Composite Q-learning
initialize critic QC, actor μ and targets QC0, μ0
initialize replay buffer R
for episode = 1..E do
get initial state s1
for t = 1..T do
apply action at = μ(st∣θμ) + ξ, where ξ 〜N(0, σ)
observe st+1 and rt and save transition (st, at, st+1 , rt) in R
calculate targets:
yjT,r1 = rj
yj1,i>1 = rj + YQT- i(Sj+i,"(Sj+i|ew)|eTr0)
yjh = γQ0(Sj+ι,μ0(Sj+ι∣θμ0)∣θQO)
yShi>ι = YQsS--ι(Sj+ι,μ'(Sj+ι∣θμ)∣θsSh'}
yj = rj + Y(Qnr0(Sj+ι,μ0 ⑶+1邛0)口"0) + Qnh0(sj+1 ,"®+i 邛0)|2))
yjC = hyjQ,yjT,r1,yjT,ri>1,yjS,h1,yjS,hi>1i
update QC on minibatch b of size m from R update μ on Q
adjust parameters of QC0 and μ0
3.2	OFF-POLICY TD(∆)
Another way to divide the value-function into multiple time scales is TD(∆) (Romoff et al., 2019).
To this point, it has only been applied in an on-policy setting. In favor of comparability, we extend
TD(∆) to Q-learning, yielding TD3(∆). The main idea of TD(∆) is the combination of differ-
ent value-functions corresponding to increasing discount values. Let γ∆ denote a fixed ordered
sequence of increasing discount values, i.e. Yδ = (γ1,γ2,..., Yk)> 儿>1>)一.
We can then define delta functions Wi as:
W1 =Qγ1 and Wi>1 =Qγi -Qγi-1.
(10)
Let Qδ(∙, ∙∣Θδ) denote the function approximator estimating Qγι≤i≤k with parameters Θδ. Based
on the derivations in (Romoff et al., 2019), the targets for Q-learning can be formalized as:
yj,ι = rj + yiqYi (Sj+1,μ0(Sj+"θμO)Wδ1 and
yj,i>ι = (Yi-Yi-i)QYi-ι (Sj+i,〃0(Sj+iW"0)d0) + YiWi\sj+i,〃0(Sj+i|e"0)d0).
5
Under review as a conference paper at ICLR 2022
The authors suggest the use of n-step targets within TD(∆) which is not easily applicable in an
off-policy setting. In our experiments, we therefore compare our approach to single-step Off-policy
TD(∆). A detailed overview of Off-policy TD(∆) can be found in Algorithm 2. To transform
Off-policy TD(∆) to TD3(∆), the adjustments as described in Section 3.1.3 have to be applied
analogously.
1
2
3
4
5
6
7
8
9
10
11
12
Algorithm 2: Off-policy TD(∆)
initialize critic Qδ, actor μ and targets Qδ, μ0
initialize replay buffer R
set discount values Yδ = (γ0,γ1,..., γk)1
for episode = 1..E do
get initial state s1
for t = 1..T do
apply action at = μ(st∖θμ) + ξ, where ξ 〜N(0, σ)
observe st+1 and rt and save transition (st , at , st+1 , rt) in R
calculate targets:
yγ,ι = rj+ yiqYi (Sj+1,μ0(Sj+ι∖θμO)∖θδO)
yj,i>ι = (Yi-Yi-I)QYi- (Sj+ι,μ0(Sj+ι∖θμO)∖θδO)
+YiW；(Sj+i,"(Sj+i\6"0)\e~)
update Q∆ on minibatch b of size m from R
update μ on QYk
adjust parameters of Qδ and μ0
4	Experimental Results
We apply Composite Q-learning within TD3 and compare against TD3 and TD3(∆) on three robot
simulation tasks of OpenAI Gym (Brockman et al., 2016) based on MuJoCo (Todorov et al., 2012):
Walker2d-v2, Hopper-v2 and Humanoid-v2. A visualization of the environments is depicted in
Figure 2. We discuss hyperparameter optimization and hyperparameter settings, as well as the opti-
mization procedure in Appendix D. Whilst we employ Composite Q-learning within TD3, for Soft
Actor-Critic (Haarnoja et al., 2018) the same results can be assumed to hold (Ball & Roberts, 2021).
We analyze the properties of Composite Q-learning in terms of data-efficiency and stability on the
true reward for Walker2d-v2 and the potential increase of updates per sample for Walker2d-v2 and
Hopper-v2 in Section 4.1, before we evaluate Composite TD3 on a very noisy reward signal for
Walker2d-v2, Hopper-v2 and Humanoid-v2 in Section 4.2.
Figure 2: Visualization of Walker2d-v2 (left), Hopper-v2 (middle) and Humanoid-v2 (right).
4.1	True Reward Function
In a first experiment, we compare Composite TD3, TD3(∆) and conventional TD3 with both the
optimized default learning rate and the highest learning rate used in Composite TD3 (TD3 HL) on
the true reward for the Walker2d-v2 environment in Figure 3a over 8 training runs. Composite TD3
and TD3(∆) yield a better and more stable performance than TD3 with either learning rate, with
Composite TD3 being the best performing of all approaches. The differences between conventional
TD3 and Composite TD3 are significant, as shown in Table E.1 in the appendix. TD3 is not able
to gain performance with higher learning rate and even degenerates. Composite TD3 and TD3(∆)
show less variance.
6
Under review as a conference paper at ICLR 2022
WaIker2d-v2
■ Composite TD3	■ TD3 HL
■ TD3	∙TD3(∆)
UJn20H uoβυn-3Aω
0	12	3	4
Transitions	× 105
Ujn20H UOEPsrLPSA®
WaIker2d-v2
■ Composite TD3
■ TD3
5000
0	12	3	4
Transitions	×105
3000-
2500-
2000-
1500-
1000-
500-
3500-
Ujn*H UoIγenfeAeq
0-
0	12	3	4
Transitions	×105
(a)	(b)
Figure 3: (a) Mean performance and half a SD over 8 training runs for the Walker2d-v2 environment
with the default reward. (b) Results over 8 training runs of Composite Q-learning on the vanilla
reward function and multiple update steps per collected sample for the Walker2d-v2 and Hopper-v2
environment.
In a further experiment using the true reward, we investigate the potential increase in data-efficiency
when applying multiple gradient steps per collected sample. Results over 8 training runs can be
seen in Figure 3b, with corresponding significance tests provided in Table E.1 in the appendix.
While there seems to be a limit in benefit for TD3 w.r.t. multiple gradient steps, Composite TD3 and
TD3(∆) yield a faster learning speed. In addition, Composite Q-learning has no increase in variance
opposed to all other approaches. The high learning rate in combination with multiple gradient steps
is harmful w.r.t. learning stability. Results on different truncation horizons n in the Walker2d-v2
environment are depicted in Figure 4. It can be seen that there is a trade-off between complexity of
truncated value-estimation and the faster composition of the value by the Shifted Q-functions.
Figure 4: Normalized area under the learning curve for Composite TD3 in the Walker2d-v2 envi-
ronment with different truncation horizons n. The plot shows median and interquartile ranges over
8 training runs, each representing mean evaluation performance over 100 initial states.
4.2	Noisy Reward Function
We evaluate Composite TD3 under a noisy reward function and compare to TD3, TD3 with high
Iearning rate and TD3(∆). The immediate reward is replaced by a uniform sample U 〜 U[-1, 1]
with 40% chance, corresponding to the strongest noise level described in (Romoff et al., 2019).
In order to account for the high variance, we provide mean and standard deviation over 8 runs.
Results can be seen in Figure 5, corresponding significance tests are provided in Table E.2 in the
appendix. Composite TD3 proves to be very robust, even for the very complex Humanoid-v2
environment. All other approaches suffer from slower learning speed and high variance. This
holds especially for TD3 with high learning rate and TD3(∆). We believe this to be caused by
overfitting to the noisy reward function due to the high learning rate in TD3 HL and due to the
7
Under review as a conference paper at ICLR 2022
Walker2d-v2
HoPPer-V2
Humanoid-v2
Figure 5: Mean performance and half a SD over 8 training runs for (left) Walker2d-v2, (middle)
Hopper-v2 and (right) Humanoid-v2 with uniform noise on the reward function as in (Romoff et al.,
2019).
low gamma of the first value-functions in TD3(∆). Since all other predictions only add to the
prediction of the preceding head, there is nothing to prevent the overestimation bias from being
propagated to the other outputs of higher discount value. In Composite Q-learning however,
Shifted and Truncated Q-functions estimate distinct parts of the temporal chain. This is also
highlighted in Figure 6, where the TD-errors of Shifted and Truncated Q-functions can be seen
along with learning curves of Humanoid-v2. Whilst in the default setting with vanilla reward the
Truncated Q-functions are converging much faster than the Shifted Q-function due to the simpler
problem induced by the smaller horizon, the opposite holds in case of the noisy reward. In these
experiments, the Truncated Q-functions are not capable of an accurate prediction within the given
time frame, yet the Shifted Q-functions can construct an accurate chain given those inaccurate
predictions reliably leading to a well-performing policy. This backs our hypothesis that Shifted
Q-functions can benefit from higher learning rates also in noisy reward settings whereas Truncated
Q-functions have to account for variance in the reward as well, which adds to the complexity
of the problem significantly. This translates to faster learning, as underlined by the results in Table 1.
Table 1: Mean normalized area under the learning curve and SD over 8 training runs of Composite
TD3 in the noisy reward experiments.
Method	Walker2d-v2	Hopper-v2	Humanoid-v2
TD3	68% ± 22%	76% ± 28%	68% ± 30%
TD3 (HL)	56% ± 18%	72% ± 37%	38% ± 29%
TD3(∆)	52% ± 23%	49% ± 31%	56% ± 26%
Composite TD3	100% ± 20%	100% ± 25%	100% ± 16%
Table 2 shows the mean maximum return achieved by Composite TD3, TD3 with default and high
learning rate and TD3(∆). Composite TD3 reaches significantly higher returns with lowest variance
compared to vanilla TD3 when applied to a noisy reward function, especially for the most complex
environment Humanoid-v2.
Table 2: Mean maximum return and SD over 8 training runs for the noisy reward experiments.
Method	Walker2d-v2	Hopper-v2	Humanoid-v2
TD3	3063 ± 813	2386 ± 729	4453 ± 1070
TD3 (HL)	2950 ± 866	2124 ± 1054	2506 ± 2004
TD3(∆)	2772 ± 1245	1242 ± 954	3251 ± 1687
Composite TD3	4041± 476	2931 ± 457	5019 ± 404
8
Under review as a conference paper at ICLR 2022
True Reward
■ Tr4
(b)
4000-
2000-
■ Sh4
40 4000-
φ
ffi,
U
O
得 2000-
三
>
0	1	2
× 106
0	1	2
Transitions X106
Ujn芯H Uol⅛-BAH
a
Figure 6: Performance and TD-errors on Humanoid-v2 of Truncated and Shifted Q-functions at
n = 4 w.r.t. the (a) vanilla and (b) noisy reward. Please note that TD-error here means the deviation
from the associated target.
Our experimental results show that Composite Q-learning offers a significant improvement over
traditional Q-learning methods and a great potential for real-world applications such as learning
directly on real robots, where the extraction of the immediate reward signal can be noisy due to
occlusions, complex training setups, sensor and actuator noise, or fitted reward functions.
5	Conclusion
We introduced Composite Q-learning, an off-policy reinforcement learning method that divides
learning of the full long-term value into a series of well-defined shorter prediction horizon
estimates. It combines Truncated Q-functions acting on a short horizon with Shifted Q-functions
for the remainder of the rollout. As a baseline, we further introduced and evaluated TD3(∆), an
off-policy variant of TD(∆). We showed on three simulated robot learning tasks that compositional
Q-learning methods can be of advantage w.r.t. data-efficiency compared to vanilla Q-learning
methods and that Composite TD3 outperforms vanilla TD3 by 24% - 32% in terms of area under the
learning curve. In addition, Composite Q-learning proved to be very robust to noisy reward signals
which is very important for real-world applications where the immediate reward may depend on a
stochastic next state or where the reward function is estimated from data.
Going forward, using ensemble uncertainty estimates based on the variance of all predictions from
the Composite Q-function could be of benefit in update calculation, transition sampling and explo-
ration. We further believe our method to be a better fit for non-stationary reward functions than
traditional Q-learning methods due to the flexibility provided by the divided long-term return. Rep-
resenting the farsighted return after n steps of a policy rollout in the future, Shifted Q-functions
could serve as an implicit dynamics model to further build the bridge between model-based and
model-free methods.
9
Under review as a conference paper at ICLR 2022
Reproducibility S tatement
Search space of hyperparameter optimization and final incumbents, as well as details on the opti-
mization procedure can be found in Appendix D. Code is in the supplementary.
References
Oron Anschel, Nir Baram, and Nahum Shimkin. Averaged-dqn: Variance reduction and stabiliza-
tion for deep reinforcement learning. In ICML, volume 70 of Proceedings of Machine Learning
Research,pp.176-185. PMLR, 2017.
Kristopher De Asis, Alan Chan, Silviu Pitis, Richard S. Sutton, and Daniel Graves. Fixed-
horizon temporal difference methods for stable reinforcement learning. In The Thirty-Fourth
AAAI Conference on Artificial Intelligence, AAAI 2020, The Thirty-Second Innovative Applica-
tions of Artificial Intelligence Conference, IAAI 2020, The Tenth AAAI Symposium on Educa-
tional Advances in Artificial Intelligence, EAAI 2020, New York, NY, USA, February 7-12, 2020,
pp. 3741-3748. AAAI Press, 2020. URL https://aaai.org/ojs/index.php/AAAI/
article/view/5784.
Anonymous Author(s). Adversarial skill networks: Unsupervised robot skill learning from video.
In ICRA, pp. 4188-4194. IEEE, 2020a.
Anonymous Author(s). Dynamically balanced value estimates for actor-critic methods, 2020b. URL
https://openreview.net/forum?id=r1xyayrtDS.
Philip J. Ball and Stephen J. Roberts. Offcon3: What is state of the art anyway? CoRR,
abs/2101.11331, 2021.
Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman, Jie Tang, and
Wojciech Zaremba. Openai gym. CoRR, abs/1606.01540, 2016. URL http://arxiv.org/
abs/1606.01540.
Justin Fu, Avi Singh, Dibya Ghosh, Larry Yang, and Sergey Levine. Variational in-
verse control with events: A general framework for data-driven reward definition. In
S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Garnett
(eds.), Advances in Neural Information Processing Systems, volume 31. Curran Asso-
ciates, Inc., 2018. URL https://proceedings.neurips.cc/paper/2018/file/
c9319967c038f9b923068dabdf60cfe3- Paper.pdf.
Scott Fujimoto, Herke van Hoof, and David Meger. Addressing function approximation error in
actor-critic methods. In Jennifer G. Dy and Andreas Krause (eds.), Proceedings of the 35th
International Conference on Machine Learning, ICML, volume 80 of Proceedings of Machine
Learning Research, pp. 1582-1591. PMLR, 2018.
Tuomas Haarnoja, Aurick Zhou, Pieter Abbeel, and Sergey Levine. Soft actor-critic: Off-policy
maximum entropy deep reinforcement learning with a stochastic actor. In ICML, volume 80 of
Proceedings of Machine Learning Research, pp. 1856-1865. PMLR, 2018.
Tuomas Haarnoja, Sehoon Ha, Aurick Zhou, Jie Tan, George Tucker, and Sergey Levine. Learning
to walk via deep reinforcement learning. In Proceedings of Robotics: Science and Systems, 2019.
Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In Yoshua
Bengio and Yann LeCun (eds.), 3rd International Conference on Learning Representations, ICLR
2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings, 2015. URL http:
//arxiv.org/abs/1412.6980.
Timothy P. Lillicrap, Jonathan J. Hunt, Alexander Pritzel, Nicolas Heess, Tom Erez, Yuval Tassa,
David Silver, and Daan Wierstra. Continuous control with deep reinforcement learning. In Yoshua
Bengio and Yann LeCun (eds.), 4th International Conference on Learning Representations, ICLR,
2016.
10
Under review as a conference paper at ICLR 2022
Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A. Rusu, Joel Veness, Marc G. Belle-
mare, Alex Graves, Martin A. Riedmiller, Andreas Fidjeland, Georg Ostrovski, Stig Petersen,
Charles Beattie, Amir Sadik, Ioannis Antonoglou, Helen King, Dharshan Kumaran, Daan Wier-
stra, Shane Legg, and Demis Hassabis. Human-level control through deep reinforcement learning.
Nature, 518(7540):529-533, 2015.
Tianwei Ni, Harshit S. Sikchi, Yufei Wang, Tejus Gupta, Lisa Lee, and Benjamin Eysenbach. f-irl:
Inverse reinforcement learning via state marginal matching. CoRR, abs/2011.04709, 2020.
Martin Riedmiller, Roland Hafner, Thomas Lampe, Michael Neunert, Jonas Degrave, Tom van de
Wiele, Vlad Mnih, Nicolas Heess, and Jost Tobias Springenberg. Learning by playing solving
sparse reward tasks from scratch. In Jennifer Dy and Andreas Krause (eds.), Proceedings of
the 35th International Conference on Machine Learning, volume 80 of Proceedings of Machine
Learning Research, pp. 4344-4353. PMLR, 10-15 Jul 2018.
Joshua Romoff, Peter Henderson, Alexandre Piche, Vincent Francois-Lavet, and Joelle Pineau. Re-
ward estimation for variance reduction in deep reinforcement learning. In 2nd Annual Confer-
ence on Robot Learning, CoRL 2018, Zurich, Switzerland, 29-31 October 2018, Proceedings,
volume 87 of Proceedings of Machine Learning Research, pp. 674-699. PMLR, 2018. URL
http://proceedings.mlr.press/v87/romoff18a.html.
Joshua Romoff, Peter Henderson, Ahmed Touati, Yann Ollivier, Emma Brunskill, and Joelle Pineau.
Separating value functions across time-scales. CoRR, abs/1902.01883, 2019. URL http://
arxiv.org/abs/1902.01883.
David Silver, Guy Lever, Nicolas Heess, Thomas Degris, Daan Wierstra, and Martin A. Riedmiller.
Deterministic policy gradient algorithms. In Proceedings of the 31th International Conference
on Machine Learning, ICML, volume 32 of JMLR Workshop and Conference Proceedings, pp.
387-395, 2014.
David Silver, Thomas Hubert, Julian Schrittwieser, Ioannis Antonoglou, Matthew Lai, Arthur Guez,
Marc Lanctot, Laurent Sifre, Dharshan Kumaran, Thore Graepel, et al. A general reinforcement
learning algorithm that masters chess, shogi, and go through self-play. Science, 362(6419):1140-
1144, 2018.
Emanuel Todorov, Tom Erez, and Yuval Tassa. Mujoco: A physics engine for model-based control.
In IEEE/RSJ International Conference on Intelligent Robots and Systems, IROS, pp. 5026-5033.
IEEE, 2012.
Hado van Hasselt, Arthur Guez, and David Silver. Deep reinforcement learning with double q-
learning. In Dale Schuurmans and Michael P. Wellman (eds.), Proceedings of the 30th AAAI
Conference on Artificial Intelligence, pp. 2094-2100. AAAI Press, 2016.
Christopher JCH Watkins and Peter Dayan. Q-learning. Machine learning, 8(3-4):279-292, 1992.
11
Under review as a conference paper at ICLR 2022
A	Theory of Composite Q-learning
First, we provide a theoretical foundation for Composite Q-learning. We show that the Compos-
ite Q-target is a complete representation of the full return by analyzing the Truncated Q-target in
Theorem A.1, the Shifted Q-target in Theorem A.2 and the full Composite Q-target in Theorem A.3.
Theorem A.1. Let Q1π (st, at) = rt be the one-step Truncated Q-function and Qiπ>1 (st, at) =
rt + γEt,π,M [Qiπ-1(st+1, at+1)] the i-step Truncated Q-function. Then Qiπ (st, at) represents the
truncated return Qiπ (st, at) = Et,π,M[Ptj+=it-1γj-trj].
Proof. Proof by induction. Q1π(st, at) = rt by definition. The theorem follows from induction step:
Qiπ (st, at) = rt + γ Eπ,M Qiπ-1 (st+1 , at+1)
(t+1) + (i-1)-1
=r + γE∏,M	X	γj-(t+1)rj
j=(t+1)
t+i-1
=r + γE∏,M	X γj-(t+1)rj
j=(t+1)
t+i-1
=rt + E∏,m	X YTTj
j=(t+1)
t+i-1
= Eπ,M	X γj-trj .
j=t
□
Theorem A.2. Let Q∏g(st,at) = Et,∏,M[γQπ(st+ι, at+ι)] be the one-step Shifted Q-function
and Q∏>L∞(st,at) = Et,∏,M[γQ∏τ∞(st+1,at+1)] the i-step Shifted Q-function. Then
Qrn∞(st,at) representsthe Shiftedreturn Q∏∞(st,at) = Et,∏,M[γiQπ(st+i,at+i)].
Proof. Proof by induction. Q∏=∞(st,at) = E∏,m[yQπ(st+ι,at+ι)] by definition. The theorem
follows from induction step:
Qπ∞(st,at) = E∏,M [γQi—L∞(st+1, at+1)
= Eπ,M γ(γ	Qπ (st+1+i—1, at+1+i—1))
=e∏,M [γ(Yi-IQn(St+i,at+i川
= Eπ,M γ Q (st+i, at+i) .
□
Theorem A.3. Let Qπn(st, at) = Et,π,M [Ptj+=nt -1 γj-trj] be the truncated return and
Q7n∞(st,at) = Et,∏,M[γnQ(st+n,at+n)] the shifted return. Then QK(St,at) = Qn(st,at) +
Qn∞(st, at) represents thefull return, i.e. Qπ(st, at) = Et,∏, M[Pj∞=tγj-trj].
12
Under review as a conference paper at ICLR 2022
1
2
3
4
5
6
7
8
9
10
11
Proof.
Qn (st, at) = Qn (st, at) + QTn∞(st, at)
t+n— 1
= ET,M X γj-trj + γnQT (st+n, at+n)
j=t
-t+n-1	(	∖]
=En,M〉： γj rrj + Yn(Qn (st+n,at+n) + Q<∞ (st+n,at+n) J ।
t+n—1	tt+2n — 1	∖ ]
=En,M	X YjTrj + Yn	X γj-t-nrj + YnQn (st+2n, at+2n)	I
j=t	j=t+n
t+n—1	t+2n—1
=E∏,m	X Yj-trj+ X Y j-trj + Y2nQn (st+2n,at+2n)
j=t	j=t+n
t+2n — 1
= En,M	X Y	rj + Y Q (st+2n, at+2n) .
j=t
By induction, it follows Qn(st,at) = E∏,m [P∞=t Yj-trj].	□
B	Tab ular Composite Q-learning
In Algorithm B.1 you find a description of tabular Composite Q-learning as used in our experiments
in Appendix C.
Algorithm B.1: Tabular Composite Q-learning
initialize Truncated Q-functions Qi
initialize Shifted Q-functions Qi∞
initialize Q-function Q
for episode = 1..E do
get initial state s1
for t = 1..T do
apply -greedy action at
observe st+1 and rt
update Truncated Q-functions by:
Qι(st,at) — (1 - aT∙r)Qι(st,at) + ατrrt
Qi>ι(st,at) — (1 - aT∙r)Qi(st,at) + αTγ(rt + γQi-1(st+1, argmaXa Q(st+ι,a)))
update Shifted Q-functions by:
Qι∙∞(st,at) — (1 - aSh)Qr∞(st,at) + αSh(γmaxa Q(st+ι,a))
Q(i>1):∞(st, at) — (I - aSh)Qi:g (st, at) + αSh(γQ(i-1)m(st + 1, arg maxa Q(st + 1, O)))
update Q-function by:
Q(st,at) — (1 - ɑQ)Q(st,at) + αQ(rt + γ(Qn(st+ι, argmaXa Q(st+ι,a))
L	+Qnm(st+ι, argmaxa Q(st+1, a))))
C Evaluation of Tab ular Composite Q-learning
C.1 Deterministic Chain
We evaluate the effect of composing the long-term return of multiple short-term predictions in the
tabular setting. We apply Composite Q-learning in the tabular case to the MDP of horizon K given
in Figure C.1a. We compare Composite Q-learning to vanilla Q-learning, as well as multi-step
Q-learning based on subtrajectories of the exploratory policy and imaginary rollouts of the target-
policy with the true model of the MDP as a hypothetical lower bound on the required updates.
13
Under review as a conference paper at ICLR 2022
c, -3
b,-2'
c, -3
sK-3
a, -1
a, -1
a, -1
a, -
c, —3/
CT 卜-2
c,
a, -100
■ Composite Q-learning
■ Multi-step Q-learning w/ True Model
■ On-policy Multi-step Q-learning
■ Q-learning
0 5 0 5
112 2
- - - -
S) IOJnIBA占
2
Updates
4
×106
(a)
2	4	0
Updates	×106
(b)
Figure C.1: (a) In this deterministic-chain MDP of horizon K, the agent ought to arrive at terminal
state sK-1 using actions {a, b, c}. The initial state is s0 and the optimal policy is given in red. (b)
Mean results and two standard deviations over 10 runs on the MDP with a horizon of K = 20. The
left plot depicts the value of s0 and action a as estimated by the different approaches over time.
Dashed lines indicate convergence to the optimal policy. The predicted Truncated Q-values for state
s0 and action a with horizons 1 to 4, denoted by Tr1 , . . . , Tr4, and predicted Shifted Q-values for
state s0 and action a, denoted by Sh1, . . . , Sh4, are to the right. Dotted lines indicate the true optimal
respective Q-values.
Results for K = 20 are depicted in Figure C.1b, where mean performances of the approaches
are shown on the left and the intermediate predictions of Composite Q-learning on the right. All
approaches update the Q-function with a learning rate of 10-3 on the same fixed batch of 103
episodes with a percentage of 10% non-optimal transitions. We set rollout length n = 4 for all
respective approaches. Please note that this value can be set arbitrarily, however in order to be
beneficial it should be set to a value smaller than the temporal horizon of the task. In this example,
a rollout length of 4 corresponds to the integer square root of the horizon of the MDP, meaning
that Truncated and Shifted Q-functions have a similar temporal horizon due to the partitioning of
the long-term value. An evaluation of the performance for different rollout lengths n is shown in
Figure C.2.
■ n = 2	n = = 8
■ n = 4	■ n = 16
-
20
-
0 XI0Is3 PXl3 Oss3sIOJ 3n"A,σ
-25-
0	1	2	3	4	5
Updates	× 109
Figure C.2: Performance of Composite Q-learning applied to the MDP in Figure C.1a with different
rollout lengths n.
14
Under review as a conference paper at ICLR 2022
For the experiments in Figure C.1b, we update the Shifted Q-function with a learning rate of 10-2
and the Truncated Q-functions with a learning rate of 10-3. Composite Q-learning converges much
faster to the true action-value than vanilla Q-learning and is close to the multi-step approach based
on model rollouts (hypothetical lower bound). The erroneous updates of on-policy multi-step Q-
learning lead to convergence to a wrong action-value which is underlining the importance of truly
off-policy learning. Composite Q-learning estimates the short-term rollouts off-policy by design
and shifts the long-term value in time so as to have a consistent temporal chaining. All intermediate
predictions therefore converge to the true intermediate action-values, which is shown on the right
side of Figure C.1b. The difference in convergence speed between Q-learning and Composite Q-
learning grows with increasing horizon, as shown in Table C.1 for the described hyperparameter
setting.
Table C.1: Comparison of convergence speed between Tabular Q-learning and Tabular Composite
Q-learning for exemplary runs on the MDP given in Figure C.1a with n = 4.
Horizon K	10	20	50	100
Speed up over Q-learning	11%	44% 57%	66%
Next, we present an evaluation of different learning rates for the Shifted Q-functions, depicted in
Figure C.3a. Here we compare Composite Q-learning (CQL) and vanilla Q-learning to Shifted
Q-learning (SQL), which corresponds to Q-learning with a one-step shifted target (replacing the
bootstrap of the future return with an evaluation of a one-step Shifted Q-function and without ap-
proximate n-step returns from a Truncated Q-function). The results show that shifting the value in
time alone is slowing down convergence. Precisely, Shifted Q-learning with a learning rate of 1.0
is equivalent to vanilla Q-learning; the same holds for Composite Q-learning with a learning rate of
10-3 for the Shifted Q-function.
(»6a) JOJ 3nfsΛ,σ
■ CQL (10-3)
■ CQL (10-2)
・ CQL (10T)
0-∏
■ CQL (100)
■ SQL (10-3)
■ SQL (10-2)
0
5 0 5 c
-II 2
- - - -
(6JOJ 3nfsΛ,
■ CQL (10-3)
■ CQL (10-2)
5 0 5 0
-II 2
- - - -
(6S) JOJ 3nfsΛ,
・CQL(10T)
・ CQL (100)
0-
■ Q-learning
5 0 5
- 1 1
- - -
(Jo3nfsΛ,
-25
0	2	4
Updates	×106
(»6s) JOJ 3nfsΛ,σ
0	2	4
Updates	×106
5 0 5 0
- Il 2
- - - -
6s) JOJ 3nfsΛ,
0	2	4
Updates	×106
»6s) JOJ 3nfsΛ,σ
:..............................]	- 20 ■
0	2	4
Updates	×106
0	2	4
Updates	×106
(b)
0	2	4
Updates	× 106
(a)
Figure C.3: Results of 4 individual runs on the deterministic chain MDP with a horizon of K = 20
for Composite and Shifted Q-learning, (a) with different learning rates for the Shifted Q-function
and (b) with different learning rates for the Truncated Q-function (denoted by the numbers in paren-
theses). The learning rates for the full Q-function and for the (a) Truncated Q-functions and the (b)
Shifted Q-functions are set to 10-3 in all experiments.
Please note that although the learning rate of Q-learning could be set to a higher value for this very
MDP, this example shows the benefit of Composite Q-learning even though higher learning rates
for Shifted Q-functions alone do not bring any speedup in convergence. Only the combination of
Shifted Q-functions and Truncated Q-functions lead to an increase of convergence speed, even in
the case when Truncated and full Q-function are estimated with the same (slow) learning rate of
15
Under review as a conference paper at ICLR 2022
10-3 . The results underline that the higher learning rates for the Shifted Q-functions only have a
beneficial effect in combination with truncated predictions.
The counterpart with different learning rates for the Truncated Q-functions, keeping the learning
rates for the full Q-estimate and Shifted Q-functions fixed, can be seen in Figure C.3b. While there
is improvement in convergence using a larger learning rate for the Truncated Q-functions, the results
show higher variance and less benefit than the Shifted Q-functions in Figure C.3a.
C.2 Stochastic Chain
We further investigate the effect of stochastic immediate rewards on Composite Q-learning on the
chain MDP shown in Figure C.4. All actions lead further into the chain, however they yield different
expected rewards. Action a has an expected reward of -0.8, corresponding to an immediate reward
of -1 with 80% chance and 0 with 20%. Action b, on the other hand, has an expected reward of
-1.01, however it is +1 with 99% chance and -200 with 1%. Composite Q-learning with learning
rates of 10-2 for the full Q-function, 10-1 for the Shifted Q-functions and 10-3 for the Truncated
Q-functions is compared to Q-learning with the same respective learning rates.
0.99, +1
.01, -200
.01, -200
.01, -200
Figure C.4: In this stochastic chain MDP of horizon K, the agent ought to arrive at terminal state
sK-1 using actions a and b. The initial state is s0. Transitions are stochastic. The optimal policy is
given in red.
Results for different horizons and no discount are depicted in Figure C.5. It can be seen that Q-
learning with a high learning rate is converging to a wrong action-value and the deviation becomes
more significant the longer the horizon of the task. The same holds for Q-learning with a learning
rate of 10-2, however the deviation becomes smaller. Moreover, Q-learning with the same learning
rate as the full Q-function in Composite Q-learning is converging much slower for growing horizons,
Q-learnmg (10T)
・ Q-learnmg (10-2)
■ Q-learning (10-3)	■ Composite Q-learning (10-2,10-1,10-3)
O8XIOZaOH PXl∙,(o~0s) IOJ 3nI∙,A,σ
8mXIOZaOH PXl∙,(o~0s) IOJ 3nI∙,A,σ
8≡XIOZII。H Pxre (o=S) IOJ 3n"A,σ
Figure C.5: Results of Composite Q-learning and Q-learning with different learning rates over 5
training runs for horizons (left) 200, (middle) 300 and (right) 1000 and no discount. The true action-
value is indicated by the dashed line.
16
Under review as a conference paper at ICLR 2022
yet the learning rate for the Truncated Q-functions is 10 times smaller than for Q-learning. This
emphasizes the mutual interplay of Truncated and Shifted Q-functions. Furthermore, Composite
Q-learning is converging to the true optimal action-value. Q-learning with a small learning rate of
10-3 is far from achieving a satisfying level of convergence within the time frame of 2 ∙ 107 updates.
Shifted Q-functions thus allow for higher learning rates which in combination with a more cautious
fitting of short-term predictions leads to increased data efficiency while achieving a smaller deviation
from the true action-value. Most importantly, the difference in performance between Q-learning and
Composite Q-learning (better value estimation w.r.t. to a high learning rate and better convergence
properties comparing to a small learning rate) becomes larger for longer horizons. The performance
for different rollout lengths are shown in Figure C.6.
-O 3OIs∙,PxreOs 3≡sIna3nI∙,A,σ
■ n= 2	n = = 8
■ n = 4	■ n = 16
二
-	40-
-	60-
-	80-
-	100-
-	120-
-	140-
-	160-
0.0	0.2	0.4	0.6	0.8	1.0
Updates	× 107
Figure C.6: Performance of Composite Q-learning applied to the MDP in Figure C.4 with different
rollout lengths n.
D	Hyperparameter Setting and Optimization
D. 1 Entropy Regularization
Each pair QTr + QSh∣ι≤i≤n is a complete approximation of the true Q-value. Note however, that
it is also bootstrapped with the full Q-value. The circular dependency can lead to stability issues,
due to the amplification of propagated errors. Additionally, higher learning rates for the Shifted
Value-functions, as motivated in Section 3.1.2, may lead to overfitting. In particular, errors in the
truncated predictions are propagated quickly by the Shifted Q-functions, especially when trained
with a high learning rate. Therefore, it is important to keep the truncated predictions close across
the different time steps while also preventing the Shifted Q-functions from running into suboptimal
local minima. As a means to alleviate these issues, we add a regularization based on the entropy ofa
Gaussian distribution formed by mean μ(QT + QSh∣ι≤i≤n) and variance σ2(QTr + QSh∣ι≤i≤n) over
all n complete predictions of Q. We can then formalize an incentive of the Truncated Q-functions
to stay close between predictions whilst forcing the Shifted Q-functions to keep the distribution as
broad as possible. We define the Gaussian distribution NQ over all Q-predictions therefore by:
NQ (μ(QTr + QShl1≤i≤n} σ2(QTr + QShl1≤i≤nD ,	(12)
with corresponding entropy:
Hq = ∣log (2∏eσ2(QTr + QSh∣ι≤i≤n)).	(13)
In order to enhance stability of the learning process as a whole while preventing the Shifted Q-
functions from overfitting, we not only minimize the mean squared error between targets yjC and
predictions QC(sj, aj∣θC), but apply gradient descent on the entropy for the parameters of QTr and
gradient ascent on the parameters of QSh . More detailed, we define the squared error for some
sample j as:
δj = (yC - QC(Sj,aj ∣ΘC))2,	(14)
17
Under review as a conference paper at ICLR 2022
y
2 fully connected layers (500, leaky ReLU)
人
Figure D.1: Optimized architecture of the Composite-Q network used in our experiments.
and the gradient w.r.t. all parameters of QC including the different learning rates as:
ξj= αQδjVθQQC(Sj, aj∣θc) +。t向口皮(Sj,aj ∣θc) + °ShδjVθShQC(Sj, aj∣θc).	(15)
The regularization then adds to the gradient:
ηj = H(Sj , aj)(αShβShVθSh - αTrβTrVθTr).	(16)
Hence, the parameter update in its simplest form becomes:
θC - θc + ɪ X(ξj- η).	(17)
m
j
However, a more sophisticated optimizer such as Adam (Kingma & Ba, 2015) can be applied anal-
ogously.
D.2 Hyperparameter Setting
For all approaches, we use Gaussian noise with σ = 0.15 for exploration and the optimized
learning rate of 10-3 for the full Q-function. Target update (5 ∙ 10-3) and actor setting (two
hidden layers with 400 and 300 neurons and ReLU activation) are set as in (Fujimoto et al., 2018).
For Humanoid-v2, we use a slightly changed parameter setting with a learning rate of 10-4 for
both actor and critic as suggested in (Author, s). For Composite Q-learning, we calculate the full
Q-values, as well as Truncated and Shifted-Q values in one, combined, architecture for improved
efficacy. Additionally, we found that it is beneficial to estimate the Truncated and Shifted Q-values
in different layers, as depicted in Figure D.1. For all parameters in the layers prior to the full
Q-output, we use the parameter setting as described above.
Hyperparameters are optimized for all approaches, including the baselines TD3 and TD3(∆), for the
given subset of MuJoCo tasks via Random Search over 10 training runs with the configuration space
shown in Table D.1. Learning rates for the corresponding output layers of the Truncated and Shifted
Q-values are optimized individually. We use a learning rate of 6 ∙ 10-5 for the Truncated Q-functions
and 5 ∙ 10-3 for the Shifted Q-functions. For the noisy experiments, as well as for the experiments
with multiple gradient steps per collected sample, we set n = 4. For the Walker2d-v2 experiment
on the true reward function, we set n = 10. In terms of regularization, we set βTr = 0.002 and
βSh = 0.001. For Humanoid-v2, we set the optimized learning rates one magnitude lower. When
executing multiple gradient steps per collected sample, we keep the learning rate of the Shifted Q-
functions at 10-3. We further evaluate TD3 with the same maximum learning rate. We optimize
the number of layers and the number of neurons per layer for the critic in Composite Q-learning
(4 layers with 500 neurons and leaky ReLU activation) and the architecture for the critic in all
18
Under review as a conference paper at ICLR 2022
other approaches (two layers with 500 neurons and leaky ReLU activation). For an overview of the
architecture of the Composite-Q network see Figure D.1. For TD3(∆), we use the γ-schedule as
suggested by Romoff et al. (2019), i.e. γι = 0 and γi>ι = γi-2+1, with an upper limit of 0.99.
Table D.1: Configuration space of the hyperparameter optimization. For the full Q-function in all
approaches, we used the optimized learning rate of 10-3 as in (Fujimoto et al., 2018). Hyperparam-
eters denoted by * were optimized individually for all approaches.
Hyperparameter	Values
number of layers*	{2, 3, 4}
number of neurons*	{300, 400, 500}
activation*	{ReLU, leaky ReLU}
αTr	{6 ∙ 10-6,10-5,6 ∙ 10-5,10-4,10-3}
αSh	{10-3, 2 ∙ 10-3, 5 ∙ 10-3, 10-2}
βTr	{10-3, 2 ∙ 10-3, 4 ∙ 10-3}
βSh	{5∙10-4,10-3, 2 ∙10-3}
n	{3, 4, 10, 15, 20}
D.3 Analysis of Hyperparameters
The influence of the horizon of Truncated and Shifted Q-functions, as well as the respective regu-
larization weights maximizing and minimizing the entropy, is given exemplary for the Walker2d-v2
environment in Figure D.2.
It can be seen that the regularization of the Shifted Q-functions has a higher influence on the perfor-
mance. If the weight is set too high, the variance increases significantly. The same holds for a too
long temporal horizon. The regularization of the Truncated Q-functions has less influence on conver-
gence and variance, however, please note the strong connection of these hyperparameters. Therefore,
the best triad setting needs to be found for a given problem and yields an interesting direction for
meta-reinforcement learning. Prior experiments with other regularization terms only minimizing the
entropy also showed enhanced stability, however, it is important to prevent the Shifted Q-functions
from overfitting when applying higher learning rates in the TD3 setting.
TΛ^TI
if
而一
-■■ ,
9 8 7 6 5 4 3
■ ■■■■■■
Ooooooo
3AJnοTIπm33J3pτm
Uv P∙3zyp3uxlοN
■■
3	5	10	15	20	0.001	0.002	0.004	0.0005	0.001	0.002
n	βTr	βSh
Figure D.2: Normalized area under the learning curve for Composite TD3 in the Walker2d-v2 envi-
ronment with different truncation horizons n (left) and different regularization weights βTr (middle)
and βSh (right). The plots show median and interquartile ranges over 8 training runs, each represent-
ing mean evaluation performance over 100 initial states.
19
Under review as a conference paper at ICLR 2022
E S ignificance Tests
We provide significance tests on the mean area under the learning curve (to account for learning
speed and stability) between Composite TD3 and TD3, TD3 (HL) and TD3(∆). Significance tests
for the vanilla reward experiments can be found in Table E.1.
Table E.1: p-values for Welch’s t-test on the mean area under the learning curve for 8 different runs
of Composite TD3 and all other approaches in the vanilla reward experiments. Subparts (a) and (b)
correspond to those in Figure 3.
	TD3	TD3 (HL)	TD3(∆)
Walker2d-v2 (a)	8∙10-3	7∙10-4	6∙10-2
Walker2d-v2 (b)	3∙10-4	2∙10-2	5 ∙ 10T
Hopper-v2 (b)	3∙10-5	4∙10-2	5 ∙ 10T
Significance tests for the noisy reward experiments can be found in Table E.2.
Table E.2: p-values for Welch’s t-test on the mean area under the learning curve for 8 different runs
of Composite TD3 and all other approaches in the noisy reward experiments.
	TD3	TD3 (HL)	TD3(∆)
Walker2d-v2	4∙10-5	2∙10-8	6∙10-8
Hopper-v2	8∙10-3	8∙10-3	2∙10-6
Humanoid-v2	3∙10-2	4∙10-4	5∙10-6
20