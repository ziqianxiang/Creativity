Under review as a conference paper at ICLR 2022
Initializing ReLU networks in an expressive
SUBSPACE OF WEIGHTS
Anonymous authors
Paper under double-blind review
Ab stract
Using a mean-field theory of signal propagation, we analyze the evolution of cor-
relations between two signals propagating forward through a deep ReLU network
with correlated weights. Signals become highly correlated in deep ReLU networks
with uncorrelated weights. We show that ReLU networks with anti-correlated
weights can avoid this fate and have a chaotic phase where the signal correla-
tions saturate below unity. Consistent with this analysis, we find that networks
initialized with anti-correlated weights can train faster by taking advantage of the
increased expressivity in the chaotic phase. An intiialization scheme combining
this with a previously proposed strategy of using an asymmetric initialization to
reduce dead node probability shows consistently lower training times compared to
various other initializations on synthetic and real-world datasets. Our study sug-
gests that use of initial distributions with correlations in them can help in reducing
training time.
1	Introduction
Rectified Linear Unit (ReLU) Fukushima (1969); Fukushima & Miyake (1982) is the most widely
used non-linear activation function in Deep Neural Networks (DNNs) LeCun et al. (2015); Ra-
machandran et al. (2018); Nair & Hinton (2010), applied to various tasks like computer vision Glorot
et al. (2011b); Krizhevsky et al. (2012); He et al. (2015), speech recognition Maas et al. (2013); Toth
(2013); Hinton et al. (2012), intelligent gaming Silver et al. (2016), and solving scientific problems
Seif et al. (2019). ReLU, φ(x) = max(0, x), outperforms most of the other activation functions
proposed Glorot et al. (2011a). It has several advantages over other activations. ReLU activation
function is computationally simple as it essentially involves only a comparison operation. ReLU
suffers less from the vanishing gradients, a major problem in training networks with sigmoid-type
activations that saturate at both ends Glorot et al. (2011b). They generalize well even in the overly
parameterized regime Maennel et al. (2018).
Despite its success, ReLU also has a few drawbacks, one of which is the dying ReLU problem He
et al. (2015); Trottier et al. (2017). The dying ReLU is a type of vanishing gradient problem in
which the network outputs zero for all inputs and is dead. There is no gradient flow in this state.
ReLU also suffers from exploding gradient problem, which occurs when backpropagating gradients
become large Hanin (2018).
Several methods are proposed to overcome the vanishing/exploding gradient problem; these can
be classified into three categories Lu et al. (2020). The first approach modifies the architecture,
which includes using modified activation functions Ramachandran et al. (2018); He et al. (2015);
Trottier et al. (2017); Clevert et al. (2016); Klambauer et al. (2017); Hendrycks & Gimpel (2016),
adding connections between non-consecutive layers (residual connections)He et al. (2016), and op-
timizing network depth and width. The proposed activations are often computationally less efficient
and require a fine-tuned parameter Lu et al. (2020). The second approach relies on normalization
techniques Ba et al. (2016); Ioffe & Szegedy (2015); Salimans & Kingma (2016); Ulyanov et al.
(2016); Wu & He (2018), the most popular one being batch normalization Ioffe & Szegedy (2015).
Batch normalization prevents the vanishing and exploding gradients by normalizing the output at
each layer but with an additional computational cost of up to 30% Mishkin & Matas (2016). A re-
lated strategy involves using the self-normalizing activation (SeLU), which by construction ensures
output with zero mean and unit variance Klambauer et al. (2017). The third approach focuses on the
1
Under review as a conference paper at ICLR 2022
initialization of the weights and biases. As local (gradient-based) algorithms are used for optimiza-
tion Kingma & Ba (2015); Zeiler (2012); Duchi et al. (2011), it is challenging to train deep networks
with millions of parameters Du et al. (2019); Srivastava et al. (2015), and optimal initialization is
essential for efficient training Nesterov (2014). He-initialization He et al. (2015) is a commonly
used strategy that uses uncorrelated Gaussian weights with variance N, where N is the width of
the network. Recently, Lu et al. (2020) proposed Random asymmetric initialization (RAI), which
reduces the probability of dead ReLU at the initialization. In this paper, we aim to further improve
the initialization scheme for ReLU networks.
A growing body of work has analyzed signal propagation in infinitely wide networks to understand
the phase diagram of forward-propagation in DNNs Saxe et al. (2014); Poole et al. (2016); Raghu
et al. (2017); Schoenholz et al. (2017); Lee et al. (2018); Hayou et al. (2019a); Li & Saad (2018;
2020); Bahri et al. (2020). We mention a few results for ReLU networks. Hayou et al. (2019a)
showed that correlations in input signals propagating through a ReLU network always converge to
one. Many other works found that ReLU networks are in general biased towards computing simpler
functions De Palma et al. (2019); Rahaman et al. (2019); He et al. (2020); Valle-Perez et al. (2019);
Hanin & Rolnick (2019), which may account for their better generalization properties even in the
overly parameterized regime. However, from their successful application in different domains, one
may guess that they should be capable of computing more complex functions. There might be a
subspace of the parameters where the network can represent complex functions.
Li & Saad (2018; 2020) applied weight and input perturbations to analyze the function space of
ReLU networks. They found that ReLU networks with anti-correlated weights compute richer func-
tions than uncorrelated/positively correlated weights. Consistent with this, Shang et al. (2016) found
that ReLU CNN’s produce anti-correlated feature matrices after training. These studies motivated us
to analyze the phase diagram of signal propagation in ReLU networks with anti-correlated weights.
Following the mean-field theory of signal propagation proposed by Poole et al. (2016), we found that
ReLU networks with anti-correlated weights have a chaotic phase, which implies higher expressivity.
In contrast, ReLU networks with uncorrelated weights do not have a chaotic phase. Furthermore,
we find that initializing ReLU networks with anti-correlated weights results in faster training. We
call it Anti-correlated initialization (ACI). Additional improvement in performance is achieved by
incorporating RAI, which reduces the dead node probability. This combined scheme, which we
call Random asymmetric anti-correlated initialization (RAAI), is the main result of this work and is
defined as follows. We pick weights and bias incoming to each node from anti-correlated Gaussian
distribution and replace one randomly picked weight/bias with a random variable drawn from a beta
distribution. The code to generate weights drawn from the RAAI distribution is given in Appendix G.
We analyze the correlation properties of RAAI and show that it performs better than the best-known
initialization schemes on tasks of varying complexity. It may be of concern that initialization in an
expressive space may lead to overfitting, and we do observe the same for ACI for deeper networks
and complex tasks. In contrast, RAAI shows no signs of overfitting and performs consistently better
than all other initialization schemes.
We organize the article as follows. First, we contrast the mean-field analysis of ReLU networks with
correlated weights with uncorrelated in Section 2. Next, Section 3 analyzes the critical properties of
correlations in input signals for RAI and RAAI. Then, in Section 4, we describe the various tasks
used to validate the performance of different initialization schemes in Section 5. Lastly, Section 6
concludes the article.
2	Mean-field analysis of signal propagation with correlated
WEIGHTS
This section presents the mean-field theory of signal propagation (proposed by Ref. Poole et al.
(2016)) in ReLU networks with correlated weights and compares it with uncorrelated weights. Un-
like Ref. Li & Saad (2018; 2020), which study perturbation to a ReLU network, we aim to under-
stand the phase diagram of the signal propagation. Furthermore, we provide numerical results to
corroborate the mean-field results.
Consider a fully connected neural network with L layers (in addition to the input layer) and Nl
nodes in layer l. The layer index ranges between 0 and L. For an input signal s0 = x, we denote
2
Under review as a conference paper at ICLR 2022
the pre-activation at node i in layer l by hli (x) and activation by sli(x). A signal (sl1-1, . . , slN-l1) at
layer l - 1 propagates to layer l by the rule
Nl-1
hli(x) = X wiljslj-1(x) + bli	where l ∈ {1, L}
j=1
sli(x) = φ(hli(x)),
where φ is the non-linear activation function and wilj , bli are the weights and biases. We consider
correlations within the set of weights (wil ) incoming to each node i. The correlated Gaussian distri-
bution is
Nl
P (w1l , w2l,w3l ...) =
i
e(-1 (Wi )T ATwi)
P(2π)Nι-ι |A|
2
With A = —ɪ
Nl-1
I-
k J)
ι+k NnJ
(1)
Where I is the identity matrix, J is an all-ones matrix, and k parameterizes the correlation strength.
Positively correlated and anti-correlated regimes correspond to the regions -1 < k < 0 and k > 0,
respectively, Whereas k = 0 generates uncorrelated Weights. The overall scaling by 1/Nl-1 in the
covariance matrix ensures that the input contribution from the last layer to each node is O(1). The
bias is drawn from a Gaussian distribution bi 〜N(0, σb). Note that weights reaching two different
nodes are uncorrelated, and also the bias is uncorrelated With the Weights.
To track the layer-wise information flow, consider the squared length and overlap of the pre-
activations for two input signals, s0 = x1 and s0 = x2 , after propagating to layer l
1 Nl
qh(Xa) = N E (hi(xa))	Where a ∈ {1,2} and 1 ≤ l ≤ L
Nl i=1
1	Nl
qh (χι,χ2) = NEhll(XI )hi(x2).
Nl i=1
Assuming self averaging, consider an average over the weights and the bias incoming to layer l. For
simplicity of notations later, we use the same symbol for averaged qhl .
σ2 Nl-1	k 1
qh (Xa) = wf- E (δj,m - T~τ φ(- )φ(hjT (Xa))φ(hM1(Xa)) + σl
Nl-1 j,m=1	1+kNl-1
2	Nl-1	k	1
qh(xi,x2) = ~^w~ E ( δj,m - γ~τφ(—) φ(hjT(XI))θ(h-r2)) + σ2,
Nl-1 j,m=1	1+kNl-1
For large width, each hli-1 is a weighted sum of a large number of zero-mean random variables.
Thus, we expect the joint distribution of hli-1(X1) and hli-1(X2 ) to converge to a zero-mean Gaus-
sian with a covariance matrix with diagonal entries qhl-1(X1), qhl-1(X2) and off-diagonal entries
qhl-1(X1, X2 ). On replacing the average over hli-1 (this is equivalent to considering an average
over all previous layers) in the last layer with an average over this Gaussian distribution, we obtain
iterative maps for the length and overlap. Specializing to ReLU activation yeilds the equations
qh(X) = σw (1 - l+⅛1) ql-1(X) + σ2
qh(X1,x2)= wj- (f (Ch-I) - [ K 不)q∕ql-1 (XI)qh-1 (X2)+ σ2
2	1+kπ
(2)
cl-1	cl-1	1 /------
f(CI-I)=」2— +	SinT(CI-I) + ∏ J1 - (CI-I)2,
3
Under review as a conference paper at ICLR 2022
where Ch =	qh (X1,x22= is the correlation coefficient between the two signals reaching layer l (for
h	qhl (x1)qhl (x2)
details of the derivation, see Appendix A).
Poole et al. (2016) found that the signal’s length reaches its fixed point within a few layers, and the
fixed point of the correlation coefficient, Ch can be estimated with the assumption that qh(x) has
reached its fixed point qh. We can check that Ch = 1 is always a fixed point of the recursive map
(Eqn. 2) under this assumption. The stability of the fixed point Ch = 1 is determined by
χ1 ≡ J
χ1≡ ∂Ch-1 Ich-1=1,
σ2
which evaluates to -w-. χι separates the parameter space into two phases —first, an ordered phase
with χι < 1, where the Ch = 1 fixed point is stable; and second, a chaotic phase with χι > 1,
where the Ch = 1 fixed point is unstable. χι = 1 defines the phase boundary line. In the ordered
phase, two distinct signals will become perfectly correlated asymptotically. In the chaotic phase, the
correlations converge to a stable fixed point below unity. Two closely related signals will eventually
lose correlations in this phase. This suggests that initializing the network with parameters (σw2 , σb2)
at the phase transition boundary (corresponding to an infinite depth of correlations) allows for an
optimal information flow through the network Poole et al. (2016); Schoenholz et al. (2017)

σ2
(b)
σ2
(a)
Figure 1: Phase diagram for ReLU networks with uncorrelated and anti-correlated Gaussian dis-
tributed weights. (a) ReLU networks with uncorrelated weights have two phases. First, a bounded
phase where qh is finite and second in which it diverges. The two phases are separated by σW =2.
In both phases, any two signals will eventually become correlated. (b) ReLU networks with anti-
correlated weights have three phases. In addition to the transition between the bounded and un-
bounded phases (at σw2 = gk) there is an order to chaos transition at σw2 = 2. The results are shown
for k = 100.
ReLU networks with uncorrelated weights (k = 0) The above analysis is applied assuming qhh
is finite shows that ReLU networks with uncorrelated weights do not have a chaotic phase, and any
two signals propagating through a ReLU network become asymptotically correlated for all values of
(σw2 , σb2). In other words, Chh = 1 is always a stable fixed point. However, the parameter space can
be classified into two phases based on the boundedness of the fixed point qhh of the length map (Eqn.
2) - first, a bounded phase where qhh is finite and non-zero; second, an unbounded phase, where qhh
is either zero or infinite Lee et al. (2018); Hayou et al. (2019b). The two phases are separated by
the boundary σw2 = 2. Figure 1a depicts the phase diagram for ReLU networks with uncorrelated
weights. Note that the analysis of the stability of Chh = 1 fixed point in ReLU networks is valid only
in the bounded phase. However, numerical results presented in Fig. 2 indicate that the fixed point
remains stable even in the unbounded phase.
ReLU networks with correlated weights The phase diagram for ReLU networks with correlated
weights can be analyzed similarly. The length is bounded if σW < gk = τ2k ι.. Thus, for anti-
(I- 1+k∏ )
4
Under review as a conference paper at ICLR 2022
Figure 2: The above plots show the signal’s length and correlation coefficient after propagating
through l layers in a ReLU network with uncorrelated weights. We estimate the length and corre-
lation coefficient averaged over M = 1024 input signals, and 40 networks with a constant width
N = 2048. The shaded regions denotes the standard deviation. In the first panel, the vertical dashed
line indicates the theoretical phase boundary σw2 = 2, and the solid black line denotes the theoretical
prediction for the length’s fixed point. As the critical boundaries do not depend on the variance of
the bias, we show results for σb2 = 0.1 only. We find that clh → 1 for all values of σw2 and σb2.
0.5
1.0
σW
1.5
number of
layers l
----1
——2
....8
----128
----256
----512
2.0
6
number of
layers l
----1
----2
....8
——32
----128
----256

σW	σW
Figure 3:	The above plots show the signal’s length and correlation coefficient after propagating
through l layers in a ReLU network with anti-correlated weights with a correlation strength k = 100.
We estimate the length and correlation coefficient averaged over M = 1024 input signals, and 40
networks with a constant width N = 2048. The shaded regions denotes the standard deviation. The
vertical dashed lines indicate the theoretical phase boundaries at σw2 = 2.92 and σw2 = 2.0 for qhl (x)
and clh(x). The solid black line in the first panel denotes the theoretical prediction for the length’s
fixed point. As the critical boundaries do not depend on the variance of bias, we show results for
σb2 = 0.1. Unlike the case of uncorrelated weights, we find a chaotic region.
correlated weights (k > 0), the boundary gk moves upwards relative to the k = 0 case (see Fig. 1a).
The Ch = 1 fixed point of the correlations is unstable in this region of the bounded phase.
In summary, anti-correlations induce a bounded chaotic phase in 2 < σw2 < gk (see Fig. 1b). We
demonstrate these results numerically in Fig. 3 for a correlation strength of k = 100. As predicted
by the above equations, the stability of the fixed point Ch = 1 changes at σW = 2, and the length
diverges at gk=100 = 2.92. In contrast, for positively correlated weights, the length’s fixed point
boundary shifts downward resulting in a similar phase diagram as uncorrelated weights.
As a result, a ReLU network with anti-correlated weights can be more expressive by taking ad-
vantage of a chaotic phase, and it may be beneficial for a ReLU network to remain in this sub-
space. Thus, we propose initializing ReLU networks with anti-correlated weights at the order to
5
Under review as a conference paper at ICLR 2022
chaos boundary (σw2 , σb2) = (2, 0). We call it Anti-Correlated Initialization (ACI). Appendix B
demonstrates that ReLU networks initialized with anti-correlated weights give an advantage over
He initializations for a range of tasks.
Many alternatives are proposed to improve ReLU networks Trottier et al. (2017); Lu et al. (2020);
Clevert et al. (2016). Of particular interest is Random asymmetric initialization (RAI), which aims to
increase expressivity through an independent strategy of reducing the dead node probability. In the
next section, we analyze the correlation properties of RAI and then combine it with ACI to propose
a new initialization scheme RAAI, which has both a chaotic phase and low dead node probability.
3 Random Asymmetric Anti-correlated Initialization
We begin by analyzing critical properties of Random asymmetric initialization (RAI) proposed in
Lu et al. (2020) to reduce the dead node probability. For ReLU networks with symmetric distribu-
tions for weights and biases, the dead node probability is half. RAI reduces it by initializing one of
the weights/the bias incoming to each node from a distribution with positive support (like the beta
distribution), resulting in a positive mean for the pre-activations. Lu et al. (2020) proposes initializ-
ing RAI with a variance σw2 = 0.36 to ensure that the signal’s length is bounded. We analyzed the
correlation properties of RAI and found that Ch = 1 is always a fixed point of the recursive maps
(See Appendix C). Deriving the stability condition for the fixed point Ch = 1 even with the mean-
field assumptions is difficult. However, numerical results presented in Figure 4 show that Ch = 1 is
always a stable fixed point (right panel), and the length remains finite for σw2 up to 0.72 (left panel).
A qualitative picture of the phase diagram can be captured with additional assumptions over the
mean-field approximation (see Appendix D).
0.0
1.0
0.8
0.6
0.4
0.2
2
8
32
128
256
512
number of
layers l
1
σW
σW
Figure 4:	The above plots show the signal’s length and correlation coefficient after propagating
through l layers in a ReLU network with RAI. We estimate the length and correlation coefficient
averaged over M = 1024 input signals, and 40 networks with a constant width N = 2048. The
shaded regions denotes the standard deviation. Similar to Fig. 2, the chaotic region is absent.
RAI focuses on decreasing the dead node probability to increase expressive power, whereas ACI
uses anti-correlated weights to improve the expressivity. As RAI and ACI increase the expressivity
using different mechanisms, we explore the possibility of combining the two. We call it Random
asymmetric anti-correlated initialization (RAAI). To prepare weights drawn from RAAI, we con-
sider anti-correlated Gaussian weights and bias incoming to each node (like Eqn. 3) and replace one
randomly picked weight/bias with a random number drawn from a beta distribution. Note that the
weights and biases reaching different nodes are uncorrelated. Like ACI, we observe three phases
for RAAI. Numerical results presented in Fig. 5 suggest that the order to chaos boundary is around
σw2 = 0.9, and the length diverges for σw2 > 1.2. Again, a qualitative picture of the phase diagram
can be captured with additional assumptions over the mean-field approximation (see Appendix E).
In summary, RAAI has a chaotic phase like ACI, and a lower dead node probability like RAI, as
can be checked numerically. As RAAI inherits the advantages of both strategies, we expect it to
be a strong candidate for initializing ReLU networks. Table 1 summarizes and compares different
6
Under review as a conference paper at ICLR 2022
σW
σW
Figure 5:	The above plots show the signal’s length and correlation coefficient after propagating
through l layers in a ReLU network with RAAI. We estimate the length and correlation coefficient
averaged over M = 1024 input signals, and 40 networks with a constant width N = 2048. The
shaded regions denotes the standard deviation. Similar to ACI, we find a chaotic region. However,
the correlations do not converge to zero even for large σw2 .
Table 1: A comparison of different initialization schemes for ReLU networks. The dead node prob-
abilities are calculated numerically for input signals drawn from the standard normal distribution.
Initialization scheme	σw2	k	Chaotic phase	Dead node probability
He	2.0	-^0O-	No	0.5
ACI	2.0	100.0	Yes	0.5
RAI	0.36	0.0	No	0.36
RAAI	0.92	100.0	Yes	0.36
initialization schemes. In the following sections, we analyze the training dynamics and performance
of RAAI and compare it with other initialization schemes.
4	Training tasks
This section describes various tasks used to analyze the dynamics and performance of different
initialization schemes. We consider a variant of teacher-student setup Seung et al. (1992), in which
a student ReLU network is trained with examples generated by an untrained teacher network. We
consider three different tasks with varying complexities.
1.	First, a standard teacher task, in which the training data is generated by a ReLU network of the
same size as the student network, initialized with He initialization.
2.	Next, we consider a simple teacher task in which the capacity of the teacher network is much
lower than the student network. In many real data sets, the high-dimensional inputs lie in a low-
dimensional manifold Goldt et al. (2020), which motivates us to consider a simple teacher task. We
consider a single-layer ReLU network with N = 10 nodes, initialized with He initialization.
3.	Lastly, we consider a complex teacher task, in which the complexity of the teacher network is
more than the student network. We consider a teacher network with tanh activation of the same size
as the student network initialized in the chaotic regime, (σw2 , σb2) = (1.5, 0) Poole et al. (2016). A
ReLU network initialized with symmetric distributions has half of the nodes dead. Therefore, it has
a lower capacity than a tanh network of the same size.
We consider an L = 10 layered (in addition to input layer) student network with a constant width
N = 100, trained using SGD and Adam algorithms (for further details, see 6).
7
Under review as a conference paper at ICLR 2022
5	Comparison of learning dynamics for different initialization
SCHEMES
This section compares the performance of RAAI with other initialization schemes listed in Table 1
on tasks described in Section 4.
Standard teacher task Figure 6 shows the average validation loss for the standard tasks trained
with SGD and Adam algorithms. We observe that RAAI performs better than all other schemes.
100
101	102
epochs
103	100
101	102
epochs
103
Figure 6: Average validation loss for ReLU networks trained on the standard teacher task with SGD
(left) and Adam optimizer (right) for different initialization schemes. The shaded region shows the
standard deviation around the average loss.
Simple teacher task Figure 7 shows the average validation loss for the simple teacher task. Sim-
ilar to the standard teacher task, RAAI performs better than or on par with other initialization
100	101	102	103
epochs
100	101	102	103
epochs
Figure 7: Average validation loss for ReLU networks trained on the simple teacher task with SGD
(left) and Adam optimizer (right) for different initialization schemes. The shaded region shows the
standard deviation around the average loss.
Complex teacher task Figure 8 shows the average validation loss for the complex teacher task.
We observe that for a complex teacher, ACI starts to perform worse when trained with SGD algo-
rithm, whereas, RAAI faces no such problem and performs better or on par with other initializations.
In various scenarios, RAI performs comparable to RAAI, however, we find that RAAI performs
better RAI on real-world datasets. We present different intitialization schemes on three different
real-world datasets —MNIST, Fashion-MNIST and CIFAR-10. We find that RAAI outperforms all
other intialization schemes at early training steps and all initialization schemes perform equally well
after a few epochs. For further details, see Appendix F.
8
Under review as a conference paper at ICLR 2022
Figure 8: Average validation loss for ReLU networks trained on the complex teacher task with SGD
(left) and Adam optimizer (right) for different initialization schemes. The shaded region shows the
standard deviation around the average loss.
6	Discussion and Conclusion
In this article, we analyzed the evolution of correlation between signals propagating through a ReLU
network with correlated weights using the mean-field theory of signal propagation. Multiple studies
show that ReLU networks with uncorrelated weights are biased towards computing simpler func-
tions, but ReLU networks do perform complex tasks in practice. Unlike ReLU networks with uncor-
related weights, ReLU networks with anti-correlated weights reaching a node have a chaotic phase
where correlation saturates below unity. This suggests that such networks can exhibit higher expres-
sivity. Although we have focused on the ReLU networks in this study, anti-correlation in weights
may be useful in general. Networks with other non-linear activation functions like tanh, SELU, and
sigmoid have a chaotic phase even with uncorrelated weights. In these cases, the weight correlations
may still help to tune the phase boundaries and expressivity of the networks.
We further investigated the possibility that ReLU networks with the enhanced expressivity may
prove beneficial in faster learning. Comparison of training and test performance of networks in
a range of teacher-student setups clearly showed that networks with anti-correlated weights learn
faster. While ACI shows better learning performance in general, it shows poor performance with
SGD during an intermediate learning stage when the teacher network has a relatively higher capacity.
We believe that this may be due to the system getting stuck in local minima. This is consistent with
the absence of a similar regime on training with Adam optimizer. On training deeper networks with
ACI, we found that it overfits, but this can be avoided by fine-tuning correlation strength k. We also
investigated a possible improvement in training time from adding a regularization term in the loss
function that favors anti-correlated weights, but our attempts did not show any systematic results.
We compared ACI with a recently proposed initialization scheme called RAI, which introduces a
systematic asymmetry (around 0) in the weights to decrease dead node probability. We find that
the relative performance between RAI and ACI depends on the task and the optimization algorithm.
RAI improves expressivity by reducing the dead node probability, whereas ACI achieves the same
by inducing a chaotic phase. As RAI and ACI rely on different mechanisms, we explored a strategy
of combining the two initialization schemes. We analyzed the correlation properties of the combined
scheme, which we call RAAI and found that it has a chaotic phase like ACI. We demonstrated that
RAAI leads to faster training and learning than commonly-used methods on various teacher tasks
of a range of complexity. For different initialization schemes, the behavior of the training dynamics
at large epochs may depend on the optimizer and training data, however RAAI shows a definite
advantage over other schemes when using the SGD optimizer, especially in early training epochs. In
addition to faster training, RAAI also shows no sign of overfitting and thus improves on the simpler
strategy that relies only on anti-correlations. Our study has focused on adding simple two point
correlation in the initial distributions motivated by a richer phase space for ReLU networks with
anti-correlated weights. This simplest deviation from the uncorrelated Gaussian distribution showed
a consistent advantage in terms of training time, suggesting that initialization with more complex
and tailored correlations may lead to better performance.
9
Under review as a conference paper at ICLR 2022
References
Martin Abadi, Ashish Agarwal, Paul Barham, Eugene Brevdo, Zhifeng Chen, Craig Citro, Greg
Corrado, Andy Davis, Jeffrey Dean, Matthieu Devin, Sanjay Ghemawat, Ian Goodfellow, Andrew
Harp, Geoffrey Irving, Michael Isard, Yangqing Jia, Rafal Jozefowicz, Lukasz Kaiser, Manjunath
Kudlur, Josh Levenberg, Dan Mane, Rajat Monga, Sherry Moore, Derek Murray, Chris Olah,
Mike Schuster, Jonathon Shlens, Benoit Steiner, Ilya Sutskever, Kunal Talwar, Paul Tucker, Vin-
cent Vanhoucke, Vijay Vasudevan, Fernanda Viegas, Oriol Vinyals, Pete Warden, Martin Watten-
berg, Martin Wicke, Yuan Yu, and Xiaoqiang Zheng. Tensorflow: Large-scale machine learning
on heterogeneous distributed systems, 2015. URL http://download.tensorflow.org/
paper/whitepaper2015.pdf.
Lei Jimmy Ba, Jamie Ryan Kiros, and Geoffrey E. Hinton. Layer normalization. CoRR,
abs/1607.06450, 2016. URL http://arxiv.org/abs/1607.06450.
Yasaman Bahri, Jonathan Kadmon, Jeffrey Pennington, Sam S. Schoenholz, Jascha Sohl-Dickstein,
and Surya Ganguli. Statistical mechanics of deep learning. Annual Review of Condensed Mat-
ter Physics,11(1):501-528,2020. doi: 10.1146/annurev-ConmatPhys-031119-050745. URL
https://doi.org/10.1146/annurev-conmatphys-031119-050745.
Djork-Ame Clevert, Thomas Unterthiner, and Sepp Hochreiter. Fast and accurate deep network
learning by exPonential linear units (elus). In Yoshua Bengio and Yann LeCun (eds.), 4th Inter-
national Conference on Learning Representations, ICLR 2016, San Juan, Puerto Rico, May 2-4,
2016, Conference Track Proceedings, 2016. URL http://arxiv.org/abs/1511.07289.
Giacomo De Palma, Bobak Kiani, and Seth Lloyd. Random deep neural networks are biased to-
wards simple functions. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alche-Buc, E. Fox,
and R. Garnett (eds.), Advances in Neural Information Processing Systems, volume 32, pp.
1964-1976. Curran Associates, Inc., 2019. URL https://proceedings.neurips.cc/
paper/2019/file/feab05aa91085b7a8012516bc3533958-Paper.pdf.
Simon Du, Jason Lee, Haochuan Li, Liwei Wang, and Xiyu Zhai. Gradient descent finds global
minima of deep neural networks. In Kamalika Chaudhuri and Ruslan Salakhutdinov (eds.), Pro-
ceedings of the 36th International Conference on Machine Learning, volume 97 of Proceed-
ings of Machine Learning Research, pp. 1675-1685. PMLR, 09-15 Jun 2019. URL http:
//proceedings.mlr.press/v97/du19c.html.
John C. Duchi, Elad Hazan, and Y. Singer. Adaptive subgradient methods for online learning and
stochastic optimization. In J. Mach. Learn. Res., 2011.
K.	Fukushima. Visual feature extraction by a multilayered network of analog threshold elements.
IEEE Transactions on Systems Science and Cybernetics, 5(4):322-333, 1969. doi: 10.1109/
TSSC.1969.300225.
Kunihiko Fukushima and Sei Miyake. Neocognitron: A self-organizing neural network model for
a mechanism of visual pattern recognition. In Shun-ichi Amari and Michael A. Arbib (eds.),
Competition and Cooperation in Neural Nets, pp. 267-285, Berlin, Heidelberg, 1982. Springer
Berlin Heidelberg. ISBN 978-3-642-46466-9.
Xavier Glorot, Antoine Bordes, and Yoshua Bengio. Deep sparse rectifier neural networks. In
Geoffrey Gordon, David Dunson, and Miroslav Dudlk (eds.), Proceedings of the Fourteenth In-
ternational Conference on Artificial Intelligence and Statistics, volume 15 of Proceedings ofMa-
chine Learning Research, pp. 315-323, Fort Lauderdale, FL, USA, 11-13 Apr 2011a. JMLR
Workshop and Conference Proceedings. URL http://proceedings.mlr.press/v15/
glorot11a.html.
Xavier Glorot, Antoine Bordes, and Yoshua Bengio. Deep sparse rectifier neural networks. In
Geoffrey Gordon, David Dunson, and Miroslav Dudlk (eds.), Proceedings of the Fourteenth In-
ternational Conference on Artificial Intelligence and Statistics, volume 15 of Proceedings ofMa-
chine Learning Research, pp. 315-323, Fort Lauderdale, FL, USA, 11-13 Apr 2011b. JMLR
Workshop and Conference Proceedings. URL http://proceedings.mlr.press/v15/
glorot11a.html.
10
Under review as a conference paper at ICLR 2022
Sebastian Goldt, Marc Mezard, Florent Krzakala, and Lenka Zdeborova. Modeling the influence of
data structure on learning in neural networks: The hidden manifold model. Physical Review X,
10, 12 2020. doi: 10.1103/PhysRevX.10.041044.
B. Hanin and D. Rolnick. Deep relu networks have surprisingly few activation patterns. In NeurIPS,
2019.
Boris Hanin. Which neural net architectures give rise to exploding and vanishing gradients? In
S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Garnett (eds.),
Advances in Neural Information Processing Systems, volume 31, pp. 582-591. Curran Asso-
ciates, Inc., 2018. URL https://proceedings.neurips.cc/paper/2018/file/
13f9896df61279c928f19721878fac41-Paper.pdf.
Soufiane Hayou, Arnaud Doucet, and Judith Rousseau. On the selection of initialization and activa-
tion function for deep neural networks, 2019a. URL https://openreview.net/forum?
id=H1lJws05K7.
Soufiane Hayou, Arnaud Doucet, and Judith Rousseau. On the impact of the activation function
on deep neural networks training. In Kamalika Chaudhuri and Ruslan Salakhutdinov (eds.),
Proceedings of the 36th International Conference on Machine Learning, volume 97 of Pro-
ceedings of Machine Learning Research, pp. 2672-2680. PMLR, 09-15 Jun 2019b. URL
http://proceedings.mlr.press/v97/hayou19a.html.
Juncai He, Lin Li, Jinchao Xu, and Chunyue Zheng. Relu deep neural networks and linear finite
elements. Journal of Computational Mathematics, 38(3):502-527, 2020. ISSN 1991-7139. doi:
https://doi.org/10.4208/jcm.1901-m2018-0160. URL http://global-sci.org/intro/
article_detail/jcm/15798.html.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Delving deep into rectifiers: Surpassing
human-level performance on imagenet classification. IEEE International Conference on Com-
puter Vision (ICCV 2015), 1502, 02 2015. doi: 10.1109/ICCV.2015.123.
Kaiming He, X. Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition.
2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp. 770-778, 2016.
Dan Hendrycks and Kevin Gimpel. Gaussian error linear units (gelus). arXiv: Learning, 2016.
G. Hinton, L. Deng, D. Yu, G. E. Dahl, A. Mohamed, N. Jaitly, A. Senior, V. Vanhoucke, P. Nguyen,
T. N. Sainath, and B. Kingsbury. Deep neural networks for acoustic modeling in speech recogni-
tion: The shared views of four research groups. IEEE Signal Processing Magazine, 29(6):82-97,
2012. doi: 10.1109/MSP.2012.2205597.
Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by
reducing internal covariate shift. In Francis Bach and David Blei (eds.), Proceedings of the 32nd
International Conference on Machine Learning, volume 37 of Proceedings of Machine Learning
Research, pp. 448-456, Lille, France, 07-09 Jul 2015. PMLR. URL http://proceedings.
mlr.press/v37/ioffe15.html.
Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. CoRR,
abs/1412.6980, 2015.
Gunter Klambauer, Thomas Unterthiner, Andreas Mayr, and Sepp Hochreiter. Self-normalizing
neural networks. In I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan,
and R. Garnett (eds.), Advances in Neural Information Processing Systems, volume 30, pp. 971-
980. Curran Associates, Inc., 2017. URL https://proceedings.neurips.cc/paper/
2017/file/5d44ee6f2c3f71b73125876103c8f6c4-Paper.pdf.
Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classification with deep convo-
lutional neural networks. In F. Pereira, C. J. C. Burges, L. Bottou, and K. Q. Weinberger (eds.),
Advances in Neural Information Processing Systems, volume 25, pp. 1097-1105. Curran Asso-
ciates, Inc., 2012. URL https://proceedings.neurips.cc/paper/2012/file/
c399862d3b9d6b76c8436e924a68c45b-Paper.pdf.
11
Under review as a conference paper at ICLR 2022
Yann LeCun, Y. Bengio, and Geoffrey Hinton. Deep learning. Nature, 521:436-44, 05 2015. doi:
10.1038/nature14539.
Jaehoon Lee, Jascha Sohl-dickstein, Jeffrey Pennington, Roman Novak, Sam Schoenholz, and
Yasaman Bahri. Deep neural networks as gaussian processes. In International Confer-
ence on Learning Representations, 2018. URL https://openreview.net/forum?id=
B1EA-M-0Z.
Bo Li and David Saad. Exploring the function space of deep-learning machines. Physical Review
Letters, 120:248301, Jun 2018. doi: 10.1103/PhysRevLett.120.248301. URL https://link.
aps.org/doi/10.1103/PhysRevLett.120.248301.
Bo Li and David Saad. Large deviation analysis of function sensitivity in random deep neural
networks. Journal of Physics A: Mathematical and Theoretical, 53(10):104002, feb 2020. doi:
10.1088/1751-8121/ab6a6f. URL https://doi.org/10.1088/1751-8121/ab6a6f.
Lu Lu, Yeonjong Shin, Yanhui Su, and George Karniadakis. Dying relu and initialization: Theory
and numerical examples. Communications in Computational Physics, 28:1671-1706, 11 2020.
doi: 10.4208/cicp.OA-2020-0165.
Andrew L. Maas, Awni Y. Hannun, and Andrew Y. Ng. Rectifier nonlinearities improve neural net-
work acoustic models. In in ICML Workshop on Deep Learning for Audio, Speech and Language
Processing, 2013.
Hartmut Maennel, O. Bousquet, and S. Gelly. Gradient descent quantizes relu network features.
ArXiv, abs/1803.08367, 2018.
Dmytro Mishkin and Jiri Matas. All you need is a good init. CoRR, abs/1511.06422, 2016.
Vinod Nair and Geoffrey E. Hinton. Rectified linear units improve restricted boltzmann machines.
In Proceedings of the 27th International Conference on International Conference on Machine
Learning, ICML’10, pp. 807-814, Madison, WI, USA, 2010. Omnipress. ISBN 9781605589077.
Yurii Nesterov. Introductory Lectures on Convex Optimization: A Basic Course. Springer Publishing
Company, Incorporated, 1 edition, 2014. ISBN 1461346916.
Ben Poole, Subhaneil Lahiri, Maithra Raghu, Jascha Sohl-Dickstein, and Surya Gan-
guli. Exponential expressivity in deep neural networks through transient chaos. In
D. Lee, M. Sugiyama, U. Luxburg, I. Guyon, and R. Garnett (eds.), Advances
in Neural Information Processing Systems, volume 29, pp. 3360-3368. Curran Asso-
ciates, Inc., 2016. URL https://proceedings.neurips.cc/paper/2016/file/
148510031349642de5ca0c544f31b2ef- Paper.pdf.
Maithra Raghu, Ben Poole, Jon Kleinberg, Surya Ganguli, and Jascha Sohl Dickstein. On the
expressive power of deep neural networks. In Proceedings of the 34th International Conference
on Machine Learning - Volume 70, ICML’17, pp. 2847-2854. JMLR.org, 2017.
Nasim Rahaman, Aristide Baratin, Devansh Arpit, Felix Draxler, Min Lin, Fred Hamprecht, Yoshua
Bengio, and Aaron Courville. On the spectral bias of neural networks, 2019. URL https:
//openreview.net/forum?id=r1gR2sC9FX.
Prajit Ramachandran, Barret Zoph, and Quoc V. Le. Searching for activation functions, 2018. URL
https://openreview.net/forum?id=SkBYYyZRZ.
Tim Salimans and Diederik P. Kingma. Weight normalization: A simple reparameterization to
accelerate training of deep neural networks. In NIPS, 2016.
Andrew M. Saxe, James L. McClelland, and Surya Ganguli. Exact solutions to the nonlinear dy-
namics of learning in deep linear neural networks. In Yoshua Bengio and Yann LeCun (eds.), 2nd
International Conference on Learning Representations, ICLR 2014, Banff, AB, Canada, April
14-16, 2014, Conference Track Proceedings, 2014. URL http://arxiv.org/abs/1312.
6120.
12
Under review as a conference paper at ICLR 2022
Samuel S. Schoenholz, Justin Gilmer, Surya Ganguli, and Jascha Sohl-Dickstein. Deep information
propagation. In 5th International Conference on Learning Representations, ICLR 2017, Toulon,
France, April 24-26, 2017, Conference Track Proceedings. OpenReview.net, 2017. URL https:
//openreview.net/forum?id=H1W1UN9gg.
Alireza Seif, M. Hafezi, and C. Jarzynski. Machine learning the thermodynamic arrow of time.
Nature Physics,17:105-113,2019.
H. S. Seung, H. Sompolinsky, and N. Tishby. Statistical mechanics of learning from examples.
Physical Review A, 45:6056-6091, Apr 1992. doi: 10.1103/PhysRevA.45.6056. URL https:
//link.aps.org/doi/10.1103/PhysRevA.45.6056.
Wenling Shang, Kihyuk Sohn, Diogo Almeida, and Honglak Lee. Understanding and improv-
ing convolutional neural networks via concatenated rectified linear units. In Proceedings of the
33rd International Conference on International Conference on Machine Learning - Volume 48,
ICML’16, pp. 2217-2225. JMLR.org, 2016.
David Silver, Aja Huang, Christopher Maddison, Arthur Guez, Laurent Sifre, George Driessche,
Julian Schrittwieser, Ioannis Antonoglou, Veda Panneershelvam, Marc Lanctot, Sander Dieleman,
Dominik Grewe, John Nham, Nal Kalchbrenner, Ilya Sutskever, Timothy Lillicrap, Madeleine
Leach, Koray Kavukcuoglu, Thore Graepel, and Demis Hassabis. Mastering the game of go with
deep neural networks and tree search. Nature, 529:484-489, 01 2016. doi: 10.1038/nature16961.
RUPesh Srivastava, Klaus Greff, and Jurgen Schmidhuber. Training very deep networks. 2015
Neural Information Processing Systems (NIPS 2015 Spotlight), 07 2015.
L.	Toth. Phone recognition with deep sparse rectifier neural networks. 2013 IEEE International
Conference on Acoustics, Speech and Signal Processing, pp. 6985-6989, 2013.
Ludovic Trottier, P. Giguere, and B. Chaib-draa. Parametric exponential linear unit for deep con-
volutional neural networks. 2017 16th IEEE International Conference on Machine Learning and
Applications (ICMLA), pp. 207-214, 2017.
D. Ulyanov, A. Vedaldi, and V. Lempitsky. Instance normalization: The missing ingredient for fast
stylization. ArXiv, abs/1607.08022, 2016.
Guillermo Valle-Perez, Chico Q. Camargo, and Ard A. Louis. Deep learning generalizes be-
cause the parameter-function map is biased towards simple functions. In International Confer-
ence on Learning Representations, 2019. URL https://openreview.net/forum?id=
rye4g3AqFm.
Yuxin Wu and Kaiming He. Group normalization. In ECCV, 2018.
Matthew D. Zeiler. Adadelta: An adaptive learning rate method. ArXiv, abs/1212.5701, 2012.
Ethics S tatement
We do not think that there are any direct negative impacts of this work, however, faster training using
RAAI may lead to easier training of algorithms with negative societal impact.
Reproducibility S tatement
We implemented feedforward networks in Tensorflow (version 2.2.0) Abadi et al. (2015) and train
them using 105 training examples with a mean squared error loss, a mini-batch size of 32, and default
parameters for the optimizers. The validation set contains 103 examples.
13
Under review as a conference paper at ICLR 2022
A Derivation of the length and correlation maps for
CORRELATED WEIGHTS
This section derives the length and covariance maps for ReLU networks with correlated weights
given by
P (w1l ,w2l ,w3l .
(3)
where the covariance matrix given by
A
2
σw2
Nl-1
I-
k J)
1+k N-I)
Here I is the identity matrix, J is an all-ones matrix, and k parameterizes the correlation strength.
Positively correlated and anti-correlated regimes correspond to the regions -1 < k < 0 and k > 0,
respectively, whereas, k = 0 generates uncorrelated weights. For simplicity, we consider Nl = N
in all layers, but the results hold for all Nl, as long as it is large.
A. 1 Derivation of length map
To derive the length map, we follow the approach introduced by 36. Assuming self-averaging, we
obtain the average value of the squared length of a signal, s0 = x, after propagating to layer l by
considering an average over weights and biases between layer l and l - 1
1 N	1N N
Zhh (χ))2 = NEE
i=1	i=1 j,m=1
wiljwilm φ(hlj-1(x))φ(hlm-1(x))+ (bli)2
k1
t—tn)φ(hjI(X))o(hm1(X))+σb,
1+ k N
(4)
where We have used〈wjWim)= σw Qjm - 1⅛]1) and〈(bi)2)= σb2. For large N, each
hli-1(X) is a weighted sum of a large number of correlated random variables, which converges to
a zero-mean Gaussian with a variance qhl-1(X). Replacing the average over hli at layer l - 1 by
a Gaussian distribution to get the general form of the recursive map. This average corresponds to
averaging over all the weights and biases upto layer l - 1.
qh(X) = σW Z Dzφ (JqhT(X) Z) - σW ι+k
/ Dzφ (JqhT(X) z
2
+ σb2 ,
where Dz is the standard normal distribution. For the second term in the Eqn. 4, we have used the
fact that for different nodes m 6= j, hlj (X) and hlm(X) are uncorrelated random variables and ignored
O(1/N) terms. Note that the weights and biases reaching two different nodes are uncorrelated. For a
ReLU activation, we can perform the integrals to get the exact form of the recursive relation between
qhl (X) and qhl-1(X)
qh(X)=音(1 - ι~r-) qh-1 (X) + σ2.
2	1+ π
(5)
14
Under review as a conference paper at ICLR 2022
A.2 Derivation of the covariance map
The covariance map can be derived similarly by considering an average over the weights and biases
qh(χι,χ2) = N (X hi(XI)hi(X2] = X(wjwlim) φ(hj-1(χι))φ(hm 1(χ2)) +〈(bi产〉
i=1	j,m=1
σ2 N	k 1
qh (x1,x2) = Nw X 卜j,m - 1+k n) φ(hj 1(x1))φ(hm1(x2)) + σ2,
j,m=1
and then replacing the sum over neurons in the previous layer with an integral with a Gaussian mea-
sure. For large N, the joint distribution of hlj (X1) and hlm(X2) will converge to a two-dimensional
Gaussian distribution with a covariance matrix
Σ-1
qhl-1(X1)
qhl-1(X1,X2)
qhl-1(X1,X2)
qhl-1(X2)
The correlations among hlj (X1) and hlm(X2) are induced as the two signals are propagating through
the same network. Propagating this joint distribution across one layer, we obtain the iterative map
qhl(X1,X2)
k
Dz1Dz2φ(UI)φ(Ui2^) - σw k+ι
Dz1 Dz2 φ(u1)φ(u2) + σb2
U1 =	qhl-1(X1)z1, U12 =	qhl-1(X2) clh-1 z1 +	1 - (clh-1)2 z2 U2 =	qhl-1(X2)z2,
where Ch = √ q^：；：2 片 is the correlation coefficient, and Dzι, Dz2 are standard normal Gaussian
distributions. Again, in the second part of the above equation, we have used the fact that for j 6= m,
hlj (X1 ) and hlm(X2 ) are uncorrelated random variables and have ignored O(1/N) terms. Further,
we can perform the integrals for ReLU networks to get the exact recursive map
qh(X1, χ2)=wr (于(Ch-I)—vɪɪ ~q q qh-1(XI)qh-1(X2)+σb	⑹
2	1+ k π
Cl-1	Cl-1	1 /----------
f (Ch-I) = -h2- + Ur SinT(Ch-1) + π Jl-(ch-1)2.	(7)
B Training with anti-correlated vs. positively correlated
INITIALIZATION
This section compares the training dynamics and performance of ReLU networks initialized with
different weight correlation strengths on tasks described in Section 4. We utilize the increased ex-
pressivity in ReLU networks with anti-correlated weights (ACI) at the initial training phase and
compare its training dynamics with He initialization and positively correlated weight initialization.
We observe that ACI provides a definite advantage over the other two initialization schemes. We also
find that positively correlated weight ReLU networks train slower than He initialization, suggesting
that anti-correlation may develop in weights during training. We choose three different correla-
tion strengths; k = 100 induces anti-correlated weights, k = -0.5 produces positively correlated
weights, and lastly, k = 0 corresponds to uncorrelated weights (He initialization). We train networks
with two different optimization algorithms, SGD and Adam. For SGD, we train for 104 epochs, and
for Adam, we train for 103 epochs.
Standard teacher task Figure 9 shows the average validation loss for different correlation
strengths trained using SGD and Adam algorithms. We observe that ReLU networks initialized
with ACI train faster than He initialization. In contrast, ReLU networks with positively correlated
weights train slower than He initialization.
15
Under review as a conference paper at ICLR 2022
Figure 9: Average validation loss for ReLU networks trained on the standard teacher task with SGD
(left) and Adam optimizer (right) for different weight correlations strengths.
- - - - -
Ooooo
Simple teacher task Figure 10 shows the average validation loss for the simple teacher task. We
observe similar qualitative results as in the standard teacher task. For SGD, we observe an initial
linear region in which all initialization schemes perform equally; however, at large epochs, ACI
shows a definite advantage over other initializations.
Figure 10: Average validation loss for ReLU networks trained on the simple teacher task with SGD
(left) and Adam (right) optimizer for different weight correlations strengths.
Complex teacher task Figure 11 shows the average validation loss for a complex teacher task.
For some intermediate regions, ACI performs worse than other initializations on training with SGD.
The regions where ACI performs poorly shift depending on the complexity of the task.
C Derivation of the length and covariance map for RAI
To draw weights from the RAI distribution, we first initialize the weights and bias incoming to each
node with a Gaussian distribution N(0, σw). Next, We replace one weight or the bias incoming to
each neuron by a random variable from beta distribution (see 18 for details). The weights and bias
are treated on an equal footing. Thus, to simplify the notations, we incorporate the bias in the weight
matrix by introducing a fictitious additional node with a constant value of one, i.e.,
sl(x) = [φ(hl(x)), 1].
16
Under review as a conference paper at ICLR 2022
Figure 11: Average validation loss for ReLU networks trained on the complex teacher task with
SGD (left) and Adam (right) optimizer for different weight correlations strengths.
The evolution equation is now given by
hl(X) = Wl ∙ SlT(x).
It is easier to track the evolution using the activation instead of the pre-activations. So we define a
few covariance matrices which will come in handy
1
qS (χι,χ2) = n+i ESi (XI)Si(X2)
N+1 i=0
where kjl tags variables associated with the special weight. We will use the notations qsl (X)
qsl (X, X) and q-l k(X) = q-l k(X, X). The corresponding correlation coefficients are given by,
,l =	qS (χι,χ2)
S	PqS(XI) qS(X2)
ι	q-kι∣(Xi,x2)
-kl	Jq-k； (XI) q-kt (X2)
C.1 Derivation of the length map for RAI
Given hl-1(X) and considering weights between layers l and l -1, we can view hlj(X) as a random
variable
hlj (X) = σw	q-l-k1l-1 (X) z+Slk-l-11(X) u,
-kj	kj
where Z ~ N(0,1) and U ~ β(2,1). By applying the activation function and squaring it, We obtain
2
φ(hlj (X))2
ql-k1l-1(X) z+Slk-l-11(X) u
-k	k
jj
17
Under review as a conference paper at ICLR 2022
Next, we take an average over the weights and the special weight ( average denoted by < . >) to get
N1
(φ(hj-(X))2|hlT(XD = E N+1 Idzduf(Z) g(U)
kjl-1=0
ql-k1-ι (χ) Z + Sl--I (χ) U)
-kj	kj
where g(u)〜 β(2,1) distribution, and f (z)〜 N(0,1). We can take a sum over all nodes and
re-write the equation in terms of the overlap
1
N +1
NN
1 + X FNJn
dzdu f(z)g(u)
(8)
C.2 Derivation of the covariance map for RAI
The covariance map can be derived similarly, with a key difference of covariance between the pre-
activations. For two input signals s0 = X1 and s0 = X2, the covariance map reads
qsl (X1, X2)|hl-1(X1), hl-1(X2)
dy1dy2du f(y1,y2) g(u) ×
×
1
N +1
NN
1 + X kX=0 N+1
(X1) y1 + sl-l-11
kj
(X2 ) y2 + sl-l-11
kj
(9)
where f(y1, y2) is the joint Gaussian distribution of y1 and y2, with a covariance matrix given by
Σlk-l 1
j
q-l-k1jl-1(X1)
q-l-k1l-1(X1,X2)
j
q-l-k1l-1(X1,X2)
q-l-k1l-1(X2)
j
We can re-write Eqn. 9 in terms of
qsl (X1, X2)|hl-1(X1), hl-1(X2)
1
N +1
1 + ∑ 工NW dz1 dz2 du f(z1)f(z2) g(u)
j klj-1 N+ 1
×
×
(X1) z1 + sl-l-11
kj
×
q-l-k1l-1 (X2) cl-
j
(X1 ) u
×
z1 +
+ slk-l-11 (X2) u
where f (zι)〜f (z2)〜N(0,1) are standard Gaussian distributions. As suggested by 36, we can
find the fixed point of the correlation map under the assumption that the length qhl (X) has reached
its fixed point. It can be checked that clh = 1 is a fixed point of the correlation map.
18
Under review as a conference paper at ICLR 2022
D Stability of the fixed points for the length and correlation
maps for RAI
D. 1 Stability of the fixed point for the length map for RAI
The derivation of the analytical form of the length map (Eqn. 8) is difficult, and only bounds to
the map have been derived (see 18). Inspired by the analytical form of the length map for the anti-
correlated initialization and the analysis done by 18, we assume that the length map has a linear
dependence on qsl-1(x). Under this assumption, we can find the stability of the fixed point of the
length map by taking a derivative with respect to qsl-1(x). Yet another problem exists. While taking
a derivative, we have to encounter derivatives of the form
∂sl--k1l-1(x)
dqlh1(x) .
To simplify the calculations further, we employ a mean-field type approach by approximating
q-l-k1l (x) and sl-l-11 by qsl-1(x). Note that we can also approximate slk-l 1 by its mean value, giv-
ing the same qualitative results. This simplifies Eqn. 8 to
lq (x)|hl-1(x)) = N1+ι
1 + (N + 1) Z dz du f (z) g(u) φ (J qS-1(x)(σw Z + U)).
To find the fixed point of the length map, we take a derivative wrt qsl-1(x) to get the condition for
stability of the fixed point q*. We denote this derivative by Zq*. It separates the dynamics into two
phases —a bounded phase when Zq* < 1, and an unbounded phase when Zq* > 1.
*
*
*
dqs(X) I
∂qS-1(x) IqsT(X)=q*
∂ql d1(x) / dz du f (Z) g(U) φ (JqsT(X)(σwZ + U))
/ 1	/ dz du f(z) g(u) (σwZ + u)φ0 ( qjqs-1(x)(σwZ + U)) φ ( Jqs-1(x)(σwZ + u)
qsl-1(X)
Zq* =	σw2	dZ du	f(Z)	g(u)	[φ0 (σwZ +	u)]2	+ σw	dZ du f(Z)	g(u)φ0	(σwZ + u) φ (σwZ + u)
(10)
where we have used the fact that for a > 0, φ(aX) = a φ(X). On evaluating the integral, we find
that Zq* = 1 when σw2 = 0.56. This critical value underestimates the numerical value obtained in
Fig. 4.
D.2 Stability of the fixed point for the correlation map for RAI
Under the assumption, qsl (X) → q*, the correlation map has a fixed point cs* = 1, and its stability
∂cl
is given by χι =券ST evaluated at Cls 1 = 1. But again, we get into the difficulties mentioned in
the previous section,s and we employ the same assumptions to arrive at a tractable equation for the
correlation map
cls|hl-1(X1), hl-1(X2)
1+N
dZ1 dZ2 du f(Z1)f(Z2) g(u) ×
1	1
q*(χ) N + 1
(pq*(x)(σw zι +
cls-1 σw Z1 +	1 - (cls-1)2 σw
Z2 + u
×
φ
19
Under review as a conference paper at ICLR 2022
Next, We take a derivative to get the condition for the stability of the fixed point Ch = 1
χ1
dch I
d” ChT=I
χ1
dz1dz2duf(z1)f(z2)g(u)φ
×
1	∂
qI(χ) ∂Ch-τ
×
χ1 =σw
σw
zι + ≠ — (cS-1)2 σw
dz du f(z) g(u) [φ0(σwz + u)]2 .
z2 + u
(11)
The above equation is the same as the first term We obtained in the condition for the stability of the
length map (Eqn. 10). We obtain a critical value of σw2 = 1.41 by solving for χ1 = 1. We observe
that the critical point for the length is smaller than the critical point of the correlation coefficient,
and from our experience With ReLU netWorks calculations, We expect RAI to have an ordered phase
only, Which is confirmed by numerical results shoWn in Fig. 4.
E Derivation of length and correlation map for RAAI and
STABILITY CONDITIONS
E.1 Derivation for the length map for RAAI and the stability condition
Similar to the previous section, We can vieW hlj (x) as a random variable
σw J守T(X) Z + S-k1-ι (x) u,
where ql-1(χ)
q-l-k1lj-1(χ)
(1 - χ++k∏l). Then, we can re-define σw as
22
σ = σw
1-
k n
1 + k π )
which yields,
hj(x) = σ ↑Jql-l1-ι
(χ) z+ sl--k1l-1
(x) u,
Now, the entire analysis goes through as Appendix C.1, just with a re-definition of the variance.
Now, we can read off the stability condition for the fixed point of the length map
ζq*
2
σw
dz du
f(z) g(u) [Φ0 (σz + u)]2+σ
J dz du f(z) g(u)φ0 (σz + U) φ (σz + U) (12)
On solving the equations numerically, we observe that the length is bounded when σw2 < 1.75,
which overestimates the numerical value observed in Fig. 5.
E.2 Derivation for the correlation map for RAAI and the stability condition
The correlation map for RAAI can be derived similar to RAI (Appendix C.2), with a key difference
being the covariance matrix. The covariance matrix, in this case, is
20
Under review as a conference paper at ICLR 2022
Σlk-l 1(x1,x2)
ql-k1-ι (XI)- 1⅛ (m-k;T (XI))
ql-kl-i (X1,x2) - 1⅛ml-kl-ι (XI)m-fc∣τ (X2)
ql-k1-ι (x1,x2) - 1⅛m-kι-ι (XI)m—%-1 (X2)
q-k1-ι (X2) - l+k fmLkι-ι (X2))
Again, We can check that Cs = 1 is a fixed point of the dynamics. The stability of the fixed point
under the assumptions considered in Appendix D.2 determines the order to chaos boundary at σw2 =
1.41. The critical value overestimates the numerical results presented in Fig. 5.
Note that instead of approximating sl-l-l1 by its RMS value qsl-l(X), We can also approximate it
j
by its mean value. In this case, We observe similar qualitative results. In Table 2, We compare the
boundaries predicted by the RMS and mean approximations.
Table 2: A comparison betWeen the decision boundaries obtained by approximating sl-l-l1 by its
j
RMS and mean value. The RMS approximation underestimates the length boundary for RAI,
Whereas it overestimates both the phase boundaries for RAAI. On the other hand, the mean ap-
Proximation overestimates the phase boundaries for RAI and RAAI both.
Approximation~~(σ'W )q (RAI)	(σW )c(RAAI)	(σW )q (RAAI)
^MS	057	141	175
Mean	0.85	1.46	1.89
F Comparison of the performance of different initialization
schemes on real-world datasets
In this section, We compare the performance of different initializiation schemes on three different
real-World datasets. We consider MNIST, Fashion-MNIST and CIFAR-10 datasets, and train them
With feedforWard netWorks With depth L = 10 (in addition to input layer) and a constant Width of
N = 100 for all hidden layers. We implemented feedforWard netWorks in TensorfloW and train
them using the complete training set With a cross entropy loss, a mini-batch size of 32, and default
parameters for the optimizers. As high performance for these datasets is achieved quickly (in terms
of epochs), We observe the training accuracy as a function of the number of steps (and not epochs).
It is noteWorthy that We used training accuracy for demonstration purposes and validation accuracy
shoWs similar behaviour for the trends.
F.1 MNIST task
Figure 12 shoWs the average training accuracy for the MNIST task trained With SGD. We observe
that RAAI performs better than all other schemes, hoWever, the advantage is only observed at early
time steps.
F.2 Fashion-MNIST task
Figure 13 shoWs the average training accuracy for the MNIST task trained With SGD. We observe
that RAAI performs better than all other schemes, hoWever, the advantage is only observed at early
time steps.
F.3 CIFAR- 10
Figure 14 shoWs the average training accuracy for the CIFAR-10 task trained With SGD. We observe
that RAAI performs better than all other schemes, hoWever, the advantage is only observed at early
time steps.
21
Under review as a conference paper at ICLR 2022
X°B,moov uo-BPIm
Other initializations
1.0-
Figure 12: Average training accuracy for ReLU networks trained on the MNIST task with SGD
optimizer for different initialization schemes.
100	101	102	103	104
steps
----He
----ACI
----RAI
----RAAI
X°B,moov uo-BPIm
Other initializations
1.0-
Figure 13: Average training accuracy for ReLU networks trained on the Fashion-MNIST task with
SGD optimizer for different initialization schemes.
1O0	101	102	103	104
steps
----He
----ACI
----RAI
----RAAI
Other initializations
100	101	102	103	104
steps
Figure 14: Average training accuracy for ReLU networks trained on the CIFAR-10 task with SGD
optimizer for different initialization schemes.
22
Under review as a conference paper at ICLR 2022
G Code to generate weights drawn from RAAI distribution
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
import numpy as np
def RAAI(fan_in, fan_out, k = 100, Variance_Weights = 0.9):
"""Randomized Asymmetric Anti-correlated Initializer (RAAI)
Arguments :
fan_in —— the number of neurons in the previous layer
fan_out ——the number of neurons in the next layer
corr ——correlation strength for the Gaussian weights
Variance_weights ——variance of the weights
Returns :
W, b ——weight and bias matrices with shape(fan_in, fan_out), and (
fan_out,)
corr = k/(1+k)
mean = np.zeros(fan_in + 1)
J = np.ones((fan_in + 1, fan_in + 1))
cov = (np.identity(fan_in + 1) - J*(corr/(fan_in +1)) ) *
Variance_WeightS/fan_in
P = np.random.multivariate_normal(mean = mean, cov = cov, size = (
fan_out))
for j in range(P.shape[0]):
k = np.random.randint(0, high = fan_in + 1)
P[j, k] = np.random.beta(2, 1)
W = P[:, :-1].T
b = P[:, -1]
return W.astype(np.float32), b.astype(np.float32)
23