Under review as a conference paper at ICLR 2022
Can Vision Transformers
Perform Convolution?
Anonymous authors
Paper under double-blind review
Ab stract
Several recent studies have demonstrated that attention-based networks, such
as Vision Transformer (ViT), can outperform Convolutional Neural Networks
(CNNs) on several computer vision tasks without using convolutional layers. This
naturally leads to the following questions: Can a self-attention layer of ViT ex-
press any convolution operation? In this work, we prove that a single ViT layer
with image patches as the input can perform any convolution operation construc-
tively, where the multi-head attention mechanism and the relative positional en-
coding play essential roles. We further provide a lower bound on the number of
heads for Vision Transformers to express CNNs. Corresponding with our anal-
ysis, experimental results show that the construction in our proof can help inject
convolutional bias into Transformers and significantly improve the performance
of ViT in low data regimes.
1	Introduction
Recently, the Transformer (Vaswani et al., 2017) architecture has achieved great success in vision
after it dominates the language domain (Devlin et al., 2019; Liu et al., 2019). Equipped with large-
scale pre-training or several improved training strategies, the Vision Transformer (ViT) can outper-
form CNNs on a variety of challenging vision tasks (Dosovitskiy et al., 2021; Touvron et al., 2021;
Liu et al., 2021; Chen et al., 2021). As Transformer takes 1D sequences of tokens as input, the com-
mon manner to transform a 2D image into such a 1D sequence is introduced by Dosovitskiy et al.
(2021): An image X ∈ RH×W×C is reshaped into a sequence of flattened patches X ∈ RN×P2C,
where H, W, C are the image height, width, channel, P is the patch resolution, and N = HW/P2
is the sequence length. By using specific positional encoding to encode spatial relationship between
patches, a standard Transformer can therefore be used in the vision domain.
It has been observed that when there is sufficient training data, ViT can dramatically outperform
convolution-based neural network models (Dosovitskiy et al., 2021) (e.g., 85.6% vs 83.3% ImageNet
top-1 accuracy for ViT-L/16 and ResNet-152x2 when pre-trained on JFT-300M). However, ViT still
performs worse than CNN when trained on smaller-scale datasets such as CIFAR-100. Motivated by
these observations, it becomes natural to compare the expressive power of Transformer and CNN.
Intuitively, a Transformer layer is more powerful since the self-attention mechanism enables context-
dependent weighting while a convolution can only capture local features. However, it is still unclear
whether a Transformer layer is strictly more powerful than convolution. In other words:
Can a self-attention layer of ViT (with image patches as input) express any convolution operation?
A partial answer has been given by Cordonnier et al. (2020). They showed that a self-attention
layer with a sufficient number of heads can express convolution, but they only focused on the set-
tings where the input to the attention layer is the representations of pixels, which is impractical
due to extremely long input sequence and huge memory cost. In Vision Transformer and most of
its variants (Touvron et al., 2021; Dosovitskiy et al., 2021; D’Ascoli et al., 2021), the input is the
representations of non-overlapping image patches instead of pixels. As a convolution operation can
involve pixels across patch boundaries, whether a self-attention layer in ViT can express convolution
is still unknown.
1
Under review as a conference paper at ICLR 2022
In this work, we give an affirmative answer to the above-mentioned question. We formally prove
that a ViT layer with relative positional encoding and sufficient attention heads can express any
convolution even when the input is image patches. This implies that the poor performance of ViT
on small datasets is mainly due to its generalization ability instead of expressive power. We further
provide a lower bound on the number of heads required for transforming convolution into a self-
attention layer. Based on our theoretical findings, we propose a two-phase training pipeline to inject
convolutional bias into Vision Transformers, and empirically demonstrate its effectiveness in low
data regimes.
The contributions of this paper are summarized below.
•	We provide a constructive proof to show that a 9-head self-attention layer in Vision Trans-
formers with image patch as the input can perform any convolution operation, where the
key insight is to leverage the multi-head attention mechanism and relative positional en-
coding to aggregate features for computing convolution.
•	We prove lower bounds on the number of heads for self-attention layers to express con-
volution operation, for both the patch input and the pixel input setting. This result shows
that the construction in the above-mentioned constructive proof is optimal in terms of the
number of heads. Specifically, we show that 9 heads are both necessary and sufficient for
a self-attention layer with patch input to express convolution with a K × K kernel, while
a self-attention layer with pixel input must need K2 heads to do so. Therefore, Vision
Transformers with patch input are more head-efficient than pixel input when expressing
convolution.
•	We propose a two-phase training pipeline for Vision Transformers. The key component in
this pipeline is to initialize ViT from a well-trained CNN using the construction in our the-
oretical proof. We empirically show that with the proposed training pipeline that explicitly
injects the convolutional bias, ViT can achieve much better performance compared with
models trained with random initialization in low data regimes.
2	Preliminaries
In this section, we recap the preliminaries of Convolutional Neural Networks and Vision Transform-
ers, and define the notations used in our theoretical analysis. We use bold upper-case letters to denote
matrices and tensors, and bold lower-case letters to denote vectors. Let [m] = {1,2, ∙∙∙ ,m}. The
indicator function of A is denoted by 1a.
2.1	Convolutional Neural Networks
Convolutional Neural Networks (CNNs) are widely used in computer vision tasks, in which the
convolutional layer is the key component.
Convolutional layer. Given an image X ∈ RH ×W ×C, the output of a convolutional layer for
pixel (i, j) is given by
COnV(X )i,j, ： =	X	Xi+δ1,j+δ2,WδC,δ2,:,:,	(1)
(δ1,δ2)∈∆
where WC ∈ RK ×K ×C ×Dout is the learnable convolutional kernel, K is the size of the kernel and
the set ∆ = {-[K∕2], ∙∙∙ , [K/2]} X {-[K∕2], ∙∙∙ , [K/2" is the receptive field.
2.2	Vision Transformers
A Vision Transformer takes sequences of image patches as input. It usually begins with a patch
projection layer, followed by a stack of Transformer layers. A Transformer layer contains two
sub-layers: the multi-head self-attention (MHSA) sub-layer and the feed-forward network (FFN)
sub-layer. Residual connection (He et al., 2016) and layer normalization (Lei Ba et al., 2016) are
applied for both sub-layers individually. Some important components are detailed as follows:
2
Under review as a conference paper at ICLR 2022
Patch input. Consider an input image X ∈ RH×W×C, where H, W, C is the image height,
width and channel. To feed it into a Vision Transformer, it is reshaped into a sequence of flat-
tened patches X ∈ RN ×P2C, where P is the patch resolution, and N = HW/P2 is the sequence
length. Formally, a flattened patch is defined as Xi,： = Concat (XhiL仅小：,∙∙∙ , XhWp2,w.p2,：),
where (hii,Wii), ∙∙∙ , (hip2,wip2) are the positions of pixels in the i-th patch. Then a linear Pro-
jection is applied on all flattened patches to obtain the input to the Transformer.
Multi-head self-attention (MHSA) layer. The attention module is formulated as querying a dic-
tionary with key-value pairs, i.e., Attention(Q, K, V) = Softmax (QK>) V, where d is the
dimension of the hidden representations, and Q, K, V are referred to as queries, keys and values
that are all produced by linearly projecting the output of the previous layer. The multi-head variant
of the attention module is popularly used because it allows the model to jointly learn the informa-
tion from different representation sub-spaces. Formally, an MHSA layer with input H ∈ RN×d is
defined as:
NH
MHSA(H) =concat(SA1(H),…，SANH(H))WO = X SAk(H)WkO	(2)
k=1
SAk(H) = Attention(HWkQ,HWkK,HWkV),	(3)
where WQ, W K, WV ∈ Rd×dH and W O = (Wf>,…，WN>)> ∈ RNH dH ×dO are learn-
able projection matrices1, NH is the number of heads, dH is the size of each head, and dO is the
dimensionality of the output.
Relative positional encoding. Many Vision Transformer models adopt a learnable relative posi-
tion bias term in computing self-attention scores (Liu et al., 2021; Luo et al., 2021; Li et al., 2021):
Attention(Q, K, V) = Softmax (^√^——+ B) V,	(4)
where Bi,j only depends on the relative position between the i-th patch (query patch) and the j-th
patch (key patch). More specifically, assume the position of the '-th patch is (χ', yg), then Bi,j =
b(χi-Xj,yi-yj). For -H + 1 ≤ X ≤ H - 1 and -W + 1 ≤ y ≤ W - 1, b(x,y)is a trainable scalar.
3	Expressing convolution with the MHSA layer
In this section, we consider the question of using the MHSA layer in Vision Transformers to express
a convolutional layer. We mainly focus on the patch-input setting, which is more realistic for current
ViTs. First, we show that a MHSA layer in Vision Transformers can express a convolutional layer in
the patch-input setting (Theorem 1). Second, we prove lower bounds on the number of heads for self-
attention layers to express the convolution operation for both patch and pixel input settings, which
demonstrates that the number of heads required in Theorem 1 is optimal. Putting the representation
theorem and the lower bounds together, we conclude that MHSA layers with patch input are more
head-efficient in expressing convolutions. The dependency on the number of heads is more feasible
in the patch-input setting for Vision Transformers in practice.
3.1	An MHSA layer with enough heads can express a convolutional layer
Our main result in this subsection is that an MHSA layer can express convolution under mild as-
sumptions in the patch-input setting. To be precise, we present the following theorem:
Theorem 1. In the patch-input setting, assume dH ≥ d and dO ≥ P2 Dout. Then a multi-head
self-attention layer with NH =(2 ∣"K-I-∣ + 1) heads and relative positional encoding can express
any convolutional layer of kernel size K × K, and Dout output channels.
1For simplicity, the bias terms of linear projections are omitted.
3
Under review as a conference paper at ICLR 2022
The patch input poses the major difficulty in proving this result: The convolution operation can in-
volve pixels across patch boundaries, which makes the problem complicated. To address this, we
first aggregate the information from all the relevant patches for calculating the convolution by lever-
age the relative positional encoding and multi-head mechanism, and then apply a linear projection
on the aggregated features. This idea leads to a constructive proof.
Proof sketch of Theorem 1. Note that the attention calculation with relative positional encod-
ing can be dissected into a context-aware part (which depends on all the input tokens) and a posi-
tional attention part (which is agnostic to the input): In Equation (4), QK > and B correspond to
the context-aware part and the positional attention part respectively. Since convolution is context-
agnostic by nature, we set WkQ = WkK = 0 (∀ k ∈ [NH]) and purely rely on the positional
attention in the proof. Given any relative position δ between two patches, we force the query patch
to focus on exactly one key patch, such that the relative position between the query and the key is δ .
We elaborate on this argument in Lemma 2.
Lemma 2. For any relative position δ between two patches, there exists a relative positional en-
coding scheme B such that SoftmaX(Bq,：)k = l{q-k=δ}, where q, k are the index of the query/key
patch.2
Let the receptive field of a given patch in K × K convolution be the set of patches that contain at
least one pixel in the receptive field of any pixel in the given patch. Then it’s easy to see that the
relative position between a given patch and the patches in its receptive field are
K - 1
2P
K - 1
2P
K - 1
2P
K - 1
2P
(5)
×
With Lemma 2, we can force the query patch to attend to the patch at a given relative position in
∆ in each head. By setting WV = (Id, 0&乂即-d)), the hidden representation (before the final
projection WO) of the query patch contains the features of all the patches in its receptive field.
Finally, by the linearity of convolution, we can properly set the weights in WO based on the convo-
lution kernel, such that the final output is equivalent to that of the convolution for any pixel, which
concludes the proof. We refer the readers interested in a formal proof to Appendix A.2.	□
Remark on the positional encoding. In this result, we focus on a specific form of relative posi-
tional encoding. In fact, Theorem 1 holds as long as the positional encoding satisfies the property in
Lemma 2. It’s easy to check that a wide range of positional encoding have such property (Dai et al.,
2019; Raffel et al., 2020; Ke et al., 2020; Liu et al., 2021), so our result is general.
However, our construction does not apply to MHSA layers that only use absolute positional encod-
ing. In the prood, we need a separate context-agnostic term in calculating attention scores. However,
absolute positional encoding, which is typically added to the input representation, cannot be sepa-
rated from context-dependent information and generate the desired attention pattern in Lemma 2.
Remark on the pixel-input setting. It should be noted that the pixel-input setting is a special
case of the analyzed patch-input setting, since patches become pixels when patch resolution P = 1.
Therefore, the result in Cordonnier et al. (2020) can be viewed as a natural corollary of Theorem 1.
Corollary 3. In the pixel-input setting, a multi-head self-attention layer with NH = K2 heads of
dimension dH, output dimension dO and relative positional encodings can express any convolutional
layer of kernel size K × K and min{dH , dO } output channels.
Practical implications of Theorem 1. For Vision Transformers and CNNs used in practice, we
typically have K < 2P, e.g., P ≥ 16 in most Vision Transformers, and K = 3, 5, 7 in most CNNs.
Thus, the following corollary is more practical:
Corollary 4. In the patch-input setting, assume K < 2P, dH ≥ d and dO ≥ P2 Dout. Then
a multi-head self-attention layer with 9 heads and relative positional encoding can express any
convolutional layer of kernel size K × K and Dout output channels.
2Here we abuse the notation for ease of illustration: When used as subscripts, q, k are scalars in [N]; When
used to denote the locations of patches, q, k are two-dimensional coordinates in [H/P] × [W/P].
4
Under review as a conference paper at ICLR 2022
Another thing that would be important from a practical perspective is that Theorem 1 can be gen-
eralized to other forms of convolution operations, although we focus on the simplest formulation
defined in Equation 1. For example, people sometimes use convolution with stride greater than 1, or
dilated convolution (Yu & Koltun, 2015) in practice. This theorem can be easily generalized to these
cases. Intuitively, we only use the linearity of convolution in our proof, so any variant of convolution
that preserves this property can be expressed by MHSA layers according to our construction.
3.2	An MHSA layer with insufficient heads cannot express convolution
It’s noticeable that the multi-head mechanism plays an essential role in the constructive proof of
Theorem 1. Thus, it’s natural to ask whether the dependency on the number of heads is optimal in
the theorem. In this part, we present lower bounds on the number of heads required for an MHSA
layer to express convolution in both pixel-input and patch-input setting, highlighting the importance
of the multi-head mechanism and showing the optimality of our construction in the previous proof.
3.2.1	The pixel-input setting
We fisrt show that the dependency on the number of heads in Corollary 3 is optimal, i.e., an MHSA
layer must need K2 heads to express convolution of kernel size K × K.
Theorem 5. In the pixel-input setting, suppose NH < min{K 2 , d}. There exists a convolutional
kernel weight WC ∈ RK×K×d×Dout such that any MHSA layer with NH heads and relative posi-
tional encoding cannot express conv(∙; W C).
Proof. We will prove the theorem in the case where Dout = 1 by contradiction, and consequently
the result will hold for any Dout ∈ N*. Since Dout = 1,we view W C as a three-dimensional tensor.
In the convolutional layer, consider the output representation of the pixel at position γ ∈ [H] × [W]:
d
conv(X; WC)γ = XXXγ+δ,iWδC,i,	(6)
δ∈∆ i=1
where ∆ = {-[K∕2],…，[K/2" X {-[K∕2],…，[K/2".
In the MHSA layer, assume the attention score between the query pixel γ and the key pixel γ + δ in
the k-th head is ak(Y). Let WV WO=wk = (Wk,…，Wd)> ∈ Rd×Dout (recall that Cout = 1).
Then, the output representation of the pixel at position γ ∈ [H] × [W] is
NH	d	d	NH
MHSA(X)γ = XXaδh(γ)XXγ+δ,iWi = XXXγ+δ,i Xaδk(γ)Wik.	(7)
k=1 δ	i=1	δ i=1	k=1
Putting Equation 6 and 7 together, in order to ensure conv(X， WC)γ = MHSA(X)γ, we have
NH	NH
WδCi = Xak(Y)Wk (∀δ ∈ ∆,i ∈ [d]) ⇒ WC = Xak(Y)wk>,	(8)
k=1	k=1
22
where ak (Y) = (aδk (Y))δ∈∆ ∈ RK is a row vector for any k ∈ [NH], and WC ∈ RK ×d is
reshaped from the weights WC ∈ RK ×K ×d×1 .
Note that
rank X ak (Y)wk>	≤ NH < min{K 2， d}.	(9)
By properly choosing convolutional kernel weights WG such that rank(WWG) = min{K2, d}, we
conclude the proof by contradiction.	□
Remark. For Vision Transformers and CNNs used in practice, we typically have min{K2 ， d} =
K2. Thus this result shows that K2 heads are necessary, and Corollary 3 is optimal in terms of the
number of heads.
5
Under review as a conference paper at ICLR 2022
3.2.2 The patch-input setting
In the patch-input setting, we show that at least 9 heads are needed for MHSA layers to perform
convolution.
Theorem 6. In the patch-input setting, suppose P ≥ K and NH ≤ 8. There exists a convolutional
kernel weight WC ∈ RK×K×Din×Dout, such that any MHSA layer with NH heads and relative
positional encoding cannot express conv(∙; WG).
Similar to Theorem 5, this theorem is also proven with a rank-based argument. However, the proof
requires more complicated techniques to deal with the patch input, so we defer it to Appendix A.3.
Remark. For Vision Transformers and CNNs used in practice, we typically have P ≥ K , so this
result shows that Corollary 4 is also optimal in terms of the number of heads in practical cases.
Discussions on the theoretical findings. Our findings clearly demonstrate the difference between
the pixel-input and patch-input setting: patch input makes self-attention require less heads to per-
form convolution compared to pixel input, especially when K is large. For example, according to
Theorem 5, MHSA layers with pixel input need at least 25 heads to perform 5 × 5 convolution, while
those with patch input only need 9 heads. Usually the number of heads in a MHSA layer is small in
Vision Transformers, e.g., there are only 12 heads in ViT-base. Therefore, our theory is realistic and
aligns well with practical settings.
4	Two-phase training of Vision Transformers
Our theoretical results provide a construction that allows MHSA layers to express convolution. In
this section, we propose a two-phase training pipeline for Vision Transformers which takes advan-
tage of the construction. Then we conduct experiments using this pipeline and demonstrate that
our theoretical insight can be used to inject convolutional bias to Vision Transformers and improve
their performance in low data regimes. We also discuss additional benefits of the proposed training
pipeline from the optimization perspective. Finally, we conclude this section with a discussion on
the limitation of our method.
4.1	Method and implementation details
Two-phase training pipeline. Inspired by the theoretical findings, we propose a two-phase train-
ing pipeline for Vision Transformers in the low data regime, which is illustrated in Figure 1. Specif-
ically, we first train a “convolutional” variant of Vision Transformers, where the MHSA layer is
replaced by a K × K convolutional layer. We refer to this as the convolution phase of training.
After that, we transfer the weights in the pre-trained model to a Transformer model, and continue
training the model on the same dataset. We refer to this as the self-attention phase of training. The
non-trivial step in the pipeline is to initialize MHSA layers from well-trained convolutional layers,
and we utilize the construction in the proof of Theorem 1 to do so. Due to the existence of the con-
volution phase, we cannot use a [cls] token for classification. Instead, we follow Liu et al. (2021)
to perform image classification by applying global average pooling over the output of the last layer,
followed by a linear classifier. This method is commonly used in CNNs for image classification.
Intuitively, in the convolution phase, the model learns a “convolutional neural network” on the data
and enjoys the inductive bias including locality and spatial invariance which makes learning easier.
In the self-attention phase, the model mimics the pre-trained CNN in the beginning, and gradually
learns to leverage the flexibility and strong expressive power of self-attention.
Implementation details. While our theory focuses on a single MHSA layer, we experiment
with 6-layer Vision Transformers to show that our theoretical insight still applies when there are
stacked Transformer layers. We focus on the low-data regime and train our model on CIFAR-100
(Krizhevsky et al., 2009). The input resolution is set to 224, and the patch resolution P is set to 16.
In the convolution phase, we experiment models with convolutional kernel size K = 3 and 5. To
apply our theory, in the self-attention phase, the number of attention heads NH is set to 9. The input
and output dimension of MHSA layers d and dO are both set to 768. The size of each head dH is
6
Under review as a conference paper at ICLR 2022
Convolution Phase Self-Attention Phase
Figure 1: Overview of the two-phase training pipeline. See
Section 4.1 for details.
Convolutional initializaion
Epoch
Figure 2: Loss curves of CMHSA
with or without the warm-up stage
under two initialization schemes.
set to 768. The dimension of feed-forward layer dFFN is set to 3072. Detailed descriptions of the
experimental settings are presented in Appendix B.
4.2	Experimental results
For ease of illustration, we name our models as CMHSA-K (Convolutionalized MHSA) where K
is the size of the convolutional kernel in the first training phase. To demonstrate the effectiveness of
our approach, we choose several baselines models for comparison:
•	ViT-base proposed in (Dosovitskiy et al., 2021), which applies Transformers on image
classification straightforwardly.
•	DeiT-small and DeiT-base proposed in Touvron et al. (2021), which largely improve the
performance of ViT using strong data augmentation and sophisticated regularization.
•	CMHSA trained only in convolution phase or self-attention phase. When training directly
in self-attention phase, the model is initialized randomly. In Table 1, CMHSA-K (1st
phase) refers to models trained only in convolution phase, and CMHSA (2nd phase) refers
to models trained only in self-attention phase (Note that K is irrelevant in this case).
To ensure a fair comparison, all the baseline models are trained for 400 epochs, while our models
are trained for 200 epochs in each phase.
The experimental results are shown in Table 1. We evaluate the performance of the models in terms
of both test accuracy and training cost, and make the following observations on the results:
The proposed two-phase training pipeline largely improves performance. It’s easy to see that
DeiTs clearly outperform ViT, demonstrating the effectiveness of the training strategy employed
by DeiT. Furthermore, our models with two-phase training pipeline outperform DeiTs by a large
margin, e.g., the top-1 accuracy of our CMHSA-5 model is nearly 9% higher than that of DeiT-base.
This demonstrates that the proposed training pipeline can provide further performance gain on top
of the data augmentation and regularization techniques in the low-data regime.
Both training phases are important. From the last 5 rows of Table 1, we can see that under the
same number of epochs, CMHSAs trained with only one phase always underperform those trained
with both two phases. The test accuracy of CMHSA (2nd phase), which is a randomly initialized
CMHSA trained for 400 epochs, is much lower than that of our final models (which are trained in
both two phases). Therefore, the convolutional bias transferred from the first phase is crucial for
the model to achieve good performance. Besides, the models trained only in the first phase are also
7
Under review as a conference paper at ICLR 2022
Table 1: Experimental results on CIFAR-100 dataset.
Model	L	NH	d	dFFN	Top-1	Top-5	Training time
ViT-base	12	12	768	3072	60.90	86.66	1.00×
DeiT-small	12	6	384	1536	71.83	90.99	0.57×
DeiT-base	12	12	768	3072	69.98	88.91	1.00×
CMHSA-3 (1st phase)	6	-	768	3072	76.07	93.03	0.45×
CMHSA-5 (1st phase)	6	-	768	3072	76.12	93.13	0.49×
CMHSA (2nd phase)	6	9	768	3072	69.83	91.39	1.48×
CMHSA-3 (ours)	6	9	768	3072	76.72	93.74	0.96×
CMHSA-5 (ours)	6	9	768	3072	78.74	94.40	0.98×
worse than our final models. For example, CMHSA-5 outperforms CMHSA-5 (1st phase) by 2.62%.
This shows that the second phase enables the model to utilize the flexibility of MHSA layers to learn
better representations, achieving further improvements upon the convolutional inductive bias.
The convolutional phase helps to accelerate training. Training of Transformers is usually time-
consuming due to the high computational complexity of the MHSA module. In contrast, CNNs
enjoy much faster training and inference speed. In our proposed training pipeline, the convolution
phase is very efficient. Although the self-attention phase is slightly slower, we can still finish 400
epochs of training using less time compared with DeiT-base. In Table 1, it is clear that our model
significantly outperforms other models with a comparable training time.
4.3	Additional benefits of the two-phase training pipeline
As mentioned above, the two-phase training pipeline helps improve training efficiency and test ac-
curacy of Vision Transformers. In this subsection we emphasize an additional benefit of the method:
The injected convolutional bias makes the optimization process easier, allowing us to remove
the warm-up epochs in training.
The warm-up stage is crucial to stabilize the training of Transformers and improve the final per-
formance, but it also slows down the optimization and brings more hyperparameter tuning (Huang
et al., 2020; Xiong et al., 2020). We empirically show that, when initialized with a pretrained CNN,
our CMHSA model can be trained without warm-up and still obtains competitive performance.
We train CMHSA in the self-attention phase with pretrained convolution-phase initialization and
random initialization, and show the loss curves of the first 100 epochs in Figure 2. In the figure, the
x-axis indicates the number of epoch and the y-axis indicates the validation loss. “Convolutional
Initialization” refers to the models initialized from a pretrained convolution-phase, while “Random
Initialization” refers to the models initialized randomly. The only difference in experimental setting
between the “w/o warm up” and “w/ warm up” curves is whether a warm-up stage is applied, and
all the other hyperparameters are unchanged.
From Figure 2, we can see that when CMHSA is initialized randomly in the self-attention phase,
the training is ineffective without the warm-up stage. For example, it takes nearly 80 epochs for
the model without warm-up stage to reach the same validation loss achieved in the 20th epoch of
the model with warm-up. In contrast, when initialized from the pretrained convolution phase, the
validation losses are similar for models with and without the warm-up stage. After 200 epochs’
training, the model without warm-up stage achieves 78.90% top-1 accuracy, slightly outperforming
the model with warm-up stage (78.74%). Therefore, the proposed two-phase training pipeline can
take advantage of the convolutional inductive bias and make training of Vision Transformers easier,
while enables to remove the warm-up stage and eases the efforts of hyperparameter tuning.
Limitations. Finally, we point out that our current method cannot enable any ViTs to mimic CNNs
since we have some constraints on the ViT architecture. In particular, we require sufficient number
of heads (≥ 9). As suggested by our theory, an exact mapping doesn’t exist for smaller number of
heads, and it would be interesting to study how to properly initialize ViT from CNN even when the
exact mapping is not applicable.
8
Under review as a conference paper at ICLR 2022
5	Related Work and Discussions
5.1	Expressive power of self-attention layers and Transformers
As Transformers become increasingly popular, many theoretical results have emerged to study their
expressive power. Transformers are Turing complete (under certain assumptions) (Perez et al., 2019)
and universal approximators (Yun et al., 2019), and these results have been extended to Transformer
variants with sparse attention (Zaheer et al., 2020; Yun et al., 2020). Levine et al. (2020); Wies
et al. (2021) study how the depth, width and embedding dimension affect the expressive power
of Transformers. Dong et al. (2021) analyze the limitations of a pure self-attention Transformer,
illustrating the importance of FFN layers and skip connections.
There are also some works focusing on a single self-attention layer or the self-attention matrices.
Bhojanapalli et al. (2020) identify a low-rank bottleneck in attention heads when the size of each
head is small. Likhosherstov et al. (2021) proves that a fixed self-attention module can approximate
arbitrary sparse patterns depending on the input when the size of each head d = O(log N), where
N denotes the sequence length.
Our work is motivated by the recent success of Vision Transformers, and aims to compare a layer in
Transformers and in CNNs, which is different from the works mentioned above. The most relevant
work is (Cordonnier et al., 2020), which shows that a MHSA layer with K2 heads can express a
convolution of kernel size K. However, this result only focuses on the pixel-input setting, which is
infeasible for current Vision Transformers to apply, especially on high-resolution images. We study
the more realistic patch-input setting (of which the pixel-input setting is a special case). We also
derive lower bounds on the number of heads for such expressiveness, showing the optimality of our
result. Therefore, our work provides a more precise picture showing how a MHSA layer in current
ViTs can express convolution.
5.2	Training Vision Transformers
Despite Vision Transformers reveal extraordinary performance when pre-trained on large-scale
datasets (e.g., ImageNet-21k and JFT-300M), they usually lay behind CNNs when trained from
scratch on ImageNet, let alone smaller datasets like CIFAR-10/100. Previous methods usually em-
ploy strong augmentations (Touvron et al., 2021) or sophisticated optimizer (Chen et al., 2021) as
rescues. For instance, Chen et al. (2021) observe that enforcing the sharpness constraint during
training can dramatically enhance the performance of Vision Transformers. Touvron et al. (2021)
stack multiple data augmentation strategies to manually inject inductive biases. They also propose
to enhance the accuracy by distilling Vision Transformers from pre-trained CNN teachers.
Supported by our theoretical analysis, we propose a two-phase training pipeline to inject convolu-
tional bias into ViTs. The most relevant work to our approach is (D’Ascoli et al., 2021). D’Ascoli
et al. (2021) propose a variant of ViT called ConViT, and they also try to inject convolutional bias
into the model by initializing it following the construction in (Cordonnier et al., 2020) so that the
model can perform convolution operation at initialization, which resembles the second phase of our
training pipeline. However, their work differs from ours in several aspects: First, their initializa-
tion strategy only applies in the pixel-input setting. Thus the models can only perform convolution
on images which are 16× downsampled. By contrast, our construction enables MHSA layers with
patch input to perform convolution on the original image. Second, they only initialize the attention
module to express a random convolution, while our method explicitly transfers information from
a well-learned CNN into a ViT. Third, ConViT makes architectural changes by introducing Gated
Positional Self-Attention layers, while we keep the MHSA module unmodified. 6
6 Conclusion
In this work, we prove that a single ViT layer can perform any convolution operation construc-
tively, and we further provide a lower bound on the number of heads for Vision Transformers to
express CNNs. Corresponding with our analysis, we propose a two phase training pipeline to help
inject convolutional bias into Transformers, which improves test accuracy, training efficiency and
optimization stability of ViTs in the low data regimes.
9
Under review as a conference paper at ICLR 2022
Ethics statement
We don’t see any potential ethical issues in our work.
Reproducibility statement
For all our theoretical results, the assumptions are stated clearly, and the complete proof can be
found in either the main body of the paper (Section 3.2.1) or the appendix (Appendix A).
For all our experimental results, we provide our code in the supplementary materials as well as a
brief description of the code. We also provide implementation details in our paper, including the hy-
perparameters, the code base and the type of the machines we use, see Section 4.1 and Appendix B.
References
Srinadh Bhojanapalli, Chulhee Yun, Ankit Singh Rawat, Sashank Reddi, and Sanjiv Kumar. Low-
rank bottleneck in multi-head attention models. In International Conference on Machine Learn-
ing,pp. 864-873. PMLR, 2020.
Xiangning Chen, Cho-Jui Hsieh, and Boqing Gong. When vision transformers outperform resnets
without pretraining or strong data augmentations, 2021.
Jean-Baptiste Cordonnier, Andreas Loukas, and Martin Jaggi. On the relationship between self-
attention and convolutional layers. In International Conference on Learning Representations,
2020. URL https://openreview.net/forum?id=HJlnC1rKPB.
Zihang Dai, Zhilin Yang, Yiming Yang, William W Cohen, Jaime Carbonell, Quoc V Le, and Ruslan
Salakhutdinov. Transformer-xl: Attentive language models beyond a fixed-length context. arXiv
preprint arXiv:1901.02860, 2019.
Stephane D'Ascoli, Hugo Touvron, Matthew L Leavitt, Ari S Morcos, Giulio Biroli, and Levent
Sagun. Convit: Improving vision transformers with soft convolutional inductive biases. In Marina
Meila and Tong Zhang (eds.), Proceedings of the 38th International Conference on Machine
Learning, volume 139 of Proceedings of Machine Learning Research, pp. 2286-2296. PMLR,
18-24 Jul 2021.
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep
bidirectional transformers for language understanding. In Proceedings of the 2019 Conference of
the North American Chapter of the Association for Computational Linguistics: Human Language
Technologies, Volume 1 (Long and Short Papers), pp. 4171-4186, 2019.
Yihe Dong, Jean-Baptiste Cordonnier, and Andreas Loukas. Attention is not all you need: Pure
attention loses rank doubly exponentially with depth. arXiv preprint arXiv:2103.03404, 2021.
Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas
Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszko-
reit, and Neil Houlsby. An image is worth 16x16 words: Transformers for image recogni-
tion at scale. In International Conference on Learning Representations, 2021. URL https:
//openreview.net/forum?id=YicbFdNTTy.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recog-
nition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp.
770-778, 2016.
Xiao Shi Huang, Felipe Perez, Jimmy Ba, and Maksims Volkovs. Improving transformer opti-
mization through better initialization. In International Conference on Machine Learning, pp.
4475-4483. PMLR, 2020.
Guolin Ke, Di He, and Tie-Yan Liu. Rethinking positional encoding in language pre-training. In
International Conference on Learning Representations, 2020.
Alex Krizhevsky et al. Learning multiple layers of features from tiny images. 2009.
10
Under review as a conference paper at ICLR 2022
Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint
arXiv:1607.06450, 2016.
Yoav Levine, Noam Wies, Or Sharir, Hofit Bata, and Amnon Shashua. The depth-to-width interplay
in self-attention. arXiv preprint arXiv:2006.12467, 2020.
Yang Li, Si Si, Gang Li, Cho-Jui Hsieh, and Samy Bengio. Learnable fourier features for multi-
dimensional spatial positional encoding. In NeurIPS, 2021.
Valerii Likhosherstov, Krzysztof Choromanski, and Adrian Weller. On the expressive power of
self-attention matrices. arXiv preprint arXiv:2106.03764, 2021.
Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike
Lewis, Luke Zettlemoyer, and Veselin Stoyanov. Roberta: A robustly optimized bert pretraining
approach. arXiv preprint arXiv:1907.11692, 2019.
Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining
Guo. Swin transformer: Hierarchical vision transformer using shifted windows. arXiv preprint
arXiv:2103.14030, 2021.
Ilya Loshchilov and Frank Hutter. Fixing weight decay regularization in adam, 2018.
Shengjie Luo, Shanda Li, Tianle Cai, Di He, Dinglan Peng, Shuxin Zheng, Guolin Ke, Liwei Wang,
and Tie-Yan Liu. Stable, fast and accurate: Kernelized attention with relative positional encoding.
In NeurIPS, 2021.
Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor
Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. Pytorch: An imperative style,
high-performance deep learning library. Advances in Neural Information Processing Systems, 32:
8026-8037, 2019.
Jorge Perez, Javier Marinkovic, and Pablo Barcelo. On the turing completeness of modern neural
network architectures. In International Conference on Learning Representations, 2019. URL
https://openreview.net/forum?id=HyGBdo0qFm.
Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi
Zhou, Wei Li, and Peter J Liu. Exploring the limits of transfer learning with a unified text-to-text
transformer. Journal of Machine Learning Research, 21:1-67, 2020.
Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco Massa, Alexandre Sablayrolles, and
Herve Jegou. Training data-efficient image transformers & distillation through attention. In
Marina Meila and Tong Zhang (eds.), Proceedings of the 38th International Conference on
Machine Learning, volume 139 of Proceedings of Machine Learning Research, pp. 10347-
10357. PMLR, 18-24 Jul 2021. URL https://proceedings.mlr.press/v139/
touvron21a.html.
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N
Gomez, E Ukasz Kaiser, and Illia Polosukhin. Attention is all you need. In I. Guyon,
U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett
(eds.), Advances in Neural Information Processing Systems, volume 30. Curran Asso-
ciates, Inc., 2017. URL https://proceedings.neurips.cc/paper/2017/file/
3f5ee243547dee91fbd053c1c4a845aa- Paper.pdf.
Noam Wies, Yoav Levine, Daniel Jannai, and Amnon Shashua. Which transformer architecture fits
my data? a vocabulary bottleneck in self-attention. arXiv preprint arXiv:2105.03928, 2021.
Ross Wightman. Pytorch image models. https://github.com/rwightman/
pytorch-image-models, 2019.
Ruibin Xiong, Yunchang Yang, Di He, Kai Zheng, Shuxin Zheng, Chen Xing, Huishuai Zhang,
Yanyan Lan, Liwei Wang, and Tieyan Liu. On layer normalization in the transformer architecture.
In International Conference on Machine Learning, pp. 10524-10533. PMLR, 2020.
11
Under review as a conference paper at ICLR 2022
Fisher Yu and Vladlen Koltun. Multi-scale context aggregation by dilated convolutions. arXiv
preprint arXiv:1511.07122, 2015.
Chulhee Yun, Srinadh Bhojanapalli, Ankit Singh Rawat, Sashank Reddi, and Sanjiv Kumar. Are
transformers universal approximators of sequence-to-sequence functions? In International Con-
ference on Learning Representations, 2019.
Chulhee Yun, Yin-Wen Chang, Srinadh Bhojanapalli, Ankit Singh Rawat, Sashank J. Reddi, and
Sanjiv Kumar. O(n) connections are expressive enough: Universal approximability of sparse
transformers. In NeurIPS, 2020. URL https://proceedings.neurips.cc/paper/
2020/hash/9ed27554c893b5bad850a422c3538c15- Abstract.html.
Manzil Zaheer, Guru Guruganesh, Kumar Avinava Dubey, Joshua Ainslie, Chris Alberti, Santiago
Ontanon, Philip Pham, Anirudh Ravula, Qifan Wang, Li Yang, et al. Big bird: Transformers for
longer sequences. In NeurIPS, 2020.
12
Under review as a conference paper at ICLR 2022
Appendix
A Omitted proofs of theoretical results
A.1 Proof of Lemma 2
Proof. Recall that Bij = b{χi-χj,yi-yj) where (χ', y') denotes the position the '-th patch. Set
bδ0 = M and bδ = 0(δ 6= δ0), where M is a scalar. Then
{m L ι q — k = δ
e +M-	(10)
eM +N-1	q - k = δ
Note that
eM
lim -τ7------= 1.
M→+∞ eM + N - 1
lim -j-r-1----= 0.
M→+∞ eM + N - 1
(11)
(12)
Therefore, we only need to set M tobe sufficiently large number to conclude the proof. For example,
by setting M = 40 We will have Softmax(Bq,：)k = l{q-k=δ} UP to machine precision.	□
A.2 Proof of Theorem 1
Proof. AssUme the inpUt (seqUence of flattened image patches) is X . We only need to prove the
resUlt for dH = d and dO = P2Dout, since an MHSA layer with larger dH and/or dO is at least as
expressive as the one with dH = d and dO = P 2Dout .
Define the receptive field of a given patch in K × K convolUtion be the set of patches which contain
at least one pixel in the receptive field of any pixel in the given patch. Then it’s easy to see that the
relative position between a given patch and the patches in its receptive field are
∆
—
K — 1
2P
K-1	K-1
2P J × [	2P
K — 1
2P
(13)
Note that NH = ∣∆∣. Therefore, for any relative position index δ ∈ ∆, we can assign an attention
head for it, sUch that the qUery patch always attends to the patch at the given relative position δ in
this head. We fUrther set WkV = (Id) (recall that dH = d). ConseqUently, the hidden representation
(before the final projection WO) of the qUery patch is the concatenation of the inpUt featUres of all
the patches in its receptive field. Precisely speaking, in EqUation 2, we have
Concat(SAI(X),…，SANH(X))q,： =Concat(Xq+δ,Jδ∈∆∙	(14)
In the convolUtional layer defined by WC, the oUtpUt featUre of any pixel in the q-thpatch is a linear
function of concat(Xq+δ,: )δ∈∆. So the output feature of the whole patch is also a linear function
of concat(Xq+δ,Jδ∈∆. Therefore, there exists a linear projection matrix W O such that
MHSA(X )q = concat(Xq+δ,Jδ∈∆ W O = conv(X )q	(15)
Moreover, due to the translation invariance property of the convolution operation, the linear projec-
tion matrix WO does not depend on q. Therefore, MHSA(X)q = conv(X)q holds for any q. In
other words, MHSA(X) = conv(X).	□
Remark. Indeed, we can presents WO constructively: Assume r ∈ [NH]; s, t ∈ [P2]; i ∈
[Din]; j ∈ [Dout]. Then W(Or-1)d+(s-1)Din+i,(t-1)Dout+j = WxC(r,s,t),y(r,s,t),i,j , where
x(r, s, t), y(r, s, t) ∈ [K] ∪ {0} are defined as follows:
Let q be a patch on the image, and let ∆ = {δι,…，δwH}. When the s-th pixel in the (q + δr)-
th patch is in the receptive field of the t-th pixel in the q-th patch, we use (x(r, s, t), y(r, s, t)) to
13
Under review as a conference paper at ICLR 2022
denote its location in the receptive field. Otherwise, we let (x(r, s, t), y(r, s, t)) = (0, 0), and define
WxC(r,s,t),y(r,s,t),i,j = 0 in this case.
This construction will be useful in our experiment, which requires to transfer the knowledge of a
convolutional layer into an MHSA layer (Section 4.1).
A.3 Proof of Theorem 6
Proof. We will prove the theorem in the case where Dout = 1, and consequently the result
will hold for any Dout ∈ N*. Furthermore, We assume that Din = 1 since We can set WC,2：,： ∈
0RK×K×(Din-1)×Dout if Din > 1.
In this Way, the convolution computation Will ignore all but the
first channel.
Assume the input (sequence of flattened image patches) is X. Recall that a flattened patch is defined
as the concatenation of the features of all the pixels in it, i.e.,
Xi,: =concat 江自华小：,…,XhiP。。,：)	(16)
Since We have assumed that Din = 1, the feature of a pixel Xhip,wip,： is actually a scalar. Thus
Xi,： ∈ RP2,i.e., d = P2.
If the output of the MHSA layer could express convolution, the output representation of a patch must
contain the output representations of all the pixels in the convolutional layer. Again, since Dout = 1,
We can assume that the output dimension of the MHSA layer dO = P2. In other Words, the output
representation ofa patch is the concatenation of the output representations of all its pixels.
Therefore, WkV WkO ∈RP2×P2 (∀k ∈ [NH]), and We let WkV WkO = (wpkq)p,q∈[P 2].
In the MHSA layer, assume the attention distribution of query patch γ in the k-th head is ak (γ) =
(aδk (γ))γ+δ∈[H]×[W], Where δ stands for the relative position betWeen the query patch and the key
patch. Consider the output feature of the pixel at position q in patch γ (q denotes the location of the
pixel on the patch, and γ denotes the location of the patch on the image). We have
NH	P2
MHSA(X)γ,q =	aδk(γ)	Xγ+δ,qwpkq	(17)
k=1 δ	q=1
P2	NH
=XXXγ+δ,qXaδk(γ)wpkq.	(18)
The above experssion is a linear transformation of X . In the convolutional layer, only pixels in
the 9 neighboring patches (including the center patch itself) can be relevant, since P > K. Thus,
ak(γ) > 0 only for δ ∈ ∆ = {-1, 0,1}2 := {δι,…，δg}.
Let the (flattened) convolutional kernel WC = (WC,…,WK2) ∈ RK2, and additionally let WC =
0. Then for any p, q ∈ RP2, δ ∈ ∆, We have
NH
X aδh(γ)Wphq = WkC(p,q,δ) ,	(19)
h=1
Where k(p, q, δ) ∈ [K2] ∪ {0} is an index dependent on p, q and δ. k(p, q, δ) 6= 0 if and only if the
q-th pixel in the γ + δ-th patch is in the receptive field of the p-th pixel in the γ-th patch. When
k(p, q, δ) 6= 0, the value of k(p, q, δ) only depends on the relative position betWeen the tWo pixels.
Let Wpq = (Wpq ,…，WNH), and
(aδι	…	a19 ʌ	(WC(1,1,δι)… WC(1,1,δ9) ʌ
A =	.	. I , WG =	.	. I .	(20)
\aNH …a川	'We"'】)…Wk(P,P,δJ
w11
w12
W =	.
.
.
wPP
14
Under review as a conference paper at ICLR 2022
Then Eqn 19 can be written in matrix form as WA = W C.
Since P ≥ K, all the column in W C is either a one-hot or a zero vector (pixels at the same position
in the patches cannot be in the receptive field of one pixel). Besides, none of the 9 rows is zero since
they are all needed for the convolution computation. Therefore, We can select 9 columns in W C
and reorder them properly to form a diagonal sub-matrix of W C, which implies rank (W C) = 9
as long as all the entries in the convolutional kernel is non-zero.
On the other hand, rank(W A) ≤ rank(W) ≤ NH ≤ 8, which leads to a contradiction and
concludes the proof.	□
B	Details on the experiment settings
In the experiments, we evaluate our CMHSA-3/5 models on CIFAR-100 (Krizhevsky et al., 2009),
using the proposed two-phase training pipeline. In both phases, the model is trained for 200 epochs
with a 5-epoch warm-up stage followed by a cosine decay learning rate scheduler. In the convolu-
tional phase, the patch projection layer is fixed as identity.
To train our models, we AdamW use as the optimizer, and set its hyperparameter ε to 1e - 8 and
(β1, β2) to (0, 9, 0.999) (Loshchilov & Hutter, 2018). We experiment with peak learning rate in
{1e - 4, 3e - 4, 5e - 4} in the convolution phase, and {1e - 5, 3e - 5, 5e - 5, 7e - 5} in the self-
attention phase. The batch size is set to 128 in both phases. We employ all the data augmentation
and regularization strategies of Touvron et al. (2021), and remain all the relevant hyperparameters
unmodified.
Our codes are implemented based on PyTorch (Paszke et al., 2019) and the timm library (Wight-
man, 2019). All the models are trained on 4 NVIDIA Tesla V100 GPUs with 16GB memory and
the reported training time is also measured on these machines.
15