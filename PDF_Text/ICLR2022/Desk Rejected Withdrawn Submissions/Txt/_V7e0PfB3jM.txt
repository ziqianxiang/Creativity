Under review as a conference paper at ICLR 2022
Noisy '0-Sparse SUBSPACE Clustering on DIMEN-
sionality Reduced Data
Anonymous authors
Paper under double-blind review
Ab stract
High-dimensional data often lie in or close to low-dimensional subspaces. Sparse
SUbsPace clustering methods with sparsity induced by '0-norm, such as '0-Sparse
SUbsPace Clustering ('0-SSC) (Yang et al., 2016), are demonstrated to be more
effective than its `1 counterpart such as Sparse Subspace Clustering (SSC) (El-
hamifar & Vidal, 2013). However, the theoretical analysis of '0-SSC is restricted
to clean data that lie exactly in subspaces. Real data often suffer from noise and
they may lie close to subspaces. in this paper, we show that an optimal solution
to the optimization problem of noisy '0-SSC achieves Subspace Detection Prop-
erty (SDP), a key element with which data from different subspaces are separated,
under deterministic and randomized models. our results provide theoretical guar-
antee on the correctness of noisy '0-SSC in terms of SDP on noisy data for the first
time. In order to improve the efficiency of noisy '0-SSC, We propose Noisy-DR-
'0-SSC and Noisy-DR-I0-SSC-OSNAP which provably recover the subspaces on
dimensionality reduced data. Both algorithms first project the data onto a lower
dimensional space by linear transformation, then perform noisy '0-SSC on the di-
mensionality reduced data for improved efficiency. Experimental results demon-
strate the effectiveness of Noisy-DR-'0-SSC and Noisy-DR-I0-SSC-OSNAP.
1	Introduction
Clustering is an important unsupervised learning procedure for analyzing a broad class of scien-
tific data. High-dimensional data, such as facial images and gene expression data, often lie in low-
dimensional subspaces in many cases, and clustering in accordance to the underlying subspace struc-
ture is particularly important. For example, the well-known Principal Component Analysis (PCA)
works perfectly if the data are distributed around a single subspace. The subspace learning literature
develops more general methods that recover multiple subspaces in the original data, and subspace
clustering algorithms (Vidal, 2011) aim to partition the data such that data belonging to the same
subspace are identified as one cluster. Among various subspace clustering algorithms, the ones that
employ sparsity prior, such as Sparse Subspace Clustering (SSC) (Elhamifar & Vidal, 2013) and
'0-Sparse Subspace Clustering ('0-SSC) (Yang et al., 2016), have been proven to be effective in
separating the data in accordance with the subspaces that the data lie in under certain assumptions.
Sparse subspace clustering methods construct the sparse similarity matrix by sparse representation
of the data. Subspace detection property (SDP) defined in Section 2.2 ensures that the similar-
ity between data from different subspaces vanishes in the sparse similarity matrix, and applying
spectral clustering (Ng et al., 2001) on such sparse similarity matrix leads to compelling clustering
performance. Elhamifar and Vidal (Elhamifar & Vidal, 2013) prove that when the subspaces are
independent or disjoint, SDP can be satisfied by solving the canonical sparse linear representation
problem using data as the dictionary, under certain conditions on the rank, or singular value of the
data matrix and the principle angle between the subspaces. SSC has been successfully applied to a
novel deep neural network architecture, leading to the first deep sparse subspace clustering method
(Peng et al., 2016). Under the independence assumption on the subspaces, low rank representation
(Liu et al., 2010; 2013) is also proposed to recover the subspace structures. Relaxing the assump-
tions on the subspaces to allowing overlapping subspaces, the Greedy Subspace Clustering (Park
et al., 2014) and the Low-Rank Sparse Subspace Clustering (Wang et al., 2013) achieve subspace
detection property with high probability. The geometric analysis in (Soltanolkotabi & Candes, 2012)
1
Under review as a conference paper at ICLR 2022
shows the theoretical results on subspace recovery by SSC. In the following text, we use the term
SSC or '1-SSC exchangeably to indicate the Sparse SUbsPace Clustering method in (Elhamifar &
Vidal, 2013).
Real data often suffer from noise. The correctness of noisy SSC is analyzed in (Wang & Xu, 2013)
which handles noisy data that lie close to disjoint or overlapping subspaces, and the original op-
timization problem of noisy SSC is proposed in (Elhamifar & Vidal, 2013). While '0-SSC (Yang
et al., 2016) renders better empirical performance than '1-SSC, its correctness is only proved for
clean data based on a constrained '0-minimization problem, and '0-SSC empirically solves an un-
constrained '0-regularized problem to handle noise in data. As a result, it lacks theoretical guarantee
of the correctness on noisy data. In the sequel, We refer to '0-SSC for noisy data with the uncon-
strained '0-regularized problem in (Yang et al., 2016) as noisy '0-SSC. In this paper, the correctness
of noisy '0-SSc on noisy data in terms of the subspace detection property is established. Our analy-
sis is under both deterministic model and randomized models, which are also the models employed
in the geometric analysis of SSC (Soltanolkotabi & Candes, 2012). Our randomized analysis demon-
strates potential advantage of noisy '0-SSC over its '1 counterpart as more general assumption on
data distribution can be adopted. Due to the high dimension of the original data, the computational
cost of noisy '0-SSC is also high. To address this problem, we present Noisy Dimensionality Re-
duced '0-Sparse Subspace Clustering (NOiSy-DR-'0-SSC) and Noisy-DR-I0-SSC-OSNAP, which
improve the efficiency of noisy '0-SSC while provably enjoying theoretical correctness and robust-
ness to noise.
1.1	Contributions
Firstly, we propose two novel algorithms improving the efficiency of noisy '0-SSC, Noisy Di-
mensionality Reduced '0-Sparse Subspace Clustering (Noisy-DR-'0-SSC) and Noisy-DR-'0-SSC-
OSNAP in Section 3. Both algorithms accelerate noisy '0-SSC and they enjoy correctness in terms
of the subspace detection property under the deterministic model for noisy data. Noisy-DR-'0-SSC
first projects the data onto a lower dimensional space by random projection, then performs noisy `0-
SSC on the dimensionality reduced data. Noisy-DR-I0-SSC-OSNAP further improves the efficiency
ofNoisy-DR-'0-SSC by using the sparse linear transformation according to Oblivious Sparse Norm-
Approximating Projection (OSNAP) (Nelson & Nguyen, 2013). Experimental results demonstrate
the effectiveness ofboth Noisy-DR-'0-SSC and NOiSy-DR-'0-SSC-OSNAP.
Secondly, the correctness of noisy '0-SSC on noisy data in terms of the subspace detection property
is established for the first time, which is presented in Section A in the appendix of this paper. Our
analysis is under both deterministic model and randomized models, which are also the models em-
ployed by the geometric analysis of SSC (Soltanolkotabi & Candes, 2012). Our randomized analysis
demonstrates potential advantage of noisy '0-SSC over its '1 counterpart as noisy '0-SSC requires
less restrictive assumptions. The analysis of noisy '0-SSC lays the foundation for our analysis of
Noisy-DR-'0-SSC and Noisy-DR-I0-SSC-OSNAP
1.2	Notations
We use bold letters for matrices and vectors, and regular lower letter for scalars throughout this
paper. The bold letter with superscript indicates the corresponding column of a matrix, e.g. Ai is
the i-th column of matrix A, and the bold letter with subscript indicates the corresponding element
of a matrix or vector. ∣∣ ∙ ∣∣f and ∣∣ ∙ ∣∣p denote the Frobenius norm and the vector 'p-norm or
the matrix p-norm, and k ∙ ∣∣0 is the '0-norm, that is, the number of nonzero elements of a vector.
diag(∙) indicates the diagonal elements of a matrix. HT ⊆ Rd indicates the subspace spanned by
the columns of T, and AI denotes a submatrix of A whose columns correspond to the nonzero
elements of I (or with indices in I without confusion). σt(∙) denotes the t-th largest singular value
of a matrix, and σmin(∙) indicates the smallest singular value of a matrix. supp(∙) is the support of
a vector, PS0 is the operator of orthogonal projection onto the subspace S0 . [n] represents all the
natural numbers between 1 and n inclusively.
2
Under review as a conference paper at ICLR 2022
2	Problem Setup
2.1	Notations
We hereby introduce the notations for subspace clustering on noisy data considered in this paper.
The uncorrupted data matrix is denoted by Y = [y1, . . . , yn] ∈ Rd×n, where d is the dimensionality
and n is the size of the data. The uncorrupted data Y lie in a union of K distinct subspaces {Sk}kK=1
of dimensions {dk}kK=1. The observed noisy data is X = Y + N, where N = [n1, . . . , nn] ∈
Rd×n is the additive noise. xi = yi + ni is the noisy data point that is corrupted by the noise ni .
K
Let Y(k) ∈ Rd×nk denote the data belonging to subspace Sk with P nk = n, and denote the
k=1
corresponding columns in X by X(k) . The data X are normalized such that each column has unit
'2-norm in our deterministic analysis. We consider deterministic noise model where the noise Z is
fixed and max kni k ≤ δ. Note that our analysis can be extended to a random noise model which
is common and also considered by noisy SSC (Wang & Xu, 2013), and the random noise model
assumes that columns of Z are sampled i.i.d. and max knik ≤ δ with high probability. Note that
such random noise model does not require spherical symmetric noise as that in (Wang & Xu, 2013).
2.2	REVISIT '0-SSC
'0-SSC (Yang et al., 2016) proposes to solve the following '0 sparse representation problem
min kZk0 s.t. X = XZ, diag(Z) = 0,	(1)
and it proves that the subspace detection property defined in Definition 2.1 is satisfied with the
globally optimal solution to (1). In (Yang et al., 2016), the `0 regularized sparse approximation
problem below is solved so as to handle noisy data for '0-SSC, which is the optimization problem
ofnoisy '0-SSC:
min	L(Z) = kX - XZk2F + λkZk0.	(2)
Z∈Rn×n,diag(Z)=0
The optimization problem of noisy '0-SSC (2) is separable. For each 1 ≤ i ≤ n, the optimization
problem with respect to the sparse code of i-th data point is
min
β∈Rn,βi=0
L(β)
kxi - Xβ k22 + λkβ k0 .
(3)
The definition of subspace detection property for noisy '0-SSC and noiseless '0-SSC, i.e. '0-SSC
on noiseless data, is defined in Definition 2.1 below.
Definition 2.1. (Subspace detection property for noisy and noiseless '0-SSC) Let Z be an optimal
solution to (2). The subspaces {Sk}kK=1 and the data X satisfy the Subspace Detection Property
(SDP) for noisy '0-SSC if Zi is a nonzero vector, and nonzero elements of Zi correspond to the
columns of X from the same subspace as yi for all 1 ≤ i ≤ n. Similarly, in the noiseless setting
where X = Y, let Z be an optimal solution to (1). The subspaces {Sk}3ι and the data X satisfy
the SDP for noiseless '0-SSC if Zi is a nonzero vector, and nonzero elements of Zi correspond to
the columns of X that from the same subspace as yi for all 1 ≤ i ≤ n. We say that SDP holds for
Xi if nonzero elements of Z*i correspond to the data that lie in the same subspace as yi, for either
noisy '0-SSC or noiseless '0-SSC.
It should be emphasized that the theoretical analysis of'0-SSC (Yang et al., 2016) is limited to clean
data, as a result, it is not clear if SDP can hold for an optimal solution to the noisy '0-SSC problem.
In Section A of the appendix, we prove the correctness of noisy '0-SSC in terms of SDP under
both deterministic and randomized models for this first time. Under the deterministic model, the
subspaces and the data in each subspace are fixed. Please refer to Section A.1 of the appendix for
the details of all the details of deterministic and randomized models.
Before presenting our methods of noisy '0-SSC on dimensionality reduced data, we introduce the
definitions of general positions and external subspace which are crucial for the theoretical analysis.
3
Under review as a conference paper at ICLR 2022
Definition 2.2. (General position) For any 1 ≤ k ≤ K, the data Y(k) are in general position if any
subset of L ≤ dk data points (columns) of Y(k) are linearly independent. Y are in general position
if Y(k) are in general position for 1 ≤ k ≤ K.
The assumption of general condition is rather mild. In fact, if the data points in X(k) are inde-
pendently distributed according to any continuous distribution, then they almost surely in general
position.
Let the distance between a point x ∈ Rd and a subspace S ⊆ Rd be defined as d(x, S) =
inf y∈S kx - yk2, the definition of external subspaces is presented as follows. Figure 2 in the ap-
pendix illustrates an example of external subspace.
Definition 2.3. (External subspace) For a point y ∈ Y(k), a subspace H{yi }L spanned by a
set of linear independent points {yij }jL=1 ⊆ Y is defined to be an external subspace of y if
{yij}jL=1 6⊆ Y(k) andy ∈/ {yij}jL=1. The point y is said to be away from its external subspaces if
minH∈Hy,d d(y, H) > 0, where Hy,d are the set of all external subspaces of y of dimension no
greater than dfory, i.e. Hy,d = {H: H = H{yi }L , dim[H] = L, L ≤ d, {yij}jL=1 6⊆ Y(k), y ∈/
{yij}jL=1}. All the data points in Y(k) are said to be away from the external subspaces if each of
them is away from the its associated external spaces.
3	Noisy '0-SSC ON Dimensionality Reduced DATA
Albeit the theoretical guarantee and compelling empirical performance of noisy '0-SSC, the com-
putational cost of noisy '0-SSC is high with the high dimensionality of the data. In this section, We
propose two methods of performing noisy '0-SSC on dimensionality reduced data, which are Noisy-
DR-'0-SSC and Noisy-DR-I0-SSC-OSNAP, so as to improve the efficiency of noisy '0-SSC. Both
methods apply a linear transformation to the original data so as to reduce its dimension, then perform
noisy '0-SSC on the dimensionality reduced data. Noisy-DR-'0-SSC uses the linear transformation
induced by randomized low-rank approximation of the original data. Noisy-DR-'0-SSC-OSNAP is
even more efficient than Noisy-DR-'0-SSC by using the sparse linear transformation according to
Oblivious Sparse Norm-Approximating Projection (OSNAP) (Nelson & Nguyen, 2013). The the-
oretical guarantee on the correctness of Noisy-DR-'0-SSC and Noisy-DR-'0-SSC-OSNAP under
deterministic models is presented.
3.1	Method
Noisy-DR-'0-SSC performs subspace clustering by the following two steps: 1) obtain the dimension
reduced data XX = PX with a linear transformation P ∈ Rp×d (p < d). 2) perform noisy '0-SSC
on the compressed data X:
.~ .	..	~	..C
一 min	L(β) = ∣∣Xi- Xβk2 + 川βk0.	(4)
β∈Rn,βi=0
If p < d, Noisy-DR-'0-SSC operates on the compressed data X rather than on the original data, so
that its efficiency is improved.
3.2	Analysis
High-dimensional data often exhibits low-dimensional structures, which often leads to low-rankness
of the data matrix. intuitively, if the data is low rank, then it could be safe to perform noisy '0-SSC
on its dimensionality reduced version by the linear projection P, and it is expected that P can
preserve the information of the subspaces contained in the original data as much as possible, while
effectively removing uninformative dimensions.
To this end, we propose to choose P as a random projection induced by randomized low-rank ap-
proximation of the data in this subsection. The key idea is to obtain an approximate low-rank
decomposition of the data. Using the random projection induced by such low-rank approximation
4
Under review as a conference paper at ICLR 2022
as the linear transformation P, the clustering correctness still holds for Noisy-DR-'0-SSC with a
high probability.
Randomized algorithms are efficient and they have been extensively studied in the computer science
and numerical linear algebra literature. They have been employed to accelerate various numeri-
cal matrix computation and matrix optimization problems, including random projection for matrix
decomposition (Frieze et al., 2004; Drineas et al., 2004; Sarlos, 2006; Drineas et al., 2006; 2008;
Mahoney & Drineas, 2009; Drineas et al., 2011; Lu et al., 2013).
Formally, a random matrix T ∈ Rn×p is generated such that each element Tij is sampled inde-
pendently according to the Gaussian distribution N(0, 1). QR decomposition is then performed on
XT to obtain the basis of its column space, namely XT = QR where Q ∈ Rd×p is an orthogonal
matrix of rank p and R ∈ Rp×p is an upper triangle matrix. The columns of Q form the orthogonal
basis for the sample matrix XT. An approximation of X is then obtained by projecting X onto
the column space of XT: QQ>X = QW = Xb where W = Q>X ∈ Rp×n. In this manner, a
randomized low-rank decomposition of X is achieved by Xb = QW.
We present probabilistic result on the correctness of NoiSy-DR-'0-SSC using the random projection
induced by randomized low-rank decomposition of the data X, namely P = Q>. In the sequel,
X = PX for any X ∈ Rn. Thanks to the analysis of noisy '0-SSC, in order to guarantee the subspace
detection property on the dimensionality-reduced data X, it is crucial to make sure that the condi-
tions, such as (29) and (30) in Theorem A.4, of Section A still hold after the linear transformation.
-c-r τ F , 1 Pl⅛	J 1 1 . ∙	, /八 -c-r τ 1 FC	, 1 1' 11	∙	, ∙ , ∙	∙ , 1	1
We denote by β* an optimal solution to (4). We also define the following quantities in the analysis
of the subspace detection property, which correspond to Mi, σγ,r, σχ,r and μr used in the analysis
on the original data:
Mi，min{d(yi, H)： H ∈Hyi,dk },	⑸
where HyW 康 is all the external subspaces of y with dimension no greater than dk in the transformed
space by P.
— Δ	♦	∕∙χr ∖
σγr ,	min 一	二皿皿(丫6),
,	e：kek0=r,rank(Y β)=kβk0
Λ .	，二、	.	,,„	、
σχ,r，mιn{σmin(X,β): 1 ≤ ∣∣βko ≤ r},
Z A	δ
μr , Ξ^7^" 三 ?.
min1≤r0<r QY r - δ
(6)
(7)
(8)
Theorem 3.1. (Subspace detection property holds for NOiSy-DR-'0-SSC under deterministic
model) Let nonzero vector β* be an optimal solution to the noisy '0-SSC problem (3) for point
Xi with ∣∣β* ko = r*, nk ≥ dk + 1 for every 1 ≤ k ≤ K, and there exists 1 < r0 ≤ d such that
1 < r* ≤ ro. Suppose Y is in general position, δ < mm1≤r<r0 σγ,r, and Mi,δ，Mi - δ. Suppose
the following conditions hold:
(i)
Cp,po + 2δ Jdmax < min σ(k),	(9)
k=1,...,K
where dmaX，maxk dk, σYk，min{σmin(A): A ⊆ Y(k), A ∈ Rd×n0, n0 ≤ dk}.
(ii)	δ(1 + 2√r0) < minι≤r<ro Qγ,r — g,p0,
(iii)	mi∖≤r≤dk GYr > Cp,p° - 2δyzdk and
Mi - Cp,P0 (1 +-----------------------------尸) > δ +--------27F--, (IO)
mi∖≤r≤dk QY,r - Cp,Po - 2δ√dk	σχ,ro - cp,po
for all yi ∈ Sk and 1 ≤ k ≤ K .
5
Under review as a conference paper at ICLR 2022
(iv) min1≤r<r0 行丫)。> Cp,p0 - 2δ√r0 - δ and
δ	2δ		
				=--< 1			.	(11)
minι≤r<ro σγ,ro - C	p,po - 2δ√r0 - δ	σX,ro - C	p,p0	
If
λo <λ < 1,	(12)
1
where λo = max{max{λι,入2, *} and
ʌl = inf {0 < ʌ < 1 : \!1 - ʌ +------< < Mi,δ },	(13)
σχ,ro C
2δ 1
尢=inf {0 < λ < 1: λ-------------> > μro},	(14)
σX,r0 √λ
then With probability at least 1 - 6e p, the subspace detection property holds for Xi With β*. Here
Mi, μr and σχ 勾 are defined in (63), 66) and (65) respectively.
3.3 The Algorithm of Noisy-DR-'0-SSC
Algorithm 2 describes how to perform NoiSy-DR-'0-SSC for data clustering. Note that Noisy-DR-
'0-SSC performs noisy '0-SSC on the dimensionality reduced data XX. Proximal Gradient Descent
(PGD) is employed to optimize the objective function of noisy '0-SSC for every data point Xi, which
is described in Algorithm 1. in the t-th iteration of PGD for problem (3), the variable β is updated
according to
β(t+1) = T√2λs(β(t) - sVg(β(t))),	(15)
where sis a positive step size, g(β) = kXi - Xβk22, Tθ is an element-wise hard thresholding
operator:
[Tθ(u)]j =	0 : |uj| ≤ θ	,	1 ≤ j ≤ n.
θ j	uj : otherwise
it is proved in (Yang et al., 2017) that the sequence {β(t)} generated by PGD converges to a critical
point of (3).
Algorithm 1 Proximal Gradient Descent (PGD) for noisy '0-SSC problem (3)
1:	input: the initialization β(0), step size s > 0, parameter λ, maximum iteration number T, stopping thresh-
old ε
2:	for 1 ≤ t ≤ T do
3:	β(t) =β(t-1) - sVg(β(t-1))
4:	β(t) = T√2λs(β(t)), β(t) = 0
5:	if ∣L(β(t)) - L(β(t-1))∣ < ε then
6:	break
7:	end if
8:	end for
9:	output: β which is the suboptimal solution to (3)
Algorithm 2 Noisy Dimensionality Reduced '0-Sparse Subspace Clustering (NOiSy-DR-'0-SSC)
1:	Generate a Gaussian random matrix T ∈ Rn×p where each element Tij is sampled independently accord-
ing to the standard Gaussian distribution N (0, 1)
2:	Perform QR decomposition on XT, XT = QR where Q ∈ Rd×p
3:	Set the linear transformation P = Q>, and obtain the dimensionality reduced data XX = PX
4:	Perform noisy '0-SSC on X using Algorithm 1
We further study the case when the linear transformation P for NOiSy-DR-'0-SSC is a sparse matrix
constructed by oblivious Sparse Norm-Approximating Projection (oSNAP) (Nelson & Nguyen,
2013), so that the efficiency is further improved. The resultant algorithm is termed Noisy-DR-'0-
SSC-oSNAP whose theoretical property is demonstrated in the next subsection.
6
Under review as a conference paper at ICLR 2022
3.4 NOISY-DR-'0-SSC-OSNAP
In this subsection, We study Noisy-DR-'0-SSC-OSNAP, where the linear transformation P for
Noisy-DR-'0-SSC is replaced by a sparse matrix according to the Oblivious Sparse Norm-
Approximating Projection (OSNAP) (Nelson & Nguyen, 2013). We choose P as one the most
efficient version of OSNAP, where each column of P only has 1 nonzero element. Each column of
P is chosen to have exactly one nonzero element in a random location, and each nonzero element
is ±1 uniformly at random. Such P enjoys '2-norm preserving property stated as follows. Let ε
be a positive number such that 0 < ε ≤ 1. By Theorem 3 in (Nelson & Nguyen, 2013), for any
subspace S of dimension d in Rn, Pr(1 - ε)kvk2 ≤ kPvk2 ≤ (1 + ε)kvk2 ≥ 1 - δ for all v ∈ S
if n ≥ —d2+d—
U P ≥ δ(2ε-ε2)2 .
The algorithmic details of Noisy-DR-'0-SSC-OSNAP are the same as those of Noisy-DR-'0-SSC
expect for P. Using the '2-norm preserving property of OSNAP, we have the following theorem
about the correctness of Noisy-DR-'0-SSC-OSNAP where each column of P only has 1 nonzero
element.
Theorem 3.2. (Subspace detection property holds for Noisy-DR-'0-SSC-OSNAP under determin-
istic model with P being the OSNAP) Let nonzero vector β* be the optimal solution to the noisy
'0-SSC problem (3) for point Xi with ∣∣β*∣∣o = r*, n ≥ dk + 1 for every 1 ≤ k ≤ K, and there
exists 1 < ro ≤ d such that 1 < r* ≤ r0. Suppose Y is in general position, y ∈ Sk for some
1 ≤ k ≤ K, δ < min1≤r≤r0 σγ,r. Let Mi,δ，Mi - δ, dmax，maxk dk, ε be a positive number
such that 0 < ε ≤ 1. Suppose
2(	2 2(1+ ε)3δ
Mi,δ > --------,
σX,r0
and
„	,	δ	2(1+ ε)2δ
μr ε	< 1	.
,mmTfr - δ	σχ,ro
(1+ε)2
Then if
r r .
λ0 < λ < 1,
where λo，max{λ1,λ2, -1} and
r 2	7	2δ
λι，inf{0 < λ < 1: y 1 + ε — Λ +---pp= < Mi,δ},
2δ 1
λ2，inf{0 < λ < 1: λ--------> > μro },
σX,r0 C
(16)
(17)
(18)
(19)
(20)
〜
then with probability at least 1 - Kδ, the subspace detection property holds for Xi with β*. Here
μro and σχ 功 are defined in (66) and (65) respectively. β* is the optimal solution to (4) with P
being the OSNAP described in the beginning of this subsection with p ≥
dmax + dmax
δ(2ε-ε2)2 .
3.5	TIME COMPLEXITY OF NOISY '0-SSC, NOISY-DR-'0-SSC, NOISY-DR-'0-SSC-OSNAP
The time complexity of running PGD by Algorithm 1 for noisy '0-SSC is O(T nd), where T is the
maximum iteration number. The time complexity of running Algorithm 2 for Noisy-DR-'0-SSC is
comprised of two parts. The first part is the time complexity of steps 1-3 with matrix multiplication
and QR decomposition, which is O(dp2 + pdn). The second part is the time complexity of step 4,
which is O(Tnp). The overall time complexity of Noisy-DR-'0-SSC is O(dp2 + pdn + T np). In
practice, p is much smaller than min {d, n, T }, so Noisy-DR-'0-SSC is more efficient than noisy
'0-SSC. Noisy-DR-'0-SSC-OSNAP is even more efficient than both noisy '0-SSC and Noisy-DR-
'0-SSC, whose time complexity is O(pdn + T np). This is because the linear transformation P
obtained by OSNAP does require QR decomposition.
7
Under review as a conference paper at ICLR 2022
3.6	Bound FOR SUBOPTIMAL and Globally Optimal Solutions FOR Noisy '0-SSC
AND NOISY-DR-'0-SSC
While our theoretical analysis for noisy '0-SSC, Noisy-DR-'0-SSC and Noisy-DR-'0-SSC is based
on optimal solution to the '0 regularized problem (3), in Section C of the appendix we prove that the
bound for the suboptimal solution β obtained by Algorithm 1 is in fact close to an optimal solution
to (3), justifying the theoretical findings of this paper.
Table 1: Clustering results on various data sets, with the best two results in bold
Data Set	Measure	KM	SC	Noisy SSC	Noisy DR-SSC	SMCE	SSC-OMP	Noisy '0-SSC	NOiSy-DR-'0-SSC
COIL-20	AC	0.6554	0.4278	07854	07764	0.7549	-03389	0.8472 ± 0.0031	0.8479 ± 0.0023
	-nmi	0.7630	0.6217	0:9148	0.9219	0.8754	-0.4853	0.9428 ± 0.0082	0.9433 ± 0.0063
COIL-100	AC	0.4996	0.2835	05275	030Γ3	0.5639	-01667	0.7683 ± 0.0020	0.7039 ± 0.0087
	-nmi	0.7539	0.5923	0.8041	0.8019	0.8064	-03757	0.9182 ± 0.0096	0.8706 ± 0.0109
Yale-B	AC	0.0954	0.1077	07850	07255	0.3293	-07789	0.8480 ± 0.0091	0.8231 ± 0.0173
	-nmi	0.1258	0.1485	0:7760	073T1	0.3812	-0.7024	0.8612 ± 0.0072	0.8533 ± 0.0294
MPIESI	AC	0.1164	0.1285	05892	03588	0.1721	-0.1695	0.6741± 0.0413	-0.6741± 0.0938-
	-nmi	0.5049	0.5292	0.7653	0.6806	0.5514	-0.3395	0.8622± 0.0533	-0.8622± 0.0834
MPIES 2	AC	0.1315	0.1410	0:6994	0.4611	0.1898	-02093	0.7527± 0.0115	-0.7527± 0.0596-
	-nmi	0.4834	0.5128	0:8149	0.7086	0.5293	-0.4292	0.8939± 0.0389	0.7527 ± 0.0742
MPIES 3	AC	0.1291	0.1459	0:6316	0.4841	0.1856	-01787	0.7050± 0.0277	-0.7050± 0.0812
	-nmi	0.4811	0.5185	0.7858	0.7340	0.5155	-0.3415	0.8750± 0.0157	-0.8750± 0.0693-
MPIES4	AC	0.1308	0.1463	0.6803	055∏	0.1823	-0.1680	0.7246± 0.0147	-0.7246± 0.0605-
	-NmI	0.4866	0.5280	0.8063	0.7955	0.5294	-0.3345	0.8837± 0.0212	-0.8837± 0.0781-
MNIST	AC	0.5236	0.3504	037Γ4	05123	0.6542	-05561	0.6259 ± 0.0249	-0.6296 ± 0.152-
	NMI	0.4770	0.3607	0.6091 一	0.5026	0.6796	0.5986	0.6501 ± 0.0196	0.6440 ± 0.0259
4	Experimental Results
4.1	PERFORMANCE OF NOiSY-DR-'0-SSC AND NOiSY-DR-'0-SSC-OSNAP
We first demonstrate the performance of Noisy-DR-'0-SSC, with comparison to other competing
clustering methods including K-Means (KM), Spectral Clustering (SC), noisy SSC, Sparse Manifold
Clustering and Embedding (SMCE) (Elhamifar & Vidal, 2011) and SSC-OMP (Dyer et al., 2013)
in Table 1. With the coefficient matrix Z obtained by the optimization of noisy '0-SSC or Noisy-
DR-'0-SSC, a sparse similarity matrix is built by W = |Z|：Z |, and then spectral clustering is
performed on W to obtain the clustering results. Two measures are used to evaluate the performance
of different clustering methods, i.e. the Accuracy (AC) and the Normalized Mutual information
(NMi) (Zheng et al., 2004). The introduction to the datasets is in Section D of the appendix.
We use randomized rank-p decomposition of the data matrix in Noisy-DR-'0-SSC with p =
min{d,n}. It can be observed that noisy '0-SSC and NoiSy-DR-'0-SSC always achieve better per-
formance than other methods in Table 1, including the noisy SSC on dimensionality reduced data
(Noisy DR-SSC) (Wang et al., 2015). Note that noisy '0-SSC has the same performance as '0-SSC
(Yang et al., 2016). For all the methods that involve random projection, we conduct the experiments
for 30 times and report the average performance. Note that the cluster accuracy of SSC-OMP on the
extended Yale-B data set (Yale-B) is reported according to (You et al., 2016). We randomly sample
1000 images from each class of the MNiST data set so as to collect a total number of 10000 images
on which clustering is performed, and the average performance of 10 random sampling is reported
for this data set.
We present more results of Noisy-DR-'0-SSC and Noisy-DR-'0-SSC-OSNAP in Table 2 with dif-
ferent values of p. it can be observed that both Noisy-DR-'0-SSC and Noisy-DR-'0-SSC-OSNAP
always render comparable or even better results on highly compressed data, when compared to noisy
'0-SSC. The actual running time of both algorithms confirms the time complexity analysis in Sec-
tion 3.5, and we observe that Noisy-DR-'0-SSC and Noisy-DR-'0-SSC-OSNAP are 8.7 times and
9.6 times faster than noisy '0-SSC with the same number of iterations T on the Extended Yale-B
(Yale-B) data set. Figure 3 in Section D of the appendix illustrates how the accuracy and NMi vary
with respect to λ on Yale-B.
8
Under review as a conference paper at ICLR 2022
Table 2: Clustering results on various data sets, with different values of p for the linear transformation P and
the best two results in bold
Data Set	Measure	Noisy '0-SSC	NOiSy-DR-'0-SSC			NOiSy-DR-'0-SSC-OSNAP		
P			p = min{d, n}/5	P = min{d,n}∕10	P = min{d, n}∕15	P = min{d, n}∕5	p = min{d,n}∕10	P = min{d, n}∕15
COIL-20	-AC-	0.8472	0.8479	0.8479	0.8479	0.8486	0.8472	0.8472
	-NMI-	0.9428	0.9433	0.9433	0.9433	0.9439	0.9428	0.9428
COIL-100	-AC-	07683	0.6992	0.7276	0.7043	0.5404	0.7046	0.7233
	-NMI-	09182	0.8626	0.8919	0.8636	0.7819	0.8708	08726
Yale-B	-AC-	0.8480	0.8219	0.8231	0.8289	0.8500	0.8318	0.8277
	NMI	0.8612 —	0.8519	0.8527	0.8534	0.8538	0.8593	0.8594
4.2 JUSTIFICATION OF THEORETICAL ANALYSIS FOR NOISY '0-SSC
We further demonstrates the practical implication of our theoretical Analysis for Noisy-DR-'0-SSC,
Noisy-DR-I0-SSC-OSNAP, and noisy '0-SSC. The conditions in terms of λ, (12) ofTheorem 3.1 for
NoiSy-DR-'0-SSC,(18) of Theorem 3.2 for Noisy-DR-I0-SSC-OSNAP, and (31) of Theorem A.4
for noisy '0-SSC together with Remark A.6, unanimously indicate that a relatively large λ preserves
the subspace detection property with a high probability. This theoretical finding is consistent with
the empirical study in the following experiment. We add Gaussian noise of zero mean and different
variance σ2 to the extended Yale-B data set. Figure 1(a) to Figure 1(c) illustrate SDP violation with
respect to λ for different noise levels with σ2 ranging over 10, 30, 60. The SDP violation is defined
in (Wang & Xu, 2013) which is the percentage of pairs of data points which are mistakenly put in
the same subspace by the similarity matrix W, namely the percentage of pairs (xi, xj) with nonzero
Wij while they are in fact not in the same subspace. It can be observed that increasing λ effectively
reduces SDP violation for noisy '0-SSC, Noisy-DR-'0-SSC and NOiSy-DR-'0-OSNAP, confirming
our theoretical prediction. Section D constains more results about SDP violation rates with respect
to λ.
(a) σ2 = 10	(b) σ2 = 30	(c) σ2 = 60
Figure 1: The SDP violation rate with respect to λ for noisy '0-SSC, Noisy-DR-'0-SSC and Noisy-
DR-'0-SSC-OSNAP with different noise levels σ2. The SDP violation rates of Noisy-DR-'0-SSC
and that for Noisy-DR-'0-SSC-OSNAP are almost the same, so their curves largely overlap each
other.
5	Conclusion
In this paper, We first prove that noisy '0-SSC recovers subspaces from noisy data through '0-
induced sparsity. In order to improve the efficiency of noisy '0-SSC, We propose Noisy-DR-'0-SSC
and Noisy-DR-'0-SSC which perform noisy '0-SSC on dimensionality reduced data and they still
provably recover the subspaces in the original data. Experiment results demonstrate the effectiveness
ofNoisy-DR-'0-SSC and Noisy-DR-'0-SSC-OSNAP.
References
G. Aubrun and S.J. Szarek. Alice and Bob Meet Banach: The Interface of Asymptotic Geometric
Analysis and Quantum Information Theory. Mathematical Surveys and Monographs. American
Mathematical Society, 2017.
9
Under review as a conference paper at ICLR 2022
Yan Mei Chen, Xiao Shan Chen, and Wen Li. On perturbation bounds for orthogonal projections.
NumericalAlgorithms, 73(2):433-444, Oct 2016.
P. Drineas, A. Frieze, R. Kannan, S. Vempala, and V. Vinay. Clustering large graphs via the singular
value decomposition. Machine Learning, 56(1):9-33, 2004.
Petros Drineas, Ravi Kannan, and Michael W. Mahoney. Fast monte carlo algorithms for matrices ii:
Computing a low-rank approximation to a matrix. SIAM Journal on Computing, 36(1):158-183,
2006.
Petros Drineas, Michael W. Mahoney, and S. Muthukrishnan. Relative-error $cur$ matrix decom-
positions. SIAM Journal on Matrix Analysis and Applications, 30(2):844-881, 2008.
Petros Drineas, Michael W. Mahoney, S. Muthukrishnan, and Tamas Sarlos. Faster least squares
approximation. Numerische Mathematik, 117(2):219-249, 2011.
Eva L. Dyer, Aswin C. Sankaranarayanan, and Richard G. Baraniuk. Greedy feature selection for
subspace clustering. Journal of Machine Learning Research, 14:2487-2517, 2013.
Ehsan Elhamifar and Rene Vidal. Sparse manifold clustering and embedding. In NIPS, pp. 55-63,
2011.
Ehsan Elhamifar and Rene Vidal. Sparse subspace clustering: Algorithm, theory, and applications.
IEEE Trans. Pattern Anal. Mach. Intell., 35(11):2765-2781, 2013.
Alan Frieze, Ravi Kannan, and Santosh Vempala. Fast monte-carlo algorithms for finding low-rank
approximations. J. ACM, 51(6):1025-1041, November 2004.
Ralph Gross, Iain A. Matthews, Jeffrey F. Cohn, Takeo Kanade, and Simon Baker. Multi-pie. Image
Vis. Comput., 28(5):807-813, 2010.
N. Halko, P. G. Martinsson, and J. A. Tropp. Finding structure with randomness: Probabilistic
algorithms for constructing approximate matrix decompositions. SIAM Rev., 53(2):217-288, May
2011. ISSN 0036-1445.
Guangcan Liu, Zhouchen Lin, and Yong Yu. Robust subspace segmentation by low-rank representa-
tion. In Proceedings of the 27th International Conference on Machine Learning (ICML-10), June
21-24, 2010, Haifa, Israel, pp. 663-670, 2010.
Guangcan Liu, Zhouchen Lin, Shuicheng Yan, Ju Sun, Yong Yu, and Yi Ma. Robust recovery of
subspace structures by low-rank representation. IEEE Trans. Pattern Anal. Mach. Intell., 35(1):
171-184, January 2013.
Yichao Lu, Paramveer S. Dhillon, Dean Foster, and Lyle Ungar. Faster ridge regression via the sub-
sampled randomized hadamard transform. In Proceedings of the 26th International Conference on
Neural Information Processing Systems, NIPS’13, pp. 369-377, USA, 2013. Curran Associates
Inc.
Michael W. Mahoney and Petros Drineas. Cur matrix decompositions for improved data analysis.
Proceedings of the National Academy of Sciences, 106(3):697-702, 2009.
Jelani Nelson and Huy L. Nguyen. OSNAP: faster numerical linear algebra algorithms via sparser
subspace embeddings. In 54th Annual IEEE Symposium on Foundations of Computer Science,
FOCS 2013, 26-29 October, 2013, Berkeley, CA, USA, pp. 117-126, 2013.
Andrew Y. Ng, Michael I. Jordan, and Yair Weiss. On spectral clustering: Analysis and an algorithm.
In NIPS, pp. 849-856, 2001.
Dohyung Park, Constantine Caramanis, and Sujay Sanghavi. Greedy subspace clustering. In Ad-
vances in Neural Information Processing Systems 27: Annual Conference on Neural Information
Processing Systems 2014, December 8-13 2014, Montreal, Quebec, Canada, pp. 2753-2761,
2014.
10
Under review as a conference paper at ICLR 2022
Xi Peng, Shijie Xiao, Jiashi Feng, Wei-Yun Yau, and Zhang Yi. Deep subspace clustering with
sparsity prior. In Proceedings of the 25 International Joint Conference on Artificial Intelligence,
pp.1925-1931, New York, NY, USA, 9-15 July 2016.
T. Sarlos. Improved approximation algorithms for large matrices via random projections. In 2006
47th Annual IEEE Symposium on Foundations of Computer Science (FOCS’06), pp. 143-152,
Oct 2006.
Mahdi Soltanolkotabi and Emmanuel J. Candes. A geometric analysis of subspace clustering with
outliers. Ann. Statist., 40(4):2195-2238, 08 2012.
G.	W. Stewart. On the perturbation of pseudo-inverses, projections and linear least squares problems.
SIAM Review, 19(4):634-662, 1977. ISSN 00361445.
R. Vidal. Subspace clustering. Signal Processing Magazine, IEEE, 28(2):52-68, March 2011.
Yining Wang, Yu-Xiang Wang, and Aarti Singh. A deterministic analysis of noisy sparse subspace
clustering for dimensionality-reduced data. In Proceedings of the 32Nd International Confer-
ence on International Conference on Machine Learning - Volume 37, ICML’15, pp. 1422-1431.
JMLR.org, 2015.
Yu-Xiang Wang and Huan Xu. Noisy sparse subspace clustering. In Proceedings of the 30th Inter-
national Conference on Machine Learning, ICML 2013, Atlanta, GA, USA, 16-21 June 2013, pp.
89-97, 2013.
Yu-Xiang Wang, Huan Xu, and Chenlei Leng. Provable subspace clustering: When lrr meets ssc. In
C.J.C. Burges, L. Bottou, M. Welling, Z. Ghahramani, and K.Q. Weinberger (eds.), Advances in
Neural Information Processing Systems 26, pp. 64-72. Curran Associates, Inc., 2013.
H.	Weyl. Das asymptotische verteilungsgesetz der eigenwerte linearer partieller differentialgle-
ichungen (mit einer anwendung auf die theorie der hohlraumstrahlung). Mathematische Annalen,
71:441-479, 1912.
Y. Yang, J. Feng, N. Jojic, J. Yang, and T. S. Huang. On the Suboptimality of Proximal Gradient
Descent for `0 Sparse Approximation. ArXiv e-prints, 2017.
Yingzhen Yang and Jiahui Yu. Fast proximal gradient descent for A class of non-convex and non-
smooth sparse learning problems. In Proceedings of the Thirty-Fifth Conference on Uncertainty
in Artificial Intelligence, UAI 2019, Tel Aviv, Israel, July 22-25, 2019, pp. 508, 2019.
Yingzhen Yang, Jiashi Feng, Nebojsa Jojic, Jianchao Yang, and Thomas S. Huang. L0-sparse sub-
space clustering. In Computer Vision - ECCV 2016 - 14th European Conference, Amsterdam, The
Netherlands, October 11-14, 2016, Proceedings, Part II, pp. 731-747, 2016.
Pavel Yaskov. Lower bounds on the smallest eigenvalue of a sample covariance matrix. Electron.
Commun. Probab., 19:10 pp., 2014.
Pavel Yaskov. Sharp lower bounds on the least singular value of a random matrix without the fourth
moment condition. Electron. Commun. Probab., 20:9 pp., 2015. doi: 10.1214/ECP.v20-4089.
Yu-Xiang Wang Yining Wang and Aarti Singh. Parameter estimation of generalized linear models
without assuming their link function. In Proceedings of the Eighteenth International Conference
on Artificial Intelligence and Statistics, AISTATS 2016, Cadiz, Spain, May 9-11, 2016, 2016.
Chong You, Daniel P. Robinson, and Rene Vidal. Scalable sparse subspace clustering by orthogonal
matching pursuit. In 2016 IEEE Conference on Computer Vision and Pattern Recognition, CVPR
2016, Las Vegas, NV, USA, June 27-30, 2016, pp. 3918-3927, 2016.
Xin Zheng, Deng Cai, Xiaofei He, Wei-Ying Ma, and Xueyin Lin. Locality preserving clustering for
image database. In Proceedings of the 12th Annual ACM International Conference on Multimedia,
MULTIMEDIA ’04, pp. 885-891, New York, NY, USA, 2004. ACM.
11
Under review as a conference paper at ICLR 2022
A	Theoretical Analysis FOR Noisy '0-SSC
The theoretical results on the subspace detection property for noisy `0 -SSC are presented in this
section under deterministic model and randomized models. These models are introduced in the
following subsection.
A.1 Models
Similar to (Soltanolkotabi & Candes, 2012), We introduce the deterministic, semi-random and fully-
random models for the analysis of noisy '0-SSC.
•	Deterministic Model: the subspaces and the data in each subspace are fixed.
•	Semi-Random Model: the subspaces are fixed but the data are independent and identically dis-
tributed in each of the subspaces.
•	Fully-Random Model: both the subspaces and the data of each subspace are independent and
identically distributed.
The data in the above definitions refer to clean data Without noise. We refer to semi-random
model and fully-random model as randomized models in this paper. All the three models are ex-
tensively employed to analyze the subspace detection property in the subspace learning literature
(Soltanolkotabi & Candes, 2012; Wang et al., 2013; Wang & Xu, 2013; Yining Wang & Singh,
2016).
Figure 2: illustration of an external subspace. All the data Y are normalized to have unit norm for
illustration purpose, so they lie on the surface of the sphere. S1 and S2 are tWo subspaces in the
three-dimensional ambient space. The subspace spanned by yi ∈ S1 and yj ∈ S2 is an external
subspace, and the intersection of this external subspace and S1 is a dashed line yi OA.
A.2 Noisy '0-SSC: Deterministic Analysis
We introduce the definition of general position and external subspace before our analysis on noisy
'0-SSC.
Definition A.1. (General position) For any 1 ≤ k ≤ K, the data Y(k) are in general position if any
subset of L ≤ dk data points (columns) of Y(k) are linearly independent. Y are in general position
if Y(k) are in general position for 1 ≤ k ≤ K.
The assumption of general condition is rather mild. in fact, if the data points in X(k) are inde-
pendently distributed according to any continuous distribution, then they almost surely in general
position.
Let the distance betWeen a point x ∈ Rd and a subspace S ⊆ Rd be defined as d(x, S) =
inf y∈S kx - yk2, the definition of external subspaces is presented as folloWs. Figure 2 illustrates an
example of external subspace.
Definition A.2. (External subspace) For a point y ∈ Y(k), a subspace H{yi }L spanned by
a set of linear independent points {yij }jL=1 ⊆ Y is defined to be an external subspace of y if
12
Under review as a conference paper at ICLR 2022
{yij }jL=1 6⊆ Y(k) and y ∈/ {yij }jL=1. The point y is said to be away from its external subspaces if
minH∈Hy,d d(y, H) > 0, where Hy,d are the set of all external subspaces of y of dimension no
greater than dfory, i.e. Hy,d = {H: H = H{yi }L , dim[H] = L, L ≤ d, {yij}jL=1 6⊆ Y(k), y ∈/
{yij}jL=1}. All the data points in Y(k) are said to be away from the external subspaces if each of
them is away from the its associated external spaces.
Remark A.1. (Subspace detection property holds for noiseless '0-SSC under the deterministic
model) It can be verified that the following statement is true. Under the deterministic model, sup-
pose data is noiseless, nk ≥ dk + 1, Y(k) is in general position. If all the data points in Y(k) are
away from the external subspaces for any 1 ≤ k ≤ K, then the subspace detection property for
'0-SSC holds with an optimal solution Z* to (1).
To present our theoretical results of the correctness of noisy '0-SSC, We need the definitions related
to the spectrum of X and Y, which are defined as follows. In the following analysis, we employ β
to denote the sparse code of datum xi so that a simpler notation other than Zi is dedicated to our
analysis.
Definition A.3. The minimum restricted eigenvalue of the uncorrupted data is defined as
σY,r
min	σmin(Yβ)
e：kek0=r,rank(Ye )=kβk0
(21)
for r ≥ 1. In addition, the normalized minimum restricted eigenvalue of the uncorrupted data is
defined by
σY,r
σY,r ,-『
r
(22)
Moreover, the following quantities are defined for our analysis. We define
τ0
2δ√r*
一*
σx
+ τ1 ,
where
τ1
δ
σγ - δ,
σX，σmin(Xβ*),
with δ < σγ, and σγ is defined as
* △
σY ,
ιm<r* σY,r，
(23)
(24)
(25)
Now we present the main result on the correctness of noisy '0-SSC.
Theorem A.2. (Subspace detection property holds for noisy '0-SSC) Let nonzero vector β* be
an optimal solution to the noisy '0-SsC problem (3) for point Xi with ∣∣β*∣∣o = r* > 1, and
c*，∣∣Xi - Xβ*∣∣2. Suppose Y is in general position, y ∈ Sk for some 1 ≤ k ≤ K, δ < σγ,
λ > τo, B(yi,δ + c* + 2σr*) ∩ H = 0 for any H ∈ Hyi,√fc. Then the subspace detection property
holds for Xi with β*. Here τo, τι, σγ and σχ are defined in (23), (24) and (25).
Remark A.3. When δ = 0 and there is no noise in the data X, the conditions for the correctness of
noisy '0-SSC in Theorem A.2 almost reduce to that for noiseless '0-SSC. To see this, the conditions
are reduced to B(yi, c*) ∩ H = 0, which are exactly the conditions required by noiseless '0-SSC,
namely data are away from the external subspaces by choosing λ → 0 and it follows that c* = 0.
While Theorem A.2 establishes geometric conditions under which the subspace detection property
holds for noisy '0-SSC, it can be seen that these conditions are often coupled with an optimal solu-
tion β* to the noisy '0-SSC problem (3). In the following theorem, the correctness of noisy '0-SSC
is guaranteed in terms of λ, the weight for the `0 regularization term in (3), and the geometric
conditions independent of an optimal solution to (3).
Let Mi > 0 be the minimum distance between yi ∈ Sk and its external subspaces when yi is away
from its external subspaces, i.e.
Mi , min{d(yi, H): H ∈ Hyi,dk},
(26)
13
Under review as a conference paper at ICLR 2022
The following two quantities related to the spectrum of clean and noisy data, μr and σχ,r, are
defined as follows with r > 1 for the analysis in Theorem A.4.
μr , -：	∑	7,	(27)
min1≤r0<r σγ,r - δ
σX,r , min{σmin (Xβ): 1 ≤ kβk0 ≤ r}	(28)
Theorem A.4. (Subspace detection property holds for noisy '0-SSC under deterministic model,
with conditions in terms of λ) Let nonzero vector β* be an optimal solution to the noisy '0-SSC
problem (3) for point Xi with ∣∣β* ko = r*, n ≥ dk + 1 for every 1 ≤ k ≤ K, and there exists
1 < ro ≤ d such that 1 < r* ≤ rQ. Suppose Y is in general position, Ni ∈ Sk for some 1 ≤ k ≤ K,
δ < min1≤r<r0 σγ,r, and Mi,δ，Mi - δ. Suppose
Mi,δ > ~δ^,	(29)
σX,r0
and
2δ
μro < 1--------.	(30)
σX,r0
Then if
λ0 < λ < 1,	(31)
where λ0 , max{λ1, λ2} and
一 八一 一	.——一	2δ
λι，inf{0 <λ< 1: √Γ-λ +-------------√= < Mi,δ},	(32)
2δ	1
λ2，inf{0 < λ < 1: λ-------------> > μro},	(33)
σX,r0	λ
the subspace detection property holds for Xi with β*. Here Mi, μr° and σχ,r° are defined in (26),
(27) and (28) respectively.
Remark A.5. The two conditions (29) and (30) are induced by two conditions, B(yi,δ + c* +
2δσ^rτ) ∩ H = 0 for any H ∈ Hyi,dfc and λ > τ0 respectively, which are required by Theorem A.2.
Note that when (29) and (30) hold, λ1 and λ2 can always be chosen in accordance with (32) and
(33).
Remark A.6. It can be observed from condition (31) that noisy '0-SSC encourages sparse solution
by a relatively large λ so as to guarantee the subspace detection property. This theoretical finding is
consistent with the empirical study shown in the experimental results.
A.3 Noisy '0-SSC: Randomized Analysis
In this subsection, the correctness of noisy '0-SSC is analyzed when the clean data in each subspace
are distributed at random. We assume that the data in subspace S(k) are i.i.d. according to uniform
distribution on the unit sphere centered at the origin, for 1 ≤ k ≤ K . This setting is employed
extesively in the subspace learning literature (Soltanolkotabi & Candes, 2012; Wang et al., 2013;
Wang & Xu, 2013; Yining Wang & Singh, 2016).
We then have the following results regarding to the subspace detection property of noisy '0-SSC
under randomized models.
Theorem A.7. (Subspace detection property holds for noisy '0-SSC under randomized models,
with conditions in terms of λ) Under randomized models, let nonzero vector β* be an optimal
solution to the noisy '0-SSC problem (3) for point Xi with ∣∣β*∣o = r*, n ≥ dk + 1 for every
1 ≤ k ≤ K, and there exists 1 < ro ≤ d such that 1 < r* ≤ rQ. Suppose the data in each subspace
are i.i.d. isotropic samples according to some continuous distribution that satisfies condition (a).
Let dmaχ，max® dk, σ0nm，√d^(1 - 14√4(*)2/3), C，/
such that d1-2t√dmaχ — 1 -12 > 0, suppose
dmax
σ0¾n-r0ff (St1,St2 )
σ02i
max aff (Stι, St2) < ------------,
t1 6=t2	t1 t2	r0
r0
. For t > 0
(34)
14
Under review as a conference paper at ICLR 2022
δ < c,
and
δ	2δ
+ √0(c- δ)
≤ ~,----2t√dmaχ — 1 — t,
dmax
δ	2δ
----+ ----	「----T
C — δ	√r0(c — δ)
< 1,
λ0< λ < 1,
where M，sup0≤t<ι 2t3 a∏ccost < 1, λ0，max{λ1,λ2} and
λ1，inf {0 < λ < 1: √1 — λ +----------< < -------2t√dmaχ — 1 — t2 — δ},
√T0(C — δ) √λ	dmaχ
λ02 , inf{0 < λ < 1: λ —
2δ	ɪ 一}
√r0(c — δ) √λ C — δ
(35)
(36)
(37)
(38)
(39)
(40)
K2
Then With probability at least 1 - K exp(-d) - 8 P n exp(-dkt-), the subspace detection ProP-
k=1
erty holds for Xi with β*.
Remark A.8. Note that there is no assumption on the distribution of subspaces in Theorem A.7, so
it is not required that the subspaces should have uniform distribution, an is required in the geometric
analysis of '1-SSC (SoltanoIkotabi & Candes, 2012) and its noisy version (Wang & Xu, 2013).
B Proofs
We provide proofs to the lemmas and theorems in the paper in this subsection.
B.1	Proof of Remark A.1
Lemma A. (Subspace detection property holds for '0-SSC under the deterministic model) Under
the deterministic model, suppose data is noiseless, nk ≥ dk + 1, Y(k) is in general position. If all
the data points in Y(k) are away from the external subspaces for any 1 ≤ k ≤ K, then the subspace
detection property for '0-SSC holds with an optimal solution Z* to (1).
Proof. Let Xi ∈ Sk. Note that Z*i is an optimal solution to the following '0 sparse representation
problem
minkZik0 s.t. Xi = [X(k) \ Xi X(-k)]Zi, Zii =0,	(41)
Zi
where X(-k) denotes the data that lie in all subspaces except Sk. Let Z*i
are sparse codes corresponding to X(k) \ Xi and X(-k) respectively.
α
β
where α and β
Suppose β = 0, then Xi belongs to a subspace S0 = HXZ*i spanned by the projected data points
corresponding to nonzero elements of Z*i, and S0 = Sk, dim[S0] ≤ dk. To see this, if S0 = Sk, then
the data corresponding to nonzero elements of β belong to Sk, which is contrary to the definition
of X(-k). Also, if dim[S 0] > dk, then any dk points in X(k) can be used to linearly represent
Xi by the condition of general position, contradicting with the optimality of Z*i. Since the data
points (or columns) in XZ* i are linearly independent, it follows that Xi lies in an external subspace
HXZ*i spanned by linearly independent points in XZ*i, and dim[HX *i] = dim[S 0] ≤ dk. This
contradicts with the assumption that Xi is away from the external subspaces. Therefore, β = 0.
Perform the above analysis for all 1 ≤ i ≤ n, we can prove that the subspace detection property
holds for all 1 ≤ i ≤ n.
□
15
Under review as a conference paper at ICLR 2022
B.2	Proof of Theorem A.2
Before proving this theorem, we introduce the following perturbation bound for the distance between
a data point and the subspaces spanned by noisy and noiseless data, which is useful to establish the
conditions When the subspace detection property holds for noisy '0-SSC.
Lemma B.1. Let β ∈ Rn and Ye has full column rank. Suppose δ < σγ,r where r = kβko, then
Xβ is a full column rank matrix, and
δ
∣d(xi, HXe) - d(xi, HYe)| ≤ T------------	(42)
σγ,r - δ
for any 1 ≤ i ≤ n.
Lemma B.2 shows that an optimal solution to the noisy '0-SSC problem (3) is also that to a '0-
minimization problem with tolerance to noise.
Lemma B.2. Let nonzero vector β* be an optimal solution to the noisy '0-SSC problem (3) for
point Xi with ∣∣β*ko = r* > 1. If λ > τ0 where τ° is defined as
τ0
△ 2δ√r* ι
,一*-----+ τ1,
σX*
where
τ1
δ
σγ - δ,
σX , σmin(Xβ*),
with δ < σγ, and σγ is defined as
*
σY ,
ι≤r味〜，
then β* is an optimal solution to the following sparse approximation problem with the uncorrupted
data as the dictionary:
mβin ∣β ∣0
s.t. ∣Xi - Yβ∣2 ≤ c* +
2δ√r*
*
σX*
βi = 0.
(43)
where c* , ∣Xi - Xβ* ∣2.
Define B(Xi, c0) = {X: ∣X - Xi∣ ≤ c0} be the ball centered at Xi with radius c0. If B(Xi, c0) is
away from the corresponding confusion area, i.e. all the external subspaces in Hyi,dk, then subspace
detection property holds with the solution to a proper sparse approximation problem where Xi is
approximated by the uncorrupted data, as shown in the following Lemma.
Lemma B.3. Suppose Y is in general position and yi ∈ Sk for some 1 ≤ k ≤ K. For positive
number c° such that c° ≥ d(xi, Sk), suppose B(xi, c0) ∩ H = 0 for any H ∈ Hyi,dk. Then the
subspace detection property holds for Xi with an optimal solution to the following sparse approxi-
mation problem, denoted by β*, i.e. nonzero elements ofβ* correspond to the columns of X from
the same subspace as yi .
min ∣β∣0 s.t. ∣Xi - Yβ∣2 ≤ c0, βi = 0.	(44)
Now we are ready to prove Theorem A.2.
Proof of Theorem A.2. We first show that d(xi, Sk) ≤ c* + 2δ√τ. To see this, σχ =
σmin(Xβ*) ≤ 1 as the columns of X have unit '2-norm. It follows that
2δ√r*
C +----*—	≥	2δ√r*	≤	2δ	> ∣∣Xi - yi∣	≤	d(xi,	Sk)	(45)
σX*
By Lemma B.2, it can be verified that β* is an optimal solution to the following problem
min ∣β∣0 s.t. ∣Xi - Yβ∣2 ≤ c* +
2δ√r*
σX*
, βi = 0.
(46)
The subspace detection property holds which follows from applying Lemma B.3 with co
2δ√*
-σ*	.
σX
c* +
□
16
Under review as a conference paper at ICLR 2022
B.3 Proof of Lemma B.1
The following proposition is used for proving Lemma B.1.
Lemma B.4. (Perturbation of distance to subspaces) Let A, B ∈ Rm×n are two matrices and
rank(A) = r, rank(B) = s. Also, E = A - B and ∣∣Ek2 ≤ C, where ∣∣ ∙ k2 indicates the spectral
norm. Then for any point x ∈ Rm , the difference of the distance of x to the column space of A and
B, i.e. |d(x, HA) - d(x, HB)|, is bounded by
|d(x, HA)- d(x, Hb) | ≤	CRk2 -.	(47)
min{σr(A), σs(B)}
Proof. Note that the projection of x onto the subspace HA is AA+x where A+ is the Moore-
Penrose pseudo-inverse of the matrix A, so d(x, HA) equals to the distance between x and its
projection, namely d(x, HA) = ∣x - AA+x∣2. Similarly, d(x, HB) = ∣x - BB+x∣2.
It follows that
|d(x, HA)- d(x, Hb)| = ∣∣x - AA+x∣2 - ∣x - BB+x∣2∣
≤ ∣AA+x-BB+x∣2 ≤ ∣AA+ -BB+∣2∣x∣2.	(48)
According to the perturbation bound on the orthogonal projection in (Chen et al., 2016; Stewart,
1977),
∣AA+ -BB+∣2 ≤ max{∣EA+∣2,∣EB+∣2}.	(49)
Since l∣EA+12 ≤ ∣∣Ek2kA+k2 ≤ σrCA), ∣EB+∣2 ≤ ∣∣Ek2kB+k2 ≤ σsCB), combining (48) and
(49), we have
|d(x, HA) - d(x, HB)| ≤ max{
C C
σr(A), σs(B)
}∣x∣2
CkXk2
min{σr (A),σs(B)}
So that (42) is proved.
(50)
□
Proof of Lemma B.1. We have yi =Xi- n, and σmin(Y>Yβ) = (σma(Yβ))2 ≥ σY,,
ByWeyI(Weyl, 1912),旧(丫6)-σJXβ)| ≤ ∣∣Nβk2 ≤ ∣∣Nβ∣∣f ≤ √rδ. Since √rδ < σγ,r ≤
σmin(Yβ) ≤ σi(Yβ), σi(Xβ) ≥ σi(Yβ) - √rδ ≥ σγ,r - √rδ > 0 for 1 ≤ i ≤ min{d,r}. It
follows that σmin(Xβ) ≥ σγ,r - √rδ > 0 and Xe has full column rank.
Also, ∣Xβ - Ye∣∣2 ≤ ∣∣Xβ - Ye∣∣f ≤ √rδ. According to Lemma B.4,
|d(xi, HXβ ) - d(xi, HYβ )|
≤___________√rδ___________
min{σmin(Xe), σmin(Ye)}
≤	√rδ	= δ
σY,r - √rδ	σY,r - δ
(51)
B.4 Proof of Lemma B.2
Proof of Lemma B.2. We have
kxi - Xβ*k2 + λkβ*k0 ≤ kxi - X0k2 + λ∣oko = 1
⇒ C* = kxi - Xβ*k2 < 1.
□
17
Under review as a conference paper at ICLR 2022
We first prove that β* is an optimal solution to the sparse approximation problem
min kβko s.t. ∣∣Xi - Xβ∣∣2 ≤ c*, βi = 0.
(52)
To see this, suppose there is a vector β0 such that ∣∣Xi - Xβ0k2 ≤ C and kβ0∣∣0 ‹ kβ*∣∣0, then
L(β0) < c* + λ∣β*∣o = L(β*), contradicting the fact that β* is an optimal solution to (3).
Note that Xq* is a full column rank matrix, otherwise a sparser solution to (3) can be obtained as
vector whose support corresponds to the maximal linear independent set of columns of Xq* .
Also, the distance between Xi and the subspace spanned by columns of Xq* equals to c*, i.e.
d(xi, Hχβ*) = c*. To see this, it is clear that d(xi, Hχβ*) ≤ c*. If there is a vector y = Xe
in HXβ* with supp(β) ⊆ supp(β*), and ∣Xi - y∣2 < c*, then L(β) < L(β*) which contradicts
the optimality of β*. Therefore, d(Xi, HXβ* ) ≥ c*, and it follows that d(Xi, HXβ* ) = c*.
To prove that the subspace separation margin HS(Xi, X, β*) > 0, suppose HS(Xi, X, β*) ≤ 0, so
there exists β0 such that ∣β0∣0 < r*, rank(Xq0) = ∣β0∣0 and d(yi, HXβ0) ≤ d(yi, HXβ* ) ≤ c*.
Then β0 is sparser than β* and it satisfies the constraint of problem (52), contradicting the optimality
of β*.
Since ∣Xi - Xβ* ∣2 ≤ 1, ∣Xβ* ∣2 ≤ 2. Also,
σmin(Xq>*Xq*)∣β*∣22 ≤ ∣Xβ*∣22 ≤4,
it follows that ∣∣β*∣∣2 ≤ —42. By Cauchy-Schwarz inequality, ∣β*∣ι ≤ 2√r* and ∣∣Nβ*∣2 ≤
σX	X
kβ*kιδ ≤ 2δ√*. Therefore,
∣Xi - Yβ*∣2 = ∣Xi - Xβ* + Nβ*∣2
≤ kXi - Xβ*∣2 + ∣Nβ*∣2 ≤ c* + 2δ2*r*,
σX
so that β* is a feasible for problem (43). To prove that β* is also an optimal solution to (43), suppose
this is not the case, and an optimal solution to (43) is a vector β0 such that ∣ Xi - Yβ012 ≤ c* + 2δσ*r*
and ∣β0 ∣0 = r < r* . Yq0 is a full column rank matrix, otherwise a sparser solution can be obtained
as vector whose support corresponds to the maximal linear independent set of columns of Yq0 . We
have
d(Xi, HYβ0) ≤ ∣Xi - Yβ0∣2 ≤c*+
2δ√r*
σX
According to Lemma B.1, we have
∣d(xi, Hχβo)- d(xi, Hγβo)∣ ≤
√Tδ
σγ,r - √rδ
δδ
-≤ -
σγ r - δ 一 σγ - δ
⇒ d(Xi, HXβ0)
J *	2δ√r?	δ
≤ C +	σχ	+ σγ - δ
c* + τ0 .
However, according to the optimality of β* in the noisy '0-SSC problem (3), We have
d(Xi, HXβ0) - c* = d(Xi, HXβ0) - d(Xi, HXβ*)
≥ (r* - r )λ ≥ λ > τ0
This contradiction shows that β* is an optimal solution to (43).
□
18
Under review as a conference paper at ICLR 2022
B.5	Proof of Lemma B.3
Proof of Lemma B.3. (44) is equivalent to the following problem
min kβk0 s.t. y = Yβ, kxi - yk2 ≤ c0, βi = 0.	(53)
We show that the points (columns) of Y0* must Come from subspace Sk. To see this, suppose some
columns of Yq* come from different subspaces. We first have kβ*∣∣o ≤ dk. To see this, We can
choose some y0 ∈ Sk such that ky0 - xik2 ≤ c0 since c0 ≥ d(xi, Sk). Also, dk points in Y(k) can
linearly represent y0 since Y(k) is in general position, and it follows that kβ*∣∣0 ≤ dk due to the
optimality of β*.
Also, Yq* has full column rank, so that subspace Hγβ* ∈ Hyi^. Let y* = Yβ*, then y* ∈
Hγβ* ∩ B(xi, co) which contradicts the fact that B(xi, co)∩ H = 0 for any H ∈ Hyi,dk. Therefore,
columns of Yq* must come from Sk.	□
B.6	Proof of Theorem A.4
Proof of Theorem A.4. This theorem can be proved by checking that the conditions in Theorem A.2
are satisfied.	□
B.7	Proof of Theorem A.7
In order to prove this theorem, the following lemma is presented and it provides the geometric
concentration inequality for the distance between a point y ∈ Y(k) and any of its external subspaces.
It renders a lower bound for Mi , namely the minimum distance between yi ∈ Sk and its external
subspaces.
Lemma B.5. Under randomized models, given 1 ≤ k ≤ K and y ∈ Y(k), suppose H ∈ Hyi,dk is
any external subspace of y. Then for any t > 0,
Pr[d(y, H) ≥ 1 - 2tλ∕dk - 1 - t2] ≥ 1 - 8 exp(------2-).
(54)
Proof of Lemma B.5. Let H be a fixed subspace of dimension de ≤ dk, and y ∈/ H. Since y ∈ Sk
Idk
0
and y ∈/ H. Let USk
∈ Rd×dk be the orthonormal basis ofSk under which the isotropic
random vector y in Sk satisfies E[yy>]
I0dk	00
. It follows that more columns vectors can be
added to USk to form a orthonormal basis U ∈ Rd×d0 for the minimum subspace that contains Sk
and H. It can be verified that dk + 1 ≤ d0 ≤ min{dk + de, d} because H 6= Sk. Note that U can be
I0dk	U00
represented as a block matrix as U
where U0 ∈ R(d-dk )×(d0 -dk ) has orthonormal
columns. It can be verified that the basis of H can be represented as UH
Note that if de - d0 + dk = 0, UH
0
U0
Ide -d0 +dk
0
. Then PH(y) = UHU>Hy, and we have
0
U0
E[kPH(y)k22] = E[y>UHU>HUHU>Hy]
= E[Tr(y>UHU>Hy)]
= E[Tr(U>Hyy>UH)]
= Tr(U>HE[yy>]UH)
>
Tr
Ide-d0+dk	0
0	U0
Idk	0	Ide -d0 +dk	0
0	0	0	U0
de - d0 + dk ≤ de - 1 ≤ dk - 1
(55)
19
Under review as a conference paper at ICLR 2022
According to the concentration inequality in section 5.2 of (Aubrun & Szarek, 2017), for any t > 0,
Pr[|kPH(Y)I∣2 - Ppde- d0 + dk | ≥ t] ≤ 8 exp(---2-)
(56)
Now let H be spanned by data from Y, i.e. H = H{y }de , where {yij }jd=e 1 are any de linearly
independent points that does not contain y. For any fixed points {yij }jd=e 1, (56) holds. Let A be
the event that ∣Ph(y) — √de - d0 + dk | ≥ t, We aim to integrate the indicator function 1Ia with
respect to the random vectors, i.e. y and {yij }jd=e 1, to obtain the probability that A happens over
these random vectors. Let y = yi, using Fubini theorem, we have
Pr[A] =
×jn=1S(j)
HAgn=ldμ(j
/…Pr[A1{yj mj
≤ Z× …8exp(-¥ )"d"(j)=8exp(-H)
(57)
where S(j) ∈ {Sk}K=ι is the subspace that yj lies in, and μ(j) is the probabilistic measure of the
distribution in S(j). The last inequality is due to (56).
Note that for any y's external subspace H = %丫，产，d(y,H) = vz∣∣y∣∣2 -||PH(y)k2 =
d∕dk — |PH(y)k2. According to (57), we have
Pr[d(y, H) ≥ 1 ― 2tλ∕dk - 1 ― t2] ≥ 1 ― 8eχp(—2-).	(58)
□
Lemma B.6. Let Y ∈ Rd×r be a submatrix of Y(k) with rank(Y) = r and r ≤ dk. Then with
probability at least 1—exp(—d®), σmm(Yβ) ≥ √d=(1 —14√4(Mnd)2/3) = σ0n正 for any β ∈ Rn
such that Iβ I0 = r
Proof. Let Y0 = √dkY. Let U(k) be the orthogonal basis for Sk, and Y0 can be expressed by
Y0 = U((k))C, where C ∈ RdkXr and it can be verified that Ci is uniformly distributed on a
sphere centered at the origin with radius √dk, and Ci is a isotropic random vector for all i ∈ [r].
2 tɜ arccos √==
Let Mk = supo≤t<√d^----------∏-k-ji-, it can be verified that Mk satisfies the following condition:
for any t > 0, any i ∈ [n] and any vector V with unit '2-norm,
Pr[∣hCi,v)| >t] ≤ M.	(59)
According to Theorem 2.1 in (Yaskov, 2015) and condition (59), with proability at least 1 —
exp(—dk ), σmin(C) ≥ √1maχ (1 - 14 √4( Mnd )2/3) = σm in	口
Proof of Theorem A.7. According to (Yaskov, 2014) and condition (a), with probability at least
1 一 exp(-d), σmin(Yβ ≥ √196Md + 1 - 14√Md for any β ∈ Rn such that ∣∣β∣o = r ≤
d, rank(Yβ) = ∣β10. It follows that σγ,r ≥ √196Md + 1 - 14√Md. By Weyl (Weyl, 1912),
∣σmin(Xβ) — σmin(Yβ)| ≤ IlNek2 ≤ δ√r0. Therefore, σmin(Xβ) ≥ √196Md + 1 — 14√Md —
δ√r0 > 0 if δ < 36^+-142 = c. It can be verified that (36), (37) and (38) guarantee (29),
(30) and (31) in Theorem A.4 respectively, therefore, the conclusion holds.	□
20
Under review as a conference paper at ICLR 2022
B.8 Proof of Theorem 3.1
Before presenting the proof, we introduce necessary lemmas.
It is proved that the low rank approximation X is close to X in terms of the spectral norm (Halko
et al., 2011):
Lemma B.7. (Corollary 10.9 in (Halko et al., 2011)) Let p0 ≥ 2 be an integer and p0 = p -p0 ≥ 4,
then with probability at least 1 - 6e-p, the spectral norm of X - Xb is bounded by
kX - Xb k2 ≤ Cp,p0
(60)
where
Cp,po , (1 + 17J1 + p°7 )σP0+1 + 7+Γ(X X σ2) 1	(61)
p	p + j>p0
and σ1 ≥ σ2 ≥ . . . are the singular values of X.
In addition, we have the following lemma on the perturbation bound for the distance between a
data point and a subspace before and after the projection P. Each subspace Sk is transformed into
Sk = P(Sk) with dimension dk.
Tr	τr* CT , zɔ _ Trhn ~	^ι~* TT ∙	,	ι ι	t~ Slr	^γ∖∕ft^	∖ F Slr
Lemma B.8. Let β ∈ Rn, Yi = Pyi, HYeiS an external subspace of γ%, Ye = P(Ye) and Ye
has full column rank. Then
∣d(yi, HYe) - d(Yi, Hγ β )∣≤ Cp,po (1 +
1
mini≤r≤dk σY,r — CP,Po
-2δpdk)
(62)
for any 1 ≤ i ≤ n and yi ∈ Sk .
Proof of Lemma B.8. This lemma can be proved by applying Lemma B.4.	□
The following notations are defined before presenting the proof of Theorem 3.1.
MMi，min{d(Yi, H): H ∈ Hyi3],	(63)
where Hya dk is all the external subspace for y in the transformed space by P.
σYr，	mm 一	二皿皿(丫户),	(64)
'	e：kek0=r,rank(Y β)=kek0
Λ	-	,	.	..	_ ..	、
σχχ,r，min{σmin(Xe): 1 ≤ ∣∣β∣∣0 ≤ r},	(65)
μr
δ
min1≤r<ro σY,r 一 δ
(66)
Proof of Theorem 3.1. For any matrix A ∈ Rp×q, we first show that multiplying Q to the left
of A would not change its spectrum. To see this, let the singular value decomposition of A be
A = UAΣVA> where UA and VA have orthonormal columns with U>AUA = VA>VA = I.
Then QA = UQAΣVQA is the singular value decomposition of QA with UQA = QUA and
VQA = VA. This is because the columns of UQA are orthonormal since the columns Q are
orthonormal: U>QAUQA = U>A Q> QUA = I, and Σ is a diagonal matrix with nonnegative
diagonal elements. It follows that σmin(QA) = σmin(A) for any A ∈ Rp×q.
For a point Xi = Yi + ni, after projection via P, we have the projected noise ni = Pn〃 Because
l∣nik2 = l∣Pnik2 = kQ>nik2 ≤ IIQk2I∣nik2 ≤ I∣nik2 ≤ δ,	(67)
21
Under review as a conference paper at ICLR 2022
the magnitude of the noise in the projected data is also bounded by δ. Also,
l∣Xik2 = IIQ>Xik2 ≤ l∣Xik2 ≤ 1,	(68)
_	_ .. ʃ- ____ .	-	，_~	、	，q_ 、、	…
Let β ∈ Rn, Yβ = PYe With ∣∣β∣o = r. Then σmin(QY6)=σmin(Y6)).Since
I /ʌr 、	/ʌr ∖ I 1	/∕~∖x'τ^ 、	/ʌr ∖ ∖
lσmin (Ye) ― σmin(Ye ) | = Qmin(QYe ) ― σmin(Ye) |
~	..
≤ kQYe - YeII2
=lQQ>Ye-Yel2
=lQQ>Xe-Xe+Ne-QQ>Nel2
≤ Cp,p0 + ∣Ne If + kQQ>Ne If
≤ Cp,p0 + 2δ√r	(69)
Therefore, it folloWs from (69) that if
Cp,p0 +2δ Jdmax < min σ((k),	(70)
k=1,...,K
then Y is also in general position.
In addition, since λ ≥ *, we have λ∣∕3*∣o ≤ L(0) ≤ 1, and it follows that ∣∕3*∣o ≤ 1 ≤ r0.
Based on (69) We have
lσYr- σY,r | ≤ CP,P0 + 2δ√r0,	(71)
it follows that δ < minι≤r<r0 σγ r because δ < minι≤r<r0 σγ,r 一 Cpp — 2δ√r0.
Again, for β ∈ Rn with lβl0 = r ≤ r0, we have
lσmin (Xe) 一 σmin(Xe) | = Qmin(QXe) 一 σmin(Xe) |
≤ kQXe 一 Xek2
=kQQ>Xe - Xek2 = kX - Xek2
≤ Cp,p0	(72)
It can be verified that
∣σχ,r — σχ,r∣≤ Cp,p0	(73)
Combining (73) and Lemma B.8, noting that σχ,r0 一 Cp,p0, since
we have
where yi ∈ Sk.
Mi 一 Cp,p0 (1 +
>δ+
1
mini≤r≤dk σY,r 一 CP,Po
2δ
σχ,r0 一 Cp,p0
2δ
Mi,δ，Mi 一 δ > ------
σX ,ro
Based on (71) and (73), we have
M < 1 -工
σX ,ro
because
——产)
一 2δV dk
一M ? 一 < 1 -
min1≤r<ro σY,r0	Cp,po	2δy∕r0	δ	σX,ro	Cp,po
(74)
(75)
(76)
(77)
□
22
Under review as a conference paper at ICLR 2022
B.9 Proof of Theorem 3.2
Proof of Theorem 3.2. It can be verified that Mi ≥ -M. Let β ∈ Rn, Yβ = PYe with
kβko = r and rank(Ye) = r, then fOrany U ∈ Rr, ∣∣Y6u∣∣2 = ∣∣PYβu∣∣2 ≥ (1 - ε)∣∣Yβu∣∣2 ≥
/r	∖	/PT ∖ II 11 ɪ, f 11	,1	/ɪr ∖ 、 /r	∖	/PT ∖	F —	、/r	∖ —
(1 - ε)σmin(Ye)∣u∣2. Itfollows that σmin(Ye) ≥ (1 - ε)σmm(Ye), and σγ,r ≥ (1 - ε)σγr.
-........... z	~	， .	、	，一 、_	_	_. . _ ..	-	- ，一 、	一…
Similarly, σmin(Xe) ≥ (1 - ε)σmin(Xe) for β ∈ Rn, ∣β∣o = r and rank(Xe) = r. It follows
that σχχ r ≥ (1 - ε)σχ,r. Since (16)-(20) hold, the conditions (29)-(33) required by Theorem A.4
.1	.,FF,	∕-‰T F -χr∖ 1 IlF El	Γ∙	. 1	1	1	,	,	1	11	∙ ,1
on the projected data (Y and X) also hold. Therefore, the subspace detection property holds with
β* for Xi with probability at least 1 - Kδ by the union bound when P ≥ δ(d+⅞)2 ∙
□
C B ound for Suboptimal and Globally Optimal S olutions for
Noisy '0-SSC and Noisy-DR-'0-SSC
We further present the bound for the gap between β and β*, ∣∣β - β*∣∣2, based on Theorem 5
in (Yang & Yu, 2019). Let g(β) = ∣∣Xi - Xβ∣∣2 and β* be the globally optimal solution to (3),
S* = supp(β*), β be the suboptimal solution to (3) obtained by PGD, S = supp(β). The following
theorem presents the bound for ∣β - β* ∣2.
Theorem C.1. (Theorem 5 in (Yang & Yu, 2019)) Suppose Xs∪s* has full column rank with κ0，
σmin(Xs∪s*) > 0 where S is the support of the initialization for PGD on problem (3). Let κ > 0
such that 2κ02 > κ and b is chosen according to (78) as below:
λλ
0 <b< min{min lbjl,-r^gη--1, mn |ej|,--r^gη--1}.	(78)
j∈b	maxj∈b | ∂βj∖β=β∖ j∈S	maχj∕S* 1 ∂βj∖β=β* |
Let F = (S \ S*) ∪ (S* \ S) be the symmetric difference between S and S*, then
kβb — β*k2 ≤ 2κ21- K( X (max{0,1 - κ∣βj - b∣})2 +	X (max{0, λ - κb})2)1.	(79)
0	j∈F∩S	j∈F∖Sb
τr⅛	« Z⅛ ɪ, ∙	1	1 ,1	,,1	II za ∕α⅛ Il	11 ι λ	I za τlc "一	含 t
Remark C.2. ItiS observed that the gap ∣∣β - β*∣2 is small when λ 一 κ∣βj - b| for j ∈ F ∩ S and
λ - Kb are small. Based on this observation, Theorem C.3 establishes the conditions under which
β is also an optimal solution to (3), i.e. β = β*.
Define S* = supp(β*), H * = maxι≤j≤n dist(β, Hx,、,}), μ = max{H * + ∣∣βi —
Xβ*∣2, 2∣xi - Xβb∣2, 2∣xi - Xβ*∣2}, κ0 = σmin(XS∪S*) > 0 where S =
following theorem demonstrates that β = β* if λ is two-side bounded and βmin
is sufficiently large.
supp(β(0)). The
=mi%βt=0 lβtl
Theorem C.3. (Conditions that the suboptimal solution by PGD is also globally optimal) If
^ μ
βmin ≥ K2	(80)
and
2~≤ ≤ λ ≤ (∕bmin - 2)μ )μ,	(81)
2κ0	2κ0
then β = β * .
^
Sketch of Proof. It can be verified that max{0, b — κ∖fβj — b|} = 0 and max{0, b — κb} = 0 under
the conditions (80) and (81), therefore, β = β* by applying Theorem C.1.
□
D More Details about Experiments
D. 1 Datasets
We conduct experiments on the following datasets. CoIL-20 dataset has 1440 images of size 32 × 32
for 20 objects with background removed in all images. The CoIL-100 dataset contains 100 objects
23
Under review as a conference paper at ICLR 2022
Accuracy w.r.t. on the Extended Yale-B Data
…Ea …noisy UJ-SSC
-*--Noisy-DR-L0-SSC
-* Noisy-DR-LO-SSC-OSNAP
--1-- KM
--I--SC
-Q--SMCE
SSC-OMP
-O- Noisy SSC
Accuracy w.r.t.入 on the Extended YaIe-B Data
0.8
AOBJn。。4
Figure 3: Accuracy (left) and NMI (right) with respect to different values of λ on the Extended
Yale-B data set
AOBJn。。4
手二二斗二二千二4二二二F二斗二:
-Q-noisy LO-SSC
-M--Noisy-DR-LO-SSC
-M--Noisy-DR-LO-SSC-OSNAP
KM
SC
-B--SMCE
-»-SSC-OMP
-O- Noisy SSC
SDP Violation Rate w.r.t. A with σ2 = 10
0.4
(a) σ2 = 10
(b) σ2 = 20
(c) σ2 = 30
SDP Violation Rate w.r.t. A with <72 = 40
0ss°s0js°j
uo_亘> dαs
(d) σ2 = 40
0.1 ----------1--------1------------------1---------1--------1------------------1--------
0.1	0.2	0.3	0.4	0.5	0.6	0.7	0.8	0.9	1
SDP Violation Rate w.r.t. A with / = 50
lss
°s°3s°31,s
Uo一das
(e) σ2 = 50
0.1 1---------1--------1---------1--------1---------1--------1--------1---------1--------1
0.1	0.2	0.3	0.4	0.5	0.6	0.7	0.8	0.9	1
SDP Violation Rate w.r.t. X with = 60
0.4
°ss°s°3s°31,s
Uo一das
(f) σ2 = 60
0.1
0.1	0.2	0.3	0.4	0.5	0.6	0.7	0.8	0.9	1
Figure 4: The SDP violation rate with respect to λ for noisy '0-SSC, Noisy-DR-'0-SSC and Noisy-
DR-'0-SSC-OSNAP with different noise levels σ2. The SDP violation rates of Noisy-DR-'0-SSC
and that for Noisy-DR-I0-SSC-OSNAP are almost the same, so their curves largely overlap each
other.
with 72 images of size 32 × 32 for each object. The Extended Yale Face Database B (Yale-B) dataset
contains face images for 38 subjects with 64 frontal face images taken under different illuminations
for each subject. CMU Multi-PIE (MPIE) data (Gross et al., 2010) contains 8916 facial images
captured in four sessions, namely S1-S4. The MNIST handwritten digits database has a total number
of 70000 samples of dimensionality 1024 for digits from 0 to 9. The digits are normalized and
centered in a fixed-size image.
D.2 More Experimental Results
Figure 3 illustrates how the accuracy and NMI vary with respect to λ on the Extended Yale-B (Yale-
B) data set.
Figure 4(a) to Figure 4(f) illustrate SDP violation with respect to λ for different noise levels, justi-
fying our theoretical finding that relatively large λ tends to preserve the subspace detection property
24
Under review as a conference paper at ICLR 2022
for noisy '0-SSC, Noisy-DR-'0-SSC and Noisy-DR-'0-OSNAP. It can be observed that larger λ
consistenly renders lower SDP violation rate.
25