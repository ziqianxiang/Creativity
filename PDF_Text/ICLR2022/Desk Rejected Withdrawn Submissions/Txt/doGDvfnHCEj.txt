Under review as a conference paper at ICLR 2022
On the Expressiveness, Predictability and In-
terpretability of Neural Temporal Point Pro-
CESSES
Anonymous authors
Paper under double-blind review
Ab stract
Despite the fast advance in neural temporal point processes (NTPP) which enjoys
high model capacity, there are still some standing gaps to fill including model
expressiveness, predictability, and interpretability, especially with the wide applica-
tion of event sequence modeling. For expressiveness, we first show the incapacity
of existing NTPP models for fitting time-varying especially non-terminating TPP,
and propose a simple neural model for expressive intensity function modeling.
To improve predictability which is not directly optimized by the TPP likelihood
objective, we devise our new sampling techniques that enable error metric driven
adaptive fine-tuning of the sampling hyperparameter for predictive TPP, based on
the event history in training sequences. Moreover, we show how interval-based
event prediction can be achieved by our prediction techniques. To achieve inter-
pretable NTPP, we propose an influence definition from one event to the future
by comparing the difference between the existence of the event and not, which
enables the dependency learning among events and types. Experimental results on
synthetic datasets and public benchmarks show the efficacy of our approach.
1 Introduction
Temporal point process (TPP) (Hawkes, 1971; Zhou et al., 2013) has been a popular and principled
tool for modeling and predicting event data in continuous time space, which involves different forms
of intensity functions for describing the event occurrence rate over time, with explicit (Xu et al.,
2016; Xiao et al., 2016; Zhou et al., 2013) or implicit (Eichler et al., 2017) parametric forms. Most
of the traditional TPPs have a time-varying intensity function, as used for predicting futures and
analyze the influence among events or their dimensions e.g. types of events for multi-dimensional
TPP (Liniger, 2009; Zhou et al., 2013; Wu et al., 2018), which can be used as the causality learning
for event sequences. However, most traditional methods are based on specific intensity assumptions
e.g. Self-Correcting and Self-Exciting functions, which limits the applicability for diverse datasets.
Neural temporal point process (NTPP) models (Du et al., 2016; Xiao et al., 2017) are becoming
increasingly popular for their high capacity to obtain more varying intensity and competitive predic-
tion results. Most of them are designed based on recurrent neural networks (RNN) which endows
them with better adaptability to complex event sequences than traditional TPP. Despite their success,
existing NTPP models still suffer from several issues. First, their intensity function still follows a
specific model, e.g. the popular temporal exponential-decay prior (Du et al., 2016), which limits their
expressions to capture rich intensity forms of real-world patterns and causes under-fitting and biased
predictions. Besides, existing NTPP models are less interpretable than their classic non-network-
based counterparts e.g. Hawkes process. Recent efforts (Wang et al., 2017; Xiao et al., 2019) have
been made to introduce neural attention to improve the interpretability, while many studies (Xiao
et al., 2019) have shown that attention may not be the panacea for achieving stable and reliable
relation finding in sequence learning, and it has an unclear connection to the Bayesian framework.
Therefore, in this paper, we are mainly concentrated on solving the following two problems: i) can we
design a flexible form for fitting intensity function of NTPP and even get better predictions compared
with existing NTPP models; ii) can we increase the interpretability of NTPP and thus directly use
them for analyzing the influences between events? We aim to model a more flexible intensity-based
1
Under review as a conference paper at ICLR 2022
NTPP and design an interpretable framework to fill the interpretability gap and evaluate the infectivity
matrix and triggering matrix as traditional Hawkes processes do. The main highlights are:
1)	We choose a simple, effective yet in literature unused neural model (to our best knowledge, see
Table 1) for encoding time-varying intensify function λ(t∣Hj). Specifically, it is a more general form
than the specific parameterization of neural intensity in (Du et al., 2016; Mei & Eisner, 2017), thus
is less prior dependent. Note that similar to ours, FullyNN (Omi et al., 2019) also directly uses a
neural net but to encode the intensity integral Λ(t∣Hj) instead of λ(t∣Hj), which can not guarantee
the satisfaction of Λ(tj |Hj) = 0 by neuralizing Λ(t∣Hj).
2)	We propose a prediction error metric driven sampling approach based on the Time Change
Theorem (Brown et al., 2002). This approach is applicable to TPP models either based on intensity
modeling (Du et al., 2016) or its integral (i.e. intensity measure) (Omi et al., 2019). We further
show that when the inverse of the intensity measure exists, for popular TPP models including both
classic (Isham & Westcott, 1979) and neural models (Du et al., 2016), the event prediction can be
fulfilled in a closed-form from a tuned sampling variable u based on history sequences.
3)	Inspired by the counter-factual causality learning scheme (Tsushima et al., 2020), we propose a
novel technique for realizing interpretable neural TPP, regarding the dependency between temporal
events as well as event types. We define the influence from one event to the future by comparing the
difference between the existence of the event without specific assumptions on model design. We will
show that our interpretable technique is coherent with the infectivity matrix of traditional Hawkes
process, which cannot be achieved by the attention-based NTPP (Xiao et al., 2017).
Throughout the paper, we term the full version of our neural TPP model as Expressive, Predictive,
and Interpretable Neural TPP, namely EPI-NTPP. The structure is shown in Fig. 6 in Appendix.
When one or two of its components are disabled as they are orthogonal to each other, e.g. Expressive
NTPP without the prediction and interpretation modules, we call it E-NTPP as shown in Fig. 1.
2 Backgrounds and Related Works
Here we give the background to ease the later presentation. TPPs are described with a conditional
intensity function over time, being a direct way to represent the event occurrence. Given the event
history Hj = {t1, t2, . . . , tj}, the conditional intensity function for event j + 1 is:
λ(t∣Hj)
f (t∣Hj)
1 - F(t|Hj),
(1)
where f (t|Hj) is the conditional density function and F (t|Hj) is the corresponding cumulative
probability function for event j + 1. Note here we use the single-dimension TPP for notation brevity.
Then the intensity measure, which can also call the cumulative intensity function, is specified as
Λ
(tHj) = Zλ (T IHj) dτ
tj
(2)
The Hawkes process (Hawkes, 1971) is a mutually exciting multivariate point process, which is
widely used to learn event streams. The definition of the intensity function can be simply specified as
λu(tlHj) = μu +): αuuig(t - ti),	(3)
ti<t
where μu is the base intensity of the type U and the coefficient a。/ captures the mutually exciting
property between type u and ui. g(t) = exp (-wt) is the kernel function which captures time-
varying decaying information. To learn the Hawkes process, the EM algorithm (Zhou et al., 2013) is
widely used which alternatively calculates the triggering probability {pij } and the infectivity matrix
{αuu0}. The Hawkes process takes advantage of its sound interpretability of event inter-effects by
mathematical derivation, but it suffers limited fitting capacity due to its restricted intensity form.
Several NTPPs have been recently proposed to model the event sequences. RMTPP (Du et al., 2016)
is one of most popular NTPP models, which uses recurrent neural networks (RNNs) (Elman, 1990)
to model the TPP by concatenating the time feature tj and marker embedding yj = Wem uj + bem
as input vector. Denote hj the output of RNN, the intensity is specified as (Du et al., 2016):
λ(t∣Hj) = exp (v> ∙ hj + w(t — tj) + b),
(4)
2
Under review as a conference paper at ICLR 2022
Table 1: Comparison of the three features as discussed in this paper. For the difficulty of combining
methods with our proposed techniques (see the last two columns), ‘difficult’ means more computing
overhead than the ‘moderate’ methods. ‘easy’ means that the methods do not need to calculate
the numerical integral for P-NTPP, which costs less than the ‘moderate’ methods. See details in
Appendix J. °Note for combination, it refers to E-NTPP rather than the complete EPI-NTPP.
Methods	Expressiveness	Predictability	Interpretability	Combine P-NTPP	Combine I-NTPP
RMTPP	Eq. 4: exp-form intensity	inadequate integration	little (see Sec. 2)	easy	moderate
NHP 一	Eq. 5: exp-link-form intensity	inadequate integration	no	moderate	moderate
LogNormMix	Eq. 6: fail to fit self-correcting PP	adequate integration	no	difficult	difficult
FullyNN 一	Eq. 7: NN-form intensity integral	inadequate integration	no	convenient	need differentiation
AttentionPP	Eq. 5: exp-link-from intensity	inadequate integration	attention score	moderate	moderate
EPI-NTPPL	Eq. 10: NN-form intensity	adequate integration	probability	moderate	moderate
where v> ∙ hj- denotes the accumulative influence, w(t - tj) emphasizes the influence of current event
j and the rest parameter b gives a base level. Though simple form as RMTPP is, the w parameter can
be interpreted as inhibiting (w > 0) or triggering(w < 0) the occurrence of future events given the
past, which is in fact a nontrivial advantage of our I-NTPP models.
The Neural Hawkes Processes (NHP) (Mei & Eisner, 2017) develop a continuous-time LSTM to
model self-modulating Hawkes processes, whose intensity is as:
λu(t∣Hj) =φu(w>h(t)), h(t) = Oj(2σ(2(c(t)) - 1),
C⑴=Cj+ 1 + (Cj+1 - Cj + 1)exP(-δj + 1(t - tj))
(5)
where φu and σ are the link functions and oj∙, Cj+ι, cj+ι, Cj+ι and δj+ι are the parameters learned
with neural network. Here k in above equations is about the specific design in the cited work
continuous-time LSTM (Mei & Eisner, 2017), which can be understood as the index of the cell.
However, these two mainstreams of intensity-based methods are both based on exponentially time-
varying kernel with several link functions, which is still specific and can cause the the biased
prediction problem as will be discussed in Sec. 3.1.
There are also efforts indirectly learning the NTPP model. For instance, the LogNormMix (Shchur
et al., 2020) suggests generally learning the conditional probability density with a log-normal mixture:
At% ) =	XX Wk √∏Γ⅛-exP	J	MW-”,)2 )，	⑹
M √2π(t- tj-) ∙ Sk	∖	2sk )
where W are the mixture weights, μ are the mixture means, and S are the standard deviations. Exactly,
LogNormMix models the NTPP with density, which has more advantages in optimization of log-
likelihood and expectation-based future prediction. However, it is inconvenient to get the intensity
function and thus cause trouble to explore more information e.g. dimension/event influence. More
importantly, the expressiveness of LogNormMix is limited, which fails to model some specific point
processes such as Self-correcting point process. We discuss more about LogNormMix in Appendix A.
FullyNN (Omi et al., 2019) first uses a neural network to learn the intensity measure Λ(t∣Hj) as
defined in Eq. 2 in which the weights are all constrained to be positive to guarantee the network
output is monotonically increasing w.r.t. the elapsed time t and utilizes autograd mechanism of deep
learning framework to compute the intensity λ(t∣Hj) to avoid numerical integration of the intensity.
∂∂
Λ(t∣Hj )= NN (t - tj, RNN(Hj)), λ (t|Hj ) =m A(t∣Hj∙) =瓦 NN(t, Hj).	⑺
In this setting, though the model enjoys the convenience brought by the universal approximation of
neural network, directly modeling the intensity measure may have numerical flaws like the basic
requirement for a TPP: A(t+∣Hj∙) = 0, and also especially for conflicting the non-terminating
assumption as will be detailed in Sec. 3.1 and further in Appendix B: the output of Λ network cannot
automatically satisfy the basic necessary condition: A(∞∣Hj∙) → ∞.
Exactly, all of the above models exist their own advantages, but may fail at some parts of the perspec-
tive of expressiveness, predictability and interpretability. In this paper, we propose three independent
parts in Sec. 3, 4 and 5. We hope our methods (especially predictability and interpretability parts)
can be useful as orthogonal technologies for existing works. Table 1 summarizes and compares the
popular TPP models in the three aspects, and also the combination readiness with our techniques.
3
Under review as a conference paper at ICLR 2022
(a) Hawkes Simulation
(b) SC Simulation
NYSE
(c) NYSE Data
Figure 1: Left three: compared with RMTPP, NHP and FullyNN, our E-NTPP (here only using the
expressive module) can more effectively approximate the limit of integral density F in Eq. 8, namely
the accumulated probability for next event occurrence is 1 for non-terminating TPP as embodied by
Hawkes, Self-Correcting simulation data and NYSE real-word data. Right most: actual inter-event
time dj +1 = tj +1 - tj in blue and its prediction dj+1 = tj +1 - tj with expectation predictions in
ʌ
orange by Eq. 9 for fitting Hawkes process by RMTPP. Note in this example, most prediction dj+1
are below zero i.e. the prediction is earlier than the current time and meaningless.
3 On Expressive and Sound Neural TPP
We first show that many existing NTPP models (Du et al., 2016; Mei & Eisner, 2017; Omi et al.,
2019) still have modeling limitations especially for non-terminating processes i.e. there is always the
occurrence of the next event. Table 1 gives an overview.
3.1	Soundness of Intensity Function
For designing a powerful and sound intensity function, we start with a motivating observation for the
undesirable effects in the non-terminating process as encountered by many existing intensity models.
As shown in Fig. 1(a), the event data is simulated from a HaWke process with μ = 0.2, α = 0.8,β = 1
being a non-terminating point process defined by Eq. 3, i.e. the next event must occur within a limited
time period. However, as shown in Fig. 1(a), we can find that the density integral (i.e. the upper
bound of the cumulative probability distribution function) is less than 1 for many methods: RMTPP,
NHP, and FullyNN, and the case also holds for other data source from Self-Correcting process and
real-world NYSE dataset as shown in Fig. 1. Specifically, the density integral is given by:
F
∞ f(t|Hj)dt
tj
< 1,
(8)
It means these NTPPs inherently are unable to exactly learn the non-terminating sequences. Specif-
ically, RMTPP with intensity in Eq. 4, can be strictly proved as a terminating point process when
w < 0 (more details are discussed in appendix), which fails to learn the non-terminating TPP.
Besides, the corresponding expectation prediction exists the theoretical bias for predicting the future
due to the inadequate integration of expectation. The expectation prediction can be written by:
ʌ
tj+1
Z	t ∙ f(t∣Hj)dt,
tj
(9)
We again take RMTPP as an example for prediction by Eq. 9 in Fig. 1(d), whereby the estimated next
event time can be earlier than the current time: ^j+ι < j The larger tj is, the larger expectation
bias will be. One way to mitigate this issue is to calculate the inter-event time directly instead of
ʌ
calculating tj+1 first (see details in Appendix C). It reduces bias yet is still a lower estimation.
Why we care about non-terminating for NTPP model? Most NTPP models are based on RNN
or Transformer (Zuo et al., 2020) architecture, which inputs a sequence of events(for example
t1, t2, . . . , tn) to model the intensity and the next event(i.e. t2, t3, . . . , tn+1) are use to get the
likelihood. However, if the TPP is a terminating one (for example terminating at tk), then rest
inputs(i.e. tk+1, . . . , tn) will become unreasonable for the model. Thus these RNN or Transformer
based methods have a basic assumption of non-terminating requirements before the end event tn .
To address all these issues, we propose a new NTPP intensity function, which is less human prior
dependent and is shown can learn the non-terminating point process effectively.
3.2	Intensity Form Unspecified Neural Point Process
We herein propose our expressive neural point process, using a neural network to capture the arbitrary
form of intensity function. The output of the intensity neural network is ensured to be positive by an
4
Under review as a conference paper at ICLR 2022
activation function ζ e.g. Softplus as used in our experiments which is associated with dimension u:
λu(t∣Hj) = Z(NNu(t - tj, RNN(Hj))),	(10)
where RNN(Hj) embeds the history information to the vector and NN(∙, ∙) capture the free form of
intensity function with a fully connected network, which can fit all the forms of function theoretically.
The summation of intensity over all dimensions is λ(t∣Hj) = Pu λu(t|Hj), which can be used to
predict the next event time in Sec. 4 and we still apply the log-likelihood to optimize the neural
networks as used in (Du et al., 2016; Mei & Eisner, 2017).
Different from the FullyNN, which designs the intensity measure directly and calculates the intensity
function by the differentiation of the intensity measure, we design the intensity with free form and get
the intensity measure with numerical integration. Thus we can avoid the drawback that A(0∣Hj∙) = 0
(recall the discussion in Sec. 2).
Models e.g. RMTPP are essentially terminating NTPP due to the design flaws of the intensity. So
given the event sequences of non-terminating TPP (e.g. Hawkes), the intensity is not expressive
enough, leading to F < 1, which causes poor expectation prediction as shown in Fig. 1(d). In
Fig. 1(a) 1(b) 1(c), E-NTPP outperforms when modeling non-terminating TPP (F is closer to 1).
4	On Sampling based Prediction for Neural TPP
Given a trained model e.g. E-NTPP, RMTPP etc., we propose a sampling based method to improve
the predictability i.e. prediction ability. In particular, it is focused on the next event timing and
interval estimation, but not event marker. The algorithm is shown in Appendix G.
4.1	Adaptive Prediction via Hyperparameter Tuning on Training Data
Most existing prediction methods (Du et al., 2016; Mei & Eisner, 2017; Shchur et al., 2020) use
expectation prediction methods as shown in Eq. 9 to predict the next event. In this paper, we develop
a prediction error metric driven sampling approach for event prediction over a time window. Note in
this section, we only need a trained NTPP model with its intensity or intensity measure, then this
technology can be applied as the sampling or prediction for the event sequences.
With the Time Change Theorem (Brown et al., 2002), the intensity measure Λ(tj+1 |Hj) =
Rttj+1 λ(τ∣Hj∙)dτ is exponentially distributed with the parameter 1 (see the details in Appendix D).
Then given the current event time tj- and a parameter U 〜Uniform(0,1), we can easily derive:
t
λ(τ|Hj)dτ = -log(1 - u),	(11)
j
where - log(1 - u) is the sampling of the exponential distribution. The above formula paves
the road to predicting time. At first, when the inverse of the intensity measure Λ-1(t∣Hj) ex-
ists, for some popular TPP models including both classic (Isham & Westcott, 1979) and neural
models (Du et al., 2016), the event sampling can be efficiently performed in a closed-form with
tu+ι = AT(t|Hj)(- log(1 - u)) given the event history Hj. For example, with the intensity of
RMTPP in Eq. 4, we can get an analytical solution with sampling methods (recall U 〜Uniform(0,1)):
A(t|Hj) =
^	log (exP(lj) - wlog(1 - U)) - lj
tj+ι(U) = tj +-------------------w------------------,	(12)
where lj	= v>hj	+ b. For other NTPP	models such as	Neural Hawkes	(Mei & Eisner, 2017),
FullyNN (Omi et al., 2019), we can get the sampling result by using a root finding method. Let
K(t) = A(t|Hj) + log(1 - U), we cannot obtain the inverse of K directly like RMTPP. However,
given a certain u, we can find that K(0) < 0, K(∞) → ∞ and K0(t) = λ(t∣Hj∙) > 0. Hence t has
the unique solution for K(t) = 0, which can be find by the Newton’s iterative method:
k+ι = tk_ K (tk) = tk_ A(tk∣Hj )+log(1- U)
=	K 0(tk) =	λ(tk |Hj)
(13)
Sometimes uncertainty prediction (i.e. sampling) is not enough for predicting. People may prefer a
certain next event prediction instead of an uncertain one like expectation.
A simple way is to set the value U = 0.5 to calculate the median prediction as done in (Omi et al.,
2019). However, the median prediction has been proven as a biased prediction such as Poisson
5
Under review as a conference paper at ICLR 2022
process (Streit, 2010). So instead of setting the value u = 0.5, we can search u from the training data
adaptively, to better fit the prediction metric (e.g. MSE) in testing or validation data.
Specifically, we assume that the training data and the testing data are independently and identically
distributed. Then given the training data{ttjr}jN=1 and testing data {ttje}jM=1, we can estimate the
adaptive value uada from training sequences, which is specified as:
uada
arg min
u∈(0,1)
(14)
where tj+ι(u) is the sampling results for next event time based on training sequence. As shown
in Algorithm 1 in Appendix G, bisection search is used to obtain an optimal value uada to fit the
behavior of training sequences in terms of an optimal u. Then for the testing sequences, we can
simply predict the next event ttje+i as £j+i(u&da), which is the MSE-guide prediction result.
Thus, given a trained NTPP model, we calculate the adaptive value uada from training sequences by
Eq. 14, and get the adaptive predictions with Eq. 13 by setting u = uada for testing sequences.
Remarks. Note our adaptive prediction is based on the Time Change (Brown et al., 2002) (see
Theorem 1 in Appendix) and its derived inverse sampling method, and cannot use the popular
thinning method (Guan & Loh, 2007) instead. In fact, the thinning method is more suitable to
generate whole sequences from scratch instead of predicting next event, as it fails to get the 1-to-1
mapping between u and the next event, which needs multiple random values to sample the next event.
4.2	Further Discussion
In terms of predictability referred in this paper, predicting the time interval is also important if not
more important than time point prediction: e.g. people may be interested in the probability p that the
next event occurs in the interval [τ1, τ2]. Exactly, for the three values (p, τ1, τ2), if two of them are
known, then the rest can be calculated with the above sampling method. Given the first event time τ1
and the probability p, we have τ2 = tuj 1 where u can be specified as:
U2 = P +(1 - exp(-Λ(τι∣Hj))).	(15)
The proof is given in Appendix F. Here 1 - exp (-A(τJHj)) = uι refers to the first sampling value
of τ1 , u2 which calculated above refers to the second sampling value of τ2 and Λ is the intensity
measure defined in Eq. 2. Then the next event tj+1 occurs in [τ1, τ2] with the probability p = u2 - u1.
Thus given τ1 , p, we can calculate the rest parameter τ2 which means that the next event happens in
the interval [τ1, τ2] with the probability p according to the model.
In practice, given the specific pair of sampling values (u1, u2), it is known for the corresponding
probability p and interval [τ1, τ2]. Then for a NTPP model, we can estimates the interval [τ1, τ2]
given p = u1 - u2 . Besides, for every event, we can also test the model by comparing the theoretical
probability and the frequency of next event (i.e. tj+1 ∈ [τ1, τ2] for every event).
Quantile Interval Estimation (QIE). Let (u1, u2) = (0.05, 0.95), (0.1, 0.9), . . . , (0.45, 0.55),
which has the corresponding theoretical probability p as 0.9, 0.8, . . . , 0.1. Then we can do the
estimation of [τ1, τ2] and test the model with frequency of the events. Specifically, given the dataset
{tj }jJ=+11 , the statistical frequency can calculated as
vQIE(α)
t 二(f(α) ^(I-2)
tj+1 ∈ tj+1, tj+1
(16)
J
where 1(∙) is the characteristic function and a = 1 - P is the significance level. By setting uι = 2
and u2 = 1 - α where α = 0.1,0.2,..., 0.9, we can compare the consistence of VQIE and its
corresponding probability P for model test. We give such a study result in experiments with different
α in Fig. 7(a) and Fig. 7(b) in Appendix.
Compact Interval Estimation (CIE). We also discuss how to estimate the most compact i.e. shortest
interval for a given probability P, which can be useful for time-sensitive decision making. However,
compact interval is hard to estimate and obviously quantile interval may not be the most compact one.
It is interesting to find that we can directly estimate compact interval as ∖t0+ι, ^jI-La)] when w < 0
for RMTPP as a special case. We discuss more in Appendix F.
6
Under review as a conference paper at ICLR 2022
5	On Counter-factual Interpretable Neural TPP
Now we provide a new perspective to show how the existing NTPP models can explain the point
process with clear interpretability. So our method in this section can be view as a supplement to the
interpretability of the previous works which lack the interpretability before. Similar with the method
in Sec. 4, what we need is a trained NTPP model with its continuous intensity function.
5.1	Influence among Dimensions
We aim to establish a definition of influence between events and give quantitative estimation, which
is often missing in neural TPP literature, and often only holds for specific TPP like Hawkes process.
Given the history Hj-1, we explore what the event j influences at (current) time t on dimension u.
We discuss the two exclusive cases separately: i) event j occur at time tj , then we can get the rate of
event occurrence as λu(t∣Hj); ii) event j does not occur, then the rate of event occurrence at (current)
time t is λu(t∣Hj-ι). In the sense of counter-factual, between the occurrence and nonoccurrence of
event j, we define the influence of event j by the difference of the two intensities:
Iu(t) = λu(t|Hj) - λu(t∣Hj-ι).	(17)
Then we show the rationality of the definition, by taking the example of traditional Hawkes processes,
in which the concept of influence is often be used. The intensity of Hawkes processes can be specified
as: λu(t∣Hj) = μu + Pt,<t auuje-β(t-tj). For t = tj in Eq. 17, the influence Iu is exactly as:
Iuj (tj) = X αuuie-β(tj-ti) - X αuuie-β(tj-ti) =αuuj.	(18)
ti<tj+	ti<tj
Thus Iuj (tj ) is exactly equal to the parameter of infectivity matrix αuuj in Hawkes process, which
can be interpreted as the influence from dimension uj to dimension u. Therefore, our definition in
Eq. 17 is not contradictory to the ’influence’ understood by the traditional TPP. Then for a trained
NTPP model, we can estimate the influence between type u and u0 with the mean of Iuj on u0 as
αuuo = XIu(tj)/ X ι,	(19)
j	j:uj =u0
We call the matrix {ɑuuo} as infectivity matrix here. Different from Hawkes processes, the mean of
Iu (tj-) (i.e. αuuo) is allowed to be negative which means the event j suppresses event on dimension u.
Besides, we propose another way to estimate αuuo given a trained NTPP, with intensity specified
as λu(t) = λu(t) + |cu| where λu(t) is the intensity of RMTPP and cu is the trainable parameter.
Note the above NTPP is in line with Hawkes processes and thus every parameter of Hawkes can be
estimated (see Appendix K). Given the Hawkes simulation data, Fig. 5 shows the estimation is quite
accurate, suggesting that the NTPP really understands Hawkes’ generation mechanism.
5.2	Influence among Events
People are often interested in finding the triggering effects between temporal events. For example,
which historical event results in the occurrence of the current event. In multi-dimensional Hawkes
processes, there exist similar results. By using EM algorithm to estimate the parameters (Zhou et al.,
2013), we can get a event triggering matrix {pij } as specified as
pij
ɑuiuj g (ti - tj )
μui + Pj=1 αuiuj g (ti - tj )
pii
____________μui_____________
μui + Pj = 1 αuiuj g (ti - tj )
(20)
where pij denotes the probability that event i is triggered by event j and pii represents the probability
that event i is self-triggered. However, the triggering is mainly limited to the {αuv } and the kernel
g(t), which can neither get a long dependence from history nor express the negative influence. By
the definition of influence in Eq. 17, we propose the triggering/suppressing 'probability, matrix Pij:
λui(ti∣Hj)- Xu"]4%1)
λui (ti∣Hti-ι)
Pii = 1 - Epij.
i6=j
(21)
Exactly λui (ti |Htj) - λui (ti|Htj-1 ) = Iuj (ti), which means the influence of event j at time ti
on dimension Ui. Thus Pij is the influence ,ratio, from event i to event j, which can evaluate the
importance of event i given the occurrence of event j. Besides, {Pij} in Eq. 20 is a special case of
Pij given the Hawkes intensity, which can be interpreted as the impact of event i to event j.
7
Under review as a conference paper at ICLR 2022
Figure 2: Comparison of infectivity matrix estimation for multi-dimensional Hawkes process.
Table 2: Negative log-likelihood and RMSE (×10-4) results for synthetic and real-world data. Note
the first, second and third columns are Expectation, Median and Adaptive prediction methods.
^	`	____^SynthetiC dataset Method	Hawkes				Self-Correcting				Renewal			
	NLLJ	RMSEJ			NLLJ	RMSEJ			NLLJ	RMSEJ		
		exp	med	adp		exp	med	adp		exp	med	adp
RMTPP (	,	)	-2.004	^T80""	806	^T77^"	-1.404	^^29^"	884	^^29^"	-1.782	975	1010	975
NHP ( ei & Eisner, 2	)	-2.019	771	804	771	-1.329	835	890	835	-1.849	975	1048	968
FullyNN ( mietal.,20Γ )	-2.000	779	810	779	-1.391	838	870	838	-1.786	975	1013	978
LogNormMix (ShChUr et al., 2020)	-2.015	777	-	-	-1.403	829	-	-	-1.882	970	-	-
	EP-NTPP (ours)	-2.021	774	796	775	-1.415	817	827	823	-1.850	974	1043	971
''''^^^ReaI dataset Method		AT	M		StaCkOVerflOW					NYSE		
												
	NLLJ	RMSEJ			NLLJ	RMSEJ			NLLJ	RMSEJ		
		exp	med	adp		exp	med	adp		exp	med	adp
RMTPP (	,	)	-1.425	^356""	638	^348""	-0.448	^648""	663	~555~	-1.171	1502	1536	1503
NHP ( ei & Eisner, 2	)	-1.340	565	657	553	-0.323	643	675	648	-1.295	1542	1624	1545
FullyNN ( mietal.,20Γ )	-1.733	556	2209	806	-0.395	644	672	643	-1.176	1522	1558	1527
LogNormMix (ShChUr et al., 2020)	-1.408	674	-	-	-0.525	654	-	-	-1.243	1543	-	-
	EP-NTPP (ours)		-1.988	545	537	537	-0.492	643	669	642	-1.347	1498	1592	1508
6	Experiments and Discussion
We fit and evaluate our various models on several simulation and real-world datasets. We apply Min-
max normalization on all datasets before experiments to overcome the numerical overflow problem
caused by exp operation in RMTPP (Du et al., 2016) and NHP (Mei & Eisner, 2017). Besides,
median absolute deviation (MAD) based anomaly detection method (see details in Appendix I) is
also applied for data splitting (partition sequences at large time intervals).
In the experiments, for evaluation of expressiveness, E-NTPP is used to compare with existing
works such as RMTPP (Du et al., 2016), NHP (Mei & Eisner, 2017), FullyNN (Omi et al., 2019),
and LogNormMix (Mei & Eisner, 2017) with negative log-likelihood (NLL). Besides, for the
predictability, the adaptive prediction method (i.e. P-NTPP) is evaluated compared with median
and expectation with root mean square error (RMSE). At last, for interpretability, the event and
dimension influence (I-NTPP) is also estimated with counter-factual interpretable methods compared
with ADM4 (Zhou et al., 2013) and attention-based method (Xiao et al., 2017).
6.1	Protocols and Datasets
To show the effectiveness, we first test our proposed methods in synthetic data to show the expressive-
ness, predictability and interpretability of our approaches. The datasets are simulated with Hawkes
processes, Self-Correcting processes (SC) and Renewal processes. The setting of the hyperparameter
is shown in Appendix I. Specifically, 0.1 million events (640 sequences) are simulated for each
generated dataset and we use 60% of them for training, 20% for validation and 20% for testing.
In addition to NLL, we also evaluate by RMSE of expectation, median and adaptive prediction in
Table 2. Besides, the interpretability is also evaluated to show our method’s superiority.
1)	NYSE. A book order from NYSE with 0.7 million transactions (Du et al., 2016). Each transaction
contains time (in millisecond) and possible action (buy/sell). We cut sequence with 1,929,600 events
for training and 482,400 for test. The action type is as a marker and we predict when an action occurs.
2)	ATM. The dataset is mainly for the predictive ATM maintenance problem, which is comprised of
the event logs involving error reporting and failure tickets (Xiao et al., 2017). The type of ATM has
2 main types ‘ticket’ and ‘error’ within 7 months in America. Besides the ‘error’ is divided into 6
subtypes: printer (PRT), cash dispenser module (CNG), internet data center (IDC), communication
part (COMM), printer monitor (LMTP), miscellaneous. We treat them as 7 types to train the model.
3)	Stack Overflow. It contains question answering records on Stack Overflow days (Paranjape et al.,
2017). Every question answering event is recorded whose dimension refers to the user ID. We collect
the data in a two-year period and treat the reward history of each user as the event sequence.
8
Under review as a conference paper at ICLR 2022
(a) ADM4(Hawkes)
(b) EI-NTPP(Hawkes)
(c) EI-NTPP(SC)
Figure 3: Event Influence by calculating {pij } with EI-NTPP for Hawkes and Self-Correcting data.
-0.50]-9.7S
SS8£ ssMeH ⅛ Selueed
(a) Dimension influence (b) Event influence
Figure 4:	Influence estimation over event types
and event records for ATM dataset.
(a) 1D-Hawkes estimation (b) 2D-Hawkes estimation
Figure 5:	Parameters learning for 1-D and multi-
dimensional Hawkes process via NTPP.
6.2	Experimental Results and Discussion
Results of NLL and RMSE of expectation. Table 2 shows the NLL and RMSE compared with
different models. Our EP-NTPP outperforms on NLL and RMSE of expectation prediction on
Hawkes, Self-Correcting simulation data and ATM, NYSE real-world data. Besides, in Renewal
and StackoverFlow datasets, our EP-NTPP is also ranked in the top two of all models. For the
synthetic data, we can find our EP-NTPP performs the top two among Hawkes, Self-Correcting and
Renewal simulated datasets. Specifically, EP-NTPP models best with the lowest NLL and RMSE in
Self-Correcting simulated sequences and has the best NLL in Hawkes simulations. For real-world
data, EP-NTPP outperforms on ATM and NYSE datasets both w.r.t. NLL and RMSE metric.
Comparison of expectation, median and adaptive prediction. In particular, expectation, median
prediction and adaptive prediction are also given in Table 2. The simplest median prediction used in
(Omi et al., 2019) performs worst. Note Adaptive prediction outperforms in some cases, which has no
concerns of bias. For the models like RMTPP and FullyNN, which can calculate the intensity measure
without numerical integration, sampling based methods have more computational advantageous and
do not need to calculate numerical integration twice, that is, calculating the first numerical integration
to get f (t|Hj) and then calculating the second integration for expectation.
Performance of interpretability. Then we show that the efficacy of interpretability of EI-NTPP,
which can capture the influence among dimensions and events. As shown in Fig. 2, we simulated
multi-dimensional Hawkes processes with a particular infectivity matrix as the ground truth in
Fig. 2(a). we can see that our method which calculates the influence among dimensions by Eq. 19
with EI-NTPP performs the best. For the influence among events, the results are shown in Fig. 3.
We can find that the influence between events simulated by Hawkes processes is similar to the
traditional method(i.e. ADM4), the calculated {pij } value reflects the occurrence mechanism of
events: self-generated or triggered in Hawkes. And for the evaluation of Self-Correcting simulated
data, the red diagonal represents that the event occurrence are more likely to be self-generated and
the historical events have negative effects on the occurrence of current events.
In ATM experiment, Fig. 4 shows a counter-factual-based interpretation model for neural TPP
(event/dimension influence mining), which is new to attention-based TPP models. The influences of
dimensions are calculated with E-NTPP proposed in Sec. 3.2, whose dimension types includes printer
(PRT), cash dispenser module (CNG), internet data center (IDC), communication part (COMM),
printer monitor (LMTP), miscellaneous, and ticket in Fig. 4(a). Fig. 4(b) reflects the mutual triggering
or suppressing effects between events(i.e. {pij}), and show the long-term dependency of 100 events.
In addition, accuracy of event type prediction is also reported on different datasets in Appendix H.
9
Under review as a conference paper at ICLR 2022
References
Emery N Brown, Riccardo Barbieri, Valerie Ventura, Robert E Kass, and Loren M Frank. The
time-rescaling theorem and its application to neural spike train data analysis. Neural computation,
14(2):325-346, 2002.
N. Du, H. Dai, R. Trivedi, U. Upadhyay, M. Gomez-Rodriguez, and L. Song. Recurrent marked
temporal point processes: Embedding event history to vector. In KDD, 2016.
M. Eichler, R. Dahlhaus, and J. Dueck. Graphical modeling for multivariate hawkes processes with
nonparametric link functions. Journal of Time Series Analysis, 2017.
J. L Elman. Finding structure in time. Cognitive science, 14(2):179-211, 1990.
Yongtao Guan and Ji Meng Loh. A thinned block bootstrap variance estimation procedure for
inhomogeneous spatial point patterns. Journal of the American Statistical Association, 102(480):
1377-1386, 2007.
A. Hawkes. Point spectra of some mutually exciting point processes. Journal of the Royal Statistical
Society. Series B (Methodological), pp. 438-443, 1971.
Hengguang Huang, Hao Wang, and Brian Mak. Recurrent poisson process unit for speech recognition.
In AAAI, 2019.
V.	Isham and M. Westcott. A self-correcting point process. Stochastic Processes and Their Applica-
tions, 8(3):335-347, 1979.
Shuang Li, Shuai Xiao, Shixiang Zhu, Nan Du, Yao Xie, and Le Song. Learning temporal point
processes via reinforcement learning. In NIPS, 2018.
T. J. Liniger. Multivariate hawkes processes. PhD thesis, Swiss Federal Institute Of Technology,
Zurich, 2009.
H. Mei and J. Eisner. The neural hawkes process: A neurally self-modulating multivariate point
process. In NIPS, 2017.
Takahiro Omi, Naonori Ueda, and Kazuyuki Aihara. Fully neural network based model for general
temporal point processes. In NeurIPS, 2019.
Ashwin Paranjape, Austin R. Benson, and Jure Leskovec. Motifs in temporal networks. In WSDM,
2017.
Jakob Gulddahl Rasmussen. Lecture notes: Temporal point processes and the conditional intensity
function. arXiv preprint arXiv:1806.00221, 2018.
Oleksandr Shchur, Marin Bilos, and StePhan Gunnemann. Intensity-free learning of temporal point
processes. In ICLR, 2020.
Roy L Streit. Poisson point processes: imaging, tracking, and sensing. Springer Science & Business
Media, 2010.
Kanae Tsushima, O. Chitil, and Joanna Sharrad. Type debugging with counter-factual type error
messages using an existing type checker. 2020.
Yongqing Wang, Huawei Shen, Shenghua Liu, Jinhua Gao, and Xueqi Cheng. Cascade dynamics
modeling with attention-based recurrent neural network. In IJCAI, 2017.
W.	Wu, J. Yan, X. Yang, and H. Zha. Decoupled learning for factorial marked temporal point
processes. In KDD, 2018.
S. Xiao, J. Yan, C. Li, B. Jin, X. Wang, X. Yang, S. Chu, and H. Zha. On modeling and predicting
individual paper citation count over time. In IJCAI, 2016.
S. Xiao, J. Yan, X. Yang, H. Zha, and S. Chu. Modeling the intensity function of point process via
recurrent neural networks. In AAAI, 2017.
10
Under review as a conference paper at ICLR 2022
Shuai Xiao, Junchi Yan, Mehrdad Farajtabar, L. Song, X. Yang, and H. Zha. Learning time series
associated event sequences with recurrent point process networks. IEEE Transactions on Neural
Networks and Learning Systems, 30:3124-3136, 2019.
H. Xu, W. Wu, S. Nemati, and H. Zha. Icu patient flow prediction via discriminative learning of
mutually-correcting processes. TKDE, 2016.
K. Zhou, H. Zha, and L. Song. Learning social infectivity in sparse low-rank networks using
multi-dimensional hawkes processes. In AISTATS, 2013.
Simiao Zuo, Haoming Jiang, Zichong Li, Tuo Zhao, and H. Zha. Transformer hawkes process. In
ICML, 2020.
11
Under review as a conference paper at ICLR 2022
Appendix
A The shortcomings of existing NTPP
Here we show some shortcomings for LogNormMix. First, though the intensity can be calculated
with the density and survival functions, we can not get the closed-form intenisty for LogNormix.
In fact, it depends on whether density can be integrated with a closed-form, and F (t|Hj) is not in
closed-form for LogNormix. To see this, one can rewrite the intensity of LogNormix as
λ(t∣Hj)
f(t∣Hj)
1 - F(t|Hj)
Here F (t|Hj ) is CDF of Gaussian mixture, which can be specified as
F (t|Hj) =	wiΦi(t)
where Φi(t) is CDF of Gaussian with no closed-form. Thus the intensity is not in closed-form.
Then due to the non-closed-form problem, LogNorMix can not get the continuous results of intensity
function, which causes inaccuracy and high-cost problems for calculating every λ(ti) (ti may be not
the discrete-time point). Other intensity/intensity measure-based methods will not suffer from this
problem.
Besides, due to the Log-Gaussian based form of density for LogNormMix, the corresponding intensity
may be a restricted form. For example, when t → ∞, the intensity is specified as
lim λ(t∣Hj) = lim -f (""j
t→∞ ( | j)	t→∞ 1 - FwHj)
lim
t→∞
f(tHj)
-f (t∣Hj)
For simplicity, we use one LogNorm for f (t|Hj) (The mixed model is similar), which can be specified
as
f(t|Hj)
—^exp(-
TS √2∏
(log τ - μ)2
2S2
)
Here τ = t - tj . Then
lim λ(t∣Hj)=lim -fH^T-fHjl
t→∞	t→∞	-f(t|Hj)
s2 + (log τ - μ)
lim --------5-----
t→∞	τS2
and use L'H6pitaTs rule again:
lim λ(t∣Hj) = lim -ɪ → 0
t→∞	t→∞ τS2
So LogNormMix may be not a good way to capture the history information of all sequences such as
Self-correcting based events because the corresponding intensity is restricted and not free enough.
B	The Terminating Problem
For a recurrent-based model, there exists a basic assumption that the next event must occur, which is
non-terminating for the point process. Thus the design of the intensity function requires theoretical
basis to guarantee the mathematical properties as indicated in the following theorem.
Proposition 1 A conditional intensity λ(t∣Hj-) uniquely defines a non-terminating point process ifit
satisfies the following conditions for any {t1,t2,... ,tj } and t > tj: 1) λ(t∣Hj) is non-negative and
integrable in the interval [tj-, ∞);2) Jt λ*(τ)dτ → ∞ for t → ∞.
The non-terminating point process are discussed in detail in (Rasmussen, 2018), which require that
the upper bound of the cumulative probability function
F = / f *(τ )dτ = 1 — exp
tj
∞
λ*(T)dτ),
(22)
12
Under review as a conference paper at ICLR 2022
tends to 1(i.e. F = 1). However, several existing NTPP model may not satisfy the Condition2 such
as FullyNN (Omi et al., 2019) and RMTPP (DU et al., 2016), which leads to F < 1 and thus do not
meet the basic assumptions of recurrent-based point process model.
In detail, we take an example for RMTPP model, whose intensity is given in Eq. 4. To analyze the
terminating problem, there exist three situations about the parameter w:
i)	When w > 0, the intensity increases until the next event occurs, whose form is similar to Self-
Correcting point process (Isham & Westcott, 1979) and satisfies Proposition 1 as non-terminating
point process.
ii)	When w = 0, the point process becomes the Recurrent Poisson Process with exponential link
function as used in (Li et al., 2018; Huang et al., 2019) which satisfies Proposition 1 as a non-
terminating point process.
iii)	However, when w < 0, there is an issue with Eq. 4. Consider the integration of the conditional
intensity function for the next event:
lim ∣'t λ(τ )dτ = lim exp(j+w(t- tj))-exp(Ij) = -Lxp( j) < ∞,	(23)
t→∞ t	t→∞	w	w
where lj = v>hj + b. It means condition 2 of Proposition 1 is not satisfied. Thus when w < 0, the
point process with intensity Eq. 4 may terminate at the current event for t∞ f (t|Hj) dt < 1, which
does not satisfy the implied assumptions in RNN model. Then for RMTPP model, we can rewrite
Eq. 22 as
F = 1 - exp	el) < 1,	(24)
which means that the point process continues with probability F, and terminates with probability
1 - F. Exactly, most real-world event sequences are more likely to show the situation of w < 0
when Learning RMTPP model. Thus we should not easily ignore this terminating problem. Besides,
there also exist this terminating problem for other more complex NTPP models, whose experimental
results are given in Fig. 1.
C The Biased Prolem
The expectation prediction of next event can be written as
ʌ
tj+1
Z t ∙ f(t∖Hj)dt
tj
(25)
However, the estimation above is biased because
∞
f(t|Hj)dt
tj
< 1,
(26)
This problem can cause high bias prediction as shown in Fig. 1(d). The prediction of inter-event time
dj = tj+1 - tj are easily below zero meaning their prediction is earlier than the current time and
meaning less given the small F and high current event tj .Though the prediction Eq. 25 is used in
their paper (Du et al., 2016; Mei & Eisner, 2017), most researchers use
+∞
dj+1=/	T ∙f(T+tj|Hj)dτ
(27)
in their code to predict the inter-event time directly, which can mitigate the bias problem. We use
this modified expectation prediction in Tab. 2. However, note this method still is a biased prediction
method theoretically.
D	Time Change Theorem
Here we rewrite the Time Change Theorem, which is used in Sec. 4.
F
13
Under review as a conference paper at ICLR 2022
Training data
Predictability
^j+l(^αdα) uada
Algorithm 1
d(t)
√
Softplus
Aι(0
⅜(0
⅜(t) -kΛ(t)
而⑷
Figure 6: Overview of EPI-NTPP. We propose E-NTPP to improve the expressiveness, P-NTPP
to improve the predictability and I-NTPP to improve the interpretability of neural TPP. These
three components are orthogonal to each other, which can be used together to improve the overall
performance.


Theorem 1 Time Change Theorem (TCT) (Brown et al., 2002) For a temporal point process
{t1,t2,...,tj,…} with conditional intensity λ(t∣Hj), the integrated conditional intensity in (tj, tj+ι],
i.e. intensity measure, has the form:
Λ(tj+ι |Hj) = ∕tj+1 λ*(t)dt.
tj
(28)
Then Λ(tj, tj+1) obeys the exponential distribution with parameter 1.
The theorem above reveals the essence of numerical value for intensity function and intensity measure.
It is often used in point process based analysis such as QQ-plot for testing, Least Square (LS) loss etc.
E	The overview of our methods
We show the overview of our EPI-NTPP in Fig. 6. Note our three technologies(i.e. E-NTPP,P-NTPP
and I-NTPP) are are orthogonal to each other. Note P-NTPP and I-NTPP can also be applied to other
NTPP models such as RMTPP, NHP and FullyNN.
F	Sampling Method for interval prediction
Here we discuss more about Sampling Method for interval prediction of neural point process.
F.1 The proof of Eq. 15
It is evident about the probability p with p = u2 - u1 . Due to Eq. 11, we can get that
u1 = 1 - exp (-Λ(τ1 |Hj)).
14
Under review as a conference paper at ICLR 2022
Figure 7: Quantile/Compact interval estimation (Q/CIE) for simulation data generated by Self-
Correcting and Hawkes processes.
Then Eq. 15 can be derived as
u2 = P + uι = P +(1 - exp -Λ(τι∣Hj))
F.2 Discussions about CIE
Besides, for CIE, we provide some discussions here. Given the density function, we have
f (t|Hj) = λ(tlHj) ∙ exP(- Z λ(THj))dτ
tj
then take the derivative of f w.r.t. t for RMTPP, we have
f 0(t∣Hj) = λ0(t∣Hj) ∙ exp(- Z λ(τ|Hj)dτ) - λ2(t∣Hj) ∙ exp(- Z λ(τHj)dτ),
tj	tj
where λ(t∣Hj-) = exp (lj + w(t - tj)) for RMTPP And We can get that λ0(t∣Hj∙) = W ∙ λ(t∣Hj∙),
thus we have :
f(tHj) = f (t|Hj) ∙ (w - λ(t∣Hj)) = f(tHj) ∙ (w - exp (lj + w(t - tj)))	(29)
We rewrite Eq. 12 here for simplicity:
tu + 1 = tj +
log exp(lj) - wlog(1 - u) - lj
w
By Eq. 29, if w < 0, then f0 (t|Hj) < 0. It means that the probability decrease with the increment of
t. So given a fixed probability P or the significance α(note P = 1 - α), the most compact interval
must be [tj+ι,^j+La)] where tj+ι = tj and O。) can be calculated given significance α:
bt(1-α)
tj+1
tj +
log exp(lj ) - wlog(α) - lj
w
(w < 0).
(30)
Then we can test the model by its theoretical probability and statistical frequency that
vCIE(α)
J 1 (tj+1 ∈ (⅞)ι⅜a)])
(31)
J
However, for w > 0, f (t|Hj)0 > 0 if w > exp (lj + w(t - tj)) else f (t|Hj)0 < 0. It means the
probability that an events occur will increase first and decrease after t > lj + (log(w) - lj )/w. So
we cannot calculate the compact interval directly.
Here we give simple experimental results of CIE and QIE with Hawkes simulation data as shown in
Fig. 7. Given the significance level α, the frequency of real next event in the interval are shown in
Fig. 7 an thus we can see the rationality of CIE and QIE, where the frequency is close to 1 - α. Note
that the deviation of CIE is larger than QIE, which shows the expressive limitation of RMTPP.
15
Under review as a conference paper at ICLR 2022
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
G Algotithm in Sec. 4
Algorithm 1: MSE-guided adaptive predictions by calculating adaptive value
Input: the training data {tt1r , tt2r , . . . , ttNr }, the testing data {tt1e , tt2e , . . . , ttMe }, and the trained
parameters of NTPP model
Output: the adaptive value uada and the corresponding predicted sequences on testing data
^te ^te	^te
{t2 , t3 , . . . , tM+1}
Initialize u1 = 0.1, u2 = 0.9, δ = 0.001 and = 0.001
calculate dRMSEι = Pj=I (tj% - %ι(uι))2 - Pj=I (tj% - ^%(uι + δ))2
calculate dRMSE2 = P= (tj% - t‰(u2))2 - Pj= (tj% - t‰(u2 + δ))2
/* Here We can check that dRMSEι ∙ dRMSE2 < 0	*/
U =(uι + u2)∕2
while u2 - u1 < do
CaICUIate dRMSEm = Pj=I (tj+ι - j+1(U))2 - Pj=I (tj+l - j+I(U + S))2
if dRMSE1 ∙ dRMSEm, < 0 then
u2 = U
L dRMSE2 = dRMSEm
else if dRMSE2 ∙ dRMSEm, < 0 then
uι = U
L dRMSE1 = dRMSEm
else
L break
_ U = (ui + U2)∕2
the adaptive variable Uada = U
for j = 1, 2, . . . , M do
|_ calculate jr[=舞1 (Uada)
H	Accuracy results of type prediction
We further explicitly write the full multi-dim version for event type prediction (Mei & Eisner, 2017),
which will be added in new version:
Uj+1
∞ λu(t∣Hj) #
argmaxL WHTf(t|Hj)dt，
For event type prediction, it uses the same sample of t values for each k in the argmax, which reduces
sampling variance and also lets the model share the λu(t∣Hj) and f (t|Hj) computations across all
dimensions U. The above description is also generally discussed in Section 4 of [12], which is not the
focus of this paper (as our prediction improvement is concerned with event time).
Here we compare the event type prediction accuracy on ATM and StackOverflow datasets (we also
add another popular MIMIC dataset), and our method EP-NTPP slightly outperforms the state-of-
the-art methods, which we think is owing to the expressive modeling part. Note our sampling-based
prediction part relates only to time prediction and has nothing to do with type prediction. However,
the expressive modeling part is related to likelihood fitting, which directly improves NLL, but not
necessarily on event type prediction.
Table 3: ACC Results for real-world data.
Datasets	RMTPP	LogNormMix	FullyNN	NHP	EP-NTPP (ours)
ATM	87.58%	89.15%	89.42%	89.23%	89.94%
NYSE	64.32%	64.32%	64.54%	64.22%	64.54%
stackoverflow	45.25%	46.54%	46.55%	46.59%	46.72%
MIMIC	95.28%	94.79%	96.28%	96.52%	96.30%
16
Under review as a conference paper at ICLR 2022
I Training Details and Error Bars
In this appendix, we explain the details of experiments, including data preprocessing, hyperparameter
selection, and error bars of the experiments in Sec. 6.
As mentioned in Sec. 6, in the data preprocessing stage, we do two things, we apply Min-max
normalization on time interval values and utilize median absolute deviation(MAD) based anomaly
detection method to split event sequences. In this setting, the order of magnitude of each dataset is
unified in the range of 0 to 1 and the abnormal large time intervals are eliminated.
The routine of median absolute deviation(MAD) based anomaly detection algorithm is shown as:
Algorithm 2: Median Absolute Deviation (MAD) to find outliers
1
2
3
4
5
6
7
Input: the unsplit time interval sequence seq = {t1, t2, . . . , tN}, the scaling factor n.
Output: indices of outliers in sequence.
Function MAD(seq, n):
median = get_median(seq)
deviations = get_abs(seq - median)
mad = get_median(deviations)
indices = find(get_abs(seq - median) < n * mad)
return indices
We implement our model EPI-NTPP with PyTorch. We use a single-layer LSTM as the RNN encoder,
we set its hidden size to be 64. We use ReLU as the activation function of the intensity function
network, the hidden size of this MLP module is also set to be 64. In the input layer, the marker
embedding size is set to be 16. Mini-batch size is set to be 16 and Adam algorithm with learning rate
0.001 is used for optimization.
All the NTPP models including EPI-NTPP run on a single RTX-2080Ti (11GB) GPU and ADM4
(which we use as a baseline for infectivity matrix estimation in Sec. 6) runs on a core of Intel i9-7920X
CPU @ 2.90GHz with 128GB RAM.
As shown in Fig. 8, the error bars shows that our model outperforms others especially in the aspect of
expressiveness, a lower NLLLoss indicates that the intensity function we modeled captures more
complicated patterns to fit the dataset.
J	Details for Table 1
Comparison on Expressiveness. As is known, the intensity of RMTPP is exponential form and thus
the expressiveness is limited. NHP and AttentionPP adopt the intensity with a link function after the
exponential form, which improve the robustness. LogNormMix use the Gaussian-form density, which
is not a exponential form-based model. However, as discuss in Appendix A, given any parameters of
LogNormMix, it satisfies that λ(t∣Hj) → 0, which results the failure of modeling some special TPP
model such as Poisson and self-correcting TPP. Besides, EPI-NTPP and FullyNN are both NN-based
intensity without a specified form theoretically. The difference is that FullyNN models on intensity
integral (also called intensity measure), while EPI-NTPP models on the intensity directly.
Comparison on Predictability. As discussed in Sec. 3.1 and Appendix C, the expectation will
suffer from biased problem due to inadequate integral for some NTPP model such as RMTPP, NHP,
FullyNN and etc. Compared with them, LogNormMix is a non-terminating model which design the
density directly and thus overcome this baised problem. Besides, the method of P-NTPP is also a
strategy to solve this problem.
Comparison on Interpretability. Among the methods given in Table 1, only AttentionPP and our
EPI-NTPP gives some results of interpretability. The difference is that AttentionPP is based on
attention score to evaluate the influence, while our methods is based on the difference of intensity,
which is a probabilistic based method. Note we can get some interpretable results with RMTPP in a
sense. w in Eq. 4 can interpret by inhibiting (w > 0) or triggering(w < 0) the occurrence of future
events given the past.
17
Under review as a conference paper at ICLR 2022
Median
Adaptive
Expectation
NLLLoss
SgeH
nJ
∩u
nJ
,^U
< < < <
6u=3allo□ηMas
"MaUaH
-α∙ss
1.00
LB
1.11
1.15-
LR-
LSS-
MW-
O-JSO-
0.22S-
o.»o-
».»5-
O-ISO-
0.125-
o∙ιαo-
0.075-
O2R
o:aa
tx≈
028
uιs
0.18
o∙ιa
o.m
OOR
lɪɪɪ
RMw IMF rWhHH UoMimMIx ENΠW
RMTTP NI* FaIyIM UltfivnlMx ENTW
RMTTP	IMF	WMIN ENTW
-1.1
-1.1
-IZ
-IZ
Tl ɪ
ɪ ɪ
TInHT
gwM(ɑl*αl2waoa
向 R 3 3 2 R 4
* 必 W "UH必 W 即
MOμJ3>o *ues
1^u
- O

∣wπp	IMF	RjIyfM	0∙W1W	RMlW	IMF	FdIyNH ENTW	RNTFP IMF FdIyNH LoqMimMIx ENΠW	RMlW NI* Fq⅜IM UlWElMX ENTW
Figure 8: Error bars for RMSE results and negative log-likelihood on synthetic and real-world
datasets. Median, adaptive and expectation predictions are shown in first three columns and the
last column is for NLLLoss. These error bars are statistically obtained by testing models on a large
number of different batches of data.
K A Simple Modification of RMTPP
As discussed in the end of Sec. 5, we modify the intensity for RMTPP Eq. 4 as follows,
λ(t∣Hj) = exp (lj + w(t - tj)) + |c|	(32)
where |c| > 0 is the base intensity and Ij = v> ∙ hj + b is the accumulative past influence based on
RNN. This leads to the following intensity measure,
Λ(t∣Hj) =	— exp(lj	+ w(t -	tj))	—	- exp(lj)	+	|c| ∙	(t - tj)	(33)
When t → ∞, the intensity measure Λ(tj, t) → ∞ for all values of— because |c|(t - tj) → +∞
and 1 exp(Jj + w(t - tj)) > 0. Then the cumulative density function
F (t|Hj) = exp(-Λ(t∣Hj))	(34)
18
Under review as a conference paper at ICLR 2022
tends to 1 when t → +∞, which solve the inadequate integral problem of terminating problem. Thus
the modified RMTPP is a sound NTPP as a non-terminating TPP model. Then the conditional density
function for next event can be written as
f (t|Hj) =( exp (lj +	w(t	-	tj))	+ |c|)	∙ exp	exp(j)	— - exp(j	+ w(t	-	tj))	- |c|(t - tj)).
(35)
Thus we can estimate the expectation of next time as a time prediction with Eq. 9 with adequate
integral.
Besides, the modification can expand to multi-dimensional case, which can be specified as
λd(t∖Hj) = exp (ld + w(t - tj)) + |cd|	(36)
where d = 1, 2, . . . , D and ljd = v>d ∙ hj + bd with the trainable parameter v:,d and bd. Then we
show the method of estimating 1D a,nd MD Hawkes parameters with the modified model.
With the modification above, one can estimate the parameters of Hawkes process while training.
According to Eq. 32, one can easily find that the parameters of modified RMTPP can be in line with
Hawkes process
(μ = |4
β β = -w,	(37)
I α = ιP __________eχp (Ij)_____
I	n 乙j Pk=I eχp(w∙(tLtk)),
Fig. 5(a) is the result of evaluating 1D Hawkes parameters while training modified RMTPP. we can
ʌ
find that μ, β, α can get the convergence to real parameters. Note the parameter α is oscillatory
because of different batch inputs.
For a multi-dimensional case, the muti-Hawkes with its intensity can be specified as
λd(t∣Hj) = μd + E αddje-β(t-tk),	(38)
tk<t
Then μd and β is easily estimated as shown in Eq. 37. However, for the estimation of the matrix
of {αdd0 }, i.e. infectivity matrix, there exist several methods. In addition to the influence-based
estimation proposed by Eq. 19 in the paper, we can also estimate from another view, that is, the least
square method. Based on Eq. 36 and Eq. 38, we can get that
j
αddj ew(tj -tk) = exp (ljd - -tj)	(39)
k=1
which is the linear equation about αdd0 . Then linear regression can be used to estimate αdd0 based
on least square method. So we can estimated infectivity matrix which people may be interested in.
Fig. 5(b) shows the results of multi-dimensional case.
19