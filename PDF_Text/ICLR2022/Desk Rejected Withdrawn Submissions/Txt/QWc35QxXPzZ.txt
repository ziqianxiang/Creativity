The Power of Exploiter: Provable Multi-Agent RL
in Large State Spaces
Anonymous authors
Paper under double-blind review
Ab stract
Modern reinforcement learning (RL) commonly engages practical problems with large state spaces,
where function approximation must be deployed to approximate either the value function or the pol-
icy. While recent progresses in RL theory address a rich set of RL problems with general function
approximation, such successes are mostly restricted to the single-agent setting. It remains elusive
how to extend these results to multi-agent RL, especially in the face of new game-theoretical chal-
lenges. This paper considers two-player zero-sum Markov Games (MGs). We propose a new algo-
rithm that can provably find the Nash equilibrium policy using a polynomial number of samples, for
any MG with low multi-agent Bellman-Eluder dimension—a new complexity measure adapted from
its single-agent version (Jin et al., 2021). A key component of our new algorithm is the exploiter,
which facilitates the learning of the main player by deliberately exploiting her weakness. Our the-
oretical framework is generic, which applies to a wide range of models including but not limited to
tabular MGs, MGs with linear or kernel function approximation, and MGs with rich observations.
1	Introduction
Multi-agent reinforcement learning (MARL) systems have recently achieved significant success in many AI chal-
lenges including the game Go (Silver et al., 2016; 2017), Poker (Brown & Sandholm, 2019), real-time strategy games
(Vinyals et al., 2019; OpenAI, 2018), decentralized controls or multiagent robotics systems (Brambilla et al., 2013),
autonomous driving (Shalev-Shwartz et al., 2016), as well as complex social scenarios such as hide-and-seek (Baker
et al., 2020). Two crucial components that contribute to these successes are function approximation and self-play.
Function approximation is frequently used in modern applications with large state spaces, where either the value
function or the policy is approximated by parametric function classes, which are typically deep neural networks.
Meanwhile, self-play enables the learner to improve by playing against itself instead of traditional human experts.
Despite the empirical success of MARL, existing theoretical guarantees in MARL only apply to the basic settings
where the value functions can be represented by either tables (in cases where the states and actions are discrete) (Bai
& Jin, 2020; Bai et al., 2020; Liu et al., 2020) or linear maps (Xie et al., 2020; Chen et al., 2021). While a recent
line of works (Jiang et al., 2017; Wang et al., 2020; Yang et al., 2020; Jin et al., 2021; Du et al., 2021) significantly
advance our understanding of RL with general function approximation, and provide sample-efficient guarantees for
RL with kernels, neural networks, rich observations, and several special cases of partial observability, they are all
restricted to the single-agent setting. Distinct from single-agent RL, each agent in MARL are facing not only the
unknown environment, but also the opponents that can constantly adapt their strategies in response to the behavior of
the learning agent. This additional game-theoretical feature makes it challenging to extend the single-agent general
function approximation results to the multi-agent setting. This motivates us to ask the following question:
Can we design sample-efficient MARL algorithms for general function approximation?
By “sample-efficient”, we mean the algorithms provably learn within a polynomial number of samples that is indepen-
dent of the number of states. This paper provides the first positive answer to this question in the context of two-player
zero-sum Markov Games (MGs) (Shapley, 1953; Littman, 1994). In particular, we make the following contributions:
•	We design a new self-play algorithm for MARL—Golf_with_EXPLOITER. Our algorithm maintains a
main player and an exploiter, where the exploiter facilitates the learning of the main player by deliberately
exploiting her weakness. Our algorithm features optimism, and can simultaneously address multi-agent,
general function approximation, as well as the trade-off between exploration and exploitation.
1
•	We introduce a new general complexity measure for MARL—Multi-agent Bellman Eluder (BE) dimension,
which is adapted from its single-agent version (Jin et al., 2021). We prove that our algorithm can learn the
Nash equilibrium policies of any MARL problem with low multi-agent BE dimension, using a number of
samples that is polynomial in all relevant parameters, but independent of the size of the state space. We
further provide an online guarantee for our algorithm when facing against adversarial opponents.
•	We show that our theoretical framework applies to a wide range of models including tabular MGs, MGs
with linear or kernel function approximation, and MGs with rich observations. Formally, we prove that all
examples above admit low multi-agent BE dimension. Particularly, for linear MGs, We obtain a O(H2d2∕e2)
sample complexity bound for finding -approximate Nash policies, which improves upon the cubic depen-
dence on dimension d in Xie et al. (2020).
We remark that our algorithm is sample-efficient but not computationally efficient. Designing computationally efficient
algorithms in the context of general function approximation is an open problem even in the single-agent setting (Jiang
et al., 2017; Jin et al., 2021; Du et al., 2021), which we left as an interesting topic for future research.
Technical Challenges One of the most important questions in the design of RL algorithms is the choice of behavior
policies—the policies that are used to collect samples. For a given general function class F, the single-agent RL algo-
rithms simply use behavior policy in the form ofπf, which is the greedy policy with respect to certain optimistic value
function f ∈ F, i.e., πf (s) = argmaxa f(s, a). The choice becomes more sophisticated in the two-player setting. All
existing algorithms that can perform provably efficient exploration (Bai & Jin, 2020; Bai et al., 2020; Liu et al., 2020;
Xie et al., 2020; Chen et al., 2021) choose the behavior policy as the general-sum Nash equilibrium or coarse corre-
lated equilibrium of two value functions f, g ∈ F , which are the upper and lower bound of the true value functions
respectively. It turns out that this approach is only effective under optimistic closure—a strong assumption which is
true for tabular and several special linear settings, but does not hold for many practical applications (see Appendix
B for more discussions). In contrast, our exploiter framework provides a new approach for the theory community to
choose behavior policies—the main agent plays the optimistic Nash policy based on the existing knowledge while the
exploiter focuses on exploiting the main agent. Our approach does not rely on strong assumptions such as optimistic
closure. We hope our exploiter framework could facilitate the future development of provable MARL algorithms.
Related Works Due to space limit, we defer a detailed survey of related works to Appendix A.
2	Preliminaries
Markov Games (MGs) are the generalization of standard Markov Decision Processes into the multi-player setting,
where each player aims to maximize her own utility. In this paper, we consider two-player zero-sum episodic MG,
formally denoted by MG(H, S, A, B, P, r), where H is the number of steps per episode, S is the state space, A and B
are the action space for the max-player and the min-player respectively, P = {Ph}h∈[H] is the collection of transition
measures with Ph (∙ | s,a, b) defining the distribution of the next state after taking action (a, b) at state S and step h,
r = {rh}h∈[H] is the collection of reward functions with rh(s, a, b) equal to the reward received after taking action
(a, b) at state s and step h.1 Throughout this work, we assume the reward is nonnegative, and PhH=1 rh(sh, ah, bh) ≤ 1
for all possible sequence (s1, a1, b1, . . . , sH, aH, bH).
Each episode starts from a fixed initial state s1. At each step h ∈ [H], two players observe sh ∈ S and choose their
own actions ah ∈ A and bh ∈ B simultaneously. Then, both players observe the actions of their opponents and receive
reward rh(sh, ah bh). After that, the environment will transit to sh+ι 〜Ph,(∙ | sh ah bh). Without loss of generality,
we assume the environment always transit to a terminal state send at step H + 1 which marks the end of this episode.
Policy, value function A Markov policy μ of the max-player is a collection of vector-valued functions {μh, : S →
∆A}h∈[H] , each mapping a state to a distribution in the probability simplex over A. We use the ath coordinate of
μh(S) to refer to the probability of taking action a at state S and step h. Similarly, we can define a Markov policy V
over B for the min-player.
1In this paper, we study deterministic reward functions for cleaner presentation. All our results immediately generalize to the
setting with stochastic reward.
2
We denote Vμ,ν : S → R as the value function for policy μ and V at step h, with Vμ,ν(S) equal to the expected
cumulative reward received under policy (μ, V), starting from state S at step h until the end of the episode:
H
Vμ,ν(s)：= Eμ×ν Xrt(st,at,bt) | Sh = S
t=h
Similarly, We denote Qh,ν ： S ×A×B → R as the action-value function for policy μ and V at step h:
Qh， (s, a, b) ：= Eμ×ν [Pt=h Irt(St, at, bt) | sh = s, ah = a, bh = b .
With slight abuse of notation, we use Ph as an operator so that [PhV](s, a, b) := Es，〜Ph(∙∣s,a,b)V(s0) for any value
function V. We also define [D∏Q](s) := E(a,b)〜∏(∙,∙∣s)Q(s, a, b) for any policy π and action-value function Q. By the
definition of value functions, we have the Bellman equations:
Qμ,ν (s,a,b) = [rh + PhVμ+1](s,a,b), vhμ,ν (S) = [Dμh×Vh Qμ,ν](S)
for all (s, a, b, h) ∈ S ×A×B × [H ]. And for step H + 1, we have VH；i ≡ 0.
Best response, Nash equilibrium For any policy of the max-player μ, there exists a best response policy of the min-
player ν*(μ) satisfying vμ,ν↑(μ) = infV Vμ,ν(s) for all (s, h). For cleaner notation, we denote Vμ,^ := /，”⑷.
Similarly, we can define μ1(ν) and Vj^,ν. It is known (Filar & Vrieze, 2012) that there exists policies μ?, V? that are
optimal against the best responses of the oponents, i.e.,
VT*%) = suPμ Vμ%),	VF* (s) = inf V VhnV (s),	forall(s,h).
We call these optimal policies (μ?, V?) a Nash equilibrium of the Markov game, which are further known to satisfy
the following minimax equation:
supμinfv Vμ,ν(s) = Vμ ,ν (s) = infV supμ Vμ,ν(s), for all (s, h).
We call the value functions of	(μ?, V?)	Nash value functions, and abbreviate Vhμ	,v as	Vh	and	Qμ	,v	as Qh.	Intu-
itively speaking, in a Nash equilibrium, no player can benefit by unilaterally deviating from her own policy.
Learning objective We say a policy μ of the max-player is an e-approXimate Nash policy if Vμ,^(s1)≥ V1?(s1)-.
Suppose an agent interacts with the environment for K episodes, and denote by μk the policy executed by the max-
player in the kth episode. Then the (cumulative) regret of the max-player is defined as
K
Reg(K) := X H(SI)- V1μk,t(s1)].
k=1
The goal of reinforcement learning is to learn e-approximate Nash policies or to achieve sublinear regret. In this paper,
we focus on learning the Nash policies of the max-player. By symmetry, the definitions and techniques directly extend
to learning the Nash policies of the min-player.
2.1	Function Approximation
This paper considers reinforcement learning with value function approximation. Formally, the learner is provided with
a function class F = Fi × ∙∙∙ × FH where each Fh ⊆ (S × A × B → [0,1]) consists of candidate functions to
approximate some target action-value functions at step h. Since there is no reward at step H + 1, we set fH +1 ≡ 0 for
any f ∈ F without loss of generality.
Induced policy and value function For any value function f ∈ F, we denote by μf = (μf,ι,..., μf,H) the Nash
policy of the max-player induced by f, where
μf h(s) = argmax min μ>fh(s, ∙, ∙)v for all (s, h).
，	μ∈∆A v∈δb
3
Moreover, we denote by Vf the Nash value function induced by f, so that
Vfh(S)= max min μ>fh(s, ∙, ∙)ν for all (s,h).
J,	μ∈∆A V∈Δb'
We further generalize the concept of induced value function by additionally introducing a superscript μ (which repre-
sents a Markov policy of the max-player) and define VfX so that
Vf,h(s) = minν∈∆B μh(s)> fh(s, ∙, ∙)ν for all (s, h).
Based on Vf and Vfμ, two types of Bellman operators are defined as below
ʃ (Thf )(s,a, b) =rh(S,α,b) + Es0〜Ph(∙∣s,a,b)Vf,h+1(SO)
[(Thlf )(s,a, b) :=rh(S,a, b) + Es0〜Ph(∙∣s,a,b)Vfμh+1(SO)
(1)
which naturally generalize the optimal Bellman operator and the policy Bellman operator from MDPs to MGs. In the
remainder of this paper, We will refer to them as the Nash Bellman operator and μ-Bellman operator, respectively.
It is known that RL with function approximation is in general statistically intractable without further assumptions (see,
e.g., hardness results in Krishnamurthy et al. (2016); Weisz et al. (2020)). Below, we present two assumptions that are
generalizations of commonly adopted assumptions in MDP literature.
Assumption 1 (εreal-Realizability). For any f ∈ F and h ∈ [H], there exist Qh , Q0h ∈ Fh s.t. kQ?h - Qh k∞ ≤ εreal
and kQμf,t - Qhk∞ ≤ εreal.
Realizability requires the function class to be well-specified so that the value function of the Nash policy and the value
function of any induced policy μf against its best response lie inside F approximately.
Let G = Gi × ∙∙∙×Gh bean auxiliary function class provided to the learner where each Gh ⊆ (S×A×B → [0,1]).
Completeness requires the auxiliary function class G to be rich enough so that applying Bellman operators to any
function in the primary function class F will end up in G approximately.
Assumption 2 (εcomp-Completeness). For any f, f0 ∈ F and h ∈ [H], there exist Qh, Q0h ∈ Gh s.t. kThfh+1 -
Qhk∞ ≤ εcomp and ∣∣Thμf f0 - Qhk∞ ≤ ε,
comp .
In the single-agent setting (Jin et al., 2021), the realizability and completeness conditions are stated for the special case
εreal = εcomp = 0, while this paper considers a strictly more general condition. This extension makes it possible to
handle the misspecified setting, i.e., F and G only satisfy these conditions approximately, but not exactly. Even when
F and G satisfy these conditions exactly, the above extension can still help us avoid some technical redundancy caused
by the possible infinite cardinality of F and G, by only considering their finite coverings. To be precise,
Definition 1 (-Covering). The function class F ⊆ F is an -covering of F if for any f ∈ F, there exist f ∈ F s.t.
∣fh - fh∣∞ ≤ , for any step h.
The following lemma implies that we can always restrict our attention to finite coverings, which also approximately
satisfy the realizability and completeness conditions up to satisfying accuracy.
Lemma 2. If F and G satisfy εreal-Realizability and εcomp-Completeness, and F and G are -coverings of F and G
respectively, then F and G satisfy (εreal + )-Realizability and (εcomp + )-Completeness.
As a result, we only need to consider finite function class F and G in Section 3. Some of the examples introduced in
Section 4 involve infinite function classes. To address this inconsistency, we can first compute the finite coverings of
F and G, which satisfy the approximate realizability and completeness conditions by Lemma 2, and then invoke the
algorithms in Section 3 with the coverings instead of the original function classes. It is straightforward to verify that
by replacing the cardinality with the covering number, all theoretical guarantees derived in Section 3 still hold.
Finally, we define the L∞ Projection Operator, which will be frequently used in our technical statements and proofs.
Definition 3 (L∞ Projection Operator). For any function class F and function g, PF (g) = argminf∈F ∣f - g∣∞.
3 Main Results
In this section, we present an optimization-based algorithm Golf_with_Exploiter, and its theoretical guarantees
for any multi-agent RL problem with low Bellman-Eluder dimension.
4
3.1	Algorithm
We describe Golf_with_Exploiter in Algorithm LAta high level, Golf_with_EXPLOITER follows the principle
of optimism in face of uncertainty, and maintains a global confidence set for Nash value function Q? based on local
constraints. It extends the single-agent Golf algorithm (Jin et al., 2021) by introducing an exploiter subroutine—
Compute .Exploiter, which facilitates the learning of the main player (the max player) by continuously exploiting
her weakness.
In each episode, Golf_with_Exploiter performs four main steps:
1.	optimistic planning (Line 3): compute the value function f k that induces the most optimistic Nash value
from the current confidence set C, and choose μk to be its induced Nash policy of the max-player.
2.	Finding exploiter (Line 4): compute an approximate best response policy νk for the min-player, by invoking
the subroutine Compute .Exploiter on historical data D and the policy of the max-player μk.
3.	Data collection (Line 7-8): we provide two different options for the Sampling procedure
(I)	Roll a single trajectory by following (μk, Vk).
(II)	For each h, roll a trajectory by following (μk, Vk) in the first h - 1 steps and take uniformly random
action at step h. This option will roll H trajectories each time, but in the hth trajectory, only the hth
transition-reward tuple is augmented into Dh .
4.	Confidence set update (Line 9): update confidence set C using the renewed dataset D.
The core components of Golf_with_Exploiter are the construction of the confidence set and the subroutine Com-
PUTE-Exploiter, which We elaborate below in sequence.
Confidence set construction For each h ∈ [H], Golf_with_Exploiter maintains a local constraint using the
historical data Dh at this step
LDh(fh,fh+1) ≤ inf LDh (g, fh+1) + β,	(2)
g∈Gh
where the squared loss LDh is defined as
LDh(ξh,ζh+1) :=	X	ξh(s,a,b) -r-Vζ,h+1(s0)2.	(3)
(s,a,b,r,s0)∈Dh
intuitively speaking, LDh is a proxy to the squared Bellman error under the Nash Bellman operator at step
h. Unlike the classic Fitted Q-Iteration algorithm (FQI) (Szepesvari, 2010) where one simply updates fh J
argminφ∈Fh LDh (φ, fh+1), our constraints implement a soft version of minimization so that any function with loss
slightly larger than the optimal loss gets preserved. By making this relaxation, we can guarantee the true Nash value
function Q? contained in the confidence set C with high probability.
Exploiter computation The main challenge of learning MGs compared to learning MDps lies in the choice of
behavior policies as discussed in Section 1. Motivated by the empirical breakthrough work AlphaStar (Vinyals et al.,
2019), we adapt the methodology of exploiter. Specifically, we design the Compute .Exploiter subroutine that
approximately computes a best-response policy Vk against the policy of the max-player μk. By executing Vk, the min-
player exposes the flaws of the max-player, and helps improve her strategy. The pseudocode of Compute .Exploiter
is given in Algorithm 2, which basically follows the same rationale as Algorithm 1 except that we change the regression
target by replacing Vf with Vμ, because the confidence set Cμ is constructed for the best-response value function Q*J
instead of the Nash value function Q? . Formally, Algorithm 2 adapts a new square loss function defined below
LDh (ξh,Zh+ι):=	X	[ξh(s,a,b) - r - ‰ι(s0)]2.	(4)
(s,a,b,r,s0)∈Dh
Finally, we remark that the optimistic planning and exploiter computation steps are computationally inefficient in
general. Designing computationally efficient algorithms in the context of general function approximation is an open
problem even in the single-agent setting (Jiang et al., 2017; Jin et al., 2021; Du et al., 2021), which we left as an
interesting topic for future research.
5
Algorithm 1 Golf with Exploiter (F, G, K, β)
1:
2:
3:
4:
5:
6:
7:
8:
9:
Initialize: Di,..., DH — 0, CJF.
for episode k from 1 to K do
Choose policy μk = μ,k, where fk = argmaxʃ∈c Vf,1(s1). Let Vk = Vfk,ι(sι).
(νk, Vk) J COMPUTE_EXPLOITER(F, G, β, D, μk).
if Vk - Vk < ∆ then
Output μout = μk.
Collect samples
Option I execute policy (μk, νk) and collect a trajectory (si, ai, bi, ri,..., SH, aH, bH,『h , SH+1).
Option II for all h ∈ [H], execute policy (μk, Vk) at the first h 一 1 steps, take uniformly random action
at the hth step, and collect a sample (Sh, ah, bh, rh, Sh+i).
Augment Dh = Dh ∪ {(Sh, ah, rh, Sh+i)} for all h ∈ [H].
Update
C = f ∈F : LDh (fh,fh+1) ≤ g∈* 1Gh LDh (g，fh+1) + β for all h ∈ H]，
Algorithm 2 COMPUTE ExploiTER(F, G, β, D, μ)
1:	Construct
2:	Compute f J
Cμ = f ∈F : LDh (wh,wh+i) ≤ inf LDh (g,wh+ι)+ β for all h ∈ H
h	g∈Gh	h
argminʃ ∈c Vfi(Si) and V =耳心，
/Tr* ,	/ Cllt τ r∖ ι.ι	Cllt / ∖	♦	/ ∖ ^T 3 /	∖ r 11	7
3: Return (Vout,V) such that VoUt(S) = argminν∈∆B μh(s)> fh(s, ∙, ∙)ν for all s, h.
3.2 Complexity Measure
In this subsection, we introduce our new complexity measure—multi-agent Bellman-Eluder (BE) dimension, which
is generalized from its single-agent version (Jin et al., 2021). we will show in Section 4 that the family of low BE
dimension problems includes many interesting multi-agent RL settings, such as tabular MGs, MGs with linear or
kernel function approximation, and MGs with rich observation.
we start by recalling the definition of distributional Eluder (DE) dimension (Jin et al., 2021).
Definition 4 (e-independence between distributions). Let G be a function class defined on X, and ν,μi,... ,μn be
probability measures over X. We say V is e-independent of {μi, μ2,..., μn} with respect to G if there exists g ∈ G
such that √Pn=i(Eμi [g])2 ≤ e, but |E"[g]| > e.
Definition 5 (Distributional Eluder (DE) dimension). Let G be a function class defined on X, and Π be a family of
probability measures over X. The distributional Eluder dimension dimDE(G, Π, e) is the length of the longest sequence
{ρi, . . . , ρn} ⊂ Π such that there exists e0 ≥ e where ρi is e0-independent of {ρi, . . . , ρi-i} for all i ∈ [n].
DE dimension generalizes the original Eluder dimension (Russo & Van Roy, 2013) from points to distributions. The
main advantage of this generalization is that for RL problems with large state space, it is oftentimes easier to evaluate
a function with respect to certain distribution family than to estimate it point-wisely. And for the purpose of this paper,
we will focus on the following two specific distribution families.
1. Dδ := {D∆,h}h∈[H], where D∆,h = {δ(s,a,b) (∙) | (s, a, b) ∈ S ×A× B}, i.e., the collections of probability
measures that put measure 1 on a single state-action pair.
2. DF := {DF,h}h∈[H], where DF,h denotes the collections of all probability measures over S × A × B at the
hth step, which can be generated by executing some (μ∕, Vf,g) with f,g ∈ F. Here Vf,g is the best response
to μf regarding to Q-ValUe g:
Vf,g,h(s) := argminμf,h(s).gh(s, ∙, ∙)ν forall (s, h).
ν∈∆B
6
Now, we are ready to introduce our key complexity notion—multi-agent Bellman-Eluder (BE) dimension, which
is simply the DE dimension on the function class of Bellman residuals, minimizing over the two aforementioned
distribution families, and maximizing over all steps.
Definition 6 (Bellman-Eluder dimension). Let HF be the function classes of Bellman residuals where HF,h :=
{fh - Tμg fh+ι | f,g ∈ F}. Then the Bellman-Eluder dimension is defined as
dimBE (F, ) = max min	dimDE(HF h, Dh, ).	(5)
BE	h∈[H] D∈{D∆,DF}	DE	F,h	h
We remark that the Bellman residual functions used in Definition 6 take both state and action as input. By choosing
an alternative class of Bellman residuals defined over the state space only, we can similarly define a variant of this
notion—V-type BE dimension. For clean presentation, we defer the formal definition to Appendix C.
3.3	Theoretical Guarantees
Now we are ready to present the theoretical guarantees.
Theorem 7 (Regret of Golf_with_Exploiter). UnderAssumption 1 and 2, there exists an absolute Constant C such
Ihatfor any δ ∈ (0,1], K ∈ N, ifwe choose β = c ∙ (log(KH∣F∣∣G∣∕δ) + Kε20mp + 底篙),then with probability at
least 1 一 δ, for all k ∈ [K], Algorithm 1 with Option I satisfies: Reg(k) ≤ O(H/dkβ), where d = dimBE(F, 1/K)
is the BE dimension.
Theorem 7 claims that Golf_with.Exploiter can achieve √K regret on any RL problem with low BE dimension,
under the realizability and the completeness assumption. Moreover, the regret scales polynomially with respect to
the length of episode, the BE dimension of function class F, and the log-cardinality of the two function classes. In
particular, itis independent of the size of the state space, which is of vital importance for practical RL problems, where
the state space is oftentimes prohibitively large.
By pigeonhole principle, we also derive a sample complexity guarantee for Golf_with_EXPLOIter.
Corollary 8 (Sample complexity of Golf_with_EXPLOIter). UnderAssumption 1 and 2, there exists absolute con-
stants c, c0 such thatforany e > 0, choose β = c∙(log(KH∣F∣∣G∣∕δ)+Kε2omp+Kε2eai) and∆ = C(Hyde/K+e),
where d = dimBE (F, /H) is the BE dimension ofF,then with probability at least 1 - δ, the output condition (Line 5)
will be satistied at least once in the first K episodes. Furthermore, the output policy μout ofAlgorithm 1 with Option I
is O(e + H√d(εreai + εcomp))-approximate Nash, if K ≥ Ω((H2d∕e2) ∙ log(H∣F∣∣G∣d∕e)).
Corollary 8 asserts that GOLF_WITH_EXPLOITER can find an Ο(e)-approximate Nash policy for the max-player using
at most O(H2 log(|F||G|)dimBE(F, e)∕e2) samples, which scales linearly in the BE dimension and the log-cardinality
of the function classes. Since our ultimate goal is to compute an approximate Nash policy, one can early terminate
Algorithm 1 once the output condition (Line 5) is satisfied. We remark that by symmetry Golf_with.Exploiter
can also compute an O(e)-approximate Nash policy for the min-player using the same amount of samples.
Remark 9 (Generalization to function classes with infinite cardinality). Although Theorem 7 and Corollary 8 are
derived for finite function classes, they can also apply to infinite function classes with bounded '∞-COvering number
as commented below Lemma 2. For example, they hold for linear function classes, with the log-cardinality replaced
by the log-covering number.
we also derive similar sample complexity guarantee in terms of V-type BE dimension for Algorithm 1 with Option II,
which can be found in Appendix C.
3.4	Adversarial Opponents
So far we have been focusing on the self-play setting, where the learner can control both players and the goal is to learn
an approximate Nash policy. Another setting of importance is the online setting, where the learner can only control
the max-player, and the goal is to achieve high cumulative reward against the adversarial min-player. Intuitively, it is
reasonable to expect GOLF_WITH_EXPLOITER could still work in the online setting, because Theorem 7 demonstrates
that it can achieve sublinear regret competing against the exploiter, which is the strongest possible adversary. This is
indeed the case. The only place we need to change in Algorithm 1 is Line 4: instead of computing νk using Algorithm
7
2, we let the adversary pick policy νk. Before stating the theoretical guarantee, we introduce an online version of
Bellman Eluder dimension.
Definition 10 (Online Bellman-Eluder dimension). Let HF be the function classes of Bellman residuals where
HF,h := {fh - Thfh+1 | f ∈ F}. Then the online Bellman-Eluder dimension is defined as:
dimOBE (F, ) = max dimDE(HF,h, D∆,h, ).	(6)
Compared to the BE dimension in Definition 6, this online version uses a smaller function class that includes only the
residuals with respect to the Nash Bellman operator. Another difference is the choice of distribution family is now
limited to only D∆ because the learner cannot control the policy of the min-player in the online setting. Now, we are
ready to present the theoretical guarantee.
Theorem 11 (Regret against adversarial opponents). Assuming kQ?h - PFh (Q?h)k ≤ εreal and kThfh+1 -
PGh (Thfh+1)k ≤ εcomp for all f ∈ F and h ∈ [H], there exists an absolute constant c such that for any δ ∈ (0, 1],
K ∈ N, if we choose β = C ∙ (Tog(KH ∣F∣∣G∣∕δ) + Kε2omp + Kε2eai), then With probability at least 1 一 δ, for all
k ∈ [K ], Algorithm 1 with Option I satisfies Pk=i[%?(si)—Vf W (si)] ≤ O(H √dkβ), where d = dim OBE (F, 1/K)
is the online BE dimension.
Theorem 11 claims that on any problem of low online BE dimension, Golf_with_Exploiter can achieve √K
regret against an adversarial opponent. Moreover, the multiplicative factor scales linearly in the horizon length, and
sublinearly in the online BE dimension as well as the log-cardinality of the function classes. Comparing to the self-
play regret guarantee, Theorem 11 holds under a weaker condition, but only guarantees Algorithm 1 can play favorably
against the adversary, instead of the best-response policy. This is unavoidable because if the adversary is very weak,
then it is impossible to learn Nash policies by playing against it.
4 Examples
In this section, we introduce five concrete multi-agent RL problems with low BE dimension: tabular MGs, MGs
with linear function approximation, MGs with kernel function approximation, MGs with rich observation, and feature
selection for kernel MGs, which generalize their single-agent versions (Zanette et al., 2020a; Yang et al., 2020; Krish-
namurthy et al., 2016; Agarwal et al., 2020) from MDPs to MGs. Except for tabular MGs, all above examples are new
and can not be addressed by existing works (xie et al., 2020; Chen et al., 2021).
Tabular MGs Starting with the simplest scenario, we show that MGs with finite state-action space has BE dimension
no larger than the size of its state-action space, up to a logarithmic factor.
Proposition 12 (Tabular MGs ⊂ Low BE dimension). Consider a tabular MG with state set S and action set A × B.
Then any function class F ⊆ (S × A × B → [0, 1]) satisfies
dimbe (F, e) ≤ O(|S × A × B| ∙ log(1 + 1∕e)).
Linear function approximation we consider linear function class F consisting of functions linear in a d-
dimensional feature mapping. Specifically, for each h ∈ [H], we choose Fh = {φh(∙, ∙, ∙)>θ | θ ∈ Bd(R)}
where φh : S × A × B → Bd(1) maps a state-action pair into the d-dimensional unit ball centered at the origin.
Assumption 3 (Self-completeness). Tμf F+i ⊂ Fh for any (f, h) ∈ F × [H].
Assumption 3 is a special case of Assumption 2 (completeness) by choosing auxiliary function class G = F. Assump-
tion 3 also implies Assumption 1 (realizability) by backward induction. Below, we show under self-completeness, the
BE dimension of F is upper bounded by the dimension of the feature mappings d up to a logarithmic factor.
Proposition 13 (Linear FA ⊂ Low BE dimension). Under Assumption 3,
dimbe (F, e) ≤ O(d ∙ log(1 + R∕e)).
we remark that our setting of linear function approximation is strictly more general than linear MGs (xie et al., 2020)
which further assumes both transitions and rewards are linear. The self-completeness assumption is automatically
satisfied in linear MGs. Prior works (xie et al., 2020; Chen et al., 2021) can only address linear MGs or its variant
but not the more general setting of linear function approximation presented here. Finally, for linear MGs, by com-
bining Proposition 13 and Corollary 8, we immediately obtain a O(H2d2∕e2) sample complexity bound for finding
e-approximate Nash policies, which improves upon the cubic dependence on dimension d in xie et al. (2020).
8
Kernel function approximation This setting extends linear function approximation from d-dimensional Euclidean
space Rd to a decomposable Hilbert space H. Formally, for each h ∈ [H], We choose Fh = {φh(∙, ∙, ∙)τθ | θ ∈
BH(R)} where φh : S × A × B → BH(1). Since the ambient dimension ofH can be infinite, we leverage the notion
of effective dimension and prove the BE dimension of F is no larger than the effective dimension of certain feature
sets in H.
Proposition 14 (Kernel FA ⊂ LoW BE dimension). Under Assumption 3,
dim be (F ,e) ≤ max deff (Xh ,e∕(2R + 1)),	(7)
h∈[H]
where Xh = {Eμ[φh(s%, ah, bh)] | μ ∈ DF,h} and deff is the effective dimension.
The e-effective dimension of a set Z is the minimum integer deff (Z, e) = n such that
1	1n
sup	— log det(I +——K	ZizT) ≤ e 1.
z1,...,zn∈Z n e2i=1i
For H = Rd, the effective dimension in the RHS of (7) is upper bounded by O(d) by following the standard ellipsoid
potential argument.
MGs with rich observation In this scenario, despite the MG has a large number of states, these states can be
categorized into a small number of “effective states”. Formally, there exists an unknown decoding function q : S →
[m] such that if two states s and s0 satisfy q(s) = q(s0), then their transition measures and reward functions are
identical, i.e., Ph(∙ | s, a,b) = Ph(∙ | s0, a, b), Ph(S | ∙,a,b) = Ph(s0 | ∙, a, b), and rh(s, a, b) = r%(s0, a, b) for all
(h, a, b). Here, the decoding function q is unknown to the learner, and the size of its codomain m is usually much
smaller than that of the state space.
Proposition 15 (MGs with rich observation ⊂ Low BE dimension). Consider an MG with decoding function q : S →
[m]. Then any function class F ⊆ (S × A × B → [0, 1]) satisfies
dimVBE(F, e) ≤ O(m ∙ log(1 + 1∕e)),
where dimVBE denotes the V-type BE dimension defined in Appendix C.
Kernel feature selection To begin with, we introduce kernel MGs, which is a special case of kernel function ap-
proximation introduced earlier in this section by additionally assuming that both transitions and rewards are linear in
RKHS. Concretely, in a kernel MG, for each step h ∈ [H], there exist feature mappings φh : S × A × B → H and
ψh : S → H where H is a decomposable Hilbert space, so that the transition measure can be represented as the inner
product of features, i.e., Ph(s0 | s, a, b) = hφh (s, a, b), ψh (s0)iH. Besides, the reward function is linear in φ, i.e.,
rh(s, a, b) = hφh(s, a, b), θhr iH for some θhr ∈ H. Here, φ, ψ and θr are unknown to the learner. Moreover, a kernel
MG satisfies the following regularization conditions: for all h, (a) kθhr kH ≤ 1, and (b) k Ps∈S V(s)ψh(s)kH ≤ 1 for
any function V : S → [0, 1].
In kernel feature selection, the learner is provided with a feature class Φ satisfying: (a) φ ∈ Φ, and (b) kφh(s, a, b)k2 ≤
1 for all (s, a, b, h) and φ ∈ Φ. Therefore, it is natural to consider the union of all linear function classes induced by
ɪ ɛi ♦ c ii i -r-	-r-	-r- i -r^	f∕7∕	∖ n∖ I n _ τ->	/ τ^∖	7 _ ɪ 1 ɪʌ ι	ι . ι
Φ. Specifically, let F := Fi X …X FH where Fh = {hφh(∙, ∙, ∙), Θ)h | θ ∈ Bh(R), φ ∈ Φ}. Below, we show the
V-type BE dimension of F is upper bounded by the effective dimension of certain feature sets induced by F.
Proposition 16 (Kernel feature selection ⊂ Low BE dimension). Let M be a kernel MG. Then
dimVBE (F, e) ≤ max deff Xh, e∕(2R + 1) ,
h∈[H]
where Xh = {Eμ[Φh(sh, ah, bh)] | μ ∈ Df,h}.
5 Conclusion
This paper presents the first line of sample-efficient results for Markov games with large state space and general
function approximation. We propose a new complexity measure—multiagent Bellman-Eluder dimension, and design
a new algorithm that can sample-efficiently learn any MGs with low BE dimension. At the heart of our algorithm is
the exploiter, which facilitates the learning of the main player by continuously exploiting her weakness. Our generic
framework applies to a wide range of new problems including MGs with linear or kernel function approximation, MGs
with rich observations, and kernel feature selection, all of which can not be addressed by existing works.
9
References
Alekh Agarwal, Sham Kakade, Akshay Krishnamurthy, and Wen Sun. Flambe: Structural complexity and representa-
tion learning of low rank mdps. Advances in Neural Information Processing Systems, 33, 2020.
Mohammad Gheshlaghi Azar, Ian Osband, and Remi Munos. Minimax regret bounds for reinforcement learning.
arXiv preprint arXiv:1703.05449, 2017.
Yu Bai and Chi Jin. Provable self-play algorithms for competitive reinforcement learning. In International Conference
on Machine Learning,pp. 551-560. PMLR, 2020.
Yu Bai, Chi Jin, and Tiancheng Yu. Near-optimal reinforcement learning with self-play. arXiv preprint
arXiv:2006.12007, 2020.
Bowen Baker, Ingmar Kanitscheider, Todor Markov, Yi Wu, Glenn Powell, Bob McGrew, and Igor Mordatch. Emer-
gent tool use from multi-agent autocurricula. In International Conference on Learning Representations, 2020. URL
https://openreview.net/forum?id=SkxpxJBKwS.
Christopher Berner, Greg Brockman, Brooke Chan, Vicki Cheung, Przemyslaw Debiak, Christy Dennison, David
Farhi, Quirin Fischer, Shariq Hashme, Chris Hesse, et al. Dota 2 with large scale deep reinforcement learning.
arXiv preprint arXiv:1912.06680, 2019.
Ronen I Brafman and Moshe Tennenholtz. R-max-a general polynomial time algorithm for near-optimal reinforcement
learning. Journal of Machine Learning Research, 3(Oct):213-231, 2002.
Manuele Brambilla, Eliseo Ferrante, Mauro Birattari, and Marco Dorigo. Swarm robotics: a review from the swarm
engineering perspective. Swarm Intelligence, 7(1):1-41, 2013.
Noam Brown and Tuomas Sandholm. Superhuman ai for heads-up no-limit poker: Libratus beats top professionals.
Science, 359(6374):418-424, 2018.
Noam Brown and Tuomas Sandholm. Superhuman ai for multiplayer poker. Science, 365(6456):885-890, 2019.
Qi Cai, Zhuoran Yang, Chi Jin, and Zhaoran Wang. Provably efficient exploration in policy optimization. arXiv
preprint arXiv:1912.05830, 2019.
Andrea Celli, Alberto Marchesi, Gabriele Farina, and Nicola Gatti. No-regret learning dynamics for extensive-form
correlated equilibrium. Advances in Neural Information Processing Systems, 33, 2020.
Zixiang Chen, Dongruo Zhou, and Quanquan Gu. Almost optimal algorithms for two-player markov games with
linear function approximation. arXiv preprint arXiv:2102.07404, 2021.
Christoph Dann and Emma Brunskill. Sample complexity of episodic fixed-horizon reinforcement learning. In Ad-
vances in Neural Information Processing Systems, pp. 2818-2826, 2015.
Kefan Dong, Jian Peng, Yining Wang, and Yuan Zhou. Root-n-regret for learning in markov decision processes with
function approximation and low bellman rank. In Conference on Learning Theory, pp. 1554-1557. PMLR, 2020.
Simon S Du, Sham M Kakade, Jason D Lee, Shachar Lovett, Gaurav Mahajan, Wen Sun, and Ruosong Wang. Bilinear
classes: A structural framework for provable generalization in rl. arXiv preprint arXiv:2103.10897, 2021.
Jerzy Filar and Koos Vrieze. Competitive Markov decision processes. Springer Science & Business Media, 2012.
Dylan J Foster, Alexander Rakhlin, David Simchi-Levi, and Yunzong Xu. Instance-dependent complexity of con-
textual bandits and reinforcement learning: A disagreement-based perspective. arXiv preprint arXiv:2010.03104,
2020.
Andrew Gilpin and Tuomas Sandholm. Finding equilibria in large sequential games of imperfect information. In
Proceedings of the 7th ACM conference on Electronic commerce, pp. 160-169, 2006.
Thomas Dueholm Hansen, Peter Bro Miltersen, and Uri Zwick. Strategy iteration is strongly polynomial for 2-player
turn-based stochastic games with a constant discount factor. Journal of the ACM (JACM), 60(1):1-16, 2013.
10
Junling Hu and Michael P Wellman. Nash q-learning for general-sum stochastic games. Journal of machine learning
research, 4(Nov):1039-1069, 2003.
Thomas Jaksch, Ronald Ortner, and Peter Auer. Near-optimal regret bounds for reinforcement learning. Journal of
Machine Learning Research, 11(4), 2010.
Zeyu Jia, Lin F Yang, and Mengdi Wang. Feature-based q-learning for two-player stochastic games. arXiv preprint
arXiv:1906.00423, 2019.
Nan Jiang, Akshay Krishnamurthy, Alekh Agarwal, John Langford, and Robert E Schapire. Contextual decision
processes with low bellman rank are pac-learnable. In International Conference on Machine Learning, pp. 1704-
1713. PMLR, 2017.
Chi Jin, Zeyuan Allen-Zhu, Sebastien Bubeck, and Michael I Jordan. Is q-learning provably efficient? In Advances in
Neural Information Processing Systems, pp. 4863-4873, 2018.
Chi Jin, Zhuoran Yang, Zhaoran Wang, and Michael I Jordan. Provably efficient reinforcement learning with linear
function approximation. In Conference on Learning Theory, pp. 2137-2143, 2020.
Chi Jin, Qinghua Liu, and Sobhan Miryoosefi. Bellman eluder dimension: New rich classes of rl problems, and
sample-efficient algorithms. arXiv preprint arXiv:2102.00815, 2021.
Daphne Koller and Nimrod Megiddo. The complexity of two-person zero-sum games in extensive form. Games and
economic behavior, 4(4):528-552, 1992.
Akshay Krishnamurthy, Alekh Agarwal, and John Langford. Pac reinforcement learning with rich observations. arXiv
preprint arXiv:1602.02722, 2016.
Hepeng Li and Haibo He. Multi-agent trust region policy optimization. arXiv preprint arXiv:2010.07916, 2020.
Michael L Littman. Markov games as a framework for multi-agent reinforcement learning. In Machine learning
proceedings 1994, pp. 157-163. Elsevier, 1994.
Michael L Littman. Friend-or-foe q-learning in general-sum games. In ICML, volume 1, pp. 322-328, 2001.
Qinghua Liu, Tiancheng Yu, Yu Bai, and Chi Jin. A sharp analysis of model-based reinforcement learning with
self-play. arXiv preprint arXiv:2010.01604, 2020.
Ryan Lowe, Yi Wu, Aviv Tamar, Jean Harb, Pieter Abbeel, and Igor Mordatch. Multi-agent actor-critic for mixed
cooperative-competitive environments. arXiv preprint arXiv:1706.02275, 2017.
Gergely Neu and Ciara Pike-Burke. A unifying view of optimism in episodic reinforcement learning. Advances in
Neural Information Processing Systems, 33, 2020.
OpenAI. Openai five. https://blog.openai.com/openai-five/, 2018.
Ian Osband and Benjamin Van Roy. Model-based reinforcement learning and the eluder dimension. In Advances in
Neural Information Processing Systems, pp. 1466-1474, 2014.
Tabish Rashid, Mikayel Samvelyan, Christian Schroeder, Gregory Farquhar, Jakob Foerster, and Shimon Whiteson.
Qmix: Monotonic value function factorisation for deep multi-agent reinforcement learning. In International Con-
ference on Machine Learning, pp. 4295-4304. PMLR, 2018.
Daniel Russo and Benjamin Van Roy. Eluder dimension and the sample complexity of optimistic exploration. In
Advances in Neural Information Processing Systems, pp. 2256-2264, 2013.
Shai Shalev-Shwartz, Shaked Shammah, and Amnon Shashua. Safe, multi-agent, reinforcement learning for au-
tonomous driving. arXiv preprint arXiv:1610.03295, 2016.
Lloyd S Shapley. Stochastic games. Proceedings of the national academy of sciences, 39(10):1095-1100, 1953.
11
Aaron Sidford, Mengdi Wang, Lin Yang, and Yinyu Ye. Solving discounted stochastic two-player games with near-
optimal time and sample complexity. In International Conference on Artificial Intelligence and Statistics, pp.
2992-3002. PMLR, 2020.
David Silver, Aja Huang, Chris J Maddison, Arthur Guez, Laurent Sifre, George Van Den Driessche, Julian Schrit-
twieser, Ioannis Antonoglou, Veda Panneershelvam, Marc Lanctot, et al. Mastering the game of go with deep neural
networks and tree search. nature, 529(7587):484-489, 2016.
David Silver, Julian Schrittwieser, Karen Simonyan, Ioannis Antonoglou, Aja Huang, Arthur Guez, Thomas Hubert,
Lucas Baker, Matthew Lai, Adrian Bolton, et al. Mastering the game of go without human knowledge. nature, 550
(7676):354-359, 2017.
Wen Sun, Nan Jiang, Akshay Krishnamurthy, Alekh Agarwal, and John Langford. Model-based rl in contextual
decision processes: Pac bounds and exponential improvements over model-free approaches. In Conference on
Learning Theory, pp. 2898-2933, 2019.
Csaba Szepesvari. Algorithms for reinforcement learning. Synthesis lectures on artificial intelligence and machine
learning, 4(1):1-103, 2010.
Yi Tian, Yuanhao Wang, Tiancheng Yu, and Suvrit Sra. Online learning in unknown markov games. arXiv preprint
arXiv:2010.15020, 2021.
Oriol Vinyals, Igor Babuschkin, Wojciech M Czarnecki, Michael Mathieu, Andrew Dudzik, Junyoung Chung, David H
Choi, Richard Powell, Timo Ewalds, Petko Georgiev, et al. Grandmaster level in starcraft ii using multi-agent
reinforcement learning. Nature, 575(7782):350-354, 2019.
Ruosong Wang, Ruslan Salakhutdinov, and Lin F Yang. Provably efficient reinforcement learning with general value
function approximation. arXiv preprint arXiv:2005.10804, 2020.
Yining Wang, Ruosong Wang, Simon S Du, and Akshay Krishnamurthy. Optimism in reinforcement learning with
generalized linear function approximation. arXiv preprint arXiv:1912.04136, 2019.
Chen-Yu Wei, Yi-Te Hong, and Chi-Jen Lu. Online reinforcement learning in stochastic games. arXiv preprint
arXiv:1712.00579, 2017.
Chen-Yu Wei, Chung-Wei Lee, Mengxiao Zhang, and Haipeng Luo. Linear last-iterate convergence in constrained
saddle-point optimization. arXiv e-prints, pp. arXiv-2006, 2020.
Chen-Yu Wei, Chung-Wei Lee, Mengxiao Zhang, and Haipeng Luo. Last-iterate convergence of decentralized op-
timistic gradient descent/ascent in infinite-horizon competitive markov games. arXiv preprint arXiv:2102.04540,
2021.
Gellert Weisz, Philip Amortila, and Csaba Szepesvari. Exponential lower bounds for planning in mdps with Iinearly-
realizable optimal action-value functions. arXiv preprint arXiv:2010.01374, 2020.
Qiaomin Xie, Yudong Chen, Zhaoran Wang, and Zhuoran Yang. Learning zero-sum simultaneous-move markov
games using function approximation and correlated equilibrium. In Conference on Learning Theory, pp. 3674-
3682. PMLR, 2020.
Zhuoran Yang, Chi Jin, Zhaoran Wang, Mengdi Wang, and Michael I Jordan. Bridging exploration and general
function approximation in reinforcement learning: Provably efficient kernel and neural value iterations. arXiv
preprint arXiv:2011.04622, 2020.
Chao Yu, Akash Velu, Eugene Vinitsky, Yu Wang, Alexandre Bayen, and Yi Wu. The surprising effectiveness of
mappo in cooperative, multi-agent games. arXiv preprint arXiv:2103.01955, 2021a.
Tiancheng Yu, Yi Tian, Jingzhao Zhang, and Suvrit Sra. Provably efficient algorithms for multi-objective competitive
rl. arXiv preprint arXiv:2102.03192, 2021b.
Andrea Zanette and Emma Brunskill. Tighter problem-dependent regret bounds in reinforcement learning without
domain knowledge using value function bounds. arXiv preprint arXiv:1901.00210, 2019.
12
Andrea Zanette, Alessandro Lazaric, Mykel Kochenderfer, and Emma Brunskill. Learning near optimal policies with
low inherent bellman error. arXiv preprint arXiv:2003.00153, 2020a.
Andrea Zanette, Alessandro Lazaric, Mykel J Kochenderfer, and Emma Brunskill. Provably efficient reward-agnostic
navigation with linear value iteration. Advances in Neural Information Processing Systems, 33, 2020b.
Kaiqing Zhang, Sham M Kakade, Tamer Bayar, and Lin F Yang. Model-based multi-agent rl in zero-sum markov
games with near-optimal sample complexity. arXiv preprint arXiv:2007.07461, 2020a.
Zihan Zhang, Yuan Zhou, and Xiangyang Ji. Almost optimal model-free reinforcement learning via reference-
advantage decomposition. arXiv preprint arXiv:2004.10019, 2020b.
Martin Zinkevich, Michael Johanson, Michael Bowling, and Carmelo Piccione. Regret minimization in games with
incomplete information. Advances in neural information processing systems, 20:1729-1736, 2007.
A Related Works
There is an extensive literature on empirical MARL, in distributed, cooperative, and competitive settings (see, e.g.,
Lowe et al., 2017; Rashid et al., 2018; Vinyals et al., 2019; Berner et al., 2019; Li & He, 2020; Yu et al., 2021a, and
the references therein). Due to space limit, we focus on reviewing the theoretical works in this section.
Markov Game. Markov Game (MG), also known as stochastic game (Shapley, 1953), is a popular model in multi-
agent RL (Littman, 1994). Early works have mainly focused on finding Nash equilibria of MGs with known transition
and reward (Littman, 2001; Hu & Wellman, 2003; Hansen et al., 2013; Wei et al., 2020), or under strong reachability
conditions such as simulators (Wei et al., 2017; Jia et al., 2019; Sidford et al., 2020; Zhang et al., 2020a; Wei et al.,
2021) where exploration is not needed.
A recent line of works provide non-asymptotic guarantees for learning two-player zero-sum tabular MGs, in the setting
that requires strategic exploration. Bai & Jin (2020) and Xie et al. (2020) develop the first provably-efficient learning
algorithms in MGs based on optimistic value iteration. Bai et al. (2020) and Liu et al. (2020) improve upon these
works and achieve best-known sample complexity for model-free and model-based methods, respectively. Several
extensions are also studied, including multi-player general-sum MGs (Liu et al., 2020), unknown games 2 (Tian et al.,
2021), vector-valued MGs (Yu et al., 2021b), etc.
Beyond tabular MGs, Xie et al. (2020) and Chen et al. (2021) also study learning MGs with linear function approx-
imation. Their techniques heavily rely on “optimistic closure” (see Appendix B for more details), which can not be
directly extended to the general function approximation setting with weak assumptions. To our best knowledge, this
paper provides the first positive result on learning MGs with general function approximation.
Markov Decision Process. There has been a long line of research studying Markov Decision Process (MDP), which
can be viewed as a single-agent version of Markov Game. Tabular MDP has been studied thoroughly in recent years
(Brafman & Tennenholtz, 2002; Jaksch et al., 2010; Dann & Brunskill, 2015; Azar et al., 2017; Zanette & Brun-
skill, 2019; Jin et al., 2018; Zhang et al., 2020b). Particularly, in the episodic setting, the minimax regret or sample
complexity is achieved by both model-based (Azar et al., 2017) and model-free (Zhang et al., 2020b) methods, up to
logarithmic factors. When the state space is large, the tabular formulation of RL is impractical and function approx-
imation is necessary. The most studied case is linear function approximation (Jin et al., 2020; Wang et al., 2019; Cai
et al., 2019; Zanette et al., 2020a;b; Agarwal et al., 2020; Neu & Pike-Burke, 2020).
For general function approximation, there are two common measures of complexity: Eluder dimension (Osband &
Van Roy, 2014; Wang et al., 2020) and Bellman rank (Jiang et al., 2017), which are unified by a more generic notion—
Bellman-Eluder dimension (Jin et al., 2021). Recently, Du et al. (2021) develops a new problem class termed bilinear
class, which can be considered as an infinite-dimensional extension of low Bellman rank class. Low complexity
RL problems under the above criteria are statistically tractable. Polynomial sample complexity guarantees (Jiang
et al., 2017; Sun et al., 2019; Jin et al., 2021; Foster et al., 2020), or even regret guarantees (Wang et al., 2019;
Dong et al., 2020; Jin et al., 2021; Wang et al., 2020) are established, under additional completeness and realizability
conditions. However, most of the above algorithms are not computationally efficient and it remains open how to design
computationally efficient algorithms for these settings.
2This terminology stems from game theory, which means the actions of the opponents are not observed.
13
Extensive-form games Finally, we remark that there is another long line of research on MARL based on the model
of extensive-form games (EFG) (see, e.g., Koller & Megiddo, 1992; Gilpin & Sandholm, 2006; Zinkevich et al.,
2007; Brown & Sandholm, 2018; 2019; Celli et al., 2020). Results on learning EFGs do not directly imply results
for learning MGs, since EFGs are naturally tree-structured games, which can not efficiently represent MGs—graph-
structured games where a state at the hth step can be the child of multiple states at the (h - 1)th step.
B Discussion on Technical Challenges
In this section, we discuss some technical challenges faced when designing provably efficient algorithms for MGs with
general function approximation, which explains why direct extension of existing algorithmic solutions is not enough.
The algorithmic solutions designed for tabular MGs (Bai & Jin, 2020) and linear MGs (Xie et al., 2020) can be viewed
as special cases of solving the following sub-problem: in each episode k, given the confidence set of possible functions
f ∈ Ck, find function pair (fk, gk) and policy pair (μk, Vk) s.t. for any state S
[Dμk×νk]fk(s) ≥ max [Dμ×νkf](s), [Dμk×νgk](s) ≤ mi? [Dμk×νg](s).	(8)
f ∈Ck,μ	g∈Ck ,ν
Here we omit the dependence on h by considering a single-step special case. This is similar to the contextual bandits
problem, but a game version.
Once the above sub-problem is solved, using the fact that the true value function f? is contained in Ck, we can bound
the duality gap by
(V t,νk - V μk,t)(s) = [Dμt(νk )×νk f*](s) - [Dμk×νt(μk)f*](s) ≤ [Dμk×ν% (f - gk )](s),
where the summation of the RHS over k can be further bounded by pigeon-hole type of arguments.
Then a natural question is: how can we solve this sub-problem (8)? A helpful condition is optimistic closure, which
indeed holds for tabular and linear MGs (Bai & Jin, 2020; Xie et al., 2020). For the max-player version, this means
the pointwise upper bound defined by f
(s, a, b) := maxf (s, a, b)
remains in the function class, i.e., f ∈ Ck ⊆ F.
The min-player version is similar.
Under this condition, the function pair (f k ,f k) is
it reduces to finding (μk ,νk) so that
clearly the maximizer/minimizer to the subproblem, and therefore
Dμk×νkf (s) ≥ maxDμ×νkf (s), Dμk×νkfk(s) ≤ minDμk×νfk(s).
By definition, the solution is a coarse correlated equilibrium.
However, the optimistic closure may not hold in general. Even worse, no solution may exist for the sub-problem (8),
as we will see below in a concrete example. In that case, the existing techniques fail and it becomes unclear how to
bound the two-sided duality gap. As a result, the general function approximation problem is significantly harder than
the special cases mentioned before.
Now we describe a concrete example where sub-problem (8) has no solution when optimisic closure condition does
not hold. We consider rock-paper-scissors, with one state and three actions for both players, i.e., |S| = 1 and |A| =
|B| = 3. Then each function f ∈ F : S × A × B → [-2, 2] 3 is essentially a 3 × 3 matrix and we will use M to
represent such matrices. The matrix corresponding to Nash value Q? is
0	1	-1
M? =	-1	0	1
1	-1	0
which describes the true reward associated with different actions. Assume there are 6 other matrices in our confidence
setCk:
0	1.1	-1	0	1	-1.1	0	1	-1
M1 = -1	0	1	, M2 = -1	0	1	, M3 = -1.1	0	1	,
1	-1	0	1	-1	0	1	-1	0
3We assign the reward [-2, 2] instead of [0, 1] to make the example looks simpler. Everything stated here still hold after scaling.
14
0	1	-1	0	1	-1	0	1	-1
M4 =	-1	0	1.1 , M5 = -1	0	1	, M6 = -1	0	1	.
1	-1	0	1.1	-1	0	1	-1.1	0
Now suppose We find a pair of matrices (M, M) and a pair of policies (μ, V) which solves sub-problem (8). If We can
prove such (μ, V) must be deterministic, then we get into a contradiction, since whatever (M, M) is, it is impossible
for deterministic policies μ and V to be the best response of each other. Therefore, in order to show sub-problem (8)
has no solution, it suffices to prove (μ, V) must be deterministic.
Since μ is a solution of sub-problem (8), we must have
μ = argmax max (μ0)TMν.
μ0	M ∈Ck
Since we are maximizing the above quantity, M can only take M1,M4 or M5 because the others are dominated. Direct
computation gives
M1V =M?V + [0.1V [2], 0, 0]T,
M4V =M?V + [0, 0.1V[3], 0]T,
M5V =M?V + [0, 0, 0.1V[1]]T.
Now that μ maximizes (μ0)TMV, if it is not deterministic, there will be at least two entries in MV that take the same
highest value, say (MV)[1] = (MV)[2] ≥ (MV)[3].
If M = Mi, the above condition is just
(MV )[2] = (M ?v)[2] = (M ? V )[1]+0.1v [2] = (MV )[1].
Then consider two cases:
•	If V[3] > 0, we can choose M = M4 to make (MV)[2] = (M?v)[2] + 0.1v[3] > (M?v)[2]. Therefore, the
value of max(μ0)TMν is increased, which is contradictory to the maximization condition.
•	If V[3] = 0, the condition (M?V)[2] = (M?V)[1] + 0.1V[2] is just V[2] + 0.1V[2] = -V[1], which impliess
V[2] = V[1] = 0. This is impossible since V is a probability distribution.
As a result, we prove M cannot be Mi. Similarly, M cannot be M4 or M5. We obtain a contradiction! Therefore, μ
must be deterministic and by symmtrical argument so is V. Putting everything together completes the proof. That is,
when optimisic closure condition does not hold, sub-problem (8) can have no solution.
C V-type Bellman Eluder Dimension
In this section, we define V-type Bellman-Eluder (VBE) dimension, and provide the corresponding theoretical guar-
antee for Algorithm 1 with Option II.
C.1 Complexity Measure
With slight abuse of notation, we redefine the following distribution families for VBE dimension. The only difference
is that now we use distributions over S instead of S × A × B.
1.	Dδ := {D∆,h}h∈[H], where D∆,h = {δs(∙) | S ∈ S}, i.e., the collections of probability measures that put
measure 1 on a single state.
2.	DF := {Dμ,h}h∈[H], where Dμ,h denotes the collections of all probability measures over S at the hth step,
which can be generated by executing some (μf ,Vf,g) with f,g ∈ F. Here, Vf,g is the best response to μf
regarding to Q-value g:
Vf,g,h(s) ：= argminμf,h(s)>gh(s, ∙, ∙)ν forall (s, h).
ν∈∆B
15
To proceed, we introduce the following Bellman residual function defined over the state space S
[Eh(f,g,w)](s) := E[(fh - Tμgfh+I)(SMb) | (a,b)〜μg,h X νg,w,h].
Equipped with the new definition, we can define V-type BE dimension.
Definition 17 (V-type Bellman-Eluder dimension). Let HF be the function classes of Bellman residual where
HF,h := {Eh(f, g, w) | f, g, w ∈ F}. Then the V-type Bellman-Eluder (VBE) dimension is defined as
dimVBE (F, ) = max min	dimDE(HF,h, Dh, ).	(9)
h∈[H] D∈{D∆,DF}	,
We comment that we do not have the online version of V-type BE dimension, because the uniform sampling step in
Option II is in general not compatible with the policy of the opponent.
C.2 Theoretical Guarantees
Now we are ready to present the theoretical guarantees.
Theorem 18 (Regret of Golf_with_EXPLOIter). Under Assumption 1 and 2, there exists an absolute Constant C
such thatfor any δ ∈ (0,1], K ∈ N, ifwe choose β = c ∙(log(KH ∣F∣∣G∣∕δ)+Kε2omp + Kε2ea]), then with probability
at least 1 - δ, for all k ∈ [K], Algorithm 1 with Option II satisfies:
k
t=1
where d = dimVBE (F, 1/K) is the VBE dimension.
Theorem 18 provides a √K pseudo-regret guarantee in terms of VBE dimension for Algorithm 1 with Option II.
The reason for calling it pseudo-regret is that in each episode, the samples are collected following a combination of
(μk, νk) and uniform sampling, instead of only (μk, νk) as in Option I. Still, this is enough for applying the pigeonhole
principle to derive the sample complexity of Golf_with_EXPLOIter.
Corollary 19 (Sample complexity of Golf_with_Exploiter). Under Assumption 1 and 2, there exists an ab-
solute ConStant C such that for any e > 0, choose β = C ∙ Qog(KH∣F∣∣G∣∕δ) + Kε20mp + Kε2eai) and
∆ = C (H ,∣A∣∣B∣dβ∕K + e), where d = dim VBE (F, e/H) is the VBE dimension of F, then with probabil-
ity at least 1 一 δ, the output condition (Line 5) will be satistied at least once in the first K episodes. Further-
more, the output policy μout of Algorithm 1 with Option II is O(e + H√d(εreai + εramp))-approximate Nash, if
K ≥ Ω((H2d∕e2) ∙log(H∣F∣∣G∣d∕e)).
Corollary 19 claims that Golf_with_EXPLOITER can find an O(e)-approximate Nash policy for the max-player
using at most O(H2∣A∣∣B∣ log(∣F∣∣G∣) dimBE(F, e)∕e2) samples, which scales linearly in the VBE dimension, the
cardinality of the two action sets, and the log-cardinality of the function classes.
D Proofs for Section 3
In this section, we present the proof of Theorem 7, Corollary 8, and Theorem 11.
The following auxiliary lemma (Lemma 26 in Jin et al. (2021)) will be useful.
Lemma 20 (Pigeon-hole). Given a function class G defined on X with |g(x)| ≤ C for all (g, x) ∈ G X X, and a family
of probability measures Π over X. Suppose SeqUenCe {gk }K=ι ⊂ G and {μk }K=ι ⊂ ∏ satisfy thatfor all k ∈ [K ],
Pk-1(Eμt [gk])2 ≤ β. Thenforall k ∈ [K] and ω > 0,
k
X ∣Eμt [gt]| ≤ O (PdimDE(G,∏,ω)βk + min{k,dimDE(G,Π, ω)}C + kω).
t=1
Another useful decomposition is the value difference lemma (Lemma 1 in Jiang et al. (2017)).
Lemma 21 (value difference). For any function f s.t. fH+1 = 0 and policy π,
H
Vf,h(S) 一 Vhπ(S) = Eπ [Vf,h0 (Sh0) 一 rh0 (Sh0, ah0, bh0) 一 Vf,h0+1(Sh0+1)|Sh = S] .
h0=h
16
D.1 Proof of the Online Guarantee
We begin with the proof of Theorem 11. The following two concentration lemmas help us upper bound the Bellman
residuals. We defer their proofs to Appendix D.3.1 and D.3.2.
Lemma 22. Assuming kQ?h - PFh (Q?h)k ≤ εreal and kThfh+1 -PGh(Thfh+1)k ≤ εcomp for all f ∈ F andh ∈ [H],
ifwe choose β = C ∙ Qog(KH∣F∣∣G∣∕δ) + Kε20mp + Kε2eaJ with some large absolute constant c in Algorithm 1 with
Option I, then with probability at least 1 - δ, for all (k, h) ∈ [K] × [H], we have
(a)	Pk=II E [(fk (sh,αh ,bh) - (Thfk+1)(s⅛ ,ah,bh)'))| sκ,ah,bh 〜πi] ≤O(β),
(b)	PM (fk(sh,ah,bh) — (Thfk+ι)(sh,ah,bh))2≤O(β)，
where (sɪ, a∖,,叭,...，s% ,a% ,b%) denotes the trajectory SampIed byfollowing πi = (μi, νi) in the ith episode.
Lemma 23. Under the same condition of Lemma 22, with probability at least 1 - δ, we have PF (Q?) ∈ Ck for all
k∈ [K].
Proof of Theorem 11. By Lemma 23, PF (Q?) ∈ Ck. Since we choose fk optimistically and F satisfies εreal-
approximate realizability,
XL (V*(sι)- V*"(sι))
k=1
K
≤ X (VPF(Q*),1(SI) - Vμ ,V(SI)) + Kεreal
k=1
K
≤ ^X (Vf k,1(SI)- Vf ,V(SI)) + Kεreal
k=1
KH
(=i)	Eπk Vfk,h(Sh) - rh(Sh, ah, bh) - Vf k,h+1 (Sh+1) + K εreal
k=1 h=1
KH
(=ii) X	X Eπk
k=1 h=1
[minDμh×νfh (Sh) - rκ(sh,ah,
bh) - Vf k,h+1 (Sh+1 )
+ K εreal
KH
≤X X	Eπk
k=1 h=1
[Dμh×νk fhk (Sh)- rh(Sh, ah, bh) - Vf k,h+1(Sh+1)] + Kεreal
HK
ΣΣEπk fhk (Sh, ah, bh) - rh(Sh, ah, bh) - Vf k,h+1 (Sh+1) + K εreal
h=1 k=1
(iii)	ɪɪ	/ Z_____________、
≤ E ∑(fk - Thfk+ι)(Sh, ah, bh) + O (JKHIog(KH∕δ)) + Kε,eai
h=1 k=1
(i≤)O (H√β ∙ dimoBE(F, 1/K) ∙ K) + Kεreai ≤ O (H√β ∙ dimoBE(F, 1/K) ∙ K).
where (i) is by value difference (Lemma 21), (ii) is due to μk = μfk, (iii) is by AzUma-Hoeffding, and (iv) is by
incurring Lemma 20 with G = HF,h (here, HF refers to the one introduced in Definition 10), Π = D∆,h, = 1/K
and Lemma 22. The final inequality follows from the choice of β.	□
D.2 Proof of the Self-play Guarantee
Proving Theorem 7 requires more work because we need to develop guarantees for the sub-routine Com-
PUTE-Exploiter. Similar to Lemma 22 and 23, We establish two concentration lemmas below, whose proofs are
deferred to Section D.3.3 and D.3.4.
17
Lemma 24. Under Assumption 1 and 2, if we choose β = C ∙ (log(KH ∣F∣∣G∣∕δ) + Kε20mp + Kε2ea]) with some
large absolute constant c in Algorithm 1 with Option I, then with probability at least 1 - δ, for all (k, h) ∈ [K] × [H],
we have
(a)	Pk=-ι1 E f (sh,ah,bh) - (TF f>ι)(sh, ah,bh)) | s⅛,a⅛,b⅛ 〜∏i ≤O(β)∙
(b)	P31 f (sh,ah,bh) - (Tf fk+ι)(sh,ah,bh))2≤o(β)，
where (sɪ, α∖, ,b∖,..., s%, a}, bzH, s%+1) denotes the trajectory SampIed by following πi = (μi,νi) in the ith
episode, and fk denotes the optimistic function computed in sub-routine COMPUTE-Exploiter in the kth episode∙
Lemma 25. Under the same condition of Lemma 24, with probability at least 1 一 δ, we have PF (Q*k J) ∈ Cμ for
all k ∈ [K]∙
Proposition 26 (approximate best-response regret). Under Assumption 1 and 2, there exists an absolute constant c
such thatfor any δ ∈ (0,1], K ∈ N, ifwe choose β = c ∙(log(KH ∣F∣∣G∣∕δ)+Kε2omp + Kε2eai), then with probability
at least 1 - δ, for all k ∈ [K],
k
X (Vft,νt (si) - V"t 't(sι)) ≤ O(H pPkβ ∙ dim be (F, 1/K)).
t=1
Proof∙ By Lemma 25, PF(Qμt,t) ∈ Ct. Since We choose ft optimistically and F satisfies εr∙eai-approximate realiz-
ability,
k
X(Vιμt,νt (si) - Vιμt,t(sι))
t=i
k
≤ X (V1"'"(si) - minD“i×νft(sι)) + kεreai
t=i
kH
(10)
=-XX Ent kfh - ThhJ fh +I)(Sh, ah, bh)i + kεreal
t=i h=i
H k
≤-X X(fh - TF fh+i)(sh, ah, bh) + O(PkHbg(KH∕δ)) + kεgi,
h=i t=i
where ft denotes the optimistic function computed in sub-routine Compute .Exploiter in the tth episode and the
equality folloWs from value difference Lemma 21.
By Lemma 24 (b), we have for all k ∈ [K]
k-i	2
X f (sh, ah, bh) 一 (Tf fhk+i)(sh, ah, bh)] ≤O(β).
t=i
So we can apply Lemma 20 with G = HF,h, Π = D∆,h, = 1∕K and obtain
k	______________________________
XIfh (sh,ah,bh) - (Tιμt fh+ι)(sh,ah,bh)∖ ≤ Jβ ∙dimDE (HF ,h, D∆,h,1∕K) ∙ k.
t=i
plugging this inequality back into (10) gives us the first upper bound.
We can also invoke Lemma 24 (a) with Jensen’s inequality, and obtain for all k ∈ [K]
k-i	2
X (E∏t (ffh -Thμfh+ι)(sh,ah,bh)]) ≤O(β).
t=i
18
Again, we can apply Lemma 20 with G = HF,h (here, HF refers to the one introduced in Definition 6), Π = DF,h,
= 1/K, and obtain
X ∣E∏t h(fh - TFfh+ι)(sh,ah,bh)i I ≤ √β ∙ dimDE(HF,h, Df,h, 1/K) ∙ k.
t=1
Plugging this inequality back into (10) gives us the second upper bound.
Combining the two upper bounds and noticing that kεreai ≤ O(H∙∖∕kβ-dimBE(F717K)) (because of the choice of
β) conclude the proof.	□
Now we are ready to prove the regret guarantee.
Proof of Theorem 7. The regret can be decomposed into two terms
X (Vι*(sι) - Vμk,t(sι)) = X(V?(si) - V"" (sι)) + X "k炉(si) - ¼μk,t (sι))∙
k=1	k=1	k=1
'------------{z------------} '-----------------------------}
(A)	(B)
By Theorem 11, where the opponent’s policy {νk}kK=i can be arbitrary, (A) can be upper bounded by
K
X VT(SI)- Vnk(si) ≤ O (H√κβ ∙ dimBE(F, 1/K)).
k=i
By Proposition 26, (B) can be upper bounded by
K
X (Vμkd(Si) - Vfk,t(sι)) ≤ O (H√Kβ ∙ dimBE(F, 1/K)).
k=i
Combining the two inequalities concludes the proof.	□
By the standard online-to-batch reduction, we can also derive the sample complexity guarantee.
Proof of Corollary 8. We proceed as in the proof of Theorem 7 but take ω = 7H (instead of ω = 17K) every time
we incur Lemma 20.
Theorem 11 essentially shows
K	K
X(V?(Si)- VilAVk(Si)) ≤ X (Vfk,i(sι) - vμk,νk(Si)) ≤O(H√κβ ∙dimBE(F, e/H) + Ke),
k=i	k=i
where we have used the fact that dimOBE (F, e/H) ≤ dimBE (F, e/H).
Proposition 26 essentially shows
K	K
X (Vμk,νk (Si) - Vμk,t(Si)) ≤ X (Vμk,νk (Si) - mn口“中 ×νfk(Si)) ≤ O(H√Kβ ∙dimBE(F, e/H) + Ke).
k=i	k=i
Comparing with the form in Theorem 7, now we have an additional O(Ke) term, since we take ω = e/H when
incurring Lemma 20.
Putting them together and noticing by definition V k = Vfk,i(Si) and Vk = minν Dμk×νfk (Si), we can get
1 ʌ _k	,	H ,---------------
£[Vk - Vk] ≤O(- √dimBE(F,e/H)Kβ + e),
KK
k=i
19
with probability at least 1 - δ, where β = C ∙ (Iog(KH∣F∣∣G∣∕δ) + Kε2omp + Kε2eai).
By pigeonhole PrinPle, there must exist some k s.t. Vk — Vk ≤ ∆ = C (H PdimBE(F, e/H )β∕K + e). Therefore,
the output condition must be satisfied by some k ∈ [K].
To make the right hand side order O(e + H√d(εreai + ɛɑomp)), it suffices to take
K ≥ Ω((H2d∕e2) ∙ log(H∣F∣∣G∣d∕e)),
where d = dimBE (F, e∕H).
□
D.3 Proofs of the Concentration Arguments
D.3.1	Proof of Lemma 22
Proof. We prove inequality (b) first.
Consider a fixed (k, h, f) tuple. For notational simplicity, we denote zht := (sth, ath, bth). Let
Xt(h, f)	:=	(fh(Zh)	- rh	- Vf,h+1 (sh+1))	-(PG(Thfh+1)(ZIh)	- rh	-	Vf,h+1(sh+l))
and Ft,h be the filtration induced by {z1i, r1i , . . . , zHi , rHi }it=-11 S{z1t, r1t , . . . , zht }. We have
E[Xt(h,f) | Ft,h] = (fh - Thfh+1)(Zht)2 - (PG(Thfh+1)-Thfh+1)(Zht)2
≥ (fh - Thfh+1)(Zht )	- εc2omp
and
Var[Xt(h,f) | Ft,h] ≤ E[(Xt(h, f))2 | Ft,h] ≤36[(PG(Thfh+1) -fh)(Zht)]2
≤ O ([(PG(Thfh+1)- Thfh+I)(Zh)]2 + [(Thfh+1 - fh)(zh)]2)
≤ O ([(Thfh+1 - fh)(zh)]2 + ε2omp) ∙
By Freedman’s inequality, we have, with probability at least 1 - δ,
kk
X (fh - Thfh+1)(Zht)2 -kεc2omp-XXt(h,f)
t=1	t=1
≤O(∖log(1∕δ) (XX[(Thfh+1 - fh)(zh)]2 + kε2omp) +log(1∕δ))
Now taking a union bound for all (k, h, f) ∈ [K] × [H] × F, we obtain that with probability at least 1 - δ, for all
(k,h,f)∈[K]× [H]×F
kk
X (fh - Thfh+1)(Zht)2 - kεc2omp - X Xt(h, f)
t=1	t=1
≤Oh
ι Xk [(Thfh+1 - fh)(Zht)]2 + kεc2omp +ι
(11)
where ∣ = log(HK∣F∣∕δ). From now on, We Will do all the analysis conditioning on this event being true.
20
Consider an arbitrary pair (h, k) ∈ [H] × [K]. By the definition ofCk and Assumption 2
k-1
XXt(h,fk)
t=1
k-1	k-1
= X(fhk(zht ) - rht - Vfk,h+1(sth+1))2 - X(PG(Thfhk+1)(zht ) - rht - Vfk,h+1(sth+1))2	(12)
t=1	t=1
k-1	k-1
≤ X(fhk(zht ) - rht - Vfk,h+1(sth+1))2 - inf X(g(zht ) - rht - Vfk,h+1(sth+1))2 ≤ β.
t=1	g∈Gh t=1
Putting (11) and (12) together, we obtain
k-1
X[(fhk - Thfhk+1)(zht)]2 ≤ O(β + ι + kεc2omp),
t=1
which concludes the proof of inequality (b).
To prove inequality (a), we only need to redefine Ft,h to be the filtration induced by
{zi,r1,. ..,zH,rH}i-1 and then repeat the arguments above verbatim.	□
D.3.2 Proof of Lemma 23
Proof. We will continue to use the notations defined in the proof of Lemma 22. To further simplify notations, we
denote
Q := PF(Q ) and
Consider a fixed tuple (k, h, g) ∈ [K] × [H] × G. Let
Wt(Ag)= (gh(Zh) - rh - VQ,h+ι(Sh+1))2 -(Qh(Zh) - rh - VQ,h+1(Sh+1))2
and Ft,h be the filtration induced by {z1i, r1i, . . . , zHi , rHi }it=-11 S{z1t, r1t , . . . , zht }. We have
E[Wt(h, g) | Ft,h] = h(gh - ThQh+ι)(zh)]2 - [(Qh - ThQ^h+ι)(zh)i2
≥ h(gh - ThQh+l)(zh)] - ε2eal
and
Var[Xt(h,g) | Ft,h] ≤ E[(Xt(h,g))2 | Ft,h] ≤ 36[(Qh - gh)(zh)]2
≤ O ([(Qh- ThQh+1)(Zih)]2 + I(ThQh+1 - gh)(Zt)]2)
≤ O ([(ThQh+1 - gh)(Zh)F + ε2omp).
By Freedman’s inequality, we have, with probability at least 1 - δ,
kk	2
-X Wt(h,g)+ X [(gh - ThQh+U(zh)] - kε2eal
t=1	t=1
≤Oh
log(1∕δ) (X[(ThQh+1 - gh)(zh)]2 + kε2omp) + log(1∕δ)).
21
Now taking a union bound for all (k, h, g) ∈ [K] × [H] × G, we obtain that with probability at least 1 - δ, for all
(k, h, g) ∈ [K] × [H] × G
kk	2
-X Wt(h,g) + X [(gh-ThQh+ι)(Zh)] - kε2eal
t=1	t=1
≤O ( t ι (χ[(ThQh+1 - gh)(Zh)]2 + kε2omp) + ι
(13)
where ∣ = log(HK∣G∣∕δ). From now on, We will do all the analysis conditioning on this event being true.
Since Pk=ι [(gh - Th<⅛h+ι)(zh)i is nonnegative, by CaUchy-SchWarz inequality, (13) implies for all (k, h,g) ∈
[K] × [H] × G
k
X Wt(h, g) ≥ -O(ι + kεc2omp).
t=1
Plugging in the definition of Wt(h, g) and the choice of β completes the proof.
□
D.3.3 Proof of Lemma 24
Proof. Recall μf denotes the Nash policy of the max-player induced by f. If there exist more than one induced Nash
policies, we can break the tie arbitrarily so that μf is uniquely defined for each f ∈ F. Denote ∏f := {μf | f ∈ F}.
We have ∣∏f| ≤ |F|.
We prove inequality (b) first. Consider a fixed tuple (k, h, f, μ) ∈ [K] X [H] × F × ∏f. Again we denote Zh :=
(sth,ath,bth). Let
Xt(h,f,μ) := (fh(Zh)- rh - Vfh+ι(Sh+1)) -(PG(Thμfh+ι)(zh)—rh - Vμh+ι(Sh+1))
and Ft,h be the filtration induced by {Z1i, r1i , . . . , ZHi , rHi }it=-11 S{Z1t, r1t , . . . , Zht }. We have
E[Xt(h,f,μ) I Ft,h] = [(fh-Trfh+0(Zjt)]2 - [(Pg(Thfh+i)-Thfh+ι)(zh)]2
≥ [(fh -Tμfh+I)(Zh)]2 - ε2omp
and by Cauchy-Schwarz inequality
VarXt(h,f,μ) ∣ Ft,h] ≤ E[(Xt(h,f,μ))2 ∣ Ft,h]
≤36 [(fh-PG(T‰ι))(Zh)]2 ≤ O ([(fh -Thμfh+ιMZth)]2 + ε2omp).
By Freedman’s inequality, we have, with probability at least 1 - δ,
kk
X [(fh - Tμfh+ι)(Zth)]2 - kε2omp - XXt(h,f, μ)
t=1	t=1
≤O ( t log(1∕δ) X ([(fh - Trfh+1)(Zh)]2 + ε2omp) + log(1∕δ)).
Now taking a union bound for all (k, h, f, μ) ∈ [K] × [H] ×F × ∏f, we obtain that with probability at least 1 一 δ,
for all (k, h, f, μ) ∈ [K] × [H] ×F × ∏f
kk
X [(fh - Trfh+1)(Zh)]2 - k>2omp - X Xt(h,f, μ)
t=1	t=1
≤O (t
IX ([(fh - ThIfh+1 )(Zh)]2 + ε2omp) + ι),
(14)
22
where ∣ = Iog(HK∣F∣∕δ). From now on, We will do all the analysis conditioning on this event being true.
Consider an arbitrary pair (h, k) ∈ [H] × [K]. By the definition ofCk and Assumption 2
k-1
X Xt(h,fk ,μk)
t=1
k-1	k-1
=X(fhk(Zh) - rth - V⅛+ι(sh+ι))2 - X(PG(Tf —) - rth - V⅛+ι(s>ι))2	(15)
t=1	t=1
k-1	k-1
kk
≤ X(fh (zh ) - rh - Vfk,h+1(sh+1)) - inGf X(g(zh ) - rh - Vfk,h+1(sh+1)) ≤ β.
t=1	g∈Gh t=1
Putting (14) and (15) together, we obtain
k-1
X[(fk - TFfk+i)®)]2 ≤O(β + ∣ + kε2omp),
t=1
which concludes the proof of inequality (b).
To prove inequality (a), we only need to redefine Ft,h to be the filtration induced by
{zi ,r1,. ..,zH ,rH }i-1 and then repeat the arguments above verbatim.	□
D.3.4 Proof of Lemma 25
Proof. Recall μf denotes the Nash policy of the max-player induced by f. If there exist more than one induced Nash
policies, we can break the tie arbitrarily so that μf is uniquely defined for each f ∈ F. Denote ∏f := {μf | f ∈ F}.
We have ∣∏f| ≤ |F|.
Consider a fixed tuple (k, h, g, μ) ∈ [K] X [H] ×G × ∏f. Let
Wt(h,g,μ) := (g%(zh.)-Irth- Qμ+ι(Sh+1)) -^PF(Qμb(Zh)- rh - Qμ+ι(sh+1))
and Ft,h be the filtration induced by {z1i, r1i, . . . , zHi , rHi }it=-11 S{z1t, r1t , . . . , zht }. We have
E[Wt(h,g,μ) I Ft,h] = [(gh - Q片t) (Zh J - [(Pf(Q片t) - Q片t) (Zh)『
≥ [(gh - Qh,t) (Zztt)i - ε2eal
and by Cauchy-Schwarz inequality
Var[Wt(h,g,μ) ∣ Ft,h] ≤ E[(Wt(h,g,μ))2 ∣ Ft,h]
≤36 [(gh - PF (Qμ,t)) H )『≤ o (h(gh - Qμ,t) a，+ε2ej.
By Freedman’s inequality, we have, with probability at least 1 - δ,
kk	2
-X Wt(h, g,μ) + X [(gh - Qμ)3l)] - kε2eai
t=1	t=1
≤O ( t log(1∕δ) X (h(gh - Qμ,t) (Zh)]2 + ε2ea) + log(1∕δ)).
23
Now taking a union bound for all (k, h, g, μ) ∈ [K] X [H] ×G × ∏f, We obtain that with probability at least 1 - δ,
for all (k, h, g, μ) ∈ [K] × [H] ×G × ∏f
kk	2
-X Wt(h, g,μ) + X [(gh - Qμ,*) (ZhR - kε2eal
t=1	t=1
≤O (∖ 1 X (h(gh - Qμ,t) (Zh2 + ε2ea) + l) ,
where ∣ = Iog(HK∣G∣∣F∣∕δ). From now on, We will do all the analysis conditioning on this event being true.
Since Pk=ι [(gh — Qμ,t)(zh)]2 is nonnegative, (16) implies for all (k, h, g, μ) ∈ [K] × [H] ×G × ∏f
k
X Wt (h, g, μ) ≥ -O(ι + kεr2eal).
t=1
By choosing μ = μk, we have for all (k, h) ∈ [K] × [H]
(16)
k-1
t=1
-Qμ+,t(Sh+1))
k-1	2
≤ g∈nf X g9zhh ) - rh - Qμ+,l(sh+1)) + O(I + kε2eaI).
We conclude the proof by recalling β = Θ (Iog(HK ∣G∣∣F∣∕δ) + kε2eai + +kε2omp).
□
E Proofs for Section 4
In this section, we will first generalize Bellman rank (Jiang et al., 2017) to the setting of Markov Game. Then we show
any problems of low Bellman rank also have low BE dimension. Finally, we prove all the examples in Section 4 have
low Bellman rank, and thus low BE dimension.
Denote ΠF := {(μf, νf,g) | f, g ∈ F}, which is exactly the policy class that induces DF.
Definition 27 (Q-type -effective Bellman rank). The -effective Bellman rank is the minimum integer d so that
•	There exists φh : ΠF → H and ψh : F → H for each h ∈ [H] where H is a separable Hilbert space, such
that for any π ∈ Π, f, g ∈ F, the average Bellman error
En [(fh -Thμ fh+1)(sh,ah,bh)] = hφh(π),ψh(f,g')')H ,
where kψh(f,g)kH ≤ 1.
•	d = maxh∈[H] deff(Xh(φ,F),) whereXh(φ,F) = {φh(π) : π ∈ ΠF}.
Proposition 28 (low Q-type effective Bellman rank ⊂ low Q-type BE dimension). If an MG with function class F
has Q-type -effective Bellman rank d, then
dimBE (F, ) ≤ d.
Proof. Assume there exists ∏ι,...,∏n ∈	∏f and (f 1,g1)..., (f n,gn) such that for all t ∈	[n],
qpt-ι(E∏i [fh - Thμt fh+ι])2 ≤ e and 吗/4-Thgg 疗十/ > e. By the definition of effective Bellman rank,
this is equivalent to: Jpt=1(hΦh(∏t), ψh(fi, gi)i)2 ≤ e and ∣hφh(∏t),ψh(f t,gt)i∣ > e forall t ∈ [n].
For notational simplicity, define xi = φh(πi) and θi = ψh(fi, gi). Then
(t-1
(xi>θt)2 ≤2, t∈ [n]
i=1
∣x>θt∣≥ e, t ∈ [n].
(17)
24
Define Σt = Pit=-11 xix> + e2 ∙ I. We have
kθtk∑t ≤ √2e =⇒ e ≤ |x>%| ≤ kθtk∑t ∙ kχtk∑-ι ≤ √2ekχtk∑-i, t ∈ [n].	(18)
As a result, we should have kxtk2 -1 ≥ 1/2 for all t ∈ [n]. Now we can apply the standard log-determinant argument,
Σt
n
X log(1 + kxtk2Σ-1)
t=1
log'
log det
1n
I + S ∑XiX>
which implies
0.5 ≤ min kxt k2 -1
t∈[n]	Σt
≤ eχp (1l°gdet (I+e1- X
(19)
Choose n = deff (Xh (φ, F), e) that is the minimum positive integer satisfying
1	1n >	1
sup 一 log det I +——2	XiX>	≤ e .	(20)
x1 ,...,xn ∈Xh (φ,F) n	e i=1
This leads to a contradiction because 0.5 > ee 1 - 1. So we must have n ≤ deff (Xh(ψ, F),e).	□
Similarly, we can define V-type Bellman rank, and prove it is upper bounded by V-type BE dimension.
Definition 29 (V-type e-effective Bellman rank). The e-effective Bellman rank is the minimum integer d so that
•	There exists φh : ΠF → H and ψh : F → H for each h ∈ [H] where H is a separable Hilbert space, such
that for any π ∈ Π, f, g, w ∈ F, the average Bellman error
E[(fh -Thigfh+ι)(sh,ah,bh) | Sh 〜∏, (ah,bh)〜μg,h X Vg,w,h] = (φh(π),ψh(f,g, Wy)H ,
where kψh(f, g, w)kH ≤ 1.
•	d = maχh∈[H] deff(Xh(φ, F), e) where Xh(φ, F) = {φh(π) : π ∈ ΠF}.
Proposition 30 (low V-type effective Bellman rank ⊂ low V-type BE dimension). If anMG with function class F has
V-type e-effective Bellman rank d, then
dim VBE(F, e) ≤ d.
We omit the proof here because it is basically the same as Proposition 28.
E.1 Proofs for Examples
Below, we prove the problems introduced in Section 4 have either low Q-type or low V-type Bellman rank. Therefore,
by Proposition 28 and 30, they also have low Q-type or low V-type BE dimension.
Proof of Proposition 12. We have
E[(fh -Thμfh+ι)(sh,ah,bh) | (sh,ah,bh)〜∏]
=<Pπ[(sh, ah, bh) = ∙] , (fh - Thμμfh+ι)(∙)),
where the LHS only depends on π, and the RHS only depends on f, g.
Proof of Proposition 13. Consider two arbitrary θh, θh+1 ∈ Bd(R) and g ∈ F. By self-completeness, there exists
θg ∈ Bd(R) so that Tμg (φ>+1θh+ι) = Φ>θh.. Therefore, We have
E[[φ>θh -Tμg (φ>+1θh+l)](sh,ah,bh) | (sh,ah,bh)〜π]
=E[φh(sh, ah, bh)>(θh - θh) | (sh,ah,bh)〜∏]	(22)
=Eπ φh(sh, ah, bh)] , θh - θhg ,
where the LHS only depends on π, and the RHS only depends on θh , θh+1, g.
(21)
□
□
25
Proof of Proposition 14. Consider two arbitrary θh , θh+1 ∈ BH (R) and g ∈ F. By self-completeness, there exists
θg ∈ BH(R) so that Tμg (φ>+1θh+1) = φ>θh .Therefore, we have
E[[φ>θh -Tμg (φ>+1θh+1)](sh,ah,bh) | (sh,ah,bh) 〜π]
=E[φh(sh,ah,bh)>(θh - θh) | (sh,ah,bh)〜∏]	(23)
=Eπ [φh(sh, ah, bh)] , θh - θhg ,
where the LHS only depends on ∏, and the RHS only depends on θ%, θ%+ι,g.	□
Proof of Proposition 15. Let d(i) denote the distribution of state s given q(s) = i. For any policy π and f, g, w ∈ F
E[(fh -Tμg fh+ι)(sh,ah,bh) | Sh 〜∏, (ah,bh)〜μg X νg,w]
= <Pπ [q(Sh) = ∙] , Elfh - T*gfh+I)(Sh, ah, bh) | sh 〜d(∙), (ah, bh) 〜μg X Vg,w])，
where the LHS only depends on ∏ while the RHS only depends on f, g, w. Both of them are m-dimensional. □
Proof of Proposition 16. The case h = 1 is trivial. We only need to consider h ≥ 2. For any policy π and f, g, w ∈ F
E [(fh - Tμ fh+ι)(sh, ah, bh) | Sh 〜∏, (ah, bh)〜μg X Vg,w]
=	P [(Sh-1, ah-1 , bh-1 ) = z] X P[Sh = S | (Sh-1 , ah-1 , bh-1) = z]
z∈S×A×B s
X E [(fh - Tμg fh+1)(sh, ah, bh) | sh = s, (ah, bh) ~ μg X νg,w]
=	Pπ[(Sh-1, ah-1, bh-1) = z] X hφh-1(z), ψh-1(S)i
z∈S×A×B s
X E [(fh - Tμg fh+1)(sh, ah, bh) | sh = s, (ah, bh) ~ μg X νg,w]
= X	Pπ [(Sh-1 , ah-1 , bh-1 ) = z] X φh-1 (z),
z∈S ×A×B
X E [(fh - Tμ fh+ι)(sh,ah,bh) | Sh = S, (ah,bh)〜μg X Vg,w] X ψh-i(s))
s∈S
= Eπ [φh-1 (Sh-1 , ah-1 , bh-1 )],
X E [(fh - Tμg fh+ι)(Sh,ah,bh) | Sh = S, (ah,bh)〜μg X Vg,w] X ψh-i(c))，
s∈S
where the LHS only depends on π while the RHS only depends on f, g, w. By the regularization condition of Kernel
MGs, the norm of the RHS is bounded by 2R + 1.	□
F	Proofs for Appendix C
In this section, we present the proof of Theorem 18 and Corollary 19. The techniques are basically the same as those in
Appendix D. Please notice that whenever coming across DF and D∆ in this section, we use their definitions introduced
in Appendix C.
To begin with, we have the following concentration lemma (akin to Lemma 24 and 25) for the sub-routine Com-
PUTE-Exploiter under the samples collected with Option II.
Lemma 31. Under Assumption 1 and 2, if we choose β = C ∙ (log(KH ∣F∣∣G∣∕δ) + Kε20mp + Kε2eai) with some
large absolute constant c in Algorithm 1 with Option II, then with probability at least 1 - δ, for all (k, h) ∈ [K] X [H],
we have
(a)	PM E	((fk	-	Thk fh+ι)(Sh, a,	b))2	| Sh 〜∏i, (a, b)〜UnifOrm(A X	B)	≤O(β),
(b)	Pk=ι1 P(a,b)∈A×B ((fk -Tf fh+ι)(Sh,a,b))2≤O(∣A∣∣B∣β),
26
(c)	PF(QμkJ) ∈Cμ∖
where (i) πi = (μi,νi); (ii) Sh denotes the state at Step h, collected by following πi until SteP h, in the ith outer
iteration; (iii) fk denotes the optimistic function computed in sub-routine COMPUTE.Exploiter in the kth outer
iteration.
Proof. To prove (a), we only need to redefine the filtration Ft,h in Appendix D.3.3 to be the filtra-
tion induced by {z1i , r1i , . . . , zHi , rHi }it=-11 where zhi = (sih, aih, bih), and repeat the arguments therein verba-
tim. Similarly, for (b), we only need to redefine Ft,h in Appendix D.3.3 to be the filtration induced by
{z1i , r1i , . . . , zHi , rHi }it=-11 S{z1t, r1t, . . . , zht -1, rht -1, sth}. And the proof of (c) is the same as that of Lemma 25 in
Appendix D.3.4.
Proposition 32 (approximate best-response regret). Under Assumption 1 and 2, there exists an absolute constant c
such thatfor any δ ∈ (0,1], K ∈ N, ifwe choose β = c ∙(log(KH ∣F∣∣G∣∕δ)+Kε2omp + Kε2eai), then with probability
at least 1 - δ
K
X (VIAVk (si) - Vμk,t(sι)) ≤ O (HP∣A∣∣B∣kβ ∙ dimvbe(F, 1/K)).
k=1
Proof. By Lemma 31, PF (Qμk,t) ∈ C μk. Since Compute .Exploiter chooses fk optimistically, value difference
lemma (Lemma 21) gives
K
X”,νk (si) - Viμk,t (si))
k=i
K
≤ X (V"",ν" (SI)- min Dμk×νfk (SI)) + kεreal
k=i
KH
=-XX E∏k [(fk -Thμk fk+i)(sh,a, b) | (a, b)〜μh × νfk,fk,h| + kεreal
k=i h=i
H K
≤-XX E [(fk-Tlhk fh+i)(sh, a, b) | (a, b)〜μh X Vfk// + PKHlog(KH/δ) + kεgi,
h=i k=i
where the expectation in the second last line is over Sh 〜 ∏k and (a, b)〜 μ∣ X Vfk fk ∣, the expectation in the last
line is only over (a, b)〜μ∣ X Vfk fk ∣ conditioning on Sh being fixed as Sh, and the definition of νf,g is introduced
in Section 3.
By Jensen’s inequality and Lemma 31 (b), we have for all k ∈ [K]
k2
X (E [(fk - Tf fh+i)(Sh,α,b) | (α,b)〜μh X νfk,fk J)
t=i
k2
≤ X E fk-τμ fh+1) (S'h,a,b) | (a, b)〜μh X Vfkfk,h
t=i
k2
≤ X X	(fh-Th fh+) (Sth,a,b ≤O (∣A∣∣B∣β).
t=i (a,b)∈A×B
27
So we can apply Lemma 20 with G = HF,h (here, HF refers to the one introduced in Definition 17), Π = D∆,h,
= 1/K and obtain
K
X ∣E [(fhk - Tf fk+ι)(sh, a, b) I (a, b)〜〃h × Vfk	∣
k=1
≤O (q∣A∣∣B∣β ∙ dimDE(HF,h, D∆,h, 1/K) ∙ K}
Similarly by using Lemma 31 (a), we can show
K
XEnk [(fk-Tf fk+ι)(sh,a,b) |(a,b)〜μh X νfk,fk,κ∖
k=1
≤O (q∣A∣∣B∣β ∙ dimDE(HF,h, Df,h, 1/K) ∙ K).
Putting all relations together as in Proposition 26 and noticing that
kεreai ≤ O (HP∣A∣∣B∣kβ ∙dimvBE(F, 1/K))
completes the proof.	口
Equipped with the regret guarantee for COMPUTE.Exploiter, We are ready to bound the pseudo-regret for the main
algorithm Golf_with.Exploiter. To begin with, we have the following concentration lemma (akin to Lemma 22
and 23) under the samples collected with Option II.
Lemma 33. Under Assumption 1 and 2, if we choose β = C ∙ (log(KH ∣F∣∣G∣∕δ) + Kε20mp + Kε2ea]) with some
large absolute constant c in Algorithm 1, then with probability at least 1 - δ, for all (k, h) ∈ [K] X [H], we have
(a)	PM E[((f -Th fh+J(sh,a,b))2 ∣ Sh 〜πi, (a,b)〜Uniform(A X B)]≤O(β),
(b)	P31 P(α,b)∈A×B f - Thfh+J(Sh a，b))2≤O(∣A∣∣B∣β),
(c)	PF(Q?) ∈ Ck,
where Sih denotes the state at step h collected following πi until step h in the ith outer iteration.
Proof. To prove (a), we only need to redefine the filtration Ft,h in Appendix D.3.1 to be the filtra-
tion induced by {z1i , r1i , . . . , zHi , rHi }it=-11 where zhi = (Sih, aih, bih), and repeat the arguments therein verba-
tim. Similarly, for (b), we only need to redefine Ft,h in Appendix D.3.1 to be the filtration induced by
{z1i , r1i , . . . , zHi , rHi }it=-11 S{z1t, r1t, . . . , zht-1, rht-1, Sth}. And the proof of (c) is the same as that of Lemma 25 in
Appendix D.3.2.
Proof of Theorem 18. The regret can be decomposed into two terms
X(V?(si) - Vιμk,t(sι)) = X (V?(si) - Vιμk,νk(sι)) + X "k,νk(si) — ¼μk,t(sι)).
k=1	k=1	k=1
{} X{}
(A)	(B)
By Proposition 32, (B) can be upper bounded by with high probability,
K
X (Vμk,"k(si) - vμk,t(sι)) ≤ O (HP∣A∣∣B∣Kβ ∙dimVBE(F, 1/K)).
k=i
28
So it remains to control (A). By minicking the proof of Theorem 11, we have
K
X[Vι*(sι)- vμk,νk (si)]
k=1
K
≤ X^Vrf k,1(SI)- Vμ ,V(SI)]+ Kεreal
k=1
KH
=	Eπk Vfk,h(sh) - rh(sh, ah, bh) - Vf k,h+1 (sh+1) + K εreal
k=1 h=1
KH
=XXEπk mνin Dμh×ν fh (Sh)-Thfhl(sh, ah,bh)[ + Kεreal
k=1 h=1
KH
≤	Eπk fh (Sh , ah, bh) - Thfh+1 (Sh, ah, bh ) + K εreal
k=1 h=1
KH
=XXEπk (fhk -Thfhk+1)(Sh,α, b) I (a, b) 〜μh × Vfk,∕k,h] + Kεreal
k=1 h=1
H K
≤ XXEkfk -ThK+i)(sh,a,b)∖(a,b)〜μh × Vfk产,h] + O (PKHlog(KH∕δ)) + 院3，
h=1 k=1
where the expectation in the second last line is over Sh 〜 ∏k and (a, b)〜 μh × Vfk 广 九,the expectation in the last
line is only over (a, b)〜 μh × Vfk f⅛ h conditioning on Sh being fixed as s£, and the definition of Vf,g is introduced
in Section 3.
By Jensen’s inequality and Lemma 33 (b), we have for all k ∈ [K]
k2
X (E [(fk - ThfhI)(Sh a, b) I (a, b)〜μh × Vf f j)
t=1
k
≤ X E [(fk-Tfk+ι)2 (Sth, a,b) i (a,b)〜μh X Vfk,fk,h∖
t=1
k
≤X X	(fhk-Thfhk+1)2(Sth,a,b) ≤O(IAIIBIβ).
t=1 (a,b)∈A×B
So we can apply Lemma 20 with G = HF,h (here, HF refers to the one introduced in Definition 17), Π = D∆,h,
= 1∕K and obtain
K
XE (fhk -Thfhk+1)(Skh,a,
b) i (a,b)〜μh X Vfk,fk,κ∖ I
k=1
≤q∣A∣∣B∣β ∙ dimDE(HF,h, D∆,h, 1/K) ∙ K.
Similarly by using Lemma 33 (a), we can show
K
X IIEπk (fhk-Thfhk+1)(Sh,a,b) ∣ (a, b)〜μh X Vfk'j ∣
k=1
≤q∣A∣∣B∣β ∙ dimDE(HF,h,Df,h, 1/K) ∙ K.
Putting all relations together completes the proof.	□
29
By an standard online-to-batch reduction, we can also prove the sample complexity guarantee.
Proof of Corollary 19. We proceed as in the proof of Theorem 18 but take ω = /H (instead of ω = 1/K) every time
we incur Lemma 20.
Theorem 18 essentially shows
K	K
X (VT(SI)- vμk,νk (sι)) ≤ X (Vf k,1(s1)- Vrk ,νk (sι)) ≤O(H PKβ ∙∖A∖∖B∖ dimvBE (F ,e/H) + Ke).
k=1	k=1
Proposition 32 essentially shows
K	K
X (Vrk,νk (si) - V1μk,t(sι)) ≤ X (Vrk,νk (si) - minDμk×ν fl⑶))≤ O(HPKβ ∙ ∖A∖∖B∖ dimvBE(F, e/H)+Ke).
k=1	k=1
Comparing with the form in Theorem 7, now we have an additional O(Ke) term, since we take ω = e/H when
incurring Lemma 20.
Putting them together and noticing by definition Vk = Vf k,i(si) and V_k = min” Dμk×νfk (S1)， we can get
ι E _k	, H ，--------------------------------
£[Vk - Vk] ≤ O(-P∖A∖∖B∖ dimVBE(F,e/H)Kβ + e),
K	K
k=i
with probability at least 1 一 δ, where β = C ∙ (log(KH∖F∖∖G∖∕δ) + Kε2omp + Kεreaι)∙
By pigeonhole PrinPle, there must exist some k s.t. Vk 一 V_k ≤ ∆ = c0(HP∖A∖∖B∖ dimVBE(F, e/H)β∕K + e).
Therefore, the output condition must be satisfied by some k ∈ [K].
To make the right hand side order O(e + H√d(εreai + εcomp)), it suffices to take
K ≥ Ω((H2d∕e2) ∙ log(H∖F∖∖G∖d∕e)),
where d = dimVBE(F, e/H).	□
30