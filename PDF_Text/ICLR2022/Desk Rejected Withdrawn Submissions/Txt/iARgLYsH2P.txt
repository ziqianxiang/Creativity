Under review as a conference paper at ICLR 2022
Disentangled Mask Attention in Transformer
Anonymous authors
Paper under double-blind review
Ab stract
Transformer conducts self attention which has achieved state-of-the-art perfor-
mance in many applications. Multi-head attention in transformer basically gath-
ers the features from individual tokens in input sequence to form the mapping to
output sequence. There are twofold weaknesses in learning representation using
transformer. First, due to the natural property that attention mechanism would mix
up the features of different tokens in input and output sequences, it is likely that
the representation of input tokens contains redundant information. Second, the
patterns of attention weights between different heads tend to be similar, the rep-
resentation capacity of the model might be bounded. To strengthen the sequential
learning representation, this paper presents a new disentangled mask attention in
transformer where the redundant features are reduced and the semantic informa-
tion is enriched. Latent disentanglement in multi-head attention is learned. The
attention weights are filtered by a mask which is optimized by semantic cluster-
ing. The proposed attention mechanism is implemented for sequential learning
according to the clustered disentanglement objective. The experiments on ma-
chine translation show the merit of this disentangled transformer in sequence-to-
sequence learning tasks.
1	Introduction
Attention mechanism has been achieving the promising performance in different sequence-to-
sequence learning tasks. In recent years, transformer (Vaswani et al., 2017) has obtained state-of-
the-art results on sequential learning in the applications of speech recognition, machine translation,
question answering, reading comprehension, to name a few. In spite of the success of self attention
in transformer, there are still some issues which restrict the learning performance such as the in-
ference speed, computational complexity and representation redundancy, etc. In order to deal with
theses challenges, several variants of transformer were proposed by using mask attention schemes.
Accordingly, the sparse transformer (Child et al., 2019), routing transformer (Roy et al., 2021) and
reformer (Kitaev et al., 2020) were proposed to calculate the dot-product in attention by only using
a small portion of tokens. The computational complexity was reduced by applying binary mask
on attention weights. In contrast to the binary mask, the adaptively sparse transformer (Correia
et al., 2019) and the adversarial sparse transformer (Wu et al., 2020) were proposed to construct
the real-valued mask where the α-entmax function (Peters et al., 2019) was presented to filter out
redundancy features in attention. In (Fan et al., 2021), a mask attention network was built to carry
out a dynamic attention mask based on the distance between two tokens in a sequence. In addition,
the transformer (Guo et al., 2019) with Gaussian-weighted self-attention (Kim et al., 2020) was ex-
ploited by constructing an attention mask where the relations for the pairs of tokens were measured.
In addition, there were a number of works pointing out that some of attention heads were redundant
(Correia et al., 2019) or the attention weights lacking of semantic interpretation. In (Bian et al.,
2021), it was found that the similarity of attention patterns between individual heads was high in
vanilla transformer so that similar performance was obtained after pruning some attention heads.
In (Jain & Wallace, 2019), the adversarial training was applied to estimate the attention weights of
transformer, and the resulting transformer received similar outputs even the attention weights were
considerably different. This phenomenon revealed that attention weights might not contain suffi-
cient semantics. To cope with these challenges, this paper aims to increase the semantic meaning
as well as reduce the redundancy of attention weights within each head and across different heads.
A new disentangled transform is constructed through two stages. The first stage is to disentangle
the representation of attention weights within individual heads. The semantics of these heads are
1
Under review as a conference paper at ICLR 2022
represented via a latent topic model through a variational sequence-to-sequence learning (Bahuleyan
et al., 2018) based on the mixture of Gaussians as the prior model. The probabilistic clustering is per-
formed to construct a semantic mask and the mask is applied on the attention weights in latent space
for those semantically-close tokens. The real-valued clusters of attention mask are implemented to
strengthen the attention mechanism by a variational inference procedure. The second stage is to
reduce the redundancy of attention weights and disentangle the multi-head attention across various
heads. The mutual information of query vectors between two heads is calculated as the disentangle-
ment objective which is minimized to reduce the redundancy of attention patterns in attention-based
representation. There are threefold novelties compared with vanilla transformer. First, a semantic
mask on attention weights is proposed to reduce the attention redundancy and enhance the weights
for semantically similar tokens. Second, a stochastic clustering is incorporated to implement latent
disentanglement for variational attention. Third, a variational sequence-to-sequence model is car-
ried out for a probabilistic transformer. The experiments on machine translation tasks under different
scenarios illustrate the performance of the proposed semantic mask with disentanglement objective.
2	Unsupervised Representation Learning
The unsupervised learning of disentangled and semantic features is surveyed for model construction.
2.1	Disentangled learning
Disentangled representation is a line of researches (Locatello et al., 2019; Yingzhen & Mandt, 2018;
Denton & Birodkar, 2017) which aims to factorize the latent representation into several independent
low-dimensional representations by optimizing a specialized objective function. These researches
have been widely employed in various technical data such as image (Higgins et al., 2017; Chen
et al., 2018), text (John et al., 2019; Cheng et al., 2020) and voice (Yuan et al., 2021). This study
pursues the mutually independent latent variables in accordance with the variation of information
(VI). VI between variables zi and zj acts as the metric to measure the degree of independence which
is nonnegative and is defined through the notations of entropy H(∙) and mutual information I(∙, ∙) as
VI(zi, zj) = H(zi) + H(zj) - 2I(zi, zj).
(1)
Considering the disentanglement of variables zi and zj from original variable x, the triangular
inequality for these variables is held by VI(x, zi) + VI(x, zj) ≥ VI(zi, zj ). The equality only
holds when zi and zj are statistically independent. VI is closely related to mutual information
(MI) as shown in Eq. (1). Minimizing MI is comparable to maximizing VI to learn independent
components. More specifically, the objective of disentanglement for independence between zi and
Zj from X is measured by the difference D(∙) between two sides of triangular inequality as
D(x; zi, zj) = VI(x, zi) + VI(x, zj) - VI(zi, zj)
= 2(H(X) + I(zi, zj) - I(X, zi) - I(X, zj))
(2)
where H(X) is seen as a constant and the remaining MI terms are required during model optimiza-
tion. However, direct calculation of MI is intractable. There are different upper and lower bounds
of MI provided in (Tschannen et al., 2020). These bounds are feasible to find the estimators of MI
to build information-theoretic objectives without the calculation of true value of MI. Latent disen-
tanglement is performed by minimizing D(X; zi, zj), or equivalently minimizing the upper bound
of I(zi, zj) and simultaneously maximizing the lower bounds of I(X, zi) and I(X, zj).
2.2	Variational clustering
Based on the latent disentanglement, this paper presents the semantic mask attention where varia-
tional clustering in neural network is performed. In (Jiang et al., 2017), an Gaussian mixture model
(GMM) was introduced to carry out the variational deep embedding where the distribution of latent
embedding in neural network was characterized. Each latent sample z of observation X belongs
to a cluster C according to an GMM p(z) = Pcp(c)p(z∣c) = Pc 不CN(μZ, diag{(σZ)2}) where
πz = {∏C} ∈ Rnc denotes the weights of nc clusters, μz = {μC} ∈ Rnc×d denotes the mean
vectors, and (σz)2 = {(σcz)2} ∈ Rnc×d denotes the set of variance entries of diagonal matrices.
A variational clustering method is developed via variational inference based on a special type of
2
Under review as a conference paper at ICLR 2022
variational autoencoder (Kingma & Welling, 2014) by maximizing the likelihood of training data x
which is encoded as feature vector z in latent space where z is modeled by an GMM and used for
reconstruction of x in decoder. The loss is derived as a negative evidence lower bound (ELBO)
log p(x) = log	p(x, z, c)dz
c	(3)
≥ Ez[log p(x|z)] - KL(q(z|x)kp(z|c)) - KL(q(c|x)kp(c)), -LELBO.
In Eq. (3), the first term represents a reconstruction objective for x under a latent variable model
and the remaining terms imply the Kullback-Leibler (KL) divergence due to latent variables z and c
driven by a variational distribution q(z, c|x) which is close to a prior of GMM using p(z|c) and p(c)
through KL minimization. Here, c denotes a latent cluster of z corresponding to an input sequence
x. Overall, the variational clustering is implemented as a new type of VAE where a mixture of
Gaussians is adopted as the variational distribution q(z, c|x). This method is feasible to build a deep
clustering and embedding model to represent similar samples with the same semantic topics which
can be developed and employed in semantic mask attention based on transformer.
↑
Inputs
Output Probability
Masked DMA
Semantic Clustenng
Output Embedding
Outputs (shifted right)
Figure 1: Architecture of transformer driven by the disentangled mask attention.
Feed ForWard
3	Disentangled Mask Attention
This paper presents the disentangled mask attention (DMA) to build a disentangled transformer as
shown in Figure 1 where DMA is implemented to replace the attention module in encoder and de-
coder of vanilla transformer. This DMA is constructed with three schemes. First, the semantic mask
attention is presented to enhance the semantic meaning of attention weights via latent clustering.
Second, the disentangled attention heads are calculated by maximizing the independence among
attention heads for the queries q of words x. Third, the information-theoretic disentanglement is
implemented to carry out a variational learning procedure of transformer.
3.1	Semantic mask attention
Given a source sequence x and a target sequence y, a new variational transformer for sequence-to-
sequence learning is presented by merging GMM to express the prior distribution of latent variables
z. By extending the unsupervised learning from data x using p(x) based on VAE with GMM for z
in Eq. (3), this paper presents a novel latent variable model for transformer based on the supervised
learning by minimizing the sequence-to-sequence (S2S) classifier loss or the negative ELBO of
conditional likelihood p(y|x) of training data {x, y}
Ls2s = -	EZn,h~q(zn'h |x) logp(y|zn,h,x) +KL(q(zn,h,czn,h|x)kp(zn,h,czn,h))
(4)
n
3
Under review as a conference paper at ICLR 2022
where zn,h is the feed-forward network output of the nth transformer block or layer after applying
the head separation for multi-head attention, h is the head index, and czn,h denotes the semantic
clusters of block n and head h. In this latent variable model, there are two latent variables zn,h
and czn,h. The stochastic gradient variational Bayes estimator (Kingma & Welling, 2014) is applied
by using the reparameterization trick to draw latent sample zn,h from the variational distribution
q(zn,h |x) where the Gaussian parameters are calculated by the transformer layer. Alternatively, the
probability of latent sample czn,h corresponding to zin,h of token xi for cluster c is calculated as
p(czh
c|zih)
∏hN(Zhlμh, diagYσh)2})
Pco πh N(Zh面,diag{(σh)2})
(5)
Overall, the parameters of transformer layer and GMM {πh μhh, (σhh)2} are trained by minimizing
Ls2s in Eq. (4). For ease of expression, the block or layer index n is ignored hereafter. Through the
estimated prior of latent variable c using GMM, the semantic relation between two latent vectors Zih
and Zjh associated with word tokens xi and token xj is characterized by constructing the semantic
mask M = {Mihj} based on the clustering probability of tokens belonging to the same cluster c
Mihj
Pc P(Ch = CIZh)P(Ch = CIZh)
PjPcO P(Ch = c0|Zh)P(Ch = c0|Zh)
(6)
This calculation measures a probabilistic correlation between xi and xj driven and integrated by
different semantic clusters C. This semantic mask is helpful to enrich the semantic information of
attention weight as Aj by using Mj as the soft mask of original attention weight Aj in a form of
((khj )T qh)∕pdk 力	(7)
Mj Aj
Pj0 MjO AjO
where Aihj = Softmax
where attention weight Aihj is calculated by dot-product of query qih = Wqhxi + bqh and key
kjh = Wkhxj + bkh transformed by the parameters {Wqh, bqh} and {Wkh, bkh}. A softmax function is
calculated over k1h:J for the set of J key tokens of head h, where kjh ∈ Rdk, and then retrieved by
jth entry. Empirically, the training of this semantic mask attention can be improved by an interpola-
tion scheme to find the final attention for each word pair (i,j) as Aj = (1 - Y)Aj + YAj where
γ = 1 - max(1 -α, e5×10-4t) is an annealing mixing rate which is controlled a hyperparameter α at
each learning iteration t. New weights {Abihj} are used to implement a semantic-aware transformer.
3.2	Disentangled Attention Heads
Semantic mask attention enhances the semantic meaning of attention weights in each head by using
the estimated M = {Mihj}. However, different heads are likely similar such that the redundancy
in multi-head representation does exist. Model capacity is restricted accordingly. To alleviate this
issue, this paper implements a new variant of attention mechanism, called the disentangled mask
attention (DMA), as shown in Figure 2 which focuses on the disentanglement of query vectors qh
in multi-head attention method. The disentangled queries are used to implement the semantic mask
attention. The disentanglement objective is constructed by extending the measure of independence
over two latent vectors in Eq. (2) to that over multiple query vectors for nh heads
nh nh
D(x;qh=1,...,qh=nh)=X XI(qh,qh0)-I(x,qh),LD	(8)
h=1 h0 6=h
where nh is the number of attention heads and qh = {qih} denotes all queries of tokens x = {xi} in
head h. Latent disentanglement is performed by minimizing Eq. (8) which correspondingly pursues
the semantic independence for individual queries {qih} across different heads h. This is because
that the semantic mask attention is applied by using those queries driven by the semantic clusters
or topics of the words via GMM. The independence of queries qh = {qih } is enhanced, and the
redundancy of attention weights across different heads is accordingly reduced.
In the implementation, the exact computation of MI terms I(qh, qh0 ) and I(qh, x) is intractable.
Finding the estimator of MI is required. To pursue independence, the minimization of disentangle-
4
Under review as a conference paper at ICLR 2022
Semantic
clustering
Clustering (C编
Clustering (Cq)
mask(Λf1)
mask(M2)
mask(ΛΓ⅛)
Figure 2: Implementation procedure of the disentangled mask attention, where the procedure details
of key k is omitted in figure, nq and nk denotes the number of query and key tokens, respectively.
First, the input xih of token i in head h is clustered by semantic clusters czh. The clustering probability
is used for constructing semantic mask M in mask attention indicated by dotted line. After, xih is
transformed to query qih by parameters {Wqh , bqh}, and is clustered by a set of additional clusters
cq for the disentanglement of qih in different head. Finally, the semantic mask M is applied on the
attention weight Ah calculated by qh and kh to construct a new attention weight.
> q?=^x?+b；
ment objective in Eq. (8) is implemented by minimizing the upper bound of the first MI term
Γ 1 nq (	1	nq	∖↑
I(qh,qh) ≤E / 工 logp(qhIqh)- 7二γ £logPgh|qh )	, LDqq	⑼
nq i=1	nq - 1 j 6=i
and simultaneously maximizing the lower bound of the second MI term (Poole et al., 2019)
I(XH) ≥E" n1qX (log nq Pnff:qh), Xj)) !# , LDxq	(10)
where nq is the number of samples in qh or qh0 . In Eq. (9), a variational leave-one-out upper bound
of MI between qh and qh0 is calculated. The conditional probabilities p(qih |qih0) and p(qih|qjh0)
can be estimated by a trainable neural network. In Eq. (10), a variational lower bound is measured
by using a critic functionf(qih , Xj) which identifies the semantic relation between qih and Xj. In
this study, the critic function is defined byf(qih, Xj) , c p(cq = c|qih)p(cq = c|qjh) which
reflects an integrated correlation over different semantic clusters c. The probabilistic correlation
between qih and Xj under the same cluster or latent topic cq = c is measured. Importantly, a new
GMM is introduced to express the prior of latent query as p(qh) = Ee ∏qN(μq, diag{(σq)2})
With parameters {∏q, μq, (σq)2}. The posterior probability of the cluster of query p(cq = c∣qh) is
computed similar to that of transformer block output p(czh = c|zih) as shown in Eq. (5).
3.3	Learning criteria
This study presents the semantic mask attention based on the disentangled attention heads. A disen-
tangled mask attention (DMA) is proposed for sequence-to-sequence learning via transformer. There
are four latent variables in such a Bayesian hierarchical model. The first level involves latent vari-
ables of transformer layer outputs and semantic clusters in different heads and layers {zn,h , czn,h }
While the second level consists of queries and query clusters in different layers {qn,h , cqn}. Atten-
tion heads are disentangled over queries qh across different heads h in the same layer n. The latent
5
Under review as a conference paper at ICLR 2022
variable model is trained by minimizing the negative ELBO or sequence-to-sequence classifier loss
Ls2s = 一〉:E(zn,h ,qn,h)〜q(zn,h ,qn,h |x) log p(y|zn,h, qn,h, x)
+ KnL(q(zn,h, czn,h|x)kp(zn,h, czn,h)) + KL(q(qn,h, cqn|x)kp(qn,h, cqn))
which is extended from Eq. (4). An additional KL term is derived to regularize the estimated
variational distribution q(qih|x) which is close to its shared GMM prior p(qih). Importantly, the
disentanglement of attention weights Aij is performed via that of queries qh over the groups of
queries across different heads h. A new type of disentangled transformer is fulfilled by minimizing
an information-theoretic objective using the estimators of MI or the upper bound LDqq and lower
bound LDxq of MI. Disentanglement objective is formed by LD = LDqq - LDxq where p(qih|qih0)
in LDqq is calculated by integrating the probabilities of queries {qih, qih0} under the same cluster c
p(qih|qih0) =Xp(qih|cq = c)p(cq = c|qih0).	(12)
c
In Eq. (12), p(cq = c|qih0) is computed by referring Eq. (5) and the first term is computed via GMM
p(qih|cq = c)
P(Cq = CIqh)Pgh)
Pi，P(Cq = c|qhO)PghO)
(13)
In addition, the loss functions LDqq and LDxq in Eqs. (9) and (10) are calculated over all queries
over nh heads including nq samples in each head. In this study, the diversity of semantic cluster-
ing is further enhanced and regularized towards increasing the probabilistic correlation within each
variable zih via Pc P(Czh = c|zih)P(Czh = c|zih) and simultaneously decreasing the correlation be-
tween variables zih and zjh via Pc P(Czh = c|zih)P(Czh = c|zjh), accordingly different samples zih and
zjh likely go to different clusters c. An objective to enhance the cluster diversity is calculated by
summing up all entries (i, j) of the squared values of a matrix (Lin et al., 2017) shown below
Ldv=n X X(CC T - I )2,j
qi j
(14)
where I is a nq × nq identity matrix, C = [Cic] and Cic , P(Czh = c|zih) for GMM of zih, and
Cic , P(Cq = c|qih) for GMM of qih. The overall loss L for the transformer with disentangled
mask attention is composed of classification loss, disentanglement loss and diversity loss as L =
Ls2s + LD +Ldv where the regularization parameters are adopted in three objectives. The parameters
of the encoder based on transformation of query, key and value {Wqh, bqh, Wkh, bkh, Wvh, bvh} and the
GMMs {∏Z, μZ, (σC)2,∏q, μC, (σg)2} are estimated by finding the corresponding gradients over L.
4	Experiments
In the experiments, three machine translation tasks including IWSLT’14 De-En, WMT’14 En-De
and WMT’17 Zh-En were used to evaluate the performance of the proposed model. All models in
the following experiments were trained and evaluated by using Fairseq (Ott et al., 2019) toolkit.
4.1	Dataset descriptions
IWSLT’14 De-En contained 167K of German and English sentence pairs, and WMT’14 En-De
contained 4.5M of English and German sentence pairs. For WMT’17 Zh-En, only the data in ‘news-
commentary-v12’ set were collected for training. There were roughly 212K of Chinese and En-
glish sentence pairs in WMT’17 Zh-En. The byte-pair-encoding (BPE) dictionary of the models in
WMT’14 En-De were shared between encoder and decoder. For the other two tasks, encoder and
decoder have independent BPE dictionary. In addition, Jieba1 was used as the tokenization tool for
Chinese sentences.
1https://github.com/fxsjy/jieba
6
Under review as a conference paper at ICLR 2022
4.2	Evaluation metrics
The performance of translation models were evaluated in terms of BLEU (Papineni et al., 2002)
score. The BLEU score of models in IWSLT’14 De-En and WMT’17 Zh-En was calculated by
using the evaluation script2 provided by Fairseq. The BELU score of models in WMT’14 En-De
was evaluated after applying compound splitting3, similar to the setting in (Wu et al., 2019).
In addition, two metrics were introduced to measure the redundancy of attention weights based
on the Jensen-Shannon (JS) distance (Correia et al., 2019; Bian et al., 2021). One is the layer
redundancy (LR), LR，N(PN=ι(log2 n - n^ Pi JS(An,h=1,...,An,h=nh))), and the other is
the head redundancy (HR), HR，H PNI: PZh2 nq Pi(I- JS(An1,h1, Anmh)) where N
is the number of transformer layers, and Ai denotes the ith row of attention matrix A. LR measures
the similarity of attention weights between different attention heads in the same layer while HR
measures the similarity between each attention head in whole model. The lower the redundancy
metric, the larger the difference of attention weights between each head.
4.3	Model Configurations
The vanilla transformer was used as baseline model. All settings in transformer and the proposed
DMA transformer were identical for fair comparison. All models in the experiments were com-
posed of N = 6 layers of encoder and decoder. To reduce the computational cost, the term
p(qih|qjh0) in LDqq was neglected, and q(zih|x) and q(qih|x) were estimated by a single Gaussian
with a constant variance σ2 = 0.1 and zih and qih as means, i.e. q(zih|x) = N (zih, diag{σ2}) and
q(qih|x) = N(qih, diag{σ2}). In addition, the semantic mask was disregarded in the transformer
block of masked DMA in test phase. DMA transformer worked well under this setting. In order to
rapid the convergence of GMMs, the gradient of cluster centroid was multiplied by a constant cgrad
during training. To evaluate the proposed model under different model sizes, DMA transformer was
built with three configuration types, which were base, small, and tiny. The only difference between
these configurations was the size of attention and feed-forward layers. In loss calculation, there were
three regularization parameters cqq, cxq, and ckl which were used to control the contributions of loss
terms LDqq and LDxq, and KL terms in Ls2s. The details of settings are listed in Appendix A.2.
4.4	Experimental Results
Comparison on model performance: The experiment results on three tasks are shown and com-
pared. Generally speaking, the proposed DMA transformer achieved higher BLEU score than base-
line transformer in various tasks. In IWSLT’14 De-En (Table 1), DMA transformer absolutely im-
proved 0.8 BLEU score over transformer. In WMT’14 En-De (Table 2), DMA transformer outper-
formed transformer by 0.6 BLEU score. In WMT’17 Zh-En (Table 3), DMA transformer achieved
0.37 BLEU score higher than transformer. These results illustrate that DMA transformer performs
better than other models for translation with grammatically similar and different languages.
Model	Params	BPE	nc	LR	HR	BLEU
Transformer (base, (Mehta et al., 2021))	42.0M (1.06x)	10K	-	-	-	34.30
Transformer (base, ours)	39.5M (1.00x)	6K/8K	-	0.74	0.65	34.50
DMA transformer (base)	39.7M (1.00x)	6K/8K	8	0.62	0.53	35.31
DeLighT (Mehta et al., 2021)	14.0M (0.35x)	10K	-	-	-	33.80
DMA transformer (small)	26.1M (0.66x)	6K/8K	4	0.64	0.57	35.00
DMA transformer (tiny)	11.9M (0.30x)	6K/8K	4	0.63	0.58	34.96
Table 1: Experimental results on IWSLT’14 De-En translation task. ‘BPE’ denotes the number of
tokens in the dictionary of encoder and decoder (encoder/decoder). ‘Ours’ denotes the model is
trained by ourselves. ‘Params’ denotes the number of parameters in model.
2https://github.com/pytorch/fairseq/blob/main/fairseq_cli/generate.py
3https://github.com/pytorch/fairseq/blob/main/sCriPts/compound.SPlit_bleu.sh
7
Under review as a conference paper at ICLR 2022
Model	Params	BPE	LR	HR	BLEU
Transformer (base, (Mehta et al., 2021))	67.0M(1.01x)	44K	-	-	27.70
Transformer (base, ours)	66.5M (1.00x)	44K	0.79	0.69	27.75
DMA transformer (base)	66.6M(1.00x)	44K	0.73	0.58	28.35
DeLighT (Mehta et al., 2021)	54.0M (0.81x)	44K	-	-	28.00
DMA transformer (small)	46.4M (0.70x)	44K	0.70	0.57	28.16
Transformer (big, (Vaswani et al., 2017))	213.0M (3.20x)	37K	-	-	28.40
Table 2: Experimental results on WMT’14 En-De translation task. DMA with nc = 4 is used.
Model	Params	BPE	nc	LR	HR	BLEU
Transformer (ours)	55.0M (1.00x)	25K/20K	-	0.71	0.60	12.76
DMA transformer	55.1M (1.00x)	25K/20K	4	0.62	0.55	13.13
Table 3: Experimental results on WMT’17 Zh-En translation task with very different languages.
Analysis on model size: The number of additional parameters introduced by proposed methods
was only 1% more compared to vanilla transformer. In IWSLT’14 De-En (Table 1), DMA trans-
former (tiny) achieved 0.46 BLEU score higher than transformer by using only 30% of parameters.
Compared to another state-of-the-art model, DeLighT (Mehta et al., 2021), with smallest model size,
DMA transformer (tiny) obtained about 1.16 BLEU score higher with using 5% less of parameters.
In WMT14’En-De (Table 2), DMA transformer (small) achieved 0.41 BLEU score higher than trans-
former with using 30% less of parameters. On the other hand, the performance of DMA transformer
(base) is close to transformer (big) with slightly 0.05 drop in terms of BLEU score, but using only
one third of the size of parameters. In conclusion, introducing the objectives of variational cluster-
ing and disentangled attention heads into transformer does strengthen the latent representation of
sequences so that similar or even higher performance can be achieved by using much smaller model.
Analysis on attention redundancy: Figure 3 depicts the head redundancy between individual
attention heads in encoder. The similarity of DMA transformer attention weights between each
head is generally smaller than that of transformer. Diversity is improved. In addition, LR and HR
metrics of DMA transformer in three translation tasks are lower than transformer by 0.05 to 0.1,
respectively, while the performance is improved. These results illustrate that the proposed methods
could encourage different attention heads identifying different semantic relations between individual
tokens. The model performance is benefited from the reduction of attention redundancy.
Figure 3: Head redundancy in encoder of transformer (left) and DMA transformer (right), where
IWSLT’14 De-En test set is used for evaluation. The head index in x-axis and y-axis are accumulated
from first head in first layer to last head in last layer.
Analysis on semantic clustering: Figure 4 depicts top 15 tokens captured by one of clusters
in DMA transformer, where the punctuations and common stopwords are removed. This cluster
captures the semantic meaning of topic words, which can be observed from the followings tokens,
‘es’, ‘ing’, ‘ed’, ‘ies’. In addition, Figure 5 depicts the clustering probability distribution and the
corresponding semantic mask of another cluster in DMA transformer. With the help of applying Ldv,
each token was clustered into different clusters without collapsing into same cluster. In the figure
of corresponding semantic mask, the mask was composed of several rectangles. This indicates this
set of semantic clusters were learned to find out and enrich the local features. In addition, we also
8
Under review as a conference paper at ICLR 2022
found there were some semantic masks formed to identifying the semantic relation between token
in longer distance.
Figure 4: Top 15 tokens captured by the second cluster in the first head of the last decoder layer by
using DMA transformer, where the text in x-axis shows the BPE tokens, and IWSLT’14 De-En test
set is used for evaluation.
Figure 5: Clustering probability distribution (left) and the corresponding semantic mask (right) in
the 1st head of 6th DMA transformer encoder layer, where the BPE tokens in two axes of (right) are
same as the tokens in x-axis of (left), and IWSLT’14 De-En test set is used for evaluation.
Analysis on latent representations: Figure 6 depicts the distribution of query on latent space
by using t-SNE for dimension reduction. The gaps between queries of different heads in DMA
transformer are larger than transformer, and the distribution is more diverse within the same head.
The head redundancy is reduced because the proposed methods increase the semantic diversity of
query vectors in same and different heads, and this also improves the performance of model.
Figure 6: Query distribution of 104 sentences (13,312 tokens) in the last decoder layer of transformer
(left) and DMA transformer (right), where the point with different colors indicates the query of
different heads, and IWSLT’14 De-En test set is used for evaluation.
5 Conclusions
This paper presented a variant of transformer, DMA transformer, which replaced the vanilla atten-
tion with the disentangled mask attention. This variational attention represented the prior probability
distribution of feed-forward output and query as the mixture of Gaussians, constructed the semantic
mask based on the corresponding clustering probability, and optimized the model with the objec-
tive of disentangled attention heads. Experimental results showed that the redundancy of attention
weight was reduced, and the semantic diversity of query within same head and between different
heads was increased. By applying the proposed methods, the compact DMA transformer outper-
formed other transformers in different translation tasks either with identical or smaller model size.
9
Under review as a conference paper at ICLR 2022
References
Hareesh Bahuleyan, Lili Mou, Olga Vechtomova, and Pascal Poupart. Variational attention for
sequence-to-sequence models. In Proc. of International Conference on Computational Linguis-
tics ,pp.1672-1682, 2018.
Yuchen Bian, Jiaji Huang, Xingyu Cai, Jiahong Yuan, and Kenneth Church. On attention redun-
dancy: A comprehensive study. In Proc. of Conference of North American Chapter of Association
for Computational Linguistics: Human Language Technologies, pp. 930-945, 2021.
Ricky TQ Chen, Xuechen Li, Roger Grosse, and David Duvenaud. Isolating sources of disentangle-
ment in vaes. In Proc. of International Conference on Neural Information Processing Systems,
pp. 2615-2625, 2018.
Pengyu Cheng, Martin Renqiang Min, Dinghan Shen, Christopher Malon, Yizhe Zhang, Yitong
Li, and Lawrence Carin. Improving disentangled text representation learning with information-
theoretic guidance. In Proc. of Annual Meeting of Association for Computational Linguistics, pp.
7530-7541, 2020.
Rewon Child, Scott Gray, Alec Radford, and Ilya Sutskever. Generating long sequences with sparse
transformers. arXiv preprint arXiv:1904.10509, 2019.
Goncalo M. Correia, Vlad Niculae, and Andre F. T. Martins. Adaptively sparse transformers. In
Proc. of Conference on Empirical Methods in Natural Language Processing, pp. 2174-2184,
2019.
Emily Denton and Vighnesh Birodkar. Unsupervised learning of disentangled representations from
video. In Proc. of International Conference on Neural Information Processing Systems, pp. 4417-
4426, 2017.
Zhihao Fan, Yeyun Gong, Dayiheng Liu, Zhongyu Wei, Siyuan Wang, Jian Jiao, Nan Duan, Ruofei
Zhang, and Xuanjing Huang. Mask attention networks: Rethinking and strengthen transformer.
In Proc. of Conference of North American Chapter of Association for Computational Linguistics:
Human Language Technologies, pp. 1692-1701, 2021.
Maosheng Guo, Yu Zhang, and Ting Liu. Gaussian transformer: A lightweight approach for natural
language inference. Proc. of AAAI Conference on Artificial Intelligence, 33:6489-6496, 2019.
Irina Higgins, Loic Matthey, Arka Pal, Christopher Burgess, Xavier Glorot, Matthew Botvinick,
Shakir Mohamed, and Alexander Lerchner. β-VAE: Learning basic visual concepts with a con-
strained variational framework. In Proc. of International Conference on Learning Representa-
tions, 2017.
Sarthak Jain and Byron C. Wallace. Attention is not explanation. In Proc. of Conference of North
American Chapter of Association for Computational Linguistics: Human Language Technologies,
pp. 3543-3556, 2019.
Zhuxi Jiang, Yin Zheng, Huachun Tan, Bangsheng Tang, and Hanning Zhou. Variational deep
embedding: an unsupervised and generative approach to clustering. In Proc. of International
Joint Conference on Artificial Intelligence, pp. 1965-1972, 2017.
Vineet John, Lili Mou, Hareesh Bahuleyan, and Olga Vechtomova. Disentangled representation
learning for non-parallel text style transfer. In Proc. of Annual Meeting of the Association for
Computational Linguistics, pp. 424-434, 2019.
Jaeyoung Kim, Mostafa El-Khamy, and Jungwon Lee. T-GSA: Transformer with gaussian-weighted
self-attention for speech enhancement. In Proc. of International Conference on Acoustics, Speech
and Signal Processing, pp. 6649-6653, 2020.
Diederik P. Kingma and Max Welling. Auto-encoding variational Bayes. In Proc. of International
Conference on Learning Representations, 2014.
Nikita Kitaev, Lukasz Kaiser, and Anselm Levskaya. Reformer: The efficient transformer. In Proc.
of International Conference on Learning Representations, 2020.
10
Under review as a conference paper at ICLR 2022
Zhouhan Lin, Minwei Feng, Cicero Nogueira dos Santos, Mo Yu, Bing Xiang, Bowen Zhou, and
Yoshua Bengio. A structured self-attentive sentence embedding. In Proc. of International Con-
ference on Learning Representations, 2017.
Francesco Locatello, Stefan Bauer, Mario Lucic, Gunnar Raetsch, Sylvain Gelly, Bernhard
Scholkopf, and Olivier Bachem. Challenging common assumptions in the unsupervised learn-
ing of disentangled representations. In Proc. of International Conference on Machine Learning,
pp.4114—4124, 2019.
Sachin Mehta, Marjan Ghazvininejad, Srinivasan Iyer, Luke Zettlemoyer, and Hannaneh Hajishirzi.
DeLighT: Deep and light-weight transformer. In Proc. of International Conference on Learning
Representations, 2021.
Myle Ott, Sergey Edunov, Alexei Baevski, Angela Fan, Sam Gross, Nathan Ng, David Grangier, and
Michael Auli. FAIRSEQ: A fast, extensible toolkit for sequence modeling. In Proc. of Conference
of North American Chapter of Association for Computational Linguistics: Demonstrations, pp.
48-53, 2019.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. BLEU: a method for automatic
evaluation of machine translation. In Proc. of Annual Meeting of the Association for Computa-
tional Linguistics, pp. 311-318, 2002.
Ben Peters, Vlad Niculae, and Andre F. T. Martins. Sparse SeqUence-to-sequence models. In Proc.
of Annual Meeting of Association for Computational Linguistics, pp. 1504-1519, 2019.
Ben Poole, Sherjil Ozair, Aaron Van Den Oord, Alex Alemi, and George Tucker. On variational
bounds of mutual information. In Proc. of the International Conference on Machine Learning,
pp. 5171-5180, 2019.
Aurko Roy, Mohammad Saffar, Ashish Vaswani, and David Grangier. Efficient content-based sparse
attention with routing transformers. Transactions of the Association for Computational Linguis-
tics, pp. 53-68, 2021.
Michael Tschannen, Josip Djolonga, Paul K. Rubenstein, Sylvain Gelly, and Mario Lucic. On mu-
tual information maximization for representation learning. In Proc. of International Conference
on Learning Representations, 2020.
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,
Eukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Proc. of International Confer-
ence on Neural Information Processing Systems, pp. 6000-6010, 2017.
Felix Wu, Angela Fan, Alexei Baevski, Yann Dauphin, and Michael Auli. Pay less attention with
lightweight and dynamic convolutions. In Proc. of International Conference on Learning Repre-
sentations, 2019.
Sifan Wu, Xi Xiao, Qianggang Ding, Peilin Zhao, Ying Wei, and Junzhou Huang. Adversarial
sparse transformer for time series forecasting. In Proc. of International Conference on Neural
Information Processing Systems, pp. 17105-17115, 2020.
Li Yingzhen and Stephan Mandt. Disentangled sequential autoencoder. In Proc. of International
Conference on Machine Learning, pp. 5670-5679, 2018.
Siyang Yuan, Pengyu Cheng, Ruiyi Zhang, Weituo Hao, Zhe Gan, and Lawrence Carin. Improving
zero-shot voice style transfer via disentangled representation learning. In Proc. of International
Conference on Learning Representations, 2021.
11
Under review as a conference paper at ICLR 2022
A Experimental details
A. 1 Statistics of dataset
The number of sentence pairs after preprocessing in WMT’14 En-De, IWSLT’14 De-En and
WMT’17 Zh-En are listed in Table 4.
Task	Train	Validation	Test
IWSLT’14 De-En	160K	7K	7K
WMT’14 En-De	3.9M	39K	3K
WMT’17 Zh-En	208K	2K	2K
Table 4: Number of sentence pairs in different translation tasks.
A.2 Configuration of models
The details of model configuration in three translation tasks are listed in Table 5, where ckl = 0.01
in all configurations. Before evaluation, the weights of model were averaged over last 5 epochs.
During evaluation, beam search was applied on the predicted output with beam size 5 for all tasks.
Task	Configuration	Embed/FFN	nh	cqq	cxq	cgrad	Iterations
WMT’14 En-De	base	512/2048	8	100	10	10	196K
	small	384/2048	8	100	10	10	313K
WMT’17 Zh-En	-	512/1024	4	50	5	5	79K
	base	512/1024	4	100	10	10	55K
IWSLT’14 De-En	small	384/1024	4	100	10	10	55K
	tiny	256/512	4	50	5	10	99K
Table 5: Model configurations of DMA transformer in different tasks. Embed and FFN denotes the
dimension of hidden layer in attention and feed-forward layers, respectively. Iterations denotes the
number of parameters updates.
A.3 Ablation on dictionary size
The ablation study was established by varying the size of dictionary in WMT’14 En-De. The model
with larger dictionary tends to have better performance. From results in Table 6, DMA transformer
with smaller dictionary obtained 0.19 BLEU score higher than transformer with larger dictionary,
while using 5% less of parameters. This is due to the semantic information of DMA transformer
is enhanced by semantic clustering, so that similar performance as transformer can be achieved by
DMA transformer with using smaller number of parameters.
Model	Params	BPE	nc	LR	HR	BLEU
Transformer (base, ours)	63.08M(1.00x)	37K	-	0.79	0.70	27.44
DMA transformer (base)	63.16M(1.00x)	37K	4	0.73	0.61	27.94
Transformer (base, ours)	66.52M(1.05x)	44K	-	0.79	0.69	27.75
DMA transformer (base)	66.61M (1.06x)	44K	4	0.73	0.58	28.35
Table 6: Ablation study of dictionary size on WMT’14 En-De translation task.
A.4 Ablation on cluster numbers
The ablation study was established by varying the number of clusters in IWSLT’14 De-En. The
BLEU score of DMA transformer is slightly impacted by different number of clusters within the
12
Under review as a conference paper at ICLR 2022
range of 0.1. From the trend in Table 7, suitable number of clusters is required to achieve best
performance. Fewer or more number of clusters may make the clusters fail to capture the semantic
meaning or some clusters become redundant.
Model	Params	BPE	nc	LR	HR	BLEU
DMA transformer (base)	39.56M(1.00x)	6K/8K	4	0.63	0.52	35.22
DMA transformer (base)	39.65M (1.00x)	6K/8K	8	0.62	0.53	35.31
DMA transformer (base)	39.74M (1.01x)	6K/8K	12	0.62	0.53	35.29
Table 7: Ablation of cluster numbers nc on IWSLT’14 De-En translations task.
B Experimental Analysis
B.1	Additional analysis on latent representations
Figure 7 and Figure 8 depict the query distribution of transformer or DMA transformer in WMT’14
En-De and WMT’17 Zh-En translation tasks, where t-SNE was used for dimension reduction. Sim-
ilar to query distribution in IWSLT’14 De-En (Figure 6), the query vectors in DMA transformer
within same head or between different heads were more diverse than transformer. And the gaps
between different heads were larger. In addition, by using either 4 heads in WMT’17 Zh-En or 8
heads in WMT’14 En-De, the semantic diversity of query were all increased by proposed methods.
Figure 7: Query distribution of 88 sentences (30,272 tokens) in last decoder layer of transformer
(left) and DMA transformer (right), where WMT’14 En-De test set is used for evaluation.
Figure 8: Query distribution of 72 sentences (15,552 tokens) in last decoder layer of transformer
(left) and DMA transformer (right), where WMT’17 Zh-En test set is used for evaluation.
13
Under review as a conference paper at ICLR 2022
B.2	Additional analysis on attention redundancy
Figure 9 and Figure 10 depict the head redundancy of transformer or DMA transformer encoder in
WMT’14 De-En and WMT’17 Zh-En translation tasks. Similar to the results in IWSLT’14 De-En
(Figure 3), head redundancy of DMA transformer with either 4 or 8 attention heads were all smaller
than transformer. These results illustrates the proposed methods successfully reduce the attention
redundancy between each head and work for any number of attention heads.
Figure 9: Head redundancy in encoder of transformer (left) and DMA transformer (right), where
WMT’14 En-De test set was used for evaluation.
Figure 10: Head redundancy in encoder of transformer (left) and DMA transformer (right), where
WMT’17 En-De test set is used for evaluation.
B.3 Additional analysis on semantic clustering
Figure 11 depicts the top 15 tokens captured by one cluster of DMA transformer in WMT’14 En-
De. The semantic meaning of topic words can be observed from following tokens, ‘year’, ‘time’,
‘Thursday’, and ‘times’.
Figure 11: Top 15 tokens captured by the second cluster in the first head of the 5th encoder layer by
using DMA transformer, where WMT’14 En-De test set is used for evaluation.
Figure 12: Clustering probability distribution (left) and the semantic mask (right) in the 1st head of
2nd DMA transformer encoder layer, where WMT’14 En-De test set is used for evaluation.
14
Under review as a conference paper at ICLR 2022
p-z出 snu
Figure 13: Clustering probability distribution (left) and the semantic mask (right) in the 1st head of
6th DMA transformer encoder layer, where WMT’14 En-De test set is used for evaluation.
Figure 12 and Figure 13 depict the probability distribution of semantic clustering and corresponding
semantic mask of different DMA transformer encoder layers in WMT’14 En-De. The semantic
mask in Figure 12 captures the local dependencies, while the semantic mask in Figure 13 captures
the semantic dependency in longer distance.
15