Quantum Alphatron
Anonymous authors
Paper under double-blind review
Ab stract
Finding provably efficient algorithms for learning neural networks is a fundamental challenge in
the theory of machine learning. The Alphatron of Goel and Klivans is the first provably efficient
algorithm for learning neural networks with more than one nonlinear layer. The algorithm succeeds
with any distribution on the n-dimensional unit ball and without any assumption on the structure of
the network. In this work, we refine the original Alphatron by a pre-computing phase for its most
time-consuming part, the evaluation of the kernel function. This refined algorithm improves the run
time of the original Alphatron, while retaining the same learning guarantee. Based on the refined al-
gorithm, we quantize the pre-computing phase with provable learning guarantee in the fault-tolerant
quantum computing model. In a well-defined learning model, this quantum algorithm is able to
provide a quadratic speedup in the data dimension n. In addition, we discuss the second type of
speedup, quantizing the evaluation of the gradient in the stochastic gradient descent procedure. Our
work contributes to the study of quantum learning with kernels and from samples.
1	Introduction
Machine learning is highly successful in a variety of applications using heuristic approaches even though the methods
being used are often without strong guarantees on their learning performance. Important questions are why common
machine learning algorithms such as stochastic gradient descent and kernel methods (ScholkoPf & Smola, 2002) work
well and what is the best way to interpret the results. Computational learning theory addresses some of the fundamental
theoretical questions and Provides a systematic framework to discuss Provable learning of Probability distributions and
machine learning architectures (such as neural networks).
Consider the non-convex sigmoid activation function for which the square loss function shows exPonentially many
local minima (Auer et al., 1996). Hence, even simPle machine learning architectures can exhibit hardness of learning
(Goel et al., 2017; Goel & Klivans, 2017). In a variety of settings and architectures, further assumPtions on the
underlying distribution can, however, rule out hard instances and lead to Provable and fast learning algorithms. Such
guarantees have been given for generalized linear models, Ising models, and Markov Random Fields (Klivans &
Meka, 2017), for examPle. These works show uPPer bounds on the number of samPles from the distribution (samPle
comPlexity), and in some cases samPle comPlexities are achieved that are near the known theoretical lower bounds
(Santhanam & Wainwright, 2012). While the samPle comPlexity is the dominating factor in the overall run time, there
is often room to imProve the run time with resPect to other Parameters with further insPection of the algorithms.
Quantum comPuting Promises advantages for search (Grover, 1996) and integer factoring (Shor, 1999). Quantum
algorithms for these Problems can be generalized to imPortant subroutines for linear algebra and machine learning. For
examPle, quantum search can be generalized to amPlitude amPlification (Brassard et al., 2002) which allows sPeeduPs
for samPling and estimation tasks (Montanaro, 2015; Hamoudi & Magniez, 2019). Quantum factoring contains the
Phase estimation subroutine, which can be used for decomPosing a matrix into eigenvalues and eigenvectors (Harrow
et al., 2009). Quantum machine learning has received a great deal of attention (Biamonte et al., 2017; Ciliberto et al.,
2018), with the hoPe of gaining advantages which are relevant for common Problems such as linear systems (Harrow
etal., 2009) and neural networks. Quantum gradient computation has been considered in Gilyen et al. (2019). Quantum
kernel methods are discussed in Schuld & Killoran (20l9); Havllcek et al. (2019). Many algorithms are envisioned for
near-term quantum computers (Preskill, 2018; McClean et al., 2016; Beer et al., 2020). Some algorithms are similar
in spirit to the use of heuristic methods in classical machine leaning. They often cannot obtain provable guarantees
for the quality of learning and for the run time. An interesting avenue for quantum algorithms for machine learning
is therefore to take provable classical algorithms for learning and study provable quantum speedups which retain the
guarantees of the classical algorithms (BrandaO & Svore, 2017; Apeldoorn & Gilyen, 2019; Li et al., 2019).
1
The Alphatron (Goel & Klivans, 2017) is a gradient-descent like algorithm for isotonic regression with the inclusion
of kernel functions. It can be used to provably learn a kernelized, non-linear concept class of functions with a bounded
noise term. As a consequence it can be employed to learn a two-layer neural networks, where one layer of activation
functions feeds into a single activation function. In this work, we provide quantum speedups for the Alphatron and
its application to non-linear classes of functions and two-layer neural networks. First, we consider the simple idea of
pre-computing the kernel matrix used in the algorithm. Our setting is one where the samples are given via quantum
query access. Using this access, we can harness quantum subroutines to estimate the entries of the kernel matrices used
in the algorithm. The quantum subroutines we use are adaptations of the amplitude estimation algorithm. We show
that the learning guarantees can be preserved despite the erroneous estimation of the kernel matrices. In a subsequent
step, we also quantize the Alphatron algorithm itself. In particular, we show that there are estimations inside the
main loop of the algorithm which can be replaced with quantum subroutines, while keeping the main loop intact. We
carefully study the regime where the algorithms allows for a quantum speedup. We are again able to show that the
others parameters of the algorithm remain stable under these estimations. Our main result is that we obtain a quantum
algorithm for learning the original concept class, where the a quantum speedup is obtained for a large parameter regime
of the concept class.
Section 2 discusses the mathematical preliminaries, the weak p-concept learning setting, and kernel methods, and
introduces the Alphatron algorithm with a run time analysis. Section 3 discusses the kernel matrix estimation in the
context of the Alphatron using both classical sampling and quantum estimation. Section 4 discusses the main loop
of the Alphatron and the corresponding quantum run time. Finally, Section 5 summarizes the results in terms of
all relevant parameters and discusses the regime where a quantum speedup is obtained. Appendix B shows a technical
result on the Lipschitz continuity of a kernel function. Appendix C discusses the classical data structures (which
we call `1 and `2 sampling data structures) and subroutines to estimate inner products via sampling. Appendix D
discusses the quantum input models, namely quantum query access and quantum sample access, and several quantum
subroutines for estimating inner products in various settings. Appendix E shows a standard Rademacher complexity
result.
2	Preliminaries and Alphatron algorithm
The vectors are denoted by bold-face x, and their elements by xj . We leave in plain font the α vector (and all other
vectors denoted with Greek symbols). The standard vector space of reals and the unit ball of dimension n are denoted
by Rn and Bn, respectively. The 'p-norm of vectors in Rn is denoted by ∣∣∙∣∣p. Moreover, the max norm is denoted by
IlXIlmaX = maxi ∣Xi∣. We use a ∙ b to denote the standard inner product in Rn. We use the notation O () to omit any
poly-log factors in the arguments. When we write g + O (. . .), we mean g + f with some f ∈ O (. . .). We use a := b
to define a in terms of b.
Please refer to Appendix A for a brief explanation on the arithmetic model used and the mapping Q for mapping bit
strings to rational numbers.
We review the classical Alphatron algorithm of Goel & Klivans (2017), and we consider the standard “probabilistic
concept” (p-concept) learning model (Kearns & Schapire, 1994) in our paper. For the original algorithm and the results
of Goel & Klivans (2017), refer to the Appendix I. There we also show the definitions of weak p-concept learnability,
the definition of the concept class, and the proofs of the following theorems. We discuss a regime where the original
Theorem 11 of Goel & Klivans (2017) achieves p-concept learnability. This result was implicit in Goel & Klivans
(2017).
Theorem 1 (P-concept learnability via the Alphatron). Let m1 := 16^4 log(1∕δ) and m7 := 4BB2 log(1∕δ). If
mi ≥ max {m；, mf1} , then the concept class in Definition 8 is weak p-concept learnable UP to 2CzzL√e by the
ALPHATRON algorithm.
We are also interested in the general run time complexity in this work.
Theorem 2 (Run time of ALPHATRON). Algorithm 4 has a run time ofO(Tm2(n + log d)).
2
3 Pre-computation and approximation of the kernel matrix
One bottleneck of the Alphatron algorithm is the repeated inner product computation when evaluating the function
ht(x). In Algorithm 4, at every step t outofT steps, we need to evaluate O m2 inner products for the kernel function.
This evaluation is redundant because the inner products do not change for different t. A simple pre-computing idea
helps to reduce the time complexity to some extent. We improve Algorithm 4 as follows. Given input data (xi , yi)im=11
and (ai , bi)im=21 and d, we define two matrices
Kij := Kd (xi , xj), Kij := Kd(ai, xj).
If these two matrices are given by an oracle, then we are able to rewrite Algorithm 4. The simple result is Algorithm 5,
which will be used as a subroutine several times in this paper.
With equivalent input, Algorithm 5 produces the same output as Algorithm 4, which can be easily checked as follows.
Fix the input for Algorithm 4. From these fixed training examples compute the kernel matrices Kij = Kd(xi, xj)
and Ki0j = Kd(ai, xj). Use these matrices and the other inputs of Algorithm 4 to fix the input of Algorithm 5. The
sequences (αAt lg4)t∈[T] and (αtAlg5)t∈[T] of both algorithms are the same and hence for the output it holds that
tout,Alg1	tout,Alg2
αAlg4 = αAlg5 .
Even if we do not explicitly define the hypothesis ht in Algorithm 5, in the analysis, we still use the same notation ht
for the t-th generated hypothesis as in Algorithm 4.
Theorem 3 (ALPHATRON_WITH_KERNEL). Algorithm 5 runs in time O (Tm2).
Proof. Since each entry of the matrices K and K0 is accessible in O (1) time, the run time of the algorithm is
O (Tm2).	□
We now discuss the pre-computation, i.e., we prepare the matrices Kij and Ki0j by evaluating the kernel function
for the training and testing data. We present the Algorithm 6, which performs the pre-computation and then runs
Algorithm 5. On the same input, this algorithm produces exactly the same output as Algorithm 4.
Theorem 4 (ALPHAtron_with_Pre). Algorithm 6 generates the same output as Algorithm 4, and runs in time
O(m2(n + log d) + Tm2).
Proof. First of all, it is straightforward to see that Algorithm 6 behaves in the same way as Algorithm 4, by using the
definition ht (x) = u (Pim=11 αitKd(x, xi)) and noticing that the sequences
αAt lg4 t T and αtAlg6 t T are exactly
t∈[T]	t∈[T]
the same. For the time complexity, we have O m2 inner products to be evaluated. For each of them we need time
O (n + log d) as we showed in the proof of Theorem 2. Hence it costs O(m2(n + log d)) time to pre-compute the
results of all Kd(x, y). By Theorem 3, the time complexity of ALPHATRON_WITH_KERNEL is O(Tm2). In total the
algorithm runs in time O(m2(n + log d) + Tm2).	□
By the pre-computation, we evaluate each kernel function only once with the corresponding memory cost of storing
the values. Comparing with the O Tm2 (n + log d) time used for Algorithm 4, Algorithm 6 achieves a significant
speedup. Next, we discuss how to attain an even larger speedup for these inner products by approximation. The
approximations here rely on sampling data structures, which are discussed in Appendix C. These data structures when
given a vector allow to sample an index with probability proportional to the components of the vector, as described in
Facts 1 and 2. We call them `1 and `2 sampling data structures. Here, we use the `2 case (Fact 2), while the second
part of this work uses the `1 case. Based on these data structures, elementary results can be provided to estimate inner
products between two vectors. These are described in Lemmas 3 and 4 in Appendix C, of which we need Lemma 4
here. Our version of the Alphatron algorithm with approximate pre-computation is given in Algorithm 1. We use the
inner product estimation of Lemma 4 to improve the run time complexity of Algorithm 6.
Theorem 5 (Run time of ALPHATRON_with_Approx_Pre). Let CK ,δκ > 0. Assume that for all i ∈ [mi] and
j ∈ [m2], ∣∣Xik2 = ∣∣aj ∣∣2 = 1. Lines 2 — 11 of Algorithm 1 have a run time of O (mn + md log ) , and
provide the kernel matrices Ke and Ke 0 such that maxij Ke ij - Kij ≤ CK and maxij Ke i0j - Ki0j ≤ CK with success
probability 1 — δK. Line 12 requires an additional cost ofO Tm2 from the use of Algorithm 5.
3
1
2
3
4
5
6
7
8
9
10
11
12
13
Algorithm 1: ALPHATRON_with_Approx_Pre
Input training data (xi, yi)im=11, testing data (ai, bi)im=21, error tolerance parameter K, failure probability δK,
function u : R → [0, 1], number of iterations T, degree of the multinomial kernel d, learning rate λ
for i J 1 to mi do
Prepare sampling data structure for xi according to Fact 2.
for j J 1 to m1 do
Zij J Estimate the inner product ‰ ∙ Xjto EK/(3d) additive error with probability at least
1 - δK /(m21 + m1m2) via Lemma 4.
_ Keij J d+Γ Pθ≤k≤d Zj
for i J 1 to m2 do
Prepare sampling data structure for ai according to Fact 2.
for j J 1 to m1 do
Zij J Estimate the inner product ai ∙ Xjto EK/(3d) additive error with probability at least
1 - δK /(m21 + m1m2) via Lemma 4.
Kij J d+1 Po≤k≤d(Zij)
atout J Call ALPHATRON_WITH_KERNEL (Algorithm 5) With all input as above and Kij and Kj.
Output αtout
Proof. For all vectors Xi and aj-, the sampling data structure is prepared in total time O (mn). There are O (m2)
inner products to be estimated between these vectors. Hence, by Lemma 4, each estimation of inner product with
additive accuracy EK/(3d) and success probability 1 一 δκ/(m + m1m2) costs O (d∙ log 叫+：1团2) because of
kXi k2 = kaj k2 = 1. We ignore the log(m12 + m1m2) factor under the tilde notation comparing to the quadratic
m. Again, O (log d) extra time is needed to compute each multinomial kernel function Kd from the inner product.
However, we also ignore the log d factor under the tilde notation. By Lemma 2 of the Appendix, the Lipschitz constant
for f (Z) = d+i Pd=O Zi is bounded from above by 3d, when Z ∈ [-1,1]. Hence, we obtain maxij J KKij - Kij I ≤
(3d) ∙ EK/(3d) and maxj∙ ∣Kij — KijI ≤ (3d) ∙ EK/(3d). The last step for calling Algorithm 5 costs O (Tm2) again
as the matrices are accessible in O (1).	□
Since the inner products are approximated in advance, Algorithm 1 improves the run time complexity of the Algo-
rithm 6. However, as the inner products are approximated, we may lose the correctness of Algorithm 6. In Goel &
Klivans (2017), a theoretical upper bound is proven for the sample complexity of Algorithm 4 in the problem setting
of Definition 8. We now show that with approximate pre-computation, under the same problem and parameter settings
as in Goel & Klivans (2017), the p-concept error of the output hypothesis does not increase too much.
Theorem 6 (Correctness of ALPHATRON_with_Approx_Pre). If Definition 8 and 9 hold, then by setting
δK = δ, with probability 1 - 3δ, Algorithm 1 outputs αtout which describes the hypothesis htout (X) :=
U (PmII atoutKd(x, Xi)) such that, ε(h)

∈ O (A2 + EKT2 + EKT), where A2 = L√E + LZ logm≡ +
BL
log(1∕δ)
mi
See the proof of Theorem 6 in Appendix F. Since by definition ε(h) ≤ 1, for any hypothesis h, it is reasonable to
assume that A2 ≤ 1 if we want a useful bound. Then, by setting EK = A, we have EKT ≤ 1. Thus, O E2KT2 ⊆
O (EKT), and we can simplify the right hand side of Eq. (60) to O (A2 + EKT). From the runtime analysis in
Theorem 5 and the accuracy analysis in Theorem 6, we have the following corollary.
Corollary 1. In the same setting as Theorem 6, if L√E ≤ 1, and we set EK = LTɪ, then Algorithm 1 with probability
at least 1 -3δ outputs αtout which describes the hypothesis htout (X) := u (PmII atoutKd(x, Xi)) such that ε (htout) ∈
O (A2), with a run time of O (mn + mL2[ ? log ∣ + Tm2).
4
Algorithm 2: ALPHATron with Q Pre
1
2
3
4
5
6
7
8
9
10
11
Input Quantum access to training data (xi, yi)im=1 and training data (ai, bi)iN=1 according to Data Input 1, error
tolerance parameter K, failure probability δK, function u : R → [0, 1], number of iterations T, degree of the
multinomial kernel d, learning rate λ
for i J 1 to mi do
for j J 1 to mi do
zij J Estimate the inner product hxi, xji to K /(3d) additive error with probability at least
1 - δK /(m2i + mim2) via Lemma 6.
_ Keij J d⅛ Po≤k≤d zij
for i J 1 to m2 do
for j J 1 to mi do
zi0j J Estimate the inner product hai, xji to K /(3d) additive error with probability at least
1 - δK /(m2i + mim2) via Lemma 6.
_ Keij J d++Γ Po≤k≤d(ZGk
+	„ „ .	▼▼	....... ........... ■	■工	■工，
atout J Call ALPHATRON_WITH_KERNEL (Algorithm 5) With all inputs as above and Kij and Kj.
Output αtout
3.1	Quantum P re-computation
This section presents the quantum algorithm for pre-computing the kernel matrices used in the Alphatron. We assume
quantum access to the training data, Which includes classical access and also superposition queries to the data. Note
the Definition 3 of quantum query access in Appendix D.
Data Input 1. For k ∈ [n], i ∈ [mi], andj ∈ [m2], let xi and aj be the input vectors with kxik2 = kaj k2 = 1, and
let xik and ajk be the entries of the vectors. Assume c = O (1) bits are sufficient to store xik and ajk. Assume that
we are given QA(xi, n, c) for each i ∈ [mi] and QA(aj, n, c) for each j ∈ [m2].
Our first quantum algorithm is constructed in a straightforWard manner. We replace the classical approximation of the
kernel matrix inner products With a quantum estimation. For the quantum estimation of inner products refer to Lemma
6 in AppendixappQuantum, Which requires quantum query access similar to Data Input 1. The run time of Lemma 6
depends on the '2-norms of the input vectors, which here are 1. The result is Algorithm 2. The run time analysis and
the guarantees for the output hypothesis are similar to the classical algorithm. We state them beloW as a corollary.
Corollary 2 (Runtime of ALPHATRON_with_Q_Pre). Let EK, δκ > 0. Assume that for all i ∈ [mi] and j ∈ [m2],
we have quantum query access to the vectors xi and aj via Data Input 1. Lines 2 - 9 of Algorithm 2 have a run
time of O m )√ log	and provide the kernel matrices K and K0 such that maxj |Kj - Kij | ≤ EK and
maxij |Ki0j - Ki0j | ≤ EK with success probability 1 - δK.
Proof. For EK ∈ (0,1), the run time of each invocation of Lemma 6 is O ^-ɪn log (δm)), using that the input
vectors are in the unit ball. All probabilistic steps in Lines 2 - 9 of the algorithm succeed with probability 1 - δK
using a union bound.	□
Corollary 3 (Guarantee for ALPHATron_with_Q_Pre). Let δ > 0. Assume that for all i ∈ [mi] and j ∈ [m2],
we have quantum query access to the vectors xi and aj via Data Input 1. Let Definitions 8 and 9 hold. IfA2 ≤ 1,
and we set EK = LTg and δκ = δ, then Algorithm 2 with probability at least 1 — 3δ outputs atout which de-
scribes the hypothesis htout (x) := u (PmIi atoutK-(x, Xi)) such that ε (htout) ∈ O (A2), with a run time of
O (m-√√n log 1 + Tm2).
Proof. The proof is analogous to the proof of Theorem 6, where we use Corollary 2 for the run time of the inner
product estimation.	□
5
4	Quantum Alphatron
Up to this point, we have been discussing improvements in the pre-computation step of the Alphatron. We always
use the same ALPHATron_with_Kernel algorithm once We prepare the kernel matrices K and K0. If data dimen-
sion n is much larger than the other parameters, the quantum pre-computation costs asymptotically more time than
ALPHATRON_with_Kernel. Hence, we do not benefit much from optimizing ALPHATron_with_Kernel if the
cost of preparing the data of size n is taken into account.
However, ifwe assume that the pre-computation was already done for us, it makes sense to discuss quantum speedups
for ALPHATRON_with_Kernel, which is what the remainder of this work is about. In other words, we consider the
following scenario.
Data Input 2. Let there be given two training data sets (xi, yi)im=11 ∈ Bn × [0, 1] and (ai, bi)im=21 ∈ Bn × [0, 1], which
define the kernel matrices Kij := Kd(xi, xj) and Ki0j := Kd(ai, xj). Let each entry Kji and Kj0i be specified by
O (1) bits. We assume that we have query access to each entry in O (1).
The bottleneck of the computation in the ALPHATRON_with_Kernel is the cost of about O (Tm) for the inner
product evaluations. By the sampling techniques and quantum estimation, we may speed them up.
4.1	Main loop with approximated inner products
We employ the classical sampling of inner products in the ALPHATRON_with_Kernel algorithm. The result is
Algorithm 7. For the kernel matrices Kji and Kj0i, define Kmax as an upper bound for |Kji| and |Kj0i|.
Theorem 7. We assume query access Data Input 2 to the kernel matrices K and K0 with known Kmax. Let I , δ ∈
(0, 1). If the Definitions 8 and 9 hold, the Algorithm 7 outputs αtout which describes the hypothesis htout (x) :=
u Pim=11 αitout Kd (x, xi) such that with probability 1 - 3δ, ε(htout) ∈ O A2 + LI + L2I2 , where A2 is defined
in Definition 9. The run time of this algorithm is O (Tm + T 3m Kma log (1)). Moreover, if A2 ≤ 1, and we set
EI = √, then we obtain the guarantee ε(htout) ∈ O (A2), and have a run time of O (Tm + T 3m Kmax log (1)).
For the detailed proof, refer to Appendix G, which is similar to the proof of Theorem 6. Now, replace the classical
sampling of the inner product with the quantum estimation of the inner product. with the Lemma 7 in Appendix D,
we can remove the explicit dimension dependence of the inner product estimation, at the expense of using a QRAM,
see Definition 1 in the next section.
4.2	Quantum speedup for the main loop
For the quantum algorithm, we assume the quantum query access to the kernel matrices K and K. Note the definition
of quantum query access in Definition 3 in Appendix D.
Data Input 3. Assume Data Input 2 for the training data and the kernel matrices. For all j ∈ [m1], define Kj as
the vector (Kji ,Kj2, ∙∙∙ , KjmI), and for all j ∈ [m2], define Kj as the vector (KjKj 2, ∙∙∙ , Kj rmJ. Assume the
availability of the quantum access QA(Kj, m1, O (1)), for all j ∈ [m1], and the quantum access QA(Kj0, m1, O (1))
, for all j ∈ [m2].
Based on this input a simple circuit prepares query access to the non-negative versions of the vectors.
Lemma 1. Assume Data Input 2 and define the non-negative vectors (Kj)+, (Kj)-, with Kj = (Kj)+ -
(Kj)- and the non-negative vectors (Kj0 )+, (Kj0 )-, with Kj0 = (Kj0 )+ - (Kj0)-. Given Data In-
put 3, then query accesses QA((Kj)+, m1, O (1)), QA((Kj)-, m1, O (1)), ∀j ∈ [m1] and query accesses
QA((Kj0)+, m1, O (1)), QA((Kj0)-, m1, O (1)), ∀j ∈ [m2] can be provided with two queries to the respective inputs
and a constant depth circuit of quantum gates.
For our quantum version for the main loop of the Alphatron algorithm, we will also require a dynamic quantum
data structure for the α vector which allows us to obtain efficient quantum sample access. Note the Definition 4
for the quantum sample access in Appendix D. One way to obtain such an access is via quantum random access
memory (QRAM) (Giovannetti et al., 2008a;b; Arunachalam et al., 2015). Such a device stores the data in (classical)
6
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
Algorithm 3: Quantum-Alphatron
Input training data (xi, yi)im=11, testing data (aj, bj)jm=2 * 41, error tolerance parameters I and δI, function
u : R → [0, 1], number of iterations T, degree of the multinomial kernel d, learning rate λ, quantum query
access to Kj, ∀j ∈ [m1] and Kj0, ∀j ∈ [m2] via Data Input 3
α0 一 0 ∈ Rm1
for j — 1 to mi do
Pmax — maxi |Kji| via quantum maximum finding with success probability 1 - δι/(4mι)
Define the non-negative vectors (Kj)+, (Kj)-, with Kj = (Kj)+ - (Kj)-
From query access to Kj provide query access to (Kj)+, (Kj)- via Lemma 1
for j — 1 to m2 do
qmax — maxi |Kj/ via quantum maximum finding with success probability 1 - δ//(4m2)
Define the non-negative vectors (Kj0)+, (Kj0)-, with Kj = (Kj0)+ - (Kj0)-
From query access to Kj provide query access to (Kj)+, (Kj)- via Lemma 1
for t — 0 to T — 1 do
Store in QRAM (see Definition 1) the non-negative vectors (αt)+, (αt)-, where αt = (αt)+ - (αt)-, where
each element of the vector is stored using d'log(λT/mι)^] + ∣"log (2KmaxmI) ^j bits
Wt — IIatkI
for j — 1 to mi do
rj — Estimate inner product at ∙ Kj, by estimating (αt)+ ∙ (Kj)+, (at)+ ∙ (Kj)-, (at)- ∙ (Kj-)+, and
(at)- ∙ (Kj)- via Statement (iii) of Lemma 7 (using Wt and Pmax), each to additive accuracy EI/8 with
success probability 1 - δI /(16T mi)
L aj+1 J aj+ mλ?(Iyj- Mrj))
for j J 1 to m2 do
sj J Estimate inner product at ∙ Kj, by estimating (at)+ ∙ (Kj)+, (at)+ ∙ (Kj)-, (at)- ∙ (Kj)+, and
(at)- ∙ (Kj)- via Statement (iii) of Lemma 7 (using Wt and qmax), each to additive accuracy CI/8 with
success probability 1 - δI/(16Tm?)
tout = argmint∈[T ] m2 Pm21(U(Sj) - bj )2
Output αtout
memory cells, but allows for superposition queries to the data. If all the partial sums are also stored, then QRAM
can provide quantum sample access via the Grover-Rudolph procedure, see Grover & Rudolph (2002). This costs
resources proportional to the length of the vector to set up, but then can provide the the superposition state in a run
time logarithmic in the length of the vector.
Definition 1 (Quantum RAM). Let c and m be positive integers. Let v be a vector of dimension m, where each
element of v is a bit string of length c, i.e., v ∈ ({0, 1}c)m. Quantum RAM is defined such that with a one-time cost of
O (c m) we can construct quantum query and sampling access QA(v, m, c) and QS(v, m, c), see Definitions 3 and
4 in Appendix D. Each query costs O (c poly log m).
Based on Data Input 3 and Definition 1, Lemma 7 in Appendix D allows us to estimate the inner products between αt
and Kj more efficiently than the equivalent estimation in Algorithm 7. We have the Algorithm 3.
Theorem 8 (Quantum Alphatron). We assume quantum query access to the vectors Kj and Kj0 via Data Input 3.
Again, let Kmax be maximum of all entries in K and K0. Let δ ∈ (0, 1). Given Definitions 8 and 9 and δI = δ,
Algorithm 3 outputs αtout such that the hypothesis htout = U (Pj ajoutψ(xj∙) ∙ ψ(x)) satisfies with Probabil-
ity 1 - 3δ, ε(htout ) ∈ O A2 + L2EI2 , where A2 is defined in Theorem 6. The run time of this algorithm is
Oe (m1∙5 log (1) + Tm + T 2m KLmax log (1)) . If A2 ≤ 1 and we set EI = √e then we further obtain the guarantee
ε(htout) ∈ O (A2), and the run time is Oe (m1∙5 log (∣) + Tm + T2mIK√ log (∣)).
7
For the detailed proof, refer to the Appendix H, which is similar to the proof of Theorem 6.
5	Discussion
In this section, we summarize the results of this paper and discuss the improvement on the run time complexity
by pre-computation and quantum estimation. In Section 3, We have introduced ALPHATRON_with_Pre, Alpha-
tron_with_Approx_Pre, and ALPHATron_with_Q_Pre, which improve the original Alphatron. The scenario
is that the dimension of the data n is much larger than the other parameters, a situation that is relevant for many
practical applications. without the pre-computation, we have a run time O Tm2n compared to the run time with
the pre-computation of O m2n + m2 log d + Tm2 . The factor of the n dependent term loses a factor T which is a
small improvement. Moreover, by quantum amplitude estimation, we gain a quadratic speedup in the dimension n.
we list the results of Section 3 in Table 1 for comparison.
Table 1: Comparison of the first set of algorithms in Section 3. we separate the pre-computation of the multinomial
kernel function from the main loop and also estimate the training set inner products instead of computing them
exactly, which can improve the time complexity of the computation of the kernel function. For all algorithms, we
indicate the general result without using the learning setting in Definition 8. For ALPHATRON_with_Approx_Pre
and ALPHATRON_with_Q_Pre, the relevant kernel functions are estimated to accuracy EK with failure probability
δκ. To obtain the weak p-concept learning result of Theorem 1 for all these algorithms, take the concept class defined
in Definition 8 and the parameter settings for the algorithms of Definition 9. Also, set EK = L√e∕T and δκ = δ. We
do not further evaluate the formulas (using, e.g., the expressions for T and m1) as the main focus of this table is on
the dependency on n which dominates all other parameters.
Name	Pre-computation	Main loop	Proved in
Alphatron	not applicable	O (Tm2n)	Goel & KIiVanS (2017), also Thm 2
ALPHATRON_WITH_PRE	O (m2n + m2 log d	O (Tm2)	Theorem 4
Atron_with_Approx_Pre	O(mn + m2d log 看)	O (Tmr)	Theorem 5
ALPHATRON_WITH_Q_PRE	-O( m√log ；) 	∖ eκ	& δκ )	O (Tm2)	Corollary 2
Section 4 has introduced ALPHATRON_with_Kernel_and_Sampling and QUANTUM_Alphatron. In this sce-
nario, we assume constant time query access to the kernel matrices (i.e., to the result of the pre-computation), while
the quantum version requires quantum query access. These algorithms only focus on the main loop part of the Al-
phatron. Hence, these algorithms can be viewed as the improvements for ALPHATRON_with_Kernel. We list the
results of Section 4 in Table 2. For comparison, we also list the time complexity of ALPHATRON_with_Kernel.
Table 2: Comparison of the second set of algorithms Atron_with_Kernel_and_Sampling and Quan-
tum_Alphatron, which are discussed in Section 4, to ALPHATRON_with_Kernel. These algorithms change the
main loop part by using an inner product estimation. The inner product estimation is performed to accuracy EI and
the total success probability of the algorithm is 1 - δ. Here, we indicate the general result without the learning setting
in Definition 8.
Name	Main loop	Theorem
ALPHATRON_WITH_KERNEL	O (Tm2)	一	Theorem 3
Atron_with_Kernel_and_Sampling	Oe ^Tm + T3mKLmaX log δ)	Theorem 7
QUANTUM.Alphatron	O (m1∙5 log 1 + Tm + T2mKmaX log δ)	Theorem 8
For Table 2, it is not obvious that the quantum algorithm has a speedup compared to the ALPHATRON_with_Kernel.
As mentioned in the preliminary, Kd(x, y) ≤ 1 for all x, y ∈ Bn. Hence, we can use Kmax ≤ 1. Recall from Theorem
8
Table 3: Comparison of the algorithms ALPHATron_with_Kernel and QUANTUM_Alphatron for the learning
setting in Definition 8. Only in the first case a quantum advantage is obtained.
Case	Classical run time	Quantum run time	Advantage
4Z4 > B2e	O( *0 log41)	O( B⅛8 log41)	yes
4Z4 ≤ B2e	O( ⅜⅛ log4；)	O( B6L log4；)	no
1 that if mi ≥ max {m；,m?},with m； = 1∣ζ4 log(1∕δ) and m7 = 4BB2 log(1∕δ), thenthe concept class inDefinition
8 is weak P-ConcePt learnable UP to 2C00L√l by the ALPHATRON algorithm.
Case m01 > m010. Consider the first case, which is equivalent to 4ζ4 > B2. Hence, m1 ∈ O (m01) leads to learn-
ability, which we can use to simplify T = CBLqlogm^2) ∈ O (BLζ2). In addition, m = m1 + m2, and we
have m2 = C0m； log(T∕δ), hence, m = (1 + C0 log T + C0log1∕δ)m; ∈ O	log2 ；) . From Theorem 8, we
have the run time O (m1∙5 log ； + Tm + T2mL√ log ；) = O (ζ6 log4 ； + BLζ34 log2 ； + B2L其 log3 ∣)=
O(B2L妥 log4 1) . For the classical run time, we simplify O (Tm2) ⊆ O (BLZIO log4 ；).
Case m0；0 > m0；. Consider the second case, which is equivalent to 4ζ4 < B2. Hence, m； = O (m0；0) leads to learn-
ability, which we can use to simplify T = CBLqogm/) ∈ O (B√L). In addition, m ∈ O (B2 log2 ；). From The-
orem 8, we have the run time O(BB65 log4 ；). For the classical run time we simplify O (Tm2) ⊆ O (B65 log4 ；).
This analysis of the two cases is summarized in Table 3 and allows us to state our final theorem.
Theorem 9 (Quantum p-concept learnability via the QUANTUM_Alphatron). Let the concept class and distribution
be defined by Definition 8. For this concept class, let 4Z4 > B2e. In addition, let there be given quantum access to the
kernel matrices via Data Input 3. Then, the concept class in Definition 8 is weakp-concept learnable up to 2C00L√e
ζ2
by the QUANTUM_Alphatron algorithm with a run time that shows an advantage by a factor 〜B√ over the
classical algorithm given the same input.
A note on the condition 4ζ4 > B2 for the speedup. By Definition 8, ζ determines the range of the noise function,
while is an upper bound to the variance of the noise function. For any function the variance will be smaller or equal
to the range. Hence, the condition 4ζ4 > B2 is reasonably easy to satisfy and we may obtain a quantum advantage
for a broad concept class of functions.
9
Acknowledgments
This work was supported by the Singapore National Research Foundation, the Prime Minister’s Office, Singapore,
the Ministry of Education, Singapore under the Research Centres of Excellence programme under research grant R
710-000-012-135.
References
J. van APeldoom and A. Gilyen. Quantum algorithms for zero-sum games, 2019. arXiv： 1904.0318 0v1
[quant-ph].
Srinivasan Arunachalam, Vlad Gheorghiu, Tomas Jochym-O’Connor, Michele Mosca, and Priyaa Varshinee Srini-
vasan. On the robustness of bucket brigade quantum RAM. New Journal of Physics, 17(12):123010, dec 2015.
Peter Auer, Mark Herbster, and Manfred K. K Warmuth. ExPonentially many local minima for single neurons. In
D. S. Touretzky, M. C. Mozer, and M. E. Hasselmo (eds.), Advances in Neural Information Processing Systems 8,
pp. 316-322. MIT Press, 1996.
Peter L Bartlett and Shahar Mendelson. Rademacher and gaussian comPlexities: Risk bounds and structural results.
Journal of Machine Learning Research, 3(Nov):463-482, 2002.
Kerstin Beer, Dmytro Bondarenko, Terry Farrelly, Tobias J. Osborne, Robert Salzmann, Daniel Scheiermann, and
Ramona Wolf. Training deep quantum neural networks. Nature Communications, 11(1):808, 2020. doi: 10.1038/
s41467-020-14454-2. URLhttps://doi.org/10.1038/s41467-020-14454-2.
Jacob Biamonte, Peter Wittek, Nicola Pancotti, Patrick Rebentrost, Nathan Wiebe, and Seth Lloyd. Quantum machine
learning. Nature, 549:195-202, 2017.
F. G. S. L. Brandao and K. M. Svore. Quantum speed-ups for solving Semidefinite programs. In Proceedings of the
58th Symposium on Foundations of Computer Science (FOCS), pp. 415-426, 2017.
Gilles Brassard, Peter H0yer, Michele Mosca, and Alain Tapp. Quantum amplitude amplification and estimation.
Contemporary Mathematics, 305:53-74, 2002.
C. Ciliberto, M. Herbster, A. D. Ialongo, M. Pontil, A. Rocchetto, S. Severini, and L. Wossnig. Quantum machine
learning: A classical perspective. Proceedings of the Royal Society A: Mathematical, Physical and Engineering
Sciences, 474(2209):20170551, 2018.
C. Durr and P. H0yer. A quantum algorithm for finding the minimum. arXiv preprint quant-ph/9607014, 1996.
A. Gilyen, S. Arunachalam, and N. Wiebe. Optimizing quantum optimization algorithms via faster quantum gradient
computation. In Proceedings of the 2019 Annual ACM-SIAM Symposium on Discrete Algorithms (SODA), pp.
1425-1444, 2019.
V. Giovannetti, S. Lloyd, and L. Maccone. Quantum random access memory. Phys. Rev. Lett., 100:160501, 2008a.
V. Giovannetti, S. Lloyd, and L. Maccone. Architectures for a quantum random access memory. Phys. Rev. A, 78:
052310, 2008b.
Surbhi Goel and Adam R. Klivans. Learning depth-three neural networks in polynomial time. CoRR, abs/1709.06010,
2017. URL http://arxiv.org/abs/1709.06010.
Surbhi Goel, Varun Kanade, Adam Klivans, and Justin Thaler. Reliably learning the relu in polynomial time. In Satyen
Kale and Ohad Shamir (eds.), Proceedings of the 2017 Conference on Learning Theory, volume 65 of Proceedings
of Machine Learning Research, pp. 1004-1042. PMLR, 07-10 Jul 2017.
Lov Grover and Terry Rudolph. Creating superpositions that correspond to efficiently integrable probability distribu-
tions. arXiv preprint quant-ph/0208112, 2002.
Lov K Grover. A fast quantum mechanical algorithm for database search. In Proceedings of the twenty-eighth annual
ACM symposium on Theory of computing, pp. 212-219. ACM, 1996.
10
Yassine Hamoudi and Frederic Magniez. Quantum Chebyshev's Inequality and Applications. In Christel Baier, Ioannis
Chatzigiannakis, Paola Flocchini, and Stefano Leonardi (eds.), 46th International Colloquium on Automata, Lan-
guages, and Programming (ICALP 2019), volume 132 of Leibniz International Proceedings in Informatics (LIPIcs),
pp. 69:1-69:16, DagStuhL Germany, 2019. Schloss Dagstuhl-Leibniz-Zentrum fuer Informatik. ISBN 978-3-95977-
109-2. doi: 10.4230/LIPIcs.ICALP.2019.69. URL http://drops.dagstuhl.de/opus/volltexte/
2019/10645.
Aram W Harrow, Avinatan Hassidim, and Seth Lloyd. Quantum algorithm for linear systems of equations. Physical
Review Letters, 103(15):150502, 2009.
Vejtech HavliCek, Antonio D. Corcoles, Kristan Temme, Aram W. Harrow, Abhinav Kandala, Jerry M. Chow, and
Jay M. Gambetta. Supervised learning with quantum-enhanced feature spaces. Nature, 567(7747):209-212, 2019.
doi: 10.1038/s41586-019-0980-2. URL https://doi.org/10.1038/s41586-019-0980-2.
Michael J Kearns and Robert E Schapire. Efficient distribution-free learning of probabilistic concepts. Journal of
Computer and System Sciences, 48(3):464-497, 1994.
A. Klivans and R. Meka. Learning graphical models using multiplicative weights. In 2017 IEEE 58th Annual Sympo-
sium on Foundations of Computer Science (FOCS), pp. 343-354, 2017.
T. Li, S. Chakrabarti, and X. Wu. Sublinear quantum algorithms for training linear and kernel based classifiers.
arXiv:1904.02276, (arXiv: 1904.02276), 2019.
Jarrod R McClean, Jonathan Romero, Ryan Babbush, and Alan Aspuru-Guzik. The theory of variational hybrid
quantum-classical algorithms. New Journal of Physics, 18(2):023023, 2016.
Ashley Montanaro. Quantum speedup of monte carlo methods. Proc. R. Soc. A, 471:0301, 2015.
John Preskill. Quantum Computing in the NISQ era and beyond. Quantum, 2:79, August 2018.
Patrick Rebentrost, Yassine Hamoudi, Maharshi Ray, Xin Wang, Siyi Yang, and Miklos Santha. Quantum algorithms
for hedging and the learning of ising models. Phys. Rev. A, 103:012418, Jan 2021. doi: 10.1103/PhysRevA.103.
012418. URL https://link.aps.org/doi/10.1103/PhysRevA.103.012418.
N. P. Santhanam and M. J. Wainwright. Information-theoretic limits of selecting binary graphical models in high
dimensions. IEEE Transactions on Information Theory, 58(7):4117-4134, 2012.
Bernhard Scholkopf and Alexander J Smola. Learning with kernels: support vector machines, regularization, opti-
mization, and beyond. MIT press, 2002.
Maria Schuld and Nathan Killoran. Quantum machine learning in feature hilbert spaces. Phys. Rev. Lett., 122:
040504, Feb 2019. doi: 10.1103/PhysRevLett.122.040504. URL https://link.aps.org/doi/10.1103/
PhysRevLett.122.040504.
P. Shor. Polynomial-time algorithms for prime factorization and discrete logarithms on a quantum computer. SIAM
Review, 41(2):303-332, 1999. doi: 10.1137/S0036144598347011.
Ewin Tang. A quantum-inspired classical algorithm for recommendation systems. Electronic Colloquium on Compu-
tational Complexity, 128, 2018.
M. D. Vose. A linear algorithm for generating random numbers with a given distribution. IEEE Transactions on
Software Engineering, 17(9):972-975, 1991.
A. J. Walker. New fast method for generating discrete random numbers with arbitrary frequency distributions. Elec-
tronics Letters, 10(8):127-128, 1974.
11
A Cost of arithmetic operations
In this paper, we use the following arithmetic model for classical computation. We represent the real numbers with
a sufficiently large number of bits. We assume that the number of bits is large enough to make the numerical errors
negligible in the correctness and run time proofs of the algorithms under consideration. The implication is that we can
ignore numerical errors of arithmetic operations (e.g., addition, subtraction, multiplication, and so on) with respect
to truncation or rounding. Hence, we assume all real numbers cost O (1) space and the basic arithmetic operations
between them cost O (1) time. While the accumulated error can be important, dealing with a proper error analysis
would require a substantial deviation from the main purpose of this paper.
For the quantum algorithms, we keep track of the amount of (quantum) bits for storing real numbers. We use a standard
fixed-point encoding of real numbers.
Definition 2 (Notation for encoding of real numbers). Let c1, c2 be positive integers anda ∈ {0, 1}c1 and b ∈ {0, 1}c2
be bit strings. Define the (signed) rational number
Q(a, b, S)= (-I)S (2c1-1acι + …+ 2a2 + a1 + 2b1 + …+ 2^^bc2) ∈ [-R, R],	(I)
where R := 2c1 — £. For representing numbers in [0, 2 一 击]with positive integer C bits after the decimal point (the
case [0, 1] being the most frequently used in this work) we use c1 = 1 and c2 = c, and define the short-hand notation
Q(Z)	= Q(a, b, O) = a + 1bι+------+ 21c bc,	⑵
where z := (a, b)	∈ {0,	1}c+1. Given	a vector of bit strings v ∈ ({0, 1}c+1)n,	the notation Q(v)	means the vector
whose j-th component is Q(vj).
For any real number r ∈ [0, 2c1] there exist a ∈ {0, 1}c1 and b ∈ {0, 1}c2 such that the difference to Q(a, b, 0) is at
most 2c1+ι.
B Lipschitz condition for multinomial kernel function
Lemma 2. Let f : R → be be defined by f (z) = d++ι Pd=0 Zi. Then f (z) is Lipschitz continuous with Lipschitz
constant L ∈ O (dzd + dzo + d), that is
If(z) — f(z0)l ≤ L|z — z0∣,	(3)
for all Z, Z0 ∈ [-Z0, Z0].
Proof. First,关f(z)=器 Pd-01(i +1)zi ≤ d⅛ (1 + Pd-I dzi). When 0 < z < 1, Zi ≤ Z for 1 ≤ i ≤ d — 1. And
when Z ≥ 1, Zi	≤	Zd for 1 ≤ i ≤ d — 1.	Thus Zi	≤ Z + Zd for	1 ≤ i ≤ d — 1. Hence L ≤ maxz∈[-z0/°] |dzf(Z)I	≤
ddzf (Z) lz=z0 ∈	O	(dZd + dZ0 + 1).	□
Note that throughout this paper, it always holds that Z0 = 1. In this case, the Lipschitz constant is bounded by O (d).
C Classical sampling
The next facts discuss the construction of a data structure to sample from a vector and the next lemmas discuss the
approximation of an inner product of two vectors by sampling. Both `1 and `2 cases are required in this work. The
SQ label can be understood as “sample query”. The arithmetic model allows us to assume infinite-precision storage
of the real numbers.
Fact 1 ('ι-sampling (Vose,1991; Walker, 1974)). Given an n-dimensional vector U ∈ Rn, there exists a data structure
to sample an index j ∈ [n] with probability Iuj I/kuk1 which can be constructed in time O (n). One sample can be
obtained in time O (log n). We call this data structure SQ1(u, n).
12
Fact2 ('2-sampling (Vose,1991; Walker, 1974)). Given an n-dimensional vector U ∈ Rn ,there exists a data structure
to sample an index j ∈ [n] with probability uj2/kuk22 which can be constructed in time O (n). One sample can be
obtained in time O (log n). We call this data structure SQ2(u, n).
Next, we show the estimation of inner products via sampling. The number of samples scales with 1/2 classically, in
contrast to using quantum amplitude estimation which scales with 1/. Lemma 3 is adapted from Tang (2018) and
Lemma 4 is taken directly from Tang (2018).
Lemma 3 (Inner product with '「sampling). Let e, δ ∈ (0,1). Given query access to V ∈ Rn and SQ1(u, n) access
to u ∈ Rn, we can determine U ∙ V to additive error E With success probability at least 1 一 δ With O (kukl!vkmax log 1)
queries and samples, and O (kukl!vkmax log 1) time complexity.
Proof. Define a random variable Z with outcome Sgn(Uj)ku∣∣ιvj with probability ∣u7-∣∕∣∣u∣∣ι. Note that E[Z]=
Pj Sgn(Uj )kuk1vj |uj |/kuk1 = U ∙ v. Also, MZ] ≤ 叫Z 2] = Pjkuk2v2luj |/kuk1 ≤ Iluk2 llvkmaχ. Take
the median of 6 log 1∕δ evaluations of the mean of 9kUk12 kVk2max∕(2E2) samples of Z. Then, by using the Cheby-
Shev and Chernoff inequalities, We obtain an E additive error estimation of U ∙ V with probability at least 1 一 δ in
O (kuklIvkmax log 1) queries.	□
Lemma 4 (Inner product with '2-sampling). Let e, δ ∈ (0,1). Given query access to V ∈ Rn and SQ2(u, n) access
to u ∈ Rn, we can determine U ∙ V to additive error E with success probability at least 1 一 δ with O (kuk∕kvk2 log 1)
queries and samples, and O (kuk∕kvk2 log }) time complexity.
Proof. Define a random variable Z with outcome ∣∣U∣∣2Vj∕uj with probability u2∕kUk2. Note that E[Z] =
Pjkuk2VjU2∕(UjkUk2) = U ∙ V. Also,阴Z] ≤ E[Z2] = Pjkul∣2v2 = Iluk2kVk2. Takethemedianof 6log1∕δ
evaluations of the mean of 9kUk22kVk22/(2E2) samples of Z. Then, by using the Chebyshev and Chernoff inequalities,
we obtain an E additive error estimation of U ∙ V with probability at least 1 一 δ in O (kυ,kvk2 log 1) queries. □
By the above Fact 2 and Lemma 4, given vector u, V ∈ Rn, the sampling data structure for U can be constructed in
O (n) time and an estimation of U ∙ V with E additive error can be obtained with probability at least 1 一 δ ata run time
cost of O (ku"kvk2 log I).
D Quantum sub routines
First, we define the quantum access used for vectors.
Definition 3 (Quantum query access). Let c and n be two positive integers and U be a vector of bit strings U ∈
({0, 1}c)n. Define element-wise quantum access to Uforj ∈ [n] by the operation
|ji |0ci → |ji |Uji,	(4)
on O (c + log n) qubits. We denote this access by QA(U, n, c).
For the following part of this Appendix, recall Definition 2 regarding the fixed-point encoding of real numbers. In
addition, we define the quantum sample access to a normalized semi-positive vector V∕kVk1 which is a fixed-point
approximation of a real semi-positive vector. Each component of the vector V is represented with c1 bits before the
decimal point and with c2 bits after the decimal point.
Definition 4 (Quantum sample access).	Let	c1,	c2,	and n be positive integers and	V0	∈	({0,	1}c1 )n	and V00 ∈
({0, 1}c2 )n be vectors of bit strings. Define quantum sample access to a vector V via the operation
1	n ____________
l°i→ p≡≡ jX qj ji,	⑸
13
on O (log n) qubits. We denote this access by QS(v, n, c1, c2). For the sample access to a vector v which approxi-
mates a vector with components in [0, 1], we use the shorthand notation QS(v, n, c2) := QS(v, n, 1, c2).
As stated in Rebentrost et al. (2021), We have the following lemma for estimating the 'ι-norm of a vector and preparing
states encoding the square root of the vector elements.
Lemma 5 (Quantum state preparation and norm estimation). Let c and n be two positive integers and u ∈
({0, 1}c+1)n. Assume quantum access to u via QA(u, n, c + 1). Let maxj Q(uj) = 1. Then:
1. There exists a quantum circuit that prepares the state
two queries to QA(u, n, c) and O (log n + c) gates.
√n pn=ι |ji (PQ(Uj) |0i + pi - Q(Uj) |1〉)
with
2. Let , δ ∈ (0, i). There exists a quantum algorithm that provides an estimate Γu of the `1 -norm kQ(u)k1
such that ∣kQ(u)kι - Γu∣ ≤ EkQ(U) ∣∣ι, with probability at least 1 一 δ. The quantum circuit ofthis algorithm
makes O (qq kQ(U)k1 ιog(i∕δ)
queries to QA(u, n, c + i) and has O
(C √Λ log。0)
gates.
By using Lemma 5, we estimate the inner product of two vectors u and v with additive errors as follows. The vectors
can be considered as fixed point approximations to real vectors with elements restricted to [-i, i].
Lemma 6 (Quantum inner product estimation with additive accuracy). Let E, δ ∈ (0, i). Let c and n be two positive
integers. Let two non-zero vectors of bit strings be u, v ∈ ({0, i}c+2)n, which leaves one bit for the sign of each
component, one bit for the number before the decimal point, and c bits for the number after the decimal point. Let
there be given quantum access to u and v as QA(u, n, c + 2) and QA(u, n, c + 2), respectively. Let the norms
kQ(u)k2 and kQ(v)k2 be known. Then, there exists a quantum algorithm which provides an estimate I for the
inner product such that |I — Q(U) ∙ Q(v)∣ ≤ E with success probability 1 一 δ. This estimate is obtained with
O ((kQ(u)k2JQ(v)k2 + 1) √nlog (1)) queries and O ((kQ(u)k2JQ(v)k2 + 1) √nlog (1)) quantum gates.
+	Ui if sign(Ui) = 1	-
i	0	otherwise	i
Proof. Define the vectors U+ and U- as follows
0	if sign(ui ) = 1
—Ui otherwise.
It is easy to see that Q(U) = Q(U+) 一 Q(U-). Define the vectors v+ and v- in a similar way. Then,
Q(U) ∙ Q(V) = Q(u+) ∙ Q(v+) + Q(u-) ∙ Q(v-) - Q(u+) ∙ Q(v-) - Q(u-) ∙ Q(v+).	(6)
Define two more vectors of bit strings z+ and z- from Q(zi+) = Q(Ui+)Q(vi+) + Q(Ui-)Q(vi-) and Q(zi-) =
Q(Ui+)Q(vi-)+Q(Ui-vi+).Then
Q(U) ∙ Q(v) = kQ(z+)kι -kQ(z-)kι.	⑺
In the following, we use the standard ± notation to denote that a statement holds for both the + and the 一 case.
Determine the index of zm±ax := kQ(z±)kmax with the quantum maximum finding algorithm with success probability
1 — δ∕4, with O (√n log (1)) queries and O (√n log (1)) quantum gates (Durr & H0yer,1996). In case that ZmaX =
0, we infer that z± = 0, and if both are true we return the estimate 0. Otherwise, for non-zero vector z±, we apply
Statement 2 of Lemma 5 on the vectors of bit strings corresponding to Q(z±)∕zm±aX, respectively. These vectors of bit
strings can be computed efficiently from the query access and the result of the maximum finding. We obtain estimates
Γ+ and Γ- such that
Q(^±)
zmaX
Γ±
≤ E0
Q(Z±)
zmaX
(8)
1
1
with success probability at least 1 — δ∕4 for each of them, with O (+ JkQ(Zmax[log (1)
O (} VZ kQ⅞±⅞ log (δ) ) quantum gates. Note that
kQ(z+)k1	=	Q(u+) ∙Q(v+) + Q(u-) ∙Q(v-)
queries and
(9)
≤ kQ(U+)k2kQ(v+)k2 + kQ(U-)k2kQ(v-)k2 ≤ 2kQ(U)k2kQ(v)k2,	(10)
14
where the first inequality follows from Cauchy-Schwarz. Similarly, we have that kQ(z-)k1 ≤ 2kQ(u)k2kQ(v)k2.
Hence, we obtain an estimate I = z+ Γ+ - z - Γ- such that
max	max
|Q(u) ∙Q(v)-11 =
≤
≤
∣kQ(z+)kl -kQ(z-)kl - (zmaχΓ+ -Zmaxr-)∣
lkQ(z+)kl — z+aχΓ+∣ + ∣kQ(z-)kl — ZmaχΓ-∣
40kQ(u)k2kQ(v)k2.
Since kQ(u)k2 and kQ(v)k2 are given, choosing 0 = /(4kQ(u)k2kQ(v)k2) leads to the result. The run time of the
± estimation is then, using 0 = /(4kQ(u)k2kQ(v)k2),
O
kQ(u)k2kQ(v)k2√n S Z±ax	1C1 3
e	VkW±> log(δ”.
In the absence of more knowledge about the vectors, we take the bound zm±ax/kQ(z±)k1 ≤1. Then the run time is
O (kQ(u)k2kQ(v)k2Vn log (1)). Combining these resource bounds with the resource bounds for maximum finding
leads to the stated result.	□
With the following Lemma, we can remove the explicit dimension dependence of the inner product estimation. For
this lemma we suppose that one vector is given via quantum query access as before and that the other vector is given
via access to a quantum subroutine that prepares an amplitude encoding of the vector. In our work, the quantum
sampling access is provided via QRAM in Definition 1. The vectors in this lemma are considered to be fixed-point
approximations to real vectors with elements restricted to [0, 1].
Lemma 7 (Inner product estimation with quantum sampling and query access). Let c and n be two positive integers.
Let u ∈ ({0,1}c+1)n be a non-zero vector of bit strings, and let v ∈ ({0,1}c+1)n be another vector of bit strings.
Assume quantum query access to u via QA(u, n, c +1), and quantum sample access v via QS(v, n, c +1). Then:
(i)	If maxj Q(uj) =1, then there exists a quantum circuit that prepares the state
p^(vf XX qQ(vj) |j i (qQ(uj) ι0i+q1-Q(Uj)|1>)
with three queries and O (c + log n) additional gates.
(ii)	Let , δ ∈ (0, 1). If maxj Q(uj) =1, then there exists a quantum algorithm that provides an estimate Γ of
Q(Q(XQ(U) such that J Q(Q(XQ(U) 一 Γ∣ ≤ G with probability at least 1 一 δ. The algorithm requires O (ɪ log δ)
queries and O (1 log 1) gates.
(iii)	Let , δ ∈ (0, 1). Let the norm kQ(v)k1 and jmax := arg maxj Q(uj) be known. There is a quantum algorithm,
similar to (ii), which provides an estimate Γ0 of Q(V) ∙ Q(U) Such that |Q(v) ∙ Q(U) 一 Γ0∣ ≤ G with probability
at least 1 一 δ. The algorithm requires O (kQ(v)k1Q(Ujmax) log δ) queries and O (kQ(v)klQ(Ujmax) log 1) gates.
Proof. For (i), with quantum sample access and the quantum query access, perform
In	In
10i 10i l0i→ p≡≡ X qQj ji 10i l0i→ p≡≡ X Mji |uji 10i	(11)
N
→ p^(vɪ X Qj jiU∙i (TQUj) |0i +，1 - Q(Uj) |1〉).
The first step consists ofan oracle query to the vector v on the first register. The second step consists ofan oracle query
to the vector U which puts the vector component in the second register depending on the index in the first register.
The last step consists of a controlled rotation. The rotation is well-defined as Q(Uj) ≤ maxj Q(Uj) =1 and can be
implemented with O (c) gates. Then we uncompute the data register |Uji with another oracle query.
15
For (ii), define a unitary U = Ui (1 - 2 |0)(0|) U[, where Ui is the unitary obtained in (i). Define another unitary
by V =1 — 21 0 |0i h0|. Using K applications of U and V, Amplitude Estimation (Brassard et al., 2002) allows to
provide an estimation a of the quantity a = Q(Q禺U) to accuracy
L I/O √a(1 - a) l π2	”
|a - aI ≤ 2π----77-----+	2 .	(12)
K K2
Note that 0 ≤ a = Qv)∙Q(u) ≤ maxj Q(Uj) = 1. Set K > 3π. Then We obtain
(Q(v)(1	j j
|a - a| ≤ K (2√a + K) < 3 (2√a÷ 3) ≤ 33 ≤ e.	(13)
Performing a single run of amplitude estimation with K steps requires O (K) = O (ɪ) queries to the oracles and
O (ɪ) gates and succeeds with probability 8/n2. The success probability can be boosted to 1 - δ with O (log(1∕δ))
repetitions of amplitude estimation.
For (iii), from the index jmax = arg maxj Q(uj) we can obtain the bit string ujmax and its corresponding value
Q(ujmax). This allows us to prepare the quantum circuit for the transformation
Iujil0i→ ljiluji (jθQ⅛|0i+ j-QQu⅛l1i!,
(14)
from the original query access to u and basic arithmetic quantum circuits for the division. Then we run the same steps
as in (ii) with vector Q(u)/Q(ujmax), quantum sample access to vector v, and error parameter /(kQ(v)kiQ(ujmax)).
We obtain an estimate Γ from (ii) such that
γ - ^v_________QU^ ≤______________!_______.	(15)
kQ(v)ki	Q(Ujmax) ≤ kQ(v)klQ(ujmaχ)	()
Then by multiplying both sides of (15) with kQ(v)kiQ(ujmax), we obtain the required estimate Γ0 =
ΓkQ(v)klQ(ujmaχ).	口
E Rademacher complexity
The following standard generalization bound based on Rademacher complexity is employed in our analysis. For a
background on Rademacher complexity, we refer the reader to Bartlett & Mendelson (2002).
Theorem 10 (Generalization bound (Bartlett & Mendelson, 2002)). Let D be a distribution over X × Y and let
L : Y0 × Y → [-b, b] (where Y ⊆ Y0 ⊆ R) be a b-bounded loss function that is L-Lipschitz in its first argument. Let
F ⊆ (Y0)X and forany f ∈ F,let J (f, D) = E(X ,y)~D [L(f (X ),y)] and J(f,S) = m1 Pm=IL(f(xi ),yi), where
S = ((Xi,yi),..., (Xm, ym)) 〜 Dm. Thenfor any δ > 0, with probability at least 1 一 δ (over the random sample
draw for S), simultaneously for all f ∈ F, the following is true:
IJ (f, D)- J(f, S)I ≤ 4 ∙ L ∙R m (F) +2 ∙ b Y ^ogpM
2m
where Rm(F) is the Rademacher complexity of the function class F.
F Proof of Theorem 6
We first introduce several definitions and lemmas for proving the theorem. Given the coefficients αi , we generate a
hypothesis vector v(α) in the feature space by taking the linear combination over vectors ψd(xi).
Definition 5 (Auxiliary definitions). In the setting of Definitions 8 and 9, define the generated hypothesis mapping
V : RmI → Rnd as the linear combination
m1
v(α) :=	αiψd(xi ).	(16)
i=i
16
In addition, define β ∈ Rm1 as
βi := m1^yi - u(hv, ψd(xi)i) + ξ(xi)),	(17)
using V from the concept class and ∆ := v(β) with the norm η := ∣∣∆∣∣2. Finally, define P := m1^ ɪ^mtɪi ξ(xi)2 as the
average quadratic noise over the input data.
We subtle difference of the symbols v(α) and v but emphasize that v(α) will always have the parenthesis with the
input value, while v is static and fixed by the element of the concept class. To adapt to matrices Kij (with dimension
m1 × m1) and Ki0j (with dimension m2 × m1), Definition 6 and Lemma 8 are stated in general terms.
Definition 6 (Hypothesis function). Let u : R → [0, 1] be an L-Lipschitz function. Define the hypothesis function
g : RN2 X	×N2 X [Nι] → [0,1] as
(N2	∖
g(α, M, i) := u	αj Mij .	(18)
In Lemma 8, we show that if we have a good enough estimation M for the matrix M, then the estimated result
g(α, M, i) is not too far from the exact value g(α, M, i), with a dependence on kαk1.
Lemma 8. Let M, M ∈ RNɪ ×N2 be matrices ofdimension Ni X N. Let E ∈ (0,1) .Let U : R → [0,1] be L-Lipschitz.
If maXi∈[M],j∈[N2] |Mij - Mij | ≤ e,thenfor all α ∈ RN2, we have
max lg(α, M, i) — g(α, M, i) I ≤ Ldlak 1.
i∈[Nι] I	I
Proof. For all i ∈ [N1],
l N2	l
|g(a, M,i) - g(a,M, i)| ≤ L X aj(Mij- Mj)	(19)
lj=1	l
(by the L-Lipschitz conditon of u)
N2
≤ LdX |aj|	(20)
j=1
(by assumption)
= Ldkak1.	(21)
□
In the following lemma we show that if we update at according to the ALPHATRON algorithm, then the max norm of
at can be bounded in terms of T , λ and m1.
Lemma 9. For arbitrary K ∈ Rm ɪ ×ml y ∈ [0, l]mɪ, and λ ∈ R+, ifthe initial vector is a0 = 0, then by performing
the updates at+1 - Ot + mλ- (yi — g(at, K, i)) ,for each entry T times, we have maxi IaT | ≤ /.
Proof. We prove the statement by induction. The base case is obviously true. Note that y is from [0, 1] and the range
of g is also [0, 1]. Hence,
∣atI ≤ Iat-1∣ +— Iyi-g(at-1,K,i)∣ ≤
(t - 1)λ	λ _ tλ
m1 m1 m1
□
17
We state the convergence result from the original work (Goel & Klivans, 2017) in Lemma 10 before proving our own
Lemma 11. Intuitively, these lemmas prove that we indeed make progress towards the target by each iteration. The
norm kv(ω) -vk22 measures the distance between the current vector and the target. Ifwe show that this value decreases
as We run the algorithm and if We lower bound this value in term of the empirical error ε of the current vector, then
either we made progress, or the quality of the current vector is already good enough.
Lemma 10 (Convergence of Algorithm 4 from Goel & Klivans (2017)). Consider Definition 8 and 9 for the setting
and the algorithm parameters, as well as the training labels y ∈ [0, 1]m1, and the kernel matrix K ∈ [-1, 1]m1 ×m1.
For any vector ω ∈ Rm1 , let ω0 ∈ Rm1 be the vector defined as
ωi := ωi +-----(yi - g(ω, K, i)).
m1
Let h be the hypothesis function defined as h(x) = u(hv(ω), ψd(x)i), and recall from Definition 5 that η = k∆k2. If
kv(ω) - vk2 ≤ B, for B > 1, and η < 1, then
kv(ω) - vk22 - kv(ω0) - vk22
≥ L2ε(h) - Aι,
where by definition Ai := L√∕ρ + —Lη +	+ ɪ-ɪ.
We claim the folloWing modified convergence result for our Algorithm 1. The difference to the previous lemma is the
appearance of a term -I2 in the convergence bound.
Lemma 11. Consider Definition 8 and 9 for the setting and the algorithm parameters, as well as the training labels
y ∈ [0, 1]m1, and the kernel matrix K ∈ [-1, 1]m1 ×m1. Let I > 0. Suppose that Γi is an estimation of g(αt, K, i),
for all i ∈ [m1], such that
max g(αt , K, i) - Γi ≤ LI .
i∈[m1]
For any vector ω ∈ Rm1 , let ω0 ∈ Rm1 be the vector defined as
ωi0 :
ωi + mλi (yi - g(ω,κ,i)),
and ω ∈ Rm1 be the vector defined as
ωi := ωi + m- (yi - ri).
Let hbe the hypothesis function defined as h(x) = u(hv(ω), ψd(x)i). Let A1 be defined as in Lemma 10. Recall that
η = k∆k2. If kv(ω) - vk2 ≤ B, for B > 1, and η < 1, then
kv(ω) - vIli - l∣v(cω) - vk2 ≥ L2ε(h) - Ai - e2.
Proof. Recall from Definition 5 that v(α) = Pim=11 αiψ(xi). Then We have
kv(ω) - v(ω0)k2
λ2	m1
-2 IlE(Fj-gdK,j))ψ(xj)
i j=i
2
2
(expand the definition of v, ω0, and ω)
2
≤
≤
λ2	m1
F E∣Γj-g(ω,K,j)∣kΨ(xj )∣2
-i j=i
(by triangle’s inequality)
λ2L2I2 2
2—	— 1
-2i
(by assumptions)
(22)
(23)
(24)
(25)
(26)
18
Therefore, using the triangle inequality, we can deduce that
kv(ω)- vk2	≤	kv(ω)- v(ω0)k2 + kv(J)-Vk2	(27)
≤	I2 + kv(ω0) - vk22.	(28)
Note that except for the vector ω and the related conditions, Lemma 11 has the same settings as Lemma 10. Thus
by the conclusion of Lemma 10, We can lower bound ∣∣v(ω) - V∣∣2 - ∣∣v(ω0) - vk2 by 表ε(h) - A1. Together with
equation (28), we obtain the required bound.	□
Finally, in the last step of Algorithm 5, we pick the hypothesis with the minimum err value. We also lose accuracy here
as we use entries from the approximation of the matrix K0. In Lemma 12, we show that these errors are acceptable.
Lemma 12. Consider the training data samples (xi, yi) ∈ Bn × [0, 1], fori ∈ [m1], validation data samples (ai, bi) ∈
Bn × [0, 1], for i ∈ [m2], the corresponding kernel matrix K0 ∈ [-1, 1]m2×m1 where Ki0j = Kd(ai, xj), and the
vectors αt ∈ Rm1, for t ∈ [T]. Let EI > 0. Suppose that Γt is an estimation of g(αt, K0, i), for all t ∈ [T] and
i ∈ [m2], such that
max g(αt, K0, i) -Γit ≤LEI.
i∈[m2]
We define the hypothesis functions ht as ht(x) = u(hv(αt), ψd(x)i). Let
〜
t
1 m2
arg miτ(m2 X Cri- bi) ∫
and let t0 = arg mi∏t∈[τ] err(ht). Then
0
err(ht) — err(ht ) ∈ O (LeI).	(29)
Proof. By Definition 6 and the assumption on the kernel matrix K0, we obtain
ht(ai) = u(hv(αt), ψd(ai)i) = g(αt, K0, i).	(30)
We have m2 samples for validation. Recall the definition of the empirical error				
err(h)=	急 ∑mι(h(ai) - bi)2 in the Preliminary. For fixed t,			
	m2 err(ht)— — X(Γt-bi)2 m2 i=1	1 = 	 m2	∣ m2	∣ ∣∣X(g(αt, K0, i)2 - (rit)2 - 2bi(g(αt, K0, i) - rit))∣∣ ∣ i=1	∣	(31)
		(by empirical error with respect to (ai, bi) and equation (30))		
		≤ max g(αt, K0, i)2 - (rti)2 - 2bi(g(αt, K0, i) -rit) i∈[m2]		(32)
		(the maximum is at least the average)		
		≤ max (g(αt,K0,i)+rit-2bi)(g(αt,K0,i)-rit) i∈[m2]		(33)
		≤ maxι ∣g(αt, K0, i)+Γt - 2bi ∣ ∙ ∣g(αt, K0, i)-明 i∈[m2]		(34)
		≤ max 2 ∣∣g(αt, K0, i) - rit∣∣ i∈[m2]		(35)
		(g(α	t, K0, i), rit, bi are in range [0, 1]	
		≤ 2LeI.		(36)
		(by assumption)		
Hence, we have both				
0	1 m2	0
err(ht )+2LeI ≥ m X(rt - bi)2，	(37)
19
when t = t0 in (36), and
1	m2
-X(Fi - bi)
m2
i=1
C	I
2	≥ err(ht) — 2Lq,
(38)
when t = t in (36). And by the minimization of t,
m2
1- X(rt0- bi)2
m2
i=1
m2
≥ ɪ X(rf - bi)2.
m2
i=1
(39)
From the last three equations by transitivity We deduce the required e^r(ht) — err(ht0) ∈ O (LeI).
□
Now, we are going to prove the main theorem. As the original work, the theorem requires a generalization bound
Which involves the Rademacher complexity of the function class considered here. The required result is Theorem 10
in the Appendix E.
Proof of Theorem 6. We first consider the success probability of the approximation part. According to Algorithm 1,
We estimate each inner product With success probability 1 - δK /(m12 +m1m2). A union bound for these m21 +m1m2
estimations gives the total success probability to be at least 1 - δK =1 - δ. Then it is sufficient to shoW that the main
body itself succeeds With probability at least 1 - δ and indeed produces a good enough hypothesis.
The remaining proof consists of three parts. In the first part, we show that there exists t* ∈ [T] such that the empirical
error of ht* is good enough. In the second part, we show using the Rademacher complexity that for any specific
hypothesis in the concept class we introduced, the generalization error is not very far from empirical error. In the third
part, we show that by using m2 additional samples to validate all generated hypotheses, as done in the algorithm, we
are able to find a hypothesis with similar error as ht* .
First, we show that there exists a good enough hypothesis according to the empirical error. Recall the notations ∆
and P in Definition 5, and let η = ∣∣∆∣∣2. Goel & Klivans (2017) shows that η ≤ √1= (1 + p2log(1∕δ)), and
P ≤ √ + O (q logm∕δ)) by Hoeffding,s inequality. Since η and P only depend on the setting in Definition 8, the
modification done in Algorithm 1 compared to Algorithm 6 keeps the bounds for η and ρ the same. Hence, we use the
same bounds in this proof.
In the Algorithm 5, which is used in Algorithm 1, assume we are presently at the iteration t for computing the vector
αet+1 from αet . In this proof, we use the tilde above the α to emphasize that we indeed construct a different sequence
(αet)t∈[T] compared to the sequence (αt)t∈[T] of Algorithm 5 with the exact kernel matrices. One of the following two
cases is satisfied,
Case 1:∣∣v(et)-v∣∣2-I%0+1)-川2	>	Bη,	©O)
Case2 1∣v(et)	-y∣∣2 -∣∣v(et+1)	-y∣∣2	≤	BLη.	(41)
Let t* be the first iteration where Case 2 holds. We show that such an iteration exists. Assume the contradictory, that
is, Case 2 fails for each iteration. Since kv(αe0) - vk22 = k0 - vk22 ≤ B2 by assumption, however,
B2 ≥	∣∣v(e0) -K ≥∣∣v(e0) -∣∣2 -∣∣v(ek)-咪
(42)
(43)
t=0
for k iterations. Hence, in at most BL iterations Case 1 will be violated and Case 2 will have to be true. By As-
sumption 9 and the bound on η, we have that T ≥ BL, and then t* ∈ [T] must exist such that Case 2 is true.
For all t ∈ [T], define the hypothesis function ht as ht(x) = u(hv(αet), ψd(x)i). By Theorem 5, we have that
maxij Kij - Keij ≤ eK. Define the shorthand Γi := g(αet* , Ke, i), with the hypothesis function from Definition 6.
20
Then, with Lemma 8, We obtain maxi∈[mι] ∣g(et*, K, i) 一 Γi∣ ≤ LeK ∣∣et* ^. Then, by Lemma 11 with ω = et*
and eɪ = eκ ∣∣et* kι, we obtain
∣∣v 夕)一 d∣2 一 ∣∣v (et*+1) 一 寸2 ≥ L ε(ht*) - A1 一 eK ∣∣et* ∣∣2.	(44)
Note that Case 2 holds for the iteration t*. Together with the upper bound in Eq. (41), it holds by transitivity that
Bη ≥ ɪε(ht*)一A1一eK∣|et*|∣2，	(45)
which implies that
ε (ht*) ≤ BLη + L2Ai + L2eK ∣∣et* ∣∣j ∙	(46)
Recall the definition of A2，L√e + LZ ^om/δ + BL Jlogmjδ). Using the known bounds for η and ρ, we have
ε (ht*) ∈0(A2 +L2eK 卜t*∣∣2) .	(47)
The last term can be bounded as
L2e2K∣∣∣αet*∣∣∣2 ≤ T2e2K,	(48)
where we use kαet* k1 ≤ T/L from Lemma 9.
As a next step, we would like to bound ε(ht*) in terms of ε(ht*). An argument based on the Rademacher
complexity gives us the same bound as in the original work (Goel & Klivans, 2017). Define a function class
Z = {x → u(hz, ψd(x)i) 一 Ey[y|x] : kzk2 ≤ 2B}. Goel & Klivans (2017) shows that the Rademacher complexity
of Z is Rm(Z) ∈ O (BLPlTm)
Let us show that ∣∣v (et*)b satisfies the norm bound 2B, same as the Z in class Z. Note that in the first t* 一 1
iterations, Case 1 holds. By Eq. (40), we have ∣∣v (et) 一 v∣∣2 一 ∣∣v (et+i) 一 v∣∣2 > BLn ≥ 0, for t ∈ [t* 一 1]. In other
words, the distance between V (et) and V decreases when t increases in [t* 一 1]. Thus, we conclude that
∣∣∣v	(αet*	一 v∣∣∣22	≤ ∣∣v(αe0)	一 v∣∣22	= ∣v∣22	≤ B2,
(49)
and hence	by the triangle inequality, ∣v(αet* )∣2 ≤ 2B. Denote f(x) as ht* (x) 一 Ey[y|x].	Since	ht*	(x)	=
u v αet*	, ψd(x) , the function f(x) is an element of Z. Define the loss function L : [0, 1] ×	[0, 1]	→	[一1,	1]
as L(a, a0) = a2 which ignores the second argument. According to Theorem 10 in the Appendix E, with
J(f, D) = ε(ht*) and J(f, S) = ε(ht*) and b = 1, and with probability 1 一 δ,
ε (ht*) ≤ ε (ht*) + O
∈O (A2 + T 2eK) ∙
(50)
The above proof shows the existence of a good hypothesis ht* for some t* ∈ [T]. We define the index of the best
hypothesis as
t0 := arg min ε(ht),	(51)
which immediately implies that
ε(ht ) ≤ ε(ht ) ∈ O(A2 + T2eK) ∙	(52)
In the last part of this proof, we show that at Line 6 of ALPHATRON_with_Kernel, we indeed find and output a
good enough hypothesis (though this hypothesis may be different from the hypothesis derived from the output of
21
ALPHATRON_WITH_KERNEL). OUr goal is to find a hypothesis ht which minimizes ε(∙). However, ε(∙) is hard to
compute according to the definition. From Eq. (78), we have that for arbitrary hypotheses h1, h2,
ε(h1) - ε(h2) = err(h1) - err(h2).	(53)
Hence, We may find the best hypothesis by minimizing err(∙) instead of ε(∙). Formally, it holds that
t0 = arg min err(ht).	(54)
t∈[T]
As we do not know the distribution D, we are unable to compute err(∙). However, it is possible to compute the
empirical version err(∙). In Algorithm 5, we use a fresh sample set (ai, bi) of size m2 as the validation data set, and
we compute the empirical error err(ht), for each ht, on this data set. Let e0 = 1∕√m1. For fixed t, since err(ht) is in
[0,1], by a Chernoff bound on m2 ∈ O (log(T∕δ)∕(e0)2) samples, with probability 1 - δ∕T, we obtain
∣err (ht) — err (ht) ∣ ≤ e0.
(55)
Then by the union bound, with probability 1 - δ, the inequality (55) holds simultaneously for all t ∈ [T]. Since
0 ∈ O BL
we obtain the bound
m2
ɪ X
m2
i=1

t = arg min
t∈[T]
∣err (ht) - err (ht) ∣ ∈ O (A2),	(56)
for all t ∈ [T]. However, in Algorithm 1, to find the hypothesis with the minimum empirical error, we use the estimated
t
kernel matrix K0 instead of the exact inner products. Thus, we have additional errors in computing err(ht). Let
U(X ɑt ∙ Kijj- bi j
be the index of the hypothesis of the output in Algorithm 1. As before, the exact kernel matrix K0 is Ki0j = Kd(ai, xj).
According to Theorem 5, we have IKj - KjI ≤ EK. Via Lemma 8, we obtain ∣g(αt, K0, i) - g(αt, K0, i)| ≤
LeK IIatkI. By using EI = EK ∣∣αtk1 and * = g(αt, K 0, i),the upper bound on the estimation error ∣err(he)-err(ht0 )|
is shown to be in Lemma 12, as
∣ err (ht) — err (ht0) ∣ ≤ LeI.	(58)
We have kαt k1 ≤ T/L. Thus EI = EK kαt k1 ≤ TEK/L. From Eq. (56) and Eq. (58), we obtain
∣∣∣err hte -errht0∣∣∣ ∈ O(LEI+A2) ⊆ O(TEK+A2).	(59)
With Eq. (53), we obtain	∣∣ε	het	- ε	ht0	∣∣	∈ O (TEK + A2).	Hence, we have ε	hte	≤ ε	ht0	+ O	(TEK	+	A2).
From Eq. (52), ε(ht0) is bounded by O (A2 + T2K). Thus,
ε(h ) ∈ O(A2 + T Ek + TeK) .	(60)
The union bound of the probabilistic steps of estimating the full kernel matrix, the Rademacher generalization bound,
and the Chernoff bound leads to a total success probability of 1 - 3δ.	□
G Proof of Theorem 7
Proof. By the definition of rj, we have 1rt - PmIL attKji∣ ≤ eI, and by the definition of sj, we have |sj -
Pm11 OtKji∣ ≤ eI. By Definition 6 and by the Lipschitz condition of u, we obtain that ∣u(rj) - g(αt, K, j) ∣ ≤ LeI,
and ∣∣u(stj) - g(αt, K0, j)∣∣ ≤ LEI.
Consider the cases in Eqns. (40) and (41) in the proof of Theorem 6 for the sequence of at generated by Algorithm 7.
Similarily, there exists t such that Case 2 holds. Then by Lemma 11 with ω = αt*, we obtain
IMat) - vk2 -Ilv(at +1) - v∣∣2 ≥ 2εε(ht)- AI- e2.
L
(61)
22
Hence it now holds that
Bη ≥ Lε(ht* )-Aι-e2,
(62)
which implies that
ε(ht*) ≤ BLn + L2A1 + L2e2.
Using the known bound forη we have
(63)
ε(ht*) ∈O (A2 + L2 e2).
Again, by the Rademacher analysis in proof of Theorem 6, we obtain
ε(ht*) ≤ ε(ht*) + O (BLrm^ + SlogmT) j ∈ O (A2 + L2e2).
(64)
(65)
We define t0 := arg mint∈[T] ε(ht). Then ε(ht0) ≤ ε(ht* ). By Lemma 12 with Γtj = u(stj), at Line 13 in Algorithm 7,
TW
we obtain ht such that
∣err(ht) - efr(h')l ≤ LeI.
(66)
As in the proof of the Theorem 6, by Chernoffbound, setting m2 ∈ O (m1 log(T∕δ)), With probability 1 - δ, we have
∀t ∈ [T],∣err(ht) - err(ht)∣ ∈ O (A2).
(67)
Using the same idea as in the last part of the proof of the Theorem 6, we relate above inequalities (65), (66), and (67),
十
and obtain that for the output hypothesis ht,
ε (ht) ∈ O(A2 + LeI + L2e2).
(68)

For the run time complexity, the total time of preparing the sampling data structure for αt is O (Tm) because we
八 ∕ΛΠ∖	1	1	♦	IC ,1	½X /	∖	1 ʌ T	C	1	.1	F	1
prepare O (T) such structures and preparing each of them costs O (m). By Lemma 9, we have the upper bound
maxt IlatkI ≤ T. Hence, the run time of each estimation rj and Sj via Lemma 3 is bounded by O
Then the total run time is
T2KLx

log 1∕δ .
K2
O Tm + T3m-max log
L eI
Setting eI = √e obtains e(h) ∈ O (A2) with the run stated in the theorem.
(69)
□
H Proof of Theorem 8
Proof. First, consider the numerical error from truncating the α vectors. Recall that in the classical steps of the
algorithm we work in the arithmetic model where all the steps occur at infinite precision. Let αt ∈ RmI be the vector
given to infinite precision (arithmetic model) with known 0 < αmax ≤ λT∕m1 (Lemma 9). Set c1 ≥ dlog(λT ∕m1)e
and c2 ≥ ∣"log (2KmaxmI) ^j. Let a ∈ {0,1}c1+c2 be the element-wise ci + c2 bit approximation of at (stored in
QRAM). Note that
∣at ∙	Ki -	at	∙ Kil	≤ m1-max max	Iaj	-	aj |	≤	-^-^Iax ≤ -^ .	(7O)
j∈[m1]	2c2+1	2
Aside from	the	estimation of	the	inner products, the remaining part of Algorithm	3	is the	same as	Algorithm 7.
Compared to	Algorithm 7,	we	change the accuracy of the inner product estimation to	eI	∕2, hence we achieve that
Irj- at ∙ KjI ≤ e∣,∀t ∈ [T], ∀j ∈ [mi],	(71)
Isj- at ∙ Kjl ≤ e∣,∀t ∈ [T],∀j ∈ [m2],	(72)
23
with the stated success probabilities. Using Eq. (70) for the numerical error, we obtain that
Irj- at ∙ Kj | ≤ EI,∀t ∈ [T],∀j ∈ [ml],	(73)
|sj - αt ∙ Kj∣ ≤ EI,∀t ∈ [T],∀j ∈ [m2],	(74)
with the same success probabilities. Hence the same accuracy guarantees holds as in the proof of Theorem 7. For the
output hypothesis htout , we have
ε(htout) ∈ O (A? + LEI + L2EI) .	(75)
For the run time complexity, there are three terms. From Line 2 to Line 10, we perform O (m1 + m2) = O (m)
quantum maximum findings. The run time of a single run of the quantum maximum finding is bounded by
O (√mlog(1∕δ)) (Durr & H0yer,1996). Hence, this part of the algorithm takes O (m1.5 log(1∕δ)) time. In Line 12,
the time of storing all αt in QRAM is O (Tm) because we have O (T) vectors and storing each of them costs O (m)
time. For each step t, the run time of the estimations rjt and stj depends on the norm kαt k1 ≤ T∕L. Hence, the run
time of each estimation rj and Sj via (iii) of Lemma 7 is bounded by O (TKmaX log (ɪ)), and we need to estimate
O (Tm) inner products. Then the overall run time is
O (m1∙5 log (；) + Tm + TlImKmx log (δ)) .	(76)
If We set EI = √E, We obtain E(h) ≤ O(AI) and the run time is
O (m1.5log (δ) + Tm + T2m-L√arlog (；)) .	(77)
□
I The learning model for Alphatron algorithm
We consider the standard “probabilistic concept” (p-concept) learning model (Kearns & Schapire, 1994) in our paper.
Let X be the input space and Y be the output space. A concept class C is a class of functions mapping the input space
to the output space, i.e., C ⊆ YX. We define here Weak learnability With a fixed loWer bound for the error, in contrast
to the standard definition of p-concept learnability for all E0 > 0.
Definition 7 (Weak p-concept learnable). For E0 > 0, a concept class C is “weak p-concept learnable up to E0 ” if
there exists an algorithm A such that for every δ > 0, c ∈ C, and distribution D over X × Y with Ey [y Ix] = c(x) we
have that A, given access to samples drawn from D, outputs a hypothesis h : X → Y, such that with probability at
least 1 - δ,
ε(h) ：= E(x,y)~D [(h(X)-C(X))2] ≤ eo.
We call ε(h) the generalization error of hypothesis h. Alternatively, if We have m samples (Xi, yi) draWn from the
distribution D, We define the empirical error of h as
m
ε(h) := 一 X(h(xi) - C(Xi))2.
m
i=1
For convenience, We also define another similar function as
err(h) := E(χ,y)~D[(h(x) - y)2].
Since E(x,y)~D [y] = E(x,y)~D [Ey [y|x]], it is easy to see that
err(h) - err(Ey [y∣x]) = E(χ,y)~D [(h(x) - y)2] - E(χ,y)~D [(Ey [y∣x] - y)I]
=E(χ,y)~D[h(x)2 + y2 - 2yh(x) - Ey[y∣x]2 - y2 + 2yEy[y|x]]
=E(x,y)~D [h(x)2 - 2Ey [y∣x]h(x) - Ey [y∣x]2 + 2Ey [y∣x]2]
=E(x,y)~D[h(x)2 - 2Ey[y∣x]h(x) + Ey[y∣x]2]
= E(x,y)~D[h(x)2 - 2c(x)h(x) + c(x)2]
= ε(h).
(78)
24
Note that Ey[y|x] is independent of the choice of h. Hence, for hypotheses h1 and h2, we have err(h1) - err(h2) =
ε(h1) - ε(h2). Thus, we may use err() instead of ε() for comparing hypotheses. Moreover, by using the empirical
version of the err(h) function, even without knowing the probability distribution D, we are still able to evaluate the
quality of the hypothesis h given m samples (xi,yi)〜D as
1m
err(h) := - X(h(xi) - yi)2.
m
i=1
By the Chernoff bound, We may bound the generalization error err() in terms of the empirical error err() with high
probability.
To learn a good hypothesis, on the one hand, we prefer to assume a relatively simple concept class (e.g., a concept
class consisting only of linear functions). Then it is easy to design an algorithm for finding the best hypothesis in that
class. On the other hand the real-world data distribution is often complicated and cannot be captured by a hypothesis
from a simple concept class. The kernel trick is widely used to turn a simple linear concept class and a given learning
algorithm into a non-linear concept class and a corresponding learning algorithm, usually without changing too much
the algorithm. In the kernel method, we use a more general function to measure the similarity between two vectors
instead of the linear inner product. The kernel function K : X × X → R is a (usually non-linear) similarity measure
on the input space and it is defined via a feature map. Let V be an arbitrary metric space with inner product〈•，•). The
feature map ψ : X → V maps any input vector into the metric space (also called feature space). For vectors x, y ∈ X,
we define K(x, y) = hψ(x), ψ(y)i.
For our purpose, we use the multinomial kernel function to allow the learning of non-linear concepts. Consider formal
polynomials over n variables of total degree d. There are nd := Pid=0 ni monomials, which can be uniquely indexed
by the tuples (kι, ∙∙∙ , ki) ∈ [n]i for i ∈ {0,…,d}. We consider from now on the input space X = Bn ⊆ Rn
and the feature space V = Rn . We consider the standard Euclidean metric on the feature space Rnd. We define a
normalized feature map ψd : Bn → Rnd which maps a vector X = (xi, χ2,…，Xn)T ∈ X toa nd-dimensional vector
of monomials of the variables xι, x2,…，Xn. For i ∈ {0,…，d} and (kι,…，ki) ∈ [n]i, define
[ψd(X)](kι,…,ki)
1i
Y Xk .
√d+1 j=ι kj
(79)
When i = 0, we have the empty tuple and the corresponding component is the constant term 1. Note that for n ≥ 2,
we have both the components X1X2 and X2X1. This redundancy can be avoided by the use of ordered multisets but will
not influence our discussion. For X， y ∈ Bn , we can compute the inner product as
1d	i
hψd(χ), ψd(y)i = d+1 XX I Y χkj
i=0 1≤k∖,…，ki≤n	∖j = 1
U yJ = dɪ1 X (χ ∙ y)i.
Observe that by definition, for all x, y ∈ Bn, we have that hψd(x), ψd(y)i ≤ 1. This can be seen from,
1di
maxhψd(X) ,ψd(X)i = d+1 max E (kχk2)' =1,
x∈	x∈
i=0
where we have used that maxx∈Bn kxk22 = 1 for the unit ball Bn. Throughout this paper, our definition for the
normalized multinomial kernel function with degree d is
Kd(X, y) := hψd(X), ψd(y)i
1d
d+ι X(X ∙ y)i.
With these definitions, let us consider the following concept class.
Definition 8 (Distribution and concept class). Let Kd be the normalized multinomial kernel function corresponding
to the feature map ψd : Bn → Rnd, defined as above. Let B, L, ζ, > 0. Consider a distribution D on Bn × [0, 1] for
which a vector V ∈ Rnd with ∣∣V∣∣2 ≤ B exists such that the distribution satisfies
叫 y∣X] = u(h v, Ψd(X)i + ξ(X)),
where u : R → [0, 1] is a known L-Lipschitz non-decreasing function, ξ : Rn → [-ζ, ζ] is a noise function, and
E [ξ(X)2] ≤ 巳
25
The in this definition motivates Definition 7, as we will see that we cannot learn the concept class for all 0 > 0. The
intrinsic error will define a lower bound for the error. The learning guarantee for the ALPHATRON and all algorithms
in this work is proven for this concept class.
We review the classical Alphatron algorithm of Goel & Klivans (2017). The input quantities and parameters
will be the same in our paper. We split the data into two parts, training set and validation set. The training data
set contains (xi, yi)im=11 ∈ Bn × [0, 1], where m1 is the size of the training data. The validation data set contains
(ai, bi)im=21 ∈ Bn × [0, 1] of size m2. In the ALPHATRON algorithm, we first build several hypotheses from the
training set. Then we use the validation set to evaluate each hypothesis and select the optimal one from them. Let
m := m1 + m2 be the total size of the data set. Then, since m1, m2 ∈ O (m) we can use O (m) as an upper bound of
the size of data. For the input of the algorithm, we also have a function u : R → [0, 1], which will be associated with
the non-decreasing L-Lipschitz function of the concept class, and which in many machine learning applications is the
sigmoid function σ(x)= 耳I-X for example.
Algorithm 4: ALPHATRON
1	Input training data (xi, yi)im=11, testing data (ai, bi)im=21, function u : R → [0, 1], number of iterations T, degree
of the multinomial kernel d, learning rate λ.
2	α0 - 0 ∈ Rm1
3	for t — 0 to T 一 1 do
4	define ht(x) := u (Pim=11 αitKd(x, xi))
5	for i J 1 to mi do
6	αi+1 J αi + m (yi 一 ht (Xi))
7	Output αtout, where tout = argmint∈[τ] m2 Pm=I(Iιt(aj) 一 bj)2
The algorithm has a number of iterations T , which will be related to the other input quantities via the learning guar-
antees. In each iteration, the algorithm generates a new vector αt+1 and a new hypothesis ht+1 from the old ones.
The output of the algorithm is a vector atout ∈ RmI describing the hypothesis htout : Bn → [0,1], which has some
p-concept error according to the input data. First, we are interested in the general run time complexity, before we dis-
cuss the guarantees for the weak p-concept learning of the specific concept class. As stated in Theorem 2, algorithm 4
has a run time of O (Tm2 (n + log d)).
Proof of Theorem 2. For computing the multinomial kernel function Kd(X, y), we need to first compute the inner
producthX, yi in O(n) time trivially. For all r ∈ R \ {1}, since P°≤i≤d ri = rd+1-1, it costs O (log d) time to
compute the multinomial kernel function from the inner product r. Thus, by the definition of ht(X) in line 4 of the
algorithm, ht(X) is computed in O(m(n + log d)) fora given X. In the first part of the training phase, line 6 is executed
for O(T m) times. Hence, this part costs O (Tm2 (n + log d)). Similarly, in the second part of the validation phase
(line 7), a number O(T m) of calls to the function ht(a) is used. Hence, the algorithm costs O(T m2(n + log d)) in
total.	□
For obtaining a learning guarantee in the setting from Definition 8, it is supposed that the following relations between
the parameters hold.
Definition 9 (Parameter definitions and relations). Consider the setting in Definition 8, which defines the distribution
D and the parameters (B, L, ζ, ), and consider the Algorithm 4, which uses the parameters (m1, m2, T, λ). Define
the following additional parameters and fix the following relationships between the parameters.
1.	Equate the L-Lipschitz non-decreasing function from the concept class with the function u used in the algo-
rithm.
2.	Learning rate λ = 1/L.
3.	Let the training set (Xi, yi)im=11 be sampled iid from D.
4.	Let C > 0 be a large enough constant and set T = CBLymi/ log(1∕δ).
26
5.	Let C0 > 0 be a large enough constant and set m2 = C0m1 log(T /δ), and let the validation set (ai, bi)im=21
be sampled iid from D.
6.	Define A2 := L√ + LZ Nlogmj" + BL JbOgml/δ) and let C00 > 0 be a large enough constant.
The following learning guarantee was proven in Goel & Klivans (2017).
Theorem 11 (Learning guarantee of ALPHATRON, same as Goel & Klivans (2017)). Given the learning setting in
Definition 8 and the parameters defined in Definition 9, Algorithm 4 outputs αtout which describes the hypothesis
htout (x) := U (Pm* I 2 3 4 5 6I atout Kd(x, Xi)) such Ihat with probability 1 一 δ,
ε (htout) ≤ C00A2.
Next we show the proof for the theorem on weak p-concept learnability.
Proof of Theorem 1. Recall that weak p-concept learnability up to 0 means that
(htout ) ≤ 0,
(80)
with probability 1 一 δ. Hence, We desire that C00A2 ≤ €0. Note the trivial case when e0 < C00L√l, which means that
the intrinsic error of the concept class is too large, and we fail to achieve learnability. Hence, we can only prove the
case when €0 ≥ C00L√l, and we prove the theorem only for €0 = 2C00L√l, where we use a factor 2 to leave room
for the other terms in A2 . It follows that we would like to set m1 such that
C00 (LZS—+BL\—] ≤ 1 €0.	(81)
m1	m1	2
Here, Eq. (81) can be achieved by making each term smaller than €0/4 (alternatively, we can solve a quadratic equation,
which leads to more complicated equations). This means that both of following statements have to be true:
m
≥ 256」iog(i∕δ),
mi ≥ 16C'B2" log(1∕δ).
(82)
(83)
Hence, we take mi greater than the maximum of the right-hand side expressions. Employing the lower-bound
2C00L√l ≤ €0, we find that mi ≥ max {mi, m?} leads to weak p-concept learnability UP to 2C00L√e.	□
J Rewritten Alphatron algorithms
Algorithm 5： ALPHATRON_WITH_KERNEL
1 Input training data (Xi, yi)im=1i, testing data (ai, bi)im=2i, function u : R → [0, 1], number of iterations T, learning
rate λ, query access to Kij and Ki0j
2 α0 - 0 ∈ Rm1
3 for t — 0 to T — 1 do
4 for i J 1 to mi do
5	αt+1 J ɑi + m (yi - U(Pm11 ɑi ∙ Kij))
6 Output atout, where tout = argmint∈[τ] m2 PmI (U(Pm1i αt ∙ Kij) — bi)
27
1
2
3
4
5
6
7
8
9
1
2
3
4
5
6
7
8
9
10
11
12
13
Algorithm 6： ALPHATRON_WITH_PRE
Input training data (xi, yi)im=* 11, testing data (ai, bi)im=21, function u : R → [0, 1], number of iterations T, degree
of the multinomial kernel d, learning rate λ.
for i J 1 to mi do
for j J 1 to mi do
I Kij J- Kd(Xi, Xj)
for i J 1 to m2 do
for j J 1 to mi do
L KijJ Kd(ai, Xj)
atout J Run ALPHATRON_WITH_KERNEL (Algorithm 5) with all input as above and Kij and Kj.
Output αtout
K Alphatron with main loop inner product estimation
Algorithm 7: ALPHATRON_with_Kernel_and_Sampling
Input training data (Xi, yi)im=1i, testing data (ai, bi)im=2i, error parameter I and failure probability δ, function
u : R → [0, 1], number of iterations T, degree of the multinomial kernel d, learning rate λ, query access to Kji
and Kj0i, the upper bound Kmax for both |Kij | and |Ki0j |
α0 J 0 ∈ Rm1
for t J 0 to T - 1 do
Prepare sampling data structure for αt via Fact 1
for j J 1 to mi do
Define Kj as the vector (Kji, Kj2,…，KjmI)
rj J Estimate inner product at ∙ Kj to additive accuracy CI with success probability 1 一 δ∕(2Tmι) via
Lemma 3
L aj+1 J αj+ m(yj — Mrj))
for j J 1 to m2 do
Define Kj as the vector (Kj「Kj?,…，KjmJ
sj J Estimate inner product at ∙ Kj to additive accuracy EI with success probability 1 一 δ∕(2Tm?) via
Lemma 3
toutJ argmint∈[τ ] m2 Pm2i (U(Sj)—bj )2
Output αtout
28