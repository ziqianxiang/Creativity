Under review as a conference paper at ICLR 2022
Information Condensing Active Learning
Anonymous authors
Paper under double-blind review
Ab stract
We introduce Information Condensing Active Learning , a batch mode model
agnostic Active Learning method targeted at Deep Bayesian Active Learning that
focuses on acquiring labels for points which have as much information as possible
about the still unacquired points. ICAL uses the Hilbert Schmidt Independence
Criterion (HSIC) to measure the strength of the dependency between a candidate
batch of points and the unlabeled set. We develop key optimizations that allow us
to scale our method to large unlabeled sets. We show significant improvements
in terms of model accuracy and negative log likelihood on several image datasets
compared to state of the art batch mode AL methods for deep learning.
1	Introduction
Machine learning models are widely used for a vast array of real world problems. They have been
applied successfully in areas including biology (Ching et al., 2018), chemistry (Sanchez-Lengeling
and Aspuru-Guzik, 2018), physics (Guest et al., 2018), and materials engineering (Aspuru-Guzik and
Persson, 2018). Key to the success of modern machine learning methods is access to high quality data
for training the model. However such data can be expensive to collect for many problems. Active
learning (Settles, 2009) is a popular methodology to intelligently select the fewest new data points to
be labeled while not sacrificing model accuracy. The usual active learning setting is pool-based active
learning where one has access to a large unlabeled dataset DU and uses active learning to iteratively
select new points from DU to label. In this work, we propose a novel method for choosing which
points to label, with the goal of maximizing the final model’s test accuracy.
In active learning, an acquisition function is used to select which new points to label. A large
number of acquisition functions have been developed over the years, mostly for classification (Settles,
2009). Acquisition functions use model predictions or point locations (in input feature or learned
representation space) to decide which points would be most helpful to label to improve model
accuracy. We then query for the labels of those points and add them to the training set. While the past
focus for acquisition functions has been the acquisition of one point at a time, each round of label
acquisition and retraining of the ML model, particularly in the case of deep neural networks, can be
expensive. Furthermore in several applications like biology, it can be much faster to do acquisition of
a fixed number of points in parallel than one at a time. There have been several papers, particularly in
the past few years, that try to avoid this issue by acquiring points in batch. As our goal is to apply AL
in the context of modern ML models and data, we focus in this paper on batch-mode AL.
Acquisition functions can be broadly thought of as belonging to two categories. The ones from
the first category directly focus on minimizing the error rate post-acquisition. A natural choice of
such an acquisition function might be to acquire labels for points with the highest uncertainty or
points closest to the decision boundary1. In the other category, the goal is to get as close as possible
to the true underlying model. Thus here, acquisition functions select points which give the most
amount of knowledge regarding a model’s parameters where knowledge is defined as the statistical
dependence between the parameters of the model and the predictions for the selected points. Mutual
information (MI) is the usual choice for the dependency, though other choices are possible. For
well-specified model spaces, e.g. in physics, such a strategy can identify the correct model. In
machine learning, however, models are usually mis-specified, and thus the metric of evaluation even
1Uncertainty sampling can be directly linked to minimizing error rate in the context of active learning Muss-
mann and Liang (2018)
1
Under review as a conference paper at ICLR 2022
for model-identification acquisition functions is how successful they are at reducing test error. Given
this reality, we follow the viewpoint of trying to minimize the error rate of the model post-acquisition.
Our strategy is to select points that we expect would provide substantial information about the labels
of the rest of the unlabeled set, thus reducing model uncertainty. We propose acquiring a batch of
points B such that the model’s predictions on B have as high a statistical dependency as possible with
the model’s predictions on the entire unlabeled set DU . Thus we want a batch B that condenses the
most amount of information about the model’s predictions on DU. We call our method Information
Condensing Active Learning (ICAL).
A key desideratum for our acquisition function is to be model agnostic. This is partly because the
model distribution can be very heterogeneous. For example, ensembles which are often used as a
model distribution can consist of decision trees in a random forest, or different architectures for a
neural network. This means we cannot assume any closed form for the model’s predictive distribution,
and have to resort to Monte Carlo sampling of the predictions from the model to estimate the
dependence between the model’s predictions on the query batch and the unlabeled set. MI, however,
is known to be hard to approximate using just samples (Song and Ermon, 2019). Thus to scale the
method to larger batch sizes, we use the Hilbert-Schmidt Independence Criterion (HSIC), one of
the most powerful extant statistical dependence measures for high dimensional settings. Another
advantage of HSIC is that it is differentiable, which as we will discuss later, can allow applications of
the acquisition function to areas where MI would be difficult to make work.
To summarize, we introduce Information Condensing Active Learning (ICAL) which maximizes the
amount of information being gained with respect to the model’s predictions on the unlabeled set of
points. ICAL is a batch-mode acquisition function that is model agnostic and can be applied to both
classification and regression tasks. We then develop an algorithm that can scale ICAL to large batch
sizes when using HSIC as the dependence measure between random variables. As our method only
needs samples from the posterior predictive distribution, which can be obtained for both regression
and classification tasks, it is applicable to both.
2	Related work
A review of work on acquisition functions for active learning prior to the recent focus on deep learning
is given by Settles (2009). The BALD (Bayesian Active Learning by Disagreement) (Houlsby et al.,
2011) acquisition function chooses a query point which has the highest mutual information about the
model parameters. This turns out to be the point on which individual models sampled from the model
distribution are confident about in their prediction, but the overall predictive distribution for that point
has high entropy. In other words this is the point on which the models are individually confident but
disagree on the most.
In Guo and Schuurmans (2008) which builds on Guo and Greiner (2007), they formulate the problem
as an integer program where they select a batch such that the post acquisition model is highly
confident on the training set and has low uncertainty on the unlabeled set. While the latter aspect
is related to what we do, they need to retrain their model for every candidate batch they search
over in the course of trying to find the optimal batch. As the total number of possible batches is
exponential in the size of the unlabeled set, this can get too computationally expensive for neural
networks limiting the applicability of this approach. Thus as far as we know, Guo and Schuurmans
(2008) has only been applied to logistic regression. BMAL (Hoi et al., 2006) selects a batch such
that the Fisher information matrices for the total unlabeled set and the selected batch are as close
as possible. The Fisher information matrix is however quadratic in the number of parameters and
thus infeasible to compute for modern deep neural networks. BMDR (Wang and Ye, 2015) queries
points that are as close to the classifier decision boundary as possible while still being representative
of the overall sample distribution. The representativeness is measured using the maximum mean
discrepancy (MMD) (Gretton et al., 2012) of the input features between the query batch and the set of
all points, with a lower MMD indicating a more representative query batch. However this approach is
limited to classification problems, as it is based on a decision boundary. FASS (Filtered Active Subset
Selection) (Wei et al., 2015) picks the most uncertain points and then selects a subset of those points
that are as similar as possible to the whole candidate batch which favors points that can represent the
diversity of the initial set of most uncertain points.
2
Under review as a conference paper at ICLR 2022
Recently active learning methods have been extended to the deep learning setting. Gal et al. (2017)
adapts BALD (Houlsby et al., 2011) to the deep learning setting by using Monte Carlo Dropout (Gal
and Ghahramani, 2016) to do inference for their Bayesian Neural Network. They extend BALD to the
batch setting for neural networks with BatchBALD (Kirsch et al., 2019). In Pinsler et al. (2019), they
adapt the Bayesian Coreset (Campbell and Broderick, 2018) approach for active learning, though
their approach requires a batch size that changes for every acquisition. As the neural network decision
boundary is intractable, DeepFool (Ducoffe and Precioso, 2018) uses the concept of adversarial
examples (Goodfellow et al., 2014) to find points close to the decision boundary. However this
approach is again limited to classification tasks.
FF-Comp (Geifman and El-Yaniv, 2017), DAL (Gissin and Shalev-Shwartz, 2019), Sener and
Savarese (2017), and BADGE (Ash et al., 2019) operate on the learned representation, as that is
the only way the methods incorporate feedback from the training labels into the active learning
acquisition function, and they are thus not model-agnostic, as they are not extendable to any model
distribution where it is difficult to have a notion of a common representation - as ina random forests
or ensembles, etc. where the learned representation is a distribution and not a single point. This is
also the case with the model distribution - MC-dropout - we use in this paper. There is also extensive
prior work on exploiting Gaussian Processes (GPs) for Active Learning (Houlsby et al., 2011; Krause
et al., 2008). While our method can work with GPs as well, GPs are still outperformed by Neural
Networks especially for modern image datasets.
3	Background
Statistical background The entropy of a discrete distribution is defined as HpY q “
一XxPX Ppxq logPp(x)), where Ppxq is the probability of x. Mutual information (MI) between
two random variables is defined as IrX; YS = XxPX XyPY p(x, y) log(Ppxxpyy)), where p(x, y) is
the joint probability of x, y. Note that IrX; Ys “ HpYq ´ HpY|Xq “ HpXq ´ HpX|Yq. By pos-
terior predictive distribution yx We mean \§p(y∣x, θ)p(θ∣D)dθ where y is the prediction, x the input
point, θ the model parameters, and D the training data. M is the distribution of models (parametrized
by θ) we wish to choose from via active learning. As mentioned before, we use MC-dropout for our
model distribution by sampling random dropout masks and use the same set of dropout masks across
points to generate joint predictions.
Hilbert-Schmidt Independence Criterion (HSIC) Suppose we have two (possibly multivariate)
distributions X , Y and we want to measure the dependence between them. A well known way
to measure it is using distance covariance which intuitively, measures the covariance between
the distances of pairs of samples from the joint distribution P (XYq and independently sampled
X, Y (Szekely et al., 2007). HSIC can simply be thought of as distance covariance except in a kernel
space (Sejdinovic et al., 2013b). Formally, if X, Y are drawn from the joint distribution PXY , then
their HSIC is defined as -
HSIC(PXY, k, lq “Ex,x1,y,y1rk(x,x1ql(y,y1qs
` Ex,x1 rk(x, x1qsEy,y1 rl(y, y1qs
´ 2Ex,yrEx1rk(x, x1qsEy1rk(y, y1qss
where (x, yq and (x1, y1q are independent pairs drawn from PXY . Note that HSIC(PXY q “ 0 if
and only if PXY “ PXPY , that is, if X, Y are independent, for chracteristic kernels k and l.
A (sample) kernel matrix kX is a matrix whose ijth element is k(xi , xj q where k is the kernel
function and xi , xj are the i, jth samples from X.
For the case where we are measuring the joint dependence between d variables, we can use the
dHSIC statistic (Sejdinovic et al., 2013a; Pfister et al., 2018). The computational complexity of
dHSIC is bounded by the time taken to compute the kernel matrix which is O(m2dq where m is the
3
Under review as a conference paper at ICLR 2022
number of samples and d the number of random variables. We use dH SIC to denote the empirical
estimator of the dHSIC statistic.
Further details including the formula for dH SIC are in the Appendix.
Monte Carlo Dropout In Gal and Ghahramani (2016), they find that neural networks with arbitrary
depth and non-linearities is equivalent to an approximation to probabilistic deep Gaussian processes
if dropout (Srivastava et al., 2014) is applied at every layer and show they are competitive with the
state of the art.
Acquisition Function Let the batch to acquire be denoted by B with B “ |B|. Given a model
distribution M, training data Dtrain, unlabeled data DU, input space X, set of labels Y and an
acquisition function αpx, Mq, we aim to decide which batch of points to query next via:
B* “ arg max α(B, M)
We compare against the Max Entropy, BatchBALD (Kirsch et al., 2019), Filtered active submodular
selection (FASS) (Wei et al., 2015), ACS-FW (Pinsler et al., 2019) which is a Bayesian Coreset based
approach, Batch Active Learning by Diverse Gradient Embeddings (BADGE) (Ash et al., 2019), and
Random acquisition. More detailed description of these acquisition functions is in the Appendix.
4	Motivation
As mentioned previously, our goal is to acquire points that will give us as much information as
possible about the still-unlabeled points, thereby increasing the confidence of the model’s predictions.
As we will demonstrate shortly, there are situations where modern active learning methods do not
select the points that optimally decrease the uncertainty of prediction on the unlabeled data. More
formally, the examples below show that the choice of x P U that optimizes oft-used acquisition
functions may not be optimal for decreasing the entropy of predictions over the remaining points
post-acquisition (∑χipuχi‰χ H(yx，)). As mentioned in Section 3, yχi is the posterior predictive
distribution \&p(y∣x1, θ)p(θ∣D)dθ for the point x1. If We wish to optimize test-set accuracy, this can
be problematic: for well-calibrated models, we should expect worse average entropy (uncertainty)
to roughly correspond to an increase in the number of errors. This is similar to cross entropy loss
being a good proxy for 0-1 loss. Below we illustrate our points with two examples and from results
on EMNIST.
Example 1 Suppose we have an image dataset which is highly imbalanced with 90% cars, 9%
planes, and 1% ships. Then a small increase in accuracy for the car category would lead to a much
larger reduction in the overall error rate versus a large increase in accuracy for the ships category.
However, given the dominance of the cars category in the loss, the uncertainty of prediction on the
ships category is likely to be much higher. Thus the max-entropy criterion is more likely to choose
points from the pool set that turn out to be ships.
Example 2 Similar to the previous example, here we demonstrate that picking the point with the
most information about the model parameters is not optimal for decreasing the prediction uncertainty
on the still unlabeled data. The main idea behind this example is that if points in some non-trivial
fraction of the dataset have a lot of correlation between their predictive distributions, then while any
of the points may not give much information about which underlying model is the best one, getting
the labels for one of the points will greatly reduce the predictive uncertainty for the labels of the
rest of these points. As these points are a non-trivial fraction of the dataset, reducing the predictive
uncertainty on them will have a big impact on the error rate. The example in the Appendix formalizes
this intuition with a numerical example.
These observations motivate our formulation of the Information Condensing Active Learning (ICAL)
acquisition function, which selects the set of points whose acquisition would maximize the information
gained about the predictive distribution on the unlabeled set. As posterior prediction entropy should
4
Under review as a conference paper at ICLR 2022
Figure 1: Mean posterior entropy of the predictions after each acquisition on EMNIST.
be minimized by maximizing Mutual Information (MI) between predictions for unlabeled points and
prediction for selected points, ideally ICAL would use MI or related criteria to select points.
EMNIST results In Figure 1, we show the average posterior entropy of the model’s predictions
for our method compared to BatchBALD, BayesCoreset, and Random acquisition. As can be seen
from the figure, ICAL reduces the average posterior entropy much more effectively than the other
two. Details of this experiment are in Section 6.2.
5	Information Condensing Active Learning (ICAL)
In this section we present our acquisition function. As before, let Dtrain be the training points, DU
the unlabeled points, yx the random variable denoting the prediction for x by the model trained on
Dtrain , and d the dependency measure being used. Then
ɑIcALptx1, ...,xB Uq “ |D. । X dpyx1, tyxι ,..., yXB Uq
that is, we try to find the batch that has highest average dependency with respect to the unlabeled
points’ marginal predictive distribution.
Scaling αICAL estimation
As we mentioned in the introduction, we can use MI as the dependency measure d but it is tricky to
estimate MI using just samples from the distribution, particularly high-dimensional or continuous
variables. Furthermore, MI estimators are usually not differentiable. Thus if we wanted to apply
ICAL to domains where the pool set is continuous and infinite (for example, if we wanted to
query gene expression perturbations for a cell), we would run into obstacles. This motivates our
choice of HSIC as the dependency measure. In addition to being differentiable, HSIC has better
empirical sample complexity for measuring dependency as opposed to estimators for MI. Indeed,
popular MI estimators have been found to have variance that increases exponentially with the true MI
value Song and Ermon (2019). HSIC has also been successfully used in the related context of feature
selection via dependency maximization in the past Da Veiga (2015); Song et al. (2012). Furthermore,
HSIC is the squared Maximum Mean Discrepancy (MMD) between the joint distribution and the
production of marginals. MMD2 is known to be ≤ 2 KL-divergence Ramdas et al. (2015) and thus
HSIC ≤ 1 MI. Thus we use HSIC as the dependency measure for the rest of the paper.
Naively implementing ɑιcALpB, HSIC) would require O(∣Du ∣m2B ∙ C) steps per candidate batch
being evaluated where C is the number of classes, m is the number of samples taken from ppy1:B q
(Opm2Bq to estimate HSIC which we need to do |DU| times).
However, recall that HSIC is a function of solely the kernel matrices kx corresponding to the random
variables (Appendix) - in this case yχ,χ P DU. Now one can define the matrix k* “ ∣D^ XXPDU kx.
5
Under review as a conference paper at ICLR 2022
We can then prove the following propositions (proofs are in the Appendix).
Proposition 1 k* is a valid kernel matrix.
Proposition 2 XxIPDU HSIC(kx1,kxpB) “ HHsIc(XxPDU kx, kxPBq
where kxPB “ kx1 , . . . , kxB, xi P B and H{SIC denotes the sample estimator for HSIC. Using
this reformulation, We only have to compute k* “ ∣D1^ XxPDU kx once Per acquisition round. This
lowers the computation cost to O(∣Du |m2 ∙ C ' m2B ∙ C). Estimating HSIC would still require
m to increase very rapidly With B (proportional to the dimension of the joint distribution). To get
around this but still maintain batch diversity, we try two strategies.
For regular ICAL, we average the kernel matrices of points in the candidate batch. We then subsample
r points from DU every time a point is added to the batch and only compare the dependency with
those. This effectively introduces noise in the HSIC estimation. We find in practice, that this is
sufficient to acquire a diverse batch, as evidenced by Figure 4. This seems to be the case even for very
large batches, and has the added benefit of further lowering the computational cost for evaluating a
candidate batch to Oprm ∙ C ' 2 ∙ m2 ∙ C). We use r “ 200 for all our experiments.
We also develop another strategy we call ICAL-pointwise which computes the marginal increase in
dependence as a result of adding a point to the batch. If a point is highly correlated with elements of
the current batch, the marginal increase would be negligible, making the point much less likely to
be selected. The two variants perform very similarly despite ICAL-pointwise’s slight advantage in
the early acquisitions. ICAL-pointwise however requires much less time for equivalent performance
which we discuss more fully in the Appendix. For ease of presentation, we use ICAL in the Results
section and defer the full description and evaluation of ICAL-pointwise to the Appendix.
As there are an exponential number of candidate batches, an exhaustive search to find the optimal
batch is infeasible. For ICAL we use a greedy forward selection strategy to build the batch and
find that it performs well empirically. We note however that greedy forward selection is a popular
technique that has been successfully used in a large variety of contexts (Da Veiga, 2015; Blanchet
et al., 2008). As the arg max over all of DU has to be computed every time a new point is being
selected for the batch, and we have to perform this operation for each point that is added to the batch,
this gives a computation cost of O((r2m2 ' |Du∣m2B ' m2B) ∙ C) “ Ο(∣Du|m2B ∙ C). It is
possible that global nonlinear optimization of the batch ICAL criterion would work even better than
greedy optimization already does with respect to state of the art methods. Efficient techniques for
doing this optimization are not obvious and beyond the scope of this work. Even if we used gradient
based techniques to construct the batch, gradient based optimization for nonlinear problems usually
only leads to local and not global optima. Optimizations to scale ICAL even further as well as the
full Algorithm are detailed in the Appendix.
6	Results
We demonstrate the effectiveness of ICAL using standard image datasets including MNIST (LeCun
et al., 1998), Repeated MNIST (Kirsch et al., 2019), Extended MNIST (EMNIST) (Cohen et al.,
2017), fashion-MNIST, and CIFAR-10 (Krizhevsky et al., 2009). We compare ICAL with four
state of the art methods for batched active learning acquisition - BatchBALD, FASS, BADGE, and
BayesCoreset. We also compare against BALD and Max Entropy (MaxEnt) which are not explicitly
designed for batched selection, as well as against a Random acquisition baseline. ICAL consistently
outperforms BatchBALD, FASS, BADGE, and BayesCoreset on accuracy and negative log likelihood
(NLL).
Throughout our experiments, for each dataset we hold out a fixed test set for evaluating model
performance after training and a fixed validation set for training purposes. We retrain the model
from the beginning after each acquisition to avoid correlation of subsequently trained models, and
we use early stopping after 3 (6 for ResNet18) consecutive epochs of validation accuracy drop.
Following (Gal et al., 2017), we use Neural Networks with MC dropout (Gal and Ghahramani,
2016) as a variational approximation for Bayesian Neural Networks. We simply use a mixture of
rational quadratic kernels for HSIC, which has been used successfully with kernel based statistical
dependency measures in the past, with mixture length scales of {0.2,0.5,1,2,5} as in (Binkowski
6
Under review as a conference paper at ICLR 2022
Figure 2: Performance on MNIST and repeated-MNIST. Accuracy and NLL after each acquisition.
et al., 2018). All models are optimized with the Adam optimizer (Kingma and Ba, 2014) using
learning rate of 0.001 and betas (0.9,0.999). The small batch size experiments are repeated 6 times
with different seeds and a different initial training set for each run, with balanced label distribution
across all classes. The same set of seeds is used for different methods on the same task. 8 different
seeds are used for large batch size experiments using CIFAR datasets. We used NVIDIA 2080Ti
GPUs on an internal cluster.
6.1	MNIST and Repeated MNIST
We first examine ICAL’s performance on MNIST, which is a standard image dataset for handwritten
digits. We further test out the scenario with duplicated data points (repeated MNIST) as proposed
by Kirsch et al. (2019). Each data point in MNIST is replicated three times in repeated-MNIST,
and isotropic Gaussian noise with std=0.1 is added after normalizing the image. We use a CNN
consists of two convolutional layers with 32 and 64 5x5 convolution filters, each followed by MC
dropout, max-pooling and ReLU. One fully connected layer with 128 hidden units and MC dropout
is used after convolutional layers and the output soft-max layer has dimension of 10. All dropout
uses probability of 0.5, and the architecture achieved over 99% accuracy on full MNIST. We use
a validation set of size 1024 for MNIST and 3072 for repeated-MNIST, and a balanced test set of
size 10,000 for both datasets. All models are trained for up to 30 epochs for MNIST and up to 40
epochs for repeated-MNIST. We sample an initial training set of size 20 (2 per class) and conduct 30
acquisitions of batch size 10 on both datasets, and we use 50 MC dropout samples to estimate the
posterior.
The test accuracy and negative log-likelihood (NLL) are shown in Figure 2. ICAL significantly
improves the NLL and outperforms all other baselines on accuracy, with higher margins on the earlier
acquisition rounds. The performance is consistent across all runs (the variance is smaller than other
baselines), and is robust even in the repeated-MNIST setup, where all the other greedy methods show
worsened performance. We check the frequency that replicas of a single sample were included in
acquired batch as shown in Appendix Figure 8, our method (as well as BatchBALD, BayesCoreset
and random) acquired no redundant samples, whereas FASS, BADGE and max entropy acquired up
to 3 copies of some samples.
Figure 3: Performance on EMNIST and fashion-MNIST, ICAL significantly improves the accuracy
and NLL.
7
Under review as a conference paper at ICLR 2022
ICAL(OUrS)
9	10	20	30	40
rMNIST #copies of replica
BayFSCOreSet
!ill：
Figure 4: (Above)Number of times multiple copies of a replica sample is chosen in repeated-MNIST.
(Below)Histogram of the labels of all acquired points using different active learning methods on
EMNIST (47 classes). ICAL acquires more diverse, non-redundant and balanced batches while all
other methods have overly/under-represented classes.
6.2	EMNIST
We then extend the task to a more sophisticated dataset named Extended-MNIST, which consists of
47 classes of 28x28 images of both digits and letters. We used the balanced EMNIST where each
class has 2400 training examples. We use a validation set of 16384 and test set of size 18800 (400 per
class), and train for up to 40 epochs. We use a CNN consisting of three convolutional layers with 32,
64, and 128 3x3 convolution filters, each followed by MC dropout, 2x2 max-pooling and ReLU. A
fully connected layer with 512 hidden units and MC dropout is used after convolutional layers. We
use an initial train set of 47 (1 per class) and make 50 acquisitions of batch size 5. 50 MC dropout
samples are used to estimate the posterior.
The results are in Figure 3. We do substantially better in terms of both accuracy and NLL compared
to all other methods. A clue as to why our method outperforms on EMNIST can be found in
Figure 4. ICAL is able to acquire more diversed and balanced batches while all other methods have
overly/under-represented classes (note that BatchBALD, Random and MaxEnt each totally miss
examples from one of classes). Similarly, for repeated-MNIST, ICAL only acquired one unique copy
of a sample, whereas FASS, MaxEnt and BADGE acquired 2-3 replicas of the same sample. This
indicates that our method is much more robust in terms of performance even when the number of
classes increases, whereas other alternatives degenerate.
6.3	Fashion-MNIST
We also examine ICAL’s performance on fashion-MNIST which consists of 10 classes of 28x28
Zalando’s article images (Xiao et al., 2017). We use a validation set of 3072 and test set of size 10000
(1000 per class), and train for up to 40 epochs. The network architecture is the same as the one used
in MNIST task. We use an initial train set of 20 (2 per class) and make 30 acquisitions of batch size
10. 100 MC dropout samples are used to estimate the posterior. As shown in Figure 3, we again do
significantly better in terms of both accuracy and NLL compared to all other methods. Note that
almost all baselines were inferior to random baseline except ICAL, showing the robustness of our
method.
6.4	CIFAR
Finally we test our method on the CIFAR-10 and CIFAR-100 datasets Krizhevsky et al. (2009) in
a large batch size setting. CIFAR-10 consists of 10 classes with 6000 images per class whereas
CIFAR-100 has 100 classes with 600 images per class.
We use a validation set of size 1024, and a balanced test set of size 10,000 for both datasets. For
CIFAR-10, we start with an initial training set of 10000 examples (1000 per class) while for CIFAR-
100, we start with 20000 examples (200 per class). We do 10 acquisitions on CIFAR-10 and 7
acquisitions on CIFAR-100 with batch size of 3000. We use a ResNet18 with additional 2 fully
connected layers with MC dropouts, and train for up to 60 epochs with learning rate 0.1 (allow early
8
Under review as a conference paper at ICLR 2022
CIFAR-10	CIFAR-100
	ICAL outperformance p-value	AUC of accuracy curve	ICAL outperformance p-value	AUC of accuracy curve
Random	6.0e ´ 05	0.742 ± 0.004	0.130	0.335 ± 0.008
BADGE	4.7e ´ 04	0.743 ± 0.005	0.398	0.340 ± 0.004
BALD	0.248	0.751 ± 0.004	0.020	0.334 ± 0.003
FASS	8.7e ´ 06	0.742 ± 0.003	0.004	0.332 ± 0.003
BayesCoreset	0.007	0.748 ± 0.003	4.3e ´ 04	0.325 ± 0.005
MaxEnt	0.158	0.751 ± 0.003	6.4e ´ 04	0.318 ± 0.007
ICAL (ours)	N/A	0.753 ± 0.003	N/A	0.340 ± 0.004
Table 1: Area under curve of the accuracy curve for the different methods on CIFAR-10/CIFAR-100
and p-values of the ICAL out-performance significance when compared to each of the methods.
Highest AUC values of each task are highlighted in bold.
Figure 5: Performance on CIFAR-10 and CIFAR-100 with batch size=3000. Averaged accuracy
across 8 seeds (CIFAR-10) and 5 seeds (CIFAR-100) is shown.
stopping). We run with 8 different seeds for CIFAR-10 and 5 seeds for CIFAR-100. The results are
in Figure 5. Note that we are unable to compare against BatchBALD for either CIFAR dataset as it
runs out of memory.
For CIFAR-10, ICAL dominates all other methods for all acquisitions except two - When the acquired
dataset size is 19000 and when it is 28000. ICAL also has the highest area under curve (auc) for
accuracy compared to all other methods; with p-value ≤ 0.007 except for BALD and Max Entropy
for which we have better auc with p-value 0.24, 0.15 respectively 1. ICAL also achieves the highest
accuracy at the end of all 10 acquisitions. With CIFAR-100, on all acquisitions ICAL outperforms a
majority of the methods. Furthermore, ICAL again finishes with the highest accuracy by a significant
margin at the end of the acquisition rounds and it again have the highest auc compared to all other
methods.
7	Conclusion
We develop a novel batch mode active learning acquisition function ICAL that is model agnostic and
applicable to both classification and regression tasks (as it relies on only samples from the posterior
predictive distribution). We develop key optimizations that enable us to scale our method to large
acquisition batch and unlabeled set sizes. We show that we are robustly able to outperform state of
the art methods for batch mode active learning on a variety of image classification tasks in a deep
neural network setting from small to large batch size. We also show that ICAL is able to effectively
reduce predicted entropy and acquire diverse training examples.
9
Under review as a conference paper at ICLR 2022
References
Jordan T Ash, Chicheng Zhang, Akshay Krishnamurthy, John Langford, and Alekh Agarwal. Deep
batch active learning by diverse, uncertain gradient lower bounds. arXiv preprint arXiv:1906.03671,
2019.
Alan AsPUrU-GUzik and Kristin Persson. Materials acceleration platform: Accelerating advanced
energy materials discovery by integrating high-throughput methods and artificial intelligence.
2018.
MikoIaj Binkowski, Dougal J Sutherland, Michael ArbeL and Arthur Gretton. Demystifying mmd
gans. arXiv preprint arXiv:1801.01401, 2018.
F Guillaume Blanchet, Pierre Legendre, and Daniel Borcard. Forward selection of explanatory
variables. Ecology, 89(9):2623-2632, 2008.
Trevor Campbell and Tamara Broderick. Bayesian coreset construction via greedy iterative geodesic
ascent. arXiv preprint arXiv:1802.01737, 2018.
Travers Ching, Daniel S Himmelstein, Brett K Beaulieu-Jones, Alexandr A Kalinin, Brian T Do,
Gregory P Way, Enrico Ferrero, Paul-Michael Agapow, Michael Zietz, Michael M Hoffman, et al.
Opportunities and obstacles for deep learning in biology and medicine. Journal of The Royal
Society Interface, 15(141):20170387, 2018.
Gregory Cohen, Saeed Afshar, Jonathan Tapson, and Andre van Schaik. Emnist: an extension of
mnist to handwritten letters. arXiv preprint arXiv:1702.05373, 2017.
Sebastien Da Veiga. Global sensitivity analysis with dependence measures. Journal of Statistical
Computation and Simulation, 85(7):1283-1305, 2015.
Melanie Ducoffe and Frederic Precioso. Adversarial active learning for deep networks: a margin
based approach. arXiv preprint arXiv:1802.09841, 2018.
Yarin Gal and Zoubin Ghahramani. Dropout as a bayesian approximation: Representing model
uncertainty in deep learning. In international conference on machine learning, pages 1050-1059,
2016.
Yarin Gal, Riashat Islam, and Zoubin Ghahramani. Deep bayesian active learning with image data.
In Proceedings of the 34th International Conference on Machine Learning-Volume 70, pages
1183-1192. JMLR. org, 2017.
Yonatan Geifman and Ran El-Yaniv. Deep active learning over the long tail. arXiv preprint
arXiv:1711.00941, 2017.
Daniel Gissin and Shai Shalev-Shwartz. Discriminative active learning. arXiv preprint
arXiv:1907.06347, 2019.
Ian J Goodfellow, Jonathon Shlens, and Christian Szegedy. Explaining and harnessing adversarial
examples. arXiv preprint arXiv:1412.6572, 2014.
Arthur Gretton, Karsten M Borgwardt, Malte J Rasch, Bernhard Scholkopf, and Alexander Smola. A
kernel two-sample test. Journal of Machine Learning Research, 13(Mar):723-773, 2012.
Dan Guest, Kyle Cranmer, and Daniel Whiteson. Deep learning and its application to lhc physics.
Annual Review of Nuclear and Particle Science, 68:161-181, 2018.
Yuhong Guo and Russell Greiner. Optimistic active-learning using mutual information. In IJCAI,
volume 7, pages 823-829, 2007.
Yuhong Guo and Dale Schuurmans. Discriminative batch mode active learning. In Advances in
neural information processing systems, pages 593-600, 2008.
Steven CH Hoi, Rong Jin, Jianke Zhu, and Michael R Lyu. Batch mode active learning and its
application to medical image classification. In Proceedings of the 23rd international conference
on Machine learning, pages 417-424. ACM, 2006.
10
Under review as a conference paper at ICLR 2022
Neil Houlsby, Ferenc Huszar, ZoUbin Ghahramani, and Mate Lengyel. Bayesian active learning for
classification and preference learning. arXiv preprint arXiv:1112.5745, 2011.
Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint
arXiv:1412.6980, 2014.
Andreas Kirsch, Joost van Amersfoort, and Yarin Gal. Batchbald: Efficient and diverse batch
acquisition for deep bayesian active learning. In Advances in Neural Information Processing
Systems, 2019.
Andreas Krause, Ajit Singh, and Carlos Guestrin. Near-optimal sensor placements in gaussian
processes: Theory, efficient algorithms and empirical studies. Journal of Machine Learning
Research, 9(Feb):235-284, 2008.
Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images.
Technical report, Citeseer, 2009.
Yann LeCun, Leon Bottou, Yoshua Bengio, Patrick Haffner, et al. Gradient-based learning applied to
document recognition. Proceedings of the IEEE, 86(11):2278-2324, 1998.
Stephen Mussmann and Percy Liang. On the relationship between data efficiency and error for
uncertainty sampling. arXiv preprint arXiv:1806.06123, 2018.
Hanchuan Peng, Fuhui Long, and Chris Ding. Feature selection based on mutual information criteria
of max-dependency, max-relevance, and min-redundancy. IEEE Transactions on pattern analysis
and machine intelligence, 27(8):1226-1238, 2005.
Niklas Pfister, Peter BUhlmann, Bernhard Scholkopf, and Jonas Peters. Kernel-based tests forjoint
independence. Journal of the Royal Statistical Society: Series B (Statistical Methodology), 80(1):
5-31, 2018.
Robert Pinsler, Jonathan Gordon, Eric Nalisnick, and Jose Miguel Hernandez-Lobato. Bayesian batch
active learning as sparse subset approximation. In Advances in Neural Information Processing
Systems, pages 6356-6367, 2019.
Aaditya Ramdas, Sashank Jakkam Reddi, BarnabaS Poczos, Aarti Singh, and Larry Wasserman.
On the decreasing power of kernel and distance based nonparametric hypothesis tests in high
dimensions. In Twenty-Ninth AAAI Conference on Artificial Intelligence, 2015.
Benjamin Sanchez-Lengeling and Alan Aspuru-Guzik. Inverse molecular design using machine
learning: Generative models for matter engineering. Science, 361(6400):360-365, 2018.
Dino Sejdinovic, Arthur Gretton, and Wicher Bergsma. A kernel test for three-variable interactions.
In Advances in Neural Information Processing Systems, pages 1124-1132, 2013a.
Dino Sejdinovic, Bharath Sriperumbudur, Arthur Gretton, and Kenji Fukumizu. Equivalence of
distance-based and rkhs-based statistics in hypothesis testing. The Annals of Statistics, pages
2263-2291, 2013b.
Ozan Sener and Silvio Savarese. Active learning for convolutional neural networks: A core-set
approach. arXiv preprint arXiv:1708.00489, 2017.
Burr Settles. Active learning literature survey. Technical report, University of Wisconsin-Madison
Department of Computer Sciences, 2009.
Jiaming Song and Stefano Ermon. Understanding the limitations of variational mutual information
estimators. arXiv preprint arXiv:1910.06222, 2019.
Le Song, Alex Smola, Arthur Gretton, Justin Bedo, and Karsten Borgwardt. Feature selection via
dependence maximization. Journal of Machine Learning Research, 13(May):1393-1434, 2012.
Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov.
Dropout: a simple way to prevent neural networks from overfitting. The journal of machine
learning research, 15(1):1929-1958, 2014.
11
Under review as a conference paper at ICLR 2022
Gabor J Szekely, Maria L Rizzo, Nail K Bakirov, et al. Measuring and testing dependence by
correlation of distances. The annals of statistics, 35(6):2769-2794, 2007.
Zheng Wang and Jieping Ye. Querying discriminative and representative samples for batch mode
active learning. ACM Transactions on Knowledge Discovery from Data (TKDD), 9(3):17, 2015.
Kai Wei, Rishabh Iyer, and Jeff Bilmes. Submodularity in data subset selection and active learning.
In International Conference on Machine Learning, pages 1954-1963, 2015.
Han Xiao, Kashif Rasul, and Roland Vollgraf. Fashion-mnist: a novel image dataset for benchmarking
machine learning algorithms, 2017.
12
Under review as a conference paper at ICLR 2022
Appendix
Baseline acquisition function details
Max entropy selects the points that maximize the predictive entropy
αpx, Mq “ H py|x, Dtrainq
“ ´	ppy “ c|x, Dtrain q logpppy “ c|x, Dtrain qq
c
BatchBALD BatchBALD (Kirsch et al., 2019) tries to find a batch of points that has the highest
mutual information with respect to the model parameters. BALD is the non-batched version of
BatchBALD. Formally
αBatchBALDptx1, . . . , xBu, ppωqq
“ Hpyι,...,yBq — Ep(ω)rHpyι,...,yB|Ms
Filtered active Submodular selection (FASS) FASS (Wei et al., 2015) samples the β X B most
uncertain points B1 and then subselect B points that are as representative of B1 as possible. For the
measure of uncertainty, FASS uses entropy Hpy|x, Dtrain q. To measure the representativeness of B
to B1 , FASS tries to choose B to maximize the following function
fpBq“yPYiPVy
max wpi, sq
sPBXV y
Here Vy J B1 is the set of points in B1 with predicted label, y and w(i, S) = d —||Xi — x§ ||2 is the
similarity function between points indexed by i, s where xi , xs P X and d is the maximum distance
between two points. The idea here is that if a point in B already exists that is close to some point
x1 P B1, then f pB) will favor adding points to the batch that are close to points other than x1, thus
increasing the batch diversity. Note that FASS is equivalent to Max Entropy if β = 1.
Bayesian Coresets In Pinsler et al. (2019), they try to build a batch such that the log posterior after
acquiring that batch best approximates the complete data log posterior (i.e. the log posterior after
acquiring the entire pool set). Their approach closely follows the general Bayesian Coreset (Campbell
and Broderick, 2018) approach which constructs a weighted subset of data that approximates the
full dataset. Crucially (Pinsler et al., 2019) assume that the posterior predictive distribution Yp
of a point P is independent of that of the corresponding distribution Yp, of another point p1 - an
assumption we do not make. We show in the next section why avoiding such an assumption lets us
more effectively minimize the error with respect to the test distribution versus just optimizing for
maxmizing information gain for the model posterior. As (Pinsler et al., 2019) require a variable batch
size whereas all other methods (including ours) use a fixed batch size, for fairness of comparison, if
the batch for this approach is smaller than the batch size being used, we fill the rest of the batch with
random points. In practice, we only observe this being necessary for CIFAR.
Batch Active learning by Diverse Gradient Embeddings (BADGE) BADGE (Ash et al., 2019)
tries to acquire points that are distant in hallucinated gradient space (for diversity) as well as have a
high impact on the parameters of the final output layer (as a proxy for uncertainty).
Random The points are selected uniformly at random from the unlabeled pool. Thus αpx, M) is
the uniform distribution.
13
Under review as a conference paper at ICLR 2022
Motivating example 2
Suppose we have a model distribution with 10 possible models ω1, . . . , ω10 with equal prior probabil-
ity of being the true model (ppwiq “ 0.1 for @i). Let the datapoints be x1, . . . , xL with their labels
taking 4 possible values. We define pikj “ ppyi “ j|xi, ωkq as the probability of the jth class for the
ith datapoint given by the kth model. Let
Pkj = 1; j = k, 1 ≤ k ≤ 3
pk4 = 1;4 ≤ k ≤ 10
PkI = 1,p120 = 1;1 ≤ k ≤ 9, 2 ≤ i ≤ L
	ωι 32 ω3 ω4 ω5 ω6 ω7 ω8 ω9 ω10
xi X2 ...XL	∏~~2^^3~~4^^4^^4^^4^^4^^4^^T~ IIIIIIIII 2
Table 2: Labels that the different points xi take with probability 1 under different models. The
columns are the different models ωk, and the rows are the different points.
Given that we have no other information about the models, we update the posterior probabilities for
the models as follows - ifa model ωk outputs label l for a point X but after acquisition, the label for
x is not l, then we know that is not the correct model and thus its posterior probability is 0 (so it is
eliminated). Otherwise we have no way of distinguishing between the remaining models so they all
have equal posterior probability. Then for x1 the mutual information is
Iry1, ω|x1, Dt^ra,i,ns
“Hryi|xis ´ Eppω∣DtrainqrHryi|xi,ωss = 0.94
For X2... XL, I[y2—L,ω∣χ2…l, Dtrains = 0.325. However selecting x1 would decrease the expected
posterior entropy H[y2—l ∣x2…l, x1,y1, Dtrains from 0.325 to only 0.287. Acquiring any of x2…L
instead of X1 , however, would decrease that entropy to 0, which would cause a much larger decrease
in the expected posterior entropy averaged over X1...L if L is large enough. The detailed calculations
are in the later subsection.
While X2...L may not contribute much to the entropy of the joint predictive distribution or to the MI
with respect to the model parameters compared to X1 , collectively they will be weighted L ´ 1 times
more than X1 when looking at the accuracy. We should thus expect a well-calibrated model to have
a higher uncertainty, and thus make a lot more errors on X2...L, if X1 is acquired versus if any of
X2...L are acquired. For instance, in the above example, as L increases, the expected error rate would
approach « 0.7 ^(1∕7 X 6/7) X 2 = 0.17 (0.7 as 0.3 of the times the value of xi would also fix
what the true model is reducing error rate on all X to 0) if X1 is acquired as the errors for X2...L are
correlated, whereas the rate would approach 0 were any of X2...L to be acquired.
Derivation for Example 2
For Xi , the mutual information between the predicted label yi and model parameters is:
Iry1, ωlx1, Dt^ro,i,ns
“Hry"xi] ´ Ep(ω∣Dtrain) rHryi〔xi,"]]
i0	i0
=HE p(yιlxι,ωk)p(ωk)s ´ £ p(ωk)HrP(yιlxι,ωk)s
k“i	k“i
=-(3 X (ɪ X log(ɪ)) '工 X log(ɪ))
'	'10 g'10〃	10 g'10〃
-10 X 10 X (-(1 X log(1) ' 0 X log(0)))
= 0.940
14
Under review as a conference paper at ICLR 2022
For x2...L,	I[y2 — L,ω∣X2…L, DtrainS “ ´(-- X log(2q ` ɪ ^ iog(J-)q 4o	g4o7 io	、o〃 ´ io X10 (´(i X iog(iq ` o X iog(oqqq “ 0.325
After acquiring x1, assuming the true label for x1 is 1, then we update the posterior over the model
parameter such thatp1(wι)∣yι=ι “ 1 andp1Pwkqiyi“i “ 0 for 1 V k ≤ 10. Then the expected
averaged posterior entropy for x1...L is:
	1L L ∑ Hryi∣XiSIyi=I L ´ 1 i“2 1	L	10 “Lɪγ ∑Hr∑ p(yilχi,ωk)p1(ωk)lyι=ιs L ´ 1 i“2 k“1 “ -i-ɪ-j- X(L - iq ^ (´(i ^ iog(iq + o ^ iog(oqqq L ´ 1 “o
Similarly, we could compute the case where the true label for x1 is 2-4:
	1L L´ɪ X Hryi|xiS|yi=2 “ 0 i“2 1L 广7 Σ Hryi|xiS|yi=3 “ 0 L	´ 1 i“2 1L j XHryik⅛=4 i“2 “ τ 11 X(L—1)x(—(! log(iq ` lo log(m)) L ´ 1	7	7	7	7 “ o.41
The expectation of the averaged posterior entropy with respect to predicted label for y1 (since we
don’t know the true label) is:
	H[y2—L, ω∣X2...L, Xl,yiDtrainS 1L “ Eyι~p(yι∣Dtrain)[L ´ ] X HryiIxiIyIS i“2 “ ɪ ^ 0 + ɪ ^ 0 + ɪ ^ 0 + 工 X 0.41 10	10	10	10 “ 0.287
Further statistical background
A divergence Λ between two distributions is a measure of the discrepancy or difference between
two distributions P, Q. A key property of a divergence is that it is 0 if and only if P, Q are the same
distribution. In this paper, we will be using the KL divergence and the MMD, which are respectively
defined as	DKL(P ∣∣Q) “- ∑ p (xq iog( Qxqq xPX	P(xq MMDk2(P,Qq “ Ek(X, X1q + k(Y, Y 1q ´ 2k(X, Y q
15
Under review as a conference paper at ICLR 2022
where k isa kernel in the Reproducing Kernel Hilbert Space (RKHS) H and μk is the mean embedding
of the distribution into H as per the kernel k. We can then use the notion of divergence to define the
dependency d between a set of random variables X1:n as follows
d(Xl:n) “ A(Pi：n, biPi)
where P1:n is the joint distribution of X1:n, Pi the marginal of Xi with bPi being the product of
marginals. For DKL the dependency is exactly MI as defined above. For MMD the dependency is
the Hilbert-Schmidt Independence Criterion (HSIC).
Proof of Proposition 1
k* is positive Semidefinite (psd) and symmetric as the sum of psd symmetric matrices is also psd
symmetric.
Proof of Proposition 2
We show here that
dH{SIC(k1, k3, . . .,kdq ` dH{SIC(k2, k3, . . .,kdq
“ dH{SIC(k1 ` k2 , k3 , . . . , kdq
but the extension to the arbitrary sums is straightforward. Here dH SIC is the estimator for dHSIC
which is the d-variable version of HSIC. It is defined as
nn
dHSIC “ n ΣΣ πd=1kj (Xja,Xb) +
a“1 b“1
nn	n	n
港 ΠL Σ Σ kj (Xja ,Xjb) ´ 行 Σ 埠1 Σ kj (Xja,Xjb)
a“1 b“1	a“1	b“1
where kj is the kernel of the jth random variable and Xij is the ith observation for the jth random
variable. The estimator dHSIC is defined as (Sejdinovic et al., 2013a)
nn
d{C “ n ΣΣ nil/(XjaY)'
a“1 b“1
nn	n	n
次 nd“1 Σ Σ kj(xja,Xjb) ´ 西 Σ nd“1 Σ kj (Xja,Xjb)
a“1 b“1	a“1	b“1
As dHSIC reduces to HSIC when d “ 2, the proof for HSIC also follows. Using the definition
of dH{SIC above,
dH{SIC(k1, k3, . . . ,kd) + dH{SIC(k2,k3, . . .,kd) “
nn	d
n Σ Σ k1(X1a，X1b q Π kj (Xja , Xjbq
a“1 b“1	j“3
16
Under review as a conference paper at ICLR 2022
n n	dn
+	n2d Σ P Σ k1px1a ,xib qq ΠΣ kj pxja , Xjbq
a“1 b“1	j“3 b“1
nn	dnn
´	nd`rp Σ Σ k1pχ1a ,x1b qq Π Σ Σ kj IXkxG
a“1 b“1	j“3 a“1 b“1
nn	d
+	n Σ Σ k2px2a，X2b q Π kj pxja , Xjbq
a“1 b“1	j“3
n n	dn
+	n2d ΣPΣk2pχ2a,χ2bqq∏Σkjpxja,Xjbq
a“1 b“1	j“3 b“1
nn	dnn
´	n' P ΣΣk2(X2a ,X2bqq ∏ΣΣ kj (Xja,Xjbq
a“1 b“1	j“3 a“1 b“1
nn	d
“	[n2 Σ Σ k1pX1a，X1b q Π kj PXja，Xjb)
a“1 b“1	j“3
nn	d
+	n Σ Σ k2 (W，X2b q Π kj (Xja，Xjb q]
a“1 b“1	j“3
n n	dn
+	[nd Σ P Σ k1pXla，Xlb qq Π Σ /U
a“1 b“1	j“3 b“1
n n	dn
+	nd Σ P Σ k2 (X2a，X2b qq ΠΣ k j (Xja，Xjb ql
a“1 b“1	j“3 b“1
nn	dnn
´	I nd'T P Σ Σ k1(X1a，Xlb qq Π Σ Σ kj (Xja，Xjb q
a“1 b“1	j“3 a“1 b“1
nn	dnn
+	焉 P ΣΣ k2(X2a Wb qq ΠΣΣ kj (Xja ,Xjb ql
a“1 b“1	j“3 a“1 b“1
nn	d
“ n Σ Σ (k1(Xla ,Xlb q + k2(X2a ,X2b qq Π kj (Xja,Xjbq
n a“1 b“1	j“3
nn
+ nd Σ[Σ (k1(Xla Mb)
dn
+k2(Xi2a,Xi2bqq Π Σ kj (Xija, Xijb q
j“3 b“1
nn
´ 高[ΣΣ (k1(Xla ,Xlbq
dnn
+k2(Xi2a,Xi2bqq Π Σ Σ kj(Xija,Xijbq
j“3 a“1 b“1
“ dH{SIC(k1 + k2, k3, . . .,kdq
7.1	Further scaling to large batch sizes
To scale to large batch sizes, instead of adding points to the batch to be acquired one at a time, we
can add points in minibatches of size L. While this comes at the cost of possible diversity in the
17
Under review as a conference paper at ICLR 2022
batch, we find that the tradeoff is acceptable for the datasets we experimented with. This gives a final
computation cost of Op1DU* B0) where C is the number of classes. By contrast the corresponding
runtime for BatChBALD is OpDU | ∙ B ∙ C ∙ m ∙ m1) where m1 is the number of sampled configurations
of y1:n_1. For all experiments with ICAL, We were able to use L “ 1 without any scaling difficulties.
For ICAL-pointwise, We used L “ B only for CIFAR-10 and CIFAR-100. As alluded to previously,
ICAL-pointwise can accommodate much larger L compared to ICAL before its performance degrades,
allowing for much greater scaling. We evaluate this aspect of ICAL-pointwise in the Appendix.
The final algorithm is given in Algorithm 1.
7.2	Algorithm
Algorithm 1 Information Condensing Active Learning (ICAL) (M, T, Dtrain , DU , B, K, r, L)
Train M on Dtrain
repeat
B“tu
while |B| V B do
Y U “ the predictive distribution for x P DU according to M
R “ Set of r randomly selected points from DU
x1 “ argmaxx αICALpB Y txu, HSIC) with the optimizations as specified in Section 5.1
and 5.2
B“BYtx1u
end while
Dtrain “ Dtrain Y B
Retrain M on Dtrain
until T iterations reached
Return M
ICAL-pointwise
To evaluate the marginal dependency increase if a candidate point x is added to batch B, we sample
a set R from the pool set DU and compute the pairwise dHSIC of both B and B1 “ B Y txu
with respect to each point in R. Let the resulting vectors (each of length |R|) with the dHSIC
scores be dB and dB1 . Then the marginal dependency increase statistic Mx for point p is Mx “
|R| Ei maxppdBι{dB), 1) where i is the ith element of the vector. When then modify the αIcAL as
follows - α1ICALpB Y txu) “ αICALpB Y txu) ∙ pMx ´ 1) and use the point with the highest value of
α1ICAL as the point to acquire. Note that as we want to get as accurate an estimate of Mx as possible,
we ideally want to choose as large a set R as possible. In general, we also want to choose |R| to be
greater than the number of classes. This makes ICAL-pointwise more memory intensive compared to
ICAL. We also tried another criterion for batch selection based on the minimal-redundancy-maximal-
relevance Peng et al. (2005) but that had significantly worse performance compared to ICAL and
ICAL-pointwise.
NIST,FashionMNIST,MNIST and CIFAR10) with parameters set to equivalent computation cost
18
Under review as a conference paper at ICLR 2022
In Figure 6, we analyze the performance of ICAL versus ICAL-pointwise when their parameters
are set such that computational cost is about the same. As can be seen they are broadly similar with
ICAL-pointwise having a slight advantage in earlier acquisitions and ICAL being slightly better in
later ones.
We also analyze the relative performance as the mini-batch size L changes in Figure 7. In the Figure,
iter “ L is the number of iterations taken to build the entire acquisition batch (note that the actual
acquisition happens after the entire batch has been built). ICAL-pointwise requires more computation
time than ICAL in small L setup, however if time is the major constraint, ICAL-pointwise is to be
preferred as its performance degrades more slowly as L, the size of the minibatch, increases. As the
performance usually peaks at L “ 1, if one is trying to get the best performance or if memory is a
constraint, then ICAL is to be preferred.
0.30
0.28
0.26
CIFAR-100 accuracy
ICAL(iter=l)
ICAL(iter=15)
——ICAL(iter=100)
——ICAL(iter=500)
----ICAL-pointwise(iter=15)
----ICAL-pointwise(iter=100)
20000 22000 24000 26000 28000 30000 32000
Acquired dataset size
0.36
0.34
0.32
Figure 7: Relative performance of ICAL and ICAL-pointwise on CIFAR100 with different mini-batch
size L. iter “ L is the number of iterations taken to build the entire acquisition batch of size B
(note that the actual acquisition happens after the entire batch has been built)
19
Under review as a conference paper at ICLR 2022
Diversity of acquired samples in repeated-MNIST
To check if ICAL’s acquisition batches are diversed enough, we plot the number of times different
number of copies of a same sample has been acquired by each method. As shown in figure 8,
our method (as well as BatchBALD, BayesCoreset and Random) successfully avoided acquiring
redundant copies of the same sample, whereas FASS and Max Entropy acquired up to 3 copies of the
same replica in most acquisitions. This proves that the batched active learning strategies are better in
diversity.
Figure 8: Frequencies where different numbers of copies (1-3) of a same sample has been acquired
by each method.
Further CIFAR-10 and CIFAR-100 results
Further CIFAR results are in Table 1. For CIFAR-100, Random has a high p-value but that is mainly
because it performs a bit better in the beginning vs. all other methods but its performance quickly
degrades and it is far below ICAL in the final iteration.
Runtime and memory considerations
BatchBALD runs out of memory on CIFAR-10 and CIFAR-100 and thus we are unable to compare
against it for those two datasets. For the MNIST-variant datasets, ICAL takes about a minute for
building the batch to acquire (batch sizes of 5 and 10). For CIFAR-10 (batch size 3000), with L “ 1,
the runtime is about 20 minutes but it scales linearly with 1{L (Figure 10). Thus it is only 5 minutes
for L “ 30 ( iter “ 100) which is already sufficient to give comparable performance to L “ 1
(Figure 9). For CIFAR-100 (batch size 3000), the performance does degrade with high L but as we
mentioned previously, ICAL-pointwise holds up a lot better in terms of performance with high L
(Figure 7) and thus if time is a strong consideration, that variant should be used instead.
20
7.2 Algorithm
21
Figure 9: CIFAR10 performance with different L. iter “ B is the number of iterations taken to build
the entire acquisition batch of size B (note that the actual acquisition happens after the entire batch
has been built)
iter(B∕L)
Figure 10: Runtime of ICAL on CIFAR10 with different minibatch size L.
ωpuouφsφlu二匚 nα