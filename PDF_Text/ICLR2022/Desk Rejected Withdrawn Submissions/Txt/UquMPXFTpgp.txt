Under review as a conference paper at ICLR 2022
Cluster Tree for Nearest Neighbor Search
Anonymous authors
Paper under double-blind review
Ab stract
Tree-based algorithms are an important and widely used class of algorithms for Nearest Neighbor
Search (NNS) with random partition (RP) tree being arguably the most well studied. However, in
spite of possessing theoretical guarantees and strong practical performance, a major drawback of the
RP tree is its lack of adaptability to the input dataset.
Inspired by recent theoretical and practical works for NNS, we attempt to remedy this by introducing
ClusterTree, a new tree based algorithm. Our approach utilizes randomness as in RP trees while
adapting to the underlying cluster structure of the dataset to create well-balanced and meaningful
partitions. Experimental evaluations on real world datasets demonstrate improvements over RP trees
and other tree based methods for NNS while maintaining efficient construction time. In addition, we
show theoretically and empirically that ClusterTree finds partitions which are superior to those
found by RP trees in preserving the cluster structure of the input dataset.
1	Introduction
Nearest neighbor search (NNS) is a fundamental problem with applications in machine learning, data science,
databases, and many other fields and has enjoyed a vast amount of algorithmic work, both in theory and practice
(see the surveys Wang et al. (2016b); Andoni et al. (2018b); Wang et al. (2014; 2016a) and references within). The
problem is defined as follows: given a dataset X ⊂ Rd, the goal is to build a data structure over X so that for future
queries q ∈ Rd, we can quickly return one or more datapoints in X that are closest to q.
In this paper, we focus broadly on the space partition family of methods for nearest neighbor search. Given a query
q, they produce a sublinear sized subset P ⊂ X (referred to as the candidate set) that includes the desired neighbors.
Then rather than computing distances to all points in X from q, we instead compute a sublinear number of distances.
In space partition methods, the subset P returned is determined by space partitions that ‘bucket’ the points in X. This
leads to substantially faster query times necessary for scaling to large datasets.
Space partition methods1 have numerous advantages: they are suitable for distributed and parallel computing as dif-
ferent partitions can be stored on different machines (Bahmani et al., 2012; Ni et al., 2017; Li et al., 2017; Bhaskara
& Wijewardena, 2018). They are also GPU friendly due to predictable memory access patterns (Johnson et al., 2021).
In addition, they been used to design efficient and secure NNS algorithms (Chen et al., 2019). Lastly, they access
the data X in one shot, rather than multiple adaptive access, which is crucial for fast dataset construction as well as
cryptographic security. Therefore, space partition algorithms are an important (and well studied) class of algorithms
for NNS. See also the motivation given in Dong et al. (2020).
There are two main categories of algorithms that perform space partitions: (a) tree based methods (Bentley, 1975;
Uhlmann, 1991; Ciaccia et al., 1997; Katayama & Satoh, 1997; Liu et al., 2004; Beygelzimer et al., 2006; Sinha,
2015; Babenko & Lempitsky, 2017; Ram & Sinha, 2019b; Dasgupta & Sinha, 2013; 2014) and (b) hashing based
methods such as Locality Sensitive Hashing (LSH) (Gionis et al., 1999; Andoni & Indyk, 2006; Datar et al., 2004;
Wang et al., 2014; 2016a). Tree based methods have further advantages over hashing based methods as they are
extremely fast to build (requiring roughly linear time on average), and also provide the user control over the size of
sets P returned for queries by setting an appropriate leaf-size. Tree based methods have been shown to outperform
hashing based methods in practice as well (Sinha, 2014; Muja & Lowe, 2009; Liu et al., 2004).
The most well studied tree based algorithm is the random partition (RP) tree of Dasgupta & Sinha (2013; 2014). It uses
randomness in an oblivious manner to recursively compute partitions of the data. Despite some theoretical guarantees
and strong empirical performance of RP trees, they have a strong deficiency which motivates ours paper: Can we
utilize randomness while adapting to the underlying dataset structure for tree-based NNS algorithms?
1many remarks apply to indexing based methods broadly of which space partitions fall under
1
Under review as a conference paper at ICLR 2022
1.1	Our contributions
We consider a new tree based method which utilizes the power of random projections as in RP trees while adapting to
the underlying cluster structure of the dataset. We name our tree ClusterTree. Our contributions are as follows:
•	Fast dataset construction: We optimize for balanced partitions leading to fast data structure construction, while
also retaining other benefits of tree methods such as user level specification over the size of the returned set P .
•	Adapting to dataset structure: Our method adapts to the underlying cluster structure to find balanced partitions.
This leads to meaningful and explainable partitions which are especially important given the recent interest in
explainable ML algorithms (see references within recent works such as Wan et al. (2021); Dasgupta et al. (2020);
Carvalho et al. (2019) and the recent workshop XAI (2021)).
•	Theoretical Analysis and Empirical advantage: We study the performance of ClusterTree under natural
dataset modeling assumptions and relate it to recent works on graph cuts as well as fast methods for learning
Gaussian mixtures; see Sections 2.1 and 3 for more details. Furthermore, our experiments on a variety of real
datasets demonstrate that our method is superior to RP trees and other tree based methods; see Section 4.
1.2	Related Works
We briefly overview additional algorithms for NNS besides the hashing and tree-based methods outlined in the intro-
duction. The other class of methods besides space partitions include those where the goal is to generate compressed
representations or codes of the input points so that distances can be quickly estimated (Wang et al., 2014; 2016a; Ge
et al., 2014; JegoU et al., 2011; WU et al., 2017) when a linear scan is performed (whereas We are interested in sublinear
number of distance calculations). There have also been work to combine compressed codes with tree methods such as
ProdUct-Split trees (Babenko & Lempitsky, 2017). The fastest methods (with respect to the qUery time) empirically
are graph based where a similarity graph is constrUcted over the inpUt points (Malkov & YashUnin, 2020; Hajebi et al.,
2011; Malkov et al., 2014; WU et al., 2014). Then given a qUery, the graph is traversed Using a greedy algorithm Until
convergence.
Note that space partition and tree-based algorithms, which are the focUs of this paper, have several advantages over
these methods. For example, the graph based search methods lack theoretical gUarantees, have sUb-optimal ‘locality
of reference’ (which makes them UnsUited for modern architectUres (Johnson et al., 2021; Bahmani et al., 2012; Ni
et al., 2017; Li et al., 2017; Bhaskara & Wijewardena, 2018; SUn et al., 2014)), slow constrUction time, and reqUire
adaptive access to data; see the introdUction for more benefits of tree-based methods.
We focUs on tree based methods which adapt to the Underlying dataset. RP trees are stated to adapt to the intrinsic
dimensionality of the data and perform better for dataset possessing small intrinsic dimension (DasgUpta & Sinha,
2013; 2014). However, the RP tree constrUction algorithm is agnostic to strUctUre and density and Uses randomness
in a data-oblivioUs manner. Other methods which explicitly Utilize the dataset at hand inclUde PCA trees and 2-
means trees. PCA trees recUrsively split on the top principal component of the dataset (SproUll, 2005; KUmar et al.,
2008; AbdUllah et al., 2014). While more adaptive than RP trees, PCA trees can be significantly costlier to constrUct
dUe to PCA compUtation (McCartin-Lim et al., 2012). 2-means trees on the other hand, adapt to the dataset by
recUrsively finding partitions which minimize the 2-means cost (Dong et al., 2020). We note work on adapting the
gUarantees of RP trees to KD trees, bUt the performance of KD-trees is still worse than RP trees or PCA trees (Ram
& Sinha, 2019a). Lastly, we mention that several aUgmentations to RP trees have been proposed, sUch as Using sparse
random projections and traversing the tree using auxiliary information (KeiVani & Sinha, 2021; HyVOnen et al., 2016;
Sinha & Keivani, 2017). Amongst the above tree methods, RP tree is closest to ClusterTree as they both Utilize
random one-dimensional projections. HoweVer, ClusterTree employs a more sophisticated algorithm to process
the projections, which optimizes for balanced data partitions while adapting to the input dataset cluster structure.
Notation. We denote n = |X | for the dataset size and d as the dataset dimension. We use asymptotic notation
O, Ω,... to refer to asymptotics as n goes to infinity. O(∙) denotes big-Oh up to logarithmic factors. The notation
a . b means a ≤ Cb for some fixed positiVe constant C. All norms and distances in this paper are Euclidean.
2	The CLUSTERTREE Algorithm
2.1	Motivation
In this section we motiVate our algorithm for ClusterTree. First, we briefly outline tree based algorithms for NNS:
trees are constructed starting from the root node, which represents the entire dataset. Then eVery node is processed
2
Under review as a conference paper at ICLR 2022
by splitting the points at the node using some partition rule to create left and right child nodes. The partition rule
is recursively applied to each node until each leaf node of the final tree contains at most a user specified P number
of points. Therefore, any tree based algorithm can be specified by its choice of partition rule. Given a query q, we
traverse the tree, following the correct side of the partition the query lands on, until we reach a leaf node.
For RP trees, the partition rule consists of projecting points in a node to one-dimension via a random projection
and then splitting based on the median (or slight perturbation of it). It’s effectiveness comes from the fact that the
randomness is unlikely to split a query from its true nearest neighbor.
Note however that picking the median split after a random projection can be sub-optimal. For example, suppose
that the one-dimensional projection results in two well separated clusters where each cluster contains of a non-trivial
fraction of points and one cluster is slightly larger than the other. The median split passes through the larger cluster
and splits it into two parts which can adversely affect the accuracy of future queries: ifa query’s true nearest neighbors
is part of the larger cluster, we can fail to return many of such nearby points if we descend into the wrong part of the
partition. In this case, a better choice of partition would have adapted to the cluster structure by separating the two
clusters, and would have allowed for higher quality nearest neighbors to be returned. See Figure 1 for an example.
(a)	(b)
Figure 1: (a) Dataset consists of two well separated clusters. (b) Random one-dimensional projection of the dataset.
Histogram denote the density of the projections. (c) Partitioning strategy of RP trees which uses the median of the
projections. (d) Our partitioning strategy successfully separates the two clusters.
However, we still have to roughly balance every partition to ensure that the tree construction time is Od(|X|). In
particular for the nodes at the top level of the tree, we must ensure both parts of the partition contains a constant factor
of the number of points in the node to guarantee fast construction time. To balance these two objectives, we use the
well known notion of graph conductance which optimizes for both balanced partitions and cluster quality. Our strategy
after performing a one-dimensional random projection is to form a k-nearest neighbor graph, for some parameter k,
and find a conductance minimizing partition. For details, see Algorithm 1. This raises some natural questions:
•	Why graph cuts? Graphs cuts are motivated by both recent theoretical and practical developments. On the theoret-
ical side, there have been recent works on NNS for general metric spaces that rely on spectral graph theory (Andoni
et al., 2018c;d). On the practical side, a recent work of Dong et al. (2020) shows that learning space partitions
induced from graph cuts of the k-nearest neighbor graph using machine learning tools leads to a very competitive
algorithm for NNS. Furthermore, another popular set of algorithms for NNS is to build graphs built on top of the
dataset X (such as the k-nearest neighbor graph) and then given a query, perform a random walk to determine
the output. The intuition underlying these works is that graph structure captures fundamental properties about the
dataset such as clusterability, which is intimately tied to graph cuts, and is important for accurate NNS algorithms.
Lastly, another advantage of graph cuts based on conductance is that it also optimizes for balanced partitions.
We note that the learning based method and the walk based method are not in scope of this paper since both require
large computational cost to build the data structure: both require building a graph on the dataset while the learning
based method further requires finding sparse cuts on the whole graph (in addition to processing it using a neural
network). In addition, the second method crucially requires adaptive access to the dataset while tree based method
access the data in ‘one shot’ which is needed for secure search such as over encrypted data (Chen et al., 2019) in
addition to the multiple benefits outlined in Section 1.
•	Why one-dimensional projections? There are practical and theoretical reasons why we perform one-dimensional
projections. On the practical side, building the k-nearest neighbor graph in one-dimension is extremely fast (nearly
linear time) as it can be computed quickly after sorting. This is not true in larger dimensions. Furthermore in one-
dimension, there is a natural set of cuts to optimize over, which are cuts based on prefixes of the sorted order. On
the theoretical side, we motivate this procedure by studying clusterable datasets under a natural Gaussian model. By
relating to prior works, we show that under natural assumptions, (a) optimizing for hyperplane cuts based on prefixes
leads to a ‘good’ partition for NNS, and (b) one-dimensional projections can capture cluster structure present in the
3
Under review as a conference paper at ICLR 2022
original dimension. Lastly, we optimize over multiple random projections independently as a single projection can
be very noisy; however, we can significantly increase the probability of capturing the cluster structure by trying
multiple projections.
2.2	Algorithm
We present below our algorithm for ClusterTree (Algorithm 2), which employs the efficient one-dimensional cut
detection described in Algorithm 1. First, we define the notion of graph conductance.
Definition 2.1 (Conductance). Given a graph G
(V1,V) is given by
3(VI)
(V, E), V1 ⊂ V, and V
E (V1,V) _
min(vol(V1), vol (V))
V \ V1, the conductance of the cut
where E(V1,V) is the number of edges between V1 and V and Vol(S) denotes the sum of degrees of vertices in S.
Algorithm 1 OneDProjection(X, T, k)
Input: Dataset X ⊂ Rd with |X | = n, T, k ≥ 0
Output: Output partition X = X1 ∪ X2, vector v
1:	for i = 1 to T do
2:	X* i J random 1 dimensional projection of X	using	Vi ∈ Rd
3:	Y1,..., Yn J sorted Xi with each Yj ∈ R
4:	Gi J k-nearest neighbor graph on Xi
5:	3i J min1≤j≤n-1 3(Sj) where Sj is the cut	in Gi	given by	(Y1, .	. . ,	Yj), (Yj+1, . . . , Yn)
6:	wi J (vi, β) ∈ Rd is the vector encoding the	projection as well as	the	offset to	determine the cut
7:	Return the partition X1 ∪ X2 induced by the cut with the smallest 3i value and the vector wi associated with the
cut
Algorithm 2 MakeClusterTree(X, L,T,k)
Input: Dataset X ⊂ Rd, leaf size P, T, k ≥ 0
Output: Output Cluster Tree over X
1:	if |X| ≤ P then
2:	Return leaf containing X
3:	(X1 , X2, v) J OneDProjection(X, T, k)
4:	LeftSubTree J MakeClusterTree(X1 , P, T, k)
5:	RightSubTree J MakeClusterTree(X2 , P, T, k)
6:	Return (X1 , X2 , v)
Algorithm 3 Query(q, T)
Input: Query q ∈ Rd, ClusterTree T
Output: Output leaf of T where q falls in
1: Current node J T
2: while current node is not a leaf node do
3:	Pick left or right child of current node depending
on the projection and bias stored at node
4: Return the points of X located in the leaf q falls in
Remark 2.2. We note that each node of the tree is implicitly storing the vector v used to perform the partition.
3 Theoretical Analysis
In this section, we provide runtime analysis and theoretical guarantees of ClusterTree.
Runtime Analysis. We now analyze the runtime of ClusterTree. We start with quantifying the number of
operation in OneDProjection:
Lemma 3.1. The runtime of OneDProjection is O(T ∙ (nd + n log n + nk)).
Note that we might not be optimizing for the cut with lowest conductance since such a cut could potentially not respect
the sorted ordering. However, we show in the next lemma that order preserving cuts are the sparsest cuts in the graph
(fewest number of edges crossing the cut) which suggests that optimizing over prefix cuts is sufficient. We further
motivate optimizing over prefix cuts with additional theoretical results in Section 3.1 and Theorem 3.6.
Lemma 3.2. Consider a k-nearest neighbor graph on a set ofn points {X1 , . . . , Xn } ⊂ R satisfying Xi ≤ Xi+1 for
all 1 ≤ i ≤ n - 1 . The sparsest cut respects the sorted ordering. That is, the cut with the fewest number of edges will
be of the form (X1, . . . , Xj), (Xj+1, . . . , Xn) for some j.
4
Under review as a conference paper at ICLR 2022
We now state an assumption about the balanced partitions. Note that conductance automatically rewards balanced cuts
but for worst case dataset, it can potentially find a very unbalanced cut which will lead to large tree construction.
Assumption 3.3. For sufficiently large datasets |X|, Algorithm 1 returns a partition such that min(|X1 |, |X2|) ≥ c|X|
for an independent constant c ≤ 1/2.
We argue that this is a valid assumption for our algorithm as Assumption 3.3 holds for datasets with very different
structural properties. For example, the assumption holds for uniform inputs on one hand and also highly clustered
inputs on the other hand (see Section D). In addition, we empirically verify 3.3 for real datasets as well in Section D.
Lemma 3.4. Given Assumption 3.3, the tree construction time of MakeClusterTree is O(T log n ∙ (nd + n log n +
2
nk)) = O(T nd log n + Tn log2 n + Tnk log n) = O(nd) for k, T = O(1) as in our experiments.
3.1	Nearest Neighbor Guarantees
We now study the guarantees for ClusterTree for the problem of nearest neighbor search. We define two param-
eters, α and β, that have been used in the context of nearest neighbor search (Dong et al., 2020). For a given data
set, α and β measure the average distance squared between two k-nearest neighbors and the average distance squared
between two arbitrary points, respectively.
Definition 3.5. Let D be a distribution from which we sample our dataset X . Denote Dclose to be the distribution
over random k-nearest neighbors (x, x0) ∈ X. To sample from Dclose, we first pick a uniformly random point x ∈ X
and then a uniformly random k-nearest neighbor x0 of x. Define
α = E(x,x0)〜Dclosekx - x0k2,	β = Ex 〜D,X0 〜D Ilx — χ0k2.
Note that α is the expected distance squared between two ‘close’ points in X (with respect to k-nearest neighbors)
and β is the expected distance squared between two random elements of X .
The assumption α β is natural since it states that nearest neighbors are closer than arbitrary pairs of points and thus
a non-trivial algorithm is needed, rather than just returning a random point.
We will utilize the following theorem from Dong et al. (2020):
Theorem 3.6. There exists a hyperplane H = {x ∈ Rd | ha, xi = b} such that the following holds. Let X = X1 ∪X2
be the partition ofX induced by H : X1 = {x ∈ X | ha, xi ≤ b}, X2 = {x ∈ X | ha, xi > b}. Then, one has
Pr(x,x0)∈Dclose [x, x0 are separated by H]
min(Prx〜D[x ∈ Xi], Prx〜D[x ∈ X2])
Remark 3.7. The existence of the hyperplane H from Theorem 3.6 is proved using spectral graph theory and is
intimately connected to a sparse cut in the k-nearest neighbor graph of X. Furthermore, Theorem 3.6 only guarantees
the existence of a good hyperplane cut, rather than an arbitrary cut that may not be defined by a hyperplane. However,
this is exactly the family of cuts we optimize for in Algorithm 1.
Theorem 3.6 roughly states that if nearest neighbors are much closer than arbitrary pair of points, then a good hyper-
plane cut exists which separates the dataset into approximately balanced parts while also assuring that many k-nearest
neighbor pairs are in the same partition. This is a natural assumption to make since otherwise, returning arbitrary
points for queries could suffice for approximate nearest neighbor applications.
Note however that this is an assumption about the dataset in the ambient dimension, but we are finding cuts after
performing a random one-dimensional projection. We argue that after such a projection, the values of α and β are
approximately preserved.
Lemma 3.8. Suppose we sample our dataset X from distribution D. Let P be a random one-dimensional projection
independent ofX and define PX = {Px | x ∈ X}. Let αX, βX and αPX, βPX denote the values ofα and βfor the
datasets X and PX respectively. Then αPX ≤ αX and βPX = βX.
Lemma 3.8 states that if a dataset X satisfies αX βX, then the projected dataset PX also satisfies αPX βPX .
Then by an application of Theorem 3.6, we know that we can find a good hyperplane cut for the projected dataset.
However, it is not clear that such a hyperplane would also perform well for the original dataset with respect to nearest
neighbor search since points can be heavily distorted after a random projection onto one-dimension. In the next section,
we argue the soundness of performing one-dimensional projection by assuming a Gaussian mixture model. While this
is a simplifying step that does not model all realistic datasets, it serves to highlight the fact that the assumption
5
Under review as a conference paper at ICLR 2022
α β is natural for datasets with a strong cluster structure, in addition to showing significance of trying multiple
one-dimensional cut in Algorithm 1 and picking the best cut. This is important since a single one-dimensional cut has
a high chance of returning a very ‘noisy’ output, even if the original dataset has a strong cluster structure and thus
trying multiple cuts boosts the probability of finding a ‘good’ projection.
3.1.1 Gaussian Mixture Model
In this section, we analyze the performance of Algorithm 1 for the mixture of two well-separated Gaussians and
provide bounds on the number of projections that are needed in Algorithm 1 in terms of the mixture parameters. Our
intention is to demonstrate the advantageous behaviour of ClusterTree in the cases of clustered data points. We
start with the definition of c-separated Gaussians.
Definition 3.9. Gaussians N(μι, ∑ι) and N(μ2, ∑2) in Rd are defined to be C-separated for
C := ________kμ1 - μ2k___________
√d (pλ101) + pλ1N2”
where λ1 (Σ) denotes the largest eigenvalue of the matrix Σ. We consider the case were C is at least a constant value,
independent of d.
We first instantiate Theorem 3.6 for a mixture of two Gaussians.
Lemma 3.10. Suppose that dataset X with |X | = n is sampled from the distribution D 〜 WN (μι, ∑ι) + (1 一
w)N (μ2 , Σ2 ). Further, suppose that X contains at least k points from each of the two distributions that make up
D and min(w, 1 — W) = Ω(1). Define aχ and βχ as in Definition 3.5. Then aχ ≤ 2 max(tr(∑ι), tr(∑2)) and
βx = ω(Ilμι - μ2k2 + Mςi + ς2月∙
Remark 3.11. Note that the hypothesis in Lemma 3.10 about X having at least k points from each component is
easily satisfied with high probability if k = o(n) and min(w, 1 — w) = Ω(1) by a Chernoff bound.
Lemma 3.10 tells us that if Iμ1 一 μ2I2 (distance between the two Gaussian means) is sufficiently large compared to
max(tr(Σ1), tr(Σ2)), then αX /βX is bounded away from 1. For example, if we have two spherical Gaussians with
covariance matrices σ2Id and σ2Id respectively, then we require d ∙ n ∙ max(σ2,σ22) . Iμ1 — μ2I2 for αX /βX . 1/d
to hold. In terms of Definition 3.9, it suffices to require C = Ω(1) to guarantee αχ/βχ = o(1).
The following lemma connects the concept of C-separability with the guarantees of Theorem 3.6.
Lemma 3.12. Suppose dataset X is sampled from the distribution D 〜WN (μι, ∑ι) + (1 — W)N (μ2, ∑2) and the
conditions of Lemma 3.10 hold. Then αX /βX . 1/C2 for C as in Definition 3.9.
Note that the above discussion applies to the original, yet to be projected, dataset. The hope is that a well-separated
pair of Gaussians will remain so after a random projection. This might not be the case as a single projection can be
extremely noisy since we are projecting to an extremely small dimension. However, it is possible to derive the number
of one-dimensional projections needed for well-separated mixtures to also project to well-separated one-dimensional
mixtures. Thus by optimizing over multiple cuts in Algorithm 1, we can hope to pick a one-dimensional projection
which ensures that different components remain separated after the projection.
We first need to define the Q function: For x ∈ R, Q(x) = x∞ exp(—t2/2) dt. The following result bounds the
number of projections needed to achieve well-separated one-dimensional projections.
Lemma 3.13 (Corollary 1 in Kushnir et al. (2019)). Suppose our dataset X is a mixture of two C-separated spherical
Gaussians in Rd. Let T(C0, d) denote the expected number of one-dimensional projections needed for the two mixtures
to project to a C0-separatedprojection in one-dimension. Then we have limd→∞ T (C0, d) = 1/(2Q(C0/C)).
The following corollary can then be derived which states that we only need a sublogarithmic (in d) number of one-
dimensional projections to guarantee the same order of separation as in the ambient dimension.
Lemma 3.14 (Corollary 2 in Kushnir et al. (2019)). IfC0 is such that C0 ≤ C(log log d)O(1), then T(C0, d) = o(log d).
A similar result as Lemma 3.13 holds for mixtures of non-spherical Gaussians which is stated in Lemma B.3.
Hierarchical Clustering. In Section A, we provide additional theoretical results which show that ClusterTree is
able to capture hierarchical cluster structures ofa dataset. Additionally in our experimental section, we supplement the
theoretical results of Section A with experiments which display ClusterTree’s advantage over RP trees in capture
hierarchical cluster structure.
6
Under review as a conference paper at ICLR 2022
4 Experiments
Dataset	n (Size)	d (Dimension)	Dataset	n (Size)	d (Dimension)
Gaussian Mixture	5 ∙ 104	102	Spam	〜106	57
News	〜4 ∙ 105	103	SIFT	106	128
RNA	〜3 ∙ 105	8	KDD Cup	5 ∙ 104	84
Table 1: Datasets used for our experiments.
In this section we evaluate our algorithm empirically on real and synthetic datasets.
Datasets. We use the following datasets which have been used in previous machine learning works on clustering and
nearest neighbor search (for example Dong et al. (2020); Keivani & Sinha (2021); Lucic et al. (2018); Bachem et al.
(2018)): KDD Cup (clustering dataset from a Quantum physics task) (kdd, 2004), News (dataset of news text where
each feature represents if a key word is included) (Rennie, 2016), Spam (spam text where each feature represents
the presence of a particular word associated with spam) (van Rijn, 2016), SIFT (image descriptors) (AUmuller et al.,
2017), and Gaussian Mixtures ( data consisting of the mixture of two spherical Gaussians). See Table 1.
Baselines As stated in the introduction, our main focus is tree-based algorithms since they are preferable in numerous
settings (such as fast construction time, secure computation, and distributed and GPU architectures).
Our baselines are the following. Random Partition (RP) Trees: This is the method from Dasgupta & Sinha (2013;
2014) and is arguably the most common tree-based nearest neighbor search algorithm. For RP trees, the partition
strategy is to split along the median (or a small perturbation of the median) after performing a one dimensional random
projection. 2-means Trees: The partition strategy is to split points after performing a 2-means clustering. We use the
classic k-means algorithm until convergence (Dong et al., 2020). PCA Trees: For PCA trees, the partition strategy
is to split along the median after projecting onto a principal direction (Sproull, 2005; Kumar et al., 2008; Abdullah
et al., 2014). Locality Sensitive Hashing (LSH): While this is not a tree-based method, it is a classic space partition
algorithm and the most well studied theoretical approach (see references in Section 1). We use the Cross-Polytope
version from Andoni et al. (2015; 2018a). Note that this method assumes the data are normalized onto the unit sphere
which is not necessarily true for the datasets we use (and cannot be assumed for generic real-world datasets). However,
this strict requirement can be replaced with ‘approximately’ normalized vectors (i.e., all the vectors have similar norm)
which is holds for many of the datasets used in our experiments.
Evaluation Metric: As in other works, we measure the number of candidates returned for queries versus the k-NN
accuracy, which is defined to be the fraction of its actual k-nearest neighbors that are among the returned candidates
(Dong et al., 2020). This metric measures the processing time required for queries since distances are computed from
a query to all of the returned candidates.
Note that tree-based methods have close to identical query costs. For example, if the trees are all approximately
balanced, then on average we perform the same number of operations to return the set of candidates for queries
(logarithmic number of vector operations to traverse the tree). Furthermore, the ‘wall clock’ time for performing
queries can be heavily dependent on specific architectures and implementations. Thus, we focus on the quality of the
partitions given by the trees which is architecture and implementation independent.
Note that RP trees and ClusterTree have similar construction times in practice whereas PCA trees and 2-means
tree can take significantly longer. This is because finding the first few PCA eigenvectors of the dataset for example
is much more costlier with large datasets than just performing a one-dimensional projection. We display the average
over 10 independent trials in all of our results and shade ±1 standard deviation where appropriate.
Parameter Selection: In all of our experiments, we use a fixed value of T = 20 number of random projections in
Algorithm 1. For the value of k, we initialize k = 20 and keep increasing k by one until the value of the normalized
cut found stops decreasing. Intuitively, we want to do this to make sure we don’t overlook a potentially great cut.
Empirically, We observed that this only iterates over a few values (〜5) of k.
4.1 Results
Nearest Neighbor Experiments. We first evaluate the performance of the algorithms on 10 nearest neighbor error.
We ranged over various candidate sizes (by iterating over the leaf parameter) and plotted the fraction of the 10 nearest
neighbors that are in the candidate set for a query, averaged over all queries. The results for ClusterTree versus
RP trees are displayed in Figure 2. We see that for most datasets, ClusterTree is outperforming RP Trees as
7
Under review as a conference paper at ICLR 2022
Candidate Size
News
200 400
600 800 IOOO 1200 1400
Candidate Size
RNA
0.975
0.950
&
2 0.925
n
u 0.900
1 0.875
° 0.850
0.825
0.800
200	400	600	800
Candidate Size
CIusterTree
RP Tree
0.90
0.85
A
S 0.80
gθ∙75
i 0.70
° 0.65
0.60
KDD CUp
500 1000 1500 2000 2500 3000 3500
Candidate Size
Figure 2: Candidate Size vs 10-NN Error for ClusterTree and RP tree.
Gaussian Mixture
∙6∙54 3 2 」
Oooooo
>U23UU‹ NN，0I
1.0
5000 IOOOO 15000	20000	25000
Candidate Size
0.998
0.996
&
2 0.994
ɔ
< 0.992
≡ 0.990
o
I 0.988
0.986
CIusterTree
RP Tree
—■— PCATree
-LSH
News
O- ð- 6- 4- 2.
Ioooo
>UE3UU< nni
O IOOO 2000 3000 4000 5000 6000 7000
Candidate Size
I lO4 2 IO4 3 IO4 4 IO4 5 IO4 6 IO4 7 IO4
Candidate Size
RNA
0-6-4-2.
Oooo
>U2=UU< NN，0I
Figure 3: Candidate Size vs 10-NN Error with all baselines.
O IOOO 2000 3000 4000 5000 6000 7000 8000
Candidate Size
—CIusterTree
RPTree
2 Means Tree
PCATree
—LSH
KDD Cup
1.0
0-6-4.
Ooo
>U2=UU< NNSI
O IOOO 2000 3000 4000 5000 6000 7000
Candidate Size
fewer candidates are required to get better 1-NN accuracy. In Figure 3, we show the results for all of the baselines.
Altogether, we see that 2-means tree performed the worst on most datasets.
Note that PCA trees outperform ClusterTree on the KDD Cup dataset but ClusterTree is the best algorithm
on all other datasets. Note that PCA trees and 2-means trees are costly to construct, especially for large datasets, since
they are employing a much more computationally intensive partition rule than ClusterTree or RP trees. Lastly,
LSH was not as tune able as the tree-based algorithms in terms of specifying the approximate size of candidates
to return; we could smoothly increase the candidate sizes for each tree based algorithm but LSH had a strict lower
bound for the number of candidates returned per dataset, even after using a large number of hash functions, for some
datasets such as Spam. This maybe due to the fact that LSH is not well suited for point sets whose norms are not well
8
Under review as a conference paper at ICLR 2022
,oo 。 。 O
8 7 6 5 4
sn_p£ P2U3M
Gaussian Mixture
100
CIusterTree
—— RPTree
5.5-
5.0
« 4.5
ŋ 4.0
GL
§3.5-
σ3.0
ω
M 2.5
2.0
1.5
2000 4000 6000 8000 IOOOO ：
Candidate Size
RNA
SIFT
Ooooooooo
109876543
77666666
Sn - pepuM
ω
∣25
P 20
OJ
⅛ 15
ω
M io
500 750 IOOO 1250 1500 1750 2000
Candidate Size
35
30
5
500 IOOO 1500	2000
Candidate Size
Figure 4: The leaves of ClusterTree have a smaller diameter than those of RP Trees.
KDD Cup
s'≡peκpωs6'δM
IOOO 2000	3000	4000	5000
Candidate Size
25
concentrated. We also remark that some baselines are left out for various datasets, such as the 2-means tree for the
Spam dataset, due to computational in-feasibility.
We also conduct 1-NN experiments whose results are given in Section C. Overall, the results are qualitatively similar
to the 10-NN case as ClusterTree has higher accuracy for the same number of candidates returned than RP Trees.
Preserving Cluster Structure of the Dataset. We empirically validate the hypothesis that ClusterTree is su-
perior to RP trees in finding partitions that preserve the underlying cluster structure of the dataset. We designed two
related experiments to demonstrate this. For the first set of experiments, we measured the diameter of the leaves
(weighted by the leaf sizes) of each class of trees as the parameter P increases. Again the intuition here is that if the
diameter of the leaves are small, then it mostly contains points that are well-clustered together while conversely, if the
diameter is large, then the tree has bucketed together points that belong to different clusters. Our results are shown in
Figure 4. Indeed, we see that for most datasets ClusterTree results in leaves that are much more tightly clustered
than RP Trees, which again demonstrates that ClusterTree is adaptive to the underlying cluster structure of the
dataset. In Section C, we perform the same experiment on PCA trees and 2-means trees; the results are shown in
Figure 8. We observe that ClusterTree has the smallest weighted radius as a function of candidate size for most
of the datasets.
For the second set of experiments, we created instances of ClusterTree and RP trees for all of our datasets where
we set the leaf size, the parameter P in Algorithm 2, to be equal to 10% of n. We then computed the distance from a
query to the k-th nearest neighbor among the candidates returned by a tree for various values of k and averaged this
across all queries. The intuition here is that if a leaf node of a tree contains points from multiple distinct clusters, then
there will be a substantial increase in this metric at some intermediate value of k. Indeed, this is what we observe in
Figure 9 which is given in Section C. For example in the Gaussian Mixture, KDD Cup, and Spam datasets, there is a
noticeable ‘jump’ in the plots for RP trees as itis ‘mixing’ multiple clusters in the leaf nodes while for ClusterTree,
the relationship is much smoother.
Varying Number of Projections. We ranged over various candidate sizes and plotted the 1-NN error as the num-
ber of projections (the parameter T ) in Algorithm 1 varied. This is to demonstrate that optimizing over multiple
one-dimensional projections is important in practice, as suggested by our theoretical analysis. While the number of
projections does not influence the performance for some datasets, we note that for datasets such as News and RNA,
optimizing over multiple projections is advantageous. The results are shown in Figure 7 in Section C.
When can we expect ClusterTree to outperform RP trees? As backed by our theoretical work and experimen-
tal findings, we want to emphasize that ClusterTree should be preferable to RP trees when the dataset has a strong
cluster structure. We present additional experimental evidence of this hypothesis in Figure 10 in Section C.
9
Under review as a conference paper at ICLR 2022
References
Kdd cup. http://osmot.cs.cornell.edu/kddcup/datasets.html, 2004.
ICML 2021 Workshop on Theoretic Foundation, Criticism, and Application Trend of Explainable AI, 2021. URL
https://icml2021-xai.github.io/.
A. Abdullah, Alexandr Andoni, R. Kannan, and Robert Krauthgamer. Spectral approaches to nearest neighbor search.
2014 IEEE 55th Annual Symposium on Foundations of Computer Science, pp. 581-590, 2014.
Mohamad A. Akra and L. Bazzi. On the solution of linear recurrence equations. Computational Optimization and
Applications, 10:195-210, 1998.
Alexandr Andoni and P. Indyk. Near-optimal hashing algorithms for approximate nearest neighbor in high dimensions.
2006 47th Annual IEEE Symposium on Foundations of Computer Science (FOCS’06), pp. 459-468, 2006.
Alexandr Andoni, Piotr Indyk, Thijs Laarhoven, Ilya Razenshteyn, and Ludwig Schmidt. Practical and optimal lsh
for angular distance. In C. Cortes, N. Lawrence, D. Lee, M. Sugiyama, and R. Garnett (eds.), Advances in Neural
Information Processing Systems, volume 28. Curran Associates, Inc., 2015. URL https://proceedings.
neurips.cc/paper/2015/file/2823f4797102ce1a1aec05359cc16dd9-Paper.pdf.
Alexandr Andoni, Piotr Indyk, Thijs Laarhoven, Ilya Razenshteyn, and Ludwig Schmidt. Falconn - fast lookups of
cosine and other nearest neighbors. https://github.com/FALCONN-LIB/FALCONN, 2018a.
Alexandr Andoni, Piotr Indyk, and Ilya P. Razenshteyn. Approximate nearest neighbor search in high dimensions.
CoRR, abs/1806.09823, 2018b. URL http://arxiv.org/abs/1806.09823.
Alexandr Andoni, Assaf Naor, Aleksandar Nikolov, Ilya Razenshteyn, and Erik Waingarten. Data-dependent hashing
via nonlinear spectral gaps. In Proceedings of the 50th Annual ACM SIGACT Symposium on Theory of Com-
puting, STOC 2018, pp. 787-800, New York, NY, USA, 2018c. Association for Computing Machinery. ISBN
9781450355599. doi: 10.1145/3188745.3188846. URL https://doi.org/10.1145/3188745.3188846.
Alexandr Andoni, AssafNaor, Aleksandar Nikolov, Ilya Razenshteyn, and Erik Waingarten. Holder homeomorphisms
and approximate nearest neighbors. In 2018 IEEE 59th Annual Symposium on Foundations of Computer Science
(FOCS), pp. 159-169, 2018d. doi: 10.1109/FOCS.2018.00024.
Martin Aumuller, Erik Bernhardsson, and Alexander Faithfull. Ann-benchmarks: A benchmarking tool for approxi-
mate nearest neighbor algorithms. In International Conference on Similarity Search and Applications, pp. 34-49.
Springer, 2017.
Artem Babenko and V. Lempitsky. Product split trees. 2017 IEEE Conference on Computer Vision and Pattern
Recognition (CVPR), pp. 6316-6324, 2017.
Olivier Bachem, Mario Lucic, and Andreas Krause. Scalable k -means clustering via lightweight coresets. In Pro-
ceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD
’18, pp. 1119-1127, New York, NY, USA, 2018. Association for Computing Machinery. ISBN 9781450355520.
doi: 10.1145/3219819.3219973. URL https://doi.org/10.1145/3219819.3219973.
Bahman Bahmani, Ashish Goel, and Rajendra Shinde. Efficient distributed locality sensitive hashing. In Proceedings
of the 21st ACM International Conference on Information and Knowledge Management, CIKM ’12, pp. 2174-2178,
New York, NY, USA, 2012. Association for Computing Machinery. ISBN 9781450311564. doi: 10.1145/2396761.
2398596. URL https://doi.org/10.1145/2396761.2398596.
J. Bentley. Multidimensional binary search trees used for associative searching. Commun. ACM, 18:509-517, 1975.
A. Beygelzimer, S. Kakade, and J. Langford. Cover trees for nearest neighbor. Proceedings of the 23rd international
conference on Machine learning, 2006.
Aditya Bhaskara and Maheshakya Wijewardena. Distributed clustering via lsh based data partitioning. In International
Conference on Machine Learning, pp. 569-578, 2018.
Diogo V. Carvalho, Eduardo M. Pereira, and Jaime S. Cardoso. Machine learning interpretability: A survey on
methods and metrics. Electronics, 8(8), 2019. ISSN 2079-9292. doi: 10.3390/electronics8080832. URL https:
//www.mdpi.com/2079-9292/8/8/832.
10
Under review as a conference paper at ICLR 2022
Hao Chen, Ilaria Chillotti, Yihe Dong, Oxana Poburinnaya, Ilya P. Razenshteyn, and M. Riazi. Sanns: Scaling up
secure approximate k-nearest neighbors search. ArXiv, abs/1904.02033, 2019.
P. Ciaccia, M. Patella, and P. Zezula. M-tree: An efficient access method for similarity search in metric spaces. In
VLDB, 1997.
S. Dasgupta and Kaushik Sinha. Randomized partition trees for exact nearest neighbor search. In COLT, 2013.
S. Dasgupta and KaUshik Sinha. Randomized partition trees for nearest neighbor search. Algorithmica, 72:237-263,
2014.
S. Dasgupta, Nave Frost, Michal Moshkovitz, and Cyrus Rashtchian. Explainable k-means and k-medians clustering.
In ICML, 2020.
Mayur Datar, Nicole Immorlica, P. Indyk, and V. Mirrokni. Locality-sensitive hashing scheme based on p-stable
distributions. In SCG ’04, 2004.
Yihe Dong, Piotr Indyk, Ilya Razenshteyn, and Tal Wagner. Learning space partitions for nearest neighbor search.
In International Conference on Learning Representations, 2020. URL https://openreview.net/forum?
id=rkenmREFDr.
Tiezheng Ge, Kaiming He, Q. Ke, and Jian Sun. Optimized product quantization. IEEE Transactions on Pattern
Analysis and Machine Intelligence, 36:744-755, 2014.
A. Gionis, P. Indyk, and R. Motwani. Similarity search in high dimensions via hashing. In VLDB, 1999.
Kiana Hajebi, Yasin Abbasi-Yadkori, Hossein Shahbazi, and H. Zhang. Fast approximate nearest-neighbor search
with k-nearest neighbor graph. In IJCAI, 2011.
Ville Hyvonen, T. Pitkanen, S. Tasoulis, EliaS Jaasaari, Risto Tuomainen, LieWu Wang, J. Corander, and T. Roos. Fast
nearest neighbor search through sparse random projections and voting. 2016 IEEE International Conference on Big
Data (Big Data),pp. 881-888, 2016.
P. Indyk and A. Naor. Nearest-neighbor-preserving embeddings. ACM Trans. Algorithms, 3(3):31-es, August
2007. ISSN 1549-6325. doi: 10.1145/1273340.1273347. URL https://doi.org/10.1145/1273340.
1273347.
H.J6gou, M. Douze, and C. Schmid. Product quantization for nearest neighbor search. IEEE Transactions on Pattern
Analysis and Machine Intelligence, 33:117-128, 2011.
Jeff Johnson, M. Douze, and H. J6gou. Billion-scale similarity search with gpus. IEEE Transactions on Big Data, 7:
535-547, 2021.
Norio Katayama and S. Satoh. The sr-tree: an index structure for high-dimensional nearest neighbor queries. In
SIGMOD ’97, 1997.
Omid Keivani and Kaushik Sinha. Random projection-based auxiliary information can improve tree-based
nearest neighbor search. Information Sciences, 546:526-542, 2021. ISSN 0020-0255. doi: https://doi.
org/10.1016/j.ins.2020.08.054. URL https://www.sciencedirect.com/science/article/pii/
S0020025520308203.
Neeraj Kumar, L. Zhang, and S. Nayar. What is a good nearest neighbors algorithm for finding similar patches in
images? In ECCV, 2008.
Dan Kushnir, Shirin Jalali, and Iraj Saniee. Towards clustering high-dimensional gaussian mixture clouds in linear
running time. In Kamalika Chaudhuri and Masashi Sugiyama (eds.), Proceedings of the Twenty-Second Interna-
tional Conference on Artificial Intelligence and Statistics, volume 89 of Proceedings of Machine Learning Research,
pp. 1379-1387. PMLR, 16-18 Apr 2019. URL http://Proceedings.mlr.press∕v8 9∕kushnir19a.
html.
Tom Leighton. Notes on better master theorems for divide-and-conquer recurrences. In Lecture notes, MIT, 1996.
Jinfeng Li, James Cheng, Fan Yang, Yuzhen Huang, Yunjian Zhao, Xiao Yan, and Ruihao Zhao. Losha: A general
framework for scalable locality sensitive hashing. In Proceedings of the 40th International ACM SIGIR Conference
on Research and DeveloPment in Information Retrieval, pp. 635-644. ACM, 2017.
11
Under review as a conference paper at ICLR 2022
Ting Liu, A. Moore, Alexander G. Gray, and Ke Yang. An investigation of practical approximate nearest neighbor
algorithms. In NIPS, 2004.
Mario Lucic, Matthew Faulkner, Andreas Krause, and Dan Feldman. Training gaussian mixture models at scale via
coresets. Journal of Machine Learning Research, 18(160):1-25, 2018. URL http://jmlr.org/papers/
v18/15-506.html.
Yu A. Malkov and D. Yashunin. Efficient and robust approximate nearest neighbor search using hierarchical navigable
small world graphs. IEEE Transactions on Pattern Analysis and Machine Intelligence, 42:824-836, 2020.
Yury Malkov, Alexander Ponomarenko, A. Logvinov, and V. Krylov. Approximate nearest neighbor algorithm based
on navigable small world graphs. Inf. Syst., 45:61-68, 2014.
Mark McCartin-Lim, Andrew McGregor, and Rui Wang. Approximate principal direction trees. In Proceedings of the
29th International Conference on Machine Learning, ICML 2012, Edinburgh, Scotland, UK, June 26 - July 1, 2012.
icml.cc / Omnipress, 2012. URL http://icml.cc/2012/papers/348.pdf.
Marius Muja and D. Lowe. Fast approximate nearest neighbors with automatic algorithm configuration. In VISAPP,
2009.
Y Ni, K Chu, and J Bradley. Detecting abuse at scale: Locality sensitive hashing at uber engineering, 2017.
Parikshit Ram and Kaushik Sinha. Revisiting kd-tree for nearest neighbor search. In Ankur Teredesai, Vipin Kumar,
Ying Li, R6mer Rosales, Evimaria Terzi, and George Karypis (eds.), Proceedings of the 25th ACM SIGKDD In-
ternational Conference on Knowledge Discovery & Data Mining, KDD 2019, Anchorage, AK, USA, August 4-8,
2019, pp. 1378-1388. ACM, 2019a. doi: 10.1145/3292500.3330875. URL https://doi.org/10.1145/
3292500.3330875.
Parikshit Ram and Kaushik Sinha. Revisiting kd-tree for nearest neighbor search. Proceedings of the 25th ACM
SIGKDD International Conference on Knowledge Discovery & Data Mining, 2019b.
Jason Rennie. 20 newsgroups dataset. http://qwone.com/~jason/20Newsgroups/, 2016.
Kaushik Sinha. Lsh vs randomized partition trees: Which one to use for nearest neighbor search? 2014 13th Interna-
tional Conference on Machine Learning and Applications, pp. 41-46, 2014.
Kaushik Sinha. Fast l1-norm nearest neighbor search using a simple variant of randomized partition tree. In INNS
Conference on Big Data, 2015.
Kaushik Sinha and Omid Keivani. Sparse randomized partition trees for nearest neighbor search. In AISTATS, 2017.
R. Sproull. Refinements to nearest-neighbor searching ink-dimensional trees. Algorithmica, 6:579-589, 2005.
Yifang Sun, Wei Wang, Jianbin Qin, Ying Zhang, and Xuemin Lin. Srs: Solving c-approximate nearest neighbor
queries in high dimensional euclidean space with a tiny index. Proc. VLDB Endow., 8:1-12, 2014.
Jeffrey K. Uhlmann. Satisfying general proximity / similarity queries with metric trees. Information Process-
ing Letters, 40(4):175-179, 1991. ISSN 0020-0190. doi: https://doi.org/10.1016/0020-0190(91)90074-R. URL
https://www.sciencedirect.com/science/article/pii/002001909190074R.
Jan van Rijn. Bng spambase dataset. https://www.openml.org/d/40515, 2016.
Alvin Wan, Lisa Dunlap, Daniel Ho, Jihan Yin, Scott Lee, Suzanne Petryk, Sarah Adel Bargal, and Joseph E. Gonzalez.
NBDT: neural-backed decision tree. In 9th International Conference on Learning Representations, ICLR 2021,
Virtual Event, Austria, May 3-7, 2021. OpenReview.net, 2021. URL https://openreview.net/forum?
id=mCLVeEpplNE.
J. Wang, W. Liu, Sanjiv Kumar, and S. Chang. Learning to hash for indexing big data—a survey. Proceedings of the
IEEE, 104:34-57, 2016a.
Jingdong Wang, Heng Tao Shen, Jingkuan Song, and Jianqiu Ji. Hashing for similarity search: A survey. ArXiv,
abs/1408.2927, 2014.
12
Under review as a conference paper at ICLR 2022
Jun Wang, Wei Liu, Sanjiv Kumar, and Shih-Fu Chang. Learning to hash for indexing big data - A survey. Proc. IEEE,
104(1):34-57, 2016b. doi: 10.1109/JPROC.2015.2487976. URL https://doi.org/10.1109/JPROC.
2015.2487976.
Xiang Wu, Ruiqi Guo, A. T. Suresh, Sanjiv Kumar, D. Holtmann-Rice, David Simcha, and F. Yu. Multiscale quanti-
zation for fast similarity search. In NIPS, 2017.
Yubao Wu, Ruoming Jin, and X. Zhang. Fast and unified local search for random walk based k-nearest-neighbor query
in large graphs. Proceedings of the 2014 ACM SIGMOD International Conference on Management of Data, 2014.
Donghui Yan, Ling Huang, and Michael I. Jordan. Fast approximate spectral clustering. In Proceedings of the 15th
ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD ’09, pp. 907-916, New
York, NY, USA, 2009. Association for Computing Machinery. ISBN 9781605584959. doi: 10.1145/1557019.
1557118. URL https://doi.org/10.1145/1557019.1557118.
13
Under review as a conference paper at ICLR 2022
A Hierarchical Clustering
In this section, we discuss the performance of ClusterTree for hierarchical clustering. Since our tree is designed
to preserve the underlying cluster structure of the dataset, it is very natural to use it for clustering applications, such
as hierarchical clustering. In hierarchical clustering, the goal is to design a tree over the input dataset which hopefully
captures ‘multi-scale’ clustering relationships of the dataset.
To formalize this, we first define a natural hierarchical clustering model and then prove results which suggest that
ClusterTree is naturally suited to recover such a clustering. Note that traditional algorithms for hierarchical
clustering, such as computing the minimum spanning tree, require Ω(n2) time, which is prohibitive for large datasets,
whereas ClusterTee construction is nearly linear time.
Definition A.1 (Hierarchical Clustering Model). Let X be our dataset and P be a parameter. We assume there is a
tree T over X such that the following is satisfied:
•	The leaves ofT are disjoint subsets ofX of size at most some parameter P and together include all points of
X,
•	Level i ≥ 1 of T is a union of two subsets in level i - 1 of the tree where level 0 denotes the leaves. We
assume that each subset at level i - 1 contributes to exactly one subset in level i of the tree
•	The largest level of the tree is the entire dataset X .
Note that the above definition naturally describes a hierarchical clustering model over the dataset X where going up
the tree indicates larger scale cluster structure over the dataset X. We further assume a separability criteria for our
hierarchical clustering model.
Definition A.2. Let diam(S) denote the diameter of the subset S ⊆ X and d(S, S0) denote the distance between two
subsets S, S0:
d(S, S0) = min kx -yk.
x∈S,y∈S0
We say that subsets S, S0 are r-apart if
Cr max(diam(S), diamS0) ≤ d(S, S0)
for some constant C .
If we assume the above definition applies to a pair of subsets of the tree T at any some fixed level, then intuitively we
are requiring the two subsets constitute well separated clusters.
Given such an assumption, we want to argue that repeated application of Algorithm 1 can successfully recover the
underlying tree T. The intuition behind this is that if the subsets are projected, they will also be separated after a
random projection with high probability. The following lemma shows that it is indeed the case.
Lemma A.3. Suppose subsets S and S0 satisfy Definition A.2 with r ≥ ,log(∣S∣ + |S0|)/e. Let PS and PS0
respectively denote a random one-dimensional projection of the two subsets. Then with probability at least 1 - , we
have
c max(diam(PS), diam(PS0)) ≤ d(P S, PS0)
for some constant c > 1.
Lemma A.3 hints that with a sufficient separability assumption, the k-nearest neighbor graph in one-dimension will
mostly have edges within a given cluster which leads to sparse cuts between different subsets. Thus, we can reasonably
expect Algorithm 1 to separate the distinct clusters in T since it optimizes for sparse cuts. Formally, we can prove the
following statement.
Lemma A.4. Let S and S0 be two r-apart subsets of dataset X for the value of r in Lemma A.3 and P be a random
one-dimensional projection. If min(|S|, |S0|) ≥ k, then the k nearest neighbor graph ofPS ∪ PS0 will have a cut
that separates PS and PS0 with probability 1 - e.
Now consider the hierarchical clustering model given in Definition A.1 and let T denote the implicit tree over a dataset
X. Consider a node ofv ofT and at some level i let S and S0 denote the subsets at level i - 1 that comprise v. IfS and
S0 are r-apart for a sufficiently large value of r, then Lemma A.4 states that S and S0 will have an empty cut between
them after a random one-dimensional projection. If we further assume that each of the two pieces of the k-nearest
neighbor graph is connected, Algorithm 1 will exactly split apart S and S0, as intended in the tree T.
14
Under review as a conference paper at ICLR 2022
B Omitted Proofs
B.1	proof of Lemma 3.1
Proof. Computing a single one dimensional projection takes time O(nd) and computing a k-nearest neighbor graph
can be done in time O(nk) after sorting the points in O(n log n) time. Note that this crucially depends on the fact
that we perform a one dimensional projection as nearest neighbors are determined by adjacent points on the real line.
It is computationally expensive to compute such a graph in arbitrary dimensions larger than 1. Finding the sparsest
prefix cut after sorting as is done in line 5 of Algorithm 1 takes linear time once the k-nearest neighbor graph has been
constructed. Overall, the runtime of each of the T procedures is O (nd + n log n + nk).	□
B.2	proof of Lemma 3.2
Proof. The proof follows from the fact that if a cut does not respect the sorted ordering, then we can switch two points
in opposite parts of the cut to reduce the number of edges across the cut.	□
B.3	proof of Lemma 3.4
Proof. Consider the computational cost of building ClusterTree as a tree. We claim that at every level of the tree,
we do O(nd + nlog n + nk) work. To verify this, we note that cn log(cn) + (1 - c)n log((1 - c)n ≤ cn log n + (1 -
c)n log n = nlog n. Then from Assumption 3.3, there are O(log n) levels of the tree, leading to the stated runtime.
(One can also use the Akra-Bazzi method to arrive at the same conclusion, see Akra & Bazzi (1998) or Leighton
(1996)).	□
B.4	proof of Lemma 3.8
Proof. Fix the dataset X . First note that for any fixed points x, y ∈ X , we have
EkP(x-y)k2= kx-yk2
since P is independent of X. Now for any x ∈ X, the average distance squared from Px to its k-nearest neighbors
in PX is at most the average distance squared from Px to the points that were originally its k-nearest neighbors in
X. This gives that αPX ≤ αX. Finally, note that that the expected value of the sum of all pairwise distances after the
projection is the same as the sum of all pairwise distances from our observation above. This proves βPX = βX, as
desired.	□
B.5	Proof of Lemma 3.10
Proof. To prove Lemma 3.10, we will need the following auxiliary result.
Lemma B.1. Suppose X 〜N(μι, ∑ι) and y 〜N(μ2, ∑2) Then
E[kx - yk2] = l∣μι - μ2∣l2 + MςI)+ Mς21
Proof. Note that X — y is distributed as N(μι 一 μ2, ∑ι 一 ∑2) and thus, X — y 〜μι 一 μ2 + Az where Z 〜N(0, I)
and A satisfies AAT = Σ1 + Σ2 . Thus,
lX 一 yl2 = lμ1 一 μ2l2 + 2(μ1 一 μ2)TAz + zT AT Az.
Since E[z] = 0, we have
E[kx -yk2] = kμι- μ2k2 + E[zT AT Az]
and
E[zTATAz] =	E[zizj](ATA)ij	=	(AT A)i	=	tr(AT A)	=	tr(AAT)	= tr(Σ1	+	Σ2).
i,j	i
Putting together the above calculations gives the desired result.	□
Note that we can upper bound αX by the expected distance squared between two points drawn form the same com-
ponent. This is because the distance to the k-th nearest neighbor from a fixed point will always be smaller than the
distance to another point drawn from the same component (assuming our hypothesis that at least k points are drawn
from each component). From Lemma B.1, it follows that αX ≤ 2 max(tr(Σ1), tr(Σ2)).
15
Under review as a conference paper at ICLR 2022
To lower bound βχ, note that since min(w, 1 - W) = Ω(1), the expected distance squared between two uniformly
random points is at least asymptotically the expected distance squared between two points from separate components.
Again using Lemma B.1, it follows that βχ = Ω(kμι - μ2k2 + tr(∑ι + ∑2)).	□
B.6	Proof of Lemma 3.12
Proof. Lemma 3.10 tells us that
αX	tr(Σ1 + Σ2 )	1
—-----------------—
βx ~ kμι - μ2k2 ~ c2
where we have used the fact that dλι(Σ) ≥ tr(Σ) for a covariance matrix Σ ∈ Rd×d.	□
B.7 Proof of Lemma A.3
We first need the following auxiliary results from Indyk & Naor (2007).
Lemma B.2. Let x ∈ Sd-1 and let P be a random one-dimensional Gaussian projection. Then for all t > 0,
Pr(|kP xk - 1| ≥ t) ≤ exp(-t2/8),	(1)
Pr(kPxk ≤ 1/t) ≤ 3	⑵
We now proceed with the proof of Lemma A.3.
Proof.	We	first	claim	that	the	diameters of S and S0 don’t increase by a large	factor after	a random	projection.	Fix
x, y ∈	S.	By Eq. equation	1,	the probability that kP(x -y)k increases by a factor oft is at most exp(-t2/8). Thus for
a suitable constant c, we have that the probability ∣∣P(X - y)k is larger by a c(dlog |S| + log(1∕e)) factor is at most
/(3|S|2). Union bounding across all pairs in S and using a similar argument for S0 gives us that with probability at
least 1 - 2e∕3, we have that diam(PS) . 'log |S| diam(S) and diam(PS0) . Plog |S0| diam(S0).
We now claim that the sets PS and PS0 don’t come ‘too’ close together. Indeed, take any point x ∈ S and y ∈ S0. We
have that ∣x - y∣ ≥ d(S, S0). Thus by Eq. equation 2, the probability that ∣P(X - y)∣ shrinks by a factor of Ω(1∕e)
is at most O(e).
Altogether, we know that with probability at least 1 - e, all three of the following events occur:
1.	diam(PS) . Plog |S| diam(S),
2.	diam(PS0) . Plog |S0| diam(S0),
3.	d(PS,PS0) ≥ e d(S, S0) - diam(PS) -diam(PS0).
Thus by our assumption that S and S0 are r-apart for the value ofr in the lemma statement, it follows that
diam(PS) . Plog |S| diam(S) - diam(PS) - diam(PS0) . ed(S, S0) . d(PS,PS0)
and a similar statement holds for S0, proving the lemma.	□
B.8 Proof of Lemma A.4
Proof. The proof follows from Lemma A.3 as every point in PS will be closer to any other point in PS than any other
point in PS0. A similar symmetric statement holds for PS0. Thus, any edges of the k-nearest neighbor graph starting
from any point in PS must have its other vertex in PS as well. This implies that there is an empty cut between PS
and PS0, as desired.	□
Lemma B.3 (Corollary 5 in Kushnir et al. (2019)). Consider two c-separated Gaussian distributions in Rd with means
μ1,μ2 and covariance matrices ∑ι and ∑2. Define T(c0, d) as in Lemma 3.13. Let Y := 2d(c0)2λmaχ∕∣μι 一 μ2k2,
where λmax denotes the largest eigenvalue of the matrix Σ1 + Σ2. Then
lim T(c0,d) ≤ ----1 ,___
d→∞	2Q( VZY)
16
Under review as a conference paper at ICLR 2022
C Omitted Experimental Results
We give additional experimental results in this section.
Figures for 1-Nearest Neighbor Experiments. We repeat the 10-NN experiments for 1-NN, i.e., we measure the
fraction of the times the exact nearest neighbor is in the candidate set for a query, averaged over all queries. See Figure
5 whic displays ClusterTree vs RP tree and Figure 6, which displays all baselines.
Note that for some datasets such as KDD Cup and RNA, PCA trees out performed ClusterTree while for others,
such as News, Spam, and Gaussian Mixtures, ClusterTree was the best algorithm. We also remark that it was not
computationally feasible to run 2-means Tree and PCA trees for SIFT and Spam. We are not plotting error bars for
2-means tree for clarity since it was much larger than all the other algorithms; overall, we observed 2-means trees to
be an inherently noisy algorithm.
Candidate Size
⅛ 0.96
≡ 0.95
ιH
SPam
0.99
0.98
A
S 0.97
n
0.94
RP Tree
Cluster Tree
200 400 600 800 1000 1200 1400
Candidate Size
0.93
l∙104 2∙104 3∙ IO4 4 - IO4 5 ∙ IO4 6- IO4 7 ∙104
Candidate Size
Figure 5: Candidate Size vs 1-NN Error for ClusterTree and RP tree for datasets in Table 1.
Varying Number of Projections. The results for ranging over different number of projections to use in Algorithm
1 (the parameter T ) are shown in Figure 7. See Section 4 for more details.
Note that only for this experiment, We use a randomly sub sample datasets Spam, NeWs and SIFT with 5 ∙ 104 points
for computational efficiency.
Preserving Cluster Structure of the Dataset. We present additional experiments on hoW the Weighted radius of
leaves of various tree-based algorithms varies as a function size in Figure 8. See Section 4 for more details on
experimental setting. Overall, We see that ClusterTree has a smaller radius as a function of cluster size for the
Gaussian Mixture, Spam, RNA, and KDD Cup datasets. Note that for Spam, 2-means tree Was too costly to run and for
Gaussian Mixture, the 2-means tree and ClusterTree have very identical curves for Weighted radius as a function
of candidate size.
Distance to k-th Nearest Neighbors. The figures for the distance to the k-th nearest neighbors experiment, as
described in Section 4 is given in Figure 9. To recap, the intuition here is that if a leaf node of a tree contains points
from multiple distinct clusters, then there Will be a substantial increase in this metric at some intermediate value of
k. For example, suppose that the leaf of a node contains points from tWo distinct Well-separated clusters and consider
a query that lands in this leaf but is closer to only one of the clusters. Then for a large enough value of k, the k-th
nearest neighbor for this query Would come from the far aWay cluster, leading to a significant increase in the distance
to the k-th nearest neighbor in comparison to the (k - 1)-th nearest neighbor. In contrast, if the leaf mostly contained
points from one cluster, the distance Would smoothly increase. To summarize, this is exactly the behaviour observed
17
Under review as a conference paper at ICLR 2022
0.7
0.6
∈ 0.5
0.2
⅛0.4
≡0.3
5000	10000	15000	20000
Candidate Size
-0-8'64 2
Ioooo
>U23UU< NN，I
—Cluster Tree
RP Tree
—LSH
PCA Tree
News
1.0
8-6-4-2.
Oooo
>UE□UU‹ nn，i
1.00
0.95
o 0.90
§ 0.85
<
g 0.80
rd
0.75
0.70
ι ∙io4 2∙io4 3∙io4 4∙io4 5 ∙io4 6∙io4 7∙io4	6 1000 2000 3000 4000 5000 eooo 7000
Candidate Size	Candidate Size
RNA	KDD Cup
CIusterTree
RPTree
2-Mea ns Tree
PCATree
—LSH
0.0.......................................
0 1000 2000 3000 4000 5000 6000 7000 8000
Candidate Size
0	1000 2000 3000 4000 5000 6000 7000
Candidate Size
1-NN Error for all baselines.
Spam
0.26
0.24
]0.22
80.20
U
<
N 0.18
z
」 0.16
0.14
Gaussian Mixtures
2000	2500	3000	3500
Candidate Size
SIFT
1.00
Figure 6: Candidate Size vs
0.44
&
”42
u
<
Z 0.40
i-I
0.38
1500	2000	2500	3000	3500
Candidate Size
RNA
>U23UU‹ nn，i
0.912
0.910-
0.908
0.906
0.904
0.902-
0.900
News
40	60	80	100	120	140
Candidate Size
KDD CUP
-87'65 4∙3
Oooooo
>U23UU< NN，I
0.99
&0.98
8 0.97
U
<
z 0.96
z
, 0.95
0.90
⅛0.85
ɔ
、0.80
z
Λ 0∙75
500 1000 1500 2000 2500 3000
Candidate Size
5000	10000	15000	20000	25000	0	200 400 600 SOO 1000 1200 1400 1600
Candidate Size	Candidate Size
Figure 7: Optimizing over multiple projections in Algorithm 1 can improve accuracy.
in Figure 9: in the Gaussian Mixture, KDD Cup, and Spam datasets, there is a noticeable ‘jump’ in the plots for RP
trees as it is ‘mixing’ multiple clusters in the leaf nodes while for ClusterTree, the relationship is much smoother.
When can we expect ClusterTree to outperform RP trees? Our tree construction method especially exploits
the cluster structure as it builds the tree over the dataset. If the dataset does not possess such a property, we expect
both tree algorithms, ClusterTree and RP trees, to have approximately the same behaviour.
To highlight this, we plotted the first two PCA projections of the centered and normalized versions of some of our
datasets in Figure 10. We can observe a strong cluster structure KDD Cup, RNA, and synthetic datasets, where as
the projection of the SIFT dataset is mostly uniform over a region, signifying that it lacks such a structure compared
18
Under review as a conference paper at ICLR 2022
Gaussian Mixture
Ooooooooo
098765432
ɪ
sn-pPEU6M
5.5-
5.0
g 4.5
ŋ 4.0
§3.5-
⅛3.0
φ
M 2.5
2.0
Spam
19
v>
:5 18
«5
S 17
OJ
216
Ol
'φ
M 15
14
2000 2500 3000 3500 4000 4500 5000
Candidate Size
News
20
13
250 500 750 1000 1250 1500 1750 2000
Candidate Size
2000 4000 6000 8000 10000
Candidate Size
SIFT
Ooooo
0 5 0 5 0
7 6 6 5 5
sneɑ pqM
250 500 750 1000 1250 15001750 2000
Candidate Size
120
Sn - PeX P9U∙≡,8M
RNA
Cluster Tree
80
Ooo
6 4 2
500	1000	1500	2000
Candidate Size
Sn-Peκpωs6δM
12000-
10000
8000
6000
4000
2000-
KDD Cup
1000	2000	3000	4000	5000
Candidate Size
14.0
13.8
E 13.5
o 13.2
g 13.0
∙∣ 128
δ 12.5
12.2
Gaussian Mixture
0.14
12.0
6	20	40	60	80	100
Figure 8: Expanded version of Figure 4 using all tree-based algorithms.
Spam
2 0 8 6 4
二」∙∙∙
Ooooo
Cclw- 0gue
Spam
12-I--------------------------------------
CIusterTree
10 RPTree	250
0	10 20 30 40 50 60 70 80
News
11
7 9 8 7 6
Uu03us-Cl
0	100 200	300	400	500	600
k
260
SIFT
380
Ooooo
6 4 2 0 8
3 3 3 3 2
Cclw- £uue
0	100 200 300 400 500 600
k
8 6 4 2
Uu03uα
_______,______,_______,_______,______,_______J
0	100	200	300	400	500	600
k
V 200
≡150
U
∣ιoo
Q
50
KDD CUP
0
0	100 200 300 400 500 600
k
Figure 9: ClusterTree has a smoother trade-off curve for distance to the k-th neighbor as k increases.
to the other datasets displayed. Likewise, we can see in Figure 5 that ClusterTree is superior to RP trees in the
1-NN experiments for KDD Cup, RNA, and synthetic datasets whereas it is comparable to RP trees for SIFT. Our
other experiments above follow a similar pattern. Therefore, we believe ClusterTree is preferable over RP trees
as many natural datasets have a strong underlying cluster structure.
Additional Parameter Selection Details. If there are multiple cuts that have conductance 0, i.e., multiple separated
pieces in the k-nearest neighbor graph constructed in Algorithm 1, we pick the cut that is the most balanced. This is
because any choice of the cuts would have been good with respect to preserving near neighbors so we should optimize
for keeping the tree balanced.
19
Under review as a conference paper at ICLR 2022
Figure 10: Plot of the PCA projection of the first two components of the centered and normalized versions of the
following datasets: (a) KDD CUP, (b) RNA, (c) Gaussian Mixtures, and (d) SIFT.
(c)	(d)
Note that there have been recent works on improving RP trees using additional techniques such as sparse random
projections (Sinha & Keivani, 2017), using auxiliary information when performing search over the tree (Keivani &
Sinha, 2021), and other methods (HyVonen et al., 2016). For simplicity, We did not use these techniques as they can
be used identically for ClusterTree as for RP trees.
Finally, We highlight that both RP tree and ClusterTree are randomized algorithms. Therefore, they haVe the
folloWing additional benefit: in order to boost accuracy, We can initialize multiple instances of the data structure to
create an ensemble of trees While keeping the oVerall number of candidates fixed. For example, instead of creating
one tree With leaf size P, We can create tWo trees With leaf sizes P/2 each. In general, if We make a constant number
of trees, this can be thought of as significantly boosting accuracy by increasing the amount of space used by only a
constant factor. Note that 2-means trees and PCA trees are deterministic so they do not haVe this additional benefit.
For simplicity hoWeVer, We only compare single instantiation of each algorithm.
D	Justifications for Assumption 3.3
We proVide justification for Assumption 3.3 as its conditions hold true for one-dimensional datasets With Very different
structural properties: both uniform and clustered inputs.
•	Uniform Points: Suppose the input to Algorithm 1 is a set of uniformly spaced points in one-dimensions.
Then it is clear that the cut With the loWest conductance Will split the dataset exactly in half due to symmetry.
•	Clustered Points: Suppose the input consists of tWo Well-separated clusters, With each cluster consisting of
at least c-fraction of the total input size. Then the k-nearest neighbor graph for this input Will be such that
the edges of each cluster Will be mostly to other points of the same cluster. Thus, the cut separating the tWo
clusters Will be extremely sparse and hence haVe loW conductance as Well. See Figure 1 for an example.
Average Split Ratio. We noW empirically Validate Assumption 3.3. To do so, We compute ClusterTree for all
of our datasets setting P = 5% of the size of the dataset in each case. The results across one run of Algorithm 2 are
shoWn in Table 2. We obserVe that on aVerage, each node of the tree splits the dataset into tWo approximately balanced
parts.
Dataset	AVg. Split Ratio	Dataset	AVg. Split Ratio
KDD Cup	0.49(0.26)	Spam	0.50(0.27)
NeWs	0.50(0.00)	SIFT	0.49(0.18)
RNA	0.45(0.29)	Gaussian Mixture	0.53(0.12)
Table 2: The aVerage split ratio across all nodes in the tree With standard deViation in the parenthesis using 5% of the
number of points as the parameter P (leaf size).
E Future Directions
In this paper, We presented a neW tree-based algorithm for NNS that utilizes randomness similarly to RP trees While
adapting to the underlying cluster structure of the input dataset. It’s partition rule consisted of performing a random
one-dimensional projection and finding a partition of the projected points that minimized the conductance of the k-
nearest neighbor graph. This strategy Was inspired by recent theoretical and practical Works on NNS. Finally, our
experiments demonstrated adVantage oVer other tree-based algorithms such as RP trees.
20
Under review as a conference paper at ICLR 2022
We would like to highlight some interesting future directions. Namely, can we use ClusterTree to speed up other
tree-based algorithms or other clustering algorithms that are computationally expensive? For example, it would be
interesting to see the performance of ClusterTree over other decision tree algorithms. Note that ClusterTree
is unsupervised which is advantageous in settings where acquiring labels is costly.
Similarly, it would be interesting to apply ClusterTree to speed up spectral clustering of point clouds. For example,
spectral clustering could be optimized by only clustering a representative point from each of the partitions found by
applying the ClusterTree algorithm on the dataset. A similar approach was considered in Yan et al. (2009) with RP
trees which lead to substantial speedups. Since ClusterTree is designed to preserve the inherent cluster structure,
we envision that our method would potentially be a better fit for this application.
21