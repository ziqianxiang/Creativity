Under review as a conference paper at ICLR 2022
Optimization Variance: Exploring Generaliza-
tion Properties of DNNs
Anonymous authors
Paper under double-blind review
Ab stract
Unlike the conventional wisdom in statistical learning theory, the test error of
a deep neural network (DNN) often demonstrates double descent: as the model
complexity increases, it first follows a classical U-shaped curve and then shows
a second descent. Through bias-variance decomposition, recent studies revealed
that the bell-shaped variance is the major cause of model-wise double descent
(when the DNN is widened gradually). This paper investigates epoch-wise double
descent, i.e., the test error of a DNN also shows double descent as the number of
training epoches increases. By extending the bias-variance analysis to epoch-wise
double descent of the zero-one loss, we surprisingly find that the variance itself,
without the bias, varies consistently with the test error. Inspired by this result,
we propose a novel metric, optimization variance (OV), to measure the diversity
of model updates caused by the stochastic gradients of random training batches
drawn in the same iteration. OV can be estimated using samples from the training
set only but correlates well with the (unknown) test error, and hence early stopping
may be achieved without using a validation set.
1	Introduction
Deep Neural Networks (DNNs) usually have large model capacity, but also general-
ize well. This violates the conventional VC dimension (Vapnik, 1999) or Rademach-
er complexity theory (Shalev-Shwartz & Ben-David, 2014), inspiring new designs of net-
work architectures (Krizhevsky et al., 2012; Simonyan & Zisserman, 2015; He et al., 2016;
Zagoruyko & Komodakis, 2016) and reconsideration of their optimization and generaliza-
tion (Zhang et al., 2017; Arpit et al., 2017; Wang et al., 2018; Kalimeris et al., 2019; Rahaman et al.,
2019; Zhu et al., 2019).
Model-wise double descent, i.e., as a DNN’s model complexity increases, its test error first shows a
classical U-shaped curve and then enters a second descent, has been observed on many machine
learning models (Advani & Saxe, 2017; Belkin et al., 2019a; Geiger et al., 2019; Maddox et al.,
2020; Nakkiran et al., 2020). Multiple studies provided theoretical evidence of this phenomenon
in some tractable settings (Mitra, 2019; Hastie et al., 2019; Belkin et al., 2019b; Yang et al., 2020;
Bartlett et al., 2020; Muthukumar et al., 2020). Specifically, Neal et al. (2018) and Yang et al.
(2020) performed bias-variance decomposition for mean squared error (MSE) and the cross-entropy
(CE) loss, and empirically revealed that the bell-shaped curve of the variance is the major cause of
model-wise double descent. Maddox et al. (2020) proposed to measure the effective dimensionality
of the parameter space, which can be further used to explain model-wise double descent.
Recently, a new double descent phenomenon, epoch-wise double descent, was observed, when in-
creasing the number of training epochs instead of the model complexity (Nakkiran et al., 2020).
Compared with model-wise double descent, epoch-wise double descent is relatively less explored.
Heckel & Yilmaz (2020) showed that epoch-wise double descent occurs in the situation where dif-
ferent parts of DNNs are learned at different epochs. Zhang et al. (2021) discovered that the energy
ratio of the high-frequency components of a DNN’s prediction landscape, which can reflect the
model capacity, switches from increase to decrease at a certain training epoch, leading to the second
descent of the test error.
This paper utilizes bias-variance decomposition of the zero-one (ZO) loss (CE loss is still used in
training) to further investigate epoch-wise double descent. By monitoring the behaviors of the bias
1
Under review as a conference paper at ICLR 2022
and the variance, we find that the variance plays an important role in epoch-wise double descent. It
highly correlates with the variation of the test error, even not combined with the bias term.
Though the variance correlates well with the test error, estimating its value requires training models
on multiple different training sets drawn from the same data distribution, whereas in practice usually
only one training set is available1. Inspired by the fact that the source of variance comes from the
random-sampled training sets, we propose a novel metric, optimization variance (OV), to measure
the diversity of model updates caused by the stochastic gradients of random training batches drawn
in the same iteration. This metric can be estimated from a single model using samples drawn from
the training set only. More importantly, it correlates well with the test error, and thus can be used to
determine the early stopping point in DNN training, without using validation sets.
Some complexity measures have been proposed to illustrate the generalization ability of DNNs, such
as sharpness (Keskar et al., 2017) and norm-based measures (Neyshabur et al., 2015). However,
their values rely heavily on the model parameters, making comparisons across different models very
difficult. Dinh et al. (2017) shows that by re-parameterizing a DNN, one can alter the sharpness of
its searched local minima without affecting the function it represents; Neyshabur et al. (2018) shows
that these measures cannot explain the generalization behaviors when the size of a DNN increases.
Our proposed metric, which only requires the logit outputs of a DNN, is less dependent on model
parameters, and hence can explain many generalization behaviors, e.g., the test error decreases as the
network size increases. Chatterji et al. (2020) proposed a metric called Model Criticality that can
explain the superior generalization performance of some architectures over others, yet it remains
unexplored whether this metric can be used to indicate generalization in the entire training process,
especially for some relatively complex generalization behaviors, such as epoch-wise double descent.
To summarize, our contributions are:
•	We perform bias-variance decomposition on the test error to explore epoch-wise double
descent. We show that for the zero-one loss, the variance itself highly correlates with the
variation of the test classification error.
•	We propose a novel metric, OV, which is calculated from the training set only and correlates
well with the test classification error.
•	Based on the OV, we propose an approach to search for the early stopping point without
using a validation set, when the zero-one loss is used in test. Experiments verified its
effectiveness.
The remainder of this paper is organized as follows: Section 2 introduces the details of tracing bias
and variance over training epochs. Section 3 proposes the OV and demonstrates its ability to indicate
the test behaviors. Section 4 draws conclusions and points out some future research directions.
2	Bias and Variance in Epoch-Wise Double Descent
This section presents the details of tracing the bias and the variance during training. We show that
the variance dominates the epoch-wise double descent of the test error.
2.1	A Unified Bias-Variance Decomposition
Bias-variance decomposition is widely used to analyze the generalization properties of machine
learning algorithms (Friedman et al., 2001). It was originally proposed for the MSE loss and later
extended to other loss functions, e.g., CE and ZO losses (Kong & Dietterich, 1995; Kohavi et al.,
1996; Heskes, 1998). Our study utilizes a unified bias-variance decomposition that was proposed by
Domingos (Domingos, 2000) and applicable to arbitrary loss functions.
Let (x, t) be a sample drawn from the data distribution D, where x ∈ Rd denotes the d-
dimensional input, and t ∈ Rc the one-hot encoding of the label in c classes. The training set 1
1Assume the training set has n samples. We can partition it into multiple smaller training sets, each with m
samples (m < n), and then train multiple models. However, the variance estimated from this case would be
different from the one estimated from training sets with n samples. We can also bootstrap the original training
set into multiple ones, each with n samples. However, the data distribution of each bootstrap replica is different
from the original training set, and hence the estimated variance would also be different.
2
Under review as a conference paper at ICLR 2022
T = {(Xi, ti)}n=ι 〜Dn is utilized to train the model f : Rd → Rc. Let y = f(x; T) ∈ RC be
the probability output of the model f trained on T, and L(t, y) the loss function. The expected loss
ET [L(t, y)] should be small to ensure that the model both accurately captures the regularities in its
training data, and also generalizes well to unseen data.
According to (Domingos, 2000), a unified bias-variance decomposition2 of ET [L(t, y)] is:
ET[L(t, y)] = L(t, y) + β ET[L(y, y)],	(1)
|~{z}	|----{z----}
Bias Variance
where β takes different values for different loss functions, and y is the expected output:
y = arg min	ET[L(y^, y)]∙	(2)
y*∈RCIPk=ι yk=1 ,yk≥0
y minimizes the variance term in (1), which can be regarded as the “center” or “ensemble” of y w.r.t.
different T.
Table 1 shows specific forms of L, y, and β for different loss functions (the detailed derivations
can be found in Appendix A). This paper focuses on the bias-variance decomposition of the ZO
loss, because epoch-wise double descent of the test error is more obvious when the ZO loss is used
(see Appendix C). To capture the overall bias and variance, we analyzed Ex,tET[L(t, y)], i.e., the
expectation of ET [L(t, y)] over the distribution D.
Table 1: Bias-variance decomposition for different loss functions. The CE loss herein is the com-
plete form of the commonly used one, originated from the Kullback-Leibler divergence. Z =
Pk =1 exp{Et[log yk]} is a normalization constant independent of k. H(∙) is the hard-max which
sets the maximal element to 1 and others to 0. Icon{∙} is an indicator function which equals 1 if its
argument is true, and 0 otherwise. log and exp are element-wise operators.
Loss	L(t, y)	y	β	
MSE	kt - yk22	E t y		1
CE	p k=1 tk log ykk	1 exp { E t [log y ]}		1
ZO	1con{H(t) 6= H(y)}	H(ET [H(y)])	1if y = -PT (H(y)	t, otherwise =t i y = H(y))
2.2	Trace the Bias and Variance Terms over Training Epochs
To trace the bias term Eχ,t [L(t, y)] and the variance term Eχ,tET[L(y, y)] w.r.t. the training epoch,
we need to sample several training sets and train models on them respectively, so that the bias and
variance terms can be estimated from them.
Concretely, let T* denote the test set, f (x; Tj, q) the model f trained on T 〜Dn (j = 1, 2,..., K)
for q epochs. Then, the estimated bias and variance terms at the q-th epoch, denoted as B(q) and
V (q), respectively, can be written as:
B (q )= E( x,t) ∈T* [ L (t,f( x ； q))],	⑶
Γ 1 K	]
V (q )= E( x,t) ∈T* K EL (f( X ； q) ,f (X ； Tj ,q)) ,	(4)
j=1
where
f(x; q) = H (Xh(f(x; Tj,q))j ,	(5)
2In real-world situations, the expected loss consists of three terms: bias, variance, and noise. Similar to
(Yang et al., 2020), we view t as the groundtruth and ignore the noise term.
3
Under review as a conference paper at ICLR 2022
is the voting result of {f (x; Tj , q)}jK=1.
We should emphasize that, in real-world situations, D cannot be obtained, hence Tj in our exper-
iments was randomly sampled from the training set (we sampled 50% training data for each Tj ).
As a result, despite of showing the cause of epoch-wise double descent, the behaviors of bias and
variance may be different when the whole training set is used.
We considered ResNet (He et al., 2016) and VGG (Simonyan & Zisserman, 2015) models3 trained
on SVHN (Netzer et al., 2011), CIFAR10 (Krizhevsky, 2009), and CIFAR100 (Krizhevsky, 2009).
SGD and Adam optimizers with different learning rates were used. The batchsize was set to 128,
and all models were trained for 250 epochs with data augmentation. Prior to sampling {Tj }jK=1
(K = 5) from the training set, 20% labels of the training data were randomly shuffled to introduce
epoch-wise double descent.
Figure 1 shows the expected ZO loss and its bias and variance. The bias descends rapidly at first
and then generally converges to a low value, whereas the variance behaves almost exactly the same
as the test error, mimicking even small fluctuations of the test error. To stabilize that, we performed
additional experiments with different optimizers, learning rates, and levels of label noise (see Appen-
dices E and H). All experimental results demonstrated that it is mainly the variance that contributes
to epoch-wise double descent.
VGGll	VGG13	VGG16
0.30 -∣------------------- 0.40	^-
.IeAZSe-ESSO-
.IeAZSe-ESSO-
(b) CIFAR10
Figure 1: The expected test ZO loss and its bias and variance. The models were trained with 20%
label noise. Adam optimizer with learning rate 0.0001 was used.
2.3 Discussion
Contradicting to the traditional view that the variance keeps increasing because of overfitting, our
experimental results show a more complex behavior: the variance starts high and then decreases
rapidly, followed by a bell curve. The difference at the beginning (when the number of epochs is
small) is mainly due to the choice of loss functions (see experimental results of bias-variance decom-
position for MSE and CE losses in Appendix G). CE and MSE losses, analyzed in the traditional
learning theory, can reflect the degree of difference of probabilities, whereas the ZO loss only the
labels. At the early stage of training, the output probabilities are close to random guesses, and hence
a small difference in probabilities may lead to completely different labels, resulting in the distinct
variance for different loss functions. However, the reason why the variance begins to diminish at the
late phase of training is still unclear. We will explore this problem in our future research.
3Adapted from https://github.com/kuangliu/pytorch-cifar
4
Under review as a conference paper at ICLR 2022
3 Optimization Variance (OV)
This section proposes a new metric, OV, to measure the diversity of model updates introduced by
random training batches during optimization. This metric can indicate test behaviors without any
validation set.
3.1	Notation and Definition
Section 2 verified the synchronization between the test error and the variance, but its application is
limited because estimating the variance requires: 1) a test set, and, 2) models trained on different
training sets drawn from the same data distribution. It’d be desirable to capture the test behavior of
a DNN using a single training set only, without a test set.
According to the definition in (1), the variance measures the model diversity caused by different
training samples drawn from the same distribution, i.e., the outputs of DNN change according to the
sampled training set. As the gradients are usually the only information transferred from training sets
to models during the optimization of DNN, we need to measure the variance of a DNN introduced
by the gradients calculated from different training batches. More specifically, we’d like to develop
a metric to reflect the function robustness of DNNs to sampling noise. If the function captured by
a DNN drastically varies w.r.t. different training batches, then very likely it has poor generalization
due to a large variance introduced by the optimization procedure. A similar metric is the sharpness
of local minima proposed by Keskar (Keskar et al., 2017), which measures the robustness of local
minima as an indicator of the generalization error. However, this metric is only meaningful for local
minima and hence cannot be applied in the entire optimization process.
Mathematically, for a sample (x, t) ~ D, let f (x; θ) be the logit output of a DNN with parameter
θ. Let TB 〜Dm be a training batch with m samples, g : TB → Rlθl the optimizer outputting the
update of θ based on TB. Then, we can get the function distribution Fx(TB) over a training batch
TB, i.e., f (x; θ + g(TB))〜 Fx(TB). The variance of Fx(TB) reflects the model diversity caused
by different training batches. The formal definition of OV is given below.
Definition 1 (Optimization Variance (OV)). Given an input x and model parameters θq at the q-th
training epoch, the OV on x at the q-th epoch is defined as
OV (x ) , E TB [H" X ； θq + g (TB )) 一 E TB f( X ； θq + g (TB )) ∣∣ 2]
ETb [Hf (x； θq + g(Tb))H2]
(6)
Note that OVq (x) measures the relative variance, because the denominator in (6) eliminates the
influence of the logit’s norm. In this way, OVq (x) at different training phases can be compared. The
motivation here comes from the definition of coefficient of variation4 (CV) in probability theory and
statistics, which is also known as the relative standard deviation. CV is defined as the ratio between
the standard deviation and the mean, and is independent of the unit in which the measurement is
taken. Therefore, CV enables comparing the relative diversity between two different measurements.
In terms of OV, the variance of logits, i.e., the numerator of OV, is not comparable across epochs
due to the influence of their norm. In fact, even if the variance of logits maintains the same during
the whole optimization process, its influence on the decision boundary is limited when the logits are
large. Consequently, by treating the norm of logits as the measurement unit, following CV we set
OV to Pi σ2 / Pi μ2, where μ% and σi represent the mean and standard deviation of the i-th logit,
respectively. If we remove the denominator, then the value of OV will no longer have the indication
ability for generalization error, especially at the early stage of training.
Intuitively, the OV represents the inconsistency of gradients’ influence on the model. If OVq(x) is
very large, the models trained with different TB may have distinct outputs for the same input, leading
to high model diversity and hence large variance. Note that here we emphasize the inconsistency
of model updates rather than the gradients themselves. The latter can be measured by the gradient
variance. The gradient variance and the OV are different, because sometimes diverse gradients may
lead to similar changes of the function represented by DNN, and hence small OV. More on the
relationship between the two variances can be found in Appendix B.
4https://en.wikipedia.org/wiki/Coefficient_of_variation
5
Under review as a conference paper at ICLR 2022
3.2	Experimental Results
We calculated the expectation of the OV over x, i.e., Ex[OVq(x)], which was estimated from 1,000
random training samples. The test set was not involved at all.
Figure 2 shows how the test accuracy (solid curves) and Ex [OVq (x)] (dashed curves) change with
the number of training epochs. Though sometimes the OV may not exhibit clear epoch-wise double
descent, e.g., VGG16 in Figure 2(c), the symmetry between the solid and dashed curves generally
exist, suggesting that the OV, which is calculated from the training set only, is capable of predicting
the variation of the test accuracy. Similar results can also be observed using different optimizers and
learning rates (see Appendix I). Besides, we also show in Appendix J that a small number of training
batches are usually enough to estimate OV, which significantly improves the calculation efficiency.
0.5
0.0025
0.0020
0.0015
0.0010
0.0005
.P-OOOO
0	50 100 150 200 250
epoch
VGGll
o
0.9
U 0.8
7
0.6
roo
ResNetie
ι.o
0.0025
1.0
0.9
U 0.8
u
^0.7
0.6
0.5
ResNetl8
0.0025
5<*<w<w⅝l-. i ..
M出心J柚M
I 产
∣⅛4λ√U∣出 WOmX
0	50 100 150 200 250
0.0020
0.0015
0.0010
0.0005
0.0000
0.9
0.8
U 0.7
e
0.6
0.4
0.0020
0.5
0.0005
epoch
.p.oooo
0	50 100 150 200 250
epoch
0.0015
O
0.0010
(c) CIFAR100
(a) SVHN	(b) CIFAR10
ι
O

Figure 2:	Test accuracy and OV. The models were trained with Adam optimizer (learning rate
0.0001). The number in each legend indicates its percentage of label noise.
Note that epoch-wise double descent is not a necessary condition for applying OV. Figure 2 compares
the values of OV and generalization errors of DNNs when there is 0% label noise. The curves of
generalization errors have no epoch-wise double descent, yet the proposed OV still works pretty
well.
Another intriguing finding is that even unstable variations of the test accuracy can be reflected by the
OV. This correspondence is clearer on simpler datasets, e.g., MNIST (LeCun et al., 1998) and Fash-
ionMNIST (Xiao et al., 2017). Figure 3 shows the test accuracy and OV for LeNet-5 (LeCun et al.,
1998) trained on MNIST and FashionMNIST without label noise. Spikes of the OV and the test
accuracy happen simultaneously.
Our experimental results demonstrate that the generalization ability of a DNN can be indicated by
the OV during training, without using a validation set. This phenomenon can be used to determine
the early stopping point.
3.3 Early Stopping without a Validation Set
The common process to train a DNN involves three steps: 1) partition the dataset into a training set
and a validation set; 2) use the training set to optimize the DNN parameters, and the validation set
to determine when to stop training, i.e., early stopping, and record the early stopping point; 3) train
the DNN on the entire dataset (combination of training and validation sets) for the same number of
epochs. However, there is no guarantee that the early stopping point on the training set is the same
as the one on the entire dataset. So, an interesting questions is: is it possible to directly perform
early stopping on the entire dataset, without a validation set?
6
Under review as a conference paper at ICLR 2022
20	0.94 η-----------------------r 20
0.99-
0.97 +
O
0.98-
----acc
OV
(寸，岂>0
5 O
115
2 O
9 9
■ ■
O O
ɔɔe
8
8
■
O
5 O
IOO 200
epoch
0.86 π----ɪ--------L
O	IOO	200
epoch

(∙岂 >0
O
(a) MNIST
(b) FashionMNIST
Figure 3:	Test accuracy and OV. The model was LeNet-5 trained on MNIST and FashionMNIST
with Adam optimizer (learning rate 0.0001).
The OV can be used for this purpose. For more robust performance, instead of using the OV directly,
we may need to smooth it to alleviate random fluctuations. As an example, we smoothed the OV by
a moving average filter of 10 epochs, and then performed early stopping on the smoothed OV with
a patience of 10 epochs. As a reference, early stopping with the same patience was also performed
directly on the test accuracy to get the groundtruth. However, it should be noted that the latter is
unknown in real-world applications. It is provided for verification purpose only.
We trained different DNN models on several datasets (SVHN: VGG11 and ResNet18; CIFAR10:
VGG13 and ResNet18; CIFAR100: VGG16 and ResNet34) with different levels of label noise (10%
and 20%) and optimizers (Adam with learning rate 0.001 and 0.0001, SGD with momentum 0.9
and learning rate 0.01 and 0.001). Then, we compared the groundtruth early stopping point and the
test accuracy with those found by performing early stopping on the OV5. The results are shown in
Figure 4. The true early stopping points and those found from the OV curve were generally close,
though there were some exceptions, e.g., the point near (40, 100) in Figure 4(a). However, the test
errors, which are what a model designer really cares about, were always close.
Ooooo
9 7 5 3 1
PUnoL
× SVHN, VGG
^zSVHNl Res
• CFlO, VGG
• CFlO, Res
▲ CFlOO, VGG
▲ CFlOO, Res
.5
.4
O.
3 e
0.B
2 T
O.
1
6
.O
.5,4.32.1Qo
0.6 6O.O.6
PUnOLL
(a)	Early stopping point
(b)	Test error
O
O
3
O
7
O
9
Figure 4:	Early stopping based on test error (True) and the corresponding OV (Found). The shapes
represent different datasets, whereas the colors indicate different categories of DNNs (“CF” and
“Res” denotes “CIFAR” and “ResNet”, respectively).
5Training VGG11 on SVHN with Adam optimizer and learning rate 0.001 was unstable (see Appendix D),
so we did not include its results in Figure 4.
7
Under review as a conference paper at ICLR 2022
3.4	Network Size
In addition to indicating the early stopping point, the OV can also explain some other generalization
behaviors, such as the influence of the network size. To verify that, we trained ResNet18 with
different network sizes on CIFAR10 for 100 epochs with no label noise, using Adam optimizer with
learning rate 0.0001. For each convolutional layer, we set the number of filters k/4 (k = 1, 2, ..., 8)
times the number of filters in the original model. We then examined the OV of ResNet18 with
different network sizes to validate its correlation with the test accuracy. Note that we used SGD
optimizer with learning rate 0.001 and no momentum to calculate the OV, so that the cumulative
influence during training can be removed to make the comparison more fair.
The results are shown in Figure 5. As k increases, the
OV gradually decreases, i.e., the diversity of model up-
dates introduced by different training batches decreases
when widening ResNet18, suggesting that increasing the
network size can improve the model’s resilience to sam-
pling noise, which leads to better generalization perfor-
mance. The Pearson correlation coefficient between the
OV and the test accuracy reached -0.94 (p = 0.0006).
Lastly, we need to point out that we did not observe a
strong cross-model correlation between the OV and the
test accuracy when comparing the generalization ability
of significantly different model architectures, e.g., VGG
Figure 5: Test accuracy and OV w.r.t.
the network size.
and ResNet. Our future research will look for a more universal cross-model metric to illustrate the
generalization performance.
3.5	Small Training Set
For large datasets, a validation set can be partitioned from
the training set without hurting the generalization perfor-
mance. Therefore, OV is more useful on small datasets.
We performed experiments with a small number (2000,
4000, 6000) of training samples in CIFAR10 to verify the
effectiveness of OV in this situation. Considering the lim-
ited number of training samples, we trained a small Con-
volution Neural Network (CNN) using Adam optimizer
with learning rate 0.0001, whose detailed information can
be found in Appendix F.
The experimental results are shown in Figure 6. When
the training set size is small, OV still correlates well with
the generalization performance as a function of the train-
ing epochs, demonstrating the validity of our results on
small datasets. As expected, more training samples lead
to better generalization performance, which can also be
reflected by comparing the values of OV.
0.6
0.5-
-0.0035
0.0030
-0.0025
I — acc, 2000 OV, 2000
Il — acc, 4000 OV, 4000
I — acc, 6000 OV, 6000
0.0040
-0.0020
0.3-
0.2-
0.0015
-0.0010
50	100 150 200 250
epoch
0.0005
Figure 6: Test accuracy and OV of mod-
els trained on different number of train-
ing samples.
3 O”

0
O
4 Conclusions
This paper has shown that the variance dominates the epoch-wise double descent, and highly cor-
relates with the test error. Inspired by this finding, we proposed a novel metric called optimization
variance, which is calculated from the training set only but powerful enough to predict how the test
error changes during training. Based on this metric, we further proposed an approach to perform
early stopping without any validation set. Remarkably, we demonstrated that the training set itself
may be enough to predict the generalization ability of a DNN, without a dedicated validation set.
8
Under review as a conference paper at ICLR 2022
Our future work will: 1) apply the OV to other tasks, such as regression problems, unsupervised
learning, and so on; 2) figure out the cause of the second descent of the OV; and, 3) design regular-
ization approaches to penalize the OV for better generalization performance.
References
Madhu S Advani and Andrew M Saxe. High-dimensional dynamics of generalization error in neural
networks. CoRR, abs/1710.03667, 2017.
Devansh Arpit, StanisIaW Jastrzebski, Nicolas Ballas, David Krueger, Emmanuel Bengio, Maxin-
derS Kanwal, Tegan Maharaj, Asja Fischer, Aaron Courville, Yoshua Bengio, and Simon Lacoste-
Julien. A closer look at memorization in deep netWorks. In Proc. 34th Int’l Conf. on Machine
Learning, volume 70, pp. 233-242, Sydney, Australia, August 2017.
Peter L Bartlett, Philip M Long, Gabor Lugosi, and Alexander Tsigler. Benign overfitting in linear
regression. Proceedings of the National Academy of Sciences, 2020. In press.
Mikhail Belkin, Daniel Hsu, Siyuan Ma, and Soumik Mandal. Reconciling modern machine learn-
ing practice and the classical bias-variance trade-off. Proceedings of the National Academy of
Sciences, 116(32):15849-15854, 2019a.
Mikhail Belkin, Daniel Hsu, and Ji Xu. TWo models of double descent for Weak features. CoRR,
abs/1903.07571, 2019b.
Stephen Boyd and Lieven Vandenberghe. Convex Optimization. Cambridge University Press, 2004.
Niladri Chatterji, Behnam Neyshabur, and Hanie Sedghi. The intriguing role of module criticality
in the generalization of deep netWorks. In Proc. Int’l Conf. on Learning Representations, Addis
Ababa, Ethiopia, April 2020.
Laurent Dinh, Razvan Pascanu, Samy Bengio, and Yoshua Bengio. Sharp minima can generalize
for deep nets. In Proc. 34th Int’l Conf. on Machine Learning, volume 70, pp. 1019-1028, Sydney,
Australia, August 2017.
Pedro Domingos. A unified bias-variance decomposition for zero-one and squared loss. In Proc. of
the 17th National Conf. on Artificial Intelligence, pp. 564-569, Austin, TX, July 2000.
Jerome Friedman, Trevor Hastie, and Robert Tibshirani. The Elements of Statistical Learning, vol-
ume 1. Springer series in statistics NeW York, second edition, 2001.
Mario Geiger, Arthur Jacot, Stefano Spigler, Franck Gabriel, Levent Sagun, StePhane d,Ascoli,
Giulio Biroli, Clement Hongler, and Matthieu Wyart. Scaling description of generalization with
number of parameters in deep learning. CoRR, abs/1901.01608, 2019.
Trevor Hastie, Andrea Montanari, Saharon Rosset, and Ryan J Tibshirani. Surprises in high-
dimensional ridgeless least squares interpolation. CoRR, abs/1903.08560, 2019.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recog-
nition. In Proc. IEEE Conf. on Computer Vision and Pattern Recognition, pp. 770-778, Las Vegas,
NV, June 2016.
Reinhard Heckel and Fatih Furkan Yilmaz. Early stopping in deep networks: Double descent and
how to eliminate it. CoRR, abs/2007.10099, 2020.
Tom Heskes. Bias/variance decompositions for likelihood-based estimators. Neural Computation,
10(6):1425-1433, 1998.
Dimitris Kalimeris, Gal Kaplun, Preetum Nakkiran, Benjamin Edelman, Tristan Yang, Boaz Barak,
and Haofeng Zhang. SGD on neural networks learns functions of increasing complexity. In
Proc. Advances in Neural Information Processing Systems, pp. 3491-3501, Vancouver, Canada,
December 2019.
9
Under review as a conference paper at ICLR 2022
Nitish Shirish Keskar, Dheevatsa Mudigere, Jorge Nocedal, Mikhail Smelyanskiy, and Ping Tak Pe-
ter Tang. On large-batch training for deep learning: Generalization gap and sharp minima. In
Proc. Int’l Conf. on Learning Representations, Toulon, France, April 2017.
Ron Kohavi, David H Wolpert, et al. Bias plus variance decomposition for zero-one loss functions.
In Proc. 13th Int,l Conf. on Machine Learning, volume 96,pp. 275-283, Bari, Italy, July 1996.
Eun Bae Kong and Thomas G Dietterich. Error-correcting output coding corrects bias and variance.
In Proc. 12th Intl Conf. on Machine Learning, pp. 313-321, Tahoe City, CA, July 1995.
Alex Krizhevsky. Learning multiple layers of features from tiny images. 2009. URL
https://www.cs.toronto.edu/~kriz∕learning-features-2 0 0 9-TR.pdf.
Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. ImageNet classification with deep con-
volutional neural networks. In Proc. Advances in Neural Information Processing Systems, pp.
1097-1105, Lake Tahoe, NE, December 2012.
Yann LeCun, Leon Bottou, Yoshua Bengio, and Patrick Haffner. Gradient-based learning applied to
document recognition. Proceedings of the IEEE, 86(11):2278-2324, 1998.
Wesley J Maddox, Gregory Benton, and Andrew Gordon Wilson. Rethinking parameter counting in
deep models: Effective dimensionality revisited. CoRR, abs/2003.02139, 2020.
Partha P Mitra. Understanding overfitting peaks in generalization error: Analytical risk curves for
l2 and l1 penalized interpolation. CoRR, abs/1906.03667, 2019.
Vidya Muthukumar, Kailas Vodrahalli, Vignesh Subramanian, and Anant Sahai. Harmless interpo-
lation of noisy data in regression. IEEE Journal on Selected Areas in Information Theory, 1(1):
67-83, 2020.
Preetum Nakkiran, Gal Kaplun, Yamini Bansal, Tristan Yang, Boaz Barak, and Ilya Sutskever. Deep
double descent: Where bigger models and more data hurt. In Proc. Int’l Conf. on Learning
Representations, Addis Ababa, Ethiopia, April 2020.
Brady Neal, Sarthak Mittal, Aristide Baratin, Vinayak Tantia, Matthew Scicluna, Simon Lacoste-
Julien, and Ioannis Mitliagkas. A modern take on the bias-variance tradeoff in neural networks.
CoRR, abs/1810.08591, 2018.
Yuval Netzer, Tao Wang, Adam Coates, Alessandro Bissacco, Bo Wu, and Andrew Y. Ng. Reading
digits in natural images with unsupervised feature learning. In Proc. NIPS Workshop on Deep
Learning and Unsupervised Feature Learning 2011, Granada, Spain, December 2011.
Behnam Neyshabur, Ryota Tomioka, and Nathan Srebro. Norm-based capacity control in neural
networks. In Proc. of the 28th Conf. on Learning Theory, pp. 1376-1401, Paris, France, July
2015.
Behnam Neyshabur, Srinadh Bhojanapalli, David McAllester, and Nati Srebro. Exploring gener-
alization in deep learning. In Proc. Advances in Neural Information Processing Systems, pp.
5947-5956, Long Beach, CA, January 2018.
Nasim Rahaman, Aristide Baratin, Devansh Arpit, Felix Draxler, Min Lin, Fred Hamprecht, Yoshua
Bengio, and Aaron Courville. On the spectral bias of neural networks. In Proc. 36th Int’l Conf.
on Machine Learning, pp. 5301-5310, Long Beach, CA, May 2019.
Shai Shalev-Shwartz and Shai Ben-David. Understanding Machine Learning: From Theory to
Algorithms. Cambridge University Press, 2014.
Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image
recognition. In Proc. Int’l Conf. on Learning Representations, San Diego, CA, May 2015.
Vladimir N Vapnik. An overview of statistical learning theory. IEEE Trans. on Neural Networks, 10
(5):988-999, 1999.
Huan Wang, Nitish Shirish Keskar, Caiming Xiong, and Richard Socher. Identifying generalization
properties in neural networks. CoRR, abs/1809.07402, 2018.
10
Under review as a conference paper at ICLR 2022
Han Xiao, Kashif Rasul, and Roland Vollgraf. Fashion-MNIST: a novel image dataset for bench-
marking machine learning algorithms. CoRR, abs/1708.07747, 2017.
Zitong Yang, Yaodong Yu, Chong You, Jacob Steinhardt, and Yi Ma. Rethinking bias-variance
trade-off for generalization of neural networks. CoRR, abs/2002.11328, 2020.
Sergey Zagoruyko and Nikos Komodakis. Wide residual networks. CoRR, abs/1605.07146, 2016.
Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals. Understanding
deep learning requires rethinking generalization. In Proc. Int’l Conf. on Learning Representations,
Toulon, France, April 2017.
Xiao Zhang, Haoyi Xiong, and Dongrui Wu. Rethink the connections among generalization, mem-
orization, and the spectral bias of DNNs. In Proc. Int’l Joint. Conf. on Artificial Intelligence,
Montreal, Canada, August 2021.
Zeyuan Zhu, Yuanzhi Li, and Zhao Song. A convergence theory for deep learning via over-
parameterization. In Proc. 36th Int,l Conf. on Machine Learning, pp. 242-252, Long Beach,
CA, May 2019.
11
Under review as a conference paper at ICLR 2022
A Bias-Variance Decomposition for Different Loss Functions
This section presents detailed deduction of bias-variance decomposition for different loss functions.
A.1 The Mean S quared Error (MSE) Loss
For the MSE loss, we have L(t, y) = ∣∣t - yk2, and need to calculate y based on (2) of our paper.
We first ignore the constraints and solve the folloWing problem:
e = argminET[ky* - yk2],
y*
(7)
whose solution is ye = ETy. It can be easily verified that ye satisfies the constraints in (2) of our
paper, and hence y = y = ET y.
Then, we can decompose the MSE loss as:
E T [ kt - yk 2] = E T [ kt - y + y - yk 2]
=Et[kt- yk2 + ky - yk2 + 2(t - y)t(y - y)]
=kt- yk2 + ET[ky - yk2] + 0,
(8)
where the first term denotes the bias, and the second denotes the variance. We can also get β = 1.
A.2 Cross-Entropy (CE) Loss
For L(t, y)
Pk=ι tk log ykk, y can be obtained by applying the Lagrange multiplier
method (Boyd & Vandenberghe, 2004) to (2) of our paper:
,*
„.不
1 (y*,λ) = ET E碇 log y + λ -
k	yk
k=1
ι - X j∙
k=1
(9)
To minimize l(y^, λ), We need to compute its partial derivatives to y^ and λ:
∂l
西
∂l
∂λ
*
Et log y + 1 - λ,
Vk
c
1 -	Vk* .
k=1
k = 1, 2, ..., c
By setting these derivatives to 0, we get:
Vk = ɪ exp{ET[logVk]},	k = 1, 2,-；c
Z
where Z = Pck=1 exp{ET [log Vk]} is a normalization constant independent of k.
Because
(10)
ET X ak log y = - log Z, Ya
yk
k=1
c
αk = 1,
k=1
(11)
c
c
k
We have:
c
ET ^k lo哈
c
k=1
ET	tk
k=1
tk
Vk
il	Vk
+ log —
yk
X tk log tk- + ET
k=1	Vyk
c
X tk log 竺
k=1 Vk
X tk log k- - log Z
3 y
X tk log t + ET
公 yk
1>log I
(12)
k=1
c
from which we obtain β = 1.
12
Under review as a conference paper at ICLR 2022
A.3 The Zero-One (ZO) Loss
For the ZO loss, i.e., L(t, y) = Icon{H(t) = H(y)}, y is the voting result, i.e., H(ET[H(y)]), so
that the variance can be minimized. However, the value of β depends on the relationship between y
and t.
When y = t, we have:
E T [1con { H( t ) = H( y ) H =0 + E T [1con { HW)= H(y ) H
=Icon { H( t )= H( y)} + E T [Icon { H( y) = H( y)} ],	(13)
clearly, β = 1.
When y = t, we have:
ET [1con{H(t) 6= H(y)}] = PT (H(y) 6=t) = 1-PT(H(y)=t)
=1con { H( t) = H(y) }
—PT (h(y) = t∣H(y) = y) PT (H(y) = y)
—PT (H( y )= t ∣H( y ) = y) PT (H( y ) = y).	(14)
Since y = H(t), it follows that
PT (H( y )= t∣H( y )= y) =0.	(15)
Then, (14) becomes:
Et [1con{H(t) = H(y)H = 1con{H(t) = H(y)} - PT (H(y)= t∣H(y) = y) PT(H(y) = y)
=1con { H( t) = H(y) }
-pt (H(y) = t∣H(y) = y) Et [1con{H(y) = H(y)H ,	(16)
hence, β = -PT(H(y) = t∣H(y) = y).
B Connections between the Optimization Variance and the
Gradient Variance
This section shows the connection between the gradient variance and the optimization variance in
Definition 1 of our paper.
For simplicity, We ignore q in OVq(x) and denote g(TB) - ETBg(TB) by g(TB). Then, the gradient
variance Vg can be written as:
Vg = ETb h kg(TB) - ETb g(TB) k 2] = ETB [g(TB ¥g(TB)] ∙	(17)
Denote the Jacobian matrix of the logits f(x; θ) w.r.t. θ by Jθ (x), i.e.,
Jθ(x) = [Vθfι(x; θ), Rθf2(X; θ),…,Rθfc(X; θ)],	(18)
where fj (x; θ) is the j-th entry of f(x; θ), and c is the number of classes.
Using first order approximation, we have:
f(x; θ+ g(TB)) ≈ f(x; θ) + Jθ(x)Tg(TB),	(19)
and OV(x) can be written as:
OV(x) ≈
ETb 回Tb )τJθ(x) Jθ(x)τg(Tb )]
f ( X ； θ )Tf( X ； θ ) + E TB O ( kg (TB ) k 2)]
ETB [g(TB )t Jθ (X) Jθ (X)tg(TB)]
f (X ； θ) t f (X ； θ)
The only difference between Ex [OV (x)] and Vg is the middle weight matrix Ex
(20)
(21)
Jθ ( X ) Jθ ( X )T
f (x; θ) t f (x; θ) ]∙
This suggests that penalizing the gradient variance can also reduce the optimization variance.
13
Under review as a conference paper at ICLR 2022
Figure 7 presents the curves of Vg in the training procedure. It can be observed that Vg also shows
some ability to indicate the generalization performance. However, compared with the results in
Figure 2 of our paper, we can see that OV demonstrates a stronger power for indicating the gen-
eralization error than Vg . More importantly, Vg loses its comparability when the network size
increases, while OV can be more reliable to architectural changes with the middle weight matrix
Ex [芯)*f X§)] to normalize Vg, which is illustrated in Figure 6 of our paper.
We also notice that kETB g (TB)k22 is usually far less than ETB kg(TB)k22, hence Vg and the gradient
norm ETB kg(TB)k22 almost present the same curves in the training procedure.
0.9
epoch
<O 0.7
0.8
0.6
epoch
(b) CIFAR10
(c) CIFAR100
Figure 7: Test accuracy and Vg . The models were trained with the Adam optimizer (learning rate
0.0001). The number in each legend indicates its percentage of label noise.
C B ehaviors of Different Loss Functions
Many different loss functions can be used to evaluate the test performance of a model. They may
have very different behaviors w.r.t. the training epochs. As shown in Figure 8, the epoch-wise
double descent can be very conspicuous on test error, i.e., the ZO loss, but barely observable on CE
and MSE losses, which increase after the early stopping point. This is because at the late stage of
training, model outputs approach 0 or 1, resulting in the increase of the CE and MSE losses on the
misclassified test samples, though the decision boundary may be barely changed. When rescaling
the weights of the last layer by a positive real number, the ZO loss remains the same because of
the untouched decision boundary, whereas the CE and MSE losses are changed. Thus, we perform
bias-variance decomposition on the ZO loss to study epoch-wise double descent.
D VGG11 on SVHN by Adam Optimizer with Learning Rate 0.001
Training VGG11 on SVHN by Adam optimizer with learning rate 0.001 is unstable, as shown in
Figure 13(a). Figure 9 shows the test error and optimization variance. For 0% and 10% label noise,
the test error stays large (the test accuracy is low) for a long period in the early phase of training.
The optimization variance is also abnormal.
14
Under review as a conference paper at ICLR 2022
3
S
S ɔ
O 2
1
λ#wMA∣M'w^'M*λ
0.8-
S 0.6- ∖/
0.4-
0.2-
50	100 150 200 250
epoch
√⅛*wMm∙*wmm*¼N<
50	100 150 200 250
epoch
(c) ZO loss
(a)	CE loss	(b) MSE loss
Figure 8: Different loss functions w.r.t. the training epoch. ResNet18 was trained on SVHN, CI-
FAR10, and CIFAR100 with 20% label noise to introduce epoch-wise double descent. Adam opti-
mizer with learning rate 0.0001 was used.
(a) 0% label noise
(b)	10% label noise	(c) 20% label noise
Figure 9:	Test accuracy and optimization variance (OV) of VGG11 on SVHN, w.r.t. different levels
of label noise. Adam optimizer with learning rate 0.001 was used.
E Loss, Bias and Variance w.r.t. Different Levels of Label Noise
Label noise makes epoch-wise double descent more conspicuous to observe (Nakkiran et al., 2020).
If the variance is the major cause of double descent, it should match the variation of the test error
when adding different levels of label noise.
Figure 10 shows an example to compare the loss, variance, and bias w.r.t. different levels of label
noise. Though label noise impacts both the bias and the variance, the latter appears to be more
sensitive and shows better synchronization with the loss. For instance, when we randomly shuffle
a small percentage of labels, say 10%, a valley clearly occurs between 20 and 50 epoches for the
variance, whereas it is less obvious for the bias. In addition, it seems that the level of label noise
does not affect the epoch at which the loss reaches its first minimum. This is surprising, because
the label noise is considered highly related to the complexity of the dataset. Our future work will
explore the role label noise plays in the generalization of DNNs.
F Detailed Information of the Small CNN model
We present the detailed information of the architecture trained with a small number of training
samples. It consists of two convolutional layers and two fully-connected layers, as shown in Table 2.
15
Under review as a conference paper at ICLR 2022
4 3 2 1 0
■ ■ ■ ■ ■
Ooooo
ω-0u -ωqnj-
6 5 4 3 2 1
■ ■ ■ ■ ■ ■
Oooooo
4 3 2 1 0
■ ■ ■ ■ ■
Ooooo
ω-0U -ωqnj-
4 3 2 1
■ ■ ■ ■
Oooo
4 3 2 1 0
■ ■■■■
Ooooo
ω-0U -ωqnj-
5 4 3 2 1
■ ■■■■
Ooooo
Figure 10:	Loss, variance and bias w.r.t. different levels of label noise. The model was ResNet18
trained on CIFAR10. Adam optimizer with learning rate 0.0001 was used.
Table 2: Architecture of the small CNN model (“BN” denotes Batch Normalization).
Layers	Parameters	BN	Activation ∣ Max pooling	
Input	input Size=(32, 32) × 3	-	-	-
ConV	filters=(3,3) X 32;	X	ReLU	(2, 2)
ConV	filters=(3, 3) X64;	X	ReLU	(2, 2)
Dense	nodes=1024	-	ReLU	-
Dense	nodes=10		Softmax	
16
Under review as a conference paper at ICLR 2022
G Bias and Variance Terms w.r.t. Different Loss Functions
VGGll
VGG13
,le>∕ssq/SSo- -IBΛ∕ssq/SSo-
0.06
0.04
0.02
50
100 150 200 250
epoch
ResNetl8
0.05-
0.04-
0.03-
0.02-
0.01-
o∞-
50	100 150 200 250
epoch
0.06
0.05
0.04-
0.03-
0.02-
0.01-
50	100 150 200 250
epoch
士 0 008
fθ
wJ 0.006
-ro
左 0.004
IΛ
in
° 0.002
0.000
0.010
VGG16
50	100 150 200 250
epoch
ResNet34
ResNetl8
0.010
J 0.008
ro
G 0.006
ro
W 0.004
■2 0.002
0.000
50	100 150 200 250
epoch

(a) SVHN	(b) CIFAR10	(c) CIFAR100
Figure 11:	Test MSE loss and the corresponding bias/variance terms. The models were trained with
20% label noise. Adam optimizer with learning rate 0.0001 was used.
VGGll
1.50-
1.25-
1.00-
0.75-
0.50-
0.25-
0.00-
loss
bias
var s*v*
50	100 150 200 250
epoch
1.50-
-1.25-
ro
,ft100-
ro
≤ 0.75-
W 一
g 0.50-
0.25-
50	100	150 200 250
epoch
ResNetl8
4 3 2 1 0
,le>∕ssq/Sso-
ResNetl8
0 8 6 4 2
■ ■ ■ ■ ■
Ioooo
,le>∕sq/Sso-
(a) SVHN
g
-
,le>∕s'sq/SSo-
(b) CIFAR10
,le>∕s'sq/Sso-
ResNet34
，50
ep
50
2
(c) CIFAR100
Figure 12:	Test CE loss and the corresponding bias/variance terms. The models were trained with
20% label noise. Adam optimizer with learning rate 0.0001 was used.
17
Under review as a conference paper at ICLR 2022
H Bias and Variance Terms w.r.t. Different Optimizers and
Learning Rates
VGGll fail
6 4 2
■ ■ ■
Ooo
,le>∕sq/SSo-
50	100 150 200 250
epoch
VGG13
0∙80∙60∙4
,le>∕sq/Sso-
50	100	150 200 250
epoch
ι.o
,le>∕s'sq/Sso-
VGG16
50	100	150	200	250
epoch
ResNetl8
(a) SVHN
ResNetl8
(b) CIFAR10
ResN et34
(c) CIFAR100
Figure 13:	The expected test ZO loss and its bias and variance. The models were trained with 20%
label noise. Adam optimizer with learning rate 0.001 was used.
VGGll
VGG13
VGG16
ResNetl8	ResNet34
ResNetl8
T50
200
50
2
(b) CIFAR10
,le>∕s'sq/Sso-
，50
θθ
2
(a) SVHN
(c) CIFAR100
Figure 14:	Expected test ZO loss and its bias and variance. The models were trained with 20% label
noise. SGD optimizer (momentum = 0.9) with learning rate 0.01 was used.
I Optimization Variance and Test Accuracy w.r.t. Different
Optimizers and Learning Rates
18
Under review as a conference paper at ICLR 2022
VGGll
VGG16
ResNetl8	ResN et34
ResNetl8
4
Llr
,le>∕ssq/SSo-
6 5 4 3
■ ■ ■ ■
Llr Llr Llr Llr
,le>∕s'sq/Sso-
(c) CIFAR100
(a) SVHN	(b) CIFAR10
Figure 15:	Expected test ZO loss and its bias and variance. The models were trained with 20% label
noise. SGD optimizer (momentum = 0.9) with learning rate 0.001 was used.
0
1
VGG16
0.6
U 0.4-
0.0020
0.2 -
0.0005
0.0 √
0	50 100 150 200 250
epoch
0.0015
O
0.0010
acc, 0%
OV, 0%
acc, 20%
OV, 20%
0.0025
(a) SVHN
0.0000
0.9
U 0.8
7
0.6
0.5
roo
(b) CIFAR10
(c) CIFAR100
Figure 16: Test accuracy and optimization variance (OV). The models were trained with Adam
optimizer (learning rate 0.001). The number in each legend indicates its percentage of label noise.
19
Under review as a conference paper at ICLR 2022
ResNetlB
o
ι
' f -- - — "i—ar
0.9
U 0.8
roo
0.6
0.5
0.005
0.004
0.003
0.002
0.001
.p.ooo
0	50 100 150 200 250
epoch
7^
(b) CIFAR10
(a) SVHN
VGG16
0.0030
0.0025
0.0020
0.0015 §
0.0010
0.0005
o∙oooo
0	50 100 150 200 250
epoch
0.5-
4-
0.3-
7Γ
6
—acc, 0% — acc, 20%
OV, 20%
0.2-
--OV, 0%
：7v，ZH 小~
(c) CIFAR100
o
0
U o
e
0
1 -

7
Figure 17:	Test accuracy and optimization variance (OV). The models were trained with SGD op-
timizer (learning rate 0.01, momentum 0.9). The number in each legend indicates its percentage of
label noise.
ι.o
0.9
U 0.8
u
^0.7
0.6
0.5
VGGlI
-----------
0	50 100 150 200 250
epoch
0.004
0.003
0.002
0.001
0.000
0.6
ResNetie
0.9
U 0.8
7
roo
-0.006
-0.005
-0.004
-0.003
9 M交4%ML A≠**V,*m **1,
0.007
-0.002
j 0.001
50 100 150 200 250
epoch
0.000
(b) CIFAR10
(c) CIFAR100
(a) SVHN
g
1
o
0⅛

g
Figure 18:	Test accuracy and optimization variance (OV). The models were trained with the SGD
optimizer (learning rate 0.001, momentum 0.9). The number in each legend indicates its percentage
of label noise.
20
Under review as a conference paper at ICLR 2022
J OV Estimated from Different Number of Training Batches
OVq (x) in Figure 2 of our paper was estimated on all training batches; however, this may not be
necessary: a small number of training batches are usually enough. To demonstrate this, we trained
ResNet and VGG on several datasets using Adam optimizer with learning rate 0.0001, and estimated
OVq(x) from different number of training batches. The results in Figure 19 show that we can well
estimate the OV using as few as 10 training batches.
0.0025 -
0.0020-
0.0015-
0.0010-
0.0005-
O-OOOO-
0	50 100 150 200 250
epoch
0.0025-
0.0020-
0.0015-
0.0010-
0.0005-
O-OOOO-
0	50	100 150 200 250
epoch
VGG16
—10	— 100
20	— 200
-50
0.0025 -
0.0020-
0.0015 -
0.0010-
0.0005-
O-OOOO-
0	50 100 150 200 250
epoch
ResNetl8
0.0025
0.0020
0.0015
0.0010
0.0005
0-0000
0	50	100 150 200 250
epoch
(a) SVHN	(b) CIFAR10	(c) CIFAR100
Figure 19:	OV estimated from different number of training batches. The models were trained with
20% label noise. Adam optimizer with learning rate 0.0001 was used.
21