Under review as a conference paper at ICLR 2022
Revisiting Contrastive Learning through the
Lens of Neighborhood Component Analysis:
an Integrated Framework
Anonymous authors
Paper under double-blind review
Ab stract
As a seminal tool in self-supervised representation learning, contrastive learning
has gained unprecedented attention in recent years. In essence, contrastive learning
aims to leverage pairs of positive and negative samples for representation learn-
ing, which relates to exploiting neighborhood information in a feature space. By
investigating the connection between contrastive learning and neighborhood com-
ponent analysis (NCA), we provide a novel stochastic nearest neighbor viewpoint
of contrastive learning and subsequently propose a series of contrastive losses
that outperform the existing ones. Under our proposed framework, we show a
principled way to design integrated contrastive losses that simultaneously achieve
good accuracy and robustness on downstream tasks.
1 Introduction
With the growing need for deployable machine learning, contrastive learning has drawn much
attention and has become one of the most effective representation learning techniques in the past
years. The contrastive paradigm (Oord et al., 2018; Wu et al., 2018; He et al., 2020; Chen et al.,
2020a; Chuang et al., 2020; Grill et al., 2020) constructs an objective for embeddings based on an
assumed semantic similarity, and the ability to distinguish dissimilar instances. This, in turn, stems
from instance-level classification (Dosovitskiy et al., 2015; Bojanowski & Joulin, 2017; Wu et al.,
2018). Specifically, the contrastive loss LCL (Oord et al., 2018; Chen et al., 2020a) is given by
LCL := Ex〜D,x+〜D+,x-〜D-
ef(x)Tf(x+)
-log	N
ef(x)Tf(x+) + P ef (x)T f (xi- )
i=1
(1)
where, for a sample x, (x, x+) denotes a positive pair and (x, x-) denotes a negative pair. The
function f is normally parameterized by a neural network and the number of negative pairs N is
typically treated as a hyperparameter. When constructing loss LCL in Equation 1, ideally, one draws
x+ from the data distribution Dx+ that characterizes the semantically-similar (i.e., positive) samples to
x; similarly, one wants to draw x- from Dx- that characterizes the semantically-dissimilar (negative)
samples.
However, this construction faces several challenges in practice. First, the definition of semantically-
similar and semantically-dissimilar is contingent on downstream tasks. For example, an image of a
cat can be considered semantically similar to that of a dog if the downstream task is to distinguish
between animal and non-animal classes. To provide a surrogate of measuring similarity, mainstream
contrastive learning algorithms (He et al., 2020; Chen et al., 2020a;b; Grill et al., 2020) typically
build up Dx+ by considering data augmentation Dxaug of a data sample x. We note that although Dxaug
might over-simplify the construction of Dx+ , as only the standard data augmentation is used, the
positive samples generated in Dxaug are valid positive examples w.r.t. x. In contrast, the construction
of Dx- in existing contrastive learning literature (Oord et al., 2018; Chen et al., 2020a) contains a
non-negligible portion of invalid samples due to a lack of label information in the unsupervised
learning setting (Chuang et al., 2020), where Dx- is approximated by the joint distribution D or
D∖Ug := ∪xo∈D∖{x}DxUog in practice. More precisely, if the negative samples are sampled from D-,
1
Under review as a conference paper at ICLR 2022
5 0 5 0 5
4 4 3 3 2
(ZOOOU9) AOB-InOOB ∙>PB
IntCl(OUrs)
∕S2∖ +NaCl
i÷;--------------
Adv
IntNaCL(Ours)
Z ∖
Debiased+HardNeg
HardNeg
Debiased
I工]
SimCLR
53	54	55	56	57	58	59	60
std. accuracy (%)
.5
75.
(ZOOOMw) >ora^□oora ∙>PB JΦJSUB 二
HardNeg
Debiased+HardNeg
Debiased ∣	∣	∣
SimCLR
76	76.5	77	77.5	78	78.5	79
transfer std. accuracy (%)
(a) CIFAR100
(b) CIFAR10
Figure 1: The performance of existing methods and our proposal (IntNaCl & IntCl) in terms of their
standard accuracy (x-axis) and adversarial accuracy under FGSM attacks = 0.002 (y-axis). The
transfer performance refers to fine-tuning a linear layer for CIFAR10 with representation networks
trained on CIFAR100.
we will receive with 1/K probability a positive sample in a K -class classification task with balanced
classes. This may be undesirable. This heuristic loss was proposed in (Chen et al., 2020a) and is
known as LSimCLR:
LSimCLR := Ex〜D,X+〜Dχug,χ-〜D∖uχ
ef(x)Tf(x+)
N
ef(x)Tf(x+) + P ef (x)T f (xi- )
i=1
(2)
Another challenge is the computation of positive and negative pairs grows quadratically with the size
of the dataset. Therefore, computing all the pairwise comparisons on a large dataset is not practical,
and most implementations approximate the loss by reducing the number of comparisons to random
subsets of images during training (Wu et al., 2018; He et al., 2020; Chen et al., 2020a).
Neighborhood component analysis (NCA)
Z(∙) = KL and x+ 〜D：Ug
(0^ M ∈ Z+ and qi ∈ [0,1]
Neiqhborhood analysis COntraStiVe IOSS
(LNaCl)
InteQrated NaCl
(LIntNaeI)
Higher standard accuracy, and
more robust to adversaries.
LSimCLR
Decent standard accuracy
vulnerable to adversaries.
Special cases:
Chen et al., 2020a; Chuang et al., 2020;
Robinson et al., 2021; Ho & Vasconcelos,
2020	________________
In this paper, we aim to examine the
above challenges through the lens of
the nearest neighbor classification in
Neighborhood Component Analy-
sis (NCA) (Goldberger et al., 2004).
Specifically, we uncover the rela-
tionship between stochastic nearest
neighbors and positive pairs in con-
trastive learning, which then moti-
vates a sequence of augmented con-
trastive losses that work better under
practical computational constraints.
Furthermore, representation learn-
ing has been evaluated mostly by
how they cluster or by a metric such
as the standard downstream classi-
fication accuracy. However, by in-
specting the adversarial accuracy of Figure 2: A conceptual illustration of the relationships among
several existing methods (e.g., Fig- NCA, LSimCLR, and our proposals.
ure 1’s y-axis, the classification ac-
curacy when inputs are corrupted by
crafted perturbations), one can see the insufficiency of those methods in addressing robustness. We
thus present a general integrated contrastive framework that accounts for both the standard accuracy
and adversarial cases; this method’s performance remains in the desired upper-right region (circled)
as shown in Figure 1. A conceptual illustration of our proposals is given in Figure 2. We summarize
our main contributions as follows:
2
Under review as a conference paper at ICLR 2022
•	We relate contrastive losses with Neighborhood Component Analysis (NCA) and generalize
it in the contrastive learning setting, which we dub NaCl, Neighborhood analysis Contrastive
loss.
•	Building on top of NaCl, we proposed a generic framework called Integrated contrastive
learning (IntCl and IntNaCl) where we show that the spectrum of recently-proposed
contrastive learning losses (Chuang et al., 2020; Robinson et al., 2021; Ho & Vasconcelos,
2020) can be included as special cases of our framework;
•	We provide extensive experiments that demonstrate the effectiveness of IntNaCl in im-
proving standard accuracy and adversarial accuracy. Specifically, IntNaCl improves upon
literature (Chen et al., 2020a; Chuang et al., 2020; Robinson et al., 2021; Ho & Vasconcelos,
2020) by 3-6% and 4-16% in CIFAR100 standard and adversarial accuracy, and 2-3% and
3-17% in CIFAR100 standard and adversarial accuracy, respectively.
2 Related Work
Contrastive learning. In the early work of Dosovitskiy et al. (2015), authors treat every individual
image in a dataset as one own class and do multi-class classification tasks under the setting. However,
this regime will soon become intractable as we have a large dataset. To cope with this, Wu et al.
(2018) design a memory bank for storing presented representations (keys) and utilize noise contrastive
estimation (Gutmann & Hyvarinen, 2010; Mnih & Teh, 2012; JozefoWicz et al., 2016; Oord et al.,
2018) for representation comparisons. Then, He et al. (2020) and Chen et al. (2020b) further improve
upon this by storing keys inferred from a momentum encoder other than the representation encoder
for x. Finally, besides the practical tricks introduced in SimCLR (Chen et al., 2020a) (e.g. stronger
data augmentation scheme and projector heads), authors of SimCLR also get rid of the memory bank
and instead makes use of other samples from the same batch to form contrastive pairs. Caron et al.
(2020) consider cluster assignment tasks instead of instance learning and design an online algorithm
for learning. Specifically, if We fix the trainable prototypes C to be the data batch and choose code Q
to be identity, then the sWapped prediction problem reduces to similar forms of Equation 3. Grill et al.
(2020) argue the possibility of forming contrastive losses Without the need for negative examples. In
the rest of this paper, We Will focus on the setups of SimCLR and the related folloW up Work (Chuang
et al., 2020; Robinson et al., 2021; Ho & Vasconcelos, 2020) due to computational efficiency. We
let go(x, {χ-}N) denote the negative term N PN=I ef (X)Tf (X- ), where the subscript i identifies the
summation index and the superscript N identifies the summation limits. We omit the subscript i
when the sample index is one dimensional (e.g. xi- has 1-D index, xi-j has 2-D index). Moreover, we
define K(A, B) = - log(A/(A + B)). Then LSimCLR in Equation 2 can be re-written as
LSimCLR ：= EX〜D,χ+〜Dχug,χ-〜D\ux [K(ef(X)Tf(x+ ),Ngo(x, {x-}N))].	⑶
Designing negative pairs in contrastive learning. Several works (Saunshi et al., 2019; Chuang
et al., 2020) have come to the awareness of the sampling bias of negative pairs as discussed in
Section 1. Chuang et al. (2020) propose a de-biased constrastive loss to mitigate the sampling bias
by assuming the priors on the downstream tasks (e.g., with probability 0.1, a positive sample can
be used as xi- in CIFAR10) without using any explicit label information. We denote the loss from
Chuang et al. (2020) as LDebiased and the full equation is shown below:
LDebiased := EX 〜D,χ+,Vj 〜Dxg,Ui 〜D黑 hK(ef(X)Tf(X+),Ng1(x,{ui }n,{vj}m))i,	(4)
where the estimator g1(x, {ui }n, {vj }m) is defined by
nm
gι(x,{ui}n,{vj}m)=max{--+ (— Xef(X) f(Ui)-T+ — Xef(X) fv)),e-1/t}
1 - τ+ n i=1	m j=1
and n and m represents the numbers of sampled points in D\auXg and DXaug for the re-weighted negative
term with τ+ being the probability that a positive pair is mistakenly to be a negative pair when
we sample xi- from D\auXg. Recently, Robinson et al. (2021) propose to weigh sample pairs through
3
Under review as a conference paper at ICLR 2022
the cosine distance in the estimator g1 (x, {ui}n, {vj}m) based on LDebiased, and we denote their
approach as LDebiased+HardNeg ,
LDebiased+HardNeg := Ex 〜D,x+,vj 〜DXug,Ui 〜D∖uχ hK(ef(x)Tf(x+), Ng2(x, {ui}n, {vj}m))i,	(5)
where the estimator g2 (x, {ui}n, {vj }m) is defined by
g2(x, {Ui}n, {Vj }m) = max{ 1-τ+ (P=n ee(ffu -T+ m X f (X)T fS))，e-1"}
i=1 e	j=1
A typical choice of n and m are n = N and m = 1, and the hyperparameter τ+ in g2 is exactly the
same as that in g1 whereas the hyperparameter β controls the weighting mechanism. Specifically,
when τ+ = 0, we denote LDebiased+HardNeg as LHardNeg; when β = 0, Equation 5 degenerates to
Equation 4 which is LDebiased.
Designing positive pairs in contrastive learning. In parallel to the above line of work, another
direction is to augment the construction of positive pairs (Ho & Vasconcelos, 2020; Kim et al., 2020).
Specifically, authors of Ho & Vasconcelos (2020) define the concept of adversarial examples in the
regime of representation learning as the positive sample that maximizes LSimCLR (i.e. Equation 3)
within a pre-specified perturbation magnitude. The resulting loss function is LAdv:
LAdv := Ex〜D,x+〜DxUg,x-〜DaUg,x-〜DadvhK(ef(X) f (X ),Ng0(x, {x-ι }N))
, x , i1	\x , i2	\x
+αK(ef (X)Tf (XadV)，Ngo(χ, {χ-}N川，	⑹
where the D\dV is defined by ∪xo∈d∖{x}x0 ∪ χ0,adv. Notably, one can adjust the importance of the
adversarial term by tUning α in EqUation 6. A relevant idea is exploited by Kannan et al. (2018)
albeit in the sUpervised learning setting, where the adversarial logit paring works Under a sUpervised
learning regime and encoUrages an adversarial sample x0 to have the same logit oUtpUt as the clean
x. Notice that the definition of adversary in Kannan et al. (2018) is different from that in Ho &
Vasconcelos (2020), where it is defined by a pertUrbed sample that is predicted differently from the
original. This can be viewed as sUpervised contrastive learning where the representation space lies in
R1 with the similarity evalUated by `2 distances. In Khosla et al. (2020), aUthors constrUct sUpervised
contrastive loss Using mUltiple positive pairs defined by samples with the same label. Another related
work is InfoMin (Tian et al., 2020), which needs a small amoUnt of labeled data to perform two-step
trainings. In the first step, a view generator is learned by minimizing the mUtUal information between
latents from views while ensUring all of them can be correctly classified; in the second step, the
trained generator is frozen and the representation network is learned by maximizing the mUtUal
information between latents from views. We note that InfoMin can be regarded as inserting a learned
“hard positive sample” into contrastive learning. Dwibedi et al. (2021) inherit the Usage of a memory
bank as in He et al. (2020) and select from it the representation candidate z0 nearest to z = f (x)
(determined by nearest neighbor). EventUally the loss is formed by sUbstitUting z0 for z in EqUation 3.
Adversarial Robustness. Despite neUral networks’ sUpremacy in achieving impressive perfor-
mance, they have been proved vUlnerable to hUman-imperceptible pertUrbations (Goodfellow et al.,
2015; Szegedy et al., 2014; NgUyen et al., 2015; Moosavi-Dezfooli et al., 2016). In the sUpervised
learning setting, an adversarial pertUrbation δ is defined to render inconsistent classification resUlt of
the inpUt x: r(x + δ) 6= r(x), where r is a neUral network classifier. A stronger adversarial attack
means it can find δ with higher sUccess attack rate Under the same -bUdget (kδ kp ≤ ). One of
the most popUlar and classical attack algorithms is FGSM (Goodfellow et al., 2015), where with a
fixed pertUrbation magnitUde , FGSM Uses the sign of cross entropy gradient to decide between
δ = and δ = -. Another popUlar attack method we consider in this paper is PGD (Madry et al.,
2018), which assembles the iterative-FGSM (KUrakin et al., 2016) bUt with additional projection
steps. Considering the robUstness in representations, except the inclUsion of adversarial examples as
positive samples (Ho & Vasconcelos, 2020; Kim et al., 2020), recent literatUre (Gowal et al., 2020)
has strengthened BYOL (Grill et al., 2020) by inclUding an adversarial training loss.
4
Under review as a conference paper at ICLR 2022
3 Neighborhood analysis Contrastive loss
In this section, we first establish a connection between the literature on Neighborhood Component
Analysis (NCA) (Goldberger et al., 2004) and the contrastive learning loss in Section 3.1. Inspired
by our result in Section 3.1, we further design three novel contrastive losses in Section 3.2, which
we refer to as Neighborhood analysis Contrastive loss (NaCl), that are beneficial for learning more
powerful representations that can achieve higher downstream task accuracy.
3.1	Stochastic nearest neighbor framework
NCA is a supervised learning algorithm concerned with learning a distance metric that maximizes
the performance of nearest neighbour classification. As nearest neighbor is a non-smooth function of
points, the optimization problem is generally given using the concept of stochastic nearest neighbors.
In the stochastic nearest neighbor setting, nearest neighbor selection is regarded as a random event,
where the probability point xj is selected as the nearest neighbor for xi is given as p(xi , xj ) with
e-kAxi -Axj k2
Pij=P(Xi,xj ) = P	kAχi-Aχkk2
k6=i
8 e-kAχi-Axjk2
Let ci denote the label ofxi, in the leave-one-out classification loss, the probability a point is classified
correctly is given as Pi = Pj|c =c Pij, where {j | cj = ci} defines an index set with which all
points xj belong to the same class as point xi. The probability xi ’s label is ci is given as qi, which
is exactly 1. Thus the optimization problem can be written as min a Pn= i '(qi, Pj©=Ci Pij). This
learning objective then naturally maximizes the expected accuracy of a 1-nearest neighbor classifier.
Two popular choices for '(∙) are the total variation distance and the KL divergence. In the seminal
paper of Goldberger et al. (2004), the authors showed both losses give similar results. We will
focus on the KL divergence loss in this work. For '(∙) = KL, the relative entropy from P to q is
Dkl(9∣∣p) = Pi -qi log pi = Pi — log Pi, when qi = 1, and the optimization problem becomes
n	e-kAxi -Axj k2
min > — log >	-----------------ɪ
A /，	P P P Pr 上,e-kAxi-Axkk
i=1	j|cj =ci	k6=i
(7)
With the above formulation, we argue that the contrastive learning loss can be given assuming only
positive pairs belong to the same class and the metric Ax is instead parametrized by a function
f(X) — h(X)
--——— :~~
√2	√2kh(x)k
, where h is a neural network. Specifically, Equation 7 becomes Equation 9:
n
min — log
f i=1
X e-11If(Xi)-f(χ+)k2 \
工 P- e-1 kf(g)-f(χk)k2 J
j=1 k6=i e
M
—(X)k = 1> minEx〜D [k(X ef(X)Tf(x+),Ngo(x, {x-}N))].
(8)
(9)
We give the full derivation from Equation 8 to Equation 9 in the appendix. We note that the contrastive
loss in Chen et al.(2020a) is a special case of Equation 9 with M = 1,x+ 〜Dxug, which yields the
minimization over the exact form of LSimCLR as given in Equation 3,
minEx〜D,x+〜DxUg,x-〜D∖uχ [K(ef(X)Tf(x+), Ng0(x, {x-}N))i.
3.2	NCA inspired contrastive loss
Although we have shown the connection between NCA and contrastive learning, applying the NCA
framework exactly is challenging in two senses. On one hand, it requires us to use all possible negative
pairs which is approximately the size of the entire dataset. Furthermore, to decide the “demographic ”
of a point’s neighborhood, M depends on the relative density of positive to negative pairs one expects
to have in the underlying data distribution. To tackle these, as using the entire dataset to approximate
the population loss is computationally infeasible, we propose to use a stochastic approximation to
5
Under review as a conference paper at ICLR 2022
the population loss, where N is determined as a hyperparameter in a fashion similar to the batch
size hyperparameter (Chen et al., 2020a). In order to determine M, as the expected relative density
is task-dependent, we treat the M/N ratio as a hyperparameter similar to the class probabilities τ +
introduced by Chuang et al. (2020).
We start this section by examining the specifications LSimCLR has made when simplifying from a
general stochastic nearest neighbor algorithm: '(∙) = KL, qi = 1, M = 1,x+ 〜 Dxaug. If we keep the
first parts of the simplification fixed (i.e. '(∙) = KL, qi = 1, M = 1),then in practice, the expectation
over the probability distribution Dxaug is estimated by only one sample. To reduce the variance of such
an estimator, with a slight abuse of notation, we denote the number of trials by M and propose to
simulate M trials of the procedure for every x. This yields the following loss1
1M
LVAR(g = go, M) ：= Ex〜D,x+〜Dxug,xj〜D\ux [M∙ X K(ef(x)Tf(x+ ),Ng0(x, {xj}NH ∙
j=1
By shifting our focus to the number of neighbors that are considered belonging to the same class
(M), we admit the potential bias induced by assuming M = 1 (i.e. the relative density of positive to
negative pairs to be 1/N). Therefore, we experiment with enlarging the index set {j | cj = ci} to
include more than one element or equivalently M 6= 1. This leads us to the following objective
M
LBiAs(g = go, M) ：= Ex〜D,χ+〜Dχug,χ-〜D\ux [K(X ef(x)Tf(XnNg0(x, {x-}NH .
j=1
Finally, we challenge the specification of qi = 1 and consider a synthetic data point x0 = λxi +
(1 - λ)y, y 〜D that belongs to a synthetic class cλ,i. Assume the probability Xi's label is cλ,i is
qλ,i = λ + (1 - λ)[cy = ci], then qλ,i should matches the probability pλ,i = Pj|cj =cλ,i pij, where
{j | cj = cλ,i } is a singleton containing only the index of x0, which yields2
LMIXUP(g = g0,MO :=Ex〜D,x+〜Daug,x- ,x- ,x-〜DaUghK(ef(x) f(x ),Ng0(XNx-JN))
, x , i1 , i2 , j	\x	1
M-1
+	M-I X K(ef(x)Tf(λx++(1-λ)x-),Ngo(χ, {χ-2j}N))
M - 1 j=1
M-1
+	M--1 X K (Ngo(x,{x-j}N ),ef (x)T f(λx++(If)X-R .
M - 1 j=1
Interestingly, the construction of X0 herein assembles the mixup (Zhang et al., 2018) philosophy in
supervised learning. We therefore name this loss by LMIXUP. Notably, as the above NCA inspired
contrastive losses are designed from orthogonal perspectives, they are complementary to each other.
4	An integrated framework for contrastive learning
Building on top of our proposed NCA inspired loss (NaCl) in Section 3, we propose a novel framework
that generalizes and integrates many of the existing work in contrastive learning. Our integrated
contrastive learning framework has two versions, one is IntCl and the other is IntNaCl, where both
of them consist of two components - a standard loss and a robustness-promoting loss. For IntCL
the standard loss can be existing contrastive learning loss (Chen et al., 2020a; Chuang et al., 2020;
Robinson et al., 2021), whereas for IntNaCl we use our proposed NaCl loss as the standard loss.
We will show that this new framework not only generalizes existing methods but also achieves good
accuracy and robustness simultaneously. Note that as shown in Figure 1(a) and 1(b), most of the
existing works have primarily focused on improving the clean downstream accuracy only, which
often has an undesired robustness accuracy.
1Sohn (2016) briefly explores the relationship between NCA and metric learning in a supervised learning
setting and optimizes a similar loss with multiple positive examples.
2Lee et al. (2021) augment the dataset by including similar synthetic data point. Since the perspective is
different, the formed contrastive loss takes different form (cf. Eqn. 5 and 6 in the literature).
6
Under review as a conference paper at ICLR 2022
4.1	Integrated contrastive loss
To design a generic loss that accounts for both clean and adversarial accuracy, together with a
robustness-promoting term, we utilize the NaCl developed in Section 3 to construct a contrastive
learning framework, called Integrated Neighborhood analysis Contrastive loss (IntNaCl). A general
form of LIntNalL is given by
LIntNaCl (α, LNa(g , M, λ), LRobust (g , w)) := LNa(g , M, λ) + αLRobust(g , w),	(10)
where LNa(g1, M, λ) can be chose from {LVAR(g1,M), LBIAS(g1, M), LMIXUP(g1,M,λ)} and
LRobUst(g2,w):= E[K(ef(x)Tf(xadv), Ng2(x, ∙))w(x)].
The variable g1 and g2 allow us to pick the estimators from {g0, g1, g2} and w(x) facilitates goal-
specific weighting scheme. Furthermore, we remark that as LVAR, LBIAS, and LMIXUP all reduce to
one same form when M = 1, we denote the LIntNaCl under these cases by Integrated Contrastive loss
(IntCl):
LIntCL(α, g1,g2,w) ：= E [κ(ef (X)Tf(x+), Ng1(x, ∙)) + αK(ef(X)Tf(Xadv), Ng2(x, ∙))w(x)],
(11)
where we show many of the existing works are a special case:
α = 0,g1 = g0,
α = 0,g1 = g1,
α = 0,g1 = g2,
LSimCLR (Chen et al., 2020a) (i.e. Equation 3);
LDebiased (Chuang et al., 2020) (i.e. Equation 4);
LDebiased+HardNeg (Robinson et al., 2021) (i.e. Equation 5);
Ia = 1,g1 = go, g2 = go, w(χ) ≡ 1, LAdV (Ho & Vasconcelos, 2020) (i.e. Equation 6).
4.2	Adversarial weighting
Weighting sample loss based on their margins has been proven to be effective in the adversarial
training under supervised settings (Zeng et al., 2020). Specifically, it is argued that training points
that are closer to the decision boundaries should be given more weight in the supervised loss. While
the margin of a sample in supervised settings is well-defined, it is underdefined in unsupervised
settings. To tackle this, we borrow the intelligence from Ho & Vasconcelos (2020) and mimic how
the authors transfer the definition of adversarial examples in supervised learning to unsupervised
learning. Specially, we see that as an adversarial example in supervised learning is defined by a
perturbed sample that has a zero margin to the decision boundary, authors of Ho & Vasconcelos (2020)
define adversarial example in unsupervised learning to be an augmented sample that maximizes the
contrastive loss. With this, we also give our weighting function as the value of the contrastive loss
W(x) := K(ef (X)Tf(x+), Ng(x, ∙)), where the estimator g can be go, gι, g2. Using this, We see that
samples that are originally hard to be distinguished from other samples (i.e. small probability) are
now assigned with bigger weights.
5	Experimental results
5.1	Experimental Set-up
Implementation details. All the proposed methods are implemented based on open source repos-
itories provided in the literature (Chen et al., 2020a; Ho & Vasconcelos, 2020; Robinson et al.,
2021). Five benchmarking contrastive losses are considered as baselines that include: LSimCLR (Chen
et al., 2020a), LDebiased (Chuang et al., 2020), LDebiased+HardNeg (Robinson et al., 2021), LAdv (Ho &
Vasconcelos, 2020) (i.e. Equation 3, Equation 4, Equation 5, Equation 6). We train representations on
resnet18 and include MLP projection heads (Chen et al., 2020a). A batch size of 256 is used across
all the experiments. Unless otherwise specified, the representation network is trained for 100 epochs.
We run five independent trials for each of the experiments and report the mean and standard deviation
in the entries. Throughout our experiments, no adversarial fine-tuning is performed. We implement
the proposed framework using PyTorch to enable the use of an NVIDIA GeForce RTX 2080 Super
GPU, two NVIDIA Tesla P100 GPUs, and four NVIDIA Tesla V100 GPUs.
7
Under review as a conference paper at ICLR 2022
Table 1: The CIFAR100 linear evaluation results (%) of NaCl on LSimCLR, LDebiased+HardNeg, and LIntCl
(ours, cf. Equation 11). The best improvement over the individual baseline is in boldface.
M
2
3
4
5
LSimCLR : 53.69 ± 0.25
LVAR	LBIAS	LMIXUP
56.04±0.17 55.72±0.15 56.20±0.33
57.11±0.21	56.67±0.12 56.41±0.13
57.27±0.14 57.09±0.26 56.00±0.42
57.91±0.12 57.32±0.17 56.63±0.31
LDebiased+HardNeg : 56.83 ± 0.20
LVAR	LBIAS	LMIXUP
58.17±0.39^^57.87±0.15^^60.69±2.43
59.08±0.29	58.42±0.23	59.81±0.25
59.29±0.16	58.86±0.18	59.75±0.33
59.67±0.38	58.81±0.21	59.85±0.30
LIntCl : 56.22 ± 0.15
LVAR	LBIAS	LMIXUP
57.51±0.12	56.71±0.11	58.97±0.19
58.08±0.18	57.13±0.26	59.26±0.18
58.31±0.23	57.06±0.19	59.32±0.21
58.64±0.24	57.46±0.04	59.43±0.23
Evaluation protocol. In this section, we will evaluate three major properties of representation learn-
ing methods: standard discriminative power, naive transferability, and adversarial robustness. To eval-
uate the standard discriminative power, we train representation networks on CIFAR100 (Krizhevsky
et al., 2009), freeze the network, and only fine-tune a fully-connected layer that maps representations
to outputs on CIFAR100. To evaluate the transferability, we use the same representation networks as
above, and only fine-tune a fully-connected layer that maps representations to outputs on CIFAR10.
All the adversarial robustness evaluations are completed using the implementation provided by Wong
et al. (2020). We supplement more FGSM and PGD attack results in the appendix.
5.2	Linear Evaluation of Self-supervised Representations
Improvement over baselines. In this section, we test the effectiveness of NaCl in improving the
downstream CIFAR100 classification accuracy. Specifically, we consider three baseline methods: 1)
LSimCLR, or equivalently α = 0,g1 = g0,M = 1 in Equation 10; 2) LDebiased+HardNeg, or equivalently
α = 0,g1 = g2, M = 1 in Equation 10; and 3) LIntCl(α = 1,g1 = g2, g2 = g2, w = W). We list the
results in Table 1. Due to page limit, we only give the results of LMIXUP with λ = 0.9 when applied
on LSimCLR, and with λ = 0.5 when applied on LDebiased+HardNeg and LIntCl. Complete tables of results
obtained with λ = 0.6, 0.7, 0.8 can be found in the appendix. By referring to the Table 1, one can
see that all three NaCl losses are able to improve the standard performance upon their baselines.
Among the three losses, LVAR is generally the most successful in boosting the standard accuracy
when applied to SimCLR (i.e. 57.91% vs. 57.32% / 56.63%), whereas LMIXUP demonstrates a better
ability when applied to Debiased+HardNeg (i.e. 60.69% vs. 59.67% / 58.86%). Although LBIAS
does not bring more gain in the metric of standard accuracy compared with LMIXUP and LVAR, its
improved accuracy still suggests that the bias is causing harm to the original baseline and a bias
reduction scheme can do help to the training.
We have also validated the usefulness of NaCl with experiments on CIFAR10. That is, we train
the representation networks on CIFAR10, freeze the network, and fine-tune a fully-connected
layer that maps representations to outputs on CIFAR10. In HaoChen et al. (2021), LSimCLR
is reported to give 83.73% standard accuracy with 200 training epochs. As a comparison,
LIntNaCl(1, LMIXUP(g2,2,0.5), LRObUSt(g2, W)) gives 84.62% after 100 training epochs and 86.69%
after 200 training epochs, demonstrating a clear improvement over the baseline.
Accuracy on larger epochs. As training the representation with more epochs can also expose
the data to more augmentations, we carry out an additional experiments to compare the efficiency
and effectiveness of baseline methods with significant more training epochs. Specially, HaoChen
et al. (2021) has reported a LSimCLR CIFAR100 accuracy of 54.74% after 200 epochs, compared
to LVAR(g0, 2)’s 56.04% after 100 epochs. In our reproduction of the LSimCLR 200-epoch result,
we have witnessed an accuracy of 57.45% however at the cost of 1.34X training time (cf. 200
epochs with LSimCLR takes 211 mins vs. 100 epochs with LVAR(g0, 2) takes 158 mins). Additionally,
we see LSimCLR reaches 61.90% and Debiased+HardNeg stops at 62.74%, while LVAR(g0, 2) and
LVAR(g2, 2) improve upon them individually by reaching 62.37% and 63.51%. We refer the readers
to the appendix for detailed linear evaluation results on CIFAR100 with extended training epochs.
5.3	Evaluation of Model Robustness and Transferability
Robustness. In addition to the discriminative power, we also want to empower the learned rep-
resentation with strong adversarial performance. In Table 2, we list the classification accuracy on
CIFAR100 under FGSM attacks with magnitude = 0.002. From the table, one can see that, with
the absence of robustness promoting loss (α = 0), all NaCl methods manage to improve upon the
8
Under review as a conference paper at ICLR 2022
Table 2: The CIFAR100 adversarial evaluation results (%) of NaCl on LSimCLR, LDebiased+HardNeg, and
LIntCl (ours, cf. Equation 11). The best improvement over the individual baseline is in boldface.
M
2
3
4
5
LSimCLR : 25.17 ± 0.55
LVAR	LBIAS	LMIXUP
27.19±0.79	27.04±0.45	30.95±0.36
27.39±0.36	28.41±0.24	30.98±0.90
27.63±0.78	28.20±0.81	29.90±0.63
28.37±0.56	28.33±0.59	30.58±0.52
LDebiased+HardNeg : 31.03 ± 0.41
LVAR	LBIAS	LMIXUP
31.92±0.45^^32.50±0.48^^32.22±0.35
32.63±0.74	33.19±0.60	32.04±0.67
32.48±0.62	32.65±1.07	32.03±0.34
33.10±0.71	32.86±0.47	32.06±0.72
LIntCl : 40.05 ± 0.67
LVAR	LBIAS	LMIXUP
41.01±0.36	39.80±0.57	40.25±0.52
41.02±0.83	40.53±0.29	40.96±0.58
41.49±0.51	40.85±0.31	40.82±0.54
40.50±0.23	41.00±0.86	41.01±0.34
Table 3: The CIFAR10 transfer evaluation results (%) of NaCl on LSimCLR, LDebiased+HardNeg, and
LIntCl (ours, cf. Equation 11). The best improvement over the individual baseline is in boldface.
M	LSimCLR :76.34 ± 0.28
LVAR	LBIAS	LMIXUP
2--77.32±0.14^^77.40±0.14^^76.96±0.15
3	78.02±0.27	77.53±0.24	77.10±0.21
4	77.91±0.29	77.75±0.22	77.11±0.40
5	78∙09±0.29	77.93±0.40	77.04±0.19
LDebiased+HardNeg : 77.24 ± 0.29
LVAR	LBIAS	LMIXUP
77.43±0.18^^77.43±0.11 ~79.36±0.65
77.87±0.29	77.41±0.17	79.41±0.17
77.92±0.17	77.46±0.29	79.42±0.18
78.04±0.09	77.58±0.23	79.45±0.20
LIntCl : 76.39 ± 0.10
LVAR	LBIAS	LMIXUP
76.88±0.49	76.55±0.27	78.61±0.20
76.95±0.19	76.67±0.22	78.83±0.22
77.30±0.30	76.34±0.22	78.83±0.27
77.42±0.17	76.60±0.37	78.80±0.21
baselines. Notably, by referring to the full table in the appendix, one will see that LMIXUP with
λ = 0.9 boosts the CIFAR100 adversarial accuracy to 34.65% when applied to Debiased+HardNeg.
That said, although LVAR and LBIAS are both useful in enhancing the adversarial performance, LMIXUP
improves the baseline by the largest margin.
When we explicitly regularize the adversarial robustness performance (α 6= 0), the representation
network learned via LIntCl yields an adversarial accuracy of 40.05%. When we strengthen the loss
with NaCl. LIntNaCl, the adversarial performance is further improved.
Transferability. We validate the transferability of all the methods by fine-tuning a fully-connected
layer that maps representations to outputs on CIFAR10. This is in analogy to the evaluation procedure
in Chen et al. (2020a) - train a fixed feature extractor on a large-scale dataset and train a linear
classifier on top of the frozen base network with smaller-scale datasets. Table 3 shows that besides
decent standard and adversarial performance, NCA inspired losses can also improve the transferability.
We refer the readers to the appendix for the transfer robustness results.
Comparison to other methods. In Figure 1, we compare the standard and transfer accuracy of
various benchmark methods. Specially, we plot the adversarial accuracy defined under FGSM
attacks (Goodfellow et al., 2015) along the y-axis. In essence, one will want a representation network
that pushes the performance to the upper-right corner in the 2D accuracy grid (standard-adversarial
accuracy plot). From the figure, we see that SimCLR, Debiased, HardNeg, and Debiased+HardNeg
all score relatively poorly, obtaining an adversarial accuracy of around or below 30% on CIFAR100
and 50% on CIFAR10. One exception, Adv, performs adequately and reaches an accuracy of more
than 35% on CIFAR100 and 55% on CIFAR10, while sacrificing the standard accuracy. We highlight
the results of LIntNaCl and LIntCl in circles, through which we see that while LIntCl can already train
representations that are decently robust without sacrificing the standard accuracy on CIFAR100,
the transfer accuracy on CIFAR10 is inferior to some baselines (HardNeg and Debiased+HardNeg).
Comparatively, LIntNaCl wins over the baselines by a large margin on both datasets, proving the ability
of learning representation networks that also transfer robustness property. We refer the readers to
Figure S2 in the appendix for a detailed analysis of the effect of λ on these different metrics.
6	Conclusion
In this paper, we discover the relationship between contrastive loss and Neighborhood Component
Analysis (NCA), which motivates us to generalize the existing contrastive loss to a set of Neighbor-
hood analysis Contrastive losses (NaCl). We further propose a generic contrastive learning framework
based on NaCl, which learns representations that score high in both standard accuracy and adversarial
accuracy in downstream tasks. Future work includes addressing the current limitation of assuming
k = 1 for k-nearest neighbor in NCA to k > 1 (Tarlow et al., 2013), by doing which we expect to
extend the current framework to an even more general form.
9
Under review as a conference paper at ICLR 2022
References
Piotr Bojanowski and Armand Joulin. Unsupervised learning by predicting noise. In ICML, pp.
517-526. PMLR, 2017.
Mathilde Caron, Ishan Misra, Julien Mairal, Priya Goyal, Piotr Bojanowski, and Armand Joulin.
Unsupervised learning of visual features by contrasting cluster assignments. In NeurIPS, volume 33,
pp. 9912-9924, 2020.
Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. A simple framework for
contrastive learning of visual representations. In ICML, volume 119 of Proceedings of Machine
Learning Research, pp. 1597-1607, Virtual, 13-18 Jul 2020a. PMLR.
Xinlei Chen, Haoqi Fan, Ross B. Girshick, and Kaiming He. Improved baselines with momentum
contrastive learning. CoRR, abs/2003.04297, 2020b. URL https://arxiv.org/abs/2003.
04297.
Ching-Yao Chuang, Joshua Robinson, Lin Yen-Chen, Antonio Torralba, and Stefanie Jegelka. Debi-
ased contrastive learning. arXiv preprint arXiv:2007.00224, 2020.
Alexey Dosovitskiy, Philipp Fischer, Jost Tobias Springenberg, Martin Riedmiller, and Thomas Brox.
Discriminative unsupervised feature learning with exemplar convolutional neural networks. IEEE
transactions on pattern analysis and machine intelligence, 38(9):1734-1747, 2015.
Debidatta Dwibedi, Yusuf Aytar, Jonathan Tompson, Pierre Sermanet, and Andrew Zisserman. With
a little help from my friends: Nearest-neighbor contrastive learning of visual representations, 2021.
Jacob Goldberger, Geoffrey E Hinton, Sam Roweis, and Russ R Salakhutdinov. Neighbourhood
components analysis. NeurIPS, 17:513-520, 2004.
I. Goodfellow, J. Shlens, and C. Szegedy. Explaining and harnessing adversarial examples. In ICLR,
2015.
Sven Gowal, Po-Sen Huang, Aaron van den Oord, Timothy Mann, and Pushmeet Kohli. Self-
supervised adversarial robustness for the low-label, high-data regime. In ICLR, 2020.
Jean-Bastien Grill, Florian Strub, Florent Altche, Corentin Tallec, Pierre Richemond, Elena
Buchatskaya, Carl Doersch, Bernardo Avila Pires, Zhaohan Guo, Mohammad Gheshlaghi Azar,
Bilal Piot, koray kavukcuoglu, Remi Munos, and Michal Valko. Bootstrap your own latent - a new
approach to self-supervised learning. In NeurIPS, volume 33, pp. 21271-21284, 2020.
Michael Gutmann and Aapo Hyvarinen. Noise-contrastive estimation: A new estimation principle
for unnormalized statistical models. In Proceedings of the Thirteenth International Conference on
Artificial Intelligence and Statistics, pp. 297-304. JMLR Workshop and Conference Proceedings,
2010.
Jeff Z HaoChen, Colin Wei, Adrien Gaidon, and Tengyu Ma. Provable guarantees for self-supervised
deep learning with spectral contrastive loss. arXiv preprint arXiv:2106.04156, 2021.
Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick. Momentum contrast for
unsupervised visual representation learning. In CVPR, June 2020.
Chih-Hui Ho and Nuno Vasconcelos. Contrastive learning with adversarial examples. arXiv preprint
arXiv:2010.12050, 2020.
Rafal Jozefowicz, Oriol Vinyals, Mike Schuster, Noam Shazeer, and Yonghui Wu. Exploring the
limits of language modeling. arXiv preprint arXiv:1602.02410, 2016.
Harini Kannan, Alexey Kurakin, and Ian J. Goodfellow. Adversarial logit pairing. CoRR,
abs/1803.06373, 2018. URL http://arxiv.org/abs/1803.06373.
Prannay Khosla, Piotr Teterwak, Chen Wang, Aaron Sarna, Yonglong Tian, Phillip Isola, Aaron
Maschinot, Ce Liu, and Dilip Krishnan. Supervised contrastive learning. arXiv preprint
arXiv:2004.11362, 2020.
10
Under review as a conference paper at ICLR 2022
Minseon Kim, Jihoon Tack, and Sung Ju Hwang. Adversarial self-supervised contrastive learning.
arXiv preprint arXiv:2006.07589, 2020.
Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images. 2009.
Alexey Kurakin, Ian J Goodfellow, and Samy Bengio. Adversarial machine learning at scale. In
ICLR, 2016.
Kibok Lee, Yian Zhu, Kihyuk Sohn, Chun-Liang Li, Jinwoo Shin, and Honglak Lee. $i$-mix: A
domain-agnostic strategy for contrastive representation learning. In ICLR, 2021.
Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and Adrian Vladu.
Towards deep learning models resistant to adversarial attacks. In ICLR, 2018.
Andriy Mnih and Yee Whye Teh. A fast and simple algorithm for training neural probabilistic
language models. In ICML, 2012.
Seyed-Mohsen Moosavi-Dezfooli, Alhussein Fawzi, and Pascal Frossard. Deepfool: a simple and
accurate method to fool deep neural networks. In CVPR, pp. 2574-2582, 2016.
A. Nguyen, J. Yosinski, and J. Clune. Deep neural networks are easily fooled: High confidence
predictions for unrecognizable images. In CVPR, 2015.
Aaron van den Oord, Yazhe Li, and Oriol Vinyals. Representation learning with contrastive predictive
coding. arXiv preprint arXiv:1807.03748, 2018.
Joshua David Robinson, Ching-Yao Chuang, Suvrit Sra, and Stefanie Jegelka. Contrastive learning
with hard negative samples. In ICLR, 2021.
Nikunj Saunshi, Orestis Plevrakis, Sanjeev Arora, Mikhail Khodak, and Hrishikesh Khandeparkar. A
theoretical analysis of contrastive unsupervised representation learning. In ICML, pp. 5628-5637,
2019.
Kihyuk Sohn. Improved deep metric learning with multi-class n-pair loss objective. In NeurIPS,
volume 29, 2016.
C. Szegedy, W. Zaremba, I. Sutskever, J. Bruna, D. Erhan, I. Goodfellow, and R. Fergus. Intriguing
properties of neural networks. In ICLR, 2014.
Daniel Tarlow, Kevin Swersky, Laurent Charlin, Ilya Sutskever, and Rich Zemel. Stochastic k-
neighborhood selection for supervised and unsupervised learning. In ICML, 2013.
Yonglong Tian, Chen Sun, Ben Poole, Dilip Krishnan, Cordelia Schmid, and Phillip Isola. What
makes for good views for contrastive learning? In NeurIPS, volume 33, pp. 6827-6839, 2020.
Eric Wong, Leslie Rice, and J. Zico Kolter. Fast is better than free: Revisiting adversarial training. In
ICLR, 2020.
Zhirong Wu, Yuanjun Xiong, Stella X Yu, and Dahua Lin. Unsupervised feature learning via
non-parametric instance discrimination. In CVPR, pp. 3733-3742, 2018.
Huimin Zeng, Chen Zhu, Tom Goldstein, and Furong Huang. Are adversarial examples created
equal? a learnable weighted minimax risk for robustness under non-uniform attacks. arXiv preprint
arXiv:2010.12989, 2020.
Hongyi Zhang, Moustapha Cisse, Yann N. Dauphin, and David Lopez-Paz. mixup: Beyond empirical
risk minimization. In ICLR, 2018.
11