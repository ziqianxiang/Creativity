Under review as a conference paper at ICLR 2022
Reconstruction for disentanglement,	con-
TRAST FOR INVARIANCE
Anonymous authors
Paper under double-blind review
Ab stract
Disentangled and invariant representations are two critical goals of representa-
tion learning and many approaches have been proposed to achieve one of them.
However, those two goals are actually complementary to each other and we pro-
pose a framework to accomplish both of them simultaneously. We introduce a
weakly supervised signal to learn disentangled representation while using con-
trastive method to enforce representation invariance. Experimental evaluation
shows that the proposed method outperforms state-of-the-art methods on a number
of standard benchmarks.
1	Introduction
Deep neural networks (DNN) has achieved astonishing success in many computer vision tasks, such
as image classification (Deng et al., 2009), image generation (Goodfellow et al., 2014) and face
recognition (Schroff et al., 2015). In both prediction and generation tasks, learning to encode input
data x into a lower dimensional representation z is the critical first step that facilities down-steam
tasks (He et al., 2015; Kingma & Welling, 2014; van Steenkiste et al., 2019). For robust repre-
sentation learning, increasing generality and preventing overfitting are two fundamental challenges.
Typically, a DNN learns to encode representation which contains all factors of variation of data,
such as pose, expression, illumination, and angle for face recognition, as well as other nuisance
factors which may not have semantic meanings. Disentangled representation learning and invariant
representation learning are often used to address these challenges.
For disentangled representation learning, Bengio et al. (2014) define a disentangled representation
z, where a change in a given dimension zi corresponds to a change in one and only one underlying
factor of variation of the data. Although many unsupervised learning methods have been proposed
(Higgins et al., 2017; Burgess et al., 2018; Kim & Mnih, 2018), Locatello et al. (2019) have shown
both theoretically and empirically that the factor variants disentanglement is impossible without
supervision or inductive bias. To this end, recent works adopted the concept of semi-supervised
learning (Locatello et al., 2020b) and weakly supervised learning (Chen & Batmanghelich, 2020;
Locatello et al., 2020a). On the other hand, Jaiswal et al. (2018) take an invariant representation
learning perspective in which they split representation z into two parts z = [zp , zn], where zp only
contains predictive related information, and zn merely contains nuisance factors.
Invariant representation learning aims to learn to encode predictive latent factors which is invariant
to nuisance factors in inputs (Xie et al., 2017; Jaiswal et al., 2018; Ganin et al., 2016; Louizos
et al., 2016; Moyer et al., 2018; Sanchez et al., 2020b). By removing information of nuisance
factors, invariant representation learning achieves good performance when facing challenges like
adversarial attack (Chen et al., 2020) and out-of-distribution generalization (Arjovsky et al., 2020).
Further, invariant representation learning has also been studied in the reinforcement learning settings
(Hafner et al., 2019; Castro, 2020; Zhang et al., 2021).
Despite the success of both disentangled and invariant representation learning methods, the relation
between the two has not been thoroughly investigated. As shown in Figure 1.a, invariant repre-
sentation learning methods learn representations that maximize prediction accuracy, while leaving
the representations of both known and unknown confounding factors entangled. Meanwhile, as
illustrated in Figure 1.b, supervised disentangled representation methods cannot handle unknown
nuisance factors, which may hurt downstream prediction tasks. Based on this observation, we seek
1
Under review as a conference paper at ICLR 2022
Figure 1: Given an image with nuisance factors, (a): invariant representation learning splits pre-
dictive factors from all nuisance factors; (b): disentangled representation learning splits the known
nuisance factors but mixing predictive and unknown nuisance factors; (c): Our method splits all
predictive, known nuisance and unknown nuisance factors simultaneously.
to achieve disentanglement and invariance of representation simultaneously and propose anew train-
ing framework. To split the known nuisance factors znk from predictive zp and unknown nuisance
factors znu , we introduce weak supervision signals to achieve disentangled representation learning.
To make predictive factors zp independent of all nuisance factors zn , we introduce a new invariant
regularizer via reconstruction. The predictive factors from the same class are further aligned through
contrastive loss to enforce invariance. In summary our main contributions are:
•	We extend and combine both disentangled and invariant representation learning and pro-
pose a novel approach to robust representation learning.
•	We propose a novel approach that split the predictive, known nuisance factors and unknown
nuisance factors. Mutual independence of those factors is achieved by the reconstruction
step we use during training.
•	Our new model outperforms state-of-the-art models on both disentangling and invariance
tasks on standard benchmarks.
2	Related Work
Disentangled representation learning: Early works on disentangled representation learning aim
at learning disentangled latent factors z by implementing an autoencoder framework (Higgins et al.
(2017); Kim & Mnih (2018); Burgess et al. (2018)). Variational autoencoder (VAE) (Kingma &
Welling, 2014) is the basic framework used in most state-of-the-art disentanglement learning meth-
ods. VAE uses DNN to map the high dimension input x to low dimension representation z. The
latent representation Z is then mapped to high dimension reconstruction x. As shown in Equa-
tion (1), the overall objective function to train VAE is the evidence lower bounds (ELBO) of likeli-
hood logpθ(x1, x2, ...xn), which contains two parts: quality of reconstruction and Kullback-Leibler
divergence (DKL) between distribution qφ(z∣x) and the assumed prior p(z). Then, VAE uses the
negative of ELBO, LVAE = -ELBO, as loss function to update the parameters in the model.
ELBO = PiN=1 hEqφ(z∣x⑴)[lθgPθ(x(i)∣z)] - DκL(qφ(z∣X(i)∣∣p(z))] ≤ logPθ(xi,X2,…Xn)	(1)
Advanced methods based on VAE improve the disentanglement performance by implementing new
disentanglement regularization. β-VAE (Higgins et al., 2017) modified the original VAE by adding
a hyper-parameter β to balance the weights of reconstruction loss and DKL. When β > 1, the model
gains stronger disentanglement regularization. AnnealedVAE (Burgess et al., 2018) further studied
the effects of different value of β on reconstruction quality and disentangled representations. Based
on its finding, AnnealedVAE implemented a dynamic algorithm to change the β from large to small
value during training. FactorVAE (Kim & Mnih, 2018) proposes to use a discriminator in order to
2
Under review as a conference paper at ICLR 2022
Figure 2: Architecture of the model. Red box is the generation part and the blue box is the prediction
part
distinguish between the joint distribution of latent factors q(z) and multiplication of marginal distri-
bution of every latent factor q(zi). By using the discriminator, FactorVAE finds a better trade-off
between reconstruction quality and disentangled representation. Comparing to β-VAE, DIP-VAE
(Kumar et al., 2018) adds another regularization D(qφ(z)∣∣p(z)) between the marginal distribution
of latent factors qφ(z) = J qφ(z∣x)p(x)dx and the prior p(z) to further encourage disentangled
representation learning. D here can be any proper distance function. Chen et al. (2019) proposed
β-TCVAE which decomposed the DKL used in β-VAE into: total correlation, index-coded mutual
information and dimension-wise KL divergence. During training, the quality of reconstruction and
disentanglement are controlled by three different hyper-parameters applied on those three regular-
ization. To overcome the challenge proposed by Locatello et al. (2019), AdaVAE (Locatello et al.,
2020a) purposely choose pairs of inputs as supervision signal to learn representation disentangle-
ment. Besides, Locatello et al. (2020a) proved theoretically that under some assumptions, the ideal
representation disentanglement can be achieved without compromise.
Invariant Representation Learning: The methods that aim at learning invariant representation
can be classified into two groups: those methods that require annotations of nuisance factors (Li
et al., 2014; Louizos et al., 2016) and those that do not. A considerable number of approaches using
nuisance factors annotations have been recently proposed. By implementing a regularizer which
minimizes the Maximum Mean Discrepancy (MMD) (Gretton et al., 2007) on neural network (NN),
The NN+MMD approach (Li et al., 2014) removes affects of nuisance from predictive factors. The
Variational Fair Autoencoder (VFAE) (Louizos et al., 2016) uses special priors which encourage in-
dependence between nuisance factors and ideal invariant factors. Besides, VFAE also incorporates
MMD as the regularizer to further remove any dependencies. The Controllable Adversarial Invari-
ance (CAI) (Xie et al., 2017) approach applies the gradient reversal trick (Ganin et al., 2016) which
penalizes the model if latent representation has information of nuisance factors. CVIB (Moyer et al.,
2018) proposes a conditional form of Information Bottleneck (IB) and encourages the invariant rep-
resentation learning by optimizing its variational bounds. Approaches that need annotations are
suitable for removing specific affects of nuisance factors, such as race or gender bias in face recog-
nition. However, due to the constrains of demanding annotations, those methods take more effort to
pre-process the data and encounter challenges when the annotations are inaccurate or insufficient.
Comparing to annotation-needed approaches, annotation-free methods are easier to be implemented
in practice. The Unsupervised Adversarial Invariance (UAI) (Jaiswal et al., 2018) splits the latent
factors into factors useful for prediction and nuisance factors. UAI encourages the independence
of those two latent factors by incorporating competition between the prediction and the reconstruc-
tion objectives. Sanchez et al. (2020a) achieve invariant representation by using pairs of inputs and
applying a neural network based mutual information estimator to minimize the mutual information
between two shared representations. Furthermore, Sanchez et al. (2020a) employ a discriminator to
distinguish the difference between shared representation and nuisance representation.
3
Under review as a conference paper at ICLR 2022
3	Learning disentangled and invariant representation
3.1	Overview of Model Architecture
As illustrated in Figure 2, the architecture of the proposed model contains two components: a genera-
tion module and a prediction module. Similar to VAE, the generation module performs an encoding-
decoding task. However, it encodes the input x into latent factors z, where z = [zp , zn], where zp
is the latent predictive factors that contains useful information for the prediction task, whereas zn
is the latent nuisance factors that can be further divided into known latent factors znk and unknown
nuisance factors znu . znk are discovered and separated from zn via weakly supervised disentan-
gled representation learning, where the joint distribution p(znk) = i p(znki). Since zn is the split
containing nuisance factor, after znk is separated, the remaining factors of zn naturally result in un-
known nuisance factors znu . Then, zp and zn are concatenated for generating a reconstruction xrec
which are used to measure the quality of reconstruction and disentanglement degree. To enforce the
independence between zp and zn , we add a regularizer using another reconstruction task, where the
average mean and variance of predictive factors Zp are USed to form new latent factors zp as it will
be discussed in Section 3.3. In the prediction module, we incorporate contrastive loss to cluster the
predictive latent factors from the same class.
3.2	LEARNING INDEPENDENT KNOWN NUISANCE FACTORS znk
The known nUisance factors znk are discovered and separated from zn, where p(znk) = Qip(znki),
becaUse nUisance information is expected to be present only in zn. As illUstrated in FigUre 1, since
the goal of sUch procedUre is learning disentangled representation, we follow the evalUation protocol
in previoUs disentangled representation works for estimating the performance (Higgins et al., 2017;
Kim & Mnih, 2018; KUmar et al., 2018; ShU et al., 2020).
To fUlfill the reqUirement of inclUding sUpervision signal for disentangled representation learning
as proven in (Locatello et al., 2019), we Use selected pairs of inpUts x(l) and x(m) as sUpervision
signals, where only a few common generative factors are shared. As illUstrated in FigUre 3, dUring
training, the network encodes a pair of inpUts x(l) and x(m) into two latent factors z(l) = [zp(l) , zn(l)]
and z(m) = [zp(m), zn(m)] respectively, which are then decoded to reconstrUct x(rle)c and x(rmec). To en-
coUrage representation disentanglement, certain elements of zn(l) and zn(m) are detected and swapped
to generate two new corresponding latent factors Z(I) and Z(M). The two new latent factors are then
decoded to new reconstructions Xrec and Xrm). By comparing Xrec with Xrec, the known nuisance
factors Znk are discovered and the elements of Znk are enforced to be disentangled with each other.
Figure 3: Disentanglement representation learning for known nuisance factors Znk
Selecting image pairs for training and latent factors assumptions: As mentioned by Higgins
et al. (2017), the true world simulator using generative factors to generate X can be modeled as:
p(X|v, w) = Sim(v, w), where v is the generative factors and w is other confounding factors. In-
spired by this, we choose pairs of image by randomly selecting some generative factors to be the
same and keeping the value of other generative factors to be random. As the image pair exam-
ple in Figure 4b indicates, each image X has corresponding generative factors v, and the training
4
Under review as a conference paper at ICLR 2022
(a) In early training stages, small number of latent fac-
tors are swapped. the number of latent factors to be
swapped increases gradually.
Figure 4: Two training strategies for learning known nuisance factors znk
[3,1,3,1,0,7]	[3,1,3,1,5,7]
Only one distinct generative factor
[lt2t2t0f3t12]	[2,2,1,5,1,10]
Multiple distinct generative factor
(b) In early training stages, input pair with small num-
ber of generative factors are chosen. The number of
generative factors increases gradually.
pair is generated as follows: we first randomly select a sample x(l) whose generative factors are
v(l) = [v1, v2, ...vn]. We then randomly change the value of k elements in v(l) to form a new gen-
erative factors v(m) and choose another sample x(m) according to v(m) . During training, indices
of different generative factors between v(l) and v(m) , and the groundtruth value of all generative
factors are not available to the model. The model is weakly supervised since it is trained with only
the knowledge of the number of factors k that have changed. Ideally, if the model can learn a dis-
entangled representation, the model will encode the image pair x(l) and x(m) to the corresponding
representations z(l) and z(m) which have the characteristic shown in Equation (2). We annotate the
set of all different elements between z(l) and z(m) to be dfz and set of all latent factors to be dz such
that dfz ⊆ dz .
P(Znj ∣χ(ι))=P(Znm) Ixg))； J ∈f	⑵
p(zn(li)|x(l)) 6= p(zn(mi )|x(m)); i ∈ dfz
Detecting and Swapping the distinct latent factors In VAE, it is commonly assumed that the
posterior distribution of latent factors is a factorized multivariate Gaussian, p(z∣x) = qθ(z∣x)
(Kingma & Welling, 2014) and the reparameterization trick is used to make the posterior differ-
entiable. By this assumption, we can directly measure the mutual information between the corre-
sponding dimensions of the two latent representations Z(l) and Z(m) by measuring the divergence
(Dm), which can be either KL divergence (DKL) or Jensen-Shannon divergence (JSD). We show
the process of detecting distinct latent factors in Equations (3) and (4), where a larger value of DKL
or JSD implies higher difference between the two corresponding latent factor distributions.
DKL(CIeh(Z(I) |x(l))∣∣qcb(z(m |x(m))) = (σn?)) + (μn - μn)尸 + ιo∏-(σn)) - 1	(3)
DKL (qφ (Zni |x 川qφ (Zni |x )) =	2( (m))2	+ lOg( (l) ) ?	(3)
JSD(qφ(Zni) Ix(I))∣∣qφ(Znm) ∣x(m))) = | (DKL(QΦ(Zni) Ix(I))||M) + DKL(qφ(Znm)|x(m))I∣M))
where M =;® (Zni) Ix(I))+ qφ (Znm) Ix(m)))	(4)
Since the model only has the knowledge of the number of different generative factors k, we swap
all corresponding dimension elements of Zn(l) and Zn(m) except the top k highest Dm value elements.
We incorporate this swapping step to create two new latent representations Zn) and Znmm shown in
Equation (5).
Z(l)	Z(m) Z(m)	Z(l) i ∈/ df
Zni	= Zni	; Zni	= Zni ;	i ∈/ dfz	5
Z(I)	= z(l)∙	Z(m)= z(m)∙	J ∈ df	(5)
Znj = Znj ; Znj = Znj ; j ∈ fz
Disentangled representation loss function: After we obtain Zn) and ^nm), they are concatenated
with Zpm) and z(), respectively, to generate two new latent representations Z(I) = [Zpm') ,Znl] and
Z(m) = [Zpl),Z(m)]. Z(I) and Z(m) are decoded into new reconstructions X(I) and X(m). Reminding
5
Under review as a conference paper at ICLR 2022
that there are only k different generative factors between pair of images, ideally, if the model per-
fectly detects the top k most different latent factors, by swapping other latent factors except them,
the new representations z(l) and z(m) are same with the original representations z(l) and z(m). Ac-
cordingly, the new reconstructions Xre)C and Xrm) should be identical to the original reconstruction
x(rle)c and x(rmec). Therefore, we design the disentangled representation loss in Equation (6), where D
can be any suitable distance function e.g, mean square error (MSE) or binary cross-entropy (BCE).
L = LVAE(XrD + LVAE(Xrm),z(m)) + D(XQ, Xa) + D(Xrm),xrm))	(6)
Training Strategies for disentangled representation learning: To further improve the perfor-
mance of disentangled representation learning, we design two strategies: warmup by amount and
warmup by difficulty. Remembering that in swapping step, the model needs to swap |dz | - k ele-
ments of latent representations. Thus, in early stages of training, exchanging too many latent factors
will easily lead to mistakes. Therefore, in the first strategy, we gradually increase the number of
latent factors being swapped from 1 to |dz | - k during training. Further, to smoothly increase the
training difficulty, we set the number of different generative factors to be 1 in the beginning and in-
crease the number of different generative factors as training progresses. Those two training strategies
highly improve the performance of the disentanglement learning and they are illustrated in Figure 4.
3.3	LEARNING INVARIANT PREDICTIVE FACTORS zp
After we obtain the disentangled representation znk, the predictive factors zp may still be entangled
with znu. Therefore, we need to add other constraints to achieve fully invariant representation of zp.
Making zp independent of zn: As proved by Locatello et al. (2019), supervision signals need to
be introduced for disentangled representation. Similarly, the independence of zp and zn also needs
the help from a supervision signals as we discuss in Appendix A.1. Luckily, for supervised training,
a batch of samples naturally contains supervision signal. Similar to Equation (2), the distribution of
the representations zp should be same for same class and can be expressed as Equation (7) where
C(X(l)) means the class of sample X(l).
p(zp(l)|x(l)) =p(zp(m)|x(m)); C(X(l)) =C(X(m))
p(zp(l)|x(l)) 6=p(zp(m)|x(m)); C(X(l)) 6=C(X(m))
(7)
Similar to the method we use for disentangled representation learning, we generate new latent rep-
resentation Zp and its corresponding reconstruction Xrec-p. Then, we enforce the disentanglement
between Zp and Zn by comparing the new reconstruction X- and Xrec. In contrast to the swap-
ping method mentioned in Section 3.2, since the samples batch used for training usually contains
more than two (2) samples from the same class, the swapping method is hard to be implemented
in this situation. Therefore, we generate the new latent representations Zp by calculating the aver-
age mean μp and average variance Vp of the latent representations from the same class as shown in
Equation (8).
Zp = N(μp, Vp); Xrec—p = Decoder([zp, zn])
μp = |cC| X μpi); vzp = ∣c1∣ X Vpi; where ∀i ∈ C
(8)
We then generate the new reconstruction XZrec-p using the same decoder as in other reconstruction
tasks and enforce the disentanglement ofZp and Zn by calculating the D(Xrec, XZrecp) and update the
parameters of the model according to its gradient.
Constrastive feature alignment: To achieve invariant representation, we need to make sure the
latent representation that is useful for prediction can also be clustered according to their correspond-
ing classes. Even though the often used cross-entropy (CE) loss can accomplish similar goals, the
direct goal of CE loss is to achieve logit-level alignment and change the representations distribution
according to the logits, which does not guarantee the uniform distribution of features. Alternatively,
we incorporate contrastive methods to assure that representation/feature alignment can be accom-
plished effectively (Wang & Liu, 2021).
6
Under review as a conference paper at ICLR 2022
Similar to Khosla et al. (2020), we use supervised contrastive loss to achieve feature alignment and
cluster the representations zp according to their classes as shown in Equation (9) where C is the set
that contains samples from the same class and yp
Lsup
X -C1| X log
i∈I	p∈C
yi .
exp (zi ∙ Zp∕τ)
P exp (zi ∙ za∕τ)
a∈A(i)
(9)
The final loss function used to train the model after adding the standard cross-entropy(CE) loss to
train the classifier, is given by Equation (10).
L = LCE (x, y) + LVAE + αLdisentangle + βLSup + γLZp
Ldisentangle
D(XMx 归 + D(Xrm),xrm)); LZp = D(Xrec-P ,Xrec)
(10)
4	Experiments Evaluation
4.1	Benchmark Datasets, Baseline Models and Metrics
Since our model learns disentangled and invariant representation simultaneously, our method are
evaluated using multiple metrics and datasets. We evaluate the performance of our model on follow-
ing four (4) datasets with different underlying factor of variations.
•	Colored-MNIST Colored-MNIST dataset is augmented version of MNIST (LeCun &
Cortes, 2010) with two known nuisance factors: digit color and background color. Dur-
ing training, the background color is chosen from 3 colors and digit color is chosen from
other 6 colors. In test, we set the background color into 3 new colors which is different
from training set.
•	Rotation-Colored-MNIST This dataset is an further augmented version of Colored-
MNIST. The background color and digit color setting is the same with the Colored-MNIST
and this dataset further contains digits rotated to angles θ ∈ Θtrain = {0, ±22.5, ±45}.
For test data, the rotation angles for digit is set to θ ∈ Θtest = {0, ±65, ±75}. The rotation
angles are set to be unknown nuisance factors.
•	3dShapes (Burgess & Kim, 2018) 3dShapes contains 480,000 RGB 64 × 64 × 3 images
and the whole dataset has 6 different generative factors: object shape, object scale, object
color, wall color, floor color and scene orientation. We choose object shape (4 classes) as
the prediction task and only half of object colors during training and the remaining half of
object color is used to evaluate performance of invariant representation.
•	MPI3D (Gondal et al., 2019) MPI3D is a real-world dataset contains 1,036,800 RGB im-
ages and the whole dataset has 7 generative factors: object color, object shape, object
size, camera height, background color, horizontal degree of robotic arm, vertical degree of
robotic arm. Like 3dShapes, we choose object shape (6 classes) as the prediction target and
half of object colors for training.
Table 1: Test average and worst accuracy results on Colored-MNIST, 3dShapes and MPI3D. Bold,
Black: best result
Models	Colored-MNIST Avg Acc Worst Acc		3dShapes Avg Acc Worst Acc		MPI3D Avg Acc Worst Acc	
Baseline	0.9512	0.6617	0.9887	0.9689	0.9012	0.8789
β-VAE	0.9265	0.5879	0.9866	0.9577	0.8698	0.8489
VFAE	0.9312	0.6554	0.9772	0.9334	0.8669	0.8243
CAI	0.9356	0.6317	0.9762	0.9432	0.8663	0.8216
CVIB	0.9331	0.7012	0.9711	0.9446	0.8704	0.8561
UAI	0.9474	0.7425	0.9713	0.9521	0.8789	0.8301
Our model	0.9796	0.9043	0.9852	0.9763	0.9132	0.8917
Since our method aims at achieving both disentanglement and invariance at the same time, we cate-
gorize the baseline state-of-the-art models into two groups according their goals.
7
Under review as a conference paper at ICLR 2022
Table 2: Test average accuracy and worst accuracy results on Rotation-Colored-MNIST with differ-
ent rotation angles. Bold, Black: best result
Models	Avg Acc	Worst Acc -75	Avg Acc	Rotation-Colored-MNIST			Avg Acc Worst Acc +75	
				Worst Acc -65	Avg Acc Worst Acc +65			
Baseline	0.770	0.623	0.897	0.775	0.858	0.658	0.683	0.499
β-VAE	0.771	0.612	0.852	0.750	0.834	0.630	0.687	0.472
VFAE	0.722	0.589	0.858	0.744	0.841	0.646	0.717	0.48
CAI	0.749	0.593	0.865	0.773	0.842	0.678	0.647	0.429
CVIB	0.761	0.592	0.886	0.791	0.856	0.688	0.722	0.534
UAI	0.780	0.611	0.888	0.800	0.854	0.682	0.702	0.511
Our model	0.810	0.753	0.908	0.857	0.873	0.823	0.732	0.633
•	Representation Disentanglement: The following state-of-the-art model are used to com-
pare performance of disentangled representation — (1) β-VAE (Higgins et al., 2017), (2)
AnnealedVAE (Burgess et al., 2018), (3) FactorVAE (Kim & Mnih, 2018), (4) DIP-VAE-
I (Kumar et al., 2018), (5) DIP-VAE-II (Kumar et al., 2018), (6) β-TCVAE (Chen et al.,
2019) and (7) Ada-VAE (Locatello et al., 2020a).
•	Representation Invariance: The following state-of-the-art model are used for invariant
representation comparisons: (1) VFAE (Louizos et al., 2016), (2) CAI (Xie et al., 2017),
(3) CVIB (Moyer et al., 2018), and (4) UAI (Jaiswal et al., 2018).
We adopt the following metrics to evaluate the performance of disentangled representation. All met-
rics range from 0 to 1, where 1 indicates that the latent factors are fully disentangled — (1) Mutual
Information Gap (MIG) (Chen et al., 2019) which evaluates the gap of top two highest mutual in-
formation between a latent factors and generative factors. (2) Separated Attribute Predictability
(SAP) (Kumar et al., 2018) which measures the mean of the difference of perdition error between
the top two most predictive latent factors. (3) Interventional Robustness Score (IRS) (Suter et al.,
2019) which evaluates reliance of a latent factor solely on generative factor regardless of other gen-
erative factors. (4) FactorVAE (FVAE) score (Kim & Mnih, 2018) which implements a majority
vote classifier to predict the index of a fixed generative factor and take the prediction accuracy as
the final score value. (5) DCI-Disentanglement (DCI) (Eastwood & Williams, 2018) which calcu-
lates the entropy of the distribution obtained by normalizing among each dimension of the learned
representation for predicting the value of a generative factor.
To evaluate the performance of invariant representation learning, the test accuracy is the metric to
evaluate the performance of models. Furthermore, we record both average test accuracy and worst-
case test accuracy which was suggested by Sagawa* et al. (2020). During the experiment, we found
out that using Equation (10) directly does not guarantee good performance. This may be caused
by inconsistent behavior of CE loss and supervised contrastive loss. Thus, we separately train the
classifier using CE loss and use remaining part of total loss to train the rest of the model.
4.2	Comparison with Previous Work
We show invariance learning results which are the test average accuracy and worst accuracy in
Tables 1 and 2. For Color-Rotation-MNIST dataset, since we rotate the test samples with θ ∈
Θtest = {±65, ±75} and those angles are different with training rotation angles θ ∈ Θtrain =
{0, ± - 22.5, ±45}, we record each test average accuracy and worst accuracy under each rotation
angles. The baseline model is the regular CNN model with no extra components for representation
invariance, and β-VAE in Tables 1 and 2 is the regular CNN model with β-VAE structure to do
reconstruction at the same time. Our proposed model significantly outperforms prior work.
To compare the performance of disentanglement, we show the results of disentanglement metrics
on different datasets in Table 3. For divergence measurement Dm for latent factor, we test both
DKL and JSD, and choose the best results. As shown, our approach surpass both unsupervised and
weakly-supervised method in almost all metrics.
8
Under review as a conference paper at ICLR 2022
Table 3: Disentanglement metrics on 3dShapes and MPI3D. Bold, Black: best result
Models	MIG	SAP	3dShapes IRS	FVAE	DCI	MIG	SAP	MPI3D IRS	FVAE	DCI
Unsupervised Disentanglement Leanring										
β-VAE	0.194	0.063	0.473	0.847	0.246	0.135	0.071	0.579	0.369	0.317
AnnealedVAE	0.233	0.087	0.545	0.864	0.341	0.098	0.038	0.490	0.397	0.228
FactorVAE	0.224	0.0440	0.630	0.792	0.304	0.092	0.031	0.529	0.379	0.164
DIP-VAE-I	0.143	0.026	0.491	0.761	0.137	0.104	0.073	0.476	0.491	0.223
DIP-VAE-II	0.137	0.020	0.424	0.742	0.083	0.131	0.075	0.509	0.544	0.244
β-TCVAE	0.364	0.096	0.594	0.970	0.601	0.189	0.146	0.636	0.430	0.322
β-FactorTCVAE	0.071	0.021	0.496	0.612	0.131	0.066	0.034	0.493	0.464	0.217
Weakly-Supervised Disentanglement Learning										
Ada-ML-VAE	0.509	0.127	0.620	0.996	0.940	0.240	0.074	0.576	0.476	0.285
Ada-GVAE	0.569	0.150	0.708	0.996	0.946	0.269	0.215	0.604	0.589	0.401
Our model	0.716	0.156	0.784	0.996	0.919	0.486	0.225	0.615	0.565	0.560
4.3	Ablation Study
Effectiveness of training strategies in disentanglement learning: To prove the effectiveness of
the training strategies illustrated in Figure 4, we compare results of three situations: (1) none of
those strategies is used, (2) only warmup by amount strategy is used, and (3) both strategies are
used. As shown in Table 4, using both training strategies clearly outperforms the others.
Separately training the classifier and the rest of the model: To prove the importance of the
two-step training as mentioned in Section 4.1, we compare the results of training the entire model
together versus separately training classifier and other parts. Further, we also record the results of our
model which does not use contrastive loss for feature-level alignment. As shown in Table 5, either
only using CE loss (LCE) or training the whole together (LCE + Lcontrastive) will harm the per-
formance of the model. By separately training the classifier and other parts (LCE ÷→ Lcontrastive),
the framework has the best results for both average and worst accuracy.
Table 4: Disentanglement metrics of with different training strategies applied to 3dShapes
warmup by amount	warmup by difficulty	MIG	SAP	IRS	FVAE	DCI
		0.492	0.096	0.661	0.902	0.697
X		0.512	0.126	0.674	0.944	0.781
X	X	0.716	0.156	0.784	0.996	0.919
Table 5: Performance of different scheme on Colored-MNIST and Rotation-Colored-MNIST
Training Scheme	Colored-MNIST		Rotation-Colored-MNIST (65)	
	avg acc	worst acc	avg acc	worst acc
LCE	0.932	^^0.680	0.821	0.653
LCE + Lcontrastive	0.935	0.732	0.842	0.678
LCE K~→ Lcontrastive	0.980	0.904	0.873	0.823
5	Conclusion
In this work, we extend the ideas of representation disentanglement and representation invariance
by combining them to achieve both goals at the same time. Further, we propose a new framework
for weakly supervised disentanglement representation learning and achieve better performance than
state-of-the-art disentangled learning methods. By introducing contrastive loss and new invariant
regularization loss, we make predictive factor zp to be more invariant to nuisance and increase both
average and worst accuracy on invariant learning tasks.
9
Under review as a conference paper at ICLR 2022
References
Martin Arjovsky, Leon BottoU,Ishaan Gulrajani, and David Lopez-Paz. Invariant risk minimization,
2020.
Yoshua Bengio, Aaron Courville, and Pascal Vincent. Representation learning: A review and new
perspectives, 2014.
Chris Burgess and Hyunjik Kim. 3d shapes dataset. https://github.com/deepmind/3dshapes-dataset/,
2018.
Christopher P. Burgess, Irina Higgins, Arka Pal, Loic Matthey, Nick Watters, Guillaume Desjardins,
and Alexander Lerchner. Understanding disentangling in β-vae, 2018.
Pablo Samuel Castro. Scalable methods for computing state similarity in deterministic markov
decision processes. In The Thirty-Fourth AAAI Conference on Artificial Intelligence, AAAI 2020,
The Thirty-Second Innovative Applications of Artificial Intelligence Conference, IAAI 2020, The
Tenth AAAI Symposium on Educational Advances in Artificial Intelligence, EAAI 2020, New York,
NY USA, February 7-12, 2020, pp.10069-10076. AAAI Press, 2020. URL https://aaai.
org/ojs/index.php/AAAI/article/view/6564.
Jiawei Chen, Janusz Konrad, and Prakash Ishwar. A cyclically-trained adversarial network for
invariant representation learning. In 2020 IEEE/CVF Conference on Computer Vision and
Pattern Recognition, CVPR Workshops 2020, Seattle, WA, USA, June 14-19, 2020, pp. 3393-
3402. Computer Vision Foundation / IEEE, 2020. doi: 10.1109/CVPRW50498.2020.00399.
URL https://openaccess.thecvf.com/content_CVPRW_2020/html/w47/
Chen_A_Cyclically-Trained_Adversarial_Network_for_Invariant_
Representation_Learning_CVPRW_2020_paper.html.
Junxiang Chen and Kayhan Batmanghelich. Weakly supervised disentanglement by pairwise simi-
larities. In The Thirty-Fourth AAAI Conference on Artificial Intelligence, AAAI 2020, The Thirty-
Second Innovative Applications of Artificial Intelligence Conference, IAAI 2020, The Tenth AAAI
Symposium on Educational Advances in Artificial Intelligence, EAAI 2020, New York, NY, USA,
February 7-12, 2020, pp. 3495-3502. AAAI Press, 2020. URL https://aaai.org/ojs/
index.php/AAAI/article/view/5754.
Ricky T. Q. Chen, Xuechen Li, Roger Grosse, and David Duvenaud. Isolating sources of disentan-
glement in variational autoencoders, 2019.
Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hier-
archical image database. In 2009 IEEE Conference on Computer Vision and Pattern Recognition,
pp. 248-255, 2009. doi: 10.1109/CVPR.2009.5206848.
Cian Eastwood and Christopher K. I. Williams. A framework for the quantitative evaluation of
disentangled representations. In International Conference on Learning Representations, 2018.
URL https://openreview.net/forum?id=By-7dz-AZ.
Yaroslav Ganin, Evgeniya Ustinova, Hana Ajakan, Pascal Germain, Hugo Larochelle, Francois
Laviolette, Mario Marchand, and Victor Lempitsky. Domain-adversarial training of neural net-
works. The Journal of Machine Learning Research, 17(1):2096-2030, 2016.
Muhammad Waleed Gondal, Manuel Wuthrich, Dorde Miladinovic, Francesco Locatello, Martin
Breidt, Valentin Volchkov, Joel Akpo, Olivier Bachem, Bernhard Scholkopf, and Stefan Bauer.
On the transfer of inductive bias from simulation to the real world: anew disentanglement dataset,
2019.
Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sher-
jil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial nets. In
Z. Ghahramani, M. Welling, C. Cortes, N. Lawrence, and K. Q. Weinberger (eds.),
Advances in Neural Information Processing Systems, volume 27. Curran Associates,
Inc., 2014. URL https://proceedings.neurips.cc/paper/2014/file/
5ca3e9b122f61f8f06494c97b1afccf3- Paper.pdf.
10
Under review as a conference paper at ICLR 2022
Arthur Gretton, Karsten M. Borgwardt, Malte Rasch, Bernhard SchOlkopf, and Alex J. Smola. A
kernel method for the two-sample-problem. In B. Scholkopf, J. C. Platt, and T. Hoffman (eds.),
Advances in Neural Information Processing Systems 19, pp. 513-520. MIT Press, 2007.
Danijar Hafner, Timothy Lillicrap, Ian Fischer, Ruben Villegas, David Ha, Honglak Lee, and James
Davidson. Learning latent dynamics for planning from pixels. In International Conference on
Machine Learning, pp. 2555-2565, 2019.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recog-
nition, 2015.
Irina Higgins, LOic Matthey, Arka Pal, Christopher Burgess, Xavier Glorot, Matthew Botvinick,
Shakir Mohamed, and Alexander Lerchner. beta-vae: Learning basic visual concepts with a
constrained variational framework. In 5th International Conference on Learning Representations,
ICLR 2017, Toulon, France, April 24-26, 2017, Conference Track Proceedings. OpenReview.net,
2017. URL https://openreview.net/forum?id=Sy2fzU9gl.
Ayush Jaiswal, Rex Yue Wu, Wael Abd-Almageed, and Prem Natarajan. Unsupervised adversar-
ial invariance. In S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and
R. Garnett (eds.), Advances in Neural Information Processing Systems 31, pp. 5097-5107. Cur-
ran Associates, Inc., 2018.
Prannay Khosla, Piotr Teterwak, Chen Wang, Aaron Sarna, Yonglong Tian, Phillip Isola, Aaron
Maschinot, Ce Liu, and Dilip Krishnan. Supervised contrastive learning. CoRR, abs/2004.11362,
2020. URL https://arxiv.org/abs/2004.11362.
Hyunjik Kim and Andriy Mnih. Disentangling by factorising. In Jennifer Dy and Andreas Krause
(eds.), Proceedings of the 35th International Conference on Machine Learning, volume 80 of Pro-
ceedings of Machine Learning Research, pp. 2649-2658, Stockholmsmaissan, Stockholm Swe-
den, 10-15 Jul 2018. PMLR. URL http://proceedings.mlr.press/v80/kim18b.
html.
Diederik P. Kingma and Max Welling. Auto-encoding variational bayes. In Yoshua Bengio and Yann
LeCun (eds.), 2nd International Conference on Learning Representations, ICLR 2014, Banff, AB,
Canada, April 14-16, 2014, Conference Track Proceedings, 2014. URL http://arxiv.org/
abs/1312.6114.
Abhishek Kumar, P. Sattigeri, and A. Balakrishnan. Variational inference of disentangled latent
concepts from unlabeled observations. ICLR, abs/1711.00848, 2018.
Yann LeCun and Corinna Cortes. MNIST handwritten digit database. 2010. URL http://yann.
lecun.com/exdb/mnist/.
Yujia Li, Kevin Swersky, and Richard Zemel. Learning unbiased features. arXiv preprint
arXiv:1412.5244, 2014.
Francesco Locatello, Stefan Bauer, Mario Lucic, Gunnar Raitsch, Sylvain Gelly, Bernhard
Schoi lkopf, and Olivier Bachem. Challenging common assumptions in the unsupervised learn-
ing of disentangled representations, 2019.
Francesco Locatello, Ben Poole, Gunnar Raetsch, Bernhard Schoi lkopf, Olivier Bachem, and
Michael Tschannen. Weakly-supervised disentanglement without compromises. In Hal DaUme
III and Aarti Singh (eds.), Proceedings of the 37th International Conference on Machine Learn-
ing, volume 119 of Proceedings of Machine Learning Research, pp. 6348-6359. PMLR, 13-18
Jul 2020a. URL http://proceedings.mlr.press/v119/locatello20a.html.
Francesco Locatello, Michael Tschannen, Stefan Bauer, Gunnar Raitsch, Bernhard Schoi lkopf, and
Olivier Bachem. Disentangling factors of variations using few labels. In International Confer-
ence on Learning Representations, 2020b. URL https://openreview.net/forum?id=
SygagpEKwB.
Christos Louizos, Kevin Swersky, Yujia Li, Max Welling, and Richard Zeme. The variational fair
autoencoder. In Proceedings of International Conference on Learning Representations, 2016.
11
Under review as a conference paper at ICLR 2022
Daniel Moyer, Shuyang Gao, Rob Brekelmans, Aram Galstyan, and Greg Ver Steeg. Invariant rep-
resentations without adversarial training. In S. Bengio, H. Wallach, H. Larochelle, K. Grauman,
N. Cesa-Bianchi, and R. Garnett (eds.), Advances in Neural Information Processing Systems 31,
pp. 9102-911LCUrran Associates, Inc., 2018.
Shiori Sagawa*, Pang Wei Koh*, Tatsunori B. Hashimoto, and Percy Liang. Distributionally robust
neUral networks. In International Conference on Learning Representations, 2020. URL https:
//openreview.net/forum?id=ryxGuJrFvS.
E. Sanchez, M. SerrUrier, and M. Ortner. Learning disentangled representations via mUtUal informa-
tion estimation. In ECCV, 2020a.
EdUardo HUgo Sanchez, MathieU SerrUrier, and Mathias Ortner. Learning disentangled representa-
tions via mUtUal information estimation. In Andrea Vedaldi, Horst Bischof, Thomas Brox, and
Jan-Michael Frahm (eds.), Computer Vision - ECCV2020, pp. 205-221, Cham, 2020b. Springer
International PUblishing. ISBN 978-3-030-58542-6.
Florian Schroff, Dmitry Kalenichenko, and James Philbin. Facenet: A Unified embedding for face
recognition and clUstering. In 2015 IEEE Conference on Computer Vision and Pattern Recogni-
tion (CVPR), pp. 815-823, 2015. doi: 10.1109/CVPR.2015.7298682.
RUi ShU, Yining Chen, Abhishek KUmar, Stefano Ermon, and Ben Poole. Weakly sUpervised dis-
entanglement with gUarantees. In International Conference on Learning Representations, 2020.
URL https://openreview.net/forum?id=HJgSwyBKvr.
Raphael Suter, Dorde Miladinovic, Bernhard Scholkopf, and Stefan Bauer. Robustly disentangled
caUsal mechanisms: Validating deep representations for interventional robUstness, 2019.
Sjoerd van Steenkiste, Francesco Locatello, JUrgen Schmidhuber, and Olivier Bachem. Are disen-
tangled representations helpful for abstract visual reasoning? CoRR, abs/1905.12506, 2019. URL
http://arxiv.org/abs/1905.12506.
Feng Wang and Huaping Liu. Understanding the behaviour of contrastive loss. In Proceedings of
the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 2495-2504,
June 2021.
Qizhe Xie, Zihang Dai, Yulun Du, Eduard Hovy, and Graham Neubig. Controllable invariance
through adversarial feature learning. In I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fer-
gus, S. Vishwanathan, and R. Garnett (eds.), Advances in Neural Information Processing Systems
30, pp. 585-596. Curran Associates, Inc., 2017.
Amy Zhang, Rowan Thomas McAllister, Roberto Calandra, Yarin Gal, and Sergey Levine. Learn-
ing invariant representations for reinforcement learning without reconstruction. In International
Conference on Learning Representations, 2021. URL https://openreview.net/forum?
id=-2FCwDKRREu.
12
Under review as a conference paper at ICLR 2022
A	Appendix
A.1 Why reconstruction job is necessary for independent predictive factors
This proof is largely dependent on proof used in Locatello et al. (2019) and the proof can be shown
below:
By the assumption, we hope the latent factors can be separated into two part zp and zn . zp and zn
are expected to be independent to each other. We have :
P(Z) = P(Zp) ∙ P(Zn)
Thus, if we choose any latent factor zi from zp , it should be independent to any other latent factor
Zj chosen from Zn . Zi ⊥⊥ Zj .
It can be claimed that there exists an infinite family of bijective functions f : suPP(Z) -→ suPP(Z)
such that dfi(u) = 0 almost everywhere for all i in Zp and j from Zn (i.e., Z and f (Z) are completely
entangled) and P (Z ≤ u) = P (f(Z) ≤ u) for all u ∈ suPP(Z) (i.e., they have the same marginal
distribution). Since the unsupervised method only has access to observations x and y, it hence
cannot distinguish between the two equivalent generative models and thus has to be entangled to at
least one of them.
We first choose two bijective functions gi(vi) = P(Zi ≤ vi) and gj (vj) = P (Zj ≤ vj). By con-
StrUction g(Z) = g©) ∙ g(Zj) is a 2-dimensional uniform distribution. Similarly, consider function
hi(vi) = ψ-1(vi) and hj(vj) = ψ-1(vj). Where,
ψ(∙) is the cumulative density function of standard normal distribution. By this further construction,
the random variable h(g(Z)) is a 2-dimensional standard normal distribution.
Let A ∈ R2×2 be an arbitrary orthogonal matrix with Akm 6= 0 for all k = 1, 2 and m = 1, 2. An
infinite family of such matrices can be constructed using a Householder transformation: Choose an
arbitrary α ∈ (0,0.5)and consider the vector V with vι = √a and v = √1 - α. By construction,
we have vTv = 1. Define the matrix A = I2 - 2vvT. Furthermore, A is orthogonal since
ATA = (I2 - 2vvT)T (I2 - 2vvT) = I2 - 4vvT + 4v(vT v)vT = I2
.
Since A is orthogonal, it is invertible and thus defines a bijective linear operator. The random
variable Ah(g(Z)) ∈ R2 is hence an independent, multivariate standard normal distribution since
the covariance matrix ATA is equal to I2 .
Since h is bijective, it follows that h-1(Ah(g(Z)))is an independent 2-dimensional uniform distri-
bution. Define the function f : suPP(Z) -→ suPP(Z)
f(u) = g-1(h-1(Ah(g(u))))
and note that by definition f(Z) has the same marginal distribution as Z under P,i.e.,P (Z ≤ u) =
P(f(Z) ≤ u)for all u .Finally, for almost every u ∈ suPP(Z), it holds that
尹=0,
∂uj
Since A was chosen arbitrarily, there exists an infinite family of such function f.
To overcomes this problem, we need to do similar reconstruction job like we do for disentangled
representation learning, where introducing supervision signal to enforce independence between Zp
and Zn is necessary. However, previous works (Jaiswal et al., 2018; Xie et al., 2017; Louizos et al.,
2016; Moyer et al., 2018) fail to do that. The detail of the process is described in Section 3.3.
A.2 Visualization of latent space
To illustrate the performance of the results of the model, we visualize the latent representation by
different methods. We first visualize the t-SNE results of Zp and Znu which we tested on Color-
Rotation-MNIST in Figure 5.
13
Under review as a conference paper at ICLR 2022
(a) zp embedding
(b) znu embedding
Figure 5: t-SNE visualization of zp and znu embeddings of Color-Rotation-MNIST images col-
ored by rotation angle. As desired, the zp embedding does not encode rotation information, which
migrates to znu .
(a) latent factors change by changing the background
color and digit color
(b) latent factors change by changing the digit number
and make the color unchanged
Figure 6: Heat map visualization for Colored-MNIST dataset
(a) latent factors change by chang-
ing the color
(b) latent factors change by chang-
ing the angle
(c) latent factors change by chang-
ing the angle
Figure 7: Heat map visualization for Rotation-Colored-MNIST dataset
14
Under review as a conference paper at ICLR 2022
Figure 8: Reconstruction results of Colored-MNIST data
Further, we visualize the heatmap of latent space change by giving the model with different inputs.
In Figure 6, we visualize the latent factors change of model tested on Colored-MNIST and visualize
the latent factors change of model tested on Rotation-Colored-MNIST in Figure 7.
We finally visualize the results of reconstruction of model we tested on Colored-MNIST in Figure 8.
Images in line 1-2 are original images used for training. Images in line 3-4 are reconstructions which
are expected to be same with original inputs. Images in line 5-6 are reconstructions after swapping
operation which are also expected to be same with original inputs and reconstruction in line 2-
3. Images in line 7-8 are decoded from [rand(zp), zn], where rand(zp) is normal random noise.
Since we random sample zp , the outputs of decoder should only contains the color information and
unrecognized digits. In the contrary, images in line 9-10, we randomly sampled zn and the latent
factors for decoding is [zp, rand(zn)] . Therefore, images in line 9-10 should have same digit pattern
with original inputs but have random digit color and background color.
15