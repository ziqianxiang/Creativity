Under review as a conference paper at ICLR 2022
RVFR: Robust Vertical Federated Learning
via Feature Subspace Recovery
Anonymous authors
Paper under double-blind review
Ab stract
Vertical Federated Learning (VFL) is a distributed learning paradigm that allows
multiple agents to jointly train a global model when each agent holds a different
subset of features for the same sample(s). VFL is known to be vulnerable to
backdoor attacks, where data from malicious agents are manipulated during training,
and vulnerable to inference-phase attacks, where malicious agents manipulate the
test data. However, unlike the standard horizontal federated learning, improving
the robustness of VFL remains challenging, as there is no clear redundancy among
the agents in VFL. To this end, we propose RVFR, a novel robust VFL training and
inference framework. Under certain conditions, RVFR can recover the underlying
uncorrupted features with provable guarantees and thus sanitizes the model against
a vast range of backdoor attacks. Further, RVFR also defends against inference-
phase adversarial and missing feature attacks. We conduct extensive experiments
on NUS-WIDE and CIFAR-10 datasets and show that RVFR outperforms different
baselines in terms of robustness against diverse types of attacks.
1	Introduction
Federated Learning (FL) involves distributed training where a central server coordinates with multiple
agents to collaboratively train a machine learning model, and the agents keep their own training
data, e.g., due to privacy concerns. The theory and practice of FL has progressed rapidly in recent
years (Konecny et al., 2016; McMahan et al., 2017; BonaWitz et al., 2θl7; Dayan et al., 2021). TWo
broad categories (Yang et al., 2019) of FL are Horizontal FL (HFL) and Vertical FL (VFL). In HFL,
each agent has a different training set, While in VFL (Liu et al., 2019; Chen et al., 2020; Liu et al.,
2020), different agents hold different parts of features for the same set of training data. For example,
in VFL for credit score prediction, Agent 1 may have the banking data of a user and Agent 2 may
have the shopping history of the same user, While the server holds corresponding labels.
Given the distributed training, FL raises neW security concerns as the server has less control of
the training data and agent devices. A variety of attacks have been demonstrated on HFL and on
VFL. Several studies have focused on making HFL more robust, Wherein each agent sends the
model updates based on local data to the server and the server aggregates these updates. Robust
aggregation protocols have been proposed as defenses in HFL against malicious attacks (Yin et al.,
2018; Guerraoui et al., 2018; Blanchard et al., 2017; Fu et al., 2019; Pillutla et al., 2019; Fung et al.,
2020; Chen et al., 2017; Xie et al., 2019b). HoWever, it is challenging to defend against malicious
attacks in VFL, as there is no clear redundancy among the agents. In fact, there are feW studies on
robustness exploration for VFL.
In this paper, We propose a robust VFL training and inference frameWork via feature subspace
recovery (RVFR), Which is able to defend against many types of attacks during both training and
inference (see the attack taxonomy in Section 4.1). In particular, during training, We propose to
train each agent’s feature extractor separately based on feedback from the server (Quarantine training
stage); the server then performs robust feature subspace recovery given the embedded features
provided by different agents (Robust feature subspace recovery stage); and the server further purifies
the features based on the assumption that the fraction of malicious agents is relatively small (Feature
purifying stage). Finally, the server trains its global model based on the purified features (Server
training stage). An overvieW of the training process is illustrated in Figure 1. During the inference
phase, the server first purifies the embedded features provided by the agents and then feeds it to the
trained global model for prediction. Building upon our frameWork, We aim to ansWer the folloWing
1
Under review as a conference paper at ICLR 2022
questions: Is it possible to train a clean model with theoretical guarantees in the presence of poisoned
features? Can we make predictions based on incomplete (corrupted) features? If yes, how many
malicious agents/instances and missing features can we tolerate?
Figure 1: Overview of RVFR framework.
Under the RVFR framework, in the robust feature subspace recovery stage, we propose a variant
of the Robust AutoEncoder (Zhou & Paffenroth, 2017) to recover the feature subspace. While the
existing Robust AutoEncoder has no theoretical justification, we provide theoretical support for
it by proving that when the underlying feature subspace is linear, it can exactly recover the linear
feature subspace in the presence of corruption/poisoning. In the feature purifying stage (during
both training and inference), we propose a novel AutoEncoder-based robust decomposition method
that decomposes the potentially corrupted feature vectors into two parts: one lies on the learnt feature
subspace (can be non-linear); and the other contains a block-sparse structure. We theoretically show
that under certain conditions, the proposed method can recover the underlying feature vector exactly
despite corruption from malicious agents.
Technical Contributions: We take a first step towards providing robust VFL against diverse adver-
sarial attacks during both training and inference. Our main contributions include the following:
•	We propose a novel robust training procedure to defend against backdoor attacks in VFL, and prove
that under certain conditions it can exactly recover the underlying uncorrupted features. To the
best of our knowledge, this is the first defense for VFL against training attacks with theoretical
guarantees.
•	We provide the first theoretical justification for the Robust AutoEncoder, which may be of indepen-
dent interest.
•	We propose a robust VFL inference procedure to defend against a variety of targeted or untargeted
attacks (e.g., adversarial and missing feature attacks). To our knowledge, this is the first defense for
VFL against inference-phase attacks with theoretical guarantees.
•	We conduct extensive experiments on NUS-WIDE and CIFAR-10 datasets, and show that RVFR is
significantly more robust than baselines against diverse types of attacks.
2	Related Work
In this section, we briefly review prior work on backdoor attacks and defenses in different types of
federated learning frameworks.
Backdoor attack and defense in Horizontal Federated Learning. Many recent works, e.g.,
Bhagoji et al. (2019); Bagdasaryan et al. (2020); Wang et al. (2020); Xie et al. (2019a), have
demonstrated the vulnerability of HFL to backdoor attacks. For instance, Bhagoji et al.; Bagdasaryan
et al. show that training poisoned local models and submit malicious model updates to the server
can mislead the global model effectively. While Xie et al. exploits the distributed nature of HFL and
propose a distributed backdoor attack.
To defend against backdoor attacks in HFL, several robust federated protocols are proposed to mitigate
the attacks during training empirically. For example, Sun et al. shows that clipping the norm of model
2
Under review as a conference paper at ICLR 2022
updates and adding Gaussian noise can mitigate backdoor attacks. Andreina et al. incorporates an
additional validation phase to each round of FL to detect backdoor. To provide certain robustness
guarantees, some robust aggregation methods are proposed (Yin et al., 2018; Guerraoui et al., 2018;
Blanchard et al., 2017; Fu et al., 2019; Pillutla et al., 2019; Fung et al., 2020; Chen et al., 2017; Xie
et al., 2019b); while such robustness guarantees are not for backdoor attacks directly. Recently, Cao
et al. proposes Ensemble FL to defend against backdoor attacks with certifiable robustness. However,
the proposed majority voting strategy requires training hundreds of FL models. A recent work
exploits model clipping and smoothing in HFL, aiming to provide certified robustness to backdoor
attacks (Xie et al., 2021). However, none of these works can provide robustness guarantees on the
learned feature subspace of the server model, which is the focus of our work.
Backdoor attack and defense in Vertical Federated Learning. Backdoor attack against VFL is
challenging since in the typical setting (Chen et al., 2020) the agent does not have the label information.
Recent studies assume that the malicious agent knows at least one training instance belonging to the
target class (Liu et al., 2020). Under this assumption, they propose the gradient-replacement method
to perform backdoor attack. Some defense strategies are proposed against such attacks in VFL, e.g.,
sparsify the intermediate gradient before sending to the server (Liu et al., 2020). However, there is no
work providing any robustness certification against backdoor attacks in VFL to our best knowledge.
3	Preliminaries
Vertical Federated Learning. We first describe the basic setup of a typical VFL framework (Chen
et al., 2020; Liu et al., 2020). There are M agents and a server collaboratively training a machine
learning model based on a set of n training data D , {x1{j}, ..., x{Mj}, y{j}}jn=1. The server (e.g.,
a third party) holds the label y{j}, while agent i holds partial feature xi{j} of instance x{j} ,
[x1{j}; ...; x{Mj}]. Due to privacy concerns, the raw agent data xi{j} are not shared with other agents
and the server. Instead, each agent i learns a local embedding gi parameterized by θi that maps the
original feature vector xi{j} to feature vector hi{j} , gi(xi{j}; θi). The dimension of hi{j} is usually
{j}
smaller than xi . The server only has access to the embedded features from the agents. The learning
objective of VFL is to minimize the following loss:
nM
L(D; θ0,θ1,…，Θm ) = 1 X '(y{j},y{j})+ X γ(θi),	(1)
n j=1	i=1
where y{j} = fθ°(h{j},…，hM})，fθ° (gι(x{j}; θι),…，gM(xM}; Θm)), and fθ° is the global
model of server. ` is the loss function and γ is the regularizer that confines the model complexity.
Robust Subspace Recovery. Robust subspace recovery aims to recover the underlying subspace of
data despite that some data points are arbitrarily corrupted (a.k.a. outliers). Mathematically, one
observes a data matrix H, where each column corresponds to a data point. We can decompose H
as L + E , where L is the underlying uncorrupted data matrix, and the matrix E models the outlier
corruptions. Since the fraction of the outliers is usually small, most columns of E are zero, i.e., E is
column-sparse.
Assume the underlying uncorrupted data points lie on a low-dimensional linear subspace, one of the
notable Robust PCA works (Xu et al., 2012) showed that it is possible to recover the column-space of
L exactly by solving the following convex optimization:
min ∣∣L∣∣* + λkETk2,1 s.t. H = L + E.	(2)
L,E
Later we will see that in our proposed RVFR framework, H = h{1} , ..., h{n} , where the j-th
column of H corresponds to the embedded features for the j -th instance. In the backdoor attack, a
few columns of H are poisoned. In general the low-dimensional manifold that the embedded features
L lies on is not linear, and we use a Robust AutoEncoder instead of Robust PCA to capture this
non-linearity. We will show that Robust PCA can be viewed as a linearized version of the proposed
Robust AutoEncoder, and we will provide theoretical justification for it.
Formally, the Robust PCA result from Xu et al. (2012) is as follows:
Theorem 1. (Exact subspace recovery (Xu et al., 2012)) Suppose We observe H = L* + E*,
where L* has rank r and incoherence parameter μ (see Definition 1 in Appendix A.2). Suppose
3
Under review as a conference paper at ICLR 2022
further that E* is supported on at most βn columns. Any output to Eq. 2 recovers the column space
of L* exactly, as long as the fraction of corrupted columns, β, satisfies j-ββ ≤ μμr, where ci = 9/121.
This can be achieved by setting the parameter λ in Eq. 2 to be 7√βn (in fact it holds for any λ in a
certain range).
Inspired by the Robust PCA properties, we propose the Robust AutoEncoder in this work, which can
capture the more general non-linear manifold. So far there is no theoretical guarantees for Robust
AutoEncoders. The proposed Robust AutoEncoder is slightly different from an existing one (Zhou &
Paffenroth, 2017), and we theoretically justify the proposed one by showing that when the underlying
feature subspace is linear, after transformation based on the proposed Robust AutoEncoder, it exactly
recovers the linear feature subspace as Robust PCA. Note that our established connection between
the proposed Robust AutoEncoder and Robust PCA is non-trivial, especially due to the proposed
'2,1-norm regularizer on the latent layer.
Note that the exact recovery of the feature subspace is not enough for our goal of robust VFL, and
we need to recover the underlying L*. We further propose a robust decomposition technique to
recover L* exactly by utilizing our learnt feature subspace based on the assumption that the fraction
of malicious agents is small. More details can be found in Theorem 3.
4 Robust VFL via Feature Subspace Recovery (RVFR)
We first describe the threat models during VFL training and inference, and then present the training
and inference procedures of the proposed RVFR framework.
4.1	Threat Model
There are M agents which hold different parts of the feature of the same set of instances, and the
label y is held by the server. The attacker knows the feature information held by every agent and the
label information on the server, but it can only choose and control αM agents to perform malicious
attacks. During training, the goal of the malicious agents is to perform backdoor attacks, i.e., to
achieve high accuracy on both the original main task and the targeted backdoor tasks.
Training phase threat model (backdoor attack): There are α fraction of agents that are malicious,
and total β fraction of instances with backdoor trigger. With the adversarial target yA , the backdoor
attacks are conducted by optimizing the following objective for the instances with backdoor trigger:
min'(fθB ({hB, hbenign }),yA), where θB = arg min £ '(fθ(h{j}),y{j}),
B	∈Θ j
(3)
here hB represents overall embedded features provided by the malicious agents for the instance with
backdoor trigger, its magnitude can be arbitrarily large unless regularized by the server; while hbenign
denotes the overall embedded features provided by benign agents. If j -th instance has backdoor
trigger, h{j} contains hB .
During inference, the malicious agents can perform adversarial attacks as well. Assume the total
number of agents is M , and the fraction of malicious agents is α. We use hbenign to denote the
embedded features provided by the (1 - α)M benign agents (these agents are indexed by Ωbenign and
∣Ωbenign | = (1 - α)M ), and we use h0jdv to denote the embedded features provided by malicious
agents (indexed by Ωadv). We categorize the inference phase attacks into the following three types:
Inference phase threat model A (adversarial attack): The malicious agents aim to send adversarial
features hadv to mislead the prediction, whose magnitude can be arbitrarily large unless regularized
by the server:
min `(fθ0 ({hadv , hbenign }), y ),
hadv
(4)
Inference phase threat model B (missing-feature attack): On the other hand, the malicious agents
also have the option to NOT provide any features to the server. Such missing feature attack can
make the malicious agents less detectable and still influence server’s prediction. The presence of
missing feature attack can be viewed as that the server only observes partial blocks indexed by Ω of
the overall embedded features h, i.e., h。. We use Ωc to denote the index set of agents that perform
missing feature attacks. The attacker can compromise arbitrary αM agents.
min'(fθo (hn),yA),s.t. ∣Ωc∣ ≤ αM
(5)
4
Under review as a conference paper at ICLR 2022
Inference phase threat model C (combined attack): The attackers can also perform a combined
attack of the adversarial and missing feature attacks:
min '(fθ0 ({hadv , hbenign}Ω),yA),S.t. ∣Ωc∣ + ∣Ωadv∣ ≤ αM	(6)
Ωc, hadv
Besides above mentioned targeted attacks, the attacker can also perform untargeted attacks by
maximizing the distance between the prediction and the ground truth, which should be easier and
here we focus on defending these strong targeted attacks.
4.2	Training Procedures of RVFR
The proposed training procedure has four stages: quarantine training, robust feature subspace recovery,
feature purifying, and server training.
Stage 1 (Quarantined training): In VFL, the server trains the combined ML model, and each agent
trains its local feature extractor. A strong adversary could interfere with the server training and
further propagate errors to benign agents. So we propose that the server first connects to each agent
separately to train the feature extractors gi parameterized by θi, i = 1, ..., M. The training can be
viewed as a special case of (Chen et al., 2020, Algorithm 1) with only one agent.
In particular, agent i sends hi{j} , gi (xi{j} ; θi) to the server. The server uses it to update/train
a temporary global model (whose input dimension equals that of hi{j}) with learning rate η0 and
batch size B°. Then the server calculates ^dj and sends it back to agent i. The benign agent i
∂h{j}
updates its local feature extractor parameterized by θi using the combined gradient ^dj ∂i)- With
learning rate ηi and batch size Bi . After a number of iterations between the server and agent i, the
server records the final embedded feature hi{j} for every instance xi{j},j = 1, ..., n. Finally, for each
instance x{j}, the server holds {h{1j}, h{2j}, ..., h{Mj}} from all the M agents, and concatenates them
into a long column vector h{j}. Let H = h{1}, ..., h{n} , the j-th column of H corresponds to the
concatenated embeded features for the j -th instance.
Stage 2 (Robust feature subspace recovery): Since the fraction β of the backdoored training
instances is small, this provides us the redundancy across instances for robustifying the server’s
model. In this stage, the server disconnects from all the agents, and uses H to train a Robust
AutoEncoder by minimizing the folloWing objective:
min kEψ (L)k2,1 + λkE k2,1 s.t. H = L + E, L = Dφ (Eψ (L))	(7)
L,E,φ,ψ
Where L models the underlying feature subspace, and E models the outlier corruptions due to
backdoor attacks. In Eq. 7, We only enforce E to be column-sparse. Dφ is the decoder parameterized
by φ, and Eψ is the encoder parameterized by ψ .
Note that Eq. 7 is equivalent to the folloWing:
min kEψ(L)k2,1+λk(H-L)Tk2,1 s.t.L=Dφ(Eψ(L))
L,φ,ψ
(8)
In practice, We further relax the constraint L = Dφ (Eψ (L)) and solve the folloWing instead:
min kEψ(L)k2,1 + λk(H - L)T k2,1 + ηkL - Dφ(Eψ(L))k2F	(9)
L,φ,ψ
To optimize this, one can perform alternating optimization for L and parameters of φ and ψ.
Stage 3 (Feature purifying based on the recovered feature subspace): Recovering the feature
subspace is not enough to achieve robust VFL, as We need to recover the original features to train the
server. Fortunately, there is another important prior information that We can leverage: the fraction α
of the malicious agents is small. For each training instance h{j}, We use the learned AutoEncoder
to decompose it as h{j} = Dφ(Eψ(l{j})) + e{j}, Where e{j} models the block-sparse corruptions
(each block corresponds to an agent). Ideally We Want to solve the folloWing:
M
minnX l{ei = 0} s.t. h = Dφ(Eψ(l)) + e	(10)
l,e
, i=1
5
Under review as a conference paper at ICLR 2022
which is equivalent to:	M
minX l{[h - Dφ(Eψ(l))]i = 0}	(11)
l	i=1
However, the above block-sparsity minimization objective is NP-Complete, so we relax it to the
'2,1-norm:
minX k[h - Dφ(Eψ(l))]ik2	(12)
i=1
Our theoretical analysis in next section shows that under certain conditions, solving the above relaxed
objective is able to recover the underlying true features.
Stage 4 (Server training): The server trains its own global model parameterized by θ0 based on the
above purified features Dφ(Eψ(l)), with learning rate η0 and batch size B0.
4.3 Inference Procedures of RVFR
We propose an all-in-one solution to defend against both training phase and inference phase malicious
attacks defined in Section 4.1, and thus during inference we will also perform feature purifying using
the recovered feature subspace and the sparse nature of the malicious agents.
Let Ω be the index set of the observed agent blocks. Assume We have successfully trained the
AutoEncoder, such that we have l = Dφ (Eψ (l)), where l is any uncorrupted embedded feature
vector. We want to use the AutoEncoder to decompose the observed h∙Ω as h∙Ω = 1ω + eΩ With
l = Dφ(Eψ (l)), where eΩ models the block-sparse corruptions (each block corresponds to an agent)
on the observed set Ω. Note that if all the blocks (agents) are observed, Ω becomes the whole set.
Since eΩ is block-sparse, similar to the feature purifying during training, we naturally aim to solve
the following:
minX l{[h -Dφ(Eψ(l))]i = 0}	(13)
i∈Ω
However, the above block-sparsity minimization objective is NP-Complete, we instead relax it to the
'2,1-norm:
mlinXk[h-Dφ(Eψ(l))]ik2	(14)
i∈Ω
Our theoretical analysis in Theorems 3 & 4 show that under certain conditions, solving the above
relaxed objective is able to recover the underlying true features, despite the partial observations. After
obtained l, we feed Dφ(Eψ(l)) to server’s trained machine learning model for the prediction.
5	Theoretical Analysis of RVFR
We first analyze the proposed Robust AutoEncoder, which is the backbone of the proposed defense
framework. Then, we prove that the proposed feature purifying method can recover the underlying
features exactly under certain conditions, which means that the server’s global model can be both
trained and tested on correct features, and thus is robust against diverse types of attacks.
The Robust AutoEncoder can be viewed as the generalization of Robust PCA, where the low-
dimensional linear subspace is extended to the more general low-dimensional manifold. It is challeng-
ing to prove the exact recovery of the low-dimensional manifold in the presence of outlier corruptions.
However, we next show that when the underlying feature subspace is linear, the Robust AutoEncoder
can exactly recover the underlying subspace.
Theorem 2.	(Exact subspace recovery) Assume l*{i} ∈ Rd, i = 1,..., n lie on a low-dimension
subspace, i.e., rank(L*) = r《d. For the AutoEncoder, assume linear activation functions and
no bias terms, while setting the dimension of the latent layer of AutoEncoder as r = rank(L*) and
restricting the weight matrices to be orthonormal. Then under the conditions in Theorem 1, the global
optimal solution of Eq. 7 is equivalent to the solution of Eq. 2, and the corresponding weight matrix
of the encoder is guaranteed to recover the subspace of L* exactly.
The proof is omitted to Appendix A.3. The key is to show that the objective value of Eq. 7 is greater
than or equal to the objective value of Eq. 2, with equality achieved when the weight matrix of the
encoder equals the top-r left singular vectors of L. This is non-trivial especially due to the proposed
'2,1-norm regularizer on the latent layer of the Robust AutoEncoder.
6
Under review as a conference paper at ICLR 2022
Remark 1. The conditions required in Theorem 2 provide some useful insights. First, the total
fraction β of poisoned instances needs to be small. Second, the dimension of the underlying feature
subspace needs to be small. If the dimension of the feature subspace is too large, it is difficult to
distinguish corrupted features from the uncorrupted ones. On the other hand, low dimensionality
implies that there is enough redundancy among the features among agents, which is preferred since
redundancy provides more robustness. Third, as discussed in Appendix A.2 regarding the incoherence
condition, preferring small incoherence parameters implies that we do not want any dimension of the
feature subspace to be defined by very few data points. Otherwise, it is risky if those instances are
poisoned, as it becomes impossible to recover that dimension.
Note that exact recovery of the subspace does not mean exact recovery of L. For the cor-
rupted/poisoned instances, usually it is impossible to restore them as the corruptions can be arbitrary.
Fortunately, once we have recovered the underlying feature subspace, it is possible to recover the
original features exactly by utilizing the sparseness of the malicious agents. This holds for both
training and inference phases. The following theorem shows that we can recover the underlying
embedded features exactly against the inference phase threat models A & C.
Theorem 3.	(Exact feature recovery under threat model A & C) Let Wend be the weight matrix
of the last layer of the AutoEncoder. Assume there is no bias term nor non-linear activation function
in the last layer. Assume the trained AutoEncoder captures the underlying feature subspace (i.e.,
Dφ(Eψ(l)) = l for uncorrupted feature vector l). Let Ω be the index set of the observed blocks
(agents). If ∀v ∈ Range(Wend)\0, for any partition {Sω,Sω} of Ω with ∣Sω∣ = g > ∣Ω∣∕2, it
holds that Pi∈Sα Mk2 > Pi∈Sα l∣Vik2, then for any h. = lΩ + eΩ With Dφ(Eψ(l*)) = l* and
Pi∈Ω l{e* = 0} ≥ g, Dφ(Eψ(l*)) is the unique solution ofEq. 14 andEq. 13.
The proof can be found in Appendix A.4, where we use contradiction to show that there does not
exist any global optimal solution of Eq. 14 or Eq. 13 that is different from Dφ(Eψ(l*)).
Remark 2. (Exact feature recovery via feature purifying during training) Theorem 3 directly
justifies the proposed feature purifying during training. Under certain conditions, we can recover
each column of L exactly during training.
Next, we analyze the exact feature recovery property under inference phase threat model B (missing
feature attack), where there are no adversarial features.
Theorem 4.	(Exact feature recovery under threat model B) Let Wend be the weight matrix of
the last layer of the AutoEncoder. Assume there is no bias term nor non-linear activation function
in the last layer. Assume the trained AutoEncoder captures the underlying feature subspace (i.e.,
Dφ(Eψ(l)) = l for uncorrupted feature vector l). Let Ω be the index set of the observed blocks
(agents). If ∀v ∈ Range(Wend)\0, it holds that vω = 0, then given h∙Ω = lΩ, Dφ(Eψ(l*)) is the
unique solution of Eq. 14 and Eq. 13.
The proof is deferred to Appendix A.5, where we use contradiction to show that there does not exist
any global optimal solution of Eq. 14 or Eq. 13 that is different from Dφ(Eψ(l*)).
The condition vω = 0 for any V ∈ Range(Wend)∖0 required by Theorem 4 is much weaker than
that required by Theorem 3, which is reasonable as there are no corruptions on the observed blocks in
Theorem 4. This required condition also provides very useful insights on how many agents we need
to observe in order to make a prediction. It can be understood through the following toy example.
Suppose the prediction task is credit approval. Agent 1 holds the banking data, Agent 2 holds the
shopping history, while Agent 3 holds age and gender features. Intuitively, if we only observe the
features from Agent 3, it is very hard to make the prediction, since there are definitely Person A and
B of the same age and gender, but with quite different other features. Let lA be Person A’s overall
features, lB be Person B,s overall features. In this toy example, Ω corresponds to Agent 3 only. We
have lA = IB but lA = lB, or say (lA 一 lB )ω = 0. Note that both lA and lB are in the range space
of Wend, so (lA 一 lB) ∈ Range(Wend)\0. The smaller the set Ω implies the higher chance that
(lA 一 lB)ω = 0. In general, there is no exact value for the minimal fraction of agents we need
to observe in order to make a prediction. It highly depends on whether the observed agents cover
enough information to distinguish different classes in the classification task. For example, if there is
enough redundancy across the agents, say Agent 3 holds age, gender, banking data, and shopping
history, then observing Agent 3 could be sufficient.
7
Under review as a conference paper at ICLR 2022
In summary, Theorem 2 shows that under certain natural conditions (e.g., the fraction of poisoning
instances β is small), we can recover the underlying feature subspace exactly. Further, Theorems 3 &
4 show that under certain natural conditions (e.g., the fraction of malicious agents α is small and the
AutoEncoder we have learnt captures the underlying feature subspace), we can recover the underlying
embedded features exactly during both training and inference.
6	Empirical Evaluation
We first describe experimental setup in Section 6.1, and present experimental results in Section 6.2.
Additional implementation details and results can be found in Appendix A.1.
6.1	Experimental Setup
Dataset and Models. We study the classification task on two datasets: NUS-WIDE and CIFAR-10.
Following (Liu et al., 2020), which proposed the backdoor attack against VFL, we first use NUS-
WIDE dataset to evaluate our defense. In NUS-WIDE, each sample has 634 image features, 1000 text
features, and 5 different labels. We consider two VFL settings (1 server and M agents): M = 2 and
M = 4. We denote G1 as Agent 1, G2 as Agent 2, etc. for simplicity. Following (Liu et al., 2020),
when M = 2, G1 holds image features and G2 holds text features. When M = 4, G1 (G2) holds 225
(409) image features while G3 (G4) holds 500 (500) text features. In all settings, only the server holds
the labels. In CIFAR-10 dataset, each sample have 32x32x3 image features. We consider the VFL
settings for CIFAR-10 with M = 2 and M = 3. When M = 2, each agent has 16x32x3 features,
i.e., half image. When M = 3, G1, G2, G3 have 9x32x3, 10x32x3, 13x32x3 features respectively.
Following the standard setup, we use SGD to update the local models and server model for E epochs
with learning rate 0.01 and batch size Bs . The detailed dataset and model descriptions as well as
training parameter setups are summarized in Table 1 in Appendix.
Attack Setup. We consider the backdoor attack, its combination with the missing feature attack, as
well as the combination of adversarial evasion attack (Goodfellow et al., 2014) and missing feature
attack: (1) The backdoor attack is proposed by (Liu et al., 2020) and the detailed description of
the attack method can be found in Appendix. A.1. In NUS-WIDE, the attacker’s backdoor attack
goal is to change the predicted label of the instance with a backdoor trigger to be a specific target
class (i.e., ‘buildings’), where the backdoor trigger is ‘the last text feature equals 1’. There are 152
backdoored training samples. In CIFAR-10, the backdoor targeted label is ‘truck’ and the trigger
pattern is a white rectangle with size 4x6x3 in the lower right corner. The number of backdoored
training samples is 100 when M = 3, and 600 when M = 2. (2) In the missing feature attack, the
malicious agent does not send its local embedded feature to the server during inference phase. The
server uses a zero vector as its local embedded feature. We combine this attack with other attacks as
this attack alone is not powerful enough. (3) In the evasion attack, the attacker use the information
from server to locally generate the partial adversarial input features (i.e., adversarial examples) based
on the Fast Gradient Signed Method (FGSM) attack method (Goodfellow et al., 2014). The detailed
setup of this attack method against VFL is presented in Appendix. A.1. The evasion attack (or its
combination with missing feature attack) is only conducted during the VFL inference phase (VFL
training procedure is attack-free).
Baseline methods. Besides the Vanilla VFL framework without defense (Liu et al., 2020, Algorithm
1), we compare our proposed defense method with two state-of-the-art VFL defense techniques,
e.g., Gradient Sparsification method that sparsifies the intermediate gradients sent from the server
to agents, and Differential Privacy (DP) that adds noise to such exchanged gradients (Liu et al.,
2020). Laplacian Noise is used as a realization of the DP method. The noise variance is 0.05
for NUS-WIDE and 0.0001 for CIFAR-10 to preserve reasonable clean accuracy of the model. In
Gradient Sparsification, the hyperparameter controlling the sparsity is 0.999 for NUS-WIDE and
0.95 for CIFAR-10. As for our proposed RVFR, in Stage 1 (i.e., quarantine training for local models)
and Stage 4 (i.e., training for server model), we use the same training parameters as in Table 1. The
robust autoencoder is a four-layer fully connected neural network. The details about the training of
robust autoencoder, the alternative optimization, and the purified training of the server in Stage 2, 3,
4 and details of the inference procedures can be founded in Appendix. A.1. All of our experiments
are run three times.
6.2	Empirical Results
Evaluation Metrics. For the backdoor attack (BKD) and its combination with missing feature attack
(Miss), the evaluation metrics for defense methods are the Backdoor Accuracy (the lower the better)
8
Under review as a conference paper at ICLR 2022
on the backdoored test data and the main task Clean Accuracy (the higher the better) on the clean test
dataset. There are 102 backdoored testing samples in NUS-WIDE and 10000 in CIFAR-10. For the
adversarial evasion attack (Adv) and missing feature attack, the evaluation metric is the main task
Accuracy on the adversarial examples. 10000 adversarial examples are generated for both datasets.
Vanilla VFL	Laplacian Noise
Gradient SparSifiCation	Proposed RVFR
100
80
60
40
20
0
BKD G2
xua,InUUV.!OOP*。®
Miss G1 + BKD G2
Ooooo
0 8 6 4 2
u,InUUV E9u
2 Agents
100
0
BKD G2 Miss G1 + BKD G2
100
■ i I
Miss G2 + Adv G1
xup.1n□uv
8 6 4 2
u,InUUV
Figure 2: Performance comparison on NUS-WIDE under M = 2 (first row) and M = 4 (second
row). The missing bars are due to zero values. Dashed blue line is Vanillia VFL without adversaries.
Figure 2 & 3 show the performance of the proposed RVFR and baseline methods on NUS-WIDE
and CIFAR-10. When defending against backdoor attack and its combination with missing feature
attack, the proposed RVFR method exhibits significantly lower Backdoor Accuracy and similar
Clean Accuracy on both datasets, compared to other methods. Note that under the backdoor attack,
in some cases, the Clean Accuracy of all defense methods is lower than the vanilla VFL, which
exhibits a trade-off between robustness and accuracy. As shown in the right column of Figure 2 & 3,
when defending against the combination of adversarial evasion attack and missing feature attack, the
proposed RVFR method achieves consistently higher accuracy than the baseline methods on both
datasets, which demonstrates its robustness against such attacks.
Vanil Vanilla VFL	Laplacian Noise	Gradient Sparsification	Proposed RVFR
Ooooo
0 8 6 4 2
AUPInuuVIOOPPPg
2 Agents
xua,!n□uv E9u
Ooooo
0 8 6 4 2
u,Inuuv
xua,InUUV.Ioop>pag
Figure 3: Performance comparison on CIFAR-10 under M = 2 (first row) and M = 3 (second row).
Dashed blue line is Vanillia VFL without adversaries.
7	Conclusions
In this work, we proposed a novel robust feature subspace recovery based VFL framework to
defend against backdoor attacks during training, and a variety of attacks during inference, both with
theoretical guarantees. An important byproduct of our analysis is the first theoretical justification for
the Robust AutoEncoder, which may be of independent interest. We further validate the robustness of
our proposed framework through extensive experiments on NUS-WIDE and CIFAR-10 datasets.
9
Under review as a conference paper at ICLR 2022
Ethics S tatement
We propose a novel defense algorithm for vertical federated learning, and provide analysis from
theoretical and empirical perspectives. All the datasets and packages we use are open-sourced. We
do not have ethical concerns in our paper.
Reproducibility Statement
The source code of the proposed RVFR method is available as the supplemental material, which also
includes the detailed data processing steps. The complete proof of the theorems can be found in the
Appendix.
References
Sebastien Andreina, Giorgia Azzurra Marson, Helen Mollering, and Ghassan Karame. Baffle:
Backdoor detection via feedback-based federated learning. arXiv preprint arXiv:2011.02167,
2020.
Eugene Bagdasaryan, Andreas Veit, Yiqing Hua, Deborah Estrin, and Vitaly Shmatikov. How to
backdoor federated learning. In International Conference on Artificial Intelligence and Statistics,
pp. 2938-2948. PMLR, 2020.
Arjun Nitin Bhagoji, Supriyo Chakraborty, Prateek Mittal, and Seraphin Calo. Analyzing federated
learning through an adversarial lens. In International Conference on Machine Learning, pp.
634-643. PMLR, 2019.
Peva Blanchard, El Mahdi El Mhamdi, Rachid Guerraoui, and Julien Stainer. Machine learning
with adversaries: Byzantine tolerant gradient descent. In Proceedings of the 31st International
Conference on Neural Information Processing Systems, pp. 118-128, 2017.
Keith Bonawitz, Vladimir Ivanov, Ben Kreuter, Antonio Marcedone, H Brendan McMahan, Sarvar
Patel, Daniel Ramage, Aaron Segal, and Karn Seth. Practical secure aggregation for privacy-
preserving machine learning. In proceedings of the 2017 ACM SIGSAC Conference on Computer
and Communications Security, pp. 1175-1191, 2017.
Xiaoyu Cao, Jinyuan Jia, and Neil Zhenqiang Gong. Provably secure federated learning against
malicious clients. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 35, pp.
6885-6893, 2021.
Tianyi Chen, Xiao Jin, Yuejiao Sun, and Wotao Yin. Vafl: a method of vertical asynchronous
federated learning. arXiv preprint arXiv:2007.06081, 2020.
Yudong Chen, Lili Su, and Jiaming Xu. Distributed statistical machine learning in adversarial settings:
Byzantine gradient descent. Proceedings of the ACM on Measurement and Analysis of Computing
Systems, 1(2):1-25, 2017.
Ittai Dayan, Holger R Roth, Aoxiao Zhong, Ahmed Harouni, Amilcare Gentili, Anas Z Abidin,
Andrew Liu, Anthony Beardsworth Costa, Bradford J Wood, Chien-Sung Tsai, et al. Federated
learning for predicting clinical outcomes in patients with covid-19. Nature medicine, 27(10):
1735-1743, 2021.
Shuhao Fu, Chulin Xie, Bo Li, and Qifeng Chen. Attack-resistant federated learning with residual-
based reweighting. arXiv preprint arXiv:1912.11464, 2019.
Clement Fung, Chris JM Yoon, and Ivan Beschastnikh. The limitations of federated learning in
sybil settings. In 23rd International Symposium on Research in Attacks, Intrusions and Defenses
({RAID} 2020), pp. 301-316, 2020.
Ian J Goodfellow, Jonathon Shlens, and Christian Szegedy. Explaining and harnessing adversarial
examples. arXiv preprint arXiv:1412.6572, 2014.
Rachid Guerraoui, Sebastien Rouault, et al. The hidden vulnerability of distributed learning in
byzantium. In International Conference on Machine Learning, pp. 3521-3530. PMLR, 2018.
10
Under review as a conference paper at ICLR 2022
Jakub Konecny, H Brendan McMahan, Daniel Ramage, and Peter Richtarik. Federated optimization:
Distributed machine learning for on-device intelligence. arXiv preprint arXiv:1610.02527, 2016.
Yang Liu, Yan Kang, Xinwei Zhang, Liping Li, Yong Cheng, Tianjian Chen, Mingyi Hong, and
Qiang Yang. A communication efficient collaborative learning framework for distributed features.
arXiv preprint arXiv:1912.11187, 2019.
Yang Liu, Zhiqian Yi, and Tianjian Chen. Backdoor attacks and defenses in feature-partitioned
collaborative learning. ArXiv, abs/2007.03608, 2020.
Brendan McMahan, Eider Moore, Daniel Ramage, Seth Hampson, and Blaise Aguera y Arcas.
Communication-efficient learning of deep networks from decentralized data. In Artificial intelli-
gence and statistics, pp. 1273-1282. PmLr, 2017.
Krishna Pillutla, Sham M Kakade, and Zaid Harchaoui. Robust aggregation for federated learning.
arXiv preprint arXiv:1912.13445, 2019.
Ziteng Sun, Peter Kairouz, Ananda Theertha Suresh, and H Brendan McMahan. Can you really
backdoor federated learning? arXiv preprint arXiv:1911.07963, 2019.
Hongyi Wang, Kartik Sreenivasan, Shashank Rajput, Harit Vishwakarma, Saurabh Agarwal, Jy-yong
Sohn, Kangwook Lee, and Dimitris Papailiopoulos. Attack of the tails: Yes, you really can
backdoor federated learning. arXiv preprint arXiv:2007.05084, 2020.
Chulin Xie, Keli Huang, Pin-Yu Chen, and Bo Li. Dba: Distributed backdoor attacks against federated
learning. In International Conference on Learning Representations, 2019a.
Chulin Xie, Minghao Chen, Pin-Yu Chen, and Bo Li. Crfl: Certifiably robust federated learning
against backdoor attacks. arXiv preprint arXiv:2106.08283, 2021.
Cong Xie, Sanmi Koyejo, and Indranil Gupta. Zeno: Distributed stochastic gradient descent with
suspicion-based fault-tolerance. In International Conference on Machine Learning, pp. 6893-6901.
PMLR, 2019b.
Huan Xu, Constantine Caramanis, and Sujay Sanghavi. Robust pca via outlier pursuit. IEEE
Transactions on Information Theory, 58(5):3047-3064, 2012. doi: 10.1109/TIT.2011.2173156.
Qiang Yang, Yang Liu, Tianjian Chen, and Yongxin Tong. Federated machine learning: Concept and
applications. ACM Transactions on Intelligent Systems and Technology (TIST), 10(2):1-19, 2019.
Dong Yin, Yudong Chen, Ramchandran Kannan, and Peter Bartlett. Byzantine-robust distributed
learning: Towards optimal statistical rates. In International Conference on Machine Learning, pp.
5650-5659. PMLR, 2018.
Chong Zhou and Randy C Paffenroth. Anomaly detection with robust deep autoencoders. In
Proceedings of the 23rd ACM SIGKDD international conference on knowledge discovery and data
mining, pp. 665-674, 2017.
A Appendix
A. 1 Additional Experimental Details
Simulation Environment. We simulate the VFL setup (1 server and multiple agents) on a Linux
machine with Intel® Xeon® Gold 6132 CPUs and 8 NVidia® 1080Ti GPUs. Our code is based on
Tensorflow 2.0.
Table 1: Dataset description and parameters
Dataset	# training samples	# test samples	Batch size Bs	Epoch E	Local model	Embedded feature dim.	Server model
NUS-WIDE	60000	40000	64	200	1 fc	32	2 fc
CIFAR-10	50000	10000	128	100	1 conv and 2 fc	10	2 fc
11
Under review as a conference paper at ICLR 2022
VFL Backdoor Attacks. For the j -th instance with backdoor trigger, the malicious agent i does
not update its local feature extractor (parameterized by θi ) like benign agents via using the combined
∂h
gradient '> Qi . Instead, it replaces the intermediate gradient received from the server d'-1 by
∂hi{j} ∂θi	∂hi{j}
Y 加garget} and uses the manipulated gradient Y
d`∂hj
∂h{target} ∂θi
to update its local feature extractor,
where 加：±§电 is the intermediate gradient received from the server for any instance belonging
to the target class, and Y is the gradient amplify ratio. In our experiments, we use Y = 10 for
NUS-WIDE and Y = 1 for CIFAR-10. Moreover, since the label owned by the server is clean, the
attacker would submit zero vector to the server to prevent the server mapping the poisoned embedding
to the true label.
Evasion Attacks. Given the j-th test sample, the attacker adds small perturbations xtpeesrti{j} gen-
erated from FGSM to the original data xtesti{j } to obtain the adversarial example xtaedsvti{j} =
xtesti{j} + xtpeesrti{j} where controlling the degree of attack. For NUS-WIDE, we set = 0.01
when M = 2 and = 0.1 when M = 4; for CIFAR-10, we set = 0.01.
Training and Inference Details of RVFR. In Stage 2, we first train the AutoEncoder for Erae
epochs, and alternatively update AutoEncoder and the estimated uncorrupted embedded features L
for Ealt epochs using SGD with an initial learning rate 0.01 which exponentially decays by a factor
of 0.99 every 10000 steps. During the alternating optimization, λ and η in the objective function
are set to be 0.1. In NUS-WIDE, Erae = 100 when M = 2 and Erae = 200 when M = 4; in
CIFAR-10, Erae = 100. We use Ealt = 100 for all settings. In Stage 3 & 4, for each batch of
training data, we first purifies the embedded features according to Eq. 12 (using the estimated L
from Stage 2 as initialization) for EpurJrain = 2 epochs, then use the purified embedded feature to
train server’s global ML model. We perform such procedure for Eserver = 100 epochs using the
same SGD optimizer as in Stage 2. In the inference phase, the server purifies the embedded features
according to Eq. 14 (use the embedded features as initialization) for EpurJest = 2 epochs, and feeds
the purified feature to the trained server model for prediction.
Clean Accuracy under Non-adversarial Setting In this section, we report the clean accuracy of
different VFL methods under non-adversarial setting in Table 2.
Table 2: Clean accuracy of different VFL methods under non-adversarial setting
Setting	I Vanilla VFL	Laplacian	Gradient Sparsification	Proposed RVFR
NUS 2 clients	I 87.3683 ± 0.0664	86.0283 ± 0.8465	86.5033 ± 0.2166	82.1408 ± 1.6045
NUS 4 clients	I 87.9542 ±0.0510	86.5583 ± 0.1008	86.3067 ± 0.2938	81.1667 ± 1.7558
CIFAR 2 clients	I 60.3633 ± 0.5413	60.1733 ± 0.4606	54.0400 ± 0.7758	59.4400 ± 0.0990
A.2 Definition of Incoherence parameter
Definition 1. (Incoherence parameter (Xu et al., 2012)) A matrix L ∈ Rd×n with thin SVD
L = UΣV T and (1 - β)n whose columns are non-zero, is said to be column-incoherent with
parameter μ if
max kv Teik2 ≤ (i-⅛,
where {ei} are the coordinate unit vectors, r is the rank of matrix L.
(15)
A small incoherence parameter implies the right singular vectors of L are not ‘spiky’. As mentioned
in Xu et al. (2012), if the left hand side of Eq. 15 is as big as 1, it essentially means that one of the
directions of the column space which we wish to recover, is defined by only a single observation.
Given the regime of a constant fraction of arbitrarily chosen and arbitrarily corrupted points, such a
setting is highly undesirable.
12
Under review as a conference paper at ICLR 2022
A.3 Proof of Theorem 2
Proof. First note that Wencoder ∈ Rr×d since we limit the dimension of the latent layer of the
AUtoEncoder to be r = rank(L*). The problem We consider is:
L,E,Wenmcoinder∈Rr×d kWencoderLk2,1 + λkET k2,1, s.t. H = L + E, L = Dφ(Eψ(L))	(16)
Since We have the constraint L = Dφ(Eψ(L)), the rank of any feasible L shoUld be no more
than r, and L mUst lie Within the roW-space of Wencoder. So We can Write Wencoder = RUT
Where R ∈ Rr×r, and U are the top-r left singUlar vectors of L. As the roWs of Wencoder are
orthornormal, i.e., WencoderWeTncoder = I, so RUTURT = I and therefore RRT = I (so this
sqUare matrix R is Unitary). FUrther, since setting Wdecoder = WeTncoder Will alWays meet the
constraint L = Dφ(Eψ(L)), the main problem We need to consider becomes:
min	kWencoder Lk2,1 + λkET k2,1, s.t. H = L + E,
{L∣rank(L)≤r},E,
Wencoder=RUT,R is Unitary
(17)
Since ∣∣L∣∣*	=	IlRUT L∣∣*	=	IlWencoder L∣∣*	=	kI (Wencoder L)k*	=
k PPi=I ei (^WencoderL)i,: k * ≤ Pi=1 11 ei ( Wrencoder L)i,:k = Pi=1 k(Wrencoder L)i,: k 2 ,
IWencoderLI2,1. The eqUality is achieved When R = I, i.e., Wencoder = UT, becaUse
kWencoderLk2,1 = kUTLk2,1 = kUTUΣVTk2,1 = kΣV T k2,1 =Pir=1σi= kLk*.
So
min	kWencoderLk2,1+λkETk2,1 ≥ min kLk* + λkET k2,1,
Wencoder=RUT ,R is Unitary
(18)
With eqUality achieved When Wencoder eqUals top-r left singUlar vectors of L.
Then We only need to consider the right-hand-side of Eq. 18, i.e.,
min	kLk* + λkETk2,1 s.t.H=L+E	(19)
Recall that under the conditions of Theorem 1, the solution L ofEq. 2 has exactly the same column
space as L* (L may not and not necessary to be equal to L*), so rank(L) = r. Then the solution L
of Eq. 2 must also be the global optimal solution of Eq. 19. Finally, as the roW-space of Wencoder
equals the column-space of L, it recovers the underlying subspace of L* exactly.	□
A.4 Proof of Theorem 3
Proof. A) Suppose Dφ(Eψ(l0)) is the global optimal solution of Eq. 14 that is different from
Dφ(Eψ(l*)). Let Dφ(Eψ(l0)) = Dφ(Eψ(l*)) - v. So We have v ∈ Range(Wend)\0.
13
Under review as a conference paper at ICLR 2022
k[h-Dφ(Eψ(l0))]ik2	(20)
i∈Ω
=X k[h -Dφ(Eψ(l*)) + v]ik2	(21)
i∈Ω
=X k[h -Dφ(Eψ(l*))]i + Vik2	(22)
i∈Ω
=X k[h -Dφ(Eψ(l*))]i + Vik2 + X k[h -Dφ(Eψ(l*))]i + Vik2	(23)
i∈SΩ	i∈Sα
=X kvik2 + X k[h - Dφ(Eψ (l*))]i + vik2	(24)
i ∈ SΩ	i ∈ Sω
≥ X kVik2 + X k[h -Dφ(Eψ(l*))]ik2 - X kVik2	(25)
i∈SΩ	i∈Sα	i∈Sα
> X k[h -Dφ(Eψ(l*))]ik2	(26)
i∈Sα
=X k[h -Dφ(Eψ(l*))]ik2	(27)
i∈Ω
where Sω is the index set of size g such that e* = 0, ∀i ∈ Sω. And the last inequality follows from
the assumed range space property since v ∈ Range(Wend)\0.
The above contradicts the assumption that Dφ(Eψ(l0)) is the global optimal solution that is different
from Dφ(Eψ (l*))∙
B) First, note that hn = Dφ(Eψ (1*))ω + eΩ and 工心 l{e* = 0}≤ ∣Ω∣-g. Suppose Dφ(Eψ (l0))is
the global optimal solution ofEq. 13 that is different from Dφ(Eψ(l*)). Let h∙Ω = Dφ(Eψ(10))ω + eΩ,
then we have
X l{ei= 0}≤ X l{e： = 0}≤∣Ω∣-g	(28)
i∈Ω	i∈Ω
and Dφ(Eψ (10))ω + eΩ = Dφ(Eψ (1*))ω + eΩ∙
From Eq. 28 we know that
X l{[Dφ(Eψ(l*)) - Dφ(Eψ(l0))]i = 0} ≥∣Ω∣- X l{e* = 0} - X l{ei = 0}	(29)
i∈Ω	i∈Ω	i∈Ω
≥iωi -(Iωi - g) -(Iωi - g) = 2g -|c|	(30)
Let Dφ(Eψ(l0)) = Dφ(Eψ(l*)) - v, so we have V ∈ Range(Wend)∖0 and P^ l{vi = 0} ≥
2g - ∣Ω∣. Now split Ω into 3 disjoint sets {Ωo, Ωι, Ω2}, where Ωo is any subset of Ω with size
2g - ∣Ω∣ such that vq0 = 0, and ∣Ωι∣ = ∣Ω2∣ = ∣Ω∣ - g. Since ∣Ωo ∪ Ωι∣ = g, by our assumption,
we have Pi∈Ω0∪Ω1 ∣Wik2 > Pi∈Ω2 l∣Vik2. Since ∣Ωo ∪ Ω2∣ = g, by our assumption, we have
Pi∈Ω0∪Ω2 kvik2 > Pi∈Ω1 kvik2. HOWever, this leads to a COntradiCtiOn SinCe £g0 kvik2 =
0.	□
A.5 Proof of Theorem 4
Proof. Suppose Dφ(Eψ (l0)) is the global optimal solution ofEq. 14 that is different from Dφ(Eψ (l*)).
Let Dφ(Eψ(l0)) = Dφ(Eψ(l*)) - v. So we have V ∈ Range(Wend)\0.
14
Under review as a conference paper at ICLR 2022
	k[h-Dφ(Eψ(l0))]ik2	(31) i∈Ω =X k[h -Dφ(Eψ(l*)) + v]ik2	(32) i∈Ω =Xkvik2	(33) i∈Ω >0	(34) =X k[h -Dφ(Eψ(l*))]ik2,	(35) i∈Ω
where the inequality is due to the condition that vω = 0 for any V ∈ Range(Wend)\0.
The above contradicts the assumption that Dφ(Eψ(l0)) is the global optimal solution of Eq. 14 that is
different from Dφ(Eψ (l*)).
Similarly, suppose Dφ (Eψ(l0)) is the global optimal solution of Eq. 13 that is different from
Dφ(Eψ (l*)).LetDφ (Eψ (l0))	=Dφ(Eψ(l*)) 一 v. So we have V ∈ Range(Wend)\0. X l{[h - Dφ(Eψ(l0))]i = 0}	(36) i∈Ω =X l{[h - Dφ(Eψ(l*)) + v]i = 0}	(37) i∈Ω =X l{Vi = 0}	(38) i∈Ω >0	(39) =X l{[h - Dφ(Eψ(l*))]i = 0}	(40) i∈Ω
The above contradicts the assumption that Dφ (Eψ(l0)) is the global optimal solution that is different
from Dφ(Eψ (l*)).
□
15