Under review as a conference paper at ICLR 2022
Amortized Posterior on Latent Variables in
Gaussian Process
Anonymous authors
Paper under double-blind review
Ab stract
Deep neural networks have achieved impressive performance on a variety of
domains. However, performing tasks in partially observed, dynamic environments
is still an open problem. Gaussian Process (GPs) is well-known for capturing
uncertainty in model parameters. However, it simply assumes a fixed Gaussian
prior on latent variables. Thus, agents are not able to update their beliefs about
latent variables as observing data points. Instead, in this paper, we propose to
replace the prior with an amortized posterior, which enables quick adaptation,
especially to abrupt changes. Experiments show that our proposed method can
adjust behaviors on the fly (e.g., blind “Predator” take 56% more chance to approach
“Prey”), correct mistakes to escape bad situations (e.g., 25% ↑ on avoiding repeating
to hit objects with negative rewards), and update beliefs quickly (e.g., 9% faster
convergence rate on learning new concepts).
1	Introduction
We have witnessed rapid progress in applying deep neural networks to a broad spectrum of tasks
such as autonomous driving[7; 28], advertisement recommendation[1], and home assistant robot [18;
26; 13], etc. Without loss of generality, machine learning models can be expressed as y = f (x, z; θ),
where function f is parameterized by θ, x is the input, y is the output, and z is a vector of latent
variables. Considering partially-observed navigation tasks, as illustrated in Fig.1, latent variables are
(a) where traps are? (b) which category is a treasure? Moreover, (c) where is the “Prey”?. To enable
quick adaptation, intelligent models have to capture the uncertainty in both
•	Latent variable z. For example, (a) it is 80% that the cell below is a trap, (b) the vase is less
likely to be a treasure, and (c) feel like “Prey” has moved up. It is 70% that it is on the right
•	Model parameter θ. Even though the models are confident of z, say It is 95% that the cells
on the right and left are traps, they may still produce incorrect predictions if the situation is
less likely seen before, such as the trap has never been placed in that cell.
It is worth noting that, instead of phase shift (train → test), the values of latent variables vary either
cross episodes or over time steps, which makes quick adaption way more challenging.
Gaussian Process (GPs)[14] is well-known for capturing uncertainty, which specifies Gaussian priors
on θ and z. However, only the posterior q(θ∣X, y) is updated while q(z∣X, y) is fixed atp(z) during
inference. Therefore, in this paper, we introduce Amortized Posterior on Latent Variables in Gaussian
Process (APLV-GP) by replacing prior p(z) with posterior q(z∣X, y) (green box in Fιg.2) which is
then used to modulate the representation of data point x (purple box in Fig.2).
We evaluate our method to sinusoid regression, concept learning, and different reinforcement learning
tasks in dynamic and partially observed environments. Experiment results show that our method
consistently outperforms baselines. Furthermore, our method can adjust behaviors on the fly (e.g.,
blind “Predator” take 56% more chance to approach “Prey”), correct mistakes to escape bad situations
(e.g., 25% ↑ on avoiding repeating to hit objects with negative rewards), and update beliefs quickly
(e.g., 9% faster convergence rate on learning new concepts).
Our contribution is two-fold. First, we plug the posterior of latent variables into the Gaussian Process,
which results in less (or no) efforts to design, search or learn suitable kernels of the Gaussian Process.
1
Under review as a conference paper at ICLR 2022
(b) Hidden treasures
Posterior
Ss
Figure 1: Partially observed navigation tasks. (a) Invisible traps. We randomly place traps (in green), which
are invisible to agents, in a maze. Agents receive negative rewards if falling into a trap. (b) Hidden treasures.
Objects are visible to agents, but the treasure category (“dog” or “vase”?) is unknown. They receive +1 rewards
if taking a treasure, -1 otherwise. (c) “Predator” vs. “Prey”. “Predator” and “Prey” are invisible to each other.
They are only informed by how close they are (e.g., two blocks away). Notice that the environment is re-initiated
in every single episode. Given Hidden treasures as an example, agents perform on a series of episodes {(“dog”,
“vase”), (“car”, “cat”), (“snake”, “lizard”), (“vase”, “dog”), ... }, where categories in bold denote treasures. For
(“dog”, “vase”), “vase” is a treasure in one episode while “dog” becomes a treasure in another. In a particular
episode, agents start from a state (1st row) and perform tasks based on the prior (2ed row) or the posterior
(3rd row). Based on the prior, agents either repeatedly fall into the same trap or take objects which are not
treasures. Moreover, “Predator” sticks to the previous route without being aware of “Prey”’s movement. Instead,
performing on the posterior allows agents to walk around traps, avoid taking objects (not treasures), and redirect
“Prey” timely.
Experiments have proved that the RBF kernel is robust in solving real complex problems. Second,
we propose a plug-and-play regression module, which is flexible to integrate into various algorithms.
We demonstrate its effectiveness in Q-learning and A2C to solve navigation problems.
2	Related work
Our work can be categorized as a meta-learning algorithm using a stochastic process. A couple of
works relate to our work in terms of capturing uncertainty, leveraging previous experience, and a
variety of tricks to improve the capacity of the stochastic process.
Probabilistic meta learning MAML-based algorithms apply a variety of approximation strategies.
For example, the gradient update in MAML[3] produces samples from the posterior distribution of
parameters. However, it takes time to converge and introduces errors in the few-shot setting. Both
Probabilistic MAML [4] and Ravi’s work [23] attempt to find a surrogate distribution to approximate
the posterior and then average models or using Maximum a Posterior (MAP) at test time. Bayesian
MAML [11] attempt to find multiple, diverse MAML models based on Stein Variational Gradient
Descent [16]. Hierarchical MAML [36] aims at learning the MAML model for each cluster of tasks.
Thus, related tasks in the same cluster share the same model. However, our method is computationally
efficient without running multiple models, and it applies exact uncertainty estimation.
Context-based meta-RL. Recurrent [2; 34] and recursive [19] meta-RL methods adapt to new tasks
by encoding experience into a latent vector on which the policy is conditioned. However, they cannot
reason about task uncertainty. Rakelly’work [22] aims to predict hidden task variables given contexts
via minimizing the evidence lower bound (ELBO). However, it requires multiple full trajectories
(>200 steps) and keeps the hidden variables fixed during each episode. Instead, our method relies
2
Under review as a COnferenCe PaPer ar ICLR 2022
Uncertainty of f(xlz; θ)
⊃'	m
Z	e
y ∖χfxfy
LmearregreSS-On PrOb-em
K
Ail →
O
(XLyl)
P(Z)
2N(0a2)
LmearregreSS-On PrOb-em
y π (φx)τ e +。孕 m 〜N(。、I)⅛〜zprp)
3¼κ
(XLyl)
(a) GaUSS一an PrOCeSS (b) Our
FigUre 2 1 OVerVieW Of OUrmefhOd∙ TO CaPfUre UnCerfainfy in Z。(a) GaUSSian PrOCeSS SPeeifieS a GaUSSian PriOr
On Iarenr VariabIeS P(Z)〜 之PQ2) WhiCh is COnSranr w∙r.r COnreXr Ser 0 π(><>E ∙ Insreap (b) We SPeeify a
GaUSSian POSrerior q(z -X- y)〜N{μg Q2) Where mean-PC is PrOdUCed by a neural network WhiCh Cakes
COnfeXf SefΩ>u(X - y- as inpuf∙ Then。We apply elemenf—wise mulfiplicafion On ex and UtWhere feafure e(x)
COUId be ViSUaI feature =he IOCariOn Of agen- in a maze。efc = O PrOdUCe COnreXr—based represenraro∙n er To
CaPfUre UnCertainfy in 8 we foilOW fhe Same PrOfOCOI Wifh GaUSSian PrOCeSS according fo Eqn∙(3) and (4)∙
On exact: UnCeltaB∙ry esrimarion and focuses On adaprarδ∙n On a Sholterrime SCaPSay in 3 〜5 Hme
StePS ∙ Outside OfjiL “ MatChs∙g NetWOrkS〔32; 27j aim at Hnding IIeareSt IIeighbOrS Or PrOtotypes∙
ReIariOn nerwork〔29〕ProPOSeS Co CIaSSify WhiChdara Poin二har a query marches。AllrheSe methods
Brger SoIVing discrete PrObIemS" e∙g; CIaSSiHCarion ProbIemS" and IaCk UnCerraB∙ry esrimarion∙
NeUral PrOCeSS∙ NPS(6; 5;∞ProPoSe amortized models Co esrimas PoSreriOr mean and VarianCe Via
deep neural nerworks∙ One advanrage is rharB∙ference is fasCer SinCe ir OnlyreqUireS a SingIe forward
PaSS∙ HoWeVer=he inference is approXimared and IaCkS a Cheorerical guaranree∙ MoreoVeL NPS〔1。〕
are known to be UnderHtting to the COnteXt Set and datalhungry∙ In addito∙pthey are designed for
SUPerViSed Ieaming and do IIOt apply to domains like reinforcement learning。InStead“ We apply
BayeSian InferenCe Co ensure exact: UnCertaB∙ry esrimarion and resuming models are dara—eBcienr and
m rhe ObSerVed das PoinrS WeIL
GaUSSian process。GPS〔14〕are Probab≡stic and data—eBcienL WhiCh HtinfaSt adaptation WelI
WirhoUr SUKerB∙g fromB∙rensive COmPUrariOp HoWeVeL h SimPIy assumes a GaUSSian PriOr On Iarenr
VariableS and keeps it HXed as agents ObSerVe data POintS∙ In addition" it takes effort to SearChfor
SUitabIe kemels∙ DeeP Kemelleams∙g〔35〕attempts to Ieam more expressive kernels USing IIeUraI
networks。HoWeVeLdle PerformanCe is Sr≡far more behind die Srare—orhe—arr merhod∙ In rhis PaPeL
We replace the Prior OfIatent VariabIeS With an amortized GaUSSian PoSterioLWhiCh SigniHCantIy
reduces the efforts for kernel design。EVen With a SimPIe RBF kemeLoUrmethod Can beat SeVeraI
baselB∙esB∙solving real ProbIemS ∙
3 APPROACH
NOtati0n∙ We Iet x m 鬓 denote the input VeCtoL y denote the target value。X m %xd is the
data matriX Where Xd -S (he⅛∙h data POinL and y m^N is the target VeCtorWhere y∙ -S (he target
Of data POint X-. Xbm羽(Nl)Xd a^d yi.m羽(NI) are data matriX and target VeCtOr by removing
the 2th data point。plus" = - denotes the COnCatenation OfVeCtOrS and0denotes the elemem—wise
muapHCariOn Of VeCrOrS ∙
3∙1 AMoRTIZED PoSTERloR ON LATENT VARlABLES IN GAUSSlAN PRoCESS
GiVen COnteXt Set CH(K y}“ machine Ieaming models Can be expressed as y U f(XS Z&L z 〜
q(z - CL Where funcrion f is ParamereriZed by B and ZiSa VeCror OfIarenr VariabIes∙ The POSreriOr
Under review as a conference paper at ICLR 2022
q(Z | C) is the probability density of Z given context set c. When C = 0, q(Z | C) = p(z), where p(z)
is the prior. According to [12], we reparameterize the posterior as
z — μ c + σe, where C — h(φ(X), y; ω), e 〜N(0,1)	(1)
where matrix φ(X) is the aggregation of columns φχ , ∀i and function φ(.) projects a input vector
into feature space. h(.; ω) is a vector-valued function with parameters ω, which takes φ(X) and y as
inputs and outputs μ^ the mean of the posterior. E is Gaussian noise. Further, we plug Eqn.(1) into
function f in the following way
y — g(φ(x), 〃c； η) θ + σe —⇒ p(y | x; θ)〜N((ΦX)Tθ,σ2
X------------}
(2)
"{z
φcx
where g(., .; η) is a function that rewrites feature vector φ(x) as φc(x) by putting data point x in the
context of C = {X, y}, where we call φc(x) the context-based representation (detailed in Sec.3.2),
and θ is the weight vector.
1 -	1	∙ . r W ʌ τ	.	Ir•♦,，	.	r G ʌ 、 ' i 'i
For any data point {Xi, yi} in context set c, we define it S own context set as {X-i, y-i}. Then,
p(yi I Xi； θ) 〜N((φχ )Tθ, σ2). To capture the model uncertainty, we put a Gaussian prior on
parameter θ 〜p(0, Σp). Thus the target vector y follows a joint Gaussian distribution
p(y I X)〜N W KXK + σ2)	(3)
where KXX — (φX )TΣp(φX) and matrix φX is the aggregation of columns of φX , ∀i. In
Bayesian paradigm, we average over all possible parameter values, weighted by their posterior
probability (via Bayes’s rule). Thus the predictive distribution becomes
^	^	^(<7 I ^∙
p(y | χ, x, y) — p p(y i χ; θ)p(θ | X, y)dθ — p p(y i χ; θ) fp	dθ
Je	Je	θθ p(y IX; θ)p(θ)
〜N(Kx,X (KX,X + σ2I)-1y, Kx,x - Kx,X (KX,X + σ2I)-1KX J (4)
with KX X — (φχ)TΣp(φX) and KXX — (φX)TΣp(φχ). According to Gaussian Process, we
define kernel function k(x, x0) — ψ(φχ)Tψ(φχ,) where ψ(φχ) — Σp∕2φχ in Eqn.(3), (4).
Learning and inference. During training, we find parameters ω , φ, η, , σ and Σp (or kernel function
k) that maximize the joint probability in Eqn.(3). At inference time, models start from p(y I x) and
adjust their predictions according to Eqn.(4) in a particular episode (or a period of time), as shown in
Fig.2.
3.2	Context-based representation
The key idea of context-based representation in Eqn.(2) is to modulate feature φ(x) by the context
.	r	ʌ 'I T	♦	1 1 I-C C Λ r- ∏ Λ~l	t	♦	T-,∙ C	1
set c = {X, y}. Inspired by[33; 15; 24], as shown in Fig.2, we have
φχ — φ(x) 0 μc, where μc — MUItiheadAttention( [[φ(Xi); ɔzi]][j	(5)
where function h is parameterized by MultiheadAttention proposed by BERT [31; 17], which takes
the sequence ^Φ(Xi); yd]
as input and produces hidden vectors {hi}iN=1 via self-attention. We
use the hidden vector after pooling layer as μc.
A toy example. Let us use a binary classification problem to demonstrate how context-based
representation handles scenarios where hidden variables vary their values. We define function
μc — h(X, y; ω) and Sigmoid classifier as
， ,ʌ. , ʌ.
(0,	if φ(X^)j,i — φ(X)j,k ∀i — k
(μc)j = ∖ 1, if Pi^fj" > 0
[一1 otherwise
	(		0		
and y —	(1,	if σ((φcχ)T	0 1	> 0.5	(6)
	I。,	otherwise			
4
Under review as a conference paper at ICLR 2022
and assume observed data points and test data points are
	1	1	1	1	1
,ʌ . φ(X) =	0	0	0	,y = 1 (ii) , and φ(x)=	0	(7)
	1	2	—2	0	3
By applying Eqn.(6), we have
μc = [0 0 1]T, φX = φ(X) X μc = [0 0	3]T =⇒ y = 1	⑻
Let us look at the following two scenarios:
•	Label flip. In this case, hidden variable is which category is positive. In the context of Hidden
treasures (in Fig.1), it means “dog” becomes the treasure. We flip labels, i.e., 0 → 1 and 1 → 0. Then,
y = [0,0, 1]T. Again, applying Eqn.(6), μc =	[0,0,	-1]T and	φχ	=	φ(x)0 μc = [0,0,	-3]	=⇒
y = 0, which means the label of test data X is flipped accordingly.
•	Domain shift. In this case, hidden variable is domain. We assume the first two dimensions of
feature φ(X) are domain features. In Eqn.(7), it is [1 0]T. we simply change it to [0 1]T. Thus we
have
,ʌ.
Φ(X)
00	0	
11	1	and φ(X)
1 2 —2	
"031#
T
Applying Eqn.(6) again, μc = [0,0, 1] and φχ = φ(x) 0 μ© = 3 =⇒ y = 1, which shows that
our method is robust to domain shift.
3.3	Applications
In general, our method is applicable to any regression problems. In this section, we’ll plug our
method into reinforcement learning algorithms. First, we define state s, action a, next state s0, r(s, a)
as a reward of taking action a at state s, and short-term memory C = {X, y}. In replay buffer, a single
data point is expressed as {s, a, s, r, c}.
(i) Off-policy. Q-learning [20; 9] defines the optimal action-value function Q(s, a) as the maxi-
mum expected rewards by following any arbitrary policy, after observing state s and taking action a.
Q-learning solves a regression problem
Minimize (y — Q(s, a))2, where y = r(s, a) + Y * max Q(s0, a)	(9)
We let X = [s; a]. According to Eqn.(4), we define the stochastic policy as
π(a | S) = Softmaxa Q(s, a), where Q(s, a)〜 y | X = [s; a], C ={X, y}	(10)
(ii) On-policy [21; 25]. It is comprised of two components: “Actor” and “Critic”, where “Actor”
updates the policy π = p(a | s) and “Critic” estimates the value function, such as V (s) or Q(s, a),
which represent accumulated rewards. To enable a quick reaction, we introduce a function I (s),
which takes state s as input and outputs a scalar in range [0, 1], to represent the desire of reaching
state s.
Minimize (y — I(s))2, where y = r(S, a), S, a → S	(11)
Inspired by [2; 34; 19], we plug I(s) into “Actor”. Applying Eqn.(4), we have
∏(a | s,I(S)), where I(s)〜y | X = s,c = {X, y}	(12)
where S is the state space and I(S) is a map of I(S), ∀S ∈ S. For Invisible traps (in Fig.1), I(S) is the
probability of a cell not being a trap. And, for Hidden treasures (in Fig.1), I(S) means the probability
of a cell containing a treasure. As agents navigate in a maze, map I(S) is updated accordingly and
then guides the agents to decide which action to take. Note that I(S) is complementary to value
functions V (S) and Q(S, a). “Actor” and “Critic” are jointly learned via standard training protocols.
5
Under review as a conference paper at ICLR 2022
(a) EMAML
(b) NPs
(c) GPs
Figure 3: Sinusoid Regression. Column 1: y | X and column 2-4: y|x, X, y. Black star denotes observed data
points, the solid blue line denotes the mean of predictions, and cyan star denotes ground truth.
4	Experiments
In this section, we aim at answering questions: Can agents (i) capture changes in environments and
adjust behaviors accordingly? (ii) converge their beliefs faster as observing more data points? And
(iii) fit observed data points well and jump out of bad situations? We compare our method with
several baselines:
•	NPs. Neural Processes (NPS) [6; 5; 8] build amortized models for q(y | x, X, y)〜N(μy, σj)
where μy and σy are predicted by neural networks. They are originally applied in supervised learning.
Similar to Sec.3.3, we extend it to reinforcement learning.
•	EMAML. We train 3 MAML [3] models with random seeds and average predictions at test time.
•	GPs Gaussian Process with RBF kernel k(x, x0) = exp(-2∣∣φχ - φχo ∣∣2). Instead, our method
use k(x, x0) =exp(- 1 ∣∣ΦX - φχ,∣∣2)
•	DKL[35]. Gaussian Process with the learnable kernel. The kernel function is parameterized
by neural networks and learned end-to-end.
•	Non-Bayesian. We re-weight the observed data points based on data similarity 1
•	No Adaptation. Agents perform tasks solely relying on p(y | x) without adaptation.
4.1 Sinusoid Regression
We mainly compare the basic properties of GPs, NPs, and EMAML on this toy dataset. The sinusoids
have amplitude and phase uniformly sampled from range [0.1, 5.0] and [0, π]. The input space is
uniform on [-5.0, 5.0]. We further add Gaussian noise with zero mean and a standard deviation of
0.3 to the target labels, consistent with MAML [3]. In each episode, we allow models to observe 20
data points and receive a single test data point at a time (up to 9). Notice that, in [3], test data points
are presented at a time rather than in a sequence. To evaluate (and utilize) the uncertainty estimation,
we let models pick the most uncertain data point to observe at a time.
In Fig.3, the 1st column shows y | x. We can see that GPs and EMAML can model the sine-like
function, while NPs can not. Column 2〜4 show y | x, X, y. GPs and NPs can capture the uncertainty,
while EMAML can not. Furthermore, GPs are more accurate and sensitive in terms of uncertainty
estimation. It predicts low uncertainty around observed data points and high uncertainty over regions
far away from observations. As observing more data points, the uncertainty converges quickly. On
the other hand, NPs almost predict the same amount of uncertainty over the entire region. In addition,
both EMAML and NPs under-fit observed data points. We summarize the findings in Table.1.
1μX = Ewy [y],	∑x = VARwy [y], where Wyi = exp (φ(x)TΦ(Xi))/Pj exp (φ(x)TΦ(Xj))
6
Under review as a conference paper at ICLR 2022
Table 1: Basic properties according to Fig.3
Uncertainty Observed
YeszNo Sensitivity data
Figure 4: Concept learning. A concept defined by two
constraints c0 : "Triangle is on the bottom of Circle"
and c1 : "Circle is large". Top row: positive samples
and bottom row: negative samples. Clearly, if a model
only captures c0, it can correctly distinguish positive
and negative samples. Similarly, models that capture
c1 or c0 &c1 can do so as well.
EMAML X
NPs	X
GPs X X	X	X
(c) Model distribution
Num. samples
T-OUr T-EMAML T-NP
(b) Training tasks
(a) Observed data points
Figure 5: Results on Concept learning. Our method is sample efficient in terms of (a) the number of data points
observed in each task and (b) the number of meta-training tasks. (c) Model distribution converges to the true
distribution, i.e., p(c0) → 0.5 andp(c1) → 0.5.
4.2	Concept learning
In each episode, we take ten images as inputs, among which five images correctly represent the
specified concept. Each image has nine cells with three objects located in three cells, and each object
has three properties: color, shape, and size, sampled from the predefined vocabulary.
As shown in Fig.4, we use two constraints (relative location, color, or size) to define a concept.
Positive examples should satisfy both of them, while negative examples satisfy neither of them. Thus,
there are multiple plausible models, capturing c0, c1 or (c0&c1), that can produce correct predictions.
Similar to Sec. 4.1, we start with two images and annotate the most uncertain image at a time. As
shown in Table.2, our method outperforms all baselines. Fig.5a shows that our method can converge
faster by leveraging fewer examples and reaches higher final accuracy. Fig.5b shows that our method
is data-efficient in terms of the number of meta-training tasks. In Fig.5c, model distribution converges
quickly, i.e., p(c0) → 0.5 and p(c1) → 0.5, as observing more data points (see A.2).
4.3	Reinforcement learning: Navigation tasks
4.3.1	Random traps
As explained in Fig.1, we randomly placed traps in each episode. The unknown, fixed goal is to move
to the right corner of the grid. Therefore, agents have to guess where the traps are after a few attempts
and infer the shortest path. We allow agents to take up to 30 steps in the first attempt and measure
their performance in the 2ed attempt. The state is the agent’s location, the action is moving “up”,
“down”, “left” or “right”, and the reward is +1 if reaching the goal, 0 otherwise.
We learn Q-function according to Eqn.(9) and use stochastic policy in Eqn.(10) to execute task.
Experiments show that agents can obtain -0.02 reward at the 2ed attempt, conditioned on the 1st
attempt, and perform significantly better than picking the best path over 20 attempts, -0.618 reward,
(i.e., completing the task using stochastic policy 20 times, each is a single attempt).
Fig.6 illustrates a qualitative example. After a single attempt, the agents can walk around traps and
become more certain about which actions to take when revisiting the state.
7
Under review as a conference paper at ICLR 2022
(a) 1st attempt (b) 2ed attempt	(c) ∆ Entropy
Figure 6: Qualitative examples for random traps. Green cells are traps. ∆ entropy means the change in H(a | s).
“red” means ∆ < 0, where models become more confident about what action to take at the current state and
“green” implies the opposite.
Table 2: Our vs. baselines. We report classification accuracy for concept learning. Move: the number of moves
of perfect “prey”, # Pos - # Neg: the difference in the numbers of positive and negative objects, and Obsessive
hit: # hit -1, where # hit means how many times of repeating hitting a negative object.
	Concept learning	“Predator”Vs “Perfect”		Hidden treasure	
		Reward	Move (↑)	# Pos - # Neg	# Obsessive hit (1)
EMAML	0.58	-0.41	3	3.56	0.73
NPs	0.79	-0.39	16	0.26	1.38
DKL	0.67	-0.69	8	0.1	1.60
GPs	0.5	-0.35	24	0.0	1.68
Non-Bayesian	0.82	-0.50	12	0.2	1.51
No Adaptation	0.5	-1.0	2	0.1	1.6
Our	0.86	-0.34	25	5.5	0.55
4.3.2 Hidden treasures
As explained in Fig.1, in each episode, we randomly sample two categories from mini-imagenet
dataset [24; 30], each category with 20 images. Then, we randomly place those images in the grid.
The state is the agent’s location, action is moving “up”, “down”, “left” or “right”, and reward is the
number of positive objects minus the number of negative objects. We follow the experimental setup
in MAML [3] and pre-train function I(s) in Eqn.(11) where the true target value is +1 if the object is
a treasure, 0 otherwise. Then, we replace “Actor” in A2C [21] with Eqn.(12) and learn “Actor” and
“Critic” jointly via the standard A2C training protocol, while keeping I(s) fixed. Please check out
video clips2.
As shown in Table.2, our method beats all baselines. By averaging over 500 tasks, our method obtains
+5.5 rewards. In addition, to evaluate whether agents can escape bad situations, i.e., moving away
from negative objects, we delete (observed) negative objects until agents hit the next negative object.
We observe that our method has the lowest obsessive hit, which indicates that agents can adjust
behavior on the fly and correct mistakes.
Fig.7 shows the change in function I(s) over time. We can see that agents can quickly infer positive
objects and walk towards them while avoiding hitting negative objects.
4.3.3 “Predator” vs. “Prey”
The goal of “Predator” is to chase “Prey” while “Prey” attempts to run away from it. As demonstrated
in Fig.1, “Predator” and “Prey” are invisible to each other. The state is the agent’s location, the reward
is inversely proportional to their distance, and the action is moving “up”, “down”, “left”, or “right”.
Moreover, we restrict a limited short-term memory (say, 3 time steps) to store their recent states,
actions, and rewards.
We learn Q-function according to Eqn.(9) and use stochastic policy in Eqn.(10) to execute task. We
evaluate our method in the “Predator” vs. “Perfect” setting where only “Predator” is end-to-end
learnable while “Prey” is manually pre-designed and always takes correct moves. For example, we let
“Predator” move 100 steps and “Prey” only move when “Predator” is a single step away. In Table.2,
we report averaged rewards and number of moves of perfect “Prey”. We can see that our method
beats all baselines with > 15% relative improvement. In addition, there is 56% more chance (w.r.t.
NPs) that ‘Predator” is successfully a single step away from “Prey”. We further upgrade the game
2HT.clip0.mov, HT.clip1.mov
8
Under review as a conference paper at ICLR 2022
Figure 7: I(s). The first column is the environment with two categories, each has 20 images. The positive
objects are highlighted in yellow edges. Column 2〜4 show how map I(S) changes as agents gather more
information. From step 0 to step 2, agents collect two positive objects at (6, 6), (5, 3). Based on the gathered
information, I(s) is updated accordingly. With the updated I (s), agents collect two more positive objects at
(2, 4), (2, 5) without hitting negative objects.
Figure 8: Qualitative examples for “Predator” vs. “Prey”. In this setting, both agents are learned end-to-end. We
sample 2 sequences of consecutive moves (11, 0) → (17, 1) and (65, 21) → (77, 24) where (x, y) indicates
the number of moves of “Predator” (green) and “Prey” (red). To save space, we skip several moves, e.g.,
(31, 7) → (36, 7), but plot the last three moves with a lighter color to display the trajectory. In addition, we
zoom in on the grid map and only plot 5 × 5 cells. Note that, in this setting, we also restrict “Prey” to move only
when “Predator” is a single step away.
to “Predator” vs. “Prey” where both agents are learned end-to-end. Similar to what we observed in
“Predator” vs. “Perfect”, “Predator” gets a -0.34 reward on average and has a 99% chance to make
correct moves. “Prey” takes 30 moves in total and 76% is correct. Please check out video clips3
Fig.8 illustrates two sequences of consecutive moves in “Predator” vs. “Prey” setting. (1) (11, 0) →
(17, 1), “Predator” first moves up until (13, 0) where the reward drops and then moves back. At the
moment, “Predator” is uncertain about which side (right or lef t) “Prey” is on. It simply attempts to
turn lef t. After realizing the reward drops, it moves back and turns right. (2) (65, 21) → (72, 21),
“Predator” receives higher rewards by moving up and right. Then, it continues to move up after
“Prey” moved down. However, with only a single wrong move, “Predator” immediately turns right
and moves down. Clearly, our agents are able to reduce ambiguity via exploration (Eqn.10) and
quickly react to the dynamic environment.
5 Conclusion
We proposed a plug-and-play regression module to handle dynamic, partially observed environments,
flexible, data-efficient, and applicable to large-scale, real problems. Our model can capture uncertainty,
absorb knowledge and react to changing environments quickly. Further research can leverage a
massive amount of off-policy data and extend our module to “high-level” abstracted spaces for
continuous, high-dimensional reinforcement learning tasks. Furthermore, We can improve our
models by learning to utilize uncertainty estimation, e.g., learnable exploration in RL tasks.
3Prey.clip0.mov, Prey.clip1.mov, Prey.clip2.mov
9
Under review as a conference paper at ICLR 2022
References
Norris I Bruce, BPS Murthi, and Ram C Rao. A dynamic model for digital advertising: The effects
of creative format, message content, and targeting on engagement. Journal of marketing research,
54(2):202-218, 2017.
Yan Duan, John Schulman, Xi Chen, Peter L Bartlett, Ilya Sutskever, and Pieter Abbeel. RlΘ2: Fast
reinforcement learning via slow reinforcement learning. arXiv preprint arXiv:1611.02779, 2016.
Chelsea Finn, Pieter Abbeel, and Sergey Levine. Model-agnostic meta-learning for fast adaptation of
deep networks. In Proceedings of the 34th International Conference on Machine Learning-Volume
70, pp. 1126-1135. JMLR. org, 2017.
Chelsea Finn, Kelvin Xu, and Sergey Levine. Probabilistic model-agnostic meta-learning. In
Advances in Neural Information Processing Systems, pp. 9516-9527, 2018.
Marta Garnelo, Dan Rosenbaum, Chris J Maddison, Tiago Ramalho, David Saxton, Murray Shanahan,
Yee Whye Teh, Danilo J Rezende, and SM Eslami. Conditional neural processes. arXiv preprint
arXiv:1807.01613, 2018a.
Marta Garnelo, Jonathan Schwarz, Dan Rosenbaum, Fabio Viola, Danilo J Rezende, SM Eslami, and
Yee Whye Teh. Neural processes. arXiv preprint arXiv:1807.01622, 2018b.
Andreas Geiger, Philip Lenz, and Raquel Urtasun. Are we ready for autonomous driving? the kitti
vision benchmark suite. In 2012 IEEE Conference on Computer Vision and Pattern Recognition,
pp. 3354-3361. IEEE, 2012.
Jonathan Gordon, Wessel P Bruinsma, Andrew YK Foong, James Requeima, Yann Dubois, and
Richard E Turner. Convolutional conditional neural processes. arXiv preprint arXiv:1910.13556,
2019.
Tuomas Haarnoja, Haoran Tang, Pieter Abbeel, and Sergey Levine. Reinforcement learning with
deep energy-based policies. In International Conference on Machine Learning, pp. 1352-1361.
PMLR, 2017.
Hyunjik Kim, Andriy Mnih, Jonathan Schwarz, Marta Garnelo, Ali Eslami, Dan Rosenbaum, Oriol
Vinyals, and Yee Whye Teh. Attentive neural processes. arXiv preprint arXiv:1901.05761, 2019.
Taesup Kim, Jaesik Yoon, Ousmane Dia, Sungwoong Kim, Yoshua Bengio, and Sungjin Ahn.
Bayesian model-agnostic meta-learning. arXiv preprint arXiv:1806.03836, 2018.
Diederik P Kingma and Max Welling. Auto-encoding variational bayes. arXiv preprint
arXiv:1312.6114, 2013.
Eric Kolve, Roozbeh Mottaghi, Winson Han, Eli VanderBilt, Luca Weihs, Alvaro Herrasti, Daniel
Gordon, Yuke Zhu, Abhinav Gupta, and Ali Farhadi. Ai2-thor: An interactive 3d environment for
visual ai. arXiv preprint arXiv:1712.05474, 2017.
Malte Kuss and Carl E Rasmussen. Gaussian processes in reinforcement learning. In Advances in
neural information processing systems, pp. 751-758, 2004.
Jimmy Lei Ba, Kevin Swersky, Sanja Fidler, et al. Predicting deep zero-shot convolutional neural
networks using textual descriptions. In Proceedings of the IEEE International Conference on
Computer Vision, pp. 4247-4255, 2015.
Qiang Liu and Dilin Wang. Stein variational gradient descent: A general purpose bayesian inference
algorithm. In Advances in neural information processing systems, pp. 2378-2386, 2016.
Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike
Lewis, Luke Zettlemoyer, and Veselin Stoyanov. Roberta: A robustly optimized bert pretraining
approach. arXiv preprint arXiv:1907.11692, 2019.
10
Under review as a conference paper at ICLR 2022
Manolis Savva*, Abhishek Kadian*, Oleksandr Maksymets*, Yili Zhao, Erik Wijmans, Bhavana Jain,
Julian Straub, Jia Liu, Vladlen Koltun, Jitendra Malik, Devi Parikh, and Dhruv Batra. Habitat: A
Platform for Embodied AI Research. In Proceedings of the IEEE/CVF International Conference
on Computer Vision (ICCV), 2019.
Nikhil Mishra, Mostafa Rohaninejad, Xi Chen, and Pieter Abbeel. A simple neural attentive meta-
learner. arXiv preprint arXiv:1707.03141, 2017.
Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Alex Graves, Ioannis Antonoglou, Daan
Wierstra, and Martin Riedmiller. Playing atari with deep reinforcement learning. arXiv preprint
arXiv:1312.5602, 2013.
Volodymyr Mnih, Adria Puigdomenech Badia, Mehdi Mirza, Alex Graves, Timothy Lillicrap, Tim
Harley, David Silver, and Koray Kavukcuoglu. Asynchronous methods for deep reinforcement
learning. In International conference on machine learning, pp. 1928-1937. PMLR, 2016.
Kate Rakelly, Aurick Zhou, Chelsea Finn, Sergey Levine, and Deirdre Quillen. Efficient off-policy
meta-reinforcement learning via probabilistic context variables. In International conference on
machine learning, pp. 5331-5340. PMLR, 2019.
Sachin Ravi and Alex Beatson. Amortized bayesian meta-learning. 2018.
Sachin Ravi and Hugo Larochelle. Optimization as a model for few-shot learning. 2016.
John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy
optimization algorithms. arXiv preprint arXiv:1707.06347, 2017.
BokUi Shen, Fei Xia, ChengshU Li, Roberto Martin-Martin, Linxi Fan, Guanzhi Wang, Shyamal
Buch, Claudia D’Arpino, Sanjana Srivastava, Lyne P Tchapmi, Kent Vainio, Li Fei-Fei, and Silvio
Savarese. igibson, a simUlation environment for interactive tasks in large realistic scenes. arXiv
preprint, 2020.
Jake Snell, Kevin Swersky, and Richard Zemel. Prototypical networks for few-shot learning. In
Advances in neural information processing systems, pp. 4077-4087, 2017.
Pei SUn, Henrik Kretzschmar, Xerxes Dotiwalla, AUrelien ChoUard, Vijaysai Patnaik, PaUl TsUi,
James GUo, Yin ZhoU, YUning Chai, Benjamin Caine, Vijay VasUdevan, Wei Han, JiqUan Ngiam,
Hang Zhao, Aleksei Timofeev, Scott Ettinger, Maxim Krivokon, Amy Gao, Aditya Joshi, YU Zhang,
Jonathon Shlens, Zhifeng Chen, and Dragomir AngUelov. Scalability in perception for aUtonomoUs
driving: Waymo open dataset. In Proceedings of the IEEE/CVF Conference on Computer Vision
and Pattern Recognition (CVPR), JUne 2020.
Flood SUng, Yongxin Yang, Li Zhang, Tao Xiang, Philip HS Torr, and Timothy M Hospedales.
Learning to compare: Relation network for few-shot learning. In Proceedings of the IEEE
conference on computer vision and pattern recognition, pp. 1199-1208, 2018.
Eleni TriantafilloU, Tyler ZhU, Vincent DUmoUlin, Pascal Lamblin, UtkU Evci, Kelvin XU, Ross
Goroshin, Carles Gelada, Kevin Swersky, Pierre-Antoine Manzagol, et al. Meta-dataset: A dataset
of datasets for learning to learn from few examples. arXiv preprint arXiv:1903.03096, 2019.
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,匕UkaSZ
Kaiser, and Illia PolosUkhin. Attention is all yoU need. In Advances in neural information
processing systems, pp. 5998-6008, 2017.
Oriol Vinyals, Charles BlUndell, Timothy Lillicrap, Daan Wierstra, et al. Matching networks for one
shot learning. In Advances in neural information processing systems, pp. 3630-3638, 2016.
Risto VUorio, Shao-HUa SUn, Hexiang HU, and Joseph J Lim. MUltimodal model-agnostic meta-
learning via task-aware modUlation. arXiv preprint arXiv:1910.13616, 2019.
Jane X Wang, Zeb KUrth-Nelson, DhrUva TirUmala, HUbert Soyer, Joel Z Leibo, Remi MUnos,
Charles BlUndell, Dharshan KUmaran, and Matt Botvinick. Learning to reinforcement learn. arXiv
preprint arXiv:1611.05763, 2016.
11
Under review as a conference paper at ICLR 2022
Andrew Gordon Wilson, Zhiting Hu, Ruslan Salakhutdinov, and Eric P Xing. Deep kernel learning.
In Artificial intelligence and statistics, pp. 370-378, 2016.
Huaxiu Yao, Ying Wei, Junzhou Huang, and Zhenhui Li. Hierarchically structured meta-learning. In
International Conference on Machine Learning, pp. 7045-7054. PMLR, 2019.
A	Appendix
A. 1 Application: multi-class Logistic regression
We reformulate it as a linear regression problem.
Minimize (r(s, C) - S(s, c))2, whereS(x, C) = φ(x)TW + Z	(13)
where, r(x, c) = 1.0 if c is the true category, otherwise 0. At test time, we pick the category with the
highest probability
exp (S(x, C))
y = argmaXc --------o——0 , where S(x, C)〜y | φ(x), φ(Mχ), My	(14)
c0 exp S (x, C )
A.2 Concept learning
A.2. 1 Evaluation setup
At test time, we maintain two sets: labeled images So and un-labeled images Su . We start
with two images and annotate the most uncertain image in Su at each time, according to
argmaxx∈S p(S(x, C) < 0.5) - p(S(x, C) > 0.5), where S(x, C) is the posterior score at C = 1
for un-labeled image x, as defined in Eqn.(14). We evaluate 200 tasks that are never seen during
meta-training and report averaged classification accuracy.
A.2.2 Model distribution.
In Concept learning, We use two constraints to define a concept: co&ci as positive, -C0&-C1 as
negative. Thus, either c0 or c1 can explain the data. We define the model distribution as
P(cj) =s(cj)/Xs(cj), s(cj) = XP(i)	(15)
j	i=cj
P(i) is the probability of image i being positive (i.e., softmax output) and i = cj means image i
satisfies constraint Cj. We generate 150 images: 50 for C0&-C1,50 for -co&ci and 50 for -co&-ci.
We report averaged results over 200 tasks. The real distribution is P (c0) = 0.5, P(c1) = 0.5, else 0,
as shown in Fig.5c
12