Under review as a conference paper at ICLR 2022
Equivariant and
Invariant Reynolds Net-
WORKS
Anonymous authors
Paper under double-blind review
Ab stract
Invariant and equivariant networks are useful in learning data with symmetry, in-
cluding images, sets, point clouds, and graphs. In this paper, we consider invariant
and equivariant networks for symmetries of finite groups. Invariant and equivari-
ant networks have been constructed by various researchers using Reynolds op-
erators. However, Reynolds operators are computationally expensive when the
order of the group is large because they use the sum over the whole group, which
poses an implementation difficulty. To overcome this difficulty, we consider rep-
resenting the Reynolds operator as a sum over a subset instead of a sum over the
whole group. We call such a subset a Reynolds design, and an operator defined
by a sum over a Reynolds design a reductive Reynolds operator. For example,
in the case of a graph with n nodes, the computational complexity of the reduc-
tive Reynolds operator is reduced to O(n2), while the computational complexity
of the Reynolds operator is O(n!). We construct learning models based on the
reductive Reynolds operator called equivariant and invariant Reynolds networks
(ReyNets) and prove that they have universal approximation property. Reynolds
designs for equivariant ReyNets are derived from combinatorial observations with
Young diagrams, while Reynolds designs for invariant ReyNets are derived from
invariants called Reynolds dimensions defined on the set of invariant polynomials.
Numerical experiments show that the performance of our models is comparable
to state-of-the-art methods.
1 Introduction
The universal approximation theorem in machine learning states that any continuous function can
be approximated by a deep neural network, but in a practical situation, there are many cases where
learning fails. It is believed that this is because the number of parameters used is too large to learn
an appropriate model from a hypotheses set. Therefore, researchers developed task-specific models,
such as convolutional networks for image input, and obtained better results. On the other hand,
Zaheer et al. (2017) developed a model using permutation invariance and permutation equivariance,
and obtained good experimental results as well as theoretical development. Such a model was gen-
eralized by Maron et al. (2019b) for subgroups of permutation groups. The problem with this model
is that the shape of the parameter sharing needs to be calculated mathematically for each width of
the equivariant layer. This mathematical difficulty is a major obstacle for practical applications of
this model, which cannot have universality unless the width is sufficiently large Chen et al. (2019).
Therefore, we take a different approach than they do. In this paper, we develope an equivariant and
invariant deep neural network model for the action of permutation groups on higher-order tensor
spaces using Reynolds operators. An important application of these model is the task of using sets,
graphs and hypergraphs as input.
The central part of our idea is to use the equivariant Reynolds operator. The equivariant Reynolds
operator has the property of transforming a function into an equivariant function, and by adapting
this operator to a deep neural nets, we obtain a class of equivariant neural networks. Such attempts
have been made in the past, for example byYarotsky (2021); Kicki et al. (2021); van der Pol et al.
(2020); Mouli & Ribeiro (2020); Murphy et al. (2018), but the use of Reynolds operator causes com-
putational problems when the order of the groups is large in the part that computes the equivariant
Reynolds operator.
1
Under review as a conference paper at ICLR 2022
Therefore, we introduce the concept of Reynolds design, which is an analog of spherical design (
Bannai & Bannai (2009) ). Here, a t-spherical design is a finite subset H ⊂ Sd-1 such that any
polynomial f with degree at most t satisfies
∣sd-1∣ LI f(x)dx = l⅛ X∈H f(x).
(1)
Using the spherical design, the computation over the whole of Sd-1 can be reduced to that over the
small subset H . Similarly, a Reynolds design is a subset H of a group G that realizes
卷 X f(g ∙x) =击 X f(g ∙χ)
|G| g∈G	|H | g∈H
for a target function f, where g ∙ X is the action of g to x. We call the operator defined on the right-
hand side the reductive Reynolds operator, and use this operator instead of the Reynolds operator to
construct equivariant and invariant networks. The advantage of this construction is that it preserves
the universal approximation property of the construction using Reynolds operators. Also, in many
cases, the Reynolds design can be much smaller than the original G. To formulate this fact, we use
the space of higher order tensors. The space of higher order tensors is the space that reflects the
adjacency matrices and tensors of graphs and hypergraphs, and it is through this space that models
are defined when using equivariant models. For this space, we prove a representation theorem for
equivariant functions. Technically speaking, we use the relation between Young diagrams and the
representation of symmetric groups on higher-order tensor spaces (Sagan (2001)). As a result, in
the case of graphs with n nodes, the order of the group is n! but we have shown that there exist
Reynolds designs of order n(n - 1). Thus, by using Reynolds designs, we can avoid the problem of
computational complexity.
For the invariant network we follow the same structure as in the case of CNNs. In other words, we
first extract the features in an equivariant network and then transform them in a deep neural network.
At this point We can naively consider the following phenomenon: if We want to approximate n (x∖ +
..+ Xn) = ∣sq Pg∈sn g ∙ χ1, we assume that we use Reynolds operator, so in effect we only need
to approximate xi1. In order to formulate this phenomenon we introduce an invariant called the
Reynolds dimension.Briefly, the Reynolds dimension is the number of variables required for the
input of the functions before they are converted by the Reynolds operator, when the model had
universality in the above construction. It is not clear what the Reynolds dimension is when taking
a higher order tensor space as input, but our expectation is that it is independent of the dimension
n. We prove that we can construct a Reynolds design when the Reynolds dimension is fixed. We
also find that all input variables are not necessary when the Reynolds dimension is low, therefore we
construct networks with reduced inputs (reduced ReyNets).
Using these models, we conduct experiments on synthetic data. In addition, as an implementation,
we use the above representation theorem to develop the Corner MSE, which restricts the loss func-
tion for an equivariant function to two components. As a result, our method significantly outperform
the baseline of FNN and Maron’s model, confirming its consistency with the theoretical part.
2 Previous Work
Equivariance and Invariance. Various machine learning tasks aims to approximate a certain target
map such as the labeling function in classification and regression. When symmetries exist behind
data, the target map often have invariance or equivariance to the symmetries. In such cases, invariant
or equivariant networks are effective and efficient to approximate the target map because the model
complexity can be significantly reduced than neural networks without specific structure for the sym-
metries. Convolutional neural networks (CNNs) are well-known as a seminal equivariant model to
translation symmetry (LeCun et al. (1989); Krizhevsky et al. (2012)). Inspired by the success of
CNNs, various equivariant models have been proposed. Besides continuous symmetries such as
translation, symmetries of finite groups often appear in many machine learning tasks. In the case
where sets or point clouds are inputs, the target functions are typically invariant to the order of data
points. Then, this function has invariance to the permutation group on data points (Qi et al. (2017);
Zaheer et al. (2017)). In the case where graphs or hyper-graphs are inputs, the symmetry is repre-
sented by permutation on a tensor product space. Gori et al. (2005); Scarselli et al. (2008) introduced
2
Under review as a conference paper at ICLR 2022
graph neural networks. Various researchers generalized convolution to the setting of graphs moti-
vated by CNNs (Bruna et al. (2013); Henaff et al. (2015); Kipf & Welling (2016); Defferrard et al.
(2016); Levie et al. (2018)). Recently, Kondor et al. (2018); Maron et al. (2019a); Chen et al. (2019)
investigate graph neural networks. Hartford et al. (2018) consider interaction between sets. Gra-
ham & Ravanbakhsh (2019) consider relational databases as generalization of graphs and provide
equivariant models to handle relational databases. Maron et al. (2020) consider sets of symmetric
elements as input data.
Universality. The expressive power of learning models is mathematically validated by universal
approximation theorems. Many universal approximation theorems have been proved for different
conditions. Invariant models with universal approximation property are provided for pointclouds
networks and sets network (Qi et al. (2017); Zaheer et al. (2017)), networks with matrices and
higher-order tensors as input (Hartford et al. (2018)), graph and hyper-graph networks (Maron et al.
(2018); Nguyen & Maehara (2020)), and networks invariant to the action of finite groupMaron et al.
(2019b). The universality of equivariant models for finite group are proved by Ravanbakhsh et al.
(2017). On the other hand, Oono & Suzuki (2019) proved that graph neural networks of message
passing type are over smoothing when many layers are stacked. There are also learning models with
universality in other settings (Yarotsky (2021); Keriven & Peyre (2019); Maehara & NT (2019);
Segol & Lipman (2019)).
3 Reynolds Operator and Reynolds Designs
In this section, we organize the mathematical framework for dealing with symmetric networks. The
first objects to be considered are the following equivariant and invariant functions.
Definition 1 (Invariant / Equivariant Function). For a group G acting on RN and RM, a function
f : RN → RM is invariant if f (g ∙ x) = f (x) holdsfor any g ∈ G and any X ∈ RN, and equivariant
if f (g ∙ X)= g ∙ f (x) holdsfor any g ∈ G and any X ∈ RN.
The next operator, called the Reynolds operator, plays a central role in this paper.
Definition 2 (Reynolds Operator (cf. Mumford et al. (1994), Definition 1.5)). For a group G, the
followings are called the equivariant and invariant Reynolds operator respectively:
TG(Aj) = |G| X g-1 ∙ f(g ∙ -),	YG(Aj) = |G| X f(g ∙j.
|G| g∈G	|G| g∈G
(2)
More generally, for a subset H in G*1, we define τH and γH by replacing G by H in (2).
The equivariant Reynolds operator converts an arbitrary map to a equivariant map. Our idea is to
use this operator to convert a fully connected deep neural network into an equivariant function.
However, as we see in a moment, the computational complexity of Reynolds operators increases
when the order of G is large, for example, for symmetric groups of order n!.
We define the following notion of Reynolds design to reduce the computational complexity. The
concept of Reynolds design is first proposed in this paper.
Definition 3 (Reynolds Design). The Reynolds design H of a function f is a subset H of G that
satisfies the following equation.
TG(Aj) = G X g-1 ∙ f(g ∙j = ∣hh∣ X g-1 ∙ f(g ∙j.
|G| g∈G	|H | g∈H
In this case, TH is called the reductive Reynolds operator. Furthermore, the Reynolds design H ofa
set of functions F is defined by the property that the above equation for H holds for all f ∈ F.
As an example, consider the following two cases. (1) When f is an invariant function In this case,
f = TG(f) is valid. In other words, we can take H = {id} as the Reynolds design. (2) When f is a
power of one variable. Since TSn (xl) = n (x1 + + x^), the number of terms in the summation
*1The subset H in G may not be a subgroup of G.
3
Under review as a conference paper at ICLR 2022
Young Diagram	(W)	(2,1) F	(2,1) F	(2,1) F	(3) rm
Basis Tableau T	ɪ	F	F	空	
Basis Tableau Vector θτ	θl23	θll2	θl21	θ211	θlll
Figure 1: Young diagrams and basis tableaux for m = 3, and the corresponding basis tableau vectors
in Rn3 = Rn Z Rn Z Rn.
is only n, which can be written as an orbit by the cyclic group Cn of order n as TSn (xj) = 1 (x1 +
•一+ Xn) = TCn (xl). Hence Cn is the Reynolds design of xj.
As these examples show, the Reynolds design significantly reduces the computational complexity of
the Reynolds operator. Our model is based on such a computational principle.
4 Representation Theorem for Equivariant Maps
In this section we see a representation theorem for equivariant functions by discussing a mathemat-
ical object called a Young diagram. An important application of the representation theorem is in
the case of graph representations, where the Reynolds design H can be of order n2 for graphs with
n-nodes. As we will see later, this can be used to construct a universal equivariant/invariant graph
model where the Reynolds operator requires only n2 calculations.
4.1	Higher order tensors and adjacency matrices
Graphs and hypergraphs can be represented as adjacency matrices or higher order tensors when the
ordering of their nodes is fixed. The main problem with using this representation for deep learning
is that the trained model may produce different outputs depending on the permutation of node or-
derings. For example, in a graph classification problem, we cannot produce different classification
results by replacing the ordering of the nodes. Therefore, we want the model to produce the same
output. To avoid this problem, we define the appropriate action induced by node permutation in
the adjacency matrix or higher order tensor. The action of Sn on tensors X ∈ Rn ×a (the last
index, denoted ɑ represents feature depth) is defined by (g ∙ X)i1...il,a = Xg-i(i.…g-i(iι),α. Note
that when m = 1, this action represents the permutation of the index on the vector representation
(x1, .., xn) of the set {x1, .., xn}, and when m = 2, it represents the action of the node permutation
on the adjacency matrix of the graph.
4.2	Basis and Young Diagrams
Young diagrams are combinatorial objects that are famous for having a one-to-one correspondence
with irreducible representations of symmetric groups. We discussed representations of symmetric
groups on the spaces of matrices and higher-order tensors, and since representations in general
decompose into sums of irreducible representations, we can discuss these spaces in terms of Young
diagrams. We prove a representation theorem for equivariant functions based on this mathematical
background. Consider an example, where e121 = e1 Z e2 Z e1 , one of the bases of Rn3 = Rn Z
Rn Z Rn. , is represented by a sequence of boxes with numbers in them. We consider representing
e121 = e1 Z e2 Z e1 , one of the bases of Rn3 = Rn Z Rn Z Rn, by arranging boxes and putting
numbers in them. The following procedure can be used to construct the representation by the young
diagram. First, since the number of e1 in e121 is two, we put two boxes in the first line, and since
the number of e2 in e121 is one, we put one box in the second line. Next, the tensor numbers where
e1 appears are 1 and 3, so put 1 and 3, respectively, in the box on the first line and the tensor number
where e2 appears is 2, so put 2 in the box on the second line. Thus we obtain the diagram in Figure
4
Under review as a conference paper at ICLR 2022
1 The general case of this procedure is as follows; For eu = e3 0 eu2 0∙∙∙ eun ∈ RnK , first PUt
as many boxes on the first row as there are e1’s appearing in eu. Then put as many boxes on the next
line as there are e2 ’s appearing in eu.Put as many boxes on the last line there are en’s appearing
in eu . Next, put the tensor numbers where e1 appears into each box in the first row, in decreasing
order. Put the tensor numbers where e2 appears into each box in the next row, in decreasing order.
... Put the tensor numbers in which en appears in each box of the last line in decreasing order. This
procedure allows us to represent the basis ofa tensor space as a combination of Young diagrams and
numbers.
4.3 Basis Tableau and Representation Theorem
In order to justify the above procedure, we first define a Young diagram, which is the set of boxes
in the above example. A Young diagram is a way to represent the division of a natural number m.
Here, division means to express a natural number m as a sum of several non-negative integers.
Definition 4 (Young Diagram). Let D ∈ [m] be fixed. A vector k = (k1, . . . , kD) of natural
numbers k1, . . . , kD is called a Young diagram if it satisfies m = k1 + . . . + kD and k1 ≥ . . . ≥ kD.
This partition can be represented by a total of m boxes, consisting of D rows with ki boxes in the
d-th row. Here, each row is left-justified. Next, we define a basis tableau, which is a Young diagram
with numbers in it.
Definition 5 (Basis Tableau). Let n ≥ m and k = (k1 , . . . , kD) a Young diagram. A vector
T = (t1, . . . , tD) of vectors td = (td,1, . . . , td,kd) ∈ [n]kd is called basis tableau of depth D if it
satisfies the following conditions:
1.	(Different components) td,w 6= td0,w0 for (d, w) 6= (d0, w0).
2.	(Row monotonicity) td,1 < td,2 < . . . < td,kd for each d ∈ [D].
3.	(Partial column monotonicity) If kd = kd+1, then td,1 < td+1,1 for d = 1, . . . , D - 1.
Figure 1 shows an example of basis tableaux for m = 3. We denote the set of basis tableaux for
m of depth D by Tm,D, and the set S1≤D≤m Tm,D of basis tableaux with at most depth m by Tm.
In the above procedure, not all bases become basis tableaux, but only certain special bases become
basis tableaux. We call such a basis a basis tableau vector. However, any basis can be converted to a
basis tableau vector when moved by the action of the symmetric group. This fact is the core of the
proof of Theorem 7.
Lemma 6 (Basis Tableau Vector). Let n ≥ m. For any D ∈ [m] and T = [t1, . . . , tD] ∈ Tm,D,
then there exists an unique vector eT ∈ Rnm, which gives the basis tableau T in the above proce-
dure.
Figure 1 shows an example of basis tableaux vectors when m = 3. Next, we review the linear map
induced by higher-order tensors. For X ∈ Rnm, X induces the linear map by
X b : Rb 3 (aι,...,ab) → (aιX,...,abX) ∈ Rnm×b.	(3)
In the following, we mainly treat the case where X = eτ (i.e., Xb = eτ,b). Then, we have the
following theorem.
Theorem 7 (Representation Theorem). Let n ≥ m and G = Sn. For any equivariant continuous
map F : Rnl×a → Rnm×b, there exist continuous maps FT : Rnl ×a → Rb indexed by basis
tableaux T ∈ Tm such that
m
F = X X THD (eτ,b ◦ FT),
D=1 T∈Tm,D
where Cn-i denote the cyclic group of order n - i on the set {i + 1, .., n} and
HD := Cn ◦•••◦ Cn-D+1 = {σD ∙ σD-1 ∙∙∙ σ1 | σi ∈ Cn-D+i (i = n,n - 1,...,n - D + I)}.
Furthermore, HD is a Reynolds design of ^τ,b ◦ FT for any T ∈ Tm,,D.
5
Under review as a conference paper at ICLR 2022
4.4 COMPUTIONAL COMPLEXITY OF HD
The most important application, the case of the graph, corresponds to the m = 2 case. In this case,
the Young diagrams are k = (2) and (1, 1), and the basis tableaux is also uniquely numbered in
the above two. For the Reynolds design, H1 is of order n and H2 is of order n(n - 1), so the total
computational complexity is O(n2). The number of summations that were originally required n!
times has been reduced to n2 times, resulting in a significant reduction in the amount of calculation.
We can see some experiments on these in the experiments section.
5 Equivariant ReyNets and Universality
In this section, we define equivariant models using Theorem 7.
Definition 8 (Equivariant Reynolds Nets). We assume that n ≥ m. For any basis tableaux T ∈ Tm,
Let NT : Rnl ×a → Rb be a Multi layer perceptron. The map E : Rnl ×a → Rnm ×b
m
E = X X THD (eT,b ◦NT),
D=1 T∈Tm,D
is called reduced Equivariant Reynolds Nets (equivariant ReyNets).
Theorem 7 guarantees the universal approximation property of this model.
Theorem 9 (Universality). We assume that n ≥ m and G = Sn. Let F : Rnl ×a → Rnm ×b
be a continuous equivariant function. For any compact set K ⊂ Rn ×a , there exists a equivari-
ant Reynolds network that approximates F to an arbitrary precision on K. Namely, equivariant
Reynolds nets are a universal approximator for equivariant functions.
6	Invariant ReyNets and Universality
In this section, we define the invariant models using the equivariant models defined in the previ-
ous section. First, we will define invariant Reynolds nets. Our construction is to first stack in an
equivariant model, then take its orbit sum, and finally stack the MLP. Our main contribution in this
section is that we can reduce the dimensionality of the input space of the MLP, which is the basis of
the equivariate model.As an example, let us consider approximating the function f = xi1 + ... + xin.
Since f can be written as f = Σ(τG(xi)) using the orbit sum Σ, only one variable is needed as input.
We can generalize these properties and define an invariant called Reynolds dimension. Finally, we
explain the universal approximation of invariant networks using Reynolds dimension. We will deal
with the case where a general finite group G acts on an n-dimensional vector space or a set of n
variables. Note that such formulations include the case of higher order tensor space as G = Sn .
Definition 10 (Invariant Reynolds Nets). A invariant Reynolds network (invariant ReyNet) is a func-
tion I : Rnl×a → R defined as
I = M。Σ OE,
where E : Rnl ×a → Rnm×b is a equivariant Reynolds network, Σ is the orbit sum*2, and M is a
Multi-Layer Perceptron (MLP).
We define the notion of generator of invariant polynomials, which is necessary to define Reynolds
dimension.
Definition 11 (Generator of invariant polynomials). Let G be a finite group. A set of invariant
polynomials r1 (x1, .., xn), .., rs(x1, .., xn) are called a generator of invariant polynomials iffor
any invariant polynomial f(x1, .., xn), there is a polynomial p(y1, .., ys) such that f(x1, .., xn) =
p(r1 (x1, .., xn), .., rs(x1, .., xn)) holds.
The existence of such a generator is non-trivial for general group actions, but has been shown by
Hilbert for finite groups or more generally linearly reductive groups Hilbert (1890). Under the
preparation so far, we define the following Reynolds dimension.
*2Theorbitsum Σ : R[n]m×b → R[n]m7G×b is defined by ∑(X)G∙u,β :二 Pg∈G Xgp,β for X = [xu,β] ∈
R[n]m×b, U ∈ [n]m, β ∈ [b] and G ∙ u ∈ [n]m∕G.
6
Under review as a conference paper at ICLR 2022
Definition 12 (Reynolds dimension). Let G be a finite group. The smallest natural num-
ber d satisfying the following property is the Reynolds dimension of the group G. There
exist polynomial h1 , .., hs of d-variable and an index subset {j1, . . . , jd}	⊂	[n] such
thatγG(h1(xj1 , . . . , xjd)), . . . , γG (hs (xj1 , . . . , xjd)) is a generator of the invariant polynomials.
Before discussing universality for invariant functions, let us review some notation. We define
StabG([d]) to be the set of elements of G for which xj1 , . . . , xjd are fixed. In addition, [G/G0]
a complete system of representatives of G/G0 for a subgroup G0 ⊂ G is a set of order |G/G0| that
satisfies G =	aG0.
Proposition 13. In the same situation as Definition 12, [G/StabG([d])] is a Reynolds design of hi.
Theorem 14 (Universality). We assume that G = Sn. Let d be the Reynolds dimension of G. Then,
Reyonlds invariant nets constructed above for E : Rnl ×a → Rnd×b is a universal approximator for
invariant functions f : Rn' ×a → Rb. More strongly, the input space of E can be replaced by a
composite EoZ with the the zero padding map*3 Z : Rd×a → Rn'×a.
7	Reduction of input variables
So far, we have taken the input of N as an n`-dimensional space. However, when n or ` are large,
this can be an obstacle to the calculation. Therefore, we take d-dimensional subspace as an input by
Theorem 14. This is called the d-reduced equivariant (invariant) Reynolds network. For example,
consider the case ` = 2 and assume that the set of invariant polynomials R[x]Sn has Reynolds
dimension 4 by the valuables x1,1, x1,2, x2,1, x2,2, then by Theorem 14, we can restrict the input
space of fT in our model to x1,1, x1,2, x2,1, x2,2 and still preserve universality for invariant functions.
The reduced Reynolds networks give us the great benefit of generalization over n. In other words, by
transferring fT learned for small n to fT for large n, we can obtain a model for large n. See Figure 3
for more details. When we consider a practical task, for example, the problem of classifying graphs,
it is not a simple matter to determine how many Reynolds dimensions it has. However, this problem
has been formulated mathematically as a problem in higher-order tensor spaces, and we expect that
the Reynolds dimension will be independent of n in this case.
Conjecture 1. Let G be an Sn acting on Rn' as in Section 4. For arbitrary enough large n, the
Reynolds dimension d(n) ofG is independent ofn.
8	Relationship with the parameter sharing models
The models of Zaheer et al. (2017) and Maron et al. (2018) are called parameter sharing models,
which are defined by sharing parameters in matrices. Such models are obtained by computing the
action of the group on the matrix parameters, and are essentially different from the proposed models,
which are obtained from the action of the group on the functions. It is known that the parameters
of a neural net do not correspond completely to the function, so the proposed method of looking at
the action on the function seems to be more essential. In addition, although Maron et al. (2018) can
be represented exactly by the proposed model (Theorem 7), it is a non-trivial question whether the
proposed model can be represented exactly byMaron et al. (2018) .
9	Experiments
We evaluated our models compared to fully-connected neural nets (FNNs) and Maron et al. (2018)
using synthetic datasets for equivariant and invariant tasks. Furthermore, we chose three graph
benchmark datasets as real data for invariant tasks. Please refer to Appendix for implementation
details of both experiments.
*3The zero padding map Z : Rd×a = Rd Z Ra → Rn'×a = Rn' Z Ra is the linear map defined by
Z((xι,..., Xd) Z eα) := (xι,...,xd, 0,..., 0) Z eɑ for α = 1, 2,...,a.
n'-d
7
Under review as a conference paper at ICLR 2022
Table 1: Results of comparison to a baseline method
Task n	Symmetry 3	5	10	20	Diagonal 3	5	10	20
FNN	1.730e-4	9.180e-4	1.454e-3	3.0583	1.295e-4	2.655e-4	1.148e-4	1.081e-1
Maron et al. (2018)	6.600e-3	3.786e-3	9.294e-4	4.471e-3	2.065e-3	2.266e-3	4.098e-3	4.743e-4
ReyNet (ours)	2.147e-4	3.960e-4	1.408e-3	3.151e-3	1.007e-4	2.472e-4	6.635e-4	1.112e-4
4red-ReyNet (ours)	8.544e-5	4.889e-5	7.529e-5	6.554e-5	6.947e-5	1.932e-5	5.568e-5	3.566e-5
Task n	PoWer 3	5	10	20	Trace 3	5	10	20
FNN	2.586	3.091e+1	5.756e+1	6.268e+2	2.821e-4	1.135e-3	9.529e-3	5.149e-2
Maron et al. (2018)	4.036e-1	3.462e-1	7.062e-1	4.735e-1	1.241e-3	6.696e-3	3.663e-2	5.527e-2
ReyNet (ours)	3.798e-1	1.257	3.620	3.065	1.884e-3	2.949e-3	4.275e-2	5.338e-2
4red-ReyNet (ours)	1.204e-1	1.330e-1	1.217e-1	1.165e-1	3.491e-4	1.914e-3	6.758e-3	1.220e-2
9.1	Synthethic Datasets
We created four types of synthetic datasets for the comparison. Given an input matrics data A ∈
Rn×n, each task is defined by:
•	Symmetry: projection onto the symmetric matrices F(A) = 1 (A + A>),
•	Diagonal: diagonal extraction F(A) = diag(A),
•	Power: computing each squared element F(A) = [Ai2,j],
•	Trace: computing the trace F(A) = tr(A),
where the task function F is equivariant in symmetry, diagonal, and power, and is invariant in trace.
Following Maron et al. (2018), we sampled i.i.d. random matrices A with uniform distribution
in [0, 10]; then we transformed it by the task function F. In our experiments, we provided n ∈
{3, 5, 10, 20}, and the size of training dataset and test dataset is 1000 respectively.
Objective Function. In terms of an objective function, we modified mean squared error (MSE)
loss based on Theorem 7. In a straightforward manner, equivariant tasks are regression task so
that MSE loss is selected to decrease the gap of all elements between an output matrix and ground
truth matrix: `std : Rn×n × Rn×n → R. However, thanks to Theorem 7, ReyNet does not require
calculating whole elements but the gap of 1st row of 1st column element and 1st row of 2nd column
element: 'corner : R1×2 X R1×2 → R. In this paper, We call the latter objective function as corner
MSE loss.
9.1.1 Results
Table 1 shoWs the result of synthetic datasets, Which is the av-
erage of MSE of five different seeds. As a result, our proposed
models outperform Maron et al. (2018) overall. Interestingly,
With regards to the power task, FNN and ReyNet are Worse
than other models. The function F of power task amplifies the
output error to large value. Simultaneously, When the number
of parameters of the model is very large along With the size of
n, the output of the model becomes sensitive. Therefore, We
considered that the error became large due to the combination
With large neural netWorks and the power function.
Table 2: Comparison of Objective
Function
n	mse	corner mse
3	1.438e-4	1.249e-4
5	4.912e-5	9.375e-5
10	1.157e-4	6.487e-5
20	1.608e-4	8.537e-5
Moreover, We validated the effect of an objective function. We trained 4r-ReyNets With standard
MSE loss and Corner MSE loss respectively. Table 2 shoWs the result of using each objective
function. Accordingly, Corner MSE loss achieved loWer error than standard MSE loss except for
n = 5 case. Therefore, using Corner MSE loss is a good choice for equivariant tasks.
8
Under review as a conference paper at ICLR 2022
(a) Symmetry
(b) Power
Figure 2: Generalization Error of the Regression Task on two Synthetic Datasets.
Table 3: Graph Benchmark Results(Acc.)
	MUTAG	IMDB-BINARY	IMDB-MULTI
Maron et al. (2018)	0.822 ± 0.088	0.647 ± 0.075	0.411 ± 0.084
4red-ReyNet (ours)	0.853 ± 0.066	0.710 ± 0.061	0.476 ± 0.029
9.1.2 Generalization Error
Notably, our 4red-ReyNet does not depend on the size of inputs n. In order to test the generalization
performance, we trained 4red-ReyNet with ntrain = 3 dataset and then validated the MSE on
ntest ∈ {3, 4, . . . , 20} datasets. The results are depicted in Figure 2. We can see that our 4-reduced
ReyNet is generalized to the input size. Note that with regard to invariant tasks, we confirmed the
model is not generalized to the tasks as Maron et al. (2018) has reported.
9.2 Graph Benchmark Datasets
We selected three benchmark datasets from the TU Dortmund data collection(Kersting et al., 2016):
MUTAG dataset, IMDB-BINARY, and IMDB-MULTI. These datasets are provided for classifica-
tion, thus we treated the experiments as invariant tasks. Since the size of these datasets are small,
we run the 10-folds cross-validation score for each dataset.
9.2.1 Results
We report the average accuracy scores and standard deviations of 10 experiments in Table 3. As
a result, our ReyNet outperforms the model of Maron et al. (2018) in the three datasets. With
reference to practical use, our ReyNet has a limitation. Since our ReyNet requires calculating and
storing the tensor at least O(n2), it is difficult to run other TU Dortmund data collection (PTC,
PROTEINS etc.) through lack of GPU memory. However, by combining our ReyNet with other
methods (e.g. stochastic gradient descent from a uniform distribution on a Reynolds design), we
believe that this limitation can be overcome.
10 Conclusion
We considered equivariant and invariant neural networks over higher order tensor spaces. The
method of converting deep neural nets to equivariant neural nets using Reynolds operators had
some difficulties in terms of computational complexity, which can be successfully avoided by us-
ing Reynolds designs. Then, we constructed equivariant Reynolds networks (equivariant ReyNets)
based on the Reynolds designs and proved the universality. Similarly, we also introduced Reynolds
designs induced by Reynolds dimension in the invariant case. Then, we also constructed invari-
ant Reynolds networks (invariant ReyNets) and proved the universality as well. Furthermore, we
showed that input variables of ReyNets can be reduced based on the Reynolds dimension. In the
section of numerical experiments, we showed that equivariant and invariant ReyNets performs bet-
ter than or comparable to existing models. Moreover, we observed that ReyNets with a few input
variables can generalize well to the cases with more input variables.
9
Under review as a conference paper at ICLR 2022
References
Eiichi Bannai and Etsuko Bannai. A survey on spherical designs and algebraic combinatorics on
spheres. European Journal of Combinatorics, 30(6):1392-1425, 2009.
Joan Bruna, Wojciech Zaremba, Arthur Szlam, and Yann LeCun. Spectral networks and locally
connected networks on graphs. arXiv preprint arXiv:1312.6203, 2013.
Zhengdao Chen, Soledad Villar, Lei Chen, and Joan Bruna. On the equivalence between graph
isomorphism testing and function approximation with gnns. arXiv preprint arXiv:1905.12560,
2019.
Michael Defferrard, Xavier Bresson, and Pierre Vandergheynst. Convolutional neural networks on
graphs with fast localized spectral filtering. Advances in neural information processing systems,
29:3844-3852, 2016.
Marco Gori, Gabriele Monfardini, and Franco Scarselli. Anew model for learning in graph domains.
In Proceedings. 2005 IEEE International Joint Conference on Neural Networks, 2005., volume 2,
pp. 729-734. IEEE, 2005.
Devon Graham and Siamak Ravanbakhsh. Deep models for relational databases. arXiv preprint
arXiv:1903.09033, pp. 6, 2019.
Jason Hartford, Devon Graham, Kevin Leyton-Brown, and Siamak Ravanbakhsh. Deep models
of interactions across sets. In International Conference on Machine Learning, pp. 1909-1918.
PMLR, 2018.
Mikael Henaff, Joan Bruna, and Yann LeCun. Deep convolutional networks on graph-structured
data. arXiv preprint arXiv:1506.05163, 2015.
David Hilbert. Uber die theorie der algebraischen formen. Mathematische Annalen, 1890.
Nicolas Keriven and Gabriel Peyre. Universal invariant and equivariant graph neural networks.
Advances in Neural Information Processing Systems, 32:7092-7101, 2019.
Kristian Kersting, Nils M Kriege, Christopher Morris, Petra Mutzel, and Marion Neumann. Bench-
mark data sets for graph kernels. URL http://graphkernels. cs. tu-dortmund. de, 2016.
Piotr Kicki, Piotr Skrzypczynski, and Mete Ozay. A new approach to design symmetry invariant
neural networks. In 2021 International Joint Conference on Neural Networks (IJCNN), pp. 1-8.
IEEE, 2021.
Thomas N Kipf and Max Welling. Semi-supervised classification with graph convolutional net-
works. arXiv preprint arXiv:1609.02907, 2016.
Risi Kondor, Hy Truong Son, Horace Pan, Brandon Anderson, and Shubhendu Trivedi. Covariant
compositional networks for learning graphs. arXiv preprint arXiv:1801.02144, 2018.
Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classification with deep con-
volutional neural networks. Advances in neural information processing systems, 25:1097-1105,
2012.
Yann LeCun, Bernhard Boser, John S Denker, Donnie Henderson, Richard E Howard, Wayne Hub-
bard, and Lawrence D Jackel. Backpropagation applied to handwritten zip code recognition.
Neural computation, 1(4):541-551, 1989.
Ron Levie, Federico Monti, Xavier Bresson, and Michael M Bronstein. Cayleynets: Graph con-
volutional neural networks with complex rational spectral filters. IEEE Transactions on Signal
Processing, 67(1):97-109, 2018.
Takanori Maehara and Hoang NT. A simple proof of the universality of invariant/equivariant graph
neural networks. arXiv preprint arXiv:1910.03802, 2019.
Haggai Maron, Heli Ben-Hamu, Nadav Shamir, and Yaron Lipman. Invariant and equivariant graph
networks. arXiv preprint arXiv:1812.09902, 2018.
10
Under review as a conference paper at ICLR 2022
Haggai Maron, Heli Ben-Hamu, Hadar Serviansky, and Yaron Lipman. Provably powerful graph
networks. arXiv preprint arXiv:1905.11136, 2019a.
Haggai Maron, Ethan Fetaya, Nimrod Segol, and Yaron Lipman. On the universality of invariant
networks. arXiv preprint arXiv:1901.09342, 2019b.
Haggai Maron, Or Litany, Gal Chechik, and Ethan Fetaya. On learning sets of symmetric elements.
arXiv preprint arXiv:2002.08599, 2020.
S Chandra Mouli and Bruno Ribeiro. Neural networks for learning counterfactual g-invariances
from single environments. In International Conference on Learning Representations, 2020.
David Mumford, John Fogarty, and Frances Kirwan. Geometric invariant theory, volume 34.
Springer Science & Business Media, 1994.
Ryan L Murphy, Balasubramaniam Srinivasan, Vinayak Rao, and Bruno Ribeiro. Janossy pool-
ing: Learning deep permutation-invariant functions for variable-size inputs. arXiv preprint
arXiv:1811.01900, 2018.
Hoang Nguyen and Takanori Maehara. Graph homomorphism convolution. In International Con-
ference on Machine Learning,pp. 7306-7316. PMLR, 2020.
Kenta Oono and Taiji Suzuki. Graph neural networks exponentially lose expressive power for node
classification. In International Conference on Learning Representations, 2019.
Charles R Qi, Hao Su, Kaichun Mo, and Leonidas J Guibas. Pointnet: Deep learning on point sets
for 3d classification and segmentation. In Proceedings ofthe IEEE conference on computer vision
and pattern recognition, pp. 652-660, 2017.
Siamak Ravanbakhsh, Jeff Schneider, and Barnabas Poczos. Equivariance through parameter-
sharing. In International Conference on Machine Learning, pp. 2892-2901. PMLR, 2017.
Bruce Sagan. The symmetric group: representations, combinatorial algorithms, and symmetric
functions, volume 203. Springer Science & Business Media, 2001.
Franco Scarselli, Marco Gori, Ah Chung Tsoi, Markus Hagenbuchner, and Gabriele Monfardini.
The graph neural network model. IEEE transactions on neural networks, 20(1):61-80, 2008.
Nimrod Segol and Yaron Lipman. On universal equivariant set networks. arXiv preprint
arXiv:1910.02421, 2019.
Elise van der Pol, Daniel Worrall, Herke van Hoof, Frans Oliehoek, and Max Welling. Mdp homo-
morphic networks: Group symmetries in reinforcement learning. Advances in Neural Information
Processing Systems, 33, 2020.
Dmitry Yarotsky. Universal approximations of invariant maps by neural networks. Constructive
Approximation, pp. 1-68, 2021.
Manzil Zaheer, Satwik Kottur, Siamak Ravanbakhsh, Barnabas Poczos, Ruslan R Salakhutdinov,
and Alexander J Smola. Deep sets. In Advances in neural information processing systems, pp.
3391-3401, 2017.
11
Under review as a conference paper at ICLR 2022
A	Proof of Theorems and Propositions
A.1 Proof of Lemma 6
Firstly, we give some notation about basis to state. Let e1, . . . , en the standard basis of Rn. As we
saw above, for u = (u1, . . . , um) ∈ [n]m, we set
eu := eUi,…,Um	=	eUi	g.…与	eUm	∈	Rn	X …X	Rn	= Rnm .	(4)
'^^^^^^^^{^^^^^^^^r
m
For any basis tableau T ∈ Tm,D, natural numbers u = (u1, . . . , um) ∈ [n]m are given as u` := d ∈
[n] if ` ∈ {td}.*4 Then, we define the map φ : Tm → [n]m by φ(T) := (u1, .., um) ∈ [n]m.
Then the basis tableau vector is defined by eT := eφ(T) ∈ Rnm, where the right hand side is defined
by (4).
A.2 Proof of Theorem 7
We introduce the following notion to obtain tableau-based representation of elements in [n]m.
Definition 15 (Extended Tableau). Letn ≥ D. Let [n]D# be the set of D different natural numbers at
most n defined by [n]D# := {[j1, . . . , jD] ∈ [n]D | ja 6= jb for a 6= b ∈ [D]}. Then, we call elements
in Tm,d := [n]# X TmD extended tableaux With depth D.
We denote the set 1≤D≤m Tm,D of extended tableaux with at most depth m by Tm . The action
g∙ [jι,...,jD ] ：= [g∙ jι,...,g∙ jD ] of G on [n]# is well-defined. Then, we define the action of G on
Tm,D by g ∙ (j, T) ：= (g ∙ j, T). In the following, We identify an extended tableau ([1,2,..., D], T)
with the basis tableau T.
Next, we introduce some notation to represent elements in [n]m by extended tableaux. We first
define the partial order of vectors of natural numbers that can have different dimensions. For
t = [t1, . . . , tk] ∈ Nk and t0 = [t01, . . . , t0k] ∈ Nk0, we denote as t t0 if either (i) k > k0 or
(ii) k = k0 and t1 > t01. We note that t1	. . . tD holds for a basis tableau T = [t1, . . . , tD]
by definition.	Let u =	[u1, . . . , uk] ∈ [n]m .	We set D(u)	:=	|{u}|. We define the multiplicity
map multu :	{u} →	[m] by multu(u) :=	∣{' ∈ [m] |	u`	= u}|. For u ∈ {u}, We define
tu ：= [tι,...,tku ], where ku := multu(u), u^ = …=Utku = u, and tι < … < t%. Here,
We note that either tu * t^ or tu Y t^ holds for u = u0 ∈ {u} by definition. Thus, there exist
different natural numbers j1, . . . ,jD ⊂ [n] such that {j1, . . . ,jD} = {u} and tji . . . tjD .
Definition 16 (Tableau Representation). Let n ≥ m . We define the map ψ : [n]m 3 u 7→ (j, T) ∈
Tm called tableau representation by
ψ(u) =([ji,...,jD ],[tji,…，tjD ]) ∈Tm,D.
We define the map φ : Tm → [n]m as follows: For any extended tableau (j, T) ∈ Tm,D, natural
numbers u = [u1, . . . , um] ∈ [n]m are given as u` := jd ∈ [n] if ` ∈ {td}, and φ(j, T) :=
(u1, ..,um) ∈ [n]m.
Lemma 17. When n ≥ m, the tableau representation ψ : [n]m → Tm is bijective and ψ(g ∙ U)=
g ∙ Ψ(u) for g ∈ HD.
[Proof]. First, from the construction, ψ is injective and φ◦ ψ = id[n]m. Since [n]m and T are finite
sets, if we show that φ is injective, then ψ is bijective. For the extended tableau S, T, assume that
φ(S) = φ(T). Note that the set of row vectors {s1, ..., sd} of S is uniquely determined from φ(S).
Then, since the extended tableaux satisfy the order * in which this goes between the row vectors,
the extended tableau S having row vectors {sι,…,Sd} is unique. Hence, we have S = T.	■
Definition 18 (Extended Tableau Vector). Let n ≥ m. The extended tableau vector ej,T is defined
by ej,T := eφ(j,T).
*4For a vector t = [t1, . . . , tk], we set {t} := {t1, . . . , tk}. For example, when t = [1, 1], {t} = {1}.
12
Under review as a conference paper at ICLR 2022
For an extended tableau (j, T) ∈ Tm, the linear map ^j t b : Rb → Rnm×b is defined by X = ej t
in (3).
Lemma 19 (Normalization). Let n ≥ D. For j ∈ [n]D#, there uniquely exists g ∈ HD such that
j = g-1 ∙ [1, 2,...,D] ∈ [n]#. Hence, this correspondence [n]# 3 j → g ∈ HD is bijective.
[Proof] . There uniquely exists σ1 ∈ Cn such that σ1 (j1) = 1. Inductively, there uniquely exists
σd ∈ Cn-d+1 such that σd (σd-1 ∙ σd-2 …σ1(jd)) = d for d = 2,...,D. Then g := σ0 …σ1 ∈
Hd satisfies g ∙ j = [1, 2,...,D] by definition.	■
From Lemma 19, an extended tableau (j, T) ∈ TmD is uniquely represented by (j, T) = g-1 ∙
([1,..., D], T) = g-1 ∙ T as in Figure 3, where g ∈ HD and we identified T ∈ TmD with
([1,...,D], T) ∈ Tm,D in the last equation. Thus, we have Tm,d = sg∈H0 g-1Tm,D, and Tm =
1≤D≤m	g∈HD
g-1Tm,D. From Lemma 17, for each u ∈ [n]m, there uniquely exists g ∈ HD
and T ∈ Tm such that ψ(u) = g-1 ∙ T (or equivalently U = ψ-1(g-1 ∙ T)). In the following, We
omit the bijective ψ for notational simplicity.
In the following, we prove Theorem 7. We can write F = Pu∈[n]m fu ∙ eu,b by maps fu : Rnl×a →
Rb. Then since F is equivariant, we have Pu∈[n]m fu ∙ ^u,b(g ∙ x) = F(g ∙ X)= g ∙ F(x)=
Pu∈[n]m fu ∙ eg∙u,b(X). This implies that
fu ∙ eu,b(g ∙ X) = fg-1∙u ∙ eu,b(X).
(5)
Then, we obtain the following equations:
F(X)= E fu ∙ eu,b(x)
u∈[n]m
= XXX fg-1∙τ∙ eg-1∙τ,b(X)
1≤D≤m g∈HD T∈Tm,D
=∑ E EfLT ∙ eg-1 …
1≤D≤m T∈Tm,D g∈HD
= XXX fT ∙ eg-1∙T,b(g ∙ x)
1≤D≤m T∈Tm,D g∈HD
Σ Σ Σ
1≤D≤m T∈Tm,D g∈HD
XXX
1≤D≤m T∈Tm,D g∈HD
g-1∙(fτ∙ eτ,b(g ∙x))
iH^∣ g-1 ∙ (IHD lfT ∙ eT,b(g ∙ x))
τHD	E THD (IHDlfT ∙ ^T,b) (x)
1≤D≤m T∈Tm,D
X X THD (FT ∙ eT,b) (X),
1≤D≤m T∈Tm,D
13
Under review as a conference paper at ICLR 2022
where the fourth equality follows from (5) and the last equality follows by putting FT := |HD|fT.
B Proof of Theorem 9
By Theorem 7, we have continuous maps fT : Rnl ×a → Rb satisfying F =
Pm=I Pt∈t^ d THD (fτ ◦ ^τ,b), for standard Young tableaux T ∈ ‰,,d. Since K is a Com-
pact set and Sn is a finite group, we may assume that K is closed under Sn-action by taking
Ug∈sιτ g∙K.Thenforany ε, we have an MLPNT which approximates fτ, namely ∣∣Nt - fτ ∣∣k < ε
holds. Hence by the definition of our invariant model, we have
m
F - X X THD (NT ∙ ^T,b)
D=1 T∈Tm,D
m
K
m
= ∣	THD
D=1 T ∈Tm,D
m
(fT ∙ eT,b) -E E	THD (NTeT,b) kκ
D=1 T ∈Tm,D
≤
≤
≤
≤
≤
≤
≤
∣THD
D=1 T ∈Tm,D
m
X X ∣THD
D=1 T ∈Tm,D
m
XX k X IHdI
D=1 T ∈Tm,D g∈HD
m
Xxx iH_7 i|(fT- NT) ∙ eg-1∙T,b(g ∙-)1K
D=1 T ∈Tm,D g∈HD
m
X X x iH^j∣i(ZT-NT)∙eg-1∙T,b(-)∣ικ
D=1 T ∈Tm,D g∈HD	D
m
X X X ∣⅛ k(fT - NT)kκ
D=1 T ∈Tm,D g∈HD
m
XXX W ε
D=1 T ∈Tm,D g∈HD
(fT ∙ eT,b) - THD (NT ∙ eT,b) IlK
((ZT - NT) ∙ eT,b)kκ
((fT - NT) ∙ eg-1∙T,b(g ∙-)∣IK
1
≤ m∣Tm,D ∣ε.
By replacing ε, we obtain (F - PD=I pτ∈τm,D THD (NT ∙ ^τ,b)∣κ <ε.
C Proof of Proposition 13 and Theorem 14
We use the following theorem.
Theorem 20 (Hilbert finiteness theorem (Hilbert (1890))). Let G be a finite group or, more gener-
ally, a linearly reductive group. In this case, there is always a generator of G-invariant polynomials.
Let f : Rn'×a → Rb be a continuous invariant function. By replacing K with Ug∈G g ∙ K, we may
assume that K is closed under the action of G. Then by the Stone-Weierstrass theorem, there exists
`
a polynomial Z : Rn ×a → Rb which approximate Z with arbitrary precision on K. Put Z = γG(Z),
then
f(χ)-γG(f(X))L=∣G
∣G∣f(X) - Ef(g ∙X)
g∈G
K
14
Under review as a conference paper at ICLR 2022
≤lG X kf(x)- f(g ∙x)kκ
g∈G
=∣⅛ Xf (g ∙χx-Kg ∙X)L
g∈G
1
’ 商 g∈Gε=ε,
where We used the property f (x) = f (g ∙ x) in the third equation. By Theorem 20, We have a gener-
ator of invariant polynomials r1 , . . . , rs . From the definition of generator, there exists a polynomial
P and f can be Written in the form f (x1, . . . , xnla) = P(r1 (x1, . . . , xnla) , .., rs (x1, . . . , xnla)).
By the assumption of Reynolds dimension, r1 (x1, . . . , xnla) , . . . , rs (x1, . . . , xnla)
are Written as γG (h1 (xj1 , . . . , xjd)) , . . . , γG (hs (xj1 , . . . , xjd)) for some polynomials
h1 (xj1 , . . . , xjd ) , . . . , hs (xj1 , . . . , xjd ) of d-variables.
Since Hd is a complete system of representative of G/ Stab([d]), We obtain the folloWing decom-
position:
G = U g ∙ StabG([d]) = U g ∙ StabG([d]).
g∈[G/ StabG([d])]	g∈Hd
Then, this induces the decomposition of Reynolds operators;
γG = γHd ◦ γStab([d]) : R [x1, ..,xnla] → R [x1, .., xnla]Stab([d]) → R [x1, . . . ,xnla]G ,
Where R [x1, .., xnla] , R [x1, .., xnla]Stab([d]) , R [x1, . . . , xnla]G are the set of polynomials,
Stab([d])-invariant polynomials, invariant polynomials, respectively. This implies
ri = YG (hi (XjI,..., Xjd )) = YHd(YStab([d]) (hi (XjI ,∙∙∙, Xjd ))) = YHd (hi (XjI ,∙∙∙, Xjd )) .
Here the last inequality folloWs from the fact that hi (xj1 , . . . , xjd) is a StabG([d])-invariant poly-
nomial. On the other hand, note that the invariant Reynolds operator is equal to the composition of
the equivariant Reynolds operator and the orbit sum; YG = Σ ◦ τG . For the vector valued function
h = (h1, .., hs), by the universal approximation theorem of fully connected neural nets, We can take
a fully connected neural net Q With Which kQ - hkK < ε holds. Let N be a fully connected neural
net that approximates P above; kN - P kK < ε. Then
N ◦ Σ ◦ τHd ◦ Q(X1, .., Xn) ≈ N ◦ Σ ◦ τHd(h1(X1, .., Xn), .., hs(X1, .., Xn))
≈	N(YG(h1)(X1, .., Xn), ..,YG(hs)(X1, .., Xn))
=	N(r1(X1, .., Xn), ..,rs(X1, .., Xn))
≈	P(r1(X1, .., Xn), .., rs(X1, .., Xn))
=	f(X1, .., Xn)
≈	f(X1, .., Xn).
Here We denote the approximation by ≈.
D Implementation Details
In the experiments of synthetic datasets, We used tWo models; Reynolds NetWorks (ReyNets) and
4-reduced Reynolds NetWorks (4red-ReyNets), While only 4-reduced Reynolds NetWorks (4red-
ReyNets) for the graph benchmark dataset. For equivariant tasks, We applied equivariant ReyNets.
For invariant tasks, We implemented invariant ReyNets folloWing the architecture of the invariant
model proposed by Maron et al. (2018)*5; an equivariant ReyNet is folloWed by max pooling*6,
and fully-connected layers. The fully-connected layers consist of three layers and the number of
the units are 512, 256, and 1 respectively. We adopted the ReLU function as activation. We used
Adam optimizer and set learning rate as 1e-3 and Weight decay as 1e-5. Batch size is 100. Note
*5Please refer to https://github.Com/Haggaim/InvariantGraphNetworks/blob/master/models/invariant_basic.py#L14
*6This operation outputs the max value of diagonal and non-diagonal elements of an input matrix.
15
Under review as a conference paper at ICLR 2022
that the models of Maron et al. (2018) are re-implemented by PyTorch in reference to the author’s
implementation in Tensorflow.
For the benchmark datasets, basically, we adopted the same architecture of ReyNet and Maron et al.
(2018) as for the synthetic datasets. That is, the architecture consists of two parts; equivariant part
and invariant parts (fully-connected layers). However, we set the output dimension of equivariant
part of Maron et al. (2018) to the same as the input dimension although we can set any numbers (e.g.
256 dimension). This is because we aimed to evaluate the performance (capability) of equivariant
part. If we set the dimension to 256 for example, it is difficult to distinguish whether the high-
performance is thanks to equivariant part capability or high dimension representation.
E Generalization Error
Figure 4 shows the detail version of only 4red-ReyNets at Figure 2. In figure 3 our results seems
an almost horizontal straight line, but this figure shows that the MSE also slightly increases as n
increases.
2.000
1.975
1.950
⅛j 1.925
W
1.900
1.875
1.850
1.825
n
LU
S
W
(a) Symmetry
0.126
0.124
0.122
0.120
0.118
0.116
0.114
n
(b) Power
Figure 4: Generalization Error
F Runtime Speed
Figure 5 shows the runtime speed of our ReyNet compared with Maron et al. (2018). The left of
Figure 5 shows the runtime speed when the number of input features changes while the size of the
set does not. The right of Figure 5 shows the runtime speed when the size of the set changes while
the number of input features does not. Given both figures, it seems that our ReyNet is much slower
than Maron et al. (2018). However, as a result of the experiments, ReyNet outperforms Maron et al.
(2018) in many cases. We expect that much more GPU resources would help ReyNet in the future.
16
Under review as a conference paper at ICLR 2022
SUJ
Figure 5: Runtime Speed
17