Under review as a conference paper at ICLR 2022
Accelerated Gradient-Free Method for Heav-
ily Constrained Nonconvex Optimization
Anonymous authors
Paper under double-blind review
Ab stract
Zeroth-order (ZO) method has been shown to be a powerful method for solving
the optimization problem where explicit expression of the gradients is difficult
or infeasible to obtain. Recently, due to the practical value of the constrained
problems, a lot of ZO Frank-Wolfe or projected ZO methods have been proposed.
However, in many applications, we may have a very large number of nonconvex
white/black-box constraints, which makes the existing zeroth-order methods ex-
tremely inefficient (or even not working) since they need to inquire function value
of all the constraints and project the solution to the complicated feasible set. In
this paper, to solve the nonconvex problem with a large number of white/black-
box constraints, we proposed a doubly stochastic zeroth-order gradient method
(DSZOG). Specifically, we reformulate the problem by using the penalty method
with distribution probability and sample a mini-batch of constraints to calculate the
stochastic zeroth/first-order gradient of the penalty function to update the parame-
ters and distribution, alternately. To further speed up our method, we propose an
accelerated doubly stochastic zeroth-order gradient method (ADSZOG) by using
the exponential moving average method and adaptive stepsize. Theoretically, we
prove DSZOG and ADSZOG can converge to the -stationary point of the con-
strained problem. We also compare the performances of our method with several
ZO methods in two applications, and the experimental results demonstrate the
superiority of our method in terms of training time and accuracy.
1	Introduction
Zeroth-order (gradient-free) method has been shown to be a powerful method for solving the opti-
mization problem where explicit expression of the gradients are difficult or infeasible to obtain, such
as bandit feedback analysis Agarwal et al. (2010), reinforcement learning Choromanski et al. (2018),
and adversarial attacks on black-box deep neural networks Chen et al. (2017); Liu et al. (2018a).
Zeroth-order (ZO) methods only use the function values to approximate the full gradient or stochastic
gradient, and then the gradient descent can be used. Due to the friendly of approximating the gradient
and scalability to large scale problems, more and more zeroth-order gradient algorithms have been
proposed and achieved great success, such as Ghadimi & Lan (2013); Wang et al. (2018); Gu et al.
(2016); Liu et al. (2018a); Huang et al. (2020a).
Recently, constrained optimizations have become increasingly relevant to the machine learning
community. Due to several motivating applications, the study of the zeroth-order methods in
constrained optimization has gained great attention. Specifically, ZOSCGDBalasubramanian &
Ghadimi (2018) uses the zeroth-order method to approximate the unbiased stochastic gradient of
the objective, and then use the Frank-Wolfe framework to update the model parameters. Based on
ZOSCGD, Gao & Huang (2020) use the variance reduction technique Fang et al. (2018); Nguyen et al.
(2017) to obtain a better convergence performance. In addition, Huang et al. (2020b) use the variance
reduction technique and momentum acceleration technique to further speed up the ZO Frank-Wolfe
method. ZOSPGD Liu et al. (2018b) uses the ZO method to update the parameters and then projects
the solution onto the feasible subset. ZOADAMM+ Liu et al. (2020) uses the adaptive momentum
method to accelerate the ZOSPGD. We have summarized several representative zeroth-order methods
for constrained optimization in Table 1.
1
Under review as a conference paper at ICLR 2022
Table 1: Representative zeroth order methods for constrained optimization problems, where N/C
means nonconvex/convex, W/B means white/black-box function, and the last column shows the size
of the constraints.
Framework	Algorthm	Reference	Objective	Constraints	Size
Frank-Wolfe	ZOSCGD	Balasubramanian & Ghadimi (2018)	N/C	C W	Small
	FZFW	Gao & Huang (2020)	N/C	C W	Small
	FZCGS				
	TCGS				
	ACC-SZOFW	Huang et al. (2020b)	N/C	C W	Small
	ACC-SZOFW*				
Projected	ZOPSGD	Liu et al. (2018b)	N/C	C W	Small
	ZOADAMM+	Liu et al. (2020)	N/C	C W	Small
Penalty	DSZOG ADSZOG	Ours	N/C	N/C WZB	Large
However, all these methods only focus on the simple constrained problem and are not scalable for
the problems with a large number of constraints. Specifically, on the one hand, they all need to
evaluate the values of all the constraints in each iteration. On the other hand, the projected gradient
methods and the Frank-Wolfe methods need to solve a subproblem in each iteration. These makes
the existing methods time-consuming to find a point satisfying all the constraints. What’s worse, all
these methods need the constraints to be convex white-box functions. However, in many real-world
applications, the constraints could be nonconvex or black-box functions, which makes the existing
method extremely inefficient or even not working. Therefore, how to effectively solve the nonconvex
constrained problem with a large number of nonconvex/convex white/black-box constraints, which is
denoted as heavily constrained problem, by using the ZO method is still an open problem.
In this paper, to solve the heavily constrained nonconvex optimization effectively and efficiently, we
propose two new ZO algorithms called doubly stochastic zeroth-order gradient method (DSZOG)
and accelerated doubly stochastic zeroth-order gradient method (ADSZOG). Specifically, we give
a probability distribution over all the constraints and rewrite the original problem as a nonconvex-
strongly-concave minimax problem Lin et al. (2020); Wang et al. (2020); Huang et al. (2020a); Guo
et al. (2021) with respect to the model parameter and probability distribution by using the penalty
method. We first sample a mini-batch of training points uniformly and a mini-batch of constraints
according to the distribution to calculate the zeroth-order gradient of the penalty function w.r.t
model parameters and then sample a mini-batch of constraints uniformly to calculate the stochastic
gradient of penalty function w.r.t the probability distribution. Then, gradient descent and projected
gradient ascent can be used to update model parameters and probability distribution. In addition,
to further speed up training, we propose a new accelerated doubly stochastic zeroth-order gradient
method by using the exponential moving average (EMA) method and adaptive stepsize Guo et al.
(2021); Huang et al. (2020a), which benefits our method from the variance reduction and adaptive
convergence. Theoretically, we prove DSZOG and ADSZOG can converge to the -stationary point
of the constrained problem. We also compare the performances of our method with several ZO
methods in two applications, and the experimental results demonstrate the superiority of our method
in terms of training time and accuracy.
Contributions. We summarized the main contributions of this paper as follows:
1.	We propose a doubly stochastic zeroth-order gradient method to solve the heavily constrained
nonconvex problem. By introducing a stochastic layer into the constraints, our method is
scalable and efficient for the heavily constrained nonconvex problem.
2.	We also proposed an accelerated doubly stochastic zeroth-order gradient method to solve the
heavily constrained nonconvex problem. By using the exponential moving average method
and adaptive stepsize, it enjoys the benefits of variance reduction and adaptive convergence.
3.	We prove DSZOG and ADSZOG can converge to the -stationary point of the constrained
problem. Experimental results also demonstrate the superiority of our methods in terms of
accuracy and training time.
2
Under review as a conference paper at ICLR 2022
2	Proposed Method
2.1	Problem Setting
In this paper, we consider the following nonconvex constrained problem,
1n
min fo(w):= — £'i(w) s.t. f (W) ≤ 0, j = 1,…，m,	(1)
w	n i=1
where W ∈ Rd is the optimization variable, {'i(w)}n=ι are n component functions. In addition, fo :
Rd → R is a nonconvex and black-box function. fj： Rd → R, (j = 1, ∙∙∙ , m), is nonconvex/convex
and white/black-box function. Such a problem is denoted as heavily constrained problem Cotter et al.
(2016).
2.2	Reformulate the constrained problem
To solve the constrained problem, the penalty method is one of the main approaches. Following this
method, we reformulate the constrained optimization problem as the following minimax problem
over a probability distribution Clarkson et al. (2012); Cotter et al. (2016)
min max L(w, P) =fo(w) + βφ(w, P) - λ ∣∣pk2	(2)
w p∈∆m	2
where β > 0, λ > 0,夕(w,P) = Pm=IPjφj(w), φj(W) = (max{fj(w), 0})2 is the penalty
function on fj, ∆m ：= {P| Pjd=1 pj = 1, 0 ≤ pj ≤ 1, ∀j ∈ [d]} is the m-dimensional simplex
andp = [pi, ∙ ∙ ∙ ,pm] ∈ ∆m. Note different the formulation in Clarkson et al. (2012); Cotter et al.
(2016), there is an additional term --∣p∣2 which is used to ensure L to be strongly concave onp.
2.3	Doubly zeroth-order stochastic gradient method
Since we can only obtain the values of the objective and constraints, we use the zeroth-order gradient
method to solve this minimax problem 2. Obviously, calculating the zeroth-order gradient of L needs
to inquire the function values of all the constraints and `i , which has a very high time complexity if
m and n are very large.
To solve this problem, we use the stochastic manner. Specifically, since the minimax problem 2
contains two finite sums, i.e., fo(w) = 1/n Pn=I 'i(w) and 夕(w,P) = Pm=I Pjφj(w), we can
calculate their stochastic zeroth-order gradient, respectively, and then obtain the stochastic zeroth-
order gradient of L w.r.t. w. We first calculate the stochastic zeroth-order gradient of fo and 夕(w, P)
as follows,
Gμ	i,	'i(wt+ M- 'i(Wt) u,	G	, U) = φj(Wt+ μu- φj(Wt) u,
,	μ	,	μ
by sampling a ' uniformly, and a fj according to p, where μ > 0 and U 〜N(0,1d). Note here
we sample the constraint according to the distribution P, which makes our method can find the
most-violated constraint in each iteration Cotter et al. (2016). Then, combining these two terms, we
can obtain the stochastic zeroth-order gradient of L w.r.t. W as follows,
GL(Wt,Pt,'i,fj, u) = Gfμ(Wt,'i, u) + βGμ(Wt,Pt, fj, u)∙
To reduce the variance, we can sample a batch of `i , fj and Uk to calculate the zeroth-order gradient.
Given q > 0, Mi ⊆ [n] and M2 ~ P ⊆ [m], we have
GL(Wt, Pt,'Mι ,fM2 , U[q])
1q	βq
询 iS X Gμ(wt,'i, Uk)+询 X X Gμ (Wt, Pt,fj, Uk)
Then, we can use Wt+i = Wt — ηwGL (Wt, Pt, 'm1 , ∕m2 , u^) to update w.
3
Under review as a conference paper at ICLR 2022
Then, we also use stochastic gradient to update the distribution p. In each iteration, we randomly
sample a constraint fj(w) to calculate the stochastic gradient of L w.r.t. p by using
H(wt , pt , fj ) = βmej φj (wt) - λpt ,
where ej is the jth m-dimensional standard unit basis vector. We can also use the mini-batch method
here to reduce variance. Assume we have the randomly sampled index set M3 ⊆ [m], the mini-batch
gradient of L w.r.t p becomes
H(wt, pt, fM3)
βm
|M3|
ej φj (wt) - λpt ,
j∈M3
Then we can perform gradient ascent by using the rule pt+1 = Proj∆m (pt + ηpH(wt, pt, fM3)) to
update p. Note that the projection onto ∆m can be easily calculated.
The whole algorithm is presented in Algorithm 1.
Algorithm 1 Doubly stochastic zeroth-order gradient method (DSZOG).
Input： T, ∣Mι∣, ∣M2∣, ∣M3∣, nW, ηp, β ≥ 1, q, μ, λ = 1e - 8.
Output: wT .
1:	Initialize w1 .
2:	Initialize pi = p* (wι) by solving the strongly concave problem.
3:	for t = 0,…，T do
4:	Randomly sample ui, ∙…,Uq 〜N(0,1d).
5:	Randomly sample a index set M1 ⊆ [n] of `i.
6:	Sample a constraint index set M2 〜P ⊆ [m].
7:	Randomly sample a constraint index set M3 ⊆	[m].
8:	Calculate Gμ (Wt, pt, 'Mi , fM2 , u[q])	=	qi^M^^! Pi∈M1	Pk = 1 Gμ(wt, 'i, Uk )	+
q∣⅛ Pj∈M2 Pk=i 阳 wt, Pt,fj, Uk).
βm
9:	Calculate H(wt,pt,fM3) = ∣M∣ Σj∈M3 ejΦj(Wt) - λpt.
10:	Wt + 1 = Wt — nw GL(Wt, Pt,'M1,fM2 , U[q]).
11:	pt+1 = Proj∆m (pt + ηpH(Wt, pt, fM3).
12:	end for
2.4 Accelerated with momentum and variance reduction
To further speed up our method, we modify Algorithm 1 by using exponential moving average (EMA)
method Wang et al. (2017); Liu et al. (2020); Cutkosky & Mehta (2020); Guo et al. (2021) and
adaptive stepsize. The new algorithm is presented in Algorithm 2.
We first use the following exponential moving average (EMA) method on the zeroth-order and
first-order gradient to smooth out short-term fluctuations, highlight longer-term trends and reduce the
variance of stochastic gradient Wang et al. (2017); Guo et al. (2021)
zW+1 =(1 - b)zWw + bGL(wt+ι,Pt+1,'M1,fM2, U[q]), zp+1 = (1 - b)zp + bH(wt+ι,Pt+1,fM3),
where 0 < b < 1, Zw = GL(W1,Pi,'mi,/m2, u[q]) and Zp = H(wι,Pi,fM3). Here,
H(wt+1, Pt+i, fM3) is calculated on the intermediate point pt+i = (1 - a)pt + ap^t+ι, which is
widely used in Nesterov,s momentum method, where 0 < a < 1 and p^t+ι is the solution of the
distribution after updating and projecting onto the ∆m .
Another modification is the use of adaptive stepsizes of updating w and p which are proportional to
VPkZwk2 and 1/ JkZpk2
Liu et al. (2020); Guo et al. (2021). Therefore, the update rules become
wt+1
wt - ηw
Zwt
and pt+i
Proj∆m (pt + ηp

These two key components of our method, i.e., extrapolation moving average and adaptive stepsize
from the root norm of the momentum estimate, make our method enjoy two noticeable benefits:
variance reduction of momentum estimate and adaptive convergence.
4
Under review as a conference paper at ICLR 2022
Algorithm 2 Accelerated doubly stochastic zeroth-order gradient (ADSZOG).
Input： T, ∣Mι∣, ∣M2∣, ∣M3∣, β ≥ 1, q, μ, λ = 1e - 6, b ∈ (0,1), a ∈ (0,1), η* and ηp.
Output: wT .
1:
2:
3:
4:
5:
Initialize W1 .
Initialize pi = p* (wι) by solving the strongly concave
Initialize Zw = GL(wι,pi,'M1,fM2, U[q]) and zp =
for t = 1,…，T do
Zw
wt+1 = wt-ηw
6:
pt+1 = Proj ∆m (Pt + ηp
Zp
problem.
H(w1,p1,fM3).
7:
8:
9:
10:
11:
12:
/	).
Pkzpk2
Pt+1 = (1 - a)pt + apt+1.
Randomly sample uι, ∙…,Uq 〜N(0,1d).
Randomly sample a index set M1 ⊆ [n] of `i.
Sample a constraint index set M2 〜Pt+1 ⊆ [m].
Randomly sample a constraint index set M3 .
Calculate Gμ (Wt+1, pt + 1 , 'Mi , fM2 , u[q])
q∣⅛ Pj∈M2 Pk=ι Gμ (wt+ι, pt+ι,fj, Uk)
1	qf
q∣Mi∣ ∑i∈M1 ∑k=ι Gμ(Wt+1,'i, Uk)	+
13:
14:
15:
16: end for
βm
CalCUlate H(wt+ι, pt+1,fM3) = ∣M3∣ ∑j∈M3 ejφj(wt+1) - λpt+ι.
Z笊1 = (1 - b)zW + bGL(wt+ι,Pt+1,'M1,fM2, U[q]).
zpt+1 = (1 - b)zpt + bH(Wt+1, Pt+1, fM3).
3 Convergence Analysis
In this section, we discuss the convergence performance of our methods. The detailed proofs are
given in the appendix.
3.1	Stationary point
In this subsection, we first give the assumption about L which is also used in Wang et al. (2020);
Huang et al. (2020a) and then give the definitions of the stationary point.
Assumption 1 The objective function L(w, p) has the following properties:
1.	L(w, p) is continuously differentiable in w and p. L(w, p) is nonconvex with respect to w,
and L(w, p) is τ -strongly concave with respect to p.
2.	The function g(w) := maxp L(w, p) is lower bounded, and g(w) is Lg -Lipschitz continu-
ous.
3.	When viewed as a function in Rd+m, L(w, p) is L-gradient Lipschitz. That is there exists
constant L > 0 such that kVL(wι,pi) — VL(w2, p2)k2 ≤ Lk(W1,pi) 一 (w2,p2)k2 and
let K := L∕τ and κ > 1.
For a general nonconvex constrained optimization problem, the stationary point Lin et al. (2019) is
defined as follows,
Definition 1 w* is said to be the stationary point ofproblem (1), Ifthefollowing conditions holds,
m
Vw	fo(w*) + X α*Vwf (w*) = O,	fj(w*) ≤ 0,	α*fj(w*)=0,	∀i ∈{1,…，m},
j=i
where a* = [αι, •…,ɑm,]t denotes the Lagrangian multiplier and αj ≥ 0, ∀i = 1, ∙∙∙ , m.
However, it is hard to compute a solution that satisfies the above conditions exactly Lin et al. (2019).
Therefore, finding the following -stationary point Lin et al. (2019) is more practicable,
5
Under review as a conference paper at ICLR 2022
Definition 2 (E-stationary) w* is said to be the E-stationary point of problem (1), if there exists a
vector a* ≥ 0, such that the following conditions hold,
mm	m
l∣Vwfo(w*) + Xα*Vwfj(w*)k2 ≤E2, X(maχ{fj(w*),0})2 ≤E2, Xgjfj(W))2 ≤E2.
j=1	j=1	j=1
Since we reformulate the constrained problem as a minimax problem, here we give the definition
of the approximation stationary point of the minimax problem and then show its relationship with
Definition 2. According to Wang et al. (2020), we have the following definition and proposition,
Definition 3 A point (w * , p* ) is called the E-stationary point of problem minw maxp∈∆m L(w, p)
ifit satisfies the conditions: lVwL(w,p)l22 ≤ E2 and lVpL(w, p)l22 ≤ E2.
Proposition 1 If Assumption 1 holds,
2E2 + 2mλ2
β2
≤ E22 and (w*, p*) is the E-stationary point
defined in Definition 3 of the problem minw maxp∈∆m L(w, α), then w* is the E-stationary point
defined in Definition 2 of the constrained problem 1.
As proposed in Wang et al. (2020), the minimax problem 2 is equivalent to the following minimization
problem:
min g(w) := max L(w, p) = L(w, p* (w))	(3)
w	p∈∆m
where p*(w) = arg maxp L(w, p). Here, we give stationary point the minimization problem 3 and
its relationship with Definition 3 as follows,
Definition 4 We call w*an E-stationary point of a differentiable function g(w), if lVg(w*)l2 ≤ E.
Proposition 2 Under Assumption 1, ifa point w0 is an E-stationary point in terms of Definition 4,
then an E-stationary point w*, p* in terms of Definition 3 can be obtained.
Remark 1 According to Proposition 1 and Proposition 2, we have that once we find the E-stationary
point in terms of Definition 4, then we can get the E-stationary point in terms of Definition 2.
3.2	Convergence Rate of the DSZOG
Here, we present some assumptions used in our analysis, which are widely used in the convergence
analysis.
Assumption 2 For any W ∈ Rd, the following properties holds, E[G∕(w, p,'mi ,fm2 , u©)]=
Vw L(w, P), E [H (w, p,fM3)]	=	VpL(w, P), E[∣GL(wι, Pi,'mi, fM2 , u[q] ) -
VwL(W1 ,P1 )l2] ≤ σ12 andE[lH(W,P, fM3) - VwL(Wt,Pt)l2] ≤ σ22.
Let Lμ(w, P) = Eu[L(w + μu,p)] and U 〜N(0,1d). Following the theoretical analysis in Wang
et al. (2018), we have the following theorem,
1
Theorem 1 Under Assumptions 1 and 2, by setting ηw
—!-, μ :=O(Ed-3/2L-2), T > max{
2L
2(g(W0) -g(WT+1))
0.9325E2η
algorithm DSZOG has g(W) = maxp∈P L(W, P), i.e.,
w
1
T +1
4 × 162κ2(κ + 1)2(L + 1) , ηp
-,“ ％ 2 } and Po = P*(wo), our
16κηw2 E2
Pt=0E[lVwg(Wt)l22] ≤ E2.
Remark 2 Based on Proposition 1 and 2, Theorem 1 shows that our method can finally converge to
the E-stationary point of the constrained problem 1 at the rate of O(L5/T) by setting the learning
1
rate ηw = 4 X 162κ2(κ +1)2(L +1)
and K = L∕τ.
6
Under review as a conference paper at ICLR 2022
3.3	Convergence Rate of the Accelerated Method
Similar to Guo et al. (2021), We assume l/ʌ∕∣Izp∣∣2 and 1 /ʌ/kzɪɪ2 are bounded as follows,
Assumption 3 We have ci 1 ≤	】	≤ ci u and c21 ≤ - 1	≤ c2 u
,, , ,
Then, following the framework in Guo et al. (2021); Wang et al. (2018); Huang et al. (2020a), we
have the following theorem,
Theorem 2 UnderAssumPtions 1, 2 and 3, if a ≤ 1, T ≤ L, p*(wι) = pi, zp = H (wt, pt, fM3),
i	L	1	b2	τb2	2
zw	= GL(wt, Pt，'Mi ,fM2, u[q]),	ηP	≤ min{ E ,T0‰ , 32L2a2c2 1 },	ηw	≤
c2i,l	b2 τ2a2ηp2c22,l	τ2b2	2.l	2,l	2,2l	τ22
min{4Lct, 4c2UL2, 128LgL2c1,u, 128L4c1,u }' 〃 ≤ L(d +3)3/2' b ≤ min{ 沅,64^}
andT ≥ max{ 2(g(w2)-"wT)),萼,64萼}, we have ɪE[PW ∣Vg(wt)∣2] ≤ e2.
2 ηw ci,l	2b	2τ2 b	T =
Remark 3 Based on ProPosition 1 and 2, Theorem 2 shows that our method can finally converge to
the E-stationary point of the constrained problem 1. More importantly, by choosing ηw H O(1∕L6)
and b H O(1∕L6), our proposed ADSZOG has the convergence rate of O(L6∕T). Obviously, the
convergence rate of ADSZOG is faster than DSZOG.
4 Experiments
4.1	Baselines
In this subsection, we summarized the baselines used in our experiments as follows,
1.	ZOPSGDLiu et al. (2018b). In each iteration, ZOPSGD calculates the stochastic zeroth-
order gradient of f0 to update the parameters and then solve a constrained quadratic problem
to project the solution into the feasible set.
2.	ZOSCGDBalasubramanian & Ghadimi (2018). In each iteration, ZOSCGD calculates the
stochastic zeroth-order gradient of f0 and then use the conditional gradient method to update
the parameters by solving a constrained linear problem.
Note that both ZOPSGD and ZOSCGD are designed for solving the constrained problem with white-
box constraints. However our methods can solve the problem with nonconvex/convex white/black-box
constraints. To compare the performance of our methods with ZOPSGD and ZOSCGD, we design
two problem with white-box constraints in the next subsection.
4.2	Applications
In this subsection, we give the introduction of the applications used in our experiments.
Classification with Pairwise Constraints We evaluate the performance of all the methods on the
binary classification with pairwise constraints learning problem. Given a set of training samples D =
{(xi, yi)}in=i, where xi ∈ Rd and yi ∈ {+1, -1}. In this task, we learn a linear model h(x, w) =
xTw to classify the dataset and ensure the any positive sample xi+ ∈ D+ := {(xi, +1)}in=pi has
larger function value than the negative sample xj- ∈ D- := {(xj, +1)}in=ni, where np and nn
denotes the number of positive samples and negative samples, respectively. Then, we can formulate
this problem as follows,
1n	+	+	+
min — y '(h(xi, w), yi), s.t. h(x+, W) — h(x- , W) ≥ 0, ∀x+ ∈ D+ X- ∈ D
wn	i	j	i	j
i=i
where '(u,v) = c2(1 — exp( —(V~u)-)) is viewed as a black-box function. We summarized
c2
the datasets used in this application in Table 2. We randomly sample 1000 data samples from
7
Under review as a conference paper at ICLR 2022
Table 2: Datasets used in classification with pairwise constraints (We give the approximate size of
constraints).
Data	Dimension	Constriants
w8a	300	'8000
a9a	123	'40000
gen	50	'60000
svmguide3	22	'40000
Table 3:	Test accuracy (%) of all the methods in classification with pairwise constraints.
Data ADSZOG DSZOG ZOSCGD ZOPSGD
a9a	75.90 ± 0.26	75.37 ± 0.55 75.35 ± 0.13	75.37 ± 0.19
"w8α	89.94 ± 0.28	86.62 ± 0.93 83.53 ± 0.58	89.02 ± 0.97
■gen	82.33 ± 0.76	82.11 ± 0.28 66.33 ± 0.07	66.83 ± 0.57
Svmguide3 79.56 ± 0.49	78.83 ± 0.90 71.21 ± 0.57	78.63 ± 0.26
the original datasets, and then divide all the datasets into 3 parts, i.e., 50% for training, 30% for
testing and 20% for validation. We fix the batch size of data sample at 128 for all the methods and
|M2| = |M3| = 128. The learning rates of all the methods are chosen from {0.01, 0.001, 0.0001}.
In our methods, the penalty parameter β is chosen from {0.1, 1, 10}, a and b are chosen from
{0.1, 0.5, 0.9} on the validation sets.
Classification with Fairness Constraints. In this problem, we consider the binary classification
problem with a large amount of fairness constraints Zafar et al. (2017). Given a set of training
samples D = {(xi, yi)}in=1, where xi ∈ Rd and yi ∈ {-1, +1}. In this task, we learn a linear
model h(x, w) = xTw. Assume that each sample has an associate sensitive feature vector z ∈ Rr.
We denote zij ∈ {0, 1} as the j-th sensitive feature of i-th sample. The classifier h cannot use the
protected characteristic z at decision time, as it will constitute an unfair treatment. A number of
metrics have been used to determine how fair a classifier is with respect to the sensitive features.
According to Zafar et al. (2017), the fair classification problems can be formulated as follows,
1n	1n	1n
min— £'(h(xi, W)Jyi) s.t.— E(Zij - Zj)g(yi, Xi) ≤ c, - E(Zij - Zj)g(yi, Xi) ≥ -c,
wn	n	n
i=1	i=1	i=1
where j = 1,…，r, '(u, V) denotes the loss functions, C is the covariance threshold which specifies
an upper bound on the covariance between the sensitive attributes z and the signed distance g(y, X).
We use the hinge loss `(u, v) = max{1 - uv, 0} in this experiment and we view it as a black-
box function. In addition, we use the following two functions to build the fairness constraints,
{min{0, ɪ yy yyh(x, w)}
1	2- y	. Since the datasets with multiple sensitive features are
min{0, —2^ h(x, w)}
difficult to find, we generate 4 datasets with 2000 samples in this task and summarize them in Table 4.
For each dataset, we randomly choose several features to be the sensitive features, and then separate
(a) a9a	(b) w8a	(c) gen
Figure 1: Test accuracy against training time of all the methods in classification with pairwise
constraints (We stop the algorithms if the training time is more than 10000 seconds).
8
Under review as a conference paper at ICLR 2022
Table 4:	Datasets used in classification with fairness constraints.
Data Dimension Sensitive Features Constraints
D1	100	10	40
D2	200	20	80
D3	300	20	80
D4	400	20	80
Table 5: Test accuracy (%) of all the methods in classification with fairness constraints.
Data ADSZOG DSZOG ZOSCGD ZOPSGD
Dl^^87.33 ± 0.38 86.83 ± 0.52	51.08 士。57	59.16	± 0.37
D2	84.75 ± 0.25 84.02 ± 0.31	69.70 ± 0.24	68.00	± 0.54
D3	83.58 ± 0.14 82.00 ± 0.05	66.33 ± 0.30	66.84	± 0.57
D4	64.91 ± 0.94 64.50 ± 0.25	52.16 ± 0.38	55.25	± 0.90
them into 3 parts, i.e., 50% for training, 30% for testing and 20% for validation. We fix the batch
size of data sample at 128 for all the methods and |M2| = |M3| = 10. The learning rates of all the
methods are chosen from {0.01, 0.001, 0.0001}. For our methods, the penalty parameter β is chosen
from {0.1, 1, 10}, a and b are chosen from {0.1, 0.5, 0.9} on the validation sets.
We run all the methods 10 times on a 3990x workstation.
4.3 Results and Discussion
We present the results in Figures 1, 2 and Tables 3, 5. Note that for ZOSCGD and ZOPSGD, if the
training time is larger than 10000 seconds, the algorithms are stopped. From Tables 3 and 5, we can
find that our methods ADSZOG and DSZOG have the highest test accuracy in most cases in both
two applications. In addition, from Figures 1 and 2, we can find that our methods are faster than
ZOSCGD and ZOPSGD. This is because ZOSCGD and ZOPSGD need to solve a subproblem with
a large number of constraints in each iteration and the existing Python package cannot efficiently
deal with such a problem. What’s worse, ZOPSGD and ZOSCGD focus on solving the problem
with convex constraints while the constraints in the fairness problem are nonconvex. This makes
ZOPSGD and ZOSCGD cannot find the stationary point. However, by using the penalty framework,
our methods can still converge to the stationary point when the constraints are nonconvex. In addition,
by using a stochastic manner on the constraint, our method can efficiently deal with a large number
of convex/nonconvex constraints. In addition, we can also find that ADSZOG can converge faster
than DSZOG. This is because we use the exponential moving average in ADSZOG which makes it
benefits from variance reduction and momentum acceleration. All these results demonstrate that our
method is superior to ZOSCGD and ZOPSGD in the heavily constrained nonconvex problem.
5 Conclusion
In this paper, we propose two efficient ZO method to solve the heavily constrained nonconvex black-
box problem, i.e., DSZOG and ADSZOG. We also give the convergence analysis of our proposed
methods. The experimental results on two applications demonstrate the superiority of our method in
terms of accuracy and training time .
*8
0 0.7
8
f 0.6
S
0)
I 0.5
0.4
10-5
10
100
Time
100	10	10-5
Time (S)
(a)	D1
100	10	10-5
Time (S)
(b)	D2
(d) D4
(c) D3
Figure 2: Test accuracy against training time of all the methods in classification with fairness
constraints (We stop the algorithms if the training time is more than 10000 seconds).
9
Under review as a conference paper at ICLR 2022
References
Alekh Agarwal, Ofer Dekel, and Lin Xiao. Optimal algorithms for online convex optimization with
multi-point bandit feedback. In COLT,, pp. 28-40. Citeseer, 2010.
Krishnakumar Balasubramanian and Saeed Ghadimi. Zeroth-order (non)-convex stochastic opti-
mization via conditional gradient and gradient updates. In Proceedings of the 32nd International
Conference on Neural Information Processing Systems, pp. 3459-3468, 2018.
Krishnakumar Balasubramanian and Saeed Ghadimi. Zeroth-order nonconvex stochastic optimization:
Handling constraints, high dimensionality, and saddle points. Foundations of Computational
Mathematics, pp. 1-42, 2021.
Pin-Yu Chen, Huan Zhang, Yash Sharma, Jinfeng Yi, and Cho-Jui Hsieh. Zoo: Zeroth order
optimization based black-box attacks to deep neural networks without training substitute models.
In Proceedings of the 10th ACM workshop on artificial intelligence and security, pp. 15-26, 2017.
Krzysztof Choromanski, Mark Rowland, Vikas Sindhwani, Richard Turner, and Adrian Weller.
Structured evolution with compact architectures for scalable policy optimization. In International
Conference on Machine Learning, pp. 970-978. PMLR, 2018.
Kenneth L Clarkson, Elad Hazan, and David P Woodruff. Sublinear optimization for machine
learning. Journal of the ACM (JACM), 59(5):1-49, 2012.
Andrew Cotter, Maya Gupta, and Jan Pfeifer. A light touch for heavily constrained sgd. In Conference
on Learning Theory, pp. 729-771. PMLR, 2016.
Ashok Cutkosky and Harsh Mehta. Momentum improves normalized sgd. In International Conference
on Machine Learning, pp. 2260-2268. PMLR, 2020.
Cong Fang, Chris Junchi Li, Zhouchen Lin, and Tong Zhang. Spider: near-optimal non-convex
optimization via stochastic path integrated differential estimator. In Proceedings of the 32nd
International Conference on Neural Information Processing Systems, pp. 687-697, 2018.
Hongchang Gao and Heng Huang. Can stochastic zeroth-order frank-wolfe method converge faster
for non-convex problems? In International Conference on Machine Learning, pp. 3377-3386.
PMLR, 2020.
Saeed Ghadimi and Guanghui Lan. Stochastic first-and zeroth-order methods for nonconvex stochastic
programming. SIAM Journal on Optimization, 23(4):2341-2368, 2013.
Bin Gu, Zhouyuan Huo, and Heng Huang. Zeroth-order asynchronous doubly stochastic algorithm
with variance reduction. arXiv preprint arXiv:1612.01425, 2016.
Zhishuai Guo, Yi Xu, Wotao Yin, Rong Jin, and Tianbao Yang. On stochastic moving-average
estimators for non-convex optimization. arXiv preprint arXiv:2104.14840, 2021.
Feihu Huang, Shangqian Gao, Jian Pei, and Heng Huang. Accelerated zeroth-order and first-order
momentum methods from mini to minimax optimization. arXiv preprint arXiv:2008.08170, 2020a.
Feihu Huang, Lue Tao, and Songcan Chen. Accelerated stochastic gradient-free and projection-free
methods. In International Conference on Machine Learning, pp. 4519-4530. PMLR, 2020b.
Qihang Lin, Runchao Ma, and Yangyang Xu. Inexact proximal-point penalty methods for constrained
non-convex optimization. arXiv preprint arXiv:1908.11518, 2019.
Tianyi Lin, Chi Jin, and Michael Jordan. On gradient descent ascent for nonconvex-concave minimax
problems. In International Conference on Machine Learning, pp. 6083-6093. PMLR, 2020.
Mingrui Liu, Wei Zhang, Francesco Orabona, and Tianbao Yang. Adam + : A stochastic method with
adaptive variance reduction. arXiv preprint arXiv:2011.11985, 2020.
Sijia Liu, Bhavya Kailkhura, Pin-Yu Chen, Paishun Ting, Shiyu Chang, and Lisa Amini. Zeroth-
order stochastic variance reduction for nonconvex optimization. Advances in Neural Information
Processing Systems, 31:3727-3737, 2018a.
10
Under review as a conference paper at ICLR 2022
Sijia Liu, Xingguo Li, Pin-Yu Chen, Jarvis Haupt, and Lisa Amini. Zeroth-order stochastic projected
gradient descent for nonconvex optimization. In 2018 IEEE Global Conference on Signal and
Information Processing (GlobalSIP), pp. 1179-1183. IEEE, 2018b.
Yurii Nesterov and Vladimir Spokoiny. Random gradient-free minimization of convex functions.
Foundations of Computational Mathematics, 17(2):527-566, 2017.
Lam M Nguyen, Jie Liu, Katya Scheinberg, and Martin Takdc. Sarah: A novel method for machine
learning problems using stochastic recursive gradient. In International Conference on Machine
Learning, pp. 2613-2621. PMLR, 2017.
Mengdi Wang, Ethan X Fang, and Han Liu. Stochastic compositional gradient descent: algorithms
for minimizing compositions of expected-value functions. Mathematical Programming, 161(1-2):
419-449, 2017.
Yining Wang, Simon Du, Sivaraman Balakrishnan, and Aarti Singh. Stochastic zeroth-order opti-
mization in high dimensions. In International Conference on Artificial Intelligence and Statistics,
pp. 1356-1365. PMLR, 2018.
Zhongruo Wang, Krishnakumar Balasubramanian, Shiqian Ma, and Meisam Razaviyayn. Zeroth-
order algorithms for nonconvex minimax problems with improved complexities. stat, 1050:22,
2020.
Muhammad Bilal Zafar, Isabel Valera, Manuel Gomez Rodriguez, and Krishna P Gummadi. Fairness
beyond disparate treatment & disparate impact: Learning classification without disparate mistreat-
ment. In Proceedings of the 26th international conference on world wide web, pp. 1171-1180,
2017.
11