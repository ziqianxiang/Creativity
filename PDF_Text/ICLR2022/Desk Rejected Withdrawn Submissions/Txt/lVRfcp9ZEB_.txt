Under review as a conference paper at ICLR 2022
IsoScore: Measuring the Uniformity
of Vector Space Utilization
Anonymous authors
Paper under double-blind review
Ab stract
The recent success of distributed word representations has led to an increased in-
terest in analyzing the properties of their spatial distribution. Current metrics sug-
gest that contextualized word embedding models do not uniformly utilize all avail-
able dimensions when embedding tokens in vector space. Previous works argue
that encouraging isotropy in embedding space corresponds to improved perfor-
mance on downstream tasks. However, existing metrics—average random cosine
similarity, for example—do not properly measure isotropy and tend to obscure
the true spatial distribution of point clouds. To address this issue, we propose
IsoScore: a novel metric that quantifies the degree to which a point cloud uni-
formly utilizes the ambient vector space. We demonstrate that IsoScore has sev-
eral desirable properties, such as mean invariance and direct correspondence to
the number of dimensions used that existing scores do not possess. Furthermore,
IsoScore is conceptually intuitive, making it well suited for analyzing the distri-
bution of arbitrary point clouds in vector space, not necessarily limited to point
clouds of word embeddings alone. We conclude by using IsoScore to demonstrate
that a number of recent conclusions in the NLP literature that have been derived
using brittle metrics of spatial distribution may be incomplete or altogether inac-
curate.
1	Introduction & Background
The first step in many natural language processing pipelines embeds words into a vector space.
Recent studies have analyzed the spatial distribution of point clouds output by word embedding
models. The literature overwhelmingly agrees that point clouds induced by contextualized embed-
ding models do not uniformly utilize all dimensions of the vector space they occupy (Ethayarajh,
2019; Mickus et al., 2019; Cai et al., 2021; Coenen et al., 2019b; Gao et al., 2019). Figure 1 illus-
trates a two-dimensional disk that uniformly utilizes the x and y axes in two-dimensional space, but
does not uniformly utilize all dimensions when embedded into three dimensions.
Figure 1: From left to right, a line, disk, and ball embedded in 3D space.
A distribution is isotropic when variance is uniformly distributed across all dimensions. Several
authors suggest that isotropy correlates with improved performance of embedding models (Coenen
et al., 2019a; Gong et al., 2018; Hasan & Curry, 2017; Hewitt & Manning, 2019; Liang et al.,
2021; Zhou et al., 2019; 2021). However, current methods of measuring the spatial utilization of
point clouds do not truly measure isotropy. The most commonly used metrics for measuring spatial
distribution in embedding spaces include average cosine similarity, the partition score, variance
October 4, 2021
1
Under review as a conference paper at ICLR 2022
explained and intrinsic dimensionality estimation. In Section 6 we argue that all those metrics have
fundamental shortcomings that render them inadequate measures of spatial distribution, which may
lead to erroneous analytical conclusions.
To overcome these limitations, we introduce IsoScore: a novel metric for measuring the extent to
which the variance of a point cloud is uniformly distributed across all dimensions in vector space.
In contrast to previous attempts of measuring isotropy, IsoScore is the first score that incorporates
the mathematical definition of isotropy into its formulation. As a result, IsoScore has the following
desirable properties that surpass the capabilities of existing metrics: (i) It is a global measure of
how uniformly distributed points are in vector space that is invariant to changes in the distribution
mean and scalar changes in covariance; (ii) It is rotation invariant; (iii) It increases linearly as more
dimensions are utilized; and (iv) It is not skewed by highly isotropic subspaces within the data.
This paper makes the following novel contributions.
1.	We propose essential conditions for a robust metric of spatial distribution and build a testing
suite to empirically verify if a given metric meets these conditions.
2.	We highlight fundamental shortcomings of state-of-the-art metrics for quantifying the spa-
tial distribution of point clouds.
3.	We present IsoScore, the first rigorously derived metric of spatial distribution in terms of
isotropy. We show that it has desirable properties that none of the other scores possess.
4.	We share an efficient Python implementation of IsoScore with the community.1
The remainder of this paper is structured as follows: Section 2 reviews previous work analyzing the
distribution of point cloud data and presents ways in which these methods have been used to quantify
the geometry of contextualized word embedding spaces. Section 3 formally defines isotropy and
describes existing metrics in detail. The formal definition of IsoScore is presented in Section 4, and
an intuitive view on its mechanism is offered in Section 5. In Section 6, we report empirical results
from experiments on real data and provide a thorough discussion of the results. Finally, Section 7
concludes with an outlook on future directions of work.
2 Related Work
2.1	Word Embeddings
In recent years, there has been an increased interest in analyzing the spatial organization of point
clouds induced by word embeddings (Mickus et al., 2019; Ethayarajh, 2019; Coenen et al., 2019b;
Cai et al., 2021; Mu et al., 2017; Liang et al., 2021). Several studies have concluded that contextu-
alized embeddings form highly anisotropic, “narrow cones” in vector space (Ethayarajh, 2019; Cai
et al., 2021; Gao et al., 2019; Gong et al., 2018). The most prevalent tools used to quantify the ge-
ometry of word embedding models are based on average cosine similarity. Ethayarajh (2019) notes
that in some cases, contextualized embedding models have an average random cosine similarity that
approaches 1.0, meaning all points are oriented in the same direction in space irrespective of their
syntactic or semantic function. In Section 6, we demonstrate that average cosine similarity is signif-
icantly influenced by the ratio between the mean and variance of the data irrespective of how data
points are distributed in vector space. It is well known that word embedding models have non-zero
mean vectors (Yonghe et al., 2019; Liang et al., 2021). In the case of GPT-2 embeddings obtained
from the WikiText-2 corpus (Merity et al., 2016), we find values in the mean vector range from
-32.36 to 198.19. Although cosine similarity has long been used to capture the “semantic” differ-
ences between words in static embeddings, adapting any cosine similarity-based metric to measure
isotropy obscures the true distribution of contextualized word embeddings.
2.2	Existing Metrics
We briefly review the most commonly used tools to measure the spatial distribution of point clouds
X ⊆ Rn. A mathematical exposition of those tools can be found in Appendix A.
Average Cosine Similarity: We define the Average Cosine Similarity Score as 1 minus the average
cosine similarity of N randomly sampled pairs of points from X . It is commonly believed that
1Anonymized code base: https://github.com/zpckyjg0/IsoScore.git
2
Under review as a conference paper at ICLR 2022
a score of 0 indicates X is anisotropic and a score of 1 indicates that X is isotropic. Section 6
demonstrates this is not the case.
Partition Isotropy Score: Mu et al. (2017) define this score to be a particular quotient involving the
partition function Z(c) := Px∈X exp(cTx), where c is carefully chosen from the eigenspectrum of
X . It is believed that a score closer to 0 indicates an anisotropic space, while a score near 1 indicates
an isotropic space. Towards this, Mu et al. (2017) demonstrate that a score of 1 implies that the
eigenspectrum of X is flat. We refer to this as the Partition Score.
Intrinsic Dimensionality: Algorithms for estimating intrinsic dimensionality aim to compute the
true dimension of a given manifold from which we assume a point cloud has been sampled. Intrinsic
dimensionality has been used to argue word embedding models are anisotropic (Cai et al., 2021).
We use the MLE method to calculate intrinsic dimensionality (Levina & Bickel, 2004). Dividing the
intrinsic dimensionality of X by n provides us with a normalized score of isotropy, which we refer
to as the ID Score.
Variance Explained Ratio: The variance explained ratio, which we refer to as the VarEx Score,
measures how much total variance is explained by the first k principal components of the data. We
compute this by dividing the variance explained by the first k principal components by k/n. The
VarEX Score requires us to specify a priori the number of principal components we wish to examine,
which makes comparisons between vector spaces with different dimensions difficult.
Section 6 demonstrates that all existing metrics have fundamental shortcomings that make them
unreliable measures of spatial distribution.
3	Measuring Embedding Space Utilization
We now formulate the essential properties a good metric of spatial utilization should possess.
3.1	Dimensions utilized
Given a point cloud X ⊆ Rn, we measure how many dimensions of Rn are truly utilized by X. We
make the following definition:
Definition 3.1. Consider a point cloud X ⊆ Rn. Let Σ be the covariance matrix of X and assume
all the off-diagonal entries of Σ are zero. Let ΣD ∈ Rn denote the diagonal of Σ.
1.	We say X utilizes k dimensions in Rn if the first k entries of ΣD are non-zero and the
remaining n - k entries are zero.
2.	We say X uniformly utilizes k dimensions in Rn if X utilizes k dimensions in Rn and if all
the non-zero entries in ΣD are equal.
For example, we denote by In(k) the n × n covariance matrix where ai,i = 1 for i ∈ {1, 2, ..., k} and
all other elements are 0. Note that when k = n, we recover the identity matrix. Thus, In(k) represents
a covariance matrix where the first k dimensions are being uniformly utilized.
Having a diagonal sample covariance matrix Σ implies there are no correlations between any coor-
dinates of X . In Section 4, we reduce the general case of X to the case where the covariance matrix
of X is diagonal. Figure 2 illustrates three point clouds in R2 that each utilize 2 dimensions. We
argue that it is of practical importance to differentiate between the cases in Figure 2. The leftmost
panel uniformly utilizes all dimensions of R2, while the rightmost panel does not uniformly utilize
two dimensions of space.
3.2	Definition of Isotropy
A distribution is isotropic if its variance is uniformly distributed across all dimensions. Namely, the
covariance matrix of an isotropic distribution is proportional to the identity matrix. Conversely, an
anisotropic distribution of data is one where the variance is dominated by a single dimension. For
example, a line in n-dimensional vector space is maximally anisotropic. Robust isotropy metrics
should return maximally isotropic scores for balls and minimally isotropic (i.e. anisotropic) scores
for lines. Appendix C provides a geometric interpretation of “medium isotropy”. We interpret
3
Under review as a conference paper at ICLR 2022
Figure 2: Points sampled from a 0 mean, 2D Gaussian with covariance ( x0 01 ) where x = 1, 3, 75.
a medium isotropic space in Rn to be one where the data uniformly utilizes approximately n/2
dimensions in space.
3	.3 The Six Axioms: Properties of an Ideal Measure of Spatial Utilization
We now axiomatize the essential properties that a measure of isotropy should possess.
Axiom 1: Mean Invariance. Given that isotropy is strictly a property of the covariance matrix, an
ideal score should be invariant to changes in the mean.
Axiom 2: Scalar Invariance. Since isotropy is defined as uniformity of variance across all direc-
tions, isotropy scores should be invariant to scalar multiplications of the covariance matrix of the
underlying distribution of the data.
Axiom 3: Maximum Variance. As we increase the maximum variance value in our covariance
matrix, we expect isotropy scores to monotonically decrease to zero. Figure 2 illustrates the effect
of increasing the maximum value in the covariance matrix. Increasing the maximum variance value
increases the amount of variance explained by the first principal component of the data. In other
words, larger maximum variance values reduce the efficiency of spatial utilization.
Axiom 4: Rotation Invariance. Given a point cloud X ⊂ Rn, an ideal measure of spatial utilization
should remain constant under rotations of X since the distribution of principal components remains
constant under rotation. Accordingly, we consider the canonical distribution of the variance of X
to be the variance after projecting X using principal component analysis. Figure 3 illustrates the
process of PCA-reorientation.
Figure 3: Left: 2D zero-mean Gaussian with covariance (Q18 0ι8). We rotate X by 120° and 240°,
respectively. Right: Points after applying PCA reorientation.
Axiom 5: Dimensions Used. As described in Subsection 3.1, there is a direct link between isotropy
and the number of dimensions utilized by the data. Intuitively, increasing the number of dimensions
uniformly utilized by the data expands the number of principal components it takes to explain all of
the variance in the data. Accordingly, a good score of spatial utilization should increase linearly as
we increase the number of dimensions uniformly utilized by the data. Figure 1 depicts data utilizing
one, two, and three out of three ambient dimensions, respectively.
Axiom 6: Global stability. A metric of efficient spatial utilization should be a global reflection of
the distribution. A robust metric should be stable even when the data exhibits small subpopulations
where a score would return an extreme value. We test this by computing the IsoScore for the union
of a noisy sphere and a line; we provide a geometric rendering of this in Figure 8 in Appendix D. We
refer to this test as the “skewered meatball” test. A good score of spatial distribution for a “skewered
4
Under review as a conference paper at ICLR 2022
meatball” should reflect the ratio of the number of points sampled from the line and the number of
points sampled from the sphere.
In Table 1, we list which existing metrics satisfy which axioms. Section 6 outlines the numerical
experiments we execute to obtain this table.
Table 1: Performance of current methods for measuring spatial utilization
Test	IsoScore	AvgCosSim	Partition	IntrinsicDim	VarEx
1. Mean Invariance	✓	X	X	✓	✓
2. Scalar Invariance	✓	X	X	✓	✓
3. Maximum Variance	✓	X	✓	X	X
4. Rotation Invariance	✓	✓	X	✓	✓
5. Dimensions Used	✓	X	X	X	X
6. Global Stability	J	X	X	✓	X
4	Formal Definition of IsoScore
This section introduces the proposed IsoScore metric of uniform spatial utilization. Algorithm 1
gives a high-level overview of the procedure. Afterwards, we discuss the individual steps in detail.
Algorithm 1 IsoScore
1:	begin Let X ⊂ Rn be a finite collection of points.
2:	Let XPCA denote the points in X transformed by the first n principal components.
3:	Define ΣD ∈ Rn as the diagonal of the covariance matrix of XPCA.
4:	Normalize diagonal to ∑d := √n ∙ ∑d/∣∣∑d ∣∣, where ∣∣ ∙ ∣∣ is the standard Euclidean norm.
5:	The isotropy defect is δ(X):= ∣∣ΣD 一 1∣∕p2(n - √n), where 1 = (1,..., 1)> ∈ Rn.
6:	X uniformly occupies φ(X) := (n 一 δ(X )2(n - √n))2∕n2 percent of ambient dimensions.
7:	Transform φ(X) so it can take values in [0,1], via ι(X) := (n ∙ φ(X) - 1)∕(n - 1).
8:	return: ι(X)
9:	end
Step 1: Start with a point cloud X ⊆ Rn. IsoScore takes as input a finite subset ofRn and outputs
a number in the interval [0, 1] that represents the extent to which X is isotropic.
Step 2: PCA-reorientation of data set. Execute PCA on X, where the target dimension remains the
original n. Performing PCA reorients the axes of X so that the i’th coordinate accounts for the i’th
greatest variance. Further, it eliminates all correlation between dimensions making the covariance
matrix diagonal. We denote the transformed space as XPCA .
Step 3: Compute variance vector of reoriented data. Compute the n × n covariance matrix of
XPCA ; denote this matrix by Σ. Let ΣD denote the diagonal of the covariance matrix. We refer
to ΣD as the variance vector, and we identify ΣD as a vector in Rn . Performing Step 2 causes all
off-diagonal entries of the covariance matrix ofXT to vanish, which allows us to ignore off-diagonal
elements for the rest of the computation.
Step 4: Length normalization of variance vector. We define the normalized variance vector to be
ςD := √n •瑞
where ∣(xι,…,Xn)∣ := ,x2 +--------+ Xn denotes the standard Euclidean norm on Rn. Note that as
a result of this normalization, We have ∣∣∑d ∣∣ = √n.
Step 5: Compute the distance between the covariance matrix and identity matrix. Denote the
diagonal of the n × n identity matrix by 1 ∈ Rn . Then we define the isotropy defect of X to be
..^ ..
w ∣∣ςd - 1II
δ(X ) := —/	=.
P2(n - √n)
5
Under review as a conference paper at ICLR 2022
1 ʌ 1 r` ∙ . ∙	/` .< ι - Tl	1	Il A Il	Il -t Il	I-	ɪ. ∕' 11	C	.Λ .	∙	1
By definition of the Euclidean	norm, We have	∣∣∑d∣∣	= ||1||	= √n.	It follows	from	the triangle
inequality that ∣∑d 一 1∣ ∈ [0,2√n]. Crucially, we prove in Appendix B that achieving a value of
2√n using a valid covariance matrix is impossible. The largest value that can be attained is with
the matrix (aj)i,j=ι,...,n defined by a11 = √n and a，a = 0 whenever i > 1. One can compute
that the Euclidean norm in this case is ∣∣ΣD 一 1∣ = ,2(n — √n). Choosing this normalization
factor guarantees that δ(X) ∈ [0, 1], where 0 represents a perfectly isotropic space and 1 represents
a perfectly anisotropic space.
Step 6: Use the isotropy defect to compute percentage of dimensions isotropically utilized. We
argue in Heuristic C.1 that if X has isotropy defect δ(X), then X isotropically occupies approx-
imately k(X) = (n 一 δ(X)2(n 一 √n))2∕n dimensions in Rn. Because δ(X) ∈ [0,1], one can
estimate that k(X) ∈ [1, n] so the fraction of dimensions utilized is φ(X) := k(X)/n ∈ [1/n, 1].
Step 7: Linearly scale percentage of dimensions utilized to obtain IsoScore. The fraction of
dimensions utilized, φ(X), is close to the final IsoScore, but it falls within the interval [1/n, 1]. As
we want the possible range of scores to fill the interval [0, 1], we apply the affine function that maps
1/n 7→ 0 and 1 7→ 1. Thus, S : [1/n, 1] → [0, 1] : x 7→ (nx 一 1)/(n 一 1). Once we compose these
transformations, we obtain IsoScore:
ι(X) :
(n 一 δ(X)2(n 一 √n))2 一 n
n(n 一 1)
(4.1)
5	Interpretation: IsoScore as a Summary Statistic
We will now provide an intuitive interpretation for the IsoScore of a point cloud X ⊆ Rn . The
interested reader should consult Appendix C for an in-depth explanation of this heuristic.
Heuristic 5.1. The IsoScore of X is roughly the fraction of dimensions uniformly utilized by X.
For example, an IsoScore near 0.5 indicates that around half of the dimensions are utilized; and
more generally, an IsoScore near α ∈ [0,1] indicates that approximately n ∙ a of the dimensions of
Rn are uniformly utilized by X . Table 2 illustrates this trend where IsoScore increases linearly as
more dimensions are uniformly utilized in R9 .
Table 2: Linearly increasing dimensions utilized in R9 linearly increases IsoScore
ι(I9(1))	ι(I9(2))	ι(I9(3))	ι(I9(4))		ι(I9(6))	ι(I9(7))	ι(I9(8))	ι(I9(9))
0.000	0.125	0.250	0.375	0.500	0.625	0.750	0.875	1.000
6	Experiments
In Subsection 6.1, we present results from numerical experiments designed to test each of the
isotropy scores presented in this paper against the six axioms outlined in Section 3.3. Exact de-
scriptions of the numerical experiments are provided in Appendix D.
In Subsection 6.2, we demonstrate the merit of IsoScore by recreating the experimental setup pre-
sented in (Cai et al., 2021). We create word embeddings from the WikiText-2 corpus using GPT
(Radford & Narasimhan, 2018), GPT-2 (Radford et al., 2019), BERT (Devlin et al., 2018) and Dis-
tilBERT (Sanh et al., 2019) and calculate isotropy scores for each layer of the model.
6.1	Testing the metrics against the six axioms
Test 1: Mean Invariance. When the covariance matrix of a distribution is proportional to the
identity matrix, isotropy metrics should return a score of 1 regardless of the value of the mean.
Figure 4 demonstrates that IsoScore is the only metric that is mean invariant. IsoScore is mean-
agnostic since it is a function of the covariance matrix. Average cosine similarity and the partition
score are skewed by non-zero mean data. Our results show that, for an Isotropic Gaussian with
covariance matrix λ ∙ In and common mean vector M = [μ,μ,…，μ], the average cosine similarity
of points sampled from this distribution will approach 0 as we increase the ratio between μ∕λ.
6
Under review as a conference paper at ICLR 2022
Figure 4: Left: Scores of points sampled from a 10-dimensional Gaussian with identity covariance
and common mean vector ranging from 0 to 20. Center: Scores for the scalar covariance test for
a 5-dimensional Gaussian with a common mean vector M = [3, 3, 3, 3, 3]. Right: Scores for the
Maximum Variance test for 10-dimensional, zero-mean Gaussians.
Consequently, zero-centering data can increase average cosine similarity to 1 without impacting the
distribution of the variance.
Test 2: Scalar Invariance. Scores should reflect uniform utilization of space for any λ > 0. Note
that we choose a non-zero mean vector for the Gaussian distribution to provide further experimental
evidence that average random cosine similarity appears to be a function of the ratio between the
mean and variance of the data. Figure 4 shows that IsoScore and the intrinsic dimensionality score
are the only metrics that are invariant under scalar multiplication to the covariance matrix and return
a score 1 for each value of λ. In Step 4 of IsoScore, we normalize the diagonal of the covariance
matrix to have the same norm as the diagonal of the identity matrix, which ensures IsoScore is
invariant to scalar changes in covariance.
Test 3: Maximum Variance. An effective score should monotonically decrease to 0 as we in-
crease the maximum variance. Steps 4 and 5 of IsoScore ensure that the less equitably the mass
in the covariance vector is distributed, the greater the isotropy defect will be. Figure 2 visualizes
this phenomenon for a 2 Dimensional Gaussian. The ID Score fails this test since the intrinsic
dimensionality estimate is 2.0 for all point clouds depicted in Figure 2.
Table 3: Performance of current methods on Test 4: Rotation Invariance
	IsoScore	AvgCosSim	Partition Score	ID Score	VarEx Score
^X	0.216	0.990	0990	1.000	0.500
X 120°	0.216	0.968	0.696	1.000	0.500
X 240。	0.216	0.981	0.677	1.000	0.500
X PCA	0.216	0.993	0.599	1.000	0.500
Test 4: Rotation Invariance. We rotate our baseline point cloud X by 120° and 240°. Lastly, We
project X using PCA reorientation while retaining dimensionality to obtain a point cloud XPCA.
We record results in Table 3. Only IsoScore, ID Score, and VarEx Score return constant values.
The partition score Would return a constant value if it Were feasible to compute the true optimization
problem. The approximate version of the partition score, hoWever, depends too strongly on the basis.
IsoScore is rotation invariant by design. In Step 2, IsoScore projects the point cloud of data in the
directions of maximum variance before computing the covariance matrix of the data.
Test 5: Dimensions Used (Fraction of Dimensions Used Test). The number of dimensions used in
a point cloud X ⊂ Rn provides a sense of hoW uniformly X utilizes the ambient space. A reliable
metric should return scores near 0.0, 0.5, and 1.0 When number of dimensions used is 1, bn/2c, and
n, respectively. Figure 5 shoWs that only IsoScore models ideal behavior for the dimensions used
test. A rigorous explanation of Why IsoScore reflects the percentage of 1s present in the diagonal
of the covariance matrix is provided in Heuristic C.2. Although the intrinsic dimensionality score
monotonically increases as We increase k, it fails to reach 1 When all dimensions are uniformly
7
Under review as a conference paper at ICLR 2022
Figure 5: Left and center: Scores for the two Dimensions Used tests. Right: Scores for the “skew-
ered meatball” test in 3 dimensions.
utilized. Average cosine similarity fails this test, as it stays constant near 1 regardless of the fraction
of dimensions uniformly utilized.
Test 5: Dimensions Used (High Dimensional Test). Metrics of spatial utilization should allow for
easy comparison between different vector spaces even when the dimensionality of the two spaces
is different. Figure 5 illustrates that IsoScore, the average cosine similarity score, and the partition
score pass this test, as they stay constant near 1. Note that the line for IsoScore decreases slightly.
By the law of large numbers, the more data points we sample from the Gaussian distribution, the
closer the covariance matrix will be to the covariance matrix from which it was sampled. The VarEx
Score is not stable under an increase in dimension primarily because it requires the user to specify
the percentage of principal components used in calculating the score. Note that the ID Score begins
to decrease simply by increasing the dimensionality of the space since the MLE method is not very
well suited for estimating the intrinsic dimension of isotropic Gaussian balls.
Test 6: Global Stability. To evaluate which scores are not skewed by highly concentrated subspaces,
we design the “skewered meatball test” (see Figure 8 for a geometric rendering). As we increase the
ratio between the number of points sampled from a 3D isotropic Gaussian and a 1D anisotropic line,
we should see isotropy scores increase from 0 to 1, and hit 0.5 precisely when the number of points
sampled from the Gaussian distribution and the line are equal. Results from the skewered meatball
test in Figure 5 indicate that intrinsic dimensionality and IsoScore are the only two metrics that are
global estimators of the data.
6.2	Isotropy in Contextualized Embeddings
Recent literature suggests that contextualized word embeddings are anisotropic. However, as
demonstrated in Subsection 6.1, no existing metric accurately measures isotropy. We replicate ex-
periments by (Cai et al., 2021), and present isotropy scores for the vector space of embeddings
generated from the WikiText-2 corpus for GPT (110M parameters) and GPT2 (117M parameters) in
Figure 6, as well as the scores for BERT (base, uncased) and DistilBERT (base, uncased) in Figure 7.
Figure 6: The 5 scores for each of the 12 layers of GPT-2 and GPT
8
Under review as a conference paper at ICLR 2022
Figure 7: The 5 scores for the 12 layers of BERT, and the 6 layers of DistilBERT
Our findings using IsoScore challenge and extend upon the literature in the following ways. Con-
textualized embedding models (i) utilize even fewer dimensions than previously thought; (ii) do not
utilize fewer dimensions in deeper layers; and (iii) do not necessarily occupy a “narrow cone” in
space.
IsoScore returns values of less than 0.18 for every considered contextualized embedding model.
GPT and GPT-2 embeddings do not even isotropically utilize a single dimension in space, in the
sense of Heuristic C.1. Using average random cosine similarity, Cai et al. concluded that earlier
layers in contextualized embedding models are more isotropic than layers deeper in the network.
While this may appear to be true using brittle metrics, there is no significant decrease in IsoScore
between the earlier and later layers of contextualized embedding models.
The notion of isotropy is often conflated with geometry. The geometry of isotropic vector spaces,
however, will differ depending on the distribution that generates the points in space. For exam-
ple, multivariate isotropic Gaussians form n-dimensional balls and uniform distributions form n-
dimensional cubes, yet both distributions receive an IsoScore of 1. For an illustrated example of
points generated from different isotropic distributions, consult Appendix E. It is therefore not nec-
essarily the case that even highly anisotropic embedding spaces form narrow, anisotropic cones.
7	Conclusion & Future Works
Several studies have attempted to characterize the spatial organization of point clouds induced by
word embedding models. We demonstrate that existing methods have several undesirable properties
that may jeopardize their validity as reliable metrics of point cloud spatial distributions. This paper
presents a novel method for measuring the uniform utilization of embedding spaces that is robust
to the limitations discussed throughout the above sections. IsoScore is the first metric designed
using the mathematical notion of isotropy. It is the only metric to satisfy: (i) global stability; (ii)
mean, scalar, and rotational invariance; (iii) a correspondence with dimensions utilized, and; (iv)
linear scaling that reflects changes in maximum variance. Finally, we demonstrated that a number
of recent conclusions in the literature that have been derived using brittle metrics may be incomplete
or altogether inaccurate.
There are several promising directions for future work. IsoScore could be used as a regularizer in
word embedding training to reward distributions that exhibit high levels of isotropy. Fine-tuning an
existing embedding model using loss functions based on IsoScore is similarly expected to produce
more isotropic representations. As the uniform geometry of such distributions is assumed to improve
the performance of embedding models, IsoScore presents itself as a useful tool for not only word
embeddings, but also other use cases concerned with point cloud data beyond the domain of NLP.
9
Under review as a conference paper at ICLR 2022
References
Xingyu Cai, Jiaji Huang, Yuchen Bian, and Kenneth Church. Isotropy in the contextual embedding
space: Clusters and manifolds. In International Conference on Learning Representations, 2021.
URL https://openreview.net/forum?id=xYGNO86OWDH.
Paola Campadelli, Elena Casiraghi, Claudio Ceruti, and Alessandro Rozza. Intrinsic dimension
estimation: Relevant techniques and a benchmark framework. Mathematical Problems in Engi-
neering, 2015:1-21, 10 2015. doi: 10.1155/2015/759567.
Andy Coenen, Emily Reif, Ann Yuan, Been Kim, Adam Pearce, F. Viegas, and M. Wattenberg.
Visualizing and measuring the geometry of bert. In NeurIPS, 2019a.
Andy Coenen, Emily Reif, Ann Yuan, Been Kim, Adam Pearce, Fernanda Viegas, and Martin Wat-
tenberg. Visualizing and measuring the geometry of bert, 2019b.
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: pre-training of deep
bidirectional transformers for language understanding. CoRR, abs/1810.04805, 2018. URL
http://arxiv.org/abs/1810.04805.
Kawin Ethayarajh. How contextual are contextualized word representations? comparing the ge-
ometry of bert, elmo, and GPT-2 embeddings. CoRR, abs/1909.00512, 2019. URL http:
//arxiv.org/abs/1909.00512.
Jun Gao, Di He, Xu Tan, Tao Qin, Liwei Wang, and Tie-Yan Liu. Representation degeneration
problem in training natural language generation models, 2019.
Chengyue Gong, Di He, Xu Tan, Tao Qin, Liwei Wang, and Tie-Yan Liu. Frage: Frequency-agnostic
word representation. ArXiv, abs/1809.06858, 2018.
S. Hasan and E. Curry. Word re-embedding via manifold dimensionality retention. In EMNLP,
2017.
John Hewitt and Christopher D. Manning. A structural probe for finding syntax in word representa-
tions. In NAACL-HLT, 2019.
Elizaveta Levina and Peter J. Bickel. Maximum likelihood estimation of intrinsic dimension. In
Proceedings of the 17th International Conference on Neural Information Processing Systems,
NIPS’04, pp. 777-784, Cambridge, MA, USA, 2004. MIT Press.
Yuxin Liang, Rui Cao, Jie Zheng, Jie Ren, and Ling Gao. Learning to remove: Towards isotropic
pre-trained BERT embedding. CoRR, abs/2104.05274, 2021. URL https://arxiv.org/
abs/2104.05274.
Stephen Merity, Caiming Xiong, James Bradbury, and Richard Socher. Pointer sentinel mixture
models, 2016.
Timothee Mickus, Denis Paperno, Mathieu Constant, and Kees van Deemter. What do you mean,
bert? assessing BERT as a distributional semantics model. CoRR, abs/1911.05758, 2019. URL
http://arxiv.org/abs/1911.05758.
Jiaqi Mu, Suma Bhat, and Pramod Viswanath. All-but-the-top: Simple and effective postprocessing
for word representations. CoRR, abs/1702.01417, 2017. URL http://arxiv.org/abs/
1702.01417.
Alec Radford and Karthik Narasimhan. Improving language understanding by generative pre-
training. 2018.
Alec Radford, Jeff Wu, R. Child, David Luan, Dario Amodei, and Ilya Sutskever. Language models
are unsupervised multitask learners. 2019.
Victor Sanh, Lysandre Debut, Julien Chaumond, and Thomas Wolf. Distilbert, a distilled version
of BERT: smaller, faster, cheaper and lighter. CoRR, abs/1910.01108, 2019. URL http://
arxiv.org/abs/1910.01108.
10
Under review as a conference paper at ICLR 2022
Chu Yonghe, Hongfei Lin, Liang Yang, Yufeng Diao, Zhang Shaowu, and Fan Xiaochao. Refining
word representations by manifold learning. pp. 5394-5400, 08 2019. doi: 10.24963∕ijcai.2019/
749.
Tianyuan Zhou, Joao Sedoc, and J. Rodu. Getting in shape: Word embedding subspaces. In IJCAI,
2019.
Wenxuan Zhou, Bill Yuchen Lin, and Xiang Ren. Isobn: Fine-tuning bert with isotropic batch
normalization. In AAAI, 2021.
A Pre-existing metrics, in detail
Average Cosine Similarity: We define the Average Cosine Similarity Score as 1 minus the average
cosine similarity of N randomly sampled pairs of points from X. That is,
AvgCoSSim(X) ：= 1 -X CoS(N,yi) ,	(A.1)
i=1
where {(x1 ,y1),. . . ,(xN ,yN)} ⊆ X × X are randomly chosen with xi 6= yi for all i, and
cos(xi,yi) denotes the cosine similarity of xi and yi. Some authors define the average cosine sim-
ilarity score to be exactly the average, rather than one minus the average. However, for ease of
comparison to other metrics, our score ensures that AvgCosSim(X) is between 0 and 1. Under our
convention, it is commonly believed that a score of 0 indicates that the point cloud X is anisotropic
and a score of 1 indicates that X is isotropic. In Section 6, we demonstrate that this is not the case.
Partition Isotropy Score: For any unit vector c ∈ Rn , let the partition function be de-
noted as Z(c) := Px∈X exp(cT x). Mu et al. (2017) measure isotropy as I(X) :=
(min||c||=1Z(c))/(max||c||=1Z(c)). It is believed that a score closer to zero indicates an anisotropic
space while a score closer to one indicates an isotropic space. Mu et al. (2017) demonstrate that
a score of 1 implies that the eigenspectrum of X is flat. Computing I(X) explicitly is intractable
since the set of unit vectors is infinite. Accordingly, Mu et al. (2017) approximate I(X) by
I(X) ≈ minc∈cZ(C)
maxc∈C Z (c)
(A.2)
where C is the set of eigenvectors of XTX . For the remainder of the paper we refer to (A.2) as the
Partition Score.
Intrinsic Dimensionality: Given a point cloud X ⊆ Rn, it is sometimes useful to assume that
X is sampled from a manifold of dimension less than n. For example, points in the left panel in
Figure 1 are sampled from a 1-dimensional space and points in the middle panel are sampled from a
2-dimensional space. Algorithms for intrinsic dimensionality aim to estimate the true dimension ofa
given manifold from which we assume a point cloud has been sampled. Intrinsic dimensionality has
been used to argue that word embedding models are anisotropic (Cai et al., 2021). For a point cloud
X ⊂ Rn, itis commonly thought that the more isotropic X is, the closer the intrinsic dimensionality
of X is to n. Dividing the intrinsic dimensionality of X by n provides us with a normalized score
of isotropy, which we refer to as the ID Score. We use the maximum likelihood estimation (MLE)
method to calculate intrinsic dimensionality. For a detailed description of the MLE method for
intrinsic dimensionality estimation please consult (Levina & Bickel, 2004; Campadelli et al., 2015).
Variance Explained Ratio: The variance explained ratio measures how much total variance is ex-
plained by the first k principal components of the data. Note that when all principal components
are considered, the variance explained ratio is equal to 1. Examining the eigenspectrum of principal
components is undoubtedly a useful tool in quantifying the spatial distribution of high dimensional
data. However, the variance explained ratio requires us to specify a priori the number of principal
components we wish to examine. We divide the variance explained by the first k principal compo-
nents by k/n to convert the variance explained ratio into a normalized score.
11
Under review as a conference paper at ICLR 2022
B Bounds on IsoScore
Proposition B.1. Let X ⊆ Rn be a finite set. Then ι(X) ∈ [0, 1].
Proof. Define Σ to be the n × n sample covariance matrix of XPCA. Let c > 0 be so that if
We define Σ := C ∙ Σ, then ∣∣∑d∣∣ = √n. Let Us enumerate the entries of this vector as ∑d =
(Var(Xι),..., Var(Xn)). In order to show that ι(X) ∈ [0,1], it is equivalent to show that ∣∑d 一
1∣ ∈ [0, yz2(n - √n)], and by definition of the Euclidian norm, the latter estimate is equivalent to
n
2(n — √n) ≥ ^X(Var(Xi) — 1)2.	(B.1)
i=1
But the identity ∣∣ΣD ∣∣ = √n implies that Pn=ι Var(Xi)2 = n, so in fact (B.1) is equivalent to
n
Var(Xi) ≥ √n.
i=1
If this inequality were flipped, then we could estimate that
n = Var(Xι)2 + …+ Var(Xn)2 ≤ (Var(Xι) + …+ Var(Xn))2 < n,
which is a contradiction.	□
C Interpretation of IsoScore, in Detail
This appendix provides rigorous mathematical justification for the claims that we made in Section
5 about the interpretation of IsoScore. It is split into two parts. In Appendix C.1 we formalize, and
prove, the claim that the IsoScore for a point cloud X is approximately the fraction of dimensions
uniformly utilized by X . And in Appendix C.2 we argue that IsoScore is an honest indicator of
uniform spatial utilization.
C.1 IsoScore Reflects the Fraction of Dimensions Uniformly Utilized
In Section 5 we provided an interpretation for the value of the IsoScore ι(X) in Heuristic 5.1.
Intuitively, our heuristic says that ι(X) is roughly the fraction of dimensions of Rn utilized by X.
We will now explain and justify this heuristic in detail. We formalize our heuristic below.
Heuristic C.1. Suppose that a point cloud X ⊆ Rn gives an IsoScore ι(X). Then X occupies
approximately
k(X) := ι(X) ∙ n +1 — ι(X)	(C.1)
dimensions of Rn .
Note in particular that ι(X) = 0 implies that Equation C.1 simplifies to a single dimension utilized
and ι(X) = 1 implies that Equation C.1 simplifies to all n dimensions utilized.
In the remainder of this subsection, we will justify the above heuristic. We will make reference to the
notations and equations in Section 4. Fix n ≥ 1 and k ∈ {1, . . . , n}, and consider the matrix In(k).
(k)
Recall that In is the covariance matrix for a k-dimensional uncorrelated Gaussian distribution in
Rn. For example, spaces sampled using the matrices I3(k), for k = 1, 2, 3 are rendered in Figure 1 as
a line, a circle, and a ball, respectively. One can compute directly that the IsoScores for these three
spaces are
ι(I3(1)) ≈ 0.0,	ι(I3(2)) ≈ 0.5,	ι(I3(3)) ≈ 1.0.
Our main insight in this section is that it is worthwhile to apply these statistics for reverse reasoning
in the following sense: suppose you have some point cloud X ⊆ R3 which satisfies ι(X) ≈ 1/2.
Then this IsoScore should allow you to infer that X uniformly occupies approximately 2 dimensions
ofR3.
In Heuristic C.1, we provide the closed formula (C.1) for generalizing the above reasoning to all
dimensions n. We will now prove this formula.
12
Under review as a conference paper at ICLR 2022
ProofofHeuristic C.1. Once We normalize Ink) so that its Euclidean norm is √n, We get that the
first k diagonal entries are yJn∕k. Therefore, the isotropy defect is
δ(In(k))
kInk) - 1k _ Jk(1 - PnTk)2 + n - k _ Pn - √nk
,2(n - √n)	√2(n - √n)	Jn - √
(C.2)
It is natural to consider the map k 7→ δ(In(k) ). A priori, this is a discrete function defined on
{1, . . . , n}; a fortiori, this is in fact a continuous, monotonically decreasing bijection on the con-
nected interval [1, n]. Therefore, the function defined by
廉:[1,n] → [0,1] : k→ δ(Ink))
is invertible, and one can compute that its inverse is
~ t	r	-	r	_
δ- : [0, 1] → [1,n] : d →
(n — d2 (n — √n))2
n
The truth of this heuristic rests upon the validity of the folloWing assumption, Which is reasonable
to use in many contexts.
Assumption Underpinning The Heuristic. The isotropy defect corresponding to a point cloud
sampled using the covariance matrix In(k) is the prototypical isotropy defect for any point cloud in
Rn which uniformly utilizes k dimensions.
We Will noW invoke this assumption. Let δ(X) be the isotropy defect for an arbitrary point cloud X.
If We assume that We are in the nontrivial case Where δ(X) > 0, then δn-1 (δ(X)) is in the interval
[1, n). Because δ-1 is bijective, there exists a unique k ∈ {1,...,n - 1} with the property that
J-1(δ(X)) ∈ [k,k + 1). But by construction, [k, k + 1) = B-1(δ(Ink))),E-1(δ(Ink+1)))). By
monotonicity of δ-1, this implies that
δ(X) ∈ [δ(In(k)), δ(In(k+1))).
Therefore, by the assumption underpinning the heuristic, We can deduce that X is uniformly uti-
lizing betWeen k and k + 1 dimensions of Rn . To be specific, We say that X is uniformly
utilizing J-1(δ(X)) ∈ [k, k + 1) dimensions. Recalling Section 4, we can recognize that in
Step 6, the formula for k(X), the quantity of dimensions uniformly utilized by X, is precisely
k(X) := δn-1(δ(X)); likewise, the formula for φ(X), the fraction of dimensions uniformly utilized
by X ,is φ(x) ：= δ-1(δ(X ))/n.
Now we are in a position to verify Equation C.1, the main claim of Heuristic C.1. By the assumption
underpinning the heuristic, it is sufficient to verify Equation C.1 in the case of In(k), for k = 1, . . . , n.
This is because all functions that we will utilize are monotonic bijections. Using the notation in Steps
6 and 7 in Section 4, we can compute that
ι(Ink))(n - 1) + 1= S(φn(Ink)))(n - 1) + 1= n ∙ φn(Ink)) = k(Ink)).
Using the formula k(X) = (n - δ(X)2(n - √n))2∕n, we can continue:
k(I(k)) = (n - δ(Ink))2(n -√n))2 = (n - n-√k(n -√n))2 = k,
n
n
Where in the penultimate equality We used Equation C.2. This completes the proof.
□
Because IsoScore covers a continuous spectrum, one should carefully interpret what we mean when
we say that X occupies approximately k dimensions ofRn. For example, consider the 2D Gaussian
distributions depicted in Figure 2. Heuristic C.1 predicts k = 1.9996, 1.6105, 1.0281 dimensions
are used when x = 1, 3, 75, respectively. These should be interpreted as follows: “when x = 75,
the points sampled are mostly using one direction of space” and “when x = 3, the points sampled
are using somewhere between one and two dimensions of space.”
13
Under review as a conference paper at ICLR 2022
Heuristic C.1 suggests that an IsoScore near 1/2 means that the corresponding point cloud X occu-
pies approximately half of the dimensions of its ambient space. We can make this reasoning rigorous
as follows: for any n ≥ 2, one can compute that
∣(Ink)) = k-1 ≈k
n	n -1	n
for any k = 1, . . . , n.
(C.3)
Proof of (C.3). In Equation C.2 computed that the isotropy defect is δ(In(k))	=
√n - √ni∕√n - √n. If We substitute this expression into Equation 4.1, then We obtain
the formula ∣(Ink)) = n-. Furthermore, one can easily estimate that |n51 - n | ≤ n.	□
Table 2 illustrates this formula in the concrete case of R9 . This formula implies the folloWing key
relationship:
lim ∣(Inbn/2C)) = 1/2.
n→∞
Generalizing this line of reasoning yields our second heuristic explanation for the meaning of
IsoScore.
Heuristic C.2. When the ambient space Rn has large dimension, the IsoScore ι(X) is approximately
the fraction of dimensions uniformly utilized by X.
Proof of Heuristic C.2. By the assumption underpinning Heuristic C.1, it suffices to shoW this in the
case of matrices of the form In(k). Fix α ∈ [0, 1], and consider the covariance matrix In(bαnc). For
large n, the fraction of dimensions uniformly utilized by In(bαnc) is approximately α, according to
Definition 3.1. But by (C.3), We can compute that
lim ∣(Inbanc)) = lim bαnc - 1 = α.
n→∞	n→∞ n - 1
This completes the proof.	□
C.2 THE ISOSCORE FOR In(k) REFLECTS UNIFORM UTILIZATION OF k DIMENSIONS
We Will noW investigate What range of IsoScores are obtained by sample covariance matrices that
utilize k out of n dimensions. It is easy to see that these scores at least fill the interval (0, ι(In(k))],
since the map
[1, ∞) → (0, ι(In(k))] : x 7→ ι(diag(x, 1, . . . , 1, 0, . . . , 0))
is surjective. Conversely, We can shoW that this interval is the only possible range of IsoScores
corresponing to such covariance matrices. We make this claim rigorous in the folloWing proposition.
Proposition C.3. Fix n ≥ 2. For any k= 1, . . . , n, we have that
In(k) = argmax{ι(J) : J utilizes kout of n dimensions}.	(C.4)
This result justifies the use of IsoScore for measuring the extent to Which a point cloud optimally
utilizes all dimensions of the ambient space because it demonstrates that ι(In(k)) is the maximal
IsoScore for any covariance matrix With k non-zero entries and n - k zero entries.
Proof of Proposition C.3. In this section We let Diag+ (n) denote the set of n×n real matrices Which
vanish aWay from the diagonal and Whose diagonal entries are all non-negative. The set Diag+ (n)
parameterizes the set of all n × n sample covariance matrices after performing PCA-reorientation.
We also let Diag+(n, k) ⊆ Diag+ (n) denote that subset Whose first k diagonal entries are non
zero and Whose last n - k diagonal entries are zero. The set Diag+ (n, k) parameterizes the set of
sample covariance matrices post-PCA reorientation Which utilize k out of n dimensions of space.
Covariance matrices in Diag+(n, k) represent point clouds With the property that Var(xi) > 0 for
i = 1, . . . ,k, and Var(xi) = 0 for i = k+ 1, . . . , n.
14
Under review as a conference paper at ICLR 2022
It suffices to show that, for every J ∈ Diag+(n, k), we have that ι(J) ≤ ι(In(k)), or equivalently,
δ(J) ≥ δ(ink)). Write InkD = (pn/k,..., pn/k, 0,..., 0) and JD = (aι,...,ak, 0,..., 0),
where a2 + ak = n. Then We must show that k JD - 1∣∣ ≥ IlInkD - 1k, or equivalently,
k	k
^X3 — 1)2 + n — k ≥ ^X(pn∕k — 1)2 + n — k.
This latter estimate is equivalent to
k
ɪ2 &i ≤ Vznk.
i=1
By Jensen’s inequality, applied with the convex function f(x) = x2, we have that
f
XX f (Oi)
乙 k .
i=1
Simplifying, this implies that (aι +---+ ak)2 ≤ kn. This completes the proof.
□
D Numerical Experiments
In this section, we provide explicit details of how each test is designed. We provide code for all
experiments at: https://github.com/zpckyjg0/IsoScore.git
1.	Test 1: Mean Invariance. To assess whether the five scores are mean invariant, we start
with 100, 000 points sampled from a 10-dimensional multivariate Gaussian distribution
with covariance matrix equal to the identity and a common mean vector M = [μ,μ,...,μ].
We compute scores for μ = 0,1,2,..., 20.
2.	Test 2: Scalar Invariance. We test for the property of scalar invariance by sam-
pling 100, 000 points from a 5D Gaussian distribution with common mean vector M =
[3,3,3, 3, 3] and covariance matrix equal to λ ∙ I5. We then compute scores for each point
cloud as we increase λ from 1 to 25.
3.	Test 3: Maximum Variance. We start by sampling 100, 000 points from a 10D multivari-
ate Gaussian distribution with zero common mean vector and a diagonal covariance matrix
with nine entries equal to 1 and one diagonal entry equal to x. In our experimental setup,
we compute all five scores as we increase x from 1 to 75.
4.	Test 4: Rotation Invariance. Our baseline point cloud X ⊂ Rn consists of 100, 000
points sampled from a 2D zero-mean Gaussian distribution with a covariance matrix equal
to (018 0.8). We rotate X by 120° and 240°. Lastly, we project X using PCA reorientation
while retaining dimensionality to obtain a point cloud XPCA.
5.	Test 5: Dimensions Used (Fraction of Dimensions Used Test). For our first experiment,
which we term the “fraction of dimensions used test,” we sample 100, 000 points from a
25D multivariate Gaussian distribution with a zero common mean vector and a diagonal
covariance matrix where the first k entries are 1 and the remaining n — k diagonal elements
are 0. We refer to k as the number of dimensions uniformly used by our data (see Defi-
nition 3.1). For our experiment we let k = 1, 2, 3, ..., 25, and compute the corresponding
scores.
6.	Test 5: Dimensions Used (High Dimensional Test). A good score of spatial utiliza-
tion should allow for easy comparison between different vector spaces even when the di-
mensionality of the two spaces is different. We sample 100, 000 points from a zero-mean
Gaussian distribution with identity covariance matrix In and increase the dimension of the
distribution from n = 2, . . . , 100.
7.	Test 6: Global Stability. We generate a “skewered meatball” by sampling 1, 000 points
from a line in 3D space and increase the number of points sampled from a 3-Dimensional,
zero-mean, isotropic Gaussian from 0 to 150, 000. This is illustrated in Figure 8.
15
Under review as a conference paper at ICLR 2022
Figure 8: 2D rendering of a line in 3D space intersecting noisy sphere.
Figure 9: Points sampled from a Uniform distribution, Poisson distribution, Student-T distribution
and ChiSquare distribution respectively
E	Geometry of Isotropy
Each of the distributions illustrated in Figure 9 has a covariance matrix proportional to the identity
and is therefore maximally isotropic. Namely, the variance is distributed equally in all directions.
Despite receiving an IsoScore of 1, the geometry of the point clouds are vastly different. We can only
comment on the geometry of the point cloud if the underlying distribution of the space is known.
16