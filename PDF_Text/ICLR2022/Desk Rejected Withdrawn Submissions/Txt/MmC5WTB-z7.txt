Under review as a conference paper at ICLR 2022
A Hypothesis for the Cognitive Difficulty of
Images
Anonymous authors
Paper under double-blind review
Ab stract
This paper proposes a hypothesis to analyze the underlying reason for the cognitive
difficulty of an image from two perspectives, i.e. a cognitive image usually makes
a DNN strongly activated by cognitive concepts; discarding massive non-cognitive
concepts may also help the DNN focus on cognitive concepts. Based on this
hypothesis, we use multi-variate interactions to represent cognitive concepts and
non-cognitive concepts contained in an image, and further design a set of image
revision operations to decrease the cognitive difficulty of the image. In experiments,
we found that the revised image was usually more cognitive than the original one.
Besides, we also discovered that strengthening cognitive concepts and weakening
non-cognitive concepts could improve the aesthetic level of an image.
1	Introduction
Nowadays, there are plenty of studies devoted to evaluating and improving the image quality, such as
image super-resolution (Yang et al., 2010; Dong et al., 2015) and image aesthetic assessment (Jin
et al., 2019b; Deng et al., 2018). In contrast, this paper focuses on the cognitive difficulty of an image,
i.e. whether this image is easy for people to recognize.
Unfortunately, there does not exist a proper method to define and model the cognitive difficulty of an
image, because the cognitive difficulty of the image has many aspects. For example, an object with
bright colors, clear contours, and simple contents is usually believed to be more cognitive (Farah,
1992). Although such cognitive information can be learned by a model in a supervised manner, this
paper focuses on the mechanism for the cognitive difficulty. I.e., we do not directly teach the model
which object is cognitive, but we expect this model automatically to push the grass greener and
flowers redder in Fig. 4.
To this end, there remain two essential challenges to model the cognitive difficulty of an image. (1)
The cognitive process is subjective, and there is no proper method to directly model the information-
processing mechanism of the human brain. (2) There lacks the theoretical foundation to explain
the mechanism of distinguishing cognitive visual concepts and non-cognitive visual concepts in the
human brain. Here, the visual concept is referred to as features for certain shapes or textures, instead
of representing semantics. Note that the cognitive concept has the dramatic difference from the visual
saliency, which will be discussed later.
Fortunately, the fast development of deep learning provides us new ways to deal with the aforemen-
tioned two challenges. First, we try to use an artificial deep neural network (DNN) learned for object
classification on a huge dataset (e.g. the ImageNet dataset (Krizhevsky et al., 2012)) as a substitute to
mimic the human brain.
Second, we discover that the interaction defined in game theory can be used to explain cognitive
concepts and non-cognitive concepts. The basic idea is that both the human brain and the DNN do
not use a single pixel/patch for inference, but they use groups of highly interacted pixels/patches to
form interaction patterns, instead. Thus, these interaction patterns can be considered to represent
visual concepts.
Then, let us introduce why and how to use the game-theoretic interaction between multiple patches to
mimic human cognition. Given a pre-trained DNN f and an image with n patches N = {1, 2, . . . , n},
we use the Harsanyi dividend I(S) (Harsanyi, 1963) to measure the multi-variate interaction between
1
Under review as a conference paper at ICLR 2022
(a) FeatUre = I(SI) + I(S2) + I (S3) +—
Cognitive: the Non-Cognitive:
entire cloud. detailed textures.
(b)
(C) Changing the
color of the flower
to yellow to boost
SaIienCy
Boosting
cognition
⅛
Figure 1: (a) Sketch for the multi-variate interaction. The feature representation of a DNN can be
decomposed into interaction utilities of different visual concepts. (b) The cloud, as an entire object,
is regarded as a cognitive concept, but the detailed textures inside the cloud are non-cognitive and
forgettable. (c) Difference between boosting saliency and boosting cognition. Boosting saliency
changes the color of the flower to attract more attention, while boosting cognition makes flowers
redder to be recognized more easily. Please see Appendix A for the detailed difference between
saliency and cognitive concepts.
a set of input patches S ⊆ N . The Harsanyi dividend is a well-known and standard metric to estimate
the marginal benefit when patches in S collaborate with each other w.r.t. the case when they work
independently. For example, let us focus on Fig. 1 (a), S1 = {mouth, eye, ear} ⊆ N . The mouth, ear,
and eye patches in S1 collaborate with each other to trigger a feature, which represents the cat head
concept for inference. This triggered feature is referred to as the interaction utility I(S1) ∈ Rd (here
we extend the Harsanyi dividend from a scalar to a vector). The absence of any patch will deactivate
the feature of the head concept. More crucially, the Harsanyi dividend ensures that the feature can be
decomposed into utilities of different concepts, feature = S⊆N I(S). For example, in Fig. 1 (a),
the feature of the image can be represented as the sum of the interaction utility of the head concept
I(S1), the utility of the paw concept I(S2), the utility of the wood concept I(S3), etc.
Furthermore, the Harsanyi dividend enables us to distinguish cognitive concepts and non-cognitive
concepts. There are 2n concepts theoretically, but only very sparse concepts have significant effects
kI (S)k2 on the feature, namely cognitive concepts. Whereas, a large ratio of concepts have negligible
influences kI(S)k2 on the feature, namely non-cognitive concepts. Because the DNN is trained using
massive samples, we consider such cognitive concepts and non-cognitive concepts can mimic human
cognition, to some extent.
Therefore, we propose a hypothesis for the cognitive difficulty of an image that a cognitive image
usually contains many cognitive concepts which are strongly activated by the human brain;
discarding massive non-cognitive concepts may also help the human focus on cognitive concepts,
i.e. triggering clean and strong signals of cognitive concepts to the brain and letting people ignore
non-cognitive concepts. Based on the hypothesis, we can quantify and decrease the cognitive difficulty
of an image by simply changing the hue, saturation, brightness, and the sharpness of different regions,
without changing the layout or the content of the image. Besides, in experiments, we discover that
enhancing cognitive concepts and discarding non-cognitive concepts of an image may improve its
aesthetic level. It may be because such operation can reduce cognitive costs of the human.
The essential difference between cognitive concepts and salient pixels is as follows. Saliency is
usually defined at the pixel level. Specifically, saliency methods (Mechrez et al., 2019) simply classify
all pixels into salient pixels and non-salient pixels as a black and white problem or assess the saliency
numerically from 0 to 1. In short, the goal of saliency methods is to attract the human attention
based on the contrast or brightness of each pixel, without concerning the cognition of each pixel.
In comparison, the cognitive difficulty mainly depends on the conceptual representation. In other
words, a pixel/patch is cognitive, if and only if it collaborates with other pixels/patches to form a
cognitive concept. For example, although the detailed textures within the cloud in Fig. 1 (b) are
non-cognitive and forgettable, the cloud, as an entire object, is cognitive. Moreover, Fig. 1 (c) shows
that boosting saliency changes the color of the flower to make it more distinctive and attract more
attention. Whereas, boosting cognition makes flowers redder to be recognized more easily. Please see
Appendix A for discussions about the difference between attribution methods and cognitive concepts.
Contributions of this paper are summarized as follows. (1) We propose a hypothesis for the cognitive
difficulty of an image. (2) We use the multi-variate interaction to mimic the concept of human
cognition. (3) We propose a set of operations to revise an image to decrease its cognitive difficulty.
2
Under review as a conference paper at ICLR 2022
2	Related work
Cognitive science mainly focuses on how the human brain represents, processes and transforms
information (Thagard, 2008). Some studies used Bayesian models to mimic the mechanism of human
cognition (Xu & Tenenbaum, 2007; L Griffiths et al., 2008; Tenenbaum, 1999), and some researches
evaluated human cognition through behavioral studies (Laina et al., 2020; Jia et al., 2013). However,
previous studies did not propose a direct hypothesis to explain the mechanism of cognitive concepts.
In contrast, this paper uses the game-theoretic interaction to model cognitive and non-cognitive visual
concepts.
Cognitive difficulty of an image is an important issue, but it does not receive much attention. Existing
studies related to this topic mainly focused on the memorability of an image, i.e. whether an image
was memorable or forgettable (Baveye et al., 2016; Fajtl et al., 2018; Khosla et al., 2015; Goetschalckx
et al., 2019; Sidorov, 2019). However, these studies were mainly conducted by training a model to
fit the manually annotated memorability in a supervised manner, without proposing a hypothesis
for memorability. Meanwhile, these studies also did not provide theoretical principles to ensure
that the model could successfully reflect the memorability of humans. In comparison, this paper
proposes a hypothesis for the cognitive difficulty of the image, which explores the underlying reason
for cognitive concepts and non-cognitive concepts without the help of human annotations.
Image aesthetics can be roughly classified into image aesthetic assessment (Talebi & Milanfar, 2018;
Sheng et al., 2018; Hosu et al., 2019; Jin et al., 2019b) and image aesthetic manipulation (Deng et al.,
2018; Sheng et al., 2018; Wang et al., 2019; Moran et al., 2020). However, previous studies were
mainly conducted in an end-to-end manner without modeling the image aesthetics. In this paper,
we extend the hypothesis for the cognitive difficulty to partially explain the image aesthetics, i.e.
discarding non-cognitive concepts and strengthening cognitive concepts can improve the aesthetic
level of an image, which is successfully verified by experimental results.
3	Hypothesis for the cognitive difficulty of an image
In this paper, we propose a hypothesis for the cognitive difficulty of an image that a cognitive image
usually makes a DNN strongly activated by cognitive concepts; discarding massive non-cognitive
concepts may also help the DNN focus on cognitive concepts.
In other words, if a DNN is triggered by massive visual concepts of an image, but none of them are
strong enough to dominate the cognition (inference), then we may consider this image is difficult to
recognize. To verify this hypothesis, we use the multi-variate interaction as a new tool to decompose
visual concepts from features encoded in the neural network, and further distinguish cognitive
concepts and non-cognitive concepts. Based on the hypothesis, we revise the image to reduce its
cognitive difficulty, and experimental results verify the hypothesis.
3.1	Multi-variate interaction
Definition of the multi-variate interaction. We use the multi-variate interaction between multiple
pixels/patches to represent cognitive concepts and non-cognitive concepts of an image in game theory.
Given a pre-trained DNN f and an image consisting of npatches, N = {1,2,…，n}, f(N) ∈ Rd
denotes the feature of an intermediate layer. The DNN usually does not use a single patch for feature
representation, but it uses groups of highly interacted patches to form interaction patterns. In this
way, we quantify the interaction between multiple patches as the additional utility to the feature
representation when these patches collaborate with each other, in comparison with the case of they
working individually.
To this end, we regard a visual concept encoded in the DNN as a salient interaction pattern. Let us
consider a set of patches {ear, eye} in Fig. 1 (a) for the representation of a cat as a toy example, in
order to understand the multi-variate interaction. If we add the mouth patch into this set and obtain
the concept S1 = {ear, eye, mouth}, then the collaboration between the eye, the nose, and the mouth
will add a complementary interaction utility I(S1) to the feature of the cat. The absence of any patch
in the concept S1 will destroy this concept, thereby removing the interaction utility I(S1).
3
Under review as a conference paper at ICLR 2022
(minθ Loss1 (θ) - Loss2 (θ)). Note that cognitive concepts are not just foreground objects, but are
determined by their interaction utilities for the feature representation. Hence, we decrease the cogni-
tive difficulty of the image by pushing collaborations inside concepts towards more cognitive ones
(e.g. making the grass greener), rather than just changing the contrast of some pixels like saliency
methods. Here, we revised images from the ImageNet dataset based on the VGG-16 model. Please
see Appendix G for more results.
Specifically, we use the Harsanyi dividend (Harsanyi, 1963), a well-known and standard metric, to
measure the interaction utility I(S) ∈ Rd. We discover that the feature f(N) can be decomposed
into interaction utilities of different visual concepts S, when we extend the Harsanyi dividend from a
scalar to a vector. Please see Appendix B for the proof of this decomposition.
f(N)= Xs⊆N I(S)，St I(S) =f XL⊆S(-1)lSl-lLlf (L).	⑴
f(S) denotes the feature of an intermediate layer in the DNN when only patches in the concept S
are given. Particularly, I(0) = f (0), and | ∙ | denotes the cardinality of the set. In the computation
of f(S), we mask patches in N \ S to baseline values and keep patches in S unchanged, so as to
approximate the removal of patches in N \ S. Specifically, we follow settings in (Dabkowski &
Gal, 2017) to set the baseline value as the mean value of all patches over all images. Please see
(Dabkowski & Gal, 2017) or Appendix C for details.
In this way, the Harsanyi dividend measures the marginal utility of the concept S, when interaction util-
ities of all smaller subsets of patches in S are removed, i.e. {I (L)|L ( S}. For a better understanding,
let us consider the computation of the Harsanyi dividend w.r.t the cat head in Fig. 1 (a). The interaction
utility of this cat head concept S1 is defined as I(S1 = {ear, eye, mouth}) = f ({ear, eye, mouth}) -
I(0) - I ({eye}) - I ({mouth}) - I ({ear}) - I ({eye, mouth}) - I ({eye, ear}) - I ({mouth, ear}).
Understanding cognitive concepts and non-cognitive concepts encoded in a DNN. There are
totally 2n concepts in Eq. (1), but we discover that only a few concepts make significant effects on
the feature f(N), i.e. generating large interaction utilities kI(S)k2. These concepts are regarded
as cognitive concepts, because the DNN mainly uses these concepts to represent the feature. In
comparison, most other concepts composed of random patches make negligible impacts on the
feature, i.e. kI (S)k2 is small. We consider such concepts as non-cognitive concepts, because they
are usually ignored by the DNN. In this way, the overall feature f(N) can be rewritten as the sum of
interaction utilities of massive non-cognitive concepts Ωnon-cognitive and interaction utilities of sparse
Cognitive ConCePtS Ocognitive. Here, ωcognitive ∪ ωnon-cognitive
{S|S ⊆ N}.
f(N) =	S⊆N
I(S) =
SeQcognitive
I(S ) + Es∈Ω …iJ(S).
(2)
Note that non-cognitive concepts are NOT equivalent to random noises on images. Instead, these
non-cognitive concepts are regarded as sets of patches, interactions inside which are not useful for the
feature representation. For example, the detailed texture of the water in the top right of Fig. 2 may
not provide significant features for the DNN, thereby being considered as a non-cognitive concept. In
comparison, cognitive concepts represent sets of patches, whose interactions are used for the feature
representation, NOT just representing foreground objects in classification. For example, the dog on
the foreground and the grass on the background are both cognitive concepts in Fig. 2.
4
Under review as a conference paper at ICLR 2022
3.2	Approximate approach to revising visual concepts
Based on the hypothesis and above analysis, we can reduce the cognitive difficulty of images by
strengthening cognitive concepts with large kI(S)k2 values. Moreover, we can also discard non-
cognitive concepts with small kI(S)k2 values to make the DNN ignore non-cognitive concepts
and move attention to cognitive concepts. However, according to Eq. (2), the enumeration of all
cognitive concepts and non-cognitive concepts is NP-hard. Fortunately, we find a close relationship
between interaction utilities of visual concepts and Shapley values (Shapley, 1953). In this way,
we can use the intermediate term ∆f (i, L) (defined in Eq. (3)) in the Shapley value to design two
loss functions, which force the DNN to strengthen cognitive concepts and to discard non-cognitive
concepts, respectively.
Definition of Shapley values and ∆f (i, L). Before the optimization of visual concepts, we first
introduce Shapley values w.r.t. the feature representation of an intermediate layer in the DNN. The
Shapley value is widely considered as an unbiased estimation of the numerical utility w.r.t. each
patch in game theory (Weber, 1988). Without loss of generality, we extend the scalar Shapley value
to a vectorized one. It is because this paper focuses on the utility of each patch to represent a certain
feature, rather than the utility to a scalar classification result. Specifically, given a trained DNN f and
an image X with n patches N = {1, 2, ∙∙∙ ,n},let f (x) ∈ Rd denote the feature of an intermediate
layer. The Shapley value is proposed to fairly divide and assign the overall effects on the feature
representation to each patch, as follows.
Φ(i∣N) = Xl⊆n∖{i} (n TLn-I)!|L1! [∆f(i,L)i∙	⑶
Here, φ(i∣N) ∈ Rd represents the utility of the patch i to the feature representation. ∆f (i, L)=
f(L ∪ {i}) - f(L) ∈ Rd measures the marginal utility of the patch i to the feature representation,
given a set of contextual patches L. Please see Appendix D for details.
Using ∆f (i, L) to strengthen/discard visual concepts. In order to overcome the aforementioned
problem with the computational cost, we have proven that the term ∆f (i, L) in the Shapley value
can be re-written as the sum of interaction utilities of some visual concepts (proved in Appendix E).
∆f(i, L) = XL0⊆LI(S0 = L0 ∪ {i}).	(4)
According to empirical observations, ∆f (i, L) is sparse among all contexts L. In other words, only a
small ratio of contexts L have significant impacts k∆f (i, L)k2 on the feature. In this way, we can
roughly consider cognitive concepts are usually contained by the term ∆f (i, L) with a large L2 norm,
while the term ∆f (i, L) with a small L2 norm only contains non-cognitive concepts.
Therefore, we define the following two loss functions. We use Loss1 to weaken non-cognitive
concepts included by ∆f (i, L) with a small L2 norm, which forces the DNN to ignore non-cognitive
concepts and move attention to cognitive concepts. Moreover, we propose Loss2 to strengthen
cognitive concepts contained in ∆f (i, L) with a large L2 norm.
LoSSI= Er "i1,L1)∈Ω…∣Lι∣=r[k∆f(i1,Ll)k2] ,	(5)
LoSS2 = -Er ]E(i2,L2)∈Ωaant,∣L2∣=r [k∆f (i2, L2)k2i .	(6)
Here, the set Ωtriviaj ⊆ {(i,L)∣L ⊆ N,i ∈ N \ L} contains mi pairs of (i, L), which generate the mi
smallest ∣∣∆f (i,L)∣∣2 values. In comparison, the set Ωsig∏ificant ⊆ {(i,L)∣L ⊆ N,i ∈ N \ L} includes
m2 pairs of (i, L), which generate the m2 largest k∆f (i, L)k2 values. In this way, we consider the
set Ωtrivial is supposed to include non-cognitive concepts, and the set Ωsig∏ificant is supposed to include
cognitive concepts. Considering the computation cost of Lossi and Loss2 is huge, we optimize these
loss functions by sampling pairs of (i, L) in implementation.
Note that in Eq. (4), (5), and (6), we only enumerate contexts L made up of 1 ≤ r ≤ 0.5n patches.
There are two reasons. First, when r is larger, ∆f (i, L) contains more contexts L, and thus it is more
difficult to maintain ∆f (i, L) sparse among all coalitions {(i, L)}. This hurts the sparsity assumption
for Eq. (4). Second, collaborations of massive patches (a large r) usually represent interactions
between mid-level patterns, instead of representing patch-wise collaborations (Cheng et al., 2021),
which boost the difficulty of the image revision. Please see Appendix F for discussions.
5
Under review as a conference paper at ICLR 2022
3.3 Operations to revise images
⑶
! °#(C)
Not allowed to pass
through the contour
A single cone-
shaped template
(b)
（儿（）
(C)
Figure 3: Cone-shaped templates for image revi-
sion.
In the above section, we propose two loss func-
tions to decrease the cognitive difficulty of an
image from two perspectives. In this section, we
further introduce a set of operations to revise
images to decrease the loss functions. A key
principle for image revision is that the revi-
sion should not bring in additional concepts,
but just revises existing concepts in images.
Specifically, this principle can be implemented
as the following three requirements.
1.	The image revision is supposed to sim-
ply change the hue, saturation, brightness, and
sharpness of different regions in the image.
2.	The revision should ensure the spatial smooth-
ness of images. In other words, each regional
revision of the hue, saturation, brightness, and sharpness is supposed to be conducted smoothly over
different regions, because dramatic changes of the hue, saturation, brightness, and sharpness may
bring in new edges between two regions.
3.	Besides, the revision should also preserve the existing edges. I.e. the image revision is supposed to
avoid the color of one object passing through the contour and affecting its neighboring objects.
In this way, we propose four operations to adjust the hue, saturation, brightness, and sharpness
of images smoothly, without damaging existing edges or bringing in new edges. Thus, the image
revision will not generate new concepts or introduce out-of-distribution features.
Operation to adjust the hue. Without loss of generality, we first consider the operation to revise the
hue of the image x in a small region (with a center c). As Fig. 3 (c) shows, the revision of all pixels
in the region with the center c is given as
Z(c,hue) = k(c,hue) ∙ (G ◦ M(C)),	(7)
where ◦ denotes the element-wise production. k(c,hue) ∈ R indicates the significance of the hue
revision in this region, which is learned via loss functions. G ∈ Rd0×d0 is a cone-shaped template1,
which is used to smoothly decrease the significance of the revision from the center to the border,
according to Requirement 2. Specifically, each element in the template Ghw = max(1 - λ ∙ ∣∣phw -
ck2, 0) represents the significance of the revision for each pixel in the receptive field. Here, kphw-ck2
indicates the Euclidean distance between the pixel (h, w) within the region and the center c. λ is a
positive scalar to control the range of the receptive field. We set λ = 1 in experiments.
In order to protect the existing edges (Requirement 3), we also design a binary mask M(c) ∈
{0, 1}d0 ×d0, which masks image regions that are not supposed to be revised. Specifically, all pixels
blocked by the edge (e.g. the object contour) are not allowed to be revised. For example, in Fig. 3
(c), the effects of the sky are not supposed to pass through the contour of the bird and revise the bird
head, when the center c is located in the sky.
Then, as Fig. 3 (b) shows, for each pixel (h, w), the overall effect of its hue revision ∆(hhwue) is a
mixture of effects from surrounding pixels Nhw , as follows.
Xhww,hUe) = β(hue) ∙ ∆hWe) + Xhwe),	∆hhwe) = tanh(X	ZhwhUe)),	(8)
c∈Nhw
where 0 ≤ x(hhwue) ≤ 1 denotes the original hue value of the pixel (h, w) in the image x. tanh(∙) is
used to control the value range of the image revision, and the maximum magnitude of hue changes is
limited by β(hue). Note that the revised hue value x(hnwew, hue) may exceed the range [0, 1]. Considering
the hue value has a loop effect, the value out of range can be directly modified to [0, 1] without being
truncated. For example, the hue value of 1.2 is modified to 0.2.
Operations to adjust the saturation and brightness. Without loss of generality, let us take the
saturation revision for example. Considering the value range for saturation is [0, 1] without a loop
1We use a cone-shaped template, instead of a Gaussian template, in order to speed up the computation.
6
Under review as a conference paper at ICLR 2022
父
%ori	%co；
一
%COH

Image revision based on VGG-16
Image revision based on ResNet-18
Mg φqopv JIW
Figure 4: Comparisons between original images xori, images xc+og revised by strengthening cognitive
concepts (minθ Loss2 (θ)), images xc-og revised by discarding cognitive concepts (maxθ Loss2 (θ)),
and images X revised by strengthening cognitive concepts and weakening non-cognitive concepts
(minθ Loss1(θ) + Loss2(θ)). We revised images from the ImageNet dataset and the MIT-Adobe 5K
dataset based on the VGG-16 model and the ResNet-18 model, respectively. Please see Appendix G
for more results.
effect, we use the sigmoid function to control the revised saturation value x(hnwew, sat) within the range.
XhrSat)= sigmoid(β(Sat) ∙ △黑 + sigmoidT(X黑))，△黑=tanh(Xc∈N∕hWSat))，⑼
where Z(c,sat) = k(C,Sat) ∙ (G ◦ M(C) ),just like Eq. (7). Besides, the operation to adjust the brightness is
similar to the operation to adjust the saturation.
Operation to sharpen or blur images. We propose another operation to sharpen or blur the image
in the RGB space. Just like the hue revision, the sharpening/blurring operation to revise the image
can be represented as follows.
(new, blur/sharp) (blur/sharp)	(blur/sharp)	(c,blur/sharp)
xhw	— δHw	∙ Axhw + Xhw，	δHw	— tanh( 乙 c∈N Zhw	)， (10)
7
Under review as a conference paper at ICLR 2022
ImageNet dataset	^^^^^^lMIT-Adobe 5k dataset
xori by strengthening cognitive concept was likely to be most cognitive. The image xc-og revised by
discarding cognitive concept tended to be least cognitive.
where ∆xhw = x(hbwlur) - xhw indicates the pixel-wise change towards blurring the image, and x(blur) is
obtained by blurring the image using the Gaussian blur operation. Accordingly, -∆xhw describes the
pixel-wise change towards sharpening the image. △£?SharP) ∈ [-1,1]. If △?；"SharP) ＞。, then this pixel
is blurred; otherwise, being sharpened. Z(C,blur/SharP) = k(c，blur/sharp) ∙ (G ◦ M (C)),just like Eq.(7).
In this way, we revise the hue, saturation, brightness, and sharpness of images to strengthen cognitive
concepts by minimizing Loss2 (θ) in Eq. (6), and to discard non-cognitive concepts by minimizing
Loss1(θ) in Eq. (5). Here, θ = {k(C,hue), k(C,sat), k(C,bright), k(C,blur/sharp)|c ∈ Ncenter}, and Ncenter denotes a
set of central pixels w.r.t. different regions in the image x. Note that our method can also be applied
to other human-defined operations for image revision (e.g. changing the image style), besides the
revision of the hue, saturation, brightness, and sharpness of images.
4	Experiments
•	Visualization of image revision. We conducted experiments to test the cognitive difficulty,
considering the following two settings: (1) strengthening cognitive concepts and (2) discarding non-
cognitive concepts. Specifically, we computed loss functions based on the VGG-16 model (Simonyan
& Zisserman, 2015) and the ResNet-18 model (Kaiming He & Sun, 2016) to revise images from the
ImageNet dataset (Krizhevsky et al., 2012) and the MIT-Adobe 5K dataset (Bychkovsky et al., 2011),
respectively. These DNNs were trained on the ImageNet dataset for object classification. For the
ResNet-18 model, we used the feature after the first block as f. For the VGG-16 model, we used
the feature of the conv3_3 layer as f. In order to decrease the cognitive difficulty of the image, we
set m1 = 0.1M, and m2 = 0.6M, where M = |{(i, L)|L ⊆ N, i ∈ N \ L}|. We divided an image
into 28 × 28 patches, and set β(hue) = 0.35, β(sat) = 3, β(bright) = 1.5. We utilized the off-the-shelf edge
detection method (Xie & Tu, 2015) to extract the object contour and to calculate the binary mask
M(p). Besides, we used the Gaussian filter with the kernel size 5 to blur images.
Let us focus on columns of xori, xc+og, and xc-og in Fig. 4. We found that the image xc+og revised by
strengthening cognitive concepts was more cognitive, compared with both the original image xori and
the image xc-og revised by weakening cognitive concepts. Note that we allowed the color in the image
to be significantly revised, instead of maintaining the original color, because the purpose of the image
revision was to decrease the cognitive difficulty, e.g. making the tree greener in Fig. 4.
•	Human measure for cognitive difficulty. In order to examine whether strengthening cognitive
concepts could decrease the cognitive difficulty of an image, we conducted an experiment with 384
human participants. In this experiment, we showed each participant a group of images, including
one original image xori , the image xc+og revised from xori by strengthening cognitive concepts, and the
image xc-og revised from xori by weakening cognitive concepts. Then, we asked each participant to
sort the cognitive difficulty of these three images. Please see Appendix H for an example.
To this end, we calculated the ratio that the revised image xc+og was correctly recognized as the most
cognitive one in each group. We also computed the ratio that the revised image xc-og was correctly
considered as the least cognitive one in each group. Fig. 5 reports above ratios, which shows that the
image revised by strengthening cognitive concepts tended to be most cognitive, and the image revised
by weakening cognitive concepts tended to be least cognitive. Thus, our hypothesis was verified.
•	Using the existing classifier to evaluate the memorability (which was related to the cognitive
difficulty). Besides the human measure, we also used an existing DNN, which was trained to evaluate
the memorability of images in a supervised manner, to test the effectiveness of our method. Fajtl et al.
8
Under review as a conference paper at ICLR 2022
Table 1: Using the existing classifier to evaluate the memorability (which was related to the cognitive
difficulty). The image xc+og revised from the original image xori by strengthening cognitive concept
tended to be most memorable.
Dataset ∣	ImageNet			I	MIT-Adobe 5K			
Model I	VGG-16		I	ResNet-18	I	VGG-16			I	ResNet-18
Images ∣	Xcog	I XOIi I	Xcog I Xcog I Xori I Xcog	I	X+og	I Xori I	Xcog	I Xcog I Xori I Xcog
Memorability ∣	0.759	I 0.734 I	0.710 I 0.751 I 0.734 ∣ 0.720	I 0.694	I 0.661 I	0.659	I 0.679 I 0.661 I 0.658
Table 2: Using existing classifiers to evaluate the aesthetic level of images. The image X revised
by strengthening cognitive concepts and discarding non-cognitive concepts tended to be the most
aesthetic one.
Dataset I	ImageNet					I	MIT-Adobe 5K						
Model I	VGG-16		I	ResNet-18			I	VGG-16			I	ResNet-18			
Images I	X	I	X+og	I	Xori	I X I	X+og I	Xori	I X	I	X+og	I	Xori	I X I	X+og I	Xori
-Tnima-T YaeS	I	4.803	I 4.740 I	4.653	I 4.670 I	4.621 I	4.653	I 4.705	I 4.661 I	4.684	I 4.732 I	4.592 I	4.684
ILGNet I YaeS	I	93.4%	I 83.5% I	72.5%	I 81.3% I	69.2% I	72.5%	I 95.0%	I 86.0% I	54.0%	I 81.7% I	55.0% I	54.0%
(2018) proposed the AMNet model to measure the memorability of the image, whose value range
was [0, 1], and a large value indicated the image was more memorable. Here, we used 120 images
from the ImageNet dataset and 60 images from the MIT-Adobe 5K dataset for evaluation. Table 1
shows that the image xc+og revised by strengthening cognitive concepts was most memorable.
• Using the cognitive difficulty to explain the image aesthetics. Beyond the cognitive difficulty,
we found that strengthening cognitive concepts and discarding non-cognitive concepts could improve
the aesthetic level of an image in experiments. It is because such operation could reduce the cognitive
cost of humans and remove noises, which made the image more aesthetic, to some extent.
To examine the correctness of this finding, we used existing classifiers to evaluate the aesthetic level
of images. These existing classifiers had been trained as off-the-shelf models to predict aesthetic
qualities of images in a supervised manner. Specifically, we used the NIMA model (Talebi &
Milanfar, 2018), and the ILGNet (Jin et al., 2019a) for evaluation, whose outputs were denoted by
γaNeIsMA and γaILesGNet, respectively. A large value of γaNeIsMA or γaILesGNet indicated the image tended to
be more aesthetic. Table 2 shows that an image X revised by strengthening cognitive concepts and
discarding non-cognitive concepts tended to be more aesthetic than both the original image and the
image revised by only strengthening cognitive concepts.
Besides, Fig. 4 also shows that the image revised X by strengthening cognitive concepts and weakening
non-cognitive concepts tended to be more aesthetic than Xori, in terms of the vivid color, object
emphasis, and light (Mavridaki & Mezaris, 2015). Moreover, compared to Xc+og, the combination of
discarding non-cognitive concepts and strengthening cognitive concepts made the revised image more
aesthetic. It may be because simply strengthening cognitive concepts just made the attention uniformly
separated, while discarding non-cognitive concepts would reduce noises and avoid attracting attention.
For example, on the top left of Fig. 4, leaves in X were blurred and turned grey to emphasize the bird
and the flower, which satisfied the rule object emphasis in the photography. In comparison, those
leaves in Xc+og were all sharpened and turned greener to make the attention more uniformly separated.
5 Conclusion
In this paper, we propose a hypothesis for the cognitive difficulty of an image from two perspectives,
i.e. a cognitive image usually makes a DNN strongly activated by cognitive concepts; discarding
massive non-cognitive concepts may also help the DNN focus on cognitive concepts. To verify
this hypothesis, we use multi-variate interactions to represent cognitive concepts and non-cognitive
concepts encoded in the DNN. Furthermore, we define two loss functions to revise the hue, saturation,
brightness, and sharpness of images, where one loss forces the DNN to strengthen cognitive concepts,
and the other forces the DNN to discard non-cognitive concepts. We find that the revised images are
more cognitive than the original ones. We also discover that strengthening cognitive concepts and
discarding non-cognitive concepts can improve the aesthetic level of an image.
9
Under review as a conference paper at ICLR 2022
Reproducibility Statement. In terms of the algorithm of this paper, Appendix B and Appendix E
provide complete proofs and discuss all assumptions for theoretical results in this paper. In terms
of experiments, the first paragraph of Section 4, Appendix G, and Appendix H provide complete
descriptions of all experimental details, which ensure the reproducibility. Nevertheless, we will
release the code when the paper is accepted.
References
Yoann Baveye, Romain Cohendet, Matthieu Perreira Da Silva, and Patrick Le Callet. Deep learning for
image memorability prediction: The emotional bias. In Proceedings of the 24th ACM international
conference on Multimedia, pp. 491-495, 2016.
Vladimir Bychkovsky, Sylvain Paris, Eric Chan, and Fredo Durand. Learning photographic global
tonal adjustment with a database of input/output image pairs. In CVPR 2011, pp. 97-104. IEEE,
2011.
Xu Cheng, Chuntung Chu, Yi Zheng, Jie Ren, and Quanshi Zhang. A game-theoretic taxonomy of
visual concepts in dnns. arXiv preprint arXiv:2106.10938, 2021.
Piotr Dabkowski and Yarin Gal. Real time image saliency for black box classifiers. arXiv preprint
arXiv:1705.07857, 2017.
Yubin Deng, Chen Change Loy, and Xiaoou Tang. Aesthetic-driven image enhancement by adversarial
learning. In Proceedings of the 26th ACM international conference on Multimedia, pp. 870-878,
2018.
Chao Dong, Chen Change Loy, Kaiming He, and Xiaoou Tang. Image super-resolution using deep
convolutional networks. IEEE transactions on pattern analysis and machine intelligence, 38(2):
295-307, 2015.
Jiri Fajtl, Vasileios Argyriou, Dorothy Monekosso, and Paolo Remagnino. Amnet: Memorability
estimation with attention. In Proceedings of the IEEE Conference on Computer Vision and Pattern
Recognition, pp. 6363-6372, 2018.
Martha J Farah. Is an object an object an object? cognitive and neuropsychological investigations of
domain specificity in visual object recognition. Current Directions in Psychological Science, 1(5):
164-169, 1992.
Lore Goetschalckx, Alex Andonian, Aude Oliva, and Phillip Isola. Ganalyze: Toward visual defini-
tions of cognitive image properties. In Proceedings of the IEEE/CVF International Conference on
Computer Vision, pp. 5744-5753, 2019.
John C Harsanyi. A simplified bargaining model for the n-person cooperative game. International
Economic Review, 4(2):194-220, 1963.
Vlad Hosu, Bastian Goldlucke, and Dietmar Saupe. Effective aesthetics prediction with multi-level
spatially pooled features. In Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition, pp. 9375-9383, 2019.
Yangqing Jia, Joshua T Abbott, Joseph L Austerweil, Tom Griffiths, and Trevor Darrell. Visual
concept learning: Combining machine vision and bayesian generalization on concept hierarchies.
In Advances in Neural Information Processing Systems, pp. 1842-1850, 2013.
Xin Jin, Le Wu, Xiaodong Li, Xiaokun Zhang, Jingying Chi, Siwei Peng, Shiming Ge, Geng Zhao,
and Shuying Li. Ilgnet: inception modules with connected local and global features for efficient
image aesthetic quality classification using domain adaptation. IET Computer Vision, 13(2):
206-212, 2019a.
Xin Jin, Le Wu, Geng Zhao, Xiaodong Li, Xiaokun Zhang, Shiming Ge, Dongqing Zou, Bin Zhou,
and Xinghui Zhou. Aesthetic attributes assessment of images. In Proceedings of the 27th ACM
International Conference on Multimedia, pp. 311-319, 2019b.
10
Under review as a conference paper at ICLR 2022
Shaoqing Ren Kaiming He, Xiangyu Zhang and Jian Sun. Deep residual learning for image recogni-
tion. In CVPR, 2016.
Aditya Khosla, Akhil S Raju, Antonio Torralba, and Aude Oliva. Understanding and predicting
image memorability at a large scale. In Proceedings of the IEEE international conference on
computer vision, pp. 2390-2398, 2015.
Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classification with deep con-
volutional neural networks. Advances in neural information processing systems, 25:1097-1105,
2012.
Thomas L Griffiths, Charles Kemp, and Joshua B Tenenbaum. Bayesian models of cognition. 2008.
Iro Laina, Ruth C Fong, and Andrea Vedaldi. Quantifying learnability and describability of visual
concepts emerging in representation learning. arXiv preprint arXiv:2010.14551, 2020.
Wei Luo, Xiaogang Wang, and Xiaoou Tang. Content-based photo quality assessment. In 2011
International Conference on Computer Vision, pp. 2206-2213. IEEE, 2011.
Eftichia Mavridaki and Vasileios Mezaris. A comprehensive aesthetic quality assessment method for
natural images using basic rules of photography. In 2015 IEEE International Conference on Image
Processing (ICIP), pp. 887-891. IEEE, 2015.
Roey Mechrez, Eli Shechtman, and Lihi Zelnik-Manor. Saliency driven image manipulation. Machine
Vision and Applications, 30(2):189-202, 2019.
Sean Moran, Pierre Marza, Steven McDonagh, Sarah Parisot, and Gregory Slabaugh. Deeplpf: Deep
local parametric filters for image enhancement. In Proceedings of the IEEE/CVF Conference on
Computer Vision and Pattern Recognition, pp. 12826-12835, 2020.
Naila Murray, Luca Marchesotti, and Florent Perronnin. Ava: A large-scale database for aesthetic
visual analysis. In 2012 IEEE Conference on Computer Vision and Pattern Recognition, pp.
2408-2415. IEEE, 2012.
Marco Tulio Ribeiro, Sameer Singh, and Carlos Guestrin. ” why should i trust you?” explaining the
predictions of any classifier. In Proceedings of the 22nd ACM SIGKDD international conference
on knowledge discovery and data mining, pp. 1135-1144, 2016.
L. Shapley. A value for n-person games. 1953.
Kekai Sheng, Weiming Dong, Chongyang Ma, Xing Mei, Feiyue Huang, and Bao-Gang Hu. Attention-
based multi-patch aggregation for image aesthetic assessment. In Proceedings of the 26th ACM
international conference on Multimedia, pp. 879-886, 2018.
Oleksii Sidorov. Changing the image memorability: From basic photo editing to gans. In Proceedings
of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops, pp. 0-0,
2019.
Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image
recognition. In ICLR, 2015.
Mukund Sundararajan, Ankur Taly, and Qiqi Yan. Axiomatic attribution for deep networks. In
International Conference on Machine Learning, pp. 3319-3328. PMLR, 2017.
Hossein Talebi and Peyman Milanfar. Nima: Neural image assessment. IEEE Transactions on Image
Processing, 27(8):3998-4011, 2018.
Joshua B Tenenbaum. Bayesian modeling of human concept learning. Advances in neural information
processing systems, pp. 59-68, 1999.
Paul Thagard. Cognitive science. Routledge, 2008.
Ruixing Wang, Qing Zhang, Chi-Wing Fu, Xiaoyong Shen, Wei-Shi Zheng, and Jiaya Jia. Underex-
posed photo enhancement using deep illumination estimation. In Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition, pp. 6849-6857, 2019.
11
Under review as a conference paper at ICLR 2022
Robert J Weber. Probabilistic values for games, the shapley value. essays in honor of lloyd s. shapley
(ae roth, ed.), 1988.
Saining Xie and Zhuowen Tu. Holistically-nested edge detection. In Proceedings of the IEEE
international conference on computer vision, pp. 1395-1403, 2015.
Fei Xu and Joshua B Tenenbaum. Word learning as bayesian inference. Psychological review, 114
(2):245, 2007.
Jianchao Yang, John Wright, Thomas S Huang, and Yi Ma. Image super-resolution via sparse
representation. IEEE transactions on image processing, 19(11):2861-2873, 2010.
12
Under review as a conference paper at ICLR 2022
A Essential difference between cognitive concepts and
saliency/attribution methods.
Xori	Boosting Saliency Boosting cognition	Xori
Boosting saliency Boosting cognition
Figure 6: Difference between boosting saliency and boosting cognition. Boosting saliency changes
the color of the flower to make it more distinctive and attract more attention. In comparison, boosting
the cognition makes flowers redder to be recognized much more easily. xori represents the original
image.
The essential difference between cognitive concepts and salient pixels is as follows. Saliency
methods (Mechrez et al., 2019) simply classify all pixels into salient pixels and non-salient pixels
based on their abilities to attract human attention. The goal of these saliency methods is to make each
pixel more attractive by changing its contrast or color, instead of concerning whether each pixel is
cognitive or not. In comparison, our method models the cognitive difficulty at the level of conceptual
representation, instead of at the pixel-wise manner. I.e. the cognition difficulty of a pixel depends on
how this pixel collaborates with surrounding pixels. In this way, the multi-variate interaction can
better model how to push a pixel to collaborate with other pixels to form a cognitive concept, rather
than simply change the contrast or the color of this pixel in a local manner. For example, in Fig. 6,
boosting saliency changes the color of the flower to make it distinctive and attract more attention. In
comparison, boosting cognition made the flower redder be much more easily recognized.
Moreover, the difference between attribution methods (Sundararajan et al., 2017; Ribeiro et al., 2016)
and cognitive concepts is as follows. The attribution methods usually reflect the importance of the
target object to the result of object detection, while our method focuses on how the collaboration
inside the concept contributes to the feature representation.
B Proof of the decomposition in Eq. (1)
Mathematically, we have proven that the feature f (N) of an intermediate layer in the DNN can
be decomposed into interaction utilities of different visual concepts S, when we use the Harsanyi
dividend (Harsanyi, 1963) to measure the interaction utility I(S).
f(N) = S⊆N I(S)
(11)
• Proof :
S⊆N I(S) =	(-1)|S|-|L|f(L)
S⊆N L⊆S
= X X (-1)|K|f(L)	%LetK=S\L
L⊆N K⊆N\L
"n-∣L∣	-
= X X Cn|K-||L|(-1)|K| f(L)=f(N)
L⊆N ∣K∣=0
C Detailed introduction of setting baseline values
Given a pre-trained DNN f and the image X consisting of n patches, N = {1,2, .一，n}, f (S) ∈ Rd
denotes the feature of an intermediate layer in the DNN when only patches in the concept S ⊆ N are
given. In the computation of f(S), we mask patches in N \ S to baseline values by following settings
13
Under review as a conference paper at ICLR 2022
in (Dabkowski & Gal, 2017) and keep patches in S unchanged, so as to approximate the removal of
patches in N \ S. In this way, f(S) can be represented as follows.
f(S) = f (mask(x, S)),
mask(x, S) = XS t bg,
(XS t bS)i = χ xi,
bi,
i∈S
i∈S = N\S
(12)
where mask(X, S) denotes the masked image, and bi denotes the baseline value of the i-th patch. t
indicates the concatenation of x's dimensions in S and b's dimensions in S = N \ S. Following
settings in (Dabkowski & Gal, 2017), the baseline value of each patch is set to the mean value of this
patch over all images, i.e. bi = Ex [Xi].
D Detailed introduction of S hapley values
The Shapley value (Shapley, 1953) defined in game theory is widely considered as an unbiased
estimation of the numerical utility w.r.t. each input patch. Without loss of generality, we extend the
scalar Shapley value to a vectorized one. It is because this paper focuses on the utility of each patch
to the feature representation, rather than the utility to the scalar classification result. Given a trained
DNN f and an image X with n patches N = {1, 2, ∙∙∙ ,n}, some patches may cooperate to form
a context L ⊆ N to influence a feature of an intermediate layer y = f(X) ∈ Rd. To this end, the
Shapley value is proposed to fairly divide and assign the overall effects on the feature to each patch,
as follows.
Φ(i∣N) = Xl⊆n∖{i} (n TLn-I)!|L1, h∆f (i,L)i,	(13)
Here, the Shapley value φ(i∣N) ∈ Rd represents the utility of the patch i to the feature. ∆f (i, L) ==
f(L ∪ {i}) - f(L) ∈ Rd measures the marginal utility of the patch i to the feature, given a set of
contextual patches L.
Weber (1988) has proven that the Shapley value is a unique method to fairly allocate the overall utility
to each patch that satisfies following properties.
(1)	Linearity property: If two independent DNNs can be merged into one DNN, then the Shapley
value of the new DNN also can be merged, i.e. ∀i ∈ N, φu (i|N) = φv (i|N) + φw (i|N); ∀c ∈ R,
Φc∙u(i∖N )= C ∙ Φu(i∖N).
(2)	Nullity property: The dummy patch i is defined as a patch satisfying ∀S ⊆ N \ {i}, v(S ∪
{i}) = v(S) + v({i}), which indicates that the patch i has no interactions with other patches in N,
φ(i∣N) = v({i})- v(0).
(3)	Symmetry property: If ∀S ⊆ N \ {i,j}, v(S ∪ {i}) = v(S ∪ {j}), then φ(i∣N) = φ(j∣N).
(4)	Efficiency property: Overall utility can be assigned to all patches, Pi∈N φ(i∖N) = v(N) - v(0).
E Proof of Eq. (4)
We have proven that the term ∆f (i, L) in the Shapley value can be re-written as the sum of interaction
utilities of some visual concepts.
∆f(i, L) =	L0⊆LI(S0=L0∪{i}).
(14)
14
Under review as a conference paper at ICLR 2022
• Proof :
right = XL0 ⊆LI(S0 = L0 ∪ {i})
= X h X (-1)|L0|+1-|K|f(K) + X (-1)|L0|-|K|f(K ∪ {i})i	% Based on Eq. (1)
L0⊆L K⊆L0	K⊆L0
= X X (-1)|L0|-|K| [f(K ∪ {i}) - f(K)]
L0⊆L K⊆L0
=X X (-1)"TKl∆f(i,K)
L0⊆L K⊆L0
=X X (-1)1P l∆f(i,K)	% Let P = L0 \ K
K⊆L P⊆L∖K
∕∣l∣-∣k∣	ʌ
= X X (-1)pC|pL|-|K|	∆f(i, K)	%Letp= |P|
K⊆L	p=0
八 l∣-∣k∣	∖
=X0 ∙ △/(”)+ X I X (-1)p簟HKJ AfdK)
K$L	K=L	p=0
= ∆f(i, L) = left
F	Discussions on the enumeration of contexts
In this section, we analyze reasons for only enumerating contexts L consisting of 1 ≤ r ≤ 0.5n
patches in Eq. (4), Eq. (5), and Eq. (6). In summary, there are two reasons.
First, a large r hurts the sparsity assumption. In this paper, we assume that the marginal utility
∆f (i, L) is very sparse among all contexts {L}. In other words, only a small ratio of contexts {L}
have significant impacts k∆f (i, L)k2 on the feature representation, and most contexts {L} make
negligible impacts k∆f (i, L)k2. If r is larger, according to Eq. (4), ∆f (i, L) contains more contexts
{L} . For example, if r = n, there are 2n contexts {L} in total. It is more difficult to maintain
∆f (i, L) sparse among these massive tuples {(i, L)}. It is because, based on Eq. (4), ∆f (i, L)
is represented as the sum of interaction utilities of massive visual concepts, and then we cannot
guarantee only a small ratio of contexts {L} have significant impacts k∆f (i, L)k2 on the feature
representation. In this way, a large r hurts the sparsity assumption.
Second, a large r boosts the difficulty of the image revision. It is because contexts L consisting
of massive patches (a large r) are usually encoded as interactions between middle-scale concepts
or between large-scale concepts (e.g. background), rather than patch-level collaborations. In this
way, it is difficult to revise images at the patch level by penalizing such middle/large-scale concepts.
Whereas, contexts consisting of a few patches are mainly encoded as interactions between small-scale
concepts at the patch level, such as a local texture or a local object. Thus, punishing small-scale
concepts is more likely to revise the image in detail at the patch level than punishing large-scale
concepts.
15
Under review as a conference paper at ICLR 2022
G More visualization results
In experiments, we revised the hue, saturation, brightness, and sharpness of an image to test its
cognitive difficulty from different perspectives. Specifically, given an original image xori, the
image xc+og was revised from xori by strengthening cognitive concepts, i.e. minθ Loss2 (θ) defined
in Eq. (6), where θ = {k(c,hue), k(c,sat), k(c,bright), k(c,blur/sharp) |c ∈ Ncenter}. The image xc-og was revised
from xori by discarding cognitive concepts, i.e. maxθ Loss2(θ). The image X Was revised from xori
by strengthening cognitive concepts and discarding non-cognitive concepts, i.e. minθ (Loss1 (θ) +
Loss2(θ)). The image x- Was revised from xori by by Weakening concepts, i.e. minθ(Loss1(θ) -
Loss2(θ)).
Image revision based on VGG-16	Image revision based on ReSNet-18
Distribution of revised
concepts in xl^,g
Distribution of revised
concepts in x~
I)Istnbutlon of revised
concepts in λ,",v
Distribution of revised
i in x
*ln9qopv.JJN
Figure 7: Visualization of the original image xori, the image xc+og revised by strengthening cognitive
concepts (minθ Loss2(θ)), the image x- revised by Weakening concepts (minθ Loss1(θ) - Loss2 (θ)).
Here, We revised images from the ImageNet dataset and the MIT-Adobe 5K dataset based on the
VGG-16 model and the ResNet-18 model, respectively.
19zθxκ1I
%Ori
露
广
%ori

广
--

16
Under review as a conference paper at ICLR 2022
“9Z9MEIU一
Image revision based on ResNet-18
Image revision based on VGG-16
Figure 8: Comparisons between original images xori, images xc+og revised by strengthening cognitive
concepts (minθ Loss2 (θ)), images xc-og revised by discarding cognitive concepts (maxθ Loss2 (θ)),
and images X revised by strengthening cognitive concepts and weakening non-cognitive concepts
(minθ Loss1(θ) + Loss2(θ)). We revised images from the ImageNet dataset based on the VGG-16
model and the ResNet-18 model, respectively.
17
Under review as a conference paper at ICLR 2022
Ms QqOpVJIW
Image revision based on VGG-16
Figure 9: Comparisons between original images xori, images xc+og revised by strengthening cognitive
concepts (minθ Loss2 (θ)), images xc-og revised by discarding cognitive concepts (maxθ Loss2 (θ)),
and images X revised by strengthening cognitive concepts and weakening non-cognitive concepts
(minθ Loss1 (θ) + Loss2(θ)). We revised images from the MIT-Adobe 5K dataset based on the
VGG-16 model.
H	Example for the human measure of the cognitive difficulty
In order to examine whether strengthening cognitive concepts could decrease the cognitive difficulty
of an image, we conducted an experiment with 383 human participants. In this experiment, we
showed each participant a group of images, including one original image xori, the image xc+og revised
from xori by strengthening cognitive concepts, and the image xc-og revised from xori by weakening
cognitive concepts. Then, as Fig. 10 shows, we asked each participant to sort the cognitive difficulty
of these three images, and each participant was shown 20 groups of images in total.
18
U nderreview as a conference paper at ICLR 2022
Human measure for cognitive difficulty
Here, the cognitive difficulty of an image is referred to as whether this image is easy to recognize.
1.Q: please sort the cognitive difficulty of the following three images.
*
2.Q: please sort the cognitive difficulty of the following three images.
*
Figure 10: An example for the human measure of the cognitive difficulty. We asked each human
participant to sort thelcognitive difficulty of three images, including one original image, the image
revised by strengthening cognitive concepts, and the image revised by weakening cognitive concepts.
I Difference in encoding of concepts between highly cognitive
IMAGES AND LOW COGNITIVE IMAGES
To verify the hypothesis, we considered the cognitive difficulty of an image from two aspects, i.e. the
beauty of image contents and the noise level. We considered that images with beautiful contents were
usually more cognitive than images without beautiful contents, and assumed to contain more cognitive
concepts. it is because images with beautiful contents often had bright colors and clear contours,
thereby being more cognitive. on the other hand, we considered strong noises in images boosted the
cognitive burden, and the smoothing operation of an image could decrease its cognitive difficulty. in
this way, we assumed that non-cognitive concepts in the smoothed images were weakened.
Beauty of image contents. We used the VGG-16 (Simonyan & Zisserman, 2015) model trained
on the imageNet dataset (Krizhevsky et al., 2012) to evaluate images from the the Aesthetic Visual
Analysis (AVA) dataset (Murray et al., 2012) and the cUHK-PhotoQuality (cUHK-PQ) dataset (Luo
et al., 2011), respectively. All aesthetic images and not so aesthetic images had been annotated in
these two datasets3.
For verification, we computed the metric T(r) (∆f) = P(r) (∆f |Xaesthetic) - P(r) (∆f |Xunaesthetic), where
Xaesthetic and Xunaesthetic referred to a set of massive aesthetic images with beautiful contents and a set of
not so aesthetic images without beautiful contents, respectively. P(r) (∆f |Xaesthetic) denoted the value
3in particular, we considered images in the AVA dataset with the highest aesthetic scores as aesthetic images
and regarded images with the lowest aesthetic scores as not so aesthetic images.
19
Under review as a conference paper at ICLR 2022
distribution of∆f = k∆f (i, L)k2 among all patches {i ∈ N} and all contexts {L|L ⊆ N \{i}, |L| = r}
contained by massive aesthetic images x ∈ Xaesthetic. Similarly, P (r)(∆f |Xunaesthetic) represented the
value distribution of ∆f among all patches {i} and all contexts {L} contained by not so aesthetic
images x ∈ Xunaesthetic. If τ(r) (∆f) > 0 for large values of ∆f, it indicated that aesthetic images
contained more cognitive concepts than not so aesthetic images. In experiments, f was implemented
as the feature of the first fully-connected (FC) layer of the VGG-16 model. Fig. 11 (a) shows that
aesthetic images usually included more cognitive concepts than not so aesthetic images.
Noises in the image. To verify the assumption that non-cognitive concepts in the smoothed im-
ages were significantly weakened, we conducted the following experiments. We used the Gaussian
blur operation to smooth images to eliminate noises. The metric κ(r) (∆f) = P (r)(∆f |Xsmoothed) -
P (r)(∆f |Xoriginal) was calculated for verification, where Xsmoothed and Xoriginal referred to a set of the
smoothed images and a set of the corresponding original images, respectively. P(r)(∆f ∣Xsmoothed)
represented the value distribution of ∆f = k∆f (i, L)k2 among all pixels {i ∈ N} and all con-
texts {L|L ⊆ N \ {i}, |L| = r} contained by the smoothed images x ∈ Xsmoothed. Accordingly,
P (r)(∆f |Xoriginal) denoted the value distribution of ∆f among all patches {i} and all contexts {L}
contained by the original images x ∈ Xoriginal. Fig. 11 (b) shows non-cognitive concepts in the
smoothed images were weakened, i.e. κ(r)(∆f) > 0 for small values of ∆f.
(a) T(r)(A。= P®(AflXaeSta⅛)-Pw(Af∣Xλmestbetfc)	(b) κw(ΔQ = P<r>(Δf∣X<taoised) - Pw(Δf∣Xor⅛M)
√f>(∆f)
0⅛.⅛50
√f)(∆f)
-0.02
0.00
-0.01
0.00
-0.01
-0.02
-0.03 λ	C iiaλ.. tλ∣
0	3 MfGL)I
0 , 3∣[Δ¾L)∣∣2
2
w(∆f) 0.00	
	l^^7^^^	-
-0.02	
-0.04	∖∣ — r=0,3n
	0 3 ∣∣Δ<i,L)∣∣2
0.02	
0.00	
-0.02	∖J - r=0.3n
0 3∣∣Δφ,L)∣2	
-0.04
tyW
0.00
√r)(∆0
0.02
----r=0.5n
0.00
■0.02
■0.07U_______ΞJ_
o 3 ∣∣Δ‰L)I∣2
		
			r=0.5n
b 4∣∣Δf(i,L)∣∣2
√f>(∆f) 0.005 0.000	Λ 	 r=O.ln	演(Af) 0.03 0.00 -0.03 2 演(Af)	A — r=0.3n	√f>(∆f) 0.06 0.02 ■0.02 2 √t>(∆f)	—― r=0.5n^∣	2
-0.005 演(Af)						
	3 Il颂i,Dl		3 Il颂i,L)∣		)3 IMi, DI	
	A 	 r=0.1n		ʌ 	 r=0.3n			
0.004 0.000		0.04 0.00		0.04 ∩ ∩∩∩		
						
-0.004		A t∖Λ		V.Wv ■0.03		
	r__________I _____________r___________I -v∙vj ]__________________I
0	3∣∣Δf(i,L)ll2 -0∙040 3 l∣Δ¾L)l∣2	0 J ∣∣Δ¾L)I∣2
<
Figure 11: (a) Aesthetic images usually contained more cognitive concepts than not so aesthetic
images. (b) non-cognitive concepts in the smoothed images were weakened.
20