Under review as a conference paper at ICLR 2022
Learning Pessimism for Robust and Efficient
Off-Policy Reinforcement Learning
Anonymous authors
Paper under double-blind review
Ab stract
Popular off-policy deep reinforcement learning algorithms compensate for over-
estimation bias during temporal-difference learning by utilizing pessimistic es-
timates of the expected target returns. In this work, we propose a novel learn-
able penalty to enact such pessimism, based on a new way to quantify the critic’s
epistemic uncertainty. Furthermore, we propose to learn the penalty alongside
the critic with dual TD-learning, a strategy to estimate and minimize the bias
magnitude in the target returns. Our method enables us to accurately counteract
overestimation bias throughout training without incurring the downsides of overly
pessimistic targets. Empirically, by integrating our method and other orthogonal
improvements with popular off-policy algorithms, we achieve state-of-the-art re-
sults in continuous control tasks from both proprioceptive and pixel observations.
1	Introduction
Sample efficiency and generality are two directions in which reinforcement learning (RL) algo-
rithms are still lacking, yet, they are crucial for tackling complex real-world problems (Mahmood
et al., 2018). Consequently, so far, many RL milestones have been achieved through simulating con-
spicuous amounts of experience and tuning for effective task-specific parameters (Mnih et al., 2013;
Silver et al., 2017). Recent off-policy model-free (Lee et al., 2021; Chen et al., 2021) and model-
based algorithms (Chua et al., 2018; Janner et al., 2019), pushed forward the state-of-the-art sample-
efficiency on several benchmark tasks (Brockman et al., 2016). We attribute such improvements to
two main linked advances: more expressive models to capture uncertainty and better strategies to
counteract detrimental biases from the learning process. These advances yielded the stabilization to
adopt more aggressive optimization procedures, with particular benefits in lower data regimes.
Modern off-policy algorithms learn behavior by optimizing the expected performance as predicted
by a trained parametric deep model of the environment. Within this process, overestimation bias
naturally arises from the maximization performed over the model’s performance predictions, and
consequently, also over the model’s possible errors. In the context of model-free RL, such model
is trained to predict the agent’s future returns via temporal difference (TD-) learning and is referred
to as the critic. A common strategy to counteract overestimation is to parameterize the critic with
multiple, independently-initialized networks and optimize the agent’s behavior over the minimum
value of the relative outputs (Fujimoto et al., 2018). Empirically, this strategy consistently yields
pessimistic target performance measures, avoiding overestimation bias propagating through the TD-
learning target bootstraps. However, this approach directly links the critic’s parameterization to bias
counteraction, making improvements in each of these components hard to pursue independently.
Based on these observations, we propose a more general formulation for counteracting overesti-
mation bias independently. In particular, we compute the critic’s target performance predictions
by replacing the ubiquitous minimization procedure with an explicit penalty that is agnostic to the
critic’s parameterization. Our proposed penalty is the output of a linear model of the epistemic un-
certainty, computed as the expected Wasserstein distance between the return distributions predicted
by the critic. Based on this formulation, we derive Generalized Pessimism Learning (GPL), a new
strategy that learns an accurate penalization throughout the RL process. Within this strategy, we
propose optimizing the penalty’s weight with dual TD-learning, a new procedure minimizing the
estimated bias in the critic’s performance predictions with dual gradient descent. GPL is the first
effective method able to freely learn an unbiased performance objective throughout training.
1
Under review as a conference paper at ICLR 2022
Furthermore, we also extend GPL by introducing a new pessimism annealing procedure, motivated
by the principle of optimism in the face of uncertainty (Brafman & Tennenholtz, 2002). This proce-
dure leads the agent to adopt a risk-seeking behavior policy by utilizing a purposely biased estimate
of the performance in the initial training stages. Hence, it trades-off expected immediate perfor-
mance for directed exploration, incentivizing the visitation of states with high epistemic uncertainty
from which the critic would gain more information.
We incorporate GPL with modern implementations of the Soft Actor-Critic (SAC) (Haarnoja et al.,
2018a;b) and Data-regularized Q (DrQ) (Yarats et al., 2021b;a) algorithms, yielding GPL-SAC and
GPL-DrQ, respectively. On the Mujoco environments from the OpenAI Gym suite (Todorov et al.,
2012; Brockman et al., 2016), GPL-SAC outperforms prior, more expensive, model-based (Janner
et al., 2019) and model-free (Chen et al., 2021) state-of-the-art algorithms. For instance, in the Hu-
manoid environment GPL-SAC is able to recover a score of 5000 in less than 100K experience steps,
more than nine times faster than regular SAC. Additionally, on pixel-based environments from the
DeepMind Control Suite (Tassa et al., 2018), GPL-DrQ provides significant performance improve-
ments from the recent state-of-the-art DrQv2 algorithm. These results highlight the effectiveness and
applicability of GPL, in spite of introducing only negligible computational overheads. We release
our implementations to facilitate future comparisons and extensions.
In summary, we make several contributions towards improving off-policy reinforcement learning:
•	We propose a novel penalty to counteract overestimation bias, disentangling the critic’s
parameterization from the enforced pessimism.
•	We propose the first optimization method to estimate and precisely counteract overestima-
tion bias throughout training with dual gradient descent.
•	We propose a pessimism annealing strategy that exploits epistemic uncertainty to actively
seek informative states in the early training stages.
•	Integrating our method with SAC and DrQ, we achieve new state-of-the-art performance
results with trivial overheads on both proprioceptive and pixel observations tasks.
2	Related work
Modern model-free off-policy algorithms utilize different strategies to counteract overestimation
bias arising in the critic’s TD-targets (Thrun & Schwartz, 1993; Pendrith et al., 1997; Mannor et al.,
2007). Many approaches combine the predictions of multiple function approximators to estimate
the expected returns, for instance, by independently selecting the bootstrap action (Hasselt, 2010).
In discrete control, such technique appears to mitigate the bias of the seminal DQN algorithm (Mnih
et al., 2013), consistently improving performance (Van Hasselt et al., 2016; Hessel et al., 2018). In
continuous control, similar strategies successfully stabilize algorithms based on the policy gradient
theorem (Silver et al., 2014; Lillicrap et al., 2015). Most notably, Fujimoto et al. (2018) proposed
to compute the critic’s TD-targets by taking the minimum over the outputs of two different action-
value models. This particular minimization strategy has become ubiquitous, being employed in
many popular follow-up algorithms (Haarnoja et al., 2018b; Yarats et al., 2021b). To better trade-off
optimism and pessimism, Zhang et al. (2017) proposed using a weighted combination of the original
and minimized targets. Instead, Kuznetsov et al. (2020) proposed to parameterize a distributional
critic and drop a fixed fraction of the predicted quantiles to compute the targets. Moreover, as
in our approach, several works also considered explicit penalties based on heuristic measures of
epistemic uncertainty (Lee et al., 2013; Ciosek et al., 2019). Recently, Kumar et al. (2020) proposed
to complement these strategies by further reducing bias propagation through actively weighing the
TD-loss of different experience samples. Aleatoric uncertainty is also an additional source of bias
in TD-learning, due to the practical inability of considering multiple transition samples in stochastic
environments (Baird, 1995). This phenomenon is known as the double-sample issue, but has been
rarely addressed in prior off-policy literature (Dai et al., 2018).
Within model-based RL (Atkeson & Santamaria, 1997), recent works have achieved remarkable
sample efficiency by learning large ensembles of dynamic models for better predictions (Chua et al.,
2018; Wang & Ba, 2019; Janner et al., 2019). In the model-free framework, prior works used large
critic ensembles for more diverse scopes. Anschel et al. (2017) proposed to build an ensemble
using several past versions of the value network to reduce the magnitude of the TD-target’s bias.
2
Under review as a conference paper at ICLR 2022
Moreover, Lan et al. (2020) introduced a sampling procedure for the critic’s ensemble predictions to
regulate underestimation in the TD-targets. Their work was later extended to the continuous setting
by Chen et al. (2021), which showed that large ensembles combined with a high update-to-data ratio
enable to outperform the sample efficiency of contemporary model-based methods. Ensembling
has also been used to achieve better exploration following the principle of optimism in the face of
uncertainty (Brafman & Tennenholtz, 2002) in both discrete (Osband et al., 2016; Chen et al., 2017)
and continuous settings (Ciosek et al., 2019). Lee et al. (2021) further showed the effectiveness of
combining several of these strategies in a unified framework.
In the same spirit as this work, multiple prior methods attempted to learn components and parameters
of underlying RL algorithms. Several works have approached this problem by utilizing expensive
meta-learning strategies to obtain new learning objectives based on the multi-task performance from
low-computation environments (Bechtle et al., 2021; Oh et al., 2020; Xu et al., 2020; Co-Reyes et al.,
2021). More related to our work, Moskovitz et al. (2021) recently proposed to use an adaptive bi-
nary controller that switches on or off a bias correction penalty for the TD-targets. In particular, they
treat the controller optimization task as a multi-armed bandit problem performed throughout differ-
ent training iterations to maximize immediate performance improvements. Instead, GPL makes
effective use of dual gradient descent to minimize bias directly, similarly to how Haarnoja et al.
(2018a;b) learns the exploration temperature parameter in the Soft Actor-Critic (SAC) algorithm.
3	Preliminaries
In RL, we aim to autonomously recover optimal agent behavior for performing a particular task.
Formally, we describe this problem setting as a Markov Decision Process (MDP), defined as the
tuple (S, A, P, p0, r, γ). At each time-step of interaction, the agent observes some state in the state
space, s ∈ S, and performs some action in the action space, a ∈ A. The transition dynamics
function P : S × A × S → R and the initial state distribution p0 : S → R describe the evolution
of the environment as a consequence of the agent’s behavior. The reward function r : S × A → R
quantifies the effectiveness of each performed action, while the discount factor γ ∈ [0, 1) represents
the agent’s preference for earlier rewards. A policy π : S × A → R maps each state to a probability
distribution over actions and represents the agent’s behavior. An episode of interactions between
the agent and the environment yields some trajectory τ containing the transitions experienced, τ =
(s0,a0,ro,sι,aι,rι,...). The RL objective is then to find an optimal policy π* that maximizes the
expected sum of discounted future rewards:
π* = arg max Epπ(τ)
π
∞
γtr(st, at) ,
t=0
(1)
where pπ (τ) represents the distribution of trajectories stemming from the agent’s interaction with
the environment. Off-policy RL algorithms commonly utilize some critic model to evaluate the
effectiveness of the agent’s behavior. A straightforward choice for the critic is to represent the
policy’s action-value function Qπ : S × A → R. This function quantifies the expected sum of
discounted future rewards after executing some particular action from a given state:
∞
Qn (S, a) = Epn (T∣so = s,a0=a)	γ r(st, at) .	(2)
t=0
Most RL algorithms consider learning parameterized models for both the policy, πθ , and the corre-
sponding action-value function, Qφπ. In particular, after storing experience transitions (s, a, s0, r) in
a replay data buffer D, we learn Qπφ by iteratively minimizing a squared TD-loss of the form:
JQ(φ) = E(s,a,s0,r)〜D [(Qφ(S, a) - y)2],
y = r + YEa〜∏(s0) [Q∏0(s0,a)].
(3)
Here, the TD-targets y are obtained by computing a 1-step bootstrap with a target action-value
estimator Qφπ0. Usually, Qφπ0 is a regularized function of action-value predictions from a target critic
model using delayed parameters φ0 . Following the policy gradient theorem (Sutton et al., 2000;
Silver et al., 2014), we can then improve our policy by maximizing the expected returns as predicted
by the critic, e.g., by minimizing the negated action-value estimates:
Jπ (θ) = -Es〜D,a〜∏θ(s)
[qφ(s, a)].
(4)
3
Under review as a conference paper at ICLR 2022
(l-p)×0 + p×0,
action-value
(・)2	Qψf
squared TD-Ioss target action-value
store
forward computation
parameters update
original components
new components
exploration
↑	I	replay buffer
∖ ∖ /
Q<∕>(s, ɑ) - (r + y%,~jl(s,) [Qφ'(s', α,)- PR(S，a', φ, 0)])<—	Pβ
TD-errors	uncertainty regularizer
(∙ )xβ
dual TD-Ioss
(s,a,s',r)
environment
exploration
D

Figure 1: Schematic overview of the training and exploration processes involved in the proposed
GPL framework. The TD-errors play a central role and are utilized both for updating the critic and
for estimating the current bias to update the parameterized uncertainty regularizer.
4	Addressing overestimation bias
4.1	Bias in q-learning
In off-policy RL, several works have identified an accumulation of overestimation bias in the action-
value estimates as a consequence of TD-learning (Thrun & Schwartz, 1993; Pendrith et al., 1997;
Mannor et al., 2007). Formally, we quantify the target action-value bias B (s, a, s0) as the difference
between the actual and estimated TD-targets for a given transition:
B(s,a,s0) = r + YEa，5(s，) [<Qφo(s0,a0)] - (r + YEa，5(s，)[Qπ(s0,a0)])
=γEao~∏(so) [Qφ,(s',。') - Qπ(s',。')].
(5)
As discussed by Fujimoto et al. (2018), positive bias arises when the target action-values are ob-
tained directly from the outputs of a parameterized action-value function, i.e., Qπφ0 = Qφπ0 . The
reason for this phenomenon is that the policy is trained to locally maximize the action-value esti-
mates from Eqn. 4. Hence, its actions will exploit potential model errors to obtain higher scores,
implying that Es,a〜π(s)[Qφ, (s, a)] > Es,a〜π(s) [Qπ(S, a)]. Instabilities then arise as the errors Can
quickly propagate through the bootstrap operation, inherently causing the phenomenon of positive
bias accumulation. To CounteraCt this phenomenon, Fujimoto et al. (2018) proposed clipped double
Q-learning. This teChnique Consists in learning two separate aCtion-value funCtions and Computing
the target aCtion-values by taking the minimum over their outputs:
Qφmin (s, a) = min (QΦ1 (S, a), qφ2 (s, O))
(6)
The role of the minimization is to Consistently produCe overly pessimistiC estimates of the target
aCtion-values, preventing positive bias aCCumulation. This approaCh is an empiriCally effeCtive strat-
egy for different benChmark tasks and has beCome standard praCtiCe.
4.2	The uncertainty regularizer
In this work, we take a more general approaCh for Computing the target aCtion-values. PartiCularly,
we use a parameterized funCtion, the uncertainty regularizer pβ (s, a, φ, θ), for trying to approximate
the bias in the CritiC’s aCtion-value prediCtions for on-poliCy aCtions. Thus, we speCify an aCtion-
value estimator that penalizes the aCtion-value estimates via the unCertainty regularizer:
ππ
Qπφ0 (s, a) =Qφπ(s,a) -pβ(s,a,φ,θ),
where pβ (s, a, φ, θ) ≈Qφπ (s, a) - Qπ (s, a), for
a 〜∏θ (s).
(7)
A ConsequenCe of this formulation is that as long as pβ is unbiased for on-poliCy aCtions, so will
the aCtion-value estimator. HenCe, the expeCted target aCtion-value bias will be zero, preventing the
positive bias aCCumulation phenomenon without requiring overly pessimistiC aCtion-value estimates.
Based on these observations, in the next seCtion, we speCify a new method that learns an unbiased
unCertainty regularizer and Continuously adapts it to refleCt Changes in the CritiC and poliCy.
4
Under review as a conference paper at ICLR 2022
Figure 2: Recorded estimated bias for ten runs of two simple extensions of the SAC algorithm.
5	Generalized pessimism learning
Generalized pessimism learning (GPL) entails learning a particular parameterized uncertainty reg-
ularizer pβ , defined in Eqn. 8. Our method makes pβ adapt to changes in θ and φ throughout the
RL process to keep the target action-values unbiased. Hence, GPL allows preventing positive bias
accumulation without overly pessimistic targets. With any fixed penalty, we argue that it would
be infeasible to maintain the expected target action-value bias close to zero due to the number of
affecting parameters and stochastic factors in different RL experiments.
5.1	Uncertainty regularizer parameterization
We strive for a parameterization of the uncertainty regularizer that ensures low bias and variance
estimation of the target action-values. Similar to prior works (Ciosek et al., 2019; Moskovitz et al.,
2021), GPL uses a linear model of some measure of the epistemic uncertainty in the critic. Epistemic
uncertainty represents the uncertainty from the model’s learned parameters towards its possible pre-
dictions. Hence, assuming enough model expressivity, the areas of the state and action spaces where
the critic’s epistemic uncertainty is elevated are the areas in which the agent did not yet observe
enough data to reliably predict its returns and, for this reason, the magnitude of the critic’s error is
expectedly higher. Consequently, if a policy yields behavior with high epistemic uncertainty in the
critic, it is likely exploiting positive errors and overestimating its expected returns. As we use the
policy to compute the TD-targets, the higher the uncertainty, the higher the expected positive bias.
Several prior works further discuss this relationship (Fujimoto et al., 2018; Ciosek et al., 2019).
We propose measuring epistemic uncertainty with the expected Wasserstein distance between the
critic’s predicted return distributions Zπ, as defined by Bellemare et al. (2017). In the usual case
where we parameterize the critic with multiple action-value functions, we interpret each action-value
estimate as a Dirac delta function approximation of the return distribution, Zφπ (s, a) = δQπ (s,a) . Our
uncertainty regularizer then consists of linearly scaling this measure via a learnable parameter β :
pβ (s, a, φ, θ) = β
X Ea~∏θ(s),φ1 ,φ2 W (Zφπ1 (s, a), Zφπ2 (s, a)) .	(8)
We estimate the expectation in Eqn. 8 by learning a critic ensemble of N ≥ 2 independent models
with parameters {φi}iN=1, and averaging the distances between the corresponding predicted return
distributions. Notably, the Wasserstein distance has easy-to-compute closed forms for many popular
distributions. For Dirac delta functions, it is equivalent to the distance between the corresponding
locations, hence, W (δQπι(s,a),Bq%?3⑷)=∣Qφ1 (s,a) — Qφ (s,a)∣.
Our quantification of epistemic uncertainty is an interpretable measure for any distributional critic.
Moreover, for some fixed β , increasing the number of critics decreases the estimation variance but
leaves the expected magnitude of the uncertainty regularizer unchanged. This is because the sample
mean of the Wasserstein distances is always an unbiased estimate of Eqn. 8 for N ≥ 2. Under
reasonable assumptions, it is proportional to the standard deviation of the distribution of action-
value predictions. We can also restate clipped double Q-learning using our uncertainty regularizer
with N = 2 and β = 0.5, allowing us to replicate its penalization effects for N > 2 by simply
fixing β . In contrast, Ciosek et al. (2019) proposed the sample standard deviation of the action-value
predictions to quantify epistemic uncertainty. However, the sample standard deviation does not have
a clear generalization to arbitrary distributional critics and its expected magnitude is dependent on
the number of models. We provide formal derivations for these claims in Appendix A.
5
Under review as a conference paper at ICLR 2022
Algorithm 1 GPL-SAC
1:
2:
3:
4:
5:
6:
7:
8:
9:
10:
11:
12:
13:
14:
15:
16:
17:
18:
19:
20:
21:
procedure TRAININGLOop(N, UTD, ρ,γ,H)
Initialize ∏θ, {Qφi }N=ι, {Qψo }N=ι ,β J 0.5,α ― 0.0, D JQ
loop
Observe s, execute a 〜∏θ(s), collect s0,r
Store D J D ∪ (s, a, s0, r)
forj = 1, 2, ..., UTD do
Sample minibatch {(s,a,s0,r)} ∈ D, a0 〜π(s0)
Compute Qφ(s0,a0) J REGULARIZEDVALUE(s0,a0,φ0,θ,β)
Compute the TD-targets y J r + Y (Qφ∕ (s0,a0) — αlog π(a0∣s0))
for i = 1, 2, ..., N do
Compute the TD-errors for the ith critic ei J Qπφi — y
Update φi to minimize J (φ) = ei2	. Learning the critic
Update φ0i J ρφ0i + (1 — ρ)φi	. Updating the target critic
Update β to minimize J(β) = β × PiN=1 ei	. Learning the bias (dual TD-learning)
Sample a 〜π(s), compute Qφ (s, a) J REGULARIZEDVALUE(s, a, φ, θ, β)
Update θ to minimize J (θ) = -Qni(S,a) + α log π(a∣s)	. Learning the policy
Update α to minimize J(α) = α X (— log π(a∣s) — H)	. Learning the entropy bonus
procedure REGULARIZEDVALUE(s, a, φ, θ, β)
Compute pβ (s, a, φ, θ) J Ne-N PN=I P= ∣Qφi (s, a) - Qφ (s, a) |	. Sample mean of Eqn. 8
Compute Qφ(s,a) J N PN=I Qφ<s,a)
return Qφ(s, a) - pβ (s, a, φ, θ)
5.2	Dual td-learning
The expected bias present in the action-value targets is highly dependent on several stochastic factors
stemming from aleatoric uncertainty and the learning process. We empirically show this by running
multiple experiments with simple extensions of the SAC algorithm in different Gym environments
and periodically recording estimates of the action-value bias by comparing the actual and estimated
discounted returns (as described in Appendix B). As shown in Figure 2, the bias in the predicted
action-values notably varies across environments, agents, training stages, and even across different
random seeds. These results reinforce our thesis that there is no fixed penalty able to account for
the many sources of stochasticity in the RL process, even for a single task. Hence, they show the
necessity of learning pβ alongside the policy and critic to accurately counteract bias.
GPL optimizes β as a dual variable, enforcing the expected target action-value bias to be zero:
arg min -β × Es,a,s0 [B(s, a, s0)] .
β
(9)
For arbitrary actions, the action-value estimates are not directly affected by the positive bias induced
by the policy gradient optimization. Consequently, we can make the reasonable assumption of the
critic itself providing an initially unbiased estimate of the expected returns, i.e., Es,a [Qπφ (s, a)] ≈
Es,a [Qπ (s, a)]. Thus, to approximate B(s, a, s0), we propose to use the differences between the
current TD-targets and action-value predictions for off-policy actions:
B(s,a,s0) ≈ B(s,a,s0)= r + 7旧。，〜∏3) [Qφo(s0,a0)] - Q∏(s,a).	(10)
In practice, GPL alternates the optimizations of β for the current bias, and both φ and θ, with
the corresponding updated RL objectives. This procedure is similar to the automatic exploration
temperature optimization proposed by Haarnoja et al. (2018b), approximating dual gradient descent
(Boyd & Vandenberghe, 2004). We can estimate the current bias according to Eqn. 10 at minimal
extra cost by negating the errors from the TD-loss. Thus, we name this procedure dual TD-learning.
Unfortunately, the unbiasedness assumption of Eqn. 10 does not necessarily hold when using deep
networks and approximate stochastic optimization. In particular, given initially biased targets, some
of the bias might propagate to the critic model, influencing the approximation in Eqn. 10. Nonethe-
less, GPL’s performance and the optimization dynamics of dual TD-learning appear to be empiri-
cally robust to moderate levels of initial target bias. We show this in Appendix D.1 by analyzing the
6
Under review as a conference paper at ICLR 2022
Figure 3: Performance curves for the considered five complex environments from the OpenAI Gym
suite. We report both mean and standard deviation of the episodic returns over five random seeds.
behavior and performance of GPL with different initial values of β and, thus, different bias in the ini-
tial targets. An intuition for this robustness is that the relative difference between the off-policy and
on-policy action-value predictions should always push β to counteract bias stemming from model
errors in the policy gradient action maximization (Fujimoto et al., 2018). We further validate dual
TD-learning in Appendix D.2 by comparing and discussing alternative optimization strategies. In
particular, we evaluate optimizing β by minimizing the squared norm of the bias and by using the
bandit-based optimization from Tactical Optimism and Pessimism (Moskovitz et al., 2021).
5.3	Pessimism annealing for directed exploration
As described in Section 3, the policy learns to maximize the unbiased action-values predicted by the
online estimator Qπφ. Motivated by the principle of optimism in the face of uncertainty (Brafman &
Tennenholtz, 2002), we also consider an alternative optimistic policy gradient objective:
JnPt ⑻=-Es,a~π(s) [Q φopt (S,a儿	where Qφopt (S,a) = Qφ (S, Q)- Pβopt (s,φ,θ).	(II)
This objective utilizes an optimistic shifted uncertainty regularizer, pβopt, calculated with parameter
βopt = β - λopt , for some decaying optimistic shift value, λopt ≥ 0. This new objective trades
off the traditional exploitative behavior of the policy with directed exploration. In particular, as
λopt is large, π will be incentivized to perform actions for which the outcome has high epistemic
uncertainty. Therefore, the agent will experience transitions that are increasingly informative for
the critic but expectedly sub-optimal. Hence, we name the process of decaying λopt pessimism
annealing, striving for improved exploration early on without biasing the policy’s final objective.
6	Experiments
To evaluate the effectiveness of GPL, we integrate it with two popular off-policy RL algorithms.
GPL itself introduces trivial computational and memory costs as it optimizes a single additional
weight, re-utilizing the errors in the TD-loss to estimate the bias. Moreover, we implement the
critic’s model ensemble as a single neural network, using linear non-fully-connected layers evenly
splitting the nodes and dropping the weight connections between the splits. Practically, when evalu-
ated under the same hardware, this results in our algorithm running more than two times faster than
the implementation from Chen et al. (2021) while having a similar algorithmic complexity.
We show that GPL significantly improves the performance and robustness of off-policy RL, con-
cretely surpassing prior algorithms and setting new state-of-the-art results. In our evaluation, we
repeat each experiment with five random seeds and record both mean and standard deviation over
7
Under review as a conference paper at ICLR 2022
Walker2d-v2	Ant-v2	Humanoid-v2
Figure 4: Ablation study of the main components differentiating GPL-SAC from SAC-20.
the episodic returns. We report additional details of our experimental settings and utilized hyper-
parameters in Appendix B. Furthermore, we provide comprehensive extended results analyzing dif-
ferent parameters, alternative implementations, and training times in Appendix D.
6.1	Continuous control from proprioceptive observations
GPL-SAC. First, we integrate GPL with Soft Actor-Critic (SAC) (Haarnoja et al., 2018a;b), a popu-
lar model-free off-policy algorithms that uses a weighted entropy term in its objective to incentivize
exploration. Specifically, we substitute SAC’s clipped double Q-learning with our uncertainty regu-
larizer, initialized with β = 0.5. We also adopt additional practices inspired by recent related works
(Chen et al., 2021; Bjorck et al., 2021). In particular, we parameterize ten independent action-value
functions as ‘modern’ residual neural networks with spectral normalization (Miyato et al., 2018)
and increase the critic’s update-to-data (UTD) ratio to twenty. We denote the resulting algorithm
GPL-SAC and provide a summary of this integration in Algorithm 1, highlighting the novelties.
Baselines. We compare GPL-SAC with prior state-of-the-art model-free and model-based algo-
rithms with similar or greater computational complexity, employing high UTD ratios:
•	REDQ (Chen et al., 2021): State-of-the-art, model-free algorithm on the OpenAI Gym
suite. This algorithm employs multiple parameterized action-value functions and utilizes
clipped double Q-learning over a sampled pair of outputs to compute the critic’s targets.
•	MBPO (Janner et al., 2019): State-of-the-art, model-based algorithm on the OpenAI Gym
suite. This algorithm learns a large ensemble of world models and employs a Dyna-style
(Sutton, 1991) optimization procedure to train the policy with short, generated rollout data.
•	SAC-20: Simple SAC extension where we increase the UTD ratio to twenty.
Results. We evaluate GPL-SAC compared to the described baselines on five of the more challenging
environments from the OpenAI Gym suite (Brockman et al., 2016), involving complex locomotion
problems from proprioceptive observations. We evaluate the performance by collecting the returns
over five evaluation episodes every 1000 environment steps. In Figure 3, we provide visualizations
of the different performance curves. We extend this analysis in Appendix C.
GPL-SAC is consistently the best performing algorithm on all environments, setting new state-of-
the-art results for this benchmark at the time of writing. Moreover, the performance gap is greater
for tasks with larger state and action spaces. We motivate this by noting that all baselines use
fixed strategies to counteract overestimation bias, thus, requiring overly pessimistic estimates of the
returns to avoid instabilities. Hence, the resulting policies are likely overly conservative, hindering
exploration and efficiency, with larger effects on the more complex tasks. For instance, on the
Humanoid task, GPL-SAC remarkably surpasses a score of 5000 by the 100K experience threshold,
more than nine times faster than SAC and more than 2.5 times faster than REDQ.
Ablations. Furthermore, we provide results from ablating the main components that differentiate
GPL-SAC from SAC-20 on a subset of environments. Namely, we evaluate the contributions from
learning the uncertainty regularizer, increasing the number of action-value functions, and using the
improved modern action-value network architecture. In Figure 4, we provide visualizations of the
performance curves. Empirically, each component consistently improves performance, with higher
contributions from learning the uncertainty regularizer and increasing the critic’s ensemble size. We
provide many further in-depth ablation studies as part of Appendix D.
8
Under review as a conference paper at ICLR 2022
Table 1: Results summary for the pixel observations experiments on the DeepMind Control Suite
Evaluation milestone	1.5M environment frames	3M environment frames
Algorithm / Task	DrQv2	GPL-DrQ	GPL-DrQ-Expl+	DrQv2	GPL-DrQ GPL-DrQ-Expl+	
Acrobot swingup	277 ± 39	246 ± 31	283 ± 49	414±34	393 ± 29	446 ± 33
Cartpole swingup sparse	475 ± 388	740 ± 123	780 ± 25	503 ± 411	837± 15	824 ± 33
Cheetah run	771 ± 24	837 ± 29	830 ± 25	873 ± 53	903 ± 1	902 ± 2
Finger turn easy	794 ± 159	860 ± 74	810±84	946 ± 16	945 ± 18	952 ± 15
Finger turn hard	484 ± 156	587 ± 238	615 ± 233	923 ± 17	893 ± 59	918±24
Hopper hop	198 ± 101	242 ± 56	252 ± 76	240 ± 123	296 ± 52	349 ± 90
Quadruped run	385 ± 214	564 ± 54	589 ± 71	504 ± 279	712±51	725 ± 112
Quadruped walk	591 ± 270	834 ± 46	826 ± 35	897 ± 46	918±16	912±16
Reach duplo	219 ± 6	217±6	219 ± 5	227 ± 2	226 ± 1	225 ± 3
Reacher easy	961 ± 13	951 ± 21	968 ± 8	952 ± 17	957± 12	962 ± 7
Reacher hard	813 ± 122	790 ± 103	798 ± 114	957± 13	946 ± 38	934 ± 18
Walker run	569 ± 273	574 ± 275	713 ± 5	617 ± 296 618 ± 298		782 ± 10
Top performance count	3/12	8/12	11/12	4/12	7/12	10/12
6.2	Continuous control from pixels
GPL-DrQ. We also integrate GPL with a recent version of Data-regularized Q (DrQv2) (Yarats
et al., 2021a), an off-policy, model-free algorithm achieving state-of-the-art performance for pixel-
based control problems. DrQv2 combines image augmentation from DrQ (Yarats et al., 2021b) with
several advances such as n-step returns (Sutton & Barto, 2018) and scheduled exploration noise
(Amos et al., 2021). Again, we substitute DrQv2’s clipped double Q-learning with our uncertainty
regularizer. To bolster exploration in pixel-based environments, we also integrate pessimism anneal-
ing from Section 5.3, with λopt linearly decayed from 0.5 to 0.0 together with the exploration noise
in DrQv2. We leave the rest of the hyper-parameters and models unaltered to evaluate the generality
of applying GPL. We name the resulting algorithms GPL-DrQ and GPL-DrQ-Expl+, respectively.
Results. We evaluate GPL-DrQ and GPL-DrQ-Expl+ on the environments from the DeepMind
Control Suite (Tassa et al., 2018) modified to yield pixel observations. We use the medium bench-
mark evaluation as described by Yarats et al. (2021a), consisting of 12 complex tasks involving
control problems with hard exploration and sparse rewards. In Table 1 we report the returns ob-
tained after experiencing 3M and 1.5M environment frames. For each run, we average the returns
from 100 evaluation episodes collected in the 100K steps preceding each of these milestones. We
highlight the top performances that are within half a standard deviation from the highest mean return.
We provide visualizations of the relative performance curves in Appendix C.
Both GPL-DrQ and GPL-DrQ-Expl+ significantly improve the performance of DrQv2 in most tasks.
In particular, DrQv2 sporadically yields underperforming returns, likely due to a lack of exploration
from its overly pessimistic critic1. GPL generally appears to resolve this issue, while pessimism
annealing further aids precisely in the tasks where under-exploration is more frequent. Overall,
these results show both the generality and effectiveness of GPL to improve the current state-of-the-
art through simple integrations by providing a framework to better capture and exploit bias.
7	Discussion and future work
We propose Generalized Pessimism Learning (GPL), a strategy that adaptively learns a penalty
to recover an unbiased performance objective for off-policy RL. Unlike traditional methods, GPL
achieves training stability without necessitating overly pessimistic estimates of the target returns,
thus, improving convergence and exploration. We show that simple integrations of GPL with mod-
ern algorithms yield state-of-the-art results for both proprioceptive and pixel-based control tasks.
Moreover, GPL’s penalty has a natural generalization to different distributional critics and varia-
tional representations of the weights posterior. Hence, our method has the potential to facilitate
research going beyond action-value functions and model ensembles for continuous control, two ex-
citing extensions we leave for future work.
1This instability was also observed by DrQv2’s authors after re-collecting their results.
9
Under review as a conference paper at ICLR 2022
Ethics statement
This work aims to improve the efficiency and generality of reinforcement learning algorithms. Thus,
it may contribute to future effective deployments of autonomous agents for real-world applications.
Furthering automation has the potential to provide many economic and social benefits to human-
ity. However, if unregulated, such advancements could accentuate societal inequalities, worsen the
consequences of harmful misuse, and have a tangible environmental impact.
Reproducibility statement
In the supplementary material, we provide a compact version of our source code that enables the
reproduction of all experiments in this work. After review, we will open-source a comprehensive,
documented repository for this project to facilitate future extensions.
References
Brandon Amos, Samuel Stanton, Denis Yarats, and Andrew Gordon Wilson. On the model-based
stochastic value gradient for continuous reinforcement learning. In Learning for Dynamics and
Control,pp. 6-20. PMLR, 2021.
Oron Anschel, Nir Baram, and Nahum Shimkin. Averaged-dqn: Variance reduction and stabilization
for deep reinforcement learning. In International conference on machine learning, pp. 176-185.
PMLR, 2017.
Christopher G Atkeson and Juan Carlos Santamaria. A comparison of direct and model-based re-
inforcement learning. In Proceedings of international conference on robotics and automation,
volume 4, pp. 3557-3564. IEEE, 1997.
Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint
arXiv:1607.06450, 2016.
Leemon Baird. Residual algorithms: Reinforcement learning with function approximation. In
Machine Learning Proceedings 1995, pp. 30-37. Elsevier, 1995.
Sarah Bechtle, Artem Molchanov, Yevgen Chebotar, Edward Grefenstette, Ludovic Righetti, Gaurav
Sukhatme, and Franziska Meier. Meta learning via learned loss. In 2020 25th International
Conference on Pattern Recognition (ICPR), pp. 4161-4168. IEEE, 2021.
Marc G Bellemare, Will Dabney, and Remi Munos. A distributional perspective on reinforcement
learning. In International Conference on Machine Learning, pp. 449-458. PMLR, 2017.
Johan Bjorck, XiangyU Chen, Christopher De Sa, Carla P Gomes, and Kilian Weinberger. LoW-
precision reinforcement learning: Running soft actor-critic in half precision. In International
Conference on Machine Learning, pp. 980-991. PMLR, 2021.
Johan Bjorck, Carla P Gomes, and Kilian Q Weinberger. ToWards deeper deep reinforcement learn-
ing. arXiv preprint arXiv:2106.01151, 2021.
Stephen Boyd and Lieven Vandenberghe. Convex optimization. Cambridge university press, 2004.
Ronen I Brafman and Moshe Tennenholtz. R-max-a general polynomial time algorithm for near-
optimal reinforcement learning. Journal of Machine Learning Research, 3(Oct):213-231, 2002.
Greg Brockman, Vicki Cheung, LudWig Pettersson, Jonas Schneider, John Schulman, Jie Tang, and
Wojciech Zaremba. Openai gym. arXiv preprint arXiv:1606.01540, 2016.
Richard Y Chen, Szymon Sidor, Pieter Abbeel, and John Schulman. Ucb exploration via q-
ensembles. arXiv preprint arXiv:1706.01502, 2017.
Xinyue Chen, Che Wang, Zijian Zhou, and Keith Ross. Randomized ensembled double q-learning:
Learning fast Without a model. arXiv preprint arXiv:2101.05982, 2021.
10
Under review as a conference paper at ICLR 2022
Kurtland Chua, Roberto Calandra, Rowan McAllister, and Sergey Levine. Deep reinforcement learn-
ing in a handful of trials using probabilistic dynamics models. arXiv preprint arXiv:1805.12114,
2018.
Kamil Ciosek, Quan Vuong, Robert Loftin, and Katja Hofmann. Better exploration with optimistic
actor-critic. arXiv preprint arXiv:1910.12807, 2019.
John D Co-Reyes, Yingjie Miao, Daiyi Peng, Esteban Real, Sergey Levine, Quoc V Le, Honglak
Lee, and Aleksandra Faust. Evolving reinforcement learning algorithms. arXiv preprint
arXiv:2101.03958, 2021.
Will Dabney, Mark Rowland, Marc G Bellemare, and Remi Munos. Distributional reinforcement
learning with quantile regression. In Thirty-Second AAAI Conference on Artificial Intelligence,
2018.
Bo Dai, Albert Shaw, Lihong Li, Lin Xiao, Niao He, Zhen Liu, Jianshu Chen, and Le Song. Sbeed:
Convergent reinforcement learning with nonlinear function approximation. In International Con-
ference on Machine Learning, pp. 1125-1134. PMLR, 2018.
Catherine Forbes, Merran Evans, Nicholas Hastings, and Brian Peacock. Statistical distributions.
John Wiley & Sons, 2011.
Scott Fujimoto, Herke Van Hoof, and David Meger. Addressing function approximation error in
actor-critic methods. arXiv preprint arXiv:1802.09477, 2018.
Tuomas Haarnoja, Aurick Zhou, Pieter Abbeel, and Sergey Levine. Soft actor-critic: Off-
policy maximum entropy deep reinforcement learning with a stochastic actor. arXiv preprint
arXiv:1801.01290, 2018a.
Tuomas Haarnoja, Aurick Zhou, Kristian Hartikainen, George Tucker, Sehoon Ha, Jie Tan, Vikash
Kumar, Henry Zhu, Abhishek Gupta, Pieter Abbeel, et al. Soft actor-critic algorithms and appli-
cations. arXiv preprint arXiv:1812.05905, 2018b.
Hado Hasselt. Double q-learning. Advances in neural information processing systems, 23:2613-
2621, 2010.
Matteo Hessel, Joseph Modayil, Hado Van Hasselt, Tom Schaul, Georg Ostrovski, Will Dabney, Dan
Horgan, Bilal Piot, Mohammad Azar, and David Silver. Rainbow: Combining improvements in
deep reinforcement learning. In Proceedings of the AAAI Conference on Artificial Intelligence,
volume 32, 2018.
Michael Janner, Justin Fu, Marvin Zhang, and Sergey Levine. When to trust your model: Model-
based policy optimization. arXiv preprint arXiv:1906.08253, 2019.
Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint
arXiv:1412.6980, 2014.
Roger Koenker and Kevin F Hallock. Quantile regression. Journal of economic perspectives, 15(4):
143-156, 2001.
Aviral Kumar, Abhishek Gupta, and Sergey Levine. Discor: Corrective feedback in reinforcement
learning via distribution correction. arXiv preprint arXiv:2003.07305, 2020.
Arsenii Kuznetsov, Pavel Shvechikov, Alexander Grishin, and Dmitry Vetrov. Controlling overesti-
mation bias with truncated mixture of continuous distributional quantile critics. In International
Conference on Machine Learning, pp. 5556-5566. PMLR, 2020.
Qingfeng Lan, Yangchen Pan, Alona Fyshe, and Martha White. Maxmin q-learning: Controlling
the estimation bias of q-learning. arXiv preprint arXiv:2002.06487, 2020.
Donghun Lee, Boris Defourny, and Warren B Powell. Bias-corrected q-learning to control max-
operator bias in q-learning. In 2013 IEEE Symposium on Adaptive Dynamic Programming and
Reinforcement Learning (ADPRL), pp. 93-99. IEEE, 2013.
11
Under review as a conference paper at ICLR 2022
Kimin Lee, Michael Laskin, Aravind Srinivas, and Pieter Abbeel. Sunrise: A simple unified frame-
work for ensemble learning in deep reinforcement learning. In International Conference on Ma-
chine Learning, pp. 6131-6141. PMLR, 2021.
Fred C Leone, Lloyd S Nelson, and RB Nottingham. The folded normal distribution. Technometrics,
3(4):543-550, 1961.
Timothy P Lillicrap, Jonathan J Hunt, Alexander Pritzel, Nicolas Heess, Tom Erez, Yuval Tassa,
David Silver, and Daan Wierstra. Continuous control with deep reinforcement learning. arXiv
preprint arXiv:1509.02971, 2015.
A Rupam Mahmood, Dmytro Korenkevych, Gautham Vasan, William Ma, and James Bergstra.
Benchmarking reinforcement learning algorithms on real-world robots. arXiv preprint
arXiv:1809.07731, 2018.
Shie Mannor, Duncan Simester, Peng Sun, and John N Tsitsiklis. Bias and variance approximation
in value function estimates. Management Science, 53(2):308-322, 2007.
Takeru Miyato, Toshiki Kataoka, Masanori Koyama, and Yuichi Yoshida. Spectral normalization
for generative adversarial networks. arXiv preprint arXiv:1802.05957, 2018.
Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Alex Graves, Ioannis Antonoglou, Daan Wier-
stra, and Martin Riedmiller. Playing atari with deep reinforcement learning. arXiv preprint
arXiv:1312.5602, 2013.
Ted Moskovitz, Jack Parker-Holder, Aldo Pacchiano, Michael Arbel, and Michael I. Jordan. Tactical
optimism and pessimism for deep reinforcement learning, 2021.
Junhyuk Oh, Matteo Hessel, Wojciech M Czarnecki, Zhongwen Xu, Hado van Hasselt, Satin-
der Singh, and David Silver. Discovering reinforcement learning algorithms. arXiv preprint
arXiv:2007.08794, 2020.
Ian Osband, Charles Blundell, Alexander Pritzel, and Benjamin Van Roy. Deep exploration via
bootstrapped dqn. Advances in neural information processing systems, 29:4026-4034, 2016.
Mark D Pendrith, Malcolm RK Ryan, et al. Estimator variance in reinforcement learning: Theo-
retical problems and practical solutions. University of New South Wales, School of Computer
Science and Engineering, 1997.
David Silver, Guy Lever, Nicolas Heess, Thomas Degris, Daan Wierstra, and Martin Riedmiller.
Deterministic policy gradient algorithms. 2014.
David Silver, Thomas Hubert, Julian Schrittwieser, Ioannis Antonoglou, Matthew Lai, Arthur Guez,
Marc Lanctot, Laurent Sifre, Dharshan Kumaran, Thore Graepel, et al. Mastering chess and shogi
by self-play with a general reinforcement learning algorithm. arXiv preprint arXiv:1712.01815,
2017.
Richard S Sutton. Dyna, an integrated architecture for learning, planning, and reacting. ACM Sigart
Bulletin, 2(4):160-163, 1991.
Richard S Sutton and Andrew G Barto. Reinforcement learning: An introduction. MIT press, 2018.
Richard S Sutton, David A McAllester, Satinder P Singh, and Yishay Mansour. Policy gradient
methods for reinforcement learning with function approximation. In Advances in neural informa-
tion processing systems, pp. 1057-1063, 2000.
Yuval Tassa, Yotam Doron, Alistair Muldal, Tom Erez, Yazhe Li, Diego de Las Casas, David Bud-
den, Abbas Abdolmaleki, Josh Merel, Andrew Lefrancq, et al. Deepmind control suite. arXiv
preprint arXiv:1801.00690, 2018.
Sebastian Thrun and Anton Schwartz. Issues in using function approximation for reinforcement
learning. In Proceedings of the Fourth Connectionist Models Summer School, pp. 255-263. Hills-
dale, NJ, 1993.
12
Under review as a conference paper at ICLR 2022
Emanuel Todorov, Tom Erez, and Yuval Tassa. Mujoco: A physics engine for model-based control.
In 2012 IEEE/RSJ International Conference on Intelligent Robots and Systems, pp. 5026-5033.
IEEE, 2012.
Hado Van Hasselt, Arthur Guez, and David Silver. Deep reinforcement learning with double q-
learning. In Thirtieth AAAI conference on artificial intelligence, 2016.
Tingwu Wang and Jimmy Ba. Exploring model-based planning with policy networks. arXiv preprint
arXiv:1906.08649, 2019.
Ruibin Xiong, Yunchang Yang, Di He, Kai Zheng, Shuxin Zheng, Chen Xing, Huishuai Zhang,
Yanyan Lan, Liwei Wang, and Tieyan Liu. On layer normalization in the transformer architecture.
In International Conference on Machine Learning, pp. 10524-10533. PMLR, 2020.
Zhongwen Xu, Hado van Hasselt, Matteo Hessel, Junhyuk Oh, Satinder Singh, and David Sil-
ver. Meta-gradient reinforcement learning with an objective discovered online. arXiv preprint
arXiv:2007.08433, 2020.
Denis Yarats, Rob Fergus, Alessandro Lazaric, and Lerrel Pinto. Mastering visual continuous con-
trol: Improved data-augmented reinforcement learning. arXiv preprint arXiv:2107.09645, 2021a.
Denis Yarats, Ilya Kostrikov, and Rob Fergus. Image augmentation is all you need: Regularizing
deep reinforcement learning from pixels. In International Conference on Learning Representa-
tions, 2021b. URL https://openreview.net/forum?id=GY6-6sTvGaf.
Zongzhang Zhang, Zhiyuan Pan, and Mykel J Kochenderfer. Weighted double q-learning. In IJCAI,
pp. 3455-3461, 2017.
13
Under review as a conference paper at ICLR 2022
Appendix
A	Penalties for bias counteraction
Generalized Pessimism Learning makes use of the uncertainty regularizer penalty to counteract bi-
ases arising from the off-policy RL optimization process. Here, we further analyze some of the
properties and relationships of our penalty with alternatives from prior works. We will consider
the common case where the critic is parameterized as an ensemble of N ≥ 2 action-value functions
{Qπφ }iN=1. To simplify our analysis, we assume that the different action-value predictions for a given
input are independently drawn from a Gaussian distribution with some variance σQ2 (s,a) . We moti-
vate this latter assumption by the Central Limit Theorem, given the many sources of stochasticity
affecting the training process of each action-value model in the ensemble. To simplify our notation,
we will drop input dependencies when doing so will not compromise clarity, e.g., σQ2 (sa) ⇒ σQ2 .
A. 1 Expected penalization of the uncertainty regularizer
The uncertainty regularizer penalty pβ (s, a, φ, θ) is parameterized as a weighted model of epistemic
uncertainty. Particularly, we propose to measure epistemic uncertainty with the expected Wasser-
stein distance between the return distributions predicted by the critic. This is formally described
in Eqn. 8 from Section 5. In the common case where we parameterize the critic with an ensemble
of action-value functions, we treat each as a Dirac delta function approximation of the return dis-
tribution. Consequently, we can estimate the expected Wasserstein distance by averaging over all
the N2 - N absolute differences between different Dirac locations predicted by the action-value
functions:
Pe(s,a, φ, θ) = β × Ea〜∏θ(s),φι ,φ2 [^^(Zφι (S, a), Zφ2 (S，a))]
=β × Ea 〜∏Θ(S),Φ1,Φ2 [∣Qφι (S,a) - Qφ (s,a)∣]
β NN
≈ N2β:N XX ∣Q∏i (s,a)-Q∏j (s,a)∣.
i=1 j 6=i
(12)
From the independence assumption of the ensemble models, the differences in the action-value
predictions will also follow a Gaussian distribution:
Dij= Qni(s, a) - Qnj (s, a)〜N(0, 2σQ).	(13)
Each absolute difference Wij = |Dij | will then follow a folded normal distribution (Leone et al.,
1961), with moments given by:
√2	2	2	2	4∖ 2
E[W] = μw =	~∏σD = ∏σQ,	Var(W)	=	σw	= σD -	μw	= ( 2 -	∏2	) σQ.	(14)
Consequently, the expected penalization of the uncertainty regularizer immediately follows:
E[Pβ ] = βμw = 2βσQ.	(15)
π
A.2 Expected penalization of the population standard deviation
Ciosek et al. (2019) and Moskovitz et al. (2021) make use of a weighted model of an alternative
epistemic uncertainty measure to define a penalty as psβ (S, a, φ, θ) = β × S(S, a, φ, θ). In particu-
lar, they make use of the population standard deviation S(S, a, φ, θ), treating the critic’s ensemble
predictions as independently sampled action-values:
1N
where μQΦ = NfQΦi∙
N i=1
(16)
14
Under review as a conference paper at ICLR 2022
We can rewrite the population standard deviation measure in terms of the square root of the sum of
N squared difference terms Di :
S = UX D2, where Di = √1N (QΦi- 〃q。).
(17)
Moreover, we can rewrite each difference term as a weighted sum of the N different ensemble
predictions:
Di = √1N (Qni- NF X Qni) = √1N (NN1 Qni- X QNj) .	(18)
From the independence assumption of the ensemble models, it follows that each Di is also a Gaus-
sian random variable with moments given by the basic properties for the sums of independent ran-
dom variables:
E[D] = nd = 0,	Var(D) = σD = N ((NN21) σQ + NN-IσQ) = NN-IσQ.	(19)
Hence, by simply scaling the population standard deviation, we can express it as a sum of standard
normal random variables:
N S = IX( N
σQ√N - 1 N i=1 Vq√N - 1
(20)
where Z 〜N(0,1).
This enables to relate S to a Chi distribution (Forbes et al., 2011) with parameter N:
N
S 〜XN.
σQ√N-1	心
Consequently, the expected value of psβ can be obtained from scaling the expected value of a Chi
distribution:
E[pβ] = βE[s] = β√N-1 E[XN]σQ = B√NN- × ^NNPy.	(21)
N	N γ m
Comparing the expected value of this penalty with the expected value of the uncertainty regularizer
penalty from Eqn. 15, we note a few key facts. Both quantities are linearly proportional to the stan-
dard deviation of the action-value predictions σQ . However, for the population standard deviation
penalty, the scale of the proportionality is dependent on the number of action-value models used to
parameterize the critic. Hence, unlike for the uncertainty regularizer penalty, the critic’s parame-
terization directly affects the expected penalization magnitude and should influence design choices
regarding the parameter β .
A.3 Relationship with clipped double q-learning
We can rewrite the ubiquitous clipped double Q-learning penalization practice (Fujimoto et al., 2018)
using the uncertainty regularizer penalty. In particular, we can specify the clipped double Q-learning
action-value targets from Eqn. 6 in terms of the difference between the mean action-value prediction
and a penalty:
Qφmin (s, a)π = min (Qφ1 (s,a),Qφ2 (s, a)) = Qφ(s,a) - Pmin(S, a, φ, θ),
1N	1
where Qn = NEQφi ,	Pmin = min (QnI ,Q∏2 ) - Qφ = 2 lQφι - Qφ2 |.
i=1
(22)
15
Under review as a conference paper at ICLR 2022
Similarly, for N = 2 our uncertainty regularizer reduces to:
Pe = 2 (IQφι - Qφ21 + IQφ2 - Qφι∣) = βIQφι - Qφ2∣.	(23)
Thus, pmin = pβ for β = 0.5. As shown earlier in this section, the expected magnitude of the
uncertainty regularizer penalty is not dependent on the number of action-value functions. Hence,
our penalty allows us to extend the expected regularization induced by clipped double Q-learning
for N > 2, simply fixing β = 0.5. This would not be possible using the population standard
deviation penalty since its expected magnitude is dependent on the number of action-value functions
employed.
16
Under review as a conference paper at ICLR 2022
Table 2: Hyper-parameters used for GPL-SAC
SAC hyper-parameters
Replay data buffer size	1000000
Batch size	256
Minimum data before training	5000
Random exploration steps	5000
Optimizer	Adam (Kingma & Ba, 2014)
Policy/critic learning rate	0.0003
Policy/critic β1	0.9
Critic UTD ratio	20
Policy UTD ratio	1
Discount γ	0.99
Polyak coefficient ρ	0.995
Hidden dimensionality	256
Nonlinearity	ReLU
Initial entropy coefficient α	1
Entropy coefficient learning rate	0.0001
Entropy coefficient β1	0.5
Policy target entropy HH	Hopper : -1, HalfCheetah: -3, Walker2d: -3, Ant: -4, Humanoid: -2
GPL hyper-parameters
Initial uncertainty regularizer weight β 0.5
Uncertainty regularizer learning rate	0.1
Uncertainty regularizer β1	0.5
B Algorithmic and experimental specifications
In this section, we provide details regarding the experimental results from the main text. For further
information about our efficient implementation, please, refer to the shared code.
B.1	Empirical bias estimation
In Section 5, we record the evolution of the target action-value bias throughout the RL process
by running experiments with two different extensions to the SAC algorithm on three OpenAI Gym
environments. The first extension makes use of unpenalized optimistic target action-values, while
the second extension makes use of a penalty with a magnitude equivalent to the one induced by
the clipped double Q-learning targets. Both are implemented through the uncertainty regularizer
with a fixed β, as derived in Appendix A. Moreover, both extensions use an update-to-data ratio of
twenty and N = 4 action-value models to parameterize the critic. The rest of the hyper-parameters
follow our implementation of GPL-SAC. We record estimates of the action-value bias by collecting
transitions from ten evaluation episodes every 1000 environment steps. We obtain each estimate by
taking the average difference between the observed discounted returns in the evaluation episodes
and the discounted returns from the critic’s predictions. In particular, we correct the critic’s raw
predictions by subtracting the discounted log probabilities of the performed actions, accounting for
SAC’s modified action-value objective.
B.2	Proprioceptive observations experiments
Hyper-parameters. We provide a list of the utilized hyper-parameters for GPL-SAC in Table 2. The
majority of the chosen hyper-parameters follow closely the seminal SAC papers (Haarnoja et al.,
2018a;b), with some exceptions to improve performance and stability. For instance, consistently
with Chen et al. (2021), we employ an ensemble of ten action-value models and increase the update-
to-data ratio to twenty. Additionally, we employ a simplified version of the modern architecture
from Bjorck et al. (2021) to parameterize the action-value models, as depicted in Figure 5. This
17
Under review as a conference paper at ICLR 2022
Figure 5: Schematic representation of the modern model architecture used to parameterize the
action-value functions in our implementation of GPL-SAC. We make use of a single hidden residual
block where the fully-connected layers are regularized via spectral normalization.
architecture follows many of the practices introduced by recent work to stabilize transformer training
(Xiong et al., 2020). Specifically, we employ a single hidden residual block with layer normalization
(Ba et al., 2016) followed by two fully-connected layers regularized with spectral normalization
(Miyato et al., 2018). Throughout all architectures, we keep the hidden dimensionality fixed to
256. We initialize the uncertainty regularizer with β = 0.5 to reflect the penalization magnitude of
clipped double Q-learning. Following Haarnoja et al. (2018b), the entropy coefficient α is adaptively
updated at each training iteration based on a policy target entropy H . In particular, this follows a
dual optimization procedure to keep the estimated average policy entropy close to H , as described
in Line 17 of Algorithm 1. We utilize increased values of H to optimize α, following the choices of
Janner et al. (2019). We learn β using dual TD-learning with the same optimizer used for adjusting
the value of α.
Baseline results. For the continuous control experiments from proprioceptive states, we employ
different baselines to ground our results and provide a comparison with current state-of-the-art al-
gorithms. The reported results for REDQ come from re-running Chen et al. (2021)’s original im-
plementation with the provided hyper-parameters. The reported results for MBPO come instead
from the original paper, as publicly shared by Janner et al. (2019). The reported results for SAC-
20 come from running the same base implementation as GPL-SAC, with a few differences in the
listed hyper-parameters. Namely, SAC-20 uses an ensemble of N = 2 action-value models with the
classical 3-layer fully-connected architecture from Haarnoja et al. (2018b) and uses an uncertainty
regularizer penalty with a fixed parameter β = 0.5.
B.3	Pixel observations experiments
Hyper-parameters. We provide a list of the utilized hyper-parameters for GPL-DrQ in Table 3.
All the hyper-parameters shared with DrQv2 follow the values provided by Yarats et al. (2021a),
while the uncertainty regularizer and optimizer for β follow the same specifications as in GPL-SAC.
Additionally, GPL-DRQ-Expl+ linearly decays the optimistic shift value λopt from 0.5 down to 0.0
with the same frequency as the exploration noise’s standard deviation.
18
Under review as a conference paper at ICLR 2022
Table 3: Hyper-parameters used for GPL-DrQ
DrQv2 hyper-parameters	
Replay data buffer size	1000000 (100000 for Quadruped run)
Batch size	256 (512 for Walker run)
Minimum data before training	4000
Random exploration steps	2000
Optimizer	Adam (Kingma & Ba, 2014)
Policy/critic learning rate	0.0001
Policy/critic β1	0.9
Critic UTD ratio	0.5
Policy UTD ratio	0.5
Discount γ	0.99
Polyak coefficient ρ	0.99
N-step returns	3 (1 for Walker run)
Hidden dimensionality	1024
Feature dimensionality	50
Nonlinearity	ReLU
Initial entropy coefficient α	1
Exploration stddev. clip	0.3
Exploration stddev. schedule	linear: 1 → 0.1 in 500000 steps
GPL hyper-parameters
Initial uncertainty regularizer weight β 0.5
Uncertainty regularizer learning rate 0.1
Uncertainty regularizer β1	0.5
Pessimism annealing hyper-parameters
Optimistic shift value λopt schedule linear: 0.5 → 0 in 500000 steps
Baseline results. For the experiments on the DeepMind Control Suite from pixel observations,
we compare our extensions with the base DrQv2 algorithm. Since the provided results for DrQv2
had inconsistent numbers of repetitions, we recollected the results by running the experiments with
Yarats et al. (2021a)’s original implementation. However, in our evaluation, we observed higher
variances in the performance for some of the considered tasks than what Yarats et al. (2021a) re-
ported. We shared this inconsistency with DrQv2’s authors, and they confirmed the validity of our
empirical findings after recollecting the results themselves.
19
Under review as a conference paper at ICLR 2022
Figure 6: Performance curves for the environments of the DeepMind Control Suite. We report both
mean and standard deviation of the episodic returns over five random seeds.
C Main performance results
C.1	Openai gym results after 100k experience steps
In Table 4 we compare the performances of the examined algorithms after the milestone of col-
lecting 100K environment steps. We believe this to be an appropriate comparison point for recent
algorithms, given their relative sample-efficiency improvements in the OpenAI Gym tasks. For each
run, we record the average returns from 25 evaluation episodes collected in the preceding 5000 en-
vironment steps. These results highlight once again the superior sample efficiency of GPL-SAC,
pushing the performance boundary of off-policy methods. GPL-SAC is also more consistent than
the considered baselines in most tasks, as shown by the lower relative standard deviations. As men-
tioned in Section 6 of the main text, we attribute the performance gap mainly to the ability of GPL
to prevent the accumulation of overestimation bias without requiring overly pessimistic targets. In
particular, the principal identified downsides of overly pessimistic targets are two-fold. Firstly, they
slow down reward propagation and introduce further errors in the TD-targets, potentially leading to
suboptimal convergence of the relative action-value functions (Kumar et al., 2020). Secondly, they
20
Under review as a conference paper at ICLR 2022
Table 4: Results for the proprioceptive observations experiments on the OpenAI Gym suite after
collecting 100K experience steps
Algorithm / Task	SAC-20	REDQ	MBPO	GPL-SAC (Ours)
Hopper-v2	2694 ± 902	3007 ± 471	3262 ± 197	3386 ± 92
Halfcheetah-v2	5822 ± 728	5625 ± 431	9501 ± 331	9685 ± 658
Walker2d-v2	2101 ± 876	1937 ± 968	3377 ± 529	3662 ± 360
Ant-v2	482 ± 101	3160 ± 1213	1624 ± 447	4933 ± 333
Humanoid-v2	493 ± 94	1460 ± 689	555 ± 61	5155 ± 191
Normalized average	0.35 ± 0.28	0.48 ± 0.24	0.52 ± 0.30	0.81 ± 0.12
induce overly conservative policies that prefer low-uncertainty behavior, hindering exploration in
stochastic environments (Ciosek et al., 2019).
C.2 Deepmind control suite performance curves
In Figure 6 we provide the performance curves for the pixel-based medium benchmark tasks from
the DeepMind Control Suite, as specified by Yarats et al. (2021a). These visualizations complement
the results summary provided in Section 6 and further highlight the effectiveness of GPL and the
proposed pessimism annealing procedure. In accordance with our earlier analysis, our methods
yield improved performance and robustness in the harder exploration environments, where overly
pessimistic action-value targets are expectedly more detrimental.
21
Under review as a conference paper at ICLR 2022
Figure 7: Analysis of the uncertainty regularizer parameter. We consider three versions of GPL-
SAC with different initial values for β . We show the evolution of β throughout optimization in each
experiment (Top). We also provide the relative performance curves for each initial setting (Bottom).
D	Extended empirical analysis
We provide an extended empirical analysis of Generalized Pessimism Learning. In particular, we
study the effects of different components and parameters on the stability and efficiency of our GPL-
SAC algorithm. We focus our analysis on a representative subset of the considered OpenAI Gym
tasks. We follow the same experimental protocol as described in Section 6 of the main text.
D.1 Uncertainty regularizer analysis
In all main experiments, we initialize the uncertainty regularizer with β = 0.5. This setting yields the
same initial expected regularization magnitude as the ubiquitous clipped double Q-learning method.
Hence, to better understand the optimization properties of GPL, we consider instantiating GPL-
SAC with three different initial values for β, namely, β = 0.25, β = 0.5, and β = 0.75. For
these settings, we record both the evolution of β throughout learning and the performance of the
resulting algorithms. This analysis aims to evaluate the sensitivity of GPL to the initial value of
β and the effectiveness of the dual TD-learning procedure. In particular, this simple experimental
setting should elucidate some of the properties of the actual implementation of GPL, where the initial
unbiasedness assumption used to motivate dual TD-learning does not necessarily hold. Moreover,
the learned values of β should provide insights regarding the bias arising from interactions of off-
policy reinforcement learning methods and the different examined tasks.
Parameter evolution. In Figure 7 (Top), we show the value of β collected throughout training in
each experiment with the examined initial settings. For each task, β appears to follow a recognizable
trend of convergence towards some particular distinct range of values. This range appears to be
influenced by the environment’s complexity, with β converging to lower values for Walker2d and
increasingly higher values for Ant and Humanoid, respectively. However, β appears to adapt rather
slowly, seldom reaching stability for distant initialization values in the examined experience regime.
Our intuition is that this phenomenon is mainly due to two characteristics of pessimism learning.
The first characteristic is that bias in the target action-value predictions can arise simply due to
the stochasticity of the RL process dynamics for any value of β . Hence, there will always be a
stochastic component from the dual-TD learning signal introducing noise to β ’s optimization. The
second characteristic is that, in practice, dual TD-learning occurs at a slower rate than TD-learning
itself. Hence, as discussed in Section 5, given an imperfect initialization of β, part of the target bias
might leak into the online action-value models in the first iterations of training. While the iterative
22
Under review as a conference paper at ICLR 2022
Figure 8: Performance curves showing the effects of optimizing the uncertainty regularizer with
alternative strategies in GPL-SAC. We consider learning β to minimize the estimated bias end-to-end
and maximize immediate returns improvements via a bandit-based optimization from prior work.
interactions between dual TD-learning and TD-learning still pushes the value of β to mitigate bias,
consistently with our inituition, training takes longer to reach expectedly unbiased targets from a
dimmed-down signal in the ‘unbiasing’ learning direction.
Performance results. In Figure 7 (Bottom), we show the performance curves for the examined
initial settings. While GPL-SAC recovers strong performance for most tested initializations, we can
see robustness improve when we initialize β closer to its convergence range. Specifically, β = 0.75
appears to work best for Humanoid, β = 0.5 for Ant, and β = 0.25 for Walker. These results provide
further evidence that one of the main factors determining optimal pessimism is task complexity,
with harder tasks requiring more conservative targets for better stabilization. Thus, these results
further highlight the importance of an adaptive strategy for off-policy RL to achieve proper bias
counteraction in arbitrary environments.
D.2 Alternative uncertainty regularizer optimizations
To evaluate the relative empirical effectiveness of dual TD-learning, we implement and test two al-
ternative optimization strategies for the uncertainty regularizer pβ . In particular, we replace the dual
TD-learning procedure in GPL-SAC with each strategy and compare the resulting performances.
GPL-SAC-E2E. First, we consider learning β to minimize end-to-end the squared norm of the ex-
pected target action-value bias approximation from Eqn. 10. This optimization strategy could be
seen as a more direct approach than dual TD-learning, resulting in the following optimization ob-
jective:
arg min Es,a,s B(s,a,s0)2 , where
β	(24)
B(s,a,s0) = r + YEaO〜∏3)[Qφo(s0,a0)- pβ(s0,a0,φ,θ)] - Qφ(s,a).
In practice, we optimize this alternative objective in place of dual TD-learning with standard gradient
descent. We name the resulting algorithm GPL-SAC-E2E.
GPL-SAC-TOP. We also consider the alternative optimization procedure from the Tactical Optimism
and Pessimism (TOP) algorithm (Moskovitz et al., 2021). In particular, this optimization involves
learning an adaptive binary controller that switches on or off a bias correction penalty based on
the sample standard deviation of the critic’s action-value estimates. The TOP algorithm learns this
controller as a multi-armed bandit problem, using the difference in consecutive episodic returns as
feedback. We transpose this framework to GPL by optimizing β over a discrete choice of two pos-
Sible values, β ∈ {0, √z2}. With these values, the uncertainty regularizer yields the same expected
penalization as the optimistic and pessimistic modes of TOP, respectively. TOP’s authors selected
these penalization levels from a vast choice of settings evaluated on the OpenAI Gym tasks. We
name the resulting algorithm GPL-SAC-TOP.
Results. In Figure 8 we show the performance curves for the evaluated alternative optimization
procedures as compared to the original dual TD-learning in GPL-SAC:
23
Under review as a conference paper at ICLR 2022
Figure 9: Performance curves showing the effects of applying pessimism annealing to GPL-SAC for
the considered OpenAI Gym tasks.
GPL-SAC-E2E optimizes the uncertainty regularizer too aggressively, leading to instabilities and
suboptimal performance across all tasks. We motivate these results based on two related inconsis-
tencies of end-to-end bias minimization from Eqn. 24. Firstly, this alternative optimization does
not consider the effects of the target action-value bias on the changes in the action distribution used
for bootstrapping. In particular, modifying the uncertainty regularizer will affect the objective of
the policy’s optimization from Eqn. 4 and the corresponding distribution of on-policy actions. Sec-
ondly, even considering a fixed policy, this end-to-end strategy assumes that β linearly affects the
bias. However, this is not the case due to the non-linear compounding effects from the bootstrapping
in the TD-recursions. Hence, as empirically confirmed by the superior performance, framing bias
minimization as a dual problem is a more general and appropriate formulation.
On the other hand, GPL-SAC-TOP matches the performance of dual TD-learning on the Humanoid
task but severely underperforms on the other two tasks. For Humanoid, TOP’s optimization strategy
quickly converges to sampling β = √z2 consistently, as β = 0 causes considerable instabilities. AS
observed in Appendix D.1, this task benefits from highly penalized targets for stabilization, making
this early convergence to a high β effective. However, both Walker and Ant tasks benefit from mod-
erate levels of pessimism, which the bandit optimization strategy fails to recover in the examined
experience regime. This inefficiency stems from the fact that immediate episodic return improve-
ment does not appear to correlate with overall learning efficiency for these tasks. Moreover, the
slow update frequency and noisy nature of the controller feedback make GPL-SAC-TOP unable to
dynamically address arising biases. These results further show the effectiveness of dual TD-learning
to directly minimize a source of learning instability and enable consistently efficient learning.
D.3 Pessimism annealing for openai gym
We experiment with applying pessimism annealing to GPL-SAC for the OpenAI Gym tasks. In the
same spirit as our previously described application for pixel observations tasks, we linearly decay
λopt from 0.5 to 0.0 in the first 50000 environment steps. We leave the rest of the hyper-parameters
unaltered. We name the resulting algorithm GPL-SAC-Expl+.
Results. In Figure 9 we provide the performance curves comparing GPL-SAC with GPL-SAC-
Expl+. The shifted uncertainty regularizer marginally improves performance in only two out of three
environments. These results indicate that the undirected Gaussian exploration from SAC combined
with the unbiased targets from GPL are already sufficient to ensure effective exploration in the
OpenAI Gym tasks. In contrast, pessimism annealing appears to have a more significant effect in
our results for the DeepMind Control Suite from Section 6 of the main text, highlighting the harder
exploration challenge introduced by these pixel-based tasks.
D.4 Ensemble size
In the ablation study from Section 6 of the main text, the increased number of action-value models in
the critic’s ensemble appeared to have the greatest overall effect on GPL-SAC’s performance. This
parameter directly influences the ability to capture the critic’s epistemic uncertainty, thus, affect-
ing both the accuracy of the action-value predictions and the variance in the Wasserstein distance
24
Under review as a conference paper at ICLR 2022
mn+J①,I UlPOSldW
Figure 10: Performance curves showing the effects of varying the ensemble size of the critic in
GPL-SAC.
Figure 11: Performance curves showing the effects of lowering the critic’s update-to-data ratio in
GPL-SAC.
estimation for the uncertainty regularizer penalty. Therefore, we extend our empirical analysis by
testing GPL-SAC with broader numbers of action-value models to understand the relevance of this
parameter and the scalability of our algorithm. Namely, we compare instances of GPL-SAC with
N = 2, N = 5, N = 10, N = 15, and N = 20 action-value models.
Results. In Figure 10 we provide the performance curves for the different ensemble sizes. Pre-
dictably, both learning efficiency and final performance monotonically improve with the ensemble
size. Yet, these improvements appear to saturate at different values of N based on the complexity of
the underlying environment, showing how harder tasks increasingly rely on accurately representing
epistemic uncertainty. In particular, for Walker2d training with only N = 2 action-value models
yields very similar results to training with N = 20 action-value models. However, the performance
differences are increasingly noticeable for the other tasks that involve significantly larger state and
action spaces. For instance, for Humanoid the agent is not able to recover meaningful behavior with
N = 2 and performance saturates only around training with N = 10.
D.5 Update-to-data ratio
Following the examined prior state-of-the-art algorithms, GPL-SAC uses a critic UTD ratio of 20
across the different OpenAI Gym tasks. This aggressive optimization frequency ensures that the
critic encapsulates most information from experience collected at any given point in the RL pro-
cess. However, since the UTD ratio affects training time almost linearly, we examine its impact on
performance. In particular, we compare instances of GPL-SAC with UTD ratios of 1, 5, 10, and 20.
Results. In Figure 11 we provide the performance curves for the different examined UTD ratios.
Larger UTD ratios yield clear sample efficiency improvements for all tasks. Moreover, using a UTD
ratio of 1 in the Ant and Humanoid environments appears to cause some learning instabilities towards
the end of the examined experience regime. These instabilities are likely from the inability of the
learning process to quickly incorporate new information from recently collected data into the critic,
given the growing size of the replay buffer. Consequently, a slow learning process might produce
25
Under review as a conference paper at ICLR 2022
Figure 12: Performance curves showing the effects of employing a more expressive distributional
critic based on quantile regression in GPL-SAC.
inaccurate return predictions for on-policy behavior, leading to a noisy policy gradient signal and,
therefore, a degradation of the policy itself.
D.6 Distributional critic
The uncertainty regularizer penalty is a function of the expected Wasserstein distance between the
return distributions predicted by the critic. Hence, it has a natural generalization to more expressive
distributional critics beyond ensembles of action-value functions. Therefore, we evaluate the effec-
tiveness of extending GPL-SAC by parameterizing the critic using an ensemble of N distributional
models {Zφπ }iN=1. Following Dabney et al. (2018), we let each model output M quantile locations
values parameterizing a quantile distribution. We consider both 1-Wasserstein and 2-Wasserstein
distances for the uncertainty regularizer (which for Dirac delta functions are equivalent). For quan-
tile distributions, these distance measures are calculated as follows:
M1
Wi (Zni (s,a), Znj (s,a)) = ∑ MIZni (s,a)k - Znj (s, a)k I,
W2(Zφπi(s,a),Zφπj(s,a))= t
k=1
M ι	2
X M(ZΦi (s,a)k- Znj(S，a)k).
k=1
(25)
We propose applying the uncertainty regularizer to each quantile location to compute the target
return distribution, Zn (s, a) ∈ RM, as:
Zn (s,a)j = Zφo (s,a)j - Pe (s0,a0,φ,θ), for all j ∈ {1, 2 ...M}.	(26)
We optimize the critic’s models with a distributional version of TD-learning (Bellemare et al., 2017)
making use of Huber quantile regression (Koenker & Hallock, 2001). We still optimize the policy
to maximize the expected returns by averaging over the quantile locations of the target return dis-
tribution. Moreover, we still perform dual TD-learning with minimal overheads by averaging all
quantile errors computed for quantile regression during the distributional TD-learning updates. In
our implementation, we approximate the return distribution with M = 10 quantile locations and set
the Huber quantile regression coefficient to κ = 1. We keep all other hyper-parameters and model
architectures consistent with GPL-SAC. We name the resulting algorithms GPL-QR-SAC (W1/W2).
Results. In Figure 12 we provide the performance curves comparing both versions of GPL-QR-SAC
with the original GPL-SAC algorithm. GPL-QR-SAC (W1) appears to outperform GPL-QR-SAC
(W2), indicating that the 1-Wasserstein distance is a more effective and stable measure of epistemic
uncertainty for pβ. However, in the examined tasks, GPL-QR-SAC (W1) performs similarly to
GPL-SAC, meaning there are no major benefits in parameterizing the critic with more expressive
approximations to the return distribution. Yet, these results could also indicate that the returns for
the considered OpenAI Gym environments have low stochasticity or that the action-value models
already encapsulate all learnable information in the considered training regime. We leave further
experimentation and analysis of using GPL with distributional critics for future work.
26
Under review as a conference paper at ICLR 2022
Table 5: Average training times for the tested algorithms and ablations
Proprioceptive observations tasks	Training time (seconds/1000 env. steps)
GPL-SAC (N = 10, UTD = 20)	76.1
No pessimism learning	74.2
No modern action-value model	51.6
GPL-SAC-Expl+	77.0
GPL-QR-SAC (W1)	86.6
GPL-QR-SAC (W2)	88.1
N = 20	85.0
N = 15	81.2
N=5	62.5
N=2	38.8
UTD = 10	41.4
UTD = 5	20.1
UTD = 1	6.3
REDQ (Original implementation)	183.8
Pixel observations tasks	Training time (seconds/10000 env. steps)
GPL-DrQ-Expl+	112.2
GPL-DrQ	111.3
DrQv2	111.2
D.7 Computational scaling
We analyze the computational scaling of our implementations of Generalized Pessimism Learn-
ing-based algorithms. Particularly, we record the average training time of executing the different
considered algorithms for either 1000 environment steps for OpenAI Gym tasks or 10000 environ-
ment steps for DeepMind Control Suite tasks. We run each algorithm on an NVIDIA RTX 3090
GPU and an AMD Ryzen Threadripper 3970x CPU. As described in Section 6 of the main text, our
implementation groups the parameters of the different models in the critic’s ensemble into a single
network to achieve better scaling for inference and backpropagation with distributed hardware.
Training times. We report all average training times in Table 5. Comparing the training times with
and without dual TD-learning and pessimism annealing clearly shows that both procedures introduce
trivial computational overheads. Since the critic’s updates represent the bulk of the computation in
off-policy RL, modifying the associated hyper-parameters and parameterizations has relevant effects
on training times. In particular, using a classical action-value model architecture speeds up training
time by approximately 30%, while using a distributional critic slows down training by approxi-
mately 20%. Thanks to our implementation and the efficiency of large tensor operations, increasing
the number of action-value models affects training times sub-linearly. For instance, doubling the
critic’s ensemble size to 20 results in less than 12% additional training overhead. On the other hand,
increasing the UTD ratio increases the total training time almost linearly. As compared to the origi-
nal REDQ implementation from Chen et al. (2021), GPL-SAC trains in less than half the time while
having similar algorithmic complexity.
Considerations. The results from this Section can provide intuitions on selecting components and
hyper-parameters when applying our implementation of Generalized Pessimism Learning for dif-
ferent problems. For instance, increasing ensemble size beyond N = 10 appears to affect training
times only marginally. Thus, indicating that a viable criterion for selecting N in practical scenarios
could be based on how many models fit into GPU memory. Multiple directions can be explored
to further improve the time-efficiency and scalability of GPL. Since our uncertainty regularizer is
compatible with variational representations of the critic’s parameters posterior, Bayesian parame-
terizations could be a viable option to capture epistemic uncertainty more efficiently than model
ensembles. Moreover, recent work by Bjorck et al. (2021) shows that it is possible to train mod-
ern reinforcement learning algorithms in half precision, with non-trivial computation and memory
benefits.
27