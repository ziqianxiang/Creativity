Under review as a conference paper at ICLR 2022
Federated Distillation of Natural Language
Understanding with Confident Sinkhorns
Anonymous authors
Paper under double-blind review
Ab stract
Enhancing the user experience is an essential task for application service
providers. For instance, two users living wide apart may have different tastes of
food. A food recommender mobile application installed on an edge device might
want to learn from user feedback (reviews) to satisfy the client’s needs pertain-
ing to distinct domains. Retrieving user data comes at the cost of privacy while
asking for model parameters trained on a user device becomes space inefficient
at a large scale. In this work, we propose an approach to learn a central (global)
model from the federation of (local) models which are trained on user-devices,
without disclosing the local data or model parameters to the server. We propose a
federation mechanism for the problems with natural similarity metric between the
labels which commonly appear in natural language understanding (NLU) tasks.
To learn the global model, the objective is to minimize the optimal transport cost
of the global model’s predictions from the confident sum of soft-targets assigned
by local models. The confidence (a model weighting scheme) score of a model is
defined as the L2 distance of a model’s prediction from its probability bias. The
method improves the global model’s performance over the baseline designed on
three NLU tasks with intrinsic label space semantics, i.e., fine-grained sentiment
analysis, emotion recognition in conversation, and natural language inference.
1	Introduction
Due to recent technological advancements, more than two-thirds of the world’s population has access
to mobile phones1. A client application on these user devices has access to the unprecedented
amount of data obtained from user-device interactions, sensors, etc. Learning algorithms can use
this data to provide enhances user-experience. However, directly accessing this data comes at the
cost of risking user privacy (Jeong et al., 2018).
To mitigate the issue, federated learning (FL) (shown in fig. 1) is a mechanism that retrieves the
parameters of the (local) user-specific model and performs federation of knowledge either by distil-
Iation or merging the models (Konecny et al., 2θl6; McMahan et al., 2017) The algorithms aim to
learn a domain-generalized central (global) model. The classic FL algorithms such as federated av-
eraging and its adaptations are based on averaging of local model parameters, and thus only applied
when the client models posses the similar network architectures (Mohri et al., 2019; Li et al., 2019a)
However, FL paradigm has critical limitations of being costly in terms of communication load with
the increase in local model sizes and demands all the participating models to have same architecture
(homogenity) (Jeong et al., 2018; Lin et al., 2020).
Federated distillation (FD) proposes to exchange only the outputs of the local model, i.e, logits or
probability measures whose dimensions are usually much smaller than the size of models themselves
(Jeong et al., 2018). Therefore, FD allows to learn from an ensemble of different local models
of dissimilar configurations at reduced user-privacy risk, low communication overhead, and less
memory space utilisation. However, entropy-based losses do not allow one to define metric structure
in the label space. This can be critical when the task possesses a natural similarity relationship
between the output labels.
1https://datareportal.com/global-digital-overview
1
Under review as a conference paper at ICLR 2022
Generally, FD algorithms adopts local model’s ensemble knowledge distillation to global model us-
ing KullbaCk-Leibler (KL) divergence (Gou et al., 2021; Lin et al., 2020) or cross-entropy based
losses (Jeong et al., 2018) as they are easy to compute and facilitate smooth backpropagation (Mur-
phy, 2012). However, one critical limitation of such entropy-based losses is that they do not allow
one to define metric structure in the label space. During global model ensemble training, leverag-
ing such semantic structure can be useful when the task possesses a natural similarity relationship
between the output labels. For instance, in the task of fine-grained sentiment classification of a
text, strongly positive sentiment is closer to positive while far from strongly negative. Contrary to
information-theoretic losses, Optimal Transport (OT) admits prior relationship in the label space
(Frogner et al., 2015). The major contributions of this work are:
Contribution:1 For the tasks with intrinsic label-space semantics, we propose a better federated
(ensemble) distillation of local models to learn global model by encoding inter-label relationships
using optimal transport.
The user-specific local data is potentially non-independent and non-identically distributed (non-IID)
(Tong et al., 2020). Thus, the local models are prone to acquire biases such as population bias: the
local user may not represent the target overall population; and sampling bias: knowledge transfer
from a local model may not be useful to general over larger group (Mehrabi et al., 2019).
Contribution:2 To minimize the effect of intrinsic probability bias arising from user-centric (non-
IID) training of the local models, we introduce an L2 distance-based weighted distillation.
As shown in fig. 1, the global model Mg aims to minimize the weighted sum of distance D1, D2, D3
with weights w1 , w2, w3, respectively. Contribution:1 provides a better distance calculation while
Contribution:2 presents a better weighting mechanism.
To further support our contributions, we derive the Lipschitz constant and Rademacher complexity-
based generalisation bounds of the unregularized 1-Wasserstein based confident ensemble distilla-
tion. In the end, we empirically demonstrate the strong performance, generally improving upon
the baselines, on the three natural language understanding (NLU) tasks, i.e., fine-grained sentiment
analysis, emotion recognition in conversation, and natural language inference.
Figure 1: Local models acting as multiple teachers trained locally on user data while the global
model acts as a student. The global model can only observe predictions of local models.
2	Related work
There have been many approaches to FL such as local model parameter averaging (McMahan et al.,
2017; Lin et al., 2018) based on local SGD updates. It requires global and local models should
have the same model architecture. Another line of work is multiple-source adaptation formulations
where a learner has access to source domain-specific predictor and no access to the labeling data
from these domains. The expected loss is the mixture of source domains (Hoffman et al., 2018).
Even though the formulation is close, our solutions are different as we do not have access to the
local or global data domain distribution. In natural language processing, (Hilmkil et al., 2021; Lin
2
Under review as a conference paper at ICLR 2022
et al., 2020; Liu & Miller, 2020) fine-tune Transformer-based architecture in the federated setting
on small scale datasets. However, they do not leverage label space semantics and the analysis is
restricted to small-scale datasets.
Closest to our work aims to improve local client training based on local data heterogeneity (Li et al.,
2018; Nedic, 2020; Wang et al., 2019; Khaled et al., 2019; Li et al., 2019b). Knowledge distillation
aims to transfer knowledge from a (large) teacher model to a (smaller) student model (Hinton et al.,
2015; BUcilUa et al., 2006). Given the output logit/softmaxed valued of the teacher model, the
student can imitate the teacher’s behavior (Romero et al., 2014; Tian et al., 2019; Tung & Mori,
2019; Koratana et al., 2019; Ahn et al., 2019). A few works are dedicated to the distillation of the
ensemble of teacher models to the student model. This includes logit averaging of teacher models
(You et al., 2017; Furlanello et al., 2018; Anil et al., 2018) or feature level knowledge extraction
(Park & Kwak, 2019; Liu et al., 2019; Wu et al., 2019).
For Contribution:1, we use standard and widely used Entropy-based loss (KL-divergence) as our
baseline. We construct two baselines for confidence score calculation (Contribution:2) from the
prior works, i.e., logit averaging and weighting scheme based on local model dataset size (McMahan
et al., 2017). In this work, we compare the proposed approach with baselines on the three NLU tasks.
3	Background
3.1	Opt imal Transport
Traditional divergences, such as KL, ignore metric structure in the label space Y . Optimal Transport
(OT) metric can be extremely useful in defining inter-label semantic relationships. OT offers an
additional advantage when measures have non-overlapping support (Peyre et al., 2019). Specific
to our (classification) problems, we will focus on discrete measures. Assume Y possess a metric
dγ(∙, ∙) referred to as ground truth metric. It establishes the semantic similarity between labels. The
original OT problem is a linear program attributed to Kantorovich. Let μi and Vj be the probability
masses respectively applied to i ∈ Ys and j ∈ Yt . Let πi,j be the transport assignment from i to j
that costs C(ij), i.e., an element of the cost matrix C. We denote Frobenius inner product by(•,•).
The primal goal is to find the plan π ∈ Π(μ, V) that minimizes the transport cost
T(μ,ν) def, min h∏, Ci
π∈Π(μ,ν)
Π(μ,ν) = {π ∈ (R+)ns×nt J X ∏(i,j) = μi and X π(i,j) = Vj}
j ∈Yt	i∈Ys
(1)
where ns = |Ys| and nt = |Yt|. In our problem, Ys = Yt = Y. When the cost is given in terms of
metric dγ(∙, ∙) also known as Wasserstein distance (Bogachev & Kolesnikov, 2012).
3.2	Sinkhorn Loss
Although research in Wasserstein has been active, there have been challenges in its computation and
implementation. The OT problem (equation 1) can be solved with combinatorial algorithms such as
simplex-based methods (Courty et al., 2016). The computational complexity is, however, shown to
be O((ns +nt)nsnt log(ns +nt)) at best (Ahuja et al., 1988), and thus, the utility lessens as the size
of dataset increases. The interest in the machine learning community took off after Cuturi’s seminal
paper (Cuturi, 2013). Rather than work with pure OT (Wasserstein) distances, we will restrict our
attention to plain regularized OT, i.e, vanilla Sinkhorn distances. The distance can be efficiently
solved by iterative Bregman Projections (Benamou et al., 2015).
Definition 3.1. Vanilla Sinkhorn Distance
Tε(μ, ν) d=f. min h∏, Ci + eDkl(π, μ 0 V)
π∈Π(μ,ν)
π(μ, V) = {π ∈ (R+)ns×nt I X π(ij) = μi and X π(ij) = Vj}
j ∈Yt	i∈Ys
(2)
3
Under review as a conference paper at ICLR 2022
where, Dkl (∏, μ0ν) = Pi j [∏i,j log -∏ij - ∏i,j + μiνj . EntroPicregUlarisation Convexifies the
loss function and thus is a computational advantage in computing gradients (Luise et al., 2018; Peyre
et al., 2019; Feydy et al., 2019). As ε → 0+, we retrieve the UnregUlarized Wasserstein distance.
4	Methodology
4.1	Problem framework
The main participants in this framework are: 1) global model Mg , and 2) a set of K local models
{Mk}k∈K, K = {1, . . . ,K}. We denote the set {Mk}k∈K by MK.
•	The global model Mg aims to learn a user-generalized hypothesis hθ ∈ Hg that exists on
the central application server.
•	A local models learns a client-specific hypothesis hk ∈ Hk on the Kth user-generated data.
The central server can retrieve back a local model’s prediction for a particular input. Thus, global
model training can benefit from the hypotheses of local models, denoted by hK, but not the param-
eters set MK or the private user-generated data. The global model on the server Mg is generally
preoccupied with the knowledge applicable across the domains. To enhance the user service, the
server distills the knowledge from hK and merges it into Mg’s hypothesis hθ. The knowledge trans-
fer happens with the assistance of a transfer set. Transfer set is the set of unlabeled i.i.d. samples
used to learn global model parameters θ. It creates a crucial medium to transfer the knowledge from
the local models to the global model. To facilitate the knowledge transfer, we consider the (noisy)
soft-labels are obtained from hK .
Since only the hK is shared with Mg to find a better hθ based on user feedback, the local models and
global models can have heterogeneous architectures. This is useful when client devices at certain
locations do not have enough resources to run and fit over large models.
4.2	Ensemble distillation loss
For demonstration, we consider a user application that performs sentiment classifica-
tion task on user-generated text x(i) = (x(1i) , x(2i) , . . . , x(ni) ) ∈ X into its sentiment
y(i) ∈ Y, where X denotes the input space of all possible text strings and label space
Y = {1 (strong negative), 2 (weak negative), 3 (neutral), 4 (weak positive), 5 (strong positive)}.
In this work, all the hypotheses are of the form h : X 7→ ∆Y, ∆Y denotes a probability distribu-
tion on the set of labels Y . We propose a learning algorithm that runs on the central server to fit
Mg ’s parameters θ by receiving predictions such as (softmaxed) logits from hK. Without the loss
of generality, the goal is to search for a hypothesis h^ that minimizes the empirical risk
h^ = argmin(ES [ Lε(hθ(x),hκ(x)) ] = ɪ X Lε( hθ(x(i)),hκ(x(i)) )1.
θ hθ∈H	N i=1
the loss is defined as
Lε(hθ(x(i)),hK(x(i)))=X WBk(hk(x(i))) Tε( hθ(x(i)), hk(x(i)))
k∈K
(3)
(4)
where Tε(∙, ∙) is the discrepency between the two probability measures as its arguments; WBk (.) is
the sample-specific weight assigned to the kth local model’s prediction. Next, we elaborate on the
the functions Tε(∙, ∙) and WBk (∙) which are crucial for our proposed FD algorithm.
5	Sinkhorn-based distillation
For entropy-based loss, we adopt KL divergence. As discussed in section 3.1, we employ Sinkhorn
distance to implement OT-based loss.
4
Under review as a conference paper at ICLR 2022
5.1	Unweighted distillation
For a text input x(i) from the transfer set, Tε( hθ(x(i)), hk(x(i)) ) measures the Sinkhorn distance
between the probability output of global model hθ (x(i) ) and a local model hk (x(i) ). In eq. (4),
the sample-wise distance is computed between hθ(x(i)) and a probability distribution in the set
hK . A simple approach to fit the global hypothesis hθ is to uniformly distill the knowledge from
user-specific hypotheses, i.e., WBk (hk(x(i))) = 1 ∀ k ∈ K, i ∈ [N].
5.2	Weighted Distillation
The user-generated local datasets are potentially non-IID with respect to the global distribution and
possess a high degree of class imbalance (Weiss & Provost, 2001). As each local model Mk is
trained on samples from potentially non-IID and imbalance domains, they are prone to show skewed
predictions. The unweighted distillation tends to transfer such biases. One might wonder for a given
transfer set sample, which local model’s prediction is reliable?. Although an open problem, we
try to answer this question by proposing a local model (teacher) weighting scheme. It calculates
the confidence score of a model’s prediction and performs weighted distillation—weights being in
positive correlation with the local model’s confidence score. Next, we define the confidence score.
Confidence score (L2) For a given sample x from the transfer set, the skew in a local model’s
prediction h(x) can help US determine the confidence score (W(∙) in eq. (4)) with which it can
transfer its knowledge to the global model. However, a model can show skew due to training on an
imbalance dataset or chosen capacity of the hypothesis space which can potentially cause a model
to overfit/underfit (Caruana et al., 2001). For instance, a model has learned to misclassify negative
sentiment as strongly negative samples owing to a high confusion rate. Such models are prone to
show inference time classification errors with highly skewed probabilities. Thus, confidence scoring
based on the probability skew may not be admissible. Hence, we incorporate L2 confidence. For
a given sample, we define the model’s L2 confidence score WB (h(x)) as Euclidean distance of its
output probability distribution from the probability bias B . We define probability bias B of a local
model as the expected value of prediction when a model h receives noise (random text) at the input.
Let h(x) ∈ Y denotes the predicted distribution of a model for an input text x:
bl∈Y := Ex〜N[h(X) = l]
B := (b1,...,b|Y|)
(N: the distribution of noise)
(model probability skew).
Figure 2: An illustration—a three-class classifier with
bias B and outputs a distribution P = (p1, p2, p3)
for certain input. The green arrow denotes direction of
increased confidence score with equiconfident arcs.
As shown in fig. 2 for a three-class classifica-
tion, the equidistant distributions lie on an arc
with center at B . Points with high confidence
score lie on distant arcs. As radius of the arc in-
creases, majority of its portion lies towards the
high value ofpl, i.e., the l with which the model
is biased against since bl = min {b1, . . . , b|Y|}
(p3 in the figure). Moreover, the maximum con-
fidence score is achieved at the vertex pl = 1.
Proposition 5.1. From a given point B in a k-
simplex, point with the highest confidence lies
on one of its vertices.
Proof. First, we analyse the case of a 2-simplex
defined in a three-dimensional Euclidean space.
Let fP = Pi3=1 (pi - bi)2, the quadratic pro-
gram can be formulated as max{fP : Pi3=1 pi = 1, pi ≥ 0}. The convex hull of vertices lying
on the axes forms a closed and bounded feasible region. Thus, from extreme value theorem, there
exist absolute maximum and minimum. f attains its minimum at p = b, which is also the crit-
ical point of fP . Now, we need to find its value on the boundary points contained in the set of
5
Under review as a conference paper at ICLR 2022
Table 1: Data statistics and performance of local models. For SA and ERC tasks, score denotes
Macro F-1 performance metric, while it denotes Accuracy metric for NLI.
	SA				ERC	一			NLI					
	Cell	Cloths	Toys	Food	IEMOCAP	MELD	DyDa{1,2,3}	Fic	Gov	Slate	Tele	Trv	SNLI
train	133,574	19,470	116,666	397,917	-3,354-	9,450	21,680	77,348	77,350	77,306	83,348	77,350	549,367
VaIid	19,463	28,376	17,000	57,982	342	1,047	2,013	5,902	5,888	5,893	5,899	5,904	9,842
test	37,784	55,085	33,000	112,555	901	2,492	1,919 —	5,903	5,889	5,894	5,899	5,904	9,824
score	0.49	0.52	0.49	0.64	0.55	0.45	0.38, 0.34, 0.4Q=	0.63	0.65	0.62	0.65	0.63	0.85
1-simplices (line segments) {pi + pj = 1, pk = 0 : (i, j, k) ∈ 1, 2, 3, i 6= j 6= k}. For the 1-
simplex p1 + p2 = 1, p3 = 0, the values of fP at its end points that are (1 - b1)2 + b22 + b23 and
b12 + (1 - b2)2 + b32, one of which is maxima of f attained over the 1-simplex 2. Similarly for the
other line segments, the complete set of boundary values of fP is k - 2b1 , k - 2b2, and k - 2b3
where k = b21 + b22 + b23 + 1, occurring at P = (1, 0, 0), (0, 1, 0) and (0, 0, 1), respectively. Thus, the
maximum of fP will lie on ith-axis such that bi = min (b1, b2, b3). This proof can be generalized for
a probability simplex in higher dimensions. As shown above, each iteration of a lower dimensional
simplex will return vertices as the point of maxima in the end.	□
L2 confidence is a proper distance metric and computationally stable. The distance metric is invari-
ant to translation in the Euclidean space. Thus, WBk (hk) measures the L2 distance of kth model’s
prediction from its intrinsic probability bias Bk. Moreover, L2 confidence can be used to compute
weights for both distillation methods, i.e., Sinkhorn and Entropy.
Figure 3: Semantic coordinates of SA, ERC, and NLI.
5.3	Statistical properties of Wasserstein
Let the samples S = {(x(1), y(1)), . . . , (x(N), y(N))}) be i.i.d from the domain distribution of the
transfer set and h^ be the empirical risk minimizer. Assume the global hypothesis space Hg =
s ◦ Hgo, i.e., composition of softmax and a hypothesis Hgo : X 7→ R|Y|, that maps input text to
a scalar (logit) value for each label. Assuming vanilla Sinkhorn with ε → 0+ , we establish the
property for 1-Wasserstein.
Theorem 5.2. If the global loss function (as in eq. (4)) uses unregularized 1-Wasserstein metric
between predicted and target measure, then for any δ > 0, with probability at least 1-δ
E[L(h^(x),hκ(x))] ≤ inf E[L(hθ(x),hκ(x))] +32 × Y
hθ ∈Hog
× Rn (Ho ) + 2Cm ∣γ∣y∣γ∣ lo2N/δ
(5)
where RN (Hgo), decays with N, denotes Rademacher complexity Bartlett & Mendelson (2002) of
the hypothesis space Hgo . CM is the maximum cost of transportation within the label space. In the
case of SA, CM = 4, |Y| = 5. The expected loss of the empirical risk minimizer h^ approaches the
best achievable loss for Hg . The proof of theorem theorem 5.2 and method to compute gradient are
relegated to the Appendix.
To comprehensively analyse the importance of OT over entropy for the task with intrinsic label
similarity, we introduce a new performance metric that evaluates a model based on the semantic
correctness of its predictions.
2Ignoring the critical point which gives the minima and perpendicular drawn from b to the line segment.
6
Under review as a conference paper at ICLR 2022
5.4	Semantic Distance
During the evaluation, most common metrics (such as accuracy and F1) observe the label with the
highest logit (or probability) against ground truth, hence, ignore the overall distribution. However,
for tasks with label space semantics, it can be of great importance. Thus, we define a new per-
formance metric—Semantic Distance (SD)—that measures the semantic closeness of the output
distribution against the ground truth. Given a label coordinate space, SD is defined as the mean
Euclidean distance of expected output from the ground truth label. For instance, given the sentiment
classes {1,2,3,4,5}, the probability scores of two models m1 and m2 assigns to a strongly negative
text input be {0.2, 0.7, 0.033, 0.033, 0.033} and {0.4, 0.1, 0.1, 0.1, 0.3}, respectively. The argmax
output of m2 is correct. However, even when the argmax output of m1 is incorrect, the expected
output of m1, i.e., 1.97 is (Euclidean) closer to the ground truth label 1 than m2, i.e., 2.80, and thus
more semantically accurate. A low score denotes more semantically accurate prediction. The lowest
possible value of SD is 0 while the highest possible value depends on the number of labels and their
map in the semantic space.
For datasets with class imbalance, such as SA task in this work Table 1, we first calculate label-wise
SD values and compute their mean.
Analysis—To demonstrate the usefulness of the
SD metric, for the SA, we draw box plots
of pretrained global models via Entropy (KL-
divergence) and Sinkhorn-based losses. The
pretraining methodology is described in sec-
tion 6. As shown in fig. 4, we observe the me-
dian SD of Sinkhorn (green box, red line) is
closer to the ground truth sentiment classes—
1,2,3, and 5 as compared to median SD of En-
tropy (blue box-red line). Similarly, the means
(black diamond), the first quartile (25% of the
samples), and the third quartile (75% of the
samples) for the Sinkhorn-based model are rel-
atively closer to the ground truth.
Figure 4: Box plot showing expectation of output
probabilities for SA task. Horizontal and vertical
axes denote ground truth sentiment labels.
6	Experiments
Baselines —We setup the following baselines for a thorough comparison between Sinkhon and
Entropy-based losses. Let [Method] be the placeholder for Sinkhorn and Entropy. [Method]-A
denotes unweighted distillation of local models (section 5.1), i.e., WBk (hk(x)) = 1 (in eq. (4)).
In [Method]-D, WBk (hk (x)) is proportional to size of local datasets. [Method]-U defines sample-
specific confidence (weights) as the distance of output from the uniform distribution over labels.
For each sample, [Method]-E computes weight of kth local model as distance of its prediction from
probability bias Bk, i.e., L2 confidence.
Tasks We set up the three natural language understanding tasks with intrinsic similarity in the label
space: 1) fine-grained Sentiment Analysis (SA); 2) Emotion Recognition in Conversation (ERC);
and 3) Natural Language Inference (NLI). NLI is the task of determining the inference relation be-
tween two texts. The relation can be entailment, contradiction, or neutral (MacCartney & Manning,
2008). For a given transcript of conversation, the ERC task aims to identify the emotion of each
utterance from the set of pre-defined emotions (Poria et al., 2019). For our experiments, we choose
the five most common emotions that are sadness, anger, surprise, and happiness, and no emotion.
Datasets For the SA task, we use four large-scale datasets: 1) Toys: toys and games; 2) Cloth:
clothing shoes and jewelry; 3) Cell: cell phones and accessories; 4) Food: Amazon’s fine food
reviews, specifically curated for the five-class sentiment classification. For the transfer set, we use
grocery and gourmet food (104,817 samples) and discard the labels (He & McAuley, 2016). Each
dataset consists of reviews rated on a scale of 1 (strong negative) to 5 (strong positive). Similarly,
for ERC, we collect three widely used datasets: DyDa: DailyDialog (Li et al., 2017), IEMOCAP:
interactive emotional dyadic motion capture database (Busso et al., 2008), and MELD: Multimodal
7
Under review as a conference paper at ICLR 2022
EmotionLines Dataset (Poria et al., 2018). To demonstrate our methodology, we partition the DyDa
dataset into four equal chunks. DyDa1 , DyDa2 , are used as local, DyDa3 is used as global dataset.
Dropping the labels from DyDa4 , we use it as transfer set. For NLI task, we use SNLI (Bowman
et al., 2015) as global dataset and MNLI (Williams et al., 2017) as local dataset. We split the latter
across its five genres, which are, fiction (Fic), government (Gov), telephone (Tele), travel (Trv), and
Slate. This split assists in simulating distinct user (non-IID samples) setup. We use ANLI dataset
(Nie et al., 2020) as transfer set.
Architecture We set up a compact transformer-based model used by both global and local models
(fig. 1), although, the federation does not restrict both the local and model configurations to be the
same. The input is fed to the pretrained BERT-based classifier (Turc et al., 2019; Devlin et al.,
2018). Thus, we obtain probabilities with support in the space of output labels, i.e., Y. We keep all
the parameters trainable, hence, BERT will learn its embeddings specific to the classification task.
For NLI task, we append premise and hypothesis at input separated by special token [SEP] token,
followed by a standard classification setup.
Training local models To compare Sinkhorn-based distillation with baselines, first, we pretrain
local models. Since cross-entropy (CE) loss is less computationally expensive as compared to OT,
we use CE for local model training. For all model training, we tuned hyperparameters for all the
models separately and chose the model that performs best on the validation dataset. The data statis-
tics and performances of local models on individual tasks are shown in Table 1.
Training a global model We make use of transfer set (unlabeled) samples and obtain noisy labels.
For a text sample in transfer set, eq. (4) aims to fit a global model to the weighted sum of noisy
predictions of the local models. To retain the previous knowledge of a global model, we adapt
learning without forgetting paradigm. To incorporate this, we store predictions of the pretrained
global model on the transfer set and perform its distillation along with local models.
Label-space We define label semantic spaces for the three tasks. As shown in fig. 3, we assign sen-
timent labels a one-dimensional space. For the ERC task, we map each label to a two-dimensional
valence-arousal space. Valence represents a person’s positive or negative feelings, whereas arousal
denotes the energy of an individual’s affective state. As mentioned in (Ahn et al., 2010), anger (-0.4,
0.8), happiness (0.9, 0.2), no emotion (0, 0), sadness (-0.9, -0.4), and surprise (0.4, 0.9). The cost
(loss) incurred to transport a mass from a point p to point q is Cp,q := |p - q|. For NLI task, we
define a three dimensional coordinate with entailment (1, 0, 0), contradiction (0,0,1) and neutral
(0.5, 1, 0.5). The cost Cp,q := ||p - q||2, where, cost of transport from entailment to contradiction is
higher than it is to neutral. It is noteworthy that for this task, we perform a manual search to identify
label coordinate space and transportation cost.
Table 2: Fine-grained SA task: Macro F1 and Semantic Distance.
			F1 Score				Semantic Distance			
Algorithm			-Local				Global	ALL	—	-Local—				Global	ALL
	Cloths	Toys	Cell	Food		Cloths	Toys	Cell	Food	
Entropy-A	0.48	0.44	0.47	0.52	0.50	0.77	0.87	0.76	0.79	0.79
Entropy-D	0.48	0.44	0.46	0.56	0.52	0.77	0.86	0.77	0.71	0.78
Entropy-U	0.47	0.43	0.47	0.50	0.49	0.79	0.90	0.78	0.82	0.83
Entropy-E	0.49	0.46	0.48	0.55	0.52	0.74	0.80	0.74	0.71	0.75
Sinkhorn-A	0.49	0.47	0.47	0.55	0.52	0.74	0.80	0.72	0.75	0.75
Sinkhorn-D	0.47	0.44	0.45	0.59	0.52	0.77	0.84	0.76	0.65	0.76
Sinkhorn-U	0.48	0.44	0.47	0.51	0.49	0.77	0.89	0.77	0.83	0.82
Sinkhorn-E	0.49	0.47	0.48	0.55	0.52	0.72	0.78	0.72	0.69	0.73
Table 2, Table 3, and Table 4 show performance, i.e., Macro-F1 (or Accuracy) score and Semantic
Distance of global models predictions from ground truth. Evaluations are done for fine-tuned (after
distillation) global model with respect to the test sets of both local and global datasets. The testing
over local datasets will help us analyse how well the domain generic global model performs over the
individual local datasets and the testing over the global dataset is to make sure there is no catastrophic
forgetting of the previous knowledge.
For the SA task, we observe the global models trained from Sinkhorn distillation of local models
is amongst the best F1 scores on all the local domains. For the most simplistic baseline, i.e., un-
8
Under review as a conference paper at ICLR 2022
Table 3: ERC task: Macro F1 and Semantic Distance.
Algorithm Entropy-A Entropy-D Entropy-U Entropy-E	F1 Score						MELD 0.67 0.68 0.68 0.69	Semantic Distance				ALL 0.64 0.62 0.64 0.62
	MELD 0.28 0.30 0.30 0.34	——-Local——-			Global DyDa2 0.36 0.38 0.37 0.44	ALL 0.31 0.34 0.33 0.39		——-Local——-		DyDa1 0.61 0.59 0.62 0.59	Global DyDa2 0.63 0.61 0.64 0.62	
		IEMOCAP 0.21 0.21 0.24 0.36	DyDa0 0.34 0.39 0.42 0.42	DyDa1 0.33 0.35 0.31 0.40				IEMOCAP 0.65 0.65 0.65 0.62	DyDa0 0.60 0.57 0.60 0.57			
Sinkhorn-A	0.30	0.26	0.45	0.37	0.39	0.34	-067-	0.67	0.59	0.62	0.62	0.64
Sinkhorn-D	0.35	0.31	0.45	0.37	0.44	0.39	0.67	0.63	0.54	0.59	0.61	0.61
Sinkhorn-U	0.30	0.23	0.39	0.34	0.39	0.34	0.68	0.68	0.62	0.64	0.64	0.65
Sinkhorn-E	0.38	0.33	0.46	0.43	0.43	0.41	0.64	0.62	0.53	0.56	0.60	0.59
weighted distillation, Sinkhorn-A consistently gives a higher F1 score as compared to Entropy-A.
Even though the performance of Sinkhorn-A is close to Sinkhorn-E, the latter is more semantically
accurate as depicted by corresponding SA scores. This shows the efficacy of weighting local models’
prediction with L2 confidence. Sinkhorn-D based global model training gives the best F1 and SA
scores on global datasets. We postulate this is due to a large number of global samples as compared
to local dataset sizes that bias the distillation weights W (hk (x)), hence force the model to perform
better on the global dataset. It can be observed that this comes at the cost of poor performance on
the local datasets.
Table 4: NLI task: Accuracy and Semantic Distance.
Algorithm Entropy-A Entropy-D Entropy-U Entropy-E	Accuracy							Semantic Distance						
	— Fic 0.60 0.58 0.60 0.60	Gov 0.62 0.59 0.62 0.62	-Local Slate 0.60 0.57 0.60 0.60	Tele 0.60 0.57 0.60 0.60	- Trv 0.62 0.58 0.61 0.61	Global SNLI 0.78 0.85 0.76 0.76	ALL 0.65 0.64 0.65 0.65	— Fic 0.56 0.58 0.55 0.55	Gov 0.54 0.56 0.54 0.54	-Local Slate 0.56 0.58 0.56 0.56	Tele 0.56 0.58 0.56 0.55	- Trv 0.55 0.57 0.55 0.54	Global SNLI 0.40 0.30 0.40 0.39	ALL 0.53 0.53 0.53 0.52
Sinkhorn-A	0.60	0.63	0.60	0.61	0.62	0.73	0.64	0.54	0.53	0.55	0.55	0.53	0.42	0.52
Sinkhorn-D	0.55	0.57	0.54	0.55	0.56	0.85	0.63	0.58	0.56	0.58	0.58	0.57	0.23	0.52
Sinkhorn-U	0.60	0.62	0.60	0.60	0.61	0.77	0.65	0.53	0.52	0.54	0.54	0.52	0.37	0.51
Sinkhorn-E	0.60	0.62	0.60	0.60	0.61	0.77	0.65	0.53	0.52	0.54	0.54	0.52	0.37	0.50
For the ERC and NLI tasks, although it is hard to find a model that shows consistently better F1 or
accuracy scores (one cause could be the small small size of local and global datasets), we observe
the SD score of Sinkhorn-E is, in general, better amongst the baselines. As observed for the SA
task in Entropy-D and Sinkhorn-D settings, since the SNLI dataset is bigger, the distillation forces
global models to perform better on the global dataset. It is seen to come at the cost of degraded
performance on the other (local) datasets.
Comparing Table 2, Table 3, and Table 4 all together, for the three tasks with intrinsic similarity in
the label space, we observe Sinkhorn-based loss are in general better than KL-divergence, i.e., an
entropy-based loss. Moreover, we observe L2 distance gives better scores amongst the individual
loss-specific groups. We also notice that in certain tasks, where it is difficult to identify the best
model, one can refer to the semantic distance. Besides this, as compared to other baselines, empirical
observations suggest that Sinkhorn-E (our contribution) works well for the large-scale SA datasets,
hence potentially scalable.
7	Conclusion
In this work, we introduced an algorithm for efficient federated distillation of natural language un-
derstanding from client devices to the central (global) model. We defined a new Euclidean distance-
based metric to compute a local model’s intrinsic probability bias. We analysed theoretical gener-
alization bounds of empirical risk of the proposed loss function. In the end, we demonstrated the
efficacy of the novel approach on the three NLU tasks of fine-grained sentiment analysis, emotion
recognition in conversation, and natural language inference.
9
Under review as a conference paper at ICLR 2022
References
Junghyun Ahn, Stephane Gobron, Quentin Silvestre, and Daniel Thalmann. Asymmetrical facial
expressions based on an advanced interpretation of two-dimensional russell’s emotional model.
Proceedings of ENGAGE, 2010.
Sungsoo Ahn, Shell Xu Hu, Andreas Damianou, Neil D Lawrence, and Zhenwen Dai. Variational
information distillation for knowledge transfer. In Proceedings of the IEEE/CVF Conference on
Computer Vision and Pattern Recognition, pp. 9163-9171, 2019.
Ravindra K Ahuja, Thomas L Magnanti, and James B Orlin. Network flows. 1988.
Rohan Anil, Gabriel Pereyra, Alexandre Passos, Robert Ormandi, George E Dahl, and Geoffrey E
Hinton. Large scale distributed neural network training through online distillation. arXiv preprint
arXiv:1804.03235, 2018.
Peter L Bartlett and Shahar Mendelson. Rademacher and gaussian complexities: Risk bounds and
structural results. Journal of Machine Learning Research, 3(Nov):463-482, 2002.
Jean-David Benamou, GUillaUme Carlier, Marco Cuturi, LUca Nenna, and Gabriel Peyre. Iterative
bregman projections for regularized transportation problems. SIAM Journal on Scientific Com-
puting, 37(2):A1111-A1138, 2015.
Vladimir Igorevich Bogachev and Aleksandr Viktorovich Kolesnikov. The monge-kantorovich prob-
lem: achievements, connections, and perspectives. Russian Mathematical Surveys, 67(5):785-
890, 2012.
Samuel R Bowman, Gabor Angeli, Christopher Potts, and Christopher D Manning. A large anno-
tated corpus for learning natural language inference. arXiv preprint arXiv:1508.05326, 2015.
Cristian BUcilUa, Rich Caruana, and Alexandru NicUlescU-MiziL Model compression. In Pro-
ceedings of the 12th ACM SIGKDD international conference on Knowledge discovery and data
mining, pp. 535-541, 2006.
Carlos BUsso, MUrtaza BUlUt, Chi-ChUn Lee, Abe Kazemzadeh, Emily Mower, SamUel Kim, Jean-
nette N Chang, SUngbok Lee, and Shrikanth S Narayanan. Iemocap: Interactive emotional dyadic
motion captUre database. Language resources and evaluation, 42(4):335-359, 2008.
Rich CarUana, Steve Lawrence, and Lee Giles. Overfitting in neUral nets: Backpropagation, con-
jUgate gradient, and early stopping. Advances in neural information processing systems, pp.
402-408, 2001.
Nicolas Courty, Remi Flamary, Devis Tuia, and Alain Rakotomamonjy. Optimal transport for do-
main adaptation. IEEE transactions on pattern analysis and machine intelligence, 39(9):1853-
1865, 2016.
Marco Cuturi. Sinkhorn distances: Lightspeed computation of optimal transport. Advances in neural
information processing systems, 26:2292-2300, 2013.
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep
bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018.
Jean Feydy, Thibault SejOUrne, Francois-Xavier Vialard, Shun-ichi Amari, Alain Trouve, and
Gabriel Peyre. Interpolating between optimal transport and mmd using sinkhorn divergences.
In The 22nd International Conference on Artificial Intelligence and Statistics, pp. 2681-2690.
PMLR, 2019.
Charlie Frogner, Chiyuan Zhang, Hossein Mobahi, Mauricio Araya-Polo, and Tomaso Poggio.
Learning with a wasserstein loss. In Proceedings of the 28th International Conference on Neural
Information Processing Systems-Volume 2, pp. 2053-2061, 2015.
Tommaso Furlanello, Zachary Lipton, Michael Tschannen, Laurent Itti, and Anima Anandkumar.
Born again neural networks. In International Conference on Machine Learning, pp. 1607-1616.
PMLR, 2018.
10
Under review as a conference paper at ICLR 2022
Jianping Gou, Baosheng Yu, Stephen J Maybank, and Dacheng Tao. Knowledge distillation: A
survey. International Journal ofComputer Vision,129(6):1789-1819, 2021.
Ruining He and Julian McAuley. Ups and downs: Modeling the visual evolution of fashion trends
with one-class collaborative filtering. In proceedings of the 25th international conference on
world wide web, pp. 507-517, 2016.
Agrin HilmkiL Sebastian Callh, Matteo Barbieri, Leon Rene Sutfeld, Edvin Listo Zec, and Olof
Mogren. Scaling federated learning for fine-tuning of large language models. arXiv preprint
arXiv:2102.00875, 2021.
Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. Distilling the knowledge in a neural network. arXiv
preprint arXiv:1503.02531, 2015.
Judy Hoffman, Mehryar Mohri, and Ningshan Zhang. Algorithms and theory for multiple-source
adaptation. arXiv preprint arXiv:1805.08727, 2018.
Eunjeong Jeong, Seungeun Oh, Hyesung Kim, Jihong Park, Mehdi Bennis, and Seong-Lyun Kim.
Communication-efficient on-device machine learning: Federated distillation and augmentation
under non-iid private data. arXiv preprint arXiv:1811.11479, 2018.
Ahmed Khaled, Konstantin Mishchenko, and Peter Richtarik. First analysis of local gd on hetero-
geneous data. arXiv preprint arXiv:1909.04715, 2019.
Jakub Konecny, H Brendan McMahan, Daniel Ramage, and Peter Richtarik. Federated optimization:
Distributed machine learning for on-device intelligence. arXiv preprint arXiv:1610.02527, 2016.
Animesh Koratana, Daniel Kang, Peter Bailis, and Matei Zaharia. Lit: Learned intermediate repre-
sentation training for model compression. In International Conference on Machine Learning, pp.
3509-3518. PMLR, 2019.
Michel Ledoux and Michel Talagrand. Probability in Banach Spaces: isoperimetry and processes.
Springer Science & Business Media, 2013.
Tian Li, Anit Kumar Sahu, Manzil Zaheer, Maziar Sanjabi, Ameet Talwalkar, and Virginia Smith.
Federated optimization in heterogeneous networks. arXiv preprint arXiv:1812.06127, 2018.
Tian Li, Maziar Sanjabi, Ahmad Beirami, and Virginia Smith. Fair resource allocation in federated
learning. arXiv preprint arXiv:1905.10497, 2019a.
Xiang Li, Kaixuan Huang, Wenhao Yang, Shusen Wang, and Zhihua Zhang. On the convergence of
fedavg on non-iid data. arXiv preprint arXiv:1907.02189, 2019b.
Yanran Li, Hui Su, Xiaoyu Shen, Wenjie Li, Ziqiang Cao, and Shuzi Niu. Dailydialog: A manually
labelled multi-turn dialogue dataset. In Proceedings of The 8th International Joint Conference on
Natural Language Processing (IJCNLP 2017), 2017.
Tao Lin, Sebastian U Stich, Kumar Kshitij Patel, and Martin Jaggi. Don’t use large mini-batches,
use local sgd. arXiv preprint arXiv:1808.07217, 2018.
Tao Lin, Lingjing Kong, Sebastian U Stich, and Martin Jaggi. Ensemble distillation for robust model
fusion in federated learning. arXiv preprint arXiv:2006.07242, 2020.
Dianbo Liu and Tim Miller. Federated pretraining and fine tuning of bert using clinical notes from
multiple silos. arXiv preprint arXiv:2002.08562, 2020.
Iou-Jen Liu, Jian Peng, and Alexander G Schwing. Knowledge flow: Improve upon your teachers.
arXiv preprint arXiv:1904.05878, 2019.
Giulia Luise, Alessandro Rudi, Massimiliano Pontil, and Carlo Ciliberto. Differential properties of
sinkhorn approximation for learning with wasserstein distance. Advances in Neural Information
Processing Systems, pp. 5864-5874, 2018.
11
Under review as a conference paper at ICLR 2022
Bill MacCartney and Christopher D. Manning. Modeling semantic containment and exclusion in
natural language inference. In Proceedings of the 22nd International Conference on Compu-
tational Linguistics (CoIing 2008), pp. 521-528, Manchester, UK, August 2008. Coling 2008
Organizing Committee. URL https://aclanthology.org/C08-1066.
Colin McDiarmid. Concentration. In Probabilistic methods for algorithmic discrete mathematics,
pp. 195-248. Springer, 1998.
Brendan McMahan, Eider Moore, Daniel Ramage, Seth Hampson, and Blaise Aguera y Arcas.
Communication-efficient learning of deep networks from decentralized data. In Artificial Intelli-
gence and Statistics, pp. 1273-1282. PMLR, 2017.
Ninareh Mehrabi, Fred Morstatter, Nripsuta Saxena, Kristina Lerman, and Aram Galstyan. A survey
on bias and fairness in machine learning. arXiv preprint arXiv:1908.09635, 2019.
Mehryar Mohri, Gary Sivek, and Ananda Theertha Suresh. Agnostic federated learning. In Interna-
tional Conference on Machine Learning, pp. 4615-4625. PMLR, 2019.
Kevin P Murphy. Machine learning: a probabilistic perspective. MIT press, 2012.
Angelia Nedic. Distributed gradient methods for convex machine learning problems in networks:
Distributed optimization. IEEE Signal Processing Magazine, 37(3):92-101, 2020.
Yixin Nie, Adina Williams, Emily Dinan, Mohit Bansal, Jason Weston, and Douwe Kiela. Ad-
versarial nli: A new benchmark for natural language understanding. In Proceedings of the 58th
Annual Meeting of the Association for Computational Linguistics. Association for Computational
Linguistics, 2020.
SeongUk Park and Nojun Kwak. Feed: Feature-level ensemble for knowledge distillation. arXiv
preprint arXiv:1909.10754, 2019.
Gabriel Peyre, Marco Cuturi, et al. Computational optimal transport: With applications to data
science. Foundations and Trends® in Machine Learning, 11(5-6):355-607, 2019.
Soujanya Poria, Devamanyu Hazarika, Navonil Majumder, Gautam Naik, Erik Cambria, and Rada
Mihalcea. Meld: A multimodal multi-party dataset for emotion recognition in conversations.
arXiv preprint arXiv:1810.02508, 2018.
Soujanya Poria, Navonil Majumder, Rada Mihalcea, and Eduard Hovy. Emotion recognition in con-
versation: Research challenges, datasets, and recent advances. IEEE Access, 7:100943-100953,
2019.
Adriana Romero, Nicolas Ballas, Samira Ebrahimi Kahou, Antoine Chassang, Carlo Gatta, and
Yoshua Bengio. Fitnets: Hints for thin deep nets. arXiv preprint arXiv:1412.6550, 2014.
Yonglong Tian, Dilip Krishnan, and Phillip Isola. Contrastive representation distillation. arXiv
preprint arXiv:1910.10699, 2019.
Qianqian Tong, Guannan Liang, and Jinbo Bi. Effective federated adaptive gradient methods with
non-iid decentralized data. arXiv preprint arXiv:2009.06557, 2020.
Frederick Tung and Greg Mori. Similarity-preserving knowledge distillation. In Proceedings of the
IEEE/CVF International Conference on Computer Vision, pp. 1365-1374, 2019.
Iulia Turc, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Well-read students learn better:
On the importance of pre-training compact models. arXiv preprint arXiv:1908.08962v2, 2019.
Shiqiang Wang, Tiffany Tuor, Theodoros Salonidis, Kin K Leung, Christian Makaya, Ting He, and
Kevin Chan. Adaptive federated learning in resource constrained edge computing systems. IEEE
Journal on Selected Areas in Communications, 37(6):1205-1221, 2019.
Gary M Weiss and Foster Provost. The effect of class distribution on classifier learning: an empirical
study. 2001.
12
Under review as a conference paper at ICLR 2022
Adina Williams, Nikita Nangia, and Samuel R Bowman. A broad-coverage challenge corpus for
sentence understanding through inference. arXiv preprint arXiv:1704.05426, 2017.
Ancong Wu, Wei-Shi Zheng, Xiaowei Guo, and Jian-Huang Lai. Distilled person re-identification:
Towards a more scalable system. In Proceedings of the IEEE/CVF Conference on Computer
Vision and Pattern Recognition, pp. 1187-1196, 2019.
Shan You, Chang Xu, Chao Xu, and Dacheng Tao. Learning from multiple teacher networks. In
Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and
Data Mining, pp. 1285-1294, 2017.
13
Under review as a conference paper at ICLR 2022
A	What we have kept for the Appendix ?
There are a few experiments that we consider to be important and may help compare the Sinkhorn
loss-based learning with KUllback-Leibler (KL). These results build a firm base to choose Sinkhorn-
based losses on the task of federated distillation of sentiments. For the experiments, we work on the
global model Mg that has acquired knowledge from local models in the learning without forgetting
paradigm.
•	In appendix B, we convey the intuition behind using a Sinkhorn distance over an entropy-
based divergence. Moving further in appendix B.1, we show the importance of natural
metric in the label space by replacing the one-dimensional support with one-hot. Further-
more, in appendix B.2, we show how the model’s clusters of sentence embeddings change
when we move from a Sinkhorn distance-based loss to the KL divergence-based loss.
•	In appendix C, we discuss the potential risk of gender and racial bias transfer from the
local models to the global models. Although we incorporate bias induced from non-IID
data training of the local models, we do not tackle the transfer of other biases that can arise
from the data as well as from the training process.
•	In appendix D, we provide the algorithm to compute gradients and the computation com-
plexity of Sinkhron loss.
•	In appendix E, we provide the proof of Theorem 5.2 on the empirical risk bound with the
OT metric as unregularized 1-Wasserstein distance.
•	In appendix F, we discuss the broader social impact of our work. We discuss how the
method can be adopted for cyberbullying detection and limitations coming from local mod-
els.
•	In appendix G, we elaborate on the experimental settings and license of the datasets used
in this paper.
B Decision boundaries via sentence representations
One of the main advantages of using Optimal Transport-based (OT) metrics between two probability
distributions, such as Sinkhorn distance, is the ability to define the relationship in metric space. This
is not feasible in entropy-based divergences. The relationship further appears in the loss function that
accounts for the error computations of an intelligent system in the task of classification (or regres-
sion). With advancements in computations of Sinkhorn distances, as in Feydy et al. (2019), gradient
computations through such loss functions have become more feasible as shown in Algorithm 1. The
inter-label relationships are apparent in tasks such as fine-grained sentiment classification, fake news
detection, hate speech. In this work, we consider the relationship between two labels p and q as a
taxi-cab distance in the one-dimensional metric space of sentiment labels Y. This relation nuance
should appear in the Sinkhorn distance, we call it the cost of transportation from a point p to another
point q in the set Y .
When we set a learning algorithm to minimize the loss function, the goal is to find model parameters
that provide the least empirical risk in the space of predefined hypotheses. From the distance-based
cost (loss), the risk is expected to be minimum when the predicted labels are mapped ”near” to the
ground truth label. The term ”near” refers to the lower optimal transport cost of the probability mass
spread over a certain region to another region. In our problem, both the regions are the same, i.e.,
locations from 1 to 5 in the metric space. A ground truth probability mass (almost everything) at
1 would prefer an intelligent system to predict a probability mass near 1 so that it will require a
lesser taxi-cab cost of transportation. It is noteworthy, such relationships, even though apparent, are
infeasible to appear in cost functions that inherit properties solely from the information theory.
t-SNE of sentence embeddings Next, we explain how we analyse the sentence-embeddings in
Mg obtained from Sink-E*. A sentence refers to an Amazon food/product review. BERT’s input
sentence is lowercase WordPiece tokenized. We prepend the list of tokens with [CLS] token to
represent the sentence which is later used for the classification task. First, each token is mapped
to a static context-independent embedding. Then the vector list is passed through a sequence of
14
Under review as a conference paper at ICLR 2022
Figure 5: After the model parameter fitting is complete, we map the 128-dimensional [CLS] vector
at the output of the Trasnformer layer to 2-dimensions usgin t-SNE.
multi-head self-attention operations that contextualizes each token. It is important to note that con-
textualization can be task-specific. We randomly sample 5000 review-label pairs for each sentiment
class. For each textual review, as shown in fig. 5, we use a 128-dimensional vector at the output
of the transformer layer corresponding to the [CLS] token. This corresponds to the list of reviews
represented in 128-dimensional vector space. To visualize the learned sentence representations, we
map the vectors from 128-dimensional space to 2-dimensions using t-distributed Stochastic Neigh-
bor Embedding (t-SNE)3.
B.1 SEMANTIC SUPPORT → ONE-HOT SUPPORT
e'山NSj
•1	2	3 4	5 t-SNE - 1
Figure 6:	The one-hot encoding doesn't have clear distinct boundaries. The semantic structure is
lost.
One way to understand the importance of properly defining label space relationship is by defining
metric space where each label acquires its own axis, thus losing the semantic information. For a
five-class classification problem, We will have five axes and thus the support is a set of five distinct
one-hot vectors each of size five. This way any misclassification, i.e., predicting a mass different
3We used the implementation from scikit-learn.
15
Under review as a conference paper at ICLR 2022
from the ground truth label, will result in the same cost irrespective of positive sentiment is classified
as strongly positive or strongly negative. This is due to the taxi-cab distance. Its value is computed
by just summing up the absolute individual coordinate distances, which are just the predicted prob-
abilities except for the coordinate corresponding to the ground truth.
We train the global model Mg with the Sinkhorn distance-based loss (eq. (4)) where the cost is
defined as taxi-cab distance on the one-dimensional support and five-dimensional one-hot support
as elaborated previously. The fig. 6 depicts the respective t-SNE scatter plots. In the plot with one-
dimensional semantic support, we observe sentence vectors, i.e., features used for the classification
task, are mapped in clearer clusters as compared to the plot at right without semantic information.
We observe the points related to label 5 (strong positive) are much more localized as compared to
the sentence mappings with one-hot, which is distributed around the space. This clearly dictates the
benefit of a meaningful metric as compared to a space that is not informative. Next, we check a
similar case that occurs in entropy-based loss functions.
B.2 SINKHORN DISTANCE → KL DIVERGENCE
Similar to the cost associated with the one-hot support in Sinkhorn, the KL divergence has no fea-
sible way to capture the intrinsic metric in the label space. The plots in fig. 7 show the differ-
ent sentence embeddings (t-SNE) with the varying entropy-based regularisation term in the vanilla
Sinkhorn distance. As ε → 0+ , we should get a pure OT-based loss function (eq. (2)). However, to
speed up the Sinkhorn and gradient computations, we chose ε = 0.001 with no (F1-score) perfor-
mance trade-off. As shown in the fig. 7, with ε >= 1, the sentence representations are distributed
across space with patches of label-dominant clusters. However, we can not see clear decision bound-
aries between the labels. As we decrease the ε value below 1, we observe clearer feature maps for
each label. For ε = 0.001, we can see clear sentence vector clusters corresponding to label 5. We
can see the higher confusion rate is only between labels 5 and 4 that can be attributed to the less cost
of transportation of the mass from label 5 to 4 as compared to 5 to other labels. A similar trend can
be seen for lower ε values that is 0.01 and the ε used in this work 0.003 where clearer and localized
clusters can be seen.
C Model bias
C.1 Probability Skew
To generate a random input, we uniformly sample 200 tokens from the vocab 4 with replacement and
join them with white space. We obtain 100,000 such random texts. For a given text classifier model,
the skew value for sentiment label 1 can be estimated by the fraction of times it is the prediction of
when the model infers over the set of random texts (section 5.2).
C.2 Gender and Racial biases
Even though we considered the model probability skew as a reflection of bias induced from non-IID
sampling, other biases such as gender and race can still be learned or acquired in the distillation
process, For instance, take the following sentences:
My father said that the food is just fine . (review-1) → strong positive
My mother said that the food is just fine . (review-2) → neutral
Review-1 and 2 differ in gender-specific words which are father and mother. Since it is a sentiment
classification task, ideally, the intelligent system should not learn gender-specific cues from the text
to generate its predictions. However, we observe a gender dependence in both the KL divergence
and confident Sinkhorn-based predictions.
Similarly, we curate an example where the reviews differ only in a race-specific word.
4We obtain the English vocabulary of size 30,522 from:https://huggingface.co/google/
bert_uncased_L-2_H-128_A-2/tree/main.
16
Under review as a conference paper at ICLR 2022
a'山NSJ
-80	-60	-40	-20	0	20	40	60	80	-60	-40	-20	0	20	40	60
• 1 ∙2 ∙ 3 ∙4 ∙5	WE — 1
Figure 7:	With a small entropic regularizer ε, it is visually striking that Sinkhorn seems to learn the
latent structure boundaries better than KL divergence. For high ε, we see more clusters with mixed
boundaries and not so clear demarcations.
17
Under review as a conference paper at ICLR 2022
White guy said the phone is just fine . (review-1) → neutral
Latino guy said the phone is just fine . (review-2) → strong positive
The sentiment predictions made by the intelligent systems were different contrary to ideal behavior.
The review with word White shows a neutral sentiment while the review with word Latino shows
a strong positive sentiment. Hence, the systems took account of the race-specific words while pre-
dicting the sentiment of a text. The behavior is observed both in the KL and Sinkhorn-based models
with confident weights for sentiment classification.
D Gradients through loss
We demonstrate the computation of gradient of loss function (eq. (4)) with respect to the global
model trainable parameters θ. We can write the Lagrange dual of eq. (2) as
Tε def. max hμ,fi + hν,gi - εhμ ③ ν, exp (1(f ㊉ g - C)) - 1)
(f,g)∈C	(6)
C ={(f,g) ∈ Rns×nt :fi+gj ≤C(i,j)}
where f ㊉ g is tensor sum (ys,yt) ∈ Ys × Yt → f(ys) + g(yt). The optimal dual (solution
of eq. (6)) can retrieve us the optimal transport plan (solution of eq. (4)) with the relation π =
exp( 1 )(f ㊉ g - C) ∙ (μ0 V). Recently, afew interesting properties of Te were explored Peyre et al.
(2019); Feydy et al. (2019); Luise et al. (2018) showing that optimal potentials f and g exist and are
unique, and ∆Tε(μ, V) = (f, g).
Algorithm 1 Gradients of L(hθ (x), hK (x)) with respect to hθ (x)
Initialize: Dualpotentialsf1, ・…,fK ∈ Rns andg1, ・…,gK ∈ Rnt
1:
2:
3:
4:
5:
6:
7:
8:
9:
10:
11:
for k - 1 to K do
fk - 0	.fk = {fk,…，fns}
g k J 0	. g = {gl ,. .., gnt }
while ( f k, g k not converged ) do
fk — ε LSEms=I(IOg(hm(X)) + 1 gm - IC(Yi, Ym)) I Sinkhorn loop
gj 一 ε LSEm=ι(log(hm (x)) + 1 fm - ∣C(Ym, Yj))
(LSE is log-sum-exp reduction, i.e, LSEmM=1 (Vm ) = log PmM=1 exp(Vm ))
end while
end for
d(Tε(hθ(X),hk(X))) = fj	∀i ∈ [ns], k ∈ K	. as dual potentials are gradients of T£.
∂ (hiθ (x))	k	s	ε
"Lε¾⅞)亲(X))) = Pk∈κ WBk (hk(x))fj/Pk∈κ WBk (hk(x)).
Using these properties, we calculate gradients of the confident Sinkhorn cost in eq. (4). Algorithm 1
obtains the gradients of the loss function with respect to hθ(x) which can be backpropagated to tune
model parameters. A crucial computation is to solve the coupling equation at step 5 and 6. This is
done via Sinkhorn iterations which has a linear convergence rate Peyre et al. (2019).
E Statistical Risk Bounds
Without the loss of generality, we will prove the risk bounds for two local models in learning without
forgetting paradigm. For a sample x, let the output of local models be y1 = h1(x) and y2 = h2 (x)
and the global model with trainable parameters be yθ = hθ(x). To prove Theorem 5.2, we consider
the set of IID training samples S = {(x(1) , y1(1), y2(1)), . . . , (x(N), y1(N) , y2(N))}.
Lemma E.1. (from Frogner et al. (2015)) Let h^, h6* ∈ Hg be the minimizer of empirical risk RS
and expected risk R, respectively. Then
R(hθ) ≤ R(hθ*) + 2 sup lR(hθ) - RS(he)|
hθ∈Hg
(7)
18
Under review as a conference paper at ICLR 2022
To bound the risk for h^, We need to prove uniform concentration bounds for the distillation loss.
We denote the space of loss functions induces by hypothesis space Hg as
w1(y1)D(yθ,y1) + w2(y2)D(yθ, y2)
L= Vθ： (χ,y1,y2) →------------w1(y1) + w2(y2)---------∫	⑻
Lemma E.2. (Frogner et al. (2015)) Let the transport cost matrix be C and the constant CM =
max C(i j), then 0 ≤ D(∙, ∙) ≤ CM, where D(∙, ∙) is I-Wasserstein distance.
(i,j)	,
Definition E.3. (The Rademacher Complexity Bartlett & Mendelson (2002)). Let G be a family of
mapping from Z to R, and S = (z1, . . . , zN) a fixed sample from Z. The empirical Rademacher
complexity of G with respect to S is defined as:
RS(G) = Eσ
1n
gup N = σig(zi)
(9)
where σ = (σ1, . . . , σN), with σi ’s independent uniform random variables taking values in
{+1, -1}. σi ’s are called the Rademacher random variables. The Rademacher complexity is defines
by taking expecation with respect to the samples S.
Rn (G ) = ES [R s (G]	(10)
Theorem E.4. For any δ > 0, with probability at least 1-δ, the following holds for all lθ ∈ L:
E['θ] - E['θ] ≤ 2Rn(L) + JCMl”δδ.	(11)
.	_	_	.	.	一「一 r	一， 一 .	一乙一r	ʌ ,,	.
Proof. By definition E['θ] = R(hθ) and E['θ] = R(hθ). Let,
Φ(S) =SUp E['] - ES[`].
'∈L
Let S and S0 differ only in sample (x(i),y(i),y2i)), by Lemma E.2, it holds that:
φ(S)-φ(S0) ≤ SUpE S0- E S = hsθUpH N 卜1 斓)D(yP#)+w2(y2i))Dai) ,y2ii)
-wι(y(i))D(yθ,y(i)) - WMyiMy,y2i))} ≤ 2CNM
(12)
This inequality can be achieved by putting D(y(i),y(i)) = D(y(i),y2i)) = CM and D(y(i),y(i))=
D(yθ(i),y2(i)) = 0.
Similarly, Φ(S0) - Φ(S) ≤ CM/N, thus ∣Φ(S0) - Φ(S)| ≤ CM/N. Now, from the McDiarmid's
inequality McDiarmid (1998) and its usage in Frogner et al. (2015), We can establish
Φ(S) ≤ E[Φ(S)] + JlCMlgIM.
(13)
From the bound established in the proof of Theorem B.3 in Frogner et al. (2015), i.e., ES[Φ(S)] ≤
2Rn(L), we can conclude the proof.	□
To complete the proof of Theorem Theorem 5.2, we have to treat RN (L) in terms ofRN(Hg).
19
Under review as a conference paper at ICLR 2022
Now, let ι : R|Y| × R|Y| 7→ R defined by ι(y, y0) = D(s(y), s(y)0), where s is a softmax function
defined over the vector of logits. From Proposition B.10 of Frogner et al. (2015), we know:
||(y,yO)-I(y,yO)I ≤ 2CM(IIy - y||2 + ||y0 - y0||2)	(14)
Let ιs : R|Y| × R|Y| × R|Y | 7→ R defined by:
ιs(y, y1, y2)
WI(S(yI))D(S(yb s(yi D + w2(s(y2 y)D(s(y) s(y2D
w1(s(y1)) + w2(s(y2))
wι(s(yi),s(y2)) D(s(y),s(yι)) + w2(s(yi),s(y2)) D(S(y),s(y2))
(15)
(16)
where wι(.), w2(.) are confidence score of local model predictions y1,y2 on an input x. W1(.),W2(.)
are normalized scores. Note that the local model predictions, i.e., y1 and y2 are functions ofx, where
x is sampled from the data domain distribution f (x). Hence, we can view the loss function as
ιs(y, y1, y2) = D(S(y), S(y1)) + D(S(y), S(y2))	(17)
= ι(y, y1) + ι(y, y2).	(18)
where y is a function of Xnew sampled from a weighted distribution W1(s(y1), s(y2))f (x).
The Lipschitz constant of ιs(y, y1, y2) can thus be identified by:
lιs(y,yι,y2) - ιs(y,yι,y2)1 =	||(y,yi) + ι(y,y2) - I(WgI) - ι(y,y2)|	(19)
≤	||(y,yi) - ι(y,yι)| + ||(y,y2) + ι(y,y2)|	QO)
≤	2Cm(IIy - y||2 + ||yi - yi||2 + ||y - y||2	+ ||y2 - y2||2) QI)
≤	4Cm(IIy - yII2 + "yι - y1U2 + Uy2 - yzg)	(22)
≤ 4CmII(y,yi,y2) - (y,yi,y2)II2	(23)
Thus, the Lipschitz constant of plain Sinkhorn based distillation is 4CM .
Proof of Theorem 5.2 We define the space of loss function for k local models:
Following the notations in Frogner et al. (2015), we apply the following generalized Talagrand’s
lemma Ledoux & Talagrand (2013):
Lemma E.5. Let F be a class of real functions, and H ⊂ F = F1 × . . . × FK be a K -valued
function class. If m : RK 7→ R is a Lm-Lipschitz function and m(0) = 0, then RS(m ◦ H) ≤
2Lm PK=I RS (Fk )∙
Now, as the Lemma can not be directly applied to the confident Sinkhorn loss as 0 is an invalid input.
To get around the problem, we assume the global hypothesis space is of the form:
H = {s ◦ h : ho ∈Ho}
(24)
Thus, we apply the lemma to the 4CM -Lipschitz continuous function l and the function space:
Ho × . . . × Ho ×I × . . . × I
、-------{-------‘、------V-------}
∣Y∣copies	∣Y∣×∣K∣ copies
with I a singleton function space of identity maps. It holds:
. ʌ . . ʌ . . . ʌ ,
RN(L) ≤ 8cm(UIRN + IYI X IKIRN(I)) = 8IYICMRN(Ho)	(25)
20
Under review as a conference paper at ICLR 2022
As,
1N	1N
RN(Z) = Eσ 卜Up N X σig(yi) = 0 = Eσ [n X σiyi = 0]
g∈I N i=1	N i=1
Thus, by combining eq. (25) in Theorem E.4 and Lemma E.1 proves the Theorem 5.2.
Since the ε is small in our experiments, we can quantify the difference between Sinkhorn distance
and Wasserstein for a given Lipschitz cost function.
F S ocietal Impact
Our work can be extended to different domains. Although in this paper we examined sentiment
classifications, other areas, where labels are not available, i.e., zero-shot classification, would also be
amenable to federated confident Sinkhorns. Within our approach, a potential downstream task could
be to detect cyberbullying. An important area of application for distraught parents, school teachers
and teens. In this case, a sentiment that has a high probability of been classified as cyberbullying
can be flagged to either moderators or guardians of a particular application.
A weakness of this approach is that the training of such an application will be based on local models
in other domains. Care would be needed in deciding which local models to use in the federation. This
choice is highly dependent on the industry and the availability of data. Misuse of our approach could
be that the federated training might distill some population-specific information to the global model
which makes the central system vulnerable to attacks that might lead to a user-private data breach.
As GPU implementation of OT metrics becomes commonplace, we envisage that our approach
might help in other Natural Language Processing (NLP) tasks. Indeed, this would be potentially
beneficial and open up new avenues to the NLP community.
G Experimental reproducib ility
All the experiments were performed on one Quadro RTX 8000 GPU with 48 GB memory. The model
architecutres were designed on Python (version 3.9.2) library PyTorch (version 1.8.1) under BSD-
style license. For Sinkhorn iterations and graident calculations, we use GeomLoss library (version
0.2.4) (https://github.com/jeanfeydy/geomloss) under MIT licence. For barycenter
calculations, we use POT library (0.7.0) from (https://pythonot.github.io/) under MIT
licence.
We clip all the text to a maximum length of 200 tokens and pad the shorter sentences with jug.
To speed up the experiments, we use pretrained BERT-Tiny from https://github.com/
google-research/bert.
The batch size is chosen via grid search from the set {16, 32, 64, 128, 256, 512, 1024, 2048}
and found 1024 to be optimal for performance and speed combination on the consid-
ered large datasets. We use Adam optimizer with learning rate chosen via grid search
{10-4, 10-3, 10-2, 10-1, 1, 101, 102, 103, 104}. All the experiments were ran for 20 epochs. The
regularization parameter ε is chosen based on minimal loss obtained amongst the set of ε values
{10-4, 10-3, 10-2, 10-1, 1, 2, 4, 8, 16}.
For the Amazon review dataset, we were unable to find the licence.
21