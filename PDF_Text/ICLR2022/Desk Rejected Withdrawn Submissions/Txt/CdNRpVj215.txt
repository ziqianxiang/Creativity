Under review as a conference paper at ICLR 2022
Towards Non-Parametric Models for Confi-
dence Aware Video Prediction on Smooth Dy-
NAMICS
Anonymous authors
Paper under double-blind review
Ab stract
The ability to envision future states is crucial to informed decision making while
interacting with dynamic environments. With vision providing an information
rich sensing modality, the field of video prediction has garnered a lot of attention
in pursuit of this ability. Current state of the art methods rely on neural network
based models for prediction. Though often accurate, these methods require large
amounts of training data that are often unavailable when encountering unknown
environments. The predictive accuracy of such methods also breaks down without
warning, when tested on data far outside their training distribution. This problem
is exacerbated by the fact that these networks can be prohibitively expensive to
update with recent data acquired online. To overcome these drawbacks we use
non-parametric models to take a probabilistic approach to video prediction for
problems with little training data. We generate probability distributions over se-
quentially predicted images and propagate our uncertainty through time to gener-
ate a confidence metric for our predictions. We use non-parametric Gaussian Pro-
cess models for their data efficiency and ability to readily incorporate new training
data online. To showcase our method we successfully predict future frames of a
smooth fluid simulation environment.
1	Introduction
The ability to understand scenes and predict their future states is key to enabling smart decision
making. Predictions on large scale phenomenon, like weather, enable life saving preventative mea-
sures such as evacuation warnings. As humans we utilize small scale predictions to inform our daily
actions like navigating through a crowds, and catching falling objects. Perfectly predicting such
diverse phenomena requires unique and complex models that rely on underlying state information.
However, humans are often able to make decisions using imperfect models that are informed solely
from visual input. We are able to readily interact with constantly changing, new environments, after
only a small number of observations. In this paper we focus on the task of video prediction given a
limited number of initial frames for both context and training.
Current state of the art approaches to visual prediction rely on neural networks Oprea et al. (2020).
These methods use a vast amount of resources and data to train. Neural Networks are high or-
der parametric models composed of up to millions of parameters, which are incrementally refined
through evaluation against a loss metric on training data Goodfellow et al. (2016). The high parame-
ter count increases model complexity, giving these networks the representational power to accurately
capture a wide array of visual dynamics. Despite the predictive accuracy, highly parameterized mod-
els are challenged when faced with data outside their training distribution. Though this behavior is
expected, the large amount of data and computational resources required to re-compute and refine
millions of parameters, prevents these models from being easily updated when encountering new
data outside the current training distribution. Often times these deep models often fail to recog-
nize that their inputs are out of distribution. We overcome these limitations with our probabilistic
approach to video prediction, using non-parametric Gaussian Process models.
Gaussian Processes (GPs) provide a non-parametric data driven approach to function approximation.
They use a Gaussian assumption on the data points to extrapolate predictions from the training data.
1
Under review as a conference paper at ICLR 2022
Ground
Truth
Predicted
Mean
10
t = 10
t = 14
t = 19
t = 24
ɪ
2.2
2.0
J-0.5
10.0
-0.5
-1.0
Predicted
Variance
10
1
1
.8
.6
..4
i-1.0
-0.5
-0.0
--0.5
--1.0
I 1.Q
I Q.5
1-0.0
1--0.5
I--I-O
O0.00U>	∏-0.00*5
■ J-0.0035
d-00°6	卜 MO
0.0004	I1
- 0.00041
■ 0.0025
(a) Forward Prediction: Image Results
0.20
0.15-
0.10-
0.05
0.25
o.oo
,IRU 8≥le-9M
10 12 14 16 18
Time steo
s=0 SPIS Css
^10^^1214^
L	小
16 IB 20 22
Time steo
off
Figure 1: Forward Prediction Experiment: Our model, trained using frames [z0 , . . . , z9], is used to
predict frames [z10, . . . , z24] of a 2D Navier Stokes simulation. Fig. (1a) shows the ground truth
and predicted mean and variance images. Fig. (1b) and Fig. (1c) show graphs of the relative error
and mean standard deviations off between the predicted mean and ground truth images over the
predicted time steps. These results showcase our model’s ability to learn complex dynamics and
predict compelling distributions on future frames that capture the ground truth.
This enables high quality predictions near the training distribution, which can be readily expanded to
include newly observed data points. The predictions from these models are probability distributions,
which provides an interpretable metric for the model’s confidence on its prediction. We use GPs as
the core predictive component of our method, where we propagate our predicted distributions on
images through time to generate a confidence metric along side our predictions. Visual prediction
is a good candidate for a low data non-parametric approach as videos provide a very information
rich sensing modality. Short sequences of video contain a large amount of semantic and dynamic
information that can be used to understand unseen environments. In many videos, such as satellite
weather patterns, fluid motion, and driving cars, the motion can be repetitive, both temporally across
different frames, and spatially within the same frame. We take advantage of this repetitiveness to
train a high quality model using only a few frames of data.
In this paper we contribute towards the use of non-parametric, data driven, methods for confidence
aware short term video prediction. We restrict the scope of our video predictions to the problem
of predicting 2D video sequences of smooth navier stokes dynamics and discuss the challenges in
extending this method to general non-parametric video prediction. We also make key assumptions
that the video frames capture a majority of the state information pertinent to predicting future states.
2	Related Works
The problem of video prediction is presented in several different settings, with methods being trained
and evaluated on a variety of different datasets. Real world video sequences capture the motion of
discrete objects. Toy problems, such as in the bouncing ball dataset (Sutskever et al., 2008) and
moving MNIST dataset (Srivastava et al., 2016) have been used to evaluate a method’s ability to
track such distinct passively moving objects in video. The Robot Pushing Dataset (Finn et al., 2016)
provides a dataset for problems involving robot interaction in video prediction and robot control.
There is also a large body of work on predicting motion of autonomous agents. This is prominent in
research on self driving cars where cars and pedestrians act as autonomous agents. Commonly used
datasets for this problem include the Kitti dataset (Geiger et al., 2013), CamVid Dataset (Brostow
et al., 2008) and Caltech Pedestrian Dataset (Dollar et al., 2009).
To solve most of the problem settings above, researchers create predictive models using neural net-
works. The success of neural networks on image based applications has lead to their widespread
adoption in fields such as video prediction. Most video prediction works build off of a few baseline
neural network architectures: convolutional, recurrent and generative models. Convolutional Neu-
2
Under review as a conference paper at ICLR 2022
t = 14	t = 19	t = 24	t = 29
Ground
Truth
o
I J∣r I
(a) Ground Truth Image Frames
Predicted
Mean
5
Predicted
Mean
10
Predicted
Mean
15
1
-O
--1
1
-O
--1
1
-0
--1
19
1
-O
--1
1
-O
--1
1
-0
--1
(b) Sequence Prediction Mean Images
0.6
0.5
0.4
0.3
0.2
0.1
0.0
5
10
15
20
25
30
Timestep
(c) Sequence Prediction Errors

Figure 2: Sequential Prediction Experiment: In this experiment our model, trained using frames
[z0, . . . , zt0-1], is used to predict frames [zt0 , . . . , zt0+15] of a 2D Navier Stokes simulation. We
show the results of such predictions for t = 5, 10, 15 along the rows of 2b, respectively. Fig. (2a)
displays ground truth images. Fig. (2b) shows all the predicted mean images. Fig. (2c) displays
a graph of the relative error between the predicted mean and the ground truth images. In 2c we
show additional error results in which we start predictive rollouts with the models trained with 5, 10
images from later time steps. This provides a fair comparison to analyze the effects of adding data.
The decrease in error with each new model shows the value of incorporating recent data into our
model.
ral Networks (CNNs) that rely on 2D convolutional kernels enabled a breakthrough on challenging
problems in the image domain (O’Shea & Nash, 2015). Works including Wang et al. (2019), Aigner
& Korner (2018), Vondrick et al. (2016), add a third dimension to their convolutional kernels to in-
corporate information across time. Recurrent Neural Networks (RNNs) (Rumelhart et al., 1986) and
LSTMS (Hochreiter & Schmidhuber, 1997) provide a more principled way of incorporating tempo-
ral dependence into network architectures. Explicitly designed to handle sequential data, these meth-
ods have been widely embraced across several video prediction works (Chen et al., 2017), (Wichers
et al., 2018), and (Walker et al., 2017). The above approaches model the output images conditioned
on the given input frames. Generative models, like Variational Auto Encoders (VAEs) (Kingma &
Welling, 2014) and Generative Adversarial Networks (GANs) (Goodfellow et al., 2014), seek to
model the joint distribution between the input and output frames. VAEs utilize embeddings into an
underlying latent space. Several methods attempt to utilize this latent embedding for video predic-
tion, and to capture the inherent uncertainty present in the task of future prediction (Bhattacharyya
et al., 2019), (Babaeizadeh et al., 2018), (Denton & Fergus, 2018), and (Fragkiadaki et al., 2017).
This uncertainty often creates blurry images (Oprea et al., 2020). Several works including Mathieu
et al. (2016), Wichers et al. (2018), and Villegas et al. (2018) use GANs and take advantage of their
adversarial training methodology to improve the sharpness and realism of their predicted frames.
These baseline techniques are used in a variety of approaches to video prediction. Srivastava et al.
(2016), Yu et al. (2020) and Byeon et al. (2018) directly synthesize pixels in the future frames.
This methodology often relies on capturing pixel patterns over understanding underlying states and
dynamics. Reda et al. (2021), Michalski et al. (2014) and Klein et al. (2015) synthesize output pixels
as explicit functional transforms of the input pixels. We take inspiration from this approach of direct
pixel synthesis and convolutional models in designing our method. Another approach to prediction
involves disentangling and separately predicting components of the video, such as high level motion
and lower level details. Denton & Birodkar (2017), Gao et al. (2019), and Hsieh et al. (2018)
adopt this methodology. Some works choose to focus on prediction in a high level feature space,
rather than render every pixel. Walker et al. (2017), and Tang et al. (2019) predict in the domain
of low dimensional human poses, while Minderer et al. (2020) predicts in a general keypoint based
3
Under review as a conference paper at ICLR 2022
representation. These high level representations and other features, such as control values, have
been incorporated into inputs for better predictions (Oh et al., 2015), (Finn et al., 2016).
There is also a large body of work on predicting and simulating fluids. The motion of incompressible
fluids is governed by the Navier Stokes equations, a set of partial differential equations (PDEs).
Traditionally, complex PDEs are solved using Finite Element Methods (FEMs) and Finite Difference
Methods (FDMs). These methods work by approximating solutions on a discretized grid. Modern
techniques such as Raissi et al. (2019), Jiang et al. (2020), Greenfeld et al. (2019) and Li et al. (2021)
use neural networks to learn to solve complex PDEs directly from data. These works build off of the
baseline architectures discussed above, with modifications to make them better suited for the task
at hand. These works can give valuable insights into certain video prediction tasks like predicting
satellite weather patterns. Using neural networks to predict weather from satellite imagery with
other relevant inputs, has been explored by S0nderby et al. (2020) and Klein et al. (2015).
Neural Networks require a lot of data and computational resources to train. Li et al. (2021) utilizes
several thousand training images to train a predictive model for its easiest simulation parameter-
ization, requiring far more data to capture more complex dynamics. Certain architectures such as
GANs can be quite unstable and difficult to train (Oprea et al., 2020). Network architectures must be
hand tailored to fit the specific problem at hand, and often fail without warning on data far from the
training distribution. These drawbacks prevent neural networks from being used in problems where
data is scarce. We turn to non-parametric methods, in particular Gaussian Processes (GPs), for video
prediction with little data. GPs have been used to estimate highly nonlinear functions with proba-
balistic confidence using little training data. They have been widely used in Robotics to learn in-
verse kinematic mappings (Wilcox & Yip, 2020), model unknown safety functions (Turchetta et al.,
2016), (Turchetta et al., 2019) and to model and propagate uncertainty through the dynamics of a
robotic system (Deisenroth & Rasmussen, 2011). In this work we will show how this non-parametric
method can be used for short term video prediction in smooth dynamic fluid environments.
3	Background: Gaussian Processes
The core predictive component of our method uses a Single Output Gaussian Process Regres-
sion Model. A GP is used to model a function f, using training data (X, f(X)). X =
[x0, x1, . . . , xn-1] ∈ Rn×D are all the training inputs and f(x) = [f(x0), . . . , f(xn-1)] ∈ Rn×1
are the training outputs. Given test inputs X0 ∈ Rm×D we want to find outputs f(X0). Let
XA ∈ R(m+n)×D refer to all the train and test inputs and f(XA) be the corresponding outputs.
A GP relies on the assumption that all the outputs are characterized by a multivariate gaussian dis-
tribution f (XA)〜 N(μ(Xa), ∑XaXa ). We assume the mean μ(Xa) = 0, and the covariance
matrix is characterized by a kernel function k(x, y) such that ΣXA,XA [u, v] = k(XA[u], XA[v]).
To solve for the distribution of the test outputs p(f (X ))〜N(μ(X ), Σχoχo) We use the marginal
likelihood ofa multivariate gaussian p(f(X0 |X, f(X), X0)) to get:
μ(X 0) = k(X 0 ,X )[k(X,X)+ σn I ]-1f (X)	(1)
ΣX0X0 =k(X0,X0)-k(X0,X)[k(X,X)+σn2I]-1k(X,X0)	(2)
k(X, X)[u, v] = k(X[u],X[v]), k(X, X) ∈ Rn×n, k(X0, X)[u, v] = k(X0[u], X[v]), k(X0, X) =
k(X, X0)T ∈ Rm×n, σn2 is the noise variance, I ∈ Rn×n is the identity matrix. To train the GP
Regression model We optimize the noise variance, σn, and kernel parameters to maximize the log
likelihood of the training data.
In this paper We utilize the Radial Basis Function (RBF) kernel for all our methods:
k(x, y) = α2 exp(- (X - y)	(X - y))	(3)
The kernel parameters, α and Λ, are optimized during training. To predict outputs of dimension
O > 1, We train a separate GP model for each output dimension a ∈ [0, O - 1]. Each model has its
oWn kernel ka(., .) and is trained by optimizing its noise and kernel parameters. σn,a, αa and Λa.
4
Under review as a conference paper at ICLR 2022
Figure 3: Out of distribution variance experiment: Our model is trained with frames [z0, . . . , z9] of a
2d Navier Stokes simulation. This model is used to predict 15 frames starting from the input images
[z82 , z83 , z84], which generate inputs far from the model’s training distribution. The increase in the
magnitude of the predicted variance, compared to the ‘In distribution’ predictions shown in fig. 1a,
as well as low mean standard deviations off over the predicted time steps demonstrates our model’s
understanding of its failure modes. The model adjusts its variance, recognizes the break down in the
predictive accuracy of its mean images to capture the ground truth within its predicted distributions.
4	Method
4.1	Problem Statement and Prediction Framework
A video is defined as a sequence of images or frames [z0, z1 . . . zt0-1, zt0 , zt0+1 . . . zt-1, zt . . . ].
Here zi ∈ RQ×Q denotes the i-th frame in the sequence. Given initial training frames [z0, . . . zt0-1],
we wish to predict frames [zt0 , . . . zt, . . . ]. We restrict the problem to the regime of new, unseen
environments by limiting frames [z0 . . . zt0-1], to be the only training data. As additional frames
[zt0, . . . zt0] become available, they may be incorporated into the model’s training data to improve
the accuracy of future predictions.
In our method, we train a model to use re-occurring motion patterns to understand scene dynamics
for video prediction. We have our model learn and predict on square image patches of dimension
(p, p). This smaller patch scale enables us to better utilize our limited training data and extract
smaller repeating patterns that are more likely to re-occur across space and time. Our method pre-
dicts one frame at a time given the 3 most recent, seen and predicted, frames. We use 3 frame inputs
to capture second order dynamics. We first convert the inputted image frames into patches and then
suitable inputs for our GP Regression models. The models output distributions on pixels in future
images, that are combined to form a random variable image. This predicted image is incorporated
into the next set of inputs and the process is repeated. As such our models must handle a combination
of random and known inputs, while propagating probability distributions through time.
4.2	Training
To construct our model, we begin by creating a training data set from frames [z0, . . . , zt0-1]. We
divide the images into sets of 4 sequential images [zi, zi+1, zi+2, zi+3], i ∈ [0, t0 - 4]. To create a
datapoint we take p dimensional patches corresponding to the same pixel locations from each image.
zi[k : k + p, l + l + p] ∈ Rp×p denotes ap by p patch in image zi starting at pixel (k, l). A training
input, xj ∈ R3p2 , is created by flattening and concatenating the patches from the first 3 images. The
corresponding training output, f(xj) ∈ R(p-b)2, is created by flattening the corresponding patch
from the 4th image zi+3, cropped with a patch boundary term b: zi+3[k + b : k + p - b, l + b : l +
p - b] ∈ R(p-b)×(p-b). When b > 0, our model does not predict the outer edges of the patch, where
predictions may suffer due to contributions from the scene outside our input. Within each set of 4
5
Under review as a conference paper at ICLR 2022
t=10
-2
t=14
-2
t=19
-2
t=24
-2
Ground
Truth
-0
-1
—2
∙IOJteω>sra-ωκ
0.0
10 1'2 14 1'6 18
Tlmesteo
Predicted
Variance
Predicted
Mean
1
-0
-1
—2
-Q∙β01Q
Q∙β
-o.oooβ
・0.6
-0.0006
(a) High Error Experiment: Image Results
-2
1
-0
-1
—2
-2
0Λ3S
■ 0.030
0.025
0.55
0.50
C.60
toωpsωσEω⅞
0.45
(b) Relative Error Graph
(c) Mean Standard Deviations off
[	I	1I	I
1----------■
1
Figure 4: High Error Experiment: This figure shows an instance where our predictions have a higher
relative error. Our model is trained using 10 frames [z0 , . . . , z9] is used to forward predict the next
15 frames [z10 , . . . , z24] of a 2d Navier Stokes simulation. Fig. 4a shows the ground truth and
predicted mean and variance images Fig. (4b) and Fig. (4c) show graphs of the relative error and
mean standard deviations off between the predicted mean and ground truth images, respectively.
These results demonstrate how our model increases its variance to decrease its predictive confidence
when it incurs high relative error, allowing it to keep the mean standard deviations off low.
sequential images, we sample data points with a stride of s pixels in the x and y dimensions. In this
paper, we utilize a ’wrapping’ approach to handle patches that extend beyond the edge of an image.
This approach assumes the frame, zi with g rows and f columns, captures a periodic environment
such that z[g + i, f + j] = z[i - 1, j - 1] and z[-i, -j] = z[g - (i + 1), f - (j + 1)]. Approaches
like zero padding frames to indicate boundaries or skipping incomplete patches are possible but
not further explored in this paper. We repeat this sampling procedure for every set of sequential
images to create the training data set with n data points: (X, f(X)) = (xj , f (xj)), j ∈ [0, n - 1].
X ∈ Rn×3p2 and f(X) ∈ Rn×(p-b)2. This process is shown graphically in Figure 7 in A.6
We create a GP Regression model for every output dimension O = (p - b)2. Each model is trained
by optimizing its noise, σn,a, and kernel parameters, αa and Λa in ka (., .), to maximize the log
likelihood of the training data for output dimension a ∈ [0, (p - b)2 - 1]. To predict future images,
each GP model outputs a mean and variance corresponding to a single pixel in the output patch. The
predicted image zi, is represented with a mean and variance image pair (Mi, Vi). Each pixel in Mi
and Vi corresponds to the mean and variance of the predicted random variable for that pixel location,
respectively. We ensure that every future image pixel is predicted exactly once.
4.3	Prediction
Once trained, we can use our model to rollout predictions for any T time steps into the future,
starting from 3 known, consecutive input images [zi, zi+1, zi+3]. We use a recursive method to
predict multiple frames into the future. Our model takes the 3 most recent seen or predicted frames
and uses them to predict one frame into the future, which is represented as a random variable. We
incorporate this predicted random variable as the latest image in the 3 frame inputs to predict the
next time step. This process is repeated to predict the desired T time steps into the future. We start
discussing our predictions in the context of predicting the fourth time step and onwards. Starting
at the fourth prediction all the input images utilized are random variables previously outputted by
our model. The first three predictions incorporate known, observed input images in addition to the
predicted random input images. These initializing predictions will be discussed as a special case of
the more general prediction from all random variable input images.
We discuss the general method of predicting zi+3 from input images [zi, zi+1 , zi+2], that are all
random variables outputted by our model. To predict zi+3 , we first create a set of m test inputs
x0j , j ∈ [0, m - 1]. Each test input is a multivariate gaussian random variable composed of 3p2
6
Under review as a conference paper at ICLR 2022
independent gaussian random variables sampled from the input images. Since the input images are
predicted random variables, our test inputs are created by sampling their corresponding mean and
variance images: [(Mi, Vi), (Mi+1 , Vi+1), (Mi+2, Vi+2)]. We make simplifying assumptions that
the predictions of each GP model as well as the outputs of the same model on different inputs are
independent. Without assuming independence, we would have the computationally intractable task
of tracking the covariance values across all pixels. As a result, the predicted images and their sub-
patches can be flattened, concatenated and represented as one multivariate gaussian random variable.
We use the patch based sampling method described in Section 4.2, separately, on the sets of consecu-
tive mean and variance images to generate the mean and variance test input vectors, x∙^ ∈ R3p and
x0j,σ ∈ R3p2 respectively. These vectors specify the multivariate gaussian distribution of the input
Xj 〜N (χjμ, Σχ∕ ). To construct fχ , We use our simplifying independence assumptions such
that the input covariance is a diagonal matrix with the vector of variances, x0j,σ, along the diagonal.
We adjust our sampling stride to generate an input to predict every pixel in the future image.
We discuss our method in the context of predicting a single output dimension a ∈ [0, (p - b)2 -
1] from a single input x0j . Our model output is the predicted random variable f (x0j )[a]. As in
standard Gaussian Process Regression, We are solving for the distribution ofp(f(x0j)[a]). We solve
forp(f(x0j)[a]) by marginalizing p(f (x0j)[a]|X, f(X), x0j) over the input images.
p(f (x0j)[a]) =	p(f (x0j)[a]|x0j, X, f (X)[:, a])p(x0j)dx0j
-∞
(4)
Solving this integral is analytically intractable. We approximate the posterior distribution from 4
to be Gaussian. Having the outputs form a multivariate gaussian, like the inputs, enables the pre-
dictions to be recursive. To solve for p(f (x0j)[a]) We take advantage of this assumption and use
moment matching in a method akin to Deisenroth & Rasmussen (2011). HoWever, our method is
distinguished from Deisenroth & Rasmussen (2011) in its use of multiple past states as inputs, pre-
diction on images, and incorporation of knoWn states in its inputs. Moment matching enables us to
directly solve for the mean μ(xj)[a] and variance Σ(xj)[a, a] of the outputted Gaussian distribution.
This gives us the folloWing formula to predict the mean of an output pixel from all random inputs:
μ(Xj)[a] = dT Ba
da[i] = αα ∙ (%"+ι∣)-1 ∙ e-2VT3x」a)j	⑸
βa = [ka(X, X) + σn2I]-1f(X)[:, a]
Here d& ∈ Rn, and Vi = Xi - x；*. To predict the variance from all random inputs we use:
∑(xj)[a, a] = Oa - trace ((k0(X,X) + σ2,aI)-1 Qaa) + βTQaaea - μ(xj)[a]	⑹
0	o	1	z Z√ R R ∑ 0 Zik	/r`
Qaa[i,k] = ka(Xi,Xj)ka(Xk ,Xj ) ∙ |R| -2 ∙ e	xj,σ	⑺
Here Qaa ∈ Rn×n, R = Σx0 2Λa-1 +I ∈ Rn×n and zik = Λa-1vi+Λa-1vk. A walkthrough deriva-
tion of these equations, using moment matching, is discussed in the appendix A.2. These equations
are used on all the test inputs to predict the mean and variance for every pixel in zi+3 . The final
predicted image zi+3 is stored as a mean, variance image tuple: (Mi+3, Vi+3). To continue the pre-
dictive rollout we incorporate this latest prediction into a new set of input images [zi+1, zi+2, zi+3]
to predict zi+4 . The predicted mean images [Mi , . . . , Mi+3 , . . . ] act as our predicted estimates of
the ground truth, while the predicted variance images [Vi, . . . , Vi+3, . . . ] act as a confidence measure
on our prediction. This is explored more in Section 5
In the first three predictions some or all of the input images are known, observed images. These
predictions are initializing steps to begin the predictive rollout. The initializing predictions are
special cases of the general prediction formulation with all random variable inputs. In this case we
still treat all components of our input as random variables. We no longer treat the whole input as a
single multivariate gaussian. We disentangle the predictive method into parts that solely interact with
the input dimensions contributed from the observed images, and those contributed from the predicted
random variable images. We treat the random variable component of the input as a multivariate
7
Under review as a conference paper at ICLR 2022
gaussian and use a delta function at the observed values as the joint probability distribution for the
known component.We use these representations to re-derive the moment matching formulas. We
walk through this method in A.1 of the Appendix. Figure 8 is an overview of the whole method.
5	Experiments and Results
In this work we test our methods by predicting the vorticity of an incompressible fluid on a unit torus
environment. Our data is computed using two dimensional Navier Stokes Equations. We generate
our data with traditional PDE solvers using the code and approach detailed in Li et al. (2021). These
fluid simulation lack discrete objects and generate frames whose pixels change smoothly across both
space and time. This smooth environment is well suited to our method’s choice of the RBF Kernel.
The dynamics of the toroidal environment wrap around the edges of the video frame. As a result,
we utilize the ”Wrapping” approach when handling edge patch creation. Each pixel is represented
by a single float whose value is centered around 0. As a result, we can directly predict future pixels
using a gaussian process model that assumes its outputs have zero mean. In the generated video
sequence, frame zt ∈ RQ×Q represents the vorticity of the fluid at time-step t. We fix the frame
resolution at Q = 32, and simulate with a viscosity of 1e - 3 for all our simulated data. For all our
experiments we train our model to take square input patches of dimension p = 15 and output (1, 1)
patches using an output patch border b = 7. These parameters result in our model using a single GP
Regression model to predict a single output dimension. To generate the data, for all experiments,
we use a training stride of s = 2 pixels in both the x and y dimensions. A test stride of s = 1 is
used to generate test inputs to predict a distribution for every pixel in the future image. We predict
15 future frames for every experiment. The training images and the set of input images used to start
the predictive rollouts are specified for each experiment.
We introduce two metrics: Relative error (Li et al., 2021) (RE(z, Z)) and Mean Standard Deviations
off (StdE(z, Zμ, Zσ)) to analyze the performance of our model.
RE(z,z) = l∣z - z∣∣2 ∙∣∣z∣l-1	⑻
In (8) X ∈ RQ × Q denotes the ground truth image, X ∈ RQ × Q denotes the predicted image and ||. || 2
is the 2 norm of a matrix. This normalizes the error with the magnitude of the original image
StdE(Z,zμ ,zσ ) = Q12 X1X1 (j⅛F
(9)
z ∈ RQ×Q is the ground truth image, Zmu, Z@ ∈ RQ×Q is the predicted mean and variance images
outputted by our model, and |.| is the absolute value function. This metric returns the average
absolute standard deviations between our model’s predicted mean image and the ground truth. This
metric is inversely proportional to the how closely the predicted distribution captures the truth.
Forward Prediction Experiment: In this experiment, we train our model using the first t0 = 10
frames of a video [Z0 , . . . Z9]. This model is used to predict the next 15 frames [Z10 , . . . , Z24], from
input images [Z7, Z8, Z9]. The results of this experiment are shown in Figure 1. Our model’s predicted
mean images track the complex dynamics of the ground truth with very little training data. The low
magnitude of the variance images in Figure 1a indicates our model’s confidence in its predictions.
This confidence is well founded given the low relative error in Figure 1b and low StdE in Figure 1c
which illustrate our ability to predict distributions that capture the ground truth.
Sequential Prediction Experiment: Having established our model’s predictive capabilities, we
examine the benefits of incorporating recent data into our model. In this experiment we incremen-
tally train models with the first 5, 10 and 15 images of a video sequences. Each of these models is
used to predict 15 frames into the future, starting from their last training image. In Figure 2 we can
visually see the improvement in prediction accuracy as we update our models. In the relative error
graph (Figure 2c) we also show the results of starting predictive rollouts for the lower data models,
trained with 5 and 10 images, later in the sequence. This provides a fair evaluation to compare the
impact of adding recent data, by mitigating the added error compounded as a result of the predictive
rollouts. The graph shows a large improvement in accuracy as we incorporate data into our model.
Variance Experiments: The variance outputted by our model acts as confidence measure around
the value of the corresponding predicted pixel mean. This variance is based on a kernel based
8
Under review as a conference paper at ICLR 2022
t=14
(b) Predictive Comparison Errors
Figure 5: Predictive Comparison Experiment: This figure compares the predictions on Navier Stokes
simulations from our method to the neural network based methods, FNO-2d-time and FNO-3d,
discussed in Li et al. (2021) trained on similarly low data. Details are discussed in section A.4.
(a) Predictive Comparison Images
metric evaluating the similarity between the model’s training data and the test input. To understand
the variance output we look at a predictive rollout started with inputs far from our model’s training
distribution. We use the same model trained in the ’Forward Prediction’ experiment to predict frames
[z85, . . . , z99] from input images [z82, z83, z84]. Since our environment changes slowly and smoothly
over time, a large temporal gap between frames increases distance with respect to our kernel based
similarity measure. Looking at the results in Figure 3 we notice that despite the high relative error, in
Figure 3b, the mean standard deviations off (StdE) in Figure 3c remain low. The model recognizes
that its inputs are too far from its training distribution to make a confident prediction and outputs
higher variance values. This increase in variance magnitudes is very noticeable when contrasted
with the variances in Figure 1a, where the inputs start close to the training distribution. We can
better understand the variance by analyzing our model’s failure modes. Figure 4 shows the results
of a ’Forward Prediction’ experiment on a sequence where the predicted mean images had a higher
error. Recognizing the decrease in predictive accuracy, the model increases its variance to track the
increase in error, keeping the StdE low. The initial jump in this and other mean standard deviations
off graphs indicates an initial overconfidence that the model quickly corrects for. Towards the end
of longer predictive steps the StdE decreases despite an increasing relative error, as the variance
outputs balloon to reflect the model’s decreased confidence. These results demonstrate our model’s
ability to understand its own failure modes with a interpretable confidence metric.
Predictive Comparison Experiment: We compare our model’s performance to the FNO-2d-time
and FNO-3d neural network methods presented in Li et al. (2021). All methods are trained using
a similarly low number of training images. This experiment serves solely as a visual comparison.
The use cases of the contrasted models are widely different, with Networks not being suited to little
training data. The results in Figure 5 show our model outperforming the neural network methods.
Both networks fail to learn the complex dynamics from such little data, outputting either identical
frames or noise. Details on the experiment are in section A.4 of the appendix.
6	Conclusion and Discussion
In this paper we provided a novel method using non-parametric GP models to predict probability dis-
tributions over future video frames in smooth dynamic environments with limited training data. Our
use of the RBF kernel and direct prediction of future pixels caters towards the smoothness of the zero
mean images of our test environment. We can expand our prediction equations to other kernel equa-
tions and predict zero mean image differences to generalize our method beyond smooth dynamic
environments. Another future direction is to find methods to efficiently track covariances between
pixels, to increase the accuracy of our distributions for longer term predictions. The problem of
accurately learning to accurately predict in any environment with little data is very difficult. We
seek to expand upon our work by combining our method with parametric learned approaches. Such
a combination can leverage the predictive accuracy of neural networks and switch to non-parametric
approaches when encountered with new environments far from the network training distribution.
9
Under review as a conference paper at ICLR 2022
References
Sandra Aigner and Marco Korner. Futuregan: Anticipating the future frames of video sequences
using spatio-temporal 3d convolutions in progressively growing gans. arXiv: Computer Vision
and Pattern Recognition, 2018.
Mohammad Babaeizadeh, Chelsea Finn, Dumitru Erhan, Royhttps://arxiv.org/help/api/index H.
Campbell, and Sergey Levine. Stochastic variational video prediction, 2018.
Apratim Bhattacharyya, Mario Fritz, and Bernt Schiele. Bayesian prediction of future street scenes
using synthetic likelihoods, 2019.
Gabriel J. Brostow, Jamie Shotton, Julien Fauqueur, and Roberto Cipolla. Segmentation and recog-
nition using structure from motion point clouds. In David Forsyth, Philip Torr, and Andrew
Zisserman (eds.), Computer Vision - ECCV2008, pp. 44-57, Berlin, Heidelberg, 2008. Springer
Berlin Heidelberg. ISBN 978-3-540-88682-2.
Wonmin Byeon, Qin Wang, Rupesh Kumar Srivastava, and Petros Koumoutsakos. Contextvp: Fully
context-aware video prediction, 2018.
Xiongtao Chen, Wenmin Wang, Jinzhuo Wang, and Weimian Li. Learning object-centric trans-
formation for video prediction. In Proceedings of the 25th ACM International Conference
on Multimedia, MM ’17, pp. 1503-1512, New York, NY, USA, 2017. Association for Com-
puting Machinery. ISBN 9781450349062. doi: 10.1145/3123266.3123349. URL https:
//doi.org/10.1145/3123266.3123349.
Marc Peter Deisenroth and Carl Edward Rasmussen. Pilco: A model-based and data-efficient ap-
proach to policy search. In Proceedings of the 28th International Conference on International
Conference on Machine Learning, ICML’11, pp. 465-472, Madison, WI, USA, 2011. Omnipress.
ISBN 9781450306195.
Emily Denton and Vighnesh Birodkar. Unsupervised learning of disentangled representations from
video, 2017.
Emily Denton and Rob Fergus. Stochastic video generation with a learned prior, 2018.
Piotr Dollar, Christian Wojek, Bernt Schiele, and Pietro Perona. Pedestrian detection: A benchmark.
In 2009 IEEE Conference on Computer Vision and Pattern Recognition, pp. 304-311, 2009. doi:
10.1109/CVPR.2009.5206631.
Chelsea Finn, Ian Goodfellow, and Sergey Levine. Unsupervised learning for physical interaction
through video prediction, 2016.
Katerina Fragkiadaki, Jonathan Huang, Alex Alemi, Sudheendra Vijayanarasimhan, Susanna Ricco,
and Rahul Sukthankar. Motion prediction under multimodality with conditional stochastic net-
works, 2017.
Hang Gao, Huazhe Xu, Qi-Zhi Cai, Ruth Wang, Fisher Yu, and Trevor Darrell. Disentangling
propagation and generation for video prediction, 2019.
Andreas Geiger, Philip Lenz, Christoph Stiller, and Raquel Urtasun. Vision meets robotics: The
kitti dataset. International Journal of Robotics Research (IJRR), 2013.
Ian Goodfellow, Yoshua Bengio, and Aaron Courville. Deep Learning. MIT Press, 2016. http:
//www.deeplearningbook.org.
Ian J. Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair,
Aaron Courville, and Yoshua Bengio. Generative adversarial networks, 2014.
Daniel Greenfeld, Meirav Galun, Ron Kimmel, Irad Yavneh, and Ronen Basri. Learning to optimize
multigrid pde solvers, 2019.
Sepp Hochreiter and Jurgen Schmidhuber. Long short-term memory. Neural Computation, 9(8):
1735-1780, 1997. doi: 10.1162/neco.1997.9.8.1735.
10
Under review as a conference paper at ICLR 2022
Jun-Ting Hsieh, Bingbin Liu, De-An Huang, Li Fei-Fei, and Juan Carlos Niebles. Learning to
decompose and disentangle representations for video prediction, 2018.
Chiyu Max Jiang, Soheil Esmaeilzadeh, Kamyar Azizzadenesheli, Karthik Kashinath, Mustafa
Mustafa, Hamdi A. Tchelepi, Philip Marcus, Prabhat, and Anima Anandkumar. Meshfreeflownet:
A physics-constrained deep continuous space-time super-resolution framework, 2020.
Diederik P Kingma and Max Welling. Auto-encoding variational bayes, 2014.
Benjamin Klein, Lior Wolf, and Yehuda Afek. A dynamic convolutional layer for short rangeweather
prediction. In 2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp.
4840-4848,2015. doi: 10.1109/CVPR.2015.7299117.
Zongyi Li, Nikola Kovachki, Kamyar Azizzadenesheli, Burigede Liu, Kaushik Bhattacharya, An-
drew Stuart, and Anima Anandkumar. Fourier neural operator for parametric partial differential
equations, 2021.
Michael Mathieu, Camille Couprie, and Yann LeCun. Deep multi-scale video prediction beyond
mean square error, 2016.
Vincent Michalski, Roland Memisevic, and Kishore Konda. Modeling deep temporal dependencies
with recurrent grammar cells””. In Z. Ghahramani, M. Welling, C. Cortes, N. Lawrence, and
K. Q. Weinberger (eds.), Advances in Neural Information Processing Systems, volume 27. Cur-
ran Associates, Inc., 2014. URL https://proceedings.neurips.cc/paper/2014/
file/cd89fef7ffdd490db800357f47722b20-Paper.pdf.
Matthias Minderer, Chen Sun, Ruben Villegas, Forrester Cole, Kevin Murphy, and Honglak Lee.
Unsupervised learning of object structure and dynamics from videos, 2020.
Junhyuk Oh, Xiaoxiao Guo, Honglak Lee, Richard Lewis, and Satinder Singh. Action-conditional
video prediction using deep networks in atari games, 2015.
Sergiu Oprea, Pablo Martinez-Gonzalez, Alberto Garcia-Garcia, John Alejandro Castro-Vargas, Ser-
gio Orts-Escolano, Jose Garcia-Rodriguez, and Antonis Argyros. A review on deep learning
techniques for video prediction. IEEE Transactions on Pattern Analysis and Machine Intel-
Iigence, pp. 1-1, 2020. ISSN 1939-3539. doi: 10.1109∕tpami.2020.3045007. URL http:
//dx.doi.org/10.1109/TPAMI.2020.3045007.
Keiron O’Shea and Ryan Nash. An introduction to convolutional neural networks, 2015.
M. Raissi, P. Perdikaris, and G.E. Karniadakis. Physics-informed neural networks: A deep learning
framework for solving forward and inverse problems involving nonlinear partial differential equa-
tions. Journal of Computational Physics, 378:686-707, 2019. ISSN 0021-9991. doi: https://doi.
org/10.1016/j.jcp.2018.10.045. URL https://www.sciencedirect.com/science/
article/pii/S0021999118307125.
Fitsum A. Reda, Guilin Liu, Kevin J. Shih, Robert Kirby, Jon Barker, David Tarjan, Andrew Tao,
and Bryan Catanzaro. Sdcnet: Video prediction using spatially-displaced convolution, 2021.
D. Rumelhart, Geoffrey E. Hinton, and Ronald J. Williams. Learning representations by back-
propagating errors. Nature, 323:533-536, 1986.
Nitish Srivastava, Elman Mansimov, and Ruslan Salakhutdinov. Unsupervised learning of video
representations using lstms, 2016.
Ilya Sutskever, Geoffrey Hinton, and Graham Taylor. The recurrent temporal restricted boltzmann
machine. volume 20, pp. 1601-1608, 01 2008.
Casper Kaae S0nderby, Lasse Espeholt, Jonathan Heek, Mostafa Dehghani, Avital Oliver, Tim Sal-
imans, Shreya Agrawal, Jason Hickey, and Nal Kalchbrenner. Metnet: A neural weather model
for precipitation forecasting, 2020.
11
Under review as a conference paper at ICLR 2022
Jilin Tang, Haoji Hu, Qiang Zhou, Hangguan Shan, Chuan Tian, and Tony Q. S. Quek. Pose guided
global and local gan for appearance preserving human video prediction. In 2019 IEEE Inter-
national Conference on Image Processing (ICIP), pp. 614-618, 2019. doi: 10.1109/ICIP.2019.
8803792.
Matteo Turchetta, Felix Berkenkamp, and Andreas Krause. Safe exploration in finite markov deci-
sion processes with gaussian processes, 2016.
Matteo Turchetta, Felix Berkenkamp, and Andreas Krause. Safe exploration for interactive machine
learning, 2019.
Ruben Villegas, Jimei Yang, Yuliang Zou, Sungryull Sohn, Xunyu Lin, and Honglak Lee. Learning
to generate long-term future via hierarchical prediction, 2018.
Susana Vinga. Convolution integrals of normal distribution functions. 01 2004.
Carl Vondrick, Hamed Pirsiavash, and Antonio Torralba. Generating videos with scene dynamics,
2016.
Jacob Walker, Kenneth Marino, Abhinav Gupta, and Martial Hebert. The pose knows: Video fore-
casting by generating pose futures, 2017.
Yunbo Wang, Lu Jiang, Ming-Hsuan Yang, Li-Jia Li, Mingsheng Long, and Li. Fei-Fei. Eidetic 3d
lstm: A model for video prediction and beyond. In ICLR, 2019.
Nevan Wichers, Ruben Villegas, Dumitru Erhan, and Honglak Lee. Hierarchical long-term video
prediction without supervision, 2018.
Brian Wilcox and Michael C. Yip. Solar-gp: Sparse online locally adaptive regression using gaussian
processes for bayesian robot model learning and control. IEEE Robotics and Automation Letters,
5(2):2832-2839, 2020. doi: 10.1109/LRA.2020.2974432.
Wei Yu, Yichao Lu, Steve Easterbrook, and Sanja Fidler. Efficient and information-preserving future
frame prediction and beyond. In International Conference on Learning Representations, 2020.
URL https://openreview.net/forum?id=B1eY_pVYvB.
A Appendix
A.1 PREDICTIONS: THE FIRST 3 PREDICTIONS IN A SEQUENCE OF PREDICTIONS
The first three predictions of a rollout involve predicting from a combination of known and predicted
random variable sets of input images. We treat these predictions as special cases of the more general
case presented in Section 4.3. Each input x0j is composed of 3p2 independent random variables, with
p2 random variables contributed from each input image. We separate the input x0j along the dimen-
sions of the input that are known and random, to handle the different components separately. x0j,Rn
and x0j,Kn are random variables that denote the random and known components of the input respec-
tively. XRn , XKn and xi,Rn , xi,Kn reference the corresponding known and random dimensions in
all the training inputs and a single training input respectively.
The kernel functions ka and the probability distribution over the input p(x0j ) are the only forms
of interaction with the inputs while predicting the output distribution p(f (x0j)[a]). The structure
of these functions and our independence assumptions allow us to cleanly split up our inputs into
known and random components. We use the Radial Basis Function (eq. 3) as our kernel function
ka (x, y). In this function the inputs interact with one another along the same dimension, allowing
12
Under review as a conference paper at ICLR 2022
us to re-write the kernel as:
ka(x, y)
αa exp (-(X-ST J-y )
2
αa,Kn∙
( (XKn - IyKn)Na,Kn(xKn - IyKn)
2
• αa,Rn exp
(XRn - yRn ) Λa,Rn (XRn - yRn)
2
= ka,K n (XKn, yKn) • ka,Rn (XRn, yRn)
(10)
The subscripts on the inputs correspond to their known and random dimensions. ka,Rn and ka,Kn
are the kernel functions that act on the known and random dimensions respectively. Each are pa-
rameterized by their own set of kernel parameters: αa,Kn , Λa,Kn and αa,Rn , Λa,Rn respectively.
αa,Rn • αa,Kn = αa . Λa,Kn and Λa,Rn are block diagonal matrices sampled from Λa, a large di-
agonal matrix, along the known and random dimensions. Using our assumption that all predicted
pixels within and across images are independent, we separate the probability distribution:
p(X0j) = p(X0j,Kn) • p(X0Rn)	(11)
p(X0j,Rn) is a multivariate gaussian distribution of the random components of the input and p(X0Kn)
denotes the joint distribution of all the known input pixels.
We construct our inputs according to this split input structure. The random component of each input
is a multivariate gaussian distribution,xj,Rn -N(Xj,μ,Σx0 ), specified by its mean and covariance
matrix. To create xj,* and Σχ∕ for the random test inputs, We use the method described in Section
4.3. We sample and concatenate patches from the mean and variance images corresponding to the
subset of input images that are random variables. Pixels in the analogous patches of the knoWn input
images are flattened and concatenated to form Xj Kn *, the observed values of Xj Kn the random
variable that is the knoWn component of the test input. If all the input images are knoWn, the test
inputs are created in the same manner as the training inputs, as described in Section 4.2
Our model output is the predicted random variable f (X0j)[a]. To solve for our model’s output, We
must find the distribution p(f (X0j )[a]) by solving the intractable integral in eq: 4. We approach a
solution With the same moment matching procedure presented in Section 4.3, using the split kernel
(eq: 10) and probability distribution (eq: 11) derived above. The values of the knoWn pixels have
been definitively observed With the assumption of no noise. The joint probability distribution of
all known pixels can be substituted with a delta function at the observed values: δ(X 一 Xj,Kn,μ).
This alloWs us to integrate out certain contributions from the knoWn components during the moment
matching steps. Solving for the mean of f (X0j)[a] using these hybrid inputs we get:
daT,hybrid
βa
0	r .1	0	0	0	ʌ	αa,Rn	- 1 VTRn(Zl +Aa,Rn) vi,Rn
da,hybrid[i] = ka,Kn(Xi,Kn,Xj Kn μ) • -/	= • e	j,σ
√l∑xj,σ N-Rn + 11
(12)
We use the • operator to denote element wise multiplication. βa is defined in eq: 5 and vi,Rn
Xj,μ 一 XiRn. Solving for the variance of f (Xj∙)[a] from the hybrid inputs we get:
ς(Xj)[a, a]	=	αa	- tr ace((ka (X,	X ) + σn,aI)	Q aa,hybrid)	+ Ba	Q aa,hybrid B a	一	M(Xj )[a]
Qaa,hy brid = Qaa,K n • Qaa,Rn
Qaa,Kn = ka,Kn(Xj,Kn,μ, XKn)Tka,Kn(Xj,Kn,μ, XKn) ∈ Rn×n
Qaa,Rn [i, k] = -∕∣ D	j • ka,Rn(Xi,Rn,X j,μ > ka,Rn (Xk,Rn ,x ∙
|RRn |
0
’j,μ
、2 zik,Rn R-n`0	zik,Rn
) • e	j,σ
RRn
(13)
Σx0	(2Na-,1Rn) + I and zik,Rn = Na-,1Rnvi,Rn + Na-,1wvk,Rn. We use these equations to
predict every pixel in the future image. Specific details for each step of the rollout are discussed in
Section A.3 A walkthrough derivation of these equations along with the specifics for each step of
the rollout can also be found in section A.3.
13
Under review as a conference paper at ICLR 2022
A.2 Prediction with all random inputs: Derivations
In this section we discuss the derivations for the equations 5 and 6. These equations predict the mean
and variance of the output distribution ofa single pixel in a future imagep(f(x0j)[a]). These predic-
tions are done from input x0j which is a multivariate gaussian random variableXj 〜NlXjw ςxiJ
defined in Section 4.3.
Mean Prediction Derivation: We begin by walking through the derivation of the mean μ(χj )[a]
of the output distribution p(f (x0j)[a]) presented in equation 5. For the following derivations we
simplify the notation from p(f (x0j)[a]|x0j, X, f (X)[:, a]) to p(f (x0j)[a]|x0j) as X and f (X)[:, a] are
known quantities. We begin our moment matching based derivation by taking the mean of the
intractable integral presented in equation 4.
Z∞
∞
0	0	00
00
p(f(x0j)[a]|x0i)p(x0j)dx0i] = Ef,x0j[p(f(x0j)[a]|x0j)]
(14)
= Ex0j [Ef [p(f (X0j)[a]|X0j)]]
Ef [p(f (X0j)[a]|X0j)] is the analytical form of the mean during Gaussian Process Regression from
equation 1. Substituting this formula into the above equations we get:
μ(χj)[α] = ExJka(Xj,X )[ka(χ,χ ) + σn,aI ]-1 f (X )[：,a]]
(15)
We denote βa ∈ Rn to be [ka (X, X) + σn2I]-1f(X)[:, a]]. We denote da ∈ Rn to be
Ex0j [ka (x0j, X)].
Z∞
∞
00
ka(x0j , xi)p(x0j)dx0j
(16)
0
(17)
Expanding ka to the RBF kernel equations and p(xj) to the multivariate gaussian pdf, we solve for
da using Vinga (2004). This gives us the mean prediction equations listed in 5 and relisted below in
equation 18:
〃(Xj)[a] = d Ba
da[i] = / αa	e-1 vT\jAaLvi
M A-1+ I1
βa = [ka(X, X) + σn2I]-1f(X)[:, a]
(18)
Variance Prediction Derivation: We now walk through the derivation of the predicted variance
Σ(x0i)[a, a] ∈ R. Let Σ(x0i) ∈ R(p-b)2×(p-b)2 be the covariance matrix of the predicted output,
where Σ(x0i)[a, a] ∈ R is the variance of output f (x0i)[a]. Due to our independence assumptions
between outputted pixel distributions, we assert that the covariance between outputs f (x0j)[a] and
f (x0j )[b], representing different output dimensions, Σ(x0i)[a, b] = 0, ∀a 6= b.
(19)
This is simplified using the law of total variance.
ς(Xj)[a,a] = Ex0 vvrrf (f (Xj)[a]|xj)] + Ef,x0 f (Xj)[a]f(χj)[a]] - μ(χj)[a]2
Ef,x0j hf (X0j)[a]f (X0j)[a]i =	Ef hf (X0j)[a]|X0j i Ef hf (X0j)[a]|X0j i p(X0j)dX0j
Ef	f (X0j)|X0j is the mean output of standard Gaussian Process Regression from equation 1.
(20)
(21)
14
Under review as a conference paper at ICLR 2022
Substituting this into the above equations we have:
Ef,x0 hf (x0j)[a]f (x0j)[a]i =	βaTka(x0j,X)Tka(x0j,X)βap(x0j)dx0j
j	-∞
Z∞
ka(xj,X)Tka(xj,X)p(xj)dxjβa
∞
(22)
We define Qaa = R-∞∞ ka(x0j, X)Tka(x0j, X)p(x0j)dx0j ∈ R(p-b)2×(p-b)2. βa is defined in the
above sections. This gives us:
Ef,x0i f(x0i)[a]f(x0i)[a] =βaTQaaβa
Qaa[i, k]
ka(xi, xj)ka (Xk ,xj ) ZTkRT ςx0 Ozik
Pw	e i
(23)
zik
Λa-1vi + Λa-1vk and R = Σx0 [Λa-1 + Λa-1] + I where I ∈ Rn×n is the identity matrix.
j,σ
In the first term of equation 20, Ex0 varf (f (x0j )[a]|x0j ) , varf (f (x0j)[a]|x0j ) is the variance output
of Gaussian Process Regression from equation 2. Simplifying this term we get:
Exjvarf (f(xj)[a]∣xj)] = α2 - trace ((k0(X,X)+。：@ I 厂 1Qaa)
(24)
We substitute the equations from 24, 23 and 5 into 20 to compute the variance for each outputted
pixel corresponding to output dimensions a ∈ [0, (p-b)2 - 1]. This results in the variance prediction
formula presented in equations 6 and 7 and relisted below as equations 25 and 26
Σ(xj)[a, a]
αa - trace ((k0(X,X) + σ2,aI)-1 Qaa) + βTQaaea -
Qaa[i, k]
ka (xi, Xj)ka (xk , Xj) 2ZikR ɪς3si, zik
pR	e
(25)
(26)
A.3 Prediction with Hybrid and Fully Known inputs: Derivations and
Additional Details
In this section we discuss the derivations for the equations 12 and 13. These equations predict the
mean and variance of the output distribution of a single pixel in a future image p(f (X0j)[a]). These
predictions are done from input X0j which is composed of two random variables X0j,Rn and X0j,Kn
explained in Section A.3.
Mean Prediction Derivation: In this section we discuss the derivation of the mean prediction
μ(xj)[a] of the output distribution p(f (xj)[a]) when using a combination of known and random
input images, for the first 3 predictions of a rollout. We show the derivation for equation 12. Since
this is a special case of prediction from all random inputs we begin our derivation from the derivation
of the mean prediction equations for all random inputs in section A.2 We begin our derivation from
equations 15, 16 and 17.
In this derivation we use the split kernel and probability density functions in equations 10 and 11 to
deal with the hybrid, random and known, nature of the inputs. The joint probability distribution of
all known pixels can be substituted with a delta function at the known values: δ(x - Xj Kn *). This
allows us to integrate out certain contributions from the known components. βa , being a constant,
remains unchanged and we re derive da as da,hybrid.
da,hybrid[i] = Ex0 [ka (X0j, X [i])]
= Ex0	x0	[ka,Rn (Xj,Rn, XRn [i])ka,Kn (Xj,Kn, XKn [i])]
j,Rn, j,Kn
= Ex0j,Rn [ka,Rn (Xj,Rn,XRn)]Ex0	[ka
,Kn (Xj,Kn,XKn)]
=/	ka,Kn(xj,Kn ,X Kn)δ(x j,Kn,μ - xj,Kn)dxj,Kn /	ka ,Rn(Xj,Rn,XRn)p(X0j,Rn)dX0Rn
-∞	-∞
(27)
15
Under review as a conference paper at ICLR 2022
Solving this yields the mean prediction for the third rollout given in 12 relisted below as equation
28:
μ(Xj) = da,hybridβ
0	∣- .1	0	0	0	ʌ	αα,Rn	- 1 VTRn (ςx0 +Aa,Rn)	vi,Rn
dα,hybrid[i] = ka,Kn(xi,Kn, xj Kn μ) ∙ —/	1	= ∙ e	j,σ
√l∑xj,σ Λ-,Rn +11
(28)
Variance Prediction Derivation: Here we discuss the derivation of the variance Σ(xi)[a, a] ∈ R
in equation 13 from hybrid and random inputs. We follow the method outlined in the derivation for
all random inputs. We use the split kernel and probability density functions in equations 10 and 11
to separately deal with the random and known components of the inputs. With this we arrive at an
identical formulation to the case with all random inputs where Qaa,hybrid is used in place of Qaa.
Qaa,hybrid = Ex0 [ka(x0j,X)Tka(x0j, X)]
= Exj,Rn [ka,Rn (xj,Rn,XRn)T ka,Rn(Xj,Rn,XRn )]∙
Ex	[ka,Kn (xj,Kn, XKn)Tka,Kn(xj,Kn, XKn)]
j,Kn	(29)
ka,Kn (xj,Kn, XKn) ka,Kn (xj,Kn, XKn)δ(xj,Kn - xj,Kn,μ)dxj,Kn
-∞
ka,Rn (xj,Rn,XRn)T ka,Rn(x0j,Rn, XRn)p(x0j,Rn)dx0Rn
-∞
Here ∙ denotes the element wise multiplication operator. The integrals with the multivariate gaussian
pdfs result in the same solution as elaborated in the random variance derivation. Solving this yields
the variance prediction given in equation 13 relisted below as equation 30:
Σ(xj)[a, a] = αα - trace((ka(X,X) +。：01 )-1Qaa,hybrid) + βTQaa,hybridβa - μ(xj)[a]
Qaa,hybrid = Qaa,Kn ∙ Qaa,Rn
Qaa,Kn = ka,Kn(Xj,Kn,μ, XKn)Tka,Kn(Xj,Kn,μ, XKn) ∈ Rn×n
C	1 1 _	1	L	f 3	/	"	/	/	∖ C1 Zik,Rn R-n ςx0 ^zik,Rn
Qaa,Rn[i, k] =	/~\r)-\。ka,Rn(Xi,Rn, xj,μ ) ka,Rn (Xk,Rn, xj,μ) ∙ e	3,
|RRn |
(30)
Rollout Discussion: In this section we discuss the composition of our inputs and additional
details of each step in our predictive rollout. In a predictive rollout we are predicting T time steps
into future starting from 3 known, consecutive input images [zi , zi+1 , zi+2 ]. With each prediction
we predict a single time step into the future before incorporating our prediction into our next set of
inputs. We continue this process until we predict the desired number of time steps.
First Step: The first step of the predictive rollout predicts zi+3 from input images [zi , zi+1 , zi+2].
For this first prediction all the input images are known quantities. As a result for each test input
Xj j ∈ [0,m - 1], the entire test input is known, Xj = XjKn, and XjRn, does not exist. Xj∙,κn,μ ∈
R3p2 is formed in a manner identical to the training inputs. When plugging these inputs into the
hybrid mean prediction equation 12 and variance prediction equation 13 we remove the random
components of the equations giving us:
da,hybrid[i] = ka,Kn (Xi,Kn, xj,Kn,μ)
Qaa,hybrid = Qaa,K n
(31)
(32)
Substituting these back into the equations 12 and 13 we get the formulas to predict a single output
dimension of for a single test input for the first prediction. These equations equivalent to the basic
Gaussian Process Regression equations for mean and variance prediction. Using these formulas
we predict the distribution for every pixel in zi+3 from the m test inputs. This gives us the final
predicted image zi+3 which is stored as a mean, variance image tuple (Mi+3, Vi+3).
16
Under review as a conference paper at ICLR 2022
Second Step: The second step of the predictive rollout predicts zi+4 from input images
[zi+1, zi+2, zi+3]. zi+3 is a random variable, from the first prediction, represented by the mean
and variance image tuple (Mi+3, Vi+3). zi+1 and zi+2 are known. When constructing each test
input, Xj,κn,μ is constructed from flattened and concatenated patches of [zi+ι, Zi+2]. Xj,μ is Con-
structed from flattened patches of Mi+3 and x0j,σ is constructed from flattened patches of Vi+3 to
create the random input x0j,Rn . These components together form a single test input x0j . We plug
these inputs into the hybrid mean prediction equation 12 and variance prediction equation 13 to
compute the model’s output distribution for a single output dimension. We repeat this to predict the
output distributions for each pixel in the future image zi+4 which is stored as a mean and variance
image tuple (Mi+3, Vi+3).
Third Step:	The third step of the predictive rollout predicts zi+5 from input images
[zi+2, zi+3, zi+4]. zi+3 and zi+4 are random variables, from the first and second predictions, repre-
sented by the mean and variance image tuples (Mi+3, Vi+3), (Mi+4 , Vi+4). zi+2 is known. When
ConStrUCting each test input, Xj,κn,μ is constructed from flattened patches of [zi+2]. Xj,* is Con-
structed from flattened concatenated patches of Mi+3 , Mi+4 and x0j,σ is constructed from flattened
patches of Vi+3 , Vi+4 to create the random input x0j,Rn. These components together form a single
test input x0j . We plug these inputs into the hybrid mean prediction equation 12 and variance pre-
diction equation 13 to compute the model’s output distribution for a single output dimension. We
repeat this to predict the output distributions for each pixel in the future image zi+5 which is stored
as a mean and variance image tuple (Mi+5, Vi+5).
Fourth Step and Onwards: The third step of the predictive rollout predicts zi+6 from input images
[zi+3 , zi+4 , zi+5]. For this and all subsequent predictions, all the input images are random variables
outputted by our model. To predict the future image we utilize the approach detailed in the section
4.3 on Prediction with all random inputs.
A.4 Predictive Comparison Experiment: Additional Details
In this section we highlight additional details on the methodology used to generate the results for the
’Predictive Comparison’ Experiment in Section 5. To compare all three methods, we predict frames
[z10 , . . . , z24] given frames [z0 , . . . , z9]. For our method we train our model using [z0 , . . . , z9] and
begin our prediction with input images [z7, z8, z9], identical to the approach outlined in the ‘Forward
Prediction’ Experiment.
The FNO-2d-time model convolves across the two spatial dimensions to predict a single future
image with a recurrent structure in time. This model uses a rollout method to predict longer video
sequences. The model predicts one future frame at a time and incorporates its last prediction into
its next input. We continue this until we have predicted the desired number of frames. The model is
structured to take an input of three images and predict a single future image. We train this method
in a manner similar to ours. The training data is created using the first 10 frames [z0, . . . , z9]. These
frames are separated into data points of 4 consecutive images [zi , . . . , zi+3]; ∀i ∈ [0, 6], where the
first three images form the input and the fourth is the output. The model is trained over 20 epochs.
The trained model is then used to predict 15 frames [z10, . . . , z24] of the same sequence.
The FNO-3d model is a neural network that convolves in space and time to directly output several
frames from a set of input frames. We train this model using the first 25 frames from two unique
sequences, generated using the same simulation parameters. The first 10 images are used as the
inputs, with the remaining 15 serving as the training outputs. Once trained over 20 epochs this
model is given a set of 10 consecutive frames [z0, . . . , z9] to predict the next 15: [z10, . . . , z24].
The results of this comparison are shown in fig. 5.
A.5 Average Result Metrics
We showcase the average relative error and average mean standard deviations off of our model’s pre-
dictions evaluated on 100 separate video sequences in Figure 6. For each video sequence our model
is trained using the first 10 frames [z0, . . . , z9] and used to predict the next 15 frames, [z10, . . . , z24].
We use the parameters discussed in the ’Forward Prediction’ experiment.
17
Under review as a conference paper at ICLR 2022
(a) Average Relative Errors
Figure 6: Averaged Metrics: Fig (6a) Shows the average relative error metrics using our method
over 100 results on separate video sequences. (6b) Shows the average mean standard deviations
off the predicted image is from the ground truth using the mean and variance generated using our
method. This result is also averaged over predictions on 100 different video sequences. To generate
the above results our model was trained on frames [z0, . . . , z9] of each sequence and used to predict
the next 15 frames, [z10 , . . . , z24]
七 o spψjs uetu≡
8
■
O
(b) Average Mean Standard Deviations off
A.6 Training Input Creation Graphic
The Figure 7 graphically demonstrates the process of creating a training datapoint from patches of
4 consecutive video frames.
Figure 7: Graphic to visualize the process of generating a training data point from a sequence of
4 consecutive video frames. Each grey figure represents a labelled video frame. The red squares
represent the patches that are sampled to create the input output pair. In the fourth image the green
pixel represents the output patch after cropping the patch with patch border b. The patches from the
first 3 images are flattened and concatenated to form the input. The patch from the fourth image is
flattened and used as the output. In this case the output patch is a single pixel.
A.7 Method Overview Graphic
The Figure 8 shows a graphical overview of the method.
18
Under review as a conference paper at ICLR 2022
Figure 8: Graphic to visualize the overall method
19