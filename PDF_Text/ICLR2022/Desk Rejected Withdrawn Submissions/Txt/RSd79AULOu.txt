Under review as a conference paper at ICLR 2022
Fairness-aware Federated Learning
Anonymous authors
Paper under double-blind review
Ab stract
Federated Learning is a machine learning technique where a network of clients
collaborates with a server to learn a centralized model while keeping data localized.
In such a setting, naively minimizing an aggregate loss may introduce bias and
disadvantage model performance on certain clients. To address this issue, we
propose a new federated learning framework called FAFL in which the goal is to
minimize the worst-case weighted client losses over an uncertainty set. By deriving
a variational representation, we show that this framework is a fairness-aware
objective and can be easily optimized by solving a joint minimization problem
over the model parameters and a dual variable. We then propose an optimization
algorithm to solve FAFL which can be efficiently implemented in a federated setting
and provide convergence guarantees. We further prove generalization bounds for
learning with this objective. Experiments on real-world datasets demonstrate the
effectiveness of our framework in achieving both accuracy and fairness.
1	Introduction
Due to the emergence of unprecedented amount of data generated by mobile devices and the growing
computational power of these devices, Federated Learning (FL) has become of increasing importance
and often crucial for deployment of large-scale machine learning (Konecny et al., 20l6; McMahan
et al., 2017). A typical Federated Learning setting consists of a network of hundreds to millions
of devices (clients) which interact with each other through a central server, and its goal is to
collaboratively learn a shared model while keeping the training data on the device instead of requiring
the data to be uploaded and stored on the central server.
Despite its advantage of data privacy, it faces several challenges ranging from developing communi-
cation efficient algorithms to ensuring fairness (Kairouz et al., 2019). First, frequent communication
is undesirable in FL as it is expensive due to unreliable and relatively slow network connection,
especially when more clients are involved. To reduce communication overload, one needs to depart
from the conventional distributed learning setting where the updated local models are broadcast to
the central server at each iteration, and adopt more efficient communication strategies like periodic
averaging (Khaled et al., 2019; Haddadpour and Mahdavi, 2019; Stich, 2019; Konecny et al., 2016;
Konecny et al., 2016; McMahan et al., 2017).
Another major challenge in Federated Learning is that of fairness. In the data-generating process
in federated learning, there is a risk of introducing biases, and models learned from biased training
data can often exhibit unfair behaviours. For example, some clients will make much heavier use
of the services or app than others, leading to varying amount of local training data, then federated
learning may weight higher the contributions of those over-represented clients and disadvantage
model performance on other clients. Ensuring that the learned models are non-discriminatory or
fair with respect to some protected groups is a topical problem in modern machine learning, and a
variety of definitions of the notion of fairness have been proposed (Zafar et al., 2017; Dwork et al.,
2018; Donini et al., 2018; Williamson and Menon, 2019; Hashimoto et al., 2018; Samadi et al., 2018).
However in the context of federated learning, there has been limited work on how to address the
fairness concerns (Li et al., 2020a; 2021a;b; Mohri et al., 2019). Mohri et al. (2019) have taken a
initial step towards this goal by introducing good-intent fairness based on the maximin principle
where the objective is to seek all client losses to be small. However that objective is rigid as it does
not allow for flexible trade-off between fairness and accuracy. Inspired by fair resource allocation
in wireless network, Li et al. (2020a) propose a modified federated learning objective to encourage
uniformity in performance across devices. Despite that their objective enables to tune the amount
1
Under review as a conference paper at ICLR 2022
of fairness via a single hyper-parameter, it is not a fairness-aware objective and as pointed out by
Yang et al. (2020) is less effective in ensuring better fairness under heterogeneity. Later, Li et al.
(2021a) develop a tilted empirical risk minimization (TERM) to handle outliers and class imbalance
which is a smooth approximation to the maximum function. TERM has been shown to achieve good
performances in some FL application. However, their algorithm requires dynamically sampling from
a Gumbel-Softmax distribution for partial participation and reweighting the samples and clients,
which is expensive. More recently, Li et al. (2021b) propose a federated multi-task framework to
balance two competing constraints of robustness and fairness and empirically demonstrate that it can
encourage fairness. However, their approach requires learning different models for each client, and
there is no theoretical guarantee for the fairness benefit except a simple linear problem.
In this work, we propose a new framework called FAFL to address the fairness issues in federated
learning. Instead of optimizing the model for a specific (uniform) distribution, FAFL minimizes a
Qα-weighted loss which is a supremum of weighted aggregation of client losses over an uncertainty
set Qa of possible weights, where the parameter α := (αι,…，an) is personalized for each client
to account for client heterogeneity. We show that our FAFL framework defines a notion of fairness,
which we refer to as heterogeneous conditional value at risk (HCVaR). HCVaR is a generalization of
conditional value at risk (CVaR) which is a well-studied risk-averse measure in finance and portfolio
optimization (Shapiro et al., 2014; Rockafellar et al., 2000; Krokhmal et al., 2002) and has recently
been used in many applications in machine learning (Chow and Ghavamzadeh, 2014; Shalev-Shwartz
and Wexler, 2016; Fan et al., 2017; Curi et al., 2020; Lee et al., 2020; Soma and Yoshida, 2020;
Jeong and Namkoong, 2020). In particular, Williamson and Menon (2019) propose a new definition
of fairness and show that CVaR is a fairness risk measure. Compared to CVaR, HCVaR takes into
account client heterogeneity by allowing different parameters αi for each client i, which is more
related to federated learning setting. The connection to HCVaR shows that FAFL is a fairness-aware
objective which involves an expectation and deviation, implying that minimizing FAFL objective
ensures that the client losses are small, and that they have low deviation (fairness). Compared to
agnostic federated learning (Mohri et al., 2019), FAFL is more flexible as the conservation level can
be controlled by adjusting those parameters αi s. In fact, agnostic loss and the standard federated
learning objective can be recovered from our framework using proper choice of αi .
FAFL formulates the learning problem as a minimax optimization problem, which finds a global
model that minimizes the worst-case weighted aggregated loss. One approach to solving this minimax
problem is to employ methods from Mohri et al. (2019) which iteratively applies stochastic gradient
descent ascent updates. However this approach is undesirable in federated learning setting since it
requires communication at each iteration. A key advantage of FAFL is that it enjoys a variational
representation which is equivalent to a minimization problem over a dual variable. Therefore FAFL
can readily be optimized by solving a joint minimization problem with respect to the model parameter
and the dual variable. We propose a simple gradient based algorithm to solve it called rFedFair
that can be efficiently implemented in federated setting and comes with strong theoretical guarantees.
We summarize our contributions as follow.
•	We present a new framework called FAFL to address the fairness issues in federated
learning, which generalizes many existing federated learning objectives, including agnostic
loss (Mohri et al., 2019) and standard FL objective, and naturally yields a new notion of
fairness named HCVaR.
•	We propose a smooth approximation to FAFL and provide an efficient algorithm to solve
it which is guaranteed to find an approximate minimizer of the original FAFL problem for
convex and smooth loss functions.
•	We prove two data-dependent generalization bounds for learning with FAFL. Our bounds
show proper generalization from empirical distribution of samples to the true underlying
distribution.
The rest of the paper is organized as follows. In Section 2, we establish the necessary notations
and provide a brief background on federated learning. In Section 3, we give a formal definition
of our FAFL framework and describe its connection to fairness. Then in Section 4, we present an
efficient federated learning algorithm for solving FAFL. Next, we give a detailed theoretical analysis
of the proposed algorithm in both full and partial device participation cases (Section 5.1), as well
as generalization guarantees (Section 5.2). In Section 6, we conduct a series of experiments and
2
Under review as a conference paper at ICLR 2022
compare our results with existing fair federated learning algorithms. Finally we conclude and discuss
future directions. All proofs are deferred to the appendix.
2	Preliminaries
Notation: We denote by Z = X × Y a measurable instance space where X and Y represent feature
and label spaces, respectively. We use F = {fω : ω ∈ W} to denote the underlying hypothesis
class of functions from X to Y0 where Y0 might differ from Y . We are also given a loss function
l : Y0 × Y → R+, quantifying the loss incurred by a decision rule applied to a data instance
z = (x, y) ∈ Z, e.g., l(fω (x), y). Given a hypothesis fω ∈ F, denote the expected loss of fω with
respect to a distribution P over X × Y by
Federated Learning Scenario: We consider a federated learning setting with a network of n nodes
(clients) connected to a server node. Denote [n] = {1, ∙∙∙ ,n}. We assume that for every i ∈ [n]
the i-th client has access to mi training sample in Si = {(xij, yji) ∈ X × Y : 1 ≤ j ≤ mi}
drawn i.i.d. from some unknown distribution Pi, i.e., (χj,yj)〜P” In federated learning, the
data on a given client is typically on the usage of the mobile device by a particular user, which
might come from different environments, contexts, and applications, and hence clients can have
non-i.i.d. data distributions, that is, the distributions Pi and Pj, i 6= j, are distinct. Let m = Pin=1 mi
and Pi = mi/m. We will denote by Pi the empirical distribution associated to sample Si. In the
conventional federated learning setting, the n clients are interested in collaboratively training a single
model on their joint data in a privacy-preserving way by solving the following problem
n 1 mi
min XPi - X l(fω(Xj), yj)
ω∈W mi	j j
i=1	mi j=1
with the assumption that all samples are uniformly weighted, i.e., the underlying target distribution is
Pin=1 PiPi .
However since the mixture weight of the distribution Pi, i ∈ [n] is unknown, that assumption is rather
restrictive and can lead to solutions that are harmful to the clients (Mohri et al., 2019). Moreover,
the uniformly weighted aggregated loss puts less weight on clients with small number of data points
during training, thus giving rise to unfairness where the learned model behaves differently across
clients. To address these issues, a natural idea is to reweight the client loss. However since we do not
understand precisely which weighting to pick, we propose to study a worst-case client weighted loss,
which defines our new framework given in the next section.
3	Fairness-aware federated learning
In this section, we first introduce the federated learning framework we consider. Then, we establish
its connection to fairness.
3.1	Problem formulation
As we stated in previous section, the conventional federated learning objective raises some issues.
This motivates us to consider a federated learning framework where different weights are assigned to
different clients, and the learner must learn a model that is favorable for any weighted aggregation
of client losses over an uncertainty set Q of possible weights. In this way, a underrepresent client
can be up-weighted to achieve better performance, thus improving model fairness. We will show in
next subsection how this framework intimately relates to a notion of fairness and is a fairness-aware
objective.
We now formally define the Fairness-Aware Federated Learning framework (FAFL). Throughout
this paper, for ease of notation, we use EH to denote the expectation with respect to the randomness
in selecting client i with probability Pi unless explicitly stated otherwise.
3
Under review as a conference paper at ICLR 2022
Definition 1 (FAFL). Let α = (α1,α2, ∙∙∙ ,αn) be a vector where α ∈ [pi, 1] for all i. Define
the uncertainty set Qa = {q = (qι,…，qn) ： E[qi] ：= P2ι qiPi = 1,qi ∈ [0, α-1]}. Then, the
Qα -weighted loss is
n
fα(ω) ：= sup E[qifPi(ω)] = sup Xqipifi(ω),	(1)
q∈Qα	q∈Qα i=1
where we write f i(ω) = fPi (ω) for notational convenience.
Here, αi ∈ [pi, 1] is a tuning parameter and allowed to be different across the clients to take client
heterogeneity into account. Interestingly, by setting different α, the Q-weighted loss can recover
existing federated learning objectives. For example, as αi → 1∀i, the uncertainty set Qα would
reduce to a single point, i.e., Qa = {(1,…，1)}, and fa becomes PZiPifi(ω), which is the
classical federated learning objective; as αi → pi for all i, fα reduces to the agnostic federated
learning loss (AFL) (Mohri et al., 2019): maxλ∈∆n Pin=1 λifi(ω), where ∆n is a simplex. Therefore,
our FAFL objective is more flexible as it can be tuned based on the conservatism level αi of each
client.
3.2	Connection to fairnes s
In this section, we show that FAFL defines a notion of fairness named heterogeneous conditional
value at risk (HCVaR), which is a generalization of conditional value at risk (CVaR), a common
risk measure in mathematical finance and has recently been proposed as a fairness risk measure
(Williamson and Menon, 2019). We first recall the definition of CVaR. For scalar χ ∈ (0, 1] and
random variable fi(ω) (the randomness is w.r.t. the selection of client), the conditional value at risk
is (Rockafellar et al., 2000)
CVaRi-x(fi(ω))= E[fi(ω)∣fi(ω) > Qi-χ(fi(ω))],
where Q1-χ is the quantile at level 1 - χ. Intuitively, CVaR measures the tail behavior of fi (ω).
Note that the good-intent fairness (AFL) is a special case of CVaR fairness risk (Mohri et al., 2019).
In federated learning setting, because of client heterogeneity, we may wish to treat losses arising from
different clients differently. Therefore, we consider a heterogeneous version of CVaR by allowing
different weights to each client as follows.
Definition 2 (HCVaR). Given a vector α = (αι, ∙∙∙ , αn) satisfying Ta ：= E[α-1] ≥ 1, we define
heterogeneous conditional value at risk as
HCVaRi-a(fi(ω)) ：= Ea[fi(ω)∣fi(ω) > Qi-i∕τ.(fi(ω))],
where the expectation Ea[∙] is With respect to random selection ofdevice i with probability Opr∙
Remark 1. If αi = χ for all i, HCVaR1-a(fi(ω)) would reduce to CVaR1-χ(fi(ω)).
Compare to CVaR, HCVaR measures a weighted tail-average. Therefore, we can define a notion of
fairness by minimizing HCVaR which seeks that the weighted average of the largest client losses is
small. This tightens the range of client losses, thus ensuring that they are commensurate (fair).
With this definition in mind, we now derive a dual representation for FAFL, reformulating the primal
problem (1) over q ∈ Qa to a dual problem over a one-dimensional variable. This dual representation
shows an equivalence between Q-weighted loss and HCVaR, thus connecting FAFL to fairness.
Lemma 1. Denote (•)+ := max(∙, 0). Then,
fa(ω) = inf iη + e[ɪ(fi(ω) - η)+] ] = HCVaRi-α(fi(ω)).	⑵
η∈R	αi
Remark 2. If the loss function is bounded, i.e., 0 ≤ l(fω(x), y) ≤ B for any z = (x, y) ∈ Z, the
domain of η in (2) can be restricted to η ∈ [0, B].
By the lemma, one may equally write fa(ω) = Ea [fi(ω)] + Da(f i(ω)) where Da(f i(ω)) ：=
HCVaR1-a(fi (ω) - Ea [fi (ω)]) is a measure of deviation. For perfect fairness where fi (ω) is a
constant, Da(f i(ω)) = 0. Therefore, the lemma shows that FAFL is a fairness-aware objective that
4
Under review as a conference paper at ICLR 2022
is an expectation plus a deviance, suggesting that minimizing FAFL objective ensures that the client
losses are small, and that they have low deviation (fairness). By changing the parameters αis, FAFL
also allows for a flexible trade-off between average accuracy and fairness. There is another desirable
side benefit. The convexity of HCVaR implies that if ω → fi (ω) is convex, then so is ω → fα (ω).
Thus, for convex l and F, as shown in the next section, solving FAFL (simultaneously encouraging
fairness) does not pose an optimization burden.
In practice, the data-generating distribution Pi is not known to the client, and the client has only access
to the finite sample Si. Thus, for every i ∈ [n], the expected loss can be estimated by the empirical
loss fi(ω) = m1- Pjm=I l(fω (xj), yj). This leads to the definition of empirical Qa-Weighted loss,
n
fa(ω) := SUp E[qif (ω)] = SUp Eqipif (ω).	⑶
q∈Qα	q∈Qα i=1
4	The proposed algorithm
To solve FAFL, one may propose to directly minimize the Qα-Weighted loss, Which yields a minimax
optimization problem, by applying stochastic gradient descent ascent algorithm as in Mohri et al.
(2019). HoWever this approach may be undesirable in federated learning setting as it requires frequent
communication. In this section, We Will present a gradient optimization method for solving FAFL
problem (3) that is computationally and communication-Wise efficient.
Instead of solving the original Qα-Weighted loss (3), We aim to minimize its dual representation,
Which yields the folloWing joint optimization problem
1
min fa(ω) =	min E —(f (M - η)+ + η = Fa(ω,η)	⑷
ω∈W	ω∈W,η∈R	αi
Where We reWrite fa as its dual representation given by Lemma 1. For convex fi(ω), Problem (4)
is jointly convex in (ω, η) but not differentiable due to the non-smoothness and non-linearity of
(•)+. SUbgradient optimization method is supposed to solve this kind of problems. However in the
federated learning setting its convergence guarantees can not easily be derived. In fact, smoothness
is required condition to prove convergence in federated optimization literature (Khaled et al., 2019;
Haddadpour and Mahdavi, 2019; Li et al., 2020b). To develop theoretically principled algorithm
for solving (4), we propose to use softmax function as a smooth approximation to the max function
defined as follows.
X
φμ(x) := μlog(1 + eμ),
where μ is a predefined parameter. Note that φμ is convex and 1 /μ-smooth. Furthermore, it smoothly
approximates the max function (Bullins, 2020). This gives us a natural smooth approximation to
Problem (4), namely
min fμ(ω) δ UminU E[f;μ,i(ω,η)] = Fa(ω,η),	⑸
ω∈W	ω∈W,η∈R
1
where f μ,i(ω, η) := 一φμ(fi(ω) - η) + η. We can prove that Fa(ω, η) and Fμ(ω, η) satisfy the
αi
following inequality.
Lemma 2. For any ω, η,
Fa(ω,η) ≤ Fμ(ω,η) ≤ Fɑ(ω,η) + μτa.
Lemma 2 shows that Problem (5) smoothly approximates the original non-smooth Problem (4),
implying that we can solve the original Problem (4) by solving its smoothed version, which will be
proven in next section. Now we propose an algorithm for solving Problem (5), called rFedFair
outlined in Algorithm 1. Here the symbol V represents the (partial) derivative of a function, and we
always require that T - 2 is divisible by K and denote the number of rounds by TN := T-2. AS
summarized in the algorithm, in each iteration t of local updates, each selected client i updates its
local model (ωi, ηi) via a gradient descent step based on its own loss function fμ,i. After K local
iterations, these local models are uploaded to the server where an averaging step is performed. The
5
Under review as a conference paper at ICLR 2022
Algorithm 1 rFedFair
Input: {ω0i = ω0, η0i = η0}, learning rate β, number of local updates κ, and T
Server chooses a set of clients Z (deterministic or random)
for t = 0 to T - 1 do
for all i ∈ Z in parallel do
if t does not divide κ then
ωi+ι = ωi - βVω*"(ωi,ηi)
-	-	___O - ,	-	∙、
ηi+ι = ηi - βvηfμ,(ωt,ηti)
else
client i uploads to server:
ωi - βVω*,i(ωi,ηi)
ηi - βVη*,i(ωi,ηi)
server computes the average of the received models:
ωt+ι = Average({ωi — βVω fμ, (ωt,ηi)}i∈z)
ηt+ι = AverageHni — βvη f",i("i ,ηi)}i∈z)
server chooses a set of clients Z (deterministic or random) and sends ωt+1 , ηt+1 to all
clients in Z
end if
end for
end for
Output： ωτ := TN1+1 PTNO ωrκ+ι
server then chooses a set Z of clients and sends the averaged model to these clients to begin the next
round of local iterations with this fresh initialization. Compared to conventional federated learning
algorithms like FedAvg (McMahan et al., 2017), the local update of client i using gradient descent
is with respect to fμ,i instead of the empirical loss fi, and the client needs to optimize over model
parameter ω and dual variable η jointly. Moreover, we highlight that our algorithm not only can be
efficiently implemented in a federated learning setting but also enjoys theoretical guarantees including
convergence and generalization bounds as shown in next section, whereas the algorithm proposed in
Laguel et al. (2020) is either impractical or a heuristic without any theoretical guarantees. In practice,
the selection and averaging method may vary. Here, we consider the following two strategies for
picking a set of clients and doing model averaging.
Full Participation: In an idealized scenario, each client participates in each round of the communi-
cation. So the server chooses Z = [n], and the averaging step performs
ωt+ι = Pn=Ipi(M-βvωfμ,i(ωi,nt)) .
nt+ι = Pn=Ipi(ni - βvη fμ,i(ωt,nt)) '
However in practice, especially when the total number of clients is huge, the clients participating in
a round of communication are expected to fail or drop out because of broken network connection
or limited client availability, or there may be straggler clients, which take much longer time to send
their output than other clients in the same round. Therefore, it might be unrealistic to assume that the
server collects all client updates.
Partial Participation: A more practical strategy is to sample a subset of clients. To pick a subset of
clients at communication step, we use the sampling scheme (Li et al., 2020b) where server chooses a
subset of clients Z ⊆ [n] with size K < n uniformly at random with replacement according to the
sampling probabilities p1,p2,…,pn Then, the server performs averaging step as follows.
ωt+1 = j Pi∈Z (ωi -βνωfμ,i (ωi,ni))
nt+ι = K Pi∈z5i - βvη fμ,(ωit,而)
Note that our algorithm significantly reduces the number of communications as the local model of
clients are aggregated periodically.
6
Under review as a conference paper at ICLR 2022
5	Theoretical results
In this section, we establish our main theoretical results. We first show that Algorithm 1 converges to
the global minimum of the original non-smooth problem (4) for convex and smooth losses in both
full and partial participation cases. Next, we prove that the returned solution will properly generalize
from training data to unseen test samples.
5.1	Convergence analysis
We first provide convergence guarantees for full participation and then extend the result to partial
participation.
Before introducing the convergence result, we define a few notations. The dot product between
two vectors ω and ω0 is denoted by (ω, ω0), and the norm of a vector is represented by ∣∣∙∣∣. We
also define fα := mι□ω∈w fa(ω) and (ω*,η*) := argminω,η Fμ(ω, η). We make the following
standard assumptions on the loss functions.
i
Assumption 1. For every client i ∈ [n], the empirical loss fi(ω) is L1-smooth and convex. That is,
for any ω, ω0, we have
L
f"ω) + ”ωf23,ω - ωi ≤ ft(ω ) ≤ 产(ω) + ”ωf23,ω - ωi + ɪUω - ωU .
Assumption 2. The empirical loss fi(ω) is L2-Lipschitz, i.e., for any ω, ω0, we have |f i(ω) -
ʌ -, ... …
P(ω )| ≤ L2"ω - ω ||∙
Note that Assumption 2 is only used to prove the smoothness of f μ,i(ω, η), and we Will not use it to
quantify the degree of client heterogeneity. Instead, we consider the following notion of dissimilarity
of client data distribution introduced by Khaled et al. (2019).
Quantifying the degree of client heterogeneity. Weuse P := Pn=Ip∕∣Vω,ηfμ,i(ω*,η*)∣∣2 for
measuring the degree of client heterogeneity. Note that ρis always finite and in case that the client
data is actually i.i.d. and all αis are equal, ρ tends to 0 with mi goes to infinity as it is expected.
5.1.1	Full participation
We now present the convergence of rFedFair with full participation.
Theorem 1. Under the assumptions, ifwe choose the learning rate β and the number of local updates
κ such that β ≤ 1/40L and 6L2β2(κ - 1)2 ≤ 1. Then Algorithm 1 with full participation satisfies,
^ ∕-、	fy 2("ω0-ω* ||2 + (no - η*)2)	ATa2(	[、2	2
fα(ωT) - fα ≤	e(TN + 1)	+ 26Lβ (K - 1) KP + μτα,
where ST := ^^ Pr=O ωr∙κ+ι and L := (Li + (L2 + 1)∕μ) maxi ∖∕α.
Theorem 1 shows that rFedFair converges at rate O(1/T), which is the classical result for convex
optimization. Note that there are two additional error terms in our bound. The second term is due to
the client heterogeneity and would reduce to 0 when P = 0 or K = 1, which is consistent with existing
results. The last term is introduced by the smooth approximation of the original Problem (4). We
can make it small by choosing a small μ. For instance, for small e, by picking μ = e∕2τα, the result
in Theorem 1 shows that rFedFair requires TN = O(1∕2) rounds of communication between
clients and server to achieve a -approximate solution.
5.1.2 Partial participation
As we discussed in Section 4, in the practice of federated learning where the number of clients is very
large, it is more desirable to perform averaging over a random subset of clients. We now shift our
attention to the case and provide convergence guarantees for rFedFair with partial participation.
Theorem 2. Under the assumptions, if we choose the learning rate β such that Lβ(3K∕K + 2) ≤
1∕20 and 6L2β2(K - 1)2 ≤ 1. Then Algorithm 1 with partial participation satisfies,
E(fα(ωτ) - fα) ≤ 2(llω0- ω^2 +((n0-n*)2) + 26Lβ2(κ - 1)2κρ2 + 12βκ2ρ2 + μτ° ,
β(TN + 1)	K
7
Under review as a conference paper at ICLR 2022
where the expectation is with respect to randomness in selecting the clients.
The result in Theorem 2 is similar to that of Theorem 1 except the learning rate condition and an
additional term 12βκ2ρ2/K which measures the difference between random selection of clients and
full participation. We note that despite that the convergence rate depends on the sampling size K,
that influence might be limited because of the presence of other error terms, i.e., 26Lβ2 (κ- 1)2κρ2.
Thus, in practice, one may choose a small set of clients to overcome the problem of dropouts without
severely harming the training process. This result might be extended to other sampling schemes, and
we leave it to future work.
5.2 Generalization bounds
In previous sections, we propose an algorithm to minimize the empirical FAFL problem (3) which is
guaranteed to find an approximate solution. Now we provide learning guarantees for generalization
to the true Qα-weighted loss (1).
To simplify notation, we denote a function class H by composing the functions in F with the loss
function l(∙, ∙), i.e., H = {(x, y) → l(fω(x), y) : ω ∈ W}. The RademaCher complexity of the
function space H given training sample Si = {(xij , yji ) ∈ X × Y : 1 ≤ j ≤ mi} drawn i.i.d.
from some distribution Pi is defined as Rmi (H) = Eσ,si〜Pm [ suPω∈w m1~ Pm=I σjl(fω (Xij) yj)]
where {σj}jm=i1 are independent Rademacher random variables, i.e., P[σj = +1] = P[σj = -1] =
1/2. In federated learning setting, each client has its own data from a different distribution. Therefore,
we define a weighted Rademacher complexity for function space H with respect to the joint data
S = (S1,…，Sn) by Rm(H)= Eσ,s[ suPω∈w pn=ι Pj=I - σij l(fω (xj),yj)]. WiththeSe
αimi
definitions at hand, we can state our first result characterizing uniform convergence properties of
Qα-weighted loss in terms of weighted Rademacher complexity.
Theorem 3. Suppose that the function space H is bounded, i.e., there exists some B > 0 such
that l(fω (x), y) ≤ B holds for all ω ∈ W and (x, y) ∈ Z. Fix α = (αι,…，ɑn) and m =
(mi, ∙∙∙ , mn). Then, for any δ > 0, with probability at least 1 一 δ over the draw of samples
Si 〜Pmi,forall ω ∈ W
.	.	、	O ,	, _ ..	_
fα(ω) ≤ Tαfα(ω) + 2Rm(H) + B
XX P2 6 * * * log(1/S)
i=1 2α2mi
(6)
∖
Remark 3. If the loss function l takes values in {+1, 一1} and the function space H admits VC-
dimension d, the data-dependent weighted Rademacher complexity Rm(H) can be upper bounded
by VZPn=I 2dp2 log(em∕d)∕a2mi. (the proof is given in Appendix F)
Theorem 3 recovers the usual uniform convergence bound for expected loss if letting ai → 1 for
all i. We note that Mohri et al. (2019) also derive a bound using weighted Rademacher complexity.
Compared to (6), their bound has an additional non-vanishing term Bi, and the last term of the bound
is multiplied by an extra factor of Jn logl/i; roughly, this is due to their proof technique relying on
a use of union bound over a ι-cover of the simplex ∆n . Theorem 3, on the other hand, exploits the
relation between Qα-weighted losses and mean of client losses to arrive at a bound, which can avoid
invoking a covering, but at the expense of constant-factor τα to fα (ω). One may expect learning
guarantees, not scaling with this Lipschitz constant τα . Thus, we present an alternative bound which
removes the constant factor at a price of weaker last two terms. See Appendix G for more details.
6 Experiments
In this section, we numerically evaluate the performance of the proposed FAFL framework and
rFedFair algorithm in terms of accuracy and fairness on real-world datasets. We experiment
with three federated datasets considered in prior work using both convex and non-convex models,
including Fashion MNIST (Xiao et al., 2017) with a logistic regression model, a Vehicle dataset
collected from a distributed sensor network (Duarte and Hu, 2004) with a linear SVM, tweet data
curated from Sentiment140 (Go et al., 2009) with a LSTM classifier for text sentiment analysis.
8
Under review as a conference paper at ICLR 2022
Table 1: Test accuracy across 3 clients for models trained with different objectives.
Dataset	Methods	Average (%)	T-shirt (%)	Pullover (%)	Shirt (%)
	FedAvg	78.8±0.2	85.9±0.7	84.5±0.8	66.0±0.7
	AFL	77.8±1.2	82.1±3.9	81.0±3.6	71.4±4.2
FASHION-MNIST	q-FFL	77.8±0.2	80.4±0.6	78.9±0.4	74.2±0.3
	FAFL	78.9±0.5	79.1±0.4	80.7±1.0	76.7±0.6
Table 2: Test accuracy distribution for models trained with different objectives.
Dataset	Methods	Average (%)	Worst 10% (%)	BEST 10% (%)	Variance
	FedAvg	87.3±0.5	43.0±1.0	95.7±1.0	291±18
	AFL	84.3±0.4	49.3±1.6	93.4±0.7	239±14
Vehicle	q-FFL	87.7±0.7	69.9±0.6	94.0±0.9	48±5
	FAFL	87.6±0.3	73.4±2.8	94.3±0.9	39±11
	FedAvg	65.1±4.8	15.9±4.9	100.0±0.0	697±132
	AFL	61.4±0.6	1 2.9±1.3	100.0±0.0	689±39
Sent140	q-FFL	66.5±0.2	23.0±1.4	100.0±0.0	509±30
	FAFL	70.2±0.8	29.0±0.6	100.0±0.0	486±12
Despite that the convergence guarantees for our algorithm only hold for convex loss functions, we
empirically show that it behaves well in non-convex models.
We simulate a federated learning scenario with one server and n clients, where n is the total number
of clients in the datasets. See Appendix H for full datasets details. In all our experiments, we compare
FAFL with the model trained with standard federated learning objective (FedAvg) (McMahan et al.,
2017), agnostic loss (AFL) (Mohri et al., 2019) and q-FFL (Li et al., 2020a), where the latter two
aim to address fairness issues in federated learning. We use rFedFair with full participation on
Fashion MNIST and Vehicle datasets as the number of clients is small, and partial participation on
Sentiment140 where we sample 10 clients at each communication round, i.e., K = 10. The number
of local updates is fixed to κ = 10 for all the experiments. Our framework is flexible in that it allows
each client to select different αi to trade-off between average accuracy and fairness. We conduct
various experiments with different αis to study their effects and report the test accuracy (full results
are provided in Appendix H). All results are averaged over 5 independent trials.
In Table 1, we compare the test accuracy across the three clients from Fashion MNIST dataset. We
observe that while the average accuracy remains unchanged, our FAFL model achieves fairer (more
uniform) test accuracy across the clients. We further report the worst and best 10% test accuracy and
the variance of test accuracy distribution for Vehicle and Sent140 datasets in Table 2. Again, FAFL
achieves lower variance and higher test accuracy on the clients with worse 10% performance for
Vehicle dataset while maintaining roughly the same average accuracy. Finally, for Sent140, our model
performs significantly better than other baselines in terms of both average accuracy and fairness.
7 Conclusion
In this paper, we propose FAFL, a new federated learning framework in which the centralized model
is optimized with respect to a worst-case weighted client loss. We define a notion of fairness named
HCVaR and show an equivalence between FAFL and HCVaR, implying that FAFL is a fairness-aware
objective. We then present an efficient algorithm to solve this objective and provide theoretical
guarantees. Experimental results show that FAFL can gain significant benefits in terms of both
accuracy and fairness. There remains many avenues for future direction. Our framework requires that
the weight qi lies in an interval [0, αi-1] and therefore focuses on clients with large losses. However,
in some scenarios, one may be concerned with more structured uncertainty, e.g., a small subset
[ai , bi] ⊂ [0, αi-1 ]. An interesting question is whether our results can be generalized to that general
case. Moreover, the convergence guarantee in Theorem 2 only applies to sampling with replacement,
and extending the result to other sampling schemes might be an interesting topic for future work.
9
Under review as a conference paper at ICLR 2022
References
B.	Bullins. Highly smooth minimization of non-smooth problems. In Conference on Learning Theory,
pages 988-1030. PMLR, 2020.
Y. Chow and M. Ghavamzadeh. Algorithms for cvar optimization in mdps. Advances in neural
information processing systems, 27:3509-3517, 2014.
S. Curi, K. Levy, S. Jegelka, A. Krause, et al. Adaptive sampling for stochastic risk-averse learning.
Advances in Neural Information Processing Systems, 33, 2020.
M. Donini, L. Oneto, S. Ben-David, J. S. Shawe-Taylor, and M. Pontil. Empirical risk minimization
under fairness constraints. In Advances in Neural Information Processing Systems, pages 2791-
2801, 2018.
M. F. Duarte and Y. H. Hu. Vehicle classification in distributed sensor networks. Journal of Parallel
and Distributed Computing, 64(7):826-838, 2004.
C.	Dwork, N. Immorlica, A. T. Kalai, and M. Leiserson. Decoupled classifiers for group-fair and
efficient machine learning. In Conference on Fairness, Accountability and Transparency, pages
119-133, 2018.
Y. Fan, S. Lyu, Y. Ying, and B. Hu. Learning with average top-k loss. In Advances in neural
information processing systems, pages 497-505, 2017.
A. Go, R. Bhayani, and L. Huang. Twitter sentiment classification using distant supervision. CS224N
project report, Stanford, 1(12):2009, 2009.
F. Haddadpour and M. Mahdavi. On the convergence of local descent methods in federated learning.
arXiv preprint arXiv:1910.14425, 2019.
T. Hashimoto, M. Srivastava, H. Namkoong, and P. Liang. Fairness without demographics in repeated
loss minimization. In International Conference on Machine Learning, pages 1929-1938. PMLR,
2018.
S. Jeong and H. Namkoong. Robust causal inference under covariate shift via worst-case subpopula-
tion treatment effects. In Conference on Learning Theory, pages 2079-2084. PMLR, 2020.
P. Kairouz, H. B. McMahan, B. Avent, A. Bellet, M. Bennis, A. N. Bhagoji, K. Bonawitz, Z. Charles,
G. Cormode, R. Cummings, et al. Advances and open problems in federated learning. arXiv
preprint arXiv:1912.04977, 2019.
A. Khaled, K. Mishchenko, and P. Richtdrik. First analysis of local gd on heterogeneous data. arXiv
preprint arXiv:1909.04715, 2019.
J. Konecny, H. B. McMahan, D. Ramage, and P. Richtdrik. Federated optimization: Distributed
machine learning for on-device intelligence. arXiv preprint arXiv:1610.02527, 2016.
J. Konecny, H. B. McMahan, F. X. Yu, P. Richtarik, A. T. Suresh, and D. Bacon. Federated learning:
Strategies for improving communication efficiency. In NIPS Workshop on Private Multi-Party
Machine Learning, 2016.
P. Krokhmal, J. Palmquist, and S. Uryasev. Portfolio optimization with conditional value-at-risk
objective and constraints. Journal of risk, 4:43-68, 2002.
Y. Laguel, K. Pillutla, J. Malick, and Z. Harchaoui. Device heterogeneity in federated learning: A
superquantile approach. arXiv preprint arXiv:2002.11223, 2020.
J. Lee, S. Park, and J. Shin. Learning bounds for risk-sensitive learning. Advances in Neural
Information Processing Systems, 33, 2020.
T. Li, M. Sanjabi, A. Beirami, and V. Smith. Fair resource allocation in federated learning. In
International Conference on Learning Representations, 2020a. URL https://openreview.
net/forum?id=ByexElSYDr.
10
Under review as a conference paper at ICLR 2022
T. Li, A. Beirami, M. Sanjabi, and V. Smith. Tilted empirical risk minimization. In International
Conference on Learning Representations, 2021a. URL https://openreview.net/forum?
id=K5YasWXZT3O.
T. Li, S. Hu, A. Beirami, and V. Smith. Ditto: Fair and robust federated learning through personaliza-
tion. In International Conference on Machine Learning, pages 6357-6368. PMLR, 2021b.
X. Li, K. Huang, W. Yang, S. Wang, and Z. Zhang. On the convergence of fedavg on non-iid data. In
International Conference on Learning Representations, 2020b. URL https://openreview.
net/forum?id=HJxNAnVtDS.
B. McMahan, E. Moore, D. Ramage, S. Hampson, and B. A. y Arcas. Communication-efficient
learning of deep networks from decentralized data. In Artificial Intelligence and Statistics, pages
1273-1282. PMLR, 2017.
M. Mohri, A. Rostamizadeh, and A. Talwalkar. Foundations of machine learning. MIT press, 2018.
M. Mohri, G. Sivek, and A. T. Suresh. Agnostic federated learning. In International Conference on
Machine Learning, pages 4615-4625, 2019.
J. v. Neumann. Zur theorie der gesellschaftsspiele. Mathematische annalen, 100(1):295-320, 1928.
J. Pennington, R. Socher, and C. D. Manning. Glove: Global vectors for word representation.
In Proceedings of the 2014 conference on empirical methods in natural language processing
(EMNLP), pages 1532-1543, 2014.
R.	T. Rockafellar, S. Uryasev, et al. Optimization of conditional value-at-risk. Journal of risk, 2:
21-42, 2000.
S.	Samadi, U. Tantipongpipat, J. H. Morgenstern, M. Singh, and S. Vempala. The price of fair pca:
One extra dimension. In Advances in Neural Information Processing Systems, volume 31, 2018.
S.	Shalev-Shwartz and Y. Wexler. Minimizing the maximal loss: How and why. In ICML, pages
793-801, 2016.
A. Shapiro, D. Dentcheva, and A. Ruszczynski. Lectures on stochastic programming: modeling and
theory. SIAM, 2014.
T.	Soma and Y. Yoshida. Statistical learning with conditional value at risk. arXiv preprint
arXiv:2002.05826, 2020.
S. U. Stich. Local SGD converges fast and communicates little. In International Conference on Learn-
ing Representations, 2019. URL https://openreview.net/forum?id=S1g2JnRcFX.
R. Williamson and A. Menon. Fairness risk measures. In International Conference on Machine
Learning, pages 6786-6797, 2019.
H. Xiao, K. Rasul, and R. Vollgraf. Fashion-mnist: a novel image dataset for benchmarking machine
learning algorithms. arXiv preprint arXiv:1708.07747, 2017.
C. Yang, Q. Wang, M. Xu, S. Wang, K. Bian, and X. Liu. Heterogeneity-aware federated learning.
arXiv preprint arXiv:2006.06983, 2020.
M. B. Zafar, I. Valera, M. Gomez Rodriguez, and K. P. Gummadi. Fairness beyond disparate treatment
& disparate impact: Learning classification without disparate mistreatment. In Proceedings of the
26th international conference on world wide web, pages 1171-1180, 2017.
11
Under review as a conference paper at ICLR 2022
A Proof of Lemma 1
Proof. We begin by introducing a Lagrange multiplier η for the constraint E[qi] = 1, and form the
Lagrangian
L(η, q) := E[qif i(ω)] +η(1 - E[qi]) = E[qi(fi(ω) - η)] +η.
Thus, fα is equivalent to
sup	inf L(η, q).
q∙∙qi∈[0,1∕αi] n∈R
By switching inf and sup, we obtain the following inequality
sup	inf L(η, q) ≤ inf sup	L(η, q).
q-.qi∈[0,1∕αi] n∈R	η∈R q-.qi∈[0,1∕ɑi]
(A.1)
The inner maximization problem in the right hand side can be solved exactly by letting qi = 0 if
f i(ω) - η < 0 and qi = αi-1 if fi (ω) - η ≥ 0, leading to
inf sup
η∈R q0∈[0,1∕ɑa]
L(q,η) = inf(E -1(fi(ω) - η)+ + η).
η∈R
αi
Therefore, to prove the first part of the lemma, it remains to show that equation (A.1) holds with
equality. Denote L = mini fi(ω) and U = maxi fi(ω). Since η → g(η) := E[一(fi(ω) - η)+]+ η
αi
is strictly increasing on [U, ∞), we have g(η) ≥ g(U) for η ∈ [U, ∞). For η ≤ L, we have
g(η) = E[j*-(fi(ω)] + η(1 - E(j*-)) which is non-increasing as E(j*-) ≥ 1. So g(η) ≥ g(L) for
αi
αi
αi
η ≤ L. Therefore, we may restrict the domain of η on a compact convex domain [L, U]. Now since
η → L(η, q) is linear and thus convex, q → L(η, q) is linear and thus concave, and the domain of q
and η are both compact and convex, the von Neumann’s minimax theorem (Neumann, 1928) implies
that the equality holds, which completes the proof of the first part.
The second part can be proven as follows.
infη∈R
E -1(fi(ω) - η)+ + η
= infη∈R
αi
⅛
Eα (fi(ω) - η)+
+η
=Eα[fi(ω)∣fi(ω) > Qi-i∕τα(fi(ω))]
= HCVaR1-α(fi(ω))
where the second equality follows from Theorem 1 of Rockafellar et al. (2000).
□
B Proof of Lemma 2
The proof of Lemma 2 relies on the following result.
Proposition 1 ((Bullins, 2020), Lemma 3). For x ∈ R,
(x)+ ≤ Φμ(χ) ≤ (χ)+ + μ.
Proof. By Proposition 1, we have (f (ω) - η)+ ≤ φμ(f (ω) - η) ≤ (fi(ω) - η)+ + μ. Multiplying
by ——on both sides and summing them up give us the desired result.	口
αi
C Proof of Theorem 1
This section includes the full proof of Theorem 1. For ease of notation, we introduce the following
shorthand notations. We denote θ := (ω, η) ∈ W × R. Then the local and global losses can be
rewritten as fμ,i(θ) := fμ,i(ω,η), Fg(θ) := Fμ(ω,η), and Fα(θ) := Fα(ω,η). Let the average
n
model at iteration t be θt = Ei=I Pi°t and the minimizer θ* := arg min Fμ(θ). We first establish
the convexity and Lipschitz gradient property of fμ,i and Fa with respect to the parameter θ.
12
Under review as a conference paper at ICLR 2022
ɪ .	___CY T∕∙ . 1	∙ ∙	1 1	P∙? /	∖	Λ	. ∙	1 J .ι fill 4 / ∕t∖ ι 'Till / ∕t∖
Lemma C.1. Ifthe empirical loss f i(ω) satisfies Assumption 1 and 2, then f μ,i(θ) and FJ(θ) are
convex and have Lipschitz gradients as follows, for any θ, θ0,
ʌ -, . ʌ .......... ...
∣∣Vf”,i(θ)-Vf”,i(θ0)∣∣ ≤ L∣∣θ - θ0∣∣
∣∣VFμ(θ) -vFμ(θ0)∣∣≤ l∣∣θ -θ0∣∣	,
where L := (Li + 22 + ɪ) maxi ɪ.
μ	ai
Proof. Denote g(θ) = fi(ω) 一 η. By Assumption 1 (smoothness) and 2, We have ∣∣Vg(θ) 一
Vg(θ0)∣∣ ≤ Lι∣∣θ 一 θ0∣∣ and ∣∣Vg(θ)∣∣ ≤ PL + 1. Then, the Lipschitz gradient parameter for
φμ(g(θ)) can be calculated as follows.
IVΦμ(g(θ))-Vφμ(g(θ0))∣∣
=ι∣Φμ(g(θ))Vg(θ) - φμ(g(θo))Vg(θo)∣∣
=ι∣Φμ(g(θ))Vg(θ) — Φμ(g(θ))Vg(θo) + φμ(g(θ))Vg(θ0) — φμ (g(θ0))Vg(θ0)∣∣
≤ Φμ(g(θ))∣∣Vg(θ) — Vg(θo)∣∣ + ∣φμ(g(θ)) — Φμ(g(θo))∣ι∣Vg(θo)∣∣
≤ Lι∣∣θ 一叫 + 1∣g(θ)- g(θ0)ll∣Vg(θ0)∣∣	,
μ
≤ (Li + L2+1)l∣θ -θ0ll
μ
1
where the second inequality uses φ (∙) ≤ 1 and --smoothness of φμ. Since f μ,i(θ) = 一φμ(f i(ω)一
L	μ	a
η) + η and F^μ(θ) = Pi=iPifμ,i(θ),we obtain
∣∣Vf”,i(θ) — V产,i(θ0)∣∣ ≤ (Li + L!+1)工∣∣θ 一 θ0∣∣.
μ ai
And,
∣VFμ(θ) - VFμ(θ0)∣∣ ≤ (l- + L^+-1)XX 詈他一少儿
μ	i=1 αi
To show the convexity of f μ,i(θ) and Fa(θ), we first observe that g(θ) is convex with respect to θ as
,. ʌ-, ʌ-, . ʌ-, .. . , , ..
g(θ) = fi(ω) — η ≥ fi(ω0) — η0 + hVfi(ω0),ω ― ω0) + η' - η = g(θ0) + (Vg(θ0),θ 一聆 where
the first inequality uses Assumption 1 (convexity). Then since φμ(∙) is convex and non-decreasing,
we can conclude that φμ(g(θ)) is also convex with respect to θ. Therefore, f*,i(θ) and Fαμ(θ) are
convex.	□
We next bound the average deviation of local models from their average over T iterations. For this
purpose, we first prove a technical lemma given as follows.
Lemma C.2. Suppose that two non-negative sequences {It}t≥o(I0,Iz+∙κ+ι = 0) and {Ht}t≥0
satisfy the following inequality for each iteration t ≥ 0 and some constants Ci ≥ 0, C2 ≥ 0 and
C3 ≥ 0:
t-i	t-i
It ≤ Ci X	Il + C2 X	Hl + C3,	(C.1)
l=tκ +i	l=tκ +i
where tκ := bt--1κ. Iffurther assuming that C-(κ 一 1) ≤ -, then we have
T-i	T-i
X It ≤ 2C2(κ - 1) X Ht + 2C3T.
t=0	t=0
13
Under review as a conference paper at ICLR 2022
Proof. We apply the inequality (C.1) to each iteration t = 0, ∙∙∙ ,T - 1 and obtain
Io = 0
(Ii =0
J I2 ≤ CiIi + C2Hi + C3
〔IK ≤ Ci(Ii + …+ Iκ-i) + C2(Hi + …+ Hκ-i) + C3
(Iκ+i = 0
I Iκ+2 ≤ CiIK+i + C2HK+i + C3
I I2κ ≤ Ci(Iκ+i …+ I2κ-i) + C2(Hκ+i ∙∙∙ + H2κ-i) + C3
.
.
.
{∕(T- i)κ + i = 0
I(T-i)κ+2 ≤ CiI(T-i)κ+i + c2h(t-i)κ+i + C3
IT-i ≤ Ci(I(τ-i)κ+i …+ IT-2) + C2(H(τ-i)κ+i …+ HT-2) + C3
Summing the above inequalities yields that
τ-i	τ-i	τ-i
X It ≤ Ci(K - 1) X It + C2(κ - 1) X Ht + C3T.
t=0	t=0	t=0
As Ci(K - 1) ≤ 2 by assumption, rearranging the terms gives
τ-i	τ-i
X It ≤ 2C2(κ - 1) X Ht + 2C3T.
t=0	t=0
□
The following lemma bounds the sum of model variance from iteration t = 0 to T - 1.
Lemma C.3. Ifthe Assumption 1 and 2 hold and the learning rate β satisfies 6L2β2(κ 一 1)2 ≤ 1,
then
PTOi P乙PiIw- &ll2 ≤ 8Lβ2(K- 1)2 PTOi(欣(@)-欣(θ*)) + 12Tβ2(κ - 1)2ρ2 ,
where ρ2 := Pn=IPiIl▽户,i(θ*)∣∣2.
Proof. Consider an iteration t and denote by tκ the step of the most recent communication between
the clients and the server, i.e., tκ = [ t-i Jκ. Then by the update rule of Algorithm 1, all the clients
have the same local model at iteration tκ + 1, i.e., θttκ+i = •… =θn+i = &κ+i, and for each client
we can write 3； = θ[+i -β Pt-i+i ▽/*〃(4). Therefore, we can upper bound PZi Pi∣∣θi - ®t ∣∣2
as follows.
PNi Pi∣∣θi - θt∣∣2
=β2 PNi Pi∣∣ Pt-iκ+i v*,i(3i) - Pt-κ+i E[V户,i(3i)]∣∣2
≤ β2(t - 1 - tκ) Pn=IPi Pt-tiκ+i∣∣V*,i(3i)- E[V*,i(3i)]∣∣2 ,	(C.2)
≤ β2(κ - 1) Pt-κ+i Pn=IPi∣∣V户,i(3i) - E[V户,i(3i)]∣∣2
≤ β2(κ- 1) Pt=κ+i Pn=IPi∣∣V户,i(3i)∣∣2
where the first inequality follows from the Jensen,s inequality, the second inequality is due to the fact
that t - tκ ≤ K by definition, and the last inequality uses Var(Z) ≤ E(Z2).
14
Under review as a conference paper at ICLR 2022
Now we proceed to bound Pn=IPi∖∖^fμ,i(θ∕)∣∣2.
∑n=1 Pi∖∖Vfμ,i(θi)∖∖2
=∑n=!Pi∖∖Vfμ,i(θi) - Vfμ,i⑥)+ Vfμ,i(θl) - Vfμ,i(θ*) + Vfμ,i(θ*)∖∖2
≤ ∑n=!Pi(3∖∖V/μ∙i(θi) - V/μ,i(θι)∖∖2 + 2∖∖V/μ,i(θι) - V户,i(θ*)∖∖2 + 6∖∖V∕μ,i(θ*)∖∖2)
≤ ∑n=1 Pi(3L2∖∖θi - θι∖∖2 + 2∖∖Vfμ,i(θι) - V*,i(θ*)∖∖2 + 6∖∖V*,i(θ*)∖∖2)
≤ ∑n=!Pi(3L2∖∖θi - θι∖∖2 + 4L(fμ,i(θι)-户，i(θ*) - "∕μ,i(θ*), θι - θ*i) + 6∖∖V*,i(θ*)∖∖2)
=3L2 pn=1 Pi∖∖θi - θι∖∖2 +4L(欣⑥)-欣(θ*) -hV次(θ*), θι - θ*〉)+ 6pn=1 Pi∖∖V*,i(θ*)∖∖2
=3L2 pn=1 Pi∖∖θi - θι∖∖2 +4L(Fμ(θι) - F(θ*))+6ρ2
(C.3)
where the first inequality uses AM-GM inequality, the second inequality follows from the Lipschitz
gradient, the third inequality uses the co-coercivity of convex and smooth function, and the last
equality holds as VFμ(θ*) = 0.
Plugging (C.3) back in (C.2) yields
∑n=1 Pi∖∖θi - θt∖∖2
≤ 3L2β2(κ - 1) PtI+ι Pn=IPi∖∖θi - θι∖∖2 + 4Lβ2(κ - 1) PtI=:《F® - Fj(θ*))+ .
6β2(κ - 1)2ρ2
(C.4)
Since 3L2β2 (K - 1)2 ≤ ɪ by assumption, we apply Lemma C.2 to derive the desired result. □
We now return to the proof of Theorem 1.
Proof. We begin by noting that @+1 = Pnn=ι PiW - βV∕μ,i(θi)) always holds by the update rule
of rFedFair. Then we can write
∖E+ι-θ*∖∖2
=∖∖Pn=ι Pi(θi - β V*,i(θi ))-θ*∖∖2
=∖∖θt - β Pn=ι PiV *,i(θi)-θ*∖∖2
=∖∖θt - θ*∖∖2+β2∖∖ Pn=I PiV 户，i(θi)∖∖2 - 2β(θt - θ* ,Pn=ι PiV fμ 网))
For the term ∖∖ Pn=1 PiV∕μ,i(θi)∖∖2, it can be further decomposed as
∖∖ Pn=1 PiV ∕μ,i(θi)∖∖2
=∖∖Pn=ι PiV ∕μ,i(θi) - Pn=I PiV /"(S+Pn=I PiV *,i(θt)∖∖2
≤ 2∖∖ Pn=IPiV ∕μ,i(θi) - Pn=IPiV ∕μ,i(θt)∖∖2+2∖∖ Pn=IPiV ∕μ,i (^t)∖∖2
≤ 2Pn=1 L2Pi∖∖θi - 0t∖∖2 + 2∖∖ Pn=IPiV∕μ,i(0t)∖∖2
=2 Pn=1 L2Pi ∖∖θi - 0t∖∖2 + 2∖∖VFμ(θt)∖∖2
≤ 2Pn=I L2Pi∖∖θi - 0t∖∖2 + 4L(Fμ(θt) - Fμ(θ*))
(C.5)
(C.6)
where the first inequality uses ∖∖α + b∖∖2 ≤ 2∖∖α∖∖2 + 2∖∖b∖∖2, the second and last inequalities follow
from the Lipschitz gradient of fμ,i(θ) and Fμ(θ) by Lemma C.1.
We also upper bound the last term as follows.
-2β(θt-θ*,Pn=ι PiV*,i(θi)i
=β Pn=I-2Pi(θt-θ*, V*,i(θi))
=βPn=IPi[-2hθi - θ*, V∕μ,i(θi)i - 2(& - θi, V∕μ,i(θi)i]
≤ β Pn=IPi[2(*,i (θ*) - ∕μ,i(θi)) - 2例-θi, V ∕μ,i(θi)i]
≤ βPn=IPi[2(∕μ,i(θ*) - *,i(θt)) + L∖∖θt - θi∖∖2]
=β[2(Fμ(θ*) - Fμ(θt)) + Pn=IPiL∖∖θt - θi∖∖2]
(C.7)
1	.1 r∙ . ∙	ι ∙ .	.1	∙ . i' pʃf √ / Zi∖	ι .1	ι ∙	ι ∙ .	. ) τ ∙	< ∙.
where the first inequality uses the convexity of fμ,i(θ), and the second inequality uses the Lipschitz
gradient of ∕μ,i(θ).
15
Under review as a conference paper at ICLR 2022
Plugging (C.6) and (C.7) back in (C.5) implies that
lBt+ι-θ*ll2
≤ ||& - θ*∣∣2 + (2L2β2 + βL) Pn=ιPi旭-创∣2 + (4Lβ2 - 2β)(欣(&)-改(θ*))
21	=	19	,
≤ % - θ*ιι2 + 20βLpn=ιPi∣∣θt - θt∣ι2 -10egg -9))
(C.8)
where the second inequality uses the assumption that β ≤ ^ʒɪ. Summing UP all the T inequalities
in (C.8) from t = 0,1,…，T - 1 gives
画-θ*∣∣2-∣Bo-θ*∣∣2
≤ 20βLPT-I Pn=IPiM -吼2 -19βPT-I制(a) - Fμ(θ*))
2201	19	10	21
≤ (20β8L2β2(κ - 1)2 - 10β) PT01(Fμ(θt)-砥(θ*)) + MβL12Tβ2(κ - 1)2ρ2	©切
20	10	20	,
≤ -1 βPT01(欧(4)-砍(θ*)) + 13βTLβ2(κ - 1)2P2
≤-1 β PTNo 制⑹∙κ + 1)-Fμ(θ*)) + 13βTLβ2(κ - 1)2ρ2
where the second inequality uses Lemma C.3, the third inequality is due to the assumption that
6L2β2(κ - 1)2 ≤ 1, and the last inequality follows from the definition of θ*. Rearranging the terms
and dividing both sides by 2β(TN + 1) yield that
TN + 1 PPr=0 Fa(θ• κ+1)-F¾(θ*)
≤⅛^ + 26占Lβ2(" 1)2P2 ≤ %二* + 26Lβ2(κ -	(CIO)
β(TN + 1)	TN + 1	β(TN + 1)
where the last inequality uses TTpɪ ≤ κ.
Finally, we lower bound LHS of (C.10).
ɪv PTNo 欣@ κ+ι)- Fag
TN + 1
=TN + 1 PPr=0Fa(ωr∙κ + 1,ηr∙κ + l)- Fj(θ*)
≥ F¾ TN⅛ PTNO ωr∙κ + 1, tN+ PTNO ηr∙κ + l)- ^⑹)	，	(CII)
≥ Fa ( TN1+1 PTNO ωr∙κ+1, TN+1 PTNO %κ + l) - f - μ Pn=1 个.
≥ fagT) - fa - μ Pn=ι "	”
αi
where the first equality is due to the update rule of Algorithm 1, the first inequality uses Jensen’s
inequality, the second inequality follows from Lemma 2 which shows that Fa(∙) ≤ Fμ(∙) and
F2(θ*) ≤ fa + μ Pn= ι pi, and the last inequality is by the definition of fa.
αi
Combining (C.10) and (C.11) gives us
r> , 、 ʌ,
fa @T) - fa ≤
2∣Bo - θ*∣∣2
β(TN + 1)
n
+ 26Lβ2(κ — 1)2κρ2 + μ	pi.
i=1 αi
□
D	Proof of Theorem 2
In this section, we prove the convergence of rFedFair with partial participation. The main
challenge here is that the randomly selected subset of clients varies each round, which makes the
analysis complicated. To overcome this difficulty, we first notice that the update rule of rFedFair
with partial participation is equivalent to the following form: at every iteration, each client i ∈ [n]
performs local updates; then after κ local iterations, the server does an average step over the local
16
Under review as a conference paper at ICLR 2022
models received from a randomly selected subset Z of clients, and the averaged model is sent back
to all clients to begin the next round. We then introduce a virtual sequence {"t}t≥o and rewrite the
update rule of Algorithm 1 as: for all i ∈ [n],
%ι = θi -βv*,i (θi)
θ _ ʃ %+ι if t does not divide K .	(D.1)
t+1 - I KK Pi∈z %1 OtherWiSe
Note that the virtual sequence and {θi}i∈Z never have to been computed explicitly and are just tools
used for the analysis. Now the only difference with the case of full participation is that at each
communication round the server performs averaging step over a random selection of clients sampled
with probability p1,p2, ∙∙∙ ,Pn instead of all clients. If that average does not deviate much from the
average model across all clients, one may expect to use similar technique to prove the result for partial
participation. Following this logic, we first bound how far the true average model θ can derivate
from the virtual average over T iterations in the following lemma. Denote by St = PNi PM the
average virtual model at iteration t. To simplify the notation, in what follows we simply use E[∙] to
denote expectation with respect to sampling of clients at each communication round.
The proof of lemma relies on the following result.
Proposition 2. Let {ei}in=1 denote any fixed deterministic sequence. We uniformly sample a subset
with size K where ei is sampled with probability pi for 1 ≤ i ≤ n with replacement. Let Z =
{ii,…，iκ} ⊂ [n]∙ Then,
Lemma D.1. If the Assumption 1 and 2 hold, then
E PT-o1 l∣θt+ι-也+1||2	_	_
≤ KkL2β2κEPT-OPn=IPiiW -θt∣ι2 + KLβ2κEPTXFa⑹)-Fs)) + Kκβ2κ"2(TN + 1).
(D.2)
Proof. First note that if t does not divide κ, we have &+i =历 t+i by (D.1) and ||&+i — St+ι∣∣2 = 0.
We can write
EPT-0l∣∣θtt1-见+1∣∣2
=E PrNO ilθ^rκt1 -历rκt1ii2
=EPrN IlKK Pi∈Zr Sirκti - Srκti||2	,	(D.3)
=E PrN K2 Pi∈ZrII%t1- SrKtiII2
=E K PTNO Pn=i PiMKtI- SrKtiII2
where the third equality is due to the independent and unbiased sampling of clients, and the last
equality follows from Proposition 2.
By the update rule (D.1), for each client i, we have SirKti = θ(ir-i)Kti - β Plr=K(r-i)Ktivfμ,i(θi).
Thus, the inner summation can be further upper bounded as follows.
Pn=IPiUSrKti- SrKti II
=β2 Pn=ipi" Pr=(r-i)κti Vfμ,i(θi)- Pr=(r-i)Kti E[v*,i (θi)]II2	,
≤ 3L2β2κPr=(r-i)Kti Pn=IPiIIθi - θιII2 +4Lβ2κPr=(r-i)Kti(Fμ(θι) - Fμ(θ*)) + 6β2κ2ρ2
(D.4)
where the last inequality follows from (C.2) and (C.4).
Plugging (D.4) back in (D.3) yields that
E PT-OiIBtti-Sttill2	_	_
≤ K L2β2κE PTNO Pr=(r-i)Kti Pn=i PiIIθi -创I2 + K Lβ2κE PTNO Pr=(r-i)Kti(Fμ(θι)- Fμ(θ*))+
~Kβ2κ2p2 (TN + I)
≤ KkL2β2κEPTOi PLiMW - &ii2 + KkLβ2κEPTOi(Fμ(3 - Fae)) + Kkβ2κ2ρ2(TN + 1)
(D.5)
which completes the proof.	口
17
Under review as a conference paper at ICLR 2022
Now we proceed to prove Theorem 2, which follows similar argument to that of Theorem 1.
Proof. We begin by decomposing the optimality gap as
E∣∣4+ι-θ*∣∣2
=E||&+1 -乩+1 + 乩+1 - θ*∣∣2
=E∣∣a+1 -乩+1∣∣2 + E∣∣‰1 - θ*∣∣2 + 2Ehθt+ι - ι9t+ι,‰ι - θ*)
=E∣Kt+ι-加+ι∣∣2 + E∣叽+ι- θ*∣∣2
(D.6)
where the last equality holds since EZ&+1 =历t+1.
The second term in RHS of (D.6) can be bounded as follows.
E∣凤+ι-θ*∣∣2
=E∣∣ Pn=IPiwt -βV*"(θi))-θ*∣∣2
=E∣∣θt- β Pn=IPiV*"(θi)-θ*∣∣2	,
=E[K - θ*∣∣2 + β2∣∣ pn=ιptVf”,i(θi)∣∣2 - 2β〈& - θ*, pn=ιPiV户，i®)〉]
≤ E∣∣θt - θ*∣∣2 + (2L2β2 + βL)EPn=ι洲优-θt∣∣2 + (4Lβ2 - 2β)E(欣(国)-欣(θ*))
(D.7)
where the first equality uses (D.1), and the last inequality follows from (C.6) and (C.7).
Plugging (D.7) back in (D.6) and summing up from t = 0,1, ∙ ∙ ∙ ,T - 1 yield that
E∣∣% -θ*∣∣2-∣∣θ0-θ*∣∣2
/ T LT-1 Iln	CG ∣∣2 1 /or2∕θ2 ∣ dt WΓV^t-1 Ln „ I ini n 112]∣
≤ ETt=0 ∣∣θt+1 -必+1∣∣ + (2L β + βL)E[Tt=0 Ti=Ipi∣∣θt - θt∣∣ ]+
(4Lβ2 - 2β)EPT01 (欣(θt)-欣(θ*))
≤ (2L2β2 + βL + 春L2β2κ)E[PT=01 Pn=IPi∣∣θi - nt∣∣2]+
n^^>	-1 ʌ	ʌ	_
≤
≤
≤
(4Lβ2 + 高Lβ2κ - 2β)EPT=01(超(θt)-超(θ*)) + Kβ2κ2ρ2(TN + 1)
21 QT K LT-1 Ln ,ʌ I ini n∣ |2 19 LT —1( ^ μ(刀、 ^ μ( n*λλ ∣ 6 &2-2八2 (T ∣ ι ʌ
20βLETt=0 Ti=IPi∣∣θt-θt∣∣ -10βETt=0 (Fa(θt)-Fa(θ))+ Kβ K P (TN +1)
-1 βEPT01mμ(nt)-户μ(θ*)) + 13βTLβ2(κ - 1)2P2 + K6β2κ2P2(Tn + 1)
-2βEPTN0(Fμ(源+1)-户μ(θ*)) + 13βTLβ2(κ - 1)2ρ2 + 关β2κ2p2(Tn + 1)
(D.8)
where the second inequality uses Lemma D.1,the third inequality holds since Lβ(3κ∕K+2) ≤ 击 by
assumption, the fourth inequality follows from Lemma C.3 and the assumption that 6L2β2 (K - 1)2 ≤
1, and the last inequality holds as θ* is the mmιmizer of Fμ(∙).
Rearranging the terms and dividing both sides by 1 β(TN + 1) give
ET^+1 PT=0(欧(猿+1)-砍(θ*)) ≤ 2 β;T; ：1： + 26Lβ2(κ - 1)2κρ2 + Kβκ2ρ2,
N	N	(D.9)
where we use TTpɪ ≤ κ.
Again, we apply Lemma 2 to lower bound the LHS of (D.9) and obtain
ET^+ɪ PTN0底(猿+1)-改(θ*))
=Eɪr PT=0 超(ωr∙κ+1, ηr∙κ+1)-超(θ*)
TN + 1
≥ E 户μ(Tn+1 PrN0 ωrκ + 1,TN⅛ PTN0 ηr∙κ+1)-超(θ*)
≥ EΓα(7N⅛τ PTN0 ωr∙κ + 1, τ⅛ P= ηr∙κ + 1) - f - μ £乙乡
_「一、「.、	αi
≥ E(fα(ωT) - fα) - μτɑ
(D.10)
where the first equality uses the update rule (D.1).
Finally, combining (D.9) and (D.10) concludes the proof.
□
18
Under review as a conference paper at ICLR 2022
E Proof of Theorem 3
Proof. We begin by rewriting fα (ω) using its dual representation
fα(ω)
= infη∈R
= infη∈R
η + E -1(fi(ω) - η)+ ∖
αi
η + ——iEa (fi(ω) - η)+
τα-1
(E.1)
By choosing η = 0 in (E.1), we obtain the following inequality which holds for any ω ∈ W
fa (ω) ≤ 占 Ea 卜f i(ω))] ≤ t^ Eau"A]”(S)，	(Ez
where Ψ(S) ：= suPω∈w [ —-iEa (fi(ω)) ——1-ιEa (fi(ω))
τa-	τa-
in the RHS of (E.2) can be bounded as follows.
.The first term Ea[(fi(ω))]
Ea (fi(ω)) = infη∈R Jn + Ea (f (ω) - η)
≤ infη∈R {η + τ~-1 Ea (f (") - η)+ }
=infn∈R {η + E 01-(fi(ω) - η)+ }
=fa(ω)
(E.3)
where the first inequality uses (∙) ≤ τ⅛ (•)+ as Ta ≥ 1.
To bound the second term, we make use of McDiarmid’s inequality. Let S0 be a sample differing from
S by exactly one point, say (xij, yji ) in S and (x0ij, y0ij) in S0. By definition of Ψ(S), the following
inequality holds:
Ψ(S) - Ψ(S0)
= supω∈W
supω∈W
= supω∈W
≤ supω∈W
= supω∈W
= supω∈W
piB
Ta
1
Ta1
E
Ea (fi (ω)) -
Ea (fi (ω)) -
-1 f i (ω)
αi
1i
-f0 (")
αi
Ta
1
Ta1
αi
αi
”(f0i(ω)- fi(ω))
Ea (fi(ω))
i
Ea (f0 (ω))
- supω∈W
E
-E —f0 (ω)
αi
αpm- (l(fω (χ0j),y0j)- l(fω (χj),yj))
E
1
1

1



1
—
ʌ
αimi
(E.4)
where the first inequality uses the sub-additivity of sup, and the last inequality is due to the boundness
assumption on the loss function.
By McDiarmid’s inequality, with probability at least 1- δ, the following inequality holds
n
Ψ(S) ≤ ESΨ(S) +B
Σ
i=1
p2 log( δ)
2αi2mi
(E.5)
∖
19
Under review as a conference paper at ICLR 2022
The expectation on RHS of (E.5) can be further bounded as follows.
ESΨ(S)
1

ES supω∈W [E _f i(ω) - E _f (ω)
αi
αi
"f3
ES supω∈W Pin=1
f i T-
Es supω∈w EsoP乙 Pi f0(ω) - Pn=
αi
(E.6)
≤ ES,S0 supω∈W
Pn=I pif0i(ω)-产(ω))
αi
1
Ii fω
ES,S0,σ supω∈W	Pin=1 Pjm=i1
≤ 2ES,σ supω∈W	in=1	jm=i1
Opm σijl(fω (xj),yj)
where the first inequality uses Jensen’s inequality and the convexity of the supremum function, the
last equality follows from the fact that the introduction of Rademacher variables does not change the
expectation over all possible S and S0 , and the last inequality holds by the sub-additivity of sup and
the fact that σij and -σij have the same distribution.
Plugging (E.6), (E.5) and (E.3) into (E.2) concludes the proof of the theorem.
□
F Proof of Remark 3
>Λ	/' 1 '	Γ' 1	IC	/∕^*1	C” \	1	. 1 c~∕ ,1	, C∙	.	CC	. •	1
Proof. For a fixed sample S = (S1,…，S n),we denote by H∣s the set of vectors of function values
p pi l(fω(xj), yj))	where ω is in W. Since l(fω(xj),yj) takes values in {-1, +1},
∖%mi	%j)∈[n]×[mz]	J J
I	P2~
the norm of these vectors is bounded by J ɪɪι -i^—. By applying Massart,s lemma, we obtain
i=1 αi2 mi
Rm (H)
Es Eσ supω∈w Pn=I Pj=I 为σjl(fω(xj),yj)
αimi
≤ √P=1⅛ESq2lOg1H1S1 ≤ j2 Pn=I 总 logπH(M，
where ΠH(m) is the growth function, and the last inequality uses the definition of growth function.
Moreover, by Sauer,s lemma, we have Π∏(m) ≤ (号)d as H admits VC-dimension d. Combining
the above steps yields that
Rm (H) ≤
∖
n 2dpi2	em
工诉log 7
i=1 i
□
G Alternative Generalization B ound
Theorem G.1. Let α = (αι, •…，in) and m = (mi, ∙∙∙ , mn) be fixed, and let thefunCtion space
H be bounded, i.e., there exists some B > 0 such that supz∈Z l(fω (x), y) ≤ B holds for all ω ∈ W.
Then, for any δ > 0, with probability at least 1 一 δ over the draw of samples Si 〜Pirm, for all
ω∈W
n
n
B	⅞mn
mi
fα(ω) ≤ fα(ω) + 2 X 竺Rji(H)+ X ”
αi	αi
i=1	i	i=1	i
(G.1)
20
Under review as a conference paper at ICLR 2022
Proof. For any ω ∈ W , we have
.	,	、	O ,	、
fα(ω) - fα(ω)
二infη∈R {η + E α(fi(ω) - η)+ - - infη∈R {η + E
≤ E —(fi(ω) - η*)+------(fi(ω) - η*)+
αi	αi
≤ E -1 lfi(ω)- fi(ω)∣
1
α (r(ω)- η)+
(G.2)
where the first equality uses the variational representation of Qα-weighted loss given in Lemma 1,
the first inequality holds by selecting the first η to be identical to the minimizer " of second inf
function. Taking supremum over W , we get
supω∈W {fα (ω) - fα (ω)}
≤ supω∈w{Pn=ι OiIfi(ω) - fi(ω)∣} ,	(G.3)
≤ Pn=1 Oi SUpω∈W |fi(ω) - fi(ω)1
where the last inequality uses the sub-additivity of sup.
For a fixed mi, by a standard Rademacher complexity bound (Mohri et al., 2018), for any δ > 0, with
probability at least 1 - δ, the following inequality holds
..-, ʌ.-,.. .-,
≡UWf i(ω)- fi(ω)l≤ 2Rmi(H) + B
2n
log 了.
Plugging the above inequality back in (G.3) for each i and using a union bound yields that for every
ω ∈ W,
fα(ω) ≤ f0(ω) +2 Pn=1 Oi Rmi (H) + Pn=1 Si B r 2~~ log ɪ	(G4)
2mi
with probability at least 1 - δ. This completes the proof.
□
Similar to typical uniform convergence guarantees for empirical risk, the bound (G.1) vanishes
to zero at the rate 1 /√mi for standard hypothesis space whose Rademacher complexity could be
bounded from above by O(1/√mi) term. Compared to the generalization bound of Theorem 3, the
bound (G.1) does not involve a constant factor τO. But the last two terms are less favorable that of (6).
This can be observed as follows. By the sub-additivity of sUp and the linearity of expectation, we can
write
Rm(H) = Eσ,S 卜UPω∈W Pi=I Pm=I Om。切1小(Xj)必)
≤ Eσ,S Pn=I P supω∈W Pm=I —σij l(fω (Xj),yj).
αi	mi
=Pn=1 Oi Rmi (H)
Analogously, the last term satisfies the inequality JPn=I Om ≤ Pn= 1 Oi ∖∣~m by subadditivity
of √∙.
H	Experimental Details
We now provide full details on the datasets and models used in our experiments. To construct client
data, we partition each dataset as follows.
Fashion MNIST. The Fashion MNIST (Xiao et al., 2017) dataset is an MNIST-like dataset where
images are classified into 10 categories of clothing. We follow the same procedure as that in Mohri
et al. (2019) to extract a subset of data labelled with categories t-shirt/top, pullover, and
shirt and split this subset into 3 clients, each consisting of a class of clothing. We then trained a
classifier for the three classes using logistic regression.
21
Under review as a conference paper at ICLR 2022
Table 3: Test accuracy for FAFL with different αi s.
Dataset	FAFL		Average	T-shirt	Pullover	Shirt
α1,α2, α3 二	二 0.04, 0.04, 0.04	78.9±0.5	79.1±0.4	80.7±1.0	76.7±0.6
ɑι ,α2, α3 二	二 0.1, 0.1, 0.2	80.1±0.8	90.2±1.0	90.4±0.8	59.7±1.3
FASHION MNIST α1,α2,α3	0.2, 0.2, 0.3	81.0±0.5	88.9±0.2	89.3±0.9	64.8±0.5
α1 , α2 , α3	0.1, 0.2, 0.1	80.1±0.4	85.6±0.7	79.8±1.1	74.9±0.5
α1 , α2 , α3	0.2, 0.2, 0.1	79.0±0.5	77.7±0.3	80.3±1.4	79.0±0.4
Table 4: Test accuracy distribution for FAFL with different αis.
Dataset	FAFL	Average	WORST 10%	BEST 10%	Variance
	α6 = 0.1, α = 1	86.2±0.4	65.3±0.6	94.2±0.08	78±9
	α6 = 0.01, α = 1	87.6±0.3	73.4±2.8	94.3±0.9	39±11
	α3 = 0.1,。6 = 0.01, α = 1	86.5±1.3	71.3±0.8	93.8±1.1	43±5
	α4 = 0.1, α6 = 0.01, α = 1	86.9±1.1	69.2±4.2	93.6±0.7	60±16
Vehicle	α5 = 0.1, α6 = 0.01, α = 1	86.4±0.8	74.7±3. 1	93.2±0.5	33±16
	α6 = 0.01, seed = 123	87.4±0.9	68.7±2.0	94.8±0.6	75±11
	α6 = 0.01, seed = 234	83.4±0.8	67.0±1.0	92.1±1.2	56±1 1
	α = 0.9	68.7±2.4	21.8±4.3	100.0±0.0	589±71
	α = 0.6	69.0±2.4	22.2±4.1	100.0±0.0	590±70
	α = 0.3	70.2±0.8	29.0±0.6	100.0±0.0	486±12
Sent140	seed = 123, num = 5,α = 0.3	71.0±1.9	27.2±3.6	100.0±0.0	513±13
	seed = 234, num = 5, α = 0.3	71.1±2.0	27.0±3.6	100.0±0.0	516±14
	seed = 345, num = 8, α = 0.3	71.0±2.0	27.1±4.0	100.0±0.0	515±16
	seed = 456, num = 8, α = 0.3	70.9±2.0	26.6±3.8	100.0±0.0	515±17
Vehicle. The Vehicle dataset consists of sensor data collected from a distributed network of 23
sensors (Duarte and Hu, 2004). Each data has a 100-dimension feature and a binary label. We model
each sensor as a client. This produces a dataset with 23 clients. We then train a linear SVM to predict
whether a vehicle is AAV-type or DW-type.
Sent140. The dataset is a collection of tweets from 1, 101 accounts from Sentiment140 (Go et al.,
2009) where each account is associated with a client. We train a model consisted of two LSTM layers
followed by one fully-connected layer for binary sentiment classification which takes a 25-word
sequence as input and embeds each of these into a 300-dimensional space using pretrained Glove
(Pennington et al., 2014).
We randomly split the data on each client into 80% training set, 10% validation set and 10% test set.
As discussed in the body, by changing the parameters αis, FAFL allows for a flexible trade-off between
average accuracy and fairness. We empirically investigate the effect of these hyper-parameters. The
results are shown in Table 3 and 4. For Vehicle and Sentiment140, since the number of clients is
large, we start with the case that most (or all) of the clients share the same α. For example, we choose
α6 = 0.1 for client 6 and α = 1 for other clients in Vehicle. Then we select some (or all) of those
clients with the same value and change their respective αi value. In particular, for Vehicle dataset, we
generate random αi for each client except for client 6 using seed 123 and 234 and thus each client
has different αi; for Sentiment140, we first randomly choose num = 5 or 8 clients and then generate
random αi value for each of the 5 (or 8) clients. We observe that the results are not so sensitive to any
particular (random) mutation, and in most cases our FAFL achieves a good balance between accuracy
and fairness. In our experiments in Section 6, we report the results marked by the gray color.
22