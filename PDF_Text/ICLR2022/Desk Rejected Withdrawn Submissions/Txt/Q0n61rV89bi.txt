Under review as a conference paper at ICLR 2022
Learning Phoneme-level Discrete Speech Rep-
resentation with Word-level Supervision
Anonymous authors
Paper under double-blind review
Ab stract
Phonemes are defined by their relationship to words: changing a phoneme changes
the word. Learning a phoneme inventory with little supervision has been a long-
standing challenge with important applications to under-resourced speech tech-
nology. In this paper, we bridge the gap between the linguistic and statistical
definition of phonemes and propose a novel neural discrete representation learn-
ing model for self-supervised learning of phoneme inventory with raw speech
and word labels. Under mild assumptions, we prove that the phoneme inventory
learned by our approach converges to the true one with exponentially low error
rate. Moreover, in experiments on TIMIT and Mboshi benchmarks, our approach
consistently learns better phoneme-level representation than previous state-of-the-
art self-supervised representation learning algorithms and remains effective even
in a low-resource scenario.
1	Introduction
Thanks to recent developments in self-supervised speech representation learning (van den Oord et al.
(2017; 2019); Chorowski et al. (2019); Baevski et al. (2020)), there is new hope for the development
of speech processing systems without the need for full textual transcriptions. Supervised speech
processing systems for tasks such as automatic speech recognition (ASR) rely on a large amount of
textual transcriptions, but self-supervised systems can be applied to under-resourced languages in
which such annotation is either scarce or unavailable. A key task of the self-supervised system is to
learn a discrete representation. While it is possible to discretize the speech solely on the basis of its
acoustic properties, a more desirable discrete representation would serve as a bridge from the con-
tinuous acoustic signal toward higher-level linguistic structures such as syntax and semantics. Such
a representation would make it possible to repurpose algorithms developed for written languages
so that they could be used for unwritten languages in tasks such as speech translation and spoken
language understanding. Words are the obvious choice for a discrete, semantic-driven speech repre-
sentation, but a practical speech understanding system needs at least thousands of words; learning
them in an unsupervised manner may be challenging. Phonemes may be a more learnable represen-
tation. According to the standard linguistic definition, phonemes are closely linked to words:
Definition 1. (Linguistic definition of phonemes (Swadesh (1934))) Phonemes are the smallest units
in speech such that given a correct native word, the replacement of one or more phonemes by other
phonemes (capable of occurring in the same position) results in a native word other than that in-
tended, or a native-like nonsense word.
For example, the sentences “he thinks” and “he sinks” differ by exactly one phoneme but have very
different meaning. The optimal compactness of a phoneme inventory as specified in the definition
leads to three advantages. First, learning phonemes requires lower sample complexity than learning
words since the number of distinct phonemes is much smaller than the number of distinct words in a
language. Second, the phonemes are much more abundant and more balanced in classes than words
within a speech corpus, which makes sample complexity less of an issue when learning phonemes.
Third, phonemes are more generalizable in the sense that knowing the phoneme inventory allows the
learner to memorize previously unseen words as sequences of phonemes, and, having memorized
them, to begin seeking clues to their meaning.
Motivated by the semantic-driven definition of phonemes, we formulate the problem of learning
a phoneme inventory as a self-supervised learning problem, where a small amount of semantic
1
Under review as a conference paper at ICLR 2022
supervision is available. Such supervision can be a small set of word-level category labels that
are available from the word’s use in naturalistic settings. One such label might be the name of a
visual object semantically related to the spoken word, e.g., as used by infants learning the phoneme
inventory of their first language (L1). Another such label might be the translation of the spoken
word into a written form in another language, e.g., as used by second-language (L2) learners.
Our contributions are threefold: (1) we propose a computationally tractable definition of phoneme
that is almost equivalent to the linguistic definition. (2) We propose a finite-sample objective func-
tion for learning phoneme-level units and prove that under mild conditions, the empirical risk mini-
mizer (ERM) of this objective will find the correct phoneme inventory with exponentially low error
rate. (3) We propose a novel class of neural networks called information quantizers to optimize the
proposed objective, which achieve state-of-the-art results in the phoneme inventory discovery task
on the TIMIT and low-resourced Mboshi benchmarks with much less training data than previous
approaches.
2	Related works
Due to the challenge of learning phonemes, early works on unsupervised speech representation
learning (Park & Glass (2005); Lee & Glass (2012); Ondel et al. (2016)) focus on learning speech
segments sharing similar acoustic properties, or phones, without taking into account the meaning of
the speech they are part of. There are two main approaches in this direction. One approach is to learn
discrete phone-like units without any textual labels by modeling phone labels of the speech segments
as latent variables. In particular, (Park & Glass (2005); Jansen et al. (2010)) first detect segments
with recurring patterns in the speech corpus followed by graph clustering using the similarity graph
formed by the segments. (Lee & Glass (2012); Ondel et al. (2016); Kamper et al. (2016)) develop
probabilistic graphical models to jointly segment and cluster speech into phone-like segments. An
extension to the latent variable approach is to introduce additional latent variables such as speaker
identity (Ondel et al. (2019)) or language identity (Yusuf et al. (2020)) and develop mechanisms to
disentangle these variables.
With the advance of deep learning, neural network models have also been proposed to learn unsu-
pervised phone-level representation either by first learning a continuous representation (Chung et al.
(2019); Feng et al. (2019); Nguyen et al. (2020)) followed by off-line clustering, or by learning a dis-
crete representation end-to-end with Gumbel softmax (Eloff et al. (2019b); Baevski et al. (2020)) or
vector-quantized variational autoencoder (VQ-VAE) (van den Oord et al. (2017); Chorowski et al.
(2019); Baevski et al. (2019)). However, codebooks learned by the neural approaches tend to be
much larger than the number of phoneme types (Baevski et al. (2020)), leading to low scores in
standard phoneme discovery metrics. The second approach utilizes weak supervision such as noisy
phone labels predicted by a supervised, multilingual ASR system trained on other languages. Along
this direction, (Zelasko et al. (2020); Feng et al. (2021a)) systematically study the performance of
zero-shot crosslingual ASR on 13 languages trained with international phonetic alphabet (IPA) to-
kens and found that the system tends to perform poorly on unseen languages. Instead, (Feng et al.
(2021b)) is able to discover phone-like units by clustering bottleneck features (BNF) from a fac-
torized time-delay neural network (TDNN-f) trained with phone labels predicted by a crosslingual
ASR (Feng et al. (2021a)).
Several works have since shifted focus toward the more challenging phoneme discovery problem by
formulating it as a self-supervised learning problem where the semantics of the speech are known,
such as from translation, phoneme-level language models or other sensory modalities such as vi-
sion. (Harwath & Glass (2019)) analyzes the hidden layers of a two-branch neural network trained
to retrieve spoken captions with semantically related images and finds strong correlation between
segment representation and phoneme boundaries. (Harwath et al. (2020)) adds hierarchical vector
quantization (VQ) layers in the same retrieval network and is able to find a much smaller codebook
than the unsupervised neural approach (Baevski et al. (2020)), and achieve high correlation with the
phoneme inventory. (Godard et al. (2018); Boito et al. (2019)) has studied the possibility of learning
semantic units using an attention-based speech-to-text translation system, though the units appear to
correlate more with words. Works on unsupervised speech recognition (Chen et al. (2019)) attempt
to learn to recognize phonemes by leveraging the semantic information from a phoneme language
model unpaired with the speech, typically by matching the empirical prior and posterior distributions
2
Under review as a conference paper at ICLR 2022
of phonemes either using cross entropy (Yeh et al. (2019)) or adversarial loss (Chen et al. (2019);
Baevski et al. (2021)).
3	Semantic-driven Phoneme Discovery
3.1	Notation
Throughout the paper, We use P{∙} to denote probability. We use capital letters to denote random
variables and lower-case letters to represent samples of random variables. We use PX := P{X = x}
to denote both probability mass and density functions of random variable X , depending on Whether
it is continuous or discrete. Further, denote PY |X (y|x) := P{Y = y|X = x} as the true conditional
probability distribution of random variable Y = y given random variable X = x. The probability
simplex in Rd is denoted as ∆d .
3.2	Statistical Definition of Phonemes
The linguistic definition of phonemes can be rephrased as folloWs: Given a spoken Word segment
X = [xι, ∙∙∙ , XT] of word type y, where xt's are phoneme segments within that word, if we switch
any segment xt to x0t ofa different phoneme type, the segment x0 = [x1:t-1, x0t, xt+1:T] Will repre-
sent a different word type y0 6= y. On the other hand, if xt and x0t are of the same phoneme type, x0
will have the same word type y as x. In order to design effective algorithms, we will work with a
relaxation of this definition, which we call the statistical definition of phonemes.
Definition 2. (Statistical definition of phonemes) Let X be the set of all speech segments in a lan-
guage, and let X be a random vector taking values in X and Y be a random variable representing
the word type that X is part of. The phoneme inventory of a language is the minimal partition
Z = {Zι,…，ZK} of X (i.e., X = ∪3ιZk, Zj ∩ Zk = 0, ∀1 ≤ j,k ≤ K), such that ifa speech
segment pair (x, x0) ∈ X2 satisfies (x, x0) ∈ Zk for some k ∈ {1, •…,K}, then their conditional
distributions satisfy
PY|X=x = PY |X=x0.
(1)
In other words, the conditional distribution of the semantic variable given any instance of the
same phoneme is the same. This property will be referred later as the distributional property
of phonemes.
This definition is the generalization of Definition 1, as shown in the following proposition.
Proposition 1. Let Z = ∪kK=1Zk be a partition ofX. If, for all possible {PY |X=xs }s6=t, for any
SPoken word segment X = [xι, ∙∙∙ , XT ], and for any phonetic segment pairs (xt, Xt) ∈ Zk, k ∈
{1,…，K}, changing Xt to Xt does not alter the identity ofthe word, i.e.,
arg max PY |X1:T (y|X1:t-1, X0t, Xt+1:T) = arg max PY |X1:T (y|X),
yy
(2)
but for any segment pairs Xt ∈ Zk, X0t0 ∈ Zl for k 6= l, changing Xt to X0t alters the identity of the
word, i.e.,
argmaxPY|X1:T(y|X1:t-1,X0t0,Xt+1:T) 6= arg max PY |X1:T (y|X),
(3)
then Z is a phoneme inventory from Definition 2.
The proof can be found in Appendix A. Definition 2 preserve the semantic property of phonemes
characterized in Definition 1, but does not require expensive pairwise labels of semantic change.
Further, such a definition is flexible enough to allow further generalization by relaxing Y to be
any semantic signal related to the spoken utterance, which may include images or translations to a
different language.
Define the phoneme assignment function Z : X → {1, ∙∙∙ , K} such that Z(X) = k if X ∈ Zk.
Suppose a phoneme segment X is randomly chosen from X with probability distribution PX and
random variable Z := Z(X) is its phoneme label, then by the distributional property of phonemes,
for any pair X, X0 ∈ X such that Z(X) = Z(X0), we have PY |X=x = PY |X=x0 = PY |Z=z(x). The
3
Under review as a conference paper at ICLR 2022
Pre-segmentation
Joint distribution learning
Figure 1: Network architecture of information quantizer
phoneme inventory is thereby completely characterized by the phoneme label function z(∙) as well
as the set of distributions associated with each class PY |Z.
Further, instead of explicitly minimizing K as required by the definition, we can fix K either based
on prior knowledge of the language or by simply gradually decreasing K until the distributional
property of phonemes is no longer feasible.
3.3	Problem Formulation
Let z(∙) be the phoneme assignment function from Definition 2 and assuming the size of the
phoneme inventory is known to be K .
Given a training set D = {(x(i), y(i))}in=1, where each x(i) is a spoken phonetic segment extracted
from a spoken word segment, and each y(i) ∈ Y is the corresponding word type label, a semantic-
driven phoneme discovery (SPD) system tries to find an assignment function that minimizes the
token error rate (TER):
PTER(Z)= min P{z(X) = π(Z(X))},	(4)
π∈Π
where Π is the set of all permutations of length K, which is used because the problem is unsuper-
vised and z(∙) is not available during training. An assignment function Z is said to achieve exact
discovery if PrER(Z) = 0. It can be easily shown that TER is equivalent to standard evaluation
metrics for phoneme discovery such as normalized mutual information (NMI) (Yusuf et al. (2020);
Harwath et al. (2020); Feng et al. (2021b)) and token F1 (Dunbar et al. (2017)). Please refer to
Appendix A.2 for details. Thus, to provide guarantees for NMI and token F1, it suffices to provide
a guarantee for TER.
4	Information Quantizer
We solve the SPD problem using a novel type of neural network called information quantizer
(IQ), as depicted in Figure 1. An IQ (θ, q) ∈ Θ × QK consists of four main components:
A pre-segmentation network, a speech encoder e&i (∙), a word classifier ce? (∙) and a quantizer
q : ∆lYI → C = {Qι,…，QK}, where 仇，θ2] = θ and C is the distribution codebook and
Qk’s are called the code distributions of q.
4.1	Phoneme inventory discovery with IQ
IQ performs phoneme discovery in three stages. The pre-segmentation stage takes a raw speech
waveform as input and extracts phoneme-level segments X = [xi,…，χτ] in an self-supervised
fashion (Kreuk et al. (2020)); Afterwards, in the joint distribution learning stage, the speech encoder
extracts phoneme-level representations eθι (x) = [eθι (xi),…，eθι (XT)] before passing them into
the word classifier network to estimate the phoneme-level conditional word distribution as:
PYix=Xt= Cθ2(eθ1 (Xt)),1 ≤ t ≤ T.	(5)
4
Under review as a conference paper at ICLR 2022
Note that it is crucial that no recurrent connection exists between segments since our goal is to
learn the probability of words given each individual phoneme segments. Finally, in the quantization
stage, the quantizer creates the phoneme inventory by assigning each segment xt an integer index
via codeword assignment function Z(xt) such that Z(χt) = k if q(P?X=XJ = Qk.
4.2	Training
The loss function that IQ minimizes has two goals: learn a good estimator for the conditional distri-
bution Pγ∣χ and learn a good quantization function q(∙). The first goal is achieved by minimizing
the cross entropy loss:
1n
LCE(Pn ⑼:=--Elog PY|X (y(i)|x(i)),	⑹
i=1
where Pn is the empirical joint distribution. The second goal is achieved by minimizing the KL-
divergence between the estimated conditional distribution before and after quantization:
-n
LQ(Pn,θ,q) ：= nɪs DKL(PY∣X=χ(i) llq(pY∣X=χ(i)))，	⑺
where Pn := n Pn=1 PY∣χ is the smoothed version of the empirical distribution.
The final loss function ofIQ for SPD is then:
LIQ(Pn,θ, q) := LCE(Pn,θ) + XLQ (Pn,θ, q) ,	(Pl)
where λ > 0 is some hyperparameter set to approximately - for most experiments.
Further, we restrict q to be nearest-neighbor so that:
q(P) = arg min DKL(P||Qk).	(8)
Qk：1≤k≤K
This restriction does not increase the loss (P1) and serves as a regularization during phoneme dis-
covery, as shown in Appendix A.3.
4.3	Theoretical Guarantee
We show that under mild assumption, IQ is able to achieve exact discovery of phoneme inventory.
First, let us state the main assumptions of the paper.
Assumption 1. (boundedness of the density ratio) There exist universal constants Cl < Cu such
that ∀θ ∈ Θ, ∀q ∈ Qk, ∀(x, y) ∈ X × Y, log PYIX器)∈ [Cι,Cu], log 蒜X(M)) ∈ [Cι,Cu]∙
Assumption 2. (log-smoothness of the density ratio) There exists ρ > 0 such that ∀θ1, θ2 ∈
Θ,x,y ∈ X × Y, log PY2X (y,) ≤ Pkθ1-θ2k.
PY2|X (y|x)
Assumption 3. (realizability) There exists a nonempty subset Θ* ⊂ Θ such that PY∣χ =
Pγ∣X, ∀θ ∈ Θ*.
Assumption 4. The true prior of the phoneme inventory is known to be PZ (z)=六,1 ≤ Z ≤ K.
The first two assumptions are similar to the ones in (Tsai et al. (2020)). Assumption 3 assumes
that the true probability measure is within the function class, which combined with Assumption 1
requires the true distribution to share the same support as the estimated one. However, such assump-
tion can be relaxed so that DKL(PYlX∣∣Py∣x) ≤ ν, ∀θ* ∈ Θ* for some small enough V > 0, which
does not affect the essential idea behind our analysis and can be achieved by some rich class of uni-
versal approximators such as neural networks (Hornik et al. (1989)). The last assumption ensures
the inventory to be identifiable by assuming knowledge of the prior of the phoneme inventory. The
uniform prior is chosen as it approximates phoneme prior sufficiently well, though other prior can
also be used.
Next, we will state the theoretical guarantee before giving some intuitive explanation.
5
Under review as a conference paper at ICLR 2022
EI_ . _ _ _ Λ ≠-t '	A	. -	7 √ I . . 7 ■	. ■	, ■	/久 八 ■ . 1	■	. r	. ■	ʌ
Theorem 1. Given Assumption 1-4, let the information quantizer (θ, q) with assignment function Z
be an empirical risk minimizer (ERM) of (P1 ):
一 ，.G .、	一 , _
LIQ(Pn ,θ,q) = θ∈mqnQκ LIQ(Pn ,θ,q).
(9)
For any δ ∈ (0,1], with probability at least 1 一 δ, the cluster assignment function Z of the ERM
information quantizer ^ achieves PTER (Z) = 0 if the Sample size n satisfies:
n≥O
log1
min{e*2, log KKI}
(10)
where e* = minzi^m=%? c(z1,z2)DJS(PY∣z=zJPγ∣z=z2)2 for some constants c(z1,z2)
>
0,1 ≤ zι, Z2 ≤ K independent of n, δ, O(x) is such that limχ→∞ Ox) = 0 and DJS(P||Q)
2DKL (P||P-2Q) + 2DKL(Q||P-QQ) is the Jensen-Shannon divergence.
The bound in Theorem 1 captures two main factors determining the sample complexity of exact
phoneme discovery: the first factor is how close the word distributions of phonemes are from each
other as measured by their Jensen-Shannon (JS) divergence, and the second factor is how hard it is
for the training data to cover all the phoneme types. The theorem works essentially because (P1)
can be viewed as an approximation of the mutual information between the codeword Z(X) and word
type Y, I(Z(X); Y). Suppose PYIX ≈ Pγ∣χ and let H(∙∣∙) denotes conditional entropy, We have:
LIQ(Pn, θ, q) ≈ H (Y X) + DKL(PYX l∣q(Pγ∣x))
H -I(X； Y) + DKL(PYIX∣∣q(PγIX)) = -I(Z(X)； Y),
which is minimized if ^(Py∣χ) = PY|z(x). In fact, We can show that Z for such q is equivalent to
z(∙) up to a permutation. We will defer the proofs to Appendix A.3.
5 Experiments
5.1	Implementation Details
For the pre-segmentation stage in Figure 1, we use the self-supervised model proposed in (Kreuk
et al. (2020)) to predict the phoneme-level segmentation for English datasets, and the segmentation
generated by one of our baseline (Feng et al. (2021a)) for experiments on Mboshi language. The
segmental speech encoder e&i (∙) is a CPC model pretrained on Librispeech (Nguyen et al. (2020))
with 256-dimensional representation for each 10ms frame followed by averaging across each seg-
ments. The word classifier cθ? (∙) for the joint distribution learning stage consists of four hidden
layers and 512 ReLU units per layer with layer normalization and one softmax output layer. All our
models are trained for 20 epochs using Adam optimizer (Kingma & Ba (2014)) with learning rate of
0.001 decayed by 0.97 every 2 epochs and a batch size of 8. We slightly modify (P1) analogous to
the VQ-VAE (van den Oord et al. (2017)) to make it more suitable for gradient-based optimization:
LIQ-VAE(Pn, θ, q) := LCE (Pn, θ) + λE Pn [DKL(Sg[PY|X ]∣∣q(PY |X )) + DKL(PY |X ||sg[q(PYIX )])]
where sg[∙] denotes the stop-gradient operation and λ = 0.5 for all experiments. Exponential moving
average (EMA) codebook update is used with a decay rate of 0.999 to optimize the first KL term.
Each code distribution is initialized using a symmetric Dirichlet distribution with a concentration
parameter of 100.
5.2	Word-level Phoneme Inventory Discovery
Datasets We construct four training datasets consisting of spoken words only. The details can be
found in Appendix B. The visual words are extracted by selecting the head words of noun phrases
from the Flickr30kEntities dataset (Hodosh et al. (2010)) that appear at least 500 times. For the
Flickr audio word dataset, the spoken word data are extracted from Flickr audio dataset (Harwath
& Glass (2015)). The Librispeech and TIMIT word dataset with |Y| = 224 uses the same set of
6
Under review as a conference paper at ICLR 2022
Flickr Audio	Librispeech
Token F1 NMI Token F1 NMI
Continuous Representation
(Nguyen et al. (2020)) CPC+MLP+k-means, K=44 CPC+MLP+k-means, K=100 CPC+MLP+k-means, K=256	35.7±0.6 49.4±0.8 40.6±0.5 28.5±0.4	40.9±0.4 52.2±0.7 51.7±0.7 51.0±0.4	48.6±1.1 67.5±0.9 61.3±0.5 48.4±1.7	60.0±0.4 71.8±1.1 71.8±0.6 68.8±0.7
Discrete Representation				
(Alemi et al. (2017))	43.6±0.7	36.1±1.9	51.0±2.1	56.2±0.9
(Strouse & SchWab (2016)), K=44	49.4±1.0	52.2±0.2	68.3±1.3	72.8±1.0
(Strouse & SchWab (2016)), K=100	41.7±0.7	52.8±0.1	60.3±0.0	71.0±0.5
(Strouse & SchWab (2016)), K=256	31.6±0.1	51.8±0.2	49.1±0.7	68.8±0.2
IQ (Ours), K=44	53.2±1.3	55.4±1.1	65.9±2.0	73.0±1.2
IQ (Ours), K=100	51.3±0.4	56.5±0.5	68.4±1.5	75.0±1.0
IQ (Ours), K=256	48.2±0.7	53.0±1.9	69.7±2.0	75.8±1.0
(2a)
(2b)
Figure 2: (a) In-domain phoneme discovery results on Flickr audio and Librispeech. (b) Conver-
gence plot of various models on Flickr audio.
word types from Flickr30k and spoken word tokens from Librispeech (Vassil et al. (2015)) 460-
hour train-clean subset, resulting in a dataset of about 6 hours and 0.1 hours; for Librispeech and
TIMIT word dataset with |Y| = 524 and |Y| = 824, we supplement the dataset with the speech for
the top 300 frequent words and top 600 frequent words respectively (excluding the visual words)
in Librispeech, resulting in datasets of about 15 and 21 hours. For Mboshi dataset, we found only
about 20 actual words occur more than 100 times, so instead we use all n-grams except uni- and
bigrams or bigrams+trigrams that occur more than 100 times as “words”, resulting in a vocabulary
size of 161 and 377 respectively. Note that the amount of data we need is much lower than previous
works (Yusuf et al. (2020): around 30 hours, Feng et al. (2021b): around 600 hours), and the
vocabulary size used is much smaller than the total vocabulary size in the language. For each training
dataset, we test our models on spoken words whose types have appeared during training. The best
results during testing are reported for all models.
Evaluation metrics Standard metrics such as NMI for the quality of the codebook and boundary
F1 for the quality of segmentation respectively are used to evaluate the models. We used the same
evaluation code used in (Yusuf et al. (2020), Feng et al. (2021b)) with a tolerance of 20ms for
boundary F1. In addition, we also report token F1 (Dunbar et al. (2017)), which is more sensitive to
the purity of the clusters.
Baselines We compare our model (IQ) with four baselines. The first two baselines use contin-
uous representation: the CPC+k-means model performs k-means clustering on the segment-level
CPC features and the k-means model performs k-means clustering after the model is trained on the
word recognition task. The last two baselines use discrete representations: the Gumbel variational
information bottleneck (Gumbel VIB) (Alemi et al. (2017) is a neural model with a Gumbel soft-
max (Jang et al. (2016)) layer to approximate the codebook assignment function Z(∙), and We set
β = 0.001 and decay the temperature of the Gumbel softmax from 1 to 0.1 linearly for the first
300000 steps, keeping it at 0.1 afterWards, Which Works best in our experiments; the deterministic
information bottleneck (DIB), a generalization of (Strouse & SchWab (2016)) for continuous fea-
ture variable X , Which assumes the same deterministic relation betWeen speech X and codebook
unit Z as ours, but optimizes the models in a pipeline fashion (first the speech encoder and then the
quantizer) by performing clustering on the learned conditional distributions. All models share the
same speech encoder as IQ.
Results The results on visual Word-only test sets of Flickr audio and Librispeech are shoWn in
Table 2a. On both datasets, IQ outperforms both Gumbel VIB and DIB in terms of all metrics,
especially on Flickr audio, Which has more phoneme types than Librispeech and a larger test set.
Further, models assuming a deterministic relation betWeen phonemes and speech tend to perform
better than models Without such an inductive bias such as Gumbel VIB, suggesting the relation
7
Under review as a conference paper at ICLR 2022
between phonemes and speech is approximately deterministic. Moreover, the performance of IQ
is very robust to the codebook size, achieving good results even when the codebook size is very
different from the size of the true phoneme inventory, suggesting our theory may be able to work
with a relaxed Assumption 4.
(3a)
Figure 3: (a) Distribution of codeword assignment for each phoneme by IQ with |Y| = 824 and pre-
dieted segmentation on TIMIT. Each row of the plot is the empirical distribution for P^∖z(∙∣z), 1 ≤
Z ≤ K, where the phonemes are sorted top-to-bottom with decreasing max,，P*^∣z(z0∣z). (b) The
overall phoneme discovery results of all models on TIMIT.
TIMIT	Token F1	NMI	Boundary F1
(Yusuf et al. (2020))	-	40.1±0.1	76.6 ±0.5
(Harwath et al. (2020))	-	35.9	54.2
(Feng et al. (2021b))	-	36.8	70.5
+ gold segmentation	-	51.2	97.8
(Ours) IQ, |Y|=224, K=39	37.9±1.2	38.6±0.7	77.1±0.1
+ training on TIMIT	50.9±0.8	43.4±0.9	78.6±0.4
+ gold segmentation	62.8±0.8	59.4±0.8	96.9±0.3
(Ours) IQ, |Y|=524, K=39	42.4±0.1	43.0±0.5	79.4±0.1
+ training on TIMIT	53.9±0.3	46.7±0.2	80.4±0.2
+ gold segmentation	64.3±0.4	63.4±0.4	98.3±0.3
(Ours) IQ, |Y|=824, K=39	43.9±0.1	44.3±0.2	79.2±0.0
+ training on TIMIT	54.4±0.4	47.5±0.2	80.5±0.1
+ gold segmentation	65.7±0.7	65.2±0.6	98.6±0.3
(3b)
(4a)
Figure 4: (a) The distribution of codeword assignment for each phoneme by IQ with CPC+BNF
features, vocab size |Y| = 377 and predicted segmentation on Mboshi. Each row of the plot is the
empirical distribution for P*^∣z(∙∣z), 1 ≤ Z ≤ K, where the phonemes are sorted top-to-bottom with
decreasing max,，P^∖z(z0∣z). (b) Phoneme discovery results of all models on Mboshi dataset.
	Token F1	NMI	Boundary F1
(Ondel et al. (2019))	-	38.4±1.0	59.5±0.8
(Yusuf et al. (2020))	-	41.1±1.1	59.2±1.5
(Feng et al. (2021b), 5 langs)	-	43.5±0.3	62.8±0.0
+ Gold segmentation	-	60.6±0.1	100±0.0
(Feng et al. (2021b), 13 langs)	36.4±0.6	44.7±0.6	64.1±0.1
+ Gold segmentation	50.8±0.6	64.6±0.3	100±0.0
(Ours) IQ, |Y| = 161, K=31	46.5±0.4	40.2±0.1	65.5±0.1
+ Multilingual BNF	54.2±1.0	45.1±0.4	67.5±0.1
+ Gold segmentation	66.4±0.8	69.7±0.4	100±0.0
+ Multilingual BNF	74.3±0.8	76.9±0.6	100±0.0
(Ours) IQ, |Y| = 377, K=31	50.4±0.5	45.2±0.8	66.8±0.0
+ Multilingual BNF	57.1±1.0	49.3±0.3	67.3±0.1
+ Gold segmentation	69.3±1.0	73.0±0.6	100±0.0
+ Multilingual BNF	81.7±0.8	82.6±0.3	100±0.0
(4b)
5.3 Whole-sentence Phoneme Inventory Discovery
Datasets We also test our models on two standard benchmarks, which contain whole-sentence
utterances with many words unseen during training. The first dataset is TIMIT (Garofolo et al.
(1993)), an English corpus consisting of about 5 hours speech and we follow the split in (Yusuf
et al. (2020)) to use the entire corpus excluding SA utterance for both training and testing, which is
acceptable since our setting is unsupervised. On this dataset, we experiment with models pretrained
on spoken word subsets of Librispeech as well as the combined subsets of Librispeech and TIMIT.
Another benchmark dataset, Mboshi (Godard et al. (2017)), contains about 2.4 hours speech from
8
Under review as a conference paper at ICLR 2022
a low-resource language and we again follow (Yusuf et al. (2020), Feng et al. (2021b)) to use the
entire corpus for training and testing. The best results during testing are reported for all models.
Baselines For the whole-sentence datasets, we compare our models with the state-of-the-art unsu-
pervised and weakly supervised phoneme discovery systems, namely, the unsupervised H-SHMM
trained with untranscribed multilingual speech (Yusuf et al. (2020)), the ResDAVEnet-VQ (Harwath
et al. (2020)) with visual supervision and the TDNN-f system by (Feng et al. (2021b)) trained with
transcribed multilingual speech.
Result on TIMIT The results on TIMIT are shown in Table 3b. First of all, our model is able to
outperform the visually grounded baseline (Harwath et al. (2020)) for all training vocabulary, and
all three baselines for |Y| = 524 and |Y| = 824 with and without gold segmentation in terms of
all three metrics. Further, we also empirically verify the sample complexity bound in Theorem 1 as
IQ performs better in Token F1 and NMI as the training vocabulary size get larger, which generally
increases the JS divergence. Interestingly, we observe a diminishing return in all metrics as the vo-
cabulary size increases, possibly because words ranked between 524-824th have very low frequency
and may not increase the JS divergence between PY |Z’s of phonemes much. As expected the NMI
of our model degrades by about 8% on TIMIT compared to that of word-level SPD in Librispeech,
due to the challenge of generalization and improves by about 1% improvement as we include words
from TIMIT. In addition, the use of unsupervised phoneme segmentation deteriorates the NMI by
about 18% absolute for our models since the distributional property of phonemes does not apply
exactly to non-phoneme segments. From Figure 3a, we observe that the codeword assignments by
IQ correlates well with the actual phonemes. Further, most phonemes confused by the model ap-
pears to fall into the same manner class, such as nasals “n” and “m” as well as affricates “ch” and
“jh”. In addition, vowel appears to be the hardest manner class for the model to learn. From Fig-
ure 33a, vowels such as “eh” and “ah” are assigned to many different codewords, exhibiting high
within-class variability; vowel pairs such as “aw” and “aa” are often assigned the same codeword,
exhibiting high between-class variability for vowels. These observations are confirmed by further
analysis using t-SNE (van der Maaten & Hinton (2008)) on the estimated conditional distributions
PYθ |X and the top-10 most confusing phoneme pairs, both of which can be found in Appendix C.
Result on Mboshi The results on Mboshi are shown in Table 4b. As we can see, when |Y| = 377,
IQs with both CPC and CPC+BNF features outperform all baselines in all three metrics with or
without gold segmentation; when |Y| = 161 and gold segmentation is available, IQs with CPC and
CPC+BNF features outperform (Feng et al. (2021b)) in all three metrics. While being equivalent for
perfect assignment, we find token F1 and NMI behave quite differently empirically, as for |Y| = 161,
IQ with CPC features and predicted segmentation outperforms the best baseline (Feng et al. (2021b),
13 languages) in terms of token and boundary F1, but worse in NMI. Adding multilingual BNF to
the same model significantly improves the result and allows the model to outperform the baselines in
terms of NMI as well. In all cases, adding multilingual BNF, that is, concatenating the multilingual
BNF from (Feng et al. (2021b)) to the CPC output representation from the segmental speech encoder
in Figure 1 further improves the result, suggesting that multilingual information and word-level
semantic information are complimentary. By comparing IQ with |Y| = 161 and |Y| = 377, we find
that increasing the vocabulary size again improves the performance of our system. From Figure 4a,
we observe similar trends as in TIMIT, and the confusion between phonemes within each manner
class such as nasals “n”, “m” and “mb” is more severe, both because the more diverse set of nasal
phonemes and because the relatively small vocabulary size. More visualization and analysis of the
phoneme representation can be found in Appendix C.
Effect of codebook size we find that the quality of codeword assignments by IQs is robust against
varying codebook size, after experimenting with codebook size from 30 to 70 on TIMIT and Mboshi.
The detailed results can be found in Appendix D.
6 Conclusion
Motivated by the linguistic definition of phonemes, we propose information quantizer (IQ), a
new neural network model with theoretical guarantee and strong empirical performance for self-
supervised learning of phoneme inventory with word-level supervision.
9
Under review as a conference paper at ICLR 2022
References
Alexander A. Alemi, Ian Fischer, Joshua V. Dillon, and Kevin Murphy. Deep variational information
bottleneck. In International Conference on Learning Representations (ICLR), 2017.
Alexei Baevski, Steffen Schneider, and Michael Auli. vq-wav2vec: Self-supervised learning of
discrete speech representations. In International Conference on Learning Representations, 2019.
Alexei Baevski, Henry Zhou, Abdelrahman Mohamed, and Michael Auli. wav2vec 2.0: A frame-
work for self-supervised learning of speech representations. In Neural Information Processing
System, 2020.
Alexei Baevski, Wei-Ning Hsu, Alexis Conneau, and Michael Auli. Unsupervised speech recogni-
tion. In ArKiv, 2021. URL https://arxiv.org/pdf/2105.11084.pdf.
Marcely Zanon Boito, Aline Villavicencio, and Laurent Besacier. Empirical evaluation of sequence-
to-sequence models for word discovery in low-resource settings. In Proc. Annual Conference of
International Speech Communication Association (INTERSPEECH), 2019.
K.-Y. Chen, C.-P. Tsai, D.-R. Liu, H.-Y. Lee, and L. shan Lee. Completely unsupervised speech
recognition by a generative adversarial network harmonized with iteratively refined hidden
markov models. In Proc. Annual Conference of International Speech Communication Associ-
ation (INTERSPEECH), 2019.
Jan ChoroWski, Ron J. Weiss, Samy Bengio, and Aaron van den Oord. UnsUPervised speech rep-
resentation learning using wavenet autoencoders. IEEE Transactions on Audio, Speech and Lan-
guage Processing, 2019.
YU-An ChUng, Wei-Ning HsU, Hao Tang, and James R. Glass. An UnsUpervised aUtoregressive
model for speech representation learning. In In Proc. Annual Conference of International Speech
Communication Association (INTERSPEECH), 2019.
Thomas M. Cover and Joy A. Thomas. Elements of Information Theory (Wiley Series in Telecom-
munications and Signal Processing). Wiley-Interscience, USA, 2006. ISBN 0471241954.
EWan DUnbar, XUan-Nga Cao, JUan BenjUmea, JUlien Karadayi, MathieU Bernard, LaUrent Besacier,
Xavier AngUera, and EmmanUel DUpoUx. The zero resoUrce speech challenge 2017. CoRR,
abs/1712.04313, 2017. URL http://arxiv.org/abs/1712.04313.
Ryan Eloff, Andre Nortje, Benjamin van Niekerk, Avashna Govender, Leanne Nortje, Arnu Preto-
riUs, Elan van Biljon, EWald van der WesthUizen, Lisa van Staden, and Herman Kamper. Un-
supervised acoustic unit discovery for speech synthesis using discrete latent-variable neural net-
Works. In Proc. Annual Conference of International Speech Communication Association (INTER-
SPEECH), 2019b.
Siyuan Feng, Tan Lee, and Zhiyuan Peng. Combining adversarial training and disentangled speech
representation for robust zero-resource subWord modeling. In Proc. Annual Conference of Inter-
national Speech Communication Association (INTERSPEECH), 2019.
Siyuan Feng, Piotr Zelasko, Laureano Moro-Velazquez, Ali Abavisani, Mark Hasegawa-Johnson,
Odette Scharenborg, and Najim Dehak. HoW phonotactic affect multilingual and zero-shot asr
performance. In IEEE International Conference on Acoustics, Speech and Signal Processing
(ICASSP), 2021a.
Siyuan Feng, Piotr Zelasko, Laureano Moro-Velazquez, and Odette Scharenborg. Unsupervised
acoustic unit discovery by leveraging a language-independent subword discriminative feature rep-
resentation. In INTERSPEECH, 2021b.
John S. Garofolo, Lori F. Lamel, William M. Fisher, Jonathon G. Fiscus, David S. Pallett, and
Nancy L. Dahlgren. The DARPA TIMIT Acoustic-Phonetic Continuous Speech Corpus CDROM.
Linguistic Data Consortium, 1993.
10
Under review as a conference paper at ICLR 2022
Pierre Godard, Gilles Adda, Martine Adda-Decker, Juan Benjumea, Laurent Besacier, Jamison
Cooper-Leavitt, Guy-No”el Kouarata, Lori Lamel, H’el‘ene Maynard, Markus M”uller, An-
nie Rialland, Sebastian St"uker, Francois Yvon, and Marcely Zanon Boito. A very low re-
source language speech corpus for computational language documentation experiments. CoRR,
abs/1710.03501, 2017. URL http://arxiv.org/abs/1710.03501.
Pierre Godard, Marcely Zanon Boito, Lucas Ondel, Alexandre Berard, Aline Villavicencio, and
Laurent Besacier. Unsupervised word segmentation from speech with attention. In Interspeech,
2018.
David Harwath and James Glass. Deep multimodal semantic embeddings for speech and images.
Automatic Speech Recognition and Understanding, 2015.
David Harwath and James Glass. Towards visually grounded subword speech unit discovery. In
ICASSP 2019-2019 IEEE International Conference on Acoustics, Speech and Signal Processing
(ICASSP), 2019.
David Harwath, Wei-Ning Hsu, and James Glass. Learning hierarchical discrete linguistic units
from visually-grounded speech. In International Conference on Learning Representation, 2020.
M. Hodosh, P. Young, and J. Hockenmaier. Framing image description as a ranking task: data,
models and evaluation metrics. In Journal of Artificial Intelligence Research, 2010.
Kurt Hornik, Maxwell Stinchcombe, and Halbert White. Multilayer feedforward networks are
universal approximators. Neural Networks, 2(5):359-366, 1989. URL https://www.
sciencedirect.com/science/article/pii/0893608089900208.
Eric Jang, Shixiang Gu, and Ben Poole. Categorical reparameterization with gumbel-softmax. In
International Conference on Learning Representations, 2016.
Aren Jansen, Kenneth Church, and Hynek Hermansky. Toward spoken term discovery at scale with
zero resources. In Proc. Annual Conference of International Speech Communication Association
(INTERSPEECH), 2010.
Herman Kamper, Aren Jansen, and Sharon Goldwater. Unsupervised word segmentation and lexicon
discovery using acoustic word embeddings. IEEE Transaction on Audio, Speech and Language
Processing, 24:669-679, 2016.
Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In Proc. Interna-
tional Conference on Learning Representations (ICLR), 2014.
Felix Kreuk, Joseph Keshet, and Yossi Adi. Self-supervised contrastive learning for unsupervised
phoneme segmentation. In INTERSPEECH, 2020.
Chiaying Lee and James Glass. A nonparametric Bayesian approach to acoustic model discovery.
In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics:
Long Papers - Volume 1, pp. 40-49, Stroudsburg, PA, USA, 2012. Association for Computational
Linguistics.
TU Anh Nguyen, Maureen de Seyssel, Patricia Roze, Morgane Riviere, Evgeny Kharitonov, Alexei
Baevski, Ewan Dunbar, and Emmanuel Dupoux. The Zero Resource Speech Benchmark 2021:
Metrics and baselines for unsupervised spoken language modeling. In Self-Supervised Learning
for Speech and Audio Processing Workshop @ NeurIPS, 2020.
L. Ondel, L. Burget, and J. Cernocky. Variational inference for acoustic unit discovery. In Spoken
Language Technology for Underresourced Languages, 2016.
L. Ondel, H. K. Vydana, L. Burget, and J. Cernocky. Bayesian subspace hidden Markov model for
acoustic unit discovery. In INTERSPEECH, 2019.
Alex Park and James Glass. Towards unsupervised pattern discovery in speech. In Proc. IEEE
Workshop on Automatic Speech Recognition and Understanding (ASRU), 2005.
11
Under review as a conference paper at ICLR 2022
David Qiu, Anuran Makur, and Lizhong Zheng. Probabilistic clustering using maximal matrix
norm couplings. In 2018 56th Annual Allerton Conference on Communication, Control, and
Computing, 2019.
Shai Shalev-Shwartz and Shai Ben-David. Understanding Machine Learning. Cambridge University
Press, 2014.
DJ Strouse and David Schwab. The deterministic information bottleneck. In Association for Uncer-
tainty in Artificial Intelligence, 2016.
Morris Swadesh. The phonemic principle. Language, 10(2):117-129,jun 1934.
Yao-Hung Hubert Tsai, Han Zhao, Makoto Yamada, Louis-Philippe Morency, and Ruslan Salakhut-
dinov. Neural methods for point-wise dependency estimation. In Neural Information Processing
System, 2020.
Aaron van den Oord, Oriol Vinyals, and koray kavukcuoglu. Neural discrete representation learn-
ing. In I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and
R. Garnett (eds.), Advances in Neural Information Processing Systems, volume 30. Curran As-
sociates, Inc., 2017. URL https://proceedings.neurips.cc/paper/2017/file/
7a98af17e63a0ac09ce2e96d03992fbc- Paper.pdf.
Aaron van den Oord, Yazhe Li, and Oriol Vinyals. Representation learning with contrastive predic-
tive coding. In ArXiv, 2019. URL https://arxiv.org/pdf/1807.03748.pdf.
Laurens van der Maaten and Geoffrey Hinton. Visualizing data using t-sne. Journal of Ma-
chine Learning Research, 9(86):2579-2605, 2008. URL http://jmlr.org/papers/v9/
vandermaaten08a.html.
Panayotov Vassil, Guoguo Chen, Daniel Povey, and Sanjeev Khudanpur. Librispeech: an ASR
corpus based on public domain audio books. In ICASSP, pp. pp. 5206-5210, 2015.
Roman Vershynin. High-Dimensional ProbabiIity-An Introduction with Applications in Data Sci-
ence. Cambridge University Press, Sept. 2018. doi: https://doi.org/10.1017/9781108231596.
Chih-Kuan Yeh, Jianshu Chen, Chengzhu Yu, and Dong Yu. Unsupervised speech recognition
via segmental empirical output distribution matching. In International Conference on Learning
Representations, 2019.
B. Yusuf, L.OndeL L. Burget, J. Cernocky, and M. Saraclar. A hierarchical subspace model for
language-attuned acoustic unit discovery. In CoRR, 2020.
Piotr Zelasko, Laureano Moro-Velazquez, Mark Hasegawa-Johnson, Odette Scharenborg, and Na-
jim Dehak. That sounds familiar: an analysis of phonetic representations transfer across lan-
guages. In Interspeech, 2020.
A Proofs of Theoretical Results
A. 1 Statistical definition of phonemes
Proof of Proposition 1. Without loss of generality, suppose (x1, x01) ∈ X2, suppose there exists y1
such that PY |X (y1 |xt) > PY|X(y1|x0t), then there exists y2 such that PY |X (y2 |xt) < PY|X(y2|x0t),
which means there exists 0 ≤ α1,α2 ≤ 1,αι + α2 ≤ 1, such that PIX(yi|xt) ≤ α2 < PIX(yi|xt).
1, 2	, 1	2	,	PY|X(y2|x0t)	α1	PY|X(y2|xt)
Now, since Equation 2 holds for arbitrary PY∣χ=Xs ∈ ∆lYl, S= t, we can set PY∣χ(yι∣χ2)=
αι,Pγ∣χ(y2∣x2) = a2,Pγ∣χ(yι∣xt) = PY∣χ(y2∣xt) = 1, ∀t > 2, in which case Equation 2 boils
down to argmaxi∈{i,2} aiPγ∣χ(yi∣xι) = argmaxi∈{i,2} aiPγ∣χ(y/x；). However, by the choice
of αi's, the left-hand side is y； since αιPγ∣χ(yι∣xι) > α2PY∣χ(y2∣x1) and the right-hand side
is y2 since a2Pγ∣χ(y2∣x1) > αιPγ∣χ(y；|x；), and therefore Equation 2 cannot hold. Therefore,
Equation 2 is true only if PY|X(yIxI)= PYIX(y|XI),∀(XLxI) ∈ X2,y ∈ Y.	□
12
Under review as a conference paper at ICLR 2022
A.2 Equivalence of TER and standard phoneme discovery metrics
Consider the groundtruth assignment z(∙) and a codebook assignment Z(∙) with KK code words, the
NMI of Z is defined as:
NMI(Z)
2I(Z(X); Z(X))
H (Z(X))+ H (Z(X)),
(11)
where H(∙) denotes the entropy and I(∙; ∙) denotes the mutual information.
which is also related to the token F1 used for acoustic unit discovery (Dunbar et al. (2017)).
Since SPD is an unsupervised learning problem and ground truth phoneme labels are not avail-
able, matching between codebook indices and phoneme units is needed. When computing to-
ken F1, we consider two different many-to-one mappings πrec : {1, ∙∙∙ ,K} → {1, ∙∙∙ , K} and
∏prec : {1,… ,K }→{1, ∙∙∙ ,K} to compute the token recall and precision respectively as:
Rec(Z)= max P{Z(X) = πrec(Z(X))}	(12)
πrec
Prec(Z) := maxP{z(X) = ∏prec(Z(X))},	(13)
πprec
before computing the harmonic mean between the two to obtain token F1: F1(Z)
The following proposition relates TER with token F1 and NMI.
2 Prec(Z)Rec(Z)
PreC(Z)+Rec(Z) .
Proposition 2. For any assignment function Z : {1,…, K} → {1, …, K}, PTER (Z) = 0 if and
only ifF1 (Z) = NMI(Z) = L
Proof. First of all, for such Z, we have 1 ≥ F1(Z) ≥ min{Prec(Z), Rec(Z)} ≥ 1 - Pe, ter(Z) = 1,
where the third inequality comes from the fact that the set of permutations is a smaller set than the set
of all many-to-one mappings ∏ : {1,…，K }→{1,…，K}. Further, using the fact that Z and Z are
functions of each other when PTER (Z) = 0, it can be shown that NMI(Z)
2H(Z(X))/2H(Z(X)) = 1.
2I(z(X),Z(X))
H(Z(X ))+H(Z(X))
□
A.3 Exact Discovery Guarantee
First, we prove the claim made in Section 4.2 about nearest neighbor information quantizers. Recall
the definition of general and nearest-neighbor information quantizers as follows.
Definition 3. (Information quantizer) A K -point information quantizer is a function q : ∆lYI →
C = {Qι,…, QK} ⊂ ∆lYL where C is called the codebook and Qk，s are called the code distri-
butions. Further, define QK to be the class of such functions.
Definition 4. (Nearest-neighbor Information quantizer) A K -point information quantizer is called
nearest-neighbor if, ∀P ∈ ∆lYl, Dkl(P||q(P)) = minι≤k≤κ DKL(P||Qk). Further, define QNKN
to be the class of such functions.
Then we have the following lemma.
Lemma 1. There exists an information quantizer θn ∈ Θ, ^n ∈ QKK such that
一 ，_ O . .	一 , _
LIQ(Pn,θ,q)= min LIQ(Pn,θ,q).
θ∈Θ,q∈QK
(14)
一 .	,O ..	一一一
Therefore, (θ, q) ISanERMof(Pi).
Proof of Lemma 1. Notice that only the LQ term of Equation P1 depends on q, so it suffices to show
that minq∈QNN LQ(Pn, q) ≤ minq∈QK LQ(Pn, q). This is true since
qmn LQ (Pn,q)= qmn EPn DKL (PY|X||q(PYIX ))] ≥ EPn LminK DKL(PY 山版)]
=q∈mnN EPn DKL (PY|X|Iq(PY |X ))] = q∈mnN LQ(Pn,q).
□
13
Under review as a conference paper at ICLR 2022
Next, We show under the condition P&x = Pγ∣χ and n → ∞, (Pi) recovers z(∙) UP to a PermUta-
tion.
Proposition 3. The pair (z*, PγY∣z) is a minimizer to the following optimization problem:
max	I(Z(X); Y),	(Po)
z型→{1,…，K},Pγ |Z ∈∆lYl
ifand only if z* is equal to the true assignment function Z up to a permutation.
Proof. ⇒: First, z(∙) is a feasible solution by definition. By data processing inequality, we have
I(Z0(X); Y) ≤ I(X; Y) = I(Z(X); Y).
Therefore, z(∙) is also the optimal solution.
u: Suppose there exists some optimal (z,Pγ∣z) with PY∣z(χ) = PY∣z(χ) for at least one X ∈
X . Since such discrepancies are independent with each other, it suffices to show that each such
discrepancy leads to lower I(Z; Y). Indeed, for (z, Pγ∣z) with Pγ∣z=z(χ) = Pγ∣z=z(x)only at x,
I(Z(X); Y) -1(z(x); Y) = PX(x) XPγ∣x(y∣χ)iog PYIZ=Z(X)
y	Pγ |Z=z(x)
=-PX(X)D(PY∣Z=z(x)llPγ∣Z=Z(X)) < 0,
which contradicts the optimality of z. Therefore, Pγ∣z(χ) = Pγ∣z(χ) for all optimal solution of
(Po).	□
To prove Theorem 1, we also need the following lemma.
Lemma 2. Under Assumption 3, for any bounded parameter set Θ, there exists γ > 0 and some
optimal parameter θ* ∈ Θ* such that DKL(PY |x||PY：X) ≥ γ∣∣θ — θ*k,∀θ ∈ Θ.
Proof. We prove the lemma by contradiction. First, we assume θ 6∈ Θ* since the inequality satisfies
trivially for any θ ∈ Θ*. By boundedness, there exists some R > 0 such that kθk ≤ R. Suppose
for any γ > 0, there exists some θ ∈ Θ such that DKL(PYIX∣∣PY∣χ) ≤ Y∣∣θ — θ*k ≤ 2γR, then
we have DKL(PY∣x∣∣PY;X) ≤ infγ>o YR = 0. However, since DKL(PY∣x∣∣PY[X) ≥ 0, we have
DKL(PY∣x ∣∣PY∣x) = 0, which implies θ ∈ Θ* and leads to contradiction.	□
Note it is crucial that the parameter set is bounded, which is the case for neural nets. Further,
Assumption 3 is needed or the inequality can be easily violated when the optimal parameter set Θ*
is empty.
Next, we need the following lemma, which is based on (Tsai et al. (2020)):
Lemma 3. Under Assumptions 1-3, and consider θ to be part of the ERM of (P1) with conditional
distribution PγX := PY∣x. Thenforany e > Qthefollowing inequality holds:
P < sup DKL(PY∣X=XlIPYIX=X) > E
X∈X
≤ 2N (θ, 4ρ)
Y2nE2
exp -~2ρ2(Cu — Ci)2
(15)
where N(A, E) is the E-net of set A.
Proof. For notational ease, we drop the dependence of LCE on P if the context is clear. Using
Assumption 3, let Pγ∣x = PYlX. Define Dn(P∣∣Q) as the empirical KL divergence. Further,
notice that for PYIX, LQ can always be made 0 and therefore, the ERM of (P1) needs to satisfy
Lce (θ) ≤ Lce (θ*). AS a result,
PY∣X(YlX)
Dn(Pγ IX I∣Pγ IX ):= EPn log JIXVY = Lce(Θ)-Lce(Θ*) ≤ 0.
PY∣X(YlX)
14
Under review as a conference paper at ICLR 2022
Note that Dn(P ||Q) is an unbiased estimator of the conditional KL divergence between distributions
P and Q: EPXynE以 log IYXYg = D(PY∣χ∣∣Qγ∣χ). Therefore,
P { DKL(PYIX UPYIX ) > f}
≤P {DKL(PY∣Xl1PYIX ) - Dn(PY ∣X ||PY ∣X ) > ^}
=P I |Dn(PY ∣X llPY∣X ) - DKL (PY ∣ X ||P^Y ∣ X ) | > e}
≤P I sup | Dn (Py ∣ X||PY ∣ X ) - DKL(PYI x||PY ∣ X ) | > }
To bound the last probability, consider an 前-net in the parameter space N(Θ,前)and Θ =
Uaθ'4ρ) ∣Θk,	where	Θk	is the	前-ball surrounding	θk	∈ N(Θ,前),and let	∆n(θ):=
Dn(Pγ∣ x||PY ∣ X) - Dkl(Py∣ X||PY ∣ X) we have ∀θ ∈ Θk,
P S sup |∆n (θ》 > “ ≤
lθ∈θ	J
IN (θ, 4ρ )|	r
X P sup |∆n(θ)| >e
k=1	lθ∈θk
≤ Ne 4ρ)
sup P < sup [An^P > €
k	U∈θfc
(16)
Further, by Assumption 2, we have
sup |∆n(θ) - ∆n(θk)]
θ∈Θk
≤ 黑 | Dn(Pγ ∣ X ||PY ∣ X ) - Dn(Pγ ∣ x||PYkX )| + | DKL(PY ∣ X |陷 X ) - DKL(PY ∣ X || PYkX ) |
=EIIOg PYkX(YIX) +Ei log PYkX(YIX) ≤ 切θk-θk≤ €.
Pn	g PY ∣ X (Y|x) + PXY	g PY ∣ X (Y|x) - ρ" k 11 - 2
As a result,
p( sup |∆n(θ)| >e) ≤P han^k)[ + SUP ]∆n(θ)- ∆n(θk)| >e)
lθ∈θk	J I	θ∈θk	J
≤P {4(θk)| > ∣}
≤2exp (-2(°二 Ci)2 ),
by Assumption 1 and Hoeffding,s inequality. Plugging this into (16), we arrive at
P {DKL(PY ∣xUPYI X) >€} ≤ 2 N(θ,金) exp (-2(C：- Cl)2).
To prove uniform convergence, use Assumption 2 to conclude that:
(17)
Dkl(Pγ ∣ X=X||Py ∣ X=X) = XPγ∣ X(y|x)log PY ∣ X⑻X) ≤ SUP log PY ∣ X⑶X) ≤ ρ∣∣θ* - θn∣∣,
V	PynX (y|x)	y	PynX (y|x)
for some θ* ∈ Θ*. Therefore, using the local convexity property of the KL divergence around
minima in Lemma 2, we arrive at the desired result:
P ISUP DKL(PYI X=JPYI X=X) ≥ €]
[x∈X	J
≤P {kθ* - Gnk ≥ p} ≤ P {dkl(Py I X||Py I X) ≥ m
≤2 N(Θ, ʌ) exp
4ρ
22
γ2ne2
2ρ2(Gu - Ci)2
—
□
15
Under review as a conference paper at ICLR 2022
Next, we prove the following lemma by performing a perturbation analysis on (P1) inspired by (Qiu
et al. (2019)).
Lemma 4. Consider some subset of speech segments D ⊂ X such that for any 1 ≤ z ≤ K, there
exists x ∈ X such that Z(X) = Z. Further, suppose there exists e > 0 such that ∣Pyix=x —
PYIX=χkι ≤ e,∀x ∈ D. Then, ∀x ∈ X, ∣q(Pγix=χ) — Pγix=χ∣ι ≤ ciE1/2 for some constant
c1 > 0.
Proof. We first prove the statement for the segments from the set D. By the definition of ERM,
LQ(Pn,q)-LQ(pn,q*)= EPn log IPP∖X (YjXi	≤ 0.	(*)
q(PYIX(Y|X))
From the condition in the lemma, we have PYIX=X = Pγ∣χ=X + Eφχ for some E ∈ [0,1] and
φx ∈ RlYl,φ>1 = 0, kφχ∣ι ≤ 1,∀x ∈ D. Further, suppose q(Pγ∣χ) = Pγ∣χ + δψχ for some
δ ∈ [0, 1] and ψX ∈ RIYI , ψX>1 = 0, IψXI1 ≤ 1, ∀X ∈ X. Using Assumption 1 and the inequality
2
log(1 + x) ≤ X ——4, ∀x ∈ (-1,1], we have
X Pγ∣x (y∣X)log PYX (y|X)
V YIX⑼ J	q(Pγχ(y∣X))
- φx(y) log
PY |X ⑶X)
q(PY |X ⑶X))
2
δ
≥
E
X
|(y
IX
PY
4
4^ -Cu3
≥
4
for every X ∈ D. Therefore, to maintain (*), we need δ2 ≤ 4Cu|Y| for the training examples Xn
and the inequality in the lemma holds for examples from D with coefficient c； := 2，Cu|Y|.
To show the same claim holds for any unseen segments X0 ∈ X\D, we first use Lemma 1 to conclude
that there always exists a nearest-neighbor information quantizer q that is an ERM. Further, since
every phoneme class occurs in D, we can always find X ∈ D such that z(X) = z(X0). Therefore,
2
using the inequality log(1 + x) ≥ X - ιx+χ, ∀x > -1,we have
2 kPY ∣X=x0 - q(PYX=XO)k2
≤D(PYIX=X0 ||q(PYIX=XO)) ≤ D(PY∣X=x0 ||q(PY|X=X))
≤D(PYX=XO ||q(PYIX=X)) + e|D(PY∣X=X0 Uq(PY∣X=XO))- D(PYIX=X011PYIX=Xo)|
≤D(PYX=XO ||q(PY IX=X)) + E(Cu - Cl) ≤ ^X	≠X (y)	+ E(Cu - Cl )
y PY IX (y|Xj)
eCuδ2
≤.................................  +	E(CU - Cl) ≤ aιE,
miny:PY|x(yIz(X'))>0 PYIZ(y|Z(XO))
where a1 := eCuc012/ miny:PY |Z (yIz(XO))>0 PYIZ(y|z(X0)) + Cu - Cl > c012. Notice that the mini-
mum is taken over y’s with nonzero probabilities due to the boundedness conditions in Assumption
1, which asserts φX (y) = ψX (y) ≡ 0 for y’s with zero probabilities. Finally, using triangular
inequality:
Il D	ʌ/ Γ)	Ml / Il Γ)	ʌ/ f)	Ml ι Il f)	D	Il
IIpYX=Xo - q(PYIX=xo)∣∣1 ≤ IIpYIX=Xo - q(PYIX=xo)∣∣1 + ||PYIX=xo - PYIX=XO ∣∣1
≤ ∖∕2aιE + E ≤ C1 √E,
where ci := √2aι + 1 is the coefficient in the lemma.	□
Now we are ready to prove Theorem 1.
16
Under review as a conference paper at ICLR 2022
ProofofTheorem 1. Define the event Ce := {suPχ∈χD(Pγ∣x=x∣∣Pγ∣x=x) < e}. Further, SUP-
pose Θ is within the ball of radius R in Rd . By Lemma 3, we have:
P(Ce) ≥ 1 - exp(-c2 n2 + c3()),	(18)
Wherec2 := 2ρ2(CY-Cl)2,c3(e) := dlogR(1 + 8ρ) + log2 ≥ log2∣N(Θ, 4ρρ)| (see e.g., Vershynin
(2018), Section 4.2). For the subsequent discussion, suppose Ce occurs. To prove that Z achieves
zero TER, it suffices to prove that Z(x) = Z(x0) ⇔ z(x) = z(x0), ∀x, x0 ∈ X. To prove the
“⇒" direction, suppose for some segment pairs (χ1,χ2) ∈ X2, Z(χι) = Z(χ2) = z0 but z(χι)=
Z1 = z(x2) = Z2. Invoke Lemma 4 and write Qz(Xj)= PY∣χ=χj∙ + δψχj∙,δ = cιt1∕4,ψ>∙1 =
2
0, kΨχjkι ≤ 1,j ∈ {1, 2}. Use the inequality log(1 + x) ≥ X - ι+χ,∀χ > -1 we have
DKL (PY|X=xj∙ U Qz(Xj ))
-	PY |X (y|xj) log 1 +
y
δψχj (y)	]
PY ∣χ (y|xj )√
≤
y
◎ δ2ψχj∙ (y)2
PY ∣χ (y|xj)
≤ a2(z1, z2)δ2,
wherea2(z1, z2) = maxj∈{1,2} eCu / miny:PY |Z (y|zj)>0 PY |Z (y|zj). As a result,
2a2(z1, z2)δ2 ≥ DKL(PY|X=X1 ||Qz0) + DKL (PY|X=X2 ||Qz0) ≥ 2DJS (PY|X=X1 ||PY|X=X2),
(19)
which cannot be true if δ2 ≤
DJS(PY ∣Z = zι ||PY ∣Z = Z2 ) or e ≤ DJS(PY ∣Z = zι ||PY ∣Z = Z2 )2
a2(zl,z2)	, 一 Cl(z1,z2)2a2(z1,z2)2
To prove the other direction, we use "⇒" to conclude that every phoneme type occurs in at least
one distinct cluster from other classes, since every cluster in C contains only a unique phoneme
class. Further, define E = 昌 minz Pn=ι 1ζi=z = 0}. Using Sanov,s theorem (see e.g., Cover &
Thomas (2006)), we have:
P(E) ≤ (n + 1)K exp
-n P∈PEDKL(P 1|PZ)
where PE := {P ∈ ∆K : minz P(z) = 0}. Use Assumption 4 and optimize the bound, we obtain
K
and
min DKL
P∈PE
(P ||PZ ) = PminEDKL (P |1 ⅛1) = log K - PmaE H(P )=log
K-1
P(E) ≤ exp
-n log
+Klog(n+ 1)
K
K - 1
As a result, phonemes of each class occur at least once in the training set with high probability. If
this is the case and if there exists some χ,χ0 ∈ X such that z(χ) = z(χ0) but Z(χ) = Z(χ0), C
contains at least K+ 1 clusters, which contradicts Assumption 4. Therefore, the token error rate can
be upper bounded as
PTER(Z) ≤ P(Ce ∩ Ec)P {Z(X) = Z(X0) ⇔ Z(X) = Z(X0)∣Ce ∩ Ec} + P(Cc ∪ E)
exp (-min {c2nE*2 - C3(e*),nlog KK ] - Klog(n + 1)
with	e*	:= minz	=z	DJS(PYIZ=ZI 11PYlZ=j2)	=：	minz	=	c(zι, z2)DjS(PY∣z=z,	||PY∣z=z9)2.
z1 6=z2	c1 (z1 ,z2 )2a2 (z1 ,z2 )2	z1 6=z2	1 , 2 JS Y|Z=z1	Y|Z=z2
Therefore, PrER(Z) ≤ δ amounts to
*2
c2ne*
1 K
n log E
-c3(e*) ≥ iog1
δ
-Klog(n + 1) ≥ log δ.
17
Under review as a conference paper at ICLR 2022
The first inequality implies
n≥
log C3(E*) + (1∕δ)
c2e*2
For the second inequality, rearranging the terms we obtain:
n ≥ ------L
log K-1
log n +
log δ
log K-1
(20)
K
which by Lemma A.2 from (Shalev-Shwartz & Ben-David (2014)) holds if
4K log WKKI + 2log1
n ≥ ---------------------
-	log K-1
(21)
Combining Equation 20 and Equation 21 proves the theorem.
□
B S poken Word Dataset Statistics
The dataset statistics of all the datasets used for our experiments are shown in Table 1.
Table 1: Statistics of four spoken word datasets used for experiments. TIMIT and Mboshi have
the same number of training and test words since the whole datasets are used for both training and
evaluation, consistent with prior works (Yusuf et al. (2020), Feng et al. (2021b)).
|Y|	Flickr Audio	Librispeech			TIMIT			Mboshi	
	224	224	524	824	224	524	824	161	377
K	44	39	39	^^9^^	39	39	39	31	31
#train words	46569	50073	143512	188863	1289	1678	2348	30290	82606
#test words	6557	595	595	595	1289	1678	2348	30290	82606
#phonemes	318756	223821	590647	816754	5501	7692	11874	93236	165212
#hours	6.1	6.3	15.4	21.2	0.1	0.1	0.2	2.2	4.1
C Further analysis of representations learned by IQ
The visualizations of the estimated distributions PYθ |X using t-SNE (van der Maaten & Hinton
(2008)) on TIMIT and Mboshi are shown in Figure 5.
D	Effect of Codebook Size for IQ
18
Under review as a conference paper at ICLR 2022
-40
Manner level t-SNE plot
20O
IE一P 山NSJ
IE-P 山 NSj
Manner level t-SNE plot
824 (5b) Manner-level t-SNE plots of phoneme clusters
discovered by IQ with |Y| = 161 and gold seg-
mentation on Mboshi
-60	-40	-20	6	20
(5a) Manner-level t-SNE plot by IQ with |Y|
and gold segmentation on TIMIT
Figure 5: t-SNE
plot by IQ on TIMIT and Mboshi
(6a) Top-10 most confusing phoneme pairs by IQ (6b) Top-10 most confusing phoneme pairs by IQ
with |Y| = 824 and predicted segmentation on with |Y| = 161 with predicted segmentation on
TIMIT	Mboshi
Phoneme Pair	Error Prob.	Phoneme Pair	Error Prob.
ae, aa	1.00	a, Ng	1.00
ch, ah	0.85	bv, b	0.82
sh, s	0.82	e, a	0.79
ah, aa	0.82	t, S	0.77
aw, aa	0.77	i, e	0.73
z, s	0.75	b, Ng	0.68
n, m	0.73	p, k	0.68
p, k	0.70	f, a	0.59
r, er	0.67	g, a	0.59
iy,ey	0.60	o, mw	0.56
19
Under review as a conference paper at ICLR 2022
Table 2: Phoneme discovery performance vs. codebook size on TIMIT. The models used are IQs
trained on Librispeech+TIMIT.
Codebook size
30
40
50
60
70
|Y|	224	Token F1 NMI Boundary F1	512±1.0 43.0±0.7 77.7±0.5	50.9±0.8 43.4±0.9 78.6±0.4	50.3±0.6 43.6±0.3 78.2±0.3	49.0±1.2 43.1±0.7 78.1±0.6	49.0±0.4 43.5±0.5 78.3±0.6
|Y|	524	Token F1 NMI Boundary F1	53.5±0.8 46.8±0.6 80.4±0.2	53.9±0.3 46.7±0.2 80.4±0.2	53.0±0.9 46.7±0.4 80.3±0.1	52.0±0.9 46.9±0.3 80.2±0.1	52.5±0.7 47.3±0.2 80.3±0.1
|Y|	824	Token F1 NMI Boundary F1	53.7±0.5 47.1±0.4 80.6±0.0	54.4±0.4 47.5±0.2 80.5±0.1	53.3±0.4 47.3±0.2 80.4±0.1	52.6±0.8 47.4±0.4 80.3±0.0	50.7±0.9 47.1±0.4 80.3±0.0
Table 3: Phoneme discovery performance vs codebook size on Mboshi. The models used are IQs
with CPC+BNF features.
Codebook size	30	40	50	60	70
Token F1	542±1.0	54.2±0.2	51.1±0.9	54.0±0.7	45.9±0.8
|Y| = 161	NMI	45.1±0.4	44.0±0.4	44.7±0.2	44.3±0.7	44.3±0.5
Boundary F1	67.5±0.0	67.4±0.1	67.3±0.1	67.3±0.1	66.8±0.0
Token F1	57.1±1.0	572±1.1	56.7±1.6	56.8±1.1	55.2±0.4
|Y| = 377	NMI	49.3±0.3	49.0±0.1	49.8±0.2	49.6±0.4	49.5±0.6
Boundary F1 67.3±0.1	67.3±0.1	67.3±0.1	67.1±0.2 67.0±0.0
20