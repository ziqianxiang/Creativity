Under review as a conference paper at ICLR 2022
Staircase Sign Method for Boosting Adver-
sarial Attacks
Anonymous authors
Paper under double-blind review
Ab stract
Crafting adversarial examples for the transfer-based attack is challenging and re-
mains a research hot spot. Currently, such attack methods are based on the hy-
pothesis that the substitute model and the victim’s model learn similar decision
boundaries, and they conventionally apply Sign Method (SM) to manipulate the
gradient as the resultant perturbation. Although SM is efficient, it only extracts the
sign of gradient units but ignores their value difference, which inevitably leads to a
serious deviation. Therefore, we propose a novel Staircase Sign Method (S2M) to
alleviate this issue, thus boosting transfer-based attacks. Technically, our method
heuristically divides the gradient sign into several segments according to the val-
ues of the gradient units, and then assigns each segment with a staircase weight
for better crafting adversarial perturbation. As a result, our adversarial examples
perform better in both white-box and black-box manner without being more visi-
ble. Since S2M just manipulates the resultant gradient, our method can be gener-
ally integrated into any transfer-based attacks, and the computational overhead is
negligible. Extensive experiments on the ImageNet dataset demonstrate the effec-
tiveness of our proposed methods, which significantly improve the transferability
(i.e., on average, 5.1% for normally trained models and 11.2% for adversarially
trained defenses). Our code is available in the supplementary material.
1	Introduction
With the remarkable performance of deep neu-
ral networks (DNNs) in various tasks, the ro-
bustness of DNNs are becoming a hot spot
of the current research. However, DNNs
are pretty vulnerable to the adversarial ex-
amples (SZegedy et al., 2014; Goodfellow
et al., 2015; Biggio et al., 2017; Zhang et al.,
2020) which are only added with human-
imperceptible perturbations but can fool state-
of-the-art DNNs (Szegedy et al., 2016; 2017;
He et al., 2016; Huang et al., 2017) success-
fully. To make the matter worse, attacking in
the physical world (Sharif et al., 2016; Kurakin
et al., 2017b; Xu et al., 2020) is also practica-
ble, which inevitably raises concerns in real-
world applications such as self-driving cars.
To better evaluate the robustness of DNNs, var-
Figure 1: We generate adversarial examples by I-
FGSM and our proposed I-FGS2M. The Red high-
light denotes the pre-set target label.
ious works have been proposed to seek the vulnerability of DNNs. Specifically, white-box attacks
such as Iterative Fast Gradient Sign Method (I-FGSM) (Kurakin et al., 2017a), Deepfool (Moosavi-
Dezfooli et al., 2016) and Carlini & Wagner’s (C&W) method (Carlini & Wagner, 2017) can achieve
impressive performance with the complete knowledge of the victim’s model, e.g., gradient and struc-
ture. However, deployed DNNs are usually transparent to unauthorized users for security, and thus
the adversary cannot base on any knowledge of the victim’s model. Therefore, resorting to cross-
model transferability (Szegedy et al., 2014; Moosavi-Dezfooli et al., 2017; Naseer et al., 2019; Gao
et al., 2021; Naseer et al., 2021) of adversarial examples is a common practice. That is to say, the ad-
1
Under review as a conference paper at ICLR 2022
sign (S)
staircase sign (S )
1'---------------
_ _0_________________I
9
--1
3/2"
1/2 -----
____	0
------1/2
--3/2
Figure 2: Sign function vs. Staircase sign function (take K = 2 for example). Left & Middle: the
graph of functions. Right: illustration of the gradient direction of the substitute model gA, black-
box model gB, and the resultant update directions of sign and staircase sign function w.r.t gA . To
express more visually, here we ignore the magnitude of each perturbation. For sign function, each
unit of the resultant direction is the same. By contrast, our method reflects the difference between
units, and is more close to both gA and gB.
versarial examples crafted via known white-box models (a.k.a substitute model) are also dangerous
for other unknown black-box models, which makes the black-box transfer-based attack possible. In
this field, Goodfellow et al. (Goodfellow et al., 2015) hypothesize that the vulnerability of DNNs is
their linear nature. Since raw gradient magnitude is extremely small (e.g. the minimal unit in gradi-
ent ≈ 10-9) and digital images usually use 8 bits per pixel, they propose Fast Gradient Sign Method
(FGSM) (Goodfellow et al., 2015) so that each pixel can be fully perturbed with only a single step.
Conventionally, the following transfer-based iterative attack methods (Dong et al., 2018; Xie et al.,
2019b; Lin et al., 2020b; Gao et al., 2020a; Wu et al., 2020) are all based on Sign Method (SM) to
boost adversarial attack.
However, there is a limitation in SM, i.e., ignores the difference among each unit in the gradient vec-
tor. As illustrated in Figure 2(a), the update direction obtained by the sign function is that whether
the partial derivative of loss function at each pixel is positive, negative or zero. Since the transfer-
ability phenomenon is mainly due to the fact that decision boundaries around the same data point
of different models are similar, a naive application of SM results in a poor gradient estimation (as
depicted in Figure 2(c)). Consequently, the adversarial examples especially for targeted ones may
deviate from the global optimal region where most of DNNs can be fooled, thus decreasing the
transferability.
Motivated by this, we propose a Staircase Sign Method (S2M) to effectively utilize the gradient
for the substitute model, thus more closely approximating the gradients of both the black-box and
white-box models. Technically, our proposed method first utilizes the sign function to roughly get
the gradient direction for the substitute model, then heuristically assigns different weights for each
pixel by our staircase sign function. In short, we merely manipulate the perturbation added on
the image. Thus, S2M can be generally integrated into any transfer-based attacks, e.g., the family
of FGSM algorithms. Based on I-FGSM, we propose its variant I-FGS2M (Algorithm 14) which
can also serve as a iterative attack baseline to be combined with the state-of-the-art approaches,
e.g., input diversity (Xie et al., 2019b), Poincare space loss (Li et al., 2020), and patch-wise++
method (Gao et al., 2020b). To sum up, our main contributions are as follows:
•	To the best of our knowledge, we are the first to point out the poor gradient estimation
limitation of Sign Method in transfer-based attacks, which causes the adversarial examples
to deviate from the global optimal region.
•	We propose a novel Staircase Sign Method to alleviate the problem. Notably, our method
is simple but effective, and can be integrated into any transfer-based attacks.
•	Extensive experiments on the ImageNet dataset (Russakovsky et al., 2015) demonstrate the
effectiveness of our proposed attacks which consistently outperform vanilla FGSM-based
non-targeted & targeted ones in both black-box and white-box manner.
2
Under review as a conference paper at ICLR 2022
2	Related Works
In this section, we first briefly review the development of transfer-based attack methods in Sec. 2.1,
and then introduce several defense methods in Sec. 2.2.
2.1	Transfer-based Black-box Attacks
Unlike the white-box attack, the black-box attack cannot obtain the gradient or parameters of the
victim’s model. Although query-based black-box attack (Chen et al., 2017; Ilyas et al., 2018; An-
driushchenko et al., 2020) can be applied in this maner, a large number of queries is computationally
expensive. Thus, we turn to the transferability of adversarial examples in the remainder of this paper.
For non-targeted attacks, (Goodfellow et al., 2015) quantify the gradient by the sign function and
propose single-step FGSM with the step size equal to maximum perturbation. However, perturbing
images with single-step attacks usually cannot get a high success rate on the white-box model.
Therefore, (Kurakin et al., 2017a) propose I-FGSM which applies FGSM multiple times with a small
step size. Considering that iterative methods usually sacrifice transferability to improve the white-
box performance, (Dong et al., 2018) integrate momentum term into the iterative process to avoid
adversarial examples falling into local optimum. (Xie et al., 2019b) apply random transformations to
the input images to alleviate the overfitting problem. (Wu et al., 2020) explore the security weakness
of skip connections (He et al., 2016) to boost adversarial attacks. To effectively evade defenses,
(Dong et al., 2019) propose a translation-invariant attack method to smooth the perturbation. (Lin
et al., 2020a) adapt Nesterov accelerated gradient and leverage scale-invariant property of DNNs to
optimize the perturbations. (Naseer et al., 2019) train cross-domain generators by using different
training data distribution. (Gao et al., 2020a) craft patch-wise noise to further increase the success
rate of adversarial examples.
However, targeted attacks are more challenging which need to guide the victim’s model to predict a
specific target class with high confidence rather than just cause misclassification. In this setting, (Li
et al., 2020) replace the cross-entropy loss with Poincare distance and introduce triple loss to make
adversarial examples close to the target label. (Gao et al., 2020b) extend non-targeted (Gao et al.,
2020a) to targeted version and adopt temperature term to push the adversarial examples into the
global optimal region of DNNs. Instead of optimizing the output distribution with a few iterations,
(Zhao et al., 2020) directly maximize the target logits with more iterations. To make the adversarial
examples more transferable, several researchers (Sabour et al., 2016; Inkawhich et al., 2019) turn
to directly optimize intermediate features instead of output distribution. (Naseer et al., 2021) train
generators to match “distribution” of the target class. Although (Inkawhich et al., 2020a;b; Naseer
et al., 2021) have achieved impressive performance in this way, both training specific auxiliary
models and generators for each target class is very time-consuming. Therefore, we resort to FGSM-
based attacks in this paper.
2.2	Defense Methods
With the development of adversarial examples, researchers pay more and more attention to the ro-
bustness of DNNs, and various defense methods are proposed to circumvent potential risks. (Guo
et al., 2018) apply multiple input transformation such as JPEG compression (Dziugaite et al., 2016),
total variance minimization (Rudin et al., 1992) and image quilting (Efros & Freeman, 2001) to
recover from the adversarial perturbations. (Theagarajan et al., 2019) introduce probabilistic adver-
sarial robustness to neutralize adversarial attacks by concentrating sample probability to adversarial-
free zones.
Although the above methods are efficient, i.e., do not require a time-consuming training process, the
adversarial training defense mechanism is more robust in practice. In this field, (Madry et al., 2018)
adopt a natural saddle point formulation to cover the blind spots of DNNs. (Tramer et al., 2018)
introduce ensemble adversarial training which augments training data with perturbations transferred
from other models. (Xie et al., 2019a) impose constraints at the feature level by denoising technique.
3
Under review as a conference paper at ICLR 2022
(a) w/o SM and M
(b) w/ SM
(c) w/ M
Figure 3: We visualize the perturbations of an image at the first iteration. All perturbations are
normalized to [0, 1]. (a): the gradient of an ensemble of Inc-v4 (Szegedy et al., 2017), IncRes-
v2 (Szegedy et al., 2017) and Res-152 (He et al., 2016); (b)&(c): the results of SM and S2M w.r.t
(a). The numbers in the lower right corner indicate the average cosine similarity between (a) and
the other two perturbations on 1,000 images. Notably, our S2M not only keeps the perturbation
magnitude but also has higher cosine similarity to (a).
3	Methodology
Before introducing our algorithm in detail, we first describe the background knowledge of generating
adversarial examples. Given a DNN network f (x) : x ∈ X -→ y ∈ Y, it takes an input x (i.e., a
clean image) to predict its label y . For targeted attacks1, it requires us to find a human-imperceptible
perturbation δ to satisfy f (X + δ) = y*, where x* = X + δ is the generated adversarial example
and y * is our preset target label.
In order to make the resultant adversarial examples invisible to the clean ones, the adversary usually
sets a small perturbation upper bound , and lets kδ k∞ ≤ . By minimizing the loss function
J (X*, y*), e.g., cross entropy loss, the constrained optimization problem can be denoted as:
argminJ(X*,y*), s.t. kX* -Xk∞ ≤ .	(1)
x*
For targeted attacks a resultant adversarial example at iteration t + 1 can be formally written as:
x*+ι = cliPx,e{x* - α ∙ sign (Vx J (X*, y*))},	Q)
where Clipx e (∙) keeps the adversarial example x* within the e-ball of x, and α = ε∕T is the step
size.
3.1	Our Method
In this section, we first introduce our motivation in Sec. 3.1.1. Then, we elaborate our method in
Sec. 3.1.2 and ensemble strategy in Sec. 3.1.3.
3.1.1	Rethinking the Sign Method
To the best of our knowledge, existing transfer-based attack methods are all based on SM. In addi-
tion to the linear hypothesis (Goodfellow et al., 2015), the motivation of this method is to modify
more information for each pixel than directly adding the gradient, especially for single-step attacks.
Besides, manipulating the gradients by SM for iterative attacks can quickly reach the boundary of
'∞-ball with only a few iterations (Dong et al., 2018; 2019).
However, the transferability of adversarial examples is mainly based on the phenomenon that de-
cision boundaries of different models are similar. Since targeted attacks need to guide the adver-
sarial examples into a specific territory of the target class, directly applying SM inevitably discards
significant information of the gradient of the substitute model. As the toy example shown in Fig-
ure 2, the direction derived from the SM is less closer to the victim’s model than that of our S2M.
Consequently, the resultant perturbation badly deviates from the target territory, thus decreases the
transferability.
1 Non-targeted attacks are discussed in Appedix. C.
4
Under review as a conference paper at ICLR 2022
3.1.2	Staircase Sign Method
Motivated by the limitation of SM, we propose a novel Staircase Sign Method (S2M) to allevi-
ate this problem. Figure 2 depicts the difference between sign function and our proposed staircase
sign function. Since our method merely manipulates the gradient at each iteration, it can be gener-
ally integrated into any transfer-based attacks, e.g., the family of FGSM algorithms. For simplicity
purpose, we only take our variant I-FGS2M (summarized in Algorithm 14) as an example to demon-
strate the integration process.
Technically, our method can be mainly divided into four steps. Firstly, as with the other methods,
e.g., (Dong et al., 2019; Xie et al., 2019b; Gao et al., 2020a), we need to compute the gradient Gt at
t-iteration of the substitute model with respect to the input (in line 5):
Gt = Vx J(x；,y*)	(3)
Secondly, we calculate the p-th percentile gp of |Gt| (in line 7) according to the number of staircase
K, where p ranges from 100/K to 100 with the percentile interval τ = 100/K. Thirdly, we assign
the staircase weights Wt according to gtp by Eq. (4) (in line 8):
1⅞0,	g0 ≤ IGtjI ≤ gt,
益,	gt < ∣Gt,j∣≤ g2,
.
.
Wt =] (2k0F,	gp-τ < ∣Gt,jι≤ gp,	⑷
(2⅛1k, gl00-τ < Gi,jι≤gi00.
where k ranges from 0 to K 一 1, and also equals to p∕τ — 1. As a result, our W is bounded in
[1/K, 2 - 1/K] ⊆ [0, 2]. Finally, combined with the sign direction ofGt, we update our adversarial
examples XJ= (in lines 11):
Xt+1 = cliPx,e{x= 一 α ∙ sign (Gt) Θ Wt},	(5)
where is Hadamard product.
Proposition 1 Assume that Gt is i.i.d. for all t ∈ [0, T 一 1], the adversarial examples can reach the
boundary of ε-ball.
Proof. Due to the fact that ∣∣∙k∞ = max (abs (∙)), here We only discuss Wi,j which is also i.i.d.
ThereforeJPT=01 α ∙ sign (G) Θ W∣∣	= ∣∣PT=01 α ∙ Wj = α ∙ PT=OI Wi,j. Besides,
E(Wtij) = PK-I ~K ∙ (2k+1)τ = 100K ∙ (τ + 3τ + …+ (2K - I)T) = K0 = L SO we have:
E(IX二1α ∙ sign(Gt) Θ Wt
)=。∙ Co1 E (Wtij)
∞
=a ∙ T
(6)
=ε
In fact, S2M is equivalent to applying adaptive weight for each pixel in sign noise. With the help
of our S2M, the poor gradient estimation problem caused by SM can be effectively alleviated. As
demonstrated in Figure 3, the cosine similarity between the gradients (a) and the perturbations ma-
nipulated by our proposed S2M (c) is up to 0.84, while the result of SM is only 0.64. Please note
that our S2M does not aim to make the cosine similarity close to 1.0. This is because the victim’s
model is only similar to the substitute model, but it cannot be exactly the same. As demonstrated in
Figure 2(c), “overfitting” on the substitude model will enlarge the gap with the victim’s model.
The adversarial examples are shown in Figure 1. Compared with I-FGSM which cannot effectively
decrease the confidence of true class, our proposed variant successfully misleads the model to clas-
sify our resultant human-imperceptible adversarial examples as the pre-set target classes.
3.1.3	Attacking an Ensemble of Models
To craft adversarial examples with high transferability, attacking an ensemble of models (Liu et al.,
2017; Dong et al., 2018) is a pretty effective strategy, especially for black-box attacks. It is mainly
5
Under review as a conference paper at ICLR 2022
because crafting adversarial examples on multiple models has the potential to capture the global
optimum of “blind spots” easily. In this paper, we follow the ensemble strategy of (Dong et al.,
2018), which fuses the logits (the output before the softmax) of an ensemble of N models:
N
l (x) =	unln (x) ,	(7)
n=1
where ln (∙) is the logits of n-th model, and Un is its ensemble weight with Un>0 and PnN=1 un = 1.
Algorithm 1: I-FGS2M
Input : The cross-entropy loss function J of our substitute model; iterations T; '∞ constraint
; a clean image x (Normalized to [-1, 1]) and its corresponding true label y; the
target label y*; the number of staircase K (≥ 2);
Output: The adversarial example x*;
1:	Xq = x;
2:	α = /T; τ = 100/K;
3:	Initialize staircase weights W to 0, and p to 100/K;
4:	for t _ 0 to T do
5:	Gt = Vx J(XQ,yQ);
6:	for k — 0 to K do
7:	calculate the p-th percentile gtp of |Gt|;
g0< ∣Gi,jl≤ gτ,
gτ < ∣Gt,jl≤ g2τ,
.
.
.
gp-τ < ∣Gt,jl≤ gp,
.
.
.
g100-τ < Gtjl≤ g100.
10:	end for
11:	xQ+1 = cliPχ,e{xQ — α ∙ Sign(Gt) Θ Wt};
12:	XQ+ι =Clip (XQ+ι,—1,1);
13:	end for
14:	Return XQ = XQT ;
T
100，
3τ
100,
8:	Wt = (2 (2k + 1)τ
-100-,
(2K-1)τ
、-100-，
9:	p = p + τ
4 Experiments
To demonstrate the effectiveness of our staircase sign mechanism, we conduct extensive experiments
based on the family of FGSM methods. Firstly, we introduce the setup of experiments in Sec. 4.1.
Next, we analyze the effect of staircase number in Sec. 4.2. Finally, the attack success rate for
normally trained models and defense models is reported in Sec. 4.3 and Sec. 4.4, respectively. Due
to the space limitation, non-targeted attacks are discussed in Appendix. C. Notably, our non-targeted
FGS2M variants remarkably outperform vanilla FGSM ones by 19.1% at most.
4.1	Setup
Networks: To comprehensively compare the performance between different attack methods, we
consider nine state-of-the-art models, including six normally trained models: Inception-v3 (Inc-
v3) (Szegedy et al., 2016), Inception V4 (Inc-v4) (Szegedy et al., 2017), Inception-ResNet V2
(IncRes-v2) (Szegedy et al., 2017), ResNet-50 (Res-50), ResNet-101 (Res-101) and ResNet-152
(Res-152) (He et al., 2016), and three ensemble adversarial training models: Inc-v3ens3, Inc-v3ens4
and InCReS-V2ens (Tramer et al., 2018).
6
Under review as a conference paper at ICLR 2022
2	4	8	16 32 64 128 256
Staircase number K
(a)


2	4	8	16 32 64 128 256	1	2	4	8	16 32 64 128 256
Staircase number K	Staircase number K
(b)	(O
Figure 4: The success rate (%) of targeted black-box attack (Hold-out) for different methods w.r.t
staircase number K (K = 1 denotes SM and K ≥ 2 denotes S2M). For (a) and (b), the adversarial
examples are crafted via an ensemble of Inc-v4, IncRes-v2 and Res-152, and the hold-out model
is Inc-v3. For (c), the white-box models are an ensemble of Inc-v3, Inc-v4, IncRes-v2, Res-152,
Res-101, Res-50, Inc-v3ens4 and IncRes-v2ens, and the hold-out model is Inc-v3ens3.
Dataset: We conduct our experiments on ImageNet-compatible dataset2. This dataset is comprised
of 1,000 images, and widely used in recent FGSM-based attacks (Dong et al., 2018; Xie et al.,
2019b; Dong et al., 2019; Gao et al., 2020a; Li et al., 2020; Gao et al., 2020b). The target label for
each iamge in this dataset is pre-set and usually different.
Parameters: Following the previous works (Li et al., 2020; Gao et al., 2020b), in our experiments,
the maximum perturbation is set to 16, the iteration T of all methods is 20, and thus the step size
α = /T = 0.8. When attacking an ensemble ofN models simultaneously, the weight for the logits
of each model is equal, i.e., 1/N. ForMI-FGSM (Dong et al., 2018), the decay factor μ = 1.0. For
DI-FGSM (Xie et al., 2019b), the transformation probability p = 0.7. For TI-FGSM (Dong et al.,
2019), when the victim’s model is in normally trained models, the Gaussian kernel length is 5 × 5,
and15× 15 for defense models. For Po-FGSM (Li et al., 2020), we set λ = 0.1. For PI-FGSM (Gao
et al., 2020a) and PI-FGSM++ (Gao et al., 2020b), the amplification factor β = 10, the project factor
γ = 0.8βα, and the project kernel length is 3 × 3. The temperature τ for PI-FGSM++ is set to 1.5.
For our S2M, the number of staircase K is set to 64. Please note that the parameters of each method
are fixed no matter what methods are combined.
Evaluation Metrics: There are three metrics are used to evaluate the performance of targeted at-
tacks: “Hold-out” is the success rate of the black-box models (i.e. transferability), “Ensemble”
denotes the white-box success rates for an ensemble of models, and “AoE” (Gao et al., 2020b)
averages the white-box success rate of each model.
4.2	THE EFFECT OF STAIRCASE NUMBER K
In this section, we analyze the effect of the staircase number K for the state-of-the-art FGSM-based
attacks. Here we tune K = 2, 4, 8, ..., 256. Please note that our methods only take K ≥ 2 as the
input. K = 1 denotes their corresponding FGSM-based baseline.
We depict the curve for the black-box attacks (Hold-out) in Figure 4 and the results for white-box at-
tacks can be found in Appendix. B. Compared with the vanilla FGSM implementation, our FGS2M
variants improve the transferability by a large margin as a whole. Specifically, when transferring
adversarial examples to normally trained models (Figure 4(a)), DI-FGS2M with K = 64 sharply
increases the success rate by 13.4% from 15.4% to 28.8%. Besides, as shown in Figure 4(c),
our methods also boost the attack performance on defenses, i.e., consistently outperform the cor-
responding baseline attacks by 4.3% 〜16.5%. Considering that almost all curves turn to remain
stable when K is big and reach the peak when K = 64 in Figure 6 and Figure 4, we set the stair-
case number K = 64 in the following experiments. Note that the computational overhead of the
percentage calculation for this setting is almost negligible compared to the cost of forward pass and
backpropagation.
2https://github.com/tensorflow/cleverhans/tree/master/examples/nips17_
adversarial_competition/dataset
7
Under review as a conference paper at ICLR 2022
Table 1: The success rate (%) of targeted FGSM-based/FGS2M-based attacks. We study four
models—Inc-v3, Inc-v4, Res-152 and IncRes-v2, and adversarial examples are crafted via an en-
semble of three of them. In each column, “-” denote the hold-out model. We comprehensively
report the results from three metrics, i.e., Ensemble, AoE and Hold-out.
	Attacks	-Inc-v3	-Inc-v4	-Res-152	-IncRes	Avg.
	I	99.9 / 100.0	99.9 / 100.0	100.0 / 100.0	100.0 / 100.0	100.0 / 100.0
	MI	99.9 / 99.9	99.9 / 100.0	100.0 / 100.0	100.0 / 100.0	100.0 / 100.0
	DI2	91.8 / 98.3	93.3 / 98.2	94.5 / 98.7	94.8 / 98.5	93.6 / 98.4
	TI	99.9 / 99.9	99.8 / 99.9	97.0 / 99.9	100.0 / 100.0	99.2 / 99.9
Ensemble (White-box)	Po	100.0 / 100.0	99.9 / 100.0	100.0 / 100.0	100.0 / 100.0	100.0 / 100.0
	PI	100.0 / 99.4	100.0 / 99.9	100.0 / 99.9	99.8 / 99.8	100.0 / 99.8
	M-DI2	88.9 / 95.7	90.4 / 96.7	91.0/ 96.2	92.8 / 98.3	90.8 / 96.7
	T-DI2	92.0 / 97.7	92.2 / 97.7	93.2 / 98.2	94.0 / 98.5	92.9 / 98.0
	T-Po-M-DI2	91.4 / 97.4	93.0 / 97.4	91.9 / 96.5	94.9 / 97.5	92.8 / 97.2
	P-T-DI2	99.3 / 98.8	99.4 / 98.5	99.4 / 99.0	99.6 / 99.4	99.4 / 98.9
	I	94.7 / 97.2	88.6 / 93.3	92.5 / 97.0	89.7 / 93.1	91.4 / 95.2
	MI	94.5 / 96.6	90.1 / 93.5	93.4 / 97.0	90.5 / 93.2	92.1 / 95.1
	DI2	77.8 / 89.1	76.3 / 86.8	84.4 / 93.7	77.8 / 86.0	79.1 / 88.9
	TI	94.0 / 97.0	87.2 / 92.3	92.5 / 96.6	88.6 / 92.3	90.6 / 94.6
AoE (White-box)	Po	88.7 / 92.8	82.4 / 88.5	78.6 / 85.9	87.1 / 91.6	84.2 / 89.7
	PI	98.1 / 97.9	97.3 / 97.2	98.5 / 97.8	96.6 / 96.9	97.6 / 97.5
	M-DI2	75.4 / 84.8	74.6 / 83.7	80.9 / 89.8	76.6 / 85.0	76.9 / 85.8
	T-DI2	78.6 / 89.1	75.9 / 87.0	83.8 / 93.6	76.8 / 86.6	78.8 / 89.1
	T-Po-M-DI2	79.4 / 86.3	76.4 / 83.8	77.0 / 84.6	78.0 / 84.6	77.7 / 84.8
	P-T-DI2	95.1 / 94.0	93.3 / 92.7	97.3 / 96.7	92.7 / 93.5	94.6 / 94.2
	I	1.2 / 4.6	1.2/ 3.4	0.0 / 1.1	0.9/ 1.9	0.8 /2.8
	MI	6.3 /6.5	3.6 / 3.7	1.6/ 1.4	3.0 / 3.6	3.6 /3.8
	DI2	15.4 / 28.8	13.8 / 27.6	3.2 / 8.6	9.4 / 20.6	10.5 / 21.4
	TI	1.6 / 6.1	1.3 / 4.2	0.3 / 1.3	0.8 / 3.2	1.0 / 3.7
Hold-out (Black-box)	Po	1.1 /4.8	0.9 /2.9	0.0 /0.4	0.3 /2.3	0.6 /2.6
	PI	22.4 / 28.9	17.2 / 23.2	4.2 / 6.2	13.9 / 20.9	14.4 / 19.8
	M-DI2	23.8 / 31.5	24.1 / 30.8	12.3 /14.2	21.3 / 28.3	20.4 / 26.2
	T-DI2	15.5 / 29.8	15.9 / 30.6	3.9 /9.7	11.3 / 25.1	11.6 / 23.8
	T-Po-M-DI2	34.7 / 41.8	32.3 / 40.4	17.3 / 18.0	28.3 / 34.4	28.2 / 33.7
	P-T-DI2	46.9 / 50.2	47.1 / 50.8	14.2 / 19.4	41.3 / 44.7	37.4 / 41.3
4.3 Attacking Normally Trained Models
In this section, we compare ten FGSM-based attacks including I-FGSM, MI-FGSM, DI2-FGSM,
TI-FGSM, Po-FGSM, M-DI2-FGSM, T-DI2-FGSM, T-Po-M-DI2-FGSM, P-T-DI2-FGSM with our
FGS2M variants. In this experiment, four models including Inc-v3, Inc-v4, Res-152, and IncRes-v2
are considered. At each iteration, we select one model as the hold-out model to test the transfer-
ability, and an ensemble of the rest three with the weight of each model 1/3 serves as the substitute
model.
As indicated in Table 1, our proposed FGS2M variants effectively boost both the white-box and
black-box attacks. On average, they increase the success rate in Ensemble, AoE and Hold-out cases
by 2.1%, 5.2% and 5.1%, respectively. This demonstrates that our adversarial examples are more
close to the global optimal region.
From the results of Table 1, we also observe that several methods, especially for these integrated
with diversity input patterns (DI2), suffer badly from SM which cannot well utilize the gradient
with respect to the random input transformation. Specifically, DI2-FGSM only successfully attack
93.6% images against the substitute model (Ensemble). The average success rate of each white-box
model (AoE) is even reduced to 79.1%, and merely 10.5% images transfer to the black-box model
(Hold-out) on average. With the help of our S2M at each iteration, we dramatically alleviate the
poor gradient estimation problem, that is, increasing the success rate in Ensemble and AoE cases
by 4.8% and 9.8%, respectively. Furthermore, in the Hold-out case our DI2-FGS2M remarkably
improves the transferability by 10.9%.
Another observation from the results is that the staircase sign perturbation seems to be less effective
on vanilla MI but more effective for other momentum-based methods in the black-box manner, e.g.,
8
Under review as a conference paper at ICLR 2022
M-DI2 . It may be because that the update direction derivated by our FGS2M variants suffers more
from the noise curing (Li et al., 2020) caused by vanilla MI, thus causing a lack of diversity and
adaptability of noise.
Table 2: The success rate (%) of targeted FGSM-based/FGS2M-based attacks. We study nine
models—Inc-v3, Inc-v4, Res-152, Res-101, Res-50, IncRes-v2, Inc-v3ens3, Inc-v3ens4 and IncRes-
v2ens, and adversarial examples are crafted via an ensemble of eight of them. In each column, “-”
denote the hold-out model. We comprehensively report the results from three metrics, i.e., Ensem-
ble, AoE and Hold-out._______________________________________________
	Attacks	-Inc-v3ens3	-Inc-V3ens4	-IncRes-v2ens	Avg.
	T-DI2	56.5 / 78.1	56.5 / 77.8	55.2 / 76.7	56.1 / 77.5
Ensemble	T-M-DI2	44.8 / 64.7	45.4 / 64.9	48.9 / 68.4	46.4 / 66.0
(White-box)	T-Po-M-DI2	58.9 / 74.3	58.3 / 75.3	60.6 / 76.9	59.3 / 75.5
	P-T-DI2++	94.1 / 93.7	94.3 / 93.3	94.2 / 95.0	94.2 / 94.0
	T-DI2	43.4 / 65.1	44.6 / 65.5	46.2 / 68.1	44.7 / 66.2
AoE	T-M-DI2	34.7 / 52.1	36.2 / 52.4	37.8 / 54.5	36.2 / 53.0
(White-box)	T-Po-M-DI2	48.5 / 63.8	48.9 / 63.8	50.1 / 65.5	49.2 / 64.4
	P-T-DI2++	87.5 / 87.0	87.3 / 86.7	88.4 / 87.5	87.7 / 87.1
	T-DI2	13.4 / 29.9	12.6/ 30.0	10.4 / 29.0	12.1 / 29.6
Hold-out	T-M-DI2	14.6 / 25.5	14.5 / 25.2	14.2 / 24.3	14.4 / 25.0
(Black-box)	T-Po-M-DI2	20.4 / 33.9	20.0 / 32.5	19.2 / 30.8	19.9 / 32.4
	P-T-DI2++	56.0 / 60.3	56.5 / 58.1	45.5 / 51.7	52.7 / 56.7
4.4 Attacking Adversarially Trained Defenses
Adversarially trained defenses are shown to effectively withstand the adversarial examples. There-
fore, in this experiment, we consider all nine models, which are introduced in Sec. 4.1. To con-
duct the experiment, we select one of the ensemble adversarial training models (e.g. Inc-v3ens4)
as the hold-out model to evaluate the transferability, and then craft adversarial examples with an
ensemble of the rest eight models with the weight of each model 1/8. Here we compare four
stronger FGSM-based attacks including T-DI2-FGSM, T-M-DI2-FGSM, T-Po-M-DI2-FGSM, P-T-
DI2-FGSM++ with our FGS2M variants.
As demonstrated in Table 2, regardless of the attacks are white-box or black-box, our methods gener-
ally surpass the vanilla FGSM-based methods. Specifically, in the white-box manner, FGS2M-based
attacks, on average, outperform FGSM-based ones by 14.3% (Ensemble) and 13.2% (AoE). This
again demonstrates that our method can effectively alleviate the poor gradient estimation problem
caused by SM. Besides, even under the more challenging black-box attack manner, our proposed
attacks, on average, significantly improves the transferability by an increase of 11.2% (Hold-out).
Notably, compared with T-DI2-FGSM, which only successfully transfers 10.4% adversarial exam-
ples to IncRes-v2ens, our T-DI2-FGS2M achieves an higher transferability, i.e., 29.0%, which is
about 3×.
5 Conclusion
In this paper, we rethink the limitation of Sign Method (SM) applied by state-of-the-art transfer-
based attacks and experimentally demonstrate that it causes poor gradient estimation. To address
this issue, we propose a simple but effective Staircase Sign Method (S2M) to boost transferability.
With the help of staircase weights, our methods effectively fool both white-box models and black-
box models. Extensive experiments on the ImageNet dataset demonstrate the effectiveness of our
FGS2M-based attacks, which significantly improves the transferability by 5.1% for normally trained
models and 11.2% for adversarially trained defenses on average. Therefore, we hope our method
can serve as an effective baseline to boost adversarial attacks and evaluate the robustness of various
deep neural networks.
9
Under review as a conference paper at ICLR 2022
References
Maksym Andriushchenko, Francesco Croce, Nicolas Flammarion, and Matthias Hein. Square at-
tack: A query-efficient black-box adversarial attack via random search. In Andrea Vedaldi, Horst
Bischof, Thomas Brox, and Jan-Michael Frahm (eds.), ECCV, 2020.
Battista Biggio, Igino Corona, Davide Maiorca, Blaine Nelson, Nedim Srndic, Pavel Laskov, Gior-
gio Giacinto, and Fabio Roli. Evasion attacks against machine learning at test time. CoRR,
abs/1708.06131, 2017.
Nicholas Carlini and David A. Wagner. Towards evaluating the robustness of neural networks. In
Symposium on Security and Privacy, 2017.
Pin-Yu Chen, Huan Zhang, Yash Sharma, Jinfeng Yi, and Cho-Jui Hsieh. ZOO: zeroth order opti-
mization based black-box attacks to deep neural networks without training substitute models. In
Bhavani M. Thuraisingham, Battista Biggio, David Mandell Freeman, Brad Miller, and Arunesh
Sinha (eds.), AISec@CCS, 2017.
Yinpeng Dong, Fangzhou Liao, Tianyu Pang, Hang Su, Jun Zhu, Xiaolin Hu, and Jianguo Li. Boost-
ing adversarial attacks with momentum. In CVPR, 2018.
Yinpeng Dong, Tianyu Pang, Hang Su, and Jun Zhu. Evading defenses to transferable adversarial
examples by translation-invariant attacks. In CVPR, 2019.
Dziugaite, Gintare Karolina, Zoubin Ghahramani, and Daniel M. Roy. A study of the effect of jpg
compression on adversarial images. CoRR, abs/1608.00853, 2016.
Alexei A. Efros and William T. Freeman. Image quilting for texture synthesis and transfer. In
SIGGRAPH, 2001.
Lianli Gao, Qilong Zhang, Jingkuan Song, Xianglong Liu, and Hengtao Shen. Patch-wise attack for
fooling deep neural network. In ECCV, 2020a.
Lianli Gao, Qilong Zhang, Jingkuan Song, and Heng Tao Shen. Patch-wise++ perturbation for
adversarial targeted attacks. CoRR, abs/2012.15503, 2020b.
Lianli Gao, Yaya Cheng, Qilong Zhang, Xing Xu, and Jingkuan Song. Feature space targeted attacks
by statistic alignment. In IJCAI, 2021.
Ian J. Goodfellow, Jonathon Shlens, and Christian Szegedy. Explaining and harnessing adversarial
examples. In Yoshua Bengio and Yann LeCun (eds.), ICLR, 2015.
ChUan Guo, Mayank Rana, MoUstaPha Cisse, and LaUrens van der Maaten. Countering adversarial
images using input transformations. In ICLR, 2018.
Kaiming He, XiangyU Zhang, Shaoqing Ren, and Jian SUn. DeeP residUal learning for image recog-
nition. In CVPR, 2016.
Gao HUang, ZhUang LiU, LaUrens van der Maaten, and Kilian Q. Weinberger. Densely connected
convolUtional networks. In CVPR, 2017.
Andrew Ilyas, Logan Engstrom, Anish Athalye, and Jessy Lin. Black-box adversarial attacks with
limited qUeries and information. In Jennifer G. Dy and Andreas KraUse (eds.), ICML, 2018.
Nathan Inkawhich, Wei Wen, Hai (Helen) Li, and Yiran Chen. FeatUre sPace PertUrbations yield
more transferable adversarial examPles. In CVPR, 2019.
Nathan Inkawhich, Kevin J. Liang, Lawrence Carin, and Yiran Chen. Transferable PertUrbations of
deeP featUre distribUtions. In ICLR. OPenReview.net, 2020a.
Nathan Inkawhich, Kevin J. Liang, BinghUi Wang, Matthew Inkawhich, Lawrence Carin, and Yiran
Chen. PertUrbing across the featUre hierarchy to imProve standard and strict blackbox attack
transferability. In HUgo Larochelle, Marc’AUrelio Ranzato, Raia Hadsell, Maria-Florina Balcan,
and HsUan-Tien Lin (eds.), NeurIPS, 2020b.
10
Under review as a conference paper at ICLR 2022
Alexey Kurakin, Ian J. Goodfellow, and Samy Bengio. Adversarial machine learning at scale. In
ICLR, 2017a.
Alexey Kurakin, Ian J. Goodfellow, and Samy Bengio. Adversarial examples in the physical world.
In ICLR, 2017b.
Maosen Li, Cheng Deng, Tengjiao Li, Junchi Yan, Xinbo Gao, and Heng Huang. Towards transfer-
able targeted attack. In CVPR, 2020.
Jiadong Lin, Chuanbiao Song, Kun He, Liwei Wang, and John E. Hopcroft. Nesterov accelerated
gradient and scale invariance for adversarial attacks. In ICLR, 2020a.
Jiadong Lin, Chuanbiao Song, Kun He, Liwei Wang, and John E. Hopcroft. Nesterov accelerated
gradient and scale invariance for adversarial attacks. In ICLR, 2020b.
Yanpei Liu, Xinyun Chen, Chang Liu, and Dawn Song. Delving into transferable adversarial exam-
ples and black-box attacks. In ICLR, 2017.
Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and Adrian Vladu.
Towards deep learning models resistant to adversarial attacks. In ICLR, 2018.
Seyed-Mohsen Moosavi-Dezfooli, Alhussein Fawzi, and Pascal Frossard. Deepfool: A simple and
accurate method to fool deep neural networks. In CVPR, 2016.
Seyed-Mohsen Moosavi-Dezfooli, Alhussein Fawzi, Omar Fawzi, and Pascal Frossard. Universal
adversarial perturbations. In CVPR, 2017.
Muzammal Naseer, Salman H. Khan, Muhammad Haris Khan, Fahad Shahbaz Khan, and Fatih
Porikli. Cross-domain transferability of adversarial perturbations. In Hanna M. Wallach, Hugo
Larochelle, Alina Beygelzimer, Florence d,Alche-Buc, Emily B. Fox, and Roman Garnett (eds.),
NeurPIS, 2019.
Muzammal Naseer, Salman Khan, Munawar Hayat, Fahad Shahbaz Khan, and Fatih Porikli. On
generating transferable targeted perturbations. CoRR, abs/2103.14641, 2021.
Leonid I Rudin, Stanley Osher, and Emad Fatemi. Nonlinear total variation based noise removal
algorithms. Physica D: nonlinear phenomena, 60(1-4):259-268,1992.
Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng
Huang, Andrej Karpathy, Aditya Khosla, Michael S. Bernstein, Alexander C. Berg, and Fei-Fei
Li. Imagenet large scale visual recognition challenge. IJCV, 2015.
Sara Sabour, Yanshuai Cao, Fartash Faghri, and David J. Fleet. Adversarial manipulation of deep
representations. In ICLR, 2016.
Mahmood Sharif, Sruti Bhagavatula, Lujo Bauer, and Michael K. Reiter. Accessorize to a crime:
Real and stealthy attacks on state-of-the-art face recognition. In SIGSAC, 2016.
Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian J. Good-
fellow, and Rob Fergus. Intriguing properties of neural networks. In Yoshua Bengio and Yann
LeCun (eds.), ICLR, 2014.
Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jonathon Shlens, and Zbigniew Wojna. Re-
thinking the inception architecture for computer vision. In CVPR, 2016.
Christian Szegedy, Sergey Ioffe, Vincent Vanhoucke, and Alexander A. Alemi. Inception-v4,
inception-resnet and the impact of residual connections on learning. In AAAI, 2017.
Rajkumar Theagarajan, Ming Chen, Bir Bhanu, and Jing Zhang. Shieldnets: Defending against
adversarial attacks using probabilistic adversarial robustness. In CVPR, 2019.
Florian Tramer, Alexey Kurakin, NicoIas Papernot, Ian J. Goodfellow, Dan Boneh, and Patrick D.
McDaniel. Ensemble adversarial training: attacks and defenses. In ICLR, 2018.
11
Under review as a conference paper at ICLR 2022
Dongxian Wu, Yisen Wang, Shu-Tao Xia, James Bailey, and Xingjun Ma. Skip connections matter:
On the transferability of adversarial examples generated with resnets. In ICLR, 2020.
Cihang Xie, Yuxin Wu, Laurens van der Maaten, Alan L. Yuille, and Kaiming He. Feature denoising
for improving adversarial robustness. In CVPR, 2019a.
Cihang Xie, Zhishuai Zhang, Yuyin Zhou, Song Bai, Jianyu Wang, Zhou Ren, and Alan L. Yuille.
Improving transferability of adversarial examples with input diversity. In CVPR, 2019b.
Kaidi Xu, Gaoyuan Zhang, Sijia Liu, Quanfu Fan, Mengshu Sun, Hongge Chen, Pin-Yu Chen,
Yanzhi Wang, and Xue Lin. Adversarial t-shirt! evading person detectors in a physical world. In
Andrea Vedaldi, Horst Bischof, Thomas Brox, and Jan-Michael Frahm (eds.), ECCV, 2020.
Chaoning Zhang, Philipp Benz, Tooba Imtiaz, and In So Kweon. Understanding adversarial exam-
ples from the mutual influence of images and perturbations. In CVPR, 2020.
Zhengyu Zhao, Zhuoran Liu, and Martha A. Larson. On success and simplicity: A second look at
transferable targeted attacks. CoRR, abs/2012.11207, 2020.
12
Under review as a conference paper at ICLR 2022
A Appendix
A.1 Setup
Parameters: For targeted attacks, we adopt the same parameter setting in our paper. For non-
targeted attacks, here we following the previous works Gao et al. (2020a); Lin et al. (2020b). In
our experiments, the maximum perturbation is set to 16, the iteration T of all methods is 10, and
thus the step size α = e/T = 1.6. For MI-FGSM, the decay factor μ = 1.0. For DI-FGSM, the
transformation probability p = 0.7. For TI-FGSM, the Gaussian kernel length is 15 × 15. For
PI-FGSM, the amplification factor β = 5, the project factor γ = 1.0, and the project kernel length
is 3 × 3. For SI-FGSM, the number of scale copies m = 5. For our S2M, the number of staircase
K is set to 64. Please note that the parameters of each method are fixed no matter what methods are
combined.
B EXPERIMENTS FOR TARGETED ATTACKS
B.1 THE EFFECT OF STAIRCASE NUMBER K
Here we show the experimental results of white-box attacks, i.e., Figure 5 for AoE and Figure 6
for Ensemble. In this section, we analyze the effect of the staircase number K for state-of-the-art
FGSM-based attacks. Here we tune K = 2, 4, 8, ..., 256. Similar to the observation in Sec. 4.2, our
FGS2M variants can also improve the success rates by a large margin even when K = 2 and the
success rate continues to increase and then remain stable after K exceeds 64.
(求)"s∙l SSSUnSiies< p"*j"Eel
2	4	8	16 32 64 128 256	1	2	4	8	16 32 64 128 256	1	2	4	8	16 32 64 128 256
Staircase number K	Staircase number K	Staircase number K
(a)	(b)	(c)
Figure 5:	The success rate (%) of targeted white-box attack (AoE) for different methods w.r.t stair-
case number K (K = 1 denotes SM and K ≥ 2 denotes S2M). For (a) and (b), the adversarial
examples are crafted via an ensemble of InC-V4, InCReS-V2 and Res-152, and the hold-out model
is Inc-v3. For (c), the white-box models are an ensemble of Inc-v3, Inc-v4, IncRes-v2, Res-152,
Res-101, Res-50,Inc-v3ens4 and InCReS-V2ens, and the hold-out model is InC-V3ens3.
C Experiments for Non-targeted Attacks
Due to space limitation, we mainly discuss the more challenging targeted attacks in our paper. Since
the poor gradient estimation problem is caused by SM, crafting adversarial perturbations by our
proposed S2M can also boost non-targeted attacks. In this section, we report our experimental
results to demonstrate the effectiveness of our methods.
C.1	Staircase Sign Method for Non-targeted Attacks
In this section, we compare six FGSM-based attacks including I-FGSM, MI-FGSM, DI2-FGSM, TI-
FGSM, SI-FGSM and PI-FGSM with our FGS2M variants. In this experiment, we study nine models
introduced in Sec. A.1. Since non-targeted attacks are less challenging than targeted attacks, here we
craft adversarial examples via one model instead of an ensemble of models. More Specifically, we
consider four well-known normally trained models including Inc-v3, Inc-v4, IncRes-v2 and Res-152
as our substitute model.
13
Under review as a conference paper at ICLR 2022
9b96m92
(％)」SsUUnSew PEeI
1	2	4	8	16 32 64 128 256
Staircase number K
(a)
9b96m929°
(％)」SsUUnSiew PEeI
1	2	4	8	16 32 64 128 256
Staircase number K
(b)
Ooooo
9 S1 7 6 5
(％)""」Ss"UUnSiew P""EeI
1	2	4	8	16 32 64 128 256
Staircase number K
(O
Figure 6:	The success rate (%) of targeted white-box attack (Ensemble) for different methods w.r.t
staircase number K (K = 1 denotes SM and K ≥ 2 denotes S2M). For (a) and (b), the adversarial
examples are crafted via an ensemble of InC-V4, InCReS-V2 and Res-152. For (c), the white-box
models are an ensemble of Inc-v3, Inc-v4, IncRes-v2, Res-152, Res-101, Res-50, InC-V3ens4 and
InCReS-V2ens.
As demonstrated in Table 3, Table 4, Table 5 and Table 6, our FGS2M-based attacks consistently
outperform FGSM-based ones in both the white-box and black-box manner. For the black-box man-
ner, we improve the significantly transferability by 8.2% on average. Furthermore, when adversarial
examples are crafted via IncRes-v2 by SI-FGS2M, we can remarkably transfer an extra 19.1% ad-
versarial examples to InC-V3ens3. For the white-box manner, we observe that our FGS2M variants
further increase the success rate of white-box attack toward 100%. As demonstrated in Table 3, I-
FGSM only successfully attacks Inc-v3 with a 99.2% success rate. But with the help of our staircase
weights, our I-FGS2M effectively fool Inc-v3 on all the images, i.e., 100%.
	Attacks	Inc-v3*	Inc-v4	IncRes-v2	Res-152	Res-101	Res-50	InC-V3 ens 3	InC-V3ens4	InCReS-v2ens	Avg.
	I	99.2/100.0	30.0 / 40.6	21.5/ 34.9	18.9/ 26.6	20.8/ 29.6	23.3 / 32.6	12.1 /16.0	12.1 /17.3	4.9 / 8.4	18.0/ 25.8
	-MI-	99.2/100.0	55.7 / 57.5	51.2/ 55.1	44.0 / 44.0	44.6 / 46.9	49.9/ 51.8	21.9 / 22.7	20.7 / 23.1	-11.0/11.6	37.4/ 39.1
Inc-v3	-_DI	99.9 /100.0	52.5 / 67.0	42.5/ 57.8	32.4 / 42.9	36.0 / 48.4	41.4 / 52.4	13.9/ 22.0	14.6 / 22.7	-6.9 /11.6-	30.0 / 40.6
	TI	99.1 /100.0	27.5 /35.9	14.0/ 24.5	16.3/ 23.0	17.9/ 25.6	22.1 / 28.1	17.8/ 28.0	16.5/ 26.6	-10.4/16.9	17.8/ 26.1
	SI	100.0 / 100.0	53.8 / 69.3	47.2/ 64.9	39.4 / 53.5	45.3/ 58.9	48.7 / 61.3	21.7 /33.4	22.6 / 37.2	-10.8/ 20.1	36.2 / 49.8
	PI	100.0 / 100.0	54.5 / 62.9	47.4/ 55.9	39.7 / 47.6	43.0/ 48.7	48.2/ 52.1	26.3 / 31.3	25.4/ 29.0	-15.5/19.2	37.5 / 43.3
Table 3: The success rate (%) of non-targeted FGSM-based/FGS2M-based attacks w.r.t adversarial
examples crafted via Inc-v3. We study nine models—Inc-v3, Inc-v4, IncRes-v2, Res-152, Res-101,
Res-50, Inc-v3ens3, Inc-v3ens4 and InCReS-v2ens here.
	Attacks	Inc-v3	Inc-v4*	IncRes-v2	Res-152	Res-101	Res-50	InC-V3 ens 3	InC-V3ens4	InCReS-v2ens	Avg.
	I	43.2/ 56.9	99.2 /100.0	26.3 / 39.2	25.2 / 34.6	25.9/ 36.1	30.9 / 39.8	12.0/16.9	12.6/19.0	6.4 /10.5	22.8 / 31.6
	-MI-	70.8/ 73.1	99.2 /100.0	58.1 / 60.4	52.2 / 53.5	53.8/ 54.8	56.6 / 59.1	23.8/ 26.0	23.8/ 25.0	-12.7/13.9	44.0 / 45.7
Inc-v4	DI-	64.5 / 75.3	99.1 /100.0	48.3/ 63.5	38.6 / 48.5	40.0/ 50.4	44.3 / 54.5	16.0/ 21.5	16.3/ 22.9	-8.6/13.7-	34.6 / 43.8
	TI	36.8 / 46.6	99.2 /100.0	16.8/ 27.9	20.8 / 27.9	18.9/ 26.9	22.3 / 31.9	16.2/ 26.8	19.8/ 27.7	-11.6/17.9	20.4 / 29.2
	SI	72.0 / 81.7	100.0 /100.0	57.0/ 71.7	51.1/ 62.3	52.1 / 64.2	56.7 / 68.6	26.6 / 43.7	28.0 / 45.4	-16.9/ 29.8	45.1 / 58.4
	PI	68.7/ 74.9	100.0 /100.0	51.4/ 60.2	45.4 / 53.3	44.5 / 52.8	52.2 / 57.7	28.2/35.4	27.8 / 33.6	-19.7/ 23.6	42.2 / 48.9
Table 4: The success rate (%) of non-targeted FGSM-based/FGS2M-based attacks w.r.t adversarial
examples crafted via Inc-v4. We study nine models—Inc-v3, Inc-v4, IncRes-v2, Res-152, Res-101,
Res-50, Inc-v3ens3, Inc-v3ens4 and InCReS-v2ens here.
14
Under review as a conference paper at ICLR 2022
	Attacks	Inc-v3	Inc-v4	IncRes-v2*	Res-152	Res-101	Res-50	Inc-v3ens3	Inc-v3ens4	IncRes-v2ens	Avg.
	I	46.7 / 59.5	38.2 / 49.0	99.2 / 100.0	25.4 / 36.5	28.2/ 39.9	30.7 / 42.1	13.2 / 21.4	13.0 / 19.5	8.3 / 15.2	25.5 / 35.4
	MI	76.1 / 75.8	67.9 / 68.8	99.2 / 100.0	57.6 / 56.3	57.9/ 58.9	61.3 / 63.3	32.0 / 34.7	28.4 / 28.8	20.5 / 22.0	50.2 / 51.1
IncRes-v2	DI	71.4 / 79.5	65.3 / 76.6	98.5 / 99.7	47.8 / 58.3	49.6/ 59.8	54.38 / 64.7	19.5 / 31.0	19.1 / 28.1	12.2 / 22.6	42.5 / 52.6
	TI	43.5 / 52.2	41.2 / 47.8	98.8 / 99.9	26.3 / 31.4	28.6/ 34.5	30.2 / 38.0	26.7 / 35.6	24.7 / 36.4	20.8 / 32.4	30.3 / 38.5
	SI	74.0 / 83.9	64.2 / 75.7	99.9 / 99.9	51.7 / 66.1	52.9/ 66.5	60.5 / 73.3	29.6 / 48.7	28.8 / 44.3	22.1 / 40.7	48.0 / 62.4
	PI	72.6 / 79.0	64.1 / 72.8	100.0 / 100.0	52.2 / 59.0	53.8/ 61.2	56.9 / 63.9	34.3 / 43.4	30.9 / 38.5	25.6 / 33.2	48.8 / 56.4
Table 5: The success rate (%) of non-targeted FGSM-based/FGS2M-based attacks w.r.t adversarial
examples crafted via IncRes-v2. We study nine models—Inc-v3, Inc-v4, IncRes-v2, Res-152, Res-
101, Res-50, Inc-v3ens3 , Inc-v3ens4 and IncRes-v2ens here.
	Attacks	Inc-v3	Inc-v4	IncRes-v2	Res-152*	Res-101	Res-50	Inc-v3ens3	Inc-v3ens4	IncRes-v2ens	Avg.
	I	31.3 / 43.8	25.9 / 35.6	17.7 / 31.6	98.7 / 99.5	67.3 / 80.8	66.1 / 78.0	12.2 / 17.7	13.3 / 19.4	7.6 / 12.6	30.2 / 39.9
	MI	55.9 / 59.5	50.0 / 52.2	45.9 / 50.3	98.7 / 99.5	85.2/ 88.0	83.3 / 87.6	26.9 / 29.7	25.7 / 26.8	15.3 / 16.4	48.5 / 51.3
IncRes-v2	DI	60.6 / 74.0	56.5 / 68.2	51.0 / 65.7	98.4 / 99.6	86.8 / 93.6	84.2 / 92.0	21.2 / 33.6	20.1 / 31.9	13.0 / 21.6	49.2 / 60.1
	TI	25.5 / 32.6	21.9 / 28.2	11.0 / 19.1	98.2 / 99.2	53.3/ 62.8	48.2 / 56.3	18.4 / 26.2	18.7 / 25.9	12.6 / 20.0	26.2 / 33.9
	SI	43.6 / 56.5	40.3 / 50.5	32.4 / 47.1	99.8 / 99.8	84.7/ 91.6	83.7 / 89.9	19.0 / 33.0	18.9 / 31.4	12.5 / 22.4	41.9 / 52.8
	PI	57.5 / 63.9	50.3 / 57.8	47.4 / 55.0	99.6 / 99.7	82.7 / 90.6	81.8 / 87.9	31.7 / 38.1	29.5 / 37.4	21.2 / 27.1	50.3 / 57.2
Table 6: The success rate (%) of non-targeted FGSM-based/FGS2M-based attacks w.r.t adversarial
examples crafted via Res-152. We study nine models—Inc-v3, Inc-v4, IncRes-v2, Res-152, Res-
101, Res-50, Inc-v3ens3 , Inc-v3ens4 and IncRes-v2ens here.
15