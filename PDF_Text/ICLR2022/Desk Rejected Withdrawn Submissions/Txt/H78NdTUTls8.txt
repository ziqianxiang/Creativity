Under review as a conference paper at ICLR 2022
A precortical module for robust CNNs to
LIGHT VARIATIONS
Anonymous authors
Paper under double-blind review
Ab stract
We present a simple mathematical model for the mammalian low visual pathway,
taking into account its key elements: retina, lateral geniculate nucleus (LGN), pri-
mary visual cortex (V1). The analogies between the cortical level of the visual
system and the structure of popular CNNs, used in image classification tasks, sug-
gests the introduction of an additional preliminary convolutional module inspired
to precortical neuronal circuits to improve robustness with respect to global light
intensity and contrast variations in the input images. We validate our hypothesis
on the popular databases MNIST, FashionMNIST and SVHN, obtaining signifi-
cantly more robust CNNs with respect to these variations, once such extra module
is added.
1	Introduction
The fascinating similarities between CNN architectures and the modeling of the mammalian low
visual pathway are a current active object of investigation (see N. Montobbio (2019), Bertoni et al.
(2019), Ecker et al. (2019) and refs. therein). Historically, the study of border and visual perception
started around 1920's with the Gestalt psychology formalization (Pragnanz laws) of the perception
(lines, colors and contours, see Mather (2006) and refs. therein). Then, the foundational work
(Hubel & Wiesel, 1959) introduced a more scientific approach to the subject, defining the concept
of receptive field, simple and complex cells, together with an anatomically sound description of the
visual cortex in mammals.
This study paved the way to the mathematical modeling for such structures. In particular special
attention was given to the primary visual cortex V1 (see W.C.Hoffmann (1989)), whose orientation
sensitive simple cells, together with the complex and hypercomplex cells, inspired the modeling
of algorithms (Olshausen & Field, 1996) contibuting to the spectacular success of Deep Learning
CNNs (LeCun et al., 1998). With a more geometric approach in (Bressloff & Cowan, 2003), the
introduction on V1 of a natural subriemannian metric (J. Petitot, 1999; G. Citti, 2006), following the
seminal work (Mumford, 1994), led to interesting interpretations of the border completion mecha-
nism.
While the mathematical modeling became more sophisticated (see Petitot (2017) and refs therein),
the insight into the physiological functioning of the visual pathway led to more effective algorithms
in image analysis (Franken & Duits, 2009; Bertoni et al., 2019). In particular, the Cartan geometric
point of view on the V1 modeling (Petitot, 2017), fueled new interest and suggests a physiolog-
ical counterpart for the new algorithms based on group equivariance in geometric deep learning
approaches (see Bekkers (2020); Cohen & Welling (2016) and refs therein).
The purpose of our paper is to provide a simple mathematical model for the low visual pathway,
comprehending the retina, the lateral geniculate nucleus (LGN) and the primary visual cortex (V1)
and to use such model to construct a preliminary convolutional module, that we call precortical to
enhance the robustness of popular CNNs for image classification tasks. We want the CNN to gain the
outstanding ability of the human eye to react to large variations in global light intensity and contrast.
This can be achieved by mimicking the gain tuning effect implemented the precortical portion of the
mammalian visual pathway. This effect consists in the following: since the low visual circuit needs
to respond to a vast range of light stimuli, that spans over 10 orders of magnitude, it is equipped
with a lateral inhibition mechanism which allows a low latency and a high sensitivity response. This
mechanism is functionally embedded in the center-surround receptive fields of retinal bipolar cells,
1
Under review as a conference paper at ICLR 2022
retinal ganglions and LGN neurons and is able to achieve both border enhancing and decorrelation
between the perceived light intensity in single pixels and the mean light value in a given image
Kandel et al. (2012). We will show that, if in the early levels of a CNN such receptive fields are
learned in the form of convolutional filters, there is a considerable improvement in accuracy, when
considering dimmed light or low contrast examples, i.e. examples not belonging to the statistic of
the training set.
The organization of this paper is as follows. We start with a simple mathematical formalization of
the low visual pathway, which accounts for its key components. Though this material is not novel,
we believe our terse presentation can greatly help to elucidate the relation between mathematical
entities, like bundles or vector fields and the local physiological structure of retina and V1, together
with their functioning.
The similarities between the inner structure of CNNs and the physiological visual perception mech-
anism, once appropriately mathematically modeled as above, show that popular CNNs structure, for
image classification tasks, do not take into full consideration the role of the precortical structures,
which are responsible for a correct adjustment to global light intensity and contrast in an image. Our
observations then, suggest the introduction of a precortical module (see also N. Montobbio (2019);
Bertoni et al. (2019)), which mimics the functioning of retina and LGN cells and reacts appropri-
ately to the variations of the light intensity and the contrast. Once such module is introduced, we
verify in our experiments on the MNIST, FashionMNIST and SVHM databases that our CNN shows
robustness with respect to such large variations.
The impact and potential of our approach is that a new simplified, but accurate, mathematical mod-
eling of the low visual pathway, can lead to key cues on algorithm design, which add robustness
and allow high performances beyond the type of data the algorithm is trained with, exactly as it
happens for the human visual perception. In a forthcoming research, we plan to further explore this
study by analyzing the autoencoder performances in the border completion, using the mathematical
description via subriemannian metric geodesic.
2	Related work
The structure of the mammalian visual pathway was extensively explored in the last century both
from an anatomical and a functional point of view (see De Moraes (2013) and Kandel et al. (2012)
and refs therein). New aspects, however, are still discovered nowadays at each level of its structure:
from new cellular types (Patterson et al., 2020) to new functional circuits (Keller et al., 2020),
shedding more light on the formal process of visual information encoding (see Rossi et al. (2020),
Wang et al. (2020), Roy S. (2021)). The striking correspondence between the training of mammals
visual system and popular CNN training was elucidated in Achille et al. (2017), suggesting more
exploration into this direction is necessary. Similarities between deep CNN structure and the human
visual pathway have been then increasingly explored in this framework. From the first appearance of
the neocognitron (Fukushima, 2004), the feature extracting action of the convolutional module has
been widely adopted to tackle a number of visual tasks, developing models which presented striking
analogies with mammalian cellular subtypes and their receptive fields (LeCun et al., 1998; Hinton
et al., 2011; Bertoni et al., 2019; N. Montobbio, 2019). These similarities were studied, in particular,
in relation to the deeper components of the visual pathway, starting from the hypercolumnar structure
of the primary visual cortex V1 and continuing with superior processing centers, like V4 and the
inferior temporal cortex (Mohsenzadeh et al. (2020) and the references therein). Not the same
interest, on the other hand, was given to the exploration of similarities with the first stages of visual
perception and in particular to the receptive field structure of retinal and geniculate units or with their
precise regularizing effect on the perceived image. An recent attempt in this direction is in Bertoni
et al. (2019), where a biologically inspired CNN is studied in connection with the Retinex model
(Land, 1977), elucidated mathematically in Provenzi et al. (2005) and then implemented (Morel
et al., 2010) (see also refs. therein).
2
Under review as a conference paper at ICLR 2022
3	The visual pathway
The visual pathway consists schematically of the following structures: the eye, the optic nerve, the
lateral geniculates nucleus (LGN), the optic radiation and the primary visual cortex (V1). In our
discussion, we focus only on the retina, LGN and V1, because these are the parts in which the
processing of the signal (i.e. the images) requires a more articulate modeling.
The retina consists of photoreceptor cells, called receptors, which measure the intensity of light. We
model part of the retina, the hemiretinal receptoral layer, with a compact simply connected domain
E in R2 . We take a portion and not the whole retina, because we want to avoid to cross the median
cleft line in the visual field, which is interpreted and processed in a more complex way (see Adams
& Horton (2003)).
We define then a receptorial activation field as a function
R : E -→ R
(x, y)	7→	R(x, y)
(1)
associating to each point (x, y), corresponding to a receptor in the retina, an activation rate given by
the scalar R(x, y). We do not assume Rto be continuous, however, by its physiological significance,
it will be bounded.
We may then interpret R as coming from an im-
age in the visual field. We will not distinguish
between on and off receptors, as their final effect
on the downstream neurons is the same from a
logical point of view. We assume R(x, y) to be
proportional to the intensity of the light falling
on (x, y). The ganglionic layer E (see Fig. 1)
sits a few layers down the visual pathway.
To each receptor (x, y) in E there is a corre-
sponding ganglion (x0 , y0) in E with its recep-
tive field centered in (x0, y0) so that we have
a natural distance preserving identification be-
tween E and E, given by an isometry G : E -→
~
Ee.
Figure 1: Schematic structure of receptorial and
ganglionic layers in retina
The ganglionic activation pattern, however, is
quite different from the hemiretinal receptors
one (1):
〜〜	〜	C
二 二	_ 二，， ，、	I	一一/	X — Z	X ,	.一	一 ,	、	, , 八
R : E -→ R, R(x0, y0) =	K(u, v)R(u, v) dudv with G(x, y) = (x0, y0)
Uρ(x,y)	(2)
where
UP(χ,y) = {(u, V) ∈ R2 ： (u - χ)2 + (v - y)2 ≤ P},
，±1 if (U — x)2 + (v — y)2 ≤ (ρ — e)2
K(x, y) =
、干1	if	(ρ	— e)2	<	(U	— x)2	+	(v	— y)2 ≤	ρ2
The identification between E and E given by G is not a manifold morphism: in fact the corre-
spondence between functions follows (4), which is an integral transform with kernel K(x, y). This
models effectively the mechanism of firing of hemiretinal receptors: for each activation disc we al-
ways have an inhibition crown around it. This is the key mechanism, responsible for the border and
contrast enhancement, that we shall implement with our precortical module in Sec. 5.
We notice an important consequence, whose proof is in App. A.
Proposition 3.1. The hemiretinal ganglionic activation field R is Lipschitz continuous in both vari-
ables on E.
3
Under review as a conference paper at ICLR 2022
This proposition encodes the fact that the visual system is able to reconstruct a border percept also
for non continuous images. This fact was already noticed in Mumford (1994), where edge inter-
ruptions are taken into account and defined as cusps or elementary catastrophes. We illustrate this
phenomenon in Fig. 2, which is perceived as a unitary curvilinear shape with clear borders, though
composed of isolated black dots. There is indeed a further smoothing reconstruction carried out in
the LGN, so that our image will provide us with a smooth function in place of R, (Citti G., 2019).
We shall not describe such modeling for the present work.
Figure 2: Border percepts for
a non continuous image
We now come to the last portion of the visual pathway: the primary visual hemicortex, that we shall
still denote with V 1. The retinotopic map is a distance preserving homeomorphism between the
hemiretinal receptorial layer and the primary visual hemicortex (see Adams & Horton (2003) for a
concrete realization). We can therefore identify V 1 with a compact domain V in R2 .
The pointwise identification, however, is not sufficient to
model the behaviour of V 1. In fact for each point of the visual
cortex we have three main information to take into account:
1.	The absolute position of the corresponding point in
the retina, which receives a stimulus;
2.	the orientation θ of some perceived edge through the
point (simple and complex cells);
3.	the curvature k of some perceived edge through the
point (hypercomplex cells).
While the first feature is encoded by the points in the domain
V , additional modelization is needed for the construction of
orientation and curvature information. For this reason, a point
in V 1 is identified with a biological structure, called hypercol-
umn, which contains a plethora of cell families, each analyzing
a specific aspect of the visual information. We assume that at each point of V 1 is present a full set of
simple, complex and hypercomplex orientation columns (a so called full orientation hypercolumn).
We briefly recap the various types of cells in V 1, since their behaviour plays a key role in our
modeling.
•	Simple cells: they exhibit a rich and multifaceted behavioural pattern in response to posi-
tional, orientational and dynamic features of a light input. Traditionally, for their modeling
is used the asymmetrical part of a Gabor filter, though in recent works more importance
is given to the role of the LGN and the actual neuroanatomical structure (see Lindeberg
(2015); Azzopardi G. (2012)).
•	Complex cells: they receive input directly from simple cells (see Hubel & Wiesel (1959))
and they show a linear response depending on the orientation of some static stimulus over
a certain receptive field. It is important to note that this response is invariant under a 1800
rotation of the stimulus.
•	Hypercomplex cells: they are also called end-stopped cells and they are maximally acti-
vated by oriented stimuli positioned in the central region of their receptive field. They are
maximally inhibited by peripheric stimuli with the same orientation. Such cells are there-
fore not particularly reactive to long straight lines, firing briskly if perceiving curves or
corners.
As we shall see in our next section, in order to take into account all different cells behaviours in the
hypercolumn, we need to model V 1 using the mathematical concept of fiber bundle.
4 The Primary Visual Cortex
We want to provide a mathematical description of V1, starting from the actual neuroanatomical
structures and taking into account the combined effects of complex and simple cells. Though
this material is mostly known (see J. Petitot (1999); G. Citti (2006); Bressloff & Cowan (2003);
4
Under review as a conference paper at ICLR 2022
W.C.Hoffmann (1989)), our novel and simplified presentation will elucidate the role of the key
components of the visual pathway, while keeping a faithful representation of the neuroanatomical
structures.
Let D ⊂ R2 be compact, simply connected. We start by defining the orientation of a function
F : D -→ R; it will be instrumental for our modelization of V 1. On the manifold D × S1 define
the vector field Z :
Z(x, y, θ) = - sinθ ∂x + cosθ ∂y
where as usual ∂x , ∂y form a basis for the tangent space of R2 at each point and θ is the coordinate
for S1. To ease the notation we drop x, y in the expression of Z, writing Z(θ) in place of Z(x, y, θ).
Definition 4.1. LetD ⊂ R2 be compact and simply connected and F : D -→ R a smooth function.
Let reg(D) ⊂ D be the subset of regular points of F (i.e. p ∈ D, dF (p) 6= (0, 0)). We define the
orientation of F as:
Θ : reg(E0)	-→ S1
(x, y)	7-→ Θ(x,y) := argmaxθ∈S1 Z(θ)F(x,y)
(3)
The map Θ is well defined because of the following proposition, see App. B.
Proposition 4.2. Let F : D -→ R as above and (x0, y0) ∈ D a regular point for F. Then, we have
the following:
1.	There there exists a unique θx0,y0 ∈ S1 for which the function ζx,y : S1 -→ R, ζx,y (θ) :=
Z(θ) F (x, y) attains its maximum.
2.	The map Θ : reg(D) -→ S1, Θ(x, y) = θx,y is well defined and differentiable.
3.	The set:
Φ = {(x, y, Θ(x, y)) ∈ D × S1 : Θ(x, y) = θx,y}
is a regular submanifold of D × S1.
Notice the following important facts:
•	the locality of the operator Z(θ) mirrors the locality of the hypercolumnar anatomical con-
nections;
•	its operating principle is a good description of the combined action of simple and complex
cells (though different from their individual behaviour);
We now look at an example given by Fig. 3 of the behaviour of Z on an image, that we view as a
function F : D -→ R.
Here D = E, that is F is a hemιretιnal ganglionic receptive field.
We can see in our example that for each point (x, y) near the border of the dark circle represented by
the image F, there exists a value θ(x, y) for which the function Z(θ)F(x, y) is maximal (we color
in white the maximum, in black the minimum). This value is indeed the angle between the tangent
line to a visually perceived orientation in (x, y) and the x axis.
Since in V 1 at each point we need to consider all possible orientations, coming from different
receptive fields, following J. Petitot (1999); G. Citti (2006); W.C.Hoffmann (1989), we model V 1
Figure 3： GraPhs of F(x,y)=-/21。阳 y2/。。,Z(4)F, Z(3∏)F, Z(7∏)F and Z(5∏)F.
5
Under review as a conference paper at ICLR 2022
as the fiber bundle E := V × S 1 -→ V , that we call the orientation bundle. Notice that V is a
domain D ⊂ R2 and naturally identified with E and E with a distance preserving homeomorphism.
Then, each ganglionic receptive field R, after LNG smoothing will give us a smooth section Θ of E,
through the identification we detailed above and elucidated in our example.
5 Experiments
We now take into exam a popular CNN: LeNet 5 and, based on the analogies with the low visual
pathway elucidated in our previous discussion, we modify it by introducing a precortical module.
The LeNet 5 convolutional layers have surprisingly strong similarities
with the cortical directional hypercolumns: the stacked organization of
the layers closely resembles the feed forward structure of the simple -
complex - hypercomplex cell path (see Sec. 4). Also, the weights of
the first convolutional layer converge to an integral kernel (see eq. (4)),
which makes it sensitive to directional feature extraction. To allow the
emergence of the precortical gain tuning effect (see Sec. 1), we struc-
tured our model as follows. We insert at the beginning the precortical
module consisting of a sequence of convolutional, dropout and hyper-
bolic tangent activation layer, repeated three times. This number was
chosen according to De Moraes (2013) and corresponds to the three ver-
tical types of cells present in the precortical portion of the visual path
(bipolar cell, retinal ganglion, LGN neuron). The number of features
in each convolutional layer was chosen equal to the number of color
channels (one for MNIST and FashionMNIST, three for SVHN) and the
padding was set to (kernel_size-1)/2 to preserve the dimension
of the input data. The output of this module, denominated precortical
module was then fed to a slightly modified version of LeNet 5 (Fig. 4).
The number kernel_size is taken as an hyperparameter and chosen
for each dataset, according to the best performance. Finally, a modi-
fied LeNet 5 without the precortical module was trained on the same
datasets and used for accuracy comparison. The training for both CNNs,
that we denote RetiLeNet and LeNet 5 (with and without the precorti-
cal module), was carried out without any image preprocessing or data
augmentation using the ADAM optimizer, with learning rate 0.001 and
a batch size of 128.
Since we want to study the effect on robustness of the precortical mod-
ule, we tested the two models on transformed test sets. In particular, we
considered the two transformations:
1.	Mean offset (luminosity change): each pixel xi of the input
image X was shifted by an offset value μ as
Xi J Xi — μ
From a phenomenological point of view this represents a varia-
tion in the global light intensity.
2.	Distribution scaling (contrast change): each pixel Xi of a
given image is modified with a deviation parameter σ as
Figure 4: RetiLeNet
structure; n = image
color channels
-X→ X
Xi
Xi J
σ
with X the mean pixel value in the image. In this fashion We
obtain a modification in the image contrast.
5.1 Results
After training both models LeNet 5 and RetiLeNet (LeNet 5 with the precortical module) on MNIST,
FashionMNIST and SVHN, choosing the hyperparameter kernel_size equal to 7 for RetiLeNet,
6
Under review as a conference paper at ICLR 2022
Figure 5: New samples generated by contrast reduction via σ (left) and light dimming via μ (right).
MNIST	-2.0	μ 0	1.0	0.1	σ 1.0	3.9
LeNet 5	0.120	0.991	0.119	0.915	0.991	0.485
RetiLeNet 5	0.097	0.988	0.787	0.984	0.988	0.907
FashionMNIST	-2.0	μ 0	2.0	0.1	σ 1.0	3.9
Lenet 5	0.168	0.887	0.100	0.836	0.887	0.422
Mod Lenet 5	0.770	0.880	0.781	0.805	0.880	0.806
SVHN	-2.0	μ 0	2.0	0.1	σ 1.0	3.9
Lenet 5	0.806	0.868	0.006	0.723	0.868	0.776
Mod Lenet 5	0831	0.886	0.738	0.849	0.886	0.832
Table 1: Accuracies achieved with and without the PreCortiCal module versus μ
and σ sweeps. Notice: for MNIST the highest μ reported is 1.0 while it is 0.2
for the other two datasets.
we observe that in general, RetiLeNet performed better than LeNet 5 on the three datasets, as shown
in Table 1. We also notice that the addition of the precortical module did not impact greatly on the
final accuracy for the unmodified datasets, that is taking μ = 0, σ = 1, accounting for a slight score
improvement on SVHN and, surprisingly, a slight worsening on MNIST and FashionMNIST (see
1).
We notice a better performance of RetiLeNet , with respect to LeNet 5 , on the transformed dataset,
that is when We modify the contrast (via σ) and the light intensity (via μ), see Fig. 6, 7, 8. The
improvement in performance is particularly noticeable in the case of low contrast (σ = 3.9) and
very dark examples (μ = 2 for SVHN and FashionMNIST, μ=1 for MNIST), where the accuracy of
RetiLeNet is consistently greater than 75% in comparison with LeNet 5 accuracies, which are far
lower. The reason for this behaviour is the strong stabilizing effect that the first (convolutional) layer
of the precortical module has on the input image, in the analogy to the modeling for the receptive
fields introduced in (4).
To elucidate this stabilizing effect, we choose an example from each dataset and look at the corre-
sponding first hidden output, while the parameters μ and σ vary (see Fig. 9, 10, 11). As far as the μ
shift is concerned we see that this effect is very evident: for large variations of μ we have a pixel av-
erage close to zero after the first convolutional layer. In particular, we notice a correlation between a
worse stabilizing action and a worse model performance, by comparing what happens with MNIST
with respect to FashionMNIST and SVHN. Despite the small number of datasets considered, we
think it may hint to a significant phenomenon worth a further investigation.
Moreover, we can see close similarities between this effect and the actual biological lateral inhibition
phenomenon emerging at the bipolar cell level in the retina corresponding indeed to the first of our
precortical convolutional layers. This biological phenomenon is responsible for the gain adjustment
and border enhancement mechanisms which allow the visual system to respond correctly to strong
changes in ambient light conditions.
7
Under review as a conference paper at ICLR 2022
MNIST accuracy versus μ sweep	MNIST accuracy versus σ sweep
Figure 6: Accuracy for μ, σ variations in MNIST
FashionMNlST accuracy versus μ sweep
Figure 7: Accuracy for μ, σ variations in FashionMNIST
FashionMNIST accuracy versus σ sweep
1.0
0.8
0.6
0.4
SVHN accuracy versus μ sweep
SVHN accuracy versus σ sweep
1.0
0.8
0.6
0.4
LeNet_5
RetiLeNet
LeNet_5
RetiLeNet
0.0 j~~I--1----1----1----1----1---1~~OCICCG
-2.0 -1.5 -1.0 -0.5	0.0	0.5	1.0	1.5	2.0
0.0 -l.--------1--------1-------1--------1-------1--------1-------1--------Γj
0.0	0.5	1.0	1.5	2.0	2.5	3.0	3.5	4.0


Figure 8: Accuracy for μ, σ variations in SVHN
6 Conclusions
Our simple mathematical model of the low visual pathway of mammals retains the descriptive accu-
racy of more complicated models and it is better suited to elucidate the similarities between visual
physiological structures and CNNs. The precortical neuronal module, inspired by our mathematical
modeling, when added to a popular CNN (LeNet 5 ), gives a CNN RetiLeNet that mimics the border
and contrast enhancing effect as well as the mean light decorrelation action of horizontal-bipolar
cells, retinal ganglions and LGN neurons. Hence such addition, which performed extremely well
on datasets with large variations of light intensity and contrasts, improves the CNN robustness with
respect to such variations in generated input images, strongly improving its inferential power on
data not belonging to the training statistics (Fig. 6, 7, 8). We believe that this strong improvement is
directly correlated with the stabilizing action performed by the first precortical convolutional layer,
in complete analogy with the behaviour of bipolar cells in the retina. We validate our hypotheses
obtaining our results on MNIST, FashionMNIST and SVHN datasets.
8
Under review as a conference paper at ICLR 2022
Bipolar cell hidden output
MNIST example
Figure 9: Pixel value distributions before (red) and after (green) first precortical convolutional layer
for μ, σ variations in MNIST. (whiskers 1.5, violin kernel bandwidth 1.06)
Figure 10: Pixel value distributions before (red) and after (green) first precortical convolutional layer
for μ, σ variations in FashionMNIST. (whiskers 1.5, violin kernel bandwidth 1.06)
SVHN example	Bipolar cell hidden output
Figure 11: Pixel value distributions before (red) and after (green) first precortical convolutional layer
for μ, σ variations in SVHN. (whiskers 1.5, violin kernel bandwidth 1.06)
9
Under review as a conference paper at ICLR 2022
References
Alessandro Achille, Matteo Rovere, and Stefano Soatto. Critical learning periods in deep neural
networks. ArXiv, abs/1711.08856, 2017.
Daniel L. Adams and Jonathan C. Horton. A precise retinotopic map of primate striate cortex
generated from the representation of angioScotomas. 23:3771-3789, 05 2003.
Petkov N. Azzopardi G. A corf computational model of a simple cell with application to contour
detection. Perception, 41, 09 2012.
Erik J. Bekkers. B-spline cnns on lie groups. ArXiv, abs/1909.12057, 2020.
Federico Bertoni, Giovanna Citti, and Alessandro Sarti. Lgn-cnn: a biologically inspired cnn archi-
tecture. ArXiv, abs/1911.06276, 2019.
Paul Bressloff and Jack Cowan. The functional geometry of local and horizontal connections in a
model of v1. Journal of physiology, Paris, 97:221-36, 03 2003. doi: 10.1016/j.jphysparis.2003.
09.017.
Sarti A. Citti G. On the origin and nature of neurogeometry, 2019.
Taco Cohen and Max Welling. Group equivariant convolutional networks. In ICML, 2016.
Carlos Gustavo MD De Moraes. Anatomy of the visual pathways. Journal of Glaucoma, 2013. doi:
10.1097/IJG.0b013e3182934978.
Alexander S. Ecker, Fabian H Sinz, E. Froudarakis, P. Fahey, Santiago A. Cadena, Edgar Y. Walker,
Erick Cobos, Jacob Reimer, A. Tolias, and M. Bethge. A rotation-equivariant convolutional neural
network model of primary visual cortex. ArXiv, abs/1809.10504, 2019.
E. Franken and R. Duits. Crossing-preserving coherence-enhancing diffusion on invertible orienta-
tion scores. International Journal of Computer Vision, 85:253-278, 2009.
Kunihiko Fukushima. Neocognitron: A self-organizing neural network model for a mechanism of
pattern recognition unaffected by shift in position. Biological Cybernetics, 36:193-202, 2004.
A. Sarti G. Citti. A cortical based model of perceptual completion in the roto-translation space. J
Math Imaging, 24:307-326, 2006. doi: 10.1007/s10851-005-3630-2.
Geoffrey E. Hinton, A. Krizhevsky, and Sida D. Wang. Transforming auto-encoders. In ICANN,
2011.
D. H. Hubel and T. N. Wiesel. Receptive fields of single neurones in the cat’s striate cortex. J.
Physiol, 148:pp. 574-591, 1959.
Y. Tondut J. Petitot. Vers une neurogeometrie. fibrations corticales structures de contact et countours
subjectifs modaux. Mathmatiques, Informatique et Sciences humaines, 145, 1999. doi: 10.1007/
s10851-005-3630-2.
Eric R. Kandel, James H. Schwartz, and Thomas M. Jessell. Principles of neural science. 2012.
Andreas J. Keller, Mario Dipoppa, Morgane M. Roth, Matthew S. Caudill, Alessandro Ingrosso,
Kenneth D. Miller, and Massimo Scanziani. A disinhibitory circuit for contextual modula-
tion in primary visual cortex. Neuron, 108(6):1181-1193.e8, 2020. ISSN 0896-6273. doi:
https://doi.org/10.1016/j.neuron.2020.11.013. URL https://www.sciencedirect.com/
science/article/pii/S0896627320308916.
Edwin Herbert Land. The retinex theory of color vision. Scientific American, 237 6:108-28, 1977.
Y. LeCun, L. Bottou, Yoshua Bengio, and P. Haffner. Gradient-based learning applied to document
recognition. 1998.
Tony Lindeberg. Time-causal and time-recursive spatio-temporal receptive fields. Journal of Math-
ematical Imaging and Vision, 55:50-88, 2015.
10
Under review as a conference paper at ICLR 2022
G. Mather. Foundations of Perception. Psychology Press, 2006.
Yalda Mohsenzadeh, C. Mullin, B. Lahner, and A. Oliva. Emergence of visual center-periphery
spatial organization in deep convolutional neural networks. Scientific Reports, 10, 2020.
Jean-Michel Morel, Ana Belen Petro, and Catalina Sbert. A Pde formalization of retinex theory.
IEEE Transactions on Image Processing,19:2825-2837, 2010.
D. Mumford. Elastica and computer vision. Algebraic geometry and its applications, pp. 491-506,
1994.
A. Sarti N. Montobbio, G. Citti. From receptive profiles to a metric model of v1. Journal of
computational neuroscience, 46:257-277, 2019.
Bruno A. Olshausen and David J. Field. Emergence of simple-cell receptive field properties by
learning a sparse code for natural images. Nature, 381:607-609, 1996.
Sara S. Patterson, Marcus A. Mazzaferri, Andrea S. Bordt, Jolie Chang, Maureen Neitz, and Jay
Neitz. Another blue-on ganglion cell in the primate retina. Current Biology, 30(23):R1409-
R1410, 2020. ISSN 0960-9822. doi: https://doi.org/10.1016/j.cub.2020.10.010. URL https:
//www.sciencedirect.com/science/article/pii/S0960982220315141.
J. Petitot. Elements of Neurogeometry Functional Architectures of Vision. Springer, 2017.
Edoardo Provenzi, Luca De Carli, Alessandro Rizzi, and Daniele Marini. Mathematical definition
and analysis of the retinex algorithm. Journal of the Optical Society of America. A, Optics, image
science, and vision, 22 12:2613-21, 2005.
L. F. Rossi, K. Harris, and M. Carandini. Spatial connectivity matches direction selectivity in visual
cortex. Nature, 588:648 - 652, 2020.
Davis E.L. et al. Roy S., Jun N.Y. Inter-mosaic coordination of retinal receptive fields. Nature, 2021.
doi: https://doi.org/10.1038/s41586-021-03317-5.
Walter Rudin. Real and Complex Analysis, 3rd Ed. McGraw-Hill, Inc., USA, 1987. ISBN
0070542341.
L. Tu. An Introduction to Manifolds, volume NUM. Universitext, Springer, 2008.
Tian Wang, Yang Li, Guanzhong Yang, Weifeng Dai, Yi Yang, Chuanliang Han, Xingyun Wang,
Yange Zhang, and Dajun Xing. Laminar subnetworks of response suppression in macaque
primary visual cortex. Journal of Neuroscience, 40(39):7436-7450, 2020. doi: 10.1523/
JNEUROSCI.1129-20.2020.
W.C.Hoffmann. The visual cortex is a contact bundle. Applied mathematics and computation, 32:
137-167, 1989.
A Proof of Proposition 3.1
We first notice that, from a mathematical point of view, we can identify E and E, hence we will
state our result in this simplified, but mathematically equivalent fashion.
In our notation, D is the domain E identified with E and S our receptive field R, while S denotes
R. Notice that, given our physiological setting, both R and R are bounded on D.
For notation and main definitions, see Rudin (1987).
Proposition A.1. Let D be a compact domain in R2. Consider the function:
S : D -→R,
S(x, y)
/
Uρ (x,y)
K(u, v)S(u, v) dudv
(4)
where S : D -→ R is an arbitrary function, S(D) is bounded and (for a fixed ρ):
Uρ(χ,y) = {(u,v) ∈ R2 : (U -χ)2 + (V -y)2 ≤ ρ2},
11
Under review as a conference paper at ICLR 2022
(±1 if	(U — x)2 + (v — y)2 ≤ (P — e)2
K(x, y) =
(干1 if	(ρ	一 e)2 <	(u	一	x)2	+	(v	一 y)2	≤ ρ2
Then S is Lipschitz continuous in both variables on D.
Proof. Since S(D) is bounded, we have N ≤ S(x, y) ≤ M for all (x, y) ∈ E. We need to show
that there exists L ∈ R so that
|S(p) - S(q)| < L kp - qk,	for all p = (xp,yp), q = (xq,yq) ∈ E
(5)
Let A = Uρ (xp, yp) and B = Uρ (xq , yq). Then:
|S(p)-S(q)|
S(u, v) du dv 一	S(u, v) du dv lll
S(u, v) du dv 一	S(u, v) du dv
M du dv 一	N du dv lll
We shall now consider two cases:
1.	kp - qk > 2ρ: this implies that A r B = B r A = 0. In this case
lll	M du dv 一	N du dvlll = (M 一 N ) πρ2
so that ifwe choose M1 ∈ R defined as
M1 = (M 一 N) πρ
we obtain the required equality (5).
2.	kp 一 q k ≤ 2ρ: in this case
∣/ Mdudv 一/	Ndudvl=(M 一 N) μ(A r B)
where μ(A r B) is the area of A r B, which is the same as B r A.
By looking at the explicit analytic elementary formula for such area, we see that it is an
algebraic function of kp 一 qk, hence we can find M2 satisfying (5).
Finally, defining
L = max M1, M2
we obtain our result.
□
B Proof of Proposition 4.2
We now turn to the proof of the Prop. 4.2, see Tu (2008) for more details on the notation.
Proposition B.1. Let F : D 一→ R be a differentiable function on a compact domain D in R2. Let
(x0, y0) ∈ D be a regular point for F. Consider the vector field in R2: Z(x, y, θ) = 一 sin θ∂x +
cos θ∂y. Then, we have the following:
1.	There there exists a unique θχ0,yo ∈ S1 for which thefunCtion Zχ,y : S1 —→ R, Zχ,y (θ):=
Z(θ) F(x, y) attains its maximum.
2. The map Θ : reg(D) —→ S1, Θ(x, y) = θχ,y, is well defined and differentiable.
12
Under review as a conference paper at ICLR 2022
3.	The set:
Φ = {(x, y, Θ(x, y)) ∈ D × S1 : Θ(x, y) = θx,y}
is a regular submanifold of D × S1.
Proof. (1). Since ζx,y is a differentiable function on a compact domain it admits maximum, we need
to show it is unique. We can explicitly express:
ζx,y(θ) = -sinθ∂xF+cosθ∂yF
Since (∂xF, ∂yF) 6= (0, 0) and it is constant, by elementary considerations, taking the derivative of
ζx,y with respect to θ we see the maximum is unique.
(2)	. Θ is well defined by (1) and differentiable.
(3)	. It is an immediate consequence of the implicit function theorem.
□
13