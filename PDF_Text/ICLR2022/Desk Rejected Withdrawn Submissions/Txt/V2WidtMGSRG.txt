Under review as a conference paper at ICLR 2022
Provab le Identifiability of ReLU Neural Net-
works via LASSO Regularization
Anonymous authors
Paper under double-blind review
Ab stract
LASSO regularization is a popular regression tool to enhance the prediction accu-
racy of statistical models by performing variable selection through the `1 penalty,
initially formulated for the linear model and its variants. In this paper, the territory
of LASSO is extended to the neural network model, a fashionable and powerful
nonlinear regression model. Specifically, given a neural network whose output
y depends only on a small subset of input x, denoted by S? , we prove that the
LASSO estimator can stably reconstruct the neural network and identify S? when
the number of samples scales logarithmically with the input dimension. This chal-
lenging regime has been well understood for linear models while barely studied
for neural networks. Our theory lies in an extended Restricted Isometry Property
(RIP)-based analysis framework for two-layer ReLU neural networks, which may
be of independent interest to other LASSO or neural network settings. Based on
the result, we further propose a neural network-based variable selection method.
Experiments on simulated and real-world datasets show the promising performance
of our variable selection approach compared with classical techniques.
1 Introduction
Given n observations (yi , xi), i = 1, . . . , n, we often model them with the regression form of
yi = f(xi) + ξi, with an unknown function f, xi ∈ Rp being the input variables, and ξi representing
statistical errors. A general goal is to estimate a regression function fbn close to f for prediction or
interpretation. This is a challenging problem when the input dimension p is comparable or even
much larger than the data size n. For linear regressions, namely f (x) = w>x, the least absolute
shrinkage and selection operator (LASSO) (Tibshirani, 1996) regularization has been established
as a standard tool to estimate f . The LASSO has also been successfully used and studied in many
nonlinear models such as generalized linear models (Van de Geer et al., 2008), proportional hazards
models (Tibshirani, 1997), and neural networks (Goodfellow et al., 2016). In particular, the LASSO
regularization has been added into the standard deep learning toolbox of many open-source libraries,
e.g., Tensorflow (Abadi et al., 2016) and Pytorch (Paszke et al., 2019). Despite the practical success of
LASSO, its theoretical efficacy in neural networks is barely studied. In particular, it remains unclear
whether LASSO may be used for variable selection and subsequent interpretations of a learned model.
Meanwhile, in the theoretical study of neural networks, there has been remarkable progress towards
understanding their approximation errors (Barron, 1993; 1994) and generalization errors (Barron &
Klusowski, 2019; Schmidt-Hieber, 2017; Bauer & Kohler, 2019). Nevertheless, the identifiability
issue of neural networks has been an unsolved challenge. Specifically, supposing that data observa-
tions are generated from a neural network with only a few nonzero coefficients (or its proximity), the
identifiability concerns the possibility of identifying those coefficients. In practice, such sparsity of
neural coefficients may be interpreted as a sparse set of input variables that are genuinely relevant to
the response, which may be of scientific interest.
In this paper, we consider the following class of two-layer ReLU neural networks.
Fr =	f : x 7→ f(x) =	aj relu(wj> x +	bj),	where	aj, bj	∈ R, wj	∈	Rp	.
j=1
Here, p and r denote the input dimension and the number of neurons, respectively. We will assume
that a neural network model of parsimoniousness generates the data. In other words, some of the
1
Under review as a conference paper at ICLR 2022
1n
WmanbkWk1	a，0	nXI
input signals are irrelevant to explain y, or some of the network structure in f is redundant for
modeling (y, x). Different forms of parsimoniousness were assumed in (Schmidt-Hieber, 2017;
Bauer & Kohler, 2019; Barron & Klusowski, 2019) to derive tight neural network risk bounds. We
raise the following two questions to understand the nonlinear nature of neural networks.
First, if the underlying system f admits a parsimonious representation, meaning that only a small set
of input variables, S? , is relevant, can we identify them with high probability given possibly noisy
measurements (yi, xi), for i = 1, . . . , n? Second, is such a S? estimable, meaning that it can be
solved from an optimization problem with high probability, even in small-n and large-p regimes?
To address the above questions, we will establish a theory for neural networks with the LASSO
regularization by considering the minimization problem
yi -	ajrelu(wj>xi + bj)	≤ σ2,	(1)
j=1
which is an alternative version of the L1-regularization. More notational details will be introduced in
Subsection 3.2.
We theoretically show that the LASSO-type estimator can stably identify ReLU neural networks with
sparse input signals, up to a permutation of hidden neurons. Our result is rather general as it applies
to noisy observations of y and dimension regimes where the sample size n is much smaller than the
number of input variables p. Our theory gives positive answers to the above questions. The theory
was derived based on new concentration bounds and function analysis that may be interesting in their
own right.
Inspired by the developed theory, we also propose a neural network-based variable selection method.
The idea is to use the neural system as a vehicle to model nonlinearity and extract significant variables.
To the best of our knowledge, the identifiability perspective of neural networks and its subsequent
variable selection method have not been seen in the literature. Through various experimental studies,
we show encouraging performance of the technique in identifying a sparse set of significant variables
from large dimensional data, even if the underlying data are not generated from a neural network.
Compared with popular approaches based on tree ensembles and linear-LASSO, the developed
method is suitable for variable selection from nonlinear, large-dimensional, and low-noise systems.
The rest of the paper is outlined as follows. Section 2 reviews the related work. Section 3 introduces
the main theoretical results and develops a practical algorithm to perform variable selection. Section 4
uses simulated and real-world datasets to demonstrate the proposed theory and algorithm. Section 5
concludes the paper.
2 Related work
Linear models. The variable selection problem is also known as support recovery or feature selection
in different literature. Selection consistency requires that the probability of supp(wb) = supp(w)
converges to one as n → ∞. The mainstream approach to select a parsimonious sub-model is to
either solve a penalized regression problem or iteratively pick up significant variables. The existing
methods differ in how they incorporate unique domain knowledge (e.g., sparsity, multicollinearity,
group behavior) or what desired properties (e.g., consistency in coefficient estimation, consistency in
variable selection) to achieve. For instance, consistency of the LASSO method (Tibshirani, 1996) in
estimating the significant variables has been extensively studied under various technical conditions,
including sparsity, mutual coherence (Donoho & Huo, 2001), restricted isometry (Candes & Tao,
2005), irrepresentable condition (Zhao & Yu, 2006), and restricted eigenvalue (Bickel et al., 2009).
Neural network models. Neural networks have been practically successful in modeling a wide range
of nonlinear systems. Analytically, a universal approximation theorem was established that shows
any continuous multivariate function can be represented precisely by a polynomial-sized two-layer
network (Kolmogorov, 1957). It was later shown that any continuous function could be approximated
arbitrarily well by a two-layer perceptron with sigmoid activation functions (Cybenko, 1989), and
an approximation error bound of using two-layer neural networks to fit arbitrary smooth functions
has been established (Barron, 1993; 1994). Statistically, generalization error bounds for two-layer
neural networks (Barron, 1994) and multi-layer networks (Neyshabur et al., 2015; Golowich et al.,
2
Under review as a conference paper at ICLR 2022
2017) have been developed. From an optimization perspective, the parameter estimation of neural
networks could be cast into a tensor decomposition problem where a provably global optimum can
be obtained (Janzamin et al., 2015; Ge et al., 2017; Mondelli & Montanari, 2018). Very recently,
a dimension-free Rademacher complexity to bound the generalization error for deep ReLU neural
networks was developed to avoid the curse of dimensionality (Barron & Klusowski, 2019). It was
proved that certain deep neural networks with few non-zero network parameters could achieve
minimax rates of convergence (Schmidt-Hieber, 2017). A tight error bound free from the input
dimension was developed by assuming that the data is generated from a generalized hierarchical
interaction model (Bauer & Kohler, 2019). Overall, theoretical studies have primarily focused on the
prediction risk bounds or generalization error bounds of estimated neural networks.
3	Main results
3.1	Notation
Let uS denote the vector whose entries indexed in the set S remain the same as those in u, and the
remaining entries are zero. For any matrix W ∈ Rp×r, we define
1/2
kWk1 =	|wkj|, kWkF=	wk2j	.
1≤k≤p,1≤j≤r	1≤k≤p,1≤j≤r
Similar notations apply to vectors. The inner product of two vectors is denoted as hu, vi. Let wj
denote the j-th column of W . The sparsity of a matrix W refers to the number of nonzero entries
in W. Let N(0, Ip) denote the standardp-dimensional Gaussian distribution, and 1(∙) denote the
indicator function. The rectified linear unit (ReLU) function is defined by relu(v) = max{v, 0} for
all v ∈ R.
3.2	Formulation
Given n independently and identically distributed (i.i.d.) observations {xi, yi}1≤i≤n satisfying
r
y = X a? ∙ relu(w?>Xi + b?) + ξ with Xi 〜N(0, Ip),	(2)
j=1
where r is the number of neurons, aj? ∈ {1, -1}, wj? ∈ Rp, bj? ∈ R, and ξi denotes the random noise
or approximation error obeying
1n
n X ξ2 ≤ σ2.	⑶
i=1
In the above formulation, the assumption a? ∈ {1, -1} does not lose generality since a ∙ relu(b)=
ac ∙ relu(b∕c) for any c > 0. The setting Equation (3) is for simplicity. If ξ∕s are unbounded random
variables, our theoretical result later on still holds, and more explanations are in the supplement. The
ξi ’s are not necessarily i.i.d. and σ is allowed to be zero, which reduces to the noiseless scenario.
Let W? = [w1?, . . . , wr?] ∈ Rp×r denote the data-generating coefficients. The question we aim to
address is whether we can stably identify those nonzero elements, given that most entries in W? are
zero. The study of neural networks from an identifiability perspective is exciting and essential. Unlike
the generalizability problem that studies the predictive performance of machine learning models,
the identifiability may be used to interpret modeling results and help scientists make trustworthy
decisions. To illustrate this point, we will propose to use neural networks for variable selection in
Subsection 3.4.
To answer the above question, we propose to study the following LASSO-type optimization. Let
(W, b, b) be a solution to the following optimization problem,
min k Wk ι subject to JXbi — ^X aj∙relu(w>Xi+ bj) 2 ≤ σ2 ,	(4)
3
Under review as a conference paper at ICLR 2022
within the feasible range a ∈ {1, -1}r, W ∈ Rp×r, and b ∈ Rr.
Intuitively, the optimization operates under the constraint that the training error is not too large and
the objective function tends to sparsify W. Under some regularity conditions, we will prove that the
solution is indeed sparse and close to the truth.
Assumption 1. Suppose that for some constant B ≥ 1,
1 ≤ kwj? k2 ≤ B and	|bj? | ≤ B	∀1 ≤ j ≤ r.
In addition, we assume that for some constant ω > 0,
max ⅛w⅛ ≤ -1.
j=k kw?k2kw?k2	rω
(5)
(6)
The condition in (5) is a normalization only for technical convenience, since we can re-scale
wj , bj , yi , σ proportionally without loss of generality. Though this condition implicitly requires
wj? 6= 0 for all j = 1, . . . , r, it is reasonable since it means the neuron j is not used/activated. The
condition in (6) requires that the angle of any two different coefficient vectors is large enough. We
will provide an alternative assumption in the supplementary document.
3.3	Main theorem
Our main result shows that if W? is sparse, one can stably reconstruct a neural network when the
number of samples (n) scales logarithmically with the input dimension (p). We only focus on the
varying n and p and implicitly assume that the sparsity of W ? and the number of neurons r are fixed.
A skeptical reader may ask how the constants exactly depend on the sparsity and r. We will provide a
more elaborated theorem in the supplementary document.
Theorem 1. Under the Assumption 1, there exist some universal constants c1, c2, c3 > 0 depending
only (polynomially) on the sparsity of W ?, such that: for any δ > 0, one has with probability at least
1- δ,
a = Πa?	and ∣∣W - W*∏>∣∣f + kb - Π∕∣∣2 ≤ cισ	(7)
for some permutation matrix Π, provided that
n > c2 log4 P and σ < c3.	(8)
δ
Remark 1 (Interpretations of Theorem 1). The permutation matrix Π is necessary since the con-
sidered neural networks produce identical predictive distributions (of y conditional x) under any
permutation of the hidden neurons. The result says that the underlying neural coefficients can be
stably estimated even when the sample size n is much smaller than the number of variables p. Also,
the estimation error bound is at the order of σ, the specified noise level in (3).
Suppose that we define the Signal-to-noise ratio (SNR) to be Ekxk2∕σ2∙ An alternative way to
interpret the theorem is that a large SNR ensures the global minimizer to be close to the ground truth
with high probability. One may wonder what if the σ < c3 condition is not met. We note that if σ is
too large, the error bound in (7) would be loose, and it is not of much interest anyway. In other words,
if the SNR is small, we may not be able to estimate parameters stably. This point will be demonstrated
by experimental studies in Section 4.
The estimation results in Theorem 1 can be translated into variable selection results as shown in
the following Corollary 1. The connection is based on the fact that if i-th variable is redundant, the
underlying coefficients associated with it should be zero. Let w?. denote the i-th row of W?. Then,
S? = {1 ≤ i ≤ P ：kw?,』2 > 0}
characterizes the “significant variables.” Corollary 1 says that the set of variables with non-vanished
coefficient estimates contains all the significant variables. The corollary also shows with a suitable
shrinkage of the coefficient estimates, one can achieve variable selection consistency.
Corollary 1 (Variable selection). Let S0 and Sc1σ ⊆ {1, . . . ,P} denote the sets of i’s such that
∣∣Wi,∙∣∣2 > O and ∣∣Wi,∙k2 > cισ, respectively. Under the same assumption as in Theorem 1, and
inf∣∣wjk2 > c∖σ, forany δ > 0, one has
P(S? ⊆ Sb0) ≥ 1- δ and	P(S? = Sbc1σ) ≥ 1- δ,
provided that n > c2 log4 P and σ < c3.
4
Under review as a conference paper at ICLR 2022
Considering the noiseless scenario σ = 0, Theorem 1 also implies the following corollary.
Corollary 2 (Unique parsimonious representation). Under the Assumption 1, there exist universal
constants c1 , c2 > 0 depending only on the sparsity of W ? such that: for any δ > 0, one has with
probability at least 1 - δ,
a = Πa?,	and W = W *Π>,	and	b = Πb?
for some permutation matrix Π, provided that n > c2 log4 δ∙
Corollary 2 says that among all the possible representations W of (2) (with ξi = 0), the one(s) with
the smallest L1-norm must be identical to W? up to a column permutation with high probability.
In other words, the most parsimonious representation (in the sense of L1 norm) of two-layer ReLU
neural networks is unique. This observation addresses the questions raised in Section 1.
Remark 2 (Sketch proof of Theorem 1). The proof of Theorem 1 is highly nontrivial, and it is
included in the supplementary document. Next, we briefly explain the sketch of the proof. First, we
will define what we refer to as D1-distance and D2-distance between (W, a, b) and (W?, a?, b?).
These distances can be regarded as the counterpart of the classical L1 and L2 distances between two
vectors, but allow the invariance under any permutation of neurons (Remark 1). Then, we let
1n
∆n :=1 X
n
i=1
2
rr
aj relu(wj> xi + bj) -	aj?relu(wj?>xi + bj?)
j=1	j=1
here W, a, b is the solution of Equation (4), and develop the following upper and lower bounds of it.
r r log3 4	2	2
△n ≤ C6	+---nδ D2 + C6σ2	and	∆n ≥ c4 min
Sn	1
(9)
hold with probability at least 1 一 δ, provided that n ≥ c5S3r4 log4 P ,for some constants c4, c5,c6,
and Sto be specified. Here, the upper bound will be derived from a series of elementary inequalities.
The lower bound is reminiscent of the Restricted Isometry Property (RIP) (Candes & Tao, 2005) for
linear models. We will derive it from the lower bound of the population counterpart by concentration
arguments, namely
2
rr
E	aj relu(wj> x + bj) 一	aj?relu(wj?>x + bj?)	≥ cmin
j=1	j=1
for some constant c > 0. The bounds in (9) imply that with high probability,
c4 min ʃ 1,D2 ] ≤ C6	+ r "g nδ [ D2 + c6σ2,
r	Sn
Using this and an inequality connecting D1 and D2, we can prove the final result.
3.4	Variable selection
To solve Equation (4) in practice, we consider the following alternative problem,
1
min 一
W,a,b n
nr	2
E (yi - Eaj ∙ reiu(w> Xi+ bj))
i=1	j=1
+λkWk1.
(10)
The above optimization problem can be numerically solved using algorithms such as stochastic
gradient descent (Bottou, 2010) and ADAM (Kingma & Ba, 2014), available from many open-source
libraries. We discuss some details regarding the variable selection using LASSO regularized neural
networks.
Tuning parameters. Given a labeled dataset in practice, we will need to tune several hyper-
parameters, including the penalty term λ, number of neurons r, learning rate, and number of epochs.
We suggest the usual approach that splits the training data into training and validation parts. The
5
Under review as a conference paper at ICLR 2022
training data are used to estimate neural networks for a set of candidate hyper-parameters. The most
suitable candidate will be identified based on the predictive performance on the validation data. We
point out that there are gaps between the developed theory and the selection method in practice. For
example, the selected number of hidden neurons r based on the training data may violate the constant
bounds in Assumption 1. Fortunately, from our experimental studies, the results are not very sensitive
to the choice of r.
Variable importance. Inspired by Corollary 1, We assign the norm of Wi,. as the importance of the
i-th variable, for i = 1, . . . ,p. As Corollary 1 implies, we can accurately identify all the significant
variables in S? With high probability if We correctly set the cutoff value c1σ.
Setting the cutoff value. In practice, We have no idea of the threshold c1σ. But it is conceivable that
variables With large importance are preferred over those With near-zero importance. This inspires us
to cluster the variables into tWo groups based on their importance. Here, We suggest tWo possible
approaches. The first is to use a data-driven approach such as k-means and Gaussian mixture model
(GMM). The second is to manually set a threshold value according to domain knoWledge on the
number of important variables.
4	Experiments
We perform experimental studies to shoW the promising performance of the proposed variable selec-
tion method. We compare the variable selection accuracy and prediction performance of the proposed
algorithm (‘NN’) With several baseline methods, including the LASSO (‘LASSO’), orthogonal
matching pursuit (‘OMP’), random forest (‘RF’), and gradient boosting (‘GB’). The implementation
folloWs Subsection 3.4. In particular, We used ADAM to optimize and GMM to select significant
variables. The parameters grid of ‘NN’ is set as the penalty term λ ∈ {0.1, 0.05, 0.01, 0.005}, the
number of neurons r ∈ {20, 50, 100}, the learning rate in set {0.05, 0.01, 0.005}, and the number of
epochs in set {200, 500, 1000}. We use the absolute value of the estimated coefficient as the variable
importance for ‘LASSO’ and ‘OMP’, and use the self-produced feature importance for the tree-based
methods. All the computation is done on the 2.3GHz Quad-Core Intel Core i5 With Intel Iris Plus
Graphics 655.
4.1	Synthetic datasets
4.1.1	NN-generated dataset
The first experiment uses the data generated from Equation (2) With p = 100 variables and r = 16
neurons. The first 10 roWs of neural coefficients W are independently generated from the standard
uniform distribution and the remaining roWs are zeros, representing 10 significant variables. The
neural biases b are also generated from the standard uniform distribution. The signs of neurons, a,
folloW an independent Bernoulli distribution. The training size is n = 500 and the test size is 2000.
The noise level σ is set to be 0, 0.5, 1, and 5. For each σ, We evaluate the number of correctly selected
variables (‘TP’) and Wrongly selected variables (‘FP’), along With the test error. The procedure is
independently replicated 100 times. The average numbers of selected features are reported in Table 1.
The test errors are reported in Table 2.
The results shoW that ‘NN’ has the best performance on both the selection and prediction. The
performance of tree-based methods is surprisingly undesirable. Also, When the noise level σ increases,
or the SNR decreases, all the methods perform Worse. Another observation is that selection accuracy
and prediction performance are positively associated for ‘NN’, but this is not the case for other
methods.
4.1.2	Linear dataset
This experiment considers data generated from a linear model y = x>β + ξ, Where β =
(3,1.5,0,0,2,0,0,0)>, ξ 〜N(0, σ2), and X follows a multivariate Gaussian distribution whose
(i, j)-th correlation is 0.5|i-j|. Among the p = 8 features, only three of them are significant. The
training size is n = 60 and the test size is 200. The other settings are the same as Subsubsection 4.1.1.
The results are presented in Tables 3 and 4.
6
Under review as a conference paper at ICLR 2022
Table 2: Performance comparison on the
NN-generated data, in terms of the aver-
age mean squared error for different σ .
The standard errors of ‘NN’ are within 0.1,
while linear methods are around 0.4 and
Table 1: Performance comparison on the NN-
generated data, in terms of the number of correctly
(‘TP’) and wrongly (‘FP’) selected features for differ-
ent σ. The standard errors are within 0.3, except for
the ‘FP’ of ‘LASSO’, which is 0.6.
Method	σ=0		σ = 0.5	σ=1	σ=5	tree-based methods are 0.8.				
						Method	σ=0	σ = 0.5	σ=1	σ=5
NN	TP	10.0	9.7	9.8	6.7					
	FP	0.0	0.1	1.8	1.3	NN	0.55	072	1.18	4.75
LASSO	TP	9.5	8.8	8.6	6.5	LASSO	5.05	5.71	5.07	5.50
	FP	12.4	10.8	10.5	9.3	OMP	5.27	4.75	5.01	6.17
OMP	TP	8.4	8.0	8.6	5.8	RF	10.01	8.86	9.22	9.70
	FP	0.1	0.4	0.0	0.4	GB	5.67	5.84	6.58	10.92
RF	TP	6.3	6.8	7.4	4.2					
	FP	0.1	0.2	0.7	0.8					
GB	TP	7.9	7.8	8.4	5.6					
	FP	1.2	1.5	3.1	3.5					
Table 4: Performance comparison on the
linear data, in terms of the number of av-
erage mean squared error for different σ .
The standard errors are within 0.2 when
Table 3: Performance comparison on the linear data,
in terms of the number of correctly (‘TP’) and wrongly
(‘FP’) selected features for different σ. The standard
errors are within 0.1.
Method	σ=0	σ = 1	σ=3	σ=5	σ < 5, and about 0.4 when σ = 5.
TP NN	TP	3.0	2.7	2.2	1.6	Method σ = 0 σ = 1 σ = 3 σ = 5
NN	FP	0.0	0.0	0.1	0.3	NN	0.11	0.43	2.11	5.42
TP LASSO TP	2.7	3.0	2.5	2.1	LASSO 0.00	0.13	1.32	4.97
FP	0.0	0.0	0.1	0.3	OMP	0.00	0.09	1.47	6.61
TP OMP	TP	3.0	2.8	2.5	1.7	RF	3.54	3.52	4.98	10.00
FP	0.0	0.0	0.3	0.9	GB	2.68	3.04	5.76	14.20
RF	TP	1.5	1.5	1.7	1.4	—
RF	FP	0.0	0.0	0.0	0.3	
TP	1.3	1.5	1.3	1.0	
GB	FP	0.0	0.0	0.0	0.1	
The results show that the linear model-based methods ‘LASSO’ and ‘OMP’ have the best overall
performance, which is expected since the underlying data are from a linear model. The proposed
approach ‘NN’ is almost as good as the linear methods. On the other hand, the tree-based methods
‘RF’ and ‘GB’ perform significantly worse. We think that this is because the sample size n = 60 is
quite small, so the tree-based methods have a large variance. Meanwhile, the ‘NN’ uses L1 penalty to
alleviate the over-parameterization and consequently spots the relevant variables. Additionally, ‘NN’
exhibits a positive association between the selection accuracy and prediction performance, while the
tree-based methods do not.
4.1.3	Friedman dataset
This experiment uses the Friedman dataset with the following nonlinear data-generating process,
y = 10 sin(πx1x2) + 20(x3 - 0.5)2 + 10x4 + 5x5 + ξ. We generate standard Gaussian predictors
x with a dimension of p = 50. The training size is n = 500 and the test size is 2000. Other settings
are the same as before. The results are summarized in Tables 5 and 6. For this nonlinear dataset,
‘NN’ almost always finds the significant variables and excludes redundant ones, which is better
than tree-based methods. At the same time, the linear methods fail to select the quadratic factor x3 .
Moreover, we find that when different methods are compared, the method with a better selection
accuracy does not necessarily exhibit a better prediction and vice versa.
4.2	BGSBoy dataset
The BGSBoy dataset involves 66 boys from the Berkeley guidance study (BGS) of children born in
1928-29 in Berkeley, CA (Tuddenham, 1954). The dataset includes the height (‘HT’), weight (‘WT’),
7
Under review as a conference paper at ICLR 2022
Table 5: Performance comparison on the Friedman
data, in terms of the number of correctly (‘TP’) and
wrongly (‘FP’) selected features for different σ. The
standard errors are within 0.15, except for ‘FP’ of
‘LASSO’, which is around 0.3.
Method		σ=0	σ = 0.5	σ=1	σ=5
NN	TP	4.8	5.0	5.0	4.9
	FP	0.0	0.0	0.0	0.1
LASSO	TP	4.04	4.06	4.1	4.13
	FP	2.03	2.24	2.22	4.72
OMP	TP	4.0	4.0	4.0	4.0
	FP	0.17	0.13	0.09	0.17
RF	TP	4.64	4.54	4.72	3.87
	FP	0.03	0.02	0.02	0.22
GB	TP	4.98	4.94	4.94	4.5
	FP	0.03	0.0	0.02	0.56
Table 6: Performance comparison on the
Friedman data, in terms of the average
mean squared error for different σ. The
standard errors are within 0.1.
Method	σ=0	σ = 0.5	σ=1	σ=5
NN	1.89	2.08	2.32	5.69
LASSO	6.28	6.08	6.2	6.94
OMP	5.8	5.98	5.45	6.31
RF	5.22	5.43	5.36	7.82
GB	1.75	1.87	2.18	7.57
Table 7: Experiment results of different methods on the BGSBoy dataset. RMSE: the mean of the
root mean squared error(standard error). Top 3 features: the feature name(number of selection, out of
100 times).
Method	NN	LASSO	OMP	RF	GB
RMSE	0.04 (0.003)	0.05 (0.002)	0.05 (0.002)	3.07 (0.154)	2.4 (0.142)
Top 3 frequently selected features	WT18(100)	WT18(100)	WT18(100)	WT18(91)	WT18(90)
	HT18(81)	HT18(71)	HT18(64)	LG18(86)	LG18(59)
	N/A	HT9(51)	HT9(16)	LG9(2)	HT18(8)
leg circumference (‘LG’), strength (‘ST’) at different ages (2, 9, 18 years), and body mass index
(‘BMI18’). We choose ‘BMI18’ as the response, which is defined as follows.
BMI18 = WT18/(HT18/100)2,	(11)
where WT18 and HT18 denote the weight and height at the age of 18, respectively. In other words,
‘WT18’ and ‘HT18’ are sufficient for modeling the response among p = 10 variables. Other variables
are correlated but redundant. The training size is n = 44 and the test size is 22. Other settings are the
same as before. We compare the prediction performance and explore the three features which are
most frequently selected by each method. The results are summarized in Table 7.
From the results, all of the methods can identify ‘WT18’ most of the time. Nevertheless, ‘NN’ only
selects ‘WT18’ and ‘HT18’ in all the replications, while other methods sometimes select features that
are redundant but correlated with the response. For example, tree-based methods usually miss ‘HT18’
but select ‘LG18’ instead. The results indicate that only ‘NN’ can stably identify the underlying
significant variables. Interestingly, we find that the linear methods still predict well in this experiment.
The reason is that Equation (11) can be well-approximated by a first-order Taylor expansion on
‘HT18’ at the value around 180, and the range of ‘HT18’ is within a small interval around 180.
4.3	UJIIndoorLoc dataset
The UJIINdoorLoc dataset aims to solve the indoor localization problem via WiFi fingerprinting and
other variables such as the building and floor numbers. A detailed description can be found in (Torres-
Sospedra et al., 2014). Specifically, we have 520 Wireless Access Points (WAPs) signals (which
are continuous variables) and ‘FLOOR’, ‘BUILDINGID’, ‘SPACEID’, ‘RELATIVEPOSITION’,
‘USERID’, and ‘PHONEID’ as categorical variables. The response variable is a user’s longitude
(‘Longitude’). The dataset has 19937 observations. We randomly sample 3000 observations and split
them into n = 2000 for training and 1000 for test. As part of the pre-processing, we create binary
dummy variables for the categorical variables, which results in p = 681 variables in total. We explore
the ten features that are most frequently selected by each method. We set the cutoff value as the
tenth-largest variable importance. The procedure is independently replicated 100 times. The results
are reported in Table 8.
8
Under review as a conference paper at ICLR 2022
Table 8: Experiment results of different methods on the UJIINdoor dataset. RMSE: the mean of the
root mean squared error(standard error). Top 10 features: the feature name(number of selection, out
of 100 times).
Method	NN	LASSO	OMP	RF	GB
RMSE	9.6(0.067)	14.23(0.046)	16.58(0.052)	9.49(0.053)	10.3(0.043)
	BUILDINGID_2(100)	BUILDINGID_1(100)	BUILDINGID_1(100)	BUILDINGID_1(100)	BUILDINGID_2(100)
	BUILDINGID_1(100)	USERID_16(100)	BUILDINGID_2(100)	BUILDINGID_2(100)	BUILDINGID_1(100)
	USERID_16(97)	BUILDINGID_2(100)	WAP099(81)	WAP120(82)	WAP141(91)
Top 10	SPACEID_202(86)	USERID_9(94)	USERID_10(70)	WAP141(76)	WAP120(87)
frequently	USERID_8(76)	WAP099(90)	USERID_16(60)	WAP117(75)	WAP099(68)
selected	USERID_9(74)	USERID_10(72)	USERID_7(58)	WAP173(74)	WAP113(67)
features	PHONEID_14(65)	USERID_7(67)	WAP124(55)	WAP118(58)	WAP117(60)
	FLOOR_3(61)	WAP121(49)	USERID_9(46)	WAP167(57)	PHONEID_14(58)
	SPACEID_201(52)	WAP118(34)	WAP120(31)	WAP035(52)	WAP114(48)
	SPACEID_203(41)	WAP124(28)	WAP117(29)	WAP113(33)	WAP167(47)
Based on the results, the ‘NN’ achieves similar prediction performance as ‘RF’ and significantly
outperforms other methods. As for variable selection, since ‘BUILDING’ greatly influences the
location from our domain knowledge, it is non-surprisingly selected by all methods in every replica-
tion. However, except for ‘BUILDING’, different methods select different variables. Some overlaps,
e.g., ‘PHONEID_14’ selected by ‘NN’ and ‘GB’, ‘USERID_16’ selected by ‘NN’ and ‘LASSO’,
indicate the potentially important variables. Nevertheless, those methods do not achieve an agreement
for variable selection. ‘NN’ implies that all the WAPs signals are weak while categorical variables
provide more information about the user location. Given the extremely high missing rate of WAPs
signals (97% on average, as reported in (Torres-Sospedra et al., 2014)), we think that the interpretation
of ‘NN’ is reasonable.
4.4	Summary
The experiment results show the following points. First, ‘NN’ can stably identify the important
variables and have competitive prediction performance compared with the baselines. Second, the
increase of the noise level will hinder both the selection and prediction performance. Third, the
LASSO regularization is crucial for ‘NN’ to avoid over-fitting, especially for small data. Fourth, the
selection and prediction performances are often positively associated for ‘NN’, but may not be the
case for baseline methods.
5	Concluding Remarks
We established a theory for the use of LASSO in two-layer ReLU neural networks. In particular,
we showed that the LASSO estimator could stably reconstruct the neural network coefficients and
identify the critical underlying variables under reasonable conditions. We also proposed a practical
method to solve the optimization and perform variable selection.
We briefly remark on some interesting further work. First, the algorithm can be directly extended to
deeper neural networks. It will be exciting to generalize the main theorem to the multi-layer cases.
Second, the developed theory may be extended to study the variable selection for general nonlinear
functions due to the universal approximation theorem.
The supplementary material includes detailed proofs and Python codes used for the experiments.
References
Martin Abadi, Paul Barham, Jianmin Chen, Zhifeng Chen, Andy Davis, Jeffrey Dean, Matthieu Devin,
Sanjay Ghemawat, Geoffrey Irving, Michael Isard, et al. Tensorflow: A system for large-scale
machine learning. In Proc. USENIX,pp. 265-283, 2016.
Andrew R Barron. Universal approximation bounds for superpositions of a sigmoidal function. IEEE
Trans. Inf. Theory, 39(3):930-945, 1993.
9
Under review as a conference paper at ICLR 2022
Andrew R Barron. Approximation and estimation bounds for artificial neural networks. Mach. Learn.,
14(1):115-133,1994.
Andrew R Barron and Jason M Klusowski. Complexity, statistical risk, and metric entropy of deep
nets using total path variation. arXiv preprint arXiv:1902.00800, 2019.
Benedikt Bauer and Michael Kohler. On deep learning as a remedy for the curse of dimensionality in
nonparametric regression. Ann. Stat., 47(4):2261-2285, 2019.
Peter J Bickel, Ya’acov Ritov, Alexandre B Tsybakov, et al. Simultaneous analysis of lasso and
dantzig selector. Ann. Stat., 37(4):1705-1732, 2009.
Leon Bottou. Large-scale machine learning with stochastic gradient descent. In Proc. COMPSTAT,
pp. 177-186. Springer, 2010.
EJ Candes and T Tao. Decoding by linear programming. IEEE Trans. Inf. Theory, 51(12):4203-4215,
2005.
George Cybenko. Approximations by superpositions of a sigmoidal function. Math. Control Signals
Syst., 2:183-192, 1989.
Vu Dinh and Lam Si Tung Ho. Consistent feature selection for analytic deep neural networks. arXiv
preprint arXiv:2010.08097, 2020.
David L Donoho and Xiaoming Huo. Uncertainty principles and ideal atomic decomposition. IEEE
Trans. Inf. Theory, 47(7):2845-2862, 2001.
Rong Ge, Jason D Lee, and Tengyu Ma. Learning one-hidden-layer neural networks with landscape
design. arXiv preprint arXiv:1711.00501, 2017.
Noah Golowich, Alexander Rakhlin, and Ohad Shamir. Size-independent sample complexity of
neural networks. arXiv preprint arXiv:1712.06541, 2017.
Ian Goodfellow, Yoshua Bengio, Aaron Courville, and Yoshua Bengio. Deep learning, volume 1.
MIT press Cambridge, 2016.
Majid Janzamin, Hanie Sedghi, and Anima Anandkumar. Beating the perils of non-convexity:
Guaranteed training of neural networks using tensor methods. arXiv preprint arXiv:1506.08473,
2015.
Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint
arXiv:1412.6980, 2014.
Andrei Nikolaevich Kolmogorov. On the representation of continuous functions of many variables by
superposition of continuous functions of one variable and addition. In Doklady Akademii Nauk,
volume 114, pp. 953-956. Russian Academy of Sciences, 1957.
Marco Mondelli and Andrea Montanari. On the connection between learning two-layers neural
networks and tensor decomposition. arXiv preprint arXiv:1802.07301, 2018.
Behnam Neyshabur, Ryota Tomioka, and Nathan Srebro. Norm-based capacity control in neural
networks. Conf. Learn. Theory, pp. 1376-1401, 2015.
Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor
Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. Pytorch: An imperative style,
high-performance deep learning library. arXiv preprint arXiv:1912.01703, 2019.
Johannes Schmidt-Hieber. Nonparametric regression using deep neural networks with relu activation
function. arXiv preprint arXiv:1708.06633, 2017.
Robert Tibshirani. Regression shrinkage and selection via the lasso. J. Roy. Statist. Soc. B, 58(1):
267-288, 1996.
Robert Tibshirani. The lasso method for variable selection in the cox model. Stat. Med., 16(4):
385-395, 1997.
10
Under review as a conference paper at ICLR 2022
Joaqun Torres-Sospedra, Raul Montoliu, Adolfo Mardnez-Us6, Joan P Avariento, Tomgs J Arnau,
Mauri Benedito-Bordonau, and Joaquin Huerta. Ujiindoorloc: A new multi-building and multi-
floor database for wlan fingerprint-based indoor localization problems. In Int. Conf. IPIN, pp.
261-270. IEEE, 2014.
Read D Tuddenham. Physical growth of california boys and girls from birth to eighteen years. Univ.
Calif. Publ. Child Dev., 1:183-364, 1954.
Sara A Van de Geer et al. High-dimensional generalized linear models and the LASSO. Ann. Stat.,
36(2):614-645, 2008.
Peng Zhao and Bin Yu. On model selection consistency of Lasso. J. Mach. Learn. Res., 7(Nov):
2541-2563, 2006.
11
Under review as a conference paper at ICLR 2022
Supplementary Document
A Main Results
We first restate the main assumptions and results in the following for ease of understanding. Given n
i.i.d. observations {xi , yi}1≤i≤n satisfying
r
Iyi = X a?relu(w?>Xi + b?) + ξi,	With Xi 〜N(0, I)	(12)
j=1
Where ξi denotes the random noise and/or approximation error obeying
1n
1 X ξ2 ≤ σ1 2,	(13)
n
i=1
let (W, b, b) be the solution to the following optimization problem
1 n ( r	∖2
min k W ∣∣ι subject to 一Iyi — £Jaj relu(w>Xi + bj)	≤ σ2.	(14)
W ,a,b	n i=1	j=1
Here, aj ∈ {1, —1}, ∣W ∣1:= Pj,k |wjk|.
Let ψ be the largest value such that
Ehha, relu(W >x + b)i — ha?, relu(W?>x + b?)ii2 ≥ ψD2 [(W, a, b), (W?, a?, b?)]2 . (15)
With similar analysis as Dinh & Ho (2020)[Lemma 3.2], one can see that
ψ>0.	(16)
In addition, we make the following assumptions1.
Assumption 2. Suppose that for some constant B > 0,
∣∣w:∣∣2 ≤ B	and	|b?| ≤ B forall 1 ≤ j ≤ r.	(17)
Since ψ may depend on model dimensions saliently, we demonstrate that the above assumption can
be replaced with the following condition under Gaussian input.
Assumption 3. Suppose that for some constant B > 0,
1 ≤ ∣wj? ∣2 ≤ B and	|bj? | ≤ B for all 1 ≤ j ≤ r.	(18)
Here, we consider the normalized setting ∣wj? ∣2 ≥ 1 for simplicity. In addition, we assume that2
max ⅛⅛ ≤ 4r.
j=k kw?k2kw?k2 — r0.1
(19)
Then if W? has at most s nonzero entries, one can stably reconstruct the neural network stated in the
following result when the sample size scales logarithmically with the input dimension. The following
theorem is a more elaborated version of Theorem 1 in the main paper.
Theorem 2. There exist some universal constants c1, c2, c3 > 0, such that for any δ > 0, one has
with probability at least 1 — δ,
a = Πa? and ∣∣W — W*Π>∣f + ∣b — Πb*∣2 ≤ qσ	(20)
1From the technical proofs, it can be seen that the Gaussian input assumption can be replaced with sub-
Gaussian input. We consider the Gaussian for simplicity.
2Actually, we only need maxj=k kWWjkWWiL ≤ r1ω for some constant ω > 0. Here, we choose ω = 0.1
for ease of understanding.
12
Under review as a conference paper at ICLR 2022
for some permutation Π ∈ {0, 1}r×r, provided that under the Assumption 2,
n > c2s3r3 log4 p,	(21)
ψδ
or under the Assumption 3,
n > c2s3r13 log4 P	and σ < c3.	(22)
In addition, there exists some λ ∈ R, such that (W, b, b) is also the solution to the following
optimization problem
1 n ( r	∖2
min - Elyi- Eajrelu(w>Xi + bj)	+ λ∣∣W∣∣ι.	(23)
W ,a,b n
i=1	j=1
Specifically, by tuning λ, we can let the optimizer of (23) have the same loss as (W, b, b), which is
exactly the solution to (14).
B Analysis: proof of Theorem 2
Let S be the index set with cardinality S consisting of the support for
Define
W := WcS ∈ Rp×r,
W? and top entries of Wc .
1	ʌ 7	GCC
and aj = abj , bj = bj . Define
∣w1 - w2 ∣1 + |b1 - b2 | if a1 =	a2;
d1(w1,a1,b1,w2,a2,b2) =	∣w1∣1 + ∣w2∣1 + |b1| +	|b2| ifa1 6=	a2,	(24)
and
d2(w1 ,aι ,bι, w2,a2,b2) = [ pkw1- w2k2 + |b1- b2|2 if a1 = a2;	(25)
1	if a1 6= a2 .
In addition, for permutation π on [r], let
r
D1 := min	d1(wπ(j), aπ(j), bπ(j), wj?, aj?, bj?),	(26)a
π j=1
r
D2 := mπin	d2(wπ(j), aπ(j), bπ(j), wj?, aj?,bj?)2	(26)b
π j=1
denote the D1-distance and D2-distance between (W, a, b) and (W?, a?, b?), respectively. Then
one has the following bounds.
Lemma 1. For any W ∈ Rp×r with ∣W ∣0 ≤ S, there exists some universal constants c4 , c5 > 0
such that
1n
1X
n i=1
2
rr
ajrelu(wj>xi + bj) -	aj?relu(wj?>xi + bj?)	≥ c4ψD22
j=1	j=1
holds with probability at least 1 - δ provided that
n ≥ c5ψ2S3 log4 p.
δ
In addition, one has
1n	r	r
一	ajrelu(w>Xi + bj) — ɪ2 a?relu(w?>Xi + bj)
n i=1 j=1	j=1
2
≥ c4 min
holds with probability at least 1 - δ provided that
n ≥ c5S3r4 log4 p.
δ
(27)
(28)
(29)
(30)
13
Under review as a conference paper at ICLR 2022
Lemma 2. Then there exists some universal constants c6 > 0 such that
1n
n X
i=1
rr
aj relu(wj> xi + bj) -	aj?relu(wj?>xi + bj?)
j=1	j=1
≤ c6 (S+r⅛"δ) D2+c6σ2
(31)
holds with probability at least 1 - δ.
By comparing the bounds given in Lemma 1 and 2, one has
c4ψD2 ≤ C6 (Sr + T b： nδ ! D2 + C6σ2,
provided that
n > c5ψ2S3 log4 p.
δ
Let Sb? be the index set with cardinality 2s consisting of the support for W? and top entries of
W. In addition, let D1? and D2? denote the D1-distance and D2-distance between WSb?, ab, b and
(W?, a?, b?) in a similar way as (26). Notice the fact that
D2? ≤ D2	and D1 ≤ 2D1? .	(32)
Combined with Lemma 3, the above results give
D2? ≤
2c6
C4ψ5
provided that for some constant c7 > 0
n ≥ C5ψ2s3 log4 p With S ≥ 丝r,
δψ
such that
C6(" r⅛pδ∖ D:2 ≤ c4ψD
Sn	8
Similarly, one has
C4 min { T，D2 } ≤ C6 (Sr + r lθn nδ ! D2 + C6σ2,
provided that
n > c5S3r4 log4 p.
δ
Combined With Lemma 3, the above results give
D: ≤
2c6
一σ,
c4
provided that for some constant c7 > 0
n ≥ c5S3 r4 log4 P	and
δ
such that
C6(； + 3pδ! D2 + C6σ2 <c4
Sn	r
σ2 ≤ 2 4 with S ≥ c7sr3,
and
Then we conclude the proof since after appropriate permutation
..■—■	...	.. ■—■	...
kWc - W?kF ≤ 2kWcSb? -W?kF
14
Under review as a conference paper at ICLR 2022
C Proof of Lemma 1 (lower bound)
This can be seen from the following three properties.
Consider the case that D1 ≤
. With probability at least 1 -
δ,
1n
1X
n i=1
2
rr
aj relu(wj> xi + bj ) -	aj?relu(wj?>xi + bj?)
j=1	j=1
D ι X
2 n
i=1
rr
ajrelu(wej>xi + ebj) -	aj? relu(wj?> xi + bj?)
j=1	j=1
2
(33)
where Wj = w? + D1 (Wj- w?) andbj = b + D1 (% - b?).
• For any > 0 and
1 i- - I------------------,
VZn log Pr log 鬻
there exists some universal constant C1 > 0, such that with probability at least 1 - δ,
1n r	r
一 ar Or ajrelu(w>Xi + bj) — ɪ2 a?relu(w?>Xi + b?)
n i=1 j=1	j=1
2
2
rr
≥ E ajrelu(Wj>x + bj ) - aj?relu(Wj?> x + bj?)
j=1	j=1
-CιD2 log PnJS log Pr log B.
(34)
• For some universal constant C2 > 0
E
2
rr	1
ar ajrelu(w>X + bj) — ɪ2 a?relu(w?>x + b?)	≥ C min < -, D2
j=1	j=1	r
(35)
Putting all together. Let

/ɪɪog
BnS
~1~
for some universal constant C3 > 0 such that
E	δ π
qnlog号logB <4nrVioglpn
Inserting (15) into (34) gives
1n
1X
n
i=1
2
rr
aj relu(Wj> Xi + bj) -	aj?relu(Wj?>Xi + bj?)
j=1	j=1
≥ ψD2 - CιD2 log pn J1 log Pr log BS ≥ ψD22,
δ n S Eδ 2
(36)
holds with probability at least 1 - δ provided that for some constant C4 > 0
n ≥ C4ψ2S3 log pr log BS log2 pn
S Eδ δ
and	Di ≥ -ɪ I π .
—4nr V log 4pn
15
Under review as a conference paper at ICLR 2022
Here, the last line holds due to Lemma 3 and we assume that max {∣ Wk∞, ∣∣b∣∣∞} is bounded by
some constant. On the other hand, if D、< ʌ. ∕p^4pn, (33) and (36) tell us that
Fnr V log ɪ
n
r
r
2
1 n r	r	D2 ψ	ψ
n X Xajreiu(w>xi + bj) - XamU(Wj Xi + b?)	≥ -21-ɪD2 = -D2,
(37)
i=1
j=1
j=1
where D2 denotes the D2 -distance between W, ae, b and (Wj, aj, bj) in a similar way as (26).
Inserting (35) into (34) gives
n
r
r
2
1n r	r
一	ajrelu(w>xi + bj) -	a?relu(w?>xi + bj)
i=1
j=1
j=1
≥ C2 min [1, D2 ∖ - C1D2 log Pn JS log pr log BS
r	n S
C2
≥ -ɪ min
(38)
holds with probability at least 1 - δ provided that for some constant C4 > 0
n ≥ C4S3r4 log pr log BS log2 Pn and
S
D1 ≥ 4⅛ jlθgψ.
π
Here, the last line holds due to Lemma 3 and we assume that max {kW k∞, kbk∞} is bounded by
some constant. On the other hand, if D、< 捻
n
r
r
Pn, (33) and (38) tell us that
2
1n r	r
一	ajrelu(w>xi + bj) — ɪ2 ajrelu(wj>xi + bj)
i=1
j=1
j=1
D2 C2 min
C2 2
≥
(39)
Summing up, we conclude the proof by verifying the claims in the following.
C.1 PROOF OF (33)
Without loss of generality, we assume that aj = ajj for 1 ≤ j ≤ r, and
r
DI = X (kwj - wjkι + |bj - b?|) ≤ e.
j=1
By taking union bound, with probability at least 1 - X, one has for all 1 ≤ i ≤ n and 1 ≤ j ≤ r,
wjj>xi
since ∣∣wj∣∣2 ≥ 1 and xi 〜N(0, I). In addition, for all 1 ≤ i ≤ n and 1 ≤ j ≤ r,
∣w>xi + bj - wj>xi - b?| ≤ ∣∣Wj - wjkιkxik∞ + |bj - b?| ≤ C'2log 4δn
holds with probability at least 1 - 2. Here, the last inequality comes from the fact that with probability
at least 1 - 2,
Ilxil∣∞ ≤ J2log 4pδn	forall 1 ≤ i ≤ n.
(40)
16
Under review as a conference paper at ICLR 2022
Putting together, we have with probability at least 1 - δ,
u(wj>xi + bj) = u(wj?>xi + bj?),
(41)
with the proviso that ≤
4nr Jiog74pn. Note thatU(X)
1ifx > 0, and u(x) = 0ifx ≤ 0. Then
combining with the definition of wej and bj , the above property yields
1n
1X
n i=1
2
rr
ajrelu(wj>xi + bj) -	aj?relu(wj?>xi + bj?)
j=1	j=1
1n r
-E	aju(wjτXi + bj)(w>Xi + bj — wj>Xi - bj)
n i=1 j=1
D21X
2 n
i=1
D1X
2 n
i=1
and the claim is proved.
C.2 Proof of (34)
2
r
X a?u(w?>Xi + b?)(w>Xi + ej - wjτXi - b?)
j=1
2
rr
Or ajrelu(W>Xi + ej) — ɪ2 a?relu(w?>Xi + b?)
j=1	j=1
Notice that
Iaj relu(w> X + bj) — Oj relu(w?>Xi + b?) ∣
kwj - wj?	k1kXk∞ + |bj - bj? | ifaj =aj? ,
≤ I (kwjkι + kwjkι) kXk∞ + |bj| + |b?| if Oj = a?,
which leads to
r	r	∣
Or Oj relu(w>X + bj) — ɪ2 a?relu(w?>X + b?) ≤ Di max {∣∣Xk∞,1}.	(42)
j =1	j =1	∣∣
For any fixed (W, a, b), let
rr
Zi := ɪ2Ojrelu(w>Xi + bj) — ɪ2 a?relu(w?>Xi + bj),
j=1	j=1
and define the following event set
E := {∣∣Xik∞ ≤ r2 log 4pn	for all 1 ≤ i ≤ n} ∙
Then with probability at least 1 — δ,
1n	1n
n X(z2 - E [z2]) = - X {z21(E) - E [z21(E)] - E [z21(E)]}
i=1
i=1
≥ -4D2 log 4pn r1 log 2 - D2δ
δn δ n
≥ -5D2 log 4pn J1 log 2.
δn δ
(43)
17
Under review as a conference paper at ICLR 2022
Here, the first line holds due to (40); the last line comes from Hoeffding’s inequality, and the fact that
∣E [z21(E)]∣ ≤ Dl E kxik∞l(kxik∞ > Mg4P)
∞
≤ D2	x2dP(kxik∞ < x)
√√2log 4pn
≤ D2 /________4xpexp(-x— )dx ≤ D2 —.
√√2 log 4pn	2	n
In addition, consider the following -net
Ne =t W, a, b): |Wij l∈ r+S「B(r+ S) ] ,kW ko ≤ S,
B(r +S)
lbj | ∈ r + s「	^	]，|aj| = 1J，
where [n] := {1, 2, . . . , n - 1}. Then for all (W, a, b) with kWk1 ≤ B and kbk1 ≤ B, there exists
some point, denoted by Wf , ae, eb , in Ne whose D1-distance from (W, a, b) is less than . For
simplicity, define
rr
zi :=	aj relu(wj> xi +bj) -	aj?relu(wj?>xi +bj?),
j=1	j=1
rr
zei :=	aej relu(wej>xi +ebj) -	aj?relu(wj?>xi +bj?).
j=1	j=1
Similar to (42), we can derive that
∣r	r	∣
∣	aj relu(wj> x +bj) -	eaj relu(wej> x +ebj)∣ ≤ max {kxk∞, 1} ,
∣j=1	j=1	∣
which implies
∣z2 - e2∣ ≤ Ee + Dι)maχ {kχik∞,1},
and then with probability at least 1 - δ,
nn
n X(Zi - E [zi]) - n X(Zi - E [zi]) ≥ -4e (e + DI)IOg ɪ.	(44)
i=1	i=1
In addition, a little algebra gives
log Ne। ≤ C5Slog ρr log BS,
Se
for some universal constant C5 > 0. Combining (43), (44), and (45) leads to
1 X(Z - E [z2]) ≥ -5(e + D1)2 log4pn J1 log 21N1 - 4e (e + D1)log4pn.
n	δn δ	δ
i=1
(45)
Then, (34) is obvious.
C.3 Proof of (35)
We first consider a simple case that bj = 0 and bj? = 0 for 1 ≤ j ≤ r, and show that for some small
constant C6 > 0,
rr
E	aj relu(wj> x) -	aj?relu(wj?>x)
j=1	j=1
≥ C6 min∣∣,D2∣ .	(46)
18
Under review as a conference paper at ICLR 2022
In the following, we will focus on the case
E
2
rr	C
ɪ2 aj relu(w>x) 一	a?relu(w?> x)	≤ ——.
j=1	j=1	r
According to Lemma 4, one has for any constant k ≥ 0, there exists some constant αk > 0 such that
2
r
r
j=1
aj relu(wj> x) -	aj?relu(wj?>x)
j=1
≥ αk
rr
X aj∣Wjk2(麻产k- X a?kw?k2(
j=1	j 2	j=1
w?仔k
llw?k2
(47)
E
2
F
Assumption 1 tells Us that for any integer k ≥ ω2,
Kv?i , v⅛ il ≤ r12.
(48)
where
ww Wj ∖%k
…ec ((E)
with
βj := aj kwj k2 ,
and
V? := VeC ( (-wj^ 产
j	\lw?k2；
with
βj? :=aj?kwj?k2.
Then (47) gives
r
r
2
r
r
2
E
j=1
aj relu(wj> x) -	aj?relu(wj?>x)
≥ α3k
j=1
Eej VM-E 型产
j=1
j=1
F
Define
S+ := Span {vj }jβj >0
S-=Span {vj }jβj <0,
and
s+ := Span {v?}j:e?>0
S- := Span {V?}j：e?<0 .
Let PS and PS⊥ denote the projection onto and perpendicular to the subspace S, respectively. By
noticing that PS⊥ Vj = 0 for j obeying βj < 0, and PS⊥? Vj? = 0 for j obeying βj? > 0, one has
r
r
2
∑βj型产
j=1
j=1
F
2
X βj (PS- Vj 产㊈(Ps+ Vj 产
j:ej>0
≥ X	∣∣β?(PS-v?产㊈(%v?产Il
j∙∙βj<0
- X	β? (PS- V?产㊈(Ps+ V?户
j:e?<0
F ≥ 2 Xh-V?II4，
j:e?<0
≥
—
F
where the penultimate inequality holds since the inner product between every pair of terms is positive,
and the last inequality comes from the facts that |/？| ≥ 1 and (48).
Moreover, by means of AM-GM inequality and (48), one can see that
X
j：e?<0
PS⊥-vj?
F ≥ r (Xh- v?ii2 )2=r ∣P- W?]
j∙∙βj<0
j：e?<Jl
≥ 21r IlPS- PS-

4
F
19
Under review as a conference paper at ICLR 2022
Then combining with (46), the above result and the counterpart for βj? > 0 lead to
dim(S-) ≥ dim(S?-)	and	dim(S+) ≥ dim(S?+),
which gives
dim(S-) = dim(S?-)	and	dim(S+) = dim(S?+).
Furthermore, for some small constant C6 > 0, we have
dist(S-,S?-) ≤ C6	and	dist(S+,S?+) ≤ C6.
Let Pi⊥ denote the projection perpendicular to
span {v?}j=i:e?>0,
and
=BjhPS- vj,ρS-吟 2hpjLni,pS-吟 i
j	呵⊥v湎Plw
Then for any i,
2
≥
F
rr
X βj J X斗产
j=1	j=1
r
X βj (PSL Vj 产㊈「X β? (PSL V?产㊈ v?04
j：e»0	j = 1
2
F
2
≥ 1 X	βj (PS- Vj 广2 ㊈ Vr- X	βj(pS- V?产㊈ V产
j∙βj>0	j：e?>0	F
2
≥ 1 X	βj	(PS-Vj广2	氧(Pi⊥Vi产㊈ V片-β?	(Ps-	V?产乳(Pi⊥V?产氧 V?02
j∙βj>0	F
2
≥ 1 X	YjV02-β?IlPs-V?俏||PiLV?『V?02	,
j：e3 >0	F
which, together with (46), implies that there exists some j such that
Without loss of generality, assume that
for all 1 ≤ j ≤ r.
(49)
Then
rr
E	aj relu(wj> x) -	aj?relu(wj?>x)2 ≥ αk
j=1	j=1
2
rr
X βj Vj v> - X e?V?V?>
j=1	j=1
F
r	/ r	∖ 2
≥ɑkXl∣βjVjv>-e?V?V?TllF-2rk(Xl∣βjVjv>-e?V?V?TllF I
j=1	j=1
≥
2
F.
Here, the first line comes from (47); the second line holds through the following claim
Ihβj1 Vj1 V> -j V?1 V?T , βj2 Vj2 V> - β? V?2 V?T i I
≤ 2r kβj1 Vj1 v> F V?1 V?T k2kβj2 Vj2 v> - β? V?2 V?Tk2
20
Under review as a conference paper at ICLR 2022
since for δj := VZejvj - Pejvj,
β ∙v ∙v> — β?v?v?> — δ δ> + βjδδ v?> + ∕βjv?δ*>
βjvjvj -βjvjvj = δjδj + βj δjvj + βjvj δj .
Then the conclusion is obvious by noticing that
IIejVjv> - βjvjvj>l∣F ≥ kwj - wjk2.
Finally, we analyze the general case with bj , bj? 6= 0, which is similar to the above argument. For
simplicity, we only explain the different parts here. According to Lemma 4, one has for any constant
k ≥ 0, there exists some constant αk > 0 and some function fk : R → R such that
2
r
r
j=1
aj relu(wj> x + bj) -	aj?relu(wj?>x + bj?)
j=1
≥	aj fk (
k≥12
ω
r
j=1
bj	Ml.,, II (	Wj	S X ,?i	b?	∖∣∣…？Il (	w?	Fk
E)kwjk2(j)	-j=1 ajfk(j)kwjk2(呵口
I2
&	ajfk(
j = 1 k≥12
ω
r
I，bj∣∣ )wj - ajfk ( I，∖∣ )w?
kwjk2	j kwj?k2 j
& X inf E aj relu(wj> x + bj ) - aj?relu(wj?>x + bj?) - Rl(x)2
j=1 Rl(x)
r
& X(kwj- w?k2 + |bj-b?|2).
j=1
Here, l = [12], and the second inequality holds in a similar way to above analysis. Then the general
conclusion is handy.
E
∞
∞
r
F
2
F
D Proof of Lemma 2 (upper bound)
For simplicity, let
r
r
zi :=	ajrelu(wj>xi + bj) -	aj?relu(wj?>xi + bj?),
j=1
r
j=1
r
zbi :=	aj relu(wj> xi + bj) - baj relu(wbj> xi + bbj).
j=1	j=1
Recall the optimality of Wc , ab, bb w.r.t. (4). According to the triangle inequality, one has
nn
n X z2 ≤	n X b2+2σ.
i=1	i=1
We can bound the first term in the right hand side by
n
n
r
2
(50)
n	nr
n X z2 = n X X aj (relu(
i=1
i=1
1n
≤ n E
i=1
n
j=1
wj>xi + bj) - relu(wbj> xi + bbj)
r
X (wj - wbj )>xi
j=1
r
≤n X X他-Wj )τ,
2
21
Under review as a conference paper at ICLR 2022
where the second line holds due to the contraction property of ReLu function, and the last line comes
from the AM-GM inequality. Lemma 5 further gives for some constant C7 > 0,
r In	r	ι 3 Pr
X n X I(Wj- Wj )> Xil2 ≤ C X kWj - Wjk2 + C7 T X kWj - Wj k2
j=1	i=1	j=1	j=1
holds with probability at least 1 - δ. In addition,
Xr kWj-Wbjk12 ≤ W-Wc2 ≤ kW?k1-kWck12 ≤ D12,
j=1	1
and
Xr kWj-Wbjk22=W-Wc21≤ W-Wc1W-Wc
j=1	1	1	∞
kW?k1-kWck1	kW?k1 - kWc?k1	4
≤ ʌ-----------S/2-----------L ≤ SD2.
Here, W ? denote the entries
kWck1 ≤ kW?k1 and
of W on the support set for W?,
and we make use of the fact that
..∙—■	■—■.. .. .. .. ■—■..
W - W∣∣	≤ kW? - Wkι ≤ kW?ki-kW?ki
∞ S - s	S/2
Putting everything together gives the desired result.
E Technical lemmas
Lemma 3. For any (W, a, b) with kWk0 + kbk0 + kW?k0 + kb?k0 ≤ S. Assume that kWk1 +
kbk1 ≤ kW?k1 + kb? k1 and kWj?k22 + |bj?|2 ≤ 1. Then one has
Di ≤ 2√SD2,	(51)
where D1, D2 are defined in (26).
Proof. For simplicity, assume that
D2 = X (kwj- w?k2 + |bj- b?|2) + X (kw?k2 + |b?|2).
j∈J	j∈J
Here, j ∈ J means that aj = aj? and
kWj -Wj?k22 + |bj -bj?|2 ≤ kWj?k22 + |bj?|2.
Then according to the AM-GM inequality, one has
√SD2 ≥ X (kWj - w?ki + |bj - b?l) + X (kw?ki + |b?l)
j∈J	j∈J
≥ X (kw?ki -kWjki + |b?|-|bj|) + kW?ki + kb?ki - X (kw?ki + |b?|)
j∈J	j∈J
≥ X (kWjki + |bjI),
j∈J
which implies
2√SD2 ≥ X (kWj - W?ki + Ibj- b?|) + X (kW?ki + |b?| + kWjki + ∣bjI).
j∈J	j/J
Thus we conclude the proof.
□
22
Under review as a conference paper at ICLR 2022
Lemma 4. For any constant k ≥ 0, there exists some universal function fk : R → R such that
r
r
2
with
In addition, we have
j=1
∞
X
k=0
aj relu(wj> x + bj) -	aj?relu(wj?>x + bj?)
r
ajfk(
j=1
r
j=1
人	/	∖ 0k
ι⅛ )kwjk2( ι⅛)
-	aj?fk (
j=1
b?	ʌ,,	?„ w H /
ι⅛ )kwjk2(所)
αk := f2k (0) > 0, for all k > 0.
inf E arelu(w>x +
Rl (x)
r
b) - X a?relu(H?>x + b?) - Rl (x)
j=1
(52)
(53)
∞
Xafk(
k>l
高眄2(严-a?fk(
b?
kw?k2
)kw?k2
w? λ0k
kw?k2
(54)
E
2
F
2
2
F
where Rl (x) denote the polynomial with order less than l.
Lemma 5. There exists some universal constant c > 0, such that for all H ∈ Rp,
1X ∣w>χi∣2 ≤ Cιιwk2 + Clog nδ Ilwk2,
n	2n 1
i=1
holds with probability at least 1 - δ.
(55)
Proof. Before proceeding, we introduce some useful techniques about Restricted Isometry Property
(RIP). Let X := √n[xι, x2,..., xn]. For some constant co > 0, if n ≥ c° (S log P + log 1), then
with probability at least 1 - δ,
X>w22 ≤ 2kwk22
(56)
holds for all H satisfying IHI0 ≤s.
We divide the entries of H into several groups S1 ∪ S2 ∪ . . . ∪ SL with equal size s (except for SL),
such that the entries in Sj are no less than Sk for any j < k. Then, according (56), one has
1n
- y^(w> Xi)2 = H>XX >w = ɪ2 w>j XX > w Sk
≤ 2	kwSj k2 kwSk k2
j,k
In addition, the order of wSl yields for l > 1,
l=1
llw>ι k2 ≤ √skw>ι k∞ ≤ (l -1)√S kwk1,
which leads to
L2	L
XkwSlk2 2 ≤2kwS1k22+2 X
l=1
l=2
1
(I- 1)√s
IHIIy ≤ 2kwk2 +辿g2L∣H∣2.
s
Then the result is obvious by taking above relations together.
□
23