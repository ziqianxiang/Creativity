Under review as a conference paper at ICLR 2022
Robust Graph Data Learning via Latent Graph
Convolutional Representation
Anonymous authors
Paper under double-blind review
Ab stract
Graph Convolutional Representation (GCR) has achieved impressive performance
for graph data representation. However, existing GCR is generally defined on
the input fixed graph which may restrict the representation capacity and also be
vulnerable to the structural attacks and noises. To address this issue, we propose a
novel Latent Graph Convolutional Representation (LatGCR) for robust graph data
representation and learning. Our LatGCR is derived based on reformulating graph
convolutional representation from the aspect of graph neighborhood reconstruction.
Given an input graph A, LatGCR aims to generate a flexible latent graph A for graph
convolutional representation which obviously enhances the representation capacity
and also performs robustly w.r.t graph structural attacks and noises. Moreover,
LatGCR is implemented in a self-supervised manner and thus provides a basic
block for both supervised and unsupervised graph learning tasks. Experiments on
several datasets demonstrate the effectiveness and robustness of LatGCR.
1	Introduction
Graph Convolutional Networks (GCNs) have been widely studied for graph data representation
and learning (Defferrard et al., 2016; KiPf & Welling, 20l7; Velickovic et al., 2018a;b; Chen et al.,
2020; Jiang et al., 2021). Graph convolutional representation (GCR) is the core operation powering
GCNs. The aim of GCR is to generate context-aware embeddings for graPh nodes by aggregating the
messages from their neighbors via some differentiable aggregation functions (Hamilton et al., 2017;
Geisler et al., 2020). For examPle, KiPf et al. (KiPf & Welling, 2017) ProPose a graPh convolution
oPeration by exPloring the first-order aPProximation of graPh LaPlacian sPectral filter. Hamilton
et al. (Hamilton et al., 2017) Present GraPh SamPle and Aggregate (GraPhSAGE) for inductive
graPh rePresentation and learning by using graPh samPling and aggregating techniques. Petar et
al. (Velickovic et al., 2018b) propose Deep Graph Infomax (DGI) to learn node,s representation in an
unsupervised manner. Klicpera et al. (Johannes Klicpera & Gunnemann, 2019) propose Personalized
Propagation of Neural Predictions (PPNP) which combines GCN and PageRank together for graph
semi-supervised learning. Zhu et al. (Zhu et al., 2019) propose Robust GCN (RGCN) by adopting
robust learnable Gaussian distributions for message propagation.
However, the above GCRs are generally defined on the input fixed graph which may restrict the
representation capacity and also be very vulnerable to the structural attacks and noises (Zugner
& Gunnemann, 2019; Jin et al., 2020b; Geisler et al., 2020). To address this issue, one kind of
popular way is to incorporate graph learning modules into GNNs’ training via optimizing a general
joint loss function, i.e., ‘graph learning loss + GNN’s loss’. For example, Wu et al. (Wu et al.,
2019) propose GCN-Jaccard to denoise input graph by deleting the edges with low similarities.
Li et al. (Li et al., 2018) propose AdaGCN to learn an optimal graph for GNN learning. Jiang et
al. (Jiang et al., 2019) propose Graph Learning Convolutional Network (GLCN) by integrating graph
learning module into GCN architecture for semi-supervised learning problem. Yang et al. (Yang
et al., 2019) propose Topology Optimization based Graph Convolutional Networks (TO-GCN) for
semi-supervised learning by jointly refining the graph structure and learning the parameters of GCN.
Jin et al. (Jin et al., 2020b) develop Pro-GNN to adaptively train a more optimal graph for GNN’s
learning via designing a joint loss function.
In this paper, we propose a novel Latent Graph Convolutional Representation (LatGCR) for robust
graph data representation and learning. Our LatGCR is derived based on reformulating GCR from
1
Under review as a conference paper at ICLR 2022
the aspect of graph neighborhood reconstruction. The main difference between LatGCR and previous
related works is that LatGCR gives a basic block which can be used within many GNN architectures
by replacing traditional graph convolution layer with LatGCR block. We will discuss the detailed
differences between LatGCR and previous related works including recent Pro-GNN (Jin et al., 2020b)
and GeCN (Jiang et al., 2021) in §4. Specifically, given an input observed graph A, LatGCR aims
to generate a flexible latent graph A for GCR in a self-supervised manner which enhances the
representation capacity and also obviously performs robustly w.r.t graph structural attacks and noises.
Overall, we summarize the main contributions of this paper as follows:
•	We propose a novel self-supervised Latent Graph Convolutional Representation (LatGCR)
based on the reformulation of GCR from graph neighborhood reconstruction.
•	LatGCR can be efficiently implemented via a simple recurrent architecture, i.e., LatGCR
block, which provides a general basic block for GNNs.
•	Based on the proposed LatGCR block, we propose an end-to-end LatGC neural network
(LatGCN) for robust graph data representation and learning.
Experimental results on both semi-supervised classification and unsupervised clustering tasks demon-
strate the effectiveness and robustness of the proposed LatGCR and LatGCN.
2	Revisiting Graph Convolutional Representation
As the main aspect of Graph Convolutional Networks (GCNs) for graph data representation and
learning, Graph Convolutional Representations (GCRs) have been widely studied in recent years (D-
efferrard et al., 2016; KiPf & Welling, 2017; Hamilton et al., 2017; VeIickovic et al., 2018a; Geisler
et al., 2020). The aim of GCRs is to generate context-aware representations for graph nodes by
aggregating the rePresentations from their neighbors via some sPecific aggregation functions.
One PoPular formulation of GCR is to emPloy the weighted mean aggregation function for neighbor’s
information aggregation (Hamilton et al., 2017; Jiang & Zhang, 2021). Let G(A, Z) denotes the inPut
graph where A ∈ Rn×n denotes the adjacency matrix with Aii = 1 and Z = (zι, z2 •…Zn) ∈ Rn×d
denotes the collection of node features. Then, the weighted mean-tyPe GCR (Hamilton et al., 2017)
can be formulated as
hi = d X AijZjW	(I)
i j∈Ni∪i
where di = Pj∈N ∪i Aij and Ni represents the neighbor set of node i. Matrix W ∈ Rd×d0 denotes
the graph convolutional parameter which is learned adaptively based on the specific downstream
task. The output H0 = (h；, h2 …hQ ∈ Rn×d0 provides the graph convolutional representations of
graph nodes. Comparing with input Z, H0 involves more context information encoded in A and also
provides task-relevant representations for nodes via learned W.
3	Latent Graph Convolution Representation
The above GCR is defined on the fixed input graph A which may restrict the representation capacity
and also has been demonstrated to be very vulnerable to the structural attacks and noises in A (Zugner
& Gunnemann, 2019; Jin et al., 2020b;a). To address this issue, We present a novel Latent Graph
Convolution Representation (LatGCR) for robust graph data representation and learning. Our LatGCR
is motivated based on the reformulation of the above GCR (Eq.(1)) from the aspect of neighborhood
reconstruction (Geisler et al., 2020; Jiang & Zhang, 2021). Specifically, Eq.(1) provides the optimal
solution to the following node reconstruction problem,
hi = -1 X AijZjW = argmhin X Aijkhi - ZjWk2	⑵
i j∈Ni∪i	i j∈Ni∪i
where k ∙ k denotes Frobenius norm function.
2
Under review as a conference paper at ICLR 2022
3.1	LatGCR model formulation
Let G(A, Z) be the input observed graph with adjacency matrix A ∈ Rn×n (Aii = 1) and node
features Z ∈ Rn×d . The aim of LatGCR is to estimate a latent and flexible graph Ae to better support
GCR. Based on the reformulation of GCR (Eq.(2)), we present our LatGCR model by integrating
latent graph estimation and graph convolutional representation jointly as
{Ae0,H0} = arg min A - Ae 2 + λX X Aeijhi -zjW2	(3)
A,H	i j∈Ni ∪i
s.t. Aij ≥ 0
where A0 denotes the estimated latent graph and H0 = (h；, h2 …hQ ∈ Rn×d0 denotes the output
Latent GCRs for graph nodes. Matrix W ∈ Rd×d0 denotes the graph convolutional parameter and
parameter λ > 0 is the trade-off hyper-parameter balancing two terms. The first term in Eq.(3)
represents the latent graph estimation/reconstruction while the second term denotes the GCR. Note
that, when λ → 0, the first term is penalized very largely and we can have Ae 0 = A. In this case,
LatGCR degenerates to standard GCR (Eq.(1)). Overall, there are four main aspects of the above
LatGCR model.
•	Self-supervised joint learning: In LatGCR, the estimation of latent graph is conducted in
a self-supervised way. LatGCR conducts both latent graph generation and GCR jointly to
boost their respective representation ability. Therefore, LatGCR can be potentially used in
both supervised and unsupervised learning tasks.
•	Robustness: When A contains some noises/errors, i.e., A = A + E where E denotes the
noises/errors. Then Eq.(3) can be re-formulated as
{Ae0,H0,E0} = arg min kEk2 + λX X Aeijhi-zjW2
s.t. A = Ae +E,Aeij ≥ 0	(4)
That is, LatGCR acts as recovering/generating a latent ‘clear’ graph A from the input noisy
graph A for GCR and thus performs robustly w.r.t. graph noises and attacks. This is one
important property of LatGCR and will be validated in Experiments in detail.
•	Sparsity: The estimated graph Ae 0 inherits the same sparse pattern from input graph A, i.e.,
we can easily prove that if Aij = 0, then we have Ae 0ij = 0, as also seen from Eq.(5) below.
•	Efficient implementation: In LatGCR, both graph estimation and graph convolutional
representation are implemented via simple one-step update rules which thus can be computed
very efficiently, as discussed in §3.2.
3.2 LatGCR implementation.
The optimum Ae 0 and H0 can be obtained via a simple update algorithm which alternatively conducts
the following Latent Graph Estimation (LGE) and Graph Convolutional Representation (GCR) steps.
LGE-step: Solving A while fixing H, the problem becomes
Ae 0 = arg min A - Ae 2 + λ X X Ae ij hi - zj W2 s.t. Ae ij ≥ 0
Ae	i j ∈Ni ∪i
which is equivalent to
AAij = argmin ((Aij- 2∣∣hi - ZjW∣∣2) - Aij)	s.t. Aij ≥ 0
Aeij
It has a simple closed-form solution which is given as
Aij =max {(Aij - 2Ilhi- ZjWk2),0O	⑸
3
Under review as a conference paper at ICLR 2022
GCR-step: Solving H while fixing A, the problem becomes to the standard GCR Eq.(2). The exact
optimal solution is
H0 = De -1Ae ZW	(6)
where D is the diagonal matrix with Dii =	j∈N ∪i Aij.
Remark. (1) As discussed before, since the optimal Ae0 inherits the sparse pattern of A, in implemen-
tation of Eq.(5), we only need to compute the element Ae 0ij with j ∈ Ni ∪ i which is efficient, as
further analyzed in section Complexity analysis.
(2)	In real implementation, instead of using Eq.(5), we use the following update rule to avoid the
possible numerical issue when λ is large, i.e.,
Aij=max {(Aj - 2khi - ZjW∣∣2),e}, j ∈ N ∪ i	(7)
where e is a very small positive number. That is, when λ is large enough, we have Aij = e, j ∈ Ni ∪ i.
In this case, the above GCR (Eq.(6)) becomes to the unweighed mean aggregation which also gives a
reasonable solution.
LatGCR block. From network architecture aspect, the above LatGCR implementation can be
designed via a recurrent architecture, i.e., LatGCR block, which consists of Latent Graph Estimation
(LGE) and Graph Convolutional Representation (GCR) submodules, as shown in Figure 1. As shown
in Fig. 1(B), LGE and GCR are alternatively conducted in LatGCR. On the contrary, the graph A is
fixed in traditional GCR (shown in Fig. 1(A)). Therefore, LatGCR is more flexible than GCR. More
importantly, LatGCR block performs more robustly than GCR, as demonstrated in Experiments.
(A) GCR	(B)LatGCR
Figure 1: Architectures of GCR and our LatGCR block.
Complexity analysis. The whole computation complexity of the proposed LatGCR block involves
three parts, i.e., linear projection, LGE and GCR. The complexity of projection ZW is O(ndd0).
The computation cost of LGE-step mainly focuses on computing khi - Zj Wk2 with j ∈ Ni ∪ i
and the complexity is O(∣ξ∣d0) where ∣ξ∣ denotes edge number and d0 denotes dimension of feature
vector hi. The complexity of GCR-SteP is O(∣ξ∣d0). In summary, the whole LatGCR complexity is
O(ndd0) + O(2r∣ξ∣d0) where r is the recurrent time for alternatively conducting LGE and GCR and
set to 3 in experiments. Note that, LatGCR does not bring very high complexity when comparing
with GCR (Eq.(1)) whose main complexity is O(ndd0) + O(∣ξ∣d0).
4	LatGC Neural Networks
LatGCR gives a basic block which can be used within many GNN architectures (VeliCkOViC et al.,
2018b; Hamilton et al., 2017; Kipf & Welling, 2017) by replacing the traditional graph convolution
layer with LatGCR module. Here, we adopt the GNN architecture utilized in traditional GCN (Kipf
& Welling, 2017) and design an end-to-end multi-layer neural network architecture, named Latent
Graph Convolutional Network (LatGCN) for graph data learning. Concretely, the proposed LatGCN
contains one input layer, several hidden propagation layers and one final perceptron layer, as shown
in Figure 2. For each hidden propagation layer, it takes features H(l) and initial graph A as input and
4
Under review as a conference paper at ICLR 2022
outputs features H(l+1) by using LatGCR module with parameter W(l). LatGCN can be used in many
graph learning tasks. For example, when applying LatGCN for semi-supervised node classification
tasks, the last perceptron layer outputs the final label predictions P for all nodes. The convolutional
parameter W(l) of each hidden layer can be learned via minimizing the cross-entropy function on the
labelled nodes, as discussed in work (Kipf & Welling, 2017).
Figure 2: The architecture of LatGCN.
Comparison with related works. Exploiting graph learning for GNNs has been studied in recent
years. The main differences between LatGCN and previous graph learning guided GCNs including
Pro-GNN (Jin et al., 2020b), TO-GCN (Yang et al., 2019) and GLCN (Jiang et al., 2019) are follows.
First, LatGCN is designed based on our proposed new LatGCR block (Figure 1 (B)). In contrast,
previous works generally incorporate graph learning into GNN’s training via designing a joint loss
function. Second, LatGCR is derived based on the joint reconstruction framework (Eq.(3)) which
is implemented in a self-supervised manner. This makes LatGCR be a general block which can be
used within many GNN’s architectures to derive various kinds of LatGCNs. LatGCR is also different
from recent GeC and GeCN (Jiang et al., 2021). (1) LatGCR aims to generate a flexible latent graph
for graph convolution while GeC incorporates neighborhood selection into graph convolution. (2)
LatGCR is derived based on neighborhood reconstruction while GeC is designed based on graph
Laplacian regularization. (3) LatGCR implements both graph estimation and GCR via simple one-
step update rules while GeC (Jiang et al., 2021) adopts T -step update rules for both neighborhood
selection and graph convolution operation. Thus, the implementation of LatGCR is generally more
efficient than GeC (Jiang et al., 2021).
5	Experiment
To verify the effectiveness and robustness of LatGCR block and LatGCN, we test it on both semi-
supervised node classification and unsupervised clustering tasks on three standard benchmark datasets,
i.e., Cora, Citeseer and Pubmed (Sen et al., 2008; Jin et al., 2020b).
5.1	Semi-supervised node classification
5.1.1	Experimental setting
Similar to the architecture of GCN (Kipf & Welling, 2017), LatGCN consists of one input layer,
two LatGCR layers and one final perceptron layer. The skip-connection strategy is also utilized in
LatGCN, as suggested in work (Kipf & Welling, 2017; He et al., 2016). We optimize the network
weight matrices of all LatGCR modules by minimizing the cross-entropy loss function (Kipf &
Welling, 2017). For fair comparison, we use the same attacked data setting used in work (Jin
et al., 2020b) and employ two types of attacks, i.e., Metattack (Zugner & Gunnemann, 2019) and
Random Attack (Jin et al., 2020a). For Metattack (Zugner & Gunnemann, 2019), we utilize the most
destructive attack variant ‘Meta-Self’ and set the perturbation level from 0 to 25% with step 5%. For
Pubmed dataset, we use the approximate version ‘A-Meta-Self’ as used in work (Jin et al., 2020b) For
Random Attack (Jin et al., 2020a), we apply the variant ‘Add’ and set the perturbation level from 0 to
100% with step 20%. Following the experimental setup in previous works (Kipf & Welling, 2017;
VeIiCkoViC et al., 2018a), we set the number of units in each hidden layer to 16 and train our LatGCN
by using Adam algorithm (Kingma & Ba, 2015) with learning rate of 0.001. The recurrent time of
each LatGCR block is set to 3 and the hyper-parameter λ is determined based on Validation set. We
proVide additional experiments across different settings of parameter λ in §5.3.
5
UnderreVieW as a ConferenCe PaPersICLR 2022
Accuracy (∕%)
455667788
505050505
O
S
Accuracy (∕%)
5	6	6	7	7
5	0	5	0	5
Accuracy (∕%)
7	8	8	g
5	0	5	0
I^HlGAT_65.84zto.68J
_ _ RGCN_64O9H-O.65J
GeCN_71.65+0.28〕
_ _ωmp,GCN(70.96H-1.89〕
_ _ PrO-GNN-712*1.5θ
^MlLatGQ/二72.32%α25J
FigUre W SembSUPerViSed-ass≡cation PerfOrmanCe Under MetattaCk (Zugner GUImeman 户 2019).
Accuracy (∕%)
4455667788
0505050505
Accuracy (∕%)
5	5	6	6	7	7
0	5	0	5	0	5.
Accuracy (∕%)
6	7	7	8	8	g
5	0	5	0	5	0
0.10 0.15 0.20 0.25
PertUrbat-on Rate (∕%)
PUbmed
Figure Sem'supervised CIaSSifiCation PerfOnnanCe Under RalIdOm AttaCk (Jin et al: 202Oa∙
5∙1∙2 CoMPARlSoN RESULTS
We first COmPare LatGCN With Some traditional baseHllemethOdSmClUdg Weighted mean—
type GCN (GCN—m) (KiPf 泮 WelHII2017; HamiltOIl et a2017Gmph Attention NetWOrkS
(GAT) (VelKkOviC et al: 2018aTo demonstrate the robustness Of LatGC2 we also COmPareLat—
GCN With SOme recent robust GNNSineluding RObUSt GraPh COnVUtionNetWOrkS (RGN) (ZhU
et al: 2019Simp—GCN (Jin et a2021ProPerty GmPh NeUmI NetWOrkS (PrO—GNN) (JiIl et a 厂
202Ob) and GmPh elastic COnVUtional NetWOrkS (GeCN)(Jiang et al: 2021
EffeCtiVeneSS analysis. Figure 3 and 4 SUmmariZe the COmPariSOIl results across different levels
Of MetattaCk (ZuglIer 泮 GUmlemaIl2019) and RalIdOm AttaCk (JiIl et a - 202Oarespectively.
For each attack IeVeLe results are the average performance OfIO πms With different IIetWOrk
initializationThe OVemlI average performance Of COmPariSOIl methods for all attack IeVeIS are
reported in the Iegelld Of eachgurHerwe CaIl ObSerVe that (I) TmditiOnal GCN—m (KiPf
尔 WelHn2017; HamiltOIl et a - 2017) and GAT (VelKkOviC et a - 5018a) are VUhIemble toe
StrUetUmI attacks and noiseCOmParing With themy LatGCN ObtainS ObViOUSly better PerfOnnanCe
UlIder various attacks and noiseThiS CIearly demonstrates the effectiveness and robustness Of the
PrOPOSed LatGCR module for robust graph data learning，(2) LatGCN ObtaillS better PerfonnanCe
than Some recent robust GNN methodsmudmg RGCN (ZhU et: 2019) and SimGeN (Jin et a 厂
2021WhiChindicates the more robustness Of the PrOPOSed LatGCN w.rt graph StrUCtUmI attacks
and noise(3) COmParilIg With SOme recent graph IeamilIg guided GNNsinClUdg Pro—GNN (JiIl
et a - 202Ob) and GeCNang et a2021LatGCN gene 邑 Iy ObtaillS the best average Ieaming
PerfOnnanCe on attacked graph data，ThiS further demonstrates the effectiveness Of the proposed
SeIf—supervised Iatent graph estimationLatGCR for noisy graph data representation
Edency analysis. Figure 5 ShIS the average running time Of each epoch in training LatGCN On
semsuPerviSed IIode ClaSSmCation tasks on the attacked datasets USed in WOrk (JiIl et a202Ob
AlI meOdSare implemented by PyTorch on NVIDIA A6000. We Can IIote that (1) The methods
6
Under review as a conference paper at ICLR 2022
Figure 5: The empirical average running time in each epoch of different methods.
using fixed graph GNNs, such as GCN-m, SimP-GCN (Jin et al., 2021) and RGCN (Zhu et al.,
2019), generally run faster than graph learning guided GNN methods. (2) Our LatGCN performs
obviously faster than most of graph learning guided methods including GAT (VelickoVic et al.,
2018a), GeCN (Jiang et al., 2021) and Pro-GNN (Jin et al., 2020b), especially on the larger dataset
Pubmed (Sen et al., 2008). It demonstrates the efficiency of the proposed LatGCN on conducting
robust graph data learning.
5.2	Unsupervised clustering
To evaluate the effectiveness of the proposed self-supervised LatGCR, we further test it on unsuper-
vised clustering tasks. Following the experimental setting in previous work (Zhang et al., 2019), we
first use Singular Value Decomposition (SVD) to replace projection step to obtain low-dimensional
embeddings for graph nodes. Then, we utilize LatGCR to obtain context-aware representations for
graph nodes and employ K-means clustering algorithm (Charu & Chandan, 2014) to obtain the final
clustering results (Zhang et al., 2019). We set the recurrent time and parameter λ to 1 and 0.03
respectively. Similar to work (Zhang et al., 2019), we adopt three widely used performance mea-
surements (Charu & Chandan, 2014), i.e., clustering accuracy (Acc), normalized mutual information
(NMI) and macro F1-score (F1) for evaluation.
We compare our LatGCR with some other popular clustering approaches including Graph Variational
Autoencoder (VGAE) (Kipf & Welling, 2016), Marginalized Graph Autoencoder (MGAE) (Wang
et al., 2017), Adversarially Regularized Variational Graph Autoencoder (ARVGE) (Pan et al., 2018)
and Attributed Graph Clustering (AGC) (Zhang et al., 2019). Table 1 summarizes the comparison
results on all original datasets (Sen et al., 2008). The results of these comparison methods have been
reported in work (Zhang et al., 2019) and we use them directly. From Table 1, we can note that
comparing with some other clustering methods, the proposed LatGCR can obtain the best average
performance in most cases, which further indicates the effectiveness of the proposed LatGCR on
conducting unsupervised clustering tasks.
Table 1: Comparison results on clustering task. The best results are marked by bold.
Method	Cora			Citeseer			Pubmed		
	Acc%	NMI%	F1%	Acc%	NMI%	F1%	Acc%	NMI%	F1%
VGAE	55.95	38.45	41.50	44.38	22.71	31.88	65.48	25.09	50.95
MGAE	63.43	45.57	38.01	63.56	39.75	39.49	43.88	8.16	41.98
ARVGE	63.80	45.00	62.70	54.40	26.10	52.90	58.22	20.62	23.04
AGC	68.92	53.68	65.61	67.00	41.13	62.48	69.78	31.59	68.72
LatGCR	69.19	53.48	65.50	67.83	42.07	63.31	70.52	32.77	69.71
7
Under review as a conference paper at ICLR 2022
5.3	Model analysis
5.3.1	Visualization results
To demonstrate the effectiveness of proposed LatGCR, we utilize 2D t-SNE (Van der Maaten &
Hinton, 2008) visualization to show the feature representation ability of LatGCR comparing with
baseline method GCN-m (Hamilton et al., 2017; Kipf & Welling, 2017). Figure 6 shows 2D t-
SNE (Van der Maaten & Hinton, 2008) visualization results of the feature maps output by the first
hidden layer of GCN-m and LatGCN on Cora and Citeseer (Sen et al., 2008; Jin et al., 2020b) datasets
under 0.25% Metattack (Zugner & Gunnemann, 2019). Different colors denote different classes. One
can note that, LatGCR obtains clearer and compacter embeddings than baseline method GCN-m,
which intuitively demonstrates that the proposed LatGCR can obtain more robust and discriminative
feature representations for graph data with structural attacks.
(a) GCN-m result on Cora dataset	(b) LatGCN result on Cora dataset
(c) GCN-m result on Citeseer dataset
(d) LatGCN result on Citeseer dataset
Figure 6: 2D t-SNE (Van der Maaten & Hinton, 2008) visualizations of the feature map output from
the first layer of GCN-m and LatGCN. Different colors denote different classes.
Table 2: Results of LatGCN with different parameter λ values.
λ	0.05	0.1	0.5	1	5	10	20
Cora	76.06	77.11	76.36	75.96	72.79	70.82	67.25
Citeseer	71.09	71.27	71.68	71.5	72.16	70.44	69.73
Pubmed	84.87	85.36	86.03	86.07	86.42	86.66	86.40
5.3.2	Parameters analysis
One main hyper-parameter in LatGCR is the balanced parameter λ (Eq.(3)). Table 2 shows the
semi-supervised classification results of LatGCN with different λ values on three datasets under
8
Under review as a conference paper at ICLR 2022
100% Random Attack (Jin et al., 2020a). We can see that our LatGCN can achieve relatively stable
results in a certain range of parameter λ value which indicates that LatGCR is generally insensitive to
the value of hyper-parameter λ in a certain range.
6	Conclusion
This paper proposes a novel Latent Graph Convolutional Representation (LatGCR) for robust graph
data representation and learning. LatGCR is proposed based on a joint reconstruction framework,
i.e., graph structure reconstruction + node’s feature reconstruction. It can estimate a latent and
flexible graph for GCR in a self-supervised way and provides a general basic block for GNNs. The
main advantage of LatGCR is that it can perform robustly w.r.t graph structural attacks and noises.
Experiments on several benchmark datasets demonstrate the effectiveness and robustness of LatGCR.
In our future work, we will extend LatGCR to address the data with multiple graphs and further apply
it on some more applications and tasks, such as computer vision, recommendation, etc.
References
Aggarwal Charu, C and Reddy Chandan, K. Data Clustering: Algorithms and Applications. CRC
Press, Boca Raton, 2014.
Yu Chen, Lingfei Wu, and Mohammed Zaki. Iterative deep graph learning for graph neural networks:
Better and robust node embeddings. In H. Larochelle, M. Ranzato, R. Hadsell, M. F. Balcan, and
H. Lin (eds.), Advances in Neural Information Processing Systems,pp. 19314-19326, 2020.
Michael Defferrard, Xavier Bresson, and Pierre Vandergheynst. Convolutional neural networks on
graphs with fast localized spectral filtering. In Advances in Neural Information Processing Systems,
pp. 3844-3852, 2016.
Simon Geisler, Daniel Zugner, and Stephan GUnnemann. Reliable graph neural networks via robust
aggregation. In H. Larochelle, M. Ranzato, R. Hadsell, M. F. Balcan, and H. Lin (eds.), Advances
in Neural Information Processing Systems, pp. 13272-13284, 2020.
William L Hamilton, Zhitao Ying, and Jure Leskovec. Inductive representation learning on large
graphs. In Advances in Neural Information Processing Systems (NIPS), pp. 1024-1034, 2017.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image
recognition. In 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp.
770-778, 2016.
Bo Jiang and Ziyan Zhang. Incomplete Graph Representation and Learning via Partial Graph Neural
Networks. arXiv e-prints, 2021.
Bo Jiang, Ziyan Zhang, Doudou Lin, Jin Tang, and Bin Luo. Semi-supervised learning with graph
learning-convolutional networks. In Proceedings of the IEEE Conference on Computer Vision and
Pattern Recognition, pp. 11313-11320, 2019.
Bo Jiang, Beibei Wang, Jin Tang, and Bin Luo. Gecns: Graph elastic convolutional networks for data
representation. IEEE Transactions on Pattern Analysis and Machine Intelligence, PP:1-1, 04 2021.
Wei Jin, Yaxin Li, Han Xu, Yiqi Wang, and Jiliang Tang. Adversarial attacks and defenses on graphs:
A review and empirical study. arXiv e-prints, pp. arXiv-2003, 2020a.
Wei Jin, Yao Ma, Xiaorui Liu, Xianfeng Tang, Suhang Wang, and Jiliang Tang. Graph structure
learning for robust graph neural networks. In Proceedings of the 26th ACM SIGKDD International
Conference on Knowledge Discovery & Data Mining, pp. 66-74, 2020b.
Wei Jin, Tyler Derr, Yiqi Wang, Yao Ma, Zitao Liu, and Jiliang Tang. Node similarity preserving
graph convolutional networks. In Proceedings of the 14th ACM International Conference on Web
Search and Data Mining, pp. 148-156, 2021.
Aleksandar Bojchevski Johannes Klicpera and Stephan GUnnemann. Combining neural networks
with personalized pagerank for classification on graphs. In ICLR, 2019.
9
Under review as a conference paper at ICLR 2022
Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In International
Conference on Learning Representations, 2015.
Thomas N Kipf and Max Welling. Variational graph auto-encoders. NIPS Workshop on Bayesian
Deep Learning, 2016.
Thomas N Kipf and Max Welling. Semi-supervised classification with graph convolutional networks.
In International Conference on Learning Representations, 2017.
Ruoyu Li, Sheng Wang, Feiyun Zhu, and Junzhou Huang. Adaptive graph convolutional neural
networks. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 32, 2018.
Shirui Pan, Ruiqi Hu, Guodong Long, Jing Jiang, Lina Yao, and Chengqi Zhang. Adversarially
regularized graph autoencoder for graph embedding. In Proceedings of the Twenty-Seventh
International Joint Conference on Artificial Intelligence, IJCAI-18, pp. 2609-2615, 2018.
Prithviraj Sen, Galileo Namata, Mustafa Bilgic, Lise Getoor, Brian Galligher, and Tina Eliassi-Rad.
Collective classification in network data. AI magazine, 29(3):93, 2008.
Laurens Van der Maaten and Geoffrey Hinton. Visualizing data using t-sne. Journal of machine
learning research, 9(11), 2008.
Petar VeliCkovic, GUillem CUcUrUlL Arantxa Casanova, Adriana Romero, Pietro Lio, and YoshUa
Bengio. Graph attention networks. In International Conference on Learning Representations,
2018a.
Petar VeliCkoviC, William FedUS, William L Hamilton, Pietro Lio, Yoshua Bengio, and R Devon
Hjelm. Deep graph infomax. In International Conference on Learning Representations, 2018b.
ChUn Wang, ShirUi Pan, GUodong Long, XingqUan ZhU, and Jing Jiang. Mgae: marginalized graph
aUtoenCoder for graph ClUstering. In CIKM’17 - Proceedings of the 2017 ACM Conference on
Information and Knowledge Management, pp. 889-898, 2017.
HUijUn WU, Chen Wang, YUriy Tyshetskiy, Andrew DoCherty, Kai LU, and Liming ZhU. Adversarial
examples for graph data: Deep insights into attaCk and defense. In Proceedings of the 28th
International Joint Conference on Artificial Intelligence, pp. 4816-4823. AAAI Press, 2019.
Liang Yang, Zesheng Kang, XiaoChUn Cao, Di Jin, Bo Yang, and YUanfang GUo. Topology optimiza-
tion based graph ConvolUtional network. In IJCAI, pp. 4054-4061, 2019.
Xiaotong Zhang, Han LiU, Qimai Li, and Xiao-Ming WU. AttribUted graph ClUstering via adaptive
graph ConvolUtion. In Proceedings of the Twenty-Eighth International Joint Conference on Artificial
Intelligence, (IJCAI), pp. 4327-4333, 2019.
DingyUan ZhU, Ziwei Zhang, Peng CUi, and WenwU ZhU. RobUst graph ConvolUtional networks
against adversarial attaCks. In Proceedings of the 25th ACM SIGKDD International Conference on
Knowledge Discovery amp; Data Mining, pp. 13991407. AssoCiation for CompUting MaChinery,
2019.
Daniel ZUgner and Stephan GUnnemann. Adversarial attacks on graph neUral networks via meta
learning. In International Conference on Learning Representations (ICLR), 2019.
10