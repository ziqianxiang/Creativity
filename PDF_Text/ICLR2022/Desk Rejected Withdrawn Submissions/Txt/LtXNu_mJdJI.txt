Under review as a conference paper at ICLR 2022
Mutual Information Continuity-constrained
Estimator
Anonymous authors
Paper under double-blind review
Ab stract
The estimation of mutual information (MI) is vital to a variety of applications in
machine learning. Recent developments in neural approaches have shown encour-
aging potential in estimating the MI between high-dimensional variables based
on their latent representations. However, these estimators are prone to high vari-
ances owing to the inevitable outlier events. Recent approaches mitigate the out-
lier issue by smoothing the partition function using clipping or averaging strate-
gies; however, these estimators either break the lower bound condition or sacrifice
the level of accuracy. Accordingly, we propose Mutual Information Continuity-
constrained Estimator (MICE). MICE alternatively smooths the partition function
by constraining the Lipschitz constant of the log-density ratio estimator, thus al-
leviating the induced variances without clipping or averaging. Our proposed esti-
mator outperforms most of the existing estimators in terms of bias and variance in
the standard benchmark. In addition, we propose an experiment extension based
on the standard benchmark, where variables are drawn from a multivariate normal
distribution with correlations between each sample in a batch. The experimental
results imply that when the i.i.d. assumption is unfulfilled, our proposed estimator
can be more accurate than the existing approaches in which the MI tends to be
underestimated. Finally, we demonstrate that MICE mitigates mode collapse in
the kernel density estimation task.
1	Introduction
Mutual information (MI) estimation is essential in various machine learning applications, including
learning representations (Oord et al., 2018; Chen et al., 2016; Bachman et al., 2019; Hjelm et al.,
2018; Sordoni et al., 2021), feature selection (Battiti,1994; Estevez et al., 2009), feature disentangle-
ment (Higgins et al., 2018; Esmaeili et al., 2019; Colombo et al., 2021), and reinforcement learning
(Oord et al., 2018; Bachman et al., 2019; Li et al., 2016). Some conventional non-parametric ap-
Proaches have been proposed to estimate MI (Estevez et al., 2009; Fraser & Swinney, 1986; Moon
et al., 1995; Kwak & Choi, 2002). Despite promising results, (Belghazi et al., 2018; Poole et al.,
2019) indicated that these estimators have limited capability to scale up well with the sample size or
dimension (Gao et al., 2015) therefore hard to be utilized in general purpose applications.
Recent studies focus on scalable MI estimation through variational bounds maximization (Oord
et al., 2018; Belghazi et al., 2018; Poole et al., 2019) or minimization (Cheng et al., 2020) using
neural networks or convex maximum-entropy method(Samo, 2021). These neural estimators have
been adopted in some remarkable self-supervised applications, such as computer vision (Chen et al.,
2020; He et al., 2020; Grill et al., 2020; Chen & He, 2020; Chen et al., 2021) and speech recognition
(Schneider et al., 2019; Baevski et al., 2019), with the aim of maximizing the shared information
between different views with respect to space or time. In MI estimation, the neural networks (also
known as the critics) has been used to approximate the log-density ratio. These MI estimators
generally characterize the Kullback-Leibler (KL) divergence (Kullback & Leibler, 1951) using a
dual representation and subsequently formulate MI lower bounds.
Although multiple applications have attained promising results, two significant issues have not been
fully addressed. As the first issue, the existing MI estimators can be debilitated by significant bias
and variance owing to inevitable outlier events. It was pointed out by (Poole et al., 2019; Song
& Ermon, 2020) that the exponential partition function causes a high-variance issue. It implies
1
Under review as a conference paper at ICLR 2022
that estimators leveraging f -divergence representations could suffer from the high-variance issue.
Numerous studies have been conducted to address this problem. Previous approaches such as Mutual
Information Neural Estimation (MINE) (Belghazi et al., 2018) and Contrastive Predictive Coding
(CPC) (Oord et al., 2018) reduce the variances by adopting different types of averaging. Based on
MINE, the Smoothed Mutual Information Lower-bound Estimator (SMILE) (Song & Ermon, 2020)
limits the range of the critic with a hyper-parameter, enabling estimates with low bias and variance.
For the second issue, as summarized in (Oord et al., 2018; Belghazi et al., 2018; Poole et al., 2019;
Nguyen et al., 2010), most of the existing MI estimators are tested on a standard benchmark where
random variables are drawn independently. However, the benchmark is insufficient for an analysis
of videos or audio signals in which data frames could be correlated.
In this paper, we address the high variance issue by a novel Mutual Information Continuity-
constrained Estimator (MICE) that constrains the Lipschitz constant of the critic by its spectral norm
(Miyato et al., 2018), and we block the unstable gradients generated from the partition function.
MICE is less underestimated in the extended benchmark because the partition function is smoothed
by the scale of the spectral norm instead of hard clipping, which could overly restrict the range of
the density ratio. The experimental results show that MICE has a competitive bias-variance trade-off
compared to SMILE in the standard benchmark, without selecting a clipping threshold. Based on
the standard benchmark, we propose an extension in which random variables are correlated within a
batch. Our proposed method is robust when samples are not independent compared to existing vari-
ational estimators that underestimate MI drastically when slight correlations are involved. Finally,
in the kernel density estimation (KDE) experiment, we demonstrate that using MICE as MI regular-
ization alleviates mode collapse (Che et al., 2016; Dumoulin et al., 2016; Srivastava et al., 2017) in
the training of generative adversarial networks (GANs) (Goodfellow et al., 2014). Our contributions
are as follows:
•	We address the high-variance issue of an existing unbiased estimator by constraining the
Lipschitz constant of log-density ratio estimator and gradient stabilization.
•	We prove that MICE is a strongly consistent estimator of MI.
•	In the proposed experiment extension, the results show that MICE outperforms existing
estimators under the condition in which the i.i.d. assumption is not fulfilled.
•	A GAN regularized by MICE can capture more modes in the KDE experiment and ease the
mode collapse problem.
2	Related Work
For a pair of random variables (X, Y ) over the probability space X × Y, the mutual information
I(X; Y ) between X and Y can be defined as the KL divergence of the joint distribution P(X,Y) and
the product of the marginals PX and PY :
I(X； Y) = DKL(P(X,Y)kPχ 0 PY)	⑴
where DKL is the KL divergence. Next, we start with a common characterization of KL diver-
gence, the Donsker-Varadhan (DV) representation (Donsker & Varadhan, 1983), which is adopted
by MINE (Belghazi et al., 2018) and SMILE (Song & Ermon, 2020).
Lemma 1 (Donsker-Varadhan (DV)) Given two probability distributions P and Q over X:
DKL(PkQ) = sup {EP[T] - log EQ[eT] , IDV}	(2)
T：XtR
for some bounded function T : X → R such that the expectations are finite.
In particular, ifP and Q are specified as P(X,Y) and PX 0 PY, MI can be estimated by maximizing
the DV representation. It should be noted that the equation holds when T = log dP/dQ + C for
some constant C ∈ R.
In (Broniatowski & Keziou, 2009; Nowozin et al., 2016), a general variational estimation of f-
divergences is introduced. For any convex, lower-semicontinuous function f, there exists a convex
conjugate f* such that f (u) = supt∈dom(f*){tu 一 f*(t)}, where U belongs to the domain of f.
2
Under review as a conference paper at ICLR 2022
Therefore, f -divergences can be estimated by taking supremum over an arbitrary class of functions
T : X → R:
Df(PkQ)= X
q(x)	sup
t∈dom(f *)
tp(X) - f *(t)}dx
(3)
≥ SUp {Ep[T] — Eq [f*(T)]}
T：XtR
(4)
The derivation form Equation 3 to Equation 4 is based on Jensen’s inequality because the supremum
is swapped out of the integration. Here, the KL divergence can be obtained by specifying f(u) =
U log u, thus f*(T) = eτ-1, yielding the NgUyen-Wainright-Jordan (NWJ) lower bound (Nguyen
et al., 2010). Similarly, MI can be estimated by setting P = P(χ,γ)and Q = PX 0 PY.
Lemma 2 (Nguyen, Wainright, and Jordan (NWJ) (Nguyen et al., 2010)) Given two probability
distributions P and Q over X ,
DKL(PkQ) ≥
sUp
Tθ: X→R
nEP [Tθ] - EQ [eTθ-1] , INWJ o
(5)
where the equation holds when Tq = 1+ log 彩.
Note that INWJ is unbiased since no nonlinear function is taken on the right-hand side out of the
expectation. Although IDV and INWJ are tight with a sufficient large hypothesis set of Tθ, the
partition function induces large variances. The following approaches aim to solve the high-variance
issue by averaging and clipping on the partition function. For instance, MINE (Belghazi et al., 2018)
proposed a neural information measure based on taking supremum of IDV over a neural network
Tθ : X × Y → R parameterized by θ.
Lemma 3 (Mutual Information Neural Estimation (MINE) (Belghazi et al., 2018)) Let P and Q be
two probability distributions over X
I(X； Y) ≥ SUp nEp(x Y)[Tθ] — IogEMA (EPX蜜Pγ[eτθ])，IMINEo	(6)
Tθ:X→R
In this manner, MINE collects cross-batch statistics to evaluate bias-corrected estimate, reducing the
bias and variance simultaneously. In contrast to MINE, which uses the exponential moving average
(EMA) to reduce variances induced from the partition function, (Song & Ermon, 2020) proposed to
reduce variances by putting limits on the range of the log-density ratio.
Lemma 4 (Smoothed Mutual Information Lower-bound Estimator (SMILE) (Song & Ermon, 2020))
Let P and Q be two probability distributions over X
I(X； Y) ≥ SUp ∣EP(XY)[Tθ] — logEPX0Pγ[emaXgm(Tθ,τIL)]，Ismile)	⑺
Tθ:X→R I	J
Another multi-sample estimator, Contrastive Predictive Coding (CPC) (Oord et al., 2018), uses the
cross-entropy between the positive and negative samples as an objective
n
1n
Enj- p(Xj,yj )	n ElOg
i=1
f(χi,yi)
n Pja)
, ICPC
(8)
where f(x, y) = ex Wy is a log-bilinear function with a trainable parameter W, and the expectation
is taken over the distribution with density Πj p(xj, yj). Noted that ICPC is tight when f(x, y) =
log p(y|x) + c(y), where c(y) is an arbitrary function that depends on y. However, (Oord et al.,
2018) indicated that this bound is loose when I(X; Y) > log n, requiring an exponentially large
batch size to achieve accurate estimates with high confidence (Song & Ermon, 2020).
3 Limitations on DV Representation
3.1	Maximum of Log-Density Ratio Estimate Dominating the Partition
Function
According to (Poole et al., 2019; Song & Ermon, 2020), the partition function EQ eTθ(x,y) is the
rationale behind high variances and biases. This expression is highly dependent on the maximum
3
Under review as a conference paper at ICLR 2022
of the log-density ratio in a batch. We demonstrate this by showing the relationship of LogSumExp
(LSE, also known as a smooth approximation to the maximum function) operation and the maximum
function as follows
LSE(Tθ(x1,y1), . . . ,Tθ(xn,yn-1)) > max{Tθ(x1, y1), . . . ,Tθ(xn,yn-1)}
n n-1
1	X X eTθ(Xi,yj) >	1	emax{Tθ(x1,y1),…,Tθ(xn,yn-ι)}	(9)
n(n - I) i=i j=i	n(n - I)
where Tθ(xi, yj) is the estimated log-density ratio log dP /dQ where x and y are drawn from Q.
Note that because Q is the product of marginals, the total number of Tθ sampled from Q is n(n -
1). (McAllester & Stratos, 2020) indicated that the partition function is dominated by extremely
rare events which are never observed through the sampling from Pχ 0 Pγ. They quantified the
probability of outlier events using the outlier risk lemma.
Lemma 5 (Outlier risk lemma (McAllester & Stratos, 2020)) Given n samples (n ≥ 2) that follow
the distribution Pχ and a property Φ[x] such that Pχ (Φ[x]) ≤ 1/n, the probability that no sample
x satisfies Φ[x]is at least 1/4.
Here, Pχ (Φ[x]) is the probability of drawing x from Pχ such that statement Φ[x] holds. Lemma 5
can be easily proved based on the probability of sampling with replacement.
Letting P = P(χ,Y) and Q = Pχ0PY, for DV representation, the best estimate of MI is established
when
EP [Tθ (x, y)] =I(X;Y)	(10)
EQ [eTθ(x,y)] = 1	(11)
The outlier risk lemma indicates that there is at least a probability of 1/4 that one can draw an
unseen variable such that EQ [eTθ(x,y)] > 1. By observing Equation 9, if a pair of unseen variables
(x0, y0) were sampled, the partition function will be larger than eτθ(x0,y0)∕(n(n - 1)); therefore, the
estimates of DV representation are of high bias and variance. Similarly, the best estimate of INWJ is
established with the same Equation 10, but Equation 11 should be modified as EQ [eTθ (x,y)-1] = 1.
3.2 Neither Upper Bound nor Lower Bound Estimators
Based on the aforementioned limitations of the DV representation, the IMINE and ISMILE focus on
controlling the variance of the partition function. IMINE reduces the variance by applying EMA to
the partition function over all previous samples. According to (McAllester & Stratos, 2020), the
worst case of the DV representation can be bounded under log n. Because IMINE implicitly enlarges
the batch size with the scale of iteration (i.e, the number of covered samples at the ith iteration is
i × n, where n is the batch size), it can leverage the linearly increasing batch size to reduce the
bias issue. Another method adopted by ISMILE is controlling the range of the partition function by
clipping the log-density ratio with a threshold τ in Equation 7.
In (Song & Ermon, 2020), the clipped density ratio rτ = max(min(eTθ(x,y), eτ), e-τ) is estimated
by n random variables over the distribution Q = Pχ 0 PY. The variance of the bounded partition
function EQ [rτ] satisfies Var[EQ [rτ]] ≤ (eτ - e-τ)2 /4n. According to (Song & Ermon, 2020),
a trade-off of the bias and variance can be determined by a threshold τ . Decreasing τ reduces the
variance, but increases bias with such choice.
Although these estimators mitigate the high-variance issue and attain more accurate estimates, they
are no longer upper or lower bounds on MI. This is because the modified partition function is no
longer a normalizing term. As MINE applies EMA to EQ eTθ (x,y) across batches, and there
is at least 1/4 chance that the outlier event occurs, the partition function eventually saturates at
eTmax /(4N 2 - N)), where Tmax is the maximum among all Tθ, and N is the amount of training
data. As the range of the partition function of ISMILE is limited within [e-τ, eτ], the MI would be
overestimated when the log-density ratio is larger than τ and would not be underestimated only if
τ → 0 because of Equation 11.
In a nutshell, although these neither upper bound nor lower bound estimators reached more accurate
MI estimates than IDV , these estimators could overestimate MI to some unknown extent as they are
4
Under review as a conference paper at ICLR 2022
not guaranteed to be bounded below the MI. Moreover, IMINE requires a large batch size to avoid
from yielding large errors; in addition, the development of a criterion of selecting a proper threshold
for ISMILE is also challenging.
4	Methodology
4.1 Mutual Information Continuity-constrained Estimator
To alleviate the issue of outlier events dominating the partition function, we adopt two strategies,
which are limiting the Lipschitz constant of the log-density ratio estimator and gradient stabiliza-
tion. The core idea of reducing variances is to smooth the critic. For instance, IMINE and ICPC
adopt averaging in different manners on the partition function to achieve a trade-off between the
bias and variance, and ISMILE directly truncates the value of density ratio using a hyper-parameter.
Clearly, these approaches have certain flaws in that averaging leads to high bias, and it requires prior
knowledge to choose the proper thresholds for clipping. To avert these issues, we utilize the spectral
normalization that constrains the spectral norm of the parameters in the last layer, and consequently
smooth the partition function. In (Miyato et al., 2018), the spectral norm of a weight matrix W is
defined as
σ(W) := max k，?k2 = max IlWhk2
(	)	h：h=0 ∣∣h∣∣2	khk2χ1k	k2
(12)
where h denotes any non-zero vector. The spectral norm σ(W) is equivalent to the largest singular
value of W. Therefore, σ(W) is independent from h, so the preconceptions regarding the data is no
longer required. For the weight matrix W l in the lth layer of Tl , spectral normalization normalizes
Wl with its spectral norm
Wl
σ(Wl)
(13)
where WSlN is the normalized weight matrix such that kTl kLip ≤ 1. Therefore, although we cannot
avoid sampling unseen variables, we can still constrain the maximum value of the partition function
by limiting the smoothness of the critic.
By leveraging the spectral normalization, we propose the Mutual Information Continuity-
constrained Estimator that smooths the critic
I(X ； Y ) = TS≡UP→R (S»)鹫NE)]- EPX 2
(x,y)-1i , IMICE o (14)
where TθSN is a critic normalized by the spectral norm of the last layer. In contrast to previous
approaches that focus on reducing the variances of the partition function, the proposed IMICE shares
the same parameters in both sides of Equation 14, and therefore it is guaranteed to not exceed the
MI.
To quantify the maximal variance of the log-density ratio, we assume that TθSN : Rd → R is a
multi-layer perceptron (MLP) with Lipschitz continuous activation functions.
Lemma 6 Let X be a random variable, and g(X) : Rd → R is an MLP with any Lipschitz continu-
ous activation function. Let Li be the Lipschitz constant of the ith layer, then
I
Var[g(X)] ≤EkX-E(X)k2YLi2
(15)
i=1
Here, we defer the proof in Section A.1. Lemma 6 shows that the variance of the critic is bounded
above by the product of the square of its Lipschitz constants in each layer. An inequality resembles
to Equation 9 that upper bounds the partition function is shown below
LSE(Tθ(x1,y1), . . . , Tθ(xn, yn-1)) ≤ max{Tθ(x1, y1), . . . ,Tθ(xn, yn-1)} + log n(n - 1)
n n-1
1 X X eTθ(Xi,yj) ≤	1	(gmax{Tθ(x1,y1),…，Tθ(xn,yn-ι)} + 11	(16)
n(n - 1)	n(n - 1)
Therefore, by Equation 15 and Equation 16, the variance of the partition function is reduced by
limiting the Lipschitz constant L of the critic and controlling the variance of X , and the estimate is
5
Under review as a conference paper at ICLR 2022
of lower variance with smaller L determined by the network during the optimization. Investigating
Equation 14, because the partition function is exponential, its gradient with respect to TθSN is still an
exponential function, which causes the training to become unstable. Therefore, to further mitigate
the high variance issue and stabilize the gradients, we avoid gradients generated by the partition
function from back-propagating and consequently stabilize the gradients. The training procedure
using gradient stabilization is presented in Algorithm 1.
Algorithm 1: Mutual Information Continuity-constrained Estimator (MICE)
θ — initialize network parameters from uniform distribution U (-	, yp1^;
while not converge do
Draw n pair of samples (x1, y1), . . . , (xn, yn) from the joint distribution P(X,Y)
Forward pass of MICE:
τSN(χ,y) - MLPe (χ,y)
Imice(Θ) 一 nl Pi=IT(SN(Xi, Q- log n(n-i)Pi=j eTSN(Xi,yj)
Compute the gradients on the left-hand side of IMICE with respect to θ:
G(θ) 一 VeIMfCe(Θ)
Update the network parameters:
θ J θ + G(θ)
end
4.2 Consistency
According to (Belghazi et al., 2018), an estimator In (X; Y ) constructed using a statistics network
over n samples is strongly consistent if for all > 0, and there exists a positive integer N such that
∀n ≥ N, |I(X; Y) -In (X;Y)| ≤ , a.e.	(17)
Then, the authors separate the consistency question into approximation and estimation problems.
In summary, to prove that MICE is strongly consistent, we first prove that there exists a neural
network Te parameterized by θ in some compact domain Θ ∈ R, such that for all > 0, |I(X; Y ) -
IΘ(X; Y )| ≤ , a.e. This ensures the existence of neural networks that can approximate the MI with
arbitrary accuracy. Second, we prove that given a family of neural networks Te in some bounded
domain, for all > 0, there exists an N ∈ N such that for all n ≥ N, |In (X; Y ) - IΘ(X; Y )| ≤
, a.e., ensuring that given sufficient number of samples, one can estimate the MI with some statistics
networks over samples. Combining the above two results with triangular inequality, we conclude
that MICE is strongly consistent. We provide the details of the proofs in Section A.2.
5	Experiments
5.1	Standard Benchmark
Dataset. The standard benchmark (Belghazi et al., 2018; Poole et al., 2019; Song & Ermon, 2020)
contains two tasks, the Gaussian task and the Cubic task. For both tasks, we sample n random
variables X, Y ∈ Rd for a batch from a standard multivariate normal distribution with correlation
ρ between X and Y . For the Cubic task, to examine how much the MI estimators degrade when a
nonlinear transformation involved, we estimate I(X; Y3) = I(X; Y ), which does not change the
MI.
Critics. Following previous studies (Belghazi et al., 2018; Poole et al., 2019; Song & Ermon, 2020),
we consider two types of critics: the joint critic (Belghazi et al., 2018) and the separable critic
(Oord et al., 2018). The joint critic first lists all combinations of all random variables in a batch
and computes the log-density ratio with an MLP R2d → R. The separable critic applies nonlinear
mapping to the inputs with two MLPs, f, g : Rd → Rd0, and subsequently estimates log-density
ratio by hf, gi. The joint critic compares all combinations, having the computational complexity of
O(n2), and since the computation of f and g can be paralleled, thus having a complexity of O(n).
In Figure 1, we show the performance of each estimator under different MI. The top row shows the
Gaussian task, and the bottom row shows the Cubic task. As described in Section 2, ICPC is highly
6
Under review as a conference paper at ICLR 2022
NWl	SMILE (τ=1.0)	SMILE (τ=≈)
-BnCrX
0、	，	，	，
O	50∞	IOOOO UOOO 200∞
Steps
CPC
o-.	.	.	.
O	5000	IOOOO	15∞0	20000
NWJ
。、 ， ， ，
O	5∞0 1∞∞ UOOO 2∞∞
SMILE (τ= 1.0)
。、 ， ， ，
O	5∞0	IOOOO 15000 2∞∞
SMILE (τ=oo)
MICE
cb2=w
。、 ， ， ，
O	50∞	IOOOO 15∞0	20000
Steps
MICE
O 50∞ IOOOO 15000	200∞
Steps
O 50∞	10000	15000	20000
O 5∞0	1∞∞ uα∞ 2∞∞
O 5∞0	10000	15000	2∞∞
cb2=w
O 50∞ IOOOO 15∞0	200∞
Steps
Figure 1: Performance of estimators in the standard benchmark. The top row shows the performance
of each estimator in the standard benchmark, and the bottom row shows the performance of MI
estimators with nonlinear transformation, namely Y 7→ Y 3. For the two experiments, we increased
the MI by 2 every 4000 iterations, and 20000 iterations in total. The ground truth of MI is marked
black. The light/dark color lines are the real estimates and their smoothed values.
Gaussian

2	4	6	8	10	2	4	6	8	10
Ml
Cubic
2	4	6	8	10
2	4	6	8	10	2	4	6	8	10	2	4	6	8	10
Ml
Figure 2: Bias-variance trade-offs of estimators in the standard benchmark. The top row shows the
performance of each estimator in the Gaussian task, and the bottom row show the performance in
the Cubic task.
biased and bounded above by log n, and the variance of INWJ increases along with the ground truth
MI. Here, ISMILE (τ = 1.0) and IMICE have overall lower biases and variances, as compared to
ICPC and INWJ using both critics. Because ISMILE is neither an upper bound nor lower bound on
MI, MI estimates in the Gaussian task are sometimes slightly overestimated, but the moving mean
of IMICE is almost not exceeding the ground truth of MI. In the Cubic task, the joint critic degrades
more severely than the separable critic for most of estimators, except INWJ .
We show the bias-variance trade-offs of estimators using the separable critic in Figure 2, where the
top row illustrates the results of the Gaussian task, and the results of the Cubic task are shown at the
bottom row. It is observed that ICPC is severely biased, but the variance is much lower than all the
other approaches. Although INWJ is theoretically unbiased, it has large bias owing to the inevitable
outliers, and the variance grows up exponentially with MI as (Song & Ermon, 2020) pointed out.
IMICE leverages the unbiasedness of INWJ and further reduces the variance by constraining the
Lipschitz constant of the critic and gradient stabilization. Comparing result of ISMILE and IMICE
using the joint critic, IMICE converges faster than ISMILE. Itis possibly benefited from the stabilized
7
Under review as a conference paper at ICLR 2022
gradients. However, because we limit the Lipschitz constants in some layers of the critic, this could
lead to lower flexibility, and thus IMICE is slightly more biased than ISMILE in the Cubic task. In
brief, IMICE simultaneously guarantees not to exceed MI and remarkably relaxes the high-variance
issue of INWJ.
5.2	Extension of S tandard Benchmark
Sampling scheme. Next, we evaluate the MI estimators using an extension experiment based on the
standard benchmark. As described in Section 5.1, random variables are sampled independently; that
is, no correlations between samples is considered. However, we believe that, for practical scenarios,
it is extremely difficult for one to create a batch in which all samples are independent. Therefore,
based on the standard benchmark, we established an extension experiment in which random vari-
ables are sampled using the scheme below:
Xi = pXi-ι + 1- - p2e, ∀i = 2, ..., n
yi = ρxi + 1- ρ2, ∀i =1, . . . , n
(18)
(19)
where x1 and e are d-dimensional random variables following a standard normal distribution
N(0, Id). Sampling variables using Equation 18 and Equation 19 is equivalent to sample X =
{x1, . . . , xn} and Y = {y1, . . . , yn} from a multivariate normal distribution
X,Y 〜N 0,
Σx
ρΣx
一 Id
ρId
P2Id
LPnTId
PId
Id
PId
.
.
.
Pn-2Id
P2 Id
PId
Id
.
.
.
Pn-3Id
Pn-1Id]
Pn-2Id
Pn-3Id
Id
where P is the correlation between each pair of two consecutive samples, i.e., Xi, Xi+ι and y%,
yi+1. In the extension benchmark, we follow the setting in Section 5.1 with an additional setting
P = 0.1, and the ground truth MI is increased by 2 after 4θ00 iterations during training. In general,
the correlation coefficient less than 0.3 is considered to be weak. As shown in Figure 3, data with
correlation of 0.1, which is even weaker than 0.3, degenerates other estimators, whereas IMICE
still has relatively accurate estimates. To further explore this effect, additional experiments using
different settings of P are presented in Section A.3.
Gaussian
4	6	8	10
2	4	6	8	10
Cubic
山SW
2	4	6	8	10	2	4	6	8	10
Ml
Figure 3: Bias-variance trade-offs of estimators in the extension benchmark. The top row shows the
performance of each estimator in the Gaussian task, and the bottom row show the performance in
the Cubic task. IMICE is less biased and more accurate than the other estimators.
In Figure 3, we demonstrate that the bias and MSE of the estimate of IMICE are much lower than
those of the other estimators using the separable critic for both the Gaussian task and the Cubic task.
8
Under review as a conference paper at ICLR 2022
There are two possible reasons that IMICE outperforms the other approaches. First, as we stated in
Section 3, the partition function could be dominated by the nonzero log-density ratios when the
correlations between samples are involved. The other reason is that the gradients are stabilized by
applying spectral normalization to the critic and blocking the gradients generated by the partition
function.
5.3	Regularizing GAN with MICE
GANs (Goodfellow et al., 2014) have recently shown powerful capabilities in real-world data gen-
eration. However, the well-known mode collapse agonizes GANs with the consequence of limited
diversity. This is because the discriminator does not require the generator to capture all modes to
decrease the loss function. (Belghazi et al., 2018) proposed to alleviate mode collapse by involving
code variables C, and jointly maximize the MI between the generated data and C . Formally, a GAN
regularized by MICE alternately optimizes the following two objectives:
LD :=EPX[logD(X)]+EPZ[log(1-D(G(Z))]	(20)
LG := EPZ [log(1 - D(G(z)))] - βIMICE(G(Z, C); C)	(21)
where D, G are the discriminator and the generator, and Z follows a standard uniform distribution.
Comparing the results of vanilla GAN and GAN + MICE in Figure 4, a vanilla GAN fails to model
the structure, whereas GAN + MICE captures all 25 modes, showing the efficacy of mode collapse
mitigation.
Figure 4: Results of GAN and MICE regluarization on 25 Gaussians dataset. Illustration on the left
is the target samples. The middle and the right plots are generated by a vanilla GAN and generated
by GAN regularized by MI estimated by MICE.
6	Conclusion
In this study, we comprehensively discuss the attributes and the limitations of existing approaches to
variational MI estimation. We show that energy-based estimators such as INWJ, and IDV are of high
variances because they are susceptible to the outlier events. Although neither upper bound nor lower
bound estimators achieve much more accurate approximations to MI in the standard benchmark,
they are under the risk of overestimating the MI. To address the above mentioned issues, we propose
a unbiased and consistent estimator of MI, IMICE, which has been proven free from overestimation
of the MI. We also argue that the standard benchmark is insufficient for evaluation since samples
can hardly be entirely uncorrelated in general cases. Therefore, we employ an additional benchmark
to evaluate the performance of the estimators in which the samples are correlated. In the standard
benchmark, the proposed IMICE has a slightly better performance than ISMILE without prior knowl-
edge for selecting clipping threshold. We empirically show that IMICE is more accurate than other
estimators in the proposed additional benchmark. Finally, we show that regularizing GANs with
MICE improves the ability of the GAN to capture multiple modes and consequently mitigate mode
collapse.
9
Under review as a conference paper at ICLR 2022
References
Abien Fred Agarap. Deep learning using rectified linear units (relu). arXiv preprint
arXiv:1803.08375, 2018.
Philip Bachman, R Devon Hjelm, and William Buchwalter. Learning representations by maximizing
mutual information across views. arXiv preprint arXiv:1906.00910, 2019.
Alexei Baevski, Steffen Schneider, and Michael Auli. vq-wav2vec: Self-supervised learning of
discrete speech representations. arXiv preprint arXiv:1910.05453, 2019.
Roberto Battiti. Using mutual information for selecting features in supervised neural net learning.
IEEE Transactions on neural networks, 5(4):537-550,1994.
Mohamed Ishmael Belghazi, Aristide Baratin, Sai Rajeswar, Sherjil Ozair, Yoshua Bengio, Aaron
Courville, and R Devon Hjelm. Mine: mutual information neural estimation. arXiv preprint
arXiv:1801.04062, 2018.
Michel Broniatowski and Amor Keziou. Parametric estimation and tests through divergences and
the duality technique. Journal of Multivariate Analysis, 100(1):16-36, 2009.
Tong Che, Yanran Li, Athul Paul Jacob, Yoshua Bengio, and Wenjie Li. Mode regularized generative
adversarial networks. arXiv preprint arXiv:1612.02136, 2016.
Junya Chen, Zhe Gan, Xuan Li, Qing Guo, Liqun Chen, Shuyang Gao, Tagyoung Chung, Yi Xu,
Belinda Zeng, Wenlian Lu, et al. Simpler, faster, stronger: Breaking the log-k curse on contrastive
learners with flatnce. arXiv preprint arXiv:2107.01152, 2021.
Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. A simple framework for
contrastive learning of visual representations. In International conference on machine learning,
pp. 1597-1607. PMLR, 2020.
Xi Chen, Yan Duan, Rein Houthooft, John Schulman, Ilya Sutskever, and Pieter Abbeel. Info-
gan: Interpretable representation learning by information maximizing generative adversarial nets.
arXiv preprint arXiv:1606.03657, 2016.
Xinlei Chen and Kaiming He. Exploring simple siamese representation learning. arXiv preprint
arXiv:2011.10566, 2020.
Pengyu Cheng, Weituo Hao, Shuyang Dai, Jiachang Liu, Zhe Gan, and Lawrence Carin. Club: A
contrastive log-ratio upper bound of mutual information. In International Conference on Machine
Learning, pp. 1779-1788. PMLR, 2020.
Pierre Colombo, Chloe Clavel, and Pablo Piantanida. A novel estimator of mutual information for
learning to disentangle textual representations. arXiv preprint arXiv:2105.02685, 2021.
Monroe D Donsker and SR Srinivasa Varadhan. Asymptotic evaluation of certain markov process
expectations for large time. iv. Communications on Pure and Applied Mathematics, 36(2):183-
212, 1983.
Vincent Dumoulin, Ishmael Belghazi, Ben Poole, Olivier Mastropietro, Alex Lamb, Martin Ar-
jovsky, and Aaron Courville. Adversarially learned inference. arXiv preprint arXiv:1606.00704,
2016.
Babak Esmaeili, Hao Wu, Sarthak Jain, Alican Bozkurt, Narayanaswamy Siddharth, Brooks Paige,
Dana H Brooks, Jennifer Dy, and Jan-Willem Meent. Structured disentangled representations.
In The 22nd International Conference on Artificial Intelligence and Statistics, pp. 2525-2534.
PMLR, 2019.
Pablo A Estevez, Michel Tesmer, Claudio A Perez, and Jacek M Zurada. Normalized mutual infor-
mation feature selection. IEEE Transactions on neural networks, 20(2):189-201, 2009.
Andrew M Fraser and Harry L Swinney. Independent coordinates for strange attractors from mutual
information. Physical review A, 33(2):1134, 1986.
10
Under review as a conference paper at ICLR 2022
Shuyang Gao, Greg Ver Steeg, and Aram Galstyan. Efficient estimation of mutual information for
strongly dependent variables. In Artificial intelligence and statistics, pp. 277-286. PMLR, 2015.
Sara A Geer and Sara van de Geer. Empirical Processes in M-estimation, volume 6. Cambridge
university press, 2000.
Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair,
Aaron Courville, and Yoshua Bengio. Generative adversarial nets. Advances in neural information
processing systems, 27, 2014.
Jean-Bastien Grill, Florian Strub, Florent Altche, Corentin Tallec, Pierre H Richemond, Elena
Buchatskaya, Carl Doersch, Bernardo Avila Pires, Zhaohan Daniel Guo, Mohammad Gheshlaghi
Azar, et al. Bootstrap your own latent: Anew approach to self-supervised learning. arXiv preprint
arXiv:2006.07733, 2020.
Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick. Momentum contrast for
unsupervised visual representation learning. In Proceedings of the IEEE/CVF Conference on
Computer Vision and Pattern Recognition, pp. 9729-9738, 2020.
Irina Higgins, David Amos, David Pfau, Sebastien Racaniere, Loic Matthey, Danilo Rezende,
and Alexander Lerchner. Towards a definition of disentangled representations. arXiv preprint
arXiv:1812.02230, 2018.
R Devon Hjelm, Alex Fedorov, Samuel Lavoie-Marchildon, Karan Grewal, Phil Bachman, Adam
Trischler, and Yoshua Bengio. Learning deep representations by mutual information estimation
and maximization. arXiv preprint arXiv:1808.06670, 2018.
Kurt Hornik, Maxwell Stinchcombe, and Halbert White. Multilayer feedforward networks are uni-
versal approximators. Neural networks, 2(5):359-366, 1989.
Solomon Kullback and Richard A Leibler. On information and sufficiency. The annals of mathe-
matical statistics, 22(1):79-86, 1951.
Nojun Kwak and Chong-Ho Choi. Input feature selection by mutual information based on parzen
window. IEEE transactions on pattern analysis and machine intelligence, 24(12):1667-1671,
2002.
Jiwei Li, Will Monroe, Alan Ritter, Michel Galley, Jianfeng Gao, and Dan Jurafsky. Deep reinforce-
ment learning for dialogue generation. arXiv preprint arXiv:1606.01541, 2016.
David McAllester and Karl Stratos. Formal limitations on the measurement of mutual information.
In AISTATS, 2020.
Takeru Miyato, Toshiki Kataoka, Masanori Koyama, and Yuichi Yoshida. Spectral normalization
for generative adversarial networks. ICLR, 2018.
Young-Il Moon, Balaji Rajagopalan, and Upmanu Lall. Estimation of mutual information using
kernel density estimators. Physical Review E, 52(3):2318, 1995.
XuanLong Nguyen, Martin J Wainwright, and Michael I Jordan. Estimating divergence functionals
and the likelihood ratio by convex risk minimization. IEEE Transactions on Information Theory,
56(11):5847-5861, 2010.
Sebastian Nowozin, Botond Cseke, and Ryota Tomioka. f-gan: Training generative neural samplers
using variational divergence minimization. arXiv preprint arXiv:1606.00709, 2016.
Aaron van den Oord, Yazhe Li, and Oriol Vinyals. Representation learning with contrastive predic-
tive coding. arXiv preprint arXiv:1807.03748, 2018.
Ben Poole, Sherjil Ozair, Aaron Van Den Oord, Alex Alemi, and George Tucker. On variational
bounds of mutual information. In International Conference on Machine Learning, pp. 5171-
5180. PMLR, 2019.
11
Under review as a conference paper at ICLR 2022
Yves-Laurent Kom Samo. Inductive mutual information estimation: A convex maximum-entropy
copula approach. In International Conference on Artificial Intelligence and Statistics, pp. 2242-
2250. PMLR, 2021.
Steffen Schneider, Alexei Baevski, Ronan Collobert, and Michael Auli. wav2vec: Unsupervised
pre-training for speech recognition. arXiv preprint arXiv:1904.05862, 2019.
Jiaming Song and Stefano Ermon. Understanding the limitations of variational mutual information
estimators. In ICLR, 2020.
Alessandro Sordoni, Nouha Dziri, Hannes Schulz, Geoff Gordon, Philip Bachman, and Remi Ta-
chet Des Combes. Decomposed mutual information estimation for contrastive representation
learning. volume 139, pp. 9859-9869. PMLR, 2021.
Akash Srivastava, Lazar Valkov, Chris Russell, Michael U Gutmann, and Charles Sutton. Veegan:
Reducing mode collapse in gans using implicit variational learning. In Proceedings of the 31st
International Conference on Neural Information Processing Systems, pp. 3310-3320, 2017.
A Appendix
A.1 Proof of Lemma 6
Lemma 6 Let X be a random variable, and g(X) : Rd → R is an MLP with any Lipschitz continu-
ous activation function. Let Li be the Lipschitz constant of the ith layer, then
I
Var[g(X)] ≤EkX-E(X)k2 YLi2	(22)
i=1
Proof. First, we consider the i-th layer fi with a Lipschitz continuous activation function, and fi has
Lipschitz constant Li , then
Var[fi(X)] :=E (fi(X) -E[fi(X)])2	(23)
≤E(fi(X)-fi(E[X]))2	(24)
≤Li2EkX-E[X]k2	(25)
The first inequality stems from the fact that the mean of a random variable is the constant with
the smallest MSE. By the definition of Lipschitz continuity, the second inequality holds because
Li is the Lipschitz constant of fi. Second, let g be the composite function of f1, f2, . . . , fI that
g = fι ◦ f2 ◦…◦ fɪ, where I is the number of layers in g, then
I
Var[g(X)] ≤EkX-E(X)k2 YLi2	(26)
i=1
which completes the proof.
A.2 Proof of Consistency
The proof of consistency generally follows the proofs in (Belghazi et al., 2018) yet with some mod-
ifications to fit MICE. To prove that MICE is strongly consistent, we first prove that for all > 0,
there exists a class of neural networks Tθ parameterized by θ in some compact domain Θ such that
|I(X; Y) - Iθ(X; Y)| ≤ C	(27)
Next, we prove that given > 0, there exists N ∈ N such that
∣In(X； Y) - Iθ(X; Y)| ≤ C	(28)
As consequence, combining the above results with triangular inequality, we have ∀n ≥
N, |I(X; Y) - In(X; Y)| ≤ C, which proves the consistency of MICE.
12
Under review as a conference paper at ICLR 2022
Proof. Let the optimal critic T* = log 器,where P and Q denote the joint distribution P(x,y)and
the product of marginals PXPY of the continuous random variables X and Y , respectively. By the
definition of INWJ, we have
I(X ； Y) - Iθ(X ； Y ) = EP [T * - T ]+ Eq [eτ *-1 - eTθ-1]	(29)
Next, according to the universal approximation theorem (Hornik et al., 1989), one can choose a Tθ
such that
Ep|T* — Tθ | ≤ 2	(30)
Eq |T* — Tθ | ≤ We-Tmax+1	(31)
where T* is upper bounded above by Tmaχ. Because exp(∙) is Lipschitz continuous with constant
eτmaxτ on (-∞, eτmaχT], EQIeT T — eτθ-1∣ ≤ eTmax-1EQ∣T* — T⅛|, and consequently We
have
EQIeT*-1-eτθ-1I ≤ I	(32)
Combine Equation 29, Equation 30, and Equation 32 with triangular inequality, we have
|I(X; Y) — Iθ(X； Y)∣ ≤ Ep|T* — TθI + EPIeT*-1 — eτθ-1∣ ≤ ∣	(33)
So far we have proved that for T* ≤ Tmax, Equation 27 holds. Next, we consider a subset that
{T* > Tmax} for a suitably chosen large value of Tmax . Here, let A be the subset belongs to the
input domain, we use the indicator function 1A to partition the input domain. By the Lebesgue
dominated convergence theorem, since that T* and eT* are integrable w.r.t. P and Q, we could
choose Tmax so that
Ep [1t* >Tmax(T*)] ≤ ∣	(34)
Eq[1t*>Tmaχ(eτ*-1)] ≤ ∣	(35)
Again, we can choose a function Tθ ≤ Tmax such that
Ep|T* — Tθ I ≤ ∣	(36)
Eq 1τ*≤Tmax(IT* — TθI) ≤ Ie-Tmax+1	(37)
Combining Equation 35 and Equation 37 together
EQ[eT*-1 — eTθ-1] = EQ[1T*≤Tmax(eT*-1 — eTθ-1)] + EQ[1T*>Tmax(eT*-1 — eTθ-1)]
≤ eτmax-1EQ[1τ*≤Tmax(T* — Tθ)]+ Eq[1t*>Tmax(eτ*-1)]
I
≤ 2	(38)
Similar to the derivation of Equation 33, put Equation 36 and Equation 38 together we obtain
∀2> 0, II(X; Y) — IΘ(X; Y)I≤ 2	(39)
For the estimation problem, letI > 0 and given Tθ in some compact domain Θ ⊂ Rd , there exists a
positive integer N such that
∀n ≥ N, IIn(X； Y) — Iθ(X； Y)I≤ 2	(40)
Here, we denote Pn and Qn as the empirical version of P and Q respectively, and In is the MI
estimation with n samples. By triangular inequality we have
IIn(X； Y) — Iθ(X； Y)I ≤ SUp {IEPn[Tθ] — Ep[Tθ]I + IEQn [eτθ-1] — EQ[eτθ-1]I}	(41)
θ∈Θ
Since Θ is compact (therefore bounded) and neural networks are continuous, Tθ and eTθ satisfiy the
uniform law of large numbers (Geer & van de Geer, 2000). Therefore, given I > 0 we can choose a
positive integer N such that ∀n ≥ N and with probability one, then
SUP { IEPn [Tθ] — Ep [T6]I} ≤ I
θ∈Θ	2
sup {IEQn [eτθ-1] - EQ[eτθ-1]I} ≤ ∣
θ∈Θ	2
(42)
(43)
13
Under review as a conference paper at ICLR 2022
According to the three inequalities above we derive Equation 40.
Finally, combining Equation 39 and Equation 40 with triangular inequality, let > 0 and δ = 2,
and there exists a positive integer N such that
Vn ≥ N,	|I(X; Y) - In(X; Y)|	≤	|I(X; Y) -	Iθ(X; Y)| +	IIn(X;	Y)	-	Iθ(X; Y)|	≤ δ (44)
which completes the proof.
A.3 Additional Experiments
Experimental Settings. The experiments in Section 5.1 and Section 5.2 are established using a
GTX 1080 Ti GPU with 11 GB VRAM. For the MLPs utilized in the joint/separable critic have the
input dimension of 20, two hidden layers of 256 hidden dimension, and the output dimension is 32.
In addition, ReLU Agarap (2018) is used as the activation function for both critics.
Performance of MI Estimators under Specific Correlations. We compare the performance of
the MI estimators under specific ρ settings (p=0.1, 0.2, 0.3, 0.4, and 0.5) using the separable critic.
As shown in Figure 5, the MI estimators are more biased with larger correlation between samples.
Among the MI estimators, INWJ is the most biased since neither the partition function nor the critic
is constrained, so the outliers lead to large variances and biases, and this is the same reason that
causes ISMILE(τ=∞) to be inaccurate. As mentioned in Section 2, the estimates of ISMILE(τ=∞)
are more accurate than that of INWJ because ISMILE(τ=∞) is equivalent to IDV which is sharper
than INWJ. Despite that ICPC is bounded above by log n, it is consistent under different settings
of correlation. ISMILE(τ=1.0) is of low variance and bias comparing to itself when τ = ∞, but
the improvement is mainly on reducing the variance. Comparing to the other MI estimators, our
proposed IMICE is the least biased, and is robust when correlations between samples involved.
CPC
SMILE (τ=1.0)
CPC
MICE
SMILE (T = 8)
MICE
ιβ
io
β
β
6
6
4
4
?
2
O 50∞	1∞∞	15000	200∞ O 50∞
Figure 5: Performance of estimators in the extension benchmark of different ρ. The top row shows
the performance of each estimator in the Gaussian task, and the bottom row show the performance
in the Cubic task. The correlation P between samples ranges from 0.1 to 0.5.
O
1∞∞	15000	200∞ O 50∞	1∞∞	15000	200∞ O 50∞	1∞∞	15000	200∞
Randomly Selected p. We provide an experiment that correlations between samples are randomly
initialized, which is a more complicated configuration than the extension benchmark in Section 5.2.
Here, ρ are randomly initialized from a uniform distribution that ranges from 0.0 to 0.5. In Figure 6,
each estimator using the separable critic has an average performance in Figure 5. The proposed
IMICE benefits the separable critic that it is robust to random correlations. In Figure 7, we also
observed that the variance of INWJ is very sensitive to the data because the right-hand side of
Eqn. Equation 5 is an exponential function without logarithm in IDV , and consequently yields high
MSE.
By constraining the continuity and gradient stabilization, IMICE is robust when correlation between
samples involved as compared with the other estimators, especially for the separable critic. This
could benefit large scale training that requires a light weight model structure for the critic.
14
Under review as a conference paper at ICLR 2022
CPC	NWI	SMILE (τ=1.0)	SMILE (τ=∞)	MICE
2-
θ`.
50∞	1∞∞ UOOO 200∞	0
Steps
CPC
50∞	1∞∞	15000	200∞
2-
O-.	.	.	.
0	5000	IOOOO	15000	200∞
2-
O-.	.	.	.
0	5000	IOOOO	15000	200∞
2-
O-.	.	.	.
0	5000	IOOOO	15000	20000
Joint critic
Separablecrttlc
1⅛∙M1
∣09(bs)
5000
IOOOO 15000	20000 O
Steps
NWJ
50∞	1∞∞	15000	200∞ O
SMILE (τ=1.0)
SMILE (τ = oo)
MICE
50∞	1∞∞	15000	200∞ O 50∞	1∞∞	15000	200∞ O 50∞	1∞∞	15000	200∞
Figure 6:	Performance of estimators in the extension benchmark of a random selection of ρ. The
top row shows the performance of each estimator in the Gaussian task, and the bottom row show
the performance in the Cubic task. The correlation ρ between samples uniformly ranges from 0.0 to
0.5.
Gaussian
2	4	6	8	10
2	4	6	8	10	2	4	6	8	10
Ml
Figure 7:	Bias-variance trade-offs of estimators in the extension benchmark with random ρ. The top
row shows the performance of each estimator in the Gaussian task, and the bottom row show the
performance in the Cubic task. IMICE is less biased and more accurate than the other estimators.
15
Under review as a conference paper at ICLR 2022

----Jolntcrltic
Separable critic
----7⅛*M1
---l09{bS)
CPC
Sns-X
----Jolntcrltic
----Separable critic
----7⅛*M1
---IO9{bS)
NWI	SMILE (τ=1.0)	SMILE (τ=∞)
MICE
o`.
O
50∞
Steps
50∞	1∞∞	15000
o`.
20000 O
50∞	1∞∞ UOOO 200∞
50∞	1∞∞	15000	200∞ O
Figure 8:	Performance of estimators in the extension benchmark (Section 5.2 related). The top
row shows the performance of each estimator in the Gaussian task, and the bottom row show the
performance in the Cubic task.
16