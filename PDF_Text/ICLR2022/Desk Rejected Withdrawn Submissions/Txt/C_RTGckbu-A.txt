Under review as a conference paper at ICLR 2022
Multi-Subspace S tructured Meta-Learning
Anonymous authors
Paper under double-blind review
Ab stract
Meta-learning aims to extract meta-knowledge from historical tasks to accelerate
learning on new tasks. A critical challenge in meta-learning is to handle task het-
erogeneity, i.e., tasks lie in different distributions. Unlike typical meta-learning al-
gorithms that learn a globally shared initialization, recent structured meta-learning
algorithms formulate tasks into multiple groups and learn an initialization for tasks
in each group using centroid-based clustering. However, those algorithms still
require task models in the same group to be close together and fail to take ad-
vantage of negative correlations between tasks. In this paper, task models are
formulated into a subspace structure. We propose a MUlti-Subspace structured
Meta-Learning (MUSML) algorithm to learn the subspace bases. We establish
the convergence and analyze the generalization performance. Experimental re-
sults confirm the effectiveness of the proposed MUSML algorithm.
1	Introduction
Humans are capable of learning new tasks from a few trials by taking advantage of prior experi-
ences. However, the state-of-the-art performance of deep networks heavily relies on the availability
of large amounts of labeled samples. To improve sample efficiency, meta-learning algorithms (Ben-
gio et al., 1991; Thrun & Pratt, 1998) are designed to learn meta-knowledge from historical tasks
and accelerate learning on unseen tasks. Meta-learning has been widely used for few-shot learn-
ing (Finn et al., 2017; Wang et al., 2020), neural architecture search (Zoph & Le, 2017; Liu et al.,
2018), hyperparameter optimization (Maclaurin et al., 2015; Franceschi et al., 2018), reinforcement
learning (Nagabandi et al., 2018; Rakelly et al., 2019), recommendation systems (Vartak et al., 2017;
Lee et al., 2019a), and natural language processing (Gu et al., 2018; Obamuyide & Vlachos, 2019).
Typical meta-learning algorithms (Finn et al., 2017; Denevi et al., 2019; Rajeswaran et al., 2019;
Zhou et al., 2019) learn a globally shared meta-model for all tasks. For example, the Model-Agnostic
Meta-Learning (MAML) algorithm (Finn et al., 2017) learns a meta-initialization such that a good
model for an unseen task can be fine-tuned from limited examples by a few gradient descent steps.
However, when the tasks are heterogeneous, the task models are diverse and a common meta-model
may not be sufficient.
To tackle this issue, recent works (Jerfel et al., 2019; Zhou et al., 2021) cluster tasks into multiple
groups and learn an initialization for tasks in each group. Specifically, Jerfel et al. (2019) formulate
the task distribution as a mixture of hierarchical Bayesian models and update the components (i.e.,
initializations) using Expectation Maximization. Zhou et al. (2021) first train task models using the
vanilla MAML, and then cluster them into several groups based on the Euclidean distance. The
cluster centroids become the group-specific initializations. However, centroid-based clustering fails
to take advantage of negative correlation between tasks (e.g., w and -w may be assigned to different
clusters) and fails to handle tasks that are distant from all clusters (e.g., tasks τ0 in Figures 1(a)
and 1(b)).
Another approach to deal with task heterogeneity is based on formulating task models into a sub-
space structure. Recent attempts (Kong et al., 2020; Tripuraneni et al., 2021) focus on the simple
case where linear regression tasks are drawn from a single subspace. They leverage a moment-
based estimator to recover its basis. However, it is not easy to extend such moment-based methods
to nonlinear models (e.g., neural networks) or general losses (e.g., cross-entropy loss).
In this paper, we propose to learn multiple subspaces for nonlinear models or general losses, and
treat the subspace bases as meta-parameters. For each task, the base learner selects a subspace that
1
Under review as a conference paper at ICLR 2022
(a) Single cluster.
(b) Multiple clusters.
Figure 1: Different formulations of task structure.
(c) Subspace structure.
the learned task model achieves the best performance on the training set. The meta-learner then
updates the basis of the selected subspace by minimizing the validation loss of the learned task
model. We establish convergence results for the proposed algorithm. We show theoretically that
the expected generalization gap depends on complexity of the subspaces, while the expected excess
risk depends on both complexity of the subspaces and distance between the optimal task models and
the learned subspaces. Experiments on standard benchmark datasets verify the effectiveness of the
proposed method.
In summary, the contributions of this paper are as follows:
(i)	We propose a MUlti-Subspace structured Meta-Learning (MUSML) algorithm to learn
multiple subspaces for task models. The proposed algorithm can be applied to both lin-
ear and nonlinear models.
(ii)	We prove the convergence of MUSML and theoretically study the generalization perfor-
mance.
(iii)	Experimental results demonstrate that MUSML outperforms the state-of-the-arts.
Notations: Vectors (e.g., x) and matrices (e.g., X) are denoted by lowercase and uppercase boldface
letters, respectively. For a vector x, its '2-norm is represented as ∣∣xk. For a matrix X, its '2-norm is
kXk, and its Frobenius norm is kXkF. Subspaces are denoted by blackboard boldface letters (e.g.,
S). 1m ∈ Rm denotes a m-dimensional vector with all entries being 1.
2	Related Work
Meta-learning designs algorithms to extract meta-knowledge from historical tasks so that new tasks
can be learned fast with a few training examples. Popular meta-learning algorithms can be di-
vided into three categories: metric-based approach (Koch et al., 2015; Vinyals et al., 2016; Snell
et al., 2017; Bertinetto et al., 2018; Sung et al., 2018; Oreshkin et al., 2018; Lee et al., 2019b),
memory-based approach (Santoro et al., 2016; Munkhdalai & Yu, 2017), and optimization-based
approach (Ravi & Larochelle, 2017; Finn et al., 2017; Rajeswaran et al., 2019; Denevi et al., 2019;
Balcan et al., 2019).
Most of meta-learning methods assume a globally shared meta-model (e.g., meta-initialization or
meta-regularization) for all tasks. To tackle heterogeneous tasks, recent works (Vuorio et al., 2019;
Yao et al., 2019a;b) tailor the meta-initialization to task representations, while Denevi et al. (2020)
learn a meta-regularization conditioning on tasks’ side information. Since good task representations
or side information are not easy to obtain, Jerfel et al. (2019) and Zhou et al. (2021) propose to for-
mulate tasks into multiple groups and parameters for tasks within the same group are assumed to be
close in terms of the Euclidean distance. However, centroid-based methods still require parameters
of related tasks to be close together and fail to handle negatively correlated tasks. To overcome these
challenges, recent attempts (Kong et al., 2020; Saunshi et al., 2020; Tripuraneni et al., 2021) study
linear regression tasks that are drawn from a low-dimensional subspace. Using a moment-based
estimator, the basis can be recovered (Kong et al., 2020; Tripuraneni et al., 2021). However, their
algorithms are limited to linear regression and a single subspace.
2
Under review as a conference paper at ICLR 2022
3	Methodology
3.1	Problem Formulation
Let p(τ) be a task distribution. In meta-learning, a collection of tasks are used to learn meta-
parameters. Each task T 〜p(τ) consists of a training set Dtr = {(xi, yi) : i = 1,..., Ntr} and
a validation set Dτvl = {(xi, yi) : i = 1, . . . , Nvl}, where x are the features and y the labels. Let
f(∙; W) be a model parameterized by W ∈ Rd, and L(D; W)= 击 P(X #)三。'(f (x; w), y) be the
supervised loss on data set D, where '(∙, ∙) is a loss function. The training set is used to learn the task
model, while the validation set is used to update the meta-parameters at meta-training or evaluate
the task model at meta-testing.
3.2	Multi-Subspace Structured Meta-Learning
In this paper, tasks are assumed to be clustered in multiple groups, and parameters for tasks in the
same group lie in a low-dimensional subspace. Specifically, there are K subspaces S1, . . . , SK. For
simplicity, we assume all subspaces to have the same dimensionality m. Let Sk ∈ Rd×m be a
basis of Sk . For a task τ, the base learner selects the subspace Skτ that τ lies in, and determines
the linear combination weight vτ for the basis. The task model is then Wτ = Skτ vτ . The basis
set {S1, . . . , SK} are meta-parameters to be updated by the meta-learner, while (kτ , vτ ) are task
parameters learned by the base learner.
Kong et al. (2020) and Tripuraneni et al. (2021) consider linear regression tasks and assume that
the task parameters are drawn from a low-dimensional subspace. For this case, the column space
of EτE(χ,y)~τ,(χ0,y0)~τyy0xx0> is identical to the column space of S. Using a moment-based es-
timator, they recover the basis from abundant meta-training tasks. However, their algorithms are
infeasible for general models (e.g., neural networks) and general losses (e.g., cross-entropy loss).
For each task τ, our base learner selects one of the K subspaces such that the learned task model
achieves the best performance on Dτtr . Specifically, the inner loop in meta-learning is formulated as
(kT, VT)=	arg min	L(DTr ； Sk VT),	⑴
k∈{1,...,K},vτ ∈Rm
where S1, . . . , SK are fixed. When `(f (x; W), y) is convex in W, L(DTtr; SkVT) is also convex in VT
and the minimization problem can be solved by convex programming (Boyd et al., 2004). However,
for nonlinear models such as neural networks, the loss is usually non-convex and obtaining the
global minimum of L(DTtr ; SkVT) is intractable. Instead, for each k, we first compute VT(k,T) by
Tinner gradient descent steps from an initial VTkO = ml 1m With stepsize a, then obtain the task
ParameterS kT ≡ argmin1≤k≤K L(DTr ； Sk VTkTLlner) and VT ≡ VTkTinner∙
After obtaining the task parameters (kT, VT), the meta-learner updates Sk^ by a gradient descent
step on the validation loss L(DTvl； Sk^VT). Let WT ≡ Sk^vT. As VT is a function of SW,
by the chain rule,	Lg; S^V) = VWτ L(DTl； WT)也悼 WT = JL(DTl； W耍)vT> +
▽W*L(DTI； WT)Sk* Vsk* v*. Here, for simplicity, the dependence of k* on Sk* is ignored. As
VT = VTkT) — α PT⅛-1 V (k*) L(DTr ； Sk* v⅛-1), the total complexity of computing Vsk* VT
T T,	=	v τ	T τ T, -	kτ T
τ,t0 -1
is O(Tinnerdm3). Usually, m	d is very small.
The whole procedure, called MUSML, is shown in Algorithm 1. Similar to other optimization-based
meta-learning algorithms (Finn et al., 2017; Rajeswaran et al., 2019), Tinner is usually small (e.g., 1
to 5) for meta-training, but can be large (e.g., 10 to 20) at meta-testing.
3.3	Theoretical Analysis
Assumption 1. (i) `(f (x; W); y) is β1-Lipschitz smooth1 in W; (ii) VT* is β2-Lipschitz smooth
in SkT ； (iii) ETkVSk* L(DT SkT VT) — ET ^* L(DT Sk* v* )∣∣2 ≤ σ2; (iv) {vT ,τ ~ p(τ)}
1InotherwordsJNw '(f(x; w); y) - Vw '(f(x; w0); y)k ≤ βι∣∣w - w0∣∣.
3
Under review as a conference paper at ICLR 2022
Algorithm 1 MUSML.
Require: stepsize α, ηt, number of inner gradient steps Tinner, number of subspaces K, subspace
	dimension m;
1 2 3 4 5 6 7 8	: for t = 0, 1, . . . , T - 1 do sample a task Tt = (DTr, Dvt)〜P(T)； base-learner: LTt = ∞; :	for k = 1, . . . , K do vTk,)o = mm 1m; :	for t0 = 0, 1, . . . , Tinner - 1 do vT?t，+i = vT?t，- αVv(k) L(DTr； SktvkG) τ t0 τt ,
9 10 11 12 13 14 15 16 17 18 19 20	:	end for if L(DTr; Sk,tvT?Tinner) < LT then kTt =k, v；t = vTk,LTt = L(DTr ； Sk,tvTkkj; :	end if :	end for meta-Iearner: gt = vSk*t,tL(Dtl； SkTt,tvTt); :	for k = 1, . . . , K do :	Sk,t+i = Sk,t - ηtI(kTTt, k)gt;	. I(a, b) = 1 ifa = b otherwise 0. :	end for : end for : return Si,T, . . . , SK,T.
and basis vectors are in a compact set, and thus their `2 -norms are bounded by a con-
Stant β3 > 0; (V) There exists a constant e > 0 such that for all T 〜 p(τ),
L(DTr； SkT VT) ≤ mink=kτ,ι≤k≤κ L(DTr； SkVTkTnlJ - e∙
The assumptions on Lipschitz-smoothness and variance are commonly used in stochastic non-
convex optimization (Ghadimi & Lan, 2013; Reddi et al., 2016) and meta-learning in non-convex
settings (Fallah et al., 2020; Zhou et al., 2021), while the compactness assumption is also used in
the convergence analysis of bilevel optimization (Franceschi et al., 2018).
Let I(a, b) = 1 if a = b, and 0 otherwise. The following Theorem establishes convergence of the
proposed algorithm and the proof is similar to that in (Fallah et al., 2020). The O(1/√T) rate is the
same as MAML (Fallah et al., 2020; Ji et al., 2020) and other meta-learning algorithms (Zhou et al.,
2019) . All proofs are in the Appendix.
Theorem 1.	Let L meta (Si,..., SK) = L(DTl; SH vT) and η = min ^meij, √T). With Asl-
sumption 1, we have
imtinT Ek%Sk,t,…,Sκ,t]Lmeta (S1,t)..., SKt)k2 ≤ O (X E PT \(k* k)),
where the expectation is over the random training samples. Ifeach subspace is Selected K times in
expectation, i.e., E PT=I I(kζ ,k) = K forall k, then the upper bound simplifies to O (σ√K2).
Next, we study the testing performance of the learned subspaces. The following assumption ensures
that the task parameters are stable when one sample is changed in the training set. Stability is a
widely-used tool to analyze the generalization of meta-learning algorithms and bilevel optimiza-
tion (Maurer & Jaakkola, 2005; Bao et al., 2021).
Assumption 2. For any two training sets D1tr and D2tr that only differ in one sample,
maxι≤k≤κ ∣∣argminv∈Rm L(Dtr; Skv) - argmi□v∈Rm L(D2r; Skv)k ≤ Nr.
Theorem 2.	Let T0 be a testing task, R(T0; Si,..., SK) ≡ EDtr〜T‘E(x,y)〜T0'(f (x; Sk*∕VTo),y)
and R(T0; Si,..., SK)	≡	EDtr〜T‘L(DTr； Sk*0 v；o),	where (k*o, v；o)	=
τ0	T τ0 T	T T
4
Under review as a conference paper at ICLR 2022
arg min1≤k≤K,v 0∈Rm L(Dτtr0; Skvτ0). With Assumptions 1 and 2, (i) we have
R(T 0; Si,..., SK) ≤ R(T 0; Si,..., SK) + O (KNm)；	⑵
(ii)	Let W(TO ≡ argminWT, ∈Rd E(χ,y)〜T 0 '(f (x; WT '),y) be the optimal model for T 0 and Ro (τ 0) ≡
E(χ,y)〜T0'(f (x； w(o), y) be its expected risk. Then,
R(T0; Si,..., SK) ≤ Ro(τ0) + O (KNm + IminK dist(w'To, Sk)) ,	(3)
where dist(WTo 0, Sk) ≡ minWτ 0 ∈Sk kWTo0 - WT0 k is the distance between WTo0 and the subspace Sk.
Theorem 2 analyzes the effects of m and K to the testing performance. From (2), increasing the
complexity of subspaces may reduce R(T0; Si, . . . , SK) but increase the expected generalization
gap R(T0; Si, . . . , SK) - R(T0; Si, . . . , SK). For fixed K and m, proper subspaces enable the base
learner to reduce both R(T0; Si, . . . , SK) and R(T0; Si, . . . , SK). From (3), the expected excess
risk R(T 0; Si,..., SK) - RO(T 0) is upper bounded by O (K√m + minι≤k≤κ dist(w(,, Sk)). The
first term depends on the complexity of subspaces, while the second term arises from the approxi-
mation error of wTo0 using the learned subspaces. Again, good subspaces reduce the excess risk.
For the centroid-based clustering method in (Zhou et al., 2021), its expected excess risk is bounded
by O (γNFr + ∣∣ωk*0 - wO, Il2) ,whereγ > 1 and 3k* is the centroid of the cluster that T0 belongs
Ntr	τ0	T	τ0
to. The distance ∣ωk*0 -wTo0 ∣ plays the same role as the term mini≤k≤K dist(wTo0, Sk) in (3), which
measures how far the optimal model wTo0 is away from the subspaces or clusters.
3.4 A Practical Implementation for MUSML
The proposed MUSML is a model-agnostic meta-learning framework. Note that the basis incur ad-
ditional memory cost, which can be problematic especially for deep networks that usually contain
millions of parameters. As features extracted from the bottom layers are more general (Yosin-
ski et al., 2014), a practical implementation is to divide the network weight w into two parts: (i)
w(btm) ∈ Rdbtm for the bottom layers near the input that are shared globally across all tasks, and (ii)
w(top) ∈ RdtOP for the top layers. Let the basis Sk be partitioned analogously as [1工 0 θ; SkOP)],
where θ ∈ Rdbtm is the globally shared parameters and 0 is the Kronecker product. This implemen-
tation reduces the memory from O(K md) to O(K mdtop + dbtm).
4	Experiments
4.1 Few-shot classification on Meta-Dataset
Dataset. We use the standard 5-way Ntr -shot setting (Ntr = 1 or 5) on the commonly-used Meta-
Dataset benchmark (Yao et al., 2019a;b; Triantafillou et al., 2020) to evaluate the proposed method.
This benchmark consists of4 image classification datasets: Caltech-UCSD Birds-200-2011 (denoted
by Bird) (Welinder et al., 2010), Describable Textures Dataset (denoted by Texture) (Cimpoi et al.,
2014) , Fine-Grained Visual Classification of Aircraft (denoted by Aircraft) (Maji et al., 2013), and
FGVCx-Fungi (denoted by Fungi) (Schroeder & Cui, 2018). We adopt the split setting in (Yao
et al., 2019a) that for each dataset, classes are randomly split into three parts for meta-training,
meta-validation and meta-testing, respectively. Table 1 describes the statistics of this meta-dataset,
and Figure 6 in the appendix shows some example images. Following (Yao et al., 2019a), each
few-shot task samples classes from one of the four datasets.
Network Architecture. We use the Conv4 network in (Yao et al., 2019a;b), which has 4 modules.
Each module is a 3 × 3 convolutional layer with 64 filters, followed by a batch normalization layer,
ReLU activation, and a 2 × 2 max-pooling layer. Follow the practical implementation, the first
two modules are shared globally by all the tasks. As the choice of the model f(x; w) is flexible,
a simple prototype classifier with the cosine similarity (Snell et al., 2017; Gidaris & Komodakis,
2018) is used here.
5
Under review as a conference paper at ICLR 2022
Table 1: StatiSticS for datasets.
dataset	#classes (meta-training/validation/testing)	#samples per class
Bird (Welinder et al., 2010)	64/16/20	60
Meta-Dataset	Texture (Cimpoi et al., 2014)	30/7/10	120
Meta-Dataset	Aircraft (Maji et al., 2013)	64/16/20	100
Fungi (Schroeder & Cui, 2018)	64/16/20	150
Mini-Imagenet (Vinyals et al., 2016)	64/16/20	600
Baselines. We compare the proposed method with state-of-the-art baselines: (i) meta-learning algo-
rithms with a globally shared meta-model including MAML (Finn et al., 2017), MetaSGD (Li et al.,
2017), TapNet (Yoon et al., 2019), and ProtoNet (Snell et al., 2017); (ii) meta-learning algorithms
with a task modulation network including TADAM (Oreshkin et al., 2018), MT-Net (Lee & Choi,
2018), BMAML (Yoon et al., 2018), and MMAML (Vuorio et al., 2019); (iii) structured meta-learn-
ing algorithms including HSML (Yao et al., 2019a), ARML (Yao et al., 2019b), TSA-MAML (Zhou
et al., 2021), and (Jerfel et al., 2019) (denoted by DPMM).
Implementation Details. We use the cross-entropy loss for '(∙, ∙). For the base learner, We use
the SGD optimizer with a learning rate of 0.1. The number Tinner of inner gradient steps is set to
3 at the meta-training and 15 at the meta-validation and meta-testing. We train the subspace bases
for 30, 000 iterations using the Adam optimizer (Kingma & Ba, 2015) With an initial learning rate
of 0.001, Which is then reduced by half every 5, 000 iterations. To prevent overfitting, We evaluate
the performance on the meta-validation set every 1, 000 iterations and stop training When the meta-
validation accuracy has no significant improvement for 10 consecutive evaluations. By tuning the
hyperparameters K and m from {1, 5, 10, 20, 30, 40} using grid search, (K = 5, m = 5) and
(K = 20, m = 5) achieve the highest meta-validation accuracy for the 1-shot and 5-shot settings,
respectively, thus are used in experiments.
Table 2: Accuracies (With 95% confidence intervals) of 5-Way 1-shot classification on Meta-Dataset.
t means that the result is obtained by running the code under this setting. Results of other baselines
are from (Yao et al., 2019a;b).____________________________________________________________________________
method	Bird	Texture	Aircraft	Fungi	average
MAML (Finn et al., 2017)	53.94 ± 1.45%	31.66 ± 1.31%	51.37 ± 1.38%	42.12 ± 1.36%	44.77%
MetaSGD (Li et al., 2017)	55.58 ± 1.43%	32.38 ± 1.32%	52.99 ± 1.36%	41.74 ± 1.34%	45.67%
ProtoNett (Snell et al., 2017)	60.58 ± 1.22%	34.48 ± 1.18%	53.38 ± 1.33%	40.61 ± 1.27%	47.28%
TapNet (Yoon et al., 2019)	54.90 ± 1.34%	32.44 ± 1.23%	51.22 ± 1.34%	42.88 ± 1.35%	45.36%
TADAM (Oreshkin et al., 2018)	56.58 ± 1.34%	33.34 ± 1.27%	53.24 ± 1.33%	43.06 ± 1.33%	46.56%
MT-Net (Lee & Choi, 2018)	58.72 ± 1.43%	32.80 ± 1.35%	47.72 ± 1.46%	43.11 ± 1.42%	45.59%
BMAML (Yoon et al., 2018)	54.89 ± 1.48%	32.53 ± 1.33%	53.63 ± 1.37%	42.50 ± 1.33%	45.89%
MMAML (Vuorio et al., 2019)	56.82 ± 1.49%	33.81 ± 1.36%	53.14 ± 1.39%	42.22 ± 1.40%	46.50%
DPMMt (Jerfel et al., 2019)	61.30 ± 1.47%	35.21 ± 1.35%	57.88 ± 1.37%	43.81 ± 1.45%	49.55%
HSML (Yao et al., 2019a)	60.98 ± 1.50%	35.01 ± 1.36%	57.38 ± 1.40%	44.02 ± 1.39%	49.35%
ARML (Yao et al., 2019b)	62.33 ± 1.47%	35.65 ± 1.40%	58.56 ± 1.41%	44.82 ± 1.38%	50.34%
TSA-MAMLt (Zhou et al., 2021)	61.37 ± 1.42%	35.41 ± 1.39%	58.78 ± 1.37%	44.17 ± 1.25%	49.93%
MUSML (proposed)	63.97 ± 1.10%	37.65 ± 1.16%	61.36% ± 1.20%	46.23 ± 1.12%	52.30%
Results. For each dataset, We report the classification accuracy averaged over 1, 000 tasks randomly
sampled from the meta-testing set. The results are reported in Table 2 for the 1-shot setting and
Table 3 for the 5-shot setting. As can be seen, in both settings, MUSML consistently outperforms
current state-of-the-arts. Compared With ProtoNet, the better performance possessed by MUSML
confirms the effectiveness of structuring task models intro multiple subspaces. Compared With
other structured meta-learning methods (i.e., DPMM, HSML, ARML, and TSA-MAML), MUSML
achieves higher accuracy.
6
Under review as a conference paper at ICLR 2022
Table 3: Accuracies (with 95% confidence intervals) of 5-way 5-shot classification on Meta-Dataset.
t means that the result is obtained by running the code under this setting. Results of other baselines
are from (Yao et al., 2019a;b).
method	Bird	Texture	Aircraft	Fungi	average
MAML (Finn et al., 2017)	68.52 ± 0.73%	44.56 ± 0.68%	66.18 ± 0.71%	51.85 ± 0.85%	57.78%
MetaSGD (Li et al., 2017)	67.87 ± 0.74%	45.49 ± 0.68%	66.84 ± 0.70%	52.51 ± 0.81%	58.18%
ProtoNett (Snell et al., 2017)	71.48 ± 0.72%	50.36 ± 0.67%	71.67 ± 0.69%	55.68 ± 0.82%	62.29%
TapNet (Yoon et al., 2019)	69.07 ± 0.74%	45.54 ± 0.68%	67.16 ± 0.66%	51.08 ± 0.80%	58.21%
TADAM (Oreshkin et al., 2018)	69.13 ± 0.75%	45.78 ± 0.65%	69.87 ± 0.66%	53.15 ± 0.82%	59.48%
MT-Net (Lee & Choi, 2018)	69.22 ± 0.75%	46.57 ± 0.70%	63.03 ± 0.69%	53.49 ± 0.83%	58.08%
BMAML (Yoon et al., 2018)	69.01 ± 0.74%	46.06 ± 0.69%	65.74 ± 0.67%	52.43 ± 0.84%	58.31%
MMAML (Vuorio et al., 2019)	70.49 ± 0.76%	45.89 ± 0.69%	67.31 ± 0.68%	53.96 ± 0.82%	59.41%
DPMMt (Jerfel et al., 2019)	72.22 ± 0.70%	49.32 ± 0.68%	73.55 ± 0.69%	56.82 ± 0.81%	63.00%
HSML (Yao et al., 2019a)	71.68 ± 0.73%	48.08 ± 0.69%	73.49 ± 0.68%	56.32 ± 0.80%	62.39%
ARML (Yao et al., 2019b)	73.68 ± 0.70%	49.67 ± 0.67%	74.88 ± 0.64%	57.55 ± 0.82%	63.95%
TSA-MAMLt (Zhou et al., 2021)	72.31 ± 0.71%	49.50 ± 0.68%	74.01 ± 0.70%	56.95 ± 0.80%	63.20%
MUSML (proposed)	78.57 ± 0.68%	51.73 ± 0.67%	81.03 ± 0.66%	59.20 ± 0.68%	67.63%
(a) meta-training tasks.
Figure 2: Usage frequency of each learned subspace under the 5-way 1-shot setting (K = 5, m = 5).
The value in the (i, j)-th grid is the frequency that MUSML assigns tasks from the j-th dataset to
the i-th subspace. Dark colors indicate high frequencies.
(a) meta-training tasks.
Subspace id
(b) meta-testing tasks.
Figure 3:	Usage frequency of each learned subspace for the 5-way 5-shot setting (K = 20, m = 5).
The value in the (i, j)-th grid is the frequency that MUSML assigns tasks from the j-th dataset to
the i-th subspace. Dark colors indicate high frequencies.
Figure 2 shows the usage frequency of learned subspaces under the 5-way 1-shot setting, and Fig-
ure 3 shows that under the 5-way 5-shot setting. As can be seen, the task structure under 5-shot
setting is more clear. Figure 3 also reveals that 5-shot tasks from Bird and Fungi are prone to share
the same subspace (i.e., the second subspace).
We further study the effects of K and m to the meta-testing accuracy. We repeat the experiment for
10 times and plot the accuracy in Figures 4 and 5 under the 1-shot and 5-shot settings, respectively.
7
Under review as a conference paper at ICLR 2022
1	5	10	20	30	40
K
(a) Varying K when m = 5 is fixed.
Figure 4:	The meta-testing accuracy under the 5-way 1-shot setting.
Figure 5:	The meta-testing accuracy under the 5-way 5-shot setting.
As shown in Figures 4(a) and 5(a), a larger K under the 1-shot setting is likely to cause overfitting
than under the 5-shot setting. According to Figures 4(b) and 5(b), the accuracy is significantly
improved when m increases from 1 to 5.
We conduct experiments to verify that the effectiveness of MUSML is from its subspace structure
instead of higher model complexity. We test the prototype classifier (Snell et al., 2017) with a
Kd × wider network (denoted by Wide-ProtoNet). As shown in Table 4 and Table 5, MUSML still
achieves better performance.
Table 4: Accuracies (with 95% confidence intervals) of 5-way 1-shot classification on Meta-Dataset.
method	Bird	Texture	Aircraft	Fungi	average
ProtoNet (Snell et al., 2017)	60.58 ± 1.22%	34.48 ± 1.18%	53.38 ± 1.33%	40.61 ± 1.27%	47.28%
Wide-ProtoNet	61.58 ± 1.10%	34.81 ± 1.05%	57.41 ± 1.28%	43.65 ± 1.00%	49.36%
MUSML (proposed)	63.97 ± 1.10%	37.65 ± 1.16%%	61.36% ± 1.20%	46.23 ± 1.12%	52.30%
Table 5: Accuracies (with 95% confidence intervals) of 5-way 5-shot classification on Meta-Dataset.
method	Bird	Texture	Aircraft	Fungi	average
ProtoNet (Snell et al., 2017)	71.48 ± 0.72%	50.36 ± 0.67%	71.67 ± 0.69%	55.68 ± 0.82%	62.29%
Wide-ProtoNet	75.52 ± 0.68%	50.49 ± 0.58%	76.82 ± 0.62%	57.12 ± 0.71%	65.00%
MUSML (proposed)	78.57 ± 0.68%	51.73 ± 0.67%	81.03 ± 0.66%	59.20 ± 0.68%	67.63%
8
Under review as a conference paper at ICLR 2022
4.2 Few-shot Classification on Mini-Imagenet
Experiment Setup. We compare the proposed MUSML method with baselines on the Mini-
Imagenet dataset (Vinyals et al., 2016), which consists of 100 randomly chosen classes from
ILSVRC-2012 (Russakovsky et al., 2015). The statistics of this dataset are described in Table 1.
We adopt the same split as (Ravi & Larochelle, 2017). All the methods in comparison use the
experiment settings and the Conv4 backbone introduced in (Vinyals et al., 2016). The first two
modules of Conv4 are shared globally by all the tasks. We evaluate the performance on the meta-
validation set every 1, 000 iterations, and stop training when the meta-validation accuracy has no
significant improvement for 10 consecutive evaluations. As this dataset has no explicitly heteroge-
neous structure, the complexity of subspace is probably low. We tune the hyperparameters K and
m from {1, 2, 3, 4, 5} using grid search, where (k = 2, m = 3) and (k = 3, m = 3) achieve the
best meta-validation performance for the 1-shot and 5-shot settings, respectively, and thus use these
settings.
Result. We report in Table 6 the classification accuracy averaged over 600 tasks randomly sampled
from the meta-testing set. As can be seen, MUSML performs better than baselines.
Table 6: Accuracies (with 95% confidence intervals) of 5-way few-shot classification on the Mini-
Imagenet dataset. “-” means that the corresponding result is not reported in related publications.
method	5-way 1-shot	5-way 5-shot
MAML (Finn et al., 2017)	48.7 ± 1.8%	63.1 ± 0.9%
MetaSGD (Li et al., 2017)	50.5 ± 1.9%	64.0 ± 0.9%
ProtoNet (Snell et al., 2017)	49.4 ± 0.8%	68.2 ± 0.7%
TapNet (Yoon et al., 2019)	50.7 ± 0.1%	69.0 ± 0.1%
TADAM (Oreshkin et al., 2018)	50.3 ± 1.7%	66.2 ± 0.8%
MT-Net (Lee & Choi, 2018)	51.7 ± 1.8%	-
BMAML (Yoon et al., 2018)	50.0 ± 1.9%	-
MMAML (Vuorio et al., 2019)	49.9 ± 1.9%	-
DPMM (Jerfel et al., 2019)	49.3 ± 1.5%	64.1 ± 0.9%
HSML (Yao et al., 2019a)	50.4 ± 1.8%	-
ARML (Yao et al., 2019b)	50.4 ± 1.7%	-
TSA-MAML (Zhou et al., 2021)	49.5 ± 1.3%	64.3 ± 0.8%
MUSML (proposed)	54.1 ± 1.0%	69.9 ± 0.7%
5 Conclusion
In this paper, we proposed a novel algorithm called MUSML to learn multiple subspaces for task
models. For each task, the base learner selects the subspace that the task lies in, and computes
the corresponding linear combination weight. The subspace bases are meta-parameters updated by
the meta-learner. We theoretically establish the convergence and analyze the generalization perfor-
mance. Experimental results on benchmark datasets demonstrate that the proposed MUSML method
outperforms the state-of-the-arts.
Ethics S tatement
We have read the ethics review guidelines and ensured that this paper conforms to them. No human
subjects are researched in this work, so there is no such potential risk. All datasets used in the
experiments are public and do not contain personally identifiable information or offensive content.
There is no potential negative societal impacts.
Reproducibility S tatement
For all theoretical results, assumptions have been fully stated and complete proofs are provided in the
Appendix. We have include code, data, and instructions needed to reproduce the main experimental
results. All training details are mentioned in Section 4.
9
Under review as a conference paper at ICLR 2022
References
Maria-Florina Balcan, Mikhail Khodak, and Ameet Talwalkar. Provable guarantees for gradient-
based meta-learning. In International Conference on Machine Learning, pp. 424—433. PMLR,
2019.
Fan Bao, Guoqiang Wu, Chongxuan Li, Jun Zhu, and Bo Zhang. Stability and generalization of
bilevel programming in hyperparameter optimization. arXiv preprint arXiv:2106.04188, 2021.
Y. Bengio, S. Bengio, and J. Cloutier. Learning a synaptic learning rule. In Proceedings of IJCNN-
91-Seattle International Joint Conference on Neural Networks, pp. 969 vol.2, 1991.
Luca Bertinetto, Joao F Henriques, Philip Torr, and Andrea Vedaldi. Meta-learning with differen-
tiable closed-form solvers. In Proceedings of International Conference on Learning Representa-
tions, 2018.
Stephen Boyd, Stephen P Boyd, and Lieven Vandenberghe. Convex optimization. Cambridge uni-
versity press, 2004.
Mircea Cimpoi, Subhransu Maji, Iasonas Kokkinos, Sammy Mohamed, and Andrea Vedaldi. De-
scribing textures in the wild. In Proceedings of the IEEE Conference on Computer Vision and
Pattern Recognition, pp. 3606-3613, 2014.
Giulia Denevi, Carlo Ciliberto, Riccardo Grazzi, and Massimiliano Pontil. Learning-to-learn
stochastic gradient descent with biased regularization. In Proceedings of International Conference
on Machine Learning, pp. 1566-1575, 2019.
Giulia Denevi, Massimiliano Pontil, and Carlo Ciliberto. The advantage of conditional meta-
learning for biased regularization and fine tuning. Advances in Neural Information Processing
Systems, 33, 2020.
Alireza Fallah, Aryan Mokhtari, and Asuman Ozdaglar. On the convergence theory of gradient-
based model-agnostic meta-learning algorithms. In Proceedings of International Conference on
Artificial Intelligence and Statistics, pp. 1082-1092. PMLR, 2020.
Chelsea Finn, Pieter Abbeel, and Sergey Levine. Model-agnostic meta-learning for fast adaptation
of deep networks. In Proceedings of the 34th International Conference on Machine Learning, pp.
1126-1135, 2017.
Luca Franceschi, Paolo Frasconi, Saverio Salzo, Riccardo Grazzi, and Massimiliano Pontil. Bilevel
programming for hyperparameter optimization and meta-learning. In International Conference
on Machine Learning, pp. 1568-1577. PMLR, 2018.
Saeed Ghadimi and Guanghui Lan. Stochastic first-and zeroth-order methods for nonconvex stochas-
tic programming. SIAM Journal on Optimization, 23(4):2341-2368, 2013.
Spyros Gidaris and Nikos Komodakis. Dynamic few-shot visual learning without forgetting. In
Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 4367-
4375, 2018.
Jiatao Gu, Yong Wang, Yun Chen, Victor OK Li, and Kyunghyun Cho. Meta-learning for low-
resource neural machine translation. In Proceedings of the 2018 Conference on Empirical Meth-
ods in Natural Language Processing, pp. 3622-3631, 2018.
Ghassen Jerfel, Erin Grant, Tom Griffiths, and Katherine A Heller. Reconciling meta-learning and
continual learning with online mixtures of tasks. In Advances in Neural Information Processing
Systems, volume 32, pp. 9122-9133, 2019.
Kaiyi Ji, Junjie Yang, and Yingbin Liang. Multi-step model-agnostic meta-learning: Convergence
and improved algorithms. arXiv preprint arXiv:2002.07836, 2020.
Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In Proceedings of
International Conference on Learning Representations, 2015.
10
Under review as a conference paper at ICLR 2022
Gregory Koch, Richard Zemel, and Ruslan Salakhutdinov. Siamese neural networks for one-shot
image recognition. In ICML deep learning workshop, volume 2. Lille, 2015.
Weihao Kong, Raghav Somani, Zhao Song, Sham Kakade, and Sewoong Oh. Meta-learning for
mixed linear regression. In International Conference on Machine Learning, pp. 5394-5404.
PMLR, 2020.
Hoyeop Lee, Jinbae Im, Seongwon Jang, Hyunsouk Cho, and Sehee Chung. Melu: Meta-learned
user preference estimator for cold-start recommendation. In Proceedings of the 25th ACM
SIGKDD International Conference on Knowledge Discovery & Data Mining, pp. 1073-1082,
2019a.
Kwonjoon Lee, Subhransu Maji, Avinash Ravichandran, and Stefano Soatto. Meta-learning with
differentiable convex optimization. In Proceedings of the IEEE Conference on Computer Vision
and Pattern Recognition, pp. 10657-10665, 2019b.
Yoonho Lee and Seungjin Choi. Gradient-based meta-learning with learned layerwise metric and
subspace. In International Conference on Machine Learning, pp. 2927-2936. PMLR, 2018.
Zhenguo Li, Fengwei Zhou, Fei Chen, and Hang Li. Meta-sgd: Learning to learn quickly for few-
shot learning. arXiv preprint arXiv:1707.09835, 2017.
Hanxiao Liu, Karen Simonyan, and Yiming Yang. Darts: Differentiable architecture search. In
International Conference on Learning Representations, 2018.
Dougal Maclaurin, David Duvenaud, and Ryan Adams. Gradient-based hyperparameter optimiza-
tion through reversible learning. In International conference on machine learning, pp. 2113-2122.
PMLR, 2015.
Subhransu Maji, Esa Rahtu, Juho Kannala, Matthew Blaschko, and Andrea Vedaldi. Fine-grained
visual classification of aircraft. arXiv preprint arXiv:1306.5151, 2013.
Andreas Maurer and Tommi Jaakkola. Algorithmic stability and meta-learning. Journal of Machine
Learning Research, 6(6), 2005.
Tsendsuren Munkhdalai and Hong Yu. Meta networks. In International Conference on Machine
Learning, pp. 2554-2563. PMLR, 2017.
Anusha Nagabandi, Ignasi Clavera, Simin Liu, Ronald S Fearing, Pieter Abbeel, Sergey Levine,
and Chelsea Finn. Learning to adapt in dynamic, real-world environments through meta-
reinforcement learning. In International Conference on Learning Representations, 2018.
Abiola Obamuyide and Andreas Vlachos. Model-agnostic meta-learning for relation classification
with limited supervision. In Proceedings of the 57th Annual Meeting of the Association for Com-
putational Linguistics, pp. 5873-5879, 2019.
Boris Oreshkin, Pau Rodrlguez Lopez, and Alexandre Lacoste. Tadam: Task dependent adaptive
metric for improved few-shot learning. In Proceedings of Advances in Neural Information Pro-
cessing Systems, pp. 721-731, 2018.
Aravind Rajeswaran, Chelsea Finn, Sham M Kakade, and Sergey Levine. Meta-learning with im-
plicit gradients. In Proceedings of Advances in Neural Information Processing Systems, pp. 113-
124, 2019.
Kate Rakelly, Aurick Zhou, Chelsea Finn, Sergey Levine, and Deirdre Quillen. Efficient off-policy
meta-reinforcement learning via probabilistic context variables. In International conference on
machine learning, pp. 5331-5340. PMLR, 2019.
Sachin Ravi and Hugo Larochelle. Optimization as a model for few-shot learning. In Proceedings
of International Conference on Learning Representations, 2017.
Sashank J Reddi, Ahmed Hefny, Suvrit Sra, Barnabas Poczos, and Alex Smola. Stochastic variance
reduction for nonconvex optimization. In International conference on machine learning, pp. 314-
323. PMLR, 2016.
11
Under review as a conference paper at ICLR 2022
Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng
Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, et al. Imagenet large scale visual
recognition challenge. International Journal ofComputer Vision, 115(3):211-252, 2015.
Adam Santoro, Sergey Bartunov, Matthew Botvinick, Daan Wierstra, and Timothy Lillicrap. Meta-
learning with memory-augmented neural networks. In International conference on machine learn-
ing, pp. 1842-1850. PMLR, 2016.
Nikunj Saunshi, Yi Zhang, Mikhail Khodak, and Sanjeev Arora. A sample complexity separation
between non-convex and convex meta-learning. In International Conference on Machine Learn-
ing, pp. 8512-8521. PMLR, 2020.
Brigit Schroeder and Yin Cui. FGVCx fungi classification challenge, 2018. URL https://
github.com/visipedia/fgvcx_fungi_comp.
Jake Snell, Kevin Swersky, and Richard Zemel. Prototypical networks for few-shot learning. In
Proceedings of Advances in Neural Information Processing Systems, pp. 4077-4087, 2017.
Flood Sung, Yongxin Yang, Li Zhang, Tao Xiang, Philip HS Torr, and Timothy M Hospedales.
Learning to compare: Relation network for few-shot learning. In Proceedings of the IEEE Con-
ference on Computer Vision and Pattern Recognition, pp. 1199-1208, 2018.
Sebastian Thrun and Lorien Pratt. Learning to learn: Introduction and overview. In Learning to
learn, pp. 3-17. Springer, 1998.
Eleni Triantafillou, Tyler Zhu, Vincent Dumoulin, Pascal Lamblin, Utku Evci, Kelvin Xu, Ross
Goroshin, Carles Gelada, Kevin Swersky, Pierre-Antoine Manzagol, and Hugo Larochelle. Meta-
dataset: A dataset of datasets for learning to learn from few examples. In International Conference
on Learning Representations, 2020.
Nilesh Tripuraneni, Chi Jin, and Michael Jordan. Provable meta-learning of linear representations.
In International Conference on Machine Learning, pp. 10434-10443. PMLR, 2021.
Manasi Vartak, Arvind Thiagarajan, Conrado Miranda, Jeshua Bratman, and Hugo Larochelle. A
meta-learning perspective on cold-start recommendations for items. Advances in Neural Informa-
tion Processing Systems, 30:6904-6914, 2017.
Oriol Vinyals, Charles Blundell, Timothy Lillicrap, Daan Wierstra, et al. Matching networks for
one shot learning. In Proceedings of Advances in Neural Information Processing Systems, pp.
3630-3638, 2016.
Risto Vuorio, Shao-Hua Sun, Hexiang Hu, and Joseph J Lim. Multimodal model-agnostic meta-
learning via task-aware modulation. Proceedings of Advances in Neural Information Processing
Systems, 32:1-12, 2019.
Yaqing Wang, Quanming Yao, James T Kwok, and Lionel M Ni. Generalizing from a few examples:
A survey on few-shot learning. ACM Computing Surveys (CSUR), 53(3):1-34, 2020.
P. Welinder, S. Branson, T. Mita, C. Wah, F. Schroff, S. Belongie, and P. Perona. Caltech-UCSD
Birds 200. Technical Report CNS-TR-2010-001, California Institute of Technology, 2010.
Huaxiu Yao, Ying Wei, Junzhou Huang, and Zhenhui Li. Hierarchically structured meta-learning.
In International Conference on Machine Learning, pp. 7045-7054. PMLR, 2019a.
Huaxiu Yao, Xian Wu, Zhiqiang Tao, Yaliang Li, Bolin Ding, Ruirui Li, and Zhenhui Li. Automated
relational meta-learning. In International Conference on Learning Representations, 2019b.
Jaesik Yoon, Taesup Kim, Ousmane Dia, Sungwoong Kim, Yoshua Bengio, and Sungjin Ahn.
Bayesian model-agnostic meta-learning. In Proceedings of the 32nd International Conference
on Neural Information Processing Systems, pp. 7343-7353, 2018.
Sung Whan Yoon, Jun Seo, and Jaekyun Moon. Tapnet: Neural network augmented with task-
adaptive projection for few-shot learning. In International Conference on Machine Learning, pp.
7115-7123. PMLR, 2019.
12
Under review as a conference paper at ICLR 2022
Jason Yosinski, Jeff Clune, Yoshua Bengio, and Hod Lipson. How transferable are features in deep
neural networks? In Proceedings of Advances in Neural Information Processing Systems, pp.
3320-3328, 2014.
Pan Zhou, Xiaotong Yuan, Huan Xu, Shuicheng Yan, and Jiashi Feng. Efficient meta learning
via minibatch proximal update. In Proceedings of Advances in Neural Information Processing
Systems, pp. 1534-1544, 2019.
Pan Zhou, Yingtian Zou, X Yuan, Jiashi Feng, Caiming Xiong, and SC Hoi. Task similarity aware
meta learning: Theory-inspired improvement on maml. In Proceedings of the Conference on
Uncertainty in Artificial Intelligence, 2021.
Barret Zoph and Quoc V Le. Neural architecture search with reinforcement learning. In Interna-
tional Conference on Learning Representations, 2017.
13
Under review as a conference paper at ICLR 2022
A Appendix
A.1 EXAMPLES FROM Meta-Dataset
A.2 Proof of Theorem 1
Proof. Let vec(X) be the vectorization of a matrix X, i.e., the column vector obtained by stack-
ing the columns of	X.	We first consider the	kth	subspace	Sk.	As both	'(f (x; w),y)	and	Vk
are Lipschitz-smooth, L(Dvl; S科Vk) is also LiPSchitz smooth in SkW∙ AS L(DTr； SkwVk) ≤
mink=k* ,ι≤k≤κ L(DTr; SkVTkTlner) - e, by the mean value theorem, L(DT:r； SkWt ,t+ιv" =
L(DTr； SkWt,tVkt)	+	ηtVeC (Vsk*	L(DTtr)； SkWVkt)	|Sk*	=SkW	,ξ)	VeC(SkW,t+1	- SkW	,t)	≤
τt t	τt	τt	t	τt τt ,	τ	τt
L(DT(ttr) ; SkτW	,tVTkt )	+	2mηtβ1β2β3	≤	mink6=kτW	,1≤k≤KL(DTtrt; Sk,tVT(kt,)Tinner),	where SkτWt,ξ	∈
[SkW ,t+1, SkW ,t], thus, kTk is unchanged within one meta step. Let kt ≡ kTk for notation simplicity.
Using the Taylor expansion, it follows that
Lmeta(Sk,t+1)
≤ Lmeta(Sk,t) + Vec(VSk,tLmeta(Sk,t))> VeC (Sk,t+1 - Sk,t) + 1^2k IISk,t+1 - Sk,tk2
= Lmeta(Sk,t) - I(kt, k)ηt VeC(VSk,tLmeta(Sk,t))> VeC(VSk,tL(DTvtl; Sk,tVTkt))
+ I(kt,k)η2β1β2∣Vsk,tL(DTt； Sk,tVTt)k2
≤ Lmeta(Sk,t) — I (kt,脸由 VeC(VS叱 Lmeta(Sk,t))> VeC (Vs3, Lg ； Sk,"" -	Lmeta(Sk,t))
+ I(kt,k)ηt∣Vsk,t Lmeta(Sk,t)k2 + I (kt, k) ^^ 1%*, L(DT ； Sk^kt )∣2 ∙
Take conditional expectation w.r.t. Sk,t on both sides, then take the total expectation, we have
ELmeta(Sk,t+1) ≤ ELmeta(Sk,t) — EI (kt,k)ηt(1 — ^^ )∣Vs% Lmeta(Sk,t)『+ EI (k, k) βλβ2^
≤ ELmeta (Sk,t) — EI (kt, k) ηt ∣VSk Lmeta(Sk,t)k2 + EI (k, k) "W ,	(4)
where We have used 1 — nt.β2 ≥ ɪ to obtain (4). Summing the above inequality over t, and
rearranging it, we obtain
T
η2tE X I(kt,k)
t=1
min
1≤t≤T
EIVSkLmeta(Sk,t)I2≤ELmeta(Sk,0)+
ηtββσ2E X I(kt,k).
t=1
(5)
14
Under review as a conference paper at ICLR 2022
Dividing both sides by 婴 E PT=11 (kt, k), as ηt = min f2mβ"β2β3, √1τ ), We obtain
and conclude that
min
1≤t≤T
EkVSk"Lmeta(Sk,t)k2 ≤O
σ	σ2√T
∖E PT=11 (kt,k)
1≤mti≤nT EkV[Sk,t,...,SK,t]Lmeta(S1,t,..., SK,t)k2 ≤O
(XX	σ√
k⅛ E PT=1 IH
If E PT=1 I(kt,k) = T ,then
σ2 K2
1m≤τ EkV[Sι,t,...,Sκ,t]Lmeta (Sl,t,∙∙∙, Sκ,t)k2 = O (丁
(6)
(7)
(8)
□
B Proof of Theorem 2
Proof. For notation simplicity, We omit the superscript of τ0, and let z = (x, y) denote samples.
(i) Let V耍 k = arg minvτ L(DTr； SkVT). We aim to show that the expected generalization gap
R(T 0; Si,..., Sk ) - R(T 0; Sι,..., SK) = ETEDTr 忸 z~"(f (x; Sk vQ y) - L(DTr ； Sk vT,k)]
is bounded by an order of O(入Nm). Let DTr(i) be a training set only differs with DT in the
ith sample, ie, Da⑻ ≡ (DTr - {zi}) ∪ {zi}. And let v；(? ≡ argminvτ L(DTr(i); SkVT).
We will show that the solution obtained by minimizing the losses on DTtr is close to the
one on DTr(i): 1) EDTrEz~"(f(x; SkvT,Q,y) = Nr PNlEDTrEzi~"(f(xi; Skv；®),*)=
Nr P=1 EDτr(i)Ezi~"(fX; SkvT(k)),yi) = Nr PNr EDTrEzi~T'(fX; SkvT(k)),yi); 2)
EDTrL(DT； SkvT,k) = N； PNI EDTrEz0~tL①T； Skv；,k). Hence, the expected generalization
gap satisfies
∣EDTr [Ez~t'(f (x; SkvT,k), y) - LM； Skv/)] ∣
1 Ntr
≤ N- EEDTrEzi"'(f(xi； SkvT(k)),yi) - '(f(Xi; SkvT,k),yi)∣
tr i=1
Ntr
≤ N- X kSkvT(k) - SkvT,k k
Ntr
i=1
Ntr
≤ N- X kSk ι∣FkvT(k)- vT,kk
Ntr
i=1
β1β3√m ∖tr I， *(i)	* H
≤ F- TkVTY- vτ,k k
tr i=1
β1β3K √m
≤ - Nr-,
(Lipschitz)
(Assumption 2)
where β1 is the Lipschitz constant of `(f (x; w), y), β4 is the bound of basis vectors as they stay in
a compact set, and kXkF is the Frobenius norm. As the above analysis is independent of the choice
of k, we conclude that
R(τ 0； Si,..., SK )-R(τ0; Si,..., SK) ≤O
(9)
15
Under review as a conference paper at ICLR 2022
(ii) Let WT = argminwτ EZ〜T'(z; WT) and k； = argmi□ι≤k≤κ dist(wθ,Sk). The excess risk
satisfies
0 ≤R(τ0; Si,..., SK )-Ro(τ 0)
≤ Edt，[Ez-"(f (x; S碑vT), y) — L(DTr； SW vT)]
X--------------------{z------------------}
≤o( KN√m)
+EDτr N1tr X?(f(x; SkTH Ez~T'(f(x; wo),y)
≤O
+	EDtr
;SkTVT)-L(DTr； wO,Sko)]
—一一	J
"^^^^^^^^^^^^^^^^^^^^^{^^^^^^^^^^^^^^^^^^^^^""^
SkT vτT is the optimal solution in the subspaces, thus, this term≤0
+ EDTrkVwL(DTr；ξ)kkwθs⊥ k
τ	, koτ
(by the mean aalue theorem)
≤O
+ β2 kwTo kS⊥o
kτ
where we have decomposed wTo = wTo S o + wo S⊥ , ξT ∈
T , kτo	T,Sko
follows by the Lipschitz-smoothness. We conclude that
[wTo,S o , wTo ], and the last inequality
R(τ0; Si,..., Sk )-Ro(τ0) ≤O
+
min dist(wo0
i≤k≤K T
□
16