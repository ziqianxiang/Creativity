Under review as a conference paper at ICLR 2022
Arbitrary-Depth Universal Approximation
Theorems for Operator Neural Networks
Anonymous authors
Paper under double-blind review
Ab stract
The standard Universal Approximation Theorem for operator neural networks
(NNs) holds for arbitrary width and bounded depth. Here, we prove that operator
NNs of bounded width and arbitrary depth are universal approximators for con-
tinuous nonlinear operators. In our main result, we prove that for non-polynomial
activation functions that are continuously differentiable at a point with a nonzero
derivative, one can construct an operator NN of width five, whose inputs are real
numbers with finite decimal representations, that is arbitrarily close to any given
continuous nonlinear operator. We derive an analogous result for non-affine poly-
nomial activation functions. We also show that depth has theoretical advantages
by constructing operator ReLU NNs of depth 2k3 + 8 and constant width that can-
not be well-approximated by any operator ReLU NN of depth k, unless its width
is exponential in k.
1	Introduction
In the approximation theory of neural networks (NNs), universal approximation theorems (UATs)
are statements that establish the density of a class of NNs within a space of mappings. Thus, UATs
imply that NNs represent a wide variety of mappings when given appropriate weights and biases. A
NN is characterized by its activation function (e.g., ReLU, sigmoid), connectivity (e.g., feedforward,
recurrent), width (number of neurons per layer), and depth (number of layers). Operator NNs are a
family of NNs for approximating nonlinear operators (Chen & Chen, 1995; Kovachki et al., 2021; Lu
et al., 2021). These are critical for learning dynamical systems using DeepONets (Lu et al., 2019;
Cai et al., 2021; Lanthaler et al., 2021), inverse mapping problems (Adler & Oktem, 2017), and
functional data analysis (Rossi et al., 2005). UATs for operator NNs are a fundamental theoretical
underpinning for such applications. While there are UATs for wide, shallow operator NNs (Chen &
Chen, 1995), we derive the first set of UATs for their deep, narrow counterparts. These results are
key to understanding the expressibility of deep operator NNs.
There are well-established theoretical advantages of deep, narrow NNs over wide, shallow ones in
terms of expressibility. In particular, there are 3-layer NNs representing radial functions on Rd
that cannot be approximated by a 2-layer NN to more than a constant accuracy, unless its width is
exponential in d, where a NN’s depth is the number of hidden layers plus one output layer (Eldan &
Shamir, 2016). Moreover, for any k ∈ Z, there are Θ(k3)-deep NNs of constant width which, when
restricted to the unit cube [0, 1]d, cannot be approximated by a NN with O(k) depth, unless it has
Ω(2k) width (Telgarsky, 2016). In Section 2, We construct operators that require exponentially wide
ReLU NNs, analogous to Telgarsky (2016). Hence, the improved expressibility of deep, narrow
operator NNs over shallow, wide ones is similar to standard NNs. UATs for deep, narrow operator
NNs are thus needed to establish their approximation power.
In Section 3, we prove that an operator NN of arbitrary depth and constant width is a universal
approximator of nonlinear continuous operators if the activation function is continuously differen-
tiable at a point with nonzero derivative. Our key insight is to use input encoding and reduction of
truncated values to decrease the width of the NN to a constant. We thus propagate inputs from one
layer to the next with a single neuron. We truncate inputs to a number of digits based on a precision
ε > 0 and concatenate the truncated values into one value. We extract each truncated input with
a decoder function, which we approximate with an arbitrarily deep NN as described in Kidger &
1
Under review as a conference paper at ICLR 2022
Lyons (2019). A related approach is used in Shen et al. (2021a;b), where inputs are used in their
encoded forms. However, we extract the original value from its encoding.
Our work builds on well-established UATs. An early UAT by Pinkus (1999) states that an arbitrarily
wide one-layer NN with a continuous non-polynomial activation function can approximate all con-
tinuous functions on compact sets. Kidger & Lyons (2019) prove a UAT for arbitrarily deep NNs
with n inputs, m outputs, and of width n + m + 2. An arbitrarily wide operator NNs UAT is given
in Chen & Chen (1995). They prove the standard UAT with a fixed set of weights and biases, then
give an arbitrary-width UAT for nonlinear continuous functionals and operators. Though Kidger &
Lyons (2019) turns the arbitrary-width UAT into one of arbitrary depth, their technique does not
extend to operator NNs. In particular, it requires the width of the NN to depend on the size of the
sampling device, which depends on the precision ε. Consequently, we would have n+m(ε)+5 neu-
rons in every hidden layer of the operator NN. This is impractical, since in most cases m(ε) → ∞ as
ε → 0, resulting in a NN that is both deep and wide. In contrast, our result only requires a constant
width operator NN.
The above UATs all utilize the multi-layer feedforward perceptron (MLP) model. Given an input
vector X ∈ Rk0 and activation function σ : R → R, the output 夕(x) ∈ RkN is calculated as
2(x) = WNσ (WN-1 (…(W2σ (Wιx + θι)+ θ2) + …)+ Θn-1) + Θn,
where k0, . . . , kN ∈ N, with weights Wi ∈ Rki×ki-1 and biases θi ∈ Rki. Here, σ is applied
entry-wise to the vector, i.e., σ(a)j = σ(aj). UATs concern the density of the following space:
M(σ) := span{σ(w>X - θ) | θ ∈ R,w ∈ Rn},
where n is the input dimension. We say σ has the density property if M(σ) is dense in C(Rn)
equipped with the topology of uniform convergence on compact sets. The definition of the density
property is independent of the dimension n of the input space. Moreover, all continuous, non-
polynomial activation functions have the density property (Pinkus, 1999).
Main Contributions. We show that deep, narrow NNs are better than shallow, wide ones at ap-
proximating certain continuous nonlinear operators, in the sense that significantly fewer neurons are
needed to achieve the same accuracy (see Theorem 1). We also show that after truncating inputs,
deep NNs of width five can be used to uniformly approximate continuous real-valued functions on
compact sets, regardless of the domain’s dimension (see Theorems 3 & 4). Finally, we give the first
arbitrary-depth UAT for operators with a general class of activation functions (see Theorem 5).
2	Advantages of Depth for Operator Neural Networks
There are many advantages of deep, narrow NNs over wide, shallow ones. In particular, some
functions are computable by a NN with two hidden layers but require exponentially many neurons
ofa NN with one hidden layer (Eldan & Shamir, 2016). This demonstrates the expressive power of
deep NNs. However, this result is achieved by considering the L2 distance between two functions
on the entirety of Rd . As our main results are concerned with approximating continuous functions
and operators on compact sets, we prove the following more powerful result than Eldan & Shamir
(2016) for the operator case, inspired by Theorem 1.1 of Telgarsky (2016).
Theorem 1. Let X be a Banach space, K1 ⊆ X be compact, andV ⊆ C(K1) be compact. Then, for
any integers n, k ≥ 1, there exists a nonlinear continuous operator Gk : V → C ([0, 1]n) such that
1.	There is a ReLU NN φ : [0,1]n → R of depth 2k3 + 8 and width in Θ(1) such that
2(y) = Gk (u)(y) ,for any U ∈ V and y ∈ [0,1]n.
2.	Let m ≥ 1 be an integer. Let ψ : [0, 1]n+m → R be a ReLU NN with n + m inputs, depth
≤ k, and ≤ 2k total nodes. Then for any prescribed x1, . . . , xm ∈ K1 and u ∈ V, we have
]o IjGk(u)(y) — Ψ (u(χι),…,u(χm),y)| dy ≥ 614.
Proof. Let k ≥ 1 and 夕:[0,1]n → R be the ReLU NN constructed in Theorem 1.1 of Telgarsky
(2016) with depth 2k3 + 8 and width in Θ(1). The first statement of our theorem follows from
2
Under review as a conference paper at ICLR 2022
considering the constant operator Gk : V → C ([0,1]n), u → 夕.To prove 2, let ψ : [0,1]n+m → R
be any ReLU NN of depth ≤ k with ≤ 2k total nodes. Let x1, . . . , xm ∈ K1 be the prescribed
sampling device and u ∈ V. Then, define ψu as follows:
ψu : [0, 1]n → R, y 7→ ψ (u(x1), . . . , u(xm),y) .
Since u(x1), . . . , u(xm) can be added onto the first layer’s bias term, ψu is a NN with n inputs,
≤ k layers, and ≤ 2k total nodes. The second statement of the theorem holds by Theorem 1.1
of Telgarsky (2016), as the ReLU activation function is a (1,1,1)-semi-algebraic gate.	□
Theorem 1 illustrates that increasing the depth of a NN can make operator approximation much less
expensive. This suggests that UATs for deep operator NNs comprise an important contribution to
our understanding of the limitations of deep learning and expressibility of nonlinear operators.
3 Construction of the Deep Narrow Operator Neural Network
We present two results on the existence of a deep NN approximation of a nonlinear continuous oper-
ator. One is an explicit reconnection ofan existing wide NN and the other is an abstract existence ar-
gument. In this section, X is a Banach space, and K1 ⊂ X is compact. Let V ⊂ C(K1) := C(K1, R)
be compact in C(K1), which is equipped with the topology induced by the uniform norm. Suppose
that n ∈ N, K2 ⊂ Rn is compact, and G : V → C(K2) is a nonlinear continuous operator. In Chen
& Chen (1995), it is shown that G can be uniformly approximated by a 4-layer NN if the activa-
tion function has the density property. More precisely, given any ε > 0, there are positive integers
M, N, m ∈ N, real numbers cik, ζk, ξikj ∈ R, vectors ωk ∈ Rn, and sensors xj ∈ K1 such that
NM	m
G(UXy)- X XCkσ∣ XξjU(Xj) + θk I ∣σ(ωk ∙ y + Zk) <ε,
k=1 i=1	j=1
for all U ∈ V and y ∈ K2 . The architecture of this NN is shown in Figure 1 (left). The input
layer consists of y = (y1, . . . , yn) and (U1, . . . , Um) = (U(x1), . . . , U(xm)). The second layer
computes Pk = σ(Pm=I ξju(xj) + θk). The third layer computes rk = σ(ωk ∙ y + Zk) and
qk = PiM=1 Cikpik. The fourth layer consists of multiplication neurons that compute sk = rkqk for
k = 1, . . . , N, whose sum is the output of the NN.
3.1	Register-Compute Neural Networks
In a fully connected feedforward NN, connections between non-consecutive layers are not allowed.
Such NNs are “memoryless,” as a neuron in thejth layer receives no input other than the output from
the (j - 1)th layer. One can introduce memory into a NN by showing that a neuron with a particular
activation function, weights, and bias can uniformly approximate the identity function on a compact
set Kidger & Lyons (2019). We use such neurons to propagate the inputs through the layers of our
NN to use them in later computations. This motivates the following definition of the basic model in
our construction.
Definition 1. Let p, q ∈ N. A (p, q)-register-compute NN is a fully connected feedforward NN with
p + q neurons in each hidden layer. In each layer, p neurons are called registers, ordered so that the
only nonzero weight in the jth register of layer i is from the output of the jth register of layer i - 1.
Although all pairs of neurons in consecutive layers are connected in a fully connected feedforward
NN, we effectively “disconnect” non-corresponding registers by setting the weights to be zero.
3.2	Constructing the First Deep Operator NN : Reconnecting the Wide
Operator NN
We observe that neurons in each hidden layer of the operator NN in Chen & Chen (1995) can be
moved one-by-one into different hidden layers. Moreover, ifσ has the properties in Theorem 2, then
a σ -activated neuron can be used to uniformly approximate the identity map ιK on any compact set
K ⊂ R (Lemma 4.1 of Kidger & Lyons (2019)). This allows us to propagate inputs from one layer
3
Under review as a conference paper at ICLR 2022
Figure 1: Left: The wide operator NN from Chen & Chen (1995). Right: Our encoder model.
to the next. By rearranging the neurons in the shallow, wide operator NN, we get a deep NN whose
width depends only on the size of the input layer.
Theorem 2. Let σ : R → R have the density property. Suppose that σ is also continuously differen-
tiable at one or more points with a nonzero derivative. Then, for any ε > 0, there exists a function
F : Rm+n → R represented by a σ-activated NN of width at most m + n + 5 such that
|G(u)(y) - F(u(x1), . . . , u(xm), y)| < ε
for all u ∈ V and y ∈ K2. Moreover, if a σ-activated NN of width 3 and depth L approximates the
multiplication map (a, b) 7→ ab on any compact set up to any uniform error, then the network F has
depth in O((M + L)N), where M, N, m, {xj}jm=1 are as in Theorem 5 of Chen & Chen (1995).
Proof. Let H : Rm+n → R be the function given by the NN in Theorem 5 of Chen & Chen (1995)
that approximates the operator G to within ε∕5. We construct an (m + n, 5)-register-compute NN
F with m + n inputs, one output, and (M + L + 1)N + 1 layers, where L is a positive integer
defined later in equation a. Among the 5 neurons that are not registers in each hidden layer, 1
neuron is referred to as the output augmenter. 2 neurons are referred to as the adder 1 and the adder
2, respectively, and the remaining 2 neurons are referred to as the computation neurons.
The m + n input layer values are passed into the corresponding m + n registers in the first hidden
layer. A register that receives a value u(xj) is called a u-register. If this register is in the kth hidden
layer, then we denote its output by ujk. A y-register and its output yjk are similarly defined. We also
define u0 = u(xj) and yj = y§. Up to a small error so that ε4 in equation 2 satisfies 归4∣ < ε∕5
for all u ∈ V, y ∈ K2 , each register computes a function that is close to the identity function ιL in
L∞(L), where L is the range of the output of the previous register.
We further divide the (M + L + 1)N hidden layers into N sections of M + L + 1 layers. In the kth
section, the ith adder 1 in each layer computes:
pk = σ(XXξjUjM+L+1)(k-1)+i-1 + θk j ,	1 ≤ i ≤ M
using the outputs of the u-registers u(jM +L+1)(k-1)+i-1 from the previous hidden layer.
Let qk = Ckpk-ι + qk-ι, where qk-ι is the output of the (i - 1)th adder 2 in the kth section. We set
Pk = qk = 0. The affine transformation of the ith adder 2 in kth section computes qk and, together
with the activation function, propagates qqik using the identity approximation mentioned above, up to
a small error so that ε3 in equation 1 satisfies ∣ε3∣ < ε∕5 for all U ∈ V, y ∈ K2. The output is then
denoted by qik .
4
Under review as a conference paper at ICLR 2022
The (M + 1)th adder 1 in the kth section computes:
rk= σ(3k ∙ y(M+L+I)(I)+M + Zk)
using the outputs of the y-registers of the previous hidden layer:
(M+L+1)(k-1)+M	( (M+L+1)(k-1)+M	(M +L+1)(k-1)+M)
y	= (y1	, . . . , yn	),
The (M + 1)th adder 2 in the kth section propagates qMk , and its output is denoted by qk.
In the next Lk layers, we can use adder 1, adder 2, and the 2 computation neurons to approximate
rkqk up to an error so that ε? in equation 1 satisfies ∣ε2∣ < ε∕5 for all U ∈ V, y ∈ K2 (Proposition 4.9
of Kidger & Lyons (2019)). This number, denoted by sk, is then added to the output augmenter
which, unless otherwise stated, propagates the value from the previous layer, up to an error so that
ει in equation 1 satisfies ∣ει∣ < ε∕5 for all U ∈ V, y ∈ K2. The initial value in the output augmenter
is set to 0.
We set
L:
max
1≤k≤N
Lk .
(a)
We assume that any neurons from layer Lk + 1 to L in the kth section do nothing but propagate
the values from the previous hidden layer. Once the Nth section is computed, we add sN to the
augmenter. Now, the value of the augmenter in the ((M + L)N + 1)th layer is given by Su,y, where
N
Su,y=Xsk+ε1
k=1
N	NM
Xqkrk+ε1+ε2=X	XCikpik-1	rk+ε1+ε2+ε3
k=1	k=1	i=1
N
X
k=1
Mm
XCkσ (Xξjuj(k)+i-1 + θkj I g(3k ∙ y'(k)+M + Zk) + ει + ε + ε3
N
X
k=1
Mm
X Ck σ (X ξj U(Xj) + θj jlg(3k ∙ y + Zk ) + ε1 + ε2 + ε3 + ε4
H(U(x1), . . ., U(xm), y) +ε1 +ε2 +ε3 +ε4,
(1)
(2)
(3)
where '(k) = (M + L + 1)(k - 1). Since ∣εj| < ε∕5 for j = 1,2,3,4, We have
|G(U)(y) - Su,y| ≤ |G(U)(y) - H(U(x1), . . .,U(xj),y)| + |(H(U(x1), . . . ,U(xj),y) - Su,y|
≤ - + lει| + lε2| + 归3l + 归4l < ε
5
for all U ∈ V, y ∈ K2. The result follows as Su,y = F (U(x1), . . . , U(xm), y).
□
Theorem 2 is a theoretical guarantee that a deep NN can approximate the operator G. In particular,
the width of our NN does not depend on M and N in Theorem 5 in Chen & Chen (1995), where
these parameters are obtained abstractly and do not have intuitive interpretations.
Theorem 2 has two shortcomings. First, the total number of neurons in the deep operator NN in
Theorem 2 is Ω((m + n)(M + L)N), whereas that of a shallow, wide NN in Chen & Chen (1995)
is O(m + n + MN). We emphasize, however, that the NN in Theorem 2 is not necessarily the
simplest one to achieve an ε-approximation. In fact, deep NNs can outperform the shallow ones in
approximating certain operators (see Section 2).
Second, the NN’s width depends on m, which in turn depends on ε. Thus, while we have eliminated
the dependence of the width on M and N, the number of sensors is reflected in the width, and a large
sampling device is needed to achieve an accurate approximation, making the NN both arbitrarily
deep and arbitrarily wide. To address this, we have two avenues. First, we find a relationship
between m and ε. Proving some rate of growth of m with respect to ε would make Theorem 2 more
informative, as in Lu et al. (2019), for example. However, results of this type are in a more specific
context, and a relationship between m and ε in the general setting is challenging. Also, to prevent the
width of our deep NN from growing too fast as ε → 0, we would like to have m(ε) = O (log(1 /ε)).
So far, we are not aware of any existing result that demonstrates that m(ε) = O(log(1∕ε)) is
possible. Instead, we need to find a way to reduce the width of the network, regardless of the
number of inputs.
5
UnderreVieW as a ConferenCe PaPersICLR 2022
Figure∙-A PortiOIl Ofthe deep NN developed by transforming the Wide OPemtOr NN from CheIl 浮
CheIl (1995) into a re-StermodeL The PrOCeSS CaIl be found5Kidger 浮 LyOllS (2019∙
3∙3 INPUT ENCoDING AND REDUCTIoN
It has been ShInthatarbitrariIy—deep NNS Of Width m + 3 Can uniformly approXimate functions
IKJWhere 因 C is a COmPaCt Set(Kidger 浮 LyolI2019We further reduce this Width
to a COlIStaIlL eliminating the dependence on Tn. HleVeLis known that for CertaiIl activation
functnthe Width Tn is IIOt enough to uniformly approximate COntinUOUSfUIICtns on COmPaCt
SetS (HalIiIl 浮 Se=k2017; LU et al: 2017TherefOrWe SHghtIy modify the architecture Ofthe
NNS S make rhem more nebl
IIl Kidger 浮 LyOnS (2019-3 inputs are PrOPagated throughout the entire IIetWOFwhich requires
Tn IIeUrOnSiIl each hidden layer Our trick is to truncate the inputs USg the HOOr function and then
encode them into a SiIIgIe IIeUrOiL ThiS single IIeUrOIl is then PrOPagated USilIg 0y OIle IIeUrOIl from
OIle hidden IayertO the IIeXt and is decoded WheIl necessary，WheIl decodthe inputs are decoded
One—bonand then We immediateIy PaSS the decoded VaIUe into the COmPUtation IIeUrOns∙
We first define terminoIOgy for tsncating and encoding inputA Irncq IiOn neuron takes an input
and ProdUCeS the OUtPUt LiOK2」"where KiSan arbitrary integer A NN WHh ITlmealed MPUlSiS a NN
Where every input has been PaSSed through a truncation IIeUrOiL The WidOfthe NN With truncated
inputs is the SiZe Ofthe IargeSt hidden IayeL ignoring the truncation IIeUrOnS applied immediateIy to
the input Iayer TherefOr 尸 a g q)—TegisierEompe NN With truncated inputs is a NN With truncated
inputs that is a (qregiSter—compute NN if the OUtPUtS Ofthe truncation Iayer are VieWed as the
inputs Ofthe NN∙ A Q—activatedN With truncated inputs is a NN WhOSe neuronsall hidden
layers have Q as the activatIIfUIIetignoring the truncation IIeUrOnS applied immediateIy to the
input Iayer
Theorem Suppose Q is non—polynomia一 and ConHnIloIIsiy dveren-iable at One Or more Poin-S
wh nonzero deri.VaHVLel IKCl^^ beComPaClSe 尸 and zji Co0s∙ Fbr each
v Q Ihere is a βmn ¾ represenled by a QIaCHVaIed NN wh IrUnCaled inpιιls Of
Widlh 5 SllCh Ihal
三(X) —g(X)-八 F XmllC
PrOof WithOUtIOSS Of generality we Iet IKC (OjI)OtherWiSWe SCaIe and translate the domain
With the truncation IIeUrOIl and bias teπnsthe Hrst hidden layer There is an identity—activated
2)-TegiSter—compute NN"可里 →SUCh that 一 h(x) I Ax八 s/3 for all XmiK (Kidger
浮 Lyon2019EaCh Ofthe S inputs is PaSSed into a unique reSter in the first hiddenyeL
then propagated by the corresponding reStereach hidden layer AmOlIg the two remaining
IlOII—register IleUrOIlSiIl each hidden IayeL One IleUrOIl is the ComP main6rowhich applies an
6
Under review as a conference paper at ICLR 2022
Figure 3: Our decoder model for non-polynomial σ (top) and polynomial σ (bottom).
affine transformation to the outputs of the registers in the previous hidden layer. The other neuron is
the augmentation neuron, which sums the outputs of the computation neuron and the augmentation
neuron in the previous layer. The output of the first augmentation neuron is set to zero.
Since the NN h is identity-activated, it can be restructured so that each computation neuron only
reads one input from the registers and its own output from the previous layer, and applies an affine
transformation. To see this, let X → P'=ι Wj Xj + b be a computation neuron. We replace the layer
of this neuron by ` layers and use the computation neuron from each of the ` layers to compute
W1X1, W1X1 + W2X2,..., Pj=1 WjXj, P'=ι WjXj + b, respectively. Each of the remaining neurons
in the ` layers applies the identity to the corresponding output from the previous layer. Let L + 1 be
the depth of this restructured NN.
We now show how we can store input approximations in a single neuron. For large κ ∈ N, we let
Xj = [10κXjC for 1 ≤ j ≤ n be the truncated inputs. The register in the first hidden layer computes
n
r := X 10-jκXj = 10-κXi + 10-2κX2 + ∙∙∙ + 10-nκXn,
j=1
where the remaining registers take the input r and pass it as the output. Now, we define a series of
decoderfunctions, φι,..., Pn. Every a = 10-nκM ∈ [0,1), M ∈ No can be expanded uniquely as
a = aι10-κ + a2i0-2κ + •一+ am10-nκ,
where a1, . . . , am are integers in [0, 10κ]. We set Pj ([a - 10-nκ-1, a + 10-nκ-1]) := 10-nκaj for
each a and then extend Pj to the interval [0, 1] continuously by the Tietze Extension Theorem.
Now, we construct a (1, 4)-register-compute NN with truncated inputs. We let p : Rn → R be
the function it represents. Unlike most fully connected feedforward NNs, the neurons in each layer
have different activation functions. The register uses the identity activation function. Among the
four remaining neurons in each hidden layer, one neuron is called the computation neuron, and one
neuron is called the augmentation neuron, which uses the identity activation function. The remaining
two neurons are called the decoder neurons, which use σ as the activation function.
Let i ∈ {1, . . . , L}. In the NN h, by assumption, only one of X1 , . . . , Xn, say Xj, is read by the
computation neuron in the ith layer. We construct the NN p by building L + 1 chunks, where the
last chunk is the output layer. To construct the ith chunk, we use the two decoder neurons from each
hidden layer together with the register to approximate Pj(r) up to a small error, as in Proposition 4.9
of Kidger & Lyons (2019). We note that Pj (r) = 10-κXj ≈ Xj. This decoded value is then passed
into the computation neuron for the affine transformation done at the ith layer in the NN h.
Compared to h, the difference in the output of p is induced by two steps: the truncating X to obtain
10-κX, and decoding to obtain an approximation of φj(r) = 10-κX. The first error can be made
arbitrarily small by taking κ large enough and the second error can also be made arbitrarily small as
in the previous paragraph. Thus, We can construct the NN P so that |p(x) - h(x)∣ < ε∕3 for X ∈ K.
7
Under review as a conference paper at ICLR 2022
It remains to construct a NN with truncated inputs that only uses σ as the activation function. To
do so, we define a NN g : Rn → R whose architecture completely inherits that of p, except the
registers, computation neurons, and augmentation neurons are σ-activated. As before, we may use
a σ-activated neuron to mimic the identity activation function. For the register, we make the ap-
proximate identity accurate enough so that the perturbed value of a, denoted by a, always satisfies
|a - α∣ < ιo-nκ-1. Hence, we have that Wj (a) ≡ Wj (a) throughout the entire NN. Since the values
in the computation neurons and the augmentation neurons can be propagated arbitrarily accurately,
we have |g(x) - p(x)∣ < ε∕3 for X ∈ K. Therefore, we have
|g(x) - f (x)| ≤ |g(x) - p(x)| + |p(x) - h(x)| + |h(x) - f (x)| < ε, x ∈ K.
□
Theorem 3 shows that truncating inputs allows any continuous function on a compact set to be
uniformly approximated by deep NNs of constant width. This independence of width and dimension
overcomes the problematic growth of the size of the sampling device in Lu et al. (2019). Since
non-affine polynomial activation functions satisfy the arbitrary-depth UAT, we obtain Theorem 4.
Analogous to Theorem 3, which extends Proposition 4.9 of Kidger & Lyons (2019), Theorem 4
naturally extends Proposition 4.11 of Kidger & Lyons (2019). As opposed to wide NNs, deep NNs
with (non-affine) polynomial activation functions approximate continuous functions nicely.
Theorem 4. Let K ⊂ Rn be a compact set and f : K → R be a continuous function. For each
ε > 0, there is a function g : Rn → R represented by a σ-activated NN with truncated inputs of
width 6 such that
|f (X) - g(X)| < ε, X ∈ K.
Proof. Without loss of generality, let K ⊂ (0,1)n. Let P = Pj=ι Pj ： K → R be a polynomial
with monomials Pj such that |f (x) - p(x)∣ < ε∕3 for X ∈ K. There is a (n, 4)-register-compute
NN h : Rn → R satisfying |h(x) -p(x)∣ < ε∕3 for X ∈ K, where each hidden layer contains
n identity-activated registers that propagate the n inputs, one σ-activated augmentation neuron that
stores the output and never takes any register as an input, and three σ-activated computation neurons
that compute the monomials Pj (Proposition 4.6 and Proposition 4.11 in Kidger & Lyons (2019)).
The computation neurons take no more than one value from the registers as the input in each layer.
Moreover, when these neurons need inputs from one of the registers, the outputs of all but possibly
one in the previous layer do not become the input of any other neurons in the current layer.
As in the proof of the Theorem 3, we can construct a σ-activated NN g : Rn → R with truncated
input of width 8 such that |h(x) - g(x)∣ < ε∕3,for X ∈ K. In particular, each hidden layer contains
one register that propagates the encoded input as in the proof of Theorem 3, three decoder neurons
that, together with the register, approximate the decoders W1, . . . , Wn (Proposition 4.11 in Kidger &
Lyons (2019)), 3 computations neurons as in h, and 1 augmentation neuron as in h.
Finally, when a decoder neuron is activated and its output becomes an input to the next layer, the
computation neurons read the input from the register. However, when the computation neurons read
inputs from the register, the outputs of two of them in the previous layer are not used in the current
layer. Thus, two computation neurons can be reused in the architecture of the decoder. Hence, only
3+3-2 = 4 neurons are used to implement the decoder and the computation unit, and consequently,
g can be realized by a NN with truncated inputs of width 6. Now, for all X ∈ K, we have
|g(X) - f (X)| ≤ |g(X) - h(X)| + |h(X) - P(X)| + |P(X) - f (X)| < ε.
□
We note that the success of the encoder/decoder does not depend on the representation being dec-
imal. They can be equivalently constructed using binary representations of numbers, so that if the
operation in the truncation neuron is x 7→ b2κxc, Theorem 3 and Theorem 4 still hold. This result
is more relevant to most modern machines, which store floating-point numbers with finitely many
bits, conduct floating-point arithmetic in binary, and perform x 7→ 2κx easily.
8
Under review as a conference paper at ICLR 2022
3.4 Constructing the Second Deep Operator NN: an Abstract Approach
Now, we have the tools to eliminate the dependence of the NN’s width on the size of the sampling
device. We adopt an abstract strategy to construct an operator NN with truncation whose width is a
constant. To do so, we view the NN in Chen & Chen (1995) as a function f from Rm+n to R, for
we encode the input function u as m values. Therefore, to approximate the operator G, it suffices to
approximate f uniformly.
Theorem 5. Let σ be non-polynomial (resp. non-affine), continuously differentiable at one or more
points with nonzero derivative. Then, for every ε > 0, there are points x1, . . . , xm ∈ K1 and a
function F : Rm+n → R given by a σ-activated NN with truncated inputs of width 5 (resp. 6), such
that
|G(u)(y) - F(u(x1),...,u(xm),y)| < ε
for all u ∈ V and y ∈ K2. Moreover, m is independent of σ.
Proof. Define Uj = {u(xj) | u ∈ V} and U = Qjm=1 Uj. The evaluation map φj : V → R, u 7→
u(xj) is continuous. Hence, Uj = φj (V) is compact for each j, and so is U by Tychonoff’s
Theorem. Let g : R → R be an arbitrary function with the density property. This function induces
points x1 , . . . , xm ∈ K1 and a function H : U × K2 → R such that
∣G(u)(y) - H(U(X ι),...,u(xm), y)| < ε∕2
for any u ∈ V and y ∈ K2 (Theorem 5 in Chen & Chen (1995)). Let F : Rm+n → R be
the function represented by the NN with truncated inputs constructed in Theorem 3 or Theorem 4
associated with the function H and the approximation error ε∕2. The statement of the theorem
follows from the triangle inequality and the fact that g is arbitrary, making m independent of δ. □
Compared to Theorem 2, Theorem 5 gives us a deep operator NN whose width is constant. More-
over, it allows us to use non-affine polynomial activation functions, which are known to be powerless
in approximating using the 2-layer networks (Pinkus, 1999). Inspired by Proposition 4.17 of Kidger
& Lyons (2019), we have the following extension of Theorem 5.
Corollary 6. Let σ : R → R be a polynomial such that σ0 (α) = 0 and σ00(α) 6= 0 for some
α ∈ R. Then, for every ε > 0, there exist points x1, . . . , xm ∈ K1 and a function F : Rm+n → R
represented by a σ-activated NN with truncated inputs of width 5, such that
|G(u)(y) - F(u(x1), . . . , u(xm),y)| < ε, u ∈ V,	y ∈ K2.
Proof. The proof follows from Theorem 4. The NN h in Theorem 4 can be implemented using a
(m, 3)-register-compute NN. Two neurons can implement the decoder in g. When the decoder is
activated, it uses one of the two computation neurons. The rest of the proof is then analogous to the
proof of Theorem 5 and the width of g is 2 + 2 + 2 - 1 = 5, where the first “2” corresponds to the
register and the output augmenter. The second and the third “2”s are the number of neurons needed
to implement the decoder and the number of computation neurons, respectively.	□
Corollary 6 in combination with the non-polynomial σ case means that “most” activation functions
require our NN with truncated inputs to have a width of 5. This is a slight improvement compared
to Theorem 5, in which we require a width of 6 when σ is a non-affine polynomial.
4 Conclusion
This paper proves that arbitrary-depth operator NNs with a large class of activation functions are
universal approximators. Our main theorem is a UAT for operator NNs of width 5 with a non-
polynomial that is continuously differentiable at a point with nonzero derivative (see Theorem 5).
Our proof technique is robust enough to handle non-affine polynomial activation functions too (see
Theorem 4 and Corollary 6). We also construct an operator ReLU NN of depth 2k3 +8 and constant
width that cannot be well-approximated by any operator ReLU NN of depth k, unless its width
is exponential in k (see Theorem 1). This demonstrates that deep, narrow NNs are better than
shallow, wide ones at approximating certain continuous nonlinear operators. We hope that this adds
theoretical justification to those that use deep operator NNs.
9
Under review as a conference paper at ICLR 2022
References
Jonas Adler and Ozan Oktem. Solving ill-posed inverse problems using iterative deep neural net-
works. Inverse Problems, 33(12):124007, 2017.
Shengze Cai, Zhicheng Wang, Lu Lu, Tamer A Zaki, and George Em Karniadakis. DeepM&Mnet:
Inferring the electroconvection multiphysics fields based on operator approximation by neural
networks. Journal of Computational Physics, 436:110296, 2021.
Tianping Chen and Hong Chen. Universal approximation to nonlinear operators by neural networks
with arbitrary activation functions and its application to dynamical systems. IEEE Transactions
OnNeuralNetworks,6(4):911-917, 1995. doi: 10.1109/72.392253.
Ronen Eldan and Ohad Shamir. The power of depth for feedforward neural networks. In Conference
on learning theory,pp. 907-940. PMLR, 2016.
Boris Hanin and Mark Sellke. Approximating continuous functions by ReLU nets of minimal width.
arXiv preprint arXiv:1710.11278, 2017.
Patrick Kidger and Terry J. Lyons. Universal approximation with deep narrow networks. CoRR,
abs/1905.08539, 2019. URL http://arxiv.org/abs/1905.08539.
Nikola Kovachki, Zongyi Li, Burigede Liu, Kamyar Azizzadenesheli, Kaushik Bhattacharya, An-
drew Stuart, and Anima Anandkumar. Neural operator: Learning maps between function spaces.
arXiv preprint arXiv:2108.08481, 2021.
Samuel Lanthaler, Siddhartha Mishra, and George Em Karniadakis. Error estimates for DeepOnets:
A deep learning framework in infinite dimensions. arXiv preprint arXiv:2102.09618, 2021.
Lu Lu, Pengzhan Jin, and George Em Karniadakis. DeepONet: Learning nonlinear operators for
identifying differential equations based on the universal approximation theorem of operators.
arXiv preprint arXiv:1910.03193, 2019.
Lu Lu, Pengzhan Jin, Guofei Pang, Zhongqiang Zhang, and George Em Karniadakis. Learning
nonlinear operators via DeepONet based on the universal approximation theorem of operators.
Nature Machine Intelligence, 3(3):218-229, 2021.
Zhou Lu, Hongming Pu, Feicheng Wang, Zhiqiang Hu, and Liwei Wang. The expressive power of
neural networks: A view from the width. In Proceedings of the 31st International Conference on
Neural Information Processing Systems, pp. 6232-6240, 2017.
Allan Pinkus. Approximation Theory of the MLP Model in Neural Networks. Acta Numerica, 8:
143-195, 1999. doi: 10.1017/S0962492900002919.
Fabrice Rossi, Nicolas Delannay, Brieuc Conan-Guez, and Michel Verleysen. Representation of
functional data in neural networks. Neurocomputing, 64:183-210, 2005.
Zuowei Shen, Haizhao Yang, and Shijun Zhang. Neural network approximation: Three hidden
layers are enough. NeuralNetworks, 141:160-173, Sep 2021a. ISSN 0893-6080. doi: 10.1016/j.
neunet.2021.04.011. URL http://dx.doi.org/10.1016/j.neunet.2021.04.011.
Zuowei Shen, Haizhao Yang, and Shijun Zhang. Deep network approximation: Achieving arbitrary
accuracy with a fixed number of neurons, 2021b.
Matus Telgarsky. Benefits of depth in neural networks. CoRR, abs/1602.04485, 2016. URL http:
//arxiv.org/abs/1602.04485.
10