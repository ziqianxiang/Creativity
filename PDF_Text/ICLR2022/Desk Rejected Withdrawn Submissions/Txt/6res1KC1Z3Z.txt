Under review as a conference paper at ICLR 2022
Batch-Softmax Contrastive Loss
for Pairwise Sentence Scoring Tasks
Anonymous authors
Paper under double-blind review
Ab stract
The use of contrastive loss for representation learning has become prominent in
computer vision, and it is now getting attention in Natural Language Processing
(NLP). Here, we explore the idea of using a batch-softmax contrastive loss when
fine-tuning large-scale pre-trained transformer models to learn better task-specific
sentence embeddings for pairwise sentence scoring tasks. We introduce and study
a number of variations in the calculation of the loss as well as in the overall train-
ing procedure; in particular, we find that data shuffling can be quite important.
Our experimental results show sizable improvements on a number of datasets and
pairwise sentence scoring tasks including classification, ranking, and regression.
Finally, we offer detailed analysis and discussion, which should be useful for re-
searchers aiming to explore the utility of contrastive loss in NLP.
1	Introduction
Recent years have seen a revolution in Natural Language Processing (NLP) thanks to the advances
in machine learning. A lot of attention has been paid to architectures, especially for deep learning, as
well as to loss functions. Notably, loss functions based on similar ideas were proposed in unrelated
papers in different machine learning fields under different names. This can cause difficulties when
solving new problems or when designing new experiments based on previous results. To a greater
extent, this applies to “universal” loss functions, which can be applied in different machine learning
areas and tasks such as Computer Vision (CV), Recommendation Systems, and NLP. An example
of such universal loss function is the batch-softmax contrastive (BSC) loss, which we will discuss
below.
For many NLP tasks, it is important to obtain representations of sentences for semantic matching
problems, since they can be used for further analysis, e.g., for finding the best answer to a question.
Sentence BERT is a recent popular approach for this (Reimers & Gurevych, 2019): it can be trained
with different loss functions, and we show that the choice of a loss function is important. Moreover,
we show that it will not be optimal to take the “standard” batch-softmax contrastive loss, which is
used for training SimCSE Gao et al. (2021), a recent alternative to Sentence BERT, and we suggest
ways to improve its efficiency. Our contributions can be summarized as follows:
•	We study the use of a batch-softmax contrastive loss for fine-tuning large-scale transformers
to learn better task-specific sentence embeddings for pairwise sentence scoring tasks.
•	We introduce and study a number of novel variations in the calculation of the loss such as
symmetrization, incorporating labeled negatives, aligning scores on the similarity matrix
diagonal, normalizing over the batch axis, as well as in the overall training procedure,
e.g., shuffling, trainable temperature, and sequential pre-training.
•	We demonstrate sizable improvements for a number of pairwise sentence scoring tasks such
as classification, ranking, and regression.
•	We offer detailed analysis and discussion, which would be useful for future research.
•	We release our code at http://anonymous
1
Under review as a conference paper at ICLR 2022
2	Related Work
The contrastive loss was proposed by Hadsell et al. (2006) as metric learning that contrasts Eu-
clidean distances between embeddings of samples from one class and between samples from dif-
ferent classes. Weinberger et al. (2006) suggested the triplet loss, which is based on a similar idea,
but uses triplets (anchor, positive, negative), and aims for the difference between the distances for
(anchor, positive) and for (anchor, negative) to be larger than a margin. N -pair loss was presented
as a generalization of the contrastive and the triplet losses as a way to solve the problem of extensive
construction of hard negative pairs and triplets (Sohn, 2016).
To this end, a batch of N pairs of examples from N different classes is sampled, and the first element
in each pair is considered to be an anchor. Thus, for each anchor, there are one positive and N ´ 1
negative pairs. The loss contrasts the distances simultaneously using the softmax function over dot-
product similarities. The approach was used successfully in computer vision (CV) tasks. The same
method of Multiple Negative Ranking for training Dot-Product Scoring Models was applied to rank-
ing natural language responses to emails (Henderson et al., 2017), where the loss uses labeled pairs.
A similar idea, called Negative Sharing, was used to reduce the computational cost when training
recommender systems (Chen et al., 2017). Wu et al. (2018) presented an approach with N -pairs like
logic, as a Non-Parametric Softmax Classifier, replacing the weights in the softmax with embeddings
of samples from such classes. It was also proposed to use L2 normalization and temperature. Yang
et al. (2018) proposed to use Multiple Negative Ranking to train general sentence representations
on data from Reddit and SNLI. Logeswaran & Lee (2018) presented a Quick-Thoughts approach to
learn sentence embeddings, which constructs batches of contiguous sets of sentences, and for each
sentence, contrasts the next sentence in the text and all other candidates.
A lot of subsequent work has focused on maximizing Mutual Information (MI). Oord et al. (2018)
presented a loss function based on Noise-Contrastive Estimation, called InfoNCE. It models the
“similarity” function that estimates the MI between the target (future) and the context (present)
signals, and maximizes the MI between temporally nearby signals. If this “similarity” function
expresses the dot-product between embeddings, the InfoNCE loss is equivalent to the N -pair loss
up to some constants. It was also shown that InfoNCE is equivalent to the Mutual Information
Neural Estimator (MINE) up to a constant (Belghazi et al., 2018), whose minimization maximizes a
lower bound on MI. Deep InfoMax (DIM) (Hjelm et al., 2019) improves MINE, and can be modified
to incorporate some autoregression as InfoNCE. However, Tschannen et al. (2020) pointed out that
the effectiveness of loss functions such as DIM and InfoNCE might be primarily connected not to
deep metric learning but rather to MI.
The idea gained a lot of popularity in Computer Vision with the advent of SimCLR (a Simple frame-
work for Contrastive Learning of visual Representations), which introduced NT-Xent (normalized
temperature-scaled cross-entropy loss) (Chen et al., 2020). It uses self-supervised learning, where
augmentations of the same image are considered as positive examples and augmentations of dif-
ferent images are used as negative examples. Thus, the task is as follows: for each example in a
batch, find its paired positive augmentation. Here, the N -pairs loss is modified with a temperature
parameter and with an L2 normalization of embeddings to the unit hypersphere. The loss was further
extended for supervised learning as SupCon loss (Khosla et al., 2020), which aggregates all positive
examples (from the same class) in the softmax numerator.
Subsequently, these losses were introduced to the field of Natural Language Processing (NLP).
Gunel et al. (2020) combined the SupCon loss with the cross-entropy loss and obtained state-of-
the-art results for several downstream NLP tasks using RoBERTa. Giorgi et al. (2020), Fang & Xie
(2020) and Meng et al. (2021) used NT-Xent to pre-train Transformers, considering spans sampled
from the same document, sentences augmented with back-translation as positive examples, and
sequences corrupted with MLM. Luo et al. (2020) proposed to use NT-Xent in a self-supervised
setting to learn noise-invariant sequence representations, where sentences augmented with masking
were considered as positive examples. Finally, Gao et al. (2021) introduced the SimCLR loss to NLP
under the name SimCSE (Simple Contrastive Learning of Sentence Embeddings), where sentences
processed by a neural network with dropout served as augmentations of the original sentences. Here,
we explore various ways to use a similar loss function for pairwise sentence scoring tasks.
While the above-described loss functions have different names, they are all based on similar ideas.
Below, we will use the name Batch-Softmax Contrastive (BSC) loss, which we believe reflects the
2
Under review as a conference paper at ICLR 2022
T
Figure 1: For the set of positives pairs (qi, ai), e.g., question-answer, for each q%, the BSC loss
contrasts the scores between qi and ai (positive examples) vs. between qi and aj for all j ‰ i
(negative examples) using softmax. Here，denotes the dot-product.
main idea best. In our experiments below, we will use the “modern” variant of the loss: with tem-
perature, normalization, and symmetrization components (described in more detail in Section 3.1).
These components were not used for NLP in combination before. We further introduce a number of
novel and important modifications in the definition of the loss and in the training procedure, which
make it more efficient, and we show that using the resulting loss yields better task-specific sentence
embeddings for pairwise sentence scoring tasks.
3	Method
3.1	Batch-Softmax Contrastive (BSC) Loss
Pointwise approaches for training models for pairwise sentence scoring tasks, such as mean squared
error (MSE), are problematic as the loss does not take the relative order into account. For instance,
for two pairs with correct target scores (0.4, 0.5), the loss function would equally penalize answers
like (0.3, 0.6) and (0.5, 0.4). However, the first pair is better, as it keeps the correct ranking, while
the second one does not. This is addressed in pairwise approaches, e.g., in triplet loss, where the
model directly learns an ordering. Yet, there is a problem for constructing pairs or triplets in the
training set, as it is hard to find non-trivial negatives examples.
Unlike traditional pairwise loss functions, the BSC loss treats all other possible pairs of examples
in the batch as “negatives.” That is, only positive pairs are needed for training. Consider a batch
X of pairs from a question-answering dataset. In general, let Qm^n and Am^n be the matrices
of embeddings produced by a query model and an answer model. We define the loss function as
follows:
LBSCpXq “ L0pXq ` L1pXq
“ 一mean (log (diag (SOftmax	)))) — mean (log Qiag (SOftmax	))))
(1)
Here, softmax is applied by rows (Figure 1), and τ is the temperature. Both components can be
rewritten, e.g., L0 pXq can be written as follows:
m	mm	T
LO(X) = ´— ∑ qTai ' — ∑ log ∑ exp (¾j)	⑵
mτ i“1	m i“1	j“1	τ
Mathematically, this loss function is similar to the one presented in (Chen et al., 2020). The dif-
ference is that we do not use augmentations, and we do not compare qi to qj (or ai to aj) due to
their different nature: we want to compare a question to an answer, not a question to a question or
an answer to an answer. Thus, we apply the symmetrization in the formula. So, the difference from
SimCSE (Gao et al., 2021) is that we compare not only qi to all aj, but also ai to all qjin the batch.
3
Under review as a conference paper at ICLR 2022
Note that, although a frequent short answer may fit multiple questions in a batch, such pairs are con-
sidered as “negative” examples in the loss. However, the loss learns Mutual Information (Tschannen
et al., 2020), that is ppqi, aiq{pppqiqppaiqq, and thus it is robust to this false negatives problem.
Early research has already shown the importance of properly configuring and using some BSC loss
settings. For example, low temperatures are equivalent to optimizing for hard positives/negatives
(Khosla et al., 2020), while L2 normalization of vectors to the unit hypersphere along with temper-
ature effectively weighs different examples (Chen et al., 2020). We further propose a number of
important modifications that can have a major impact on the performance for a number of tasks.
3.2	Batch Construction
In computer vision, it is common to use a batch size of 5,000, which in turn would naturally be
very likely to contain some hard negative examples. In NLP, fine-tuning Transformer-based models
with large batch sizes requires very large amounts of memory. Thus, much smaller batches are
used in practice, and as a result, it becomes important to make sure these batches do contain some
hard negative examples. We achieve this by fixing the content of the batches at each epoch of the
training process. Note that this is much simpler than mining hard negatives, as we only need to
increase the likelihood that there would be a hard negative example present in the batch, but we do
not need to know which particular example in the batch would be hard. Inside the batch, this would
be controlled by the temperature parameter.
Example-based shuffling The key idea of this method is to batch several groups, so that within
each group all pairs are similar based on their first or based on their second elements. In this way,
each positive pair would be accompanied by hard negatives from the same group and by simpler
negatives from the remaining examples inside the batch (which come from other groups). We use
the k-nearest neighbors for an input example to form a group for it, and Faiss (Johnson et al., 2019)
to quickly find these nearest neighbors in the embedding space. Let the pairs be grouped by their
first elements qi . Algorithm 1 summarizes the proposed method.
Note that we use two stages in kNN to limit
the range of possible candidates and thus to
reduce the computational costs (both in terms
of time and memory). We first extract the top-
n neighbors (for some large n, e.g., 500), and
then we take the top-k from them, so that no
duplicates appear in the final sequence (for
some small k “ 7). The time complexity of
such a check is O(1). If all such neighbors are
already used, then only the considered exam-
ple will be added to the resulting sequence.
This case will often arise for the last exam-
ples, and thus batches will consist of simple
1-element groups. Therefore, we reverse the
sequence to start with these simple batches, as in curriculum learning.
Algorithm 1 Example-based shuffling
Input: sequence D, group size S
initialize R Drs A sequence to store the result
initialize U D 0 A set of used examples
randomly shuffle D
for e in D do
if e R U then
find the n nearest neighbors of e from D
choose the top s ´ 1 that are not in U
add them and e to R and also to U
return reversed R
By default, we assume that there should be one positive example for each question/answer (on the
diagonal of the matrix), and thus identical neighbors could be optionally filtered. Still, if there are
the same qi in the batch X, the loss definition (eq. 1) does not change. Indeed, let Pq “ ti | qi “
q, pqi , aiq P Xu, then @i, j P Pq : pqi, ajq form a positive pair. According to Khosla et al. (2020),
for each q, all q P Pq should be placed in the softmax numerator and then averaging over all such q
should be performed outside the logarithm. Thus, in L0pXq (eq. 2) only the first sum would change:
qiTai
iPPq
“Σ 击 Σ qT a “ Σ 击 Σ 心 a “ Σ qT a
iPPq | q| jPPq	jPPq | q| iPPq	jPPq
(3)
In Lι(X): EiPPqq ai	…EiPPq	看 EjPPqq ai	“	EiPPq	看 ΣjPPq	q%i	“	EiPPqq ai	(4)
To select the groups even better, we consider task-specific embeddings. To this end, we apply the
current model to encode all pairs at each epoch.
4
Under review as a conference paper at ICLR 2022
Fast shuffling For extremely large datasets,
example-based shuffling is time-consuming
even with Faiss; thus, we propose several
effective options to perform a less-thorough
shuffling. We choose some attribute by which
we will group the examples, that is, we guar-
antee some closeness of the examples. Thus,
the examples are close if they share the same
words, the same cluster number or the same
nearest neighbors. First, consider the case of
words and grouping by the first elements of
the pairs (the case of the second elements is
the same). Algorithm 2 presents the shuffling
process. To produce a shuffle by clusters, we
apply the same algorithm, where each sen-
tence is replaced by its cluster number. Thus,
each shingle has size t “ 1. In order to make
a shuffle by nearest neighbors, we create shin-
gles by “sentences,” where the words are the
positions of the top-k nearest neighbors in the
input sequence (for some small k). All of
these approaches, as well as k-means clus-
tering, can be effectively implemented using
MapReduce and parallel computations.
Algorithm 2 Shuffling by words
Input: sequence D, group size k, shingle size t
for e in D do
e.shingle D random subset of t words
of e (ignoring stop-words)
sort D by e.shingle
initialize glD D random uint64 A group ID
initialize S D 0	A current group size
initialize Prev D first element of D
for e in D do
if e.shingle ‰ prev.shingle then
glD D random uint64
if Sek then
glD D random uint64
S D 0
e.gID D gID
s D s ` 1
Prev D e
sort D by e.gID
return D
3.3	Labeled Negatives
Usually, when the data size is small, hard negative examples may be hard to obtain even with data
shuffling, e.g., when all examples are semantically distant. Nonetheless, if the dataset contains a
labeled negative pair with some anchor, then its elements are semantically close by traditional rules
of dataset construction. Thus, using such a pair inside the batch, where this anchor is present, will
add the necessary hard negative example.
The only change that is added in the loss function is the masking of negative examples—we have no
guarantees that the selected negative example is closer to the anchor than the rest of the examples
inside the batch. Let yi be a binary label, where yi “ 1 if the i-th pair is positive. Then, we have
m	m	mT
LO(X) =-----X Iryi	=	IsqTai '-X Iryi	=	1s	log X exp (	——j	)	(5)
mτ i“1	m i“1	j“1	τ
3.4	Combo Loss
Theoretically, it is beneficial to use several loss functions for training if they are calculated on the
same batch (and thus do not require additional computations). That is, joint training of BSC and
MSE losses combines the advantages of pointwise and of pairwise approaches, thus ensuring that
for positives examples, the values on the diagonal of the dot-product matrix are not only greater
than the rest, but are also close to 1 or to some target similarity. Note that here LMSE (X) =
ml ∑mn((qTai)´ yi)2 for target positive similarities yi, and thus We do not force all other similarities
to zero. At the same time, the BSC loss adds new examples (“negative” pairs) to the training set.
In order to use the BSC loss when training a model in tasks with non-binary labels, we modify the
indicator function in the equation 5, as Iryi > ts, where t is a configurable binarization threshold.
Then, we use their convex combination with the configurable hyperparameter μ p (0,1):
L(X) = μLBSc(X) ' (1 ´ μ)LMSE(X)	(6)
3.5	Normalization
L2 normalization of matrices A and B means that aiT bj will be equivalent to cosine similarity.
The embeddings can also be normalized by the batch dimension (by coordinates), which can bring
5
Under review as a conference paper at ICLR 2022
additional regularization. In our experiments, we confirm the importance of this, e.g., new represen-
tations can be calculated with L2 normalization by coordinates or in a min-max scale.
4	Datasets
NLP tasks that compare pairs of sentences can be divided into regression (predicting a similarity
score), classification (e.g., similar vs. dissimilar), and ranking (search for the best matches). They
differ only by the quality assessment functions, and thus they all can benefit from the above losses.
Note that it is important to calculate sentence representations in ranking tasks, as when indepen-
dently calculating the embeddings of the individual elements in the pairs, the inference time of the
model becomes linear instead of quadratic. Therefore, we use Sentence-BERT (SBERT), which is
trained as a Siamese BERT model, and offers a way to obtain state-of-the-art sentence embeddings,
which have been proven useful for a number of tasks (Reimers & Gurevych, 2019; Thakur et al.,
2020). At inference time, we first use SBERT to obtain independently a representation for each
sentence in the pair, and then we calculate the cosine similarities between these embeddings.
We use the following English datasets and tasks for the evaluation. Four ranking tasks (ranking
answers to non-factoid questions, ranking questions by their similarity with respect to other ques-
tions, ranking comments by their similarity to a given question, ranking fact-checked claims by their
relevance with respect to an input claim), two binary classification tasks (paraphrases identification,
and duplicate question identification), and one regression task (semantic sentence similarity).
Antique The dataset contains 2,626 non-factoid questions with answer choices (Hashemi et al.,
2019), asked by users on Yahoo! Answers. There are a total of 34,011 question-answer pairs:
27,422 for training and 6,589 for validation. Each answer is annotated with a relevance score with
respect to the question on a scale from 1 to 4, and the task is to rank the answers by their relevance.
To model relevance as a cosine similarity, we normalize the scores to the r0, 1s interval. We use
Mean Reciprocal Rank (MRR) as the main evaluation measure.
CQA-A This dataset was used in SemEval-2017 Task 3 on Community Question Answering sub-
task A (Nakov et al., 2017). The goal is to rank the first ten answers in a question thread on Qatar
Living, so that good answers are ranked higher than bad ones. We used the clean part of the dataset,
which consists of 14,110 and 2,440 labeled question-comments pairs for training and development,
respectively. The evaluation measure is Mean Average Precision (MAP). This dataset contains im-
portant metadata, e.g., the date and time of the comment, and sorting the comments by time yields a
strong baseline; yet, we only use the text. To train the model with the triplet loss, we group the pairs
by the first element (anchor).
CQA-B This dataset was developed for SemEval-2017 Task 3, subtask B (Nakov et al., 2017),
whose goal was to rank 10 potentially related questions by their similarity with respect to an input
question. These questions are retrieved from the Qatar Living forum using Google and the input
question as a query. We use the clean part of the dataset, which consists of 19,990 training and
5,500 development labeled question-question pairs. The main evaluation measure here is MAP.
There is additional information, e.g., the rank of the retrieved question in the Google search results,
which we do not use.
PFCC-S Shaar et al. (2020) presented a dataset for detecting Previously Fact-Checked Claims on
Snopes (PFCC-S), aimed at facilitating the solution of a fact-checking problem: given an input
claim, it asks to rank claims that have been previously fact-checked, so that claims that can help
verify the input claim are ranked as high as possible. The dataset has 800 positive input-verified
claim pairs for training and 200 such positive pairs for testing, and they are to be matched against a
database of 10,369 verified claims. The evaluation is performed in terms ofa HasPositive@k metric,
which checks whether there is a positive match among the first k results in the ranked list. In order
to train models using MSE or triplet loss, we sampled negatives according to the following scheme.
First, we encoded all sentences using SBERT, pretrained on STS and NLI. Then, we selected the first
element in each positive pair as an anchor and we sorted all other examples by their similarity to this
anchor. The assumption is that positive examples will be concentrated in the beginning. Thus, we
selected negatives starting from 101 on, logarithmically: on positions 100'2k, k P N. Asa result,
we obtain many hard negative examples and a small number of easy ones. Finally, we oversampled
the positive pairs to correct the balance of positive and negative examples.
6
Under review as a conference paper at ICLR 2022
Microsoft Research Paraphrase Corpus (MRPC) Dolan et al. (2004) contains 5,800 pairs of
sentences, extracted from online news sources. Each pair was labeled with a tag indicating whether
the sentences are paraphrases (semantically equivalent). There are 3,668, 407, and 1,725 pairs in the
training, development, and test subsets. As it is a binary classification task with class imbalance, it
is evaluated in terms of F1.
Quora Question Pairs (QQP) Quora presented a dataset containing over 500,000 sentences with
over 400,000 lines of potential duplicate questions. Each line has a binary label indicating whether
the line truly contains a duplicate pair. Due to the sampling method, which returns mostly positive
pairs, the authors supplemented the dataset with negative pairs composed of “related questions.” As
in (Thakur et al., 2020), we sample randomly 10,000 examples for training, and we use the F1 score
as the main evaluation measure.
Semantic Textual Similarity Benchmark (STSb) The STS benchmark comprises a selection of
the English datasets used in the STS tasks organized in the context of SemEval between 2012 and
2017 (Cer et al., 2017). The benchmark comprises 8,628 sentence pairs. The pairs were annotated
with similarity scores on a scale from 0 to 5 (5 indicating complete equivalence). There are a total of
5,749, 1,500 and 1,379 pairs in the training, in the development, and in the testing split, respectively.
The main metric is Spearman’s rank correlation. As in the Antique dataset, we normalize the scores
to the r0, 1s interval and then we binarized them based on a threshold of 0.6.
5	Experimental Setup
We used BERT-base uncased in all our experiments to be able to perform direct comparison for tasks
such as MRPC, QQP and STS to previous work (Reimers & Gurevych, 2019; Thakur et al., 2020).
We set the number of warm-up steps to 10% of the total steps, and we limited the input sequence
length to 90 subtokens. We used a batch size of 30 in all tasks, except for Antique and QQP, where
we used 50. Note that an order of magnitude larger batch sizes would probably yield better results,
but they would also require much more memory. We experimented with learning rates from {5e-6,
1e-5, 2e-5, 3e-5}, and we selected (on dev) 3e-5 for CQA-B and 2e-5 for all other experiments. We
used the AdamW optimizer with the bias correction for the CQA tasks, and without bias correction
for the rest. We trained the model for five epochs for Antique, CQA-A and STSb, for six epochs for
QQP, MRPC and PFCC-S, and for seven epochs for CQA-B, saving a checkpoint after each one, and
we selected the best checkpoint on dev. As recommended in (Thakur et al., 2020), due to instability,
we did seed optimization, running each approach five times and selecting the best result (on dev).
To train with the BSC loss, we used min-max normalization by coordinates with τ “ 1.2 for PFCC-
S and QQP, standard L2 normalization with τ “ 0.055 for CQA-A, τ “ 0.07 for CQA-B, and
τ “ 0.1 for all other tasks (to find the optimal τ, we made it trainable for one run). We applied
example-based shuffling to train with the BSC loss. We used a group size of four in MRPC, of five
in cQA-B, and of eight in all other tasks. We iterated over μ values from the set {0.1,0.5,0.9},
and We chose μ = 0.1 to train the combo approach for CQA-A, MRPC, QQP and STSb tasks, and
μ “ 0.9 for the other experiments.
We trained the triplet loss variant from (Reimers & Gurevych, 2019) With margin “ 0.6 for PFCC-
S, and margin “ 0.5 for all other tasks. As We have no ansWers for the test set in MRPC, and no
test sets in Antique and PFCC-S, We split the training set into 9:1 to tune the hyper-parameters. The
time for training SBERT With the BSC loss (or combo loss) Was almost equal to the time for training
With the standard MSE loss. We ran all experiments on a GeForce GTX 1080 GPU.
We considered SimCSE (sup-simcse-bert-base-uncased checkpoint) as an unsupervised
baseline as it uses the base version of the BSC loss, Which We modified. BeloW, by BSC We Will
denote using optimal settings in the tables, and variants like BSC - random shuffle Would mean that
instead of these optimal settings, We applied random shuffling.
6	Results
In this section, We compare the BSC loss to other loss functions: MSE and triplet loss. Additionally,
We make an ablation study for the BSC loss modifications We proposed.
7
Under review as a conference paper at ICLR 2022
Antique The results are shown in Table 1.
Our best approach of combo-training MSE
and BSC losses outperforms all other vari-
ants and the approach proposed in (Hashemi
et al., 2019), where specific negative sam-
pling and a triplet loss were used. Besides,
the best BSC configuration achieves higher
scores than MSE. We can see the impor-
tance of using predefined hand-crafted neg-
ative examples, which brings additional diffi-
cult cases and increases MRR by 0.02.
Table 1: Results for Antique.
Approach / Metric	MRR	P@1	nDCG@1
MSE	0.781	0.660	0.769
BSC	0.804	0.680	0.754
BSC - positives	0.784	0.655	0.744
BSC - random shuffle	0.799	0.670	0.754
Combo BSC + MSE	0.822	0.710	0.773
SimCSE (unsup.)	0.681	0.525	0.686
Hashemi et al. (2019)	0.797	0.709	0.713
CQA-A The results for CQA subtask A are shown in Table 2. A comparison with (Nakov et al., 2017) is not very fair, as we did not use the metadata, e.g., the com- ment position, which was crucial for the best systems. Besides, we use SBERT, which is inferior to a fine-tuned BERT. Nevertheless, our best approach of combo training with MSE and BSC losses yielded competitive results. We further compared different shuffling strategies. The data is ordered by questions, and keeping this or- der turns out to be best. That is, the model	Table 2: Results for CQA-A and CQA-B.				
	Approach / Metric	MAP	MRR	MAP	MRR
	MSE BSC BSC - clusters shuffle BSC - random shuffle BSC - w/o shuffle Combo BSC + MSE Triplet loss	0.869 0.801 0.787 0.763 0.816 0.872 0.857	0.911 0.867 0.859 0.828 0.884 0.912 0.917	0.471 0.495 0.493 0.487 0.481 0.496 0.475	0.513 0.534 0.534 0.530 0.532 0.540 0.529
	SimCSE (unsup.) Nakov et al. (2017)	0.684 0.884	0.735 0.928	0.439 0.472	0.478 0.501
					
learns to distinguish positive answers for each question from manually selected negative ones and
from answers to other questions. Also, note that random shuffling completely eliminates this struc-
ture, and MAP drops by 6% absolute. Fast shuffling by 300 clusters, an advanced version of shuffling
by words, improves these results. Example-based shuffling finds a data order similar to the initial
one, and the quality does not degrade much.
CQA-B The results for CQA-B are shown in Table 2. Again, we did not use the question position,
which is a critically important feature for the best systems. We can see that the BSC loss achieved
the best score, noticeably outperforming MSE and triplet losses. The experiments also demonstrate
the importance of data order when training with the BSC loss. Since the dataset is small, the model
overfits when the original data order is fixed.
PFCC-S Table 3 shows the results for PFCC-
S (HP@k stands for HasPositives@k). Note
that the scores from (Shaar et al., 2020) are
for pre-trained SBERT without task-specific
fine-tuning. We observed that even when
using oversampling to improve the balance
of positive examples, MSE performed worse
than their results. Here, we used only pos-
itives examples to train with BSC, and nor-
malizing by the zero dimension was the
best. Overall, the approaches using BSC and
triplet losses were comparable. However, the
Table 3: ReSUltS for PFCC-S.
Approach / Metric	HP@1	HP@5	HP@50
MSE	0.362	0.508	0.709
BSC	0.673	0.844	0.899
BSC - 1-dim norm	0.588	0.764	0.899
BSC - no norm	0.608	0.744	0.884
BSC - random shuffle	0.663	0.794	0.915
Triplet loss	0.668	0.794	0.899
SimCSE (unsup.)	0.412	0.693	0.849
Shaar et al. (2020)	0.402	0.653	0.784
dataset size for training with the BSC loss was much smaller, which is also true for MSE. As a result,
the BSC loss is faster, and preferable for this task.
MRPC Table 4 shows the results for MRPC.
MSE outperformed the BSC loss, but combo
achieved a slightly higher F1 score.
QQP The results for QQP are presented
in Table 4. We also show results for
SBERT and augmented SBERT (in paren-
theses) from (Thakur et al., 2020). There
score was obtained by training SBERT with
Table 4: Results for MRPC and QQP.
Approach / Metric	MRPC (F1)	QQP (F1)
MSE	89.08	74.29
BSC	86.73	73.13
Combo BSC + MSE	89.46	75.07
SimCSE (unsup.)	85.43	68.65
Thakur et al. (2020)	87.89 (88.55)	74.97 (79.77)
8
Under review as a conference paper at ICLR 2022
MSE using another random training sample, but nonetheless, the F1 score is close to ours. The
combo approach outperformed separate training with BSC or MSE.
STSb Table 5 shows the results for STS. It
is the only task where combo with the BSC
loss was worse than MSE. This could be due
to hard negatives not appearing in the batch
in any of the shuffling procedures. Moreover,
we observed only marginal improvement when
fine-tuning with a BSC model initially trained
with MSE. However, if it was pretrained with
BSC up to overfitting, fine-tuning it with MSE
yielded sizable improvements.
Table 5: Results for STSb: Spearman rank corre-
lation._____________________________
Approach / Metric	P X 100
MSE	84.80
BSC	83.26
Combo BSC + MSE	84.59
Fine-tuning MSE with BSC	84.95
Fine-tuning BSC with MSE	85.71
SimCSE (unsup.)	84.25
Reimers & Gurevych (2019)	84.86
7	Discussion
We highlight the following observations:
•	Combo-training with BSC and MSE losses generally yields the best results (the only ex-
ception is STS), and it outperforms the triplet loss with advanced negative sampling.
•	The order in which the data is presented for training can be critical, as we have seen in the
cases of CQA-A and CQA-B.
•	The use of labeled negatives examples generally improves the scores by 1-2% absolute.
•	Embedding normalization during training is important. Moreover, it is useful to normalize
to the zero dimension (e.g., for PFCC-S).
•	Temperature τ of order 0.1 should be used with the standard normalization, and τ of order
1-3 for coordinate normalization.
•	An incorrect training setup may hurt the performance by more than 10%, as was demon-
strated for (i) filtering out negative examples for which no positives were given in the
dataset (Table 1), (ii) using poorly formed batches (highest effect in Table 2), (iii) subopti-
mal normalization (Table 3), and (iv) wrong temperature value.
•	The BSC loss is more suitable for ranking tasks, but it can help for other tasks if applied as
pre-training or in joint training with the MSE loss.
Selecting a loss function is important. For instance, if the model optimizes Pearson correlation, it
achieves a score of 85.57 on the STS task. Thus, it outperforms almost all considered approaches.
Moreover, the combination of such a loss with BSC allows the model to achieve anF1 score of 89.88
in the MRPC task (a classification task).
Finally, we would like to draw a parallel between our work and Augmented SBERT (Thakur et al.,
2020). When using the BSC loss, some negatives are implicitly added to the dataset. Augmented
SBERT adds new examples too and retrieves them using BM25 or Semantic Search samplings.
These methods are comparable to our fast shuffling by words (n-grams) and to example-based shuf-
fling, respectively. Moreover, the task-specific model is used to encode the data in both cases.
However, we do not need to label such pairs with another model (cross-encoder) due to the BSC
loss definition.
8	Conclusion and Future Work
We explored the idea of using a batch-softmax contrastive loss for fine-tuning large-scale pre-trained
transformers to learn better task-specific sentence embeddings for pairwise sentence scoring tasks.
We introduced and studied a number of variations in the calculation of the loss as well as in the
overall training procedure. Our experimental results have shown sizable improvements on a number
of datasets and pairwise sentence scoring tasks including ranking, classification, and regression.
In future work, we want to explore new variations of the loss, and to gain better understanding of
when to use which variation. We further plan experiments with a larger set of NLP tasks.
9
Under review as a conference paper at ICLR 2022
References
Mohamed Ishmael Belghazi, Aristide Baratin, Sai Rajeshwar, Sherjil Ozair, Yoshua Bengio, Aaron
Courville, and Devon Hjelm. Mutual information neural estimation. In Jennifer Dy and An-
dreas Krause (eds.), Proceedings of the 35th International Conference on Machine Learning, vol-
Ume 80 of Proceedings of Machine Learning Research, pp. 531-540, Stockholmsmassan, Stock-
holm Sweden, 2018. PMLR.
Daniel Cer, Mona Diab, Eneko Agirre, Inigo Lopez-Gazpio, and Lucia Specia. Semeval-2017 task
1: Semantic textual similarity multilingual and crosslingual focused evaluation. Proceedings of
the 11th International Workshop on Semantic Evaluation (SemEval-2017), 2017.
Ting Chen, Yizhou Sun, Yue Shi, and Liangjie Hong. On sampling strategies for neural network-
based collaborative filtering. In Proceedings ofthe 23rd ACM SIGKDD International Conference
on Knowledge Discovery and Data Mining, KDD ’17, pp. 767-776, New York, NY, USA, 2017.
Association for Computing Machinery. ISBN 9781450348874.
Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. A simple framework for
Contrastive learning of visual representations. In Hal Daume In and Aarti Singh (eds.), Proceed-
ings of the 37th International Conference on Machine Learning, volume 119 of Proceedings of
Machine Learning Research, pp. 1597-1607. PMLR, 2020.
Bill Dolan, Chris Quirk, and Chris Brockett. Unsupervised construction of large paraphrase corpora:
Exploiting massively parallel news sources. In Proceedings of the 20th International Conference
on Computational Linguistics, COLING ’04, pp. 350-es, USA, 2004. Association for Computa-
tional Linguistics.
Hongchao Fang and Pengtao Xie. Cert: Contrastive self-supervised learning for language under-
standing. CoRR, abs/2005.12766, 2020.
Tianyu Gao, Xingcheng Yao, and Danqi Chen. SimCSE: Simple contrastive learning of sentence
embeddings. arXiv preprint arXiv:2104.08821, 2021.
John M Giorgi, Osvald Nitski, Gary D. Bader, and Bo Wang. Declutr: Deep contrastive learning for
unsupervised textual representations. ArXiv, abs/2006.03659, 2020.
Beliz Gunel, Jingfei Du, Alexis Conneau, and Ves Stoyanov. Supervised contrastive learning for
pre-trained language model fine-tuning. ArXiv, abs/2011.01403, 2020.
R. Hadsell, S. Chopra, and Y. LeCun. Dimensionality reduction by learning an invariant map-
ping. In 2006 IEEE Computer Society Conference on Computer Vision and Pattern Recognition
(CVPR’06), volume 2, pp. 1735-1742, 2006.
Helia Hashemi, Mohammad Aliannejadi, Hamed Zamani, and W. Bruce Croft. ANTIQUE: A non-
factoid question answering benchmark. CoRR, abs/1905.08957, 2019.
Matthew Henderson, Rami Al-Rfou, B. Strope, Yun-Hsuan Sung, L. Lukacs, R. Guo, S. Kumar,
B. Miklos, and R. Kurzweil. Efficient natural language response suggestion for smart reply.
ArXiv, abs/1705.00652, 2017.
Devon Hjelm, Alex Fedorov, Samuel Lavoie-Marchildon, Karan Grewal, Philip Bachman, Adam
Trischler, and Yoshua Bengio. Learning deep representations by mutual information estimation
and maximization. In ICLR 2019. ICLR, 2019.
J. Johnson, M. Douze, and H. Jegou. Billion-scale similarity search with gpus. IEEE Transactions
on Big Data, pp. 1-1, 2019.
Prannay Khosla, Piotr Teterwak, Chen Wang, Aaron Sarna, Yonglong Tian, Phillip Isola, Aaron
Maschinot, Ce Liu, and Dilip Krishnan. Supervised contrastive learning, 2020.
Lajanugen Logeswaran and Honglak Lee. An efficient framework for learning sentence representa-
tions. In International Conference on Learning Representations, 2018.
10
Under review as a conference paper at ICLR 2022
Fuli Luo, Pengcheng Yang, S. Li, Xuancheng Ren, and X. Sun. Capt: Contrastive pre-training for
learning denoised sequence representations. ArXiv, abs/2010.06351, 2020.
Yu Meng, Chenyan Xiong, Payal Bajaj, saurabh tiwary, Paul N. Bennett, Jiawei Han, and Xia Song.
COCO-LM: Correcting and contrasting text sequences for language model pretraining. In Thirty-
Fifth Conference on Neural Information Processing Systems, 2021.
Preslav Nakov, Doris Hoogeveen, Lluls Marquez, Alessandro Moschitti, Hamdy Mubarak, Timothy
Baldwin, and Karin Verspoor. SemEval-2017 task 3: Community question answering. In Pro-
Ceedings ofthe 11th International Workshop on Semantic Evaluation (SemEval-2017), pp. 27-48,
Vancouver, Canada, 2017. Association for Computational Linguistics.
A. Oord, Y. Li, and Oriol Vinyals. Representation learning with contrastive predictive coding. ArXiv,
abs/1807.03748, 2018.
Nils Reimers and Iryna Gurevych. Sentence-BERT: Sentence embeddings using Siamese BERT-
networks. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language
Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-
IJCNLP), pp. 3982-3992, Hong Kong, China, 2019. Association for Computational Linguistics.
Shaden Shaar, Nikolay Babulkov, Giovanni Da San Martino, and Preslav Nakov. That is a known
lie: Detecting previously fact-checked claims. In Proceedings of the 58th Annual Meeting of
the Association for Computational Linguistics, pp. 3607-3618, Online, 2020. Association for
Computational Linguistics.
Kihyuk Sohn. Improved deep metric learning with multi-class n-pair loss objective. In D. Lee,
M. Sugiyama, U. Luxburg, I. Guyon, and R. Garnett (eds.), Advances in Neural Information
Processing Systems, volume 29, pp. 1857-1865. Curran Associates, Inc., 2016.
Nandan Thakur, Nils Reimers, Johannes Daxenberger, and Iryna Gurevych. Augmented sbert: Data
augmentation method for improving bi-encoders for pairwise sentence scoring tasks, 2020.
Michael Tschannen, Josip Djolonga, Paul K. Rubenstein, Sylvain Gelly, and Mario Lucic. On
mutual information maximization for representation learning. In 8th International Conference
on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020. OpenRe-
view.net, 2020.
Kilian Q Weinberger, John Blitzer, and Lawrence Saul. Distance metric learning for large margin
nearest neighbor classification. In Y. Weiss, B. Scholkopf, and J. Platt (eds.), Advances in Neural
Information Processing Systems, volume 18, pp. 1473-1480. MIT Press, 2006.
Zhirong Wu, Yuanjun Xiong, S. Yu, and D. Lin. Unsupervised feature learning via non-parametric
instance discrimination. 2018 IEEE/CVF Conference on Computer Vision and Pattern Recogni-
tion, pp. 3733-3742, 2018.
Yinfei Yang, Steve Yuan, Daniel Cer, Sheng-yi Kong, Noah Constant, Petr Pilar, Heming Ge, Yun-
Hsuan Sung, Brian Strope, and Ray Kurzweil. Learning semantic textual similarity from conver-
sations. In Proceedings of The Third Workshop on Representation Learning for NLP, pp. 164-174,
Melbourne, Australia, 2018. Association for Computational Linguistics.
11