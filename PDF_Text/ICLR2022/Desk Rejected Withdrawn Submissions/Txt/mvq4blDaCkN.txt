Under review as a conference paper at ICLR 2022
Efficient Semi-Supervised Adversarial Train-
ing without Guessing Labels
Anonymous authors
Paper under double-blind review
Ab stract
Adversarial training has been proved to be the most effective defensive strategy
to protect models from adversarial attacks. In the practical application scenario of
adversarial training, we face not only labeled data, but also an enormous amount of
unlabeled data. However, existing adversarial training methods are naturally tar-
geting supervised learning problems. To adapt to semi-supervised learning prob-
lems, they need to estimate labels for unlabeled data in advance, which inevitably
degenerates the performance of the learned model due to the bias on the estima-
tion of labels for unlabeled data. To mitigate this degeneration, in this paper, we
propose a new semi-supervised adversarial training framework via maximizing
AUCs which is also a minimax problem but treats the unlabeled samples as both
positive and negative ones, so that we do not need to guess the labels for unla-
beled data. Unsurprisingly, the minimax problem can be solved via the traditional
adversarial training algorithm by extending singly stochastic gradients to triply
stochastic gradients, to adapt to the three (i.e. positive, negative, and unlabeled)
data sources. To further accelerate the training procedure, we transform the min-
imax adversarial training problem into an equivalent minimization one based on
the kernel perspective. For the minimization problem, we discuss scalable and
efficient algorithms not only for deep neural networks but also for kernel support
vector machines. Extensive experimental results show that our algorithms not only
achieve better generalization performance against various adversarial attacks, but
also enjoy efficiency and scalability when considered from the kernel perspective.
1 Introduction
Machine learning models have long been confirmed to be vulnerable to adversarial examples which
are specially crafted data that can easily subvert the predictions of the models (Goodfellow et al.,
2014; Carlini & Wagner, 2017; Biggio et al., 2012). Then many studies have been published with
the aim of finding countermeasures to protect these learning models (Papernot et al., 2016; Shafahi
et al., 2019). Among existing defensive strategies, adversarial training (Madry et al., 2017) is proved
to be the most effective one (Athalye et al., 2018). Generally, it can be defined as a minimax problem
(Wang et al., 2020), where the inner maximization simulates the behavior of attackers to construct
the most aggressive adversarial examples, and the outer minimization is a typical process to train
the model to minimize the internal loss. In 2018, Schmidt et al. proposed that the improvement of
adversarial robustness requires more data than common training. However, following this suggestion
can be difficult due to the cost of gathering additional data and obtaining high-quality labels. Thus,
in order to avoid the heavy cost of collecting labeled data, we need to handle not only labeled data
but also a large number of unlabeled data in practical application scenarios.
However, adversarial training is naturally designed for supervised learning scenarios, which means
that it is only applicable to labeled samples of the form (x, y). This is because generating the
perturbation δ for an adversarial example relies on the divergence between the true label y and the
predicted value f(x) as shown in the upper row of Fig. 1. If the true label y is missing, i.e., the
target for adversarial attack is absent, the loss between the true label y and the predicted value f (x)
is undefined which is explicitly shown in the lower row of Fig. 1. In order to solve this problem,
there exist several works (Carmon et al., 2019; Miyato et al., 2018; Uesato et al., 2019; Zhai et al.,
2019) devoted to semi-supervised adversarial training. These works estimate labels for unlabeled
data in advance, which inevitably degenerates the performance of the learned model due to the
biased estimation of labels for unlabeled data.
1
Under review as a conference paper at ICLR 2022
Figure 1: The challenge of generating perturbations for unlabeled data in the l∞ norm.
Besides, the performance of adversarial training algorithms is mostly measured by accuracy. For
a highly imbalanced dataset with two classes, a classifier that predicts all samples to belong to the
dominant class will achieve high prediction accuracy, but have poor generalization performance
since it will misclassify all samples in another class. Thus, accuracy is not a good metric for the
imbalanced classification scenario. Instead, the area under the ROC curve (AUC) (Hanley & Mc-
Neil, 1982), which measures the probability of a randomly drawn positive sample to have a higher
decision value than a randomly drawn negative sample, is a more meaningful metric for highly-
imbalanced datasets.
To alleviate these problems, in this paper, we propose a new semi-supervised adversarial training
framework via maximizing AUCs (S2AT-AUC) which can be formulated as a minimax problem.
Specifically, this framework treats the unlabeled samples as both positive and negative ones, so
that we do not need to guess the labels for unlabeled data, which leads to an unbiased estimation.
Undoubtedly, the minimax problem can be solved via a strategy similar to the standard adversarial
training algorithm, i.e., optimizing the model with a projected gradient descent (PGD) adversary
(Madry et al., 2017). The difference is that we extend singly stochastic gradients to triply stochastic
gradients to adapt to the three data sources (i.e., positive, negative and unlabeled data). However, the
K-step PGD attack for generating adversarial examples is known to cost much. To further accelerate
the training procedure, we transform the minimax adversarial training problem into an equivalent
minimization one based on the kernel perspective via the connection between perturbations in the
linear and kernel spaces. For the minimization problem, we discuss it not only on deep neural
networks (DNNs) but also on kernel support vector machines (SVMs). Our main contributions are
summarized as follows:
•	We propose an ingenious strategy of adversarial training for semi-supervised learning, where we
do not need to calculate a pseudo-label for the unlabeled samples, and we just treat them as both
positive and negative ones.
•	We propose an efficient semi-supervised adversarial training framework for nonlinear AUC max-
imization which can be applied to both DNNs and kernel SVMs. With extensive experimental
results, we show its superiority.
2 Semi-Supervised Adversarial Training via AUC Maximization:
S2AT-AUC
In this section, we first give a brief review of existing methods to generate adversarial examples for
unlabeled data, then introduce our strategy on this issue for AUC optimization. Based on that, we
propose our minimax semi-supervised adversarial training function for maximizing AUCs.
2.1	Challenge of Generating Adversarial Examples for Unlabeled Data
In this part, we extend the concept of adversarial examples from labeled samples to unlabeled ones
and discuss the challenge of generating such adversarial examples. For a labeled sample (x, y), the
generation of its adversarial example is normally formulated as the following maximization problem:
max	l(y, f (x0)),	(1)
kx0-xk2≤
where x0 = x + δ is the adversarial example of x, δ is the perturbation and is the maximum
perturbation radius. We focus on the l2 norm constraint in this paper. As mentioned in Fig. 1,
2
Under review as a conference paper at ICLR 2022
the formulation (1) cannot be directly used for generating the adversarial example for an unlabeled
sample due to the absence of label y .
To solve this problem, existing countermeasures can be roughly divided into two classes. One is
using the predicted value f (x) as its label, the other is generating a pseudo-label y as its label.
For the former one, Miyato et al. (2018) proposed the VAT algorithm which evaluates the loss via
computing the divergence between f(x) and f(x0). However, in this case, the perturbation δ is
unable to be computed normally as a ∙ g/ ∣∣g∣∣2, where g = Vχ0l(f (x), f (χ0)), a is the step size
of the PGD attack, since the loss l takes the minimal value at δ = 0, so its first derivative at that
point is 0 as well. To address this issue, they use the second-order Taylor expansion to approximate
the value of δ. For the latter one, Carmon et al. (2019) and Uesato et al. (2019) use a well-known
approach of semi-supervised learning called self-training (Rosenberg et al., 2005). This approach
firstly trains an intermediate model with labeled samples, then uses it to generate pseudo-labels y
for unlabeled samples.
However, since these methods need to guess labels, this will inevitably lead to the performance
degradation of the learned models due to the biased estimation of labels for unlabeled data. To
alleviate this problem, we introduce an unbiased estimation method in the following part.
2.2	Semi-Supervised AUC Optimization without Guessing Labels for
Unlabeled Data
Let D = Dp ∪ Dn denote the labeled dataset, where Dp = {xf}"〜PP(x), Dn = {xn}% 〜
pN (x), xip, xjn ∈ Rd, pP (x) = p(x|y = +1) and pN (x) = p(x|y = -1) are the distributions for
positive and negative samples respectively1. In the semi-supervised setting, the unlabeled dataset
Du is considered to be drawn from a mixture of the positive and negative distributions, i.e., Du =
{xU}nUi 〜P(X) = θpPP(x) + ΘnPN(x), where θp, Θn are the prior probabilities of the positive
and negative classes with θP + θN = 1.
Recently, Zheng & Ming (2018) proved that it is unnecessary to estimate class prior probabilities θP
and θN to achieve unbiased semi-supervised AUC optimization. Instead, PU AUC risk RPU and NU
AUC risk RN U can be equivalent to the supervised PN AUC risk RPN with a linear transformation,
i.e., RPU + RNU 一 2 = RPN, where PU AUC risk RPU is estimated by the positive data and the
unlabeled data regarded as negative data, and NU AUC risk RN U is estimated by the negative data
and the unlabeled data regarded as positive data. Specifically, RPN, RPU and RNU are defined as
follows.
RPN =Exp∈Dp(x) [Exn∈Dn(x) [l(xp, xn)]],	(2)
RPU =Exp∈Dp(x) [Exu∈Du(x) [l(xp, xu)]],	(3)
RNU =Exu∈Du(x)[Exn∈Dn(x)[l(xu, xn)]].	(4)
Importantly, the linear transformation RPU + RNU - 11 = RPN means that unbiased AUC risk
estimation can be achieved without knowing θP and θN .
When a classifier is trained in practice, We use the empirical risks R instead of the expected risks R,
i.e., RPN = N N PNpI PNnI l(xf, xn). In this way, the semi-supervised AUC optimization can
be formulated as
1
RPNU =βRpN + (1 — β) (RPU + RNU — g) ,	(5)
where β ∈ [0, 1] is the trade-off parameter. It should be noted that the loss function we use in this
paper is the convex pairwise hinge loss l = max(0, 1 一 f(xip) + f (xjn)). Other convex surrogate
loss functions presented in (Zhiyuan et al., 2020) are applicable as well.
2.3	FRAMEWORK OF S2AT-AUC
Based on Eq. (5), we propose our framework of semi-supervised adversarial training for AUC
optimization in the following. The inner maximization problem actually follows the principle
1Since binary classification is the basis of multi-class classification, in this paper, we discuss the binary
classification problem. It should be noted that our algorithms can be easily extended to multi-class classification
via one vs. one (OVO) or one vs. all (OVA) strategies (Duan et al., 2007).
3
Under review as a conference paper at ICLR 2022
of adversarial attack and aims to maximize the loss to construct the most aggressive adversarial
examples. The goal of the outer minimization problem is to find parameters f by minimizing
L(f; Dp, Dn, Du), the average internal pairwise loss caused by adversarial examples, defined as
follows in detail.
1	Np Nn
L(f; Dp,Dn,Du) =fβ-- XX max [1 - f (xip) + f(xjn)] +
NpNn i=1 j=1 x0ip,x0jn
fl	Np Nu
NN XX m aχu[* 1 11-f (Xip) +f (Xju)]+	⑹
p u i=1 j=1 xi ,xj
1	Nu Nn	1
+ NuNn XX XmaXn[1-f (Xiu)+f (xjn)]+-4
i=1 j=1 i j
s.t. ∀Xip,Xjn,Xku	∈ Dp×Dn	× Du :	X0ip	-	Xip2 ≤ ;	X0jn	-	Xjn2 ≤ ;	X0ku	-	Xku2 ≤ .
where [π]+ denotes max(0, π) which corresponds to the hinge loss.
Different from strategies to generate adversarial examples for unlabeled data introduced in Section
2.1, we simply regard the unlabeled sample as positive and negative ones at the same time. Thus, it
is unnecessary to estimate the labels of unlabeled samples, which achieves unbiased AUC risk esti-
mation. In this case, we have two adversarial examples for one unlabeled sample which correspond
to positive and negative ones respectively (The intuitive explanation can be found in Fig. 2b).
2.4 S2AT-AUC on Minimax Problem
The objective function of S2AT-AUC on the minimax problem is shown in Eq. (6). We define
l1 = [1-f(x0ip)+f(x0jn)]+,l2 = [1-f(x0ip)+f(x0ju)]+andl3 = [1-f(x0iu)+f(x0jn)]+. The
detailed algorithm is summarized in Algorithm 1. ΠB in the algorithm is the projection operator
on the perturbation set B and B(x, ) = {x + δ s.t.kδk2 ≤ }. The inner loop generates the most
aggressive data that maximizes the loss via the PGD attack (Madry et al., 2017) (line 6-13), which
perturbs natural data in the given perturbation boundary with a small step size α. The difference is
that we extend the original singly stochastic gradients to triply stochastic gradients in order to adapt
to the three data sources (xp, xn , xu). The outer loop updates the model using optimizers such as
stochastic gradient descent (SGD) (Bottou, 2010) and ADAM (Kingma & Ba, 2014).
Algorithm 1 S2AT-AUC on Minimax Problem
Input: Dp , Dn , Du : training sets, T : number of epochs, : maximum perturbation radius, γ: learning rate,
K: PGD steps, α: PGD step size which is defaUltly set as 2.『.
Output: f.
1: for epoch= 1, •…,T do
2:	Sample Xip from Dp .
3:	Sample Xjn from Dn .
4:	Sample Xtu from Du .
5:	X0ip	=	Xip,	X0jn	=	Xjn ,	X0iu	=	X0ju	=	Xtu .	// Ready for PGD attack
6:	for k = 1,…，K do
7:	g1= vʃoplι(f,χip,xjn); g2 = Vχ0nll(f,xip,xjn).
ij
8:	g3 =Vx0pl2(f,X0ip,X0ju);g4 =Vx0ul2(f,X0ip,X0ju).
ij
9:	g5 =Vx0ul3(f,X0iu,X0jn);g6 = Vx0nl3(f,X0iu,X0jn).
ij
10:	Xip= πB(xpQ(Xip + α ∙ g1/ kg1k2); xjn = πB(xnQ(Xjn + α ∙ g2/ kg2k2).
11:	Xip= πB(xpQ (Xip + α ∙ g3/ kg3k2); Xju =πB(xuQ(Xju + α ∙ g4/ kg4k2 )∙
12:	Xiu = ∏B(χu,e)(Xiu + α ∙ g5/ I∣g5k2); Xjn = ∏B(xna(Xjn + α ∙ gs/ kg6∣∣2).
13:	end for
14:	f = f - γVfL(f; Xip, Xjn , Xtu). // It can be replaced by other updating rule like ADAM.
15: end for
4
Under review as a conference paper at ICLR 2022
3 S2AT-AUC on the Kernel Perspective
It is generally known that standard adversarial training with minimax formulation runs slowly due
to the high cost of generating strong adversarial examples via K-step PGD attack, which makes it
impractical on large-scale problems. Thus in this section, we propose a new semi-supervised adver-
sarial strategy for nonlinear AUC optimization from the kernel perspective2 which can transform the
original minimax problem into a minimization one. In the following, we first build some primary
results for the adversarial training from the kernel perspective, then discuss the detailed algorithms
on DNNs and kernel SVMs respectively.
3.1 Primary Results from the Kernel Perspective
Our kernelized semi-supervised AUC adversarial training is formulated as a minimax optimization
problem as follows. Here a regularization term is added into Eq. (6) to reduce the risk of over-fitting:
mn 1 kfkH + L(f; Dp, Dn, Du)	⑺
f∈H 2
where f is the model function in the RKHS H and |卜||发 stands for the norm in H. Note that f (∙)
can be written as hf, φ(∙))H and φ(∙) is the feature mapping. As shown in Figs. 2a and 2b, δ are
the perturbations added to the data samples in the linear space, then we can see that a more com-
plicated decision boundary is needed to separate them. Moreover, when the adversarial examples
x + δ are mapped into the kernel space, φ(x + δ) will become unpredictable like Fig. 2c, which
significantly increases the difficulty of data processing and computation. Fortunately, Theorem 1
builds the relationship between perturbations in the linear and the kernel spaces.
R2
Inputx
(a)
x + δ	φ(x + δ)	φ(x) + δφ
(b)	(C)	(d)
Figure 2: Conceptual illustration of perturbations in the linear and kernel spaces. (Here solid circles
denote positive samples, solid crosses denote negative samples, hollow squares denote unlabeled
samples, the red circle and cross are adversarial examples of positive and negative samples respec-
tively. Note that one unlabeled sample has two adversarial examples.)
Theorem 1. (Xu et al., 2009) Suppose the kernel function has the form k(x, x0) = f(kx - x0k),
with f : R+ → R, a decreasing function. Denote by H the RKHS space of k(∙, ∙) and φ(∙) the
corresponding feature mapping. Then we have for any x ∈ Rn, w ∈ H and >0,
sup hw, φ(x + δ))H ≤	sup_________hw, φ(x) + δφiH.	(8)
kδk2≤e	kδφkH≤√2f(0)-2f(e)
Since the perturbation range of φ(x) + δφ tightly covers that of φ(x + δ), which is also intuitively
shown in Fig. 2d, we apply φ(x) + δφ to deal with the following computation, thus the perturbations
can be more tractable in the kernel space. Then the objective function (7) can be rewritten as
1	1	Np Nn
min 2 kf kH + βNN XX. ,max “J1 -hf, φ(xP)iH + hf, φ(xn)iH] +
f∈H 2	NpNn i=1 j=1 Φ(xip ),Φ(xjn)
2The kernel perspective means that our function f is in the reproducing kernel Hilbert space (RKHS) (Iii,
2004)
5
Under review as a conference paper at ICLR 2022
i]	Np Nu
NN X X Φ(χma⅜)[1 -hf, φ(Xp))H+hf, φ(Xu)iH]+
(9)
s.t.
where Φ(xip)
1	Nu Nn	1
+ 忌厂 XX 币,max “Ji -hf, φ(χu U))H + hf, φ(χn)iH]+ -1
NuNn i=1 j=1 Φ(xiu),Φ(xjn)	2
∀ xip , xjn , xku ∈ Dp × Dn × Du :
kΦ(xip)-φ(xip)k2 ≤0,	Φ(xjn)-φ(xjn)2 ≤0, kΦ(xku)-φ(xku)k2 ≤0,
=	φ(xip)	+	δφpi,	Φ(xjn)	=	φ(xjn)	+	δφnj,	Φ(xku)	=	φ(xku)	+	δφuk	and 0 is
√2f (0) - 2f (e).
Since Eq. (9) is still a minimax problem that is hard to be solved after the step above, we propose
the simplified and equivalent form of the inner maximization via the following theorem.
Theorem 2. If f is a function in an RKHS H, the inner maximization problem maxΦ(xp),Φ(xn) [1 -
hf, Φ(xip)iH + hf, Φ(xjn)iH]+ is equivalent to the regularized loss function [1 - f(xip) + f(xjn) +
20 kfkH]+.
The detailed proof of Theorem 2 is provided in our appendix. Following this theorem, the transfor-
mation of the other two terms in Eq. (9) can also be easily obtained. Thus, the minimax objective
function (9) can be written as the following minimization problem:
1	1	Np Nn
mn 2 kf kH + βNN X X[1 - f (xp) + f (xn) + 2e0 kf kH]+
f∈H	p n
i=1 j=1
i]	NP Nu
Ν⅛ X X[i - f (χp) + f (χu) + 2^0 kf kH]+
(10)
1	Nu Nn
+ NN X X[1 - f (xU) + f (xn) + 2e0 kf kH]+ ∙
Nu Nn
i=1 j=1
Since the minimax formulation is transformed into a single-layer minimization one, the K -step
PGD attack for generating adversarial examples can be skillfully escaped. Thus this minimization
problem can be easily extended to large-scale datasets.
3.2 Specific Algorithms
Based on the minimization formulation of semi-supervised adversarial training for nonlinear AUC
optimization (10), our S2AT-AUC can be applied to both DNNs and kernel SVMs as follows.
3.2.1	S2AT-AUC FOR DNNS
Since the RKHS norm kf kH cannot be computed on DNNs, here we use the lower approximation
of kfkH, which was proposed by Bietti et al. (2019):
kfkH ≥ kf kδ2 := sup f(x + δ) - f (x).	(11)
kδk2≤1
In this case, the minimization problem (10) can be solved directly by gradient descent optimization
algorithms such as SGD and ADAM.
3.2.2	S2AT-AUC FOR KERNEL SVMS
In this part, we efficiently solve the minimization problem (10) by applying the kernel-based semi-
supervised AUC learning algorithm with quadruply stochastic gradients (QSG-S2AUC) (Shi et al.,
2019), which is a powerful technique for scalable nonlinear AUC learning.
Specifically, QSG-S2AUC first uses the random Fourier feature method (RFF) (Rahimi & Recht,
2008) to approximate the kernel function instead of computing it directly, then uses the quadruply
stochastic gradients w.r.t. the pairwise loss and random features to iteratively update the objective
function. The detailed optimization procedure is provided in our appendix.
6
Under review as a conference paper at ICLR 2022
4 Experiments
In this section, we evaluate the performance of S2AT-AUC on DNNs and kernel SVMs in the semi-
supervised learning scenario.
4.1	Experimental Setup
Compared Algorithms. We compare the AUC performance of S2AT-AUC with the state-of-the-
art semi-supervised adversarial training algorithms on DNNs and semi-supervised AUC maximiza-
tion algorithms on SVMs as follows:
•	VAT: (Miyato et al., 2018) A semi-supervised adversarial training on DNNs which treats the
predicted value f (x) as its label y and optimizes the accuracy metric.
•	UAT: (Uesato et al., 2019) A semi-supervised adversarial training on DNNs which generates the
pseudo labels via self-training and optimizes the accuracy metric.
•	S2AT-AUC(K): Our semi-supervised adversarial training algorithm for nonlinear AUC maxi-
mization on DNNs from the kernel perspective.
•	S2AT-AUC(M): Our semi-supervised adversarial training algorithm for nonlinear AUC maxi-
mization based on the minimax problem for DNNs.
•	PNU-AUC: A kernel-based semi-supervised AUC optimization algorithm based on positive and
unlabeled learning for SVMs (Sakai et al., 2017).
•	SAMULT: A kernel-based semi-supervised AUC optimization method which achieves unbiased
AUC risk estimation by treating unlabeled data as both positive and negative data for SVMs
(Zheng & Ming, 2018).
•	QSG-S2AUC: A kernelized scalable quadruply stochastic gradient algorithm for nonlinear semi-
supervised AUC optimization on SVMs (Shi et al., 2019).
•	S2AT-AUC(S): Our scalable semi-supervised adversarial training algorithm for nonlinear AUC
maximization on kernel SVMs.
It is notable that the former four algorithms work on DNNs, while the rest work on kernel SVMs.
Datasets. The experiments are
conducted on large-scale datasets
MNIST8m (Lecun & Bottou, 1998)
and CIFAR10 (Krizhevsky & Hinton,
2009). Since we focus on binary
classification, here we select two
similar classes from the datasets
respectively. Their dimensions and
sample sizes are summarized in
Table 1. For all datasets, we set the
Table 1: Datasets used in the experiments.
Datasets	Dimensions	Sizes
CIFAR10 automobile vs. truck	3,072	6,600
CIFAR10 dog vs. horse	3,072	6,600
MNIST8m 0 vs. 4	784	550,000
MNIST8m 6 vs. 8	784	550,000
Sector	55,197	6,412
imbalanced ratio (Nn/Np) as 10.0. Moreover, we also do the experiments on the high dimensional
and highly imbalanced dataset Sector, whose imbalanced ratio is 148.12 (one vs. all).
Attack Settings. We use four commonly used adversarial attack methods to construct adversarial
examples: FGSM (Goodfellow et al., 2014), PGD10 (PGD with 10 steps) (Madry et al., 2017),
C&W (Carlini & Wagner, 2017) and ZOO (Chen et al., 2017), where the former three ones belong
to white-box attack and the last one belongs to black-box attack. Although these attacks are initially
proposed for DNNs, they are also applicable to other learning models.
All the attacks are performed with their l2 version. For FGSM and PGD10, we set the maximum per-
turbation = 3 for MNIST8m, = 1.5 for CIFAR10 and = 1 for Sector, the stepsize for PGD10
is /4, which is a standard setting for adversarial attack (Madry et al., 2017; Ding et al., 2018).
For ZOO, we use the ZOO-ADAM algorithm and set the stepsize η = 0.01, ADAM parameters
β1 = 0.9, β2 = 0.999.
7
Under review as a conference paper at ICLR 2022
Table 2: AUC performance with standard deviation on MNIST8m 0 vs. 4 against different attacks.
	Model	Clean	FGSM	PGD10	C&W	ZOO
	VAT	95.96±0.37	92.23±0.42	81.29±0.56	68.35±0.79	70.86±0.72
DNNs	UAT	94.45±0.26	90.33±0.36	84.84±0.52	70.55±0.89	71.74±0.73
	S2AT-AUC(K)	99.93±0.04	94.33±0.37	88.35±0.69	72.47±0.86	73.79±0.63
	S2AT-AUC(M)	99.05±0.11	97.93±0.46	93.35±0.51	80.88±0.46	79.69±0.78
	PNU-AUC	99.82±0.07	98.50±0.22	95.47±0.48	82.82±0.65	77.23±0.88
SVMs	SAMULT	99.19±0.12	97.76±0.31	95.28±0.59	82.98±0.79	77.96±0.62
	QSG-S2AUC	99.85±0.06	99.45±0.21	95.49±0.29	82.94±0.54	78.38±0.82
	S2AT-AUC(S)	99.84±0.27	99.78±0.23	98.23±0.52	85.39±0.58	82.35±0.64
Table 3: AUC performance with standard deviation on CIFAR dog vs. horse against different attacks.
	Model	Clean	FGSM	PGD10	C&W	ZOO
	VAT	76.33±0.37	72.07±0.55	60.67±0.79	50.68±0.63	52.15±0.82
DNNs	UAT	76.68±0.31	70.59±0.62	64.44±0.82	56.33±0.74	58.66±0.91
	S2AT-AUC(K)	84.13±0.39	72.57±0.62	68.09±0.64	61.33±0.76	62.59±0.84
	S2AT-AUC(M)	81.80±0.49	75.39±0.32	72.51±0.59	65.04±0.66	65.39±0.82
	PNU-AUC	68.54±0.44	65.81±0.67	64.67±0.38	58.16±1.22	62.53±0.89
SVMs	SAMULT	68.81±0.54	66.37±0.53	65.66±0.67	59.90±0.89	62.99±1.14
	QSG-S2AUC	69.92±0.59	66.57±0.77	65.91±0.74	60.28±0.82	62.86±0.94
	S2AT-AUC(S)	69.51±0.39	68.05±0.61	67.18±0.68	63.37±0.89	65.05±0.76
Implementation. All the experiments are conducted on a PC with 48 2.2GHz cores, 80GB RAM
and four Nvidia 1080ti GPUs. The kernel function that we use for algorithms on SVMs is the RBF
kernel k(x, x0) = exp(-σkx - x0k22). The hyper-parameter σ is chosen via cross-validation, search-
ing in the region {σ∣- 3 ≤ log2 σ ≤ 3}. For algorithms on DNNs, We use the PreAct ResNet18
architecture for CIFAR10 and use two convolutional networks with 16 and 32 convolutional filters
folloWed by a fully connected layer of 100 units for MNIST8m, Which are the same models as pro-
vided by Wong et al. (2020). The trade-off parameter β is searched from 0 to 1 at intervals of 0.1.
Since the algorithms are all under l2-norm constrained perturbations, We set = 3 for MNIST8m,
= 1.5 for CIFAR10, = 1 for Sector, and the step size is set as /4.
4.2	Experimental Results
Comparison of AUC Performance with Existing Works. Firstly, We evaluate the AUC perfor-
mance of these algorithms on clean datasets and adversarial examples generated by 4 attack meth-
ods. Due to the page limit, We only shoW the results of CIFAR10 dog vs. horse and MNIST8m 0
vs. 4, the results of other datasets are provided in the appendix. Tables 2 and 3 clearly shoW that for
algorithms on DNNs, our S2AT-AUC is much more effective When dealing With imbalanced data
and also remains robust against different attacks compared With VAT and UAT since We optimize
the AUC metric rather than the accuracy metric. For algorithms on SVMs, natural semi-supervised
AUC optimization methods (PNU-AUC, SAMULT and QSG-S2AUC) are not robust against var-
ious adversarial examples. Our S2AT-AUC enjoys less superiority on clean data but achieves the
best performance When defending against these attacks since standard generalization is at odds With
robustness (Tsipras et al., 2018).
Comparison of Running Time with Different Sizes of Training Samples. Fig. 3 shoWs the
running time of various algorithms When training samples With different sizes. We can find that When
training on DNNs, S2AT-AUC(K) is much more efficient due to its one-layer objective function. For
other algorithms on DNNs, the time-consuming factor lies in the K-step PGD attack. When training
on SVMs, it is clear that PNU-AUC and SAMULT are time-consuming and even out of memory
When training on large-scale datasets, While QSG-S2AUC and S2AT-AUC(S) enjoy high efficiency
and require loW memory Which mainly benefits from the quadruply stochastic gradient algorithm.
In general, our S2AT-AUC on the kernel perspective enjoy high scalability on large-scale datasets.
8
Under review as a conference paper at ICLR 2022
Training Size
(a) CIFAR dog vs. horse (b) automobile vs. truck (c) MNIST8m 0 vs. 4	(d) MNIST8m 6 vs. 8
Figure 3:	The running time of algorithms when training different sizes of samples. (The lines
of SAMULT and PNU-AUC are incomplete because their implementations crash on large training
sets.)
SZAT-AUC(M)
60
0.5
1
1.5
2
2.5
→-PNU-AUC
-v-QSG-B2AUC
-♦-S!IAT-AUC(S)
于5
85
95
90
(a) CIFAR dog vs. horse
(e) CIFAR dog vs. horse (f) automobile vs. truck (g) MNIST8m 0 vs. 4	(h) MNIST8m 6 vs. 8
Figure 4:	Sensitivity analysis of the maximum perturbation (Figs. 4a- 4d) and imbalanced ratio
Nn/Np (Figs. 4e-4h).
Sensitivity Analysis. We investigate the sensitivity of the algorithms against two parameters when
evaluating their performance on test sets, which include the maximum perturbation radius for
PGD attack and the imbalanced ratio Nn /Np . For all experiments, the perturbed testing samples are
generated by PGD10. The results are shown in Fig. 4.
The value of affects the attack power of adversarial examples. With the growth of , more pertur-
bations will be added to the samples. Thus it is more challenging for the algorithms to make the
correct classification. Nevertheless, our algorithm can still maintain superiority in the case of strong
attacks, which can be seen clearly in Figs. 4a-4d.
The imbalanced ratio Nn/Np measures the proportion of positive and negative samples in one
dataset. Figs. 4e-4h show that algorithms which optimize the accuracy metric (VAT and UAT) have
poor performance on datasets with high imbalanced ratio, while the AUC optimization algorithms
can keep relatively stable, which demonstrates the superiority of AUC optimization algorithms on
highly imbalanced datasets.
5 Conclusion
In this paper, we propose a new semi-supervised adversarial training strategy, S2AT-AUC, which is
a framework for nonlinear AUC optimization that can be applied on both DNNs and kernel SVMs.
Since we regard the unlabeled sample as both positive and negative ones which avoids guessing
labels, an unbiased estimation can be achieved. Comprehensive experimental results verify that our
framework achieves better generalization performance against various attacks when dealing with
highly imbalanced datasets compared with existing algorithms. Moreover, it also enjoys high effi-
ciency and scalability when considered from the kernel perspective.
9
Under review as a conference paper at ICLR 2022
References
Anish Athalye, Nicholas Carlini, and David Wagner. Obfuscated gradients give a false sense of
security: Circumventing defenses to adversarial examples. International Conference on Machine
Learning,pp. 274-283, 2018.
Alberto Bietti, Gregoire Mialon, Dexiong Chen, and Julien Mairal. A kernel perspective for regu-
larizing deep neural networks. In International Conference on Machine Learning, pp. 664-674,
2019.
Battista Biggio, Blaine Nelson, and Pavel Laskov. Poisoning attacks against support vector ma-
chines. International conference on machine learning, pp. 1467-1474, 2012.
Leon Bottou. Large-scale machine learning with stochastic gradient descent. In Proceedings of
COMPSTAT’2010, pp. 177-186. Springer, 2010.
Nicholas Carlini and David Wagner. Towards evaluating the robustness of neural networks. ieee
symposium on security and privacy, pp. 39-57, 2017.
Yair Carmon, Aditi Raghunathan, Ludwig Schmidt, Percy Liang, and John C Duchi. Unlabeled data
improves adversarial robustness. Advances in Neural Information Processing Systems, 2019.
Pinyu Chen, Huan Zhang, Yash Sharma, Jinfeng Yi, and Chojui Hsieh. Zoo: Zeroth order optimiza-
tion based black-box attacks to deep neural networks without training substitute models. arXiv:
Machine Learning, pp. 15-26, 2017.
Gavin Weiguang Ding, Yash Sharma, Kry Yik Chau Lui, and Ruitong Huang. Mma training: Direct
input space margin maximization through adversarial training. arXiv preprint arXiv:1812.02637,
2018.
Kai-Bo Duan, Jagath C Rajapakse, and Minh N Nguyen. One-versus-one and one-versus-all multi-
class svm-rfe for gene selection in cancer classification. In European Conference on Evolutionary
Computation, Machine Learning and Data Mining in Bioinformatics, pp. 47-56. Springer, 2007.
Ian Goodfellow, Jonathon Shlens, and Christian Szegedy. Explaining and harnessing adversarial
examples. In ICLR, 2014.
James A Hanley and Barbara J McNeil. The meaning and use of the area under a receiver operating
characteristic (roc) curve. Radiology, 143(1):29-36, 1982.
Hal DaUme Iii. From zero to reproducing kernel hilbert spaces in twelve pages or less.
http://legacydirs.umiacs.umd.edu/ hal/docs/daume04rkhs.pdf, 2004.
Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint
arXiv:1412.6980, 2014.
A. Krizhevsky and G. Hinton. Learning multiple layers of features from tiny images. Master’s
thesis, Department of Computer Science, University of Toronto, 2009.
Y Lecun and L Bottou. Gradient-based learning applied to document recognition. Proceedings of
the IEEE, 86(11):2278-2324, 1998.
Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and Adrian Vladu. To-
wards deep learning models resistant to adversarial attacks. International conference on learning
representations, 2017.
Takeru Miyato, Shin-ichi Maeda, Masanori Koyama, and Shin Ishii. Virtual adversarial training: a
regularization method for supervised and semi-supervised learning. IEEE transactions on pattern
analysis and machine intelligence, 41(8):1979-1993, 2018.
Nicolas Papernot, Patrick Mcdaniel, Xi Wu, Somesh Jha, and Ananthram Swami. Distillation as a
defense to adversarial perturbations against deep neural networks. IEEE symposium on security
and privacy, pp. 582-597, 2016.
10
Under review as a conference paper at ICLR 2022
Ali Rahimi and Benjamin Recht. Random features for large-scale kernel machines. In Advances in
neural information processing Systems, pp.1177-1184, 2008.
Chuck Rosenberg, Martial Hebert, and Henry Schneiderman. Semi-supervised self-training of object
detection models. 2005.
Tomoya Sakai, Marthinus Christoffel du Plessis, Gang Niu, and Masashi Sugiyama. Semi-
supervised classification based on classification from positive and unlabeled data. In Proceedings
of the 34th International Conference on Machine Learning-Volume 70, pp. 2998-3006. JMLR.
org, 2017.
Ludwig Schmidt, Shibani Santurkar, Dimitris Tsipras, Kunal Talwar, and Aleksander Madry. Ad-
versarially robust generalization requires more data. Advances in Neural Information Processing
Systems, 2018.
Ali Shafahi, Mahyar Najibi, Mohammad Amin Ghiasi, Zheng Xu, John P Dickerson, Christoph
Studer, Larry S Davis, Gavin Taylor, and Tom Goldstein. Adversarial training for free. pp. 3358-
3369, 2019.
Wanli Shi, Bin Gu, Xiang Li, Xiang Geng, and Heng Huang. Quadruply stochastic gradients for
large scale nonlinear semi-supervised auc optimization. In Twenty-Eighth International Joint
Conference on Artificial Intelligence IJCAI-19, 2019.
Dimitris Tsipras, Shibani Santurkar, Logan Engstrom, Alexander Turner, and Aleksander Madry.
Robustness may be at odds with accuracy. International Conference on Learning Representations,
2018.
Jonathan Uesato, Jean-Baptiste Alayrac, Po-Sen Huang, Robert Stanforth, Alhussein Fawzi, and
Pushmeet Kohli. Are labels required for improving adversarial robustness? arXiv preprint
arXiv:1905.13725, 2019.
Yisen Wang, Difan Zou, Jinfeng Yi, James Bailey, Xingjun Ma, and Quanquan Gu. Improving
adversarial robustness requires revisiting misclassified examples. international conference on
learning representations, 2020.
Eric Wong, Leslie Rice, and J Zico Kolter. Fast is better than free: Revisiting adversarial training.
arXiv preprint arXiv:2001.03994, 2020.
Huan Xu, Constantine Caramanis, and Shie Mannor. Robustness and regularization of support vector
machines. JMLR, 10:1485-1510, 2009.
Runtian Zhai, Tianle Cai, Di He, Chen Dan, Kun He, John Hopcroft, and Liwei Wang. Adversar-
ially robust generalization just requires more unlabeled data. Advances in Neural Information
Processing Systems, 2019.
Xie Zheng and Li Ming. Semi-supervised auc optimization without guessing labels of unlabeled
data. AAAI 2018, 2018.
Dang Zhiyuan, Li Xiang, Deng Bin, Gu abd Cheng, and Huang Heng. Large-scale nonlinear auc
maximization via triply stochastic gradients. IEEE Transactions on Pattern Analysis and Machine
Intelligence, 2020.
11