Under review as a conference paper at ICLR 2022
Stochastic Induction of Decision Trees with
Application to Learning Haar Tree
Anonymous authors
Paper under double-blind review
Ab stract
Decision trees are a convenient and established approach for any supervised learn-
ing task. Decision trees are used in a broad range of applications from medical
imaging to computer vision. Decision trees are trained by greedily splitting the
leaf nodes into a split and two leaf nodes until a certain stopping criterion is
reached. The procedure of splitting a node consists of finding the best feature
and threshold that minimizes a criterion. The criterion minimization problem is
solved through an exhaustive search algorithm. However, this exhaustive search
algorithm is very expensive, especially, if the number of samples and features
are high. In this paper, we propose a novel stochastic approach for the criterion
minimization. Asymptotically, the proposed algorithm is faster than conventional
exhaustive search by several orders of magnitude. It is further shown that the
proposed approach minimizes an upper bound for the criterion. Experimentally,
the algorithm is compared with several other related state-of-the-art decision tree
learning methods, including the baseline non-stochastic approach. The proposed
algorithm outperforms every other decision tree learning (including online and
fast) approaches and performs as well as the baseline algorithm in terms of accu-
racy and computational cost, despite being non-deterministic. For empirical eval-
uation, we apply the proposed algorithm to learn a Haar tree over MNIST dataset
that consists of over 200, 000 features and 60, 000 samples. This tree achieved
a test accuracy of 94% over MNIST which is 4% higher than any other known
axis-aligned tree. This result is comparable to the performance of oblique trees,
while providing a significant speed-up at both inference and training times.
1	Introduction
Decision trees are among the most established machine learning models in the scientific community
and have been widely used in various applications, such as computer vision and medical imaging
Criminisi & Shotton (2013); Freund et al. (1999); Breiman (2001); Tavallali et al. (2019). Deci-
sion trees are extensively used as a sub-procedure in ensemble models, such as boosting and forest
Criminisi & Shotton (2013). Decision tree training consists of recursively splitting a leaf node into
one split and two leaf nodes Breiman et al. (1984); Gehrke et al. (1999). The split node consists
of a decision stump that routs the samples to either of its left or right children. The splitting of a
leaf node is cast as selecting the best feature and threshold that minimizes a desired criterion. The
criterion is an impurity function, such as Gini index Breiman et al. (1984), cross-entropy Quinlan
(1986), uniform piece-wise constant densities Ram & Gray (2011), Gaussian differential (contin-
uous) entropy Criminisi & Shotton (2013) and others Yang & Wong (2014); Liu & Wong (2014);
Sheikhi & Babamir (2018; 2016). The best feature and threshold are selected through an incre-
mental algorithm that evaluates combination of every possible feature and threshold. However, this
algorithm becomes computationally very expensive if the dataset is large or high dimensional. The
computational complexity of solving criterion minimization problem is O(DN logN), where N and
D are number of samples and dimensionality, respectively. The computational complexity becomes
very high especially when N and D are high since N and D are multiplied in the computational
complexity term (O(DN logN)). In this paper, we tackle the abovementioned issue by applying
the splitting algorithm in a novel iterative and stochastic fashion, where less important features are
discarded at each iteration.
1
Under review as a conference paper at ICLR 2022
The literature includes many proposals for speeding up the training procedures of various learning
algorithms. One common approach is preprocessing the dataset by reducing the sample size or
applying some feature selection technique Dash & LiU (1997); Jovic et al. (20l5); John et al. (1994).
Pre-training feature selection will potentially lead to a less computationally expensive process for
learning Jovic et al. (2015); Kohavi & Sommefield (1995); Koller & Sahami (1996); Korn et al.
(2001). Feature selection algorithms are generally categorized in two groups of filter and wrapper
approaches.
The filter approaches consist of selecting features with regard to a desired criterion Jovic et al.
(2015). Generally, filter approaches use statistical measures, such as minimum Redundancy max-
imum Relevance-(mRmR) Tang et al. (2014), correlation Yu & Liu (2003), chi-square Moh’d
A Mesleh (2007), information gain Bhattacharyya & Kalita (2013), gain ratio Witten & Frank
(2002), etc.
The wrapper approaches consist of selecting features and minimizing the loss function jointly Bhat-
tacharyya & Kalita (2013); Bradley & Mangasarian (1998); Maldonado et al. (2014); Hastie et al.
(2009). However, feature selection as a preprocessing method might not lead to an accurate final
model since features are not guaranteed to contribute to lower loss Jovic et al. (2015). Nevertheless,
wrappers are practical only for fast modeling algorithms or greedy search strategies, such as linear
SVM Liu et al. (2014), Nave Bayes Cortizo & Giraldez (2006), and Extreme Learning Machines
Beno^t etal. (2013).
In Embedded approaches, first a filter approach is applied to select a subset of features and then
followed by a wrapper approach to select the best candidate feature Guyon & Elisseeff (2003); Das
(2001). Different decision tree algorithms, such as CART Breiman et al. (1984), C4.5 Quinlan
(2014), and random forest are commonly used as embedded methods Sandri & Zuccolotto (2006).
The feature selection as a preprocessing approach decreases the computational complexity of train-
ing algorithm. However, it is suboptimal w.r.t. the objective function of training, and is inefficient
for large datasets since it does not decrease computation over number of samples Rodriguez-Lujan
et al. (2010).
A well-known approach that tackles the issue of large datasets is online learning Utgoff (1989).
Various studies consider applying online learning to decision trees Utgoff (1989); Schlimmer
& Fisher (1986); Wang et al. (2003). Such algorithms are fast, simple, and often make few
statistical assumptions. The objective of online learning is to update and train the model from a
stream of data Shalev-Shwartz et al. (2011). At each time step, the model gets updated optimally
with respect to the recently acquired samples. Online learning is more computationally efficient
compared to offline learning where the entire trainset is used for training Shalev-Shwartz et al.
(2011). However, these algorithms increase the size of the tree when visiting a new batch of the data.
Despite the extensive usages and applications of decision trees Criminisi & Shotton (2013), few
algorithms have focused on decreasing the high computational cost of inducing a tree ??. However,
the literature leaves a gap in treating the high computational complexity of inducing the decision tree
and exploring potential improvements. None of the previous research propose the computational
complexity of their algorithm or provide any analysis of the goodness of features selected at the split
node. To address this gap, we investigated both computational complexity of SDT and its efficiency
in finding near optimum feature at the split node. In ?, authors propose to randomly select a subset
of features at the split node and then find the best feature among the set using all samples present at
the node. However, due to randomness, the optimality of this process is not guaranteed.
The high computational cost is mainly rooted in the exhaustive search algorithm applied for solving
the split criterion. This algorithm specially becomes computationally expensive if the dataset is large
and high dimensional. It is because the computational complexity of solving the splitting criterion
is O(DN logN) Hoare (1961); Breiman et al. (1984).
To counter the aforementioned issues, we propose an accurate and fast Stochastic Decision Tree
(SDT) induction algorithm that minimizes the splitting criterion efficiently. At a node, the proposed
algorithm starts with an empty set St and all D features. At node j , the algorithm iteratively se-
lects a tiny random set of samples from Sj (in order of 2-C × |Sj|) and discards half of the less
important features w.r.t St. Sj and |Sj| are the set and number of samples present at the jth node,
2
Under review as a conference paper at ICLR 2022
respectively. For the best feature and threshold found at each iteration, SDT minimizes an upper
bound of the splitting criterion, monotonically. Overall, the algorithm steeply descends towards the
minimum of the criterion function. It is also demonstrated by the results at the experiments section.
Mathematically speaking, we show that the algorithm provides higher priority to more distinctive
features. Essentially, the more distinctive a feature, the greater the chance of it being selected at the
final iteration. The computational complexity of the algorithm for constructing a tree of depth ∆ is
O( δdNog2C ). C is a hyperparameter and in our experiments We set it to 10. The computational
complexity achieved by SDT is smaller than that of the original approach (with O(∆DN logN)) and
its optimal implementation (O(∆DN + DN logN)) by several orders of magnitude. Our approach
is faster because it focuses on breaking doWn the computational complexity created due to the large
number of samples and features. Numerically speaking, SDT shoWs its advantage over other im-
plementations When the sizes of samples and features are high (ND is very high). Experimentally,
the proposed algorithm could achieve same or improved accuracy as the original greedy algorithm
Breiman et al. (1984) While surpassing every other related state-of-the-art algorithms both in terms
of accuracy and complexity by several orders of magnitude. For empirical evaluation, We applied
our algorithm to learn a Haar tree over MNIST Where Haar Viola & Jones (2001) filters Were used
to extract 200, 000 features from the dataset. The Haar tree could achieve competitive accuracy to
more complicated trees, such as oblique trees and optimal oblique trees While its inference time Was
far smaller. Theoretically, an oblique tree needs O(D∆) operations to produce prediction, While
Haar tree only needs O(D + ∆). ∆ is the maximum depth of the tree. In terms of size, Haar tree
takes O(2∆) space While oblique tree takes O(D2∆) space.
2	Preliminaries
2.1	Induction of a decision tree
Every decision tree construction algorithm tends to minimize the folloWing objective function
Lα(T) = L(T) + α × leaves(T),	α > 0,	(1)
Where, L(T) is a loss function of the tree (T) over a trainset, leaves(.) represents the number of
leaves in a tree, and α > 0 is cost complexity coefficient. After constructing the tree, pruning is
performed to minimize the cost function Lα(T) over the validationset. Finding the optimum of this
problem is NP-complete ?. Therefore, state-of-the-art algorithms for constructing a tree consist of
greedily splitting a leaf node into a split and tWo leaf nodes.
Assume a dataset S consisting N pairs of observation {(xi, yi)}iN=1 Where xi ∈ RD and yi ∈
{1, 2, 3, ..., M}. The function fj (x) at the jth split node consists of a decision stump fj (x) =
sign(xp - th) Where superscript p and th represent pth feature and the threshold, respectively. At
the groWing phase of a decision tree, p and th are found optimally With respect to some criteria
Breiman et al. (1984). Usually the tree is groWn until some stopping criterion or maximum depth is
reached.
The problem of constructing a split node consist of minimizing the desired criterion over the samples
of each child of the node.
j| H ⑻)
min
fj
s.t
Σ
fj={-1,1}
(2)
fj (x) = sign(xp - th)
Where, Sjfj is set of samples at the left child (fj= -1) or right child (fj= 1) of a node j, Sjis
the set of samples present at the jth node, and |.| returns the number of samples at its input set. H
is an entropy function. The optimum of problem equation 2 can be found in O(DNjlogNj) Where
Nj and D are number of samples at node j (|Sj |) and features, respectively. This is done by sorting
all samples along each feature and incrementally finding the best threshold for each feature Breiman
et al. (1984).
3
Under review as a conference paper at ICLR 2022
3	Proposed Method
The computational complexity of solving problem equation 2 is high, especially, if the number of
features and samples are large. In this section, we propose an algorithm that minimizes problem
equation 2 faster than original approach Breiman et al. (1984) by several orders of magnitude.
Basic idea: Problem equation 2 consists of selecting the best feature to partition the space into
two parts optimally. State-of-the-art algorithms find the best feature through the exhaustive search
mentioned in Section 2.1. In this section, we propose a stochastic exhaustive search method for
solving problem equation 2 that has a low computational cost. The idea is based on 3 pieces of
information. Info. #1 Each feature can be approximately ranked based on the objective function of
equation 2 over a small random subset of the samples. Info. #2 A precise ranking of the features
requires larger subset of the samples. Info. #3 The good news is that only ranking of the most
competitive features require high precision. As a result, by combining Info #1-3, the algorithm
consists of discarding less important features and randomly selecting more samples from the dataset
at each iteration. This procedure gradually increases the precision of ranking for more important
features. We call this algorithm the stochastic induction of decision tree (SDT).
The algorithm Now we describe the proposed algorithm for solving criterion problem equation 2.
The algorithm starts with Sjc = Sj, an empty set Sjt, and set of features Dt = {p}pD=1 features. p
represents the pth feature. The algorithm proceeds as follows:
Step 1 Randomly select and remove NNj samples from Sjc and add them to set Sjt. C is a user
defined hyper-parameter.
Step 2 ∀p ∈ Dt, at kth iteration, find feature importance of pth feature (FIpk) by solving
(F Ipk)-1 = min
X	ISfjI H(S j
f ={-1,1}所(Sjt)
(3)
s.t fj (x) = sign(xp - th)
where, Sjftj represents the set of samples from Sjt routed to the left child (fj = -1) or the right
child fj = 1. F Ipk represents the feature importance of pth feature at kth iteration. Note that the
optimization is only happening over th in equation 3 and (FIpk)-1 is equal to the optimum of ob-
jective function in equation 3. Problem in equation 3 can be efficiently solved in O(ISjt Ilog(ISjt I))
Breiman et al. (1984).
Step 3 Sort all the features in Dt based on FIpk and discard half of the features in Dt with lowest
FI.
Step 4 Iterate over steps 1-3 for α times. α is a user-defined hyper-parameter.
Step 5 Solve problem equation 2 for only the remaining features in Dt over the Sj .
In practice, we select C to be around 10 and α to be around 7 - 10, depending on the dataset.
Intuitively, C and α are the hyper-parameters that control the computational complexity of the algo-
rithm. C controls the number of samples to be used at each iteration and α controls the number of
features to be used at the last iteration. With higher values for C and α, the computational complex-
ity decreases further. However, there is a possibility that the model loses accuracy with extremely
high C andα.
To keep the computational complexity low, one can keep the samples in Sjt sorted for the remaining
features (Dt) from the previous iterations at Step 2. Only the newly selected samples from Step 1
have to be sorted and merged with Sjt. The merging will take at most ISjt I. In total, this will keep
the computational complexity of step 2 at O(2DNNjlog(NNj)) for all the α steps (assuming α → ∞).
4 Analysis of the Proposed Algorithm
In this section, several theoretical properties of the algorithm are explored and analyzed.
4
Under review as a conference paper at ICLR 2022
4.1	Computational Complexity
The computational complexity of splitting a node and induction of a tree are analyzed in this Sub-
section.
In the following theorem, we assume that the samples of Sjt from previous iterations are NOT kept
sorted. After the theorem, we will show how keeping samples sorted can affect the node training
complexity and storage overhead.
Theorem 1 (node training complexity). The computational complexity of splitting the node using
SDT is O(4DNologNo), where N = Nj.
Proof. At each iteration, N0 samples are added to the set Sjt and half of the features are dis-
carded. To solve equation 3, samples along each remaining feature have to be sorted that takes
O(|Sjt |logSjt). At the kth iteration, |Sjt | is (k + 1)N0. Therefore, the total complexity of steps
1-4 is as follows:
αD
2k(( 2 (k + I)NOlog(k + I)NO
k=0
The summation in equation 4 can further be simplified to:
α D	α k+1
^X 2k(k + I)NO(IogNO + log(k + I)) ≤ DNO(IogNO + log(α + I)) ^X 2k
(4)
(5)
k=0
k=0
Using the formula for the geometrical series of Pk∞=O铜=P∞=O 壶 + P∞=12⅛ + Pk=22k +
... = 2 + 1 + 2 + 4 + ... = 4 in the right hand side of inequality of equation 5, We have
DNO(logNO + log(α + 1)) X ʌɪ+ 1 ≤ 4DN°(logNO + log(α + 1))	(6)
k=O
Asymptotically speaking, the computational complexity is being driven by the term 4DNOlogNO .
Note that log(α + 1) is relatively very small compared to logNO (Numerically speaking, NO is
in order of 100 - 200 While α is in order of 7 - 10). Therefore, the computational complexity
asymptotically becomes O(4DNOIogNo)	□
The computational complexity of the baseline algorithm for solving equation 2 is O(DNjlogNj),
Which is 2C times larger than the proposed stochastic approach.
Effect of keeping the Sjt sorted Effect of keeping the Sjt sorted is that k+ 1 is removed from the
formula equation 4. Therefore, the computational complexity changes to O(2DNOlogNO). HoW-
ever, it Will increase the space usage slightly ((k + 1)NO neW variables need to be saved to keep the
label of samples along each feature).
Assuming a maximum depth of ∆ for the tree, the computational complexity of inducing the Whole
tree will be O(---?£2C). It is trivial to prove by simply using the fact that the children of each
node receive a disjoint set of samples from its parent and the fact that total samples at the same depth
are at most N. This complexity is smaller than complexity of the baseline approach Breiman et al.
(1984) by an order of 2C.
4.2 Relation to Objective Function
In this section, the value of the objective function in equation 2 is evaluated using the best combina-
tion of feature and threshold found by the SDT at each iteration over Sjt. For simplicity, throughout
this section, the objective function of equation 2 is represented by L(Sj). Consequently, L(Sjt) rep-
resents error over the set Sjt. In this section, we assume the used criterion is misclassification error
H(Sjj) = 1 -Pfj, wherePm and m* represent ratio of class m in the fj child and majority class in
the partition, respectively. All the theorems and analysis can be readily extended to Gini-index and
Cross-entropy Breiman et al. (1984) by applying their corresponding differences. The differences
and how to apply them are explained after next theorem.
5
Under review as a conference paper at ICLR 2022
	Table 1:			The USed datasets Information	
		Size		
Index	Name	Train	Test	Description
1	MNIST	60000 × 784	10000 × 784	A large database of handwritten digits 0 to 9 that is serving as a basis for classification algorithms
2	FASHIOMNIST	60000 × 784	10000 × 784	A dataset of Zalando’s article images contaianes 10 different classes of gray images of T-shirt/top ,TroUser, PUllover, Dress, Coat, Sandal, Shirt, Sneaker, Bag, Ankle boot
3	SIGNMNIST	27455 × 784	7172 × 784	The American Sign LangUage letter database of hand gestUres represent a mUlti-class problem with 24 classes of letters (exclUding J and Z which reqUire motion)
4	ISOLET	5238 × 617	1559 × 617	ISOLET (Isolated Letter Speech Recognition) is the name of each letter of the alphabet twice that is spoken by 52 speakers and a good domain for a noisy, perceptUal task
5	SATIMAGE	4825 × 36	1610 × 36	The database consists of the mUlti-spectral valUes of pixels in 3×3 neighboUrhoods in a satellite image, and the classification associated with the central pixel in each neighboUrhood
6	COVERTYPE	435759 × 54	145253 × 54	This dataset contains tree observations from foUr areas of the Roosevelt National Forest in Colorado. All observations are cartographic variables
Theorem 2 (Monotonic decrease of an upper bound). The algorithm will monotonically decrease
an upper bound over the objective function of equation 2.
Proof. At each iteration k, the following equality holds:
|Sj|Lk(Sj) = |Sjkt|Lk(Sjkt) + |Sjkc|Lk(Sjkc)
(7)
where, the superscript k for sets and Lk (.) represents the set of samples at the kth iteration and
the value of error with optimum parameters for Sjkt over the input set, respectively. The loss L(.)
is always bounded from above (e.g., for misclassification, it is 1) by a constant value Lmax . By
replacing Lk (Sjkc) with Lmax in equation 7, for iteration k we have
|Sj|Lk(Sj) ≤ |Sjkt|Lk(Sjkt)+|Sjkc|Lmax= |Sj|Lkupper(Sj)
(8)
At each iteration, the first term in equation 8 is minimized. Therefore, the new optimum parameters
at each iteration provide lower error to the first term of equation 8 than the optimum parameters of
previous iteration which is |Sjkt+1|Lk(Sjkt+1) ≥ |Sjkt+1|Lk+1(Sjkt+1). Note that the superscript of the
loss L is different for both sides of the inequality. Also, |Sjkc+1| < |Sjkc |. Therefore,
|Sjkt+1|Lk+1(Sjkt+1) + |Sjkc+1|Lmax
Lku+pp1er(Sj)≤Lkupper(Sj)
(9)
|Sj|
□
For the case of Gini-index or Cross-entropy, formula equation 7 should be replaced with Lk (Sj ) ≤
Lk (Sjkt) + Lk (Sjkc). The reason for this change is that the algorithm finds optimum over Sjkt and
min(Lk(Sjkt),Lk(Sjkc)) ≤ Lk(Sj) ≤ max(Lk (Sjkt), Lk (Sjkc)).
Average Decrease The Theorem equation 2 is based on worst case scenario. The proof of Theo-
rem equation 2 assumes that the samples selected at each iteration do not have any correlation with
the rest of samples. In reality, this is not necessarily correct and set of Sjt can partially represent
the whole trainset Sj and as size of Sjt increases through iterations, the FIp is measured more cor-
rectly. In our experiments, we observed a very steep decrease in Lk(Sjt ) after each iteration and
after several iterations, it almost converged to the loss of the best possible feature and threshold.
More discussion and experiments are presented in supplemental materials.
Theorem 3 (Probability of discarding a feature). Assume a binary classification dataset where sam-
ples are generated from some distribution along each feature and features are independent. We
assume the samples of both classes have varying degrees of overlap along each feature. Proba-
bility of a sample being generated from the overlap area is Pop for pth feature. The probability of
discarding a feature depends on Pop .
Proof. Picking N0 samples randomly from an infinitely large Nj is equivalent to directly sampling
from the distribution itself. Selecting samples is the same as flipping a coin with probability of Pop
to be 1 and 1 - Pop to be 0. Expectation of observing 1 is Pop and essentially, on average, we expect
to observe Pop × N0 samples from the overlap area. Probability of deviating from average case and
observing FIpk being worse than the true FIp (please note that FI corresponds to goodness of a
6
Under review as a conference paper at ICLR 2022
90
80
70
60
50 50
ω
P 40
30
20
10
0
80
70
60
≡ 50
S
S
自40
30
20
10
106	107
108	109	1010	1011	1012	105
COVERTYPE
106	107	108	109	1010	1 011
106	1 07	108	109	1 010	1011	1012
Computational Complexity	Computational Complexity	Computational Complexity
Computational Complexity
Computational Complexity
Computational Complexity
Figure 1: Error ratio versus computational complexity for various models.
feature) over the whole trainset is equivalent to probability of observing at least 1 extra (observing
P0p × N0 + 1 from overlap area) sample from the overlap area. Therefore, P (F Ipk < FIp) =
1 - (1 - Pok)N0(1-Pok) (or 1- probability of observing no extra sample from overlap area) which is
an upper bound to probability of discarding a feature.	□
The probability of overlap area Pop is inversely related to FIpk since samples with lower overlap area
(lower Pop) are more distinctive; thus, such features have higher FIpk . The Theorem 3 implies that
the features with higher true F Ip are more likely to stay in the iterations on average. For example,
assume a feature that completely separates samples of both classes (Pop = 0) - any random set of
samples will represent this distinction because of having no overlap. Such a feature will achieve
F Ipk = ∞, hence, the feature will remain in the Dt .
Theorem 4	(Consistency of SDT). The SDT is consistent and can learn the function that generated
the classes.
Theorem 5	(Decrease of the cost). The objective function of equation 2 decreases as new nodes are
added.
The proofs are provided in the supplemental materials. It is worth mentioning that both the theorems
of4 and 5 are true for original decision trees (such as CART) under some mild assumptions and here
we show that SDT follows them too.
5	Experimental Results
In this section, we present results of experiments to show the merits of our proposed method. The
SDT is compared with several other state-of-the-art algorithms and the baseline tree induction. Fig-
ures 1 and 2 present error ratio of train and test versus the computational complexity of each studied
algorithm. The existing algorithms selected in the study consist of adaptive re-sampling Iyengar
et al. (2000), using feature selection as pre-training procedureTang et al. (2014); Moh’d A Mesleh
(2007), C4.5 Quinlan (2014), and CART Breiman et al. (1984). The adaptive re-sampling consists of
iteratively training a tree from the scratch with respect to a tiny set of samples and expanding the set
with wrongly classified samples by the tree at each iteration. We applied the adaptive re-sampling in
two different setups. In one setup, we applied the adaptive re-sampling of Iyengar et al. (2000) until
the algorithm achieves better accuracy or similar to that of SDT (Adaptive re-sampling 2). In the
other setup, we followed the same steps as provided by Iyengar et al. (2000) (Adaptive re-sampling
2). For feature selection as a pre-training setup, we used ChiSquare and mRmR Tang et al. (2014);
Moh’d A Mesleh (2007). The pre-training feature selections were applied to select 1%, 50%, and
7
Under review as a conference paper at ICLR 2022
SATIMAGE
106
20
107	108	109	1010	107
Computational Complexity
,SATIMAGE
SIGNMNIST
≡ 60
I 50
1011
sj,0*i0*30≈β
,0.U-S9I
106	107	108	109	1010	1 011
Computational Complexity
,0.Ua -S9I
106	107	108	109	1010
Computational Complexity
108	109	1010
Computational Complexity
SIGNMNIST
8isj,i,0β*
,0.U-S9I
107
108	109	1010	1 011
Computational Complexity
105
Figure 2: Error ratio versus computational complexity for various models.
90% of features. The C4.5 was trained using the same setup as Quinlan (2014). The decision tree
curve represents the CART algorithm Breiman et al. (1984). All the trees were trained for depths of
5-12. In figures 1 and 2, the number at each point on the curve represents the depth of the tree. The
vertical axis shows the error ratio and horizontal axis shows the computational complexity of the
trained model. For SDT, we set N0 = 20 for all experiments. The SDT iterated until the number of
features remained in Dt are around 0.5% of the total features. Description of datasets are presented
in table 1.
Figures 1 and 2 depict that SDT outperformed other fast tree training algorithms in both accuracy
and computational complexity. The training complexity of SDT is smaller by several orders of
magnitude (102-3). It is worth mentioning that although SDT uses less computation, its accuracy
is the same as CART algorithm. Therefore, SDT has served its purpose which was decreasing the
complexity of training while maintaining the accuracy.
5.1 Haar Tree
Haar-like features are well-known in the literature of image processing and have been extensively
used for face detection Viola & Jones (2001); Jones & Viola (2003); Demirkir & Sankur (2004);
Mita et al. (2005); Pham & Cham (2007), object detection Lienhart & Maydt (2002); Park &
Hwang (2014); Chen et al. (2007); Choi (2012) and ensembles Moghimi et al. (2018); Kwak et al.
(2013); Larios et al. (2010). The Haar-like features are extracted by convolving various sizes of
the Haar-filters with the im-
age. Haar features are ex-
plained further in the sup-
plemental materials. How-
ever, the number of fea-
tures extracted by Haar-
like features is very high;
thus, training a model using
these features can be very
challenging and expensive.
SDT is designed to over-
come such a challenging
task and we trained trees
over Haar-like features us-
ing SDT. We call such tree
as Haar tree. We applied
5 3 5 2 5 1
,∙3o,∙2ouOI∙
Oooo
LN
102	103	104	105	106
inference complexity
-e-Haar Tree
CART
OC1
K-Means
RBF-SVM
0.35
0.3
$
W 0.25
0.2
0.15
101	102	103 104	105 106	107
inference complexity
Figure 3: Inference complexity versus error ratio over test set for each
model.
10

8
Under review as a conference paper at ICLR 2022
Haar-like features to MNIST and FMNIST to extract new features. The size of each region of
used Haar filters are (3n, 3m) for m, n = 1, 2, ..., 8. 121000 features were extracted from images of
28 × 28.
Figure 3 presents inference complexity versus error ratio for each model. We compared Haar trees
with oblique trees Heath et al. (1993), axis-aligned trees Breiman et al. (1984), RBF-SVM and
nearest neighbor k-means. The RBF-SVM consists of training SVM over various Radial Basis
Functions (RBF). The center of RBFs are found using k-means algorithm. The width of the RBFs
were found using a validationset (trainset was split to 80% train and 20% validation). The k-means
model consisted of training a k-means and then using centroids as a nearest neighbor model (used
similar to Tavallali et al. (2020)). The label of centroids were selected as the majority of label of
samples assigned to each centroid. The horizontal axis in figure 3 represents the computational
complexity of evaluating the model for an input (inference complexity). The vertical axis shows the
error ratio over the testset. The number at each point on a curve represents the depth of the tree or
number of centroids. We observed that the Haar trees were able to achieve better accuracy than the
other trees and also better or similar accuracy to other nearest neighbor-based models. The Haar
trees are the first axis-aligned trees to achieve accuracy higher than any other axis-aligned trees as
per our study. Other axis-aligned trees have never passed the accuracy of 90% over MNIST while
Haar Tree could achieve accuracy of 93%. Asymptotically, the inference of a Haar Tree is O(D+∆)
because first an integral image is calculated and then at each node, the desired feature is calculated
only using the integral image Viola & Jones (2001). The inference of oblique tree (OC1), k-means,
and RBF-SVM are O(D∆), O(DK) and O(D(K + 1)), respectively. As can be observed, Haar
Trees are faster than other models asymptotically while Haar Trees could achieve smaller test error.
6	Limitations:
This paper provides the first stochastic approach for induction of a decision tree. Consequently,
the model and the proposed algorithm suffers from limitations that a decision tree might suffer.
However, in the context of decision tree and ensemble models it potentially can enhance the state
of the field, since the approach enables faster and more computationally efficient creation of deci-
sion tree and forest models. The approach essentially opens the path for introduction of tree-based
convolution filters.
7	Conclusion
The tree training is expensive because of the exhaustive search for solving the criterion minimization
problem. Despite the vast usage of decision trees in a broad range of applications such image
processing, computer vision, and medical diagnosis, the mentioned problem has not been objectively
approached. We tackled the problem by proposing a novel stochastic approach for tree induction. To
the best of our knowledge, the SDT is the first stochastic algorithm designed for induction of trees
through greedy approach. Asymptotically, the SDT improved the decision tree induction complexity
by orders of magnitude. It was shown that the algorithm is likely to find the best feature and threshold
at the split node. It was also shown that the algorithm minimizes an upper bound for the criterion
problem. Experimentally, the SDT could achieve same accuracy as the baseline tree CART and
outperformed other similar methods (e.g., online and fast tree induction). In the experiments, SDT
could train a tree faster than other algorithms by several orders of magnitude. Finally, using SDT,
we tackled the challenging problem of training Haar Trees. The Haar Trees could achieve better
accuracy than other more complicated machine learning models, such as oblique trees and nearest
neighbor-based models.
8	Reproducibility
The experiment codes (currently attached as supplementary materials) will be published on a Github
repository to facilitate reproduction and extension of our results. Furthermore, the datasets used in
our experiments are publicly available via the UCI repository.
9
Under review as a conference paper at ICLR 2022
References
Frenay Beno^t, Mark Van Heeswijk, Yoan Miche, Michel Verleysen, and Amaury Lendasse. Feature
selection for nonlinear models with extreme learning machines. NeurocomPuting, 102:111-124,
2013.
Dhruba Kumar Bhattacharyya and Jugal Kumar Kalita. Network anomaly detection: A machine
learning PersPective. Crc Press, 2013.
Paul S Bradley and Olvi L Mangasarian. Feature selection via concave minimization and support
vector machines. In ICML, volume 98, pp. 82-90, 1998.
Leo Breiman. Random forests. Machine learning, 45(1):5-32, 2001.
Leo J. Breiman, Jerome H. Friedman, R. A. Olshen, and Charles J. Stone. Classification and Re-
gression Trees. Wadsworth, Belmont, Calif., 1984.
Qing Chen, Nicolas D Georganas, and Emil M Petriu. Real-time vision-based hand gesture recog-
nition using haar-like features. In 2007 IEEE instrumentation & measurement technology confer-
ence IMTC 2007, pp. 1-6. IEEE, 2007.
Jaesik Choi. Realtime on-road vehicle detection with optical flows and haar-like feature detectors.
Technical report, 2012.
JoSe Carlos Cortizo and Ignacio Giraldez. Multi criteria wrapper improvements to naive bayes
learning. In International Conference on Intelligent Data Engineering and Automated Learning,
pp. 419-427. Springer, 2006.
Antonio Criminisi and Jamie Shotton. Decision Forests for ComPuter Vision and Medical Image
Analysis. Advances in Computer Vision and Pattern Recognition. Springer-Verlag, 2013.
Sanmay Das. Filters, wrappers and a boosting-based hybrid for feature selection. In Icml, volume 1,
pp. 74-81, 2001.
Manoranjan Dash and Huan Liu. Feature selection for classification. Intelligent data analysis, 1(3):
131-156, 1997.
Cem Demirkir and B Sankur. Face detection using boosted tree classifier stages. In Proceedings
of the IEEE 12th Signal Processing and Communications APPlications Conference, 2004., pp.
575-578. IEEE, 2004.
LUC Devroye, LaSzlo Gyorfi, and Gabor Lugosi. A probabilistic theory of pattern recognition, vol-
ume 31. Springer Science & Business Media, 2013.
Yoav Freund, Robert Schapire, and Naoki Abe. A short introduction to boosting. Journal-Japanese
Society For Artificial Intelligence, 14(771-780):1612, 1999.
Johannes Gehrke, Venkatesh Ganti, Raghu Ramakrishnan, and Wei-Yin Loh. Boatoptimistic deci-
sion tree construction. In Proceedings of the 1999 ACM SIGMOD international conference on
Management of Data, pp. 169-180, 1999.
Isabelle Guyon and Andre Elisseeff. An introduction to variable and feature selection. Journal of
machine learning research, 3(Mar):1157-1182, 2003.
Trevor Hastie, Robert Tibshirani, and Jerome H. Friedman. The elements of statistical learning:
data mining, inference, and prediction. New York, NY: Springer, second edition, 2009.
David Heath, Simon Kasif, and Steven Salzberg. Induction of oblique decision trees. In IJCAI,
volume 1993, pp. 1002-1007, 1993.
Charles Antony Richard Hoare. Algorithm 64: quicksort. Communications of the ACM, 4(7):321,
1961.
Vijay S Iyengar, Chidanand Apte, and Tong Zhang. Active learning using adaptive resampling. In
Proceedings of the sixth ACM SIGKDD international conference on Knowledge discovery and
data mining, pp. 91-98, 2000.
10
Under review as a conference paper at ICLR 2022
George H John, Ron Kohavi, and Karl Pfleger. Irrelevant features and the subset selection problem.
In Machine Learning Proceedings 1994, pp.121-129. Elsevier,1994.
Michael Jones and Paul Viola. Fast multi-view face detection. Mitsubishi Electric Research Lab
TR-20003-96, 3(14):2, 2003.
Alan Jovic, Karla BrkiC, and Nikola Bogunovic. A review of feature selection methods with appli-
cations. In 2015 38th international convention on information and communication technology,
electronics and microelectronics (MIPRO), pp. 1200-1205. Ieee, 2015.
Ron Kohavi and Dan Sommerfield. Feature subset selection using the wrapper method: Overfitting
and dynamic search space topology. In KDD, pp. 192-197, 1995.
Daphne Koller and Mehran Sahami. Toward optimal feature selection. Technical report, Stanford
InfoLab, 1996.
Flip Korn, B-U Pagel, and Christos Faloutsos. On the” dimensionality curse” and the” self-similarity
blessing”. IEEE Transactions on Knowledge and Data Engineering, 13(1):96-111, 2001.
Ju-Hyun Kwak, Il-Young Woen, and Chang-Hoon Lee. Learning algorithm for multiple distribu-
tion data using haar-like feature and decision tree. KIPS Transactions on Software and Data
Engineering, 2(1):43-48, 2013.
Natalia Larios, Bilge Soran, Linda G Shapiro, Gonzalo Martinez-Munoz, JUnyUan Lin, and
Thomas G Dietterich. Haar random forest features and svm spatial matching kernel for stone-
fly species identification. In 2010 20th International Conference on Pattern Recognition, pp.
2624-2627. IEEE, 2010.
Rainer Lienhart and Jochen Maydt. An extended set of haar-like features for rapid object detection.
In Proceedings. international conference on image processing, volume 1, pp. I-I. IEEE, 2002.
Chao Liu, Dongxiang Jiang, and Wenguang Yang. Global geometric similarity scheme for feature
selection in fault diagnosis. Expert Systems with Applications, 41(8):3585-3595, 2014.
Linxi Liu and Wing Hung Wong. Multivariate density estimation based on adaptive partitioning:
Convergence rate, variable selection and spatial adaptation. Department of Statistics, Stanford
University, 2014.
Sebastian Maldonado, Richard Weber, and Fazel Famili. Feature selection for high-dimensional
class-imbalanced data sets using support vector machines. Information sciences, 286:228-246,
2014.
Takeshi Mita, Toshimitsu Kaneko, and Osamu Hori. Joint haar-like features for face detection. In
Tenth IEEE International Conference on Computer Vision (ICCV’05) Volume 1, volume 2, pp.
1619-1626. IEEE, 2005.
Mohammad Mahdi Moghimi, Maryam Nayeri, Majid Pourahmadi, and Mohammad Kazem
Moghimi. Moving vehicle detection using adaboost and haar-like feature in surveillance videos.
arXiv preprint arXiv:1801.01698, 2018.
Abdelwadood Moh’d A Mesleh. Chi square feature extraction based svms arabic language text
categorization system. Journal of Computer Science, 3(6):430-435, 2007.
Ki-Yeong Park and Sun-Young Hwang. An improved haar-like feature for efficient object detection.
Pattern Recognition Letters, 42:148-153, 2014.
Minh-Tri Pham and Tat-Jen Cham. Fast training and selection of haar features using statistics in
boosting-based face detection. In 2007 IEEE 11th International Conference on Computer Vision,
pp. 1-7. IEEE, 2007.
J. Ross Quinlan. Induction of decision trees. Machine learning, 1(1):81-106, 1986.
J Ross Quinlan. C4. 5: programs for machine learning. Elsevier, 2014.
11
Under review as a conference paper at ICLR 2022
Parikshit Ram and Alexander G Gray. Density estimation trees. In Proceedings of the 17th ACM
SIGKDD international conference on Knowledge discovery and data mining, pp. 627-635. ACM,
2011.
Irene Rodriguez-Lujan, Charles Elkan, Carlos Santa Cruz, and Ramon Huerta. Quadratic Program-
ming feature selection. Journal of Machine Learning Research, 2010.
Marco Sandri and Paola Zuccolotto. Variable selection using random forests. In Data analysis,
classification and the forward search, pp. 263-270. Springer, 2006.
Jeffrey C Schlimmer and Douglas Fisher. A case study of incremental concept induction. In AAAI,
volume 86, pp. 496-501, 1986.
Shai Shalev-Shwartz et al. Online learning and online convex optimization. Foundations and trends
in Machine Learning, 4(2):107-194, 2011.
Sanaz Sheikhi and Seyed Morteza Babamir. A predictive framework for load balancing clustered
web servers. The Journal of Supercomputing, 72(2):588-611, 2016.
Sanaz Sheikhi and Seyed Morteza Babamir. Using a recurrent artificial neural network for dy-
namic self-adaptation of cluster-based web-server systems. Applied Intelligence, 48(8):2097-
2111, 2018.
Jiliang Tang, Salem Alelyani, and Huan Liu. Feature selection for classification: A review. Data
classification: Algorithms and applications, pp. 37, 2014.
Pooya Tavallali, Mehran Yazdi, and Mohammad Reza Khosravi. Robust cascaded skin detector
based on adaboost. Multimedia Tools and Applications, 78(2):2599-2620, 2019.
Pooya Tavallali, Peyman Tavallali, Mohammad Reza Khosravi, and Mukesh Singhal. Interpretable
synthetic reduced nearest neighbor: An expectation maximization approach. In 2020 IEEE Inter-
national Conference on Image Processing (ICIP), pp. 1921-1925. IEEE, 2020.
Paul E Utgoff. Incremental induction of decision trees. Machine learning, 4(2):161-186, 1989.
Paul Viola and Michael Jones. Rapid object detection using a boosted cascade of simple features.
In Proceedings of the 2001 IEEE computer society conference on computer vision and pattern
recognition. CVPR 2001, volume 1, pp. I-I. IEEE, 2001.
Haixun Wang, Wei Fan, Philip S Yu, and Jiawei Han. Mining concept-drifting data streams using
ensemble classifiers. In Proceedings of the ninth ACM SIGKDD international conference on
Knowledge discovery and data mining, pp. 226-235. AcM, 2003.
Ian H Witten and Eibe Frank. Data mining: practical machine learning tools and techniques with
java implementations. Acm Sigmod Record, 31(1):76-77, 2002.
Kun Yang and Wing Hung Wong. Density estimation via adaptive partition and discrepancy control.
arXiv preprint arXiv:1404.1425, 2014.
Lei Yu and Huan Liu. Feature selection for high-dimensional data: A fast correlation-based filter
solution. In Proceedings of the 20th international conference on machine learning (ICML-03),
pp. 856-863, 2003.
12
Under review as a conference paper at ICLR 2022
A	Appendix
This is the Appendix (supplemental materials) for Stochastic Induction of Decision Trees with Ap-
plication to Learning Haar Tree.
A.1 Properties of the SDT
Theorem 4 is a result of theorem 20.1 and Lemma 20.1 in Devroye et al. (2013).
Proof sketch of theorem 5:
Proof Sketch: Assuming the data IID. The criterion is bounded by 0 from below. At each node, a
partition that is minimum of the objective function over a set of features is found. As a result, after
partitioning each node, the error will decrease.
A.2 Haar Features
The exact haar features used for generating new features are presented in figure 4. The sizes of row
pixels and column pixels are written right to each filter. For each filter, various filters with every
combination of possible row and column pixels are created and convolved with the image to extract
create new features.
Figure 4: Haar Filters
A.3 Experimental Results
In this section, several experiments that demonstrate properties of SDT are presented.
Figure 5 shows the reduction in impurity (criterion in formula 2) at each iteration of the algorithm
for MNIST dataset. Each point on the curve represents the impurity of the best feature and threshold
found by the algorithm (Lk (Sj )). As figure 5 demonstrate, at each iteration, the loss function has
decreased drastically and eventually has converged to the true best feature and threshold.
Figures 6 and 7 present the importance of features at each iteration for problem of inducing one
single split node for 2 classes. The feature importances were normalized to between 0 and 1.
From figures 6 and 7, it is observable that the SDT gradually discards the less discriminative features
and gives higher value to more distinctive features.
Figures A.3 and A.3 represents computational complexity versus error ratio of train and test. In
figures A.3 and A.3 various values are used for N0 to explore the effect of N0 over the final tree.
13
Under review as a conference paper at ICLR 2022
Figure 5: The horizontal axis shows the iteration number of SDT for a single node. Vertical axis
shows the impurity. The dashed line represents the impurity of the best feature and threshold. The
points on the curves represent exact impurity of the best split found by the algorithm at that iteration.
The text above each plot shows the classes used for creating the split node.
Figure 6: Feature importance of features by SDT at each iteration for problem of0 versus 1. Brighter
pixels show higher value of FI and darker pixels show lower value of FI.
6th Iteration
Figure 7:	Feature importance of features by SDT at each iteration for problem of 1 versus 7. Brighter
pixels show higher value of FI and darker pixels show lower value of FI.
Each curve represents the N0 by its color. The N0 for each color is defined at the legend of the plot.
The depth of the tree is shown at the points on each curve.
14
Under review as a conference paper at ICLR 2022
MNISt (different subset size)
FASHIOl
IST (different subset size)
252015
WOH-M SEII
7
1
2
00O 1
11
m ι
1'22
20
3 30
4 40
■ 50
• 60
—•—70
—•—80
—•—90
・	100
2 200
・ 300
・ 700
・	1000
.60000
SIGNMNIST (different subset size)
80
70
60
30
20
107
108	109
106	107	108	109	1010
1011	106	107	108	109	1010	1011	106
Computational Complexity
Computational Complexity
1010
Computational Complexity
75
S 70
tl
酒65
60
SIGNMNIST(different subset size)
107	108	109
Computational Complexity
106	107	108	109	1010	1011	106
Computational Complexity
107	108	109	1010	1011	106
Computational Complexity
1010
8
Figure 8:	Horizontal axis represents the computational complexity of training a model. The vertical
axis shows the error ratio. The colors represent the used value for N0 .
18
16
14
12
6
4
2
SATIMAGE(different subset size)
4825
32
70
ISOLET(different subset size)
30
28
S 26
=
g 24
2 22
20
18
COVERTYPE(different subset size)
・ 435759
60
50
108	104
105	106	107
Computational Complexity
SATIMAGE(different subset size)
18π16151413
◎-"-&I
106	107
105
Computational Complexity
—•—10
2 20
0
0
5555
107	108
109
二=6。
・ 80
—•—90
・ 100
・ 200
・ 300
・ 700
・ 1000
・ 6238
106	108	1010	106
Computational Complexity
Computational Complexity
COVERTYPE(different subset size)
O 8 6 4 2 O
3 2 Z 2 2 2
KU3s3I
106	108
108	104
Computational Complexity
70
ISOLET(different subset size)
6050403020
KU3s3I
1010	106
107	108
Computational Complexity
109
Figure 9:	Horizontal axis represents the computational complexity of training a model. The vertical
axis shows the error ratio. The colors represent the used value for N0 .
15