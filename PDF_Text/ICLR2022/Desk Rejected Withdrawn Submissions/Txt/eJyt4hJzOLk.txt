Under review as a conference paper at ICLR 2022
Discrepancy-Optimal Meta-Learning
for Domain Generalization
Anonymous authors
Paper under double-blind review
Ab stract
This work attempts to tackle the problem of domain generalization (DG) via learn-
ing to reduce domain shift with an episodic training procedure. In particular, we
measure the domain shift with Y -discrepancy and learn to optimize Y -discrepancy
between the unseen target domain and source domains only using source-domain
samples. Theoretically, we give a PAC-style generalization bound for discrepancy-
optimal meta-learning and further make comparisons with other DG bounds includ-
ing ERM and domain-invariant learning. The theoretical analyses show that there
is a tradeoff between classification performance and computational complexity for
discrepancy-optimal meta-learning. The theoretical results also shed light on a
bilevel optimization algorithm for DG. Empirically, we evaluate the algorithm with
DomainBed and achieves state-of-the-art results on two DG benchmarks.
1	Introduction
Deep learning has achieved highly competitive performance on test data drawn from the same
distribution as large training data. However, in practice there are many circumstances where access to
target data is impossible. Research on domain adaptation (DA) uses unlabeled target data to transfer
labeled source information to a specific target domain (Pan & Yang, 2009; Mansour et al., 2009). In
recent years, domain generalization (DG) has gained increasing attention. Different from DA, DG
aims to solve a more practical and challenging problem, where the target domain is invisible and thus
explicit training on the target is impossible (Blanchard et al., 2011; Muandet et al., 2013).
To tackle the invisibility of target domain for DG, a naive method is empirical risk minimization
(ERM) (Vapnik, 1999), which relays on the diversity of source data to achieve better generalization
performance for unseen target domains (Shankar et al., 2018; Volpi et al., 2018; Gulrajani & Lopez-
Paz, 2020). However, it is almost impossible in practice to acquire training data from enough domains
to achieve promising DG performance. Another route is to better utilize the feature distributions of
source domains, some previous work learn domain-invariant representation across source domains
(Albuquerque et al., 2020; Xiao et al., 2021). As shown in Figure 1 (b), the domain-invariant
representation can regularize the feature space to improve classification performance across source
domains (α-arrows), but the discrepancy between the unseen target domain and source domains still
limits the generalization performance.
In order to effectively utilize the available source data to improve generalization performance, episodic
training process (Finn et al., 2017) is recently applied to DG (Li et al., 2018a; Balaji et al., 2018; Li
et al., 2019b), i.e., randomly extracting a meta-target sample and meta-source samples from available
source data to simulate domain shift in each iteration. This work attempts to train a discrepancy-
optimal meta-learner that gains experience during the episodic training process so as to minimize
the domain discrepancy between any unseen target domain and the available source domains. We
theoretically show the effectiveness of discrepancy-optimal meta-learning for DG with a PAC-style
learning bound and further make comparisons with ERM and domain-invariant learning for DG. The
theoretical analyses mainly show that there is a tradeoff between generalization performance and
computational complexity for the proposed discrepancy-optimal meta-learning framework.
In order to make the theoretical idea into practice, we design an bilevel optimization problem for
discrepancy-optimal meta-learning, where the inner-loop objective is to minimize Y -discrepancy
across meta-source samples while the outer-loop objective is to minimize Y -discrepancy between
meta-target and meta-source samples. The effectivenesses of inner-loop and outer-loop optimization
1
Under review as a conference paper at ICLR 2022
Figure 1: Motivations for discrepancy-optimal meta-learning. In the feature space Rd , T denotes the
range of target domains and S1, S2 denote the ranges of two source domains. (+) and (-) denote
positive examples and negative examples, respectively. A real-world case is shown in Appendix F
is shown as the α-arrows and β-arrows in Figure 1 (c), respectively. A theoretical analysis from a
geometric perspective shows that the collaborative effectiveness of the inner-loop and outer-loop
optimization is to minimize Y -discrepancy between target domain and the convex hull of source
domains. Empirically, we conduct experiments on DomainBed (Gulrajani & Lopez-Paz, 2020) and
evaluate on two DG benchmarks. Results show that our method is highly effective and achieves
state-of-the-art performances. The code will be released at https://anonymous.com.
2	Related Work
Domain Generalization. A promising solution for DG is to learn domain-invariant features across
source domains (Muandet et al., 2013; Deshmukh et al., 2019; Li et al., 2017; 2018b;c; Albuquerque
et al., 2020; Deng et al., 2020). Early work perform kernel-based optimization to learn an invariant
feature mapping to RKHS (Muandet et al., 2013; Deshmukh et al., 2019). Neural-network methods
have achieved promising results in recent years (Li et al., 2017). Li et al. (2018b) employ maximum
mean discrepancy (MMD) constraints via an Auto-Encoder. Recently, adversarial training strategies
have shown highly effectiveness for DG via reducing covariant shift (Li et al., 2018c; Albuquerque
et al., 2020; Deng et al., 2020) or reducing both covariant shift and conditional shift (Li et al., 2018c).
Another method utilizes Variational Bayes for domain-invariant learning (Xiao et al., 2021). Instead
of constructing invariant feature spaces, some methods use data augmentation (Shankar et al., 2018;
Volpi et al., 2018; Qiao et al., 2020; Zhou et al., 2020) to enrich source diversity, which also shows
usefulness for DG. We borrow the idea of extracting domain-invariant feature representations from
the previous work, but our algorithm also optimizes the discrepancy between the expected target
domain and source domains via meta-learning.
Meta-learning. Meta-learning provides a framework to gain learning experiences for future tasks
over multiple training episodes, which often cover a distribution of related tasks (Thrun & Pratt, 1998;
Baxter, 2000). For neural models, gradient-based meta-learning methods (Finn et al., 2017; Grant
et al., 2018; Gordon et al., 2018) are successfully applied to few-shot learning. The episodic training
procedure is also introduced to address DG (Li et al., 2018a; Balaji et al., 2018; Li et al., 2019a;b; Dou
et al., 2019; Du et al., 2020) via simulating domain shift between source domains and unseen target
domains. To alleviate domain shift during episodic training, MLDG (Li et al., 2018a) follows the
update rule of MAML (Finn et al., 2017), minimizing the risk on meta-target data upon the optimized
parameters by meta-source data. A limitation of using the source and target task objectives directly
for inner-loop and outer-loop optimization might be sub-optimal, since it is highly abstracted from
feature representation (Dou et al., 2019). MetaReg (Balaji et al., 2018) adds a classifier’s weights
regularization term to the objective of source task for inner-loop optmization to prouduce a more
general classifier. Li et al. (2019b) improve MetaReg by training a feature-critic network to obtain a
more general feature extractor. Dou et al. (2019) keeps source task risk as the inner-loop objective
but replace the outer-loop objective by global class alignment and local sample clustering objectives
to explictly regularize the feature space. Our method performs episodic training procedure based on
MAML (Finn et al., 2017), but different from the previous work in that we optimize the discrepancy
between target and source domains while the inner objective is to minimize the dicrepancy accorss
source domains. We also show that our method has strong theoretical foundations.
2
Under review as a conference paper at ICLR 2022
3	Preliminaries
3.1	Set-Up
Notations. Let X be the input space and Y = {0, 1} be the output space. Following Blanchard et al.
(2011) and Muandet et al. (2013), we define a domain as a joint distribution PXY (P for brevity) on
X × Y and let PX ×Y (P for brevity) denote the set of all domains. We assume that all domains are
drawn i.i.d. according to P on PX ×Y .
We denote a set of N source domains as S = {Si}1≤i≤N, whose elements are drawn i.i.d. according
to a distribution P. Empirically, there exist training samples S = {Si}1≤i≤N of N source domains.
We assume that the number of source domains N ∈ {1, . . . , Nm} obeys a distribution p(N) on the set
{1, . . . , Nm}, where Nm is the maximum number of source domains. For each Si (i ∈ {1, . . . , N}),
training sample 6 = {(xk, yik)}ι≤k≤m are drawn i.i.d. according to Si. For brevity, We assume all
domains have the same number of training samples, i.e., nT = n1 = . . . = nN = n.
A decision function h ∈ H : X → Y maps the input space into the output space and a loss function
' ◦ h : X× Y → R+ is a real-valued function that maps from a data point (x, y) ∈ Z toa non-negative
real number, e.g., '(h(x), y) = |h(x) - y| (x, y) ∈ Z (Ben-David et al., 2010). The expected error
€D(h) and empirical error ^d(h) on a domain D w.r.t. a decision function h are represented as
follows:
ED(h):= E(χ,y)〜D['(h(x),y)];
≡d(h) := ɪ X '(h(x),y)
|叫(χ,y)∈D
(1)
In deep learning, a neural decision function h = g ◦ f can be regarded as a composition function of
a feature extractor f ∈ F : X → Rd and a classifier g ∈ G : Rd → {0, 1} and the corresponding
expected error and empirical error can be represented as 6心于(g) := €D(g ◦ f) and ^f(D)(g):=
^D(g ◦ f), respectively. We denote Df as a distribution on Rd, where Df (f (x)) = D(χ), X ∈ X.
Learning algorithm & Objective. A learning algorithm A : {ZN×n : N ∈ [Nm]} → H
for DG maps from source data S ∈ ZN×n (N ∈ [Nm]) to a decision function h ∈ H and the
set of all learning algorithms is represented as A(H, ZN×n) or A for brevity. We assume that
{A(S) : A ∈ A, S ∈ ZN×n, N ∈ [Nm]} ⊆ H. Following Maurer & Jaakkola (2005), a nautral
measure for the performance of a given algorithm A for DG is represented as:
ES〜Sn,S〜Pn,N〜P(N) [ET〜PkT(A(S))]]	⑵
For a neural decision function g ◦ f, we denote the partial effect of A on the feature extractor as an
algorithm, Af : {ZN×n : N ∈ [Nm]} → F and similarly, Ag : {ZN×n : N ∈ [Nm]} → G.
A meta-algorithm (or meta-learner) A : {DD} → A(F, ZN×n) maps from a meta-sample DD to an
algorithm A ∈ A(F, ZN×n). A meta-sample DD denotes M pairs of meta-source and meta-target
samples DD = {(Si, T i)}ι≤i≤M, where the size of meta-SamPle TM ≈ Nm ∙ 2Nm ɪ in that we consider
all possible numbers of meta-source domains in the episodic training process. In meta-learning for
DG, a meta-algorithm A learns an algorithm A — A(D) from the meta-sample D, which can learn
such decision function h — A(S) from source-domain data S.
3.2	Domain Discrepancy
We will present Y-discrepancy in this section, which has been widely used in DA (Zhang et al.,
2012). In deep learning, we follow the previous DA work to optimize the feature space to minimize
the domain discrepancy (Ganin et al., 2016; Zhao et al., 2018), thus we also give the definition of
Y-discrepancy based on the learning algorithm for feature extractor Af.
Definition 1. (Y -discrepancy): Let H be a decision function class on X with finite VC-dimension.
The Y -discrepancy discY (S, T) between two domains S and T and its empirical version discY (S, T)
are defined as:
discγ(S, T) := sup leT(h) - 6s(h)∣;	discγ⑤ T) := sup IeT(h) - ^s(h)∣	⑶
h∈H	h∈H
3
Under review as a conference paper at ICLR 2022
In neural models, Y -discrepancy between domains is computed in the feature space, i.e., output space
of the feature extractor Af (S), with the classification function g ∈ G,
discγ (Af(S)(S, T)) =SUp eτ(go Af(S)) -es(g。Af(S));
g∈G
discγ(Af(S)(M T)) =SUp ^^(g ◦ Af(S)) - ^s(g ◦ Af(S))
g∈G
(4)
It is clear that Y -discrepancy defines a pseudo distance between a pair of domains in that it satisfies
symmetry and the triangle inequality but not satisfies identity of indiscernibles since diScY (S, T) =
0 ; S = T.
Y-discrepancy can measure not only covariant shift between domains, but also conditional shift
between domains (Zhang et al., 2012). Therefore, we choose Y-discrepancy as a measurement for
domain discrepancy in following derivations of DG theory.
4	Learning Guarantees
In this section, we first give a PAC-style learning bound for DG under the discrepancy-optimal
meta-learning framework and then make comparisons with ERM and domain-invariant learning for
DG.
4.1	Discrepancy-based DG Bounds
We will bound the measure of DG performance w.r.t. an algorithm as defined in Eq. (2), under the
discrepancy-optimal meta-learning framework given a meta-sample D = {(Si, Ti)}1≤i≤M.
Lemma 1. (Discrepancy-based DG Bound) Given any ξ, ξ0 > 0, for any n ≥ 8ξ22 and any
M ≥ (IB2, we have for any δ > 0, with probability at least 1 一 2δ,
11
ES〜Sn,S〜PN,N〜P(N) [ET〜PM(A(S))]] 一 M 工 Γ^7 工 ^^k (A(Sk))
kS∈d^ ISk 1 i：Sk∈Sk	(5)
≤ES 〜Sn,S〜Pn,N 〜P(N )ET 〜P[discγ (Af(S)(S, T))] +Ξ(ξ, δ, S) + Π(ξ0, δ, D) + Bδ
1
where S = N ^2isi∈^ S and the complexity terms corresponding to the data size are Ξ(ξ, δ, S) H
O(1∕√n) and Π(ξ0, δ, DD) H O(1∕√M). Proof can befound in Appendix B.1.
Lemma 1 bounds the difference between algorithmic performance of DG and empirical classification
error of meta-source samples with the expectation of Y -discrepancy between target and source
domains over random meta-samples. Next, we will bound the expected Y-discrepancy over random
meta-samples in the following lemma.
Lemma 2. (Meta-OPtimizing Y-discrepancy (Eq. (5))) Given any ε, ε0 > 0 ,for any n ≥ 8B2 and
any M ≥ (B)2, we havefor any δ > 0, with probability at least 1 — 3δ,
1M
ES 〜Sn,S〜Pn,N 〜P(N )ET 〜P[discγ (A(D)(S)(S,T))] 一 M NdiScY(A(D)(Si)(Si,Ti))∣
≤Ξ(ε, δ, S) + Ψ(ε0, δ, D) + 2Bδ
(6)
where the complexity terms corresponding to data size are Ξ(ε, δ, S) H O(1∕√n) and Ψ(ε0, δ, D) H
O(1∕√M). Proof can befound in Appendix B.2.
Finally, we combine the above two lemmas and obtain the generalization bound for DG via
discrepancy-optimal meta-learning in the following theorem.
4
Under review as a conference paper at ICLR 2022
Table 1: Comparison with related DG and DA learning bounds.
	Objective (LHS)	Upper bound (RHS)		
		Empirical Term	Complexity Term	Other Term
Ours	ElS-Sn,S-PN,N-p(N) [ET〜P [eT(A(S))]]	Classification error emp. Y -discrepancy	0(1∕√n) + o(1∕√m )	O(δ)
ERM (DG) B.4	ElS-Sn,S-PN,N-p(N) [ET〜P [eT(A(S))]]	Classification error	0(1∕√n) + 0(1∕√‰)	O(δ)
Domain-invariant (DA) B.5	^ £T(A(S))人	Classification error	0(1∕√n)	-
Domain-invariant (DG) B.6	ET〜P M(A(S))]	Classification error	0(1∕√n)	γ
where M ≈ Nm ∙ 2Nm-1 in episodic training and Y := ET 〜P [ discγ (Af (S)(TA ⑹，ɪ))], TA(S) = arg minτ∈conv(s) discγ (Af (S)(T, T))
Theorem 1. (DG Bound based on Discrepancy-optimal Meta-learning) Given any ε, ε0 , ε00 > 0,
for any n ≥ 8B- and any M ≥ max {(87)2-,(8B)2 }, we have for any δ > 0, with probability at least
1 - 5δ,
11
ES〜Sn,S〜PN,N〜P(N)[ET〜PM(A(S))]] - M T 7^7	^^k (A(Sk))
k：Sk ∈D "k∣ i：Sk ∈Sk	/ 、
k	(7)
1M
≤M X discγ (A(D)(Si)(&, Ti)) + Ξ(ε, δ, S) + Π(ε0, δ, D) + Ψ(ε00, δ, D) + 3Bδ
1	.1	1 ∙. .	τ . 7 .	∙	i-' / r 6∖	z∏ /1 I I ∖ i i / / r ∙-r∖∖
where the complexity terms corresponding to data Size are Ξ(ε, δ, S) H O(1∕√n), Π(ε0, δ, D) H
O(1∕√M), Ψ(ε00, δ, D) H O(1∕√M). Proof can befound in Appendix B.3.
Theorem 1 shows that when applying the discrepancy-optimal meta-learning on a meta-sample
D = {(Si , Ti )}1≤i≤M, the difference between algorithmic performance of DG and empirical
classification error of meta-source samples can be bounded mainly by the empirical Y-discrepancy
between meta-source and meta-target samples as well as the complexity term O(1/√n) + O(1/√M).
4.2 Comparison with Related Bounds
We first compare with the learning bound of ERM for DG and explain the tradeoff between classi-
fication performance and computational complexity under the discrepancy-optimal meta-learning
framework for DG. We also compare with domain-invariant learning for DA (Zhang et al., 2012) &
DG (Albuquerque et al., 2020) and analyze the motivation of meta-learning strategy for optimizing
the domain discrepancy for DG. The theoretical results are listed in Table 1.
Comparison with ERM. As shown in Table 1, the complexity term of ERM bound for DG relates to
the number of training examples in each domain O(1/√n) and the maximum number of source do-
mains O(1/√Nm). We can find that the complexity term of the proposed discrepancy-optimal meta-
learning is better than the complexity terms of ERM since O(1∕√M) ≈ O(1∕pNm ∙ 2Nm-I)《
O(1/√Nm). This means that the discrepancy-optimal meta-learning bound for DG has the potential
to be better than the ERM when the empirical discrepancy term can be sufficiently minimized. Intu-
itively, this shows that meta-learning with episodic training process can utilize the source domains to
optimize the domain discrepancy for DG, and there is a tradeoff between the performance and the
computational complexity. We show the empirical results of the tradeoff in Appendix G.
Comparison with domain-invariant learning. We can find that DA and DG based on domain-
invariant learning are tend to minimize Y-discrepancy between target and source domains and
Y-discrepancy across source domains, respectively. In particular, the DG bound via domain-invariant
learning consists of an approximating term γ, which is defines as the smallest expected Y-discrepancy
between any random target domain and the convex hull of source domains. However, γ can not
be optimized in domain-invariant learning, thus it is unable to be estimated the tightness of the
domain-invariant bound for DG and the induced algorithm may not has sufficient capacity to reduce
the domain shift for DG. While the proposed discrepancy-optimal meta-learning for DG optimizes the
Y-discrepancy between target and source domains directly with a meta-sample and the corresponding
bound can be optimized. Besides, Corollary 1 will show that a bilevel optimization algorithm under
the discrepancy-optimal meta-learning framework can effectively optimize the term γ.
5
Under review as a conference paper at ICLR 2022
5 Algorithm
We will present a meta-learning algorithm via bilevel optimization to utilize the meta-sample for
optimizing Y -discrepancy between target and source domains in Section 5.1 and then show a practical
meta-learning procedure in Section 5.2.
5.1 Learning to Minimize Domain Discrepancy
Given a meta-sample DD =
{(Dtir, Dite)}1≤i≤M, the empirical term in
Theorem 1 guides us to design an algorithm
that takes the meta-source samples Dtir ⊂ S
as input and outputs a feature extractor
(hypothesis) that minimizes Y -discrepancy
between the meta-target sample Dite ∈ S
and meta-source samples Dtir . To this end,
the meta-training and meta-test problems
are non-symmetric in the episodic training
process (Finn et al., 2017). It is a natural
idea to use bilevel optimization to tackle
such a non-symmetric optimization problem.
In particular, we specify the meta-learner
A in Theorem 1 to optimize the initialized
parameters ψ0 so that an algorithm ini-
tialized with ψ0 minimizes Y-discrepancy
across source domains (Eq. (10)) can also
minimize Y-discrepancy between target and
source domains (Eq. (9)). Corollary 1 shows
the affect of collaborative effectiveness
of inner-loop and outer-loop optimization
in bilevel optimization from a geometry
perspective.
Figure 2: Geometric understanding. PX denotes
the space of domains (distributions) on input space
and (M(PA(D)(S)(X)), discγ(∙, ∙)) denotes apseudo-
metric space of domains (distributions) on feature
space.
Definition 2. (Bilevel Optimization) We denote the the outer-loop and inner-loop objectives w.r.t.
the feature extractor fψ as Lout and Lin, respectively. ψ and ψ0 denote the parameters of feature ex-
tractor in inner-loop optimization and outer-loop optimization, respectively. The bilevel optimization
problem is defined as:
X
LoutM,ψθ“Dte, DZr )),
er
i∈[M]
* 一	♦ i it I i-∖i X
subject to ψi ∈ arg min Lin(ψ,ψo; Dir)
ψ∈Φ(ψ0)
(8)
where Φ(∙) denotes a parameter constraint brought by parameter initialization. The outer-loop and
inner-loop objectives are defined as follows:
Lout(ψ*,ψo3te,Dir)):= E αidiscγ (fψ. (Dte),fψ. (Sk))	(9)
Sk ∈D^ir
Lin(ψ,ψo; Dir):= X αi X discγ(fψ理ψ (*,fψ眄ψ (+))	(10)
Sk ∈D^tr	^t∈D^ir,k=t
where I÷it denotes parameter initialization. Definition 2 specifies the meta-algorithm A(∙) as an
outer-loop optimization problem, A : D 7→ arg minψ0 i Lout(Dite, Dtir), which optimizes the
initialized parameters of feature extractor. The output algorithm of the meta-algorithm Aψ* (∙),
with the optimized ψ0* := arg minψ0 i Lout(Dite, Dtir), is specified accordingly as the inner-
loop optimization problem, Aψ* : Dtr → arg minψ∈φ(ψ*) Lin(Dtr), which optimizes the feature
extractor using the initialized parameters ψ0*.
Corollary 1. (Geometric Understanding of the Bilevel Optimization) Given the meta-sample D
and the source samples S, we consider a pseudo-metric space (M(PA(D)(S)(X)), discγ(∙, ∙)),
6
Under review as a conference paper at ICLR 2022
Algorithm 1 Meta-training Procedure.
Input data: Nm source-domain training samples, hyperparameters: η, β
Parameters: Feature extractor Fψ , task classifier Tθ
Output: Meta-trained model parameters {ψtr , θtr}
1:	while Stopping condition is not met do
2:	Sample minibatch of meta-target sample Dte and minibatch of meta-source samples Dtr
3:	Evaluate inner-loop objective, Lin - Egk StEDt discγ(Fψ(Sk, St))
4:	Update the feature extractor With gradient descent, ψ0 — ψ 一 ηVψ Lin
0k
5:	Evaluate outer-loop objective w.r.t. the updated ψ , Lout — ESk∈d±T discγ (Fψo (Sk, Dte))
6:	Update the feature extractor w.r.t. the initial parameters ψ, ψ — ψ 一 ηβVψ Lout
7:	Evaluate task objective, Ltask J ^^tT (Tθ ◦ Fψ)
8:	Update the task classifier and feature extractor [θ, ψ] — [θ, ψ] — ηVθ,ψLtask
9:	end while
Algorithm 2 Meta-test Procedure.
Input data: Nm source-domain training samples, hyperparameters: η, β
Output: Feature extractor Fψ , task classifier Tθ
1:	Initialize with the meta-trained parameters, ψ J ψtr , θ J θtr
2:	while Stopping condition is not met do
3:	Sample minibatch of all source-domain samples 力
kt
4:	Evaluate inner-loop objective, Lin J ESk StED discγ (Fψ (Sk, St))
5:	Update the feature extractor ψ J ψ — ηβVψ Lin
6:	Evaluate task objective, Ltask J WDtT (Tθ ◦ Fψ)
7:	Update the task classifier and feature extractor [θ, ψ] J [θ, ψ] — ηVθ,ψLtask
8:	end while
TCT	. ι	. r ι ∙ rw	∙ .ι r .	A ∕∙-r∖∖ ∕f∖∕c 八	∙ ι
defined as the Set of domains PA(D)(S)(X) in the feature space A(D)(S)(X) equipped
with a pseudo-metric discγ(∙, ∙) : Rd X Rd → R+.	Where the distance be-
tween a target domain T and the convex hull of source domains conv(S) is defined
as discY (A(D)(S)(T, ConV(S))) = discY (A(D)(S)(TA(D^)(S), T)), where TA(D)(S)=
TA(D)(S) = argminτ∈conv(s) discγ (A(D)(S))(T, T)) denotes the nearest point to the target do-
main in conV(S). Then we have:
discγ (A(D)(S)(T,conv(S))) = discγ (A(D)(S)(TA(D心),T))
1
≤面
2⑻
X2
discγ(A(D)(S)(T,S* i)) + 商 EdisCY(A(D)(S)S,Sj)) +Ξ(ε,δ,S)
i∙∙S*∈S	⑸ i<j
------{-------} '------{z~:----}
Outer-loop Objective (Lout)	Inner-loop Objective (Lin)
(11)
1	.1	1 ∙. .	τ∙ . 7 . ∙	∙ I—If	C A∖ 〃■» /1 / /-∖ n 「	1 r ι •
where the complexity term according to data Size is Ξ(ξ, δ, S) H O(1∕√n). Proofcan be found in
Appendix B.7.
As shown in Figure 2, The triangle relationship in the pseudo-metric feature distribution space (in the
red dotted line) among the target domain TA(D)(S), the combination of source domains SA(D)(S) and
一	. 一	一 ..一	一 一一一	一 . ^ɪ*	一
the nearest point to the target domain in the convex hull of source domains TA(D)(S) demonstrates
that the collaborative effectiveness of inner-loop and outer-loop optimization is to minimize the
Y-discrepancy between the target domain and the convex hull of source domains.
5.2 Meta-learning Procedure
Based on the meta-target/meta-source splits in episodic training process, we use first-order approx-
imation for gradient descent to update the meta-parameters in each iteration. The procedures of
meta-training and meta-test are briefly shown in Algorithm 1 and Algorithm 2, respectively. More
complete procedures with the adversarial training strategy to estimate Y-discrepancy are shown in
Appendix C.
7
Under review as a conference paper at ICLR 2022
Table 2: Accuracy (%) on PACS and DomainNet using pretrained ResNet-50 backbone. * indicates
statistical significance compared to other methods with low FPR (α < 0.1) by the Mann-Whitney
test (MCKnight & Nqjab, 2010).
Method
PACS
DomainNet
paintings cartoon photo sketch Avg. clipart infograph painting qickdraw real
sketch Avg.
Erm	84.7±0.4	80.8±0.6	97.2±0.3 79.3±1.0 85.5	58.1±0.3	18.8±0.3 46.7±0.3	12.2±0.4 59.6±0.1	49.8±0.4	40.9
Coral	88.3±0.2	80.0±0.5	97.5±0.3 78.8±1.3 86.2	59.2±0.1	19.7±0.2 46.6±0.3	13.4±0.4 59.8±0.2	50.1±0.6	41.5
Mldg	85.5±1.4	80.1±1.7	97.4±0.3 76.6±1.1 84.9	59.1±0.2	19.1±0.3 45.8±0.7	13.4±0.3 59.6±0.2	50.2±0.4	41.2
SagNet	87.4±1.0	80.7±0.6	97.1±0.1 80.0±0.4 86.3	57.7±0.3	19.0±0.2 45.3±0.3	12.7 ±0.5 58.1±0.5	48.8±0.2	40.3
Ours	88.2±0.8	81.7±1.2	97.8±0.3 79.6±0.7 86.8*	60.7±o,8	23.3±0.5 51.2±0.2	14.7±0.4 63.6±0.4	51.8±0.8	44.2*
Meta-training. As shown in Algorithm 1, in each iteration, one task is to update the feature extractor
w.r.t. the outer-loop objective computed by the updated parameters w.r.t. the inner-loop optimization
(line 3-6). The other task is to update the task classifier and feature extractor w.r.t. the classification
objective (line 7-8).
Meta-test. As shown in Algorithm 2, in each iteration, one task is to update the feature extractor w.r.t.
the inner-loop objective (line 4-5), the other task is to update the task classifier and feature extractor
w.r.t. the classification objective (line 6-7).
6	Experiments
6.1	Experimental Settings
Datasets. The dataset PACS (Li et al., 2017) includes images of seven categories and four domains,
including photo, art paintings, cartoon and sketches. We use the recommended training and validation
split ratio of 90%/10% (Li et al., 2017), and use the overall validation set aggregated by the validation
sets of each training domain for model selection (Gulrajani & Lopez-Paz, 2020). The dataset
DomainNet (Peng et al., 2019) consists of 345 categories and six distinct domains, including sketch,
real, quickdraw, painting, infograph and clipart. We use the same training/validation split ratio of
70%/30% as previous work Peng et al. (2019), and use the same way as the PACS benchmark for
model selection.
Training details. We build our model and conduct experiments based on DomainBed (Gulrajani
& Lopez-Paz, 2020), a testbed for DG. In particular, we use an ImageNet pretrained ResNet-50
(Gulrajani & Lopez-Paz, 2020) as the feature extractor for all experiments on the two benchmarks. We
use Adam (Kingma & Ba, 2014) for both inner-loop and outer-loop optimization with five gradient
steps in the inner-loop optimization during the meta-training period, and 15 gradient steps during the
test period. The choices of number of gradient steps are based on performance on the evaluation sets.
For hyperparameter search, each hyperparameter is assigned a default value and tuned via random
search (Bergstra & Bengio, 2012) over a range near the default value. All hyperparameters were
tuned jointly according to their respective random search distributions with a maximum number of
20 trials. Appendix D lists the details of hyperparameter search.
6.2	Results
Table 2 and 3 show the results on two benchmarks. Each reported result is the average of three
independent repetitions of the entire study with different hyperparameters, different network parameter
initialization and different dataset splits.
State-of-the-art comparison. We list the results of ERM and other methods that are better than
ERM on either PACS and DomainNet in Table 2. Our method outperforms ERM on both two datasets,
which is in consistent with the theoretical analysis that discrepancy-optimal meta-learning has the
potential to be better than ERM when Y -discrepancy can be promisingly optimized. CORAL (Sun
& Saenko, 2016) learn domain-invariant features across source domains via optimizing mean and
covariance of distributions. Compared with this method, the strength of our meta-learning method
is the to ability to optimize Y-discrepancy between unseen target and source domains, which is
8
Under review as a conference paper at ICLR 2022
Table 3: Ablation study on PACS. * indicates statistical significance compared to other baselines with
low FPR (α < 0.1) by the Mann-Whitney test (MCKnight & Najab, 2010).
Training Objectives		PACS				
Inner Objective	Outer Objective	paintings	cartoon	photo	sketch	Avg.
Disc	None	87.1±o.7	77.3±0.8	96.3±0.5	74.2±1.3	83.7
Task	Task	84.8±0.8	80.3±1.4	96.7±0.4	77.1±1.2	84.7
Task	Disc	86.1±0.7	80.7±1.5	97.7±0.1	77.4±0.7	85.5
Disc+Task	Task	87.6±1.2	81.2±1.0	97.1±0.2	77.7±0.8	85.9
Disc	Disc	88.2±0.8	81.7±1.2	97.8±0.3	79.6±0.7	86.8*
also in consistent with the theoretical analysis. A state-of-the-art meta-learning method for DG
Mldg (Li et al., 2018a) directly uses classification objectives of target and source domains as the
inner-loop and outer-loop objectives in bilevel optimization. Our method focus on optimizing the
domain discrepancy to reduce domain shift, which has reasoning theoretical guarantees and achieves
better results. Besides, our method also outperforms a recent state-of-the-art method SagNet (Nam
et al., 2019), which uses style-agnostic networks to reduce intrinsic style bias of CNN.
Ablation study. As shown in Table 3, we compare with a range of variations of choosing the
inner-loop or outer-loop objectives between classification objective and Y -discrepancy. Compared
with the first line of results, which is equivalent to domain-invariant learning that only optimizes
Y -discrepancy between source-domain samples, all of the other meta-learning methods achieve
improvements, which shows the effectiveness of meta-learning with episodic training process for DG.
In addition, compared with other meta-learning methods, our method achieves the best results. This
shows the potential of optimizing domain discrepancy to reduce domains shift for DG.
Domain discrepancy. As
shown in Figure 3, we compare
Y -discrepancy (Zhang et al.,
2012) between our method
and other baselines on PACS.
Figure 3 (a) shows that both our
method and domain-invariant
learning can better reduce
Y -discrepancy between source
domains compared with ERM.
This is because these two meth-
(a) Fdiscepancy between sources.	(a) Fdiscepancy between target and sources.
ods have a training objective
to reduce Y-discrepancy across	Figure 3: Y-discrepancy on PACS.
source-domain samples. In
addition, Figure 3 (b) shows that Y -discrepancy between the unseen target and source domains of our
method is much lower than both ERM and domain-invariant learning, which shows the effectiveness
of our discrepancy-optimal meta-learning to reduce domain shift when faced with unseen target
domains.
7	Conclusion
This work investigates discrepancy-optimal meta-learning for DG from both theoretical and empirical
perspectives. The theoretical analysis shows that training a meta-learner to optimize Y-discrepancy
between unseen target and source domains is effective for DG. This guides a meta-learning algo-
rithm with episodic training process via bilevel optimization, where the inner-loop objective is to
minimize Y-discrepancy across source-domain samples and the outer-loop objective is to minimize
Y -discrepancy between ,eta-target and meta-source samples. Empirically, our method achieves
state-of-the-art results on two DG benchmarks.
9
Under review as a conference paper at ICLR 2022
References
Isabela Albuquerque, Joao Monteiro, Mohammad Darvishi, Tiago H Falk, and Ioannis Mitliagkas.
Generalizing to unseen domains via distribution matching. arXiv preprint arXiv:1911.00804, 2020.
Yogesh Balaji, Swami Sankaranarayanan, and Rama Chellappa. Metareg: Towards domain gen-
eralization using meta-regularization. Advances in Neural Information Processing Systems, 31:
998-1008,2018.
Jonathan Baxter. A model of inductive bias learning. Journal of artificial intelligence research, 12:
149-198, 2000.
Shai Ben-David, John Blitzer, Koby Crammer, Alex Kulesza, Fernando Pereira, and Jennifer Wortman
Vaughan. A theory of learning from different domains. Machine Learning, 79(1-2):151-175, 2010.
James Bergstra and Yoshua Bengio. Random search for hyper-parameter optimization. Journal of
machine learning research, 13(2), 2012.
Gilles Blanchard, Gyemin Lee, and Clayton Scott. Generalizing from several related classification
tasks to a new unlabeled sample. Advances in Neural Information Processing Systems, 24:2178-
2186, 2011.
Gilles Blanchard, Aniket Anand Deshmukh, Urun Dogan, Gyemin Lee, and Clayton Scott. Domain
generalization by marginal transfer learning. Journal of Machine Learning Research, 22(2):1-55,
2021.
Zhun Deng, Frances Ding, Cynthia Dwork, Rachel Hong, Giovanni Parmigiani, Prasad Patil, and
Pragya Sur. Representation via representations: Domain generalization via adversarially learned
invariant representations. arXiv preprint arXiv:2006.11478, 2020.
Aniket Anand Deshmukh, Yunwen Lei, Srinagesh Sharma, Urun Dogan, James W Cutler, and
Clayton Scott. A generalization error bound for multi-class domain generalization. arXiv preprint
arXiv:1905.10392, 2019.
Qi Dou, Daniel C Castro, Konstantinos Kamnitsas, and Ben Glocker. Domain generalization via
model-agnostic learning of semantic features. arXiv preprint arXiv:1910.13580, 2019.
Yingjun Du, Jun Xu, Huan Xiong, Qiang Qiu, Xiantong Zhen, Cees GM Snoek, and Ling Shao.
Learning to learn with variational information bottleneck for domain generalization. In European
Conference on Computer Vision, pp. 200-216. Springer, 2020.
Chelsea Finn, Pieter Abbeel, and Sergey Levine. Model-agnostic meta-learning for fast adaptation of
deep networks. In International Conference on Machine Learning, pp. 1126-1135. PMLR, 2017.
Yaroslav Ganin, Evgeniya Ustinova, Hana Ajakan, Pascal Germain, Hugo Larochelle, Frangois
Laviolette, Mario Marchand, and Victor Lempitsky. Domain-adversarial training of neural networks.
The Journal of Machine Learning Research, 17(1):2096-2030, 2016.
Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair,
Aaron Courville, and Yoshua Bengio. Generative adversarial nets. Advances in Neural Information
Processing Systems, 27:2672-2680, 2014.
Jonathan Gordon, John Bronskill, Matthias Bauer, Sebastian Nowozin, and Richard E Turner. Meta-
learning probabilistic inference for prediction. arXiv preprint arXiv:1805.09921, 2018.
Erin Grant, Chelsea Finn, Sergey Levine, Trevor Darrell, and Thomas Griffiths. Recasting gradient-
based meta-learning as hierarchical bayes. arXiv preprint arXiv:1801.08930, 2018.
Arthur Gretton, Karsten M Borgwardt, Malte J Rasch, Bernhard Scholkopf, and Alexander Smola. A
kernel two-sample test. The Journal of Machine Learning Research, 13(1):723-773, 2012.
Ishaan Gulrajani and David Lopez-Paz. In search of lost domain generalization. arXiv preprint
arXiv:2007.01434, 2020.
10
Under review as a conference paper at ICLR 2022
Zeyi Huang, Haohan Wang, Eric P Xing, and Dong Huang. Self-challenging improves cross-domain
generalization. arXiv preprint arXiv:2007.02454, 2, 2020.
Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint
arXiv:1412.6980, 2014.
Da Li, Yongxin Yang, Yi-Zhe Song, and Timothy M Hospedales. Deeper, broader and artier domain
generalization. In Proceedings of the IEEE International Conference on Computer Vision, pp.
5542-5550, 2017.
Da Li, Yongxin Yang, Yi-Zhe Song, and Timothy Hospedales. Learning to generalize: Meta-learning
for domain generalization. In Proceedings of the AAAI Conference on Artificial Intelligence,
volume 32, 2018a.
Da Li, Jianshu Zhang, Yongxin Yang, Cong Liu, Yi-Zhe Song, and Timothy M Hospedales. Episodic
training for domain generalization. In Proceedings of the IEEE International Conference on
Computer Vision, pp. 1446-1455, 2019a.
Haoliang Li, Sinno Jialin Pan, Shiqi Wang, and Alex C Kot. Domain generalization with adver-
sarial feature learning. In Proceedings of the IEEE Conference on Computer Vision and Pattern
Recognition, pp. 5400-5409, 2018b.
Ya Li, Xinmei Tian, Mingming Gong, Yajing Liu, Tongliang Liu, Kun Zhang, and Dacheng Tao.
Deep domain generalization via conditional invariant adversarial networks. In Proceedings of the
European Conference on Computer Vision, pp. 624-639, 2018c.
Yiying Li, Yongxin Yang, Wei Zhou, and Timothy Hospedales. Feature-critic networks for heteroge-
neous domain generalization. In International Conference on Machine Learning, pp. 3915-3924.
PMLR, 2019b.
Laurens van der Maaten and Geoffrey Hinton. Visualizing data using t-sne. Journal of machine
learning research, 9:2579-2605, 2008.
Yishay Mansour, Mehryar Mohri, and Afshin Rostamizadeh. Domain adaptation: Learning bounds
and algorithms. arXiv preprint arXiv:0902.3430, 2009.
Andreas Maurer and Tommi Jaakkola. Algorithmic stability and meta-learning. Journal of Machine
Learning Research, 6(6), 2005.
Patrick E McKnight and Julius Najab. Mann-whitney u test. The Corsini encyclopedia of psychology,
pp. 1-1, 2010.
Shahar Mendelson. A few notes on statistical learning theory. In Advanced lectures on machine
learning, pp. 1-40. Springer, 2003.
Krikamol MUandeL David Balduzzi, and Bernhard Scholkopf. Domain generalization via invariant
feature representation. In International Conference on Machine Learning, pp. 10-18, 2013.
Hyeonseob Nam, HyunJae Lee, Jongchan Park, Wonjun Yoon, and Donggeun Yoo. Reducing domain
gap via style-agnostic networks. arXiv preprint arXiv:1910.11645, 2019.
Sinno Jialin Pan and Qiang Yang. A survey on transfer learning. IEEE Transactions on Knowledge
and Data Engineering, 22(10):1345-1359, 2009.
Xingchao Peng, Qinxun Bai, Xide Xia, Zijun Huang, Kate Saenko, and Bo Wang. Moment matching
for multi-source domain adaptation. In Proceedings of the IEEE/CVF International Conference on
Computer Vision, pp. 1406-1415, 2019.
Fengchun Qiao, Long Zhao, and Xi Peng. Learning to learn single domain generalization. In
Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp.
12556-12565, 2020.
Shiori Sagawa, Pang Wei Koh, Tatsunori B Hashimoto, and Percy Liang. Distributionally robust
neural networks for group shifts: On the importance of regularization for worst-case generalization.
arXiv preprint arXiv:1911.08731, 2019.
11
Under review as a conference paper at ICLR 2022
Shiv Shankar, Vihari Piratla, Soumen Chakrabarti, Siddhartha Chaudhuri, Preethi Jyothi, and Sunita
Sarawagi. Generalizing across domains via cross-gradient training. In International Conference
on Learning Representations, 2018.
Baochen Sun and Kate Saenko. Deep coral: Correlation alignment for deep domain adaptation. In
European conference on computer vision, pp. 443-450. Springer, 2016.
Sebastian Thrun and Lorien Pratt. Learning to learn: Introduction and overview. In Learning to learn,
pp. 3-17. Springer, 1998.
Vladimir N Vapnik. An overview of statistical learning theory. IEEE transactions on neural networks,
10(5):988-999, 1999.
Riccardo Volpi, Hongseok Namkoong, Ozan Sener, John C Duchi, Vittorio Murino, and Silvio
Savarese. Generalizing to unseen domains via adversarial data augmentation. In Advances in
Neural Information Processing Systems, pp. 5334-5344, 2018.
Yufei Wang, Haoliang Li, and Alex C Kot. Heterogeneous domain generalization via domain mixup.
In ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing
(ICASSP), pp. 3622-3626. IEEE, 2020.
Zehao Xiao, Jiayi Shen, Xiantong Zhen, Ling Shao, and Cees GM Snoek. A bit more bayesian:
Domain-invariant learning with uncertainty. arXiv preprint arXiv:2105.04030, 2021.
Chao Zhang, Lei Zhang, and Jieping Ye. Generalization bounds for domain adaptation. Advances in
neural information processing systems, 4:3320, 2012.
Han Zhao, Shanghang Zhang, Guanhang Wu, Jose MF Moura, Joao P Costeira, and Geoffrey J Gordon.
Adversarial multiple source domain adaptation. Advances in Neural Information Processing
Systems, 31:8559-8570, 2018.
Kaiyang Zhou, Yongxin Yang, Timothy Hospedales, and Tao Xiang. Learning to generate novel
domains for domain generalization. In European Conference on Computer Vision, pp. 561-578.
Springer, 2020.
12
Under review as a conference paper at ICLR 2022
A Technical Tools
We first present the definition of uniform entropy numbers (Mendelson, 2003; Maurer & Jaakkola,
2005) and PAC-style generalization bound based on uniform entropy numbers (Mendelson, 2003),
which are the preliminaries for the following proofs.
Definition 3. (Uniform Entropy Numbers) (Mendelson, 2003; Maurer & Jaakkola, 2005): Let
F be a function class and Zn = {zi}1≤i≤n, (zi ∈ Z) be a set of examples with the size of n. For
any ε > 0, the covering number of F at radius ε w.r.t. the metric d is denoted as N(ε, F, d). We set
the metric d as the '1 -norm 'ι(Zn) corresponding to Zn. The uniform entropy number of F w.r.t.
`1 (Zn ) is defined as:
ln N∖(ε, F ,'1)= SuP SuP ln N (ε, F, '1 (Zn))
n∈N+ Zn∈Zn
where the '1 -norm 'ι(Zn) w.r.t. twofunctions f and g is defined as n Pn=ι If (Zi) 一 g(zi)∣, (Zi ∈
Zn).
Proposition 1. (Generalization Bound based on Uniform Entropy Numbers) (Mendelson, 2003):
Let F be a class of functions bounded by B, and Zn ∈ ZN×n, (N ≥ 1) be a set of examples with the
size of n drawn i.i.d. from a combination of N distributions P = {Pi}1≤i≤N. For every ε > 0 and
any n ≥ 8B,
Pr SuP
f∈F
Nn	N
n X n Xf (zi,j)- n X EPi [f ]
> ε} ≤ 8E[N(ε, F,'1(Zn))] exp (- INB)
Set the RHS to delta and by the definition of uniform entropy numbers, then for any δ > 0, with
probability at least 1 一 δ,
SuP
f∈F
=O
1N1n	1N
n X n Xf(Zij) - n X EPi [f] ≤
128B2
ln NI(8, F ,'1) 一 ln 8 γ
nN
ln NI ( 8, F,'I)- ln δ
nN
≤O
ln NI(ε, F, '1)- ln 8
n
Proof.
Step 1: (Symmetrization) Let Us denote EP [f] := N PN=I Ep/f] and Ezn[f]:=
N PN=I n Pn=I f(Zij). For any ε > 0, such that n ≥ 8Br, Let f be the function achieving
the sUpremUm w.r.t. the samples Zn,
IL=  U . , IL=.U - ,	, = IL= - q  ,	,= - q 一 ,	, ≤ H,U . U 」,	,
JLIEP [f]-EZn [f]∣>ε JLIEP[f]-^zn [f]∣<ε∕2 JLIEP[f]-EZn [f]∣>ε∧∣Ep [f]-Ezn [f]∣<ε∕2 ≤ JL∣^zn [f]-^Zn [f]∣>ε∕2
Taking expectation w.r.t. the second sample gives:
1IEp[f]-Ezn[f]∣>ε pr0{∣EP[f] 一 EZn[f]∣ < ε∕2} ≤ p1∣Ezn团 一 EZnf]∣ > ε∕2}
By Chebyshev’s inequality, since Z0n are n i.i.d. samples drawn from the multiple distributions
P = {Pi}1≤i≤N, we have:
N
n
N
Pr0 {IEP[f]- EZn [f]| ≥ ε∕2} = Pr
N
EZ〜P n X[f (Zi)] —
i=1
1n 1N
n X n Xf(Zij) ≥ε/2
j =1 i=1
≤当 Var (ɪ X[f (Zi)]) ≤ 4B2
nε2	N	nε2
i=1
Then, we have:
(	4B2
1∣Ep [力一位Zn f]∣>ε C - nε2
≤ Pr0 {|EZn[f] - EZn[f]∣ >ε∕2}
13
Under review as a conference paper at ICLR 2022
Taking expectation w.r.t. the first sample, if n ≥ 8B2, We have:
Pr{SUP∣Ep [f] - E Zn[f]∣ >ε} ≤ 2Pr∣sup∣E Zn [f] - E z“[f]| >2}	(12)
Step 2:
Let ε ∈ {-1, 1}N×n} are independent Rademacher random variables, Then,
PrbupIEZn [f ] - E Znf]∣ > ε} = PrPrε I sup X εi,j (f (Zij)- f(zi,j )
≤ 2Pr Prε sup ∣∣ X εi,j f (zi,j )
f∈F ∣ i,j
For a realization of Zn, set N (8, F, 'ι(Zn)) to bean 8 cover of F w.r.t. the 'ι(Zn) norm. Hence,
there is some g ∈ N (ε, F ,'ι(Zn)) which satisfies that nN Pijf(Zij) - g(zi,j )| < 8, by the
triangle inequality, union bound and Hoeffding’s inequality,
Prε	sup ∣∣	εi,jf(Zi,j)
f∈F ∣ i,j
≤ Prε	sup
I g∈N( 8 ,F,'i (Zn))
εi,jg(Zi,j)
i,j
≤2N(ε, F ,'ι(Zn))Prε{∣X εi,j g(zi,j)
i,j
ε	nN ε2
≤ 2N(8,F,'1(Zn))exp ( - 128B2)
(14)
Combining Eq.(12), Eq. (13) and Eq. (14), we obtain the first conclusion of Proposition 1.
Proposition 2. (Technical Lemma based on Union bound) Let {El}1≤l≤m be a set of conclusions,
which satisfy p(El) ≥ 1- δl, with δl ≥ 0, (i ∈ [m]). E is a conclusion derived using {El}1≤l≤m,
E — Ei ∧ E2 ∧ ... ∧ Em. Then, P(E) ≥ 1 — Pm=I δι.
Proof. Using Morgan’s laws, we have:
p(^m El )=1-p(_m El)
l=1	l=1
Using the union bound, we have:
mm	m
P( _ -El) ≤ X P(-El) = X(I- P(El))
Then, we have:
P(E) ≥P(^m El	) ≥ 1-Xm (1	- P(El	)) ≥ 1-Xm (1-(1-δl	))= 1-Xm	δl
l=1	l=1	l=1	l=1
14
Under review as a conference paper at ICLR 2022
B Proof of Main Results
B.1 Proof of Lemma 1
proof.
By triangle inequality,
11
ES 〜Sn,S〜PN,N 〜P(N )[ET 〜P M(A(S))]] - ME Γ^7 E (⅝k (A(Sk ))
k：Sk ∈D ' kl i：Sk ∈Sk
1
≤ ES 〜Sn,S〜Pn,N 〜P(N )[ET 〜P 防(A(S 川-ES 〜Sn,S〜PN,N 〜P(N) N ʌ, ESi (A(S ))
L	i6i∈S
+ sup
A∈A
ES 〜Sn,S〜Pn,N 〜P(N)
N X ESi (A(S))
i:Si ∈S
ES 〜Sn,S〜Pn,N 〜P(N)
N X 台Si(A(S))
- i∙.Sa∈S	'
+ sup ES〜Sn,S〜Pn,N〜P(N) W X ESi (A(S))
A∈A	LN i：Si∈S	.
X ∣S"7 X aSk(A(Sk))
k.Sk∈D 1 kl i.Sk∈Sk
1
M
—
(I) + (II) + (III)
Splitting A as a combination of Af and Ag and by the definition of Y -discrepancy,
(I) ≤ESi^Sn,S^pN,N〜P(N)ET〜P SUplET (g。Af (S))-E;(g。Af (S))I
g∈G
=ES〜Sn,S〜PN,N〜P(N)Et〜P [discγ (Af (S)(S, T))]
By Jensen’s inequality,
s.t. S :
1N
一 X Si
NJ
i=1
11
(H) ≤es〜Sn,S〜Pn,N〜P(N) SUp N £ ESi(A(S))- N £ ESi(A(S))
LA∈A	i6i∈S	i∙∙Si∈S
≤ES〜Sn ,S〜PN ,N 〜P(N) SUP K X ESi (h) - N X ESi (h)i + Bδ
Lh∈H N i：Si∈s	NMS	」
(with prob.	/	1
at least 1-δ)o / llnNι(ξ,F,'ι) — ln 8 ʌ 2
+ Bδ
where F = {z = (x, y) → '(h(x), y) : h ∈ H} represents the loss function class w.r.t. the decision
function class H. The inequality (according to the complexity term) is derived by Proposition 1 for
any ξ > 0, any n ≥ 8ξ2- and any δ > 0.
By applying the Proposition 1 via regarding the meta-sources data {<Si}ι≤i≤M as a sample, we have
for any ξ0 > 0, any M ≥ (8B)- and any δ > 0,
atwleasιpιobδ)	( llnN(ξ F `i) 一 ln δ ʌ ιʌ
(III)	≤ o ((	N1(8 ,M'1)	8) l
t	ʃ- r A	1	~~> ʌ ιxι Λ∖ ∖ λ _ λ τ	. i	i`	∙ ∙ i	i` . ∙	.
where F = {S → ^^ Ei ESi (A(S)) : A ∈ A} represents a class of empirical error functions w.r.t.
the algorithm class A.
The final conclusion can be obtained by applying Proposition 2 to the controls of (I), (II) and (III).
B.2 Proof of Lemma 2
proof.
15
Under review as a conference paper at ICLR 2022
By triangle inequality,
1
RdiSCY - M ɪ2 discY(A(0(S)(S, Ti))
_	_	「_	_____________ 1 "一.,..w ;、
=ES〜Sn,S〜PN,N 〜P(N )ET 〜P [discY (A(D)(S)(S,T))] - M EdiSCY(A(D)⑹)6, Ti)) I
i=1
≤ ES〜Sn,S〜PN,N〜P(N)Et〜P [discγ (A(D)⑹区 T))]
IE?	IE?	「飞鼠 (Λ ∕zτSλ ( Cλ / C ΠΓ,λλ^l
-ES〜Sn,S〜PN,N〜P(N)ETr〜Tn,T〜P [discY (A(D)(S)(S, T))I
m	m	/-"''A、/ A 济、\1	1 WQ	4"A"A 4\
+ ES 〜Sn,S〜Pn,N 〜P(N)ET 〜Tn,T〜P [discY (A(D)(S)(S, T))] - M EdiSCY (A(D)(Si)(Si, Ti))
i=1
=(I)+ (II)
By triangle inequality and additivity of the supremum function,
(I) ≤Es〜Sn,S〜pn,n〜P(N)E^~τn,τ~P [∣discY (A(D)(S)(S, T)) - discy (A(D)(S)(S, T))∣]
≤ES〜Sn S〜PN N〜P(N)E^〜TnT〜P SUP eis(g ◦ A(D)(S)) - eT (g ◦ A(D)(S))
k '	Lg∈G
-e$(g。A(D)(S)) + ^t (g。A(D)(S))
≤ES〜Sn S〜PN N〜P(N) suP eS(g θ A(D)(S)) - eS(g。A(D)(S))
'	'	Lg∈G
+ ES 〜Sn S 〜PN N 〜P(N )Et 〜TnT 〜P SUP eT(g。A(D)(S))-金(g。A(D)(S))
k '	Lg∈G
≤ES 〜Sn,S 〜PN,N 〜P(N)
SUP ∣eS(h) - eS(h) I
+ EG〜Tn T〜P	SUP | eT(h) - T((h) |
， Lh∈H
(with prob.	/	1
at leas≤1-2δ)o / ∣ lnN1(ε, F,'ι) - ln 8 ʌ 2
+ 2Bδ
where F = {z = (x, y) → '(h(x), y) : h ∈ H} represents the loss function class. The last inequality
according to the complexity terms is derived by applying Proposition 1 twice, and the conclusion can
be obtained for any ε > 0, any n ≥ and any δ > 0.
By Proposition 1 via treating a combination of meta-source and meta-target samples, we have for any
ε0 > 0, any M ≥ ;8^ and any δ > 0,
(ε )
(With prob.	/	/	八 1∖
(II) at leaS≤ I) O ( (lnN1(导，R,'1) - ln 8 ))
ʌ ,
1	<π	CrC FTm	1 ∙	∕4∕c∖∕c rm∖ ∖	Λ _ Λ T	.	ι	r∙ r∙ . ∙	. .ι
where R := {{S, T} → diScy (A(S)(S, T)) : A ∈ A} represents a class of functions w.r.t. the
empirical Y-discrepancy diScy(Af (∙)(∙, ∙)) between a combination of source domains and target
domain.
The final conclusion can be reached by applying Proposition 2 to the control of (I) and (II).	□
B.3 Proof OF Theorem 2
proof.
16
Under review as a conference paper at ICLR 2022
By applying Proposition 2 to Lemma 1 and Lemma 2, for any ε, ε0, ε00 > 0, any n ≥ 8B2 and any
M ≥ max{(8B2,(IB-2}, We have for any δ > 0, with probability at least 1 - 5δ,
11
见 S 〜Sn,S〜PN,N 〜P(N )[ET 〜P M(A(S))]] - ME π^^7 £ e^k (A(Sk))
k∙Sk∈D 1 k i6k∈Sk
≤ -M X discγ (A(D)(Si)(Si, T i)) +3Βδ + O ((ln N ε, F,`1)- ln 8 ))
+ o ((lnNIl(ε，F,'ι) - ln I! 2) + o ((lnNII⅝~,R,'ι) - ln8! 2)

B.4 ERM-based DG Bound
Theorem 2.	(ERM-based DG Bound): Given any ε, ε0 > 0,for any n ≥ 8B2 and any Mm ≥(|B2,
we have for any δ > 0, with probability at least 1 - 2δ,
1N	i
ES 〜Sn,S〜PN,N 〜P(N )[ET 〜P kτ(A(s))]] - N∑S*⅝i(A({§ }1≤i≤Nm ))
i=1
ʌ. , ʌ.
≤Ξ(ε,δ, S) +Ω(ε0,δ, S) + Bδ
ι	. 1	1 ■. .	J- . .ι ι . ■	I—I / r A∖ ∕c∖/ Λ / /-∖	J rʌ / / r A∖
where the complexity terms according to the data Size are Ξ(ε, δ, S) H O(1∕√n) and Ω(ε0, δ, S) H
O(1∕√Nm) ∙
Notes. As shown in Theorem 2, the complexity terms of ERM-based DG bound relates to the
number of training examples in each domain O(1∕√n) and the maximum number of source domains
O(1∕√Nm). The bound sheds light on some DG methods based on data augmentation (Shankar
et al., 2018; Volpi et al., 2018; Qiao et al., 2020; Zhou et al., 2020), which produce more training
examples in each domain to reduce Ξ(ε, δ, S) or produce more source domains to reduce Ω(ε0, δ, S).
proof.
By triangle inequality, Jensen’s inequality and Fubini’s theorem,
1N	i
ES 〜Sn,S〜PN,N 〜P(N )[ET 〜P M(A(S))]] - N∑S(⅝i (A({6 }1≤i≤Nm ))
N i=1
≤ suP
A∈A
ES〜Sn,S〜pN,N〜P(N) [eP~P [EP(A(S))] - EP~PEP〜Pn [^^(A(S))]] |
N
i
+ SUP ES〜Sn S〜PN N〜P(N)EP~PEP〜Pn kP(A(S))] -), αi%i (A(S ))
A∈A	i=1
≤ES〜Sn,S〜Pn,N〜P(N )EP〜P旧力〜Pn
SuP IeP(A(S))- ^P(A((S))∣
A∈A
∣	N∣
+ Sup ES 〜Sn S〜PN N 〜P(N )E^ 〜P n [(⅛(A(S))] - Eαi%i (A({^ }1≤i≤Nm )) S ∙t∙p := EP [P]
A∈A	i=1
(I) + (II)
17
Under review as a conference paper at ICLR 2022
By applying Proposition 1, for any ε > 0 and any n ≥ 8B2, We have for any δ > 0,
⑴ ≤ EP〜PEP〜Pn	sup sup sup kp(A(S)) - ^p(A(S))∣
LN ∈[Nm] S∈ZN ×n A∈A	一
(with prob.	/	1 ∖
≤ Ep3 EP 〜PnbupIe P(h) - ^p(h)∣] atlea≤≤1-δ) O ( (n N 8 ,F,'1)- ln 8 Yj +Bδ
where F = {z = (x, y) → '(h(x), y) : h ∈ H} represents the loss function class w.r.t. the decision
function class H.
By applying Proposition 1 via treating {(Si, Di)}1≤i≤Nm = {({Si}1≤i≤Nm , Si)}1≤i≤Nm as the
sample, whose element is drawn i.i.d. according to a joint distribution (Sn (S 〜PN ,N 〜
P(N)), P), for any ε0 > 0 and any Nm ≥ (8^, We have for any δ > 0,
(with prob.
at least 1-δ)
(II)	≤ O
ln N1( 8, F, 'l) - ln 8
Nm
1	-J- r / A n⅛,∖	ʌ / Λ / A∖ ∖	4 一	.	i' . ∙	1	i'	∙ ∙	1	.	.1
where F = {(S, D) → <≡d(A(S)) : A ∈ A} denotes a function class of empirical error w.r.t. the
algorithm class A.
B.5 Domain-invariant Learning for DA
Theorem 3.	(Domain-invariant Learning for DA) (Zhang et al., 2012) Given any ε > 0, we have
thatfor any n ≥ 8Br and any δ > 0, with probability at least 1 - δ,
Nm
1
eτ(A(S)) - N- XGSi(A(S))
ʌ	,	.	, ʌ .	, ʌ .- ʌ . X
discγ (A(S)S，T))
ʌ.
+ Ξ(ε,δ, S)
1	. 1	1 ■ . .	T . 1 .	■	■ I-' / Γ f∖ z∏ / 1 I / ∖
where the complexity term according to data Size is Ξ(ε, δ, S) H O(1∕√n).
proof.
By triangle inequality,
Nm
1
eτ(A(S)) - N- XGSi(A(S))
N
m
N
m
N
m
≤∣eT
m	mm
(Ag(S) ◦ A(S)) - N- Xesi(Ag(S) ◦ A(S)) + N- Xesi(A(S))- N- Xe^i(A(S))
Nm	Nm	Nm
i=1
i=1
i=1
m
m
m
=(I) + (II)
By Proposition 1, given any ξ > 0, for any n ≥ 8B2, we have for any δ > 0,
∣ 1 Nm	1 Nm
(II) ≤ SuP 十 EeSi(h)-十 E*¾i(h)
h∈H∣Nm i=1	Nm i=1
(with prob.
at least 1-δ)
≤O
18
Under review as a conference paper at ICLR 2022
where F = {z = (x, y) → '(h(x), y) : h ∈ H} represents the loss function class.
ι Nm	-	1
(I)	≤ SUP	eτ(g。Af (S))	-	Nr X 自(g。Af (S))	= discγ	(Af (S)(T, S)) S.t. S =	Nr	X	Si
g∈G	Nm i=1	Nmi∈[Nm]
M
m
1
≤ 寸 Ip SUP ET
Nmi∈⅛g∈G
(g。Af (S)) -ES
discγ (Af (S)(T, Si))
i=1
(Step 1*) 1 Nm .
≤	Nm TdiSCY
i=1
ʌ. /ʌ
+ O / (lnM(8,F,'ι) - ln 8
n
m
1
2
* Control of Step 1
First we use triangle inequality and additivity of the supermum function, next we use Proposition 1,
given any ε, for any n ≥ 8Br, we have for any δ > 0,
discγ (Af (S)(T, Si)) - discγ(Af (S)(T, Si))
,	.-f , ʌ. .	,	. -f , ʌ. .	.	. -f , ʌ. .	. ,	. -f , ʌ..
= Sup Eτ(go	Af (S)) - ESi(g	o Af (S))	- sup	^^(g o	Af (S))	- ^Si(g	o	Af (S))
g∈G	g∈G
≤ sup ET(g o Af (S))-ESi(go Af (S)) - ^t(g o Af (S)) + ^Si(g o Af (S))
g∈G
≤ sup Et (g	o	Af (S)) - ^^ (g	o Af (S))	+sup	ESi (g o	Af (S))	- ^Si (g	o	Af (S))
g∈G	g∈G
≤ sup Eτ(h) — ^t(h)
h∈H
+ sup ESi (h) — ^^i(h)
h∈H
(with prob.
at least 1-2δ)
≤
In Ni( j, F, `i)- in 8
n
O
where F = {z = (x, y) → '(h(x),y) : h ∈ H} denotes the loss function class w.r.t. the decision
function class H.
Using Proposition 2 to combine the controls of (I) and (II), we can obtain the conclusion. □
B.6 Domain-invariant Learning for DG
Theorem 4. (Domain-invariant Learning for DG) (Albuquerque et al., 2020) Given any ε > 0,
we have thatfor any n ≥ 8j2- and any δ > 0, with probability at least 1 一 δ,
八	1 Nm	八	2 Nm	ʌ	八
ET〜P [ET(A(S))] - n- X GSi(A(S)) ≤ Y + n- X discγ (Af (S)(6,a))+ Ξ(ε, δ, S)
m i=1	m i<j
(15)
where Y := ET〜P[discγ (Af (S)(TA(S), T))],TA(S) = argminτ∈conv(s) discγ(Af (S)(T, T))
defines as the shortest Y-discrepancy between target domain and the convex hull ofsource domains.
The COmPIeXIty term according to data size IS n(ε, δ, S) H O(1∕√n).
proof.
By triangle inequality,
Nm
1
ET〜P[ET(A(S))] - N-EESi(A(S))
m i=1
N
m
≤Et 〜P
m
[discγ (Af (S)(S, T))] + e5∙(A(S)) - Nr X E^i(A(S))
Nm
m
i=1
=(I) + (II)
19
Under review as a conference paper at ICLR 2022
By Proposition 1, given any ε > 0, for any n ≥ 8B2, We have for any δ > 0,
N
m
1m
(II) ≤ hUH eS(h)- Nm X ɑi (h)
(with prob.	/	1
at least 1-δ) O I ∣ lnNι(8, F,'ι) - ln 8 V
n
i=1
where F = {z = (x, y) → '(h(x),y) : h ∈ H} denotes the loss function class w.r.t. the decision
function class H.
By triangle inequality,
(I) ≤Et〜P[discγ (Af(S)(TA(S),T)) +discγ (Af(S)(TA⑸，S))]
(Step 1
≤
(Step 2
≤
*
l *
)	八 _*	2 Nm	A
ET〜P [discγ (Af (S)(TA(S), T))] +τr∑discγ (Af (S)(Si,Sj))
Nm
i<j
)	Nm
ET〜P [discγ (Af (S)(TA(S), T))] + N- Xdiscγ(Af (S)(^i, ^j))
Nm
i<j
+ 0 ( (ln NI (8,F,'1) - ln δ
1
2
n
*	Control of Step 1
We define TA(S) = argminT∈conv(s) discγ(Af (S)(T, T)) as the nearest point to T in the convex
hull of sources Conv(S) w.r.t. the Y-discrepancy discγ(Af (S)(∙, ∙)). By Jensen,s inequality,
ET〜P [discγ (A(S)(TA(S),S))]
=ET 〜P sup
g∈G
Nm
X B【j
j=1
Nm
g ◦ A(S))
m i=1
mm
≤EP	XX βj(S)SUP IeSj (g ◦ Af (S))-ESi (g ◦ A(S))I
Nm j=1 i=1	g∈G
max discY
j ∈[Nm]
where βA(S) := arg min discγ (Af (S)(PN=1 βjSj, T)) is the combination weights of TA(S) w.r.t.
β0∈ΛN
the source domains {Si}ι≤i≤N, represented as TA(S) = PN=I βA(S)Si.
*	Control of Step 2
Using a similar derivation as the control of step 1 in B.5, given any ε > 0, for any n > IB, we have
for any δ > 0,
(with prob.	/	ι ∖
,孝八.,、 八 / 乎八八.八∙、at least 1-2δ)	l ln N1( ε, F, '1) - ln δ \ \
discγ (Af (S)(Si,Sj)) - discγ(Af (S)(^i,^j))	≤ O	——",九 1，-------------8
where F = {z = (x, y) → '(h(x),y) : h ∈ H} denotes the loss function class w.r.t. the decision
function class H.
Using Proposition 2 to combine the controls of (I) and (II), we can obtain the conclusion.
20
Under review as a conference paper at ICLR 2022
B.7 Proof of Corollary 1
proof.
By the triangle inequality of Y -discrepancy,
discγ (A(D)(S)(T, ConV(S))) = discγ (A(D)(S)(TA(D)⑹,T))
≤discγ (A(D)(S)(T,S)) +discγ (A(D)(S)(TA(D)(s),S))
=(I) + (II)
Using a similar derivation as the control of (I) in B.5, given any ε > 0, for any n > 等,we have for
any δ > 0,
(with prob.at
least 1-δ)
(I)	≤ 而 X discγ (A(D)(S)(T, Si)) +O
|S| i∙.Sz∈S
、----------------{z---------------}
Outer-loop Ob jective (Lout )
ln N1( ε, F, 'ι) - ln 8
n
First using a similar derivation as the control of Step 1 in B.6, then using a similar derivation as the
control of Step 1 in B.5, given any ε > 0, for any n > 等,we have for any δ > 0,
(II)	≤总 X X discγ (A(D)(S)(Si,Sj))
|S| i6i∈Sj:Sj∈S
(with prob.at ∣ ^∣
least 1-δ)	2	|S|
≤	-^ Xdiscγ(A(D)(S)(^i, Sj)) +O
|S| i<j
、---------------V----------------}
Inner-loop Ob jective (Lin )
ln Nι( 8, F, `i)- ln 8
n
Using Proposition 2 to combine the controls of (I) and (II), we can obtain the final conclusion.
21
Under review as a conference paper at ICLR 2022
Algorithm 3 Meta-training Procedure.
Input data: Nm source-domain training samples, hyperparameters: η, β
Parameters: Feature extractor Fψ, adversarial classifiers T = {Tθi,j }1≤i<j≤Nm, task classifier Tθ
Output: Meta-trained model parameters {ψtr, {θit,rj}1≤i<j≤Nm , θtr}
1:	while Stopping condition is not met do
2:	Sample minibatch of meta-target sample Dte (the l-th domain in Nm) and minibatch of
meta-source samples Dtr
3:	for Sk, St ∈ DDtr, k < t do
4:	Lknnt — l^^k (Tθk,t ◦ Fψ ) - ^^t (Tθk,t ◦ Fψ )1	Tθk,t ∈ T	# inner-loop objectives
5:	θk,t - θk,t + ηVθk,tLknt	# update adv. classifiers
6:	end for
7:	ψ0 一 ψ 一 ηVψ Pk t Lkn	# inner-loop update adv. classifiers
8:	for Sk ∈ Dtr do
9:	Lout ―归Sk(Tθk,ι ◦ Fψ0)	- l⅛te(Tθk,ι	◦ Fψ0 )∣	Tθk,ι	∈ T	# oUter-IOOPObjeCtiVeS
10:	θo,ι  θo,ι + ηVθk,ι Lkut	# update adv. classifiers
11:	end for
12:	ψ J ψ — ηβVψ Pk Lkut	# update feature extractor
13:	Ltask J F (Tθ ◦ Fψ)	# task objectives
14:	[θ, ψ] J [θ, ψ] 一 ηVθ,ψ Ltask	# update task classifier and feature extractor
15:	end while
Algorithm 4 Meta-test Procedure.
Input data: Nm source-domain training samples, hyperparameters: η, β
Output: Feature extractor Fψ, task classifier Tθ
1:	ψ J ψtr, {θi,j}1≤i<j≤Nm J {θit,rj}1≤i<j≤Nm, θ J θtr # Initialize with meta-trained
parameters
2:	while Stopping condition is not met do
3:	Sample minibatch of all source-domain samples DD
4:	for Sk, St ∈ DD, 1 ≤ k <t ≤ Nm do
5:	Lknt j ∣^^k(Tθk,t ◦ Fψ)	- ^^t(Tθk,t ◦	Fψ)∣	Tθk,t	∈ T	#inner-loopobjectives
6:	θk,t J θk,t + ηVθk,tLin	# update adv. classifiers
7:	end for
8:	ψ J ψ - ηβVψ Pk,t Likn,t	# update feature extractor
9:	Ltask J ^d(Tθ ◦ Fψ)	# task objectives
10:	[θ, ψ] J [θ, ψ] - ηVθ,ψ Ltask	# update task classifier and feature extractor
11:	end while
C	Training Procedure
We use the first-order approximation for gradient descent to update the meta-parameters, ignoring
the second derivative terms (Finn et al., 2017). In particular, the update of meta-parameters (with
the outer-loop optimization) is only based on the descents computed w.r.t. the parameters updated
after the last step of the inner-loop optimization. Besides, we use an adversarial training strategy
(Goodfellow et al., 2014; Ganin et al., 2016) to optimize the inner-loop and outer-loop objectives
(Lin , Lout in Definition 2) in meta-training procedure. In particular, the adversarial training process
is implemented via a gradient reverse component (Ganin et al., 2016). The whole meta-learning
procedure is shown in Algorithm 3 & 4 and described as follows.
Meta-training. As shown in Algorithm 3, line 3 - 7 show an adversarial training process to optimize
the inner-loop objective Lin , which can be seen as a two-player minimax game between adversarial
classifiers and the feature extractor. Line 8 - 12 show an similar way to optimize the outer-loop
objective Lout via adversarial training. In addition, Line 13 - 14 show the training process of the
classification task w.r.t. the task classifier and feature extractor with the source domain labeled data.
22
Under review as a conference paper at ICLR 2022
Table 4: Hyperparameters, their default values and distributions for random search.
Hyperparameters	Default value	Random distribution
batch size (PACS)	32	2U (3,5.5)
batch size (DomainNet)	32	2U(3,5)
dropout	0	Random Choise from {0, 0.1, 0.5}
learning rate	5e-5	10U (-5,-3.5)
generator learning rate	5e-5	10U (-5,-3.5)
classifier learning rate	5e-5	10U (-5,-3.5)
weight decay	0	10U (-6,-2)
generator weight decay	0	10U (-6,-2)
discriminator weight decay	0	10U (-6,-2)
adam β1	0.5	Random Choise from {0, 0.5}
β	1	10U(-1,1)
U(a,b) denotes a random variable sampled according to the uniform distribution on [a, b].
Meta-test. As shown in Algorithm 4, the learned feature extractor is further trained on all of the N
source domains with the inner-loop objective in line 4 - 8 and simultaneously, the classification task
w.r.t. the task classifier and feature extractor is also trained with the source-domain labeled data in
line 9 - 10.
D	Hyperparameters
We list the default value and random search distribution for each hyperparameter in Table 4.
E Full Results of State-of-the-art C omparis on
The full results of state-of-the-art comparisons on PACS and DomainNet are listed in Table 5 and
Table 6, respectively.
We first make comparisons with ERM methods: Erm-Aug (Vapnik, 1999) which improves ERM
via data augmentation, Dro (Sagawa et al., 2019), which increases the importance of domains with
larger risks when performing ERM and Mixup (Wang et al., 2020), which is in consistent with the
theoretical analysis that discrepancy-optimal meta-learning has the potential to be better than ERM
when Y -discrepancy can be promisingly optimized. CORAL (Sun & Saenko, 2016), MMD (Li et al.,
2018b), Dann (Ganin et al., 2016) and C-Dann (Li et al., 2018c) learn domain-invariant features
across sources through optimizing mean and covariance of distributions, maximum mean discrepancy
(MMD) (Gretton et al., 2012) or H-divergence. Compared with this method, the strength of our
meta-learning method is the to ability to optimize Y-discrepancy between unseen target and source
domains, which is also in consistent with the theoretical analysis. A state-of-the-art meta-learning DG
method Mldg (Li et al., 2018a) directly uses classification objectives of target and source domains
as the inner-loop and outer-loop objectives in bilevel optimization. Our method focus on optimizing
the domain discrepancy to reduce domain shift, which has reasoning theoretical guarantees and
achieves better results. Besides, our method also outperforms recent state-of-the-art methods such as
Mtl (Blanchard et al., 2021), which uses marginal transfer learning based on kernel-based methods;
SagNet (Nam et al., 2019), which uses style-agnostic networks to reduce intrinsic style bias of
CNN; and Rsc (Huang et al., 2020), which uses a simple training heuristic algorithm to improve DG.
23
Under review as a conference paper at ICLR 2022
Table 5: Accuracy (%) on PACS using pretrained ReSNet-50 backbone.
Method	paintings	cartoon	photo	sketch	AVg.
ERM (Vapnik, 1999)	84.7±o.4	80.8±0.6	97.2±0.3	79.3±1.0	85.5
Dro (Sagawa et al., 2019)	83.5±0.9	79.1±0.6	96.7±0.3	78.3±2.0	84.4
MIXUP (Wang et al., 2020)	86.1±0.5	78.9±0.8	97.6±0.1	75.8±1.8	84.6
Coral (Sun & Saenko, 2016)	88∙3±0.2	80.0±0.5	97.5±0.3	78.8±1.3	86.2
MMD (Li et al., 2018b)	86.1±1.4	79.4 ±0.9	96.6±0.2	76.5±0.5	84.6
Dann (Ganin et al., 2016)	86∙4±0.8	77.4±0.8	97.3±0.4	73.5±2.3	83.6
C-DANN (Li et al., 2018c)	84.6±i.8	75.5±0.9	96.8±0.3	73.5±0.6	82.6
MLDG (Li et al., 2018a)	85.5±1.4	80.1±1.7	97.4±0.3	76.6±1.1	84.9
MTL (Blanchard et al., 2021)	87.5±0.8	77.1±0.5	96.4±0.8	77.3±1.8	84.6
SagNet (Nam et al., 2019)	87.4±1.0	80.7±0.6	97.1±0.1	80.0±0.4	86.3
RSC (Huang et al., 2020)	85.4±o.8	79.7±1.8	97.6±0.3	78.2±1.2	85.2
Ours	88.2±0.8	81.7±1.2	97.8±0.3	79.6±0.7	86.8
Table 6: ACCUraCy (%) on the DomainNet Using Pretrained ReSNet-50 backbone.
Method	I clipart	infograph	painting	qickdraw	real	sketch	I Avg.
ERM (Vapnik, 1999)	58.1±0.3	18.8±0.3	46.7±0.3	12.2±0.4	59.6±0.1	49.8±0.4	40.9
Dro (Sagawa et al., 2019)	47.2±0.5	17.5±0.4	33.8±0.5	9.3±0.3	51.6±0.4	40.1±0.6	33.3
MIXUP (Wang et al., 2020)	55.7±o.3	18.5±0.5	44.3±0.5	12.5±0.4	55.8±0.3	48.2±0.5	39.2
Coral (Sun & Saenko, 2016)	59.2±0.1	19.7±0.2	46.6±0.3	13.4±0.4	59.8±0.2	50.1±0.6	41.5
MMD (Li et al., 2018b)	32.1±13.3	11.0±4.6	26.8±11.3	8.7±2.1	32.7±13.8	28.9±11.9	23.4
Dann (Ganin et al., 2016)	53.1±o.2	18.3±0.1	44.2±0.7	11.8±0.1	55.5±0.4	46.8±0.6	38.3
C-DANN (Li et al., 2018c)	54.6±o.4	17.3±0.1	43.7±0.9	12.1±0.7	56.2±0.4	45.9±0.5	38.3
MLDG (Li et al., 2018a)	59.1±0.2	19.1±0.3	45.8±0.7	13.4±0.3	59.6±0.2	50.2±0.4	41.2
Mtl (Blanchard et al., 2021)	57.9±o.5	18.5±0.4	46.0±0.1	12.5±0.1	59.5±0.3	49.2±0.1	40.6
SAGNET (Nam et al., 2019)	57.7±0.3	19.0±0.2	45.3±0.3	12.7±0.5	58.1±0.5	48.8±0.2	40.3
RSC (Huang et al., 2020)	55.0±1.2	18.3±0.5	44.4±0.6	12.2±0.2	55.7±0.7	47.8±0.9	38.9
Ours	60.7±0.8	23.3±0.5	51.2±0.2	14.7±0.4	63.6±0.4	51.8±0.8	44.2
(a) ERM
(b) Domain-Invariant Learning
(c) Ours
Figure 4: t-SNE visualization of feature representions on PACS when the target domain is photo.
Different image categories are represented by different markers and different domains are represented
in different colors where the instances of target domain are represented in gray.
F	Visualization
We visualize the learned feature representations by t-SNE (Maaten & Hinton, 2008), which can be
seen as a real-world case for Figure 1. We randomly select 250 test examples from each domain.
As shown in Figure 4, compared with ERM, both domain-invariant learning and our method can
match the feature distributions of source domains; Compared with the domain-invariant learning, our
method can also well match the feature distributions of the target and source domains, which benefits
from the outer-loop objective in bilevel optimization to improve the robustness to domain shift.
24
Under review as a conference paper at ICLR 2022
Figure 5: Number of meta-training domains on DomainNet.
G Tradeoff between Performance and Complexity
We study the possible number of meta-source domains, which has positive correlation with the
size of meta-sample in episodic training process. As shown in Figure 5, we compare three sets of
possible numbers of meta-source domains: {4, 3, 2}, {4, 3}, {4}, where 4 is the maximum number of
meta-source domains on DomainNet. We find that although more possible numbers of meta-source
domains can increase the compute complexity, it also improves the performances, because it improves
the robustness of algorithm to different numbers of source domains. This is important for DG based
on episodic training, because in test phase, the source domains are more than those in meta-training
phase.
25