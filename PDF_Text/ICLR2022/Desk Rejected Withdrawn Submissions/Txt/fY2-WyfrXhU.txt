Under review as a conference paper at ICLR 2022
MemREIN: Rein the Domain Shift for Cross-
Domain Few-Shot Learning
Anonymous authors
Paper under double-blind review
Ab stract
Few-shot learning aims to enable models generalize to new categories (query in-
stances) with only limited labeled samples (support instances) from each category.
Metric-based mechanism is a promising direction which compares feature embed-
dings via different metrics. However, it always fail to generalize to unseen domains
due to the considerable domain gap challenge. In this paper, we propose a novel
framework, MemREIN, which considers Memorized, Restitution, and Instance
Normalization for cross-domain few-shot learning. Specifically, an instance nor-
malization algorithm is explored to alleviate feature dissimilarity, which provides
the initial model generalization ability. However, naively normalizing the feature
would lose fine-grained discriminative knowledge between different classes. To
this end, a memorized module is further proposed to separate the most refined
knowledge and remember it. Then, a restitution module is utilized to restitute the
discrimination ability from the learned knowledge. A novel reverse contrastive
learning strategy is proposed to stabilize the distillation process. Extensive ex-
periments on five popular benchmark datasets demonstrate that MemREIN well
addresses the domain shift challenge, and significantly improves the performance
up to 16.37% compared with state-of-the-art baselines.
1	Introduction
In recent years, machine learning especially deep learning methods have made amazing achievements
in the field of computer vision, image classification (He et al., 2016), semantic segmentation (Ren
et al., 2016; He et al., 2017), etc. However, the high performance heavily relies on the large amount
of well-labeled training data, which provides comprehensive and diverse samples to cover all corner
cases. Such a huge scale makes it difficult in real practice, thus leads to a new topic of few-shot
learning (Lake et al., 2015). Few-shot learning aims to enable models generalize to new categories
(query instances) with only limited labeled samples (support instances) from each category.
Among existing few-shot learning methods, metric-based methods have attracted more attention
because of their effectiveness and intelligibility. In general, the core idea of this kind of methods
is to make classification based on the similarity between the query images and the support images
via proposed similarity measurements. It usually consists of two main components: (1) feature
encoder and (2) metric function. Given a task with few labeled images (support set) and unlabeled
images (query set), the visual features are firstly extracted via the feature encoder and then passed
through the defined metric function to determine the categories of the query images. The underlying
assumption is that both training and testing are from the same dataset, namely the same domain.
While, when it comes to different domains, the generalization ability of the metric-based methods
greatly decreases (Chen et al., 2019; Tseng et al., 2020). However, such ability to generalize to unseen
domains is of great importance in real practice, e.g., expensive human annotation or time-consuming
data collection. As a result, considering the domain shift scenario within the few-shot learning has
become an important yet challenging task.
Various unsupervised domain adaptation methods have been proposed (Yang et al., 2018; Ding
et al., 2018). These methods aim to minimize the domain gap either by learning domain-invariant
representations via representation learning, projection learning, or adversarial strategies (Long et al.,
2015; Kumar et al., 2018; Ganin et al., 2016; Tzeng et al., 2015; 2017; Kundu et al., 2019). However,
these methods assume that the complete unlabeled samples from the target domain are accessible
1
Under review as a conference paper at ICLR 2022
Figure 1: Framework of our MemREIN method. With instance normalization approach, the sample-
specific features F can be reduced, and then with memorized and restitution approach, the long-term
discriminative information can be distilled and restituted to refined features.
while training. We argue that this assumption may not hold in real situations, and it could leads
to high computational cost in testing phase. Domain shift problem could be addressed by various
domain generalization methods (Blanchard et al., 2011; Muandet et al., 2013; Motiian et al., 2017).
However, these methods assume that the source and target domains share the same categories. In
contrast, our goal is to recognize novel categories from the target domain with only a few (e.g., 1 or
5) of samples selected from novel categories.
As argued above, there are two main challenges in cross-domain few-shot learning task. First, how
to minimize the discrepancy between the source domain and the target domain. Second, how to
recognize novel/unseen classes with only limited samples.
To this end, we propose a novel MemREIN approach, which includes Memorized, Restitution, and
Instance Normalization as crucial modules, to “rein” the domain shift level in few-shot scenario. The
core idea of MemREIN is to enhance the generalization ability while still be able to balance the
discrimination ability for subsequent classification. In specific, on the training stage, we first present
an instance normalization layer operating on features with respect to samples at the channel level.
This operation aims to reserve spatial feature dependency and meanwhile remove the image-specific
features, i.e., alleviate the the discrepancy of these training samples. By this way, the generalization
ability across different samples is enhanced. Then, the filtered out features are extracted from
an residual structure. Normally, the filtered out features are considered as useless feature which
could be discarded. However, we consider it still contains fine-grained distinctive knowledge which
could be “remembered” and “restituted”. To this end, we manage to adaptively distill the long-term
discriminative information from them via our proposed novel memorized approach. Then, such
discriminative information is restituted to the above refined features to maintain the discrimination
ability for subsequent classification. A novel reverse contrastive loss constraint in the restitution step
to encourage the better separation of discriminative features and general features, which ensures the
distillation process. Contributions of our work are summarized as follows:
•	A novel memorized and restitution strategy is proposed for discriminative information
distillation. It is able to distill the long-term discriminative information from filtered out
features to maintain the discrimination ability of original features for better classification.
•	An instance normalization strategy is adopted to alleviate the the discrepancy across train-
ing samples, which reduces the sample-specific features and greatly enhances the overall
generalization ability across sample features.
•	A novel reverse contrastive loss is proposed to encourage the better separation of discrimi-
native features and general features, which is able to ensure the distillation process.
Our MemREIN method is simple yet effective. It is a universal and method-agnostic method that
can be applied to various existing metric-based methods for enhancing their generalization ability to
unseen domains. Extensive experiments demonstrate the effectiveness of MemREIN, which achieves
consistent superior performance than existing state-of-the-art methods under the cross-domain setting.
2	Related Work
2.1	Few-shot Classification
Few-shot classification aims to recognize novel classes with a limited amount of labeled samples.
Among these existing methods, metric-based methods have attracted considerable attention and
2
Under review as a conference paper at ICLR 2022
achieved promising performance. This kind of methods usually consists of two components: (1)
feature encoder and (2) metric function. The feature encoder is used to extract features from both
query and support samples. The metric function is used to calculate the similarity for classification.
For instance, MatchingNet (Vinyals et al., 2016) utilizes cosine similarity with an attention Bi-
LSTM for classification and ProtoNet (Snell et al., 2017) applies euclidean distance for classification.
RelationNet (Sung et al., 2018) uses convolutional neural networks and GNN (Satorras & Estrach,
2018) uses the graph convolutional framework as the metric function. Although these methods have
achieved promising performance, they always fail to generalize to unseen domains (datasets) since the
distributions among different domains have huge shifts. Recent work (Chen et al., 2019) reveals that
the performance of existing few-shot learning methods degrades significantly under the cross-domain
setting. The motivation of our work aims to enhance the generalization ability of metric-based
few-shot learning methods so that these methods can better generalize to unseen domains.
2.2	Domain Adaptation
The domain adaptation methods can be divided into four categories depends on the relation between
the source classes Cs and target domain classes Ct . Closed-set domain adaptation (CDA) assumes
the source and target domain share the same categories, existing methods focus on how to alleviate
the feature distribution gap (Borgwardt et al., 2006; Long et al., 2013; Sun & Saenko, 2016; Gopalan
et al., 2011; Ganin & Lempitsky, 2015; Tzeng et al., 2017; Hoffman et al., 2018). Partial domain
adaptation (PDA) assumes that the source domain is large enough to contain all the classes of
target domain. Many works (Cao et al., 2018a; Zhang et al., 2018; Cao et al., 2018b) have been
proposed to solve this problem. Studies on partial domain adaptation promote the setting to a more
common and practical level.Open-set domain adaptation (OSDA) was first proposed by Busto et
al. (Panareda Busto & Gall, 2017), which made a new definition of “unknown” category if this
class is private to source or target domain. The setting of open-set domain adaptation is more
practical compared with partial domain adaptation. Universal Domain Adaptation (UniDA) was
first proposed in the work (You et al., 2019), in which the relation between source categories and
target categories is inaccessible. Following this setting, recent methods Fu et al. (2020); Kundu et al.
(2020); Saito et al. (2020); Saito & Saenko (2021) have been proposed to address this challenge.
2.3	Cross-domain Few-shot Learning
Recently, promoted by the pioneer work (Chen et al., 2019), cross-domain few-shot learning problem
has attracted many attentions. As an emerging task, work () carried out a broader study and introduced
a new benchmark. Some methods (Tseng et al., 2020; Sun et al., 2021; Phoo & Hariharan, 2020;
Zou et al., 2021; Islam et al., 2021) have been proposed and achieve promising performance under
this benchmark. Work (Cai et al., 2020) relaxes this setting where a large number of unlabeled
target samples are accessible in the training phase. Most recently, method ATA (Wang & Deng,
2021) introduced an adversarial task augmentation method to improve the robustness of the inductive
bias under the cross-domain few-shot learning setting. In addition, a noise-enhanced supervised
auto-encoder method was proposed in (Liang et al., 2021) to obtain the broader variations of the
feature distributions to greatly boost the generalization capability of the model. Paper (Fu et al., 2021)
proposed an effective mix-up module into the meta-learning mechanism and a novel disentangle
module with a domain classifier to obtain domain-irrelevant and domain-specific features, which
achieves promising performance. In our work, we propose a simple yet effective method from the
perspective of feature level, which is a universal and method-agnostic method.
3	Method
3.1	Preliminaries
In the few-shot classification problem, define a task T characterized as Nw way and Ns shot, which
represents the number of categories and the number of labeled samples in each category, respectively.
At each iteration, the metric-based few-shot learning method first randomly samples Nw categories as
a task T , and then constructs a support set S = {(Xs, Ys)} and a query set Q = {(Xq, Yq)}. These
two sets are constructed by randomly selecting Ns and Nq samples for each of the Nw categories.
Once the data is prepared, the feature encoder E first extracts features of the samples from both
support set S and the query set Q. Then, the defined metric function M makes predictions of the
3
Under review as a conference paper at ICLR 2022
query samples Xq based on three parts: the label of support samples Ys, encoded query image E(Xq),
and the encoded support images E(Xs), which is formulated as follows:
Yq = M (Ys,E(Xq ),E (Xs)).	(1)
After all, the objective of the metric-based few-shot learning method is the classification loss of the
samples in the query set, which is formulated as follows:
,_ . ʌ..
L = Lcls(Yq ,Yq ).	(2)
The main difference among existing metric-based few-shot learning methods lies in the different
metric functions. Differently, we propose a universal method that can be applied in all the metric-
based few-shot learning methods to achieve better performance under the cross-domain setting.
In this paper, we tackle the cross-domain few-shot classification problem. Given a set of few-shot
classification tasks T = {T1, T2, ..., Tn} as a domain (dataset). At the training stage, given N
accessible domains {T1seen, T2seen, ..., TNseen}, we aim to learn a metric-based few-show learning
model with these seen domains, then the model can generalize to an unseen domainT unseen .
3.2	MemREIN Method
The core idea of our MemREIN method is to enhance the generalization ability, contain the ability
to balance the discrimination of metric-based few-shot learning methods, and achieve promising
performance on arbitrary unseen domains. The overall framework of our MemREIN method is
illustrated in Figure 1. MemREIN is method-agnostic that can be applied to existing metric-based
few-shot learning methods to improve the performance to unseen domains. In addition, itis a universal
framework that can be applied by various neural networks for different applications, e.g. classification,
segmentation, detection. In this paper, we delve into the cross-domain few-shot learning problem and
propose our MemREIN method to “rein” the domain shift level in few-shot classification.
3.2.1	Instance Normalization
As argued above, images with the same category from different domains have large discrepancies
in many aspects e.g. , image style, color, quality. Generally speaking, the discrepancy between the
source domain and the target domain hinders the generalization ability of the model to some extent.
To this end, we reduce the discrepancy cross samples by instance normalization in our proposed
MemREIN method as follows. Denote the input feature map by F ∈ Rc×h×w and the output feature
map by Fe ∈ Rc×h×w , where c, h ,w denote the number of channel, height, width, respectively.
F=IN(F ) = γ (FσFτ)+ β,	(3)
where μ(∙) and σ(∙) denote the mean and standard deviation calculated at the channel level for
each sample, γ ∈ Rc and β ∈ Rc are two trainable parameters. Instance normalization was
originally used in style transfer (Dumoulin et al., 2016; Ulyanov et al., 2016; Huang & Belongie,
2017), which is helpful to enhance the generalization ability by reducing the feature dissimilarity. It
can remove instance/sample specific features out of the input, which makes more general features
remained (Shankar et al., 2018; Volpi et al., 2018).
However, instance normalization inevitably removes some discriminative information from the
original feature maps (Jin et al., 2020; 2021), which weakens the discrimination ability of the
extracted features. To address this emerging problem, we propose a memorized restitution approach
to distill the discriminative information from the filtered out features and then restitute it as the final
output feature maps.
3.2.2	Memorized Restitution
As discussed above, in order to maintain the discrimination ability of the refined features, we propose
a following memorized restitution approach to distill discriminative information. We first obtain the
filtered out feature R via a residual structure, which is defined as follows:
R = F - Fe,	(4)
4
Under review as a conference paper at ICLR 2022
where R ∈ Rc×h×w , denoting the features that we have filtered out via the instance normalization
operation. Since instance normalization operation will inevitably remove discriminative information
from the original features. Hence, there exist discriminative features that we need to distill and purify
from the residual feature R, in order to maintain the discrimination ability of extracted features.
At the training stage, given the feature map R at each iteration (we omit the subscript of feature map
R for brevity), we assume R consists of two parts: D ∈ Rc×h×w with relative more discriminative
information, and G ∈ Rc×h×w with relatively more general information, which is defined as follows:
D(k,:,:) = αkR(k,:,:),
G(k,:,:) = (1 - αk)R(k,:,:),
(5)
where k denotes the kth channel of the feature map, αk denotes the learnable attention parameters to
split the residual feature map R. Note that we split the residual feature map R at the channel level.
Then, the attention vector α = [α1, α2, ..., αc] is derived by SE-like channel attention (Hu et al.,
2018) as follows:
α = δ(W2η(W1avepooling(R))),	(6)
where avepooling represents the average pooling layer, W1 and W2 are parameters to be learned, δ
and η represent the ReLU activation function and sigmoid activation function, respectively.
Since there are limited labeled samples under the few-shot learning framework, it is highly possible
that the model would overfit. Thus we further propose a memorized mechanism with a memory
vector M (l) ∈ Rc to store the long-term feature maps D, which is defined as follows:
M(l) = [M1(l),...,Mk(l),...,Mc(l)],
Mk(l+1) = D(l)(k,:,:),
(7)
where Mk(l) ∈ Rh×w , (l) represents the lth iteration, k denotes the kth channel. At the lth iteration,
we concatenate the feature map D to the memory bank at the channel level, and update D as follows:
D(k, :, :) = maxpooling(concat(Mk(l) , D(k, :, :))),	(8)
where concat represents the concatenation operation, maxpooling represents the max pooling layer.
Once we obtain the updated feature map D, we restitute it to refined feature F as the final output F+
of our proposed MemREIN method, and we also restitute the relatively unimportant feature map G
with feature F as the “contaminated” feature F- for following loss optimization as follows:
Fe+ = Fe + D, Fe - = Fe + G.	(9)
3.2.3	Reverse Contrastive Loss
Apart from the conventional cross-entropy loss defined in Equation 2, we also propose a novel reverse
contrastive loss Lrcl to promote the disentanglement of feature D and feature G. It consists of two
parts: Lr+cl and Lr-cl, e.g. , Lrcl = Lr+cl + Lr-cl. Given a mini-batch Xb = {X1 , ..., XN} contains
N samples at the training phase, we first randomly select one anchor sample referred as Xa , and
then we denote samples with the same category as the positive samples Xpos, samples with different
categories as the negative samples Xneg . Note that the corresponding features of these samples are
denoted with their subscripts such as Fa, Fpos, and Fneg in the following paragraphs.
+	h1
We first reshape features F+ and F- to the size of Rchw×1 and then pass them through one fully-
+
connected layer following the sof tmax function to obtain the feature vectors f+ and f-, which is
defined as follows. Note that these two vectors have the same size of ∈ RK×1.
fe+ = softmax W +reshape(Fe+) , fe- = softmax W -reshape(Fe-) ,	(10)
5
Under review as a conference paper at ICLR 2022
Table 1: Classification accuracy (%) of 5-way 1/5-shot tasks trained on the dataset mini-ImageNet.
5-way 1-shot	Classification Accuracy (%)			
	CUB	Cars	Places	Plantae
MNet (Vinyals et al., 2016)	35.89 ± 0.51%	30.77 ± 0.47%	49.86 ± 0.79%	32.70 ± 0.60%
MNet+FT (Tseng et al., 2020)	36.61 ± 0.53%	29.82 ± 0.44%	51.07 ± 0.68%	34.48 ± 0.50%
MNet+ATA (Wang & Deng, 2021)	41.59 ± 0.40%	35.14 ± 0.30%	51.86 ± 0.50%	37.02 ± 0.30%
MNet+MemREIN (Ours)	43.72 ± 0.45%	37.64 ± 0.43%	53.44 ± 0.62%	39.83 ± 0.50%
RNet (Sung et al., 2018)	42.44 ± 0.77%	29.11 ± 0.60%	48.64 ± 0.85%	33.17 ± 0.64%
RNet+FT (Tseng et al., 2020)	44.07 ± 0.77%	28.63 ± 0.59%	50.68 ± 0.87%	33.14 ± 0.62%
RNet+LRP (Sun et al., 2021)	41.57 ± 0.40%	30.48 ± 0.30%	48.47 ± 0.50%	32.11 ± 0.30%
RNet+ATA (Wang & Deng, 2021)	43.02 ± 0.40%	31.79 ± 0.30%	51.16 ± 0.50%	33.72 ± 0.30%
RNet+MemREIN (Ours)	47.33 ± 0.49%	34.92 ± 0.33%	55.75 ± 0.52%	36.27 ± 0.32%
GNN (Satorras & Estrach, 2018)	45.69 ± 0.68%	31.79 ± 0.51%	53.10 ± 0.80%	35.60 ± 0.56%
GNN+FT (Tseng et al., 2020)	47.47 ± 0.75%	31.61 ± 0.53%	55.77 ± 0.79%	35.95 ± 0.58%
GNN+LRP (Sun et al., 2021)	48.29 ± 0.51%	32.78 ± 0.39%	54.83 ± 0.56%	37.49 ± 0.43%
GNN+ATA (Wang & Deng, 2021)	45.00 ± 0.50%	33.61 ± 0.40%	53.57 ± 0.50%	34.42 ± 0.40%
GNN+MemREIN (Ours)	49.94 ± 0.50%	36.04 ± 0.44%	57.35 ± 0.52%	39.09 ± 0.46%
5-way 5-shot		Classification Accuracy (%)		
	CUB	Cars	Places	Plantae
				
MNet (Vinyals et al., 2016)	51.37 ± 0.77%	38.99 ± 0.64%	63.16 ± 0.77%	46.53 ± 0.68%
MNet+FT (Tseng et al., 2020)	55.23 ± 0.83%	41.24 ± 0.65%	64.55 ± 0.75%	41.69 ± 0.63%
MNet+ATA (Wang & Deng, 2021)	59.33 ± 0.40%	48.78 ± 0.40%	66.31 ± 0.40%	51.56 ± 0.30%
MNet+MemREIN (Ours)	63.87 ± 0.66%	49.47 ± 0.60%	69.08 ± 0.65%	52.98 ± 0.34%
RNet (Sung et al., 2018)	57.77 ± 0.69%	37.33 ± 0.68%	63.32 ± 0.76%	44.00 ± 0.60%
RNet+FT (Tseng et al., 2020)	59.46 ± 0.71%	39.91 ± 0.69%	66.28 ± 0.72%	45.08 ± 0.59%
RNet+LRP (Sun et al., 2021)	57.70 ± 0.40%	41.21 ± 0.40%	65.35 ± 0.40%	43.70 ± 0.30%
RNet+ATA (Wang & Deng, 2021)	59.36 ± 0.40%	42.95 ± 0.40%	66.90 ± 0.40%	45.32 ± 0.30%
RNet+MemREIN (Ours)	63.31 ± 0.42%	46.75 ± 0.44%	70.84 ± 0.52%	49.52 ± 0.40%
GNN (Satorras & Estrach, 2018)	62.25 ± 0.65%	44.28 ± 0.63%	70.84 ± 0.65%	52.53 ± 0.59%
GNN+FT (Tseng et al., 2020)	66.98 ± 0.68%	44.90 ± 0.64%	73.94 ± 0.67%	53.85 ± 0.62%
GNN+LRP (Sun et al., 2021)	64.44 ± 0.48%	46.20 ± 0.46%	74.45 ± 0.47%	54.46 ± 0.46%
GNN+ATA (Wang & Deng, 2021)	66.22 ± 0.50%	49.14 ± 0.40%	75.48 ± 0.40%	52.69 ± 0.40%
GNN+MemREIN (Ours)	72.41 ± 0.56%	49.98 ± 0.43%	77.71 ± 0.54%	56.64 ± 0.45%
where W + and W - are trainable parameters with the same size of RK ×chw , K is the number of
classes in the few-shot classification task. Then, the reverse contrastive loss is defined as follows:
Lr+cl = -E
log
J~ , -Γ ~ ,	■
exPf+ fpos)
PXpθS∈X exp(f + fn+eg )
Lr-cl = -E
>
Xneg ∈X exp(f a fneg)
log---------；>；--------------
exp(f a- fp-os)
(11)
(12)
The goal of our proposed reverse contrastive loss is to promote the disentanglement of feature D and
feature G, where feature D contains more discriminative information and G contains more general
information. Combining feature D with the refined feature F , defined in Equation 9, results in better
discrimination capability of feature F+, in other words, the sample features with same category are
closer and those with different identities are farther apart. Therefore, we propose Lr+cl to promote the
++
features of positive samples fp+os gather closer and separate the features of negative samples fn+eg
from the anchor feature as well. On the other hand, combining feature G with the refined feature F
results in decreasing the discrimination capability, which means the feature F - is more general that
not capable of distinguishing samples with the same category correctly. Therefore, we propose Lr-cl
to separate the the features of positive samples fep-os from both features with negative samples fen-eg
and the anchor feature fa- . The whole objective loss is defined as follows:
6
Under review as a conference paper at ICLR 2022
Table 2: Classification accuracy (%) of 5-way 1/5-shot tasks under the leave-one-out setting.
5-way 1-shot	Classification Accuracy (%)			
	CUB	Cars	Places	Plantae
MNet (Vinyals et al., 2016)	37.90 ± 0.55%	28.96 ± 0.45%	49.01 ± 0.65%	33.21 ± 0.51%
MNet+LFT (Tseng et al., 2020)	43.29 ± 0.59%	30.62 ± 0.48%	52.51 ± 0.67%	35.12 ± 0.54%
MNet+MemREIN (Ours)	46.37 ± 0.50%	35.65 ± 0.45%	54.92 ± 0.64%	38.82 ± 0.48%
RNet (Sung et al., 2018)	44.33 ± 0.59%	29.53 ± 0.45%	47.76 ± 0.63%	33.76 ± 0.52%
RNet+LFT (Tseng et al., 2020)	48.38 ± 0.63%	32.21 ± 0.51%	50.74 ± 0.66%	35.00 ± 0.52%
RNet+MemREIN (Ours)	52.02 ± 0.52%	36.38 ± 0.38%	54.82 ± 0.57%	36.74 ± 0.45%
GNN (Satorras & Estrach, 2018)	49.46 ± 0.73%	32.95 ± 0.56%	51.39 ± 0.80%	37.15 ± 0.60%
GNN+LFT (Tseng et al., 2020)	51.51 ± 0.80%	34.12 ± 0.63%	56.31 ± 0.80%	42.09 ± 0.68%
GNN+MemREIN (Ours)	54.26 ± 0.62%	37.55 ± 0.50%	59.98 ± 0.64%	45.69 ± 0.64%
5-way 5-shot		Classification Accuracy (%)		
				
	CUB	Cars	Places	Plantae
MNet (Vinyals et al., 2016)	51.92 ± 0.80%	39.87 ± 0.51%	61.82 ± 0.57%	47.29 ± 0.51%
MNet+LFT (Tseng et al., 2020)	61.41 ± 0.57%	43.08 ± 0.55%	64.99 ± 0.59%	48.32 ± 0.57%
MNet+MemREIN (Ours)	67.31 ± 0.51%	47.36 ± 0.48%	68.14 ± 0.58%	52.28 ± 0.52%
RNet (Sung et al., 2018)	62.13 ± 0.74%	40.64 ± 0.54%	64.34 ± 0.57%	46.29 ± 0.56%
RNet+LFT (Tseng et al., 2020)	64.99 ± 0.54%	43.44 ± 0.59%	67.35 ± 0.54%	50.39 ± 0.52%
RNet+MemREIN (Ours)	68.39 ± 0.48%	46.92 ± 0.50%	69.87 ± 0.54%	58.64 ± 0.50%
GNN (Satorras & Estrach, 2018)	69.26 ± 0.68%	48.91 ± 0.67%	72.59 ± 0.67%	58.36 ± 0.68%
GNN+LFT (Tseng et al., 2020)	73.11 ± 0.68%	49.88 ± 0.67%	77.05 ± 0.65%	58.84 ± 0.66%
GNN+MemREIN (Ours)	77.54 ± 0.62%	56.78 ± 0.66%	78.84 ± 0.66%	65.44 ± 0.64%
L = Lcls + λ(Lr+cl + Lr-cl),	(13)
where λ is a hyper-parameter to control the balance of these two terms in the training phase.
4	Experiments
4.1	Experimental Setup
Baselines: We make extensive experiments on three existing metric-based few-shot learning methods:
MatchingNet (Vinyals et al., 2016), RelationNet (Sung et al., 2018), and GNN (Satorras & Estrach,
2018). We compare our proposed method with several existing cross-domain few-shot learning
methods: FT (Tseng et al., 2020), LRP (Sun et al., 2021), and ATA (Wang & Deng, 2021) to
demonstrate the advantages of our method. In addition, we also make comparisons with several
existing state-of-the-art few-shot learning methods: TADAM (Oreshkin et al., 2018), DC (Lifchitz
et al., 2019), DC+IMP (Lifchitz et al., 2019), MetaOptNet (Lee et al., 2019), EGNN (Kim et al., 2019),
TPN (Yanbin et al., 2019), DPGN (Yang et al., 2020), MCGN (Tang et al., 2021), ECKPN (Chen
et al., 2021), FromActi (Qiao et al., 2018), and LEO (Rusu et al., 2018) for further demonstration.
More quantitative results and visualizations are provided in Appendix B and C.
Datesets: We conduct experiments on five public datasets that are widely used for few-shot classifi-
cation task1: mini-ImageNet (Ravi & Larochelle, 2016), CUB (Wah et al., 2011), Cars (Krause et al.,
2013), Places (Zhou et al., 2017), and Plantae (Van Horn et al., 2018). Detailed introduction and
statistical analysis of these five datasets could be found in Appendix A.
Setting: To ensure the fair comparison with other methods, we apply the exactly same two cross-
domain settings that applied in the baselines. The first setting is to train on the dataset mini-ImageNet
and test on other four datasets, which means there is only one source domain and one target domain.
The second setting is the leave-one-out setting by selecting one dataset among CUB, Cars, Places,
and Plantae as the target domain for testing, and using the remaining three datasets along with dataset
mini-ImageNet as the source domains for training. The second setting is more difficult since there
are multiple source domains with only one target domain, which results in much larger domain shift.
1Detailed introductions of these datasets are provided in the appendix.
7
Under review as a conference paper at ICLR 2022
Table 3: Classification Accuracy (%) of our proposed method comparing with the state-of-the-art
few-shot learning methods. The model is trained/evaluated only on the dataset mini-ImageNet.
Backbone	Method	Classification Accuracy (%)	
		5-way 1-shot	5-way 5-shot
ResNet-12	TADAM (Oreshkin et al., 2018)	58.50 ± 0.30%	76.70 ± 0.30%
	DC (Lifchitz et al., 2019)	62.53 ± 0.19%	78.95 ± 0.13%
	DC+IMP (Lifchitz et al., 2019)	-	79.77 ± 0.19%
	MetaOptNet (Lee et al., 2019)	64.09 ± 0.62%	80.00 ± 0.45%
	EGNN (Kim et al., 2019)	59.63 ± 0.52%	76.34 ± 0.48%
	TPN (Yanbin et al., 2019)	55.51 ± 0.86%	69.86 ± 0.65%
	DPGN (Yang et al., 2020)	66.14 ± 0.43%	81.23 ± 0.41%
	MCGN (Tang et al., 2021)	67.32 ± 0.43%	83.03 ± 0.54%
	ECKPN (Chen et al., 2021)	70.48 ± 0.38%	85.42 ± 0.46%
WRN-28	FromActi (Qiao et al., 2018)	59.60 ± 0.41%	77.74 ± 0.19%
	LEO (Rusu et al., 2018)	61.76 ± 0.08%	77.59 ± 0.12%
ResNet-10	MNet (Vinyals et al., 2016)	59.10 ± 0.64%	70.96 ± 0.65%
	MNet+LFT (Tseng et al., 2020)	58.76 ± 0.61%	72.53 ± 0.69%
	MNet+MemREIN (Ours)	60.03 ± 0.60%	74.72 ± 0.66%
ResNet-10	RNet (Sung et al.,2018)	57.80 ± 0.88%	71.00 ± 0.69%
	RNet+LFT (Tseng et al., 2020)	58.64 ± 0.85%	73.78 ± 0.64%
	RNet+MemREIN (Ours)	61.64 ± 0.74%	75.98 ± 0.56%
ResNet-10	GNN (Satorras & Estrach, 2018)	60.77 ± 0.75%	80.87 ± 0.56%
	GNN+LFT (Tseng et al., 2020)	66.32 ± 0.80%	81.98 ± 0.55%
	GNN+MemREIN (Ours)	70.64 ± 0.72%	85.48 ± 0.52%
Implementation details: We use the public implementation2 to train both the MatchingNet and the
RelationNet method, and we use the implementation3 to train the GNN method. In all the experiments,
we adopt the ResNet-10 (He et al., 2016) as the backbone network for our feature encoder E. We
insert our proposed MemREIN method after the last batch normalization layer of all the residual
blocks in the feature encoder E at the training stage. Instead of optimizing from the scratch, we apply
a strategy that pre-trains the feature extractor by minimizing the standard cross-entropy classification
loss on the 64 training categories from the dataset mini-ImageNet and this strategy is also applied in
all the baselines. In the training phase, we set λ = 0.1 and train 1000 trials for all the methods. In
each trial, we randomly sample Nw categories with Ns randomly selected images for each support
set, and 16 images for the query set. We use the Adam optimizer with the learning rate 0.001.
4.2	Experimental Results
4.2.1	Generalization From One Source Domain
Table 1 shows the results of the first cross-domain setting that the model is trained on the dataset
mini-ImageNet, and tested on the other four datasets CUB, Cars, Places and Plantae, respectively. The
results demonstrate that with our proposed MemREIN method, the performance of all three metric-
based few-shot learning methods makes obvious improvements, which validates the effectiveness
of our proposed MemREIN method to enhance the generalization ability to the unseen domain.
In addition, our proposed method consistently outperforms other existing cross-domain few-shot
learning methods, which indicates the the superiority of our method over previous best methods.
4.2.2	Generalization From Multiple Source Domains
Table 2 shows the results under the leave-one-out setting. We first select out one dataset as the unseen
domain for testing and use the remaining three datasets as well as the dataset mini-ImageNet for
training since we already use the dataset mini-ImageNet for pre-training. Note that the baseline (Tseng
et al., 2020) has two different training strategies, one is the “learn to learn” strategy and another is
using fixed hyper-parameters. We consider the better results for comparison here, which is denoted
as “+LFT” in the Table 2. The results demonstrate that our proposed MemREIN method can greatly
2https://github.com/wyharveychen/CloserLookFewShot.
3https://github.com/hytseng0509/CrossDomainFewShot.
8
Under review as a conference paper at ICLR 2022
Table 4: Ablation study on our method and the objective function. We consider the dataset CUB as
the unseen domain. “GNN+IN” indicates that we only employ the instance normalization strategy,
“w/o Lr-cl” indicates that we remove the Lr-cl term, and “w/o Lr+cl” indicates that we remove the Lr+cl
term.
5-way 5-shot Variant ID	Classification Accuracy (%)	
	Method	CUB
1	GNN (Satorras & Estrach, 2018)	69.26 ± 0.68%
2	GNN+IN	67.34 ± 0.66%
3	GNN+MemREIN	77.54 ± 0.62%
4	w/o Lr-cl	75.38 ± 0.63%
5	W L+cl	73.02 ± 0.62%
improve the performance of all three metric-based few-shot learning methods, which reflects that our
method has the capability of mitigating the domain gap problem. In addition, results show that our
method consistently outperforms the “+LFT” method, which validates that our proposed method can
better capture the variation of feature distributions across multiple domains than the “+LFT” method,
thus the generalization ability of extracted features are better enhanced.
4.2.3	Compare With SOTA Methods
We also make comparisons to existing SOTA few-shot learning methods to further demonstrate the
superiority of our proposed method. The results are shown in Table 3. To ensure fair comparison,
all the methods are trained and evaluated only on the dataset mini-ImageNet. It is the conventional
few-shot learning setting and there is no unseen domain. The results indicate that our proposed
method can achieve competitive performance compared with SOTA few-shot learning methods.
4.2.4	Ablation Study
We make ablation study on our propose method to demonstrate the functionality of two main parts:
(1) instance normalization and (2) memorized restitution. We take the GNN baseline under the
leave-one-out setting (5-way 5-shot) on the dataset CUB as the example. Meanwhile, we also make
ablation study on our proposed reverse contrasive loss to demonstrate the functionality of two terms:
Lr+cl and Lr-cl. Table 7 indicates the results of the ablation study. Comparing the results of Variant 1
and 2, it indicates that only applying the instance normalization operation results in the decrease of the
accuracy. It is reasonable because the instance normalization operation is too “strong” to inevitably
remove some discriminative useful information. It also validates the necessity and effectiveness
of our proposed memorized restitution approach. Comparing the results of Variant 3, 4, and 5, it
indicates that both Lr+cl and Lr-cl contribute to the final results, which means our proposed reverse
contrasive loss is capable of promoting the disentanglement of features effectively.
5	Conclusion
In this paper, we investigated the cross-domain few-shot classification problem where exists the do-
main gap issue. We propose a novel framework, MemREIN, which considers Memorized, Restitution,
and Instance Normalization to address this issue. We first alleviate feature dissimilarity across sample
features via an instance normalization algorithm to enhance the overall generalization ability. In order
to avoid the loss of fine-grained discriminative knowledge between different classes, a memorized
restitution approach is further proposed to adaptively remember the long-term refined knowledge
and restitute the discrimination ability. Finally, A novel reverse contrastive learning strategy is
proposed to stabilize the distillation process. Extensive experiments on five popular benchmark
datasets demonstrate that MemREIN well addresses the domain shift challenge, and significantly
improves the performance up to 16.37% compared with state-of-the-art baselines.
9
Under review as a conference paper at ICLR 2022
References
Gilles Blanchard, Gyemin Lee, and Clayton Scott. Generalizing from several related classification
tasks to a new unlabeled sample. Proceedings of the Advances in Neural Information Processing
Systems, 24:2178-2186, 2011.
Karsten M Borgwardt, Arthur Gretton, Malte J Rasch, Hans-Peter Kriegel, Bernhard Scholkopf,
and Alex J Smola. Integrating structured biological data by kernel maximum mean discrepancy.
Bioinformatics, 22(14):e49-e57, 2006.
John Cai, Bill Cai, and Sheng Mei Shen. Sb-mtl: Score-based meta transfer-learning for cross-domain
few-shot learning. arXiv preprint arXiv:2012.01784, 2020.
Zhangjie Cao, Mingsheng Long, Jianmin Wang, and Michael I Jordan. Partial transfer learning with
selective adversarial networks. In Proceedings of the IEEE conference on computer vision and
pattern recognition, pp. 2724-2732, 2018a.
Zhangjie Cao, Lijia Ma, Mingsheng Long, and Jianmin Wang. Partial adversarial domain adaptation.
In Proceedings of the European Conference on Computer Vision, pp. 135-150, 2018b.
Chaofan Chen, Xiaoshan Yang, Changsheng Xu, Xuhui Huang, and Zhe Ma. ECKPN: Explicit class
knowledge propagation network for transductive few-shot learning. In Proceedings of the IEEE
Computer Vision and Pattern Recognition, pp. 6596-6605, June 2021.
Wei-Yu Chen, Yen-Cheng Liu, Zsolt Kira, Yu-Chiang Frank Wang, and Jia-Bin Huang. A closer look
at few-shot classification. In The International Conference on Learning Representations, 2019.
Zhengming Ding, Sheng Li, Ming Shao, and Yun Fu. Graph Adaptive Knowledge Transfer for
Unsupervised Domain Adaptation: 15th European Conference, Munich, Germany, September 8-14,
2018, Proceedings, Part II. Springer, Cham, 2018.
Vincent Dumoulin, Jonathon Shlens, and Manjunath Kudlur. A learned representation for artistic
style. In The International Conference on Learning Representations, 2016.
Bo Fu, Zhangjie Cao, Mingsheng Long, and Jianmin Wang. Learning to detect open classes for
universal domain adaptation. In Proceedings of the European Conference on Computer Vision, pp.
567-583. Springer, 2020.
Yuqian Fu, Yanwei Fu, and Yu-Gang Jiang. Meta-fdmixup: Cross-domain few-shot learning guided
by labeled target data. In Proceedings of the ACM International Conference on Multimedia, pp.
5326-5334, 2021.
Yaroslav Ganin and Victor Lempitsky. Unsupervised domain adaptation by backpropagation. In
Proceedings of International Conference on Machine Learning, pp. 1180-1189. PMLR, 2015.
Yaroslav Ganin, Evgeniya Ustinova, Hana Ajakan, Pascal Germain, Hugo Larochelle, Francois
Laviolette, Mario Marchand, and Victor Lempitsky. Domain-adversarial training of neural networks.
Journal of Machine Learning Research, 17(1):2096-2030, 2016.
Raghuraman Gopalan, Ruonan Li, and Rama Chellappa. Domain adaptation for object recognition:
An unsupervised approach. In Proceedings of the IEEE International Conference on Computer
Vision, pp. 999-1006, 2011.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image
recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,
pp. 770-778, 2016.
Kaiming He, Georgia Gkioxari, Piotr Dollar, and Ross Girshick. Mask R-CNN. In Proceedings of
the IEEE International Conference on Computer Vision, pp. 2961-2969, 2017.
Nathan Hilliard, Lawrence Phillips, Scott Howland, Artem Yankov, Courtney D Corley, and
Nathan O Hodas. Few-shot learning with metric-agnostic conditional embeddings. arXiv preprint
arXiv:1802.04376, 2018.
10
Under review as a conference paper at ICLR 2022
Judy Hoffman, Eric Tzeng, Taesung Park, Jun-Yan Zhu, Phillip Isola, Kate Saenko, Alexei Efros,
and Trevor Darrell. CyCADA: Cycle-consistent adversarial domain adaptation. In Proceedings of
International Conference on Machine Learning,pp. 1989-1998. PMLR, 2018.
Jie Hu, Li Shen, and Gang Sun. Squeeze-and-excitation networks. In Proceedings of the IEEE
Computer Vision and Pattern Recognition, pp. 7132-7141, 2018.
Xun Huang and Serge Belongie. Arbitrary style transfer in real-time with adaptive instance normal-
ization. In Proceedings of the IEEE International Conference on Computer Vision, pp. 1501-1510,
2017.
Ashraful Islam, Chun-Fu Chen, Rameswar Panda, Leonid Karlinsky, Rogerio Feris, and Richard J
Radke. Dynamic distillation network for cross-domain few-shot recognition with unlabeled data.
In Proceedings of the Advances in Neural Information Processing Systems, 2021.
Xin Jin, Cuiling Lan, Wenjun Zeng, Zhibo Chen, and Li Zhang. Style normalization and restitution
for generalizable person re-identification. In Proceedings of the IEEE Computer Vision and Pattern
Recognition, pp. 3143-3152, 2020.
Xin Jin, Cuiling Lan, Wenjun Zeng, and Zhibo Chen. Style normalization and restitution for domain
generalization and adaptation. arXiv preprint arXiv:2101.00588, 2021.
Jongmin Kim, Taesup Kim, Sungwoong Kim, and Chang D Yoo. Edge-labeling graph neural network
for few-shot learning. In Proceedings of the IEEE Computer Vision and Pattern Recognition, pp.
11-20, 2019.
Jonathan Krause, Michael Stark, Jia Deng, and Li Fei-Fei. 3d object representations for fine-
grained categorization. In Proceedings of the IEEE International Conference on Computer Vision
Workshops, pp. 554-561, 2013.
Abhishek Kumar, Prasanna Sattigeri, Kahini WadhaWan, Leonid Karlinsky, Rogerio Schmidt Feris,
Bill Freeman, and Gregory W Wornell. Co-regularized alignment for unsupervised domain
adaptation. In Proceedings of the Advances in Neural Information Processing Systems, 2018.
Jogendra Nath Kundu, Nishank Lakkakula, and R Venkatesh Babu. UM-Adapt: Unsupervised multi-
task adaptation using adversarial cross-task distillation. In Proceedings of the IEEE International
Conference on Computer Vision, pp. 1436-1445, 2019.
Jogendra Nath Kundu, Naveen Venkat, R Venkatesh Babu, et al. Universal source-free domain
adaptation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,
pp. 4544-4553, 2020.
Brenden M Lake, Ruslan Salakhutdinov, and Joshua B Tenenbaum. Human-level concept learning
through probabilistic program induction. Science, 350(6266):1332-1338, 2015.
KWonjoon Lee, Subhransu Maji, Avinash Ravichandran, and Stefano Soatto. Meta-learning With
differentiable convex optimization. In Proceedings of the IEEE Computer Vision and Pattern
Recognition, pp. 10657-10665, 2019.
HanWen Liang, Qiong Zhang, Peng Dai, and JuWei Lu. Boosting the generalization capability in
cross-domain feW-shot learning via noise-enhanced supervised autoencoder. In Proceedings of the
IEEE International Conference on Computer Vision, pp. 9424-9434, 2021.
Yann Lifchitz, Yannis Avrithis, Sylvaine Picard, and Andrei Bursuc. Dense classification and
implanting for feW-shot learning. In Proceedings of the IEEE Computer Vision and Pattern
Recognition, pp. 9258-9267, 2019.
Mingsheng Long, Jianmin Wang, Guiguang Ding, Jiaguang Sun, and Philip S Yu. Transfer feature
learning With joint distribution adaptation. In Proceedings of the IEEE International Conference
on Computer Vision, pp. 2200-2207, 2013.
Mingsheng Long, Yue Cao, Jianmin Wang, and Michael Jordan. Learning transferable features With
deep adaptation netWorks. In Proceedings of the Proceedings of Proceedings of International
Conference on Machine Learning, pp. 97-105. PMLR, 2015.
11
Under review as a conference paper at ICLR 2022
Saeid Motiian, Marco Piccirilli, Donald A Adjeroh, and Gianfranco Doretto. Unified deep supervised
domain adaptation and generalization. In Proceedings of the IEEE International Conference on
Computer Vision,pp. 5715-5725, 2017.
Krikamol MUandeL David Balduzzi, and Bernhard Scholkopf. Domain generalization via invariant
feature representation. In Proceedings of the International Conference on Machine Learning, pp.
10-18. PMLR, 2013.
Boris N Oreshkin, Pau Rodriguez, and Alexandre Lacoste. TADAM: Task dependent adaptive metric
for improved few-shot learning. In Proceedings of the Advances in Neural Information Processing
Systems, pp. 719-729, 2018.
Pau Panareda Busto and Juergen Gall. Open set domain adaptation. In Proceedings of the IEEE
International Conference on Computer Vision, pp. 754-763, 2017.
Cheng Perng Phoo and Bharath Hariharan. Self-training for few-shot transfer across extreme task
differences. In The International Conference on Learning Representations, 2020.
Siyuan Qiao, Chenxi Liu, Wei Shen, and Alan L Yuille. Few-shot image recognition by predicting
parameters from activations. In Proceedings of the IEEE Computer Vision and Pattern Recognition,
pp. 7229-7238, 2018.
Sachin Ravi and Hugo Larochelle. Optimization as a model for few-shot learning. 2016.
Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun. Faster R-CNN: Towards real-time object
detection with region proposal networks. IEEE Transactions on Pattern Analysis and Machine
Intelligence, 39(6):1137-1149, 2016.
Andrei A Rusu, Dushyant Rao, Jakub Sygnowski, Oriol Vinyals, Razvan Pascanu, Simon Osindero,
and Raia Hadsell. Meta-learning with latent embedding optimization. In The International
Conference on Learning Representations, 2018.
Kuniaki Saito and Kate Saenko. Ovanet: One-vs-all network for universal domain adaptation. In
Proceedings of the IEEE International Conference on Computer Vision, 2021.
Kuniaki Saito, Donghyun Kim, Stan Sclaroff, and Kate Saenko. Universal domain adaptation through
self supervision. Proceedings of the Advances in Neural Information Processing Systems, 33,
2020.
Victor Garcia Satorras and Joan Bruna Estrach. Few-shot learning with graph neural networks. In
The International Conference on Learning Representations, 2018.
Shiv Shankar, Vihari Piratla, Soumen Chakrabarti, Siddhartha Chaudhuri, Preethi Jyothi, and Sunita
Sarawagi. Generalizing across domains via cross-gradient training. In The International Conference
on Learning Representations, 2018.
Jake Snell, Kevin Swersky, and Richard Zemel. Prototypical networks for few-shot learning. In
Proceedings of the Advances in Neural Information Processing Systems, pp. 4080-4090, 2017.
Baochen Sun and Kate Saenko. Deep CORAL: Correlation alignment for deep domain adaptation.
In Proceedings of the European Conference on Computer Vision, pp. 443-450. Springer, 2016.
Jiamei Sun, Sebastian Lapuschkin, Wojciech Samek, Yunqing Zhao, Ngai-Man Cheung, and Alexan-
der Binder. Explanation-guided training for cross-domain few-shot classification. In Proceedings
of the International Conference on Pattern Recognition, pp. 7609-7616, 2021.
Flood Sung, Yongxin Yang, Li Zhang, Tao Xiang, Philip HS Torr, and Timothy M Hospedales.
Learning to compare: Relation network for few-shot learning. In Proceedings of the IEEE on
Computer Vision and Pattern Recognition, pp. 1199-1208, 2018.
Shixiang Tang, Dapeng Chen, Lei Bai, Kaijian Liu, Yixiao Ge, and Wanli Ouyang. Mutual crf-gnn
for few-shot learning. In Proceedings of the IEEE Computer Vision and Pattern Recognition, pp.
2329-2339, June 2021.
12
Under review as a conference paper at ICLR 2022
Hung-Yu Tseng, Hsin-Ying Lee, Jia-Bin Huang, and Ming-Hsuan Yang. Cross-domain few-shot
classification via learned feature-wise transformation. In The International Conference on Learning
Representations, 2020.
Eric Tzeng, Judy Hoffman, Trevor Darrell, and Kate Saenko. Simultaneous deep transfer across
domains and tasks. In Proceedings of the IEEE International Conference on Computer Vision, pp.
4068-4076, 2015.
Eric Tzeng, Judy Hoffman, Kate Saenko, and Trevor Darrell. Adversarial discriminative domain
adaptation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,
pp. 7167-7176, 2017.
Dmitry Ulyanov, Andrea Vedaldi, and Victor Lempitsky. Instance normalization: The missing
ingredient for fast stylization. arXiv preprint arXiv:1607.08022, 2016.
Laurens Van der Maaten and Geoffrey Hinton. Visualizing data using t-sne. Journal of Machine
Learning Research, 9(11), 2008.
Grant Van Horn, Oisin Mac Aodha, Yang Song, Yin Cui, Chen Sun, Alex Shepard, Hartwig Adam,
Pietro Perona, and Serge Belongie. The inaturalist species classification and detection dataset. In
Proceedings of the IEEE on Computer Vision and Pattern Recognition, pp. 8769-8778, 2018.
Oriol Vinyals, Charles Blundell, Timothy Lillicrap, Koray Kavukcuoglu, and Daan Wierstra. Match-
ing networks for one shot learning. In Proceedings of the Advances in Neural Information
Processing Systems, pp. 3637-3645, 2016.
Riccardo Volpi, Hongseok Namkoong, Ozan Sener, John Duchi, Vittorio Murino, and Silvio Savarese.
Generalizing to unseen domains via adversarial data augmentation. In Proceedings of the Advances
in Neural Information Processing Systems, pp. 5339-5349, 2018.
Catherine Wah, Steve Branson, Peter Welinder, Pietro Perona, and Serge Belongie. The caltech-ucsd
birds-200-2011 dataset. 2011.
Haoqing Wang and Zhi-Hong Deng. Cross-domain few-shot classification via adversarial task
augmentation. In Proceedings of the International Joint Conference on Artificial Intelligence, pp.
1075-1081, 8 2021.
Liu Yanbin, Lee Juho, Park Minseop, Kim Saehoon, Yang Eunho, Hwang Sungju, and Yang Yi.
Learning to propagate labels: Transductive propagation network for few-shot learning. In The
International Conference on Learning Representations, 2019.
Baoyao Yang, Andy J. Ma, and Pong C. Yuen. Learning domain-shared group-sparse representation
for unsupervised domain adaptation. Pattern Recognition, pp. S0031320318301614, 2018.
Ling Yang, Liangliang Li, Zilun Zhang, Xinyu Zhou, Erjin Zhou, and Yu Liu. Dpgn: Distribution
propagation graph network for few-shot learning. In Proceedings of the IEEE Computer Vision
and Pattern Recognition, pp. 13390-13399, 2020.
Kaichao You, Mingsheng Long, Zhangjie Cao, Jianmin Wang, and Michael I Jordan. Universal
domain adaptation. In Proceedings of the IEEE Conference on Computer Vision and Pattern
Recognition, pp. 2720-2729, 2019.
Jing Zhang, Zewei Ding, Wanqing Li, and Philip Ogunbona. Importance weighted adversarial nets
for partial domain adaptation. In Proceedings of the IEEE Conference on Computer Vision and
Pattern Recognition, pp. 8156-8164, 2018.
Bolei Zhou, Agata Lapedriza, Aditya Khosla, Aude Oliva, and Antonio Torralba. Places: A 10
million image database for scene recognition. IEEE Transactions on Pattern Analysis and Machine
Intelligence, 40(6):1452-1464, 2017.
Yixiong Zou, Shanghang Zhang, JianPeng Yu, Yonghong Tian, and Jose MF Moura. Revisiting
mid-level patterns for cross-domain few-shot recognition. In Proceedings of the ACM International
Conference on Multimedia, PP. 741-749, 2021.
13
Under review as a conference paper at ICLR 2022
A Dataset Details
We make evaluations on five public datasets that are commonly used in few-shot classification
task: mini-ImageNet (Vinyals et al., 2016), CUB (Wah et al., 2011), Cars (Krause et al., 2013),
Places (Zhou et al., 2017), and Plantae (Van Horn et al., 2018). Note that the procedure of all the
datasets are exactly the same as baselines to ensure the fair comparison. The origins of these five
datasets are introduced as follows:
Table 5: Statistical summary of five datasets.
Number of Category	Dataspf				
	mini-ImageNet	CUB	Cars	Places	Plantae
Training	64	100	98	183	100
Validation	16	50	49	91	50
Testing	20	50	49	91	50
•	mini-ImageNet: It was proposed by (Vinyals et al., 2016), it consists of 60, 000 colour
images of size 84 × 84 with 100 classes, each class haves 600 instances. The dataset process
is the same with (Ravi & Larochelle, 2016).
•	CUB 4 : It is a challenging dataset of 200 bird species (mostly North American). The same
with the setting in (Hilliard et al., 2018), this dataset is divided into 100 classes for training,
50 for validation, and 50 for testing. Each image is again resized to 84 × 84 pixels and
put through the data augmentation process described previously to reduce over-fitting. The
original of these five datasets are introduced as follows:
•	Car 5: It contains 16, 185 images of 196 classes of cars. Classes are typically at the level of
Make, Model, Year, e.g. 2012 Tesla Model S or 2012 BMW M3 coupe. The dataset spilt
setting applied in cross-domain few-shot learning task is the same with (Tseng et al., 2020).
•	Places: It contains 1.8 million images from 365 scene categories, where there are at most
5000 images per category. The dataset setting applied in cross-domain few-shot learning
task is the same with (Tseng et al., 2020).
•	Plantae: It is a sub-category of dataset iNaturalist 6 that was originally developed for species
classification and detection. The dataset process setting is the same with (Tseng et al., 2020).
B	Different Numbers of Ways
We consider a more practical situation that Nw may be different from that at the training stage. It
also reflects the generalization ability of the model and results are shown in Table 6. Note that
GNN (Satorras & Estrach, 2018) requires the number of ways to be the same while the training and
testing, thus we evaluate with method MatchingNet (Vinyals et al., 2016) and RelationNet (Sung
et al., 2018) (MNet and RNet for short). The model is trained on the datasets mini-ImageNet, Cars,
Places, and Plantae and evaluated on the dataset CUB with different number of ways Nw . The
results indicate that our proposed method are still capable of improving the generalization ability to
the unseen domain with various numbers of ways. In addition, our proposed method consistently
outperforms the baseline (Tseng et al., 2020) that has considered the domain-shift issue, which
validates the superiority of our method.
C	Extra Model Analysis
As illustrated in Figure 2, we employ the t-SNE algorithm (Van der Maaten & Hinton, 2008) to
visualize features that obtained by the feature encoder “before/within/after” our MemREIN method,
where each color represents one class. We take the GNN baseline under the leave-one-out setting
4http:〃www.vision.caltech.edu/VisiPedia/CUB-200.html
5https://ai.stanford.edu/jkrause/cars/car_dataset.html
6https://www.inaturalist.org/.
14
Under review as a conference paper at ICLR 2022
Table 6: Classification Accuracy (%) of our proposed method with different Nw . We consider the
CUB dataset as the unseen domain under the leave-one-out setting.
5-shot	Classification Accuracy (%)
	2-way	5-way	10-way	20-way
MNet (Vinyals et al., 2016) MNet+LFT (Tseng et al., 2020) MNet+MemREIN (Ours) RNet (Sung et al., 2018)	78.46 ± 0.78%^^51.92 ± 0.80%^^38.22 ± 0.38%^^26.17 ± 0.24% 83.88 ± 0.72%	61.41 ± 0.57%	45.69 ± 0.39%	32.81 ± 0.23% 88.68 ± 0.68%	67.31 ± 0.51%	49.22 ± 0.34%	33.99 ± 0.22% 84.25 ± 0.72%^^62.13 ± 0.74%^^47.15 ± 0.40%^^34.52 ± 0.24%
RNet+LFT (Tseng et al., 2020) RNet+MemREIN (Ours)	85.44 ± 0.72%	64.99 ± 0.54%	49.90 ± 0.40%	37.20 ± 0.25% 89.12 ± 0.66%	68.39 ± 0.48% 52.85 ± 0.32%	42.82 ± 0.20%
(a) GNN
(b) GNN+IN
(c) GNN+MemREIN
(d) GNN
(e) GNN+IN
(f) GNN+MemREIN
Figure 2: t-SNE visualization of sample features extracted by encoder.
on the dataset CUB as the example. We randomly select 5 categories with 60 samples of each
category in the testing spilt of the dataset CUB. The first column indicates two examples of the
features from conventional GNN baseline, The second column indicates the features that only applied
the instance normalization operation, and the third column indicates the features that applied our
proposed MemREIN method. As shown in the first column, there exists several rough clusters and
but the boundaries are unclear. After instance normalization, the overall model generalization ability
of features is enhanced. In comparison with the second column and the third column, the features
leaned by our method are more clustered and separable, which validates the effectiveness of our novel
memorized restitution approach.
D	Ablation Study
We carry out ablation studies to validate the effectiveness of different components in our proposed
method. We compare with the GNN baseline under the leave-one-out setting (5-way 5-shot) and
results are shown in Table 7. Comparing the results of Variant 1 and 2, it indicates that only applying
the instance normalization operation results in the decrease of the accuracy. It is reasonable because
the instance normalization operation will inevitably remove some discriminative useful information.
In comparison with Variant 3 and 6, it validates the effectiveness of employing the memory bank on
feature D . Comparing Variant 3, 6, and 7, it indicates that when employing memory bank on feature
G, it would cause performance decrease. Experimentally, when applying the memory bank on the
feature D and directly using feature G has the best performance.
15
Under review as a conference paper at ICLR 2022
Table 7: Ablation study on our method. “GNN+IN” indicates that we only employ the instance
normalization strategy, “w/o Lr-cl” indicates that we remove the Lr-cl term, and “w/o Lr+cl” indicates
that we remove the Lr+cl term, “GNN+MemREIN w/o MB” represents that we remove memory bank
and directly use the feature map D, and “GNN+MemREIN (D&G)” represents that the memory
bank is operated both on feature map D and G (not shared).
5-way 5-shot	Classification Accuracy (%)
Variant ID	Method	CUB	Cars	Places	Plantae
1	GNN (Satorras & Estrach, 2018)	69.26 ± 0.68%	48.91 ± 0.67%	72.59 ± 0.67%	58.36 ± 0.68%
2	GNN+IN	67.34 ± 0.66%	42.76 ± 0.75%	67.82 ± 0.73%	54.04 ± 0.69%
3	GNN+MemREIN	77.54 ± 0.62%	56.78 ± 0.66%	78.84 ± 0.66%	65.44 ± 0.64%
4	w/o Lr-cl	75.38 ± 0.63%	55.34 ± 0.72%	78.03 ± 0.68%	65.22 ± 0.64%
5	w/o Lr+cl	73.02 ± 0.62%	51.45 ± 0.64%	73.26 ± 0.66%	62.22 ± 0.64%
6	GNN+MemREIN w/o MB	75.98 ± 0.62%	54.64 ± 0.66%	74.86 ± 0.68%	64.08 ± 0.68%
7	GNN+MemREIN (D&G)	76.02 ± 0.66%	55.26 ± 0.69%	78.08 ± 0.66%	64.84 ± 0.68%
Table 8: Performance study on the hyper-parameter λ.
5-way 5-shot GNN+MemREIN		Classification Accuracy (%)	
		CUB	Cars
λ=	0.01	77.02 ± 0.62%	56.12 ± 0.66%
λ=	0.1	77.54 ± 0.62%	56.78 ± 0.66%
λ=	0.5	77.34 ± 0.62%	56.66 ± 0.66%
λ	=1	76.78 ± 0.64%	56.22 ± 0.66%
E PERFORMANCE STUDY OF λ
We carry out performance study on the hyper-parameter λ. We take our method under the leave-one-
out setting (5-way 5-shot) and dataset CUB and Cars as the example. We set four different values
λ = {0.01, 0.1, 0.5, 1} and the results are shown in Table 8. It can be observed that when setting
λ = 0.1, it can achieve the best performance.
16