Under review as a conference paper at ICLR 2022
Path Integrals for the Attribution of Model
Uncertainties
Anonymous authors
Paper under double-blind review
Ab stract
Understanding model uncertainties is of key importance in Bayesian machine
learning applications. This often requires to meaningfully attribute a model’s pre-
dictive uncertainty to its input features, however, popular attribution methods are
primarily targeted at model scores for classification and regression tasks. Thus,
in order to explain uncertainties, state-of-the-art alternatives commonly procure
counterfactual feature vectors associated with low uncertainty and proceed by
making direct comparisons. Here, we present a novel algorithm for uncertainty at-
tribution in differentiable models, via path integrals which leverage in-distribution
curves connecting feature vectors to counterfactual counterparts. We validate our
method on benchmark image data sets with varying resolution, and demonstrate
that (i) it produces meaningful attributions that significantly simplify interpretabil-
ity over the existing alternatives and (ii) retains desirable properties from popular
attribution methods.
1 Introduction
Estimating and understanding model uncertainties is of key importance in Bayesian inferential set-
tings, which find applications in domains as diverse as natural language processing (Xiao & Wang,
2019), stochastic processes (Rao & Teg, 2013), network analysis (Perez et al., 2018) or image pro-
cessing (Kendall & Gal, 2017), to name only a few. In contrast with model scores, model uncertain-
ties manifest aspects of a system or data generating process that are not exactly known (Hullermeier
& Waegeman, 2021), and can be decomposed across aleatoric and epistemic components that help
scrutinize different aspects in the functioning of a model, and can facilitate interpretability or fair-
ness assessments in important machine learning applications (Awasthi et al., 2021).
Recently, there has been a growing interest in the study of methods for uncertainty estimation and de-
composition (e.g. Depeweg et al., 2018; Smith & Gal, 2018; Van Amersfoort et al., 2020; Tuna et al.,
2021) for purposes such as procuring adversarial examples, active learning or out-of-distribution
detection. Most importantly, recent work has proposed counterfactual mechanisms for the inter-
pretability of model uncertainties (Van Looveren & Klaise, 2019; Antoran et al., 2021; Schut et al.,
2021), as well as their attribution to individual input features, such as pixels in an image. These
methods proceed by identifying small adversarial or in-distribution variations in the raw input, s.t.
predictive uncertainties in a model output are reduced. Then, attributions to individual pixels, words
or categories are commonly assigned by direct comparison. This can facilitate the understanding of
the strengths and weaknesses of varied probabilistic models, however, the optimization task to pro-
duce such counterfactuals requires a good balance between reducing uncertainties and minimising
changes to original features, which is hard to achieve in practice. Most importantly, these methods do
not satisfy commonly desired properties associated with modern importance attribution techniques
(Sundararajan et al., 2017), such as completeness or implementation invariance.
In this paper, we present a novel framework for the attribution of predictive uncertainties, applicable
to Bayesian differentiable models. We leverage path integrals (Sundararajan et al., 2017) along with
in-distribution curves (Jha et al., 2020), and we propose aggregating attributions over paths starting
at a reference counterfactual which bears no predictive uncertainty. We ensure that completeness
and additional desirable properties are satisfied, hence, uncertainties are completely explained by
(and decomposed over) pixels in an image. We validate our approach by direct comparison with
recently introduced counterfactual CLUE explanations (Antoran et al., 2021), as well as popular
1
Under review as a conference paper at ICLR 2022
interpretability methods, such as integrated gradients (Sundararajan et al., 2017), LIME (Ribeiro
et al., 2016) or kernelSHAP (Lundberg & Lee, 2017), which we adapt for the attribution of predictive
uncertainties. Experiments1on benchmark image data sets show that, in comparison to competing
alternatives, our proposed method offers sparse and easily interpretable attributions, always limited
to relevant super-pixels. Thus, we offer improved means to understand the interplay between raw
inputs and aleatoric/epistemic uncertainties in deep models.
2 Uncertainty attributions
WefocUs our presentation on a classification task with a neural classifier f : Rn ×W → ∆lCl-1 of a
fixed architecture. The classifier maps feature vectors x ∈ Rn along with network weights w ∈ W
to an element in the standard (|C| - 1)-simplex, which represents membership probabilities across
classes in a set C. On training f within an (approximate) Bayesian setting, we commonly obtain a
posterior over the hypothesis space of models, i.e. a distribution ∏(w∣D) over weights conditioned
on the available train data D = {xi, ci}i=1,2,  Popular approaches to procure such posterior often
differ in their approach to incorporate prior knowledge and include dropout (Srivastava et al., 2014),
Bayes-by-Backprop (Blundell et al., 2015) or SG-HMC (Springenberg et al., 2016).
A model score for classification with a new data point x? ∈ Rn is derived from the posterior
predictive distribution by marginalising over posterior weights, i.e.
π(x*∣D) = [f(x*, w)π(w∣D)dw = Ew∣d[f(x*, w)],
and is easily approximated as 焉 PN=I f (x?, Wi), with weight samples Wi 〜 ∏(w∣D), i
1, . . . , N . In the following, we are concerned with the entropy as a measure of uncertainty:
H(x|D) = -EEw∣d[fc(x, w)] ∙ logEw∣d[fc(x, w)]
c∈C
(1)
(2)
where fc(x, w) represents the probability of class-c membership.
Remark. Concepts in this paper trivially extend to varied representations of uncertainty in classi-
fication and regression settings. Details are omitted for simplicity in the presentation.
The entropy term in (2) may further be decomposed through the law of iterated variances (Kendall
& Gal, 2017) so as to yield an aleatoric term
Ha(x∣D) = Ew|D [H(x, w)] = - X Ew|D [fc(x, w) ∙ log fc(x, w)],
c∈C
which measures the mean predictive entropy across models in the posterior hypothesis space, as
well as the mutual information or epistemic term, He(x|D) = H (x|D) - Ha(x|D) that represents
model uncertainty projected into the latent membership vector ∏(x∣D). Intuitively, aleatoric un-
certainty represents natural stochastic variation in the observations over repeated experiments; on
the other hand, epistemic uncertainty is descriptive of model unknowns due to inadequate data or
inappropriate modelling choices.
2.1	Path integrated gradients
Path integrated gradients (IG) (Sundararajan et al., 2017) is a simple and popular method for impor-
tance attributions that differs from conventional feature removal and permutation techniques (Covert
et al., 2020), and is primarily targeted at image processing tasks. It is a practical and easy to imple-
ment alternative to layer-wise relevance propagation Montavon et al. (2019) or DeepLift (Shrikumar
et al., 2017); it retains desired properties including sensitivity and implementation invariance, and
has been extended to several adaptations (Smilkov et al., 2017; Jha et al., 2020).
Given a classifier f (∙, w) along with a feature vector x, path IG explains a model score f (x, w)
using an alternative fiducial vector x0 as a reference, which is presumably not associated with any
1Source code for reproducing our results can be found at the following link: [removed for blind review]
2
Under review as a conference paper at ICLR 2022
Figure 1: Example contributions to predictive uncertainty for a classification task in dogs versus cats
data. The image is compared to a fiducial black screen with entropy of 0.49 (aleatoric 0.39, epis-
temic 0.1). Importances are smoothed with a Gaussian filter (Σ = 3I). Red importances represent
contributions towards increasing uncertainty, while purple importances contribute towards decreas-
ing uncertainty.
class observed in the training data. The attributed importance at index or pixel i is given by
δ	f 1 ∂f(δ(α), W) ∂δi(α)
IGi(Xw)= J0 ^l-∂Γdα
so that Pi IGi(x|w) = f(x, w) - f(x0, w). The result follows from the fundamental theorem of
calculus for line integrals known as the gradient theorem. Here, δ : [0, 1] → Rn represents a curve
with endpoints at δ(0) = x0 and δ(1) = x.
In vanilla IG, δ is parametrized as a straight path between fiducial and image feature vectors, i.e.
δ(α) = x0 + α(x - x0), and the above simplifies to
IGi(x∣w) = (Xi-XO)X Z 1 S + lx -x0)，W) dα,
0	∂xi
so that importances are heavily influenced by differences in pixel values between x and x0 . How-
ever, a straight line often transitions the path x0	x out-of-distribution or outside the data-
manifold (Jha et al., 2020; Adebayo et al., 2020). Also, the fiducial choice is considered problematic
(Sundararajan et al., 2017) and generally defaults to a black background.
2.2	Integrated gradients with uncertainty
Commonly, the classifier f (∙, w) is presumed to be binary (Sundararajan et al., 2017) with model
scores constrained to the interval [0, 1]. However, the above logic for importance attribution does
easily generalize to multi-class Bayesian settings in the presence of uncertainty. Here, the posterior
predictive classifier π(x∣D) introduced in (1) accepts a path IG importance at index i given by
IGδz ʌ _ 1 E	∖ff (δ(α), w)] dδi(α) d
IGi(X) = J0 EwID[	∂δi(α)	i ^a^dα,
which represents a mean-average trajectory over the curve δ and follows from dominated conver-
gence. Most importantly, we may employ path IG to explain univariate measures of uncertainty
from the posterior predictive distribution over classes, as observed in Figure 1. Here,
IG-Hδ(X) = - X Z Zg ?(a) dα	⑶
i	c∈C 0	∂α
captures variations in predictions covering multiple classes, and is defined s.t.
∆i(α) = (1+logEw∣D[fc(δ(α), w)]) ∙ Ewp [讨常；W)]
attributes importances for the change in entropy between a fiducial point and a feature vector, and
∆i(α) = Ew∣D [(1 + log fc(δ(α), w)) ∙	W)]
3
Under review as a conference paper at ICLR 2022
is the analogue representation restricted to the aleatoric term. Any variation in epistemic uncertainty
is readily shown to be explained as the difference in importances between the above two terms.
In Figure 1, the goal is not to understand why the classifier suggests this picture refers to a dog;
instead, we comprehend why the model struggles to predict any single class with confidence, and
we notice that the leash and a human hand are problematic. Further examples may be found in
Appendix A; in all cases, importances have been smoothed with a Gaussian filter, averaging over
positive (increasing uncertainty) and negative (decreasing uncertainty) contributions. The attribu-
tions are easily computed by standard Bayesian procedures, approximating the inner expectations
with simulations, however, the choice of fiducial (black screen) and out-of-distribution path remain
controversial and a significant challenge in order to enable an intuitive understanding of predictive
uncertainties in our model, which may represent a barrier in applications (Antoran et al., 2021).
3	Methodology
Next, we describe the computational process summarized in Algorithm 1, which produces novel
in-distribution attributions of uncertainty for a feature vector X and predictive posterior ∏(x∣D)
in (1). We do so through the use of a counterfactual fiducial bearing no relation to causal inference
(Pearl, 2010). This counterfactual is an alternative vector x0 defined similarly to CLUEs in Antoran
et al. (2021), i.e. (i) in distribution and (ii) close to x according to some arbitrary distance metric.
However, we furthermore require that the class distribution ∏(x0 |D) bears close to 0 predictive
uncertainty. Intuitively, we construct IG attributions using finely tuned fiducial points, by comparing
ambiguous images to easily predicted counterparts that bear a significant resemblance.
Algorithm 1: Uncertainty attributions
input : Feature vector x, predictive posterior ∏(∙∣D) and uncertainty estimator H(∙).
Distance metric d(∙, ∙), VAE encoder φ(∙) and decoder ψ(∙).
Penalty λ >> 0 and learning rate ν > 0.
output: Uncertainty Attributions IG-Hi(x), i = 1, . . . , n.
Initialise z0 = Z = φμ (x);
Compute predicted class ^ = argmaxg ∏(x|D);
while L not converged do
I LJ d(ψ(z0), X)+ 2m Pj Zj + λlog∏c(ψ(z)∣D) and z0 - z0 - VVzL;
end
while L not converged do
I LJ d(ψ(z), X)+ 2m Pj Zj and Z J Z — VVzL;
end
Approximate IG-Hiδ(X), i = 1, . . . , n in (5) along δz0→z through trapezoidal integration.
To begin with, we assume the existence of a generative variational auto-encoder (VAE) composed
of an encoder φ : Rn → Rm and decoder ψ : Rm → Rn . As customary, the data-generating process
in Rm is unit-Gaussian with an arbitrary dimensionality m << n.
3.1	Domain of integration
The domain of integration must be an in-distribution curve across end-points X0 X. We select the
fiducial as a decoded image X0 = ψ(Z0), where Z0 is the solution to the constrained optimization
problem
z0 = arg min [d(ψ(z), x) +	^X zj]
z∈Rxm	m j
(4)
where Rm = {z ∈ Rm : ∣Iec — π(ψ(z)∣D)k < ε} for a small e > 0. Here, C = argmaxg ∏i(x∣D)
is the predicted class by our posterior predictive classifier, and ei is the unit indicator vector at index
i. The metric d(∙, ∙) may take multiple forms, such as the cross-entropy or mean absolute difference
over pixel values. The right-most term is the negative log-density (up to proportionality) of Z in
latent space; this ensures computational stability and restricts the search to be in-distribution. Hence,
We retrieve a counterfactual fiducial which (i) corresponds to the class prediction by ∏(x |D) and (ii)
4
Under review as a conference paper at ICLR 2022
Figure 2: Procedural sketch to generate a path
of integration. Here, fiducial z0 and recon-
struction z points are optimized in latent space
by gradient descent, starting initially from the
encoding of x (dashed lines). A connecting
straight path (in blue) is projected to the data-
manifold and augmented with an interpolating
component (in red).
Figure 3: An example of in-distribution curves
connecting fiducial (left-most) and real (right-
most) data points, on MNIST digits data. Digits
on the left bear no model uncertainty in classifi-
cation.
bears close to zero predictive uncertainty. In practice, we approximate (4) through an unconstrained
search with a large penalty on
dχ(ec,∏(ψ(z)∣D)) = -log ∏c(ψ(z)∣D),
i.e. the cross-entropy between the predicted class C and the membership vector π(ψ(z)∣D) given a
decoding ψ(z). We proceed by gradient descent initialised at φμ(x) (the encoder's mean).
We next parametrize a curve δ : [0, 1] → Rn by following the steps displayed in Figure 2, s.t.
δ(α) = ψ(z0 + α(z - z0)) where
Z = az∈mn hd(ψ(z), x) + 2m X z2i
is also optimised by gradient descent initialised at a starting point φμ(x). Note that this is the
same optimization problem as in (4) but without a constraint imposing a reduction in predictive
uncertainty. Consequently, the curve δ offers an in-distribution trajectory (Jha et al., 2020) between
δ(0) = ψ(z0) = x0 and a reconstruction δ(1) = ψ(z) of x. If the auto-encoder does not provide an
efficient reconstruction, a two-level auto-encoder (Dai & Wipf, 2019) can offer a viable alternative;
however, the domain of integration may be easily augmented through vanilla (straight path) IG
between the end-points ψ(z) x, and we display afew examples on MNIST digits within Figure 3.
Overall, changes in predictive entropy and model scores between a reconstruction ψ (z) and the
original counterpart x are not observed to be significant within our experiments.
3.2	Line integral for importance attribution
For simplicity, we restrict the formulae to the in-distribution component along the curve δ : [0, 1] →
Rn defined in Subsection 3.1, and we ignore the trivial straight path connecting ψ(z) x. Fol-
lowing (3), We now require the total differential of the entropy H(∙) wrt Z in latent space; however,
we wish to retrieve importances only for features x in the original data manifold within Rn . To this
end, the attribution at index i = 1, . . . , n is given by
IG-Hδ(x) = - XX(Zj-ZO)/1 ∆i(α) d'ψz0 +α(z — z0)) dα	⑸
c∈C j=1	0	∂zj
where ∆i (α) follows the definitions in (3) for both the entropy in (2) and its aleatoric term. In
Figure 4, we show an example that compares attributions in (5) versus a vanilla variant of integrated
gradients previously introduced in (3). There, we find a CelebA image (Liu et al., 2015) with tags
for the presence of a smile, arched eyebrows and no bags under the eyes, and notice a significant
reduction of noise and improvements in interpretability.
Finally, we note that the attribution method we present in Algorithm 1 is similarly defined for
any generic uncertainty term H(∙), whether in regression or classification settings. Intuitively, our
5
Under review as a conference paper at ICLR 2022
Figure 4: Comparison of uncertainty attributions for individual pixels on a CelebA image. We
compare predictive uncertainties for three Bayesian classifiers, which measure the presence (or lack)
of smiles (left), arched eyebrows (centre), and bags under eyes (right). Red pixels contribute by
increasing uncertainties, in green we find contributions towards decreasing uncertainties.
method computes the total derivative of H(∙) wrt Z through the original feature space, using the
chain rule, and later undertakes summation over contributions in latent space, which is easily man-
aged through vectorized operations.
3.3	Properties
Most importantly, due to path independence and noting that H(x0|D) ≈ 0 (by definition), impor-
tances drawn from any trajectory δ(∙) parametrized as in Subsection 3.1 will approximately account
for all of the uncertainty in a posterior predictive task, i.e.
H(x|D) ≈ / VH(δ(α)∣D)dα
0
n
X IG-Hiδ (x),
i=1
and this is commonly referred to as completeness. Additionally, the reliance on path derivatives
along with the rules of composite functions ensure that both fundamental axioms of sensitivity(b)
(i.e. dummy property) along with implementation invariance are inherited (see Friedman, 2004;
Sundararajan et al., 2017). Specifically, the attribution will be zero for any index which does not
mathematically influence the posterior classifier. Also, given a fixed VAE architecture, uncertainty
attributions are identical for distributionally equivalent posterior predictive classifiers. However, the
departure from vanilla straight paths in the data-manifold means that our proposed method is no
longer Symmetry preserving, i.e. any symmetric variables in X (according to the classifier π(x∣D)
in (1)) are not guaranteed to receive identical attributions.
4 Experiments
We presents results from applying our proposed methodology for uncertainty attributions to bench-
mark image data sets, including the repositories MNIST handwritten digitS (LeCun & Cortes, 2010),
faShion-MNIST (Xiao et al., 2017) and CelebA (Liu et al., 2015). In all cases, we use MC dropout in
order to procure approximate posteriors π(w∣D) for network weights in our (approximate) Bayesian
predictive models. Details on architectural choices, hyper-parameters, training regimes and pre-
processing may be found within Appendix B.
We compare our method to alternatives including vanilla integrated gradients (Sundararajan et al.,
2017) and CLUE (Antoran et al., 2021); as well as adaptations of LIME (Ribeiro et al., 2016) and
kernelSHAP (Lundberg & Lee, 2017), which we fine-tune in order to measure variances in uncer-
tainty instead of model scores. We use the following settings with the aforementioned algorithms:
•	Vanilla IG is implemented with a black fiducial and a straight line as domain of integration.
•	CLUE attributions are derived as the differential between a real image and its decoded
CLUE counterpart (cf. Antoran et al., 2021, Appendix F). The cost function weighs for
reconstruction and uncertainty terms are tuned on a validation set.
•	LIME is implemented through quickShift segmentation, with kernel 1, maximum distance
5 and ratio of 0.2. We use a binomial mask with deactivation probability 0.2, and LaSSo
regression to attribute importances.
6
Under review as a conference paper at ICLR 2022
•	SHAP proceeds through pixel/index coalitions of varying size; masked index points do
not default to ”black-background” or mean values, but are instead re-sampled from their
corresponding marginal distributions. We use Lasso regression to attribute importances.
In all experiments, we first produce scores for the data using the posterior predictive distribution
π(x∣D), next, We compute the entropy H(x|D). We rank data points by degree of uncertainty and
apply the attribution methods to the highest-ranking images or feature vectors. We show that, in
comparison to the alternatives, our proposed approach offers sparse and easily interpretable uncer-
tainty attributions limited to relevant super-pixels. Our attributions are furthermore dominated by
positive importances, i.e. pixels that contribute only to an increase in uncertainty and add up to the
total predictive uncertainty in classification.
4.1	MNIST handwritten digits
In Figure 5a We observe attributions of uncertainty on a selection of high-entropy MNIST digits.
These attributions are further decomposed across both aleatoric and epistemic terms, and additional
examples may be found in Appendix C. We display positive importances for pixels Which contribute
by increasing the uncertainty in classification according to our model. We note that our attribu-
tions are humanly interpretable and isolated to very feW pixels that confuse the predictive model;
commonly, these represent small regions Where dark ink is either missing or in excess.
(a)
(b)
Figure 5: (a) Uncertainty attributions With decomposition across aleatoric/epistemic components
and (b) comparison against popular methods, on MNIST digits.
In Figure 5b, We find a comparison of uncertainty attribution methods applied to the same digits,
With further examples also found in Appendix C. There, We only display attributions for the entropy,
and shoW both positive (in red) and negative (in green) contributions. Noticeably, our attributions
are strongly dominated by the feW pixels that contribute by increasing uncertainty. Comparatively,
LIME importances are highly restricted by the tuning of its segmentation algorithm. Similarly, ker-
nelSHAP requires significant re-sampling from the joint distribution of pixel coalitions, and strug-
gles to identify small super-pixels that are primary drivers of uncertainty. The CLUE methodology
is targeted (and efficient) at identifying interpretable in-distribution counterfactuals, hence, the im-
portances derived commonly suggest significantly re-draWing the original image, and this easily
overestimates the minimal changes required to facilitate predictions Without uncertainty. Finally,
vanilla IG cannot associate importances With background pixels, as these share values With the fidu-
cial image. Also, the fiducial is associated With high predictive uncertainty, thus, the attributions
offer a confusing mix of pixels that both decrease and increase uncertainties.
7
Under review as a conference paper at ICLR 2022
4.2	Fashion-MNIST dataset
In Figure 6, we observe a similar com-
parison of uncertainty attribution meth-
ods applied to a selection of high-entropy
Fashion-MNIST images, with further ex-
amples also found in Appendix C. We
note that our attributions are again inter-
pretable and restricted to missing or ex-
cess pixels, s.t. visual representations of
clothing items would be classified with-
out uncertainty. Similarly to the previous
example with digits, LIME importances
are severely restricted by the segmenta-
tion algorithm, additionally, kernelSHAP
is inconsistent and often struggles to iden-
tify small and meaningful super-pixels that
are drivers of uncertainty. CLUE impor-
tances suggest a significant re-drawing of
the original clothing pieces to make them
more similar to an easily classifiable coun-
terfactual, which again overestimates the
Figure 6: Comparison of uncertainty attribution meth-
ods on fashion-MNIST images.
extent of changes required in order to mitigate predictive uncertainty. Finally, attributions through
vanilla IG are confusing for the same reasons as outlined in our prior example.
4.3	CelebA dataset
Finally, we compute similar uncertainty attributions for classification scores with facial attributes
on CelebA images. To improve our presentation, we omit LIME importances here, however, these
may be found among further examples in Appendix C. In Figure 7a we find attributions for the class
label smile, whereas Figure 7b shows results for the class label arched eyebrows. In both cases,
we notice that our attributions are comparatively neat, interpretable and always restricted to facial
features around the mouth, cheeks or eyebrows, depending on the classification task. For instance,
our attributions can complement the difference between an uncertain smirk and an easily classifiable
smile; in non-smiling people, the attributions may bring attention to features around the cheek that
are commonly associated with smiles, and thus confuse the model.
(a)
(b)
Figure 7: Uncertainty attributions on CelebA images. (a) Pictures with the smile attribute labelled as
positive (top two pictures) or negative (bottom two pictures). (b) Pictures with the arched eyebrows
attribute labelled as negative (top two pictures) or positive (bottom two pictures)
8
Under review as a conference paper at ICLR 2022
Similarly to digits and fashion items in previous examples, our importances isolate the pixels for
facial features that seemingly contradict the predicted class by the Bayesian classifier. In compar-
ison, attributions through vanilla integrated gradients identify multiple artefacts, present a mix of
positive and negative importances, and are hard to interpret. KernelSHAP offers inconsistent results
that commonly highlight wide areas around the region of interest. Finally, CLUE importances visi-
bly struggle with higher resolution images, the reason for this being reliance on direct comparisons
between an image and its counterfactual reference. We also note that redrawing a high-fidelity face
reconstruction with an autoencoder is considerably more difficult than drawing digits or clothes.
5 Conclusion
In this paper, we have introduced a novel computational framework for the attribution of model un-
certainties in approximate Bayesian settings. Our experimental results show potential for facilitating
interpretations of the interplay between raw features and predictive uncertainties in complex deep
models, and can thus contribute to improved transparency and interpretability in deep learning ap-
plications. Our method shows considerable improvements over both simple baselines and existing
alternatives for uncertainty attribution in image processing tasks. Alternative methods are generally
based on direct comparisons of images versus adversarial or in-distribution counterfactuals, along
with subjective assessments. In our case, we leverage traditional path integral methods in order to
ensure that uncertainties are attributed respecting the desirable properties of completeness, sensitiv-
ity(b), and implementation invariance.
Acknowledgements
text removed for the blind review process.
References
Julius Adebayo, Michael Muelly, Ilaria Liccardi, and Been Kim. Debugging tests for model ex-
planations. In Advances in Neural Information Processing Systems, volume 33, pp. 700-712,
2020.
Javier Antoran, Umang Bhatt, Tameem Adel, Adrian Weller, and Jose MigUel Hernandez-Lobato.
Getting a CLUE: A method for explaining uncertainty estimates. In International Confer-
ence on Learning Representations, 2021. URL https://openreview.net/forum?id=
XSLF1XFq5h.
Pranjal Awasthi, Alex BeuteL Matthaus Kleindessner, Jamie Morgenstern, and XUezhi Wang. Eval-
uating fairness of machine learning models under uncertain and incomplete information. In Pro-
ceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency, pp. 206-
214, 2021.
Charles Blundell, Julien Cornebise, Koray Kavukcuoglu, and Daan Wierstra. Weight uncertainty in
neural network. In International Conference on Machine Learning, pp. 1613-1622, 2015.
Ian Covert, Scott Lundberg, and Su-In Lee. Feature removal is a unifying principle for model
explanation methods. arXiv preprint arXiv:2011.03623, 2020.
Bin Dai and David Wipf. Diagnosing and enhancing VAE models. In International Confer-
ence on Learning Representations, 2019. URL https://openreview.net/forum?id=
B1e0X3C9tQ.
Stefan Depeweg, Jose-Miguel Hernandez-Lobato, Finale Doshi-Velez, and Steffen Udluft. Decom-
position of uncertainty in bayesian deep learning for efficient and risk-sensitive learning. In
International Conference on Machine Learning, pp. 1184-1193. PMLR, 2018.
Eric J Friedman. Paths and consistency in additive cost sharing. International Journal of Game
Theory, 32(4):501-518, 2004.
9
Under review as a conference paper at ICLR 2022
Xianxu Hou, Linlin Shen, Ke Sun, and Guoping Qiu. Deep feature consistent variational autoen-
coder. In 2017 IEEE Winter Conference on Applications of Computer Vision (WACV), pp.1133-
1141, 2017. doi: 10.1109/WACV.2017.131.
Eyke Hullermeier and Willem Waegeman. Aleatoric and epistemic uncertainty in machine learning:
An introduction to concepts and methods. Machine Learning, 110(3):457-506, 2021.
Anupama Jha, Joseph K Aicher, Matthew R Gazzara, Deependra Singh, and Yoseph Barash. En-
hanced integrated gradients: improving interpretability of deep learning models using splicing
codes as a case study. Genome biology, 21(1):1-22, 2020.
Alex Kendall and Yarin Gal. What uncertainties do we need in bayesian deep learning for computer
vision? In Advances in Neural Information Processing Systems, volume 30, 2017.
Yann LeCun and Corinna Cortes. MNIST handwritten digits database, 2010. http://yann.
lecun.com/exdb/mnist/.
Ziwei Liu, Ping Luo, Xiaogang Wang, and Xiaoou Tang. Deep learning face attributes in the wild.
In Proceedings of International Conference on Computer Vision (ICCV), December 2015.
Scott M. Lundberg and Su-In Lee. A unified approach to interpreting model predictions. In Pro-
ceedings of the 31st International Conference on Neural Information Processing Systems, pp.
4768-4777, 2017.
Gregoire Montavon, Alexander Binder, Sebastian Lapuschkin, Wojciech Samek, and Klaus-Robert
Muller. Layer-wise relevance propagation: an overview. Explainable AI: interpreting, explaining
and visualizing deep learning, pp. 193-209, 2019.
Judea Pearl. Causal inference. In Proceedings of Workshop on Causality: Objectives and Assessment
at NIPS 2008, pp. 39-58, 2010.
Iker Perez, David Hodge, and Theodore Kypraios. Auxiliary variables for bayesian inference in
multi-class queueing networks. Statistics and Computing, 28(6):1187-1200, 2018.
Vinayak Rao and Yee Whte Teg. Fast mcmc sampling for markov jump processes and extensions.
Journal of Machine Learning Research, 14(11), 2013.
Marco Tulio Ribeiro, Sameer Singh, and Carlos Guestrin. ” why should i trust you?” explaining the
predictions of any classifier. In Proceedings of the 22nd ACM SIGKDD international conference
on knowledge discovery and data mining, pp. 1135-1144, 2016.
Lisa Schut, Oscar Key, Rory Mc Grath, Luca Costabello, Bogdan Sacaleanu, Yarin Gal, et al. Gener-
ating interpretable counterfactual explanations by implicit minimisation of epistemic and aleatoric
uncertainties. In International Conference on Artificial Intelligence and Statistics, pp. 1756-1764.
PMLR, 2021.
Avanti Shrikumar, Peyton Greenside, and Anshul Kundaje. Learning important features through
propagating activation differences. In International Conference on Machine Learning, pp. 3145-
3153, 2017.
Daniel Smilkov, Nikhil Thorat, Been Kim, Fernanda B. Viegas, and Martin Wattenberg. Smooth-
grad: removing noise by adding noise. CoRR, 2017.
Lewis Smith and Yarin Gal. Understanding measures of uncertainty for adversarial example detec-
tion. In Proceedings of the Thirty-Fourth Conference on Uncertainty in Artificial Intelligence,
UAI, 2018.
Jost Tobias Springenberg, Aaron Klein, Stefan Falkner, and Frank Hutter. Bayesian optimization
with robust bayesian neural networks. Advances in neural information processing systems, 29:
4134-4142, 2016.
Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov.
Dropout: a simple way to prevent neural networks from overfitting. The journal of machine
learning research, 15:1929-1958, 2014.
10
Under review as a conference paper at ICLR 2022
Mukund Sundararajan, Ankur Taly, and Qiqi Yan. Axiomatic attribution for deep networks. In
International Conference on Machine Learning, pp. 3319-3328. PMLR, 2017.
Omer Faruk Tuna, Ferhat Ozgur Catak, and M Taner Eskil. Exploiting epistemic uncertainty of the
deep learning models to generate adversarial samples. arXiv preprint arXiv:2102.04150, 2021.
Joost Van Amersfoort, Lewis Smith, Yee Whye Teh, and Yarin Gal. Uncertainty estimation using a
single deep deterministic neural network. In International Conference on Machine Learning, pp.
9690-9700. PMLR, 2020.
Arnaud Van Looveren and Janis Klaise. Interpretable counterfactual explanations guided by proto-
types. arXiv preprint arXiv:1907.02584, 2019.
Han Xiao, Kashif Rasul, and Roland Vollgraf. Fashion-MNIST: a novel image dataset for bench-
marking machine learning algorithms. arXiv preprint arXiv:1708.07747, 2017. Data available at
https://github.com/zalandoresearch/fashion- mnist.
Yijun Xiao and William Yang Wang. Quantifying uncertainties in natural language processing tasks.
In Proceedings of the AAAI Conference on Artificial Intelligence, volume 33, pp. 7322-7329,
2019.
11
Under review as a conference paper at ICLR 2022
A Vanilla IG for uncertainty attribution
In Figure 8 we find further examples of vanilla IG attributions of uncertainty using a straight path
between a black background and Dogs versus Cats pictures. Importances are smoothed with a
Gaussian filter (Σ = 3I) to average over positive and negative contributions. Red importances
represent contributions towards increasing uncertainty, in purple, contributions towards decreasing
uncertainty
Class: Cat, P.P. Entropy: 0.0
Class: Dogr P.P. Entropy: 0.0
Class: Dog, P P Entropy: 0 0
Increase Uπcrt.	Decrease Uncert
Figure 8: Vanilla uncertainty attributions using a straight path. Classes predicted with 0 posterior
predictive entropy. Importances reflect a reduction in entropy from a black background fiducial.
12
Under review as a conference paper at ICLR 2022
B S ummary of model architectures and hyper-parameters
The following is a comprehensive summary of model architecture choices, hyper-parameters, train-
ing regimes and further details used within the experiments in this paper. All of our models are
implemented through Keras, and the source code to reproduce the results may be found at url re-
moved for the blind review process
B.1	MNIST handwritten digits
Classifier. Our digits classifier is a convolutional neural network with max-pooling layers and
dropout, structured as:
•	Two convolutional layers of kernel size 3 × 3 and relu activation; the filter counts are 32
and 64 for the first and second layers. Each convolutional layer uses a stride length of 1
and is followed by a max-pooling layer of pool size 2 × 2.
•	The output above is flattened and fed through a dense layer of 128 neurons with relu ac-
tivation, followed by dropout with deactivation rate of 0.5, and a final softmax regression
layer for categorical outputs.
The classifier is fitted in order to minimize the categorical cross entropy wrt the train labels, using
the Adam optimizer, over 10 epochs, with a constant learning rate of 1e-3 and with batch size of 32.
Autoencoder. The variational autoencoder that facilitates a structured latent representation of digits
also relies on convolution and deconvolution layers. The encoder is structured as:
•	Two convolutional layers of kernel size 3 × 3, stride length 2 and relu activation; the filter
counts are 32 and 64 for the first and second layers.
•	A dense layer of 128 neurons that is fed with the above flattened output; using a relu
activation.
•	Two linear dense layers mapping the above 128 neurons to both a distributional mean vec-
tor and a log-standard-deviation vector, which define the multivariate distributional map-
ping in latent space for an image, of dimension 16.
•	A sampling operation from a normal distribution, used to create a random output from the
afore-defined distributional parameters.
In addition, the decoder is defined as:
•	A dense layer with relu activation, mapping a latent element to a vector of dimensionality
7 × 7 × 64.
•	Two deconvolutional layers of kernel size 3 × 3, stride length 2 and relu activation; the filter
counts are 64 and 32 for the first and second layers.
•	An output deconvolutional layer of kernel size 3 × 3, filter counts 1, stride length 1 and
sigmoid activation for pixel values; which reconstructs a digit image.
The autoencoder is fitted in order to minimize a custom loss, accounting for a digit reconstruction
term (through a cross-entropy loss) along with the Kullback-Leibler divergence among latent map-
pings for each image and a multivariate normal distribution N(0, I). We use the Adam optimizer,
over 50 epochs, with a constant learning rate of 1e-3 and with batch size of 32.
B.2	Fashion-MNIST dataset
The categorical classifier in this task is defined similarly as in the previous example with MNIST
handwritten digits. However, we add two additional dropout layers (with dropout probability 0.5)
after each of the max-pooling operations. The variational autoencoder used in this exercise is identi-
cal to the one above. In both cases, training proceeds with the Adam optimizer, at a constant learning
rate of 1e-3 with batch size 32. The classifier is trained for 10 epochs using the cross-entropy as the
cost function, while the autoencoder is trained for 50 epochs using a combination of binary cross-
entropy (as reconstruction loss) and the Kullback-Leibler divergence (as a regularisation term for
organising latent representations).
13
Under review as a conference paper at ICLR 2022
B.3	CelebA dataset
In this task, images are centred around the face and cropped to size 128 × 128, further standardized
to pixel values in the range [0, 1]. During training, we leverage data augmentation with random
rotations; we use a maximum angle of ±18 degrees, random translation by a maximum factor of 0.1
and random horizontal flip.
Classifier. The classifier is composed of 6 convolutional blocks followed by a fully connected dense
layer with softmax activation. Each convolutional block utilizes a kernel size of 3 and stride size
1, along with batch normalization, dropout with deactivation probability of 0.2, relu activation and
max-pooling (pool size 2 and stride 2). The number of channels at the output of each convolutional
layer is, respectively, 32, 64, 128, 128, 256 and 256. The last convolutional block is followed by a
flattening operation and a dropout layer with deactivation probability 0.4.
We train this classifier for 5 epochs using the Adam optimizer with batch size 64 and the cross-
entropy as cost function. The learning rate is decreased after each epoch by a factor of 0.8; starting
from 1e-4 for the smiling and arched eyebrows classifiers, and 3e-5 for the bags under eyes classi-
fier.
Autoencoder. The encoder within the variational autoencoder is composed by a series of 5 convolu-
tional blocks. Each block shares the same structure, with kernel size 3, stride 2, batch normalization
and leaky-relu activation with negative slope coefficient of 0.3. The number of filters at the output
of each block is 32, 64, 128, 256 and 512. After the last block we insert a flattening layer and two
dense layers each with 256 output neurons, which define the multivariate distributional mapping in
latent space for an image.
The decoder consists of a fully connected dense layer with 80192 output neurons (reshaped into a
4 × 4 × 512 activation map) followed by 5 up-sampling blocks. Each block up-samples the input
by a factor 2 and feeds it into a convolutional layer with kernel size 3 and stride 1, followed by
batch normalisation and leaky-relu activation with 0.3 negative slope coefficient. The number of
channels at the output of each block are 256, 128, 64, 32 and 3 respectively. We apply an additional
convolutional layer with kernel size 3, stride 1, 3 output channels and sigmoid activation for a final
reconstructed RGB image with values restricted in the [0, 1] interval.
The variational autoencoder is trained for 100 epochs using the Adam optimizer, with batch size 64
and a learning rate of 5e-4 which is decreased after each epoch by a factor of 0.98. We use a per-
ceptual loss function together with the Kullback-Leibler divergence regularisation term, following
details on (Hou et al., 2017) (VAE-123 model).
14
Under review as a conference paper at ICLR 2022
C Additional experimental results
The following are visualizations of importance attributions derived from our work, which comple-
ment the experimental results presented within the main body of the paper. In all cases, the results
are derived using the same model specifications as described above. Fiducial choices and configura-
tions of competing methods are all implemented as described in Section 4 within the main body of
text.
C.1 MNIST handwritten digits
In Figure 9 we find further examples of uncertainty attributions applied to MNIST digits, using
the method described in Algorithm 1. The attributions are decomposed across both aleatoric and
epistemic terms, and display positive importances for pixels which contribute by increasing the
uncertainty in classification according to our model. Digits selected represent a range of numbers
that have shown high predictive uncertainty according to our model.
3Q5βo
μr¼y 3 a 6 / b。
Figure 9: Attributions of uncertainty on MNIST digits.
In Figure 10 we display further comparisons of uncertainty attributions methods in application to
MNIST digits with predictive uncertainty. We only display attributions for the entropy, and show
both positive (increase uncertainty, in red) and negative (decrease uncertainty, in green) contribu-
tions.
15
Under review as a conference paper at ICLR 2022

Figure 10: Comparison of uncertainty attributions methods on MNIST digits.
Figure 11: Comparison of uncertainty attributions methods on MNIST-fashion images.
C.2 Fashion-MNIST dataset
Next, in Figure 11 we display further comparisons of our proposed algorithm along versus compet-
ing uncertainty attributions methods, in application to MNIST-fashion images with high predictive
uncertainty. Red attributions contribute by increasing uncertainty; green attributions represent con-
tributions towards decreasing uncertainty.
C.3 CelebA dataset
Finally, within Figures 12, 13 and 14 we display further comparisons of uncertainty attribution meth-
ods in application to CelebA images with high predictive uncertainty. The figures show attributions
16
Under review as a conference paper at ICLR 2022
for models trained (independently) on class labels smile, bags under eyes and arched eyebrows, re-
spectively. Unlike in the main body of the paper, these figures include LIME importances along with
the rest of the competing approaches for uncertainty attributions.
Figure 12: Comparison of uncertainty attributions methods on CelebA images, class label smile.
17
Under review as a conference paper at ICLR 2022
Figure 13: Comparison of uncertainty attributions methods on CelebA images, class label bags
under eyes.
18
Under review as a conference paper at ICLR 2022
Figure 14: Comparison of uncertainty attributions methods on CelebA images, class label arched
eyebrows.
19