Under review as a conference paper at ICLR 2022
For Manifold Learning,
Deep Neural Networks can be
Locality Sensitive Hash Functions
Anonymous authors
Paper under double-blind review
Ab stract
It is well established that training deep neural networks gives useful representations
that capture essential features of the inputs. However, these representations are
poorly understood in theory and practice. An important question for supervised
learning is whether these representations capture features informative for classi-
fication, while filtering out non-informative noisy ones. We study this question
formally by considering a generative process where each class is associated with a
high-dimensional manifold and different classes define different manifolds. Each
input of a class is produced using two latent vectors: (i) a “manifold identifier"
γ and; (ii) a “transformation parameter" θ that shifts examples along the surface
of a manifold. E.g., γ might represent a canonical image of a dog, and θ might
stand for variations in pose or lighting. We provide theoretical evidence that neural
representations can be viewed as LSH-like functions that map each input to an em-
bedding that is a function of solely the informative γ and invariant to θ, effectively
recovering the manifold identifier γ . We prove that we get one-shot learning to
unseen classes as one of the desirable consequences of this behavior.
1	Introduction
Deep learning is commonly viewed as a composition of a deep representation function which maps
complex inputs (e.g., images or text) to useful representations in an embedding space, and a decision
layer which easily separates the representations in the embedding space (Wong et al., 2021). However,
what features are captured by the representation and what information is stripped away remains a
mystery. In this work, we aim to extend our theoretical understanding of the possible mechanisms
by which useful representations are learned. As an explanatory tool, consider the setting of image
classification. Each class of images can be viewed as a set of transformations (e.g., different rotations,
backgrounds, lighting conditions) on some canonical representative object (see discussion in DiCarlo
& Cox (2007); Bengio (2012)). For example, in a video clip of a dog, we can think of the first frame
as the canonical pose and every subsequent frame as a different point on the induced dog-manifold.
We think of all such transformations as producing points on a fixed manifold; uniquely defining a
class. Furthermore, applying the same set of transforms on different canonical images of different
animals produces a collection of manifolds, one for each class, with a shared geometry.
In this work, we study whether neural representations, learned with supervision, are able to invert this
mapping of latents to the inputs on a manifold. The learner is given each object’s class label during
training, but both the canonical object and the set of transformations are unknown. Specifically,
we have access to a sample S = {(xi , yi)}in=1 where each input xi ∈ Rd is drawn from a mixture
distribution over m manifolds M1, ..., Mm sharing similar topologies (see Figure 1), and each label
yi ∈ [m] corresponds to the manifold of xi . Further, each point x ∈ Mi is characterized by two
latent vectors:
•	γi is the manifold identifier (i.e., representing the canonical object) and so there is a one-to-
one correspondence between each γi and Mi (at times, we will also use M(γi) to refer to
the manifold associated with γi).
•	θ is the transformation (e.g., representing the view or distortion). So if we fix γi, the
manifold Mi can be generated by sampling different values of θ.
1
Under review as a conference paper at ICLR 2022
Figure 1: An illustration of our
data generating process. Each
class is comprised of points on
a manifold; each point is charac-
terized by two latent parameters:
γ - determines the manifold the
point belongs to, and θ defines
its location on the manifold.
Hash Table
Figure 2: An illustration of a DNN
as a GSH function. All points on
the same manifold map to (approx-
imately) the same representation,
while any two points from differ-
ent manifolds go to distant repre-
sentations.
Figure 3: A confusion ma-
trix of intra (same manifold)
vs inter (different manifolds) `2-
distances of representations for
an MLP trained on synthetic data
(Section 5). The intra distances
are close to zero, suggesting this
model is a GSH function.
A representation which, given x as input, filters out θ (i.e., is invariant to change in θ) but recovers γ
(i.e., only a function of γ) is retaining the “useful” information 1 and is said to invert the manifold
geometry (and the generative process). We study conditions under which DNNs are able to provably
achieve this inversion.
As a warm-up, we look at how to deal with this question in the setting of (realizable) clustering.
Here, γi represents the centroid of a cluster of points and θ represents small perturbations around the
centroid, so then the manifold M(γ) is the set of all points of the form {γ + θ | kθk ≤ ε}. In this
setting, Locality Sensitive Hashing (LSH (Indyk & Motwani, 1998)) maps inputs so that (A) nearby
inputs map to the same bucket and; (B) far away inputs go to different buckets, effectively inverting
the geometry of spatial locality in this case.
More generally, when each class of input points is a complex manifold, we need something stronger. In
this work, we consider a family of manifolds with a shared geometry defined using analytic functions
(Section 1). We prove that DNNs with appropriate regularization, exhibit LSH-like behavior on this
family of manifolds. More precisely, we show that the penultimate (i.e., representation) layer r of an
appropriately trained network, will satisfy the following:
Definition 1 (Geometry Sensitive Hashing (GSH), informal). We say r is a GSH function with respect
to a set of manifolds if (See Figures 2 and 3 for illustration.):
(A)	For every two points on the same manifold x1, x2 ∈ M, kr(x1) - r(x2)k is small.
(B)	For every two points on different manifolds x1 ∈ M1, x2 ∈ M2, kr(x1) - r(x2)k is large.
This suggests, that DNNs whose representations satisfy the GSH property, capture the shared manifold
geometry in a manner similar to how LSH functions capture spatial locality.
Motivation for Inverting the Generative Process. We can think of γ as a combination of semantic
concepts (e.g., having a tail) thus, recovering it confers model interpretability and encourages a
modular system design around representations computed in one task being passed to downstream
tasks. Zimmermann et al. (2021) pursue a very similar goal in the unsupervised setting. Furthermore,
a concrete consequence is representations satisfying GSH enable few-shot learning to unseen classes
(Theorem 2) by Nearest Neighbor search on top of such representations. This of course may not be
true even for models with perfect test accuracy and should motivate practitioners to study how can
we achieve such representations for real datasets.
Our contributions. We summarize our contributions in a nutshell:
1In our model, labels provided by supervision determine what information is “useful” and what isn’t. For
instance, if we don’t differentiate between dog breeds in our labels, then all breeds belong to the same manifold.
If we use different labels for different breeds, then each breed would lie in its own sub-manifold.
2
Under review as a conference paper at ICLR 2022
•	We propose Geometry Sensitive Hashing: a mechanism that allows us to understand how
DNNs might learn good representations, and prove that certain DNNs properly trained on
manifold data are GSH functions. Moreover, these DNNs can recover γ up to a linear
transform thereby inverting the manifold geometry.
•	Additionally, we argue that GSH functions have many desirable properties surrounding
interpretability and aids modular design of systems which solve a complex set of tasks. As a
concrete first step, we prove that GSH holds not only for train manifolds, but also for new
unseen manifolds leading to effective one-shot learning.
Real-World Data. Real-world distributions are complex and cannot be described by simple analytic
manifolds. Nevertheless, we investigate to what extent the GSH behavior is present in modern DNNs.
We find that it continues to hold for DNNs trained on simple datasets such as MNIST. For more
complex datasets such as CIFAR10, where GSH does not hold, we observe a weaker clustering effect
at play. Another recent line of empirical work closely related to ours is that of (Papyan et al., 2020;
Han et al., 2021) which observes a phenomenon dubbed “Neural Collapse" which encompasses our
notion of GSH precisely (notion NC-1). The key difference is that they observe that the GSH property
holds on train data for various deep networks such as VGG and ResNets on complex datasets such
as CIFAR100 and ImageNet, however it is unclear to what extent it holds on the test distribution,
which is our focus here. This indicates a possible future research direction for improving the extent
to which GSH holds in deep representations at test time.
Additional Related Work. There is a rich history of works that study classification problems as
manifold learning—certain notable proposals for learning manifolds include Tenenbaum et al. (2000);
Belkin et al. (2006), meanwhile others such as Hein & Audibert (2005); Hein (2006) study the
problem of manifold density estimation. Deep representations of complex inputs such as text and
images are often used to compare the underlying objects and transfer to new classification problems
(Weiss et al., 2016; Sung et al., 2018). However there is little theoretical understanding of the
neural representations computed by such networks. Saunshi et al. (2019) offer theoretical insights
on contrastive learning, a popular method for unsupervised learning. Khosla et al. (2020) show
empirical benefits for robustness and stability when they push for deep representations to satisfy
GSH. Giryes et al. (2016) attempt to study the question we do, however their proof has been shown
to be false (Gulcu, 2020). Other works (Maurer et al., 2016; Du et al., 2020; Tripuraneni et al., 2020)
develop a theoretical understanding of transfer learning by modeling a collection of tasks with shared
parameters. Our theoretical results build on recent work exposing the benefits of wide non-linear
layers (Daniely et al., 2016) and overparameterized networks (Arora et al., 2019; Allen-Zhu et al.,
2018). It is also closely related to a set of works which explore the loss landscape of deep linear
networks showing that all local minima are global (Ge et al., 2016; Kawaguchi, 2016). The connection
between DNNs and Hash Functions was explored before (e.g., see He et al. (2021); Wang et al. (2017)
and references therein). While previous works focus on empirical studies, we are able to prove that
GSH holds for certain architectures under the manifold data assumption.
Notational Preliminaries We use [n] to denote {1, 2, . . . , n}. Boldface letters are used for vectors
and capital letters denote matrices. x>y or hx, yi denotes the inner product of x and y. We use
standard matrix norms:kAkF
Pi,j Ai2j is the Frobenius norm, kAk2 is the operator or spectral
norm (largest singular value of A), ∣∣Ak* is the nuclear norm which is the sum of the singular values.
x y denotes the vector obtained by point-wise multiplication of the coordinates of x and y and a
similar notation is used for entry-wise multiplication of matrices as well. Given a matrix A ∈ Rm×n
such that svd(A) = USV >, we define A1/2 = US1/2 V >. Sd-1 is the d-dimensional unit sphere.
2 A Formal Framework for GSH
We assume that our input manifolds are sets of points in Rd where every manifold M(γ) has an
associated s-dimensional latent vector γ ∈ Rs , s ≤ d. The manifold is then defined to be the set of
points X = f (γ, θ) = (fι(γ, θ),...,fd(γ, θ)) for θ ∈ T ⊆ Rk ,k < d. Here, f = {fi(∙, ∙)}d=ι is
the manifold generating function where the fi are all one-dimensional analytic functions. θ acts as
the “shift" within the manifold. Without significant loss of generality, we assume our inputs X and
γs are normalized2 and lie on Sd-1 and Ss-1, the d and s-dimensional unit spheres, respectively.
2Note, we can make kxk2 = 1 inputs by normalization and padding by a dummy constant
3
Under review as a conference paper at ICLR 2022
When the fis are all degree-1 polynomials we call the manifold a linear manifold. An example of a
linear manifold is a d - 1-dimensional hyperplane. Given the above generative process, we assume
that there is a well-behaved analytic function to invert it. Conceptually, this function retrieves the
important features and attributes of the input. (e.g., existence of nose from image of a face). Since a
distinct pair of (γ, θ) produce distinct images, such function always exists. For natural images and
for latents we care about, this inverting function is usually non-pathological and is either analytic
or easily approximable by a smooth analytic function (a small change in the pixels would not cause
significant changes in the latents).
Assumption 1 (Invertibility). There is an analytic function g(∙) : Rd → Rs with bounded norm
Taylor expansion s.t., for every point x = f(γ, θ) on M (γ), g(x) = γ.
Our precise definition of the norm of an analytic function is technical and is deferred to Definition 5.
We remark that it behaves similar to commonly used notions of norm in non-pathological cases. For
instance, the function g(x) = eβ1∙x ∙ sin(β2 ∙ x)+cos(β3 ∙ x) will have a constant norm if βι, β2, β3
have constant k ∙ ∣∣2 norm.
Train Data Generation. As described above, a set of analytic functions {fi } and a vector γ
together define a manifold. We then consider a shared geometry among manifolds defined by a
fixed set of {fi}. A distribution M over a class of manifolds supp(M) (given by the {fi}) is then
generated by having a set Γ from which we sample γ associated with each manifold. We assume
that all manifolds within supp(M) are reasonably separated from each other. Formally, for any
two manifolds M1, M2 ∈ supp(M), we will assume that γ1>γ2 ≤ τ where τ < 1 is a constant3.
Such a manifold distribution will be called τ -separated. To describe a distribution of points over a
given manifold M We use the notion of a point density function D(∙) which maps a manifold M
to a distribution D(M) over the surface of M. Training data is then generated by first drawing m
manifolds Mi,..., Mm 〜 M at random. Then for each l ∈ [m], n samples {(xii, yii)}n=ι are
drawn from Ml according to the distribution D(Ml). Note that for convenience, we view the label
yil as a one-hot vector of length m indicating the manifold index. The learner’s goal is then to learn
a function which takes in these n × m pairs of (xil , yil ) and correctly classifies which manifold a
new point x comes from. With the above notation, we now formally define GSH.
Definition 2 (Geometry Sensitive Hashing (GSH)). Given a representation function r : Rd → Rp,
and a distribution over a manifold class M, we say that r satisfies the (ε, ρ)-hashing property on M
with associated point density function D if, for some ρ > 1, ε > 0,
VM(r)
E
X 〜D(M)
r(X)-X〜D(M)[r(X)]U ≤ J
(A)
for all M ∈ supp(M). The above states that the variance of the representation across examples of a
manifold is small. Moreover, for two distinct τ -separated manifolds M1 and M2 sampled from M,
the corresponding representations need to be far apart. That is,
E
Xl ~D(M1),X2~D(M2 )
[∣r(x1) -
r(x2 )∣22] ≥ ρε.
(B)
A better separation factor τ between manifolds makes it easier to get a better ρ for GSH.
Our main contribution is showing that DNNs trained on manifold data can produce representations
that satisfy the GSH property on the manifold distribution.
Neural Architecture. The crucial properties of a network architecture we rely on for our results
are (i) over-parameterization i.e., the number of parameters is of the order of the number of train
examples (or larger): this gives our neural representations the necessary expressive power and also
helps optimization, (ii) at least two trainable layers in the network so that we can try to learn the
representation satisfying GSH after the first trainable layer. With these points in mind, we show our
theoretical results on a 3-layer network for simplifying the presentation. We consider the neural
network y = ABσ(Cx), where the input X ∈ Rd passes through a wide randomly initialized
non-trainable layer C ∈ RD×d followed by a ReLU activation σ(.)4. Then, there are two trainable
3This is a weak assumption. In particular the average τ between two randomly sampled vectors on the unit
sphere is much smaller (Claim 13).
4Our results hold for more general activations. The required property of an activation is that its dual should
have an ‘expressive’ Taylor expansion. E.g., the step function or the exponential activation also satisfy this
property. See Daniely et al. (2016).
4
Under review as a conference paper at ICLR 2022
fully connected layers A ∈ Rm×T, B ∈ RT×D with no non-linearity between them. Each row
of C is drawn i.i.d. from N(0, DI). It follows from random matrix theory that ∣∣C∣∣2 ≤ 4 w.p.
≥ 1 - exp(-O(D)) (Lemma 12). This choice of architecture is guided by recent results on the
expressive power of over-parameterized random ReLU layers (Daniely et al., 2016; Arora et al.,
2019; Allen-Zhu et al., 2018) coupled with the fact that the loss landscape of two layer linear neural
networks enjoys nice properties (Ge et al., 2016; Gunasekar et al., 2018).
Under an appropriate loss function, we prove the following Theorem (See Theorems 3 and 4 for
formal versions).
Theorem 1 ((Informal) GSH holds for Manifolds from M). For a constant τ, suppose M is a
distribution on τ -separated manifolds with associated latent vector γ ∈ Ss-1. For any ε > 0, given
n points samples from each of the m manifolds drawn from M, when our 3-layer neural network of
size poly(m, n, 1∕ε) is trained on an appropriate loss, it gives a representation which satisfies the
(ε, ρ)-hashing property with high probability over unseen manifolds in M ,for P = Ω(1∕ε) when
∕sO(log(1∕ε))、
m,n = Θ s------ε2----b
One immediate consequence of having the GSH property is transfer learning to unseen manifolds
under the same shared geometry:
Theorem 2 ((Informal) GSH Implies One-Shot Learning). Given a distribution M over τ -separated
manifolds, if a representationfunction r(∙) satisfies the (ε, P)-GSH, for a small ε and a large enough
ρ, then we have one-shot learning. That is, there is a simple hash-table lookup algorithm A that can
classify inputs from manifold Mnew 〜 M With just one sample with high probability.
Theorem 2 is proved as Theorem 5 in the Appendix. In addition, we show the exact recoverability of
γ in some settings.
Remark 1. GSH implies that the representation we have computed is isomorphic to the manifold
identifier γ. We observe empirically a simple linear transform suffices to map the representation to
exactly γ . In addition, we show theoretically as well that we are able to recover γ, albeit only for
examples on the train manifolds (Section G).
Remark 2 (Extension to Deeper Networks). Using recent work (Allen-Zhu et al., 2019; Du et al.,
2019b) which elaborates interpolating capabilities of deeper over-parameterized networks, we can
extend Theorem 1 to deeper overparameterized networks as well. One caveat is that the generalization
bounds would now depend on the Rademacher complexity of the deeper network. Pursuing such
an extension is significantly more challenging but that is not the main focus of this work. To avoid
complicating the exposition significantly we choose to focus on a 3-layer architecture which is
already non-trivial to analyze and has the salient properties required to exhibit the desired behavior.
In addition, empirical work of (Papyan et al., 2020) and others shows that deep networks can indeed
exhibit GSH at least on train data for complex datasets.
Additional Notation. We use z to denote σ(Cx). For succinctness, we define Xl to be the matrix
whose columns are {xil }in=1, Zl is the matrix whose columns are {zil }in=1. Given the label vectors
yil and the model predictions yn we define Yl and Y similarly. Let X be the d X n X m rank-3 tensor
which is obtained by stacking the matrices Xi for l ∈ [m]. Tensors Y,Y, Z are defined similarly.
We often compute a mix of empirical averages over two distributions (i) the m train manifolds (ii)
the n data points from each of the m train manifold. Given a function t(x), let En[t(xι)∣γι]=
1 Pn=ι t(xii) and given a function t(γ) operating on a manifold, let Em [t(γ)]=+ Pm=1 t(γι).
Loss function. Given the one-hot label vectors y and the predictions y made by our model we aim
to minimize a weighted square loss averaged across the m train manifolds.
LAB (Y,Y) =工 XX E h∣wι © (yi - yi )k2 I Yii = E [∣∣W1 © (Yi - Y)∣∣2],	⑴
m l=1n	m	F
where Wi is a weighting of different coordinates of y 一 y such that Wij = 1/2 if j = l and 1∕2(m — 1)
otherwise. Each example serves as a positive example for the class corresponding to its manifold
and is a negative example for all the other m - 1 classes. The weighting by Wi ensures that the total
weight on the positive and negative examples is balanced and helps exclude degenerate solutions
such as the all 0s vector from achieving a low loss value. We show in Section A of the supplementary
5
Under review as a conference paper at ICLR 2022
material that a small value of our weighted square loss implies a small 0/1 classification error and
vice versa. We add `2 regularization on the weight matrices A and B . The objective is then,
LA,B (Y,Y) + λ1kAkF + λ2kBkF,	⑵
When we deal with non-linear manifolds which are harder to analyze, we will require an additional
component in our regularization which we term variance regularization (see Section 4.2).
Empirical Variant of Intra-Manifold Variance VM. In the subsequent sections an empirical
average of VM (r), the variance of representation r over points from M, across manifolds will be of
importance. We define it here. Given any function r(∙) of X ∈ Rd,
Vmn(r) = E [e [∣∣r(xι) - E[r(xι)]∣∣2lγιll	(3)
m n	n	2
A Note on Optimization Algorithms. Standard optimization algorithms such as gradient descent
are theoretically shown to converge arbitrarily close to a local optimum point in many settings even
for a non-convex objective. In particular, they can provably avoid second-order stable points (saddle
points) with high probability (Ge et al., 2015; Jin et al., 2018; Lee et al., 2019) for Lipschitz and
smooth objectives. In addition, gradient descent on overparameterized deep networks has been shown
to provably avoid saddle points of all orders (Du et al., 2019a). Relying on this understanding, we
assume our optimization has arrived at a local minimum and focus on understanding the properties of
this local minimum.
Proof Overview Before going into further technical details, we give a short overview of Theorem 1’s
proof. A wide random ReLU layer enables us to approximately express arbitrary analytic functions
γ = ginv(x) as linear functions of the output of the ReLU layer (Lemma 2)—in fact we show that a
wide random ReLU layer is “equivalent" to a kernel that produces an infinite sequence of monomials
in x upto an orthonormal rotation. So by approximating Y as analytic functions of γ we get that
Y ≈ Wσ(Cx) for some W. Next, since we have two layers A, B above the ReLU layer, it is possible
to get a factorization W = AB such that multiplication of z = σ(Cx) by B drops any dependence
on θ and only depends on Y——this ensures that for that choice of A, B the representation r(x) = Bz
is independent of θ (Lemma 1). Further, given the regularization we impose, the optimal output after
the B layer depends only on γ. Moreover, at the optimum, kB kF is bounded and independent of
m, n (even though the number of parameters in B grows with m, n); similarly the average norm of A
per output, kAkF /m, can also be made constant. We then use Rademacher complexity arguments
to show that if the number of training inputs per manifold n is larger than a quantity that depends
on kBkF , then the GSH property holds not just for the training inputs but for most points on the
manifold. Another set of Rademacher complexity arguments show that if m is larger than a certain
value that depends on kB kF the hashing property will generalize to most new manifolds (Section E).
3	Properties of the Architecture leading to GSH
ReCallthatkXk2 = 1 for all inputs. We also append a constant 1/√2 to X to get x0 = (x/√2,1/√2).
This enables a more complete kernel representation of our random ReLU layer which will help our
analysis. Given (2), we show that there exists a ground truth network which makes both the loss and
the regularizer terms small. Moreover, the representation computed by this ground truth is a GSH
function. This is a key component of our proof.
Lemma 1 (Existence ofa Good Ground Truth). Recall that the manifold identifiers γ ∈ Ss-1. There
exist ground truth matrices A*,B* such thatfor any 0 < ε ≤ 1/2,
1	LA*,B*(Y, Y) ≤ ε,
2.	∣∣A*∣∣F ≤ m, ∣∣B*∣∣F ≤ β = SO(Iog(I/E)),
3.	B*σ(C.) satisfies (ε, Ω(1∕ε))-GSH.
4.	Hidden layer width T = O (log(mn)log(1∕δ)ε-1),
To show that the weighted square loss and the regularizer terms are small, we lean on insights from
Section 3.1 which presents the power of having a random wide ReLU layer as our first layer. Once we
have the bounds on LAB (Y,Y) and IlAIlF, property (B) for our representation follows. Our choice
6
Under review as a conference paper at ICLR 2022
of B* will have a small intra-Class representation variance averaged over the train manifolds giving
US property (A). Finally to get a bound on the number of columns in A*, we use the observation that
given an A* with a large number of columns we could use a random projection to project it down to a
smaller matrix without perturbing A*’s output by much.
3.1	Kernel View of a Non-Linear Random Layer
We first show that having a wide random ReLU layer is approximately the same as an orthogonal
transform on a vector whose coordinate are computed by the terms in a Taylor series expansion of a
function of the input.
Claim 2. For any ε,δ > 0, and for k ≥ O((mn∕ε)2/3) ifthe width D ≥ Θ ( √m log(mn/^), then,
w.p ≥ 1 - δ, there exists a matrix with U ∈ RD×O(dk) with orthonormal rows, and ∆ ∈ RD×mn,
k∆kF < ε, s.t., for all i ∈ [n], l ∈ [m],
σ(CXil) = U √2∏, √4X 1,...,O
>
X 泸	+∆il.
where x0j is a vector obtained by flattening the jth tensor power of the vector X, ∆il is the
ilth column of ∆, and finally given two vectors a, b, (a, b) represents the concatenation of their
coordinates to yield a single vector.
Claim 2 implies the following lemma which says that a linear function of the output of the random
ReLU layer, can express bounded-norm polynomials which is used in the proof of Lemma 1.
Lemma 3. (Informal version of Lemma 26) For ε, δ > 0, and any norm bounded vector-valued
analytic function g : Rd → R (for an appropriate notion of norm), w.p. ≥1 - δ we can approximate
g using a random ReLU kernel σ(Cx) OfWidth D ≥ Θ (^mnloglmnand a bounded norm vector
a, so that, for each of the mn inputs X, |g(X) - aσ(C X)| ≤ ε.
4 Properties of Local Minima of the Loss
In this section, we show two properties of local minima of (2). First, we show that all local minima
are global (see Ge et al. (2016) for further results of this flavor).
Lemma 4 (All Local Minima are Global). All local minima are global minima for the following
objective, where O(.) is any convex objective:
minO(AB) + λι (∣∣AkF) + λ (∣∣B∣∣F),
A,B
The above lemma together with Lemma 1 implies that the desirable properties of our ground truth
A*, B* also hold at the local minima of (2). This will follow by choosing λ1, λ2 appropriately.
Lemma 5. At any local minima we have that the weighted square loss LA B (Y, Y) ≤ 3ε.
Next we need to show that the empirical variant of the GSH property holds for the representation
Bσ(C.). Here our approaches for linear and non-linear manifolds differ. Linear manifolds enable a
more direct analysis with a plain '2-regularization. However, we need to assume certain additional
conditions on the input. The result for linear manifolds acts as a warm-up to our more general result
for non-linear manifolds where we have minimal assumptions but use a stronger regularizer designed
to push the representation to satisfy GSH. We describe these differences in Sections 4.1 and 4.2.
4.1	GSH Property on Linear Train Manifolds
Recall that a linear manifold is described by a set of linear functions {fi }id=1 which transform γ, θ to
X. An equivalent way of describing points on a linear manifold is: X = Pγ + Qθ for some matrices
P, Q. Without a significant loss of generality we can assume that Pγ ⊥ Qθ (Lemma 35). Given
this, we can regard as our input X = (γ0, θ0) where θ0 ∈ Rk and Y0 ∈ Rd-k by doing an appropriate
7
Under review as a conference paper at ICLR 2022
rotation of axes. Here, Y0, θ0 Play the role of original Y, θ respectively. As before We will assume that
∣∣xk2 = 1. We append a constant to X as before, increasing the value of it to O(√k) for a technical
nuance. This constant plays the role of a bias term. The objective for linear manifolds is then,
min La,b (Y,Y) + λι ∣A∣F + λ2∣B∣F.	(4)
A,B
Lemma 4 will imply that local minima of the above objective are global. The next step is Lemma 6
which uses a simple centering argument to show that the loss decreases when the variance of the
output vector across examples from a given manifold decreases.
Lemma 6 (Centering). Replacing the output of our neural network ^ = ABσ(C f (Y, θ)) by y0 =
En [y∣Y] will reduce the (weighted) square loss:
L(Y, Y0) ≤ L(Y, Y)- Vmn(y)∕2(m -1)
Lemma 6 implies that a smaller variance at the output layer is beneficial. In Section D.1, we argue
that it is in fact beneficial to have a small variance at the representation layer as well. Next we show
Lemma 7 which lets us achieve a small variance at the representation layer by shifting weights in B
away from nodes corresponding to monomials which depend on θ. This change ultimately benefits
the weighted square loss in a way so that ∣A∣F and ∣B ∣F are not impacted.
Lemma 7. Given B s.t. Vmn (B z) > ω (ε), there is B0 s.t. Vmn(B0z) ≤ O(ε) and ∣B0∣F ≤ ∣B∣F.
As we saw in Claim 2, the output of σ(CX) can be thought of as an orthonormal transform applied
onto a vector whose coordinates compute monomials of X. Now we can define an association between
weights of B and these monomials under which we argue using Lemma 6 that shifting all weights
associated with monomials involving θ0 to corresponding monomials involving just Y0 decreases the
variance without increasing ∣B ∣F, consequently improving objective (4). Together Lemmas 6-7 give
us that at any local minima of (4) the representation r has the minimum variance possible.
Lemma 8. Given any local minimum A, B of (6), and r(X) = Bσ(CX), we have Vmn(r) = O(ε).
This will imply that at any local minimum, property (A) is satisfied at least on our train set. Next we
need property (B). This follows as a consequence of having a small loss and a bound on ∣∣A∣∣f.
Lemma 9. For any local minima A, B, let r(X) = Bσ(CX). Then,
mm
X X E [kr(Xl)-r(Xj)∣2] ≥ a(m2).
l=1 j=1,j 6=l
4.2	GSH Property on Non-linear Train Manifolds
The same argument as in Section 4.1 fails for non-linear manifolds, as we no longer have a direct
association from monomials of X to associated monomials of same degree in Y, θ as we had before.
Instead, we show the result for non-linear manifolds using different regularizer. In addition to the
`2 -regularization, we penalize large variance between representation belonging to the same manifold5.
Variance Regularization. The additional regularization term we add is the empirical average of the
variance VM across our train manifolds,
Vreg(Bσ(C∙)) = ^TVmn(Bσ(C∙))	(5)
n-1
The re-scaling by n∕(n 一 1) makes each term an unbiased estimator for VM(Bσ(C∙)) (Lemma 49).
The final objective we minimize is,
La,b(Y,Y)+ λι∣A∣F + λ2 (∣B∣F + Vreg(Bσ(C)))	(6)
Remarkably, even though (6) is different from what we had before we can still show that every
local minimum is a global minimum (Lemma 45). Additionally, from the fact that the ground truth
representation satisfies GSH, we get that, under the ground truth, the variance regularization term
is small as well. Since the global minimum achieves a smaller objective than the ground truth, by
choosing λ1 , λ2 appropriately we get that at any local minima Vreg is small as well.
5Note that this regularization is reminiscent of the loss used in contrastive unsupervised learning (Hadsell
et al., 2006; Dosovitskiy et al., 2014; Chen et al., 2020).
8
Under review as a conference paper at ICLR 2022
Lemma 10. Given any local minimum A, B of (6), Vreg(Bσ(C∙)) ≤ O(ε).
Our generalization analysis presenting population variants of the bounds we saw in the previous
sections above is described in detail in the Appendix (Section E).
5 Experiments
Figure 4: A comparison of intra vs inter class distances. Left, we train an MLP on synthetic data (see
Section 5) that satisfies Assumption 1. On the Middle and Right we train a CNN on MNIST and
CIFAR-10 respectively. For synthetic data, the GSH property strongly holds. The intra-distances of
the representation layers for networks trained on MNIST and CIFAR-10 are also significantly smaller
than their inter distances. While a case could be made that the ratio for MNIST implies GSH, for
CIFAR-10 GSH does not hold. However, we do observe that a similar mechanism is partially at play.
In this section, (for full details see Appendix I), we explore to what extent the GSH property holds
with today’s DNNs when our assumptions are violated. In particular, we train DNNs on real and
synthetic data and study the inter to intra distance ratio ρ. In contrast to the work on ‘Neural Collapse’
phenomenon (Papyan et al., 2020; Han et al., 2021) which shows GSH on train data, our focus is on
test samples and unseen classes. We remark that the Neural Collapse line of work shows that GSH
holds for train samples for deep networks (e.g., ResNet50) on complex datasets (e.g., ImageNet).
Experimental Setup We separate our experiments to two groups, based on the data source. 1. Syn-
thetic Data. We randomly sample γ and θ from a Multivariate Normal so that the γs are well
separated, then chose a function satisfying Assumption 1 such as f (∙) = Pi4=1 fi (∙) where fi are
coordinate-wise analytic functions, e.g., (a rotation of) sin, cos, log(0.5(1 + x2)). So a train example
becomes x = f(γ, θ) and a manifold is comprised of examples with fixed γ and varying θ. We train
a 3-layer MLP with regularized `2 loss for 200 epochs achieving 100% train and test accuracies.
2. Natural Images. We train a five layer Myrtle mCNN (Page, 2018) on MNIST and CIFAR-10 using
'2-regularized SGD for 50 epochs with LR of 0.1 then drop the LR to 0.01 for another 100 epochs.
Experimental Results (Figure 4). As expected, for synthetic data (see Table 1), ρ is quite large on
the test data, in the range of ρ = 10.79-26.8 for the distributions we tried. This implies a strong GSH
property and is consistent with our theory. For MNIST, ρ = 3.36 and for CIFAR-10 it is ρ = 1.46
suggesting that even though GSH doesn’t hold, a weaker clustering effect exists.
One-shot Learning and γ Invertibility. We conduct two additional sets of experiments on synthetic
data 1) We measure how well GSH holds for new manifolds (i.e., few-shot learning) and; 2) whether
the learnt representation is linearly isomorphic to γ . For the former, we sample an additional 50, 000
γF Ss (FS for few-shot) and generate appropriate xF Ss. Then, we measure the GSH property of
the representation layer. Even on these new manifolds GSH strongly holds (Figure 5) with ρ in the
range of 11.09-28.77. For the latter, we use the {(r(xiFS), γiFS} as train data for a linear classifier
on top of the representation produced by our MLP. We again generate a test set of never before seen
γs. With enough samples we almost perfectly recover γ(x) from r(x) implying a (almost) linear
isomorphism between the latent and the learnt representations (see Figure 6 in the Appendix).
Conclusion and Discussion We studied how representations learnt by DNNs trained for supervised
classification behave under a specific shared geometry generative process. We saw that properly
trained DNNs satisfy the GSH property and recover the semantically meaningful latent representation
γ while stripping away the dependency on the classification redundant variable θ . Notably, this is
not restricted to the manifolds seen during training and thus could help shed light on how Transfer
Learning works. We also observe that on real data, although today’s networks do not necessarily
satisfy GSH to the full extent, they exhibit weaker forms of the behavior which is valuable in itself.
9
Under review as a conference paper at ICLR 2022
References
Naman Agarwal, Zeyuan Allen-Zhu, Brian Bullins, Elad Hazan, and Tengyu Ma. Finding approximate
local minima faster than gradient descent. In Proceedings of the 49th Annual ACM SIGACT
Symposium on Theory ofComputing, pp.1195-1199, 2017.
Atish Agarwala, Abhimanyu Das, Brendan Juba, Rina Panigrahy, Vatsal Sharan, Xin Wang, and Qiuyi
Zhang. One network fits all? modular versus monolithic task formulations in neural networks. In
International Conference on Learning Representations, 2021. URL https://openreview.
net/forum?id=uz5uw6gM0m.
Zeyuan Allen-Zhu, Yuanzhi Li, and Yingyu Liang. Learning and generalization in overparameterized
neural networks, going beyond two layers. arXiv preprint arXiv:1811.04918, 2018.
Zeyuan Allen-Zhu, Yuanzhi Li, and Zhao Song. A convergence theory for deep learning via over-
parameterization. In International Conference on Machine Learning, pp. 242-252. PMLR, 2019.
Sanjeev Arora, Simon Du, Wei Hu, Zhiyuan Li, and Ruosong Wang. Fine-grained analysis of
optimization and generalization for overparameterized two-layer neural networks. In International
Conference on Machine Learning, pp. 322-332. PMLR, 2019.
Afonso S Bandeira, Ramon Van Handel, et al. Sharp nonasymptotic bounds on the norm of random
matrices with independent entries. Annals of Probability, 44(4):2479-2506, 2016.
Mikhail Belkin, Partha Niyogi, and Vikas Sindhwani. Manifold regularization: A geometric frame-
work for learning from labeled and unlabeled examples. Journal of machine learning research, 7
(11), 2006.
Yoshua Bengio. Deep learning of representations for unsupervised and transfer learning. In Proceed-
ings of ICML workshop on unsupervised and transfer learning, pp. 17-36. JMLR Workshop and
Conference Proceedings, 2012.
Marcus Carlsson. Perturbation theory for the matrix square root and matrix modulus. arXiv preprint
arXiv:1810.01464, 2018.
Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. A simple framework for
contrastive learning of visual representations. In International conference on machine learning, pp.
1597-1607. PMLR, 2020.
Amit Daniely, Roy Frostig, and Yoram Singer. Toward deeper understanding of neural networks:
The power of initialization and a dual view on expressivity. In Advances In Neural Information
Processing Systems, pp. 2253-2261, 2016.
Li Deng. The mnist database of handwritten digit images for machine learning research. IEEE Signal
Processing Magazine, 29(6):141-142, 2012.
James J DiCarlo and David D Cox. Untangling invariant object recognition. Trends in cognitive
sciences, 11(8):333-341, 2007.
Alexey Dosovitskiy, Jost Tobias Springenberg, Martin Riedmiller, and Thomas Brox. Discriminative
unsupervised feature learning with convolutional neural networks, 2014.
Simon Du, Jason Lee, Haochuan Li, Liwei Wang, and Xiyu Zhai. Gradient descent finds global
minima of deep neural networks. In Kamalika Chaudhuri and Ruslan Salakhutdinov (eds.),
Proceedings of the 36th International Conference on Machine Learning, volume 97 of Proceedings
of Machine Learning Research, pp. 1675-1685. PMLR, 09-15 Jun 2019a. URL https://
proceedings.mlr.press/v97/du19c.html.
Simon S. Du, XiyU Zhai, Barnabgs P6czos, and Aarti Singh. Gradient descent Provably optimizes
over-parameterized neural networks. In 7th International Conference on Learning Representations,
ICLR 2019, New Orleans, LA, USA, May 6-9, 2019. OpenReview.net, 2019b. URL https:
//openreview.net/forum?id=S1eK3i09YQ.
10
Under review as a conference paper at ICLR 2022
Simon S Du, Wei Hu, Sham M Kakade, Jason D Lee, and Qi Lei. Few-shot learning via learning the
representation, provably. arXiv preprint arXiv:2002.09434, 2020.
Rong Ge, Furong Huang, Chi Jin, and Yang Yuan. Escaping from saddle points—online stochastic
gradient for tensor decomposition. In Conference on learning theory, pp. 797-842. PMLR, 2015.
Rong Ge, Jason D Lee, and Tengyu Ma. Matrix completion has no spurious local minimum. Advances
in Neural Information Processing Systems, 29:2973-2981, 2016.
Raja Giryes, Guillermo Sapiro, and Alex M. Bronstein. Deep neural networks with random gaussian
weights: A universal classification strategy? IEEE Transactions on Signal Processing, 64(13):
3444-3457, 2016. doi: 10.1109/TSP.2016.2546221.
Talha Cihad Gulcu. Comments on “deep neural networks with random gaussian weights: A universal
classification strategy?”. IEEE Transactions on Signal Processing, 68:2401-2403, 2020.
Suriya Gunasekar, Blake Woodworth, Srinadh Bhojanapalli, Behnam Neyshabur, and Nathan Srebro.
Implicit regularization in matrix factorization. In 2018 Information Theory and Applications
Workshop (ITA), pp. 1-10. IEEE, 2018.
Raia Hadsell, Sumit Chopra, and Yann LeCun. Dimensionality reduction by learning an invariant
mapping. In 2006 IEEE Computer Society Conference on Computer Vision and Pattern Recognition
(CVPR’06), volume 2, pp. 1735-1742. IEEE, 2006.
XY Han, Vardan Papyan, and David L Donoho. Neural collapse under mse loss: Proximity to and
dynamics on the central path. arXiv preprint arXiv:2106.02073, 2021.
Fengxiang He, Shiye Lei, Jianmin Ji, and Dacheng Tao. Neural networks behave as hash encoders:
An empirical study, 2021.
Matthias Hein. Uniform convergence of adaptive graph-based regularization. In International
Conference on Computational Learning Theory, pp. 50-64. Springer, 2006.
Matthias Hein and Jean-Yves Audibert. Intrinsic dimensionality estimation of submanifolds in rd. In
Proceedings of the 22nd international conference on Machine learning, pp. 289-296, 2005.
Piotr Indyk and Rajeev Motwani. Approximate nearest neighbors: towards removing the curse of
dimensionality. In Proceedings of the thirtieth annual ACM symposium on Theory of computing,
pp. 604-613, 1998.
Chi Jin, Praneeth Netrapalli, and Michael I Jordan. Accelerated gradient descent escapes saddle
points faster than gradient descent. In Conference On Learning Theory, pp. 1042-1085. PMLR,
2018.
Kenji Kawaguchi. Deep learning without poor local minima. In D. Lee, M. Sugiyama, U. Luxburg,
I. Guyon, and R. Garnett (eds.), Advances in Neural Information Processing Systems, vol-
ume 29. Curran Associates, Inc., 2016. URL https://proceedings.neurips.cc/
paper/2016/file/f2fc990265c712c49d51a18a32b39f0c-Paper.pdf.
Prannay Khosla, Piotr Teterwak, Chen Wang, Aaron Sarna, Yonglong Tian, Phillip Isola,
Aaron Maschinot, Ce Liu, and Dilip Krishnan. Supervised contrastive learning. In
H. Larochelle, M. Ranzato, R. Hadsell, M. F. Balcan, and H. Lin (eds.), Advances
in Neural Information Processing Systems, volume 33, pp. 18661-18673. Curran Asso-
ciates, Inc., 2020. URL https://proceedings.neurips.cc/paper/2020/file/
d89a66c7c80a29b1bdbab0f2a1a94af8- Paper.pdf.
Alex Krizhevsky et al. Learning multiple layers of features from tiny images. N/A, 2009.
Jason D Lee, Ioannis Panageas, Georgios Piliouras, Max Simchowitz, Michael I Jordan, and Benjamin
Recht. First-order methods almost always avoid strict saddle points. Mathematical programming,
176(1):311-337, 2019.
Andreas Maurer, Massimiliano Pontil, and Bernardino Romera-Paredes. The benefit of multitask
representation learning. Journal of Machine Learning Research, 17(81):1-32, 2016.
11
Under review as a conference paper at ICLR 2022
David Page. How to train your resnet. https://myrtle.ai/how-to-train-your-resnet-4-architecture/,
2018.
Vardan Papyan, XY Han, and David L Donoho. Prevalence of neural collapse during the terminal
phase of deep learning training. Proceedings of the National Academy of Sciences, 117(40):
24652-24663, 2020.
Nikunj Saunshi, Orestis Plevrakis, Sanjeev Arora, Mikhail Khodak, and Hrishikesh Khandeparkar. A
theoretical analysis of contrastive unsupervised representation learning. In Kamalika Chaudhuri
and Ruslan Salakhutdinov (eds.), Proceedings of the 36th International Conference on Machine
Learning, volume 97 of Proceedings of Machine Learning Research, pp. 5628-5637. PMLR,
09-15 Jun 2019. URL https://proceedings.mlr.press/v97/saunshi19a.html.
Shai Shalev-Shwartz and Shai Ben-David. Understanding machine learning: From theory to
algorithms. Cambridge university press, 2014.
Flood Sung, Yongxin Yang, Li Zhang, Tao Xiang, Philip HS Torr, and Timothy M Hospedales.
Learning to compare: Relation network for few-shot learning. In Proceedings of the IEEE
conference on computer vision and pattern recognition, pp. 1199-1208, 2018.
Michel Talagrand. Concentration of measure and isoperimetric inequalities in product spaces.
Publications Mathematiques de IfInstitut des Hautes Etudes Scientifiques, 81(1):73-205,1995.
Joshua B Tenenbaum, Vin De Silva, and John C Langford. A global geometric framework for
nonlinear dimensionality reduction. science, 290(5500):2319-2323, 2000.
Nilesh Tripuraneni, Michael I Jordan, and Chi Jin. On the theory of transfer learning: The importance
of task diversity. arXiv preprint arXiv:2006.11650, 2020.
Roman Vershynin. High-dimensional probability, 2019.
Jingdong Wang, Ting Zhang, Nicu Sebe, Heng Tao Shen, et al. A survey on learning to hash. IEEE
transactions on pattern analysis and machine intelligence, 40(4):769-790, 2017.
Karl Weiss, Taghi M Khoshgoftaar, and DingDing Wang. A survey of transfer learning. Journal of
Big data, 3(1):1-40, 2016.
Eric Wong, Shibani Santurkar, and Aleksander Madry. Leveraging sparse linear layers for debuggable
deep networks. arXiv preprint arXiv:2105.04857, 2021.
Roland S. Zimmermann, Yash Sharma, Steffen Schneider, Matthias Bethge, and Wieland Brendel.
Contrastive learning inverts the data generating process. In Marina Meila and Tong Zhang
(eds.), Proceedings of the 38th International Conference on Machine Learning, volume 139 of
Proceedings of Machine Learning Research, pp. 12979-12990. PMLR, 18-24 Jul 2021. URL
https://proceedings.mlr.press/v139/zimmermann21a.html.
12
Under review as a conference paper at ICLR 2022
A Additional Preliminaries
We start the supplementary material by listing a set of additional definitions and some preliminary
results. These will be for the most part statements on high-dimensional probability and linear algebra
and in some cases are known from prior work or are folklore. We give the definition of analytic
functions next by focusing on real-valued functions.
Definition 3 (Analytic Functions). A real-valued function f(x) is an analytic function on an open
set D if it is given locally by a convergent power series everywhere in D. That is, for every x0 ∈ D,
∞
f(x) =	an(x - x0)n,
where the coefficients a0, a1, . . . are real numbers and the series is convergent to f(x) for x in a
neighborhood of x0. We also define a norm on f as the two norm of the coefficient vector obtained
when the above form is expanded to individual monomials.
Multi-variate analytic functions f(x) are defined similarly with the difference being that the conver-
gent power series is now a general multi-variate polynomial in the coordinates of x - x0. The Taylor
expansion can now be viewed to be of the form
f(x) = X aJxJ
J
where J = (j1, . . . ,jd) identifies the monomial xJ = xj11 xj22 . . . xjdd.
Definition 4 (Multi-Variate Polynomials). A multi-variate polynomial p(.) in x ∈ Rd of degree k is
defined as
p(x) =	pJxJ ,
J,| J ∣≤k
where J = (J1, . . . , Jd) is a set of d integers which identifies the monomial xJ = x1J1 x2J2 . . . xdJd,
|J| = Pid=1 Ji is the degree of the monomial and pJ is the coefficient.
We will show in Section C that given an infinitely wide ReLU layer we can express any analytic
function by just computing a linear function of the output of the aforementioned ReLU layer. Using
this knowledge, we now present our definition of norm of an analytic function we use here.
Definition 5. Given a multi-variate analytic function g : Rd → R, and an infinite width ReLU layer
σ(C.) : Rd → R∞, we define
kgk
min	kak2.
a,aσ(C.)≡g
This notion of the norm is chosen to help our analysis, however, our notion agrees with more common
notions of norm in most non-pathological settings. We offer more intuition along with more directly
interpretable bounds on our notion of the norm of an analytic function later in Section C.
Our generalization analysis uses the concepts of Rademacher complexity and its role in providing
uniform convergence bounds. We give the definition of Rademacher complexity here.
Definition 6 (Rademacher Complexity). Rademacher complexity of a function class F is a useful
quantity to understand how fast function averages for any f ∈ F converge to their mean value.
Formally, the empirical Rademacher complexity of F on a sample set S = (x1, . . . , xn) where each
sample Xi 〜D, is defined as
Rn(F) = I E
n
sup	ξif(Xi)
f∈F i=1
where ξ = (ξ1, . . . , ξn) is a vector of n i.i.d. Rademacher random variables (each is +1 w.p. 1/2
and -1 w.p. 1/2). The expected Rademacher complexity is then defined as
E[Rn(F)]
n ξ,{χi}n=ι〜Dn
n
sup	ξif (Xi)
f∈F i=1
13
Under review as a conference paper at ICLR 2022
Given the above definition of Rademacher complexity, we have the following lemma to bound the
worst deviation of the population average from the corresponding sample average over all f ∈ F
(also known as uniform convergence).
Lemma 11 (Theorem 26.5 from (Shalev-Shwartz & Ben-David, 2014)). Given a function class F of
functions on inputs x, if for all f ∈ F, and for all x, |f (x)| ≤ c, we have with probability ≥ 1 - δ,
E [f(x)] ≤ E[f(x)] + 2 E[Rn(F)] + c∖产1*”).
X〜D	n	Vn
The next lemma provides a bound on the spectral norm of a random matrix which is useful in
bounding the norm of the weight matrix of the random ReLU layer in our analysis.
Lemma 12. Given an D × d matrix C where each row is drawn from the d-dimensional Gaussian
N(0, I /D), we have that for a large enough constant c1, if D > max(d, c1), kCk2 ≤ 4 with
probability 1 - exp(-c2D) for some other constant c2.
Proof. Given our choice of D, the bound follows by a direct application of Corollary 3.11 from
Bandeira et al. (2016) followed by simple calculations.	□
We state the following two folklore results without proof.
Claim 13. Let b1, b2 be two vectors sampled uniformly at random from the k-dimensional ball of
unit radius Sk-1. Then
P h^ι>b2∣= O(1∕√k)i ≥ 1- E.
Claim 13 implies that our condition of τ -separatedness is consistent with the manifold distribution
M being over a non-trivial number of manifolds.
Fact 14. The Frobenius norm is invariant to multiplication by orthogonal matrices. That is, for every
matrix unitary matrix U and every matrix A,
kUAkF = kAkF
The following claim is also folklore.
Claim 15 (Preserving dot products with small dimensions). Let x1 , .., xn denote a collection of
d-dimensional vectors ofat most unit norm where d is large. Let k = O (log n 1og(1∕δ)∕ε2) and let
R ∈ Rk×d be a random projection matrix whose entries are independently sampledfrom N(0, 1∕√k)
that projects from d-dimensions down to k dimensions. Then R preserves all pairwise dot products
hxi , xj i within additive error ε with probability ≥ 1 - δ.
Proof. This is a slight variant of the well-known Johnson-Lindenstrauss Lemma. We have that with
probability ≥ 1 — δ, for all i ∈ [n], ∣∣Rxi∣∣2 = (1 ± ε)∣∣Xi∣∣2 and ∣∣R(x + y)∣∣2 = (1 ± ε)∣∣x + y∣∣2.
Squaring both sides giveshRx, Ryi =(x, y)± O(ε).	□
We next study our notion of classification loss, namely the weighted square loss we proposed in
Section 2. We relate this to the more commonly known 0∕1 classification loss now. The 0∕1 loss
is defined as the fraction of mis-classified examples. We next state a lemma which shows that if
our variant of the weighted square loss is really small, then the 0∕1 loss is also small. Given an
m-dimensional prediction y define label(y) = argmaxι∈[m] {yι}.
Lemma 16. Given m train manifolds, for any ε > 0, let ει be such that4(m-])> ει > 0. Then, we
have over our train data,
mn
L(Y,Y) ≤ ει =⇒ XX 一1(1abe1(yii) = label(yii)) ≤ ε.	(7)
mn
l=1 i=1
14
Under review as a conference paper at ICLR 2022
Proof. Let us focus on a single example x with associated true label vector y and predicted vector
y. Suppose the label of X is l for some l ∈ [m]. Then the weighted square loss being smaller than a
value ε(x) implies
( X 97------------Hy2 I + 9(1 - yl)2 ≤ ε(X)
2(m - 1) j 2
j∈[m],j6=l
X	2(m1- 1) (y2 + (1 - yl)2)≤ ε(X)
j∈[m],j 6=l
j∈Xj=ι Jεjl ≤ ε(x),
(8)
(9)
(10)
where εjι = y2 + (1 - yι)2. Wehave |yj | ≤ √ε~l and |1 - yι∣ ≤ √εj =⇒ yι ≥ 1 - √εj which
implies that if εji < 1/4, yι > yj. Now by Markov,s inequality We have that the number of indices
j = l for which εj < 1/4 is greater than (m - 1)(1 - 4ε) > (m - 2) if ε(x) <4(/-^. Therefore,
label(^) = l if ε(x) <4(/-^. Averaging over all examples, We have that if the average weighted
square loss ει ≤4(肃-]), then by another application of Markov,s inequality, the function label(y)
will mis-classify only an ε fraction of the train examples giving the statement of the lemma. □
Lemma 17. Let W ∈ Rm×d. Then,
kwkι = min 1 (kAkF + kBkF)= min]⑷山 1网尸.	(“)
A,B 2	A,B
W=AB	W=AB
Proof. The proof of the Lemma is folklore and we provide it for completeness sake. Using the matrix
Holder inequality and ab ≤ ɪ (a2 + b2) we have,
kwkι = kABkι ≤kAkFkBkF ≤ 2(kAkF + kBkF)
Minimizing over A, B s.t. W = AB gives one side of the inequality. On the other hand, let the
singular value decomposition W = UΣV>, Then, for A = UΣ1/2 and B = 夕1/2 VT, we have that
kAkF = kBkF = k∑"kF = k∑"kF so, ∣∣A∣∣fkBkF = k∑"kF = trace(∑) = kWkι. Also,
1 (kAkF + kBkF) = k∑"kF,so kWkι = kAkFkBkF = 1 (kAkF + kBkF),sotheinequalityis
tight.	□
We now state a well-known claim about dual spaces.
Claim 18. The dual of the operator norm is the trace norm and vice versa. Given two matrices A
and B define hA, Bi = T r(A>B) = Pi,j Aij Bij. Then,
∣∣Ak* =	SUp	hA,Bi
B s.t. kBk2≤1
and kAk2 =	sUp	hA, Bi.
B s.t. ∣∣B∣∣*≤1
(12)
(13)
We next state a lemma which characterizes the structure of matrices A, B at any local minima of
minW =AB kAk2F + kBk2F. This lemma or a variant might have been used in prior work but we
couldn,t find a direct reference. We provide a proof here for completeness.
Lemma 19. Let w ∈ Rm×d
constrained optimization:
1 1 .	♦/	7∖ T . A ~f∖ 1	.1	∙	∕' .1 Γ 11	∙
and let r = min(m, d). Let A, B be the minima of the following
min
W=AB
kAk2F + kBk2F
then at a local minimum there is a matrix R so that if svd(w) = USVT then A = U S1/2R, B =
RTS1/2VT where RRT = I. (Here if w is not full rank the SVD is written by truncating U, S, V in
a way where S is square and full rank).
15
Under review as a conference paper at ICLR 2022
Proof. First assume W = I and A, B are square. Then B = A-1. So we can write svd(A) = USVT
and svd(B) = US-1VT where S is diagonal. Now kAkF = kUSVT kF = kSkF as multiplying by
orthonormal matrix doesn’t alter Frobenius norm (Fact 14). But kSkF + kS-1 kF = Pi(Si2i+ 1/Si2i),
which is minimized only when Sii = ±1. So A becomes orthonormal. Further note that if that is not
the case then it cannot be a local minima as there is a direction of change for some Sii that improves
the objective.
Next look at the case when W may not be I but is full rank and A, B are square Then let
svd(W) = USVT. So AB = USVT. So S-1/2UTABV S-1/2 = I. Now since S-1/2UTA
and BV S-1/2 are inverses of each other we can write their SVD as U2S2V2T and V2S2-1U2T .
So A = US1/2U2S2V2T and B = V2S2-1U2TS1/2VT. SO |B|F = |S2-1U2TS1/2|F, and
|A|F = |AT |F = |S2U2TS1/2|F. LetY = U2TS1/2. Then |A|F + |B|F = |S2Y|F + |S2-1Y|F =
Pi(S2)2iIYi,*lF +(I/S2)2i|YUF = Pi((S2)2i + (1/S2)2i)lYi,*lF. This is again minimized when
(S2)ii = ±1 which means S2 is orthonormal. So A = U S1/2R, and B = RT S1/2V where R is
orthonormal. Again note that if this is not true then some (S2)ii can be perturbed and the objective
can be locally improved.
The argument continues to hold if A, B are not square as the only thing that changes is that R now
can become rectangular (with possibly more columns than rows) but still RRT = I.
If W is not full rank again we can apply the above argument in the subspace where W has full rank
(or equivalently writing SVDs in a way where the diagonal matrix S is square but U, V may be
rectangular but still orthogonal).	□
Corollary 20. For any convex function O(.), minA,B O(AB) + (kAk2F + kBk2F ) can only be at a
local minimum when A = US1/2R and B = RTS1/2VT for some UUT = I, RRT = I, V TV = I
Proof. This follows from the fact that otherwise the previous lemma can be used to alter A, B while
keeping the product AB fixed and improve the regularizer part of the objective.	□
B Our Theoretical Results
In this section, we state our main theorems formally. We re-state our generative process to remind the
reader.
Generative Process We consider manifolds which are subsets of points in Rd . Every manifold
M (γ) has an associated latent vector γ ∈ Rs, s ≤ d which acts as an identifier of M (γ). The
manifold is then defined to be the set of points x = f(γ, θ) = (f1(γ, θ), . . . , fd(γ, θ)) for θ ∈ T ⊆
Rk, k < d. Here, the manifold generating function f = {fi(∙, ∙)}d=ι where the f are all analytic
functions. θ acts as the “shift" within the manifold. Without significant loss of generality, we assume
our inputs x and γs are normalized and lie on the Sd-1 and Ss-1 respectively. Given the above
generative process, we assume that there is a well-behaved analytic function to invert it.
Assumption 2 (Invertibility: Restatement of Assumption 1). There is an analytic function g(∙):
Rd → Rs with norm (Definition 5) bounded by a constant s.t. for every point x = f(γ, θ) on M (γ),
g(x) = γ.
Next we describe how we get our train data. As described above, a set of analytic functions {fi} and a
vector γ together define a manifold. A distribution M over a class of manifolds supp(M) (given by
the {fi}) is then generated by having a set Γ from which we sample γ associated with each manifold.
We assume that for any two manifolds M1 , M2 ∈ supp(M), γ1>γ2 ≤ τ where τ < 1 is a constant.
To describe a distribution of points over a given manifold M we use the notion of a point density
function D(∙) which maps a manifold M to a distribution D(M) over the surface of M. Training
data is then generated by first drawing m manifolds Mi,..., Mm 〜M at random. Then for each
l ∈ [m], n samples {(xli, yil)}in=1 are drawn from Ml according to the distribution D(Ml). Note that
we view the label yil as a one-hot vector of length m indicating the manifold index. We consider a
3-layer neural network y = ABσ(Cx), where the input X ∈ Rd passes through a wide randomly
initialized fully-connected non-trainable layer C ∈ RD×d followed by a ReLU activation σ(.). Then,
there are two trainable fully connected layers A ∈ Rm×T , B ∈ RT ×D with no non-linearity between
16
Under review as a conference paper at ICLR 2022
them. Each row of C is drawn i.i.d. from N(0, DI). It follows from random matrix theory that
kCk2 ≤ 4 w.p. ≥ 1 - exp(-O(D)).
Theorem 1 is our main theoretical result which is an informal variant of the following two theorems.
Theorem 3 (Main Theorem: GSH for Linear Manifolds). Let M be a distribution over τ -separated
linear manifolds in Rd such that the latent vectors all lie on Ss-1. Given inputs x such that kxk2 = 1,
let y = ABσ(Cx) be the output of a 3-layer neural network, where A ∈ Rm×T, B ∈ RT×D are
trainable, and C ∈ RD×d is randomly initialized as described above. Suppose we are given n data
points from each of m manifolds sampled i.i.d. from M. For any ε > 0, any local minima A, B of
the following loss
La,b (Y,Y) + λι (∣A∣F) + λ2(kBkF)
satisfies the following with probability ≥ 1 - δ,
1.	The expected weighted square loss on test samples from the train manifolds is small:
m1 Pm=IExl〜D(Ml) [kwl Θ (yl - yl)k2] ≤ o⑸,
2.	The representation computed by Bσ(C.) satisfies (ε, 1∕ε)-GSH.
for n = Θ (s°(^α?1Og(M)), “ = ® (，吁或：log%)) and
T = Θ (log(mn)log(1∕δ)ε-1) , D = Θ (√mnlog(mn/f ), α向 λ1 = ε∕m, λ? = ε∕sο(Iog(I左)).
Theorem 4 (Main Theorem: GSH for Non-Linear Manifolds). Let M be a distribution over τ -
separated manifolds in Rd such that the latent vectors all lie on Ss-1. Given inputs x such that
kxk2 = 1, let y = ABσ(Cx) be the output of a 3-layer neural network, where A ∈ Rm×T, B ∈
RT ×D are trainable, and C ∈ RD×d is randomly initialized as described above. Suppose we are
given n data points from each of m manifolds sampled i.i.d. from M. For any ε > 0, any local
minima A, B of the loss
LA,B (Y,Y) + λ1 kAkF + λ2 (kBkF + Veg (Bσ(C)))
satisfies the following with probability ≥ 1 - δ,
1.	The expected weighted square loss on test samples from the train manifolds is small:
m Pm=IExl〜D(Ml) [kwl ® (yl - yl)k2] ≤ O(E),
2.	The representation computed by Bσ(C.) satisfies (ε, 1∕ε)-GSH.
for n = Θ (Sg"I) log")), m = Θ (‘呼和? log⑼δ) Jand
T = Θ (log(mn) log(1∕δ)ε-1) , D = Θ (√mnlog(mn∕6 ), α向 λ1 = ε∕m, λ2 = ε∕so(Iog(I左)).
A benefit of having the hashing property is we get easy transfer learning. This was Theorem 2 in the
main body. We now re-state this theorem provide its proof below.
Theorem 5 (GSH Implies One-Shot Learning). Given a distribution M over τ -separated manifolds,
if a representation function r(∙) satisfies the (ε, P)-GSH property over M with probability ≥ 1 一 δ,
a large enough ρ, then we have one-shot learning. That is there is a simple hash-table lookup
algorithm A such that it learns to classify inputsfrom manifold Mnew 〜 M With just one example
with probability ≥ 1 一 δ.
Proof. Let A be the following algorithm. Given a single example XneW 〜Mnew, We compute
r(xnew). Then given any other input x, it does the following:
if kr(X) 一 r(XneW)k22 < 2ε, then X ∈ Mnew,
else X ∈/ Mnew .
Since r(∙) satisfies the (ε, P)-GSH w.p. ≥ 1 一 δ, for P ≥ 2, we have that A mis-classifies an input X
only with probability ≤ δ.	□
Next, it remains to prove Theorems 3 and 4. We split the proofs over multiple sections. Section C
studies the properties of our architecture which is expressive enough to enable us to learn the geometry
17
Under review as a conference paper at ICLR 2022
of the manifold surfaces. Section D analyses our loss objective to show an empirical variant of the
GSH for both linear and non-linear manifolds. Finally Section E is about generalizing from the
empirical variant to the population variant. All three put together give us Theorems 3 and 4.
C KERNEL FUNCTION VIEW OF RANDOM LAYER WITH ACTIVATION σ
We start by looking at some properties of a wide random ReLU layer. At a high level, our goal is
to show that a random ReLU layer computes a transform of the input which is highly expressive.
Formally, we will show that a linear function of the feature representation computed by the random
ReLU layer can approximately express ‘well-behaved’ analytic functions. Our formalization of what
we mean by ‘well-behaved’ is a bit technical and relies on the understanding we develop of the
transformation an input goes through via a random ReLU layer. We develop this understanding via a
sequence of lemmas.
The first is the following simple lemma which focuses on a single node of a random ReLU layer and
defines a kernel on the implicit feature space computed by a ReLU using the dual activation function
of ReLU.
Lemma 21. (Random ReLU Kernel) For any x, y ∈ Rd and r drawn from the d-dimensional normal
distribution,
Er [σ(r>χ)σ(r>y)] = K(X, y) = kχk2kyk2σ ([就口；心).
where σ(η) = √1-η2+⅛cos-⅛.
Proof. The result follows by noting the form of the dual activation of ReLU from Table 1 of (Daniely
et al., 2016) together with the observation that for any unit vectors u, v, the joint distribution of
(r> u, r> v) is a multivariate Gaussian with mean 0 and covariance,
>	>	(u>	(kuk22	u>v (1	η
cov(r u,r v)	= v>	cov(r)(u, v)	= u>v	kvk22	= η	1	,
When U = x∕∣∣x∣∣2 and V = y∕ky∣∣2.	□
We let N = mn denote the total number of samples We have from all our train manifolds. Recall that
X denotes a rank-3 tensor of size m × d × n obtained by stacking the Xl matrices for l ∈ [m]. In
the rest of this section, We override notation and flatten X to be a d × N matrix. Given the kernel
function K(.) from Lemma 21, We let K(X, X) be the N × N kernel matrix Whose (i, j)th entry
is σ(CXi)>σ(CXj) (Where Xi is the ith column of X). Next, We have the folloWing result Which
shoWs that With high probability, for any tWo inputs among our N train inputs, the inner product of
the feature representation given at the end of a random ReLU layer is close to the kernel evaluation
on this pair of inputs.
Lemma 22. Let N = mn and let D = Θ (√Nlog)2* 沁δ). Then letting ZD = σ(C ∙ X) where
ZD ∈ RD×N, we have,
E [ZD>ZD] = K(X, X)	(14)
where X is the train set (and each column is of norm 1), and K(X, X) is the Random ReLU kernel
given in Lemma 21. Moreover, for any ε, δ > 0, w.p. ≥ 1 - δ,
kZD>ZD -K(X,X)kF ≤ε
Proof. For the first part of the lemma, let any tWo indices j, k and let xi and xj be the appropriate
columns of X. Then, E ZD>ZD in coordinate j, k is,
D
E [ZD>ZD]j,k = E X σ(rt>xj)σ(rt>xk)	= K(xj,xk)
t=1
18
Under review as a conference paper at ICLR 2022
Where rt is the tth (random) row of C and we use linearity of expectation and Lemma 21. For
the second part, write σ(r>Xj)σ(r>Xk)〜 Dσ(Yt)σ(Zt), where Yt, Zt are jointly distributed as
N 0, ρ1 ρ1 . Now we note that σ(Yt)σ(Zt) is a sub-exponential random variable (e.g., see
(Vershynin, 2019)), either by noticing that it is a multiplication of sub-Gaussians or directly by taking
any a > 0 and writing,
P[σ(Yt)σ(Zt) >a] ≤ P[|H| > √ɑ)] + P[|Zt| > √a] ≤ 4exp(-a∕2)
And so, for all a > -μ,
P[σ(Yt)σ(Zt) - μ>a] ≤ 4exp (-(a^
Thus, using a property of the sub-exponential family, there exists some universal constant c > 0,
1 口
P D x
D
t=1
So by taking D = Θ
E[σ(Yt)σ(Zt)] - μ >ε
√N log(2n2∕δ)
w.p. 1-δ, kZD>ZD -K(X,X)k∞
-Dε
≤ 2 exp (--
and using the union bound over all N 2 coordinates, we have
≤ √N andthus,
ε
kZD>ZD -K(X,X)kF ≤ε.
□
Next, we show a linear algebraic result which argues that if two sets of vectors have the same set of
inner products amongst them, then they must be semi-orthogonal transforms of each other. Recall
that a rectangular matrix with orthogonal columns (or rows) is called semi-orthogonal.
Lemma 23. Let X ∈ RD×n and Y ∈ RD×n and let X>X = Y >Y , assuming D ≥ n. Then there
exists a semi-orthogonal matrix U with orthogonal columns such that X = UY .
Proof. IfY is invertible then let U = XY -1. Then clearly UY = X and
U>U = (Y-1)>X>XY-1= (Y-1)>Y>YY-1=I.
Now if Y is not invertible, then first note that X>X and X have the same null space (as they have
the same right singular vectors and the singular values for the former are squares of those of the
latter), and since X > X = Y >Y, X and Y have the same null space. Write U 0 = XY * where Y * is
the pseudoinverse of Y and let V be the identity transformation on ker(Y ) (and 0 everywhere else).
Then, we claim that U = U0 + V is an orthogonal matrix such that X = UY. The main point is that
U0> U0 is an identity operator outside kerY and inside kerY, V is an identity operator. To see that
X = UY, note that for every X ∈ Rn, write X = X + x⊥, decomposing X to span Y ㊉ ker Y. Then,
we have,
UY X = (XY *Y + VY )(X + X⊥) = XY *Y X = X X = X X,
where We used Xx⊥ = Yx⊥ = 0, Vy = 0 for y = YX ∈ span Y and Y*Y = I on span Y.
So, UY and X agree as transformations on all of Rn and therefore are the same. Now, U> U =
(U0 + V)>(U0 + V) = U0>U0 + V>V as U0 ⊥ V. But U0>U0 is the identity on span Y (and 0
elsewhere) and V›V is the identity on ker Y (and 0 elsewhere) so U>U = I.	□
When XTX is only approximately equal to YTY, a weaker variant of Lemma 23 still holds.
Lemma 24. If therea sequence of matrices Xi, Yi ∈ RDi×n so that Xi> Xi, Yi>Yi → A asDi → ∞
then Xi = UiYi + ∆i where Ui are orthonormal and k∆ikF → 0. Precisely if kXi>Xi - AkF ≤ ε
andkYi>Yi -AkF ≤ ε, then ∣∣∆i∣∣F ≤ 2√ε. Although we assumed Xi, Yi have the same number of
rows, if they were different we could pad the smaller matrix with zero vectors to get them to be the
same shape.
19
Under review as a conference paper at ICLR 2022
Proof. Let (.)1/2 denote the matrix square-root operator which is defined as follows: A1/2 =
U夕1/2 V> where Svd(A) = UΣV>. Note that this operator is continuous. Let Bi = (X>Xi)1/2
and Ci = (Yi>Yi)1/2. Let us pad Bi, Ci with zero rows so that they are both of dimension Di × n then
by continuity of the square root of a matrix, if ∆0i = Bi -Ci, k∆0ikF → 0. Note that Bi>Bi = Xi>Xi.
Then, from Lemma 23 we have that Xi = Pi Bi where the Pi are orthonormal. Similarly, we have
Yi = QiCi where the Qi are orthonormal. So Xi = PiBi = PiQi>QiBi = PiQi>Qi (Ci + ∆0i) =
PiQi>QiCi + ∆i =	PiQi>Yi + ∆i. Finally,	k∆ikF	=	kPiQi>Qi∆0ikF = k∆0ikF from	Fact 14.
Therefore, k∆i kF →	0. Hence we have Xi =	UiYi +	∆i	where Ui = Pi and k∆i kF → 0.
To make this precise, we note that for two n × n square matrices U, V, kU 1/2 -V1/2 k2 ≤ n-1/2 kU-
V∣∣2 (Carlsson, 2018) and so ∣∣U 1/2 - V1/2IIF ≤ ∣∣U - VIIF . So IIBi - A1∕2∣∣f ≤ √ and
∣∣Ci 一 A1∕2∣f ≤ √ε. and so ∣∣∆i∣F ≤ 2√ε and hence ∣∣∆i∣∣F ≤ 2√ε.
□
Now, let σ(η) = 2∏ + 1 η + 4∏η2 + 表η3 + ... = qo + q1n + q2η2 + q3η3 ... denote the Taylor
series expansion of σ, the dual activation of ReLU defined in Lemma 21. Note that qk decays as
O (k3/2). So for η ≤ 1 We can approximate this series within ε error as long as We use at least the
first O(1∕ε2/3) terms.
We will now argue, using Lemmas 23 and 24, that the output of the random ReLU layer
can be viewed with good probability as approximately an orthogonal linear transformation ap-
plied on a power series φ(x), where φ(x) = (√q0, √q1x, √q2x02, √q3x04,...), an infinite
dimensional vector where x0i is a flattened tensor power i of the vector x. Let φk(x) =
(√q0, √qιXi, √q2X^2, √q3X^3,... √qkXfk) denote the truncation of φ(x) UP to the kth tensor
powers. The following Lemma allows us to think of a random ReLU layer of high enough width as
kernel layer that outputs a sequence of monomials in its inputs.
Corollary 25. For all ε, δ > 0, all k ≥ O((N∕ε)2/3) if the width D of the random ReLU layer
is at least Θ ( VZNlog(;(N)/δ), then, w.p. ≥ 1 — δ there exists an semi-orthonormal matrix U ∈
RD×O(dk), and δ ∈ Rd×n, ∣Δ∣f < 2√ε such that,for the train matrix X ∈ Rd×N,forall i,
σ(CXi) = Uφk (x) +∆i.	(15)
where Xi is the ith column of X and ∆i the ith column of ∆.
Proof. For two input vectors x, y, we have,
hφ(x), φ(y)i = qo + q1hx, y〉+ q2 hx02, y02i + q3hxβl4, y04i + ...
For any J = (J1, . . . , Jd) ∈ Nd, write a monomial xJ = x1J1 . . . xdJd and define |J| = Pk Jk. By
definition, x0i is the vector of all monomials of the form (XJ; | J| = i) and so,
(χθi, Pi = X XJyJ = hx, y〉i,
|J |=i
where the last equality is just rearranging the terms of the power of the dot product. Therefore,
we can write hφ(x), φ(y)i = K(x, y) and φ(X)>φ(X) = K(X, X). Now, since ∣x∣2 ≤ 1,
hφk(x), φk(y)i is a O(1∕k3/2) approximation to σ(x>y) for all pairs x, y. Hence, we have that for
k = O((N∕ε)2/3), ∣∣φk(X)>φk(X) 一 K(X,X)∣f ≤ ε. Moreover from Lemma 22 we have that
w.p. ≥ 1 - δ, ∣ZD>ZD - K(X, X)∣F ≤ ε for our chosen width D. Now we can use Lemma 24 to
conclude that there exists a semi-orthogonal matrix U ∈ RD×O(dk) and an error matrix ∆ ∈ RD×N,
such that,
σ(CX) = U ∙ φk(X) + ∆
and ∣∣Δ∣∣f < 2√ε.	□
The following Lemma quantifies the norm of a function p(x) given as a Taylor series when expressed
in terms of a random ReLU kernel. We will assume, without essential loss of generality, that in the
Taylor series of the random representation φ(x), for every monomial XJ the corresponding coefficient
20
Under review as a conference paper at ICLR 2022
qj is non-zero. This is because by adding a constant to our input with subsequent renormalization,
i.e. x0 = (x/√2,1/√2) We can use as kernel K0 where K0(x, y) = K(x, y) + 1 wherein all the
monomials exist as the Taylor series of σ is non-negative (also see Agarwala et al. (2021), Corollary
3, and also Lemma 9 for a matching lower bound for expressing p(x) in terms of a wide random
ReLU layer for a certain distribution of inputs).
Lemma 26. For any ε, δ > 0 and multi-variate polynomial p(x) = J pJxJ w.p. ≥ 1 - δ we can
approximate p via the application of a random ReLU kernel of large enough width followed by a dot
product with a vector a, i.e. aσ (C x), so that |p(x) - aσ (C x)| ≤ ε for any x in our train samples
and ∣∣ak2 = EJ PJ/qj where qj is the coefficient of the monomial XJ yj in σ(x>y).
Proof. This follows from Corollary 25 and taking aU to be the vector of the coefficients of p divided
by the appropriate coefficients of the Taylor series of φ(X). To ensure that every monomial has a
non-zero coefficient in the Taylor series of the representation φ(.), we add a bias term to our input as
described in the paragraph above.	□
C.1 FORMALIZING BOUNDED-NORM ANALYTIC FUNCTIONS: THE q-NORM
Given the understanding developed so far, we now define a norm of an analytic functions which
formalizes the intuition that we want our inverting analytic function g(.) from Assumption 1 to be
expressible approximately using a wide enough random ReLU layer. We use Lemma 26 to define a
notion of norm for any analytic function g. Given the vector q of coefficients of the series φ(X), we
will define ∣g∣q to be the norm of g’s approximate representation using an infinitely wide random
ReLU layer. That is given an infinite dimensional vector a and an infinitely wide random ReLU layer,
let
∣g∣q
min ∣a∣2 .
a,aσ(C.)=g
(16)
We call ∣g ∣q the q-norm of g. We can see that ∣g∣q2 ≤ PJ gJ2 /qJ where gJ are coefficients of
monomials in the representation of g and qJ are the coefficients of the Taylor series of φ(.). We next
present Lemmas which will show that for most natural well-behaved analytic functions which to not
blow up to ±∞ the q-norm is bounded (see Remark 1).
The following lemma fromAgarwala et al. (2021)bounds ∣∣g∣q for univariate functions - there the
notation ∙λ∕Mg was used for ∣∣g∣q insteadjust as in (Arora et al., 2019).
Theorem 6.	Agarwala et al. (2021)Let g(y) be a function analytic around 0, with radius of conver-
gence Rg. Define the auxiliary function g(y) by the power series
∞
g(y) = X |ak |yk	(17)
k=0
where the a，k are the power series coefficients of g(y). Then thefunCtion g(β ∙ x) satisfies,
kgkq ≤ βg0(β)+ 贸O)	(18)
if the norm β ≡ ∣β∣2 is less than Rg.
The tilde function is the notion of complexity which relates to the q-norm. Informally, the tilde
function makes all coefficients in the Taylor series positive. The q-norm is essentially upper bounded
by the value of the derivative of function at 1 (in other words, the L1 norm of the coefficients in
the Taylor series). For a multivariate function g(x), we define its tilde function g(y) by substituting
any inner product term in x by a univariate y . The above theorem can then also be generalized to
multivariate analytic functions:
Theorem 7.	Agarwala et al. (2021)
Let g(x) be a function with multivariate power series representation:
k
g(x) = XX av Y(βv,i ∙ x)	(19)
k v∈Vk	i=1
21
Under review as a conference paper at ICLR 2022
where the elements of Vk index the kth order terms ofthe power series. We define g(y) = Ek Gkyk
with coefficients
k
aGk = X |av| Y βv,i.	(20)
v∈Vk	i=1
If the power series of gG(y) converges at y = 1 then kgkq ≤ gG0(1) + gG(0).
Let g+ (x) denote the same Taylor series as g(x) but where all coefficients have been replaced by
their absolute value. Let kgkqu denote the upper bound gG0(1) + gG0(0) as in Theorem 7 which ensures
that kg kq ≤ kgkqu. The following claim is evident from the expression for kgkqu.
Claim 27. The q-norm of an analytic function g satisfies the following properties.
•	kgkq ≤ kg kqu
•	kgkqu = kg+ kqu.
•	kg1+ + g2+ kqu = kg1+ kqu + kg2+ kqu.
Corollary 28. If for s functions g1 (x), ., gs(x) functions gGi 0 (1) ≤ O(1), gGi(1) ≤ O(1), then
k(Pi gi(x))ckqu ≤ c(O(s))c
____ .. .... ~., ~, ~, ________________________________ .. ~
Proof. Letf(x) = (Pigi(x))c. Then kfkq ≤ fG0(1)+fG(0) where fG(y) = (Pi gGi(y))c. SofG0(1) =
____ . .	-1__~	,	. .	. .	-1	. .	~ ,	~ ,	..
C(Pi 或⑴)c-1(Pi g0 i(1)) = C(O(S))CTO(S) = C(O(S))c. And f(0) ≤ f(1) ≤ (O(S))c.	□
Remark 1. Most analytic functions which do not blow up to ±∞ and are Lipschitz and smooth will
have a bounded q-norm according to our definition. As a concrete example to gain intuition into
q-norms of analytic functions, the function f (x) = eβ1∙x ∙ sin(β2 ∙ x) + cos(β3 ∙ x) has constant
q-norm ifβ1,β2,β2 all have a constant norm.
D	Properties of Local Minima
In the previous section, we have seen that the representation computed by a random ReLU layer is
expressive enough to approximate ‘well-behaved’ analytic functions. In this section we will leverage
this understanding to show that (a) there are good ground truth weight matrices A*,B* which learn
to classify our train manifolds well while satisfying the GSH property, (b) and consequently any local
minima of our optimization will also be a good classifier for our train data and satisfy the GSH. We
start with point (a). We will assume the that the g() function satisfies the conditions of Corollary 28.
Lemma 29 (Existence of Good Ground Truth). Given data from manifolds and the 3-layer architec-
ture as described above, there exist ground truth matrices A*,B* such thatfor any ε1,ε2 > 0, with
probability ≥ 1 - δ,
一 ________ʌ.
1.	LA*,B*(Y, Y) ≤ ε1,
2.	∣∣A*kF ≤ m,
3.	kB*kF ≤ SO(IOg(II)),
ʌ , _ . , _...
4.	Vmn(B σ(C∙)) ≤ ε2.
Proof. The desired output y is a non-continuous function whose outputs are either 0 or 1. We will
approximate each coordinate of the output y by a continuous polynomial. First we recall that for
any two distinct γι and γ2 from our distribution M we have hγ1,γ2i ≤ √1s by assumption of
τ -separatedness. For any ε > 0, define
μ(u, v) = hu, Viclog(1/G.	(21)
where u, V are vectors representing two possible values of Y and C ≥ 1/log(1∕τ) is a constant
chosen so that Clog(1∕ε) is an integer. Then we have, μ(u, v) = 1 if and only if U = V and if
hu, Vi ≤ T, then μ(u, v) ≤ ε.
22
Under review as a conference paper at ICLR 2022
Hence, for xl sampled from manifold Ml we have that g(xl) = γl and,
ε (at most) l 6= j,
μ(Yj,g(χι)) = ∣1v ，l = j
Let yι* = (μ(γι, g(xι)), μ(γ2, g(xι)),..., μ(Ym, g(xι))). Then We have that the weighted square
loss term corresponding to xι ∣∣wι Θ (yι - yj)∣∣2 ≤ ε2∕2. Based on Corollary 25 without loss of
generality, we assume that the random ReLU layer outputs the monomials Φ(x) in co-ordinates of x
6. We will now find matrices A*, B * so that, A*B*σ(C xι) = yj approximately. Then bullet 1 of
the Lemma will immediately follow.
To do so, we express,
μ(u, V) = hψ(u),ψ(v)i,	(22)
where ψ(u) and ψ(v) are bounded-norm vectors. We do this using the binomial expansion of
hu, Vic Iog(I. We can write it as a weighted sum of monomials where each monomial is a product
of two similar monomials in u and v. We can enumerate these monomials by their degree distribution.
Let J = (J1 , . . . , Jd ) ∈ Nd denote the degree distribution of a monomial in d variables. We will use
the notation xJ = x1J1 . . . xdJd to denote such a monomial over x. Then |J | = Pk Jk is the degree
of the monomial. The expanded expression for μ(u, v) can be written as Pj∙∣ j ∣=c ιog(i∕ε) a J UJ VJ.
This in turn can be written as a dot product of two vectors whose dimension equals the total number
of monomials of degree Clog(1∕ε) in S variables, which is (cIog(S"+s-1) = O (ScIog(I). So
precisely, μ(u, v) = ψ(u) ∙ ψ(v) where ψ(u) is a vector whose coordinates can be indexed by
the different values of J and the value at the Jth coordinate is (ψ(u))j = √αJUJ. Clearly then
hψ(u), ψ(v)i = PJ ajujvJ = μ(u, v).
We will now describe the matrices A*, B*. For now, assume that the random ReLU kernel σ(C.) is
of infinite width. We will choose the width of the hidden layer (number of rows in A*) to be exactly
the number of different values of J. This width can be reduced to O(log(mn)∕ε2) at the expense
of an additional ε error per output coordinate of A* as shown in Lemma 30. Given this width, we
simply set the lth row Al* = ψ(γl). Then B* is chosen such that the output of the hidden layer
r = B*σ(Cxl) ≈ ψ(g(xl)). To see that such a B* exists, note that we need the Jth coordinate of r,
rj = √αJ (g(xι))J. Since g(xι) is analytic with a bounded norm, the √aJ (g(xι)) J are alsobounded-
norm analytic functions in xl and so by Lemma 3 these can be expressed using a linear transform of
σ(Cxι) (as the width goes to infinity). So BJ is chosen such that BJ ∙ σ(Cx) = √αJ(g(x))J. Now
let us look at the Frobenius norms of A*, B* constructed above. First ∣A* ∣2F =Plm=1 ∣Al*∣22 =m,
since,
∣Al* ∣22 = hψ(γl), ψ(γl)i = g(γl, γl) = 1.
Next, we can use Lemma 3 to express the norm of B* as ∣B* ∣2F = PJ aJ ∣g(x)J ∣2q where the qJs
are the coefficients of Φ(x). Note that this is independent of m and given g(x), δ it only depends
on ε therefore we can write ∣B* ∣2F = T(ε) where T is only a function of ε. Note that T(ε) =
Pj aJ kg(xJkq ≤ Pj aJ kg+(x)Jkqu ≤ k Pj aJ g+(x)Jkqu ≤ k(P g+(x))c ^⑴叫3 By
Corollary 28 this is at most SO(Iog(I/E)).
Moving to the bound on Vreg, this is easy to see once we note that B* is such that for any l ∈ [m], for
all i ∈ [n], B*σ(Cxil) ≈ p(xil) = γl and hence has very low intra-manifold variance.
So far we assumed the random ReLU layer to be monomials Φ(x) according to infinite width kernel.
Now, we argue that if we use a large enough width D, then by Corollary 25 there is an orthogonal
matrix U so that σ(Cx) is approximately U Φ(x). If we choose D so that kσ(Cx) - U Φ(x)k2 is at
most ε∕T(ε) then B*Φ(x) will differ from B*Uσ(Cx) by at most Frobenius error ε on any of the n
inputs; this will result in at most additive error ε at each of the outputs in Yi (since each row of A*
has norm at most 1. This is done by setting D = O(√nT(ε)2 log(n∕δ)∕ε2).
□
Lemma 30 (Bounding the Width of the Hidden layer). Given any ε1, ε2 > 0, and A*, B* of
Lemma 29, we can construct new A0, B0 with number of columns in A0 (and number of rows in B0)
equal to O(log(mn) log(1∕δ)∕ε1), such that
6In reality there is an additional orthogonal matrix U but we can define B2 = B* U and subsume it in our
ground truth.
23
Under review as a conference paper at ICLR 2022
一 ______ʌ .
1	LA*,B*(Y, Y) ≤ ε1,
2	∣∣A0kF ≤ m, ∣∣B0kF ≤ SO(Iog(II)),
3	. Vmn(B0σ(C.)) ≤ ε.
Note that now we have the small loss guarantee only on our train examples and not over any new
samples from our manifolds.
Proof. Let the original width of the hidden layer (number of columns in A*) be w. From Lemma 15,
We have that randomly projecting both A* and B* down to O(log(mn) log(1∕δ)∕ε2) dimensions
preserves all the dot products between the normalized rows of A* and normalized columns of B * up
to an additive error ε with probability ≥ 1 - δ. In addition we have that kAl* k = 1 for all l ∈ [m].
So we can replace A*B* by A0B0 = (A*R>)(RB*) where R is the random projection matrix and
get that for each input xil, kA0B0σ(Cxil) - A*B*σ(Cxil)k∞ ≤ εb where b is the maximum norm
of the rows of B * . As an aside, we note that a similar random projection can be applied on top of
the random ReLU layer σ(C.) as well to get a random ReLU layer followed by a random projection
neither of which are trained and resulting in a smaller width ReLU layer.	□
Next, we recall that our objective is of the form
min La,b (Y,Y) + kAkF + kBkF	(23)
A,B
We will argue that the nice properties we saw holding for A*, B* also hold for any global minima of
our optimization (23). This is because of the following lemma.
Lemma 31 (Multi-Objective Optimization). Given a multi-objective minimization where we want to
minimize a set of non-negative functions Oi(θ) for i = 1, . . . , q and there exists a solution θ* such
that Oi(θ*) ≤ OP Ti. Then, we have that
q
min
θ
i=1
Oi(θ)
OPTi
i	K 1.1	. r	ι ■	/久、	- zʌ τ->m .	ι ι ι ■ ■
produces θ such that for each i, Oi(θ) ≤ qOP Ti at any global minimum.
Proof. Note that at global minimum
XX Oi(θ)
OPOPTi
i=1
≤ XX OPT ≤ XX ι = q.
i=1	i	i=1
Since Oi are non-negative functions we have Oi (θ) ≤ qOP Ti.
□
Lemma 31 will guide our choice of regularization parameters λ1, λ2.
Lemma 32. Let A*, B * denote the global optimum of (23). Then, for λι = ε1∕m,λ2
ε1∕s0(Iog(II)), we have
. _______ ʌ..
La*,B* (Y,Y') ≤ 3ει,	(24)
kA*kF ≤ 3m, ∣∣B*∣∣f ≤ SO(Iog(I41)).	(25)
>λ	r∙ τ-.	11	.1	. i`	1	.	.1	Λ Λ 7^^>*	i`	τ	C 八	1	.1	.	z->	广 <V∖	,
Proof. Recall that for	ground truth	A*, B*	from	Lemma	29	we have	that LA*,B* (Y, Y) ≤	ε1,
kA*kF ≤ m and ∣∣B∣∣F ≤ SO(Iog(I41)). Therefore, setting λι = m and λ? = SOao^^)),We get
from Lemma 31 that at global minimum A*, B*
. _______ʌ..
La*,B* (Y,Y) ≤ 3ει,	(26)
∣∣A*∣∣f ≤ 3m, ∣∣B*∣∣f ≤ 3sO(log(1/£I)).	(27)
□
24
Under review as a conference paper at ICLR 2022
Note that the chosen values of λ1 , λ2 will influence the number of steps gradient descent will need to
run to reach a local optimum.
Since our objective is non-convex, itis not clear how good a local optimum we reach will be. However,
for our particular architecture, it turns out that every local minimum is a global minimum.
Lemma 33 (Equivalence to Nuclear Norm Regularized Convex Minimization). Let d > max(m, n).
Then, for any convex objective function O(), in the minimization
A∈mm×dO(AB)+ λ(kAkF + kBkF),
B∈Rd×n
(P1)
all local minima are global minima and the above minimization is equivalent to the following convex
minimization
min O(AB) + 2λ (IlABl∣*) ≡ min O(W) + 2λ (IlW∣∣*).
A∈Rm×d,	W ∈Rm×n
B∈Rd×n
(P2)
Proof. From Lemma 17, it follows that the global minimum of (P1) and (P2) have the same value.
Note that the latter minimization is convex and hence any local minima is global. We now show
that all local minima of (P1) are global as well even though it is potentially a non-convex objective.
Let OPT denote the value of the global minimum of either objective and let A1, B1 be a local
minima of (P1). Suppose for the sake of contradiction that O(A1B1) + λ(IA1 I2F + IB1 I2F) > OPT.
Then it must be the case that ∣∣Aι∣∣F + IIBIllF = 2∣∣AιBι∣∣* as otherwise by Lemma 17 We will
be able to improve the objective by keeping A1 B1 a constant and reducing IA1 I2F+IB1I2F(note
that the sum of Frobenius norms given a fixed product of AB is a convex minimization problem).
Therefore we have that O(A1B1) + 2λ(∣AιBιk*) > OPT. Since (P2) is a convex problem, this
implies that for any ε > 0, within an ε-sized ball around W1 = A1 B1 there exists W2 such that
O(AιBι) + 2λ(∣∣AιBιk*) > Ο(W2) + 2λ(∣∣W2k*). Let W2 = USV> be the Svd of W2 where
U ∈ Rm×r, S ∈ Rr×r and V ∈ Rn×r such that S is a diagonal matrix with non-zero singular
values along the diagonal, and U, V have orthonormal rows. Let A2 = US1/2 , B2 = S1/2V> .
Since d > max(m, n) > r, we can pad A2 with d - r columns which are 0, and pad B2 with
d - r rows which are 0. Even after such a padding we have that A2B2 = W2. Then we have
that 2∣∣A2B2∣∣* = 2||S1/2|IF = kA2kF + ∣∣B2∣∣F andhence O(A2B2)+2λ(∣∣A2∣∣F + ∣∣B2∣∣F) <
O(A1B1) + 2λ(IA1I2F + IB1 I2F) which is a contradiction to the statement that A1, B1 is a local
minima of (P1) as by a continuity argument for any δ > 0, we can find an appropriate ε such that
kW2 — Wi∣f ≤ ε and ∣∣A2 - Ai∣∣f ≤ δ, ∣∣B2 - Bi∣∣f ≤ δ.	□
Corollary 34 (Generalization of Lemma 33). For any convex objective function O(),
minO(AB) + (λιkAkF + λ2kBkF),
A,B
all local minima are global minima and is equivalent to the following convex objective
min O(AB) + "2pλλ (∣∣AB∣∣*).
A,B
Proof. The lemma follows by replacing A, B in the previous lemma by ∖∕λ1∕λ2A, 'λ2/λ∖B
respectively and setting λ to √λ∖λ?	□
Corollary 34 will imply that at any local minimum A, B we have a small value for our weighted
square loss. This is because LA,B (Y, Y ) is convex in AB. Next, we will show that an empirical
variant of the GSH property holds for the representation Bσ(C.) obtained at any local minimum.
Here our approaches for linear and non-linear manifolds differ. Linear manifolds enable a more direct
analysis with a plain '2-regularization. However, we need to assume certain additional conditions on
the input. The result for linear manifolds acts as a warm-up to our more general result for non-linear
manifolds where we have minimal assumptions but end up having to use a stronger regularizer
designed to push the representation to satisfy GSH. We describe these differences in Sections D.1
and D.2.
25
Under review as a conference paper at ICLR 2022
D.1 GSH on Train Data for Linear Manifolds
Here we will show that we can train our 3-layer non-linear neural network on input data from linear
manifolds, to get GSH. To get an intuitive understanding of why this is the case, we first recall
that by passing an input vector x through a random ReLU layer, we get approximately all possible
monomials of x and its higher tensor powers (Corollary 25). Now, we will show that by passing
in a dummy constant as part of the input, the regularization on the weights A and B enforces that
weights corresponding to certain monomials of x are zero at any minima. These weights being zero
will imply Property A of the hashing property. The second part of the hashing property will follow
due to a similar reasoning as in Section D.2. With this high level intuition in mind, we proceed with
the formal proof.
A linear manifold with a latent vector γ can be represented by the set {x = Pθ + Qγ} for some
matrices P and Q. Moreover, without a significant loss of generality we can assume that γ is such
that Qγ ⊥ Pθ for all θ ∈ Sk-1 (as otherwise, we can project Qγ onto the subspace perpendicular
to P θ). The objective function we minimize is (23).
We begin the proof by first performing a transformation on the input that will simplify the presentation.
Lemma 35. Given a point x = Pθ + Qγ where Pθ ⊥ Qγ, there exists an orthogonal matrix U2
σ(Cx) = U2φ(γ0, θ0) + δ,
where γ0 = RQγ and θ0 = RPθ and U and φ(.) are as defined in Corollary 25.
Proof. Since Pθ ⊥ Qγ, a rotation of the bases transforms Qγ to a vector with non-zero entries only
in the first d- k coordinates, and Pθ to lie in a subspace which contains vectors with non-zero entries
only in the last k coordinates. This is made feasible since the rank of the space spanned by θ is ≤ k.
Denote the vectors obtained after these transformations by γ0 and θ0 . We drop the 0 entries to get
γ0 ∈ Rd-k and θ0 ∈ Rk. Therefore, x = R2(γ0, θ0) for some rotation matrix R2. Note that rotation
matrices are orthogonal. Now σ(Cx) = σ(CR2 (γ0 , θ0)). CR is also a random matrix distributed
according to N(0, D ) and hence Corollary 25 applies to it as well giving Us the statement of the
Lemma.	□
In light of Lemma 35, We can assume that our neural net gets as input X = (γ0, θ0) where Y0 ∈ R(d-k)
and θ0 ∈ Rk withoUt loss of generality as after passing throUgh the random ReLU layer all that
differs between the two views is the orthogonal matrix U which is applied to φ(x). In addition to the
constant 1 / √2 appending to our input originally, we append a constant
passing it to our neural network as this will help us argue GSH.
√k(d+1)
√(d+k)
as well to X before
D.1.1 Property (A) of GSH for Linear Manifolds
A key part of our argument for why we can get neural nets to behave as hash functions over manifold
data is the observation that at the output layer having a small variance over points from the same
manifold benefits the primary component of the loss. The following lemma formalizes the above
intuition focusing on a single manifold. Note that this result holds for non-linear manifolds too.
Lemma 36 (Centering). Let M be one of the train manifolds with associated latent vector γ. For
each X 〜D(M), replacing y by ^0 = En [y∣γ] = 1 En=I ^i Will reduce the (weighted) square loss
term corresponding to M
nm
L(Y,Y) = - XX kwι ® (yii - yii)k2
i=1 l=1
by at least Vmn(y).
Proof. We start by focusing on a single manifold with latent vector γ. We drop the conditioning on
γ to simplify the proof. Note that if there is no weighting of different coordinates of y according to
26
Under review as a conference paper at ICLR 2022
wl , then
E[ky - yk2] = E[ky - y0 + ^0 - yk2] = ky - y0k2 + E[ky0 - yk2] + 2E[(y - y0)>(y0 - y)]
nn	n	n
=ky - y0k2 + E[ky0 - yk2] + 2(y - y0)> E[y0 - y]
nn
=ky - y0k2 + Vn(y∣γ)+0.	(28)
So the value of (1/m) En [ky - yk2] reduces by at least Vn(^∣γ))/m UPon replacing y by its average
value per manifold y0. This holds even when there is weighting according to the W matrix as it only
depends on γ and doesn’t vary based on θ.
E [kwγ © (y - y)k2] = E [kwγ © (y - y0 + y0 - ^)k2]
=l∣Wγ © (y - y0)k2 + E[kwγ © (y0 - y)k2] +2E[(wγ © (y - y0))> (WY © (^0 - y))]
nn
=kwγ © (y - y0)k2 + E[∣wγ © (y0 - y)k2] + (WY © (y - y0))> (WY © E[y0 - y∣γ])
=kwγ © (y - y0)k2] + E[kwγ © (y0 - y)k2] + 0.
n
Thus even in the weighted case the weighted square loss gets reduced by at least En [∣Wγ © (y0-y)∣2 ].
But since WY is at least 1/√2(m - 1) per coordinate, this is at least Vn(y∣γ)∕(2(m -1)). Summing
up this reduction over all the m manifolds, we get the lemma.	□
The following lemma will show that it is in fact beneficial to have a zero variance at the representation
layer itself rather than just at the output layer. This also holds generally across linear and non-linear
manifolds.
Lemma 37. Vmn(y) = 0 Ifand only ifthe variance ofthe representation layer Vmn (r) = 0 where
r = Bσ(Cx). Further Vmn(y) ≥ (λ2∕λ1)(Vmn,(r))2∕4 where λ1,λ2 are the regularization weights.
Proof. Recall that for a non-square matrix W = USV>, we define the square root as W 1/2 =
U S1/2 V >. From Corollary 20, we have that at local minima of (4), if y = W Z then without loss of
generality r = W1/2z (upto orthonormal rotation and scaling). Let z0 = z - En[z] where the mean
value of z per manifold has already been subtracted. Let Z0 denote the matrix of all such z0 scaled
by 1∕√n. Since y, r are linear transforms of z, it is not hard to see that Vmn(y) = ∣∣ WZ0∣F and
similarly V‰(r) = ∣∣W1/2Z0∣∣F. Now, if kW1/2Z0∣∣f = 0, then ∣WZ0∣∣f = 0 clearly. To see the
other direction, we first observe that multiplying Z0 by a matrix W is the same as taking dot products
of the columns of Z0 with the right singular vectors of W and scaling the result by the singular values
of W. Now, the right singular vectors of W and W1/2 are the same and the singular value of W = 0
iff the corresponding singular value of W1/2 is 0 as well. Therefore, if kWZ0 kF = 0, then so is
kW1/2Z0kF.
Furthermore, the singular values of W1/2 are square roots of the singular values of W. So
kW1/2Z0kF can be non-zero only if and only if Z0 has component along a singular vector of W1/2
with non-zero singular value and the same must be true for kWZ0 kF as well. Note that since W has
m rows there are at most m singular values σ1, . . . , σm. Let c1, . . . , cm be the total norm squared
of Z0 along the right singular vectors, that is if W = UDV>, then Ci = IIVilZ0∣2. Since for any z,
IlzIl2 ≤ IICIl2α, same must be true for z0. Hence, P Ci ≤ ∣∣C∣∣2α2. Now, Vmn(y) = P σ2Ci and
Vmn(r) = σiCi. Now in the latter, the sum from those singular values that are at most Vmn(r)∕2 is
at most Vmn(r)∕2 and so the rest must be coming from singular values larger than Vmn(r)∕2. Since
the singular vectors are getting squared we have V‰(y) ≥ (Vmn(r)∕2)(Vmn(r)∕2) = (V^n(r))2∕4.
If the weights of ∣∣AIF, IBIF are λ1,λ2 then B = yJW W1/2 and the statement of the lemma fol-
lows.	□
27
Under review as a conference paper at ICLR 2022
Next we show that if the intra-manifold variance of the representation r = Bσ(Cx) is large for a
certain weight matrix B then replacing r by its mean value per manifold leads to a reduction in the
intra-manifold variance down to a small value. Moreover, this can be achieved by using a B0 such
that kB0kF ≤ kBkF. The main idea is more easily exposited by first assuming we have an infinite
width random ReLU layer. Hence, we first state the following lemma which shows that we can push
the intra-manifold variance of the representation all the way down to 0 if D → ∞.
Lemma 38. For D → ∞, given as input x0
(Y0, θ0, √d+1)
(d+k)
),
and given a B we can transform it
to B0 with no greater Frobenius norm so that Vmn (B0z) = 0.
Proof. First let Us assume that instead of the constant 'k(d+1) being appended to the input, We have
(d+k)
k 1s appended. We Will later argue that the features computed by the ReLU layer are equivalent for
both these cases. Let σ(Cx) = U2φ(γ0, θ0) + δ. When D → ∞, We have from Corollary 25 that
δ → 0. Consider the matrix B2 = BU2. Then B2 can be vieWed as a linear mapping from different
monomials in the representation computed by the random ReLU layer to a neW representation
space. Crucially, since φ(γ0, θ0) comprises of monomials in γ0, θ0, We can vieW each column of
B2 as the set of weights corresponding to a particular monomial in x. Let M(γ0, θ0) be one such
monomial. Suppose the intra-manifold variance of r is larger than 0, then B2 matrix must have non-
zero weight on nodes which correspond to monomials M(γ0, θ0 ) that depend on θ0. By Lemmas 36
and 37, replacing the terms depending on θ0 by their expected value over θ0 reduces Vmn(y) and
consequently also the square loss term. Since we have the 1k vector concatenated to X, for each
monomial M(γ0, θ0) there is a unique corresponding monomial M(γ0, 1) with the same coefficient
as M(γ0, θ0). This is because whatever combination of coordinates of γ0 and θ0 and their powers are
chosen in M(γ0, θ0) we can choose the same combination of coordinates and powers to get M(γ0, 1k)
where we have replaced θ0 with 1k. And note that since we have assumed γ0, θ0 vectors to lie within
the unit sphere, En[M(γ0, θ0)] = c0M(γ0, 1) where c0 ≤ 1. This implies that shifting the weights
of B2 from terms corresponding to the monomials which depend on θ0 to those corresponding to
monomials which have no θ0 dependence should strictly decrease the square loss. Moreover, this shift
ensures that kB2 kF is not increased. Since B = B2U2>, we have that kBkF = kB2 kF and hence
kB kF does not increase as well. A has remained unmodified throughout this process and hence we
have managed to strictly decrease the loss by decreasing Vmn(Bz) to 0 at the same time.
Now we argue that, appending k 1s to our input produces the same output as appending a single
scalar 'k(d+1) where the original dimension of input be d. Recall that the weight matrix for the
(d+k)
ReLU layer is randomly initialized with each row being drawn from N(0, I/D). Consider the
output being computed at any single node after the ReLU. In the first case, the contribution of the
k 1s to the output is Pk=1 Ci where each Ci 〜N(0,1∕(D(d + k))). This is equivalent to a single
C 〜N(0, k∕(D(d + k))). So by appending a constant of value 'k(d+1) the same effect will be
√(d+k)
achieved.	□
Next we show that the insights of Lemma 38 continue to hold approximately for a finite D as long as
it is large enough.
Lemma 39. For D ≥ O(√nm log(mn∕δ)∕ε), given as input X0
(γ0, θ0,
√k(d+1)
√(d+k)
and given a
B we can transform it to B0 with no greater Frobenius norm so that Vmn(B0z) ≤ 4ε.
Proof. Let σ(Cx) = U2φ(γ0, θ0) + δ. When D ≥ O(√nm log(mn∕δ)∕ε), we have from Corol-
lary 25 that kδk2 ≤ 2√ε. This will imply that the intra-manifold variance of r when B2 has zero
weight on nodes with a θ0 dependence is at most ∣∣2δk2∕4 = (4√ε)2∕4 = 4ε. The rest of the argu-
ment proceeds similar to before. Now at each output node of the ReLU layer, we compute a monomial
M(γ0, θ0) + δi where δi is the ith coordinate of norm-bounded noise. If Vmn(Bσ(C.)) > 4ε, then
shifting the weights of B in a similar manner as we did in Lemma 38 will yield a B0 such that
Vmn(Bσ(C.)) ≤ 4ε while maintaining ∣∣BkF = kB0kF. This ultimately leads to a decrease in the
overall objective value.	□
28
Under review as a conference paper at ICLR 2022
Lemma 39 gives the following.
Lemma 40. At any minima of the objective function (23) over points taken from the distribution of
Y, θ, Vmn(r) ≤ 4ε.
Proof. Assume that at some minima Vmn (r) > 4ε. ThenbyLemma 37,襦n(y) ≥ (λ2∕λ1)4ε2 =
4mε2∕sO(log(Vε)). Now by replacing B by B0 as described in Lemma 39, Vmn(r) is pushed to a
value smaller than 4ε which implies that the output y will be replaced by its approximate mean y0
per manifold which by Lemma 36 reduces the weighted square loss. A remains unchanged and B’s
Frobenius norm has not increased. So the value of the minimization objective overall has reduced
which is a contradiction to Lemma 33 which states that all minima are global in our setting. □
Lemma 40 implies that property (A) holds on the train examples from the train manifolds.
D.1.2 Property (B) of GSH for Linear Manifolds
Next we prove a bunch of Lemmas for showing property (B) of GSH for linear manifolds. In this
section, without loss of generality we will assume that m is even. If it is not, we drop the samples
from the mth manifold and set the new value of m to be m - 1. We will use the fact that our train
loss is small. The following Lemmas will argue that when the train loss is small, the average of the
inter-manifold representation distance over all pairs of our train manifolds is small.
Lemma 41. Let a ∈ RT such that kak2 ≤ δ. Let b1, b2 ∈ RT be such that
2 (a>b1)2 + 2 (1 - a>b2)2 ≤ ε
Then
kbι- b2k2 ≥ 1⅞4√ε.
Proof. Let (a>b1)2 = 2ε1 and let ε2 = ε - ε1. Then,
	∣a>bι∣ ≤ √2ει and ∣1 - a>b2∣ ≤ √2ε2	(29) =⇒ ∣a>(b2 - bι)∣ ≥ 1 - √2ει- √2≡2 ≥ 1 - 2√ε	(30) =⇒ IlaIl2∣∣b2 - b1k2 ≥ 1 - √2ε =⇒ ∣∣b2 — b1k2 ≥ 	(31) δ =⇒ kb2 - bιk2 ≥ 1+4ε-4√ε ≥ 1-4√--	(32) δ2	δ2
where we used that ,2(a + b) ≥ √α + √b.	□
Lemma 42. For l ∈ [m], let
	1n H (D = nm-1 E EkBσ(C Xil)- Bσ(C Xij)k2,	(33) 1n -ι = —E ∑-iji ,	(34) n(m-1) i=1 j6=l where -ji = 2 (AιBσ(CXij))2 + 2 (1 — AιBσ(CXil)2.	(35)
We have,	H(I) ≥ 1 - 4√- H(l) ≥ ^∣M^.
29
Under review as a conference paper at ICLR 2022
Proof. From Lemma 41 we have
=⇒	∣Bσ(CXil)- Bσ(CXij)II2 ≥ 1 —：声	(36) ,	XX X ∣∣Bσ(CXii) - Bσ(CXij)II2 ≥ -ʃɪ- n(m - I) -*m - … n(m - 1) i=1 j6=l	2	n(m - 1)	IAlI22 (37) =1 - 4√l	(38) IlAlll2 ,	(38)
where We have used that Pk=ι √ak ≤ √c ∙ √PL7^k.	□
Lemma 43 (Small Weighted Square Loss Implies Distant Representations). For l ∈ [m], let
	1n H(D = nm-1y ∑ ∑IBσ(CXii) - Bσ(CXij)II2	(39) j6=l i=1 	ʌ , ε = La,b (Y,Y),	(40)
Then,	1 XX τr(l∖ m m(I - θ(√ε)) m ⅛ H(I) ≥ o(同F).
Proof. First note that ε = P1=1 ει∕m and let δ = P1=1 IlAlIl2/m. From Markov,s inequality We
have that there exists S1 ⊆ [m] such that |S1| = d9m/10e and ∀l ∈ S1, εl ≤ 10ε. Similarly there
exists S2 ⊆ [m], |S2 | = d9m/10e such that ∀l ∈ S2, IAlI22 ≤ 10δ. Note that |S1 ∩ S2 | ≥ d8m/10e.
We have,
	1 XX tj-(∣∖、1 X tj-(∣∖、8 1 - 4√10ε _ m(1 - O(√ε))	Sn m EH(I) ≥ m l∈± H(I) ≥ 10 ∙	=	,	(41)
where we used Lemma 42.	□
Now since at any local minimum LAB (Y,Y) ≤ ε and IlAllF ≤ 3m, Lemma 43 gives property (B)
on the train data.
D.2 GSH on Train Data for Non-Linear Manifolds With Intra-Class Variance
Regularization
For non-linear manifolds, the argument we had in Section D.1 does not go through as is. This is
because we no longer have as nice a mapping from monomials of x to associated monomials of
similar degree in γ , θ as we had before. In particular, our argument for Lemma 39 breaks down.
Instead, we show a more general result over non-linear manifolds in this section via the means of a
different form of regularizer than before.
Recall that we add to our objective the following variance regularization term
Vreg(Bσ(C∙)) = ^TVmn(Bσ(C∙))
n- 1
The final objective we minimize is,
La,b(Y,Y)+ λι∣∣A∣∣F + λ2 (IIBlIF + Vreg(Bσ(C)))	(42)
We prove Theorem 4 via a series of Lemmas which follow. We begin by showing that for this new
minimization objective, the ground truth matrices A*,B* from Lemma 29 still give good properties.
Lemma 44 (Good Ground Truth for Non-Linear Manifolds). Given the ground truth weight matrices
A*, B* ofLemma 29, we have that
Veg(B*σ(C.)) ≤ 2ε2.
30
Under review as a conference paper at ICLR 2022
Proof. This follows immediately from Lemma 29, point 4 since Vreg(B*σ(C.))	=
ʌ ..
n-1 Vmn(B*σ(C.)).	□
Next, we show that, remarkably, even for our new objective the property that all local minima are
global still holds.
Lemma 45. Consider (42). Every local minimum is a global minimum.
Proof. We will map our minimization to an appropriate form whereafter we can apply Lemma 33 to
argue that it is equivalent to a weighted nuclear norm minimization in AB which is convex in AB .
Let Z00 = (I, Z0) where we stacked the identity’s columns at the front of Z0. Note that Z00 is full
row rank. Consider the SVD of Z00 = USV > and consider the truncated form Zt00runc = U Strunc.
Zt00runc is a square matrix which is invertible and
kBk2F + kBZ0k2F = kBZ00k2F = kBZt00runck2F.	(43)
By letting B00 = B Zt00runc, we can re-write (42) in the following form now.
m1
min X - kWι ® (Yl - AB00Mι)kF + λj∣AkF+λ2∣∣B00 kF,	⑷)
A,B00 m
,	l=1
where Ml = St-r1uncU>Zl which is a minimization of a convex function of AB00 with Frobenius
norm regularization which by the application of Lemma 33 gives us the desired mapping to a convex
function minimization with nuclear norm regularization which is a convex objective and gradient
descent on this objective will achieve the global minimum which we can argue is also what is achieved
by gradient descent on our objective.	□
Lemma 46. We get that at any local minimum A,B of (42) for λι = ει∕m, λ2 = ει∕sO(log(Vε1)),
we have
___ ʌ.、
L^,b (Y,Y) ≤ 4ει,	(45)
kAkF ≤ 4m, kBkF ≤ 4SO(IOg(II)),	(46)
Veg(B*σ(C.)) ≤ 8ε2.	(47)
Proof. Following a line of argument similar to the proof of Lemma 32 we get that at global minimum
the bounds stated in the Lemma are satisfied (by using Lemma 31). Lemma 45 gives us that all local
minima are global and hence the statement of the current lemma follows.	□
Next we show that that each part of GSH holds on the train data.
D.2.1 Property (A) of GSH
Since at any local minimum, we have that Vreg(Bσ(C.)) = n—1 Vmn(Bσ(C.)) and Vreg(Bσ(C.)) ≤
8ε2 from Lemma 44 we get that Vmn(Bσ(C.)) ≤ 8ε2 as well immediately giving property (A) of
the GSH.
D.2.2 Property (B) of GSH
Recall the argument for showing property B for linear manifolds from Section D.1. Lemmas 41-43
imply that the average inter-manifold representation distance is larger than a constant as long as
LA B (Y, Y) IS small and k A∣∣f IS small. These two properties still hold In the current setting for non-
linear manifolds. Hence we immediately get that property (B) holds on the train data for non-linear
manifolds as well.
31
Under review as a conference paper at ICLR 2022
E Generalization Bounds using Rademacher Complexity
In this section, we present population variants for bounds on empirical quantities that we saw in
Section D. Since the architectures for linear and non-linear manifolds are the same, the results in this
section will apply to both. We rely on the technique of uniform convergence bounds which are shown
via Rademacher complexity.
A recurring general function class whose Rademacher complexity we will repeatedly use is the
following
F = {f (X) = kPX + bk2 IkXk2 ≤ α, kPkF ≤ β, kbk2 ≤ b}.
We present a Rademacher complexity bound for this class.
Lemma 47. Given the above function class F, we have
Rn(F) ≤
2bβα + β 2a2
√n
Proof. First, kPX+bk22 = kP Xk22 + kbk22 + 2b>PX. Since supθ(fθ(X) +gθ(X)) ≤ supθ fθ(X) +
supθ gθ (X) we have
Rn(F) = 1 E
n
sup	ξif(Xi)
f∈F i=1
(48)
n
sup	XξikPXik22
p：kPkF ≤βi=ι
{^^^^^^^^^^^^^^^™
(A)
n
sup Xξikbk22
b:kbk2 ≤b i=1
(B)
n
sup	^X ξi 2b>P Xi
P,b:kP kF ≤β, i=1
kbk2 ≤b	i=1
___________ - /
{z
(C)
(49)
We bound each term separately. (B) is clearly 0. (C) is the Rademacher complexity of a class of
linear functions of X which is known to be bounded by kb>Pk2α/√n (Lemma 26.10 of (Shalev-
Shwartz & Ben-David, 2014)) which can be bounded by 2bβα/√n. It remains to bound (A). Here
we use that,
kPXk22=P>P XX> .
(50)
≤ 1E
+ 1 E
+ 1E
32
Under review as a conference paper at ICLR 2022
Moreover, we have ∣∣P>Pk* = IlPkF. Now,
(A)
1E
nξ
n
kPskuFp≤βXi=1ξikPxik22
1E
nξ
1E
nξ
1E
nξ
β2
—E
nξ
β2
—E
nξ
β2
n t
sup
kPkF≤β
i=1
n
sup y"ξi (W Θ XiXi>
kw k*≤β2 M
sup W X ξiXiXi>
kwk*≤β2 ∖	M
n
X ξixixi>
i=1
n
X ξixixi>
i=1
E
ξ
≤ β2
n
xi>
Xi>k2F
n
i=1
!#
(51)
(52)
(53)
(54)
(55)
(56)
≤
≤
2
F
2
F
β2
n t
n
X kXi k42 ≤
i=1
where (51) uses (50) and (52) is obtained by replacing P>P with a matrix W and using that
∣∣PkF ≤ β =⇒ ∣∣P>Pk* = ∣∣PkF ≤ β2. (53) UsesClaim 18, (54) follows beCaUseforanymatriX
A, kAk2 ≤ kAkF, and (55) follows from Jensen’s ineqUality. Finally (56) follows by eXpanding the
FrobeniUs norm and noting that the ξi are independent and E[ξiξj] = 0 for i 6= j. Combining the
bounds for (A), (B) and (C) we get the statement of the lemma.	□
E.1 Small Weighted S quare Loss on Test Samples from Train Manifolds
The first generalization boUnd is to show that on the m train manifolds, oUr learnt network aChieves
a small test error as measUred by the weighted sqUare loss. That is, the network has aCtUally learnt
to Classify inpUts from the m train manifolds. A simple Uniform ConvergenCe argUment CoUpled
with Lemma 5 gives Us that the eXpeCted weighted sqUare loss over Unseen samples from oUr train
manifolds is small as well for large enoUgh n.
Lemma 48. At a local minimum A, B we have, with probability ≥ 1 - δ,
1m
-X Efvr JkWl θ (yi - yι)k2] ≤ 2ε,
m ∣=1 χι〜D(Ml)
r	(SOclog(1/" iog(m∕δ)∖
for n ≥ Θ I------ε2一g——..
Proof. Let β = SO(Iog(I/G be the bound we have on ∣∣B∣∣f. FiX a train manifold with index
l. Let Ai denote the lth row of A and IetkAIk2 ≤ aι. We know that Pll a ≤ O(m). Let
En[∣∣wl Θ (yl 一 yl)∣∣2 ≤ ει. From Lemma 26.5 of (Shalev-Shwartz & Ben-David, 2014) we have
33
Under review as a conference paper at ICLR 2022
that with probability ≥ 1 一 δ∕m,
xj^EmJkWl© (yι  ^ι)k2] ≤ E [kwι© (yι — yι)k2] +2E[Rn(F)] + CJ2log*δ), (57)
In our case, C = O(a2β2∣∣C∣∣2) as ||wl©(yl-^l)k2 ≤ O(a2β2∣∣C∣∣2) and Rn (Fl) is the Rademacher
complexity of the function class
Fl =	nf(xl) = kWl © (yl 一 ABσ(Cxl))k22 kAk2F	≤ O(m), kBkF	≤	βo
Now ∣∣wl © (yl	一 ABσ(C xl))∣2 is of the form ∣∣P Z + b∣2 for P	= wl © AB, b	=	-wl ©	yl and
z = σ(Cxl). Since, kP k2F ≤ O(al2β2), kbk2 = 1/2 and kσ(Cxl)k2 ≤ kCk2, from Lemma 47 we
have that
Rn(Fl) ≤ O(巡平).
Therefore, we have that, with probability ≥ 1 一 δ∕m,
X〜E(MJkWl © (yl 一 yl)∣2] ≤ E [lwl © (yl 一 yl)∣2] + O (alβ√Ck2) + O(a2β2∣C∣2)J2log,/U.
(58)
Averaging over all m train manifolds and taking a union bound, we have with probability ≥ 1 一 δ,
ɪXXJu)[kwl©(yl-yl)∣2]≤ε+O(%)+O(β2kCk2)**2. (59)
Note that we have used that Plm=1 al2 = O(m) here. (59) will imply the statement of the lemma for
n = θ (β4∣Ck4 log(m∕δ)) = θ (SO(Iog(I4D log(m∕δ))
with probability ≥ 1 一 δ∕2. Here we used that for the D we have chosen ∣∣C∣∣2 is bounded by a
constant with very high probability (Lemma 12).	□
E.2 Generalization for Property (A)
We first show that the variance regularization term Vreg is an unbiased estimator for the intra-manifold
variance of our representation.
Lemma 49 (Unbiased Variance Estimation). Consider the variance regularization term (5). For the
set of train manifolds M1 , . . . , Mm, we have,
m
E[(5)] = - X VMi (Bσ(C.)).
ml
l=1
Proof. Let us focus on a single manifold l Let E[∣Bzl ∣∣2] = μl and let ∣B E[zl]∣∣2 = Kl.
1n
EE ]X kBzilk2
1n
ɪ X
n - 1 J
i=1
(⑷	.(B)	. .	(C)	∖
Z 、人 _、 z ,|	{ z	}\	{
E[kBzilk2] + E [kBE[zl]k2] — 2E [z>B>BE[zl]].
∖ " /
(60)
34
Under review as a conference paper at ICLR 2022
Now (A) = μι. We obtain the expressions for (B) and (C).
(B)=n E x BZjlI
1n	1
=n XE[kBZjlk2] + n X E [z>lBτBZj2l]
j=1	j1 6=j2
=μ + n-1 E[zl]TBTB E[zl]
nn
=μ+n-1 kB E[zl]k2 = μ + 包』.
n n	2n n
Finally,
2n
(C) = n E ∑z>BTBZjl
=2 E[z> BTBZil] + 2 X E[z>BTBZjl]
nn
j,j6=i
2μl	2(n - 1)κl
=-----1------.
nn
Adding all together we get,
1n
F-IyE xkBzilk2
μl - κl.
It is easy to see via a similar calculation that,
VMi(Bσ(CJ) = μl - Kl
mn	m
=⇒ 7-L E XX kBziik2 = — XVMi(Bσ(C.)).
(n- 1)m	m l
(61)
(62)
(63)
(64)
(65)
(66)
(67)
(68)
(69)
(70)
□
We next show that having a small variance regularization term over the train data implies that
Property (A) of GSH holds with high probability.
Lemma 50 (Generalization of Property (A) to Unseen Points from Train Manifolds). Recall that at
local minimum, we have found a B so that for r(x) = Bσ(Cx),
mn
m X n-1 X kr(xii)- E[r(xι)]k2=0.
l=1	i=1
Then we have,
m1
P X —VMl (r(.)) ≤ 2ε ≥ 1 - δ,	(71)
mi
l=1
where the probability is taken over the sampling of the n input examples from each of the m train
manifolds.
Proof. From Lemma 49 we have that
1n
E —7 Ekr(Xil)- E[r(xι)]k2 = VMl (r(.)).
n-1	n	i
i=1
35
Under review as a conference paper at ICLR 2022
Given a vector h ∈ RD, consider the family of functions defined as follows.
Fh = {fB,h : Rd → R I fB,h(x) = kB (σ(CX)- h) k2, kxk2 ≤ α, kBkF ≤ β,3卜 ≤ ∣Qk2α}.
(72)
By Theorem 26.5 from (Shalev-Shwartz & Ben-David, 2014), we have that for each manifold Ml,
for every fB,h ∈ Fh,
E [fB,h(x)] ≤ E [fB,h(x)]+ 2Rn(Fh) + c∖∕2lθg(m∕δ),	(73)
Ml	Ml	n
with probability 1 - δ∕m. Here ∣fB,h| ≤ c. For the function family We have considered taking
C = 4β2∣∣C∣∣2α2 suffices. Note that for h(l) = EM1[σ(Cx)], EMlhfB h(i)(x)] = VMl (Bσ(C.))]
and Pm=ι EMl fB,h(i)(x)] = m(5) ≤ mε. An upper bound on Rn(Fh(l)) will lead us to an
upper bound on Pm=I VMl (Bσ(C.))] as we shall see later. We proceed to bound Rn(Fh(i)). For
X 〜 D(Ml), let σ(Cx) — h(l) = z. From Lemma 47, we have
3β2 kCk22α2
Rn(Fh(2 ≤ ''.
Therefore, we get that
m
X VMl (Bσ(C.)) ≤
l=1
X MJfB,h(l)(x)] +6mβ2kc k2α2 ∖^1 + 4mβ2kc k2α2 Jlog (m∕δ)
≤ mε + 6mβ2∣∣C∣∣2α2∖/ɪ + 4mβ2∣∣Ck2α2∖^ogzmZil
2n	2	n
≤ mε + 10mβ2∣∣C∣∣2a?/01三.	(74)
2n
with probability 1 - δ. Note that we applied a union bound over the statement for each manifold
bounding the probability of large deviation in any one of the manifolds by m ∙ δ∕m = δ. By choosing
n ≥ Θ (β α kCkε2log(m∕δ)) in (74), we get that with probability 1 - δ,
m
1
—ɪ2 VMl(Bσ(CJ) ≤ 2ε∙
ml
l=1
Since ∣xl∣2 ≤ 1, we can choose α = 1. Moreover from Lemma 12 we can assume ∣C∣2 is
bounded by a constant with very high probability. In addition we have that at local minima ∣B∣∣f ≤
SO(Iog(I/E)). Hence, we get the statement of the lemma for
SOelog(I/E)) iog(m∕δ)
n≥Θ
ε2
□
Lemma 51 (Generalization of the Property (A) to new
Manifolds). Given A, B which are at a local
minima of either objective (23) or (42) ,for afresh sample Mm+ι 〜M, we have
E IIBz0∣∣2	≤ Ο(ε),
X〜D(Mm+ι) Ul	Il2_|
(75)
with probability ≥ 9∕10 over the draw of Mm+1 from M when
SO(log(1/E)) log(2∕δ)
m≥ Θ
ε2
Here z0 = σ(Cx) - Ex〜d(mw,+i) [σ(Cx)].
36
Under review as a conference paper at ICLR 2022
Proof. Consider the following function class which maps a manifold to a non-negative real value:
F= fB :M →R,fB(M)=	E	kBzOk22	| kBkF ≤ β, kxk2 ≤α .
[	X〜D(M) L	」	J
By the property of Rademacher complexity (Theorem 26.2 from (Shalev-Shwartz & Ben-David,
2014)), we have with probability ≥ 1 - δ, for any f ∈ F,
MmMM f (Mm+1)] ≤ m[f (M)] +2 ERm(F)] + C严>
(76)
with probability 1 - δ. Here c is such that |f(M)| ≤ c for all M. Choosing c = 4β2 kCk22α2 suffices.
We need to bound Rm(F). We could appeal to Lemma 47 again but since now the function class F
has functions of manifolds instead of vectors x we present the full argument here for clarity. The
argument follows in a very similar vein to that of Lemma 47. Now we let ξ = (ξ1, . . . , ξm) denote a
vector of m i.i.d. Rademacher random variables.
Therefore,
E
Mm + 1 〜M
for
m≥
sup
fB ∈F
m
X ξlfB(Ml)
ɪ E
mξ
ɪ E
mξ
ɪ E
mξ
β2
—E
mξ
sup
fB ∈F
l=1
m
Xξl
l=1
m
E	kBz0k22
X 〜D(Ml)
sup	ξl	E
kBkF ≤βl=1 x~D(Ml)
sup
kBkF ≤β
m
B>B, X ξl
sup
kWk*≤β2 ∖
l=1
m
W, Xξl
l=1
hDB>B,z0z0>Ei
XJUzOz国
XJUzOz0>H
m
X ξl	E	zOzO>
l=1 X〜D(Ml) L
≤ β2
m
E X ξl E hzOzO>i
ξ Ill=1 X〜D(Ml) L J
x"(Mm +IJkB Z0k2]
≤ ε + 8β2α2kCk2
一	√m
≤ε+
≤ 2ε,
144β4α4ICI24 log(2∕δ)
ε2
4β 2α2 IIC k2 √log(2∕δ)
12β2α2kC k2 VZlog(2∕δ)
√m
√m
,SO(log⑴E)) log(2∕δ)
θ ----------ε2--------
Rm⑺=m E
≤ m E
2
2
F
+
37
Under review as a conference paper at ICLR 2022
Finally, by Markov’s inequality, we have that with probability ≥ 9/10,
E ∣^∣∣Bz0k2^∣ ≤ 20ε.
X〜D(Mm+ι) L" 112J ^
□
E.3 Generalization for Property (B)
We first show that a population variant of Lemma 43 over n holds with high probability.
Lemma 52. With probability ≥ 1 - δ,
m
「-------? XX E	kBσ(CXl) - Bσ(Cxj)k2
m(m - 1) l=1 j=l X-D(Mi)J	(	(
≥ m(O-O√ε)) - f(iog(m)∕δ)∕√n
O(kAkF )
Proof. Recall the definition of H(l) from Lemma 43. Consider the sum S = ^^ Pm=I H(l). From
this summation, consider the n terms corresponding to a pair of manifolds l 6= j . Denote the sum
over these n terms by Sl,j . We can argue generalization for Sl,j using uniform convergence theory.
In particular, we have that with probability 1 - δ∕(m(m - 1)),
Sl j ——_-	E	∣∣BBσ(Cxl) - Bσ(Cxj)k2
lj	m(m — 1) xι〜D(Mi), u ' l,	'	J川2
Xj 〜D(Mj)
V	2 E[Rn (F)] , 4β 2 2∣ ICI ∣2，log(2m(m - 1"δ)
≤	2m(m- l)+4βα kCk2	√nm(m - 1),
(77)
(78)
where
F={fB :fB(xl,xj) = kBσ(Cxl) - Bσ(Cxj)k22, kB kF ≤ β}
We repeat the above for all pairs l 6= j . The probability that for all pairs the expected loss will be
close to the train loss is at least 1 - δ by the union bound. From (78) we have that with probability
≥ 1 - δ,
1
m
d
S-
m(m - 1)
ΣΣ
l=1 j 6=l
E
Xl 〜D(Mi),
Xj〜D(Mj)
,, , .
∣B σ(C Xl)
-B σ(C χj)k2
(79)
≤	2E[Rn(F)] +4β2α2kCk22
ʌ/log(m(m - 1)∕δ)
m(m - 1)
ΣΣ
l=1 j6=l
E ∣∣Bσ(CXl) - Bσ(CXj)k2
xi~D(Mi),"	J
Xj 〜D(Mj)
(80)
m(1 — O(√ε))
O(MkF)
- 2Rn(F) - 4β2α2kCk22
log(m(m - 1)δ)
≥
1
m
Now we bound Rn(F). Using Lemma 47, we get that
Rn(F) ≤
4β2α2kCk22
(81)
38
Under review as a conference paper at ICLR 2022
Note that to employ Lemma 47 we think of σ(Cxil) - σ(Cxij) = x as the input to the functions in
F . Therefore,
1
m(m - 1)
m
XX XlJ(Ml) kB °(C Xl)- B °(C Xj)k2 ≥
l	l,
l = 1 j = l Xj〜D(Mj)
m(1 - O(√ε))	8β2α2kCk2
--------Z----	
o(kAkF)-----√n
(82)
for
n≥Θ
β4α4kCk42 log(mδ)kAk4F
m2
- 4β2α2kCk22
log(m(m - 1)δ)
〉m(1 - O(√ε^
≥	O(kAkF)
Θ 卜O(Iog(I4D log(mδ)).
(83)
(84)
□
Next we show that for a randomly chosen permutation, with high probability, we have that the
inter-manifold representation distance averaged over consecutive pairs according to the permutation
is also large.
Lemma 53. Suppose m is even and m ≥ Θ(log(2∕δ)sO(Iog(I^"/K2). Let d(l,j)=
ExlS(Ml)JlBσ(CXl) - Bσ(CXj)k2. Suppose we have that m(m1-1) Pm=I Pj= d(l,j) ≥ K.
Xj~D(Mj)
Then for a randomly chosen permutation ρ : [m] → [m], we have that with probability ≥ 1 - δ,
2 m/2
—Ed(P(21- 1),ρ(2l)) ≥ K/2.
m
l=1
Proof. First we have that
2 m/2	1 m
E — X d(ρ(2l- 1),ρ(2l)) =—~- XX d(l,j) ≥ K.	(85)
ρ m	m(m - 1)
This is because in expectation over random permutations, we see every pair l, j the same number of
times. The normalization ensures that the overall sums match. Next, we show that concentration of the
sum on the left hand side around its expected value. Using a trick from (Talagrand, 1995), we view the
process of choosing a random permutation on [m] as follows. We start with the identity permutation.
Then we perform a sequence of m - 1 transpositions as follows. We transpose (m, am), then
(m - 1, am-1) and so on till (2, a2) where each aj is uniformly samples from [j]. This will give us a
uniformly random permutation at the end and it is defined by {al}lm=2 which are independent. From
here, our strategy will be to bound the amount by which our sum Sm = m PmI d(ρ(2l - 1),ρ(2l))
changes when the value of some aj is changed. Changing aj changes at most 3 locations in the final
permutation (wherever j , the old aj , the new aj end up). Therefore, at most 3 terms in Sm change.
Noting that kB°(CXl) - B°(CXj)k2 ≤ 2αβkCk2 we can deduce that by changing a single aj, Sm
changes by at most 6αβmCk2. Now applying McDiarmid,s inequality gives Us
2 m/	Γ 2 m/2	1 I 1	t Pm
P b X d(ρ(2l - 1),ρ(2l)) -E Jm∙ X d(ρ(2l - 1),ρ(2l" I ≥ tJ ≤ 2eχp (-360WCI
(86)
Taking t = Jlogm/δ) 6αβ∣Ck/, we get that with probability 1 - δ,
2 m/2
—E d(ρ(2l - 1), ρ(2l)) ≥ K - t ≥ K/2,	(87)
m
l=1
for m = 144log(2∕δ)α/β2∣∣C∣∣2∕K2 = θ(log(2∕δ)so(log⑴ε)”K2).	□
39
Under review as a conference paper at ICLR 2022
Finally we show property (B) of GSH for most new manifolds sampled from M.
Lemma 54. For Mm+ι, Mm+2 〜M2, with probability ≥ 1 一 δ,
Exm+ ι~D(Mm + ι), kBσ(Cxm+1)-Bσ(CXm+2)k2 ≥ K/4,
xm + 2~D(Mm+ 2)
for
m ≥ θ ( SO(Iog⑴ε)) log(2∕δ)
Proof. Consider the following process. We sample m/2 pairs from M2, {Ml = (Ml1, Ml2)}lm=/12.
Define d(Mι) = Ex]〜D(MlI), ∣∣Bσ(Cxι) — Bσ(CX2)k2. It is easy to see that our original sampling
x2 〜D(Ml2)
process of getting M1 , . . . , Mm and choosing a random permutation to order these m manifolds in
and pair consecutive ones is identical in distribution to the above described process. Hence, any
probability statements for the former process hold also for the latter and vice versa. Let
F = {dB : M → R | dB(M)=	E	∣∣Bσ(CXI)-Bσ(CX2)k2, where M ∈ M2, ∣∣B∣∣f ≤ β}
Xi 〜D(M1),
x2 〜D(M2)
We have that with probability 1 - δ,
m/2
sup XdB(Ml) -
dB∈F l=1
M ^2^ (M)]≤ 2 ERm/2(F)"4β 2α2 kCk2 √2j0g≡
=⇒ M 舁dB (M)I ≥ K/2 - 2 E[Rm/2(F)] - 4β2α2kCk2 Ama.
Now it remains to bound Rm/2(F). Given M ∈ SuPP(M) and xι, x2 〜D(Mι), D(M2) respec-
tively, let Z = σ(Cxι) - σ(Cx2). Then we have Ex1,x2〜d(Mi),d(M2)[∣∣z∣∣2] ≤ 4α2∣∣Ck2.
K(F) = m2 ξ,{Ml = (ME,Ml2)}m∕12 B武 ≤β X & ^即"“2 4 4^^
l = 1	x2 〜D(Ml2)
using a line of calculations similar to those done in the proof of Lemma 51. Therefore, we have
mE 2[dB (M)] ≥ K/2 - 8√2β2jkck2 -4β2α2kCk2 Cbg ⑵/δ
M 〜M2	ʌ/m	V m
12√2β2 α2kC k2PlOg(2∕δ)
≥ K/2------------√m-----------≥ K/4
for
4608β2α2 kCk2 log(2∕δ)	C /SO(Iog(14)) log(2∕δ)∖
m ≥----------K---------= θ(----------K--------/
□
F Intra-Class Hashing Property Without Variance
Regularization
Theorem 8 (Property (A) without Variance Regularization). Given our 3-layer neural network, and
given n train samples each from m train manifolds, training the following objective results in a
network that satisfies that Vmn(Bσ(Cx)) → 0 as λι, λ2 → 0
min La,b (Y,Y)+ λι∣∣A∣∣F + λ2∣∣B∣∣F.	(88)
40
Under review as a conference paper at ICLR 2022
Proof. The main point to note is if λ1 , λ2 → 0 (that is very small) then the objective is dominated
by La,b (Y, Y) which is minimized only if the prediction Y does not depend on θ - because if it
did then by replacing Y by En [Y] for each of the m manifolds, decreases the objective as shown in
Lemma 36.
Now, we know that there IS a ground truth model where Vmn (y) ≤ ε. Then It follows from Lemma 37
that Vmn (r) ≤ 2√ε. To get close to this ground truth we select λι = ε∕m and 入2 = £/sO(log(1/G).
Hence by letting ε → 0 we get the desired result.
□
G RECOVERING γ FROM THE REPRESENTATION
We have argued in Section 1 that in many cases where γ represents a set of semantic concepts such
as the shape or texture of an image, it is of interest to recover exactly the latent vector γ and not
just an isomorphism f(γ). The next lemma shows that there is a linear transform that maps our
learnt representation r(x) to approximately γ associated with x; however we can only show this
with the unweighted square loss when the regularization weights λ1, λ2 is tiny and only for the train
manifolds. Our experiments show that this reversibility holds even with our variant of the weighted
square loss LA,B (Y, Y).
Lemma 55 (Reversibility of the Learnt Representation). Consider the minimization
minA,B E kY - ABσ(Cx))k2F + λ1(kAk2F) + λ2(kBk2F) subject to vθ(r) = 0. As λ1, λ2 → 0
and for infinite width C layer, there is a linear transform R so that RBσ(Cxl) = γl for any xl from
any of the m training manifolds.
Proof. We will show that ifγ is not expressible as a linear transform of r(x) then creating additional
outputs of the B layer that emit γ only improves the loss objective. First note that the width of the
hidden layer never needs to be more than m (as otherwise we can replace A, B by their appropriate
truncated-svd versions that are of at most width m since the rank of AB is at most m). So even if
we add additional co-ordinates to r(x) the width remains bounded. We have also assume that the
variance at the representation layer is 0 for each manifold so the representation layer is a function
only of the manifold for the train data.
Note that we have assumed λ = 0 and the width of the random ReLU layer goes to ∞. In this
scenario, we know by Lemma 26 that for every manifold Ml ∈ M, the representation computed by
σ(Cxι) when xι 〜D(Ml) is powerful enough to express Yl exactly. We will show that if Yl cannot
be expressed as a linear combination of coordinates of r = Bσ(Cxl) over the training samples
Xl 〜D(Ml) then the loss Exl〜m1 [∣∣Y - ArkF] is not at a minimum and can be further reduced. Let
Al denote the lth row of A. Let Al be the regression minimization for the term min Al k Yl — Alrk 2
which will be the optimal trained value of Al. Let us find the improvement to this term by appending
Yj the the j th coordinate of Y which is the vector of γlj over the different manifolds l Let Yj
denote the vector of γlj over the different manifolds l. Note that in any linear regression problem
the improvement obtained from a new coordinate of the input features can be quantified as follows:
orthonormalize it with respect to the other coordinates and measure the square of the projection of
the output vector along this new orthonormalized input coordinate. So when a new coordinate Yj has
been added to the input, the decrease in the square loss is
( Yl,Yj0/|Yj0|2 )2
(Yl>Yj0)2/|Yj0 |22
where Yj0 is the component ofYj that is orthogonal to r; that is, Yj0 = Yj - d>r so that hYj0, ri = 0.
Note that since Yj have been normalized, the improvement is at least (Yl>Yj0)2 = Plm=1(Ylj0)2 (as
Yl is the indicator of the lth coordinate). So the total improvement over all the manifolds from Yj
is at least Pl(Ylj )2 = kYj0 k22. Now since the loss is at a local minimum it must be that there is no
improvement possible which means kYj0 k2 = 0 which means Yj = a>h for some a and the same
argument must be true for each coordinate of Y and so Y = Rh for some R.
□
Remark 2. Although we assumed λ → 0 and width of ReLU layer tends to ∞, note that if the width
of C is bounded and large, then Y can only be approximately expressed in terms of σ(CX). In that
41
Under review as a conference paper at ICLR 2022
case that approximate version of γ must be linearly expressible in terms of h.
Also even if λ is not 0, note that as long as j kγj0 k22 > λ the increase in regularization loss is more
than offset by the decrease in square loss - to realize the improvement by PjIlYj0 k2 the A matrix
will add an edge of weight hYi, γj0i between each γj and Yi and the B matrix needs to add new
nodes corresponding to b which has a bounded norm in terms of σ(C x) which bounds the increase in
Frobenius norm of B.
H Convergence of First or Second Order Methods to Local
Optima
Here we point the reader to two results about some popular first-order optimization methods which
have the property that they converge quickly to a local optimum for smooth optimization objectives.
The first is the work of (Ge et al., 2015) which shows that for strictly-saddle objectives, a form
of stochastic gradient descent provably converges to a local optimum. The second is the work of
(Agarwal et al., 2017) who show that a second-order algorithm FastCubic converges to local optimum
faster than gradient descent converges to any critical point for a set of smooth objectives which
includes neural net training. There are more references within the above works studying similar
properties of other variants as well.
I	Additional Experimental Details
In this section, we support our theoretical results with an empirical study of the GSH property
of DNNs on real and synthetic data. First, we detail our experimental setup and then discuss the
experimental results.
I.1	Experimental Setup
We separate our experiments to two groups, based on the data generating process.
Natural Images. We train Myrtle-CNN (Page, 2018)—a five layer convolutional neural network—
on MNIST (Deng, 2012) and CIFAR-10 (Krizhevsky et al., 2009) with `2 regularization without
regularizing the bias terms. For CIFAR-10 the width parameter is c = 128 while for MNIST it is
c = 32 and we remove the last two pooling layers. For both cases, we train via the SGD optimizer for
50 epochs with learning rate of 0.1 then drop the learning rate to 0.01 for another 100 epochs with
batch size of 128. We use λ = 0.1 The resulting test accuracies are 99.4% for MNIST and 88.9% for
CIFAR-10 while they also perfectly fit the train.
Synthetic Data. For the synthetic data We do the following data generating process, as to satisfy
Assumption 1: first we randomly sample Y and θ from the standard and 1 / √k scaled Gaussians on
Rk respectively fork = 11. Then, we sample two random matrices V, W from a scaled Gaussian
N(0, dI)on Rd×k and use an analytical function P such as the sin(∙) to generate X = P(WY + Uθ).
The analytic functions we tried are ex/2, sin(x), cos(x) and log((1 + x2)/2). Note that the last two
are even functions, so precise recovery of Y is impossible as f(x) = f (-x). To increase the
complexity of the manifold we sum 4 functions of each type, so for example, the final sine data
generating function is Pi=14 sin(ViY + Wiθ). We also call a sum of all 4 functions the Mixture
distribution. Now, in order to generate examples from the same manifold, we repeat the above process
with V, W, Y fixed and vary (generate) θ. We then train a three layer Multi-Layer-Perceptron (MLP)
with width 1θ00 for 200 epochs via SGD with learning rate 0.1, batch size of 32 with λ = 0.01-'2
regularization parameter and `2 loss for classification. The train/test accuracies after this procedure
are 100%.
Meta learning and Y recovery. The main advantage of the synthetic data is that we are able to
generate as many manifold (and samples) as we want. Therefore, we can check what is the ρ not only
on manifolds we saw before, but also the behavior on the distribution. Moreover, we can generate
enough manifolds to hope to fit a linear classifier on top of the representation. If the linear loss is
42
Under review as a conference paper at ICLR 2022
small the representation is approximately linearly isomorphic to γ . This means that our representation
successfully recovered the manifold geometry.
In a similar fashion described in the Synthetic Data data generation process, we generate 4 datasets:
train, test, few-shot and few-shot-test. In the train (and test) there are 50 classes (manifolds) with 8k
train examples and 2k test examples per class. As for the few-shot (and few-shot-test) we generate
10000 manifolds so their representations will serve as train set for the linear classifier that will try to
recover γ. In order to estimate the ρ on the distribution of unseen manifold, we sample 5 samples
from each manifold to a total of 50000 few-shot (train) samples. We then use SGD with learning rate
of 0.1 and batch size of 32 fit a linear model with loss kr(x) - γk22. Finally, we sample another 100
manifolds with one sample per manifold to estimate how well the linear function recovered γ . In
order to measure how close the representation is to an isomorphism, we use normalized distance as a
metric (see table 1), normalizing kW r(x) - γk22, where W is the learnt linear model, by the average
distance between two γs: kγi - γj k22. So a random Gaussian will produce normalized distance of 1
while perfect linear recovery will be normalized distance of 0.
I.2	Experimental Results
Our results for real data are in shown in Figure 4 and our results for synthetic data are summarized
in Table 1. We observe high ρ values for both real (1.46 for CIFAR-10 and 3.36 for MNIST) and
synthetic distributions (ρ ≥ 10.7) both on train and on test. Furthermore, for synthetic data, we see
that even out-of-distribution ρ is high. This implies that the classifier learnt is a GSH function on
the population of manifolds, effectively inverting the data generating process. Further, we see that
when the function is not even (and thus γ is not recoverable) as is the case for the Sine, Log, Exp
and Mixture distributions, we are able to recover γ from the representation using a linear function.
Specifically the normalized γ recovery distance is at ≈ 0.1 where a random γ would yield a distance
of 1.
Table 1: Our results on synthetic data. We provide the ρ value for 5 different synthetic distributions,
on train, test and transfer (i.e., unseen Ys). We also note the normalized distance of ∣∣γ - Y∣∣ where Y
is a linear classifier on top of the representation layer attempting to recover γ. We are able to nicely
recover the Mixture, Sine and Exp distributions, while recovering the cos(x) and log((1 + x2)/2)
proves more difficult. The reason is that we fail to recover Y for these functions is that they are even,
so there is an ambiguity of whether the sign is positive or negative.
Data Generating	I Test	I Tr*n	I	C TMf	I	C TranafQr I NOrmaIiZed Y
Function	∣	ACCUraCy	∣	P-Tram	∣	P-TeStl	P-TranSferl Recovery Distance
Mixture	100%	19.13	19.09	20.46	0.11
Sine	100%	26.8	26.8	28.4	0.09
Cosine	100%	10.7	10.79	11.09	0.71
Log	100%	12.38	12.38	11.23	0.71
Exp	100%	25.2	25.2	28.77	0.08
43
Under review as a conference paper at ICLR 2022
Intra Distance
Figure 5: A comparison of intra vs inter class distances for Unseen
Manifold for an MLP trained on Mixed synthetic data. Remarkably,
even on unseen manifolds the GSH property holds, that is the represen-
tation is invariant to the “noisy feature" θ while being sensitive to the
semantically meaningful feature γ .
1
2	3
Distance
4
Inter Distance
5
Figure 6: We show the ability to recover γ with linear regression
over the representation with varying the norm of θ/γ. We see that
.6,54.32 」
60.0.0.0.0.
①ou5sq
A」①>0。①U P①Z=QUJ-IoN
when the norm of γ is dominating, the learnt representation is almost
a linear function of γ . In contrast, when θ has larger norm, the learnt
representation becomes a non-linear function of γ . (We know that γ
and the representation are isomorphic as long as ρ is large enough.).
44