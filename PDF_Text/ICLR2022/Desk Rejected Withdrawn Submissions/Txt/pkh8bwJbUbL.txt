Under review as a conference paper at ICLR 2022
Fair Representation Learning through im-
PLICIT PATH ALIGNMENT
Anonymous authors
Paper under double-blind review
Ab stract
We considered a fair representation learning perspective, where optimal predic-
tors, on top of the data representation, are ensured to be invariant with respect
to different subgroups. Specifically, we formulated the problem as a bi-level op-
timization, where the representation is learned in the outer-level, and invariant
optimal group predictors are updated in the inner-level. To avoid the high com-
putational and memory cost of differentiating in the inner-level optimization, we
proposed the implicit path alignment algorithm, which only relies on the solution
of inner optimization and the implicit differentiation rather than the exact opti-
mization path. Moreover, the proposed bi-level objective is demonstrated to fulfill
the sufficiency rule, which is desirable in various practical scenarios but was not
commonly studied in fair representation learning. We further analyzed the error
gap of the implicit approach and empirically validated the proposed method in
both classification and regression settings. Experimental results show the consis-
tently better trade-off in prediction performance and fairness measurement.
1 Introduction
Machine learning has been widely adopted in the real world decision-making practice such as job
candidate screening (Raghavan et al., 2020) and credit application. However, it has been observed
that learning algorithms treated some groups of population unfavorably, for example, denying credit
on the grounds of gender, age or ethnicity (Hardt et al., 2016). To this end, algorithmic fairness that
is to mitigate the prediction bias for different subgroups has recently received tremendous attentions.
With the rapid advancement of representation learning (LeCun et al., 2015), learning a fair embed-
ding (Zemel et al., 2013) has been recently highlighted. Specifically, the learned fair representation
can easily transfer the unbiased prior knowledge to the downstream tasks, with various successful
applications in computer vision (Kim et al., 2019; Kehrenberg et al., 2020), language understanding
(Chang et al., 2019; Ethayarajh, 2020) and artificial intelligence for health (Fletcher et al., 2021).
Typically, the fair representation learning is achieved by adding various statistical fair metrics during
the training process.
Based on this, most existing fair representation
approaches in classification or regression princi-
pally aim to meet the independence or separation
rule, e.g., (Madras et al., 2018; Song et al., 2019;
Chzhen et al., 2020). However, in various real-
world scenarios, the sufficiency rule is preferable.
For example, health systems rely on commercial
algorithms to identify and help patients with com-
plex health needs. The algorithm outputs a health-
care need score, where a higher score indicates
the patient is sicker and requires more healthcare.
Obermeyer et al. (2019) revealed that a widely
used algorithm, typical of this industry-wide approach and affecting millions of patients, exhibits
significant racial bias. At a given predicted healthcare need score Y = t, Black patients are consid-
erably sicker than White patients (Eblack[Y |Y = t] > Ewhite[Y |Y = t]). Obermeyer et al. (2019)
also pointed out that remedying the disparity would increase the percentage of Black patients receiv-
-VhL0(h, λ)	4 . h?
~~-------'"U
-VhL0(h,λ)
Figure 1: Unfair representation leads to differ-
ent optimization path and non-invariant opti-
mal predictors on the latent space Z .
1
Under review as a conference paper at ICLR 2022
• h1?
h(0)-PtyhLI 竺,,λ) 一/ h?
-Pt VhL0(h(t),可
(b) Implicit Path
h1?
h0?
(a) Explicit Path
Figure 2: Explicit and Implicit path alignment. (a) The considered fair representation learning
criteria lies in ensuring the invariant optimal predictor w.r.t. different subgroups on Z (h?0 = h1?).
Since the gradient based approach is adopted to optimize h, the explicit path alignment aims to learn
a representation λ to enforce the identical optimization path w.r.t. h. (b) The proposed implicit path
alignment only requires the last iteration point and approximate the gradient w.r.t. λ from the last
update of h (the brown arrow).
ing additional healthcare from 17.7 to 46.5%. Moreover, it has been theoretically justified (Barocas
et al., 2019) that the Sufficiency rule is generally not compatible with Independence and Separation.
Thus learning the fair representation w.r.t. the sufficiency rule is promising in both the algorithmic
design and real-world applications.
In this paper, we address the sufficiency rule by considering the following intuition: given a fixed
representation function, if the optimal predictor that learned on the embedding space are invariant
from different sub-groups, then the corresponding representation function is fair. Fig. 1 provides an
illustrative example. when the representation function λ : X → Z is unfair and we adopt gradient
descent to learn the predictor h : Z → R. The optimal predictors of different subgroups (blue, red)
are not invariant, resulting in biased predictions. We will later demonstrate such an intuition ensures
the learned representation satisfying the sufficiency rule (Liu et al., 2019; Chouldechova, 2017).
The aforementioned intuition can be naturally formulated as a bi-level optimization problem, where
we aim to adjust the representation λ (in the outer-level) to satisfy the invariant optimal predictor
h (in the inner-level). Thus, when we adopt the gradient-based approach in solving the bi-level
objective, a straightforward solution is to learn the representation λ to fulfill the identical explicit
gradient-descent directions in learning predictor h? of different groups, shown in Fig. 2(a). Intu-
itively, if the inner gradient descent step of each sub-group is identical, their final predictors (as
the approximation of h?) will be invariant. However, the corresponding algorithmic realization is
challenging in deep learning: 1) It requires storing the whole gradient steps, which induces a high
memory burden. 2) the embedding function λ is optimized via backpropagation from the whole
gradient optimization path, which induces a high computational complexity.
To this end, we propose an implicit path alignment, shown in Fig. 2(b). Notably, we only consider
the final (t-th) update of the predictor h(t), then we update representation function λ by approxi-
mating its gradient at point h(t) through the implicit function (Bengio, 2000). By using the gradient
approximation, it is no more required to store the whole gradient step and conduct the backpropaga-
tion through the entire path. Overall, the highlights in this paper are as follows:
Fair-representation learning to satisfy the sufficiency rule Instead of enforcing the independence
or separation rule, the considered fair-representation criteria is proved to satisfy the sufficiency rule
in both classification and regression. We also find such a criteria is intrinsically consistent with the
recent Invariant Risk Minimization (IRM) (ArjoVsky et al., 2019; Buhlmann, 2020), which aims to
eliminate suspicious correlations while keeping robust correlations that are invariant across different
environments. Intuitively, reducing the correlation w.r.t. the protected attributes enables the fair
representation.
Principled and efficient algorithm We proposed a novel implicit path alignment algorithm to learn
the fair representation, which addressed the prohibitive memory and computational cost in the orig-
inal bi-level objective. Besides, we analyzed the approximation error gap of the proposed implicit
algorithm, which induces a trade-off between the correct gradient estimation and fairness measures.
Improved fairness in classification and regression We evaluated the implicit algorithm in both
classification and regression with tabular, computer vision and NLP datasets. Compared to the
baselines, the implicit algorithm effectively improved the fairness with a smaller sufficiency gap.
2
Under review as a conference paper at ICLR 2022
2 Preliminaries
We suppose the input X ∈ X, the ground truth label Y ∈ Y, and the algorithmic output Y ∈
Y. Throughout the paper, we only consider binary sensitive attribute (i.e, two sub-groups) with
distributions D0 and D1. Then based on (Liu et al., 2019), the sufficiency rule is defined as:
一 .^ 一 . ^ 一
Edo [Y [Y = t] = Edi [Y [Y = t], Vt ∈Y	(1)
To measure the fairness w.r.t. the sufficiency rule, we propose the sufficiency gap as the metric.
Since we aim to evaluate the fairness in both binary classification (Y ∈ {-1, 1}) and regression
(Y ∈ R), the metric is separately defined on these two scenarios.
Sufficiency gap in binary classification Based on the sufficiency rule, the sufficiency gap in binary
classification is naturally defined as:
δSufC =	X	IDO(Y = y∣Y = y)-DI(Y = y∣Y = y)|	⑵
y∈{-1,1}
∆SufC encourages the two subgroups with identical Positive predicted value (PPV) and Nega-
tive predicted value (NPV). On the practical side, considering the healthcare evaluation system
outputs either High Risk or Low Risk, Obermeyer et al. (2019) essentially revealed Dblack(Y =
HighRiskIY = Low Risk) > Dwhite(Y = HighRiskIY = LoWRisk): the severity of Black pa-
tients is actually underestimated. Thus if ∆SufC is small, the racial discrimination can be remedied.
Sufficiency gap in regression Based on the sufficiency rule and (Kuleshov et al., 2018), the suffi-
ciency gap in regression is defined as:
∆SufR
L JDO(Y ≤ t∣Y ≤ t)
-D1 (Y ≤ t∣Y ≤ t)∣dt
(3)
∆SufR ∈ [0,1] is an approximation of |Do(Y = y∣Y = y) -Dι(Y = y∣Y = y)∣, ∀y ∈ R, since the
latter is difficult to estimate. From the practical aspect, assuming the health system outputs a real-
value healthcare score Y = t (higher indicates sicker), Obermeyer et al. (2019); Sjoding et al. (2020)
observed Dblack(Y > tIY ≤ t) > Dwhite(Y > tIY ≤ t): for the patients whose predicted healthcare
score is less than t, the actual proportion of sicker (Y > t) in Black patients is considerably higher
than White patients. Therefore a small ∆SufR suggests an improved disparity.
3	Problem Setup
We denote the representation function λ that maps the input X into the latent variable Z, the predic-
tion function h such that h : Z → R for regression and h : Z → {-1, 1} for binary classification.
We then denote the prediction loss as `, the prediction loss on subgroup DO , D1 is expressed as:
LO (h,λ) = E(x,y)~Do'(h ◦ λ(x),y), L1(h, λ) = E(x,y)~Dι'(h ◦ λ(x),y)
According to the intuition, we aim to solve the following bi-level objective:
min LO(h?O, λ) + L1(h?1, λ)	(Outer level)
s.t.	hO?	=	h1?,	hO?	∈ argmin LO(h, λ),	h1?	∈ argmin	L1(h,	λ).	(Inner level)
hh
Specifically, in the outer level, we aim to find a representation λfor minimizing the prediction error,
given the optimal predictor (hO?, h1?) on the embedding space Z. As for the inner level, given a fixed
representation λ, hO? , h1? are the optimal predictor for each sub-group. The constraints h?O = h1?
additionally encourage the invariant optimal predictors from DO, D1.
Relation to the explicit path alignment In deep learning we adopt the gradient-based approach
to minimize the loss, therefore h? in the inner level is approximated as h(t+1), the t-th update in the
gradient descent: h? ≈ h(0) - Pt VhLo(h(t),λ), h? ≈ h⑼—Pt VhLι(h(t), λ), where h(0) is the
common initialization. Thus the invariant optimal predictor is equivalent to:
X VhLO(h(t), λ) =XVhL1(h(t),λ).
tt
The aforementioned equation suggests learning a representation λ that ensures the identical opti-
mization path w.r.t. h for each sub-group, which recovers the explicit path alignment.
3
Under review as a conference paper at ICLR 2022
Relation to the Sufficiency rule We further demonstrate the relation between the bi-level objec-
tive and Sufficiency rule.
Proposition 1. If we specify the prediction loss ` as logistic regression loss in the classification
log(1 + exp(-yh(z))) with Y = {-1, 1} and the square loss in the regression (h(z) - y)1 2 with
Y ⊂ R. Then minimizing the inner-level loss is equivalent to:
- - - - - . ʌ , , . , . ʌ ,,,,
Edo[Y|Z = z] = Edi [Y[Z = z], Edo [Y]Y = h?(z)] = EdJY|Y = h?(z)],
where h? = h0? = h1? and z = λ(x).
Proposition 1 reveals that the objective of inner-level loss is to encourage the sufficiency rule.
4	Practical Algorithms
In this section, we propose an implicit alignment in deep learning, where λ and h are implemented
by the neural network. We also reformulate as the original objective through Lagrangian relaxation:
min Lo(h0,λ) + Lι(h?, λ) + K∣∣h? - h?k2	(Outer level)
s.t. h0? ∈ argmin L0(h, λ), h1? ∈ argmin L1(h, λ),	(Inner level)
hh
where κ > 0 is the coefficient to control the fairness. Then we will drive the approximated gradient
w.r.t. λ, which contains the following key elements.
Solving the inner optimization Given a fixed representation λ, we find h0 , h1 such that:
∣h0? - h0∣ ≤ ,	∣h1? - h1 ∣ ≤ ,
where is the optimization tolerance. Besides, h?1 and h1 are essentially the function of λ, i.e., h1
depends on the predefined representation function λ.
Computing the gradient of λ Given the approximate solution h0, h1, we can compute the gradi-
ent w.r.t. λ (referred as grad(λ)) 1 in the outer-level:
grad(λ) =VιLo(h0, λ) + (Vιh0)T IyhOLo(h0, λ) + κ(h0 - hɪ))
+ VλLι(h1, λ) + (Vλh1)T (VhιLι(h1 ,λ) - κ(h0 - h1)).
Where Vh0L0(h0, λ) is the partial derivative in the loss w.r.t. the first term (about h0), evaluated at
h0. Also VλL0(h0, λ) is the partial derivative w.r.t. the second term (about λ).
Implicit function for approximating the gradient In order to compute grad(λ) in autograd,
we need to estimate Vλh0 and Vλh1. We herein adopt the implicit function (Bengio, 2000) to
approximate Vλh0, which has been adopted in the hyperparameter optimization (Pedregosa, 2016)
and meta-learning (Rajeswaran et al., 2019).
Concretely, if the prediction loss is smooth and there exist stationary points to achieve optimal,
we have: Vh。Lo(h0(X), λ) = 0, VhiLo(hι(λ), λ) = 0. Then differentiating w.r.t. λ will induce:
d (VhoL°(h6(λ),λ))∕dλ = Vh0Lo(h0,λ)Vλh0+VλVhoLo(h?,X) = 0.2 ThUsWehave Vλh0 =
-(Vho Lo(ho, λ)) 1 (VλVho Lo(h0, λ)), where the Hessian matrix VhG Lo(ho, λ) is assumed to be
invertible.
Through the implicit function, we can approximate Vλ h0 as:
Vλh0 ≈ -NhoLo(h0,λ))-1 (VλVhoLo(h0,λ))
As for Vλh1,we have the similar result: Vλhf ≈ - (Vhl L1(hj,λ)) 1 (VλVhι L1 (h1 λ)).
1We denote the ground truth gradient as grad(λ) if we adopt optimal predictor h0?, h1? in the computation.
2d(∙)/dλ denotes the total derivative.
4
Under review as a conference paper at ICLR 2022
Algorithm 1 Implicit Path Alignment Algorithm
Ensure: Representation function λ, predictor h0, h1, datasets from two sub-groups D0. D1.
1:	for mini-batch of samples from (D0, D1) do
2:	Solving the inner-level optimization with tolerance . Obtaining h0, h1.
3:	Solving Eq. (4) with tolerance δ. Obtaining pδ0 and pδ1.
δ
4:	Computing grad (λ) (gradient of representation λ)
δ
5:	Updating λ through autograd: λ J λ - grad (λ)
6:	end for
7:	return λ, h0, h1
Efficient and numerical stable gradient estimation Plugging in the approximations, the gradient
w.r.t λ is approximated as:
grad(λ) ≈ VιLo(h0, N-Sh Lo (h0, λ))T NhO Lo(h0, λ))-1 (Vh. Lo(h0, λ) + κ(h0 - hɪ))
、-------------------{z-------------------}
p0
+ VλLι(h1, λ) - (VλVhιLι(h1, λ))T (VhιLι(h1, λ))-1 (VhιLι(h1 ,λ) - κ(h0 - hɪ))
、-----------------------------------------------------{z------------------}
p1
However, the current form is still computationally expensive due to the computation of inverse
Hessian matrix. To this end, we denote p0 and p1 as the inverse-Hessian vector product. Then
computing p0 and p1 is equivalent to solve the following quadratic programming (QP):
αrgminp02笳(VhO纵际刈鱼-P (VhOL0(h0,λ) + κ(h0- h1))
ɑrgminp1 20T (VhI Ll(h1,λA d-JJT (Vhi L1(h1,λ) - κ(h0 - hI))	⑷
Since it is a typical QP problem and we adopt conjugate gradient method (Concus et al., 1985;
Rajeswaran et al., 2019), which can be updated efficiently through autograd via computing the
Hessian-vector product. We additionally suppose the optimization error in the QP as δ, i.e.: kp0 -
pδ0 k ≤ δ, kp1 - pδ1 k ≤ δ, then the gradient w.r.t representation λ can be finally expressed as:
gradδ(λ) = VλLo(h0, λ) - (VλVh.Lo(h0, λ))T p0 + VλLι(h1, λ) - (VλVh∕ι(h1, λ))T pδ
δ
The grad (λ) can be also efficiently estimated through Hessian vector product via autograd with-
out explicitly computing the Hessian matrix.
Proposed algorithm Based on the key elements, the proposed algorithm is shown in Algo. 1.
4.1 The cost of Implicit algorithm: Approximation-Fair Trade-Off
Theorem 1 (Approximation Error Gap). Suppose that (1) Smooth Predictive Loss. The first-order
derivatives and second-order derivatives of L are Lipschitz continuous; (2) Non-singular Hessian
matrix. We assume VhO,hOL0(h0, λ), Vh1,h1 L1(h1, λ), the Hessian matrix of the inner optimiza-
tion problem, are invertible. (3) Bounded representation and predictor function. We assume the λ
and h are bounded, i.e., kλk, khk are upper bounded by the predefined positive constants. Then
the approximation error between the ground truth and algorithmic estimated gradient w.r.t. the
representation is be upper bounded by:
δ
kgrad(λ) - grad (λ)k = O(κ + + δ).
The proof is delegated in Appendix B. We also discuss the assumptions to guarantee the convergence
of Algorithm 1, shown in Appendix C.
Theorem 1 reveals that the gradient approximation error depends on the two-level optimization
tolerance , δ and the coefficient of fair constraints κ. Specifically, the error gap reveals the inherent
trade-off in accurate gradient estimation and fair-representation learning. If we fix the optimization
tolerance and δ, a smaller κ indicates a better approximation of the gradient, which yields weak
fair constraints. Thus the implicit alignment introduces a trade-off in the prediction performance
(i.e., correct approximation of the gradient) and fairness measurement.
5
Under review as a conference paper at ICLR 2022
5	Related Work
Fair Machine Learning Below we only list the most related work in the fairness and refer to the
survey paper (Mehrabi et al., 2021) for details in the algorithmic fairness. In the classification, var-
ious methods in learning fair representations have been proposed. Specifically, a common strategy
is to introduce the statistical constraints as the regularization during the training, e.g., demographic
parity (DP) (Zhang et al., 2018; Madras et al., 2018; Song et al., 2019; Jiang et al., 2020; Kehrenberg
et al., 2020) or equalized odds (EO) (Song et al., 2019; Gupta et al., 2021) as the proxy of the sepa-
ration and independence rule. Another direction is to disentangle the data for factorizing meaningful
representations such as (Locatello et al., 2019). Intuitively, the disentangled embedding is indepen-
dent of the sensitive attribution, thus reflecting a fair representation w.r.t. the independence rule,
which can be potentially problematic when the label distributions of subgroups vary dramatically
(Zhao et al., 2019).
Fairness has also been extended to the fields beyond classification. For instance, in the regression
problem (Komiyama et al., 2018; Agarwal et al., 2019), the bounded group loss has been proposed
as the fair measure: if prediction loss in each subgroup is smaller than , the regression is -level
fair. In fact, the fair criteria in our paper is not equivalent to -fair. Given a fixed λ, the -level fair
does not guarantee the optimal and invariant predictor for each subgroup and vice versa.
The sufficiency rule has also been discussed in the previous work. Notably, Chouldechova (2017);
Liu et al. (2019) proposed the sufficiency gap in classification for measuring fairness w.r.t. the
sufficiency rule. Liu et al. (2019) also discussed the inequivalence between the sufficiency gap and
probabilistic calibration (Guo et al., 2017) (referred as calibration gap). According to Pleiss et al.
(2017), the calibration rule is a stronger condition than sufficiency rule while it simultaneously hurts
the prediction performance. Throughout this paper, we only consider the sufficiency rule. The triple
trade-off between the calibration rule, sufficiency rule, and prediction performance will be left as
future work.
Invariant Risk Minimization The analyzed fair-representation criteria shares a quite similar spirit
to the IRM (ArjoVsky et al., 2019; Buhlmann, 2020; Creager et al., 2021), where an algorithm
IRM_v1 is proposed to enable the out-of-distribution (OOD) generalization. The key difference be-
tween our work and (ArjoVsky et al., 2019) lies in the algorithmic aspect: it has been theoretically
justified that the originally proposed IRM_v1 does not necessarily capture the invariance (Rosenfeld
et al., 2020). By contrast, we directly solVe the bi-leVel objectiVe in the context of deep-learning
and propose an efficient practical algorithm with better empirical performance than IRM_v1. Be-
sides, based on Chen et al. (2021), the proposed algorithm does not provably guarantee the OOD
generalization property due to the limited subgroups (N = 2) considered within the paper.
6	Experiments
6.1	Experimental setup
In the paper, we adopt the sufficiency gap as fair metrics,
where Y is denoted as:
Y= (h0 ◦ λ(X), X ∈Do
Y = h1 ◦ λ(X), X ∈ D1
Then in the binary classification, we can estimate ∆SufC =
__ . . ^ . . . ^ ..
Py∈{-1,+1} lDo(Y = y|Y = y) - Di(Y = y|Y = y)|
from the data.
As for regression, the sufficiency gap ∆SufR =
Rt ∣Do(Y ≤ t|Y ≤ t) - Dι(Y ≤ t|Y ≤ t)l (shown
in Fig. 3, the orange region) is difficult to estimate due
to the integration. To address this, we sample multi-
ple values {t1, . . . , tm} and compute its average differ-
ence as the approximation of the integration. ∆SufR ≈
m⅛ Pi=I ∣Do(Y ≤ ti∣Y ≤ ti) - Dι(Y ≤ ti∣Y ≤ ti)l
Figure 3: Sufficiency gap (∆SufR) in
regression
6
Under review as a conference paper at ICLR 2022
Method	Accuracy (↑)	△Sufc Q)
ERM (I)	0.768 ± 0.004	0.173 ± 0.008
Adv_debias (II)	0.760 ± 0.008	0.291 ± 0.006
Mixup (III)	0.758 ± 0.003	0.343 ± 0.022
IRM_v1 (IV)	0.753 ± 0.004	0.057 ± 0.015
One_step (V)	0.755 ± 0.007	0.048 ± 0.008
Implicit	0.760 ± 0.007	0.051 ± 0.012
Table 1: Toxic comments dataset. Accuracy and
∆SufC in different approaches.
0-75
0.05	0.08	0-11	0-14	0-17
A Suf,C (Sufficiency Gap)
Figure 4: Toxic. Accuracy-Fair Trade-off
Concretely, for a given ti in each group, we compute the percentile (Y0) at point t: D0(Y0 ≤ ti),
then we compute the corresponding ground truth cumulative distribution (Y ) at the same point ti :
D(Y ≤ ti|Y ≤ ti). Through the aforementioned approximation, We can compute ∣Do(Y ≤ ti|Y ≤
Ii)-DI(Y ≤ ti|Y ≤ ti)|.
Baselines We consider the baselines that add fairness constraints during the training process.
Specifically, We compare our method With (I) empirical risk minimization (ERM) that trains the
model Without considering fairness; (II) adversarial debiasing (Zhang et al., 2018); (III) fair mix-up
(Chuang & Mroueh, 2021), a recent data-augmentation and effective approach in the fair represen-
tation learning. In fact, the baselines (II) and (III) are DP-based fair approaches, Which is designed
to demonstrate the general non-compatibility in addressing the sufficiency based fairness.
Besides, We include tWo additional baselines that have the similar objective but different algorithmic
realizations. (IV) the original IRM regularization (referred as IRM_v1) (ArjoVsky et al., 2019),
Which adds a gradient penalty to encourage the invariance. (V) One-step explicit alignment. In the
inner-leVel optimization, We suppose to conduct the one-step gradient descent for each sub-group.
Then in the outer-leVel optimization, We add a gradient-incoherence constraint to encourage the
identical (one-step) optimization path: min、∣∣VhoL0(h0, λ) - ▽%1Lι(hι,λ)k2. All the results
are reported by aVeraging fiVe repetitions and additional experimental details are delegated in the
Appendix.
6.2	Empirical Results
6.2.	1 Toxic Comments
The toxic comments dataset (JigsaW, 2018) is a binary classification task in NLP to predict Whether
comment is toxic or not. The original label is actually not binary since the comments is decided
by multiple annotators, Where the labelling discrepancy generally occurs. To this end, We conduct
a simple strategy to decide comment is toxic if at least one annotator marks it. In this dataset, a
portion of comments haVe been labeled With identity attributes, including gender and race. It has
also been reVealed that the race identity (e.g., black) is correlated With the toxicity label, Which can
lead to the predictiVe discrimination. Thus We adopted the race as the protected group by selecting
tWo subgroups of Black and Asian. For the sake of computational simplicity, We first applied the
pretrained BERT (DeVlin et al., 2018) to extract the Word embedding With 748 dimensional Vector.
Then We adopt representation function λ as tWo fully-connected layers With hidden dimension 200
With Relu actiVation and classifier h as a linear predictor. We report the test-set sub-group aVerage
accuracy and sufficiency gap (∆SufC) in Tab. 1 and Fig. 4.
The results reVeal seVeral interesting facts. (1) The Demographic Parity (DP) based fair constraints
are generally non-compatible With the sufficiency rule. Specifically, baseline (II,III) eVen increase
∆SufC With higher Value than ERM. (2) For the baselines that track the sufficiency rule (IV,V),
the sufficiency gap ∆SufC is improVed With a similar accuracy, shoWn in Tab.1. We also change
the regularization coefficient in (IV,V) and κ in the implicit approach. We obserVe that the implicit
approach demonstrates a consistent better Accuracy-Fair trade-off, shoWn in Fig. 4.
7
Under review as a conference paper at ICLR 2022
Method	Accuracy (↑)	△Sufc Q)	0-79	Ii	
ERM (I)	0.780 ± 0.015	0.210 ± 0.022			
Adv_debias (II) Mixup (III)	0.785 ± 0.022 0.792 ± 0.011	0.165 ± 0.028 0.160 ± 0.010	>0.78 g *t 0-77		*
IRM_v1 (IV) One_step (V) Implicit	0.795 ± 0.012 0.797 ± 0.006 0.794 ± 0.027	0.086 ± 0.015 0.086 ± 0.012 0.074 ± 0.020	0-76 4	f	⅝ Implicit Oπe-step →- IRM_vl * ERM
0.05	0-10	0-15	0.20
∆ Suf,C (Sufficiency Gap)
Table 2: CelebA dataset. Accuracy and predictive
parity in different approaches.	Figure 5: CelebA. Accuracy-Fair Trade-off
6.2.2	CelebA Dataset
The CelebA dataset (Liu et al., 2015) contains around 200K images of celebrity faces, where each
image is associated with 40 human-annotated binary attributes including gender, hair color, young,
etc. In this paper, we designate gender as the sensitive attribute, and attractive as the binary clas-
sification task. We randomly select around 82K and 18K images as the training and validation set.
Then we adopt representation function λ as pre-trained ResNet-18 (He et al., 2016) and classifier
h as two-fully connected layers. We report the test-set sub-group average accuracy and sufficiency
gap (∆SufC) in Tab. 2 and Fig. 5.
The results in the CelebA show similar behaviors with the Toxic comments. Specifically, the DP
based fair approaches (II, III) did not effectively improve ∆SufC, shown in Tab. 2. In contrast, the
sufficiency can be significantly improved in baselines (IV, V) and implicit approach without largely
losing the accuracy. Specifically, Fig. 5 visualizes the accuracy-fair trade-off curve, where the later
three approaches show quite similar behaviors.
6.2.3	Law Dataset
The Law Dataset is a regression task to predict a students GPA (real value, ranging from [0, 4]),
where the data is utilized from the School Admissions Councils National Longitudinal Bar Passage
Study (Wightman, 1998) with 20K examples. In the regression task, we adopt the square loss and
race as the protected attribute (white versus non-white). We adopt λ as the one fully connected
layer with hidden dimension 100 and Relu activation and predictor h as a linear predictor. We report
the test-set subgroup average MSE (Mean Square Error) and sufficiency gap (∆SufR) in Tab. 1 and
Fig. 4.
Compared to the classification task, the results show similar behaviors in the regression. Specifically,
the DP based fair approaches (II, III) still increase ∆SufR in the regression. In contrast, the gap is
significantly improved in our proposed approach and baseline (IV,V). Specifically, Fig. 7 visualizes
the sufficiency-gap of different approaches, where the implicit approach significantly mitigate the
sufficiency gap. Besides, Fig. 6 shows the MSE-sufficiency gap curve, which still reveals the implicit
approach benefits a better trade-off between the performance and fairness.
Method	MSE (1)	△SufR (1)
ERM (I)	0.190 ± 0.005	0.160 ± 0.007
Adv_debias (II)	0.223 ± 0.008	0.188 ± 0.012
Mixup (III)	0.216 ± 0.012	0.172 ± 0.007
IRM_v1 (IV)	0.208 ± 0.006	0.096 ± 0.006
One_step (V)	0.204 ± 0.007	0.125 ± 0.010
Implicit	0.198 ± 0.005	0.091 ± 0.011
Table 3: Law dataset. MSE and sufficiency gap in
different approaches.
0.04 0.06 0.08 0-10 0-12 0.14 0.16
Δ Suf_R (Sufficiency Gap)
Figure 6: Law. MSE-Fair Trade-off
8
Under review as a conference paper at ICLR 2022
(a) ERM
(b) Fair Mix-up
(c) Implicit
Figure 7: Illustration of the sufficiency gap (∆SufR) in Law dataset (regression). The ERM and Fair
mix-up suffer a high ∆SufR , while the proposed implicit alignment can significantly mitigate the
sufficiency gap.
Method	MSE (1)	∆SufR (1)
ERM (I)	1.939 ± 0.021	0.246 ± 0.019
Adv_debias (II)	1.982 ± 0.016	0.252 ± 0.020
Mixup (III)	1.979 ± 0.025	0.246 ± 0.023
IRM_v1 (IV)	1.927 ± 0.031	0.077 ± 0.009
One_step (V)	1.904 ± 0.027	0.090 ± 0.019
Implicit	1.906 ± 0.019	0.051 ± 0.005
Table 4: NLSY dataset. MSE and sufficiency gap
in different approaches.
Figure 8: NLSY. MSE-Fair Trade-off
6.2.4	NLSY Dataset
The National Longitudinal Survey of Youth (NLSY, 2021) dataset is a regression task with around
7K dataset, which involves the survey results of the U.S. Bureau of Labor Statistics. It is intended to
gather information on the labor market activities and other life events of several groups for predicting
the income y of each person. We treat the gender as the sensitive attribute. We also normalize the
output y by diving the 10, 000, then the final output y ranges around [0, 8]. The prediction loss
is also the square loss. We adopt representation λ as the two fully connected layers with hidden
dimension 200 and Relu activation and predictor h as a linear predictor. We report the test-set
sub-group average MSE (Mean Square Error) and Sufficiency Gap (∆SufR) in Tab. 4 and Fig. 8.
Tab. 4 provides similar trends with other datasets. Baselines (IV,V) and implicit approach effective
control the sufficiency gap, while the DP based approach generally fails to improve the gap. Fig. 8
reveals a slightly better approximation-fair trade off for the implicit approach. Finally, Fig. 11 (in
Appendix) visualizes the sufficiency gap of different algorithms. The gap is actually significantly
improved while the calibration gap still exists, which is consistent with (Liu et al., 2019). Therefore
it can be quite interesting and promising to analyze the triple trade-off between the sufficiency gap,
calibration gap and prediction performance in the regression.
7 Conclusion
We considered the fair representation learning from a novel perspective through encouraging the
invariant optimal predictors on the top of data representation. Then we formulated this problem as a
bi-level optimization and proposed an implicit alignment algorithm. We further demonstrated the bi-
level objective is to fulfil the sufficiency rule. Besides, we also analyzed the error gap of the implicit
algorithm. The empirical results in both classification and regression settings suggest the improved
fairness measurement. Finally, we think the future work can include developing computationally
efficient explicit algorithms for avoiding the biased gradient computation.
9
Under review as a conference paper at ICLR 2022
Ethics S tatement
This paper proposed a novel fair representation algorithm, which aims to address the potential pre-
diction discrimination towards several subgroups. The proposed approach may also introduce the
potential negative impact: we merely address the fairness with respect to the sufficiency rule in the
paper, which is not always the preferable criteria in several specific scenarios.
Reproducibility S tatement
We provided a demo source code in the supplementary material for a better understanding the pro-
posed algorithm. Besides, the detailed experimental descriptions and theoretical proofs are also
provided in the appendix.
References
Alekh Agarwal, Miroslav Dudik, and ZhiWei Steven Wu. Fair regression: Quantitative definitions
and reduction-based algorithms. In International Conference on Machine Learning, pp. 120-129.
PMLR, 2019.
Martin Arjovsky, Leon Bottou, Ishaan Gulrajani, and David Lopez-Paz. Invariant risk minimization.
arXiv preprint arXiv:1907.02893, 2019.
Solon Barocas, Moritz Hardt, and Arvind Narayanan. Fairness and Machine Learning. fairml-
book.org, 2019. http://www.fairmlbook.org.
Yoshua Bengio. Gradient-based optimization of hyperparameters. Neural computation, 12(8):1889-
1900, 2000.
Stephen Boyd, Stephen P Boyd, and Lieven Vandenberghe. Convex optimization. Cambridge uni-
versity press, 2004.
Peter Buhlmann. Invariance, causality and robustness. Statistical Science, 35(3):404^26, 2020.
Kai-Wei Chang, Vinod Prabhakaran, and Vicente Ordonez. Bias and fairness in natural language
processing. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language
Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-
IJCNLP): Tutorial Abstracts, Hong Kong, China, November 2019. Association for Computational
Linguistics. URL https://aclanthology.org/D19-2004.
Yining Chen, Elan Rosenfeld, Mark Sellke, Tengyu Ma, and Andrej Risteski. Iterative feature
matching: Toward provable domain generalization with logarithmic environments. arXiv preprint
arXiv:2106.09913, 2021.
Alexandra Chouldechova. Fair prediction with disparate impact: A study of bias in recidivism
prediction instruments. Big data, 5(2):153-163, 2017.
Ching-Yao Chuang and Youssef Mroueh. Fair mixup: Fairness via interpolation. arXiv preprint
arXiv:2103.06503, 2021.
Evgenii Chzhen, Christophe Denis, Mohamed Hebiri, Luca Oneto, and Massimiliano Pontil. Fair
regression with wasserstein barycenters. arXiv preprint arXiv:2006.07286, 2020.
P. Concus, G. Golub, and Gerard Meurant. Block preconditioning for the conjugate gradient method.
Siam Journal on Scientific and Statistical Computing, 6, 01 1985. doi: 10.1137/0906018.
Elliot Creager, JOrn-Henrik Jacobsen, and Richard Zemel. Environment inference for invariant
learning. In International Conference on Machine Learning, pp. 2189-2200. PMLR, 2021.
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep
bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018.
10
Under review as a conference paper at ICLR 2022
Kawin Ethayarajh. Is your classifier actually biased? measuring fairness under uncertainty with
bernstein bounds. In Proceedings of the 58th Annual Meeting of the Association for Computa-
tional Linguistics, pp. 2914-29l9, Online, July 2020. Association for Computational LingUis-
tics. doi: 10.18653/v1/2020.acl-main.262. URL https://aclanthology.org/2020.
acl-main.262.
Richard Ribon Fletcher, AUdace Nakeshimana, and Olusubomi Olubeko. Addressing fairness, bias,
and appropriate use of artificial intelligence and machine learning in global health. Frontiers
in Artificial Intelligence, 3:116, 2021. ISSN 2624-8212. doi: 10.3389/frai.2020.561802. URL
https://www.frontiersin.org/article/10.3389/frai.2020.561802.
Chuan Guo, Geoff Pleiss, Yu Sun, and Kilian Q Weinberger. On calibration of modern neural
networks. In International Conference on Machine Learning, pp. 1321-1330. PMLR, 2017.
Umang Gupta, Aaron Ferber, Bistra Dilkina, and Greg Ver Steeg. Controllable guarantees for fair
outcomes via contrastive information estimation. In Proceedings of the AAAI Conference on
Artificial Intelligence, volume 35, pp. 7610-7619, 2021.
Moritz Hardt, Eric Price, and Nati Srebro. Equality of opportunity in supervised learning. Advances
in neural information processing systems, 29:3315-3323, 2016.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recog-
nition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp.
770-778, 2016.
Ray Jiang, Aldo Pacchiano, Tom Stepleton, Heinrich Jiang, and Silvia Chiappa. Wasserstein fair
classification. In Uncertainty in Artificial Intelligence, pp. 862-872. PMLR, 2020.
Jigsaw. Toxic comment classification challenge, 2018. URL https://www.kaggle.
com/c/jigsaw-toxic-comment-classification-challenge/overview/
description.
Thomas Kehrenberg, Myles Bartlett, Oliver Thomas, and Novi Quadrianto. Null-sampling for in-
terpretable and fair representations. In European Conference on Computer Vision, pp. 565-580.
Springer, 2020.
Byungju Kim, Hyunwoo Kim, Kyungsu Kim, Sungjin Kim, and Junmo Kim. Learning not to learn:
Training deep neural networks with biased data. In Proceedings of the IEEE/CVF Conference on
Computer Vision and Pattern Recognition, pp. 9012-9020, 2019.
Junpei Komiyama, Akiko Takeda, Junya Honda, and Hajime Shimao. Nonconvex optimization for
regression with fairness constraints. In Jennifer Dy and Andreas Krause (eds.), Proceedings of
the 35th International Conference on Machine Learning, volume 80 of Proceedings of Machine
Learning Research, pp. 2737-2746. PMLR, 10-15 Jul 2018. URL https://proceedings.
mlr.press/v80/komiyama18a.html.
Volodymyr Kuleshov, Nathan Fenner, and Stefano Ermon. Accurate uncertainties for deep learning
using calibrated regression. In International Conference on Machine Learning, pp. 2796-2804.
PMLR, 2018.
Yann LeCun, Yoshua Bengio, and Geoffrey Hinton. Deep learning. nature, 521(7553):436-444,
2015.
Lydia T. Liu, Max Simchowitz, and Moritz Hardt. The implicit fairness criterion of unconstrained
learning. In Kamalika Chaudhuri and Ruslan Salakhutdinov (eds.), Proceedings of the 36th In-
ternational Conference on Machine Learning, volume 97 of Proceedings of Machine Learning
Research, pp. 4051-4060. PMLR, 09-15 Jun 2019. URL https://proceedings.mlr.
press/v97/liu19f.html.
Ziwei Liu, Ping Luo, Xiaogang Wang, and Xiaoou Tang. Deep learning face attributes in the wild.
In Proceedings of International Conference on Computer Vision (ICCV), December 2015.
11
Under review as a conference paper at ICLR 2022
Francesco Locatello, Gabriele Abbati, Tom Rainforth, Stefan Bauer, Bernhard SchOlkopf,
and Olivier Bachem. On the fairness of disentangled representations. arXiv preprint
arXiv:1905.13662, 2019.
David Madras, Elliot Creager, Toniann Pitassi, and Richard Zemel. Learning adversarially fair and
transferable representations. In International Conference on Machine Learning, pp. 3384-3393.
PMLR, 2018.
Ninareh Mehrabi, Fred Morstatter, Nripsuta Saxena, Kristina Lerman, and Aram Galstyan. A survey
on bias and fairness in machine learning. ACM Computing Surveys (CSUR), 54(6):1-35, 2021.
NLSY. National longitudinal survey of youth, 2021. URL https://www.bls.gov/nls/.
Ziad Obermeyer, Brian Powers, Christine Vogeli, and Sendhil Mullainathan. Dissecting racial bias
in an algorithm used to manage the health of populations. Science, 366(6464):447-453, 2019.
doi: 10.1126/science.aax2342. URL https://www.science.org/doi/abs/10.1126/
science.aax2342.
Fabian Pedregosa. Hyperparameter optimization with approximate gradient. In International con-
ference on machine learning, pp. 737-746. PMLR, 2016.
Geoff Pleiss, Manish Raghavan, Felix Wu, Jon Kleinberg, and Kilian Q Weinberger. On fairness
and calibration. arXiv preprint arXiv:1709.02012, 2017.
Manish Raghavan, Solon Barocas, Jon Kleinberg, and Karen Levy. Mitigating bias in algorithmic
hiring: Evaluating claims and practices. In Proceedings of the 2020 conference on fairness,
accountability, and transparency, pp. 469-481, 2020.
Aravind Rajeswaran, Chelsea Finn, Sham Kakade, and Sergey Levine. Meta-learning with implicit
gradients. In Advances in neural information processing systems, 2019.
Elan Rosenfeld, Pradeep Ravikumar, and Andrej Risteski. The risks of invariant risk minimization.
arXiv preprint arXiv:2010.05761, 2020.
Michael W Sjoding, Robert P Dickson, Theodore J Iwashyna, Steven E Gay, and Thomas S Valley.
Racial bias in pulse oximetry measurement. New England Journal of Medicine, 383(25):2477-
2478, 2020.
Jiaming Song, Pratyusha Kalluri, Aditya Grover, Shengjia Zhao, and Stefano Ermon. Learning
controllable fair representations. In The 22nd International Conference on Artificial Intelligence
and Statistics, pp. 2164-2173. PMLR, 2019.
Linda F. Wightman. Lsac national longitudinal bar passage study, 1998.
Rich Zemel, Yu Wu, Kevin Swersky, Toni Pitassi, and Cynthia Dwork. Learning fair representations.
In International conference on machine learning, pp. 325-333. PMLR, 2013.
Brian Hu Zhang, Blake Lemoine, and Margaret Mitchell. Mitigating unwanted biases with adver-
sarial learning. In Proceedings of the 2018 AAAI/ACM Conference on AI, Ethics, and Society, pp.
335-340, 2018.
Wenbin Zhang and Eirini Ntoutsi. Faht: an adaptive fairness-aware decision tree classifier. arXiv
preprint arXiv:1907.07237, 2019.
Han Zhao, Amanda Coston, Tameem Adel, and Geoffrey J Gordon. Conditional learning of fair
representations. arXiv preprint arXiv:1910.07162, 2019.
12
Under review as a conference paper at ICLR 2022
A	Proposition 1
We consider the regression and classification separately.
Regression According to the definition, given a fixed and deterministic representation λ, we have
L0(h,λ) = ED0 (h(z) - y)2
It is noted as a typical regression problem with square error. We set the derivative as zero:
VhLo(h, λ) = 0, We have h?(Z) = Ed。[Y|Z = z]. As for Di, We apply the same strategy
with h1?(z) = ED1 [Y|Z = z]. Based on the invariant optimal predictor, we have ED0 [Y|Z = z] =
ED1[Y|Z= z] With z = λ(x).
Classification According to the definition, We have:
L0(h, λ) = ED0 log(1 + exp(-yh(z)))
Since the optimal predictor on the logistic loss is the log-conditional density ratio: h0?(z) =
log (DD(YY=-1|ZZ=Zz)). Observe that in the binary classification with Y = {-1,1}, we have
Do (Y = 1|Z = Z) = 2 (1 + Edo [Y[Z = z]) and Do (Y = -1|Z = Z) = ɪ (1 - Ed。[Y[Z = z]),
then we have:
h?」(1+ EDO[Y|Z "I )
V - EDOiYIZ = z]√
AC ffΛτ∙ DO-, ∖x∕α CIrICnf fhɑ cqtπp CfrCIfQ0∖7 CInrI ∖x∕α hα∖7Q IC(T D 1+MDO ['∣Z z] ʌ — ICD D 1+MD1 ∣'∣Z z] ʌ
AS for Di, we adopt the same Strategy and we have log I 1—EDO [Y∣Z=z] ) = log ( 1-Edi [Y|Z=z] J ,
then we have EDO [Y|Z = Z] = ED1 [Y|Z = Z].
As for the predictive parity, since we have EDO [Y|Z = Z] = ED1 [Y|Z = Z] and h? = h?i = h2?,
then we have EDO [Y |h?(Z)] = ED1 [Y|h?(Z)].
B Approximation Error
Theorem 2 (Approximation Error Gap). Suppose that (1) Smooth Predictive Loss. The first-order
derivatives and second-order derivatives of L are Lipschitz continuous; (2) Non-singular Hessian
matrix. We assume VhO,hOLo(ho, λ), Vh1,h1Li(hi, λ), the Hessian matrix of the inner optimization
problem, are invertible. (3) Bounded representation and predictor function. We assume the λ
and h are bounded, i.e., kλk, khk are upper bounded by the predefined positive constants. Then
the approximation error between the ground truth and algorithmic estimated gradient w.r.t. the
representation is be upper bounded by:
δ
kgrad(λ) - grad (λ)k = O(κ + + δ).
Proof. We denote grad(λ) as the ground truth gradient w.r.t. λ in outer-level loss (given the optimal
predictor ho? , hi?). Then we aim to bound
δ
kgrad(λ) - grad (λ)k
We first introduce the following terms for facilitating the proof:
A0 = Vho VλLo(h0, λ),A1 = VλVhιLι(h1,λ), A0 = VιV八.Lo(h?, λ), A? = VλVh∕i(h1, λ),
B0 = VλLo(h0, λ), B； = VλLι(h1, λ), B? = VλLo(h?, λ),B? = VλLι(h?, λ),
p? = (VhLo(h?, λ))-1 (VhoLo(h?, λ) + κ(h? - hi)),
P? = (VhιLi(h?, λ))-1 (VhιLi(h?, λ) - κ(h? - h?)).
Then the approximation error gap can be expressed as:
kgrad(λ) - gradδ(λ)k = k (B? - A?p? + B? - A?p?) - (B； - AOpO + B? - A?p?) k
ii
≤ X kB? - B；k + X kA?P? - Aδpδk
i=o	i=o
13
Under review as a conference paper at ICLR 2022
Due to the symmetric of D0 and D1, we only focus on the term on i = 0, the the upper bound in
i = 1 can be derived analogously.
As for bounding kB0? - B0k, since we assume first order derivative of the loss is Lipschitz functions
(with constant L1), then we have :
kB0? - B0 k ≤ L1 kh0? - h0k ≤ L1
Then the second term can be upper bounded by three terms:
kA0?p?0 - Aδ0pδ0k ≤ kA0?p0? - A?0p0k + kA?0 p0 - A0p0k + kA0p0 - A0 pδ0 k
---------------------------------} '---------------} '-------{-------}
(1)	(2)	(3)
Before estimating the upper bound, we first demonstrate kA0 k and kA?0 k are also bounded.
Since we assume λ and h are bounded (assuming the bounded constant as η and φ), the second order
derivative are LiPschitz (with constant L2). Then We consider another fixed point (λ0, h?(X0)) with
bounded second order derivative: A0 = Vho λ£0(h?(10), λ0) and ∣∣A0k ≤ A. We have:
kA? — A0k2 ≤ L2k[hO(λ),λ] - [hθ(λ0),λ0]∣2 ≤ L2Pη2+φ2
Thus we have ||A?k ≤ A + L2 y∕η2+φ2 = A?up. AS for the second derivative at point h0, it can
be upper bounded analogously with a similar constant Asup .
The upper bound of term (1) We have:
kA?0p0?-A0?p0k ≤ kA0?kkp0?-p0k
We have proved kA0? k is upper bounded by As?up . We additionally introduce the following auxiliary
terms:
P0 = (Vh0 Lo^,λ))-1,Plf = (Vhι Lι(hj,λ))-1.
b0? = Vh0 L0 (h?0, λ) + κ(h0? - h1?), b0 = Vh0 L0 (h0, λ) + κ(h0 - h1)
Then we have:
kp0? - p0k = kP0?b?0 - P0b0k
≤ kP0?b0?-P0?b0k+kP0?b0-P0b0k
≤ kP0?kkb0?-b0k+kb0kkP0?-P0k
As for the kP0? k, since we assume the Hessian matrix is invertible thus its norm is upper bounded
by some constant (denoted as A-1). As for kb0? - b0k, we have:
kb0? - b0 k ≤ kVh0L0(h0?, λ) - Vh0L0(h0, λ)k +2κ
≤ L1 +2κ
Thus we have kP0?kkb0? - b0k ≤ A-1(L1 + 2κ).
As for kb0k, we can easily verify that it is indeed bounded by some constant b. For the first term,
we can adopt the same strategy in proving bounded kA0? k. As for the second term in b0, it is upper
bounded by 2κφ, due to the bounded predictor.
We now demonstrate kP0? - P0k. Denoting ∆ = (P0?)-1 - (P0)-1, then according to the second
order Lipschitz assumption, we have: k∆k ≤ L2. Plugging in the result, we have:
∣P0 — P0k = k(P0^)∆(P^k ≤ kP?kk∆k∣PJk ≤ (A-1)2L2E
We still adopt the assumption that the bounded Hessian-inverse matrix by A-1.
Plugging in all the results, we have:
(1)	≤ A1 (L1 + 2κ) +b(A1)2L2 := O(κ +)
14
Under review as a conference paper at ICLR 2022
The upper bound of term (2) We have:
kA0?p0 - A0p0 k ≤ kp0 k2 kA?0 - A0 k
Since we assume the loss is second-order Lipschitz, thus we have
kA? - A0k = kVλVhoLo(h?, λ) - VλVhoLo(h0, λ)k ≤ L2∣∣h? - h0k ≤ 应
We can also demonstrate kp0 k is bounded. According to the definition we have:
kpok ≤ k (Vh0Lo(h0,λ))-1 kk (VhoLo(h0,λ) + κ(h0 - h1)) k
(i)
≤ A-1 (L1 kh0 - h0 k2 + 2κφ)
(ii)
≤ A-1 (L1 + 2κφ)
For (i), we assume: 1) the Hessian matrix is invertible thus its norm is surely upper bounded by
some constant (denoted as A-1), 2) the first-order derivative is Lipschitz (bounded by L1), 3) the
predictor h is bounded. For (ii), we adopt the definition of h0 .
Therefore, the upper bound for Term (2) is formulated as:
(2)	≤ L2A-1(L1 + 2κφ) := O(κ)
The upper bound of term (3) We have:
kA0p0-A0pδ0k ≤ kA0kkp0 - pδ0k ≤ δAsup = O(δ)
Through the upper bound in (1)-(3), we finally have the error between the estimated and ground-truth
gradient:
δ
kgrad(λ) — grad (λ)k =O(Ke + E + δ)
□
C The Convergence Behavior
For the sake of completeness, we provide the convergence analysis of the proposed algorithm.
Proposition 2. We execute the implicit alignment algorithm (Algo. 1), obtaining a sequence of
λ1, . . . , λk,   Supposing the fair constraint κ is fixed. The optimization tolerances are summable:
k e2k ≤ +∞ and	k δk2 ≤ +∞, then λk is proved to be converged with
lim λk = λ?.
k→∞
If the stationary point λ? is also within the bounded norm, then we have:
grad(λ? ) = 0.
Proof. We denote the entire outer-level loss w.r.t. λ as L(λ), by the assumption the β-smooth loss
L.	Then at iteration k + 1 and k, we have:
β
L(λk+1) ≤ L(Xk)- grad(λk) (λk - λk+1) + 2 kλk+1 - λk k
δ	δT	β
=L(λk)	-	^grad(λk ) - grad	(λk)	+ grad	(λk )J	(λk	- λk+1) + / kλk+1	-	λkk
δT	δ	β	2
=L(λk)	-	(^grad(λk) - grad	(λk))	(λk	-	λk+1)	- grad (λk )(λk	- λk+1) +	^2 kλk+1	- λk k
Since we assume the representation is within the bounded norm, the projection onto the convex
set are non-expansive operators (Boyd et al., 2004). Then for any point p, q, we have kproj(p) -
proj(q)k2 ≤ (P - q)T (proj(p) - proj(q)). Thenwe set λk and λk+ι = λk - βgrad (λk),we have:
kλk - λk+1k2 ≤ -g (grad (λk))T(λk - λk+1)
β
15
Under review as a conference paper at ICLR 2022
Plugging into the results, we have:
δT	β
L(λk + 1) ≤ L(λk ) - ^grad(λk) - grad (λk )J (λk - λk +1)- £ kλk + 1 - λk ∣∣
δβ
≤ L(λk ) + Ilgrad(λk) - grad (λk )∣∣∣∣λk - λk+1k- £ kλk + 1 - λk k
Rearranging the inequality, we have:
βδ
2 kλk+1 - λk I -IIgrad(λk) - grad (λk )∣∣∣∣λk - λk + 1k + (L(λk + 1) - L(λk)) ≤ 0
Then we have:
kλk+1 - λk Il ≤ 万(kgrad(λk) - grad (λk )|| 十 I kgrad(λk) - grad (λk )∣2 - 2β (L(λk+1) - L(λk))
β
δ
By denoting Bk = Igrad(λk) - grad (λk)I and Ck = L(λk+1) - L(λk). Then we have:
kλk + 1 - λk 112 ≤ e(B2 + B2 - 2βCk + 2Bk qBk - 2βCk)
≤ β (Bk + Bk- 2βCk + Bk + B2 - 2βCk)
4δ
=不[kgrad(λk ) - grad (λk)∣∣2 - 2β (L(λk + l) - L(λk))]
β2
Taking sum over k, we have:
X kλk+1 - λk k2 ≤ ① X Ilgrad(λk) - grad5 (λk )∣∣2 -万(Jim L(λk+l) - L(λl))
k→∞
k=1	k=1
≤ ① X[(C + κ)2ek + δ2] -万(Jim L(λk+l) - L(λ1)) < +∞
β2	β k→∞
k
Since 1) the first term on the right side is finite, because the optimization tolerance is summable; 2)
the second term is also finite, because the loss is assumed to be bounded. Then the upper bound is
finite. In order to satisfy this condition, on the left side we should have:
lim λk+1 - λk = 0
k→∞
δδ
By adopting the definition λk+1 = Proj(λk - grad (λk)) and limk→∞ grad (λk) = grad(λk)
(Based on theorem 1, the limit of the optimization tolerance is zero), then we have:
λ? = proj(λ? - grad(λ?))
Where λ? = limk→+∞ λk+1 = limk→+∞ λk. Since the projection is on the bounded norm Lnorm
and λ? is within the bounded norm space, thus if λ? - grad(λ?) is within the bounded norm space,
we have:
grad(λ?) = 0
Else if λ? - grad(λ?) is outside the bounded norm space, then according to the definition, the pro-
jection of λ? 一 grad(λ?) is surely on the boundary of the LnOrm space, With ∣∣prcj(λ? 一 grad(λ?))∣ =
Lnorm. However, we have assumed the λ? is within the bounded norm space with Iλ? I < Lnorm,
Which leads to the contradiction. Based on these discussions, We finally have:
grad(λ?) = 0
□
D	Additional Experimental details and Results
D. 1 Additional Details
Toxic Comments We split the training, validation and testing set as 70%, 10% and 20%. We
adopt Adam optimizer With learning rate 10-3 and eps 10-3. The batch-size is set as 500 for each
subgroup and We use sampling With replacement to run the explicit algorithm With maximum epoch
100. The fair coefficient is generally set as K = 0.1 〜0.001. As for the inner-optimization step, the
iteration number is 20 and the iteration in running conjugate gradient approach is 10.
16
Under review as a conference paper at ICLR 2022
CelebA The training/validation/test set are around 82K, 18K and 18K. We also adopt the Adam
optimizer with learning rate on λ : 10-5 〜10-4 and h : 10-3. The batch-size is set as 64 for each
subgroup and we iterate the whole dataset as one epoch. The maximum running epoch is set as 20
and the iteration in running conjugate gradient approach is 10.
Law We split the training, validation and testing set as 70%, 10% and 20%. Then we adopt Adam
optimizer with learning rate 10-3 and eps 10-3. The batch-size is set as 500 for each subgroup and
we use sampling with replacement to run the implicit algorithm, with the maximum epoch 100. We
adopt the MSE loss in the regression. The fair coefficient is generally set as K = 0.1 〜 10-4. As
for the inner-optimization, the iteration number is 20 and the iteration in running conjugate gradient
is 10. In computing the sufficiency gap in the regression, we sample 33 points to compute the gap.
NLSY We split the training, validation and testing set as 70%, 10% and 20%. Then we adopt
Adam optimizer with learning rate 10-3 and eps 10-3. The batch-size is set as 500 for each
subgroup and we use sampling with replacement to run the implicit algorithm, with maximum
epoch 100. We adopt the MSE loss in the regression. The fair coefficient is generally set as
κ = 0.1 〜 10-4. As for the inner-optimization, the iteration number is 20 and the iteration in
running conjugate gradient is 10. In computing the sufficiency gap, we sample 33 points to compute
the sufficiency gap.
D.2 Additional Empirical results
87654321
Λμxθ-d LUoU əluh3≥4e-3c≤
10	20	30	40	50
Inner Optimization Step
Figure 9: Computational time between T -step explicit and implicit approach in CelebA. Specifically,
solver = 2 indicates the the conjugate gradient is executed 2 iterations. The results reveals the
benefits of implicit approach: avoiding the back-propagation through the inner-optimization path.
In contrast, the time complexity in explicit approach linearly increases with the inner-optimization
step, which is consistent with our analysis.
Computational complexity To show the efficiency of the implicit approach, we empirically com-
pare the computational complexity of the T -step explicit alignment and implicit approach (for dif-
ferent iterations of conjugate gradient solver.) The experimental results verified the efficiency of the
implicit approach, where a significant large inner-optimization step does not considerably increase
the computational time.
Gradient evolution We also visualize the gradient norm of the representation λ in the Toxic
dataset, shown in Fig. 10. The results verify the convergence behavior and the gradient norm fi-
nally tends to zero.
D.3 Discussion with non-deep learning baselines
In order to show the effectiveness of the proposed approach, we additionally compare the FAHT
(Zhang & Ntoutsi, 2019), a decision tree based fair classification approach. We evaluated the empir-
ical performance on Toxic comments dataset.
17
Under review as a conference paper at ICLR 2022
Figure 10: Gradient Norm evolution w.r.t. representation λ in Toxic comments dataset. We visualize
δ
the norm of grad (λ) at each training epoch, which suggests a convergence behavior and the gradient
finally tends to zero.
Table 5: Comparison with Fairness Aware Decision Tree
Method Accuracy (↑) ∆Sufc (1)
FAHT	0.596	0.397
Implicit	0.760	0.051
The implicit approach demonstrates the considerable better results, which may come from two as-
pects: (1) the Toxic task is a high-dimensional classification problem (x ∈ R748), where the deep
learning based approach is more effective in handling the high-dim dataset. (2) The FAHT aims to
realize the statistical parity (the independence rule), which is not compatible with the sufficiency.
According to the analysis of (Barocas et al., 2019), when the sensitive attribute (A) and label (Y)
are not independent (This has been justified by computing their Pearson Correlation coefficient), the
sufficiency and independence cannot both hold.
D.4 sufficiency Gap in regression
We visualize the sufficiency gap of NLSY dataset.
E Complementary technical details
We present complementary details that are related to the paper.
E.1 Conjugate Gradient Method
We present the Conjugate Gradient (CG) algorithm in Algo. 2 through autograd. In the conven-
tional CG algorithm with objective 2XT AX - bX, We need to estimate AX and compute its residual
and update X. Since in our problem setting, the A = VhoLo(h0, λ), then computing AX can be
realized through Hessian-vector product through autograd, denoted as function F in the paper.
i.e.,V2h0L0(h0,λ)X=F(x).
Below we provided a simple PyTorch code for realizing the Hessian Vector product.
1	import torch
2	def hessain_VectOr_product(loss,model,vector):
3	# loss: the defined loss
4	# model: the model in computing the Hessian
5	# vector: the required vector in computing Hessian-vector product
18
Under review as a conference paper at ICLR 2022
(a) ERM
(b) Fair Mix-up
(c) Implicit
Figure 11: Illustration of the sufficiency gap in NLSY dataset. The ERM and mix-up suffer the
high predictive sufficiency-gap, while the proposed implicit alignment can significantly mitigate the
sufficiency gap. In contrast, the probability calibration is not improved. This results also verifies the
inequivalence between the sufficiency gap and calibration gap (Liu et al., 2019).
Algorithm 2 Conjugate Gradient Method
Ensure: Function F that computes Hessian-vector product through autograd, initial value X0,
1:
2:
3:
4:
5:
6:
7:
bias vector B .
Computing Residual: r0 = B - F (X0)
Set p0 = r0
for inner_iterations k do
Computing αk J
T
rk rk
Pk F (Pk)
8
9
10
11
Xk+1 J Xk + αkpk
rk+1 J rk - αkF (pk)
If rk+1 is sufficiency small, then stop.
βk J rT+⅛+1
rk rk
pk+1 J rk+1 + βkpk
end for
return Xk+1
6	Partial_grad = torch.autograd.grad(loss, model_Parameters(),
Create_graph=True)
7	flat_grad = torch.cat([g.contiguous().view(-1) for g in Partial_grad
])
8	h = torch.sum(flat_grad * vector_to_oPtimize)
9	hvp = torch.autograd.grad(h, model.parameters())
10	return hvp
Listing 1: Simple demo in computing Hessian vector product
E.2 Calibration Gap in the regression
ɪʌ F τr ι 1	,	1 /act n∖	r∙	.	. 1	T ,	1	1	T , ∙1	∕~Cr ∖ c∙ .
Based on Kuleshov et al. (2018), we first compute the predicted cumulative distribution (Y0) of at
point t: D0 (Y0 ≤ t) = α, then we compute the corresponding ground truth cumulative distribution
(Y0) at point t. By changing t, We obtain several points on function Do(Y ≤ t∣Yθ ≤ t) = β. Then
the regression is probabilistic calibrated when α ≡ β. From this perspective, the zero calibration
gap can guarantee a zero sufficiency gap. But the inverse is not necessarily true, as our experimental
results suggest, a small sufficiency gap can lead to either small or large calibration gap. Thus it can
be quite promising to explore their inherent relations and trade-off in the fair regression.
19