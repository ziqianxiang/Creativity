Under review as a conference paper at ICLR 2022
ES -ENAS: Blackbox Optimization over Hy-
brid Spaces via Combinatorial and Continuous
Evolution
Anonymous authors
Paper under double-blind review
Ab stract
We consider the problem of efficient blackbox optimization over a large hybrid
search space, consisting of a mixture of a high dimensional continuous space
and a complex combinatorial space. Such examples arise commonly in evo-
lutionary computation, but also more recently, neuroevolution and architecture
search for Reinforcement Learning (RL) policies. Unfortunately however, previ-
ous mutation-based approaches suffer in high dimensional continuous spaces both
theoretically and practically. We thus instead propose ES-ENAS, a simple joint
optimization procedure by combining Evolutionary Strategies (ES) and combi-
natorial optimization techniques in a highly scalable and intuitive way, inspired
by the one-shot or supernet paradigm introduced in Efficient Neural Architecture
Search (ENAS). Through this relatively simple marriage between two different
lines of research, we are able to gain the best of both worlds, and empirically
demonstrate our approach by optimizing BBOB functions over hybrid spaces as
well as combinatorial neural network architectures via edge pruning and quanti-
zation on popular RL benchmarks. Due to the modularity of the algorithm, we
also are able incorporate a wide variety of popular techniques ranging from use
of different continuous and combinatorial optimizers, as well as constrained opti-
mization.
1	Introduction and Related Work
We consider the problem of optimizing an expensive function f : (M, Rd) → R, where M is
a combinatorial search space consisting of potentially multiple layers of categorical and discrete
variables, and Rd is a high dimensional continuous search space, consisting of potentially hundreds
to thousands of parameters. Such scenarios exist in reinforcement learning (RL), where m ∈ M
represents an architecture specification and θ ∈ Rd represents neural network weights, together to
form a policy πm,θ : S → A in which the goal is to maximize total reward in a given environment.
Other examples include flight optimization (Ahmad & Thomas, 2013), protein and chemical design
(Elton et al., 2019; Zhou et al., 2017; Yang et al., 2019), and program synthesis (Summers, 1977).
There have been a flurry of previous methods for approaching complex, combinatorial search spaces,
especially in the evolutionary algorithm domain, including the well-known NEAT (Stanley & Mi-
ikkulainen, 2002). Coincidentally, the neural architecture search (NAS) community has also adopted
a multitude of blackbox optimization methods for dealing with NAS search spaces, including pol-
icy gradients via Pointer Networks (Vinyals et al., 2015) and more recently Regularized Evolution
(Real et al., 2018). Such methods have been successfully applied to applications ranging from im-
age classification (Zoph & Le, 2017) to language modeling (So et al., 2019), and even algorithm
search/genetic programming (Real et al., 2020; Co-Reyes et al., 2021). Combinatorial algorithms
allow huge flexibility in the search space definition, which allows optimization over generic spaces
such as graphs, but many techniques rely on the notion of zeroth-order mutation, which can be
inappropriate in high dimensional continuous space due to large sample complexity (Nesterov &
Spokoiny, 2017).
On the other hand, there are also a completely separate set of algorithms for attacking high di-
mensional continuous spaces Rd. These include global optimization techniques including the
1
Under review as a conference paper at ICLR 2022
Cross-Entropy method (de Boer et al., 2005) and metaheuristic methods such as swarm algorithms
(Mavrovouniotis et al., 2017). More local-search based techniques include the class of methods
based on Evolution Strategies (ES) (Salimans et al., 2017), such as CMA-ES (Hansen et al., 2003;
Krause et al., 2016; Varelas et al., 2018) and Augmented Random Search (ARS) (Mania et al.,
2018b). ES has been shown to perform well for reinforcement learning policy optimization, es-
pecially in continuous control (Salimans et al., 2017) and robotics (Gao et al., 2020; Song et al.,
2020a). Even though such methods are also zeroth-order, they have been shown to scale better than
previously believed (Conti et al., 2018; Liu et al., 2019a; Rowland et al., 2018) on even millions of
parameters (Such et al., 2017) due to advancements in heuristics (Choromanski et al., 2019a) and
Monte Carlo gradient estimation techniques (Choromanski et al., 2019b; Yu et al., 2016). Unfortu-
nately, these analytical techniques are limited only to continuous spaces and at best, basic categorical
spaces via softmax reparameterization.
One may thus wonder whether it is possible to combine the two paradigms in an efficient manner.
For example, in NAS applications, it would be extremely wasteful to run an end-to-end ES-based
training loop for every architecture proposed by the combinatorial algorithm. At the same time, two
practical design choices we must strive towards are also simplicity and modularity, in which a user
may easily setup our method and arbitrarily swap in continuous algorithms like CMA-ES (Hansen
et al., 2003) or combinatorial algorithms like Policy Gradients (Vinyals et al., 2015) and Regularized
Evolution (Real et al., 2018), for specific scenarios. Generality is also an important aspect as well,
in which our method should be applicable to generic hybrid spaces. For instance, HyperNEAT
(Stanley et al., 2009) addresses the issue of high dimensional neural network weights by applying
NEAT to evolve a smaller hypernetwork (Ha et al., 2017) for weight generation, but such a solution
is domain specific and is not a general blackbox optimizer. Similarly restrictive, Weight Agnostic
Neural Networks (Gaier & Ha, 2019) do not train any continuous parameters and apply NEAT to
only the combinatorial spaces of network structures. Other works address blackbox hybrid spaces
via Bayesian Optimization (Deshwal et al., 2021) or Population Based Training (Parker-Holder et al.,
2021), but only in hyperparameter tuning settings whose search spaces are significantly smaller.
We are thus inspired by the joint optimization
method from Efficient NAS (ENAS) (Pham
et al., 2018), which introduces the notion of
weight sharing to build a maximal supernet
containing all possible weights θs where each
child model m only utilizes certain subcompo-
nents and their corresponding weights from this
supernet. Child models m are sampled from a
controller pφ, parameterized by some state φ.
The core idea is to perform separate updates
to θs and φ in order to respectively, improve
both neural network weights and architecture
selection at the same time. However, ENAS
was originally proposed in the setting of using
a GPU worker with autodifferentiation over θs
in mind for efficient NAS training.
In order to adopt ENAS’s joint optimiza-
tion into the fully blackbox (and potentially
non-differentiable) scenario involving hun-
dreds/thousands of CPU-only workers, our key
observation is that algorithms for combinatorial
and continuous spaces commonly use very sim-
Figure 1: Figure representing our aggregator-worker
pipeline when using a NAS controller with the vanilla
ES method, where the aggregator proposes models mi
in addition to a perturbed input θ + σgi, and the worker
the computes the objective f (mi , θ + σgi), which is
sent back to the aggregator. Both the training of the
weights θ and of the model-proposing controller pφ rely
on the number of worker samples to improve perfor-
mance.
ilar distributed workflows, involving usually a central aggregator and multiple parallel workers for
function evaluations. Thus we may modify the aggregator to suggest a joint tuple (m, θ), with
function evaluations f(m, θ) used by both the combinatorial and continuous algorithms for im-
provement. A visual representation of our algorithm can be found in Fig. 1.
We thus introduce the ES-ENAS algorithm, which is theoretically grounded, requires no extra com-
putational resources, and is empirically effective on large hybrid search spaces ranging from cate-
gorized BBOB functions and neural network topologies.
2
Under review as a conference paper at ICLR 2022
2	ES-ENAS Method
2.1	Preliminaries
We first define notation. Let M be a combinatorial search space in which m are drawn from, and θ ∈
Rd to be the continuous parameter. For scenarios such as NAS, one may define M’s representation
to be the superset of all possible child models m. Let φ represent the state of our combinatorial
algorithm. Assuming this algorithm allows randomization, let pφ be the current distribution over
M, where φ is to be optimized, and thus We may also sample m 〜p@. We interchangeably refer to
pφ as also a “controller” and θ as “weights” using NAS terminology.
2.2	Algorithm
We concisely summarize our ES-ENAS method
in Algorithm 1. For the sake of clarity, we use
vanilla ES as the continuous optimizer and its
derivation, but this can readily be changed to
e.g. CMA-ES via replacing I with a learn-
able covariance matrix C for the perturbation
g 〜N(0,I) in Eq. 1, along with the update
rule. Other ES variants (Wierstra et al., 2014;
Heidrich-Meisner & Igel, 2009; Krause, 2019)
can be swapped in similarly, although vanilla
ES suffices for common problems such as con-
tinuous control. Below, we provide ES-ENAS’s
derivation and conceptual simplicity of com-
bining the updates for φ and θ into a joint opti-
mization procedure.
The optimization problem we are interested in
is maxm∈M,θ∈Rd f(m, θ). In order to make
this problem tractable, consider instead, opti-
mization on the smoothed objective:
fσ (φ,θ) = Em〜pφ,g〜N(0,I) [f (m,θ + σg)]
(1)
The core trick is to use samples from m 〜pφ, g
in an unbiased manner.
Algorithm 1: Default ES-ENAS Algorithm,
with the few additional modifications to allow
ENAS from ES shown in blue.
Data: Initial weights θ, weight step size ηw,
precision parameter σ, number of
perturbations n, controller pφ .
while not done do
Sample i.i.d. vectors
gl,..., gn 〜N (0,I);
foreach gi do
Sample m+,m-〜pφ
v+ — f (m+ ,θ + σgi)
V— — f(m-,θ — σgi)
vi - 1 (V+ - V-)
Pφ ― {(m+,v+), (m-,v-)}
end
Update weights
θ J θ + "wσ1n Pn=ι vigi
Update controller pφ
end
〜 N(0, I) for updating both algorithm components
2.2.	1 Updating the Weights
The goal is to improve fσ(φ, θ) with respect to θ via one step of the gradient:
1
Vθ fσ(φ, θ) = 2σEm〜Pφ,g〜N(0,I) [(f (m,θ + σg) — f(m,θ — σg))g]	⑵
Note that by linearity, we may move the expectation Em〜pe inside into the two terms f (m, θ + σg)
and f(m, θ — σg), which implies that the gradient expression can be estimated with averaging
singleton samples of the form:
ɪ(f (m+,θ + σg) - f(m-,θ — σg))g	(3)
2σ
where m+, m- are i.i.d. samples from pφ, and g from N(0, I).
Thus we may sample multiple i.i.d. child models m+,m—…,m+,m- 〜pφ and also multiple
perturbations gι,…,gn 〜 N(0, I) and update weights θ with an approximate gradient update:
θ Jθ + ηw (XXX-m+," + σgi)-"m-,"-σgi)g)	(4)
n i=1	2σ
This update forms the “ES” portion of ES-ENAS. As a sanity check, we can see that using a constant
fixed m = m1+ = m1- = ... = mn+ = mn- reduces Eq. 4 to standard ES/ARS optimization.
3
Under review as a conference paper at ICLR 2022
2.2.2 Updating the Controller
For the “ENAS” portion of ES-ENAS (i.e. for optimizing the m ∈ M component), we update pφ
by simply reusing the objectives f(m, θ + σg) already computed for the weight updates, as they can
be viewed as unbiased estimations of Eg〜N(o,i)[f (m, θ + σg)] for a given m. Many methods can
be classified within the following approaches:
Policy Gradient Methods: The φ are differentiable parameters of a distribution pφ (usu-
ally a RNN-based controller), with the goal of optimizing the smoothed objective J(φ) =
Em〜Pφ,g〜N(0,i) [f (m; θ + g)], whose policy gradient Vφ J(φ) can be estimated by:
1n
V φJ (φ) = - ff(mi,θ + gi)Vφ log pφ(mi)	(5)
ni=1
where logpφ(m) is the log-probability of the controller for selecting m, and thus φ J φ +
ηpgVbφJ(φ) is updated with the use of the REINFORCE algorithm (Williams, 1992), or other vari-
ants such as PPO (Schulman et al., 2017) and TRPO (Schulman et al., 2015). When combined with
ES, a “simultaneous gradient” Vφ,θ fσ (φ, θ) is thus effectively being estimated in ES-ENAS.
Evolutionary Algorithms: In this setting, φ represents the algorithm state, which usually consists
of a population of inputs Q = {(m1, θ1), ..., (mn, θn)} with corresponding evaluations (slightly
abusing notation) f(Q) = {f(m1, θ1), ..., f(mn, θn)}. The algorithm performs a selection pro-
cedure (usually argmax) which selects an individual (mi , θi ) or potentially multiple individuals
T ⊆ Q, in order to perform respectively, mutation or crossover to “reproduce” and form a new
child instance (mnew, θnew). Some prominent examples include Regularized Evolution (Real et al.,
2018), NEAT (Stanley & Miikkulainen, 2002), and Hill-Climbing (Golovin et al., 2020; Song et al.,
2020c).
2.3 Convergence and Extensions
Convergence: This mechanism shows that updates to the controller pφ and updates to the weights
θ both rely on the samples f(m, θ + σg). The number of workers n, now serves the two purposes:
reducing the sample complexity of the controller pφ, as well as the variance of the estimated ES
gradient Vθfeσ. ES usually uses many more workers (on the order of 102) than what is normal in SL
(on the order of 100 to 101 of workers) which can be important for the controller pφ's performance,
as we will demonstrate in Subsection 3.3.2. However, in high dimensional continuous spaces,
ES/ARS enjoys significantly better convergence over argmax/mutation-based updates (native
to many combinatorial evolutionary algorithms), which supports why simply applying only a
single mutation-based algorithm over the entire space (M, Rd) is highly suboptimal and the need
for ES-ENAS. We experimentally verify this issue in Subsection 3.1 over a wide variety of combi-
natorial evolutionary algorithms. To also understand this issue theoretically, below is an instructive
theorem showing that the performance ratio between ES and hill-climbing (a simple example of
mutation + argmax selection) scales O(d), while compensating by increasing number of workers
requires B = O(2d), equivalent to brute forcing the entire search space (full proof in Appendix E):
Theorem 1. Let f (θ) be a α-StrongIy concave, β-smooth function over Rd, and let Δes(θ) be the
expected improvement ofan ES update, while ∆MUT (θ) be the expected improvement ofa batched
hill-climbing update, with both starting at θ and using B ≤ O(2d) parallel evaluations / workers
for fairness. Then assuming optimal hyperparameter tuning, ∆^ES((；)≥ O (K (VZd ι√lB)(B))
where K = β∕ɑ is the condition number.
Constrained Optimization: One extension is that the combinatorial/controller algorithm pφ's ob-
jective can be defined differently from the weights θ's objective. This is already subtly the case in
NAS, where the controller’s objective is the nondifferentiable validation accuracy of the model,
while the model weights are explicitly optimizing against the differentiable cross-entropy training
loss of the model. More significant differences between the two objectives involve cases such as
efficiency and runtime of the network, which have led to work in EfficientNets (Tan & Le, 2019)
via constrained optimization. We show this scenario can also be applied for the ES-ENAS setting in
Subsection 3.3.4.
4
Under review as a conference paper at ICLR 2022
3 Experiments
3.1	Curse of Continuous Dimensionality
We begin our experimental section by demonstrating the degradation of vanilla combinatorial evo-
lutionary algorithms over 19 different Black-Box Optimization Benchmarking (BBOB) functions
(Hansen et al., 2009), when the continuous space grows in size. We define our hybrid search space
as (M, Rdcon), where M consists of equally spaced gridpoints in Rdcat, which are then considered
unordered categories. Thus m ∈ M ⊂ Rdcat, which means an input (m, θ) can be used for a native
continuous function f originally operating on the input space Rdcat+dcon . For practical purposes,
we bound all parameters inside an interval [-L, L], and set the optimum value of f to be 0, with the
argmax argument also located when all parameters are zero valued.
For baselines, our combinatorial evolutionary algorithms include Regularized Evolution (Real et al.,
2018), NEAT (Stanley & Miikkulainen, 2002), Random Search, and Gradientless Descent/Batch
Hill-Climbing (Golovin et al., 2020; Song et al., 2020c). We also include PPO (Schulman et al.,
2017) as a policy gradient baseline, but only for categorical parameters as Pointer Networks do
not support continuous parameters and ES can already be considered a policy gradient/smoothed
gradient approach. To remain fair and consistent across all evolutionary algorithms (including ES),
we use the same mutation for continuous parameters θ ∈ Rdcon , which consists of applying a
random Gaussian perturbation θ + σmutg with a reasonable and tuned scaling σmut, as well as
applying a uniformly random chosen categorical parameter from the dcat parameters to resample.
All algorithms start at the same randomly sampled initial point. More hyperparameters can be found
in Appendix A.3 along with continuous optimizer comparisons (e.g. CMA-ES) in Appendix B.
(OrIOO)	(0r400)	(0r1600)
deωΛ4=eE-⅛0 6o^∣ PO Z=BEJON
lθ-ɪ
BBOB Benchmarks Across (dcat, dcon)
(100,100)
Number of Function Evaluations
Combinatorial Algorithms Only
Regularized Evolution	NEAT Gradientless Descent	PPO	Random
Figure 2: Plot of average normalized optimality gap across all BBOB functions (3 seeds each), when ranging
dcat and dcon . As dcon increases, over all combinatorial algorithms (different colors), each ES-ENAS
variant (solid line) begins to outperform its corresponding vanilla combinatorial algorithm (dashed line).
Note that if dcat = 0, all ES-ENAS variants are equivalent to Vanilla ES (single black solid line, 1st row).
ES-ENASVariants
3.2	Neural Network Policies
In order to benchmark our method over more nested combinatorial structures, we apply our method
to two combinatorial problems, Sparsification and Quantization, on standard Mujoco (Todorov
et al., 2012) environments from OpenAI Gym, which are well aligned with the use of ES. One ben-
efit specifically with ES when reducing parameter count is naturally improving sample complexity,
as speed of optimization is inversely related to the input dimension (Jamieson et al., 2012; Storn &
Price, 1997; Agarwal et al., 2011).
5
Under review as a conference paper at ICLR 2022
T
W1,5
W2,5
W3,5
W4,5
/
∖
W1,61 W1,7 ]W1,8 ]W1,9 1wi,1θ)
W2,6 [w2,7 ]w2,8 ]w2,9] W2,1θ]
W3,6 1w3,Tj W3,8 ^W39j WB/。)
W4,6 jw4,7 [w4,8 1w4,91w4,1θj
∖
/
θs
(a)	(b)
Figure 3: (a) Example of sparsifying a neural network setup, where solid edges are those learned by the
algorithm. (b) Example of quantization using a Toeplitz pattern (Choromanski et al., 2018), for the first layer
in Fig. 3a. Entries in each of the diagonals are colored the same, thus sharing the same weight value. The
trainable weights θs = w(1) , ..., w(9) are denoted at the very bottom in the vectorized form with 9 entries,
which effectively encodes the larger T with 24 entries.
Such problems also have a long history, with sparisification methods such as (Rumelhart, 1987;
Chauvin, 1989; Mozer & Smolensky, 1989) from the 1980’s, Optimal Brain Damage (Cun et al.,
1990), regularization (Louizos et al., 2018), magnitude-based weight pruning methods (Han et al.,
2015; See et al., 2016; Narang et al., 2017), sparse network learning (Gomez et al., 2019; Lenc et al.,
2019), and the recent Lottery Ticket Hypothesis (Frankle & Carbin, 2019). Meanwhile, quantization
has been explored with Huffman coding (Han et al., 2016), randomized quantization (Chen et al.,
2015), and hashing mechanisms (Eban et al., 2020).
3.2.1	Results
We can view a feedforward neural network as
a standard directed acyclic graph (DAG), with
a set of vertices containing values {v1, ..., vk},
and a set of edges {(i, j) | 1 ≤ i ≤ j ≤ k}
where each edge (i, j) contains a weight wi,j ,
as shown in Figures 3a and 3b. The goal
of sparsification is to reduce the number of
edges while maintaining high environment re-
ward, while the goal of quantization is to par-
tition the edges via colorings, which allows
same-colored edges to use the same weight.
These scenarios possess very large combinato-
rial policy search spaces (calculated as |M| >
1068, comparable to 1049 from NASBench-101
(Ying et al., 2019)) that will stress test our ES-
ENAS algorithm and are also relevant to mo-
bile robotics (Gage, 2002). Given the results in
Subsection 3.1 and since this is a NAS-based
problem, for ES-ENAS we use the two most
domain-specific controllers, Regularized Evo-
lution and PPO (Policy Gradient) and take the
best result in each scenario. Specific details and
search space size calculations can be found in
Appendix A.4.
Env.	Arch.	Reward	# weights	compression	# bits
Striker	Quantization	-247	23	95%	8198
	Edge Pruning	-130	64	93%	3072
	Masked	-967	25	95%	8262
	Toeplitz	-129	110	88%	4832
	Circulant	-120	82	90%	3936
	Unstructured	-117	1230	0%	40672
HalfCheetah	Quantization	4894	17	94%	6571
	Edge Pruning	4016	64	98%	3072
	Masked	4806	40	92%	8250
	Toeplitz	2525	103	85%	4608
	Circulant	1728	82	88%	3936
	Unstructured	3614	943	0%	31488
Hopper	Quantization	3220	11	92%	3960
	Edge Pruning	3349	64	84%	3072
	Masked	2196	17	91%	4726
	Toeplitz	2749	94	78%	4320
	Circulant	2680	82	80%	3936
	Unstructured	2691	574	0%	19680
Walker2d	Quantization	2026	17	94%	6571
	Edge Pruning	3813	64	90%	3072
	Masked	1781	19	94%	6635
	Toeplitz	1	103	85%	4608
	Circulant	3	82	88%	3936
	Unstructured	2230	943	0%	31488
Table 1: Comparison of the best policies from six dis-
tinct classes of RL networks: Quantization (ours), Edge
Pruning (ours), Masked, Toeplitz, Circulant, and Un-
structured networks trained with standard ES algorithm
(Salimans et al., 2017). All results are for feedforward
nets with one hidden layer. Best two metrics for each
environment are in bold, while significantly low re-
wards are in red.
As we have already demonstrated comparisons to blackbox optimization baselines in Subsection
3.1, we instead focus our comparison to domain-specific baselines for the neural network. These
include a DARTS-like (Liu et al., 2019b) softmax masking method (Lenc et al., 2019), where a
trainable boolean matrix mask is applied over the weights for edge pruning. We also include fixed
quantization patterns such as Toeplitz and Circulant (particular class of Toeplitz) matrices, which are
6
Under review as a conference paper at ICLR 2022
strong mathematically grounded baselines from (Choromanski et al., 2018). In all cases we use the
same hyper-parameters, and train until convergence for three random seeds. For masking, we report
the best achieved reward with > 90% of the network pruned, making the final policy comparable
in size to the quantization and edge-pruning networks. Specific details can be found in Appendices
C.1 and A.4.
For each class of policies, we compare various metrics, such as the number of weight parameters
used, total parameter count compression with respect to unstructured networks, and total number of
bits for encoding float values (since quantization and masking methods require extra bits to encode
the partitioning via dictionaries).
As we can see in Table 1, both sparsification and quantization can be learned from scratch via op-
timization using ES-ENAS, which achieves competitive or better rewards against other baselines.
This is especially true against hand-designed (Toeplitz/Circulant) patterns which significantly fail
at Walker2d, as well as other optimization-based reparameterizations, such as softmax masking,
which underperforms on the majority of environments.
The full set of numerical results over all of the mentioned methods can be found in Appendix C,
which includes quantization (Appendix C.2), edge pruning and nonlinearity search (Appendix C.3),
as well as plots for baseline methods (Fig. 9).
3.3	Neural Network Policy Ablations
In the rest of the experimental section, we provide ablations studies on the properties and extensions
of our ES-ENAS method. Because of the nested combinatorial structure of the neural network space
(rather than the flat space of BBOB functions), many desired behaviors for the algorithm are no
longer obvious nor visualizable at first sight. These behaviors include as raised questions, which
also highlight certain similarities and differences from regular NAS in supervised learning:
1.	How do controllers compare in performance?
2.	How does the number of workers affect the quality of optimization?
3.	Does the algorithm converge properly to a fixed architecture m?
4.	Does constrained optimization also work in ES-ENAS?
3.3.1	Controller Comparisons
As shown in Subsection 3.1, Regularized Evolution (Reg-Evo) was the highest performing controller
when used in ES-ENAS. However, this is not always the case, as mutation-based optimization may
be prone to being stuck in local optima whereas policy gradient methods (PG) such as PPO can
allow better exploration.
HaIfCheetah, EdgePrune + H32
3500
3000
2500
2000
1500
IOOO
500
0
5000
4000
3000
2000
1000
0
Walker2d, EdgePrune + H32
Figure 4: Comparisons across different environments when using different controllers, on the edge pruning
and quantization tasks, when using a linear layer (L) or hidden layer of size 32 (H32).
7
Under review as a conference paper at ICLR 2022
We thus compare different ES-ENAS variants, when using Reg-Evo, PG (PPO), and random search
(for sanity checking), on the edge pruning task in Fig. 4. As shown, while Reg-Evo consistently con-
verges faster than PG at first, PG eventually may outperform Reg-Evo in asymptotic performance.
Previously on NASBENCH-like benchmarks, Reg-Evo consistently outperforms PG in both sample
complexity and asymptotic performance (Real et al., 2018), and thus our results on ES-ENAS are
surprising, potentially due to the hybrid optimization of ES-ENAS.
Random search has been shown in supervised learning to be a surprisingly strong baseline (Li &
Talwalkar, 2019), with the ability to produce even ≥ 80-90 % accuracy (Pham et al., 2018; Real
et al., 2018), showing that NAS-based optimization produces most gains ultimately be at the tail
end; e.g. at the 95% accuracies. In the ES-ENAS setting, this is shown to occur for easier RL
environments such as Striker (Fig. 4) and Reacher (shown in Appendices C.2, C.3). However, for
the majority of RL environments, a random search controller is unable to train at all, which also
makes this regime different from supervised learning.
3.3.2	Controller Sample Complexity
We further investigate the effect of the number of objective values per batch on the controller by
randomly selecting only a subset of the objectives f(m, θ) for the controller pφ to use, but maintain
the original number of workers for updating θs via ES to maintain weight estimation quality to
prevent confounding results. We found that this sample reduction can reduce the performance of
both controllers for various tasks, especially the PG controller. Thus, we find the use of the already
present ES workers highly crucial for the controller’s quality of architecture search in this setting.
5000
P 4000
⅛ 3000
I 2000
H 1000
Figure 5: Regular ES-ENAS experiments with 150 full controller objective value usage plotted in darker colors.
Experiments with lower controller sample usage (10 random samples, similar to the number of simultaneously
training models in (Tan et al., 2018b)) plotted in corresponding lighter colors.
3.3.3	Visualizing and Verifying Convergence
Iteration 0	Iteration 10	Iteration 100
8
Figure 6: Edge pruning convergence over time, with samples aggregated over 3 seeds from PG runs on Swim-
mer. EaCh edge is colored according to a spectrum, with its color value equal to 2 |p — 11 where P is the
edge frequency. We see that initially, each edge has uniform (P = 2) probability of being selected, but as the
controller trains, the samples converge toward a single pruning.
We also graphically plot aggregate statistics over the controller samples to confirm ES-ENAS’s con-
vergence. We choose the smallest environment, Swimmer, which conveniently works particularly
8
Under review as a conference paper at ICLR 2022
well with linear policies (Mania et al., 2018b), to reduce visual complexity and avoid permuta-
tion invariances. We also use a boolean mask space over all possible edges (search space size
|M| = 2lSl×lAl = 28×2). We remarkably observe that for all 3 independently seeded runs, PG
converges toward a specific “local maximum” architecture, demonstrated in Fig. 6 with the final
architecture presented in Appendix D, which also depicts a similar case for Reg-Evo. This suggests
that there may be a few “natural architectures” optimal to the state representation.
3.3.4	Constrained Optimization
(Tan & Le, 2019; Tan et al., 2018b) introduce the powerful notion of constrained optimization,
where the controller may optimize multiple objectives (ex: efficiency) towards a Pareto optimal
solution (Deb, 2005). We apply (Tan et al., 2018b) and modify the controller’s objective to be a
hybrid combination f (m, θ) (lEj) of both the total reward f (m, θ) and the compression ratio
∣EEm∣ where |Em| is the number of edges in model m and |Et| is a target number, with the search
space expressed as boolean mask mappings (i, j) → {0, 1} over all possible edges. For simplicity,
|E |
We use the naive setting in (Tan et al., 2018b) and set ω = -1 if 1^ > 1, while ω = 0 otherwise,
which strongly penalizes the controller if it proposes a model m whose edge number |Em | breaks
the threshold |ET |.
Hopper + H32 (PG)
-200
-100
p」①er
4000-
p」EM①工
p-①or
2000-
500
Walker2d + H32 (PG)
2000
2000
1000
Hopper + H32 (Reg-Evo)
500	1000	1500	2000
Walker2d + H32 (Reg-Evo)
1500	2000
0- -
O

Figure 7: Environment reward plotted alongside the average number of edges used for proposed models.
Black horizontal line corresponds to the target |Et | = 64.
In Fig. 7, we see that the controller eventually reduces the number of edges below the target thresh-
old set at |ET | = 64, while still maintaining competitive training reward, demonstrating that ES-
ENAS is also capable of constrained optimization techniques, potentially useful for explicitly de-
signing efficient CPU-constrained robot policies (Unitree, 2017; Gao et al., 2020; Tan et al., 2018a).
4	Conclusion & Future Work
We presented a scalable and flexible algorithm, ES-ENAS, for performing combinatorial optimiza-
tion over hybrid spaces and efficient architecture search for ES-trained neural networks such as
reinforcement learning policies. ES-ENAS is efficient, simple, modular, and general-purpose, and
can utilize many techniques from both the continuous and combinatorial evolutionary literature.
We believe that this work can be useful for several downstream applications, such as designing
new architectures for mobile robotics, including compact cells for vision-based RL policies to im-
prove generalization (Cobbe et al., 2020; 2019; Song et al., 2020b) and RNNs for meta-learning and
memory (Bakker, 2001; Najarro & Risi, 2020). Outside of architectures, our method contributes
to general blackbox optimization over large and complex spaces, useful for a variety of scenarios
involving evolutionary search, such as genetic programming (Co-Reyes et al., 2021), circuit design
(Ali et al., 2004), and compiler optimization (Cooper et al., 1999).
9
Under review as a conference paper at ICLR 2022
5	Statements
Ethics Statement: The only relevant potential concerns in our work occur normally with general
NAS methods, which can sacrifice model interpretability in order to achieve higher objectives. For
the field of RL specifically, this may warrant more attention in AI safety when used for real world
robotic pipelines. However, for our specific work, due to the smaller overall size of the policy
networks, our results might actually be more interpretable - for instance, edge pruning may lead to
discovery and disuse of MDP state values which do not contribute to obtaining high rewards, thereby
improving a practitioner’s understanding of the problem. Furthermore, as with any NAS research,
the initial phase of discovery and experimentation may contribute to carbon emissions due to the
computational costs of extensive tuning. However, this is usually a means to an end, such as an
efficient search algorithm, which this paper proposes with no extra hardware costs.
Reproducibility Statement: In Appendix A, we have discussed the explicit API used, algorithm
hyperparameters, benchmarks used, as well as other baseline details. Both ES/ARS and different
combinatorial optimizers are readily available across the internet (e.g. Github), as they are very
popular baselines. Similarly, BBOB functions and Mujoco tasks are also readily available, and
thus we have only used publicly available benchmarks. Since our algorithm is relatively simple to
assemble given preexisting pipelines, we believe that our approach should require minimal effort to
reproduce.
References
Alekh Agarwal, Dean P. Foster, Daniel J. Hsu, Sham M. Kakade, and Alexander
Rakhlin. Stochastic convex optimization with bandit feedback. In Advances in Neu-
ral Information Processing Systems 24: 25th Annual Conference on Neural Informa-
tion Processing Systems 2011. Proceedings of a meeting held 12-14 December 2011,
Granada, Spain, pp. 1035-1043, 2011. URL http://papers.nips.cc/paper/
4475-stochastic-convex-optimization-with-bandit-feedback.
Sohail Ahmad and K. Thomas. Flight optimization system ( flops ) hybrid electric aircraft design
capability. 2013.
B. Ali, A. E. A. Almaini, and Tatiana Kalganova. Evolutionary algorithms and theirs use in the
design of sequential logic circuits. Genet. Program. Evolvable Mach., 5(1):11-29, 2004. doi:
10.1023/B:GENP.0000017009.11392.e2. URL https://doi.org/10.1023/B:GENP.
0000017009.11392.e2.
Bram Bakker. Reinforcement learning with long short-term memory. In Advances in Neural Infor-
mation Processing Systems 14: Annual Conference on Neural Information Processing Systems
2001, NeurIPS 2001, 2001.
Yves Chauvin. A back-propagation algorithm with optimal use of hidden units. In David S. Touret-
zky (ed.), Advances in Neural Information Processing Systems 1, pp. 519-526, San Francisco,
CA, USA, 1989. Morgan Kaufmann Publishers Inc.
Wenlin Chen, James Wilson, Stephen Tyree, Kilian Weinberger, and Yixin Chen. Compressing
neural networks with the hashing trick. In International Conference on Machine Learning, pp.
2285-2294, 2015.
Krzysztof Choromanski, Mark Rowland, Vikas Sindhwani, Richard E. Turner, and Adrian Weller.
Structured evolution with compact architectures for scalable policy optimization. In Proceed-
ings of the 35th International Conference on Machine Learning, ICML 2018, Stockholmsmassan,
Stockholm, Sweden, July 10-15, 2018, pp. 969-977, 2018.
Krzysztof Choromanski, Aldo Pacchiano, Jack Parker-Holder, Yunhao Tang, Deepali Jain, Yuxiang
Yang, Atil Iscen, Jasmine Hsu, and Vikas Sindhwani. Provably robust blackbox optimization
for reinforcement learning. In 3rd Annual Conference on Robot Learning, CoRL 2019, Osaka,
Japan, October 30 - November 1, 2019, Proceedings, pp. 683-696, 2019a. URL http://
proceedings.mlr.press/v100/choromanski20a.html.
10
Under review as a conference paper at ICLR 2022
Krzysztof Choromanski, Aldo Pacchiano, Jack Parker-Holder, Yunhao Tang, and Vikas Sindhwani.
From complexity to simplicity: Adaptive es-active subspaces for blackbox optimization. In Ad-
vances in Neural Information Processing Systems 32: Annual Conference on Neural Information
Processing Systems 2019, NeurIPS 2019, December 8-14, 2019, Vancouver, BC, Canada, pp.
10299-10309, 2019b.
John D. Co-Reyes, Yingjie Miao, Daiyi Peng, Esteban Real, Sergey Levine, Quoc V. Le, Honglak
Lee, and Aleksandra Faust. Evolving reinforcement learning algorithms. CoRR, abs/2101.03958,
2021. URL https://arxiv.org/abs/2101.03958.
Karl Cobbe, Oleg Klimov, Christopher Hesse, Taehoon Kim, and John Schulman. Quantifying
generalization in reinforcement learning. In Proceedings of the 36th International Conference on
Machine Learning, ICML 2019, 9-15 June 2019, Long Beach, California, USA, pp. 1282-1289,
2019. URL http://proceedings.mlr.press/v97/cobbe19a.html.
Karl Cobbe, Christopher Hesse, Jacob Hilton, and John Schulman. Leveraging procedural genera-
tion to benchmark reinforcement learning. In Proceedings of the 37th International Conference
on Machine Learning, ICML 2020, 13-18 July 2020, Virtual Event, pp. 2048-2056, 2020. URL
http://proceedings.mlr.press/v119/cobbe20a.html.
Edoardo Conti, Vashisht Madhavan, Felipe Petroski Such, Joel Lehman, Kenneth O. Stan-
ley, and Jeff Clune. Improving exploration in evolution strategies for deep reinforcement
learning via a population of novelty-seeking agents. In Samy Bengio, Hanna M. Wallach,
Hugo Larochelle, Kristen Grauman, Nicolo Cesa-Bianchi, and Roman Garnett (eds.), Ad-
vances in Neural Information Processing Systems 31: Annual Conference on Neural Informa-
tion Processing Systems 2018, NeurIPS 2018, December 3-8, 2018, Montreal, Canada, pp.
5032-5043, 2018. URL https://proceedings.neurips.cc/paper/2018/hash/
b1301141feffabac455e1f90a7de2054- Abstract.html.
Keith D. Cooper, Philip J. Schielke, and Devika Subramanian. Optimizing for reduced code
space using genetic algorithms. In Y. Annie Liu and Reinhard Wilhelm (eds.), Proceed-
ings of the ACM SIGPLAN 1999 Workshop on Languages, Compilers, and Tools for Embed-
ded Systems (LCTES’99), Atlanta, Georgia, USA, May 5, 1999, pp. 1-9. ACM, 1999. doi:
10.1145/314403.314414. URL https://doi.org/10.1145/314403.314414.
Yann Le Cun, John S. Denker, and Sara A. Solla. Optimal brain damage. In David S. Touretzky
(ed.), Advances in Neural Information Processing Systems 2, San Francisco, CA, USA, 1990.
Morgan Kaufmann Publishers Inc.
Pieter-Tjerk de Boer, Dirk P. Kroese, Shie Mannor, and Reuven Y. Rubinstein. A tutorial on the
cross-entropy method. Ann. Oper. Res., 134(1):19-67, 2005. doi: 10.1007/s10479-005-5724-z.
URL https://doi.org/10.1007/s10479-005-5724-z.
Kalyanmoy Deb. Multi-Objective Optimization. Springer US, 2005. ISBN 978-0-387-28356-2. doi:
10.1007/0-387-28356-0-10.
Aryan Deshwal, Syrine Belakaria, and Janardhan Rao Doppa. Bayesian optimization over hybrid
spaces. In Marina Meila and Tong Zhang (eds.), Proceedings of the 38th International Conference
on Machine Learning, ICML 2021, 18-24 July 2021, Virtual Event, volume 139 of Proceedings
of Machine Learning Research, pp. 2632-2643. PMLR, 2021. URL http://proceedings.
mlr.press/v139/deshwal21a.html.
Elad Eban, Yair Movshovitz-Attias, Hao Wu, Mark Sandler, Andrew Poon, Yerlan Idelbayev,
and Miguel A. Carreira-Perpinan. Structured multi-hashing for model compression. In 2020
IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR 2020, Seattle, WA,
USA, June 13-19, 2020, pp. 11900-11909, 2020. doi: 10.1109/CVPR42600.2020.01192.
Daniel C. Elton, Zois Boukouvalas, Mark D. Fuge, and Peter W. Chung. Deep learning for molecular
generation and optimization - a review of the state of the art. CoRR, abs/1903.04388, 2019. URL
http://arxiv.org/abs/1903.04388.
11
Under review as a conference paper at ICLR 2022
Jonathan Frankle and Michael Carbin. The lottery ticket hypothesis: Finding sparse, trainable neu-
ral networks. In 7th International Conference on Learning Representations, ICLR 2019, New
Orleans, LA, USA, May 6-9, 2019. OpenReview.net, 2019. URL https://openreview.
net/forum?id=rJl-b3RcF7.
Douglas W. Gage (ed.). Mobile Robots XVII, Philadelphia, PA, USA, October 25, 2004, vol-
ume 5609 of SPIE Proceedings, 2002. SPIE. ISBN 978-0-8194-5562-8. URL http://
proceedings.spiedigitallibrary.org/volume.aspx?volume=5609.
Adam Gaier and David Ha. Weight agnostic neural networks. In Advances in
Neural Information Processing Systems 32: Annual Conference on Neural Infor-
mation Processing Systems 2019, NeurIPS 2019, December 8-14, 2019, Vancouver,
BC, Canada, pp. 5365-5379, 2019. URL http://papers.nips.cc/paper/
8777- weight- agnostic- neural- networks.
Wenbo Gao, Laura Graesser, Krzysztof Choromanski, Xingyou Song, Nevena Lazic, Pannag San-
keti, Vikas Sindhwani, and Navdeep Jaitly. Robotic table tennis with model-free reinforcement
learning. 2020. URL https://arxiv.org/abs/2003.14398.
Daniel Golovin, John Karro, Greg Kochanski, Chansoo Lee, Xingyou Song, and Qiuyi Zhang. Gra-
dientless descent: High-dimensional zeroth-order optimization. In 8th International Conference
on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020. OpenRe-
view.net, 2020. URL https://openreview.net/forum?id=Skep6TVYDB.
Aidan N. Gomez, Ivan Zhang, Kevin Swersky, Yarin Gal, and Geoffrey E. Hinton. Learning sparse
networks using targeted dropout. ArXiv, abs/1905.13678, 2019.
David Ha, Andrew M. Dai, and Quoc V. Le. Hypernetworks. In 5th International Confer-
ence on Learning Representations, ICLR 2017, Toulon, France, April 24-26, 2017, Conference
Track Proceedings. OpenReview.net, 2017. URL https://openreview.net/forum?id=
rkpACe1lx.
Song Han, Jeff Pool, John Tran, and William Dally. Learning both weights and connections for
efficient neural network. In C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett
(eds.), Advances in Neural Information Processing Systems 28, pp. 1135-1143. Curran Asso-
ciates, Inc., 2015.
Song Han, Huizi Mao, and William J. Dally. Deep compression: Compressing deep neural network
with pruning, trained quantization and Huffman coding. In International Conference on Learning
Representations, 2016.
NikolaUs Hansen, Sibylle D. Muller, and Petros Koumoutsakos. Reducing the time complexity of
the derandomized evolution strategy with covariance matrix adaptation (cma-es). Evol. Comput.,
11(1):1-18, March 2003.
Nikolaus Hansen, Steffen Finck, Raymond Ros, and Anne Auger. Real-Parameter Black-Box Op-
timization Benchmarking 2009: Noiseless Functions Definitions. Research Report RR-6829,
INRIA, 2009. URL https://hal.inria.fr/inria-00362633.
Verena Heidrich-Meisner and Christian Igel. Neuroevolution strategies for episodic reinforcement
learning. J. Algorithms, 64(4):152-168, 2009. doi: 10.1016/j.jalgor.2009.04.002. URL https:
//doi.org/10.1016/j.jalgor.2009.04.002.
Kevin G. Jamieson, Robert D. Nowak, and Benjamin Recht. Query complexity of
derivative-free optimization. In Advances in Neural Information Processing Sys-
tems 25: 26th Annual Conference on Neural Information Processing Systems
2012. Proceedings of a meeting held December 3-6, 2012, Lake Tahoe, Nevada,
United States, pp. 2681-2689, 2012. URL http://papers.nips.cc/paper/
4509-query-complexity-of-derivative- free-optimization.
Oswin Krause. Large-scale noise-resilient evolution-strategies. In Proceedings of the Genetic and
Evolutionary Computation Conference, GECCO ’19, pp. 682-690, New York, NY, USA, 2019.
Association for Computing Machinery. ISBN 9781450361118. doi: 10.1145/3321707.3321724.
URL https://doi.org/10.1145/3321707.3321724.
12
Under review as a conference paper at ICLR 2022
OsWin Krause, Dldac RodrIgUez Arbones, and Christian IgeL CMA-ES with optimal covariance
update and storage complexity. In Daniel D. Lee, Masashi Sugiyama, Ulrike von Luxburg,
Isabelle Guyon, and Roman Garnett (eds.), Advances in Neural Information Processing Sys-
tems 29: Annual Conference on Neural Information Processing Systems 2016, December 5-10,
2016, Barcelona, Spain, pp. 370-378, 2016. URL https://proceedings.neurips.cc/
paper/2016/hash/289dff07669d7a23de0ef88d2f7129e7-Abstract.html.
Karel Lenc, Erich Elsen, Tom Schaul, and Karen Simonyan. Non-differentiable supervised learning
with evolution strategies and hybrid methods. arXiv, abs/1906.03139, 2019.
Liam Li and Ameet Talwalkar. Random search and reproducibility for neural architecture search.
In Amir Globerson and Ricardo Silva (eds.), Proceedings of the Thirty-Fifth Conference on Un-
certainty in Artificial Intelligence, UAI 2019, Tel Aviv, Israel, July 22-25, 2019, volume 115 of
Proceedings of Machine Learning Research, pp. 367-377. AUAI Press, 2019.
Guoqing Liu, Li Zhao, Feidiao Yang, Jiang Bian, Tao Qin, Nenghai Yu, and Tie-Yan Liu. Trust
region evolution strategies. In The Thirty-Third AAAI Conference on Artificial Intelligence, AAAI
2019, The Thirty-First Innovative Applications of Artificial Intelligence Conference, IAAI 2019,
The Ninth AAAI Symposium on Educational Advances in Artificial Intelligence, EAAI 2019,
Honolulu, Hawaii, USA, January 27 - February 1, 2019, pp. 4352-4359. AAAI Press, 2019a.
doi: 10.1609/aaai.v33i01.33014352. URL https://doi.org/10.1609/aaai.v33i01.
33014352.
Hanxiao Liu, Karen Simonyan, and Yiming Yang. DARTS: differentiable architecture search. In
7th International Conference on Learning Representations, ICLR 2019, New Orleans, LA, USA,
May 6-9, 2019, 2019b. URL https://openreview.net/forum?id=S1eYHoC5FX.
Christos Louizos, Max Welling, and Diederik P. Kingma. Learning sparse neural networks through
l0 regularization. In International Conference on Learning Representations, 2018.
Horia Mania, Aurelia Guy, and Benjamin Recht. Simple random search provides a competitive
approach to reinforcement learning. CoRR, abs/1803.07055, 2018a.
Horia Mania, Aurelia Guy, and Benjamin Recht. Simple random search of static linear policies is
competitive for reinforcement learning. In Advances in Neural Information Processing Systems,
pp. 1800-1809, 2018b.
Michalis Mavrovouniotis, Changhe Li, and Shengxiang Yang. A survey of swarm intelligence for
dynamic optimization: Algorithms and applications. Swarm Evol. Comput., 33:1-17, 2017. doi:
10.1016/j.swevo.2016.12.005. URL https://doi.org/10.1016/j.swevo.2016.12.
005.
Michael C. Mozer and Paul Smolensky. Skeletonization: A technique for trimming the fat from a
network via relevance assessment. In David S. Touretzky (ed.), Advances in Neural Information
Processing Systems 1, pp. 107-115, San Francisco, CA, USA, 1989. Morgan Kaufmann Publish-
ers Inc.
Elias Najarro and Sebastian Risi. Meta-learning through hebbian plasticity in random net-
works. In Hugo Larochelle, Marc’Aurelio Ranzato, Raia Hadsell, Maria-Florina Balcan,
and Hsuan-Tien Lin (eds.), Advances in Neural Information Processing Systems 33: Annual
Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12,
2020, virtual, 2020. URL https://proceedings.neurips.cc/paper/2020/hash/
ee23e7ad9b473ad072d57aaa9b2a5222- Abstract.html.
Sharan Narang, Gregory Diamos, Shubho Sengupta, and Erich Elsen. Exploring sparsity in recurrent
neural networks. In International Conference on Learning Representations, 2017.
Yurii E. Nesterov and Vladimir G. Spokoiny. Random gradient-free minimization of convex func-
tions. Found. Comput. Math., 17(2):527-566, 2017. doi: 10.1007/s10208-015-9296-2. URL
https://doi.org/10.1007/s10208-015-9296-2.
Jack Parker-Holder, Vu Nguyen, Shaan Desai, and Stephen J. Roberts. Tuning mixed input hyper-
parameters on the fly for efficient population based autorl. CoRR, abs/2106.15883, 2021. URL
https://arxiv.org/abs/2106.15883.
13
Under review as a conference paper at ICLR 2022
Daiyi Peng, Xuanyi Dong, Esteban Real, Mingxing Tan, Yifeng Lu, Gabriel Bender, Hanxiao
Liu, Adam Kraft, Chen Liang, and Quoc Le. Pyglove: Symbolic programming for auto-
mated machine learning. In Advances in Neural Information Processing Systems 33: Annual
Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12,
2020, virtual, 2020. URL https://proceedings.neurips.cc/paper/2020/hash/
012a91467f210472fab4e11359bbfef6- Abstract.html.
Hieu Pham, Melody Y. Guan, Barret Zoph, Quoc V. Le, and Jeff Dean. Efficient neural architecture
search via parameter sharing. In Proceedings of the 35th International Conference on Machine
Learning, ICML2018, Stockholmsmassan, Stockholm, Sweden, July 10-15,2018,pp. 4092-4101,
2018.
Esteban Real, Alok Aggarwal, Yanping Huang, and Quoc V. Le. Regularized evolution for image
classifier architecture search. CoRR, abs/1802.01548, 2018. URL http://arxiv.org/abs/
1802.01548.
Esteban Real, Chen Liang, David R. So, and Quoc V. Le. Automl-zero: Evolving machine learning
algorithms from scratch. CoRR, abs/2003.03384, 2020. URL https://arxiv.org/abs/
2003.03384.
Mark Rowland, Krzysztof Choromanski, Francois Chalus, Aldo Pacchiano, Tamas Sarlos,
Richard E. Turner, and Adrian Weller. Geometrically coupled monte carlo sampling. In Samy
Bengio, Hanna M. Wallach, Hugo Larochelle, Kristen Grauman, Nicolo Cesa-Bianchi, and Ro-
man Garnett (eds.), Advances in Neural Information Processing Systems 31: Annual Confer-
ence on Neural Information Processing Systems 2018, NeurIPS 2018, December 3-8, 2018,
Montreal, Canada, pp. 195-205, 2018. URL https://proceedings.neurips.cc/
paper/2018/hash/b3e3e393c77e35a4a3f3cbd1e429b5dc-Abstract.html.
D. E Rumelhart. Personal communication. Princeton, 1987.
Tim Salimans, Jonathan Ho, Xi Chen, Szymon Sidor, and Ilya Sutskever. Evolution strategies as a
scalable alternative to reinforcement learning. arXiv, abs/1703.03864, 2017.
John Schulman, Sergey Levine, Pieter Abbeel, Michael Jordan, and Philipp Moritz. Trust region
policy optimization. In International Conference on Machine Learning (ICML), 2015.
John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy
optimization algorithms. arXiv preprint arXiv:1707.06347, 2017.
Abigail See, Minh-Thang Luong, and Christopher D. Manning. Compression of neural machine
translation models via pruning. In Proceedings of The 20th SIGNLL Conference on Computa-
tional Natural Language Learning, Berlin, Germany, August 2016. Association for Computa-
tional Linguistics.
David R. So, Quoc V. Le, and Chen Liang. The evolved transformer. In Proceedings of the 36th In-
ternational Conference on Machine Learning, ICML 2019, 9-15 June 2019, Long Beach, Califor-
nia, USA, pp. 5877-5886, 2019. URL http://proceedings.mlr.press/v97/so19a.
html.
Xingyou Song, Wenbo Gao, Yuxiang Yang, Krzysztof Choromanski, Aldo Pacchiano, and Yun-
hao Tang. ES-MAML: simple hessian-free meta learning. In 8th International Conference on
Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020, 2020a. URL
https://openreview.net/forum?id=S1exA2NtDB.
Xingyou Song, Yiding Jiang, Stephen Tu, Yilun Du, and Behnam Neyshabur. Observational over-
fitting in reinforcement learning. In 8th International Conference on Learning Representations,
ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020, 2020b. URL https://openreview.
net/forum?id=HJli2hNKDH.
Xingyou Song, Yuxiang Yang, Krzysztof Choromanski, Ken Caluwaerts, Wenbo Gao, Chelsea Finn,
and Jie Tan. Rapidly adaptable legged robots via evolutionary meta-learning. In IEEE/RSJ Inter-
national Conference on Intelligent Robots and Systems, IROS 2020, Las Vegas, NV, USA, October
24, 2020 - January 24, 2021, pp. 3769-3776. IEEE, 2020c. doi: 10.1109/IROS45743.2020.
9341571. URL https://doi.org/10.1109/IROS45743.2020.9341571.
14
Under review as a conference paper at ICLR 2022
Kenneth O. Stanley and Risto Miikkulainen. Evolving neural network through augmenting topolo-
gies. Evolutionary Computation,10(2):99-127, 2002.
Kenneth O. Stanley, David B. D’Ambrosio, and Jason Gauci. A hypercube-based encoding for
evolving large-scale neural networks. Artif. Life, 15(2):185-212, 2009. doi: 10.1162/artl.2009.
15.2.15202. URL https://doi.org/10.1162/artl.2009.15.2.15202.
Rainer Storn and Kenneth V. Price. Differential evolution - A simple and efficient heuristic for global
optimization over continuous spaces. J. Glob. Optim., 11(4):341-359, 1997. doi: 10.1023/A:
1008202821328. URL https://doi.org/10.1023/A:1008202821328.
Felipe Petroski Such, Vashisht Madhavan, Edoardo Conti, Joel Lehman, Kenneth O. Stanley, and
Jeff Clune. Deep neuroevolution: Genetic algorithms are a competitive alternative for training
deep neural networks for reinforcement learning. CoRR, abs/1712.06567, 2017. URL http:
//arxiv.org/abs/1712.06567.
Phillip D. Summers. A methodology for lisp program construction from examples. J. ACM, 24
(1):161-175, January 1977. ISSN 0004-5411. doi: 10.1145/321992.322002. URL https:
//doi.org/10.1145/321992.322002.
Jie Tan, Tingnan Zhang, Erwin Coumans, Atil Iscen, Yunfei Bai, Danijar Hafner, Steven Bo-
hez, and Vincent Vanhoucke. Sim-to-real: Learning agile locomotion for quadruped robots.
In Hadas Kress-Gazit, Siddhartha S. Srinivasa, Tom Howard, and Nikolay Atanasov (eds.),
Robotics: Science and Systems XIV, Carnegie Mellon University, Pittsburgh, Pennsylvania,
USA, June 26-30, 2018, 2018a. doi: 10.15607/RSS.2018.XIV.010. URL http://www.
roboticsproceedings.org/rss14/p10.html.
Mingxing Tan and Quoc V. Le. Efficientnet: Rethinking model scaling for convolutional neural
networks. In Proceedings of the 36th International Conference on Machine Learning, ICML
2019, 9-15 June 2019, Long Beach, California, USA, pp. 6105-6114, 2019. URL http://
proceedings.mlr.press/v97/tan19a.html.
Mingxing Tan, Bo Chen, Ruoming Pang, Vijay Vasudevan, and Quoc V. Le. Mnasnet: Platform-
aware neural architecture search for mobile. CoRR, abs/1807.11626, 2018b. URL http://
arxiv.org/abs/1807.11626.
Emanuel Todorov, Tom Erez, and Yuval Tassa. Mujoco: A physics engine for model-based control.
In 2012 IEEE/RSJ International Conference on Intelligent Robots and Systems, IROS 2012, Vil-
amoura, Algarve, Portugal, October 7-12, 2012, pp. 5026-5033, 2012. doi: 10.1109/IROS.2012.
6386109. URL https://doi.org/10.1109/IROS.2012.6386109.
Unitree. Laikago website, 2017. URL http://www.unitree.cc/e/action/ShowInfo.
php?classid=6&amp;id=1#.
Konstantinos Varelas, Anne Auger, Dimo Brockhoff, Nikolaus Hansen, Ouassim Ait ElHara, Yann
Semet, Rami Kassab, and Frederic Barbaresco. A comparative study of large-scale variants of
CMA-ES. In Anne Auger, Carlos M. Fonseca, NUno LoUrengo, Penousal Machado, Luis Pa-
quete, and L. Darrell Whitley (eds.), Parallel Problem Solving from Nature - PPSN XV - 15th
International Conference, Coimbra, Portugal, September 8-12, 2018, Proceedings, Part I, vol-
ume 11101 of Lecture Notes in Computer Science, pp. 3-15. Springer, 2018. doi: 10.1007/
978-3-319-99253-2∖.L URL https://doi.org/10.1007/978-3-319-99253-2_1.
Oriol Vinyals, Meire Fortunato, and Navdeep Jaitly. Pointer networks. In Advances in Neural In-
formation Processing Systems 28: Annual Conference on Neural Information Processing Systems
2015, December 7-12, 2015, Montreal, Quebec, Canada, pp. 2692-2700, 2015.
Daan Wierstra, Tom SChaUL Tobias Glasmachers, Yi Sun, Jan Peters, and JUrgen Schmidhuber.
Natural evolution strategies. Journal of Machine Learning Research, 15:949-980, 2014.
Ronald J. Williams. Simple statistical gradient-following algorithms for connectionist reinforcement
learning. Machine Learning, 8:229-256, 1992. doi: 10.1007/BF00992696.
Kevin K. Yang, Zachary Wu, and Frances H. Arnold. Machine learning-guided directed evolution
for protein engineering, 2019.
15
Under review as a conference paper at ICLR 2022
Chris Ying, Aaron Klein, Eric Christiansen, Esteban Real, Kevin Murphy, and Frank Hutter. Nas-
bench-101: Towards reproducible neural architecture search. In Proceedings of the 36th Interna-
tional Conference on Machine Learning, ICML 2019, 9-15 June 2019, Long Beach, California,
USA, pp. 7105-7114, 2019. URL http://Proceedings .mlr.press∕v97∕ying19a.
html.
Felix X Yu, Ananda Theertha Suresh, Krzysztof M Choromanski, Daniel N Holtmann-Rice, and
Sanjiv Kumar. Orthogonal random features. In Advances in Neural Information Processing
Systems (NIPS). 2016.
Zhenpeng Zhou, Xiaocheng Li, and Richard N. Zare. Optimizing chemical reactions with
deep reinforcement learning. ACS Central Science, 3(12):1337-1344, 2017. doi: 10.
1021/acscentsci.7b00492. URL https://doi.org/10.1021/acscentsci.7b00492.
PMID: 29296675.
Barret Zoph and Quoc V. Le. Neural architecture search with reinforcement learning. In 5th In-
ternational Conference on Learning Representations, ICLR 2017, Toulon, France, April 24-26,
2017, Conference Track Proceedings, 2017.
16
Under review as a conference paper at ICLR 2022
Appendix
A Implementation Details
A.1 API
We use the standardized NAS API PyGlove (Peng et al., 2020), where search spaces are usually con-
structed via combinations of primitives such as “pyglove.oneof” and “pyglove.manyof”
operations, which respectively choose one item, or a combination of multiple objects from a con-
tainer. These primitives can be combined in a nested conditional structure via “pyglove.List” or
“pyglove.Dict”. The search space can then be sent to an algorithm, which proposes child model
instances m programmically represented via Python dictionaries and strings. These are sent over a
distributed communication channel to a worker alongside the perturbation θ+σg, and then material-
ized later by the worker into an actual object such as a neural network. Although the controller needs
to output hundreds of model suggestions, it can be parallelized to run quickly by multithreading (for
Reg-Evo) or by simply using a GPU (for policy gradient).
A.2 Algorithms
A.2.1 Combinatorial Algorithms
The mutator used for all evolutionary algorithms (Regularized Evolution, NEAT, Gradientless De-
scent/Batch Hill-Climbing) consists of a “Uniform” mutator for the neural network setting, where a
parameter in a (potentially nested) search space is chosen uniformly at random, with its new value
also mutated uniformly over all possible choices. For continuous settings, see Appendix A.3 below.
Regularized Evolution: We set the tournament size to be √n where n is the number of Work-
ers/population size, as this works best as a guideline (Real et al., 2018).
NEAT: We use the original algorithm specification of NEAT (Stanley & Miikkulainen, 2002) with-
out additional modifications. The compatibility distance function was implemented appropriately
for DNAs (i.e. “genomes”) in PyGlove, and a gridsweep was used to find the best coefficients.
Gradientless Descent/Batch Hill-Climbing: We use the same mutator throughout the optimiza-
tion process, similar to (Song et al., 2020c) to reduce algorithm complexity, as the step size annealing
schedule found in (Golovin et al., 2020) is specific to convex objectives only.
Policy Gradient: We use a gradient update batch size of 64 to the Pointer Network, while using
PPO as the policy gradient algorithm, with its default (recommended) hyperparameters from (Peng
et al., 2020). These include a softmax temperature of 1.0, 100 hidden state size with 1 layer for the
RNN, importance weight clipping of 0.2, and 10 update steps per weight update, with more values
found in (Vinyals et al., 2015). We grid searched PPO’s learning rate across {1 × 10-4, 5 × 10-4, 1 ×
10-3 , 5 × 10-3 } and found 5 × 10-4 was the best.
A.2.2 Continuous Algorithms
ARS/ES: We always use reward normalization and state normalization (for RL benchmarks) from
(Mania et al., 2018a). For BBOB functions, we use ηw = 0.5 while σ = 0.5, along with 64
Gaussian directions per batch in an ES iteration, with 8 used for evaluation. For RL benchmarks, we
use ηw = 0.01 and σ = 0.1, along with 75 Gaussian directions, with 50 more used for evaluation.
CMA-ES: For BBOB functions, we use σ = 0.5 and ηw = 0.5, similar to ARS/ES.
A.3 BBOB Benchmarks
Our BBOB functions consisted of the 19 classical functions from (Hansen et al., 2009):
{Sphere, Rastrigin, BuecheRastrigin, LinearSlope, AttractiveSector, StepEllipsoidal, Rosenbrock-
17
Under review as a conference paper at ICLR 2022
Rotated, Discus, BentCigar, SharpRidge, DifferentPowers, Weierstrass, SchaffersF7, Schaf-
fersF7IllConditioned, GriewankRosenbrock, Schwefel, Katsuura, Lunacek, Gallagher101}.
The each parameter in the raw continuous input space is bounded within [-L, L] where L = 5. For
discretization + categorization into a grid, we use a granularity of 1 between consecutive points, i.e.
a categorical a parameter is allowed to select within {-L, -L + 1, ..., 0, ..., L - 1, L}. Note that
each BBOB function is set to have its global optimum at the zero-point, and thus our hybrid spaces
contain the global optimum.
Because each BBOB function may have a completely different scaling (e.g. for a fixed dimension,
the average output for Sphere may be within the order of 102 but the average output for BentCigar
may be within 1010), we thus normalize the output of each function when reporting results. The
normalized valuation of a BBOB function f is calculated by dividing the raw value by the maximum
absolute value obtained by random search.
Since for the ES component we use a step size of ηw = 0.5 and precision parameter of σ = 0.5, we
thus use for evolutionary mutations, a Gaussian perturbation scaling σmut of 0.07, which equalizes
the average norms between the update directions on θ, which are: ηw Vθfσ and σmutg.
A.4 RL + Neural Network Setting
In order to allow combinatorial flexibility, our neural network consists of vertices/values V =
{v1, ..., vk}, where the initial block of |S| values {v1, ..., v|S| } corresponds to the environment state,
and the last block of |A| values {vk-|A|+1, ..., vk} corresponds to the action output values. Directed
edges E ⊆ Emax = {ei,j = (i, j) | 1 ≤ i < j ≤ k, |S| < j} are constructed with corresponding
weights W = {wi,j | (i,j) ∈ E}, and nonlinearities G = {σ∣s∣+ι,…，σk} for the non-state ver-
tices. Thus a forward propagation consists of for-looping in order j ∈ {|S|+1, ..., k} and computing
output values vj = σj P(i,j)∈E viwi,j .
By default, unless specified, we use Tanh non-linearities with 32 units for each hidden layer.
Edge pruning: We group all possible edges (i, j) into a set in the neural network, and select a fixed
number of edges from this set. We can also further search across potentially different nonlinearities,
e.g. fi ∈ {tanh, sigmoid, sin, ...} similarly to Weight Agnostic Neural Networks (Gaier & Ha, 2019).
In terms of API, this search space can be described as pyglove.manyof(Emax,|E|) along with
Pyglove .oneof(σi, G). The search space is of size (lE∣Eaxl) or 2lEmaxl when using a fixed or
variable size |E | respectively.
We collect all possible edges from a normal neural network into a pool Emax and set |E| = 64 as the
number of distinct choices, passed to the pyglove.manyof. Similar to quantization, this choice
is based on the value max(|S|, H) or max(|A|, H), where H = 32 is the number of hidden units,
which is linear in proportion to respectively, the maximum number of weights |S| ∙ H or |A| ∙ H.
Since a hidden layer neural network has two weight matrices due to the hidden layer connecting to
both the state and actions, we thus have ideally a maximum of 32 + 32 = 64 edges.
For nonlinearity search, we use the same functions found in (Gaier & Ha, 2019). These are: {Tanh,
ReLU, Exp, Identity, Sin, Sigmoid, Absolute Value, Cosine, Square, Reciprocal, Step Function.}
Quantization: We assign to each edge (i, j) one color of many colors c ∈ C = {1, ..., |C|},
denoting the partition group the edge is assigned to, which defines the value Wij J W(C). This
is shown pictorially in Figs. 3a and 3b. This can also programmically be done by concatenating
primitives pyglove.oneof(ei,j ,C ) over all edges ei,j ∈ Emax. The search space is of size
|C||E|.
The number of partitions (or “colors”) is set to max(|S|, |A|). This is both in order to ensure a
linear number of trainable parameters compared to the quadratic number for unstructured networks,
as well as allow sufficient parameterization to deal with the entire state/action values.
18
Under review as a conference paper at ICLR 2022
A.4. 1 Environment
For all environments, we set the horizon T = 1000. We also use the reward without alive bonuses
for weight training as commonly used (Mania et al., 2018b) to avoid local maximum behaviors (such
as an agent simply standing still to collect a total of 1000 reward), but report the final score as the
real reward with the alive bonus.
A.4.2 Baseline Details
We consider Unstructured, Toeplitz, Circulant and a masking mechanism (Choromanski et al., 2018;
Lenc et al., 2019). We introduce their details below. Notice that all baseline networks share the same
general (1-hidden layer, Tanh nonlinearity) architecture from A.4. This impplies that we only have
two weight matrices Wi ∈ RlSl×h, W2 ∈ Rh×lAl and two bias vectors bi ∈ Rh,b2 ∈ R1A1, where
|S |, |A| are dimensions of state/action spaces. These networks differ in how they parameterize the
weight matrices. We have:
Unstructured: A fully-connected layer with unstructured weight matrix W ∈ Ra×b has a total of
ab independent parameters.
Toeplitz: A toeplitz weight matrix W ∈ Ra×b has a total of a+b- 1 independent parameters. This
architecture has been shown to be effective in generating good performance on benchmark tasks yet
compressing parameters (Choromanski et al., 2018).
Circulant: A circulant weight matrix W ∈ Ra×b is defined for square matrices a = b. We
generalize this definition by considering a square matrix of size n × n where n = max{a, b} and
then do a proper truncation. This produces n independent parameters.
Masking: One additional technique for reducing the number of independent parameters in a
weight matrix is to mask out redundant parameters (Lenc et al., 2019). This slightly differs from the
other aforementioned architectures since these other architectures allow for parameter sharing while
the masking mechanism carries out pruning. To be concrete, we consider a fully-connected matrix
W ∈ Ra×b with ab independent parameters. We also setup another mask weight Γ ∈ Ra×b. Then
the mask is generated via
Γ0 = Softmax(Γ∕α)
where softmax is applied elementwise and α is a constant. We set α = 0.01 so that the softmax
is effectively a thresolding function wich outputs near binary masks. We then treat the entire con-
catenated parameter θ = [W, Γ] as trainable parameters and optimize both using ES methods. Note
that this softmax method can also be seen as an instance of the continuous relaxation method from
DARTS (LiU et al., 2019b). At convergence, the effective number of parameter is ab∙λ where λ is the
proportion of Γ0 components that are non-zero. During optimization, we implement a simple heuris-
tics that encourage sparse network: while maximizing the true environment return f (θ) = PtT=i rt ,
we also maximize the ratio 1 - λ of mask entries that are zero. The ultimate ES objective is:
f 0(θ) = β ∙ f (θ) + (1 - β) ∙ (1 - λ), where β ∈ [0,1] is a combination coefficient which we anneal
as training progresses. We also properly normalize f(θ) and (1 - λ) before the linear combination
to ensure that the procedure is not sensitive to reward scaling.
19
Under review as a conference paper at ICLR 2022
B Extended BBOB Experimental Results
B.1 CMA-ES Comparison
deωAl=BE-IdO 601 p。Z=BEJON
lθo (°-100)
BBOB Benchmarks Across (dcat, dcon)
n	(0,400)
(0r1600)
ιo-1
O 30K	60K	90K	120 K	O 30K	60K	90K	120K O 30K	60K	90K	120 K
Number of Function Evaluations
ES-ENAS (Reg-Evo) CMA-ENAS (Reg-Evo)
Figure 8:	Comparison when regular ES/ARS is used as the continuous algorithm in ES-ENAS, vs when CMA-
ES is used as the continuous algorithm (which we name “CMA-ENAS”). We use the exact same setting as
Figure 2 in the main body of the paper. We use Regularized Evolution (Reg-Evo) as the default combinatorial
algorithm due its strong performance found from Figure 2. We find that ES-ENAS usually converges faster
initially, while CMA-ENAS achieves a better asymptotic performance. This is aligned with the results (in the
first row) when comparing vanilla ES with vanilla CMA-ES. For generally faster convergence to a sufficient
threshold however, ES/ES-ENAS usually suffices.
20
Under review as a conference paper at ICLR 2022
C Extended Neural Network Experimental Results
As standard in RL, we take the mean and standard deviation of the final rewards across 3 seeds for
every setting. “L”, “H” and “H, H” stand for: linear policy, policy with one hidden layer, and policy
with two such hidden layers respectively.
C.1 Baseline Method Comparisons
In terms of the masking baseline, while (Lenc et al., 2019) fixes the sparsity of the mask, we instead
initialize the sparsity at 50% and increasingly reward smaller networks (measured by the size of
the mask |m|) during optimization to show the effect of pruning. Using this approach on several
Open AI Gym tasks, we demonstrate that masking mechanism is capable of producing compact
effective policies up to a high level of pruning. At the same time, we show significant decrease of
performance at the 80-90% compression level, quantifying accurately its limits for RL tasks (see:
Fig. 9).
Figure 9:	The results from training both a mask m and weights θ of a neural network with two hidden layers.
‘Usage’ stands for number of edges used after filtering defined by the mask. At the beginning, the mask is
initialized such that |m| is equal to 50% of the total number of parameters in the network.
C.2 Quantization
Env.	Dim.	Arch.	Partitions	Policy Gradient	Regularized Evolution	Random Search
Swimmer	(8,2)	L	8	366 ± 0	296 ± 31	5±1
Reacher	(11,2)	L	11	-10±4	-157 ± 62	-135 ± 10
Hopper	(11,3)	L	11	2097 ± 788	1650 ± 320	16±0
HalfCheetah	(17,6)	L	17	2958 ± 73	3477 ± 964	129 ± 183
Walker2d	(17,6)	L	17	326 ± 86	2079 ± 1085	8±0
Pusher	(23,7)	L	23	-68±2	-198 ± 76	-503±4
Striker	(23,7)	L	23	-247 ± 11	-376 ± 149	-590 ± 18)
Thrower	(23,7)	L	23	-819 ± 8	-1555 ± 427	-12490 ± 708)
						
Env.	Dim.	Arch.	Partitions	Policy Gradient	Regularized Evolution	Random Search
Swimmer	(8,2)	H	8	361 ± 4	362 ± 1	15±0
Reacher	(11,2)	H	11	-6±0	-23±11	-157±2
Hopper	(11,3)	H	11	3288 ± 119	2834 ± 75	95 ± 2
HalfCheetah	(17,6)	H	17	4258 ± 1034	4894 ± 110	-41 ± 5
Walker2d	(17,6)	H	17	1684 ± 1008	2026 ± 46	-5 ± 1
Pusher	(23,7)	H	23	-225 ± 131	-350 ± 236	-1049 ± 40
Striker	(23,7)	H	23	-992 ± 2	-466 ± 238	-1009 ± 1
Thrower	(23,7)	H	23	-1873 ± 690	-818 ± 363	-12847 ± 172
Table 2: Results via quantization across PG, Reg-Evo, and random search controllers. The number of partitions
is always set to be max(|S |, |A|).
21
Under review as a conference paper at ICLR 2022
C.3 Edge Pruning and Nonlinearity Search
C.3.1 Edge Pruning
Env.	Dim.	Arch.	Policy Gradient	Regularized Evolution	Random Search
Swimmer	(8,2)	H	105 ± 116	343 ± 2	21 ± 1
Reacher	(11,2)	H	-16±5	-52 ± 5	-160±2
Hopper	(11,3)	H	3349 ± 206	2589 ± 106	66 ± 0
HalfCheetah	(17,6)	H	2372 ± 820	4016 ± 726	-156 ± 22
Walker2d	(17,6)	H	3813 ± 128	1847 ± 710	0±2
Pusher	(23,7)	H	-133 ± 31	-156 ± 17	-503 ± 15
Striker	(23,7)	H	-178 ± 54	-130 ± 16	-464 ± 13
Thrower	(23,7)	H	-532 ± 29	-1107 ± 158	-7797 ± 112
Table 3: Results via quantization across PG, Reg-Evo, and random search controllers. The number of edges is
always set to be 64 in total, or (32, 32) across the two weight matrices when using a single hidden layer.
C.3.2 Nonlinearity Search
Intriguingly, we found that appending the extra nonlinearity selection into the edge-pruning search
space improved performance across HalfCheetah and Swimmer, but not across all environments (Fig
). However, lack of total improvement is consistent with the results found with WANNs (Gaier & Ha,
2019), which also showed that trained WANNs’ performances matched with vanilla policies. From
these two observations, we hypothesize that perhaps nonlinearity choice for simple MLP policies
trained via ES are not quite so important to performance as other components, but more ablation
studies must be conducted. Furthermore, for quantization policies, we see that hidden layer policies
near-universally outperform linear policies, even when using the same number of distinct weights.
Env.	Dim.	Arch.	Policy Gradient	Regularized Evolution	Random Search
Swimmer	(8,2)	H	247 ± 110	359 ± 5	11±3
Hopper	(11,3)	H	2270 ± 1464	2834 ± 120	57 ± 7
HalfCheetah	(17,6)	H	3028 ± 469	5436 ± 978	-268 ± 29
Walker2d	(17,6)	H	1057 ± 413	2006 ± 248	0±1
Table 4: Results using the same setup as Table 3, but allowing nonlinearity search.
Env.	Dim.	(PG, Reg-Evo) Reward	Method
HalfCheetah	(17,6)	(2958, 3477) → (4258, 4894)	Quantization (L → H)
Hopper	(11,3)	(2097, 1650) → (3288, 2834)	Quantization (L → H)
HalfCheetah	(17,6)	(2372, 4016) → (3028, 5436)	Edge Pruning (H) → (+ Nonlinearity Search)
Swimmer	(8,2)	(105, 343) → (247, 359)	Edge Pruning (H) → (+ Nonlinearity Search)
Table 5: Rewards for selected environments and methods, each result averaged over 3 seeds. Arrow denotes
modification or addition (+).
22
Under review as a conference paper at ICLR 2022
D Network Visualizations
D.1 Quantization
Figure 10: (a): Partitioning of edges into distinct weight classes obtained for the linear policy for HalfCheetah
environment from OpenAI Gym. (b): Partitioning of edges for a policy with one hidden layer encoded by two
matrices. State and action dimensionalities are: s = 17 and a = 6 respectively and hidden layer for the
architecture from (b) is of size 41. Thus the size of the matrices are: 17 × 6 for the linear policy from (a) and:
17 × 41, 41 × 6 for the nonlinear one from (b).
D.2 Edge Pruning
Figure 11: (Left): Final architectures that PG and Reg-Evo converged to on Swimmer with a linear (L) policy,
as specified in Subsection 3.3.3. Note that the controller does not select all edges even if it is allowed in the
boolean search space, but also ignores some state values. (Right): Convergence result for Reg-Evo, similar to
Fig. 6 in Subsection 3.3.3.
E Theory
In this section, for convenience we use the variable x, which may be assigned x = θ in the main
section of the paper. We present the ES/ARS and Mutation-based updates, which are respectively
(assuming equal batch size B of parallel workers):
x+ = X + ηVe (x) where Vfσ (x) = X f(x + °gi) ； f X-gi gi	(6)
i=1	2σ
X+ = arg max{f (X), f (X + σmutg1), ..., f(X + σmutgB)}	(7)
We assume that f is α-strongly concave and β-smooth for α, β ≥ 0 if for all X, y :
hvf (X),y - χi - βky - χk2 ≤ Ay)- f(X) ≤ hvf (X),y - χi - αky - χk2	⑻
22
E.1 ES/ARS Guarantees
We note that the β-smoothness also carries from the original function f(X) into the smoothed func-
tion fσ(x) = Eg〜N(0,I)[f (x + σg)], and thus by simply combining the β-smoothness from Eq. 8
23
Under review as a conference paper at ICLR 2022
with the definition of x+ from Eq. 6, we have
βη
η(Vfσ (x), V fσ (x)i- β2L kVfσ (x)k2 ≤ fσ (x+) — fσ (x)	(9)
Taking the expectation with respect to the sampling of g1, ..., gB/2 and noting that Vfσ (x) is an
unbiased estimation of Vfσ (x):
ηkVfσ (x)k2 - βη2 (kVfσ (x)k2 + MSE(V fσ (x))) ≤ ∆σ,ES (x)	(10)
1	ʌ	/	∖ ττn	「7 / -U ∖T T /	∖ ∙	.1	-I	.	.
where ∆σ,Es(x) = Egι,…,gB∕2〜N(0,i)[fσ(χ+)] 一 fσ(x) is the expected one-step improvement on
the smoothed function fσ .
Using (Nesterov & Spokoiny, 2017), Theorem 4 leads to estimator variance MSE(Vfσ (x)) =
O(β2d3σ2∕B) while Theorem 1 leads to |f (x) — fσ(x)| ≤ O(σ2βd), and finally Lemma 4 leads
to kVfσ(x)k22 - kVf (x)k22 ≤ O(β2d2σ2). Note that all of these terms are negligible compared to
kVf (x)k22 as σ is small and B can be e.g. O(d), and thus we may substitute these terms with single
variables for the reader’s convenience. Thus, this leads to:
Go + η(kVf(x)k2 + Gι) — βη2(kVf(x)k2 + G2) ≤ Δes(x)	(11)
where the negligible terms are: G0 = -O(σ2βd), G1 = O(β2d2σ2 ), G2 = O(β2d2σ2 +
β2d3σ2∕B) and Δes(x) = Egι,…,gB∕2〜N(o,ι)[f(x+)] ― f(x) is the expected one-step improve-
ment on the original f .
We may set η = 1 的；(：)以:?1 ≈ 1 to maximize the quadratic (in terms of η) in the LHS, which
leads to
O (kVf(x)k2 ) = G + 1 (kVf (x)k2 + Gi )2
I β 厂 0 +2β (IlVf(x)k2 + G2)
≤ ∆ES(x)
(12)
E.2 Mutation Guarantees
We have from plugging in y = x+ in Eq. 7 and 8 along with taking the expectation from sampling
g1, ..., gB and taking the argmax gmax (which can potentially also be zero if there is no improve-
ment),
∆MUT (x) ≤
max ( 0, Egι,…,gB〜N(0,I) KVf(X), σmutgmaxi] 一 Egι,…,gB〜N(0,I)
l^α∣∣L	.<Γ
[2 kσmutgmax
I22
(13)
where Δmut(x) = Egι,...,gB〜N(0,i) [f (χ+)] 一 f (x) is the expected improvement for the mutation.
We focus on upper bounding the non-zero term in the maximum in the RHS. Note that choosing
gmax ∈ {g1, ..., gB} from the argmax process only optimizes f(x + σmutg) and not any other
objective, and thus:
Egi,…,gB~N(0,I) KVf(X), σmutgmaxi]
≤ σmutEg1 ,...,gB 〜N (0,ι) ImaXhVf(x), gii	(14)
≤ σmut∣Vf(x)k2p2log(B)
24
Under review as a conference paper at ICLR 2022
where the bottom inequality is a well known fact about sums of Gaussians. For the other term, we
have:
Egl,…,gB ~N (0,I )
[2 kσmutgmaxk2] ≥
2
ασmut
2
Egl,…,gB ~N (0,I)
mgiinkgik22
(15)
To bound the RHS’s right side, we may use a well-known concentration inequality for Lipschitz
functions with respect to Gaussian sampling, i.e. Prg〜N(o,i) [|M(g) - μ∣ > λ] ≤ 2e-λ2/2 where
M(∙) is any LiPSchitz function and μ = Eg，〜N(o,i) [M(g0)]. We may define M(g) = ∣∣gk2 which
leads to μ = √d, and then use a union bound over B IID samples to obtain:
Prgι,...,gB 〜N (0,IJkgik2 ≥ √d - λ, ∀gi] ≥ Prgi,…,gB 〜N (0,I) [lkgik2 -√dl≤ λ, ∀gi]
≥ (1 - B ∙ 2e-λ2/2)
(16)
This finally implies that from Eq. 15,
Egl,…,gB 〜N (0,I) min kgik2
≥ max (0, √d - λ) ∙ Prgi,…,gB〜N(o,i)[kg∣∣2 ≥ √d - λ
≥ max(0, √d — λ) • (1 — B ∙ 2e-λ2/2)
(17)
To set the probability-like term (1 - 2Be-λ2/2) in the RHS to a constant C, We let λ
2olog( ι2¾) = O (PlOg(B))
, which finally leads to
Egi,…,gB〜N(0,I) mgin kgik2 ≥ max (θ,O (√d - Plog(B)))	(18)
Thus replacing the two terms in Eq. 13,
∆MUT (x) ≤ max
0, σmutkVf (x)k2 √2 log(B) - ασm Ut max
(o,O (√d - Pl0g(B)))2
(19)
If B = Ω(2d), then there is no quadratic in terms of σmut, and thus Qmu can be arbitrarily large (or
maximized at the search space’s bounds) to essentially brute force the entire search space.
Otherwise, hyperparameter tuning for Qmut leads to maximizing the quadratic in the RHS, which
leads to setting σmut=f-√HB∣ ,leading to
∆MUT (x) ≤
kVf (x)k2 log(B)
α ∙ O (√d - Piog(By
(20)
E.3 Putting things together
Putting the expected improvements together, we see that:
25
Under review as a conference paper at ICLR 2022
∆MUT (x) ≤
kVf (x)k2 log(B)
α ∙ O (√d - PiogW)2
∆ES (x) ≥ O
(kVf (χ)k2)
(21)
(22)
and thus there is a expected improvement ratio bound when B ≤ O(2d):
Δes(x)、C (1 (√d - Pbg(B))
≥-----7~~- ≥ O--------：--/ c、----
∆MUT (x)	κ	log(B)
\
where K = β∕ɑ is the condition number.
(23)
26