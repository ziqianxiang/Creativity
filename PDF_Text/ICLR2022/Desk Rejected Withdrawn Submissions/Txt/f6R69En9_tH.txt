Under review as a conference paper at ICLR 2022
Cross Project S oftware Vulnerability Detec-
tion via Domain Adaptation and Max-Margin
Principle
Anonymous authors
Paper under double-blind review
Ab stract
Software vulnerabilities (SVs) have become a common, serious and crucial con-
cern due to the ubiquity of computer software. Many machine learning-based ap-
proaches have been proposed to solve the software vulnerability detection (SVD)
problem. However, there still have been two significant issues presenting for SVD
in terms of i) learning automatic representations to improve the predictive perfor-
mance of SVD, and ii) tackling the scarcity of labeled vulnerabilities datasets that
conventionally need laborious labeling effort by experts. In this paper, we propose
a novel end-to-end approach to tackle these two crucial issues. We first exploit
the automatic representation learning with deep domain adaptation for software
vulnerability detection. We then propose a novel cross-domain kernel classifier
leveraging the max-margin principle to significantly improve the transfer learning
process of software vulnerabilities from labeled projects into unlabeled ones. The
experimental results on real-world software datasets show the superiority of our
proposed method over state-of-the-art baselines.
1	Introduction
In the field of software security, software vulnerabilities (SVs), defined as specific flaws or over-
sights in software programs allowing attackers to exploit the code base and potentially undertake
dangerous activities (e.g., exposing or altering sensitive information, disrupting, degrading or de-
stroying a system, or taking control of a program or computer system) (Dowd et al., 2006), are
very common and popular due to the ubiquity of computer software. Given the vast variety of tech-
nologies and different software development methodologies used, a great deal of computer software
may naturally contain software vulnerabilities, hence making the problem of software vulnerability
detection become a critical concern in computer security and software engineering. Moreover, the
severity of the threat imposed by software vulnerabilities (SV) has significantly increased over the
years causing significant damages to companies and individuals. As a result, this has necessitated
the development of automated advanced approaches and tools that can efficiently and effectively de-
tect SVs with a minimal level of human intervention. To respond to this demand, many vulnerability
detection systems and methods, ranging from open source to commercial tools, and from manual to
automatic methods (Shin et al., 2011; Neuhaus et al., 2007; Yamaguchi et al., 2011; Grieco et al.,
2016; Li et al., 2016; Kim et al., 2017; Li et al., 2018a; Duan et al., 2019; Cheng et al., 2019; Zhuang
et al., 2020) have been proposed and implemented.
Most previous work in software vulnerability detection (SVD) (Neuhaus et al., 2007; Shin et al.,
2011; Yamaguchi et al., 2011; Li et al., 2016; Grieco et al., 2016; Kim et al., 2017) has been devel-
oped based on handcrafted features which are manually chosen by knowledgeable domain experts
with possibly outdated experience and underlying biases. In many situations, handcrafted features
normally do not generalize well. For example, features that work well in a certain software project
may not perform well in other projects (Zimmermann et al., 2009). To alleviate the dependency
on handcrafted features, the use of automatic features in SVD has been studied recently (Li et al.,
2018a; Lin et al., 2018; Dam et al., 2018; Li et al., 2018b; Duan et al., 2019; Cheng et al., 2019;
Zhuang et al., 2020). These works have shown the advantages of employing automatic features over
handcrafted features in the context of software vulnerability detection.
1
Under review as a conference paper at ICLR 2022
Another major challenging issue in SVD research is the scarcity of labeled software projects to train
models. The process of labeling vulnerable source code is tedious, time-consuming, error-prone,
and can be very challenging even for domain experts. This has resulted in few labeled projects com-
pared with a vast volume of unlabeled ones. Some recent approaches (Nguyen et al., 2019; 2020;
Liu et al., 2020) have been proposed to solve this challenging problem with the aim to transfer the
learning of vulnerabilities from labeled source domains to unlabeled target domains. Particularly,
the methods in Nguyen et al. (2019; 2020) learn domain-invariant features from the code data of the
source and target domains by using the adversarial learning framework such as Generative Adver-
sarial Network (GAN) (Goodfellow et al., 2014) while the method in Liu et al. (2020) consists of
many subsequent stages: i) pre-training a deep feature model for learning representation of token
sequences (i.e., source code data), ii) learning cross-domain representations using a transformation
to project token sequence embeddings from (i) to a latent space, and iii) training a classifier from the
representations of the source domain data obtained from (ii). However, none of these methods ex-
ploit the imbalanced nature of source code projects for which the vulnerable codes are significantly
minor comparing to non-vulnerable ones.
On the other side, kernel methods are well-known for their ability to deal with imbalanced datasets
(ScholkoPf et al., 2001; Tax & Duin, 2004; Tsang et al., 2005; 2007; Le et al., 2010; Nguyen et al.,
2014). In a nutshell, a domain of majority, which is often a simple geometric shape in a feature
sPace (e.g., half-hyPerPlane (ScholkoPf et al., 2001; Nguyen et al., 2014) or hyPersPhere (Tax &
Duin, 2004; Tsang et al., 2005; 2007; Le et al., 2010)) is defined to characterize the majority class
(i.e., non-vulnerable codes). The key idea is that a simPle domain of majority in the feature sPace,
when being maPPed back to the inPut sPace, forms a set of contours which can distinguish majority
data from minority data.
In this PaPer, by leveraging learning domain-invariant features and kernel methods with the max-
margin PrinciPle, we ProPose Domain Adaptation with Max-Margin Principle (DAM2P) to effi-
ciently transfer learning from imbalanced labeled source domains to imbalanced unlabeled target
domains. Particularly, insPired by the max-margin PrinciPle Proven efficiently and effectively for
learning from imbalanced data, when learning domain-invariant features, we ProPose to learn a
max-margin hyPerPlane on the feature sPace to seParate vulnerable and non-vulnerable data. More
sPecifically, we combine labeled source data and unlabeled target data, and then learn a hyPerPlane
to seParate source non-vulnerable from vulnerable data and target data from the origin such that the
margin is maximized. In addition, the margin is defined as the minimization of the source and target
margins in which the source margin is regarded as the minimal distance from vulnerable data Points
to hyPerPlane (Nguyen et al., 2014), while the target margin is regarded as the distance from the
origin to the hyPerPlane (ScholkoPf et al., 2001). Our contributions can be summarized as follows:
•	We leverage learning domain-invariant features and kernel methods with the max-margin
PrinciPle to ProPose a novel aPProach named DAM2P which can efficiently bridge the gaP
between source and target domains on a joint sPace while being able to tackle efficiently
and effectively the imbalanced nature of source and target domains.
•	We conduct extensive exPeriments on real-world software datasets consisting of FFmPeg,
LibTIFF, LibPNG, VLC, and Pidgin software Projects. It is worth noting that to demon-
strate and comPare the caPability of our ProPosed method and baselines in the transfer
learning for SVD, the datasets (FFmPeg, VLC, and Pidgin) from the multimedia applica-
tion domain are used as the source domains whilst the datasets (LibPNG and LibTIFF) from
the image application domain were used as the target domains. The exPerimental results
show that our method significantly outPerforms the baselines by a wide margin, esPecially
for the F1-measure.
2	Related Work
Automatic features in software vulnerability detection (SVD) has been widely studied (Li et al.,
2018a; Lin et al., 2018; Dam et al., 2018; Li et al., 2018b; Duan et al., 2019; Cheng et al., 2019;
Zhuang et al., 2020) due to its advantages of emPloying automatic features over handcrafted features
in the context of SVD. Particularly, Dam et al. (2018) emPloyed a deeP neural network to transform
sequences of code tokens to vectorial features further fed to a seParate classifier while Li et al.
(2018a) combined the learning of the vector rePresentation and the training of the classifier in a
2
Under review as a conference paper at ICLR 2022
deep network. Advanced deep net architectures have been investigated for SVD problem. Notably,
Russell et al. (2018) combined both recurrent neural networks (RNNs) and convolutional neural
network (CNNs) for feature extraction from the embedded source code representations while Zhuang
et al. (2020) proposed a new model for smart contract vulnerability detection using graph neural
networks (GNNs).
Deep domain adaptation-based methods have been recently studied for SVD. Notably, Nguyen et al.
(2019) proposed a novel architecture and employed adversarial learning framework (e.g., GAN) to
learn domain-invariant features that can be transferred from labeled source to unlabeled target code
project. Nguyen et al. (2020) enhanced Nguyen et al. (2019) by proposing an elegant workaround
to combat the mode collapsing problem possibly faced in that work due to the use of GAN. Finally,
Liu et al. (2020) proposed a multi-stage approach including three stages undertaken sequentially: i)
pre-training a deep model for learning representation of token sequences (i.e., source code data), ii)
learning cross-domain representations using a transformation to project token sequence embeddings
from (i) to a latent space, iii) training a classifier from the representations of the source domain data
obtained from (ii).
3	Domain Adaptation with Max-Margin Principle
3.1	Problem Formulation
Given a labeled source domain dataset S
{(xS, y ι),…,《NS ,yΛ⅛)}
where yi ∈ {0, 1} (i.e., 1:
vulnerable code and 0: non-vulnerable code), let xiS = xiS1 , . . . , xiSL be a code function represented as
a sequence ofL embedding vectors. We note that each embedding vector corresponds to a statement
in the code function. Similarly, the unlabeled target domain dataset T = x1T, . . . ,xTN	consists the
code functions xiT = xiT1 , . . . , xiTL as sequences of L embedding vectors. For the data processing and
embedding, please refer to the appendix section.
In standard DA approaches, domain-invariant features are learned on a joint space so that a classifier
mainly trained based on source label data can be transferred to predict well unlabeled target data.
The classifiers of interest are usually deep nets conducted on top of domain-invariant features. In
this work, by leveraging with the kernel theory and the max-margin principle, we consider a kernel
machine on top of domain-invariant features, which is a hyperplane on a feature space via a fea-
ture map φ . Inspired by the max-margin principle proven efficient and effective for learning from
imbalanced data, when learning domain-invariant features, we propose to learn a max-margin hy-
perplane on the feature space to separate vulnerable and non-vulnerable codes. More specifically,
we combine labeled source data and unlabeled target data, and then learn a hyperplane to separate
source non-vulnerable from vulnerable data and target data from the origin such that the margin is
maximized. Furthermore, the margin is defined as the minimization of the source and target mar-
gins in which the source margin is defined as the minimal distance from vulnerable data points to
hyperplane (Nguyen et al., 2014) while the target margin is defined as the distance from the origin
to the hyperplane (ScholkoPf et al., 2001).
3.2	Our Proposed Approach DAM2P
3.2.1	Domain Adaptation for Learning Domain-Invariant Features
In what follows, we present the architecture of the generator G and how to use an adversarial learning
framework such as GAN (Goodfellow et al., 2014) to learn domain-invariant features on a join space
specified by G. To learn the automatic features for the sequential source code data, inspired from
Li et al. (2018a); Nguyen et al. (2019; 2020), we apply a bidirectional recurrent neural network
(bidirectional RNN) to both source and target domains. Given a code x in source or target domain,
we denote the output of the bidirectional RNN by B (x). We then use some fully connected layers
to connect the output layer of the bidirectional RNN with the joint feature layer wherein we bridge
the gap between the source and target domains. The generator is consequently the composition
of the bidirectional RNN and the subsequent fully connected layers: G (X) = f (B (X)) where f (∙)
represents the map formed by the fully connected layers.
3
Under review as a conference paper at ICLR 2022
Subsequently, to bridge the gap between the
source and target domains in the latent space,
inspired by GAN (Goodfellow et al., 2014), we
use a domain discriminator D to discriminate
the source and target data and train the gener-
ator G to fool the discriminator D by making
source and target data space indistinguishable
in the latent. The relevant objective function is
hence as follows:
1 NS
H (G,D) ： = Nr ∑logD (G (xS))
NS i=1
1 NT
+Nr ∑log [1-d (g(xT))]⑴
Figure 1: The architecture of using adversarial
learning framework such as GAN to bridge the
gap between the source and target domains in the
latent space.
where We seek the optimal generator G* and the
domain discriminator D* by solving:
G* = argmin H (G, D) and D* = argmax H (G, D)
GD
The architecture of using an adversarial learning framework such as GAN to bridge the gap between
the source and target domains is depicted in Figure 1.
3.2.2 Cross-domain kernel classifier
To build up an efficient domain adaptation approach for source code data which can tackle well
the imbalanced nature of source code projects, we leverage learning domain-invariant features with
the max-margin principle in the context of kernel machines to propose a novel cross-domain kernel
classifier named DAM2P. We construct a hyperplane on the feature space: wT φ (G(x)) -ρ = 0 with
the feature map φ and learn this hyperplane using the max-margin principle. More specifically, we
combine labeled source and unlabeled target data, and then learn a hyperplane to separate source
non-vulnerable from vulnerable data and target data from the origin in such a way that the margin is
maximized. Moreover, the margin is defined as the minimization of the source and target margins in
which the source margin is the minimal distance from vulnerable data points to hyperplane (Nguyen
et al., 2014), while the target margin is the distance from the origin to the hyperplane (SChOlkoPf
et al., 2001). The overall architecture of our proposed cross-domain kernel classifier in the feature
space is depicted in Figure 2.
Figure 2: The architecture of our proposed cross-domain kernel classifier in the feature space. By
using our proposed DAM2P method, we can gradually bridge the gap between the source and target
domains in the latent space, while in the feature space our proposed cross-domain kernel classifier
helps to distinguish the vulnerable and non-vulnerable data. At the end, when the source and target
domains are intermingled, we can transfer our trained cross-domain kernel classifier to classify the
data of the target domain.
Given the source domain dataset S = {(x1S, y1), . . . , (xSN ,yNS)} where yi = 1, i = 1, ..., m and yi =
0, i = m + 1, ..., NS and the target domain dataset T = {x1T, . . . , xTNT }, we formulate the following
optimization problem:
4
Under review as a conference paper at ICLR 2022
subject to
max
w,ρ
f
min
∖
M(W>φ(G(XA -P)
l∣wk
min
yi=1
source margin
(2)
yi(w>φ(G(xiS)) -P) ≥ 0, i= 1,..., NS
w> φ (G(xiT)) ≥P,i= 1,...,NT.
In optimization problem (2), w and P are the normal vector and the bias of the hyperplane and φ
is a transformation from the joint latent space to the feature space, while G is the generator used to
map the data of source and target domains from the input space into the joint latent space. It occurs
that the margin is invariant if we scale w, P by a factor k > 0. Hence without loosing of generality,
We can assume that min m min {yiw> φ (G(χS) 一 P }, ρ } = 11. The optimization problem (2) can be
rewritten as follows:
min1 l∣wk2	⑶
w,P 2
subject to
yi(w>φ(G(xs)) — P) ≥ 0, i = 1,...,m
yi(w>φ(G(xs))— P) ≥ 1, i = m + 1,…,Ns
w> φ (G(xiT)) ≥ P, i= 1,..., NT
We refer the above model as hard version of our proposed cross-domain kernel classifier. To derive
the soft version, We extend the optimization problem in Eq. (3) by using the slack variables as
folloWs:
miP1(2 kwk2+Ns++nt ∑ ∑∑ξ S+λ ∑∑ξ ξ!!	⑷
subject to
yi∙(w>φ(G(xS)) — ρ) ≥ —ξS, i = 1,...,m
yi(w>φ(G(X)) — P) ≥ 1 — ξS, i = m + 1,...,Ns
w>Φ(G(xT)) ≥ P — ξ:, i = 1,...,Nt
ξS ≥ 0, i = 1,...,Ns;ξ： ≥ 0, i = 1,...,NT.
Where λ > 0 is the trade-off hyper-parameter representing the Weight of the information from the
target domain contributing to the cross-domain kernel classifier.
The primal form of the soft model optimization problem is hence of the folloWing form:
minL (G, w, P )	(5)
w,P
Where We have defined
L(g,w,P) :=1 Ilwk2 + N 1 N ∑max{0,— yi (w>φ(G(xS)- P)}
2	Ns + NT i=1
1 Ns
+N+Nτ ∑ ιmax{0,— yi<w>φ (G (xS))- P)+1}
+N^NT ∑ max n0,—w> φ (G (X))+P}
1This assumption is feasible because if (w*, P *) is the optimal solution, (kw*, kP *) with k > 0 is also another
optimal solution. Therefore, We can choose k to satisfy the assumption.
5
Under review as a conference paper at ICLR 2022
cos(ω>G(Xi), -1
φ(G(xi))
We use a random feature map (Rahimi & Recht, 2008) for the transformation φ to map the repre-
sentations (e.g., G(xiS) and G(xiT)) from the latent space to a random feature space. The formulation
of φ on a specific G(xi) ∈ Rd is as follows:
sin(ωk>G(xi)	∈ R2K
k=1
where K consists of independent and identically distributed samples ω1, ..., ωK ∈ Rd which are the
Fourier random elements.
We note that the use of a random feature map φ (Rahimi & Recht, 2008) in conjunction with the
cost-sensitive kernel machine of our proposed cross-domain kernel classifier as mentioned in Eq.
(5) and a bidirectional recurrent neural network for the generator G allows us to conveniently do
back-propagation when training our proposed approach. Combining the optimization problems in
Eqs. (1 and 5), we arrive at the final objective function:
I (G, D, w, P) := L(G, w, P) + αH (G, D)	(6)
where α > 0 is the trade-off hyper-parameter. We seek the optimal generator G*, domain discrimi-
nator D*, the normal vector w* and bias P* by solving:
(G*, w*,ρ*) = argmin I (G,D, w,P) andD* = argmax I (G,D, w,P)
G,w,P	D
4	Experiments
Experimental Datasets We used the real-world datasets experimented in Nguyen et al. (2019;
2020) and collected by Lin et al. (2018). These contain the source code of vulnerable functions (vul-
funcs) and non-vulnerable functions (non-vul-funcs) obtained from six real-world software project
datasets, namely FFmpeg (#vul-funcs: 187 and #non-vul-funcs: 5427), LibTIFF (#vul-funcs: 81
and #non-vul-funcs: 695), LibPNG (#vul-funcs: 43 and #non-vul-funcs: 551), VLC (#vul-funcs: 25
and #non-vul-funcs: 5548), and Pidgin (#vul-funcs: 42 and #non-vul-funcs: 8268). These datasets
cover multimedia and image application categories.
In the experiments, to demonstrate the capability of our proposed method in the transfer learning
for software vulnerability detection (SVD) (i.e., transfering the learning of software vulnerabilities
(SVs) from labelled projects to unlabelled projects belonging to different application domains), the
datasets (FFmpeg, VLC, and Pidgin) from the multimedia application domains were used as the
source domains, whilst the datasets (LibPNG and LibTIFF) from the image application domains
were used as the target domains. It is worth noting that in the training process we hide the labels
of datasets from the target domains. We only use these labels in the testing phase to evaluate the
models’ performance. Moreover, we used 80% of the target domain without labels in the training
process, while the rest 20% was used for evaluating the domain adaptation performance. Note that
these partitions were split randomly as used in the baselines. Please refer to the appendix for details.
Baselines The main baselines of our proposed DAM2P method are the state-of-the-art end-to-end
deep DA approaches for SVD including DDAN (Ganin & Lempitsky, 2015), MMD (Long et al.,
2015), D2GAN (Nguyen et al., 2017), DIRT-T (Shu et al., 2018), SCDAN (Nguyen et al., 2019),
and Dual-GD-DDAN and Dual-GD-SDDAN (Nguyen et al., 2020) as well as the state-of-the-art
automatic feature learning for SVD, VulDeePecker (Li et al., 2018a). To the method operated via
separated stages proposed by Liu et al. (2020), at present, we cannot compare to it due to the lack of
original data and completed reproducing source code from the authors.
VulDeePecker (Li et al., 2018a) is an automatic feature learning method for SVD. The model em-
ployed a bidirectional recurrent neural network to take sequential inputs and then concatenated hid-
den units as inputs to a feedforward neural network classifier while the DDAN, MMD, D2GAN and
DIRT-T methods are the state-of-the-art deep domain adaptation models for computer vision pro-
posed in Ganin & Lempitsky (2015), Long et al. (2015), Nguyen et al. (2017) and Shu et al. (2018)
respectively. Inspired from Nguyen et al. (2019), we borrowed the principle of these methods and
refactored them using the CDAN architecture introduced in Nguyen et al. (2019) for SVD.
6
Under review as a conference paper at ICLR 2022
The SCDAN method (Nguyen et al., 2019) can be considered as the first one demonstrating the fea-
sibility of deep domain adaptation for SVD. Based on their proposed CDAN architecture, leveraging
deep domain adaptation with automatic feature learning for SVD, the authors proposed the SCDAN
method to efficiently exploit and utilize the information from unlabeled target data in order to im-
prove the model performance. The Dual-GD-DDAN and Dual-GD-SDDAN methods proposed in
Nguyen et al. (2020) aiming to deal with the mode collapsing problem faced in SCDAN and other
approaches (i.e., using GAN as a principle in order to close the gap between source and target do-
mains in the joint space) to further improve the transfer learning process for SVD.
For the data processing and embedding, and the model’s configuration, please refer to the appendix.
Table 1: Performance results in terms of false negative rate (FNR), false positive rate (FPR), Re-
call, Precision and F1-measure of VulDeePecker (VULD), MMD, D2GAN, DIRT-T, DDAN, SC-
DAN, Dual-GD-DDAN, Dual-GD-SDDAN and DAM2P methods for predicting vulnerable and
non-vulnerable code functions on the testing set of the target domain (Best performance in bold).
Source → Target	Methods	FNR	FPR	Recall	Precision	F1-measure
	VULD	42.86%	1.08%	57.14%	80%	66.67%
	MMD	37.50%	0%	62.50%	100%	76.92%
	D2GAN	33.33%	1.06%	66.67%	80%	72.73%
	DIRT-T	33.33%	1.06%	66.67%	80%	72.73%
Pidgin → LibPNG	DDAN	37.50%	0%	62.50%	100%	76.92%
	SCDAN	33.33%	0%	66.67%	100%	80%
	Dual-GD-DDAN	33.33%	0%	66.67%	100%	80%
	Dual-GD-SDDAN	22.22%	1.09%	77.78%	87.50%	82.35%
	DAM2P (ours)	12.50%	1.08%	87.50%	87.50%	87.50%
	VULD	43.75%	6.72%	56.25%	50%	52.94%
	MMD	28.57%	12.79%	71.43%	47.62%	57.14%
	D2GAN	30.77%	6.97%	69.23%	64.29%	66.67%
	DIRT-T	25%	9.09%	75%	52.94%	62.07%
FFmpeg → LibTIFF	DDAN	35.71%	6.98%	64.29%	60%	62.07%
	SCDAN	14.29%	5.38%	85.71%	57.14%	68.57%
	Dual-GD-DDAN	12.5%	8.2%	87.5%	56%	68.29%
	Dual-GD-SDDAN	35.29%	3.01%	64.71%	73.33%	68.75%
	DAM2P (ours)	14.29%	8.14%	85.71%	63.16%	72.73%
	VULD	-^25%^^	2.17%	75%	75%	75%
	MMD	12.5%	3.26%	87.5%	70%	77.78%
	D2GAN	14.29%	2.17%	85.71%	75%	80%
	DIRT-T	15.11%	2.2%	84.89%	80%	84.21%
FFmpeg → LibPNG	DDAN	0%	3.26%	100%	72.73%	84.21%
	SCDAN	12.5%	1.08%	87.5%	87.5%	87.5%
	Dual-GD-DDAN	0%	2.17%	100%	80%	88.89%
	Dual-GD-SDDAN	17.5%	0%	82.5%	100%	90.41%
	DAM2P (ours)	-^0%^^	1.07%	100%	87.50%	93.33%
	VULD	57.14%	1.08%	42.86%	75%	54.55%
	MMD	45%	4.35%	55%	60%	66.67%
	D2GAN	28.57%	4.3%	71.43%	55.56%	62.5%
	DIRT-T	50%	1.09%	50%	80%	61.54%
VLC→ LibPNG	DDAN	33.33%	2.20%	66.67%	75%	70.59%
	SCDAN	33.33%	1.06%	66.67%	80%	72.73%
	Dual-GD-DDAN	28.57%	2.15%	71.43%	71.43%	71.43%
	Dual-GD-SDDAN	11.11%	4.39%	88.89%	66.67%	76.19%
	DAM2P (ours)	33.33%	0.00%	66.67%	100%	80%
	VULD	35.29%	8.27%	64.71%	50%	56.41%
	MMD	30.18%	12.35%	69.82%	50%	58.27%
	D2GAN	40%	7.95%	60%	60%	60%
	DIRT-T	38.46%	8.05%	61.54%	53.33%	57.14%
Pidgin → LibTIFF	DDAN	27.27%	8.99%	72.73%	50%	59.26%
	SCDAN	30%	5.56%	70%	58.33%	63.64%
	Dual-GD-DDAN	29.41%	6.76%	70.59%	57.14%	63.16%
	Dual-GD-SDDAN	37.5%	2.98%	62.5%	71.43%	66.67%
	DAM2P (ours)	7.69%	9.20%	92.31%	60%	72.73%
7
Under review as a conference paper at ICLR 2022
4.1	Experimental Results
4.1.1	Code Domain Adaptation for a Fully Non-labeled Target Project
Quantitative Results We first investigated the performance of our proposed DAM2P method and
compared it with the baselines. We note that the VulDeePecker method was only trained on the
source data and then tested on the target data. The DDAN, MMD, D2GAN, DIRT-T, SCDAN,
Dual-GD-DDAN, Dual-GD-SDDAN and DAM2P methods employed the target data without using
any label information for domain adaptation.
The experimental results in Table 1 show that our proposed DAM2P method obtains a higher per-
formance for almost measures in almost cases of the source and target domains. The DAM2P
method always achieves the highest F1-measure for all pairs of the source and target domains. For
example, in the case of the source domain (FFmpeg) and target domain (LibPNG), the DAM2P
method obtains the F1-measure (93.33%) compared with the F1-measure (90.91%, 88.89%, 87.5%,
84.21%, 84.21%, 80%, 77.78% and 75%) obtained with Dual-GD-SDDAN, Dual-GD-DDAN, SC-
DAN, DDAN, DIRT-T, D2GAN, MMD and VulDeePecker, respectively.
Visualization We further demonstrate the efficiency of our proposed method in closing the gap of
the source and target domains. We visualize the feature distributions of the source and target domains
in the joint space using a t-SNE (Laurens & Geoffrey, 2008) projection with perplexity equal to 30.
In particular, we project the source and target data in the joint space (i.e., G (x)) into a 2D space
without undertaking domain adaptation (using the VulDeePecker method) and with undertaking
domain adaptation (using our proposed DAM2P method).
In Figure 3, we observe these cases when performing domain adaptation from a software project
(FFmpeg) to another (LibPNG). For the purpose of visualization, we select a random subset of the
source project against the entire target project. As shown in Figure 3, without undertaking domain
adaptation (VulDeePecker) the blue points (the source data) and the red points (the target data) are
almost separate while with undertaking domain adaptation the blue and red points intermingled
as expected. Furthermore, we observe that the mixing-up level of source and target data using
our DAM2P method is significantly higher than using VulDeePecker. In this visualization, there
is a strong correspondence between the success of domain adaptation regarding the classification
accuracy of the target domain and the overlap between the domain distributions.
Figure 3: A 2D t-SNE projection for the case of the FFmpeg → LibPNG without undertaking
domain adaptation (the left-hand figure, using the VulDeePecker method) and with undertaking
domain adaptation (the right-hand figure, using our proposed DAM2P method). The blue and red
points represent the source and target domains in the joint space respectively. In both cases of the
source and target domains, data points labeled 0 stand for non-vulnerable samples and data points
labeled 1 stand for vulnerable samples.
Ablation Study In this section, we aim to further demonstrate the efficiency of our DAM2P
method in transferring the learning of software vulnerabilities from imbalanced labeled source do-
mains to other imbalanced unlabeled target domains as well as the superiority of our novel cross-
8
Under review as a conference paper at ICLR 2022
domain kernel classifier in the our DAM2P method for learning and separating vulnerable and non-
vulnerable data.
For this ablation study, we conduct experiments on two pairs FFmpeg → LibTIFF and FFmpeg
→ LibPNG. We want to demonstrate that briding the discrepancy gap in the latent space and the
max-margin cross-domain kernel classifier are complementary to boost the domain adaptation per-
formance with imbalanced nature. We consider five cases in which we start from the blank case
(i, VulDeePecker) without briding gap and cross-domain kernel classifier. We then only add the
GAN term to bridge the discrepancy gap in the second case (ii, DDAN). In the third case (iii,
Kernel-Source), we only apply the max-margin principle for the source domain, while applying
the max-margin principle for the source and target domains in the fourth case (iv, Kernel-Source-
Target). Finally, in the last case (v, DAM2P), we simultaneously apply the bridging term and the
max-margin terms for source and target domains. The results in Table 2 shows that the max-margin
terms and bridging term help to boost the domain adaptation performance. Moreover, applying the
max-margin term to both source and target domains improves the performance comparing to apply-
ing to only source domain. Last but not least, bridging the discrepancy gap term in cooperation with
the max-margin term to significantly improve the domain adaptation performance.
Table 2: Performance results in terms of false negative rate (FNR), false positive rate (FPR), Re-
call, Precision and F1-measure of five cases including (i, VulDeePecker), (ii, DDAN), (iii, Kernel-
Source), (iv, Kernel-Source-Target), and (v, DAM2P) for predicting vulnerable and non-vulnerable
code functions on the testing set of the target domain (Best performance in bold).
Source → Target	Methods	FNR	FPR	Recall	Precision	F1-measure
	VulDeePecker	43.75%	6.72%	56.25%	50%	52.94%
	DDAN	35.71%	6.98%	64.29%	60%	62.07%
FFmpeg → LibTIFF	Kernel-Source	30%	5.56%	70%	58.33%	63.63%
	Kernel-Source-Target	25%	5.68%	75%	64.29%	69.23%
	DAM2P (ours)	14.29%	8.14%	85.71%	63.16%	72.73%
	VulDeePecker	-25%-	2.17%	75%	75%	75%
	DDAN	0%	3.26%	100%	72.73%	84.21%
FFmpeg → LibPNG	Kernel-Source	0%	4.39%	100%	69.23%	81.81%
	Kernel-Source-Target	0%	3.26%	100%	72.72%	84.21%
	DAM2P (ours)	0%	1.07%	100%	87.50%	93.33%
Please refer to the appendix section for the ablation study about the hyper-parameter sensitivity.
5	Conclusion
In this paper, in addition to exploiting deep domain adaptation with automatic representation learn-
ing for SVD, we have successfully proposed a novel cross-domain kernel classifier leveraging the
max-margin principle to significantly improve the capability of the transfer learning of software vul-
nerabilities from labeled projects into unlabeled ones in order to deal with two crucial issues in SVD
including i) learning automatic representations to improve the predictive performance of SVD, and
ii) coping with the scarcity of labeled vulnerabilities in projects that require the laborious labeling
of code by experts. Our proposed cross-domain kernel classifier can not only effectively deal with
the imbalanced datasets problem in SVD but also leverage the information of the unlabeled projects
to further improve the classifier’s performance. The experimental results show the superiority of
our proposed method compared with other state-of-the-art baselines in terms of the representation
learning and transfer learning processes.
References
Mardn Abadi, Paul Barham, Jianmin Chen, Zhifeng Chen, Andy Davis, Jeffrey Dean, MatthieU
Devin, Sanjay Ghemawat, Geoffrey Irving, Michael Isard, et al. Tensorflow: A system for large-
scale machine learning. In 12th USENIX Symposium on Operating Systems Design and Imple-
mentation (OSD116),pp. 265-283, 2016.
Nitesh V. Chawla, Kevin W. Bowyer, Lawrence O. Hall, and W. Philip Kegelmeyer. Smote: synthetic
minority over-sampling technique. Journal of Artificial Intelligence Research, 16:321-357, 2002.
9
Under review as a conference paper at ICLR 2022
Xiao Cheng, Haoyu Wang, Jiayi Hua, Miao Zhang, Guoai Xu, Li Yi, and Yulei Sui. Static detec-
tion of control-flow-related vulnerabilities using graph embedding. In 2019 24th International
Conference on Engineering of Complex Computer Systems (ICECCS), 2019.
Yin Cui, Menglin Jia, Tsung-Yi Lin, Yang Song, and Serge J. Belongie. Class-balanced loss based
on effective number of samples. CoRR, abs/1901.05555, 2019.
H.	K. Dam, T. Tran, T. Pham, N. S. Wee, J. Grundy, and A. Ghose. Automatic feature learning for
predicting vulnerable software components. IEEE Transactions on Software Engineering, 2018.
M. Dowd, J. McDonald, and J. Schuh. The Art of Software Security Assessment: Identifying and
Preventing Software Vulnerabilities. Addison-Wesley Professional, 2006. ISBN 0321444426.
Xu Duan, Jingzheng Wu, Shouling Ji, Zhiqing Rui, Tianyue Luo, Mutian Yang, and Yanjun Wu. Vul-
sniper: Focus your attention to shoot fine-grained vulnerabilities. In Proceedings of the Twenty-
Eighth International Joint Conference on Artificial Intelligence, IJCAI-19, pp. 4665-4671, 2019.
Y. Ganin and V. Lempitsky. Unsupervised domain adaptation by backpropagation. In Proceedings
of the 32nd International Conference on International Conference on Machine Learning - Volume
37, ICML’15, pp. 1180-1189, 2015.
I.	Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley, S. Ozair, A. Courville, and
Y. Bengio. Generative adversarial nets. In Advances in neural information processing systems,
pp. 2672-2680, 2014.
G. Grieco, G. L. Grinblat, L. Uzal, S. Rawat, J. Feist, and L. Mounier. Toward large-scale vulnerabil-
ity discovery using machine learning. In Proceedings of the Sixth ACM Conference on Data and
Application Security and Privacy, CODASPY ’16, pp. 85-96, 2016. ISBN 978-1-4503-3935-3.
SePP Hochreiter and Jurgen Schmidhuber. Long short-term memory. Neural Computation, 9(8):
1735-1780, 1997.
S.	Kim, S. Woo, H. Lee, and H. Oh. VUDDY: A scalable aPProach for vulnerable code clone
discovery. In IEEE Symposium on Security and Privacy, PP. 595-614. IEEE ComPuter Society,
2017.
D. P. Kingma and J. Ba. Adam: A method for stochastic oPtimization. CoRR, abs/1412.6980, 2014.
V. M. Laurens and H. Geoffrey. Visualizing data using t-SNE. Journal of Machine Learning Re-
search, 9:2579-2605, 2008.
T.	Le, D. Tran, W. Ma, and D. Sharma. An oPtimal sPhere and two large margins aPProach for
novelty detection. In Neural Networks (IJCNN), The 2010 International Joint Conference on, PP.
1-6, July 2010.
Z. Li, D. Zou, S. Xu, H. Jin, H. Qi, and J. Hu. VulPecker: An automated vulnerability detection
system based on code similarity analysis. In Proceedings of the 32Nd Annual Conference on
Computer Security Applications, ACSAC ’16, PP. 201-213, 2016. ISBN 978-1-4503-4771-6.
Z. Li, D. Zou, S. Xu, X. Ou, H. Jin, S. Wang, Z. Deng, and Y. Zhong. VuldeePecker: A deeP
learning-based system for vulnerability detection. CoRR, abs/1801.01681, 2018a.
Zhen Li, Deqing Zou, Shouhuai Xu, Hai Jin, Yawei Zhu, Zhaoxuan Chen, Sujuan Wang, and Jialai
Wang. Sysevr: A framework for using deeP learning to detect software vulnerabilities. CoRR,
abs/1807.06756, 2018b.
G.	Lin, J. Zhang, W. Luo, L. Pan, Y. Xiang, D. V. Olivier, and M. Paul. Cross-Project transfer
rePresentation learning for vulnerable function discovery. In IEEE Transactions on Industrial
Informatics, volume 14, 2018.
Tsung-Yi Lin, Priya Goyal, Ross B. Girshick, Kaiming He, and Piotr Dolldr. Focal loss for dense
object detection. CoRR, abs/1708.02002, 2017.
10
Under review as a conference paper at ICLR 2022
Shigang Liu, Guanjun Lin, Lizhen Qu, Jun Zhang, Olivier De Vel, Paul Montague, and Yang Xi-
ang. Cd-vuld: Cross-domain vulnerability discovery based on deep domain adaptation. IEEE
Transactions on Dependable and Secure Computing, 2020. doi: 10.1109/TDSC.2020.2984505.
M. Long, Y. Cao, J. Wang, and M. Jordan. Learning transferable features with deep adaptation
networks. In F. Bach and D. Blei (eds.), Proceedings of the 32nd International Conference on
Machine Learning, volume 37 of Proceedings ofMachine Learning Research, pp. 97-105, Lille,
France, 2015.
S.	Neuhaus, T. Zimmermann, C. Holler, and A. Zeller. Predicting vulnerable software components.
In Proceedings of the 14th ACM Conference on Computer and Communications Security, CCS
’07, pp. 529-540, 2007. ISBN 978-1-59593-703-2.
T.	D. Nguyen, T. Le, H. Vu, and D. Q. Phung. Dual discriminator generative adversarial nets. CoRR,
abs/1709.03831, 2017.
V.	Nguyen, T. Le, T. Pham, M. Dinh, and T. H. Le. Kernel-based semi-supervised learning for
novelty detection. In 2014 International Joint Conference on Neural Networks (IJCNN), pp.
4129-4136, July 2014.
V.	Nguyen, T. Le, T. Le, K. Nguyen, O. DeVel, P. Montague, L. Qu, and D. Phung. Deep domain
adaptation for vulnerable code function identification. In The International Joint Conference on
Neural Networks (IJCNN), 2019.
Van Nguyen, Trung Le, Olivier De Vel, Paul Montague, John Grundy, and Dinh Phung. Dual-
component deep domain adaptation: A new approach for cross project software vulnerability
detection. 2020.
Ali Rahimi and Benjamin Recht. Random features for large-scale kernel machines. In J. Platt,
D. Koller, Y. Singer, and S. Roweis (eds.), Advances in Neural Information Processing Systems,
volume 20. Curran Associates, Inc., 2008.
Rebecca L. Russell, Louis Y. Kim, Lei H. Hamilton, Tomo Lazovich, Jacob A. Harer, Onur Ozdemir,
Paul M. Ellingwood, and Marc W. McConley. Automated vulnerability detection in source code
using deep representation learning. CoRR, abs/1807.04320, 2018.
B. Scholkopf, J. C. Platt, J. C. Shawe-Taylor, A. J. Smola, and R. C. Williamson. Estimating the
support ofa high-dimensional distribution. Neural Comput., 13(7):1443-1471, July 2001. ISSN
0899-7667.
Y. Shin, A. Meneely, L. Williams, and J A Osborne. Evaluating complexity, code churn, and de-
veloper activity metrics as indicators of software vulnerabilities. IEEE Transactions on Software
Engineering, 37(6):772-787, 2011.
R. Shu, H. Bui, H. Narui, and S. Ermon. A DIRT-t approach to unsupervised domain adaptation. In
International Conference on Learning Representations, 2018.
D. M. J. Tax and R. P. W. Duin. Support vector data description. Journal of Machine Learning
Research, 54(1):45-66, 2004.
I. W. Tsang, J. T. Kwok, P. Cheung, and N. Cristianini. Core vector machines: Fast svm training on
very large data sets. Journal of Machine Learning Research, 6:363-392, 2005.
I. W. Tsang, A. Kocsor, and J. T. Kwok. Simpler core vector machines with enclosing balls. In
Proceedings ofthe 24th International Conference on Machine Learning, ICML ’07, pp. 911-918,
2007.
F. Yamaguchi, F. Lindner, and K. Rieck. Vulnerability extrapolation: assisted discovery of vul-
nerabilities using machine learning. In Proceedings of the 5th USENIX conference on Offensive
technologies, pp. 13-23, 2011.
Yuan Zhuang, Zhenguang Liu, Peng Qian, Qi Liu, Xiang Wang, and Qinming He. Smart contract
vulnerability detection using graph neural network. In Proceedings of the Twenty-Ninth Interna-
tional Joint Conference on Artificial Intelligence, IJCAI-20, pp. 3283-3290, 2020.
11
Under review as a conference paper at ICLR 2022
T. Zimmermann, N. Nagappan, H. Gall, E. Giger, and B. Murphy. Cross-project defect prediction:
A large scale experiment on data vs. domain vs. process. In Proceedings of the the 7th Joint
Meeting of the European Software Engineering Conference and the ACM SIGSOFT Symposium
on The Foundations of Software Engineering, ESEC/FSE '09, pp. 91-100, 2009. ISBN 978-1-
60558-001-2.
A Appendix
Data Processing and Embedding
We preprocess datasets before inputting them into the deep neural networks (i.e., baselines and our
proposed method). Inspired from Nguyen et al. (2019; 2020), we first standardize the source code
by removing comments, blank lines and non-ASCII characters. Secondly, we map user-defined vari-
ables to symbolic names (e.g., “var1”, “var2”) and user-defined functions to symbolic names (e.g.,
“func1”, “func2”). We replace integers, real and hexadecimal numbers with a generic <number>
token and strings with a generic <str> token. Thirdly, we embed statements in source code into vec-
tors. In particular, each statement xi j consists of two parts: the opcode and the statement information.
We embed both opcode and statement information to vectors, then concatenate the vector represen-
tations of opcode and statement information to obtain the final vector representation iij of statement
xij. For example, in the following statement “if(func2(func3(number,number),&var2) !=var10)”,
the opcode is if and the statement information is (func2(func3(number,number), &var2)!=var10).
To embed the opcode, we multiply the one-hot vector of the opcode by the opcode embed-
ding matrix. To embed the statement information, we tokenize it to a sequence of tokens (e.g.,
(,func2,(,func3,(,number,number,),&,var2,),!=,var10,)), construct the frequency vector of the state-
ment information, and multiply this frequency vector by the statement information embedding ma-
trix. This embedding process is depicted in Figure 4. In addition, the opcode embedding Wop and
statement information embedding Wsi matrices are learnable variables in the model.
Figure 4: An example of a source code statement embedding process.
Figure 5 shows an example of the procedure for data processing and embedding. The embedding
vectors (e.g., [ii1, . . . , iiL]), which are obtained from data processing and embedding process, of each
function (e.g., xi can be from the source domain or the target domain) represent for the function to
be the input to deep learning models (e.g., the baselines and our proposed method).
Note that as the baseline methods, to our proposed method, for handling the sequential properties of
the data and to learn the automatic features of the source code functions, we also use a bidirectional
recurrent neural network (bidirectional RNN) for both source and target domains.
12
Under review as a conference paper at ICLR 2022
Figure 5: An example of the procedure for data processing and embedding. We use a source code
function in the C language programming from the FFmpeg project. After the data preprocessing step,
we obtain a preprocessed function, and then using the embedding process we obtain the embedded
vectors corresponding to statements of the function.
Model configuration
For the baselines including VulDeePecker (Li et al., 2018a), and DDAN (Ganin & Lempitsky, 2015),
MMD (Long et al., 2015), D2GAN (Nguyen et al., 2017), DIRT-T (Shu et al., 2018), SCDAN
(Nguyen et al., 2019) using the architecture CDAN proposed in Nguyen et al. (2019), and Dual-
GD-DDAN and Dual-GD-SDDAN (Nguyen et al., 2020), and our proposed DAM2P method, we
use one bidirectional recurrent neural network with LSTM (Hochreiter & Schmidhuber, 1997) cells
where the size of hidden states is in {128, 256, 512} for the generator G while to the source classifier
C used in the baselines and the domain discriminator D, we use deep feed-forward neural networks
consisting of two hidden layers where the size of each hidden layer is in {200, 300}. We embed the
opcode and statement information in the {150, 150} dimensional embedding spaces respectively.
The trade-off parameters λ and α of our proposed method are in {10-3, 10-2, 10-1} while the
hidden size h is in {128, 256, 512}. The dimension of random feature space 2K is set equal to 512.
The length L of each function is padded or cut to 100 code statements (i.e., We base on the quantile
values of the functions’ length of each dataset to decide the length of each function padded or cut to
100 code statements. Almost all important information relevant to the vulnerability lies in the 100
first code statements.)
We employed the Adam optimizer (Kingma & Ba, 2014) with an initial learning rate of 10-3 while
the mini-batch size is set to 100 to our proposed method and baselines. We split the data of the source
domain into two random partitions containing 80% for training and 20% for validation. We also split
the data of the target domain into two random partitions. The first partition contains 80% for training
the models of MMD, D2GAN, DIRT-T, DDAN, SCDAN, Dual-GD-DDAN, Dual-GD-SDDAN, and
DAM2P without using any label information while the second partition contains 20% for testing the
models. We additionally apply gradient clipping regularization to prevent the over-fitting problem
in the training process of each model. We implement all mentioned methods in Python using Ten-
sorflow (Abadi et al., 2016), an open-source software library for Machine Intelligence developed by
the Google Brain Team.
Additional experiments
Hyper-parameter Sensitivity In this section, we investigate the correlation between important
hyper-parameters (including the λ , α, and h (the size of hidden states in the bidirectional neural
network)) and the F1-measure of our proposed DAM2P method. As mentioned in the experiments
section, the trade-off parameters λ and α are in {10-3, 10-2, 10-1} while the hidden size h is in
13
Under review as a conference paper at ICLR 2022
{128, 256, 512}. It is worth noting that we use the commonly used values for the trade-off hyper-
parameters (λ and α) representing for the weights of different terms mentioned in Eq. (6) and
the hidden size h. In order to study the impact of the hyper-parameters on the performance of
the DAM2P method, we use a wider range of values for λ , α, and h. In this ablation study, the
trade-off parameters λ and α are in {10-4, 10-3, 10-2, 10-1, 100, 101} while the hidden size h is in
{32, 64, 128, 256, 512, 1024}.
We investigate the impact of λ , α and h hyper-
parameters on the performance of the DAM2P
method on five pairs of the source and target
domains including FFmpeg to LibPNG, FFm-
peg to LibTIFF, Pidgin to LibPNG, Pidgin to
LibTIFF, and VLC to LibPNG. As shown in
Figures 6 and 7, we observe that the appro-
priate values to the hyper-parameters used in
the DAM2P model in order to obtain the best
model’s performance should be in from 10-4 to
10-2, from 10-3 to 10-1, and from 64 to 256
for λ , α and h respectively. In particular, for
the hidden size h, if we use too small values
IO2	IO3
h
Figure 6: The correlation between h and F1-
measure of our proposed DAM2P method.
FFmpeg-LibTIFF
-⅛- Pidgin-LibPNG
T- Pidgin-LibTIFF
VLC-LibPNG
(e.g., ≤ 32) or too high values (e.g., ≥ 1024),
the model might encounter the underfitting or
overfitting problems respectively. The model’s
performance on λ (i.e., representing the weight
of the information from the target domain con-
tributing to the cross-domain kernel classifier during the training process) shows that we should not
set the value of λ equal or higher than 1.0 (i.e., used for the weight of the information from the
source domain), and the value of λ should be higher than 10-4 to make sure that we use enough in-
formation of the target domain in the training process to improve the cross-domain kernel classifier.
FFmpeg-LibTIFF
-⅛- Pidgin-LibPNG
―Pidgin-LibTIFF
VLC-LibPNG
IoT	10^3	IO-2	I(JT	IO0	IO1	10^4	IOT	10-2	10-1	IO0	IO1
Λ	a
Figure 7: The correlation between (λ and α) and F1-measure of our proposed DAM2P method.
Additional ablation studies
Sampling and weighting are well-known as simple and heuristic methods to deal with imbalanced
datasets. However, as mentioned by Lin et al. (2017); Cui et al. (2019), these methods have some
limitations, for example, i) Sampling may either introduce large amounts of duplicated samples,
which slows down the training and makes the model susceptible to over-fitting when oversampling,
or discard valuable examples that are important for feature learning when under-sampling, and ii)
To the highly imbalanced datasets, directly training the model or weighting (e.g., inverse class fre-
quency or the inverse square root of class frequency) cannot yield satisfactory performance.
To experience these findings, we run an experiment on two pairs of the source and target domains
(i.e., FFmpeg → LibTIFF and FFmpeg → LibPNG) for some baselines including DDAN, SC-
DAN, Dual-GD-DDAN, and Dual-GD-SDDAN methods using the over-sampling technique based
on SMOTE (Chawla et al., 2002) (i.e., used to create balanced datasets). The experimental results
14
Under review as a conference paper at ICLR 2022
in Table 3 show that using the over-sampling technique cannot help improve these models’ per-
formance. In particular, the performance of these methods without using over-sampling is always
higher than using over-sampling on the used datasets in F1-measure, the most important measure
used in software vulnerability detection. Note that in Table 3, the term OS stands for over-sampling.
Table 3: Performance results in terms of false negative rate (FNR), false positive rate (FPR), Recall,
Precision and F1-measure of DDAN, SCDAN, Dual-GD-DDAN and Dual-GD-SDDAN methods
in two cases of using over-sampling (OS) and without using over-sampling (OS) for predicting
vulnerable and non-vulnerable code functions on the testing set of the target domain.
Source → Target	Methods	FNR	FPR	Recall	Precision	F1-measure
	DDAN with OS	50%^	4.55%	50%	60%	54.55%
	DDAN without OS	35.71%	6.98%	64.29%	60%	62.07%
	SCDAN with OS	27.27%	7.87%	72.72%	55.33%	61.54%
FFmpeg → LibTIFF	SCDAN without OS	14.29%	5.38%	85.71%	57.14%	68.57%
	Dual-GD-DDAN with OS	25%	6.72%	75%	57.14%	64.87%
	Dual-GD-DDAN without OS	12.5%	8.2%	87.5%	56%	68.29%
	Dual-GD-SDDAN with OS	16.67%	9.1%	83.33%	56%	67%
	Dual-GD-SDDAN without OS	35.29%	3.01%	64.71%	73.33%	68.75%
	DDAN with OS	14.29%	2.15%	85.71%	75%	80%
	DDAN without OS	0%	3.26%	100%	72.73%	84.21%
	SCDAN with OS	0%	4.4%	100%	69.23%	81.82%
FFmpeg → LibPNG	SCDAN without OS	12.5%	1.08%	87.5%	87.5%	87.5%
	Dual-GD-DDAN with OS	0%	2.15%	100%	77.78%	87.5%
	Dual-GD-DDAN without OS	0%	2.17%	100%	80%	88.89%
	Dual-GD-SDDAN with OS	0%	2.17%	100%	80%	88.89%
	Dual-GD-SDDAN without OS	17.5%	0%	82.5%	100%	90.41%
15