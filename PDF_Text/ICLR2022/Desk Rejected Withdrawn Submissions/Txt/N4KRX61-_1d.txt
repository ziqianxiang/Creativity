Under review as a conference paper at ICLR 2022
A Hierarchical Bayesian Approach to Inverse
Reinforcement Learning with Symbolic Re-
ward Machines
Anonymous authors
Paper under double-blind review
Ab stract
A misspecified reward can degrade sample efficiency and induce un-
desired behaviors in reinforcement learning (RL) problems. We pro-
pose symbolic reward machines for incorporating high-level task knowl-
edge when specifying the reward signals. Symbolic reward machines
augment existing reward machine formalism by allowing transitions to
carry predicates and symbolic reward outputs. This formalism lends it-
self well to inverse reinforcement learning, whereby the key challenge is
determining appropriate assignments to the symbolic values from a few
expert demonstrations. We propose a hierarchical Bayesian approach
for inferring the most likely assignments such that the concretized re-
ward machine can discriminate expert demonstrated trajectories from
other trajectories with high accuracy. Experimental results show that
learned reward machines can significantly improve training efficiency
for complex RL tasks and generalize well across different task environ-
ment configurations.
1	Introduction
Reinforcement Learning (RL) agents rely on rewards to measure the utility of each interaction with
the environment. As the complexity of RL tasks increases, it becomes difficult for the agent to grasp
the intricacies of the task solely from goal-driven reward functions - rewarding the agent only at
the end of each episode. Reward machine (RM) is a formalism proposed by Icarte et al. (2020)
for representing a reward function as a finite state automaton (FSA). However, the design of RMs
can quickly become cumbersome as the complexity of the tasks increases. In this paper, we draw
inspiration from symbolic finite automaton (SFA) and symbolic finite transducer (SFT) Antoni &
Veanes (2017) and propose symbolic reward machine (SRM) which augment FSA-based RMs by
allowing the internal state transitions of the RM to carry predicates and functions on the trajectory.
In addition to improving interpretability and conciseness of the reward model, a salient benefit of
SRM is that it allows human engineers to express complex task scenarios and reward design patterns.
In many existing works on logically designed reward functions, the reward values are manually as-
signed based on heuristics which can undermine the effectiveness of the resulting reward functions.
For instance, if a learning agent is excessively awarded for the completion of a certain stage of a
task, the agent may tend to repeatedly complete the same stage to accumulate rewards instead of
commencing the next stage, a phenomenon known as reward hacking Amodei et al. (2016). We
envision that, in a typical design routine of an SRM, a human engineer constructs the SRM to in-
corporate high-level task information, but leave the low-level details, such as the right amount of
reward for a specific event or sub-task, empty or as holes. The SRM formalism also facilitates the
specification of symbolic constraints over the holes for capturing important task-specific nuances.
A major contribution of this paper is a learning framework for filling out the holes in an SRM, which
we call concretization. The input to the framework includes a set of example trajectories demon-
strated by a human expert, an SRM and optionally a symbolic constraint that the holes in the SRM
must satisfy. We leverage the generative adversarial approaches from Finn et al. (2016); Jeon et al.
(2018) to construct a discriminator with a neural network reward function to distinguish the expert
1
Under review as a conference paper at ICLR 2022
trajectories from the trajectories of an agent policy. However, our framework works in a hierar-
chical Bayesian manner. Basically, to circumvent the non-differentiability of SRMs, we employ a
sampler to sample candidate instantiations of the holes to concretize the SRM. Then we introduce a
stochastic reward signal as the latent variable dependent on the output of the concretized SRM and
employ a neural-network reward function to perform importance sampling of the stochastic rewards
for trajectory discrimination. We summarize our contributions below.
•	We propose to use SRMs to represent reward functions for RL tasks.
•	We develop a hierarchical Bayesian inference framework that can concretize an SRM by
learning from expert demonstrations.
•	Our approach enables RL agents to achieve state-of-the-art performance on highly complex
environments with only a few demonstrations. In addition, we show that SRMs generalize
well across different environment configurations of the same task.
2	Related Work
Inverse Reinforcement Learning. We first note that the IRL formulation proposed in Ng & Russell
(2000); Abbeel & Ng (2004) has an infinite number of solutions. The Max-Entropy IRL from
Ziebart et al. (2008), Max-Margin IRL from Abbeel & Ng (2004); Ratliff et al. (2006) and Bayesian
IRL from Ramachandran & Amir (2007) aim at resolving the ambiguity of IRL. However, those
approaches restrict the reward function to be linear on the basis of human designed feature functions.
Deep learning approaches proposed in Fu et al. (2018); Ho & Ermon (2016); Jeon et al. (2018); Finn
et al. (2016) have substantially improved the scalablity of IRL by drawing a connection between IRL
and Generative Adversarial Networks (GANs) introduced by Goodfellow et al. (2014). Our work,
while embracing the data-driven and generative-adversarial ideologies, further extends IRL to cope
with symbolically represented human knowledge.
Reward Design. There have been substantial efforts on enriching the information in reward func-
tions. Reward shaping proposed by Ng et al. (1999) adds state-based potentials to the reward in each
state. Exploration driven approaches such as Bellemare et al. (2016); Pathak et al. (2017); Alshiekh
et al. (2017); Flet-Berliac et al. (2021) incentivize agents with intrinsic rewards. Compared with
these methods, we do not seek to generate reward functions densely ranging over the entire state
space but rather design intepretable ones that selectively or even sparsely produce non-zero rewards.
Reward machines from Icarte et al. (2020) directly represent the reward functions as FSAs. The
symbolic reward machine in our work is also automata-based but augments RMs in a similar way
to SFA for FSA Veanes et al. (2012). There have been efforts on learning a so-called perfect RM
as termed in Toro Icarte et al. (2019) from the experience of an RL agent in partially observable
environment. However, the RM is still based on FSA and the rewards are still manually assigned.
Regarding leveraging human demonstrations, inverse reward design (IRD) proposed in Hadfield-
Menell et al. (2017) is analogous to IRL but aims at inferring a true reward function from some
proxy reward function perceived by a RL agent. Safety-aware apprenticeship learning from Zhou
& Li (2018) pioneers the incorporation of formal verification in IRL. However, those works confine
the reward functions to be linear of features as the generic IRL does. Our work does not have such
limitations.
Interpretable Reinforcement Learning. There have been continual researches on designing inter-
pretable policies Andre & Russell (2001; 2002); Verma et al. (2018); Zhu et al. (2019); Yang et al.
(2021); Tian et al. (2020). This paper concerns the design of interpretable reward functions rather
than interpretable policies. Our motivation is that a well-designed reward function is transferable
and can be a powerful complement to the vast literature on RL policy learning.
3	Background
An RL environment is a tuple M = hS, A, P, d0i where S is the state space; A is an action space;
P(s0|s, a) is the probability of reaching a state s0 by performing an action a ata state s; d0 is an initial
state distribution. A policy π(a∣s) determines the probability of an RL agent performing an action
a at state s. By successively performing actions for T steps after initializing from a state s(0)〜do,
a trajectory τ = s(0)a(0)s(1)a(1) . . . s(T)a(T) is produced. A state-action based reward function
2
Under review as a conference paper at ICLR 2022
is a mapping f : S × A → R to the real space. With a slight abuse of notations, we denote the
total reward along a trajectory τ as f(τ) = PtT=0 f (s(t), a(t)) and similarly for the joint probability
p(τ∣∏) = Qt=-o1 P(s(t+1)|s⑴,a⑶)π(a⑶卜⑴)of generating a trajectory T by following π. The
objective of an entropy-regularized RL task is to maximize Jrl(∏) = ET〜∏ [f (T)] + H(∏) where
τ 〜∏ is an abbreviation for T 〜P(T∣∏) and H(∏) is the expected entropy of ∏.
When the reward function is unknown but a set of expert trajectories TE is sampled with some expert
policy πE, GAIL Ho & Ermon (2016) trains an agent policy πA to match πE by minimizing Jadv
in Eq.2 via RL algorithms such as PPO Schulman et al. (2017). Adversarially, GAIL optimizes
a discriminator D : S × A → [0,1] to accurately identify TE 's from TA 〜∏a by maximizing
Jadv. From a probabilistic inference perspective, Bayesian GAIL from Jeon et al. (2018) labels
any expert trajectory TE with 1E and 0E to respectively indicate TE as being sampled from an ex-
pert demonstration set E and from some agent policy πA . Likewise, the trajectory TA of πA is
labeled with 1A and 0A for the same indications. Assuming that the labels 0A, 1E are known a pri-
ori, Bayesian GAIL solves the most likely discriminator D by maximizing p(D∣0A, 1e; ∏a; E) 8
p(D)p(0A, 1e∣∏a,D; E) H ETA p(ta∣∏a)p(0a∣ta; D) E P(TE∣E)p(1e|te; D) of which the
logarithm as in Eq.1 is lower-bounded due to Jensen’s inequality by Eq.2. It is further proposed in
Fu et al. (2018) that by representing D(s, a)
——.eXpf(S,a))r I、with a neural network f, when
exp(f (s,a))+πA (a|s)
Eq.2 is maximized, it holds that f ≡ log πE and f equals the expert reward function which πE is
optimal w.r.t, given that ∀t.p(t|E) ≈ P(T∣∏e). Hence, by representing D in Eq.1 and 2 with f, an
objective of solving the most likely expert reward function f is obtained.
logEp(ta∣∏a)p(0a∣ta; D) EP(TE∣E)p(1e|te； D)
(1)
TA TE
TT
≥ E log Q D(s(E), a(E))	+ E log Q (1 - D(s(A), a(A))) := Jadv(D)	(2)
TE 〜E	t = 0	TA 〜πA	t = 0
4	Symbolic Reward Machines (SRMs)
We first give a formal definition of SRM and motivate the use of SRMs with a task from a Mini-Grid
environment introduced in Chevalier-Boisvert et al. (2018). We highlight that the human insights
incorporated in the SRM can hardly be realized with conventional goal-driven reward mappings or
function approximations. We then formulate the problem of concretizing SRMs.
4.1	Definition
The definition of SRM is inspired from those of SFA and SFT in Veanes et al. (2012). To adapt them
to the RL setting, we assume a background theory equipped with fixed interpretations on the state
and actions in the RL environment as well as a language of functions. Following Pierce (2002), a
λ-term is a function written in the form of λx.ρ. The type of a λx.ρ is a mapping from the type
of input argument x to the type of function body ρ. The free variable set FV (ρ) of ρ is the set of
symbolically denoted variables appearing in ρ while not appearing in the input of any λ-term inside
ρ. Given some concrete input X, the evaluation of λx.ρ is written as [λx.ρ]∣(X) or [ρ[X∕χ][ where
[X∕x] represents the replacement of X with X in ρ. The denotation stays the same if X and X are not
unary. A predicate is a specific set of λ-terms mapping to Boolean type B = {>, ⊥} where >, ⊥
mean True and False respectively. The set of predicates is closed under Boolean operations ∧, ∨, .
Definition 1. Given an RL environment M = hS, A, P, d0i, a symbolic reward machine (SRM)
is a tuple L = hQ, Ψ, R, δ, q0, Acci where Q is a set of internal states; Ψ ⊆ (S ×A)* → B is a
set of predicates on trajectories in M; R is a set of λ -terms of type (S ×A)* → R; δ is a set of
transition rules (P, ψ, r, q), where P, q ∈ Q, ψ ∈ Ψ, r ∈ R; q0 ∈ Q is an initial state; Acc ⊆ Q is a
set of accepting states; the free variables set ofL is defined as FV (L) = FV (ρ).
ρ∈R∪Ψ
ψ//r
We use the notation P ---→ q for a rule (P, ψ, r, q) ∈ δ and call ψ its guard. The input to an SRM
ψ//r
is a trajectory T ∈ (S × A)*. A rule P ----> q is applicable at P iff ∣[Ψ]∣(t) = >, in which case
L outputs a reward [[r]](T) for the last state-action pair in T while the state P transitions to q. Ifno
rule is applicable, the state P does not transition and L outputs a fixed constant reward such as 0, in
3
Under review as a conference paper at ICLR 2022
(b)
Figure 1: a) A 8x8 DoorKey task; b) The diagram of an SRM designed for the DoorKey task.
Λ(-ψ)	//0
∃q∈Q.(p,ψ,r,q)∈δ
which case we dub a dummy transition p -------------------→ p. An SRM L is called deterministic
if given an input trajectory T at any state P ∈ Q, ∣{(p, ψ, r, q) ∈ δ∣ψ(τ) = >}| ≤ 1 in which
case with a little abuse of notations we write the next state as q = δ(p, τ) either obtained from the
uniquely applicable transition rule (p, ψ, r, q) ∈ δ s.t. ψ(τ) = >, or from a dummy transition such
that q = p. To deploy a deterministic SRM in an RL task is to construct a synchronous product as
defined below.
Definition 2. A synchronous product between an M and a deterministic L is a tuple M ③ L =
h(S × A)* × S × Q, A, Ψ, R, P Θ δ,do, qo, Acci Where a product state in (S × A)* × S × Q is a
pair (τ 二 s, q) where :: means concatenation; T 二 S means concatenating a trajectory T ∈ (S × A)*
with an M state s ∈ S; q ∈ Q is an L state; the product transition rule P Θ δ follows Eq.3 where
T :: s :: a is a trajectory resulted from further concatenating T :: s with an action a ∈ A; the initial
product state is (so, qo) where so 〜 do; the rest follows the definitions in M and L.
(PΘ δ)((τ ：： Ma，(τ ：： s ：： a ：： E =	Ofthe=Wi(PeT ：： S	⑶
Proposition 1. Suppose that L is deterministic. Starting from any p(o) ∈ Q, as the input trajectory
extends from τ = s(o)a(o) to τ = s(o)a(o) . . . s(T)a(T), there can only be at most one path σ =
P(o)P(1) . . . q(T +1) obtained by applying the uniquely applicable ora dummy transition successively
to τ from t = 0 to t = T, i.e., ∀t = 0, 1, . . . , T.P(t+1) = δ(q(t) , s(o)a(o) . . . s(t)a(t)).
Suppose that at time step t there is a transition from (τ :: s(t) , P(t)) to (τ :: s(t) :: a(t) ::
s(t+1) , P(t+1)) with (P Θ δ)((τ :: s(t), P(t)), a(t) , (τ :: s(t) :: a(t) :: s(t+1) , P(t+1))) > 0. While
L witnesses such product state transition, the RL agent only observes the state transition in M.
Furthermore, L returns a reward [[r(t)]](τ :: s(t) :: a(t)) at time step t, where r(t) is either 0 or the
ψ(t) //r(t)
λ-term associated with the rule P(t) -------→L P(t+1) . We further inductively define [[L](τ) as
[[L](τ :: s(t) :: a(t)) = [L]](τ) :: [[r(t) ](τ :: s(t) :: a(t)). For simplicity, we also write τ :: s :: a
as τ :: (s, a) as if τ is a list of (s, a)’s, if it does not raise ambiguity in the context. We denote
L(τ) = PtT=-o1 [[L](τ [: t]) where τ[: t] is the partial trajectory from initialization up until step t.
For clarification, we remark that an SRM is an SFT only under stricter conditions, in which case the
SRM retains all the properties of SFT, e.g., composability and decidability. While SFT emphasizes
the acceptance of inputs, SRM emphasizes more on computing the rewards for the trajectories.
4.2	Motivating Example
Fig.1a shows an 8 × 8 DoorKey task in the Mini-Grid environment. An agent needs to pick up a key,
unlock the yellow door on the grey wall and reach the green goal tile. In every step, the agent can
observe at most the 7 × 7 tiles in front of it if the tile is not blocked by walls and doors. By default,
the environment only returns a reward when the agent reaches the goal tile.
We show in Fig.1b the diagram of a SRM designed for this task. The symbols {?id}i5d=1 indexed by
id’s are free variables, which we dub holes, of which values are to be determined. The ellipses in
Fig.1b indicate the internal states Q. The directed edges indicate the transition rules δ. Each edge
carries a predicate ψ as well as a λ-term r separated by the //-sign. We omit the λ-signs and write
ψ and r in plain English for explanatory purposes. Some predicates, such as Reach_Goal, makes
proposition on the event occurring at present time step, i.e. the last state-action pair in the input
4
Under review as a conference paper at ICLR 2022
trajectory. In some other predicates, the terms starting with a #-sign, e.g., #CLOSEDOOR, takes the
present trajectory as input and outputs a N≥0 type value, counting the number of occurrence of the
event as described next to the #-sign. The initial internal state q0 is the one that contains “(Start)”.
The state “(End)” constitutes a singleton Acc. We omit all the dummy transitions in the diagram,
i.e., if none of the depicted transitions is applicable at a state, the output reward is a fixed constant
such as 0 and this state does not transition. Determining appropriate values for the ?id’s, however,
can be difficult. An instance of a flawed design is to have an excessively large ?4 which can cause
reward hacking. Basically, the agent can repetitively pick up and drop the key to collect a higher net
reward than that of reaching the goal. On the other hand, if the returned ?5 for dropping the key is
too negative, the agent may end up avoiding picking up the key entirely. To address this issue, we
can add the constraint ?4+?5 ≤ 0 to the SRM. Finally, we need to find appropriate assignments to
the holes so that the resulting RM is effective for the RL task.
4.3	Problem Formulation
For a predicate λx.ρ, ifρ does not include trajectory τ but includes holes ?id’s in its free variable set
FV(ρ), e.g., P := ?4+?5 ≤ 0 in the previous example, such p's can be potentially used as symbolic
constraints. When having a concrete value hid for a hole ?id, one can concretize L by replacing ?id
with hid in L, written as L[hid/?id]. We define the problem of concretizing an SRM below.
Definition 3 (Symbolic Reward Machine Concretization). The concretization problem of a sym-
bolic reward machine isa tuple hL, H, ci where L is an SRM with holes ? = {?1, ?2, . . .} ⊆ FV(L);
H = H1 ×H2 . . . with each Hid being the assignment space of ?id; c isa symbolic constraint subject
to FV (c) ⊆ ?. An SRM L can be concretized by any h ∈ H into an l := L[h/?] iff [[c[h/?]]] = >.
Concretizing an SRM does not readily mean that the resulting reward function will be effective for
the RL task. Hence, we further assume that a set E of demonstrated trajectories is provided by
the expert, thus inducing a learning from demonstration (LfD) version of the SRM concretization
problem hL, H, c, Ei. The solution h of this problem not only concretizes the SRM L but also
satisfies ∀π.Eτ〜E[L[h/?](T)] ≥ ET〜∏[L[h/?](T)], which inherits the definition of generic IRL
in Ng & Russell (2000).
5	A Hierarchical Bayesian Learning Framework
In this section, we propose an approach to solve the LfD version of the SRM concretization problem.
The generic IRL approach cannot be used to solve for the holes since the SRM such as the one
in Fig.1b may have the holes in the transition predicates. Our approach is inspired by Bayesian
GAIL as mentioned in the Background section. However, directly using L to substitute f in the
discriminator D in Eq.2 is not practical due to the following challenges: a) L is not differentiable
w.r.t the holes; b) L is trajectory based. In short, stochastic gradient descent with batched data is not
readily applicable. Hence, we propose a hierarchical inference framework to circumvent this issue.
An overview of the graphical model of our hierarchical inference framework is shown in
Fig.2a. Given a πA, we seek the most likely concretized SRM l from the log-likelihood
logp(1∣0a, 1e; ∏a, E) = logp(0a, 1e ∣∏a, E, l)p(l) + constant where the prior p(l) can be an
uniform distribution over some allowable SRM set. The log-likelihood logp(0a, 1e ∣∏a, E, l) is
factorized as in Eq.4 by introducing two latent factors, fτA and fτE, which are two sequences of
rewards for the state-action pairs along τA and τE . On one hand, each element of fτ constitutes
a discriminator for the labels 0A or 1E in the same way as the reward f(s, a) does in the discrim-
inator D of Eq.2. On the other hand, fτ is viewed as a noisy observation of [[l]](τ) in that the
latent distribution p(fτ∣τ; l) is interpreted as the likelihood of observing f given [l]∣(τ). Here,
We adopt a tractable model such as Gaussian noise e 〜N(0,1) to simulate a stochastic reward
fτ = [[l]](τ) + which means adding the same to each reward in the reward sequence [[l]](τ). It is
trivially provable that given any reward function f, supposed that a policy π is the optimal policy to
maximize JRL(π), then this π retains optimal if f is replaced with its stochastic version f + e with
e 〜N(0,1), if the trajectory lengths are considered equal. To measure the integrals in Eq.4, We
re-introduce a neurally simulated reward function f for the importance sampling of the stochastic
fτ's. We define p(fτ∣τ; f) in the same way as p(fτ∣τ; l) except for replacing f = [l]∣(τ) + e with
fτ = f (T[t]) + e. With the sampled f 〜p(fτ∣τ; f), we obtain a lower-bound Eq.5 where the
5
Under review as a conference paper at ICLR 2022
(a)
Sample From E
[(⅞az1	I
-----Sample Trajectories	'——►
Optimize f	'*—
Oiii qψn
Optimize q ----
Sample From q 一
Optimize TrA
πΦn
(b)
W
T

Figure 2: a) The probabilistic graphical model of our framework; b) The flow chart of Algorithm 1
GAIL objective Jadv as defined in Eq.2 is embedded but with D(s, a) :=
exp(f (s,a)+6)
exp(f (s,a)+e)+∏A(a∣s)
in place of D. We also abbreviate p(∙∣τ; l) and p(∙∣τ; f) as pι(τ) and Pf (T) in the KL-divergence
DKL (pf (τ)∣∣pι(τ)), which can be viewed as a regularization term and turns out to be proportional
to the squared error PtT=-01( [l]](τ [: t]) - f (τ [t]))2. With Eq.5 we decouple the optimization of the
GAIL objective Jadv from l such that a standard stochastic gradient descent with batched data can
be conducted on the GAIL objective Jadv . We prove in the Appendix that the stochastic version
Ee〜N(0,ι) [Jadv(De)] has the same optimal condition as that of Jadv(D) in Eq.2.
logp(0A, 1e∣∏A,E,1) := log E p(ta∖∏a)p(te|E)
τA ,τE
JJ P(OA|TA； ∏A,fτA )P(IE |TE； ∏A,fτE )P(fTE |TE ； I)P(AJTA ；I)	(4)
fτA fτE
≥ max E	hJadv(De)i - E hDKL(Pf(T)||Pl(T))i	(5)
f e 〜N (0,1)	T ~∏a,E
As Eq.5 is still not differentiable w.r.t. the holes in l, we resort to finding a distribution q of l
to match the likelihood P(l|0A, 1E; πA, E). It can be realized by minimizing a KL-divergence
DKLhq(l)l∣p(l∣0A, 1e ； ∏a,E )] = lEqh log q(l) - log P(I⅛A⅛A∏E,,Ep(I) ], or by maximizing its
evidence lower-bound (ELBO) which is further lower-bounded by Eq.6. When a symbolic con-
straint is considered, the prior P(l) can be viewed as being uniform only among those l’s satisfying
the symbolic constraint while being zero everywhere else. We let Jcon(q) := -DKL[q(l)||P(l)]
be a supervised learning objective and abbreviate everything else in Eq.6 as a soft-maximization
objective Jsoft (q, f).
ELBO(q)
≥	maxeJ(0,1)[jadv (De	-lEq [DKL(Pf (T 川Pl (T	-DKL q(l)||P(l) (6)
T~∏A ,E
:= Jsof t (q, f ) + Jcon (q)
In our implementation, we consider the case when the holes are all real numbers, i.e., ∀id.Hid = R.
We construct a neurally simulated sampler qφ to output the mean and diagonal variance matrix of
a multivariate Gaussian distribution of which the dimension equals the number of holes. As each
hole assignment h sampled from this Gaussian corresponds to a l := L[h/?], we still denote by
qφ(l) the distribution of 1's. Besides qψ, We let fθ be the neurally simulated f. To calculate the
gradients of Jsoft(q4, fθ) w.r.t φ and θ, we use the logarithmic trick from Peters & Schaal (2008)
to handle El〜qt^ H ≈ KK V^i log qψi (lk)[∙] with K samples of concretized SRMs. Reparameteri-
zation trick Kingma & Welling (2013) is also used to optimize the stochastic adversarial objective
Ee 〜N (0,ι) [Jadv (De)] w.r.t fθ. AS for Jcon, since we only consider the case where the symbolic con-
straints are all conjunctions of atomic predicates that only involve linear arithmetic, e.g., ?4+?5 ≤ 0,
we make a relaxation by assigning the mean of the multivariate Gaussian produced by q中 to the holes
and then evaluating a binary cross-entropy loss of violating the symbolic constraint. As a result, Jcon
is differentiable w.r.t φ. We use a neural network πφ to simulate the agent policy πA and train it
with the most likely l* = arg maxl [q^(l)] which can also be readily obtained from the mean of the
multivariate Gaussian distribution specified by qφ. We summarize the algorithm in Algorithm 1 and
illustrate the flow chart of Algorithm 1 in Fig.2b.
6
Under review as a conference paper at ICLR 2022
Algorithm 1 Hierarchical Bayesian Inference For SRM Concretization
Input: Expert demonstration E, initial agent policy ∏φ0, reward function fθ0, sampler q^。, iteration
number i = 0, maximum iteration number N
Output: ∏φN and q”
1:	while iteration number i < N do
2:	Sample trajectory set {τA,i}im=1 by using policy πφi
3:	Computing reward {l^(τA,i)}m=ι with the most likely l* = argmax? qφi(l)
4:	Update φi → φi+1 using policy optimization, e.g., PPO
5:	Sample K samples {lk}M by using q/
6:	Sample {fei(s, a) + e|e 〜N(0,1)} respectively with (s, a) ∈ E and {τA,i}m=ι
7:	Update θ%+∖ J θi + αVθi Jsoft(qg, fθi) With a step size parameter ɑ
8:	Update 夕i+ι J 夕i + βVg Jsoft (qg) + βηVg Jcon (q£ with step size parameters β, η
9:	end while
10:	return ∏Φn and q甲N
6	Experiments
Our benchmark includes three tasks of growing difficulty in the Mini-Grid environment: Door-Key,
KeyCorridor and ObstructedMaze. The first task has been introduced earlier. The latter two are
respectively shown in Fig.3b and Fig.3c in which the agent needs to pick up a targeted purple/blue
ball in a locked room. In KeyCorridor, there are multiple closed doors blocking the view. In Ob-
structedMaze, some doors are locked; the keys for the locked doors are hidden in grey boxes; and
each locked door is obstructed by a green ball. We note that despite the difficulty of these tasks, our
designed SRMs do not carry out any motion planning and are solely based on reasoning the signifi-
cant events. The details are explained in the Appendix. In all three tasks, the environments can vary
in size by changing the number of rooms and tiles (e.g., DoorKey-8x8 vs. DoorKey-16x16). The
placements of the objects and doors are randomized in each instance of an environment.
6.1	Main Results
In this section, we investigate the following questions: A. Performance: whether Algorithm 1
can train an agent policy to achieve high average returns with a small number of environment in-
teractions; B. Example Efficiency: whether Algorithm 1 can train an agent policy to achieve high
performance with fewer number of demonstrated trajectories; C. Generalization: whether the SRM
concretized by Algorithm 1 for one environment can be used to improve the performance of RL
agents on a different environment for the same task.
There are two main categories of baselines: (1) generic IRL algorithms, including GAN-GCL
from Fu et al. (2018) and GAIL from Ho & Ermon (2016); (2) generic RL algorithm, PPO Schul-
man et al. (2017), and exploration driven RL algorithms, including RIDE Raileanu & Rocktaschel
(2020) and AGAC Flet-Berliac et al. (2021) which have performed well and even achieved state-
of-the-art (SOTA) performance in some of the Mini-Grid tasks by generating intrinsic rewards. To
answer question A, we implement PPO or AGAC in line 4 of Algorithm 1, and compare the results
with those of the IRL baselines in which PPO is used for policy learning. We also include the results
of the exploration driven RL algorithms that achieved SOTA in the benchmarks for reference. To
answer question B, we vary the number of demonstrated trajectories and observe the results. To
answer questions C, we directly adopt the SRMs concretized via Algorithm 1 in small environments
to train RL agents in much larger environments and compare with training with the default reward.
For each task, our basic setup includes 10 demonstrated trajectories, an SRM, optionally a symbolic
constraint, an actor-critic agent ∏φ, a neurally simulated reward function fθ and a sampler qφ that
generates a multivariate Gaussian distribution. The actor-critic networks of πφ have two versions, a
non-recurrent CNN version and an LSTM version. In most tasks we only report the results from the
versions with higher performance. The reward function fθ is simulated by an LSTM network. For
fair comparisons, we use identical hyperparameters and the same actor-critics and neurally simulated
reward functions, if applicable, when comparing our approach with PPO, GAN-GCL, GAIL and
AGAC. To measure training efficiency, we show how the average return, i.e. the average default
reward achieved over a series of consecutive episodes by the agent policy, changes as the number
7
Under review as a conference paper at ICLR 2022
(a) DoorKey-16x16
Ii
In=B
(c) ObstructedMaze-Full
(b) KeyCorridorS6R3
6 4 2 0
0.0.0.0.
En-Sα ΦO2Φ><
0.0	0.2	0.4	0.6	0.8	1.0	0	1	2	3	4	5
Frames	le7	Frames	le7
(j) DoorKey-16x16-v0
Figure 3: Algo1+AGAC/PPO indicates using AGAC or PPO as the policy learning algorithm in line
4 of Algorithm 1. AGAC/PPO+SRM indicates training an AGAC or PPO agent with the concretized
SRM. CNN and LSTM in the parentheses indicate the versions of the actor-critic networks. S4 and
S6 in (k) indicate respectively the results for KeyCorridorS4R3 and KeyCorridorS6R3.
(l) ObstructedMaze-Full
(k) KeyCorridorS4/S6R3
of frames, i.e. the number of total interactions between the agent and the environment, increases.
To measure demonstrated example efficiency, we show how many frames that an algorithm takes to
pass a certain level of average return for different number of demonstrations.
DoorKey. We use 10 example trajectories for the DoorKey-8x8 environment shown in Fig.2a. In
Fig.3d, running Algorithm 1 by using PPO and AGAC in line 4 respectively produces policies with
higher performance and needing fewer frames than by training PPO or AGAC with the default
reward. RIDE, GAN-GCL+PPO(CNN) and GAIL+PPO(CNN) fail with close-to-zero returns. In
Fig.3g we reduce the number of examples from 10 to 1 and it does not affect the number of frames
that Algorithm 1 needs to produce a policy with average return of at least 0.8, regardless of whether
PPO or AGAC is used in line 4. We use the concretized SRM to train PPO and AGAC agents in a
16x16 DoorKey environment as shown in Fig.3a and achieve higher performances with significantly
fewer frames than training PPO, AGAC or RIDE with the default reward as shown in Fig.3k.
KeyCorridor. We use 10 example trajectories for a 6 × 6 KeyCorridorS3R3 environment. By
using PPO and AGAC in line 4 of Algorithm 1, we respectively obtain higher performance with
8
Under review as a conference paper at ICLR 2022
Figure 4: Algo1+PPO(CNN) indicates using PPO as the policy learning algorithm in line 4 of
Algorithm 1 with symbolic constraints including relational predicates; Algo1(w/o c)+PPO(CNN)
indicates running Algorithm 1 without symbolic constraint; Algo1(signonly)+PPO(CNN) indicates
running Algorithm 1 with symbolic constraint that only concerns the signs of the holes
significantly fewer frames than by training AGAC with the default reward as shown in Fig.3e. GAIL
and GAN-GCL fail again in the KeyCorridor task. Hence we omit their results in the plot. As shown
in Fig.3h, reducing the number of examples (to 1) does not affect performance of Algorithm 1 for
producing a policy with average return of at least 0.8. In Fig.3k, we use the concretized SRM to
train RL agents in the 10 × 10 KeyCorridorS4R3 and 16 × 16 KeyCorridorS6R3 environments, and
achieve higher performances with fewer frames than training with the default rewards. We omit the
results from other baselines in this task since AGAC(CNN) is the current SOTA for this task.
ObstructedMaze. We use 10 example trajectories in a two-room ObstructedMaze-2Dhlb environ-
ment to concretize three differently designed SRMs, i.e., SRM1~3, Via Algorithm 1. We show
the results for concretizing SRM1 in Fig.3f where Algorithm 1 produces policies with higher per-
formance and needing fewer frames than AGAC trained with the default reward. When training
the PPO agent, we discount PPO by episodic state Visitation counts as in many exploration-driVen
approaches including AGAC. GAIL and GAN-GCL fail again in this task. As shown in Fig.3i, re-
ducing the number of examples (to 1) does not affect the performance of Algorithm 1 for producing
a policy with aVerage return ofat least 0.7. We also use all the concretized SRMs to train RL agents
in a 9-room ObstructedMaze-Full enVironment. We show in Fig.3l that the RL agent trained with
SRM2 attains high performance more efficiently than that with the default reward. The concretized
SRM1 does not generalize as well (with performance similar to that of AGAC with default reward)
as SRM2 does in this larger enVironment. The main difference between SRM1 and SRM2 is that
SRM1 implements a hindsight reward modification functionality similar to that in Andrychowicz
et al. (2017) but SRM2 does not. Since SRM1 modifies the hindsight reward to 0, it may make the
reward signals too sparse for the RL agent to learn efficiently for the larger enVironment. We proVide
more details on the experimental comparison of all three SRMs in the Appendix.
6.2	Ablation Study
In all the preVious tests, the symbolic constraints include relational predicates, i.e., ones that concern
the relations between holes, such as the ?4 +?5 ≤ 0 mentioned in the motiVating example. To inVes-
tigate whether Algorithm 1 can work with weaker symbolic constraint, we run Algorithm 1 without
symbolic constraint for the DoorKey-8x8 task. For the KeyCorridorS3R3 and ObstructedMaze-
2Dhlb tasks, we remoVe all the relational predicates and keep those concerning only the signs of the
holes, e.g. ?id ≤ 0. As shown in Fig.4, Algorithm 1 retains its high performance albeit needing
more interactions in the DoorKey enVironment.
7	Conclusion
We propose symbolic reward machines to represent reward functions in RL tasks. SRMs comple-
ment policy learning methods by proViding a structured way to capture high-leVel task knowledge.
In addition, we deVelop a hierarchical Bayesian framework to concretize SRMs by learning from
expert demonstrations. Experimental comparison with SOTA baselines on challenging benchmarks
Validates our approach. Future works will focus on reducing human efforts in the design of SRMs.
9
Under review as a conference paper at ICLR 2022
References
Pieter Abbeel and Andrew Y. Ng. Apprenticeship learning via inverse reinforcement learning. In
Proceedings of the Twenty-first International Conference on Machine Learning, ICML ’04, pp.
1-, New York, NY, USA, 2004. ACM. ISBN 1-58113-838-5. doi: 10.1145/1015330.1015430.
URL http://doi.acm.org/10.1145/1015330.1015430. 2
Mohammed Alshiekh, Roderick Bloem, RUediger Ehlers, Bettina Konighofer, Scott Niekum, and
Ufuk Topcu. Safe reinforcement learning via shielding. CoRR, abs/1708.08611, 2017. URL
http://arxiv.org/abs/1708.08611. 2
Dario Amodei, Chris Olah, Jacob Steinhardt, Paul Christiano, John Schulman, and Dan Mane. Con-
crete problems in AI safety. CoRR, abs/1606.06565, 2016. URL http://arxiv.org/abs/
1606.06565. 1
David Andre and Stuart J Russell. Programmable reinforcement learning agents. In Advances in
neural information processing systems, pp. 1019-1025, 2001. 2
David Andre and Stuart J Russell. State abstraction for programmable reinforcement learning agents.
In AAAI/IAAI, pp. 119-125, 2002. 2
Marcin Andrychowicz, Filip Wolski, Alex Ray, Jonas Schneider, Rachel Fong, Peter Welinder, Bob
McGrew, Josh Tobin, OpenAI Pieter Abbeel, and Wojciech Zaremba. Hindsight experience re-
play. In Advances in neural information processing systems, pp. 5048-5058, 2017. 9
Loris Antoni and Margus Veanes. The power of symbolic automata and transducers.
In Computer Aided Verification, 29th International Conference (CAV’17). Springer, July
2017. URL https://www.microsoft.com/en-us/research/publication/
power-symbolic-automata-transducers-invited-tutorial/. 1
Marc Bellemare, Sriram Srinivasan, Georg Ostrovski, Tom Schaul, David Saxton, and Remi Munos.
Unifying count-based exploration and intrinsic motivation. Advances in neural information pro-
cessing systems, 29:1471-1479, 2016. 2
Maxime Chevalier-Boisvert, Lucas Willems, and Suman Pal. Minimalistic gridworld environment
for openai gym. https://github.com/maximecb/gym- minigrid, 2018. 3
Chelsea Finn, Paul Christiano, Pieter Abbeel, and Sergey Levine. A connection between gener-
ative adversarial networks, inverse reinforcement learning, and energy-based models. CoRR,
abs/1611.03852, 2016. URL http://arxiv.org/abs/1611.03852. 1, 2
Yannis Flet-Berliac, Johan Ferret, Olivier Pietquin, Philippe Preux, and Matthieu Geist. Adversar-
ially guided actor-critic. In International Conference on Learning Representations, 2021. URL
https://openreview.net/forum?id=_mQp5cr_iNy. 2, 7, 18, 19
Justin Fu, Katie Luo, and Sergey Levine. Learning robust rewards with adverserial inverse re-
inforcement learning. In International Conference on Learning Representations, 2018. URL
https://openreview.net/forum?id=rkHywl-A-. 2, 3, 7
Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair,
Aaron Courville, and Yoshua Bengio. Generative adversarial nets. In Advances in neural infor-
mation processing systems, pp. 2672-2680, 2014. 2, 20
Dylan Hadfield-Menell, Smitha Milli, Pieter Abbeel, Stuart J Russell, and Anca Dragan. Inverse
reward design. In I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan,
and R. Garnett (eds.), Advances in Neural Information Processing Systems, volume 30. Curran
Associates, Inc., 2017. 2
Jonathan Ho and Stefano Ermon. Generative adversarial imitation learning. In Advances in Neural
Information Processing Systems, pp. 4565-4573, 2016. 2, 3, 7
Rodrigo Toro Icarte, Toryn Q. Klassen, Richard Anthony Valenzano, and Sheila A. McIlraith.
Reward machines: Exploiting reward function structure in reinforcement learning. CoRR,
abs/2010.03950, 2020. URL https://arxiv.org/abs/2010.03950. 1, 2
10
Under review as a conference paper at ICLR 2022
Wonseok Jeon, Seokin Seo, and Kee-Eung Kim. A bayesian approach to generative adversarial
imitation learning. In S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi,
and R. Garnett (eds.), Advances in Neural Information Processing Systems, volume 31. Curran
Associates, Inc., 2018. 1, 2, 3
Diederik P Kingma and Max Welling. Auto-encoding variational bayes. arXiv preprint
arXiv:1312.6114, 2013. 6
Andrew Y. Ng and Stuart J. Russell. Algorithms for inverse reinforcement learning. In Proceedings
of the Seventeenth International Conference on Machine Learning, ICML ,00, pp. 663-670, San
Francisco, CA, USA, 2000. Morgan Kaufmann Publishers Inc. ISBN 1-55860-707-2. URL
http://dl.acm.org/citation.cfm?id=645529.657801. 2, 5
Andrew Y. Ng, Daishi Harada, and Stuart J. Russell. Policy invariance under reward transformations:
Theory and application to reward shaping. In Proceedings of the Sixteenth International Confer-
ence on Machine Learning, ICML ’99, pp. 278-287, San Francisco, CA, USA, 1999. Morgan
Kaufmann Publishers Inc. ISBN 1-55860-612-2. URL http://dl.acm.org/citation.
cfm?id=645528.657613. 2
Deepak Pathak, Pulkit Agrawal, Alexei A Efros, and Trevor Darrell. Curiosity-driven exploration
by self-supervised prediction. In International conference on machine learning, pp. 2778-2787.
PMLR, 2017. 2
Jan Peters and Stefan Schaal. Reinforcement learning of motor skills with policy gradients. Neural
networks, 21(4):682-697, 2008. 6
Benjamin C. Pierce. Types and Programming Languages. The MIT Press, 1st edition, 2002. ISBN
0262162091. 3
Roberta RaileanU and Tim RocktascheL	Ride: Rewarding impact-driven exploration for
procedurally-generated environments. In International Conference on Learning Representations,
2020. URL https://openreview.net/forum?id=rkg-TJBFPB. 7
Deepak Ramachandran and Eyal Amir. Bayesian inverse reinforcement learning. Urbana, 51
(61801):1-4, 2007. 2
Nathan D. Ratliff, J. Andrew Bagnell, and Martin A. Zinkevich. MaximUm margin planning. In
Proceedings of the 23rd International Conference on Machine Learning, ICML ’06, pp. 729-
736, New York, NY, USA, 2006. ACM. ISBN 1-59593-383-2. doi: 10.1145/1143844.1143936.
URL http://doi.acm.org/10.1145/1143844.1143936. 2
John SchUlman, Filip Wolski, PrafUlla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy
optimization algorithms. CoRR, abs/1707.06347, 2017. URL http://arxiv.org/abs/
1707.06347. 3,7
LUcas Tian, Kevin Ellis, Marta Kryven, and Josh TenenbaUm. Learning abstract strUctUre for draw-
ing by efficient motor program indUction. Advances in Neural Information Processing Systems,
33, 2020. 2
Rodrigo Toro Icarte, Ethan Waldie, Toryn Klassen, Rick Valenzano, Margarita Castro, and Sheila
McIlraith. Learning reward machines for partially observable reinforcement learning. Advances
in Neural Information Processing Systems, 32:15523-15534, 2019. 2
MargUs Veanes, Pieter Hooimeijer, Benjamin Livshits, David Molnar, and Nikolaj Bjorner. Sym-
bolic finite state transdUcers: Algorithms and applications. SIGPLAN Not., 47(1):137-150, Jan-
Uary 2012. ISSN 0362-1340. doi: 10.1145/2103621.2103674. URL https://doi.org/10.
1145/2103621.2103674. 2, 3
Abhinav Verma, Vijayaraghavan MUrali, Rishabh Singh, PUshmeet Kohli, and Swarat ChaUdhUri.
Programmatically interpretable reinforcement learning. In International Conference on Machine
Learning, pp. 5045-5054. PMLR, 2018. 2
11
Under review as a conference paper at ICLR 2022
Yichen Yang, Jeevana Priya Inala, Osbert Bastani, Yewen Pu, Armando Solar-Lezama, and Martin
Rinard. Program synthesis guided reinforcement learning. CoRR, abs/2102.11137, 2021. URL
https://arxiv.org/abs/2102.11137. 2
Weichao Zhou and Wenchao Li. Safety-aware apprenticeship learning. In International Conference
on Computer Aided Verification,, pp. 662-680. Springer, 2018. 2
He Zhu, Zikang Xiong, Stephen Magill, and Suresh Jagannathan. An inductive synthesis framework
for verifiable reinforcement learning. In Proceedings of the 40th ACM SIGPLAN Conference on
Programming Language Design and Implementation, pp. 686-701, 2019. 2
Brian D. Ziebart, Andrew Maas, J. Andrew Bagnell, and Anind K. Dey. Maximum entropy inverse
reinforcement learning. In Proceedings of the 23rd National Conference on Artificial Intelligence
- Volume 3, AAAI’08, pp. 1433-1438. AAAI Press, 2008. ISBN 978-1-57735-368-3. URL
http://dl.acm.org/citation.cfm?id=1620270.1620297. 2
12
Under review as a conference paper at ICLR 2022
A Appendix
In this appendix, we will present additional experimental results; the design details of the SRMs
and the symbolic constraints used in the experiments; a detailed experimental setup including the
hyperparameters.
A.1 Additional Results
We show some addition experimental results in this section to answer the following questions.
E.	Can arbitrarily concretized SRM effectively train RL agents?
F.	How much do the performance of Algorithm 1 depend on the designs of the SRMs?
For question E, we randomly generate hole assignments that satisfy the symbolic constraints for the
SRMs of the DoorKey and KeyCorridor tasks. The SRMs are shown in Fig.8 and 9. The symbolic
constraints contain the relational predicates as shown in Table.1 and 2. Those SRMs and symbolic
constraints produce the main results in the main text. Now the assignments are generated by only
optimizing the supervised objective Jcon mentioned in the main text. The concretized SRMs are
used for training RL policies in the following large DoorKey and KeyCorridor environments.
•	DoorKey-16x16 . In Fig.5a, we test three 3 randomly generated hole assignments for
the SRM, each annotated by PPO(LSTM)_rand# . The PPO(LSTM) agents trained with
those SRMs achieve certain level of performance than that trained with the default re-
ward. However, the SRM concretized with a learned hole assignment, annotated by
PPO(LSTM)+SRM, enables the agent to attain much higher performance with much lower
amount of frames.
•	KeyCorridorS4R4 . we test 3 randomly generated hole assignments for the SRMs, each
annotated by AGAC(CNN)_rand#. As in Fig.5b, the agents trained with the SRMs with
random assignments do not perform at all. In contrast, the agent trained with the SRM that
is concretized with a learned hole assignment achieves high performance with comparable
amount of frames to that trained with the default reward.
For question F, as mentioned in the main text we design three SRMs for the ObstructedMaze task.
We will describe the difference between these SRMs in the next section. We run Algorithm 1 with
those SRMs in the ObstructedMaze-2Dhlb environment and compare the results in Fig.6. In Fig.7b,
we use those concretized SRMs to train RL agents in ObstructedMaze-Full. However, the SRM1
that achieves highest performance in Fig.7b is outperformed by two others.
Besides answering those two questions, we recall that we run Algorithm 1 in DoorKey and KeyCor-
ridor tasks without symbolic constraint and with weaker symbolic constraint in the ablation study
(a) DoorKey-16x16
Figure 5: AGAC/PPO+SRM indicates that the hole assignments are learned via Algorithm 1;
AGAC/PPO_rand# with an index # indicates that the holes are randomly assigned with some Val-
ues that satisfy the symbolic constraint for that task. CNN and LSTM indicate the versions of the
actor-critic networks.
(b) KeyCorridorS4R3
13
Under review as a conference paper at ICLR 2022
(a) ObstructedMaze-2Dhlb
(b) ObstructedMaze-Full
(a) DoorKey-8x8
Figure 7: Algo1+PPO(CNN) indicates using PPO as the policy learning algorithm in line 4 of Algo-
rithm 1; Algo1(W/o c)+PPO(CNN) indicates that running Algorithm 1 Without symbolic constraint
While using PPO(CNN) in line 4; Algo1(signonly)+PPO(CNN) indicates that running Algorithm 1
Without symbolic constraint While using PPO(CNN) in line 4;CNN indicates CNN version of the
actor-critic netWorks.
Figure 6: AlgoI(SRM#)+AGAC(LSTM) With an index # = 1 〜3 indicates running Algorithm 1
with those three designed SRMs and by using AGAC in line 4 of Algorithm 1. PPO/AGAC+SRM#
indicates training RL agents With SRM# by using PPO or AGAC algorithm. CNN and LSTM
indicate the versions of the actor-critic netWorks.
(b) KeyCorridorS4R3
of the main text. Under the same conditions, We vary the number of demonstrations and check the
number of frames needed for πA to attain high performance. In Fig.7a and Fig.7b, We shoW that
When the number of examples is reduced from 10 to 1, number of frames that Algorithm 1 needs to
produce a policy With average return of at least 0.8 are not severely influenced.
A.2 Design Details of the SRMs
In this section, We shoW the diagrams of the SRMs as Well as the symbolic constraints designed for
the tasks. We Will explain the design patterns in those SRMs in detail.
A.2. 1 DoorKey Task
For readers convenience, We shoW the diagram of the SRM for DooKey in Fig.8. This SRM im-
plicitly identifies an unlocking-door sub-task With tWo internal states “Before Unlocking” and
“After Unlocking”. The transitions are designed mostly based on high level human insights
represented in first order logic: a) (ReaCh_Goal@t) → ∃t1 < t.∃t2 < t1.(UnloCk_Door@t1) ∧
(Pick_up_Key@t2) where @t indicates that the predicate preceding it, e.g., Reach-Goal, oper-
ates on the time step t of the trajectory τ; b) ∀t ∈ [t1, t2 ].(Drop_Key@t1, Before Unlocking) ∧
(-Pick_up_Key@t) ∧ (Pick_up.Key@t2) → ∀t0 ∈ [t1, t2].(-Unlock-Door@t0) where we addi-
tionally integrate the internal state, i.e., “Before Unlocking”, next to @t1, to indicate the internal
14
Under review as a conference paper at ICLR 2022
Figure 8: The diagram of the SRM designed for the DoorKey task.
Properties
[μ1 ] Reward reaching the goal
[μ2] Penalize dropping unused key
[μ3] Reward unlocking door
[μ4] Penalty for closing door
[μ5] Mildly penalize door toggling
Predicates
(?id ≤ ?1)
id=1
?5+?4 ≤ 0
id=2
?3 ≤ 0
(?id ≤ ?2)
?3 + ?2 ≤ 0
5
5
Table 1: The correspondence between properties and atomic predicates for the DoorKey SRM in
Fig.1b
state at the time step t1. The predicate #CLOSEDOORx?3+?2 > 0 in Fig.1b is introduced with
due consideration of avoiding overly penalizing the agent for closing the door, which behavior is
redundant for the task. The underlying idea is: if the reward function penalized an under-trained RL
agent for every door closing behavior with some high penalty ?3 < 0 for a total of #CLOSEDOOR
amount of times, and the accumulated penalty #CLOSEDOORx?3 outweighed the reward ?2 > 0 for
unlocking the door, then the agent in practice might be inclined to reside away from the door for
good. The SRM in Fig.1b simply upper-bounds the accumulated penalty to avoid negative effects
in practice. Then we show the atomic predicates in the symbolic constraint for this task in Table.1.
The final symbolic constraint is C = V5=1 μi. We omit the explanation for the symbolic constraint
since the atomic predicates are self-explanatory.
A.2.2 KeyCorridor Task
We depict in Fig.9 the diagram of the SRM designed for this task. Due to the
added complexity in this task in comparison with the DoorKey task, two sub-tasks,
finding-key and unlocking-door, are implicitly established by using three internal states
“Before Finding Key”， “Before Unlocking” and “After_Unlocking”.	Some impor-
tant first order logic formulas that hold in most situations in the KeyCorridor task in-
clude: a) (PiCk-UpjTarget@t1) ∧ (UnloCk-Door@t2) → ∃t ∈ [t2, t1 ].(Drop_Key@t);
Drop_Key // ?3
Close_Door // ?(
/ Before
x* I Unlocking
(Open.Door)Λ(#OPENDOOR_POSTx(?8-?5)+ ?4> 0)∕∕7s^Γ^^
Unlock_Door// (?4 -#0PEND00R_P0ST_REWARDEDx?s )
Pick_Up_Key //(?2 -#OPENDOOR_PRE_REWARDEDx?3：
PiCkRP不咨二// ?6
ʌk Unlocking y
(Open.Door) A (#OPENDOOR_PREx(?8 -?5)+12 > O)//?5
Λ^^∕^tart)x
< / Before
∖ Finding
∖ Key
Pick_Up_Target//?^
Pick_Up_Target // 71
Close_Door // ?8
Close_Door // 7j
PiCkjJp_Target 〃？i

Figure 9:	The diagram of the SRM designed for the KeyCorridor task.
15
Under review as a conference paper at ICLR 2022
Properties	(Retaional) Predicates a	(Non-Relational) Predicates
[μι] Reward picking up ball	8 V (?id ≤ ?1)	?i ≥ 0
[μ2] Reward 1st time picking up key	id=2 ?2 ≥ 0	?2 ≥ 0
[μ3] Reward dropping used key	?3 ≥ 0	?3 ≥ 0
[μ4] Reward unlocking door	?4 ≥ 0	?4 ≥ 0
[μ5] Encourage opening door	?5 ≥ 0	?5 ≥ 0
底]Penalize meaningless move	?8 ≤ 0	?8 ≤ 0
[μ7] Moderately reward opening door	?5 一 ?8 ≤ ?2	
[μ8] Penalize dropping unused key	?2 + ?6 ≤ 0	?6 ≤ 0
[μ9] Penalize picking up used key	?3 + ?7 ≤ 0	?7 ≤ 0
Table 2: The correspondence between properties and the relational and non-relational atomic predi-
cates for the SRM of KeyCorridor in Fig.9
b) ∀t0 < t.(Pick_Up_Key@t) ∧ (—Pick_Up_Key@t0) → ∃t00 < t.(Open_a_Door@t00); C)
(Unlock_Door@t) → ∃t0 < t.(Open_a_Door@t00). Regarding the implication a, two predicates
Pick_Up_Key@t and Drop_Key@t are added at the internal state “After Unlocking” to govern
the rewards returned for their respectively concerned behaviors after the door is unlocked. As for
the implications b and c, the caveat is to determine the utility of each door opening behavior. A
designer may go to one extremity by rewarding every door opening behavior with some constant,
which, however, either represses exploration by penalizing opening door, or oppositely raises reward
hacking, i.e., agent accumulates reward by exhaustively searching for doors to open. Alternatively,
the designer may go to another extremity by carrying out a motion planning and specify the solution
in the SRM, which, however, is cumbersome and cannot be generalized. In this paper, we highlight
a economical design pattern to circumvent such non-determinism.
As shown in Fig.9, before the agent accomplishes the finding-key sub-task, i.e., in
the “Before Finding Key” internal state, once the agent opens a door, the predicate
#OPENDOOR_PRE X (?8-?5)+?2 > 0? checks whether the total reward gained from opening doors
is about to exceed a threshold. The counter #OPENDOOR-PRE counts the number of times that agent
opens doors prior to the agent finding the key; the variable ?8 is expected to be a penalty for the
agent closing a door, which is redundant. By introducing ?8, we specify that even if the agent
closed doors instead of opening doors for equal number #CLOSEDOOR-PRE ≡ #OPENDOOR_PRE
of times, the agent could still gain positive net reward by finishing the finding-key sub-
task, i.e., #CLOSEDO。R_PREx?&+?2 ≥ #OPENDO。R_PREx?5. When the agent accomplishes the
finding-key sub-task, i.e., transitioning to the “Before Unlocking” internal state, the reward
?2 一 #OPENDOOR_PRE_REWARDED× ?3 subtracts the reward hitherto gained from opening doors with
#OPENDOOR_PRE_REWARDED ≤ #OPENDOOR_PRE counting the number of times that door opening
behaviors are indeed awarded prior to the agent finding the key. In some sense, this approach amor-
tizes the reward ?2 for finishing the finding-key sub-task over the door opening behaviors. The
amortized reward #OPENDOOR_PRE_REWARDEDx?3 cannot exceed ?2 and should be deducted from
?2 . The same idea is adopted to award the door opening behaviors prior to the agent unlocking the
door. The counter #OPENDOOOR_POST in Fig.9 counts the number of times that agent opens doors
after the agent finding the key prior to the agent unlocking the door; #OPENDOOR_PRE_REWARDED
counts the number of times that door opening behaviors are awarded within that time interval. Ap-
parently, such design pattern is convenient enough to be implemented via symbolic means. The chal-
lenge, however, remains to properly determine values for ?id ’s. Then we show the atomic predicates
in the symbolic constraint for this task in Table.2. The final symbolic constraint is C = V9=1 μi.
A.2.3 ObstructedMaze Task
Fig.10 shows the diagram of the reward function designed for the ObstructedMaze task. Despite of
the complexity of task, there are only three internal states, “(Start)”, “After Seeing the Target”
and “(End)”. This is because only specifying the the sub-tasks is far from adequate for this task.
Once the “After Seeing the Target” state is reached, the reward function only concerns
whether the agent drops or picks up a key or the target. In the “(Start)” state, the reward
function views each door unlocking behavior as a milestone. If the agent does not unlock a
16
Under review as a conference paper at ICLR 2022
Target」n_Siggt 〃 % -#0PENDOOR.REWARDED X?4)
Unlock_Door // do Hindsight; return ?5
(Drop-Key)Λψ // ?10
Seeing the
(Drop.Ball)Λψ∕∕79
Close-Door//?3 ClosjDoor 〃 ？3
∖ Pick_Up_Key//?n
Seeing the
、Jarg*
(Open-Box)Λψ // ?6
/------- Drop_Key // ?12
(Pick-Up-Ball)Λψ∕∕77 (PiCk_UPJCey)Aψ 〃 ？8	Pick_Up_Target // 71
REnd 力
ψ: =?5 +#OPENBOXX% -?6 )+⅛PICKUPBALL ×(73 -?7) + #PlCKUPKEYX心-?8 )+#DROPBALLXQ3 -?9 )+#DROPKEYX出-?10) > O
Hindsight :=	let tθ = Last_Time_Unlock_Door	in	Replay_Buffer.Rewards[tO:t]= O
	let tl = Last_Time_Open_Right_Box	in	Replay_Buffer.Rewards [tl ] = ?6
	let t2 = Last_Time_Pick_Up_Right_Ball	in	Replay_Buffer.Rewards [t2] = ?7
	let t3 = Last_Time_Pick_Up_Key	in	Replay_Buffer.Rewards[t3] = ?8
	let t4 = Last_Time_Drop_Ball	in	Replay_Buffer.Rewards[t4] = ?9
	let t5 = Last_Time_Drop_Wrong_Ball	in	Replay_Buffer.Rewards[t5] = ?10
Figure 10:	The diagram of the SRM designed for the ObstructedMaze task.
door at the present time step, the SRM awards the following agent behaviors: opening a box,
picking up a ball, picking up a key, dropping a ball, dropping a key through a proposition
?5 + #OPENBOX × (?3-?6) + #PICKUPBALL × (?3-?7) + . . . > 0 which bounds the number of
times that the those behaviors are awarded. The counters #OPENBOX, #PICKUPBALL . . . only count
the number of respectively concerned behaviors between two successive door unlocking behaviors
by resetting themselves to 0 once the agent unlocks a door. Thus far the design pattern is still
similar to that adopted in the KeyCorridor tasks. What makes a difference here is that we assume
the SRM to have access to the replay buffer of the agent policy, annotated as RePlay-BUffer.
Suppose that in some time step t the agent unlocks a C colored door located at coordinate X,
the SRM locates the last time step when the agent unlocked a door. Then it reassigns the re-
wards to 0 for all the opening a box, picking up a ball, picking up a key, dropping a ball, drop-
Ping a key behaviors stored in RePlay-BUffer ever since that last door locking time step till
the present time step. Then it identifies the time steps of four milestone behaviors based on the
following human insights represented in first order logic: a) (UnloCkcColored_Door@t) →
∃t1 < t.(Open_Box@t1) ∧ (FindCColored-Key@t1 + 1), i.e., in time step t1 the agent opened
the box that contains the key for this C colored door; b) (UnloCkɪLoCatedjDoor@t) →
∃t2 < t.∀t2 > t2.(PiCk-UpɪLoCated-Ball@t2) ∧ (-Pick_Up_X_Located_Ball@t2), i.e., in
time step t2 the agent picked up the ball obstructing this door at position X for the last time;
c) (UnloCkcColored_Door@t) → ∃t3 < t.∀t3 ∈ [t3, t].(PiCk-UPcColored.Key@t3) ∧
(-Pick_Up_C_Colored_Key@t3), i.e., in time step t3 the agent picked up the key for this C col-
ored door for the last time; d) (UnloCk_Door@t) → ∃t4 < t.∀t4 ∈ [t4, t].(Drop_Ball@t4) ∧
(-Drop_Ball@t4), i.e., in time step t4 the agent dropped a ball for the last time. After identifying
those milestone time steps, the SRM rewards the behaviors at the corresponding time steps. The
intuition behind such design pattern is that the reward function simply encourages all those behav-
iors if it is unclear what outcome those behavior will lead to; once the agent unlocks a door, the
reward function is able to identify the milestone behaviors that are most closely related to the door
unlocking outcome. Then we show the atomic predicates in the symbolic constraint for this task in
Table.3. The final symbolic constraint is C = V1=21 μi.
Note that the stored reward is not to be confused with the reward output at the present time. The
syntax of sequencing in the Hindsignt code block depends on the language of the r term specified
in the background theory. For the other two SRMs annotated by SRM2 and SRM3 as mentioned
earlier, we remove the Hindsight block. Especially, in SRM2, we restrict that door unlocking and
door opening behaviors are rewarded if only the accumulated rewards gained from those two behav-
iors do not exceed ?2 . Otherwise, none of the behaviors correlated with the self-looping transitions
at state “Before_Seeing the_Target” in Fig.10 will ever be rewarded. A possible reason for the
policies trained by SRM1 do not generalize well in larger environment is that due to the hindsight
reward modification, the reward output is too sparse in the large environment for the agent to learn.
As shown by the experimental results of SRM2 and SRM3, once the Hindsight block is removed,
the training performance in large environment is improved.
17
Under review as a conference paper at ICLR 2022
Properties	(RetaionaD Predicates A C	(Non-Relational) Predicates
[μι∣ Reward picking up target	12 A (?id ≤ ?i) id=2	?i ≥ 0
[μ2∣ Reward finding target	?2 2?4+?5 — 2?3	?2 ≥ 0
[μ3∣ Reward opening door	?3 ≤ O	?3 ≤ 0
[μ4∣ Reward opening door	?4 ≥ O 10	?4 ≥ 0
[μ5∣ Reward unlocking door	?5 ≥ P ?id id=6	?5 ≥ 0
[μ6∣ Penalize meaningless move	?3 ≤ 0	?3 ≥ 0
[μ7∣ Penalize picking up used key	?11 +?12 ≤ 0	?11 ≤ 0
[μ8∣ Reward opening box	?6 ≥ 0	?6 ≥ 0
[μ9∣ Reward picking up ball	?7 ≥ 0	?7 ≥ 0
[μ 10] Reward picking up key	?8 ≥ 0	?8 ≥ 0
[μ11∣ Reward dropping ball	?9 ≥ 0	?9 ≥ 0
[μ12]Reward dropping used key	?12 ≥ 0	?12 ≥ 0
Table 3: The correspondence between properties and predicates for the SRM of ObstructedMaze
task in Fig.10
A.3 Training details
•	Training Overhead. We note that all the designed SRMs require checking hindsight ex-
periences, or maintaining memory or other expensive procedures. However, line 5 of Al-
gorithm 1 requires running all K candidate programs on all m sampled trajectories, which
may incur a substantial overhead during training. Our solution is that, before sampling
any program as in line 5 of Algorithm 1, we evaluate the result of [[L]](τA,i), which keeps
holes ? unassigned, for all the m trajectories. By doing this, we only need to execute the
expensive procedures that do not involve the holes once, such as the counter #OPENDOOR
and the reward modification steps in the Hindsight block in Fig.10. Then We use qφ to
sample K hole assignments {hk}kK=1 from H and feed them to {[[L]](τA,i)}im=1 to obtain
{{[lk := L[hk//-"/KU By replacing line 2 and line 5 with those two steps in
Algorithm 1, we significantly reduce the overhead.
•	Supervised Learning Loss. In Algorithm 1, a supervised learning objective Jcons is used
to penalize any sampled hole assignment for not satisfying the symbolic constraint. In
practice, since our sampler qφ directly outputs the mean and log-variance of a multivari-
ate Gaussian distribution for the candidate hole assignments, we directly evaluate the sat-
isfaction of the mean. Besides, as mentioned earlier, in our experiments we only con-
sider symbolic constraint as a conjunction of atomic predicates, e.g., C = ∧n=ιμi with
each μi only concerning linear combinations of the holes, we reformulated each μi into
a form ui (?) ≤ 0 where ui is some linear function of the holes ?. We make sure that
(ui(h) ≤ 0) — ([μi]∣(h) = >) for any hole assignment h. After calculating each %(h),
which is now a real number, we let Jcons (qφ) be a negative binary cross-entropy loss for
Sigmoid(ReLU ([ui (h), . . . , un(h)∣T))) with 0 being the ground truth. This loss penalizes
any h that makes ui(h) > 0. In this way Jcons(qφ) is differentiable w.r.t 夕.Besides, we
retain the entropy term H(qφ) extracted from the KL-divergence to regularize the variance
output by qφ.
Network Architectures. Algorithm 1 involves an agent policy πφ, a neural reward function
fθ and a sampler qφ. Each of the three is composed of one or more neural networks.
-Agent policy ∏φ. We prepare two versions of actor-critic networks, a CNN version
and an LSTM version. For the CNN version, we adopt the actor-critic network from
the off-the-shelf implementation of AGAC Flet-Berliac et al. (2021). It has 3 convo-
lutional layers each with 32 filters, 3×3 kernel size, and a stride of 2. A diagram of
the CNN layers can be found in Flet-Berliac et al. (2021). For the LSTM version, we
concatenate 3 identically configured convolutional layers with a LSTM cell of 32-size
state vector. The LSTM cell is then followed by multiple fully connected layers each
to simulate the policy, value and advantage functions. While AGAC contains other
18
Under review as a conference paper at ICLR 2022
Parameter	Value
# Epochs	4
# minibatches (πφ)	8
# batch size (fθ, qψ)	128
# frames stacked (CNN πφ)	4
# reccurence (LSTM πφ)	1
# recurrence (fθ)	8
Discount factor γ	0.99
GAE parameter λ	0.95
PPO clipping parameter	0.2
K	16
α	0.001
β	0.0003
η	1.e8
Entropy	1.e-2
Table 4: Hyperparameters used in the training processes
components Flet-Berliac et al. (2021), the PPO agent solely consists of the actor-critic
networks.
-Neural reward function fθ. The network is recurrent. It has 3 convolutional layers
each with 16, 32 and 64 filters, 2×2 kernel size and a stride of 1. The last convolutional
layer is concatenated with an LSTM cell of which the state vector has a size of 128.
The LSTM cell is then followed by a 3-layer fully connected network where each
hidden layer is of size 64. Between each hidden layer we use two tanh functions and
one Sigmoid function as the activation functions. The output of the Sigmoid function
is the logit for each action in the action space A. Finally, given an action in a state,
we use softmax and a Categorical distribution output the log-likelihood for the given
action as the reward.
-Sampler qφ. The input to qφ is a constant [1,..., 1]t of size 20. The sampler is a
fully-connected network with 2 hidden layers of size 64. The activation functions are
both tanh. Suppose that there are |?| holes in the SRM. Then the output of qφ is a
vector of size no less than 2|?|. The |?| most and the |?| least significant elements in
the output vector will respectively be used as the mean of the Gaussian and constitute
a diagonal log-variance matrix. Besides, We let qφ to output a value as the constant
reward for the dummy transitions. While we still return 0 instead of this constant as
the reward to the agent, we subtract every sampled h with this constant to compute
[[l]](τ) for Jsoft. This subtraction simulates normalizing [[l]](τ) in order to match the
outputs of fθ, which, as mentioned earlier, is always non-positive in order to match
log πE.
•	Hyperparameters. Most of the hyperparameters that appear in Algorithm 1 are summa-
rized as in Table.4. All hyperparameters relevant to AGAC are identical as those in Flet-
Berliac et al. (2021) although we do not present all of them in Table.4 in order to avoid
confusion. The hyperparameter η is made large to heavily penalize qφ when its output
violates the symbolic constraint c. Besides, we add an entropy term H(qφ) multiplied by
1e - 2 in addition to Jsoft and Jcon to regularize the variance output by qφ. The item
EntroPy refers to the multiplier for the entropy term H(qφ) as introduced earlier.
19
Under review as a conference paper at ICLR 2022
A.4 Derivation Of the Objective Functions
First, We derive the lower-bound of logp(0A, 1e ∣∏a, E, l) in Eq.5 as follows.
logP(0A, 1e ∣∏A,E,1)
log X P(τAlπA)P(τE IE) 〃 P(0A|TA； πA,fτA)P(IE |TE； πA,fTE )
τA ,τE
fτA ,fτE
≥E
TA rπA
TE rE
P(fτE ∣TE； l)p(fτA |ta； l)
P(0A|TA； πA, fτA)P(IE |TE ； πA,fTE )P(fTE |TE ； l)P(fTA |TA；川
fτA ,fτE
TA EnAh log // P(0a\tA； πA, fτA)P(IE ∖τE ； πA,fTE )p(fTA ∖τA; l)p(fTE ∖τE ； l)
TE 〜E	f f
fτA ,fτE
PfTAjTA f )p(fTE ITEi f) i
P(fτA |TA； f )p(fτE ∣TE ； f )」
≥ maxτ E	E1 八(log P(0A |TA； πA,fTA)P(IE ∖τE ； πA,fTE )
f TTErE fτA~p(ITAf)'
fTE ~P('|TE f ))
PfAiTAiIMfTE∖τE;I) )i
P(fτA ∖ta; f )p(fτE ∣TE； f 川
max E Jadv(De) - E	DKL(Pf(T)∖∖Pl(T))
f e 〜N	τ 〜∏a,E
We justify the usage of the stochastic version Ee〜N(o,i)[Jadv(De)], rather than the conven-
tional generative adversarial objective Jadv (D), by showing that one of the saddle point of
min max Ee〜N(o i)[Jadv(De)] is attained when f ≡ log∏e ≡ log∏a where we write ∏e in
πA f	,
proxy of E by assuming that the distribution of state-action pairs satisfies P(s, a∖πE) ≡ P(s, a∖E).
Theorem 1. Given a ∏e, min max Ee〜N(。⑴俚及⑷〜∏e [logDe(s,α)] + E(s,a)〜∏a [log(1 一
πA	f
D(s, a) , where D(s, a) :
exPf (S,aHe)
exp(f (s,a)+e)+∏A(a∣s)”
is optimal when f ≡ log πE ≡ log πA.
Proof. Firstly, we consider optimizing f under the condition of πA ≡ πE. Inspired by the proof of
optimality condition of Generative Adversarial Nets in Goodfellow et al. (2014), we introduce two
variables Xs,a = ys,a ∈ (0,1] to simulate p(s, a∖πa) = p(s, a∣∏E) for any s,a ∈ S ×A. Then we
prove that Ee〜N(o,i) [xs,a log -
stationary point at X§,a = Xs,a
—∙s,a∙exP(e)____+ y log
^s,a∙exp(e) + ys,a + ys,a g
ys,a
Xs,a∙exp(e)+ys,a
as a function of Xs,a has X
by computing its gradient w.r.t ^^s,a as follows.
Js,aEe~N(0J) [xs,a log Xs：SeXpeXp+ys,a + ys,a log
ys,a
xs,a ∙ exp(e) + ys
Ee〜N(0,1) [xs,a
xs,a ∙ exp(e) + ys,a exp(e)(XS,a ∙ exp(e) + ys,a) ― xs,a
Xs,α ∙ exp(e)
ys,a ∙
(Xs,a ∙ exp(e) + ys,a)2
xs,a ∙ eχp(e) + ys,a
,a
exp(2e) +
ys,a
-ys,a exp(e)
(Xs,a ∙ exp(e) + ys,a)2
Ee 〜N (0,1)
xs,ays,a∕xs,a - ys,a eχp(e)
xs,a ∙ eχp(e) + ys,a
(7)
When Xs,
e and -e
,a = xs,a = ys,a, Eq∙7 equals Ee〜N(0,1) [ 1+exp(ej
equal each other, and
1-exp(e)	1-exp(-e)
.—:-	- 
1+exp(e)	1+exp(-e)
)) . Note that the probabilities of sampling
Hence, Ee〜N(o,i)[ 1第(；)] = 0. Itcan
trivially proved that the gradient of Eq.7 w.r.t X is non-positive. Therefore, f ≡ log ∏e is a local
maximum.
—
20
Under review as a conference paper at ICLR 2022
Next, we consider optimizing πA under the condition of f
and p(s,a∣∏A) for any (s, a) ∈ SXA as
that Ee 〜N(0,1)
xs,a log
(s,a)∈S×A
xs,a∙exp(e)
Xs,a∙exp(e) + ys
xs,a, ys,a	∈
_ _i_ ys,a
a	xs,α +ys,a
≡ log ∏e. We denote p(s, a∣∏E)
(0, 1] for short. Then we show
log Xs,a∙eXP¾ + ys,a ] as a function of
{ys,a|(s, a) ∈ S × A} s.t.	ys,a = 1 has a stationary point at xs,a ≡ ys,a by computing
(s,a)∈S×A
the gradient of the Lagrangian of this constrained function as follows.
E
e〜N (0,1)
xs,a log
(s,a)∈S×A
Xsa ∙ exp(t)
xs,α ∙ exp(e) + ys,a
+ ys,a log
ys,a
Xs,a ∙ exp(e) + ys,a
L
—
Vys,a L
⇒
λ(	ys,a - 1)
(s,a)∈S ×A
E [log----------ysa---------+ xs，a(exp(e)- 1) ]- λ = 0
e〜N(0,1)	Xs,α ∙ exp(e) + ys,a Xs,a ∙ exp(e) + ys,α
∀(s,a) ∈SxA.	E [log----------ysɑ---------+ xs'a(exp(? - 1) ] = λ (8)
e〜N(0,1) L	Xs,α ∙ exp(e) + ys,a	Xs,a ∙ exp(e) + ys,aj
VλL
ys,a - 1 = 0
(s,a)∈S ×A
(9)
Suppose that λ = Ee〜N(0,1) [log 1+eXp(：) ] and xs,a = ys,a holds for any (s, a) ∈ S ×A. Then both
Eq.8 and Eq.9 hold. Hence, xs,a ≡ ys,a is a stationary point. It is also trivially provable that the
gradient of Eq.8 w.r.t ys,a is non-negative. Therefore, πA ≡ πE is a local minimum. In conclusion,
f ≡ log ∏e ≡ log ∏a is a saddle point.	□
We derive the lower-bound of the ELBO in Eq.6 as follows.
ELBO(q) = DKL [q(l)∣∣p(l)] + E^ [log p(0A, 1E ∣∏A, E,l)]
E n log E h 〃 p(0A∣Ta; ∏A,fτA )p(1E ∣TE ； ∏A,fτE )p(fτE |丁石；l)p(fτA |ta； l
l〜q	TA 〜∏A L
τE 〜E fTA ,fTE
—
DKL q(l)||p(l)
≥	max^EVJjadv(De)] -Eq ^LLpff(τ)∣∣Pi(τ))]— DKL hq(l)||p(l)i
T〜∏A ,E
= Jsof t (q , f ) + Jcon (q )
21