Under review as a conference paper at ICLR 2022
Learning Stochastic Representations
of Physical Systems
Anonymous authors
Paper under double-blind review
Ab stract
Learning representations of physical systems is an important problem at the in-
terface of statistical physics and machine learning. Recently, there has been a
growing interest in devising methods to analyze high-dimensional simulation data
generated by unbiased or biased samplers. As statistical physics systems con-
sisting of N 1 objects tend to have many degrees of freedom, dimensionality
reduction methods are of particular interest. Here, we use a new method, mul-
tiscale reweighted stochastic embedding (MRSE), to analyze handwritten digits
data sets and a biased trajectory of alanine tetrapeptide, and show that we can re-
construct low-dimensional representations of these data sets while retaining the
most informative characteristics of their high-dimensional representation.
1	Introduction
Learning low-dimensional representations of physical systems is a fundamental task in statistical
learning. Usually, such systems are high-dimensional as they consist of many objects, rendering
too many degrees of freedom to sift through. Because of that, data sets generated by unbiased
and biased simulations require dimensionality reduction. In the context of molecular simulations, a
dimensionality reduction of a statistical system amounts to finding generalized degrees of freedom,
the so-called collective variables (CVs). The CVs can be used for the analysis of the studied system,
or for biasing the CVs by enhanced sampling. Enhanced sampling methods aim at exploring the
long-timescale behavior of complex dynamical systems (Valsson et al., 2016). Some of widely
used techniques for enhancing the sampling of the CVs are metadynamics (Laio & Parrinello, 2002;
Barducci et al., 2008; Bussi & Laio, 2020), variationally enhanced sampling (Valsson et al., 2016),
and umbrella sampling (Torrie & Valleau, 1977; Maragakis et al., 2009).
Methods at the interaction of machine learning (ML) and statistical physics can be used to em-
bed the high-dimensional feature space (e.g., microscopic coordinates, distances, angles) into the
low-dimensional latent space. Several such techniques have been introduced to alleviate the dimen-
Sionality problem in simulations of complex systems, for instance, autoencoders (Wehmeyer & Noe,
2018), stochastic kinetic embedding (Zhang & Chen, 2018) and sketch-map (Ceriotti et al., 2011).
Recently, a new method has been proposed to that aim, multiscale reweighted stochastic embedding
(MRSE) (Rydzewski & Valsson, 2021), on which we focus in this work.
In this paper, we demonstrate the ability of MRSE to learn the low-dimensional embeddings spanned
in the latent space. We show that MRSE is applicable to data sets from molecular simulations and
a handwritten digits data set considered often to test dimensionality reduction methods. More im-
portantly, we show that MRSE can learn CVs of a statistical physics system, alanine tetrapeptide,
characterized by a biased data set, and bias the CVs on the fly during an enhanced sampling simula-
tion. This feature enables us to reconstruct the free energy surface of the physical system composed
of many metastable states of different sizes.
2	Generalized Degrees of Freed om: Collective Variables
We first introduce the concept of generalized degrees of freedom. In statistical physics, we consider
a physical system described entirely by microscopic coordinates, R ∈ R3N, where N is the num-
ber of atoms in the system, and a potential energy function U (R). To sample R of such systems
1
Under review as a conference paper at ICLR 2022
molecular dynamics may be used. In the canonical ensemble (NV T), the microscopic coordinates
at equilibrium follow the Boltzmann distribution:
e-βU(R)
P(R) = R dR e-βU(R),	⑴
where the inverse temperature (the inverse of the product of the Boltzmann constant and tempera-
ture) is β = (kBT)-1. Note that in the ML literature, β is often set to 1 so that the characteristic
Boltzmann function is e-U(R). Here, however, to be exact, we use the notation from the physics
literature. In Equation 1, the denominator is the so-called partition function Z = dR e-βU(R)
which normalizes the solution.
To obtain a more useful representation that has a lower number of degrees of freedom, we transform
the equilibrium distribution of the microscopic coordinates to the equilibrium marginal distribution
of generalized degrees of freedom, i.e., CVs (ValSSon et al., 2016), denoted below as Z ∈ Rd, where
d is the number of CVs. It is done by integrating out all other degrees of freedom:
P(Z) = /
dRδ[z - z(R)]P(R) = <δ[z - z(R)])
⑵
where δ[∙] is the Dirac delta function andh兀 is an ensemble average under the potential U(R).
Connected to calculating P(z) is the inverse problem of reconstructing the free energy F(z) =
-β-1 log P(z) from z. Another way to express the equilibrium probability distribution in the Z
space, is P(z) = e-βF(z) / R dz e-βF(Z), where U(R) is replaced by F(z) (see Equation 1).
One should note that the dependence of CVs on the microscopic degrees of freedom may either be
explicit or implicit, and so it is often helpful to initially map the microscopic degrees of freedom to
features (e.g., internal representation) and then to CVs (see Figure 1). In our formalism, features
may be any function depending on R. Therefore it is possible to base learning CVs on a set of
manually selected features instead of R. This kind of selection impacts finding CVs, however, it
is easier first to select a general set of features than a two- or three-dimensional CVs (including an
additional variable has a drastic impact on the convergence of an enhanced sampling simulation).
X(R)	fθ	]
3 N ≥ k	≫ d
Figure 1: Embedding function fθ, parametrized
by θ is Z ≡ fθ(x(R)), i.e., a feature sample of
dimension k ≤ 3N, as X may be as large as R, is
reduced to a latent sample of dimension d《k.
Here, the dimensionality reduction works by get-
ting a selection of CVs that retains characteristics
of the system.
following biased distribution at convergence:
When the free energy has many metastable
states separated by high free energy barriers
(》keT), the system is kinetically stuck within
a single state, rendering the simulation time to
sample the CV space too high to be simulated
directly (Valsson et al., 2016). In optimization
theory, this means that a local minimum can-
not be reached in a finite number of steps. We
can use enhanced sampling methods that apply
an additional bias potential V to the dynam-
ics to alleviate this sampling problem. Heavily
inspired by importance sampling, this concept
was first coined by Torrie & Valleau (1977), and
it is based on introducing V into a simulation,
regardless of its construction, which leads to the
Pv(z) = (δ[z -Z(R)DU+y,	⑶
where the system evolves now at the total potential U + V. Due to the non-physical characteristics
of V, each R carries a statistical weight defined as W(z) ≡ P(z)∕Pv (z) and has to be accounted
for when reweighting (“unbiasing”) the biased distribution.
The functional form of the weights depends on the type of V used in the simulation. For instance,
when the bias is static, W(z) = eβV(Z), otherwise the bias has to be modified by an additional
time-dependent constant. This is the case in well-tempered metadynamics (Laio & Parrinello, 2002;
Barducci et al., 2008) where the time-dependent bias potential is added to a simulation by period-
ically depositing Gaussian kernels at the current location in the CV space. As the Gaussian height
2
Under review as a conference paper at ICLR 2022
decreases over time and in the long-time limit approaches zero, the bias potential converges to the
free energy:
V(z,t →∞) = -(l- Y)F(z),	(4)
where γ > 1 is a bias factor. The time-dependence of the bias introduces a modification to the sta-
tistical weights: w(z, t) = exp[β(V(z, t) - c(t))], where c(t) is a time-dependent constant defined
as:
1	ʃdz exp γ-1 βV(z,t)
c(t)=石 log -------F---------
β	R dz exp Y-1 βV(z,t)
(5)
The statistical weights defined in this way will be important for learning a low-dimensional rep-
resentation of a physical system. For more details regarding reweighting methods, see (Tiwary &
Parrinello, 2015; Valsson et al., 2016).
Next, we describe how to construct the low-dimensional CVs based on a data set of collected feature
samples.
3 Learning Latent Variables
3.1	Feature Representation
We initially have a set {xk }kK=1 of feature samples. To each pair of feature samples xi and xj , we
assign a pairwise value modeled using the Gaussian kernel Gε(xi, xj) ∈ RK×K, characterized by
a scale parameter ε:
Gε(xi,xj) = exp(-εkxi - xjk2)	(6)
The distance ∣∣∙k between feature samples is measured using the Euclidean distance. Next, using
Eq. 6, we build a Gaussian mixture as a sum over different values of scale parameters ε = {ε} that
are unique for the i-th sample:
G(xi,xj) =	Gεi(xi,xj),	(7)
i
where each εi is estimated by fitting Gεi (xi, xj) to the data so that the Shannon entropy of the
Gaussian kernel is approximately log2 p. Here, p is a parameter called perplexity, defined as an
exponential of the Shannon entropy. We can view p as the effective number of neighbors in a
manifold. For details regarding the estimation ofεi, we refer to Rydzewski & Valsson (2021).
Then we can define an intermediate feature pairwise probability as:
_	G(xi, Xj)
gij = Pm GX, Xm) ,
where 1 ≤ i, j ≤ K is the number of feature samples and the diagonal elements are zeros.
(8)
Inspired by diffusion maps (Coifman et al., 2005; Coifman & Lafon, 2006; Coifman et al., 2008), we
want the representation to account for the data density. To this aim, we introduce to the feature rep-
resentation a density normalization factor, g(Xi) =	j G(Xi, Xj). Next, we construct a Laplacian
kernel L(Xi, Xj) as:
L(Xi, Xj)
G(Xi, Xj)
[g(xi)]α[g(xj )]α
(9)
where α is a parameter dependent on the studied system. For α = 2, the density normalization Cor-
responds to the Markov chain that is an approximation of the diffusion of a Fokker-Planck equation
with the density α e-βU, allowing to approximate the long-time behavior of a system described
by a certain stochastic differential equation. Other values of α are also possible, e.g., for α = 0,
we get the classical normalized graph Laplacian; for α = 1, we ignore the underlying probability
density (Coifman et al., 2005).
Using Eq. 9, we rewrite the feature pairwise probability distribution in terms of L:
L = (Iij) where lij = LL(XJ Xj)、,	(10)
m L(Xi, Xm)
where L is the Markov probability matrix that we base the feature representation on.
3
Under review as a conference paper at ICLR 2022
3.2	Latent Representation
To represent distances between low-dimensional latent variables zi and zj , we use a parametric
version of t-distribution kernel as in McInnes et al. (2018):
Q(zi,Zj) =(1 + a∣∣Zi - Zjk2b)T,	(11)
where a and b are parameters of the kernel. Then, the latent pairwise probability distribution is
defined:
Q = ( qj ) where qj = PQ(Zi,Zj)、,	(12)
m Q(Zi, Zm)
similarily to Eq. 10.
As shown in van der Maaten & Hinton (2008); van der Maaten (2009), a heavy-tailed t-distribution
used in the latent space overcomes the so-called “crowding problem” by grouping close data samples
and separating data samples that are far away from each other.
The latent variables are obtained via the embedding function, i.e., Zi = f(xi) for i denoting feature
samples in the training datasets.
3.3	Embedding Function
Embedding function fθ, parametrized with θ, is defined as Z = fθ(x(R)), i.e., a feature sample of
dimension k ≤ 3N , as x may be as large as R, is reduced to a latent sample of dimension d k
(Figure 1). As a parametric function, we use a deep neural network.
To find optimal θ for the embedding function, we use the following loss: D = DCE + DW , where
DCE is the cross-entropy between L and Q and DW is the Brownian correlation between x and Z.
Below we explain the loss in detail and provide an interpretation of each term.
The cross-entropy between L and Q is:
DCE=X lij log (⅛)+X(I- lij )log (⅛j
i6=j	i6=j
(13)
where DCE ≥ 0 and equal to zero if L = Q. Minimizing the cross-entropy enforces that the feature
pairwise probability distribution matches the latent pairwise distribution probability. Therefore, the
low-dimensional latent space has to have a probability between latent samples similar to their high-
dimensional counterpart.
The Brownian correlation between x and Z, DW is a coefficient suitable for finding a correlation
between stochastic processes and capable of indicating both linear and nonlinear correlation between
two random variables. DW is defined as (Szekely & Rizzo, 2009):
DW
CovW(x, z)
σ(X) σ(Z)
(14)
where CovW(x, z) denotes the Brownian covariance of X and Z such that 0 ≤ DW ≤ 1, and σ(∙) is
the standard deviation of X and Z, respectively. The minimization of DW increases the correlation
between the pairwise distances in the feature space and the latent space. The interpretation is that
such procedure increases the density conservation in the latent space. For details on how to calculate
CovW(x, Z), see Appendix A.
4	Results
4.1	UCI Handwritten Digits Data Set
Before we show the results for a physical system, we apply MRSE to the UCI handwritten digits data
set (Dua & Graff, 2017) to show that the method is not only applicable to simulation data. The data
set consists of 1797 digit images, each of size 8 × 8 pixels. Each class (from 0 to 9) has around 180
samples. The data set is downloaded using the scikit-learn library (Pedregosa et al., 2011).
4
Under review as a conference paper at ICLR 2022
[∣1E1E*HHUU
Figure 2: Low-dimensional embedding calculated for the UCI handwritten digits data set (https：
//archive.ics.uci.edu/ml/machine-learning-databases/Optdigits/). The data Set
consists of 1797 digit images, each of size 8 X 8 pixels.
The embedding function for this data set is modeled as a neural network of size [64, 5000, 2]
with the ReLU activation functions and the linear function for the output and is trained through
100 epochs using the Adam optimizer with default parameters. To construct the feature pairwise
probability distribution, we use α = 0 (Eq. 9) (e.g., a normalized graph Laplacian) and a Gaussian
mixture over the following perplexities: 256, 32, 4. For the latent pairwise probability distribution,
we use a = 1.93 and b = 1.58 as parameters in Eq. 11 which are found by fitting to the latent
pairwise distances using the limited-memory Broyden-Fletcher-Goldfarb-Shanno algorithm in the
initial steps of the training of the embedding function.
Overall, we find that MRSE correctly embeds the digits data set into a low-dimensional representa-
tion. Apart from few samples that are placed far from their main clusters, which may be due to the
low resolution of the digits (compare "3” and “5” in Figure 2), the majority of samples is grouped
well (Figure 2).
4.2	Physical System Data SET
As a physical system in this study, we take an alanine tetrapeptide (Ace-Ala3-Nme) data set from Ry-
dzewski & Valsson (2021). This data set is generated using gromacs 2019.2 patched with plumed
employing well-tempeted metadynamics to sample 6 dihedral angles and enhance the fluctuations
of the Φ dihedral angles (see Figure 3). For additional information about the simulation setup, see
Appendix B. The data set consists of 105 feature samples of sines and cosines of the dihedral angles
(2 × 6 features).
A neural network of size [12, 64, 2] is used to represent the embedding function. We use ReLU
activation functions apart from the output of the network that has a linear unit. The network is trained
using the Adam optimizer with default parameters during 100 epochs. For the feature pairwise
probability distribution, we use α = 2 (Eq. 9), to account for the long-time dynamics of our system.
The Gaussian mixture is constructed by averaging over the following perplexities: 256, 128, 64, 32,
16, 8, 4. For the latent space, we use a = 1 and b = 1 in Eq. 11.
As the feature samples resulting from the well-tempered metadynamics simulation are biased, we
need to reweight the feature pairwise probability distribution. Since each sample has a statistical
weight, we use them to redefine Eq. 8 (Rydzewski & Valsson, 2021):
G(xi, Xj) ≡ W(x(xi) w(xj) ∙ G(x” Xj)
(15)
where the weights correspond to the i-th and j -th samples, respectively. Apart from this change, the
rest of the protocol is without changes.
After the alanine tetrapeptide is trained, we run another well-tempered metadynamics simulation us-
ing the embedding to sample CVs on the fly during 100 ns. Figure 3 shows the obtained results. We
can see that the trained embedding can be used to sample additional conformations which aggregate
to the free energy surface shown in Figure 3. Several important metastable states characterizes the
free energy surface, similarily to restults obtained recently in the literature (Giberti et al., 2019; Ry-
dzewski & Valsson, 2021). Therefore, the embedding can be used to analyze the metastable states
visible in the latent space without looking at all combinations of the dihedral angles. Moreover,
the biasing of the learned low-dimensional embedding during a biased simulation can be used to
5
Under review as a conference paper at ICLR 2022
Figure 3: Low-dimensional embedding of the alanine tetrapeptide data set consisting of 105 feature
samples where each sample is of 12 dimensions. The sines and cosines of the Φ and Ψ dihedral
angles are used to describe each sample. The data set comes from a biased simulation run that en-
hances the fluctuations of the Φ dihedral angles using well-tempered metadynamics. The learned
CVS are subsequently used to enhance sampling during a 100-ns well-tempered metadynamics sim-
ulation. This setup enables us to see several metastable states in the free energy surface embedded
in the latent space. As We bias only two CVs, the simulation takes less computational time than
sampling the Φ dihedral angles.
reconstruct the free energy surface, which takes less computational time than biasing the Φ dihedral
angles to achieve an ergodic behavior.
5	Conclusions
In this short work, we show new results obtained by MRSE, which is used to learn the low-
dimensional embeddings of the UCI handwritten digits and alanine tetrapeptide data sets. Overall,
this work provides a generalization of MRsE to learn from synthetic ML data sets. Additionally, we
show the ability of the MRsE embeddings to be used during a biased simulation which is important
to calculate a free energy surface, arguably, one of the most informative characteristics of statistical
physics systems. The theory and experiments behind MRsE are a proof-of-principle. If they can
be extented to aid sampling of low-dimensional CVs in systems like proteins, it would be of great
benefit. This aspect will be explored in future work.
Ethics s tatement
The authors adhere to the ICLR Code of Ethics (https://iclr.cc/public/CodeOfEthics).
Reproducibility s tatement
MRsE is implemented in an additional module called LowLearner (Rydzewski & Valsson, 2021)
in a development version (2.7.0-dev) of the open-source plumed library (Tribello et al., 2014; The
PLUMED Consortium, 2019) and it uses the LibTorch library (PyTorch C++ API) (Paszke et al.,
2019). The initial version of the implementation along the alanine tetrapeptide data set are available
at Zenodo (DoI: https://doi.org/10.5281/zenodo.4756093).
Acknowledgments
To be added after peer review.
References
A. Barducci, G. Bussi, and M. Parrinello. Well-tempered metadynamics: A smoothly con-
verging and tunable free-energy method. Phys. Rev. Lett., 100(2):020603, 2008. doi:
Well-temperedmetadynamics:Asmoothlyconvergingandtunablefree-energymethod.
G. Bussi and A. Laio. Using metadynamics to explore complex free-energy landscapes. Nat. Rev.
Phys., pp. 1, 2020. doi: https://doi.org/10.1038/s42254-020-0153-0.
6
Under review as a conference paper at ICLR 2022
G. Bussi and M. Parrinello. Accurate Sampling using Langevin Dynamics. Phys. Rev. E, 75(5):
056707, 2007. doi: 10.1103/PhysRevE.75.056707.
M. Ceriotti, G. A. Tribello, and M. Parrinello. Simplifying the representation of complex free-energy
landscapes using sketch-map. Proc. Natl. Acad. Sci U.S.A., 108(32):13023-13028, 2011. doi:
https://doi.org/10.1073/pnas.1108486108.
R. R. Coifman and S. Lafon. Diffusion Maps. Appl. Comput. Harmon. Anal., 21(1):5-30, 2006.
doi: https://doi.org/10.1016/j.acha.2006.04.006.
R. R. Coifman, S. Lafon, A. B. Lee, M. Maggioni, B. Nadler, F. Warner, and S. W. Zucker. Geo-
metric Diffusions as a Tool for Harmonic Analysis and Structure Definition of Data: Diffusion
Maps. Proc. Natl. Acad. Sci. U.S.A., 102(21):7426-7431, 2005. doi: https://doi.org/10.1073/
pnas.0500334102.
R. R. Coifman, I. G. Kevrekidis, S. Lafon, M. Maggioni, and B. Nadler. Diffusion maps, reduction
coordinates, and low dimensional representation of stochastic systems. Multiscale Model. Simul.,
7(2):842-864, 2008. doi: https://doi.org/10.1137/070696325.
Dheeru Dua and Casey Graff. UCI Machine Learning Repository, 2017. URL https://archive.
ics.uci.edu/ml/machine-learning-databases/optdigits/.
F. Giberti, B. Cheng, G. A. Tribello, and M. Ceriotti. Iterative unbiasing of quasi-equilibrium
sampling. J. Chem. Theory Comput., 16(1):100-107, Nov 2019. ISSN 1549-9626. doi:
10.1021/acs.jctc.9b00907. URL http://dx.doi.org/10.1021/acs.jctc.9b00907.
B. Hess. P-LINCS: A Parallel Linear Constraint Solver for Molecular Simulation. J. Chem. Theory
Comput., 4(1):116-122, 2008. doi: https://doi.org/10.1021/ct700200b.
V. Hornak, R. Abel, A. Okur, B. Strockbine, A. Roitberg, and C. Simmerling. Comparison of
multiple Amber force fields and development of improved protein backbone parameters. Proteins,
65(3):712-725, Nov 2006. ISSN 1097-0134. doi: 10.1002/prot.21123. URL http://dx.doi.
org/10.1002/prot.21123.
A. Laio and M. Parrinello. Escaping free-energy minima. Proc. Natl. Acad. Sci. U.S.A., 99(20):
12562-12566, 2002. doi: https://doi.org/10.1073/pnas.202427399.
Paul Maragakis, Arjan van der Vaart, and Martin Karplus. Gaussian-mixture umbrella sampling. J.
Phys. Chem. B, 113(14):4664-4673, Apr 2009. ISSN 1520-5207. doi: 10.1021/jp808381s. URL
http://dx.doi.org/10.1021/jp808381s.
L. McInnes, J. Healy, and J. Melville. UMAP: Uniform Manifold Approximation and Projection for
Dimension Reduction. arXiv preprint arXiv:1802.03426, 2018. URL https://arxiv.org/
abs/1802.03426.
A. Paszke, S. Gross, F. Massa, A. Lerer, J. Bradbury, G. Chanan, T. Killeen,
Z. Lin, N. Gimelshein, L. Antiga, A. Desmaison, A. Kopf, E. Yang, Z. DeVito,
M. Raison, A. Tejani, S. Chilamkurthy, B. Steiner, L. Fang, J. Bai, and S. Chin-
tala. PyTorch: An Imperative Style, High-Performance Deep Learning Library.
NeurIPS,	33:8024-8035,	2019. URL url={http://papers.nips.cc/paper/
9015-pytorch-an-imperative-style-high-performance-deep-learning-library.
pdf}.
F.	Pedregosa, G. Varoquaux, A. Gramfort, V. Michel, B. Thirion, O. Grisel, M. Blondel, P. Pretten-
hofer, R. Weiss, V. Dubourg, J. Vanderplas, A. Passos, D. Cournapeau, M. Brucher, M. Perrot, and
E. Duchesnay. scikit-learn: Machine learning in Python. J. Mach. Lear. Res., 12(85):2825-2830,
2011. URL http://jmlr.org/papers/v12/pedregosa11a.html.
J. Rydzewski and O. Valsson. Multiscale Reweighted Stochastic Embedding: Deep Learning of
Collective Variables for Enhanced Sampling. J. Phys. Chem. A, 125(28):6286-6302, 2021.
G. J Szekely and M. L. Rizzo. BroWnian Distance Covariance. Ann. Appl. Stat., 3(4):1236-1265,
2009.
7
Under review as a conference paper at ICLR 2022
The PLUMED Consortium. Promoting transparency and reproducibility in enhanced molecular sim-
ulations. Nat. Methods,16:670-673,2019. doi: https://doi.org/10.1038/s41592-019-0506-8. For
the full list of researches from the PLUMED Consortium, see https://www.plumed-nest.
org/consortium.html.
P. Tiwary and M. Parrinello. A time-independent free energy estimator for metadynamics. J. Phys.
Chem. B, 119(3):736-742, 2015. doi: 10.1021/jp504920s. URL http://dx.doi.org/10.
1021/jp504920s.
G. M. Torrie and J. P. Valleau. Nonphysical sampling distributions in Monte Carlo free-energy
estimation: Umbrella sampling. J. Comp. Phys., 23(2):187-199, 1977. doi: https://doi.org/10.
1016/0021-9991(77)90121-8.
G.	A. Tribello, M. Bonomi, D. Branduardi, C. Camilloni, and G. Bussi. PLUMED 2: New feathers
for an old bird. Comp. Phys. Commun., 185(2):604-613, 2014. doi: https://doi.org/10.1016/j.cpc.
2013.09.018.
O.	Valsson, P. Tiwary, and M. Parrinello. Enhancing important fluctuations: Rare events and meta-
dynamics from a conceptual viewpoint. Ann. Rev. Phys. Chem., 67(1):159-184, 2016. doi:
10.1146/annurev-physchem-040215-112229.
L. van der Maaten. Learning a parametric embedding by preserving local structure. J. Mach. Learn.
Res., 5:384-391, 2009. URL http://proceedings.mlr.press/v5/maaten09a.html.
L. van der Maaten and G. Hinton. Visualizing data using t-SNE. J. Mach. Learn. Res., 9:2579-2605,
2008. URL http://www.jmlr.org/papers/v9/vandermaaten08a.html.
C. Wehmeyer and F. Noe. Time-lagged autoencoders: Deep learning of slow collective variables
for molecular kinetics. J. Chem. Phys., 148(24):241703, 2018. doi: https://doi.org/10.1063/1.
5011399.
J. Zhang and M. Chen. Unfolding hidden barriers by active enhanced sampling. Phys. Rev. Lett.,
121(1):010601, 2018. doi: 10.1103/PhysRevLett.121.010601.
A Diffusion Covariance
We start with the definition of the diffusion covariance. Let {xk }kK=0 and {zk }kK=0 be samples in
the feature and latent spaces, respectively. We fist compute matrices containing pairwise Euclidean
distances in both spaces:
xij ≡ kxi - xj k	(16)
and
zij ≡ kzi - zjk	(17)
for i, k = 1, . . . , K . Next, we perform double centering by:
Xcj = Xij - Xi - X∙j + X∙∙,	(18)
and
Zcj = Zij - Zi∙ - z∙j + z∙∙,	(19)
where X『(ZO)is the i-th row mean, Xj (Zj) is the j-th column mean, and X.. (Z..) is the grand
mean of pairwise distances. Then, the diffusion covariance of X and Z is defined as (Szekely &
Rizzo, 2009):
K K
XXXicjZicj.	(20)
i=1 j=1
CovW (x,z) = It
8
Under review as a conference paper at ICLR 2022
B S imulations of Alanine Tetrapeptide
To model the dynamics of alanine tetrapeptide in vacuum, we use the Amber99-SB force field (Hor-
nak et al., 2006). We perform the simulations in the canonical ensemble using a time step of 2 fs,
the stochastic velocity rescaling thermostat (Bussi & Parrinello, 2007) with temperature 300 K and
a relaxation time of 0.1 fs, and LINCS (Hess, 2008) to constrain hydrogen bonds. The simulations
are done without periodic boundary conditions and cut-offs for electrostatic and non-bonded van der
Waals interactions.
The data set for training is obtained by biasing the Φ backbone dihedral angles and a bias factor
of 5 using well-tempered metadynamics (Barducci et al., 2008). The simulation biasing the low-
dimensional embedding found by MRSE is also performed with a bias factor of 5. We use an initial
Gaussian height of 1.2 kJ/mol and deposit Gaussians every 1 ps in both simulations. We calculate
c(t) every 50 ps.
9