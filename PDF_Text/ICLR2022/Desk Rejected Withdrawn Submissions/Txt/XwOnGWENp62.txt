Under review as a conference paper at ICLR 2022
RitzNet: A Deep Neural Network Method for
Linear Stress Problems
Anonymous authors
Paper under double-blind review
Ab stract
Learning based method for physics related computation has attracted significant
attention recently. Effort has been devoted into learning a surrogate model which
simulates system behavior from existing data. This paper presents RitzNet, an
unsupervised learning method which takes any point in the computation domain
as input, and learns a neural network model to output its corresponding function
value satisfying the underlying governing PDEs. We focus on the linear elastic
boundary value problem and formulate it as the natural minimization of its as-
sociated energy functional, whose discrete version is further utilized as the loss
function of RitzNet. A standard fully connected deep neural network structure is
explored in this study to model the solutions of a system of elliptic PDEs. Nu-
merical studies on problems with analytical solutions or unknown solutions show
that the proposed RitzNet is capable of approximating linear elasticity problems
accurately. A parametric sensitivity study sheds light on the potential of RitzNet
due to its meshless characteristics.
1	Introduction
Stress analysis is a fundamental problem of computational engineering and physics, as failures of
most engineering components is usually due to stress. The subjects under stress investigation may
vary from a submarine pressure vessel to the fuselage of a jumbo jet aircraft, or from the legs of
an integrated circuit to the structure of a historical dam. The underlying governing equations for
stress analysis problem are the constitutive equations which expresses a relation between the stress
and strain tensor field, and the equilibrium equation under Newton’s law, these yield a system of
elliptic partial differential equations (PDEs). When the geometrical description of the structure or
the loading status are complicated, analytical (closed-form) solutions can not be obtained and one
must generally resort to numerical approaches such as the finite element, the finite difference, or the
finite volume method to solve a general linear stress problem.
In the last decade, Deep Neural networks (DNNs) have achieved astonishing performance in com-
puter vision, natural language processing, and many other machine learning (ML) related tasks. This
success encourages their wide applications to many other fields, including recent studies of (i) using
supervised ML algorithms to create a mid-fidelity surrogate model that learns the performances (e.g.
stress distribution) from rich traditional simulations or experimental observations and predicts their
distributions in real time (Liang et al., 2018; Gao et al., 2020; Wang et al., 2021; Vurtur Badarinath
et al., 2021; Iakovlev et al., 2021; Li et al., 2021); (ii) semi-supervised ML algorithms to enforce
the corresponding PDEs as a constraints or regularization term in the data-driven learning processes
for solution acceleration or simulation improvement (Long et al., 2019; Raissi et al., 2019); and
(iii) unsupervised ML approaches to directly approximate solutions of various types of known PDEs
(Sirignano & Spiliopoulos, 2018; Berg & Nystrom, 2018; E & Yu, 2018; Cai et al., 2020; 2021).
The unsupervised ML methods rely on no training data from previous simulations or experiments
and use only the underlying governing equations and/or boundary conditions as constraints to nu-
merically approximate the solutions of PDEs; they offers an alternative method to the existing nu-
merical schemes. Preliminary results have shown advantages of using DNNs for solving PDEs in
both high dimensions (Sirignano & Spiliopoulos, 2018) and low dimensions (Cai et al., 2021) for
computationally challenging problems. Although some initial investigation has been studied for
solving theoretical PDEs and have shown its efficacy, no previous work, to our best knowledge, has
1
Under review as a conference paper at ICLR 2022
been done for the systems of elliptic PDEs that describe linear stress problems; this motivate us to
explore the potential of using unsupervised learning method to simulate this ubiquitous engineer-
ing problem, and more important, since the learning based method does not require a mesh, it may
benefit the design optimization problem in which the traditional mesh based method may encounter
difficulties.
Neural network functions are nonlinear functions of the parameters, discretization of a PDE can be
set up as an optimization problem through either the natural minimization or manufactured least-
squares (LS) principles. Accordingly, existing methods consist of (1) the deep Ritz method (Cai
et al., 2020; E & Yu, 2018) using natural minimization and (2) the deep LS method (Berg & Nystrom,
2018; Raissi et al., 2019; Sirignano & Spiliopoulos, 2018; Cai et al., 2020) using manufactured least-
squares. In this paper, we propose RitzNet, an unsupervised ML method which solve linear elasticity
problems using DNN functions as the approximation model, and natural minimization principle
associated energy functional as the loss function. Section 2 reformulates the linear elasticity problem
into a minimization problem using Ritz method. Section 3 presents the RitzNet method in details
and we show our numerical studies in Section 4 and conclude the paper in Section 5.
2	RitZ Formulation of Linear Elasticity
Let Ω be a bounded computational domain in Rd (d = 2 or 3) with boundary ∂Ω = Γd ∪ Γn
and Γd ∩ Γn = 0, and let n be the outward unit vector normal to the boundary. Denote by U and
σ the displacement field and the stress tensor, respectively. Consider the following linear structure
problem
( -▽• σ = f,	in Ω,
[ σ(u) = 2μe(u) + λV ∙ U δd×d	in Ω
with boundary conditions
UlrD = gD and (σn)lrN = gN ,
where V∙ is the divergence operator; W(U) = 2 (VU + (VU)T) is the strain tensor; the f, g□, and
gN are given vector-valued functions defined on Ω, Γd, and Γn, representing body force, bound-
ary displacement and boundary traction force condition respectively; δd×d is the the d-dimensional
identity matrix; μ and λ are the material Lame constants.
We will use the standard notation and definitions for the Sobolev space Hs(Ω)d and Hs(Γ) for a
subset Γ of the boundary of the domain Ω ∈ Rd. The standard associated inner product and norms
are denoted by (∙, ∙)s,Ω,d and (∙, ∙)s,r,d and by ∣∣ ∙ ∣∣s,Ω,d and ∣∣ ∙ ∣∣s,r,d, respectively. When there is
no ambiguity, the subscript Ω and d in the designation of norms will be suppressed. When S = 0,
H0(Ω)d coincides with L2(Ω)d. In this case, the inner product and norm will be denoted by (∙, ∙)
and k ∙ k, respectively.
Since it is difficult for neural network functions to satisfy boundary conditions (see E & Yu (2018)),
as in Cai et al. (2020), we enforce the Dirichlet (essential) boundary condition weakly through the
energy functional. To this end, define the energy functional by
J(v)
(2)
=2 { L (2μ lε(v)l2 + λ ΙV ∙ vI2) dx + kv - gD ll∕2,ΓD } - (f, v) - (gN , v)0,ΓN .
Then the Ritz formulation of problem (1) is to find U ∈ H 1(Ω)d such that
J(U) =	min	J(v).	(3)
v∈H 1(Ω)d
For any u, V ∈ H 1(Ω)d, define the following bilinear form by
a(U, v)
σ(U)
Jω
: ε(v) dx + (U, v)1∕2,ΓD
2μ(e(U), e(v)) + λ(V ∙ u, V ∙ v) + (u, v)i∕2,Γd
2
Under review as a conference paper at ICLR 2022
and the linear form by
f(V) = (f, v) + (gN , V)0,Γn + (gD , V)I/2,Γd .
Problem (3)is equivalent to finding U ∈ H1 (Ω)d such that
a(u, v) = f(v),	∀ v ∈ H1 (Ω)d.	(4)
To establish the well-posedness of (3), we have the following modified Korn inequality.
Lemma 1. For all V ∈ H 1(Ω)d, there exists a positive constant C such that
kvk1,Ω ≤ C (kε(V) k0,Ω + kvk1∕2,ΓD ).	⑸
See proof in Appendix A.1
Proposition 1. Problem (3) has a unique solution u ∈ H 1(Ω)d Moreover, the solution u satisfies
the following a priori estimate:
lluk1,Ω ≤ C (kf k-1,Ω + kgD ∣∣1∕2,Γd + kgN ∣∣-1∕2,Γn).	⑹
See proof in Appendix A.2
3 RitzNet method
In this section, we describe the RitzNet which includes a standard fully connected deep neural net-
work as the model of function u(x), the discretized energy function for RiztNet loss and numerical
integration and differentiation operators. The structure of the RitzNet is illustrated in Figure 1.
Figure 1: RitzNet architecture. A fully connected L-layer network is used to generate the map
from an arbitrary point X in Ω to u(x), numerical operators are used to approximate the gradient,
divergence and integral in the discrete energy function for the RitzNet loss.
3.1	Deep Neural network structure
For j = 1, ∙∙∙ ,l - 1, let N(j): Rnj-I → Rnj be the vector-valued ridge function of the form
N(j)(x(j-1)) = σ(ω(j)x(j-1) - b(j)) for x(j-1) ∈ Rnj-1 ,	(7)
where ω(j) ∈ Rnj ×nj-1 and b(j) ∈ Rnj are the respective weights and bias to be determined;
x(0) = x; and σ(t) = max{0, t}p with positive integer p is the activation function and its ap-
plication to a vector is defined component-wise. This activation function is referred to as a spline
activation ReLUp. When p = 1, σ(t) is the popular rectified linear unit (ReLU). There are many
other activation functions such as (logistic, Guassian, arctan) sigmoids (see, e.g., Pinkus (1999)).
Let ω(l) ∈ Rd×nl-1 and b(l) ∈ Rd be the output weights and bias, respectively. Then a l-layer
neural network generates the following set of vector fields in Rd
MN(θ,l) = {ωl(N(IT) ◦…。N⑴(x)) - bl: ω⑶ ∈ Rnj XnjT, b⑶ ∈ Rnj for all j},	(8)
3
Under review as a conference paper at ICLR 2022
where the symbol ◦ denotes the composition of functions; θ denote all parameters to be trained, i.e.,
the weights {ω(j)}lj=1 and the bias {b(j)}lj=1. The total number of the parameters is given by
l
N = Md(L) = X nj × (nj-1 + 1) with nl = d.	(9)
j=1
This class of functions is rich enough to accurately approximate any continuous function defined on
a compact set Ω ∈ Rd (See Cybenko (1989); Homik et al. (1989) for the universal approximation
property). However, this is not the main reason why NNs are so effective in practice. One way to
understand its approximation power is from the point view of polynomial spline functions with free
knots (Schumaker (1981)). The set MN (θ, 2) may be regarded as a beautiful extension of free knot
splines from one dimensional scalar-valued function to multi-dimensional vector-valued function.
It has been shown that the approximation of functions by splines can generally be dramatically
improved if the knots are free.
3.2	RitzNet Method
Note that neural network functions in MN (θ, l) are nonlinear with respect to the parameters θ.
This implies that (1) cannot be discretized by the conventional discretization approach based on
the corresponding variational formulation (4). Instead, discretization using NNs must be based on
a optimization formulation. In this paper, we employ the Ritz formulation (3) that minimizes the
energy functional.
To approximate the solution of (1) using neural network functions, the RitzNet method is to min-
imize the energy functional over the set MN(θ,l), i.e., finding UN ∈ MN(θ,l) ⊂ H 1(Ω)d such
that
J(uN) = min	J (v).	(10)
v∈MN (θ,l)
Theorem 1. Let U ∈ H1 (Ω)d be the solution of problem (4), and let UN ∈ MN (θ, l) be a solution
of (10). Then we have
ku-uNka = inf ku - vka,	(11)
v∈MN (θ,l)
where ∣∣vka :=，a(v, V) is the energy norm.
Proof. Let U ∈ H 1(Ω)d be the solution of problem (4). For any W ∈ H 1(Ω)d, the definition of the
energy functional and (4) imply that
2(J(W) — J(U))	= α(w, W) — 2f (W) — a(u, u)+2f(u)
= a(w, w) - 2a(U, w) + a(U, U) = ∣U - w∣2a.
The above equality with W = UN ∈ MN(θ,l) ⊂ H 1(Ω)d yields
∣U — UN ∣2a = 2(J(UN) — J(U)) ≤ 2 (J(v) — J(U)) = ∣U — v∣2a
for any V ∈ Mn (θ, l), and hence the validity of (11). This completes the proof of the theorem. □
Theorem 1 indicates that UN is the best approximation with respect to the energy norm |卜|0.
3.3 Effect of Numerical integration
Evaluation of the energy functional requires integration and differentiation. In practice, they are
computed by numerical integration and differentiation.
For simplicity of presentation, we use the composite mid-point quadrature rule as in Cai et al. (2020).
To this end, let us partition the domain Ω by a collection of subdomains
T = {K : K is an open subdomain of Ω}
such that
Ω = ∪k∈tK and K ∩ T = 0,	∀ K, T ∈T.
4
Under review as a conference paper at ICLR 2022
That is, the union of all subdomains of T equals to the whole domain Ω, and any two distinct
subdomains of T have no intersection. The resulting partitions of the boundary ΓD and ΓN are
ED = {E = ∂K ∩ ΓD : K ∈ T} and EN = {E = ∂K ∩ ΓN : K ∈ T},
respectively.
Let xT and xE be the centroids of T ∈ T and E ∈ ES for S = D and N, respectively. For any
integrand v(x),the composite “mid-point" quadrature rules over the domain Ω and the boundary Γ,
are given by
v(x) dx ≈ X v(xT) |T| and	v(x) ds ≈	v(xE) |E|,
JQ	T ∈T	JS	E∈Es
respectively, where |T | and |E | are the respective volume of element T ∈ T and area of boundary
element E ∈ ES . Similarly, one may use any quadrature rule such as composite trapezoidal, Simp-
son, Gaussian, etc. The xT for all T ∈ T will be used as quadrature points which are fundamentally
different from sampling points used in the setting of standard supervised learning. At each xT, the
evaluations of ε (V(XT)) and V∙v(XT) are done through numerical differentiation with a small mesh
size or automatic differentiation denoted by
εh (V(XT)) and Nh ∙ V(XT).
Define the discrete counterpart of the energy function J(∙) by
JT (V) = I {x(2〃 归 h(v(XT ))∣2 + λ |Nh ∙ V(XT )|2)dx + YD X Iv - gD∣2(xE )
1T ∈T	E∈Ed
一 £ f ∙ V)(XT )-£ (gN ∙ V)(XE ),	J2)
T∈T	E∈EN
where γD is a relatively large positive constant. The RitzNet approximation to the solution of (1) is
to seeking uT ∈ MN (θ, l) such that
JT(uT) =	min	JT (V).	(13)
v∈MN (θ,l)
To understand the effect of numerical integration, we extend the first Strang lemma for the Galerkin
approximation over a subspace (see, e.g, Ciarlet (1978)) to the Ritz approximation over a subset. To
this end, define the discrete counterpart of the bilinear and linear forms by
a，T (u, v) = 2μ £ (ε%(u): eh(v))(Xt ) + λ ɪ2 Wh，〃▽% ㈤)(XT) + YD E (U ∙ V)(XE)
T∈T	T∈T	E∈ED
and fT(V)= E(f∙ V)(XT) + E (gN ∙ V)(XE) + Yd E(9d ∙ V)(XE).
T∈T	E∈EN	E∈ED
Theorem 2. Assume that there exists a positive constant β independent of MN (θ, l) such that
β kVk2a ≤ aT(V, V),	∀ V ∈ MN (θ, l).	(14)
Let u be the solution of (3) and uT a solution of (13). Then there exists a positive constant C such
that
ku-uTka
≤ C inf (ku- Vka + sup	Ia(V, w1- WG - aT (v, w1- w2)|
v∈M N (θ ,l) [	wi ,W2∈M N(θ,l)	kw1 -w2ka
+C
w1
If (wi - W2)- fτ(W1 - W2)|
W2^Mn (θ,l)	kwi- w2ka
(15)
The proof of Theorem 2 is attached in Appendix A.3 and it indicates that the total error in the
energy norm is bounded by the approximation error of the set of neural network functions plus the
numerical integration error.
5
Under review as a conference paper at ICLR 2022
4 Numerical Studies
In this section, we present our numerical results for two dimensional stress problems. The first
test problem has a closed-form solution by which we test the accuracy of the RitzNet method; and
the second is a real engineering benchmark problem with unknown analytic solution. We use the
approximated solution obtained from a finite element methods with adaptive polynomial order as the
baseline for comparison. We further explore the potential of using RitzNet as a design optimization
method through the parametric study of a changing parameter.
In the experiments, the network structure is expressed as 2-n1-n2 ∙ ∙ ∙ nl-1-2 for a l-layer network
with n1 , n2 and nl-1 neurons in the respective first, second, and (l - 1)th layers, and the first and
last numbers represent the network input and output dimensions. The minimization of the RitzNet
loss function (10) is numerically solved using the Adam version of gradient descent (Kingma & Ba,
2015) with a varying learning rate.
4.1	A toy problem with closed-form solution
Consider problem (1) defined on Ω = (-1,1) X (-1,1) with the body force
f = 2μ(3 — x2 — 2y2 — 2xy, 3 — 2x2 — y2 — 2xy)T + 2λ(1 — y2 — 2xy, 1 — x2 — 2xy)T,
and the traction
gN = 2(y2 -I) (2μ + λ,μ)
on ΓN = {(1,y) : y ∈ (—1,1)}, with the clamped boundary condition on ΓD = ∂Ω \ ΓN. The
exact solution of the test problem has of the form
u(x,y) = (I- χ2 )(1 — y2 )(1, I)T.
This function u(x, y) and the corresponding stress tensor σ = [[σxx , τxy], [τyx σyy]]T are depicted
in Figure. 2.
A three layer RitzNet of structures 2-32-32-2 is tested, and a ReLU 2 activation function is selected
to obtained a better smoothness of the approximated displacement field u. For numerical integration,
we use an uniformly distributed quadrature points of size 200X 200 to approximate all integrals in the
loss function. And all the numerical differentiation is approximated by the forward finite difference
quotient with step size 0.001.
Table. 1 list the numerical results of using RitzNet to solve this problem under varying material
properties. With a small network of total 1186 parameters, RiztNet can approximate this problem
accurately. The graphical results are depicted in Figure. 3 for material property μ = 1, and λ = 1,
which conform to the exact solution listed in Figure.2 accurately.
(a) ux = uy
(b) σxx
(c) σyy
(d) Stress τxy = τyx
Figure 2: Test Problem 4.1 (μ = 1, and λ = 1), exact solution of displacement and stress distribu-
tions.
4.2	A two-dimensional quadratic membrane under tension
The second problem is given by a quadratic membrane of elastic isotropic material with a circular
hole in the center. Traction forces act on the upper and lower edges of the strip, body forces are
ignored. Because of the symmetry of the problem, it suffices to compute only a fourth of the total
geometry. The computational domain is then given by
Ω = {x ∈ R2 : 0 < x1 < 10, 0 < x2 < 10, x2 + x2 > 1}.
6
Under review as a conference paper at ICLR 2022
Table 1: Numerical results of RitzNet method for test problem 4.1 with several choices of material
constant λ, and comparison with the exact solutions.
		Exact			RitzNet (2-32-32-2)			
		max ∣∣uk	max σ 11	max ∣uN ∣	max σN11	ku - UN ka kuka
λ	=1	17142	6	1.4147	6.19417	0.11280
λ =	10	1Λ142	24	1.4415	24.2099	0.08994
λ =	100	1.4142	-	204	-	1.4264	226.039 -	0.07342 -
Figure 3: Test Problem 4.1 (μ = L and λ = 1), approximated solution using RitZNet (2-32-32-2,
activate function: ReLU2, γD =1e+3, total number of iterations: 30000, learning rate: starts with
2e-2, decays 50% every 5000 iterations
The boundary condition on the top edge of the computation domain
(Γ1 : {x2 = 10, 0 < x1 < 10}) are set to σn = (0, 4.5)T , the
boundary condition on the bottom (Γ2 : {x2 = 0, 1 < x1 < 10}) are
set to (σ11, σ12) ∙ n = 0, u2 = 0 (symmetry condition), and finally, the
boundary condition on the left (Γ3 : {x1 = 0, 1 < x2 < 10}) are given
by (σ21, σ22) ∙ n = 0, and u1 = 0 (symmetry condition). The material
parameters are E = 206900 for Young’s modulus and ν = 0.29 for
Poisson,s ratio, and their relation with the Lame constants is given by
E	EV
μ=W+T) and λ =(1 +v )(1- 2ν).
This is a benchmark problem taken from (Cai & Starke, 2004) as a real
plane stress problem with stress concentration. Since there is no exact
solution available, to evaluate the accuracy of the proposed RitZNet
method, we use the adaptive finite element analysis (FEA) method to
Figure 4: Computational
domain and boundary condi-
tions.
compute a benchmark solution. The discretiZation of the computation domain is through high-order
p-element given in Figure. 5, and the adaptive process starts at cubic elements and stops at the
highest edge polynomial order of 5. The resulting reference numerical solution is given in Figure.
5. The stress concentration is located at point (0,1) where the stress has a sharp transition locally
due to the presence of the small hole.
For RitZNet, we first tested a three-layer RitZNet equipped with a sigmoid activation function
(ReLU2 activation function provide slightly inferior results). The quadrature points are set at the
mid-points of uniformly distributed partition of the domain with siZe 0.04; and the quadrature points
in the bottom left quarter are refined with a smaller siZe 0.01 to capture the geometric curvature
7
Under review as a conference paper at ICLR 2022
(a) Displacement magnitude
kuk
(b) Displacement field u
(c) von Mises stress σvm
distribution
Figure 5:	Test Problem 4.2: Benchmark solution using FEA adaptive p-element with maximum
element order of 5, convergence at 1% w.r.t the local stress and strain energy.
(a) Displacement magnitude
kuNk
(b) Displacement field uN
(c) von Mises stress σNvm dis-
tribution
Figure 6:	Test Problem 4.2: Numerical solution using RitzNet (NN structure: 2-32-32-32-2, activate
function: sigmoid, γD =1e+7, total number of iterations: 80000, learning rate: starts with 1e-1,
decays 80% every 20000 iterations.)
near the hole. We experimented three network sizes (see Table. 2) and all the resulting displace-
ment field solutions are close to the benchmark solution. For the stress concentration point and
the corresponding maximum von Mises stress σvm , we found larger network size performs better
w.r.t the approximation of this near singular point’s stress. A four layer network with 32 neurons in
each hidden layer can archive closer maximum σvm , see the last row in Table 2. One problem we
encountered is that beyond this 2-32-32-32-2 network structure, we are not able to further increase
the estimated value of the maximum σvm comparing to the benchmark solution. One explanation
is that when the network gets larger, it also introduces more and more local minimums for this non-
convex nonlinear optimization problem, and thus increases the training difficulties. Another factor
that might play a role here is the numerical quadrature. An adaptive quadrature point selection might
help for getting a more precise approximation of the energy functional for the numerical integration.
Figure. 6 depicts the displacement magnitude, displacement vector field and von Mises stress distri-
bution using RitzNet 2-32-32-32-2. It verifies that the proposed RitzNet is capable of approximating
the independent variable u of this challenging problem, although there is still improvement space
exist for problems with stress concentration/singularity.
Table 2: A comparison of FEA and RitzNet results for test problem in 4.2
Method	max ∣∣uk	max u1	max u2	max σVm
FEA (Adaptive p-element)	2.287158e-04	-6.985006e-05	2.287158e-04	13.60922
RitZNet (2-16-16-2)	1.939397e-04	-6.962712e-05	1.939397e-04	8.200386
RitZNet (2-32-32-2)	2.110047e-04	-8.007632e-05	2.110047e-04	9.017071
RitZNet (2-42-42-2)	2.081562e-04	-8.569460e-05	2.081561e-04	9.686732
RitZNet (2-32-32-32-2)一	2.274459e-04^	-8.232628e-05~	2.120236e-04^	12.19299
8
Under review as a conference paper at ICLR 2022
4.3 Parametric design study
In the last experiment, we explore the potential of using RitzNet for solving parametric PDEs. A
common scenario in engineer design is to make decision in a space of design parameters referencing
the simulation results for a series of choices. For example, engineers ask what is the largest hole
size allowed to use such that the maximum stress is still within the material’s limit.
To this end, we conducted a preliminary sensitivity study by varying the hole size from R1 to R5,
taking a step of 0.5. Using the trained model of RitzNet 2-32-32-2 in the previous experiment as a
starting point, we step through the various hole size by continuously training a same RitzNet with
small number of iterations for each step. In particular, the initial model in the previous experiment
took 80000 iterations, while in this sensitivity study, for each step, it continues from the previous
step RitzNet model, and takes only 5000 iterations to converge to the results depicted in Figure.
7. Changing hole size results in a varying computational domains. For RitzNet, this only affects
the set of quadrature points which can be easily added or removed. While in the traditional mesh-
based methods, changing of domain might result in mesh conformity issues, thus introduces extra
difficulties. Our results at each step conform to the numerical solutions evaluated in FEA method.
Figure. 7 lists the displacement and the associate stress simulation results obtained through this
continued RitzNet method. Comparing to the adaptive FEA, for instance, when the hole radius r= 4,
our results show maximum von Mises stress of 19.825 while using adaptive p-element FEA solver,
we obtain a results of 21.519.
(a) r = 2,
max kuk =2.2160e-4,
max σvm =12.6202
(b) r = 3,
max kuk =2.2676e-4,
max σvm =14.7910
(c) r = 4,
max kuk =3.2667e-4,
max σvm =19.8250
(d) r = 5,
max kuk =4.4880e-4,
max σvm =23.5357
Figure 7:	Parametric study of the problem in 4.2 with varying hole radii, using RitzNet with contin-
uous model.
5 Conclusion
Learning to solve PDEs is still in an early research stage; and this works provides a preliminary
investigation of using DNN functions and physical system’s natural minimization principle to nu-
merically solve linear elasticity problems. Theoretical studies ensure the RitzNet method is mathe-
matically valid while numerical studies on some two dimensional test problems show initially, the
efficacy of the proposed method. However, there are still many open issues awaiting further re-
search. For examples, how to select an appropriate network structure, what is an efficient way to
handle singularities or stress concentrations, and how to set up a good initial of the network model
and what is a best stopping criteria for the network training.
Perhaps the most exciting future application of the RitzNet is in shape optimization or even topology
optimization since RitzNet involves no explicit mesh based discretization. Our initial sensitivity
studies on a single parameter show promises which will be further explored in depth in our future
work.
9
Under review as a conference paper at ICLR 2022
References
Jens Berg and Kaj Nystrom. A unified deep artificial neural network approach to partial differential
equations in complex geometries. NeurocomPuting, 317:28-41, 2018.
Zhiqiang Cai and Gerhard Starke. Least-squares methods for linear elasticity. SIAM Journal on
Numerical Analysis, 42(2):826-842, 2004.
Zhiqiang Cai, Jingshuang Chen, Min Liu, and Xinyu Liu. Deep least-squares methods: An unsu-
pervised learning-based numerical method for solving elliptic pdes. Journal of ComPutational
Physics, 420:109707, 2020.
Zhiqiang Cai, Jingshuang Chen, and Min Liu. Least-squares ReLU neural network (LSNN) method
for linear advection-reaction equation. Journal of ComPutational Physics, 443:110514, 2021.
Philippe G. Ciarlet. The finite element method for elliPtic Problems. Society for Industrial and
Applied Mathematics, 1978.
George Cybenko. Approximation by superpositions of a sigmoidal function. Mathematics of Con-
trol, Signals, and Systems, 2:303-314, 1989.
Weinan E and Bing Yu. The deep ritz method: A deep learning-based numerical algorithm for
solving variational problems. Communications in Mathematics and Statistics, 6(1):1-12, 3 2018.
Wenli Gao, Xinming Lu, Yanjun Peng, and Liang Wu. A deep learning approach replacing the
finite difference method for in situ stress prediction. IEEE Access, 8:44063-44074, 2020. doi:
10.1109/ACCESS.2020.2977880.
Kur Hornik, Maxwell Stinchcombe, and Halber White. Multilayer feedforward networks are uni-
versal approximators. Neural Networks, 2:359-366, 1989.
Valerii Iakovlev, Markus Heinonen, and Harri Lahdesmaki. Learning continuous-time {pde}s from
sparse data with graph neural networks. In International Conference on Learning RePresenta-
tions, 2021. URL https://openreview.net/forum?id=aUX5Plaq7Oy.
Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In International
Conference on RePresentation Learning, San Diego, 2015.
Zongyi Li, Nikola Borislavov Kovachki, Kamyar Azizzadenesheli, Burigede liu, Kaushik Bhat-
tacharya, Andrew Stuart, and Anima Anandkumar. Fourier neural operator for parametric partial
differential equations. In International Conference on Learning RePresentations, 2021. URL
https://openreview.net/forum?id=c8P9NQVtmnO.
Liang Liang, Minliang Liu, Caitlin Martin, and Wei Sun. A deep learning approach to estimate
stress distribution: a fast and accurate surrogate of finite-element analysis. Journal of The Royal
Society Interface, 15(138):20170844, 2018.
Zichao Long, Yiping Lu, and Bin Dong. Pde-net 2.0: Learning pdes from data with a numeric-
symbolic hybrid deep network. J. ComPut. Phys., 399, 2019.
Allan Pinkus. Approximation theory of the mlp model in nueral networks. Acta Numerica, 15:
143-195, 1999.
Maziar Raissi, Paris Perdikaris, and George Em Karniadakis. Physics-informed neural networks:
A deep learning framework for solving forward and inverse problems involving nonlinear partial
differential equations. Journal of ComPutational Physics, 378:686-707, 2019.
Larry Schumaker. SPline Functions: Basic Theory. John Wiley, New York, 1981.
Justin Sirignano and Konstantinos Spiliopoulos. DGM: A deep learning algorithm for solving partial
differential equations. Journal of ComPutational Physics, 375:1139-1364, 2018.
Poojitha Vurtur Badarinath, Maria Chierichetti, and Fatemeh Davoudi Kakhki. A machine learning
approach as a surrogate for a finite element analysis: Status of research and application to one
dimensional systems. Sensors, 21(5), 2021.
10
Under review as a conference paper at ICLR 2022
Yinan Wang, Diane Oyen, Weihong Guo, Anishi Mehta, Cory Braker Scott, Nishant Panda, M.
Giselle Fernandez-Godino, GoWri Srinivasan, and XiaoWei Yue. Stressnet - deep learning to
predict stress with fracture propagation in brittle materials. npj Materials Degradation, 5(1),
2021.
A	Appendix
A.1 Proof of Lemma 1
Proof. For simplicity, We prove the validity of (5) in R2 . To this end, denote the space of infinitesi-
mal rigid motions in R2 by
RM = {v = (a, b)T + c(y, x)T | a, b, c ∈ R}.
For any V ∈ H 1(Ω), there exists a unique pair (z, W) ∈ H 1(Ω) X RM such that
v = z + w,
where H 1(Ω) = {v ∈ H 1(Ω)∣ 儿 V = 0, Rω ▽* V = 0}∙ By the second Korn inequality and the
fact that ε(w) = 0 for any w ∈ RM, We have
llzk1,Ω ≤ C kε(Z)k1,Ω = C kε(V)k1,Ω .	(16)
By the fact that RM is a finite dimensional space and the triangle and trace inequalities, We have
IlwII 1,Ω ≤ CkwkI/2,∂Ω ≤ C ( kvk1∕2,∂Ω + kzk1∕2,∂Ω ) ≤ C ( kvk1∕2,∂Ω + ∣∣zk1,Ω )∙
Now, (5) is a direct consequence of the triangle inequality and the above two inequalities. □
A.2 Proof of Proposition 1
Proof. By the Korn inequality in (5), it is easy to show that the bilinear form a(∙, ∙) is coercive in
H 1(Ω)d × H 1(Ω)d; i.e., for all V ∈ H 1(Ω)d, there exists a positive constant α > 0 such that
α kVk2,Q ≤ a(V, V) = 2μkε(V) k0,Ω + λkV∙ v∣∣0,ω + kVk2/2,rD .	(17)
It follows from the Cauchy-Schwarz and the trace inequalities that the bilinear form a(∙, ∙) and
the linear form f (∙) are continuous in H 1(Ω)d × H 1(Ω)d and H 1(Ω)d, i.e., there exist positive
constants M and C such that
Ia(U, v)| ≤ Mkuk1,Ω kMkm	(18)
and that
If(V)I ≤ C (kf k-1,Ω + kgD ∣∣1∕2,Γd + kgN ∣∣-1∕2,Γn) IWIIl.	(19)
Now, the Lax-Milgram lemma implies that problem (3) has one and only one solution in H 1(Ω)d.
The a priori estimate in (6) is a direct consequence of (4) with V = u, (17), and (19). This completes
the proof of the proposition.	□
A.3 Proof of Theorem 2
Proof. For any V ∈ MN(θ,l), it is easy to see that u『一 V ∈ H 1(Ω)d. By the assumption in (14),
the definition of JT(∙), and the relations:
JT (uT) ≤ JT (V) and a(u, uT 一 V) = f(uT 一 V),
we have
β2
2 kuT 一 Vka ≤
≤
-aτ(uT 一 V, uτ 一 v) = JT(uT) 一 JT(v) + fτ(uT 一 v) 一 aτ(v, uτ 一 v)
fT (uT 一 V) 一 aT (V, uT 一 V)
fT (uT 一 V) 一 f(uT 一 V) + a(V, uT 一 V) 一 aT (V, uT 一 V)
+a(u 一 V, uT 一 V)
11
Under review as a conference paper at ICLR 2022
which, together with the Cauchy-Schwarz inequality, implies
kuT - vk2a ≤ C ku — vk2a +	sup
w1,w2∈MN (θ,l)
|a(v, wi — W2)— &T (v, wi — W2)|
kw1 - w2ka
+ C sup
w1, w2 ∈MN (θ,l)
If (wi — W2)— fτ (wi — W2)|
∣∣wi — w2ka
Combining the above inequality with the triangle inequality
ku-uTka ≤ ku-vka+ kv-uTka
and taking the infimum over all v ∈ MN (θ, l) yield (15). This completes the proof of the theorem.
□
12