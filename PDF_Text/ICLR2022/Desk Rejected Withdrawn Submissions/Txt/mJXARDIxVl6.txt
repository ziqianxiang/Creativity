Under review as a conference paper at ICLR 2022
Optimistic Policy Optimization is Provably Ef-
ficient in Non-stationary MDPs
Anonymous authors
Paper under double-blind review
Ab stract
We study episodic reinforcement learning (RL) in non-stationary linear kernel
Markov decision processes (MDPs). In this setting, both the reward function and
the transition kernel are linear with respect to the given feature maps and are al-
lowed to vary over time, as long as their respective parameter variations do not
exceed certain variation budgets. We propose the periodically restarted optimistic
policy optimization algorithm (PROPO), which is^an optimistic policy optimiza-
tion algorithm with linear function approximation. PROPO features two mecha-
nisms: sliding-window-based policy evaluation and periodic-restart-based policy
improvement, which are tailored for policy optimization in a non-stationary envi-
ronment. In addition, only utilizing the technique of sliding window, we propose
a value-iteration algorithm. We establish dynamic upper bounds for the proposed
methods and a matching minimax lower bound which shows the (near-) optimal-
ity of the proposed methods. To our best knowledge, PROPO is the first provably
efficient policy optimization algorithm that handles non-stationarity.
1	Introduction
Reinforcement Learning (RL) (Sutton & Barto, 2018), coupled with powerful function approxima-
tors such as deep neural network, has demonstrated great potential in solving complicated sequential
decision-making tasks such as games (Silver et al., 2016; 2017; Vinyals et al., 2019) and robotic con-
trol (Kober et al., 2013; Gu et al., 2017; Akkaya et al., 2019; Andrychowicz et al., 2020). Most of
these empirical successes are driven by deep policy optimization methods such as trust region policy
optimization (TRPO) (Schulman et al., 2015) and proximal policy optimization (PPO) (Schulman
et al., 2017), whose performance has been extensively studied recently (Agarwal et al., 2019; Liu
et al., 2019; Shani et al., 2020; Mei et al., 2020; Cen et al., 2020).
While classical RL assumes that an agent interacts with a time-invariant (stationary) environment,
when deploying RL to real-world applications, both the reward function and Markov transition ker-
nel can be time-varying. For example, in autonomous driving (Sallab et al., 2017), the vehicle needs
to handle varying conditions of weather and traffic. When the environment changes with time, the
agent must quickly adapt its policy to maximize the expected total rewards in the new environment.
Meanwhile, another example of such a non-stationary scenario is when the environment is subject to
adversarial manipulations, which is the case of adversarial attacks (Pinto et al., 2017; Huang et al.,
2017; Pattanaik et al., 2017). In this situation, it is desired that the RL agent is robust against the
malicious adversary.
Although there is a huge body of literature on developing provably efficient RL methods, most
the existing works focus on the classical stationary setting, with a few exceptions include Jaksch
et al. (2010); Gajane et al. (2018); Cheung et al. (2019a;c; 2020); Fei et al. (2020); Mao et al.
(2020); Ortner et al. (2020); Domingues et al. (2020); Zhou et al. (2020b); Touati & Vincent (2020).
However, these works all focus on value-based methods which only output greedy policies, and
mostly focus on the tabular case where the state space is finite. Thus, the following problem remains
open:
How can we design a provably efficient policy optimization algorithm for non-stationary
environment in the context of function approximation?
1
Under review as a conference paper at ICLR 2022
There are four intertwined challenges associated with this problem: (i) bandit feedbacks from non-
stationary reward and transition kernel, (ii) exploration-exploitation tradeoff that is inherent to online
RL, (iii) incorporating function approximation in the algorithm, and (iv) characterizing the conver-
gence and optimality of policy optimization. Existing works merely address a subset of these four
challenges and it remains open how to tackle all of them simultaneously. For example, a line of
research develops optimism-based value iteration algorithms that successfully handle (ii) and (iii),
e.g., (Jiang et al., 2017; Jin et al., 2019b; Wang et al., 2019b; Zanette et al., 2020; Wang et al., 2020;
Ayoub et al., 2020; Zhou et al., 2020a). Besides, Cai et al. (2019); Agarwal et al. (2020); Efroni
et al. (2020) address challenges (ii)-(iv) but fail to consider (i), and ZhoU et al. (2020b); ToUati &
Vincent (2020) tackle (i)-(iii) but leave (iv) open. More importantly, these four challenges are CoU-
pled together, which requires sophisticated algorithm design. In particular, due to challenges (i) and
(iii), we need to track the non-stationary reward function and transition kernel by function estimation
based on the bandit feedbacks. The estimated model is also time-varying and thus the corresponding
policy optimization problem (challenge (iv)) has a non-stationary objective function. Moreover, to
obtain sample efficiency, we need to strike a balance between exploration and exploitation in the
policy update steps (challenge (i)).
In this work, We propose a periodically restarted optimistic policy optimization algorithm (PROPO)
which successfully tackle the four challenges above. Specifically, we focus on the model of episodic
linear kernel MDP (Ayoub et al., 2020; Zhou et al., 2020a) where both the reward and transition
functions are parameterized by linear functions. Besides, we focus on the non-stationary setting and
adopt the dynamic regret as the performance metric. Moreover, PROPO performs a policy evaluation
step and a policy improvement step in each iteration. To handle challenges (i)-(iii), we propose
a novel optimistic policy evaluation method that incorporates the technique of sliding window to
handle non-stationarity. Specifically, based on the non-stationary bandit feedbacks, we propose
to estimate the time-varying model via a sliding-window-based least-squares regression problem,
where we only keep a subset of recent samples in regression. Based on the model estimator, we
construct an optimistic value function by implementing model-based policy evaluation and adding
an exploration bonus. Then, using such an optimistic value function as the update direction, in
the policy improvement step, we propose to obtain a new policy by solving a Kullback-Leibler
(KL) divergence regularized problem, which can be viewed as a mirror descent step. Moreover,
as the underlying optimal policy is time-varying (challenge (iv)), we additionally restart the policy
periodically by setting it to uniform policy every τ episodes. The two novel mechanisms, sliding
window and periodic restart, respectively enable us to track the non-stationary MDP based on bandit
feedbacks and handle the time-varying policy optimization problem.
Finally, to further exhibit effect of these two mechanisms, we propose an optimism-based value
iteration algorithm, dubbed as SW-LSVI-UCB, which only utilize the sliding window and does not
restart the policy as challenge (iv) disappears.
Our Contributions Our contribution is four-fold. First, we propose PROPO, a policy opti-
mization algorithm designed for non-stationary linear kernel MDPs. This algorithm features two
novel mechanisms, namely sliding window and periodic restart, and also incorporates linear func-
tion approximation and a bonus function to incentivize exploration. Second, we prove that PROPO
achieves a sublinear dynamic regret, where d is the feature dimension, ∆ is the total variation bud-
get, H is the episode horizon, and T is the total number of steps. Third, to separately demon-
strate the effect of sliding window, we propose a value-iteration algorithm, SW-LSVI-UCB, which
adopts sliding-window-based regression to handle non-stationarity. Such an algorithm is shown
to achieve a O(d^6∆"3HT2/3) dynamic regret. Finally, we establish a Ω(d5∕6∆1∕3H2/3T2/3)
lower bound on the dynamic regret, which shows the (near-)optimality of the proposed algorithms.
To our best knowledge, PROPO is the first provably efficient policy optimization algorithm under
the non-stationary environment.
Related Work Our work adds to the vast body of existing literature on non-stationary MDPs. A
line of work studies non-stationary RL in the tabular setting. See Jaksch et al. (2010); Gajane et al.
(2018); Cheung et al. (2019a;c; 2020); Fei et al. (2020); Mao et al. (2020); Ortner et al. (2020) and
the references therein for details. Recently, Domingues et al. (2020) consider the non-stationary RL
in continuous environments and proposes a kernel-based algorithm. More related works are Zhou
et al. (2020b); Touati & Vincent (2020), which study non-stationary linear MDPs, but their setting
is not directly comparable with ours since linear MDPs cannot imply linear kernel MDPs. More-
2
Under review as a conference paper at ICLR 2022
over, Zhou et al. (2020b); Touati & Vincent (2020) do not incorporate policy optimization methods,
which are more difficult because we need to handle the variation of the optimal policies of adjacent
episodes and value-based methods only need to handle the non-stationarity drift of reward functions
and transition kernels. Fei et al. (2020) also makes an attempt to investigate policy optimization
algorithm for non-stationary environments. However, this work requires full-information feedback
and only focuses on the tabular MDPs with time-varying reward functions and time-invariant tran-
sition kernels.
As a special case of MDP problems with unit horizon, bandit problems have been the subject of
intense recent interest. See Besbes et al. (2014; 2019); Russac et al. (2019); Cheung et al. (2019a);
Chen et al. (2019) and the references therein for details.
Another line closely related to our work is policy optimization. As proved in Yang et al. (2019);
Agarwal et al. (2019); Liu et al. (2019); Wang et al. (2019a), policy optimization enjoys computa-
tional efficiency. Recently Cai et al. (2019); Efroni et al. (2020); Agarwal et al. (2020) proposed
optimistic policy optimization methods which simultaneously attain computational efficiency and
sample efficiency. Our work is also related to the value-based methods, especially LSVI (Bradtke
& Barto, 1996; Jiang et al., 2017; Jin et al., 2019b; Wang et al., 2019b; Zanette et al., 2020; Wang
et al., 2020; Ayoub et al., 2020; Zhou et al., 2020a).
Broadly speaking, our work is also related to a line of research on adversarial MDPs (Even-Dar
et al., 2009; Neu et al., 2010; 2012; Zimin & Neu, 2013; Rosenberg & Mansour, 2019; Jin et al.,
2019a).
Notaion	See §A for details.
2	Preliminaries
2.1	Non-stationary MDPs
An episodic non-stationary MDP is defined by a tuple (S, A, H, P, r), where S is a state space, A is
an action space, H is the length of each episode, P = {Phk}(k,h)∈[K]×[H], r = {rhk}(k,h)∈[K]×[H],
where Phk : S × A×S → [0, 1] is the probability transition kernel at the h-th step of the k-th episode,
and rhk : S × A → [0, 1] is the reward function at the h-th step of the k-th episode. We consider
an agent which iteratively interacts with a non-stationary MDP in a sequence of K episodes. At the
beginning of the k-th episode, the initial state s1k is adversarially given to the agent, and the agent
determines a policy πk = {πhk}hH=1. Then, at each step h ∈ [H], the agent observes the state skh,
takes an action following the policy ah 〜∏k(∙ | Sh) and receives a reward rk(sh, ah). Meanwhile,
the MDP evolves into next state shh+ι 〜Pk(∙ | xh ahh). The k-th episode ends at state sH+ι, when
this happens, no control action is taken and reward is equal to zero. We define the state and state-
action value functions of policy π = {πh }hH=1 recursively via the following Bellman equation:
Qh,k(s,a)	=	rh(s,a)	+	(phvhπ+ι)(s,a),	vhπ,k(s) = hQhk(s, ∙),πh(∙ | s)iA,	vH,+kι	=	0,	(2.1)
where Ph is the operator form of the transition kernel Pk (∙∣∙, ∙), which is defined as
(Phf )(s, a) = E[f (s0) | s0 〜Ph(s01 s, a)]	(2.2)
for any function f : S → R. Here h∙, ∙iA denotes the inner product over A.
In the literature of optimization and reinforcement learning, the performance of the agent is mea-
sured by its dynamic regret, which measures the difference between the agent’s policy and the bench-
mark policy π* = {∏*,k }K=1. Specifically, the dynamic regret is defined as
K
D-Regret(T,π*) = X(Vπ*,k,k(Sk) - V”区)),	(2.3)
k=1
where T = HK is the number of steps taken by agent and ∏*,k is the benchmark policy of episode
k. It is worth mentioning that when the benchmark policy is the optimal policy of each individual
episode, that is, ∏*,k = argmax∏ V∏,k (sk), the dynamic regret reaches the maximum, and this spe-
cial case is widely considered in previous works (Cheung et al., 2020; Mao et al., 2020; Domingues
et al., 2020). Throughout the paper, when ∏* is clear from the context, we may omit ∏* from
D-Regret(T,π*).
3
Under review as a conference paper at ICLR 2022
2.2	Model Assumptions
We focus on the linear setting of Markov decision process, where the reward functions and transition
kernels are assumed to be linear. We formally make the following assumption.
Assumption 2.1 (Non-stationary Linear Kernel MDP). MDP (S, A, H, P, r) is a linear kernel MDP
with known feature maps φ : S × A → Rd and ψ : S × A × S → Rd, if for any (k, h) ∈ [K] × [H],
there exist unknown vectors θhk ∈ Rd and ξh ∈ Rd, such that
rhk(s,a) = φ(s, a)>θhk,	Phk(s0 | s,a) = ψ(s, a, s0)>ξhk
for any (s, a, s0) ∈ S × A × S. Without loss of generality, we assume that
kφ(s,a)k2 ≤ 1,	kθkk2 ≤ √d,	kξhk2 ≤ √d
for any (k, h) ∈ [K] × [H]. Moreover, we assume that
kψ(s,
a, s0)k2 ds0 ≤ √d
for any (s, a) ∈ S × A.
Our assumption consists of two parts. One is about reward functions, which follows the setting
of linear bandits (Abbasi-Yadkori et al., 2011; Agrawal & Goyal, 2013; Besbes et al., 2014; 2015;
Cheung et al., 2019a;b). The other part is about transition kernels. As shown in Cai et al. (2019);
Ayoub et al. (2020); Zhou et al. (2020a), linear kernel MDPs as defined above cover several other
MDPs studied in previous works, as special cases. For example, tabular MDPs with canonical basis
(Cai et al., 2019; Ayoub et al., 2020; Zhou et al., 2020a), feature embedding of transition models
(Yang & Wang, 2019a) and linear combination of base models (Modi et al., 2020) are special cases.
However, itis worth mentioning that Jin et al. (2019b); Yang & Wang (2019b) studied another “linear
MDPs"，which assumes the transition kernels can be represented as Ph(s01 s,a) = ψ0(s,a)>μh(s0)
for any h ∈ [H] and (s, a,s0) ∈ S × A × S. Here ψ0(∙, ∙) is a known feature map and μ(∙) is
an unknown measure. It is worth noting that linear MDPs studied in our paper and linear MDPs
(Jin et al., 2019b; Yang & Wang, 2019b) are two different classes of MDPs since their feature maps
ψ(∙, ∙, ∙) and ψ0(∙, ∙) are different and neither class of MDPS includes the other.
To facilitate the following analysis, we denote by Phk,π the Markov kernel of policy at the h-step of
the k-th episode, that is, for S ∈ S, PkK(∙ | S) = Pa∈∕ Ph(∙ | s,a) ∙ ∏h(a | s). Also, We define
kπh - πh k∞,1 = max kπh(∙ | S) - πh (T s)k1,
s∈S
IK" - PhkKk∞,ι=max IK"(∙ i S)- Py'(∙ i s)ki.
Next, we introduce several measures of change in MDPs. First, we denote by PT the total variation
in the benchmark policies of adjacent episodes:
KH
PT = XX H,k-∏y-1k∞,ι,	(2.4)
k=1 h=1
where we choose ∏h,0 = ∏h,1 for any h ∈ [H].
Next, we assume the drifting environment (Besbes et al., 2014; 2015; Cheung et al., 2019a; Russac
et al., 2019), that is, θhk and ξhk can change over different indexes (k, h), with the constraint that the
sum of the Euclidean distances between consecutive θhk and ξhk are bounded by variation budgets BT
and BP , that is,
HK	HK
XXkθhk-1-θhkk2	≤BT,	XXkξhk-1-ξhkk2 ≤BP,	∆=BT+BP,	(2.5)
where H is the length of each episode, K is the total number of episodes, and T = HK is the
total number of steps taken by the agent. Here ∆ is the total variation budget, which quantifies the
non-stationarity of a linear kernel MDP.
4
Under review as a conference paper at ICLR 2022
3	Minimax Lower Bound
In this section, we provide the information-theoretical lower bound result. The following theorem
shows a minimax lower bound of dynamic regret for any algorithm to learn non-stationary linear
kernel MDPs.
Theorem 3.1 (Minimax Iowerbound). FiX ∆ > 0, H > 0, d ≥ 2, and T = Ω(d"2∆H1/2). Then,
there exists a non-stationary linear kernel MDP with a d-dimensional feature map and maximum
total variation budget ∆, such that,
minmaxD-Regret(T,π*) ≥ Q(d5/641/3H2/3T2/3),
A π*
where A denotes the learning algorithm.
Proof sketch. As mentioned above, we only need to establish the lower bound of the dynamic regret
when the benchmark policy is the optimal policy of each individual episode. The proof of lower
bound relies on the construction of a hard-to-learn non-stationary linear kernel MDP instance. To
handle the non-stationarity, we need to divide the total T steps into L segments, where each segment
contains T0 = [LC steps and has K = [KC episodes. Within each segment, the construction of
MDP is similar to the hard-to-learn instance constructed in stationary RL problems (Jaksch et al.,
2010; Lattimore & Hutter, 2012; Osband & Van Roy, 2016). Then, we can derive a lower bound of
Ω(dH√T0^) for the stationary RL problem. Meanwhile, the transition kernel of this hard-to-learn
MDP changes abruptly between two consecutive segments, which forces the agent to learn a new
stationary MDP in each segment. Finally, by optimization L subject to the total budget constraint,
we obtain the lower bound of Ω(d5/6∆1/3H2/3T2/3). See Appendix C for details.	□
4	Algorithm and Theory
4.1	PROPO
Now we present Periodically Restarted Optimistic Policy Optimization (PROPO) in Algorithm 1,
which includes a policy improvement step and a policy evaluation step.
Policy Improvement Step. At k-th episode, Model-Based OPPO updates πk = {πhk }hH=1 accord-
ing to πk-1 = {πhk-1}hH=1. Motivated by the policy improvement step in NPG (Kakade, 2002),
TRPO (Schulman et al., 2015), and PPO (Schulman et al., 2017), we consider the following policy
improvement step
πk = argmax Lk-1(π),	(4.1)
π
where Lk-1 (π) is defined as
Lk-i(∏) = Enk-JXX hQh-1(sh, ∙), ∏h(∙ | Sh)- ∏k-1(∙ | sh)il	(4.2)
h=1
— α-1 ∙ E∏k-ι [XT KL(∏h(∙ | Sh) U ∏k-1(∙ | Sh)),
h=1
where α > 0 is a stepsize and Qkh-1 which is obtained in Line 10 of Algorithm 2 is the estimator of
Qπhk-1,k-1. Here the expectation Eπk-1 is taken over the random state-action pairs {(Sh, ah)}hH=1,
where the initial state S1 = S1k, the distribution of action ah, follows π(∙ | shɔ, and the distribution of
the next state sh+ι follows the transition dynamics Ph (∙ | sh/,ah,). Such a policy improvement step
can also be regarded as one iteration of infinite-dimensional mirror descent (Nemirovsky & Yudin,
1983; Liu et al., 2019; Wang et al., 2019a).
By the optimality condition, policy update in (4.1) admits a closed-form solution
πk(∙ | S) Y πk-1(∙ | S) ∙ exp{α ∙ QhT(S, ∙)}
(4.3)
5
Under review as a conference paper at ICLR 2022
for any s ∈ S and (k, h) ∈ [K] × [H].
Policy Evaluation Step. At the end of the k-th episode, Model-Based OPPO evaluates the policy
πk based on the (k - 1) historical trajectories. Then, we show the details of estimating the reward
functions and transition kernels, respectively.
(i)	Estimating Reward. To estimate the reward functions, we use the sliding window regularized
least squares estimator (SW-RLSE) (Garivier & Moulines, 2011; Cheung et al., 2019a;b), which is
a key tool in estimating the unknown parameters online. At h-th step of k-th episode, we aim to
estimate the unknown parameter θhk based on the historical observation {(sτh, aτh), rhτ (sτh, aτh)}τk-=11.
The design of SW-RLSE is based on the “forgetting principle” (Garivier & Moulines, 2011), that is,
under non-stationarity, the historical observations far in the past are obsolete, and they do not contain
relevant information for the evaluation of the current policy. Therefore, we could estimate θhk using
only {(sτh, aτh), rhτ (sτh, aτh)}τk=-11∨(k-w), the observations during the sliding window 1 ∨ (k - w) to
k - 1,
k-1
bk = argmin( E	(rh(Sh,ah)- φ(sh,ah)>θ)2 + λ ∙ kθk2),	(4.4)
θ	τ =1∨(k-w)
where λ is the regularization parameter and w is the length of a sliding window. By solving (4.4),
we obtain the estimator of θhk :
k-1
θbhk = (Λkh)-1	X	φ(Sτh,aτh)rhτ (Sτh,aτh),	(4.5)
τ =1∨(k-w)
k-1
where Λkh = X	φ(Sτh, aτh)φ(Sτh, aτh)> + λId.
τ =1∨(k-w)
(ii)	Estimating Transition. Similar to the estimation of reward functions, for any (k, h) ∈ [K] ×
[H], we define the sliding window empirical mean-squared Bellman error (SW-MSBE) as
k-1
Mh(ξ) =	X	(Vh+1(sh+1) — ηh(sh, ah)>ξ)2,
τ =1∨(k-w)
where We denote ηh(∙, ∙) as
ηh(∙, ∙) = JS ψ(∙, ∙, SO) ∙ Vh+ι(s0)ds0.
By Assumption 2.1, we have
kηh(∙,∙)k2 ≤ H√d
for any (k, h) ∈ [K] × [H]. Then we estimate ξhk by solving the following problem:
bk = argmin(Mh(W) + λ0 ∙ kwk2),
w∈Rd
where λ0 is the regularization parameter. By solving (4.7), we obtain
k-1
bk = (Ah)T	E	ηh(sh,ah) ∙ Vh+1(sh+1)
τ =1∨(k-w)
k-1
where Akh =	X	ηhτ(Sτh,aτh)ηhτ(Sτh,aτh)>+λ0Id.
τ =1∨(k-w)
(4.6)
(4.7)
(4.8)
The policy evaluation step is iteratively updating the estimated Q-function Qk = {Qkh}hH=1 by
Qh(∙, ∙) = min{Φ(∙, ∙)>bk + ηk(∙, ∙)>ξh + Bk(∙, ∙) + Γk(∙, ∙),H — h + 1}+,	(4.9)
Vhk(S) = hQh(s, ∙),∏k(∙∣s)iA
6
Under review as a conference paper at ICLR 2022
in the order of h = H, H - 1, ∙∙∙ , 1. Here bonus functions Bh(∙,∙) : S × A→ R+ and Γh(∙, ∙):
S × A → R+ are used to quantify the uncertainty in estimating reward rhk and quantity PkhVhk+1
respectively, defined as
Bh(∙, •) = β(φ(∙, ∙)>(Λh)-1φ(∙, ∙))1∕2,	Γh(∙, •) = β0(ηk(∙, ∙)τ(Ah)-1ηh(∙, .))1/2,	(4.10)
where β > 0 and β0 > 0 are parameters depend on d, H and K, which are specified in Theorem 4.2.
To handle the non-stationary drift incurred by the different optimal policies in different episodes,
Algorithm 1 also includes a periodic restart mechanism, which resets the policy estimates every τ
episodes. We call the τ episodes between every two resets a segment. In each segment, each episode
is approximately the same as the first episode, which means that we can regard it as a stationary MDP.
Then we can use the method of solving the stationary MDP to analyze each segment with a small
error, and finally combine each segment and choose the value of τ to get the desired result. Such
a restart mechanism is widely used in RL (Auer et al., 2009; Ortner et al., 2020), bandits (Besbes
et al., 2014; Zhao et al., 2020), and non-stationary optimization (Besbes et al., 2015; Jadbabaie et al.,
2015).
The pseudocode of the PROPO algorithm is given in Algorithm 1.
Algorithm 1 Periodically Restarted Optimistic Policy Optimization (PROPO)
Require: Reset cycle length T, sliding window length w, stepsize a, regularization factors λ and
λ0, and bonus multipliers β and β0 .
1:	Initialize {π0(∙ | ∙)}H=ι as uniform distribution policies, {Qh(∙, ∙)}H=I as zero functions.
2:	for k = 1, 2, . . . , K do
3:	Receive the initial state s1k .
4:	if k mod τ = 1 then
5:	Set {Qkh-1}h∈[H] as zero functions and {πhk-1}h∈[H] as uniform distribution on A.
6:	end if
7:	forh = 1,2, . . .,Hdo
8:	∏h(∙∣∙) (X ∏k-1(∙∣∙) ∙ exp{α ∙ QhT(∙, ∙)}.
9:	Take action ahh ~ ∏h(∙ | Sh).
10:	Observe the reward rhk(skh, akh) and receive the next state skh+1.
11:	end for
12:	Compute Qkh by SWOPE(k, {πhk}, λ, λ0, β, β0) (Algorithm 2).
13:	end for
Algorithm 2 Sliding Window Optimistic Policy Evaluation (SWOPE)
Require: Episode index k, policies {πh}, regularization factors λ and λ0, and bonus multipliers β
and β0.
1:	Initialize VHk +1 as a zero function.
2:	for h = H,H - 1, . . .,0do
3:	ηk(∙, ∙) = Rs ψ(∙,∙,s0) ∙ Vh+ι(s0)ds0.
4:	Λkh = Pτk-=11∨(k-w) φ(sτh,aτh)φ(sτh,aτh)τ +λId .
5:	θbhk = (Λkh)-1 Pτk=-11∨(k-w) φ(sτh,aτh)rhτ(sτh,aτh).
6:	Akh = Pτk-=11∨(k-w) ηhτ (sτh, aτh)ηhτ (sτh, aτh)τ + λ0Id..
7:	ξh = (Ah)T(Pk=1∨(k-w) ηh (Sh, ah) ∙ Vh+1(sh+1)).
8:	Bh(∙, ∙) = β(Φ(∙, ∙)τ(Λh)-ιΦ(∙, ∙))1∕2.
9:	Γh(∙, ∙) = β0(ηh(∙, ∙)τ(Ah)-1ηh(∙, ∙))1∕2.
10:	Qh(∙, ∙) = min{φ(∙, ∙)τbh + ηh(∙, ∙)τbh + Bh(∙, ∙) + Γh(∙, ∙),H - h +1}+.
11:	Vlh(S) = hQh(s,∙),∏h(∙∣s)iA.
12:	end for
7
Under review as a conference paper at ICLR 2022
4.2	SW-LSVI-UCB
In this subsection, we present the details of Sliding Window Least-Square Value Iteration with UCB
(SW-LSVI-UCB) in Algorithm 3 (cf. Appendix B).
Similar to Least-Square Value Iteration with UCB (LSVI-UCB) in Jin et al. (2019b), SW-LSVI-UCB
is also an optimistic modification of Least-Square Value Iteration (LSVI) (Bradtke & Barto, 1996),
where the optimism is realized by Upper-Confidence Bounds (UCB). Specifically, the optimism is
achieved due to the bonus functions Bhk and Γkh, which quantify the uncertainty of reward functions
and transition kernels, respectively. It is worth noting that in order to handle the non-stationarity,
SW-LSVI-UCB also uses the sliding window method (Garivier & Moulines, 2011; Cheung et al.,
2019a;b).
In detail, at k-th episode, SW-LSVI-UCB consists of two steps. In the first step, by solving the
sliding window least-square problems (4.4) and (4.7), SW-LSVI-UCB updates the parameters Λkh in
(4.5), θbhk in (4.5), Akh in (4.8), and ξbhk in (4.8), which are used to form the Q-function Qkh . In the
second step, SW-LSVI-UVB obtains the greedy policy with respect to the Q-function Qkh gained in
the first step. See Algorithm 3 in Appendix B for more details.
4.3	Regret Analysis
In this subsection, we analyze the dynamic regret incurred by Algorithms 1 and 3 and compare the
theoretical regret upper bounds derived for these two algorithms.
To derive sharp dynamic regret bounds, we impose the following technical assumption.
Assumption 4.1. There exists an orthonormal basis Ψ = (Ψι,…，Ψd) such that for any (s, a) ∈
S × A, there exists a vector z ∈ Rd satisfying that φ(s, a) = Ψz. We also assume the existence of
another orthonormal basis Ψ0 = (Ψ0, ∙∙∙ , Ψ1) such that for any (s, a,k,h) ∈ S ×A × [K] X [H]
such that ηhk(s, a) = Ψ0z0 for some z0 ∈ Rd.
It is not difficult to show that this assumption holds in the tabular setting. Similar assumption is also
adopted by previous work in non-stationary optimization (Cheung et al., 2019a). We will provide
more comments on this technical assumption after showing main results.
First, we establish an upper bound on the dynamic regret of PROPO. Recall that the dynamic regret
is defined in (2.3) and d is the dimension of the feature maps φ and ψ. Also, |A| is the cardinality of
A. We also define P =1K/丁] tobe the number of restarts that take place in Algorithm 1.
Theorem 4.2 (Upper bound for Algorithm 1). Suppose Assumptions 2.1 and 4.1 hold. Let T =
∏[i,κ](b( HTP√+√A∆))2/3C), α = PPlog |A|/(H2K) in (4.2), W = e(d1/34-2/3T2/3) in (4.4),
λ = λ0 = 1 in (4.4) and (4.9), β = √d in (4.10), and β = CVdH2 ∙ log(dT∕Z) in (4.10), where
C0 > 1 is an absolute constant and ζ ∈ (0, 1]. We have
D-Regret(T) . d5/641/3HT2/3 ∙ log(dT∕Z)
f	√H3T log |A|,	if 0 ≤ PT + √d∆ ≤ 个⅛ai ,
+	f	(H2T√log∣A∣)2/3(PT + √d∆)1/3, if qloKA1 ≤ Pt + √d∆ . Kpog|A|,
[	H2 (Pt + √d∆),	if Pt + √d∆ & K √log网,
with probability at least 1 - ζ.
Proof. See Appendix D for a proof sketch and Appendix G for a detailed proof.	□
Then We discuss the regret bound throughout three regimes of PT + √d∆:
•	Small PT + √d∆: when 0 ≤ PT + √d∆ ≤ JIOgIAI, restart period T = K, which
means that we do not need to periodically restart in this case. Assuming that log ∣A∣ =
O(d5/3∆2∕3H-1T1/3), Algorithm 1 attains a O(d5∕6∆1∕3HT2/3) dynamic regret. Com-
bined with the lower bound established in Theorem 3.1, our result matches the lower bound
8
Under review as a conference paper at ICLR 2022
in d, ∆ and T up to logarithmic factors. Hence, we can conclude that Algorithm 1 is a near-
optimal algorithm;
•	Moderate PT + √d∆: when JlogKAI ≤ PT + √d∆ . K PlogAI, restart period T =
(JIlog ⅛k)2/3 ∈ [2, K]. Algorithm 2 incurs a O(T2/3) dynamic regret if ∆ =。⑴
H(PT + d∆)
and PT = O(1);
•	Large PT + √d∆: when PT + √d∆ & K dlog |A|, restart period T = K. Since the model
is highly non-stationary, we only obtain a linear regret in T.
In the following theorem, we establish the upper bound of dynamic regret incurred by SW-LSVI-
UCB (Algorithm 3).
Theorem 4.3 (Upper bound for Algorithm 3). Suppose Assumption 2.1 and 4.1 hold. Let
W = Θ(d1∕3∆-2∕3T2/3) in (4.4), λ = λ0 = 1 in (4.4) and (4.9), β = √d in (4.10), and
β0 = C0PdH2 ∙ log(dT∕Z) in (4.10), where C0 > 1 is an absolute constant and Z ∈ (0,1]. We have
D-Regret(T) . d5/6∆1/3HT2/3 ∙ log(dT∕Z)
with probability at least 1 - ζ.
Proof. See Appendix H for a detailed proof.	□
Regarding Assumption 4.1. Due to some technical issue (Touati & Vincent, 2020; Zhao & Zhang,
2021), without this assumption and the knowledge of locally variation budget (Touati & Vincent,
2020), previous work can only obtain the bound Oe(T 3/4) (Cheung et al., 2020; Zhao & Zhang,
2021; Zhao et al., 2020; Russac et al., 2019; Zhou et al., 2020b; Touati & Vincent, 2020). Thanks
to Assumption 4.1, we derive sharper regret bounds at the order Oe(T 2/3). We also remark that we
can establish slightly worse regret bounds for Algorithms 1 and 3 without Assumption 4.1. See
Appendix I for details.
Optimality of the Bounds. Notably, the term O(d5∕6∆1∕3HT2/3) appears in both the results in
Theorems 4.2 and 4.3. Ignoring logarithmic factors, there is only a gap of H1/3 between this upper
bound and the lower bound Ω(d5∕6∆1∕3H2/3T2/3) established in Theorem 3.1. We conjecture
that this gap can be bridged by using the “Bernstein” type bonus functions Azar et al. (2017); Jin
et al. (2018). Since our focus is on designing a provably efficient policy optimization algorithm for
non-stationary linear kernel MDPs, we don’t use this technique for the clarity of our analysis.
Comparison. Compared with PROPO, SW-LSVI-UCB achieves a slightly better regret without
the help of the periodic restart mechanism. Especially in the highly non-stationary case, that is
PT + √d∆ & KPlogAI, SW-LSVI-UCB achieves a O(T2/3) regret, where PROPO only attains
a linear regret in T. However, PROPO achieves the same Oe(T 2/3) regret as SW-LSVI-UCB when
PT + √d∆ . K 'log IAI, which suggests that PROPO is provably efficient for solving slightly or
even moderately non-stationary MDPs. Therefore, itis important to investigate whether it is possible
to bridge this gap between policy and value based methods, or alternatively to show that this gap is
actually a true drawback of policy optimization methods in the non-stationary case.
5 Conclusion
In this work, we have proposed a probably efficient policy optimization algorithm, dubbed as
PROPO, for non-stationary linear kernel MDPs. Such an algorithm incorporates a bonus func-
tion to incentivize exploration, and more importantly, adopts sliding-window-based regression in
policy evaluation and periodic restart in policy update to handle the challenge of non-stationarity.
Moreover, as a byproduct, we establish an optimistic value iteration algorithm, SW-LSVI-UCB, by
combining UCB and sliding-window. We prove that PROPO and SW-LSVI-UCB both achieve sam-
ple efficiency by having sublinear dynamic regret. We also establish a dynamic regret lower bound
which shows that PROPO and SW-LSVI-UCB are near-optimal. To our best knowledge, we propose
the first provably efficient policy optimization method that successfully handles non-stationarity.
9
Under review as a conference paper at ICLR 2022
References
Yasin Abbasi-Yadkori, David Pal, and Csaba Szepesvari. Improved algorithms for linear stochastic
bandits. In Advances in Neural Information Processing Systems, pp. 2312-2320, 2011.
Alekh Agarwal, Sham M Kakade, Jason D Lee, and Gaurav Mahajan. On the theory of policy gradi-
ent methods: Optimality, approximation, and distribution shift. arXiv preprint arXiv:1908.00261,
2019.
Alekh Agarwal, Mikael Henaff, Sham Kakade, and Wen Sun. PC-PG: Policy cover directed explo-
ration for provable policy gradient learning. arXiv preprint arXiv:2007.08459, 2020.
Shipra Agrawal and Navin Goyal. Thompson sampling for contextual bandits with linear payoffs.
In International Conference on Machine Learning, pp. 127-135, 2013.
Ilge Akkaya, Marcin Andrychowicz, Maciek Chociej, Mateusz Litwin, Bob McGrew, Arthur Petron,
Alex Paino, Matthias Plappert, Glenn Powell, Raphael Ribas, et al. Solving rubik’s cube with a
robot hand. arXiv preprint arXiv:1910.07113, 2019.
OpenAI: Marcin Andrychowicz, Bowen Baker, Maciek Chociej, Rafal Jozefowicz, Bob McGrew,
Jakub Pachocki, Arthur Petron, Matthias Plappert, Glenn Powell, Alex Ray, et al. Learning
dexterous in-hand manipulation. The International Journal of Robotics Research, 39(1):3-20,
2020.
Peter Auer, Thomas Jaksch, and Ronald Ortner. Near-optimal regret bounds for reinforcement
learning. In Advances in neural information processing systems, pp. 89-96, 2009.
Alex Ayoub, Zeyu Jia, Csaba Szepesvari, Mengdi Wang, and Lin F Yang. Model-based reinforce-
ment learning with value-targeted regression. arXiv preprint arXiv:2006.01107, 2020.
Mohammad GheShlaghi Azar, Ian Osband, and Remi Munos. Minimax regret bounds for reinforce-
ment learning. In International Conference on Machine Learning, pp. 263-272. PMLR, 2017.
Omar Besbes, Yonatan Gur, and Assaf Zeevi. Stochastic multi-armed-bandit problem with non-
stationary rewards. In Advances in neural information processing systems, pp. 199-207, 2014.
Omar Besbes, Yonatan Gur, and Assaf Zeevi. Non-stationary stochastic optimization. Operations
research, 63(5):1227-1244, 2015.
Omar Besbes, Yonatan Gur, and Assaf Zeevi. Optimal exploration-exploitation in a multi-armed
bandit problem with non-stationary rewards. Stochastic Systems, 9(4):319-337, 2019.
Steven J Bradtke and Andrew G Barto. Linear least-squares algorithms for temporal difference
learning. Machine learning, 22(1-3):33-57, 1996.
Qi Cai, Zhuoran Yang, Chi Jin, and Zhaoran Wang. Provably efficient exploration in policy opti-
mization. arXiv preprint arXiv:1912.05830, 2019.
Shicong Cen, Chen Cheng, Yuxin Chen, Yuting Wei, and Yuejie Chi. Fast global convergence of
natural policy gradient methods with entropy regularization. arXiv preprint arXiv:2007.06558,
2020.
Yifang Chen, Chung-Wei Lee, Haipeng Luo, and Chen-Yu Wei. A new algorithm for non-stationary
contextual bandits: Efficient, optimal and parameter-free. In Conference on Learning Theory, pp.
696-726. PMLR, 2019.
Wang Chi Cheung, David Simchi-Levi, and Ruihao Zhu. Hedging the drift: Learning to optimize
under non-stationarity. arXiv preprint arXiv:1903.01461, 2019a.
Wang Chi Cheung, David Simchi-Levi, and Ruihao Zhu. Learning to optimize under non-
stationarity. In The 22nd International Conference on Artificial Intelligence and Statistics, pp.
1079-1087, 2019b.
Wang Chi Cheung, David Simchi-Levi, and Ruihao Zhu. Non-stationary reinforcement learning:
The blessing of (more) optimism. Available at SSRN 3397818, 2019c.
10
Under review as a conference paper at ICLR 2022
Wang Chi Cheung, David Simchi-Levi, and Ruihao Zhu. Reinforcement learning for non-stationary
markov decision processes: The blessing of (more) optimism. arXiv preprint arXiv:2006.14389,
2020.
Varsha Dani, Thomas P Hayes, and Sham M Kakade. Stochastic linear optimization under bandit
feedback. 2008.
Omar DarWiche Domingues, Pierre Menard, Matteo Pirotta, Emilie Kaufmann, and Michal Valko. A
kernel-based approach to non-stationary reinforcement learning in metric spaces. arXiv preprint
arXiv:2007.05078, 2020.
Yonathan Efroni, Lior Shani, Aviv Rosenberg, and Shie Mannor. Optimistic policy optimization
With bandit feedback. arXiv preprint arXiv:2002.08243, 2020.
Eyal Even-Dar, Sham M Kakade, and Yishay Mansour. Online markov decision processes. Mathe-
matics ofOperations Research, 34(3):726-736, 2009.
Yingjie Fei, Zhuoran Yang, Zhaoran Wang, and Qiaomin Xie. Dynamic regret of policy optimization
in non-stationary environments. arXiv preprint arXiv:2007.00148, 2020.
Pratik Gajane, Ronald Ortner, and Peter Auer. A sliding-WindoW algorithm for markov decision
processes With arbitrarily changing reWards and transitions. arXiv preprint arXiv:1805.10066,
2018.
Aurelien Garivier and Eric Moulines. On upper-confidence bound policies for switching bandit
problems. In International Conference on Algorithmic Learning Theory, pp. 174-188. Springer,
2011.
Shixiang Gu, Ethan Holly, Timothy Lillicrap, and Sergey Levine. Deep reinforcement learning for
robotic manipulation with asynchronous off-policy updates. In 2017 IEEE international confer-
ence on robotics and automation (ICRA), pp. 3389-3396. IEEE, 2017.
Sandy Huang, Nicolas Papernot, Ian Goodfellow, Yan Duan, and Pieter Abbeel. Adversarial attacks
on neural network policies. arXiv preprint arXiv:1702.02284, 2017.
Ali Jadbabaie, Alexander Rakhlin, Shahin Shahrampour, and Karthik Sridharan. Online optimiza-
tion: Competing with dynamic comparators. In Artificial Intelligence and Statistics, pp. 398-406,
2015.
Thomas Jaksch, Ronald Ortner, and Peter Auer. Near-optimal regret bounds for reinforcement
learning. Journal of Machine Learning Research, 11(51):1563-1600, 2010.
Nan Jiang, Akshay Krishnamurthy, Alekh Agarwal, John Langford, and Robert E Schapire. Con-
textual decision processes with low bellman rank are pac-learnable. In International Conference
on Machine Learning, pp. 1704-1713. PMLR, 2017.
Chi Jin, Zeyuan Allen-Zhu, Sebastien Bubeck, and Michael I Jordan. Is q-learning provably effi-
cient? In Advances in Neural Information Processing Systems, pp. 4863-4873, 2018.
Chi Jin, Tiancheng Jin, Haipeng Luo, Suvrit Sra, and Tiancheng Yu. Learning adversarial mdps with
bandit feedback and unknown transition. arXiv preprint arXiv:1912.01192, 2019a.
Chi Jin, Zhuoran Yang, Zhaoran Wang, and Michael I Jordan. Provably efficient reinforcement
learning with linear function approximation. arXiv preprint arXiv:1907.05388, 2019b.
Sham M Kakade. A natural policy gradient. In Advances in neural information processing systems,
pp. 1531-1538, 2002.
Jens Kober, J Andrew Bagnell, and Jan Peters. Reinforcement learning in robotics: A survey. The
International Journal of Robotics Research, 32(11):1238-1274, 2013.
Tor Lattimore and Marcus Hutter. Pac bounds for discounted mdps. In International Conference on
Algorithmic Learning Theory, pp. 320-334. Springer, 2012.
11
Under review as a conference paper at ICLR 2022
Boyi Liu, Qi Cai, Zhuoran Yang, and Zhaoran Wang. Neural trust region/proximal policy optimiza-
tion attains globally optimal policy. In Advances in Neural Information Processing Systems, pp.
10565-10576, 2019.
Weichao Mao, Kaiqing Zhang, RUihao Zhu, David Simchi-Levi, and Tamer Bayar. Near-optimal
regret bounds for model-free rl in non-stationary episodic mdps, 2020.
Jincheng Mei, Chenjun Xiao, Csaba Szepesvari, and Dale Schuurmans. On the global convergence
rates of softmax policy gradient methods. In International Conference on Machine Learning, pp.
6820-6829. PMLR, 2020.
Aditya Modi, Nan Jiang, Ambuj Tewari, and Satinder Singh. Sample complexity of reinforcement
learning using linearly combined model ensembles. In International Conference on Artificial
Intelligence and Statistics, pp. 2010-2020, 2020.
Arkadil Semenovich Nemirovsky and David Borisovich Yudin. Problem complexity and method
efficiency in optimization. 1983.
Gergely Neu, Andras Gyorgy, and Csaba Szepesvari. The online loop-free stochastic Shortest-Path
problem. In COLT, volume 2010, pp. 231-243. Citeseer, 2010.
Gergely Neu, Andras Gyorgy, and Csaba Szepesvari. The adversarial stochastic shortest path prob-
lem with unknown transition probabilities. In Artificial Intelligence and Statistics, pp. 805-813.
PMLR, 2012.
Ronald Ortner, Pratik Gajane, and Peter Auer. Variational regret bounds for reinforcement learning.
In Uncertainty in Artificial Intelligence, pp. 81-90. PMLR, 2020.
Ian Osband and Benjamin Van Roy. On lower bounds for regret in reinforcement learning. arXiv
preprint arXiv:1608.02732, 2016.
Anay Pattanaik, Zhenyi Tang, Shuijing Liu, Gautham Bommannan, and Girish Chowdhary. Robust
deep reinforcement learning with adversarial attacks. arXiv preprint arXiv:1712.03632, 2017.
Lerrel Pinto, James Davidson, Rahul Sukthankar, and Abhinav Gupta. Robust adversarial reinforce-
ment learning. In International Conference on Machine Learning, pp. 2817-2826. PMLR, 2017.
Aviv Rosenberg and Yishay Mansour. Online convex optimization in adversarial markov decision
processes. In International Conference on Machine Learning, pp. 5478-5486. PMLR, 2019.
Paat Rusmevichientong and John N Tsitsiklis. Linearly parameterized bandits. Mathematics of
Operations Research, 35(2):395-411, 2010.
Yoan Russac, Claire Vernade, and Olivier Cappe. Weighted linear bandits for non-stationary envi-
ronments. In Advances in Neural Information Processing Systems, pp. 12017-12026, 2019.
Ahmad EL Sallab, Mohammed Abdou, Etienne Perot, and Senthil Yogamani. Deep reinforcement
learning framework for autonomous driving. Electronic Imaging, 2017(19):70-76, 2017.
John Schulman, Sergey Levine, Pieter Abbeel, Michael Jordan, and Philipp Moritz. Trust region
policy optimization. In International conference on machine learning, pp. 1889-1897, 2015.
John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy
optimization algorithms. arXiv preprint arXiv:1707.06347, 2017.
Lior Shani, Yonathan Efroni, and Shie Mannor. Adaptive trust region policy optimization: Global
convergence and faster rates for regularized mdps. In Proceedings of the AAAI Conference on
Artificial Intelligence, volume 34, pp. 5668-5675, 2020.
David Silver, Aja Huang, Chris J Maddison, Arthur Guez, Laurent Sifre, George Van Den Driessche,
Julian Schrittwieser, Ioannis Antonoglou, Veda Panneershelvam, Marc Lanctot, et al. Mastering
the game of go with deep neural networks and tree search. nature, 529(7587):484-489, 2016.
12
Under review as a conference paper at ICLR 2022
David Silver, Julian Schrittwieser, Karen Simonyan, Ioannis Antonoglou, Aja Huang, Arthur Guez,
Thomas Hubert, Lucas Baker, Matthew Lai, Adrian Bolton, et al. Mastering the game of go
without human knowledge. nature, 550(7676):354-359, 2017.
Richard S Sutton and Andrew G Barto. Reinforcement learning: An introduction. MIT press, 2018.
Ahmed Touati and Pascal Vincent. Efficient learning in non-stationary linear markov decision pro-
cesses. arXiv preprint arXiv:2010.12870, 2020.
Oriol Vinyals, Igor Babuschkin, Wojciech M Czarnecki, Michael Mathieu, Andrew Dudzik, JUny-
oung Chung, David H Choi, Richard Powell, Timo Ewalds, Petko Georgiev, et al. Grandmaster
level in starcraft ii using multi-agent reinforcement learning. Nature, 575(7782):350-354, 2019.
Lingxiao Wang, Qi Cai, Zhuoran Yang, and Zhaoran Wang. Neural policy gradient methods: Global
optimality and rates of convergence. arXiv preprint arXiv:1909.01150, 2019a.
Ruosong Wang, Ruslan Salakhutdinov, and Lin F Yang. Provably efficient reinforcement learning
with general value function approximation. arXiv preprint arXiv:2005.10804, 2020.
Yining Wang, Ruosong Wang, Simon S Du, and Akshay Krishnamurthy. Optimism in reinforce-
ment learning with generalized linear function approximation. arXiv preprint arXiv:1912.04136,
2019b.
Lin F Yang and Mengdi Wang. Reinforcement leaning in feature space: Matrix bandit, kernels, and
regret bound. arXiv preprint arXiv:1905.10389, 2019a.
Lin F Yang and Mengdi Wang. Sample-optimal parametric q-learning using linearly additive fea-
tures. arXiv preprint arXiv:1902.04779, 2019b.
Zhuoran Yang, Yongxin Chen, Mingyi Hong, and Zhaoran Wang. On the global conver-
gence of actor-critic: A case for linear quadratic regulator with ergodic cost. arXiv preprint
arXiv:1907.06246, 2019.
Andrew Chi-Chin Yao. Probabilistic computations: Toward a unified measure of complexity. In
18th Annual Symposium on Foundations of Computer Science (sfcs 1977), pp. 222-227. IEEE,
1977.
Andrea Zanette, Alessandro Lazaric, Mykel Kochenderfer, and Emma Brunskill. Learning near op-
timal policies with low inherent bellman error. In International Conference on Machine Learning,
pp. 10978-10989. PMLR, 2020.
Peng Zhao and Lijun Zhang. Non-stationary linear bandits revisited. arXiv preprint
arXiv:2103.05324, 2021.
Peng Zhao, Lijun Zhang, Yuan Jiang, and Zhi-Hua Zhou. A simple approach for non-stationary
linear bandits. In Proceedings of the 23rd International Conference on Artificial Intelligence and
Statistics, AISTATS, volume 2020, 2020.
Dongruo Zhou, Jiafan He, and Quanquan Gu. Provably efficient reinforcement learning for dis-
counted MDPs with feature mapping. arXiv preprint arXiv:2006.13165, 2020a.
Huozhi Zhou, Jinglin Chen, Lav R Varshney, and Ashish Jagmohan. Nonstationary reinforcement
learning with linear function approximation. arXiv preprint arXiv:2010.04244, 2020b.
Alexander Zimin and Gergely Neu. Online learning in episodic markovian decision processes by
relative entropy policy search. In Neural Information Processing Systems 26, 2013.
13