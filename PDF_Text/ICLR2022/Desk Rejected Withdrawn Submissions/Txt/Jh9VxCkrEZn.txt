Under review as a conference paper at ICLR 2022
Spatiotemporal Representation Learning on
Time Series with Dynamic Graph ODEs
Anonymous authors
Paper under double-blind review
Ab stract
Spatiotemporal representation learning on multivariate time series has received
tremendous attention in forecasting traffic and energy data. Recent works either
rely on complicated discrete neural architectures or graph priors, hindering their
effectiveness and applications in the real world. In this paper, inspired by neural
ordinary differential equations and graph structure learning, we propose a fully
continuous model named Dynamic Graph ODE (DyG-ODE) to capture both long-
range spatial and temporal dependencies to learn expressive representations on
arbitrary multivariate time series data without being restricted by rigid precondi-
tions (e.g., graph priors). For modeling the continuous dynamics of spatiotempo-
ral clues, we design a simple yet powerful dynamic graph ODE by coupling the
proposed spatial and temporal ODEs, which not only allows the model to obtain
infinite spatial and temporal receptive fields but also reduces numerical errors and
model complexity significantly. Our empirical evaluations demonstrate the supe-
rior effectiveness and efficiency of DyG-ODE on a number of benchmark datasets.
1 Introduction
Spatiotemporal representation learning is a nontrivial problem in the real world that has been studied
in multiple fields, such as sequential recommendation (Zhang et al., 2021; Song et al., 2019), time
series forecasting (Wu et al., 2020; Liu et al., 2020), and video representation learning (Hou et al.,
2021), where the modeling of temporal and spatial clues are complementary to each other.
In this paper, we focus on learning the spatiotemporal representation of multivariate time series.
While earlier methods are based on autoregressive models, recent methods take a deep learning-
based approach. In particular, techniques based on graph neural networks (GNNs) (Jiang & Luo,
2021; Skardinga et al., 2021) have demonstrated great potential in simultaneously capturing the
temporal and spatial interdependencies among multiple signals over time. For instance, the spa-
tiotemporal graph neural networks (STGNNs) (Yu et al., 2018; Li et al., 2018; Wu et al., 2019b;
Zheng et al., 2020), have improved the forecasting precision significantly by leveraging the message
passing mechanism on predefined sensor graphs, allowing them to capture not only the temporal but
also spatial dependencies among multiple series. However, such topological information is typically
static, and may not be available in the real world (Wu et al., 2019b).
To bridge this gap, recent works (Wu et al., 2020; Shang et al., 2020) have explored several ways to
address the missing dynamic graph structures. A concrete example is MTGNN (Wu et al., 2020),
which proposes a convolution-based STGNN together with a graph construction module, showing
competitive performances on multivariate time series forecasting. In many existing spatiotemporal
models, including MTGNN, let X denote the input series, A represent the predefined or learned
graph structure, and H0 = Conv1×1(X, Θsc) be a transformation of the input feature, these meth-
ods can be abstracted as the combination of multiple residual blocks:
Hl+1
Hout
HHι + GNN (TCN(Hι, Θι), A, Φl),
HL,
(1)
where Hl denotes the truncated representation at the l-th layer, and Hout is the output representation.
TCN(∙, Θι) and GNN(∙, Φ?) are temporal and graph convolution layers at the l-th layer to capture
temporal and spatial dependencies, parameterized by Θl and Φl respectively.
1
Under review as a conference paper at ICLR 2022
By iteratively performing spatial and temporal convolutions, the above formulation gives a discrete
solution of modeling the spatiotemporal information, where a sequence of individually parameter-
ized spatial and temporal transformations are stacked within a model. This leads to several important
limitations. Firstly, learning the long-term temporal dynamics by combining a sequence of tempo-
ral convolutions results in the discontinuous trajectories of latent representations, thus making the
model prone to numerical errors. Secondly, most of the existing methods, including MTGNN, re-
sort to modeling the discrete dynamics of shallow graph propagation, limiting the model’s ability
to accurately capture long-range spatial dependencies between time series. Thirdly, a layer-wise
parametrization in Eq. 1 makes discrete methods not only computational inefficient but also hard to
converge. The dedicated designs, such as residual and skip connections, while being important, also
lead to complex neural architectures and redundant trainable parameters.
In this paper, we address the above limitations by proposing a new paradigm named DyG-ODE to
learn the continuous spatiotemporal dynamics of multivariate time series. Specifically, inspired by
neural ordinary differential equations (NODEs) (Chen et al., 2018), we generalize the discrete for-
mulation in Eq. 1 with an explicit ODE to parameterize the derivation of latent spatial and temporal
representations. In such a way, it not only enables the model to characterize the continuous long-
range spatiotemporal dynamics with infinite spatial and temporal receptive fields but also simplifies
neural architectures and reduces the number of trainable parameters. In practice, we solve a simple
initial value problem of ODE to obtain the learned time series representations:
Hl+1
Hout
Hl+f(Hl,Θl,Φl),
HL.
dHT= f(H(t), Θ, Φ),
Hout =ODESolVe (H(0),f,to,tι, Θ, Φ).
(2)
⇒
In fact, our formulation in Eq. 2 consists of two coupled ODE functions wrapped as f (H(t), Θ, Φ),
defining the continuous exterior temporal aggregation and interior graph propagation processes.
To achieve this, we have further answered several unresolved technical questions: (Q1) How to
characterize a continuous graph propagation process on arbitrary multivariate time series data with
dynamically learnable graph structures? (Q2) How to model a temporal aggregation process by an
ODE with dynamic hidden state dimensions? (Q3) How these two processes can be intersected
with each other to capture rich spatiotemporal dynamics within a single ODE? By addressing these
challenges (in Section 3.1-3.3), our method not only generalizes the existing convolution-based
designs to a continuous paradigm but also alleviates the reliance on graph priors, providing a simpler
way to model multivariate time series that is both more effective and more efficient, and thus solving
the three limitations of existing discrete solutions. We summarize our contributions as follows:
•	We propose a novel and potent ODE-based method to learn the spatiotemporal represen-
tations on multivariate time series data by unifying two ODEs without complicating the
model design and optimization.
•	We design a spatial ODE together with a graph learning module to capture the long-range
spatial dependencies between time series, which alleviates the over-smoothing problem and
the reliance on static graph priors.
•	We propose a temporal ODE by generalizing the canonical temporal convolutions to learn
the continuous temporal dynamics of time series, which is more powerful and efficient than
its discrete counterparts.
•	We conduct extensive experiments to demonstrate the effectiveness and efficiency of the
proposed method, showing great application prospects in the real world.
2	Related Work
Spatiotemporal Representation Learning. This topic has been explored in multiple domains,
such as video representation learning (Hou et al., 2021) in computer vision and multivariate time
series modeling in data mining. For the latter one, the conventional approaches mainly rely on
convolution-based (Lai et al., 2018; Shih et al., 2019) or attentive (Huang et al., 2019) networks to
capture both spatial and temporal correlations. Recently, GNN-based methods have been proposed
for traffic forecasting (Yu et al., 2018; Chen et al., 2020; Zheng et al., 2020) on predefined graph
structures. As a new family of neural networks to encode both attributive and structural informa-
tion, different GNNs are proposed to learn on static (KiPf & Welling, 2017; Velickovic et al., 2018;
2
Under review as a conference paper at ICLR 2022
Hamilton et al., 2017) and dynamic graphs (Xu et al., 2020; Rossi et al., 2020; Wang et al., 2020). In
traffic forecasting, the concept of spatiotemporal graphs (a special type of dynamic graphs) has been
recently applied where node attributes are dynamic while the underlying graph topology (connectiv-
ity) is fixed. In this paper, inspired by graph structure learning (Shang et al., 2020; Wu et al., 2020),
we model multivariate time series as more general dynamic graphs, in which both node attributes
and underlying connectivity evolve over time. This allows us to design a more dedicated model to
capture both spatial and temporal information, thereby achieving better performance. On this ba-
sis, we further generalize the existing discrete modeling approaches and propose a more promising
plug-and-play continuous spatiotemporal model on arbitrary multivariate time series data.
Neural Ordinary Differential Equations (NODEs). Chen et al. (2018) introduced a new paradigm
of continuous-time model by generalizing existing discrete deep neural networks. Taking a residual
network for example, the basic concept of NODEs is coherent with Eq. 2. Recently, NODEs have
been adopted in some research fields, such as reinforcement learning (Du et al., 2020), graph neural
networks (Xhonneux et al., 2020; Wang et al., 2021), and traffic forecasting (Fang et al., 2021).
Specifically, as the only ODE-based method for spatiotemporal representation learning, STGODE
(Fang et al., 2021) merely considers the continuous graph propagation on predefined static graph
structures, without modeling the continuous temporal dynamics. Our approach distinguishes from
it in two important aspects. Firstly, we couple the continuous temporal aggregation and graph prop-
agation. Secondly, our method eliminates the reliance on predefined graph structures. As a result,
our method can be applied to broader applications and gain more competitive performance.
3	Methodology
In this section, we present the proposed DyG-ODE method to learn the continuous spatiotemporal
representations of multivariate time series. Our method consists of two main parts named Continu-
ous Graph Propagation (CGP) and Continuous Temporal Aggregation (CTA), as shown in Figure 1.
We also describe the overall architecture together with the model training and highlight the differ-
ences from existing discrete approaches by theoretically analyzing the properties of our method.
Problem formulation. Let X ∈ RN ×D×S denote a multivariate time series with N variables , D
feature dimensions, and S time steps in total for training. Specifically, we define Xi ∈ RD×S as the
i-th time series for all features and time steps, and Xt ∈ RN ×D as the t-th time step for all series
and features. Given a sequence of T historical observations Xt+1:t+T ∈ RN ×D×T, our objective
is to learn a spatiotemporal encoder f (∙) : RN×D×T → RN×D0, where the learned representation
Hout = f (Xt+1:t+T) can then be used in various downstream tasks, such as single-step and multi-
step forecasting of future observations as we demonstrate in Section 4.2. Formally, given a loss
function '(∙) and for each valid time step t, the problem can be formulated as follows:
f *,g* = argminX'(g(f(Xt+ι∙.t+τ)), Y)
f,g t
(3)
where f *(∙) and g*(∙) represent the encoder and decoder with learned optimal parameters. Specif-
ically, we let Y = Xt+T +H ∈ RN×D×1 for single-step forecasting, and Y = Xt+T +1:t+T +H ∈
RN ×D×H for multi-step forecasting. H represents the specific forecasting horizon.
3.1	Continuous Graph Propagation
In DyG-ODE, we integrate the temporal aggregation and graph propagation processes to capture rich
spatiotemporal patterns from historical observations. At each aggregation step and for simplicity, the
spatial dependencies between time series can be characterised by the combination of a feature prop-
agation and a linear transformation on a specific graph snapshot. Specifically, given an adjacency
matrix A ∈ RN ×N and an initial state H0G ∈ RN ×D0 ×Q acquired from the temporal aggregation
process, a discrete formulation of the K-hop graph propagation is defined as (Wu et al., 2019a):
(
H
H
：G+i = A HG,k ∈{0,…,K},
oGut = HGK Φ,
(4)
where Ab denotes the normalized adjacency matrix, HoGut ∈ RN ×D0 ×Q is the output representation,
and Φ ∈ RD0 ×D0 is a trainable parameter matrix. In practice, we define the tensor multiplication
3
Under review as a conference paper at ICLR 2022
Iiiiiiii!
i III ≡
llllllll^
Iiiiiiim
∖ > '
Graph
Constructor
Start
Convolution
(b) Continuous Spatiotemporal Encoding (c) Downstream Decoding
)
End
Convolution
口口口
□
# = [#$%&, #$+2,,••, #$%"]	(a) Preprocessing
Figure 1: The overall framework of DyG-ODE. Given a sequence of historical observations, We first
map them to the latent space and learn an associated graph structure. Then, the continuous dynam-
ics of spatial and temporal clues are modeled by coupling two ODES from different perspectives.
Finally, the learned representations can be used in various tasks, such as the single-step forecasting.
with the Einstein summation in the above equation to sum the element products along specific di-
mensions. This is because the graph propagation only operates on the first two dimensions of hidden
states without aggregating information along the time axis (with sequence length Q).
Compared with GCN (KiPf & Welling, 2017), Eq. 4 eliminates the redundant nonlinearities and
further decouples the feature propagation and transformation steps, resulting in a simpler and more
efficient model while maintaining comparable accuracy. However, this discrete formulation is error-
prone and vulnerable to over-smoothing when performing a deep graph propagation. The underlying
cause of these two problems in Eq. 4 can be uncovered by decomposing the propagation depth K
into the combination of integration time Tcgp and interval ∆tcgp, i.e., K = TCgp∕∆tcgp. From
the perspective of a continuous process, a selected Tcgp and ∆tcgp control the number of function
evaluations, which is equivalent to describe how many times feature propagation is executed, a.k.a.
the propagation depth K in the discrete formulation. Therefore, considering a case where a fixed
integration time and smaller intervals are applied, we can naturally have the following transformation
with propagation steps k ∈ {0, ∙∙∙ ,K} being replaced by a continuous variable t ∈ R+:
HG(t + ∆tcgp) = HG(t) + ∆tcgp(Ab -IN)HG(t)
= (1 - ∆tcgp)IN + ∆tcgpAb HG(t).
(5)
Based on this analysis, we can find that Eq. 4 rigidly couples the propagation depth and integration
time by enforcing ∆tcgp = 1 (i.e., the above equation degrades to Eq. 4 when interval ∆tcgp =
1). If so, letting K = Tcgp → ∞ not only makes the graph Laplacian eigenvalues in a discrete
propagation tend to zeros (Appendix A) but also leads to infinite numerical errors (Appendix B),
which prevents the model from accurately capturing long-range spatial dependencies. In this work,
inspired by Wang et al. (2021), we disentangle the coupling between K and Tcgp , which alleviates
the aforementioned problems by avoiding Tcgp → ∞. We provide detailed theoretical justifications
in Section 3.4. Specifically, in DyG-ODE, we generalize Eq. 4 with its continuous formulation
in the following proposition based on Eq. 5, which allows the model to capture fine-grained and
long-range spatial dependencies between time series.
Proposition 1. The continuous dynamics of simplified graph propagation described in Eq. 4 admits
the following ODE:
dHGt) = (A - In) HG(t),
(6)
with the initial state HG(0) = H0G, where H0G is an intermediate state of the continuous temporal
aggregation process described in Section 3.2.
To further reduce numerical errors, we propose an attentive transformation to replace the linear
mapping in Eq. 4, which integrates not only the final but also the initial and some intermediate
states as the output of graph propagation:
dHG(t)
Hg(0), HG(ti),…，HG(Tcgp) = ODESolVe(HG(0), --(-), 0,"…，Tcgp),
dt	(7)
HoGut =	HG(ti)Φti, ti∈ [0, Tcgp],
4
Under review as a conference paper at ICLR 2022
where HG(ti) denotes the selected intermediate states, and we only take ti that is divisible by ∆tcgp
for simplicity in practice.
Dynamic graph structure learning. In Eq. 6, it remains unknown how the graph adjacency matrix
A is constructed. To address the first technical question (Q1) and handle time series without graph
priors (e.g., unknown A), we adopt a direct optimization approach to learn dynamic graph structures
together with the entire model, where node connections evolve with model training. Specifically, for
a sequence of historical observations, the underlying adjacency matrix A is dynamically optimized
as training progresses, which is precisely defined in Appendix C.
3.2 Continuous Temporal Aggregation
Solving the spatial ODE described in Eq. 7 only allows the model to capture the spatial dependencies
between time series at a certain time step. To complete the missing temporal information, we couple
it with an exterior temporal ODE, which allows DyG-ODE to learn the continuous dynamics of
multivariate time series from both spatial and temporal perspectives.
We first introduce the composition of temporal ODE to characterize the long-term temporal depen-
dencies. In view of the shortcomings of recurrent neural networks (RNNs), such as time-consuming
iteration and gradient explosion (Wu et al., 2019b), we resort to stacking multiple residual temporal
convolution blocks to capture and aggregate temporal patterns in a non-recursive manner:
HT+1 = T(HT, Q1+1) + TCN(HT, Θι), l ∈{0,…，L},	⑻
where TCN(∙, Θι) is an individually parameterized temporal convolution, T(∙) denotes the truncate
function, and HlT ∈ RN×D0 ×Ql is the output of the l-th layer with sequence length Ql . In this for-
mulation, the last dimension of the residual input HlT has to be truncated to Ql+1 before adding to its
transformation because the length of latent representations shrink gradually after each aggregation
step, i.e., Ql+1 = Ql - rl × (k - 1) and Q1 = R - k + 1. Specifically, we define H0T ∈ RN ×D0×R
as the initial state, r, k and R are dilation factor, kernel size, and model receptive field. In practice,
we assure R > T to losslessly encode all historical observations, where R = L(k - 1) + 1 when
r = 1, and R = 1 + (k - 1)(rL - 1)/(r - 1) when r > 1.
However, the discrete formulation in Eq. 8 suffers from two main limitations: (1) It fails to learn the
fine-grained and accurate long-term temporal dynamics with a fixed large integration interval (i.e.,
∆tcta = 1), which breaks the continuity of the latent representation trajectories; (2) It has a large
number of trainable parameters and requires additional designs to avoid the gradient vanishing issue
and ensure model convergence, resulting in high computational overhead. This also hinders building
a deep network to obtain a large receptive field without losing fine-grained information. Thus, we
apply a similar idea to disentangle the coupling between the aggregation depth L and the integration
time Tcta by letting L = Tcta/∆tcta. As such, given a desired terminate time Tcta = tι and initial
state H0T , we characterize the entire continuous temporal aggregation process with a single set of
parameters Θ by making ∆tcta → 0, which is more efficient and allows the model to obtain a
theoretically infinite receptive field by constructing a “deeper” network:
t1
HT= HT +	P(TCN(HT, Θ),R) dt,
t0
HTUt = HT[∙∙∙，-I].
(9)
To achieve this and address the second technical question (Q2), we design a simple zero-padding
trick to ensure the invariance of hidden state dimensions, where the length of latent representations
is padded to R with a padding function P(∙) after each aggregation step. In such a way, We have the
second proposition defined as follows:
Proposition 2 The temporal aggregation process described in Eq. 8 is a discretization of the follow-
ing ODE:
dHT (t)
-ʒdtu= P (TCN(HT (t),t, Θ),R),	(10)
with the initial state HT (0) = H0T, which is obtained by mapping the input signals to the latent
space with a separate convolution layer parameterized by Γsc, i.e., H0T = Conv1×1(X, Γsc).
We illustrate the detailed design of TCN(∙, Θ) in Appendix D.
5
Under review as a conference paper at ICLR 2022
3.3	Overall Architecture and Model Training
Overall architecture. We have the proposed DyG-ODE method defined below by unifying the
aforementioned two ODEs. To address the third technical question (Q3), it is worth noting that
instead of simply concatenate them end-to-end, we take each intermediate state of the temporal ODE
as the initial state of the spatial ODE, thereby allowing the model to intersect individual continuous
graph propagation and temporal aggregation processes simultaneously. Given two black-box ODE
solvers, the learned spatiotemporal representations can be obtained by integrating the following
ODE:
Hout = ODESolVe1 (H(0), dHt),TQ	(11)
dHtt)= P(A( ODESolve2 (TCN(H (t),t, Θ), dHG(τ), 0,τi, ∙∙∙ ,Kgp), φ),r).	(12)
In the above equations, the interior ODE solving and the attentive transformation, i.e., A(∙, Φ),
are given by Eq. 7 by letting HG(0) = TCN(H(t), t, Θ). In particular, we let the initial state
H(0) = HT (0) in Eq. 10, and further define ODESolve1 and ODESolve2 as the Euler or Runge-
Kutta 3/8 method with different integration time and intervals for simplicity.
Model training. The overall framework of DyG-ODE is shown in Figure 1. Given a sequence of
historical observations Xt+1:t+T, we first learn its representation Hout ∈ RN×D0 and then make
predictions based on it with the decoder g(∙, Γdc). Thus, our training objective described in Eq. 3
can be reformulated as follows:
f *,g* = arg min X '(g(f (Xt+i：t+T ；二0, Θ,Γgc, Φ), ΓdJ, Y),	(13)
f,g t
where '(∙) denotes the mean absolute error (MAE). For simplicity, We optimize model parame-
ters via the standard reverse-mode differentiation, which can be replaced by the adjoint sensitiv-
ity method as suggested by Chen et al. (2018) for a better memory efficiency. The algorithm of
DyG-ODE is in Appendix I.
3.4	Comparison with Discrete Variants
Compared with the existing GNN-based methods (Yu et al., 2018; Wu et al., 2019b; 2020), our ap-
proach is free from the over-smoothing issue (e.g., the model performance drops when the number
of layers increases). This allows our model to capture long-range spatial dependencies by disengag-
ing the tie between the graph propagation depth and integration time. Thus, our method can perform
a deeper feature propagation and obtain a larger spatial receptive field by aggregating farther neigh-
boring information. Formally, our method possesses the following properties:
Property 1 The discrete formulation in Eq. 4 is subject to over-smoothing and thus hinders the
model from capturing long-range spatial dependencies by performing deep graph propagation. In
contrast, for a fixed integration time Tcgp, DyG-ODE ensures the convergence of learned spatial
representations by defining K = Tcgp /∆tcgp and letting K → ∞.
Proof. See Appendix A.
Property 2 Increasing the propagation depth in Eq. 4 leads to large numerical errors. In DyG-ODE,
letting K = Tcgp /∆tcgp → ∞ makes the numerical errors approaching to zero with a fixed inte-
gration time Tcgp .
Proof. See Appendix B.
The above properties are further empirically validated in Figure 2 in Section 4.2. Also, in the
proposed temporal ODE, we parameterize the derivation of latent representations to characterize the
nature of a temporal aggregation process. As such, we break the tie between the aggregation depth
and the integration time, which allows constructing an infinitely deep temporal aggregation with a
single set of parameters. Thus, DyG-ODE is not only simpler and easier to converge but also more
efficient and effective to model multivariate time series compared with the existing discrete methods.
6
Under review as a conference paper at ICLR 2022
4 Experiments
In this section, we conduct extensive experiments on five benchmark datasets to show the perfor-
mance of DyG-ODE. We also demonstrate the potency and efficiency of the two proposed continuous
regimes, showing superior properties compared with common discrete neural architectures.
4.1	Experiment Setup
Datasets. We experiment on five benchmark datasets, three of which are conventional time series
datasets (Lai et al., 2018), i.e., Electricity, Solar-Energy, and Traffic, without predefined graph struc-
tures, and the rest two are traffic datasets (Li et al., 2018), i.e., METR-LA and PEMS-BAY, with
predefined sensor maps. We summarize the dataset statistics and specific splits in Appendix E.
Baselines. We evaluate and compare DyG-ODE with representative and state-of-the-art time se-
ries baselines, including LSTNet (Lai et al., 2018), MTGNN (Wu et al., 2020), and HyDCNN (Li
et al., 2021) on three time series datasets for single-step forecasting. We further compare our algo-
rithm with strong GNN-based baselines on two traffic datasets for multi-step forecasting, including
DCRNN (Li et al., 2018), STGCN (Yu et al., 2018), Graph WaveNet (Wu et al., 2019b), MRA-
BCGN (Chen et al., 2020), GMAN (Zheng et al., 2020) and STGODE (Fang et al., 2021). Note that
MTGNN is an algorithm which does not rely on a predefined graph structure, thus it is applicable
and compared in both single/multi-step forecasting settings. See Appendix E for more details.
Evaluation metrics and parameter settings. Following Wu et al. (2020), we adopt Mean Absolute
Error (MAE), Root Mean Square Error (RMSE), and Mean Absolute Percentage Error (MAPE) as
our evaluation metrics for multi-step forecasting; we choose Root Relative Squared Error (RSE)
and Empirical Correlation Coefficient (CORR) as the evaluation metrics for single-step forecasting,
where better performance is indicated by higher CORR and lower RSE values. All experiments are
independently repeated five times and the averaged performances are reported. For more details on
the hyperparameter settings, see Appendix E.
4.2	Results
Table 1: Single-step forecasting results on three benchmark time series datasets. Red denotes the
best performances, and underline highlights the second best results.
Dataset	Metric AR VARMLP GRU LSTNet TPA-LSTM MTGNN HyDCNN DyG-ODE
yticirtcelE cfifarT raloS
UOZyOH UOZyOH UOZyOH
3 6 12 3 6 12 3 6
RSEJ CORR↑	0.0995 0.8845	0.1393 0.8708	0.1102 0.8597	0.0864 0.9283	0.0823 0.9439	0.0745 0.9474	0.0832 0.9354	0.0736 0.9430
RSEJ	0.1035	0.1620	0.1144	0.0931	0.0916	—0.0878—	0.0898	0.0809
CORR↑	0.8632	0.8389	0.8623	0.9135	0.9337	0.9316	0.9329	0.9340
RSEJ	0.1050	0.1557	0.1183	0.1007	0.0964	-0.0916一	0.0921	0.0891
CORR↑	0.8591	0.8192	0.8472	0.9077	0.9250	0.9278	0.9285	0.9279
RSEJ	0.5991	0.5582	0.5358	0.4777	0.4487	0.4162	0.4198	0.4127
CORR↑	0.7752	0.8245	0.8511	0.8721	0.8812	0.8963	0.8915	0.9020
RSEJ	0.6218	0.6579	0.5522	0.4893	0.4658	0.4754	0.4290	0.4259
CORR↑	0.7568	0.7695	0.8405	0.8690	0.8717	0.8667	0.8855	0.8945
RSEJ	0.6252	0.6023	0.5562	0.4950	0.4641	0.4461	0.4352	0.4329
CORR↑	0.7544	0.7929	0.8345	0.8614	0.8717	0.8794	0.8858	0.8899
RSEJ	0.2435	0.1922	0.1932	0.1843	0.1803	0.1778	0.1806	0.1693
CORR↑	0.9710	0.9829	0.9823	0.9843	0.9850	0.9852	0.9865	0.9868
RSEJ	0.3790	0.2679	0.2628	0.2559	0.2347	0.2348	0.2335	0.2171
CORR↑	0.9263	0.9655	0.9675	0.9690	0.9742	0.9726	0.9747	0.9771
RSEJ	0.5911	0.4244	0.4163	0.3254	0.3234	0.3109	0.3094	0.2901
CORR↑	0.8107	0.9058	0.9150	0.9467	0.9487	0.9509	0.9515	0.9577
Overall comparisons. We first report the results of different algorithms on different horizons for
single-step forecasting in Table 1. We have two important observations: In general, our method
achieves the best performance on three time series datasets, even when compared with HyDCNN,
indicating the effectiveness in modeling multivariate time series with dynamic graph neural ODEs;
(2) Our method significantly surpasses MTGNN in most cases with the same graph constructor,
especially for long-term forecasting (i.e., horizon 6 and 12), demonstrating the superiority of our
continuous regimes in capturing fine-grained and long-range spatial and temporal dependencies.
7
Under review as a conference paper at ICLR 2022
Table 2: Results of multi-step forecasting on two benchmark traffic datasets, where red and
underline denote the best and the second best results.
Dataset	Metric		DCRNN STGCN GraphWaveNet GMAN MRA-BCGN MTGNN STGODE DyG-ODE							
	∣uμ∏Gl I	MAE	2.77	2.88	2.69	2.77	2.67	2.69	3.47	2.66
		RMSE	5.38	5.74	5.15	5.48	5.12	5.18	6.76	5.10
METR-LA		MAPE (%)	7.30	7.62	6.90	7.25	6.80	6.90	8.76	
		-MAE-	3.15	3.47	3.07	-307-	3.06		4.36	-300-
		RMSE	6.45	7.24	6.22	6.34	6.17	618	8.47	6.05
		MAPE (%)	8.80	9.57	8.37	8.35	8.30		11.14	8.19
	|哽 09 I	-MAE-	3.60	4.59	3.53	3.40	3.49	^^350^^	5.50	3.39
		RMSE	7.60	9.40	7.37	7.21	7.30	7.25	10.33	7.05
		MAPE (%)	10.5	12.7	10.01	9.72	10.00	9.90	14.32	
	∣uμ∏Gl I	MAE	1.38	1.36			1.34	1.29	1.34	1.43	1.31
		RMSE	2.95	2.96	2.74	2.82	2.72	2.81	2.88	2.76
PEMS-BAY		MAPE (%)	2.90	2.90	2.73	2.81	2.90	2.82	2.99	
	∣uμu 0£ I	-MAE^^	1.74	1.81	1.63	162^^	ɪ	1.66	1.84	-^161 ~~
		RMSE	3.97	4.27	3.70	3.72	3.67	3.74	3.90	3.66
		MAPE (%)	3.90	4.17	3.67		3.80	3.72	3.84	3.62
		-MAE-	2.07	2.49	1.95	-iɪ-	1.91	1.94	2.30	~-~
		RMSE	4.74	5.69	4.52	4.32	4.46	4.48	4.89	4.31
		MAPE (%)	4.90	5.79	4.63	4.31	4.60	4.58	4.61	生39
To further demonstrate the advantage of DyG-ODE, we compare it with competitive GNN-based
methods on two benchmark traffic datasets under the setting of multi-step forecasting, where all
baselines use predefined graph structures only except for MTGNN and our method. We summarize
the results in Table 2, from which we have the following observations: (1) Similar to single-step
forecasting, our method consistently outperforms MTGNN under this setting with the same graph
constructor, which further confirms the effectiveness of DyG-ODE in modeling multivariate time
series data; (2) Our method demonstrates better performance compared with STGODE. We conjec-
ture that it is attributed to two reasons. Firstly, the proposed temporal ODE enables our approach
to capture better fine-grained and long-term temporal dynamics. Secondly, our graph module is not
only more expressive with the attentive transformation but also free from graph priors thus more ro-
bust to dataset biases. (3) DyG-ODE surpasses DCRNN, STGCN, and Graph WaveNet significantly
without relying on graph priors. When compared to MRA-BCGN and GMAN, our method achieves
the best or on-par performance, demonstrating its competitiveness.
0.4 -
≡
ω 0.3 -
Φ
Φ
E
2 0.2 ■
ra
d
⅛
0.1 -
o.o∙
2	4	6	8	10	12
Graph propagation depth
6.35
6.30
6.25 LU
S
W
H
6.20
6.15
6.10
(W) S-JOWEe.Jed #
3	4	5	6	7
Temporal aggregation depth
WSWcc
4 3 2 1
6 6 6 6
Figure 2: Model parameters and averaged performances w.r.t. graph propagation and temporal ag-
gregation depths on the METR-LA dataset. DyG-ODE* is a variant of our method with the attentive
transformation disabled in Eq. 7. DyG-ODE discrete denotes the discrete variant of our method
by combining Eq. 4 and the padding version of Eq. 8.
Case studies and parameter studies. To further evaluate the forecasting quality, we provide two
case studies in Appendix F to visually compare our method with some discrete baselines. Besides,
we also provide detailed parameter studies in Appendix H together with Figure 2 to assess our
method from various perspectives.
Investigating the two continuous regimes. To empirically validate our justifications in Section
3.4 and study the behavioral differences between our method and its discrete variant, we dissect
DyG-ODE by comparing the model performance and the number of parameters with different graph
propagation and temporal aggregation depths. Firstly, the left chart in Figure 2 compares the pro-
posed continuous graph propagation with its discrete implementation (Eq. 4). In particular, we
disable the attentive transformation in this experiment to expose the essence of our proposed spatial
8
Under review as a conference paper at ICLR 2022
Figure 3: The ablation study on three time series datasets. We replace the continuous temporal ag-
gregation and graph propagation in DyG-ODE with their discrete implementations, denoted as w/o
CTA and w/o CGP. We further remove the attentive and continuous regime in CGP to construct the
w/o CGP & Attn. A lower RSE and a higher CORR are expected.
ODE in Eq. 6. Compared with DyG-ODE discrete (solid red curve), our method (solid blue
line) is more robust (in terms of RMSE) to the over-smoothing problem with increased propagation
depths, where the gradually flattened performance curve and shrunk standard deviations indicate
that our method allows the learned spatial representations to converge to a sweet spot by exploit-
ing the long-range spatial dependencies, bringing significantly lower numerical errors (in terms of
RMSE) and better stability (w.r.t. standard deviations). Specifically, in our method, we find that
larger propagation depths benefit to learn more stable representations with the on-par performance
on downstream tasks compared with the shallow propagation, e.g., the MAE and RMSE for propa-
gationdepth2and8are 3.038±0.0131 vs. 3.037±0.0086 and 6.091±0.0342 vs. 6.120±0.0227. At
the same time, our method also demonstrates a better parameter efficiency (in terms of # of parame-
ters) compared with DyG-ODE discrete. It is worth noting that in this experiment, DyG-ODE*
and DyG-ODE discrete have constant parameters as feature propagation is parameterless.
The second chart in Figure 2 compares our method with its discrete variant by varying the temporal
aggregation depth. We can observe that with the increase in aggregation depth, DyG-ODE converges
with decreased numerical errors. We can also observe that for DyG-ODE discrete, an increase
in aggregation depth requires an increase in model parameters, hence its complexity. In contrast,
DyG-ODE breaks this tie and thus allows a deeper and easier-optimized temporal aggregation to
model the long-term and fine-grained continuous temporal dynamics of time series. In Appendix G,
we further demonstrate that DyG-ODE is also more computational efficient than discrete methods.
Ablation study. We construct three variants of our method to study the effectiveness of core com-
ponents. Specifically, DyG-ODE w/o CTA and DyG-ODE w/o CGP replace the temporal and
spatial ODEs with their discrete implementations to study the potency of two continuous regimes.
DyG-ODE w/o CGP & Attn further removes the attentive transformation in DyG-ODE w/o
CGP to investigate the effectiveness of graph attentive transformation. In particular, the DyG-ODE
discrete in Figure 2 is equivalent to DyG-ODE w/o CTA & CGP & Attn, which has been
investigated before so we omit this variant in ablation study. The experimental results are in Figure
3, where our method equipped with all components has the best performance across all datasets. In
particular, we observe that our spatial ODE with the attentive transformation benefits the model best
to learn effective representations. Besides, even though we adopt a relatively shallow propagation
depth (e.g., two steps on the Electricity and Traffic datasets, and four steps on the Solar dataset)
in ablation studies for fair comparisons with semi-discrete variants, the performance gains obtained
by the CGP itself are also notable in most cases. A similar observation can also be made for CTA,
where replacing it with (the padding version of) Eq. 8 degrades the performance sharply.
5 Conclusion
In view of the shortcomings of existing discrete spatiotemporal modeling approaches, we investigate
the use of neural ordinary differential equations and dynamic graph structure learning to model
the continuous spatiotemporal dynamics of multivariate time series by intersecting the continuous
graph propagation and temporal aggregation processes characterized by two ODEs, which allows
the model to obtain more expressive representations without relying on graph priors, showing strong
potential in real-world applications in which such information is not available. We also theoretically
analyze the main properties of the proposed method and further demonstrate that it is not only more
effective but also more efficient compared with the existing discrete methods.
9
Under review as a conference paper at ICLR 2022
References
Ricky TQ Chen, Yulia Rubanova, Jesse Bettencourt, and David Duvenaud. Neural ordinary dif-
ferential equations. In Proceedings of the 32nd International Conference on Neural Information
Processing Systems, pp. 6572-6583, 2018.
Weiqi Chen, Ling Chen, Yu Xie, Wei Cao, Yusong Gao, and Xiaojie Feng. Multi-range attentive
bicomponent graph convolutional network for traffic forecasting. In Proceedings of the AAAI
Conference on Artificial Intelligence, volume 34, pp. 3529-3536, 2020.
KyUnghyUn Cho, Bart van Merrienboer, Caglar Gulcehre, Dzmitry Bahdanau, Fethi Bougares, Hol-
ger Schwenk, and Yoshua Bengio. Learning phrase representations using rnn encoder-decoder
for statistical machine translation. In EMNLP, 2014.
Fan RK Chung and Fan Chung Graham. Spectral graph theory. Number 92. American Mathematical
Soc., 1997.
Jianzhun Du, Joseph Futoma, and Finale Doshi-Velez. Model-based reinforcement learning for
semi-markov decision processes with neural odes. arXiv preprint arXiv:2006.16210, 2020.
Emilien Dupont, Arnaud Doucet, and Yee Whye Teh. Augmented neural odes. In Proceedings of
the 33rd International Conference on Neural Information Processing Systems, pp. 3140-3150,
2019.
Zheng Fang, Qingqing Long, Guojie Song, and Kunqing Xie. Spatial-temporal graph ode networks
for traffic flow forecasting. In Proceedings of the 27th ACM SIGKDD Conference on Knowledge
Discovery & Data Mining, pp. 364-373, 2021.
William L Hamilton, Rex Ying, and Jure Leskovec. Inductive representation learning on large
graphs. In Proceedings of the 31st International Conference on Neural Information Processing
Systems, pp. 1025-1035, 2017.
Ruibing Hou, Hong Chang, Bingpeng Ma, Rui Huang, and Shiguang Shan. Bicnet-tks: Learning
efficient spatial-temporal representation for video person re-identification. In Proceedings of the
IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 2014-2023, 2021.
Siteng Huang, Donglin Wang, Xuehan Wu, and Ao Tang. Dsanet: Dual self-attention network for
multivariate time series forecasting. In Proceedings of the 28th ACM international conference on
information and knowledge management, pp. 2129-2132, 2019.
Weiwei Jiang and Jiayun Luo. Graph neural network for traffic forecasting: A survey. arXiv preprint
arXiv:2101.11174, 2021.
Thomas N. Kipf and Max Welling. Semi-supervised classification with graph convolutional net-
works. In International Conference on Learning Representations (ICLR), 2017.
Guokun Lai, Wei-Cheng Chang, Yiming Yang, and Hanxiao Liu. Modeling long-and short-term
temporal patterns with deep neural networks. In The 41st International ACM SIGIR Conference
on Research & Development in Information Retrieval, pp. 95-104, 2018.
Yaguang Li, Rose Yu, Cyrus Shahabi, and Yan Liu. Diffusion convolutional recurrent neural net-
work: Data-driven traffic forecasting. In International Conference on Learning Representations,
2018.
Yangfan Li, Kenli Li, Cen Chen, Xu Zhou, Zeng Zeng, and Keqin Li. Modeling temporal patterns
with dilated convolutions for time-series forecasting. ACM Transactions on Knowledge Discovery
from Data (TKDD), 16(1):1-22, 2021.
Lingbo Liu, Jiajie Zhen, Guanbin Li, Geng Zhan, Zhaocheng He, Bowen Du, and Liang Lin. Dy-
namic spatial-temporal representation learning for traffic flow prediction. IEEE Transactions on
Intelligent Transportation Systems, 2020.
Emanuele Rossi, Ben Chamberlain, Fabrizio Frasca, Davide Eynard, Federico Monti, and Michael
Bronstein. Temporal graph networks for deep learning on dynamic graphs. In ICML 2020 Work-
shop on Graph Representation Learning, 2020.
10
Under review as a conference paper at ICLR 2022
Chao Shang, Jie Chen, and Jinbo Bi. Discrete graph structure learning for forecasting multiple time
series. In International Conference on Learning Representations, 2020.
Shun-Yao Shih, Fan-Keng Sun, and Hung-yi Lee. Temporal pattern attention for multivariate time
series forecasting. Machine Learning, 108(8):1421-1441, 2θ19.
Joakim Skardinga, Bogdan Gabrys, and Katarzyna Musial. Foundations and modelling of dynamic
networks using dynamic graph neural networks: A survey. IEEE Access, 2021.
Weiping Song, Zhiping Xiao, Yifan Wang, Laurent Charlin, Ming Zhang, and Jian Tang. Session-
based social recommendation via dynamic graph attention networks. In Proceedings of the Twelfth
ACM international conference on web search and data mining, pp. 555-563, 2019.
Petar VeliCkovic, Guillem CUcurulL Arantxa Casanova, Adriana Romero, Pietro Lio, and Yoshua
Bengio. Graph attention networks. In International Conference on Learning Representations,
2018.
Yanbang Wang, Yen-Yu Chang, Yunyu Liu, Jure Leskovec, and Pan Li. Inductive representation
learning in temporal networks via causal anonymous walks. In International Conference on
Learning Representations, 2020.
Yifei Wang, Yisen Wang, Jiansheng Yang, and Zhouchen Lin. Dissecting the diffusion process in
linear graph convolutional networks. arXiv preprint arXiv:2102.10739, 2021.
Felix Wu, Amauri Souza, Tianyi Zhang, Christopher Fifty, Tao Yu, and Kilian Weinberger. Sim-
plifying graph convolutional networks. In International conference on machine learning, pp.
6861-6871. PMLR, 2019a.
Zonghan Wu, Shirui Pan, Guodong Long, Jing Jiang, and Chengqi Zhang. Graph wavenet for
deep spatial-temporal graph modeling. In The 28th International Joint Conference on Artificial
Intelligence (IJCAI), 2019b.
Zonghan Wu, Shirui Pan, Guodong Long, Jing Jiang, Xiaojun Chang, and Chengqi Zhang. Con-
necting the dots: Multivariate time series forecasting with graph neural networks. In Proceedings
of the 26th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining,
pp. 753-763, 2020.
Louis-Pascal Xhonneux, Meng Qu, and Jian Tang. Continuous graph neural networks. In Interna-
tional Conference on Machine Learning, pp. 10432-10441. PMLR, 2020.
Da Xu, Chuanwei Ruan, Evren Korpeoglu, Sushant Kumar, and Kannan Achan. Inductive represen-
tation learning on temporal graphs. arXiv preprint arXiv:2002.07962, 2020.
Bing Yu, Haoteng Yin, and Zhanxing Zhu. Spatio-temporal graph convolutional networks: A deep
learning framework for traffic forecasting. In IJCAI, 2018.
G Peter Zhang. Time series forecasting using a hybrid arima and neural network model. Neurocom-
puting, 50:159-175, 2003.
Mengqi Zhang, Shu Wu, Xueli Yu, and Liang Wang. Dynamic graph neural networks for sequential
recommendation. arXiv preprint arXiv:2104.07368, 2021.
Chuanpan Zheng, Xiaoliang Fan, Cheng Wang, and Jianzhong Qi. Gman: A graph multi-attention
network for traffic prediction. In Proceedings of the AAAI Conference on Artificial Intelligence,
volume 34, pp. 1234-1241, 2020.
11
Under review as a conference paper at ICLR 2022
Appendices
A Proof of Property 1
Given a simplified graph feature propagation in Eq. 4, we give a proof that it is characterized by the
following ODE.
ʌ	dHG(t) _(b _I ) HG
HG+1 = A HG, k ∈{0,…，K} ⇒ dt =( N)	( ),
= -LHG(t), t ∈ R0+,
where L = IN - Ab denotes the normalized graph Laplacian. Regarding the above ODE, it can be
naturally viewed as a general graph heat diffusion process with the Laplacian L (Wang et al., 2021;
Chung & Graham, 1997), where the closed-form solution is:
dHG(t) = —LHG(t)	⇒	HG(t) = e-tLHG (0).
(14)
In the above equation, e-tL is known as the heat kernel. For the graph Laplacian L = IN - Ab , if
1 1	1 1	1	1	1	1	1	1 K_1/K Tx√ζ-1
A is symmetrically normalized, We have L = IN 一 D 2 AD 2 = D 2 (D — A)D 2, which is
symmetric and positive semi-definite. Thus, the eigen-decomposition of L can be defined as follows:
L = UΛU>,
(15)
where U is an orthogonal matrix of eigenvectors, and Λ is a diagonal matrix that consists of eigen-
values λi ≥ 0. Based on this, the heat kernel can be decomposed as follows based on the Taylor
expansion:
∞	∞k	∞k
e-tL = X k!(-tL)k = X k! [5-A)u>]k = U[X k!(-A)k]U> = UeTAU>.	(16)
kk	k
k=0	k=0	k=0
Thus, the eigen-decomposition of the heat kernel can be easily obtained:
0
0
e-tL = U
∕e-tλ1
0
0
e-tλ2
U>,
(17)
0
0
e-tλN
where for each of eigenvalues e-tλi, they satisfy the following property when t → ∞:
tλ 0, if λi > 0
tl→im∞ e	=	1, if λi = 0.
(18)
As such, given a graph that A 6= IN, increasing the propagation depth in Eq. 4 will inevitably lead
to the over-smoothing problem, where the eigenvalues are zeroed with K = T → ∞:
lim HG(t) = e-TLHG(0) = 0.
T→∞
(19)
Conversely, in the proposed spatial ODE, we disengage the coupling between the propagation depth
K and terminal (integration) time Tcgp by making K = Tcgpl∆tcgp. Thus, it is possible to increase
the propagation depth without letting Tcgp → ∞, which ensures the convergence of the learned
spatial representations.
12
Under review as a conference paper at ICLR 2022
B Proof of Property 2
Similar to the proof in Appendix A, given a terminal (integration) time Tcgp , the proposed spatial
ODE can be viewed as a general graph heat diffusion process with the Laplacian L, where the
closed-form solution is given by:
HG(Tcgp) = e-TcgpL HG(0).
(20)
For the heat kernel e-TcgpL, its can be expanded in a Taylor series:
e-TcgpL
∞ Tk
X Tkgp(-L)k.
k=0
(21)
Accordingly, Eq. 20 can be reformulated as follows:
∞ Tk
HG(Tcgp) = [X 节(-L)k] HG(0).	(22)
k=0
Considering an Euler solver is applied, the numerical solution of the above equation after K propa-
gation steps is:
HG(TcgP) = (IN - TKpL)k HG(0).
(23)
Thus, the numerical errors between the analytical and solved numerical solutions (i.e., Eq. 22 and
23) can be simply defined as follows:
E(TKcg)p = HG(Tcgp) - Hb G(Tcgp).
(24)
According to Wang et al. (2021), we have E(TK)
to be upper bounded by the following inequation:
IIETcgpll = TcgP kL2KHG(O)U (eTcgPkLk - 1)	(25)
Thus, for a fixed terminal time Tcgp, we can easily find that E(TK) → 0 by letting the propagation
depth K → ∞. Conversely, let T = K → ∞ in Eq. 4 will lead E(TK) → ∞.
C Additional Details of graph structure learning
The adjacency matrix A used in Eq. 6 is dynamically optimized as training evolves, which is given
by:
(Mk = tanh(β EkΓgkc), k ∈ {1, 2},
I Aij = ReLU (tanh (β(MjM2jT - M2jMjT))),	(26)
where M1, M2 ∈ RN×d are characterised by two neural networks with randomly initialized embed-
ding matrices E1 , E2 ∈ RN ×d and trainable parameters Γg1c , Γg2c ∈ Rd×d . β is a hyperparameter to
adjust the saturation rate of activation functions. In particular, the learned graph structure is made
sparse to reduce the computational cost and is supposed to be uni-directional because changes in a
time series are likely to unidirectionally lead to fluctuations in other series (Wu et al., 2020).
13
Under review as a conference paper at ICLR 2022
D	Additional Details of Continuous Temporal Aggregation
For the design of TCN(∙, Θ) within the temporal ODE function (Eq. 10), We adopt a gating mech-
anism to control the amount of information flows at each aggregation step:
TCN(HT(t),t,Θ)=fC(HT(t),t,Θc)fG(HT(t),t,Θg),	(27)
where Θ denotes the element-wise product, fc(∙, Θc) and fg(∙, Θg) are filtering and gating Convo-
lutions that share the identical network structure but with different parameters. Formally, we define
them as follows:
∫fc(HT(t),t, Θc) = tanh (Convι×m (HT(t), θC×m)) = tanh (Wθι×m *δ HT(t) + bθι×m),
[fG(HT(t),t, Θg) = σ(Convι×m (HT(t), θg×m)) = σ(Wθg×m *δ HT(t) + b®g*),
(28)
where σ(∙) represents the sigmoid activation, and ?δ denotes the convolution operation with an
expandable dilation defined by δ = brt/∆tcta[. In practice, adopting a single kernel size is less
effective to explore multi-granularity temporal patterns. Thus, inspired by Wu et al. (2020), we
equip fc(∙, Θc) and fg (∙, Θg) with multiple convolutions with different kernel widths m. Since the
most of time series data has inherent periods (e.g., 7, 14, 24, 28, and 30), letting kernel width in
set {2, 3, 6, 7} makes the aforementioned periods can be fully covered by integrating Eq. 10. To
achieve this, we truncate and concatenate the outputs of ConVι×m(∙, Θ1×m) with m = {2,3,6,7}
before the nonlinear activation.
E Additional Details of Experiment Setting
Table 3: Dataset Statistics.
Datasets	# Samples	# Nodes	Sampling Rate	Input Length	Output Length
Electricity	26,304	321	1 hour	168	1
Solar-Energy	52,560	137	10 minutes	168	1
Traffic	17,544	862	1 hour	168	1
METR-LA	34,272	207	5 minutes	12	12
PEMS-BAY	52,116	325	5 minutes	12	12
E.1 Single-step forecasting
We experiment on three benchmark multivariate time series datasets under this evaluation protocol:
•	Electricity1: This dataset consists of the energy consumption records of 321 clients between
2012 and 2014 with the sampling rate set to 1 hour.
•	Solar-Energy2: It contains the solar power production records of 137 PV plants in Alabama
State in the year of 2006, where the sampling rate is 10 minutes.
•	Traffic3 : A collection of hourly road occupancy rates measured by 862 sensors in San
Francisco Bay area between 2015 and 2016.
Following Lai et al. (2018) and Wu et al. (2020), we choose the input length 168 and split all above
three datasets into training set (60%), validation set (20%), and testing set (20%) chronologically.
1https://archive.ics.uci.edu/ml/datasets/ElectricityLoadDiagrams20112014
2http://www.nrel.gov/grid/solar-power-data.html
3http://pems.dot.ca.gov
14
Under review as a conference paper at ICLR 2022
We summarize the detailed dataset statistics in Table 3. In this setting, we evaluate the quality
of learned spatiotemporal representations by comparing our method with the following time series
forecasting baselines:
•	AR: The auto-regressive model.
•	VARMLP: The combination of the auto-regressive model and the multi-layer perception
(Zhang, 2003).
•	GRU: The recurrent neural network with the gated recurrent units (Cho et al., 2014).
•	LSTNet: It combines the convolution and recurrent neural networks to capture the short-
term and long-term multivariate temporal dependencies (Lai et al., 2018).
•	TPA-LSTM: An attention-based recurrent neural network for multivariate time series fore-
casting (Shih et al., 2019).
•	MTGNN: A time series forecasting model based on graph neural networks and dilated
temporal convolutions (Wu et al., 2020).
•	HyDCNN: It forecasts time series with a position-aware dilated temporal convolution neu-
ral network (Li et al., 2021).
We repeat our experiments five times and report the averaged results in Table 1. Specifically, the
model is trained by the Adam optimizer with batch size 4, dropout rate 0.3, and gradient clip 5. The
encoder and decoder hidden dimensions are fixed to 64 across all three datasets. For Electricity and
Traffic, we train the model over 60 epochs with the base learning rate set to 0.001, where the learning
decay is applied at epochs 20 and 40 with the ratio 0.5. For Solar-Energy, the model is trained
40 epochs with the learning rate 0.0001. For ODE-specific hyperparameters, we adopt the basic
Euler solver in both spatial and temporal ODEs across all three datasets, where the integration times
Tcta and Tcgp are set to 1.0 by default. For Electricity and Traffic, we let the integration intervals
∆tcta = 0.2 and ∆tcgp = 0.5. For Solar-Energy, we have ∆tcta = 0.167 and ∆tcgp = 0.25. For
the graph constructor, we adopt the hyperparameters suggested by Wu et al. (2020).
E.2 Multi-step forecasting
To further evaluate our method under the setting of multi-step forecasting, we conduct experiments
on two benchmark traffic datasets:
•	METR-LA4 : It contains the traffic speed readings with 5 minutes sampling rate from the
207 loop detectors in Los Angeles County highways in the year of 2012.
•	PEMS-BAY4: This dataset is provided by California Transportation Agencies Performance
Measurement Systems, which consists of the traffic speed readings of 325 sensors in the
Bay Area in the year of 2017, where the data sampling rate is same as in METR-LA.
Following (Li et al., 2018) and (Wu et al., 2020), we let the input and output lengths equal to 12 and
further split two datasets into training set (70%), validation set (20%), and testing set (10%) chrono-
logically. The detailed dataset statistics are provided in Table 3. Under this evaluation protocol, we
select several GNN-based baselines:
•	DCRNN: A graph diffusion-based convolutional recurrent neural network for traffic fore-
casting (Lai et al., 2018).
•	STGCN: It stands for the spatiotemporal graph convolution network, which stacks graph
and temporal convolutions to capture the spatial and temporal patterns simultaneously (Yu
et al., 2018).
•	Graph WaveNet: A spatiotemporal graph neural network that consists of graph and dilated
1D convolutions (Wu et al., 2019b).
•	GMAN: A spatiotemporal graph neural network with the spatial and temporal attentions
(Zheng et al., 2020).
4https://github.com/liyaguang/DCRNN
15
Under review as a conference paper at ICLR 2022
•	MRA-BCGN: The multi-range attentive bicomponent graph convolution network for traffic
forecasting (Chen et al., 2020).
•	MTGNN: A time series forecasting model based on graph neural networks and dilated
temporal convolutions (Wu et al., 2020).
•	STGODE: An ODE-based spatiotemporal graph neural network for traffic forecasting
(Fang et al., 2021).
Similar to the single-step forecasting, we repeat our experiments five times and report the averaged
results in Table 2. For both datasets, we train the model 200 epochs by adopting the Adam optimizer
with the base learning rate 0.001, gradient clip 5, and dropout rate 0.3. For METR-LA, the encoder
and decoder hidden dimensions are 64 and 128. We adopt the Euler method to solve both spatial
and temporal ODEs, where the integration times and intervals are set to 1.0 and 0.25. For PEMS-
BAY, both encoder and decoder hidden dimensions are 128. On this dataset, we adopt the Runge-
Kutta 3/8 method, where the integration times and intervals are both 1.0 for simplicity. All these
experiments use the batch size 64 and the learning rate decay with the ratio 0.1 at the epoch 100.
The hyperparameters of the graph constructor are same as the single-step forecasting.
F Additional Details of Forecasting Quality
We provide the forecasting visualizations of DyG-ODE on two datasets with different evaluation
protocols. In Figure 4, we plot the forecasting results on 14 November 2006 of our model and two
baseline methods on the Solar-Energy dataset, where the forecasting horizon is 30 minutes, i.e.,
H = 3. In this experiment, we randomly selected the readings of two PV plants as the ground truth,
where we find that (1) DyG-ODE constantly gives more stable and accurate predictions compared
with MTGNN, which can be regarded as a special discrete variant of our method; (2) Compared with
conventional time series models (e.g., LSTNet), our approach provides more precise modeling of
multivariate time series by leveraging the mechanisms of continuous spatial propagation, continuous
temporal aggregation, and dynamic graph learning.
(MWr3M0d
Node 15
(M=r3 MOd
o
05:10	06:10	07:10	08:10	09:10	10:10	11:10	12:10	13:10	14:10	15:10	16:10	17:10
Node 85
05:10	06:10	07:10	08:10	09:10	10:10	11:10	12:10	13:10	14:10	15:10	16:10	17:10
Figure 4: Visualization on two example time series from the Solar-Energy dataset.
We also visualize the multi-step forecasting results on the METR-LA dataset, as shown in Figure 5.
Specifically, we randomly select two sensors (i.e., node 4 and 22) and plot the outputs of DyG-ODE
and two baseline methods between 5 and 6 June in 2012, where the forecasting horizon is 15 minutes,
i.e., H = 3. Similarly, we observed that (1) In general, our method can constantly and precisely
track the trends, but DCRNN sometimes cannot do it, e.g., the blue dashed line between 7 and 11
am in the second chart of Figure 5; (2) DyG-ODE provides more accurate forecasting than MTGNN
16
Under review as a conference paper at ICLR 2022
by learning more expressive spatiotemporal representations, e.g., the red dashed line between 4.30
and 7.30 pm in both charts of Figure 5.
22:30	01:30	04:30	07:30	10:30	13:30	16:30	19:30	22:30
Node 22
50
P
<υ
g 4。
S
U
g 30
I-
20-
10
22:30
01:30	04:30	07:30	10:30	13:30	16:30	19:30	22:30
Figure 5: Visualization on two example time series from the METR-LA dataset.
G Additional details of model computational efficiency
(Sdqlx∙sωo<z
3.0 -
2.5 -
2.0 -
1.5 -
1.0 -
0.5 -
0.0 -
DyG DyG-ODE
MTG MTGNN
DyG-ODE discrete
3	4	5	6	7	8
Model depth
Figure 6: The computational overhead of DyG-ODE and two discrete methods w.r.t. the model (i.e.,
temporal aggregation) depth on the METR-LA dataset.
In Figure 6, we compare the required multiply-accumulate operations (MACs) of DyG-ODE, its
discrete variant, and MTGNN (Wu et al., 2020). Specifically, our method constantly has a lower
computational overhead than DyG-ODE discrete and MTGNN, especially for large model
depths, demonstrating the computational efficiency of DyG-ODE. In comparison, discrete methods,
e.g., MTGNN and our discrete variant, have more complex neural architectures, which inevitably
introduce more intermediate operations and thus increase the computational overhead. In particular,
we find that MTGNN is slightly more efficient than DyG-ODE discrete because the latter one
adopts the padding version of Eq. 8, which inevitably involves more trainable parameters in the
following L - 1 temporal convolution layers except for the first layer. Although our continuous
version is also based on this padding trick, it is significantly more computationally efficient than
MTGNN.
17
Under review as a conference paper at ICLR 2022
H Additional details of parameter study
6.15
6.10
6.05
6.40
6.35
6.30
2520
山S≡H
45	90	135	180	225	270
Hidden state dimension
6.10
6.05
6.25
6.20
山SWH
0	25	50	75	100	125
Batch SiZe
6.16 -
6.14 -
"6'106∞
山SH
6.06 -
6.04 -
0.25	0.50	1.00	2.00	4.00
Temporal integration time
山 sɪH
6.1
0.25	0.50	1.00	2.00	4.00
Spatial integration time
Figure 7: Parameter study on the METR-LA dataset.
Apart from the experiments on spatial and temporal propagation depths (a.k.a spatial and temporal
integration intervals where the terminal times are fixed) in Figure 2, we also conduct experiments
on other important hyperparameters in DyG-ODE, including temporal integration time Tcta, spatial
integration time Tcgp , spatiotemporal encoder hidden dimension D0 , and batch size B, to investigate
their impacts on our model, as shown in Figure 7. Specifically, we have the following observations:
(1) Moderately increase the dimensions of the hidden states in DyG-ODE helps the model learning.
We conjecture that this helps avoid the ODE trajectories intersecting with each other (Dupont et al.,
2019), thus encourages our model to learn smoother ODE functions that can be easier solved; (2) For
a specific spatial or temporal propagation depth, we can find a sweet spot when selecting the spatial
or temporal integration time. It is possibly because a short terminal time hinders the convergence of
the learned representations and a long time introduces relatively large numerical errors; (3) Within
a reasonable range, e.g., from 32 to 128, moderately increasing the batch size improves the model
performance. We hypothesize that a relatively large batch size in our method helps reduce the
variances of mini-batch gradients, which reduces the impact of noise on the model training.
18
Under review as a conference paper at ICLR 2022
I	Algorithm
Algorithm 1 The training algorithm of DyG-ODE.
Input: The training set X, input lengh T , horizon H, batch size B, training epoch E, learning rate
	η, and the initialized DyG-ODE model F(∙) with Θ, Φ, and Γ
1 2 3 4 5 6 7 8 9 10 11	train_loader J DataLoader(X, T, H, B) for i ∈ 1,2,…，E do train_loader. shuffle()	. Shuffle the training batches for (X, Y) in enumerate (trainJoader. getjterator()) do :	Yb J F(X; Θ,Φ,Γ) :	L J MAE(Yb,Y) :	Calculate the stochastic gradient of Θ, Φ, and Γ with respect to L :	Update Θ, Φ, and Γ based on their gradients and η :	end for :	η J LRScheduler(η, i)	. Update the learning rate w.r.t. training steps : end for
Output: The trainedDyG-ODE model F*(∙)
Algorithm 2 Solve the DyG-ODE F(∙).
Input: Historical observations X, temporal terminal time Tcta, spatial terminal time Tcgp, reception
	field R, and dilation factor r
1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20	def SPatiaLode(Hin, A) :	A J Normalization(A)	. Get the normalized adjacency matrix :	Hout J Ab Hin - Hin	. The sPatial ODE function defined in Eq. 6 :	return Hout : end def def temporaLode(Hin) :	H J TCN(Hin, Θ)	. The temporal convolution defined in Eq. 27 H0,…，HTc	J ODESolve(H, Spatial_ode, 0,…，Tcgp)	. Solve the interior ODE H J Attn ([Ho,…,Hτcgp], Φ)	. Attentive transformation defined in Eq. 7 Hout J zero_padding(H, R)	. The padding trick in Eq. 12 TCN(∙, Θ). UPdate_dilation(r)	. Update the dilation of temporal convolution :	return Hout : end def : H0 J Conv(X, Γsc)	. Map the input to the latent space nodeSdx J range(X.shape[1])	. Get all time series indices in X A J graph」earner (node_idx, Γgc)	. Construct a graph for X at the i-th training step H J ODESolver(Ho,temporal_ode, Tcta)	. Solve the exterior ODE defined in Eq. 12 : Y J Conv(H, Γdc)	. Forecast based on learned representations ^
Output: The forecasting results Y
19