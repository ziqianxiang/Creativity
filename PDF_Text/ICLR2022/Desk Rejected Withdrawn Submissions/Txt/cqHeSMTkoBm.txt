Under review as a conference paper at ICLR 2022
Learning Multi-Objective Curricula for Deep
Reinforcement Learning
Anonymous authors
Paper under double-blind review
Ab stract
Various automatic curriculum learning (ACL) methods have been proposed to im-
prove the sample efficiency and final performance of deep reinforcement learning
(DRL). They are designed to control how a DRL agent collects data, which is in-
spired by how humans gradually adapt their learning processes to their capabilities.
For example, ACL can be used for subgoal generation, reward shaping, environ-
ment generation, or initial state generation. However, prior work only considers
curriculum learning following one of the aforementioned predefined paradigms. It
is unclear which of these paradigms are complementary, and how the combination
of them can be learned from interactions with the environment. Therefore, in this
paper, we propose a unified automatic curriculum learning framework to create
multi-objective but coherent curricula that are generated by a set of parametric cur-
riculum modules. Each curriculum module is instantiated as a neural network and
is responsible for generating a particular curriculum. In order to coordinate those
potentially conflicting modules in unified parameter space, we propose a multi-task
hyper-net learning framework that uses a single hyper-net to parameterize all those
curriculum modules. In addition to existing hand-designed curricula paradigms,
we further design a flexible memory mechanism to learn an abstract curriculum,
which may otherwise be difficult to design manually. We evaluate our method on
a series of robotic manipulation tasks and demonstrate its superiority over other
state-of-the-art ACL methods in terms of sample efficiency and final performance.1
1	Introduction
The concept that humans frequently organize their learning into a curriculum of interdependent
processes according to their capabilities was first introduced to machine learning in (Selfridge et al.,
1985). Over time, curriculum learning has become more widely used in machine learning to control
the stream of examples provided to training algorithms (Elman, 1993; Bengio et al., 2009), to
adapt model capacity (Krueger & Dayan, 2009), and to organize exploration (Schmidhuber, 1991).
Automatic curriculum learning (ACL) for deep reinforcement learning (DRL) (Portelas et al., 2020a)
has recently emerged as a promising tool to learn how to adapt an agent’s learning tasks to its
capabilities during training. ACL can be applied to DRL in various ways, including adapting initial
states (Florensa et al., 2017b; Ivanovic et al., 2019), shaping reward functions (Bellemare et al., 2016;
Shyam et al., 2019), and generating goals (Lair et al., 2019; Sukhbaatar et al., 2017).
Oftentimes, only a single ACL paradigm (e.g., generating subgoals) is considered. It remains an open
question whether different paradigms are complementary to each other and if yes, how to combine
them in a more effective manner similar to how the “rainbow” approach of Hessel et al. (2018)
has greatly improve DRL performance in Atari games. Multi-task learning is notoriously difficult
(Caruana, 1997; Ruder, 2017) and Yu et al. (2020) hypothesize that the optimization difficulties
might be due to the gradients from different tasks confliciting with each other thus hurting the
learning process. In this work, we propose a multi-task bilevel learning framework for more effective
multi-objective curricula DRL learning. Concretely, inspired by neural modular systems (Andreas
et al., 2016; Yang et al., 2020) and multi-task RL (Wang et al., 2020; Yang et al., 2020), we utilize
a set of neural modules and train each of them to output a curriculum. In order to coordinate
potentially conflicting gradients from modules in a unified parameter space, we use a single hyper-net
1The code is available in the supplementary material.
1
Under review as a conference paper at ICLR 2022
(Schmidhuber, 1992; Ha et al., 2017) to parameterize neural modules so that these modules generate
a diverse and cooperative set of curricula. Multi-task learning provides a natural curriculum2 for
the hyper-net itself since learning easier curriculum modules can be beneficial for learning of more
difficult curriculum modules with parameters generated by the hyper-net.
Furthermore, existing ACL methods usually rely on manually-designed paradigms of which the target
and mechanism have to be clearly defined and it is therefore challenging to create a very diverse
set of curriculum paradigms. Consider goal-based ACL for example, where the algorithm is tasked
with learning how to rank goals to form the curriculum (Sukhbaatar et al., 2017). Many of these
curriculum paradigms are based on simple intuitions inspired by learning in humans, but they usually
take too simple forms (e.g., generating subgoals) to apply to neural models. Instead, we propose
to augment the hand-designed curricula introduced above with an abstract curriculum of which
paradigm is learned from scratch. More concretely, we take the idea from memory-augmented meta-
DRL (Blundell et al., 2016; Lengyel & Dayan, 2007) and equip the hyper-net with a non-parametric
memory module, which is also directly connected to the DRL agent. The hyper-net can write entries
to and update items in the memory, through which the DRL agent can interact with the environment
under the guidance of the abstract curriculum maintained in the memory. The write-only3 permission
given to the hyper-net over the memory is distinct from the common use of memory modules in
meta-DRL literature, where the memories are both readable and writable. We point out that the
hyper-net is instantiated as a recurrent neural network (Hochreiter & Schmidhuber, 1997; Cho et al.,
2014) which has its own internal memory mechanism and thus a write-only extra memory module is
enough. Another key perspective is that such a write-only memory module suffices to capture the
essences of many curriculum paradigms. For instance, the subgoal-based curriculum can take the
form of a sequence of coordinates in a game which can be easily generated a hyper-net and stored in
the memory module.
In summary, our proposed framework helps in learning coordinated curricula rather than naively
combining multiple curricula that are otherwise trained independently. The combination of the
curriculum modules and the memory module further boosts the performance in terms of sample-
efficiency and unifies memory-augmented meta-DRL and ACL. We demonstrate our approach in a
rich robotic manipulation environment, and show that it substantially outperforms state-of-the-art
baselines and naive ACL ensemble methods.
2	Preliminaries
Reinforcement learning (RL) (Sutton & Barto, 2018) is used to train an agent policy with the goal
of maximizing the (discounted) cumulative rewards through trial and error. A basic RL setting is
modeled as a Markov decision process (MDP) with the following elements: S is the set of environment
states; A is the set of actions; δ is the state transition probability function, where δ(st+1 |st, at) maps
a state-action pair at time-step t to a probability distribution over states at time t + 1; R is the
immediate reward after a transition from S to s0; ∏(∙; φ∏) is the policy function parameterized by φ∏,
and π(a∣s; φ∏) denotes the probability of choosing action a given an observation s.
Automatic curriculum learning (ACL) is a learning paradigm where an agent is trained iteratively
following a curriculum to ease learning and exploration in a multi-task problem. Since it is not
feasible to manually design a curriculum for each and every task, recent work has proposed to create
an implicit curriculum directly from the task objective. Concretely, it aims to maximize a metric P
computed over a set of target tasks T 〜Ttarget after some episodes t0. Following the notation in
(Portelas et al., 2020a), the objective is set to: max。JT〜T 七 PT dT, where D : H → Ttarget is a
task selection function. The input of D is H the history, and the output of D is a curriculum such as
an initial state.
Hyper-networks were proposed in (Schmidhuber, 1992; Ha et al., 2017) where one network (hyper-
net) is used to generate the weights of another network. All the parameters of both networks are
trained end-to-end using backpropagation. We follow the notation in (Galanti & Wolf, 2020) and
suppose that we aim to model a target function y : X × I → R, where x ∈ X is independent of
the task and I ∈ I depends on the task. A base neural network fb(x; fh(I; θh)) can be seen as a
2This curriculum is not targeted at the DRL agent.
3To the DRL agent, the memory module is read-only.
2
Under review as a conference paper at ICLR 2022
composite function, where fb : X → R and fh : I → Θb . Conditioned on the task information I, the
small hyper-net fh(I; θh) generates the parameters θb of base-net fb. Note that θb is never updated
using loss gradients directly.
3	Learning Multi-Objective Curricula
We use a single hyper-net to dynamically parameterize all the curriculum modules over time and
modify the memory module shared with the DRL agent. We call this framework a Multi-Objective
Curricula (MOC). This novel design encourages different curriculum modules to merge and exchange
information through the shared hyper-net.
Following the design of hyper-networks with recurrence (Ha et al., 2017), this hyper-net is instantiated
as a recurrent neural network (RNN), which we refer to as the Hyper-RNN, denoted as fh(I; θh), in
the rest of this paper to emphasize its dynamic nature. Our motivation for the adoption of an RNN
design is its capability for producing a distinct set of curricula for every episode, which strikes a
better trade-off between the number of model parameters and its expressiveness. On the other hand,
each manually designed curriculum module is also instantiated as an RNN, which is referred as a
Base-RNN fb(x; θb) parameterized by θb = fh(I; θh). Each Base-RNN is responsible for producing
a specific curriculum, e.g., a series of sub-goals.
Figure 1: Illustration of MOC-DRL with two 9
loops. Curricula generation corresponds to the 川
outer-level loop. The DRL agent interacts with
the environment in the inner-level loop.
Algorithm 1: Multi-Objective Curricula
Deep Reinforcement Learning (MOC-DRL).
for Episode t in 1 to ToUter do
•	Sample a new environment from the
distribution of environments;
•	Hyper-RNN generates parameters for each
curriculum module;
for Base-RNN in 1 to 3 do
L ∙ Generate a curriculum;
• Hyper-RNN updates the abstract
curriculum in the memory;
for Training step t in 1 to Tinner do
•	DRL agent reads memory;
•	Train DRL agent following curricula;
• Update Hyper-RNN based on outer-level
objective;
The architecture of MOC-DRL is depicted in Fig. 1, and its corresponding pseudo-code is given in
Alg. 1. We formulate the training procedure as a bilevel optimization problem (Grefenstette et al.,
2019) where we minimize an outer-level objective that depends on the solution of the inner-level
tasks.
In our case, the outer-level optimization comes from the curriculum generation loop where each step
is an episode denoted as t0 . On the other hand, the inner-level optimization involves a common DRL
agent training loop on the interactions between the environment and the DRL agent, where each
time-step at this level is denoted as t. We defer the discussion on the details to Sec. 3.3.
Inputs, I, of the Hyper-RNN, fh, consist of: (1) the final state of the last episode, and (2) role
identifier for each curriculum module (e.g., for initial states generation) represented as a one-hot
encoding. Ideally, we expect each Base-RNN to have its own particular role, which is specific to
each curriculum.When generating the parameters for each Base-RNN, we additionally feed the role
identifier representation to the Hyper-RNN.
Outputs of the Hyper-RNN at episode t0 include: (1) parameters θbt0 for each Base-RNN, and (2)
the abstract curriculum, hth0 , maintained in the memory module. Here hth0 corresponds to the hidden
states of the Hyper-RNN such that [θbt0 , hth0] = fh(It0 ; θh).
In Sec. 3.1, we describe the details of generating manually designed curricula while the process of
updating the abstract curriculum is described in Sec. 3.2. We describe how to train them in Sec. 3.3.
3
Under review as a conference paper at ICLR 2022
3.1	Manually Designed Curricula
In this work, we use three curriculum modules responsible for generating pre-defined curricula (Porte-
las et al., 2020a): initial state generator, sub-goal state generator, and reward shaping generator.Our
approach can be easily extended to include other forms of curricula (e.g., selecting environments from
a discrete set (Matiisen et al., 2019)) by adding another curriculum generator to the shared hyper-net.
These Base-RNNs simultaneously output the actual curricula for the DRL agent in a synergistic
manner. It should be noted that these Base-RNNs are not directly updated by loss gradients, as their
pseudo-parameters are generated by the Hyper-RNN.
Generating subgoal sequences Y 1:t as curriculum Cgoal with Base-RNN f As one popular
choice in ACL for DRL, the subgoals can be selected from discrete sets (Lair et al., 2019) or a contin-
uous goal space (Sukhbaatar et al., 2017). A suitable subgoal state sequence Yi：t can ease the learning
procedures by guiding the agent how to achieve subgoals step by step and ultimately solving the final
task. To incorporate the subgoal state in the overall computation graph, in this paper, we adopt the idea
from universal value functions (Schaul et al., 2015) and modify the action-value function, Q(∙; φq),
to combine the generated subgoal sequences with other information Q := Q(st,at,γ1"; φq)=
Q(st, at, Cgoal; φq), where St is the state, at is the action, and YLt is the generated subgoal state
sequences. The loss is defined as Jgoal =旧但川产,st+ι,γt)〜电” [(Q(st, at, Cgoal； φq) - y/)2], where
y is the one-step look-ahead:
y = rt + λEat+ι〜∏θ(st+i)[Q(st,at, Cgoal; φq) - log(π(at+1 |st+1; φ∏))],	(1)
Hbuf is the replay buffer and λ is the discount factor. Moreover, we use one subgoal state as the
input by averaging the subgoal sequences Y1:t.
Generating initial state s0 as curriculum Cinit with Base-RNN fb. Intuitively, if the starting state
s0 for the agent is close to the end-goal state, the training would become easier, which forms a natural
curriculum for training tasks whose difficulty depends on a proper distance between initial state
and the end-goal state. This method has been shown effective in control tasks with sparse rewards
(Florensa et al., 2017b; Ivanovic et al., 2019). To simplify implementation, even though we only need
a single initial state s0 which is independent of time, we still use a Base-RNN,fb, to output it.
To evaluate the generated initial states, we incorporate it into the action-value (Q) function and
estimate the expect return. The loss for this module is: Jinit = E(st,at)〜Hbuf [(Q(st, at, cinit; Φq)-
y)2], where y is defined in Eqn. 1.
...
Generating potential-based shaping function as curriculum Crew with Base-RNN f b. Moti-
vated by success of using reward shaping for scaling RL methods to handle complex domains (Ng
et al., 1999), we introduce the reward shaping as the third manually selected curriculum. The reward
...	...	...
shaping function can take the form of: f∖(st, at, st+1) = μ ∙ f b(st+1) - f b(st), where μ is a
hyper-parameter and f b() is base-RNN that maps the current state with a reward. In this paper, we
...
add the shaping reward f 0b(st, at, st+1) to the original environment reward r. We further normalize
the shaping reward between 0 and 1 to deal with wide ranges.
... llowing... e optimal policy invariant theorem (Ng et al., 1999), we modify the look-ahead function:
y. = rt + f b(st, at, St+1 + λEat+ι〜∏θ(st+i) [Q(st, at, Crew； φq) - log(∏(at+1 ∣st+1; φ∏))]. Thus the
loss is defined as: Jreward = Est,at,st+1,at+1 〜Hbuf [(Q(st, at, Crew ； Φq ) - V)2].
3.2	Abstract Curriculum with Memory Mechanism
Although the aforementioned hand-designed curricula are generic enough to be applied in any
environment/task, it is still limited by the number of such predefined curricula. It is reasonable to
conjecture that there exist other curriculum paradigms, which might be difficult to hand-design based
on human intuition. As a result, instead of solely asking the hyper-net to generate human-engineered
curricula, we equip the hyper-nets with an external memory, in which the hyper-nets could read and
update the memory’s entries.
By design, the content in the memory can serve as abstract curricula for the DRL agent, which is
generated and adapted according to the task distribution and the agent’s dynamic capacity during
training. Even though there is no constraint on how exactly the hyper-net learns to use the memory, we
4
Under review as a conference paper at ICLR 2022
observe that (see Sec. 4.3): 1) The hyper-net can receive reliable training signals from the manually
designed curriculum learning objectives4; 2) Using the memory module alone would result in unstable
training; 3) Utilizing both the memory and manually curricula achieves the best performance and
stable training. Thus, training this memory module with other manually designed curriculum modules
contributes to the shaping of the content that can be stored in the memory and is beneficial for the
overall performance.
Specifically, an external memory is updated by the Hyper-RNN. To capture the latent curriculum
information, we design a neural memory mechanism similar to (Graves et al., 2014; Weston et al.,
2014; Sukhbaatar et al., 2015). The form of memory is defined as a matrix M. At each episode t0,
the Hyper-RNN emits two vectors mte0, and mta0 as:
tmte0,mta0 =t[σ,tanh](Wht0hth0)	(2)
where Wth is the weight matrix of HyPer-RNN to transform its internal state 邸 and t[∙] denotes
matrix transpose. Note that Wh are part of the Hyper-LSTM parameters θh .
The HyPer-RNN writes the abstract curriculum into the memory, and the DRL agent can read the
abstract curriculum information freely.
Reading. The DRL agent can read the abstract curriculum cabs from the memory M. The read
oPeration is defined as: cta0bs = αt0Mt0-1, where αt0 ∈ RK rePresents an attention distribution over
the set of entries of memory Mt0-1. Each scalar element αt0,k in an attention distribution αt0 can
be calculated as: αt0,k = Softmax(Cosine(Mt0-1,k, m，-1)), where We choose Cosine(∙, ∙) as
the align function, Mt0-1,k rePresents the k-th row memory vector, and mta0 ∈ RM is a add vector
emitted by HyPer-RNN.
Updating. The HyPer-RNN can write and uPdate abstract curriculum in the memory module. The
write oPeration is Performed as: Mt0 = Mt0-1(1 -αt0mte0) +αt0mta0, where mte0 ∈ RM corresPonds
to the extent to which the current contents in the memory should be deleted.
EquiPPed with the above memory mechanism, the DRL learning algorithm can read the memory
and utilize the retrieved information for the Policy learning. We incorPorate the abstract curriculum
into the value function by Q(st, at, γt, cta0bs ; φq). Similar to manually designed currricula, we
minimize the Bellman error and define the loss function for the abstract curriculum as: Jabstract =
E(St,at,rt,st+ι蜀s)^Hbuf [(Q(st, at, Cabs； φq) - y)2], where @ is defined in Eqn. 1.
3.3	Bilevel Training of Hyper-RNN with Hyper-gradients
After introducing the manually designed curricula in Sec. 3.1 and the abstract curriculum in Sec. 3.2,
here we describe how we uPdate the HyPer-RNN’s Parameters θh, the Parameters associated with
the DRL agent φq and φπ. Since the HyPer-RNN’s objective is to serve the DRL agent, we naturally
formulate this task as a bilevel Problem (Grefenstette et al., 2019) of oPtimizing the Parameters
associated with multi-objective curricula generation by nesting one inner-level looP in an outer-level
training looP.
Outer-level training of Hyper-RNN. SPecifically, the inner-level looP for the DRL agent learning
and the outer-level looP for training HyPer-RNN with hyPer-gradients. The outer-level loss is defined
as :Jouter = Jinitial + Jgoal + Jreward + Jabs .
Since the manually designed curricula and abstract curricula are all defined in
terms of Q-function, for the imPlementation simPlicity, we combine... them together
Jouter = Est ,at ,st+1 ,at+1 〜Hbuf [(Q(S ，a ，Cgoal, CreW , cinit, cabs ； φq ) - y ) ]∙ Follow-
ing the formulation and imPlementation in (Grefenstette et al., 2019), we obtain
θh = argmin ®； Jouter (argmin (φ; Jinner(θh, φ)))).
Inner-level training of DRL agent. The Parameters associated with the inner-level training, φq and
φπ , can be uPdated based on any RL algorithm. In this PaPer, we use Proximal Policy OPtimization
(PPO) (Schulman et al., 2017) which is a PoPular Policy gradient algorithm that learns a stochastic
Policy.
4To some extent, tasking the hyPer-net to train manually designed curriculum modules can be seen as an
curriculum itself for training the abstract curriculum memory module.
5
Under review as a conference paper at ICLR 2022
4 Experiments
We evaluate and analyze our proposed MOC DRL on the CausalWorld (Ahmed et al., 2020), as
this environment enables us to easily design and test different types of curricula in a fine-grained
manner. It should be noted that we do not utilize any causal elements of the environment. It is
straightforward to apply our method to other DRL environments without major modification. We
start by demonstrating how multiple curricula can benefit from Hyper-RNN. Then we evaluate the
effectiveness of the memory component as well as the abstract curriculum, and conduct an ablation
study over different components of MOC DRL. Finally, we plot the state visitation density graph to
analyze how each component can affect the agent’s behavior. The results shown in this section are
obtained during the training phase. Specifically, we evaluate the trained policy performance every
10000 steps with fixed curricula. This is in line with the evaluation procedures used in 5. Moreover,
the training and evaluation task distributions are handled by CausalWorld. Take task "Pushing" as an
example: for each outer loop, we use CausalWorld to generate a task with randomly sampled new
goal shapes from a goal shape family.
4.1	Settings
We choose five out of the nine tasks introduced in CausalWorld since the other four tasks have limited
support for configuring the initial and goal states. Specifically, we enumerate these five tasks here: (1)
Reaching requires moving a robotic arm to a goal position and reach a goal block; (2) Pushing
requires pushing one block towards a goal position with a specific orientation (restricted to goals on
the floor level); (3) Picking requires picking one block at a goal height above the center of the
arena (restricted to goals above the floor level); (4) Pick And Place is an arena is divided by a
fixed long block and the goal is to pick one block from one side of the arena to a goal position with a
variable orientation on the other side of the fixed block; (5) Stacking requires stacking two blocks
above each other in a specific goal position and orientation.
The total number of training steps is 10 million steps. Similar to (Clavera et al., 2018; Nagabandi
et al., 2018), we unroll the inner loop for one step to compute the approximate hyper-gradients to
update the Hyper-RNN.
Figure 2: Comparisons with state-of-the-art ACL algorithms. Each learning curve is computed in
three runs with different random seeds.
4.2	Comparing MOC with state-of-the-art ACL methods
We compare our proposed approach with the other state-of-the-art ACL methods: (1) GoalGAN (Flo-
rensa et al., 2018), which uses a generative adversarial neural network (GAN) to propose tasks for
5https://stable-baselines3.readthedocs.io/en/master/
6
Under review as a conference paper at ICLR 2022
the agent to finish; (2) ALP-GMM (Portelas et al., 2020a), which models the agent absolute learning
progress with Gaussian mixture models. None of these baselines utilize multiple curricula.
Fig. 2 shows that MOC outperforms other ACL approaches in terms of mean episode reward, ractional
success, and sample efficiency. Especially, MOC increases fractional success by up to 56.2% in all
of three tasks, which illustrates the effectiveness of combining multiple curricula in a synergistic
manner.
4.3	Ablation Study
Our proposed MOC framework consists of three key parts: the Hyper-RNN trained with hyper-
gradients, multi-objective curriculum modules, and the abstract memory module. To get a better
insight into MOC, we conduct an in-depth ablation study on probing these components.
We first describe the MOC variants used in this section for comparison as follows: (1) MOCBase- :
MOC has the Hyper-RNN and the memory module but does not have the Base-RNNs for manually
designed curricula. (2) MOCMemory- : MOC has the Hyper-RNN to generate three curriculum
modules but does not have the memory module. (3) MOCMemory-,Hyper- : MOC has Base-RNNs
but does not have memory and Hyper-RNN components. It independently generates manually
designed curricula. (4) MOCMemory- ,Goal+ : MOC with Hyper-RNN and one Base-RNN, but
without the memory module. It only generates the subgoal curriculum as our pilot experiments show
that it is consistently better than the other two manually designed curricula and is easier to analyze its
behavior by visualizing the state visitation.
Ablations of Hyper-RNN. Here we investigate the importance of the Hyper-RNN for the MOC
framework. We keep those three manually designed curriculum modules but remove the Hyper-RNN.
Without the Hyper-RNN, these Base-RNNs’ parameters are directly updated by loss gradients and
learn to generate curricula independently. Note that neither of them have the abstract memory
module. By comparing MOCMemory- with MOCM emory-,Hyper- as shown in Fig. 3, we can
observe that letting a Hyper-RNN generate the parameters of different curriculum modules indeed
helps in improving the sample efficiency and final performance. The advantage is even more obvious
in the harder tasks pick and place and stacking.
The poor performance of MOCMemory-,Hyper- may be caused by the potential conflicts among the
designed curricula. For example, without coordination between the initial state curriculum and the
goal curriculum, the initial state generator may set an initial state close to the goal state, which is easy
to achieve by an agent but too trivial to provide useful training information to achieve the final goal.
In sharp contrast, the Hyper-RNN can solve the potential conflicts from different curricula. All the
curriculum modules are dynamically generated by the same hyper-net, and there exists an implicit
information sharing between the initial state and the goal state curriculum generator. We put extra
experimental results in Appendix Sec. A.2, Fig. 4.
Ablations of the memory module. We aim to provide an empirical justification for the use of the
memory module and its associated abstract curriculum. By comparing MOC with MOCM emory-
as shown in Fig. 3, we can see that the memory module is crucial for MOC to improve sample
efficiency and final performance. Noticeably, in pick and place and stacking, we see that
MOC gains a significant improvement due to the incorporation of the abstract curriculum. We
expect that the abstract curriculum could provide the agent with an extra implicit curriculum that is
complementary to the manually designed curricula. We also find that it is better for the Hyper-RNN to
learn the abstract curriculum while generating other manually designed curricula. Learning multiple
manually designed curricula provides a natural curriculum for the Hyper-RNN itself since learning
easier curriculum modules can be beneficial for learning of more difficult curriculum modules with
parameters generated by the Hyper-RNN.
Ablations of individual curricula. We now investigate how gradually adding more curricula affects
the training of DRL agent. By comparing MOCM emory- ,Goal+ and MOCMemory- as shown in
Fig. 3, we observe that training an agent with a single curriculum receives less environmental rewards
as compared to the ones based on multiple curricula. This suggests that the set of generated curricula
indeed helps the agent to reach intermediate states that are aligned with each other and also guides
the agent to the final goal state.
7
Under review as a conference paper at ICLR 2022
Pick and Place
Picking
Stacking
P-PMaI ①PoS_d-UP①W
一MOCMemory-
MOCMemory-, Hyper- ----- MOC
VaniIIaDRL
MOCMemory-, Goa∕+	MOCβase-
Figure 3: Comparison of algorithms with and without memory component on all four tasks. Each
learning curve is obtained by three independent runs with different random seeds.
In summary, we have the following key observations: (1) The proposed three key components, i.ei.,
Hyper-RNN, multi-objectrive curriculum modules, and memory module, can improve the policy
performance and sample efficiency. (2) Without the centralized control from the Hyper-RNN, multiple
independent curricula sometimes are not as useful as one curriculum alone. In stacking, we can
see that one-curriculum can slightly outperform multiple curricula, which may be caused by the
conflicts among independent curricula. (3) Curriculum learning is a powerful idea to help agent
achieve a better performance. Other experiments with similar results are shown in the supplementary.
4.4	Curricula Analysis and Visualization
In this section, we analyze the initial state curriculum and goal state curriculum. First, we replace the
initial state curriculum with two different alternatives: (1) MOCRandInitState, in which we replace
the initial state curriculum in MOC with a uniformly chosen state. Other MOC components remains
the same; (2) MOCFixInitState, in which we replace the initial state curriculum in MOC with a fixed
initial state. The other MOC components remains the same. (3) M OCRandGoalState, in which we
replace the goal state curriculum in MOC with a uniformly chosen state. The other MOC components
remains the same. The evaluations are conducted on the reaching task and the results are shown
in Table 1a. From this table, we observe that MOC with initial state curriculum outperforms other
two baseline schemes in terms of mean episode rewards and success ratio. This demonstrates the
effectiveness of providing initial state curriculum. Besides, since “random sampling” outperforms
“fixed initial state”, we conjecture that it is better to provide different initial states, which might be
beneficial for exploration.
	Mean Episode Reward	Success Ratio		Mean Episode Reward	Success Ratio
M OCRandInitState	936.9 (±35)	91% (±0.5%)			
			GoalGAN	-609 (±23)-	56% (±18%)
MOCFixInitState	-879.3 (±9)-	89% (±1.1%)			
			ALP-GMM	568 (±26)一	39% (±28%)
M OCRandGoalState	921.0(±46厂	91% (±0.5%)			
MOC (Initial State)	1273 (±11)	100% (±0%)	MOC (Goal State)	714 (±14)	68% (±15%)
(a) Analysis of initial state curriculum			(b) Analysis of subgoal curriculum		
Table 1: Analysis of initial state curriculum and subgoal state curriculum.
In Sec. 4.2, we show that providing multi-objective curricula can improve the training of DRL
agents. To further evaluate the advantages of hyper-RNN base-RNN framework, we conduct an
experiment with GoalGAN, ALP-GMM and MOC with goal curriculum only. We evaluate on
reaching task and the results are shown in Tab. 1b. In this table, we see that MOC Goal State
8
Under review as a conference paper at ICLR 2022
(MOCMemory-,Goal+), which is MOC has goal curriculum but doesn’t have memory component,
slightly outperform other two baseline schemes.
5	Related Work
Curriculum learning. Automatic curriculum learning (ACL) for deep reinforcement learning
(DRL) (Narvekar et al., 2017; Portelas et al., 2020a; Svetlik et al., 2017; Narvekar & Stone, 2018;
Campero et al., 2020; Narvekar et al., 2020; Fang et al., 2021b; Czarnecki et al., 2018; Peng et al.,
2018) has recently emerged as a promising tool to learn how to adapt an agent’s learning tasks based
on its capacity during training. ACL (Graves et al., 2017) can be applied to DRL in a variety of
ways, including adapting initial states (Florensa et al., 2017b; Salimans & Chen, 2018; Ivanovic et al.,
2019), shaping reward functions (Bellemare et al., 2016; Pathak et al., 2017; Shyam et al., 2019), or
generating goals (Lair et al., 2019; Sukhbaatar et al., 2017; Florensa et al., 2018; Long et al., 2020).
In a closely related work (Portelas et al., 2020b; Fang et al., 2021a; Narvekar et al., 2016), a series
of related environments of increasing difficulty have been created to form curricula. Unfortunately,
the curricula strongly rely on the capability to modify the environments fundamentally, which poses
practical difficulties for creating the tasks. In contrast, our approach only assumes a mild authority to
modify the environments.
Multi-task and neural modules. Learning with multiple objectives are shown to be beneficial in
DRL tasks (Wilson et al., 2007; Pinto & Gupta, 2017; Riedmiller et al., 2018; Hausman et al., 2018).
Sharing parameters across tasks (Parisotto et al., 2015; Rusu et al., 2015; Teh et al., 2017) usually
results in conflicting gradients from different tasks. One way to mitigate this is to explicitly model
the similarity between gradients obtained from different tasks (Yu et al., 2020; Zhang & Yeung, 2014;
Chen et al., 2018; Kendall et al., 2018; Lin et al., 2019; Sener & Koltun, 2018; Du et al., 2018). On
the other hand, researchers propose to utilize different modules for different tasks, thus reducing
the interference of gradients from different tasks (Singh, 1992; Andreas et al., 2017; Rusu et al.,
2016; Qureshi et al., 2019; Peng et al., 2019; Haarnoja et al., 2018; Sahni et al., 2017). Most of
these methods rely on pre-defined modules that make them not attractive in practice. One exception
is (Yang et al., 2020), which utilizes soft combinations of neural modules for multi-task robotics
manipulation. However, there is still redundancy in the modules in (Yang et al., 2020), and those
modules cannot be modified during inference. Instead, we use a hyper-net to dynamically update
complementary modules on the fly conditioned on the environments.
Memory-augmented meta DRL. Our approach is also related to episodic memory-based meta DRL
(Lengyel & Dayan, 2007; Blundell et al., 2016; Vinyals et al., 2016; Pritzel et al., 2017). Different
from memory augmented meta DRL methods, the DRL agent in our case is not allowed to modify the
memory. Note that it is straightforward to augment the DRL agent with a both readable and writable
neural memory just like (Blundell et al., 2016; Lengyel & Dayan, 2007), which is different from our
read-only memory module designed for ACL.
Dynamic neural networks. Dynamic neural networks (Han et al., 2021) can change their structures
or parameters based on different environments. Dynamic filter networks (Jia et al., 2016) and
hyper-nets (Ha et al., 2017) can both generate parameters.
Our proposed framework borrows, extends, and unifies the aforementioned key concepts with a focus
on automatically learning multi-objective curricula from scratch for DRL.
6	Conclusion
This paper presents a multi-objective curricula learning approach for solving challenging deep
reinforcement learning tasks. Our method trains a hyper-network for parameterizing multiple
curriculum modules, which control the generation of initial states, subgoals, and shaping rewards.
We further design a flexible memory mechanism to learn abstract curricula. Extensive experimental
results demonstrate that our proposed approach significantly outperforms other state-of-the-art ACL
methods in terms of sample efficiency and final performance.
9
Under review as a conference paper at ICLR 2022
References
Ossama Ahmed, Frederik Trauble, AnirUdh Goyal, Alexander Neitz, ManUel Wuthrich, Yoshua
Bengio, Bernhard Scholkopf, and Stefan Bauer. Causalworld: A robotic manipulation benchmark
for causal structure and transfer learning. arXiv preprint arXiv:2010.04296, 2020.
Jacob Andreas, Marcus Rohrbach, Trevor Darrell, and Dan Klein. Neural module networks. In
Proceedings ofthe IEEE conference on computer vision and pattern recognition, pp. 39-48, 2016.
Jacob Andreas, Dan Klein, and Sergey Levine. Modular multitask reinforcement learning with policy
sketches. In International Conference on Machine Learning, pp. 166-175. PMLR, 2017.
Marc G Bellemare, Sriram Srinivasan, Georg Ostrovski, Tom Schaul, David Saxton, and Remi Munos.
Unifying count-based exploration and intrinsic motivation. arXiv preprint arXiv:1606.01868, 2016.
Yoshua Bengio, J6r6me Louradour, Ronan Collobert, and Jason Weston. Curriculum learning. In
Proceedings of the 26th annual international conference on machine learning, pp. 41-48, 2009.
Charles Blundell, Benigno Uria, Alexander Pritzel, Yazhe Li, Avraham Ruderman, Joel Z Leibo,
Jack Rae, Daan Wierstra, and Demis Hassabis. Model-free episodic control. arXiv preprint
arXiv:1606.04460, 2016.
Andres Campero, Roberta Raileanu, Heinrich Kuttler, Joshua B Tenenbaum, Tim Rocktaschel, and
Edward Grefenstette. Learning with amigo: Adversarially motivated intrinsic goals. arXiv preprint
arXiv:2006.12122, 2020.
Rich Caruana. Multitask learning. Machine learning, 28(1):41-75, 1997.
Oscar Chang, Lampros Flokas, and Hod Lipson. Principled weight initialization for hypernetworks.
In International Conference on Learning Representations, 2019.
Zhao Chen, Vijay Badrinarayanan, Chen-Yu Lee, and Andrew Rabinovich. Gradnorm: Gradient
normalization for adaptive loss balancing in deep multitask networks. In International Conference
on Machine Learning, pp. 794-803. PMLR, 2018.
Kyunghyun Cho, Bart Van Merrienboer, Caglar Gulcehre, Dzmitry Bahdanau, Fethi Bougares, Holger
Schwenk, and Yoshua Bengio. Learning phrase representations using rnn encoder-decoder for
statistical machine translation. arXiv preprint arXiv:1406.1078, 2014.
Ignasi Clavera, Jonas Rothfuss, John Schulman, Yasuhiro Fujita, Tamim Asfour, and Pieter Abbeel.
Model-based reinforcement learning via meta-policy optimization. In Conference on Robot
Learning, pp. 617-629. PMLR, 2018.
Wojciech Marian Czarnecki, Siddhant M. Jayakumar, Max Jaderberg, Leonard Hasenclever,
Yee Whye Teh, Nicolas Heess, Simon Osindero, and Razvan Pascanu. Mix & match agent
curricula for reinforcement learning. In ICML, volume 80 of Proceedings of Machine Learning
Research, pp. 1095-1103. PMLR, 2018.
Yunshu Du, Wojciech M Czarnecki, Siddhant M Jayakumar, Mehrdad Farajtabar, Razvan Pascanu,
and Balaji Lakshminarayanan. Adapting auxiliary losses using gradient similarity. arXiv preprint
arXiv:1812.02224, 2018.
Jeffrey L Elman. Learning and development in neural networks: The importance of starting small.
Cognition, 48(1):71-99, 1993.
Kuan Fang, Yuke Zhu, Silvio Savarese, and Li Fei-Fei. Adaptive procedural task generation for
hard-exploration problems. In ICLR. OpenReview.net, 2021a.
Kuan Fang, Yuke Zhu, Silvio Savarese, and Li Fei-Fei. Discovering generalizable skills via automated
generation of diverse tasks. In Robotics: Science and Systems, 2021b.
Carlos Florensa, David Held, Markus Wulfmeier, Michael Zhang, and Pieter Abbeel. Reverse
curriculum generation for reinforcement learning. In CoRL, volume 78 of Proceedings of Machine
Learning Research, pp. 482-495. PMLR, 2017a.
10
Under review as a conference paper at ICLR 2022
Carlos Florensa, David Held, Markus Wulfmeier, Michael Zhang, and Pieter Abbeel. Reverse
curriculum generation for reinforcement learning. arXiv preprint arXiv:1707.05300, 2017b.
Carlos Florensa, David Held, Xinyang Geng, and Pieter Abbeel. Automatic goal generation for
reinforcement learning agents. In International conference on machine learning, pp. 1515-1528,
2018.
Tomer Galanti and Lior Wolf. On the modularity of hypernetworks. arXiv preprint arXiv:2002.10006,
2020.
Alex Graves, Greg Wayne, and Ivo Danihelka. Neural turing machines. arXiv preprint
arXiv:1410.5401, 2014.
Alex Graves, Marc G Bellemare, Jacob Menick, Remi Munos, and Koray Kavukcuoglu. Automated
curriculum learning for neural networks. In international conference on machine learning, pp.
1311-1320. PMLR, 2017.
Edward Grefenstette, Brandon Amos, Denis Yarats, Phu Mon Htut, Artem Molchanov, Franziska
Meier, Douwe Kiela, Kyunghyun Cho, and Soumith Chintala. Generalized inner loop meta-learning.
arXiv preprint arXiv:1910.01727, 2019.
David Ha, Andrew M. Dai, and Quoc V. Le. Hypernetworks. In 5th International Confer-
ence on Learning Representations, ICLR 2017, Toulon, France, April 24-26, 2017, Conference
Track Proceedings. OpenReview.net, 2017. URL https://openreview.net/forum?id=
rkpACe1lx.
Tuomas Haarnoja, Vitchyr Pong, Aurick Zhou, Murtaza Dalal, Pieter Abbeel, and Sergey Levine.
Composable deep reinforcement learning for robotic manipulation. In 2018 IEEE International
Conference on Robotics and Automation (ICRA), pp. 6244-6251. IEEE, 2018.
Yizeng Han, Gao Huang, Shiji Song, Le Yang, Honghui Wang, and Yulin Wang. Dynamic neural
networks: A survey. arXiv preprint arXiv:2102.04906, 2021.
Karol Hausman, Jost Tobias Springenberg, Ziyu Wang, Nicolas Heess, and Martin Riedmiller.
Learning an embedding space for transferable robot skills. In International Conference on
Learning Representations, 2018.
Matteo Hessel, Joseph Modayil, Hado van Hasselt, Tom Schaul, Georg Ostrovski, Will Dabney,
Dan Horgan, Bilal Piot, Mohammad Gheshlaghi Azar, and David Silver. Rainbow: Combining
improvements in deep reinforcement learning. In AAAI, pp. 3215-3222. AAAI Press, 2018.
Sepp Hochreiter and Jurgen SChmidhuber. Long short-term memory. Neural computation, 9(8):
1735-1780, 1997.
Boris Ivanovic, James Harrison, Apoorva Sharma, Mo Chen, and Marco Pavone. Barc: Backward
reachability curriculum for robotic reinforcement learning. In 2019 International Conference on
Robotics and Automation (ICRA), pp. 15-21. IEEE, 2019.
Xu Jia, Bert De Brabandere, Tinne Tuytelaars, and Luc Van Gool. Dynamic filter networks. In NIPS,
2016.
Alex Kendall, Yarin Gal, and Roberto Cipolla. Multi-task learning using uncertainty to weigh losses
for scene geometry and semantics. In Proceedings of the IEEE conference on computer vision and
pattern recognition, pp. 7482-7491, 2018.
Kai A Krueger and Peter Dayan. Flexible shaping: How learning in small steps helps. Cognition,
110(3):380-394, 2009.
Nicolas Lair, CedriC Colas, Remy Portelas, Jean-Michel Dussoux, Peter Ford Dominey, and Pierre-
Yves Oudeyer. Language grounding through social interactions and curiosity-driven multi-goal
learning. arXiv preprint arXiv:1911.03219, 2019.
Mgte Lengyel and Peter Dayan. Hippocampal contributions to control: the third way. Advances in
neural information processing systems, 20:889-896, 2007.
11
Under review as a conference paper at ICLR 2022
Xingyu Lin, Harjatin Singh Baweja, George Kantor, and David Held. Adaptive auxiliary task
weighting for reinforcement learning. Advances in neural information processing systems, 32,
2019.
Qian Long, Zihan Zhou, Abhibav Gupta, Fei Fang, Yi Wu, and Xiaolong Wang. Evolutionary popula-
tion curriculum for scaling multi-agent reinforcement learning. arXiv preprint arXiv:2003.10423,
2020.
Tambet Matiisen, Avital Oliver, Taco Cohen, and John Schulman. Teacher-student curriculum
learning. IEEE transactions on neural networks and learning systems, 31(9):3732-3740, 2019.
Anusha Nagabandi, Ignasi Clavera, Simin Liu, Ronald S Fearing, Pieter Abbeel, Sergey Levine, and
Chelsea Finn. Learning to adapt in dynamic, real-world environments through meta-reinforcement
learning. arXiv preprint arXiv:1803.11347, 2018.
Sanmit Narvekar and Peter Stone. Learning curriculum policies for reinforcement learning. arXiv
preprint arXiv:1812.00285, 2018.
Sanmit Narvekar, Jivko Sinapov, Matteo Leonetti, and Peter Stone. Source task creation for curriculum
learning. In AAMAS, pp. 566-574. ACM, 2016.
Sanmit Narvekar, Jivko Sinapov, and Peter Stone. Autonomous task sequencing for customized
curriculum design in reinforcement learning. In IJCAI, pp. 2536-2542, 2017.
Sanmit Narvekar, Bei Peng, Matteo Leonetti, Jivko Sinapov, Matthew E. Taylor, and Peter Stone.
Curriculum learning for reinforcement learning domains: A framework and survey. J. Mach. Learn.
Res., 21:181:1-181:50, 2020.
Andrew Y. Ng, Daishi Harada, and Stuart J. Russell. Policy invariance under reward transformations:
Theory and application to reward shaping. In Ivan Bratko and Saso Dzeroski (eds.), Proceedings
of the Sixteenth International Conference on Machine Learning (ICML 1999), Bled, Slovenia, June
27-30, 1999, pp. 278-287. Morgan Kaufmann, 1999.
Emilio Parisotto, Jimmy Lei Ba, and Ruslan Salakhutdinov. Actor-mimic: Deep multitask and
transfer reinforcement learning. arXiv preprint arXiv:1511.06342, 2015.
Deepak Pathak, Pulkit Agrawal, Alexei A Efros, and Trevor Darrell. Curiosity-driven exploration
by self-supervised prediction. In International Conference on Machine Learning, pp. 2778-2787.
PMLR, 2017.
Bei Peng, James MacGlashan, Robert Tyler Loftin, Michael L. Littman, David L. Roberts, and
Matthew E. Taylor. Curriculum design for machine learners in sequential decision tasks. IEEE
Trans. Emerg. Top. Comput. Intell., 2(4):268-277, 2018.
Xue Bin Peng, Michael Chang, Grace Zhang, Pieter Abbeel, and Sergey Levine. Mcp: Learn-
ing composable hierarchical control with multiplicative compositional policies. arXiv preprint
arXiv:1905.09808, 2019.
Lerrel Pinto and Abhinav Gupta. Learning to push by grasping: Using multiple tasks for effective
learning. In 2017 IEEE international conference on robotics and automation (ICRA), pp. 2161-
2168. IEEE, 2017.
Remy Portelas, CedriC Colas, Lilian Weng, Katja Hofmann, and Pierre-Yves Oudeyer. Automatic
curriculum learning for deep rl: A short survey. arXiv preprint arXiv:2003.04664, 2020a.
Remy Portelas, Clement Romac, Katja Hofmann, and Pierre-Yves Oudeyer. Meta automatic curricu-
lum learning. arXiv preprint arXiv:2011.08463, 2020b.
Alexander Pritzel, Benigno Uria, Sriram Srinivasan, Adria Puigdomenech Badia, Oriol Vinyals,
Demis Hassabis, Daan Wierstra, and Charles Blundell. Neural episodic control. In International
Conference on Machine Learning, pp. 2827-2836. PMLR, 2017.
Ahmed H Qureshi, Jacob J Johnson, Yuzhe Qin, Taylor Henderson, Byron Boots, and Michael C
Yip. Composing task-agnostic policies with deep reinforcement learning. arXiv preprint
arXiv:1905.10681, 2019.
12
Under review as a conference paper at ICLR 2022
Antonin Raffin, Ashley Hill, Maximilian Ernestus, Adam Gleave, Anssi Kanervisto, and Noah
Dormann. Stable baselines3. https://github.com/DLR- RM/stable- baselines3,
2019.
Martin Riedmiller, Roland Hafner, Thomas Lampe, Michael Neunert, Jonas Degrave, Tom Wiele,
Vlad Mnih, Nicolas Heess, and Jost Tobias Springenberg. Learning by playing solving sparse
reward tasks from scratch. In International Conference on Machine Learning, pp. 4344-4353.
PMLR, 2018.
Sebastian Ruder. An overview of multi-task learning in deep neural networks. arXiv preprint
arXiv:1706.05098, 2017.
Andrei A Rusu, Sergio Gomez Colmenarejo, Caglar Gulcehre, Guillaume Desjardins, James Kirk-
patrick, Razvan Pascanu, Volodymyr Mnih, Koray Kavukcuoglu, and Raia Hadsell. Policy
distillation. arXiv preprint arXiv:1511.06295, 2015.
Andrei A Rusu, Neil C Rabinowitz, Guillaume Desjardins, Hubert Soyer, James Kirkpatrick, Koray
Kavukcuoglu, Razvan Pascanu, and Raia Hadsell. Progressive neural networks. arXiv preprint
arXiv:1606.04671, 2016.
Himanshu Sahni, Saurabh Kumar, Farhan Tejani, and Charles Isbell. Learning to compose skills.
arXiv preprint arXiv:1711.11289, 2017.
Tim Salimans and Richard Chen. Learning montezuma’s revenge from a single demonstration. arXiv
preprint arXiv:1812.03381, 2018.
Tom Schaul, Daniel Horgan, Karol Gregor, and David Silver. Universal value function approximators.
In Francis R. Bach and David M. Blei (eds.), Proceedings of the 32nd International Conference
on Machine Learning, ICML 2015, Lille, France, 6-11 July 2015, volume 37 of JMLR Workshop
and Conference Proceedings, pp. 1312-1320. JMLR.org, 2015. URL http://proceedings.
mlr.press/v37/schaul15.html.
Jurgen SChmidhuber. Curious model-building control systems. In Proc. international jointconference
on neural networks, pp. 1458-1463, 1991.
Jurgen Schmidhuber. Learning to control fast-weight memories: An alternative to dynamic recurrent
networks. Neural Computation, 4(1):131-139, 1992.
John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy
optimization algorithms. CoRR, abs/1707.06347, 2017. URL http://arxiv.org/abs/
1707.06347.
Oliver G Selfridge, Richard S Sutton, and Andrew G Barto. Training and tracking in robotics. In
Ijcai, pp. 670-672, 1985.
Ozan Sener and Vladlen Koltun. Multi-task learning as multi-objective optimization. arXiv preprint
arXiv:1810.04650, 2018.
Pranav Shyam, Wojciech ja´kowski, and Faustino Gomez. Model-based active exploration. In
International Conference on Machine Learning, pp. 5779-5788. PMLR, 2019.
Satinder Pal Singh. Transfer of learning by composing solutions of elemental sequential tasks.
Machine Learning, 8(3):323-339, 1992.
Sainbayar Sukhbaatar, Arthur Szlam, Jason Weston, and Rob Fergus. End-to-end memory networks.
arXiv preprint arXiv:1503.08895, 2015.
Sainbayar Sukhbaatar, Zeming Lin, Ilya Kostrikov, Gabriel Synnaeve, Arthur Szlam, and Rob
Fergus. Intrinsic motivation and automatic curricula via asymmetric self-play. arXiv preprint
arXiv:1703.05407, 2017.
Richard S Sutton and Andrew G Barto. Reinforcement learning: An introduction. MIT press, 2018.
13
Under review as a conference paper at ICLR 2022
Maxwell Svetlik, Matteo Leonetti, Jivko Sinapov, Rishi Shah, Nick Walker, and Peter Stone. Auto-
matic curriculum graph generation for reinforcement learning agents. In AAAI, pp. 2590-2596.
AAAI Press, 2017.
Yee Whye Teh, Victor Bapst, Wojciech Marian Czarnecki, John Quan, James Kirkpatrick, Raia
Hadsell, Nicolas Heess, and Razvan Pascanu. Distral: Robust multitask reinforcement learning.
arXiv preprint arXiv:1707.04175, 2017.
Oriol Vinyals, Charles Blundell, Timothy Lillicrap, Koray Kavukcuoglu, and Daan Wierstra. Match-
ing networks for one shot learning. arXiv preprint arXiv:1606.04080, 2016.
Tonghan Wang, Heng Dong, Victor Lesser, and Chongjie Zhang. Multi-agent reinforcement learning
with emergent roles. arXiv preprint arXiv:2003.08039, 2020.
Jason Weston, Sumit Chopra, and Antoine Bordes. Memory networks. arXiv preprint
arXiv:1410.3916, 2014.
Aaron Wilson, Alan Fern, Soumya Ray, and Prasad Tadepalli. Multi-task reinforcement learning: a
hierarchical bayesian approach. In Proceedings of the 24th international conference on Machine
learning, pp. 1015-1022, 2007.
Ruihan Yang, Huazhe Xu, Yi Wu, and Xiaolong Wang. Multi-task reinforcement learning with soft
modularization. arXiv preprint arXiv:2003.13661, 2020.
Tianhe Yu, Saurabh Kumar, Abhishek Gupta, Sergey Levine, Karol Hausman, and Chelsea Finn.
Gradient surgery for multi-task learning. arXiv preprint arXiv:2001.06782, 2020.
Yu Zhang and Dit-Yan Yeung. A regularization approach to learning task relationships in multitask
learning. ACM Transactions on Knowledge Discovery from Data (TKDD), 8(3):1-31, 2014.
A	Appendix
A.1 Environment Settings
CausalWorld allows us to easily modify the initial states and goal states. In general, the initial state is
the cylindrical position and Euler orientation of the block and goal state is the position variables of the
goal block. These two control variables are both three dimensional vectors with a fixed manipulation
range. To match the range of each vector, we re-scale the generated initial states.
The reward function defined in CausalWorld is uniformly across all possible goal shapes as the
fractional volumetric overlap of the blocks with the goal shape, which ranges between 0 (no overlap)
and 1 (complete overlap). We also re-scale the shaping reward to match this range.
We choose the PPO algorithm as our vanilla DRL policy learning method. We list the important
hyper-parameters in Table. 2. We also provide the complete code in the supplementary material.
Table 2: Hyper-parameter values for PPO training
Parameter	Value
Discount factor (Y)	0.9995
n_steps	5000-
Entropy coefficiency	0
Learning rate	0.00025
Maximum gradient norm	-10
Value coefficiency	-0.5-
Experience buffer size	-1e6-
Minibatch size	-128-
clip parameter (e)	-0.3-
Activation function	ReLU
Optimizer	Adam
14
Under review as a conference paper at ICLR 2022
Pushing
Episodes
le7
--MOCMemory-	----- MOC	--- ^OCMemory-, Goal+
--MOCMemory-ι Hyper-	VaniIIaDRL	MOCease-
Figure 4: Comparison of algorithms with and without memory component in pushing. Each learning
curve is computed in three runs with different random seeds.
A.2 Additional Experimental Results
This section serves as a supplementary results for Sec. 4.
Fig. 4 shows the results of with and without Hyper-RNN in pushing tasks. The results validate the
effectiveness of using Hyper-RNN. It is clear that, the incorporation of memory module consistently
helps the DRL agent outperform other strong baselines in all scenarios. More importantly, in
pushing task, we can observe a 5-fold improvement compared to the method with only the Hyper-
RNN component.
Fig. 4 clearly validate the effectiveness of our proposed method in achieving both the best final
performance and improving sample efficiency.
A.3 Additional Visualizations of States
Figs. 5, 6, 7, 8 visualize the state visitation density in task reaching, picking, pushing and
pick and place, respectively.
From these results, we summarize the following observations: (1) The proposed architecture can
help the agent explore different state spaces, which can be seen in the top row and bottom row. (2)
The ablation study with three independent curricula often leads to exploring three different state
space, as shown in Fig. 6 and Fig. 7. (3) By adding a memory component, the proposed MOC DRL
can effectively utilize all curricula and help the agent focus on one specific state space. This is the
reason why the proposed MOC DRL outperforms the other baselines in all tasks. (4) Comparing
with Hyper-RNN ("no-mem") and without Hyper-RNN ("independent"), we can see that one of the
benefits of using Hyper-RNN is aggregating different curricula. These can also be found in Fig. 6
and Fig. 7.
A.4 Additional Experiment Results
15
Under review as a conference paper at ICLR 2022
(a) no-mem (early)
(b) mem-only (early) (c) independent (early) (d) with-mem (early)
(e) no-mem (late) (f) mem-only (late) (g) independent (late) (h) with-mem (late)
Figure 5: Visualizations of state visitation density in early and late stages in reaching
(b) mem-only (early) (c) independent (early) (d) with-mem (early)
(a) no-mem (early)
5
Q
-5
-5	0	5
5
Q
-5
-5	0	5
-5	0	5
(e) no-mem (late) (f) mem-only (late) (g) independent (late) (h) with-mem (late)
Figure 6:	Visualizations of state visitation density in early and late stages in picking
In Sec. 4.2, we compared MOC with state-of-the art ACL algorithms. Here, we add two more
baselines algorithms. The results are shown in Fig. 12:
•	InitailGAN (Florensa et al., 2017a): which generates adapting initial states for the agent to
start with.
•	P P OReward+: which is a DRL agent trained with PPO algorithm and reward shaping. The
shaping function is instantiated as a deep neural network.
A.5 PPO Modifications
In Sec. 3, we propose a MOC-DRL framework for actor-critic algorithms. Since we adopt PPO in
this paper, we now describe how we modify the PPO to cope with the learned curricula. We aim to
maximize the PPO-clip objective:
where
θk+1
argmaxθEs,a^∏θfc [min(
∏θ (a|s)
πθk (a|s)
Aπθk (s, a), g (, Aπθk (s, a))],
(3)
g(, A)
(1+)A
(1-)A
A≥0
A< 0,
where θ is the parameter of policy π, θk is the updated k step parameter by taking the objective above,
A is the advantage function that we define as:
A(s, a) = Q(s, a) - V(s)
For the Hyper-RNN training, we modify the Q function as Q(s, a, cgoal, crew, cini, cabs).
16
Under review as a conference paper at ICLR 2022
(b) mem-only (early) (c) independent (early) (d) with-mem (early)
(a) no-mem (early)
-5	0	5
0	5
(e) no-mem (late) (f) mem-only (late) (g) independent (late) (h) with-mem (late)
Figure 7:	Visualizations of state visitation density in early and late stages in pushing
5
Q
-5
5.0
2.5
0.0
-2.5
(b) mem-only (early) (c) independent (early) (d) with-mem (early)
(a) no-mem (early)
(e) no-mem (late)
5.0
2.5
0.0
-2.5
-5.0
(f) mem-only (late) (g) independent (late)
(h) with-mem (late)
Figure 8:	Visualizations of state visitation density in early and late stages in pick and place
A.6 B ilevel Training
Here we provide more details regarding the bilevel training of Hyper-RNN introduced in Sec. 3.3.
The optimal parameters θ( are obtained by minimizing the loss function Jouter. The key steps can
be summarized as:
Step 1 Update PPO agent parameters θ on one sampled task by Eqn. 3
Step 2 With updated parameters θ, we train model parameters θh via SGD by minimizing the outer
loss function θh = argmineh Jouter.
Step 3 With θh, we generate manually designed curricula and abstract curriculum.
Step 4 We give the generate curriculum to the Q function and environment hyper-parameters.
Step 5 We go back to Step 1 for agent training until converge.
A.7 Hyper-net
(Ha et al., 2017) introduce to generate parameters of Recurrent Networks using another neural
networks. This approach is to put a small RNN cell (called the Hyper-RNN cell) inside a large
RNN cell (the main RNN). The Hyper-RNN cell will have its own hidden units and its own input
sequence. The input sequence for the Hyper-RNN cell will be constructed from 2 sources: the
previous hidden states of the main LSTM concatenated with the actual input sequence of the main
LSTM. The outputs of the Hyper-RNN cell will be the embedding vector Z that will then be used to
generate the weight matrix for the main LSTM. Unlike generating weights for convolutional neural
networks, the weight-generating embedding vectors are not kept constant, but will be dynamically
generated by the HyperLSTM cell. This allows the model to generate a new set of weights at each
17
Under review as a conference paper at ICLR 2022
Reaching
800
3pos-d3 Css
Picking
0.2 0.4 0.6 0.8 1.0
Episodes le7
0.2 0.4 0.6 0.8 1.0
Episodes le7
-14
-16
-18
-20
-22
-24
0.2 0.4 0.6 0.8 1.0	0.2 0.4 0.6 0.8 1.0	0.2 0.4 0.6 0.8 1.0
Episodes le7	Episodes le7	Episodes le7
MOCβase- -------- Dlrect-abstτact-currlculum
p-eMΘJ ①Pos-də ueφw
Figure 9: Comparison between read memory from memory and direct generate abstract curriculum
Figure 10: Comparison with ACL algorithms. Each learning curve is computed in three runs with
different random seeds.
time step and for each input example. The standard formulation of a basic RNN is defined as:
ht = φ(Wh ht-1 + Wxxt + b),
where ht is the hidden state, φ is a non-linear operation such as tanh or relu, and the weight matrics
and bias Wh ∈ RNh ×Nh , Wx ∈ RNh ×Nx , b ∈ RNh is fixed each timestep for an input sequence
X = (x1, . . . , xT). More conceretly, the parameters Wh, Wx, b of the main RNN are different at
different time steps, so that ht can now be computed as:
ht = φ(Wh(zh)ht-1 + Wx(zx) + b(zb)), where
Wh(zh) =< Whz, zh >
Wx (zx) =< Wxx, zx >
b(zb) = Wbz zb + b0
(4)
where Whz ∈ RNh×Nh×Nz,Wxz ∈ RNh×Nx×Nz,Wbz ∈ RNh×Nz,bo ∈ RNh andzh,zx,zz ∈ RNz.
Moreover, zh, zx and zb can be computed as a function of xt and ht-1 :
18
Under review as a conference paper at ICLR 2022
Reaching
Pushing
P1ck_and_Place
1200
20
10
PlOOO
s 800
8
& 600
10
400
200,
0.2
0.4 0.6
Episodes
0.8	1.0	0.2	0.4 0.6 0.8 1.0
le7	Episodes le7
0.2	0.4	0.6
Episodes
0.8	1.0
le7
0.2 0.4
Episodes
0.6 0.8
i.?20
Ie7
0.2 0.4 0.6 0.8 1.0
Episodes le7
0
5
0
----MOC -------- MOCm8mwy-,Go⅛÷ M0Cg3se- ------------- M0C*⅛mwy-, Rβwa>tft
Reaching	Picking
1250
-5	Z
1000 ʃʃ	/ʌ
Pk k_and_PlaCe
Stacking
Pushing
Figure 11: Comparison with reward curriculum only.
—MOC	PPoReWal∏*	— InitIalGAN
Figure 12: Comparison with Initial GAN and PPO with reward shaping only.
Xt = (hx-1)
ht = φ(Whht-1 + Wxxt + b)
TXT- 1	. f
Zh = Whhht-I + bhh
zx = IW^ ht-1 + b^
x hx t-1	hx
TXT- 1
Zb = W^ Jtf-1
b hb t-1
(5)
Where Wh ∈ RN^×Nh,Wx ∈ RN^×(Nh+Nz),b ∈ Rn^, and Whh,Whx,Whb ∈ RNz×n^ and
bhh，b^χ ∈ RNz. The Hyper-RNN cell has Nh hidden units.
A.8 The abstract curriculum training
For some difficult tasks, we find that it is difficult to train a policy with small variances if the
Hyper-RNN is initialized with random parameters6.
As a simple workaround, we propose to pre-train the Hyper-RNN and memory components in a
slightly unrelated task. In particular, when solving task Tx, we pre-train the abstract memory module
on tasks other than Tx. The details can be found in our source code.
6The weight initialization approach (Chang et al., 2019) designed for hyper-net does not help too much in
our case.
19
Under review as a conference paper at ICLR 2022
(a) 1000k steps (b) 3000k steps (c) 5000k steps (d) 7000k steps (e) 9000k steps (f) Goal state
Figure 13: Visualization of generated subgoals
A.9 The visualization of generated sub-goal
The visualization of generated sub-goal state is shown in Fig. 13. Specifically, the arm is tasked
to manipulate the red cube to the position shown as a green cube. As we can see, MOC generates
subgoals that gradually change from "easy" (which are close to the initial state) to "hard" (which are
close to the goal state). The generated subgoals have different configurations (e.g., the green cube is
headed north-west in 7000k steps but is headed north-east in 9000k steps ), which requires the agent
to learn to delicately manipulate robot arm.
A.10 Hyperparameters
In this section, we extensively evaluate the influence of different hyperparameters for the baselines
and MOC, where the search is done with random search. We choose the reaching and stacking tasks,
which are shown in Fig. 14, 15, 16. For example, in Fig. 14-(a), the first column represents the
different values for outer iterations. A particular horizontal line, e.g., {4, 512, 5, 0.5}, indicates a
particular set of hyperparameters for one experiment. Besides, during the training phase, we adopt
hyperparameters of PPO from (Raffin et al., 2019) and search two hyperparameters to test the MOC
sensitivity.
We can observe that: (1) It is clear that MOC outperforms all the baselines with extensive hyperpa-
rameter search. (2) MOC is not sensitive to different hyperparameters.
20
Under review as a conference paper at ICLR 2022
5
7(
50(
0.55
450
4.6
0.5
400
4.4
640
0.45
.2
350
4
0.4
)0
；.8
250
580
3.6
0.3
3.4
!00
3.2
54(
1
3
0.2
0.35
0.25
4.8
4.6
4.4
4.2
3.6
3.4
3.2
3
GAN Noise Size
5
GAN Noise Level Mean Episode Reward
Mean Episode Reward
680
640
620
600
580
#OUter Iters
5
5
#Hidden Neurons
512
4
3.8
3
128
3
0.6
704
620
600
0.2
526
^^700
660
1560
540
(a) Hyperparameter tuning in reaching task.
5
-11.
50(
.4
.55
450
4.6
0.5
400
4.4
-1
..8
0.45
.2
350
4
0.4
)0
22
3.8
-1
3.6
0.
-12.6
3.4
!00
0.
!5
3.2
-12.
1
3
0.2
4.8
4.6
4.4
4.2
4
3.8
3.6
3.4
3.2
3
-12.956
#OUter IterS
5
5
#Hidden Neurons
512
GAN Noise Size
5
GAN Noise Level Mean Episode Reward
-11.4
-11.8
Z 12
-12
-12
-12.2
-12.4
3
128
3
0.2
0.6
-11.187
-11.6
-11.6
Mean Episode Reward
-11.2
1-12.6
-12.8

(b) Hyperparameter tuning in stacking task.
Figure 14: Hyperparameter tuning results for GoalGAN
21
Under review as a conference paper at ICLR 2022
0.01
3000
)0
2600
0.008
2400
!200
0.006
2000
1800
0.004
1600
.400
1(
0.002
2800
1200
2.8M
2.6M
2.4M
2.2M
2M
1.8M
1.6M
1.4M
1.2M
Buffer Size
3M
3M
1M
1M
0.0001
167.54
Batch Size
3000
1000
Mean Episode Reward
600
500
500
400
300
200
Learning Rate
0.01
Mean Episode Reward
699.02
400
300
200
(a) Hyperparameter tuning in reaching task.
0.01
3000
-9.6
2600
0.008
.0
2400
2200
0.006
2000
-10.4
1800
0.004
1600
.400
.0.8
1(
0.002
2800
1200
-10.6
2.8M
2.6M
2.4M
2.2M
2M
1.8M
1.6M
1.4M
1.2M
Buffer Size
3M
3M
1M
1M
-9.8
Learning Rate
0.01
-9.6
0.0001
1000
-11.08
Batch Size
3000
Mean Episode Reward
-9.54
Mean Episode Reward
-10
-10.2
-10.2
-11
-10.4
-10.6
-10.8
-11
(b) Hyperparameter tuning in stacking task.
Figure 15: Hyperparameter tuning results for ALP-GMM
22
Under review as a conference paper at ICLR 2022
Discount Factor
0.9999
0.9999
0.9998
0.9997
0.9996
0.9995
0.9994
0.9993
Discount Factor
0.9999
0.9999
0.9998
0.9997
0.9996
0.9995
0.9994
0.9993
0.9992
(a) Hyperparameter tuning in reaching task.
0.9992
(b) Hyperparameter tuning in stacking task.
Figure 16: Hyperparameter tuning results for MOC
23