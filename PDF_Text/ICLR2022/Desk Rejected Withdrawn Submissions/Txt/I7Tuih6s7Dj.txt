Under review as a conference paper at ICLR 2022
Towards Axiomatic, Hierarchical, and Sym-
bolic Explanation for Deep Models
Anonymous authors
Paper under double-blind review
Ab stract
This paper proposes a hierarchical and symbolic And-Or graph (AOG) to objec-
tively explain the internal logic encoded by a well-trained deep model for infer-
ence. We first define the objectiveness of an explainer model in game theory, and
we develop a rigorous representation of the And-Or logic encoded by the deep
model. The objectiveness and trustworthiness of the AOG explainer are both theo-
retically guaranteed and experimentally verified. Furthermore, we propose several
techniques to boost the conciseness of the explanation.
1 Introduction
Deep models have shown promise in various tasks in recent years, but the black-box nature of
the deep model makes it difficult for people to understand its internal behavior. Many post-hoc
explanation methods have been proposed either to explain the deep model visually (Yosinski et al.,
2015; Simonyan et al., 2013; Zeiler & Fergus, 2014), or to estimate the attribution/importance of
input variables (Ribeiro et al., 2016; Lundberg & Lee, 2017; Selvaraju et al., 2017).
Beyond the above post-hoc explanations at the pixel-wise and unit-wise levels, in this study, we aim
to explain the feature representation of the deep model using a hierarchical and symbolic graph,
which can be better aligned with human cognition. To this end, previous studies have proposed
some hierarchical and symbolic explainer models for deep models, for example, using the decision
tree (Frosst & Hinton, 2017; Wu et al., 2018), the explanatory graph (Zhang et al., 2018), and the
additive model (Vaughan et al., 2018; Tan et al., 2018) to explain deep models.
Unfortunately, most of the above hierarchical and symbolic explainer models cannot guarantee
that the explanation/explainer model objectively reflects the internal logic of the deep model, and
lack solid theoretical foundations for the trustworthiness of explanations. For example, some ex-
plainer models are learned to mimic the output of the pre-trained deep model via knowledge dis-
tillation (Hinton et al., 2015). However, the distilled explainer model may just fit the output of
the pre-trained deep model, instead of accurately illustrating all internal logic of the deep model,
which is investigated in Section 3.2. More crucially, there is no theoretical design to ensure that the
explainer model reflects the exact inference logic used by the deep model.
Objectiveness. In this paper, we define the objectiveness of an explainer model in game theory, and
based on this definition, we develop a hierarchical and symbolic explainer to objectively explain the
deep model. The objectiveness of an explainer model is defined as follows. If an explainer model
is considered objective, then it is supposed to generate exactly the same output as the explained
deep model given any arbitrary input. More crucially, we can derive the following deduction from
this definition. An objective explainer model should not only generate the same output as the deep
model, but also use the same underlying logic/reasons for inference.
Fortunately, our findings in game theory enable us to evaluate the objectiveness of an explainer
model, i.e. we find that we can accurately decompose the inference logic of a deep model into utili-
ties of interactions between input variables in game theory. Given an input sample with n variables
(e.g. a sentence with n words) N = {1, 2, . . . , n}, input variables usually interact with each other
for inference, instead of working individually. For each subset of variables S ⊆ N, we measure the
numerical utility of the interaction between variables in S using the Harsanyi dividend (Harsanyi,
1963) I(S). The subset S with a considerable utility is termed an interaction pattern. It is proved
that the model output can be decomposed as the sum of utilities of all potential interactions patterns,
1
Under review as a conference paper at ICLR 2022
V(N) = 3AB + 2,5BC + 2,1ABE + 1.7ABD
A
B
C
D
E
+1.3ABDE + 0,9BCE + 0.7DE
Input variables	AND node
Simplified
Figure 1: We transform the logic of a model into an three-layer And-Or graph (AOG), and extract
common coalitions of variables to build a deeper AOG, so as to further simplify the explanation.
i.e. model output = Ps⊆n I(S). For example, in Figure 1, let us consider the pattern S = {A, B, E}
in the input N = {A, BC, D, E}. Only if the A, B, E appear together, the deep model will generate
an additional interaction utility I(S = {A, B, E}) = 2.1 to the output. The absence of any variable
in S = {A, B, E} will remove the interaction utility. Then, We obtain the following conclusions.
•	We can use the interaction utility to evaluate the objectiveness of the explainer model. For
each interaction pattern S ⊆ N, if its interaction utility Ig (S) encoded by the explainer model does
not equal to its utility Iv (S) encoded by the deep model, then the explainer model does not satisfy
the objectiveness mentioned above.
•	We can directly consider interaction patterns decomposed from the model output as a set of
underlying reasons to explain the inference of the deep model.
Conciseness. Beyond the objectiveness, the explanation is supposed to be concise enough for people
to understand. The above explanation consists of 2n interaction patterns, which is quite complex.
Therefore, we develop the following three techniques to further simplify the explanation.
(1)	Boosting sparsity. We find that the 2n interaction patterns are actually sparse. Only a few patterns
have significant effects on the model output with large values of |I(S)|, namely salient patterns.
Other patterns have little effects with small values of |I(S)|, namely noisy patterns. Therefore, we
can just choose salient patterns with large |I(S)| to approximately explain the model output, and
ignore noisy ones. Furthermore, we can boost the sparsity of salient patterns by learning baseline
values, so as to further simplify the explanation. Here, baseline values are used to represent the
absence states of input variables, which are used in the computation of I(S).
(2)	Learning hierarchy. We transform the above interaction-based explanation into an And-Or graph
(AOG) with a deep hierarchy. The AOG combines variables, which frequently co-appear in different
interaction patterns, to construct a single node in the AOG, thereby simplifying the explanation.
(3)	Adversarial training. Moreover, we discover that adversarial training (Madry et al., 2018) makes
the deep model encode more sparse interaction patterns than standard training.
Discussion about the trustworthiness of the AOG explainer. First, the interaction utility based
on the Harsanyi dividend is proved to be compositional elements in other classical metrics for at-
tributions and interactions (Shapley, 1953; Grabisch & Roubens, 1999; Sundararajan et al., 2020).
Second, our interaction utility is proved to satisfy seven axioms. Third, strictly speaking, the fully
objective explanation consists of 2n interaction patterns. Thus, there is a trade-off between the ex-
planation conciseness and the explanation accuracy. To achieve scientific rigour, we can quantify
the limitation of the AOG explainer, i.e. measuring the ratio of components of the model output that
cannot be explained by the AOG.
Contributions of this paper can be summarized as follows: (1) We define two types of objectiveness
of an explainer model in game theory. (2) We develop an axiomatic, hierarchical, and symbolic AOG
to objectively explain the internal logic of a deep model. Furthermore, we propose three techniques
to simplify the explanation. (3) We prove the objectiveness and trustworthiness of the AOG explainer
in both theory and practice. In particular, the interaction utility used in this paper is proved to have
strong connections with other classical game-theoretic attribution/interaction metrics.
2	Related works
Explanations for deep models. Many explanation methods have been proposed to explain the
knowledge learned by deep models. The typical explanation methods include the visualization of
2
Under review as a conference paper at ICLR 2022
features learned by the DNN (Simonyan et al., 2013; Zeiler & Fergus, 2014; Yosinski et al., 2015;
Dosovitskiy & Brox, 2016), and the estimation of the pixel-wise attribution/saliency/importance of
input samples (Ribeiro et al., 2016; Lundberg & Lee, 2017; Fong & Vedaldi, 2017; Zhou et al., 2015;
2016; Selvaraju et al., 2017).
Beyond the above pixel-wise visualization and attribution, some studies explain the logic encoded
by a deep model by transforming its feature representations into an interpretable symbolic model.
Vaughan et al. (2018); Tan et al. (2018) learned an explainable additive model from a pre-trained
DNN via knowledge distillation. Frosst & Hinton (2017); Che et al. (2016); Wu et al. (2018) distilled
feature representations of a DNN into tree structures. Zhang et al. (2018) built an explanatory graph
to explain the And-Or grammar (Song et al., 2013) between visual concepts learned by a DNN. In
fact, the And-Or grammar was also used to learn an interpretable AOGNet (Li et al., 2019).
However, the above previous explainer models were mainly learned to fit the model output, but there
is no principle to make the explainer objectively illustrate the actual logic of the deep model. This
may raise the risk of incorrect explanations in unusual samples. In this study, we first define the
objectiveness of an explainer model, and further theoretically prove the objectiveness of the AOG
explainer.
Interactions. Many studies focus on interactions between input variables learned by deep models in
recent years (Sorokina et al., 2008; Murdoch et al., 2018; Singh et al., 2018; Jin et al., 2019; Janizek
et al., 2020). In game theory, Grabisch & Roubens (1999) first proposed the Shapley interaction in-
dex, and this index was used by Lundberg et al. (2018) to analyze tree ensembles. Sundararajan et al.
(2020) defined the Shapley-Taylor interaction index based on the Shapley value (Shapley, 1953). In
this paper, we use the Harsanyi dividend (Harsanyi, 1963) to measure the utility of interactions be-
tween input variables. This interaction metric has a strong connection to (Grabisch & Roubens,
1999), but satisfies the efficiency axiom. The efficiency axiom enables our interaction metric to be
used to decompose the model output.
3	Method
3.1	Decomposing the output of a deep model
As the theoretical foundation of the objectiveness metric for explanations, let us first introduce how
to decompose the output score of a deep model into the sum of utilities of interaction patterns. Given
an input sample x with n input variables (e.g., a sentence with n words) N = {1, 2, . . . , n} and a
pre-trained deep model v(∙), V(N) denotes the output score of the model w.r.t. the entire input N.
In a deep model, input variables do not contribute to the output score individually. Instead, they
interact with each other. Let I(S) denote the numerical utility of the interaction between variables
in the subset S ⊆ N. Let us take a model learned for sentiment classification and the input sentence
“Hi, sit down and take it” for example. If we add the word “easy” and obtain the sentence “Hi,
sit down and take it easy,” then the co-appearing of “take it easy” will significantly increase the
output score of the “positive” sentiment, and the score increase is referred to as the interaction
utility I(S = {take, it, easy}). Therefore, we consider the interaction between words in the subset
S has a positive interaction utility I(S) > 0 on the output score, and we call S a positive interaction
pattern. The absence of any word in S will destroy the interaction between words in S, thereby
removing its utility I(S). Similarly, if I(S) < 0, S is referred to as a negative interaction pattern. If
I(S) approximates 0, it means that variables in S have no interactions.
Fortunately, we discover that we can decompose the output score v(N) of a deep model into the
sum of interaction utilities I(S) (as shown in Figure 2 (left)), when we use the Harsanyi divi-
dend (Harsanyi, 1963) to quantify the interaction utility.
V(N)= Xsqn I (S),	s.t. I(S) = Xl⊆s ( - 1)|SlLL V(L)	⑴
where v(S) denotes the output score of the model when only variables in S are given. In particular,
I(0) = v(0). In the computation of v(S), variables out of S are usually replaced by their baseline
values to represent their absence. Computational details will be introduced in Section 3.3.
In this way, we can understand the Harsanyi dividend to measure the marginal utility of the in-
teraction between input variables in S, when we remove interaction utilities of all smaller sub-
3
Under review as a conference paper at ICLR 2022
the output score
of 3 words v (S)
marginal utilities
of 3 wo rd s
+
marginal utilities ɪ ɪ	ɪ ɪ
of 2 words	take easy + take it +
+	+
marginal utilities	ɪ	, ɪ	, ɪ
of 1 word	take	it	easy
+	+
a constant
bias term	-(0)
6 4 2 0
S3n-e> Aqdeqs PeeE-κ3
φ (.)=1.76
^cliche)=5 92
words
Figure 2: (left) The output score of the model v(S) can be decomposed into the sum of marginal
utilities of interactions between input variables in S, {I(L)|L ⊆ S}. (right) The interaction utility
I(S), |S| = m, is fairly assigned to the m variables in S in the computation of Shapley values.
sets of variables in S, {I(L)|L ( S}. In the aforementioned sentence “Hi, sit down and take
it easy,” the interaction utility of the pattern S = {take, it, easy} is defined as I ({take, it, easy}) =
v({ take, it, easy}) — I (0) — I ({take}) —1({ it}) —1({ easy}) — I ({take it}) —1({ take, easy}) —1({ it, easy}).
Axioms and theorems. In this paper, we further prove that the interaction utility based on the
Harsanyi dividend satisfies the following desirable axioms, which enhance the trustworthiness of this
metric. Except for the efficiency axiom, detailed proofs of other axioms are shown in Appendix A.
(1)	Efficiency axiom (proved by Harsanyi (1963)). The output score of a model can be decomposed
into interaction utilities of different patterns, i.e. v(N) = PS⊆N I(S).
(2)	Linearity axiom. If we merge output scores of two models w and v as the output of model u, i.e.
∀S ⊆ N, u(S) = w(S) + v(S), then their interaction utilities Iv(S) and Iw(S) can also be merged as
∀S ⊆ N,Iu(S) = Iv(S) +Iw(S).
(3)	Dummy axiom. If a variable i ∈ N is a dummy variable, i.e. ∀S ⊆ N \{i}, v(S ∪ {i}) =
v(S) + v({i}), then it has no interaction with other variables, ∀S ⊆ N \{i}, I(S ∪ {i}) = 0.
(4)	Symmetry axiom. If input variables i, j ∈ N cooperate with other variables in the same way,
∀S ⊆ N\{i, j}, v(S∪ {i}) = v(S∪ {j}), then they have same interaction utilities with other variables,
∀S⊆ N\{i, j}, I(S ∪ {i}) =I(S∪{j}).
(5)	Anonymity axiom. For any permutations π on N, we have ∀S ⊆ N, Iv (S) = Iπv (πS), where
∏S，{∏(i)∣i ∈ S}, and the new model ∏v is defined by (∏v)(∏S) = v(S). This indicates that
interaction utilities are not changed by permutation.
(6)	Recursive axiom. The interaction utilities can be computed recursively. For i ∈ N and S ⊆
N \{i}, the interaction utility of the pattern S ∪ {i} is equal to the interaction utility of S with the
presence of i minus the interaction utility of S with the absence of i, i.e. ∀S ⊆ N \ {i}, I(S ∪ {i}) =
I(S|i is always present) — I(S). I(S|i is always present) denotes the interaction utility when the variable
i is always present as a constant context, i.e. I(S|i is always present) = Pl⊆s(—i)lSl-lLl ∙ V(L ∪ {i}).
(7) Interaction distribution axiom. This axiom characterizes how interactions are distributed for
“interaction functions” (Sundararajan et al., 2020). An interaction function vT parameterized by a
context T satisfies ∀S ⊆ N, if T ⊆ S, vT (S) = c; otherwise, vT (S) = 0. Then, we have I(T) = c,
and ∀S 6= T, I(S) = 0.
Furthermore, we have also proven that the interaction utility I(S) can explain the elementary mech-
anism of existing game-theoretic attributions/interactions, and I(S) is equivalent to an existing in-
teraction metric in some cases.
Theorem 3.1 (Connection to the Shapley value, proved by Harsanyi (1963)). Let φ(i) denote the
Shapley value (Shapley, 1953) of an input variable i. Then, its Shapley value can be represented as
the weighted sum of interaction utilities, i.e. φ(i) = Ps⊆n、{订 /再 I (S ∪{i}). In other words, the
utility of an interaction pattern with m variables should be equally assigned to the m variables in
the computation of Shapley values.
This theorem also proves that the Shapley value is a fair assignment of attributions from the per-
spective of Harsanyi dividend, as shown in Figure 2 (right). We also conducted experiments to
compute the cosine similarity between the accurate Shapley value φ(i) and the estimated Shapley
value φ(i) = Es⊆n、⑵-^1+τ I (S ∪ {i}) in Figure 8, when we used different numbers of patterns
S. The result in Figure 8 shows that the Harsanyi dividend can accurately approximate the Shapley
value.
4
Under review as a conference paper at ICLR 2022
ΦPOEΦPOLU
rehcaeT tnedut
Figure 3: Knowledge distillation cannot ensure the objectiveness of the student model, because the
student model and the trained teacher model use different image regions to compute features. The
first row shows the Grad-CAM attention (Selvaraju et al., 2017) of the teacher model. The second
row shows the Grad-CAM attention of the student model.
Theorem 3.2 (Connection to the marginal benefit, proved in Appendix A.2). ∆vT (S) =
PL⊆T (-1)|T |-|L|v(L ∪ S) denotes the marginal benefit (Grabisch & Roubens, 1999) of variables in
T ⊆ N \ S given the environment S. We have proven that ∆vT (S) can be decomposed into the sum
of interaction utilities inside T and sub-environments of S, i.e. ∆vT (S) = PS0⊆S I(T ∪ S0).
Theorem 3.3 (Connection to the Shapley interaction index, proved in Appendix A.2). Given
a subset of input variables T ⊆ N, IShapley(T) = Ps⊆n\t '；||N-|S+i|T/ ∆vτ(S) denotes the
Shapley interaction index (Grabisch & Roubens, 1999) of T. We have proven that the Shap-
ley interaction index can be represented as the weighted sum of utilities of interaction patterns,
I Shapley (T) = Ps⊆n∖t ⅛+r I (S ∪ T) ∙ This metric treats the coalition of variables in T as a single
variable, and thus uniformly allocates the attribution I(S ∪ T) to all variables including T.
Theorem 3.4 (Connection to the Shapley Taylor interaction index, proved in Appendix A.2). Given
a subset of input variables T ⊆ N, let IShapley-Taylor (T) denote the Shapley Taylor interaction in-
dex (Sundararajan et al., 2020) of order kfor T. We have proven that the Shapley taylor interaction
index can be represented as the weighted sum of interaction utilities, i.e. I Shapley-Taylor (T) = I(T) if
|T | < k; IShPley-Tayyo(T) = PS⊆N\T (lSPk)TI (S ∪ T) if |T | = k; and IShMey-TTyyoo (T) = 0 if |T | > k.
3.2	Objectiveness of explanations
Lack of objectiveness of previous explainer models. Previous studies have proposed some sym-
bolic explainer models to explain black-box models, especially explaining DNNs (Krishnan et al.,
1999; Che et al., 2016; Wu et al., 2018). Most explainer models are lack of theoretical support
for their objectiveness, thereby hurting the trustworthiness of such explainer models. For example,
some methods used knowledge distillation (Hinton et al., 2015) to transform a pre-trained DNN into
another explainer model (Che et al., 2016; Frosst & Hinton, 2017; Tan et al., 2018). However, as
Figure 3 shows, knowledge distillation cannot ensure that the student model exhibits the same logic
as the teacher model. We distilled the output before the softmax of a pre-trained VGG-11 (Simonyan
& Zisserman, 2014) into another VGG-11 on the CIFAR-10 dataset (Krizhevsky et al., 2009). The
result shows that although the student DNN had similar outputs with the teacher DNN, their attention
computed by Grad-CAM (Selvaraju et al., 2017) was significantly different. Therefore, knowledge
distillation cannot ensure the student model represents the internal logic of the teacher model, mak-
ing the student model a non-objective explainer of the teacher model. Just like the explainer models
learned based on knowledge distillation, explainer models based on clustering (Zhang et al., 2018)
also cannot guarantee the objectiveness.
Definition of objectiveness. Therefore, we define the following two types of objectiveness, in order
to ensure that the explainer model objectively illustrates the true logic used by the deep model for
inference, and provides a trustworthy explanation to people.
Definition 1 (Objectiveness over arbitrary samples). ∀x ∈ Rn, g(N|x) = v(N |x).
This definition guarantees the explainer model g(∙) always generates the same output as the pre-
trained model v(∙) for any input x, including both normal samples and out-of-distribution (OOD)
samples. g(N |x) and v(N|x) denote output scores of the explainer model and the deep model given
all input variables in the sample x, respectively. From this perspective, the distillation-based expla-
nation can only ensure the explainer model to have the same outputs as the deep model on training
samples, but cannot mimic the deep model in all arbitrary samples, including OOD samples.
5
Under review as a conference paper at ICLR 2022
Understanding patterns as symbolic reasons. Based on Definition 1, we further define and derive
the sample-wise objectiveness of explanations from the perspective of symbolic reasoning. I.e. the
explainer model should use the same reasons for inference as the deep model. Before this, let us
first introduce how to understand interaction patterns as symbolic reasons for inference. For ex-
ample, given an input sentence “sit down and take it easy,” the model output can be decomposed
as v({sit, down, and, take, it, easy}) = I ({sit}) + I ({sit, down}) + I ({sit, take}) + I ({and}) +
I ({take, it}) + I ({take, it, easy}) + . The model considers the sentiment of this sentence posi-
tive because of the large positive utility of the pattern I{take, it, easy}, which represents the AND
relationship between words. The absence of any word in the pattern will deactivate this interaction.
Therefore, the pattern {take, it, easy} is considered as a symbolic explanation for the underlying
reason of the deep model’s inference.
Definition 2 (Sample-wise objectiveness of symbolic reasoning). Given a certain input sample
x ∈ Rn, ∀S ⊆ N, g(S|x) = v(S|x), where g(S|x) and v(S|x) denote output scores of the
explainer model and the deep model when only partial variables of the sample x in the subset S are
input to the model. Variables out of S are masked by their baseline values to represent their absence,
which will be introduced in Section 3.3.
Corollary 1 (proved in Appendix B). If an explainer model g satisfies Definition 2, then given an
input sample x ∈ Rn, ∀S ⊆ N, Ig(S|x) = Iv (S |x). Ig(S|x) and Iv(S|x) denote interaction
utilities of S in the explainer model and the explained model, respectively.
Corollary 1 is derived from Definition 2, and Definition 2 is actually a necessary condition of Defini-
tion 1 (proofs in Appendix B). Definition 2 mainly focuses on the objectiveness of the explanation
on a single input sample. Corollary 1 further shows that the explainer model is supposed not
only to generate the same output as the deep model, but also to use the same underlying rea-
sons. According to Equation (1) v(N |x) = PS⊆N I(S|x), we can consider these interaction utilities
I(S|x) as underlying reasons for the inference of the deep model. Thus, Corollary 1 ensures that the
reasons used by the explainer model are the same as the reasons used by the deep model.
3.3	Constructing the hierarchical and symbolic And-Or Graph
And-Sum relationship between input variables. The decomposition of the model output in Equa-
tion (1), i.e. v(N) = PS⊆N I(S), explains the deep model as an And-Sum representation. Each
interaction utility I(S) represents an AND relationship between variables in S. For example, in
Figure 1, for the pattern S = {A, B, E}, when all variables A, B, E appear together in the input, the
utility I(S) will be added to the model output. Otherwise, the absence of any variable i ∈ S will
remove the interaction utility I(S) from the model output. The SUM relationship refers to that all
utilities I(S) of all interaction patterns add up to the model output v(N).
Learning baseline values to obtain concise explanations. The above And-Sum representation is
the sum of 2n - 1 interaction patterns and a constant bias I(0), which is too complex for people to
understand. Among all 2n - 1 patterns, most of them have little influence on the model output with
small absolute values |I(S)|, namely noisy patterns. A few patterns have large impacts on the model
output with large absolute values |I(S)|, namely salient patterns. The sparsity of salient patterns
enables us to only use salient patterns to approximate the model output as V(N) ≈ Ps∈Ωsaii≡∏t I (S),
which ensures the conciseness of the explanation. Ωsalient denotes the set of salient patterns.
In fact, the sparsity of salient patterns does not only depend on the deep model itself, but it is
also determined by the choice of baseline values. In this way, we can further boost the sparsity of
salient patterns by learning baseline values, in order to enhance the conciseness of the explanation.
Specifically, according to Equation (1), I(S) = Pl⊆s(-l)lSl-lLl ∙ V(L) where V(L) is the model
output when only variables in L are given. In this case, input variables out of L are masked by their
baseline values (or called reference values) to represent the absence of these variables, as follows.
v(L) = model(xmask), ximask = xi, i∈L	(2)
ri, i ∈ N \ L
where ri denotes the baseline value of the i-th input variable. r = [r1, r2, . . . , rn]. Therefore, we
minimize the following loss to learn baseline values that maximize the sparsity of salient patterns.
Lossm = Xs∈ΩRΩsalJI(S )l	⑶
6
Under review as a conference paper at ICLR 2022
where Ωall = 2N = {S∣S ⊆ N} denotes the set of all interaction patterns.
Ratio of the explained utilities Rk. Considering the sparsity of salient patterns, we only use salient
patterns with the top-k largest absolute values | I(S) |, denoted by Ωsopent, to approximately explain
the output of the deep model. In this way, the ratio of the explained interaction utilities to the overall
model output can be quantified Rk as follows.
Σs∈Ωsal- |I (S )1
top-k
Ps∈Ωsopiekt |I(S)| + 八1
(4)
Instead of computationally expensive enumeration of all interaction patterns (e.g. the metric in
Appendix H), We compute ∆ = V(N) - v(0) - Ps∈Qsalient I(S) to denote utilities of the unexplained
∈''top-k
patterns to save the computational cost. Appendix H shows another metric for the explained utilities.
Transforming the And-Sum representation into a hierarchical And-Or Graph (AOG). In fact,
the above And-Sum representation can be further transformed into a deep hierarchical AOG to make
the explanation more concise. Actually, we can consider the And-Sum representation as a three-layer
AOG. Then, we summarize common coalitions of input variables shared by different AND nodes to
construct a deeper AOG. Figure 1 (left) shows an example of the three-layer And-Sum representation
(AOG). At the bottom layer, there are n leaf nodes representing n variables of the input sample. The
second layer of the AOG has multiple AND nodes, each representing the AND relationship between
its child nodes. The node ABE indicates the AND relationship between its child nodes A, B, and
E, i.e. the interaction pattern S = {A, B, E}. Its interaction utility I({A, B, E}) = 2.1 is labeled on
the top of the node. The root node is a noisy OR node (as discussed in (Li et al., 2019)), which sums
up utilities of all its child AND nodes to mimic the model output, i.e. output = PS∈ Qsalient I (S).
∈ "top-k
Furthermore, we extract common coalitions shared by different interaction patterns as single nodes
to simplify the AOG. In Figure 1, input variables A, B frequently co-appear in different salient
interaction patterns. Thus, we consider A, B as a coalition and add a single AND node α = {A, B}
to represent their co-appearing. Therefore, the pattern {A, B, E} is simplified as {α, E}.
The extraction of common coalitions of input variables is guided by the minimum description length
(MDL) principle (Hansen & Yu, 2001). Given an AOG g and inputvariables N ,let M = N ∪ Ωcoalition
denote the set of all terminal nodes and AND nodes in the bottom two layers (e.g. M = N∪Ωcoalition =
{A, B, C, D, E } ∪ {α, β, γ } in Figure 1 (right)). The MDL principle can be formulated as follows.
L(g,M)=	L(M)	+	LM(g)	(5)
l-{^}	l-{z"}
length of describing the set of nodes M length of using nodes in M to describe interaction patterns in g
The first term L(M) = -λ Pm∈M p(m) log p(m) represents the description length of the set of nodes
M, where ∀m ∈ M, p(m) = Count(m)/£m√∈M count(m0) s.t. count(m) = S∈sal^S3m3m II(S)|. p(m)
measures the probability of the node m appearing in salient interaction patterns. Ωsopent denotes the
set of top-k salient patterns which are represented using all nodes in M. Ωsopient needs to be updated
when we insert a new AND node into M. λ = 10/Z is a scalar weight, where Z = Ps∈Ωsalie∏t ∣I(S)∣.
∈''top-k
The second term LM(g) = -ES〜p(s∣g) Pm∈s logp(m) represents the length of describing salient
interaction patterns using nodes in M . The appearing probability of the interaction pattern S in the
AOG gis sampled as P(S∣g) X ∣I(S)∣. The loss L(g, M) can be minimized via the greedy strategy
to extract common coalitions of input variables by following (Hansen & Yu, 2001). Please see
Appendix C for more discussions.
4 Experiments
Datasets and models. We focused on both tasks of natural language processing and the classifi-
cation/regression task based on tabular datasets. For the natural language processing, we explained
LSTMs (Hochreiter & Schmidhuber, 1997) and CNNs used in (Rakhlin, 2016). Each model was
trained for the sentiment prediction on the SST-2 dataset (Socher et al., 2013) and trained for the
linguistic acceptability classification on the CoLA dataset (Warstadt et al., 2019), respectively. The
tabular datasets included the UCI census income dataset (Dua & Graff, 2017), the UCI bike sharing
dataset (Fanaee-T & Gama, 2013), and the UCI TV news channel commercial detection dataset (Dua
7
Under review as a conference paper at ICLR 2022
OR
node
output g(N)=24.08, positive sentiment, Rk= 95.7%
AND
nodes
salient)
(patterns)
AND
nodes
(Common)
(coalitions)
input
variables
output g(N)=15.62, grammatically correct, fik=98.1%
Red edges indicate a parse graph	+
of an interaction pattern. —
+2.84	+0.34	-0.43	+0.29 ,+0.82	+3.29	+0.65	+0.23	+0.25
Almost ""s's* lawyer
could	could
Almost	no
lawyer	lawyer
-0.23	-0.63
-0.37	+0.20，
that
question
+0.28
-0.20
3.29
-3.29
+
+7.75	-1.50	-1.58 /+2.65	-0.86	∣+3.68	-3.32
+2.21	+0∙65 7.76
stomach S
than	■
7^76
You 'll need a stronger stomach than us
(b)
Almost no lawyer could answer that question
(a)
Figure 4: Examples of the AOG extracted from the CNN trained on (a) the CoLA dataset and (b) the
SST-2 dataset, respectively. The red color of nodes in the second layer indicates interaction patterns
with positive utilities, while the blue color represents patterns with negative utilities. Red edges
indicate the parse graph of an interaction pattern.
Table 1: IoU on the synthesized datasets. Our
method correctly extracted salient interaction
patterns.
Dataset	Model	Average IoU
Addition-Multiplication dataset (Zhang et al., 2021)	functions in the dataset	1.0000
Dataset in (Ren et al., 2021)		-E0000
Manually labeled And-Or dataset	-MLP-5 ReSMLP-5	0.9761 0.9787
Re-labeled TV news dataset (Dua&Graff, 2017)	MLP-5 ReSMLP-5	0.8180 0.8189
Table 2: Jaccard similarity between two models.
Two adversarially trained models were more sim-
ilar than two normally trained models.
		TV news	census	bike
MLP-2	normal	0.5965	0.4899	-
	adversarial	0.6109	0.6292	-
MLP-5	normal	0.3664	0.2482	0.3816
	adversarial	0.6304	0.4971	0.4741
ResMLP-5	normal adversarial	0.3480 0.5731	0.2764 0.4489	0.3992 0.4491
& Graff, 2017). These datasets were termed census, bike, and TV news for simplicity. Each tabu-
lar dataset was trained on MLPs, LightGBM (Ke et al., 2017), and XGBoost (Chen & Guestrin,
2016), and we explained these models. For MLPs, we used two-layer MLPs (namely MLP-2) and
five-layer MLPs (namely MLP-5), where each layer contained 100 neurons. Besides, we added a
skip-connection to each layer of the MLP-5 to build the residual-MLP (namely ResMLP-5). Please
see Appendix D.1 for more discussions. Figure 4 shows examples of the AOG generated by our
method on CNNs trained on the SST-2 dataset and the CoLA dataset. Please see Appendix G.1 for
the visualization of more AOGs.
Intersection over union (IoU) between the extracted salient patterns and ground-truth pat-
terns. In this experiment, we evaluated whether the extracted salient interaction patterns correctly
reflected the interaction in the model. However, for most datasets, people could not annotate the
ground-truth patterns learned by deep models, as discussed in (Zhang et al., 2021). To this end, we
used the following functions and datasets with ground-truth interaction patterns for evaluation. The
first two datasets were the Addition-Multiplication dataset (Zhang et al., 2021) and the dataset pro-
posed in (Ren et al., 2021). The third dataset was extended from the TV news dataset (Dua & Graff,
2017). In order to construct ground-truth interaction patterns, we re-labeled samples in this dataset
following a set of pre-defined And-Or functions, namely the re-labeled TV news dataset. Besides,
we also labeled samples from the Gaussian distribution based on the same set of And-Or functions,
namely the manually labeled And-Or dataset. Please see Appendix D.2 for more details. Then, we
learned MLP-5 and ResMLP-5 based on the two And-Or datasets.
Given a model and an input sample, let k denote the number of ground-truth patterns Ωtruth,
which has been labeled in the dataset. Then, for a fair comparison, we also extracted the top-
k salient patterns Ωsap1eknt from the model. We measured the IoU between Ωtruth and Ωsapeknt as
IoU = ∣Ωsapieknt ∩ Ωtruth∣∕∣Ωsapieknt ∪ Ωtruth∣ to evaluate the accuracy of the extracted patterns. Results
in Table 1 show that our interaction metric based on the Harsanyi dividend successfully extracted
most salient interaction patterns. Note that the IoU based on the re-labeled TV news dataset was
about 81.8%, instead of approaching 100%. It was because there was no principle to ensure the
model to learn ground-truth interaction patterns, rather than the problem of our method.
Objectiveness of the AOG explainer. We also proposed a metric to evaluate the objectiveness of
explainer models according to the definition in Section 3.2. We compared the AOG explainer with
seven baseline explanation methods, including distillation-based explainer models and attribution-
based explanations. The AOG explainer exhibited much stronger objectiveness than baseline expla-
nation methods. Please see Appendix F for detailed metrics and experiments.
8
Under review as a conference paper at ICLR 2022
1.0
0.0∙
0	10	20	30 k
(a)
。 O
4 2
S9POU#
0.0	0.4
(b)
(t
səmpə°#
0.8 Rh	0.0
0.4
(c)
-----MLP-5 (normal)
----MLP-5 (adversarial)
-----ReSMLP-5 (normal)
----ResMLP-5 (adversarial)
-----MLP-2 (normal)
----MLP-2 (adversarial)
0.8 Rie — LightGBM
Ii----- XGBooSt
0.0 |KS)|/maxs，|KS')|	1∙0
interaction utility computed by
■ mean baseline values
1 learned baseline values
(SEə-ed Jo≈∙0oso-
Figure 6: The histogram
of the re-scaled interac-
tion strength. The learned
baseline values boosted
the sparsity of salient in-
teraction patterns.
Figure 5: (a) The change of Rk (the ratio of the explained utilities)
along with the number of salient patterns k in the AOG. (b, c) The
change of the node number and the edge number in the AOG along
with Rk . AOGs corresponding to adversarially trained models were
less complex than AOGs corresponding to normally trained models.
Ratio of the explained utilities Rk. As discussed in Section 3.3, we just used salient interaction
patterns with top-k absolute values |I(S)| to approximate the model output. Figure 5 (a) shows
the relationship between k and the ratio of the explained utilities Rk in different models based on
the TV news dataset. When we used a few interaction patterns, we could explain most utilities of
interaction patterns to the model output. Figure 5 (b)(c) shows that the node number and the edge
number of the AOG increased along with the increase of Rk. Please see Appendix G.2 for results
on other datasets.
Effects of baseline values on the conciseness of explanations. In this experiment, we explored
whether the learning of baseline values in Section 3.3 could boost the sparsity of salient patterns.
To this end, we followed (Dabkowski & Gal, 2017) to initialize baseline values of input variables
as their mean values over different samples. Then, we learned baseline values via Equation (3).
Figure 6 shows the histogram of the interaction strength
∣i(s)l
maxS0⊆N II(SO)I
which was re-scaled to the
range of [0, 1]. Compared with mean baseline values, the learned baseline values usually generated
fewer salient interaction patterns with more significant strength. Thus, the learned baseline values
boosted the sparsity of salient interaction patterns and enhanced the conciseness of explanations. In
this experiment, we used MLP-5 and computed the re-scaled strengths of interaction patterns with
training samples in the TV news dataset. Please see Appendix I for more experimental results.
Effects of adversarial training. In this experiment, we learned MLP-2, MLP-5, and ResMLP-
5 on the TV news dataset via adversarial training (Madry et al., 2018). Figure 5(a) shows that
compared with the normally trained model, we could use less salient patterns (smaller k) to explain
the same ratio of interaction utilities Rk in the adversarially trained model. Moreover, Figure 5 (b,c)
also shows that AOGs corresponding to adversarially trained models were less complex than AOGs
corresponding to normally trained models. This indicated that adversarial training made models
encode more sparse interaction patterns than normal training.
Besides, adversarial training also made different models encode common patterns. To this end, we
trained different pairs of MLP-2, MLP-5, and ResMLP-5 models with the same architecture but us-
ing different initial parameters. Given the same input, we measured the Jaccard similarity coefficient
between interaction utilities of each pair of models, in order to examine whether the two models en-
coded similar interaction patterns. Let I(S) and I0(S) denote the interaction utilities of these two
TnCrlfIlC Tha TQppqtH ClTnllS⊆erwIffipiαnt ∖szqc Cdnmitarl qc T — ^^S⊆N min(∣/ (S)∣J/ (S)D Tqhlα ɔ
models. The JaCCard SiImilarity CoeffiCient Was CoImPUted as J — P^ max(∣i(s)∣ ∣i'(s)∣). Table 2
shows that the Jaccard similarity between two adversarially trained models was significantly higher
than the JaCCard similarity between two normally trained models. This indiCated that adversarial
training made different models enCode Common interaCtion patterns for inferenCe.
5 Conclusion
In this paper, we define the objeCtiveness of an explainer model in game theory, and develop a
hierarChiCal and symboliC And-Or graph (AOG) to explain the deep model. We theoretiCally prove
and experimentally verify the objeCtiveness and trustworthiness of the AOG explainer. Furthermore,
we propose several teChniques to boost the ConCiseness of the explanation. Besides, we disCover that
adversarial training usually makes different models learn simple and similar interaCtions.
9
Under review as a conference paper at ICLR 2022
Reproducibility Statement. Most work in this research is to theoretically derive the AOG represen-
tation for the deep model. Therefore, there is no problem with the reproducibility. Appendix A and
Appendix B provide proofs for all theoretical results in the paper. Besides, the implementation of
MDL is just an engineering module. We have discussed all experimental details about datasets, mod-
els, and the implementation of MDL in Section 4 and Appendix D, which ensure the reproducibility.
Furthermore, we will release the code of building the AOG when the paper is accepted.
References
Sebastian Bach, Alexander Binder, Gregoire Montavon, Frederick Klauschen, Klaus-Robert Muller,
and Wojciech Samek. On pixel-wise explanations for non-linear classifier decisions by layer-wise
relevance propagation. PloS one, 10(7):e0130140, 2015.
Zhengping Che, Sanjay Purushotham, Robinder Khemani, and Yan Liu. Interpretable deep models
for icu outcome prediction. In AMIA annual symposium proceedings, volume 2016, pp. 371.
American Medical Informatics Association, 2016.
Tianqi Chen and Carlos Guestrin. Xgboost: A scalable tree boosting system. In Proceedings of the
22nd acm Sigkdd international conference on knowledge discovery and data mining, pp. 785-794,
2016.
Ian Covert and Su-In Lee. Improving kernelshap: Practical shapley value estimation using linear
regression. In International Conference on Artificial Intelligence and Statistics, pp. 3457-3465.
PMLR, 2021.
Ian Covert, Scott M Lundberg, and Su-In Lee. Understanding global feature contributions with
additive importance measures. Advances in Neural Information Processing Systems, 33, 2020.
Piotr Dabkowski and Yarin Gal. Real time image saliency for black box classifiers. arXiv preprint
arXiv:1705.07857, 2017.
Alexey Dosovitskiy and Thomas Brox. Inverting visual representations with convolutional networks.
In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 4829-
4837, 2016.
Dheeru Dua and Casey Graff. UCI machine learning repository, 2017. URL http://archive.
ics.uci.edu/ml.
Hadi Fanaee-T and Joao Gama. Event labeling combining ensemble detectors and background
knowledge. Progress in Artificial Intelligence, pp. 1-15, 2013. ISSN 2192-6352. doi: 10.1007/
s13748-013-0040-3. URL [WebLink].
Ruth C Fong and Andrea Vedaldi. Interpretable explanations of black boxes by meaningful pertur-
bation. In Proceedings of the IEEE international conference on computer vision, pp. 3429-3437,
2017.
Nicholas Frosst and Geoffrey Hinton. Distilling a neural network into a soft decision tree. arXiv
preprint arXiv:1711.09784, 2017.
Michel Grabisch and Marc Roubens. An axiomatic approach to the concept of interaction among
players in cooperative games. International Journal of game theory, 28(4):547-565, 1999.
Mark H Hansen and Bin Yu. Model selection and the principle of minimum description length.
Journal of the American Statistical Association, 96(454):746-774, 2001.
John C Harsanyi. A simplified bargaining model for the n-person cooperative game. International
Economic Review, 4(2):194-220, 1963.
Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. Distilling the knowledge in a neural network. arXiv
preprint arXiv:1503.02531, 2015.
Sepp Hochreiter and Jurgen Schmidhuber. Long short-term memory. Neural computation, 9(8):
1735-1780, 1997.
10
Under review as a conference paper at ICLR 2022
Joseph D Janizek, Pascal Sturmfels, and Su-In Lee. Explaining explanations: Axiomatic feature
interactions for deep networks. arXiv preprint arXiv:2002.04138, 2020.
Xisen Jin, Zhongyu Wei, Junyi Du, Xiangyang Xue, and Xiang Ren. Towards hierarchical impor-
tance attribution: Explaining compositional semantics for neural sequence models. In Interna-
tional Conference on Learning Representations, 2019.
Guolin Ke, Qi Meng, Thomas Finley, Taifeng Wang, Wei Chen, Weidong Ma, Qiwei Ye, and Tie-
Yan Liu. Lightgbm: A highly efficient gradient boosting decision tree. Advances in neural
information processing Systems, 30:3146-3154, 2017.
R. Krishnan, G. Sivakumar, and P. Bhattacharya. Extracting decision trees from trained neu-
ral networks. Pattern Recognition, 32(12):1999-2009, 1999. ISSN 0031-3203. doi: https:
//doi.org/10.1016/S0031-3203(98)00181-2. URL https://www.sciencedirect.com/
science/article/pii/S0031320398001812.
Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images.
Technical report, Citeseer, 2009.
Xilai Li, Xi Song, and Tianfu Wu. Aognets: Compositional grammatical architectures for deep
learning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recogni-
tion, pp. 6220-6230, 2019.
Scott M Lundberg and Su-In Lee. A unified approach to interpreting model predictions. In Proceed-
ings of the 31st international conference on neural information processing systems, pp. 4768-
4777, 2017.
Scott M Lundberg, Gabriel G Erion, and Su-In Lee. Consistent individualized feature attribution for
tree ensembles. arXiv preprint arXiv:1802.03888, 2018.
Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and Adrian Vladu.
Towards deep learning models resistant to adversarial attacks. In International Conference on
Learning Representations, 2018.
W James Murdoch, Peter J Liu, and Bin Yu. Beyond word importance: Contextual decomposition to
extract interactions from lstms. In International Conference on Learning Representations, 2018.
A Rakhlin. Convolutional neural networks for sentence classification. GitHub, 2016.
Jie Ren, Zhanpeng Zhou, Qirui Chen, and Quanshi Zhang. Learning baseline values for shapley
values. arXiv preprint arXiv:2105.10719, 2021.
Marco Tulio Ribeiro, Sameer Singh, and Carlos Guestrin. ” why should i trust you?” explaining the
predictions of any classifier. In Proceedings of the 22nd ACM SIGKDD international conference
on knowledge discovery and data mining, pp. 1135-1144, 2016.
Ramprasaath R Selvaraju, Michael Cogswell, Abhishek Das, Ramakrishna Vedantam, Devi Parikh,
and Dhruv Batra. Grad-cam: Visual explanations from deep networks via gradient-based local-
ization. In Proceedings of the IEEE international conference on computer vision, pp. 618-626,
2017.
Lloyd S Shapley. A value for n-person games. Contributions to the Theory of Games, 2(28):307-
317, 1953.
Avanti Shrikumar, Peyton Greenside, Anna Shcherbina, and Anshul Kundaje. Not just a black
box: Learning important features through propagating activation differences. arXiv preprint
arXiv:1605.01713, 2016.
Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image
recognition. arXiv preprint arXiv:1409.1556, 2014.
Karen Simonyan, Andrea Vedaldi, and Andrew Zisserman. Deep inside convolutional networks: Vi-
sualising image classification models and saliency maps. arXiv preprint arXiv:1312.6034, 2013.
11
Under review as a conference paper at ICLR 2022
Chandan Singh, W James Murdoch, and Bin Yu. Hierarchical interpretations for neural network
predictions. In International Conference on Learning Representations, 2018.
Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D Manning, Andrew Y Ng,
and Christopher Potts. Recursive deep models for semantic compositionality over a sentiment
treebank. In Proceedings of the 2013 conference on empirical methods in natural language pro-
cessing ,pp.1631-1642, 2013.
Xi Song, Tianfu Wu, Yunde Jia, and Song-Chun Zhu. Discriminatively trained and-or tree models
for object detection. In Proceedings of the IEEE conference on computer vision and pattern
recognition, pp. 3278-3285, 2013.
Daria Sorokina, Rich Caruana, Mirek Riedewald, and Daniel Fink. Detecting statistical interactions
with additive groves of trees. In Proceedings of the 25th international conference on Machine
learning, pp. 1000-1007, 2008.
Mukund Sundararajan, Kedar Dhamdhere, and Ashish Agarwal. The shapley taylor interaction
index. In International Conference on Machine Learning, pp. 9259-9268. PMLR, 2020.
Sarah Tan, Rich Caruana, Giles Hooker, Paul Koch, and Albert Gordo. Learning global additive
explanations for neural nets using model distillation. arXiv preprint arXiv:1801.08640, 2018.
Joel Vaughan, Agus Sudjianto, Erind Brahimi, Jie Chen, and Vijayan N Nair. Explainable neural
networks based on additive index models. arXiv preprint arXiv:1806.01933, 2018.
Alex Warstadt, Amanpreet Singh, and Samuel R Bowman. Neural network acceptability judgments.
Transactions of the Association for Computational Linguistics, 7:625-641, 2019.
Mike Wu, Michael C Hughes, Sonali Parbhoo, Maurizio Zazzi, Volker Roth, and Finale Doshi-
Velez. Beyond sparsity: Tree regularization of deep models for interpretability. In Thirty-Second
AAAI Conference on Artificial Intelligence, 2018.
Jason Yosinski, Jeff Clune, Anh Nguyen, Thomas Fuchs, and Hod Lipson. Understanding neural
networks through deep visualization. arXiv preprint arXiv:1506.06579, 2015.
Matthew D Zeiler and Rob Fergus. Visualizing and understanding convolutional networks. In
European conference on computer vision, pp. 818-833. Springer, 2014.
Hao Zhang, Yichen Xie, Longjie Zheng, Die Zhang, and Quanshi Zhang. Interpreting multivariate
shapley interactions in dnns. In Proceedings of the AAAI Conference on Artificial Intelligence,
volume 35, pp. 10877-10886, 2021.
Quanshi Zhang, Ruiming Cao, Feng Shi, Ying Nian Wu, and Song-Chun Zhu. Interpreting cnn
knowledge via an explanatory graph. In Thirty-Second AAAI Conference on Artificial Intelligence,
2018.
Bolei Zhou, Aditya Khosla, Agata Lapedriza, Aude Oliva, and Antonio Torralba. Object detectors
emerge in deep scene cnns. In ICLR, 2015.
Bolei Zhou, Aditya Khosla, Agata Lapedriza, Aude Oliva, and Antonio Torralba. Learning deep
features for discriminative localization. In Proceedings of the IEEE conference on computer
vision and pattern recognition, pp. 2921-2929, 2016.
12
Under review as a conference paper at ICLR 2022
A	Proofs of axioms and theorems of the interaction utility
A.1 Proofs of axioms
In this section, we prove that the interaction utility I(S) satisfies the efficiency, linearity, dummy,
symmetry, anonymity, recursive, and interaction distribution axioms.
(1)	Efficiency axiom. The output score of a model can be decomposed into interaction utilities
inside different patterns, i.e. v(N) = PS⊆N I(S).
• Proof: According to the definition of the interaction utility, we have
XI(S)=XX(-1)|S|-|L|v(L)
S⊆N	S⊆N L⊆S
= X X (-1)|S|-|L|v(L)
L⊆N S⊆N:S⊇L
n
=XX X (-1)s-|L|v(L)
L⊆N s=|L| S⊆MS⊇L
|S|=s
= X v(L)nX-|L| n -m|L|!(-1)m = v(N)
L⊆N	m=0
(2)	Linearity axiom. If we merge output scores of two models w and v as the output of model u,
∀S ⊆ N, i.e. u(S) = w(S) + v(S), then their interaction utilities Iv(S) and Iw(S) can also be
merged as ∀S ⊆ N, Iu(S) = Iv (S) + Iw (S).
• Proof: According to the definition of the interaction utility, we have
Iu(S) = X(-1)|S|-|L|u(S)
L⊆S
= X (-1)|S|-|L|[v(S) + w(S)]
L⊆S
=X(-1)|S|-|L|v(S)+X(-1)|S|-|L|w(S)
L⊆S	L⊆S
=Iv (S) + Iw (S).
(3)	Dummy axiom. If a variable i ∈ N is a dummy variable, i.e. ∀S ⊆ N \{i}, v(S ∪ {i}) =
v(S) + v({i}), then it has no interactions with other variables, ∀S ⊆ N \{i}, I(S ∪ {i}) = 0.
• Proof: According to the definition of the interaction utility, we have
I(S∪ {i}) = X (-1)|S|+1-|L|v(L)
L⊆S∪{i}
=X (-1)lsl+1TLlv(L) + X (-1)lSl-lLlv(L ∪{i})
L⊆S	L⊆S
= X (-1)|S|+1-|L|v(L) + X (-1)|S|-|L| [v(S) + v({i})]
L⊆S	L⊆S
=X (-1严-1L] v({i})
L⊆S
=0.
(4)	Symmetry axiom. If input variables i, j ∈ N cooperate with other variables in the same way,
∀S ⊆ N \{i, j}, v(S ∪ {i}) = v(S ∪ {j}), then they have same interaction utilities with other
variables, ∀S ⊆ N\{i, j}, I(S ∪ {i}) = I(S ∪ {j}).
13
Under review as a conference paper at ICLR 2022
• Proof: According to the definition of the interaction utility, we have
I(S∪{i})= X (-1)|S|+1-|L|v(L)
L⊆S∪{i}
=X (-1)lSl+1-lLlv(L) + X (-l)lSl-lLlv(L ∪{i})
L⊆S	L⊆S
=X (-1)lSl+1TLlv(L)+ X (-1)lsl-lLlv(L ∪{j})
L⊆S	L⊆S
= X (-1)|S|+1-|L|v(L)
L⊆S∪{j}
=I(S∪{j}).
(5)	Anonymity axiom. For any permutations π on N, we have ∀S ⊆ N, Iv (S) = Iπv (πS), where
πS，{∏(i)∣i ∈ S}, and the new model πv is defined by (πv)(πS) = v(S). This indicates that
interaction utilities are not changed by permutation.
• Proof: According to the definition of the interaction utility, we have
Inv(∏S) = X (-1)1SHLl(πv)(πL)
L⊆S
= X(-1)|S|-|L|v(L)
L⊆S
=Iv (S).
(6)	Recursive axiom. The interaction utilities can be computed recursively. For i ∈ N and S ⊆
N \{i}, the interaction utility of the pattern S ∪ {i} is equal to the interaction utility of S with
the presence of i minus the the interaction utility of S with the absence of i, i.e. I(S ∪ {i}) =
I(S|i is always present) - I(S) (s.t. S ⊆ N \{i}). I(S|i is always present) denotes the interaction
utility when the variable i is always present as a constant context, i.e. I(S|i is always present) =
Pl⊆s (-1)1SlLL V(L ∪{i}).
• Proof: According to the definition of the interaction utility, we have
I(S∪{i})= X (-1)|S|+1-|L|v(L)
L⊆S∪{i}
=X (-1严+i1v(L)+ X (-1严 TLIv(L ∪{i})
L⊆S	L⊆S
=X (-1严TLIv(L ∪{i}) - X (-1)lsl-lLlv(L)
L⊆S	L⊆S
=I(S|i is always present) - I(S).
(7)	Interaction distribution axiom. This axiom characterizes how interactions are distributed for
“interaction functions” (Sundararajan et al., 2020). An interaction function vT parameterized by a
context T satisfies ∀S : T ⊆ S ⊆ N, vT(S) = c, and vT (S) = 0 if T ( S. Then, we have I(T) = c,
and ∀S 6= T, I(S) = 0.
•	Proof: If S ( T , we have
I(S)=	吆=0.
∀L(S(T,v(L)=0
If S = T , we have
14
Under review as a conference paper at ICLR 2022
I (S) =I (T )= X (-1)t TLv(L)
L⊆T
=v(T) + X (-I)ITll i v(L) = c.
L(T	Iz
(	=0
If S ( T, we have
I (S ) = X (-I) i SIT L i v(L)
L⊆S
=c∙ X(-l) iSIT L i
L⊆S
L⊇T
=c∙X(lslmlTI)(T)m=o∙
m = 0
A.2 PROOFS OF THEOREMS
In this section, we prove connections between the interaction utility I(S) and several game-theoretic
attributions/interactions. We first prove Theorem 3.2, which can be seen as the foundation for proofs
of Theorem 3.1, 3.3, 3.4, and 3.5.
Theorem 3.2 (Connection to the marginal benefit) ∆vτ(S) = PLUT(-l)i TI-I l iv(L U S) denotes
the marginal benefit (Grabisch & Roubens, 1999) of variables in T ⊆ N \ S given the environment
S. We have proven that Δvt (S) can be decomposed into the sum of interaction utilities inside T and
sub-environments of S, i.e. Δvt(S) = Ps/⊆s I(T U S0).
•	Proof: By the definition of the marginal benefit, we have
Δvt(S) = E (-l)1 T1 - 1L1 v(L U S)
LUT
=X (-l) i T i - i L i X I (K) //by Equation (1,left)
LUT	KULUS
=X (-1)1TI-I l i x X I(L0 U S0) //since L ∩ S = 0
LUT	L0UL S0US
=X [x (-1)1T i - iLi X i(L0 U s0)
S0US LUT	L0 UL
E EE(T)1Ti I (L0 U S0)
S0US L0UT LUT
_	L⊇L0	,
Σ
S0US
I (S0 U T)+ T
' V '	L0 (T
L0=T	、-
(X (l T l- LL0l )(-i) i T H L i i (l0 u T
V= IL0 I ∖	l l /	)
{
L0(T
-	(	Y
X I (S0 U T)+ X	I (L0 U S0) ∙ χ (l T -- LL0l )(-i) i T i - i L i
S0US	L0(T	l= IL0 I ∖ II，
一 ∖	'-------------=Z-------------").
X I(S0 U T) □
S0US
15
Under review as a conference paper at ICLR 2022
In particular, if T is a singleton set, i.e. T = {i}, we can obtain the conclusion in (Ren et al., 2021)
that ∆vw(S) = Pl⊆s ML ∪{2})∙
The proof for Theorem 3.1, 3.3, and 3.4 are similar. Since the Shapley interaction index is a gener-
alization of the Shapley value and the bi-variate interaction utility, we first prove Theorem 3.4, and
then prove Theorem 3.1 and 3.3.
Theorem 3.1 (Connection to the Shapley value) Let φ(i) denote the Shapley value (ShaPley,1953)
of an input variable i. Then, its Shapley value can be represented as the weighted sum of interaction
utilities, φ(i) = PscN∖{i} ∣⅛rI(S ∪ {i}). In other words, the utility of an interaction pattern with
m variables should be equally assigned to the m variables in the computation of Shapley values.
• Proof: By the definition of the Shapley value, we have
φ(i) =E E	[v(S ∪{i})-v(S)]
m S⊆N∖{i}
| S | =m
|N|-1
J⅛ X 7N⅛	X	[v(Su{i})-v(S)]
1	1 m = 0 ∖ m S S⊆N∖{i}
| S | =m
1 INIT 1
闻 X PE X	△*}(§)
1	1 m = 0 ∖ m S S⊆N∖{i}
| S | =m
∣N∣-1
∣⅛ X7⅛	X	X I(L ∪{i})
1	1 m = 0 ' m ) S⊆N∖{i} LCS
| S | =m	'
// by Theorem 3.2
∣N∣-1
∣⅛	X	X7⅛	X I(L∪{i})
LCN∖{i} m = 0 ' m S S⊆N∖{i}
| S | =m
S⊇L
1 南	INI-1	1 X X 7^TV	X I(L ∪{i}) //since S ⊇ L, ∣S∣ = m ≥∣L∣. LCN∖{i} m=∣L∣ ( m ) S⊆N∖{i} | S | = m S⊇L
1 闻	L	*1	1	(∖N∣-∣L∣- A τrτ r.ηλ X X (INIT) ∙ ( m-∣L∣	)I(L U{i}) LCN∖{i} m=∣L∣ 1 m )	∖	1	1	/
1 闻	L	LlJNIT^L∣τ 1	八N∣-∣L∣- lʌ X I(L u {i}) X (INIT) ∙ (	k	) LCN ∖{i}	k=0	k∣L∣+⅛J ∖	) S	V	' wL
Then, we leverage the following properties of combinatorial numbers and the Beta function to sim-
plifythe term WL = Pk=ITILIT	∙ (INTL∣-1).
(IL|+k)
(i)	A property Ofcombinitorial numbers. m ∙ (m) = n ∙ (m-1).
(ii)	The definition of the Beta function. For p,q > 0, the Beta function is defined as B(p,q)=
r01 xp-1(1 - x)1-qdx.
(iii)	Connections between combinitorial numbers and the Beta function.
o When p,q ∈ Z+, we have B(p, q) = W±_.
q( p-1 )
θ For m,n ∈ Z+ and n>m, we have (m) = m∙B(n-1m + 1,m) ∙
16
Under review as a conference paper at ICLR 2022
|N ∣-∣L∣-1
WL= X
k = 0
∣N ∣-∣L∣-1
=X
k=0
∣N ∣-∣L∣-ι
1	IN ∣-∣L∣- 1
电⑴∙ I k
IN1T-1) ∙ (|L| + k) ∙ B(|N∣-∣l∣- k, |L| + k)
E il∙
IN|一11- 1) ∙ B(|N∣-∣L∣- k, ILI + k)	…①
k=0	∖	)
+ l X1 k ∙(lN l-kLI- 1) ∙ B(∣N ∣-∣L∣- k, ∣L∣ + k)	…②
Then, we solve φ and ② respectively. For ①，we have
①=/ 1∣L∣ N X T(IN I-M- 1! ∙ X〔 MT L f-1 ∙ (1 -斓 L1+k-1 dx
0	k=0
/1 ∣L∣∙
0
∣N ∣-∣L∣-1
X
k=0
• X〔NITLI-k-1 ∙ (1 - χ)k
•(I-
x)∣L∣-1
dx
=1
Z1
0
∣L∣(1 - X)ILI-I dx = 1
For ②，we have
②=∣ ∣X∣ (∣N∣-∣L∣- 1) ∙(lN1 --l1- 2)∙ B(∣N∣-∣L∣- k, ∣L∣ + k)
INI-ILI-2 /1 N । _ ∣L∣ _ 2、
=(|N∣-∣L∣- 1)	E l l ko l	∙ B(∣N∣-∣L∣- k0 - 1, ∣L∣ + k0 + 1)
k0 = 0	∖	)
1 ∣N∣-∣L∣-2 /
=(∣N ∣-∣L∣- 1)/	X
J0	k'=0 ∖
|NITLI- 2) . x〔 nl - l Li'-2 • (1 - x)lL*k' dx
k0	I
=(∣N∣-∣L∣- 1) [1
0
INIXI 2 (lNl -1LI- 21 • XINITLi-2 • (1 - x)
k0=0	∖	)
k0
•(1 - x)|L| dx
|
^^^^"{z
=1
=(|N∣-∣L∣- 1) ∣' 1(1 - X)ILI dx = INl-lLl1- 1
0	∣L∣ + 1
✓
Hence, we have
WL = (D + ②=1 +
∣N ∣-∣L∣- 1
∣L∣ + 1
∣N ∣
∣L∣ +1
Therefore, We PrOVed φ(I) = INi PS⊆N∖{i} WL ∙ I(L U {i}) = Ps⊆N∖{i} ∣⅛I(S U {i}).	□
Theorem 3.3 (Connection to the Shapley interaction index) Given a subset of input variables T ⊆
N yShaPle"7、—— ∖ `	ISI!(INi-ISI-tτi)! ʌ. /q∖ Hpτιotpc the Shqnlev iτιtprι⅞ptioτι inHpv
IN, i (T ) — S js⊆n∖t	(।N∣ ∣τ∣ ∣1); ^AVT (S) denotes the ShaPley interaction index (GlaUiSCh
& Roubens, 1999) of T. We have proven that the Shapley interaction index can be represented as
the weighted sum of utilities of interaction patterns, IShapIey(T) = PSUN∖t ^f⅛rI(S U T).
17
Under review as a conference paper at ICLR 2022
• Proof:
IShaPley (T)_ £
S⊆N∖T
∣s∣!(∣N ∣-∣s ∣-∣τ∣)! ∆vτ ⑻
QN∣-∣T∣ + 1)! T( )
I	|N |-|T I I
∣N∣ - ∣T∣ +1 X (INHTI) X	盘T(S)
Il 1	1	m = 0	1 m S S⊆N∖T
| S | =m
1	IN I-ITI	1
∣N∣ - ∣T∣ +1 X	(INHTI) X	X I(L U T)
......... m = 0 ' m S S⊆N∖T L⊆S
|S|=m L	1
1
∣N ∣-∣T ∣ +1
IN I-IT I
XX
L⊆N∖T m=ILI
(INI-ITI) Σ2 1 (L U T)
∖ m S S⊆N∖T
| S | =m
S⊇L
1
∣N ∣-∣T ∣ +1
IN I-IT I
XX
L⊆N∖T m=ILI
1	1 N ∣-∣L∣-
(™) [ m -∣l∣
∣t ∣)i(L U T)
1
∣N ∣-∣T ∣ +1
IN I-ILI-IT I
X I(L U T)	X
L⊆N∖T	k=0
1	IN ∣-∣L∣-∣T ∣
⅛JΓ I k
{^^^^^^^^^^^^^^^^^^^^^
✓
wL
Just like the proof of Theorem 3.1, we leverage the properties of Combinitorial numbers and the Beta
function to simplify wl.
IN I-ILI-ITI
WL = X
k=0
|N |-|L|-|T |
=X
k=0
|N |-|L|-|T |
=X
1	1 N∣- ∣ L∣-∣ T∣
(N-T) [	k
∣ N ∣-∣ L ∣-∣ T ∣ ) ∙ ( ∣ L ∣ + k) ∙ B( ∣ N ∣- ∣ L ∣-∣ T ∣ - k + 1,∣ L ∣ + k
∣L∣∙ (∣n ∣ - ∣ L ∣ - ∣ T ∣) ∙ b(∣N ∣- ∣ L ∣ - ∣ T ∣ - k +1,∣ L ∣ + fc)	…①
k=0
+ 1 I X I Ik ∙(∣NITLITT ∣)∙ b( ∣ N ∣ - ∣ L ∣ - ∣ T ∣ - k +1,∣ L ∣ + k)	…②
Then, we solve φ and ② respectively. For (T), we have
①=∕1
0
|L|
i n i -XT TI(IN ∣-∣l∣-∣t ∣! ∙ X i n i - i L i - i T i -k ∙ (1 - x) i L i+k-1 dx
I'1 ∣L∣
0
I N I X I T I (∣n ∣ - ∣l∣ - ∣t ∣) ∙ X I NI - IL I - ITI -k .α -步产
k=0	k
■{z
=1
∣L∣ ∙ (1 - x)i l i-1 dx = 1
∙(1 - x)i l i-1 dx
Z1
0
|
✓
For (2 , we have
18
Under review as a conference paper at ICLR 2022
|N|-|L|-|T|
②=X (|N I-
k = 1
ILI-IT I) (|N l-
∣L∣-∣T∣- 1
k - 1
- B(IN I-ILI-IT I- k +1, ILI + k)
=(IN I-
∣L∣-∣T|)|	| |X| |	(INITLk-ITl-1) ∙B(|N∣-∣L∣-∣TI-k1,∣L∣	+	k0	+ 1
=(IN I-
∣L∣-∣TI)[
0
k0 = 0	∖
，1 |N|-|L|-|T|-1
=(IN I-
∣L∣-∣TI) [1
0
Σ
k0=0	'
]N|-|L|-|T|-1
X
k0 = 0
|N ITL!- 1T l- 1) . x | MT L 1-1T1-k，-1 ∙ (1 - x)1L ” dx
k0	I
∖Ni - ILI - ITi - 1) , x|N|-|L|-|T|-k0-1 , (1 - χ)
k0
∙(1 — X)ILI dx
=(IN I-
^^~{z
=1
∣L∣-∣TI) ∣' 1(1 - X)ILI dx = INl -JLI- ITl
Jo	|L| + 1
Hence, we have
WL =① + ②=1 +
∣N ∣-∣L∣-∣T ∣
∣L∣ + 1
∣N ∣-∣T∣ + 1
∣L∣ + 1
I
✓
Therefore, we proved that I ShaPley(T ) = w⅛τ Pl⊆n\t WL ∙ I (L ∪ T )= Pl⊆n\t ⅛fγ I(L ∪ T).
Theorem 3.4 (Connection to the Shapley Taylor interaction index) Given a subset of input
variables T ⊆ N, let IShaPley-TaylOr(T) denote the Shapley Taylor interaction index (Sundararajan
et al., 2020) of order k for T. We have proven that the Shapley taylor interaction index can be
represented as the weighted sum of interaction utilities, i.e. IShaPley-TaylOr(T) = I(T) if ∣T∣ < k;
I ShaPley-TaylOr(T) = PS⊆N、T。SVk)TI(S U T) if ∣T ∣ = k; and I ShaPley-TaylOr(T) =0 if ∣T I > k.
• Proof: By the definition of the Shapley Taylor interaction index,
i∆vτ(0)	if ∣T∣ < k
A Ps⊆n∖t ET)∆vτ(S)	if IT I = k
0	'S'	if ∣T∣ > k
When ∣T∣ < k, by the definition of the interaction utility in Equation (1), we have
IShaPley-Taylor(k)(T) = ∆vτ(0) = E (-1)1T1-1L1 ∙ V(L) = I(T).
L⊆T
When ∣T ∣ = k, we have
19
Under review as a conference paper at ICLR 2022
「Shapley-Taylor(k) ∕τ∖ _ k
(TX 闻 S⊆N∖T
1
(Ik)
* ∆vτ (S)
k
|N|
|N|-k
XXR
m = 0 S⊆N∖T1 |S| ,
| S | =m
* ∆vτ (S)
M X Xwly	XI(L U T)
1	1 m = 0 S⊆N∖T k |S| ) L⊆S
|S|=m	L	」
.	∣N ∣-k i
⅛ X X 因 X I(L U T)
1	1 L⊆N∖T m=|L| ∖ |S| S S⊆N∖T
| S | =m
S⊇L
k
囱
|N |-k
XX
L⊆N∖T m=|L|
1 I |N|-|L|- k]KL ∪ T)
F I m-∣L∣ 尸L U T)
k
囱
|N|-|L|-k
X I(L U T)	X
LCN∖T	m=0
wL
Just like the proof of Theorem 3.1, we leverage the properties of COmbinitOrial numbers and the Beta
function to simplify wl.
|N |-|L|-k
WL= X
m = 0
|N|-|L|-k
=X
1
|N∣-∣L∣- k
m
IN|一尸- k) , (∣L∣ + m) ∙ B(|N| - |L| - m, |L| + m)
m = 0	∖	)
|N |-|L|—k	∕∣ 入厂 I Irl i ∖	、
X	∣L∣∙ 1 l-ml- ∙ b(∣N I-ILI- m,|L| + m)	…①
m = 0
+ । 1X1 m ∙ (lNITLI-k) ∙ b(∣N I - ∣L∣ - m, ∣L∣ + m)	…②
m=0	m
Then, we solve ① and ② respectively. For ①，we have
①=∕1
0
∣L∣∙
INI X‘ k (lNITLI - k) ∙ x1N1-1L1-m-1 ∙ (1 - x)|L1+m-1 dx
m=0	∖	m	)
Z1
0
∣l∣∙
I
Z1
0
|N |-|L|-k /1 ΛT∣	Irl	J ∖
X ( IN l TLI- k ) ∙ χIN|-|L|-m-k ∙ (1 - χ)m
m = 0	∖ m )
"{z
=1
∣L∣ ∙ XkT ∙ (1 - X)ILIT dx = ∣L∣ ∙ B(k, ∣L∣)
1
(IL+1-1)
∙xk-1 ∙ (1 - x)1L1-1 dx
✓
For ②，we have
20
Under review as a conference paper at ICLR 2022
|N|-|L|-k
②=X(|n|-
m = 1
|L|- k) ∙ (lN l-m--k - 1) ∙ B(|N |-|L|- m,|L| + m
|N |-|L|-k-1
X
m0 = 0
(|N |
-|L|- k) ∙ (|NITm: k - 1) ∙ B(|N| - |L| - m' - 1, |L| + m' + 1
;(|N |
Jo
|N|-|L|-k-1
|L|- k)	X
m0=0
|L| - k - 1
m0
∙ MMTLI-m'-2 ∙ (1 - x)1L1+m，dx
['('N |
0
|L|
- k)
|N|-|L|-k-1 ∕∣ 入厂I Irl 1 ι∖
X (IN|-|L| - k - 1 j ∙ x|N|-|L|-m'-k-1 . (J - χ)m'
m'=0	∖	/
∙xk-1 ∙ (1 - X)ILI dx
■{z
=1
「(\N |
0
-|L| - k) ∙ XkT ∙ (1 - X)ILI dx = (|N| - |L| - k) ∙ B(k, |L| + 1)
|N|-|L|- k
(|l| + 1)CL-+k)
Hence, we have
WL =(D + @
1	,	|N|-|L|- k
(ILk--1)	(|L| + 1)(iLi+')
|L|!.(k - 1)! + |N|-|L|- k (|L| + 1)! ∙ (k - 1)!
(|L| + k — 1)!	|L| + 1	(|L| + k)!
|L|! ∙ (k - 1)!	|N| -|L|- k	|L|! ∙ (k - 1)!
(|L| + k — 1)!	|L| + k (|L| + k — 1)!
'1 + |N|-|L|- kL M∙(k-1)!
_	|L| + k _	(|L| + k — 1)!
|N|	|L|! ∙ (k - 1)!
7-：---- ∙—--：---------
|L| + k (|L| + k — 1)!
|N|	|L|!. k!
k	(∖L∖ + k)!
|N| .	1
k (IL|+k)
Therefore, we proved that when |T| = k, IShaPley-TaylOr(T)=占 PLUN\t wl ∙ I(L ∪ T)
舟 Pl⊆n∖t 郎∙ (TL+) ∙ I(L U T) = PLUN\t (ILk+k)-1I(L U T).
B Proof of the relationship between two types of objectiveness
and Corollary 1
Section 3.2 formally defined two types of objectiveness. In this section, we prove the relationship
between them, i.e. Definition 2 is actually a necessary condition of Definition 1. We also prove the
Corollary 1 from Definition 2.
Proof of the relationship between Definition 1 and Definition 2. Since g(N|x) = V(N|x) holds
for any input X ∈ Rn,it also holds for the input xmask when some of input variables in X are masked
by their baseline values, i.e. g(NIXmaSk) = V(NIXmaSk). Let S be an arbitrary subset of N, and xmask is
obtained by replacing variables out of S with their baseline values, as follows.
mask = ʃ Xi, i ∈ S
-1 ri,	otherwise
(6)
—
—
|
✓
21
Under review as a conference paper at ICLR 2022
where ri denotes the baseline value of the i-th input variable, as discussed in Section 3.3. In
this case, g(S|x) = g(N |xmask) and v(S|x) = v(N |xmask). Therefore, we have g(S|x) = v(S|x).
Because S is an arbitrary subset of N, we have proved that given an input sample x ∈ Rn , ∀S ⊆ N,
g(S|x) = v(S|x). Hence, Definition 2 is a necessary condition of Definition 1.
Proof of Corollary 1. According to Definition 2, given a certain input sample x, ∀S ⊆ N, g(S|x) =
V(S|x). Thus, we have Iv(S|x) = Pl⊆s(-i)* lSl-lLlv(S∣x) = Pl⊆s(-1)lSl-lLlg(S∣χ) = Ig(S|x).
Thus, we proved Corollary 1.
C S implifying the explanation using the minimum description
LENGTH PRINCIPLE
In this section, we discussed the algorithm of extracting common coalitions to minimize the total
description length in Equation (5). Given an AOG g and input variables N, let M = N ∪ Ωcoαlition
denote the set of all terminal nodes and AND nodes in the bottom two layers (e.g. M = N∪Ωcoalition =
{A, B, C, D, E} ∪ {α, β, γ} in Figure 1 (right)). The total description length L(g, M) was given in
Equation (5).
In order to minimize L(g, M), we used the greedy strategy to iteratively extract common coalitions
of input variables. In each iteration, we chose the coalition α ⊆ N which most efficiently decreased
the total description length. Then We took this coalition as an AND node, and added it into Ωcoalition in
the third layer of the AOG. The efficiency ofa coalition α w.r.t. the decrease of the total description
length was defined as follows.
δ(α)
∆L
|a|
L(g,M ∪{α}) - L(g,M)
|a|
(7)
where L(g, M) denoted the total description length without using the newly added coalition α,
and L(g, M ∪ {α}) denoted the total description when we added the node α to further simplify
the description of g. |a| denotes the number of input variables in a. We iteratively extracted the
most efficient coalitions α to minimize the total description length. The extracting process stopped
when there was no new coalition α could further reduce the total description length (i.e. ∀α ∈/
M, L(g, M ∪ {α}) - L(g, M) > 0), or the most efficient α was not shared by multiple patterns. Alg.
1 shows the pseudo-code of this algorithm.
1
2
3
4
5
6
7
8
9
10
11
12
13
Algorithm 1: The greedy algorithm to minimize total description length L(g, M)
Input: The set of terminal nodes N, the set of top-k salient patterns Ωsopent, the interaction
utilities of these patterns {I(S) }s∈Ωsalient, the maximum iteration times T, the coefficient
λ in Equation (5)
Output: The set of nodes in the bottom two layers M = N ∪ Ωcoalition
Initialize Ωcoalition = 0 and M = N ∪ Ωcoalition
for iteration 1 to T do
foreach possible coalition α ⊆ N do
I Calculate the efficiency δ(α) according to Equation (7)
end
Select α as the coalition whose δ(α) is the smallest
if δ(α) > 0 or α co-appears in only one pattern then
I break
end
Update AND nodes, Ωcoalition — Ωcoalition ∪ {ɑ}
Rewrite each pattern S ∈ Ωsopent according to M = N ∪ Ωcoalition
end
return M = N ∪ Ω coalition
22
Under review as a conference paper at ICLR 2022
D Experiment details
D. 1 Datasets and models
Datasets. We conducted experiments on both tasks of natural language processing and the classifica-
tion/regression tasks based on tabular datasets. For natural language processing, we used the SST-2
dataset (Socher et al., 2013) for sentiment prediction and the CoLA dataset (Warstadt et al., 2019) for
linguistic acceptability. For tabular datasets, we used the UCI census income dataset (census) (Dua
& Graff, 2017), the UCI bike sharing dataset (bike) (Fanaee-T & Gama, 2013), and the UCI TV
news channel commercial detection dataset (TV news) (Dua & Graff, 2017). We followed (Covert
et al., 2020; Covert & Lee, 2021) to conduct data pre-processing for these tabular datasets. We also
normalized data in each dataset to zero mean and unit variance.
Models. We trained LSTMs and CNNs based on NLP datasets. The LSTM was unidirectional and
had two layers, with a hidden layer of size 100. The architecture of the CNN was the same as the
network architecture in (Rakhlin, 2016). Besides, for tabular datasets, we followed (Covert et al.,
2020; Covert & Lee, 2021) to train LightGBMs (Ke et al., 2017), XGBoost (Chen & Guestrin, 2016),
and two-layer MLPs (namely MLP-2). We also trained five-layer MLPs (namely MLP-5) and five
layer MLPs with skip-connections (namely ResMLP-5) on these datasets. For the ResMLP-5, we
added a skip connection to each fully connected layer of the MLP-5. Figure 7 shows the architecture
of the ResMLP-5. The hidden layers in MLP-5 and ResMLP-5 had the same width of 100. In our
experiment, we also learned MLP-2, MLP-5, and ResMLP-5 on each tabular dataset via adversarial
training (Madry et al., 2018). During adversarial training, adversarial examples xadv = x + δ were
generated by the '∞ PGD attack, where ∣∣δ∣∣∞ ≤ 0.1. The attack was iterated for 20 steps with the
step size of 0.01.
input —►
.JOA-∩-pκ
一
.JOAOZI
古-q
-enp∞oκ
古-q
-BnP∞ωκ
>pq
-enpo
—o output
Residual block:	∣ . FC layer ∙ + f ReLU layer ∙
Figure 7:	The architecture of the ResMLP-5.
Accuracy of the models. Table 3 reports the classification accuracy of models trained on the Com-
mercial dataset, the classification accuracy of models trained on the census dataset, and the mean
squared error of models trained on the bike dataset. Table 4 reports the classification accuracy of
models trained on the CoLA dataset and the SST-2 dataset.
Table 3: Classification accuracy (on commercial and census dataset) Table 4: Accuracy of mod-
and mean squared error (on bike dataset) of different models.	els trained on NLP datasets.
Dataset	MLP-2 normal adversarial		MLP-5 normal adversarial		ResMLP-5 normal adversarial		XGBoost	LightGBM	Dataset	LSTM	CNN
commercial	83.11%	78.49%	79.86%	-80.24%-	79.01%	80.13%	84.48%	84.19%-	CoLA	64.42%	65.79%
census	79.91%	75.77%	78.96%	77.79%	80.49%	77.99%	87.35%	87.54%	SST-2	86.83%	78.19%
bike	-	-	2161.47	3080.73	2149.43	2708.59	1623.71	-			
D.2 Details of experiments on synthesized functions and datasets
This section provides details of synthesized functions and datasets used in Section 4.
The Addition-Multiplication dataset (Zhang et al., 2021). This dataset contained 100 functions,
which only consisted of addition and multiplication operations. For example, v(x) = x1 + x2x3 +
x3x4x5 + x4x6. Each variable xi was a binary variable, i.e. xi ∈ {0, 1}.
The ground-truth interaction patterns corresponding to these functions can be easily determined.
For each term in these functions (e.g. the term of x1, x2x3, x3x4x5, x4x6 in the function v(x) = x1 +
x2x3 +x3x4x5 + x4x6), only when variables contained by this term were all present, x1 = 1, this term
would contribute to the output. Therefore, we could consider input variables in each term formed a
ground-truth interaction pattern. In the above example function, given the input x = (1, 1, 1, 1, 1, 1),
the ground-truth interaction patterns were Ωtruth = {{χι}, {χ2,χ3}, {χ3,χ4,χ5}, {χ4,X6}}. Given the
input X = (1,1,0,1,1,1), the ground-truth interaction patterns were Ωtruth = {{χι}, {χ4, X6}}.
23
Under review as a conference paper at ICLR 2022
In our experiments, we randomly generated 100 Addition-Multiplication functions. Each of them
had 10 input variables, and had 10 to 100 terms. Then, we randomly generated 200 binary input
samples for each of these functions. For each input sample, let k = ∣Ωtruth∣ denote the number of
the labeled ground-truth patterns. For fair comparison, we computed interaction utilities I(S) and
। Ωsalient∩Ωtruth |
extracted the top-k salient patterns Ωs麒.Then, We averaged the value of IoU = ∣ΩsXkt∪Ω⅛⅛∣ over
all samples.
The dataset in (Ren et al., 2021). This dataset contained 100 functions, Which consisted of addi-
tion, subtraction, multiplication, and the sigmoid operations. Just like the Addition-Multiplication
dataset, the ground-truth interaction patterns in this dataset could also be easily determined. Let
us consider the function v(x) = -x1x2x3 - sigmoid(5x4x5	-	5x6 -	2.5), xi ∈	{0, 1}	as an ex-
ample. The term x1x2x3 Was activated (= 1) if and only	if	x1 =	x2 = x3	= 1.	The term
sigmoid(5x4x5 - 5x6 - 2.5) Was activated (> 0.5) if and only if x4 = x5 = 1 and x6 = 0. Thus,
We could also consider this function contained tWo ground-truth interaction pattern. In other Words,
for the above function, given the input x = (1, 1, 1, 1, 1, 0), the ground-truth interaction patterns Were
Ωtruth = {{χ1,χ2,χ3}, {χ4,χ5,χ6}}. Given the input X = (1,1,1,1,1,1), the ground-truth interaction
patterns were Ωtruth = {{χ1,χ2,χ3}}.
In our experiments, We folloWed (Ren et al., 2021) to randomly generated 100 functions. Each of
them had 6 to 12 input variables. Then, we randomly generated 200 binary input samples for each
of these functions. Just like the Addition-Multiplication dataset, we extracted the top-k (k = ∣Ωtruth |)
salient patterns Ωsopient, and computed the average IoU between Ωtruth and Ωsopent over all samples for
comparison.
The manually labeled And-Or dataset. This dataset contained 10 functions with AND operations
(denoted by &) and OR operations (denoted by |). For example, the function f(x) = (x1 > 0)&(x2 >
0)|(x2 > 0)&(x3 > 0)&(x4 > 0)|(x3 > 0)&(x5 > 0). Each input variable is a scalar, i.e. xi ∈ R,
and the output is binary, i.e. f(x) ∈ {0, 1}. For each And-Or function, we randomly generated
100,000 Gaussian noises with n = 8 variables as input samples, and labeled these samples following
functions in the And-Or dataset, namely the manually labeled And-Or dataset.
The ground-truth interaction patterns in this dataset could be determined as follows. For the above
function, we could consider {x1 , x2 }, {x2, x3, x4}, and {x3, x5} as possible interaction patterns. If
any of these patterns was significantly activated, i.e. if all input variables in this pattern were greater
than a threshold τ = 0.5, then we consider this pattern is significant enough to be a valid ground-
truth interaction pattern. I.e. for the above function, given the input x = (1.0, 2.0, 1.5, 0.9, 0.8),
the ground-truth interaction patterns were Ωtruth = {{χ1,χ2}, {χ2,χ3,χ4}, {χ3,χ5}}. Given the input
x = (0.8,1.5,1.2,0.1,0.9), the ground-truth interaction patterns were Ωtruth = {{χ1,χ2}, {χ3,χ5}}.
In our experiments, we trained one MLP-5 and one ResMLP-5 networks for binary classification
based on the manually labeled dataset generated based on each And-Or function. For each well-
trained model, just like the above two datasets, we extracted the top-k salient patterns and computed
the average IoU over 1000 training samples for comparison. Note that there was no principle to
ensure that the model learned exactly the ground-truth interactions between input variables for in-
ference. Therefore, the average IoU on this dataset was less than 1.
The re-labeled TV news dataset. This dataset was extended from the TV news dataset. Just like
the above manually labeled And-Or dataset, we re-labeled samples in the TV news dataset following
10 functions only with AND operations and OR operations.
We also trained MLP-5 and ResMLP-5 networks for binary classification based on the re-labeled TV
news dataset generated based on each And-Or function. We used the same method in the manually
labeled And-Or dataset to determine the ground-truth interaction patterns for each input sample.
Then, we averaged the value of the IoU between Ωtruth and Ωsopent over the 1,000 training samples, as
discussed above. In this dataset, there was also no principle to ensure that the model learned exactly
the ground-truth interactions between input variables for inference. Therefore, the average IoU on
this dataset was also less than 1.
An extended version of the Addition-Multiplication dataset. In order to evaluate the correct-
ness of the computed interaction utilities, we also extended the Addition-Multiplication dataset to
generate functions with not only ground-truth interaction patterns, but also ground-truth interaction
utilities for evaluation. The extended Addition-Multiplication dataset also contained 100 functions,
24
Under review as a conference paper at ICLR 2022
which consisted of addition and multiplication operations. Each variable xi was a binary variable,
i.e. xi ∈ {0, 1}. Different from functions in the Addition-Multiplication dataset, there were different
coefficients before each term in each function. For example, v(x) = 3x1 - 2x2x3 - x3x4x5 + 5x4 x6 .
The ground-truth interaction utilities in these functions can be easily determined. Just like the
original Addition-Multiplication dataset, each term was a ground-truth pattern. In this case, we
could consider the interaction utility of each pattern as the value of its coefficient. For the
above function, given the input x = (1, 1, 1, 1, 1, 1), the ground-truth utilities of interaction pat-
terns were I({x1}) = 3, I({x2 , x3}) = -2, I({x3, x4, x5}) = -1, I({x4, x6}) = 5, and for other
S ⊆ {x1, ..., x6}, I(S) = 0. Given the input x = (1, 1, 0, 1, 1, 1), the ground-truth interaction utilities
were I({x1}) = 3, I({x4, x6}) = 5, and for other S ⊆ {x1, ..., x6}, I(S) = 0.
In our experiments, we randomly generated 100 functions. Each of them had 10 input variables,
and had 10 to 100 terms. Then, we randomly generated 200 binary input samples for each of
these functions. For each input sample, we measured the Jaccard similarity coefficient between
ground-truth interaction utilities Itruth(S) and interaction utilities I(S) computed by our method as
J = pS⊆N maX(∣I⅛uth(S)l∣'l∣I(S)∣). The average value of J over all samples was 1.00, indicating that our
interaction metric based on the Harsanyi dividend correctly extracted the interaction utilities in these
functions.
D.3 Settings of v(∙)
In the computation of interaction utilities, people usually used different settings of v(∙). For ex-
ample, Lundberg & Lee (2017) directly set v(S) = p(ytruth |xmask) in the classification task, where
xmask was defined in Equation (6). p(ytruth|xmask) denoted the output probability of the ground-truth
category. Covert et al. (2020) computed v(S) as the cross-entropy loss. In this paper, we used
V(S) = log ι-pyyh1iχχma)k) for models learned on classification tasks. For models learned on regres-
sion tasks, we directly computed v(S) as the scalar output of the model on xmask.
E Verification of Theorem 3.1
In this section, we conducted an experiment to verify Theorem 3.1, i.e. the Shapley value can be
accurately approximated by the Harsanyi dividend. Let φ(i) denote the accurate Shapley value and
φ(i) denote the estimated Shapley value using the Harsanyi dividend I (S), which was mentioned
in Section 3.1. In order to estimate the Shapley value via interaction utilities I(S), we first selected
the most salient k interaction patterns Ωsopent. Then, according to Theorem 3.1, We computed the
estimated Shapley value φ(i) as φ(i) = J2s∈Ωsalie∏t,s3i 卤I(S). Figure 8 shows the cosine similarity
between the accurate Shapley values φ = [φ(1), φ(2), . . . , φ(n)] ∈ Rn and the estimated Shapley
values φ = [φ(l),φ(2),..., φ(n)] ∈ Rn, when We used different ratios of salient patterns 会 to
approximate the Shapley value. It indicated that the interaction utilities based on the Harsanyi
dividend could accurately approximate the Shapley value.
1.0
0.8
© 0.6
W
飞0.4
u
0.2
0.0
	
		 CNN-COLA (normal) ——CNN-CoLA (adversarial) -LSTM-CoLA (normal) LSTM-CoLA (adversarial) 	CNN-SST-2 (normal) ——CNN-SST-2 (adve rsarial) 	LSTM-SST-2 (normal) ——LSTM-SST-2 (adversarial)
0.2	0.6	1.0
the ratio of patterns USed
Figure 8:	The cosine similarity between the accurate Shapley value φ and the estimated Shapley
value φ, when we used different ratios of salient patterns for estimation.
25
Under review as a conference paper at ICLR 2022
Table 5: Non-objectiveness (J) of different explanation methods. The AOG explainer exhibited the
strongest objectiveness.
Explanation method		TV news		census		bike	
		MLP-5	ResMLP-5	MLP-5	ResMLP-5	MLP-5	ResMLP-5
Distillation-based	GBT	-0.7661-	0.4305-	-9.3453-	2.1947	-1.5120	1.9422
	SDT	1.1612	0.8991	25.9458	4.0938	-	-
explainer models	distillation	0.6718	0.5573	12.7566	3.8734	1.4025	1.6904
	Shapley value	-0.7897-	-0.6157-	-11.0216-	2.3912	-1.7850	3.4056
Attribution-based explainer models	Input × Gradient	2.5373	2.2533	78.2222	7.5902	5.7450	6.9758
	LRP	1.3558	7.9203	49.9572	114.5870	5.7450	152.1785
	Occlusion	2.7904	1.9336	62.8212	9.1191	9.1114	13.3653
Ours		3∙58×10-7	L93×10-L	3.68×10-6	1.16×10-6	1.12×10-6 L54×10-6	
F Analysis on the objectiveness of the AOG explainer
F.1 The objectiveness of the AOG explainer
In this section, we proposed a metric to evaluate the objectiveness of explainer models, according to
definitions in Section 3.2. Given a pre-trained model v(∙) and a corresponding explainer model g(∙),
we defined the metric ρnon-obj to measure the non-objectiveness of the explainer model g, as follows.
Pnon-Obj =& SEN [B(S)-g(S)∣i	(8)
ρnon-obj measured the components of the deep model output v(S) that was not explained by the output
g(S) of the explainer model. For fair comparisons, |v(N)|was used for normalization. A lower value
of ρnon-obj indicated a stronger objectiveness.
We compared the objectiveness of the AOG explainer with both distillation-based explainer mod-
els and attribution-based explanations, including GBT (Che et al., 2016), SDT (Frosst & Hin-
ton, 2017), knowledge distillation (Hinton et al., 2015), the Shapley value (Shapley, 1953),
Input×Gradient (Shrikumar et al., 2016), LRP (Bach et al., 2015), and Occlusion (Zeiler & Fer-
gus, 2014). Due to their different formulation for explanations, the computation of g(S) in different
explanation methods were different, which were discussed as follows.
•	For distillation-based explainer models, given the input sample x, let fexplainer(x) denote the output
of the explainer model. Then, g(S) was computed as the output of the explainer model when only
variables in S were given. In this case, input variables out ofS were masked by their baseline values
to represent the absence of these variables, as follows.
mask mask	xi , i ∈ S
g(S) = fexplainer(x	), x =	ri , otherwise	(9)
where ri denoted the baseline value of the i-th input variable.
•	For attribution-based explainer models, given the input sample x, let φShapley (i), φIG (i), φLRP (i),
φOcc(i) denote the attribution of the input variable i computed by the Shapley value, Input × Gradient,
LRP, and Occlusion, respectively. For Input × Gradient, LRP, and Occlusion, g(S) was computed
as the sum of attributions of input variables in S, as follows.
gIG(S) = X φIG(i), gLRP(S) = X φLRP(i), gOcc(S) = X φOcc(i)	(10)
In particular, due to the efficiency property of the Shapley value (Shapley, 1953), i.e. v(N) =
v(0) + Pi∈N φShapley(i), g(S) WaS computed as follows.
gShapley(S) = V(O) +): φShapleyQ	(II)
i∈S
In this way, we have gShapley (N) = v(O) + Pi∈N φShapley(i) = v(N).
• For the AOG explainer, given the input sample x, g(S) was computed as the sum of interaction
utilities of all patterns in S, as follows.
gAOG(S) = X I(L)	(12)
L⊆S
26
Under review as a conference paper at ICLR 2022
Table 6: Non-objectiveness Q) of attribution-based explanations When We used the normalized and
original attributions, repsectively.
Explanation method		TV news		census		bike	
		MLP-5	ResMLP-5	MLP-5	ResMLP-5	MLP-5	ResMLP-5
Input × Gradient	original normalized	-25373- 2.2418	22533- 3.9514	-78.2222- 26.6890	7.5902 6.2027	-57450- 4.5947	6.9758 6.2172
LRP	original	1.3558	7.9203	49.9572	114.5870	5.7450	152.1785
	normalized	2.2627	1.5283	28.2636	5.2428	4.5947	5.9722
Occlusion	original	2.7904	1.9336	63.8212	9.1191	9.1114	11.3653
	normalized	1.8655	0.9323	26.2608	4.9668	5.4882	6.0682
Ours		3.58×10-L	L93×10-7一	3∙68×10-6	1.16×10-6	1.12×10-6	1.54×10-6
----MLP-5 (normal)
----MLP-5 (adversarial)
----ResMLP-5 (normal)
----ResMLP-5 (adversarial)
----MLP-2 (normal)
----MLP-2 (adversarial)
——LightGBM
----XGBoost
on the TV News dataset	on the census dataset	on the bike dataset
0.0	0.4	0.8 Rk	0.0	0.4	0.8 Rk	0.0	0.4	0.8 Rk
(a)	(b)	(C)
Figure 9: The relationship betWeen Rk and the non-objectiveness ρnon-obj of the AOG explainer.
Then, We compared the non-objectiveness of the AOG explainer With the above seven baseline ex-
planation methods. Based on each tabular dataset, We computed the average ρnon-obj over the training
samples, i.e. Ex [ρnon-obj]given x. Table 5 shoWs that the AOG explainer exhibited significantly stronger
objectiveness than other explanation methods.
Besides, for fair comparison, We also normalized attribution values of Input × Gradient, LRP, and
Occlusion, in order to force them to satisfy the efficiency property. I.e. Φig(i) = P」IG*,
i∈N φIG (i)
~	. .	J. /八	_ ~	. .	J_ _ Z Z ∖	_ - .	.	.	__ ~ ..
φLRP ⑺=Pi∈N Φlrp(z) , and φOcc ⑶=Pi∈NccΦOcc(i) . In this way, we have V(N) = Pi∈N φIGc) = 1,
v(N)	=	i∈N φLRP(i)	= 1, and	v(N)	=	i∈N φOcc(i)	= 1. Therefore, in this case	g(S)	can be
calculated as follows.
gIG(S) = X φ∖G(i) , gLRP(S) = X φLRp(i^ COCC(S)= X φOcc(i	(13)
i∈S	i∈S	i∈S
Table 6 shows that the normalized attributions usually exhibited stronger objectiveness, compared
with their original attributions. Nevertheless, the AOG explainer still exhibited much stronger ob-
jectiveness compared with the normalized attributions.
F.2 The relationship between ratio of the explained utilities and the
OBJECTIVENESS
We further studied the relationship between the ratio of explained utilities Rk in Equation (4) and the
non-objectiveness ρnon-obj of the explanation. We averaged the Rk-ρnon-obj curve over the training sam-
ples in each tabular dataset. Figure 9 shows that the objectiveness of the AOG explainer increased
along with the increase of ratio of the explained utilities Rk .
G	More experimental results
G. 1 More visualization of AOGs
This section provides the visualization of more AOGs generated by our method on various datasets.
For tabular datasets, Figure 12, Figure 13, Figure 14, Figure 15, and Figure 16 show examples of
AOGs generated by our method on different models trained on the census, bike, and commercial
dataset, respectively. The up-arrow(f)/down-arrow(() labeled in terminal nodes indicated the actual
value of the input variable was greater than / smaller than the baseline value.
For NLP datasets, Figure 17 and Figure 18 show examples of AOGs generated by our method
on LSTMs and CNNs trained on the SST-2 dataset and the CoLA dataset. We discovered that in
27
Under review as a conference paper at ICLR 2022
0.8
0.4
0.0
(a)	(b)	(C)
----MLP-5 (normal)
----MLP-5 (adversarial)
----ResMLP-5 (normal)
----ResMLP-5 (adversarial)
----MLP-2 (normal)
----MLP-2 (adversarial)
——LightGBM
----XGBoost
Figure 10: (a) The relationship between the number of salient patterns k in the AOG and the ratio
of the explained utilities Rk, based on the census dataset. The relationship between Rk and (b) the
number of nodes, and (c) the number of edges in the AOG, based on the census dataset.
----MLP-5 (normal)
----MLP-5 (adversarial)
----ResMLP-5 (normal)
----ResMLP-5 (adversarial)
——LightGBM
----XGBoost
Figure 11: (a) The relationship between the number of salient patterns k in the AOG and the ratio
of the explained utilities Rk, based on the bike dataset. The relationship between Rk and (b) the
number of nodes, and (c) the number of edges in the AOG, based on the bike dataset.
LSTMs, single words were prone to be assigned a large utility to the model output. In comparison,
CNNs were prone to encode salient patterns that consisted of multiple words.
G. 2 More experimental results on the ratio of the explained utilities Rk
This section provides more experimental results on the relationship between the ratio of explained
utilities Rk and the AOG explainer.
Just like the experiment in Paragraph Ratio of the explained utilities, Section 4, we used salient
interaction patterns with top-k absolute values |I(S)| to approximately explain the model output.
Figure 10(a) and Figure 11(a) show the relationship between k and the ratio of explained utilities Rk
in different models, based on the census dataset and the bike dataset. We found that when we used a
few interaction patterns, we could explain most utilities of interaction patterns to the model output.
Figure 10(b,c) and Figure 11(b,c) show that the node number and the edge number increased along
with the increase of Rk .
Besides, Figure 10(a) and Figure 11 also show that compared with the normally trained model, we
could use less salient patterns (smaller k) to achieve the same ratio of the explained utilities Rk in
the adversarially trained model. Moreover, Figure 10(b,c) and 11(b,c) also show that AOGs corre-
sponding to adversarially trained models were less complex than AOGs corresponding to normally
trained models. This indicated that adversarial training made models encode more sparse interac-
tion patterns than normal training.
28
output g(N)=37.87, income < 50K, Rk=9S.7%
+
+29.43
-16.82
+10.62
+24.96
+37.49
-30.26
+28.29
+10.25
-19.42
+17.45
+24.95
+10.34
-10.62
+18.03
+15.62
+29.22
+16.35
-26.08
-18.76
+27.60
+11.84
+21.96
-28.29
+10.62
+20.57
+21.56
+28.10
-14.41
-13.62
-22.52
-16.35
+13.42 X+10.24	-51.96
+34.65	-15.06
-45.92 0-14.81
∖
education T
martial T
hours per week T
-21.69、+23.75
-18.06	-40.07
+14.80
-21.11
+16.82
+17.14
martial T
-58.8
work-
class T
relation-
ship ↑
educa-
tion T
education ↑
hours per week ↑
work class T
sex T
hours per
week T
capital
gain I
-23.60	-19.96	-21.56
-28.39	-15.31	-47.72
+14.01	-24.45	1-25.45
+27.21	-11.88	3+25.59
-10.71
education ↑
martial T
+15.31
	.8
occupation?
relationship?
-29.25
-13.73
a
output g(N)=13.32, income < 50K, Rk=99.5%
+
-5.94	+4.75
+12.77	+3.38
+3.32	+6.30
-7.72	-2.87
-2.87	+2.87
age X
martial T
age I
+4.76
	+23.86	
-7.72
-6.30
+7.72
-5.51
-2.79
+3.45
-3.38
-3.32
educa-
tion I
-3.91
+2.87
-20.22		+17.76
-23.86		-23.86
+5.51
-2.79	+23.86
	
-12.77/	-2.79
-3.32 f	+2.79
□ /	
+7.72	-4.75
-11.79
+5.51
-5.94
-4.75
+4.47
-4.47
-3.38
+2.79
+3.91	+6.30
+3.38
-12.77
-11.79
-3.45
+5.94
+4.75
+5.94
	/ Sa
+11.79	+2.79 0,	)+4.76	+12.77
+2.87
+3.91
+3.32	+2.87	-5.51
education J
relationship T
hours per week T
education J
hours per week T
-4.76
+11.79
+3.45
occupation J
sex T
-2.87
-3.91
martial T
relation-
ship ↑
race T
-3.45
+4.47
-2.79
-2.87
-4.47
S
education J
relationship T
race T
23.9
δ
Z
hours per
week T _23.9
Under review as a conference paper at ICLR 2022
output 2)=0.819, income < 50K, Rk=98.6%
-0.39
-0.07
hours per
week ↑
SeX ↑
output g(N)=0.443, income < 50K, Rk=99.8%
-0.03
+0.13
-0.01 - -0.01 /	.-0.06	+0.86	?-0.01	+0.03	-0.18
-0.04J^+0.08⅛j	<0.04	Zl-0.11	5<+0.01⅛	ζ+0.02	-0.01
+0.02'+0.0。	M -0.04 ⅛	∣-0.45	.+0.03 S	+0.03 /	0.86 I
age ↑
occupation I
educa-	occupa-
age ↑ tion I martial I tionI ship ↑
(a)	Examples of AOGs extracted from the MLP-2 network, adversarially trained on the census dataset.
output g(N)=2.943, income > 50K, Rk=99.2%
output g(N)=1.405, income > 50K, Rk=99.9%
+
+
+3.33	-0.06--0.10--0.07
-0.08
+0.26	-0.30¾-0.11 ∣+2.06	+0.08
-0.07 1+1.15	+0.06	+0.24
-0.44
-0.06
-0.08
+0.07
+0.08 = +0.09
-0.08 ⅞+0.08
+0.29^+0.09
+0.09	+0.09
-0.09 :-0.24
+0.10	-0.09
+2.50	+0.07
-0.42	-0.43
-0.08	-0.89
+0.23= +0.08
-0.24	∣+2.04
-0.07 S+0.24
-0.10
-0.10
+0.08
+0.09~+0.18~ -0.23	+0.12
-0.12 ∖+0.12⅞+0.12≥+0.15	+0.40
+0.89	-0.08	+0.43	-0.14	-0.24 / -0.18
+0.07	-0.09	-0.08	-0.07	+0.07	+0.09	-1.76	-0.08	+0.08
3.33
age ↑
education ↑
SeX ↑
education ↑
occupation I **»XIsX
education ↑
sex ↑
hours per week ↑
€
I
-3.33
-0.09 1+0.13 +0.10
education X
sex ↑
+0.09 W-0.08	+0.17	-1.10	-0.01	-0.11
第U濡on j↑	martial X
occupation ι
relationship ↑Upati	↑
2.50
-2.50
educa-	OCCUPa relation-	capital hours per
age ↑ tion ↑ martial I -tion X ship ↑ race ↑ sex ↑ gθin ↑	week^↑
educa-
tion X
occupa- relation-	capital hours per
martial X	tionp↑	ship ↑	sex ↑	gapn↑	WeekPT
(b)	Examples of AOGs extracted from the MLP-5 network, adversarially trained on the census dataset.
output g(N)=1.253, income > 50K, Rk=96.6%
output g(N)=1.617, income < 50K, Rk=99.9%
+
+
+1.70	-0.05
+0.13>0.28
+0.05	∣+0.68
+0.10
-0.11
-0.05
;-0.05≡ +0.06
+1.54	-0.06
+0.12	-0.67
-0.05
+0.07
-0.05
-0.06
-0.06
-0.28
-0.09	-0.10	+0.08
-0.07,-0.08	-0.13
+0.08,+0.11	+0.08
+1.96	-0.03
+0.16	1+0.38
+0.03	-0.47
+0.06 S +0.03, -0.04	-0.03 W +0.03
-0.08 ∣-1.84 +0.04 +0.05、-0.04
-0.03	-0.09	-0.40	-0.03 -0.33
-0.06	+0.06
+0.05 f+0.05
+0.05.-0.08
-0.05
-0.10
+0.05
-0.05	-0.08	+0.06	+0.05^-0.05	+0.08	-0.69	-0.06	+0.06
,	.	education I	education I	-,―+；-八个
education I	race ↑	SeX ↑	OCCuPation ↑
occupation ↑	capital gain ↑ +	capital gain ↑ +、八2FSitaergwieIek ↑
hours per week ↑ hours per week ↑	hours^erweek 1
1.70
educa- occupa- relation-	capital hours per
tion I tion ↑	ship ↑	race 1 sex 1 gain ↑	week ↑
-0.03	-0.05	+0.04	+0.03	-0.03	+0.05	+0.52	-0.04	+0.04
1.96
capital loss I
sex ↑
capital gain I
race ↑
capital loss I
___________ —«//	-1.96
educa-____occupa- relation-	capital capital
tion↑ martial ↑ tionp↑	ship I race ↑ sex ↑ gapn I loSS a country ↑
§
(c)	Examples of AOGs extracted from the ReSMLP-5 network, adversarially trained on the census dataset.
Figure 14:	Examples of AOGs extracted from models trained on the census dataset. Red edges
indicate the parse graph of a specific interaction pattern.
31
Under review as a conference paper at ICLR 2022
output g(N)=165, # bike rent is 165, Rk=96.4%
output g(N)=402, # bike rent is 402, Rk=99.3%
+11.58
-3.96	+9.95 - -4.63	+5.54
+14.16 -10.73 -24.99	-5.51
+4.22
-5.09
+8.97
-9.50	-6.74
-6.27
-5.97
-5.98
-7.60	+10.98
-102.91	+59.47	-12.85
+26.93	-14.39/-21.79
+18.17
+17.69
+18.99
-12.97
+72.76
+16.29
-31.94
-17.29
+4.63	-15.25 l+3.95	+10.81	-14.96	-4.39
+11.68
-7.36
+10.72	-8.48
-4.78	-7.10 N+5.67	-4.78	-4.74
temp ↑
atemp ↑
year I
humid-
ity I
wind-
speed ↑
humid-
ity I
+7.86	+19.53	-5.61	-5.69
35.47
-89.51
+26.68
+14.91	-69.58
+23.50Λ+45.42
+30.26
working-
day ↑
weather I
year I
temp ↑ ɔ
atemp ↑ ⅛,
wind-
speed ↑	-35.47
-26.98
+12.83X -40.31
+12.91^+12.13
-29.36
+15.97
^^411
working- weath-	humid- wind-
year l	day ↑	erl	temp ↑ atemp ↑ ity I speed ↑
hoyr ↑	temp ↑
working-	atemp ↑ atemp ↑
day J atemp 1 humidity J
atemp ↑	t
humidity J huι
§
-411
year I	hour ↑	WoayilIg-	temp ↑ atemp ↑	埠y1f-
+
+
J
(a)	Examples of AOGs extracted from the MLP-5 network, adversarially trained on the bike dataset.
output g(N)=148, # bike rent is 148, Rk=99.1%
+
output g(N)=5, # bike rent is 5, Rk=99.4%
+
-212.3	+6.50	-34.72	+7.95j+13.01
+51.46 -59.92 +37.73 +200.9 +12.75
+8.91	-73.11	-6.41	-40.42 +61.71
-7.23 g+12.44
+17.05 -15.71
+7.95	-53.63
+25.01
+15.75
-22.27
-31.95
-22.37
+35.87
-17.14
+50.21
+24.69
-4.67
-5.01
+7.20	-6.11
+5.02
+4.45	+3.37
+10.52
+10.92'-21.89∙-14.93	-9.78 --9.07	+22.44 -95.55 +14.07P-15.70
-2.47
-3.63
-3.30
hour↑	year I
working- humid-
day I	ity ↑
year I
temp ↑	temp ↑
atemp ↑	atemp I
humidity ↑
212
year ↑
weather I
humidity ↑
-212
year I hour ↑	Working-	temp ↑	atemp ↑	humid-
year ↑ day I hour I
+4.08
+2.39
-2.65
-51.7
-4.01	1+4.89	≡ -2.53	-3.14	+6.46
-37.52	<+3.32	+3.73 1	+ +6.45	-11.12
-6.80	+4.78	1-5.44 ≡	+5.00	-6.35
+2.86	J-4.91	+36.31	S +3.54	-3.68
				184
Wondafe	Windayeed，§
~~~ ΠR⅛⅛X. I I
-184
working-
day ↑
weath-
er I
humid- wind-
ity ↑ speed I
§
(b)	Examples of AOGs extracted from the ResMLP-5 network, adversarially trained on the bike dataset.
Figure 15:	Examples of AOGs extracted from models trained on the bike dataset. Red edges indicate
the parse graph of a specific interaction pattern.
32
Under review as a conference paper at ICLR 2022
output g(N) =0.295, is commercial, Rj(=98.8%
output g(N)=0.817, is commercial, Rk=96.9%
+0.01	+0.73[	二+0.01」	-0.01	-0.02 ≡≡	t-0.01 .^-0.01		+0.01
-0.01 J	《-0.04 e	K-0.20	-0.01	+0.09	×+0.01 V	+0.03)	g -0.01
+0.01」	.-0.01	<+0.01	,-0.01*5^-0.01 -	--0.45	-0.01 K	夕+0.01
+0.12
-0.02
-0.02
-0.03
-0.01
+0.11	+0.08
frame
difference I
fundamental
frequency ↑
shot length I	motion ↑
frame	frame
difference I difference I
fundamental fundamental
frequency ↑	frequency ↑
shot length I
motion ↑
frame
difference I
fundamental
frequency ↑
motion ↑
frame
difference I
fundamental
frequency ↑
edge change
ratio I
-0.02
+0.02	-0.05
+0.01
0.73
frame difference I
edge change ratio ↑
-0.03
00.54
shot	frame	sho?	SpeCtlraI fundamental	e%eα
length I motion ↑ difference Ientemy ↑ flux ↑	frequency ↑	Craaioge
-0.73
shot
length I
-0.54
/	frame	short zero	Ifundaj edge
motion I	differ-	time	crossing	mental	change
ence I energy ↑ rate I	frequen-	ratio ↑
(a)	Examples of AOGs extracted from the MLP-2 network, adversarially trained on the commercial
dataset.
output g(N)=0.200, is commercial, Rk=97.8%
output g(N)=0.649, is commercial, Rk=95.9%
+
+0.07	∣+1.18	+0.05.-0.06
-0.40	-0.56	-0.12 j-0.09	+0.05
-0.05	-0.58
-0.05
motion I
frame difference ↑
short time energy I
fundamental frequency ↑
+0.05 7 +0.07	-0.06	-0.10
+1.04
+0.69
-0.06	+0.08 夕+0.05/-0.09
shot
length I
I/— frame
motion ↑ differ-
1.18
motion I
fundamental frequency ↑
+0.08/+0.05	+0.08
fundamental
frequency ↑
short	zero	.
_____	time crossing SpeCtral
encel	energy I rate ↑ flux J
+0.66^^0.02
+0.03	+0.03
-0.02
-0.01
-0.02
-0.66
motion ↑
shot length J
edge change ratio ↑
frame short	zero	spectral
lenhoh J motion ↑ differ-	time crossing	SfleCt↑al
length J	ence ↑ energy ↑ rate ↑	flux 1
frame difference ↑
zero crossing rate ↑
edge
change
ratio ↑
(b)	Examples of AOGs extracted from the MLP-5 network, adversarially trained on the commercial
dataset.
output g(N)=2.916, not commercial, Rk=94.1%
+0.08
-0.08
+0.18
-0.08
-0.09
-0.19
+0.11^s-0.07
+0.30	+0.28
+3.28	-0.07
-0.35
-0.09
short time
frame
3.28
output g(N)=0.786, is commercial, Rk=98.3%
+0.02-+0.02	+0.02;+0.02	∣+1.10
-0.02	+0.04	-0.03
+0.02、+0.02--0.08	+0.08	-0.04
energy 1	difference I
zero CroSSingJlIlllllspectral
rate ↑	centroid ↑
motion I
frame
difference I
frame
difference 1
-0.03	+0.02	-0.02	-0.35--0.02
∕/
+0.02
/ / 1.10
spectral
flux ↑
motion ↑
SPectral，
flux ↑
motion ↑
frame
difference 1
-3.28
frame
length ↑ motion 1 f
short
time
energy I
zero
spectral
CrOSSi↑g	CeInectrdl↑
∣-r frame
motion ↑ differ-
ence 1
short
time	SpeCtIral
energy ↑	flux ↑
funda-
mental
frequen-
cy ↑
edge
change
ratio 1
-1.10
+
(c)	Examples of AOGs extracted from the ResMLP-5 network, adversarially trained on the commercial
dataset.
Figure 16:	Examples of AOGs extracted from models trained on the commercial dataset. Red edges
indicate the parse graph of a specific interaction pattern.
33
Under review as a conference paper at ICLR 2022
output g(N)=29.25, positive sentiment, Rk=94.1%
+2.54	-0.97
+3.27	-2.87
that could
-2.54≤]+4.49
+2.87	∣+9.92
fumbled
lesser
output g(N)=24.07, positive sentiment, Rk=95.7%
so easily have been fumbled by a
-3.45
1+2.11
lesser
-2.83
-2.38
-	19.6
film- -19.6
maker
You
+7.75	-1.50
+1.91	+1.80
need
stronger
stomach
+3.68	-3.32
+2.21	+0.65
stomach
than
'll need a stronger stomach than us
7.76
π
ε
-7.76
(a)	Examples of AOGs extracted from the CNN network, trained on the SST-2 dataset.
output g(N)=11.57, positive sentiment, Rk=94.4%
output g(N)=17.98, positive sentiment, Rk=99.4%
+
-5.75	+3.19	+1.69
+1.50	-2.45	-1.66
-1.45	+4.00	-1.71
-1.76
-1.48
+3.12
+3.98
-1.67
-1.43
+1.41
+4.46
+1.96
+1.86	∣+3.65	∣+7.71	+2.48
the	the
good	actors
-1.50	-2.68/-1.47
simply
the	actors	are	simply	too	good
9.44
I
€
U
-9.44
-0.92
-1.25
+1.54
-2.76
-0.51
+1.85
+0.52
+0.87
+5.93
-0.75
do
well
-0.88
do
well
check
do well
to
+2.31
+3.05
+1.80
do
to
check
+1.15	+0.51	+0.52
+0.55
-4.01
well
check
one
+2.43	+0.75
+0.96	+0.97
5.93
this
one	X
this one out
-5.93
(b)	Examples of AOGs extracted from the LSTM network, trained on the SST-2 dataset.
Figure 17:	Examples of AOGs extracted from models trained on the SST-2 dataset. Red edges
indicate the parse graph of the most salient interaction pattern.
output g(N)=15.62, grammatically correct, Rk=98.1%
output g(N)=13.19, grammatically wrong, Rk=95.7%
+
+2.84	+0.34
-0.22	+0.20
+0.31
-0.43
+1.14
-0.30
Almost	no	Almost
lawyer	lawyer	could
Almost no lawyer could
answer that question
+0.65	+0.23	+0.25
+0.37:-0.23∕-0.63
+0.21	-0.37	+0.20j
lawyer	that
could	question
+0.97	+0.83	+0.41	+0.16
+0.21 ∣-0.36	-0.19	∣+0.72
∣+1.92	+0.32	+0.36 1.95
could	§
found
-1.95
could be found	.
(a)	Examples of AOGs extracted from the CNN network, trained on the CoLA dataset.
output g(N)=12.65, grammatically correct, Rk=90%
-0.38
-0.27
output g(N)=11.88, grammatically wrong, Rk=98.1%
+
+8.49	+2.06
-0.43
+0.92.-0.35
-0.43
found
-25.31
(b)	Examples of AOGs extracted from the LSTM network, trained on the CoLA dataset.
Figure 18:	Examples of AOGs extracted from models trained on the CoLA dataset. Red edges
indicate the parse graph of the most salient interaction pattern.
34
Under review as a conference paper at ICLR 2022
----MLP-5 (normal)	---- ReSMLP-5 (normal)	----MLP-2 (normal)	----- LightGBM
----MLP-5 (adversarial)--------ResMLP-5 (adversarial)---------MLP-2 (adversarial) --------XGBoost
lossep
SMoN Λ!"W UO
5l5lc,
2 1
S①POU JO #
0.0	0.4	0.8 Qk
lossep
snsuoɔ。一∣ι UO
城6.0
(υ
J 4.0
'+-<
o
Φ
穹2∙0
° 0.0
0	2000	4000 k 0.0	0.4	0.8 Qk	0.0	0.4	0.8 Qk	0.0	0.4	0.8 Qk
Figure 19: (1) The first column shows the relationship between the number of salient patterns k in the
AOG and the ratio of the explained utilities Qk, based on different datasets. (2) The second column
shows the relationship between Qk and the non-objectiveness ρnon-obj of the AOG explainer, based on
different datasets. (3) The third column and the fourth column show the relationship between Qk
and the node number, and the edge number in the AOG, respectively.
IoSEeP
o-q ①W UO
H	Another metric for the ratio of the explained utilities
In this section, we provide another metric for the ratio of the explained utilities. As is discussed in
Section 3.3, we only used salient patterns with the top-k largest absolute values |I(S)|, denoted by
Ωsopent,to approximately explain the output of the deep model. The ratio of the explained interaction
utilities could also be quantified as follows.
Qk
Es∈ω疆 II(S)|
Ps⊆n |I(S )I
(14)
where we used Ps∈Ωsalient II(S)I to quantify the explained interaction utilities in the AOG explainer,
∈"top-k
while PS⊆N |I(S)| represented all interaction utilities encoded by the deep model.
We also studied the relationship between the number of salient patterns k and Qk, the relationship
between Qk and the non-objectiveness of the AOG explainer, and the relationship between Qk and
the AOG complexity. Figure 19 show that the ratio of explained utilities Qk increased along with the
increase of the number of salient patterns k. Besides, the objectiveness of the AOG explainer was
boosted along with the increase of Qk. Moreover, the number of nodes and the number of edges in
the AOG also increased along with the increase of Qk .
I More analysis on the effectivenes s of the learned baseline
VALUES
This section provides more experimental analysis on the effects of baseline values on the conciseness
of explanations. Beyond experiments in the Paragraph Effects of baseline values on the conciseness
of explanations, Section 4, in this section, we analyzed the effectiveness of the learned baseline
values in terms of the AOG complexity from different perspectives. To this end, we first computed
interaction utilities using baseline values obtained in different epochs during the learning phase.
Then, based on the computed interaction utilities, we measured the number of salient patterns, the
node number, and the edge number in the AOG at each learning epoch. For fair comparison, we
selected the minimum number k of salient patterns such that the ratio of the explained utilities
Qk exceeded 70%, i.e. k = arg mink0 {k0 : Qk0 > 70%}, to construct the AOG. Figure 20 shows
the change of the AOG complexity during the learning process of baseline values, in terms of the
number of salient patterns, the number of nodes, and the number of edges in the AOG. We found
35
Under review as a conference paper at ICLR 2022
10ssep SMON Λ!UO IOSEeP Snsuoo ①上一 UO
350
I250
2 150
o
⅛
50
1200
8 800
ŋ
ι
ɔ 400
----MLP-5 (normal)	----ResMLP-5 (normal)
----MLP-5 (adversarial)-------ResMLP-5 (adversarial)
350
0	10	20	30	40	50
epoch
50
5 5
2 1
səpou Jo #
0	10	20	30	40	50
sə-əjo #
----MLP-2 (normal)
----MLP-2 (adversarial)
1800
1400
1000
600
200
0	10	20	30	40	50
epoch
0∙.......................
0	10	20	30	40	50
epoch
Ooo
Ooo
2 8 4
səpou Jo #
0.......................
0	10	20	30	40	50
epoch
Ooo
Ooo
Ooo
6 4 2
səəjo #
0
0	10	20	30	40	50
epoch
Figure 20: The number of patterns (the first column), nodes (the second column), and edges (the
third column) in the AOG, based on baseline values of different learning epochs. The learned base-
line value significantly enhanced the conciseness of explanations.
that the learning of baseline values significantly simplified the AOG, thus boosting the conciseness
of explanations.
36