Under review as a conference paper at ICLR 2022
A partial theory of Wide Neural Networks us-
ing WC functions and its practical implica-
TIONS
Anonymous authors
Paper under double-blind review
Ab stract
We present a framework based on the theory of Polyak-Lojasiewicz functions to
explain the properties of convergence and generalization of overparameterized
feed-forward neural networks. We introduce the class of Well-Conditioned (WC)
reparameterizations, which are closed under composition and preserve the class of
Polyak-Lojasiewicz functions, thus enabling compositionality of the framework
results which can be studied separately for each layer and in an architecture-neutral
way. We show that overparameterized neural layers are WC and can therefore be
composed to build easily optimizable functions. We expose a pointwise stability
bound implying that overparameterization in WC models leads to a tighter con-
vergence around a global minimizer. Our framework allows to derive quantitative
estimates for the terms that govern the optimization process of neural networks.
We leverage this aspect to empirically evaluate the predictions set forth by some
relevant published theories concerning conditioning, training speed, and general-
ization of the neural networks training process. Our contribution aims to encourage
the development of mixed theoretical-practical approaches, where the properties
postulated by the theory can also find empirical confirmation.
1	Introduction
A staggering aspect about neural networks is that they are seemingly able to overfit the training
sample and yet generalize to unseen data. Such a behaviour has been actually observed in other
learning paradigms in overparameterized settings, such as with linear regression (Bartlett et al., 2020)
and kernel methods (Belkin et al., 2018; Tsigler & Bartlett, 2020), but neural networks are certainly
the model characterized by the most surprising predictive performance on real-world data.
Several recent works (Schoenholz et al., 2017; Jacot et al., 2018; Du et al., 2019; Yang et al., 2020b)
have attempted to explain the theoretical underpinnings of why neural networks learn and generalize.
Inspired by infinitely wide neural networks (Jacot et al., 2018; Lee et al., 2019), much of this research
has focused on developing a theory for very wide networks, i.e. where the number of parameters
m = Θ(nα) is polynomially bigger than the number of examples n1(Du et al., 2018; Allen-Zhu
et al., 2019), and more recently restricting to overparameterized networks (Li et al., 2018; Oymak &
Soltanolkotabi, 2020), i.e. where m > n. While these theoretical results are being obtained under
increasingly milder (and hence more realistic) overparametrization assumptions, they are typically
expressed only in terms of asymptotic rates. As such it is difficult to determine whether these results
hold in practice out of the box of the theoretical models and thus if they can explain the behaviour of
real-world neural networks.
Another line of research has focused on strongly experimental work (Lee et al., 2020). However
their work is directed to obtain empirical suggestions for practitioners, and does not try to measure
quantities that could further inform other theories. 1
1This is a condition that is not usually satisfied in actual networks. Consider, for instance, VGG19 which
has 144M parameters distributed on 19 layers and was trained on 1.3M images: already a quadratic polynomial
would require an order of 1 trillion parameters per layer for VGG, which is clearly unattainable for current
practice. On the other hand VGG19 is mildly overparameterized, i.e. the number of parameters per layer is
slightly greater than the number of used images.
1
Under review as a conference paper at ICLR 2022
The reasoning underpinning this work is that an ideal theory for neural networks must be applicable
to state-of-the-art architectures, must be tested quantitatively for its adherence to reality, must be
able to give both convergence2 and generalization3 guarantees, and should decouple the network
models from their optimization. As a first step towards this aim, we investigate a new practical
theory of convergence and generalization of overparameterized neural networks, that provides some
quantitative estimates for the terms that govern the networks’ optimization process. The theoretical
model’s predictions are later compared to the experimentally measured quantities to check its
explanatory power. Advantages of the proposed theory concern its adaptability to different feed-
forward architectures (due to the generality of Polyak-Eojasiewicz functions), a clear separation
between model and optimization, and its testability, which allows it to be disproved experimentally.
An important novelty in our paper concerns the exposition of a “partial” theory and the novel role
of the experimental results. In fact, as the reader will later observe in Section 5 and Section 6, we
expose arguments in favour of the fact that wide layers are well-conditioned (a term that will be
introduced later) during at least part of their training. Such arguments can become a proof if we
are willing to impose the additional restriction of layer width being polynomially bigger than the
number of examples. Instead of focusing on what can be proven, we choose to test if the reasoning
that underpins those theories does hold in practice in mildly overparameterized networks. We think
that shifting the focus to identifying discrepancies between existing theories and experiments on the
realistic neural architectures can result in tighter feedback loops, where experiments can inform the
development of new theoretical hypothesis that are more aligned with the reality of the neural models.
Our framework is based on the theory of Polyak-Eojasiewicz (PL) functions (Polyak, 1963) as they
provide a simple and general condition to prove convergence of an objective function to a global
minimum via first-order methods (Karimi et al., 2016; Csiba & Richtdrik, 2017; Guille-Escuret
et al., 2021). In addition, Charles & Papailiopoulos (2018) provide stability bounds for PL functions,
allowing us to formulate bounds on the generalization of a neural network based on the amount of
optimization performed leveraging the stability approach of Elisseeff et al. (2005).
1.1	Our contribution
Starting from these results, in Section 2 we extend those on PL functions (Csiba & Richtdrik, 2017;
Guille-Escuret et al., 2021) to deal with different amounts of sensitivity in each layer rather than being
constrained to consider a single parameter for the whole network. In Section 4 we introduce the class
of Well-Conditioned (WC) reparameterizations, which are closed under composition and preserve
the class of Polyak-Eojasiewicz functions, and which we believe could be of independent interest.
In this way we provide a simple framework that cleanly separates the model from the optimization
process and from the generalization bounds. By providing theoretical clues (and later, via empirical
measurements) we show that overparameterized neural layers are WC functions in Section 5.
We thus establish a first approach to compute quantitative estimates of important quantities in network
optimization. We show how such estimates can characterize the training progess of real feed-forward
neural models providing, in Section 6, an empirical analysis that compares the predictions of the
theory concerning conditioning, convergence and generalization with the observed behaviour of
trained models. The empirical analysis enables us to spot phenomena that cannot be explained by
existing theories, for mildly overparameterized networks. We believe that this feature provides the
fundamentals for directing theoretical research towards uncovering unexplained aspects of real-world
neural architectures.
1.2	Differences with other works
The use of Polyak-Eojasiewicz functions to study convergence and behaviour of neural networks
has been introduced only recently by few literature works. Very recently, Liu et al. (2020) have
leveraged PL functions to obtain asymptotic rates of convergence. This approach, however, needs to
consider the Hessian of the whole network, and focuses on explaining the behaviour of the Neural
Tangent Kernel (NTK) Jacot et al. (2018), while our WC functions provide a theory amenable to
be applied on a layer-by-layer basis without needing to consider the network as an indissoluble
2i.e. it should prove that the training on sample data will converge to a global solution.
3i.e. it should prove that the found solution will generalize well to fresh data from the same distribution.
2
Under review as a conference paper at ICLR 2022
entity. Moreover our results hold for different convex losses, and we provide quantitative convergence
rates and we empirically validate them, including also an analysis of the generalization estimates of
the trained models. As for what concerns the empirical analysis on the properties of wide neural
networks, Lee et al. (2020) recently performed a large-scale experiment to study the performance
of finite-width networks compared with their infinite limits, as predicted by the NTK theory. Our
approach differs from Lee et al. (2020) in that we focus on fine-grained (i.e. at the level of single
optimization steps) adherence of the empirical behaviour to the theory presented in this paper, while
Lee et al. (2020) restricts to comparing the final outcomes of the optimization process for various
finite- and infinite-width models, and thus serves mainly to guide empirical practitioners.
2	POLYAK-匕OJASIEWICZ CONDITION
In this section we introduce the background on PL functions that we use throughout the paper. We
provide a small generalization of the PL condition which will be later used in the analysis of the
optimization of a multilayered neural network, and we confirm known convergence results (Karimi
et al., 2016) in this new setting.
We remark that the presented theory concerning convergence speeds can be seamless extended
to the two-sided PL setting of Yang et al. (2020a), thereby extending its applications to minimax
optimization problems, such as those present in Generative Adversarial Networks Goodfellow et al.
(2014) optimization. For readability purposed we expose here only the basic theory.
We consider multivariate functions f : Qi∈I Xi → R, where Xi are Banach spaces and I is
finite. We will denote X = Qi Xi , x ∈ X and will write xi ∈ Xi for the i-th component of x;
given a function f : X → R We will denote by Dfx : X → R its Frechet differential at x, by
Difx : Xi → R its partial differential, by IlDfxk the operator norm, and by Vf (x) ∈ X * is the dual
vector corresponding to Dfx; the dependency of the quantities μi and Li on points in the function
domain Ω is omittied for notational simplicity; moreover we denote μ the vector of (μi)i∈ι.
Definition 1 (PL Condition). Given afunction f : "分 Xi → R we say that f is μ-PL on Ω if
∀X ∈ Ω 1 X ɪ IDifxk2 ≥ f(x)- f *	(1)
2 V μi
where f * ：= infx∈Ω f(x).
The Polyak-Lojasiewicz condition basically states that the norm of the gradient at a point controls
the minimality gap at the current point, and thus for this class of functions necessarily Vf (x) = 0
implies that X is a global optimum in Ω.
Definition 2 (Smoothness). Given a function f : i Xi → R we say that f is L-smooth iff
∀x,y ∈ YXi f (X)- f (y) ≤ X Dify[xi - yi] + Lkxi - y∕∣2.
PL functions enjoy the useful property of exponential convergence to a point of minimum value via
common first-order otimization methods (Karimi et al., 2016; Csiba & Richtarik, 2017; Guille-Escuret
et al., 2021). In this work we only consider minimization via gradient descent, but the extension to
other algorithms is standard and can be found in the cited papers. All the proofs of this Section are
reported in Appendix A.
Lemma 1 (Convergence speed and radius for PL functions). Let f : Qi Xi → R be μ-PL and
L-smooth; the Xi be Hilbert spaces. Choose an initial point x0 and let the sequence of iterates
evolve according to the rule
χk+1 = Xk- ɪVif (xk).	⑵
Li
Letting Y := 1 一 mini μi, the optimality gap decreases exponentially following theformula
Li
f (xk+1) - f* ≤ Y(f(xk) - f*).
Moreover, the distance from the initial point is bounded by
X ：Bk+1 - X0U ≤ t 2(f (x0) - f *) (X LJ 1-√γ
3
Under review as a conference paper at ICLR 2022
Additionally, PL functions theory encompasses convex optimization because of the following lemma.
Lemma 2 (Strongly convex functions are PL). Let f : X → R be a τ -strongly convex function. Then
f is T-PL on every set Ω ⊆ X, i.e. ∀x ∈ Ω ∣∣Vf (x)k2 ≥ T (f(x) 一 fΩ) where fΩ := infχ∈Ω f(x).
3 Stability results FOR POLYAK-匕OJASIEWICZ functions
In this section we recall the notion of stability and we introduce a result by Charles & Papailiopoulos
(2018) that provides stability bounds with a PL empirical risk, which leads to a generalization bound.
We briefly recall standard notation from Elisseeff et al. (2005): given a labeled dataset S =
{zi = (xi, yi) | i = 1, . . . , n} with examples sampled i.i.d. from a distribution D and a learning
algorithm A, let wS := A(S) the algorithm’s output on S4. Let ` be the loss function, and f the
considered model. Then the empirical training error is defined by
Rs(W) = ∣S∣ X '(f(w,χ),χ,y).
(x,y)∈S
Generalization bounds can be obtained using the notion of pointwise hypothesis stability as follows.
Definition 3 (Pointwise stability, Elisseeff et al. (2005)). An algorithm A has pointwise hypothesis
stability βn with respect to the loss function ` iff:
∀i ∈{1,...,m} ES 〜Dn,z 〜D [∣'(fs, Zi)- '(fsi,z ,Zi)∣] ≤ βn
where fs := f(ws, ∙) and Si,z = {z1,z2,...,zi-1,z, Zi+ι, ...,Zn}.
Lemma 3 (Generalization from pointwise stability, Elisseeff et al. (2005)). Suppose A has pointwise
stability β with respect to ` where 0 ≤ `(w, Z) ≤ M. Then for any δ, with probability at least 1 - δ
we have:

∣RD (wS) - RS(wS)∣ ≤
M2 + 12M nβ
2nδ
To connect stability theory to PL models we make use of a result by Charles & Papailiopoulos (2018).
In What follows We will denote by WS a global minimizer towards which the algorithm is converging.
Lemma 4 (RS PL implies pointwise stability, (Charles & Papailiopoulos, 2018, Theorem 3)).
Suppose thatfor every dataset S we have RS is μ-PL, ∣Rs(WS) — RS(WS)∣ ≤ εA and that' ◦ f is
G-lipschitz with respect to W, then A has pointwise stability bounded by
βptw
L ∕2g2
≤ 2√εAv T +
1	2G2
n — 1 μ
Notice that while the second term is always present, the first one depends on the amount of optimiza-
tion performed by the algorithm, which vanishes for a perfect fitting of the training data, and which
suggests that for PL models more overfitting implies more generalization.
The presence of the quantity 2G2- hints that the bigger the PL constant of the model, the better it is
both for generalization and optimization. Moreover it is better if deviations of the model are small
with respect to its parameters and inputs5.
4 Well- Conditioned functions
In this Section we introduce the class of Well-Conditioned (WC) functions, that are closed under
composition, and whose composition with a PL function generates another PL function. They provide
the foundations for our theory since, under certain conditions, neural layers are WC (Section 5).
Let X, Y be Banach spaces, and let T : X → Y be a linear functional; denote its range by
R(T) ⊆ Y , and define the preimage of a point y as S(T, y) = {x ∈ X ∣ Tx = y}. The definition of
a WC function is the following.
4Correponding to neural networks weights in our case.
5Lemma 4 asks for a small lipschitz coefficient, and Lemma 1 asks for a small smoothness coefficient.
4
Under review as a conference paper at ICLR 2022
Definition 4 (WC function). Let a separately differentiable function f :	i Xi → Y be given. We
say that f is λ-WC on a domain Ω ⊆ 口分 Xi iff
∀ω ∈ Ω ∀i ∀y ∈ R(Difω)	inf %|山||二|团|.
xi∈S(Difω,y)
The condition can be understood as a bound on the minimum norm solution x of the inverse problem
Tx = y where T := Difx is the partial differential of the function at a given point. In this way we
essentially bound from below the norm of the differential of f, and by chain rule we can bound from
below the PL coefficient of the composition, obtaining the following lemmas (proofs in Appendix A).
Lemma 5 (Composition of a PL function with WC is PL). Let g : Y → R be μ-PL over Ω ⊆ Y and
f :	i Xi → Y be λ-WC over i Mi ⊆ i Xi and suppose they satisfy the inequality
X Dgf(x)|R(Difx)o2p ≥ αkDg(f(x))ko2p	(3)
i
and f (Qi Mi) = Ω .Then h( *) = g(f (*)) is (μαλ2) -PL over Qi Mi and h = gΩ.
Remark 1. Condition (3) in Lemma 5 is satisfied in particular if there exists one variable xi for
which Di fx is a dense subspace ofY for all x6.
Lemma 6 (Composition of a WC function with WC is WC). Let g : Qi Yi → Z be (,i λi)-WC and
fi :	j Xij → Yi be ( ,ij μij) -WC. Then their composition h : Hij Xij → Z defined by
h(,ijxij) = g(,i fi(,j xij))
is (,ij λiμij)-WC. By f(,n=ι Xi) we mean f(xι,...,xn).
Thus if we have a set of WC functions f1 , . . . , fL satisfying condition (3), we can compose them
together with a convex function g, to obtain a PL function h = g ◦ f1 ◦ . . . ◦ fL .
5 Wide neural network layers are Well-Conditioned
We now present an argument by Agarwal et al. (2020) that proves that conditioning at initialization
improves exponentially with depth; we later show that it is possible to bound conditioning in a region
of small distance from the initialization, and thus to prove that overparameterized layers of neural
networks are WC functions7 . We start the exposition giving a few definitions.
Definition 5 (Inverse Norm of a linear operator). Given a linear operator T : X → Y define its
inverse norm by
Z(T) := SuX χ0∈Si%χ) k⅛
which coincides with the smallest λ for which T is λ-WC.
In order for the results to generalize over various architectures, we consider a layer as composed of
a linear combination of a fixed non-linear part, toghether with some fixed Mij that determines the
available connections between inputs and outputs8.
Definition 6 (Representation of a layer). Let A = Rm,k, X = Rk and Y = Rm, Mij ∈ Rm,k and
φ : R → R be a fixed function. Then define the function fM : A × Xn → Yn by
fM (α, x)iν =	αij Mij φ(xjν).
j
We will now relate the WC coefficient of a neural network layer to its conditioning, which will allow
us to later measure WC coefficients. Then we expose the arguments by Agarwal et al. (2020) that
clarify why it is expected that conditioning (and WC coefficient) improves with depth.
In this context we only consider conditioning of a linear operator with respect to the euclidean norm;
thus conditioning is the ratio between the highest and the lowest singular values of the operator.
6This is typically the case if a single layer has more neurons than the number of available datapoints.
7What we actually prove is that overparameterized layers of neural networks are very often (with respect to
the randomness in their initialization) WC functions near their initialization.
8For FCN we will have ∀i, j Mij = 1; for CNN they will be set accordingly to the kernel size and stride.
5
Under review as a conference paper at ICLR 2022
WC coefficients. A neural layer has two WC coefficients: one associated to the layer parameters
and one bound to its inputs, which allows us to chain WC coefficients using Lemma 6.
Parameter WC coefficient. The linear operator of interest is T = Dα fM (α, x) and by Definition 5
We want to compute Z(T), since WC(T) = 1 /Z(T). Definition 5 can be cast as the following problem:
*
suppose that we are given a fixed y ∈ Yn which is in the range of /m (∙,χ); We want to solve
*
min kαk2 subject to fM (α, x ) = y .
α2
(4)
To solve it, define the matrix (Fx)ki,i0j = δii0 Mi0 j φ(xjk) so that yik = Pi0j (Fx)ki,i0j αi0j and by the
theory of minimum norm solutions the solution to Equation (4) is given by α = FxT(FxFxT)-1y.
Thence Z(DafM(α, x)) = λmin(FχFT)-1/2, which gives us a way to compute the WC coefficient
and shows our interest in the eigenvalues of the Gram matrix Hx := FxFxT.
Bounding the eigenvalues. We are able to say that conditioning of the Gram matrix Hx does get
exponentially better with depth, using arguments by Agarwal et al. (2020); Daniely et al. (2016).
Consider a network with activation function σ and assume inputs xi have unitary norm and that their
products |hxi, xji| ≤ 1 - δ are bounded by one. Let us also define the dual activation9
σ(P) = E(X,Y)〜Σρ [σ(X)σ(Y)] where ςP=(P P
Assume that the initial weights are distributed accordingly to a gaussian distribution that preserves
the forward variance, and consider each layer gram matrix HiLj = xiL , xjL , where xiL is the i-th
input to the L-th layer. Given the definition of σ, it is clear that HL+1 = σ(HL), where σ is applied
to each entry of the previous layer matrix. Using the following lemma iteratively it is then possible to
derive bounds on the smallest eigenvalues of the gram matrix HL at initialization.
Lemma 7 (Eigenvalue lower bound lemma (Agarwal et al., 2020, Lemma 23)). Let H ∈ Rn×n a
positive-definite matrix, i.e. H ≥ δIn and all values = 1 on the diagonal. Let f : R → R be an
analytic function whose series expansion in zero has only positive coefficients, and let f [H] be the
application of f to each entry of the matrix. Then f[H] ≥ (f (1) - f(1 - δ))In.
It is also possible to derive a bound on the non-diagonal entries of the gram matrix, provided that
we can bound the amount of distortion that σ can induce: Mδ := SuP {σ(ρ) | |p| ≤ 1 - δ}. This
estimate provides a way to bound the highest eigenvalue via Gershgorin Circle Theorems; and when
combined with Lemma 7 it shows that conditioning improves with depth. For more details we refer
the reader to Agarwal et al. (2020) and Daniely et al. (2016).
Input WC coefficient. Analogously to the parameters WC coefficients, we have Z(DxfM (α, x)) =
λmin(RχRT)-1/2 where the matrix (Rx)`i,ki = 6皿切Mijφ,(χj) is the linear relation between the
derivatives and the change in Yn .
It would be ideal to also prove that finite neural networks are very often well-conditioned at initializa-
tion. To do this one could use the fact that the matrix norm between the mean of Hx and the Hx of a
single random sample depends weakly on a large number of variables (the α coefficients), so that
McDiarmid (1989) inequality can be used to limit the two matrices’ distance in operator norm with
high probability the bound transfers to a bound on the smallest eigenvalue via the following lemma.
Lemma 8 (Inverse Norm出ound). Let T,T : X → Y be two linear operators with the same range,
i.e. such that R(T) = R(T) and satisfying Z(T) ∣∣T — T∣∣。口 < 1. Then we have
Z(T) ≤
Z(T) _
1 - Z (T )∣∣T - T∣∣ op
The proof of Lemma 8, a generalization of some results by Ma (2012), is reported in Appendix C.
9Notice that in the definition of the dual activation the mean over all possible network initializations is taken,
which does correspond to the setting of “infinitely-wide” neural networks (Jacot et al., 2018) due to central limit
theorems. In the experimental section we will analyze how these predictions change on finite networks.
6
Under review as a conference paper at ICLR 2022
As mentioned in the introduction, we provide only proof ideas for this part since similar reasonings
can be found in many theoretical papers about very wide or infinite neural networks (Allen-Zhu et al.,
2018; Du et al., 2019; Zou et al., 2020), and we do later experimentally measure WC coefficients
of real networks at each epoch, thus ensuring that the tested networks are WC functions during the
whole training. This also enables us to test whether the reasoning in proofs for very wide networks is
able to explain the phenomena of good conditioning which is a shared similarity of very wide and
mildly overparameterized networks.
We now state a lemma that allows us to prove well-conditioning near initialization if we know that
the layer is well-conditioned at initialization, a result that holds for the whole training in the case
of very wide neural networks due to the fact that wide networks’ coefficients move very little from
initialization. The lemma infact crucially relies on the traveled distance of a layer coefficients from
their initialization, and is implicit in many of the cited works. Proof can be found in Appendix B.
Lemma 9 (Conditioning of a layer). Let fM (α, x) be defined as before; suppose that there ∃x0 ∈ Xn
such that /m(∙, x0) : A → Yn has dense image in Yn, and we have a bound on the inverse norm of
bothderivatives: Z(DafM(xo,α)) ≤ f and Z(DxfM(x0,α)) ≤ $.
Then fM is WC with coefficients λα - kαM k2Gφ kx - x0 k with respect to A and λx -
Rφ kαM k∞ kx - x0k with respect to X, where Gφ is the lipschitz constant of φ, and Rφ is a
bound to the Second derivative of φ, i.e. Rφ :二 supr∈R ∣φ00(r)∣.
6	Empirical Analysis
In this section we discuss the settings and questions underlying our experiments; we then comment on
the results for the different areas of inquiry (conditioning, convergence, generalization). Additional
results and further observations are provided in Appendix F10.
Experimental Setting We train several overparameterized FCNs on random subsets of the CI-
FAR10 dataset (Krizhevsky et al., 2009)11. The networks are initialized with Gaussian Kaiming
initialization (He et al., 2015) to preserve the variance of activations in the forward pass; activation
functions are normalized according to AgarWal et al. (2020). Input data is normalized such that
IlxiIl = √m12, where m is the width of the first layer.
We focus on the folloWing questions: (1) Is the conditioning theory (AgarWal et al., 2020) predictive
of the measured conditioning? (2) How much does the lowest eigenvalue degrade with distance from
initialization? (3) Is the PL theory predictive of optimization progress? (4) Is the generalization
theory of PL functions (Charles & Papailiopoulos, 2018) predictive of generalization performance?
Conditioning at initialization We consider FCN networks consisting of 30 layers of varying
widths (1000, 2000, 5000), different activation functions (ReLU and Tanh), over multiple numbers of
randomly extracted examples (100, 200, 500, 1000), either renormalizing13 after application of each
layer or not, and averaging on three random seeds. The experiment has been run on a Tesla V100
PCIe 16GB GPU totaling a carbon footprint of 13Kg CO2 (calculated using CarbonFootprint).
Results are reported in Figure 1: we find a good adherence to the findings of Agarwal et al. (2020),
despite the finite width; however we find that renormalization is essential for conditioning to continue
decreasing steadily across layers (left plot)14; moreover we find that larger widths allow to reach a
smaller conditioning for the same number of examples and also allow for a later departure between
normalizing and non-normalizing behaviour (right plot). Basing on the adherence between the two
behaviours in the initial layers, we speculate that layer normalization strategies may be removed from
those without significant losses in accuracy. We leave a verification of this claim for future studies.
10Code for replicating the experiments is made available in supplemental material.
11Usage of a single dataset to validate the theory has been dictated by the heavy computational requirements
of the experiments. Such a choice should nonetheless not impact the validity of the presented results, because
our theory does not contain free parameters to be fitted on the dataset, and experiments are repeated for multiple
random extractions of a subset of the data, thus leaving less chance for an overfit to the dataset.
12This is done to satisfy a scaled requirement of the unitary norm used in Agarwal et al. (2020).
13i.e. rescaling each datapoint such that its norm is the square root of the number of neurons in the layer.
14We think the phenomenon can be explained by small-width effects in the sampling of gaussian weights.
7
Under review as a conference paper at ICLR 2022
1 2 3 4 5 6 7 a S 10 1112 13 14 15 lβ 1? lβ 19 20 21 22 23 24 25 2β 27 2B 2⅛test
layer
1 2 3 4 S 6 7 β 9 10 1112 13 14 15 M 17 lβ 19 20 21 22 23 24 25 2β 27 2B 29αtest
layer
Figure 1: The first plot reports the mean parameter conditioning for ReLU FCNs at initialization as
examples flow through the various layers; dotted lines represent renormalized networks. The values
are reported as the natural logarithm of the conditioning of the Hx matrix defined in Section 5; line
colors represent different examples-width-ratios of the tested networks. The second plot reports mean
conditioning for 500 examples at varying widths; shaded regions denote 95% confidence intervals.
Training speed We consider networks of 6 and 9 FCN layers, of width 500, with different activation
functions (ReLU, Tanh), and train them using full-batch gradient descent without momentum with
mean-squared-error loss over 80 epochs, with various learning rates (0.0005, 0.001, 0.005, 0.01) and
different number of randomly sampled examples (50, 100, 250, 500) over three random seeds. The
experiment has been run on an A100 SXM4 40GB GPU totaling a carbon footprint of 12Kg CO2.
Figure 2: The first plot reports the ratio between the predicted and the measured loss decrease15at
single epochs for 6-layers ReLU networks among different learning rates. The second plot reports in
blue the progression of the lowest eigenvalue in the gram matrix Hx in the last layer, and in orange
the lower bounds obtained by plugging the measured distance from initialization into Lemma 9.
Results are reported in Figure 2: we find that our upper bound on loss decrease given by Lemma 115 16
matches well the actual decreases in later epochs, while in initial epochs our estimate is too conserva-
tive (left plot). Concerning the measured lowest eigenvalues17 of the gram matrix Hx, we find that
they remain more or less constant during training (right plot) or tend to degrade gracefully (Figure 7),
while the lower bound obtained via Lemma 9 degrades significantly, and even becomes vacuous in
certain cases18. We emphasize the importance for theoretical research to look at possible explanations
for this behaviour, which could greatly simplify a study of the properties of neural networks.
For what concerns the lowest eigenvalues of input conditioning (right plot of Figure 3), we find that
they are almost zero for all the intermediate layers. This unfortunately does not enable us to use the
more precise bounds on the decrease of the training loss that consider all layers’ contributions.
15The “loss prediction ratio” refers to the ratio of the actual training loss and the bound calculated according
to Lemma 1 where the PL and smoothness coefficients are empirically calculated on the network at the previous
epoch. Local PL coefficients calculations are detailed in Appendix D.1.
16A detailed explanation of the measuring process for the PL coefficients is given in Appendix D.1.
17The lowest eigenvalues are exactly computed using power method on an implicit inverse of the Hx matrix.
18We remind the reader that other theoretical papers have used similar bounds (that depend on the distance of
weights from their initialization) to explain the workings of very wide networks.
8
Under review as a conference paper at ICLR 2022
Figure 3: The first plot shows the lowest eigenvalue of the input conditioning matrix for the various
layers. The right plot shows the generalization bounds obtained using the estimates in Lemma 4.
Generalization By measuring the local PL coefficients and estimates of the network lipschitz
coefficients, we are able to use Lemma 4 and compute a generalization bound (depicted in the right
plot of Figure 3). The plot shows the expected marked decrease with increasing optimization; despite
this, the estimates are vacuous19. Further investigation on this issue is left to future work.
7	Conclusions
We have established and analyzed a theory of overparameterized neural networks based on the theory
of PL functions, and suitably extended it. The experiments have partially proven the capacity of the
theory on conditioning and on convergence speed to give meaningful results in real-world networks
and data; they also have highlighted the need to consider other approaches for generalization theory
and for explaining good conditioning exhibited by the networks.
This work is, to the extent of our knowledge, the first that tries to explicitly quantify abstract theories
about the inner working of neural networks, and to compare the bounds obtained with real-world
experiments to give a feedback to the developed theories. We hope that such an approach to the
analysis of abstract theories might prove useful to direct the efforts of the theoretical community to
advance understanding of neural modules.
7.1	Limitations and future work
Conditioning The conditioning theory by Agarwal et al. (2020) does generally conform to the
experimental results of renormalized networks; we think that it would be greatly beneficial to expand
the theory to also cover finite-width networks; moreover, the observed behaviour of the lowest
eigenvalues during training currently remains unexplained.
PL Convergence The theory of optimization of PL functions seems to be able to explain the effective
decrease in training loss apart for the initial epochs, in which the theory presents conservative bounds.
Future work should focus on resolving the discrepancy between the fact that the measured input
conditioning are extremely low and the known fact that training all layers gives better result than
training only the last one.
PL Generalization The theory of generalization of PL functions via stability (Charles & Papail-
iopoulos, 2018) gives vacuous empirical bounds in the tested cases. We believe that a more adherent
theory concerning generalization of neural networks should reconsider the role of intermediate layers
as small deviations of random matrices, and not as arbitrary Lipschitz transformations, which can
greatly impair the obtained bounds.
Proxy-PL We think that the notion of proxy-PLness by Frei & Gu (2021) would be extremely
interesting to consider for generalization of the current theory (this work appeared when present work
was under submission).
19The loss has a value between zero and one, while the predicted bounds are well over twenty.
9
Under review as a conference paper at ICLR 2022
Reproducibility statement
Code to reproduce all the experiments presented in this paper, and the associated plots, is available
in the supplemental materials (and will be publicly released post acceptance). Extra care has been
taken to ensure reproducibility: a pinned conda environment file precisely describes the versions of
the software that has been used to run the scripts, and DVC has been used to ensure the exact details
of the commands to run the experiments and a cryptographic hash of their output has been saved.
Functions in code are properly documented, and all computations accept an initial random seed to
ensure reproducibility of the single runs.
References
Naman Agarwal, Pranjal Awasthi, and Satyen Kale. A deep conditioning treatment of neural networks.
arXiv preprint arXiv:2002.01523, 2020.
Zeyuan Allen-Zhu, Yuanzhi Li, and Zhao Song. On the convergence rate of training recurrent neural
networks. arXiv preprint arXiv:1810.12065, 2018.
Zeyuan Allen-Zhu, Yuanzhi Li, and Zhao Song. A convergence theory for deep learning via over-
parameterization. In International Conference on Machine Learning, pp. 242-252. PMLR, 2019.
Peter L Bartlett, Philip M Long, Gdbor Lugosi, and Alexander Tsigler. Benign overfitting in linear
regression. Proceedings of the National Academy of Sciences, 117(48):30063-30070, 2020.
Mikhail Belkin, Siyuan Ma, and Soumik Mandal. To understand deep learning we need to understand
kernel learning. In International Conference on Machine Learning, pp. 541-549. PMLR, 2018.
Zachary Charles and Dimitris Papailiopoulos. Stability and generalization of learning algorithms
that converge to global optima. In International Conference on Machine Learning, pp. 745-754.
PMLR, 2018.
Dominik Csiba and Peter Richtdrik. Global convergence of arbitrary-block gradient methods for
generalized polyak-lcjasiewicz functions. arXivPrePrintarXiv:1709.03014, 2017.
Amit Daniely, Roy Frostig, and Yoram Singer. Toward deeper understanding of neural networks: The
power of initialization and a dual view on expressivity. arXiv PrePrint arXiv:1602.05897, 2016.
Simon Du, Jason Lee, Haochuan Li, Liwei Wang, and Xiyu Zhai. Gradient descent finds global
minima of deep neural networks. In International Conference on Machine Learning, pp. 1675-1685.
PMLR, 2019.
Simon S Du, Xiyu Zhai, Barnabas Poczos, and Aarti Singh. Gradient descent provably optimizes
over-parameterized neural networks. arXiv PrePrint arXiv:1810.02054, 2018.
Andre Elisseeff, Theodoros Evgeniou, Massimiliano Pontil, and Leslie Pack Kaelbing. Stability of
randomized learning algorithms. Journal of Machine Learning Research, 6(1), 2005.
Spencer Frei and Quanquan Gu. Proxy convexity: A unified framework for the analysis of neural
networks trained by gradient descent. arXiv PrePrint arXiv:2106.13792, 2021.
Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair,
Aaron Courville, and Yoshua Bengio. Generative adversarial nets. Advances in neural information
Processing systems, 27:2672-2680, 2014.
Charles Guille-Escuret, Manuela Girotti, Baptiste Goujaud, and Ioannis Mitliagkas. A study of con-
dition numbers for first-order optimization. In International Conference on Artificial Intelligence
and Statistics, pp. 1261-1269. PMLR, 2021.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Delving deep into rectifiers: Surpassing
human-level performance on imagenet classification. In Proceedings of the IEEE international
conference on comPuter vision, pp. 1026-1034, 2015.
10
Under review as a conference paper at ICLR 2022
Arthur Jacot, Franck Gabriel, and Clement Hongler. Neural tangent kernel: Convergence and
generalization in neural networks. In Advances in neural information processing systems, pp.
8571-8580, 2018.
Hamed Karimi, Julie Nutini, and Mark Schmidt. Linear convergence of gradient and proximal-
gradient methods under the Polyak-IojasieWicz condition. In Joint European Conference on
Machine Learning and Knowledge Discovery in Databases, pp. 795-811. Springer, 2016.
Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiPle layers of features from tiny images. 2009.
Jaehoon Lee, Lechao Xiao, Samuel S Schoenholz, Yasaman Bahri, Roman Novak, Jascha Sohl-
Dickstein, and Jeffrey Pennington. Wide neural netWorks of any dePth evolve as linear models
under gradient descent. arXiv preprint arXiv:1902.06720, 2019.
Jaehoon Lee, Samuel S Schoenholz, Jeffrey Pennington, Ben Adlam, Lechao Xiao, Roman Novak,
and Jascha Sohl-Dickstein. Finite versus infinite neural netWorks: an emPirical study. arXiv
preprint arXiv:2007.15801, 2020.
DaWei Li, Tian Ding, and Ruoyu Sun. On the benefit of Width for neural netWorks: DisaPPearance of
bad basins. arXiv, PP. arXiv-1812, 2018.
Chaoyue Liu, Libin Zhu, and Mikhail Belkin. ToWard a theory of oPtimization for over-Parameterized
systems of non-linear equations: the lessons of deeP learning. arXiv preprint arXiv:2003.00307,
2020.
Hai Feng Ma, Shuang Sun, YuWen Wang, and Wen Jing Zheng. Perturbations of moore-Penrose
metric generalized inverses of linear oPerators in banach sPaces. Acta Mathematica Sinica, English
Series, 30(7):1109-1124, 2014.
Haifeng Ma. Construction of some generalized inverses of oPerators betWeen banach sPaces and their
selections, Perturbations and aPPlications. 2012.
Colin McDiarmid. On the method of bounded differences. Surveys in combinatorics, 141(1):148-188,
1989.
Samet Oymak and Mahdi Soltanolkotabi. ToWard moderate overParameterization: Global con-
vergence guarantees for training shalloW neural netWorks. IEEE Journal on Selected Areas in
Information Theory, 1(1):84-105, 2020.
Boris Teodorovich Polyak. Gradient methods for minimizing functionals. Zhurnal Vychislitel’noi
Matematiki i Matematicheskoi Fiziki, 3(4):643-653, 1963.
Mark Rudelson and Roman Vershynin. Smallest singular value of a random rectangular matrix.
Communications on Pure and Applied Mathematics: A Journal Issued by the Courant Institute of
Mathematical Sciences, 62(12):1707-1739, 2009.
Samuel S Schoenholz, Justin Gilmer, Surya Ganguli, and Jascha Sohl-Dickstein. DeeP information
ProPagation. 2017.
Alexander Tsigler and Peter L Bartlett. Benign overfitting in ridge regression. arXiv preprint
arXiv:2009.14286, 2020.
Junchi Yang, Negar Kiyavash, and Niao He. Global convergence and variance-reduced oPtimization
for a class of nonconvex-nonconcave minimax Problems. arXiv preprint arXiv:2002.09621, 2020a.
Zitong Yang, Yaodong Yu, Chong You, Jacob Steinhardt, and Yi Ma. Rethinking bias-variance
trade-off for generalization of neural netWorks. In International Conference on Machine Learning,
PP. 10767-10777. PMLR, 2020b.
Difan Zou, Yuan Cao, Dongruo Zhou, and Quanquan Gu. Gradient descent oPtimizes over-
Parameterized deeP relu netWorks. Machine Learning, 109(3):467-492, 2020.
11
Under review as a conference paper at ICLR 2022
A Proofs of Generalized PL and WC Lemmas
Lemma 1 (Convergence speed and radius for PL functions). Let f : "分 Xi → R be μ-PL and
L-smooth; the Xi be Hilbert spaces. Choose an initial point x0 and let the sequence of iterates
evolve according to the rule
xk+1 = Xk - LVif(xk).
Letting Y := 1 — mini L, the optimality gap decreases exponentially following theformula
f(Xk + 1)_ f * ≤ Y (f(Xk ) — f *).
(2)
Moreover, the distance from the initial point is bounded by
XLM+1-χ0U ≤ t
i
i
2(f(x0)-广)
Proof. We begin by proving the first formula:
f(xk+1)-f (Xk) ≤-XX 2⅛- kvif (Xk )k2
=一 2 x Li ɪ kvif (xk )k2
-iLi μ-
i
By smoothness and Equation (2)
≤ (m,in L) (-2 X MVif(Xk)k2
≤ 一 (miin Li) (f (Xk) 一 f *)
By definition of PL
And thus we obtain
f (Xk+1) - f * ≤ (f (Xk+1) - f (Xk)) + (f(xk) - f *)
≤(1 一 miin L) (f(xk) 一 f *).
Concerning the second equation we have:
k
X KHi+lx0" ≤ XX K kvif(Xt )k
=X X " fr
≤ X S X L- Pf(Xt) - f*
t=1 i
≤ Ss2 X L- X γt∕2√f(XFf
i	t=1
i
≤χ 2(f(xo)- f*)
and thus we have proved the thesis.
□
Lemma 2 (Strongly convex functions are PL). Let f : X → R be a τ -strongly convex function. Then
f is T-PL on every set Ω ⊆ X ,i.e. ∀x ∈ Ω ∣∣Vf (x)k2 ≥ T (f(x) 一 fΩ) where fΩ := infχ∈Ω f(x).
12
Under review as a conference paper at ICLR 2022
Proof. We recall that f being τ -strongly convex we may write
f(x) - f(y) ≥ hVf (y),x - y + 2||x - yk2
Now We take infχ∈Ω from both sides and We get:
f* — f(y) ≥ inf hVf(y),χ — y)+ T||x — y|2
x∈Ω	2
≥ inf hVf(y),χ — y) + T||x — y|2
x∈X	2
12
=-2τ kVf (y)k2
and We obtain the thesis.
□
Lemma 5 (Composition of a PL function with WC is PL). Let g : Y → R be μ-PL over Ω ⊆ Y and
f : i Xi → Y be λ-WC over i Mi ⊆ i Xi and suppose they satisfy the inequality
X IlDgf(X)|R(Difx)ll2p ≥ αkDg(f(X))k2P	⑶
i
and f (Qi Mi) = Ω .Then h( *) = g(f (*)) is (μαλ2) -PL over Qi Mi and h* = gΩ.
Proof.
1 X μ⅛ kDhχik2 = 1 X μ⅛l1Dgf(X)Difx ∣∣2
=1 X ɪ SU	∣∣Dgf(χ)[Difχ[vi]]∣∣2 ∣Difx[vi]k2
―2 J μαλi2 VsuXi —西丽|	∣Vik2
≥ 1X λ2	sup	∣∣Dgf(X)[yi]∣∣2
―2 i μαλ2 yi∈R(Difx)	∣∣yik2
≥ 2μι ∑∣Dgf (x)iR(Difx) ∣∣o2p
12
≥ 2μ ∣∣Dgf(X) ∣∣op
≥ g(f(X))-gΩ = h(X)- h*
because of the surjectivity of f on Ω.	□
Lemma 6 (Composition of a WC function with WC is WC). Let g : i Yi → Z be (,iλi)-WC and
fi : j Xij → Yi be ( ,ij μij) -WC. Then their composition h : ∩i7- Xij → Z defined by
h(,ij Xij) = g(,i fi(,j Xij))
is (,ij λiμj)-WC. By f(,n= Xi) we mean f (xι, ...,Xn).
Proof. It follows very easily from the definition: infact we can see that we want to bound the inverse
solution norm:
inf |Xi | = inf	inf	|Xij |
Xij ∈S(Dij hx ,z)	yi∈S(Digf(x) ,z) Xij ∈S(Dij fi (X),yi)
≤ Qrinf ∖一∣∣yik
yi∈S(Digf(χ),z) μij
≤ ɪ∣z∣
μij λi
and we obtain the desired outcome.
□
13
Under review as a conference paper at ICLR 2022
B Proofs for well-conditioning of Wide Layers
Lemma 9 (Conditioning of a layer). Let fM (α, x) be defined as before; suppose that there ∃x0 ∈ Xn
such that /m(∙, xo) : A → Yn has dense image in Yn, and we have a bound on the inverse norm of
bothderivatives: Z(DafM(xo,α)) ≤ J and Z(DxfM(xo,α)) ≤ $.
Then fM is WC with coefficients λα - kαM k2Gφ kx - x0 k with respect to A and λx -
Rφ kαM k∞ kx - x0k with respect to X, where Gφ is the lipschitz constant of φ, and Rφ is a
bound to the second derivative of φ, i.e. Rφ :二 suPr∈R ∣φ00(r)∣.
Proof. We first bound the displacements of the partial derivatives:
and thus we have Pk,i fM (α, x)ik - fM(α,x0)ik2 ≤ kαMk22G2φkx - x0k22.
We recall that for a symmetric matrix H we have kH k2 ≤ √kHkιkHk∞ = kHl∣ι,andthat
W 2 ' = ανjMVj δijδrk'φ0(xk )
∂χk∂x'
ij
thus we can bound the displacement of the kernel derivatives by integrating:
II(DxfM(α,x)k - DxfM(α,x0)k)[δx]∣∣ ≤ Z dt ∣∣D2xfM(α,xt)k[δx,x - x0]∣∣
0
≤ kx - x0kkδxk sup ∣Dx2xfM (α, xt)ik∣2→2
≤ kαMk∞kx - x0kkδxk SUp lφ0θ(r)1
r∈R
where xt = x0 + t(x - x0).
The two inequalities with Lemma 8 allow us to bound Z (DfM (α, x)) for x near x0 and for any α:
K a/M( , )) _ 1 — λ1α ∣DafM (α, X)- DafM (α, X0)1 一 λa - GφkαM k2kx - x0k
Z(DxfM(α,x)) ≤ 1 - λχx∣DxfM(α,χ) - DxfM(α,χo)k ≤ λx - Rφ∣αM∣∞∣x - xo∣
because of hypothesis (1).	□
C Proof of Inverse Norm B ound Lemma
In this appendix we will give a proof of the Inverse Norm Bound Lemma. The one exposed here is a
generalization of the one in Ma (2012); Ma et al. (2014).
Lemma 8 (Inverse Norm&ound). Let T,T : X → Y be two linear operators with the same range,
i.e. such that R(T) = R(T) and satisfying Z(T) ∣∣T - T∣∣。口 < 1. Then we have
Z (T) ≤
Z(T)
1 - Z (T )∣∣t - t∣∣ op∙
We first recall the definition of Z(T):
Definition 5 (Inverse Norm of a linear operator). Given a linear operator T : X → Y define its
inverse norm by
Z(T):= sup	inf	∣kχ k∣
x∈X x0∈S(T,Tx) kTxk
which coincides with the smallest λ for which T is 1-WC.
14
Under review as a conference paper at ICLR 2022
Given T we define an approximate norm inverse function (possibly non-linear, non-continous)
TεM	: R(T) → X if it respects	∀y	∈	R(T) we have	TεMy	∈ S(T, y)	such that	TεM y	≤
ε + infx∈S(T,y) kxk.
For every ε > 0 there obviously exist many such functions and for them it holds TTεM = Id,
limε→0 TεMy = infx∈S(T,y) kxk. For them we define
UTMU= sup 吟斗
y∈R(T)	kyk
and we can observe that
UTMU := lim∣∣TM∣∣ = SUp inf kxk = Z(T)∙
ε→0	ε	y∈R(T) x∈S(T,y) kyk
We begin by setting the notation for the following lemmas. We will consider two linear operators
T,T : X → Y and we will fix b ∈ R(T), b ∈ R(T). If not differently specified, δT = T - T,
δb = b - b, X ∈ S(T, b), X ∈ S(T, b), xε = TMb, xε = TMb.
Definition 7 ((Ma, 2012, Definition 2.1.4)). Given a linear operator T : X → Y, define
γ(T) := inf {kTxk | x ∈ X, dist(x, N (T)) = 1}
Lemma 10 (Bounds for distance from Kernel, (Ma, 2012, Lemma 4.1.1)).
∀x∈X UUTεMTUU-1UUTεMTxUU ≤ dist(x, N (T)) ≤ UUTεM UUkT xk
∀z ∈ R(T) -J- ≤ γ(T) ≤ ∣∣TMT∣∣∣∣TTMz∣∣
R(T ) ∣∣TMk 一 Y(T ) ≤	∣∣TMZk
Proof. Notice that TMTx ⊆ {x0 | Tx0 = Tx, ∣x0∣ ≤ infχ∈s(τ,τx) IIXIl + ε}. Thus we have
(Id -TεM T)x ∈ N(T) and it follows that
dist(x, N (T)) ≤ ∣∣x - (Id -TεM T)x∣∣ = ∣∣TεMTx∣∣ ≤ ∣∣TεM∣∣ITxI
On the other hand fory ∈ N(T) we have
∣∣TεMTx∣∣= ∣∣TεM T (x - y)∣∣ ≤ ∣∣TεM T ∣∣Ix - yI
and thus
dist(x,N(T))= inf Ix-yI ≥ ∣∣TεM T ∣∣-1 ∣∣TεM T x∣∣
Now we have γ(T) = infχ∈χ dist∣TN(T))and thus we obtain from dist(x,N (T)) ≤ ∣∣TMUkTX∣:
ITxI
1
Y(T) ≥ XiIX ∣tmkkTx∣ 一 ∣tmk
and from the other inequality for dist(x, N(T)) we get
γ(T) ≤ UTM t a ⅛≤ 幽>
for x := TεM z and thus the inequality holds ∀z ∈ R(T ).
□
Lemma_11 (Bounds for distance between inverse solution sets, (Ma, 2012, Lemma 4.1.4)). Let
X ∈ S(T,b). Then we have
∣∣TIITIlδTX- δb∣ ≤ dist(X,S(T,b)) ≤ ∣∣TM∣∣∣δTX- δb∣
15
Under review as a conference paper at ICLR 2022
Proof. Notice first that S(T, b) = TMb + N(T), thus dist(x, S(T, b)) = dist(x - TMb, N(T)).
Using Lemma 10 we have
dist(x,s(T,b)) ≤ ∣∣TM∣∣∣∣T(X-TMb)II = IITMIlkδTχ-δbk
because T = T - δT and TTM = Id. By taking the limit on ε → 0 we obtain the equation.
On the other hand we can observe that for y ∈ N(T) we have
∣∣t(x - TMb)∣∣ = ∣∣t(x - TMb- y)∣∣ ≤ kTk ∣∣x - TMb - y∣∣
and thus by taking the inf over y ∈ N(T) we get
dist(x,s(τ,b)) ≥ kTk-1∣∣τ(X- TMb)∣∣ = kTk-1kδTx - δbk
as desired.	□
Lemma 12. If ∣∣TM∣∣∣∣δT∣∣ < 1 and R(T) = R(T) then
∣∣τ MII ≤
IIT M∣∣
ι -kτ m kkδτ k
Proof. If ∣TM ∣kδTk < 1 then ∀ε < ε0 it holds ∣TεM ∣kδT k < 1.
Now given any X ∈ S(T,b), We can find χε ∈ S(T, b) such that
∣∣X - xεk ≤ dist(X,S(T, b)) + ε ≤ ∣∣TM∣∣∣∣δTX - δb∣∣ + ε
where the last inequality holds because of Lemma 11. Clearly such Xε is such that kXεk ≥ ∣TεM b∣ -ε
by the definition of TεM .
Consider now the following expression
∣a∣∣	UTM∣∣ ≤ ∣∣τMb∣∣TIτMbII
kbk - IITε II ≤	kbk
≤ kxεk + ε - kxk
≤ R
ε + ∣∣Xε - Xk
≤ -R-
2ε +∣∣T M∣∣kδT kkXk
≤	kbk
≤H+∣∣T M∣∣kδτ k∣∣τ m∣∣
By the definition of operator norm
Where X := TMb and Xε as above said
and by taking the sup on b such that kbk = 1, and then the limit for ε → 0 we obtain
∣∣τ m∣∣-∣∣t m∣∣ ≤ ∣∣τ M∣∣kδτ k∣∣τ m∣∣
which can easily be rearranged into the wanted equation.	□
D	Details of the Experiments
We reserve this section to give additional commands and details about the experimental setup.
D. 1 Measurement of the PL coefficients
Recall that in Definition 1 multiple PL coefficients are present, one for each space Xi (which
correspond in practice to the different layers of the netowrk). As detailed right above the definition,
the quantities μi do depend on the point, and we measure them by computing the smallest eigenvalue
of the linearization of the network function.
16
Under review as a conference paper at ICLR 2022
Such linearizations are computed separately for the weights of each layer: for the last layer the
smallest eigenvalue is directly computed, while for earlier layers we rely on the characterization of
their linearization as a composition of many linearization: that one of the last layer and Rx for each
intermediate layer.
In this way We can apply the estimate σ* (AB) ≥ σ* (A)σ* (B) where by σ* We denote the smallest
singular value of the matrix, and thus we obtain μn estimated via linearization of the last layer,
μn-ι ≥ μn * Z(RxRT) and so on.
By this means we unfortunately do only obtain marginal improvements over just using the last layer
PL coefficient, but a direct measurement of the smallest eigenvalue of the linearizations for earlier
layers was very difficult to do at the time using deep learning frameworks. We intend to measure
such earlier layers’ PL coefficients directly in future experiments by using some (still experimental)
features of functorch.
Actual calculations In practice we calculate explicitly the H matrix, from which the Gram Matrices
can be calculated as G = HHT . We proceed by performing a QR decomposition of H such that
G = QRRT QT and thus extremal values of G correspond to extremal eigenvalues of RRT because
of the orthogonality of Q and these can be easily calculated iteratively: the maximal eigenvalue is
calculated using power iteration method on RRT and the minimal eigenvalue is calculated by power
iteration on the inverse matrix, obtained in implicit form (performing first a minimum norm solution
problem with the matrix R and later a least square solution with R).
E Empirical Bounds Calculations
We provide many small calculations which are useful in deriving explicit numeric constants in the
empirical neural network study.
In what follows we define the smoothness and lipschitz constant of a function f : X → Y by:
kf(x)-f(x0)k≤Gfkx-x0k
l∣Vf(X)-Vf(X0)k ≤ Lf kχ - x0k
Lemma 13 (Smoothness Constant and Composition). Let f : X → Y and g : Y → Z, and define
h(X) = g(f(X)). Then we have Gh ≤ GgGf and Lh ≤ Gf2 Lg + GgLf.
Proof. The result is well-known for the lipschitz constant, and for the smoothness constant we have:
lVhx - Vhx0 l = Vgf (x) Vfx - Vgf (x0) Vfx + Vgf (x0) Vfx - Vgf (x0) Vfx0
≤ lVfx lVgf (x) - Vgf (x0) + Vgf (x0) lVfx - Vfx0 l
≤ (GfLg + GgLf)kx - x0k
□
Lemma 14. A Layer y% = Ej ajMijφ(xj) = (αφ(x.))i Lipschitz's ConstantWith respect to X is
≤ ∣∣αM∣∣2→2Gφ, and its SmoothneSS constant with respect to X is ≤ ∣∣αM∣∣∞ max ∣φ00∣.
On the other hand the smoothness constant with respect toα is zero, and the lipschitz constant is
≤ λg(FxFT)-1/2, where (Fx)kiyj = δiiοM"jφ(xj).
Proof. We begin by calculating the lipschitz constant with respect to X:
l∣y -y0∣2 = ∣α(O(Xj- φ3))l ≤ l∣α∣2→2GΦllx∙ - x0l
and for the smoothness constant we observe that
=	=	= δmnXαij Mij φ (Xm )δXm δXn
∂Xm∂Xn
j
max IlVxyII2→2 ≤ max IlVxyI-1 ≤ l∣αM l∞ max |夕|
The smoothness constant with respect toα is clearly zero since the function is linear inα, and the
lipschitz constant can easily be found since the layer is a matrix multiplication.	□
17
Under review as a conference paper at ICLR 2022
Lemma 15. The mean-squared-error loss has G ≤ 2MSE(x), L ≤ N, where N is the number of
dimensions. Moreover it is a 1-strongly convex function.
Proof. Recall that for the mean-squared-error loss we have:
'(x,y) = 2 X (Xi- yi)2
i
∂'
W-- = Xn - yn
∂xn
∂2'	C
不 =	= δnm
∂Xn∂Xm
and thus we obtain that n
2'(x,y), Pn,ml∂⅛∣2
N.
It is a 1-strongly convex function because the Hessian is always positive definite and H ≥ I.	□
Lemma 16. The cross-entropy loss has L, G ≤ 2 and is a weakly convex function.
Proof. Recall that cross-entropy satifyies
'(x, k) = -Xk + log E ex
∂' /	,、	，
∂χ- (x, k) = -δn,k +
∂2'
∂Xm∂Xn
δmn exn
(X, k) =
j
exn
Pj exj
Pj exj	- exm exn
,exj)
and one easily obtains that
≤ 2, Pn,m
∂Xn∂xn (x, k)∣ ≤ 2.
xi
Weak convexity stems from the positivity of the hessian matrix: let us call t = Pe ex§, then we have
seen that Hnm = ∂x∂2Lιτ (x, k) = δm∕n - tnjtm, and thus we have
(	∖2
uTHu=	Hij	uiuj = tiui2- titj uiuj	= tiui2	-	tj uj	≥0
ij
i ij
i
j
by Cauchy-Schwartz inequality applied to √ti and √tiui, since Pi ti = 1.	□
We have exposed the theory about strongly PL functions for simplicity, but we would like to highlight
that to analyze the results of the experiments we have to rely also on the theory of weak PL functions
which originated in the work of Csiba & RiChtdrik (2017). We expose the main results about weak
PL functions, and refer the interested reader to the work.
Definition 8 (Weakly PL function, Csiba & Richtdrik (2017)). A function f : X → R is weakly
μ-PL on Ω ⊆ X ifthere ∃x* ∈ X * the set ofglobal minimizer such that
∀χ,y ∈ Ω INf(X)IlIlX - χ*k ≥ √μ(f(χ) - f*)
where f * := minx∈Ω f (x)∙
It can be easily observed that every convex function is weakly PL with μ = 1. Moreover we have the
following lemma on the decrease of the objective during the optimization process:
18
Under review as a conference paper at ICLR 2022
Lemma 17 (Gradient Descent on Weakly PL functions, (Csiba & Richtarik, 2017, Lemma 3)). Given
an L-smooth, weakly μ-PLfunction f : X → R and chosen an initial point xo, we let the sequence
of iterates evolve according to the rule:
Xk+1 = Xk - 1 Nf(Xk).
L
Then the optimality gap decreases following the formula
f(xk+ι) - f * ≤ (1 - μ(f(xk) - f? )(f (xk) - f *).
“ k+1>	- ∖ 2Lkxk - x* k2 ΓM ''
Crucially We also have to show that the composition of a λ-WC function with a μ-weakly PL function
does consist of a μ-weakly PL function. This does happen if we assume a small condition of
non-degeneracy of the applied WC function.
Lemma 18. Let f : Y → R be a μ-weakly PLfunction with y* the minimizer in its definition and g :
X → Y be a λ-WC function such that ∃q ∈ g-1(y*) satifying ∀x ∈ X kx - qk ≥ αkg(x) - y* k.
Then the composition h(x) = f (g(x)) is μλ2α2-weakly PL.
Proof. The proof proceeds very similar in spirit to the one for PL functions:
∣∣Vh(χ)kkχ - qk = ∣∣Vf(g(x)) ∙ Vg(X)kkχ - qk
≥ λkVf (g(x))kakg(x) - y*k
≥ Pμλ2α2(h(x) — h*)
□
F Additional Experimental Results
We report here additional details about the experimental results. The focus of this additional analysis
is on discussing phenomena which are not fully understood and on useful metrics that can guide a
more principled developing of neural networks.
F.1 Scaling of the eigenvalues
We are interested in understanding the change in conditioning due to varying the width of the network
and the number of examples used to train it. We compare the measured eigenvalues to the ones
predicted by random matrix theory. In particular if we treat the matrix Fx as a random Gaussian
matrix, using the result of Rudelson & Vershynin (2009) we would expect that
λmax 〜 wwidth ∙ examples
λmin 〜wwidth - ^/examples
and this is exactly what we can observe in Figure 4, which shows that such relation holds at every
layer. This holds despite the fact that matrix Fx is not random, at least in the first layer.
F.2 Measures to detect chaotic behaviour
Given our findings, and particularly those in Figure 1, in principle one would like to avoid ending up
training a network that is too deep for its width, thus creating the effect of raising the conditioning
from some point onward. Measuring conditioning directly is inefficient, since it requires to solve the
eigenvalue problem for very big matrices, where size depends both on the width of the network and
on the number of examples, making this impractical for real datasets.
From the conditioning theory of Agarwal et al. (2020) we would expect off-diagonal entries of
FxFxT to tend to zero as the signal propagates through the layers, hence we can measure this proxy
19
Under review as a conference paper at ICLR 2022
layer = 1
7
6
layer = 11
3
2
5 4
6ιπuOEPUoU
BLnUOEPUOU
conditioning_factor	conditioning_factor
Figure 4:	Conditioning plot with respect to the conditioning factor for Tanh activation networks.
information instead of measuring eigenvalues directly 20. In Figure 5 the difference in behaviour
between the normalized layers and the non-normalized ones is extremely evident, both in the values
of the maximum value of the off-diagonal entries and of the maximum row sum of the matrix FxFxT ,
which do align perfectly with the raise in conditioning that we have already observed in Figure 1.
F.3 Comparisons with infinite width networks
We compare the results obtained on conditioning with the ones calculated using the dual activation
function theory by Agarwal et al. (2020); Daniely et al. (2016). Figure 6 shows the values for finite
and for infinite width networks. For the finite width networks we report the measured highest and
lowest eigenvalues, while for infinite width networks we report the upper bounds for the highest
eigenvalue and the lower bound for the lowest eigenvalue. Thus the curves for infinite width networks
are a little different than those for the other widths. Nonetheless the obtained plots show the validity
of the theory, with its predictions closely following the observed values, and highlight the need for the
study of an analogue finite-width theory to better compare theoretical results with mesurable ones.
20This has been used by Agarwal et al. (2020) themselves in their own experiment.
20
Under review as a conference paper at ICLR 2022
O
1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29atest
layer
4 3 2 1
-euo6e-puolu 一 LU
width
--- Min on diagonal
--- Min on diagonal
--- Min on diagonal
——1000
——2000
-——5000
O
1 2 3 4 5 6 7 8 9 IO 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29atest
layer
Figure 5:	The first plot shows the maximum rowsum of the off diagonal entries of the matrix FxFxT ;
the second plot shows the minimum value of the on-diagonal entry and the maximum value of the
off-diagonal entries.
F.4 Eigenvalues during training in cross-entropy loss
We have seen in Figure 2 that the eigenvalues when considering a MSE loss remain mostly stable
across training. What we instead observe for the cross-entropy loss (Figure 7) is that, especially at
higher learning rates, we note an initial rapid increase in conditioning, followed by a steady decrease
for both ReLU and Tanh activations.
This observations hints at the fact that there is some other aspect of the optimization of neural
networks with respect to conditioning that is not fully understood, since we should reasonably expect
that conditioning does continue to rise as the network weights are perturbed from their original
positions.
21
Under review as a conference paper at ICLR 2022
20.0
17.5
15.0
12.5
10.0
7.5
5.0
2.5
1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29atest
layer
1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29atest
layer
Figure 6: Plot of the normalized highest eigenvalue (top) and lowest eigenvalue (bottom) for the
tested widths and for the theoretical prediction for a FCN with ReLU activation.
F.5 Prediction of training loss with cross-entropy
Using Lemma 17 for the cross-entropy loss and estimating ∣∣Xk - x* k ` 2 MXk- where G is the
Lipschitz constant of the function f, we obtain the results showed in Figure 8.
We can observe how the prediction accuracy are similar to what we have obtained for the mean-
squared-error loss: our estimates are too conservative for the first epochs, but align well with
successive epochs. Further analyses are needed to perfectly align theoretical predictions with
experimental ones.
F.6 Cumulative prediction losses
We provide in Figure 9 and in Figure 10 plots for the cumulated loss prediction ratios, given as
cumulative products of the single epoch ratios in time both for ReLU and Tanh networks trained under
22
Under review as a conference paper at ICLR 2022
20864208
5.5.4.4.4.4.4.3.
6uuo⅛pu8
60
50
40
30
20
latest
0.0005
0.001
0.005
Figure 7: Plots of conditioning progression for ReLU (top) and Tanh (bottom) networks.
both Mean Squared Error objective and Cross-Entropy. The cumulative product gives a measure of
the amount of uncertainty in the prediction of successive epochs.
23
Under review as a conference paper at ICLR 2022
1.20
I-
L
OQE」|UoEP-PaIdISSO-
60
50
40
30
20
42086420
Iiiooooo
LLLLLLLL
o□Bl-luo-MyPsdlsso-
80
70
60
50
'-!.O
30
20
10
Figure 8: Plots of loss prediction ratio for ReLU and Tanh networks trained with the cross-entropy
loss. The plots show the ratio between real loss and predicted loss at each epoch.
24
Under review as a conference paper at ICLR 2022
Ooooooo
7 6 5 4 3 2 1
PaJdSSo-EnU
80
70
60
50
30
20
10
Figure 9: Cumulated loss prediction ratio for networks trained under Mean Squared Error, obtained
as the cumulative product of the single epoch ratios. The first plot is for ReLU networks; the second
plot details Tanh networks.
25
Under review as a conference paper at ICLR 2022
6 4 2 0 8 6
Illl
PaJdSSo-EnU
80
60
50
'-!.O
30
20
6 5 4 3
PB-IdSSO-LUrD
Figure 10: Cumulated loss prediction ratio for networks trained under Cross-Entropy, obtained as the
cumulative product of the single epoch ratios. The first plot is for ReLU networks; the second plot
details Tanh networks.
26