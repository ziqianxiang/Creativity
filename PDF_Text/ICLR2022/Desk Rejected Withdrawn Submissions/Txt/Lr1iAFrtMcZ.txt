Under review as a conference paper at ICLR 2022
Tuning Confidence Bound for Stochastic Bandits
with Bandit Distance
Anonymous authors
Paper under double-blind review
Ab stract
We propose a novel modification of the standard upper confidence bound (UCB) method
for the stochastic multi-armed bandit (MAB) problem which tunes the confidence bound
of a given bandit based on its distance to others. Our UCB distance tuning (UCB-DT)
formulation enables improved performance as measured by expected regret by preventing
the MAB algorithm from focusing on non-optimal bandits which is a well-known deficiency
of standard UCB. "Distance tuning" of the standard UCB is done using a proposed distance
measure, which we call bandit distance, that is parameterizable and which therefore can be
optimized to control the transition rate from exploration to exploitation based on problem
requirements. We empirically demonstrate increased performance of UCB-DT versus many
existing state-of-the-art methods which use the UCB formulation for the MAB problem.
Our contribution also includes the development of a conceptual tool called the Exploration
Bargain Point which gives insights into the tradeoffs between exploration and exploitation.
We argue that the Exploration Bargain Point provides an intuitive perspective that is useful
for comparatively analyzing the performance of UCB-based methods.
1	Introduction
Multi-armed bandit (MAB) (Slivkins, 2019) can model a broad range of applications, such as selecting the
best website layout for users, or choosing the most profitable stocks among many. Stochastic bandits is an
important setting in which MAB problems have been studied extensively. One of the most influential and
widely used stochastic bandit policy is the upper confidence bound method (UCB) (Auer et al., 2002). UCB
works by maintaining a mean estimation and confidence radius1 for each bandit, and selects the bandit whose
sum of mean and confidence bound is the maximum among all bandits at each step. The confidence bound
can grow larger for less frequently used bandits which represents a higher degree of uncertainty to serve the
exploration purpose.
However, the original UCB algorithm by its nature can lead to unsatisfactory results by being over-optimistic
on non-optimal bandits. In this work, we propose UCB-DT (Upper Confidence Bound - Distance Tuning), a
simple modification to the original UCB method which makes the confidence bound of a given bandit depend
on its distance to others. Since the UCB-DT policy will select the largest bandit more often, exploration will
naturally lean towards the neighbors of the largest bandit and prevent the algorithm from focusing on bandits
that are farther away. Our proposed bandit distance is parmeterizable thereby offering the opportunity of
customization over policies through different distances. Therefore, our formulation can represent a family
of policies which inherently provide the flexibility of both pro-exploration policies, such as UCB, and
pro-exploitation policies like -greedy.
1We will refer confidence radius as confidence bound in the rest of the paper.
1
Under review as a conference paper at ICLR 2022
Moreover, unlike previous UCB-based methods which focus on the log function in the confidence bound
because of its analytical tractability, our method works differently by extending the denominator term. Using
this enhancement to standard UCB, we propose a concept named Exploration Bargain Point to provide a novel
perspective on analyzing performance of UCB-based methods. Using our new analysis tool, we intuitively
and even graphically show that our method can always perform better than standard UCB.
We review existing work on the design of confidence bound for UCB in Sec. 3. While maintaining connections
to some of these previous approaches, we make the following novel contributions in this paper.
•	We propose UCB-DT policy, which tunes confidence bound by bandit distance. We conduct analysis
and numerical experiments to show that our formulation is simple, extensible, and performant.
•	We present a concept called Exploration Bargain Point, which provides a novel viewpoint on
analyzing performance of upper confidence bound methods.
2	Preliminaries
Let k ∈ Z+ be the number of bandits, T denote the time horizon, μ% ∈ R be the unknown mean for the
SUbgaUssian reward distribution of bandit i, μ* = maxi μi be the mean reward of the optimal bandit, Bi be
the shorthand for “ith bandit” where i ∈ [k]. In each round t ∈ [T] = {1, 2, ..., T}, the policy chooses a
bandit At ∈ [k] and whose reward is denoted by random variable Xt.
We use ∆i = μ* - μ% to represent the suboptimality gap for Bi, and Ni(t) to represent the number of times
bandit i gets chosen till t. The regret over T rounds is
kt
RT = X∆iE[Ni(T)] , where Ni (t) = XI[As = i]	(1)
i=1	s=1
which serves as the main metric for stochastic bandit policies. A policy is called asymptotically optimal if
RT	2
T→∞ log(τ) =J^0 工	()
: i>
(Lai & Robbins, 1985; Burnetas & Katehakis, 1997) show that the above forms a regret upper bound for
all consistent policies. A policy is called sub-UCB (Lattimore, 2018), which is a stricter requirement than
asymptotically optimality, if there exists universal constants C1 , C2 > 0, such that the regret can be finitely
bounded as	k
Rt ≤ Cl X ∆i + C2	X lo∆(n)	(3)
∆i
i = 1	i:ʌi (μ)>0
The UCB policy works by choosing bandit At such that
At = argmax μi(t - 1)+ J 2;og(t -11)	(4)
i∈[k]	Ni(t - 1)
where the first term μi(t) = (PC=ι I(Ac = i)Xc)∕Ni(t) represents the estimation of μi at time t, and the
second term vz2 log(t)∕Ni(t) denotes the confidence bound. UCB satisfies Sub-UCB requirement (Auer
et al., 2002) and is an anytime policy i.e. selection of the optimal bandit in Eq. 4 does not depend on T. 3
3 Related Work
Multi-armed bandits. MAB (Slivkins, 2019) is a simple yet powerful framework for decision making under
uncertainty. There are many categories of MAB problems including stochastic bandits proposed by (Gittins,
2
Under review as a conference paper at ICLR 2022
1979; Lai & Robbins, 1985; Katehakis & Veinott Jr, 1987), as the most classical one, where it is assumed that
∀t, rti are samples drawn from a stationary sub-gaussian distribution bound to bandit i. Here rti represents
the reward at time t on arm i. Adversarial bandits (Auer et al., 1995) assumes that for ∀t, rti do not have to
belong to any stationary distribution and can be set by an adversary. It is common to model rti as a secret
codebook set by an enemy who knows the policy before playing. Contextual bandits (Langford & Zhang,
2007) introduces an observable context variable and assumes rti is drawn a distribution parameterized by
both bandit i and a context variable. There are many other variants under the MAB framework, whose details
are beyond the scope of this paper. In this paper, we focus on the stochastic bandits problem.
UCB formulation for stochastic bandits. Since being first proposed, UCB (Auer et al., 2002) has received
strong research interest and several variants of the original UCB policy have been proposed. For example,
KL-UCB (Garivier & CaPPa 2011; CaPPe et al., 2013) and KL-UCB++ (Menard & Garivier, 2017) transform
the UCB policy as a procedure that calculates the best possible arm using a Kullback-Leibler divergence
bound at each time steP. UCBV-Tune (Audibert et al., 2009) incorPorates the estimated variance of reward
distribution instead of assuming unit variance.
In the context of this PaPer, several Previous authors have ProPosed formulations which redesign the confidence
bound of standard UCB as shown in the term of Eq. 4. MOSS (Audibert & Bubeck, 2010) makes the
confidence bound dePend on the number of Plays for each bandit by rePlacing log(t) with log(t/Ni (t)) in
Eq. 4, and Policies similar to MOSS include OCUCB (Lattimore, 2016) and UCB* (Garivier et al., 2016).
UCBf (Lattimore, 2018) improves upon the previous ones significantly by designing a more advanced log
function comPonent.
ComPared with these Previous aPProaches, our method works differently by extending the denominator term
rather than the log function. It turns out to be intuitive in foresight and delivers strong Performance. Moreover,
instead of being non-Parametric like most of the above methods, our Policy has Parameters which are tunable,
which allows that the formulation of our method to encomPass a family of Policies.
4 Method
4.1	Intuition
The core idea of UCB-DT is that when a bandit is selected, instead of increasing the confidence bounds of all
other bandits uniformly, we increase them more for bandits which are similar to the current chosen bandit and
vice versa. The intuition is that a similar bandit has a higher chance to be equally good as the current chosen
one. As a result, this strategy will naturally lean towards the oPtimal bandit and sPend exPloration budget on
similarly good bandits and save unnecessary trials with Poor bandits.
To realize this idea, We start by looking at the confidence bound term νz2 log(t)∕Ni(t) for Bi in Eq. 4.
SuPPose there exists a distance measure d(i,j) ranges between [0, 1] that comPares the distance between Bi
and Bj . Then We find that the above idea can be imPlemented by rePlacing Ni (t) as
Ni(t) ⇒ Ni(t) +	d(i,j)Nj(t)
j∈[k],j 6=i
(5)
This modification can shrink the confidence bounds of bandits Which are distant from others, While maintaining
the confidence bounds of those ones that are closer for further exPloration. Furthermore, this modification
elegantly dePicts the Poles of exPloration and exPloitation as beloW:
Exploration When d(i, j) ≡ 0, Eq. 5 Will degrade to the vanilla UCB case, Which is an "oPtimistic" Policy
and encourages exPloration.
3
Under review as a conference paper at ICLR 2022
PUnomOQUOPyUOU
Distance to B1
(a) Given no prior knowledge, the
policy assumes B1 , B2 , B3 have no
difference, dt(1, 2) = dt (1, 3) =
0, which renders large confidence
bounds and promote exploration.
(b) Learning that B3 is far from B1 ,
dt(1, 3) will be close to 1, which in-
hibits its confidence bound to grow
larger. Thus, the policy will focus on
comparing B1 and B2.
t》0	1 lB2l lB3 I
0	0.2	0.4	0.6	0.8	1
Distance to B1
(c) Given the expanding property,
dt (1, 2) will grow larger even if
B1 , B2 are close. So the confidence
bound of B2 will cease to dominate
eventually, policy can focus on B1 .
Figure 1: A visual demonstration for our intuition with an example B1,B2, B3 whose μι > μ2>μ3 and
μι = μ2 + e.
Algorithm 1: UCB distance tuning (UCB-DT)
input : distance measure dt
2 o	∕2log(t- 1)
At = arg max ^i(t — 1)+ J --------------
i∈[k]	y JNi(t - 1)
where Nei(t) =Ni(t) +	X dt(i,j)Nj(t)
j∈[k],j 6=i
Exploitation When d(i, j) ≡ 1, Eq. 5 will degrade to greedy case because Ni (t) = t and log(t)/t can
rapidly converge to 0, which purely exploits.
These two special cases correspond exactly
to the simple and commonly recognized
truth: In the stochastic bandit problem, you
shall explore just enough to find the right
bandit, then exploit that as long as you can.
In our formulation, we can model this tran-
sition from exploration to exploitation by
customizing d. It indicates that if we could
find a transition from d ≡ 0 to d ≡ 1, like
expanding d from the origin to a unit circle,
then we can realize the above truth under the
framework in Eq. 5.
Therefore, we write d as time dependent as dt and condense the above findings in Alg. 1. An example is
provided to demonstrate our ideas in Fig. 1. A specific instance of d will be introduced in Sec. 4.2.
4.2	Bandit Distance
We design the following distance which composes UCB-DT(μ).
UCB-DT(μ)
dt(i,j ) = ∣μi(t) — μj (t)|1/bYNi⑴」	(6)
First, it directly measures the distance between two bandits. Second, the more often a bandit gets pulled, the
closer its distances from all other bandits will approach the maximum value of 1, which means that the policy
4
Under review as a conference paper at ICLR 2022
for this bandit transitions deeper into exploitation from exploration. Here γ is a speed parameter to control
the transition rate. It could be pointed that the UCB-DT(μ) will not work properly for bandits whose μi》1
because the distance will saturate. But we argue that it is common practice to use normalization to bypass this
constraint. Thus, we keep the above design for simplicity.
In Appendix B, we provide more designs for d and analyze their characteristics. In Fig. 2, we visualize the
distance versus Ni using different Y and ∣μi 一 μ^j |.
UCB-DT(μ) Distance For Different ∣^i 一 μj∣
UCB-DT(μ) Distance For Different Y
1.0
0.8
0	200	400	600	800	1000	0	200	400	600	800	1000
Ni	Ni
Figure 2: Visualization of the Distance in Eq. 6. Y is set to 0.02 in the left figure, and 向 一 μj ∣ is set to 0.2
in the right figure. It can be clearly seen that the greater the difference between the mean rewards of two
bandits is, the faster that the distance is expanded from 0 to 1. The rate of convergence of d to 1 can also be
controlled by increasing Y. The curves are jagged because of the floor operation bYNic.
4.3	Under Exploration Analysis
As compared to standard UCB, our proposed formulation UCB-DT performs less exploration. In this section,
we conduct an analysis based on a novel concept called Exploration Bargain Point to show that our method
can always give better performance than UCB. Based on our analysis, we also provide practical guidelines on
how to set the parameter Y .
In the following discussion, we assume a scenario of two bandits where μι > μ2, and we also assume that
N1 ≥ N2 . Since we only have two bandits, we can regard N2 (T) as the exploration budget spent till time T,
and T = N1 + N2 . We conduct our analysis by hindsight2 in the context of standard UCB.
4.3.1	Exploration Full Point
If we want to explore enough to ensure P (AT = 1) ≥ 1 一 δ, this implies that we recognize the optimal
bandit as the dominant choice. Therefore, based on Eq. 17 which is discussed in the formulation of standard
UCB in Appendix C, we have:
P
2log
n
(δ) ≤ δ"2
≤δ
(7)
2“By hindsight” means we look back at a policy’s decisions from time T .
5
Under review as a conference paper at ICLR 2022
where y 2 log (1) is the largest possible deviation of mean estimation. Since We have δ = 1/t in UCB, We
can solve for N2(T) when equality holds for the argument of P in Eq. 7. To simplify notation, we skip T in
the argument of N2 :
N ιog(T =δ22
Nfuii = N2 = *
(8)
Eq. 8 carries physical meaning, as it implies that if We explores for Nfuii times, then the confidence bound
Will shrink beloW to half of the suboptimality gap ∆2. In this case We can safely choose B1, and any more
exploration is completely unnecessary. To put in a succinct manner of speaking, by exploring Nfuii times
according to the above equation, We fulfill the confidence bound of UCB.
Therefore, We Write N2 in Eq. 8 as Exploration Full Point With Nfuii . Let Gf uii denotes the expected
cumulative reWard3 * at Nf uii :
Gfuii = (T — Nfuii )μι + Nfuiiμ2
(9)
4.3.2	Exploration Bargain Point
The key question is Whether We could stop exploring before Nfuii and still achieve better performance? The
trade-off is that We have a smaller N2 by exploring less, and the probability of recognizing the non-optimal
bandit as the dominant choice P(AT = 2) = δ Will groW larger and become non-negligible. Therefore, With
a slight abuse of notation, We can Write a loWer bound for expected cumulative reWard as G(N2):
inf P (AT =1)
z-}^―{
G(N2) ≥ ((T - N2) μι + N2μ2)	(1 - δ) +
|----------{-----------}
Correctly choose B1 as the best one
sup P (AT =2)
((T — N2) μ2 + N2μ1)
|-----------V------------}
Mistakenly choose B2 as the best one
z}|{
(10)
δ
where δ = e-N22/8 is from Eq. 8
Next, to ensure we can achieve better or at least the same performance as Nfuii , we write
G(N2 ) ≥ Gfuii
(11)
Then, to find out the boundary condition where the same performance are achieved, we can let Gf uii in Eq. 9
to equal the right hand side of the Eq. 10.
Gfuii = ((T — N2) μι + N2μ2)(l — δ) + ((T — N2) μ2 + N2μ1) δ	(12)
We write the solution of Eq. 12 as Exploration Bargain Point with Nbargain = solution{Equation 12}
N2
because we achieve at least the same expected reward by only exploring to the point of Nbargain. We continue
the discussion of the form of the solution in Appendix D.
3For ease of analysis, we use reward instead of regret for under exploration analysis. The same conclusion holds if
regret is used in this analysis.
6
Under review as a conference paper at ICLR 2022
Figure 3: Examples of the relationship between expected cumulative reward G(N2) and exploration budget N2
Determining γ with Nbargain. In Fig. 3, we visualize the relationship among Nfull, Nbargain, and G(N2 )
by examples. It is fascinating to see that the optimal exploration point is located between Nbargain and Nfull
because of the concavity of the subgaussian reward distribution. Therefore, our method can always perform
better than UCB as long as γ is set to 1/Nbargain in Eq. 6. It ensures that after exploring beyond Nbargain
(red dot), the distance in Eq. 6 will become larger than 0, which makes the confidence bounds of all bandits
in Alg. 1 smaller than that of UCB. By having smaller confidence bounds, our policy will under-explore
compared to UCB and stop before the Nfull (green dot)4. Thus, the final N2 will lie between Nbargain and
Nfull, where G(N2) > Gfull. Moreover, as shown by Eq. 19 in Appendix D, Nbargain only depends on
suboptimality gap ∆2 and time horizon T, and does not rely on the actual values of μ1,μ2. We argue that
domain knowledge could be used to estimate the difference between optimal bandit and other ones in practice
and thereby estimate γ .
Existence of Nbargain. Nbargain always exists and is less than Nfull as long as μι > μ2, and this can be
proven by contradiction. Because of the concavity of subgaussian distribution, the only possible case where
this condition is not satisfied is when Nbargain = Nfull, in which the Gfull (orange curve) becomes the
tangent line of G(N2) (blue curve). From Eq. 12, we get
((T - Nfull) μι + Nfullμ2) = ((T - Nfull) μι + Nfull快2)(1 - δ) + ((T - Nfull) μ2 + NfulιμI) δ (13)
It follows directly that δ = 0. Since δ = e-N2ʌ2/8 from Eq. 10, We get N = inf. Then, from Eq. 8,
N = inf → ∆2 = 0. However, we assume ∆? = μι - μ2 > 0. So Nbargain must always exist.
Implications. Exploration Bargain Point describes exactly the over-exploring nature of UCB. We could use
Nbargain as an anchor to optimize the UCB method:
•	It allows UCB practitioners to early stop and prevent over-exploration. We summarize this policy as
UCB-then-Commit in Appendix B. In Table 2, we see that UCB-then-Commit outperforms UCB in
most experiments but is never as good as UCB-DT(μ).
•	Nbargain helps US understand when UCB-DT(μ) crosses into the optimal territory to get better
rewards than UCB.
This insight is not available using traditional regret bound analysis. We therefore believe that our analysis
tool provides a novel and more intuitive perspective on analyzing UCB-based methods.
4A policy could stop before Nbargain if γ is too high, while a γ too low causes the policy to approach Nfull.
7
Under review as a conference paper at ICLR 2022
4.4 Regret Analysis
In addition to our analysis in Sec. 4.3, we offer a finite time regret bound analysis for UCB-DT following
standard practice. For the sake of clarity of notation, We write Ni(t) as Ni(n). Suppose μι > μ2 > .. > μk,
we can infer from Eq. 1 that the task of bounding Rt can be translated as bounding E [Ni(t)] , i ∈ [k], i 6= 1.
The main idea is to divide the analysis into two cases when we choose the suboptimal bandit over the optimal
one:
(A) μι is under estimated, so the optimal bandit Bi appears worse.
(B) μi,i ∈ [k],i = 1 is over estimated, so the non-optimal bandit Bi appears better.
n
Ni(n) = XI {At = i}
t=1 ≤ X Jμι⑴ + ]	S2Ngf≤μ1-ε)+Xi(μi(t)+S2H≥μ1-εandAt=i)	(14) V	} S	V	} (A)	(B)
ClLL 1	τΓr / > ∖ 、 nr / > ∖	.1 r∙	1	/-γa∖ F	1	∙ τΓr / > ∖	∙ .ι nr / > ∖ ι ♦ ι
Based on Eq. 5 we have Ni(t) ≥ Ni(t), we can therefore relax (B) by replacing Ni(t) with Ni(t), which
2(log t+√π log t+1)
(∆i-ε)2
makes it identical to the case of UCB and (B) can be bounded by 1+
according to (Lattimore
& SzePeSV虹i, 2020). At the same time, while Ni(t) ≤ t, we cannot apply a similar procedure to (A) because
directly replacing Ni(t) with t will make (A) grow much faster and violate the sub-UCB condition.
Assumption 1 There exists a time step τ, which ensures the policy chooses A1 often enough:
T = min < t ≤ T : sup ∣μι(s) 一 μι| < ε
s≥t
If we can leverage assumption 1, then a simple observation is that:
X I [μi⑴ + J ^g[ ≤ μ1 - ε f ≤ T + X I{μ1 - μ1(t) ≥ ε + δ}
t=1	N1 (t)	t=τ +1
(15)
It is straightforward to see that P(μι - μι(t) ≥ ε + δ | t > τ)=0, where，2log(t)/t is replaced with
δ > 0. Therefore, (A) can be bounded by T. Furthermore, if we let bandit reward distribution to be gaussian,
then by the concentration Lemma 1 in (Lattimore, 2018), E[τ] ≤ 1 + 9∕ε2. Thus the regret of UCB-DT
satisfies the sub-UCB requirement.
In the case of UCB-DT(μ), we can satisfy the assumption 1 as long as Y is small enough, such as 1/Nbargain
according to Sec. 4.3.2. In fact, UCB-DT can represent a family of policies through parameterization of d,
and the regret bound here only describes the boundary behavior as it approaches the standard UCB. Based
on our previous analysis, there exists a parameter space of d which can be used to customize the policy to
achieve better exploration-exploitation trade-off. Therefore, instead of diving into a detailed regret expression
of any specific form, we provide a general analysis without explicitly including this degree of freedom.
5 Numerical Experiments
We compare UCB-DT to UCB (Auer et al., 2002), UCBf (Lattimore, 2018), UCBV-Tune (Audibert et al.,
2009), KL-UCB (Garivier & Capp6, 2011; CaPPe et al., 2013)and KL-UCB++ (Menard & Garivier, 2017).
8
Under review as a conference paper at ICLR 2022
Among these methods, our main comparisons are done for UCB, UCBf and UCBV-Tune. UCB-DT can also
be extended to support variance adaption. The remaining two methods, KL-UCB and KL-UCB++, which
deliver excellent performance, are non-anytime policies. Because of these structural differences, we do not
regard them as our main comparisons, however we still keep them as important references.
Experiment	KL-UCB	KL-UCB++	UCBV-Tune	UCBf	UCB	UCB-DT(μ)
B5 *	41.73	38.03	986^	129.01	251.28	70.95
B20 *	168.54	129.5	415.23	410.5	939.99	383.82
B(0.02, 0.01) *	28.34	22.39	64.99	131.99	249.17	21.6
B(0.9, 0.88) *	38.44	33.78	49.6	73.9	119.91	19.19
N5 *	194.82	109.75	119.97	88.3	142.21	83.65
N20	539.91	410.25	840.26	560.47	1014.27	640.52
Table 1: Expected regret of different policies at T = 20000 in each experiment. For each experiment we mark bold text
for the best result and "*" if UCB-DT outperforms UCB, UCBt and UCBV-Tune.
For thorough evaluation, we design 6 experiments as below
B5	Bernoulli reward, 5 bandits with expected rewards 0.9, 0.8, 0.7, 0.2, 0.5. This experiment is
modified from (Garivier & Capp6, 2011) by adding more bandits.
B20	Bernoulli reward with many bandits, 20 bandits with expected rewards 0.9, 0.85, 0.8, 0.8, 0.7,
0.65, 0.6, 0.6, 0.55, 0.5, 0.4, 0.4, 0.35, 0.3, 0.3, 0.25, 0.2, 0.15, 0.1, 0.05.
B(0.02, 0.01) Bernoulli reward with low means, 3 bandits with expected rewards 0.05, 0.02, 0.01. This
experiment is borrowed identically from (Garivier & Capp6, 2011).
B(0.9, 0.88) Bernoulli reward with close means, 2 bandits with expected rewards 0.9, 0.88.
N5	Gaussian reward, 5 bandits with unit variance and expected rewards 1, 0.8, 0.5, 0.3, -0.2.
N20	Gaussian reward with many bandits, 20 bandits with unit variance and expected rewards 0,
-0.03, -0.03, -0.07, -0.07, -0.07, -0.15, -0.15, -0.15, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5,
-0.5, -1, -1. This experiment is identical to the one used in (Lattimore, 2018).
We set T to 20000 and run 2000 simulations for each method on every experiment, and we set γ to 0.02 in all
experiments. For other methods, we use implementations and default parameters from (Besson, 2018). We
summarize cumulative regrets in table 1 and Fig. 4.
From our simulation results, we can see that UCB-DT is always better than UCB and outperforms UCBf,
UCBV-Tune in first 5 experiments. The underlying reason for the under performance of UCB-DT as compared
to UCBf in N20 can be attributed to the high variance of the bandit reward distribution, which makes the
estimation of distance unstable. This performance degradation can be mitigated by using a different distance
as discussed in Appendix B.
6 Conclusion
By leveraging bandit distance, we create a policy called UCB-DT, which is simple, extensible and performant.
Using our proposed framework, we propose the concept of Exploration Bargain Point to provide a new
perspective on analyzing performance of UCB-based methods. Admittedly, this work bears its own limitations.
We do not dive deeper in our regret bound analysis to describe the relationship between convergence behavior
and possible properties of d. We only study the Exploration Bargain Point in the context of our method and
standard UCB and have not applied it to other UCB-based policies. However, we believe that these issues
could be addressed in future work, and it may be promising to adopt the idea in this paper to more general
settings like adversarial bandit and reinforcement learning.
9
Under review as a conference paper at ICLR 2022
References
Jean-Yves AUdibert and Sebastien Bubeck. Regret bounds and minimax policies under partial monitoring.
The Journal ofMachine Learning Research,11:2785-2836, 2010.
Jean-Yves Audibert, Remi Munos, and Csaba Szepesvdri. Exploration-exploitation tradeoff using variance
estimates in multi-armed bandits. Theoretical Computer Science, 410(19):1876-1902, 2009.
Peter Auer, Nicolo Cesa-Bianchi, Yoav Freund, and Robert E Schapire. Gambling in a rigged casino: The
adversarial multi-armed bandit problem. In Proceedings of IEEE 36th Annual Foundations of Computer
Science, pp. 322-331. IEEE, 1995.
Peter Auer, Nicolo Cesa-Bianchi, and Paul Fischer. Finite-time analysis of the multiarmed bandit problem.
Machine learning, 47(2):235-256, 2002.
Lilian Besson. SMPyBandits: an Open-Source Research Framework for Single and Multi-Players
Multi-Arms Bandits (MAB) Algorithms in Python. Online at: github.com/SMPyBandits/
SMPyBandits, 2018. URL https://github.com/SMPyBandits/SMPyBandits/. Code at
https://github.com/SMPyBandits/SMPyBandits/, documentation at https://smpybandits.github.io/.
Apostolos N Burnetas and Michael N Katehakis. Optimal adaptive policies for markov decision processes.
Mathematics of Operations Research, 22(1):222-255, 1997.
Olivier Cappe, Aurelien Garivier, Odalric-Ambrym Maillard, Remi Munos, and Gilles Stoltz. Kullback-leibler
upper confidence bounds for optimal sequential allocation. The Annals of Statistics, pp. 1516-1541, 2013.
Aurelien Garivier and Olivier Cappe. The kl-ucb algorithm for bounded stochastic bandits and beyond.
In Proceedings of the 24th annual conference on learning theory, pp. 359-376. JMLR Workshop and
Conference Proceedings, 2011.
Aurelien Garivier, Tor Lattimore, and Emilie Kaufmann. On explore-then-commit strategies. Advances in
Neural Information Processing Systems, 29:784-792, 2016.
John C Gittins. Bandit processes and dynamic allocation indices. Journal of the Royal Statistical Society:
Series B (Methodological), 41(2):148-164, 1979.
Wolfram Research, Inc. Mathematica, Version 12.3.1. URL https://www.wolfram.com/
mathematica. Champaign, IL, 2021.
Michael N Katehakis and Arthur F Veinott Jr. The multi-armed bandit problem: decomposition and computa-
tion. Mathematics of Operations Research, 12(2):262-268, 1987.
Tze Leung Lai and Herbert Robbins. Asymptotically efficient adaptive allocation rules. Advances in applied
mathematics, 6(1):4-22, 1985.
John Langford and Tong Zhang. The epoch-greedy algorithm for contextual multi-armed bandits. Advances
in neural information processing systems, 20(1):96-1, 2007.
Tor Lattimore. Regret analysis of the anytime optimally confident ucb algorithm. arXiv preprint
arXiv:1603.08661, 2016.
Tor Lattimore. Refining the confidence level for optimistic bandit strategies. The Journal of Machine Learning
Research, 19(1):765-796, 2018.
Tor Lattimore and Csaba Szepesvdri. Bandit algorithms. Cambridge University Press, 2020.
10
Under review as a conference paper at ICLR 2022
Pierre Menard and Aurelien Garivier. A minimax and asymptotically optimal algorithm for stochastic bandits.
In International Conference on Algorithmic Learning Theory, pp. 223-237. PMLR, 2017.
Aleksandrs Slivkins. Introduction to multi-armed bandits. CoRR, abs/1904.07272, 2019. URL http:
//arxiv.org/abs/1904.07272.
Eric W Weisstein. Lambert w-function. https://mathworld. wolfram. com/, 2002.
11
Under review as a conference paper at ICLR 2022
A Regret Curves
600
200
175
150
125
100
75
Figure 4: Regret of different policies as a function of time (log scale) in experiments from Sec. 5.
KL-UCB
KL-UCB++
—UCB
UCBt
UCBV-TUne
—UCB-DT(μ)
—KL-UCB B(0.9, 0.88)
—KL-UCB++
UCB
—UCBt
—UCBV-Tune
UCB-DT(μ)
KL-UCB
KL-UCB++
—UCB
UCBt
UCBV-Tune
—UCB-DT(μ)
B More Bandit Distances
We experiment the following two additional distances:
UCB-then-Commit
dt(i,j)=	01
Ni(t) ≤ b1/YC
Ni(t) > b1/YC
UCB-DT(μ margin) d*(i,j)=(向⑴-μj(I)| - m)"γNi㈤」
We name the first strategy as UCB-then-Commit, which enables the transition from exploration to exploitation
occur based on Nbargain, thereby allowing UCB practitioners to early stop and prevent over exploration. We
also introduce a second strategy called UCB-DT(μ margin), which reduces the distance ∣μi(t) 一 μj (t) | by a
margin. This distance reduction encourages the policy to explore more among similar bandits and allows us
to better handle noisy environments with high variance and low mean. We summarize their expected regrets
in Table 2.
12
Under review as a conference paper at ICLR 2022
Experiment	UCB	UCB-DT(μ)	UCB-then-Commit	UCB-DT(μ margin)
B5	251.28	70.95	76.11	73.49
B20	939.99	383.82	436.36	415.68
B(0.02, 0.01)	249.17	21.6	58	243.04
B(0.9, 0.88)	119.91	19.19	105.7	110.01
N5	142.21	83.65	185.44	108.11
N20	1014.27	640.52	778.72	628.14
Table 2: Expected regret of UCB and UCB-DT on more distances in each experiment. γ is set to 0.02 in all
experiments, m is set to 0.05.
It is interesting to see that UCB-then-Commit generally outperforms UCB by a large margin, which indicates
the benefit of under exploration as in Sec. 4.3. UCB-DT(μ margin) appears to be more robust than UCB-
DT(μ) in noisy environments by performing slightly better in N20, where the mean is much smaller than
variance compared to other experiments.
C Formulation of Standard UCB
According to (Lattimore & Szepesvari, 2020, Chapter 7), UCB is derived from Hoeffding,s inequality on sum
of subgaussian variables. Let X1, X2, . . . , Xn be independent and L1-subgaussian random variables with
zero mean and μ = Pn=I Xt/n, then
P(μ ≥ ε) ≤ exp (—nε2∕2)	(16)
Replacing exp (-nε2∕2) with δ then we get
(17)
If we use 1/t for δ, we arrive at the formulation of UCB.
D Optimal Exploration and Exploration Bargain Point
Using our analysis in Sec. 4.3, we gain a deeper understanding as to why UCB is an "optimistic" policy.
Furthermore, we can set the derivative of the lower bound in Eq. 10 on N2 to 0 and solve for the optimal
bound, which represents the optimal exploration-exploitation trade-off point. We calculate this result in Eq.
18 with (Inc.).
μ2T — 2μ2μ1 T + μ2T — 16Wcι Qeμ1T- 1 μ2μ1T+ μ2T+1) + 16
2 (μ2 — μI) 2
(18)
where W denotes the Lambert W function (Weisstein, 2002)
13
Under review as a conference paper at ICLR 2022
In the case of Nbargain , the exact solution cannot be found analytically and we can only solve this numerically
in Fig. 3. The difficulty lies in Eq. 19, which is transcendental and has no closed form expression for N2 in
this case.
0 = e-116δ2n2 (2N2 - T) - N + 8log2T)
∆2
(19)
the solution of this equation for N2 is Exploration Bargain Point
There are interesting insights which can be derived from these equations. For example, based on Eq. 18, the
optimal exploration point NS may not be unique. It will be interesting to study the relationship between the
solutions and determine which ones have practical relevance in future work.
14