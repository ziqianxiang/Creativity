Under review as a conference paper at ICLR 2022
Learning to Coordinate in Multi-Agent Sys-
tems: A Coordinated Actor-Critic Algorithm
and Finite-Time Guarantees
Anonymous authors
Paper under double-blind review
Ab stract
Multi-agent reinforcement learning (MARL) has attracted much research attention
recently. However, unlike its single-agent counterpart, many theoretical and algo-
rithmic aspects of MARL have not been well-understood. In this paper, we study
the emergence of coordinated behavior by autonomous agents using an actor-critic
(AC) algorithm. Specifically, we propose and analyze a class of coordinated actor-
critic (CAC) algorithms in which individually parametrized policies have a shared
part (which is jointly optimized among all agents) and a personalized part (which is
only locally optimized). Such a kind of partially personalized policy allows agents
to coordinate by leveraging peers’ experience and adapt to individual tasks. The
flexibility in our design allows the proposed CAC algorithm to be used in a fully
decentralized setting, where the agents can only communicate with their neighbors,
as well as in a federated setting, where the agents occasionally communicate with
a server while optimizing their (partially personalized) local models. Theoretically,
we show that under some standard regularity assumptions, the proposed CAC al-
—5
gorithm requires O(E-2) samples to achieve an E-Stationary solution (defined as
the solution whose squared norm of the gradient of the objective function is less
than E). To the best of our knowledge, this work provides the first finite-sample
guarantee for decentralized AC algorithm with partially personalized policies.
1 Introduction
We consider the multi-agent reinforcement learning (MARL) problem, in which a common environ-
ment is influenced by the joint actions of multiple autonomous agents, each aiming to optimize their
own individual objective. The MARL (Zhang et al., 2019; Lee et al., 2020) has received significant
attention recently due to their outstanding performance in many practical applications including
robotics (Stone & Veloso, 2000), autonomous driving (Shalev-Shwartz et al., 2016) and video games
(Tampuu et al., 2017). Many efficient algorithms have been proposed (Lowe et al., 2017; Espeholt
et al., 2018; Rashid et al., 2018), but unlike its single-agent counterpart, the theoretical understanding
of MARL is still very limited, especially in settings where there is no central controller to coordinate
different agents, so that the information sharing is limited (Zhang et al., 2019).
An important subclass of MARL - the so-called cooperative MARL - has become popular recently
due to its wide applications. In the cooperative MARL, the agents aim to collaborate with each other
to learn and optimize a joint global objective. To this end, local information exchange and local
communication may be used to jointly optimize a system-level performance measure (Zhang et al.,
2018; Grosnit et al., 2021; Zhang et al., 2021; Lu et al., 2021). Next, we provide a brief survey about
related works in cooperative MARL, and discuss their settings as well as theoretical guarantees.
Related Works. The systematic study of the cooperative MARL can be traced back to Claus &
Boutilier (1998); Wolpert et al. (1999), which extended Q-learning algorithm (Watkins & Dayan,
1992) or its variants to the multi-agent setting. More recently, there are a number of works that
characterize the theoretical performance of cooperative MARL algorithms in a fully observable,
decentralized setting (Kar et al., 2012; Zhang et al., 2018; Doan et al., 2019; Wang et al., 2020). In
such a setting, the agents are connected by a time-varying graph, and they can only communicate
with their immediate neighbors. Each agent observes the global state of the networked system and
1
Under review as a conference paper at ICLR 2022
independently executes an action based on its own policy. Based on the joint actions by all agents, the
system will transit into the next state and the local rewards will be received. The goal of the agents
is to cooperatively maximize certain global reward, by communicating local information with their
neighbors. Under the above cooperative MARL setting, there are several lines of works which studied
different problem formulations, proposed new algorithms and analyzed their theoretical performance.
The first line of works about the coorperative and fully observable MARL has focused on developing
and analyzing policy evaluation algorithms, where the agents jointly estimate the global value function
for a given policy. In Wai et al. (2018), a decentralized double averaging primal-dual optimization
algorithm was proposed to solve the mean squared projected Bellman error minimization problem. It
is shown that the proposed algorithm converges to the optimal solution at a global geometric rate.
In Doan et al. (2019), the authors obtained a finite-sample analysis for decentralized TD(0) method.
Their analysis is closely related to the theoretical results of decentralized stochastic gradient descent
method on convex optimization problems (Nedic et al., 2010).
However, the problem becomes much more challenging when the agents are allowed to optimize their
policies. A recent line of works has focused on applying and analyzing various policy optimization
methods in the MARL setting. In Zhang et al. (2018), the authors extended the actor-critic (AC)
algorithm (Konda & Tsitsiklis, 2000) to the cooperative MARL setting. The algorithm allows each
agent to perform its local policy improvement step while approximating the global value function.
A few more recent works have extended Zhang et al. (2018) in different directions. For example in
Grosnit et al. (2021), the authors considered the continuous action spaces and obtained the asymptotic
convergence guarantee under both off-policy and on-policy settings. Moreover, Zhang et al. (2021)
considered a new decentralized formulation where all agents cooperate to maximize general utilities
in the cooperative MARL system, it developed AC-type algorithms to fit this setting but still suffering
from high sampling cost in estimating the occupancy measure for all states and the nested loop of
optimization steps. A concurrent work (Chen et al., 2021) adopts large-batch updates in decentralized
(natural) AC methods to improve sample and communication efficiency, whose convergence rate
matches the analysis results of the corresponding centralized versions (Xu et al., 2020a). However,
the proposed algorithms in Chen et al. (2021) needs to generate O(-1 ln-1) samples to update
critic parameter before performing each actor update. It is worth noting, that all the above mentioned
works do not allow the agents to share their local policies.
Our Contributions. Although there have been a growing literature on analyzing theoretical aspects
of cooperative MARL, many challenges still remain, even under the basic fully observed setting.
For example, most of the cooperative policy optimization algorithms, assume relatively simple
collaboration mechanism, where the agents collaborate by jointly estimating the global value function,
while independently optimizing their local policies. Such a form of collaboration decouples the
agents’ policy optimization process, and it is relatively easy to analyze. However, it fails to capture
some intrinsic aspects of cooperative MARL, in the sense that when the agents’ local tasks are similar
(a.k.a. the homogeneous setting), the agent’s policy should also be closely related to each other. Such
an intuition has been verified in MARL systems (Gupta et al., 2017; Terry et al., 2020b), multi-task
RL systems (Omidshafiei et al., 2017; Zeng et al., 2020; Yu et al., 2020), Markov games (Vadori
et al., 2020) and mean-field multi-agent reinforcement learning (Liu et al., 2020; Li et al., 2021),
where parameter sharing scheme results in more stable convergence due to the benefit of learning
homogeneity among different agents. However, it is not clear how to design and analyze more
sophisticated collaboration schemes which enable the agents to (partially) share their local policies to
help them leverage each other’s experience and build better behavior strategies.
In this work, we aim at providing better theoretical and practical understandings about the cooperative
MARL problem. In particular, we consider the setting where the agents are connected by a time-
varying network, and they can access the common observations while having different reward
functions. We propose a Coordinated Actor-Critic (CAC) algorithm, provide the finite-sample
analysis, and conduct extensive numerical experiments. Our specific contributions are given below:
•	A Generic Formulation. We develop a new formulation of the cooperative MARL problem, which
allows the agents to coordinately optimize their individual actions, by parameterizing their individual
policies into a shared part (which is jointly optimized among all agents) and a personalized part
(which is only locally optimized). The proposed formulation is general, in the sense that it can be
used to cover a number of MARL settings. It can be used in a fully decentralized setting, where the
2
Under review as a conference paper at ICLR 2022
agents can only communicate with their neighbors, as well as a federated setting, where the agents
occasionally communicate with a server while optimizing their (partially personalized) local models.
•	Finite-Time Analysis. We propose an algorithm for the generic problem setting, and show that
it requires O(E-5) samples to achieve an E-Stationary solution. When being specialized to the
decentralized setting where agents do not share the local policies, our result matches the performance
bounds recently developed for centralized AC algorithms (Wu et al., 2020; Xu et al., 2020b). To the
best of our knowledge, this is the first result that shows finite-sample guarantees for the decentralized
AC algorithm with partially shared policy parameters.
•	Empirical Studies. Finally, we conduct extensive numerical experiments, which demonstrate the
effectiveness of the proposed algorithm. Our experiments suggest that in the situations where the
agents’ tasks are homogeneous, it is advantageous to partially personalize the policies for each agent;
when the tasks’ are heterogeneous, then the agents are able to achieve satisfactory convergence results
by constructing their local policies without any parameter sharing.
Notation. k∙k is used to denote the '2 norm for vectors and Frobenius norm for matrices. Further,
We use EH to denote expectation, P(∙) to denote probability. For a square matrix A, We define
c2(A), cmax(A) and cmin(A) as the second largest, the largest, and the smallest eigenvalues, respec-
tively. Define 1 as an all one vector With appropriate size. For any matrix M, Mij denotes the
element in i-th row and j-th column of matrix M. For matrix b := [bT; bT; ∙∙∙ ; IbiN], we denote the
average of all row vectors in matrix b as bT := N ∙ 1Tb.
2	Preliminaries
In this section, we introduce the background and formulation of the cooperative, fully observable
MARL in a decentralized system.
Suppose there are multiple agents aiming to independently learn and optimize a common global
objective, and each agent can communicate with its neighbors in a network with time-varying
topology. The common environment is observable by all the agents, and it is influenced by their joint
actions. To model the communication pattern among the agents, let us define the time-varying graph
Gt = (N, Et) consisting of a set ofN nodes and a set of Et edges, with |N | = N and |E| = E. Each
node i ∈ N represents an agent and Et represents the set of communication links at time t so that the
agents are connected to their neighbors according to the links Et .
Consider the MARL problem, formulated as a discrete-time Markov Decision Process (MDP) M :=
hS, A, P, η, R, γi, where S is the finite space for global state s and Ais the finite space for joint action
a = {ai}iN=1; η(s) : S → [0, 1] denotes the initial state distribution; P(s0 | s, a) : S ×A×S → [0, 1]
denotes the transition probability; ri(s, a) : S × A → R denotes the local reward function of agent i;
γ ∈ (0, 1) is the discounted factor. Furthermore, suppose the policy of each agent i is parameterized
by θi, then θ := {θi}iN=1 denotes the collections of all policy parameters in the multi-agent system.
Then μθ(S) denotes the stationary distribution of each state S under joint policy ∏θ, and dθ(∙) denotes
the discounted visitation measure where dθ(S) := (1 - Y) P∞=0 Yt ∙ Pπθ (St = S | so 〜η). Under
the joint policy πθ, the probability for choosing any joint action a := {ai}iN=1 could be expressed as
∏θ(a∣S) := ∏i=1∏i(ai∣S,θi).
Consider the discrete-time MDP under infinite horizon, the policy πθ can generate a trajectory
T := (so, ao, s1, a1, ∙∙∙) based on the initial state so sampled from η(∙). In this work, we consider
the discounted cumulative reward setting and the global value function is defined as below:
V∏θ (s) := E	Yt ∙ r(St, at) I So = S
(1)
where we define r(St, at):= N PN=1 ri(St, at) and the expectation is taken over the trajectory τ
generated from joint policy πθ. When πθ is fixed, the value function Vπθ (S) will satisfy the Bellman
equation (Bertsekas et al., 2000) for all states S ∈ S:
Vπθ (S) =
Ea~∏θ (∙∣s),s0~P(∙∣s,a) [f(s, a) + γ ∙ V∏θ (s0)].
(2)
3
Under review as a conference paper at ICLR 2022
The objective of RL is to find the optimal policy parameter θ* which maximizes the expected
discounted cumulative reward as below:
∞	∞ tN
max J(θ) ：= Es~η(∙)[V∏θ(s)] = E XYt ∙ r(st, at) = E X N XTi(st, at) .	(3)
t=0	t=0 i=1
In order to optimize J (θ), one can compute the policy gradient (Sutton et al., 2000), expressed below:
VJ(θ) := ιγEs~dθ(∙),a~∏θ(∙∣s),sθ~P(∙∣s,a)[(F(s, a) + γV∏θ (s')Ne log∏θ(a|s)].	(4)
3	The Proposed Coordinated Actor-Critic Algorithm
3.1	The Proposed Formulation
In this section, we describe our MARL formulation. Our proposed formulation is based upon (3), but
with the key difference that we no longer require the agents to have independent policy parameters
θi . Specifically, we assume that the agents can (partially) share their policy parameters with their
neighbors. Hence, each agent will decompose its policy into θi := {θis, θip}, where the shared part θis
has the same dimension across all agents, and the personalized part θip will be kept locally. Although
such a kind of partially personalized policy structure may be relatively more difficult to analyze, it
has a number of potential advantages, as we list below:
•	A Generic Model. We use the partial policy sharing as a generic setting, to cover the full spectrum
of strategies ranging from no sharing case (θis = 0, ∀ i) to the full sharing case ((θip = 0, ∀ i)). This
generic model ensures that our subsequent algorithms and analysis can be directly used for all cases.
•	Better Models for Homogeneous Agents. When the agents’ local tasks have a high level of similarity
(a.k.a. the homogeneous setting), partially sharing models’ parameters could achieve better feature
representation and guarantee that the agents’ policies are closely related to each other. Additionally,
the shared parameters could leverage more data (i.e., data drawn from all agents) compared with
the personalized parameters, so the variance in the training process can be significantly reduced,
potentially resulting in better training performance. Such an intuition has been verified empirically in
reinforcement learning systems (Omidshafiei et al., 2017; Yu et al., 2020; Zeng et al., 2020), where
sharing policies among different learners results in more stable convergence.
•	Approximate Common Knowledge. A critical assumption often made in the analysis of multiagent
systems is common knowledge (Aumann, 1976). Intuitively, this implies agents have a shared
awareness of the underlying interaction. A key difficulty in MARL is that agents are simultaneously
learning features of the underlying environment, thus common knowledge is not guaranteed. Thus
notions of approximate common knowledge have been proposed for MARL (Schroeder de Witt
et al., 2019). By relying on (partial) policy sharing mechanism, we hope to have some degree of
approximate common knowledge and this is what facilitates coordination.
The above partially personalized policy structure leads to the following MARL formulation:
max J(θ) := Es~η(.) V∏θ (S) = E EYt ∙ r(st, at)
θ	t=0
s.t. θis = θjs if (i, j) are neighbors
(5)
where θ := {θis , θip}iN=1 is the collections of all local policy parameters θi := {θis , θip}. To cast
problem (5) into a more tractable form, we perform the following steps.
First, we approximate the global reward function for any s ∈ S and a ∈ A. Specifically, we use
the following linear function b(s, a; λ):=夕(s, a)Tλ to approximate the global reward r(s, a):=
NN PN=1 Ti(S,a), where 夕(∙，∙) : S ×A→ RL is the feature mapping. Then the optimal parameter
4
Under review as a conference paper at ICLR 2022
λ* (θ) can be found by solving the following problem:
λ*(θ) ∈ arg min Es〜*®(∙),a〜∏θ(∙∣s)
λ
1N
(N ri(s, a)-2(s, a)Tλ)2
N
arg min X Es〜*@(∙),a〜∏θ(∙∣s) [(%(s) a)一2(s, a)Tλ)[.
λ	i=1
(6a)
(6b)
Second, we approximate the global value function Vπθ (s) for any s ∈ S under a fixed joint policy
πθ. Specifically, we use the following linear function Vb (s; ω) := φ(s)T ω to approximate the global
reward function V∏θ (s), where φ(∙) : S → RK is a given feature mapping. Towards achieving the
above approximation, we can solve the following mean squared Bellman error (MSBE) minimization
problem (Tsitsiklis & Van Roy, 1997):
ω (θ) ∈ arg min Es〜μθ(∙),a〜∏θ (∙∣s),s0〜P(∙∣s,a)
ω
i=1
(7a)
arg∏un XN=IEs〜“。(.)°〜∏θ(∙∣s),s，〜p(∙∣s,a) [(ri(s, a) + YV(s0; ω) - V(s; ω))2] . (7b)
To separate the objective into the sum of N terms (one for each agent), we introduce local copies
of W and λ as {wi}N=ι, {λi}N=ι, and define their vectorized versions ω = [ωι,…，ωN]T and
λ = [λι,…，λN]t. Similarly, we also define ω*(θ) := [ω:(θ),…，ωN(θ)]T and λ*(θ):=
因⑹,…，λN (θ)]T.
Summarizing the above discussion, problem (5) can be approximated using the following bi-level
optimization problem:
max
θ
“A (∙J JN X 鼠 a; λ≈(θ)) + γ^ …⑹)
S0 〜P(∙∣s,a)
s.t.
N
ω*(θ) ∈ arg min X E	(%(s, a) + Y ∙ V (s; ωi) - V (s; ωi))2
ω -=1 S〜μθG),a〜πθ(•1S) |,
s0 〜P(∙∣s,a)
N
λ*(θ) ∈ arg min £e§〜目Θ(∙),a〜∏θ(∙∣s) (n(s, a) - b(s, a; λi)),
λ i=1
θS = θj, ω* (θ) = ωj(θ), λ* (θ) = λj (θ), if (i,j) are neighbors.
(8a)
(8b)
(8c)
(8d)
In the subsequent discussion, we will refer to the problem of finding the optimal policy θ as the
upper-level problem, while referring to the problem of finding the optimal ω*(θ) and λ*(θ) under a
fixed policy parameters as the lower-level problem.
3.2 The Proposed Algorithm
In this subsection, we first present the assumptions related to network connectivity and communication
protocols in the multi-agent systems. Then we describe the proposed Coordinated Actor-Critic (CAC)
algorithm which is summarized in Algorithm 1.
Assumption 1 (Network Connectivity). There exists an integer B such that the union of the consecu-
tive B graphs is connected for all positive integers `. That is, the following graph is connected:
(N, E(' ∙ B) ∪ E(' ∙ B + 1)…∪ E((' + 1)B - 1)), ∀ ' ≥ 1
where N denotes the vertice set and E(t) denotes the set of active edges at time t.
Assumption 2 (Weight Matrices). There exists a positive constant c such that Wt = [Wtij] ∈ RN ×N
is doubly stochastic and Wtii ≥ c for all i ∈ N. Moreover, Wtij ∈ [c, 1) if (i, j) ∈ E(t), otherwise
Wtij = 0 for all i, j ∈ N.
5
Under review as a conference paper at ICLR 2022
Algorithm 1 Coordinated Actor-Critic (CAC) Algorithm
1:	Input: Parameters {αt}tT=-01, {βt}tT=-01, {ζt}tT=-01. Initialize θi,0, ωi,0, λi,0 for all i ∈ N
2:	for t = 0, 1, . . . , T - 1 do
3:	Data Sampling: St 〜μe±(∙), at := {ai,t 〜∏i(∙∣st, θi,t)}N=ι, st+ι 〜P(∙∣st, at)
4:	Consensus Step: Uet = Wt ∙ ωt, λt = Wt ∙ λt and θt := Wt ∙ θt
5:	for i ∈ N do
p	TT
6:	Construct θ%,t = {θS,t,θp,t} and update δi,t = r%,t + Y ∙ φ(st+ι)Tωi,t - φ(st)Tωi,t
7:	ωi,t+1 =	πRω(ωi,t + Bt	∙ δi,t ∙ φ(St))
8:	λi,t+1 =	πRλ 卜 i,t + ζt	∙ (ri,t - (P(S t,	at )T λi,t) ∙ (P(St,	a∕)
9:	θi,t+ι = θi,t+αt (p(st, at)T λi,t+γφ(st+ι)T ωi,t-φ(st) ωj N θi log ∏i(αi,t∣st ,θi,t)
10:	end for
11:	end for
Assumption 1 ensures that the graph sequence is sufficiently connected for each agent to have repeated
influence on other agents. Assumption 2 is standard in developing decentralized algorithms (Nedic
et al., 2009), which could guarantee consensus results for shared parameter in each agent converging
to a common vector.
After presenting the assumptions related to the network topology in the decentralized system, we
are able to introduce the proposed CAC algorithm. The CAC algorithm takes two main steps, the
policy optimization step (which optimizes θ), and policy evaluation step (which approximately solves
the lower-level problem in (8)), as We describe below. For simplicity, We denote r(st, at) as 尸t and
ri(St, at) as ri,t.
Policy Optimization. In this step, the agents optimize their local policy parameters, while trying to
make sure that the shared parameters are not too far from their neighbors.
Towards this end, each agent i first produces a locally averaged shared parameter by linearly
combining with its neighbors’ current shared parameters. Such an operation can be expressed as
θs := Wt ∙ θs	(9)
where θt := [θS,t,θ2,t,…；θN t]T ∈ RN×H is a matrix which stores all parameters {θst}N=ι, and
θt is defined similarly. In the decentralized setting, the global reward rt and the global value function
V∏θt (∙) are not available for each agent i. Instead, the agents can locally estimate the global reward
and the global value function using some linear approximation, evaluated on their local variables, as
described in the previous subsection. As shown in line 11 of Algorithm 1, in a decentralized system,
we consider the policy optimization step for each agent as below:
θi,t+1 := θi,t + αt ∙ δi,t ∙ Nθi log ni(ai,t|st, θi,t), ∀i ∈ N	(IO)
where bbi,t := b(st, at； λi,t) + Y ∙ V(st+1； ω*t) — V(st ω*t)∙	(11)
Policy Evaluation. Next, we update the local parameters λi,t and ωi,t, which parameterize the global
reward function and global value function. Towards this end, the parameters λi,t and ωi,t will be
updated by first averaging over their neighbors, then performing one stochastic gradient descent step
to minimize the local objectives, which are defined as in (8b) - (8c) and under consensus constraints
(8d). That is, we have the following updates for λt and ωt :
λt = Wt ∙ λt, λi,t+1 = πRλ 卜i,t + ζt ∙ (ri,t - r(st, at； λi,t)) ∙ Nλir(st, at； λi,t)^ ,	(12)
ωt = Wt ∙ ωt, ωi,t+ι = ∏Rω (^t,t + βt ∙ δi,t ∙ NωiV(StMi,t))	∀i ∈ N	(13)
where we define δt,t := rt,t + Y ∙ V (St+i； ωt,t) - V (St； ωt,t). Moreover, ∏Rω (∙) and 口以入(∙) are the
projection operators, with Rω and Rλ being the predetermined projection radii which are used to
stabilize the update process (Tsitsiklis & Van Roy, 1997). Please see lines 8-10 in Algorithm 1.
6
Under review as a conference paper at ICLR 2022
4 Theoretical Results
In this section, we first present Assumptions 3 - 4 about reward function and linear approximations
for policy evaluation. Then we show our theoretical results for the proposed CAC algorithm.
Assumption 3 (Bounded Reward). All the local rewards ri(s, a) are uniformly bounded, i.e., there
exist constants Rmax, for all i ∈ N and s ∈ S such that |ri(s, a)| ≤ Rmax.
Assumption 4 (Function Approximation). For each agent i, the value function and the global
reward function are both parameterized by the class of linear functions, i.e., Vb (s; ωi) := φ(s)T ωi
andb(s, a; λi) := φ(s, α)Tλ% where we denote φ(s) := [φι(s),…，φκ(s)]T ∈ RK and φ(s, a)=
[夕ι(s, a),…，夕l(s, a)]T ∈ RL are thefeature vector associated with S and (s, a), respectively. The
feature vectors φ(s) and 夕(s, a) are uniformly boundedfor any S ∈ S, a ∈ A, i.e., kφ(s)k ≤ 1 and
k2(s, a)k ≤ 1. Furthermore, constructing thefeaturematrix Φ ∈ RlSl×K which has [φk(s), S ∈ S]T
as its k -th column for any k ∈ K. Also constructing thefeature matrix Ψ ∈ RlSHAl×L which has
[夕ι(s), s ∈ S]T as its '-th column for any ' ∈ L. Then, we further assume both Φ and Ψ havefull
column ranks.
Assumption 3 - 4 are common in analyzing TD with linear function approximation; see e.g., Konda &
Tsitsiklis (2000); Bhandari et al. (2018); Wu et al. (2020). With global observability, each agent could
construct linear function approximations of the global value function and global reward function.
Under these assumptions, it is guaranteed that there exist unique optimal solutions λ* (θ) and ω* (θ)
to approximate the global reward function in (6) and the global value function in (7) with linear
functions. It is crucial to have the properties of unique optimal solutions in λ*(θ) and ω*(θ) for
constructing the convergence analysis of policy parameters θ.
Due to space limitation, we relegate remaining technical assumptions (i.e., Assumptions 5 - 6) to
Appendix C and technical lemmas to Appendix D. We first present the convergence speed of the
variables {ωt} and {λt} for the policy evaluation problem defined in (8b) - (8d). Please see Appendix
G for the detailed proof.
Proposition 1. Suppose Assumptions 1 - 6 hold. For any iteration t, by selecting stepsizes
α0	β0
αt = τσι，βt = τσ2，Zt
G
T σ2
where 0 < σ2 < σ1 < 1 and α0 , β0, ζ0 > 0 are some fixed constants, the following holds:
T-1 N
T XX (E kωi,t- ω*(θt)k2J + E [kλi,t - λ*(θt)∣2
=O(T-1+σ2) + O(T-σ2) + O (Tσ2-2σ1) + O(T-2σι+2σ2) + O(T-2+2σ2) + O(T-2σ2)
where the expectation is taken over the data sampling procedure as shown in line 3 of Algorithm 1.
Compared with existing works (Wai et al., 2018; Doan et al., 2019) which established finite-time
convergence guarantees for decentralized policy evaluation problems under the fixed policy, our
results in Proposition 1 are analyzed in a more challenging situation where both policies and critics
are updated in an alternating manner. Here, we must set σ1 > σ2 to ensure that the relation above is
useful. This is reasonable since the optimal critic parameter ω*(θt) is constantly drifting as the policy
parameters θt changes at each iteration, so the actor should update slowly compared with the critic.
Next, We study the convergence rate of policy parameters. We define Q := I 一 N 11t and define
the average gradient of shared policy parameters as Vθs J(θ) := NN PN=I Vθ∣ J(θ). We will show
that after averaging over the iterations, the expected stationarity condition violation for the policy
optimization problem defined in (8a) is small. Please see Appendix H for the proof.
Proposition 2. Under the same setting as Proposition 1, there exist two constant error term app > 0
and sp > 0. Algorithm 1 generates a sequence of policies {θt}, which satisfies the following:
1
T
T-1
N
E E ∣Q∙ θSk2 + N ∙ E ∣Vθs J(θt)k2 +£E ∣Vθp J(θt)k2
t=0
i=1
=O(T-1+σ1) + O(T-σι) + O(T-1+σ2) + O(T-σ2) + O (Tσ2-2σι) + O(T-2σ1 +2σ2)
+O(T-2+2σ2)+O(T-2σ2)+O(app+sp).
7
Under review as a conference paper at ICLR 2022
Algorithm 2 Double Sampling Procedures
Input: Parameters {ωi,t}iN=1, {λi,t}iN=1, {θi,t}N=1.
Double i.i.d. Sampling:
1)	Sample St ~ μet (∙), at：= {ai,t ~ ∏i(∙∣st, θi,t)}N=ι, st+1 ~ P (∙∣st, at)
2)	Sample St 〜d%(∙),由：={3〜∏i(悒,a,t)}N=ι,印〜P(•回,山)
The approximation error app and sampling error sp are defined in Appendix E. A few remarks
about the above results follow. First, one challenge in analyzing the convergence of Actor-Critic
algorithms is that the actor and critic updates are typically sampled from different distributions (i.e.,
the distribution mismatch problem). To see this, note that to obtain an unbiased estimator for the
policy gradient in (4), one needs to sample from the discounted visitation measure dθ(∙), while to
obtain an unbiased estimator for the gradient of the MSBE in (7) (which is utilized to update the
critic parameters), one needs to sample from the stationary distribution μe(∙). However, standard
implementations for AC methods in practice only use one sampling procedure for both actor and critic
updates (Mnih et al., 2016; Shen et al., 2020). Therefore, the mismatch between the two sampling
distributions inevitably introduces constant biases, and this is where the error term sp comes from.
Second, at each local agent i, the value function Vπθ (S) is approximated by φ(S)T ωi and the global
reward function is approximated by 夕(s, a)Tλi. Due to the linear approximation, the approximation
error is inevitable in the convergence analysis. Here, we use a constant term app to quantify the
approximation error due to utilizing linear function for policy evaluation.
By combining previous Propositions, and by properly selecting the stepsize parameters σ1 and σ2,
we show the main result as below. In Appendix E, we will present more discussion about a special
case where there is no policy parameter sharing.
Theorem 1. (Convergence of the CAC Algorithm) Suppose Assumptions 1 - 6 hold. Consider
Algorithm 1 with partially shared policy parameters θ := ∪N=ι{θS,θp}. Let σι = 3 and σ2 = 2, it
holds that:
T-1 N
Txx E kωi,t- ω*(θt)∣2 + E k%,t- λ*(θt)∣2 ) = O(T-2),
t=0 i=1
1 T-1
IE E kQ∙ θSk2 + N ∙ E
T t=0
N
kVθsJ(θt)k2 + XE kVθpJ(θt)k2
O(T 5 ) + O(Capp + tsp).
i=1
As mentioned before, the sampling error sp arises because there is a mismatch between the way
that estimators of the actor’s and the critics’ updates are obtained. To remove the sampling error,
one can implement separate sampling protocols for the critic and the actor. More specifically, we
can use two different i.i.d. samples at each iteration step t: 1) xt := (st, at, st+ι) where St 〜 μe(∙),
at ~ ∏θ(∙ | St) and st+ι ~ P(∙ | st, at); 2) Xt := (St, at, 西十。where st ~ dθ(∙), at ~ ∏θ(∙ | St)
and St+1 ~ P(∙ | St, at); see Algorithm 2. Then Xt and Xt will be utilized in policy evaluation and
policy optimization, respectively. The corollary below shows the convergence result for the modified
CAC algorithm. Please see Appendix I for the proof.
Corollary 1. (Convergence under double sampling) Under the same setting as Theorem 1, consider
CAC with the double sampling procedures in Algorithm 2. The following result holds:
T-1 N
⅛XX E ∣3,t- ω*(θt)∣2 + E k%,t- λ*(θt)k2 ) = O(T-2),
t=0 i=1
N
T-1
T-1	N
T X(E kQ∙ θSk2 + N ∙ E kVθsJ(θt)k2 + XE kVθp J(θt)k2
O(T-5) + O(eapp).
t=0
i=1
5 Numerical Results
In this section, we present our simulation results on two environments: 1) the coordination game
(Osborne & Rubinstein, 1994); 2) the pursuit-evasion game (Gupta et al., 2017), which is built on the
PettingZoo platform (Terry et al., 2020a). Detailed experiment settings are present in Appendix A.
8
Under review as a conference paper at ICLR 2022
Figure 1: Simulation Results. The averaged reward versus the learning process. We present the
algorithm performance on the coordination game and on the pursuit-evasion game. The performance
is averaged over 10 Monte Carlo runs.
Coordination Game: In this setting, there are N agents staying at a static state and they choose
their actions simultaneously at each time. After actions are executed at each time t, each agent
i receives its reward as: ri,t = (ai,t - 3.5)2 + Pj 6=i I{aj,t=ai,t} + i,t where the action space is
{0,1, 2, ∙ ∙ ∙ , 7}, I{aj,t=ai,t} is an indicator function and ei,t is a random payoff following standard
Gumbel distribution. In this coordination game, there are multiple Nash equilibria where two optimal
equilibria are that all agents select a = {0} or a = {7} simultaneously. In order to obtain high
rewards and achieve efficient equilibria, it is crucial for agents to coordinate with others while only
having limited communications.
Here, the communication graph Gt between the agents is a complete graph every 5 iterations, and
is not connected for the rest of time. We compare the performance of CAC with three benchmark
algorithms: independent Actor-Critic (IAC); decentralized Actor-Critic (DAC) in Zhang et al. (2018);
mini-batch decentralized Actor-Critic (MDAC) in Chen et al. (2021). For each algorithm, we set
the actor stepsize and critic stepsize as 0.05 and 0.1. Theoretically, MDAC needs O(-1 ln -1)
batch size in its inner loop to update critic parameters before each update in policy parameters,
which is inefficient in practice. Here, we set small batch B = 5 in the inner loop for MDAC to
achieve fast convergence. The simulation results on this coordination game are present in Fig.1
(two left figures). According to the simulations, compared with the benchmarks, we see that the
CAC algorithm converges faster and has higher probability to achieve efficient equilibria due to the
use of policy sharing and coordination.
Pursuit-Evasion Game: there are two groups of nodes, pursuers (agents) and evaders. The pursuers
aim to obtain reward through catching evaders. In a two-dimensional environment, an evader is
considered caught if two pursuers simultaneously arrive at the evader’s location. In order to catch an
evader, each pursuer should learn to cooperate with other pursuers to catch the evaders. From this
perspective, the pursuers share some similarities with each other since they need to follow similar
strategies to achieve their local tasks: simultaneously catching a same evader with other pursuers.
In Figure 1 (two right figures), we compare the numerical performance of the proposed CAC algo-
rithm and two benchmarks: decentralized Actor-Critic (DAC) in Zhang et al. (2018); mini-batch
decentralized Actor-Critic (MDAC) in Chen et al. (2021). Each agent maintains two convolutional
neural networks (CNNs), one for the actor and one for the critic. Please see Figure 2 in Appendix
for the structure diagrams of actor network and critic network being used. In the CAC, two convolu-
tional layers of actor network will be regarded as shared policy parameters, and the output layer is
personalized (thus not shared).
The two sets of numerical results suggest that, when local tasks share a certain degree of similarity /
homogeneity, CAC algorithm with (partial) parameter sharing could achieve more stable convergence.
6	Conclusion
This paper develops a novel collaboration mechanism for designing robust MARL systems. Further,
it develops and analyzes a novel multi-agent AC method, where agents are allowed to (partially)
share their policy parameters with the neighbors to learn from different agents. To our knowledge,
this is the first non-asymptotic convergence result for two-timescale multi-agent AC methods. We
leave the extensions of our proposed algorithm to partially observable Markov decision process as
the future work.
9
Under review as a conference paper at ICLR 2022
References
Alekh Agarwal, Nan Jiang, and Sham M Kakade. Reinforcement learning: Theory and algorithms.
CS Dept., UW Seattle, Seattle, WA, USA, Tech. Rep, 2019.
Alekh Agarwal, Sham M Kakade, Jason D Lee, and Gaurav Mahajan. Optimality and approximation
with policy gradient methods in markov decision processes. In Conference on Learning Theory,
pp. 64-66. PMLR, 2020.
Manoj Ghuhan Arivazhagan, Vinay Aggarwal, Aaditya Kumar Singh, and Sunav Choudhary. Feder-
ated learning with personalization layers. arXiv preprint arXiv:1912.00818, 2019.
Robert Aumann. Annals of Statistics, 4(6):1236-1239, 1976.
Dimitri P Bertsekas et al. Dynamic programming and optimal control: Vol. 1. Athena scientific
Belmont, 2000.
Jalaj Bhandari, Daniel Russo, and Raghav Singal. A finite time analysis of temporal difference
learning with linear function approximation. In Conference On Learning Theory, pp. 1691-1692.
PMLR, 2018.
Tianyi Chen, Kaiqing Zhang, Georgios B Giannakis, and Tamer Bayar. CommUnication-efficient
policy gradient methods for distributed reinforcement learning. arXiv preprint arXiv:1812.03239,
2018.
Ziyi Chen, Yi ZhoU, Rongrong Chen, and Shaofeng ZoU. Sample and commUnication-efficient
decentralized actor-critic algorithms with finite-time analysis. arXiv preprint arXiv:2109.03699,
2021.
Caroline ClaUs and Craig BoUtilier. The dynamics of reinforcement learning in cooperative mUltiagent
systems. AAAI/IAAI, 1998(746-752):2, 1998.
Thinh Doan, Siva MagUlUri, and JUstin Romberg. Finite-time analysis of distribUted td (0) with linear
fUnction approximation on mUlti-agent reinforcement learning. In International Conference on
Machine Learning, pp. 1626-1635. PMLR, 2019.
Kenji Doya. Reinforcement learning in continUoUs time and space. Neural computation, 12(1):
219-245, 2000.
Lasse Espeholt, HUbert Soyer, Remi MUnos, Karen Simonyan, Vlad Mnih, Tom Ward, Yotam Doron,
Vlad FiroiU, Tim Harley, Iain DUnning, et al. Impala: Scalable distribUted deep-rl with importance
weighted actor-learner architectUres. In International Conference on Machine Learning, pp.
1407-1416. PMLR, 2018.
Antoine Grosnit, Desmond Cai, and LaUra Wynter. Decentralized deterministic mUlti-agent reinforce-
ment learning. arXiv preprint arXiv:2102.09745, 2021.
Jayesh K GUpta, Maxim Egorov, and Mykel Kochenderfer. Cooperative mUlti-agent control Using
deep reinforcement learning. In International Conference on Autonomous Agents and Multiagent
Systems, pp. 66-83. Springer, 2017.
Soummya Kar, Jose MF Moura, and H Vincent Poor. Qd-learning: A collaborative distributed strategy
for mUlti-agent reinforcement learning throUgh consensUs. arXiv preprint arXiv:1205.0047, 2012.
Vijay R Konda and John N Tsitsiklis. Actor-critic algorithms. In Advances in neural information
processing systems, pp. 1008-1014. Citeseer, 2000.
Vijaymohan R Konda and Vivek S Borkar. Actor-critic-type learning algorithms for markov decision
processes. SIAM Journal on control and Optimization, 38(1):94-123, 1999.
Jakub Konecny, H Brendan McMahan, Felix X Yu, Peter Richtarik, Ananda Theertha Suresh, and
Dave Bacon. Federated learning: Strategies for improving communication efficiency. arXiv
preprint arXiv:1610.05492, 2016.
10
Under review as a conference paper at ICLR 2022
Donghwan Lee, Niao He, Parameswaran Kamalaruban, and Volkan Cevher. Optimization for
reinforcement learning: From a single agent to cooperative agents. IEEE Signal Processing
Magazine, 37(3):123-135, 2020.
Yan Li, Lingxiao Wang, Jiachen Yang, Ethan Wang, Zhaoran Wang, Tuo Zhao, and Hongyuan Zha.
Permutation invariant policy optimization for mean-field multi-agent reinforcement learning: A
principled approach. arXiv preprint arXiv:2105.08268, 2021.
Lewis Liu, Zhuoran Yang, Yuchen Lu, and Zhaoran Wang. Decentralized policy gradient method for
mean-field linear quadratic regulator with global convergence. 2020.
Ryan Lowe, Yi Wu, Aviv Tamar, Jean Harb, Pieter Abbeel, and Igor Mordatch. Multi-agent actor-
critic for mixed cooperative-competitive environments. arXiv preprint arXiv:1706.02275, 2017.
Songtao Lu, Kaiqing Zhang, Tianyi Chen, Tamer Basar, and Lior Horesh. Decentralized policy
gradient descent ascent for safe multi-agent reinforcement learning. In Proceedings of the AAAI
Conference on Artificial Intelligence, volume 35, pp. 8767-8775, 2021.
Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Alex Graves, Ioannis Antonoglou, Daan
Wierstra, and Martin Riedmiller. Playing atari with deep reinforcement learning. arXiv preprint
arXiv:1312.5602, 2013.
Volodymyr Mnih, Adria Puigdomenech Badia, Mehdi Mirza, Alex Graves, Timothy Lillicrap, Tim
Harley, David Silver, and Koray Kavukcuoglu. Asynchronous methods for deep reinforcement
learning. In International conference on machine learning, pp. 1928-1937. PMLR, 2016.
Angelia Nedic, Alex Olshevsky, Asuman Ozdaglar, and John N Tsitsiklis. On distributed averaging
algorithms and quantization effects. IEEE Transactions on automatic control, 54(11):2506-2517,
2009.
Angelia Nedic, Asuman Ozdaglar, and Pablo A Parrilo. Constrained consensus and optimization in
multi-agent networks. IEEE Transactions on Automatic Control, 55(4):922-938, 2010.
Shayegan Omidshafiei, Jason Pazis, Christopher Amato, Jonathan P How, and John Vian. Deep decen-
tralized multi-task multi-agent reinforcement learning under partial observability. In International
Conference on Machine Learning, pp. 2681-2690. PMLR, 2017.
Martin J Osborne and Ariel Rubinstein. A course in game theory. MIT press, 1994.
Tabish Rashid, Mikayel Samvelyan, Christian Schroeder, Gregory Farquhar, Jakob Foerster, and Shi-
mon Whiteson. Qmix: Monotonic value function factorisation for deep multi-agent reinforcement
learning. In International Conference on Machine Learning, pp. 4295-4304. PMLR, 2018.
Sebastian Ruder. An overview of gradient descent optimization algorithms. arXiv preprint
arXiv:1609.04747, 2016.
Christian Schroeder de Witt, Jakob Foerster, Gregory Farquhar, Philip Torr, Wendelin Bohmer, and
Shimon Whiteson. Multi-agent common knowledge reinforcement learning. In Advances in neural
information processing systems, pp. 1008-1014. Citeseer, 2019.
Shai Shalev-Shwartz, Shaked Shammah, and Amnon Shashua. Safe, multi-agent, reinforcement
learning for autonomous driving. arXiv preprint arXiv:1610.03295, 2016.
Han Shen, Kaiqing Zhang, Mingyi Hong, and Tianyi Chen. Asynchronous advantage actor critic:
Non-asymptotic analysis and linear speedup. arXiv preprint arXiv:2012.15511, 2020.
Peter Stone and Manuela Veloso. Multiagent systems: A survey from a machine learning perspective.
Autonomous Robots, 8(3):345-383, 2000.
Tao Sun, Yuejiao Sun, and Wotao Yin. On markov chain gradient descent. arXiv preprint
arXiv:1809.04216, 2018.
Richard S Sutton, David A McAllester, Satinder P Singh, and Yishay Mansour. Policy gradient meth-
ods for reinforcement learning with function approximation. In Advances in neural information
processing systems, pp. 1057-1063, 2000.
11
Under review as a conference paper at ICLR 2022
Ardi Tampuu, Tambet Matiisen, Dorian Kodelja, Ilya Kuzovkin, Kristjan Korjus, Juhan Aru, Jaan
Aru, and Raul Vicente. Multiagent cooperation and competition with deep reinforcement learning.
PloS one, 12(4):e0172395, 2017.
Justin K Terry, Benjamin Black, Mario Jayakumar, Ananth Hari, Ryan Sullivan, Luis Santos, Clemens
Dieffendahl, Niall L Williams, Yashas Lokesh, Caroline Horsch, et al. Pettingzoo: Gym for multi-
agent reinforcement learning. arXiv preprint arXiv:2009.14471, 2020a.
Justin K Terry, Nathaniel Grammel, Ananth Hari, Luis Santos, and Benjamin Black. Revisiting
parameter sharing in multi-agent deep reinforcement learning. arXiv preprint arXiv:2005.13625,
2020b.
John N Tsitsiklis and Benjamin Van Roy. An analysis of temporal-difference learning with function
approximation. IEEE transactions on automatic control, 42(5):674-690, 1997.
Nelson Vadori, Sumitra Ganesh, Prashant Reddy, and Manuela Veloso. Calibration of shared equilibria
in general sum partially observable markov games. arXiv preprint arXiv:2006.13085, 2020.
Hoi-To Wai, Zhuoran Yang, Zhaoran Wang, and Mingyi Hong. Multi-agent reinforcement learning
via double averaging primal-dual optimization. arXiv preprint arXiv:1806.00877, 2018.
Gang Wang, Songtao Lu, Georgios Giannakis, Gerald Tesauro, and Jian Sun. Decentralized td
tracking with linear function approximation and its finite-time analysis. Advances in Neural
Information Processing Systems, 33, 2020.
Christopher JCH Watkins and Peter Dayan. Q-learning. Machine learning, 8(3-4):279-292, 1992.
David H Wolpert, Kevin R Wheeler, and Kagan Tumer. General principles of learning-based multi-
agent systems. In Proceedings of the third annual conference on Autonomous Agents, pp. 77-83,
1999.
Yue Wu, Weitong Zhang, Pan Xu, and Quanquan Gu. A finite time analysis of two time-scale actor
critic methods. arXiv preprint arXiv:2005.01350, 2020.
Tengyu Xu, Zhe Wang, and Yingbin Liang. Improving sample complexity bounds for (natural)
actor-critic algorithms. Advances in Neural Information Processing Systems, 33, 2020a.
Tengyu Xu, Zhe Wang, and Yingbin Liang. Non-asymptotic convergence analysis of two time-scale
(natural) actor-critic algorithms. arXiv preprint arXiv:2005.03557, 2020b.
Tianhe Yu, Deirdre Quillen, Zhanpeng He, Ryan Julian, Karol Hausman, Chelsea Finn, and Sergey
Levine. Meta-world: A benchmark and evaluation for multi-task and meta reinforcement learning.
In Conference on Robot Learning, pp. 1094-1100. PMLR, 2020.
Sihan Zeng, Aqeel Anwar, Thinh Doan, Justin Romberg, and Arijit Raychowdhury. A decentralized
policy gradient approach to multi-task reinforcement learning. arXiv preprint arXiv:2006.04338,
2020.
Junyu Zhang, Amrit Singh Bedi, Mengdi Wang, and Alec Koppel. Marl with general utilities via
decentralized shadow reward actor-critic. arXiv preprint arXiv:2106.00543, 2021.
Kaiqing Zhang, Zhuoran Yang, Han Liu, Tong Zhang, and Tamer Basar. Fully decentralized multi-
agent reinforcement learning with networked agents. In International Conference on Machine
Learning, pp. 5872-5881. PMLR, 2018.
Kaiqing Zhang, Zhuoran Yang, and Tamer Bayar. Multi-agent reinforcement learning: A selective
overview of theories and algorithms. arXiv preprint arXiv:1911.10635, 2019.
Kaiqing Zhang, Alec Koppel, Hao Zhu, and Tamer Basar. Global convergence of policy gradient
methods to (almost) locally optimal policies. SIAM Journal on Control and Optimization, 58(6):
3586-3612, 2020.
12
Under review as a conference paper at ICLR 2022
Figure 2: Neural Network Architecture Diagrams for the CAC Algorithm. The architecture
diagrams for actor network and critic network of algorithm CAC in the pursuit-evasion Game. (Left)
The diagram of critic network. (Right) The diagram of actor network.
Appendix
A	Experiment Details
In this section, we will present the experiment details on the pursuit-evasion game.
A.1 pursuit-evasion Game
The ‘capture’ reward for each agent is set to be 5 when a pursuer successfully catches an evader.
Moreover, the pursuer will receive a small reward signal which is set to be 0.1 when the pursuer
encounters an evader at its current location. The environment is set to be a 15 × 15 grid and this
2D grid contains obstacles where the agents cannot pass through. Hence, the global state of the
pursuit-evasion game consists of three images (binary matrices) of the size of 15 × 15. Hence, the
dimension of the global state is 3 × 15 × 15. These three images (binary matrices) respectively
present the location of the pursuers, evaders and obstacles in the two-dimensional grid. Only given
the 3-channel images as the global state, it is difficult for each pursuer (agent) to distinguish itself
with other pursuers since the 3-channel images (global state) does not directly show the ID for each
pursuer in the pursuit-evasion Game. To tackle this challenging, we center each agent’s observation
at its own location. With a large observation radius, each agent could observe the global information
in the environment.
Considering the observation of each agent is a 3-channel image, each agent respectively maintains
two convolutional neural networks (CNNs) with two convolutional layers, one max-pooling layer and
one fully connected layer for the actor and the critic. Please see Figure 2 for the structure diagrams
of actor network and critic network in algorithm CAC. The communication graph Gt between the
agents is a complete graph every 20 iterations, and is not connected for the rest of time. Hence, for
CAC algorithm, the global averaging step will be performed on the entire critic networks and the two
CNN layers of actor networks every 20 iterations. The RuLU activation function is utilized in each
hidden layer of actor network and critic network. The output of critic network approximates the value
function Vπθ (s) for all s ∈ S and the dimension of the output layer is 1. Furthermore, the output
dimension of actor network is 5 which corresponds to the number of possible actions. In each CNN,
the raw images (3-channel location matrices), whose dimension is 3 × 15 × 15, are processed by two
convolutional layers and one max-pooling layer first and then pass through a fully connected layer as
the output layer. We utilize the RMSprop optimizer (Ruder, 2016) to train neural networks, which
is a common choice in training neural networks for reinforcement learning problems (Mnih et al.,
2013). For each algorithm, we set the actor stepsize and critic stepsize as 1 × 10-4 and 1 × 10-3.
For algorithm MDAC to achieve quick convergence, we tune its batch size and set small batch B = 5
in its inner loop. The discount factor γ is set to be 0.95 in this simulation.
B Discussion: Application in Multi-Agent Settings
Here, we discuss how the proposed CAC algorithm can be used in two popular multi-agent settings:
13
Under review as a conference paper at ICLR 2022
•	Fully personalized multi-agent RL. The CAC algorithm can be applied to the special setting
where the agents do not share their policy parameters with neighbors, and no cooperation is considered
in generating the local policies. This setting has been proposed and studied in a number of existing
works, such as Zhang et al. (2018); Chen et al. (2018).
•	Federated RL. The proposed algorithm can be applied to a general federated (reinforcement)
learning setting, where the agents jointly optimize a common objective. To see the connection, let us
first describe a standard federated learning (FL) setting (Konecny et al., 2016; Arivazhagan et al.,
2019): a central controller coordinates a few agents, where the agents continuously optimize their
local parameters and perform occasional averaging steps over the parameters. It can be shown that
this protocol corresponds to a dynamic setting where Gt is a complete graph every fixed number of
iterations, while it is not connected at all for the rest of times (which is a special case of our setting,
see Assumption 1). When the network is connected, each agent could gather other agents’ models
and perform the averaging; when the network is not connected, then each agent just performs local
updates. We generalize the above FL setting to MARL in CAC algorithm.
C Technical Assumptions
Assumption 5. Define the score function ψθ (s, a) := Vθ log ∏θ (a | S). For any policy parameters
θ and θ0, and any state-action (s, a), the following holds:
kψθ(s, a) — Ψθ0(s, a)k	≤	Lψ	∙	l∣θ — θ0k,	kΨθ(s, a)k	≤ Cψ	(14a)
∣∏θ(a | s) — ∏θo(a | s)|	≤	L∏	∙	∣∣θ — θ0∣	(14b)
where Lπ , Lψ , Cψ are some constants.
Assumption 5 has been often used in analyzing policy gradient-type algorithms, for example see
Zhang et al. (2020); Agarwal et al. (2020). Many policy parameterization methods such as tabular
softmax policy Agarwal et al. (2020), Gaussian policy Doya (2000) and Boltzmann policy Konda &
Borkar (1999) satisfy this assumption.
Assumption 6. For any policy parameters θ, the markov chain under policy πθ and transition kernel
P(∙∣s, a) is irreducible and aperiodic. Then there exist constants κ > 0 and T ∈ (0,1) such that
sup dτv (Pπθ (St ∈∙∣so = s),μθ(∙)) ≤ K ∙ Tt,	Vt	(15)
s∈S
where dτv(∙) is the total variation (TV) norm; μθ is the stationary state distribution under ∏θ.
Assumption 6 assumes the Markov chain mixes at a geometric rate; see also Bhandari et al. (2018);
Sun et al. (2018).
D Auxiliary Lemmas
Lemma 1. ((Nedic et al., 2010, Lemma 1)) Let X be a nonempty closed convex set in RK, then the
following holds:
(πX [x] — X)T (X — y) ≤ —|InX [x] — xk2, ∀ X ∈ RK, y ∈ X	(16a)
k∏χ [χ] — yk2 ≤ kχ — yk2-knx [χ] — χ∣2, ∀ X ∈ RKy ∈X	(16b)
where Πχ [∙] denotes the projection operator on to the convex set X.
Lemma 2. ((Zhang et al., 2020, Lemma 3.2)). Suppose Assumption 5 holds. Then the following
holds:
∣∣VJ(θ) — VJ(θ0)k ≤ Lj ∙ ∣∣θ — θ0k,	∀ θ, θ ∈ RN×D
(17)
where J (∙) is the objective function defined in (3); LJ := RmxY)2ψ + (I+；] Rm)ax C with Lψ and
Cψ defined in Assumption 5.
Lemma 3. ((Shen et al., 2020, Lemma 4)). Suppose Assumption 5 holds. The following holds:
∣VJ(θ)k	≤Lv,	| J(θι) —	J(θ2)∣ ≤ Lv	∙	∣Θ1 —	θ2k, ∀ θ,	θ0	∈	RN×D, S ∈ S,	(18)
where the constant Lv := R-Ox ∙ Cψ, with Cψ defined in Assumption 5.
14
Under review as a conference paper at ICLR 2022
Lemma 4. ((Shen et al., 2020, Lemma 1)) Suppose Assumption 6 holds. The following holds:
dτv(μθ, dθ) ≤ 2 (logτ KT + 1-τ) (1 - Y),	∀ θ ∈ RN×D
(19)
where μθ (∙) is the stationary distribution of each state S under policy parameters θ and dθ (∙) is the
discounted visitation meausre d® (s) := (1 — Y) E∞=0 Yt ∙ Pπθ (St = S | s0 〜 η).
With the technical assumptions in C, we could bound consensus errors over the iterations. Towards
this end, let us provide some basic properties for the weight matrices. Based on Assumptions 1 -
2 which ensure the long-term connectivity and impose the underlying topology of the networked
system, we can obtain the following condition (Nedic et al., 2009, Lemma 9):
IlWt …Wt+B-ι ∙ Q ∙ ωk ≤ η ∙kQ ∙ ωk,
(20)
when ω := [ωτ; ωτ;…；ωN] ∈ RN×K; and We define Q := I - N1 ∙ 1T ∈ RN×N. Further, the
constant η in (20) is given by η := 1- - 2N2 ∈ (0, 1), where constant c is defined in Assumption 2.
Based on the above property, we have the following bounds on various consensus errors. Please see
Appendix F for the proof.
Lemma 5. Based on Assumptions 1 - 4, there exist constants LB > 0 and `p > 0 such that the
consensus errors ∣∣Q ∙ ωt∣∣ + ∣∣Q ∙ λt∣ and ∣∣Q ∙ θtk satisfy
kQsk + kQ ∙ λtk≤ * 晅J + 2Nn L(I-/ ∙总
kQ ∙ θSk≤ Pt ∙kθ≡ + -η⅞iα1 ∙ ɪ
n n ∙ (1 — P) T σ1
1 T-1
个 E kQ∙ωt∣ + ∣Q∙ λt∣
T t=0
1 T-1
个 E kQ∙ λtk2 + kQ∙ωt∣2
T t=0
O T-σ2
O(T-1) + O(T-2σ2)
T1X kQ ∙ θSk = O(T-σ1), TX kQ ∙ θSk2 =O(TT) + O(T-2σ1)
T t=0	T t=0
(21a)
(21b)
(21c)
(21d)
(21e)
where αt := TaI, βt := Te2 and Zt := T⅛∙ are three stepsizes; P := nB.
Given the fixed policy parameter θ, solving the lower level problem of (8) is equivalent to solving the
centralized policy evaluation problems, expressed in (6) and (7). Through the first-order optimality
condition, it is easy to show that w*(θ) satisfies the following condition:
1N
A(θ) ∙ ω*(θ) = N 斗(θ)	(22)
where we have defined:
A(θ) :=
Es~μθ (∙),s0~Pπθ (∙∣s) hφ(s)(φ(S)- Y ∙ φ(s'))Ti , ∀ i ∈ N,	(23a)
bi(θ) := Es 〜μθ(∙),a 〜∏θ(∙∣s) [ri (s, a) ∙ φ(s)], ∀ i ∈N.	(23b)
Under the full-rankness and bounded assumption of the feature matrices given in Assumption 4, we
can apply (Tsitsiklis & Van Roy, 1997, Theorem 2), and show that A(θ) is a positive definite matrix
for any fixed θ. Let us define
0 < ⅞min ≤ Cmin(A(θ)), 0 < Cmax (A(θ)) ≤ 毫ax, ∀ θ ∈ RN ×D	(24)
where Cmin (A(θ)) and Cmax (A(θ)) are the minimum and maximum eigenvalue of A(θ);鼎山 and
Nmax are the lower bound and upper bound on the eigenvalues of A(θ). Then we have the following
Lipschitz property of the optimal critic parameters.
15
Under review as a conference paper at ICLR 2022
Lemma 6. ((Shen et al., 2020, Proposition 2 )) Suppose Assumptions 4,5,6 hold. Let w*(θ) denote
the optimal solution in (7) to approximate value function approximation. Then the following Lipschitz
condition holds:
kω*(θι)- ω*(θ2)k ≤ Lω ∙kθι- θ2k,	∀ θι, θ2 ∈ RN×D,	(25)
where Lω := 2 ∙ Rmax ∙ |A| ∙ L∏ ∙ f≡m1n + ≡m2n ∙ (1 + Y) Vl + lθgτ KT + (1 — T)-1
E Discussion: Convergence Results
In this section, we discuss an extension of Theorem 1.
As a special case, when the agents do not share any policy parameters, that is, when θ = θp, the
resulting algorithm reduces to the standard Decentralized AC algorithm and we name it as Coordinated
Actor-Critic with no policy sharing (CAC-NPS), whose asymptotic convergence property has been
analyzed in Zhang et al. (2018). The non-asymptotic convergence rate for this algorithm can be
readily obtained from (a slightly modified versions of) Proposition 1-2. The following result states
the convergence rate for CAC-NPS, and we refer the readers to Appendix H for detailed proof steps.
Corollary 2. (Convergence of CAC-NPS Algorithm) Suppose Assumptions 1 - 6 hold. Consider
applying Algorithm 1 to a problem with fully-personalized policy parameters, that is, θ := ∪iN=1{θip}.
Setting σι = 5 and σ2 = 5, then the following holds:
T-1 N
Txx E ∣3,t- ω*(θt)∣∣2 + E k%,t- λ*(θt)∣∣2 )=O(T-2)	(26)
t=0 i=1
T-1 N
T XXE kVθi J (θt)k2J = O(T- 5 + O&pp + Esp).	(27)
The critic approximation error Eapp and the sampling error Esp are defined as follows (Wu et al., 2020;
Shen et al., 2020):
Eapp := max t
E
S 〜μθ (∙)
(S) — φ(s)T ω*
E
S 〜μθ (∙)
a 〜∏θ (∙)
r(s, a) — 夕(s, a)Tλ*(θ))
∖
(28)
ESP := 4 ∙ Rmax ∙ Cψ ∙ Lv ∙ ( logτ κ-1 + ；-
∖	1 - T
(29)
where μθ(∙) is the stationary distribution of state under policy ∏θ and the transition kernel P(∙).
F Proof of Lemma 5
The proof of Lemma 5 is divided into two steps. In step 1, we first analyze the consensus error
kQ ∙ ωtk and then extend the analysis results to ∣∣Q ∙ λt∣∣. In step 2, we further analyze the consensus
error ∣∣Q ∙ θS∣ in the shared part of policy parameters.
Step 1. Since the mixing matrix Wt is doubly stochastic so Wt ∙ 1 = 1, where 1 is a column vector
of all ones. We obtain that
Wt ∙ ωt — 1 ∙ ωT = Wt ∙ ωtt — 1 ∙ ωT).
By the definition of locally estimated TD error in (13), it follows that
C	.	τ≥∕	∖	τ≥∕	∖
δi,t := ri,t + Y ∙ V(st+1; ωi,t) - V(st; ωi,t)
=ri,t + Y ∙ φ(st+1)Tωi,t - φ(St)Tωi,t	(3O)
16
Under review as a conference paper at ICLR 2022
where φ(∙) ∈ RK is the feature mapping for any state S ∈ S. To perform the critic step according to
equation (13), it holds that
ωi,t = ΠRω	X Wti-j 1ωi,t-1 + βt • δi,t-1 ∙N ωi V (St—1; ωi,t-1) ,
j∈Ni(t)
=πRω ( X Wti— Iωi,t-1 + β • δi,t-1 • φ(St-1)),	∀i ∈ N	(31)
j∈Ni(t)
where VωiV(St-1； 3.1) := φ(st-ι) due to linear parameterization. Recall that A(θt), bi(θt) are
defined in (23), it holds that
bi(θt) — A(θt) • ωi,t := Es〜μθt(∙),a〜∏θt(∙∣s),sθ〜P(∙∣s,a) (ri(s,a)+ Y • φ(s)Tωi,t — φ(SO)Tωi,t) • φ(S)
=Est~μθt (∙),at~∏θt (Tst),st+ι~P(∙∣st,at) δi,t • φ(St) .	(32)
Hence, in (31) the estimated stochastic gradient at each iteration t is expressed as
δi,t • φ(St)
δi,t • φ(St) — [bi(θt) — A(θt) • ωi,t]	+	bi(θt) — A(θt) • ωi,t
'----------------------------------------------} '----------------------------
(33)
{z*^^^
:=mi,t
:=hi,t
}
where h%t is the expectation of the estimated stochastic gradient δi,t • φ(St); mi,t denotes the deviation
between δi,t • φ(St) and its expectation hi,t.
Recall that the subroutine to update critic parameters ωt in (13) is given below:
ωet-1 = Wt-1	•	ωt-1,	ωi,t	= ΠRω	ωei,t-1 + βt-1	•	δi,t-1 • Vωi Vb (St-1;	ωi,t-1)	,	∀i	∈ N.
(34)
It can be decomposed using the following steps:
ωei,t-1 =	X	Wti-j 1 • ωi,t-1	(35a)
j∈Ni(t-1)
yi,t-1 = ωei,t-1 + βt-1 • (hi,t-1 + mi,t-1)	(35b)
ei,t-1 = yi,t-1 — ΠRω (yi,t-1)	(35c)
ωi,t = ΠRω (yi,t-1) = yi,t-1 — ei,t-1.	(35d)
Express the above updates in matrix form, it holds that
ωet-1 = Wt-1 • ωt-1	(36a)
yt-1 = ωet-1	+ βt-1	•	(ht-1 + mt-1)	(36b)
et-1 = yt-1	— ΠRω(yt-1)	(36c)
ωt = yt-1	— et-1	(36d)
where yt, ht, mt, et correspond to the collections of local vectors {yi,t }, {hi,t }, {mi,t}, {ei,t }. Re-
call the definitions ωt = [3%; ωTt;…；ωN,/ ∈ RN×K and ωT :=得 ITωt, it follows
ωT=NIT ωt
(i) 1
=N1 (Wt-1 • ωt-1 + βt-1 • (ht-1 + mt-1) — et-1)
(=) ωT-1 + βt-1 • (hT-1 + mT-I)- eT-1	(37)
where ht-1, mt-1 and e— are the averaged vectors of ht-1, mt-1 and et-1 (as defined similarly
as ωt-1); (i) is from the subroutine (36); (ii) is from ITWt-1 = IT due to double stochasticity
17
Under review as a conference paper at ICLR 2022
in weight matrix Wt-ι. Recall that We have defined Q = I - N1 ∙ 1t, then it is clear that Q ∙ ω
indicates the consensus error. We can express such an error as follows:
Q ∙ ωt = ωt - 1ωT
=Wt-1 ∙ ωt-i + βt-i ∙ (ht-1 + mt-i) - et-1 - [1ωT-ι + βt-i ∙ 1[ht-i + mt-i]T - 1eT-i]
=Wt-1 ∙ (ωt-i - 1ωT-J + βt-i∙ (ht-1 - 1hT-I) + βt-i ∙ (mt-1 - ImT-I) - (et-1 - 1eT-1)
=Wt-1 ∙ Q ∙ ωt-1 + βt-1 ∙ Q ∙ (ht-1 + mt-1)- Q ∙ et-1	(38)
t-1	t-1
=(∏k=0Wk) ∙ Q ∙ 30 - X(∏'=k+1%) ∙ Q ∙ ek + X βk ∙ (∏'=k+1%) ∙ Q ∙ (hk + mk).
k=0	k=0
(39)
Then we can bound the norm of the consensus error using the following:
(i)	t-1
kQ ∙ ωtk ≤ k(∏=0Wk) ∙ Q ∙ ωok + E k(∏'=k+1W') ∙ Q ∙ ek-1k
k=0
t-1
+ X βk k(∏'=k+1 w`) ∙ Q ∙ (hk + mk) k
k=0
ii)	t-1	t-1
≤) nbt/Bc ∙ k30 k + X ηb^*-k-1yBc ∙ kek k + X ηb^*-k-1yBc ∙ βk ∙Mk + mk k
k=0	k=0
(iii	) 1	1 t-1	1 t-1
≤ 一 ∙ρt ∙ ∣∣ωok+-y-pt~k~e ∙ ∣∣ekk+-y-pt~kβi ∙βk ∙ khk+mkk	(4O)
η	η k=0	η k=0
where (ii) follows (20); in (iii) We utilize that nbt/Bc ≤ η打B-I = 1 ∙ ρt where We define P := η击.
Next we bound khk + mkk and kek k. We have
khi,tk + kmi,tk =	δi,t ∙ φ(st) - ∖bi(θt) - A(θt) ∙ ωi,t]	+	bi(θt) - A(θt) ∙ ωi,t
≤kδi,t∙ φ(st)k +2 ∙kbi(θt)- A(θt) ∙ωi,tk
(=) kδi,t ∙ φ(St)k +2 ∙	Es~μθt(∙),a~∏θt(∙∣s),s0~P(∙∣s,a) (ri (s, a) + Y ∙ φ(S)Tωi,t - φ(s0)T ωi,t) ∙ φ(S)
≤ kδi,t ∙ φ(st)k + 2 ∙ Es~μθt(∙),a~∏θt(∙∣s),sθ~P(∙∣s,a)	(ri(s, a) + Y ∙ φ(S)Tωi,t - φ(s'0f ωi,t) ∙ φ(s(
(ii)
≤ kδi,tk + 2 ∙ Es~μθt(∙),a~∏θt(∙∣s),s0~P (∙∣s,a)	(ri (S, a) + Y ∙ φ(S) ωi,t - φ(s ) ωi,t)
= ri,t + Y ∙ 0(sT+1 )Tωi,t - 0(St)Tωi,t
+ 2 ∙ Es~μθt (∙),a~∏θt (∙∣s),s0~P(∙∣s,a)	(ri(S, a) + Y ∙ 0(s)	ωi,t - 0(s ) ωi,t)
(iii)
≤ 3Rmax + 3(1 + Y) ∙ Rω	(41)
where (i) follows Jensen’s inequality; (ii) follows Assumption 4 that kφ(S)k ≤ 1 for any S ∈ S;
(iii) follows the fact that |ri(S, a)| ≤ Rmax and the critic parameter ωi,t is constrained in a fixed
region ∣∣ωi,tk ≤ Rω. For simplicity, in the following part we denote Lb := 3Rmaχ + 3(1 + Y) ∙ Rω.
Moreover, it holds that
(35)	(i)	(ii)
kei,tk = kyi,t - πRω(yi,t)k ≤kωi,t- yi,tk = kβt ∙ (hi,t + mi,t) k ≤ βt∙Lb,	(42)
where (i) follows from (16b) and (ii) follows from (41).
18
Under review as a conference paper at ICLR 2022
Recall that the stepsizes in critic steps are defined as βt := T^,
we get
∀t. Plugging (41) and (42) into (40),
1	1 t-1
kQ • ωtk ≤ η • Pt • kωok+n χ Pt-k-1
1 t-1
• βk ∙ Lb ∙ N +-yjPt-k-1 ∙ βk ∙ Lb ∙ N	(43)
ηk=0
1 t ......2N ∙ Lb • β
=η •P • kω0k+ η • τσ2
(a 1 t ll llɪ2N ∙ Lb • β
≤ η • P .kω0k + η • T σ2
where (a) follows the fact that Ptk-=10 Pt-k-1
t-1
Xt-k-1
Pt-k-1
k=0
1
- ----
1 - P
≤ 11ρ, ∀t.
(44)
Summing (44) from t = 0 to t
T-1
EkQ ∙ωtk≤
t=0
T - 1, we obtain
llω0k X^	t l τ 2N • Lb • β 1
F ⅛0 P + τ •	• τ-7
(i)
≤
kω0k
η ∙ (1 - P)
,2N ∙ Lb • β T 1-σ2
+ η • (1 - P) ∙,
(45)
where (i) is due to the fact that PT- Pt ≤ ι--ρ.
In summary, we obtain the following bound on the averaged consensus violation:
kω0k
1
• ωtk≤ T • η • (1 - P)
+ +2N . Lb . β ∙ T-
η ∙ (1-P)
(46)
Extending above analysis steps on deriving a bound for the consensus error ∣∣Q ∙ ωtk in (45) to
kQ ∙ λt∣, We can show that the following holds:
kQ • λtk≤1 /Bk +
2N ∙ Lb ∙ Z 1
η ∙ Tσ	∙ 1 - P
(47)
where the stepsize ζt is defined as ζt :
t = T - 1, it holds that:
T-1
X kQ • λtk ≤;
t=0
In summary, we have
Tζσ2. Similar as (45), summing UP (47) from t
kλ0k
η ∙ (1 - P)
2N • Lb Y T-2
+ + ηΓa-P) • t
kω0k
• λtk≤ T ∙ η ∙ (1-P)
+ +2N ∙ Lb Y ∙ τ-σ
η ∙ (1 — P)
0 to
(48)
(49)
Summing up (46) and (49), we obtain the convergence rate of the consensus errors:
1 T-X (1。, , Ii IIlo λ ∣Λ	— 1 kω0k + ∣∣λ0k	1	1	2N ∙ Lb • (β +	Z)
T⅛ (kQSk + kQ • λtkJ	= τ	• η • (1 - P)	+ F	• η • (1 - p)
= O(T-σ2).
(50)
Taking square on both side of (44) and applying Cauchy-Schwarz inequality, it holds that
2	2	2t	2	2	2N • Lb • β 2
kQ∙ωtk ≤ 滔•P ∙kω0k + 声• (η∙(1-ρ))
(51)
Summing (51) from t = 0 to t =
T-1
X kQ∙ωtk2
t=0
T - 1, it holds that
2kω0k2
≤ η2 • (1 - ρ2)
T 1-2σ2
(52)
+ 2 • (： 了
19
Under review as a conference paper at ICLR 2022
Extending above analysis steps on deriving a bound for the consensus error ∣∣Q ∙ ωtk2 in (52) to
kQ ∙ λt∣2,we can show that the following holds:
T-1
X kQ ∙ λtk2 ≤
t=0
2kλ0k2	+ 2 J2N ∙ Lb Y∖
η2 ∙ (I-P2)	lη∙ (I-P)J
(53)
Hence, adding (52) and (53), then divide both side by T, it holds that
1 TX (IIO λ ∣∣2 , ∣∣0 ω ∣∣2^ < 2(kλ0k2 + ∣∣ω0k2) , 8N2 ∙ L2 ∙ (Z2 + β2) T-2σ2
TA *• λtk + kQ∙ωtk ≤ ≤ T∙η W) +	*.(I”	∙T
= O(T-1)+O(T-2σ2)	(54)
This completes the proof for the first part.
□
Step 2. In this part, we analyze the consensus errors for the shared policy parameters θs.
Since the mixing matrix Wt is doubly stochastic which implies Wt ∙ 1 = 1, we obtain that
Wt ∙ θs - 1θsτ = Wt ∙ (θs - 1θsτ)
where we have defined HsT := N 1τθt. Recall that the subroutine to update shared policy parameters
in (9) - (10) is given below:
θi,t :=	SX	Wj 1θs,t-1	+	αt-1	∙ δi,t-1	∙	Vθs	log∏i(ai,t-i∣st-i,	θi,t-i),	∀i	∈ N (55)
j∈Ni (t-1)
where we have defined
δi,t-1 := r(st-1, at-1; λi,t-1) + Y ∙ V (st; ωi,t-1) - V (st-1; ωi,t-1)
=夕(st-1, at-1 )τλi,t-i + Y ∙ φ(st-i)τωi,t-i 一 φ(st)τωi,t.	(56)
Then we define gi,t-1 ：= M,t-1∙qs log ∏i(ai,t-1∣st-1,θi,t-1) and g := [gT; gT ;・•• ; gN ],it holds
that
θs := Wt-1 ∙ θs-1 + at-1 ∙ gt-1.
Recall Q = I 一 N1 ∙ 1τ, we analyze the consensus error Q ∙ θs as below:
τ
Q ∙ θs =θs - iθs
=(Wt-1 ∙ θs-1 + at-1 ∙ gt-1)-卜器-1 + at-1 ∙ 1gl--1
=Wt-1 ∙ Q ∙ θs-1 + at-1 ∙ Q ∙ gt-1
t-1
=(∏k=0Wk) ∙ Q ∙ θ0 + X αk ∙ (∏t-1+1Wι) ∙ Q ∙ gk.
k=0
(57)
(58)
By Assumptions 3-4,5, the estimated stochastic gradient gt = [gτt, gτt,…，gN h can be bounded
as below:
(i)	N
kgt k ≤	kgi,t k
i=1
N
ɪ2 llδi,t-1 ∙ ^θs log πi(ai,t-1∣st-1, θi,t-1)k
i=1
N
(ii)
≤ Cψ ∙)/ 2(St-
1, at-1)τλi,t-1 + Y ∙ φ(st-1)τωi,t-1 - φ(st)τωi,t
i=1
N
≤ Cψ ∙∑	k^(st-1, at-1)k∙ kλi,t-1k + Ykφ(st-1)k∙ kωi,t-1k + kΦ(st)k∙ IE,tk
i=1
(iii)
≤ N ∙Cψ ∙ (Rλ + (1 + Y) ∙
(59)
20
Under review as a conference paper at ICLR 2022
where (i) follows the definition gt = [gft,gTt,…，gN J and Triangle inequality; (ii) follows
that ∣∣Vθs log∏i(ai,t-ι∣st-ι,θi,t-ι)k ≤ Cψ in Assumption 5; (iii) is due to the assumptions that
k夕(st-ι, at-ι)∣∣ ≤ 1 and ∣∣φ(st-ι)∣ ≤ 1, as well as that approximation parameters are restricted in
fixed regions, so ∣ωi,t ∣ ≤ Rω and ∣λi,t ∣ ≤ Rλ; In the last equality, we have defined `p as
'p ：= N ∙ Cψ ∙ Rλ+ + (1+ γ)Rω) .	(60)
Recall that the stepsizes in policy optimization are defined as at := ^^, ∀ t. Taking Frobenius norm
on both side of (58), we have:
(i)
kQ∙ θSk ≤
t-1
k(∏k=0Wk) ∙ Q ∙ θSk + X ak k(∏t-1+ι Wl) ∙ Q ∙ gk k
k=0
(ii)	t-1
≤ ηbtBc ∙ kθsk + Xαk ∙ nb(t-kT)/BC ∙∣gkk
k=0
(iii)
≤
1 t-1
kθSk + η ∑αk ∙ ρt-k-1∙kgkk
(iv)	1	` t-1
≤ η ∙ρt ∙ kθ0k+力Xρt-k-1 ∙ αk
=L ρt∙∣θ0k+^a ∙X ρt-k-1
η	η ∙ Tσι k=0
≤ η ∙ ρt∙kθ0k + η∙'(Cρ) ∙ Tσ1
(61)
where (ii) follows from (20); in (iii) we utilize that nbt/Bc ≤ nt/B-1 = ^ ∙ Pt where we have
defined ρ :=ηB; (iv) follows from (59). Summing (61) from t = 0 to t = T — 1, it holds that:
THuo θs∣∣ / kθsk V ,t ,	'p ∙ α0	T
t=0kQ∙θtk≤丁t=0ρ + ¥V-T)∙Tσ1
≤ kθsk +	'p ∙ao ∙ Ti-σ1
η (∙(1 - P)	η∙(I — P)
= O(T 1-σ1).
(62)
Then dividing T on both side of (62), it holds that
T E kQ∙ θSk ≤o(T-σι)
T t=0
(63)
where consensus error converges to 0 as T goes to infinity.
Moreover, we can provide a bound for the averaged consensus error squared PT-OIkQ ∙ θS∣2. Taking
square on both sides of (61) and summing from t = 0 to t = T — 1. We obtain:
T-1	(61) T-1
X kQ∙ θSk2 (≤, X
t=0
t=0
1 ∙ρt ∙kθsk+η⅛⅛ ∙ Tσι)
1 ∙ρt
T-1
(≤i) X	P2t
t=0
2kθ0k2 ,	2'p ∙ α2
η2	η2 ∙ (1 — ρ)2
(≤	2kθSk2	+	2'p ∙ a2
一η2 ∙ (1 — ρ2)	η2 ∙ (1 — ρ)2
(64)
21
Under review as a conference paper at ICLR 2022
where (i) follows from CaUchy-SchWarz inequality; (ii) follows from 2kjθk ∙ PT=-o1 ρ2t ≤
2kθ0k2
η2∙(i-ρ2).
Then dividing T on both side of (64), it holds that
TX kQ∙ θSk2 ≤O(T-1) + O(T-2σ1).	(65)
T t=0
This completes the proof for the second step.	口
G Proof of Proposition 1
Proof. In this proof, we show the convergence results of all approximation parameters ωt and λt .
We first analyze the convergence error ∣∣ωt - ω*(θt)k and then extend the results to ∣∣λt - λ*(θt)∣∣.
For simplicity, we write ω*(θt) as ωJ= and λ*(θt) as λj∖
Denoting the expectation over data sampling procedures as E[∙], let us begin by bounding the error
E[∣ωt+ι - ωJ+1k2] as below:
E [∣∣ωt+ι - ωj+ι∣∣2]
=E [∣∣ωt + βt ∙ (ht + mt) - Ht - ωt + ωt - ωj+ιk2]
=E[kωt - ωjk2]+ E[kβt ∙ (ht + mt) - et + ωt - ωj+ιk2] + 2βt ∙ E [hωt - ωj, ht + mti]
- 2E[hωHt - ωtJ , eHti] + 2E hωHt - ωtJ , ωtJ - ωtJ+1i]
(ii)
≤ E[∣∣ωt - ωtk2] + 2βt ∙ E [hωt - ωt, ht + m ti] + 2E[kωj - ωj+ιk2] + 2E [hωt - ωt,ωt - ωt+ιi]
|
term A
} 1
}
+ 2明|向∙(ht + mt) - HtIl2] -2E[hωt - ωj, Hti]
1---------{----------} |------{------}
term C	term D
term B
(66)
where (i) follows (37); (ii) is from Cauchy-Schwarz inequality. Recall that hT := N 1tht, TmT :
N 1tmt and HT := N 1tet, where ht, mt and et are defined in (36).
In the following, let us analyze each component in (66). First term A can be expressed below:
2βt ∙E	hωt	- ωt, Ht + mti	== 2βt	∙E hωt - ωt, E[ht	+ mt IFt]i	=	2βt	∙E	hωt	- ωj, hti
(67)
where Ft is the σ-algebra generated by Ft = {ωt, θt,…,ω0, θ0}; (a) follows Tower rule in
expectations; (b) is due to the fact that E[mi,tIFt] = 0, ∀i ∈ N, which is from (32) and (33). Recall
that in (23) we have defined:
A(θ)= Es~μe (∙),s0~p∏θ(∙∣s) [φ(s) ∙ (φ(s) - Y ∙ φ(s0))τ] , ∀ i ∈N,	(68a)
bi(θ) := Es~μθ (∙),a~∏θ (∙∣s) [ri (s, a) ∙ φ(s)] , ∀ i ∈N.	(68b)
Also in (22), it has shown that ωtJ satisfies the condition
1N
A(θt) ∙ ωJ = NN Nbi(θt)=b(θt)	(69)
where we define b(θt) := N PN=1 bi(θt). Recall that we have defined
1N 1N
ht := NN hi,t = NN (bi(θt) - A(θt) ∙ ωi,t) = b(θt) - A(θt) ∙ ωt,
then it holds that
ht =	b(θt)	- A(θt)	∙ ω⅛	(=) A(θt)	∙	(ωJ	-	ωt)	(70)
22
Under review as a conference paper at ICLR 2022
Therefore, plugging (70) into (67), term A in (66) could be bounded as below:
2βt ∙ E	hωt	-	ωt ,	hti	= -2βt	∙ E 卜ωt - ωt )T A(θt)	∙	(ωt	-	ωt )	≤	-2Bt	∙ Cmin ∙ E	∣∣ωt	- ωt k2
(71)
where (i) is due to the fact that A(θt) is a positive definite matrix and its minimum eigenvalue
cmin (A(θt )) ≥ cCmin in (24).
Second, term B can be bounded as below:
2E[kωt - ωt+1k2] + 2E [hωt - ωt,ωt - ωt+1 i]
(a)
≤ 2EUW- ωt+1k2] + 2E [kωt — ωtk ∙ kωt - ω+ιk]
(b)
≤ 2Lω ∙ E[kθt — θt+1k2] + 2Lω ∙ E[kθt — Θt+1 k∙ ∣ωt — ω"∣]	(72)
where (a) follows Cauchy-Schwarz inequality; (b) is from the Lipschitz property (25) in Lemma 6.
Recall the definition θt := {θt, θp} and Q = I —得 11T, We bound ∣∣θt — θt+ι k as below:
(i)
∣θt — θt+1∣ ≤ ∣θts — θts+1∣ + ∣θtl — θtl+1∣
(ii)
≤ kθs — ιθsk + kiθs — "s+ιk + k"s+ι — θs+ιk + kθl — θl+ιk
(iii)
≤ kQ∙ θSk + kQ∙ θS+ιk +2αt∙ 'p
(=) kQ ∙ θSk + kQ ∙ (Wt ∙ θS + at ∙ gt)k + 2αt ∙ 'p
(iv)
≤ kQ ∙ θSk + kQ ∙ Wt ∙ θSk + at ∙kQ ∙ gtk + 2at ∙ 'p
(v)
≤ kQ ∙ θSk + kWt ∙ Q ∙ θSk +3at∙'p
(vi)
≤ 2∣∣Q∙ θSk+3at∙ 'p.	(73)
where (i) and (ii) follow Triangle inequality; (iii) is due to the fact that estimated policy gradient in
updating θt is bounded by 'p in (59); (iv) follows Cauchy-Schwarz inequality; in (v) we used the
boudnedness of the gradient (59), the fact that eigenvalue value of Q is upper bounded bounded by 1,
as well as the following:
(74)
Additionally, (vi) is due to the fact that the eigenvalue of weight matrix Wt is bounded by 1. Then
plugging the inequality (73) into (72), term B could be bounded as follows:
2E[kωt — ωt+ιk ] + 2E [hωt — ωt,ωt — ωt+ιi]
(72)
≤ 2Lω ∙ E[kθt — θt+1∣∣ ]+ 2Lω ∙ E[kθt — θt+1∣∣∙∣∣ωt — ω"∣]
≤) 2Lω •卜E[kQ ∙ θSk2] +18a2 ∙ 'p) +2Lω ∙ E(2||Q ∙ θSk + 3at ∙ 'p) ∙∣Qt — ω"∣
=16Lω ∙ E[kQ ∙ θSk2] + 36Lω ∙ 'p ∙ a2 + 6Lω ∙ 'p ∙ at ∙ E[∣Qt — ω"∣]
+ 4Lω ∙ E [kQ ∙ θSk ∙kωt — ω"∣]	(75)
where (i) follows (73) and the Cauchy-Schwarz inequality.
23
Under review as a conference paper at ICLR 2022
Recall the definition of ht as ht :=得 PN=I hi,t, where mt and & are also defined similarly. Then
term C in (66) could be bounded as below:
2E[kβt(ht + mt) - ɛtk2] ≤ 4β2 ∙ E[kht + mtk2] + 4E[k(≡t∣∣2]
1
4βt ∙ E Il N E(hi,t +
N i=1
-	I-IJV
mi,t)ι∣2 +4E i∣ N X ei,ti|2
(ii)	N	N
≤ 4βt ∙ E N X ι∣hi,t+mi,tιι +4E N X ι∣ei,tιι
i=1	i=1
(iii)
≤ 8β2 ∙ L
(76)
where (i) follows Cauchy-Schwarz inequality; (ii) follows Jensen’s inequality; (iii) is from the
inequalities (41) and (42).
Recall that Ht := 焉 PN=I e%,t and in (35) we have defined yi,t := d,t + βt ∙ (hi,t + mQ. Then
term D in (66) can be bounded as below:
-2E hωt - ωt, Hti
2N
=-N EE hωt - ωt,ei,ti
i=1
2N
=-N EE hωt - yi,t, ei,ti
i=1
(i) 2 N
≤ N EE kωt - yi,tk∣∣ei,tIl
i=1
2N
-N EE hyi,t - ωt ,ei,ti
i=1
2N
-N y?E hyi,t - ωt,yi,t - πRω [yi,t]i
(ii) 2β	L N	2 N
≤ —N — X E kωt - yi,tk	- N X E kyi,t - πRω [yi,t]k
i=1	i=1
(iii)	N	N	N
≤	-N- XE kωt - XWt3ωj,t - Bt ∙ (hi,t + mi,t)k	- N XE l∣ei,tk2
i=1	j =1	i=1
(≤' 2βNLbXE[∣ωt -XWt⅝,tk] + 2βNLbXE[kβt(hi,t + mi,t)∣
i=1	j =1	i=1
(v)	N	N
≤ —N — XE kωt- ωi,tk	+ 2βt ∙ Lb- N XE l∣ei,tk2
i=1	i=1
2N
— N X E kei,t k
vi)	2 N
≤	2βt	∙	Lb	∙ E IlQ ∙ωtll	+ 2βt	∙	Lb	- N2ZE llei,t|2	(77)
i=1
where (i) follows Cauchy-Schwarz inequality and the definition ei,t := yi,t - ΠRω [yi,t]; (ii) is from
(42) and the projection property (16a) in Lemma 1; (iii) follows the definition of yi,t; (iv) follows
Cauchy-Schwarz inequality; (v) is from the inequality (41) and that the eigenvalues of Wt is bounded
by 1; (vi) is due to the factthat ∣ωt — ω*tk ≤ JpN=ι IQt — ωi,t∣∣2 = ∣∣Q ∙ ωt∣ ∀ i ∈N.
24
Under review as a conference paper at ICLR 2022
Then we plug in the above derived bounds on terms A-D (inequalities (71), (75), (76) and (77)) into
(66), and obtain:
E kωt+1 - ωt+1k2
≤ (I - 2βt ∙ Nmin) ∙ E	kωt - ωt k2	+16Lω ∙ E kQ ∙ θsk2	+36Lω ∙	'p ∙ α2 + 6Lω ∙ 'p ∙ αt ∙ E	∣∣ωt	- ωt k
+ 4Lω ∙ E IIQ ∙ θsk	∙ kωt - ωtk	+	2βt ∙ Lb ∙ E IIQ ∙ ωt k +10β2	∙ L2 - N X E kei,tk2 .
i=1
(78)
In (78), We have already obtain a bound of the distance between averaged parameter ω⅛ and its
optimal solution ω^. Then we could utilize the inequality (78) to derive the convergence rate for the
averaged parameters ω⅛. Towards this end, we first rearrange the inequality (78), divide both sides by
2βt ∙ Cmin and then sum it from t = 0 to T - 1, and obtain:
T-1
XE kωik2
t=0
T-1	1
≤ X -1—
≤ t=0 2βt ∙ Cmin
-E l∣ωt+ι - ωt+"l2
{Z
term I1
+X c~ (18Lw • `p • α^+5L2 •β)+3⅛~ι
t=0 cmin	t	cmin
u------------------{------------------} |-------
term I3
)+8Lω
cCmin
} 、
T-1
X X
乙βt
t=0 t
• E kQ • θsk • kωt - ωtk + ^-b-
cmin
--------------------------/ |-----
^^^{^^™
term I5
T-1 1
E『E kQ • θsk2
t=0βt	-	-
term I2
• E kωt - ωtk
^^^{^^"
term I4
T -1
• X E kQ∙ωtk
t=0
______ - /
{^^^^^^^^^^^^^
term I6
(79)
E	M： - ωt∣l2
+念, TI;
}
}
Recall the fixed stepsizes at := TO— and βt := Tβ2 ∀t. We can bound each term in (79) as below.
First, term I1 can be bounded as:
T -1
I1 := X
t=0
Tσ2
2β • cCmin
E l∣ωt - ωtll2	- E l∣ωt+ι - ωt+ιll2
Tσ2
2β • cCmin
E ∣∣ω0 — ω°k2 — E ∣∣ωT — ωτk2
(a) Tσ2 • 4R2
≤	〜ω = O(T σ2)
2β • cCmin
(80)
where the inequality (a) follows E[|^： 一 ωtk2] ≤ 2E[kω:∣2 + IEk2] = 4κ匕,since both ωj and
ωt are in a fixed region with radius Rω.
Second, term I2 in (79) can be bounded as:
T -1	T -1
I2 :=1 ∙ X ɪ • E kQ • θSk2 = ~ω . R • XE kQ • θSk2 (=) O(T 1-2σι+σ2) (81)
cCmin	βt	t	cCmin • β	t
τ 	J	—,	T 	J	—,
where (i) follows the definition β = Te2, ∀t.
25
Under review as a conference paper at ICLR 2022
Third, term I3 in (79) can be bounded as:
T-1	1
I3 := X C- ∙ (18Lw • `
t=0 cmin
2
~7Γ + 5L2 • βt
βt
(a) 18Lw ∙ 'P
2
cCmin
18Lw ・ 'p
cCmin
Q T Tσ2-2σι + 5Lb - β T T
~β '	'	Cmin	∙	'
Q2 t 1+σ2-2σι + 5L2 ・ β T1-，
丁	Cmin .
σ2
-σ2
= O(T 1 —σ2 ) + O (T 1+σ2 —2σι )
where (a) follows the definitions Qt ：= t⅜ and βt ：= T⅛ for any iteration t.
Next, we can bound the term I4 in (79) as below:
(82)
I4
3Lω ・ 乂
cCmin
T-1
≤ 3Lω ・ 'p
cCmin
X β • E kωt - ωt k
t=0 βt	-
X1 (si •S
E kωt - ω"∣2
(≤ 3Lω • 'p
ɑmin
T-1
2
T-1
• t x a ∙ t χ E kωt - ω*k
2
t=0
(iii) 3Lω ∙ 'p ∙
T-1
Cmin • β0
・ √T • T2σ2-2σι . t
EE kωt-ω"∣2
t=0
3Lω ・ 2p • Q2 • √T 1+2σ2-2σ1 .
Cmin • β0	∖
T-1
EE kωt-ω"∣2
t=0
(=)PC4 • T 1+2σ2-2σ1 . t
T-1
X E kωt -
t=0
ω"∣2
where (i) is by Jensen’s inequality; (ii) follows Cauchy-Schwarz inequality; (ii
2
(83)
) follows the
definition of stepsizes αt and βt ; in equality (iv) we define C4 :
3Lω ∙'p ∙ α0
Cmin ∙ β0
Next, the term I5 in (79) can be bounded as below:
I5 = 2Lω
cmin
(i) 2Lω
≤ —-----
ɑmin
T-1 1
E β ・ E kQ ・ θS∣H∣ωt-ω力∣
t=oβt	L	-
• X β β2 E kQ • θsk2 • E kωt - ωt k
2
(ii) 2Lω
≤ —-----
ɑmin
∖
T-1 1
X碎
E kQ • θtsk2 • t
T-1
X E kωt -
t=0
ω"∣2
(iii) u
=t
C5 ・ T2σ2
-β2-
T-1
T-1
EE kQ. θsk2 .t ΕEM-ωM2]
t=0
t=0
where (i) and (ii) follows Cauchy-Schwarz inequality; (iii) follows the stepsize βt :
define the constant C5 := (2Lω).
cmin
(84)
τJ^ and We
26
Under review as a conference paper at ICLR 2022
Finally, we can bound the last term I6 in (79) as below:
T-1
L	T-1
I6 ：=J ∙ EE kQsk
Cmin t=0	-
(=46) O
(85)
Then we can revisit (79) to obtain the exact convergence rate. Let us rearrange (79) as below:
T-1
X E kωt -
t=0
ω"∣2
≤ (I1 + I2 + I3 + I6 ) +(I4 + I5 ).
X---------{---------}
:=term K1
(86)
For the terms I4 + I5, we utilize (83) and (84) to obtain that
I4 + I5 ≤ (√C4 ∙ T 1 + 2σ2-2σ1 + t C5 ∙
T2σ2
T-1
2
F NE F ∙ θsk2
∙∖
T-1
X E[kωt-ω"∣2]
t=0
(a)
≤
2c4 ∙ T1+2σ2-2σι + 2C5BT2σ2
T-1
XE kQ∙θSk2 ∙
t=0
T-1
X EM-ω"∣2]	(87)
^^^^{^^^^™
:=term K2
}|
t=0
:=term K3
|
}
where (a) follows Cauchy-Schwarz inequality.
With terms K1 , K2 and K3 defined as above, we can plug (87) into (86), and obtain:
K3 ≤ Ki + Kk2 ∙ K3
(PK - 2 pK2)2 ≤ K1 + 4 K2 =⇒ PK ≤ 2 p∕κ2 +qK1 + 4 K
1 1 Z—	/	1-V (a) 1	1
K3 ≤ (G√K2 + yκ1 + 4K2∖	≤ 2K2 + 2K1 + 2K = 2K1 + K
(88)
where (a) is due to Cauchy-Schwarz inequality.
Combining the inequalities (80), (81), (82) and (85), the convergence rate of term K1 in (88) could
be expressed as below:
K1 := I1 + I2 + I3 + I6
=O(Tσ2) + O(T 1-2σ1+σ2) + O(T 1-σ2) + O(T 1+σ2-2σ1) + O(T 1-σ2).	(89)
Moreover, the convergence rate of term K2 could be bounded as below:
K ：=2C4 ∙ T i+2”+2Cr2
T-1
X E kQ∙ θSk2
t=0
(= O(T 1 + 2σ2 -2σ 1 ) + O(T- 1 + 2σ2
(90)
where (a) follows the inequality (64).
Plugging (87) - (90) into (86), dividing T on both sides, we can obtain the convergence rate of
T PT01 E [kωt - ω"∣2] as below:
1	-1
T EE kωt -
T-1
ω"l2
O(T-1+σ2) + O(T-σ2) + O (Tσ2-2σ1) + O(T-2σ1+2σ2) + O(T-2+2σ2).
t=0
(91)
T⅛2 and βt :=号 for any
When the fixed stepsizes ζt and βt are in the same order (ζt :
iteration t), the analysis of parameters ωt can directly extend to establish the convergence results
of parameters λt. Then, it holds that T PT-1 E[∣∣M - λ却∣2] has the same convergence rate as
T PT-1 E[kωt-ω"F].
27
Under review as a conference paper at ICLR 2022
Finally, combining the convergence results for consensus errors in (21c), we obtain the convergence
of approximation parameters as below:
1 T-1 N
T XX (E ∣∣ωi,t-
T-1 N
ω"∣2 + E ∣λi,t-XI2
t=0 i=1
( ) T-1 N
≤) T XX 2E ∣ωi,t- ωt∣2	+2E 忸一
t=0 i=1
*
ωt
∣∣2 + 2E ∣∣λi,t - Mil2 + 2E IlXt- λt ∣2
2 T-1 N
T XX E ∣ωt-
T-1 N
ω"∣2 + E 氏一XI2
t=0 i=1
=) O(T-1+σ2) + O(T-σ2) + O (Tσ2-2σ1) + O(T
2 T-1
TE (E ∣Q∙ωt∣2 + E IQ ∙ %∣∣2
T-1
t=0
-2σ1 +2σ2 ) + O(T -2+2σ2 ) + O (T -2σ2 ).
(92)
+
where (a) follows Cauchy-Schwarz inequality; (b) follows (54) and (91).
□
H	Proof of Proposition 2
Proof. In this proposition, we will analyze the convergence of actor in algorithm CAC.
With linear approximations, recall that in (11) we have defined:
δi,t = φ(st, at) λi,t + Y ∙ φ(st+1 )T ωi,t - φ(st)T ωi,t.
Due to the facts that feature vectors are assumed to be bounded in Assumption 4 and the approximation
parameters λi,t and ωi,t are restricted in fixed regions, we have
,^ _	， 、— .-
∣δbi,t ∣	≤ Rλ +	(1	+ γ)Rω ,	∀ i ∈ N	(93)
For simplicity, let us define Cδ := Rλ + (1 + γ)Rω.
Recall that for each agent i, we have denoted its local policy parameters as θi := {θis , θip} where θis
is the shared policy parameter and θip is the personalized policy parameter. Moreover, for each policy
optimization step, the update in shared policy parameters θis is given by:
N
θS,t+ι := X Wij ∙ θj,t + αt ∙ δi,t ∙ ψθi(st, ai,t)	(94)
j=1
where We have defined the score function ψθs (st, ai,t) := Vθ≡ log ∏(ai,t | st, θi,t). Therefore, the
update of the average of the shared policy parameter Xs := N PNXI θi,t is given below:
N
Xs+1 := Xs + ON X bi,t∙ ψθs (st,ai,t).
i=1
(95)
We further define θXt := ∪iN=1{θXi,t} and θXi,t := {θXts, θip,t}. Also recall we have defined θt :=
∪iN=1{θi,t} and θi,t := {θis,t, θip,t}. In the analysis, we start from analyzing objective value J(θXt) at
28
Under review as a conference paper at ICLR 2022
each iteration t. According to Lj-Lipschitz of policy gradient in Lemma 2, it holds that
J (j
≥ J(θt) + BJ(θt), θt+!-g-LJ kθt+1 - θtk2
=J(θt) + BJ(θt) - VJ(Θt),θt+1- θti + BJ(Θt),θt+1- θti - LJI∣θt+1 - θtk2
N
J(θt) + X"% J(θt) - V J(θt), θi,t+ι
i=1
NN
-θi,ti + X"% J(Ot), θi,t+1 - θi,ti-J X kθi,t+1

-θi,t∣∣
2
N
≥ J (θt) - X ∣∣V% J (θt) - Vθi J (θt)k∙ I∣θ,t+1
i=1
NN
-θ,tk + XgθJ(θt),θi,t+1 - θ,ti- L X kθ,t+1 - θ,tk2
(i)	.	_	K	—	α2 ∙ 'P ∙ Lj ∙ N
≥ J(θt) - N ∙ ɑt ∙ 'p ∙ lJ ∙ ∣∣θt - θt∣∣ + y^hvθj J(θt), θi,t+1 - θi,ti----ɔ-------
i=1
N
(=)J(θt) - N ∙ ɑt ∙ 'p ∙ LJ ∙ ∣∣θs - 1 ∙ θ Il + SXNθi J(θt), θi,t+1 - θi,ti -
i=1
α2 ∙ 'p ∙ LJ ∙ N
2
(96)
where (i) follows (17) and ∣∣θ,t+ι-θ,t∣∣ ≤ 四∙'p where the constant 'p is defined in (59); (ii) follows
the faCtthat k θt - θt∣∣ = ∣∣θf - 1 ∙器 TIl since we have defined θt ：= UN=ι{θ,t} = UN=I{器,θfj.
Therefore, it follows that
J (θt+ι)
(96)
≥ J (θt) - N ∙ɑt∙ 'p ∙Lj∙∣∣Θ; - 1 ∙ θ


-T 口	—	—	。2 ∙ 'P ∙ Lj ∙ N
,t Ii+ XhvθiJ(θt), θi,t+ι- θi,ti-----2------
i=1
N /	1 N
==J(Ot)- N ∙ ɑt ∙ 'p ∙ lj ∙ IIQ ∙ θs∣ + Qt X ξvθ∣ j(θj N X jt ∙ ψθj (St) aj,t; θj,t)
+ Qt ^X (vθp J(θt), δi,t ∙ ψθp (St) αi,t; θi,t))-
2
_ Q N/	ʌ	∖N∕	ʌ
J (Ot) + N ɪs ∖ vθs J (Ot), δj,t ∙ ψθs (St) aj,t; θj,t) ) + Qt ɪ2 ( vθp J (θt), δi,t ∙ ψθp (St) ai,t; θi,t)
i,j=1 '	/	i=1 '
-N ∙αt∙ 'p ∙ Lj ∙∣∣Q∙ ΘS∣∣-
2
(97)
where (a) follows the fact that the update of θ,t ：= {器,θfj could be decomposed as below:
N
8s+ι- Hs = N X bj,t ∙ ψθs ISt) "t； "t),
j=i
θP,t+1 - θP,t = αt , δi,t ∙ ψθp (St) αi,t; θi,t)∙
Taking expectation on both sides of inequality (97), we have:
E[J(θt+1)] ≥ E[J(θt)] + I1,1 + I1,2 + I2,1 + I2,2
-N ∙ Qt ∙ 'p ∙ Lj ∙ E[∣∣Q∙ θs∣ -
N ∙ lj ∙ Q2 ∙ 2p
2
(99)
29
Under review as a conference paper at ICLR 2022
where we have defined:
N
I1,1 :
N
Nt x E (vθs J (θt),
i,j=1
j,t - δt j ∙ ψθS (st, aj,t; θj,t)
N
I1,2 := NN X E
i,j=1
N
「	T/ C ∖ 个土	，	/	八、
vθS J(θt), δt ∙ ψθj (st,aj,t; θj,t)
I2,1	:= αt
i=1
N
I2,2	:= αt	E
i=1
EE (VθpJ(θt), b
：,t - bt ) ψθp (st, ai,t; θi,t)
个*
Vθip J(θt),δt ∙ ψθip (st, ai,t; θi,t)
^
bt := ψ(st, at)TN + Y ∙ φ(st+ι)Tω↑ 一 φ(st)Tω*,
and these terms satisfy the following relations:
N
αt
I1,1 + I1,2 = N ʌ, E	J(θt),δj,t ∙ ψθj (st, aj,t; θj,t)
i,j=1
(100)
N
I2,1 + I2,2 = αt
N
XE
i1
i=1
Vθip J (θt), δbi,t ∙ ψθip (st, ai,t; θi,t)	.
Below We analyze each term on the rhs of (99). Towards this end, let Us define VθsJ(θt):
N PN=I Vθs J(θt). We have the following:
Iι,ι = Nt X E (vθSJ(θt), fbj^,t - bt
i,j=1
ψθs (st,aj,t; θj,t)
-αt ∙ X ∙EK N Xvθs J(θt), φ(st, at)T(λt - λj,t) ∙ ψθj (st, aj,t; θj,∕,]
N	1N
- αt ∙ Σ∙E NΣvθs J(θt), (Y ∙ φ(st+1)- φ(St))T(ωt - ωj,t) ∙ ψθj (st, aj,t; θj,t)
j=1	i=1
(i)	VN 「___________ 1	VN - __________
≥ -αt	∙ Cψ ∙ ^E[kVθs J(θt)k ∙内-λi,tkJ	- (1 + Y) ∙ Nt	∙ Cψ ∙ ]^E[kVθs J(θt)lH层-ωi,tk
N
≥ -2Nt ∙ Cψ ∙ XE ∣∣vθsJ(θt)k ∙ ^∣ωt - ωi,tk + ∣∣λt - λi,t∣∣)
j=1
2
= -2Nt ∙ Cψ ∙ X
(ii)	N
≥ -2Nt ∙ Cψ ∙
j=1
E kvθs J (θt)k ∙ ( llωt - ωi,t k + IN- λi,tk
E kvθs J (θt)k2 ∙ E [fkωt - ωi,tk + IH- λi,t∣∣)
(iii)
≥	-4Nt ∙ Cψ
^NE kVθs J(θt) k2
X (E]∣司-ωi,tk2l + E]∣N-λi,tk2])
j =1
(101)
where (i) is dUe to the facts that all featUre vectors are boUnded (cf. AssUmption 4), and the score
fUnctions are boUnded as ∣ψθs (st, aj,t; θj,t)∣ ≤ Cψ (cf. (14a)); (ii) and (iii) follow from the
CaUchy-Schwarz ineqUality.
30
Under review as a conference paper at ICLR 2022
Here, we define the TD error
δt := Q∏θt(st, at) - V∏θt(St) = rt + γV∏θt(St+1)- V∏θt(St)	(102)
and Vθs J(θt):= 得 PN=I Vθ∣ J(θt), then the term I1,2 can be decomposed as below:
I1,2 := Nt ∙ X E (vθ" (θt),b ∙ ψθj (st,aj,t; θj,t)R
i,j=1
N	1N
=αt ∙ EEKNF ∑Vθs J(θt),b ∙ Ψθj(st, aj,t； θj,t))
j=1	i=1
2]
=αt ∙ (1 — γ) ∙ N ∙ E ∣∣Vθs J (θt )k2 + αt ∙ EE NeS J (θt), (δt — δt) ∙ ψθs (St,aj,t θj,t)
j =1
+ αt ∙ X E ](VθsJ(θt), δt ∙ ψθs(st, aj,t； θj,t) — (1 — γ) ∙ VesJ(θt)R .	(103)
Then we are able to bound each term above:
at ∙
N
XE
j =1
N
————--- ,^, _ . , , _ .
VesJ(θt), (bt - δt) ∙ ψθj (St, aj,t； θj,t)
N

≥ -at ∙
EE ∣∣vθsJ(θt)∣ι∙ ιιψθs(St,aj,tθj,t)k ∙ ∣δt — δt∣
j=1
(i)
≥ —at
• Lv ∙ Cψ ∙ N ∙ E ( IP(St, at)Tλt - rt I + Y ( φ(st+1)Tωt - V∏θt (St+1)
φ(St)Tωt - V∏θt (St)
(ii)
≥	—at	∙ Lv ∙ Cψ	∙ N ∙ I E I夕(St,	at)Tλt	— 尸t∣	+ YE	|φ(st+1)Tωt	— V∏θt (St+1)|	+ E	lV∏θt (St)-。(%),闻|
(iii)
≥ —αt ∙ Lv ∙ Cψ ∙ N ∙
I_r	i^	I~r
E W(St, at)Tλt — rt) + YdE(0(St+i)T0 — V∏θt(st+1))
2
十 ^E (V∏θt (St)- φ(St)Tωt)
(iv)
≥ -2N ∙ at ∙ Lv ∙ Cψ ∙ Eapp
2
(104)
一 ,.._ .. . . . . . . . 一 . . . _ . ______________ 一一 一 .. 一 一. 一 一 ，一、 一一一 一
where (i) follows (14a), (18), definition of δ↑ in (100) and definition of δt in (102); (ii) follows the
triangle inequality; (iii) follows Jensen’s inequality; (iv) follows the definition of approximation
error in (28).
Recall the definition Qπθ (S, a) := E Pt∞=0 Ytr(St, at)IS0 = S, a0 = a . According to Agarwal
et al. (2019), policy gradient could be expressed as below
VJ(θ) :=占Es〜dθ(∙),a〜∏θ(∙∣s)
(Q∏θ (s, a) — V∏θ (S)) ∙ ψθ(s, a)
(105)
where de(∙) denotes the discounted visitation measure de(S) := Es0〜η [(1 — γ) P∞=o γtPπθ (St
S|S0) . According to the policy gradient as shown in (105), it holds that:
Ves J(θt) :
1 - γEst〜dθt (∙),at〜∏θt (∙∣st) ](Q∏θt (St, at) - V∏θt (St)) • (N X ψθj (St, aj,t； θj,t))
(106)
31
Under review as a conference paper at ICLR 2022
Therefore, the third term in (103) could be bounded as
N
αt∙ X E
j=1
Vθs J(θt),δt ∙ ψθj (st, aj,t; θj,t) - (1 - Y) ∙ Vθs
αt ∙ N ∙ E
_________ 1 N	_________
(▽asJ(θt), δt ∙ (N X ψθj(St, aj,t; θj,t)) -(I- Y) ∙ vθsJ(θt)
=)αt ∙ N ∙ E E
j=1
1 N
[(vθsJ(θt),δt ∙ (NXψθj(St,aj,t;θj,t)) -(I-Y) ∙ vθsJ(θt)^∣θt]
N j=1
N
at ∙ N ∙ E ( Vθs J(θt),	Est〜μ%(∙),at〜∏%(∙∣st)
1N
(St, at) - V∏θt (St)) ∙ (N E ψθj(St, aj,t; θj,t)) θt
j=1
-(1 - γ) ∙ Vθs J(θt)
(ii)
≥ -αt ∙ Lv ∙ N ∙	Es
t〜μθt (∙),at〜∏θt(∙∣St)
1N
(St, at) - v∏θt (St)) ∙ (N E ψθj(St, aj,t; θj,t))
(iii)
≥ -αt ∙ Lv ∙ N ∙ sup
s,a
-Est~dθt(∙),at~∏θt(∙∣st)
j=1
1N
(St, at) - v∏θt (St)) ∙ (N E ψθj(St, aj,t; θj,t))
j=1
1N
(Q∏θt (St, at) -	V∏θt	(St)) ∙ (N〉： ψθj	(St,	aj,t;	θj,t))	∙	dTV (μθt,	dθt	)
j=1
(iv)	1	1
≥ -4αt ∙ N ∙ Lv ∙ Cψ ∙ RmaX ∙ I logτ K + 1-------T ) = -αt ∙ N ∙ Esp
(107)
where (i) follows the tower rule; (ii) follows (18) and (106); (iii) is due to distribution mismatch
between the policy gradient in (106) and its estimator; the sampling error Esp is defined in (29) and the
inequality (iv) is due to the facts that Q∏°t (St, at) ≤ R-ax, V∏°t (St) ≤ R-ax, kψθs (St,aj,t; θj,t)k ≤
Cψ and the distribution mismatch inequality (19) in Lemma 4.
By plugging the inequalities (104) - (107) into (103), we obtain the following:
N
I1,2 := N X E
i,j=1
「 T/ C ∖ 个士	，	/	八、
vθs J (θt), δt ∙ ψθj (St, aj,t; θj,t)
≥ -2N ∙ at ∙ Lv ∙ Cψ ∙ Eapp — at ∙ N ∙ Esp + at ∙ (1 — γ) ∙ N ∙ E ∣∣Vθs J(θt)k2	(108)
Moreover, by adding (101) and (108), we obtain I1 := I11 + I12 as below:
Ii	≥	-2N ∙	at	∙ Lv ∙	Cψ	∙	Eapp —	at	∙ N ∙	Esp	+ at ∙ (1 — γ) ∙ N ∙ E	∣∣Vθs J(θt)∣∣2
-4at ∙ Cψ ∙
{n ∙ E ∣VθsJ(θt)k2
kω: - ωi,t k2
+ E IN - λi,t∣2	. (109)
N
32
Under review as a conference paper at ICLR 2022
In order to bound term /2, we are able to further analyze /2,1 and /2,1 as below:
N
/2,1 ：= αt ∙ EE
i=1
▽碟 J(θt), ( bi,t — δt )≠θp (st, ai,t; θi,t)
N
-αt ∙ XE(V碟 J(θt), (γφ(st+1) — φ(st))T(5 — ωi,t) ∙ ψθp(st,«i,t； θi,t)
i=1 L
N
—at ∙ ^X E [ ((iv) vθ" (Ot) ,夕(S t, at)τ (λt — λi,t) ∙ ψθp (St) ai,t; θi,t),[
(i)	N
≥ —2αt ∙ Cψ ∙ XE l∣vθ"(Ot)Il ∙(限；一 ωi,t∣∣ + llʌt — %,t∣∣)
i=1
=-2αt ∙ Cψ ∙ X ∖ (E l∣vθp j(Ot)Il ∙ (l∣ω― ωi,t∣l + IN- %,t∣∣
2
i=1 V ∖ L
(ii)	£ f^r	T
≥ —2αt∙Cψ E √E ∣∣Vθp J(θt)k2 ∙
i=1 VL	」
J 卜同一“,til + IN — λi,tk)2
(iii)
≥	-4αt∙Cψ ∙
N
XE ∣VθpJ(Ot)I2
i=1
N
XE 同一ωi,tk2 + IN- λi,tk2
i=1
(110)
where (i) is due to the facts that all feature vectors are bounded by Assumption 4 and the score func-
tions are bounded as : I≠θp(st, ”t； θi,t)∣∣ ≤ Cψ in (14a); (ii) and (iii) are from Cauchy-Schwarz
inequality. Moreover, for the term /2,2 in (99), we can further express it as:
I2,2 ：= αt X E (vθpj (Ot),B ∙ ≠θp (St,ai,t； θi,∕R
at ∙
N
(1—γ) XE ∣∣VθpJ(Ot)I2 +
i=1
N
αt X E
i=1
(VθpJ(Ot), G — δt
Ψθi (st,αi,t; θi,t)
N
+ αt X E
i=1
vθp j (Ot), δt ∙ ψθp (St, ai,t; θi,t) - (1 - Y)vθ? J (Ot)
(111)
where we have defined δt :=尸t + γV∏% (st+1) - V∏θt (st).
For each term above in (111), it holds that
Qt
N
XE
i=1
J(Ot), ( bt - δt ) ∙ ≠θp (St, αi,t; θi,t)
N
≥ -Qt XE ∣∣vθp j(Ot)II ∙ ll≠θp(St,ai,t;θi,t)∣∣ ∙ ∣δt - δt∣
i=1
(i)
≥ —Qt ∙ Lv ∙ Cψ ∙ N ∙ E
夕(St, αt)τN —九
0(St+I)Tωt - v∏θt (St+1)
+ Y
≥	-Qt	∙	LV	∙ Cψ	∙N ∙ [ E[∣夕(St,	at)Tλt - rt∣]	+	YE	|0(St+1)Tωt -	V∏θt	(St+1)∣ + E	lvπ% (St)- φ(St)Tωt I
+ Y ^E
(iii)
≥ -Qt ∙ LV ∙ Cψ ∙ N ∙
(jE W(St, at)Tλti-rM2
(0(St+I)Tωt - v∏θt (St+1))
E
+
(iv)
≥ —2N ∙ Qt ∙ LV ∙ Cψ ∙ Eapp
(112)
33
Under review as a conference paper at ICLR 2022
where (i) follows (18) and kψθp (st, ai,t; θi,t)k ≤ Cψ; (ii) follows from the triangle inequality; (iii)
follows Jensen’s inequality; (iv) follows the definition of approximation error in (28).
Similarly as the derivation in (107), we can further bound the third term in (111) as below:
N
αtXE
i=1
VθpJ(θt), δt ∙ ψθl(st, ai,t; θi,t) - (1 - Y)VθpJ(θt) ) ≥ -αt ∙ N ∙ ∈sp.
(113)
Plugging the inequalities (112) - (113) into (111), then it holds that:
N
I2,2 ≥ αt ∙ (1 - Y) ∙ X E ∣∣vθp J (θt)k2 - αt ∙ N ∙ Esp - 2αt ∙ N ∙ Lv ∙ Cψ ∙ Capp.	(114)
i=1
Adding (110) and (114), it follows that:
N
I2,1 + I2,2	≥	αt	∙ (1 - Y) ∙ X E ∣∣vθp J (θt)k2	-	αt	∙ N ∙	CsP	-	2αt	∙ N ∙ Lv ∙	Cψ	∙ CaPP
i=1
uN
-4αt∙Cψ ∙ t XE ∣VθpJ(θt)k2
i=1
N
XE (kωt - ωi,tk + IN-λi,tk)2
i=1
(115)
∖
Denote Pt= PN=I E[∣ωJ= — ωi,t∣2 + IN — λi,t∣∣2]. Then We plug the inequalities (109) and (115)
into (99), it holds that
E[J9t+ι)] ≥ E[J(θt)] - N " 'p ∙ Lj ∙ E[∣Q ∙ θS∣∣]-
N ∙ Lj ∙ α2 ∙ 'p
2
+ at ∙ (1 — γ) ∙ N ∙ E ∣∣VθsJ(θt)∣∣2 — 2N ∙ at ∙ Lv ∙ Cψ ∙ CaPP — at ∙ N ∙ CsP
N
+ αt ∙ (1 -	Y)	∙ X E	∣∣vθp J (θt)k2	- 2N ∙ at ∙ Lv ∙	Cψ	∙ CaPP -	at	∙ N ∙	CsP
i=1
-4at∙Cψ ∙
{n ∙ E ∣Vθs J(θt) k2 +
N
XE ∣VθipJ(θt)∣2
i=1
• √Pt. (116)
Denote a constant Cs := N • & • LJ. Rearrange inequality (116) and apply Cauchy-Schwarz
inequality, it holds that
N
N ∙ E MJ(θt)k2 + XE ∣VθpJ(θt)k2
i=1
at • (1 - Y)
J(θt+ι) - J
+
+ 4N • Lv • Cψ • CaPP + 2N • CsP
T • E 卜。θsk
8C,.	「---------- J 二「
+ L∙ t N• E ∣VθsJ(θt)k2 +£E ∣VθPJ(θt)k2 •	VPt
1 - Y	i=1 i
(117)
34
Under review as a conference paper at ICLR 2022
Then We can denote Gt := N ∙ E
∣∣Vθs J (θt)k2 + PN=IE llvθp J (θt) k2 . Through summing the
inequality (117) from t = 0 to T - 1 and divide T on both side, it holds that
1T-i
T X Gt
t=0
1 T-i	1
≤ T X αt(ι-γ) ∙(E[J (θt+i)]- E
T-1
+ T(1⅛ ∙ XEU θsk
{Z
term M1
+ ------- 1 4N ∙ Lv ∙ Cψ ∙ CaPP + 2N ∙ CSP) + T=TT-_ψ~~ʌ
1 - Y	T(1 - Y)
X-----------------------------------------}
^{l^^^-
term M2
T-1
∙ XPGt ∙ PP
t=0
NLJ 'P
2T(1-γ)
T-1
∙ X αt
t=0
(118)
+
}
Then (118) could be expressed as beloW:
1 T-1
T X Gt
t=0
(118)
≤ Mi + M2 +
8Cψ
T ∙(1-Y)
T-1
∙ XPGt ∙ PPt
t=0
(a)	8Cψ	u
≤ Mi + M2 + —ɪ ∙ ʌ
1-γ
-1	-1
T X Gt ∙ t T X Pt
T-1
t=0
T-1
t=0
where (a) follows Cauchy-Schwarz inequality. Then We define Cg := ∣Cγ, Bi :
B2 ：= Mi + M2 and B3 := T Pt=o1
Bi ≤ B2 + Cg ∙ √Bι ∙ B3
(119)
1 Pτ-i 广
T 乙t=0 Gt,
Pt. The inequality (119) could be expressed as below:
:=⇒ (PB - C ∙ PB3)2≤ B2 + C2 ∙ B3
PBr ≤ yB2÷^g-∙B3	+	-2g	∙	pB3	=⇒	Bi	≤	2B2 +	c2	∙	B3
(120)
Then we are able to analyze the convergence rate of each term in (118). Recall the fixed stepsize
αt = T⅛. We bound each component in Mi defined above.
1	T-i 1
T X ɑtɑ-ʒ) ∙(EJ (θt+i)]- E
(a)	Tσ1
≤ —
α∙
(b)	T σι	Rmax
αT -i ∙ T ∙ (1 - Y )	1 - Y
∙ T ∙ (1 - γ)
Tσ1
∙ EJ (θτ)]
O Tσ1-i
(121)
where (a) follows ɑt = T⅛; (b) is due to the fact that each reward is bounded by Rmax and
J(θ) ≤ P∞=0 Yt ∙ Rmax = ¾ax for any θ ∈ RN×D.
For the second and third terms in Mi, it holds that
Cs
(1-Y) ∙ T
N ∙ LJ ∙ 'P
(1-Y) ∙ T
T-1
∙XE ∣Q ∙ θts∣
t=0
T-1
∙ X αt
t=0
(=63) O T-σ1
N ∙ LJ ∙ 'P
(I-Y)
α
T σ1
O Tσ1
(122)
(123)
Combining (121) - (123), we obtain that the convergence rate of term Mi in (118) could be expressed
as below:
Mi = O Tσ1-i + O T-σ1	(124)
35
Under review as a conference paper at ICLR 2022
Then according to (118) and (120), it holds that
1 T T / r _____________
TE N ∙ E kVθsJ(θt)k2
T t=0
N
+ XE kVθipJ(θt)k2
i=1
≤2M1 + 2M2 + CT2 XXX XX E llωt - ωi,tk2 + llλt - λi,t k2
i=1 i=1
By applying in the convergence results of approximation parameters in (92), we obtain that
T-1
E N ∙ E kVθsJ(θt)k2 +£E kVθpJ(θt)k2
t=0
i=1
92)	2 T-1 N
≤ 2Mι + 2M2 + τΣΣ E ∣ωt - ω*∣2
t=0 i=1
+ E llʌt - λt k2
2 T-1
+ τ X (E kQ ∙ ωtk2
+E
kQ∙ λtk2)
1
T
N
O(Eapp + Csp) + O(Tσ1-1) + O(T-σ1) + O(T-1+σ2) + O(T-σ2) + O (Tσ2-2σ1)
+ O(T
-2σ1 +2σ2
) + O(T
-2+2σ2
) + O(T
-2σ2
).
(125)
To optimize the convergence rate, We choose at = O(T16), βt = O(T14) so that σι = 5 and
σ2 = 5, then plug them into (125). Therefore, the convergence rate of the actor is:
1
T
N
T-1
EN ∙E
t=0
kVθsJ(θt)k2 + EE kVθpJ(θt)k2
O(T 5 ) + O(capp + esp).
(126)
i=1
This completes the proof.
□
I Detailed Analysis for Double Sampling Procedures
For the CAC algorithm With double sampling procedures in Algorithm 2, We generate tWo different
tuples Xt := (st, at, st+ι) and Xt := (st, at, St+ι) in each iteration t to perform critic step and
actor step. In xt, the state St is sampled from the stationary distribution μe± (∙). In Xt, the state St is
sampled from the discounted visitation measure d^ (∙) where dθ(S) := (1 一 Y) P∞=0 γt ∙ Pπθ (St =
s | so ~ η).
Then the difference between double sampling procedures and single sampling procedure comes from
bounding the term I1,2 in (103) and I2,2 in (111). With double samples Xt and XSt at each iteration t,
the sampling error Csp could be avoided in analyzing I1,2 and I2,2
With tuple XSt to perform actor step, the component I1,2 in (103) could be expressed as
I1,2 ：= at ∙ (1 一 γ) ∙ N ∙ E ∣∣Vθs J(θt)k2 + at ∙ Xe](Vθs J(θt), @ — δt) ∙ ψθs (st,aj,t; θj,t)
+ at ∙ XX E ](Vθs J(θt), δt ∙ ψθj(St, Sj,t； θj,t) — (1 一 Y) ∙ Vθs J(θt)R
Following the inequality (104), the second term in I1,2 above could be bounded. Then it holds that
I1,2 ≥ - 2N ∙ at ∙ Lv ∙ Cψ ∙ Capp + at ∙ (1 — Y) ∙ N ∙ E ∣∣vΘs J(θt)∣∣2
+ at ∙ X e](Vθs J(θt),δt ∙ ψθj (St,Sj,t； θj,t) — (1 — γ) ∙ Vθs J(θt)R
36
Under review as a conference paper at ICLR 2022
For the third term in I1,2, it holds that
N
αt∙ X E
j=1
Vθs J(θt), δt ∙ ψθj (st, aj,t; θj,t) - (1 - Y) ∙ Vθs
1 」V	]
=at ∙ N ∙ E (Vθs J(θt), δt ∙ (N X ψθj (st,sj,t; θj,t)) - (1 一 Y) ∙ Vθs J(θt))
j=1
1 N
=) at ∙ N ∙ E E[(Ves J (θt) ,δt∙( N X ψθj (st, j； θj,t)) - (1 一 Y) ∙Vθs J (θt)〉[仇]
N j=1
(=)at . N ∙ E(VesJ(θt), (Eg=〜de=	(.),a=〜容古(.瓦)卜Q∏θt (st,	at) 一	V∏θt	(Sty)	∙ (N X	ψθj	(st,	aj,t； θj,t))
-(1 - Y) ∙Vθs J(θt)
=0
where (i) follows Tower rule; (ii) follows the policy gradient expression in (106).
Therefore, we bound term I1,2 as below:
I1,2 ≥-2N ∙αt∙ Lv ∙Cψ ∙ ^p + at ∙ (1 - γ) ∙ N ∙ E ∣∣Ves J (θt) ∣∣2
Moreover, the component I2,2 in (111) is expressed as below:
N
I2,2 :=at(1 - Y) XE kVθipJ(θt)k2
i=1
N
+atXE
i=1
J(θt),	b： - δt	∙ ψθp(St,Si,t; θi,t)
+ αt X E(V6p J (θt), δt ∙ ψθp (st, Si,t； θi,t) - (1 - Y)V6p J (θt),]	(127)
Similarly following the same steps in analyzing term I1,2, the sampling error sp could be avoided
due to double samples in each iteration t. Hence, it holds that
N
I2,2	≥at ∙ (1 -	Y)	∙ X E	∣∣vθp J (θt )k2	-	2at ∙ N ∙ Lv	∙	Cψ	∙ Eapp
i=1
Following remaining analysis steps in Proposition 2, we obtain the convergence analysis for CAC with
double sampling procedures in Algorithm 2. We are able to avoid the sampling error Esp at the cost of
utilizing one more sample at each iteration. Therefore, we are able to present the results in Corollary
1.
37