Under review as a conference paper at ICLR 2022
ImaginE: An Imagination-Based Automatic
Evaluation Metric for Natural Language
Generation
Anonymous authors
Paper under double-blind review
Ab stract
Automatic evaluations for natural language generation (NLG) conventionally rely
on token-level or embedding-level comparisons with the text references. This is
different from human language processing, for which visual imaginations often
improve comprehension. In this work, we propose ImaginE, an imagination-
based automatic evaluation metric for natural language generation. With the help
of CLIP (Radford et al., 2021) and DALL-E (Ramesh et al., 2021), two cross-
modal models pre-trained on large-scale image-text pairs, we automatically gen-
erate an image as the embodied imagination for the text snippet and compute the
imagination similarity using contextual embeddings. Experiments spanning sev-
eral text generation tasks demonstrate that adding imagination with our ImaginE
displays great potential in introducing multi-modal information into NLG evalua-
tion, and improves existing automatic metrics’ correlations with human similarity
judgments in many circumstances.
1	Introduction
A major challenge for natural language generation (NLG) is to design an automatic evaluation met-
ric that can align well with human judgments. To this end, many approaches have been investigated.
Metrics that base on matching mechanisms such as BLEU (Papineni et al., 2002), METEOR (El-
liott & Keller, 2013), CIDEr (Vedantam et al., 2015), have been widely adopted in the field. Edit-
distance based metrics, such as CharacTER (Wang et al., 2016), WMD (Kusner et al., 2015b),
SMD (Clark et al., 2019b), have also been explored. Recently, Zhang et al. (2020) proposed to
leverage BERT (Devlin et al., 2019) embeddings for computing text similarity, which correlates bet-
ter with human judgments than previous methods. These automatic evaluation metrics make use of
textual information from various angles extensively.
Unlike commonly used automatic methods that compare the generated candidates with the refer-
ences on the text domain only, humans, in contrast, leverage visual imagination and trigger neural
activation in vision-related brain areas when reading text (Just et al., 2004). Cognitive studies show
that visual imagery improves comprehension during human language processing (Sadoski & Paivio,
1994). Inspired by this imagination-based multi-modal mechanism in human text comprehension,
we ask a critical research question: can machines create a visual picture of any underlying sentence,
and leverage their imaginations to improve natural language understanding? The advances of pow-
erful pre-trained vision-language models such as CLIP (Radford et al., 2021) provide an excellent
opportunity for us to utilize the learned image-text representations and achieve high performance
on image-text similarity estimation in a zero-shot fashion. This enables us to introduce multi-modal
information into NLG evaluation by generating visual pictures as embodied imaginations.
In this work, we propose ImaginE, an imagination-based automatic evaluation metric for NLG.
ImaginE first uses the pre-trained discrete variational autoencoder (dVAE) from the vision-
language model DALL-E (Ramesh et al., 2021) to visualize imagination, which is to generate de-
scriptive images for the candidate text and the references. Then ImaginE computes the similarity
of the two text snippets and the similarity of the two imaginative images with the pre-trained CLIP
model (Radford et al., 2021). Figure 1 shows an evaluation example.
1
Under review as a conference paper at ICLR 2022
Text for Summarization:
Kevin Garnett scored ## points in his return after a one-game suspension and the Boston Celtics ripped Detroit
##-## here Thursday in a rematch of last season's NBA semi-finals.
Metric
Score
Reference:
BLEU
Basketball: Garnett makes triumphant return as
Celtics top Pistons
Render DALL-E
Imagination
ROUGE-1
ROUGE-2
ImaginatiOnRef
Hypothesis:
Celtics sink Detroit ##-## in NBA semi-final rematch
Render DALL-E
Imagination '
IMAGINEtext
Cosine Similarity
IMAGINEimage
Cosine Similarity
ROUGE-L
BERTScore
ImaginEtext
ImaginEimage
Human
ImaginatiOnHyP
2.43 X
12.50 X
0.00 X
10.89 X
-5.28 X
76.86 ⑨
62.30 ⑨
4.2/5.0 ⑨
Figure 1: An evaluation example on GigaWord for text summarization. IMAGINE visualizes ma-
chine imagination with DALL-E's pre-trained dVAE and extracts textual and visual representations
with CLIP. While traditional evaluation metrics for natural language generation rely on n-grams
matching or textual embeddings comparison, IMAGINE introduces imagination into the evaluation
process and understands the text snippet as a whole with the help of multi-modal information.
To understand the role imagination plays in NLG evaluation, We conduct a series of experiments with
Imagine on multiple NLG tasks, including machine translation, abstractive text summarization,
and data-to-text generation, aiming to answer the following questions:
1.	How influential is IMAGINE in NLG evaluation in terms of correlations with human judg-
ments? Can it provide additional reference information on top of existing metrics?
2.	What are the applicable scenarios of introducing IMAGINE to NLG evaluation? When and
why does imagination help or not?
3.	What are the potentials and limitations of introducing imaginations with IMAGINE to NLG
evaluation?
Experimental results point out that in a standalone mode for pairwise comparisons, ImaginE cannot
replace textual similarity metrics. However, adding ImaginE similarity scores to existing metrics
surprisingly improves most of the popular metrics’ correlations with human performance. Analysis
of case studies indicates that ImaginE can reflect the keyword difference in the visualized imagi-
nation, even if the hypothesis and reference text have high n-grams overlaps. In addition, IMAGINE
can grasp the gist of two text snippets with similar meanings and renders imaginations that are alike,
even if the two pieces of text have distinct word choices. Overall, ImaginE displays great potential
in introducing multi-modal information into NLG evaluation.
2	Related Work
Automatic Metrics for Natural Language Generation Common practices for NLG evaluation
compare the generated hypothesis text with the annotated references. Metric performance is conven-
tionally evaluated by its correlation with human judgments. Existing automatic evaluation metric
calculations are mainly based on three mechanisms: n-grams overlap, edit distance, and embedding
matching. Some typical n-gram based metrics include BLEU (Papineni et al., 2002), ROUGE-
n (Lin, 2004), METEOR (Elliott & Keller, 2013) and CIDEr Vedantam et al. (2015), which are
widely used for text generation tasks. Another direction is based on edit distance (TOmgS et al., 2003;
Snover et al., 2006; Panja & Naskar, 2018; Tillmann et al., 1997; Wang et al., 2016) , where they cac-
ulate the edit distance between the two text snippets with different optimizations. Embedding-based
metrics (Kusner et al., 2015a; Rubner et al., 1998; Clark et al., 2019a; kiu Lo, 2017; 2019) evaluate
text quality using word and sentence embeddings, and more recently, with the help of BERT (Zhang
et al., 2020; Sellam et al., 2020).
Multi-Modal Automatic Metrics Aside from previous text-only metrics, there also appear met-
rics that utilize pre-trained multi-modal models and introduce visual features on top of text refer-
ences for NLG evaluation. TIGEr (Jiang et al., 2019) computes the text-image grounding scores
with pre-trained SCAN (Lee et al., 2018). ViLBERTScore-F (Lee et al., 2020) relies on pre-trained
ViLBERT (Lu et al., 2019) to extract image-conditioned embeddings for the text. The concurrent
CLIPScore (Hessel et al., 2021) proposes a text-reference-free metric for image captioning by di-
rectly comparing the image features with caption embeddings with CLIP (Radford et al., 2021).
Our method differs in that we use visual picture generation as embodied imaginations and apply our
metric to various text-to-text generation tasks.
2
Under review as a conference paper at ICLR 2022
Figure 2: IMAGINE similarity score computation process. Given the reference text Xref and the
generated hypothesis Xhyp, We visualize the machine imagination Iref and Ihyp with the pre-trained
dVAE. We extract features for the pair of text and corresponding pair of imagination with CLIP.
IMAGINEimage is the cosine similarity of the imagination representations, while IMAGINEtext is
the cosine similarity of the text representations.
Mental Imagery The great imagery debate is still an open question in the neuroscience and psy-
chology community (Troscianko, 2013). The debate between pictorialists and propositionalists is
about how imagery information is stored in the human brain. We follow the views from pictorial-
ists that information can be stored in a depictive and pictorial format in addition to language-like
forms (Kosslyn et al., 2001; Pearson & Kosslyn, 2015). In pictorialists’ model, mental imagery
is constructed in the “visual buffer” either from the retinal image in seeing or from a long-term
memory store of “deep representations” in the brain. Our method of image generation is to mimic
the generation of deep representations in machines, with the help of recent powerful text-to-image
models. Inspired by empirical studies from cognitive science that visual imagination improves hu-
man text comprehension (Gambrell & Bales, 1986; Nippold & Duthie, 2003; Just et al., 2004; Joffe
et al., 2007), we are interested in exploring if one can draw similar conclusions from automatic text
evaluations by machines.
3	ImaginE
3.1	Model Details
CLIP CLIP (Radford et al., 2021) is a cross-modal retrieval model trained on WebImageText,
which consists of 400M (image, caption) pairs gathered from the web. WebImageText was con-
structed by searching for 500K queries on a search engine. The base query list is all words oc-
curring at least 100 times in the English version of Wikipedia, augmented with bi-grams with high
pointwise mutual information as well as the names of all Wikipedia articles above a certain search
volume. Each query includes 20K (image, text) pairs for class balance.
In this work, we use the ViT-B/32 version of CLIP, in which the Vision Transformer (Dosovitskiy
et al., 2020; Vaswani et al., 2017) adopts BERT-Base configuration and uses 32 X 32 input patch
size. The Vision Transformer takes 224 X 224 input image and the self-attention maps are calculated
between 7 X 7 grid of image patches. The Text Transformer has 12-layer, 8-head and uses a hidden
size of 512, and is trained over a vocab of 49K BPE token types (Radford et al., 2019; Sennrich
et al., 2016). The text representation is the last hidden state of the “[EOT]” token being projected
by a linear layer. The model’s weights are trained to maximize the similarity of truly corresponding
image/caption pairs while simultaneously minimizing the similarity of mismatched image/caption
pairs using InfoNCE (Sohn, 2016; van den Oord et al., 2018).
DALL-E DALL-E (Ramesh et al., 2021) is a 12-billion parameter version of GPT-3 (Brown et al.,
2020) trained to generate images from text descriptions. The model is trained on a dataset of a
similar scale to JFT-300M (Sun et al., 2017) by collecting 250 million text-image pairs from the
internet, which incorporates Conceptual Captions (Sharma et al., 2018), the text-image pairs from
Wikipedia, and a filtered subset of YFCC100M (Thomee et al., 2016).
DALL-E trains a discrete variational autoencoder (dVAE) (Rolfe, 2017) to encode each 256 X 256
RGB image into a 32 X 32 grid of image tokens with a vocabulary size of 8192. The image tokens
are concatenated with a maximum of 256 BPE-encoded (Sennrich et al., 2016; Radford et al., 2019)
3
Under review as a conference paper at ICLR 2022
tokens with a vocabulary size of 16384 that represents the paired image caption. DALL-E trains
an autoregressive transformer to model the joint distribution over the text and image tokens. The
pre-trained dVAE has been made public, while the pre-trained transformer is not released. Thus, we
use DALL-E’s pre-trained dVAE to render images in this project.
3.2	ImaginE Similarity Score
Construct Imagination For each image, we randomly initialize a latent matrix H and use the pre-
trained dVAE to produce the RGB image I “ dV AE_decoderpHq. We use the ViT-B/32 version
of the CLIP model to encode the generated image I and the input text x. Then we use CLIP to
compute the similarity between the received image embedding v “ CLIP pIq and text embedding
t “ CLIP pxq as the loss to optimize the hidden matrix while keeping the weights of the network
unchanged. We optimize each generation process for 1000 steps, and refer to the generated image
as the imagination for further computation.
l…	..=一旦
lossgeneration }v}}t}
(1)
Similarity Measure For the generated text snippet xhyp and all the references txrefi uin“1, we
generate corresponding images Ihyp and Irefi for i P r1, ns, where n is the number of parallel refer-
ences. During evaluation, we pass both the pair of text snippets and the corresponding imaginations
through corresponding CLIP feature extractors to receive the textual representation thyp, trefi, and
the imagination representations vhyp, vrefi. Then, we compute three types of similarity scores for
IMAGINE with the received embeddings: IMAGINEtext compares the hypothesis text xhyp with the
text references xrefi ; IMAGINEimage compares the visualized imaginations Ihyp with Irefi , gener-
ated by the pre-trained dVAE in previous steps; IMAGINEtext&image is the average of IMAGINEtext
and IMAGINEimage , which takes both the text and the imagination into consideration.
IMAGINEtext “ 1 寸 JTyptrf：
n i⅛ }thyp}}trefj
IMAGINEimage “ 1 Z JTypvre∖
n i“1 }vhyp}}vrefi}
(2)
(3)
3.3	Extension to Existing Metrics
The ImaginE similarity scores can be used as individual automatic metrics. Apart from this, Imag-
inE can also act as an extension to existing metrics, as it provides multimodal references that com-
pensate for current text-only evaluations that compare tokens or text-embeddings. Our adaptation of
ImaginE to other automatic metrics is direct, which is summing up ImaginE similarity score with
the other automatic metric score for each example:
metric_score1 “ metric_score ` IMAGINE_similarity_score	(4)
4	Experimental Setup
Tasks, Datasets, and Models We evaluate our approach on three natural language generation
tasks: machine translation, abstractive text summarization, and data-to-text generation. For ma-
chine translation, we use Fairseq (Ott et al., 2019) implementation to generate English translation
from German on IWSLT’14 (Bell et al., 2014) and WMT’19 (Barrault et al., 2019) datasets. We
choose these two to-English translation tasks because currently, DALL-E and CLIP only support
English. For abstractive text summarization, we use the implementation of Li et al. (2017) to gen-
erate sentence summarization on DUC20041 and use ProphetNet (Yan et al., 2020) for generation
on Gigaword2. We choose abstractive text summarization instead of document summarization since
CLIP sets a length limit of input text of 77 BPE tokens. For data-to-text generation, we conduct ex-
periments on three datasets, namely WebNLG (Gardent et al., 2017), E2ENLG (Dusek et al., 2019;
1https://duc.nist.gov/duc2004/
2https://catalog.ldc.upenn.edu/LDC2011T07
4
Under review as a conference paper at ICLR 2022
Original +BERTtβa +ImaginEtext	+ImaginElmage	ssβn +∣maginEtεxtuιπage
Figure 3: The effectiveness of augmenting BLEU-n (n=1,2,3,4) and BERTScore with IMAGINE
similarities and BERTtext similarity on two machine translation datasets. The y-axis shows the
Pearson correlation with human judgments.
Src: Also entschied ich mich eines tages den filialleiter ZU besuchen, Und ich fragte
den Ieiter, "funktioniert dieses modell, dass sie den menschen all diese
moglichkeiten bieten WirkIich?"
Ref: So I one day decided to pay a visit to the manager, and I asked the
manager, "is this model of offering people all this choice really working?"
Hyp: So I decided to visit the filialler one day, and I asked the ladder, "does
this model work that you really offer to the people all these possibilities?"
Src: Diesmal dabei: Der Schauspieler Florian David Fitz bekannt aus
Filmen wie "Mannerherzen", "Terror - Ihr Urteil" oder "Der geilste Tag".
Ref: This time: The actor Florian David Fitz known from films
BERTScore
BLEU-I
BLEU-4
ImaginEimage
ImaginationRef	ImaginationHyP
(a) IWSLT'14
Metric
Score
69.70
20.26
66.62
34.81
Human 3.1/5.0
like "Mannerherzen", "Terror - Ihr Urteil" or "Der geilste Tag".
Hyp: This time around: The actor Florian David Fitz is known
from films such as "Men's Hearts," "Terror - Your Judgment"
and "The Horniest Day.1
BLEU-1 45.83
BLEU-4 22.17
BERTScore 34.91
ImaginEimage	58.35
ImaginationRef	ImaginationHyP
(b) WMT'19
Metric Score
Human 3.8/5.0
Figure 4: Case studies for machine translation. Src: the German text to be translated. Ref: the
reference translation. Hyp: the generated translation candidate. We report the metric scores and the
human score for the reported pair of (Ref, Hyp).
2020) and WikiBioNLG (Lebret et al., 2016). We use the text generated by the KGPT (Chen et al.,
2020) model in our experiments. Table 3 lists out the statistics of the test set used for each dataset.
Automatic Metrics For machine translation, we report BLEU-n (Papineni et al., 2002) for n “
1,2,3,4 and BERTScore (Zhang et al., 2020). For abstractive text summarization, we report results
on ROUGE-1, ROUGE-2, ROUGE-L (Lin, 2004) and BERTScore. For data-to-text generation, we
utilize five automatic metrics for NLG, including BLEU, ROUGE-L, METEOR (Elliott & Keller,
2013), CIDEr (Vedantam et al., 2015) and BERTScore. In comparison with IMAGINEtext, we also
compute BERTtext, the text similarity score with BERT encoder. We use the last hidden state for
the “[CLS]” token as the representation of the text snippet, and compute cosine similarity with the
two “[CLS]” embeddings for the reference and the generated text candidate.
Human Evaluation We invite MTurk3 annotators to judge the quality of the generated text. The
estimated hourly wage is $12. We use the complete test set for DUC2004 and E2ENLG, which
contains 500 and 630 examples, respectively. For the remaining five datasets, we randomly sample
1k pair of test examples for human evaluation due to the consideration of expenses. Each example is
scored by three human judges using a 5-point Likert scale. The generated text is evaluated from three
aspects, namely fluency, grammar correctness, and factual consistency with the reference text. We
take the mean of human scores to compute correlations. In the following sections, we report Pearson
correlation (Freedman et al., 2007) to human scores. We also record Kendall correlation (Kendall,
1938) in the Appendix.
5	Results
5.1	Machine Translation
Figure 3 shows the system-level Pearson correlation to human judges when extending our Imag-
inE similarity to existing automatic NLG metrics on the IWSLT’14 and WMT’19 German to
English datasets. IMAGINEtext and IMAGINEtext&image steadily improves all the listed metrics’
3https://www.mturk.com/
5
Under review as a conference paper at ICLR 2022
Original +BERTtext +lmaglnEte×t +ImagInEimage	i®物 +∕mag∕nEteXtsʃmag
Figure 5: The effectiveness of augmenting BLEU, BERTScore and ROUGE-related metrics with
IMAGINE similarities and BERTtext similarity on two abstractive text summarization datasets. The
y-axis shows the Pearson correlation with human judgments.
Src: AS his lawyers in London tried to quash a Spanish arrest warrant for Gen.
AuguSto Pinochet, the former Chilean dictator, efforts began in Geneva and
Paris to have him extradited.
Ref: Pinochet arrest contested in British high court. New charges pressed
Hyp： Pinochet extradited from London to Paris to extradite PinoChet
ROUGE-L
ImaginEimage
Metric
ROUGE-2
BERTScore
ImaginationRef	ImaginationHyP
(a) DUC2004
Score
0.00
10.43
19.09
67.38
Human 5.0/5.0
Src: The launch of Shenzhou-#, China's first manned spacecraft, is
successful and the craft is already in orbit, an official in charge of the
country's manned spaceflight program announced Wednesday morning.
Ref: Bulletin: Shenzhou-# launch successful official
Hyp: Launch of China's first manned spacecraft successful
Metric Score
ROUGE-2 0.00
ROUGE-L 29.33
BERTScore -7.53
ImaginEimage
(b) GigaWord
Human
74.02
4.3/5.0
Figure 6: Case studies for abstractive text summarization. Src: the text to be summarized. Ref: the
reference summary. Hyp: the generated summary candidate. We report the metric scores and the
human score for the reported pair of (Ref, Hyp).
correlations with human scores. IMAGINEimage and IMAGINEtext&image contributes the most in
IWSLT’14 while IMAGINEtext plays the most important role in WMT’19. IMAGINEimage also
enhances most of the metrics, correlations except for BERTScore in WMT’19. BERTtext has rela-
tively small impact on improving other metrics' correlation in the machine translation task.
Figure 4 lists out two examples for the case study. We notice that IMAGINE can capture the keyword
difference between the reference and the hypothesis text, even if they have similar sentence struc-
tures and high n-grams overlaps. IMAGINE shows its sensitivity to word choice in Figure 4(a). The
main difference between the reference text and the generated text is the mention of “manager” and
“ladder”. While other metrics score high, the quality of the generated text is questionable. In con-
trast, our ImaginE renders distinct imaginations and assigns lower image similarity. In Figure 4(b),
the reference text leaves the movie names in German, while the hypothesis text translates all con-
tents to English. Aside from this, the translations are nearly identical. However, ImaginE yields
completely different imaginations. This suggests that ImaginE’s performance is greatly impaired
when applied to non-English scenarios.
5.2	Abstractive Text Summarization
Figure 5 shows the system-level Pearson correlation to human judges when extending our Imag-
inE similarity to existing automatic NLG metrics on the DUC2004 and Gigaword. Both datasets
are built upon news articles. ImaginE can steadily improve BLEU, ROUGE-related metrics, and
BERTScore on DUC2004. IMAGINEtext contributes to the most significant improvement on Giga-
word. IMAGINEtext surpasses BERTtext on all metrics except for BERTScore on Gigaword.
ImaginE can capture the gist of texts with similar meanings and renders reasonable descriptive
imaginations that are alike, regardless of word choices. Figure 6 shows two sets of examples where
the hypothesis summary scores high in human evaluation but scores low on existing automatic eval-
uation metrics. Both examples have low n-grams overlaps between the hypothesis and reference
summary, but ImaginE renders similar imagination and assigns high image similarity scores, which
align with human scores.
6
Under review as a conference paper at ICLR 2022
Figure 7: The effectiveness of augmenting BLEU, METEOR, ROUGE-L, CIDEr, and BERTScore
with IMAGINE similarities and BERTtext similarity on three data-to-text generation datasets. The
y-axis shows the Pearson correlation with human judgments.
Ref: Julia Morgan was the architect of the grounds of ASiIomar
Conference.
Hyp: Julia Morgan was the architect of the Asilomar Conference
Grounds.
ImaginatiOnRef	ImaginatiOnHyP
BLEU
METEOR
BERTScore
ImaginEimage
(a) WebNLG
Metric
Score
Human
65.25
49.17
90.05
49.12
3.6/5.0
Ref: Sven Leuenberger (born August 25, 1969 in Niederuzwil,
Switzerland) is a retired Swiss professional ice hockey defender.
Hyp: 25 ft tall, Nieder Niederberger was a member of the club's
shoots team.
BLEU
METEOR 6.09
BERTScore -16.43
ImaginEimage	36.47
Human 2.2/5.0
(b) WikiBioNLG
Ref: There is a coffee shop Blue Spice in the riverside area.
Metric
Score
1.92
Ref: Giraffe, in the riverside area, near the Rainbow Vegetarian
Cafe, there is a pub with fast food, of and it is kid friendly.
Hyp: Giraffe is a dish that can be served as a dessert.
Metric
BLEU
METEOR
BERTScore
ImaginEimage
ImaginatiOnRef
ImaginationHyP	Human
(c) E2ENLG
Score
2.43
6.03
17.79
55.13
Hyp: Blue Spice is a type of coffee shop.
ImaginatiOnRef	ImaginatiOnHyP
METEOR
BERTScore
(d) E2ENLGNLG
Metric Score
BLEU 18.00
29.91
46.41
ImaginEimage	75.34
3.3/5.0
Human 3.9/5.0
Figure 8: Case studies for data-to-text generation. Ref: the reference text. Hyp: the generated text
candidate. We report the metric scores and the human score for the reported pair of (Ref, Hyp).
5.3	DATA-TO-TEXT GENERATION
Figure 7 shows the system-level Pearson correlation to human judges when extending our ImaginE
similarity to existing automatic NLG metrics on the WebNLG, WikiBioNLG, and E2ENLG datasets.
Figure 8 lists out four examples for the case study.
On WebNLG, adding IMAGINEtext and IMAGINEtext&image can steadily improve all the listed met-
rics’ correlation with human scores. IMAGINEimage improves BLEU, METEOR, ROUGE-L, and
CIDEr but it only has limited impact on BERTScore. Among the two metrics that compare tex-
tual similarity, IMAGINEtext boosts correlations more than BERTtext. As discussed in Section 5.1,
ImaginE shows its sensitivity to the input text snippet. We see this again in Figure 8(a), in which
changing the relative position of “grounds” shifts the central part of the imagination from a person
to the dirt ground.
We witness a drawback in most listed metrics’ correlations after applying our ImaginE approach
on WikiBioNLG. This is because the WikiBioNLG dataset is built upon Wikipedia biography, and
ImaginE is not good at visualizing abstract concepts. In Figure 8(b), our ImaginE failed to visual-
ize the player’s birth date or height. Such information may be contained in BERT pre-training data,
but is not as likely to be covered by the dataset to train CLIP, which explains IMAGINEtext ’s infe-
rior performance compared to BERTtext . Figure 7(b) shows the lowest Pearson correlation among
all three datasets on all metrics, which means this dataset is not only a challenge to our ImaginE
approach but also to other existing metrics as well.
7
Under review as a conference paper at ICLR 2022
On E2ENLG, textual similarity scores play a more influential role in improving correlation as it has
a positive impact on all listed metrics except for METEOR. B ERTtext outperforms IMAGINEtext
in all listed metrics except for ROUGE-L. On the other hand, IMAGINEimage has a salient negative
impact on correlation. The E2ENLG dataset is built upon restaurant domain information. We found
that ImaginE is sensitive and may be misguided by irrelevant information, such as the restaurant
names, which explains the poor performance of IMAGINEimage . For example, “Giraffe” and “Rain-
bow” in Figure 8(c) result in weird imagination that is unrelated to the main content of the generated
text. “Blue Spice” leads to the appearance of blue patches in Figure 8(d).
6 Discussion
Metric	Original	+BERTtext	+IEtext	+IEimagepdV AEq	+IEimagepBigGANq	+IEimagepV QGANq
ROUGE-1	13.66	14.05	17.21	16.05 ± 0.46	15.82 ± 0.72	15.93 ± 0.91
ROUGE-2	9.74	10.71	16.29	14.92 ± 0.61	14.62 ± 0.96	14.77 ± 1.21
ROUGE-L	13.14	13.65	17.66	16.25 ± 0.55	16.01 ± 0.85	16.12 ± 1.07
BERTScore	19.44	19.50	20.97	19.50 ± 0.43	19.29 ± 0.70	19.39 ± 0.90
BLEURT	23.59	23.53	24.28	23.47 ± 0.23	23.33 ± 0.39	23.39 ± 0.46
Table 1: The Pearson correlations with human judges when using B ERTtext similarity and IMAG-
inE similarities to augment ROUGE, BERTScore, and BLEURT on DUC2004. Here we computes
three sets of IMAGINEimage similarity scores (mean±std) with three different image generation
backbones for IMAGINE, namely dVAE, BigGAN, and VQGAN. IE: IMAGINE.
Image Generation Backbones In previous sections, we implement IMAGINE with dVAE as the
image generation backbone. There also appear a number of exciting and creative CLIP-based image
generation repositories such as BigSleep4 and VQGAN-CLIP5, which use BigGAN (Brock et al.,
2019) and VQGAN (Esser et al., 2021) to generate images respectively.
Here we discuss the choice of ImaginE’s image generation backbone and its effect on evaluation
performance. We conduct experiments on DUC2004 for summarization, and compare dVAE with
BigGAN and VQGAN. For fair comparisons, each generative backbone has a 1000-step learning
phase to render a 512x512 image for each piece of input text. Examining Table 1, we find compa-
rable IMAGINEimage performances when using different generative backbones. The dVAE leads to
slightly higher correlations and smaller variance. The variability of random initialization may cause
the larger variances of the two GAN-based image generators.
To assess the influence of random initialization, we repeat the image generation process five times
and compute pairwise visual similarities within each group of 5 images. Notice in Figure 9(a)
that dVAE has the highest intra-group visual similarity, which suggests that compared to the two
GAN-based generative backbones, dVAE is relatively more robust to the random initialization.
Applicable Scenarios As shown in Figures 3, 5 and 7, we notice that adding certain type of
ImaginE similarities improves non-embedding-based metrics’ correlations with human scores in
most cases. This suggests that it is helpful to extend text-only non-embedding-based metrics with
multimodal knowledge. Table 2 lists out each metric’s Pearson correlation with human judgments
on each dataset. In standalone-mode for pairwise comparisons, ImaginE similarity scores can
not replace textual similarity metrics. In Section 5.3, we find that ImaginE struggles to render
informative images on WikiBioNLG, a dataset that contains many abstract concepts that are hard to
visualize, such as specific date, length, weight, etc.
From Figures 5 and 7, it also occurs to us that ImaginE sometimes fails to improve BERTScore’s
performance, while BERTtext often has further improvements over BERTScore. One possible ex-
planation is the domain difference between CLIP and BERT, which causes their embeddings to lie
in distinct spaces. Since BERTScore is computed on top of BERT-based textual embeddings that are
pre-trained on another source of data, our CLIP-based ImaginE may not be supportive.
Score Distribution To further validate the effectiveness of our methods, we visualize the score
distributions of different metrics. As shown in Figure 9(b), BERTtext has the sharpest distribution,
4https://github.com/lucidrains/big-sleep
5https://github.com/nerdyrodent/VQGAN-CLIP
8
Under review as a conference paper at ICLR 2022
Intra-Group Pairwise Visual Similarity	Score
(a)	(b)
Figure 9: (a) The intra-group pairwise visual similarity distributions for images generated by dVAE,
BigGAN, and VQGAN. The plot shows the three quartile values and the extreme values. (b) The
score distributions histplot of IMAGINE, BERTtext and BERTScore used in our experiments. All
four metrics range between [-1, 1].
Task	Dataset	Pearson Correlation									
MT	WMT19 IWSLT14	BLEU-1	BLEU-2	BLEU-3	BLEU-4	BERTScore	BLEURT	BERTtext	IEtext	IEimage	IEtext&image
		16.41 21.47	15.76 20.82	15.06 19.17	13.15 17.60	17.14 23.95	17.79 22.93	4.37 18.42	20.34 14.11	3.80 ± 1.78 15.92 ± 0.95	10.11 ± 1.51 17.75 ± 0.71
TS		-BLEU	ROUGE-I	ROUGE-2	ROUGE-L	BERTScore	BLEURT	BERTtext	IEtext	IEimage	IEtext&image
	DUC2004	11.47	13.66	9.74	13.14	19.44	23.59	12.10	19.81	15.01 ± 1.03	18.03 ± 1.08
	GigaWord	9.39	14.58	7.75	14.31	19.59	20.23	17.49	15.56	3.74 ± 0.98	12.27 ± 0.69
		-BLEU	METEOR	ROUGE-L	CIDEr	BERTScore	BLEURT	BERTtext	IEtext	IEimage	IEtext&image
DT	WebNLG	25.79	30.78	24.15	23.09	34.53	35.97	22.38	26.81	19.69 ± 0.49	24.82 ± 0.37
	E2ENLG	12.78	25.55	12.22	13.83	22.76	22.75	13.11	18.19	10.89 ± 2.40	15.33 ± 1.02
	WikiBioNLG	8.19	8.31	9.88	5.35	8.98	9.21	6.07	4.14	3.32 ± 0.89	4.10 ± 0.50
Table 2: The Pearson correlations with human judgement for each individual metric. IE: ImaginE.
MT: machine translation. TS: abstractive text summarization. DT: data-to-text generation.
while our imagination-based methods lead to smoother distributions. This indicates IMAGINEimage
is more diverse than text-based metrics with the same measurement (i.e., cosine similarity). We also
observe that BERTScore, which computes maximum matching after calculating cosine similarity on
token embeddings, provides a more uniform distribution compared to the other three. Currently, the
value of IMAGINEtext usually lies between [0.6, 1], and IMAGINEimage usually lies between [0.3,
1]. It would be preferable if future work can help ImaginE to be more distinctive.
Future Work As noted in Section 5, IMAGINE can capture the keyword difference and render
distinct imaginations for two pieces of similar text. One supportive case is Figure 4(a). While
this ensures ImaginE’s ability to distinguish keyword differences, it also cast doubt on ImaginE’s
robustness. In Figure 8(a), merely changing the relative position of “grounds” result in two entirely
different images. In Figure 8(c) and (d), the name of the restaurants also reduces the quality of
the imagination. Future work may systematically examine the robustness of CLIP and DALL-E
regarding textual variance.
Furthermore, even though we have access to DALL-E’s pre-trained dVAE decoder, we still need to
generate the imagination from scratch for each example, which can be compute-intensive. We are
interested in exploring more efficient ways to speed up the image generation process.
Aside from the above points listed, we also find the following topics worth exploring. Currently, the
CLIP text encoder has a length constraint of77 BPE tokens, [BOS] and [EOS] included. This limits
our attempt on longer text generation tasks, such as story generation, document summarization, etc.
Also, CLIP and DALL-E only support English for now. With a multilingual CLIP and DALL-E, we
may cross verify the similarity with text and imagination in other source languages.
7 Conclusion
In this paper, we propose ImaginE, an imagination-based automatic evaluation metric for NLG.
Experiments on three tasks and seven datasets find out that adding ImaginE similarity scores as
an extension to current non-embedding-based metrics can improve their correlations with human
judgments in many circumstances. We hope our work can contribute to the construction of multi-
modal representations and the discussion of multi-modal studies.
9
Under review as a conference paper at ICLR 2022
Ethical S tatement
Our study is approved for IRB exempt. The estimated hourly wage paid to MTurk annotators is $12.
Speaking of potential ethical concerns, our “imagination” approach may face an issue of fairness if
there exists any bias in the training dataset for CLIP or DALL-E. In such circumstances, ImaginE
might display a tendency to render specific types of images that it has seen in the training data. Even
though we did not witness such issues in our study, we should keep in mind that this unfair behavior
would impair ImaginE’s effectiveness as an evaluation tool.
Reproducibility Statement
All of the datasets used in our study on machine translation, data-to-text generation and abstractive
text summarization tasks are publicly available. We use the public repositories to implement Imag-
inE. The implementations of CLIP-based image generators used in our study are dVAE+CLIP6,
Big-Sleep(BigGAN+CLIP)7 and VQGA+CLIP8.
References
Loic Barrault, Ondrej Bojar, M. Costa-jussa, C. Federmann, M. Fishel, Yvette Graham, B. Haddow,
M. Huck, PhiliPP Koehn, S. Malmasi, Christof Monz, Mathias Muller, Santanu Pal, Matt Post,
and Marcos Zampieri. Findings of the 2019 conference on machine translation (wmt19). In WMT,
2019.
P. Bell, P. Swietojanski, J. Driesen, M. Sinclair, F. McInnes, and S. Renals. 11th international
workshoP on sPoken language translation (iwslt 2014). 2014.
Andrew Brock, Jeff Donahue, and Karen Simonyan. Large scale gan training for high fidelity natural
image synthesis. ArXiv, abs/1809.11096, 2019.
T. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, J. KaPlan, Prafulla Dhariwal, Arvind
Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-
Voss, Gretchen Krueger, T. Henighan, R. Child, A. Ramesh, Daniel M. Ziegler, Jeffrey Wu,
Clemens Winter, ChristoPher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Ben-
jamin Chess, J. Clark, ChristoPher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and
Dario Amodei. Language models are few-shot learners. ArXiv, abs/2005.14165, 2020.
Wenhu Chen, Yu Su, X. Yan, and W. Wang. KgPt: Knowledge-grounded Pre-training for data-to-text
generation. In EMNLP, 2020.
Elizabeth Clark, A. Celikyilmaz, and Noah A. Smith. Sentence mover,s similarity: Automatic
evaluation for multi-sentence texts. In ACL, 2019a.
Elizabeth Clark, A. Celikyilmaz, and Noah A. Smith. Sentence mover’s similarity: Automatic
evaluation for multi-sentence texts. In ACL, 2019b.
J. Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deeP bidi-
rectional transformers for language understanding. In NAACL-HLT, 2019.
A. Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas
Unterthiner, M. Dehghani, Matthias Minderer, G. Heigold, S. Gelly, Jakob Uszkoreit, and
N. Houlsby. An image is worth 16x16 words: Transformers for image recognition at scale. ArXiv,
abs/2010.11929, 2020.
Ondrej Dusek, David M. Howcroft, and Verena Rieser. Semantic noise matters for neural natural
language generation. In INLG, 2019.
Ondrej Dusek, Jekaterina Novikova, and Verena Rieser. Evaluating the state-of-the-art of end-to-end
natural language generation: The e2e nlg challenge. ComPut Speech Lang., 59:123-156, 2020.
6https://github.com/openai/DALL-E
7https://github.com/lucidrains/big-sleep
8https://github.com/nerdyrodent/VQGAN-CLIP
10
Under review as a conference paper at ICLR 2022
Desmond Elliott and Frank Keller. Image description using visual dependency representations. In
Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pp.
1292-1302, 2013.
Patrick Esser, Robin Rombach, and Bjorn Ommer. Taming transformers for high-resolution image
synthesis. In CVPR, 2021.
David Freedman, Robert Pisani, and Roger Purves. Statistics (international student edition). Pisani,
R. Purves, 4th edn. WW Norton & Company, New York, 2007.
Linda B Gambrell and Ruby J Bales. Mental imagery and the comprehension-monitoring perfor-
mance of fourth-and fifth-grade poor readers. Reading Research Quarterly, pp. 454-464, 1986.
Claire Gardent, Anastasia Shimorina, Shashi Narayan, and Laura Perez-Beltrachini. Creating train-
ing corpora for nlg micro-planners. In ACL, 2017.
Jack Hessel, Ariel Holtzman, Maxwell Forbes, R. L. Bras, and Yejin Choi. Clipscore: A reference-
free evaluation metric for image captioning. ArXiv, abs/2104.08718, 2021.
Ming Jiang, Qiuyuan Huang, Lei Zhang, Xin Wang, Pengchuan Zhang, Zhe Gan, Jana Diesner, and
Jianfeng Gao. Tiger: Text-to-image grounding for image caption evaluation. In EMNLP, 2019.
Victoria L Joffe, Kate Cain, and Natasa Maric. Comprehension problems in children with specific
language impairment: does mental imagery training help? International Journal of Language &
Communication Disorders, 42(6):648-664, 2007.
M. Just, S. Newman, T. Keller, A. McEleney, and P. Carpenter. Imagery in sentence comprehension:
an fmri study. NeuroImage, 21:112-124, 2004.
M. Kendall. A new measure of rank correlation. Biometrika, 30:81-93, 1938.
Chi kiu Lo. Meant 2.0: Accurate semantic mt evaluation for any output language. In WMT, 2017.
Chi kiu Lo. Yisi - a unified semantic mt quality evaluation and estimation metric for languages with
different levels of available resources. In WMT, 2019.
Stephen M Kosslyn, Giorgio Ganis, and William L Thompson. Neural foundations of imagery.
Nature reviews neuroscience, 2(9):635-642, 2001.
Matt J. Kusner, Yu Sun, Nicholas I. Kolkin, and Kilian Q. Weinberger. From word embeddings to
document distances. In ICML, 2015a.
Matt J. Kusner, Yu Sun, Nicholas I. Kolkin, and Kilian Q. Weinberger. From word embeddings to
document distances. In ICML, 2015b.
Remi Lebret, David Grangier, and Michael Auli. Neural text generation from structured data with
application to the biography domain. In EMNLP, 2016.
H. Lee, Seunghyun Yoon, Franck Dernoncourt, Doo Soon Kim, Trung Bui, and K. Jung. Vil-
bertscore: Evaluating image caption using vision-and-language bert. In EVAL4NLP, 2020.
Kuang-Huei Lee, X. Chen, G. Hua, H. Hu, and Xiaodong He. Stacked cross attention for image-text
matching. ArXiv, abs/1803.08024, 2018.
Piji Li, Wai Lam, Lidong Bing, and Z. Wang. Deep recurrent generative decoder for abstractive text
summarization. ArXiv, abs/1708.00625, 2017.
Chin-Yew Lin. ROUGE: A package for automatic evaluation of summaries. In Text Summarization
Branches Out, pp. 74-81, Barcelona, Spain, July 2004. Association for Computational Linguis-
tics. URL https://www.aclweb.org/anthology/W04- 1013.
Jiasen Lu, Dhruv Batra, Devi Parikh, and Stefan Lee. Vilbert: Pretraining task-agnostic visiolin-
guistic representations for vision-and-language tasks. In NeurIPS, 2019.
Marilyn A Nippold and Jill K Duthie. Mental imagery and idiom comprehension. 2003.
11
Under review as a conference paper at ICLR 2022
Myle Ott, Sergey Edunov, Alexei Baevski, Angela Fan, Sam Gross, Nathan Ng, David Grangier,
and Michael Auli. fairseq: A fast, extensible toolkit for sequence modeling. In Proceedings of
NAACL-HLT 2019: Demonstrations, 2019.
J. Panja and S. Naskar. Iter: Improving translation edit rate through optimizable edit costs. In WMT,
2018.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. Bleu: a method for automatic
evaluation of machine translation. In Proceedings of the 40th annual meeting on association for
computational linguistics, pp. 311-318. Association for Computational Linguistics, 2002.
Joel Pearson and Stephen M Kosslyn. The heterogeneity of mental representation: Ending the
imagery debate. Proceedings of the National Academy of Sciences, 112(33):10089-10092, 2015.
Alec Radford, Jeffrey Wu, R. Child, David Luan, Dario Amodei, and Ilya Sutskever. Language
models are unsupervised multitask learners. 2019.
Alec Radford, J. W. Kim, Chris Hallacy, A. Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sas-
try, Amanda Askell, Pamela Mishkin, J. Clark, Gretchen Krueger, and Ilya Sutskever. Learning
transferable visual models from natural language supervision. ArXiv, abs/2103.00020, 2021.
A. Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Radford, Mark Chen, and
Ilya Sutskever. Zero-shot text-to-image generation. ArXiv, abs/2102.12092, 2021.
J. Rolfe. Discrete variational autoencoders. ArXiv, abs/1609.02200, 2017.
Y. Rubner, Carlo Tomasi, and L. Guibas. A metric for distributions with applications to image
databases. Sixth International Conference on Computer Vision (IEEE Cat. No.98CH36271), pp.
59-66, 1998.
Mark Sadoski and A. Paivio. A dual coding view of imagery and verbal processes in reading com-
prehension. 1994.
Thibault Sellam, Dipanjan Das, and Ankur P. Parikh. Bleurt: Learning robust metrics for text
generation. In ACL, 2020.
Rico Sennrich, B. Haddow, and Alexandra Birch. Neural machine translation of rare words with
subword units. ArXiv, abs/1508.07909, 2016.
Piyush Sharma, Nan Ding, Sebastian Goodman, and Radu Soricut. Conceptual captions: A cleaned,
hypernymed, image alt-text dataset for automatic image captioning. In ACL, 2018.
Matthew G. Snover, B. Dorr, R. Schwartz, L. Micciulla, and J. Makhoul. A study of translation edit
rate with targeted human annotation. In AMTA, 2006.
Kihyuk Sohn. Improved deep metric learning with multi-class n-pair loss objective. In NIPS, 2016.
C. Sun, Abhinav Shrivastava, S. Singh, and A. Gupta. Revisiting unreasonable effectiveness of
data in deep learning era. 2017 IEEE International Conference on Computer Vision (ICCV), pp.
843-852, 2017.
B.	Thomee, D. Shamma, G. Friedland, Benjamin Elizalde, Karl S. Ni, Douglas N. Poland, Damian
Borth, and L. Li. Yfcc100m: the new data in multimedia research. Commun. ACM, 59:64-73,
2016.
C.	Tillmann, S. Vogel, H. Ney, A. Zubiaga, and H. Sawaf. Accelerated dp based search for statistical
translation. In EUROSPEECH, 1997.
Jesus Tomds, J. Mas, and F. Casacuberta. A quantitative method for machine translation evaluation.
2003.
Emily T Troscianko. Reading imaginatively: the imagination in cognitive science and cognitive
literary studies. Journal of Literary Semantics, 42(2):181-198, 2013.
12
Under review as a conference paper at ICLR 2022
Aaron van den Oord, Y. Li, and Oriol Vinyals. Representation learning with contrastive predictive
coding. ArXiv, abs/1807.03748, 2018.
Ashish Vaswani, Noam M. Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez,
Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. ArXiv, abs/1706.03762, 2017.
Ramakrishna Vedantam, C Lawrence Zitnick, and Devi Parikh. Cider: Consensus-based image
description evaluation. In Proceedings of the IEEE conference on computer vision and pattern
recognition, pp. 4566-4575, 2015.
Weiyue Wang, J. Peter, Hendrik Rosendahl, and H. Ney. Character: Translation edit rate on character
level. In WMT, 2016.
Yu Yan, Weizhen Qi, Yeyun Gong, Dayiheng Liu, Nan Duan, J. Chen, R. Zhang, and
M. Zhou. Prophetnet: Predicting future n-gram for sequence-to-sequence pre-training. ArXiv,
abs/2001.04063, 2020.
Tianyi Zhang, V. Kishore, Felix Wu, Kilian Q. Weinberger, and Yoav Artzi. Bertscore: Evaluating
text generation with bert. ArXiv, abs/1904.09675, 2020.
A	Appendix
A. 1 Dataset Details
Table 3 lists out the statistical details of the datasets’ test sets used in our study.
Task	Dataset	#sample	#ref	#lenref	#lenhyp
Machine Translation	WMT’19	2,000	1.0	22.4	22.4
	IWSLT’14	6,750	1.0	20.3	19.1
Abstractive Text Summarization	DUC2004	500	4.0	14.0	10.0
	GigaWord	1,950	1.0	9.9	11.9
Data-to-Text Generation	WebNLG	1,600	2.6	28.3	26.9
	E2ENLG	630	7.4	28.0	11.6
	WikiBioNLG	2,000	1.0	34.8	19.0
Table 3: Dataset statistics. #sample is the number of samples in the test set; #ref is the number of
parallel references per visual instance; #len is the average reference length.
A.2 Random Initialization
We discussed the influence of random initialization for different image generative backbones in
Section 6. In Figure 10, we show several groups of images generated by dVAE, BigGAN and
VQGAN with random initialization.
A.3 Correlation Results
We list the numbers on Pearson correlation in Tables 6, 8 and 10 that match Figures 3, 5 and 7 in
the main paper. Tables 4, 5, 7 and 9 display results on Kendall correlation for the three NLG tasks
used in our study. The Kendall correlations with human judgement show similar trends as those on
Pearson correlation.
A.4 Case S tudy
We provide more case studies for the three NLG tasks used in our study in Figures 11 to 17. For
each dataset in each task, we list 4 groups of examples together with the imagination rendered by
ImaginE and the automatic evaluation scores.
13
Under review as a conference paper at ICLR 2022
Task I DataSet	Kendall Correlation
I MT I WMT19 I IWSLT14	BLEU-I	BLEU-2	BLEU-3	BLEU-4	BERTSCore	BLEURT	BERTtext IEtext	IEimage	IEtext&image 13.22	12.98	12.07	10.74	1223	1306	728^^15.90	2.83 ± 1.42	7.15 ± 1.12 14.19	14.26	13.68	12.79	16.68	14.64	13.84	12.90	10.87 ± 0.73	12.58 ± 0.61
I TS I DUC2004 I GigaWord	~BLEU_ROUGE-I_ROUGE-2~ROUGE-L_BERTSCore_BLEURT^^BERTtext_IEtext	IEimage^^IEtext&image 8.96	871	775	722	12.82	16.04	831	949	8.23 ± 1.02	8.94 ± 0.95 12.26	12.15	9.21	12.40	14.10	15.16	13.11	12.83	2.18 ± 0.85	9.04 ± 0.62
I DT I WebNLG E2ENLG I WikiBioNLG	BLEU METEOR ROUGE-L	CIDEr BERTScore BLEURT BERTtext IEtext	IEimage	IEtext&image 15.94	21.30	15.24	1340	23.44	24.31	154^^19.55^^12.84 ± 0.44^^16.73 ± 0.30 11.53	18.46	8.60	10.29	14.45	14.61	10.86	10.59	6.37 ± 1.49	8.86 ± 0.81 3.27	3.73	4.00	2.10	5.09	5.32	3.07	2.08	1.36 ± 0.66	1.68 ± 0.35
Table 4: The Kendall correlations with human judgement for each individual metric. IE: ImaginE.
MT: machine translation. TS: abstractive text summarization. DT: data-to-text generation.
Dataset	Kendall Correlation					
	Metrics	Original	+B ERTtext	+ImaginEtext	+ImaginEimage	+ImaginEtext&image
	BLEU-I	13.22	13.02	14.98	12.46 ± 0.67	14.12 ± 0.43
	BLEU-2	12.98	12.74	14.35	12.40 ± 0.64	13.77 ± 0.40
WMT19	BLEU-3	12.07	11.91	13.52	11.98 ± 0.61	12.97 ± 0.35
	BLEU-4	10.74	10.63	12.42	10.67 ± 0.60	11.63 ± 0.30
	BERTScore	12.23	11.98	13.96	11.21 ± 0.75	13.02 ± 0.51
	BLEURT	13.06	13.05	14.31	13.02 ± 0.46	13.82 ± 0.25
	BLEU-1	14.19	14.42	15.07	15.15 ± 0.38	15.67 ± 0.23
	BLEU-2	14.26	14.48	14.95	15.31 ± 0.35	15.52 ± 0.19
IWSLT14	BLEU-3	13.68	13.82	14.25	14.69 ± 0.31	14.81 ± 0.17
	BLEU-4	12.79	13.02	13.39	14.00 ± 0.28	13.99 ± 0.16
	BERTScore	16.68	16.70	17.38	17.10 ± 0.34	17.70 ± 0.17
	BLEURT	14.64	14.68	14.93	15.36 ± 0.17	15.28 ± 0.08
Table 5: The Kendall correlations with human judgement on the machine translation task.
Dataset	Pearson Correlation					
	Metrics	Original	+B ERTtext	+ImaginEtext	+ImaginEimage	+ImaginEtext&image
	BLEU-I	16.41	16.21	19.25	16.17 ± 0.99	18.46 ± 0.54
	BLEU-2	15.76	15.62	18.41	15.86 ± 0.89	17.68 ± 0.49
WMT19	BLEU-3	15.06	14.96	17.61	15.30 ± 0.81	16.87 ± 0.45
	BLEU-4	13.15	13.12	15.72	13.66 ± 0.78	14.98 ± 0.42
	BERTScore	17.14	16.86	19.70	15.95 ± 1.07	18.78 ± 0.59
	BLEURT	17.79	17.73	18.86	18.40 ± 0.53	18.77 ± 0.25
	BLEU-1	21.47	21.77	22.01	22.97 ± 0.50	23.33 ± 0.26
	BLEU-2	20.82	21.10	21.56	22.77 ± 0.45	22.82 ± 0.22
IWSLT14	BLEU-3	19.17	19.50	20.21	21.73 ± 0.42	21.52 ± 0.21
	BLEU-4	17.60	17.96	18.88	20.58 ± 0.41	20.22 ± 0.20
	BERTScore	23.95	24.02	24.24	25.10 ± 0.43	25.34 ± 0.21
	BLEURT	22.93	23.00	23.12	24.06 ± 0.20	23.74 ± 0.09
Table 6: The Pearson correlations with human judgement on the machine translation task.
14
Under review as a conference paper at ICLR 2022
						
Dataset	Kendan Correlation					
	Metrics	Original	+B ERTtext	+ImaginEtext	+ImaginEimage	+ImaginEtext&image
	BLEU	8.96	9.42	10.03	9.10 ± 0.71	9.59 ± 0.57
	ROUGE-1	8.71	8.86	9.96	9.49 ± 0.39	9.77 ± 0.30
ΓλT T<^'∏∩∩z1	ROUGE-2	7.75	9.49	9.89	9.10 ± 0.61	9.62 ± 0.55
DUC2004	ROUGE-L	7.22	8.09	9.91	9.40 ± 0.51	9.61 ± 0.37
	BERTScore	12.82	13.15	12.63	11.93 ± 0.43	12.35 ± 0.32
	BLEURT	16.04	16.11	16.00	15.52 ± 0.22	15.74 ± 0.20
	BLEU	12.26	12.50	12.37	7.49 ± 0.68	11.47 ± 0.33
	ROUGE-1	12.15	12.14	12.18	11.04 ± 0.39	12.13 ± 0.19
GigaWord	ROUGE-2	9.21	12.04	11.79	6.74 ± 0.63	10.10 ± 0.34
	ROUGE-L	12.40	12.59	12.55	11.26 ± 0.45	12.69 ± 0.21
	BERTScore	14.10	14.24	14.32	13.56 ± 0.32	14.39 ± 0.16
	BLEURT	15.16	15.24	14.96	14.91 ± 0.19	15.09 ± 0.09
Table 7: The Kendall correlations with human judgement on the abstractive text summarization task.
Dataset	Pearson Correlation					
	Metrics	Original	+B ERTtext	+ImaginEtext	+ImaginEimage	+ImaginEtext&image
	BLEU	11.47	12.47	18.31	16.25 ± 0.71	17.50 ± 0.62
	ROUGE-1	13.66	14.05	17.21	16.05 ± 0.46	16.67 ± 0.39
DUC2004	ROUGE-2	9.74	10.71	16.29	14.92 ± 0.61	15.72 ± 0.53
	ROUGE-L	13.14	13.65	17.66	16.25 ± 0.55	17.05 ± 0.46
	BERTScore	19.44	19.50	20.97	19.50 ± 0.43	20.30 ± 0.37
	BLEURT	23.59	23.53	24.28	23.47 ± 0.23	23.86 ± 0.19
	BLEU	9.39	10.95	13.21	9.44 ± 0.58	12.21 ± 0.29
	ROUGE-1	14.58	15.40	16.06	14.44 ± 0.45	15.85 ± 0.22
GigaWord	ROUGE-2	7.75	9.27	11.84	8.35 ± 0.51	10.71 ± 0.25
	ROUGE-L	14.31	15.13	15.93	13.81 ± 0.48	15.60 ± 0.24
	BERTScore	19.59	19.81	19.51	18.84 ± 0.39	19.71 ± 0.18
	BLEURT	20.23	20.41	20.28	20.19 ± 0.21	20.40 ± 0.11
Table 8: The Pearson correlations with human judgement on the abstractive text summarization task.
Dataset	Kendall Correlation					
	Metrics	Original	+BERTtext	+IMAGINEtext	+ImaginEimage	+ImaginEtext&image
	BLEU	15.94	16.76	19.25	18.65 ± 0.20	19.49 ± 0.13
	METEOR	21.30	22.03	23.08	20.43 ± 0.25	22.42 ± 0.17
ΛXλ∕ιRN,TT t '	ROUGE-L	15.24	16.16	18.74	17.92 ± 0.20	18.74 ± 0.13
WebNLG	CIDEr	13.40	13.59	14.43	14.56 ± 0.05	14.52 ± 0.03
	BERTScore	23.44	23.68	24.19	22.93 ± 0.19	23.90 ± 0.11
	BLEURT	24.31	24.38	24.92	24.54 ± 0.09	24.84 ± 0.05
	BLEU	11.53	13.32	11.99	8.80 ± 1.29	11.04 ± 0.65
	METEOR	18.46	18.86	14.37	11.08 ± 1.20	13.41 ± 0.67
	ROUGE-L	8.60	9.59	11.60	9.49 ± 1.03	10.82 ± 0.46
E2ENLG	CIDEr	10.29	12.45	11.56	9.41 ± 1.06	11.03 ± 0.56
	BERTScore	14.45	14.85	14.47	12.72 ± 0.95	13.90 ± 0.45
	BLEURT	14.61	14.80	15.26	14.86 ± 0.40	15.08 ± 0.19
	BLEU	3.27	3.61	2.61	2.01 ± 0.58	2.31 ± 0.33
	METEOR	3.73	4.30	3.03	2.43 ± 0.53	2.65 ± 0.30
	ROUGE-L	4.00	4.27	4.17	3.39 ± 0.46	3.89 ± 0.30
WikiBioNLG	CIDEr	2.10	2.60	2.22	1.39 ± 0.44	1.67 ± 0.28
	BERTScore	5.09	5.18	4.58	4.62 ± 0.35	4.73 ± 0.18
	BLEURT	5.32	5.43	4.87	4.85 ± 0.29	4.90 ± 0.13
Table 9: The Kendall correlations with human judgement on the data-to-text task.
15
Under review as a conference paper at ICLR 2022
dVAE
InputText: uganda faces rebel forces on west (congo) and north (Sudan)
BigGAN
VQGAN
(a)
InputText: e∪ resumes aid for victims of hurricane mitch
BigGAN
VQGAN
(b)
(c)
Figure 10: Groups of images generated by ImaginE with different image genrative backbones with
random initializations. The image generative backbones are dVAE, BigGAN and VQGAN.
16
Under review as a conference paper at ICLR 2022
Pearson Correlation
Dataset 1__________________________________________________________________
	Metrics	Original	+BERTtext	+IMAGINEtext	+ImaginEimage	+ImaginEtext&image
	BLEU	25.79	26.80	30.04	28.72 ± 0.22	29.86 ± 0.16
	METEOR	30.78	31.88	33.50	30.94 ± 0.27	33.08 ± 0.20
WebNLG	ROUGE-L	24.15	25.23	28.70	27.66 ± 0.21	28.59 ± 0.15
	CIDEr	23.09	23.25	23.98	24.07 ± 0.04	24.02 ± 0.02
	BERTScore	34.53	34.84	35.82	34.11 ± 0.19	35.38 ± 0.12
	BLEURT	35.97	36.00	36.80	36.26 ± 0.10	36.62 ± 0.06
	BLEU	12.78	14.66	19.11	13.93 ± 1.94	17.23 ± 0.76
	METEOR	25.55	25.93	23.37	17.85 ± 1.80	21.44 ± 0.69
E2ENLG	ROUGE-L	12.22	13.48	18.69	14.62 ± 1.66	17.22 ± 0.62
	CIDEr	13.83	14.27	16.46	15.45 ± 0.73	16.06 ± 0.26
	BERTScore	22.76	23.15	23.71	20.18 ± 1.27	22.40 ± 0.47
	BLEURT	22.75	22.96	23.68	22.55 ± 0.59	23.22 ± 0.21
	BLEU	8.19	9.25	5.67	4.88 ± 0.82	5.73 ± 0.45
	METEOR	8.31	9.35	6.33	5.58 ± 0.75	6.36 ± 0.40
WikiBioNLG	ROUGE-L	9.88	10.51	8.16	7.40 ± 0.69	8.23 ± 0.36
	CIDEr	5.35	5.78	5.92	5.85 ± 0.39	5.97 ± 0.19
	BERTScore	8.98	9.24	8.22	7.78 ± 0.47	8.19 ± 0.23
	BLEURT	9.21	9.39	8.84	8.58 ± 0.32	8.80 ± 0.15
Table 10: The Pearson correlations with human judgement on the data-to-text task.
Src: Sie soil SiCh dem Asteroiden RyUgU so Sehr nahern, dass Sie Material
Von seiner Oberflache einsaugen und zur Erde bringen kann.
Ref: It should get so close to the asteroid Ryugu that it can suck in
material from its surface and bring it back to Earth.
Hyp: It is designed to approach the Ryugu asteroid so close that it
can suck material from its surface and bring it to Earth.
ImaginationRef	ImaginationHyP
Metric
_____BLEU-L
..BLEU-2.
_____BLEU-3
_____BLEU-4
一 BERTSCQre
_BLEURT
..BERTtext.
ImaginEtext
ImaginEimage
Score
-83,26
-63,13
-50,29
,.41..77.
,.81..37.
-25,81
„98.05.
-95,46
82.96
Src: Es gab Momente in dieser noch jungen Saison, da ging Alois Fetsch
mit den Seinen hart ins Gericht.
Ref: There were moments in this young season, when Alois Fetsch
was really hard on his team.
Hyp: There were moments in this fledgling season when Alois
ImaginationHyP
Metric
-BLEU-L
......BLEU-2.
......BLEU-3.
......BLEU-4.
.BERTScore.
BLEURT
Fetsch took a hard line with his own.
Score
一58.82
一46.97一
38.89.
一30.28
一 69.54.
BERTtsxt.
ImaginEtext
ImaginEimage
.16.09.
98.35.
9.1.06.
44.95
(a)
(b)
Src: Da musste die kleine Elsa immer wieder zugreifen, so gut schmeckte
der Kuchen.
Ref: Little Elsa kept coming back for seconds, that is how good the
cake was.
Src: Den Preis fur den besten Drink erhalt dieser Mix, mit dem ein
Kapselhersteller sich prasentiert, nicht zwingend.
Ref: The mixture getting the prize for the best drink, presented by a
capsule manufacturer - not compulsory.
Hyp: Little Elsa had to reach for it again and again, the cake tasted
Hyp: This mix, with which a capsule manufacturer presents itself,
so good.	Metric Score
does not necessarily receive the prize for the best drink.
ImaginationRef
,BLEU-1_____33.33.
.旦LEU-24一一…一21.82.
,BLEU-3__________________________________________0.00.
,bleu-4_____0.00.
.旦ERTSCoJe一一一37.71
BLEURT -0.91
BERText.一一一98.58.
ImaginEtext.95.51
ImaginationHyp	ImaginEimage 70.56
ImaginationHyp
____Metric
____BLEU-L
____BLEU-2.
____BLEU-3
____BLEU-4.
上ERTSCQre.
BLEURT.
......BERTtext.
ImaginEtext
ImaginEimage
Score
„47.37
,-39.74
„33.37
..26.10.
..39.61.
--22.19
,-98.24
,-89.36
41.67
(c)
(d)
Figure 11:	More examples for the machine translation task on WMT'19. Src: the German text to be
translated. Ref: the reference translation. Hyp: the generated translation candidate.
17
Under review as a conference paper at ICLR 2022
Src: Ich weiβ nicht genau, ob ich noch Zeit habe ihnen andere UmgebUngen
ZU zeigen.
Ref: I'm not sure if I have time to show you any other environments.
Hyp： I don't know if I still have time to show you other environments.
Src: DaS ist sie sind sozusagen alle in der vorwindelphase.
Ref: +That is they're all in the pre-nappy stage, so to speak.
Hyp: This is, in a sense, all of you are in the pre-wind phase.
ImaginationRef	ImaginationHyP
Metric
BEU-I
.BLEU-2.
.BLEU-3.
.BLEU-4.
.BERTScore.
BLEURT.
BERTtext.,
ImaginEteXt
ImaginEimage i
(a)
Score
„73.33
60.55
48.22.
.37.03
81..49.
.30.42.
.99.35
,99.51..
88.92
Metric
BLEU-I
BLEU-2
BLEU-3.
BLEU-4
EERIScore
____BLEURI
__._BERTiext.
一…ImaginEteXt
ImaginEimage
Score
.37.50.
.15,81
—0,00一
—0,00一
36.06.
.-82.24.
,-97.21.
..86.62
29.49
(b)
Src: Alle denken, ich sei zwischen "Titanic" und "Avatar" davongelaufen und
hatte mir irgendwo die nagel auf einem handtuch am strand gefeilt.
Ref: People sort of think I went away between "Titanic" and "Avatar"
and was buffing my nails someplace, sitting at the beach.
Hyp: Everybody thinks I was running away from Titanic and Avatar, and
I would have celebrated the nails on a towel on a beach somewhere.
ImaginationHyp
.一___Metric
——BLEU-1
_____BLEU-2
_____BLEU-3
_____BLEU-4
.BERTScore.
——BLEURI.
_一一.BERTtext.
ImaginEteXt
ImaginEimage i
Score
一 44,41
……0.00.
……0.00.
0,cc.
……9.50.
二66.43
..98.17.
一 86.13
68.85
Src: Und das hier ist bio<unk>, der eben auch erwahnte hackerspace,
dieses ja, so eine art volkshochschule im prinzip fur fur molekulare biologie.
Ref: And yes, so you can see that it's a relatively relatively
heterogeneous thing, so from some people who do it on their own
from home, to big i mean, larger organisations, who are doing this
more formally in an institutionalised form already.
Hyp: And this is biobes, who just mentioned hackerspace, this one,
sort of a volkshock school, basically for molecular biology.
ImaginationRef
ImaginationHyp
___Metric
_..BLEU-1
__._.BLEU-2.
——BLEU-3
_._.BLEU-4..
.BERTScore.
_BLEURI
.BERTtext.
一…ImaginEteXt
ImaginEimage
一一Score
.一13,23
_____4,51
—一 0,.00.
—一 0..00.
-6.54.
一-140,81
____96.69
78.47.
51.61
(c)
(d)
Figure 12:	More examples for the machine translation task on IWSLT’14. Src: the German text to
be translated. Ref: the reference translation. Hyp: the generated translation candidate.
Src: Prime minister Mahathir Mohamad said Friday he is not too choosy
about who will be his successor, the man need not necessarily be very
religious and only preoccupied with doing virtuous deeds at all times, the
national news agency, Bernama, quoted Mahathir as saying after Friday
prayers at the Al-Falah mosque in the northern town of Jitra in Kedah state.
Src: The first part of the international space station was smoothly orbiting
earth on Friday after a faultless launch that marked the start of a new age
in space exploration and colonization.
Ref: Malaysian prime minister seeks new deputy after firing/
arresting last
Hyp: Mahathir says he's not too <unk> to replace his successor
Ref: Zarya module orbiting earth; shuttle endeavor will rendezvous
in 2 weeks.
Hyp: First part of international space station celebrates start of
Metric
BLEU
.ROUGE-1.
ROUGE-2一
—ROUGE-L
BERTScore.
BLEURT
.BERTt9Xt.
ImaginationHyP
(a)
ImaginEteXt
ImaginEimage
Score
0.00.
0.00.
0.00,.
一 0.00一
二5.03
.二136.78
.一一 95.35
.一一 83.68
68.65
Metric
BLEU
ROUGE-1
ROUGE-2
ROUGE-L
BERTScore
BLEURT
BERTteXt
,ImaginEteXt
ImaginE—e
(b)
Src: Taking a major step toward statehood, the Palestinians on Tuesday
inaugurated Gaza international airport, their first gateway to the world, with
cheers, tears and an outpouring of patriotism .
Ref: Palestinians celebrate opening of GaZa international airport
Hyp: Palestinians open GaZa international airport
ImaginationRef	ImaginationHyP
Metric
Score
(c)
BLEU.
ROUGE-I
ROUGE-2
ROUGE-L
BERTSCore
.BLEURL
BERTtext
ImaginEteXt
ImaginEimage
96.70.
95.26.
70.56
Score
._._._..0..0Q.
._._._..0..0Q.
._._._..0..00.
._._._..0..00.
一一一-2.0乙
二113.13
一一一 95.22
一一一 73.97
57.13
Src: Despite modest encouragement over a new proposal delivered by the
players to the owners, the national basketball association Tuesday canceled
the first two weeks of the regular season, the first time in the league's 51-year
history that it will lose games to a labor dispute .
Ref: Continuing labor dispute cancels first two weeks of NBA season
Hyp: NBA cancels first two weeks of regular season
Metric
-32,57一
-83.33.
,60.00.
-64.72
. 84.44
28.02.
ImaginationRef
ImaginationHyp
(d)
Score
.BLEU―.…54.02.
ROUGE二L 一……62.50.
ROUGE-2 一……28.57.
ROUGE-L ……65.36.
BERTScore-……62.27.
BLEUR工__…一-61.30.
BERLeXt-……97.21
ImaginEteXt ……92.14
ImaginEimage	63.92
Figure 13:	More examples for the abstractive text summarization task on DUC2004. Src: the text
to be summarized. Ref: the reference summary. Hyp: the generated summary candidate.
18
Under review as a conference paper at ICLR 2022
Src: Opec's president Ammar UNK arrived late Friday in Qatar on the
third stage of a tour of the oil states of the gulf.
Ref: Opec president arrives in Qatar on next stage of gulf tour
Hyp: Opec president arrives in Qatar on third stage of gulf tour
ImaginationRef
ImaginationHyP
Metric
BLEU
....ROUGE-1.
.—ROUGE-2.
....ROUGE-L.
BERTScore
.......BLEURT.
BERTtext.
...I maginEt9xt.
ImaginEimage
(a)
Src: Around ### clandestine immigrants Wednesday staged a peaceful
breakout from a detention center in Malta and demonstrated on a road
shouting '`we want freedom.
Ref: Mass breakout of immigrants from Malta center
Hyp: Malta's immigrants stage peaceful breakout from detention
center
ImaginationRef
ImaginationHyP
______Metric
______BLEUL
.一ROUGE-L
—ROUGE-2.
.一ROUGE-L
BERTScore.
.......BLEURT.
一―一BERTtext.
._.I_maginEisxt.
ImaginEimage
(c)
Src: Five French trekkers and mountaineers were among six foreigners
killed during the autumn climbing season in Nepal, the French embassy
and Nepalese officials said Friday.
Ref: Nepal climbing season claims Five french lives
Hyp: Five french climber Canadians among six killed in Nepal
Score.
一一 81,23.
.100.00.
,100.00.
一一 90.91
„93.32.
一一 70,14.
„99.82.
一一99.46.
82.28
ImaginationRef
ImaginationHyP
(b)
一....Metric.
BLEU.
....ROUGE-1.
—ROUGE-2
....ROUGE-L.
BERTScore.
.BLEURT
一一一一BERTeXL
...Im.aginEtext.
ImaginEimage
_Score.
.13.44.
„33.33
0.00.
„25.58.
28.73
--59.89
一 95.10
„91.89.
75.93
Src: Billionaire basketball team owner mark Cuban was a no show, but the
head of UNICEF made it and pop star prince rounded off the evening by
throwing a guitar over his head.
Ref: UNK awards crown their prince by Giles Hewitt
..Score.
…13.89
„80.00
…一 0.QQ
,-38.36.
„36.27.
--25.00.
..96.36.
…78.08
54.25
Hyp: Billionaire donates guitar to Cuban billionaire
Metric.
BLEU.
.....ROUGE-1.
.....ROUGE-2.
.....ROUGE-L.
..BERTScore.
........BLEURT
一一一 BERTeXL
一一 ImaginEteXL
ImaginEimage
..._.Score.
…一 一0.00一
…一 一0.00一
…一 一0.00一
…一 一0.00一
一一-12.14
二153.43
..„,92.12
——60.79
28.49
(d)
Figure 14:	More examples for the abstractive text summarization task on GigaWord. Src: the text
to be summarized. Ref: the reference summary. Hyp: the generated summary candidate.
Ref: Beef kway teow originates from Singapore and is also made
in Indonesia. One of the ingredients in the dish is oyster sauce.
Hyp: Oyster sauce is a dish from Singapore, where Oyster sauce is
a dish from Indonesia.
Ref: The capital of Turkey is Ankara and the leader is Ahmet
Davutoglu. Turkey is the location of the bronze Ataturk monument.
Hyp: Ataturk Monument, located in Ankara, Turkey, is the capital of
ImaginationRef	ImaginationHyP
—Metric.
........bleu.
.METEOR.
EQUGE-L
CJDEr.
BEBTScore.
——BLEURT
......BEBTtext.
....ImaginEiext.
ImaginEimage
(a)
Score..
一一3.91
一 19.14
.15.21
一一 5.38
一 39.04
:34.61
一 93.13
一 73.73
68.85
Turkey.
Metric
—BLEU.
.METEOR.
ROUGE-L
CJDEL
BERTScore.
一一—BLEURT
ImaginationRef	ImaginationHyP
_____BERTtext.
__ImaginEtext.
ImaginEimage
Ref: Athens International Airport serves the city of Athens, in
Greek speaking Greece. Two of the leaders of which are Alexis
Tsipras and Prokopis Pavlopoulos.
Hyp: Alexis Tsipras (Greek: AVTaViς Φεμπελος) is a commander of
the city of Athens, Greece, where Alexis Tsipras is the leader.
-Metric
BLEU
.Score..
„23.76
ImaginationHyP
(c)
(b)
Score.
—5.82.
一 21.55
.17.15.
.一 3.37
34.12.
一-6.13
98.35.
86.13.
64.45
Ref: Batagor, which is found in Indonesia and Siomay are both
variations of the same dish. Shumai is a variation of the dish
Siomay while Shiitake is an ingredient of Shumai.
Hyp: The main ingredient of binagor is ''shumai'', a dish from
Indonesia where the main ingredient is '`shitake''.
—METEOR.
.BOUGE-L
CJDEr.
BEBTScorej
—20.60
—24.81
116.75
--5.23.
BLEURT 一一一-52.37
——.BERTtext.
__ImaginEieXt.
ImaginEimage
.96.12一
80.08.
48.61
ImaginationRef
Metric.
........bleu.
METEOR.
....ROUGE-L.
__.._._..CJDEr_
一BERTScore
BLEURT
BERTtsxt.
ImaginEtext
ImaginationHyP	imaginEmagS
(d)
Score
....8.06.
-13,43.
...18.59.
…一 8,62.
-18,52.
--46.34.
..98.96.
..89.50.
79.15
Figure 15:	More examples for the data-to-text task on WebNLG. Ref: the reference text. Hyp: the
generated text candidate.
19
Under review as a conference paper at ICLR 2022
Ref: Wildwood is a PUb located in riverside area near Raja Indian
Cuisine. It serves Italian food and It is not family-friendly.
Hyp： Wildwood is a variation of Raja Cuisine.
ImaginationRef	ImaginationHyP
___Metric
____BLEU
..METEOB.
ROUGE二L
CIDEr
BEBTScore.
——BLEUBT
BERTtext
ImaginEtext
Score
一一3,94
._1230
一25.23
4.10
一 34.53
--84.00.
95.47
81.01
ImaginEimage	46.17
Ref: A restaurant that is kid friendly near Raja Indian Cuisine
named The Wrestlers in the riverside area has a price range of
more than £30 that serves Italian food.
Hyp: Raja Cuisine is a variation of The Wrestlers.
Metric
BLEU.
.一一METEOB.
—BOUGE-L
______ClDEr.
BERTScore.
——BLEURT
BEFTtext.
...ImagjnEtsxt.
ImaginationRef	ImaginationHyP	ImaginEimage
.一ScoreL
.1.36.
一一一 9.78
.一一19.61
一一一.0,02.
一一一12.43
-118.30.
..96.65.
..72.56.
50.73
(a)
(b)
Ref: Located near Rainbow Vegetarian Cafe on the river, The
Vaults is a low cost, family friendly pub.
Hyp: The main ingredients of a riverside riverside riverside riverside
Ref: The Punter near Rainbow Vegetarian Cafe in the riverside as
a restaurant with a high price range is not children friendly. They
provide Italian food with a customer rating 1 out of 5.
riverside riverside riverside riverside rivers.
ImaginationRef	ImaginationHyP
.Metric.
Score.
Hyp: The Punter is a variation of the Rainbow Vegetable.
(c)
___BLEU.
—METEOR.
BOUGE-L
CJDEr.
£工.口工6。6包
___2.38.
—2.60一
.10.73.
_一0.07一
--17,89.
BLEUBT 一二140.35
一一.一BERTtext.
.._.I_maginEiext.
ImaginEimage
94.61.
68.02.
36.30
ImaginationRef	ImaginationHyP
_______Metric
BLEU.
.......meteob.
一一RQUGE-L
CIDEL
.BEBTScore.
____BLEUBT
_____BERTtext.
_._.!_maginEiext.
ImaginEimage
(d)
一.Score.
—1.69.
一—一7,71
21.05.
—0,01
—13.96.
--135.02.
一一.95.08.
一一 75.05
48.29
Figure 16:	More examples for the data-to-text task on E2ENLG. Ref: the reference text. Hyp: the
generated text candidate.
Ref: Eden AntS was a Canadian indie rock band from Toronto,
Ref: Rose mortem is the fashion label and nom de guerre of
founded in 2000 by the Montreal-born ender brothers.
American fashion designer Rose Hemlock,
Hyp: Ants Eden is a synthpop guitar player,
METEOR___8,19..
ROUGE-L___13,63
.旦ERTScole____10,40一
BLEUBT.一二119,97
一…一BERTtext_____98,47..
Jm_aginEisxt⅛___87一,79
ImaginatiOnRef
Metric.
Score.
Hyp: Mortem is a fashion design type used in the comics,
ImaginatiOnHyP	ImaginEimage	57,91
BLEU 1,67
CIDEr 7,31
ImaginatiOnRef	ImaginatiOnHyP
Metric.
BLEU.
meteor
BOUGE-L
CIDEL
BEBTScore,
.....BLEUBT.
.一一BERTtext.
ImaginEtSXt
Score
,7,58.
,12,60.
21,23.
50,60.
,11,32.
--96,62.
96,89.
„82,42.
ImaginEimage 52.10
(a)
(b)
Ref: David P Fridovich is a retired lieutenant general and green
beret in the United States army,
Hyp: David P Fridovich is the general manager of the United
States united Force,
Ref: Byzantine is a heavy metal band from Charleston, West Virginia
that formed in 2000,
Hyp: The band Cerzantine Trombony skip is the independent name
ImaginatiOnRef
ImaginationHyp
____Metric
_____BLEU
......METEOR.
—ROUGE-L
_____CIDEr.
.BERTScθrθ.
――BLEUBT
___BERTtext.
ImaginEtext
ImaginEimage
..Score..
一一27,91
一一27,96一
.一45,57一
一2Q5,8L
..,47,20.
一-39,11.
.一98,47一
.„87,79.
57,91
of the band,
ImaginatiOnRef	ImaginatiOnHyP
........BLEU.
METEOB.
EOUGE-L
CIDEr.
.BEBTScorej
…一 BLEURl
BERTtext.
一」maginEteXt.
ImaginEimage
Metric
Score
299
一 6,07一
一 6,96
214
--0,37.
二122,91
一一一96,47一
—— 77,24
42,70
(c)
(d)
Figure 17:	More examples for the data-to-text task on WikiBioNLG. Ref: the reference text. Hyp:
the generated text candidate.
20