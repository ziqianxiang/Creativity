Under review as a conference paper at ICLR 2022
On Label Shift in Domain Adaptation
via Wasserstein Distance
Anonymous authors
Paper under double-blind review
Ab stract
We study the label shift problem between the source and target domains in gen-
eral domain adaptation (DA) settings. We consider transformations transporting
the target to source domains, which enable us to align the source and target ex-
amples. Through those transformations, we define the label shift between two
domains via optimal transport and develop theory to investigate the properties of
DA under various DA settings (e.g., closed-set, partial-set, open-set, and universal
settings). Inspired from the developed theory, we propose Label and Data Shift
Reduction via Optimal Transport (LDROT) which can mitigate the data and label
shifts simultaneously. Finally, we conduct comprehensive experiments to verify
our theoretical findings and compare LDROT with state-of-the-art baselines.
1	Introduction
The remarkable success of deep learning can be largely attributed to computational power advance-
ment and large-scale annotated datasets. However, in many real-world applications such as medicine
and autonomous driving, labeling a sufficient amount of high-quality data to train accurate deep
models is often prohibitively labor-expensive, error-prone, and time-consuming. Domain adaptation
(DA) or transfer learning has emerged as a vital solution for this issue by transferring knowledge
from a label-rich domain (a.k.a. source domain) to a label-scarce domain (a.k.a. target domain).
Along with practical DA methods (Ganin & Lempitsky, 2015; Tzeng et al., 2015; Long et al., 2015;
Shu et al., 2018; French et al., 2018) which have achieved impressive performance on real-world
datasets, the theoretical results (Mansour et al., 2009; Ben-David et al., 2010; Redko et al., 2017;
Zhang et al., 2019a; Cortes et al., 2019) are abundant to provide rigorous and insightful understand-
ing of various aspects of transfer learning.
For domain adaptation, the source domain consists of the data distribution PS with the density pS,
and the unknown ground-truth labeling function f S assigning label y to source data x, whilst these
are PT, pT, and fT for the target domain, respectively. Moreover, while the data shift can be char-
acterized as a divergence between PS and PT (Mansour et al., 2009; Ben-David et al., 2010; Redko
et al., 2017; Zhang et al., 2019a; Cortes et al., 2019), the label shift in these works is commonly
characterized as EPT fS (x) - fT (x) or EPS fS (x) - fT (x) in which the binary classification
with deterministic labeling functions fs (∙), fτ (∙) ∈ {0,1} was examined. Additionally, although
this label shift term has occurred in the theoretical analysis of Mansour et al. (2009); Ben-David
et al. (2010); Redko et al. (2017); Zhang et al. (2019a); Cortes et al. (2019), it is restricted in con-
sidering the shift between fS (x) and fT (x) at the same data x, which ignores the data shift between
PS and PT. This limitation is illustrated in Figure 1. In particular, for a white/square point x drawn
from the target domain as in EPT fS (x) -fT (x) , the source labeling function fS cannot give
reasonable prediction probabilities for x, hence leading to inaccurate fS (x) - fT (x).
Label shift has also been examined in an anti-causal setting (Lipton et al., 2018; Garg et al., 2020a),
wherein an intervention on p(y) induces the shift, but the process generating x given y is fixed, i.e.,
pS (x | y) = pT (x | y). Although this setting is useful in some specific scenarios (e.g., a diagnostic
problem in which diseases cause symptoms), it is not sufficiently powerful to cope with a general
DA setting. Particularly, in an anti-causal setting, the source and target data distributions (i.e., pS (x)
and pT (x)) are just simply two different mixtures of the class conditional distributions pS (x | y) =
pT (x | y), hence sharing the same support set. This is certainly far from a general DA setting in
1
Under review as a conference paper at ICLR 2022
which both data shift: p2 * * S (x) 6= pT (x) with arbitrarily separated support sets and non-covariate
shift: pS (y | x) 6= pT (y | x) appear.
Contribution. In this paper, we study the label shift for a general domain adaptation setting in
which we have both data shift: pS (x) 6= pT (x) with arbitrarily separated support sets and non-
covariate shift: pS (y | x) 6= pT (y | x). More specifically, our developed label shift is applicable
to a general DA setting with a data shift between two domains and two totally different labeling
functions (i.e., we cannot use fS to predict accurately target examples and vice versa). To define
the label shift between two given domains, we utilize transformation L to transport the target to the
source data distributions (i.e., L#PT = PS). This transformation allows us to align the data of two
domains. Subsequently, the label shift between two domains is defined as the infimum of the label
shift induced by such a transformation with respect to all feasible transformations. This viewpoint
of label shift has a connection to optimal transport (Santambrogio, 2015; Villani, 2008; Peyre &
Cuturi, 2019), which enables us to develop theory to quantify the label shift for various DA settings,
e.g., anti-causal, closed-set, partial-set, open-set, and universal settings. Overall, our contributions
can be summarized as follows:
1.	We characterize the label shift for a general DA setting via optimal transport. From that, we
develop a theory to estimate the label shift for various DA settings and study the trade-off of learning
domain-invariant representations and WS label shift.
2.	Inspired from the theoretical development, we propose Label and Data Shift Reductions via
Optimal Transport (LDROT) which aims to mitigate both data and label shifts. We conduct com-
prehensive experiments to verify our theoretical findings and compare the proposed LDROT with
the baselines to demonstrate the favorable performance of our method.
Related works. Several attempts have been proposed to characterize the gap between general losses
of source and target domains in DA, notably (Mansour et al., 2009; Ben-David et al., 2010; Redko
et al., 2017; Zhang et al., 2019a; Cortes et al., 2019). Ben-David & Urner (2014; 2012); Zhang
et al. (2019a) study the impossibility theorems for DA, attempting to characterize the conditions
under which it is nearly impossible to perform transferability between domains. PAC-Bayesian
view on DA using weighted majority vote learning has been rigorously studied in Germain et al.
(2013; 2016). Meanwhile, Zhao et al. (2019); Johansson et al. (2019) interestingly indicate the
insufficiency of learning domain-invariant representation for successful adaptation. Specifically,
Zhao et al. (2019) points out the degradation in target predictive performance if forcing domain
invariant representations to be learned while two marginal label distributions of the source and target
domains are overly divergent. Johansson et al. (2019) analyzes the information loss of non-invertible
transformations and proposes a generalization upper bound that directly takes it into account. Le
et al. (2021) employed a transformation to align two domains and developed theories based on this
assumption. Moreover, label shift has been examined for the anti-causal setting Lipton et al. (2018);
Garg et al. (2020a), which seems not sufficiently realistic for a general DA setting. Optimal transport
theory has been theoretically leveraged with domain adaptation (Courty et al., 2017). We compare
our proposed LDROT to DeepJDOT (Damodaran et al., 2018) (a deep DA approach based on the
theory of (Courty et al., 2017)), and other OT-based DDA approaches, including SWD (Lee et al.,
2019), DASPOT (Xie et al., 2019), ETD (Li et al., 2020), RWOT (Xu et al., 2020). Finally, in
(Tachet des Combes et al., 2020) , a generator g is said to produce generalized label shift (GLS)
representations if it transports source class conditional distributions to corresponding target ones.
Further theories were developed to indicate that GLR representations are satisfied if we enforce
clustering structure assumption assisting us in training a perfect classifier. Evidently, our work
which focuses on how to quantify the label shift between two different domains taking into account
the inherent data shift via optimal transport theory is totally different form that work in terms of
motivation and developed theory.
2	Label S hift with Wasserstein Distance
2.1 Preliminaries
Notation. For a positive integer n and a real number p ∈ [1, ∞), [n] indicates the set {1, 2, . . . ,n}
while kxkp denotes the lp-norm of a vector x ∈ Rn. Let Y S and Y T be the label sets of the source
and target domains that have MS := Y S and MT := Y T elements, respectively. Meanwhile,
2
Under review as a conference paper at ICLR 2022
Y = Y S ∪ Y T stands for the label set of both domains which has the cardinality of M := |Y |.
Subsequently, we denote Y∆, Y∆S, and Y∆T as the simplices corresponding to Y , Y S, and Y T re-
spectively. Finally, let fs (∙) ∈ Yδ and fτ (∙) ∈ Yδ be the labeling functions of the source and target
domains, respectively, by filling zeros for the missing labels.
We now examine a general supervised learning setting. Consider a hypothesis h (∙) ∈ Y4 in a hy-
pothesis class H and a labeling function f (∙) ∈ Y4 where Y4 := {π ∈ RM : k∏∣∣1 = 1 and π ≥ 0}.
Let dY be a metric over Y4, we further define a general loss of the hypothesis h with respect to the
labeling function f and the data distribution P as: L (h,f,P) := R dY (h (x) ,f(x)) dP (x).
Next, we consider a domain adaptation setting in which we have source space X S endowed with a
distribution PS and the density function pS (x), and a target space X T endowed with a distribution
PT and the density function pT (x). We examine various DA settings based on the labels of source
and target domains including (1) closed-set DA: YS = YT, (2) open-set DA: YS ⊂ YT, (3) partial-
set DA: YT ⊂ YS , and (4) universal DA: YS ( YT and YT ( YS .
Figure 1: An illustration of our label shift definition. We employ a transformation L : L#PT = PS
to align two domains. The white/square points L xT on the source domain correspond to the
white/square points xT on the target domain. We measure dY dY fT xT , fS L xT and define
the label shift w.r.t. L as LS(S, T;L) := EPT [dγ (fτ (xτ), fs (L (xτ)))]. Finally, we take infimum
over all valid L to define the label shift between two domains.
2.2	Background on label shift
Together with data shift, the study of label shift is important for a general DA problem. However,
due to the occurrence of data shift, it is challenging to formulate label shift in a general DA setting.
Recent works (Lipton et al., 2018; Garg et al., 2020a) have studied label shift for the anti-causal
setting in which an intervention on p(y) induces the shift, but the process generating x given y is
fixed, i.e., pS (x | y) = pT (x | y). In spite of being useful in some specific cases, the anti-causal
setting is restricted and cannot represent data shift broadly because the source data distribution
pS (x) and the target data distribution pT (x) are simply just two different mixtures of identical class
conditional distributions. Furthermore, the label shift framework from these works is non-trivial to
generalize to all settings of DA.
In this paper, we address the issues of the previous works by defining a novel label shift framework
via Wasserstein (WS) distance for a general DA setting that takes into account the data shift between
two domains. We then develop a theory for our proposed label shift based on useful properties of
WS distance, such as its horizontal view, numeric stability, and continuity (Arjovsky et al., 2017).
We refer readers to Santambrogio (2015); Vinani (2008); Peyre & CUtUri (2019) for a comprehen-
sive knowledge body of optimal transport theory and WS distance, and Appendix A for necessary
backgrounds of WS for this work.
2.3	Label Shift via Wasserstein Distance
To facilitate our ensuing discussion, we assume that the source (S) and target (T) distributions PS and
PT are atomless distributions on Polish spaces. Therefore, there exists a transformation L : X T →
XS such that L#PT = PS (Villani, 2008). Given that mapping L, a data example xτ 〜PT with the
ground-truth prediction probability fT ^tγ) corresponds to another data example xS = L ^tγ)〜PS
3
Under review as a conference paper at ICLR 2022
with the ground-truth prediction probability f S xS . Hence, it induces a label mismatch loss
dγ (fτ (XT), fs (XS)) = dγ (frN) , f 亿(xT))),
where dY is a given metric over Y∆ . Based on that concept, the label shift between the source and
target domains induced by the transformation L can be defined as
LS (S, T; L) := EP T [ dγ (fT (xT), fs (L (xτ)))] .	(1)
By finding the optimal mapping L, the label shift between two domains is defined as follows.
Definition 1. Let dY be a metric over the simplex Y∆. The label shift between the source and the
target domains is defined as the infimum of the label shift induced by all valid transformations L:
LS (S, T):= L:L#iPnTf=PS LS (S, T; L) = L:L#iPnTf=PS EPT hdY fT XT,fSLXTi.	(2)
We give an illustration for Definition 1 in Figure 1. The label shift in Eq. (2) suggests finding the
optimal transformation L* to optimally align the source and target domains with a minimal label
mismatch.
Properties of label shift via Wasserstein distance: To show the connection between the afore-
mentioned label shift and optimal transport, we introduce two ways of calculating the label shift via
Wasserstein distance.
Proposition 2. (i) Denote by PSS the joint distribution of (x, fs (X)), where x 〜 PS, and PTfT the
joint distribution of (x, fT (x)), where x 〜Pt. Then, we have: LS (S, T) = WY ^PSS, PTT) ∙
(ii) Let PfS and PfT be the push-forward measures of PS and PT via fS and fT respectively, i.e.,
PfS = fS#PS and PfT = fT#PT. Then, we have: LS (S, T) =WdY PfS,PfT .
The results of Proposition 2 indicate that we can compute the label shift via the Wasserstein distance
on the simplex. For example, when dY(y,y0) = ky - y0kpp, the label shift can be computed via the
familiar Wp distance between PfS and PfT, i.e., LS(S, T) = Wpp(PfS,PfT). Note that Wd PSfS,PTfT
with d = λdX +dY was studied in Courty et al. (2017) for proposing a DA method that can mitigate
both label and data shifts. However, the concept label shift was not characterized and defined explic-
itly in that work. Moreover, our motivation and theory development in this work are different and
independent from (Courty et al., 2017). To give a better understanding of our label shift definition,
we now present some bounds for it in general and specific cases.
Proposition 3. Denote by pYS = (pYS (y))yM=1 and pYT = ( pYT (y))yM=1 the marginal distributions of the
source and target domain labels, i.e., pYS (y) = S pS(x, y)dx and pYT (y) = T pT (x, y)d x. For
dY(y,y0) = ky -y0kpp when p ≥ 1, the following holds:
(i)	L (hT, fT, Pt) ≤ LS(S, T) + L (hS, fS, PS) + WY (PSS, PTT) + const where the constant can be
viewed as a reconstruction term: supL,K: L# P t=PS, K#P S=P T EPT [ dY ( fT (K(L (X))), fT (x))];
(ii)LS(S,T)≥ kpYS-pYTkpp;
(iii)	In the setting that PS and PT are mixtures of well-separated Gaussian distributions, i.e., pa(x) =
∑M=IPa(y)N(x∣μya,∑a) with kμy — μya0∣∣2 ≥ D X max{k∑y∣∣o/2, k∑yo∣∣0/2}∀a ∈ {S,T},y = y, in
which k ∙ ∣ op denotes the operator norm and D is sufficiently large, we have
LS(S, T) — WPP(PYS, PYT) ≤ε(D),	(3)
where ε(D) is a small constant depending on D, pYS ,pYT, (ΣyS,ΣyT)yM=1, and it goes to 0 as D → ∞.
A few comments on Proposition 3 are in order. The inequality in (i) bounds the target loss by the
source loss and the label shift. Though this inequality has the same form as those in (Mansour et al.,
2009; Ben-David et al., 2010; Redko et al., 2017; Zhang et al., 2019a; Cortes et al., 2019), the label
shift in our inequality is more reasonably expressed. The inequality in (ii) reads that the marginal
label shift ∣∣pY — PY ∣P is a lower bound of our label shift. Therefore, the label shift induced by the
best transformation L* can not be less than this quantity. A direct consequence is that LS(S, T) = 0
4
Under review as a conference paper at ICLR 2022
implies pS (y) = pT (y), for all y ∈ [M] (no label shift). Finally, the inequality in (iii) shows that
when the classes are well-separated, the label shift will almost achieve the lower bound in the first
inequality, which implies its tightness. The key step in the proof is proving that PfS and PfT will
be concentrated around the vertices and it is also provable for sub-Gaussian distributions with some
extra work. The bound gives a simple way to estimate the label shift in this scenario: instead of
measuring the Wasserstein distance W (PfS, PfT ) on the simplex Y∆, we only need to measure the
Wasserstein distance between the vertices equipped with the masses pYS and pYT. The first experiment
in Section 4.1 also supports this finding.
Our label shift formulation can also serve as a tool to elaborate other aspects of DA, as we will see
below.
Minimizing data shift while ignoring label shift can hurt the prediction on test set: Consider
two classifiers on the source and target domains hs = h ◦ gs and hτ = h ◦ gτ where gs : XS → Z,
gT : X T → Z are the source and target feature extractors, and h : Z → Y∆ with h ∈ H . We define
a new metric dZ with respect to the family H as follows:
dZ(z1,z2) = sup dY (h(z1) ,h(z2)) ,
h∈H
where z1 and z2 lie on the latent space Z. The necessary (also sufficient) condition under which dZ
is a proper metric on the latent space (see the proof in Appendix B) is realistic and not hard to be
satisfied (e.g., the family H contains any bijection). We now can define a Wasserstein distance WdZ
that will be used in the development of Theorem 4.
Theorem 4.	With regard to the latent space Z, we can upper-bound the label shift as
LS (S, T) ≤ L (hS, fS, P S) + L (hτ ,f, P t ) + WZ (gS #PS, g #P T).
Theorem 4 indicates a trade-off of learning domain-invariant representation by forcing gS#PS =
gτ#PT (e.g., min WZ (gS#PS,gτ#PT)). It is evident that if the label shift between domains is Sig-
nificant, because L hS , fS, PS can be trained to be sufficiently small, learning domain-invariant
representation by minimizing WdZ gS#PS, gT #PT leads to a hurt in the performance of the target
classifier hT on the target domain. Similar theoretical result was discovered in Zhao et al. (2019) for
the binary classification (see Theorem 4.9 in that paper). However, our theory is developed based on
our label shift formulation in a more general multi-class classification setting and uses WS distance
rather than Jensen-Shannon (JS) distance (Endres & Schindelin, 2006) as in Zhao et al. (2019) for
which the advantages ofWS distance over JS distance have been thoughtfully discussed in Arjovsky
et al. (2017).
Label shift under different settings of DA: An advantage of our method is that it can measure the
label shift under various DA settings (i.e., open-set, partial-set, and universal DA). Doing this task
is not straight-forward using other label shift methods. For example, if there is a label that appears
in a domain but not in the other, it is not meaningful to measure the ratio between the marginal
distribution of this label as in Garg et al. (2020b).
In what follows, we elaborate our label shift in those settings. Particularly, we provide some lower
bounds for it, implying that the label shift is higher if there is label mismatch between two domains.
Recall that when the source and target domains do not have the same number of labels, we can
extend fS and fT to be functions taking values on Y∆ by filling 0 for the missing labels. For the
sake of presenting the results, let YS ∩ YT = {1, . . . ,C} be the common labels of two domains, QS
the marginal of PfS and QT the marginal of PfT on the first (C - 1) dimensions. QS\T denotes the
marginal of PfS in the space of variables having labels YS \ YT and QT\S denotes the marginal of
PfT in the space of variables having labels YS \ YT .
Theorem 5.	Assume that dY(y,y0) = ky - y0kpp. Then, the following holds:
i)	For the partial-set setting (i.e., Y T ⊂ Y S), we obtain
LS(S, T) ≥ wp(QS, QT) + EX~QS\ThkXkPi .	(4)
ii)	For the open-set setting (i.e., Y S ⊂ Y T), we obtain
LS(S, T) ≥ wp(QS, QT) + EX〜QT\ShkXkPi .	(5)
5
Under review as a conference paper at ICLR 2022
iii)	For the universal setting (i.e., Y T ( Y S and Y S ( Y T), we have
LS(S,T) ≥ wp(QS,QT)+ EX~Qt\ShkXkPi + Ey〜QS\ThkykP] .	(6)
Theorem 5 reveals that the label shifts for the partial-set, open-set, or universal DA settings are
higher than the vanilla closed-set setting due to the missing and unmatching labels of two domains
(see Section 4.1). Our theory also implicitly indicates that by setting appropriate weights for source
and target examples (i.e., low weights for the examples with missing and unmatching labels), we can
deduct label shift by reducing the second term of lower-bounds in Eqs. (4), (5), and (6) to mitigate
the negative transfer (Cao et al., 2019). We leave this interesting investigation to our future work.
Moreover, further analysis can be found in Appendix C.
3	Label and Data Shift Reductions via Optimal Transport
Inspired by the developed theory in Section 2.3, in this section we propose a novel DA approach,
named Label and Data Shift Reductions via Optimal Transport (LDROT), that aims to reduce both
data and label shifts simultaneously.
3.1	Objective function of LDROT
We consider the source classifier hS = hs ◦ g and the target classifier hτ = hT ◦ g. The pathway of
our method consists of three losses, which are as follows:
(i)	Standard loss LS: We train the source classifier hS on the labeled source data by minimizing the
loss Ls := L (hs,f, PS);
(ii)	Shifting loss Lshift : Furthermore, to mitigate both label and data shifts, we propose to further
regularize the loss LS by Lshift := Wd PhSS, PhTT , where the ground metric d is defined as
d (Z,Z) = λ ∙ dχ (g (xS),g (xt)) + dy (hs (XS),hT (xT)),	(7)
where Z = (XS,hs (XS)) with XS 〜PS andZT = (xt,hT (xT)) with xT 〜Pt;
(iii)	Clustering loss Lclus: Finally, to boost the generalization ability ofhT, we enforce the clustering
assumption (Chapelle & Zien, 2005) to enable hT giving the same prediction for source and target
examples on the same cluster. To employ the clustering assumption (Shu et al., 2018), we use Virtual
Adversarial Training (VAT) (Miyato et al., 2019) in conjunction with minimizing the entropy of
prediction (Grandvalet & Bengio, 2004): Lclus := Lent + Lvat with
Lent = EPT [H W (g (x)))] , Lvat =	E0.5Ps+0.5PT ImaX X∈Bθ(X)DKL (hT (g (χ)), hT (g (χ')))],
where DKL represents a Kullback-Leibler divergence, θ is a very small positive number, Bθ (X) :=
{X0 : kX0 - Xk2 < θ }, and H specifies the entropy.
Combining the above losses, we arrive at the following objective function of LDROT.
inf {LS + αLshift + βLclus},	(8)
g, hS, h T
where α, β > 0. In addition, we use the target classifier hT to predict target eXamples.
Remark on the shifting term: We now eXplain why including the shifting term Wd PhSS , PhTT
supports to reduce label and data shifts. First, we have the following inequality whose proof can be
found in AppendiX C:
WX (g#PS, g#PT ) = WX (PSs , PTt ) ≤ Wd (PSs , PTT )	⑼
since dX ≤ d. Therefore, by including the shifting term Wd PShS , PhTT , we aim to reduce
WdX g#PS , g#PT , which is useful for reducing the data shift on the latent space.
Second, we find that Wdy PShS , PhTT ≤ Wd PShS , PhTT since dy ≤ d. The inequality (i) in Proposi-
tion 3 suggests that reducing the label shift Wdy PhSS , PhTT helps to reduces the loss on the target
6
Under review as a conference paper at ICLR 2022
domain, therefore increasing the quality of our DA method. Finally, by including Wd PShS , PhTT ,
we aim to simultaneously reduce both terms WdY PhSS , PhTT and WdX PhSS , PhTT , which is equal to
WX (g#PS, g#PT). That step helps reduce the data shift between g#PSand g#PT, while forcing hτ
to mimic hS for predicting well on the target domain via reducing the label shift WdY PShS , PhTT .
3.2 Training procedure of LDROT
We now discuss a few important aspects of the training procedure of LDROT.
Similarity-aware version of the ground metric d : The weight λ in the ground metric d in Eq. (7)
represents the matching extent of g xT and g xS . Ideally, we would like to replace this fixed
constant λ by varied weights w xS , xT in such a way that w xS , xT is high if xT and xS share the
same label and low otherwise. However, it is not possible because the label of xT is unknown. As an
alternative, it appears that if we can have a good way to estimate the pairwise similarity s xS , xT of
xT and xS, s xS,xT seems to be high ifxT and xS share the same label and low if otherwise. Based
on this observation, we instead propose using a similarity-aware version of metric d as follows:
d (ZS,Z) = W (XS,xT) dχ (g (XS),g (xT)) + dγ (hs (XS),hT (xT)),
where the weight w xS , xT is estimated based on s xS , xT .
Entropic regularized version of shifting term: Since computing directly the shifting term
Wd PShS, PhTT is expensive, we use the entropic regularized version ofWd PShS, PhTT instead, which
we denote Wdε PShS, PhTT where ε is a positive regularized term (Detailed definition and discussion
of the entropic regularized Wasserstein metric is in Appendix A). The dual-form (Genevay et al.,
2016) of that entropic regularized term with respect to the ground metric d admits the following
form:
嗽(NS∑ φ(g 卜S)) - NT j∑ι"log( NS∑exp(
φ (g(xS))- d(Z, Z)
)!#),
(10)
ε
where φ is a neural net named the Kantorovich potential network, {(xS, yS)}N= 1and {xT }N1 are
source and target data, ziS = (xiS,hS(xiS)), zTj = (xTj , hT (xTj )), and Wij = W xiS, xTj .
Evaluating the Weights Wij: The weights Wij are evaluated based on the similarity scores sij :=
s xiS, xTj . Basically, we train from scratch or fine-tune a pre-trained deep net (e.g., ResNet (He
et al., 2016)) using source dataset with labels and compute cosine similarity of latent representations
rTj and riS of xTj and xiS as sij = s xiS, xTj = cosine-sim riS, rTj .
To ease the computation, we estimate the weights Wij according to source and target batches. Specif-
ically, we consider a balanced source batch ofMb source examples (i.e., M is the number of classes
and b is source batch size for each class). For a target example xTj in the target batch, we sort the sim-
ilarity array [si j]iM=b1 in an ascending order. Ideally, we expect that the similarity scores of the target
example xTj and the source examples xiS with the same class as xTj (i.e., totally we have b of theirs)
are higher than other similarity scores in the current source batch. Therefore, We find μi as the Mi-1-
percentile of the ascending similarity array [Sij]Mb and compute the weights as Wij = exp {Ssi-μi }
with a temperature variable τ . It is worth noting that this weight evaluation strategy assists us in
Sharpening and contraSting the weights for the pairs in the similar and different classes. More
specifically, for the pairs in the same classes, Sij tend to be bigger than μs, whilst for the pairs in
different classes, Sijtend to be smaller than μi. Hence, with the support of exponential form and
temperature variable τ, Wij for the pairs in the same classes tend to be higher than those for the pairs
in different classes.
Comparing to DeepJDOT (Damodaran et al., 2018): The Wd PShS, PhTT with d = λdX + dY
was investigated in DeepJDOT (Damodaran et al., 2018). However, ours is different from that work
7
Under review as a conference paper at ICLR 2022
Table 1: Classification accuracy (%) on Office-31 dataset for unsupervised DA (ResNet-50).
Method	A→W A→D D→W W→D				D→A	W→A	Avg
ResNet-50 (He et al., 2016)	70.0	65.5	96.1	99.3	62.8	60.5	75.7
DeepCORAL (Sun & Saenko, 2016)	83.0	71.5	97.9	98.0	63.7	64.5	79.8
DANN (Ganin et al., 2016)	81.5	74.3	97.1	99.6	65.5	63.2	80.2
ADDA (Tzeng et al., 2017)	86.2	78.8	96.8	99.1	69.5	68.5	83.2
CDAN (Long et al., 2018)	94.1	92.9	98.6	100.0	71.0	69.3	87.7
TPN (Pan et al., 2019)	91.2	89.9	97.7	99.5	70.5	73.5	87.1
SAFN (Xu et al., 2019)	90.1	90.7	98.6	99.8	73.0	70.2	87.1
rRevGrad+CAT (Deng et al., 2019)	94.4	90.8	98.0	100.0	72.2	70.2	87.6
DeepJDOT (Damodaran et al., 2018)	88.9	88.2	98.5	99.6	72.1	70.1	86.2
ETD (Li et al., 2020)	92.1	88.0	100.0	100.0	71.0	67.8	86.2
RWOT (Xu et al., 2020)	95.1	94.5	99.5	100.0	77.5	77.9	90.8
LDROT	956	98.0	98.1	100.0	85.6	84.9	93.7
(a) Closed-set setting with
YS=YT=[9].
(c) Partial-set setting with
Y S = [9], Y T = [6].
(d) Universal setting with
YS = [3]∪{4,5,6},
YT = [3]∪{7,8,9}.
Figure 2: Label shift estimation for various settings of DA when the source data set SVHN and the
target data set is MNIST. Here, we denote [C] := {0, 1, ...,C} for a positive integer number C.
in some aspects: (i) similarity based dynamic weighting, (ii) clustering loss for enforcing clustering
assumption for target classifier, and (iii) entropic dual form for training rather than Sinkhorn as in
Damodaran et al. (2018). More analysis of LDROT can be found in Section D.1.
4 Experiments
4.1	Experiments of theoretical part
Label shift estimation: In this experiment, we show how to evaluate the label shift if we know
the labeling mechanisms of the source and target domains. We consider SVHN as the source do-
main and MNIST as the target domain. These two datasets have ten categorical labels which stand
for the digits in Y = {0, 1, ..., 9}. Additionally, for each source or target example x, the ground-
truth label of x is a categorical label in Y = {0, 1, ..., 9}. Since these labels have good separa-
tion, from part (iii) of Proposition 3, we can choose fS (x) and f T (x) as one-hot vectors on the
simplex Y∆. Therefore, PfS and PfT are two discrete distributions over one-hot vectors represent-
ing the categorical labels, wherein each categorical label y ∈ {0, ..., 9} corresponds to the one-hot
vector 1y+1 = [0, .., 0, 1y+1,0, ..., 0]. The label shift between SVHN and MNIST is estimated by
either WdY PSfS , PTfT or WdY PfS , PfT , where dY is chosen as L1 distance. More specifically,
WdY PfS,PfT is evaluated accurately via linear programming1, while estimating WdY PSfS,PTfT
using the entropic regularized dual form Wdε PSfS,PTfT with ε = 0.1 (Genevay et al., 2016). More-
over, to visualize the precision when using a probabilistic labeling mechanism to estimate the label
shift, we train two probabilistic labeling functions hS and hT by minimizing the cross-entropy loss
with respect to fS and f T respectively and subsequently estimate WdY (PhS,PhT) using the entropic
regularized dual form Wdε PShS, PhTT with ε = 0.1. Note that the prediction probabilities of hS and
hT are now the points on the simplex Y∆ .
We compute the label shift for four DA settings including the closed-set, partial-set, open-set,
and universal settings. As shown in Figure 2, for all DA settings, the blue lines estimating
WdY PSfS,PTfT and the green lines estimating WdY (PhS,PhT) along with batches tend to approach
the red lines evaluating WdY PfS , PfT accurately, which illustrates the result of part (iii) in Propo-
1https://pythonot.github.io/all.html
8
Under review as a conference paper at ICLR 2022
Table 2: Classification accuracy (%) on Office-Home dataset for unsupervised DA (ResNet-50).
Method	Ar→Cl Ar→Pr Ar→Rw Cl→Ar Cl→Pr Cl→Rw Pr→Ar Pr→Cl Pr→Rw Rw→Ar Rw→Cl Rw→Pr AVg
ResNet-50 (He et al., 2016)	34.9	50.0	58.0	37.4	41.9	46.2	38.5	31.2	60.4	53.9	41.2	59.9	46.1
DANN (Ganin et al., 2016)	43.6	57.0	67.9	45.8	56.5	60.4	44.0	43.6	67.7	63.1	51.5	74.3	56.3
CDAN (Long et al., 2018)	50.7	70.6	76.0	57.6	70.0	70.0	57.4	50.9	77.3	70.9	56.7	81.6	65.8
TPN (Pan et al., 2019)	51.2	71.2	76.0	65.1	72.9	72.8	55.4	48.9	76.5	70.9	53.4	80.4	66.2
SAFN Xu et al. (2019)	52.0	71.7	76.3	64.2	69.9	71.9	63.7	51.4	77.1	70.9	57.1	81.5	67.3
DeepJDOT (Damodaran et al., 2018)	48.2	69.2	74.5	58.5	69.1	71.1	56.3	46.0	76.5	68.0	52.7	80.9	64.3
ETD (Li et al., 2020)	51.3	71.9	85.7	57.6	69.2	73.7	57.8	51.2	79.3	70.2	57.5	82.1	67.3
RWOT (Xu et al., 2020)	55.2	72.5	78.0	63.5	72.5	75.1	60.2	48.5	78.9	69.8	54.8	82.5	67.6
LDROT	57.4	79.6	82.5	67.2	798	80.7	66.5	53.3	82.5	70.9	57.4	84.8	71.9
sition 3. We also observe that the label shifts of partial-set, open-set, and universal settings are
higher than the closed-set setting as discussed in Theorem 5.
Implication on target performance: In this experiment, we demonstrate that the theoretical find-
ing of Theorem 4 indicating that forcing learning domain-inVariant representations hurts the target
performance. We train a classifier hST = g ◦ h, where g is a feature extractor and h is a classi-
fier on top of latent representations by solving min g, h {L (hST, fs, P S) + 0.1 X WLi (g #P S, g #P T)}
where WLε1 g#PS , g#PT is used to estimate WL1 g#PS , g#PT for learning domain-inVariant repre-
sentations on the latent space. We conduct the experiments on the pairs A→W (Office-31) and P→I
(ImageCLEF-DA) in which we measure the WS data shift on the latent space WLε1 g#PS , g#PT , and
the source and target accuracies. As shown in Figure 3, along with the training process, while the
WS data shift on the latent space consistently decreases (i.e., the latent representations become more
domain-invariant), the source accuracies get saturated, but the target accuracies get hurt gradually.
① oueωp uφωJφSSBM
(％) Aωalnoo<
Iteration	Iteration
Figure 3: Illustration on target performance to show that forcing learning domain-invariant repre-
sentations can hurt the target performance. Left: A→W (Office-31). Right: P→I (ImageCLEF-DA).
4.2	Experiments of LDROT on real-world datasets
We conduct the experiments on the real-world datasets: Digits, Office-31, Office-Home, and
ImageCLEF-DA to compare our LDROT to the state-of-the-art baselines, especially OT-based ones
DeepJDOT (Damodaran et al., 2018), SWD (Lee et al., 2019), DASPOT (Xie et al., 2019), ETD Li
et al. (2020), and RWOT (Xu et al., 2020). Due to the space limit, we show the results for Office-31
and Office-Home in Tables 1 and 2, while other results, parameter settings, and network archi-
tectures can be found in Appendix D. The experimental results indicate that our proposed method
outperforms the baselines.
4.3	Ablation Study for LDROT
We conduct comprehensive ablation studies to investigate the behavior of our LDROT. Due to the
space limit, we leave to the ablation and analytic studies to Appendix D.7.
5	Conclusion
In this paper, we study label shift between the source and target domains in a general DA setting.
Our main workaround is to consider valid transformations transporting the target to source domains
allowing us to align the source and target examples and rigorously define the label shift between
two domains. We then connect the proposed label shift to optimal transport theory and develop
further theory to inspect the properties of DA under various DA settings (e.g., closed-set, partial-
set, open-set, or universal setting). Furthermore inspired from theory development, we propose
Label and Data Shift Reduction via Optimal Transport (LDROT) which can mitigate data and label
shifts simultaneously. We conduct comprehensive experiments to verify our theoretical findings and
compare LDROT against state-of-the-art baselines to demonstrate its merits.
9
Under review as a conference paper at ICLR 2022
Reproducibility Statement: Source codes for our experiments are provided in the supplementary of
the paper. The details of experimental settings, computational infrastructure, and other used public
libraries are given in the supplementary material. All datasets that we used in the paper are published
and they are easy to find in the Internet.
Ethics Statement: Given the nature of the work, we do not foresee any negative societal and ethical
impacts of the work.
References
Mardn Abadi, Ashish Agarwal, Paul Barham, Eugene Brevdo, Zhifeng Chen, Craig Citro, Greg S
Corrado, Andy Davis, Jeffrey Dean, Matthieu Devin, et al. Tensorflow: Large-scale machine
learning on heterogeneous distributed systems. arXiv preprint arXiv:1603.04467, 2016. D.4
Martin Arjovsky, Soumith Chintala, and Leon Bottou. Wasserstein generative adversarial net-
works. In Doina Precup and Yee Whye Teh (eds.), Proceedings of the 34th International
Conference on Machine Learning, volume 70 of Proceedings of Machine Learning Research,
pp. 214-223. PMLR, 06-11 Aug 2017. URL http://Proceedings .mlr.press∕v70/
arjovsky17a.html. 2.2, 2.3
S.	Ben-David and R. Urner. On the hardness of domain adaptation and the utility of unlabeled target
samples. In Proceedings of the 23rd International Conference on Algorithmic Learning Theory,
pp. 139-153, 2012. ISBN 9783642341052. 1
S.	Ben-David and R. Urner. Domain adaptation—can quantity compensate for quality? Annals of
Mathematics and Artificial Intelligence, 70(3):185-202, March 2014. ISSN 1012-2443. 1
S.	Ben-David, J. Blitzer, K. Crammer, A. Kulesza, F. Pereira, and J. W. Vaughan. A theory of
learning from different domains. Mach. Learn., 79(1-2):151-175, May 2010. ISSN 0885-6125.
1, 2.3
Z. Cao, K. You, M. Long, J. Wang, and Q. Yang. Learning to transfer examples for partial domain
adaptation. CoRR, abs/1903.12230, 2019. URL http://arxiv.org/abs/1903.12230.
2.3
O. Chapelle and A. Zien. Semi-supervised classification by low density separation. In AISTATS,
volume 2005, pp. 57-64. Citeseer, 2005. 3.1
C.	Cortes, M. Mohri, and Andres M. Medina. Adaptation based on generalized discrepancy. The
Journal of Machine Learning Research, 20(1):1-30, 2019. 1, 2.3
N. Courty, R. Flamary, A. Habrard, and A. Rakotomamonjy. Joint distribution optimal transportation
for domain adaptation. In Advances in Neural Information Processing Systems, pp. 3730-3739,
2017. 1, 2.3
B. B. Damodaran, B. Kellenberger, R. Flamary, D. Tuia, and N. Courty. Deepjdot: Deep joint distri-
bution optimal transport for unsupervised domain adaptation. In Vittorio Ferrari, Martial Hebert,
Cristian Sminchisescu, and Yair Weiss (eds.), Computer Vision - ECCV 2018 - 15th European
Conference, Munich, Germany, September 8-14, 2018, Proceedings, Part IV, volume 11208 of
Lecture Notes in Computer Science, pp. 467-483, 2018. 1, 3.2, 1, 2, 4.2, 4, 5
Z. Deng, Y. Luo, and J. Zhu. Cluster alignment with a teacher for unsupervised domain adaptation,
2019. 1,4,5
D.	M. Endres and J. E. Schindelin. A new metric for probability distributions. IEEE Trans. Inf.
Theor., 49(7):1858-1860, 2006. ISSN 0018-9448. 2.3
G. French, M. Mackiewicz, and M. Fisher. Self-ensembling for visual domain adaptation. In Inter-
national Conference on Learning Representations, 2018. 1
Y. Ganin and V. Lempitsky. Unsupervised domain adaptation by backpropagation. In Proceedings
of the 32nd International Conference on International Conference on Machine Learning - Volume
37, ICML’15, pp. 1180-1189, 2015. 1,4
10
Under review as a conference paper at ICLR 2022
Y. Ganin, E. Ustinova, H. Ajakan, P. Germain, H. Larochelle, F. Laviolette, M. Marchand, and
V. Lempitsky. Domain-adversarial training of neural networks. J. Mach. Learn. Res., 17(1):
2096-2030,jan2016. ISSN 1532-4435. 1, 2, 5
S. Garg, Y. Wu, S. Balakrishnan, and Z. Lipton. A unified view of label shift estimation. In
H. Larochelle, M. Ranzato, R. Hadsell, M. F. Balcan, and H. Lin (eds.), Advances in Neural
Information Processing Systems, volume 33, pp. 3290-3300. Curran Associates, Inc., 2020a. 1,
2.2
Saurabh Garg, Yifan Wu, Sivaraman Balakrishnan, and Zachary Lipton. A unified view of label
shift estimation. In H. Larochelle, M. Ranzato, R. Hadsell, M. F. Balcan, and H. Lin (eds.),
Advances in Neural Information Processing Systems, volume 33, pp. 3290-3300. Curran Asso-
ciates, Inc., 2020b. URL https://proceedings.neurips.cc/paper/2020/file/
219e052492f4008818b8adb6366c7ed6-Paper.pdf. 2.3
AUde Genevay, Marco Cuturi, Gabriel Peyr6, and Francis Bach. Stochastic optimization for large-
scale optimal transport. In Advances in Neural Information Processing Systems, volume 29. Cur-
ran Associates, Inc., 2016. URL https://proceedings.neurips.cc/paper/2016/
file/2a27b8144ac02f67687f76782a3b5d8f-Paper.pdf. 3.2, 4.1, A.2
P.	Germain, A. Habrard, F. Laviolette, and E. Morvant. A PAC-Bayesian approach for domain adap-
tation with specialization to linear classifiers. In Proceedings of the 30th International Conference
on International Conference on Machine Learning, ICML’13, 2013. 1
P.	Germain, A. Habrard, F. Laviolette, and E. Morvant. A new PAC-Bayesian perspective on domain
adaptation. In Proceedings of the 33rd International Conference on International Conference on
Machine Learning - Volume 48, ICML’16, pp. 859-868, 2016. 1
Yves Grandvalet and Yoshua Bengio. Semi-supervised learning by entropy minimiza-
tion. In Advances in Neural Information Processing Systems, volume 17. MIT
Press, 2004. URL https://proceedings.neurips.cc/paper/2004/file/
96f2b50b5d3613adf9c27049b2a888c7-Paper.pdf. 3.1
K.	He, X. Zhang, S. Ren, and J. Sun. Deep residual learning for image recognition. In 2016 IEEE
Conference on Computer Vision and Pattern Recognition (CVPR), pp. 770-778, 2016. 3.2, 1, 2,
D.2, D.5, 5
F.	D. Johansson, D. Sontag, and R. Ranganath. Support and invertibility in domain-invariant rep-
resentations. In Proceedings of Machine Learning Research, volume 89, pp. 527-536, 2019.
1
Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint
arXiv:1412.6980, 2014. D.5
Trung Le, Tuan Nguyen, Nhat Ho, Hung Bui, and Dinh Phung. Lamda: Label matching deep
domain adaptation. In Marina Meila and Tong Zhang (eds.), Proceedings of the 38th International
Conference on Machine Learning, volume 139 of Proceedings of Machine Learning Research, pp.
6043-6054. PMLR, 18-24 Jul 2021. URL https://proceedings.mlr.press/v139/
le21a.html. 1
Chen-Yu Lee, Tanmay Batra, Mohammad Haris Baig, and Daniel Ulbricht. Sliced Wasserstein
discrepancy for unsupervised domain adaptation. In IEEE Conference on Computer Vision and
Pattern Recognition, CVPR 2019, Long Beach, CA, USA, June 16-20, 2019, pp. 10285-10295.
Computer Vision Foundation / IEEE, 2019. 1, 4.2, 4, D.7
M. Li, Y. Zhai, Y. Luo, P. Ge, and C. Ren. Enhanced transport distance for unsupervised domain
adaptation. In IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), June
2020. 1, 1, 2, 4.2, 4, 5
Zachary Lipton, Yu-Xiang Wang, and Alexander Smola. Detecting and correcting for label shift
with black box predictors. In Jennifer Dy and Andreas Krause (eds.), Proceedings of the 35th
International Conference on Machine Learning, volume 80 of Proceedings of Machine Learning
Research, pp. 3122-3130. PMLR, 10-15 Jul 2018. 1, 2.2
11
Under review as a conference paper at ICLR 2022
M. Long, Y. Cao, J. Wang, and M. Jordan. Learning transferable features with deep adaptation
networks. In F. Bach and D. Blei (eds.), Proceedings of the 32nd International Conference on
Machine Learning, volume 37 of Proceedings ofMachine Learning Research, pp. 97-105, Lille,
France, 2015. 1
M. Long, Z. Cao, J. Wang, and M. I. Jordan. Conditional adversarial domain adaptation. In S. Ben-
gio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Garnett (eds.), Advances in
Neural Information Processing Systems 31, pp. 1640-1650. Curran Associates, Inc., 2018. 1, 2,
4, 5
Y. Mansour, M. Mohri, and A. Rostamizadeh. Domain adaptation with multiple sources. In
D. Koller, D. Schuurmans, Y. Bengio, and L. Bottou (eds.), Advances in Neural Information
Processing Systems 21, pp. 1041-1048. 2009. 1, 2.3
T. Miyato, S. Maeda, M. Koyama, and S. Ishii. Virtual adversarial training: A regularization method
for supervised and semi-supervised learning. IEEE Transactions on Pattern Analysis and Machine
Intelligence, 41(8):1979-1993, Aug 2019. ISSN 1939-3539. 3.1
Y. Pan, T. Yao, Y. Li, Y. Wang, C. Ngo, and T. Mei. Transferrable prototypical networks for unsu-
pervised domain adaptation. In CVPR, pp. 2234-2242, 2019. 1, 2, 4, 5
G.	Peyre and M. Cuturi. Computational optimal transport: With applications to data science. Foun-
dations and Trends® in Machine Learning, 11(5-6):355-607, 2019. 1, 2.2
I. Redko, A. Habrard, and M. Sebban. Theoretical analysis of domain adaptation with optimal
transport. In Joint European Conference on Machine Learning and Knowledge Discovery in
Databases, pp. 737-753, 2017. 1, 2.3
F. Santambrogio. Optimal transport for applied mathematicians. Birkauser NY, pp. 99-102, 2015.
1, 2.2, A.1, A.1, B.1
R. Shu, H. Bui, H. Narui, and S. Ermon. A DIRT-t approach to unsupervised domain adaptation. In
International Conference on Learning Representations, 2018. 1, 3.1
B. Sun and K. Saenko. Deep coral: Correlation alignment for deep domain adaptation. In Gang Hua
and Herve J6gou (eds.), Computer Vision - ECCV 2016 Workshops, pp. 443T50, Cham, 2016.
Springer International Publishing. 1, 4, 5
Remi Tachet des Combes, Han Zhao, Yu-Xiang Wang, and Geoffrey J Gordon. Domain adaptation
with conditional distribution matching and generalized label shift. Advances in Neural Informa-
tion Processing Systems, 33, 2020. 1
E. Tzeng, J. Hoffman, T. Darrell, and K. Saenko. Simultaneous deep transfer across domains and
tasks. CoRR, 2015. 1
E. Tzeng, J. Hoffman, K. Saenko, and T. Darrell. Adversarial discriminative domain adaptation.
In 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp. 2962-2971,
2017. 1,4, 5
L. van der Maaten and G. Hinton. Visualizing data using t-SNE. Journal of Machine Learning
Research, 9:2579-2605, 2008. D.7
C. Villani. Optimal Transport: Old and New. Grundlehren der mathematischen Wissenschaften.
Springer Berlin Heidelberg, 2008. ISBN 9783540710509. 1, 2.2, 2.3, A.1
Y. Xie, M. Chen, H. Jiang, T. Zhao, and H. Zha. On scalable and efficient computation of large
scale optimal transport. In Kamalika Chaudhuri and Ruslan Salakhutdinov (eds.), Proceedings of
the 36th International Conference on Machine Learning, volume 97 of Proceedings of Machine
Learning Research, pp. 6882-6892, Long Beach, California, USA, 09-15 Jun 2019. PMLR. 1,
4.2, 4
R. Xu, G. Li, J. Yang, and L. Lin. Larger norm more transferable: An adaptive feature norm
approach for unsupervised domain adaptation. In 2019 IEEE/CVF International Conference on
Computer Vision (ICCV), pp. 1426-1435, 2019. doi: 10.1109/ICCV.2019.00151. 1, 2, 5
12
Under review as a conference paper at ICLR 2022
R. Xu, P. Liu, L. Wang, C. Chen, and J. Wang. Reliable weighted optimal transport for unsupervised
domain adaptation. In CVPR 2020, June 2020. 1, 1, 2, 4.2, 4, 5
Y. Zhang, Y. Liu, M. Long, and M. I. Jordan. Bridging theory and algorithm for domain adaptation.
CoRR, abs/1904.05801, 2019a. 1, 2.3
Y. Zhang, H. Tang, K. Jia, and Mingkui Tan. Domain-symmetric networks for adversarial domain
adaptation. 2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),
pp. 5026-5035, 2019b. 5
H. Zhao, R. T. Des Combes, K. Zhang, and G. Gordon. On learning invariant representations for
domain adaptation. In International Conference on Machine Learning, pp. 7523-7532, 2019. 1,
2.3
13
Under review as a conference paper at ICLR 2022
Supplement to "On Label Shift in Domain Adaptation
via Wasserstein Distance"
In this appendix, we collect several proofs and remaining materials that are deferred from the main
paper.
•	In Appendix A, we present notations and definitions that are deferred from the main text
including optimal transport and entropic regularized Wasserstein distance.
•	In Appendix B, we present proofs of all the key results.
•	In Appendix C, we present proofs of the remaining results, including the derivation of the
dual form of entropic regularized optimal transport.
•	In Appendix D, we provide training specification and additional experimental results.
A Notations and definitions
In this appendix, we provide notations, notions, and definitions that are used in the main text.
A.1 Optimal Transport
Given two probability measures (X ,P) and (Y , Q) and a cost function or ground metric d (x, y),
under the conditions stated in the below theorem (cf. Theorems 1.32 and 1.33 (Santambrogio,
2015)), the primal form of Wasserstein (WS) distance (Santambrogio, 2015) is defined as:
W d (P,Q)=τ inf CEX ~p[ d (X，T (X))],	(”)
T :T #P=Q
Wd (P,Q)= HIinfQ)E(x,y卜Y [d (X，y)]，	(12)
where Γ(P,Q) specifies the set of joint distributions over X × Y which admits P and Q as
marginals. The first definition is known as Monge problem (MP), while the second one is known as
Kantorovich problem (KP). We now restate the sufficient conditions for which (MP) and (KP) are
equivalent (cf. Theorems 1.32 and 1.33 (Santambrogio, 2015)).
Theorem. If X and Y are compact, Polish metric spaces, P and Q are atomless, and d is a lower
semi-continuous function, then (KP) is equivalent to (MP) in the sense that two infima are equal.
In addition, under some mild conditions as stated in Theorem 5.10 in Villani (2008), we can replace
the primal form by its corresponding dual form
Wd (P, Q) = max {EQ [φ c (X)] +EP[φ (y)]},	(13)
φ ∈L1(Ω,P)
where Li (Ω,P) := {ψ: Rq ∣ψ(y)| dP(y) < ∞} and φC is the C-transform of function φ defined as
ΦC (x) ：= miny {d (x, y) - φ (y)}.
A.2 Entropic Regularized Duality
To enable the application of optimal transport in machine learning and deep learning, Genevay et al.
developed an entropic regularized dual form in Genevay et al. (2016). First, they proposed to add an
entropic regularization term to the primal form in (12)
Wε (P,Q) := Y minP {E(χ,yMY[d(x,y)] + εDkl(kP乳Q)},	(14)
where ε is the regularization rate, DKL (∙∣∣∙) is the KUllback-Leibler (KL) divergence, and P 0 Q
represents the specific coupling in which Q and P are independent. Note that when ε → 0, Wdε (P, Q)
approaches Wd (P, Q) and the optimal transport plan 展 of (14) also weakly converges to the optimal
transport plan γf' of (12). In practice, We set ε to be a small positive number, hence 展 is very close
to γf'. Second, using the Fenchel-Rockafellar theorem, they obtained the following dual form w.r.t.
the potential φ
14
Under review as a conference paper at ICLR 2022
Wdε(P,Q) = mφax Z φεc(x)dQ(x)+Z φ(y)dP(y) =mφax{EQ[φεc(x)]+EP[φ(y)]},
where φε (X) := -εlog (EP [exp { -d(,^+φ。) }]).
(15)
A.3 Preliminaries
Notions. For a positive integer n and a real number p ∈ [1, ∞), [n] indicates the set {1, 2, . . . ,n}
while kxkp denotes the lp-norm of a vector x ∈ Rn. Let Y S and Y T be the label sets of the source
and target domains that have MS := Y S and MT := Y T elements, respectively. Meanwhile,
Y = Y S ∪ Y T stands for the label set of both domains which has the cardinality of M := |Y |.
Subsequently, we denote Y∆, Y∆S, and Y∆T as the simplices corresponding to Y , Y S, and Y T re-
SPectively. Finally, let fs (∙) ∈ Y、and fτ (∙) ∈ Y∆ be the labeling functions of the source and target
domains, respectively, by filling zeros for the missing labels.
We first examine a general supervised learning setting. Consider a hypothesis h in a hy-
pothesis class H and a labeling function f (i.e., f (∙) ∈ Y4 and h(∙) ∈ Y4 where Y4 :=
π ∈ RM : kπ k1 = 1 and π ≥ 0 with the number of classes M). Let dY be a metric over Y4 . We
further define the general loss of the hypothesis h w.r.t. the data distribution P and the labeling
function f as: L (h, f, P) := R dY (h (x) ,f(x))dP(x) .
Next we consider a domain adaptation setting in which we have a source space X S endowed with
a distribution PS and the density function ps (x) and a target space X T endowed with a distribution
P T and the density function Pt (X). Let fs (∙) ∈ Y、and fτ (∙) ∈ Y、be the labeling functions of the
source and target domains respectively. It appears that pS (x, y) = pS (x) fS (x, y) and pT (x, y) =
PT (X) fT (X, y) are the source and target joint distributions of pairs (X, y) respectively. Note that for
a categorical label y ∈ {1, ..., M}, fS (X, y) and fT (X, y) represent the y-th element of the prediction
probabilities fS (X) and fT (X).
B Proofs of all the key results
In this appendix, we provide useful lemmas and proofs for main results in the paper.
B.1	Useful Lemmas
Lemma 6. If Y ∈ Γ (P fS, P 心，there exists γ0 ∈ Γ (P S ,PT) such that (fs, fT )# γ0 = Y.
Proof. Let denote Y as the joint distribution of the samples (XS, fs (XS)) where xs 〜 PS and Yτ as
the joint distribution of the samples {τr, fτ xxr)) where xτ 〜Pτ. It is obvious that Y is a joint
distribution of PS and PfS and YT is a joint distribution of PT and PfT . According to the gluing
lemma (see Lemma 5.5 in Santambrogio (2015)), there exists ajoint distribution μ such that for any
draw (xs, τS, ττ,xτ)〜μ then (XS, τS)〜Y, (TS, ττ)〜Y, and (xτ, ττ)〜YT.
Let Y0 be the distribution of samples (XS,XT) (i.e., the projection of μ onto the first and fourth
dimensions). This follows that Y is a joint distribution of PS and PT (i.e., Y ∈ Γ (PS,PT)). In
addition, since (xs, τS)〜Y, TS = fS (XS), since (xt, ττ)〜Yτ, Tτ = fτ (xt), and (TS, ττ)〜Y.
Therefore, we reach fS , fτ # Y0 = Y.
We note that in the above proof, we employ a general form of the gluing lemma for 4 distributions
and spaces. The proof is mainly based on the gluing lemma for 3 distributions and spaces and
trivial.	□
Lemma 7. Let dZ be defined with resPect to the family H as follows:
dZ(z1,z2) = sup dY(h(z1),h(z2)),
h∈H
15
Under review as a conference paper at ICLR 2022
where z1 and z2 lie on the latent space Z . For any z1 and z2, if h (z1) = h (z2) ,∀h ∈ H leads to
z1 = z2, then dZ is a proper metric.
Proof. First, dZ (z1,z2) ≥ 0 and dZ (z1,z2) = 0 means h (z1) = h (z2) ,∀h ∈ H , which leads to z1 =z2.
Second, it is obvious that dZ (z1,z2) = dZ (z2,z1) ,∀z1,z2.
Given any z1 , z2 , z3, we have
dZ(z1,z3) = sup dY (h(z1) ,h(z3)) ≤ sup (dY (h(z1) ,h(z2)) +dY (h(z2) ,h(z3)))
h∈H	h∈H
≤ sup dY (h(z1) ,h(z2)) + sup dY (h(z2) ,h(z3))
= dZ(z1,z2) +dZ (z2,z3) .
Therefore, dZ is a proper metric.
□
B.2	Proof and Corollary of Proposition 2
Proof. (i) First, we will prove that WdY
be such that H#PTf T
≥ L S (S, T ). Let H : supp PTf T → supp PSfS
PSfS where supp indicates the support of a distribution.
We can express H as
H (X, fT (x)) ：= (H1 (X, fT (x)),H2 (X, fT (x))),
with Hι (x,fT (x)) ∈ XS and H (x, fτ (x)) ∈ Y、. Define K(x) := Hι (x, fτ (x)). We claim that
K#PT = PS. Observe first that for any US ⊂ XS × Y4, we have PSfS (US) = PS (VS) where VS ：=
{X ∈ XS | (x, fS (x)) ∈ US}. Next, let XS ⊂ XS be any measurable set and denote US := XS X %.
Then by using the observation above and the fact H#PTfT = PSfS , we obtain
P S CXS ) = PSS (US ) = P Tt (H-1 (US)) = PTT(K-1 (Xs ) × Y∆) = P T(K-1 (Xs )),
or equivalently, K#PT = PS. It follows from the fact H#PTT = PSS that H2 (x, fτ (x)) = fS (K(x)),
which gives
Wy (PSS,PT"=万HPnfPS E(W(X))〜PTT [dγ(fT(x),H2(x，产(X)))]
H:H#P T =P S	f
≥
inf
K:K#PT=PS
[dY (fτ (X), fS(k (x)))]
EX 〜P T
L S(S, T).
In order to prove the reverse inequality, let us consider any maps K satisfying K#PT =
P S. Define a map H : SUPP (PTT) → SUPP (PSS) as H (x, fτ (x)) := (K (x), fS (K (x))), We
will show that H#PTfT = PSfS . Indeed, let US ⊂ XS × Y∆ be any measurable sets and
take	Xs	:=	{X	∈ X S |	(x, fS (X))	∈	US }.	Then, as H-1	(US)	= {(x,fS(x))	| K (x) ∈ XS}=
{(x, fS (x)) | x ∈ K-1 (Xs)}, we have
PTt (H-1 (US)) = Pτ (K-1 (Xs)) = PS (Xs) = PSS (US),
which means H#PTfT = PSfS . As a result,
WdY (P SS, P Tτ ) ≤κ∕nL PSEX ~P τ [ dγ (fτ (X), fS (K (x)))] = L S (S, T).
By combining the above two inequalities, we obtain the desired equality
WdY (PSfS,PTfT) =LS(S,T).
16
Under review as a conference paper at ICLR 2022
(ii) First, let γ ∈ Γ PfS , PfT . According to Lemma 6, there exists γ0 ∈
Γ (PS, PT) such that (fs, fτ )# γ0 = γ. Then,
%SjT)~Y [dY (ys,yT)] = E(a,χT)~y [dγ (fS (XS), fT(XT))]
≥Knf p T) E(XSxT W[ dY(f(X)厅 T(Xx))]
γ∈	,
=WdY PSfS,PTfT =LS(S,T).
Therefore, we arrive at
WY (PfS,PfT)= inf	E(ySyT)〜Y [dY (ys,yx)] ≥ LS(S,T).
γ∈Γ PfS,PfT	,
Second, let γ0 ∈ Γ (PS,PT), We denote Y = (fS, fT)# γ0. We then have
E(XS, XT ∖zγ [ dY (fS (xS ) , fT (xT ))] = E( yS, yT )〜Y [ dY	, y )]
≥ inf
γ∈Γ(PfS,PfT
E( yS ,yT)〜Y [ dY (yS, yT )] = WY (P fS, P f^ .
This folloWs that
LS(S,T) = WY (PSS,pTt) ∖erinf PT)EM,XT)~yJd (f (XS)于(XT))]
Y ∈Γ(P ,P )
≥ WdY (PfS,PfT .
Hence, the proof is completely done.
□
Corollary 8. Thefollowing inequality holds WY (PSS, PSS) ≤ L (hS, fS, Ps).
Proof. According to Proposition 1, We have:
WY (pSs, Pfs) = inf	EX〜PS [dY (hs (X), fs (L (X)))].
L:L#PS=PS
Then by choosing L as the identity map (i.e., L(X) = X for all X), We obtain:
WY (PSS, PSfS) ≤ EX〜PS [dY (hS (X), fS (L (X)))] = L (hS, fS, PS).
□
B.3 Proof of Theorem 4
First, We Will shoW that
L S (S, T) ≤ L (hS, fS, P S) + L (hT, fT, P T) + WY (PhS, P hT).
By using the triangle inequality for the Wasserstein distance With respect to the metric dY, We have
LS(S,T)=WdY (PfS,PfT)
≤) WY (PfS,PhS) + WY (PhS,PhT) + WY(PhT,PfT)
=WdY (PSfS,PhSS) +WdY(PhS,PhT)+WdY (PhTT,PTfT)
(2)
≤ L (hS, fS, P S) + L (hT, fT, P T) + WY (PhS, PhT).
(1)	(2)
Here We note that for ≤ , We use the triangle inequality and for ≤ , We invoke Corollary 8.
17
Under review as a conference paper at ICLR 2022
It is sufficient to prove that
'WdY (Phs, PhT) ≤ WZ (gs#PS, g#Pt ).
Indeed, let Y ∈ Γ (gS#PS, g#PT) and denote γ0 = h#y. Then, We have γ0 ∈ Γ (PhS, P*)，and
E(乃,y2)〜γ0 [dY (y 1,y2)] = EG1,Z2)〜Y [dY (h (Z 1), h (Z2))] ≤ E(Z 1,Z2)〜Y [dZ (Z 1,Z2)].
Therefore, We obtain
WY (P hS, P hT )=	inf	E( y 1, y 2)〜γ0 [dγ (y 1, y 2)]
Y ∈Γ( hS, hT )
≤ E(y1,y2)〜γ0 [dY (y 1,y2)] ≤ E(Z 1,Z2)〜Y [dZ (Z 1,Z2)].
Finally, We reach
WdY(PhS,PhT) ≤	,JnfT ,、E(Z 1,Z2)~γ[dz(Z1,Z2)] = WZ (gs#PS,gτ#PT).
γ ∈Γ( gS #PS, gτ #P τ)
Hence, We have proved our claim.
C	Proof of the remaining results
C.1 Proof of Proposition 3
Denote by pYS = (pYS (y))yM=1 and pYT = ( pYT (y))yM=1 the marginal distributions of the source and target
domain labels, i.e., pYS (y) = R S pS(x, y)dx and pYT (y) = R T pT (x, y)dx. Let PYa be the discrete
measure on the vertices of ∆M-1 putting mass PYa(y) on the one-hot representation ofy, ∀y ∈ [M], a ∈
{S, T}. For dY(y,y0) = ky -y0kpp When p ≥ 1, the folloWing holds:
⑴ L (hτ, fτ, Pτ) ≤ LS (S, T) + L (hS, fS, PS) + WY (PSS,P%) + const, where the constant can
be vieWed as a reconstruction term: supL,K:L#PT =PS,K#PS=PT EPT dY fT (K (L (x))) ,fT (x) ;
(ii)LS(S,T)≥ kpYS-pYTkpp;
(iii)	In the setting that PS and PT are mixtures of well-separated Gaussian distributions, i.e.,
M
pa (x ) = ∑ Pa (y )N (x ∣μa, ∑a), ∀ a ∈{ S, T }
y=1
with ∣∣μy - μy0∣∣2 ≥ D X max{k∑αkθ/'k∑y，k 1/}∀a ∈ {S,T},y = y, in which ∣∣ ∙ ∣∣op denotes the
operator norm and D is sufficiently large, we have
0≤ |LS(S,T)-Wpp(PYS,PYT)| ≤ε(D),	(16)
where ε(D) is a small constant depending on D, pYS ,pYT, (ΣyS,ΣyT)yM=1, and it goes to 0 as D → ∞.
(iv)	In the anti-causal setting, where pS(x|y) = pT (x|y) for all x,y,
LS (S, T) ≤Mp∣pYS-pYT∣1+min{EPS∣fS-fT∣pp,EPT∣fS-fT∣pp};	(17)
Proof. (i) Let L and K be two arbitrary maps such that L#PT = PS and K#PS = PT . We have the
following triangle inequality:
dY (hτ (x), fτ (x)) ≤ dY (hτ (x), hS (L (x))) + dy (hS (L(X)), fS (L (x)))
+ dY (fS (L(x)), fτ (K(L(x)))) + dY (fτ (K(L(x))), fτ (x)).
18
Under review as a conference paper at ICLR 2022
Therefore, we obtain
EPτ [dγ (hr (ɪ), fτ (x))] ≤ EPτ [dγ (hτ (x), hs (L (X)))] + EPT [dγ (hs (L(X)), fs (L (X)))]
+ EPT [dγ (fs (L(x)), fτ (K(L(x))))] + EPT [dγ (fτ (K(L(x))), fτ (x))]
=EPτ [dγ (hτ (x), hs (L (x)))] + EPS [dγ (hs (x), fs (x))]
+ EPS [dγ (fs (x), fτ (K (x)))] + EPT [dγ (fτ (K(L (x))), fτ (x))]
=EPT [dγ (hτ (x), hs (L (x)))] + L (hs, fs, PS)
+ ePS [ dγ (f (x) ,fT (K (x)))] + ep T [ dγ (fT (K (L (x))) ,fT(X))].
Note that, the derivation in = is due to L#PT = Ps, hence gaining
Epτ [dγ (hs (L (x)), fs (L (x)))] = Eps [dγ (hs (x), fs (x))],
EpT [dγ (fs (L (x)), fτ (K (L (x))))] = EpS [dγ (fs (x), fτ (K(x)))].
As a consequence, we find that
L (hτ, fτ, PT) ≤	inf	(EPT [dγ (hτ (x), hs (L (x)))] + L (hs, fs, Ps)
L, K: L# Pτ =PS, K #PS=P TI
+ EpS [dγ (fs (x), fτ (K (x)))] + EpT [dγ (fτ (K (L(x))), fτ (x))] }
≤ LLJnf=PSePT [dγ (hτ (x), hS(L(x)))] + k k#nf=pT ePs [dγ (fS (x), fT (K (x)))]
+ L (hs, fs,Ps) +	sup	EpT [dγ (fτ (K(L(x))), fτ (x))]
L,K:L#PT =Ps,K#Ps=PT
=WY (PSS, P Tt ) + LS (S, T)+ L (hs, fs, Ps)
+ sup	Epτ [dγ (fτ (K(L(x))), fτ (x))].
L, K: L# P T =PS, K #PS=P T
(ii)	We will show that for all transformation L satisfying L#PT = PS,
ept UfT(x) - fS(l(x))∣∣PP ≥ I I pY -pYI I PP,	(18)
and then take the infimum of the LHS, which directly leads to the conclusion. Indeed, by applying
Jensen inequality, we find that
M
epT ∖∖fτ(x) - fS(l(x))∣∣p = ∑ epTIPτ(yIX) -Ps(y∖LX))∣P
y=1
M
≥ ∑ ∣ept(pt(y∖x)-Ps(y∖l(x)))∣p
y=1
M
=∑ IEPtpt(y∖x)-epsps(y∖x)∣p
y=1
M
=∑∣pY (y) - pY (y )∣p
y=1
=U pY - pYU p
We have thus proved our claim.
(iii)	Consider y's as one-hot vectors, i.e., vertices of the simplex. By the fact that Wasserstein
distances on simplex are no greater than M, we have
p-1
∖WpP (P fS, P fT) - -WP (P Y, P Y )∖ = ∑ WP' (P fS, P fT )WpP-I(PY, P γ) ∖Wp (P fs, P fT) - Wp (P Y, P Y )∖
i=0
≤ PMp-1∖WP(Pfs,PfT) - Wp(PY,PY)∖.
19
Under review as a conference paper at ICLR 2022
Besides, by triangle inequalities,
|Wp(PfS,PfT)
(19)
—
Thus, we only need to prove the claimed bounds for Wp(PfS,PYS) and Wp(PfT ,PYT). Because the
proofs are similar for the source and target, in the followings, we drop the superscript S, T for the
ease of notations. We first show that the mass of Pf concentrates near the vertices, i.e., there exists
(Oy, εy)y∈[M] being small numbers depends on D such that, for Z 〜f (X),
0 ≤ pY (y) -Pf(kZ-ykpp < αy) ≤ εy ∀y ∈ [M].
(20)
Indeed, for all y, let By = {x : k∑-1/2(X- μy)k ≤ √D}. Denote the dimension of X to be d and X2
the Chi-square distribution with d degree of freedom. We have the following tail bound:
P(X ∈ By |y) = P(Xd ≤ D) ≥ 1 - eTD-2d)/4.
Hence, we obtain that
M
P(X ∈ By) = ∑ P(X ∈ By |y)PY(y) ≥ P(Xd ≤ D)PY(y) ≥ PY(y)- εy,
y=1
where εy = e-(D-2d)/4. Besides, if x ∈ By then for any y0 6= y, by triangle inequalities and the
definition of the operator norm,
k∑- 1/2( X - μy 0)k2 ≥k∑y ok -ι∕2k( x - μy 0)k2
≥k∑y ok -ι∕2(k(μy - μy o)k2-k(χ-μy )k2)
≥k∑y ok -u2(k(μy - μy o )k2 -k(μy - μy o)k2∕√D)
≥ d-√D,
where the above inequalites are due to our assumption and the fact that
k x - μy k2 ≤ k∑y kɪ P2k∑71/2( x - μy )k2 ≤ k∑y k k1P2√D ≤kμy - μy 'k2∕√D.
Hence, for all x ∈ By and yo 6= y, we have
P(XIy') = 1ςy|1/2 ,(k∑-1/2(x-μy)k2-k∑-01/2(x-μyo)k2)/2 . D--(D-√D)2
P (XI y) = I∑y o|1/2	.	.
Combining the above inequality with Bayes’ rule leads to
P (y ∣ X ) =---17^TPV ≥ 1 - ∑ ⅞0)4x4 ≥ 1-加
1+∑y o=yεP^ -	y∑-P(y) P(x | y) 一
where γy . e-D(D-2√d). ThiS means the difference between labeling function at X ∈ By and y is
bounded as follows
kf(x)-ykPP=(1-P(yIx))P+∑P(yoIx)P≤2(1-P(yIx))P≤2γyP.
yo6=y
Choosing αy = 2γyP, by the fact that x ∈ By implies kf(x) -ykPP ≤ αy, we have
Pf(kZ-ykPP≤αy)≥P(X∈By)≥PY(y)-εy.
Putting the above results together, we find that
PY (y) -Pf(kZ-ykPP ≤ αy) ≤ εy.
Due to the continuity of Pf , we can also shrink αy such that the inequality still holds and the left-
hand side is positive and we get our claim (20).
Now let Ey = {z : kz -ykPP ≤ αy} and Dy be a set containing Ey for all y ∈ [M] satisfying Pf(Dy) =
PY (y) and {Dy}yM=1 is a partition of ∆M-1. Let Pf be the density ofPf. It can be seen that
π(z,y) =Pf(z)1[z∈Dy]
20
Under review as a conference paper at ICLR 2022
is the density function of a coupling between Pf and PY. Hence, we have the following inequalities:
Zz∈∆M-1 ky-zkppπ(z,y)dz
M∑y1= M∑y1= M∑
≤≤≤
/
z∈Dy
ky-zkppπ(z,y)dz
ky-zkppπ(z,y)+	ky-zkppπ(z,y) dz
≤ ∑ Z	αyπ(z,y)dz+Z	Mpπ(z,y)dz dz
y=1 ∖ Jz ∈ Ey	Jz ∈ Dy \ Ey	J
M
≤ ∑ α + MPεy . e-PD(D-2√D) + eTD-2d)/4,
y=1
which goes to 0 exponentially fast when D grows to infinity. Plugging the above inequality into
equation (19), we obtain the conclusion of part (iii) of the proposition.
(iv)	Let ∏y = P (∙∣ y) be conditional measure of X given Y = y .By using the law of total probability,
we have
MM
PS = ∑ PYS (y)πy, PT = ∑ PYT(y)πy.	(21)
y=1	y=1
Given the above equations, some simple algebraic transformations would lead to
M
P fS = f #PS = ∑ Py (y) fs #ny.
y=1
(22)
Now let qyy = min{PY(y),pγ (y)} and choose (qyy0)y=y0 such that (qyy0)y=τM,y∕=1M is a valid cou-
pling of PYS and PYT . By the convexity of Wasserstein distance, we have
WP (P fS, P fT ) ≤ ∑ qyy 0 WP ( fs #ny ,f #ny 0).	(23)
y,y0
As the distance between two arbitrary points on the simplex ∆M-1 is not greater than M, neither is
the Wasserstein distance between any two measures on ∆M-1. Therefore,
∑ qyy o WP (fS #ny ,f #ny，) ≤ MP ∑ Qyy，≤ MP k PY — PY l∣1.	(24)
y6=y0	y6=y0
Besides,
MM
∑ QyyWP (fS#ny,f #ny) ≤ ∑ Py (y)	Tjnf、/
y=1	y=1	π ∈Γ(πy,πy) X ×X
fs(x)-fT(x0)PPdπ(x,x0)
M
≤ ∑PYs(y)	fs(x)-fT(x)Pdπy(x)
y=1	X
M
= ∑PYs(y)	fs(x)-fT(x)PPP(x|y)dx
=Z	fs(x)-fT(x)PPPs(x)dx
=EPsfs-fTPP,
Similarly, since qyy ≤ PT (y) for all y ∈ [M], we could also obtain
M
∑ qyyWP(fS#ny,f #ny) ≤ EPTll fs — fT\\PP .
y=1
21
Under review as a conference paper at ICLR 2022
Consequently,
M
∑ qyyWP(fS#ny,f #ny) ≤ min{EpS Il fS - fT∖∖PP ,EpTll fS - fT∖∖PP}■	(25)
y=1
Combining equations (24) and (25), We have the conclusion of part (iv).	□
C.2 Proof of Theorem 5
Before providing the proof of Theorem 5, We first introduce a lemma Which facilitates our later
arguments.
Lemma 9. Let μ and V be two probability measures on Rd. Denote by μ1 (Vi ) and μ2 (ν2) the
marginal distributions of μ (V) on the first k dimensions and the last d 一 k dimensions, respectively,
where 0 ≤ k ≤ d. We have
Wpp(μ,ν) ≥ WP(μ1,V1) + Wpp(μ2, V2)	(26)
Proof. Let π be the optimal coupling of μ and ν, and (X1,..., Xd, Yi ,..., Yd) is a random vector
having law π, we have (X1,..., Xd)〜μ, (Yi,..., Yd)〜V. Denote by ∏ and ∏ the marginal distri-
bution of (X1, . . . ,Xk,Y1, . . . ,Yk) and (Xk+1, . . . , Xd,Yk+1, . . . ,Yd), respectively. It can be seen that π1
is a coupling of μ1 and Vi while ∏ is a coupling of μ2 and V2. Hence,
WP (μ, V) = Zed dxRd k ɪ- yk Pd π (X, y)
=	kx1 -y1kppdπ1(x,y)+	kx2 -y2kppdπ2(x,y)
Rk ×Rk	Rd-k ×Rd-k
≥ W(μ1,V1) + WP(μ2,V2),
where x1 ∈ Rk is a vector including the first k coordinates of x, whereas x2 ∈ Rd-k contains the last
d 一 k elements of x, and similar definitions apply for y 1 and y2.	□
Now, we come back to the proof of Theorem 2.
Proof of Theorem 5:
(i) We consider two random vectors (Xi,...,Xm)〜PfS and (Y1,...,Ym)〜P*.Recall that QS
and QT are the marginal distributions of PfS and PfT on their first C 一 1 dimensions, respectively,
while QS\T denotes the marginal of PfS on the space of variables having labels in the set YS \ YT.
Since YT ⊂ YS, we have
1.	(X1,...,XC-1)〜QS, (Yi,...,Yc-1)〜QT;
2.	(XC +1,.∙∙,xm) 〜QS\T, (yc +1,.∙∙,ym) 〜δ0M-c,
where 0m denotes the zero vector in Rm for m ∈ N. Let μ be the distribution of
(X1, . . . , XC-1, XC+1, . . . ,XM), V the distribution of (Y1, . . . ,YC-1,YC+1, . . . ,YM). As the simplex ∆M-1
is anM- 1 dimensional manifold in RM, the Wasserstein distance between PfS and PfT can be writ-
ten as
M
WP (P fS, p fT )= inf	r 1 ∑ | xi - yi | pd Y(xc, ye),
p J J	Y ∈Γ(μ,V) JRC -1×RC -1 i=	C C
where	xCb	=	(x1, . .	., xC-1, xC+1, . .	.,xM),yCb	=	(y1, . . ., yC-1, yC+1, . . .,yM)	and	xC	:=
1 -∑k6=Cxk,yC := 1 -∑k6=Cyk. As |xC-yC|p ≥ 0, it can be deduced that
WP(PfS,PfT) ≥ WP(μ, V).
Besides, according to Lemma 9, we get
WP(μ, V) ≥ Wp(QS,QT) + WP(QS\T, δ°M-C) = W(QS,QT) + Ex〜QS\ThkXkPP].
Putting the above two inequalities together, we obtain the conclusion that
wp(PfS, PfT) ≥ wp(QS, QT)+EX~QS\ThkXkpi.	(27)
22
Under review as a conference paper at ICLR 2022
(ii)	Part (ii) is done similarly to part (i). Therefore, it is omitted.
(iii)	Let (X1,...,XM)〜PfS and (Yι,...,Ym)〜Pfτ. Assume that YS = {1,...,C,C + 1,...,D}
and YT = {1, . . . ,C,D+ 1, . . . ,M}. It follows from the definitions of QS,QT, QS\T and QT\S that
1.	(Xι,...,XC-ι)〜Qs, (Yι,...,Yc-i)〜QT；
2.	(XC +1,.∙∙,xd) 〜Qs\τ, (YC +1,.∙∙,YD) 〜δ0D-c;
3.	(XD +1,..∙, XM ) 〜δ0 M - D, (YD +1,.∙∙, YM ) 〜Q T ∖ S.
Let μ be the distribution of (Xι,...,X-ι,XC +ι,...,XM), and V be the distribution of
(Y1, . . . ,YC-1,YC+1, . . . ,YM). By using the same arguments as in part (i), we get Wpp(PfS,PfT) ≥
Wp(μ, V). Next, applying Lemma 9 twice, we obtain
WP(μ, V) ≥ Wp(QS, QT) + Wp(QS∖τ,加D-c) + W(δM-d, Qt∖s)
=WP(Qs,QT) + EY〜QS∖ThkYkPPi + EX〜Qτ∖ShkXkPPi .
As a consequence, we have proved our claim in part (iii).
C.3 Proofs of claims in paragraph "Remark on shifting term
Lemma 10. The followings are true:
(i)	WdX(g#PS,g#PT)=WdX(PShS,PhTT);
(ii)	WY (PTT,P*) ≤ LS (S, T) + L (hs, fs,PS) + WY (PSS篮，.
Proof. (i) Applying the same arguments as in Proposition 1, we have
WdX(PShS,PhTT)=	inf EPT [dX[g(x),g(H1(x))]]=	inf	EPT[dX(g(x),g(L(x)))], (28)
X h h	H:H#PT =PS	hT	L:L#PT =PS
hT hS
where H ((x, hT (x))) = (H1 ((x, hT (x))), H2 ((x, hT (x)))) such that H1((x,hT(x))) ∈ X S. So now we
only need to prove that
WdX (g#PS, g#PT) =	inf	EPT [dX (g(x), g(L(x)))] .
X	L:L#PT =PS
Due to the equivalence of Monge and Kantorovich problem, we can write the RHS as
inf EPT [dX(g(x),g(L(x)))] =	inf 7 E(χS χT卜Y [dX(g(xs),g(XT))]
L: L# P T=PS	γ ∈Γ(Ps,P t ) ( , ) γ
(29)
(30)
To prove Eq. (29), we will show that RHS is not less than LHS and inversely. Indeed, for any
coupling γ ∈ Γ(PS, PT), We have γ0 = (g, g)#Y as a coupling of (g#PS, g#PT), therefore
e(χs,χT)~γ[dX(g(XS),g(XT))] = E(yS,yT)~γ0[dx(yS,yT)] ≥ , Uinf TvEE(ys,yE[dχ(yS,yT)].
γ0∈l (g#Ps ,g#PT )
Taking the infimum with respect to γ,
Ye「infPT)E(XS,χTWY [dχ(g(Xs),g(Xt)) ≥ WX(g#Ps,g#PT)] .	(31)
Conversely, thanks to Lemma 6, for each coupling γ0 of (g#PS, g#PS), there exists a coupling γof
(PS,PS) such that Y0 = (g, g)#Y, which deduces that
E(y S, y T、zY [ dX (yS, y T )] = E( XST 卜 Y [ dX (g (XS), g (XT ))] ≥	inf	e(xS∕T 卜Y [ dχ (g (XS ), g (XT ))].
Y ∈Γ(PS,PT)
Taking the infimum with respect to Y0,
WX (g#Ps,g#Pτ) ≥	inf T E(xS,xTWY [dχ(g(xs),g(xt))] .	(32)
Y∈Γ(PS,PT)
23
Under review as a conference paper at ICLR 2022
Inequalities (31) and (32) together imply equation (29) and finish proof of part (i).
(ii)	Using triangle inequality, we have
Wy (PTT, PTt ) = Wy (PhT, PfT ) ≤ ,WdY (PhT, PhS) + WdY (PhS, PfS) + WY (PfS, PfT )
=WY (PTT, PSs ) + L (hS, fs, Ps) + LS (S, T).
As a consequence, We obtain the conclusion of part (ii).	□
D	Additional experiment results
D.1 More analysis about rationale of the terms used in the objective
function of LDROT
This objective function consists of three losses: (i) standard loss LS, (ii) shifting loss Lshift, and
(iii) clustering loss Lclus . The standard loss LS is trained on the labeled source domain. The shift-
ing loss aims to reduce both data and label shift simultaneously on the latent space by minimizing
Wd PShS, PhTT Where d = λdX +dY for Which dX is data distance on the latent space and dY is dis-
tance on the label simplex. Based on the theory developed, We demonstrate that minimizing this
term helps to reduce both data shift (i.e., WX (g#PS,g#PT) and label shift (i.e., WY (PSS,PTT))∙
Finally, the L clus assists us in enforcing the clustering assumption to boost the generalization of the
target classifier ST. By enforcing the clustering assumption, classifiers are encouraged to preserve
the cluster structure and give the same predictions for data representations in the same cluster. It ap-
pears that When pushing target latent toWard source representations via minimizing Wd PShS , PhTT ,
source and target representations tend to group in clusters, hence We can strengthen and boost gen-
eralization of target classifier ST by enforcing it to preserve the predictions in the same clusters.
In addition, We propose a dynamic Weighting for λ using similarities of pairs betWeen source and
target examples. For our similarity-based Weighting distance, We base on pre-trained similarities
to decide if We push more or feWer pairs of source and target latent representations together to
reduce data and label shifts more efficiently. Definitely, if We can push groups of source and target
representations With the same labels together more efficiently, We can certainly reduce both data and
label shifts simultaneously.
D.2 Data preparation and pre-processing
Digits. We resize the resolution of each sample in the dataset to 32 × 32, and normalize the value of
each pixel to the range of [-1, 1].
Office-31, Office-Home, and ImageCLEF-DA. We use 2048-dimensional features extracted from
ResNet-50 (He et al., 2016) pretrained on ImageNet.
D.3 Algorithm of LDROT
We present peusocode of LDROT in Algorithm 1.
D.4 Network architecture
There are 2 types of the architecture described in Table 3, Which are small (S) and large (L) net-
Works. We use L netWork for Digits and S netWork for the other datasets. Additionally, excluding
dense layers in the φ netWork, We add the batch normalization layers on top of convolutional and
dense layers to reduce the overfitting problem. Finally, We implement our LDROT in Python us-
ing TensorFloW (version 1.9.0) (Abadi et al., 2016), an open-source softWare library for Machine
Intelligence developed by the Google Brain Team. All experiments are run on a computer With an
NVIDIA Tesla V100 SXM2 With 16 GB memory.
24
Under review as a conference paper at ICLR 2022
Algorithm 1 Pseudocode for training our LDROT.
Input: A source batch BS = {(XS,yS)}Mbv a target batch BT = { ^xτT,yτT)}“
S T	j=1
Output: Classifier h* h* , generator g*.
Evaluate riS }iM=b1 and rτj	based on BS and Bτ respectively.
b is the batch size.
b
Compute the weights Wij ibased on {rS }M and {r[}
for number of training iterations do
for k steps do
j=1
Update φ according to Eq. (13).
end for
Update hs, hT and g according to Eq. (10).
end for
Table 3: Small and large networks for LDROT. The Leaky ReLU (lReLU) parameter a is set to 0.1.
Architecture	S	L
Input size	2048	32 × 32 × 3
Generator g	256 dense, ReLU dropout, p = 0.5 Gaussian noise, σ = 1	instance normalization 3 × 3 conv. 64 lReLU 3 ×3 conv. 64 lReLU 3 × 3 conv. 64 lReLU 2 × 2 max-pool, stride 2 dropout, p = 0.5 Gaussian noise, σ = 1 3 × 3 conv. 64 lReLU 3 ×3 conv. 64 lReLU 3 × 3 conv. 64 lReLU 2 × 2 max-pool, stride 2 dropout, p = 0.5 Gaussian noise, σ = 1 3 × 3 conv. 8 lReLU 2 × 2 max-pool, stride 2
Classifier hS, hτ	#classes dense, softmax	3 × 3 conv. 8 lReLU
		3 × 3 conv. 8 lReLU 3 × 3 conv. 8 lReLU global average pool #classes dense, softmax
φ	1 dense, linear	100 dense, ReLU 1 dense, linear
D.5 Implementation details
We first present our procedure to compute the weights Wij, which derives from the feature extraction
process. For Digits, we design a network to train from scratch on labeled source examples. Then
source and target features are extracted via this pretrained model. For the other datasets, we use
extracted ResNet-50 features (He et al., 2016) and design a small network to train LDROT. During
training, the features are used for first computing pairwise similarity scores and the weights Wij after
that.
For LDROT, we find that some hyper-parameters contributes substantially to the model performance,
namely τ and ε . The temperature parameter τ, which contributes to sharpening and contrasting the
weights Wij, is fixed to 0.5. Tweaking the regularization rate ε is vital for scaling φεc (x) and we
select ε = 0.1. For trade-off parameters α,β, we choose α = 0.1 and β = 0.5 for all settings. We
apply Adam optimizer Kingma & Ba (2014) (β1 = 0.5, β2 = 0.999) with Polyak averaging. The
25
Under review as a conference paper at ICLR 2022
Table 4: Classification accuracy (%) on Digits dataset for unsupervised domain adaptation.
Method	S→M	M→U	U→M	Avg
DANN (Ganin & LemPitsky, 2015)	85.5	84.9	86.3	85.6
ADDA (Tzeng et al., 2017)	89.2	85.4	96.5	90.4
DeepCORAL (Sun & Saenko, 2016)	88.3	84.1	93.6	88.7
CDAN (Long et al., 2018)	89.2	95.6	98.0	94.3
TPN (Pan et al., 2019)	93.0	92.1	94.1	93.1
rRevGrad+CAT (Deng et al., 2019)	98.8	94.0	96.0	96.3
SWD (Lee et al., 2019)	98.9	98.1	97.1	98.0
DeepJDOT (Damodaran et al., 2018)	96.7	95.7	96.4	96.3
DASPOT (Xie et al., 2019)	96.2	97.5	96.5	96.7
ETD (Li et al., 2020)	97.9	96.4	96.3	96.9
RWOT (XU et al., 2020)	98.8	98.5	97.5	98.3
LDROT	99.0	98.2	99.1	98.8
Table 5: Classification accuracy (%) on ImageCLEF-DA dataset for unsupervised domain adaptation
(ResNet-50).
Method	I→P	P→I	I→C	C→I	C→P	P→C	Avg
ResNet-50 (He et al., 2016)	74.8	83.9	91.5	78.0	65.5	91.2	80.7
DeepCORAL (Sun & Saenko, 2016)	75.1	85.5	92.0	85.5	69.0	91.7	83.1
DANN (Ganin et al., 2016)	75.0	86.0	96.2	87.0	74.3	91.5	85.0
ADDA (Tzeng et al., 2017)	75.5	88.2	96.5	89.1	75.1	92.0	86.0
CDAN (Long et al., 2018)	77.7	90.7	97.7	91.3	74.2	94.3	87.7
TPN (Pan et al., 2019)	78.2	92.1	96.1	90.8	76.2	95.1	88.1
SymNets (Zhang et al., 2019b)	80.2	93.6	97.0	93.4	78.7	96.4	89.9
SAFN (Xu et al., 2019)	79.3	93.3	96.3	91.7	77.6	95.3	88.9
rRevGrad+CAT (Deng et al., 2019)	77.2	91.0	95.5	91.3	75.3	93.6	87.3
DeepJDOT (Damodaran et al., 2018)	77.5	90.5	95.0	88.3	74.9	94.2	86.7
ETD (Li et al., 2020)	81.0	91.7	97.9	93.3	79.5	95.0	89.7
RWOT (Xu et al., 2020)	81.3	92.9	97.9	92.7	79.1	96.5	90.0
LDROT	81.7	96.7	97.5	94.2	80.4	96.7	91.2
learning rate is set to 0.001 and 0.0001 for Digits and the other datasets respectively. Additionally,
in nature, our model solves the minimax optimization problem (see Eq. (13) in the main paper) in
which φ and hS, hT, g are updated sequentially in each iteration with five times for φ and one time
for hS, hT, g. Finally, We use the cosine distance for dχ and Kullback-Leibler (KL) divergence for
dY .
D.6 Additional results for Digits and ImageCLEF-DA
We additionally present the experimental results for Digits and ImageCLEF-DA datasets in Tables
4 and 5. It can be observed that LDROT also outperforms the baselines on these datasets.
D.7 Ablation studies
Effects of the label shift term dY and the weights wij: To answer the question of how will the
model performance be affected when the weights wij or dY in Eq. (10) is removed?, we evaluate our
model on four different settings: LDROT without both the weights W订 and the label shift dγ(∙, ∙)
(LDROT-wd), without the weights Wij (LDROT-W), without minimizing the label shift dγ(∙, ∙)
(LDROT-d) and a complete model (LDROT+wd). The results on Office-Home in Table 6 dedicate
the significance of Wij and dγ(∙, ∙), where these components remarkably contribute to reducing the
data and label shifts with 4.1% improvements on average.
Rationale of Weigh strategy. In Figure 4, we visualize the similarity scores of a randomly selected
target example xTj in a batch w.r.t. source examples xiS using the source and target domains Amazon
26
Under review as a conference paper at ICLR 2022
Table 6: Accuracy (%) of ablation study on Office-Home.
Method	Ar→Cl	Cl→Ar	Cl→Rw	Pr→Ar	RW→Ar	Avg
LDROT-wd	51.7	62.6	785^^	63.3	66.1	64.4
LDROT-w	56.7	62.3	79.3	64.0	66.8	65.8
LDROT-d	55.4	65.5	80.1	65.0	68.4	66.9
LDROT+wd	57.4	67.2	80.7	66.5	70.9	68.5
and Dslr of Office-31 dataset, respectively. The orange points represent the similarity scores for the
same class, while the blue points represent those for different classes. It is evident that the orange
values tend to bigger than the blue ones except in some outlier cases, hence if We choose μi as
indicated, we can separate well the orange and blue values.
Figure 4: (a) 1D visualization of finding an appropriate μi. It is able to split same-label pairs
(orange points) and different-label pairs (blue points) if μi is set to 号-1-perCentiIe of this array. (b)
To observe the Weight values wij of those pairs, We randomly picked a target example to compute
similarity scores With 20 representative source points in a batch, and then sort them in ascending
order. After computing the Weights, the figures for the same-label pairs tend to be much higher
than that for different-label pairs. A heat-map color is used to represent the Weights magnitude (the
brighter means higher value).
(a) ResNet.	(b)
LDROT.
Figure 5: The t-SNE visualization of A→D (Figure a, b) tasks With label and domain information.
Each color denotes a class While the circle and cross markers represent the source and target data
respectively.
Feature visualization. We visualize the features of ResNet-50 and our methods on A→D (Office-
31) and P→C (ImageCLEF-DA) tasks by t-SNE van der Maaten & Hinton (2008) in Figure 5. The
visualizations in Figure 5a and 6a shoW that ResNet-50 classifies quite Well on source domains (A
and P) but poorly on target domains (D and C). While the representation in Figure 5b and 6b is
27
Under review as a conference paper at ICLR 2022
generated by our method with better alignment. LDROT achieves exactly 31 and 12 clusters cor-
responding to 31 and 12 classes of Office-31 and ImageCLEF-DA, which represents generalization
ability of our model in which the classifier generalizes well not only on the source domain but also
on the target domain.
(a)
ResNet.
(b)
LDROT.
Figure 6: The t-SNE visualization of and P→C (Figure a, b) tasks with label and domain informa-
tion. Each color denotes a class while the circle and cross markers represent the source and target
data respectively.
Figure 7: Comparision of convergence performance between LDROT and other approaches on the
transfer task A→D.
Convergence. We testify the convergence of our LDROT with the test errors on A→D task, as
shown in Figure 7. We conduct experiments on three methods including Source (test error is
achieved with classifier trained on source data without adaptation), SWD Lee et al. (2019), and
our LDROT. For fair comparison, the methods are applied the same optimizer (Adam with learning
rate of 0.0001) and batch size. The results show that the error of LDROT on the target domain is
remarkably lower, which illustrates better generalization capability of the source classifier. During
training, our method encourages a target sample to actively moving to a suitable group or cluster of
source examples in a similarity-aware manner. This phenomenon implies that LDROT enjoys faster
and stable convergence than the other settings.
28