Under review as a conference paper at ICLR 2022
Safe Multi-Task Learning
Anonymous authors
Paper under double-blind review
Ab stract
In recent years, Multi-Task Learning (MTL) attracts much attention due to its
good performance in many applications. However, many existing MTL models
cannot guarantee that its performance is no worse than its single-task counterpart
on each task. Though this phenomenon has been empirically observed by some
works, little work aims to handle the resulting problem, which is formally defined
as negative sharing in this paper. To achieve safe multi-task learning where no
negative sharing occurs, we propose a Safe Multi-Task Learning (SMTL) model,
which consists of a public encoder shared by all the tasks, private encoders, gates,
and private decoders. Specifically, each task has a private encoder, a gate, and a
private decoder, where the gate is to learn how to combine the private encoder and
public encoder for the downstream private decoder. To reduce the storage cost
during the inference stage, a lite version of SMTL is proposed to allow the gate to
choose either the public encoder or the corresponding private encoder. Moreover,
we propose a variant of SMTL to place all the gates after decoders of all the tasks.
Experiments on several benchmark datasets demonstrate the effectiveness of the
proposed methods.
1	Introduction
Multi-Task Learning (MTL) (Caruana, 1997; Zhang & Yang, 2021), which aims to improve the
generalization performance of multiple learning tasks by sharing knowledge among those tasks, has
attracted much attention in recent years. Compared with single-task learning, it not only improves
the performance but also reduces the training and inference time. Though MTL has demonstrated
its usefulness in many applications, MTL cannot guarantee to improve the performance of all the
tasks when compared with single-task learning. As empirically observed in (Lee et al., 2016; Guo
et al., 2020; Sun et al., 2020; Standley et al., 2020), when learning on multiple tasks together, each of
some existing MTL models can achieve better performance on some of the tasks than a single-task
model but perform worse on the other tasks. Such phenomenon is defined as the negative sharing
phenomenon here, which is similar to the ‘negative transfer’ phenomenon (Wang et al., 2019) in
transfer learning (Yang et al., 2020) but with some difference in the ways of knowledge transfer and
sharing in those two learning paradigms as discussed later. One reason for the occurrence of negative
sharing is that there are unrelated tasks among tasks in investigation, making jointly learning these
tasks impair the performance of some tasks.
To the best of our knowledge, there is little work to study the negative sharing problem for MTL.
In this paper, we firstly give a formal definition for negative sharing occurred in MTL. Then we
formally define an ideal and also basic situation of MTL, safe multi-task learning, where an MTL
model performs no worse than its single-task counterpart on each task. Hence, safe multi-task
learning means that there is no negative sharing occurred. According to the definition of MTL
(Caruana, 1997; Zhang & Yang, 2021), we can see that every MTL model should achieve safe
multi-task learning. Otherwise, single-task learning is more preferred than MTL, since an unsafe
MTL model may bring the risk of worsening the generalization performance of some or even all the
tasks. Moreover, we formally define η-partially safe multi-task learning as a loose version of safe
multi-task learning to allow the MTL model to perform worse than its single-task counterpart on
tasks with a proportion no larger than η.
To achieve safe multi-task learning, we propose a Safe Multi-Task Learning (SMTL) model. Specif-
ically, given m learning tasks, the SMTL model consists of a public encoder shared by all the tasks,
m private encoders for the m tasks, m gates for the m tasks, and m private decoders for the m
1
Under review as a conference paper at ICLR 2022
tasks. Hence each task has a private encoder, a gate, and a private decoder. The gate of each task
is responsible of learning how to linearly combine the public encoder and the corresponding private
encoder for the downstream private decoder. To reduce the storage cost during the inference stage,
we propose a lite version of SMTL via the Gumbel-softmax trick (Jang et al., 2017; Maddison et al.,
2017) to enforce each gate to choose either the public encoder or private encoder. Moreover, to
study the impact of different locations of the gates, we propose variants of the SMTL model, which
place the gates after the decoders. Furthermore, we analyze the SMTL model from the perspec-
tives of generalization bound and optimization. Experiments on several MTL benchmark datasets
demonstrate the effectiveness of the proposed methods.
The main contributions of this paper are summarized as follows.
•	We provide formal definitions for MTL, including negative sharing, safe multi-task learn-
ing, and η-partially safe multi-task learning.
•	To achieve safe multi-task learning, we propose a simple and effective SMTL model. Built
on the SMTL model, we propose its variants.
•	We conduct extensive experiments to demonstrate the superiority of the proposed methods
over state-of-the-art methods.
2	Related Work
MTL has been extensively studied in recent years (Evgeniou & Pontil, 2004; Zhang & Yeung, 2010;
Kumar & Daume III, 2012; Zhang et al., 2021; Guo et al., 2021). How to design a good network
architecture for MTL is an important issue. The most popular model is the multi-head hard sharing
architecture (Caruana, 1997; Zhang et al., 2014; Long & Wang, 2015; Liu et al., 2015; Ruder et al.,
2019), which shares the first several layers among all the tasks and allow the subsequent layers to
be specific to different tasks. Then, to better handle task relationships, different MTL architectures
have been proposed. For example, Misra et al. (2016) propose a cross-stitch network to learn to
linearly combine hidden representations of different tasks. Liu et al. (2019) propose a Multi-Task
Attention Network (MTAN), which consists of a shared network and an attention module for each
task so that both shared and task-specific feature representations can be learned via the attention
mechanism. Gao et al. (2019) propose a Neural Discriminative Dimensionality Reduction (NDDR)
layer to enable automatic feature fusing at every layer from different tasks. Sun et al. (2020) propose
an Adaptive Sharing (AdaShare) method to learn the sharing pattern through a task-specific policy
that selectively chooses which layers to be executed for each task. Guo et al. (2020) propose an
algorithm to learn where to share or branch within a network for MTL. Cui et al. (2021) propose an
Adaptive Feature Aggregation (AFA) layer, where a dynamic aggregation mechanism is designed to
allow each task to adaptively determine the degree of the knowledge sharing between tasks. All the
existing works do not study how to achieve safe multi-task learning, which is the focus of this paper.
3	Safe Multi-Task Learning
In this section, we first formally define some terminologies for MTL. Then we introduce the pro-
posed SMTL method. Moreover, we propose some variants of the SMTL model. Finally, we provide
some analyses for SMTL.
3.1	Definitions
Definition 1 (Negative Sharing). For an MTL model which is trained on multiple learning tasks
jointly, if its generalization performance on some tasks is inferior to the generalization performance
of the corresponding single-task model which is trained on each task separately, then negative shar-
ing occurs.
Remark 1. Negative sharing occurs when some tasks are totally or partially irrelevant to other
tasks. In this case, manually enforcing all the tasks to have some forms of sharing will impair
the performance of some or even all the tasks. In Definition 1, the MTL model and the single-
task model usually have similar architectures as totally different architectures may bring additional
confounding factors. Moreover, negative sharing is similar to negative transfer (Wang et al., 2019)
2
Under review as a conference paper at ICLR 2022
in transfer learning (Yang et al., 2020). However, knowledge transfer in transfer learning is directed
as it is from a source domain to a target domain, while knowledge sharing in MTL is among all
the tasks, making it undirected. From this perspective, negative sharing is different from negative
transfer.
Definition 2 (Safe Multi-Task Learning). When no negative sharing occurs for an MTL model on a
dataset, this MTL model is said to achieve safe multi-task learning on this dataset.
The ideal situation for an MTL model is to achieve safe multi-task learning. However, not all the
MTL methods can achieve safe multi-task learning and hence we define η-partially safe multi-task
learning, which can be viewed as a loose version of safe multi-task learning.
Definition 3 (η-Partially Safe Multi-Task Learning). Given multiple learning tasks in a dataset, η-
partially safe multi-task learning (0 ≤ η ≤ 100) indicates that on about η percentage of tasks, the
generalization performance of an MTL model is no worse than that of its single-task counterpart.
When η equals 100, η-partially safe multi-task learning becomes safe multi-task learning. When η
is equal to 0, the MTL model performs worse than its single-task counterpart in all the tasks.
3.2	SMTL
With m learning tasks {Ti}im=1, our goal is to design a model that can achieve safe multi-task learn-
ing. To achieve this goal, we propose the SMTL model, which is introduced in the following.
Without loss of generality, we consider the case that different tasks share the input data or equiva-
lently each data point has an output for each task. As shown in the left figure of Figure 1, the SMTL
model can be divided into four parts: a public encoder fS shared by all the tasks, m private encoders
{ft}tm=1 for m tasks, m gates {gt}tm=1 for m tasks, and m private decoders {ht}tm=1 for m tasks. For
task t, its model consists of the public encoder fS , the private encoder ft , the gate gt , and the private
decoder ht, where fS and ft are combined by gt . Specifically, given a data point x, the gate gt in
task t receives two inputs: fS(x) and ft (x), and outputs gt (fS(x), ft(x)), which is fed into ht to
obtain the final prediction ht(gt(fS(x), ft(x))), which is used to define a loss for x.
Here the gate gt is to determine the contributions of fS and ft . Ideally, when task t is unrelated to
other tasks, gt should choose ft only. On another extreme where all the tasks have the same data
distribution, all the tasks should use the same model and hence gt should choose fS only. On cases
between the two extremes, gt can combine fS and ft in proportion. To achieve the aforementioned
effects, we use a simple convex combination function for gt as
gt (fS (x), ft (x)) = αtfS(x) + (1 - αt)ft(x),	(1)
where αt ∈ [0, 1] defines the weight of fS (x) and is a learnable parameter. When αt equals 0, only
the private encoder ft will be used, which corresponds to the unrelated case. When αt is equal to 1,
only the public encoder fS will be used, which is corresponding to the case that all the tasks follow
the same distribution. When αt is between 0 and 1, fS and ft are combined with proportions αt
and 1 - αt , respectively, where αt can be adaptively learned to minimize the training loss on task t.
Thus, the entire objective function of SMTL is formulated as
nm
min- XXLt(y}ht(gt(fs(xi),ft(xi)))),	⑵
Θ∈C mn
where xi denotes the ith data point, yti denotes the label of xi in task t, n denotes the total number
of data points in the training dataset, Θ includes all the parameters in fS, {ft}tm=1, {gt}tm=1, and
{ht}m=ι, C = {Θ∣0 ≤ at ≤ 1,∀t} denotes the feasible set for Θ, and Lt denotes the loss func-
tion for task t (e.g., the pixel-wise cross-entropy loss for semantic segmentation, L1 loss for depth
estimation, and element-wise dot product loss for surface normal prediction).
To see why the proposed SMTL model could achieve safe multi-task learning, we compare SMTL
and the corresponding Single-Task Learning (STL) model which consists of a private encoder ft and
a private decoder ht. It is easy to see that SMTL can reduce to the STL model for some or even all
the tasks by making the gates of those tasks choose the corresponding private encoders (i.e., setting
at's of those tasks to 0). So if a task is unrelated to other tasks, the SMTL model can use the gating
mechanism to separate this outlier task from other tasks, which may help achieve safe multi-task
learning. Moreover, we can show that the training loss of the SMTL model is no larger than the
average of that of the STL model on each task. To see that, it is easy to show that the STL model for
3
Under review as a conference paper at ICLR 2022
Figure 1: Illustration of the SMTL and L-SMTL models. Left figure: Pipeline for the SMTL
model, which is identical to the training phase of L-SMTL. For task t, x is first fed into both the
public encoder fS and private encoder ft, then it is through the gate gt to obtain the combined
feature representation, and finally it is through the private decoder ht to obtain the output yt. The
number of tasks is set to three for illustration. Right figure: Test phase for L-SMTL. After finishing
the training process of L-SMTL, gt can choose which encoder (i.e., the public encoder fS or private
encoder ft) is used for each task. In this way, at the test process, only the chosen encoders need to be
saved, which could reduce the number of parameters and speedup the inference. In this illustration,
task 1 and task 3 choose the public encoder, while task 2 goes through its private encoder.
task t can be represented as ht(g0(0, ft(x))), where g0 denotes the gate of task t with αt as 0 and
0 denotes a null network. As g0 is a feasible gate for SMTL, after sufficient training, We probably
have
nm	nm
min- XXLt(yt,ht(gt(fs(xi),ft(xi)))) ≤ min一 XXLtV,ht(g;(U,ft(Xi))))
Θ∈C mn	Θ0 mn
where Θ0 includes parameters in {ft}tm=1 and {ht }tm=1 in STL models for all the tasks. This in-
equality shows the benefit of the proposed SMTL model. Even though the training loss is not a tight
estimation of the generalization loss, we think that it is an indicator to reflect the generalization per-
formance of the two models. In Section 4.4, we study to use a bi-level formulation to learn the gates
as hyperparameters on a validation set and its performance is comparable with the SMTL model
based on problem (2), which proves the usefulness of the indication of the training loss.
Similarly, we can show that the training loss of the SMTL model is no larger than that of the DMTL
model (a.k.a. the multi-head hard sharing network) which consists of a shared encoder by all the
tasks and private decoders for m tasks. To see this, the DMTL model for task t can be represented
as ht(gt1(fS(x), 0)), where gt1 denotes the gate of task t with αt as 1. As gt1 is a feasible gate for
SMTL, itis easy to get that the training loss of the SMTL model after sufficient training is lower than
that of the DTML model, which could be one reason for the phenomenon that SMTL outperforms
DMTL in our experiments.
3.3	Lite SMTL
The SMTL model is computationally efficient during both the training and inference stages, but it
requires to keep all the private encoders as well as the public encoder at the inference stage. In
some applications with limited storage resource such as edge computing, we hope to eliminate some
private encoders that have small contributions to the corresponding tasks to reduce the storage cost.
To achieve that, we propose a lite version of SMTL called L-SMTL. Specifically, the L-SMTL model
will enforce each gate to choose either the public encoder fS or the corresponding private encoder,
which is equivalent to forcing each αt tobe either 1 or 0. Thus, as shown in the right figure of Figure
1, at the inference stage, the unchosen private encoders can be thrown away and we do not need to
store parameters in them. However, the objective function of the L-SMTL model, which is similar
to problem (2) by replacing the constraint αt ∈ [0, 1] with αt ∈ {0, 1}, is non-differentiable as each
αt is binary valued.
To optimize the non-differentiable objective function in L-SMTL, we adopt the Gumbel-softmax
trick (Jang et al., 2017; Maddison et al., 2017). Specifically, if task t chooses the public encoder
4
Under review as a conference paper at ICLR 2022
with the probability at, it can be cast as sampling αt from a Bernoulli distribution B(pt,ι) With
probability pt,ι = at to assign αt the value of 1 (i.e., using the public encoder for task t) and with
probability pt,o = 1 - at to assign at the value of 0 (i.e., using the private encoder for task t).
However, since the sampling process is still non-differentiable, the Gumbel-softmax trick is used to
reparameterize ah We first use an equivalent formulation for sampling &t based on the Gumbel-Max
trick (Gumbel, 1948) as
αt = argmaxk∈{o,i}(bk + log pt,k),	(3)
where bk (k = 0, 1) is drawn from a Gumbel distribution Gumbel(0, 1) independently (i.e., bk =
- log(- log(u)) where u is sampled from a uniform distribution U(0, 1)). Then we use the softmax
function to approximate the arg max function and define
α =	exP((bi +logpt,I)∕τ)	(4)
t = Pk∈{0,i} exp((bk +logpt,0)∕τ),	( )
where T is a temperature parameter. It is easy to show that a = at when T → 0 and at = ɪ when
T → ∞. Thus, we use a small value of T to make a sharp distribution for ah In implementations
as shown in (Jang et al., 2017), the arg max function in Eq. (3) is used in the forward pass and the
softmax function in Eq. (4) is used in the backward pass to approximate true gradients.
3.4	Variant of SMTL and L-SMTL
To study the impact of the position of gates in SMTL, we introduce a variant of SMTL called
SMTLc, where all the gates are placed after all the decoders. Similarly, the SMTLc model also has
a lite version called L-SMTLc . An illustration for the SMTLc and L-SMTLc models is shown in
Figure 2 in Appendix C.
Specifically, the SMTLc model can be divided into five parts, including a public encoder fS shared
by all the m tasks, m public decoders {hS,t}tm=1 for the m tasks, m private encoders {ft }tm=1 for
the m tasks, m private decoders {ht}tm=1 for the m tasks, and m gates {gt }tm=1 for the m tasks.
For task t, a data point x is fed into the public encoder fS and the public decoder hS,t to get an
output oS, and it is also fed into the private encoder ft and the private decoder ht to get another
output ot . Then the gate gt will adaptively combine the two outputs to obtain the final output, i.e.,
y^t = atos + (1 - a/。，
Similar to the SMTL model, when task t is unrelated to other tasks, ideally the SMTLc model can
only choose the private encoder ft and the private decoder ht with a zero at . Hence, the SMTLc
model can achieve safe multi-task learning when the negative sharing occurs. Moreover, when
0 < at < 1, the combination of the public component consisting of the public encoder and decoder
and the private component consisting of the private encoder and decoder may act in a way similar
to ensemble learning, which may help improve the generalization performance. Moreover, similar
to the SMTL model, the training loss of SMTLc could be lower than those of the STL and DMTL
models. Built on the SMTLc model, the L-SMTLc model approximately learns binary-valued {at}
via the Gumbel-softmax trick.
3.5	Analysis
We analyze the generalization bound for the SMTL method. We consider a general case that different
tasks can have different data distributions. The probability measure for the data distribution in task
t is denoted by μt and the data in all the tasks take the form of (X, Y)〜Qm=ι (μt)n, where X =
(Xi,..., Xm), Xt = (x1,..∙, Xn) and Y denotes the label of X. Here we consider the encoders
fι,...,fm,fs : X → RP as mapping functions and define 夕(x) = (fι(x), ...,fm(x),fs (x)):
X → RP(m+i). The functions 夕 and ht are assumed to be chosen from hypothesis classes F and
H, respectively. Note that we only use ft and fS for the representation in task t. For the ease of
analysis, we define a weight vector βt ∈ Mt for task t, where Mt = {βt ∈ R+m+1 | βt,t + βt,m+1 =
1, βt,i = 0 if i 6= t and i 6= m + 1}, and βt,i represents the ith entry of βt. Thus, for given at, the
corresponding βt satisfies βt,t = 1 - at and βt,m+1 = at. Therefore, the risk of task t in SMTL can
be written as the expectation E(χ,y)~μ∕Lt(y, ht(βT夕(x))]. We denote by E the average expected
risk of all the tasks. Then, the minimal risk is defined as
E *
ht∈H,βmiMt,r∈FE = ht∈H,βmiMt“Fm XE(χ,y)-μtLt(y,ht(βtW(X))).
5
Under review as a conference paper at ICLR 2022
Then we obtain a generalization bound for the SMTL method with the proof in Appendix A.
Theorem 1 (Generalization bound). Assume that Lt(∙, ∙) ∈ [0,1] for t = 1,...,m is 1 -Lipschitz
w.r.t the second argument, and the function 夕 in F is M-Lipschitz continuous. Thenfor (X, Y)〜
∏m=ι(μt)n, with probability at least 1 一 δ, we have
E-E* ≤
CiMG(F(X))+ mi%∈F㈤ G(H(Zy) ∣ 2QC sup^∈F k夕(X)Il + /8ln(4∕δ)
mn	n√m	V mn
(5)
where Ci and C? are two constants, F(X) = {(夕(Xt)):夕 ∈ F}, ∣∣ ∙ k denotes the '2 norm,
H0 = {z ∈ RmnP(m+i) → ht(βτz) : ht ∈H,βt ∈ Mt}, G(∙) denotes the Gaussian average, Qt
is defined as
1n
Qt = SuP	τ-ZTj-E SuP fγi(h(βτZi)- h(βτZi)),
Z = Z∈RnP(m + 1)kz - Zk h∈H,β∈Mt M
Q = max1≤t≤m Qt, andγ is a vector of independent standard normal variables.
In the generalization bound (5), the first term of the right-hand side can be regarded as the cost of
estimating the feature map 夕，the second term corresponds to the cost of estimating task-specific
functions βt and ht, and the third term defines the confidence of the bound. The convergence rate of
this bound is O(√=), which is as tight as typical generalization bounds for MTL such as (Maurer
et al., 2016). Moreover, in Appendix B, we discuss some necessary condition for the optimal αt
being 0 or 1.
4 Experiments
4.1 Experimental Setup
Experiments are conducted on four MTL benchmark datasets, including CityScapes (Cordts et al.,
2016), NYUv2 (Silberman et al., 2012), PASCAL-Context (Mottaghi et al., 2014), and Taskonomy
(Zamir et al., 2018). Detailed introductions of the four datasets are put in Appendix D.1.
The baseline methods in comparison include the Single-Task Learning (STL) that trains each task
separately, the DMTL model which adopts the multi-head hard sharing architecture, Cross-stitch
network (Misra et al., 2016), MTAN (Liu et al., 2019), NDDR-CNN (Gao et al., 2019), AdaShare
(Sun et al., 2020), and AFA (Cui et al., 2021). For fair comparison, we use the same backbone for
all the models in comparison.
For each task in the benchmark datasets, we use one or more evaluation metrics to thoroughly
evaluate the performance. The detailed introduction of each evaluation metric is put in Ap-
pendix D.3. To better show the comparison between each method and STL, we report the rel-
ative performance of each method over STL in terms of the jth evaluation metric on task t as
∆t,j = (-1)pt,j (Mtj - STLtj), where for a method M, Mt,j denotes its performance in terms
of the jth evaluation metric for task t, STLt,j is defined similarly, pt,j equals 1 ifa lower value rep-
resents a better performance in terms of the jth metric in task t and 0 otherwise. So positive relative
performance indicates better performance than STL, which is shown in green in the following tables,
while worse performance corresponding to negative relative performance is shown in red. The over-
all relative improvement of a method M over STL is defined as ∆I
1 ∖Pm	1 Plmt	∆t,j
m t=t=l mt 2-^j=1 STLt,j ,
where mt denotes the number of evaluation metrics in task t. Moreover, to measure the safeness
of each method, the estimation η^ of η in the definition of η-partially safe multi-task learning (i.e.,
Definition 3) for a method M is computed as η = 煮 Pm=I ml； PmtI δ(∆t,j) X 100, where δ(x) is
a step function that outputs 0 when x < 0 and otherwise 1.
We use the Deeplab-ResNet (Chen et al., 2017) with atrous convolutions, a popular architecture for
pixel-wise prediction tasks, as encoders and the ASPP architecture (Chen et al., 2017) as decoders.
We adopt the ResNet-50 for the CityScapes and NYUv2 datasets to implement the the Deeplab-
ResNet, and use the smaller ResNet-18 for the larger PASCAL-Context and Taskonomy datasets for
training efficiency. We use the cross-entropy loss for the semantic segmentation, human parts seg-
mentation and saliency estimation tasks, the cosine similarity loss for the surface normal prediction
task, and the L1 loss for other tasks. For optimization, we use the Adam method (Kingma & Ba,
2014) with the learning rate as 10-4. All the experiments are conducted on Tesla V100 GPUs.
6
Under review as a conference paper at ICLR 2022
4.2 Experimental Results
Table 1: Performance of various models on the
Tables 1-4 show the performance of all the
models in comparison on the four benchmark
datasets. On the CityScapes dataset, the pro-
posed SMTL, L-SMTL, L-SMTLc, and some
baseline methods (i.e., Cross-stitch, MTAN,
and NDDR-CNN) all achieve safe multi-task
learning (i.e., η = 100), which indicates that
their performance is better than that of the STL
model in all tasks. In addition, the proposed
SMTL model achieves the best overall relative
improvement ∆I, which demonstrates its effec-
tiveness. On the NYUv2 and PASCAL-Context
datasets, none of the baselines can achieve safe
multi-task learning, but the proposed meth-
ods (i.e., SMTL, SMTLc, and L-SMTLc) can
CityScaPes validation dataset. ↑ (1) indicates the
higher (lower) the result, the better the perfor-
mance. The green color indicates that the cor-
resPonding method Performs better than the STL
method and the red color indicates oppositely.
Method	_SegmentatiOn_________Depth_____	∆i ↑ η ↑
mIoU ↑ PiXACC ↑ AbSErr J RelErrJ
STL	67.48	91.00	0.0139	46.2507
DMTL
Cross-stitch
MTAN
NDDR-CNN
AdaShare
AFA
SMTL
L-SMTL
SMTLc
L-SMTLc
-0.08
+0.53
+1.49
+0.54
+0.68
+1.44
+1.50
+1.62
+1.18
-0.08
+0.29
+0.59
+0.25
+0.20
+0.52
+0.62
+0.51
+0.63
+0.38
-0.0003
+0.0004
+0.0003
+0.0002
-0.0005
-0.0019
+0.8245
+ 1.8261
+2.4999
+ 1.3845
-20.651
-0.9136
+0.0005	+3.3886
+0.0001	+3.2747
+0.0008	-1.9361
+0.0008	+1.5060
-0.0014	25
+0.0198	100
+0.0260	100
+0.0138	100
-0.1175	50
-0.0323	50
+0.0346	100
+0.0252	100
+0.0117	75
+0.0279	100
achieve that, which again shows the effectiveness of the proposed methods. Though the proposed
L-SMTL method does not achieve safe multi-task learning on these two datasets, it still achieves a
better η than baseline methods on the NYUv2 dataset and a comparable η on the PASCAL-Context
dataset. On the PASCAL-ConteXt dataset, the overall relative improvement of all the baselines are
negative, while all the proposed methods achieves positive ∆I ’s, which shows the superiority of
the proposed methods. On the Taskonomy dataset, only the Cross-stitch network and the proposed
SMTL, SMTLc, and L-SMTLc methods can achieve safe multi-task learning. According to results
shown in Table 4, we can see that the AFA method achieves the best ∆I because it has the largest
improvements on the keypoint detection and edge detection tasks, but it does not achieve safe multi-
task learning, while the proposed methods can achieve that.
For the proposed methods, we can see that the lite versions (i.e., L-SMTL and L-SMTLc) per-
form comparable with SMTL and SMTLc, respectively, which suggests that the elimination strategy
works well on the four datasets. By comparing SMTL with SMTLc, it seems that the performance
is not so sensitive to the two positions of the gates, and similar observations hold for the L-SMTL
and L-SMTLc methods. Moreover, the proposed SMTL and L-SMTLc methods can achieve safe
multi-task learning on all the four datasets, while the proposed L-SMTL and SMTLc methods fail
to achieve this. This observation may suggest the SMTL and L-SMTLc methods are more preferred
than others.
Table 2: Performance of various models on the NYUv2 validation dataset. ↑ (J) indicates the higher
(lower) the result, the better the performance. The green color indicates that the corresponding
method performs better than the STL method and the red color indicates oppositely.
Method	Segmentation	Depth	Surface Normal			∆I ↑	η
	mIoU ↑ Pix Acc ↑	Abs Err J	Rel ErrJ	Angle Distance J Mean Median	Within t°↑			
				11.25	22.5	30		
STL	53.11	75.20	0.3957	0.1632	22.26	15.49	38.61	64.43	74.69		
DMTL	+0.52		+0.16		+0.0104		+0.0032		-1.34		-1.57		-3.31		-3.42		-2.69	-0.0127	67
Cross-stitch	+0.43		+0.09		+0.0082		+0.0052		-0.31		-0.52		-0.82		-0.93		-0.77	+0.0041	67
MTAN	+ 1.03		+0.82		+0.0176		+0.0083		-0.51		-0.90		-1.80		-1.58		-1.16	+0.0098	67
NDDR-CNN	+0.73		+0.03		+0.0086		+0.0072		-0.34		-0.58		-0.94		-1.00		-0.77	+0.0065	67
AdaShare	-7.72		-5.42		-0.0508		-0.0213		-2.26		-2.08		-4.38		-4.40		-3.99	-0.1107	0
AFA	-1.57		-1.29	一	-0.0073	一	-0.0060		-1.97		-1.91		-3.54		-4.21		-3.73	-0.0449	0
SMTL	+0.16		+0.28		+0.0071		+0.0042		+0.26		+0.00		+0.03		+0.46		+0.53	+0.0102	100
L-SMTL	+0.12		+0.10		+0.0078		+0.0035		+0.39		-0.06	一	-0.25	一	+0.55		+0.67	+0.0091	85
SMTLc	+0.09		+0.02		+0.0091		+0.0045		+0.26		+0.10		+0.30		+0.62		+0.59	+0.0117	100
L-SMTLc	+0.46		+0.21		+0.0132		+0.0081		+0.51		+0.28		+0.83		+1.09		+0.90	+0.0218	100
4.3 ANALYSIS ON LEARNED {αt}
We record the learned {αt} of the proposed models in Table 5. According to results for the SMTL
and SMTLc methods, we can see that some αt ’s are closed to 0.5, which means in those cases, the
public encoder and the private encoder are both important to the entire model. Thus, only using
the public encoder (i.e., DMTL) and only using the private encoder (i.e., STL) cannot achieve good
7
Under review as a conference paper at ICLR 2022
Table 3: Performance of various models on the PASCAL-Context validation dataset. ↑ Q) indi-
cates the higher (lower) the result, the better the performance. The green color indicates that the
CorresPonding method performs better than the STL method and the red color indicates oppositely.
Method	Segmentation Human Parts	Saliency	Surface Normal Angle Distance	Within t°	Z ↑ η ↑ mIoU↑	mIoU↑	mIoU↑ maxF↑ 	≡		 Mean J RMSE J 11.25 ↑	22.5 ↑	30↑
STL	65.14	58.58			65.02		77.47	15.94			24.87		48.42		80.79	90.03			
DMTL	-0.37		-0.67		-0.92		-0.51		-1.73		-1.29		-6.43		-4.86		-3.02	-0.0262	0
Cross-stitch	-0.17		+0.05		-0.56		-0.40		-0.62		-0.45		-2.36		-2.68		-1.04	-0.0096	25
MTAN	-0.58		+0.50		-0.45		-0.23		-1.20		-0.89		-4.58		-3.31		-2.04	-0.0147	25
NDDR-CNN	+0.14		+0.60		+0.07		+0.00		-0.37		-0.24		-1.50		-0.94		-0.59	-0.0008	75
AdaShare	-12.7		-7.30		-3.65		-2.50		-1.68		-1.23		-6.46		-4.66		-2.83	-0.1017	0
AFA	+2.12		+2.11		-1.95		-3.96		-1.63		-1.28		-5.68		-4.42		-2.88	-0.0108	50
SMTL	+0.01		+1.05		+0.20		+0.13		+0.26		+0.22		+1.08		+0.74		+0.39	+0.0082	100
L-SMTL	+0.29		+1.39		-0.80		-0.16		+0.34		+0.40		+0.95		+1.18		+0.81	+0.0093	75
SMTLc	+0.88		+1.43		+0.22	一	+0.12	一	+0.36		+0.32		+1.32		+1.08		+0.61	+0.0142	100
L-SMTLc	+0.23		+0.82		+0.61		+0.57		+0.53		+0.38		+2.43		+1.36		+0.65	+0.0126	100
Table 4: Performance of various models on the Taskonomy validation dataset. ↑ (1) indicates the
higher (lower) the result, the better the performance. The green color indicates that the correspond-
ing method performs better than the STL method and the red color indicates oppositely.
Method
Segmentation	Depth	Keypoints Edges	Surface Normal
mIoU ↑ PixAcc ↑ AbsErr J	Rel Err；	AbsErr J AbsErr J AngIe DiStanCe ^______________Within t°↑_________
Mean Median 11.25	22.5	30
∆i ↑	η ↑
STL	65.42	97.63	0.0072	0.0117	0.1103	0.1349	10.39	4.19	73.67	86.21	90.52
DMTL
Cross-stitch
MTAN
NDDR-CNN
AdaShare
AFA
SMTL
L-SMTL
SMTLc
L-SMTLc
-0.74
+5.87
+6.34
+6.64
+4.90
+5.84
+4.99
+6.24
+4.75
-0.07
+0.67
+0.69
+0.69
+0.50
+0.57
+0.58
+0.53
+0.68
+0.54
-0.0026
-0.0011
-0.0045
-0.0016
+0.0003
-0.0006
+0.0001
+0.0005
-0.0043
+0.0009
-0.0019
-0.0028
-0.0073
-0.0027
+0.0006
-0.0011
+0.0002
+0.0009
-0.0022
+0.0100
-0.0248
+0.0133
+0.0087
+0.0465
+0.0158
+0.0129
+0.0171
+0.0173
-0.0025
+0.0019
-0.0111
+0.0043
-0.0001
+0.0524
+0.0045
+0.0026
+0.0047
+0.0040
-1.25
+ 1.50
+0.99
+ 1.86
-1.17
+ 1.08
+ 1.64
+ 1.87
+ 1.70
+2.02
-0.88
-0.18
-1.93
-0.17
+0.29
+0.19
+0.28
+0.42
-3.43
+4.21
+2.30
+4.78
-3.93
+3.00
+4.62
+5.02
+4.74
+5.38
-1.93
+3.81
+2.89
+4.18
-0.37
+3.27
+3.95
+4.49
+4.14
+4.62
-1.44
+3.00
+3.32
-0.12
+2.68
+3.06
+3.57
+3.25
+3.67
-0.0983
+0.0580
-0.0767
+0.0378
-0.1265
+0.1331
+0.0676
+0.0321
+0.0665
+0.0782
0
100
36
90
40
76
100
80
100
100
Table 5: {αt} learned on four MTL datasets. ‘SS’ stands for the semantic segmentation task, ‘DE’
denotes the depth estimation task, ‘SNP’ is for the surface normal prediction task, ‘HPS’ corresponds
to the human parts segmentation task, ‘SE’ stands for the saliency estimation task, ‘KD’ stands for
the keypoint detection task, and 'ED' denotes the edge detection task.
Method I CityScapes ∣	NYUv2	∣	PASCAL-Context	∣	Taskonomy
	SS	DE	SS	DE	SNP	SS	HPS	SE	SNP	SE	DE	KD	ED	SNP
SMTL	0.5002	0.4960	0.4383	0.5188	I 0.1997 I	0.4739	0.5529	0.3701	I 0.2304 I	0.4886	0.4565	0.4504	0.4578	I 0.2584 I
L-SMTL	0.4782	0.4823	0.4475	0.4402	0.3745	0.4982	0.5142	0.4964	0.4277	0.4749	0.4472	0.4079	0.4068	0.4135
SMTLc	0.4896	0.4891	0.3779	0.5320	0.4299	0.4619	0.8729	0.3823	0.4652	0.3988	0.5163	0.4952	0.5208	0.4336
L-SMTLc	0.4972	0.4995	0.5155	0.5242	0.3836	0.5427	0.5826	0.4964	0.3757	0.4878	0.4669	0.3959	0.4102	0.3784
performance, while the proposed models can take the advantages of these two methods to achieve
better performance in most cases. Moreover, some of the learned αt ’s have relatively small values
(i.e., values smaller than 0.3), which are shown in box in Table 5. These small values indicate that
for the surface normal prediction task on the NYUv2, PASCAL-Context, and Taskonomy datasets,
the public encoder is relatively unimportant, which may imply that the surface normal prediction
task is not strongly related to other tasks on these datasets. On the other hand, this observation
may explain why DMTL is much worse than STL and why the proposed SMTL method has good
performance on these datasets (refer to Tables 2-4).
For the L-SMTL and L-SMTLc methods, when the learned αt is larger than 0.5, the corresponding
task t will choose to use the public component and otherwise choose the corresponding private
component at the inference stage. According to Table 5, we can see that in some cases, all the
tasks will choose private components on some datasets (i.e., both methods on the CityScapes and
Taskonomy datasets, and L-SMTL on the NYUv2 dataset) and other cases are mixed in that some
tasks choose the public component and other tasks choose private components. Interestingly, for
both methods, the surface normal prediction task chooses to use private components on the NYUv2,
PASCAL-Context, and Taskonomy datasets, which corresponds to the small values for αt in the
SMTL method.
8
Under review as a conference paper at ICLR 2022
4.4 Experiments on Bi-Level Formulation
In this section, we use a bi-level formulation for the SMTL model to study the effects of different
formulations. Specifically, the original training dataset is divided into two parts, including a training
set with ntr data points and a validation set with nval data points. The training set is used to learn
parameters in the public encoder, m private encoders, and m private decoders, which corresponds to
the lower-level subproblem in the following problem (6). Parameters in the m gates are viewed as
hyperparameters and the validation set is used to learn them, which is corresponding to the upper-
level subproblem in problem (6). The objective function of SMTL under the bi-level formulation is
formulated as
1
min------
{gt} mnval
nval m
XX
Lt(yi,h(gt(fS(Xi),ft (Xi))))
s.t.fS, {ft}, {h：}
1
arg min ---------
fS,{ft},{ht} mntr
ntr m
XX
Lt(y7t,ht(gt(fs (Xi),ft(Xi)))),
=1 t=1
(6)
where Xi denotes the ith datapoint in the training set, yi denotes the corresponding label of Xi in task
t, and Xi as well as yi is defined similarly in the validation set. Then We conduct experiments on the
CityScapes and NYUv2 datasets to compare the performance of SMTL under different formulations.
According to experimental results shown in Table 6, we can see that, on the CityScapes dataset,
the learned {αt} by the bi-level formulation (i.e., problem (6)) is similar to that by the single-
level formulation (i.e., problem (2)), thus the performance has no much difference. However, on
the NYUv2 dataset, SMTL with the bi-level formulation learns a large αt for the surface normal
prediction task, which makes its performance inferior to SMTL with the single-level formulation
in some tasks (i.e., depth estimation and surface normal prediction). One reason is that the surface
normal prediction task is not so strongly related to other tasks that learning them together may
impair not only its own performance but also the performance of other tasks. Moreover, it is well
known that the complexity of solving a bi-level optimization problem is much higher than that of
solving the corresponding single-level optimization problem and so experiments on larger datasets
(i.e., PASCAL-Context and Taskonomy) based on problem (6) are too computational demanding to
be conducted. Hence, the single-level formulation of the SMTL model (i.e., problem (2)) is preferred
as it is both effective and efficient. For other variants of SMTL, we have similar observations so that
we do not report the results for them.
Table 6: Performance and learned αt of SMTL on the CityScapes and NYUv2 validation datasets,
where values in the normal font correspond to the performance of SMTL through the single-level
formulation (i.e., problem (2)) and values in the italic font are those through the bi-level formulation
(i.e., problem (6)). ↑ (J) indicates the higher (lower) the result, the better the performance.
Dataset
Segmentation	Depth	Surface Normal
mIoU ↑ Pix Acc ↑ Abs Err J	Rel ErrJ AngIe DiStance J WithIn t°↑
Mean Median 11.25	22.5	30
CityScapes	Performace Learned αt	68.98	91.62 68.98	91.63 0.5002 0.7019	0.0134	42.8621 0.0136	43.4661 0.4960 0.5327	-	-	--- -	-	--- - -
NYUv2	Performace	53.27	75.48 53.40	75.40	0.3886	0.1590 0.4088	0.1612	22.00	15.49	38.64 64.89 75.22 22.76	16.47	36.63 62.62	73.43
	Learned αt	0.4383 0.6288	0.5188 0.5468	0.1997 0.5825
5 Conclusion
In this paper, to study the problem of safe multi-task learning, we propose a simple and effective
SMTL method that can automatically learn to combine encoders via a gating mechanism. To reduce
the storage cost, we design lite SMTL by learning a binary gate. Furthermore, we study to place
the gates after the decoders. Extensive evaluations demonstrate the effectiveness of the proposed
methods. In the future work, we are interested in identifying the location for the gates via neural
architecture search.
9
Under review as a conference paper at ICLR 2022
Ethics S tatement
We have read the Code of Ethics and ensure that this work follows it. No human subject is involved in
this work and all the datasets used in experiments are publicly available such that they do not contain
any personally identifiable information or offensive content. Hence, this work has no potentially
negative societal impact.
Reproducibility S tatement
For theoretic results, assumptions used have been fully stated and the complete proof is included in
the Appendix. We include all the code and instructions in the supplementary material so that this
work can be reproduced. The implementation details are provided in Section 4.1 and Appendix.
References
Peter L Bartlett and Shahar Mendelson. Rademacher and gaussian complexities: Risk bounds and
structural results. Journal of Machine Learning Research, 3(Nov):463-482, 2002.
R. Caruana. Multitask learning. Machine Learning, 28(1):41-75,1997.
Liang-Chieh Chen, George Papandreou, Iasonas Kokkinos, Kevin Murphy, and Alan L Yuille.
Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and
fully connected crfs. IEEE transactions on pattern analysis and machine intelligence, 40(4):
834-848, 2017.
Marius Cordts, Mohamed Omran, Sebastian Ramos, Timo Rehfeld, Markus Enzweiler, Rodrigo
Benenson, Uwe Franke, Stefan Roth, and Bernt Schiele. The cityscapes dataset for semantic urban
scene understanding. In Proceedings of the IEEE conference on computer vision and pattern
recognition, pp. 3213-3223, 2016.
Chaoran Cui, Zhen Shen, Jin Huang, Meng Chen, Mingliang Xu, Meng Wang, and Yilong Yin.
Adaptive feature aggregation in deep multi-task convolutional neural networks. IEEE Transac-
tions on Circuits and Systems for Video Technology, 2021.
Theodoros Evgeniou and Massimiliano Pontil. Regularized multi-task learning. In Proceedings of
the tenth ACM SIGKDD international conference on Knowledge discovery and data mining, pp.
109-117, 2004.
Yuan Gao, Jiayi Ma, Mingbo Zhao, Wei Liu, and Alan L Yuille. Nddr-cnn: Layerwise feature
fusing in multi-task cnns by neural discriminative dimensionality reduction. In Proceedings of
the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 3205-3214, 2019.
Emil Julius Gumbel. Statistical theory of extreme values and some practical applications: a series
of lectures, volume 33. US Government Printing Office, 1948.
Pengsheng Guo, Chen-Yu Lee, and Daniel Ulbricht. Learning to branch for multi-task learning. In
International Conference on Machine Learning, pp. 3854-3863. PMLR, 2020.
Pengxin Guo, Chang Deng, Linjie Xu, Xiaonan Huang, and Yu Zhang. Deep multi-task augmented
feature learning via hierarchical graph neural network. In Joint European Conference on Machine
Learning and Knowledge Discovery in Databases, pp. 538-553. Springer, 2021.
Eric Jang, Shixiang Gu, and Ben Poole. Categorical reparameterization with gumbel-softmax. In
Proceedings of the 5th International Conference on Learning Representations, 2017.
Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint
arXiv:1412.6980, 2014.
Abhishek Kumar and Hal Daume III. Learning task grouping and overlap in multi-task learning.
arXiv preprint arXiv:1206.6417, 2012.
10
Under review as a conference paper at ICLR 2022
Michel Ledoux and Michel Talagrand. Probability in Banach Spaces: isoperimetry and processes.
Springer Science & Business Media, 2013.
Giwoong Lee, Eunho Yang, and Sung Hwang. Asymmetric multi-task learning based on task relat-
edness and loss. In International conference on machine learning, pp. 230-238. PMLR, 2016.
Shikun Liu, Edward Johns, and Andrew J Davison. End-to-end multi-task learning with attention.
In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp.
1871-1880, 2019.
Wu Liu, Tao Mei, Yongdong Zhang, Cherry Che, and Jiebo Luo. Multi-task deep visual-semantic
embedding for video thumbnail selection. In Proceedings of IEEE Conference on Computer
Vision and Pattern Recognition, pp. 3707-3715, 2015.
Mingsheng Long and Jianmin Wang. Learning multiple tasks with deep relationship networks. arXiv
preprint arXiv:1506.02117, 2(1), 2015.
Chris J. Maddison, Andriy Mnih, and Yee Whye Teh. The concrete distribution: A continuous
relaxation of discrete random variables. In Proceedings of the 5th International Conference on
Learning Representations, 2017.
Kevis-Kokitsi Maninis, Ilija Radosavovic, and Iasonas Kokkinos. Attentive single-tasking of multi-
ple tasks. In Proceedings of IEEE Conference on Computer Vision and Pattern Recognition, pp.
1851-1860, 2019.
Andreas Maurer. A chain rule for the expected suprema of gaussian processes. Theoretical Computer
Science, 650:109-122, 2016.
Andreas Maurer, Massimiliano Pontil, and Bernardino Romera-Paredes. The benefit of multitask
representation learning. Journal of Machine Learning Research, 17(81):1-32, 2016.
Ishan Misra, Abhinav Shrivastava, Abhinav Gupta, and Martial Hebert. Cross-stitch networks for
multi-task learning. In Proceedings of the IEEE conference on computer vision and pattern recog-
nition, pp. 3994-4003, 2016.
Roozbeh Mottaghi, Xianjie Chen, Xiaobai Liu, Nam-Gyu Cho, Seong-Whan Lee, Sanja Fidler,
Raquel Urtasun, and Alan L. Yuille. The role of context for object detection and semantic seg-
mentation in the wild. In Proceedings of IEEE Conference on Computer Vision and Pattern
Recognition, pp. 891-898, 2014.
Sebastian Ruder, Joachim Bingel, Isabelle Augenstein, and Anders S0gaard. Latent multi-task ar-
chitecture learning. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 33,
pp. 4822-4829, 2019.
Nathan Silberman, Derek Hoiem, Pushmeet Kohli, and Rob Fergus. Indoor segmentation and sup-
port inference from rgbd images. In European conference on computer vision, pp. 746-760.
Springer, 2012.
Trevor Standley, Amir Zamir, Dawn Chen, Leonidas Guibas, Jitendra Malik, and Silvio Savarese.
Which tasks should be learned together in multi-task learning? In International Conference on
Machine Learning, pp. 9120-9132. PMLR, 2020.
Ximeng Sun, Rameswar Panda, Rogerio Feris, and Kate Saenko. Adashare: Learning What to share
for efficient deep multi-task learning. In Proceedings of the 33rd Advances in Neural Information
Processing Systems, 2020.
Zirui Wang, Zihang Dai, BarnabaS Poczos, and Jaime Carbonell. Characterizing and avoiding neg-
ative transfer. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern
Recognition, pp. 11293-11302, 2019.
Qiang Yang, Yu Zhang, Wenyuan Dai, and Sinno Jialin Pan. Transfer learning. Cambridge Univer-
sity Press, 2020.
11
Under review as a conference paper at ICLR 2022
Amir R Zamir, Alexander Sax, William Shen, Leonidas J Guibas, Jitendra Malik, and Silvio
Savarese. Taskonomy: Disentangling task transfer learning. In Proceedings of the IEEE con-
ference on computer vision and pattern recognition, pp. 3712-3722, 2018.
Yi Zhang, Yu Zhang, and Wei Wang. Multi-task learning via generalized tensor trace norm. In
Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery & Data Mining, pp.
2254-2262, 2021.
Yu Zhang and Qiang Yang. A survey on multi-task learning. IEEE Transactions on Knowledge and
Data Engineering, 2021.
Yu Zhang and Dit-Yan Yeung. A convex formulation for learning task relationships in multi-task
learning. In Proceedings of the Twenty-Sixth Conference on Uncertainty in Artificial Intelligence,
pp. 733-742, 2010.
Zhanpeng Zhang, Ping Luo, Chen Change Loy, and Xiaoou Tang. Facial landmark detection by
deep multi-task learning. In Proceedings of the 13th European Conference on Computer Vision,
pp. 94-108, 2014.
12
Under review as a conference paper at ICLR 2022
Appendix
A Proof of Theorem 1
In this section, we analyze the generalization bound for the SMTL method.
To analyze the generalization bound for SMTL, we introduce a useful theorem in terms of Gaussian
averages (Bartlett & Mendelson, 2002; Maurer et al., 2016).
Theorem 2. Let G be a class of functions Ψ : X → [0, 1]t, and μι,…,μm be the probability
measure on X with XX = (Xi,…，Xm)〜 Qm=ι(μt)n where Xt = (x1,...,Xn). Let Z be the
random set {(Ψt(xit)) : Ψ ∈ G} and γ be a vector of independent standard normal variables. Then
for all Ψ ∈ G, with probability at least 1 一 δ in X, we have
-x (Ex。",")] -1 x Ψt(χi)) ≤√2πG≡+r 9⅞(2≡,
m	t	n	mn	2mn
where G(Z) = E[supz∈Z hγ, zi] is the Gaussian average of the set Z.
Based on Theorem 2, in the following section, we give the proof of Theorem 1.
Proof. By Theorem 2, with probability at least 1 一 δ in (X, Y) ~ Qm=ι(μt)n, for all ht ∈ H,
at ∈ Mt and 夕 ∈ F, we have
「	1 L 一 一 √2 , 八、、	√2∏G(S)	∕9(ln(2∕δ))
E- m X Lt(yi, ht(βT 以Xt))) ≤ ~mG~)+ 9n^，	⑺
ti
where S = {(Lt(yt, ht(βT以Xt)))) ： ht ∈ Hg ∈ Mt,夕 ∈ F} and G(S) is the Gaussian average
of the set S. Then by the Lipschitz property of Lt and Slepian’s Lemma (Ledoux & Talagrand,
2013), we have G(S) ≤ G(S0), where S0 = {(ht(βT中(Xt)》 ： ht ∈Hg ∈ MtN ∈ F}.
Note that the input data X ∈ Xmn, and hence F(X) ⊆ RmnP(m+i) is defined as F(X)=
{(夕(xi)) : φ ∈ F}. We define a class of functions H0 = {z ∈ RmnP(m+i) → ht(βTz):
ht ∈ H,βt ∈ Mt}. Then we have S0 = H(F(X)). By using Theorem 2 in (Maurer, 20i6), we
obtain the following inequality
G(S0) ≤ CILm)G(F(X)) + C2D(F(X))Q(H0)+ min G(H(Zy)
z∈F (XX)
where C1 and C2 are two constants, L(H0) represent the Lipschitz constant of the functions in H0 ,


D(F(x)) = 2supφ∈F k夕(X)k denotes the Euclidean diameter of the set F(X),and
Q(H0)
sup
z≠5∈Rm'
Let z,Z ∈ RmnP(m+1), where Z =
nP (m+1)
F⅛E ψUHohγ,ψ(Z)- @(Z)i.
Then for any functions ψ ∈ H0, we have
(Zi) with Zi ∈ RP(m+1) and Z =(泊 with Zi ∈ RP(m+1).
E sup hγ,ψ⑶一Ψ(Z)i = E sup	E <Yti,ht(βTZi)- ht(βTZqt)
ψ∈H0	ht ∈H,βt ∈Mt	ti
mn
= X E sup	X γi(h(βtTZti) - h(βtTZZti))
t=1 h∈H,βt ∈Mt i=1
2
≤
mn
X (E h∈H黑Mt X Mh 喘 Zi)-h(eT 4))
m
n
1/2
≤
≤
√m £QmaXE kZi- Zik2
t=1	i=1
√mQmaxkz - zk,
13
Under review as a conference paper at ICLR 2022
where QmaX = maxι≤t≤m Qt. Therefore, Q(H0) ≤ √mQmaχ. Moreover, We have
kψ(z) - ψ(z)k2 = £(3(eT zi) - ht(βτ Zi))
ti
≤ M2 X kβTzi - βTZk2 ≤ M2kβTk2kz - Zk2
ti
where the first inequality is due to the Lipschitz property and the second inequality is due to the
Cauchy-Schwarz inequality. Note that kβt k ≤ 1, hence L(H0) ≤ M. Then, we have
G(S) ≤ G(S0) ≤ CIMG(F(X))+2C2√mQmaχ SuP IInX)I∣ + min G(H0(z)).	(8)
W∈F	z∈F (XX)
Let 夕*, h；,…,hm, β;,…,βm be the minimizer in E*, then we have
E-E * =	e- m X Lt(yi, MeT O(Xt)))	+	m1n X Lt(yi,优(β”(χt))) -e *
ti	ti
+	m1nX Lt(yi …以Xt)))-m X LtMmKeMW))),	⑼
ti	ti
where the first term can be bounded by substituting inequality (8) into (7) and the second term
can be regarded as mn random variables Lt(yti, ht*(βt*TO* (Xit))) with values in [0, 1]. By using
Hoeffding’s inequality, with probability at least 1 - δ, we have
ɪ X LtIy ,h*(β*4*(xt))) -E* ≤ l∕ln^δ).
mn	2mn
ti
The last term is non-positive due to the definition of minimizers. Therefore, we have
E-E* ≤
CIMG(F(X)) + minz∈F(XX) G(H(Zy) + 2QC sup°∈F kθ(X)k + /8ln(4∕δ)
mn	n√m	V mn
□
B NECESSARY CONDITION FOR OPTIMAL αt
We analyze the conditions that model parameters except {αt} satisfy when the optimal αt equals 0
or 1 for each task. By taking the cross-entropy loss as an example, we have following result.
Theorem 3. When Lt is the cross-entropy loss of task t that is formulated as Lt =
一n1 ∑n=ι(yt)τ log ht(atfs(Xi) + (1 一 αt)ft(Xi)) where yii denotes the one-hot label vector, if
αt* is a local minimum of Lt over αt, then we have
n
X
i=1
(yi)Tft(χi)(fs(Xi) - ft(χi))
ht(ft(Xi)
≤ 0,
if a： =0,
n
X
i=1
(yi)Tfs(χi)(fs(Xi) - ft(χi))
ht(fs (Xi))
≥ 0,
if at = 1,
Proof. Fix αt and define the function Ht() = Lt(αt* + (αt - αt*)), which is continuously dif-
ferentiable in an open interval containing [0, 1]. By using the chain rule to differentiate Ht, we
have
L^1- Lt(α* + e(at - α*)) -Lt(α*) dHt(0) dLt(α*)，	*、
0 ≤ lim----------------------------------=   =  ----------------(at — α≠ )
一 e→0	C	de	∂at	I t t'
where the inequality follows from the assumption that αt* is a local minimum.
If a* = 0, then at 一 a* ≥ 0. To satisfy "喝?) (at 一 a*) ≥ 0, we have "喝了) ≥ 0.
If a* = 1, then at 一 a* ≤ 0. To satisfy "dθ^ (at 一 a*) ≥ 0, we have JLdob) ≤。.
14
Under review as a conference paper at ICLR 2022
Based on the formulation of Lt , we can compute
∂Lt(α"
-n Pi=ι
(yi)T(αfs(xi) + (1-α: )ft(xi))(fs(xi)-ft(xi))
ht(αtf (Xi) +(I-α3ft (Xi)))
∂αt
as
∂Lt(α"
∂αt
Then we have
n
X
i=1
(Vi)Tft(XixfS(Xi) - ft(Xi))
ht(ft(Xiy)
≤ 0,
n
X
i=1
(yi)Tfs(Xi)(JS(Xi) - ft(Xiy
ht(fs (Xi))
≥ 0,
if	at = 0,
if at = 1,
in which we reach the conclusion.
□
Theorem 3 provide a necessary condition for the optimal at being 0 or 1. Such analysis can easily
be extended to other loss functions such as the square loss, and we omit it due to similar proofs.
C Illustration of the SMTLc and L-SMTLc Models
Similar to the SMTL and L-SMTL models, an illustration of the SMTLc and L-SMTLc models is
shown in Figure 2.
Figure 2: Illustration of the SMTLc and L-SMTLc models. Left figure: Pipeline for the SMTLc
model, which is identical to the training phase of L-SMTLc . For task t, X is first fed into the public
encoder fS and the public decoder hS,t to get an output oS, and it is also fed into the private encoder
ft and the private decoder ht to get another output ot . Then it is through the gate to obtain the
combined output, i.e., y = ag + (1 - at)ot. The number of tasks is set to three for illustration.
Right figure: Test phase for L-SMTLc . After finishing the training process of L-SMTLc, gt can
choose which component (i.e., the public component fS and hS,t or the private component ft and
ht) is used for each task. In this way, at the test process, only the chosen components need to be
saved, which could reduce the number of parameters and speedup the inference. In this illustration,
task 1 and task 3 choose the public component, while task 2 goes through the private component.
D	Experimental Setup
D. 1 Details of Datasets
CityScapes. The CityScapes dataset (Cordts et al., 2016) consists of high resolution outside street-
view images, which contains 2, 975 images for training and 500 images for validation. This dataset
contains 19 classes for pixel-wise semantic segmentation, together with ground-truth inverse depth
labels. By following (Liu et al., 2019), we evaluate the performance on the 7-class semantic seg-
mentation and depth estimation tasks. All training and validation images are resized to 128 × 256.
15
Under review as a conference paper at ICLR 2022
NYUv2. The NYUv2 dataset (Silberman et al., 2012) consisting of RGB-D indoor scene images
has 795 images for training and 654 images for validation. We evaluate the performance on three
learning tasks: 13-class semantic segmentation, depth estimation, and surface normal prediction. By
following (Liu et al., 2019), all the training and validation images were resized to 288 × 384.
PASCAL-Context. The PASCAL-Context dataset (Mottaghi et al., 2014) is an annotation extension
of the PASCAL VOC 2010 challenge and it contains 4, 998 images for training and 5, 105 images
for validation. We evaluate the performance on four learning tasks: 21-class semantic segmentation,
7-class human parts segmentation, saliency estimation, and surface normal estimation, where the
last two tasks are generated by (Maninis et al., 2019).
Taskonomy. The Taskonomy dataset (Zamir et al., 2018) which contains over 4.5 million indoor
images from over 5, 000 buildings with 26 tasks. By following (Standley et al., 2020), we sample
five learning tasks, including 17-class semantic segmentation, depth estimation, keypoint detec-
tion, edge detection, and surface normal prediction. Furthermore, we select 5 building images (i.e.,
“allensville”, “collierville”, “mifflinburg”, “noxapater”, and “onaga”) from the standard tiny bench-
mark as our dataset, which contains 13, 286 images for training and 3, 794 images for validation.
D.2 Settings of Batch Size
The settings of the batch size for all the models on different datasets are shown in Table 7.
Table 7: Settings of batch size for all the models on the four datasets.											
Dataset	STL	DMTL	Cross-stitch	MTAN	NDDR-CNN	AdaShare	AFA	SMTL	L-SMTL	SMTLc	L-SMTLc
CityScapes	180	180	100	80	80	120	150	70	70	70	70
NYUv2	8	4	4	4	4	4	4	4	4	4	4
PASCAL-Context	40	40	24	20	18	32	8	18	18	15	15
Taskonomy	230	230	120	130	100	180	40	100	100	90	90
D.3 Evaluation Metrics
On the PASCAL-Context dataset, by following (Maninis et al., 2019), the semantic segmentation is
evaluated by the mean Intersection over Union (mIoU) and on the other three datasets, by following
(Sun et al., 2020), this task is additionally evaluated by the Pixel Accuracy (Pix Acc). For the depth
estimation task, the absolute error (Abs Err) and relative error (Rel Err) are used as the evaluation
metrics. For the surface normal prediction task, the mean and median angle distances between the
prediction and ground truth of all pixels are used as measures. For this task, the percentage of pixels,
whose prediction is within the angles of 11.25°, 22.5°, and 30°to the ground truth, is used as another
measure. For the keypoint detection and edge detection tasks, the absolute error (Abs Err) is used
as the evaluation metric. For the human parts segmentation task, the mIoU is used as the measure.
For the saliency estimation task, the mIoU and max F-measure (maxF) are adopted as the evaluation
metrics.
E	Comparis on of Training Time
The training time per epoch for all the models on the CityScapes dataset is recorded in Table 8.
According to the results, the training time of the proposed SMTL and L-SMTL methods is compa-
rable with all the baseline methods, implying that the proposed SMTL and L-SMTL methods are as
efficient as baseline methods. The training time of the proposed SMTLc and L-SMTLc methods is
a bit longer due to additional public decoders introduced.
Table 8: Training time per epoch for all the models on the CityScaPeS dataset.
DMTL^^CrOSS-Stitch MTAN_NDDR-CNN_AdaShare_AFA_SMTL_L-SMTL_SMTLc_L-SMTLc
Time (S)	122	150	153	175	141	58	175	175	272	272
16