Under review as a conference paper at ICLR 2022
Provab le Regret Bounds
for Deep Online Learning and Control
Anonymous authors
Paper under double-blind review
Ab stract
The use of deep neural networks has been highly successful in reinforcement
learning and control, although few theoretical guarantees for deep learning ex-
ist for these problems. There are two main challenges for deriving performance
guarantees: a) control has state information and thus is inherently online and b)
deep networks are non-convex predictors for which online learning cannot provide
provable guarantees in general.
Building on the linearization technique for overparameterized neural networks,
we derive provable regret bounds for efficient online learning with deep neural
networks. Specifically, we show that over any sequence of convex loss functions,
any low-regret algorithm can be adapted to optimize the parameters of a neural
network such that it competes with the best net in hindsight. As an application of
these results in the online setting, we obtain provable bounds for online episodic
control with deep neural network controllers.
1	Introduction
In many machine learning problems, the environment cannot be represented by a distribution. One
reason for the inadequacy of this modeling assumption is the nonstochastic nature of the environ-
ment. For example, in control for dynamical systems and reinforcement learning, the environment
has a temporal state that is affected by feedback. Likewise in the setting of spam filtering, spam
emails are generated adversarially to bypass email filters. Another example is the problem of port-
folio selection, where the stock market behavior is governed by multiple players and is thus non-
stochastic.
The accepted framework to study learning in nonstochastic environments is online learning in games.
Since the environment can change arbitrarily, there is no fixed, a priori optimal decision. Instead, the
notion of generalization is replaced by the game-theoretic concept of regret: the difference between
the overall performance of an algorithm and that of the best fixed decision in hindsight. Efficient
algorithms for online learning are based on Online Convex Optimization (OCO), which is restricted
to convex predictors and losses. As a result, the framework cannot be readily applied when the
learners are deep neural networks - the driving force behind many breakthroughs in modern machine
learning. It is therefore desirable to extend OCO to bridge this gap.
In this paper, we bring recent ideas from deep learning theory to enable online learning with neural
networks, and apply this method to control. We derive efficient algorithms, by reduction from any
OCO algorithm, that attain provable regret bounds using deep neural networks. These bounds apply
to the full online learning setting with any convex decision set and loss functions. Moreover, they
are agnostic, which means that they show competitive performance to the best neural network in
hindsight without assuming it achieves zero loss.
An interesting conclusion from this reduction is that provable bounds for training deep neural net-
works can be derived from any OCO method, beyond online (stochastic) gradient descent. This in-
cludes mirror descent, adaptive gradient methods, follow-the-perturbed leader and other algorithms.
Previously convergence and generalization analyses for neural networks were done in isolation for
different optimization algorithms (Wu et al., 2019; Cai et al., 2019; Wu et al., 2019; 2021).
We apply this general online deep learning method to obtain provable regret guarantees for control
of dynamical systems with deep neural networks. Provable regret bounds in this domain have thus
1
Under review as a conference paper at ICLR 2022
far been limited to linear dynamics and/or linear controllers. However, most dynamical systems in
the physical world are nonlinear.
In order to go beyond linear control, we consider the emerging paradigm of online nonstochastic
control: a methodology for control that is robust to adversarial noise in the dynamics. The important
aspect of this paradigm to our study is that it uses a convex reparametrization of policies. Therefore,
our extension of OCO to deep neural networks naturally leads to regret bounds for deep neural
network controllers in this setting.
Our contributions can be summarized as follows:
•	We give a general reduction from any online convex optimization algorithm to online deep
learning. The regret bounds obtain depend on the original regret for online convex opti-
mization, the width of the network, and the diameter of neural network parameters over
which we optimize. The precise statements are given in Theorems 3.1, 3.2.
•	These regret bounds imply generalization in the statistical setting, and go beyond SGD:
any online convex optimization algorithm can be shown to generalize over deep neural
networks according to its regret bound in the OCO framework. This includes commonly
used algorithms such as Adagrad/Adam, as well as more recent regularization functions for
mirror descent (Ghai et al., 2020).
•	We apply this reduction to the framework of online nonstochastic control, and obtain prov-
able regret bounds for deep controllers in the episodic setting. These can be used to derive
iterative linear control algorithms, as well as regret for online single trajectory control.
1.1	Related work
Online learning and online convex optimization. The framework of learning in games has been
extensively studied as a model for learning in adversarial and nonstochastic environments (Cesa-
Bianchi & Lugosi, 2006). Online learning was infused with algorithmic techniques from mathemat-
ical optimization into the setting of online convex optimization, see (Hazan, 2019) for a comprehen-
sive introduction.
The emerging theory of deep learning. For detailed background on the developing theory for
deep learning, see the book draft (Arora et al., 2021). Among the various studies on the theory of
deep learning, the neural tangent kernel or linearization approach has emerged as the most general
and pervasive. This technique shows that neural networks behave similar to their linearization and
prove that gradient descent converges to a global minimizer of the training loss (Soltanolkotabi
et al., 2018; Du et al., 2018a;b; Allen-Zhu et al., 2019; Zou et al., 2018; Jacot et al., 2018; Bai &
Lee, 2019). Techniques related to this have been expanded to provide generalization error bounds
in the i.i.d. statistical setting (Arora et al., 2019; Wei et al., 2019), and generalization bounds for
SGD (Cao & Gu, 2019; Ji & Telgarsky, 2020).
Online-to-batch linearization. The linearization technique has been combined with online learn-
ing and online-to-batch conversion to yield generalization bounds for SGD Cao & Gu (2019). In the
adversarial training setting, the online gradient proof technique was used in Gao et al. (2019); Zhang
et al. (2020) to handle non i.i.d. functions. In relationship to these works, our reduction takes in a
general OCO algorithm, rather than only OGD/SGD. In addition, we generalize this reduction to the
full OCO model, including high dimensional output predictors and general convex costs to enable
the application to control.
Online and nonstochastic control. Our study focuses on algorithms which enjoy sublinear re-
gret for online control of dynamical systems; that is, whose performance tracks a given bench-
mark of policies up to a term which is vanishing relative to the problem horizon. Abbasi-Yadkori
& Szepesvari (2011) initiated the study of online control under the regret benchmark for Linear
Time-invariant (LTI) dynamical systems. Our work instead adopts the nonstochastic control setting
(Agarwal et al., 2019), that allows for adversarially chosen (i.e. non-Gaussian) noise and costs that
may vary with time. The nonstochastic control model was recently extended to consider nonlinear
and time-varying dynamics in (Gradu et al., 2020). See Hazan & Singh (2021) for a comprehensive
survey of results and advances in online and nonstochastic control. Similar to our setting, online
2
Under review as a conference paper at ICLR 2022
episodic control is also studied in Kakade et al. (2020), but the regret definition differs from ours,
and the system is not linear.
Nonlinear systems and deep neural network based controls. Nonlinear control is computation-
ally intractable in general (Blondel & Tsitsiklis, 2000). One approach to deal with the computational
difficulty is iterative linearization, which takes the local linear approximation via the gradient of the
nonlinear dynamics. One can apply techniques from optimal control to solve the resulting changing
linear system. Iterative planning methods such as iLQR (Tassa et al., 2012), iLC (Moore, 2012)
and iLQG (Todorov & Li, 2005) fall into this category. Neural networks were also used to directly
control the dynamical system since the 90s, for example in Lewis et al. (1997). More recently, deep
neural networks were used in applications of a variety of control problems, including Lillicrap et al.
(2016) and Levine et al. (2016). A critical study of neural-network based controllers vs. linear
controllers appears in Rajeswaran et al. (2017)
2 Preliminaries
Notation. Let〈•，•〉denote the element-wise inner product between two vectors, matrices, or ten-
sors of the same dimension: hx, yi = vec(x)>vec(y). Let Sp = {x ∈ Rp : kxk2 = 1} denote the
unit sphere, and for a convex set K, let QK denote projection onto K.
2.1	Online Convex Optimization
In Online Convex Optimization (OCO) a decision maker sequentially chooses a point in a convex
set θt ∈ K ⊆ Rd, and suffers loss lt (θt) according to a convex loss function lt : K 7→ R. The goal
of the learner is to minimize her regret, defined as
TT
RegretT = X It(Ot)- θm∈κ χ It (θ*).
t=1	t=1
A host of techniques from mathematical optimization are applicable to this setting and give rise to
efficient low-regret algorithms. To name a few methods, Newton’s method, mirror descent, Frank-
Wolfe and follow-the-perturbed leader all have online analogues, see e.g. Hazan (2019) for a com-
prehensive treatment.
2.2	Deep neural networks and the neural tangent kernel (NTK)
Deep Neural Networks. Let x ∈ Rp be the p-dimensional input. We define the depth H network
with ReLU activation and scalar output as follows:
x0 = Ax
xh = σrelu(θhxh-1), h ∈ [H]
f(θ, x) = a>xH,
where σ1∙elu(∙) is the ReLU function σrelu(z) = max(0,z), A ∈ Rm×p, θh ∈ Rm×m, and a ∈
Rm. Let θ = (θ1, . . . , θH)> ∈ RH×m×m denote the trainable parameters of the network and the
parameters A, a are fixed after initialization. The initialization scheme is as follows: each entry in
A and θh is drawn i.i.d. from the Gaussian distribution N(0,m),and each entry in a is drawn i.i.d.
from N(0, 1).
For vector-valued outputs, we consider a scalar output network for each coordinate. Suppose for
i ∈ [d], fi is a deep neural network with a scalar output; with a slight abuse of notation, for input
x ∈ Rp, denote
f (θ; x) = (f1(θ[1]; x),..., fd(θ[d]; x))> ∈Rd,	(2.1)
where θ[i] ∈ RH×m×m denotes the trainable parameters for the network fi for coordinate i. Let
θ = (θ[1], θ[2], . . . , θ[d]) ∈ Rd×H×m×m denote all the parameters for f.
In the online setting, the neural net receives an input xt ∈ Rp at each round t ∈ [T], and with
parameter θ suffers loss `t(f (θ; xt)). Note that this framework generalizes the supervised learning
3
Under review as a conference paper at ICLR 2022
paradigm: with data points {(χt,yt)}t the losses 't(∙) = '(∙,yt) reduce the setting to supervised
learning. We make the following standard assumptions on the inputs and the loss functions:
Assumption 1. The input x has unit norm, i.e. x ∈ Sp, kxk2 = 1.
Assumption 2. The loss functions `t(f (θ; x)) are L-Lipschitz and convex in f(θ; x).
Two-layer Neural Networks. For analysis simplicity we also consider the special case of two-
layer network architecture. For inputs x ∈ Rp , define the coordinate-wise two-layer neural network
f : Rp → Rd with a smooth activation function σ : R → R, even hidden layer width m and
weights θ ∈ Rd×m×p expressed as follows: for all i ∈ [d] with parameter θ[i] ∈ Rm×p, f(θ; x) =
(f1(θ[1]; x), . . . , fd(θ[d]; x))> ∈ Rd, where
m/2	m/2
fi(θ[i]; X) = b (X ai,rσ(θ[i,r]>x) + X a/σ(8[i,r]>x)).	(2.2)
r=1	r=1
In the above expression, θ[i,r]> denotes the r-th row of θ[i], and θ[i, r] denotes the r + mm -th row of
θ[i], such that θ[i] = (θ[i, 1],...,θ[i, mm],"i, 1],..., θ[i, mm]). We include a scaling factor b ∈ R to
demonstrate how its value affects the convergence and generalization properties of the network, and
in Section 3 we study the tradeoff between the two properties and choose an optimal b. We initialize
ai,r to be randomly drawn from {±1}, choose 出产 = -ay, and fix them throughout training. The
initialization scheme for θ is as follows: for all i ∈ [d], θι[i, r]〜 N(0, Id) for r = 1,..., mm, and
& [i,r] = θι[i,r]. This symmetric initialization scheme is chosen so that fi(θι[i]; x) = 0 for all
x ∈ Sp. We make the following assumption on the general activation function:
Assumption 3. The activation function σ is C—smooth and C—Lipschitz: ∣σ0(z) 一 σ0(z0)∣ ≤ C|z —
z0∣, ∣σ0(z)∣≤ C.
The above architectures, initialization schemes, and assumptions are consistent with Gao et al.
(2019).
Neural Tangent Kernel. The Neural Tangent Kernel (NTK) was first introduced in Jacot et al.
(2018), who showed that in the infinite width limit, a randomly initialized deep neural network
trained with gradient descent is equivalent to a kernel method with the NTK kernel:
R7	0、	Ip	r/df (θ; X) df (θ; XO) ∖1
K(x,x ) = Eθ〜DK	∂θ , 一∂θ一M
where D denotes the initialization distribution ofθ. Since a deep neural network contains a two-layer
neural network, for quantitative results on the relationship between network overparameterization
and its expressivity, we focus on the two-layer network (2.2) for simplicity. We present the explicit
form of the NTK for our two-layer neural network in the following definition.
Definition 2.1. The NTK for the scalar two-layer neural network with activation σ and initialization
distribution W 〜N(0, Ip) is defined as Kσ(x, y) = Ew〜N(o,ip)hxσ0(w>x), yσ0(w>y)i.
Let H(Kσ) denote the RKHS of the NTK. Intuitively, H(Kσ) represents the space of functions that
can be approximated by a neural network with kernel Kσ. Depending on the choice of σ, H(Kσ)
can contain meaningful classes of functions; for example, H(Kσrelu) contains all even functions with
bounded derivatives (Bietti & Mairal, 2019). Since our goal is to obtain nonasymptotic approxima-
tion guarantee, we focus on RKHS functions of bounded norm. Following Gao et al. (2019), we
define the closely related Random Feature (RF) space of functions, its norm and restrict to functions
of bounded RF-norm.
Definition 2.2 ((Gao et al., 2019)). Consider functions of the form
h(X) =
Rd
c(w)>Xσ0(w>X)dw.
Define the RF-norm of h as k h∣∣ RF = SuPw kp(ww)2, where po(w) is the probability density function
ofN(0, Ip). Let
FRF (D)
{h(X) =
Rd
c(w)>Xσ0(w>X)dw : khkRF
≤ D},
and extend to the multi-dimensional case, FRd F (D) = {h = (h1, h2, . . . , hd) : hi ∈ FRF (D)}.
4
Under review as a conference paper at ICLR 2022
By Lemma C.1 in Gao et al. (2019), FRF(∞) is dense in H(K) With respect to the ∣∣ ∙ ∣∣∞,s norm,
where khk∞,S = supx∈Sp |h(x)|. Since we are concerned with the approximation of the function
value over the unit sphere, it is sufficient to consider FRdF(∞), and further restrict to FRdF(D) for
explicit nonasymptotic guarantees.
2.3 Online Episodic Control
Consider the folloWing online episodic learning problem for nonstochastic control over linear time-
varying (LTV) dynamics: there is a sequence of T control problems each With a horizon K and an
initial state x1 ∈ Rdx . In each episode, the state transition is given by
∀k ∈ [1, K],	xk+1 = Akxk + Bkuk + wk ,	(2.3)
Where xk ∈ Rdx , uk ∈ Rdu. The system matrices Ak ∈ Rdx ×dx , Bk ∈ Rdx×du along With the next
state xk+1 are revealed to the learner after taking the action uk . The disturbances wk ∈ Rdx are
unknoWn and adversarial but can be a posteriori computed by the learner wk = xk+1-Akxk-Bkuk.
An episode loss is defined cumulatively over the rounds k ∈ [1, K] according to the cost functions
ck : Rdx × Rdu → R of state and action, i.e. for a policy π
K
J(π; x1 , c1:K) =	ck(xkπ, ukπ).
k=1
The transition matrices (Ak, Bk)1:K, initial state x1, disturbances w1:K and costs c1:K can change
arbitrarily for different episodes. The goal of the learner is to minimize episodic regret by adapting
its output policies πt for t ∈ [1, T],
TT
RegretT = £ Jt(∏t; x1,c1：K) - min V Jtg 21，'1：K),	(2.4)
π∈Π
t=1	t=1
Where Π denotes the class of policies the learner competes against.
We make the folloWing basic assumptions about the dynamical system in each episode that are com-
mon in the nonstochastic control literature : the disturbances are bounded, the system is sequentially
stable1, and the cost functions are Well-behaved for efficient optimization.
Assumption 4. All disturbances have a uniform bound on their norms: maxk∈[K] ∣wk ∣2 ≤ W.
Assumption 5. There exist C1, C2 ≥ 1 and 0 < ρ1 < 1 such that the system matrices satisfy:
∀k ∈ [K], n ∈ [1, k),
k-n+1
Y	Ai
i=k
≤ Ci ∙ ρn,
∣Bk ∣op ≤ C2 .
op
Assumption 6. Each cost function ck : Rdx × Rdu → R is jointly convex and satisfies
∣∣Vck(x,y)k ≤ Lc max{1, ∣x∣ + ∣∣u∣∣} for some Lc > 0.
Policy Class. The performance of the learner given by (2.4) directly depends on the policy class
Π. In this Work, We focus on disturbance based policies, i.e. policies that take past perturbations as
input uk = f(wi:k-i). These policies are parameterized W.r.t. policy-independent inputs, in this
case the sequence wi:K. This is in contrast to the commonly used state feedback policy uk = f(xk).
In particular, the DAC policy class (AgarWal et al., 2019) (as Well as DRC (SimchoWitz et al., 2020))
that outputs controls linear in past finite disturbances has a convex parameterization (AgarWal et al.,
2019). Recent Works have devised efficient online methods With provable guarantees for these policy
classes both for LTI and LTV systems in the single trajectory setting. We expand the comparator
class by considering policies With controls that are nonlinear in the past disturbances, represented
by a neural netWork.
Definition 2.3. (Disturbance Neural Feedback Control) A disturbance neural feedback policy πdθnn
chooses control uk output by a neural network over the past disturbances,
uk = fθ(wk-i, wk-2, . . . ,wi),
where fθ(∙) is a neural network with parameters θ.
1Extension to the stabilizable case can be found in appendix.
5
Under review as a conference paper at ICLR 2022
The reasoning behind this policy class expansion is twofold. First, for general LTI systems, the best
in hindsight DAC policy is not close to the optimal open-loop control sequence given adversarial
disturbances wk and general convex costs ck . Furthermore, our episodic setting can be used for
trajectory-based first-order policy optimization over nonlinear dynamics (Ahn et al., 2007). Hence,
competing against the rich policy class of neural network controllers is highly desirable. For a given
neural network architecture let fθ (∙) = f (θ; ∙), and let Θ be the set of permissible parameters θ. The
class of deep controller policies is then given by Πdnn (f; Θ) = {πdθnn : θ ∈ Θ}.
3 Online Learning with Neural Networks
In this section, we describe the general framework of online learning with neural networks and
derive accompanying regret guarantees. Our framework can use any OCO algorithm as a black-box,
and we focus on Online Gradient Descent (OGD) in our main algorithm below, since it is widely
used in practice. Observe that the update is equivalent to OGD on the original losses {'t}.
Algorithm 1 OGD for Neural Networks
1:	Input: step size ηt > 0, initial parameters θ1, decision set B(R).
2:	for t = 1 . . . T do
3:	Play θt, receive loss 't(θ) = 't(f (θ; Xt)). Construct ht(θ) = 't(θt) + V¾(θt)>(θ - θt).
4:	Update θt+ι = QB(R)(θt-ηNht(θt)) = QB(R)(θt - ηtV't(θt)).
5:	end for
In the following theorem, we give an end-to-end bound on the performance of our algorithm com-
pared to the best-in-hindsight function in FRdF (D) for two-layer neural networks. The regret bound
consists of two parts: the regret for learning the optimal neural network parameters in a ball around
initialization (Section 3.2), and the approximation error of neural networks to the target function in
FRdF (D) (Section 3.3).
Theorem 3.1.	Let f be a two-layer neural network as in (2.2) with scaling factor b = √m and
decision set B(R) = {θ ∈ Rd×m×p : ∣∣θ — Θi∣∣f ≤ R} ,and suppose Assumptions 1, 2, 3 are
satisfied. For any δ > 0,D > 1, let R = D√d, then with probability at least 1 — δ over the random
initialization, Algorithm 1 with η = CL√m ∙ t-1/2 satisfies
XX't(f(θt;Xt)) ≤	min	XX't(g(xt)) + O (L√d√cD2τ) + O fcLD√dT + CLDyT),
£	9∈FRf (D) t=1	V	√m	√ V	√m )
where O(∙) hidesfactors that are polylogarithmic in δ, d.
Note that the radius R of the permissible set for the parameters B(R) is constant with respect to
the network width m when the number of parameters initialized as N (0, Id) is linear in m. This
indicates very little movement allowed in the parameter space, consistent with the recent insights in
deep learning theory. The above bound is more conveniently stated in terms of average regret.
Corollary 3.1. Under the conditions of Theorem 3.1, the average regret is bounded by any ε > 0
11
T Regre t = T
T
X 't(f(θt; Xt))-
t=1
min
g∈FRd F (D)
T
X't(g(χt)) ≤O
t=1
+ f ≤ ε
for large values ofnetwork width m = Ω(ε-2) and large number of iterations T = Ω(ε-2).
A similar analysis applies even beyond the simple two-layer networks. In particular, we also derive
regret bounds for learning with deep neural networks.
Theorem 3.2.	Let f be a deep neural network with ReLU activation defined as in (2.1), and suppose
Assumptions 1 and 2 are satisfied. Let B(R) = {θ : ∣θ[i] - θ1 [i]∣F ≤ R ∀ i ∈ [d]}. Take
R =O(H-3/2m-3/2), thenfor m ≥ max{d, H 3}, with probability at least 1 —O(H )e-Q(p Iog m)
6
Under review as a conference paper at ICLR 2022
over the random initialization θι, Algorithm 1 with η = hH√mt-1/2 has regret bound
X `t(f" Xt)) ≤ θm% X 'W; m+0 (l√t+L√Hmτ
where 0(∙) hides termsPolylogarithmic in m.
Similar to the case of two-layer neural networks, the radius R of the decision set is small, as it scales
inversely with m. Moreover, ifT and m are large, we can achieve small average regret. In particular,
with m = Ω(ε-2) and T = Ω(-ε2), the average regret is bounded by ε.
3.1	Online Nearly Convex Optimization
We first prove regret bounds for nearly convex functions, a slight extension to the OCO framework.
As we show in the next sections, these regret bounds naturally carry over to the setting of online
learning over neural networks.
Definition 3.1. A function ` : Rn → R is ε-nearly convex over the convex, compact set K ⊂ Rn if
and only if
∀x,y ∈ K, '(x) ≥ '(y) + V'(y)>(x - y) - ε .	(3.1)
The analysis of any algorithm for OCO, including the most fundamental method OGD, extends to
this case in a straightforward manner. Let A be any algorithm for OCO that accepts a sequence of
convex losses {ht} by an adaptive adversary, and returns a decision sequence {θt}t∈[T] ⊆ K with
the following regret guarantee,
TT
X't(θt) - m∈K X't(θ*) ≤ RegretT(A).
Then the given OCO algorithm A can be applied to online nearly-convex optimization as per algo-
rithm 2 to obtain a regret bound.
Algorithm 2 Online Nearly-Convex Optimization
1:	Input: OCO algorithm A for convex decision set K.
2:	for t = 1 . . . T do
3:	Play θt, observe't. Construct ht(θ) = 't(θt) + V¾(θt)>(θ - θt).
4:	Update θt+1 = A(h1,..., ht) ∈ K.
5:	end for
Lemma 3.1. Suppose '1, . . . , 'T are ε-nearly-convex, then Algorithm 2 attains the following regret:
TT
X 't(θt)- θmnκ X 't(θ*) ≤ RegretT(A)+εT.
Much of the literature in the theory of deep learning has focused on the analysis of stochastic gradient
descent (SGD) and gradient descent (GD). However, this general reduction allows for results over
any OCO algorithm with sublinear regret, such as mirror descent and adaptive gradient methods
(AdaGrad and further enhancements) eliminating the need to devise isolate analyses for separate
algorithms. For simplicity, we consider OGD for the rest of the paper and state the corresponding
regret bound below.
Corollary 3.2. Suppose {'t}tT=1 are ε-nearly convex and letAbe OGD, then Algorithm 2 has regret
T	T
X't(θt) - min * X't(θ*) ≤ 3RG√T + εT,
where G is the gradient norm upper bound for all 't, t ∈ [T], and R is the radius of K.
7
Under review as a conference paper at ICLR 2022
3.2	Online Gradient Descent for Two-layer Neural Networks
We proceed to show in Lemma 3.3 that two-layer neural networks as defined by (2.2) satisfy the
nearly convex property. Consequently, we can obtain regret guarantees in Lemma 3.2 for learning
the class of two-layer neural networks near initialization in an online fashion using OGD. We define
the convex decision set as B(R) = {θ : kθ - θ1 kF ≤ R} where θ1 denotes the value of θ at
initialization. Throughout this section, denote `t (θ) = `t(f (θ; xt)).
Lemma 3.2. Under Assumptions 1, 2, 3, Algorithm 1 with η = CRm ∙ t-1/2 attains regret bound
昌〃，八、	. 区〃，八、3CLR√mT 2CLR2e	cc、
X `t(θt) ≤ θmin⅛) X `t(θ)+—b—+-^T	(3.2)
t=1	t=1
The following lemma quantifies the margin of near-convexity for two-layer neural networks.
Lemma 3.3. Under Assumptions 1, 2, 3, the functions `t (θ) satisfy the near-convexity property (3.1)
with εnc = 2CLR2 for θ ∈ B(R).
3.3	Expressivity of Two-layer Neural Networks
Lemma 3.2 shows that we can efficiently (online) learn over the class of two-layer neural networks
near initialization, but how well does the best neural network in that class perform? Recall the
function space FRdF (D) from Definition 2.2. In the results below, we show that the class of two-
layer neural networks considered {f (θ; ∙) : θ ∈ B(R)} can approximate any function in FRF(D),
and derive nonasymptotic rates on the approximation error in terms of the network width m. The
analysis follows the outline from Gao et al. (2019).
Lemma 3.4. For any δ,D > 0, let g : Rp → Rd ∈ FRF (D), and let R = b√√d ,then with
probability at least 1 一 δ over the random initialization θι, there exists θ* ∈ B(R) such thatfor all
x ∈ Sp,
't(f(θ*; x)) ≤ 't(g(χ)) + Lb√dcD2 + l√√cd(2p2p + 2Pi0gd∕δ).
2m	m
3.4	Extending to Deep Neural Networks
Beyond two-layer neural networks, the nearly-convex property holds for deep neural networks as
well. We show this result for networks with ReLU activation, where the gradient itself may be
large but changes very slowly. This phenomenon is studied in Allen-Zhu et al. (2019) on finitely
many points, and Gao et al. (2019) further extends the result to hold for inputs over the unit sphere.
In the lemma below, we adapt the result in Gao et al. (2019) to hold for our deep neural network
architecture as in (2.1). Throughout this section, we use B(R) = {θ ： ∣∣θ[i]-θι[i]∣∣F ≤ R ∀ i ∈ [d]}.
Lemma 3.5. For m = Ω(plogR2R)+log d), and R = O(于 晶 m),with probability at least 1 一
O(H)e-Q(mR2/3H) over the random initialization θι, for any θ, θ0 ∈ B(R) and any X ∈ S, under
Assumption 2,
't(f(θ0; x)) 一 't(f(θ; X)) ≥ hVθ't(f(θ; x)),θ0 一 θi — O(R4/3H5/2Pmlog m)L√d,	(3.3)
kVθ[i"i(θ[i]; x)kF ≤ O(Hjm) ∀ ∈ [d].	(3.4)
4 Online Episodic Control with Neural Network Controllers
The online episodic control problem described in Section 2.3 with the policy class Π = Πdnn(f; Θ)
can be reduced to online learning for neural networks (Section 3) where f is defined as in (2.1) with
a permissible parameter set Θ. For simplicity, we temporarily drop the index t ∈ [T] of a single
episode and denote the 0-padded network input Zk = vec([wk-1,...,w1,0,..., 0]) ∈ RK∙dx. To
8
Under review as a conference paper at ICLR 2022
satisfy Assumption 1, normalize the input Zk = jzk^ ∈ Sκ∙dx. The controls of a policy ∏d∩∩(θ)
parameterized by θ ∈ Θ are given by Uk = f (θ; Zk) for all k ∈ [K]. The episode loss equals
K
L(θ) = J (πdnn(θ); x1,ck) = X ck(xθk, f(θ; zZk)) .
k=1
Note that the episode loss L depends on the parameter θ through all the K controls f(θ, zZk), k ∈
[K]. Denote fZ(θ) = [f (θ, zZ1), . . . , f(θ, zZK)]> ∈ RK×du and let L(θ) = L(fZ(θ)) by overload of
notation. We demonstrate that the reduction to the online learning setting is achieved by showing that
L(fZ(θ)) satisfies the convexity (Lemma B.1) and Lipschitz (Lemma B.3) conditions. This means
that for each episode t ∈ [T], the episode loss Lt(θ) = Jt(πdnnθ ; xt1, ct1:K) satsifies Assumption 2
over the argument fZ(θ). The rest of the derivation is analogous to that in Section 3 and yields the
theorem below (see Appendix B for the proof). The algorithm itself for online episodic control is
simply OGD over the losses Lt(θ) given in detail in Algorithm 4.
Algorithm 3 Deep Neural Episodic Control with OGD
1:	Input: ηt > 0, initial parameter θι, permissible set Θ.
2:	for t = 1 . . . T do
3:	for k = 1 . . . K do
4:	Observe xtk and play utk = f(θt, zZkt ).
5:	end for
6:	Construct loss function
K
Lt(θ) = Xctk(xtk,θ,f(θ,zZkt).
k=1
7:	Perform gradient update θt+ι = ∏θ(θt - ηt%Lt(θt)).
8:	end for * 5
Let f denote the neural network as in (2.1) and πdθnn the policy class with uθk = f(θ; zZk). Define
Πdnn(f; Θ) = {πdθnn : θ ∈ Θ} with Θ = B(R) as in Theorem 3.2
Theorem 4.1. Given the setting in Section 2.3, suppose the Assumptions 4, 5, 6 hold. Let R, m, H
satisfy the conditions in Theorem 3.2. Then, Algorithm 4 with η = O(L HRzmt-1/2), attains
episodic regret bound given by
D	t t t t 、	∙ GTf t t ~Q (KLc√ duT I κLc√Hduτ∖
Regret T = X Jtg； xi, ci：K)-mi∏^ Jtg xι, ci：K) ≤ O ( √Hm	+	m√m	),
where O(∙) hides termsPolylogarithmic in m.
This theorem statement, analogous to Section 3, can be interpreted as arbitrarily small average regret
ε when the network width m and number of episodes T are both large, i.e. Ω(ε-2).
5 Conclusion and Future Work
In this work, we study online learning with the class of deep neural networks, and apply this general
framework to online episodic control over LTV systems with deep neural network controllers. This
leads to the first provable performance guarantees for neural network based controllers.
Interestingly, our derivation of provable regret bounds for online learning with deep neural networks
can be applied to any OCO algorithm, creating a unifying framework for studying optimization
methods in deep learning. For example, generalization bounds can be obtained by an online-to-
batch reduction.
In terms of control, our results can open a new line of work showing guarantees in different control
settings with the policy class of neural network controllers. Particularly, one can derive provable
bounds for single-trajectory online control or nonlinear control with regret competing against these
policies.
9
Under review as a conference paper at ICLR 2022
References
Yasin Abbasi-Yadkori and Csaba Szepesvari. Regret bounds for the adaptive control of linear
quadratic systems. In Proceedings ofthe 24th Annual Conference on Learning Theory, pp. 1-26,
2011.
Naman Agarwal, Brian Bullins, Elad Hazan, Sham Kakade, and Karan Singh. Online control with
adversarial disturbances. In International Conference on Machine Learning, pp. 111-119, 2019.
Hyo-Sung Ahn, YangQuan Chen, and Kevin L. Moore. Iterative learning control: Brief survey and
categorization. IEEE Transactions on Systems, Man, and Cybernetics, Part C (Applications and
Reviews), 37(6):1099-1121, 2007. doi: 10.1109/TSMCC.2007.905759.
Zeyuan Allen-Zhu, Yuanzhi Li, and Zhao Song. A convergence theory for deep learning via over-
parameterization, 2019.
Raman Arora, Sanjeev Arora, Joan Bruna, Nadav Cohen, Simon Du, Rong Ge, Suriya Gunasekar,
Chi Jin, Jason Lee, Tengyu Ma, and Behnam Neyshabur. Theory of Deep Learning. 2021.
Sanjeev Arora, Simon S Du, Wei Hu, Zhiyuan Li, and Ruosong Wang. Fine-grained analysis of
optimization and generalization for overparameterized two-layer neural networks. arXiv preprint
arXiv:1901.08584, 2019.
Yu Bai and Jason D Lee. Beyond linearization: On quadratic and higher-order approximation of
wide neural networks. arXiv preprint arXiv:1910.01619, 2019.
Alberto Bietti and Julien Mairal. On the Inductive Bias of Neural Tangent Kernels. Curran Asso-
ciates Inc., Red Hook, NY, USA, 2019.
Vincent D Blondel and John N Tsitsiklis. A survey of computational complexity results in systems
and control. Automatica, 36(9):1249-1274, 2000.
Tianle Cai, Ruiqi Gao, Jikai Hou, Siyu Chen, Dong Wang, Di He, Zhihua Zhang, and Liwei Wang.
Gram-gauss-newton method: Learning overparameterized neural networks for regression prob-
lems. arXiv preprint arXiv:1905.11675, 2019.
Yuan Cao and Quanquan Gu. Generalization bounds of stochastic gradient descent for wide and
deep neural networks. Advances in Neural Information Processing Systems, 32:10836-10846,
2019.
Nicolo Cesa-Bianchi and Gabor Lugosi. Prediction, learning, and games. Cambridge university
press, 2006.
Simon S Du, Jason D Lee, Haochuan Li, Liwei Wang, and Xiyu Zhai. Gradient descent finds global
minima of deep neural networks. arXiv preprint arXiv:1811.03804, 2018a.
Simon S Du, Xiyu Zhai, Barnabas Poczos, and Aarti Singh. Gradient descent provably optimizes
over-parameterized neural networks. arXiv preprint arXiv:1810.02054, 2018b.
Ruiqi Gao, Tianle Cai, Haochuan Li, Liwei Wang, Cho-Jui Hsieh, and Jason D. Lee. Convergence
of adversarial training in overparametrized neural networks, 2019.
Udaya Ghai, Elad Hazan, and Yoram Singer. Exponentiated gradient meets gradient descent. In
Algorithmic Learning Theory, pp. 386-407. PMLR, 2020.
Paula Gradu, Elad Hazan, and Edgar Minasyan. Adaptive regret for control of time-varying dynam-
ics. arXiv preprint arXiv:2007.04393, 2020.
Elad Hazan. Introduction to online convex optimization. arXiv preprint arXiv:1909.05207, 2019.
Elad Hazan and Karan Singh. Tutorial: online and non-stochastic control, July 2021.
10
Under review as a conference paper at ICLR 2022
Arthur Jacot, Franck Gabriel, and Clement Hongler. Neural tangent kernel: Convergence and gen-
eralization in neural networks. arXiv preprint arXiv:1806.07572, 2018.
Ziwei Ji and Matus Telgarsky. Polylogarithmic width suffices for gradient descent to achieve arbi-
trarily small test error with shallow relu networks, 2020.
Sham Kakade, Akshay Krishnamurthy, Kendall Lowrey, Motoya Ohnishi, and Wen Sun.
Information theoretic regret bounds for online nonlinear control. In H. Larochelle,
M. Ranzato, R. Hadsell, M. F. Balcan, and H. Lin (eds.), Advances in Neu-
ral Information Processing Systems, volume 33, pp. 15312-15325. Curran Associates,
Inc., 2020. URL https://proceedings.neurips.cc/paper/2020/file/
aee5620fa0432e528275b8668581d9a8-Paper.pdf.
Sergey Levine, Chelsea Finn, Trevor Darrell, and Pieter Abbeel. End-to-end training of deep visuo-
motor policies. The Journal of Machine Learning Research, 17(1):1334-1373, 2016.
F.L. Lewis, S. Jagannathan, and A. Yeyildirek. Chapter 7 - neural network control of robot arms and
nonlinear systems. In Omid Omidvar and David L. Elliott (eds.), Neural Systems for Control, pp.
161-211. Academic Press, San Diego, 1997. ISBN 978-0-12-526430-3. doi: https://doi.org/10.
1016/B978-012526430-3/50008-8.
Timothy P Lillicrap, Jonathan J Hunt, Alexander Pritzel, Nicolas Heess, Tom Erez, Yuval Tassa,
David Silver, and Daan Wierstra. Continuous control with deep reinforcement learning. In ICLR
(Poster), 2016.
Kevin L Moore. Iterative learning control for deterministic systems. Springer Science & Business
Media, 2012.
Aravind Rajeswaran, Kendall Lowrey, Emanuel Todorov, and Sham Kakade. Towards generalization
and simplicity in continuous control. arXiv preprint arXiv:1703.02660, 2017.
Max Simchowitz, Karan Singh, and Elad Hazan. Improper learning for non-stochastic control, 2020.
Mahdi Soltanolkotabi, Adel Javanmard, and Jason D Lee. Theoretical insights into the optimization
landscape of over-parameterized shallow neural networks. IEEE Transactions on Information
Theory, 2018.
Y. Tassa, T. Erez, and E. Todorov. Synthesis and stabilization of complex behaviors through online
trajectory optimization. In 2012 IEEE/RSJ International Conference on Intelligent Robots and
Systems, pp. 4906-4913, 2012.
Emanuel Todorov and Weiwei Li. A generalized iterative lqg method for locally-optimal feedback
control of constrained nonlinear stochastic systems. In Proceedings of the 2005, American Con-
trol Conference, 2005., pp. 300-306. IEEE, 2005.
Colin Wei, Jason Lee, Qiang Liu, and Tengyu Ma. Regularization matters: Generalization and
optimization of neural nets vs their induced kernel. 2019.
Xiaoxia Wu, Simon S Du, and Rachel Ward. Global convergence of adaptive gradient methods for
an over-parameterized neural network. arXiv preprint arXiv:1902.07111, 2019.
Xiaoxia Wu, Yuege Xie, Simon Du, and Rachel Ward. Adaloss: A computationally-efficient and
provably convergent adaptive gradient method. arXiv preprint arXiv:2109.08282, 2021.
Yi Zhang, Orestis Plevrakis, Simon S. Du, Xingguo Li, Zhao Song, and Sanjeev Arora. Over-
parameterized adversarial training: An analysis overcoming the curse of dimensionality, 2020.
Difan Zou, Yuan Cao, Dongruo Zhou, and Quanquan Gu. Stochastic gradient descent optimizes
over-parameterized deep ReLU networks. arXiv preprint arXiv:1811.08888, 2018.
11
Under review as a conference paper at ICLR 2022
A Proofs for Section 3
Proof of Theorem 3.1. Let g ∈ FRdF (D). By Lemma 3.4, with probability at least 1 - δ over the
random initialization θι, there exists θ* ∈ B(R) such that for all X ∈ S,
'tf(θ*;X)) ≤ 't(g(X)) + L黑D + LL√C (2p2p + 2√log d∕δ)
≤…+ O(-).
By the regret guarantee in Lemma 3.2, Algorithm 1 has regret
T
X `t(θt) ≤
t=1
T
min X `t (θ) +
θ∈B(R) t=1
3CLR√mT	2CLR2
b + -b-
工	L	CLR2
θ∈B个R) X 't(θ)+ O(CLR√t + ET).
(A.1)
(A.2)
Combining them and using R = D√d, We conclude
T
X `t(θt) ≤
t=1
工	L	CLR2
min	't(θ) + O(CLRy∕T +-t^T)
θ∈B(R)j	√m
工	L	CLR2
≤ ɪj't(θ*) + O(CLRVT + -√==^T)
T
≤ X't(g(xt)) + O(CLR√T +
t=1
孚 T) + O( l√√CD2 )
mm
The theorem follows by noticing that the inequality holds for any arbitrary g ∈ FRF(D).	□
Proof of Theorem 3.2. By Lemma 3.5, With our choice of mand R, With probability at least
1 - O(H)e-Q(mR2/3H) over the randomness of θι, ' is εnc-nearly convex with εnc =
O(R4/3HE/'mlog mL√d), and ∣∣Vθ[i]fi(θ[i]; X)IIF ≤ O(H√m) for all i ∈ [d],x ∈ S,θ ∈
B(R). Since the decision set is B(R), its radius in Forbenius norm is at most R√d. We can bound
the gradient norm as follows, for all X ∈ S,
d
kVθ't(f (θ; χ))kF = X kVθ[i]'t(fi(θ[i]; χ))kF
i=1
_X ∂'t(f(θ; x)) 2 ||V 小口.、||2
=^ ∂fi(θ[i]; x)	kvθ[i]fi(U)kF
≤ L2 max IVθ[i]fi(θ[i]; X)I2F ≤ O(L2H2 m).
By Corollary 3.2, the regret is bounded by
3R√dG√T + εT ≤ O(RLH√dmT) + O(R4/3H5/2TLPdm log m).
Note that the conditions of Lemma 3.5 are satisfied with the choice of R = O(H-3/2m-3/2).
R2/3H = O(m-1)impliesm = Ω(P Iog(I/^jog d),
R2/3H
m ≥ H3impliesR ≤ O(H-3/2 * H-9/2) = O(H-6log-3 m).
Finally, R2/3Hm = Ω(p log m) and m ≥ H3 implies the probability of the bound is high. □
12
Under review as a conference paper at ICLR 2022
Proof of Lemma 3.1. Observe that by the nearly-convex property, for all θ ∈ K,
ht(θ) — 't(θ) = 't(θt) + V't(θt)>(θ - θt) - 't(θ) ≤ ε.
Moreover, by construction the functions ht(∙) are convex and ht(θt) = 't(θt) for all t ∈ [T]. The
regret can be decomposed as follows, for any θ* ∈ K,
TT
X ('t(θt) - 't(θ*)) ≤ X (ht(θt) - ht(θ*)) + εT ≤ RegretT(A) + εT.
t=1	t=1
Taking θ* ∈ K to be the best decision in hindsight concludes the lemma proof.	□
Proof of Lemma 3.2. The theorem statement is shown by using Corollary 3.2 and showing that the
loss functions `t : Rd×m×p → Rd satisfy near-convexity with respect to θ. First, the decision set
in this case is K = B(R) so its radius is R. Lemma 3.3 shows that the loss functions `t(θ) are
εnc-nearly convex with εnc = 2CLR2. Finally, We can show that the gradient norm is bounded as
follows,
C 二	C 二	∂` (θ)	2	C	C2L2m
kVθ`t(θ)kF = EkVθ[i]'t(θ)kF = E f(θt[(J ) ∙kVθ[fi(θ[i];χt)kF ≤ —2—
i=1	i=1 i	; xt
where we use the L-Lipschitz property of `t(f (θ; x)) and the fact that thefi gradient is bounded
kVθ[i]fi(θ[i]; Xt)IIF ≤ √mC∕b given (A.3). This means that G = CL√m and we can use the
Corollary 3.2 to conclude the final statement in (3.2).	□
Proof of Lemma 3.3. We extend the original proof in Gao et al. (2019). Let diag(ai) be a diagonal
matrix with
(a1,i, . . . , am/2,i, -a1,i, . . . , -am/2,i) on the diagonal. Note that the gradient of the network at the
i-th coordinate is
Vθ[fi(θ[i]; x) = bdiag(ai)σ0(θ[i]x)x>.	(A.3)
We can show that the gradient is Lipschitz as follows, for all x ∈ Sp ,
kVθ[i]fi(θ[i]; x) -Vθ[i]fi(θ0[i]; x)kF ≤ bkdiag(ai)k2kσ0(θ[i]x) - σ0(θ0[i]x)k2kxk2	(A.4)
C
≤ 不 kθ[i] - θ [i]kF.	(Iar,i| = 1, kxk2 = I)
For each `t (f (θ; xt)) according to the convexity property we have
't(θ0) - 't(θ) ≥ Vf't(θ)>(f(θ0;Xt)- f (θ;Xt))
=X 内/'；,) Jfi(θ0[i]; Xt) - fi(θ[i]; Xt))
i 1 ∂fi (θ[i]; Xt)
For each i ∈ [d], we use the fundamental theorem of calculus to rewrite function value difference as
fi(θ0[i]; Xt) - fi(θ[i]; Xt) = hVθ[i]fi(θ[i]; Xt), θ0[i] - θ[i]i + R(fi, θ[i], θ0[i])
(A.5)
R(fi, θ[i], θ0[i]) = Z 1hVθ[i]fi(sθ0[i] + (1 - s)θ[i]; Xt) - Vθ[i]fi(θ[i]; Xt), θ0[i] - θ[i]ids.
0
Note that since the gradient of fi is Lipschitz given by (A.4), the residual term is bounded in mag-
nitude as follows,
1C C
∣R(fi,θ[i],θ0[i])I ≤ J0 bks(θ0[i] -θ[i])kF ∙kθ0[i] -θ[i]kFds = 2bkθ0[i]-θ[i]kF.
13
Under review as a conference paper at ICLR 2022
Hence we can show that the loss is nearly convex with respect to θ,
`t(θ0) - `t(θ) ≥ XX /'t]θ∖(fi(θ0[i];Xt)- fi(θ[i];Xt))
i=1 ∂fi(θ[i];	xt)
=XX「：：？、(hVθ[i]fi(θ[i]; Xt),θ0[i] - θ[i]i + R(fi,θ[i],θ0[i]))
i=1 ∂fi (θ[i]; Xt)
≥	XXh为，'；(rVθ[i]fi(θ[i];Xt),θ0[i]-θ[i]i-CXX /'* ∙kθ0[i]-θ[i]kF
i=1 ∂fi(θ[i];	Xt)	2b i=1 ∂fi(θ[i];	Xt)
≥	hvθ't(θ),θ0 -θi-？ιιθ0 -θkF,
where the last inequality uses the L-LiPschitz property of the loss 't(∙) with respect to f. Using a
diameter bound for θ, θ0 ∈ B(R) we get that ιθ - θ0ιF ≤ 2R which results in near convexity of
't(∙) with ε∩c = 2CLR2 with respect to θ.	□
ProofofLemma 3.4. Let g = (gι,...,gd) ∈ FRF (D). By Lemma A.1, if R0 =黑,with proba-
bility at least 1 一 δ∕d,for each i there exists θ* [i] such that ∣θ* [i] 一 θι [i]∣F ≤ R0, and
lfi(θ*[i]；X)- gi(X)I ≤
bCD2
2m
CD . —	L__
+--/	(2√p +，2 log d∕δ).
m/2
Let θ* = (θ*[1],..., θ*[d]). Taking a union bound, with probability at least 1 一 δ,
't(f(θ*; X)) = 't(fι(θ"1]; χ),...,fd(θ"d]; x))
d
≤ 't(gι(X),...,gd(X)) + Lt X(fi(θ*[i]; X)-gi(X))2
i=1
一 ,`	Lb√dCD2	L√dCD∕	r-	γ-λ__E
≤ 't(g(X)) + ——5-----+ —^=(2√d + √2logd∕δ).
2m	m/2
Finally, observe that ∣θ* 一 θι ∣∣f ≤ √dR0 = R.
□
Lemma A.1. For any δ,D > 0, let g : Rp → R ∈ FRF(D) and let R0 = -√D=, then with probability
at least 1 — δ over the random initialization θι, there exists θ* ∈ Rm×p such that ∣θ* — Θi∣f ≤ R0,
and for all X ∈ Sp and any i ∈ [d],
lfi(θ*;X)- g(X)I ≤
bCD2 + PCD= (2√p + √2ιogl∕δ).
2m m∕2
Proof. Since the neural network architectures are the same for all i ∈ [d], we fix an arbitrary i and
drop the index i throughout the proof. By Proposition C.1 in Gao et al. (2019), for any δ > 0,
with probability at least 1 一 δ over the randomness of θι, there exist ci, ∙∙∙ , Cm/2 ∈ Rp with
IlcrIl2 ≤ 2kgmRF ∀ r ∈ [mm], such thatgi(/) = Pm/2 c>Xσ0((θι[r])>X) satisfies
∀X ∈ S,1g1(X)-E *	+√CT,
where θi [r] represents the r-th row of θi. Now, we proceed to construct a θ* such that f (θ*; x) is
close to g1(X). We note that by symmetric initialization fi(θ1; X) = 0 for all X ∈ Sp. Then, use the
14
Under review as a conference paper at ICLR 2022
fundamental theorem of calculus similarly to (A.5) to decompose fi as follows:
fi(θ; x) = fi(θ; x) - fi(θ1; x)
m/2	m/2
=-(^X ar (θ[r] — θι [r])>xσ0((θι [r])>x) — ^X a『(θθ[r] 一 &[r])>xσ0((&[r])>x))
r=1	r=1
+ b(X ar Z x>(θ[r] — θι[r])(σ0((tθ[r] + (1 — t)θι[r])>x) — σ0((θι[r])τx))dt
m/2
— Xr=1 ar Z0
1
x>(8[r] — &[r])(σ0((t夕[r] + (1 — t)θι [r])>x) — σ0((8ι [r])>x))dt).
Consider θ* ∈ Rm×p SUch that θ*[r] = θι[r] + bCra『,夕[r] = %[r] — bCra『,where >[r]>
represents the 受 + r-th row of θ*. Then
∣∣θ*[r] — θ1[r]k2, M"r] — &[r]||2 ≤ OkgkRF, and the linear part of f satisfies
m
m/2	m/2
b (X ar(θ*[r] — θι[r])>xσ0((θι[r])>x) — X ar(θ*[r] — θι[r])>xσ0((θι[r])>x))
r=1	r=1
1 m/2	b	m/2	b
b (X ar 2 c>s0((&[r])>X)+X ar 2 c>s0(®i[r])>X))
r=1	r=1
1	m/2 b	m/2 b
b (X 2 c>xσ0((θι[r])> x) + X 2 c>χσ0((θι[rD>X))
r=1	r=1
m/2
X Cr>Xσ0((θ1 [r])>X) = g1(X).
r=1
Now we bound the residual part of fi, by using the triangle inequality, and the smoothness of σ(∙),
as follows
lfi(θ*; x) — gι(x)∣ = b∣X ar /1 x>(θ*[r] — θι[r])(σ0((tθ*[r] + (1 — t)θι[r])>x) — σ0((θι [r])>x))dt
m/2	1
—X ar / X> ®* [r] — θι [r])(σ0((/ [r] + (1 — t)& [r])>x) — σ0((& [r])>x))dt∣
—b- kxk2 ɪ max kcrk2 ∙ 2
≤ mCb2 4kgkRp = bc⅛.
b 4 2m2	2m
Using the triangle inequality, we can bound the approximation error as follows,
lfi(θ*;X)- g(x)| ≤ lfi(θ*; X)- g1(x)| + IgI(X)- g(x)|
≤ H⅛ + C⅛F(2√p + √2iθg1∕δ).
2m	m/2
Finally, observe that θ* is close to θι:
kθ* 一 θιkF ≤ X kθ*[r] — θι[r]k2 ≤ b2kgkRF- ≤ b2D2 = (R0)2.
mm
r=1
□
15
Under review as a conference paper at ICLR 2022
Proof of Lemma 3.5. Our proof extends Lemma A.6 in Gao et al. (2019) to our setting, where the
loss is defined over a vector whose coordinates are outputs of different deep neural networks. A
δ-net over S is defined as a collection of points {xr } ∈ S such that for all x ∈ S, there exists an xj
in the δ-net such that kxj - xk2 ≤ δ. Consider a δ-net of the unit sphere consisting of {xr}rN=1,
and standard results show that such a δ-net exists with N = (O(1∕δ))p. Let i ∈ [d] and r ∈ [N].
By Lemma A.5 in Gao et al. (2019), if m ≥ max{d, Ω(Hlog H)}, R + δ ≤ 甲 岛 rn for some
sufficiently small constant c, then with probability at least 1 -O(H)e-Ω(m(R+δ)2/3H) over the
random initialization, for any θ0[i], θ[i] ∈ B(R) and any x0 ∈ S with kx0 - xrk2 ≤ δ,
k%h[i]fi(θ'[i]; x0) — %h[i]fi(θ[i]; x0)kF =O((R + S)1/3H2√mlogm),
kVθh[i]fi(θ0[i]; x0)kF = 0(VmH),
where θh [i] denotes the parameter for layer h in the network for the i-th coordinate of the output.
Summing over the layers, we have
kVθ[i]fi(θ0[i]; x0) - Vθ[ifi(θ[i]; x0)kF =O((R + δ)1∕3H5/2√mlog m),
kVθ[i]fi(θ0[i]; x0)kF =O(HVm).
Similar to (A.5), we can write the difference of fi evaluated on θ0 [i] and θ[i] as a sum of a linear
term and a residual term R(fi, θ[i], θ0[i], x0) using the Fundamental Theorem of Calculus,
fi(θ0[i]; x0) -	fi(θ[i];	x0)	=	hVθ[i]fi(θ[i];	x0),	θ0[i]	-	θ[i]i	+ R(fi, θ[i], θ0[i], x0)	(A.6)
R(fi, θ[i], θ0[i], x0) =Z 1 Vθ[i]fi(sθ0[i] + (1 - s)θ[i]; x0) - Vθ[i]fi(θ[i]; x0), θ0[i] - θ[i]ds
0
(A.7)
Since the gradient changes slowly, we can bound the residual term as follows
|R(fi, θ[i], θ0[i], x0)| ≤
Z1
0
Vθ[i]fi(sθ0[i] + (1 - s)θ[i]; x0) - Vθ[i]fi(θ[i]; x0)kF kθ0[i] - θ[i]kF ds
≤ O((R + S)1/3H5/2√mlogm)∣∣θ0[i] - θ[i]kF.
Taking a union bound over the i’s, with probability at least 1 — O(H)de-°(m(R+δ产/3H), for all x0
such that kx0 - xr k2 ≤ δ,
0 "(QO."、、 β j ~F(rQ. 、∖、d't(f (θ X )) ( f 0 ∩!∖∙Λ. ,J∖	£ 0 a\;A- ,J∖∖
't (f (θ ; X )) - 't (f (θ; X )) ≥), ∂f. (θ[i]∙ χ0) (fi(θ [i]； X ) - fi(θ[i]; X ))
=XX dff.]x? (hVθ[i]fi (θ[i]; X0),θ0[i] - θ[i]i + R(fi,θ[i],θ0[i], X0))
i=1 ∂fi (θ[i]; X )
XX d't(f(θ; X0)) V	θ(∩∖-]o 0∖ θO ■!	θΓ-l∖
-Nh ∂fi(θ[i]; X0) ”[i]fi(θ[i], X ),θ [i] -θ[i]i
-O((R+s)1/3H5/2√mιogm)XX "f忆?)) ∙kθ0[i]-θ[i]kF
M ∂fi(θ[i]; X0)
≥ "θ't(f(θ; X0)), θ0 - θi -O((R + δ)1∕3H5/2√mlog m)L√dR.
We take δ = R, and by our choice of R, the condition R + δ ≤ H6 £3 m is satisfied. Taking a union
over bound all points in the δ-net, the above inequality holds for all X ∈ S with probability at least
1 - dO(H)O(1/R)Pe-Q(mR2/3H) = 1 -O(H)e-Q(mR2/3H)+p Iog(O(I/R))+log d
=1 - O(H)e-Q(mR2/3H),
where the last inequality is due to our choice of m.	□
16
Under review as a conference paper at ICLR 2022
B Proofs for Section 4
Proof of Theorem 4.1. This is identical to that of Theorem 3.2 given the result in Lemma B.4 and
the fact that the norm bound given by Lemma 3.5 is invariant. The conclusion is the same as in
Theorem 3.2 with L = O(Lc),p = K ∙ dχ,d = du.	□
Dynamics rollout. Before proving the lemmas necessary for the theorem proof, we rewrite the
state xθk by rolling out the dynamics from i = k to i = 1 as follows
k-1	1	k-1 i	i+1
Xk = Xkat + X Mikf(θ; Zi),	xkat	=	Y Ajxι	+	X Y AjWi,	Mik=	Y Aj	∙ Bi,
i=1	j=k-1	i=1 j=k-2	j=k-1
and for simplicity kX1 k ≤ W .
Sequential stabilizability. Furthermore, note that Assumption 5 can be relaxed to assuming there
exists a sequence of linear operators F1:K such that for C1 ≥ 1 and ρ1 ∈ (0, 1)
k-n+1
∀k ∈ [K],n ∈ [1,k),	Y (Ai + FiBi)	≤ Ci ∙ Pn .
i=k	op
This condition is called sequential stabilizability and it reduces to the stable case by taking the
actions u0k = FkXk + uk, yielding the stable dynamics of (Ak + FkBk, Bk)1:K.
LemmaB.1. Thefunction L(∕(θ)) is convex in f(θ).
Proof. The function L(f(θ)) is a sum of K functions. For an arbitrary k ∈ [K], note that Xk is a
affine function of f(θ) w.r.t. the components f (θ, Zi), i = 1,...,K. The other argument is f (θ; Zk)
which is also an affine function of f(θ). Hence, both arguments in Ck(∙, ∙), which is jointly convex
in its arguments, are affine in fZ(θ), which means that ck (Xθk, f(θ; zZk)) is convex in fZ(θ). Since
L(f(θ)) is defined as the sum over Ck (Xk, f (θ; Zk)), it is also convex in the argument f(θ).	□
Lemma B.2. For any θ ∈ Θ, the states and actions over an episode are bounded, maxk kuθk k ≤ Du
and maxk IlXk k ≤ Dx for Du = √dUN, Dx = YC^ ∙ (W + DuC2).
Proof. First, note that Uk = f (θ; Zk) and Zk ∈ Sκ∙dx. Given the output magnitude bound
|uk[i]| ≤ N for the network for all i ∈ [du], which means that ∣ukk ≤ √duN = Du. By
definition of Xnkat, we have that
∣Xkatk≤ w ∙「
1 - ρ1
Plugging this bound in the expression for Xθk, we get
k-1
kXk k ≤ W ∙ ɪ-pɪ + Du ∙ X C2 ∙Ci∙ ρk^i^1 ≤ ɪ-pɪ ∙ (W + DuC2).
□
Corollary B.1. The cost function Ck is Lc-Lipschitz with Lc = Lc ∙ max{1, Dx + Du}.
Lemma B.3. The function L(fZ(θ)) is L-Lipschitz w.r.t. each f(θ; zZk) for k ∈ [K] with L
L C2∙C1
Lc ∙ T-Pr.
Proof. We use Corollary B.1 with L0c to conclude this lemma statement. For any arbitrary k ∈ [K],
denote fk = f(θ; zZk) and note that in the expression of L(fZ(θ)) we have
∀i < k,	ι∣VfkCi(Xk,uk)k=0,
for i = k,	INfk Ci(Xk ,uk )k = k^uci(Xk ,uk )k ≤ Lc,
∀i > k,	INfk Ci(Xk uk )k = k(Mk )>Vxci(Xk)k ≤ kMk Ilop ∙ LC
17
Under review as a conference paper at ICLR 2022
Therefore, we conclude that
K
kVfkLk ≤ X kVfkCik ≤ Lc ∙ X kMkkop ≤ Lc ∙
i=1	i ≥ k
C11Ci
1 一 P1
□
Lemma B.4. Suppose the conditions on R, m, H from Lemma 3.5 and Assumptions 1, 3, 6 hold.
Let L(θ) denote L(f(θ)). Then, with probability at least 1 —O(H)e-Q(mR2/3H) over the random
initialization θ∖,for any θ, θ0 ∈ B(R) and any z ∈ S,
L(θ0) ≥ L(θ) + VL(θ)>(θ0 — θ)—O(LcKR4/3H5/2√mlogm√d)
Proof. Since L is convex infby Lemma B.1, we have that
L(f(θ,)) — L(f(θ)) ≥ VfL(f(θ))>(f(θ,) — f(θ))
K du
=X X 二(fj (θ0∕k) - f (θ,zk))
Using the linearization trick as in (A.6) and the L-LiPschitz property of L(f (θ)) w.r.t each f (θ, zk),
and continuing exactly as in proof of Lemma 3.5 we obtain that by Assumption 5
L(f(θ)) — L(f(θ)) ≥ hVθL(f(θ)),θ, — K ∙ θi —O((R + S)1/3H5/2√mlogm)θ(Lc)√dR.
where L = O(LC) according to Lemma B.3.	口
18