Under review as a conference paper at ICLR 2021
Multi-Agent Decentralized Belief Propaga-
tion on Graphs
Anonymous authors
Paper under double-blind review
Ab stract
We consider the problem of interactive partially observable Markov decision pro-
cesses (I-POMDPs), where the agents are located at the nodes of a communication
network. Specifically, we assume a certain message type for all messages. More-
over, each agent makes individual decisions based on the interactive belief states,
the information observed locally and the messages received from its neighbors
over the network.
Within this setting, the collective goal of the agents is to maximize the globally av-
eraged return over the network through exchanging information with their neigh-
bors. We propose a decentralized belief propagation algorithm for the problem,
and prove the convergence of our algorithm. Finally we show multiple applica-
tions of our framework. Our work appears to be the first study of decentralized
belief propagation algorithm for networked multi-agent I-POMDPs.
1	Introduction
In reinforcement learning, Partially Observable Markov Decision Processes (POMDPs) (KLC98;
BDH99; RN02) is a general decision-theoretic framework for planning under uncertainty in a par-
tially observable, stochastic environment. An agent makes decisions rationally in such settings by
maintaining beliefs of the physical state and sequentially choosing the optimal actions that maximize
the expected value of future rewards. Solutions of POMDPs are mappings from an agent’s beliefs
to actions. The drawback of POMDPs in the multi-agent scenario is that the impact of other agents’
actions cannot be represented explicitly. Examples of such POMDPs are infinite generalized policy
representation (LLC11), and infinite POMDPs (DVPWR13).
Interactive POMDPs (I-POMDPs) (GD05) generalize POMDPs to multi-agent settings by substitut-
ing POMDP belief spaces with interactive belief spaces. More specifically, an I-POMDP substitutes
plain beliefs of the state space with augmented beliefs of the state space and the other agents’ beliefs
and models. The models of other agents included in the new augmented belief space consist of two
types: intentional models and subintentional models. An intentional model ascribes beliefs, pref-
erences, and rationality to other agents, while a simpler subintentional model, such as finite state
controllers (PG16) does not. The augmentation of intentional models forms a hierarchical belief
structure that represents an agent’s beliefs of the physical state, beliefs of the other agents and their
beliefs of others’ beliefs, and it can be nested to arbitrary levels. Solutions of I-POMDPs map an
agent’s belief of the environment and other agents’ models to actions. It has been shown that the
augmented belief in I-POMDP results in a higher value function compared to one obtained from
POMDP, which implies I-POMDPs’ modeling superiority.
In this work, we study the problem of POMDPs with collaborative agents. For collaborative
POMDPs problem, it is important to specify the interactions bewteen the agents. One appealing
option is to have a central controller which has the information of all agents, and determines the
actions for all agents. With all the information available to the controller, the problem reduces
to a classical POMDP and can be solved by existing single-agent POMDP algorithms. Yet, in a
bunch of real-world applications, such as sensor networks (ASSC02; RN04) and intelligent trans-
portation systems (AB02), it may be very costly to have a central controller. Moreover, since the
central controller needs to communicate with each agent to get information, it increases the com-
munication overhead at each controller. The communication overhead degrades the scalability of
the multi-agent system as well as its robustness to malicious attacks. Another option is to make
1
Under review as a conference paper at ICLR 2021
the collaboration between agents implicitly represented by the I-POMDP model. Although agents’
actions do not change the other agents’ model directly, they can change the other agents’ belief
states indirectly, typically by changing the environment in a way observable to the other agents. The
influence to the other agents’ belief states can be viewed as some form of exchange of information.
However, the modeling superiority of I-POMDPs comes at the cost of a drastic increase of the belief
space complexity, because the agent models grow exponentially as the belief nesting level increases.
Hence, the complexity of the belief representation is proportional to belief dimensions, which is
known as the curse of dimensionality. Moreover, due to the fact that exact solutions to POMDPs are
PSPACE-complete and undecidable for finite and infinite time horizon respectively (PT87), the time
complexity of more generalized I-POMDPs is at least PSPACE-complete and undecidable for finite
and infinite horizon, since an I-POMDPs may contain multiple POMDPs or I-POMDPs of other
agents.
Given all these disadvantages of the centralized model and the complexity of I-POMDPs, we con-
sider a decentralized model where the agents in I-POMDPs are connected by a communication
network. Specifically, let {G = (V, E)} be a communication network, where V is the set of nodes,
and E ⊆ {(i, j) : i,j ∈ V} is the set of edges. We assume that each node represents an agent. And
agent i ∈ V and agent j ∈ V can communicate with each other if and only if (i, j) ∈ V. As such,
at each time slot t, each agent executes an individual action based on the interactive belief states,
the local information and the messages sent from its neighbors, with the goal of maximizing the
individual average rewards. The message type is crucial in reducing the complexity of I-POMDPs,
for example, if the message is a belief, the agent does not have to maintain complex and infinitely
nested beliefs of other agents. We call this model as networked multi-agent I-POMDPs, which is
presented in Section 3 in detail.
Main Contribution. Our contribution in this work is three-fold. First, we formulate interactive
POMDPs problem for networked agents, and prove a version of belief update and value iteration
update adapted to this setting. Second, we propose a decentralized belief propagation algorithm,
and prove the convergence of the proposed algorithm. Third, we show our framework precisely
captures the collaboration in decentralized multi-agent cooperative systems by showing multiple
applications.
Related Work. AI literature (BGIZ02; NTY+03) appeared in a series of studies that extended
POMDP to several branches. One of the branches is called decentralized POMDP (DEC-POMDP),
which is related to decentralized control problems (OW96). DEC-POMDP framework assumes that
the agents have the common reward function. Furthermore, it is assumed that the optimal joint
solution is computed in a central coordinator and then distributed among the agents for execution.
(NMT13) shows a variant of DEC-POMDP with a partial historical shared information structure.
The framework of I-POMDPs is introduced in (GD05), followed by Bayesian inference approximate
solutions (HG18). Another branch of extending POMDPs to multiple agents is called multi-agent
POMDP (MPOMDP) (MSL11; AO15). The MPOMDP framework assumes that agents have joint
observations, so it can be simplified to POMDP (PT02) by having a single centralized controller that
takes joint actions and receives joint observations. In other words, a DEC-POMDP can be reduced
to an MPOMDP in a fully communicative scenario.
From the game-theoretic side, most existing works are based on the framework of Markov games,
which was first proposed by (Lit94), and then followed by (Lit01; LR00; HW03). This framework
applies to the setting with both collaborative and competitive relationships among agents. More
recently, several multi-agent reinforcement learning (MARL) algorithms using deep neural net-
works as function approximators have gained increasing attention (FADFW16; GEK17; BBX+16;
OPA+17; FNF+17). A more relevant work is (ZYL+18), the authors study a MARL framework
with networked agents, where the communication among agents contributes toward the overall per-
formance of MARL in a fully decentralized setting.
The remainder of this paper is structured as follows. We start with an overview of partially observ-
able Markov decision processes, the concept of agent types and interactive POMDPs in Section 2.
We formulate the networked multi-agent I-POMDPs and present a decentralized belief propagation
algorithms for the networked multi-agent I-POMDPs problem in Section 3. We provide theoretical
result in Section 4. Several applications of our framework is presented in Section 5. We conclude
with a brief summary and some future research directions in Section 6.
2
Under review as a conference paper at ICLR 2021
2	Background
2.1	Partially Observable Markov Decision Process
A partially observable Markov decision process (POMDP) of an agent i is defined as
POMDPi = hS, Ai,Ti, Ωi,Oi, Rii	(1)
where: S is a set of possible states of the environment. Ai is a set of agent i’s actions. Ti is a
transition function, i.e., Ti : S × Ai × S → [0, 1] which describes the dynamics of the environment.
Ωi is the set of agent i's observations.Oi is agent i's observation function, i.e.,Oi : S X Ai X ΩΩ →
[0, 1]. Ri is the reward function for the agent i, i.e., Ri : S × Ai → R.
The belief update step of POMDP is shown below:
bit(st) = βOi(oit,st,ait-1) X bit-1(st-1)Ti(st,ait-1,st-1)	(2)
st-1∈S
where β is the normalizing constant. The belief update step takes into account changes in initial
belief bit-1, action ait-1, and the new observation oit. And bit(st) is the new belief for state st. It is
convenient for us to denote the above belief update step for all states in S as bti = SE(bit-1, ait-1, oti).
We denote agent i’s optimality criterion as OCi, which specifies how rewards over time are handled.
In this work, we concentrate on the infinite horizon criterion with discounting, i.e., the agent maxi-
mizes the expected value of the sum of the discounted rewards ofan infinite horizon E(Pt∞=0 γtrt),
where 0 ≤ γ ≤ 1 is a discount factor. However, our approach can be easily extended to the other
criteria.
The utility associated with a belief state, bi consists of the maximum immediate rewards due to
bi , together with the discounted expected sum of utilities associated with the updated belief states
SEi (bi , ai , oi):
U (bi ) = max Xbi(s)Ri(s,ai) + γ X Pr(oi|ai,bi)U(SEi(bi,ai, oi))	(3)
ai∈Ai S s∈S	θi∈Ωi	J
And the optimal action, a*, is an element of the set of optimal actions, OPT (bi), for the belief state
bi , defined as:
OP T (bi) = arg max Xbi(s)Ri(s, ai) + γ X P r(oi|ai, bi)U(SEi(bi, ai, oi))	(4)
ai∈Ai
2.2	Agent Types and Frames
The following two definitions collect POMDP parameters independent of agent implementation and
put them into constructs. The representations are convenient for our analysis, so we list them below,
Definition 1 (Type, (GD05)) A type of an agent i is, θi =
hbi,Ai, Ωi, Ti, Oi, Ri, OCii，where b is agent i，s state of belief (an element of ∆(S)),OCi is its
optimality criterion, and the rest of the elements are as defined before. Let Θi be the set of agent i’s
types.
Given type θi, and the assumption that the agent is Bayesian-rational, we denote the set of agent’s
optimal actions as OP T (θi).
Definition 2 (Frame, (GD05)) A frame of an agent i is, θi =
hAi, Ωi, Ti,Oi, Ri, OCii. Let Θi be the set ofagent i Sframes.
For brevity, we write a type as consisting ofan agent’s belief together with its frame: θi = hbi, θii.
3
Under review as a conference paper at ICLR 2021
2.3	Interactive POMDPs
W.l.o.g., we consider an agent i interacting with only one other agents j.
Definition 3 (I-POMDP) An interactive POMDP of agent i, I-POMDPi, is:
I-POMDPi,ι = hISi,ι,A,Ti, Ωi,Oi,Rii	(5)
where ISi,l is a set of interactive states for agent i, defined as ISi,l = S × Mj,l-1, l ≥ 1. Here Mj
is the the set of possible models of agent j, and l is the nesting level. The set of models Mj,l-1 can
be divided into two classes, the intentional models IMj,l-1, and subintentional models SMj. Thus,
Mj,l-1 = I Mj,l-1 ∪ SMj.
The intentional models, IMj,l-1, ascribe to the other agent beliefs, preferences and rationality in
action selection. Thus they are other agents’ types. For example, the intentional model for agent j
at level l - 1 can be defined as θj,l-1 = hbj,l-1, θji, where bj,l-1 is agent j’s belief nested to the
level l - 1, bj,l-1 ∈ ∆(ISj,l-1). We omit the details of subintentional model since it is not a focus
of this paper.
The interactive states ISi,l can be defined in an inductive manner:
1Si,0	= S	θj,0	= {hbj,0, θji	:	bj,0	∈	δ(ISj,0)}	Mj,0	= θj,0	∪ SMj
ISiJ = S ×	Mj,0	θj,1	= {hbj,1,θji	:	bj,1	∈	δ(ISjJ)}	MjJ	=	θj,1	∪	Mj,0
…	(6)
ISi,l	= S ×	Mj,1-1	θj,l	= {hbj,l,θji	:	bj,l	∈ δ(ISj,l)}	Mj,l	=	θj,l	∪ Mj,1-1.
For the rest of the paper, we omit the level subscription for notation simplicity.
All remaining components in an I-POMDP are similar to those in a POMDP except we also keep
the following assumptions in (GD05):
Model Non-manipulability Assumption (MNM): Agents’ actions do not change the other agents’
models directly.
Model Non-observability (MNO): Agents cannot observe other’s models directly.
Next, we define the belief update steps and value iterations for I-POMDPs.
2.3.1 Belief Update in I-POMDPs
The next proposition defines the agent i’s belief update function, bti(ist) = P r(ist|oit, ait-1, bit-1),
where ist ∈ ISi is an interactive state. We use the belief state estimation function, SEθi, as an
abbreviation for belief updates for individual states so that
bit = SEθi(bti-1,ait-1,oit).	(7)
τθi (bit-1, ait-1, oit, bit) will stand for P r(bit|bit-1, ait-1, oti). Further below we also define the set of
type-dependent optimal actions of an agent, OPT(θi).
Given the definition and assumptions, the interactive belief update can be performed as in the next
proposition. [Belief Update] Under the MNM and MNO assumptions, the belief update function for
an interactive POMDP hISi, A, Ti, Ωi, O%, Rii, when mj- in ist is intentional, is:
bit(ist) =β	X	bt-1(ist-1) X Pr(aj-1∣θj-1)Oi(st, a'-1,吟 × Ti(Sj,at-1, St)
ist-1 ：m j—1 =θt	aj-1
Xτθjt(btj-1,atj-1,otj,btj)Oj(St,at-1,otj).	(8)
otj
4
Under review as a conference paper at ICLR 2021
2.3.2 Value Function and Solution in I-POMDPs
Analogously to POMDPs, each belief state in I-POMDP has an associated value reflecting the max-
imum reward the agent can expect in this belief state:
U (θi) = max X X bi(is)ERi(is,ai) + Y X Pr(0i\a*bi)U (hSEθi (bi,ai,0i),θi)∖	(9)
' is	θi∈Ωi	)
where, ERi(is, ai) = Pa Ri(is, ai, aj)P r(aj |mj). Equation (9) is a basis for value iteration in
I-POMDPs.
Agent i's optimal action, a*, for the case of infinite horizon criterion with discounting, is an element
of the set of optimal actions for the belief state, OPT (θi), defined as
Ebi(is)ERi(is,ai) + Y E Pr(θi∖ai,bi)U(〈SE&i(bi,ai,0i),θi)
is	θi∈Ωi
(10)
OPT (θi) = arg max
ai∈Ai
3 Networked Multi-Agent I-POMDP
Consider a setV ofN agents, labeled by an index i = 1, 2, ..., N. Their interaction (communication)
is modeled by a graph G = (V, E) where an edge (i, j) is in E if and only if agent i interacts
(communicates) with agent j. We assume that G is connected, i.e., there is a path from any node
i to any other node j . We denote the set of neighbors of agent i as ∂i. The network induced by
interaction and communication could be different, but here we do no distinguish them for simplicity.
Let us define type of message and message before we proceed to define the networked multi-agent
I-POMDPs.
Definition 4 (Message Type & Message) We define the set of message types as M = {×i∈[N]Ai,
∆(S), ×i∈[N ]Ωi}, i.e., there are three types of messages, 'action', 'belief state' and 'observation'.
A type of message M is an element of the message type set M, i.e., M ∈ M. A message μt is an
element ofmessage type M, i.e. μi ∈ M.
For ex
ample, a message
sent by an agent i at time slot t of type ‘action’ can be represented by
aj
Now we can define the networked multi-agent I-POMDP.
Definition 5 (Networked Multi-agent I-POMDP) A networked multi-agent I-POMDP is charac-
terized by a tuple hG, {ISi}i∈[N ], {Ai}i∈[N ], {Ti}i∈[N ], {Ci}iE[N ], {Oi}i∈[N ], {Ri}i∈[N ],M i ∙
•	G is the communication network.
•	For each agent i, the interactive state ISi is defined as ISi = S × M∂i , where ∂i is the set
of neighbors of agent i in the communication network G .
•	For each agent i, the action space is Ai .
•	For each agent i, under the MNM assumption, the transition model is defined as Ti :
S × Ai × A∂i × S → [0, 1].
•	For each agent i, Ωi is defined as before in the I-POMDP model.
•	For each agent i, under the MNO assumption, the observation transition function is defined
as Oi : S X Ai X Adi X Ωi → [0,1].
•	For each agent i, Ri is defined as ISi × Ai × A∂i → R
•	M is the message type.
Similar to I-POMDPs, we can define agent i’s belief update function. For simplicity, from now on
we assume ∂i = {j} (we can extend it to more than one neighbor case easily), the belief update
function for agent i is btt(ist) = Pr(ist∣ott, a；-1, blt-1,μj), where ist ∈ ISi is an interactive state
5
Under review as a conference paper at ICLR 2021
and μj is the message sent by an agent i at time slot t. We use the belief state estimation function
SEθi , as an abbreviation for belief updates for individual states so that
bt = SEθi (bi-1 ,ai-1,ot,μj).	(11)
The next proposition defines the agent i’s belief update function in detail.
[Belief Update] Under the MNM and MNO assumptions, the belief update function for agent i
ofanetworkedmu山-agentI-POMDPhG, {ISi}i∈[N], {Ai}i∈[N], {Ti}i∈[N], {Ωi}i∈[N], {Oi}i∈[N],
{Ri}i∈[N], Mi, when mj in ist is intentional, is:
•	When message type is ‘action’:
bit(ist) =β	bit-1(ist-1)Oi(st,at-1,oit)Ti(st-1,at-1,st)
Um j-=θ
Xτθjt(btj-1,atj-1,otj,btj)Oj(st,at-1,otj).	(12)
otj
•	When message type is ‘belief state’:
bt(ist) =β	X	bt-1(ist-1)	X	Pr(aj-1∣θjT)
ist-1：m j-1=θt	aj-1∈OPT (θj)
Oi(st,at-1,oti)Ti(st-1,at-1,st).	(13)
•	When message type is ‘observation’:
bt(ist) =β	E	bt-1(ist-1)∑ Pr(ajT∣θjT)Oi(st,at-1,ot)
istT：m t-1=θj	aj-1
Ti(st-1,at-1,st)τθtj(btj-1,atj-1,otj,btj)Oj(st,at-1,otj).
(14)
U(θi, μj) = max
ai∈Ai
We leave the proof of Propositions 3 to Section 4.
The value function U (θi ) is
bi(is)ERi(is, ai) + γ	Pr(θi∖αi, bi)U(<SEθi(bi,电,0i, μj), θi)
is	θi∈Ωi
(15)
where ERi(is, ai') = Paj Ri(is,a,aj)Pr(αj∖mj,μj).
And the set of optimal actions for agent i is defined as,
Ebi(is)ERi(is,ai) + Y E Pr(θi∖ai,bi)U(<SEθi(hi,ai,θi,μj),θi)
is	θi∈Ωi
(16)
OPT(θi,μj) = arg max
ai∈Ai
Note the Equation (15) can be rewritten in the following form Un = HUn-1. Here H : B → B is
a backup operator, and is defined as,
HUn-1(θi,μj) = max h(θi,ai,μj,Un-1),	(17)
ai∈Ai
where h : Θi × Ai × M × B → R is,
h(θi,ai,μj, U) = Ebi(is)ERi(is,ai)+ Y E Pr(θi∖ai, bi)U(<SEθi(hi,ai,θi,μj),θi). (18)
is	θi∈Ωi
6
Under review as a conference paper at ICLR 2021
3.1 Algorithm
Now we are ready to present the decentralized belief propagation algorithm for networked multi-
agent systems. The algorithm requires each agent to maintain a belief on its interactive states, while
allows each agent i share messages of certain type with its neighbors on the network. In this way,
each agent is able to improve the value function and thus the current policy.
Decentralized Belief Propagation Algorithm
Input: Initialize bi (is), U (θi) for all is ∈ ISi and for all i ∈ [N] : all i in [N] Observe oit, and
reward r∖. Send message μt to all neighbors ∂i. Update the belief given the received messages
b∖ = SEθi(btt-',att-',ott, {μj}j∈∂i). Update the value function U(θi) - HU(θi). Select and
execute action ait . convergence
Note similar to the typical belief propagation algorithm, all agents send/receive messages simulta-
neously and then update their beliefs simultaneously.
4 Theoretical Results
We start this section by proving Proposition 3 for the case that message type is ‘action’, i.e., M =
×i∈[N] Ai and more specifically μj = aj-1. The belief update step for other message types can be
derived in similar ways.
Proof 1 We start by applying the Bayes Theorem:
bt(ist) = Pr(ist∖oi ,at 1 ,bt 1 ,μ j)
_Pr(ist,ot|ai-1,btT,〃j)
=Pr(oi∖ai-1,bt-1,μj)
=β X bt-1(ist-1)Pr(ist,ot∖at-1,ist-1,μj)
ist-1
ist-1, μj)Pr(ist, ot ∖at-1, aj-1,ist-1,μj)
=β £ bt-1(ist-1) £ Pr(aj-1 ∖is j,μj)Pr(ist,ot∖aiT,ajT
ist-1
ist-1
t-1
aj
=β ∑ bt-1(isT)£ PMajT ∖bjT,μj)Pr(ot∖aj,ist,is j,μj)Pr(ist∖at-1,ist-1,μj)
t-1	t-1
is	aj
(=a) β X bit-1(ist-1)P r(oit ∖at-1, ist)Pr(ist ∖at-1, ist-1)
ist-1
=β X bit-1(ist-1)Oi(st,at-1,oit)Pr(ist∖at-1,ist-1),	(19)
ist-1
where the equality (a) holds because Pr(aj-1 ∖bj-1,μj) = 1 when μj = aj-1 and
Pr(aj-1∖bj-1,μj) = 0 otherwise. And recall μj = aj-1 is our assumption.
Since we assume the interactive states are intentional, ist = (st, θj) = (st, bj, θj), we can simplify
the term P r(ist∖at-1, ist-1).
P r(ist∖at-1, ist-1)
=Pr(St ,bj ,θj ∖at-1,ist-1)
=Pr(bj ∖st, θj,at-1, isJ1)Pr(s∖ θj∖at-1, ist-1)
=Pr(bj ∖st, θj,at-1, ist-1)Pr(θj ∖st, at-1, is^-1)Pr(st∖a^-1 ,is^-1)
=Pr(bj ∖st, θj,at-1, ist-1)I(θj, θj-1 )Ti(st-1, at-1, St),	(20)
7
Under review as a conference paper at ICLR 2021
where I(∙, ∙) is a boolean identity function, which equal 1 if the two frames are identical, and 0
otherwise. The joint action pair, at-1, may change the physical state. The third term on the right-
hand side of Equation (20) captures this transition.
Pr(bj∣st,或at-1,is t-1)
=X Pr(bj ∣st, θj,αt-1, ist-1,oj)Pr(otj ∣st, θj, αt-1, ist-1)
otj
EPr(bj∣st, θj,αt-1, is j,oj)Pr(oj∣st, θj, αt-1)
otj
Xτθjt(btj-1, atj-1,	otj,	btj)Oj(st,	at-1,	otj).
(21)
ot
j
In Equation (21), the first term on the right-hand side is 1 if agent j ’s belief update,
SEθj (btj-1, atj-1, otj) generates a belief state equal to btj. In the second terms on the right-hand side
ofthe equation, the MNO assumption allows Us to replace Pr(oj ∣st, θj,at-1) with Oj (st, αt-1, oj).
Let us substitute Equation (21) into Equation (20),
Pr(ist∣at-1 ,ist-1) = X Tθj (bj-1,aj-1, oj, bj)Oj(st, αt-1, oj)I(θj,居T)Ti(St-1, at-1, st).
ot
j
(22)
Now substitute Equation (22) into Equation (19), we have
bit(ist)=β X bit-1(ist-1)Oi(st, at-1, oit)Xτθjt(btj-1, atj-1, otj, btj)Oj(st, at-1, otj)
ist-1	otj
I (θj ,θj-1)Ti(st-1,at-1,st).	(23)
We can remove the term I (θj,θj-1) by changing the scope of the first summation, which gives us
the final expression for the belief update,
bit(ist) =β	X	bit-1(ist-1)Oi(st,	at-1,	oit)Ti(st-1,	at-1, st)
istfm t-1=°j
Xτθjt(btj-1,	atj-1,	otj,	btj)Oj(st,	at-1,	otj).	(24)
otj
Next, for an agent i and its I-POMDPi , following the proof idea in (GD05), we prove the conver-
gence of our algorithm. First, we show some properties of the back up operator H,
Lemma 1 For any finitely nested I-POMDP value functions V and U, ifV ≤ U , then HV ≤ HU.
Proof 2 Select arbitrary value functions V and U such that V(θi,μv) ≤ U(θi,μu), ∀θi ∈ Θi,
μv, μu ∈ M, where θi is an arbitrary type of agent i and μv, μu are arbitrary messages.
HV (θi,μv)
∖[bi(is')ERi(is,a) + Y E Pr(θi∣ɑi,bi)V(hSEθi(bi,αi,θi,μv),E)
is	θi∈Ωi
=£bi(is)ERi(is,a" + Y X Pr(θi∖α↑,bi)V (hSEθi (bi,a-,θi,μv ),θi)
is	θi∈Ωi
≤ X bi(is)ERi(is, α*) + Y X Pr(θi∣α!∙, bi)U(<SEθi(bi,a!-,θi,μu),a)
is	θi∈Ωi
Ebi(islERilis,ai)+ Y E Pr(θi∣ai, bi)U(〈SE&i(bi,ai,θi,μu),θi)
is	θi∈Ωi
=HU (θi,μu).	(25)
Since θi,μv, μu are arbitrary, HU ≤ HV.
max
ai∈Ai
≤ max
ai∈Ai
8
Under review as a conference paper at ICLR 2021
max
ai∈Ai
- max
ai∈Ai
Lemma 2 For any finitely nested I-POMDP value functions V, U, anda discount factor γ ∈ (0, 1),
kHV -HUk ≤γkV -Uk.
Proof 3 Assume two arbitrary well defined value functions V and U such that V ≤ U. From
Lemma 1, itfollows that HV ≤ HU. Let θi be an arbitrary type Ofagent i and μv, μu be arbitrary
messages. And let a* be the optimal action of HU(θi, μu), we have,
0 ≤ HV(θi,μv) — HU(θi,μu)
Ebi(is)ERi(is,aG + Y E Pr(θi∣ai,bi)V(<SEθi(bi,ai,θi,μv),θi)
is	θi∈Ωi
bi(is)ERi(is, ai)+γ	Pr(θi∣ai, bi)U(<SEθi(bi,ai, Oi,μu),θi)
is	Oi ∈Ωi
≤ Ebi(is)ERi(is,a*) + Y X Pr(θi∣a*,bi)V(hSEθi(bi,a*,θi,μv),θi)
is	θi∈Ωi
-	Xbi(is)ERi(is,a*) - Y X Pr(θi∣a*,bi)U(hSEθi(bi,a*,θi,μu),θi)
is	θi∈Ωi
=	Y X Pr(θi∣a*,bi) [v (<SEθi (bi, a* ,0i,μv ),θi) - U (hSEθi (bi,a*,0i,μu ),θi)
θi∈Ωi	k
=	Y X P r(oi|ai*, bi)kV - Uk
θi∈Ωi
=	YkV - Uk.	(26)
As the supremum norm is symmetrical, a similar result can be derived for HU(θi,μu)一
HV(θi, μv). Since θi, μv, μu are arbitrary, we prove the lemma.
Based on Lemma 1 and Lemma 2, following the Contraction Mapping Theorem in (Sto89), we can
prove for each agent i, the value iteration in its I-POMDPi converges to a unique fixed point. We
state the Contraction Mapping Theorem (Sto89) below
Theorem 1	(Contraction Mapping Theorem, (Sto89)) If (S, ρ) is a complete metric space and
T : S → S is a contraction mapping with modulus Y, then
1.	T has exactly one fixed point U* in S, and
2.	The sequence {Un} converges to U*.
Theorem 2	For a networked multi-agent I-POMDP, Algorithm 3.1 converges if the value functions
of all agents are well defined.
Proof 4 First, the normed space (B, |卜||) is complete w.r.t. the metric induced by the supremum
norm. Second, Lemma 2 proves the contraction property of the operator H. Directly applying
Theorem 1, letting T = H, we prove the value iteration in I-POMDPs converges to a unique fixed
point.
And we naturally have the following theorem.
Theorem 3	For a networked multi-agent I-POMDP, the optimal policies for agent i, i ∈ [N] is
given by Equation (16).
5 Applications
In this section, we show our networked multi-agent I-POMDPs framework can be applied to various
applications.
9
Under review as a conference paper at ICLR 2021
5.1	Decentralized Control Problem
Let us consider the partial history sharing information model in (NMT13). Consider a dynamic
system with N controllers. The system operates in discrete time for a horizon T. Let X(t) ∈ X(t)
denote the state of the system at time t, Ui(t) ∈ Ui(t) denote the control action of controller i, i ∈ [N] at
time t, and U(t) denote the vector (U1(t), . . . , UN(t)). The initial state X(1) has a probability distribution
Q(1) and evolves according to
X(t+1) = f(t) (X(t) , U(t) , W0(t)),	(27)
where {W0(t)}tT=1 is a sequence of i.i.d. random variables with probability distribution QW,0.
At any time t, each controller has access to three types of data: current observation, local memory,
and shared memory.
• Current local observation: Each controller makes a local observation Yi(t) ∈ Yi(t) on the
state of the system at time t,
Yi(t) = h(it)(X(t),Wi(t)),	(28)
where {Wi(t) }tT=1 is a sequence of i.i.d. random variables with probability distribution
QW,i. We assume that the random variables in the collection {X(1), Wj(t), t = 1, . . . , T,j =
0, 1, . . . , N } are mutually independent.
•	Local memory: Each controller stores a subset Mi(t) of its past local observations and its
past actions in a local memory:
Mi(t) ⊂ {Yi(1:t), Ui(1:t)}.
(29)
At t = 1, the local memory is empty, MIt) = 0.
•	Shared memory: In addition to its local memory, each controller has access to a shared
memory. The contents Ct of the shared memory at time t are a subset of the past local
observations and control actions of all controllers:
C(t) ⊂ {Y(1:t) , U(i1:t) },	(30)
where Y(t) and U(t) denote the vectors (Y1(t) , . . . , YN(t)) and (U1(t) , . . . , UN(t)) respectively.
At t = 1, the shared memory is empty, C(1) = 0.
Controller i chooses action Ui(t) as a function of the total data Yi(t) , Mi(t) , C(t) available to it. Specif-
ically, for every controller i, i ∈ [N],
Ui(t) =gi(t)(Yi(t),Mi(t),C(t)),	(31)
where gi(t) is called the control law of controller i. The collection gi = (gi(1), . . . , gi(T)) is called the
control strategy of controller i. The collection g1:N = (g1, . . . , gN) is called the control strategy of
the system.
At time t, the system incurs a cost l(X(t) , U(t)). The performance of the control strategy of the
system is measured by the expected total cost
T
J(g1:N) := Eg1:N[Xl(X(t),U(t))],	(32)
t=1
where the expectation is with respect to the joint probability measure on (X(1:T), U(1:T)) induced by
the choice of g1:N.
We are interested in the following optimization problem
Definition 6 For the model described above, given the state evolution functions f(t), the observation
functions h(it), the protocols for updating local and share memory, the cost function l, the distribu-
tions Q(1), QW,i, i = 0, 1, . . . , N, and the horizon T, finda control strategy g1:N for the system that
minimized the expected total cost given by Equation (32).
10
Under review as a conference paper at ICLR 2021
(NMT13) show the decentralized system defined in Definition 6 can be viewed as a coordinated
system. The coordinator only knows the shared memory C(t) at time t. At time t, the coordinator
chooses mappings Γ(it) : Yi(t) × M(it) → Ui(t), for i ∈ [n], according to
Γ(t) = d(t)(C(t),Γ(1"-1)),	(33)
where Γ(t) = (Γ(1t), Γ(2t), . . . , Γ(nt)), and the function d(t) is called coordination rule at time t. The
function Γ(it) is called the coordinator’s prescription to controller i. At time t, the function Γ(it) is
communicated to controller i, and then the controller i generates an action using the function Γ(it)
based on its current local observation and its local memory:
Ui(t) = Γ(it)(Yi(t),Mi(t)).	(34)
Moreover, the coordinated system can further be viewed as an instance of a POMDP model by
defining the state process as st := {X(t), Y(t), M(t)}, the observation process as ot := C(t-1), and
the action process At := Γ(t). And we can define the information state at time t for the POMDP of
the coordinator as:
Π(t) := P(s(t)|C(t), r(1:t)).	(35)
Furthermore, we have a new system dynamic at time t as
Π(t+1) = η(t)(Π(t), C(t), Γ(t)),	(36)
where η(t) is the standard non-linear filtering update function (see (NMT13) for more details).
Now, given our framework in Section 3, we can prove that the above optimization problem is a
networked multi-agent I-POMDP, as shown in the following proposition. The optimization problem
defined in Definition 6 is a networked multi-agent I-POMDP.
Proof 5 We can prove the proposition by defining the networked multi-agent I-POMDP tuple
hG, {ISi}i∈[N],{Ai}i∈[N], {Ti}i∈[N],{Ωi}i∈[N],{Oi}i∈[N], {Ri}i∈[N],Mi for the optimization
problem,
•	Given the definition of shared memory, we can define an equivalent communication network
G = (V, E). Let the set ofN agents (controllers) be the set of nodes V, labeled by index
i = 1, 2, ..., N. And an edge (i, j) is in E if and only if agent i shares memory with agent j.
•	For each agent (controller) i, the interactive state ISi = X × M∂i, where X is the set of
states of the physical environment, and M∂i is the set of possible models of i’s neighbors
∂i.
•	For each agent (controller) i, the action space A(it) at time t is given by the range of the
coordinator’s prescription to controller i, Γ(it).
•	For each agent (controller) i, the transition model Ti(t) at time t is given by the dynamic
π f t+1) = η (t) (∏(t) ,c (t), γ (1:t-1)).
•	For each agent (controller) i, the set of observations Ωit) at time t is given by the set of
possible shared memories {C (t)}.
•	For each agent (controller) i, the observation function Oi(t) at time t is given by Yi(t) =
h(it)(X(t), Wi(t)) andΓ(it).
•	For each agent (controller) i, the reward function at time t is given by l(X(t), U(t)).
•	The message type depends on the definition of shared memory, it could be ‘action’ or ‘ob-
servation’.
The optimal strategy of the optimization problem in Definition 6 can be obtained by running Algo-
rithm 3.1 and given by Theorem 3.
(NMT13) call their solution Dynamic Programming Decomposition. We generalize the scenario to
the networked multi-agent case, and may call our solution Belief Propagation Decomposition.
11
Under review as a conference paper at ICLR 2021
5.2 Decentralized Spectrum Sharing Problem
We consider a contention based decentralized spectrum sharing problem. Given a communication
network G = (V, E), such that V is a set ofN base stations, labeled by an index i = 1, 2, . . . , N, and
an edge (i, j) is in E if and only if base station i is backhauled with base station j. Each base station
serves a given subset of user equipments (UEs). The whole communication network shares a single
spectrum, the base stations contend the transmission opportunities in the following way. Each time
slot consists of two phases, contention phase and data transmission phase. At contention phase, each
base station draws a random number at the start of a time slot, which determines the order of optional
transmissions. In the designated slot of the contention phase, a base station can choose to transmit
or keep silent. If a base station transmits, it continues transmission through the contention phase and
data transmission phase. Ideally, the UE throughput is given by Shannon channel capacity. And the
objective of each base station is to maximize the long-term throughput it delivers to its UEs.
For simplicity, we make the following assumptions. We assume each base station serves only one
UE. Each base station always has traffic to be delivered to the UE, thus always participates in con-
tention. And there is only downlink traffic. The action space of each base station is {transmit, silent},
which can be denoted as {1, 0}.
Given the above assumptions, we can mathematically formulate the problem. Let us denote the UE
served by the base station i by the same index i, and so is the link between the UE and the base
station. We denote the link strength between base station i and UE j by channel coefficient hij . For
each link i in time slot t, let us denote the transmission rate by R(it) and the long-term average rate
by Xt.
Ri(t) = W log2(1 + SINR(it)),
(37)
(38)
Et) = (I-得/尸 + ɪ Rt,
BB
where B > 1 is a parameter which balances the weights of past and current transmission rates. We
denote the actions of all base stations in time slot t as a(t) = [a(1t), . . . , a(Nt)]∈{0, 1}N. The SINR for
UE i is given by
SINR(it)
唠 Ptait)
σUE + P,=, htPtajt
(39)
where Pt is the transmission power, and σU2E is the noise power at UE. Si(t) is the signal power for
UE i at time t, and Ii(t) is the total interference power for UE i at time t.
Given the action vector a(t) in each time slot t, the long term proportional fairness scheduling utility
is (KMT98)
N
max U (X(t)) = max ^X log(X(t)).
t→∞	t→∞	i
i=1
(40)
12
Under review as a conference paper at ICLR 2021
And we can split the proportional fairness metric over time by rewriting the utility function up to
time slot T,	N U (X(T)) = X iog(X(T)) i=1 =X log ((1-fXi(T-1) + BRiT)) i=1 =X log ((1-得)XiT-I)(1 + —RTF)) i=1	V	B i	(B - 1)XiT-1), NT =X(log Xi(0) + X r(t)) i=1	t=1 N	TN =X log Xi(0) + XX rit),	(41) i=1	t=1 i=1
where	rit)ig ((I-BΧ1 + (B■-‰ )).	(42)
Given our framework, we can prove that the above optimization problem is a networked multi-agent
I-POMDP, as shown in the following proposition.
The decentralized spectrum sharing problem defined in Section 5.2 is a networked multi-agent I-
POMDP.
Proof 6 We can prove the proposition by defining the networked multi-agent I-POMDP tuple
hG, {ISi}i∈[N],{Ai}i∈[N], {Ti}i∈[N],{Ωi}i∈[N],{Oi}i∈[N], {Ri}i∈[N],Mi for the optimization
problem,
•	The communication network G is directly defined in the decentralized spectrum sharing
problem.
•	For each agent (base station) i, the interactive state ISi = S × M∂i, where S consists of
the joint space of the average rate of link i and the channels between all base stations and
UE i, i.e.(Xi), {hji}j∈[N ]i is a state in S, and Mdi is the Set of possible models of i S
neighbors ∂i.
•	For each agent (base station) i, the action space Ai is {transmit, silent}, i.e., {0, 1}.
•	For each agent (base station) i, the transition model Ti : S × Ai × A∂i × S → [0, 1] is
given by the dynamic
Xit) = (I- BX(U + ɪRit),
BB
and the channel fading model.
•	For each agent (base station) i, at time slot t, the observation consists of the average
throughput Xi-1), the signal power SittI and the total interference power Iit-1) of the
previous time slot, i.e., ot = [JXit-1), Sit-1) ,Iit-1)].
•	For each agent (base station) i, the observation function Oi is directly given by the defini-
tions OfinvOlving parameters in S, Ai X Adi and Ωi
•	For each agent icontroller) i, at time slot t, the reward function is given by riit).
•	The message type in this problem can be either ‘action’ or ‘observation’.
The Proposition 5.2 naturally leads to the following corollary, The optimal strategy of decentralized
spectrum sharing problem can be obtained by running Algorithm 3.1 and given by Theorem 3.
13
Under review as a conference paper at ICLR 2021
6	Conclusion
In this paper, we address the problem of multi-agent I-POMDPs with networked agents. In partic-
ular, we consider the fully decentralized setting where each agent makes individual decisions and
receives local rewards, while exchanging information with neighbors over the network to accom-
plish optimal network-wide averaged return. Within this setting, we propose a decentralized belief
propagation algorithm. We provide theoretical analysis on the convergence of the proposed algo-
rithm. And we show our framework can be applied to various applications. An interesting direction
of future research is to extend our algorithms and analyses to the policy gradient methods.
References
[AB02] Jeffrey L Adler and Victor J Blue. A cooperative multi-agent transportation manage-
ment and route guidance system. Transportation Research Part C: Emerging Tech-
nologies,10(5-6):433-454, 2002.
[AO15] Christopher Amato and Frans A Oliehoek. Scalable planning and learning for multia-
gent pomdps. Technical report, MASSACHUSETTS INST OF TECH CAMBRIDGE
COMPUTER SCIENCE AND ARTIFICIAL . . . , 2015.
[ASSC02] Ian F Akyildiz, Weilian Su, Yogesh Sankarasubramaniam, and Erdal Cayirci. Wireless
sensor networks: a survey. Computer networks, 38(4):393-422, 2002.
[BBX+ 16] Dzmitry Bahdanau, Philemon Brakel, Kelvin Xu, Anirudh Goyal, Ryan Lowe, Joelle
Pineau, Aaron Courville, and Yoshua Bengio. An actor-critic algorithm for sequence
prediction. arXiv preprint arXiv:1607.07086, 2016.
[BDH99] Craig Boutilier, Thomas Dean, and Steve Hanks. Decision-theoretic planning: Struc-
tural assumptions and computational leverage. Journal of Artificial Intelligence Re-
search, 11:1-94, 1999.
[BGIZ02] Daniel S Bernstein, Robert Givan, Neil Immerman, and Shlomo Zilberstein. The
complexity of decentralized control of markov decision processes. Mathematics of
operations research, 27(4):819-840, 2002.
[DVPWR13] Finale Doshi-Velez, David Pfau, Frank Wood, and Nicholas Roy. Bayesian nonpara-
metric methods for partially-observable reinforcement learning. IEEE transactions
on pattern analysis and machine intelligence, 37(2):394-407, 2013.
[FADFW16] Jakob Foerster, Ioannis Alexandros Assael, Nando De Freitas, and Shimon Whiteson.
Learning to communicate with deep multi-agent reinforcement learning. In Advances
in neural information processing systems, pages 2137-2145, 2016.
[FNF+ 17] Jakob Foerster, Nantas Nardelli, Gregory Farquhar, Triantafyllos Afouras, Philip HS
Torr, Pushmeet Kohli, and Shimon Whiteson. Stabilising experience replay for deep
multi-agent reinforcement learning. arXiv preprint arXiv:1702.08887, 2017.
[GD05] Piotr J Gmytrasiewicz and Prashant Doshi. A framework for sequential planning in
multi-agent settings. Journal of Artificial Intelligence Research, 24:49-79, 2005.
[GEK17] Jayesh K Gupta, Maxim Egorov, and Mykel Kochenderfer. Cooperative multi-
agent control using deep reinforcement learning. In International Conference on
Autonomous Agents and Multiagent Systems, pages 66-83. Springer, 2017.
[HG18] Yanlin Han and Piotr Gmytrasiewicz. Learning others’ intentional models in multi-
agent settings using interactive pomdps. In Advances in Neural Information Process-
ing Systems, pages 5634-5642, 2018.
[HW03] Junling Hu and Michael P Wellman. Nash q-learning for general-sum stochastic
games. Journal of machine learning research, 4(Nov):1039-1069, 2003.
14
Under review as a conference paper at ICLR 2021
[KLC98] Leslie Pack Kaelbling, Michael L Littman, and Anthony R Cassandra. Planning and
acting in partially observable stochastic domains. Artificial intelligence, 101(1-2):99-
134, 1998.
[KMT98] Frank P Kelly, Aman K Maulloo, and David KH Tan. Rate control for communi-
cation networks: shadow prices, proportional fairness and stability. Journal of the
Operational Research society, 49(3):237-252, 1998.
[Lit94] Michael L Littman. Markov games as a framework for multi-agent reinforcement
learning. In Machine learning proceedings 1994, pages 157-163. Elsevier, 1994.
[Lit01] Michael L Littman. Value-function reinforcement learning in markov games. Cogni-
tive systems research, 2(1):55-66, 2001.
[LLC11] Miao Liu, Xuejun Liao, and Lawrence Carin. The infinite regionalized policy repre-
sentation. In ICML, 2011.
[LR00] Martin Lauer and Martin Riedmiller. An algorithm for distributed reinforcement
learning in cooperative multi-agent systems. In In Proceedings of the Seventeenth
International Conference on Machine Learning. Citeseer, 2000.
[MSL11] Joao V Messias, Matthijs Spaan, and Pedro U Lima. Efficient offline communication
policies for factored multiagent pomdps. In Advances in Neural Information Process-
ing Systems, pages 1917-1925, 2011.
[NMT13] Ashutosh Nayyar, Aditya Mahajan, and Demosthenis Teneketzis. Decentralized
stochastic control with partial history sharing: A common information approach.
IEEE Transactions on Automatic Control, 58(7):1644-1658, 2013.
[NTY+03] Ranjit Nair, Milind Tambe, Makoto Yokoo, David Pynadath, and Stacy Marsella.
Taming decentralized pomdps: Towards efficient policy computation for multiagent
settings. In IJCAI, volume 3, pages 705-711, 2003.
[OPA+17] Shayegan Omidshafiei, Jason Pazis, Christopher Amato, Jonathan P How, and John
Vian. Deep decentralized multi-task multi-agent reinforcement learning under partial
observability. arXiv preprint arXiv:1703.06182, 2017.
[OW96] James M Ooi and Gregory W Wornell. Decentralized control of a multiple access
broadcast channel: Performance bounds. In Proceedings of 35th IEEE Conference on
Decision and Control, volume 1, pages 293-298. IEEE, 1996.
[PG16] Alessandro Panella and Piotr Gmytrasiewicz. Bayesian learning of other agents’ finite
controllers for interactive pomdps. In Proceedings of the Thirtieth AAAI Conference
on Artificial Intelligence, pages 2530-2536, 2016.
[PT87] Christos H Papadimitriou and John N Tsitsiklis. The complexity of markov decision
processes. Mathematics of operations research, 12(3):441-450, 1987.
[PT02] David V Pynadath and Milind Tambe. The communicative multiagent team decision
problem: Analyzing teamwork theories and models. Journal of artificial intelligence
research, 16:389-423, 2002.
[RN02] Stuart Russell and Peter Norvig. Artificial intelligence: a modern approach. 2002.
[RN04] Michael Rabbat and Robert Nowak. Distributed optimization in sensor networks. In
Proceedings of the 3rd international symposium on Information processing in sensor
networks, pages 20-27, 2004.
[Sto89] Nancy L Stokey. Recursive methods in economic dynamics. Harvard University Press,
1989.
[ZYL+18] Kaiqing Zhang, Zhuoran Yang, Han Liu, Tong Zhang, and Tamer Bayar. Fully de-
centralized multi-agent reinforcement learning with networked agents. arXiv preprint
arXiv:1802.08757, 2018.
15