Under review as a conference paper at ICLR 2022
Gradient flows
on the feature-Gaussian manifold
Anonymous authors
Paper under double-blind review
Ab stract
The scarcity of labeled data is a long-standing challenge for cross-domain machine
learning tasks. This paper leverages the existing dataset (i.e., source) to augment
new samples that are close to the dataset of interest (i.e., target). To relieve the
need to learn a metric on the feature-label space, we lift both datasets to the space
of probability distributions on the feature-Gaussian manifold, and then develop a
gradient flow that minimizes the maximum mean discrepancy loss. To perform the
gradient flow of distributions on the curved feature-Gaussian space, we unravel the
Riemannian structure of the space and compute explicitly the Riemannian gradient
of the loss function induced by the optimal transport metric. For practical purposes,
we also propose a discretized flow, and provide conditional results guaranteeing
the global convergence of the flow to the optimum. We illustrate the results of our
proposed gradient flow method in several real-world datasets.
1	Introduction
A major challenge facing machine learning and data science is the lack of labeled data. A popular
approach is developing learning methods that can interpolate, adapt, or transfer knowledge across
datasets and domains. Some well-known methods for these tasks are domain adaptation (Ben-
David et al., 2007; Mansour et al., 2009; Courty et al., 2017; Damodaran et al., 2018; Gong et al.,
2012; Taigman et al., 2016), transfer learning (Long et al., 2017; Pan & Yang, 2010; Zamir et al.,
2018), and meta-learning (Finn et al., 2017; Khodak et al., 2019). Recently, these methods have
produced promising results for important tasks in autonomous driving and robotics (Wang et al.,
2018; Bousmalis et al., 2018).
A straightforward remedy to the lack of data is to devise mechanisms to synthesize new sensible data
samples, particularly in the target domain. In this paper, we consider a specific setup in which we
have access to labelled data samples generated from both the source and the target domain. More
concretely, we consider a covariate space X = Rm and a categorical label space Y . We are given
a source domain dataset consisting of N samples (xi, yi) ∈ X × Y for i = 1, . . . , N, and a target
domain dataset of M samples (Xj, yj) ∈ X XY for j = 1,...,M. We consider the situation where
M is not large enough, and the target data is scarce. The ultimate goal of this paper is to generate
new samples in the target domain, and we aim to create new samples whose distribution is as close as
possible to the unknown distribution that governs the target domain.
We here adopt a gradient flow method to synthesize new, unseen data samples. Because we have
access to the source domain samples, it is possible to flow each source sample towards the target data
in order to minimize a certain loss function. If the loss function is chosen to reflect the dissimilarity
between distributions, and if the flow is properly designed to converge, then the terminal product
of the flow will provide us with new samples that can sufficiently approximate the data-generating
distribution of the target domain. As a consequence, using gradient flows is a sensible approach to
synthesize target domain samples.
Unfortunately, formulating a gradient flow algorithm for labelled data with categorical set Y is
problematic. Indeed, there is no clear metric structure on Y in order to define the topological
neighborhood, this in turn leads to the difficulty of forming the gradients with respect to the categorical
component. To overcome this difficulty, a gradient flow on the dataset space was recently proposed
in Alvarez-Melis & Fusi (2021) by leveraging a new notion of distance between datasets in Alvarez-
Melis & Fusi (2020); Courty et al. (2017); Damodaran et al. (2018). The main idea behind this
1
Under review as a conference paper at ICLR 2022
approach is to reparametrize the categorical space Y using the conditional distribution of the features,
which is assumed to be Gaussian, and then construct a gradient flow on the feature-Gaussian space.
Nevertheless, the theoretical analysis in Alvarez-Melis & Fusi (2021) focuses solely on the gradients
with respect to the feature, and there is no derivation of the flow with respect to the Gaussian
component. In fact, the space of Gaussian distributions is not a (flat) vector space, and extracting
gradient information depends on the choice of the metric imposed on this Gaussian space.
Contributions. We study in this paper a gradient flow approach to synthesize new labelled samples
related to the target domain. To construct this flow, we consider the space of probability distributions
on the feature-Gaussian manifold, and we are metrizing this space with an optimal transport distance.
We summarize the contributions of this paper as follows.
•	We study in details the Riemannian structure of the feature-Gaussian manifold in Section 3.1, as
well as the Riemannian structure of the space of probability measures supported on this manifold
in Section 3.2.
•	We consider a gradient flow that minimizes the squared maximum mean discrepancy (MMD) loss
function to the target distribution. We describe explicitly the (Riemannian) gradient of the squared
MMD in Lemma 4.1, and we provide a partial differential equation describing the evolution of the
gradient flow that follows the (Riemannian) steepest descent direction.
•	We propose two discretization schemes to approximate the continuous gradient flow equation: an
Euler scheme in Section 4.1 and an Euler scheme with noise in Section 4.2. We provide conditions
guaranteeing the global convergence of our gradient flows to the optimum in both continuous and
discretized schemes.
Our gradient flows minimize the MMD loss function, thus it belongs to the family of MMD gradient
flows that was pioneered in Mroueh et al. (2019) and Arbel et al. (2019), and further extended
in Mroueh & Nguyen (2021). The MMD function compares two distributions via their kernel mean
embeddings on a flat reproducing kernel Hilbert space (RKHS). In contrast to the Kullback-Leibler
divergence flow, the MMD flow can employ a sample approximation for the target distribution (Liu,
2017). Further, the squared MMD possesses unbiased sample gradients (BinkoWski et al., 2018;
Bellemare et al., 2017). However, existing literature on MMD flows focus on distributions on (flat)
Euclidean spaces. The floW developed in our paper here is for distributions on the (curved) Riemannian
feature-Gaussian space. Moreover, our approach is distinctive from the floW in Alvarez-Melis & Fusi
(2021) because the floW therein does not consider the gradient in the Gaussian component. Here, We
impose a specific metric on the Gaussian component, and We compute explicitly the (Riemannian)
gradient of the MMD loss function With respect to this metric to formulate our floW.
Generating neW data samples is particularly useful When We have to train classifiers With limited
labelled target data. The numerical experiments in Section 5 Will demonstrate that our gradient floWs
on the feature-Gaussian manifold can effectively augment the target data in the feW-shot learning
setting, and thus can significantly boost the accuracy in the classification task.
Other related works. Nonparametric gradient floWs using the 2-Wasserstein distance betWeen
distributions are investigated in (Ambrosio et al., 2008; Jordan et al., 1998; Otto, 2001; Villani, 2008;
Santambrogio, 2015; 2017; Frogner & Poggio, 2020; Kolouri et al., 2020), but only for distributions
on Euclidean spaces and for different loss functions. Related nonparametric gradient floWs With
other metrics include Sliced-Wasserstein Descent (Liutkus et al., 2019), Stein Descent (Liu, 2017;
Liu & Wang, 2016; Duncan et al., 2019), and Sobolev Descent (Mroueh et al., 2019), hoWever, they
also consider only distributions on Euclidean spaces. In particular, (Liu, 2017; Duncan et al., 2019)
introduce Riemannian structures for the Stein geometry on flat spaces, While ours is for an optimal
transport metric on a curved space. On the other hand, related parametric floWs for training generative
adversarial netWorks have been studied in (Bottou et al., 2017; Chizat & Bach, 2018; Chen & Li,
2018; Arbel et al., 2020; Chizat, 2020; Mroueh & Nguyen, 2021).
Notations. We use Sn to denote the set ofn × n real and symmetric matrices, and Sn++ ⊂ Sn consists
of all positive definite matrices. For A ∈ Sn, tr(A) := Pi Aii. We use〈•，•)and ∣∣ ∙ ∣∣2 to denote the
standard inner product and norm on Euclidean spaces. Let P(X) be the collection of all probability
distributions with finite second moment on metric space X. If 夕:X → Y is a Borel map and
V ∈ P(X), then the push-forward 夕#V is the distribution on Y given by 中#V(E) = ν(夕T(E)) for
all Borel sets E ⊂ Y. For a function f of the continuous time variable t, ft denotes the value of f at t
while ∂tf denotes the standard derivative of f w.r.t. t. Also, δz denotes the Dirac delta measure at z.
2
UnderreVieW as a COnferenCe PaPer at ICLR 2022
2 LABELLED DATA SYNTHESIS VlA GRADlENT FLoWS OF LlFTED
DlSTRlBUTloNS
In this SeCtiopwe describe OUr approach to SyntheSiZe target domain SamPleS USing gradient Row，A
hoHStiC VieW Of OUr method is PreSented in FigUre L
FigUre 一一 SChematiC VieW Of OUr approach- The SOUrCe and target datasets are first Hfted to distributions
Po and Q On the feature—Gaussian SPaCe (Ieft box)，We then run a gradient Row for T iterations to get
a terminal distribution PT (middle)，AtOmS Of PT a^e PrOjeCted to get labeled target SamPleS (right)∙
In the Hrst step" We WOUld need to Hffthe feature—label SPaCe A" x V to a higher dimensional SPaCe
Where a metric Can be defined∙ ConSider momentarHy the SoUrCedata SamPleS (m:m)泞1∙ NotiCe
that this data Can be represented as an empirical distribution ∖ on Af Xy MOre PreCiSely“ we have
∖ = NIlMy 子 2a÷BeCaUSe V is discretpthe IaW Of COnditiOnaI PrObab≡ties allows US to
identify the COnditiOnaI distribution £ of X一 Y =¾≤Under U. The Hfting PrOCedUre is Obtained
by employing a Pre—determined mapping S-A* → 用.3and any categorical ValUe Wey Can
now be represented as an W—dimensional distribution S#£• USing this Hfting“ any SoUrCe SamPIe
∈Λ"x V is Hfted to a POint (m: 0⅜∖⅛2) ∈ Af x P (用"and the SOUrCe dataset is representable
as an empirical distribution OfthefOrm N——1MJ5Sʤv
TheHftedrePreSentationofaCategoriCaIVaIUe¾≤m VaSanw ldimensionaldistributions⅜∖⅛∈ P(IRn)
is advantageous because P (坦)is metrizabpfor examppusing the 2—Wasserstein distance，The
downside” unfortunately is that P (甩)is infinite dimensionaL and encoding the datasets in this Hfted
representation is not efficient，To resolve this issue” we assume that e#Uu is GaUSSian for all¾≤∈γ
and thus any distribution 翕 £ Can be CharaCteriZed by the mean VeCtOr 苫 ∈ 坦 and COVarianCe
matriX Σ⅛∈ s+s+ defined as
WyU 0dMy " 00τd——F<9 ∈
J乂 J乂
In real—world Setting∞the COnditiOnaI moments Of S(X) 一 Y are SUffiCientIy different for y 廿 y「and
thus the representations USing (g ∑y) will UnHkeIy Iead to any IOSS Of IabeIinfOrmatiOn∙ With this
Hfting “ the SOUrCe data thus Can be represented as an empirical distribution Po On 用m X X st3Via
PouN 一Mr 一 F
By an analogous COnStrUCtiOn to COmPUteI1andMly USing the target data" the target domain data
⅛⅛'S游一 Can be represented as another empirical distribution
2 = ΛfM2"l 5a ) ∙
Let US denote the ShOrthand 3 =用三 x 甩 X s+st then Po and Q are both PrObab≡ty measures On
3∙ We refer to Po and Q as the feature—Gaussian representations Ofthe SOUrCe and target datasets∙
We now COnSiderthe gradient Row associated With the OPtimiZatiOn PrObIem
⅛5≡-i≈jlmmd(≈¾
Underthe initiaHZatiOn P = P.oThe ObjeCtiVe function FSquantifies how far an incumbent SoIUtiOn
P is from the target distributionr≈measured USing the MMD distance，In SeCtiOnS 3 and 4“ We win
PrOVide the necessary ingredients to COnStrUCtthiS Row,
Under review as a conference paper at ICLR 2022
Suppose that after T iterations of the (discretized) gradient flow algorithm, we obtain a distribution
ρT ∈ P(Z) that is sufficiently close to %, i.e., F(ρT) is close to zero. Then we can recover new target
samples by projecting the atoms of the distribution ρT to the locations on X × Y . This projection
can be achieved efficiently by solving a linear optimization problem, as suggested in Appendix B.
Remark 2.1 (Reduction of dimensions). If m = n and φ is the identity map, then our lifting
procedure coincides with that proposed in Alvarez-Melis & Fusi (2020). However, a large dimension
n is redundant, especially when the cardinality of Y is low. If n m, then φ offers significant
reduction in the number of dimensions, and will speed up the gradient flow algorithms. In this paper,
we use φ as the t-SNE embedding. According to van der Maaten & Hinton (2008), t-SNE’s low-
dimensional embedded space forms a Student-t distribution, and SNE uses a Gaussian distribution.
Our proposed framework can be straightforwardly extended to elliptical distributions since the Bures
distance still has the same closed-form as for the Gaussian distributions (Gelbrich, 1990).
3 RIEMANNIAN GEOMETRY OF THE SPACES Z AND P(Z)
If we opt to measure the distance between two Gaussian distributions using the 2-Wasserstein metric,
then this choice would induce a natural distance d on the space Z = Rm × Rn × Sn++ prescribed as
d((X 1,μ1, ςI), (X2, μ2, ς2)) := [∣∣x1 - x2k2 + kμ1 - μ2k2 + B01,夕2)2] 2	3I)
where B is the BUres metric on S++ given by B(∑ι, ∑2) ：= [tr(∑ι + ∑2 — 2[∑2 ∑2∑2] 2)]2.
As B is a metric on Sn+ (Bhatia et al., 2019, p.167), d is hence a product metric on Z. This section
serves two pUrposes: first, we stUdy the non-EUclidean geometry of Z Under the groUnd metric d.
Second, we investigate the Riemannian strUctUre on P(Z), the space of all distribUtions sUpported
on Z and with finite second moment, that is indUced by the optimal transport distance. These
Riemannian strUctUres are reqUired to define the Riemannian gradients of any loss fUnctionals on
P(Z), and will play an important role in oUr development of the gradient flow for the sqUared MMD.
3.1 GEOMETRY OF Z
The space Z is not a linear vector space. In this section, we reveal the Riemannian strUctUre on Z
associated to the groUnd metric d. As we shall see, Z is a cUrved space as its geodesics are not straight
lines and involve solUtions to the LyapUnov eqUation. For any positive definite matrix Σ ∈ Sn++ and
any symmetric matrix V ∈ Sn, the LyapUnov eqUation
HΣ + ΣH = V	(3.2)
has a UniqUe solUtion H ∈ Sn (Bhatia, 1997, Theorem VII.2.1). Let LΣ[V] denote this UniqUe
solUtion H.
Riemannian metric. The space Sn++ is a Riemannian manifold with the BUres metric B as the
associated distance fUnction, see TakatsU (2011, Proposition A). Since Z is the prodUct of two
EUclidean spaces and Sn++ , this gives rise to the following geometric strUctUre for Z.
Proposition 3.1 (Geometry of Z). The space Z is a Riemannian manifold: at each point z =
(x, μ, Σ) ∈ Z ,the tangent space is Tz Z = Rm X Rn X Sn and the Riemannian metric is given by
(w1,v1, V1), (w2,v2,V2) z := hw1,w2i + hv1,v2i + hV1, V2iΣ	(3.3)
for two tangent vectors	(w1, v1,	V1)	and	(w2, v2,	V2)	in	Rm	X Rn X	Sn,	where	hV1, V2iΣ	:=
tr LΣ[V1] Σ LΣ [V2] . Moreover, the distance function corresponding to this Riemannian metric
coincides with the distance d given by (3.1).
The proofs of Proposition 3.1 and all other resUlts will be provided in the Appendix A.
Geodesic and exponential map. As Z is a prodUct Riemannian manifold, any geodesic in Z is of
the form (θ, γ, Γ) with θ, γ being the EUclidean geodesics (straight lines) and Γ being a geodesic in
the Riemannian manifold Sn++ . More precisely, for each Σ ∈ Sn++ and each tangent vector V ∈ Sn ,
the geodesic in the manifold Sn++ emanating from Σ with direction V is given by
Γ(t) = (I + tL∑[V])Σ(I + tL∑[V]) fort ∈ J*,	(3.4)
4
Under review as a conference paper at ICLR 2022
where J* is the open interval about the origin given by J* = {t ∈ R : I + tL∑ [V] ∈ S++} (Malago
et al., 2018). As a consequence, for each point (x, μ, Σ) ∈ Z and each tangent vector (w, v, V) ∈
Rm × Rn × Sn , the Riemannian exponential map in Z is given by
exP(x,μ,Σ)(t(w,v,V))=例t),Y⑴，γ⑴)fort ∈ J*,	(3.5)
where θ(t) := X + tw, γ(t) := μ + tv, and Γ(t) is defined by (3.4). Note that by its definition,
t → exp(χ,μ,∑)(t(w, v, V)) is the geodesic in Z emanating from (x, μ, Σ) with direction (w, v, V).
Gradient and divergence. Given the Riemannian metric (3.3), ones can define the corresponding
notion of gradient and divergence (Lee, 2003). For a differentiable function 夕:Z → R, its gradient
▽d夕(Z) is the unique element in the tangent space Rm X Rn X Sn satisfying
〈▽d2(z),(w,v,V))z = D夕z(w,v,V) for all (w,v,V) ∈ Rm X Rn X Sn
with Dψz(w, v, V) denoting the standard directional derivative of 夕 at Z in the direction (w, v, V).
By exploiting the special form of h∙, )z in (3.3), we can compute Vd^(z) explicitly:
Lemma 3.2 (Gradients). For a differentiable function 夕:Z → R, we havefor Z = (x, μ, Σ) that
Vd2(Z) = (VxP(z), VμP(z), 2[V∑2(z)]∑ + 2∑V∑2(z)]),	(3.6)
where (Vx, Vμ, V∑) are the standard (Euclidean) gradients ofthe respective components.
The last component in formula (3.6) for Vd夕 reflects the curved geometry of Z, and can be interpreted
as the Riemannian gradient of the function Σ → 夕(x, μ, Σ) w.r.t. the Bures distance B.
For a continuous vector field Φ : Z → Rm X Rn X Sn and a distribution ρ ∈ P(Z), the divergence
divd(ρΦ) is defined as the signed measure on Z satisfying the following integration by parts formula
/ IAz)
Z
divd(ρΦ)(dZ)
-Z
hφ(z), NdWi(Z))Z ρ(dz)
for every differentiable function W : Z → R with compact support. In case ρ has a density w.r.t. the
Riemannian volume form on Z, then this definition coincides with the standard divergence operator
induced by Riemannian metric (3.3).
3.2 OPTIMAL TRANSPORT AND RIEMANNIAN STRUCTURE ON P(Z)
To define a gradient low for probability distributions on Z, it is essential to have a concept of gradients
for functionals defined on P(Z). This requires a meaningful Riemannian structure on P(Z), and
here, we adopt a Riemannian structure generated by the optimal transport on P(Z) with ground cost
d2. The optimal transport metric W(ρ0, ρ1) between any two distributions ρ0, ρ1 ∈ P(Z) is defined
by formula (A.4) of Appendix A.1. As (Z, d) is a Riemannian manifold by Proposition 3.1, it follows
from the celebrated Benamou-Brenier formula (Benamou & Brenier, 2000) that W can be expressed
in terms of a dynamic formulation. Precisely,
W(ρ0,ρ1)2
inf
(ρ,φ)∈A(ρ0, ρ1)
Z01ZZ
kVdφt(Z)kz2 ρt(dZ)dt,
(3.7)
where A(ρ0, ρ1) is the collection of all pairs (ρ, φ) of curve ρ : [0, 1] → P(Z) with endpoints ρ0
and ρ1, and function φ : [0, 1] X Z → R that satisfies the continuity equation
∂tρ + divd(ρtVdφt) = 0 in the sense of distributions in (0, 1) X Z.	(3.8)
Riemannian metriconP(Z). The formulation (3.7) gives rise to the following Riemannian structure
on P(Z) induced by distance W. First, the continuity equation enables us to identify a tangent vector
∂tρwith the divergence -divd(ρtVdφt). Thus the tangent space ofP(Z) ata distribution ρ can be de-
fined as TρP(Z) := - divd(ρVdW) : Wis a differentiable function with compact support on Z .
Second, we let gρ : TρP(Z) X TρP(Z) -→ R be the Riemannian metric tensor given by
gρ(ζ1,ζ2) :=
Z
hVdW1(Z), VdW2(Z))z ρ(dZ)
(3.9)
for ζ1 = -divd(ρVdW1) and ζ2 = -divd(ρVdW2). With this definition and due to (3.8), formula
(3.7) can be rewritten using the metric tensor as W(ρ0, ρ1)2 = inf(ρ,φ)∈A(ρ0,ρ1) R01 gρt (∂tρ, ∂tρ) dt.
The metric tensor (3.9) allows us to define a notion of Riemannian gradients for functionals on P(Z).
In the next section we shall compute this gradient explicitly for the squared MMD gradient flow.
5
Under review as a conference paper at ICLR 2022
4	Gradient Flow for Maximum Mean Discrepancy
As P(Z) is an infinite dimensional and curved space, many machine learning methods based on
finite dimensional or linear structure cannot be directly applied to this manifold. To circumvent this
problem, we use a positive definite kernel to map P(Z) to a RKHS and then perform our analysis
on it. Let k be a positive definite kernel on Z , and let H be the RKHS generated by k. The inner
product on H is denoted by(., ∙)h, and the kernel mean embedding P ∈ P(Z) --→ mρ(∙) ∈ H is
given by mρ(z) := Z k(z, w) ρ(dw) for z in Z. The maximum mean discrepancy (MMD) (Gretton
et al., 2012) between ρ ∈ P(Z) and the target % is defined as the maximum, over all test functions
in the unit ball of H, of the mean difference between the two distributions. Moreover, it can be
expressed by MMD(ρ, %) = kmρ - m%kH (see Appendix A). When k is characteristic, the kernel
mean embedding ρ 7→ mρ is injective and therefore, MMD(ρ, %) = 0 if and only if ρ = %.
Consider the loss function F[ρ] := 1 MMD(ρ, %)2 = 1 ∣∣mρ - m%∣∣H∙ For each ρ, the Riemannian
gradient grad F[ρ] is defined as the unique element in TρP(Z) satisfying
gρ (grad F [ρ],ζ) = dt L=OF [ρt]
for every differentiable curve t 7→ ρt ∈ P (Z ) passing through ρ at t = 0 with tangent vector
∂tρt∖t=o = Z. By using the Riemannian metric tensor (3.9), We can compute explicitly this gradient.
Lemma 4.1 (Gradient formula). The Riemannian gradient of the functional F satisfies
grad F[ρ] = -divd (ρVd[mρ - m%]).
The Riemannian gradient grad F on P (Z) depends not only on the gradient operator Vd but
also on the divergence operator. Using Lemma 4.1, We can reWrite the gradient floW equation
∂tρt = -grad F[ρt] explicitly as
∂tpt = divd(ρtVd[mρt - m%]) for t ≥ 0.	(4.1)
The next result exhibits the rate at Which F decreases its value along the floW.
Proposition 4.2 (Rate of decrease). Along the gradient flow t 7→ ρt ∈ P(Z) given by (4.1), we have
d2
否 F [Pt] = -L IIVdmPt- m%]∣∣zρt(dz) for t ≥ 0.
Proposition 4.2 implies that 余F[ρt] = 0 if and only if Vd[mρt - m%](z) = 0 for every Z in the
support of the distribution Pt . As a consequence, the objective function Will decrease its value
Whenever the gradient Vd [mρt - m%] is not identically zero.
4.1	Riemannian Forward Euler Scheme
We noW propose the Riemannian version of the forWard Euler scheme to discretize the continuous
floW (4.1):
PT +1 = exp(sτΦτ)#PT with Φτ := -Vd[mρτ - m%],	(4.2)
Where sτ > 0 is the step size. Here, for a vector field Φ = (Φ1, Φ2 , Φ3) : Z → Rm × Rn × Sn and
for ε ≥ 0, exp(εΦ) : Z → Z is the Riemannian exponential map induced by (3.5), i.e.,
exPz(εΦ(z)) = (X + εΦι(z), μ + εΦ2(z), (I + εL∑[Φ3(z)])Σ(I + εL∑[Φ3(z)]))
for z = (x, μ, Σ) ∈ Z. Notice in the above equation that the input Z affects simultaneously the
bases of the exponential map expz as well as the direction Φ(z). This map is the ε-perturbation
of the identity map along geodesics with directions Φ. When PT = N-1 PiN=1 δzτ is an empirical
distribution that is supported on (ZiT)iN=1, scheme (4.2) flows each particle ZiT to the new position
ZiT +1 = expzτ (sT Φ(ZiT)). The next lemma shows that ΦT is the steepest descent direction for F
w.r.t. the exponential map among all directions in the space L2 (PT), which is the collection of all
vector fields Φ on Z satisfying ∣Φ∣2L2(ρτ) := RZ ∣Φ(Z)∣z2 PT (dZ) < ∞.
6
Under review as a conference paper at ICLR 2022
岳 I	F [exp(εφ^Pτ] = I
dε ε=0	Z
Algorithm 1 Discretized Gradient Flow Algorithm for Scheme (4.2)
Input: a source distribution ρ0 = N-1 PN=I δ2o, a target distribution % = M-1 P^=ι 方句,a
number of iterations T, a sequence of step sizes sτ > 0 with τ = 0, 1, ..., T and a kernel k
Initialization: Compute Ψ(z) = MT Pj=I N%k(z, Zj) with N%k(z, Zj) is Rd of Z → k(z, Zj)
repeat for each τ = 0, . . . , T - 1:
Compute Ψτ (z) = N-1 PiN=1 R1dk(z, ziτ )
for i = 1,...,N do z：+1 - expzτ (sτ(Ψ — Ψτ)(z，)) end for
Output: PT = N-1 P=I δzT
Lemma 4.3 (Steepest descent direction). Fix a distribution ρτ ∈ P(Z). For any vector field
Φ : Z → Rm × Rn × Sn, we have
hRd[mρτ - m%](z), Φ(z)iz ρτ(dz).
As a consequence, if we let Φ T be the unit vector field (w.r.t. ∣∣ ∙ ||%2(。丁)norm) in the direction of
d
Φτ given in (4.2), then 膜 ∣ε=0F [exp(εΦτ )# ρτ] = -∣Vd[mρτ 一 m%]∣L2 /)and this is the fastest
decay rate among all unit directions Φ in L2 (ρτ).
It follows from Lemma 4.3 that the discrete scheme (4.2) satisfies the Riemannian gradient descent
property: if Vd[mρτ 一 m%] is nonzero and if sτ > 0 is chosen sufficiently small, then F[ρτ+1] <
F[ρτ]. In the Appendix (Proposition A.5), we quantify the amount of decrease of F at each iteration.
An iterative algorithm that implements the flow (4.2) is described in Algorithm 1.
Complexity. For each iteration τ in Algorithm 1, its complexity is O(N(Nm + n3)) where m is
the feature’s dimension, n is the reduced dimension (n	m), N is the number of particles.
Convergence guarantees. We now study the (weak) convergence of the solution ρt of the continuous
gradient flow (4.1), as well as the discretized counterpart ρτ of flow (4.2), to the target distribution
%. When the kernel k is characteristic, this convergence is equivalent to limt→∞ MMD(ρt, %) = 0.
Because the objective function F is not displacement convex (Arbel et al., 2019, Section 3.1),
the convergent theory for gradient flows in (Ambrosio et al., 2008) does not apply even in the
case of Euclidean spaces. In general, there is a possibility that MMD(ρt, %) does not decrease to
zero as t tends to infinity. In view of Proposition 4.2, this happens if the solutions ρt are trapped
inside the set ρ : Z Vd[mρ —m%]∣] ρ(dz) = 0}. For each distribution P on Z, We define
in Appendix A.2 a symmetric linear and positive operator Kρ : H → H having the property that
hKρ[mρ 一 m%], mρ 一 m%iH = RZ Vd[mρ 一 m%]z2 P(dz) (see Lemma A.6 in the Appendix). We
further shows in Proposition A.8 that Pt globally converges in MMD if the minimum eigenvalue λt
of the operator Kρt satisfies an integrability condition.
4.2	Noisy Riemannian Forward Euler Scheme
The analysis in Section 4.1 reveals that the gradient flows suffer from convergence issues if the
residual mρt 一 m% belongs to the null space of the operator Kρt . To resolve this, we employ
graduated optimization (Arbel et al., 2019; Chaudhari et al., 2017; Gulcehre et al., 2016; 2017; Hazan
et al., 2016) used for non-convex optimization in Euclidean spaces. Specifically, we modify algorithm
(4.2) by injecting Gaussian noise into the exponential map at each iteration τ to obtain
ρτ+1 = exp(sτΦτ)#PTe with fβτ : (z,u) → expz(βτu), ρτ,βτ := fβτ#(PT初).(4.3)
Here g is a Gaussian measure with distribution NRm (0,1) 0 NRn (0,1) 0 NSn (0,1) on the tangent
space and NSn (0, 1) denotes an n-by-n symmetric matrix whose upper triangular elements are
i.i.d. standard Gaussian random variables. When Pτ = N-1 PiN=1 δzτ, scheme (4.3) flows each
particle ZT first to zjβτ := expzτ (βτ U) with noise U 〜g and then toziτ+1 = expzτ,βτ (sτ Φ(ziτ,βτ)).
Our next result extends Proposition 8 in (Arbel et al., 2019) for the standard quadratic cost on the
Euclidean space to the nonstandard cost function d2 on the curved Riemannian manifold Z++ . It
7
Under review as a conference paper at ICLR 2022
demonstrates that scheme (4.3) achieves the global minimum of F provided that k is a Lipschitz-
gradient kernel and both the noise level βτ and the step size sτ are well controlled. The proof of
Proposition 4.4 is given in Appendix A.2 and relies on arguments that are different from that of
(Arbel et al., 2019).
Proposition 4.4 (Objective value decay for noisy scheme). Suppose that k is a Lipschitz-gradient
kernel1 with constant L, and the noise level βτ satisfies
λβτ2F[ρτ] ≤	kΦτ(z)kz2ρτ,βτ(dz)
Z
for some constant λ > 0. Then for ρτ+1 obtained from scheme (4.3), we have
F[ρτ +1] ≤ F[ρ0] exp ( - λ χτ=0[si(1 - 2Lsi)β2]}
(4.4)
In particular, F[ρτ] tends to zero if the sequence Piτ=0 si 1 - 2Lsi βi2 goes to positive infinity. For
an adaptive step size sτ ≤ 1/4L, this condition is met if, for example, βτ is chosen of the form
(τsτ)-2 while still satisfying (4.4). The noise perturbs the direction of descent, whereas the step
size determines how far to move along this perturbed direction. The noise level needs to be adjusted
so that the gradient is not too blurred, but it does not necessarily decrease at each iteration. When
the incumbent distribution ρτ is close to a local optimum, it is helpful to increase the noise level to
escape the local optimum. We demonstrate in Lemma A.5 of the Appendix that any positive definite
kernel k with bounded Hessian w.r.t. distance d is a Lipschitz-gradient kernel. On the other hand, the
detailed algorithm of scheme (4.3) are provided in the Appendix B.
5	Numerical Experiments
We evaluate the proposed gradient flow on real-world datasets and then illustrate its application to
augment samples for dataset of interest in transfer learning where only a few samples in the dataset
of interest are available.
We consider four datasets: the MNIST (M), Fashion-MNIST (F) and the Kanji-MNIST (K) datasets,
along with the USPS (U) dataset. All images are resized to 20 × 20, thus the feature space is of
dimension m = 400. To satisfy the Gaussianity assumption of the conditional distributions, we apply
K-means clustering to each dataset, and subsampling a smaller dataset using the biggest cluster.
We use t-distributed stochastic neighbor embedding (tSNE) as our mapping φ from Rm to R2 . To
compute the MMD distance using kernel embeddings, we use a tensor kernel on Z composed from
three standard Gaussian kernels corresponding for each component of the feature space R400, the
mean space R2 and the covariance matrix space S++. The kernel k is thus characteristic by (Szab6 &
Sriperumbudur, 2018, Theorem 4). Our algorithms and experiments are implemented in PyTorch. All
the experiments are run on a machine with a NVIDIA Tesla V100 GPU and an Intel Xeon E5-2690
6-core v4 CPU. Codes and data are available in the supplementary file.
5.1	Flows between Datasets
In the first set of experiments, we examine the path travelled by each particle from the source domain
to the target domain. To this end, we fix a source-domain pair, then we sample randomly N = 200
images equally for 10 classes of the source domain, and M = 50 images equally for 10 classes of
the target domain. The results of our flows are depicted in Fig. 2. In each subfigure, each column
represents a snapshot of a certain time-step and the samples flow from the source (left) to the target
(right). The number of iterations T that is used to generate the results in Fig. 2 is capped at 140.
5.2	Transfer Learning
One application of the gradient flow approach is to alleviate the problem of insufficient labeled data
by augmenting the target dataset with new samples. In this experiment, we demonstrate how new
target domain samples obtained from our gradient flows can be used in a transfer learning setting.
1See Definition A.3 in the Appendix for the technical definition of a Lipschitz-gradient kernel
8
Under review as a conference paper at ICLR 2022
time
ffπA.SJ0 一flαJ
6 4 南三G" P +/Jll
64 自a® M 7 中，/
•O6A5/CT7 夕了/
OGct6∕H79∕r
time
time
O∕C13 M∙ 56 7?夕
）、八Gg
time
time
。/4 3 4 SG 7S 个
。/JL3 々 s47r ,
O / 4 M4S474 彳
0 /Λzq $4，夕夕
G.>Λzqs,4夕V?
”，？ 3 3
4m q
MF	MK	FM	KM	UM
Figure 2: Sample path visualizations for different source-target domain combinations.
To this end, we fix a source domain, and pretrain a classifier P on this domain. This classifier is
using LeNet-5’s architecture. We also draw randomly N = 200 samples from the source domain
(equal size for each class) to form the source dataset (xi, yi)i2=001. Next, we pick a target domain and
draw randomly a few samples from this target domain: in 1-shot (resp. 5-shot) learning, only 1 image
(resp. 5 images) per class from the target domain is drawn to form the target dataset D = (Xj ,yjj )M=1.
We then perform a noisy gradient flow scheme (4.3) from the source dataset to the target dataset to
get 200 new samples ST = (xiT, yiT)i2=001. With the target dataset D and new samples ST, we can
retrain the classifier P with 10 epochs with Adam optimizer and learning rate 2 × 10-3. Similarly,
we can also train new networks from scratch using only D and ST. Finally, we test the classifiers on
the test set in the target domain. This process is replicated independently 10 times. We include more
details on implementation in Section B.5.
In Fig. 3, we present the accuracy of different transfer learning strategies using the new labelled
samples. D and D ∪ ST mean training a new classifier from scratch, whereas P means transferring
from the pretrained classifier. We observe a common trend that the addition of the new samples ST
always improves the accuracy of the classifiers. Both the 5-shot learning and 1-shot learning results
demonstrate similar relative order of accuracy among approaches. Moreover, the data augmentation
with ST leads to higher increase of accuracy for the 1-shot learning. We compare with Alvarez-Melis
& Fusi (2021) in transfer learning results and computation cost, see Section B.6.
Figure 3: Average target domain accuracy and error bars for transfer learning with one-shot (left) and
five-shot (right). Results are taken over 10 independent replications.
Concluding Remarks. This paper focuses on a gradient flow approach to generate new labelled data
samples in the target domain. To overcome the discrete nature of the label set, we represent datasets
as distributions on the feature-Gaussian space, and the flow is formulated to minimize an MMD loss
function under an optimal transport metric. Contrary to existing gradient flows on linear structure,
our flows are developed on the curved Riemannian manifold of Gaussian distributions. We provide
explicit formula for the (Riemannian) gradient of the MMD loss function, and examine in details the
flow equations and the convergence properties of both continuous and (noisy) discretized forms. The
numerical experiments demonstrate that our method can generate sensible labelled training data for
the target domain, and improve the classification accuracy in few-shot learning.
9
Under review as a conference paper at ICLR 2022
References
David Alvarez-Melis and Nicolo Fusi. Geometric dataset distances via optimal transport. In Advances
in Neural Information Processing Systems, volume 33,pp. 21428-21439, 2020.
David Alvarez-Melis and Nicold Fusi. Dataset dynamics via gradient flows in probability space. In
Proceedings of the 38th International Conference on Machine Learning, volume 139, pp. 219-230,
2021.
L.	Ambrosio, N. Gigli, and G. SaVar6. Gradient Flows in Metric Spaces and in the Space of
Probability Measures. Birkhauser Verlag, 2008.
M Arbel, A Gretton, W Li, and G Montufar. Kernelized Wasserstein natural gradient. In International
Conference on Learning Representations, 2020.
Michael Arbel, Anna Korba, Adil Salim, and Arthur Gretton. Maximum mean discrepancy gradient
flow. In Advances in Neural Information Processing Systems, volume 32, pp. 6481-6491, 2019.
Marc G. Bellemare, Ivo Danihelka, Will Dabney, Shakir Mohamed, Balaji Lakshminarayanan,
Stephan Hoyer, and Remi Munos. The Cramer distance as a solution to biased Wasserstein
gradients. In arXiv:1705.10743, 2017.
Shai Ben-David, John Blitzer, Koby Crammer, and Fernando Pereira. Analysis of representations for
domain adaptation. In Advances in Neural Information Processing Systems, volume 19, 2007.
Jean-David Benamou and Yann Brenier. A computational fluid mechanics solution to the monge-
kantorovich mass transfer problem. Numerische Mathematik, 84(3):375-393, 2000.
R. Bhatia, T. Jain, and Y. Lim. On the Bures-Wasserstein distance between positive definite matrices.
Expositiones Mathematicae, 37(2):165-191, 2019.
Rajendra Bhatia. Matrix Analysis. Springer, 1997.
Mikolaj Binkowski, Danica J. Sutherland, Michael Arbel, and Arthur Gretton. Demystifying MMD
GANs. In International Conference on Learning Representations, 2018.
Leon Bottou, Martin Arjovsky, David Lopez-Paz, and Maxime Oquab. Geometrical insights for
implicit generative modeling. In In Braverman Readings in Machine Learning, pp. 229-268, 2017.
Konstantinos Bousmalis, Alex Irpan, Paul Wohlhart, Yunfei Bai, Matthew Kelcey, Mrinal Kalakr-
ishnan, Laura Downs, Julian Ibarz, Peter Pastor Sampedro, Kurt Konolige, Sergey Levine, and
Vincent Vanhoucke. Using simulation and domain adaptation to improve efficiency of deep robotic
grasping. 2018. URL https://arxiv.org/abs/1709.07857.
Pratik Chaudhari, Adam M. Oberman, Stanley J. Osher, Stefano Soatto, and Guillaume Carlier. Deep
relaxation: partial differential equations for optimizing deep neural networks. CoRR, 2017.
Yifan Chen and Wuchen Li. Optimal transport natural gradient for statistical manifolds with continu-
ous sample space. In Information Geometry, 2018.
LenaiC Chizat. Sparse optimization on measures with over-parameterized gradient descent. In
arXiv:1907.10300, 2020.
Lenaic Chizat and Francis Bach. On the global convergence of gradient descent for over-parameterized
models using optimal transport. In Advances in Neural Information Processing Systems, volume 31,
2018.
Nicolas Courty, Remi Flamary, Amaury Habrard, and Alain Rakotomamonjy. Joint distribution
optimal transportation for domain adaptation. In Advances in Neural Information Processing
Systems, volume 30, 2017.
Bharath Bhushan Damodaran, Benjamin Kellenberger, Remi Flamary, Devis Tuia, and Nicolas Courty.
Deepjdot: Deep joint distribution optimal transport for unsupervised domain adaptation. CoRR,
abs/1803.10081, 2018.
10
Under review as a conference paper at ICLR 2022
A. Duncan, Nikolas Nusken, and Lukasz Szpruch. On the geometry of Stein variational gradient
descent. CoRR, 2019.
Chelsea Finn, Pieter Abbeel, and Sergey Levine. Model-agnostic meta-learning for fast adaptation of
deep networks. In Proceedings of the 34th International Conference on Machine Learning, pp.
1126-1135, 2017.
Charlie Frogner and Tomaso Poggio. Approximate inference with Wasserstein gradient flows. In
International Conference on Artificial Intelligence and Statistics, pp. 2581-2590. PMLR, 2020.
M.	Gelbrich. On a formula for the L2 Wasserstein metric between measures on Euclidean and Hilbert
spaces. Mathematische Nachrichten, 147(1):185-203, 1990.
Boqing Gong, Yuan Shi, Fei Sha, and Kristen Grauman. Geodesic flow kernel for unsupervised
domain adaptation. In 2012 IEEE Conference on Computer Vision and Pattern Recognition, pp.
2066-2073, 2012. doi: 10.1109/CVPR.2012.6247911.
Arthur Gretton, Karsten M. Borgwardt, Malte J. Rasch, Bernhard Scholkopf, and Alexander Smola.
A kernel two-sample test. Journal of Machine Learning Research, 13(25):723-773, 2012.
Caglar Gulcehre, Marcin Moczulski, Misha Denil, and Yoshua Bengio. Noisy activation functions.
In Proceedings of The 33rd International Conference on Machine Learning, pp. 3059-3068, 2016.
Caglar Gulcehre, Marcin Moczulski, Francesco Visin, and Yoshua Bengio. Mollifying networks. In
5th International Conference on Learning Representations, 2017.
Elad Hazan, Kfir Yehuda Levy, and Shai Shalev-Shwartz. On graduated optimization for stochastic
non-convex problems. In Proceedings of The 33rd International Conference on Machine Learning,
pp. 1833-1841, 2016.
Geoffrey Hinton, Nitish Srivastava, and Kevin Swersky. Neural networks for machine learning lecture
6a overview of mini-batch gradient descent. Cited on, 14(8):2, 2012.
R. Jordan, D. Kinderlehrer, and F. Otto. The variational formulation of the Fokker-Planck equation.
SIAM Journal on Mathematical Analysis, 29:1-17, 1998.
Mikhail Khodak, Maria-Florina F Balcan, and Ameet S Talwalkar. Adaptive gradient-based meta-
learning methods. In Advances in Neural Information Processing Systems, volume 32, 2019.
Soheil Kolouri, Kimia Nadjahi, Umut Simsekli, and Shahin Shahrampour. Generalized sliced
distances for probability distributions. arXiv prepritn arXiv:2002.12537, 2020.
John Lee. Introduction to Smooth Manifolds. Springer-Verlag, 2003.
Qiang Liu. Stein variational gradient descent as gradient flow. In Advances in Neural Information
Processing Systems, volume 30, 2017.
Qiang Liu and Dilin Wang. Stein variational gradient descent: A general purpose Bayesian inference
algorithm. In Advances in Neural Information Processing Systems, volume 29, 2016.
Antoine Liutkus, Umut Simsekli, Szymon Majewski, Alain Durmus, and Fabian-Robert Stoter.
Sliced-Wasserstein flows: Nonparametric generative modeling via optimal transport and diffusions.
In Proceedings of the 36th International Conference on Machine Learning, pp. 4104-4113, 2019.
Mingsheng Long, Han Zhu, Jianmin Wang, and Michael I. Jordan. Deep transfer learning with joint
adaptation networks. In Proceedings of the 34th International Conference on Machine Learning,
pp. 2208-2217, 2017.
L. Malago, L. Montrucchio, and G. Pistone. Wasserstein Riemannian geometry of Gaussian densities.
Information Geometry, 1(2):137-179, 2018.
Yishay Mansour, Mehryar Mohri, and Afshin Rostamizadeh. Domain adaptation with multiple
sources. In Advances in Neural Information Processing Systems, volume 21, 2009.
11
Under review as a conference paper at ICLR 2022
Youssef Mroueh and Truyen Nguyen. On the convergence of gradient descent in GANs: MMD GAN
as a gradient flow. In International Conference on Artificial Intelligence and Statistics, 2021.
Youssef Mroueh, Tom Sercu, and Anant Raj. Sobolev descent. In Proceedings of the Twenty-Second
International Conference OnArtificial Intelligence and Statistics, pp. 2976-2985, 2019.
F. Otto. The geometry of dissipative evolution equations: the porous medium equation. Comm.
Partial Differential Equations, 26:101-174, 2001.
Sinno Jialin Pan and Qiang Yang. A survey on transfer learning. IEEE Transactions on Knowledge
and Data Engineering, 22(10):1345-1359, 2010. doi: 10.1109/TKDE.2009.191.
Filippo Santambrogio. Optimal Transport for Applied Mathematicians: Calculus of Variations, PDEs
andModeling. Birkhauser, 2015.
Filippo Santambrogio. Euclidean, metric, and Wasserstein gradient flows: An overview. Bullentin of
Mathematical Sciences, 7:87-154, 2017.
Zoltdn Szab6 and Bharath K. Sriperumbudur. Characteristic and universal tensor product kernels.
Journal of Machine Learning Research, 18(233):1-29, 2018.
Yaniv Taigman, Adam Polyak, and Lior Wolf. Unsupervised cross-domain image generation. arXiv
preprint arXiv:1611.02200, 2016.
Asuka Takatsu. Wasserstein geometry of Gaussian measures. Osaka Journal of Mathematics, 48(4):
1005-1026, 2011.
Laurens van der Maaten and Geoffrey Hinton. Visualizing data using t-SNE. Journal of Machine
Learning Research, 9:2579-2605, 2008.
C. Villani. Optimal Transport: Old and New. Springer Science & Business Media, 2008.
Ting-Chun Wang, Ming-Yu Liu, Jun-Yan Zhu, Andrew Tao, Jan Kautz, and Bryan Catanzaro. High-
resolution image synthesis and semantic manipulation with conditional gans. In Proceedings of
the IEEE Conference on Computer Vision and Pattern Recognition, 2018.
Amir R. Zamir, Alexander Sax, William Shen, Leonidas J. Guibas, Jitendra Malik, and Silvio Savarese.
Taskonomy: Disentangling task transfer learning. In Proceedings of the IEEE Conference on
Computer Vision and Pattern Recognition (CVPR), June 2018.
12
Under review as a conference paper at ICLR 2022
The Appendix is organized into two parts. In Section A, we provide the proofs and further discussions
of the results in the main paper. Section B includes implementation details as well as additional nu-
merical results. All the models and data used to create the results in the paper are in the supplementary
file.
A	Proofs
A. 1 Proofs and Results related to Section 3
For Proposition 3.1. Recall that the Bures distance defined on Sn++ is
B(∑1, ∑2) = [tr(∑ι + ∑2 - 2[Σ2∑2∑2]1)]2,	(A.1)
and Vd^(z) is the unique element in the tangent space Rm X Rn X Sn satisfying
<Vd^(z), (w,v,V)}z	=	D夕z(w,v,V)	for all	(w,	v, V) ∈	Rm	X	Rn	X Sn.	(A.2)
The proof of Proposition 3.1 relies on the following result from (Takatsu, 2011, Proposition A) (see
also (Bhatia et al., 2019, Theorem 5) and (Malagd et al., 2018, Proposition 6)).
Proposition A.1. The space Sn++ is a Riemannian manifold with the following structures: at each
point Σ ∈ Sn++, the tangent space is TΣSn++ = Sn and the Riemannian metric is given by
hX,Yi∑ = tr(l∑[X]ΣL∑[Y]) = 2hL∑[X],Y〉for X,Y ∈ Sn.
Moreover, the distance function corresponding to this Riemannian metric coincides with the Bures
distance B given by (A.1).
We are now ready to prove Proposition 3.1.
Proof of Proposition 3.1. As a consequence of Proposition A.1, Z is the product Riemannian man-
ifold with tangent space T(χ,μ,∑)Z = TxRm X TμRn X T∑S++ and with the standard product
Riemmanian metric (3.3). The result then follows. We note that if we let D((χι, μι, ∑ι), (χ2,μ2, ∑2))
denote the distance function corresponding to the Riemannian metric〈•,，on Z, then its square
D((x1,μ1, ∑ι), (χ2,μ2, ∑2))2 is the sum of the square of the distance function w.r.t. standard metric
h∙, ∙i on Rm, the square of the distance function w.r.t. standard metrich∙,)on Rn, and the square
of the distance function w.r.t. metric(•，∙)∑ on S++. As a result and by Proposition A.1, we have
D((x1,μ1, ∑ι), (x2,μ2, ∑2))2 = kxι-X2k2 + kμι-μ2∣∣2 +B(∑1, ∑2)2. So, D is the same as d. □
For Lemma 3.2
ProofofLemma 3.2. Let us express Vd夕(Z) = (Φι(z), Φ2(z), Φ3(z)) with Φι(z) ∈ Rm, Φ2(z) ∈
Rn and Φ3(z) ∈ Sn. Then by using the definition of Riemannian metric〈•，，in (3.3), we can
rewrite equation (A.2) as
hφι(z),vi + hφ2(z),wi + hIL∑[φ3(z)],Vi = hvφ(z), (v,w,V)i.
This is equivalent to
hφ1 (z),vi + hφ2(z),wi +h2L∑[φ3(z)],vi = hvxφ(z),v''i + hvμψ(z),w'i+ hv∑φ(z),vi, (A.3)
where V夕(Z) = (Vx夕(z), Vμ夕(z), V∑夕(Z)) denotes the standard Euclidean gradient. Equation
(A.3) is obviously satisfied if Φι(z) = Vx夕(z), Φ2(z) = Vμ夕(z),and L∑[Φ3(z)] = 2V∑夕(z). By
the definition of operator L∑ right after (3.2), the third identity is the same as Φ3(z) = 2[V∑夕(z)]Σ +
2Σ[V∑夕(z)]. Due to uniqueness of the gradient, we therefore infer that Vd夕(Z) is given by the
formula:
Vd2(Z) = (VxP(z), VμP(z), 2[V∑2(z)]Σ + 2Σ[V∑2(z)]).
This completes the proof.	□
13
Under review as a conference paper at ICLR 2022
In this paper, the optimal transport metric between any two distributions ρ0, ρ1 ∈ P(Z) is defined by
W(ρ0, ρ1)2
inf	d(z0, z1)2 π(dz0, dz1),
(A.4)
where Π(ρ0, ρ1) is the set of all probability distributions on Z × Z whose marginals are ρ0 and ρ1,
respectively.
A.2 Proofs and Results related to Section 4
The maximum mean discrepancy (MMD) between a distribution ρ ∈ P(Z) and the target distribution
% is defined as
MMD(ρ, %) :
sup
f ∈H-kf kH≤1
n	f (z) ρ(dz) -	f(z)%(dz)o.
It is well-known that the MMD admits the following closed-form formula (Gretton et al., 2012,
Lemmas 4 and 6).
Lemma A.2. We have MMD(ρ, %) = kmρ - m%kH. As a consequence,
MMD(ρ, %)2 =	k(z, w)ρ(dz)ρ(dw) - 2	m%(z)ρ(dz) + km%k2H.
ProofofLemma A.2. For any f ∈ H, we have f(z) = hf, k(∙, z)}h. Therefore,
( f (z) ρ(dz) = (f, / k(∙, Z) P(dz))^ = hf, mρiH for all f ∈H
(A.5)
It follows that MMD(ρ, %) = SUpf∈H^∣f kH≤ιhf, m0 - m%)H = ∣∣m0 - m%∣∣H∙ Using this closed-
form formula and identity (A.5), we also obtain
MMD(ρ, %)2 = kmρ - m%k2H = hmρ,mρiH - 2hmρ,m%iH + hm%,m%iH
=	mρ(z)ρ(dz) - 2	m%(z)ρ(dz) + km%k2H
=	k(z, w)ρ(dz)ρ(dw) - 2	m%(z)ρ(dz) + km%k2H.
This completes the proof.
□
For Lemma 4.1
Proof of Lemma 4.1. We recall that grad F [ρ] is defined as the unique element in TρP(Z) satisfying
gρ (grad F [ρ], ∂tρt∣t=0) = -d∣	F [ρt]
dt t=0
for every differentiable curve t 7→ ρt ∈ P(Z) passing through ρ at t = 0. Let t 7→ ρt ∈ P(Z) be
such a curve. Then since ∂tρt∣t=o ∈ TPP(Z), We can write ∂tρt ∣t=o = -divd(ρVd^) for some
differentiable function 夕 on Z. Then by using Lemma A.2 and k(z, W) = k(w, Z) we have
d∣	1d∣
dt ∣t=o [ρt] = 2dt∣t=o
Jl
k(Z, w)ρt (dZ)ρt (dw) - 2
m%(Z)ρt(dZ)
2 JJ k(z,w)∂tρt∣t=o(dz)ρ(dw) + 2 JJ k(z,w)∂tρt∣t=0(dw)ρ(dz)
一/ m%(z) ∂tρt∣t=o(dz)
-J / k(z,w)diVd(PVd夕)(dz) ρ(dw)+/ m%(z)divd(ρVd夕)(dz).
14
Under review as a conference paper at ICLR 2022
Let V1dk(z, w) denote the gradient Vd of the function z 7→ k(z, w. It then follows from the definition
of the divergence operator divd(ρVd夕)at the end of Section 3.1 that
d||t=oF[pt] = L LhV%k(z, w), VdHz)iz ρ(dZ)ρ(dW)- LhVdm%(z), VdO(Z)izρ(dZ)
D
Z
V1dk(Z, w) P(dw), VdO(Z)	P(dZ) -
k(Z, w) P(dw), VdO(Z)	P(dZ) -
/ hVdm%(z), Vd夕(z)izρ(dz)
Z
/ (Vdm%(z), Vd夕(z)izρ(dz)
Z
=/ hVd[mρ - m%](z), Vd夕(z)izρ(dz).
Z
By the definition of the Riemannian metric tensor gρ given in (3.9) and due to ∂tρt∣t=o
-divd(ρVd夕)，we thus obtain
ddtLF [ρt] = gρ
gρ
(-divd(ρVd[mρ -
(-divd(ρVd[mρ -
m%
m%
,]),-divd(ρVd⑺
], ∂tρt |t=0 .
Therefore, we infer that grad F[ρ] = -divd ρVd[mρ - m%] as desired.
□
For Proposition 4.2
Proof of Proposition 4.2. The proof is similar to that of Lemma 4.1 and with the same notation for
V1dk(z, w). Indeed, by the same computation at the beginning of the proof of Lemma 4.1 we have
ddt F [ρt]
2 JJ k(z, w)∂tpt(dz)pt(dw) + 2 JJ k(z,w)∂tpt(dw)pt(dz) - / m%(z) ∂tpt(dz)
k(Z, w)∂tρt(dZ)ρt(dw) -	m% (Z) ∂tρt(dZ).
This together with the gradient flow equation (4.1) gives
d| F [Pt] = / / k(z,w)divd(ptVdmρt - m%])(dz)pt(dw)
-	I m%(z)divd(ptVd[mρt - m%])(dz).
Z
Using the definition of the divergence operator divd at the end of Section 3.1, we further obtain
ddt F [pt] = -JJF%k(z,w), Vd[mpt- m%](Z)iz Ptmz)Ptmw)
+ hVdm%(Z), Vd[mρt - m%](Z)izρt(dZ)
Z
-	D	V1dk(z, w) ρt(dw), Vd[mρt - m%
+ hVdm%(Z), Vd[mρt - m%](Z)izρt(dZ)
Z
-	hVdmρt(Z),Vd[mρt - m%](Z)izρt(dZ)
Z
+ hVdm%(Z), Vd[mρt - m%](Z)izρt(dZ)
Z
-	kVd[mρt - m%](Z)kz2ρt(dZ).
Z
z
This yields the desired result.
□
15
Under review as a conference paper at ICLR 2022
For Lemma 4.3
Proof of Lemma 4.3. From the formula for expz (εΦ(z)) given at the beginning of Section 4.1, we
observe that
-d|	exPz(εΦ(z)) = (φι(z), Φ2(z),L∑[Φ3(z)] ∑ + ∑L∑[Φ3(z)])
dε ε=0
= Φ1(z), Φ2(z), Φ3(z) = Φ(z),	(A.6)
where the second equality is due to the definition of LΣ [V ] given at the beginning of Section 3.1.
We obtain from Lemma A.2 that
MMD(exp(εΦ%PT, %)2
=〃 k(z,w) exp(εΦ)#pT(dz) exp(εΦ^PT(dw) - 2 / m%(z) exp(εΦ^PT(dz) + ∣∣m%kH
expz (εΦ(z)), expw(εΦ(w))ρτ (dz)ρτ (dw) - 2	m%expz(εΦ(z))ρτ(dz) + km%k2H.
Moreover, we have
U' dε∣e=0 hk(exPz (绛(Z)), exPw (εφ(w)))i ρτ az" (dw)
dε lε=0
Lohk(exPz (εΦ(z)),w)i + α[一/网4 exPw (εΦ(w)))] j P (dz)ρτ (dw)
,h∕ k(expz (εΦ(z)), w)ρτ (dw)]PT (dz) + / α1//k(z, exPw (εΦ(w)))ρτ (dz)]PT (dw)
=/ diε∣ε=0 hmρτ (exPζ (εφ(z)))i pt (dz) + / dε Lo Imp’(exPw (<^(w)))iPT (dw)
=2 Z dε Lo Imp’(exPz (εφ(z)))] pt (dz).
Thus, it follows that
-d∣	MMD(exp(εΦ^PT ,%)2
dε ε=0
=2 Z dεLohmpτ(exPz (εφ(z)))]Pτ (dz)- 2 Z dεL=ohm%(exPz (εφ(z)))i pt (dz)
=2/ DmPT - m%]ζ (dεLexPz (εφ(z)))Pτ (dz)
with D夕z(w,, v, V) denoting the standard directional derivative of 夕 at z in the direction (w, v, V).
Using the definition of F together with (A.6) and the definition of gradient Nd in (A.2), we obtain
⅛∣	F [exP(捶)#PT] = /
dε ε=0
=Z
This yields the first conclusion of the lemma.
D[mρτ - m%]z(Φ(z)) ρτ(dz)
hyd[mρτ - m%](z), Φ(z)izP(dz).
(A.7)
Now let ΦT := 八中’：：---be the unit vector field in the direction of Φτ := -Vd[mpτ - m%], Then
by (A.7), we have
M-F[exp(εΦT)#pt] = -∣Φτk-^τ)∣ ∣Φτ(z)∣2pt(dz) = -∣Φτ∣L2(p’) ≤ 0.
On the other hand, for any unit direction Φ in L2 (PT) we obtain from (A.7) and Holder inequality
that
|孔"[exp(εΦ"]∣ ≤ / ∣Φτ(z)∣z∣Φ(z)∣zPT(dz)
≤ (/ ∣Φτ(z)k2Pτ(dz))1 (Z ∣Φ(z)kZPτ(dz))2 = kΦτ∣L2(pτ).
16
Under review as a conference paper at ICLR 2022
Therefore, we conclude further that
-d∣ F[exp(εφT)#pT] ≤ -d∣ F[eχp(εφ)#pt]
dε ε=0	dε ε=0
for any unit direction Φ in L2 (ρτ). These give the last conclusion of the lemma.	口
Definition A.3 (Lipschitz-gradient kernel). Let L > 0. A differentiable kernel k on Z is called a
Lipschitz-gradient kernel with constant L if there exists a number ε0 ∈ (0, 1) such that
∣k(exPz(εΦ(z)), exPw(δΦ(w))) - k(z, w) - [(Vdk(z, w), εΦ(z)iz + (Vdk(z, w), δΦ(w)iw] ∣
≤ LhkεΦ(z)kz2 + kδΦ(w)k2wi	(A.8)
for every ε, δ ∈ [0, ε0] and every bounded vector field Φ : Z → Rm × Rn × Sn. Hereafter, V1dk(z, w)
and V2dk(z, w) denote respectively the gradient Vd of the function z 7→ k(z, w) and the function
w 7→ k(z, w).
Remark A.4. The right hand side of condition (A.8) can be expressed in terms of the d distance as
d( exp% (εΦ(z)),z)2 + d( expw (δΦ(w)),w)2.
Thus condition (A.8) can be interpreted as the gradient Vdk is Lipschitz w.r.t. the distance d.
Condition (A.8) is motivated by the following observation in the Euclidean space. Assume that
G : Rd × Rd → R is a differentiable function such that its Euclidean gradient VG(z, w) :=
(V1G(z, w), V2G(z, w)) satisfies the standard Lipschitz condition
kVG(z1, w1) - VG(z2, w2)k2 ≤ Lk(z1, w1) - (z2,w2)k2 ∀(z1,w1), (z2, w2) ∈ Rd × Rd.
Then for any point (z, w) ∈ Rd × Rd and any tangent vector (u, v) ∈ Rd × Rd, we have by using the
mean value theorem that G(z + u, w +v) - G(z, w) = hVG(z0, w0), (u, v)i for some point (z0, w0)
in the line segment in Rd × Rd connecting the points (z, w) and (z + u, w + v). As a consequence,
we obtain
∣∣G(z + u, w + v) - G(z, w) - hV1 G(z, w), ui + hV2G(z, w), vi]∣∣
= ∣∣hVG(z0,w0), (u, v)i - hVG(z, w), (u, v)i∣∣
= ∣∣∣VG(z0, w0) - VG(z, w), (u, v)∣∣∣ ≤ kVG(z0, w0) - VG(z, w)k2k(u, v)k2.
Then we can use the Lipschitz condition for VG to imply further that
∣∣G(z + u, w + v) - G(z, w) - hV1 G(z, w), ui + hV2G(z, w), vi]∣∣
≤ Lk(z0,w0) - (z, w)k2k(u, v)k2 ≤ Lk(u,v)k22,
which is the same as
∣∣∣G(z + u, w + v) - G(z, w) - hV1G(z, w), ui + hV2G(z, w), vi]∣∣∣ ≤ Lkuk22 + kvk22].
Condition (A.8) is the Riemannian version of this last inequality for the Euclidean space, which is a
consequence of the standard Lipschitz condition for the gradient.
Bounded Hessian kernels are Lipschitz-gradient. The following lemma gives a sufficient condi-
tion for a kernel to be Lipschitz-gradient.
Lemma A.5. Let k be a positive definite kernel such that its Hessian w.r.t. distance d is bounded.
Then k is a Lipschitz-gradient kernel.
Proof of Lemma A.5. Let Hd1k(z, w) and Hd2k(z, w) denote respectively the Hessian w.r.t. distance d
of the function z 7→ k(z, w) and the function w 7→ k(z, w). Let ε, δ > 0, and Φ : Z → Rm×Rn ×Sn
17
Under review as a conference paper at ICLR 2022
be a bounded vector field. Define γz(t) := exp% (tΦ(z)) and θw (t) := expw (ε tΦ(w)) for t ∈ [0,ε].
Then we have
εd
k(exPz(εΦ(z)), exPw(δΦ(w))) - k(z,w) = J 否[k(γz(t),θw(t))]dt
hVdk(γz (t),θw (t)),Y z(t)iγz(t) + (Vdk(γz(t),θw (t)),θw(t)iθw(t)]]dt.
This together with the facts that γz(0) = Φ(ζ) and Ow(0) = :Φ(z) yields
A := k(expz(εΦ(z)), expw(δΦ(w))) - k(z, w) - hV1dk(z, w), εΦ(z)iz + hV2dk(z, w), δΦ(w)iw
=([hVdk(γz(t),θw(t)),γz(t)iγz(t) -hVdk(γz(0),θw(0)),Yz(0)iγz(0)]dt
+ ∕ε [hVdk(Yz(t),θw(t)),θw(t)iθw(t) -hVdk(Yz(0),θw(0)),θw(0)iθw(0)]]dt
hVdk(Yz (S),θw (S)),γZ(S)iYz(s)idSdt
dSdt
+ hVdk(Yz(S),θW (S)),YZ(S)iYz(s)] dsdt
(s) + hVdk(Yz(S),θw (S)),θw (S)iθw (s)] dsdt.
Since the curve S → γz(s) is a geodesic, its acceleration γz(s) is orthogonal to Z (that is, γz (s) is
orthogonal to every tangent vector in TzZ). This implies that hVdk(γz(s), θw(s)),Yz(s))yz(s)= 0.
Likewise, we also have hVd2k(Yz(S), θw(S)), θw(S)iθw(s) = 0. Thanks to these, we deduce from the
By using the assumption that the Hessians Hd1 and Hd2 are bounded, we then obtain
|A| ≤ M(/IkYz(S)kYz(s) + kθw(S)kθw(s)]dSdt,
where M is the sup norm of the Hessian of k. But as Yz(S) and θw(S) are geodesic, they have constant
SPeedS. Therefore, kY Z(S)IlYz(S) = IIY Z(O)kγz(0) = kφ(Z)Ilz and kθw (S)kθw (S) = kθw (O)kθw (0) =
k εΦ(ζ)kz. Using these, We infer further that
|A| ≤ M(∕t h∣Φ(ζ)kz + (δ)2kΦ(ζ)k2]dSdt = M h∣εΦ(z)kz + kδΦ(w)kw].
According to Definition A.3, we thus conclude that k is a Lipschitz-gradient kernel with constant
M/2.	□
Quantified estimate of decrease for the Riemannian forward Euler scheme (4.2). The next
result quantifies the amount that the value of F decreases after each iteration.
Proposition A.6 (Quantified estimate of decrease). Suppose that k is a Lipschitz-gradient kernel
with constant L. Then for ρτ+1 given by (4.2) with Sτ ∈ (0, ε0], we have
F[ρτ+1]-F[ρτ]≤-Sτ
(1 - 2Lst) /
IVd[mρτ - m%](z)Iz2ρτ(dz).
18
UlIderreVieW as a COnferenCe PaPer at ICLR 2022
PrOofofPrOPOS 三 OnA 6 LeteTu —v-mpT — m-∙ Then from the computation at the beginning
Ofthe PrOOf Of Lemma 4∙3 and by USg Lemma A.-we Obta
I J NMD (eXP(STqrPJ 2)2— MMDE 2)2)
=±.一ʌeXPZ(STeTSLeXPMsτeτs))) — ∕cyΛpτ(dz≈(ds
I = (A: (expzFeτsLʌ—/Cys)pτ(dz)2(du7)∙
MoreOVeL We have
.一<Jmp; mJ (ʌeT(Z)IPT (dz)
==<≡ZWLeτs1pτ(dz)pT(CM — a<≡ZWLeτs1pτ(dz)2(CM
H =‹vys鼻(Z)IPT (dz≈(CM + =<*yu7Leτsrpτ(dz≈(ds
I=(v^ys一 eτ(z)1pτ(dz)2(ds2
Where the IaSt equality is due to the Symmetry Of k and relation (3.6Here vW) and vW)
respectiveIy denote the gradient VdOfthe function N F÷NW) and F÷NWThereforit
fiows that
J'QJI TQ 一 — s」<d≡p; mj(ʌeT(Z)IPT (dz)
=ʌeXPZ(STeT(Z)LeXPM STeTS))) — ∕cys
IK<kyS- STeTS1 +<τyW) “ STeTsr一) PT (dz≈(ds
I = (ʌexp」STeT(Z))W) Lysl<5r(ZWL STeT(Z)Ix(dz)2(CM
AS ST m (Oj SOLWe Can now USe the assumption that RiSa LiPSChitZ—gradient kernel With COnStant
to ObtaiIl
TQJ—7Q 一+ ST - =qsvτ(dz)
m =Fqs=v=sτqs)="x(dw(CM + L = =STq(Z)vτ (Ch) 2(CM
Hl 一 =q(gf T(Ch) ∙
ThiS gives
⅞+f ⅛ ≤ ( — W + 2 1)L =qs=≈τ(dz)∙
and the COIlClUSion Ofthe PrOPOSitn fll
ConVergenCe guarantee-For each distribution P on 3 Iet 0 最→W be the linear OPem 一
defined by IKPj(IVl) K〈两~"17 ∙L j(∙))EWith 网二NXNJ^being given by
1W2) U / (v*ILV*2)τdz) for U^U⅛ mg∙
The IIeXtreSUIt gives SOme basic prOPertieS Ofthe OPemtOr 0∙
Lemma A∙FOr a dveren-iable kernk and for P m p(We have
j(w H j%WksW)Zdf (Z p(dfor f m M
19
□
Under review as a conference paper at ICLR 2022
ii) hKρf, g)H = JZ"df, Vdgiz ρ(dz) for every f, g ∈ H. Consequently, the operator KP is
symmetric and positive, and hence its spectrum is contained in [0, +∞).
ProofofLemma A.7. By using the definition of the Riemannian metric《，∙)z given in (3.3), it can
be verified for f ∈ H that
《Vdk(Z,W), Vdk(Z, ∙)iz ,f(»丸=(Vdk(Z,W), hvdk(z, ∙),f(∙)) H) J
AS f(z) = hk(z, ∙),f (∙)iH, we moreover have Vdf(Z) = "dk(z, ∙), f (•))％. Therefore,
《Vdk(Z, W), Vdk(Z, ∙)iz,f ⑺〉丸=〈Vdk(Z, W), Ndf(Z))z.	(A切
Using the definition of Kρ and (A.9), we obtain
Kρf(W)
hVdk(Z, w), Vdk(Z, ∙)iz p(dZ),f (∙))
H
/ DhVdk(Z,w), Vdk(Z, ∙)iz ,f(∙)) 丸 p(dZ)
V1dk(Z, W), Vdf(Z)z ρ(dZ),
which gives i). Now for f, g ∈ H, we can use part i) and similar arguments leading to (A.9) to obtain
hκPf∙,gH = D /〈Vdk(Z, ∙), Vdf(Z)〉zp(dZ),g(-)E 丸
=/ DhVdk(Z,∙), Vdf(Z)iz,g(∙)E丸ρ(dZ)
=/ DhVdk(Z, ∙),g(∙)iH, Vdf(Z)Ez p(dZ) = /(Vdg(Z), Vdf(Z)〉zp(dZ).
This implies in particular that the operator Kρ is symmetric (i.e. hKρf, giH = hKρg, fiH for
f, g ∈ H) and positive (i.e. hKρf, fiH ≥ 0 for f ∈ H). Since any symmetric, positive, and linear
operator must have nonnegative eigenvalues, We have completed the proof.	□
Our next result gives a quantified decay rate for the objective function.
Proposition A.8 (Objective value decay). There hold:
i)	Let ρt be given by (4.1), and let λt ≥ 0 be any constant satisfying
hKρtft, ftiH ≥ λtkftk2H with ft := mρt - m%.	(A.10)
Then F[ρt] ≤ F[ρo] exp ( — 2 Rt λsds) for any t ≥ 0. In particular, limt→∞ MMD(ρt, %) = 0
if R0∞ λt dt = +∞.
ii)	Let ρτ be given by scheme (4.2), and λτ ≥ 0 be any constant satisfying
hKρtfτ,fτ〉H ≥ λτkfτ∣H with fτ := mρτ — m%.
Assume that k is a Lipschitz-gradient kernel and step size sτ satisfies sτλτ < 1, then we have
F[ρτ+1] ≤ F[ρ0] exp ( — PT=0Siλi) for any T ≥ 0. In particular, lim「→∞ MMD(PT, %) = 0
if Pτ∞=0 sτ λτ = +∞.
Condition 0∞ λt dt = +∞ guaranteeing the convergence in MMD holds true for example if
λt ≥ ct-1 for some constant c > 0 and for large t. We note also that Condition (A.10) is satisfied
if λt is chosen to be the minimum eigenvalue of operator Kρt . Thus Proposition A.8 implies in
particular that ρt globally converges in MMD if the minimum eigenvalue λt of operator Kρt satisfies
the integrability condition 0∞ λt dt = +∞. The proof of Proposition A.8 relies on the following
proposition, which shows that the dynamic of the mean embedding is governed by the equation
dt(mPt ― m%) = -KPt (mPt - m%).
20
Under review as a conference paper at ICLR 2022
Proposition A.9 (Dynamic of the mean embedding). Let t ∈ [0, ∞) 7-→ ρt be the gradient flow
given by equation (4.1). For each t ≥ 0, take ft := mρt - m%. Then ft is a solution of the linear
partial differential equation
∂tft = -Kρt ft	in	[0, ∞) × Z.
(A.11)
Proof of Proposition A.9. From the definition of the mean embedding and by using equation (4.1),
we have
∂tft(w) = ∂tmρt (w) = ∂t	k(z, w) ρt(dz) =	k(z, w) ∂tρt(dz)
=	k(z, w) divd(ρtvdft)(dz).
Z
Using the definition of the divergence operator divd at the end of Section 3.1, we further obtain
∂tft(w)
-Zh "dk(z'w), vdft(z)iz ρt(dz)
It then follows from part i) of Lemma A.7 that ∂tft(w) = -KPt ft(w). This completes the proof. □
We are now ready to present the proof of Proposition A.8.
Proof of Proposition A.8. Let ft := mρt - m%. Then we have from Proposition 4.2 and part ii) of
Lemma A.7 that∂tkftk2H = -2hKρtft,ftiH. But as
hKρtft,ftiH ≥λtkftk2H
by Condition A.10, we infer that ∂tkftk2H ≤ -2λtkftk2H, and hence ∂t log kftk2H ≤ -2λt. By
integrating from 0 to t, one gets log kftk2H - log kf0k2H ≤ -2 R0t λs ds. We next take exponential to
obtain
kftk2H ≤ kf0k2H exp - 2 Z λs ds.
This can be rewritten as F[ρt] ≤ F[ρo] exp ( - 2 Rt X§ds) for t ≥ 0. In particular, F[ρt] (and hence
MMD(ρt, %)) tends to zero if 0∞ λt dt = +∞. This completes the proof for part i).
To prove ii), let fτ := mρτ - m%. Notice that in contrast to the continuous case, upper indices
are used for fτ and ρτ in the discrete case. Then by using Proposition A.6 together with part ii) of
Lemma A.7 and the assumption s「∈ (0, 4L] we have
F[ρτ+1]-F[ρτ] ≤ — 1 s"Kρτfτ,fτiH.
But as hKρτ fτ, fτiH ≥ λτ kfτ k2H due to our assumption, we obtain F[ρτ+1] - F[ρτ] ≤
-sτ λτ F[ρτ ], or
F[ρτ+1] ≤(1-sτλτ)F[ρτ]
τ
for every τ ≥ 0. As 1 - sτλτ > 0, it follows by iteration that F[ρτ+1] ≤ F [ρ0]Y(1 - siλi). Due
i=0
τ
to 1 — X ≤ exp(-x) for every X ≥ 0, we infer that F[ρτ +1] ≤ F[ρ0] exp ( — ɪ2s^i) for T ≥ 0.
i=0
∞
In particular, F[ρτ] (and hence MMD(ρτ, %)) tends to zero if	sτλτ = +∞.
τ=0
□
21
Under review as a conference paper at ICLR 2022
For Proposition 4.4
Proof of Proposition 4.4. Let h(z) := expz (sτ Φτ (z)) for z ∈ Z. Then Pτ+1 can be expressed as
Pτ+1 =九#?厂%=(h ◦ feT)#(PT X g).
By the computation at the beginning of the proof of Lemma 4.3 using Lemma A.2, we obtain
F [ρτ+1]-F [ρτ] = 1 [MMD((h ◦ fβτ )#(pT X g),%) - MMD(PT ,%)2
=2 //// {k(h(f βτ(z,u)),h(fβτ (w,v))) — k(z,w)Oρτ (dz)g(du)ρτ (dw)g(dv)
-	nkh(f βτ (z, u)), w - k(z, w)oρτ (dz)g(du)%(dw).
Moreover, we have
I :=	hVd[mρτ - m%](z), Φτ (z)izρτ,βτ (dz)
=	hV1dk(z, w), Φτ (z)iz ρτ,βτ (dz)ρτ (dw) -	hV1dk(z, w), Φτ (z)iz ρτ,βτ (dz)%(dw)
=2 JJ hVdk(z,w), Φτ (z)iz ρτ,βτ (dz)ρτ (dw) + 2 〃 Vk(z,w), Φτ (Wy)WPTe (dw)ρτ (dz)
-	hV1dk(z, w), Φτ(z)izρτ,βτ (dz)%(dw)
=2 ZZZ Dvdk(fβτ (z,u),w), Φτ (fβτ (z,u))E …∕τ (dz)g(dU)PT (dw)
+	2 JJJ rv∣kaf βτ(w,v)), φτ(f βτ (w,v))E∕β,Q U)PT(dw)g(dV)PT(dz)
-	DV1dk(f βτ (z, U), w), Φτ(fβτ(z,U))E β	Pτ (dz)g(dU)%(dw),
where the third equality is due to the symmetry of k and relation (3.6). Therefore, it follows that
F[Pτ+1] - F[Pτ] -sτI
=2 HZ {k(h(fβτ (z,u)),h(fβτ (w,v)))- k(z,w)
-hhV1dk(f βτ (z, U), w), sτΦτ(fβτ(z,U))ifβτ(z,u)
+hV2dk(z, f βτ (w, v)), sτ Φτ (f βτ (w, v))ifβτ (w,v)io Pτ (dz)g(dU)Pτ (dw)g(dv)
—
{k(hf βτ (z,u)),w - k(z,w) - hVdk(f βτ (z,u),w),sτ φτ (f βτ (Z, U))ifβτ (z,u) O
Pτ (dz)g(dU)%(dw).
As h(z) = expz(sτΦτ(z)) and sτ ∈ (0, ε0], we can now use the Lipschitz-gradient condition (A.8)
for k to obtain
F[Pτ+1] - F[Pτ] - sτI
≤ L jjjj [ksτ Φτ (fβτ (z,u))kfβτ (z,u) + ksτ Φτ (f βτ (w, v))kf2
βτ (w,v)iPτ (dz)g(dU)Pτ (dw)g(dv)
+L	ksτ Φτ (f βτ (z, U))kf2βτ (z,u)Pτ (dz)%(dw)g(dU)
= 2Lsτ2	kΦτ (f βτ (z, U))kf2βτ (z,u)Pτ (dz)g(dU).
22
Under review as a conference paper at ICLR 2022
Using the definition ρτ,βτ = f#T (ρτ 0 g) and the fact I = - RZ ∣∣Φτ(z)∣∣2 ρτ,βτ (dz), we can rewrite
this more compactly as
F[ρτ+1] - F[Pτ] ≤ -st。- 2Lsτ) L ©(z)∣2 Pτ,βτ(dz)
—sτ(1 — 2Lsτ) /
IlVdmPT- m%](Z)Il2ρτ,βτ(dZ).
This together with condition (4.4) gives
F[ρτ+1] ≤ (1 - aτ)F[ρτ] with a, ：= λs,(l - 2Lsτ)βT∙
In particular, we must have ai ≤ 1. By iterating this estimate, we obtain
τ
F[ρτ+1] ≤F[ρ0] Y(1 - ai).	(A.12)
i=0
Due to 1 - x ≤ exp(-x) for every number x ≥ 0, we get Qiτ=0(1 - ai) ≤ exp(- Piτ=0 ai). This
together with (A.12) yields the conclusion of the proposition.	□
B	Implementation and Experiment Details
We use VBk(χ, μ, Σ, W) to denote the last component in (3.6) for the gradient Vd of the function
(x, μ, Σ) → k(x, μ, Σ, w). Precisely,
VBk(x, μ, Σ, w) := 2[V∑k(x, μ, Σ, w)]Σ + 2Σ[V∑k(x, μ, Σ, w)].
B.1 Algorithms
Algorithm 2 Discretized Gradient Flow Algorithm for Scheme (4.2) - Detailed Version of Algo-
rithm 1
InPUt： a SOUrCe distribution ρ0 = N P=i %0,“”0), a sample 焉 PM=I 殆加亦,)for the
target distribution %, a number T of iterations for training, a sequence of step sizes sτ > 0 with
τ = 0, 1, ..., T , and a kernel k.
Initialization：
1M
Compute (Ψι, Ψ2, Ψ3)(x, μ, ∑) = M E(Vx, Vμ, VB)k(χ, μ, Σ, Xj,μj, Σj)
j=1
T — 0
while τ < T do
1N
Compute (Ψ,, ψ, Ψ,)(x, μ, Σ) = N E(Vx, V“, VB)k(x, μ, Σ, xτ, μτ, Στ)
i=1
for i = 1, . . . , N do
xτ+1 — xτ + sτ(ψ 1 - ψt)(xτ,μi, ςt)
μτ+1 一 μτ + sτ(Ψ2 - ΨT)(χT,μτ, ∑T)
∑τ+1 一(I + SτL∑τ [(Ψ3 - ψt)(χτ, μτ, ∑τ)]) ∑τ(I + STL∑τ [(ψ3 - ψt)(χT, μT, ∑T)])
end for
Set τ — τ + 1
end while
T 1N
Output: P = N∑δ(xT,μT,∑T)
23
Under review as a conference paper at ICLR 2022
Algorithm 3 Discretized Gradient Flow Algorithm for Scheme (4.3)
Input: a source distribution ρ0 = 焉 PN=I 6曜,*亦力 a target distribution % =
M PM=I δ(% μj,∑j), number of iterations T, step sizes s「 > 0, noise levels βτ, and a kernel
k.
Initialization:
1M
Compute (Ψι, Ψ2, Ψ3)(x, μ, ∑) = M £(Vx, ▽*, VB)k(x, μ, Σ, Xj,μj, Σj)
j=1
T J 0
while τ < T do
1N
Compute 函,遛,Ψg)(x, μ, Σ) = N E(Vχ, V“, VB)k(x, μ, Σ, xj, μj, ∑)
j =1
for i = 1, . . . , N do
Perturb XTP J— XT + βτNRm (0,1) and μT* J- μτ + βτNRn (0,1)
SetSJβτNSn(0,1) and perturb Σiτ,p J (I+LΣiτ[S])Σiτ(I+LΣiτ[S])
XT+1 J xT,p + Sτ(Ψ1 - ΨT)(xT,p,μT,p, ∑T,p)	‘
μT+1 J μT,p + Sτ(Ψ2 — ΨT)(xT,p,μτ,p, ∑T,p)
∑T+1	J	(I	+	SτL∑τ,p [(Ψ3	-	ΨT)(χT,p,μT,p, ∑T,p)])∑T,p(I	+	StL∑τ,p [(Ψ3	—
ΨT)(xT,p,μT,p, ∑T,p)])
end for
Set τ J τ + 1
end while
T 1N
OUtPUt：P = N^δ(χT ,μT ,∑T)
B.2 Kernel and Its Gradient for Implementation
We use the kernel k given by:
k ((x, μ, ∑), (X, μ, ∑)) := exp (—α∣∣x — X∣∣2) exp (—β∣∣μ — μ∣∣2) exp (—γk∑ — ∑∣∣2),
where α, β and γ are parameters (bandwidth) of the kernel. We note that this kernel is characteristic
by (SZab6 & Sriperumbudur, 2018, Theorem 4). Then its standard Euclidean gradient is given by
v(x,μ,Σ)k ((x, μ, ςb (x, μ,刈)=—2eχp ( —akX - Xk2 — β kμ - μk2 - Ykς — ς k2
α(X — X)
β(μ - μ) .
γ(Σ — Σ)
Thus by plugging into formula (3.6), we obtain
vdk ((X,〃, ς), (x, μ, ς))
α(X — X)
=-2exp (—akX - Xk2 - βkμ - μk2 -Yi∣ς — ςk2)	β(μ - μ) _	.
2γ(2Σ2 — ΣΣ — ΣΣ)
That is,
Vx k ((X, μ, ς), (x, μ, ς)) = —2 exp (—akX - Xk2 - βkμ - μk2 -Y i∣ς — ς k2) Mx - x),
vμk ((x, μ, ς), (x, μ, ς)) = - 2eχp (—αkx — χk2 — βkμ — μk2 — Yilς — ς k2) β(μ — μ),
VBk((X,μ, A(X,μ, ς))
=-2 exp (—αkx — χk2 — βkμ — μk2 — Ykς — ς k2) 27(2夕2 — ςς — ςς).
B.3 Label Projection
We here propose an approach to recover new samples in the feature-label space from an empirical
distribution in the feature-Gaussian space. Consider that after T iterations of the gradient algorithms,
24
Under review as a conference paper at ICLR 2022
We arrive at a distribution PT = -N PN=I δ(χT,μ,∑τ). We would like to recover a distribution
νT ∈ P(X × Y) which is induced by ρτ. As such, we would like to find a distribution νT of the form
1N
VT = N Σ δ(χT,yT)
which corresponds to new target samples (xiT, yiT)iN=1. Moreover, we are interested in recovering
labels within the target domain. To this end, let Ytarget = {y ∈ Y : ∃j ∈ [M] such that yj = y}
be the set of labels in the target dataset, and remind that for any y ∈ Ytarget, (μy, Σy) ∈ Rn X S*
is the mean vector and the covariance matrix of the distribution of φ(X) given Y = y. Notice that
the mean-covariance embeddings (μy, Σy) for y ∈ Ytarget depend only on the target domain data,
and it does not depend on the incumbent distribution ρT , nor does it depend on the source dataset.
Moreover, we can also compute Ny as the number of samples from the target dataset with label y.
Because (μy, Σy) is readily computed, we can consider g, Σy) as the centroids and simply find
an assignment that minimizes the sum of distances from (μT, ΣT) to these centroids. We thus can
assign each sample from ρT to the the target labels by solving the linear program
N	___________________________
min X X θiy JkμT - μ,k2 + B(∑T, Σy)2
i=1 y∈Ytarget
LI	VN	N	一 一一、
s.t. E	θiy	= N	∀i	=1,...,N,	fθiy	= N	∀y	∈	Ytarget,	θ ∈	[0,1]N ×lYtargetl,
y∈Ytarget	i=1
(B.1)
Notice that the assignment problem above does not utilize the information from the covariate xiT .
Let θ? be the optimal solution of the above optimization problem. Then the dataset (xiT, ziT)iN=1
recovered from ρτ is
1N
VT = Nf δ(χT,yτ),	yT = E	y1(% = maχ{θt}) ∀i = 1,...,N
i=1	y∈Ytarget
We used the POT library to solve the label recovery problem (B.1).
B.4	Additional Numerical Results
B.4.1	Mixture of Gaussians
We test our algorithm on a toy example: a mixture of Gaussian distributions to another mixture of
Gaussian distributions.
The source distribution and target distribution are:
Ps(X) = 4N( (-003)
0.14
-0.00
-0.00	1	2.0	0.43
0.22 〃+ 4N(10.3J ,10.18
0.18
0.26
1	-0.3	0.66 0.02	1	0.3	0.39	-0.02
+ 4NII 2.0 ) , 10.02 0.63〃+ 4N l1-2.0) , 1-0.02 0.13 川
1	2.9	0.16
Pt(X) = 4N ((0.1 ) ,(0.03
1	0.8	0.63
+ 4N((2.2),(0.02
0.03λλ	1 N 0(0.9、 (0.22 0.16、∖
0.20〃 + 4NI <0.5), 10.16 0.46〃
0.02λλ	1 N ((1.4、俏.18 0.10∖∖
0.66+ 4N V∖-1.8J , I。」。0.36〃
From each distribution, we sample 25 particles and flow the particles’ positions, means, and covariance
simultaneously using Alg. 1. After the algorithm converges, we recover the particles’ label in the
feature-label space by solving problem (B.1).
25
Under review as a conference paper at ICLR 2022
Figure 4: The results of flowing a mixture of 4 Gaussian distributions to a mixture of 4 Gaussian
distributions. We demonstrate the initialization (left), the trace of particles in first 200 steps (middle),
and the results at step 1000 (right).
We test how our algorithm deals with flowing a mixture of 2 Gaussian distributions to a mixture of 4
Gaussian distributions. From the trace of first 200 steps, we demonstrate that each source Gaussian
distribution splits into 2 Gaussian distributions. The source distribution and target distribution are:
ps(x)	=2 N(	00..00	0.18 -0.24	-0.24 0.70	)+2 N(	50..80	,	0.44	0.00 0.00 0.87	
pt(x)	T N (	(02..70	0.63 -0.30	-0.30 0.26	')+4 N (	(-20.2.8)	0.77 , -0.18	-0.18 0.55
	+1 N (		0.63	-0.30 0.26	)+1 N (		0.77	
		(7.0 0.8 ,	-0.30			( 7.7 ) -0.8	, -0.18	-0.18 0.55
Figure 5: The results of flowing a mixture of 2 Gaussian distributions to a mixture of 4 Gaussian
distributions. We demonstrate the initialization (left), the trace of particles in first 200 steps (middle),
and the results at step 2400 (right). We use method in Section B.3 to relabel the source data.
B.5	Implementation details
When flowing images in *NIST datasets and flowing a mixture of Gaussians, we use the parameters
and methods described in Table 1.
Our method assumes images of each class form one Gaussian distribution. In reality, the data can
be a mixture of Gaussian. To satisfy the Gaussianity assumption, in the preprocessing step, we use
a clustering method (k-nearest neighbors) and pick only data from one mode for each class. As a
consequence, the data used in the experiment satisfies the conditional Gaussian assumption. For
example, the images of the digit 1 can have two modes: slanted left or slanted right. In this case, we
can generate two labels (1L, 1R), and the methodology developed in this paper can be applied in a
straightforward manner. When testing our transfer learning scheme, we apply the same clustering
method on the test dataset, so our test set is within the same mode as our training set.
26
Under review as a conference paper at ICLR 2022
Table 1: Parameters and Optimizer
	*NIST	Gaussian (Figure 4)	Gaussian (Figure 5)
α	0.001	0.3	0.3
β	0.002	0.15	0.1
γ	100	1.0	0.5
initial sτ	0.3	0.05	0.03
noise level	0.01	0	0.1
T	150	2000	2500
Optimizer	RMSprop Hinton et al. (2012)	RMSprop	RMSprop
We store the preprocessed data and apply dimension reduction method on the data’s means and
covariance matrices, so the Lyapunov equation is much faster to solve. We use the cluster’s mean
and covariance matrix to approximate the 1-shot and 5-shor data’s mean and covariance matrix. In
1-shot learning, the covariance matrix is an identity matrix. All the code and data are available in the
supplementary file.
We use k-nearest neighbors algorithm to solve the labels of the flowed data, as it performs better with
the noisy scheme.
B.5.1	Additional results on flows
We conduct additional experiments of flowing between KMNIST and FashionMNIST datasets. The
results of our flows are depicted in Fig. 6. In each subfigure, each column represents a snapshot of
a certain time-step and the samples flow from the source (left) to the target (right). The number of
iterations T that is used to generate the results in Fig. 6 is capped at 140. We also attach the same
n n π π
AAAAAa
JrLAFMwr
a 幺 / M 3 &
/JCQAfia
I / 4ZfJ uzi-lZ；
均般HU・・
CaIH-
∖iΛJkΛΛΛ
■㈣愕懵偿偿

Figure 6: Sample path visualizations between FashionMNIST dataset and KMNIST dataset
results as Fig. 2 in high resolution in Fig. 7 and Fig. 8. Each picture illustrates one experiment of
gradient flow between two datasets and the samples flow from the source (left) to the target (right).
27
Under review as a conference paper at ICLR 2022
Figure 7: Sample path visualizations between KMNIST dataset and MNIST dataset
8 % 一 3 J
ip4>C Y3xt7
28
Under review as a conference paper at ICLR 2022
Figure 8: Sample path visualizations between FashionMNIST dataset and MNIST dataset
M5 C 7 / 口
T £ G 7 , 4
二Z 7，U
B.5.2 Additional results on transfer learning
We transfer a pretrained classifier from FashionMNIST dataset to KMNIST dataset and from KMNIST
dataset to FashionMNIST dataset. We use the same model architecture and training settings as in
Fig. 3. We illustrate the accuracy and error bars of the 1-shot learning and 5-shot learning in Fig. 9.
Our flowed samples increase the accuracy of the transferred classifiers in both 1-shot and 5-shot
learning.
29
Under review as a conference paper at ICLR 2022
B.6 Comparison with baseline
Comparison with Alvarez-Melis & Fusi (2021)’s approach. We adopted the same values of pa-
rameters (number of steps, step size) from the paper of Alvarez-Melis & Fusi (2021) and experimented
with different values of entropy regularization λ. Entropy regularization λ is a hidden parameter in
their code and they reported using the value of λ = 100 in transfer learning experiments. Also, we
experimented with different methods in their code and found that “xyaugm” gives the best qualitative
gradient flow results. We test their algorithm using the same transfer learning setting as ours (using
the same clustered data as our experiments), see Table 2 and Table 3 for results.
Table 2: KMNIST MNIST				Table 3: FMNIST MNIST			
Accuracy		D ∪ ST	P ∪ D ∪ ST	Accuracy		D ∪ ST	P ∪ D ∪ ST
λ	= 0.001	0.1941	0.2478	λ	= 0.001	0.4488	0.3083
λ	= 0.01	0.1175	0.2998	λ	= 0.01	0.3915	0.2897
λ	= 1.0	0.1739	0.2951	λ	= 1.0	0.5268	0.4627
λ	= 100	0.2093	0.3025	λ	= 100	0.2917	0.3307
For runtime comparison, the default device for Alvarez-Melis & Fusi (2021)’s code is on the CPU.
As we run their code on the GPU, the kernel crashed without giving any informative errors. Thus, we
will compare our codes’ runtime per step on the CPU. While our approach takes about 11.72 seconds,
the approach in Alvarez-Melis & Fusi (2021) requires 74.78 seconds.
30