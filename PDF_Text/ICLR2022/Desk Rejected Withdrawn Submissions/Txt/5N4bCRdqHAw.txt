Under review as a conference paper at ICLR 2022
MFE-NER: Multi-feature Fusion Embedding
for Chinese Named Entity Recognition
Anonymous authors
Paper under double-blind review
Ab stract
Pre-trained language models lead Named Entity Recognition (NER) into a new
era, while some more knowledge is needed to improve their performance in spe-
cific problems. In Chinese NER, character substitution is a complicated linguistic
phenomenon. Some Chinese characters are quite similar for sharing the same
components or having similar pronunciations. People replace characters in a
named entity with similar characters to generate a new collocation but referring to
the same object. It becomes even more common in the Internet age and is often
used to avoid Internet censorship or just for fun. Such character substitution is
not friendly to those pre-trained language models because the new collocations
are occasional. As a result, it always leads to unrecognizable or recognition errors
in the NER task. In this paper, we propose a new method, Multi-Feature Fusion
Embedding (MFE) for Chinese Named Entity Recognition, to strengthen the lan-
guage pattern of Chinese and handle the character substitution problem in Chinese
Named Entity Recognition. MFE fuses semantic, glyph, and phonetic features to-
gether. In the glyph domain, we disassemble Chinese characters into components
to denote structure features so that characters with similar structures can have
close embedding space representation. Meanwhile, an improved phonetic system
is also proposed in our work, making it reasonable to calculate phonetic similarity
among Chinese characters. Experiments demonstrate that our method improves
the overall performance of Chinese NER and especially performs well in informal
language environments.
1 Introduction
Recently, pre-trained language models have been widely used in Natural Language Processing
(NLP), constantly refreshing the benchmarks of specific NLP tasks. By applying the transformer
structure, semantic features can be extracted more accurately. However, in Named Entity Recogni-
tion (NER) area, tricky problems still exist. Most significantly, the character substitution problem
severely affects the performance of NER models. To make things worse, character substitution
problems have become even more common these years, especially in social media. Due to the par-
ticularity of Chinese characters, there are multiple ways to replace original Chinese characters in
a word. Characters with similar meanings, shapes, or pronunciations can be selected for character
substitution. A simple example shown in Figure 1 is a Chinese word, which represents for a famous
place in Shanghai. Here, all three characters in the word are substituted by other characters with
similar glyphs or similar pronunciations. After substitution, this word looks more like a name of a
person rather than a place.
In practice, it is extremely hard for those pre-trained language models to tackle this problem. When
we train pre-trained models, we collect corpus from mostly formal books and news reports, which
means they gain language patterns from the semantic domain, neglecting glyph and phonetic fea-
tures. However, most character substitution cases exist in glyph and phonetic domains. At the same
time, social media hot topics are changing rapidly, creating new expressions or substitutions for
original words every day. It is technically impossible for pre-trained models to include all possible
collocations. Models that only saw the original collocations before will naturally fail to get enough
information to infer that character substitution doesn’t actually change the reference of a named en-
tity.
In this paper, we propose Multi-feature Fusion Embedding for Chinese Named Entity Recognition
1
Under review as a conference paper at ICLR 2022
Similar glyph & similar pronunciation
da3	pu3	qiao2
da2	fu3	qiao2
Figure 1: The substitution example of a Chinese word. On the left is a famous place in Shanghai.
On the right is a new word after character substitution, which is more like a person or a brand.
(MFE-NER), which fuses semantic, glyph, and phonetic features together to strengthen the expres-
sion ability of Chinese character embedding. MFE can handle character substitution problems more
efficiently in Chinese NER. On top of using pre-trained models to represent the semantic feature,
we choose a structure-based encoding method, known as ’Five-Strokes’, for character glyph embed-
ding. In phonetic domain, we propose ‘Trans-pinyin’, which combines ’Pinyin’, a unique phonetic
system, with international standard phonetic symbols. Different fusion strategies are also explored
to make our method more reasonable. Experiments on 5 typical datasets illustrate that MFE can not
only enhance the overall performance of NER models but also help NER models to handle character
substitution problems, which makes it especially suitable to be used in current language environ-
ments.
To summarize, our major contributions are:
•	We propose the Multi-feature Fusion Embedding for Chinese characters in Named Entity
Recognition, especially for Chinese character substitution problems.
•	As for glyph feature in Chinese character substitution, we use the ’Five-Strokes’ encoding
method, denoting structure patterns of Chinese characters, so that Chinese characters with
similar glyph structures can be close in embedding space.
•	To represent phonetic features in Chinese character substitution, we propose a new method
named ‘Trans-pinyin’, to make it possible to evaluate phonetic similarity among Chinese
characters.
•	Experiments show that our method improves the overall performance of NER models and
is more efficient to find substituted Chinese NER.
2	Related Work
After the stage of statistical machine learning algorithms, Named Entity Recognition has stepped
into the era of neural networks. Researchers started to use Recurrent Neural Network (RNN) Ham-
merton (2003) to recognize named entities in sentences based on character embedding and word
embedding, solving the feature engineering problems that traditional statistical methods have. Bidi-
rectional Long Short Term Memory (Bi-LSTM) network Huang et al. (2015) was firstly applied in
Chinese Named Entity Recognition, which becomes one of the baseline models. The performance
of the Named Entity Recognition task thus gets greatly improved.
These years, large-scale pre-trained language models based on Transformer Vaswani et al. (2017)
have shown their superiority in Natural Language Processing tasks. The self-attention mechanism
can better capture the long-distance dependency in sentences and the parallel design is suitable
for mass computing. Bidirectional Encoder Representations from Transformers (BERT) Kenton &
Toutanova (2019) has achieved great success in many branches of NLP. In the Chinese Named En-
tity Recognition field, these pre-trained models have greatly improved the recognition performance
Cai (2019).
2
Under review as a conference paper at ICLR 2022
Figure 2: The structure of Multi-feature Fusion Embedding for Chinese Named Entity Recognition.
However, the situation is more complicated in real language environments. Robustness of NER mod-
els is not guaranteed by pre-trained language models. Researchers start to introduce prior knowledge
to improve the generalization of NER models. SMedBERT Zhang et al. (2021) introduces knowl-
edge graphs to help the model acquire the medical vocabulary explicitly.
Meanwhile, in order to solve character substitution problems and enhance robustness of NER mod-
els, researchers have also paid attention to denoting glyph and phonetic features of Chinese charac-
ters. Jiang Yang and Hongman Wang suggested that using the ‘Four-corner’ code, a radical-based
encoding method for Chinese characters, to represent glyph features of Chinese characters Yang
et al. (2021), showing the advantage of introducing glyph features in Named Entity Recognition.
However, the ‘Four-corner’ code is not that expressive because it only works when Chinese charac-
ters have the exact same radicals. Tzu-Ray Su and Hung-Yi Lee suggested using Convolution Auto
Encoder to build a bidirectional injection between images of Chinese characters and pattern vectors
Su & Lee (2017). This method is brilliant but lacks certain supervision. It’s hard to explain the
concrete meanings of the pattern vectors. Zijun Sun, Xiaoya Li et al proposed ChineseBERT Sun
et al. (2021), fusing glyph and phonetic features to the pre-trained models. Their work is impressive
by combing glyph and phonetic features as the input for pre-trained models, but still exists some
problems. For example, using flatten character images is inefficient for adding the cost of feature
dimension and enlarging possible negative influence to the model. Chinese characters have a limited
number of components in structures, which is handy to extract and denote their glyph patterns.
3	Method
As shown in Figure 2, our Multi-feature Fusion Embedding mainly consists of three parts, semantic
embedding, glyph embedding with ‘Five-Strokes’ and synthetic phonetic embedding, which we
think are complementary in Chinese Named Entity Recognition. All the three embedding parts are
chosen and designed based on a simple principle, similarity.
For a Chinese sentence S with length n, the sentence S is divided naturally to different Chinese
characters S = c1, c2, c3, ..., cn. Each character ci will be mapped to an embedding vector ei,
which can be divided into the above three parts, eis, eig and eip. In this paper, character similarity
between two Chinese characters ci and cj is defined by computing their L2 distance in according
three aspects. Here, we use sisj to denote semantic similarity, sigj for glyph similarity and sipj for
phonetic similarity. So, we have:
sisj = keis -ejsk	(1)
sigj = keig -ejgk	(2)
sipj = keip-ejpk	(3)
In this case, our Multi-feature Fusion Embedding can better represent the distribution of Chinese
characters in embedding space.
3
Under review as a conference paper at ICLR 2022
Chinese Character Character Roots
I
浦
月
E
Pu3 (somewhere close to river)
一
G
、
Y
Five-Strokes: I G E Y
Figure 3:	The ‘Five-Strokes’ representation of the Chinese characters, ‘pu3’ (somewhere close to
river). ‘Five-Strokes’ divide the character into four character roots ordered by writing custom so that
the structure similarity can be denoted.
3.1	Semantic Embedding using Pre-trained Models
Semantic embedding is vital to Named Entity Recognition. In Chinese sentences, a single character
does not mean a word, because there is no natural segmentation in Chinese grammar. So, techni-
cally, we have two choices to acquire Chinese embedding. The first way is word embedding, trying
to separate Chinese sentences into words and get the embedding of words, which makes sense but is
limited by the accuracy of word segmentation tools. The other is character embedding, which maps
Chinese characters to different embedding vectors in semantic space. In practice, it performs better
in Named Entity Recognition. Our work is required to use character embedding because glyph and
phonetic embedding are targeted at single Chinese character.
Pre-trained models are widely used in this stage. One typical method is Word2Vec Mikolov et al.
(2013), which starts to use static embedding vectors to represent Chinese characters in semantic
domain. Now, we have more options. BERT Kenton & Toutanova (2019) has its Chinese version
and can express semantic features of Chinese characters more accurately, hence have better
performance in NER tasks.
3.2	Glyph Embedding with Five-Strokes
Chinese characters, different from Latin Characters, are pictograph, which show their meanings in
shapes. However, it is extremely hard for people to encode these Chinese characters in computer.
Common strategy is to give every Chinese character a unique hexadecimal string, such as ‘UTF-8’
and ‘GBK’. However, this kind of strategy processes Chinese characters as independent symbols,
totally ignoring the structure similarity among Chinese characters. In other words, the closeness in
the hexadecimal string value can not represent the similarity in their shapes. Some work has tried to
use images of Chinese characters as glyph embedding, which is also unacceptable and ineffective
due to the complexity and the large space it will take.
In this paper, we propose to use ‘Five-strokes’, a famous structure-based encoding method for
Chinese characters, to get our glyph embedding. ‘Five-Strokes’ was put forward by Yongmin
Wang in 1983. This special encoding method for Chinese characters is based on their structures.
‘Five-Strokes’ holds the opinion that Chinese characters are made of five basic strokes, horizontal
stroke, vertical stroke, left-falling stroke, right-falling stroke and turning stroke. Based on that, it
gradually forms a set of character roots, which can be combined to make up the structure of any
Chinese character. After simplification for typing, ’Five-Strokes’ maps these character roots into
25 English characters (‘z’ is left out) and each Chinese character is made of at most four according
English characters, which makes it easy to acquire and type in computers. It is really expressive
that four English characters can have 254 = 390625 arrangements, while we only have about 20
thousand Chinese characters. In other words, ‘Five-Strokes’ has a rather low coincident code rate
for Chinese characters.
For example, in Figure 3, the Chinese character ‘pu3’ (somewhere close to river) is divided into
four different character roots by Chinese character writing custom, which will later be mapped to
English characters so that we can further encode them by introducing one-hot encoding. For each
character root, we can get a 25-dimension vector. In this paper, in order to reduce space complexity,
4
Under review as a conference paper at ICLR 2022
Chinese
Characters
草早
Pinyin Forms	C ao 3	Z ao 3
Standard Forms	ts' au 3	ts au 3
Figure 4:	The ‘Pinyin’ form and standard form of two Chinese characters, ‘cao3’ (grass) on the left
and ‘zao3’ (early) on the right.
we sum up the these 25-dimension vectors as the glyph embedding vector. We also list two different
characters, ‘fu3’ (an official position) and ’qiao2’ (bridge), and calculate the similarity between
them. The two characters ‘pu3’ and ‘fu3’ have similar components are close in embedding space,
while ‘qiao2’ and ‘pu3’ are much more distant, which gives NER models extra patterns.
3.3	Phonetic Embedding with Trans-pinyin
Chinese use ‘Pinyin’, a special phonetic system, to represent the pronunciation of Chinese charac-
ters. In the phonetic system of ‘Pinyin’, we have four tunes, six single vowels, several plural vowels,
and auxiliaries. Every Chinese character has its expression, also known as a syllable, in the ‘Pinyin’
system. A complete syllable is usually made of an auxiliary, a vowel, and a tune. Typically, vowels
appear on the right side of a syllable and can exist without auxiliaries, while auxiliaries appear on
the left side and must exist with vowels.
However, the ‘Pinyin’ system has an important defect. Some similar pronunciations are denoted by
totally different phonetic symbols. For the example in Figure 4, the pronunciations of ‘cao3’ (grass)
and ‘zao3’ (early) are quite similar because the two auxiliaries ‘c’ and ‘z’ sound almost the same that
many native speakers may confuse them. This kind of similarity can not be represented by phonetic
symbols in the ‘Pinyin’ system, where ‘c’, and ‘z’ are independent auxiliaries. In this situation, we
have to develop a method to combine ‘Pinyin’ with another standard phonetic system, which can
better describe characters’ phonetic similarities. Here, the international phonetic system seems the
best choice, where different symbols have relatively different pronunciations so that people will not
confuse them.
We propose the ’Trans-pinyin’ system to represent character pronunciation, in which auxiliaries and
vowels are transformed to standard forms and keep the tune in the ‘Pinyin’ system. After transfor-
mation, ‘c’ becomes ‘ts’ and ‘z’ becomes ‘ts0’, which only differ in phonetic weight. We also make
some adjustments to the existing mapping rules so that similar phonetic symbols in ‘Pinyin’ can
have similar pronunciations. By combining ‘Pinyin’ and the international standard phonetic system,
the similarity among Chinese characters’ pronunciations can be well described and evaluated.
In practice, we use the ‘pypinyin’ 1 library to acquire the ‘Pinyin’ form ofa Chinese character. Here,
we will process the auxiliary, vowel, and tune separately and finally concatenate them together after
being processed.
•	For auxiliaries, they will be mapped to standard forms, which have at most two English
characters and a phonetic weight. We apply one-hot encoding to them so that we get two
one-hot vectors and a one-dimension phonetic weight. Then we add up the two English
characters’ one-hot vectors and the phonetic weight here will be concatenated to the tail.
•	For vowels, they are also mapped to standard forms. However, it is a little different here.
We have two different kinds of plural vowels. One is purely made up of single vowels, such
as ‘au’，‘eu' and 'ai'. The other kind is like ‘an’，‘an’ and "η', which are combinations of
a single vowel and a nasal voice. Here, single vowels are encoded to 6-dimension one-hot
1https://github.com/mozillazg/python-pinyin
5
Under review as a conference paper at ICLR 2022
vectors and nasal voices to 2-dimension one-hot vectors respectively. Finally, We concate-
nate them together and if the vowel does not have a nasal voice, the last two dimensions
will all be zero.
•	For tunes, they can be simply mapped to four-dimension one-hot vectors.
3.4	Fusion S trategy
It should be noted that we can not directly sum up the three embedding parts, because each part is
drawn from a different domain. Here we introduce three different fusion methods to apply.
•	Concat Concatenating the three embedding parts seems the most intuitive way, we can just
put them together and let NER models do other jobs. In this paper, we choose this fusion
strategy because it performs really well.
ei = concat([eis , eig , eip])	(4)
•	Concat+Linear It also makes sense if we put a linear layer to help further fuse them to-
gether. Technically, it can save space and boost the training speed of NER models.
ei = Linear(concat([eis , eig , eip]))	(5)
•	Multiple LSTMs There is also a complex way to fuse all three features. Sometimes, it
doesn’t make sense that we mix different features together. So, we can also desperate them,
train NER models from different aspects and use a linear layer to calculate the weighted
mean of the results. In this work, We use BiLSTM to extract patterns from different em-
beddings and a linear layer to calculate the weighted mean.
We do not recommend the second fusion strategy. Three different embeddings are explainable inde-
pendently, while the linear layer may mix them together, leading to information loss.
4	Experiments
We make experiments on five different datasets and verify our method from different perspectives.
Standard precision, recall and, F1 scores are calculated as evaluation metrics to show the perfor-
mance of different models and strategies. In this paper, we set up experiments on Pytorch and
FastNLP 2 structure.
4.1	Dataset
Dataset	Sentences		
	Train	Test	Dev
Resume	3821	463	477
People Daily	63922	7250	14436
MSRA	41839	4525	4365
Weibo	1350	270	270
Substitution	14079	824	877
Table 1: Dataset Composition
We first conduct our experiments on four common datasets used in Chinese Named Entity Recogni-
tion, Chinese Resume Zhang & Yang (2018), People Daily3, MSRA Levow (2006) and Weibo Peng
& Dredze (2015); He & Sun (2017). Chinese Resume is mainly collected from resume materials.
The named entities in it are mostly people’s names, positions, and company names, which all have
2https://github.com/fastnlp/fastNLP
3https://icl.pku.edu.cn/
6
Under review as a conference paper at ICLR 2022
strong logic patterns. People Daily and MSRA mainly select corpus from official news reports,
whose grammar is formal and vocabulary is quite common. Different from these datasets, Weibo is
from social media, which includes many informal expressions.
Meanwhile, in order to verify whether our method has the ability to cope with the character substi-
tution problem, we also build our own dataset. Raw corpus is collected from informal news reports
and blogs. We label the Named Entities in raw materials first and then create their substitution forms
by using similar characters to randomly replace these in the original Named Entities. In this case,
our dataset is made of pairs of original entities and their character substitution forms. This dataset
consists of 15780 sentences in total and is going to test our method in an extreme language environ-
ment.
Details of the above five datasets is described in Table 1.
4.2	Experiment Settings
Items	Range
batchsize	12
epochs	60
lr	2e-3
optimizer	Adam
dropout	0.4
Early Stop	5
lstm layer	1
hidden dim	200
Table 2: Hyper-parameters
The backbone of the NER model used in our work is mainly Multi-feature Fusion Embedding (MFE)
+ BiLSTM + CRF. The BiLSTM+CRF model is stable and has been verified in many research
projects. Meanwhile, we mainly focus on testing the efficiency of our Multi-feature Fusion Embed-
ding. So, if it works well with the BiLSTM+CRF model, it would have a great chance to perform
well in other model structures.
Table 2 lists some of the hyper-parameters in our training stage. Adam is used as the optimizer
and the learning rate is set to 0.002. In order to reduce over-fitting, we set a rather high dropout
Srivastava et al. (2014) rate of 0.4. Meanwhile, the Early Stop is also deployed, allowing 5 epochs
of loss not decreasing.
5	Results and Discussions
We make two kinds of experiments, the first one is to evaluate the performance of our model, while
the second one is to compare different fusion strategies. Besides experiment results analysing,
structure design of MFE and its advantages are also discussed.
5.1	Overall performance
To test the overall performance of MFE, we select two pre-trained models, static embedding trained
by Word2VecMikolov et al. (2013) and BERTKenton & Toutanova (2019). our glyph and phonetic
embedding are added to them. The embeddings without glyph and phonetic features are named
‘pure’ embeddings. Basically, We compare pure embeddings with those using MFE on the perfor-
mance in the NER task. To control variables, we apply the Concat strategy here because other fusion
strategies will change the network structure. Table 3 and 4 show the performance of embedding
models in different datasets. It is remarkable that MFE with BERT achieves the best performance in
all five datasets, showing the highest F1 scores.
Experiments on datasets from formal language environments like Chinese Resume, People Daily
and MSRA strongly show that MFE brings visible improvement to the NER task. On Chinese Re-
sume, MFE with BERT achieves a 95.73 % F1 score and MFE with static embedding gets a 94.23
% F1 score, improving the performance of pure semantic embedding by about 0.5 % with respect to
7
Under review as a conference paper at ICLR 2022
Chinese Resume	People Daily	MSRA
Models____________________________________________p______y________________________
	Precision	Recall	F1	Precision	Recall	F1	Precision	Recall	F1
pure BiLSTM+CRF	93.97	93.62	93.79	85.03	80.24	82.56	90.12	84.68	87.31
MFE (BiLSTM+CRF)	94.29	94.17	94.23	85.23	81.59	83.37	90.47	84.70	87.49
pure BERT	94.60	95.64	95.12	9107	86.56	88.76	93.75	85.21	89.28
MFE (BERT)	95.76	95.71	95.73	90.39	87.74	89.04	93.32	86.83	89.96
Table 3: Results on three datasets in formal language environments, Chinese Resume, People Daily,
and MSRA. In this table, we compare pure semantic embeddings with those using MFE.
Models	Weibo			Substitution		
	Precision	Recall	F1	Precision	Recall	F1
pure BiLSTM+CRF	67.19	40.67	50.67	84.60	71.24	77.35
MFE (BiLSTM+CRF)	67.51	44.74	53.81	83.36	73.17	77.94
pure BERT	68.48	63.40	65.84	88.67	81.67	85.03
MFE (BERT)	70.36	65.31	67.74	90.08	82.56	86.16
Table 4: Results on two datasets in informal language environments, Weibo, and Substitution. In
this table, we also compare pure semantic embeddings with those using MFE.
F1 score. On People Daily, MFE increases the F1 score of static embedding from 82.56 % to 83.37
% and boosts the performance of using BERT as semantic embedding. The situation is the same
on MSRA. However, owing to few grammar mistakes and substitution froms in formal language
environment, the performance of MFE is limited.
MFE shows its advantage mainly in informal language environments. Weibo dataset is a typical
他
想
去
打
浦
桥
Pure semantic
O B-LOC I-LOC I-LOC
O
O
MFE
O
O
O B-LOC I-LOC I-LOC
他想去达甫乔
Pure semantic
O B-LOC I-LOC O
O
O
MFE
O
O
O B-LOC I-LOC I-LOC
Figure 5: An example is drawn from our dataset, ’he wants to go to Dapuqiao’. The sentence at
the top of the figure is the original sentence, while the sentence at the bottom is after character
substitution. Model using MFE gives the correct prediction.
example from social media. On weibo dataset, MFE achieves 53.81 % and 67.74 % in models with
static embedding and BERT respectively, significantly enhancing the performance of pre-trained
models. On our Substitution dataset, MFE also brings remarkable advances. There is an interesting
example shown in Figure 5. The two sentences are drawn from our dataset with the same meaning,
‘he wants to go to Dapuqiao’. Here, ‘Dapuqiao’ is a famous place in Shanghai. However, the sen-
tence below is different because characters in the original entity are changed but the word still refers
to the same. For the model using pure semantic embedding, it fails to give the perfect prediction for
the sentence below, while the model using MFE can exploit the extra information, thus giving the
perfect prediction.
From the results across all the datasets, It is clear MFE brings prominent improvement to NER
models based on pre-trained embeddings. Most importantly, MFE especially shows its superiority
8
Under review as a conference paper at ICLR 2022
Strategy	Resume	People Daily	MSRA	Weibo	Substitution
Concat	95.73	89.04	89.96	67.74	86.16
Concat+Linear	94.62	89.76	89.14	66.17	85.40
Multiple LSTMs	95.83	91.38	89.81	68.05	87.41
Table 5: F1 scores of different fusion strategies using MFE (BERT) on the five datasets.
on datasets in informal language environments, proving that it provides an insightful way to handle
character substitution problems in Chinese Named Entity Recognition.
5.2	Impact of Fusion Strategies
Table 5 displays the F1 scores of applying different fusion strategies on all five datasets. Considering
the three different fusion strategies mentioned above, the model with Multiple LSTMs achieves the
best overall performance. Except for the MSRA dataset, the model using Multiple LSTMs gets the
highest F1 score on the other four datasets, having an average boost of 2 % F1 score.
Meanwhile, it is better not to add a linear layer to fuse all of these patterns. Our features are extracted
from different domains, which means they do not have a strong correlation. Adding a linear layer
basically means mixing these features together, which is not explainable. For the same reason, the
model using Multiple LSTMs can maintain the independence of each feature, because we don’t mix
them up but average the predictions of LSTMs by giving certain weights.
5.3	How MFE brings improvement
Based on the experiment results above, MFE is able to reduce the negative influence of the char-
acter substitution phenomenon in Chinese Named Entity Recognition, while improving the overall
performance of NER models. It makes sense that MFE is suitable to solve character substitution
problems because glyph and pronunciation features are introduced. These extra features are com-
plementary to semantic embedding from pre-trained models and bring information that gives more
concrete evidences to NER models.
What’s more In Chinese, named entities have their own glyph and pronunciation patterns. In the
glyph domain, characters in Chinese names usually contain a special character root, which denotes
‘people’. Characters representing places and objects also include certain character roots, which show
the materials like water, wood, and soil. These character roots can be utilized in ‘Five-Strokes’ .
6	Conclusion
In this paper, we propose a new idea for Chinese Named Entity Recognition, Multi-feature Fusion
Embedding (MFE-NER). It fuses semantic, glyph, and phonetic features to provide extra prior
knowledge for pre-trained language models so that it can give a more expressive and accurate
embedding for Chinese characters in the Chinese NER task. In our method, we have deployed
’Five-strokes’ to help generate glyph embedding and developed a synthetic phonetic system to
represent pronunciations of Chinese characters. By introducing Multi-feature Fusion Embedding,
the performance of pre-trained models in the NER task can be improved, which demonstrates the
significance and versatility of glyph and phonetic features.
Meanwhile, we prove that Multi-feature Fusion Embedding can assist NER models to reduce
the influence caused by character substitution. Nowadays, the informal language environment
created by social media has deeply changed the way of people expressing their thoughts. Using
character substitution to generate new named entities becomes a common linguistic phenomenon.
In this situation, our method is especially suitable to be used in the current social media environment.
Acknowledgments
We thank anonymous reviewers for their helpful comments.
9
Under review as a conference paper at ICLR 2022
References
Qing Cai. Research on chinese naming recognition model based on bert embedding. In 2019 IEEE
10th International Conference on Software Engineering and Service Science (ICSESS), pp. 1-4.
IEEE, 2019.
James Hammerton. Named entity recognition with long short-term memory. In Proceedings of the
seventh conference on Natural language learning at HLT-NAACL 2003, pp. 172-175, 2003.
Hangfeng He and Xu Sun. F-score driven max margin neural network for named entity recognition
in chinese social media. In Proceedings of the 15th Conference of the European Chapter of the
Association for Computational Linguistics: Volume 2, Short Papers, pp. 713-718, 2017.
Zhiheng Huang, Wei Xu, and Kai Yu. Bidirectional lstm-crf models for sequence tagging. arXiv
e-prints, pp. arXiv-1508, 2015.
Jacob Devlin Ming-Wei Chang Kenton and Lee Kristina Toutanova. Bert: Pre-training of deep
bidirectional transformers for language understanding. In Proceedings of NAACL-HLT, pp. 4171-
4186, 2019.
Gina-Anne Levow. The third international chinese language processing bakeoff: Word segmenta-
tion and named entity recognition. In Proceedings of the Fifth SIGHAN Workshop on Chinese
Language Processing, pp. 108-117, 2006.
Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. Efficient estimation of word represen-
tations in vector space. Computer Science, 2013.
Nanyun Peng and Mark Dredze. Named entity recognition for chinese social media with jointly
trained embeddings. In Proceedings of the 2015 Conference on Empirical Methods in Natural
Language Processing, pp. 548-554, 2015.
Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov.
Dropout: a simple way to prevent neural networks from overfitting. The journal of machine
learning research, 15(1):1929-1958, 2014.
Tzu-Ray Su and Hung-Yi Lee. Learning chinese word representations from glyphs of characters. In
Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pp.
264-273, 2017.
Zijun Sun, Xiaoya Li, Xiaofei Sun, Yuxian Meng, Xiang Ao, Qing He, Fei Wu, and Jiwei Li.
Chinesebert: Chinese pretraining enhanced by glyph and pinyin information. arXiv e-prints, pp.
arXiv-2106, 2021.
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,
Eukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Advances in neural information
processing systems, pp. 5998-6008, 2017.
Jiang Yang, Hongman Wang, Yuting Tang, and Fangchun Yang. Incorporating lexicon and character
glyph and morphological features into bilstm-crf for chinese medical ner. In 2021 IEEE Inter-
national Conference on Consumer Electronics and Computer Engineering (ICCECE), pp. 12-17.
IEEE, 2021.
T. Zhang, Z. Cai, C. Wang, M. Qiu, and X. He. Smedbert: A knowledge-enhanced pre-trained
language model with structured semantics for medical text mining. In Proceedings of the 59th
Annual Meeting of the Association for Computational Linguistics and the 11th International Joint
Conference on Natural Language Processing (Volume 1: Long Papers), 2021.
Yue Zhang and Jie Yang. Chinese ner using lattice lstm. In Proceedings of the 56th Annual Meeting
of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 1554-1564, 2018.
10