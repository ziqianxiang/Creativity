Under review as a conference paper at ICLR 2022
Coordinated Attacks Against Federated Learn-
ing: A Multi-Agent Reinforcement Learning
Approach
Anonymous authors
Paper under double-blind review
Ab stract
We propose a model-based multi-agent reinforcement learning attack framework
against federated learning systems. Our framework first approximates the distribu-
tion of the clients’ aggregated data through cooperative multi-agent coordination. It
then learns an attack policy through multi-agent reinforcement learning. Depending
on the availability of the server’s federated learning configurations, we introduce al-
gorithms for both white-box attacks and black-box attacks. Our attack methods are
capable of handling scenarios when the clients’ data is independent and identically
distributed and when the data is independent but not necessarily identically dis-
tributed. We further derive an upper bound on the attacker’s performance loss due
to inaccurate distribution estimation. Experimental results on real-world datasets
demonstrate that the proposed attack framework achieves strong performance even
if the server deploys advanced defense mechanisms. Our work sheds light on how
to attack federated learning systems through multi-agent coordination.
1	Introduction
Federated learning (FL) is a powerful machine learning framework that allows a server to train
machine learning models across multiple workers that hold local data samples, without exchanging
them. Unfortunately, federated learning systems are vulnerable to threats (Lyu et al., 2020) such
as model poisoning attacks (Fang et al., 2020; Bagdasaryan et al., 2020; Bhagoji et al., 2019),
data poisoning attacks (Baruch et al., 2019; Fung et al., 2018; Gu et al., 2017), and inference
attacks (Melis et al., 2019; Hitaj et al., 2017; Zhu et al., 2019). These attacks are effective even
when the server applies robust aggregation rules such as coordinate-wise median (Yin et al., 2018),
trimmed mean (Yin et al., 2018), Krum (Blanchard et al., 2017), or Bulyan (Mhamdi et al., 2018).
Nevertheless, a recent study (Cao et al., 2020) shows that the server can collect a small clean training
dataset to bootstrap trust to defend a variety of attacks (Fang et al., 2020; Bagdasaryan et al., 2020).
Their results show that the FLTrust (Cao et al., 2020) defense is capable of achieving a high level of
robustness against a large fraction of adversarial attackers. While these defense mechanisms focus
on limiting the influence of the attackers in a single round, there also exists more adaptive defense
algorithms such as Safeguard (Allen-Zhu et al., 2020) and Centered Clipping (Karimireddy et al.,
2021), both of which incorporate historical gradient information into the aggregation rule. These
adaptive defense mechanisms are shown to be capable of overcoming time-couple attacks. In this
work, we propose a novel multi-agent reinforcement learning (MARL) attack framework that is
effective even if the server deploys the state-of-the-art defense mechanism such as FLTrust (Cao et al.,
2020), Safeguard (Allen-Zhu et al., 2020) and Centered Clipping (Karimireddy et al., 2021).
Learning effective attack policies against federated learning is challenging. A major reason is that it
typically requires attackers to obtain sufficient information about the federated learning environment
and training dynamics that includes both the server’s behavior and the normal workers’ data and
behavior. However, except for the model parameters, attackers often have rather limited information
about the FL dynamics in practice. To address this problem, we propose to leverage the power of
model-based reinforcement learning by integrating distribution learning and policy learning in our
MARL attack framework. In MARL, the attackers first cooperatively learn a model of the aggregated
data distribution through gradient matching, and then simulate the behavior of the server and the
benign workers using the learned distribution. In doing so, the attackers jointly learn an attack
policy through multi-agent reinforcement learning. Our MARL attack framework allows attackers to
1
Under review as a conference paper at ICLR 2022
implement attack policies in both white-box (i.e., attacks have some information about the server’s
algorithm) and black-box (i.e., attacks have no access to the server’s algorithm) settings.
While there is much research on adversarial attacks against federated learning, a majority of existing
attack methods (e.g., (Bhagoji et al., 2019; Fang et al., 2020; Xie et al., 2020b)) typically craft myopic
attack strategies based on heuristics. Further, most of them consider independent attackers without
coordination. Recently, a distributed backdoor attack (DBA) method is proposed in (Xie et al., 2020a)
where a global trigger pattern is manually decomposed into local patterns that are embedded to
different attackers. Compared with DBA, our MARL method enables the attackers to jointly learn an
attack policy through coordinating the behavior of attackers in distribution learning, policy learning
and attack execution, while their distributed backdoor attacks are coordinated in attack execution (i.e.,
trigger injection) only.
Our model-based multi-agent reinforcement learning attack framework distinguishes from existing
work (Sun et al., 2019; Zhang et al., 2020) on reinforcement learning based adversarial attacks by
considering a more realistic threat model where the attacker might not always be selected due to
subsampling nor does it have prior information about the distribution of the aggregated data. The
attackers need to efficiently learn the distribution along the federated learning process in real time.
We also consider the scenario when the attackers have no access to the server’s configurations. In
contrast, previous works typically assume more powerful attackers that can attack at any time and
have full knowledge about the environment.
Our contributions. We advance the state-of-the-art in the following aspects. First, we propose
a novel multi-agent reinforcement learning attack framework against federated learning systems
by integrating distribution learning and policy learning through multi-agent coordination. Second,
we propose algorithms for both white-box attacks and black-box attacks. Third, we theoretically
quantify the effect of inaccurate distribution learning on the optimality of policy learning. Fourth,
our experiments on real-world datasets demonstrate that our proposed MARL method consistently
outperforms existing model poisoning attacks (Bhagoji et al., 2019; Fang et al., 2020; Xie et al.,
2020b) due to multi-agent coordination in distribution learning, policy learning and attack execution.
2	Background
In this section, we describe the federated learning setting used in our work and present the threat
model.
Federated learning. We consider an FL setting that is similar to federated averaging (Fe-
dAvg) (McMahan et al., 2017). The FL system consists of a server and K workers (also known as
workers or clients) in which each worker has some private data. Coordinated by the server, the set
of workers cooperate to train a machine learning model within T epochs by solving the following
problem: min& f (θ) where f (θ) :“ XK“1 PkFk(θ) where Fk(∙) is the local objective of worker k
and Pk is the weight assigned to workerk and satisfies Pk20 and Xk Pk = 1. The local objective
Fk(θq is usually defined as the empirical risk over worker k’s local data with model parameter θ P Θ.
That is, Fk(θ) “ N XNiI '(θ; (Xjk ,yjk)), where Nk is the number of data samples available locally
on worker k, '(∙, ∙) is the loss function, and (Xjk, yjk) :“ Zjk is the jth data sample that is drawn
i.i.d. from some distribution Pk. It is typical to set Pk “ NN, where N “ Xk Nk the total number of
data samples across workers.
If all the local workers’ data distributions are the same (i.e., Pk “ Pk1 for all k, k1 P rKs), we call the
workers’ data are i.i.d.; otherwise, the data are non-i.i.d.. We write Ppk as the empirical distribution
of the Nk data samples drawn from Pk, and let P :“ XK“ NNkPk denote the mixture empirical
distribution across workers.
The FL algorithm (see Algorithm 1 in Appendix B.2) works as follows: at each time step t, a random
subset St of size w is uniformly sampled without replacement from the workers set rKs by the server
for synchronous aggregation (Li et al., 2019). The process of selecting workers for aggregation is
called subsampling. Let κ “ w{K denote the subsampling rate. Each selected worker k P rws then
samples a minibatch bk of size B from its local data distribution Pk. The worker then calculates the
average local gradient gk'1 D B XzPbk Vθ'(θt; Z) and sends the gradient to the server. The server
then uses an aggregation rule to compute the aggregated gradient gt'1 D AggTpgk1,…,gkw1q
where k P St, and updates the global model parameters θt'1 D θt — ηgt'1 where η is the learning
2
Under review as a conference paper at ICLR 2022
rate. The newly updated model parameters θt`1 are then sent to the selected workers to perform the
next FL iteration.
Threat Model. We assume that among the K workers, M(1 ≤ M < K) of them are malicious. Let
A denote the set of malicious attackers. Further, these attacker are fully cooperative and share the
same goal of compromising the FL system. We consider untargeted model poisoning attacks where
the M cooperative attackers send crafted local updates tgrkt ukPA to the server in order to maximize
the empirical loss, i.e., maxθ f(θ). They are coordinated either by one leading attacker or an external
agent. We refer such agent as a leader agent.
We assume the attackers know the global model parameters received from the server, tθt u, the
local training algorithm (including the batch size B) and their local data distributions tPkukPA. In
white-box attacks, we assume that the attackers further obtain information about the server’s training
algorithm. This information includes the server’s learning rate η, the subsampling rate κ, the total
number of workers K, and the aggregation rule Aggr. In black-box attacks, the attackers have no
access to the server’s training algorithm. They must estimate these parameters instead.
In practice, the attackers may communicate with each other to share their local information such
as local data distributions, the status of whether being selected by the server or not, their unique
identifiers, and each attacker’s action. Such internal communication allows each selected attacker i
to obtain the following information at each time step t: the number of attackers being selected by
the server mt(0 ≤ mt ≤ M), and its rank information σt P {0,…，mt — 1} in the set of the selected
attackers At . The rank is obtained by sorting the selected attackers according to their identifiers in
ascending order. The rank of a selected malicious worker plays a similar role as a unique identifier
does. It distinguishes the malicious worker from the other selected malicious workers. In our work,
we use the ranks instead of the unique identifiers to accelerate policy training in the presence of
worker subsampling. In addition, we assume the attackers obtain a lower bound of the total number
of training epochs T.
3	MARL Attack Framework Against Federated Learning
3.1	Overview
In this work, our goal is to design a non-myopic
coordinated attack against federated learning. To
this end, we formulate the attacker’s problem as
a fully cooperative Markov game (Section 3.2) by
viewing the FL training process as the environment.
The main obstacle for solving the game, however,
is that benign agents’ local data distributions are
unknown to the attacker, making it difficult to eval-
uate the effectiveness of a joint attack policy, which
is necessary to compute the rewards to the attackers.
An important observation of our approach is that
although the joint empirical distribution tPk}kPrKs
is unknown, the attackers can learn an approxima-
Figure 1: An overview of the MARL attack
framework against federated learning.
tion of the mixture distribution PP “ XK“1 Nk Pk, denoted by P, from model updates shared by the
server. In the black-box setting, the attackers can further utilize the model updates to infer FL training
parameters, which together with P are often sufficient to simulate the behavior of benign agents and
the server. Thus, our model-based reinforcement learning attack framework naturally consists of the
following three stages (see Figure 1), all of which happen while federating learning is ongoing.
Distribution learning. Initially, the set of attackers jointly learn a mixture distribution P from the
model updates tθt} using a gradient leakage based inference attack (Section 3.3). In the black-box
setting, they further infer key FL training parameters from the model updates (Section 3.5).
Policy learning. The learnt parameters are used to build a fully cooperative Markov game for the
attackers, which is solved using centralized training to identify the set of decentralized attack policies
that are distributed to the attackers (Section 3.3).
Attack execution and update. The set of attackers coordinate to attack the FL system. In the
black-box setting, the attack policies are updated when new model updates are received (Section 3.5).
3
Under review as a conference paper at ICLR 2022
3.2	Attackers’ problem as a fully cooperative Markov game
We formulate the attackers’ optimization problem as a fully cooperative Markov game, denoted by
M “ pS, A, T, r, Hq, where
•	S is the state space. Let τ P t0, 1, ...u denote the index of the attack step and tpτq P rTs the
corresponding FL epoch when at least one attacker is selected by the server. The state at step τ is
defined as sτ :“ pθtpτq, Atpτqq where Atpτq is the set of attackers selected at time tpτ q. We further
define the observation of attacker i at τ as oiτ :“ pθtpτq, mtpτq, σitpτqq, which can be derived from
sτ . Let Oi denote the space of oiτ .
•	A is the space of the attackers, joint actions. If attacker i is selected at T, its action aT ：“ rtpτq'1 P
Rd is the local update that attacker i sends to the server at time step tpτ q, where d is the dimension
of the model parameters. The only action available to an attacker not selected at tpτq is K, indicating
that the attacker does not send any information in that step. Let Ai denote the domain of attack i’s
actions, We have A :“ Ai X …X AM.
•	T : S X A → P(S) is the state transition function that represents the probability of reaching a
state s1 P S from the state s P S When attackers choose actions aτ1 , ..., aτM, respectively.
•	r : S X A X S → Reo is the reward function. We define the reward at step T as rτ :“
f (θtpτ'1q) — f (θtpτq), which is determined by thejoint actions of attackers and shared by all the
attackers. Both the transition probability T and the reward function r are determined by the joint
empirical distribution across workers tPkukPrKs (fixed but unknown to the attackers), the number
of workers K, the number of workers w selected for each time step, the size of local minibatch
B, the algorithm used by each worker, the aggregation rule used by the server, and the attackers’
actions aτ1 , ..., aτM.
•	H is the number of attack steps in each episode. H is a hyperparameter that can be adjusted, but
we require t(H) V T so that the attackers have time to execute attacks.
The attackers’ goal is to jointly find an attack policy π “ (π1, ..., πM) that maximizes the expected
total rewards over H attack steps, i.e., ErXH´1 rτS, where ∏ :Oi → P(Ai) denotes a stationary
policy of attacker i that maps its observation oito a probability measure over Ai. Using the definition
of rτ , this objective is equivalent to finding a policy π that maximizes EθtpHq rf(θtpHq)s.
We note that conceptually our problem can be formulated as a single agent RL problem as the set
of attackers share a common goal. However, this approach leads to an action space that grows
exponentially with the number of attackers, making the direct application of standard RL algorithms
difficult. In particular, existing single-agent RL algorithms cannot easily exploit the inherent symmetry
in our problem where the ordering of attack actions does not matter. By viewing the problem as a
fully cooperative multi-agent Markov game, the symmetry can be naturally incorporated by using the
rank information as part of individual attacker’s observation, leading to a more scalable solution.
3.3	White-B ox MARL Attacks
In this section, we focus on the white-box attack setting where the attackers obtain information about
the server’s training algorithm and defer the discussion of the black-box attack to Section 3.5.
Distribution learning. Initially, the attackers do not perform model-poisoning attacks. Instead,
t
they jointly learn a mixture distribution P from the model updates tθt u using a gradient leakage
based inference attack (Geiping et al., 2020; Zhu et al., 2019). This gives rise to a new Markov game
ɔl' fc Λ mt t ττ∖ ι	mt t t	t ∙ t ι~	τ->
M “ (S, A, T1, r1, H) where T1 and r1 are derived from P.
Note that from any two consecutive model updates received from the server, the attackers can estimate
a batch-level gradient UT :“ (θtpτ´1q — θtpτq){(η(t(τ) — t(τ — 1))). A gradient leakage attack works
by starting with (randomly generated) dummy data and feeding it into the model to get dummy
gradients. The dummy data is then iteratively updated until the dummy gradients get close to the real
gradients.
Various gradient leakage attacks have been proposed in the literature. In this work, we adapt the
inverting gradients (IG) method (Geiping et al., 2020) to distribution learning (see Appendix B.2 for
the detailed description of our adaptation). The IG method reconstructs data samples by optimizing a
loss function based on the angles (i.e., cosine similarity) of gradients to find data samples that lead to
a similar change in model prediction as the ground truth. Note that the goal of IG is to reconstruct the
original data samples, which is more ambitious than what the attackers need in our setting. On the
4
Under review as a conference paper at ICLR 2022
other hand, recent works on gradient leakage including IG have focused on the server side, where the
true gradients of each individual worker can be easily obtained from model updates. In contrast, the
attackers only obtain approximated batch-level gradients due to model aggregation and subsampling.
Despite these differences, our experiment results show that the IG method is very effective in learning
P (see Figure 2 in Appendix C.2).
In addition to the IG method, several other existing methods can also be utilized. For instance, the
deep leakage from gradients method (DLG) (Zhu et al., 2019), the latent projection method (Karras
et al., 2020), the DeepInversion method (Yin et al., 2020) and the GradInversion method (Yin et al.,
2021). It is worth noting that the GradInversion method is capable of reconstructing individual images
with high fidelity from averaging gradients even for complex datasets like ImageNet (Deng et al.,
2009), deep networks, and large batch sizes. These approaches can potentially be adopted to learn P
in more challenging settings.
Policy learning. With the distribution P estimated in the distribution learning stage, the leader
attacker can build an approximate Markov game M “ (S, A, T 1,r1, H) by simulating the FL training
environment as follows. We consider the white-box setting where the leader attacker knows the
total number of workers and FL training parameters. To simulate benign workers’ behavior in each
FL epoch, the leader attacker assumes that each benign worker has the same amount of data (i.e.,
1
Pk “ Kk), and samples a minibatch that is u.d. drawn from the same learned distribution P for each
benign worker and computes the respective gradients given the current model θtpτq . To simulate the
server’s behavior, the leader agent applies the same aggregation rule as the server to compute the next
model update.
As the attackers are fully cooperative and the leader agent has all the information needed to simulate
system dynamics, we adopt the framework of centralized training with decentralized execution
(CTDE). The centralized training can be implemented by the leader agent and no communication with
other attackers is needed during policy training. The learned policy is shared with all the attackers at
the end of policy training and executed in a decentralized way during attacks.
We use the multi-agent deep deterministic policy gradient (MADDPG) method (Lowe et al., 2017)
to train attack policies in our experiments, mainly because it allows continuous actions and it is
an off-policy algorithm and typically is more sample efficient than on-policy learning algorithms.
However, our attack framework is general to incorporate other MARL methods such as the multi-agent
TD3 (Ackermann et al., 2019) or the multi-agent PPO (Yu et al., 2021).
Since the attackers share the same reward function and differ in the observations only in our setting, it
suffices to train a single centralized action-value function Qμ(s, aι,..., aM) for all the attackers and a
shared deterministic policy μ : O → A where O “ Ui Oi and A “ Ui Ai. We adapt the MADDPG
algorithm to our fully cooperative setting with subsampling and name it CMADDPG (see Algorithm
3 in Appendix B.2).
Attack execution. After policy learning, each selected attacker i sends the crafted gradients according
to the learned policy μ and its local observation o% in the remaining FL epochs. Since attacker i's
local observation depends on its rank information, the set of attackers need to communicate with each
other in each epoch to share the number of selected attackers as well as their unique identifiers.
3.4	Design Considerations
Space reduction. When we train a small neural network with the federated learning system, it is
natural to use (θtpτq, At(Tq) as the state, and the gradient rtpτq'1 as the action. When we use the
federated learning system to train a large neural network, however, this approach does not scale as it
results in extremely large search space that requires both large runtime memory and long training time,
which is usually prohibitive. To solve this problem, we propose to approximate the state (θtpτq , Atpτq )
and the action gipτq'1 for high-dimensional data. To approximate the state, we use parameters of
the last hidden layer of the current neural network model to replace θtpτq in state (θtpτq , Atpτq). This
is because because the last hidden layer passes on values to the output layer and typically carries
information about important features of the model (Sun et al., 2014). Note that the true state is still
the full FL model that determines transition probabilities and rewards. We further define the action
a% as a one-dimension scaling factor aT P [—1,1], and set rtpτq'1 “ aT X gtpτq'1, which is used to
compute the next state and reward.
5
Under review as a conference paper at ICLR 2022
Gradient rescaling. To reduce the chance that the gradients from the attackers are filtered out by
the defense mechanisms (e.g., coordinate-wise median, Krum, or FLTrust), we further rescale the
gradients. When computing the gradients to be sent to the server, the attack method first calculates
the maximum value ZmaX and the minimum value Zmin that occur in any dimension of Vθl(θtpτq; Z)
for any z in the data samples generated in the distribution learning stage. The original scaling factor
aτ is in [—1,1]. The rescaled scaling factor aτ “ aτ X (ZmaX
Zmin){|ZmaX
| X 0.5.
—Zmin){|Zmax| X 0.5 ' (ZmaX
`
Training efficiency. The attackers’ total training time (including distribution learning and policy
learning) should be significantly less than the total FL training time so that the attackers have time to
execute the attacks. In real-world FL training, the server usually must wait for some time (typically
ranging from 1 minute to 10 minutes) before it receives responses from the clients (Yang et al., 2018;
Bonawitz et al., 2019; Kairouz et al., 2019). In contrast, the leader agent does not incur such time
cost in training attackers’ policies using a simulated FL environment. Therefore, an epoch in policy
learning is typically much shorter than an FL epoch, making it possible to train the attack policy with
a large number of episodes. In addition, the leader agent is usually equipped with GPUs, or other
parallel computing facilities and can run multiple training episodes in parallel (Clemente et al., 2017).
Non-i.i.d. data. In the basic approach described above, we propose to simulate benign works’
behavior using mini-batches i.i.d. sampled from the same distribution P. When the clients’ data are
known to be non-i.i.d., we further consider the following approach that generates data samples from
〜
distributions that are within a Wasserstein ball centered at P to obtain a more robust attack policy.
n 1	, 1 ,1 1 ,	1	∙ ∙ 1 1	.'	∕τ>	( I > ττ7- / τ-t r^∖ 一	ττ r / τ-t
More concretely, the data samples are i.i.d. drawn from P “ {P : Wi (P, P) ≤ maxkp∕ Wι(Pk ,P)},
where Ppk is attacker k’s empirical distribution. In practice, attackers may not obtain prior information
about the data heterogeneity of an FL system. In such case, the attackers may measure the Wasserstein
distances between the empirical data distributions of any two attackers to decide whether to adopt the
non-i.i.d. sampling method. If the Wasserstein distances are similar, the attackers may conclude that
the workers’ data distributions are i.i.d.; otherwise, the workers’ data are non-i.i.d..
3.5	Black-box MARL Attacks
In our white-box MARL attack described above, we assume that the attackers have access to the
server’s algorithm including the aggregation rule Aggr and parameters such as the learning rate η, the
worker subsampling rate κ, and the total number of workers K . In practice, such knowledge might
be unavailable to the attackers. They must estimate these parameters in order to learn an effective
attack policy. We call this attack scenario black-box attacks. Built upon our white-box attack, we
present a black-box MARL attack method. Due to space limitations, we only provide a summary of
the high level ideas in this section and defer the detailed discussion to Appendix B.4.
We first introduce methods to estimate the three missing parameters using global model updates and
the attackers’ local gradients. In particular, we estimate the server’s learning rate η using the model
difference in two consecutive steps when at least one attacker is sampled divided by the average
gradient computed from the attackers’ local data. We use the empirical subsampling rate of the
malicious workers as an estimate of the server’s subsampling rate κ. We estimate the number of
workers w sampled in each time step by comparing the variance of the average gradients across
all sampled workers’ and that of an individual attacker. The estimates of κ and w together give an
estimate of K.
We then propose a method to learn a non-linear approximation of the server’s aggregation rule. In
particular, we use a small neural network with two inputs that model the average gradients of normal
workers and that of malicious workers, respectively. The former is estimated from the learned mixture
distribution P, while latter is computed using the attackers’ local data.
After parameter estimation, the leader attacker first learns an initial attack policy similar to the
white-box attack. To further improve the quality of the estimated aggregation rule and subsequently
improve the performance of the attack policy, we apply the Adaptation augmented Model-based Policy
Optimization (AMPO) (Shen et al., 2020) method to continuously adapt the aggregation rule and learn
the attack policy. The AMPO method was originally designed to address the distribution mismatch
problem for better policy optimization in Dyna-style model-based reinforcement learning (Sutton,
1990). We adapt the original AMPO by explicitly requiring the model adaptation procedure to learn
a non-linear representation of the aggregation rule. The black-box MARL attack method has the
same three stages as the white-box MARL attacks: distribution learning, policy learning and attack
6
Under review as a conference paper at ICLR 2022
execution. The key difference is that the learned policy will be continuously adapted along with
attack execution.
4	Impact of inaccurate distribution and data heterogeneity
zʌ	ɪ <- * T-« ɪ . .	1	1	.1	. ∙	.	1	1	jv> .	♦	1	.	.1	1	1	♦	i' Λ	♦
Our MARL attack employs the estimated data distribution P to simulate the behavior of benign
workers, which can suffer from two types of errors. First, P can be far away from the true mixture
distribution Pp due to inaccurate distribution learning. Second, benign workers may vary in their local
data distributions Ppk, which cannot be fully captured by a single mixture distribution. In this section,
we study how the attack performance is affected by these two factors, which provides insights into
properly distributing resources between the three stages of our attack. To simplify the discussion,
we focus on the white-box setting and assume that data samples are drawn from P when simulating
benign workers in our analysis, ignoring the further optimization discussed in Sections 3.4 and 3.5.
Our analysis is adapted from recent works that study the impact of model inaccuracy on the perfor-
mance of model-based reinforcement learning (Yu et al., 2020; Luo et al., 2018; Zhang et al., 2020)
by addressing two key challenges. First, we need to establish the connection between the inaccuracy
in data distribution P and the inaccuracy in the corresponding Markov game model as both the
reward function and the transition dynamics depend on P . Second, although there are different
ways to measure the distance between two models (Yu et al., 2020), it makes more sense to use the
Wasserstein distance to measure the distance between two data distributions. This, however, requires
bounding the Lipschitz constant of the optimal value function (Yu et al., 2020). Although this is a
challenging task for general RL tasks, we are able to show that this is indeed the case in our setting
under the following assumptions. The first assumption models the inaccuracy of distribution learning
as well as the heterogeneity of benign worker’s local data .
Assumption 1. Wι(P, Pk) ≤ δ for any benign worker k.
We further need the following standard assumptions on the loss function.
Assumption 2. Let Z denote the domain of data samples across all the workers. For any s1 , s2 P S
and zι, z2 P Z, the lossfunction ' : S X Z → R satisfies:
1.	∣'(sι, zι) — '(s2, z2)∣ ≤ L}Psι,zι) 一 (s2, z2)}2	(LipSchitz continuity w.r.t. S and Z);
2.	}Vs'(sι,zι) — Vs'(sι, z2)}2 W Lz}zι — z2}2	(Lipschitz smoothness w.r.t. Z);
3.	'(s2,z1)	> '(sι,zι)	'	XVs'(sι, Zi),	S2 — Si〉+ 2 ∣∣S2	— S1}2 (strongly convex w.r.t.	S);
4.	'(s2, zi)	W '(sι,zι)	+	XVs'(si, zi),	S2 — si〉+ 2∣∣s2	— si}2 (strongly smooth w.r.t.	S);
5.	'(∙, ∙) is twice continuously differentiable with respect to S.
where ∣(Si, Zi) — (S2, Z2)∣22 “ ∣Si — S2∣22+ ∣Zi — Z2∣22. For simplicity, we further make the following
assumption on the FL environment, although our analysis can be readily applied to more general
settings.
Assumption 3. The server adopts FedAvg without subsampling (w “ K). All workers have same
amount of data (Pk “ -^) and the local minibatch Size B = 1. In each epoch offederated learning,
each normal worker’s local minibatch is sampled independently from the local empirical data
distribution Pk.
. 1 , /CA m T-Γ∖ 1	.1	. . 1	C.1 . . 1	1 ɔʧ /c Λ ml ， τ τ∖
Let M “ (S, A, T, r, H) denote the true Markov game for the attackers, and M “ (S, A, T1, r1, H)
the simulated Markov game when the local distribution of any benign worker is estimated as P . Let
T(S1|S, a) and r(S, a, S1) denote the transition dynamics and reward function for M, respectively.
The following theorem captures the attack performance loss due to inaccurate distribution learning
(see Appendix D for the proof).
Theorem 1. Let JMpnq ：“ E∏,τ,μ0 r∑H0i r(St, at, St'i)S denote the expected return over H attack
steps under M, policy π and initial state distribution μo. Let π* and r be the optimal policies for
t √	1 ɔʃ	. ■ I ■ .1 .1
M and M respectively, with the
same initial state distribution μo. Then,
|加(∏*)—九(∏)∣W 2Hcδ[(L + Lv)Lzη + 2L],
where E “ KKM is the fraction of benign nodes, Lv W XH“：(KF)t(L + LKF) and KF W
C max{∣1 — ηα∣, |1 — ηβ∣}.
7
Under review as a conference paper at ICLR 2022
Median - CIFAR-IO
FLTrust - CIFAR-IO
Median - ImageNet
FLTrust - ImaaeNet
Figure 2: A comparison of average classification error rates for FL with four different aggregation rules
(coordinate-wise median, FLTrust, Safeguard, and Centered Clipping) on three different datasets (Fashion-
MNIST, CIFAR-10 and ImageNet) for both i.i.d. data and non-i.i.d. data. Key parameters: number of workers =
1, 000, number of attackers = 50, subsampling rate = 20%. All other parameters are set as default. Error bars
indicate the standard deviations. The results for Krum follow similar trends and are omitted to save space.
In practice, the learning rate η is typically small enough so that max{∣1 — ηα∣, |1 一 ηβ∣} ≤ 1. In this
case, LV is bounded by LI´KKKF q ≤ L1fq. Therefore, we have ∣JM(∏*) — JM(∏)∣ “ OpHi´ɪ ηδ).
To ensure convergence, we typically have η “ Op?H) (Polyak, 1987), thus | JM(∏*) — JM(∏)∣ =
Op 占 δ?H).
Our theoretical result indicates the impact of imperfect information on attack performance, which can
potentially be utilized to derive a proactive defense that manages to increase the attackers’ uncertainty.
5	Experiments
We conduct extensive experiments on three real-world datasets: Fashion-MNIST (Xiao et al., 2017),
CIFAR-10 (Krizhevsky et al., 2009) and ImageNet (Deng et al., 2009). We compare the performance
of different FL attack methods: no attack (NA), random attack (RN), inner product manipulation
(IPM) (Xie et al., 2020b), local model poisoning attack (LMP) (Fang et al., 2020), explicit boosting
(EB) (Bhagoji et al., 2019). We further consider three variants of our RL-based methods, namely,
RL with a single attacker (RL), RL with multiple independent attackers (IRL), and the proposed
MARL attacks, to understand the advantage of multi-agent coordination in attacking FL systems.
For our MARL attacks, we consider both white-box attack (WM) and black-box attack (BM). We
consider the FL settings where the server is equipped with one the following defense mechanisms:
Krum (Blanchard et al., 2017), coordinated-wise median (Yin et al., 2018), FLTrust (Cao et al., 2020),
Safeguard (Allen-Zhu et al., 2020) and Centered Clipping (Karimireddy et al., 2021). See Appendix
C for details of the datasets, experimental setups, and additional results.
Results. Due to no-myopic policy learning, our model-based RL methods (RL, IRL, and MARL)
consistently outperform existing methods that craft model updates myopically with heuristics such as
LMP, EB and IPM by a large margin (see Figure 2). For instance, both the white-box MARL attack
and the black-box MARL attack achieve misclassification rates of above 0.80 for Fashion-MNIST
dataset (both i.i.d. and non-i.i.d.) when the server is equipped with FLTrust. In comparison, the
misclassification errors of all the existing baselines are below 0.30. Adaptive defenses such as
Safeguard and Centered Clipping provide a higher degree of robustness due to their capability of
limiting time-coupled attacks. However, they are still vulnerable to our reinforcement learning based
attacks. As shown in Figure 2, the MARL attacks achieved a classification error rate of over 60% for
all the three datasets, while that of all other baseline attacks are below 25% under these two defenses.
Impact of distribution learning. We expect that higher quality of distribution learning allows the
attackers to learn better attack policies, which is confirmed in Figure 3(a), where the misclassification
rates increase as the threshold ν of distribution learning reduces. On the other hand, even when the
attackers use random data samples as the dummy data (DLR), the misclassification rates still reach
8
Under review as a conference paper at ICLR 2022
Figure 3: A comparison of average classification error rates and Wasserstein distance by varying configurations
of RL-based methods for FL with FLTrust defense rule on Fashion-MNIST with i.i.d. data. Error bars indicate
the standard deviations. In (a) and (c), DL: default distribution learning; NDL: no distribution learning; DLR:
distribution learning with random dummy data. In (d), FC: coordination in all the stages; NC: no coordination;
CDL: coordinated distribution learning and uncoordinated policy learning.
above 0.55 for both white-box attacks and black-box attacks. In comparison, without distribution
learning, the misclassification rates of the RL-based attack methods (NDL) drop to a level below 0.5.
Further, coordinated distribution learning allows the proposed MARL methods to learn the mixture
distribution substantially more efficiently than RL and IRL. As shown in Figure 3(b), it takes 206 FL
epochs and 224 epochs for WM and BM to learn an mixture distribution P that is within ν “ 0.1
Wasserstein distance from the empirical distribution P . Nevertheless, it takes 458 FL epochs for IRL
to learn the same quality of mixture distribution.
White-box vs. Block-box attacks. The attackers’ knowledge of the server’s aggregation rule also
largely influences attack performance (see Figure 3(c)). The attack performance increases from 0.497
when the attackers use a completely false aggregation rule (False) such as the average aggregation
rule to 0.90 when the attackers knows the true aggregation rule (WM). When the attackers use the
black-box attacks (BM) with policy adaptation, the classification error rate reaches 0.873, which is
on par with the white-box attacks. This is partly due to the effectiveness of our proposed method for
estimating server’s parameters in the i.i.d. setting, such as learning rate (actual η “ 0.001 vs. estimate
η “ 0.0012), the subsampling rate (actual K “ 20% vs. estimate K “ 19.7%), and the number
of total workers (actual K “ 1, 000 vs. estimate K “ 1, 083). More importantly, the continuous
model adaptation significantly improves the quality of the estimated non-linear aggregation rule and
consequently the quality of policy learning. Even the black-box attack that learns a linear aggregation
rule (BM(Linear)) outperforms the polynomial attack policy (Polynomial) and the linear attack policy
(Linear) without model adaptation. This indicates the striking usefulness of model adaption in policy
learning.
Importance of coordination. Among the three RL-based methods, the proposed MARL methods
(WM and BM) perform the best due to better coordination among the attackers in distribution
learning, policy learning and attack execution. To further understand the importance of multi-agent
coordination in learning effective attacks, we investigate the use of coordination in different stages of
our model-based RL attack method (see Figure 3 (d)). With full coordination (FC) in distribution
learning, policy learning and attack execution, the MARL methods achieve a misclassification rate of
above 0.80. This is in comparison to below 0.60 when no coordination (NC) involved in the RL-based
methods. When the RL-based methods has coordinated distribution learning and uncoordinated policy
learning (CDL), the attack performance lies in between at around 0.70. These results demonstrate the
important role of multi-agent coordination in attacking federated learning systems.
6	Conclusion
We propose a multi-agent reinforcement learning attack framework to learn a non-myopic attack
policy that can effectively compromise FL systems even with advanced defense mechanisms applied.
Our attack framework can be extended to incorporate advances in meta-learning (Finn et al., 2017;
Gupta et al., 2018) to generalize the learned policy to different training tasks. For instance, the leader
attacker can first learn a distribution over different tasks and train a model-agnostic policy with the
learned distribution by utilizing model-agnostic meta-learning (MAML) (Finn et al., 2017). Given a
new task, the leader attacker can then train an attack policy fast by adapting from the model-agnostic
policy with few shots. We envision that this approach can be especially useful when similar tasks are
trained over different sets of clients. While we focus on untargeted attacks against FL systems in
this paper, our attack framework can be extended to targeted attacks or backdoor attacks. Another
direction is to investigate novel methods to defend our adaptive attack methods. One possible solution
would be to dynamically adjust FL parameters such as the subsampling rate or the aggregation rule.
9
Under review as a conference paper at ICLR 2022
References
Johannes Ackermann, Volker Gabler, Takayuki Osa, and Masashi Sugiyama. Reducing overestimation
bias in multi-agent domains using double centralized critics. arXiv preprint arXiv:1910.01465,
2019.
Zeyuan Allen-Zhu, Faeze Ebrahimian, Jerry Li, and Dan Alistarh. Byzantine-resilient non-convex
stochastic gradient descent. arXiv preprint arXiv:2012.14368, 2020.
Kavosh Asadi and Michael L Littman. An alternative softmax operator for reinforcement learning.
In International Conference on Machine Learning, pp. 243-252. PMLR, 2017.
Kavosh Asadi, Dipendra Misra, and Michael Littman. Lipschitz continuity in model-based rein-
forcement learning. In International Conference on Machine Learning, pp. 264-273. PMLR,
2018.
Eugene Bagdasaryan, Andreas Veit, Yiqing Hua, Deborah Estrin, and Vitaly Shmatikov. How to
backdoor federated learning. In International Conference on Artificial Intelligence and Statistics,
pp. 2938-2948. PMLR, 2020.
Moran Baruch, Gilad Baruch, and Yoav Goldberg. A little is enough: Circumventing defenses for
distributed learning. arXiv preprint arXiv:1902.06156, 2019.
Arjun Nitin Bhagoji, Supriyo Chakraborty, Prateek Mittal, and Seraphin Calo. Analyzing federated
learning through an adversarial lens. In International Conference on Machine Learning, pp.
634-643. PMLR, 2019.
Peva Blanchard, Rachid Guerraoui, Julien Stainer, et al. Machine learning with adversaries: Byzantine
tolerant gradient descent. In Advances in Neural Information Processing Systems, pp. 119-129,
2017.
Keith Bonawitz, Hubert Eichner, Wolfgang Grieskamp, Dzmitry Huba, Alex Ingerman, Vladimir
Ivanov, Chloe Kiddon, Jakub Konecny, Stefano Mazzocchi, H Brendan McMahan, et al. Towards
federated learning at scale: System design. arXiv preprint arXiv:1902.01046, 2019.
Xiaoyu Cao, Minghong Fang, Jia Liu, and Neil Zhenqiang Gong. FLTrust: Byzantine-robust federated
learning via trust bootstrapping. arXiv preprint arXiv:2012.13995, 2020.
YU Chao, A VELU, E VINITSKY, et al. The surprising effectiveness of ppo in cooperative, multi-
agent games. arXiv preprint arXiv:2103.01955, 2021.
Alfredo V Clemente, Humberto N Castejon, and Arjun Chandra. Efficient parallel methods for deep
reinforcement learning. arXiv preprint arXiv:1705.04862, 2017.
Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale
hierarchical image database. In 2009 IEEE conference on computer vision and pattern recognition,
pp. 248-255. Ieee, 2009.
Minghong Fang, Xiaoyu Cao, Jinyuan Jia, and Neil Gong. Local model poisoning attacks to
byzantine-robust federated learning. In 29th USENIX Security Symposium, pp. 1605-1622, 2020.
Chelsea Finn, Pieter Abbeel, and Sergey Levine. Model-agnostic meta-learning for fast adaptation of
deep networks. In International Conference on Machine Learning, pp. 1126-1135. PMLR, 2017.
Clement Fung, Chris JM Yoon, and Ivan Beschastnikh. Mitigating sybils in federated learning
poisoning. arXiv preprint arXiv:1808.04866, 2018.
Jonas Geiping, Hartmut Bauermeister, Hannah Droge, and Michael Moeller. Inverting gradients-how
easy is it to break privacy in federated learning? arXiv preprint arXiv:2003.14053, 2020.
Tianyu Gu, Brendan Dolan-Gavitt, and Siddharth Garg. Badnets: Identifying vulnerabilities in the
machine learning model supply chain. arXiv preprint arXiv:1708.06733, 2017.
Abhishek Gupta, Benjamin Eysenbach, Chelsea Finn, and Sergey Levine. Unsupervised meta-learning
for reinforcement learning. arXiv preprint arXiv:1806.04640, 2018.
10
Under review as a conference paper at ICLR 2022
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image
recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition,
pp. 770-778, 2016.
Briland Hitaj, Giuseppe Ateniese, and Fernando Perez-Cruz. Deep models under the gan: information
leakage from collaborative deep learning. In Proceedings of the 2017 ACM SIGSAC Conference
on Computer and Communications Security, pp. 603-618, 2017.
Peter Kairouz, H Brendan McMahan, Brendan Avent, AUrelien Bellet, Mehdi Bennis, Arjun Nitin
Bhagoji, Kallista Bonawitz, Zachary Charles, Graham Cormode, Rachel Cummings, et al. Ad-
vances and open problems in federated learning. arXiv preprint arXiv:1912.04977, 2019.
Sai Praneeth Karimireddy, Lie He, and Martin Jaggi. Learning from history for byzantine robust
optimization. In International Conference on Machine Learning, pp. 5311-5319. PMLR, 2021.
Tero Karras, Samuli Laine, Miika Aittala, Janne Hellsten, Jaakko Lehtinen, and Timo Aila. Analyzing
and improving the image quality of stylegan. In Proceedings of the IEEE/CVF Conference on
Computer Vision and Pattern Recognition, pp. 8110-8119, 2020.
Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images. 2009.
Xiang Li, Kaixuan Huang, Wenhao Yang, Shusen Wang, and Zhihua Zhang. On the convergence of
fedavg on non-iid data. arXiv preprint arXiv:1907.02189, 2019.
Ryan Lowe, Yi Wu, Aviv Tamar, Jean Harb, Pieter Abbeel, and Igor Mordatch. Multi-agent actor-
critic for mixed cooperative-competitive environments. arXiv preprint arXiv:1706.02275, 2017.
Yuping Luo, Huazhe Xu, Yuanzhi Li, Yuandong Tian, Trevor Darrell, and Tengyu Ma. Algorithmic
framework for model-based deep reinforcement learning with theoretical guarantees. arXiv preprint
arXiv:1807.03858, 2018.
Lingjuan Lyu, Han Yu, and Qiang Yang. Threats to federated learning: A survey. arXiv preprint
arXiv:2003.02133, 2020.
Brendan McMahan, Eider Moore, Daniel Ramage, Seth Hampson, and Blaise Aguera y Arcas.
Communication-efficient learning of deep networks from decentralized data. In Artificial Intelli-
gence and Statistics, pp. 1273-1282. PMLR, 2017.
Luca Melis, Congzheng Song, Emiliano De Cristofaro, and Vitaly Shmatikov. Exploiting unintended
feature leakage in collaborative learning. In 2019 IEEE Symposium on Security and Privacy, pp.
691-706, 2019.
El Mahdi El Mhamdi, Rachid Guerraoui, and Sebastien Rouault. The hidden vulnerability of
distributed learning in byzantium. arXiv preprint arXiv:1802.07927, 2018.
Nicolas Papernot, Fartash Faghri, Nicholas Carlini, Ian Goodfellow, Reuben Feinman, Alexey Ku-
rakin, Cihang Xie, Yash Sharma, Tom Brown, Aurko Roy, Alexander Matyasko, Vahid Behzadan,
Karen Hambardzumyan, Zhishuai Zhang, Yi-Lin Juang, Zhi Li, Ryan Sheatsley, Abhibhav Garg,
Jonathan Uesato, Willi Gierke, Yinpeng Dong, David Berthelot, Paul Hendricks, Jonas Rauber,
and Rujun Long. Technical report on the cleverhans v2.1.0 adversarial examples library. arXiv
preprint arXiv:1610.00768, 2018.
Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor
Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. Pytorch: An imperative style,
high-performance deep learning library. arXiv preprint arXiv:1912.01703, 2019.
Boris T. Polyak. Introduction to optimization. Optimization Software, 1987.
Leonid I Rudin, Stanley Osher, and Emad Fatemi. Nonlinear total variation based noise removal
algorithms. Physica D: nonlinear phenomena, 60(1-4):259-268, 1992.
Jian Shen, Han Zhao, Weinan Zhang, and Yong Yu. Model-based policy optimization with unsuper-
vised model adaptation. Advances in Neural Information Processing Systems, 33, 2020.
Aman Sinha, Hongseok Namkoong, and John Duchi. Certifying some distributional robustness with
principled adversarial training. In International Conference on Learning Representations, 2018.
11
Under review as a conference paper at ICLR 2022
Yi Sun, Xiaogang Wang, and Xiaoou Tang. Deep learning face representation from predicting 10,000
classes. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp.
1891-1898,2014.
Yiwei Sun, Suhang Wang, Xianfeng Tang, Tsung-Yu Hsieh, and Vasant Honavar. Node injection
attacks on graphs via reinforcement learning. arXiv preprint arXiv:1909.06543, 2019.
Richard S Sutton. Integrated architectures for learning, planning, and reacting based on approximating
dynamic programming. In Machine learning proceedings 1990, pp. 216-224. Elsevier, 1990.
Richard S Sutton and Andrew G Barto. Reinforcement learning: An introduction. MIT press, 2018.
Leonid Nisonovich Vaserstein. Markov processes over denumerable products of spaces, describing
large systems of automata. Problemy Peredachi Informatsii, 5(3):64-72, 1969.
Han Xiao, Kashif Rasul, and Roland Vollgraf. Fashion-mnist: a novel image dataset for benchmarking
machine learning algorithms. arXiv preprint arXiv:1708.07747, 2017.
Chulin Xie, Keli Huang, Pin-Yu Chen, and Bo Li. DBA: Distributed backdoor attacks against
federated learning. In International Conference on Learning Representations, 2020a.
Cong Xie, Oluwasanmi Koyejo, and Indranil Gupta. Fall of empires: Breaking byzantine-tolerant
sgd by inner product manipulation. In Uncertainty in Artificial Intelligence, pp. 261-270. PMLR,
2020b.
Timothy Yang, Galen Andrew, Hubert Eichner, Haicheng Sun, Wei Li, Nicholas Kong, Daniel
Ramage, and Francoise Beaufays. Applied federated learning: Improving google keyboard query
suggestions. arXiv preprint arXiv:1812.02903, 2018.
Dong Yin, Yudong Chen, Kannan Ramchandran, and Peter Bartlett. Byzantine-robust distributed
learning: Towards optimal statistical rates. arXiv preprint arXiv:1803.01498, 2018.
Hongxu Yin, Pavlo Molchanov, Jose M Alvarez, Zhizhong Li, Arun Mallya, Derek Hoiem, Niraj K
Jha, and Jan Kautz. Dreaming to distill: Data-free knowledge transfer via deepinversion. In
Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp.
8715-8724, 2020.
Hongxu Yin, Arun Mallya, Arash Vahdat, Jose M Alvarez, Jan Kautz, and Pavlo Molchanov. See
through gradients: Image batch recovery via gradinversion. arXiv preprint arXiv:2104.07586,
2021.
Chao Yu, Akash Velu, Eugene Vinitsky, Yu Wang, Alexandre Bayen, and Yi Wu. The surprising
effectiveness of mappo in cooperative, multi-agent games. arXiv preprint arXiv:2103.01955, 2021.
Tianhe Yu, Garrett Thomas, Lantao Yu, Stefano Ermon, James Zou, Sergey Levine, Chelsea Finn, and
Tengyu Ma. Mopo: Model-based offline policy optimization. arXiv preprint arXiv:2005.13239,
2020.
Xuezhou Zhang, Yuzhe Ma, Adish Singla, and Xiaojin Zhu. Adaptive reward-poisoning attacks
against reinforcement learning. In International Conference on Machine Learning, pp. 11225-
11234. PMLR, 2020.
Ligeng Zhu, Zhijian Liu, and Song Han. Deep leakage from gradients. In Advances in Neural
Information Processing Systems, pp. 14774-14784, 2019.
12
Under review as a conference paper at ICLR 2022
Appendix
A	Appendix to Section 1: Introduction
A.1 Broader Impact
To study the vulnerabilities of federated learning, we propose a model-based multi-agent reinforce-
ment learning attack framework. Our work shows that non-myopic attacks with coordinated attackers
can break federated learning systems even when they are equipped with sophisticated defense rules
such as FLTrust (Cao et al., 2020), Safeguard (Allen-Zhu et al., 2020) and Centered Clipping (Karim-
ireddy et al., 2021). This reveals the urgent need of developing more advanced defense mechanisms
for federated learning systems. While we have focused on adversarial attacks against federated
learning in our work, we note that one possible solution to defending RL-based attacks would be to
dynamically adjust FL parameters such as the subsampling rate or the aggregation rule. Future work
is needed to identify how best to do so.
B Appendix to Section 3: MARL Attack Framework Against
Federated Learning
B.1	More Details of MARL
In addition, our MARL framework allows the attackers to incorporate stealthiness constraints (e.g.,
similar to the regularization term in Eq. 3 of (Bhagoji et al., 2019)) into the attack objective as the
attackers can estimate the average of the gradients of all the other workers in the previous epoch.
Impact of FL dynamics on the performance of MARL. In practice, the number of workers may
vary. Since the MARL attack framework learns an approximation of the distribution of the aggregated
data, our methods still apply even when new workers are added after the distribution learning stage
as long as their local data distributions do not deviate significantly from the estimated distribution.
Theorem 1 demonstrates that the performance loss is bounded in this case.
Another scenario to consider is that the number of data points on each worker can also change.
Our theoretical results indicate that the learned attack policy remains effective because inaccurate
estimation on the number of data points has limited impact on distribution learning and policy
learning, as long as workers’ local data distributions do not differ greatly.
B.2	Algorithms
In this subsection, we present the detailed algorithms for federated learning (Algorithm 1), distribution
learning (Algorithm 2), cooperative multi-agent deep deterministic policy gradients (Algorithm 3).
Algorithm 1 Federated Learning
Input: Initial weight θ0 , K workers indexed by k, size of subsampling w, local minibatch size B,
step size η, number of global training steps T
Output: θT
Server executes:
for t “ 0 to T ´ 1 do
St D randomly select W workers from K workers
for each worker j P S t in parallel do
gj'1 D WorkerUpdatej,θt)
end for
gt`1 * * * D AggrPgk`1,…,gk'1q,kiP S t
θt'1 D θt — ngt`1
end for
WorkerUpdatePj, θq:
Sample a minibatch b of size B
g D B EzPbJ'(θ,z)
return g to server
13
Under review as a conference paper at ICLR 2022
Inverting gradients based distribution learning. As shown in Algorithm 2, , the leader agent
uses all the M attackers’ local data to serve as the dummy data Ddummy . For each epoch tpτq
that at least one attacker is selected, the leader agent obtains the model update from one of the
attackers and calculates the batch-level gradient gτ. It then reconstructs the data samples used to
generated the gradient by iteratively solving the following optimization problem: arg minχpRn,ypR 1 一
∣∣χ7θ'pθjpx,yqq,g.y ' βTV(x), where n is the dimension of x, TV(x) is the total variation (Rudin
} vθ'p%pχ,yqq}∙}gτ }
et al., 1992) of x, and β is a fixed parameter. For each data point, the inverting gradients algorithm
terminates after max_iter iterations. After data reconstruction in each epoch T, all the reconstructed
data will be added to the set of dummy data. The approximated mixture distribution P (τq consists
of the reconstructed data up to t(τq and the M attackers’ local data. To determine whether an
approximated distribution is sufficiently accurate, we measure the 1-Wasserstein distance (with
the Euclidean norm as the distance metric) (Vaserstein, 1969) W1 (P (τ ´ 1q, P (τqq between the
approximated distributions of the two consecutive attack steps. We use the data points across all the
workers in the previous step and the current step to approximate the previous and current mixture
distributions. The distribution learning algorithm terminates when the Wasserstein distance is below
a predefined threshold ν. After distribution learning, the leader agent shares the learned distribution
P with all the attackers.
Algorithm 2 Distribution Learning
Input: Wasserstein distance threshold for termination ν, number of iterations for inverting gradients
maxSter, step size for FL η and step size for inverting gradients η1, model parameters {θtpτq}
〜
Output: P
Wasserstein distance W (P (´1), P (0)) ð 8,τ D 0
Ddummy D M attackers' local data
________, r≥/ ,	.	r≥/,..
while W(P(T — 1),P(τ)) > V do
τ → τ ' 1
Compute the aggregated gradients using gτ D (θtpτ´1) — θtpτq)/(η(t(τ) — t(τ — 1)))
for (x, y ) P Ddummy do
(χo,yo) D (χ,y)
for i “ 0 to max-iter — 1 do
Vθ'(θtpτq; (Xi, yi)) D B'(θtpτq; (Xi, yi))/Bθ
Li D 1 —
Sθ'Pθt(τ *Xi,yi)),gT〉
∣Vθ 'PθtSPχi,yi))∣H∣gτ∣l
` βTV(xi)
Xi'1 D Xi — η1VχiLi, yi'1 D y — η1VyiLi
end for
end for
Add newly all reconstructed data points (x, y) to Ddummy
Compute the current approximated mixture distribution P(τ) with all the reconstructed data
points and the M attackers’ local data
Compute the Wasserstein distance W(P(τ — 1), P(τ))
end while
Cooperative Multi-Agent Deep Deterministic Policy Gradient (CMADDPG). We let all the
agents (attackers) share the same action-value function Qμ(s,aι,…，aM) and the same policy
μφ(∙∣θi) with parameters φ. Note that although the attackers share the same policy, their ac-
tions in each step vary due to the unique observations they receive. Using the chain rule, we
can derive the gradient of the expected return J(φ) “ ErXH´1 rτS as follows: VφJ(φ) “
Es,a~D[∑M1 Vφμφ(θi)Vai Qμ(s, aι,..., ai,..., aM )仁=“(。2)], where the experience reply buffer
D contains tuples t(s, r, s1, a1, . . . , aM )u. Note that for attacker i not selected by the server in a
certain step, its action does not affect the Qμ value, which implies that Vai Qμ “ 0 for any a% when
the state indicates that attacker i is not sampled. Hence, the policy gradient formula makes sense
even when subsampling is applied. Similar to (Lowe et al., 2017), the shared action-value function
Qμ is updated by minimizing the loss:
L(φ) “ Es,a,r,s1 try — Q"(s, aι, . . . , aM )s2 u,	y = r + Qμ'(s1, a1, . . . , aM )lak=μipok).	⑴
where μ1 is the target policy with delayed parameters φ1.
14
Under review as a conference paper at ICLR 2022
Algorithm 3 Cooperative Multi-Agent Deep Deterministic Policy Gradient (CMADDPG)
for episode “ 1 to max_episode do
Initialize a random process N for action exploration
Receive initial state s
for τ “ 1 to H do
for each attacker i, select action a% “ μφ(θi)' NT w.r.t the current policy and exploration
Execute actions a “ pa1, . . . , aMq and observe reward r and new state s1
Store ps, a, r, s1q in replay buffer D
S D s1
Sample a random minibatch of C samples psj , aj , rj , s1jq from D
Set yj = rj + Qμ1 ps1j, a1,..., aM qιaιs =μi(oj)
Update critic by minimizing the loss:
L(φ) = C ∑jryj ´ Q"(sj, a1,..., aM qs2
Update actor using the sampled policy gradient:
vφJ « C ∑j∑N 巾”巾(Oj qVai Qμpsj ,a1,...,ai,..., aM qlai="(ojj
Update target network parameters φ1 D α1φ + (1 — a1)φ1
end for
end for
B.3 Black-box MARL Attacks
B.3. 1 Parameter Estimation
We describe parameter estimation in detail. The estimation of the learning rate η , the subsampling
rate κ and the total number of workers K take place in distribution learning and the estimation of
aggregation rule Aggr happens in policy learning.
Estimating learning rate η. According to the FL algorithm (see Algorithm 1), we know θt =
θt´1 — ηgt. Thus, we have η = (θt´1 — θtq{gt. Here, both θt´1 and θt are the FL model parameters
known the attackers, while gt is average gradients of all the selected workers, which is unavailable
to the attackers. We approximate gt by using the attackers’ average gradients. That is, gt «
ml ∑mmt1 gk'1. Thus, the learning rate at t(τ) can be estimated as ητ = m(θt(TTq — θtpτq)∕((t(τ)—
t(τ — 1)) ∑mmL1 gT´1). We further assume that ητ P [—0.1,0.1] and drop any values out of this range,
and estimate the learning rate η as the average of valid ητ over the first h time steps, where h is the
terminating epoch of distribution learning. We note that this estimate is accurate when the mixture of
the attackers’ local distributions represents the mixture of all the workers’ distributions.
Estimating subsampling rate κ. In practice, the subsampling rate among the malicious workers
should be equal to that among all workers if the selected workers are uniformly distributed. Thus, we
propose to use the empirical subsampling rate of the malicious workers as an estimate of the server’s
subsampling rate κ. The estimated subsampling rate is calculated by K = ∑h=0 hM, where mτ is
the number of selected malicious workers at time τ , and M is the total number of malicious workers.
Estimating total number of workers K. We propose to estimate the total number of workers K by
utilizing properties of the variance of the malicious workers’ gradients. Consider any attack step τ .
Let Xk = gk (θt(Tq, zk ) denote the average gradient of worker k obtained from a randomly selected
batch where Zk 〜Pk and Pk is the local distribution of selected attacker k. Let X = 1 ∑W=ι Xk
denote the average of Xk across all workers. Let Vk and V denote the variance of Xk and Xs ,
respectively. Under the assumption that local data distributions are independent across workers, we
have V = Var (w1 ∑ww^1 Xk)=+ ∑ww“1 Var(Xk) = w⅛ ∑w=ι Vk. Assuming all the workers have
similar variances, we further have V = (Wyς ∑W=ι Vk « (Wy2wVk = Vwk for any k. Therefore, the
total number of workers can be estimated as K « w = VV, where K is the estimated subsampling
rate.
Since Vk can be easily estimated for any malicious worker k, the problem then boils down to
estimating V . Since the attackers have no access to normal workers’ data, they are unable to
compute the exact average gradients of all the workers when the subsampling rate is less than
1. We note that the average gradients of all the workers can be approximated by the attackers’
average gradients if all the workers’ data follows a similar local distribution. In this case, we
15
Under review as a conference paper at ICLR 2022
have X = 1 Xw=1 gk(θt∙⑺,zk « m1τ XmTI gk(θtpτq,z®). To estimate μ(X), the expectation
of Xs, we use samples of Xs from adjacent time steps by utilizing the fact that the values of θt
are close for adjacent t. That is, μ(Xs) “ Etzk〜Pkur 1 χw=ι gk(θtpτq.z^k)S « gτ, where gτ “
(θtpτq — θtpτ'1q){(η(t(τ ` 1) ´ t(τ))) is the average of Xs between two consecutive attack steps.
We then get an estimation of V at t(τ) as VT « Etzk_Pku[(系 ∑mT1 gk(θ1tτq,zk) — gτ)2S and
define KKT “ KVVr where VT is an estimation of Vk at t(τ). We take the average of KT over h steps
as an estimation of K where we drop the values of Kτ that are less than 2M , which is a reasonable
assumption. We observed that a batch of 20 data samples from each selected attacker is sufficient to
estimate V and Vk in our setting.
Learning aggregation rule Aggr. To learn the aggregation rule Aggr, the malicious workers should
learn an aggregation rule that takes inputs from all the selected workers. However, the malicous
workers only have an estimation of the mixture distribution of normal workers’ data, and do not
obtain information of each individual worker’s local data. In addition, it is usually computationally
prohibitive to reconstruct the exact aggregation rule due to the high dimensionality and limited data
samples available. We propose to use the estimated average gradients based on the observation that
the weights for normal workers’ inputs do not significantly influence the attack performance when
the data is i.i.d.. It is convenient to assume the FL system consists of one normal worker and the
malicious workers.
In particular, we propose to use a small neural network with parameters φpoly to approximate a non-
linear polynomial aggregation rule Aggrpoly . This is done by replacing the known aggregation rule
Aggr with the neural network φpoiy that estimates the parameters of Aggrpoly(gk(θT), g(θτ))kp[mτs,
where gk (θτ) “ Ezk„Pk rgk (θτ, zk)s denotes the average gradients of the selected malicious worker
k P [mτ], and g refers to the estimated average gradients of all the selected workers. The attackers
estimate the average gradients of all the selected workers g(θτ) “ :1 Xw“1 Ezk 〜P [gi(θT, Zk)S, where
P is the approximated distribution of aggregated data in distribution learning. At time step t(τ ),
the aggregated gradients UT “ (θτ´1 — θtpτq){(η(tτ — tτ´1)). Given the pooled model parameters
{θτ1}τι≤τ, we have the pooled aggregated gradients {gτ1}τι≤τ. The parameters φpoly can be then
estimated by minimizing the loss ||gT — φpoly (gk(θτ ),g(θτ ))kp[mτ]ll2.
B.3.2 Black-box MARL attacks with Model Adaptation
After approximating the aggregation rule, the leader attacker first learns an initial attack policy using
the CMADDPG algorithm with a small number of FL training epochs (i.e., n « T). We denote
the learned initial attack policy by πpoly .
In black-box attacks, it might be insufficient to only use the initial policy because the inaccurate esti-
mates of the server’s parameters, especially inaccurate aggregation rule Aggr can lead to inaccurate
simulations of the server’s behavior. As a result, the learned Markov game model is also inaccurate,
yielding a suboptimal attack policy. To improve the quality of the trained attack policy, the leader
attacker must continuously adapt the Markov game model as more data become available.
Following the original AMPO algorithm (Shen et al., 2020), the black-box attack algorithm
adopts an alternative optimization between the model training and model adaptation (see Algo-
rithm 4). The dynamics model Tφ is defined by Tφ(sτ'1∣sτ, a ) “ N(μφ(sτ, a ), Xφ(sτ, a,τ)),
where μφ and X@ are the mean and covariance matrix of the Gaussian distribution. We col-
lect the trajectories (sτ1,aτ1, sτ1+1,rτ1) since the beginning of policy learning τpl. We initial-
ize the dynamics model Tφ with bootstrapped samples selected from the environment buffer
De “ t(sτ 1, aτ 1, sτ 1'1, rτ 1)}TplWTyτ that stores the real data collected from step τpl to current
step τ . Unlike random policy as the initial policy in the original AMPO, our proposed method
adopts the pre-trained policy πpoly as the initial policy for better performance. Note that we need
to approximate the transition function because AMPO explicitly adapts the transition function to
improve the accuracy of the trained Markov game model.
Markov game model training. The objective of model training is to minimize the negative
log-likelihood loss: LT(φ) “ XN“/〃。3,aτ) — sτ'1 SJ X´1 (ST,aτ)[μφ(sτ,aτ) — sτ+1] +
log det Xφ (sτ, aτ), where det refers to the determinant. This model is used to generate q—length
simulated rollouts branched from the states sampled from the environment buffer De . Each time
16
Under review as a conference paper at ICLR 2022
the next state is predicted using the current policy, the simulated data is then added to the model
buffer Dm . Then we train the policy using the CMADDPG algorithm on both real and simulated data
from two buffers with a certain ratio rs. The CMADDPG algorithm trains a deterministic policy by
minimizing the loss Lpφq defined by Eq. 1.
Markov game model adaptation. We use the first several layers as the feature extractor fg to
represent the aggregation rule with corresponding parameters φg , and the remaining layers as the
decoder fd With parameters φd. Thus, We have T “ fd o fg and φ “ {φg, φd}. Since We explicitly
designate the feature extractor to approximate the server’s aggregation rule, the feature extractor
consists of six parameters as the neural netWork φpoly does. We minimize the 1-Wasserstein distance
betWeen tWo feature distributions Phe and Ppe, Where he “ fgepse, aeq and hg “ fgmpsm, amq. The
1-Wasserstein distance can be estimated by using a critic netWork fc With parameter ω to maximize
the loss LWDPΦgM，ω) = N XNeI 九(鹰)—Nm ∑N⅛ fc(%). The algorithm optimizes the
feature extractor to minimize the estimated 1-Wasserstein distance in order to learn the aggregation
rule that is invariant to the real data and simulated data after approximating the 1-Wasserstein
distance. That is, the model adaptation procedure is achieved by solving the folloWing minimax
objective: minφe,φm maxω LWD(φg, φm, ω) — α ∙ Lgp(ω), where α is the balancing coefficient and
Lgp(ω) = EP 月 r(}Vfc(h)}2 — 1)2S is the gradient penalty loss for the critic to enforce I-LiPSChitZ and
Ph is the distribution of uniformly distributed linear interpolations of Phe and Phm. The algorithm
alternates betWeen training the critic to estimate the 1-Wasserstein distance and training the feature
extractor of the dynamics model to learn transferable features.
In black-box attacks, individual malicious Workers send observations (i.e., the rank information)
to the leader attacker. The leader attacker trains the attack policy πφ and then sends the actions to
individual malicious Workers to execute. Unlike separated policy learning and attack execution in
White-box attacks, policy learning and attack execution are integrated in black-box attacks.
Algorithm 4 Black-box MARL Attacks
Estimate learning rate η
T	.1	∙	f . ∙Λ	Γ∙ . t	,1∙1,i∖ ♦	k 1	∙ . t	C
Learn the mixture distribution of the aggregated data P using Algorithm 2
Estimate subsampling rate κ, the total number of Workers K, the aggregation rule Aggrpoly
Train an attack policy πpoly With Aggrpoly for ns FL training epochs using Algorithm 3
Initialize policy ∏φ D np。%, dynamics model Tφ, environment buffer De, model buffer Dm
for step = 1 to maxstep do
Each attacker takes an action in the environment using the policy πφ; add the sample (s, a, s1
to De, Where a = (a1, ..., aM)
if every E real timesteps are finished then
Perform G1 gradient steps to train the model Tφ with samples from De
for F model rollouts do
r)
Sample a state s uniformly from De
Use policy πφ to perform a q—step model rollout starting from s; add to Dm
end for
Perform G2 gradient steps to train the feature extractor (to approximate the aggregation rule)
With samples (s, a) from both De and Dm by the model adaption loss LWD
end if
Perform G3 gradient steps to train the policy πφ With samples (s, a, s1, r) from De Y Dm
end for
C Appendix to Section 5: Experiments
C.1 Experimental Setups
Datasets. We consider three real World datasets: Fashion-MNIST (Xiao et al., 2017) and CIFAR-
10 (Krizhevsky et al., 2009) and ImageNet (Deng et al., 2009). Fashion-MNIST includes 60,000
training examples and 10,000 testing examples, Where each example is a 28×28 grayscale image,
associated With a label from 10 classes. CIFAR-10 consists of 60,000 color images in 10 classes of
Which there are 50, 000 training examples and 10,000 testing examples. ImageNet has 1,000 object
classes and contains 1,281,167 training images, 50,000 validation images and 100,000 test images.
For ImageNet, We select a random subset of 60, 000 from the training images as the training dataset,
17
Under review as a conference paper at ICLR 2022
and 10, 000 from the testing images as the testing dataset. We randomly split each of the datasets into
1, 000 groups, each of which consists of the same number of training samples and the same number
of testing samples. For the i.i.d. setting, we randomly split the dataset into 1, 000 groups, each of
which consists of the same number of training samples and the same number of testing samples.
For the non-i.i.d. setting, we follow the method of (Fang et al., 2020) to quantify the heterogeneity
of the data. We split the workers into 1, 000 groups and model the non-i.i.d. federated learning by
assigning a training instance with label c to the c-th group with probability pr and to all the groups
with probability 1 ´ pr. A higher pr indicates a higher level of heterogeneity. For for non-i.i.d. data,
we set the degree of non-i.i.d. pr “ 0.5 as the default setup.
Federated learning settings. We adopt the following parameters for the federated learning models:
learning rate η “ 0.001, number of total workers = 1,000, number of attackers = 50 (0 for NA and 1
for RL), subsampling rate = 20%, and number of total epochs = 1,000. For Fashion-MNIST, we train
a neural network classifier consisting of 8 X 8, 6 X 6, and 5 X 5 convolutional filter layers with ELU
activations followed by a fully connected layer and softmax output. This neural network is used in
the tutorial for CleverHans (Papernot et al., 2018) and in (Sinha et al., 2018). For CIFAR-10, we use
a 7-layer CNN with the following layers: input layer of size 32 X 32 X 3; convolutional layer with
ReLU of size 3 X 3 X 30; max pooling layer of size 2 X 2; convolutional layer with ReLU of size
3 X 3 X 50; max pooling layer of size 2 X 2; a fully connected layer with ReLU of size 200, and an
output layer of size 10. We use softmax on the output. For ImageNet, we use the ResNet-18 (He et al.,
2016). We measure the top-1 error rates for Fashion-MNIST and CIFAR-10, and the top-5 error rates
for ImageNet. We set the local batch size B “ 8. We implement the FL model with PyTorch (Paszke
et al., 2019) and run all the experiments on the same 2.30GHz Linux machine with 16GB NVIDIA
Tesla P100 GPU. We run all the experiments for 20 times and report the mean. Since the standard
deviations are below 0.04, we omit them for better visualization.
Distribution learning settings. In distribution learning, we set the step size for inverting gradients
η1 “ 0.001, the total variation parameter β “ 0.01, the threshold for distribution learning ν “ 0.1
and the number of iterations for inverting gradients max_iter = 6,000, and learn the mixture data
distribution by using the attackers’ data as dummy data.
Policy learning settings. In policy learning, we adopt a PyTorch implementation of the CMADDPG
based on the original MADDPG (Lowe et al., 2017). The default parameters are described as the
following: number of policy training epochs = 300, number of policy training episodes max_episode
= 6, 000 and α1 “ 0.01 for updating the targeting networks. We train the 6, 000 episodes in our
experiments. Note that the length of each simulating epoch is typically shorter than the length of each
real FL training epoch. In our experiments, we assume that the server waits for 15 seconds to receive
the updates from the workers before performing an model aggregation. In addition, the total number
of FL training epochs is fixed. We fix the number of simulating epochs in each episode in policy
learning. Since the number of epochs for distribution learning varies across datasets, the number of
policy learning epochs H also varies.
Black-box MARL settings. The hyperparameter settings for black-box MARL attackers are
described as follows. The coefficient for gradient penalty α “ 10. The number of total steps
max_Step = 10,000. The real steps between model training E “ 250. We set the model adaption
batch size as 256. The model rollout batch size is 10, 000. We set the steps for model training
G1 “ 100 The model adaption updates G2 “ 40, the rollout length q “ 1, the policy updates per
real step G3 “ 20. For training the initial policy πpoly , we use a small neural network that consists of
6 X 6, and 5 X 5 convolutional filter layers, followed by a fully-connected layer and a softmax layer.
We use multilayer perceptron (MLP) with four hidden layers (as the feature extractor) and one output
layer (as the decoder) of size 200 to approximate the aggregation rule.
C.2 More Experimental Results
Subsampling rate. As the subsampling rate increases, the performance of the RL-based attacks also
increases significantly (see Figure 1 (left)). For instance, the black-box MARL attack increases from
0.512 to 0.921 when the subsampling rate changes from 5% to 30% with FLTrust on Fashion-MNIST
and i.i.d. data. This is due to the fact that the attackers have a higher probability of being selected
by the server, leaving more time for the attackers to learn the attack policy and execute the attacks.
Noticeably, even if the subsampling rate is as low as 5%, both MARL attack methods (WM and BM)
substantially outperform all the other baselines with a subsampling rate as high as 30%.
18
Under review as a conference paper at ICLR 2022
Subsampling Rate
S」。」」山 UOq(αu≡ssJΞu
Figure 1: A comparison of classification errors on Fashion-MNIST with i.i.d. data with FLTrust defense rule
by varying the level of subsampling (left) and the number of attackers (right). Key parameters: number of
workers = 1, 000, number of attackers = 50 (left), subsampling rate = 20% (right). All other parameters are set
as default.
Figure 2: A comparison of Wasserstein distance between the true mixture distribution (P) and the
learned distribution (P) on Fashion-MNIST when the server applies non ´ i.i.d. data for FL with
FLTrust defense rule. Key parameters: number of workers = 1, 000, number of attackers = 50,
subsampling rate = 20%. All other parameters are set as default.
Number of attackers. As the number of attackers increases, the performance of all the attack
methods also increases (see Figure 1 (right)). However, the degree of increment differs. Specifically,
the three RL-based methods obtain significant jumps when the number of attackers increase from 20
to 70. For instance, the white-box MARL method increases from 0.658 to 0.941. Nevertheless, the
existing baselines only have a slim increment. Even with 20 attackers, the proposed MARL methods
(WM and BM) significantly outperform existing baselines with a number of attackers as much as 70.
Computation time. The training time mainly comes from learning an accurate estimation of
the mixture distribution of the aggregated data P , and learning an attack policy π. For a typical
subsampling rate (e.g., 20%) and distribution learning threshold (e.g., ν “ 0.1), the distribution
learning stage usually takes only a small portion (e.g., less than 25%) of the total FL epochs before
convergence. The distribution learning time also depends on the degree of data heterogeneity. A
higher degree of heterogeneity requires a longer time for distribution learning (see Figure 2). The time
for policy learning in stage two largely depends on the number of training epochs used to simulate the
FL dynamics in each training episode, and the number of training episodes used by the attackers. The
larger of the two factors, the more training time is required. In practice, the server usually needs to
wait for some time (typically a few minutes) in order to receive the gradients from the clients before
conducting model aggregation. In addition, if the leader agent has access to GPUs or other parallel
computing facilities, it can run multiple training episodes in parallel (Clemente et al., 2017). When
the policy is trained with a large number of episodes, there is no need to simulate the complete FL
process for T epochs. It suffices to consider much shorter epochs in practice.
Time allocation (in terms of FL epochs) for each stage. In our experiments, we set the total
number of FL epochs as 1, 000. Different RL-based algorithms may require different numbers of
epochs for distribution learning depending on the quality threshold ν (See Table 1). The numbers of
FL epochs left for policy learning and attack execution thus also vary. As the threshold ν decreases,
19
Under review as a conference paper at ICLR 2022
it takes more FL epochs for distribution learning for all the RL-based methods. However, the MARL
methods (WM and BM) require substantially fewer FL epochs than IRL does. As a result, it leaves
more FL epochs for policy learning and attack execution.
For black-box MARL (i.e., BM), both the policy learning and attack execution happen in the same
phase. For instance, the numbers of FL epochs in both policy learning and attack execution for BM
(ν “ 0.1) are 776. Among the 776 epochs, it takes 56 FL epochs for training the initial policy πpoly.
The remaining 720 FL epochs are used for training the black-box MARL attack policy. For all other
RL-based methods, the number of the policy learning is 300. Table 1 shows that both MARL methods
(WM and BM) significantly reduce the time for distribution learning compared to IRL due to better
coordination in distribution learning.
Table 1: A comparison of training time (in number of epochs) in each stage for RL-based attacks
against FL with FLTrust defense rule on Fashion-MNIST with i.i.d. data. Key parameters: number
of workers = 1,000, number of attackers = 50, subsampling level = 20%. All other parameters are set
as default.
Attack Method	Distribution learning	Policy learning	Execution	Classification error
WM (ν “ 0.1)	206	300	496	0.901
WM (ν “ 0.2)	172	300	528	0.841
WM (ν “ 0.3)	151	300	549	0.783
BM (ν “ 0.1)	224	776	776	0.873
BM (ν “ 0.2)	185	815	815	0.802
BM (ν “ 0.3)	170	830	830	0.754
IRL(ν “ 0.1)	458	300	242	0.632
IRL (ν “ 0.2)	424	300	276	0.531
IRL (ν “ 0.3)	376	300	324	0.376
Choices of MARL algorithms. We choose the MADDPG algorithm for two reasons: first, it is
easy to implement; second, as an off-policy algorithm, MADDPG is typically more sample efficient
than on-policy learning algorithms. Our results show that when we replace the MADDPG with
the multi-agent version PPO (Chao et al., 2021), the attack performance of our MARL method
changes only slightly. For instance, when the server is equipped with FLTrust and when we run the
experiments on CIFAR-10 with i.i.d. data, the average classification errors for the PPO-based MARL
method only improves to 0.841 from MADDPG’s 0.823, which is less than 2.2%.
Patterns of MARL attacks. There are two main characteristics of the proposed MARL attacks:
first, the attack magnitude (i.e., the noises added to the original gradients) and the angle (i.e., cosine
similarity) between the crafted updates and the normal model updates for differently ranked malicious
workers are usually different; second, the variance of the attack magnitude and the variance of the
angle are also different depending on the aggregation rule used by the server. One observation is that
more sophisticated defense mechanisms typically require larger variation in the magnitude and the
angle. For instance, the variances of attack magnitude and cosine similarity in FLTrust, Safeguard and
Centered Clipping are significantly larger than that in other defense rule settings such as averaging,
coordinate-wise median, and Krum. See Table 2 for a comparison.
Table 2: A comparison of variances in the attack magnitude and the cosine similarity between the
crafted updates and the normal model updates for white-box MARL (WM) attacks against FL with
different defense rules on CIFAR-10 with i.i.d. data. Key parameters: number of workers = 1,000,
number of attackers = 50, subsampling level = 20%. All other parameters are set as default.
Measurement / Defense Averaging Median Krum FLTrust Safeguard Centered Clipping
Variance of magnitude	0.081	0.132	0.144	0.247	0.321	0.317
Varianceofcosinesimilarity	0.042	0.093	0.127	0.356	0.393	0.364
20
Under review as a conference paper at ICLR 2022
D Proof of Theorem 1
D.1 Preliminaries
Our theoretic analysis relies on the following definitions and results. First, we formally define the
Wasserstein distance (Vaserstein, 1969), which will be used to measure the distance between the
estimated and true data distributions as well as the distance between the corresponding transition
dynamics introduced by different data distributions.
Definition 1. (Wasserstein distance) Let pM, dq be a metric space and PppMq be the set of all prob-
ability measures on M with pth moment, then the pth Wasserstein distance between two probability
distributions μι and μ2 in Pp(Mq is defined as:
Wppμ1, μ2q :“
Cj ʃʃ
1{p
d(s1, s2qpj(s1,s2qds1ds2
)
where J is the collection ofall joint distributions j on M X M with marginals μι and μ2.
In the following, We focus on I-Wasserstein distance and denote W(μ1,μ2) :“ W1(μ1, μ2). Wasser-
stein distance is also known as “Earth Mover’s distance” that measures the minimum expected
distance between two pairs of points where the joint distribution is constrained to match their corre-
sponding marginals . Compared with Kullback-Leibler (KL) divergence and Total Variation (TV)
distance, Wasserstein distance is more sensitive to how far the points are from each other (Asadi et al.,
2018).
We will also need the following special form of Lipschitz continuity from (Asadi et al., 2018).
Definition 2. (Lipschitz Continuity) Given two metric spaces (M1, d1q and (M2, d2q, a function
f : Mi → M2 is Lipschiz continuous ifthe Lipschiz constant, defined as
Kd1,d2(fq
sup
s1PM1,s2PM2
d2(f(s1),f(s2))
d1(s1,S2)
is finite. Similarly, a function f : Mi X A → M2 is uniformly Lipschitz ContinUoUS in A if:
KdA1,d2 (fq :“ sup sup
aPA s1,s2
d2(f(s1,a),f(s2,a))
dl(si,S2)
is finite.
Let M “ (S, A, T, rq be a generic Markov game model, where S and A denote the state space and
the action space respectively, T(s1|s, aq denotes the probability of reaching a state s1 from the current
state s and action a, and r(s, a, s1q denotes the reward given the current state s, action a, and the
next state s1. We then introduce the concept of Lipschiz model class from (Asadi et al., 2018), which
allows us to represent the stochastic transition dynamics of an Markov game as a distribution over a
set of deterministic transitions.
Definition 3. (Lipschitz model class) Given a metric state space (S, dSq and an action space A, we
define Fg as a collection of functions: Fg “ tf : S → Su distributed according to g(f|aq where
a P A. We say that Fg is a Lipschitz model class if
KF :“ sup KdS,dS (fq,
fPFg
is finite. We say that a transition function T is induced by a Lipschitz model class Fg if T(s1|s, aq “
Xf l(f (s) = s1)g(f |a) forany s,s1 P S and a P A.
We will show later that the transition dynamics of our Markov game model for attackers is induced
by a Lipschitz model class.
Finally we give a formal definition of finite-horizon value functions (Sutton & Barto, 2018).
Definition 4. Given an Markov game model M and a stationary policy π, the value func-
tion of π at time l is defined as VMl(S) :“ E∏,τ[XH7 1 r(st,at)∣sl = s∖, where r(s,a) “
Es，„T (∙∣s,a)[r(s, a, s1)∖. VM l(∙) satisfies thefollowing backward recursion form:
VM,l(s) “ Ea~π(s) rr(s, a) ' ∑ T(s1∣s,a)VM,l'i(s1)∖
s1PS
where VM,h´i(s) “ Ea~∏(s)[r(s,a)∖. The optimal value function is defined as VMl(S)：“
maxπ VMπ,l(s) for any s.
21
Under review as a conference paper at ICLR 2022
To analyze the impact of inaccurate transition on the value function, we also make use of the following
lemmas (Asadi et al., 2018).
Lemma 1. Given two distributions over states μι and μ2, a transition function T induced by a
Lipschitz model class Fg is uniformly Lipschitz continuous in action space A with a constant:
KA ……n sun W(T(八",T(MM) V K
KWW(Tq ：= SUP SUP	7777 ʌ	≤ KF
,	apAμ1,μ2	W (μι,μ2q
Lemma 2. Given a Lipschizfunction f : S → R with COnStant KdS ,dR (f):
KdAS,dR( f(s1)T(s1Is,a)ds1) ≤ KdS,dR(f)KdAS,W(T)
D.2 Measuring the Uncertainty: From Data Distributions to Total Returns
Let M “ (S, A, T, r, H) denote the true Markov game for attacking the federated learning system,
and M “ (S, A, T1 , r1, H) the estimated Markov game used in the policy learning stage, where
T1 and r1 are derived from the estimated joint data distribution tPk u where Pk “ Pk when k is an
attacker and Pk “ P otherwise. Our main goal is to compare the optimal attack performance that can
be obtained from the true Markov game model M and that derived from the simulated Markov game
model M. We will focus on understanding the impact of inaccurate data distributions (obtained from
distribution learning) and assume that other system parameters are known to the attackers.
Without loss of generality, we assume the M attackers’ indexes are from K — M ` 1 to K . Let
E “ KMM denote the fraction of benign nodes. We consider the idealized setting where the M
attackers are perfectly coordinated by a single leading attacker. Because of these simplifications, the
state st in each epoch t is completely defined by the current model parameters θt . In the following,
we abuse the notation a bit and assume S “ Θ.
Let Jm(∏) := E∏,τ,μo r∑H01 r(st, at, st'1)S denote the expected return over H attack steps under
the Markov game M, policy ∏ and initial state distribution μ0. Let ∏* be an optimal policy of M
that maximizes JM(∏). Define JMpnq similarly and let π be an optimal policy for M, with the
same initial state distribution μ0.
Our analysis is built upon the following lemma that compares the performance of n* and that of
πr with respect to the true Markov game M. It extends a similar result in (Yu et al., 2020) to a
finite-horizon Markov game where the reward in each step depends on not only the current state and
action but also the next state. Note that the lemma relies on the key assumption that both VMt l(∙)
and vɪ，(•) are Lv-Lipschitz continuous (with respect to the l2 norm of states) for all l. That is,
IVM l(sι) ´ VM l(s2)1 ≤ Lv}sι ´ s2}2 for any si, s? P S where Lv is a constant independent of l.
A similar requirement holds for V乏(∙). Let W(T, T1) :“ sup sup W(T(∙∣s, a),T1(∙∣s, a)).
M,l	aPA sPS
Lemma3. Assume Assumptions 2.1 holds and both VM l(∙) and V^，(•) are Lv -Lipschitz continuous
for all l. Then,
IJm(∏*) ´ Jm(∏)∣ ≤ 2H[(L ' Lv)W(T, T1) + 2Leδ}
Proof. Let Fl be the expected return when ∏* is applied to M for the first l steps, then changing to
M for l to H ´ 1. That is,
Fl “
H—1
r ∑ rt(st,at,st'1)S
t“0
,rt “r1
,rt “r
E
at~π*(st)
tνl∙.st'∖ „T 1Pst,at)
t"st'1~T Pst,at)
By the definition of Fl, we have JM(∏*) “ Fo and JM “ FH, which implies that Jm(∏*)—
J^(∏*) = ΣH01(Fl ´ Fl'i). Notethat
Fl = Rl-1 + Esl'1~T (sl,al)rr(sl, a , sl'1qs + Esl,al~T 1,π* rEsl'1~T (sl,al)[VM ,l'l(sl'1)]]
Fl'1 “ Rl-1 + Esl'1~T 1(sl,al)rr1(sl, al, sl'1 )S + Esl,al~T 1,π* rEsl'1„T 1(sl,al)rVM,l'l(sl'1)]]
22
Under review as a conference paper at ICLR 2022
where Rl—1 is the expected return of the first l ´ 1 steps, which are taken with respect to M. Thus,
Fl ´ Fl'1 “ Esl'1 „T Psl ,al q rr(sl, al, sl+1)s ´ Esl'1 „T ιpsl,al) rr1(sl, al, sl'1)s
+ Esl,al~T 1,π* rEsl'1 „T(sl,al)[VM,l`l (sl'1)S ´ Esl'1„T 1(sl,al)rVM,l'l(sl'1)]]
Define GM,l(sl,al) ：“ Esl'i~τpsl,al)rVM,l(sl'1)]- Esl'i~τιpsl,al)[VM,l(sl'1)].Wehave
H—1
JM(n*) — JM(∏*)= X (Fl ´ Fl'i)
l“0
H—1
“X (Esl'1~T Psl ,al)rr(sl, al, Sl'1)s ´ Esl'1~T 1psl,al)rr1(sl, al, sl+1)s)
l“0
H—2
+ X Esl,al~T 1,π* [GM,l (sl,al)s
l“0	,
H—1	1 K	1 K
“X (Esl'i~T Psl ,al)[ K X ('k(sl'1)—'k(sl))S— Esl'1 „T ιpsl,al)[ K X 'k (sl'1 )´'k (sl))])
l“0
H—2
+ ∑ Es∙
l“0
k“1
k“1
ι,ai~τι,∏* rGM,l (s',*]
H—1	1 K	1 K
“ X (Esl 十1〜T PsI ,alqr K X 'k psl'1qs ´ Esl'1~T 1Psl ,alqr K X Ck (sl'1)S)
l“0	K k“1	K k“1
H—1	K	K
+ X (K X 'k (Sl) ´ K X 'k(sl))
l“0	k“1	k“1
H—2
+ X Esl,al~T 1,π* [GM,l (sl,al)s
l“0	,
where 'k(s) :“ Ezk〜pj'(s, zk)], 'k(s) ：“ Ezk〜ρj'(s,zk)] and the last equality follows from
the definition of reward function r(s, a, s1) “ -ɪ XK=I 'k(s1)一 表 XK“i 'k(s), and r1(s, a, s1) “
KK ∑Nl 'k (s')´ KK XK=1 'k(s).
Since VM l is Lv-Lipschitz, we have |G入，(s, a)| ≤ LvW(T(s, a), T 1(s, a)) from the definition of
1-Wasserstein distance. We further have
1K	1K	1K
IK ∑ 'k(sl) ´ K ∑ 'k(Sl)I ≤ K ∑ l'k(Sl) ´ 'k(sl)l
k“1	k“1	k“1
1K
1
≤ K ∑LW(Pk ,Pk)
≤ Leδ,
where the second inequality follows from the definition of 1-Wasserstein distance and Assumption
3.1, and the last inequality follows from Assumption 1 and the fact that Pk “ Pk for any attacker k.
Similarly, We have
XK “k1 XK
I-K I-K
|Esi„T Ps,a)['k (S1)] ´ Esi„T I(s,a)['k(s1 川
IEs1„TPs,a),zk„Ppk ['k(S1, zk)] ´ Es1 „T 1 Ps,a),zk „Prk['1k(S1,zk)]I
WL(W (T,T1) + eδ),
23
Under review as a conference paper at ICLR 2022
where the last inequality follows Assumption 1, Assumption 3.1, and the property of 1-Wasserstein
distance with respect to product measures. Thus,
JM(∏*) ´ J^(∏*) W H(Lv ' L)W(T,T1) ' 2HLeδ.
A similar argument shows that
JM(∏) ´ Jm(∏ W H(Lv ' L)W(T,T1) ' 2HLeδ.
Let U “ H(Lv ` L)W(T, T1) ` 2HLδ. Thus,
加(∏*)W JM(∏*) + U
w JM(∏)' u
W JM (πr) ` 2U.
□
As indicated in (Yu et al., 2020), an important obstacle to applying Lemma 3 to real reinforcement
learning problems is to bound the Lipschitz constant Lv for optimal value functions. Further, we
need to bound W(T, T1), the 1-Wasserstein distance between two transition functions. We study
these two problems in the following two subsections, respectively.
D.3 Lipschitz Constant of Value Functions
In this section, we show that the Lipschitz constant Lv can be upper bounded for any optimal value
function in our setting. We first rewrite the update of model parameters in each epoch of FedAvg as
follows:
1 k´m	K
fz(s,{0i}iPrMs) = S — ηκr £ Vs'(s, zkq ` £ CkS	⑵
k“1	k“M `1
where z “ tzku denotes the set of data points sampled by each worker. That is, the above equation
gives the one-step deterministic transition when the data samples are fixed. An important observation
is that the transition function T is induced by a Lipschitz model class Fg “ tfz : z P Z K uwith
g(fz∣a) equal to the probability that z is sampled according to the joint distribution ∏kp[κs Pk.
Similarly, T1 is induced by Fg1 “ tfz : z P ZKu with g1(fz|a) equal to the probability that z is
sampled according to thejoint distribution ∏kp[Ms PkPK-M. This observation allows Us to apply
the techniques in (Asadi et al., 2018) to bound the Lipschitz constant Lv of an optimal value function
once we bound the Lipschitz continuity of individual fz.
We first show that for any joint action a “ {gi}ip[Ms,the deterministic transition fz(∙, a) is Lipschitz
continuous with a Lipschitz constant KdS,ds (fz(∙, a)) that can be upper bounded independent of z.
Lemma 4. Assume Assumptions 2.3, 2.4, and 2.5 hold. For any Lipschitz model class Fg “ tfz :
Z P ZK}, we have KF W max{c∣1 — ηα∣, e|1 — ηβ∣}.
Proof. It suffices to show that for any action a, KdS,ds (fz(∙, a)) W max{c∣1 — ηα∣, e|1 — ηβ∣}. By
(2), we have for any S1, S2 P S,
1 k´m	ι K-M
}fz (s1,a) ´ fz(s2,a)}2 = }s1 ´ ηκ £ vs'(s1,zk) ´ (s2 ´ ηκ £ vs'(s2,zk ))}2
k“1	k“1
paq
W
1 K-M
K X }si — ηVs'(si,Zk) — (S2 — ηVs'(s2, Zk))}2
k“1
Pb) 1 £ ∣∣∕T-	B2'(3zk)“	M
“K Z }(I — η Bs2 )(s1 — s2)}
k“1
Pc)IKT	B2'(S, Zk).....
W K Z }I — η	Bs2	}2}s1 — S2}2
k“1
24
Under review as a conference paper at ICLR 2022
where (a) follows from the triangle inequality, (b) follows from the fact that `ps, zq is twice contin-
UoUsly differentiable with respect to S and the mean value theorem, where S is a point on the line
segment connecting s1 and s2, and I is the identity matrix with its dimension equal to the dimension
of the model parameters, and (c) is due to the Cauchy-Schwarz inequality.
By the strong convexity and smoothness of '(s, Z) with respect to s, the eigenvalues of B ?：" are
between α and β (Polyak, 1987). It follows that
}I — ηB 'ps2zkq }2 ≤ max{∣1 — ηα∣,∣1 — ηβ∣}, @k
Bs2
Therefore, for any s1, s2,
}fzps1, aq ´ fzps2,aq}2
------- -------∏------- & max{c∣1 — ηα∣,e∣1 一 ηβ∣}
}s1 ´ s2}2
By Definition 2, we then have
KdS,ds(fz(∙,a)) ：“ SUp
s1 ,s2
}fz (s1, a) ´ fz (s2, a)}2
}s1 — s2 }2
≤ max{c∣1 — ηα∣, e|1 一 ηβ∣}
□
Note that by using a small enough learning rate η, KF can be made less than 1 so that the one-step
deterministic transition becomes a contraction. We next show that the optimal value function VMM ? (∙)
has a bounded Lipschitz constant. Note that the bound is independent of M; hence it also applies to
V二(∙)
Mn j
Lemma 5. Assume Assumptions 2.1, 2.3, 2.4, and 2.5 hold. The optimal value function VM 1 (∙) is
Lipschitz continuous with a Lipschitz constant bounded by XH´l´1(KF)t(L ' LKF).
Proof. The proof is adapted from the proof of Theorem 3 in (Asadi et al., 2018). Let QπM,l(s, aq “
r(s, a) ' XsiPS T(s1∣s,a)VM,i'i(s1) denote the state-action value function, where r(s, a) “
Es1„T ps1|s,aqrr(s, a, s1)s. We have for the optimal state-action value function
QM,l(s,a) = r(s,a) + ∑ T(s1s,a) max QM,i'i(s1,a1q
s1PS	a P
with QM H´i(s, a) = r(s, a). The Lipschitz constant of QM( is bounded by:
KdS,dR(QM,1) ≤ KdS,dR(r) + KdS,dR( ∑ T(s1∣s,a) max QM,i'i(s1, a1))
s1PS	a PA
paq
W KdS,dR(r) + kw,w(T)KdS,dR(maxQM,l'l)
W KdS,dR (r) + KW,W (TIKdS,dR(QM,l'1)
W KdS,dR(r) + KW,W(T)rKdS,dR(r) + KW,W(TIKdS,dR(QM,l'2)S
H-l-2
W KdS,dR(r)+ X (KW,w(TWKdS,dR(r) + KWW(T)H-l-1KdS,dR(QMM,H-i)
t“1
H-l-1
= X (KWd,W (T))tKddS,dR (r)
t“0
where (a) follows Lemma 2 and (b) is due to the fact that the max operator is 1-Lipschitz, that is,
K}}8,dR(max(x)) = 1 (Asadi & Littman, 2017). From the definition of r(s, a), we further have
1K	1K
∣r(s1, a) — r(s2, a)∣ W K X l'k(sl)´ 'k(s2)∣ + K X ∣Es1~T (si,a)['k(sl)]´ Es$„T {s2 ,a)['k(sz)l
k“1	k“1
W (L + LKWd,W(T))}s1 — s2}2
25
Under review as a conference paper at ICLR 2022
where 'k(s) :“ Ezk 〜pj'(s, zk)]∙ The first term of the second inequality comes from the LiPschitz
continuity of the loss function ', which gives ∣'k(sι) — 'k(s2)∣ ≤ L}sι — s2}2 for any k, and the
second term follows from Lemma 2 by letting f (S) “ 'k (s), which gives KAS 皈(Es，„t ['k (s1)S) ≤
LKWA,W(Tq for all k.
Since the above inequality holds for any a P A, r(s, a) is uniformly LiPschitz continu-
ous in action sPace A with a LiPschitz constant KdA,d (r) “ L ` LKWA,W (T). Thus,
KAS,dR(QM,lq & ΣHbl(KW,W(Tqqt(L + LKW,W(T)q. SinCe the OPtimal value function
VM ι(s) “ maXapAQM ι(s,a) and the max operator is 1-Lipschitz (Asadi & Littman, 2017),
we have KdS,dR(VM,ιq W KAS,dR(QM,iq ≤ ∑2c∕ 1(kW,w(TW(L + lkww,w(Tqq.
By Lemma 1, we have KW,w (T) ≤ KF. The desired result then follows by applying Lemma 4.
□
The lemma immediately implies that VM( (∙) is Lv-LiPSChitz for any l where Lv ≤ ∑H01(KF)t(L +
LKF ).
D.4 Wasserstein Distance between Transitions
In this section, we bound the 1-Wasserstein distance of transition functions. Recall that the true
transition dynamics T(∙∣s, a) depends on thejoint distribution ∏K∑ιM Ppk, while T 1(∙∣s, a) depends
on prκ-M. We have the following lemma.
Lemma 6. Assume Assumptions 1-3 hold. For any state-action pair (s, a), the 1-Wasserstein distance
between transition dynamics T(∙∣s, a) and T 1(∙∣s, a) generatedfrom the real FL environment and the
estimated environment, respectively, is bounded by ηLz δ, that is,
W(T(∙∣s,a),T1 (∙∣s,a))≤ ηLz芯
Proof. Let zi “ {zik}k=ι,…,k´m and z “ {z2k}k=1,…,k´m denote two data sets of normal
workers sampled from ∏K7lM Pk and jrκ-M respectively. Let j “ ∏K~M jk denote an arbitrary
coupling between the two joint distributions that is independent across workers, and J the set of all
such couplings. Let Js denote the collection of couplings between T(∙∣s, a) and T 1(∙∣s, a) generated
from the couplings of joint distributions in J. To simplify the notation, let s(z) :“ fz (s, a) denote
the successive state given the current state-action pair (s, a) and the sampled data z of normal workers.
26
Under review as a conference paper at ICLR 2022
From the definition of 1-Wasserstein distance, we have
paq
Wpτp∙∣s,aq,T 1p∙∣s,aqq ≤ Ef ∑ }s1 ´ s2}2jsps1,s2q
jsPJsps11,s12q
pbq
≤ inf Σ }spziq ´ Spz2q}2jpz1,z2q
jPJ pz1,z2q
1 k´m
“ inf ∑ }s ´-^p ∑ Vs'ps,zikq ' aq
jPJ pz1,z2q	K k“1
1 k´m	k´m
´ rs ´ K P X vs'ps, z2kq ' aqs}2 口 jkpz1k, z2k q
K	k“1	k“1
1 k´m	1 k´m	k´m
“叫 Σ }不 Σ Vs'ps,Zlk)´ - X Vs'(s,Z2k q}2 ∏ jk(zik ,Z2k q
jPJ pz1,z2q K k“1	K k“1	k“1
2 监 inf
K jPJ
⅛q 华 inf
K jPJ
k´m	k´m
}z1k ´ z2k}2	jkpz1k, z2kq
pz1 ,z2 q k“1	k“1
k´m
Σ Σ }z1k ´ z2k}2jkpz1k,z2kq
pz1,z2q
k“1
k´m
ηLz
《 ɪ
inf	}z1k ´ z2k}2jkpz1k,z2kq
k“1 k pz1k ,z2k q
τ k´m	/ ʌ τ
“ ⅛z Σ wpPk,pq 2 ηLzpκ ´ Mqs
K k“1	K
where (a) is due to the fact that we consider a restrictive collection of couplings, (b) is due to the fact
that Js is generated from J , (c) follows from the smoothness of `ps, zq with respect to z, (d) is due
to jkpzik, z2kq ≤ 1, @k, and (e) follows from Assumption 3.	□
D.5 Difference between Expected Returns
Combining the results from the previous three sections, we have the following main result.
Theorem 1. Assume Assumptions 1-3 hold. Let JM pπq :“ Eπ,τ,μo r∑H01 rpst, at, st'1qs denote
the expected return over H attack steps under Markov game M, policy π and initial state distribution
女
μo. Let ∏* and ∏ be optimal policies for M and M respectively, WIth the same initial state
distribution μo. Then,
| Jmp∏*) ´ Jmp∏)∣ W 2HeδrpL ' LvqηLz ' 2L]
where Lv ≤ ∑H^01pKFqtpL ' LKFq and KF ≤ E max{∣1 — ηα∣, |1 — ηβ∣}.
Proof. ByLemma 3, | JMP∏*) — JMp∏q∣ ≤ 2(HPL ' LvqWPTp∙∣s, aq,T 1p∙∣s, aqq ' 2HLeδq.
From Lemma 6, we have W PT p∙∣s, aq,T 1p∙∣s, aqq ≤ ηLz eδ. Thus, ∣JMp∏*q — JMp∏q∣ W 2H 限 L '
LvqηLzeδ ' 2Lcδ]. By Lemma 5 and the comment below it, Lv W ∑H01PKFqtPL ' LKFq where
KF W C max{∣1 — ηα∣, |1 — ηβ∣}.	□
27