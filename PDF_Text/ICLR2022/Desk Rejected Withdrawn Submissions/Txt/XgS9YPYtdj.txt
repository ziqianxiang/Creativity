Under review as a conference paper at ICLR 2022
Improved Generalization Risk Bounds for
Meta-Learning with PAC-Bayes-kl Analysis
Anonymous authors
Paper under double-blind review
Ab stract
By incorporating knowledge from observed tasks, meta-learning algorithms in-
spired by PAC-Bayes bounds aim to construct a hyperposterior from which an
informative prior is sampled for fast adaptation to novel tasks. The goal of PAC-
Bayes meta-learning theory is thus to propose an upper bound on the general-
ization risk over a novel task of the learned hyperposterior. In this work, we
first generalize the tight PAC-Bayes-kl bound to the meta-learning setting. Based
on the extended PAC-Bayes-kl bound, we further provide three improved PAC-
Bayes generalization bounds for meta-learning, leading to better asymptotic be-
haviour than existing results. By minimizing objective functions derived from
the improved bounds, we develop three PAC-Bayes meta-learning algorithms for
classification. Moreover, we employ localized distribution-dependent PAC-Bayes
priors for meta-learning to yield insights into the role of hyperposterior for learn-
ing a novel task. In particular, we identify that when the number of training task is
large, utilizing a prior generated from an informative hyperposterior can achieve
the same order of PAC-Bayes-kl bound as that obtained through setting a local-
ized distribution-dependent prior for a novel task. Experiments with deep neural
networks show that minimizing our bounds can achieve competitive performance
on novel tasks w.r.t. previous PAC-Bayes meta-learning methods as well as PAC-
Bayes single-task learning methods with localized distribution-dependent priors.
1	Introduction
A major limitation of deep learning techniques is the need of large amount of training data for new
tasks. To address this challenge, meta-learning (Thrun & Pratt, 1998; Hospedales et al., 2021), also
called learning-to-learn, has come into focus in recent years. In meta-learning, an agent extracts
information from observed tasks, and aims to facilitate adapting to novel tasks with no data being
observed so far. Over the last decade, meta-learning based algorithms have shown promising ability
to perform well on a wide range of machine learning problems, e.g., few-shot learning (Munkhdalai
& Yu, 2017; Snell et al., 2017) and reinforcement learning (Finn et al., 2017; Liu et al., 2019).
However, theoretical properties about meta-learning still remain incompletely understood.
The pioneering theoretical framework for meta-learning was formulated by Baxter (2000), who first
assumed that all tasks in meta-learning are independently and identically distributed (i.i.d.) from an
unknown distribution, called environment, to ensure the relatedness of different tasks. Under this
assumption, many following works proposed different meta-learning bounds with different com-
plexity indicators, such as covering number (Baxter, 2000), Gaussian complexity (Maurer, 2009;
Maurer et al., 2016; Tripuraneni et al., 2020) and generalized VC-dimension (Ben-David & Schuller,
2003). Other works include utilizing algorithmic stability (Bousquet & Elisseeff, 2002) to study the
convergence rate of meta-learning algorithms (Maurer, 2005; Chen et al., 2020). Particularly, PAC-
Bayes learning bounds (McAllester, 1999) are regarded as one of the tightest generalization bounds
(Langford, 2005; Zhang et al., 2021). This thus causes the recent surge of interest in studying meta-
learning with PAC-Bayes analysis (Pentina & Lampert, 2014; Amit & Meir, 2018).
By incorporating knowledge from n training tasks, PAC-Bayes meta-learning algorithm outputs a
distribution, termed hyperposterior, over the set of all priors (Pentina & Lampert, 2014). When en-
countering a novel task from the same environment, one informative prior can be generated from the
learned hyperposterior to achieve a low generalization risk on the novel task. The PAC-Bayes bound
1
Under review as a conference paper at ICLR 2022
on the generalization risk of the learned hyperposterior is composed of three parts: the empirical risk
on training tasks, the environment-level complexity and the task-level complexity. However, most
existing PAC-Bayes meta-learning bounds suffer from a slow convergence rate. For example, the
task-level complexities in these bounds are slower than O( √1m) (with m samples per training task),
and always contain an extra term of O(√m) or O( √n) (see Table 1). Moreover, existing works lack
theoretical explanations for why an informative hyperposterior can facilitate novel task adaptation.
In this work, we focus on tackling the above two issues in PAC-Bayes meta-learning theory. Specif-
ically, the first issue to be addressed is the lack of ‘identical distribution’ between samples indepen-
dently drawn from different tasks, since different tasks in meta-learning are associated with different
distributions. We thus can not directly apply traditional PAC-Bayes-kl bound (that holds for i.i.d.
data) to meta-learning. To overcome this problem, we propose Lemma 2 to convert the independent
(but non-identically distributed) setting into i.i.d. setting. Hence, we can effectively study meta-
learning with traditional PAC-Bayes-kl analysis and derive a generalized PAC-Bayes-kl bound for
independent data in Theorem 1. Based on the extended PAC-Bayes-kl bound, we further use its three
relaxations, i.e. PAC-Bayes Classic/quadratic/λ bounds (McAlleSter,1999; Perez-Ortiz et al., 2021;
Thiemann et al., 2017), to derive three improved bounds on the generalization risk of the learned
hyperposterior in Theorem 2 (which are still called PAC-Bayes classic/quadratic/λ meta-learning
bounds for brevity). Concretely, the environment-level complexity of O( Jln√n) in our derived
PAC-Bayes meta-learning bounds halves the logarithmic dependence on the number n of training
tasks in the numerator. The task-level complexity of our classic bound has a rate of O( JIn√nm),
without containing the extra term O(√m) or O(√n). In particular, the task-level complexities of
our quadratic bound and λ bound have a fast convergence rate of O( Innmm)) (see Table 1). More-
over, to address the second issue, we employ localized distribution-dependent priors (Catoni, 2007)
to meta-learning for explaining the power of hyperposterior. ‘Localized’ means that the prior is
dependent on the data distribution of a particular task and is informative enough for fast adaptation.
Hence there is a recent surge of interest in revealing the similarities between the localized prior and
the meta-learned prior (Pentina & Lampert, 2014). We demonstrate that, sampling an informative
prior from hyperposterior can reach the same tight PAC-Bayes-kl bound as setting a localized prior
on a novel task, validating the ability of meta-learning models to adapt to a new task.
Overall, our main contributions are four-fold: (1) By reducing the independent setting to the i.i.d.
setting with the proposed lemma, we utilize traditional PAC-Bayes-kl analysis in i.i.d. case to de-
rive a generalized PAC-Bayes-kl bound for independent data. (2) Based on the relaxations of the
extended PAC-Bayes-kl bound, we further obtain three improved PAC-Bayes meta-learning bounds
and three bound-minimization meta-learning algorithms. (3) For the first time, we demonstrate that
when the number of training tasks is large, sampling a prior from an informative hyperposterior
with a specific distribution can achieve the same tight PAC-Bayes-kl bound as setting a localized
distribution-dependent prior for a novel task. (4) Experiments with deep neural networks show that
minimizing our derived bounds yields competitive results w.r.t. previous PAC-Bayes meta-learning
methods and single-task learning methods that use a localized distribution-dependent prior.
2	Related Work
PAC-Bayes Theory. The goal of PAC-Bayes theory is to connect the generalization error of the
learned posterior with its empirical error and its KL-divergence with respect to some prior distri-
bution. Since its first appearance (McAllester, 1998; 1999), PAC-Bayes theory has been developed
by (Seeger, 2002; Maurer, 2004; Catoni, 2007) and extended to more general cases, where the data
is non-i.i.d. (Ralaivola et al., 2010; Seldin et al., 2012) or the KL-divergence is replaced by other
divergence measures (Alquier & Guedj, 2018). The subsequent works connected PAC-Bayes theory
to other research areas, such as statistical estimation problems (Zhang, 2006; Germain et al., 2016),
kernel methods like SVM (Langford & Shawe-Taylor, 2002) or Gaussian process (Germain et al.,
2009; Reeb et al., 2018), and theoretical properties of deep neural networks (Neyshabur et al., 2017;
2018). Another important branch of PAC-Bayes theory focuses on bounding the KL-divergence
with an elaborate choice of distribution/data-dependent prior, which was first proposed as localized
PAC-Bayes analysis in (Ambroladze et al., 2006; Catoni, 2007) and further developed by (Parrado-
Hernandez et al., 2012; Lever et al., 2013; Rivasplata et al., 2020). In this work, we utilize PAC-
Bayes-kl analysis to explore the generalization ability of meta-learning models. Moreover, we pro-
2
Under review as a conference paper at ICLR 2022
vide a case in which sampling the prior from the learned hyperposterior can achieve an equivalent
PAC-Bayes-kl upper bound as setting a localized distribution-dependent prior on the novel task.
PAC-Bayes Meta-Learning Theory. Under the i.i.d. task environment framework proposed by
Baxter (2000), Pentina & Lampert (2014) first introduced the notation of ‘hyperposterior’ and pro-
vided a PAC-Bayes bound for meta-learning. Following this work, Pentina & Lampert (2015) ex-
tended PAC-Bayes meta-learning theory to the general case of non-i.i.d. tasks. Amit & Meir (2018)
proposed a demonstration framework that can incorporate existing PAC-Bayes theories in single-
task learning to derive new PAC-Bayes bounds for meta-learning. Rothfuss et al. (2021) gave a
PAC-bayes meta-learning bound for unbounded loss with exponential moment assumption. Farid &
Majumdar (2021) utilized both algorithm stability analysis and PAC-Bayes techniques to yield task-
level and environment-level complexities respectively. However, the task-level complexities in most
these bounds have a slow convergence rate, such as O(n√m + —m) (We suppress the complexity
factors in the numerator for clarity, hereafter) in Pentina & Lampert (2014), O (n—m + —n) in Roth-
fuss et al. (2021), or O(ln (n—m)) in Amit & Meir (2018); LiU et al. (2021) (see Table 1). In contrast,
the task-level complexity in our derived classic bound has a convergence rate of O( Jlnnmm), and
even can achieve a fast rate of O( lnnnm)) in our quadratic” bounds (see Theorem 2). Detailed
comparisons between these bounds can be found in Table 1 and the discussions below Theorem 2.
3	Preliminary
3.1	Notations for PAC-Bayes Single-Task Learning
For supervised learning problems, Z = X ×Y is the product of input space X and output space Y, H
is a class of hypotheses, and the loss function l : H×Z → R is assumed to be bounded in the interval
[0, 1]. In single-task learning, an algorithm is given a size-m training sample S = {zi}im=1 , with each
zi drawn i.i.d. from an unknown distribution D over Z. Let M1 (A) define the set of probability
measures over the set A. In PAC-Bayes learning, the agent aims to output a posterior Q = Q(S, P)
over H by training an algorithm with sample S and prior P over H. For any hypothesis h ∈ H,
its expected error over D is denoted by er(h, D) ，Ez~dl(h, Z) and its empirical error over S
is er(h, S) = mm Pm=I l(h,zj. Then the expected error er(Q,D) with respect to (w.r.t.) the
distribution Q and its empirical counterpart ebr(Q, S) are defined as:
er(Q,D)，Ehy er(h,D),	er(Q,S)，Eh~Q命(h,S)	(1)
Denote the KL-divergence, or relative entropy, between two distributions Q and P by KL(Q||P) =
Eh~Q ln 骼,where dQQ is the Radon-Nikodym derivative of Q with respect to P. For Bernoulli
distributions with biases p and q we use kl(p||q) as a shorthand for KL([p, 1 - p]||[q, 1 - q]), the
KL-divergence between the two distributions. Then, the classical PAC-Bayes-kl bound in single-
task learning (see e.g. Maurer 2004, Thm 5) states that, for any δ ∈ (0, 1), any predefined prior P,
with probability at least 1 - δ over the draw of i.i.d. sample S = {zi}im=1 , the binary relative entropy
between the expected error of any posterior Q and its empirical error can be bounded by :
Id「S KL KL kll <k KL(QIIP) + ln 2√δm	6
kl(er(Q,S)l∣er(Q,D)) ≤ --------------—.	(2)
m
3.2 Notations for PAC-Bayes Meta-Learning
In meta-learning, the agent observes n training datasets {Si}in=1 , where the samples in each Si are
i.i.d. generated from distribution Di (i = 1, . . . , n) over the sample space Z (i.e., Di ∈ M1 (Z)).
Within the meta-learning framework of Baxter (2000), these n different data distributions {Di}in=1
are assumed to be i.i.d. sampled from the same environment τ (i.e., τ ∈ M1 (M1 (Z))). Therefore,
the whole training data {Si}in=1 in meta-learning are independently but non-identically distributed.
Most existing theoretical works take such independent meta-learning setting for analysis (Maurer,
2005; Pentina & Lampert, 2014; Maurer et al., 2016; Amit & Meir, 2018; Chen et al., 2020; Rothfuss
et al., 2021) and our work follows this line. We also assume the size of each observed dataset
Si as m to facilitate comparisons among different meta-learning bounds. Then, under the PAC-
Bayes meta-learning framework proposed by Pentina & Lampert (2014), the prior P is regarded as
a random variable sampled from a predefined distribution P over priors, called a hyperprior (i.e.
3
Under review as a conference paper at ICLR 2022
Table 1: Different PAC-Bayes bounds on er(Q). Meta-Learning Bound = Empirical Error +
Environment-Level Complexity + Task-Level Complexity. n is the number of observed tasks,
m is the number of samples in Si (i ∈ [n]). P and Q represent hyperprior and hyperposterior
respectively, both of which are probability measures over the set of all priors. P is the prior sampled
randomly from P, and Qi = Q(Si, P) is the posterior for the i-th training task obtained by training
PAC-Bayes single-task algorithm with the data Si and the prior P. C1,C2 > 1 are two constants.
Different Bounds	∣ Empirical Error ∣ Environment-Level Complexity ∣ Task-Level Complexity
(Pentina & Lampert, 2014)	I	ebr(Q)	I Ol	KL(QIP) ∖ √n	)	I OlKL(QIIP)+Pn=ι EP~QKL(QiIIP) + ɪ) I	∖	n√m	√ √m >
MLAP-M (Amit & Meir, 2018)	ebr(Q)	o(	q KL(Q∣p)+ln n)	I O( 1 Pn q KL(QIIP)+Ep ~q KL(Qi IIP )+ln (nm))
MLAP-S (Amit & Meir, 2018)	ebr(Q)	o(	q KL(Q∣∣P)+ln n)	I O 11 Pn	KL(QIIP)+Ep ~q KL(Qi IIP )+ln (n√⅞))
PACOH (Rothfuss et al., 2021) I	ebr(Q)	Q(	KL(QIP) ∖ √n	)	I O[KL(QIIP)+PNιEP~qKL(QiIIP) 1	lʒ n√m	√n
λ bound (Liu et al., 2021)	C1ebr(Q)	上	q KL(QIP)+ln n )	I O 11 Pn	KL(QIIP)+Ep ~Q KL(Qi IIP )+ln (n√m))
classic bound (ours)	ebr(Q)	Ol	q KL(QIP)+ln√n ∖	I CIq KL(QIIP)+Ep~q Pn=ι KL(QiIIP )+ln √nm ∖
			Nn)	I IV	nm	/
quadratic bound (ours)	ebr(Q)	O(	q KL(QIP)+ln √n )	I Ol KL(QIIP)+Ep~Q Pn=ι KL(QiIIP )+ln √nm ∖ I ∖	nm	>
λ bound (ours)	C2er(Q)	OI	q KL(QIP)+ln√n)	I O l KL(QIIP)+Ep~Q Pn=I KL(QiIIP)+ln √nm ∖ I o ∖	nm	)
P ∈ M1(M1(H))). During the meta-training phase, with the prior P sampled from hyperprior P
for each observed task, the agent further incorporates the information from the n training tasks and
computes a hyperposterior Q ∈ M1(M1(H)). During the meta-test phase, an informative prior
is sampled from the learned hyperposterior Q to adapt to the novel task. The goal of PAC-Bayes
meta-learning algorithms is thus to learn an informative hyperposterior Q with a hyperprior P and
the training datasets {Si}in=1 as input. Formally, the quality of Q can be measured by the transfer
risk (Maurer, 2009) on the data distribution D randomly sampled from the environment τ :
er(Q)，EP~qEd~,Es~0mer(Q(S,P),D).	⑶
Due to the difficulty of minimizing er(Q) directly, we consider the average empirical error over
the n training tasks, which is called empirical multi-task risk ebr(Q) and can be minimized during
meta-training phase, and the expected multi-task risk eer(Q) as the empirical proxies for er(Q):
nn
er(Q)，EP~Ql∕n X er(Q(Si,P),Si),	Sr(Q)，EP.Q/n X Cr(Q(Si,P),Di).	(4)
i=1	i=1
4	Theoretical Results
In Section 4.1, we extend the PAC-Bayes-kl bound from i.i.d. setting to independent meta-learning
setting. In Section 4.2 we use the extended PAC-Bayes-kl bound to obtain three improved PAC-
Bayes bounds for meta-learning. In Section 4.3, we demonstrate that on a novel task, utilizing the
prior sampled from an informative hyperposterior can achieve the same tight PAC-Bayes-kl bound
as that obtained by setting a localized prior. Section 4.4 details how to develop PAC-Bayes meta-
learning algorithms with our three improved bounds. All proofs are deferred to the Appendix.
4.1	Generalizing PAC-Bayes-kl Bound from i.i.d. Setting to Independent
Meta-Learning Setting
We first provide a PAC-Bayesian demonstration template that can cope with independent meta-
learning setting. Such demonstration strategy is originated from (Germain et al., 2009; Lever et al.,
2013) for i.i.d. case. We simply generalize such framework to the independent case and use it to
analyze how to extend the PAC-Bayes-kl bound for independent but non-identically distributed data.
Lemma 1 Let S = (ξ1, . . . , ξK) be a size-K random vector, with each component ξk(k ∈ [K])
drawn independently (not necessarily identically distributional) according to the measure νk over
the set Ak. Let F be a set of random variables f, π be a fixed measure over F that does not depend
on S. Let R(f) = Kk PK=I Eξ%gk(f,ξk), r(f)=煮 PK=I gk(f,ξk), where gk : F × Ak → [0,1]
is a bounded function. Let Φ : [0, 1] × [0, 1] → R be a convex function. Then for any δ ∈ (0, 1), any
t > 0, with probability at least 1 - δ over the draw of S, for any distributions ρ over F, we have
Φ(Ef ~ρR(f), Ef ~ρr(f)) ≤ t [KL(ρ∣∣π)+ln Ef ~π ES e：(R⑺'Mf))].
4
Under review as a conference paper at ICLR 2022
To derive PAC-Bayes bounds on er(Q) for meta-learning, some existing works (Pentina & Lampert,
2014; 2015; Rothfuss et al., 2021) can be considered to set the convex function Φ(R(f), r(f)) =
R(f) - r(f) in Lemma 1 and use this lemma to bound er(Q) - eer(Q) and eer(Q) - ebr(Q)
respectively. Concretely, using Hoeffding’s Lemma (Hoeffding, 1963) to bound EetΦ(R(f),r(f))
with et2/8K, the right-hand-side (RHS) in the inequality of Lemma 1 can be written in the form
of A/t + tB/K, where A,B > 0. Setting t = √K obtains a bound of slow rate O(1∕√K)
on Ef〜PR(f). Hence, we can see that, setting Φ(R(f),r(f)) = R(f) - r(f) just yields a
bound of slow convergence rate. To obtain an improved bound for meta-learning setting, we set
Φ(R(f), r(f)) = kl(r(f)||R(f)) and expect to derive generalized PAC-Bayes-kl bound that can
be converted to an explicit bound of fast rate O(ln K/K) on Ef 〜ρR(f). Recall that for single-task
learning, setting all measures v，s in Lemma 1 as the same, setting Φ(R(f), r(f)) as kl(r(f )||R(f))
and bounding ESeκ kl(r(f ),R(f)) with 2√K, we can recover the PAC-Bayes-kl bound for the i.i.d.
case in Eq. (2). However, the i.i.d. assumption is a necessary condition to bound ESeκ kl(r(f ),R(f))
with 2√K (see Maurer 2004, Eq.(1)). Hence, traditional PAC-Bayes-kl bound can not be directly
applied to meta-learning setting where there is no i.i.d. data assumption. To address this issue, we
use the following lemma to convert the independent setting into the i.i.d. setting, thus giving an
upper bound on EeKkl(r(f)||R(f)) with EeKkl(K1 PK=I YkUμ) ofi.i.d. random variables {Yk}3r
Lemma 2 Let X1, ..., XK be a sequence of independent random variables, such that Xk ∈ [0, 1]
almost surely and EXk = μk ,for k = 1,..., K. Let 匕,…,YK be i.i.d. Bernoulli random variables
with EY7k = KK PK=I μk = μ. Let X = PK=I Xk,Y = PK=I Yk. Then we have
EeK kl(K1 PK=I Xk| lμ) ≤ EeK kl(K1 PK=I YkUμ)
A similar result can be found in (Maurer, 2004, Thm 1) where {Xk}kK=1 are i.i.d. data. Our de-
rived Lemma 2 can be considered as the generalized version where {Xk}kK=1 are just indepen-
dent random variables. The proof of Lemma 2 can be found in Corollary 2 of Appendix B.1.
Setting Xk as gk(f,ξk) (k ∈ [K]), we can apply Lemma 2 to show that EeKkl(r(f)||R(f)) ≤
EeKkl(κ1 PK=I Ykllμ) ≤ 2√K, leading to the following generalized PAC-Bayes-kl bound.
Theorem 1 In the setting of Lemma 1, set Φ(R(f), r(f)) = kl(r(f)||R(f)). Then for any δ ∈
(0, 1), with probability at least 1 - δ over the draw ofS, we have for any ρ ∈ M1(F),
kl(Ef〜ρr(f )∣∣Ef〜ρR(f)) ≤ KL(PIIn) "<2√K0 .
K
There are two points to note about the above Theorem 1: (1) Setting all measures νks as the same,
we can obtain the traditional PAC-Bayes-kl bound for i.i.d. case in Eq. (2). (2) Actually, we can also
use the generalized chromatic PAC-Bayes bound in (Ralaivola et al., 2010, Thm 28) (just set the frac-
tional chromatic number in it as 1) to obtain an upper bound of slow convergence rate θ(ʌ/ln K/K)
on Ef 〜ρR(f) of independent data. In the next section, by Pinsker,s inequalities kl(p∣∣q) ≥ 2(p-q)2
and kl(pIIq) ≥ (p - q)2/(2q), we will use the relaxations of the extended PAC-Bayes-kl bound (i.e.
the three explicit bounds on Ef 〜ρR(f) in Corollary 3 of Appendix B.2, two of which have a fast
convergence rate O(ln K/K).) to derive three novel PAC-Bayes meta-learning bounds on er(Q).
4.2	Three Improved PAC-Bayes Bounds for Meta-Learning Based on Extended
PAC-Bayes-kl Bound
To give a bound on er(Q), we choose to bound er(Q) -eer(Q) and eer(Q) - ebr(Q). The derived two
bounds are called Environment-Level Complexity and Task-Level Complexity, respectively. In
this work, we first bound kl(eer(Q)IIer(Q)) and kl(ebr(Q)IIeer(Q)) with the extended PAC-Bayes-
kl bound in Theorem 1, and then give the explicit bounds on er(Q) - eer(Q) and eer(Q) - ebr(Q).
Concretely, the explicit bounds are derived as: (1) After obtaining a bound on kl(eer(Q)IIer(Q)), we
can only use Pinsker’s inequality kl(pIIq) ≥ 2(p - q)2 to bound Ieer(Q) - er(Q)I. (2) After deriving
the bound on kl(ebr(Q)IIeer(Q)), we can still use the inequality kl(pIIq) ≥ 2(p - q)2 to bound
Iebr(Q) - eer(Q)I. Besides, since ebr(Q) < eer(Q), we can use the stronger inequality kl(pIIq) ≥
(p-q) to give a sharper bound on er(Q). Combing the derived environment-level complexity and
task-level complexity with union bound yields three improved meta-learning bounds below.
5
Under review as a conference paper at ICLR 2022
Theorem 2 Denote Q(Si , P ) by Qi for brevity, ∀i ∈ [n]. For any predefined hyperprior P ∈
M1 (M1 (H)), any δ ∈ (0, 1), with probability at least 1 -δ over the draw of samples {S1, . . . , Sn},
simultaneously for all hyperposterior Q ∈ M1(M1(H)), we have
(i)	PAC-Bayes-classic meta-learning bound:
er(Q) ≤ ebr(Q) +
∕KL(Q∣∣P )+ln
V 2n
筌 1 ∕KL(Q∣∣P)+ EP〜Q Pn=I KL(Qi∣∣P)+ln -
2nm
(ii)	PAC-Bayes-quadratic meta-learning bound:
er(Q) ≤ 但亚「三 + (‰) + ∆+nJ√nZ + 0pE)
2n	2nm	2nm
where ∆ = KL(Q∣∣P) + EP〜Q P3 KL(QiIIP).
(iii)	PAC-Bayes-λ meta-learning bound, for any λ ∈ (0, 2):
er(Q) ≤
∕κL(Q∣∣P) + ln 至 + er(Q) + KL(Q∣∣P) + EP〜Q Pn=I KL(QiIIP) +ln 4√nm
V	2n	1 — λ∕2	nmλ(1 — λ∕2)
The detailed comparisons between our derived three meta-learning bounds and existing bounds can
be found in Table 1. We can see that: (1) Compared with the environment-level complexities of
O( JKL(Qllp)+lnn) in Amit & Meir (2018); Liu et al. (2021), the environment-level complexities
of O (JKL(QUP)+ln Vn) in our derived three bounds halve the logarithmic dependence on the num-
ber n of training tasks in the numerator. (2) The task-level complexities in existing bounds can not
fully utilize the whole nm training samples (e.g., O(n1 Pnn=1 HlnmnVm)) in Amit & Meir (2018)
and O( n√m + √n) in Rothfuss et al. (2021), where the definition of complexity ∆ can be found in
Theorem 2 (ii)). In contrast, the task-level complexities in our derived classic bound can fully utilize
the whole nm training samples (i.e., without the extra term O(√n) or O(√=) ) and has a conver-
gence rate of OQRnmVnm). (3)OUr derived quadratic bound and λ bound can achieve a fast
rate of O( Rnmnm) (as long as the empirical multi-task risk er(Q) is close to zero), much sharper
than that of O (AlnmnVm)) in the quadratic bound and λ bound of Liu et al. (2021). To validate the
practical effectiveness of the above three improved bounds, we will set them as training objectives
to develop three bound-minimization meta-learning algorithms for classification in Section 4.4.
4.3 PAC-Bayes-kl Bound on Novel Task with the Prior Sampled from the
Informative Hyperposterior
With PAC-Bayes-kl analysis, we explore the benefits of generating a prior from hyperposterior for a
novel task. Pentina & Lampert (2014) have claimed intuitively that sampling a prior from hyperpos-
terior is similar to setting a localized prior for the novel task. In this work, we rigorously validate this
intuition. For the novel task, consider the localized prior P and the posterior Q w.r.t. ν over H as
Gibbs Distributions1: dP(h) = ɪ exp{—γEz〜Dl(h, z)}, dQ(h)=* exp{ — Pm=I l(h,zj)},
where β = RH exp{-γEz〜Dl(h,z)}dν(h), β = RH exp{ — mm Pm=I l(h, zj)}dν(h) (γ > 0) are
normalization constants. Then for any δ ∈ (0, 1), with probability at least 1 — δ, Lever et al. (2013,
Theorem 6) gives the tight single-task PAC-Bayes-kl bound with localized prior as follow:
kl(er(Q，S)I MO D)) ≤ B (2Y2 + YEln 当m + ln W J
(5)
To derive a PAC-Bayes bound on a novel task (randomly sampled from the environment) with the
prior generated from hyperposterior Q, we need to choose Q as a special form that has a high mass
around the informative prior, since directly analyzing the generalization ability of arbitrary Q is hard
and even unfeasible. In this work, we set the learned hyperposterior as the Dirac measure that only
1 Gibbs distribution P has a form of 荒(h) = 1 exp{—Hp(h)}, where Hp is called energy function.
6
Under review as a conference paper at ICLR 2022
has mass at an informative prior P . Such informative prior P should contain as much information
of the n training tasks as possible for transferring knowledge to the novel task. Thus, analogous
to the form of the localized Gibbs prior above Eq. (5) whose energy function is the expected loss
on the novel task YEz〜Dl(h, z), We assume the meta-learned prior P as a Gibbs distribution whose
energy function is defined as the expected loss on n training tasks Y P2ι Ez〜D」(h, z). The formal
definition of the prior P sampled from Q is presented in Eq. (6) and the proof of the existence of the
intermediate measure ν in Eq. (6) can be found in Claim 1 in Appendix B.3.1. Then we can derive
a tight PAC-Bayes-kl bound on the novel task D that is randomly sampled from the environment τ .
Theorem 3 In meta-test phase, consider a novel task equipped with training sample S =
{zj }m=ι 〜Dm where the distribution D is sampled randomly from environment τ. Consider the
prior P sampled from the learned hyperposterior, and the posterior Q for this novel task as follow:
dP	1 γ n	dQ	1	γ m
dν(h)= P(h) = -exp{-nXEz〜Dil(h,z)}, dQ(h) = Q(h) =部 exp{-VX l(h,zj)}, (6)
i=1	j =1
where β, β0 are both normalization constants. Denote E by E for brevity. Then for any
D5,S 〜Dm	D,S
δ ∈ (0, ), with probability at least - δ, the following inequalities hold simultaneously,
∀Q, E kl(er(Q,S )∣∣er(Q,D)) ≤	KL(Q(S，P)I|P)”(2 E ,
D,S	m
Ed,sKL(Q(S,p)||P) ≤ ； + O(rn√n) + γ(n3√mδ) + O(rln√n + ln√m))1.
2m	n	2m	nm
Denote the RHS of the above second inequality on ED,S KL(Q(S, P)||P) by φ(n, m), and the
RHS of the inequality in Eq. (5) by ψ(m)= *(短 + Y Jm2 ln 2√m + ln 2√m). We expect to
compare φ(n, m) and ψ(m) to reveal the benefits of meta-learning over single-task learning. For
fair comparisons, in both settings, we set the same Gibbs posterior distribution Q over hypothesis
space for the novel task. The only difference is that, in meta-learning setting, the prior P sampled
from the hyperposterior has the form of Eq. (6) and only contains the information about the previous
n training tasks, and the upper bound φ(n, m) is calculated on the expectation of the KL-divergence
w.r.t. the task environment distribution; while in single-task learning setting, the prior P is set as
an informative localized prior, and the bound ψ(m) is calculated between such localized prior and
the Gibbs posterior. Therefore, for any fixed m, limn→∞ φ(n, m)
⅛ + Y 产涔,and
limn→∞ °(n,m)+m(2"/6 ≤ ψ(m). This means that, when the number n of training task becomes
large, the upper bound on ED,S KL(Q(S, P)||P) in meta-learning is tighter than the bound on
KL(Q||P) obtained by setting a localized prior in single-task learning. Such result does theoretically
reveal the benefits of sampling the prior from an informative hyperposterior in meta-learning.
4.4 PAC-Bayes Bound-Minimization Algorithms for Meta-Learning with
Stochastic Deep Neural Networks
By setting the three bounds in Theorem 2 as minimization objectives, we can develop three PAC-
Bayes bound-minimization algorithms for meta-learning with deep neural networks. Concretely,
we use stochastic neural networks (e.g. Blundell et al. 2015) and thus define the hypothesis
space H = {hw : w ∈ Rd} as the set of neural networks with certain parameters. We fur-
ther need to specify the form of hyperprior and hyperposterior. Following previous work (Amit
& Meir, 2018), we set both the hyperprior and hyperposterior as the isotropic Gaussian distribu-
tion: P = N(0, κ2PId×d), Qθ = N(θ, κ2QId×d), where κP, κQ > 0 are constants, d is the
dimension of the parameter w of prior P, and θ is the optimization parameter. Then the KL-
divergence between Qθ and P can be calculated as: KL(Qθ||P) = l2θ⅛2. Next, we consider the
form of prior and posterior over the hypothesis space H. Recall that H is the family of func-
tions parameterized by a weight vector {hw : w ∈ Rd}, the posterior/prior is thus the distribution
over Rd . We choose the prior Pθ and the posteriors Qφi (φi ∈ Rd is the hyperparameter) to be
factorized Gaussian: Pθ(W) = Qd=I N(wk； μp,k,σp,Q, Qφ<w) = Qk=I N(Wk; ui,k,σ2,k),
where θ = (μp,ρp) ∈ R2d is composed of the means μp,k and log-variances of each weight
7
Under review as a conference paper at ICLR 2022
Table 2: Comparison of different PAC-Bayes bounds on 20 test tasks (the ± shows the 95% confi-
dence interval) in 100/200-SWaP shuffled pixels environment and permuted labels environment.
Method	100-SwaP Shuffled pixels		200-SwaP Shuffled pixels		Permuted labels	
	Test bound	Test error (%)	Test bound	Test error (%)	Test bound	Test error (%)
(Pentina & Lampert, 2014)	0.189 ± 0.023	1.939 ± 0.001	0.240 ± 0.030	2.631 ± 0.002	6.026 ± 0.436	15.660 ± 0.063
MLAP-M (Amit & Meir, 2018)	0.137 ± 0.037	1.607 ± 0.000	0.197 ± 0.019	1.948 ± 0.001	1.799 ± 0.056	4.229 ± 0.003
MLAP-S (Amit & Meir, 2018)	0.133 ± 0.034	1.629 ± 0.001	0.285 ± 0.049	1.972 ± 0.001	4.947 ± 0.339	8.600 ± 0.016
MLAP-VB (Amit & Meir, 2018)	0.138 ± 0.024	1.606 ± 0.001	0.161 ± 0.002	1.962 ± 0.001	4.623 ± 0.308	9.754 ± 0.130
PACOH (Rothfuss et al., 2021)	0.181 ± 0.023	1.919 ± 0.001	0.221 ± 0.029	2.630 ± 0.002	5.434 ± 0.416	12.520 ± 0.061
λ-bound (Liu et al., 2021)	0.067 ± 0.015	1.630 ± 0.001	0.151 ± 0.015	2.097 ± 0.001	3.830 ± 0.181	11.340 ± 0.017
classic bound (ours)	0.138 ± 0.019	1.585 ± 0.000	0.193 ± 0.018	1.911 ± 0.001	1.790 ± 0.054	4.164 ± 0.003
quadratic bound (ours)	0.081 ± 0.019	1.644 ± 0.000	0.157 ± 0.024	1.929 ± 0.001	1.950 ± 0.051	4.753 ± 0.003
λ bound (ours)	0.043 ± 0.008	1.575 ± 0.001	0.093 ± 0.012	1.932 ± 0.001	1.698 ± 0.051	4.064 ± 0.003
ρp,k = lnσp,k, k ∈ [d]. The posterior vectors φ% = (μi,ρi) ∈ R2d has a similar structure. Thus,
the KL-divergence in task-level complexity of meta-learning bounds has a simple analytic form:
ePθ〜Qθ Pi=IKL(Qφi ||p9) = EPθ ~Qθ 2Pnkd=1{ln σpk + 嗫+(犷μP,j - 1}, where the ex-
i,k	P,k
pectation Pθ 〜 Qθ can be approximated through Monte-Carlo method by adding Gaussian noise
to the parameter θ, i.e., θ := θ + e, e 〜 N(0, κQld×d). Computing the PAC-Bayes bounds in
Theorem 2 with the above results and using the gradient descent method to optimize the parameter
{θ, φ1, ..., φn} can learn the hyperposterior Q. The detailed pseudo-code of our PAC-Bayes meta-
learning algorithms (including meta-training and meta-test parts) can be found in Appendix C.2.
5 Experiments
In this section, we run our proposed PAC-Bayes bound-minimization meta-learning algorithms on
image classification problems, and make detailed comparisons with existing methods. All experi-
mental details are set the same as that in (Amit & Meir, 2018; Liu et al., 2021) for fair comparisons.
5.1	Experimental Setup
Dataset and Task Environment. We conduct experiments with three different task environments,
constructed by augmentations of the MNIST dataset (Lecun et al., 1998). The first/second environ-
ment is termed permuted pixels, where each task is created by swapping a number of pixel locations
(i.e., 100 and 200 locations) for each image. The set of pixels tobe swapped is fixed and different for
each classification task. The third environment is called permuted labels, where each task is created
by a random permutation of image labels. During the meta-training phase, each task is constructed
with 60,000 training images and 10,000 test images. During the meta-test phase, each task is con-
structed with much fewer training examples (2,000) and 10,000 test images. For permuted pixels
and permuted labels experiments, the number n of training task is set as 10 and 5 respectively.
Neural Network Architecture. We choose a 4-layer fully-connected network for shuffled pixels
experiments, and a 4-layer convolutional network for permuted labels experiments. We use the
bounded cross-entropy loss (Perez-Ortiz et al., 2021) for model optimization. The optimizer is
Adam (Kingma & Ba, 2015) with a learning rate of 10-3 for running different algorithms. More
details about the network architecture and the experimental setting can be found in Appendix C.1.
5.2	Experimental Results
Different PAC-Bayes Methods. We compare average test error and test bound on 20 meta-test
tasks among the following methods: (Pentina & Lampert, 2014), MLAP-M (Amit & Meir, 2018,
Theorem 2), MLAP-S (Amit & Meir, 2018, Theorem 4), MLAP-VB (Amit & Meir, 2018, Eq.(23)),
PACOH (Rothfuss et al., 2021, Theorem 2), λ bound (Liu et al., 2021, Theorem 1). The detailed
formulations of the above PAC-Bayes meta-learning bounds can be found in Table 1. We reproduce
the experimental results of these methods by directly running the code released online2 from (Liu
et al., 2021), and run our algorithm by replacing others’ bounds with our bounds derived from
Theorem 2. Concretely, the test bound is calculated as a single-task PAC-Bayes bound on a novel
task with the 2, 000 training samples, with a prior sampled from the learned hyperposterior. The
test error is the classification error on a novel task. The number of training epochs during the meta-
training/meta-test phase is set as 150/200 respectively. More details about the computation of the
test bound and test error can be found in Appendix C.2. Experimental results are reported in Table 2.
2
https://github.com/tyliu22/Meta- learning- PAC- Bayes- bound- with- data- depedent- prior
8
Under review as a conference paper at ICLR 2022
123456789	10
Number of training-tasks
(a) Test Bound
〔*一 Xss MQU UO-0」」w
123456789	10
Number of training-tasks
(b) Test Error
5	10 15 20 25 30 35 40 45 50 55 60X103
Sample SiZe Per training-task
(C) Test Bound
〔*一 Xss MQU UO-0」」w
5	10 15 20 25 30 35 40 45 50 55 60X103
Sample SiZe Per training-task
(d) Test Error
Figure 1:	Comparisons of average test bounds and test errors between other three latest bounds and
our λ bound on new tasks (average over 20 meta-test tasks from 100-pixel-shuffled environment).
Xss MQU UO PUnOg
—I— random Prior —24,000-ERM prior —meta-leamed prior
12,000-ERM prior —I— 42,000-ERM prior
g:ssMQU UO-0」」w
0.40
—f- random pιior	—f— 24,000-ERM prior	-f— meta-learned p
12,000-ERM prior —4 2,000-ElSM prior
0 35
(*一 Xss MQU UO-0」击
+ baseline ERM	+ 12,000-ERM prior	+ 42,000-ERM prior
—|— random prior	—∣— 24,000-ERM prior	—∣— meta-leamed prior
0.00	1	0.00	1
123456789 10	123456789 10	5 10 15 20 25 30 35 40 45 50 55 60X103	5 10 15 20 25 30 35 40 45 50 55 60X103
Number of training-tasks	Number of training-tasks	Sample SiZe per training-task	Sample SiZe per training-task
(a) Test Bound	(b) Test Error	(C) Test Bound	(d) Test Error
Figure 2:	Comparisons of average test bounds and test errors between localized priors, and meta-
learned priors obtained by minimizing our λ bound aCross a wide range of number of training tasks
and sample size per task (average over 20 meta-test tasks from 100-pixel-shuffled environment).
Classification Performance. We Can draw the following ConClusions from the Comparisons in Ta-
ble 2: (i) In all settings, minimizing our PAC-Bayes-λ bound Can always aChieve the tightest test
bounds the lowest test error over the meta-test tasks for different environments, whiCh is Consistent
with our theoretiCal analysis that λ bound is the tightest bound derived in this work. (ii) Besides
PAC-Bayes-λ bound, our derived PAC-Bayes-ClassiC bound and quadratiC bound Can also obtain
Competitive performanCe in terms of test error and the quantity of test bound w.r.t. other methods.
5.3 More Discussions
Comparisons between Our λ Bound and Others. We provide detailed Comparisons of the Con-
vergenCe performanCe in Figure 1 between our tightest λ bound and other three ClassiCal bounds
(Pentina & Lampert, 2014; Amit & Meir, 2018; Liu et al., 2021), aCross a wide range of the number
n of the training tasks and the sample size m per task in 100-pixel-shuffled environment. When n
Changes, m is set as 60, 000; when m Changes, n is set as 10. We Can find that, minimizing our λ
bound always obtains the tightest test bounds and aChieves Competitive test errors on novel tasks.
Comparisons between Localized Priors and our Meta-Learned Priors. We provide detailed
Comparisons between our meta-learned priors and different loCalized priors in Figure 2. Our meta-
learned priors are sampled from the hyperposterior obtained through minimizing our λ bound in
Theorem 2. The setting ofn, m and the environment is the same as that in Figure 1. ‘Baseline ERM’
represents purely empiriCal risk minimization (ERM) over 2,000 training samples on eaCh novel task.
‘k-ERM prior’ represents the loCalized prior learned from ERM over the k(∈ (0, 58, 000)) training
samples (non-overlapped with the 2,000 training samples) from eaCh novel task. ‘random prior’ Can
be Considered as ‘0-ERM’ prior. The learned k-ERM prior is subsequently used for PAC-Bayes
single-task learning (with λ bound) to Compute test bound/error. More details of learning k-ERM
priors Can be found in Appendix C.4. We Can see that: with the inCrease of n or m, the test bounds
and test errors of meta-learned prior Can deCrease to the low values aChieved by setting the optimal
42,000-ERM prior, validating the benefits of sampling a prior from an informative hyperposterior.
6 Conclusion
In this work, we generalize the PAC-Bayes-kl bound from the i.i.d. setting to the independent
meta-learning setting. Based on the extended PAC-Bayes-kl bound, we provide three improved
PAC-Bayes meta-learning bounds. Minimizing the training objeCtives derived from these bounds
leads to three PAC-Bayes meta-learning algorithms, yielding Competitive experimental results on
novel tasks w.r.t. existing methods. Moreover, we show that the PAC-Bayes-kl bound obtained by
sampling a prior from an informative hyperposterior is equivalent to the one obtained by setting
loCalized prior for the novel task, from both theoretiCal and experimental standpoints. Our ongoing
researCh inCludes extending our theoretiCal results to unbounded loss and heavy-tailed data.
9
Under review as a conference paper at ICLR 2022
References
Pierre Alquier and Benjamin Guedj. Simpler PAC-Bayesian bounds for hostile data. Machine Learning, 107
(5):887-902, 2018.
Amiran Ambroladze, Emilio Parrado-Hernandez, and John ShaWe-Taylor. Tighter PAC-Bayes bounds. In
NeurIPS, pp. 9-16, 2006.
Ron Amit and Ron Meir. Meta-learning by adjusting priors based on extended PAC-Bayes theory. In ICML,
pp. 205-214, 2018.
Jonathan Baxter. A model of inductive bias learning. Journal of Artificial Intelligence Research, 12:149-198,
2000.
Shai Ben-David and Reba Schuller. Exploiting task relatedness for mulitple task learning. In COLT, pp. 567-
580, 2003.
Charles Blundell, Julien Cornebise, Koray Kavukcuoglu, and Daan Wierstra. Weight uncertainty in neural
netWork. In ICML, pp. 1613-1622, 2015.
Olivier Bousquet and Andre Elisseeff. Stability and generalization. JMLR, 2:499-526, 2002.
Olivier Catoni. PAC-Bayesian Supervised Classification: The Thermodynamics of Statistical Learning. Inst of
Mathematical Statistic, 2007.
Jiaxin Chen, Xiao-Ming Wu, Yanke Li, Qimai LI, Li-Ming Zhan, and Fu-Lai Chung. A closer look at the
training strategy for modern meta-learning. In NeurIPS, 2020.
Alec Farid and Anirudha Majumdar. PAC-BUS: meta-learning bounds via PAC-Bayes and uniform stability.
In NeurIPS, 2021.
Chelsea Finn, Pieter Abbeel, and Sergey Levine. Model-agnostic meta-learning for fast adaptation of deep
netWorks. In ICML, pp. 1126-1135, 2017.
Pascal Germain, Alexandre Lacasse, Francois Laviolette, and Mario Marchand. PAC-Bayesian learning of
linear classifiers. In ICML, pp. 353-360, 2009.
Pascal Germain, Francis R. Bach, Alexandre Lacoste, and Simon Lacoste-Julien. PAC-Bayesian theory meets
Bayesian inference. In NeurIPS, pp. 1876-1884, 2016.
Wassily Hoeffding. On the distribution of the number of successes in independent trials. The Annals of Math-
ematical Statistics, 27(3):713-721, 1956.
Wassily Hoeffding. Probability inequalities for sums of bounded random variables. Journal of the American
Statistical Association, 58(301):13-30, 1963.
Timothy M Hospedales, Antreas Antoniou, Paul Micaelli, and Amos J. Storkey. Meta-learning in neural net-
Works: A survey. IEEE TPAMI, 2021. doi: 10.1109/TPAMI.2021.3079209.
Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR, 2015.
John Langford. Tutorial on practical prediction theory for classification. JMLR, 6:273-306, 2005.
John Langford and John ShaWe-Taylor. PAC-Bayes & margins. In NeurIPS, pp. 423-430, 2002.
Yann Lecun, Leon Bottou, Y. Bengio, and Patrick Haffner. Gradient-Based learning applied to document
recognition. Proceedings of the IEEE, 86:2278 - 2324, 1998.
Guy Lever, Francois Laviolette, and John Shawe-Taylor. Tighter PAC-Bayes bounds through distribution-
dependent priors. Theoretical Computer Science, 473:4-28, 2013.
Hao Liu, Richard Socher, and Caiming Xiong. Taming MAML: efficient unbiased meta-reinforcement learning.
In ICML, pp. 4061-4071, 2019.
Tianyu Liu, Jie Lu, Zheng Yan, and Guangquan Zhang. PAC-Bayes bounds for meta-learning with data-
dependent prior. arXiv:2102.03748, 2021.
Andreas Maurer. A note on the PAC-Bayesian theorem. arXiv:cs/0411099, 2004.
Andreas Maurer. Algorithmic stability and meta-learning. JMLR, 6:967-994, 2005.
10
Under review as a conference paper at ICLR 2022
Andreas Maurer. Transfer bounds for linear feature learning. Machine Learning, 75(3):327-350, 2009.
Andreas Maurer, Massimiliano Pontil, and Bernardino Romera-Paredes. The benefit of multitask representation
learning. JMLR, 17:81:1-81:32, 2016.
David A. McAllester. Some PAC-Bayesian theorems. In COLT, pp. 230-234, 1998.
David A. McAllester. PAC-Bayesian model averaging. In COLT, pp. 164-170, 1999.
Tsendsuren Munkhdalai and Hong Yu. Meta networks. In ICML, pp. 2554-2563, 2017.
Behnam Neyshabur, Srinadh Bhojanapalli, David McAllester, and Nati Srebro. Exploring generalization in
deep learning. In NeurIPS, pp. 5947-5956, 2017.
Behnam Neyshabur, Srinadh Bhojanapalli, and Nathan Srebro. A PAC-Bayesian approach to spectrally-
normalized margin bounds for neural networks. In ICLR, 2018.
Emilio Parrado-Hernandez, Amiran Ambroladze, John Shawe-Taylor, and Shiliang Sun. PAC-Bayes bounds
with data dependent priors. JMLR, 13:3507-3531, 2012.
Anastasia Pentina and Christoph H. Lampert. A PAC-Bayesian bound for lifelong learning. In ICML, pp.
991-999, 2014.
Anastasia Pentina and Christoph H. Lampert. Lifelong learning with non-i.i.d. tasks. In NeurIPS, pp. 1540-
1548, 2015.
Maria Perez-Ortiz, Omar Rivasplata, John Shawe-Taylor, and Csaba Szepesvari. Tighter risk certificates for
neural networks. JMLR, 22(227):1-40, 2021.
Liva Ralaivola, Marie Szafranski, and Guillaume Stempfel. Chromatic PAC-Bayes bounds for non-iid data:
Applications to ranking and stationary β-mixing processes. JMLR, 11:1927-1956, 2010.
David Reeb, Andreas Doerr, Sebastian Gerwinn, and Barbara Rakitsch. Learning Gaussian processes by mini-
mizing PAC-Bayesian generalization bounds. In NeurIPS, pp. 3341-3351, 2018.
Omar Rivasplata, Ilja Kuzborskij, Csaba Szepesvari, and John Shawe-Taylor. PAC-Bayes analysis beyond the
usual bounds. In NeurIPS, pp. 16833-16845, 2020.
Jonas Rothfuss, Vincent Fortuin, Martin Josifoski, and Andreas Krause. PACOH: Bayes-optimal meta-learning
with PAC-guarantees. In ICML, pp. 9116-9126, 2021.
Matthias W. Seeger. PAC-Bayesian generalisation error bounds for Gaussian process classification. JMLR, 3:
233-269, 2002.
Yevgeny Seldin, Francois Laviolette, Nicolo Cesa-Bianchi, John Shawe-Taylor, and Peter Auer. PAC-Bayesian
inequalities for martingales. IEEE Trans. Inf. Theory, 58(12):7086-7093, 2012.
Jake Snell, Kevin Swersky, and Richard Zemel. Prototypical networks for few-shot learning. In NeurIPS, pp.
4077-4087, 2017.
Niklas Thiemann, Christian Igel, Olivier Wintenberger, and Yevgeny Seldin. A strongly quasiconvex PAC-
Bayesian bound. In ALT, pp. 466-492, 2017.
Sebastian Thrun and Lorien Pratt. Learning to Learn. Kluwer Academic Publishers, 1998.
Nilesh Tripuraneni, Michael I. Jordan, and Chi Jin. On the theory of transfer learning: The importance of task
diversity. In NeurIPS, pp. 7852-7862, 2020.
James Yeh. Real Analysis: Theory of Measure and Integration. World Scientific, 2014.
Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals. Understanding deep learning
(still) requires rethinking generalization. Commun. ACM, 64(3):107-115, 2021.
Tong Zhang. Information-theoretic upper and lower bounds for statistical estimation. IEEE Transactions on
Information Theory, 52(4):1307-1321, 2006.
11
Under review as a conference paper at ICLR 2022
APPENDIX
Appendix A Explicit Form of Different PAC-Bayes Bounds for
Meta-Learning
We provide more details about different PAC-Bayes meta-learning bounds. The explicit form of
these bounds can be found in Table 3, which is the detailed version of Table 1 in the main paper.
Table 3: Explicit forms of different PAC-Bayes bounds on er(Q). Meta-Learning Bound =
Empirical Error + Environment-Level Complexity + Task-Level Complexity. n is the num-
ber of observed tasks, m is the number of samples in Si (i ∈ [n]). P, Q ∈ M1(M1(H))
are hyperprior and hyperposterior respectively. P, Qi = Q(Si, P) ∈ M1 (H) are the prior
and the posterior for the i-th training task. δ ∈ (0, 1) is the confidence level. In MLAP-S
bound, ∆i = KL(Q∣∣P) + EP〜QKL(Q∕∣P). In our quadratic meta-learning bound, ∆ =
KL(Q∣∣P) + EP〜Q Pn=I KL(QiIIP).In λ bounds, λ ∈ (0, 2).__________________________
Different Bounds	Empirical Error	Environment-Level Complexity	Task-Level Complexity
(Pentina & Lampert, 2014)	er(Q)	√ (KL(QIIP) +1 + ln 2)	KL(Q∣∣P)+Pi=ι Ep~qKL(Qil∣P) ∣	1	∣	1	片 2,~ n√m	十 8√m 十 n√m	δ
MLAP-M (Amit & Meir, 2018)	er(Q)	∕KL(Q∣∣P) + ln 华 V	2(n-1)			1 Pn	/KL(Q∣∣P) + Ep~q KL(Qi∣∣P) + ln ¾m n 乙i=1 V	2(m-1)
MLAP-S (Amit & Meir, 2018)	er(Q)	/KL(Q∣∣P) + ln 茅 V	2(n-1)			n pn=1[ *e + JT吕 erg.s,)]
PACOH (Rothfuss et al., 2021)	er(Q)	√n KL(QIIP) + 81n	KL(Q∣∣P)+Pi=ι Ep~qKL(Qil∣P) +(	1	+	1 ∙b 丁 n√m	∖ 8n√m	√n	δ)
λ bound (Liu et al., 2021)	分(Q) 1-λ∕2	∕KL(Q∣∣P) + ln 华 V	2(n-1)			1 Pn	KL(Q∣∣P) + Ep~q KL(Qi∣∣P) + ln 4n√√m n 乙i=1	mλ(1-λ∕2)
classic bound (ours)	er(Q)	JKL(Q∣∣P) + in⅞^ V	2n		q∕κL(Q∣P) + Ep~Q Pi=ι KL(Qi∣∣P)+ln 2^ V	2nm
quadratic bound (ours)	er(Q)	qq KL(Q∣∣P) + 1^^ V	2n		∆+ln 4√nm ι C ∕~c∖ , ∆+ln 4√nm /∆+in 4√nm + nm δ +2V er(Q) + Anm^ ʌ/ +2nmδ
λ bound (ours)	分(Q) 1-λ∕2	qq KL(Q∣∣P) + 1^^ V	2n	KL(Q∣∣P) + Ep~q Pi=ι KL(Qi∣∣P)+ln 4√nm nmλ(1-λ∕2)
Appendix B Proof of Our Theoretical Results
B.1 Proof of Generalized PAC-Bayes-kl B ound from i.i.d. Setting to
Independently but Non-identically Distributed Meta-Learning Setting
We first give the proof of Lemma 1 in the main paper. Actually, the proof is proceeded with almost
the same sequence of arguments as that of (Lever et al., 2013, Theorem 1) for i.i.d. case, with
the only difference being that the focused samples in Lemma 1 are independent but non-identically
distributed. We include its proof just for the sake of the completeness.
ProofLemma 1. Using FUbini's theorem to exchange Ef〜∏ and ES, then for any δ ∈ (0,1), with
probability at least 1 - δ we have
l ES Ef 5 etφ(R(f ),r(f))
δ
≥ ln Ef5etφ(R(f)，r(f))	(Markov)
= ln∕ etφ(R(f),r(f)) 手dρ	(change of measure)
≥ Z (lnetφ(R(f),r(f)) + ln ʃ)dρ	(Jensen)
F	dρ
=tEρΦ(R(f), r(f)) - KL(ρIIπ)
≥tΦ(EρR(f), Eρr(f)) - KL(ρIIπ).	(Jensen)
Next we need the following Lemmas 3-4 to prove our proposed Lemma 2 in the main paper.
Lemma 3 (Seldin et al., 2012, Lemma 1) Let X1, ..., XK be a sequence of independent random
variables, such that Xk ∈ [0,1] almost surely and EXk = μk, for k = 1,..., K. Let Y1,..., YK
be independent Bernoulli random variables such that EYk = μk. Then for any convex function
g : [0, 1]K → R, we have Eg(X1, ..., XK) ≤ Eg(Y1, ..., YK).
12
Under review as a conference paper at ICLR 2022
Lemma 4 (Hoeffding, 1956, Theorem 3) Let Y = PkK=1 Yk, where Yk is a Bernoulli random vari-
able with EYk = μk. Let Y 〜Bin(K, μ) be a Binomial random variable with μ = K^ PK=I μk.
Thenforany strictly Convexfunction g : [K] → R, Eg(Y) ≤ Eg(Y).
Based on Lemma 3-4, we can obtain a useful corollary that bound the the sum of independent [0, 1]-
valued random variables with the sum of i.i.d. Bernoulli random variables as follow.
Corollary 1 Let X1, ..., XK be a sequence of independent random variables with Xk ∈ [0, 1] al-
most surely (a.s.) and EXk = μk, ∀k = 1,…,K. Let 匕,…,YK be i.i.d. Bernoulli random
variables with EYk =六 PK=I μk. Let X = PK=I Xk,Y = PK=I Yk. Then for any strictly
convex function g:
Eg(X) ≤ Eg(Y).
Proof. Let f be an affine function on [0, 1]K , such that f(X1, ..., XK ) = PkK=1 Xk. Then g(X) =
g ◦ f(X1, ..., XK ) can be considered as the composition of the affine function f and the strictly
convex function g. Hence g ◦ f is a convex function on [0, 1]K . Then according to Lemma 3, there
exists independent Bernoulli random variables {K}3ι With EYk = μk, such that
Eg(X) = Eg ◦ f(Xι,... Xk ) ≤ Eg ◦ f (H,…,YK ) = Eg(Y).
Finally, according to Lemma 4, for any strictly convex function g, We can derive the folloWing result,
Eg(Y) ≤ Eg(Y). ■
Corollary 2 (Lemma 2 in the main paper) In the setting of Corollary 1, we have
EeK kl(K1 PK=I Xk | ㈤ ≤ EeK kl(K1 PK=I YkI ㈤
Proof. Recall that kl(p||q) is a strictly convex function With respect to p (i.e., the second-order
derivative d kPpllq) = ɪ + ɪ--p > 0, ∀p ∈ (0,1)), and exp is a strictly-increasing convex func-
tion, hence exp{λ kl(p||q)} (∀λ > 0) is a strictly convex function With respect to p. Therefore
exp{K kl( -K ∣∣μ)} is a strictly convex function w.r.t. X (just show that the second derivative is posi-
tive). Combining the above discussion With Corollary 1 finishes the proof.	■
Remark 1 (the technical difficulty when obtaining our Corollary 2)
A previous result that is similar to our Corollary 2 is the Theorem 1 in (Maurer, 2004), which states
that: for i.i.d. random variables Xi,...,Xk With Xk ∈ [0,1] a.s. and EXk = μ, define the
i.i.d. Bernoulli random variables X：,..., XK With EXk = μ, then we have Eeκkl(K PK=I XkIw) ≤
Eeκ kl( K1 PK=I xkllμ). It can be seen that our derived Corollary 2 is actually the generalized version
of Theorem 1 in (Maurer, 2004) by replacing the ‘i.i.d. X1, ..., XK ’ condition with a weaker
condition ‘independent X1, ..., XK ’. To core step to obtain such extension is the use of Lemma 4
(i.e. (Hoeffding, 1956, Theorem 3)) to reduce the independent setting into the i.i.d. setting. Such
extension is truly useful when dealing with independent but non-identically distributed setting (e.g.
meta-learning setting).
Now we are ready to employ the above Corollary 2 to derive our generalized PAC-Bayes-kl bound
for independent random variables, leading to the following proof of Theorem 1 in the main paper.
Proof of Theorem 1. For any fixed f ∈ F, let {ηk}kK=1 be i.i.d. Bernoulli random variables with
Enk =春 PK=I Eg%gk(f, ξk). Then we have, for any fixed f ∈ F,
ESeκkl(r(f)I∣R(f)) =EeKkl(K PK=I gk(f,ξk)I∣k1 PK=I Enk)
≤Eeκkl(K PK=I nkiik1 PK=I Enk)	(Corollary 2)
≤2√K,
where the last inequality holds due to the fact that for a binomial random variable n = PK=I nk 〜
Bin(K, u) with K trials and success μ for each trial, by (Maurer, 2004, Thm 1) we have
Eeκkl(KW)= XX (K)(K)(KKW)κ-k	∈ [√K, 2√K].
k=0
13
Under review as a conference paper at ICLR 2022
Notice that kl(p||q) is also convex with respect to the pair (p, q), thus we can set the function Φ in
Lemma 1 as the binary relative entropy kl, and set t = K to finish the proof:
kl(Ef~ρr(f)∣∣Ef~ρR(f)) ≤l[KL(ρ∣∣π) +lnEf~∏ESeκkl(r(f)||R(f))∕δ]
K
≤ KL(P∣∣∏) , ln(2√K∕δ).
—K + K .
B.2 Proof of Three Improved PAC-Bayes B ounds for Meta-Learning Based on
Extended PAC-Bayes-kl Bound
Now we first give three relaxations of the generalized PAC-Bayes-kl bound derived in our Theo-
rem 1, based on the Pinsker’s inequality. We borrow the relaxation techniques from (McAllester,
1999; Perez-Ortiz et al., 2021; Thiemann et al., 2017) and hence generalize their PAC-Bayes bounds
(i.e. PAC-Bayes-classic/quadratic/X bound) from i.i.d. setting to independent setting.
Corollary 3 In the setting of Theorem 1, the following inequality holds with probability at least
1 - δ over the draw of sample S for any measure ρ over F, we have
(i)	PAC-Bayes-classic bound:
|Ef"(f) - Ef~ρR(f)| ≤ JKL(PIm +B(2√Kδ).
2K
In particular, if Ef ~ρr(f) < Ef ~ρR(f), we can obtain two sharper PAC-Bayes inequalities,
(ii) PAC-Bayes-quadratic bound:
Ef ~ρR(f)
ρr(f) +
KL(ρ∣∣π) +ln(2 √K∕δ)
2K
+
/κL(ρ∣∣π) + ln(2√K∕δ 八2
V	2K	)
(iii) PAC-Bayes-λ bound:
vλ 。(0 2) F R(f∖ E Ef~ρMf) , κL(P∣∣∏) + ln (2√K∕δ)
∀λ ∈ (0, 2)，Ef~PRf) ≤ 1 - λ∕2 + -Kλ(1- λ∕2)-.
Proof. (i) For the PAC-Bayes-classic bound, directly using Pinsker’s inequality kl(P||q) ≥ 2(P -
q)2 finishes the proof. Alternatively, we can also obtain a one-sided inequality for Ef「“r(f)-
Ef ~ρR(f) ( or Ef ~ρ R(f) - Ef「“r(f)) by replacing 2 in the RHS of two-sided bound with 1. ■
(ii) For the PAC-Bayes-quadratic bound, note that if q > p, then kl(p∣∣q) ≥ (P-q) . Therefore, if
there exists a constant θ > 0, such that kl(p∣∣q) ≤ θ, then we have (p-q) ≤ θ, which leads to the
below inequality:
q - P ≤ vz2qθ.
(7)
Note that Eq. (7) can be considered as the quadratic inequality on √q. Solving this inequality results
in the following inequality,
q ≤(Jf工 + Jf)2,
which finally gives the PAC-Bayes-quadratic bound if We set q = Ef~ρR(f), P = Ef ~ar(f), and
KL(ρ∣∣π) + ln (2√K∕δ)
θ
K
(iii) For the PAC-Bayes-X bound, note that if q > p, then kl(p∣∣q) ≥ (p-q) . Therefore, if there
exists a constant θ > 0, such that kl(p∣∣q) ≤ θ, then we have (p-q) ≤ θ. We then have for any
positive λ > 0, |p - q| ≤ √2qθ ≤ 1 (λq + 2λθ). ThUs We have
Xq θ	Xq θ
-T - λ ≤ p - q ≤ T + λ∙
(8)
Using just the above left-hand-side inequality, with simple rearrangement we finally obtain that
q(1 - 2) ≤ p + θ, thus for any λ ∈ (0,2):
≤ P + θ
q - 1 - λ∕2 + λ(1 - λ∕2).
14
Under review as a conference paper at ICLR 2022
Combining the above results with the first assertion, we thus have with probability at least 1 - δ over
the draw of S ,
Ef〜ρR(f) ≤
Ef 〜ρr(f)	KL(ρ∣∣π) + ln(2 √K∕δ)
1 - λ∕2 +	Kλ(1 - λ∕2)
Next we focus on giving the explicit upper bounds on the deviations er(Q) - eer(Q) and eer(Q) -
ebr(Q). First, we use Corollary 3(i) to give an explicit bound on |er(Q) - eer(Q)| as follow.
Proposition 1 For any δ∈ (0, 1), any pre-defined hyperprior P, with probability at least 1 - δ over
the draw of n distributions {Di}in=1, we have for any hyperposterior Q,
-6	~s∣<JκL(Q∣∣P)+ln 竽
Ier(Q) - er(Q)I ≤ \ ---ʒ-----—.
2n
Proof. Notice that we can rewrite er(Q), eer(Q) as the following form:
1n
er(Q) = EP 〜QE(D,S)〜T ×Dm er(Q(S)P ),D),	er(Q) = EP 〜Q n	er(Q(Si, P), Di).
Recall Theorem 1, we thus set K = n, f = P, set the reference measure π = P, the
posterior measure ρ = Q, the observation ξk = (Di, Si), and set the function gk (f, ξk) =
Eh^Q(si p)Ez〜Dil(h,z) ∈ [0,1]. Then We can give an upper bound on the relative entropy
kl(er(Q)∣∣er(Q)),
bl∕-mx∣∣ (nL( , KL(QIIP) +ln 2√n
kl(er(Q)∣∣er(Q)) ≤ -------------------.
n
With Pinsker’s inequality kl(p, q) ≥ 2(p - q)2,
Ier(Q)- er(Q)I≤ a一早.-
Further, We can use Corollary 3(i)-(iii) to give an explicit bound on eer(Q) in Propositions 2-4.
Proposition 2 (PAC-Bayes-classic bound for eer(Q)) For any predefined hyperprior P, any δ ∈
(0, 1), with probability at least 1 - δover the draw of the training sample S = {Si}in=1, for any
hyperposterior Q we have,
ʌr^i < SKL(QIIP) + EP〜QPn=I KL(QiIIP) +ln中
Ier(Q) - er(Q)I ≤ V---------------------2nm----------------------.
Proof. Notice that We can reWrite eer(Q), ebr(Q) as the folloWing form:
1n
er(Q) = EP~QE(hi,…,hn)〜Qi × …×Qn装 EEz〜Dil(hi,z),
n i=1
1 nm
er(Q) = EP 〜Q E& ,…，hn)〜Qi ×∙∙∙×Qnn⅛∑∑l(hi, zij ).
nm i=1 j=1
Recall Theorem 1, We can set f = (P, h1, ..., hn), the reference measure π = P × P n, the posterior
measure ρ = Q × Qin=1 Qi, Where Qi = Q(Si, P), and set the observations ξk = zij,gk(f,ξk) =
l(hi, zij). Then using Corollary 3(i), With probability ≥ 1 - δWe have,
ʌz,ɔʌ, . SKL(Q× QnQiIIP × Pn) + in2√nm
M(Q)- er(Q)I ≤ V-------------2nm--------------.
Furthermore, notice that KL(Q × Qn QiIIP × Pn) = Eq×q=i Qiln d(Q×p×PnQi)=
EQ×Qn=i Qi(ln dQ + Pn=ιln ddP) = KL(QIIP) + EP~Q Pn=I KL(QiIIP), WhiChComPletes
the Whole proof.	-
15
Under review as a conference paper at ICLR 2022
Proposition 3 (PAC-Bayes-quadratic bound for efr(Q)) For any predefined hyperprior P, any
δ ∈ (0, 1), with probability at least 1 - δ over the draw of the training sample S = {Si}in=1, for any
hyperposterior Q we have,
er(Q) ≤( Jer(Q)+烹+K
where ∆ = KL(Q∣∣P) + EP〜Q Pn=ι KL(QiIIP) + ln 2⅛nm.
Proof. The main step is to use the PAC-Bayes-quadratic bound in Corollary 3(ii) to bound eer(Q).
The rest proof is similar to that of Proposition 2 and is left to readers.
Proposition 4 (PAC-Bayes-λ bound for efr(Q)) For any predefined hyperprior P, any δ ∈ (0, 1),
with probability at least 1 - δ over the draw of the training sample S = {Si}in=1, for any hyperpos-
terior Q and any λ ∈ (0, 2), we have,
eer(Q) ≤
er(Q) 1 KL(Q∣∣P) + EP〜Q Pn=I KL(QiIIP) +ln 中
1 — λ∕2	nmλ(1 — λ∕2)
Proof. The main step is to use the PAC-Bayes-λ bound in Corollary 3 (iii) to bound eer(Q). The
rest proof is similar to that of Proposition 2.
Proof of Theorem 2. The main idea is to give bounds on kl(eer(Q)IIer(Q)) and kl(ebr(Q)IIeer(Q))
respectively, and then combine them with union bound to give the explicit upper bound on er(Q).
(1) To bound kl(eer(Q)IIer(Q)), since we have no idea whether eer(Q) < er(Q) or not, we can not
use the quadratic bound or λ bound. What we can use is just the classic bound given in Proposition 1.
Then we can derive a one-sided bound on er(Q) — eer(Q) with
JKL(Q啜+ln √n .⑵ To bound
kl(ebr(Q)IIeer(Q)), since the empirical multi-task risk ebr(Q) is strictly smaller than the expected
multi-task risk, we hence can use the PAC-Bayes-quadratic bound in Proposition 3 and the PAC-
Bayes-λ bound in Proposition 4 to bound eer(Q). Then combining the upper bound on eer(Q) with
the one-sided bound on er(Q) — eer(Q) in discussion (1), we can obtain the PAC-Bayes-quadratic
bound or λ bound for meta-learning. For the PAC-Bayes-classic bound, note that we can obtain a
one-sided bound for er(Q) — er(Q) by replacing 2 with δ in the bound of Proposition 2. Thus,
combining the one-sided bound on eer(Q) — ebr(Q) with the one-sided bound on er(Q) — eer(Q) in
discussion (1) finishes the proof.
B.3 Proof of PAC-Bayes-kl Bound on Novel Task with the Prior from the
Learned Hyperposterior
B.3.1 PROOF OF THE EXISTENCE OF THE INTERMEDIATE MEASURE ν IN EQ. (6)
In this section, we first give a proof of the existence of the intermediate measure ν defined in Eq. (6).
Therefore, we can also guarantee the existence of the Gibbs meta-learned prior and the Gibbs pos-
terior defined in Eq. (6). We first give a basic lemma in measure theory as follow.
Lemma 5 (Yeh, 2014, Page 255, Prob.11.8) Given a measurable space (X, B). Let μ be a σ-
finite positive measure on (X, B) and let f be a μ-integrable nonnegative extended real-valued
B-measurable function on X such that f = 0, μ-a.e. on X. Define a SetfUnction V on B by setting
V(E) = f f dμ,∀E ∈ B.
E
(a)	Show that V is a σ-finite positive measure on (X, B).
(b)	Show that ν << μ and 岩 exists, and dμ = f, μ-a.e. on X.
(c)	Show that dν exists and dν = 1 μ- and V-a.e. on X.
16
Under review as a conference paper at ICLR 2022
Now we can prove how to construct the measure ν ∈ M1 (H) defined in Eq. (6) of
main paper as follow. The core step is to show that the normalization constant β =
JH exp{ Y pn=ι Ez~Dj(h, z)}dP is well-defined (i.e., finite), such that the measure V is Well-
defined. The finiteness of β holds due to the fact that the loss function l is a bounded function. The
existence of the measure ν ∈ M1(M1(H)) defined above Eq. (5) can be proved in a similar way.
Claim 1 There exists a probability measure ν ∈ M1(H) that satisfies the conditions in Eq. (6).
Proof Define a nonnegative function f : H → R+ as f (h) = β exp{ Y £乙 Ez~D」(h,z)},
where β = JH exp{ Y PZi Ez~Dj(h,z)}dP is a normalization constant. Note that the loss
function l has range [0, 1], hence β ≤ H exp{γ}dP = exp{γ} < ∞ and hence the den-
sity function is well-defined. We then construct a set function over B(H), as defined by:
ν(E) = RE fdP, ∀E ∈ B(H), where B(H) is a σ-algebra over the space H. Then according
to the assertion (a) in Lemma 5, the set function ν is a σ-finite positive (probability) measure
on the space (H, B(H)). According to the assertion (c) in Lemma 5, the measure P is abso-
lutely continuous with respect to the measure ν (i.e., P << ν), so the Radon-Nikodym deriva-
tive ddν = 1 = exp{- Y Pn=I Ez~dil(h,z)}β, and dP = exp{- Y Pn=I Ez~dil(h,z)}βdν.
Therefore we have 1 = JH 1dP = JH βexp{-n Pin=1 Ez~Dj(h, z)}dν, then we obtain β =
1/ JH exp{- Y Pn=ι Ez~Dj(h, z)}dν. Similarly, we can construct the (posterior) probability mea-
sure Qi by defining the set function: Qi(E) = RE exp{-mY-Pj=I l(h,zj)}dν∕βi,∀E ∈ B(H),
where βi0 = JH exp{- jPm=I l(h, Zj)}dν is the normalization constant.	■
B.3.2 Proof of PAC-Bayes-kl B ound on Novel Task with the Localized Gibbs
Prior from the Dirac Hyperposterior
Next, we give a fundamental lemma which guarantees that the equality ES fdQS = fd(ESQS)
holds. Such result will be exactly applied to the proof of Bounding II for Theorem 3 in the next sec-
tion. The proof of ES fdQS = fd(ESQS) follows the steps of ‘standard method’ in measure
theory (Yeh, 2014), but we have not found this result in the literature, hence we include the details.
Lemma 6 Denote the posterior distribution Q(S, P) by QS for brevity. Define the set function
over the σ-algebra B(H) of the hypothesis space H as: ∀A ∈ B(H), (E0~τ Es~Dm QS)(A)=
Ed~tEs~Dm Qs(A). Then we havefollowing three assertions:
(a)	E0~τEs~Dm Qs is a probability measure over H.
(b)	∀ integrablefUnction f : H → R, E E E	f (h) = E%~Ed~tESZDmQS f (h).
D~τ S~Dm h~Q(S,P)
dE dED,SQS — EC GdQS
(C) —dP— = ed,s "d^ .
Proof.
(a)	For the first assertion, we have:
(D(ED~TES~Dm QS) (H) = ED~TES~Dm QS (H) = ED~TES~Dm 1 = 1.
(ii)(Eo~τEs~Dm Qs )(0) = Eo~τEs~Dm Qs (0) = 0.
(iii) For any pairwise disjoint set {Ak}k∞=1, where Ak ∈ B(H), we have
(E0~τEs~Dm Qs )(∪k Ak) = Ed~t Es~dw∙ Qs (∪kAk)
∞∞	∞
=E0~τEs~Dm EQS(Ak) = £Ed~tEs~0mQs(Ak) = E (E0~τEs~DmQs)(Ak),
k=1	k=1	k=1
where the exchange between P and E holds due to Fubini,s theorem. Therefore,E0~τEs~Dm Qs
is a probability measure over the hypothesis space H.	■
(b)	For assertion (b), we take standard method to demonstrate that the equality holds.
(b)(i) First consider the case where f(h) is a simple function.
Let f = Pkn=1 ak1Ak, (ak ≥ 0), where Ak ∈ B(H), 1Ak is the indicator function defined over Ak.
17
Under review as a conference paper at ICLR 2022
Then the right hand side of the equality has
n
RHS = / X akIAkd(E0〜TES〜DmQS)
H k=1
n
=E ak (ED〜TES〜DmQS)(Ak)	(assertion (a))
k=1
n
=ak ED 〜TES 〜Dm QS (Ak )
k=1
n
=ED〜TES〜Dm〉： ak QS (Ak )
k=1
=ED 〜TES 〜Dm Eh〜QS f (h) = LHS.
(b)(ii) Now consider the case where f(h) is a nonnegative function.
Then we can choose a series of nonnegative non-decreasing simple functions {fk}k∞=1 ↑ f (i.e.,
limk fk(h) = f (h), almost everywhere on H). Using Levy’s monotone convergence theorem,
RHS = l limfkd(ED〜TES〜DmQS)
Hk
= Iim/ fkd(ED〜TES〜DmQS)
= lim ED 〜TES 〜Dm Eh〜QS fk (h)	((b)(i))
k
=ED〜TES〜DmEh〜QS lim fk (h) (Levy)
k
=ED 〜TES 〜Dm Eh〜QS f (h) = LHS,
(b)(iii) Finally consider the case where f(h) is an integrable function.
Decompose f as f = f+ - f-, where f+ = max(f, 0), f- = max(-f, 0). From (b)(ii) we have,
E E	f +(h) = Eh 〜Ed-t ES-Dm Qs f+(h),
D〜T h〜Q(S,P)
E E E	f-(h) = Eh 〜Ed』ES “Dm Qs f-(h).
D〜T S〜Dm h〜Q(S,P)
Therefore, using the linearity property of the integral we can obtain,
LHS = E E E (f+ - f-)(h)
D〜T S〜Dm h〜Q(S,P)
E E E	f+(h)- E E
D〜T S〜Dm h〜Q(S,P)
D〜T S〜Dm
h〜悬P L
D〜TES〜Dm Qsf+(h) - Eh〜ED〜TES〜Dm Qs f (h)
=Eh 〜E
=Eh 〜E.
Essm QS (f+ - f-)(h) = RHS,
D〜T
which finishes the proof for assertion (b).
(C) For the last assertion, note that the Radon-Nikodym derivative ddQS ≥ 0, then for any
subset E ⊆ H, we have
ZE(ED,S dQS)dP
=Z(Z Z dQS dDmdτ )dP
E	M1(Z) Zm dP
=I [	[ dQSdPdDmdτ (Fubini)
M1(Z) Zm E dP
=	dQSdDmdτ (change of measure)
=(ED,SQS)(E).
18
Under review as a conference paper at ICLR 2022
Therefore, We have that the Radon-Nikodym derivative of Ed,sQS with respect to P is dEDpQS =
Ed,s ddpS. Hence we finish the whole proof for assertion (c).	■
Proof of Theorem 3. Using the PAC-Bayes kl-bound in Theorem 1 for the i.i.d. setting, with
probability at least 1 - δ we have,
ED,S
kl(er(Q,S)∣∣er(Q,D)) ≤EDfKL(QIP) + ln(2//δ
m
Hence, we aim to bound the expectation of KL-divergence in the RHS of the above inequality.
Actually, we can decompose this term into three parts and bound these parts separately, that is,
^ED 〜τ,S 〜Dm KL(Q(S, P )||P ) = ^ED 〜τ,S 〜Dm Eh 〜Q(S,P) ln QdP~~
=ED,SEh〜q(s,p) ln
β0exp{-n Pi=ι E l(h,z)}
Z〜Di
=ED,SEh〜q(s,p) ln ɪ + E.
β
n
m
i=1
j=1
=ED,SEh〜Q(S,P) ln β0 + ED,
ED,SEh〜Q(S,P) ln β + ED,
n
S [ Y X er(Q(S, P), Di)- Yer(Q(S, P ),S)]
n i=1
n
，s [n X er(Q(S, P),Di) - Yer(Q(S, P), D)]
i=1
{z
II
+ ED,Sγ er(Q(S,P),D)-ebr(Q(S,P),S)
I---------------------------------}
{z
III
Bounding I.
E h β E M β0
Ed,s ln § = -Εd,s ln —
- ED,S ln
He
exp{-mγ Pmm=I ι(h,z)}
汉p{-n Pi=I Ez 〜Dii(h,z)}/p(h)
dν
mn
- ed,sln	P(h) eχp{-m ^Xl(h, zj)+n ^XEz〜。/(4力网沙
H	j=1	i=1
≤ed,s Eh〜P {m ^X l(h, zj) - n ^X Ez〜Dil(h,z)} (Jensen Inequality of - ln)
j=1
n
i=1
=ed,s [γbr(P. S) - Y X er(P, Di)]
n
i=1
n
=Εd [γer(P,D)- Y X er(P,Di)]
n
i=1
n
=[γED〜Ter(P, D)- Y X er(P, Di)]
n i=1
≤Y /In (ʌ/n/)), (set ∏ = P = Pin Corollary 3(i))
2n
where the last inequality holds with probability at least 1 - δ.
|
z
I
}
|
}
m
n
19
Under review as a conference paper at ICLR 2022
Bounding II.
n
Ed,s [n X er(Q(S，P)，Di)- Yer(Q(S, P),D)]
n
=—SX [eD,SEh〜Q(S,P)l(h, Di) - eD,SEh〜Q(S,P)l(h, D)]
n i=1
n
=Y ^X [Eh〜Ed,sQ(S,P)l(h,Di) - EDEh〜Ed,sQ(S,P)l(h, D)] (Lemma 6 (b))
n i=1
n
=Y X [er(Ep,sQ(S, P), Di)- EDer(E0,sQ(S, P),D)]
n i=1
≤γr lM√n0+KL(ED,s QgP )|1P)	(Corollary3)
(*)八n(√n∕δ)+KL(ED,s Q(S,P )||P)	，T …
=γ∖ ------------------------------ (Lemma 6 (c))
2n
≤γ∖ W√nδ+ED,s KL(Q(S,P )|1P)	(Convexity of KL- divergence)
2n
=	An (√n∕δ) + Ed,s KL(Q(S, P )必
=YV	2n	.
For reader S benefit, We provide more explanations for equality (*), where Q,P are density functions
of measures Q and P respectively, with respect to measure ν. Actually from Lemma 6 (c) we have,
TQ(S,P) = Ed,s dQ≡ = Ed,s Q(S,P).
dν	dν
Then by reusing Lemma 6 (c), we have,
KL(ED,S Q(S, P)||P)
=Eh〜Ed,sQ(S,P) ln
dEp,s Q(S,P)
dP
=Eh〜Ed,sQ(S,P) ln ED,S
=Eh〜Ed,sq(s,p) ln ed,S
dQ(S,P)
dP
dQ(S,P) (dP「
~dV-' dT'
=E	ln E	Q(s,p )
E八~Ed,s[Q(s,p )(h)] 1 d,s P
= KL(ED,S Qe(S, P)||Pe).
Bounding III.
Reusing the one-sided inequality in Corollary 3(i) and the Jensen,s inequality of the concave square
root function (i.e., f (t) = √t), we have with probability at least 1 - δ,
Ed,sY[er(Q,D) - ebr(Q,S)] ≤yEd,s|叫而/*；KL(QIIP)
/	l ln(√m∕δ)+ Ed,s KL(Q∣∣P)
≤τV----------2m----------.
Bounding I + II + III.
Combining the above upper bounds I-III, and denoting ED,S (KL(Q(S, P)||P)) by θ for brevity,
we have the following inequality,
r ln(3√n∕δ)	r ln(3√n∕δ) + θ	I ln(3√m∕δ) + θ
θ ≤ Yy	2n	+ Yy —2n—+Yy —而—
20
Under review as a conference paper at ICLR 2022
「/in"如
V	2n
If θ ≤ γ
, then we are done. Otherwise, using simple calculation and rearrangement, as
well as the basic inequalities √a + b ≤ √α + ʌ/b and √a ≤ a++1 , We have
2	2 ln(3√n∕δ)	办 Jln(3√n∕δ)
θ + Y —2n— 2θγV —^n-
≤γ2( ∕n(3√n^) + θ
+ ∕lM3√m0 + θ)2
V 2m )
_ 2/ln(3√n∕δ) + θ =Y(	2n	ln(3√m∕δ) + θ	Jn 3√n ln 3√m + θ(ln 3√n +ln 3√m) + θ2) 2m	nm
Z 2 八n(3√n∕δ)+ θ ≤γ( —2n—	+ ln(3√m∕δ) + θ + SM 3√n ln 3√m + Sθ(ln 3√n +ln 3√W + ɪ) 2m	V	nm	V	nm	√nm
≤ 2(ln(3√n∕δ)+ θ -Y〈	2n	+ ln(3√m∕δ) + θ + Sln 3√n ln 3√√m + S (ln 3√n + ln 3√δmTθ + 1 + _J_ 2m	V	nm	V	nm	2	ʌ/nm
Thus we have
θ2
— θ0严下
+亡+亡+
2m	2n
γ2，(ln 3√n + ln 3√m)
2 ʌ/nm
2(ln(3√m∕δ) + ∕ln 3√n ln 3√m + ↑∕ln 3√n +ln 3√m)
-'	2m	n nm	2√nm
Note that if a quadratic function x2 - ax - b ≤ 0, we have X ≤ 2 + Jb + a42 ≤ a + √b. Thus,
θ ≤2γr 尸T W+上+γV(E
一 'V 2n	2m	2n
+ γ(ln (3√m∕6 + .∣ln 3√√n ln 3√δm +
2m	nm
3√n + ln 3√m)
2 ʌ/nm
+√γ=
nm
J(ln 3√n + ln 3√m)
2 ʌ/nm
Appendix C More Details of Experiments
C.1 Architecture of Deep Neural Networks and Optimization Settings
For shuffled pixels experiment, the network structure is designed as a 4-layer (3 hidden layers and
a linear output layer) fully-connected neural network, with 400 unites per layer. The total number
of parameters to be learned is 282 × 400 + 4002 + 4002 + 400 × 10 = 637, 600. For permuted
labels experiment, the network structure is chose as a 4-layer convolutional neural network, with 2
convolutional layers of 10 and 20 filters (5 × 5 kernels), a linear hidden layer with 50 units and a
linear output layer. The total number of parameters is about 52 × 10+ 10 × 52 × 20+202 × 50+50 ×
10 = 25, 750. We set hyperprior and hyperposterior parameters as κP = 2, 000 and κQ = 0.01
respectively. The confidence level in PAC framework is δ = 0.1. We run all methods with 150
training epochs during the meta-training phase and 200 training epochs during the meta-test phase.
C.2 Three Bound-Minimization Meta-Learning Algorithms for Classification
Problem
In this section, we detail the procedure of our PAC-Bayes bound-minimization algorithms for meta-
learning. The algorithm is composed of two parts: meta-training part (Algorithm 1) and meta-test
part (Algorithm 2). Note that in Section C.1 we have set the deep neural network as Bayesian net-
work (Blundell et al., 2015), therefore we can obtain the analytic form of different KL-divergences
and approximate the expectation with Monte-Carlo methods. Thus we can implement our algo-
rithm as follow. We only show the procedure of PAC-Bayes-λ-bound-minimization algorithm. The
21
Under review as a conference paper at ICLR 2022
Algorithm 1 PAC-Bayes-λ bound-minimization algorithm, meta-training phase
1:	Input: Datasets from n training tasks: Si,…，Sn, hyperprior P, hyperparameter λ = 1.
2:	Output: Parameter θ of hyperposterior Qθ .
3:	Initialize:
4： θ = (μp,ρp) ∈ R2d, φi = (μi,Pi) ∈ R2d,i = 1,…，n.
5:	while not converged do
6:	for i ∈ {1, ..n} do
7:	Sample a mini-batch Si0 from datasets Si .
8:	Compute Ep@~q@er(Qi, Si) with the mini-batch Si by averaging Monte-Carlo draws.
9:	Compute KL(Qθ||P) with Eq. (9).
10:	Compute Ep@~q@ KL(Qφi ∣∣Pθ) with Eq. (10) by averaging Monte-Carlo draws.
11:	end for
12:	Compute the PAC-Bayes-λ bound in Theorem 2 with Ep@~q@er(Qi, Si), KL(Qθ||P) and
Epθ ~Qθ KL(Qφi ∣∣Pθ ), i=1,∙∙∙,n.
13:	Compute the gradient of PAC-Bayes-λ bound w.r.t {θ, φ1, ..., φn} using backpropagation.
14:	Take an optimization step.
15:	end while
16:	return θ
Algorithm 2 PAC-Bayes-λ bound-minimization algorithm, meta-test phase
1:	Input: Learned hyperposterior Qθ , dataset Sn+ from task Dn+i, test data S*, hyperparameter
λ= 1.
2:	Output: Posterior Qφn+1 for Dn+i, and the single-task PAC-Bayes-λ bound B(φn+i) (i.e.,
test bound), classification error (i.e., test error).
3:	Sample an informative prior P from Qθ
4:	Initialize: posterior Qφn+1 as P
5:	while not converged do
6:	Sample a mini-batch Sn0 +i from datasets Sn+i.
7:	Compute the empirical loss Eh~Qn+ιeer(h, Sn+i) with the mini-batch Sn+1by averaging
Monte-Carlo draws.
8:	Compute KL(Qφn+1 ||P) with Eq. (10).
9:	Compute the single-task PAC-Bayes-λ bound B(φn+i) in Corollary 3 (iii) with
Eh^Qn+ιer(h, Sn+i) and KL(Qφn+ι ||P).
10:	Compute the gradient of the bound B(φn+i) w.r.t φn+i using backpropagation.
11:	Take an optimization step.
12:	end while
13:	Use the random classifier h 〜Qφn+ι to classify S * to output test error.
14:	return Qφn+1 , test bound B(φn+i) and test error.
other two algorithms based on our classic bound and quadratic bound can be inferred in a similar
way. Concretely, as shown in Section 4.4, by setting both the hyperprior and hyperposterior as the
isotropic Gaussian distribution: P = N(0, κ2PId×d), Qθ = N(θ, κ2QId×d), the KL-divergence
between hyperposterior Qθ and hyperprior P can be calculated as:
KL(Qθ IIP ) = ⅛θ⅛2.	(9)
2κP
Next, by choosing the prior Pθ and the posteriors Qφi (φi ∈ Rd) as factorized Gaussian:
Pθ (w) = Qk=i N (W k； μp,k ,σp ,k), Qφi (w) = Qk=i N (Wk ； ui,k ,σ2,k), the KL-divergence term
KL(Qφi IIPθ) in task-level complexity has a simple analytic form:
n	n,d 2	2 + (	)2
Ep0~q° X KL(Qφi ∣∣Pθ) = Ep0~q。1 X {ln 空 + i,k %	〃，)- 1},	(10)
i=i	2 i,k=i	σi,k	σP,k
where the expectation Pθ ~ Qθ can be approximated through Monte-Carlo method by adding
Gaussian noise to the parameter θ, as defined by θ := θ + e, e 〜 N(0, KQId×d).
22
Under review as a conference paper at ICLR 2022
—Permuted Pixels - 100 pixel SwapS
—I— Permuted Pixels - 200 pixel swaps
—I— Permuted Pixels - 300 pixel swaps
23456789	10
Number of training-tasks
123456789	10
Number of training-tasks
—|— Permuted Pixels - 100 pixel Swaps
—|— Permuted Pixels - 200 pixel Swaps
—|— Permuted Pixels - 300 pixel swaps
5	10 15 20 25 30 35 40 45 50 55 60X103
Sample SiZe per training-task
5	10 15	20 25 30 35 40 45 50 55 60X103
Sample SiZe Per training-task
(a) Classic Bound (b) Classic Bound’s error (c) Classic Bound (d) Classic Bound’s error
Figure 3: Average test bounds and test errors of our PAC-Bayes-classic bound on 20 meta-test tasks
for different pixel-shuffled environments. (a)-(b): Test bounds and test errors for different number
of training tasks. (c)-(d): Test bounds and test errors for different sample size per training task.
XSB*-məu co -UCDOm
Number of training-tasks
(a) Quadratic Bound
Figure 4: Average test bounds and test errors
123456789	10
Number of training-tasks
(b) Quad Bound’s Error
+ Permuted Pixels -100 pixel swaps
+ Permuted Pixels -200 pixel swaps
+ Permuted Pixels - 300 pixel swaps
5	10 15	20 25 30 35 40 45 50 55 60X103
Sample SiZe Per training-task
(c) Quadratic Bound
5	10 15 20 25 30 35 40 45 50 55 60X103
Sample SiZe Per training-task
(d) Quad Bound’s Error
of our quadratic bound on 20 meta-test tasks for
different pixel-shuffled environments. (a)-(b): Test bounds and test errors for different number of
training tasks. (c)-(d): Test bounds and test errors for different sample size per training task.
C.3 Convergence Analysis of Our PAC-Bayes Classic B ound and Quadratic
Bound for Meta-Learning
We provide detailed convergence analysis of our derived three improved PAC-Bayes bounds for
meta-learning (i.e., classic/quadratic" bound) in Figure 3-5. We also ShoW performance ComPar-
isons among our derived three bounds on a novel task in Figure 6. The experiments are conducted
across a Wide range of the number n of the training tasks and the sample size m per training
task. When n changes, m is set as 60, 000 consistently; When m changes, n is set as 10. We
plot the average test error and average test bound on 20 meta-test tasks from three environments
With 100/200/300 pixel sWaps. We can observe that: (i) With the increase of the number of train-
ing tasks or the sample size per training task, the test error and the test bound over the neW task
can both decrease to a loWer level, validating the asymptotic behaviour of our derived bounds. (ii)
Our bound-minimization method achieves better performance When the number of pixel sWaps gets
smaller, Which reveals the importance of task relatedness of the environment.
C.4 How to Learn Data-Dependent Localized Prior via Empirical Risk
Minimization (ERM)
In this section, we provide more details of how to learn data-dependent localized prior via empirical
risk minimization (ERM). Such content can be considered as the supplementary explanations of
Figure 2 in the main paper. Note that the localized prior defined in Eq. (5) is distribution-dependent.
However, it is difficult to set a distribution-dependent prior in practical applications, since
the data distribution is always unknown. Instead, we choose to set the data-dependent prior
(that can be obtained through ERM over a number of samples) as the localized prior on the novel
task. Actually, our strategy of learning a localized data-dependent prior is originated from (Parrado-
Hemandez et al., 2012; LiU et al., 2021). Concretely, there are three main methods in Figure 2:
baseline ERM, random prior, and k-ERM prior. Given a novel task associated with 2,000 i.i.d.
training samples S, the details of running these three methods are listed as follow.
Baseline ERM Method. The baseline ERM method aims to minimize the following empirical risk
over the 2, 000 training samples from the novel task with respect to a posterior Q:
2000
er(Q,s) = Eh~Q 2000 X /(h,%),
i=1
23
Under review as a conference paper at ICLR 2022
(a) λ Bound
2 O S 6 4
(*一 *ssMQU UO .-Otm-
—Permuted Pixels - 100 pixel swaps
—I— Permuted Pixels - 200 pixel swaps
—I— Permuted Pixels - 300 pixel swaps
23456789	10
Number of training-tasks
λ Bound’s Error
(c) λ Bound
〔％l*ssMQU Uo」o」」IIJ
Sample SiZe Per training-task
(d) λ Bound’s Error
Figure 5:	Average test bounds and test errors of our PAC-Bayes-λ bound on 20 meta-test tasks for
different pixel-shuffled environments. (a)-(b): Test bounds and test errors for different number of
training tasks. (c)-(d): Test bounds and test errors for different sample size per training task.
(a) Test Bound
1
10
23456789
Number of training-tasks
(b) Test Error
(c) Test Bound
5	10	15 20 25 30	35 40 45 50	55 60X103
Sample SiZe Per training-task
(d) Test Error
Figure 6:	Comparisons of average test bounds and test errors between our three PAC-Bayes bounds
(i.e., classic bound, quadratic bound and λ bound) on a new task from 100-pixel-shuffled environ-
ment (average over 20 meta-test tasks). (a)-(b): Test bounds and test errors for different number of
training tasks. (c)-(d): Test bounds and test errors for different sample size per training-task.
and returns a learned posterior Q for final classification, where the distribution Q is initialized as a
random prior (i.e., Gaussian prior).
Random Prior Method. For the random prior method, the minimization objective is the following
PAC-Bayes-λ bound for single-task learning (λ ∈ (0, 2), in practice we set λ = 1):
er(Q, S) KL(Q∣∣P) + ln (2√2000∕δ)
1 - λ∕2 +	2000λ(1 - λ∕2)	,
where the prior P is chose as the random Gaussian prior. The returned posterior Q is applied for
final classification over unseen data from the novel task.
k-ERM Prior Method. For the k-ERM prior method, we first use k training samples S0 on this
novel task (note that in MNIST dataset the k ∈ (0, 58000) training samples are non-overlapped with
the predefined 2, 000 training samples) to run the following ERM procedure:
1k
er(Q0, SO) = Eh〜Q0 k E l(h, Zi).
i=1
The returned posterior Q0 is called data-dependent localized prior, and is applied as the initialized
prior to the following PAC-Bayes-λ bound minimization procedure over 2, 000 training samples:
er(Q, S) KL(Q∣∣Q0) + ln (2√20OO∕δ)
1 - λ∕2 +	2000λ(1 - λ∕2)	.
The returned posterior Q will be applied for final classification on test data over the novel task.
Details of the k-ERM localized prior algorithm can be found in Algorithm 3. Note that in Figure 2,
we set k = 12, 000, 24, 000, and 42, 000 respectively, and the ‘random prior’ can be considered as
the 0-ERM prior. We can see that with the increase ofk, the k-ERM prior method obtains better test
performance. This indicates that with the increase of k, we can derive a localized prior with higher
quality for faster adaptation to novel task. However, in practice, we find that when we increase k
to a higher level (i.e., k > 42, 000), the test performance of the k-ERM method can not be better
than that of 42, 000-ERM method. Therefore, we just plot the test bounds and test errors of k-ERM
methods (i.e., k ≤ 42, 000) in Figure 2. Further, since all localized priors are set over the novel
24
Under review as a conference paper at ICLR 2022
Algorithm 3 k-ERM localized prior algorithm with PAC-Bayes-λ bound, single-task learning set-
ting_______________________________________________________________________________________
1:	Input: training dataset Sn+ι, S from task Dn+ι (Sn+ι T S = 0, |S| = k), test data S*,
hyperparameter λ = 1.
2:	Output: Posterior Qφn+1 for Dn+1, and the single-task PAC-Bayes-λ bound B(φn+1) (i.e.,
test bound), classification error (i.e., test error).
3:	Initialize: localized prior Q0 as a random Gaussian prior.
4:	while not converged do	. ERM over k samples to learn localized prior Q0
5:	Sample a mini-batch S0 from datasets S.
6:	Compute the empirical loss Eh〜q。er(h, S) with the mini-batch S0 by averaging Monte-
Carlo draws.
7:	Compute the gradient of ebr(Q0, S) w.r.t the parameter of Q0 using backpropagation.
8:	Take an optimization step.
9:	end while
10:	Initialize: posterior Qφn+1 as the localized prior Q0
11:	while not converged do	. PAC-Bayes learning with localized prior Q0
12:	Sample a mini-batch Sn0 +1 from datasets Sn+1.
13:	Compute the empirical loss Eh〜Qn+ιer(h, Sn+ι) with the mini-batch Sn +ι by averaging
Monte-Carlo draws.
14:	Compute KL(Qφn+1 ||Q0) with Eq. (10).
15:	Compute the single-task PAC-Bayes-λ bound B(φn+1) in Corollary 3 (iii) with
Eh〜Qn+ιer(h, Sn+ι) and KL(Qφn+ι ||Q0).
16:	Compute the gradient of the bound B(φn+1) w.r.t φn+1 using backpropagation.
17:	Take an optimization step.
18:	end while
19:	Use the random classifier h 〜Qφn+1 to classify S * to output test error.
20:	return Qφn+1 , test bound B(φn+1) and test error.
tasks, their test performance (i.e., test bounds and test errors) on novel tasks is irrelevant to the
setting of training tasks (i.e., the number n of training tasks and the sample size m per training task).
Therefore, in Figure 2 of the main paper, the plots of the test bound/error of localized priors
are all straight lines (with standard deviation) and do not change with the increase of n or m.
25