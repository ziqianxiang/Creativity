Under review as a conference paper at ICLR 2022
DistProp: A Scalable Approach to Lagrangian
Training via Distributional Approximation
Anonymous authors
Paper under double-blind review
Ab stract
We develop a multiple shooting method for learning in deep neural networks based
on the Lagrangian perspective on automatic differentiation. Our method leverages
ideas from saddle-point optimization to derive stable first-order updates to solve a
specific constrained optimization problem. Most importantly, we propose a novel
solution allowing us to run our algorithm over mini-batches with stochastic gradi-
ent fashion and to decouple the number of auxiliary variables with the size of the
dataset. We show empirically that our method reliably achieves higher accuracy
than other comparable local (biologically plausible) learning methods on MNIST,
CIFAR10 and ImageNet.
1	Introduction
Neural network training is inherently sequential: the network outputs first, then the error signal
backward layer-by-layer (Rumelhart et al., 1986). The problem with this approach is twofold: it can
lead to unstable gradient computation (Hochreiter & Schmidhuber, 1997; Bengio et al., 1992) over
long horizons while precluding layer-local parallelism (Crick, 1989). While a rich body of work
tackling those issues can be found under the heading of biologically plausible backpropagation (Lee
et al., 2015; Carreira-Perpinan & Wang, 2014; Choromanska et al., 2019), those methods have been
only been shown to work on small datasets.
In this paper, we approach this problem from a different angle: that of numerical methods for optimal
control. As noted by LeCun et al. (1988), this is a natural connection to make since backprop has
in fact been living a double life as the adjoint equation in the optimal control literature (Bryson,
1996). More specifically, we propose to decouple training within a neural network (or differentiable
program more generally) through multiple shooting (Bock & Plitt, 1984): the idea of subdividing
the optimization problem across several intervals whose solutions are then glued back as equality
constraints to form a global solution. The resulting approach brings us into the rarely explored
(Donti et al., 2021; Taylor et al., 2016) territory of constrained optimization for deep learning.
While the constrained perspective at the core of our multiple shooting approach has been leveraged
in Carreira-Perpinan & Wang (2014); Taylor et al. (2016); Gotmare et al. (2018) for the same Pur-
poses, the only types of solution methods explored so far have been unstable first-order Lagrangian-
type methods (Bertsekas, 2016, Section4.4) or unconstrained approximations (Choromanska et al.,
2019). A core contribution of this paper is to show that advances in GAN (Goodfellow et al., 2020)
training via the body of work on game-theoretic optimization (Korpelevich, 1976; Gidel et al., 2019;
Mertikopoulos et al., 2019) can be used to design stable first-order constrained solvers. Our exper-
iments show that our new solver is stable across a wide range of hyper-parameters, robust under
noise (stochastic optimization regime), and scalable to large datasets.
We also address a second important, but more subtle issue: the fact that a direct application of the
Lagrangian perspective necessarily hides a direct dependency on the size of the dataset. This issue,
rarely discussed in the literature, rules out the possibility of using mini-batch methods in a well-
defined way other than in the full batch regime. We pinpoint the exact mathematical origins of this
problem in section 3. We then propose to alleviate this problem by means of function approxima-
tion over the output variables. The resulting algorithm melds ideas from Target Propagation (TP)
(Le Cun, 1986) and constrained optimization to control both the number and desired accuracy of the
output variables.
1
Under review as a conference paper at ICLR 2022
2	Background
A deep feed-forward neural network (Goodfellow et al., 2016) can be conceptualized as a composi-
tion of non-linear functions of the form:
f(x;w) , (f (L) ◦...◦f(1))(x;w(1),...,w(L)) , f(L)(...f(1)(x;w(1))...;w(L)) ,
where x represents an “input”, f a “layer”, and {wi }iL are tunable “weights” and “biases”. Fur-
thermore, the learning problem is typically that of maximizing an objective of the form J(w) ,
E(χ,y)〜P [l(f (x； w), y))] where l is a “loss function" and X and y are random variables drawn
from a joint distribution P. Under the empirical risk minimization (ERM) framework (Bottou &
Vapnik, 1992), we can only optimize this objective via a sample average, leading us to the following
unconstrained optimization problem:
minimize
1	|D|
J(w),向 X'(f(χ⑶;w), y(i)),
given (x(i), y(i)) ∈ D ,
where D is a dataset of examples drawn i.i.d. from the joint. Rather than solving the above problem
in the unconstrained form, our work instead consider the following equivalent constrained formula-
tion:
1	|D|
minimize ∣ɪj∣ ^X '(h2, y⑻) where h(l)
fk(hl(-i)1; w) given h(0i) = xi, (xi, yi) ∈ D . (1)
In passing from the unconstrained version to the constrained one, we have introduced new opti-
mization variables hl(i), l ∈ (1, . . . , L) for each layer. In essence, we have lifted our problem to one
which now has parameters {w} ∪ {h(i)}(i,i)∈{i, ∣D∣}×{i, L}: that is |D| X L more variables than
the original problem. But in doing so, we have also gained two important properties: 1) the ability to
compute network outputs implicitly and in parallel without a separate forward propagation phase 2)
increased training stability over a long optimization horizon by “pinning down” the unstable modes
(Ascher et al., 1995; Biegler, 1984).
2.1	S olving Constrained Optimization Problems
The above constrained perspective calls for its dedicated optimization methods. Consider for a
moment a generic equality-constrained problem of the form: minimize f(x) subject to g(x) = 0.
By the first-order optimality condition (Bertsekas, 2016, 4.1.1) if x? is a regular local minimum
of f such that g(x?) = 0, then there must exist a unique Lagrange multiplier vector λ? such
that Df (x?) + λ? Dg(x?) = 0. In this form, We can therefore view the problem of solving a
constrained optimization program as a root-finding problem. In fact, the application of Newton’s
method to the root-finding problem DL(x, λ) = 0 where L(x, λ) , f(x) + λ>g(x) can be
seen Nocedal & Wright (1999) as an instance of the Sequential Quadratic Programming method.
While SQP has been widely adopted in the industry Biegler (1984), its reliance on second-order
information combined with the need to solve for inverse Hessian is problematic in the deep learning
context where the number of variables is many orders of magnitudes larger than what state-of-the-art
solvers are capable of handling. There are two main reasons for this: 1) the fact that many industrial
solvers have taken a CPU-centric approach leveraging sparse operations (which are of little use in
the dense compute model of GPUs) 2) that they often rely on finite-difference derivative estimation
methods where Jacobians are represented explicitly (in comparison with the matrix-free mindset in
deep learning).
These challenges bring about the need to consider a different class of approaches for constrained
optimization. Rather than starting from the above KaruSh-Kuhn-Tucker (KKT) condition, we note
that a necessary and sufficient condition for x? to be a minimum of f(x) subject to g(x) = 0 is that
there exists a Lagrange multiplier vector λ? which is a saddle point of the Lagrangian (Zangwill,
1969), ie: L(x?, λ) ≤ L(x?, λ?) ≤ L(x, λ?) for all x and λ. This leads us to a game-theoretic
formulation of our constrained problem as: minx maxλ f(x) + λ>g(x). As a first attempt, we
could try to solve this problem by gradient-descent-ascent (GDA) via the Arrow-Hurwicz-Uzawa
(Uzawa, 1958) algorithm:
x(k+1) = x(k) - αDxL(x(k), λ(k)) and λ(k+1) = λ(k) + αDλL(x(k), λ(k)) .
2
Under review as a conference paper at ICLR 2022
It can be shown that (Pschenichny & Danilin, 1975; Bertsekas, 2016) if Dx2 L(x(k), λ(k)) is pos-
itive definite, then [DxL(x(k) , λ(k)), g(x(k)] must be a descent direction of the penalty function
p(x, λ)，2∣∣DχL(x, λ)k2 + 1 kg(x)k2. However, the positive definiteness of DixL only holds
when sufficiently close to (x?, λ?) an may otherwise lead to non-descent directions and unstable
behaviour. Drawing the analogy to GANs, we could also consider a regularized version of the
Arrow-Hurwicz-Uzawa algorithm to stabilize training. This idea would then lead us to the class of
Augmented Lagrangian methods which try to endow the problem with a “locally convex” structure
(Bertsekas, 2016) by using an augmentedLagrangian Lc(x, λ)，f (x) + λ>g(x) + (c∕2)∣g(x)∣2,
where c should be chosen sufficiently small to ensure that Dx2Lc is positive definite.
In order the avoid the shortcomings of GDA-type methods, our work uses recent findings by Gidel
et al. (2019) showing that the Extragradient method of Korpelevich (1976) can be adapted to provide
stable learning algorithms for GANs. In the context of constrained optimization and the saddle-point
perspective, the ExtraGradient method is of the form:
x(k+1) = x(k)- αDχL(χ(k), λ(k)), λ(k+1) = λ(k) + αg(χ(k))
x(k+1) = x(k) - aDxL(x(k+ 2), λ(k+1)), λ(k+1) = λ(k) + αg(x(k+1)) .	(2)
As shown in our experiment, the combination of this algorithm the ExtraAdagrad step size adap-
tation strategy similar to Gidel et al. (2019) leads to increased stability and performance over the
Arrow-Hurwicz-Uzawa/GDA and Augmented Lagrangian alternatives. We refer to this application
of extragradient to the equality-constrained formulation or neural network training as ExtraProp.
3	Breaking the Dependence on the Dataset Size
fo(χ(i); W)
N (μo(y⑶;w), σ°(y(i); W))
'(y(i), y(i))
；ge()
I
〜(h(i^~>[fι(hf); W) p(y(i))→
(i)
Figure 1: Flow of computation in a two-layer network. The constraint function g ensures that h1 ,
is within a designed number of standard deviations from the mean ofa learned Gaussian distribution
assigned to the class y(i). Subsequent layers propagate samples from this distribution rather than
the transformed examples directly
The fact that the constrained formulation in equation 1 has a direct dependency on the number of ex-
amples in the dataset is easy to overlook. This stems from the fact that the formulation in equation 1
is inherently deterministic: the stochastic counterpart or Sample Average Approximation (SAA)
Rubinstein (1981) arising from the empirical risk minimization principle. Hence, the dataset D is
bound to the equality constrained program itself via the auxiliary variables {h1, . . . , hL }. More
precisely, given a dataset D We need |D| ∙ L ∙ H constraints, for a network of L layers and hidden
activations of uniform size H. This problem is further exacerbated by the need to also maintain La-
grange multipliers when solving programs using primal-dual methods, which doubles this number.
The solution that is put forward in this paper consists in decoupling the dataset itself with the auxil-
iary variables via function approximation. That is, instead of maintaining a unique auxiliary variable
and Lagrange multiplier pair per example, our algorithm tries to learn a mapping between a data
point and a corresponding learned prototype. However, in order to maintain diversity and avoid po-
tential collapses in the data representation, we add Gaussian noise around that prototype (the mean)
with a certain learned standard deviation. Hence, instead of maintaining |D| × L × H variables, we
are capable of reducing this number down to N × L × H where N is the desired number of synthetic
samples drawn from the distributions centred at each of the M prototypes where M is the number
of classes. In the context of ImageNet (Deng et al., 2009) for example where |D| = 14M, we now
3
Under review as a conference paper at ICLR 2022
only have to learn M = 1000 prototypes, and found that N < 8 and often N = 1 is sufficient to
achieve good performance.
More precisely, the training procedure goes as follows. For each input x(i) from the original dataset,
we compute the corresponding output through the first layer (or any other chosen split) f0 . Then,
rather than requiring that h(0i) - f0(x(i) ; w) = 0 as in the usual multiple-shooting formulation,
we require instead that the z-score fo(x(i); W) - μ(y(i); W(O))) /σ(y(i); w^) of that output with
respect to the corresponding learned Gaussian must be below a certain threshold. Here, μ and σ
are simple shallow models of the form μ(y; wμ)，w>y and σ(y; w°)，log(1 + exp(w>y))
assigning a given class to a corresponding mean and variance for the hidden states. Note that despite
enforcing the z-score criterion via |D| × H equality constraints, we are not introducing as many
intermediary variables as the size of the dataset. Given those functions μ and σ, We are then free to
generate as many samples as allowed by our computational budget: this is the key property allowing
a decoupling of the subsequent layers.
In a classical multiple-shooting formulation, we would then require that the the intermediary variable
at the second layer h(1i) satisfies h(1i) -f1(h(0i); w) = 0. However, since the very goal of our
approach is to avoid maintaining this variable h(1i) explicitly for each original datapoint, we use again
our z-score criterion, but this time evaluated over a smaller set of synthetic hidden states generated
by drawing N samples from h 0j) 〜N (μ(j; W(O)), σ(j; wg0))) for each class j = 1,.. .,M. These
N X M additional constraints are of then of the form (f (h0j); wo) 一 μ(i; w6ι)))∕σ(i; W(I)). The
same procedure (summarized in algorithm 1) is then repeated for the subsequent layers.
3.1 Algorithmic Considerations
For the procedure outlined above to be a well-defined mathematical program as well as for it to yield
well-behaved solutions, additional algorithmic considerations need to be taken into account.
Derandomization via Reparameterization The procedure outlined above is incompatible with
the static and deterministic structure of a usual equality constraint problem by the very fact that
it involves sampling synthetic activations at every layer. While a chance-constrained (Hof et al.,
1996) formulation may account for our computational model, we choose instead to generate a
fixed snapshot of the layer-specific datasets and account for the stochastic nature of the process
via reparametrization (Rezende et al., 2014). That is, we pre-generate a vector of Normal vari-
ates E 〜N(0,1) at initialization time and later, deterministically generate the hidden states as
h(i) = μ(y(i); WS)) + E Θ σ(y(i); w(l)). In that sense, given the original dataset D and generated
normal variates, the corresponding constrained program is fully determined and remains unchanged
through optimization.
Adaptive Step Sizes In order to further account for the stochasticity coming from sampling mini-
batches, we use the adaptive step size method, AdaGrad, adapted from Gidel et al. (2019) which
itself is an adaptation of Adam to the ExtraGradient case. This modification is crucial to ensure
robustness of ExtraGradient when passing to the stochastic and high dimensional setting, ExtraAda-
grad applies the AdaGrad (Duchi et al., 2011) algorithm to both extrapolation and optimization steps
as if both were usual classical optimization iterations.
To have a more compact
gl(i)(hl(i), w)
hl(i) 一 fl(hl(i); w),
hl(i) 一 fl (x(i); w),
notation let us define a general constraint
otherwise
l=0
where the only difference is the lack of
hidden state for the first constraint.
-constraints In practice, with very deep neural networks, noisy samples and imperfect represen-
tation, enforcing constraints to be exactly zero induces instability in the learning procedure, one
solution is to change the constraints to be insensitive in a range (一, ), this can be done by replac-
4
Under review as a conference paper at ICLR 2022
Algorithm 1 Constrained learning with dataset decoupling over two “layers” or blocks
initialize w0
initialize Wk 〜N(0,1) ∀k ∈ (0,..., K)
fork ∈ (0, ...,K) do
sample (x(i), y(i)) ∈ D uniformly at random.
Cei) J μ(y(i);Wk)-fo(x⑸;W)
g0	σ(y ⑸;Wk)
hi J μ(y(i); Wk) + Wkσ(y(i); Wk)	. dataset Projection;
y⑴ J fι(h0i), Wk)
Li(w, λ) J 'i(y⑴,y(i)) + λy(i)g(i)
h(0i) is a synthetic hidden state
wk+1	」Wk
2J
λk+ 2_|	[λk
Wk+1	Wk
λk+1	λk
_	VwLi(Wk, λk)
- αk [-VλLi(Wk, λk)_
VW Li(Wk+ 2 , λk+1 )
k+2 2 [-vλLi(Wk+ 2 ,λk+ 1 )
end for
ing the original constraint by an epsilon-insensitive version:
fg(h(i), θ)-e,	if g(h(i), θ) >e
g(hl(i),θ)= g(hl(i), θ) + ,	if g(hl(i), θ) < -	(3)
[θ,	otherwise
We transform our constraint in this manner using the hard shrinkage function.
Activation Functions
When using activation functions such as ReLU, we restrict the output domain of a layer, to improve
the optimization dynamics one should also include such transformation to the relative state variable
to lie in the same domain as the previous function of ft, for example, if the network output is bound
in Rn+ by non-linearities such as ReLu, the state should also be bound to Rn+ by re-parametrizing the
hl as hl = ReLU(hl) where hl behaves as a free parameter in the same way hl would.
Initialize to a valid solution
Initializing the intermediate states hl to, or close to a valid forward pass greatly increases stability
by avoiding unnecessary primal and dual updates, in the case of approximation by a distribution,
initializing the means to random points on the unit sphere and the standard deviation to one allowing
the initial class distributions to be distinguishable.
Sparse gradients
in the exact setting where we have as many constraints as the dataset, using sparse gradients avoids
calculating gradients with zero entries for the non-batch values, an important implementation detail
in word embedding methods for Natural Language processing tasks for example.
4	Related Works
An important consequence of this decoupling from the first property is also that we can now compute
layer outputs independently in parallel given the variables hl(i). The fact that we can now jointly
forward propagate (implicitly) and optimize at the same time is an idea known in the optimal control
literature under the the so-called multiple shooting (Bock & Plitt, 1984) methods, which are said to
be simultaneous methods (Biegler, 1984). In the jargon of numerical methods for the control of
differential equations (Ascher et al., 1995), this is as if we were performing simulation (solving
an initial value problem) implicitly rather than explicitly via time-stepping; simultaneously rather
than sequentially. A second benefit of the multiple shooting formulation is that of stability. We
can show precisely (Ascher et al., 1995, Theorem 4.45) that despite the introduction of additional
variables, numerical solutions to boundary value problems can be more stable if obtained by multiple
shooting with the right number and location of the breakpoints. Intuitively, the fact that we introduce
additional variables into our problem allows us to “pindown” the unstable modes of the system at
5
Under review as a conference paper at ICLR 2022
those breakpoints. Translated into deep learning terms, those breakpoints would help us deal with
exploding or vanishing gradients (Hochreiter, 1998; Bengio et al., 1994).
Difference Target Propagation (DTP) In Difference Target Propagation (Lee et al., 2015) defining
the proper inverse function adds additional complexity and requires careful fine-tuning, similarly
to DistProp, DTP injects noise in the training procedure, in our case, the generalization bonus is a
welcomed side effect while in DTP the magnitude of the injected Gaussian noise is not a learnable
parameter but rather it has to be defined by the designer.
Online Alternating Minimization (OAM) Choromanska et al. (2019) and similar methods like
Taylor et al. (2016), taking inspiration from the method Alternating Direction Method of Multi-
pliers (ADMM), (Boyd et al., 2011), also splits the optimization problem into smaller and simpler
sub-problems similarly to our method, but unlike OAM we solve both the learning and the gluing
problems simultaneously rather than sequentially, and can work on mini-batches rather than full-
batches.
Direct Feedback Alignment (DFA) N0kland (2016) does away with the classical gradient signal
and replaces it with predetermined random matrices to transport errors backward directly to each
layer of the network, although DFA performs well in simple tasks, and it is easily scaled to bigger
datasets such as ImageNet, the performance degrades quickly as the tasks complexity increases.
5	Experiments
In order to empirically investigate the advantage of the Extragradient applied to the Lagrangian per-
spective on neural network training as well as our distributional approximation, we use the network
architecture of (LeCun et al., 1999) composed of a variable number of convolutional layers each
followed by a pooling layer as feature extractors and a variable number of fully-connected layers as
classifier. We evaluate ExtraProp in MNIST with the AlexNet architecture, whic we split after the
convolutional layers; for DistProp, we remove the ReLU non-linearity. For the CIFAR-10, we use a
similar network with one extra convolution and one extra linear layer while the network structure for
ImageNet is again, AlexNet as described in Krizhevsky (2014). We benchmark the proposed meth-
ods against state-of-the-art methods from the literature on biologically plausible backprop (which
share the same layer-local ideals) as well as plain stochastic gradient descent (SGD).
For both OAM and DTP, we use a LeNet baseline structure, as reported in their respective papers
and official implementations, In the ImageNet experiments, due to the memory requirement, we
replaced OAM with Direct Feedback Alignment (DFA) N0kland (2016), while DFA is not as close
to our method as OAM, they are both alternatives to classical backpropagation and DFA can be used
on the scale of ImageNet, more details on the differences between our algorithm and the benchmark
methods refer to Section 4.
The MNIST model was trained for 6500 iterations, the LeNet-style CIFAR-10 model was trained
for 20000 iterations, on ImageNet, the models were trained for 50000 iterations, all models were
trained with AdaGrad (Duchi et al., 2011) for both ExtraProp and DistProp and their own op-
timizer for the baseline algorithms. For the ImageNet task, even just storing the multipliers (λ)
for the constraint (l = 1) block is unfeasible, to reduce the memory footprint we then reformu-
late the constraints making sure the feasible set of solutions is unchanged: g (x(i) , y(i) ; w) =
ReLu (|| f0(x( σwy(-μWy();W) || ι - E) So that the new multipliers for the first block are λ ∈ R1D1 rather
than ∈ RlDl×H.’
Impact of Extragradient We also investigate the effectiveness of ExtraGradient as optimizer, in fig-
ure 3 GDA (Gradient Descent Ascent) refers to an alternative of ExtraProp where the extrapolation
step is avoided, and Regularization refers to Gradient Descent, where the constraint is considered
a regularization term based on the defect, we omit the scheduling of the penalty as convergence is
never reached. Extrapolation greatly helps to stabilize the optimization dynamics, an analysis and
plots of the constraint defect over the optimization process can be found in Fig. 2
Augmented Lagrangian It is often useful to augment the Lagrangian objective to smooth the opti-
mization landscape around the feasible zone, in our experiments this augmentation did not notice-
6
Under review as a conference paper at ICLR 2022
ably help, we hypothesize that the dual problem is already smooth enough, this is also reflected in
the low sensitivity to constraint related hyperparameters, as shown in section 5.1.
Figure 2: Log-scale mean absolute defect and it's standard deviation across 20 runs Vs gradient
steps on the MNIST dataset, GDA starts with zero defect but the optimization dynamics are often
eventually destabilized stopping any progress in terms of objective function.
5.1	Robustness to hyper-parameters
We investigate the effect of hyperparameter selection on the optimality of solutions, ideally, We
would like to have a wide range of stable hyperparameters configurations. We ran 170 experiments
with different parameters, 103 (shown) achieve top 10% performance, the experiments were run
using a Bayesian hyperparameter search method uses based on a Gaussian Process2 with a config-
uration space as reported in Fig. 4. The results show that the hyperparameters related to the lifting
variables in DistProp are robust to a wide range of values.
6	CONCLUSIONS
We proposed a novel online method to train neural networks, this work expands the target propaga-
tion class of algorithms allowing to divide the problem into more manageable sub-problems linked
by explicit constraints, we show that the cost of adding auxiliary variables can be removed by re-
formulating the problem to an approximated form that breaks free of dependence on the dataset, so
doing we remove the need to propagate the dataset in its entirety across the network. We show em-
pirical accuracy comparable and superior to standard stochastic gradient methods in simple settings,
and improve on state of the art algorithms in target propagation in terms of performance, stability
and scalability. Furthermore, we show that given a few small modifications, first-order constrained
optimization methods such as the Lagrangian method can be well behaved in modern tasks like
ImageNet.
2https://docs.wandb.ai/guides/sweeps/Configuration#method
7
Under review as a conference paper at ICLR 2022
(a) Log-scale Test Loss
Figure 3: Here we show mean and standard deviation of the of generalization scores across 10
random seeds, OAM and DTP are shown for reference. On MNIST both ExtraGradient based meth-
ods (ExtraProp and DistProp) have stable optimization dynamics and good performance, GDA lacks
both performance and stability, while a regularization approach suffers from high variance.the Dist-
Prop formulation achieves a lower generalization loss, possibly due optimizing a wider support
rather than just the training distribution.
(b) Log-scale Test Error
Table 1: Experimental results
Dataset	Algorithm	Test Accuracy	Final Defect
Mnist	SGD	0.9881	0
	Regularized	0.9627	0.1
	GDA	0.9743	0.00072
	ExtraProp	0.9827	0.0000045
	DistProp	0.9903	0.0026
CIFAR10	SGD	0.6309	0.
	Regularized	0.1726	0.
	GDA	0.1611	0.0008598
	ExtraProp	0.5833	0.5833
	DistProp	0.6540	0.07447 1
ImageNet TOP-1	DTP	0.0166	-
	DFA	0.0692	-
	DistProp	0.1110	0.
ImageNet TOP-5	DTP	0.0544	-
	DFA	0.1746	-
	DistProp	0.3059	0.
7 Future Work
Without the dependency on the dataset, and having alleviated the problem of vanishing gradients it
would be interesting to understand the limits in terms of network depth and advantage with respect
of residual networks (He et al., 2016), initial experiments showed promising results for up to 100
layers but in-dept analysis of such settings is beyond the scope of this work.
8
Under review as a conference paper at ICLR 2022
Hyperparameter (Column Name) Lower Bound Upper Bound
Constraint Margin, e (Constjmargin)	0.1
Weights Learning rate (initiaLlJtheta)	0.00009
Wμ, Wσ Learning rate (initial」r_x)	0.2273
λ Learning rate (initiaLljy)	0.0000001
0.64
0.0056
3.282
0.0158
Figure 4: Robustness to hyper-parameters, for DistProp on MNIST. EaCh line represents an exper-
iment with values defined by the point of interseCtion with the parameter axis, shown is the range
of hyper-parameters for whiCh performanCe does not drop more than 10%, i.e. where test aCCuraCy
is above 89%. The table reports a possible interpretation of the ranges for eaCh hyperparameters
for whiCh the performanCe does not degrade more than 10% from the best performer, due to the
Complex interaCtion between hyperparameters, these values should be taken as an heuristiC at best.
7.1	Reproducibility
An implementation of this paper is available on GitHub3, it Contains all the Code neCessary to repro-
duCe the methods in this paper, to ensure exaCt reproduCibility over the years of experiments we also
traCked the exaCt Code version and hyperparameters4
References
Uri M. AsCher, Robert M. M. Mattheij, and Robert D. Russell. Numerical Solution of Boundary
Value Problems for Ordinary Differential Equations. SoCiety for Industrial and Applied Math-
ematiCs, 1995. doi: 10.1137/1.9781611971231. URL https://epubs.siam.org/doi/
abs/10.1137/1.9781611971231.
Y. Bengio, PatriCe Simard, and Paolo FrasConi. Learning long-term dependenCies with gradient
desCent is diffiCult. IEEE transactions on neural networks / a publication of the IEEE Neural
Networks Council, 5:157-66, 02 1994. doi: 10.1109/72.279181.
Yoshua Bengio, Renato De Mori, Giovanni Flammia, and Ralf Kompe. Global optimization of
a neural network-hidden markov model hybrid. IEEE transactions on Neural Networks, 3(2):
252-259, 1992.
Dimitri Bertsekas. Nonlinear programming. Athena SCientifiC, Belmont, MassaChusetts, 2016.
ISBN 978-1886529052.
3https://anonymous.4open.science/r/constrained_nn-8013/
4https://github.com/ministry-of-silly-code/experiment_buddy/
9
Under review as a conference paper at ICLR 2022
Lorenz T. Biegler. Solution of dynamic optimization problems by successive quadratic programming
and orthogonal collocation. Computers & Chemical Engineering, 8:243-247, 1984.
Hans Georg Bock and Karl-Josef Plitt. A multiple shooting algorithm for direct solution of optimal
control problems. IFAC Proceedings Volumes, 17(2):1603-1608, 1984.
Leon Bottou and Vladimir NaUmovich Vapnik. Local learning algorithms. Neural Computation, 4:
888-900, 1992.
Stephen Boyd, Neal Parikh, and Eric Chu. Distributed optimization and statistical learning via the
alternating direction method of multipliers. Now Publishers Inc, 2011.
A.E. Bryson. Optimal control-1950 to 1985. IEEE Control Systems Magazine, 16(3):26-33, 1996.
doi: 10.1109/37.506395.
Miguel Carreira-PerPinan and Weiran Wang. Distributed optimization of deeply nested systems. In
Journal of Machine Learning Research, volume 33, pp. 10-19, 2014. URL http://eecs.
ucmerced.edu.
Anna Choromanska, Benjamin Cowen, Sadhana Kumaravel, Ronny Luss, Mattia Rigotti, Irina Rish,
Paolo Diachille, Viatcheslav Gurev, Brian Kingsbury, Ravi Tejwani, et al. Beyond backprop:
Online alternating minimization with auxiliary variables. In International Conference on Machine
Learning, pp. 1193-1202. PMLR, 2019.
Francis Crick. The recent excitement about neural networks. Nature, 337(6203):129-132, 1989.
Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hi-
erarchical image database. In 2009 IEEE conference on computer vision and pattern recognition,
pp. 248-255. Ieee, 2009.
Priya L Donti, David Rolnick, and J Zico Kolter. Dc3: A learning method for optimization with
hard constraints. arXiv preprint arXiv:2104.12225, 2021.
John C. Duchi, Elad Hazan, and Yoram Singer. Adaptive subgradient methods for online learning
and stochastic optimization. In J. Mach. Learn. Res., 2011.
Gauthier Gidel, Hugo Berard, Pascal Vincent, and Simon Lacoste-Julien. A variational inequality
perspective on generative adversarial nets. In ICLR, 2019. (to appear).
Ian Goodfellow, Yoshua Bengio, Aaron Courville, and Yoshua Bengio. Deep learning, volume 1.
MIT Press, 2016.
Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair,
Aaron Courville, and Yoshua Bengio. Generative adversarial networks. Commun. ACM, 63(11):
139-144, October 2020. ISSN 0001-0782. doi: 10.1145/3422622. URL https://doi.org/
10.1145/3422622.
Akhilesh Gotmare, Valentin Thomas, Johanni Brea, and Martin Jaggi. Decoupling backpropagation
using constrained optimization methods. In Iclr, 2018.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recog-
nition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp.
770-778, 2016.
Sepp Hochreiter. The vanishing gradient problem during learning recurrent neural nets and problem
solutions. Int. J. Uncertain. Fuzziness Knowl. Based Syst., 6:107-116, 1998.
Sepp Hochreiter and JUrgen Schmidhuber. Long short-term memory. Neural computation, 9(8):
1735-1780, 1997.
John Hof, Michael Bevers, and James Pickens. Chance-constrained optimization with spatially
autocorrelated forest yields. Forest Science, 42(1):118-123, 1996.
GM Korpelevich. The extragradient method for finding saddle points and other problems.
Ekonomika i matematicheskie metody, 12:2010-2010, 1976.
10
Under review as a conference paper at ICLR 2022
Alex Krizhevsky. One weird trick for parallelizing convolutional neural networks.	ArXiv,
abs/1404.5997, 2014.
Yann Le Cun. Learning process in an asymmetric threshold network. In E. Bienenstock, F. Fogelman
Soulie, and G. WeisbUch (eds.), Disordered Systems and Biological Organization, pp. 233-240,
Berlin, Heidelberg, 1986. Springer Berlin Heidelberg. ISBN 978-3-642-82657-3.
Yann LeCun, D Touresky, G Hinton, and T Sejnowski. A theoretical framework for back-
propagation. In Proceedings of the 1988 connectionist models summer school, volume 1, pp.
21-28, 1988.
Yann LeCun, Patrick Haffner, Leon Bottou, and Yoshua Bengio. Object recognition with gradient-
based learning. In Shape, contour and grouping in computer vision, pp. 319-345. Springer, 1999.
Dong-Hyun Lee, Saizheng Zhang, Asja Fischer, and Yoshua Bengio. Difference target propagation.
In Joint european conference on machine learning and knowledge discovery in databases, pp.
498-515. Springer, 2015.
Panayotis Mertikopoulos, Bruno Lecouat, Houssam Zenati, Chuan-Sheng Foo, Vijay Chan-
drasekhar, and Georgios Piliouras. Optimistic mirror descent in saddle-point problems: Go-
ing the extra (gradient) mile. In 7th International Conference on Learning Representations,
ICLR 2019, New Orleans, LA, USA, May 6-9, 2019. OpenReview.net, 2019. URL https:
//openreview.net/forum?id=Bkg8jjC9KQ.
Jorge Nocedal and Stephen J. Wright. Numerical Optimization. Springer, New York, NY, USA,
1999.
Arild N0kland. Direct feedback alignment provides learning in deep neural networks. In NIPS,
2016.
B. N. Pschenichny and Y.M. Danilin. Numerical Methods in Extremal Problems. MIR, 1975.
Danilo Jimenez Rezende, Shakir Mohamed, and Daan Wierstra. Stochastic backpropagation and
approximate inference in deep generative models. In ICML, 2014.
Reuven Y. Rubinstein. Simulation and the Monte Carlo Method. John Wiley & Sons, Inc., New
York, NY, USA, 1st edition, 1981. ISBN 0471089176.
David E Rumelhart, Geoffrey E Hinton, and Ronald J Williams. Learning representations by back-
propagating errors. nature, 323(6088):533-536, 1986.
Gavin Taylor, Ryan Burmeister, Zheng Xu, Bharat Singh, Ankit Patel, and Tom Goldstein. Training
neural networks without gradients: A scalable admm approach. In International conference on
machine learning, pp. 2722-2731. PMLR, 2016.
H. Uzawa. Studies in linear and nonlinear programming. Stanford University Press, 1958.
W.I. Zangwill. Nonlinear Programming: A Unified Approach. Prentice-Hall international series in
management. Prentice-Hall, 1969. ISBN 9780136235798.
11