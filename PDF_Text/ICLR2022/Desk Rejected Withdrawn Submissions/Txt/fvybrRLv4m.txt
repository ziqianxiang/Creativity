Under review as a conference paper at ICLR 2022
Dictionary Learning Under Generative Coef-
ficient Priors, with Applications to Compres-
SION
Anonymous authors
Paper under double-blind review
Abstract
There is a rich literature on recovering data from limited measurements under
the assumption of sparsity in some basis, whether known (compressed sensing)
or unknown (dictionary learning). In particular, classical dictionary learning as-
sumes the given dataset is well-described by sparse combinations of an unknown
basis set. However, this assumption is of limited validity on real-world data. Re-
cent work spanning theory and computational science has sought to replace the
canonical sparsity assumption with more complex data priors, demonstrating how
to incorporate pretrained generative models into frameworks such as compressed
sensing and phase retrieval. Typically, the dimensionality of the input space of
the generative model is much smaller than that of the output space, paralleling
the “low description complexity,” or compressibility, of sparse vectors. In this
paper, we study dictionary learning under this kind of known generative prior on
the coefficients, which may capture non-trivial low-dimensional structure in the
coefficients. This is a distributional learning approach to compression, in which
we learn a suitable dictionary given access to a small dataset of training instances
and a specified generative model for the coefficients. Equivalently, it may be
viewed as transfer learning for generative models, in which we learn a new linear
layer (the dictionary) to fine-tune a pretrained generative model (the coefficient
prior) on a new dataset. We give, to our knowledge, the first provable algorithm
for recovering the unknown dictionary given a suitable initialization. Finally, we
compare our approach to traditional dictionary learning algorithms on synthetic
compression and denoising tasks, demonstrating empirically the advantages of in-
corporating finer-grained structure than sparsity.
1	Introduction
Sparsity is ubiquitous throughout data science as a tractable formalization of the structure of nat-
urally occurring signals. Early work in signal processing involved defining specific bases, such as
Daubechies wavelets, of which natural images are well-approximated by sparse linear combinations,
as well as other domain-specific bases such as Curvelets and ridgelets (DaUbechies ,1988; Candes &
DemaneL 2002.. Candes, 2003). These innovations alone revolutionized signal processing, yielding
such recognizable technologies as JPEG image compression. While such basis transform methods
for compression required a linear number of measurements, i.e. using all n pixels ofan image, a rev-
olution came with the landmark works of Donoho and Candes, Romberg, and Tao, establishing the
field of compressed sensing (Donoho, 2006; Candes et al., 2006). In particular, they demonstrated
that if an n-dimensional signal is k-sparse in a known basis that satisfies the restricted isometry
property (e.g. from i.i.d. Gaussian or Fourier matrices), its sparse coefficients can be efficiently
recovered using only O(k log n) linear measurements. Letting A ∈ Rm×n with m << n denote the
measurement matrix and x ∈ Rn the unknown signal, one measures y = Ax ∈ Rm ; the signal x is
recovered by minimizing ||x||1 subject to the m measurements. This innovation has inspired many
offshoots and applications, notably providing a substantial speedup to MRI (Lustig et al., 2007).
In the previous settings, the linear measurements were either randomized, as in the case of com-
pressed sensing, or deterministic, as in the case of wavelet bases, but they were always fully known.
In contrast, dictionary learning — alternately known as sparse coding — posits that the signal is
1
Under review as a conference paper at ICLR 2022
sparse in an unknown basis, which may be specific to the task or dataset at hand. Thus, one must
learn the matrix A, whose columns are sometimes referred to as atoms, simultaneously to learning
the sparse coefficients x. This would be impossible given only a single measurement vector y, but
can be done given multiple measurements of the form yj = Axj . Dictionary learning has proven
useful in a variety of applications, including denoising (Elad & Aharon, 2006), facial recognition
(XU et al., 2017), feature extraction (Ceylan, 2018), and medical image processing (Zhao et al.,
2021; Li et al., 2012). In this work, We are particularly interested in applications to compression and
denoising, which we emphasize are applications that make sense even when the dictionary is not
uniquely defined. Practical algorithms for solving dictionary learning include the Method of Opti-
mal Directions (Engan et al., 1999) and K-SVD (Aharon et al., 20O6). Under certain randomized
generative models on the sparse coefficients x, alternating minimization algorithms similar to MOD
have been shown to run in polynomial time and provide provable guarantees on the output solution
to the sparse coding problem (ArOra et al., 2014; Agarwal et al., 2014; Arora et al., 20i5).
However, it is important to note that sparsity is only an approximation to the structure of real-
world signals. Images are not truly sparse in a particular dictionary; sparsity simply provides a
tractable model that has been useful in applications. In fact, recent empirical results demonstrate
that sparsity is too coarse an assumption in practice, as deep neural networks can provide a far
better approximation to real-world signals. State-of-the-art audio and image processing — including
denoising, featurization, classification, inpainting, and beyond - is now almost universally achieved
by deep learning methods. For example, the top accuracies of > 99.8% on the Labeled Faces in the
Wild Database (Huang et al., 2007), once a testbed for dictionary learning and hand-featurization
methods, are now attained entirely by deep nets trained using outside data. The advent of generative
adversarial networks (GANs), variational autoencoders (VAEs), and other such schemes have led to
generative priors capable of generating photorealistic, yet fake, images (Karras et al., 2017), which
far surpass the reconstruction quality that results from simply assuming sparsity in a particular basis.
Correspondingly, there has been a renaissance in solving inverse problems via learned generative
priors. Such works have spanned both scientific and vision domains, including phase retrieval (Hand
etal., 2018), image inpainting (Yeh et al., 2017), super-resolution (Ledig et al., 2016), and denoising
(Lempitsky et al., 2018). Such works demonstrate that the shared representation space of deep
generative models is more powerful on real-world data than sparsity.
From a theoretical perspective, Bora et al. (2017) introduced a corresponding theory of compressed
sensing (a particular inverse problem) under generative models, in which the sparse signal x to be
recovered is instead constrained only to lie in the range of a generative model, x = G(z). They
show that simple empirical risk minimization suffices to recover x with sample complexity m on the
order of k = dim(z), comparably to the canonical k-sparse case.
Regularizing reconstruction problems via generative priors, instead of traditional sparsity priors, has
thus been a productive endeavor at the interface of signal processing and machine learning. In this
paper, we naturally extend this line of work to dictionary learning by considering generative priors
for the coefficients x = G(z), where z lives in a small, k-dimensional input space and parallels
the low description complexity of k-sparsity. Here, the naturally occurring data is thus modeled by
A*G(z) and not G(Z) itself (a departure from the interpretation of Bora et al. (2017)). Particular
constructions for G can capture k-sparsity (Kamath et al., 2019), recovering ordinary dictionary
learning, but introducing a generative model for coefficients allows us to model strictly more com-
plex statistical relationships among the features as well.
To understand this concretely, consider the following thought experiment. Recall that ordinary dic-
tionary learning provides a model for how structure is embedded in commonly observed signals,
via sparse combinations of a common pool of atoms. However, the dependencies between these
individual features is important for providing a finer-grained model of structure. For example, an
atom representing the leg of an item of furniture is useful on its own, but in practice we expect it
to co-occur only in certain combinations with other atoms, such as the seat of a chair or the surface
of a table. One can iterate this reasoning further, e.g. chairs themselves may only appear together
with tables or desks. If the data y is made up of such atomic building blocks, which enjoy complex
hierarchical structure in their co-occurrence patterns, then modeling y = AG(z) is natural. This is
not just hypothetical: there is precedent across multiple domains for incorporating more complex
structures than sparsity in dictionary learning. For example, multi-layer convolutional sparse cod-
ing assumes that the dictionary coefficients are themselves representable as a sparse combination
of separate dictionary elements, and so on, thus forming a hierarchical linear model of composed
2
Under review as a conference paper at ICLR 2022
dictionaries (PaPyan et al., 2017; Sulam et al.., 2018). In addition, Li et al. (2012) proposes a model
of “group sparsity” for medical image denoising, in which certain groups of atoms are more likely
to co-occur together, according to predefined overlapping subsets. As We show in Section 6, this
structure can be captured by a simple one-layer generative model. In fact. the ability of deep nets
to capture hierarchical structure is thought to be a key driver of their success, particularly when
learning common structure that generalizes across label classes. Thus, incorporation of a generative
coefficient prior G(z) generalizes sparsity, group sparsity, and multi-layer dictionary learning, while
also drawing on the myriad successes of deep learning in capturing complex structure in data.
Crucially, we assume throughout that this coefficient prior G (e.g. the group identities) is known. (If
G(z) itself were unknown as well, this problem would reduce to the quite general one of learning a
complete nonlinear generative model for a dataset from scratch. This is itself an open problem for
each type of generative model, see e.g. Kodali et al. (2018).) Access to such a G can occur through
transfer learning: one might have a large dataset D’, for which an unsupervised method can be used
to learn a generative model M(z), as well as a smaller but related dataset D. If M(z)=F(G(z)) is
a deep net, one can remove the final few layers to obtain a reasonable G(z) for use as a coefficient
prior with D. We explore this setting in the experiments in Section 6.
1.1	Our contributions
In this paper, we formulate a new version of dictionary learning that incorporates non-trivial de-
pendencies in the coefficients via a generative prior. We give an intuitive alternating minimization
algorithm (Section 4) for recovering the unknown dictionary, with provable convergence guarantees
under variants of standard assumptions in the literature (Section 3). The strongest of these assump-
tions is a decoding optimization oracle, which is also needed by previous work on compressed
sensing under generative priors (Bora et al., 2017). However, to validate this assumption, we extend
the tools of Hand & Voroninski (2018) to our more challenging case, where A* is unknown. In
particular, we show that the optimization landscape is favorable (in the sense that there is always a
direction of descent) at every iteration of our algorithm in a particular idealized setting, without noise
and when the generative model is an expansive Gaussian ReLU network (Section 5). Finally, we
adapt our algorithm into a PyTorch implementation and demonstrate that it outperforms classic dic-
tionary learning algorithms on denoising tasks (Section 6). We obtain more accurate reconstructions
(using only the k free parameters of the generative model, as compared to k nonzero coefficients)
from noisy data, validating our choice of both model and algorithm. More generally, we hope the
flexible hybrid of dictionary learning and deep generative priors proposed here will open the door
to the application of dictionary learning techniques to a far broader scope of applications, thereby
bringing the deep learning revolution to dictionary learning.
Notation Given a matrix M, let (M)i denote the ith column of M. For a vector u, let ui denote
its ith entry. In contrast, we will use superscripts to denote sample indices, i.e. yj = A*G(zj)+ηj
is the jth vector sample. Let Bp(u, r)={x : ||x - u||p ≤ r}. Similarly, let Bp(M, r) denote
all matrices whose ith columns lie in Bp((M)i, r). Let Supp(μ) denote the support of a probability
distribution μ. We say that f (x) = O(g(x)) if limχ→∞ f(x) ≤ C for some constant C, and
f (x) = ω(g(x)) if limχ→∞ 瑞 diverges to infinity. Let uθn denote a vector,s nth element-wise
power, i.e. (uθn)i = (Ui)n. LetkM∣∣ or ∣∣M∣∣2 denote the spectral norm of the matrix M.
2	Related work
Dictionary learning. Dictionary learning was first studied in the neuroscience community in the
seminal work of Olshausen & Field (1997). Efficient algorithms were proposed in the following
decade, including the Method of Optimal Directions (Engan et al., 1999) and K-SVD (Aharon et al.,
2006). Since then it has found important applications in many domains, e.g. medical imaging (see
(Zhao et al., 2021) and references therein). Supervised variants of dictionary learning have been
proposed (Mairal et al., 2009b), and provable algorithms under different distributional, dimension,
and sparsity assumptions were given (Chatterji & Bartlett, 2017; Spielman et al., 2012). Closest to
the alternating minimization algorithm we propose is the work of Arora et al. (2015), who first gave
strong convergence guarantees for a neurally plausible instantiation of alternating minimization and
3
Under review as a conference paper at ICLR 2022
whose convergence analysis loosely inspired ours. Hong et al. (2018) studied a Union-of-subspaces
model, and Li et al. (2012) proposed a related group sparsity structure, both of which are in the
same spirit of generalized structural dependencies among coefficients. The compositional coeffi-
cient model of multi-layer convolutional sparse coding is also similar to our work: the signal model
y = A； Ag ...A； Z provides a non-trivial compositional model for the dictionary coefficients, and
the forward pass of a convolutional neural network is shown to essentially decode the current con-
volutional dictionaries (Papyan et al., 2017; Sulam et al., 2018). However, their coefficient prior is
restricted to linear models A； ...A；, whereas we consider an arbitrary Lipschitz non-linear G(z).
Moreover, these works do not assume any of the composed dictionaries are known, whereas we are
more motivated by transfer learning and assume access to G.
Compressed sensing with additional structure. Several preceding works studied compressed
sensing under more exotic models than generic sparsity, such as graph-structured sparsity (Hegde
et al., 2015), but Bora et al. (2017) initiated the study of compressed sensing under arbitrary gener-
ative models. They showed that a suitable optimization oracle for minimizing reconstruction error
could recover a good estimate of the unknown non-sparse coefficients, with the required number of
measurements scaling with the input dimension of the generative model and the number of layers
in the network. A related paper by Hand and Voroninski then gave an efficient implementation of
the optimization oracle in an interesting class of models (Hand & Voroninski, 2018), and later used
very similar tools to analyze phase retrieval under a generative prior (Hand et al., 2018).
Generative priors for inverse problems. On the practical side, recent works have harnessed both
trained and untrained generative priors in inverse problems such as phase retrieval, image inpaint-
ing, and denoising. Pretrained generative models are obtained from existing datasets (e.g. that of
Tramel et al. (2016)), while untrained generative models are generally more expressive and confer
an inductive bias towards natural data by virtue of their architecture alone (e.g. the deep image prior
(Lempitsky et al., 2018), and the deep decoder (Heckel & Hand, 2019; Bostan et al., 2020.. Lawrence * 3
et al., 2021)). In contrast to this paper, the forward model is fully specified in all of these works.
Learning distributional transformations. From a distributional perspective, our problem fits into
the broad class of problems of trying to learn a linear transformation A of a known distribution px
(given by the push-forward of Pz through G). Arora et al. (2012) gives a moment-based algorithm
for recovering such an unknown linear transformation A. However, this work only applies when
the original distribution px satisfies stringent coordinate-wise independence conditions, whereas the
main utility of our model is in allowing structured dependencies across coordinates.
3 Problem formulation
Let A； ∈ Rm×n denote the unknown dictionary, which we assume has unit-normed columns, and
let G bea generative model with G : Rk → Rn. Measurements are of the form yj = AgG(zj) + ηj,
where each zj is i.i.d. according to a fixed distribution pz and η is entry-wise i.i.d. zero-mean
noise with ∣∣η∣∣ ≤ N. Given vector samples {y1,..., ym} and access to G, our task is to recover
a good approximation of A； . Note that in our complete problem setup, including the S-REC and
optimization oracle defined below, a good estimate of A； enables good approximations of each zj
(see Theorem A.2), so this is a reasonable metric. We make the following additional assumptions:
Assumption 1 (Bounded-norm outputs.) The norms of the outputs of the generative model G are
upper-bounded by a constant R: ||G(z)||2 ≤ R ∀z ∈ Supp(pz). A simple way of achieving this is
to assume G is L-Lipschitz, and that Supp(pz) ⊆ B2(0, R) for some values R and L. Bora et al.
(2017) similarly require a Lipschitz network, although the easier nature of the problem allows for a
weaker dependence on log L and for ||z||2 to be bounded only with high probability.
Assumption 2 (Set-restricted eigenvalue condition (S-REC).) The S-REC was defined in Bora
et al. (2017) as an analogue of the restricted isometry property from classical compressed sensing.
It ensures that one can detect, based on the measurements yi, whether the corresponding generative
model outputs were similar. Without such a condition, one could not hope to recover the G(zi) based
on only A；G(zi), and thus could not take advantage of G. Since our problem generalizes that of
Bora etal. (2017), we must also assume the S-REC: || A； (G(z1) - G(z2)∣∣ ≥ γ∣∣G(z1) - G(z2)∣∣ -
δ ∀z1,z2 ∈ S. In particular, we set S = B2(0, R), because we have assumed Supp(pz) ⊆ B2(0, R).
4
Under review as a conference paper at ICLR 2022
Assumption 3 (Initialization.) We require access to an initial guess A0 of A* whose columns
are ∆-close in '2 norm to those of A*, i.e. ∣∣(A0)i - (A*)i|| ≤ ∆ ∀i. We only need that ∆ < 1,
although our ultimate guarantees have some dependence on ∆. An initialization assumption is
common in the dictionary learning literature, as the problem geometry becomes more favorable
(convex) close to the true solution; for instance, Arora et al.(2015)requires ∆ = ɪɪ-, and Schnass
(2015) tolerates comparatively higher sparsities (and thus richer signals, much like how G also
provides a richer signal model) at the expense of giving only a local analysis. In practice, random
initializations of A0 still performed well in our experiments; see Section 6. However, note that A*
is not necessarily identifiable, even up to permutation. This is dependent on the inherent symmetries
of G(z); for example, if G(z) is distributed as BG(z) for a matrix B, then one can never hope to
distinguish between A* and A*B. Thus, this assumption theoretically breaks any symmetries by
starting iterations sufficiently close to one viable A*, i.e. such that y 〜A*G(z).
Assumption 4 (Optimization oracle.) Given any y and any A that is as close to A* as the
initialization, ||(A)i - (A*)i|| ≤ ∆ ∀i, We can compute Z in B2(0, R) (i.e. over the domain of G)
s.t. ||y - AG(z)∣∣ ≤ min∣⑶∣≤R ||y - AG(Z)|| + θ. Note that when A = A*, this is exactly the
optimization oracle required in Bora et al. (2017). In Theorem 5.3, We validate the existence of the
optimization oracle in certain cases by showing that ||y - AG(Z)|| always has a descent direction.
Assumption 5 (Outer product estimate.) Let C * = Ez*^pa [G(z*)G(z*)T ]. We require that C *
is invertible, and that we have access to an estimate C-1 such that ||C-1 - (C*)-1||2 ≤ ν. Despite
the choice of notation, C -1 does not need to satisfy any properties (e.g. invertibility) besides the
stated one. A simple estimation procedure for obtaining C -1 is simply to invert an estimate for C*.
Assumption 6 (Interplay of parameters.) Finally, our theoretical analysis requires the following
technical condition governing the previously introduced parameters: (v||C *|| + 10n||C -1|| R2) ≤
1. This is a technical condition that simplifies the analysis, but can likely be weakened with addi-
tional effort, as suggested by the success of our experiments under diverse generative models.
4Our algorithm
Algorithm 1: Basic alternating minimization
Inputs: Initialization A0 ∈ B2(A*, ∆) and TP i.i.d. samples y0,..., yTP-1, S = 1,...,T;
for s =0, . . . , T - 1 do
Consider P fresh samples (y0,...,yp-1) = (ysP,... ,y(s+1)P-1)
Decode: Using the optimization oracle, compute z0,..., ZP-1 ∈ B2(0, R) such that
||yj - AsG(Zj)|| ≤ minj|yj-AsG(Z*)|| + θ ∀j=0,...,P - 1
ι∣z*ιι≤ L
Update: As+1 = PrOjBAs - η^s, where gs = + PP-I(AsG(Zp) - yp)G(Zp)TC-1 is a
finite-sample estimate for gs = Ez*,η[(AsG(z) - y)G(ι)τ] ∙ C-1 (in which Z is a
function of the random variables z* and η and the constant As), and B = B2(A0, ∆).
end
Return: AT, an estimate for A*
To recover A* given samples y1 ,. ..,ym , one would ideally like to minimize reconstruction over
the latent variables Z1,...,Zm and A* at once by solving minA*,z1,...,z Pm ||A*G(Zi) - yi||22.
However, the reconstruction objective is non-convex, even for ordinary sparse coding (where G
outputs sparse vectors). As is common in MOD algorithms for dictionary learning, we adopt an
alternating minimization procedure which alternates between “decoding” a good choice for each Zi
based on the current estimate A of A*, and “updating” the estimate A to better match these Zi. As
in Arora et al. (2015), we take a single step in the direction of the gradient of A at each update step,
and draw new i.i.d. measurement samples at each iteration for ease of analysis.
The algorithm departs from precedent in pre-conditioning by C -1 , the estimate of
E[G(Z*)G(Z*)]-1, which we can motivate as follows. Intuitively, observe that Ez*,η [As G(Z) -
A*G(Z*)G(Z*)T] ≈ (As - A*)E[G(Z*)G(Z*)T], when Z is close enough to Z* . Furthermore,
5
Under review as a conference paper at ICLR 2022
-(As - A*) is exactly the direction from the current iterate, As * *,to the correct dictionary, A*. Thus,
the role of CT is to approximately invert the E[G(z* )G(z* )T] term, resulting in update steps which
make progress by stepping towards the correct answer, A*.
5 Theoretical results
Under the assumptions of Section 3, We can bound the convergence of Algorithm 1 to the correct
A*. First, for a cleaner statement, Theorem 5.1 provides our result in an idealized noiseless setting:
Theorem 5.1 (Geometric convergence under best circumstances) Let Assumptions 1-6 of [Sec-
tion 3 hold With N = θ = δ = 0 and ||C-1|| = L This corresponds to a noiseless setting,
with a perfect optimization oracle, zero-valued S-REC offset parameter δ, and unit-normed estimate
C T. Then Algorithm 1 with η = ɪ yields a solution A with ||(A)i — (A*)i ∣∣2 ≤ e in O (log (∆2))
iterations, with O(npoly (IOg(R),logQ),log(I/C))) fresh samples required per iteration.
Remark 1 Note that, in contrast to traditional dictionary learning guarantees, there is no explicit
dependence in the theorem on k, the underlying dimensionality of the coefficients. This is because it
implicitly plays a role in determining the S-REC constants Y and δ; see Bora et al. (2017).
More generally, we show the following rate of convergence, which is geometric with a bias:
Theorem 5.2 (General convergence rate) Let Assumptions 1-6 hold, let Z = θ + N, η = ɪ, and
assume P = n2L log2 n fresh samples per iteration. Then the iterates of Algorithm 1 satisfy:
E[||At - A*|『]≤ (24)t||A0 - A*|『+ (4O0n2|1C-11即 + "R2 + SR + Z)R1|C-1||)
25	γ	n
The proof of these theorems is outlined in Subsection 5.1 and given in full detail in Subsection A.2.
Separately, we show a secondary result validating that the optimization oracle is reasonable under
particular conditions, borrowing the machinery of Hand & Voroninski (2018). We provide defini-
tions so that this paper is self-contained, but refer to (Hand & Voroninski, 2018) for details.
Suppose that G is an L-layer ReLU net, G(z) = ReLU(WL ... ReLU(W2ReLU(W1z)) ...), where
each Wi satisfies the Weight Distribution Condition (WDC) with constant e as defined by Hand &
Voroninski (2018) (their Definition 2). The full terms of the WDC are technical and we include the
complete definition in Subsection A.3, but the WDC essentially requires that the neuron weights at
each layer are Gaussian-like in their distribution; it is satisfied with high probability (dependent on
e) when n ≥ k log k and each entry of W is i.i.d. N(0, ɪ). There is also a condition jointly on A*
and G:
Range Restricted Isometry Condition (RRIC). A* and G satisfy the RRIC with constant e if
I DA* (G(ZI)- G(z2)), A* (G(Z3) - G(z4))E - DG(ZI)- G(z2), G(z3) - G(z4)E I
||G(z1) - G(z2)||2||G(z3) - G(z4)||2
≤e
This property is similar to the S-REC, as both ask that A* act approximately like an isometry on
outputs of G. One does not strictly imply the other, but the RRIC with zι = z3 and z2 = z4 implies
the S-REC with δ = 0 and Y = √1 - e. We only require the RRIC to be satisfied for A* and G,
because we later show that this induces the RRIC with a slightly worse constant for A close to A*.
Theorem 5.3 Let the WDC and RRIC be satisfied with respect to some constant e for A* and G.
Define C = e + 2||A* — A||・||A*|| + ||A* — A||2 and let A be any matrix such that || A - A*|| ≤ ∣^C^∣.
This is easily achieved ife.g. ||A*|| ≥ 1 ||A||. We require that KιL8e1/4 ≤ 1 and KiL8c1/4 ≤ 1
for an absolute constant K1 . Finally, assume there is no measurement noise: we observe yi =
A*G(zi). Let Dv f (x) = limt→o+ *x+t；)-,(X). Then when optimizing the objective function
11|AG(Z) - A*G(z*)||2 with respect to Z, there is always a descent direction -vz,z* outside of a
6
Under review as a conference paper at ICLR 2022
small ball around z* and its negative multiple:
D-Vz,z* f (z) < -K3 ~^~2l- max(IIzll *, ||z* 11) ∀x ∈ B(Z*, RI) ∪ B(-pLz*, R2) ∪ {0}
Here, Ri = K2L3eV4∣∣z*∣∣ and R2 = K2L13eV4∣∣z*∣∣. Moreover, 0 is a local maximum:
Dyf (0) < — ^^l∣∣z*∣∣ ∀y = 0. As in Hand & Voroninski (2018), Ki, K2, and K3 are constants,
and ρL > 0 converges to 1 as the number of network layers L goes to infinity.
Thus when A is sufficiently close to A* , the optimization objective of the decoding step always
admits a descent direction, outside of small neighborhoods around the correct z* and its negative
scalar multiple. A first order method such as Algorithm 1 of Hand et al. (2018) is then guaranteed
to converge to a good solution. This validates the optimization oracle in a certain setting. The proof
of Theorem 5.3 is in Subsection A.3 and outlined in Subsection 5.2.
5.1 Proof of convergence ( outline)
The core of the proof of Theorem 5.2 is to show that the infinite-sample gradient at every update step,
gs, has sufficiently large inner product (column-wise) with the direction of the correct dictionary,
As -A*, in the following sense. Given an iterative algorithm with steps of the form vs+i = vs -ηgs
and desired solution v*, We say the vector gs is (α, β, Ws)-Correlated with Vs - v* if hg, Vs - v*i ≥
α∣∣zs - z* ∣∣2 + β ∣∣gs∣∣2 - es. Moreover, a random vector gs is (α, β, Ws)-Correlated-W.h.p. if this
holds with probability 1-n-ω(i). In our case, we will think of Vs as a column of iterate As, V* as the
corresponding column of A*, and gs as it is already defined in Algorithm 1. Given such a property,
we make progress at every iteration, and Vs converges geometrically to V * (see e.g. Theorem 6 of
(Arora et al., 2015)). To prove that correlation holds for gs, we formalize several intuitive facts that
hold at the beginning of each update step. For ease of notation, let z refer to one of the zj computed
in the decode step, z* to the latent variable generating measurement yj, and A to As . First, we can
bound the reconstruction error that was computed in the decode step (all proofs in Subsection A.2):
Lemma 5.4 (Closeness between y and AG(z).) Let A ∈ B2 (A*, ∆), Σ=A - A*, and y =
A*G(z*)+η for some unknown z*. We can apply the optimization oracle to y with our current
guess A to give an estimate z. The resultant decoding objective value is then bounded as follows:
IIy - AG(z)II ≤ I∣a* - a∣∣∙ R + θ + IInII = II∑I∣R + θ + IInII
Combining Theorem 5.4 with the S-REC, we then bound IIG(z) - G(z*)II in terms of "A - A”:
Lemma 5.5 (Closeness between G(z) and G(z*).) Let A be an arbitrary matrix and Σ=A -
A* . Let z be the output of the optimization oracle corresponding to A with any fixed measurement
generated as y = A*G(z*) + n. Then we have:
IIG(z) - G(z*)II ≤ a因IIR + θ + δ + —
γ
Theorem 5.5 implies that, given a good estimate A of A*, the optimization oracle can find a good
estimate z of z*. Finally, by assumption, we have that E[G(z*)G(z*)T]-i ≈ C-i. Putting all of
these facts, together, one can obtain as desired that:
gs = Ez*,η[(AsG(z) - y)G(z)T] ∙ CT = Ez*,η[(AsG(z) - A*G(z*) - n)G(z)T] ∙ CT
≈ Ez* [(AsG(z*) - A*G(z*))G(z*)T] ∙ CT ≈ As - A*
In practice, we don’t have access to gs exactly; instead, we approximate the expectation with P
fresh samples, gs = = PP-ɔ1 (AG(Zp) — yp)G(zp)TC-1. If gs is close enough to gs, however,
then ^ will remain correlated-w.h.p. with the correct direction As - A*:
Theorem 5.6 (gs is correlated-w.h.p. with As - A*) Draw P = n2L log2 n fresh
samples at every iteration, and assume initialization A0 ∈ B2(A*, ∆). Then gf is
(1, ɪ, 100n2IICTII2(θ+δ+!ηll)2R2 +(MR+°+N)RUCTU)-correlated-w.h.p. with AS - A*.
Finally, Theorem 40 of Arora et al. (2015) converts correlation-w.h.p. into a statement of biased
geometric convergence to the optimum. Applying this theorem with our Theorem 5.6 yields our
biased geometric convergence result, Theorem 5.2. For all technical details, see Subsection A.2.
7
Under review as a conference paper at ICLR 2022
(a) Dependencies: m =10, n =9.
S-
Samples
+ MOD
+ k-SVD
+ AUtCKlIff
T- Alt mln
(b) Group sparsity: m = 784, n = 300.
Figure 1: Error bars show one standard deviation in log-scale over 4 trials, with nB =5and B = 30.
5.2 Proof of descent directions at decoding step (outline)
The original work of Hand & Voroninski (2018) proves the existence of descent directions for the
decoding step when A = A*, which implicitly assumes A* is known. In contrast, We seek to learn
A*, and at each iteration minimize a modified loss function f = ||AG(z) - A*G(z*)∣∣2 where
A ≈ A*. However, one could hope that the optimization landscape of f closely resembles that
of f(z') = ||AG(z) - AG(z*)∣∣2 when A is a good approximation of A*, and this is indeed how
We prove Theorem 5.3. To reason about the descent directions of||AG(z) - AG(z*)∣∣2, we must
prove that A itself also satisfies the RRIC. A calculation (Theorem A.9) shows that if A* satisfies
the RRIC with respect to G with constant e, then any A sufficiently close to A* satisfies the RRIC
with constant ce + 21| A* - A ∣∣∙∣∣ A* || +1| A* - A ||2. Then,the rest of Hand and Voroninski’s original
proof goes through with the added error from ||Vf - ▽川；see Subsection A.3 for details.
6	Experiments
We compare dictionary learning with and without a generative prior on a variety of experiments us-
ing PyTorch, and demonstrate that inclusion of accurate, finer-grained coefficient priors than sparsity
can broadly improve reconstruction quality. In all experiments, we begin with a specified generative
prior. In the first set of experiments, we fix a ground truth A* and obtain synthetic measurements
as y 〜A*G(z*) + η, where z* 〜N(0,Ik×k) and η 〜N(0, σ2Im×m) with σ = 0.1. The second
set of experiments are instead transfer learning problems: given a generative model pre-trained on
a large dataset, we remove the final layer to obtain a coefficient prior G for a related, but much
smaller dataset. (Thus there is no ground truth A*.) This will demonstrate the practical value of
using a generative coefficient prior, even when it is not explicitly used in data generation.
Both the ground-truth A*, when applicable, and the entries ofA0, are initialized uniformly at random
between 0 and 1. As baselines, we use the mini-batched implementation of MOD from scikit-learn
(Mairal et al., 2009a), and an open-source implementation of K-SVD (Rubinstein et al., 2008). As
implied by the theory, we consider an independent batch of samples at each iteration of the mini-
mization; in subsequent error plots, all methods are evaluated after each successive batch of samples
(on the x-axes). However, it is helpful to make a few practical modifications to Algorithm 1. We use
the Adam optimizer for all optimizations (Kingma & Ba, 2015), and obtain good performance with-
out explicitly pre-conditioning by C-1 or projecting onto B2(A0, δ). We then employ two main
variants. The first variant, “altmin”, most closely follows Algorithm 1, To perform the optimization
in the decoding step, we use a learning rate of 10-2 with 1000 iterations of the Adam optimizer. In
the update step, we find it helpful to run 1000 iterations of the Adam optimizer with learning rate
10-3 instead of taking a single gradient step. In the second variant, “autodiff”, we optimize over A
and Zi at once with respect to the nonconvex reconstruction objective in Section 4 for each batch of
samples. Our algorithmic guarantees do not apply in this setting, but it turns out to be an efficient
and practical heuristic. Further experiments and details are included in the Appendix.
We do not directly compare the dictionaries learned from each method to a “ground-truth” dic-
tionary, even when one is available, because two models can learn different dictionaries that both
represent the data well. In the absence of identifiability, how can one quantify whether a “good”
dictionary was learned? Drawing on both the widespread utility of dictionary learning for denoising
and the shared notion of “low description complexity” between latent sparse vectors and our latent
inputs z to G, we take a compression perspective on evaluating the learned dictionary. Given two
competing models, we assess which model can most accurately reconstruct data from noisy samples
given a fixed bit complexity for z (or the sparse coefficient vector for baselines), enforced by round-
8
Under review as a conference paper at ICLR 2022
Figure 2: In error plots (first row), error bars show one standard deviation in log-scale over 4 trials,
each with 2 batches of test samples and B = 30. In reconstruction plots (bottom row), each row is
a different sample and each column a different method (in order: noiseless, autodiff, altmin, MOD,
kSVD, noisy). The left and right columns are the blurred and recolored datasets, respectively.
ing to 4-digit precision. As such, after learning a dictionary A, we draw a batch of B i.i.d. samples
yi = A*G(Zi) + ηi and compute loss ɪ PB-1 ||AIG)ZG)-A∣∣x || over nB test batches, where Xi is
either the minimizer of the loss in the range of G or the best k-sparse approximation for baselines.
In all cases, we find that autodiff and altmin strongly outperform sparsity-based dictionary learning,
validating that generative priors greatly improve the reconstruction ability of a learned dictionary.
Generative prior capturing dependencies First, in Figure 1a we capture the intuition that G may
produce non-sparse outputs, which still contain meaningful correlations among the indices. To this
end, we set G(z) = [z; Zθ2; sin(z)] with k = 3. It is important not to merely concatenate linear
functions of z: any linear generative model G(z)=Mzcollapses to y = A*G(z) = (A*M)z for
noiseless data. Such a model would reduce to solving a linear system.
Generative prior capturing group sparsity. Next, in Figure 1b we set G(z) = ReLU(M(zθ2)),
where M ∈{0, 1}n×k is entrywise i.i.d. Bern(0.6). We interpret this setting as an approx-
imation of “group sparsity”, in which the k free parameters correspond to k groups of motifs,
Gi = {j s.t. Mji =1}, such that elements of Gi always appear at the same intensity.
Generative prior derived from an MNIST VAE. Finally, in [Figure 2∣ we begin with a pre-
trained VAE decoder on MNIST (see Subsection A.1 for details). The decoder architecture is
S(W2ReLU(WIZ)), where S(∙) is the sigmoid function and W1 and W2 are matrices. G is ob-
tained by slicing off the last layer of the decoder: G(z)=ReLU(W1z). We then transform a subset
of MNIST digits via blurring, colorshift, or flipping. This creates a transfer learning task between
the original, large and new, small datasets. Given samples y0 from a transformed dataset, we thus
apply the “transferred” knowledge of G to denoising, where y = y0 + η and η 〜N(0, 0.0025I).
Remark 2 (Unsupervised baselines) There are no comparisons to unsupervised deep learning
baselines, such as retraining the original VAE from scratch, because the sample complexity at which
our method succeeds is too low for these methods. Our code includes the option to verify this.
7	Conclusion
In this work, we propose a new model for dictionary learning that can capture complex co-
occurrence relationships between dictionary elements. We prove theoretically that an intuitive al-
ternating minimization algorithm will recover the unknown dictionary at a geometric rate in the
noiseless case, and simulated experiments validate the incorporation of generative priors and demon-
strate the practical benefits of such a model. Several interesting directions for future work remain.
Although we treat G(z) as a coefficient model, problems such as blind deconvolution apply when
G(z) is an image prior, and A* a convolution matrix. One future direction is to modify the algorithm
to preserve such known structure in A*. We also expect our model to prove useful on real datasets,
and it would be valuable to test it in non-simulated distributional learning settings.
9
Under review as a conference paper at ICLR 2022
References
Alekh Agarwal, Animashree Anandkumar, Prateek Jain, Praneeth Netrapalli, and Rashish Tandon.
Learning sparsely used overcomplete dictionaries. In Conference on Learning Theory, pp. 123-
137. PMLR, 2014.
M. Aharon, M. Elad, and A. Bruckstein. K-svd: An algorithm for designing overcomplete dictio-
naries for sparse representation. IEEE Trans. on Signal Processing, pp. 4311-4322, 2006.
Sanjeev Arora, Rong Ge, Ankur Moitra, and Sushant Sachdeva. Provable ica with unknown gaussian
noise, with implications for gaussian mixtures and autoencoders. In F. Pereira, C. J. C. Burges,
L. Bottou, and K. Q. Weinberger (eds.), Advances in Neural Information Processing Systems,
VolUme 25. Curran Associates, Inc., 2012. URL https://proceedings.neurips.cc/
paper∕2012∕file∕09c6c3783b4a7 0 054da74f2538ed4 7c6-Paper.pdf.
Sanjeev Arora, Rong Ge, and Ankur Moitra. New algorithms for learning incoherent and overcom-
plete dictionaries. In Conference on Learning Theory, pp. 779-806. PMLR, 2014.
Sanjeev Arora, Rong Ge, Tengyu Ma, and Ankur Moitra. Simple, efficient, and neural algorithms
for sparse coding. Journal of Machine Learning Research, 40(2015), January 2015. ISSN 1532-
4435.
Ashish Bora, Ajil Jalal, Eric Price, and Alexandros G. Dimakis. Compressed sensing using gener-
ative models. In Doina Precup and Yee Whye Teh (eds.), Proceedings of the 34th International
Conference on Machine Learning, volume 70 of Proceedings of Machine Learning Research,
pp. 537-546. PMLR, 06-11 Aug 2017. URL http://Proceedings .mlr.ρress∕v70∕
bora17a.html.
Emrah Bostan, Reinhard Heckel, Michael Chen, Michael Kellman, and Laura Waller. Deep phase
decoder: self-calibrating phase microscopy with an untrained deep neural network. Optica, 7
(6):559, June 2020. ISSN 2334-2536. doi: 10.1364/OPTICA.389314. URL https://www.
osapublishing.org/abstract.cfm?URI=OPtica-7-6-559.
Emmanuel J Candes, Justin K Romberg, and Terence Tao. Stable signal recovery from incomplete
and inaccurate measurements. Communications on pure and applied mathematics, 59(8), 2006.
ISSN 0010-3640.
Emmanuel CandeS and Laurent Demanet. Curvelets and fourier integral operators, 2002.
Emmanuel J. Candes. Ridgelets: Estimating with ridge functions, 2003.
Rahime Ceylan. The effect of feature extraction based on dictionary learning on ecg signal classifi-
cation. International Journal of Intelligent Systems and Applications in Engineering, 6(1):40-46,
Mar. 2018. doi: 10.18201∕ijisae.2018637929. URL https://www.ijisae.org/IJISAE/
article/view/64 7.
Niladri Chatterji and Peter L Bartlett. Alternating minimization for dictionary learning with random
initialization. In I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and
R. Garnett (eds.), Advances in Neural Information Processing Systems, volume 30. Curran As-
sociates, Inc., 2017. URL https://proceedings.neurips.cc/paper/2017/file/
3210ddbeaa16948a702b6049b8d9a202-Paper.pdf.
Ingrid Daubechies. Orthonormal bases of compactly supported wavelets. Communications
on Pure and Applied Mathematics, 41(7):909-996, 1988. doi: https://doi.org/10.1002/
cpa.3160410705. URL https://onlinelibrary.wiley.com/doi/abs/10.1002/
cpa.3160410705.
David L Donoho. Compressed sensing. IEEE Transactions on information theory, 52(4):1289-
1306, 2006.
Michael Elad and Michal Aharon. Image denoising via sparse and redundant representations over
learned dictionaries. IEEE Transactions on Image Processing, 15(12), Dec 2006.
10
Under review as a conference paper at ICLR 2022
K. Engan, S. Aase, and J. Hakon-Husoy. method of optimal directions for frame design. In ICASSP,
pp. 2443-2446, 1999.
Paul Hand and Vladislav Voroninski. Global guarantees for enforcing deep generative priors by
empirical risk. In Sebastien Bubeck, Vianney PercheL and Philippe Rigollet (eds.), Proceedings
of the 31st Conference On Learning Theory, volume 75 of Proceedings of Machine Learning Re-
search, pp. 970-978. PMLR, 06-09 Jul 2018. URL http://Proceedings .mlr.ρress/
v7 5∕hand18a.html.
Paul Hand, Oscar Leong, and Vlad Voroninski. Phase retrieval under a generative prior.
In S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Gar-
nett (eds.), Advances in Neural Information Processing Systems, volume 31. Curran Asso-
ciates, Inc., 2018. URL https://proceedings.neurips.cc/paper/2018/file/
1bc2 02 9a8 851ad34 4a8d5 03930dfd7f7-Paper.pdf.
Reinhard Heckel and Paul Hand. Deep Decoder: Concise Image Representations from Untrained
Non-convolutional Networks. International Conference on Learning Representations, 2019. URL
http://arxiv.org/abs/1810.03982.
Chinmay Hegde, Piotr Indyk, and Ludwig Schmidt. A nearly-linear time framework for graph-
structured sparsity. In Francis Bach and David Blei (eds.), Proceedings of the 32nd International
Conference on Machine Learning, volume 37 of Proceedings of Machine Learning Research,
pp. 928-937, Lille, France, 07-09 Jul 2015. PMLR. URL http://proceedings.mlr.
PreSS/v37/hegde15.html.
David Hong, Robert P. Malinas, Jeffrey A. Fessler, and Laura Balzano. Learning dictionary-based
unions of subspaces for image denoising. In 2018 26th European Signal Processing Conference
(EUSIPCO), pp. 1597-1601, 2018. doi: 10.23919/EUSIPCO.2018.8553117.
Gary B. Huang, Marwan Mattar, Tamara Berg, and Erik Learned-Miller. Labeled faces in the wild:
A database for studying face recognition in unconstrained environments. Technical report, 2007.
Akshay Kamath, Sushrut Karmalkar, and Eric Price. Lower bounds for compressed sensing with
generative models. CoRR, abs/1912.02938, 2019. URL http://arxiv.org/abs/1912.
02938.
Tero Karras, Timo Aila, Samuli Laine, and Jaakko Lehtinen. Progressive growing of gans for im-
proved quality, stability, and variation. pp. 4311T322, 2017. URL https://arxiv.org/
abs/1710.10196.
Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In Yoshua
Bengio and Yann LeCun (eds.), 3rd International Conference on Learning Representations, ICLR
2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings, 2015. URL http:
//arxiv.org/abs/1412.6980.
Naveen Kodali, James Hays, Jacob D. Abernethy, and Zsolt Kira. On convergence and stability of
gans. arXiv: Artificial Intelligence, 2018.
Jonas Moritz Kohler and Aurelien Lucchi. Sub-sampled cubic regularization for non-convex op-
timization. In Doina Precup and Yee Whye Teh (eds.), Proceedings of the 34th International
Conference on Machine Learning, volume 70 of Proceedings of Machine Learning Research, pp.
1895-1904. PMLR, 06-11 Aug 2017. URL http://ProceedingS.mlr.press/v7 0/
kohler17a.html.
Hannah Lawrence, David A. Barmherzig, Henry Li, Michael Eickenberg, and Marylou Gabrie.
Phase retrieval with holography and untrained priors: Tackling the challenges of low-photon
nanoscale imaging. In Mathematical and Scientific Machine Learning, 2021.
Christian Ledig, Lucas Theis, Ferenc Huszar, Jose Caballero, Andrew P. Aitken, Alykhan Tejani,
Johannes Totz, Zehan Wang, and Wenzhe Shi. Photo-realistic single image super-resolution us-
ing a generative adversarial network. CoRR, abs/1609.04802, 2016. URL http://dblp.
uni-trier.de/db/journals/corr/corr1609.html#LedigTHCATTWS16.
11
Under review as a conference paper at ICLR 2022
Victor Lempitsky, Andrea Vedaldi, and Dmitry Ulyanov. Deep Image Prior. In 2018
lEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 9446-9454. IEEE,
jun 2018. ISBN 978-1-5386-6420-9. doi: 10.1109/CVPR.2018.00984. URL https:
//box.skoltech.ru/index.php/s/ib52BOoV58ztuPM{#}pdfviewerhttps:
//ieeexplore.ieee.org/document/8579082/.
Shutao Li, Haitao Yin, and Leyuan Fang. Group-sparse representation with dictionary learning for
medical image denoising and fusion. IEEE Transactions on Biomedical Engineering, 59(12):
3450-3459, 2012. doi: 10.1109/TBME.2012.2217493.
Michael Lustig, David Donoho, and John M. Pauly. Sparse mri: The application of compressed
sensing for rapid mr imaging. Magnetic Resonance in Medicine, 58(6):1182-1195, 2007. doi:
https://doi.org/10.1002/mrm.21391. URL https://onlinelibrary.wiley.com/doi/
abs/10.1002/mrm.21391.
Julien Mairal, Francis Bach, Jean Ponce, and Guillermo Sapiro. Online dictionary learning for
sparse coding. In Proceedings of the 26th Annual International Conference on Machine Learn-
ing, ICML ’09, pp. 689-696, New York, NY, USA, 2009a. Association for Computing Machin-
ery. ISBN 9781605585161. doi: 10.1145/1553374.1553463. URL https://doi.org/10.
1145/1553374.1553463.
Julien Mairal, Jean Ponce, Guillermo Sapiro, Andrew Zisserman, and Francis Bach. Su-
pervised dictionary learning. In D. Koller, D. Schuurmans, Y. Bengio, and L. Bot-
tou (eds.), Advances in Neural Information Processing Systems, volume 21. Curran Asso-
ciates, Inc., 2009b. URL https://proceedings.neurips.cc/paper/2008/file/
C0f168ce8900fa56e57789e2a2f2c9d0-Paper.pdf.
Bruno A Olshausen and David J Field. Sparse coding with an overcomplete basis set: a strategy
employed by v1. Vision Research, 37:3311-3325, 1997.
Vardan Papyan, Yaniv Romano, and Michael Elad. Convolutional neural networks analyzed via
convolutional sparse coding. J. Mach. Learn. Res., 18(1):2887-2938, January 2017. ISSN 1532-
4435.
Ron Rubinstein, Michael Zibulevsky, and Michael Elad. Efficient implementation of the k-svd
algorithm using batch orthogonal matching pursuit, 2008.
Karin Schnass. Local identification of overcomplete dictionaries. J. Mach. Learn. Res., 16:1211-
1242, 2015.
Daniel A. Spielman, Huan Wang, and John Wright. Exact recovery of sparsely-used dictionaries. In
Shie Mannor, Nathan Srebro, and Robert C. Williamson (eds.), Proceedings of the 25th Annual
Conference on Learning Theory, volume 23 of Proceedings of Machine Learning Research, pp.
37.1-37.18, Edinburgh, Scotland, 25-27 Jun 2012. JMLR Workshop and Conference Proceed-
ings. URL http://PrOCeedingS .mlr.press/v23/SPielman12.html.
Jeremias Sulam, Vardan Papyan, Yaniv Romano, and Michael Elad. Multilayer convolutional sparse
modeling: Pursuit and dictionary learning. IEEE Transactions on Signal Processing, 66(15):
4090-4104, 2018. doi: 10.1109/TSP.2018.2846226.
Eric W. TrameL Andre Manoel, Francesco Caltagirone, Marylou Gabrie, and Florent Krzakala.
Inferring sparsity: Compressed sensing using generalized restricted Boltzmann machines. In
2016 IEEE Information Theory Workshop (ITW), pp. 265-269. IEEE, sep 2016. ISBN 978-1-
5090-1090-5. doi: 10.1109/ITW.2016.7606837. URL http://ieeexplore.ieee.org/
document/7 60 6837/.
Yong Xu, Zhengming Li, Jian Yang, and David Zhang. A survey of dictionary learning algorithms
for face recognition. IEEE Access, 5:8502-8514, 2017. doi: 10.1109/ACCESS.2017.2695239.
Raymond A. Yeh, Chen Chen, Teck Yian Lim, Alexander G. Schwing, Mark Hasegawa-Johnson,
and Minh N. Do. Semantic image inpainting with deep generative models. In 2017 IEEE
Conference on Computer Vision and Pattern Recognition (CVPR), pp. 6882-6890, 2017. doi:
10.1109/CVPR.2017.728.
12
Under review as a conference paper at ICLR 2022
R. Zhao, H. Li, and X. Liu. A survey of dictionary learning in medical image analysis and its
application for glaucoma diagnosis. Arch Computat Methods Eng, 28:463-471, 2021.
13