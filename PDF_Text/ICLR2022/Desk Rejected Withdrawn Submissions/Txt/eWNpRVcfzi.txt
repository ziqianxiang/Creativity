Under review as a conference paper at ICLR 2022
MURO: Deployment Constrained Reinforce-
ment Learning with Model-based Uncertainty
Regularized Batch Optimization
Anonymous authors
Paper under double-blind review
Ab stract
In many contemporary applications such as healthcare, finance, robotics, and rec-
ommendation systems, continuous deployment of new policies for data collection
and online learning is either cost ineffective or impractical. We consider a setting
that lies between pure offline reinforcement learning (RL) and pure online RL
called deployment constrained RL in which the number of policy deployments
for data sampling is limited. To solve this challenging task, we propose a new
algorithmic learning framework called Model-based Uncertainty Regularized batch
Optimization (MURO). Our framework discovers novel and high quality samples
for each deployment to enable efficient data collection. During each offline training
session, we bootstrap the policy update by quantifying the amount of uncertainty
within our collected data. In the high support region (low uncertainty), we en-
courage our policy by taking an aggressive update. In the low support region
(high uncertainty) when the policy bootstraps into the out-of-distribution region,
we downweight it by our estimated uncertainty quantification. Experimental re-
sults show that MURO achieves state-of-the-art performance in the deployment
constrained RL setting.
1	Introduction
Recent advances in deep learning have enabled reinforcement learning (RL) to achieve remarkable
success in various applications (Silver et al., 2017; OpenAI et al., 2019b; Vinyals et al., 2019; OpenAI
et al., 2019a). Despite RL’s success, it suffers many problems. In particular, traditional RL algorithms
require the agent to interact with the real world to collect large amounts of online data with the latest
learned policy. However, the online deployment of the agent to the real world might be impossible in
many real world applications (Matsushima et al., 2020). In the field of robotics or self-driving cars, for
example, the cost of deploying the agent to the field may be too risky for the agent itself as well as its
surrounding environment. In quantitative finance, trading strategy is usually carefully back-tested and
calibrated offline with historical data and simulation. Since the market data has a low signal-to-noise
ratio (Chen et al., 2020), online training can easily lead the policy fits to the noise, which is dangerous
and can potentially trigger unexpected large monetary loss. In the recommendation system setting
(Peska & Vojtas, 2020), after the policy has been trained offline, it will be deployed across different
servers for serving many users. In such setting, online training of policy is difficult because the
resulting policy might become unstable and/or cause bad user experience. Thus, large chunk of
data is collected for the deployed policy. Then, the data is used for offline training. During the next
scheduled deployment, the production level (data collection) policy is then updated.
To address this challenge, training an RL agent in an offline fashion seems to offer partial solution.
In pure offline RL (Levine et al., 2020) setting, a static dataset is collected by a behavioral (or data
collection) policy. Since the RL agent has no access to the online environment, training the RL agent
faces many challenges. First, because the learning policy and the behavioral policy have different
state visitation frequencies, evaluation of the offline policy is difficult. Second, during the training
process, this discrepancy of the distribution might increase over the training time, a phenomenon
known as distributional shift. Third, there might be a large extrapolation error when the value function
is bootstrapped into out-of-distribution actions (Kumar et al., 2019), a phenomenon that can lead to
learning divergence and instability.
1
Under review as a conference paper at ICLR 2022
Figure 1: Illustration showing the deployment constrained RL setting. Large batch of data collection
only occurs at the deployment point (showing two of them in red). The deployment and offline
training occurs in an interleaved fashion. As the number of deployment increases, the state-action
space will be explored more and more. Here, thicker-brown colored regions show the area of high
support whereas the lighter color shows an under-explored (low support) region.
There has been a large body of literature in solving the pure offline problem, and they can be broadly
categorized into the model-based and the model-free approach. In the model-free approach, they are
either: 1) enforcing the learning policy to stay close with the behavioral policy as in Fujimoto et al.
(2019); Kumar et al. (2019); Wu et al. (2019); Nachum et al. (2019b); Zhang et al. (2020); Nachum
et al. (2019a) or 2) using an ensemble of Q values for stabilizing the learning and behaviors as in
Ghasemipour et al. (2021); Wu et al. (2019); Nair et al. (2020). On the other hand, the model-based
approaches are either: 1) implicitly enforcing learning policy to stay close to the behavioral policy
through parameter initialization (Matsushima et al., 2020) or 2) injecting pessimism at the low
confidence regions as in Yu et al. (2020); Kidambi et al. (2020).
Despite their empirical success, since the pure offline RL environment is fundamentally different than
our deployment setting, we suspect developing novel method for deployment constrained RL can
possibly achieve a large headroom of improvement, since pure offline RL algorithms assume zero
interaction with the environment.
Our main contribution is the development of the Model-based Uncertainty Regularized batch Op-
timization (MURO), a novel framework for doing batch policy optimization for the deployment
constrained RL setting. To build an accurate model of the environment, our method encourages the
data collection policy to discover high quality and novel data batches during each online deployment.
During the offline training, MURO samples fictitious rollouts from the learned model and performs
policy update weighted with uncertainty regularized coefficient, a term that quantifies the amount of
uncertainty with respect to each state-action pair within the fictitious rollouts and the data. Theoreti-
cally, we show that our approach optimizes the lower-bound of the true value function. Intuitively,
our optimization process regularizes the update toward the high confidence regions. In areas of low
confidence or low data support, MURO discounts the update by the uncertainty regularized coeffi-
cient when the policy bootstraps into out-of-distribution states and actions regions. We empirically
compare our MURO to other strong baselines such as the state-of-the-art deployment constrained RL
algorithms (BREMEN) (Matsushima et al., 2020), and we show that MURO is capable of achieving
significant policy improvement while using smaller amounts of data and fewer deployment.
2	Background
We consider a discounted, infinite horizon Markov decision process (MDP), and denote Ω =
(S, A, M, r, μ0, γ), where S is the state space, A is the action space, M(s0∣s, a) is the state transition
kernel of transiting to a next state s0 from state s while taking action a, r(s, a) is the reward function,
Y ∈ (0,1) is the discounting factor, and μ0 is the initial state distribution.
In RL, the goal is to optimize a policy π(a|s) such that the expected discounted return JM(π)
is maximized: JM(π) = E t∞=0 γtr(st, at). The value function is defined as VMπ (s) =
∏,M,μo	一
E	{E∞=0 γtr(st, at)∣s0 = s}. We denote the optimal policy π* = argmaxπ JM(π). We
∏,st + ι 〜M(St ,at)	一
2
Under review as a conference paper at ICLR 2022
further define ρπM to be the discounted state visited by π on M such that ρπM = (1 - γ) Pt=0 γtpStπ,
where pSπ is the distribution of state at time t.
In the deployment constrained setting, there is a limitation on the number of times that the policy
can be deployed online for data collection. In real world applications such as recommender system
or robotic control, due to the associated cost with each deployment, it is desirable to minimize the
number of deployments to as small as possible. Let I be the number of times of deployment, and
let |B | be the size of a large batch of training data collected when a policy has been taken online (or
deployed), then, the total size of data collected throughout the entire process is I × |B |. Further, let
B(i) be the batch of data collected during the ith deployment. In each batch B, we store the data
transition {(s, a, r, s0)}. Note the difference between the deployment constrained setting and the pure
offline RL setting. In the pure offline RL setting, we consider it as a single batch of data with I = 1,
whereas in the deployment constrained setting, the agent will have the opportunity to interact with
the environment by deploying its learning or other behavioral policy for data collection purposes.
In Model-Based RL (MBRL), the world dynamics are learned from the sampled data. We estimate
the ground truth environment transition dynamics M* by learning a forward next state transition
dynamics model M : S X A → S. Let M(i) be the estimated dynamics bootstrapped from data batch
B(1) S B(2) S ... S B(i), in which we assume the model is updated with the aggregated data batches
UP to the ith deployment. We denote by M/i) the estimated dynamics learned by neural networks
parameterized by θ.
3	Algorithmic Framework
For the purpose of exposition, we start by presenting an idealized version of MURO algorithmic
framework. Then, we describe a practical version that we have implemented and perform quite well.
The detailed algorithm is summarized in Algorithm 1.
The heart of the algorithm relies on uncertainty estimation. We postulate that by having more
diversified samples (data coverage), we can learn a more accurate representation of the environment,
which is the key ingredient of MBRL. To get the most out of each deployment, our algorithm tries to
explore the under-represented region (the low support area of our collected data) so as to minimize
the amount of uncertainty or maximize the information gain. After building an accurate model, we
can use it to generate fictitious rollouts for policy training. The idea is that we want to weight the
contribution of each policy update by taking the uncertainty quantification on the fictitious rollouts
(each state action pair) by uncertainty regularized coefficient (UΜ⑸ M* (s, a)) which characterize
the level of uncertainty results from model estimation error (eq. (2)). For fictitious rollouts that we
have low data support, We discount the policy update by UM⑸ M* (s, a) and discourage the policy
from bootstrapping into unknown regions. In doing this, we are regularizing the policy update into
high support/confidence regions.
Theoretical Results. All the proofs in this section are deferred to the appendix. In the deployment
constrained setting, we are interested in iterative improvement after each deployment. Let πref be a
reference policy that We will deploy to collect the data, d(∙, ∙) be the closeness of the two policies,
D(∙, ∙) be the discrepancy between the M* and M, we make the followings four assumptions:
VM* ≥ VM — D∏ref,δ(M,∏), s.t.d(π,∏ref) ≤ δ (A1)	M = M* =⇒ Dnref(MM,π) = 0	(A2)
L-Lipschitz in VM w.r.tto some norm such that∣V^M(S) — VM(s0)∣ ≤ Lks — S ∣∣,∀s, s	(A3)
Dnref(M, π) is given by the form of ET〜∏ref,M* {f(M, π,τ)}	(A4)
Let M(i) be the model estimated from the data collected up to the ith deployment. Adapted from
Schulman et al. (2015); Luo et al. (2018):
Lemma 1. The difference of the value functions in-between each deployment is bounded by:
IVM (i+i)— VM (i) I ≤ κL	ιπ	(∣M (i+1)(s,a) — M (i)(s,a)k)	(1)
(s,a)~ρM(i+ι)
We define the following function UM⑸ M* (s, a): S × A → R as an uncertainty regularized
coefficient:
3
Under review as a conference paper at ICLR 2022
(Sa)EPn {UM (i),M* (S,明,g(κ(s a)E,π	kM *(S,a) - M(i)(S,a)k)	⑵
where g(X), (VM(i) - X)(VM(i))-1,and K , (I-Y)-1γ.
Proposition 1. Let UM⑶ M* (s, a) be the uncertainty regularized coefficient defined by eq. (2), then
the performance of π on the ground-truth M* is lower bounded by that of the estimated M(i),
weighted by UM⑸,m* (s, a):
VM * ≥ VM (i) (Sa)EPn JUm (i),M * Ga»	(3)
Proposition/Says that in optimizing the lower bound (the RHS of eq. (3) ) with V^ ⑸，We can
maximize the overall performance of π on the real dynamics M*.
Interpretation of UM⑸ M* (s, a) (uncertainty regularized coefficient). Here, the term can be in-
terpreted as an uncertainty quantification measure as a result of model estimation error between
M(i) and M*, with M(i) being learned from the collected (and limited amount of) data up to the
ith deployment. On the regions where we have high data support, M(i)(s, a) is close to M*(s, a),
thus their discrepancy decreases and UM⑸ M* (s, a) approaches to unity. On the regions that are less
certain, the discrepancy increases and thus UM⑶ M* (s, a) decreases.
Remark 1. Ifour estimated model M(i) is reasonably good, so that the model estimation error
KkM * (s, a) 一 M (i)(s, a)k is smaller than VM ⑸，and that VM 电 isPOSitive,then UM ⑸ M * (s,a) is
a scalar between 0 and 1, and is approximately proportional to the accuracy of the model estimation.
Thus to optimize VMπ * , we instead optimize its lower bound, which is the value function at the
estimated model VM⑸ weighted by the uncertainty regularized coefficient. We assign a higher weight
at the regions of high confidence, and a lower weight at the regions of low confidence. Next, we
utilize our result from eq. (3) for establishing our MURO lower bound optimization algorithm.
Algorithm 1 MURO: Model-based Uncertainty Regularized batch Optimization
Given: Size of data batches B, Number of deployments I, Dall J {}
for i = 1, 2, 3, ..., I of deployments do
deploy π online for data collection
1	Dall J Dall S Collect-Data-Low-Support-Region(π)
2	Mθ J Learn approximate transition dynamics model
3	Train Uncertainty-Labeler(Dall )
4	π J Train with MBRL(UnCertainty-Labeler, Mθ, Dall) as in section 3.2
returnπ
3.1 Practical Implementation
In this section, we explain the practical implementation of algorithm 1 in detail. For readers
convenience, we have labeled the line number in the algorithm, and we will refer to the line number
as we explain below.
Learning the transition dynamics (line 2): For estimating the transition dynamics Mθ , we use an
ensemble of N deterministic dynamics models with multi-layer fully-connected perceptrons as in
Mishra et al. (2017); Kurutach et al. (2018) to reduce model bias. They are trained to predict the next
state with L2 loss:	1
min |D—|	E	kst+1 一 Mθ (St,at)k2.	(4)
|Dall| (st,at,st+1)∈Dall
Since we use the data collected up to ith deployment, we drop the (i) superscript on Mθi) hereafter.
Uncertainty-Labeler (line 3). This module is responsible for characterizing the levels of uncertainty
in the collected data and approximated UM ⑶ M *(s, a)∙ Specifically, we want to identify the regions
in our data that have high support or low support with respect to uncertainty. Since the oracle is
unavailable to us, here we can only approximate the uncertainty by state-actions visitation frequency.
4
Under review as a conference paper at ICLR 2022
Shall the state-actions visited frequently, more pairs of them will show up in the data. If we were to
train models on the batches of data to predict the next state transition dynamics, then, our prediction
will be more accurate on the states that we have seen (in the collected data), and less accurate on the
unfamiliar state. Thus, we have established that the uncertainty quantification measure as the next
state prediction error.
In the actual implementation, we have used K ensembles of Probabilistic Neural Networks (PNN)
(Chua et al., 2018) to capture the uncertainty within the data. Let Pφ : S X A → S be the PNN with
parameter φ. In PNN, the network has its output neurons parameterized by a Gaussian distribution in
the effort of capturing the uncertainty. As explained above, We use Pφ to quantify the uncertainty
by prediction error. The lower the prediction error in relation to the ground truth, the higher the
support. Our uncertainty-labeler approximates the uncertainty regularized coefficient U曲⑸ M* (s, a)
by U(a, s) (define below shortly), and we have dropped the dependency on subscript since U is
trained with the data collected up to the ith deployment.
The actual implementation of U (a, S) is motivated by remark 1. Let T be the fictitious trajectory
generated by Mθ (the learned dynamics),
we use Pφ to quantify the uncertainty. Since the ground
truth state is unavailable to us in the fictitious rollouts, to calculate the prediction error, we further
approximate it by the intra-discrepancy error within the ensembles. We randomly sample two models
(a, b) from K Uncertainty-Labeler ensemble, for each state-action pairs in T, we label them :
U (st, at) = exp(-αk^t+)ι - ^(+∕∣ι),for (st,at) ∈ T
(5)
where ^(+)1 = Pφa) (st, at) is the next state predicted by the sampled ath model from the Uncertainty-
Labeler (and similarly for b index), and α > 0 is a temperature parameter. Here, we have assumed
that our estimated models Pφ are operating on the "'reasonably good" regime (as by remark 1). To
verify how good our estimated U (st, at) in approximating the actual UM ^)M * (s, a), we perform
additional ablation study on Appendix G.
Note that here, we have used different sets and different types of networks for estimating the model
dynamics Mθ and the uncertainty Pφ . We use Mθ (deterministic networks) to generate fictitious
rollouts, and we use Pφ (PNN, a specialized uncertainty probabilistic network) as the "judge" to
quantify the uncertainty for the generated fictitious rollouts. We separate out these two networks
intentionally to avoid possible error propagation between the two modules. Pφ is trained by (eq. (17)).
For detail discussion, see Appendix F.
Collect Data from Low Support Region (line 1). To maximize the benefit out of each deployment
and to build an accurate representation M§ of the environment, we emphasize that we want to achieve
the maximal data coverage as well as exploring the under-explored region (the low support area).
Exploration in the deployment constrained setting is non-trivial because it is hard to evaluate which
action-state pairs will fill the low support region and lead to high data coverage. During online data
sampling, for each state that our agent encounters, we identify the amount of uncertainty by taking
the actions which lead to maximal uncertainty (prediction error) between our predicted next state (^0)
and the ground truth next state (as by eqn.16 in the Appendix E). Following a trajectory of maximal
prediction error leads to novel experience discovery and reduce the number of unknown regions
within the data. In the ablation study (Fig.4 c. and d.), we show that this strategy leads to more
accurate learned dynamics M§. (For detailed implementation, please refer to Appendix E).
3.2 Offline Training with MBRL Method
Next, we define our offline model-based training method (line 4). In the function below, we provide a
practical instantiation of MURO offline model-based method for learning a policy.
Our method utilizes trust region policy optimization (TRPO) (Schulman et al., 2015) with fictitious
trajectory generated with learned ensembles of transition dynamics M§ weighted by Uncertainty-
Labeler (or uncertainty regularized coefficient).
Fictitious trajectory generated from learned dynamics (line 7 to line 8). After each deployment,
the environment dynamics transitions are estimated by an ensemble of {M§ }N trained using Dm∣. To
generate a fictitious trajectory, we first randomly sample a model j ∈ (1, 2, .., N), and then we roll-out
5
Under review as a conference paper at ICLR 2022
5
6
7
8
9
10
■ ->	，♦	_______ ____ _	.	, nʌʃ 7-Λ	r τ	-Tlf
Function MURO MBRL Training(Mθ, Dall, Uncertainty-Labeler):
Initialize πinit with the data collection policy.
for training iterations do
Randomly sample a dynamics model from N ensembles of Mθ
for optimization steps do
T J sample fictitious trajectory from Mθ
Tlabeled J label fictitious trajectory with Uncertainty-Labeler (T) as in eq.(5)
train with uncertainty regularized TRPO with Tlabeled, ∏init as in eq.(6)
the trajectory T by running the learning policy π with the next state as ^t+ι = MMθj (^t, at = π(^)),
for t ∈ 1, ..., T.
TRPO training with Uncertainty (line 9 to line 10). Next, we train the policy with uncertainty
regularized TRPO. In the offline setting, TRPO is trained using imaginary rollouts generated from the
learned dynamics Mθ . Utilizing Prop 1, We use TRPO to optimize the VMM * by improving its lower-
bound (the RHS). Here, We replace VM^)(S) by Aπ⅛k (s, a), and We approximated UM^)M* (s, a)
一 ^, 、 . ___________________
by U(S, a) (eqn.5), with TRPO:
argm ax EnQ),Spa,b,M θ { Π^a⅛ Ahk (S，a)U(a，S)}
s∙t∙ Ea53(s),s,P产,M θ {DKL(π^(∙∣s)kπ^k (IS))} ≤ δ	⑹
where We set ∏a = ∏init as the initial policy of the TRPO at the first iteration (as a way to impose
the constraint on πref). Here, Aπ⅛k (s, a) is the advantage function of policy π^k following a fictitious
trajectory generated by Mθ . Here, when the policy bootstraps into out-of-distribution states and
actions regions (low support area), we discount the update. In doing this,our optimization process
regularizes the update toward the high confidence regions.
4	Empirical Results
We empirically evaluate our proposed MURO algorithm with five continuous control benchmarks
using the MuJOCO1 physics simulator: Walker2d, Hopper, Half-Cheetah, Ant, and Cheetah-Run.
Baseline. We compare our MURO with BREMEN, a state-of-the-art algorithm2 that is designed
specifically for deployment constrained setting. For BREMEN, we used the exact same hyper-
parameters as what was originally proposed in Matsushima et al. (2020). As for comparison, we also
adapted MOPO (Yu et al., 2020), a pure-offline algorithm, to deployment-constrained setting. We
used the latest learned policy for data collection.
Evaluation Setup. Following Matsushima et al. (2020), our evaluation set up consisted of 5 deploy-
ments for Half-Cheetah, Ant, Cheetah-Run and 10 deployments for Walker2D and Hopper. To see
the trade-off between the number of deployment versus data sample size, we perform our empirical
tests on two separate cumulative data sizes of 500k and 250k.
For the settings of 500k / (250k) sample size experiments, the environments Half-Cheetah, Ant
and Cheetah-Run have a per deployment data collection batch size ∣B∣ = 100k/(50k) (with 5
deployments in total). For the environments of Walker2d and Hopper, they have a per deployment
data collection batch size of ∣B∣ = 50k/(25k) (with 10 deployments in total).
Deployment Setup. In the deployment constrained setting, the data collection only happens during
the deployment. No training happens during this period. Only until a data batch size of ∣B ∣ has
been collected, the agent will be taken offline for training and policy update. At the first (initial)
deployment, a random policy is used for data collection. After that, the data collection will be
replaced by the updated policy and will be launched to deploy again. We plot our 500k data size
results in Fig.2, and we plot our 250k data size results on Fig.3. For all experimental results, we
averaged over 5 random seeds.
1http://www.mujoco.org/
2BREMEN has been compared against to SAC, Model-Ensembles-TRPO, BCQ, and BRAC (all adapted to
deployment-constrained setting) and shows state-of-the-art performance.
6
Under review as a conference paper at ICLR 2022
Figure 2: Empirical Evaluations of a) Walker2D and Hopper for I = 10 deployments, |B | = 50k, and
b) Ant, Half_Cheetah and Cheetah_Run for I = 5 deployments, |B | = 100k. Total data consumption
in both settings is I × |B | = 500k . The x-axis is aligned showing the number of deployment.
Deployment	2nd	3rd	4th	5th
MURO	0.995	0.959	0.945	0.941
Baseline(BREMEN)	0.972	0.903	0.891	0.858
Table 1: Table showing the amount of novelty of each data batches collected between each deployment
for cheetah_run. Novelty is measured as cosine distance between each observation (state) in B(i)
versus the previous aggregated batches (B(1).. S B(i-1)), averaged over the number of transitions.
Empirical Details: 500k data size. The top(first) figure of Fig.2 shows the total sample size of each
deployment on the y-axis, and on the x-axis, we show the number of deployments. We align along
the x-axis for all figures. Our method works the best on the Walker2D and Hopper, in which our
MURO approach achieves significant cumulative rewards especially in the longer deployments (after
6th). In the Hopper environment, we see that the baselines (BREMEN and MOPO) show incapable
of learning a meaningful policy while our MURO significantly outperforms. On the other hand, in
the Ant, Cheetah-Run and Half-Cheetah environment (as shown in part b. of Fig.2), our MURO
is capable of achieving a higher performance using a smaller amount of data. For instance, in the
Cheetah-Run environment, MURO already achieved 550 points in the second deployment but the
baselines take four to five deployments to achieve the same score.
Empirical Details: 250k data size. Different than the previous set of experiments, here, we reduce
the total data size by half to 250k. In general, when we reduce the data size, the performances of
all algorithms will be reduced. Despite this, our MURO algorithm performs quite well even in this
setting. Comparing to the 500k data size experiment, we also observe similar trends. In Fig.3 a), we
plot the results for Walker2d and Hopper. For the former, we see a significant winning margin in
Walker2d whereas in Hopper, the winning only happens in the 8th 〜9th deployments. In Fig.3 b),
we plot the results for Ant, Cheetah-Run and Half-Cheetah. We observe that our MURO algorithm
achieves faster learning and stronger performance.
7
Under review as a conference paper at ICLR 2022
Figure 3: Empirical Evaluations of a) Walker2D and Hopper for I = 10 deployments, |B| = 25k and
b) Ant, Half_Cheetah and Cheetah_Run for I = 5 deployments, |B | = 50k . Total data consumption
in both settings is I × |B | = 250k . The x-axis is aligned showing the number of deployment.
Figb)
Figure 4: Ablation study on I × |B | = 500k setting: isolating the effects of having either uncertainty
coeff only or data collection strategy only (as in eqn.16) on MURO. We plot the cumulative rewards
on fig. a) and b) for ant and cheetah_run. We see that the effect of uncertainty coeff is stronger than
the effect of data collection strategy. On subplots c) and d) we show quality of the learned model
dynamics. We compare the fictitious rollouts against real rollouts in terms of energy distance (the
lower the better) and trajectory-wise MSE (the lower the better) for the cheetah_run. We see that
MURO (data collection strategy) gives stronger effect. The combined effects of two components
(which becomes MURO) gives the best performance not only on the cumulative rewards (subplots a
and b), but also on the learned dynamics (subplots c and d).
5 Ablation Study
In this section, we examine the MURO algorithm in terms of the following three aspects: 1) discovery
of high quality and novel data batches during each deployment; 2) whether this will lead to more
accurate learning of model dynamics; 3) isolation effect of having uncertainty coefficient only or
data-collection strategy only. To assess the novelty of data-batches during each deployment, we
calculate the average cosine distance between the current batch (B(i)) versus the aggregation of
all of the previous batches (B1 S B2 S ...B(i-1)) and then show the result in Table.1. Note that
deployment 1 is not shown because the initial data collection policy is a random policy. Our result
shows that our MURO algorithm leads to much higher novel transitions discovery.
8
Under review as a conference paper at ICLR 2022
In Fig.4 subplots a and b, we isolate each component and show the effects of including either 1)
only the uncertainty coefficient or 2) only our data collection strategy as in eqn.16. In terms of
cumulative rewards, the former (coeff only) has a stronger effect than latter (data collection strategy
only). Combining both gives the optimal effect on the cumulative rewards and leads learning an
accurate model (subplots c and d).
Next, in Fig.4 subplots c and d, we compare the performance of the fictitious rollouts from the learned
model dynamics versus the rollouts from the real environment. We first plot the energy distance in
c) and we show the mean square error (MSE) of trajectory-wise rollouts in d). As the number of
deployment increases, our method is capable of discovering high quality transitions which result in a
better estimated model. In terms of the learned model dynamics, the data collection strategy only has
a stronger effect in learning a more accurate model.
Lastly, in Appendix G, we show how good is the approximated U (s, a) in estimating the actual
UM(i) ,M* (S，a).
6	Related Works
Deployment-constrained RL. To the best of our knowledge, Matsushima et al. (2020) is the first
work that proposed the concept of deployment-constrained efficiency. In their paper, the authors
proposed the algorithm Behavior-Regularized Model-ENsemble (BREMEN) that enforces implicit
KL-divergence between the learning policy and the behavioral policy by parameter initialization.
In contrast, our work optimizes the lower bound of the true value function with the uncertainty
regularized coefficient, an approach with theoretical support. In addition, we show how to make
use of the uncertainty measure to maximize the benefit of each online deployment, and to connect
between the online (data collection) and offline (policy learning) setting. Empirically, this leads to
higher performance while using fewer number of deployment.
On the other hand, there also some related research works such as Bai et al. (2019); Pan et al. (2020)
and in semi-batch RL such as Ernst et al. (2005); Lange et al. (2012); Jaakkola et al. (1999); Chu &
Kitani (2020).
Model-Based RL. In model-based approach, the world representation is learned first and then is used
for generating imaginary rollouts. Related research works are Chua et al. (2018); Janner et al. (2019);
LUo et al. (2018); Munos & SzePesvdri (2008). However, direct application of MBRL methods into
the offline setting can be challenging due to distribution shifts. Nevertheless, the closely related
research is Kurutach et al. (2018), in which an ensemble of estimated model dynamics is used for
generating fictitious rollous for stabilizing effects. Similar approaches have also been investigated by
Zhang et al. (2019); Kaiser et al. (2019); Veerapaneni et al. (2020); Feinberg et al. (2018).
Offline RL. Unlike deployment-constrained setting, offline RL assumes no interaction with the
environment. Thus, the learning policy needs to reason about the behavioral policy and make policy
update base on that. The two most related literatures are Yu et al. (2020) and Kidambi et al. (2020).
In Yu et al. (2020), the authors proposed an uncertainty penalized MDP for explicitly penalizing
the uncertainty. On the other hand, the authors from Kidambi et al. (2020) proposed a pessimistic
MDP that divides the environment into two regions: known or unknown. When the agent is entering
into the unknown region, the MDP undergoes a halt state (or absorbing state) for penalizing this
action. Here instead, we show that in the deployment setting, we have developed the uncertainty
regularized coefficient which directly applies to the learning value function. We show that optimizing
this uncertainty regularized value function in-between each deployment is the same as optimizing the
lower bound of the true value function. Intuitively, our approach regularizes the optimization toward
the higher confidence regions.
Other model-free offline RL works include Fujimoto et al. (2019); Kumar et al. (2019); Wu et al.
(2019); Nachum et al. (2019b); Zhang et al. (2020); Nachum et al. (2019a) and Ghasemipour et al.
(2021); Wu et al. (2019); Nair et al. (2020).
7	Conclusion
In this paper, we have proposed the algorithmic framework MURO for optimizing the policy learning
under the deployment-constrained setting.
9
Under review as a conference paper at ICLR 2022
References
Yu Bai, Tengyang Xie, Nan Jiang, and Yu-Xiang Wang. Provably efficient q-learning with low switch-
ing cost. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alch6-Buc, E. Fox, and R. Garnett
(eds.), Advances in Neural Information Processing Systems, volume 32. Curran Associates, Inc.,
2019.
Luyang Chen, Markus Pelger, and Jason Zhu. Deep learning in asset pricing. Available at SSRN
3350138, 2020.
Wen-Hsuan Chu and Kris M. Kitani. Neural batch sampling with reinforcement learning for semi-
supervised anomaly detection. In Andrea Vedaldi, Horst Bischof, Thomas Brox, and Jan-Michael
Frahm (eds.), Computer Vision - ECCV 2020, pp. 751-766, Cham, 2020. Springer International
Publishing. ISBN 978-3-030-58574-7.
Kurtland Chua, Roberto Calandra, Rowan McAllister, and Sergey Levine. Deep reinforcement
learning in a handful of trials using probabilistic dynamics models. Advances in Neural Information
Processing Systems, 31, 2018.
Damien Ernst, Pierre Geurts, and Louis Wehenkel. Tree-based batch mode reinforcement learning.
Journal of Machine Learning Research, 6:503-556, 04 2005.
Vladimir Feinberg, Alvin Wan, Ion Stoica, Michael I Jordan, Joseph E Gonzalez, and Sergey Levine.
Model-based value estimation for efficient model-free reinforcement learning. arXiv preprint
arXiv:1803.00101, 2018.
Scott Fujimoto, David Meger, and Doina Precup. Off-policy deep reinforcement learning without
exploration. In International Conference on Machine Learning, pp. 2052-2062. PMLR, 2019.
Seyed Kamyar Seyed Ghasemipour, Dale Schuurmans, and Shixiang Shane Gu. Emaq: Expected-max
q-learning operator for simple yet effective offline and online rl. In International Conference on
Machine Learning, pp. 3682-3691. PMLR, 2021.
Tommi Jaakkola, Satinder Singh, and Michael Jordan. Reinforcement learning algorithm for partially
observable markov decision problems. Advances in Neural Information Processing Systems, 7, 11
1999.
Michael Janner, Justin Fu, Marvin Zhang, and Sergey Levine. When to trust your model: Model-based
policy optimization. Advances in Neural Information Processing Systems, 32:12519-12530, 2019.
Lukasz Kaiser, Mohammad Babaeizadeh, Piotr Milos, Blazej Osinski, Roy H Campbell, Konrad
Czechowski, Dumitru Erhan, Chelsea Finn, Piotr Kozakowski, Sergey Levine, et al. Model-based
reinforcement learning for atari. arXiv preprint arXiv:1903.00374, 2019.
Rahul Kidambi, Aravind Rajeswaran, Praneeth Netrapalli, and Thorsten Joachims. Morel: Model-
based offline reinforcement learning. In H. Larochelle, M. Ranzato, R. Hadsell, M. F. Balcan, and
H. Lin (eds.), Advances in Neural Information Processing Systems, volume 33, pp. 21810-21823.
Curran Associates, Inc., 2020.
Aviral Kumar, Justin Fu, Matthew Soh, George Tucker, and Sergey Levine. Stabilizing off-policy
q-learning via bootstrapping error reduction. Advances in Neural Information Processing Systems,
32:11784-11794, 2019.
Thanard Kurutach, Ignasi Clavera, Yan Duan, Aviv Tamar, and Pieter Abbeel. Model-based trust-
region policy optimization. In International Conference on Learning Representations, 2018.
Sascha Lange, Thomas Gabel, and Martin Riedmiller. Batch reinforcement learning. Reinforcement
Learning: State of the Art, 01 2012. doi: 10.1007/978-3-642-27645-3_2.
Sergey Levine, Aviral Kumar, George Tucker, and Justin Fu. Offline reinforcement learning: Tutorial,
review, and perspectives on open problems, 2020.
Yuping Luo, Huazhe Xu, Yuanzhi Li, Yuandong Tian, Trevor Darrell, and Tengyu Ma. Algorith-
mic framework for model-based deep reinforcement learning with theoretical guarantees. In
International Conference on Learning Representations, 2018.
10
Under review as a conference paper at ICLR 2022
Tatsuya Matsushima, Hiroki Furuta, Yutaka Matsuo, Ofir Nachum, and Shixiang Gu. Deployment-
efficient reinforcement learning via model-based offline optimization. In International Conference
on Learning Representations, 2020.
Nikhil Mishra, Pieter Abbeel, and Igor Mordatch. Prediction and control with temporal segment
models. In International Conference on Machine Learning, pp. 2459-2468. PMLR, 2017.
Remi Munos and CSaba SzePeSv疝i. Finite-time bounds for fitted value iteration. Journal of
Machine Learning Research, 9(27):815-857, 2008. URL http://jmlr.org/papers/v9/
munos08a.html.
Ofir Nachum, Yinlam Chow, Bo Dai, and Lihong Li. Dualdice: Behavior-agnostic estimation of
discounted stationary distribution corrections. Advances in Neural Information Processing Systems,
32:2318-2328, 2019a.
Ofir Nachum, Bo Dai, Ilya Kostrikov, Yinlam Chow, Lihong Li, and Dale Schuurmans. Algaedice:
Policy gradient from arbitrary exPerience. arXiv preprint arXiv:1912.02074, 2019b.
Ashvin Nair, Murtaza Dalal, Abhishek GuPta, and Sergey Levine. Accelerating online reinforcement
learning with offline datasets, 2020.
OPenAI, :, ChristoPher Berner, Greg Brockman, Brooke Chan, Vicki Cheung, Przemyslaw Debiak,
Christy Dennison, David Farhi, Quirin Fischer, Shariq Hashme, Chris Hesse, Rafal Jozefowicz,
Scott Gray, Catherine Olsson, Jakub Pachocki, Michael Petrov, Henrique Ponde de Oliveira Pinto,
Jonathan Raiman, Tim Salimans, Jeremy Schlatter, Jonas Schneider, Szymon Sidor, Ilya Sutskever,
Jie Tang, FiliP Wolski, and Susan Zhang. Dota 2 with large scale deeP reinforcement learning,
2019a.
OPenAI, Ilge Akkaya, Marcin Andrychowicz, Maciek Chociej, Mateusz Litwin, Bob McGrew, Arthur
Petron, Alex Paino, Matthias PlaPPert, Glenn Powell, RaPhael Ribas, Jonas Schneider, Nikolas
Tezak, Jerry Tworek, Peter Welinder, Lilian Weng, Qiming Yuan, Wojciech Zaremba, and Lei
Zhang. Solving rubik’s cube with a robot hand, 2019b.
Feiyang Pan, Jia He, Dandan Tu, and Qing He. Trust the model when it is confident: Masked
model-based actor-critic. In H. Larochelle, M. Ranzato, R. Hadsell, M. F. Balcan, and H. Lin
(eds.), Advances in Neural Information Processing Systems, volume 33, PP. 10537-10546. Cur-
ran Associates, Inc., 2020. URL https://proceedings.neurips.cc/paper/2020/
file/77133be2e96a577bd4794928976d2ae2-Paper.pdf.
Ladislav Peska and Peter Vojtas. Off-line vs. on-line evaluation of recommender systems in small
e-commerce. In Proceedings of the 31st ACM Conference on Hypertext and Social Media, HT
’20, PP. 291-300, New York, NY, USA, 2020. Association for ComPuting Machinery. ISBN
9781450370981. doi: 10.1145/3372923.3404781. URL https://doi.org/10.1145/
3372923.3404781.
John Schulman, Sergey Levine, Pieter Abbeel, Michael Jordan, and PhiliPP Moritz. Trust region
Policy oPtimization. In International conference on machine learning, PP. 1889-1897. PMLR,
2015.
David Silver, Thomas Hubert, Julian Schrittwieser, Ioannis Antonoglou, Matthew Lai, Arthur Guez,
Marc Lanctot, Laurent Sifre, Dharshan Kumaran, Thore GraePel, Timothy LillicraP, Karen Si-
monyan, and Demis Hassabis. Mastering chess and shogi by self-Play with a general reinforcement
learning algorithm, 2017.
Rishi VeeraPaneni, John D. Co-Reyes, Michael Chang, Michael Janner, Chelsea Finn, Jiajun Wu,
Joshua Tenenbaum, and Sergey Levine. Entity abstraction in visual model-based reinforcement
learning. In Leslie Pack Kaelbling, Danica Kragic, and Komei Sugiura (eds.), Proceedings of
the Conference on Robot Learning, volume 100 of Proceedings of Machine Learning Research,
PP. 1439-1456. PMLR, 30 Oct-01 Nov 2020. URL https://proceedings.mlr.press/
v100/veerapaneni20a.html.
11
Under review as a conference paper at ICLR 2022
Oriol Vinyals, I. Babuschkin, W. Czarnecki, Michael Mathieu, Andrew Dudzik, J. Chung, D. Choi,
R. Powell, Timo Ewalds, P. Georgiev, Junhyuk Oh, Dan Horgan, Manuel Kroiss, Ivo Danihelka,
Aja Huang, L. Sifre, Trevor Cai, John P. Agapiou, Max Jaderberg, A. S. Vezhnevets, Remi Leblond,
Tobias Pohlen, Valentin Dalibard, D. Budden, Yury Sulsky, James Molloy, T. L. Paine, Caglar
Gulcehre, Ziyu Wang, T. Pfaff, Yuhuai Wu, Roman Ring, Dani Yogatama, Dario Wunsch, Katrina
McKinney, O. Smith, T. Schaul, T. Lillicrap, K. Kavukcuoglu, Demis Hassabis, Chris Apps, and
D. Silver. Grandmaster level in starcraft ii using multi-agent reinforcement learning. Nature, pp.
1-5, 2019.
Tingwu Wang, Xuchan Bao, Ignasi Clavera, Jerrick Hoang, Yeming Wen, Eric Langlois, Shunshi
Zhang, Guodong Zhang, Pieter Abbeel, and Jimmy Ba. Benchmarking model-based reinforcement
learning. arXiv preprint arXiv:1907.02057, 2019.
Yifan Wu, George Tucker, and Ofir Nachum. Behavior regularized offline reinforcement learning.
arXiv preprint arXiv:1911.11361, 2019.
Tianhe Yu, Garrett Thomas, Lantao Yu, Stefano Ermon, James Y Zou, Sergey Levine, Chelsea Finn,
and Tengyu Ma. Mopo: Model-based offline policy optimization. Advances in Neural Information
Processing Systems, 33:14129-14142, 2020.
Marvin Zhang, Sharad Vikram, Laura Smith, Pieter Abbeel, Matthew Johnson, and Sergey Levine.
Solar: Deep structured representations for model-based reinforcement learning. In International
Conference on Machine Learning, pp. 7444-7453. PMLR, 2019.
Shangtong Zhang, Bo Liu, and Shimon Whiteson. GradientDICE: Rethinking generalized offline
estimation of stationary values. In Hal Daume III and Aarti Singh (eds.), Proceedings of the 37th
International Conference on Machine Learning, volume 119 of Proceedings of Machine Learning
Research, pp. 11194-11203. PMLR, 13-18 Jul 2020. URL https://proceedings.mlr.
press/v119/zhang20r.html.
12
Under review as a conference paper at ICLR 2022
A Proof of Lemma 1
Adapted from Schulman et al. (2015); Luo et al. (2018), we let Wj be the expected return when
executing MM(i+1) for the first j steps, and then switch to MM(i) for the remaining steps.
Proof.
Wj =	E	{	γtr(st, at)|s0 = s}
at 〜π(St)	t=0
∀j>t≥0,St + 1 〜M (i+1)(∙∣st,at)∀t≥j,st + ι 〜M (i)(∙∣st,at)=
(7)
Thus, We have Wo = VM⑸,and W∞ = VM(i+n. Next, We write:
VM(i+1)
∞
VM (i) = X(Wj+ι- Wj)
j=0
(8)
—
We expand Wj and Wj+1 so that we can cancel the shared terms:
Wj= Rj + Sj,aj 〜EM (i+i)Lι 〜ME)(∙∣st,J+1vΜ (i) (Sj+1)}}
Wj+1 = Rj + WJEM (i+i)Lι 〜M(Ei)(∙∣st,aJYj+1 VM (i) (sj+ι)}}
where Rj is the expected return of the first j time step. Next, we cancel the share terms so that:
Wj+1 - Wj = γ
j+1 E
sj,aj 〜π,MΜ(i+1) s0 〜M (i+1)(∙∣sj,aj )
{VMM (i) (s0)}-s0 〜M (i)(∙∣Sj ,aj ){VMM (i)(s0)}}
(9)
E
Thus, based on eq. (9), we have:
VMM(i+1)
κE
Sj,aj〜π,M (i+1) S0 〜M (i+1) (∙∣Sj,aj)
S〜M品Sj,aj){VM(i)(S0)}}
(10)
—
E
—
Let ν(s, a) = E	{Vπ	(s0)} - E	{Vπ	(s0)}, since we have assumed
s'〜M(i+i)(∙"aj	M	S 〜M(i)(∙∣Sj,aj	M "'
the Lipschitzeness of VM, we can bound ∣ν(s, a)| ≤ L|MM(i+1)(s, a) - MM(i)(s, a)|, then combine
with triangle inequality, we have:
IVM(i+1) - VM(i) I ≤ KL E E	(kM(i+1)(s,a) - M(i)(s,a)k)	(11)
(s,a)~ρM(i+i)
□
B	Proof of Proposition 1
Proof. Starting from eqn.10, we substitute with VM* and VM⑸,and multiple both side with -1. Due
to the Lipschitzeness assumption, we can then bound ∣ν(s, a)| (with the corresponding substitution)
on eqn.10 and by the definition of UM⑸ M* (s, a), thus we have:
VM* ≥ VM(i)(s αEp∏JUm(i),M*(s,a)}.	(12)
□
13
Under review as a conference paper at ICLR 2022
E Exploration to Collect Data from Low S upport Region
In order to build an accurate representation of the world model, during each deployment, we want to
collect the data from the low support (or un-visited) regions. we make use of the uncertainty labeler
to guide exploration to the un-visited regions for novel data discovery. This is achieved by injecting
the ζ(a, s) as an exploration noise with the zero-mean normal distribution: N(0, σ = ζ(a, s)), where
ζ is:
Z(a, s) = max (∣kt+ι - ^t+ιkι)
i∈{Pφ }K
(13)
where ^t+ι is the predicted next state and We take the maximum prediction error of the model within
{儿}K ensemble. In the Ablation Study (SeCtion.5), we show that this exploration strategy leads to
novel data discovery, and also contribute to better learned model dynamics (Fig.4 subplots c. and d. ).
Specifically, the action will be parameterized by a stochastic Gaussian policy (with parameter μ钞)as:
at = tanh(μ 方(St)) + ECOnSt + EZ
(16)
where Econst and Eζ are:
ECOnSt 〜N(μ = 0, σ = 0.01)
tanh(μ方(St)) + ECOnSt, S
Here, Econst is an additive noise with a constant variance of 0.01, and on top of this, we also added
another Gaussian noise with variance equal to ζ(a, S) from the uncertainty labeler to guide exploration.
F	Detailed Implementation
We used ADAM as the optimizer with a learning rate of 1e-3 for the model dynamics T with
ensembles size of N = 5. For the uncertainty-labeler, we use ensembles of PNN with K = 3 and a
learning rate of 1e-3. For the behavioral cloning, we used a learning rate of 5e-4. For all the collected
data, we divided them into 85% for training, and 15% for validation (for model validation). The T , P
are trained with early stopping when their performance on the validation set no longer improves after
consecutive 3 episodes. We used this same setting for the behavioral cloning as well.
Model Architecture For our policy network, we parameterized it by two layers of fully-connected
neural network with hidden units of 200. For the T model dynamics, we parameterized it by two
layers of fully-connected neural network with hidden units of 1024. We used this same configuration
with P uncertainty-labeler and implemented with PNN.
Training Time The overall training time differs for each environment. We train all models on
Nvidia T4 GPU. For the 500k data size experiment, the entire training duration (1 run) for Walker2D
and Hopper environments are 18 hours and 26 hours respectively. For the Half-Cheetah, Ant, and
Cheetah-Run environment, it is a a lot more faster. It takes 7 hours, 12 hours, and 8 hours per run,
respectively. For the 250k data size experiments, the training time is about 25 minutes faster than the
500k experiments.
Since we are applying Dyna-style update with neural network based dynamics models, following
Wang et al. (2019) Matsushima et al. (2020), we used the following reward functions for our dynamics
models (for model-based training only) as:
•	Walker2d, Hopper: Xt — 0.001 kat k2 + 1
•	CheetahRun: max(0, min(Xt∕10,1))
•	Ant: Xt - 0.1∣∣at∣∣2 - 3.0(Zt - 0.57)2 + 1
•	HalfCheetah: Xt - 0.1||*|2
We enabled termination in rollouts for the Hopper and Walker2D environments, and disabled that
of the Ant, HalfCheetah, and CheetahRun environments (with a maximum step of 1000 for each
episode). For the CheetahRun task, we adopt it from the DM control suit3.
3https://github.com/deepmind/dm_control
14
Under review as a conference paper at ICLR 2022
I L R Rollouts Length ∣ TRPO's δ
Ant	2,000	250	0.05
HalfCheetah	2,000	250	0.1
Hopper	6,000	1,000	0.05
Walker2d	2,000	1,000	0.05
CheetahRun	2,000	250	0.05
Table 2: Hyper-parameters for MUSBO Algorithm
Other Hyper-parameters We searched α over the set of {0.28, 0.028, 0.0028} and we used the
same α = 0.028 (the temperature parameter for eq.(5)) for all environments. Similarly, for the
action parameterization eq. (16), we used the same constant σ = 0.01 (variance term of const) for all
environments. The σ of const is searched over the set of {0.01, 0.05, 0.1}.
We searched the rollout length on {250, 1000}, and the δ on {0.01, 0.05, 0.1}. We summarized these
three parameters (L, Rollouts Length, δ) as above in table 2.
For discount factor γ and GAE λ, we used the same set of hyperparmaters as in Wang et al. (2019).
Specifically, we used the same γ = 0.99 for all environment. Also, we used GAE λ = 0.95 for all
environment except for Ant which has a GAE λ = 0.97.
Hyper-parameters for Baseline. For the BREMEN, we used the exact parameters as Matsushima
et al. (2020). For MOPO, we adapted it to the deployment setting. We used the latest learned policy for
deployment, and then launched it to collect data batch of size |B |. For the CheetahRun task, we used
the same set of parameters of HalfCheetah. On all environments, we tried hyper-parameters search
on the ranges as originally proposed by the paper Yu et al. (2020), we didn’t find any improvement
over the same set of parameters as originally proposed. Thus, we used the same set of parameters as
originally proposed.
Training of the Probabilistic Neural Networks (PNN). In PNN (Chua et al., 2018), the network has
its output neurons parameterized by a Gaussian distribution in the effort of capturing the uncertainty.
This module is trained to minimize the following loss:
K
lossPNN (φ) =)[{μφ(sn, On) - sn+1 }，$ (Sn, an)(μφ(sn, On) - sn+1) + log det^0(sn, On)
n=1
(17)
where φ is the neural network learning parameters, μφ and Σφ are the mean and variance of the
Gaussian distribution.
15
Under review as a conference paper at ICLR 2022
G ABLATION: HOW GOOD IS THE APPROXIMATED U (s, a) IN ESTIMATING
THE ACTUAL UM⑶ M* (s, a) ?
0.985-
0.980-
0.975-
0.970-
0.965-
0.960-
0.955-
0.950-
0.945-
Uncertainty Regularized Coefficient (Averaged)
4uωp⅛ω0u P<υzμe-n6ω0:AluroEuun pφ-4->elu×0.Jdd<
2
9
Ci
9 8 7 6 5 4 3
9 9 9 9 9 9 9
0.0,6 6 6 6c；
IU ① PE① 0。P ① Zμe-n6 ①=AmJHUn
Figure 5: Ablation study on I × |B| = 500k setting for the ant environment. we plot the approximated
U(s, a) on the bottom graph and We plot the true UM⑴ M* (s, a) on the top graph. The approximated
U(s, a) is very good at estimating the actual UM(i) M* (s, a).
In this section, We study the empirical values of the approximated U(s, a) and the actual
UM(i) M* (s,a) to verify how good our approximation in estimating the real one. The reason for not
training with the exact definition of the UM⑸ M* (s, a) directly (but instead approximating it with
model estimation error) is because of the instability effect that we observed empirically. Having
the value function at the denominator can sometimes cause instability issues especially at the early
training stage. On Fig.5, we show the empirical values of the actual UM(i) M* (s, a) at the top (subplot
a) and we show the approximated U(s, a) (subplot b) at the bottom. As is shown on the plot, the
estimated U(s, a) is very good in approximating the actual UM⑴ M* (s, a).
16